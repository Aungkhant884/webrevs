{"files":[{"patch":"@@ -170,0 +170,2 @@\n+        $d\/cpu\/$(HOTSPOT_TARGET_CPU_ARCH)\/gc\/x\/x_$(HOTSPOT_TARGET_CPU).ad \\\n+        $d\/cpu\/$(HOTSPOT_TARGET_CPU_ARCH)\/gc\/x\/x_$(HOTSPOT_TARGET_CPU_ARCH).ad \\\n","filename":"make\/hotspot\/gensrc\/GensrcAdlc.gmk","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -152,0 +152,1 @@\n+  JVM_EXCLUDE_PATTERNS += gc\/x\n","filename":"make\/hotspot\/lib\/JvmFeatures.gmk","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -1013,1 +1013,1 @@\n-    if (!UseZGC) {\n+    if (!(UseZGC && !ZGenerational)) {\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIRAssembler_aarch64.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -101,7 +101,0 @@\n-\/\/ Store the instruction bitmask, bits and name for checking the barrier.\n-struct CheckInsn {\n-  uint32_t mask;\n-  uint32_t bits;\n-  const char *name;\n-};\n-\n@@ -113,4 +106,5 @@\n-  if ((inst & 0xff000000) != 0x18000000) {\n-    tty->print_cr(\"Addr: \" INTPTR_FORMAT \" Code: 0x%x\", (intptr_t)addr, inst);\n-    fatal(\"not an ldr (literal) instruction.\");\n-  }\n+  \/\/ Check if the barrier starts witha ldr (literal) as expected.\n+  guarantee((inst & 0xff000000) == 0x18000000,\n+            \"Nmethod entry barrier did not start with ldr (literal) as expected. \"\n+            \"Addr: \" PTR_FORMAT \" Code: \" UINT32_FORMAT,\n+            p2i(addr), inst);\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/shared\/barrierSetNMethod_aarch64.cpp","additions":6,"deletions":12,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -0,0 +1,462 @@\n+\/*\n+ * Copyright (c) 2019, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"asm\/macroAssembler.inline.hpp\"\n+#include \"code\/codeBlob.hpp\"\n+#include \"code\/vmreg.inline.hpp\"\n+#include \"gc\/x\/xBarrier.inline.hpp\"\n+#include \"gc\/x\/xBarrierSet.hpp\"\n+#include \"gc\/x\/xBarrierSetAssembler.hpp\"\n+#include \"gc\/x\/xBarrierSetRuntime.hpp\"\n+#include \"gc\/x\/xThreadLocalData.hpp\"\n+#include \"memory\/resourceArea.hpp\"\n+#include \"runtime\/sharedRuntime.hpp\"\n+#include \"utilities\/macros.hpp\"\n+#ifdef COMPILER1\n+#include \"c1\/c1_LIRAssembler.hpp\"\n+#include \"c1\/c1_MacroAssembler.hpp\"\n+#include \"gc\/x\/c1\/xBarrierSetC1.hpp\"\n+#endif \/\/ COMPILER1\n+#ifdef COMPILER2\n+#include \"gc\/x\/c2\/xBarrierSetC2.hpp\"\n+#endif \/\/ COMPILER2\n+\n+#ifdef PRODUCT\n+#define BLOCK_COMMENT(str) \/* nothing *\/\n+#else\n+#define BLOCK_COMMENT(str) __ block_comment(str)\n+#endif\n+\n+#undef __\n+#define __ masm->\n+\n+void XBarrierSetAssembler::load_at(MacroAssembler* masm,\n+                                   DecoratorSet decorators,\n+                                   BasicType type,\n+                                   Register dst,\n+                                   Address src,\n+                                   Register tmp1,\n+                                   Register tmp2) {\n+  if (!XBarrierSet::barrier_needed(decorators, type)) {\n+    \/\/ Barrier not needed\n+    BarrierSetAssembler::load_at(masm, decorators, type, dst, src, tmp1, tmp2);\n+    return;\n+  }\n+\n+  assert_different_registers(rscratch1, rscratch2, src.base());\n+  assert_different_registers(rscratch1, rscratch2, dst);\n+\n+  Label done;\n+\n+  \/\/ Load bad mask into scratch register.\n+  __ ldr(rscratch1, address_bad_mask_from_thread(rthread));\n+  __ lea(rscratch2, src);\n+  __ ldr(dst, src);\n+\n+  \/\/ Test reference against bad mask. If mask bad, then we need to fix it up.\n+  __ tst(dst, rscratch1);\n+  __ br(Assembler::EQ, done);\n+\n+  __ enter(\/*strip_ret_addr*\/true);\n+\n+  __ push_call_clobbered_registers_except(RegSet::of(dst));\n+\n+  if (c_rarg0 != dst) {\n+    __ mov(c_rarg0, dst);\n+  }\n+  __ mov(c_rarg1, rscratch2);\n+\n+  __ call_VM_leaf(XBarrierSetRuntime::load_barrier_on_oop_field_preloaded_addr(decorators), 2);\n+\n+  \/\/ Make sure dst has the return value.\n+  if (dst != r0) {\n+    __ mov(dst, r0);\n+  }\n+\n+  __ pop_call_clobbered_registers_except(RegSet::of(dst));\n+  __ leave();\n+\n+  __ bind(done);\n+}\n+\n+#ifdef ASSERT\n+\n+void XBarrierSetAssembler::store_at(MacroAssembler* masm,\n+                                        DecoratorSet decorators,\n+                                        BasicType type,\n+                                        Address dst,\n+                                        Register val,\n+                                        Register tmp1,\n+                                        Register tmp2,\n+                                        Register tmp3) {\n+  \/\/ Verify value\n+  if (is_reference_type(type)) {\n+    \/\/ Note that src could be noreg, which means we\n+    \/\/ are storing null and can skip verification.\n+    if (val != noreg) {\n+      Label done;\n+\n+      \/\/ tmp1, tmp2 and tmp3 are often set to noreg.\n+      RegSet savedRegs = RegSet::of(rscratch1);\n+      __ push(savedRegs, sp);\n+\n+      __ ldr(rscratch1, address_bad_mask_from_thread(rthread));\n+      __ tst(val, rscratch1);\n+      __ br(Assembler::EQ, done);\n+      __ stop(\"Verify oop store failed\");\n+      __ should_not_reach_here();\n+      __ bind(done);\n+      __ pop(savedRegs, sp);\n+    }\n+  }\n+\n+  \/\/ Store value\n+  BarrierSetAssembler::store_at(masm, decorators, type, dst, val, tmp1, tmp2, noreg);\n+}\n+\n+#endif \/\/ ASSERT\n+\n+void XBarrierSetAssembler::arraycopy_prologue(MacroAssembler* masm,\n+                                              DecoratorSet decorators,\n+                                              bool is_oop,\n+                                              Register src,\n+                                              Register dst,\n+                                              Register count,\n+                                              RegSet saved_regs) {\n+  if (!is_oop) {\n+    \/\/ Barrier not needed\n+    return;\n+  }\n+\n+  BLOCK_COMMENT(\"XBarrierSetAssembler::arraycopy_prologue {\");\n+\n+  assert_different_registers(src, count, rscratch1);\n+\n+  __ push(saved_regs, sp);\n+\n+  if (count == c_rarg0) {\n+    if (src == c_rarg1) {\n+      \/\/ exactly backwards!!\n+      __ mov(rscratch1, c_rarg0);\n+      __ mov(c_rarg0, c_rarg1);\n+      __ mov(c_rarg1, rscratch1);\n+    } else {\n+      __ mov(c_rarg1, count);\n+      __ mov(c_rarg0, src);\n+    }\n+  } else {\n+    __ mov(c_rarg0, src);\n+    __ mov(c_rarg1, count);\n+  }\n+\n+  __ call_VM_leaf(XBarrierSetRuntime::load_barrier_on_oop_array_addr(), 2);\n+\n+  __ pop(saved_regs, sp);\n+\n+  BLOCK_COMMENT(\"} XBarrierSetAssembler::arraycopy_prologue\");\n+}\n+\n+void XBarrierSetAssembler::try_resolve_jobject_in_native(MacroAssembler* masm,\n+                                                         Register jni_env,\n+                                                         Register robj,\n+                                                         Register tmp,\n+                                                         Label& slowpath) {\n+  BLOCK_COMMENT(\"XBarrierSetAssembler::try_resolve_jobject_in_native {\");\n+\n+  assert_different_registers(jni_env, robj, tmp);\n+\n+  \/\/ Resolve jobject\n+  BarrierSetAssembler::try_resolve_jobject_in_native(masm, jni_env, robj, tmp, slowpath);\n+\n+  \/\/ The Address offset is too large to direct load - -784. Our range is +127, -128.\n+  __ mov(tmp, (int64_t)(in_bytes(XThreadLocalData::address_bad_mask_offset()) -\n+              in_bytes(JavaThread::jni_environment_offset())));\n+\n+  \/\/ Load address bad mask\n+  __ add(tmp, jni_env, tmp);\n+  __ ldr(tmp, Address(tmp));\n+\n+  \/\/ Check address bad mask\n+  __ tst(robj, tmp);\n+  __ br(Assembler::NE, slowpath);\n+\n+  BLOCK_COMMENT(\"} XBarrierSetAssembler::try_resolve_jobject_in_native\");\n+}\n+\n+#ifdef COMPILER1\n+\n+#undef __\n+#define __ ce->masm()->\n+\n+void XBarrierSetAssembler::generate_c1_load_barrier_test(LIR_Assembler* ce,\n+                                                         LIR_Opr ref) const {\n+  assert_different_registers(rscratch1, rthread, ref->as_register());\n+\n+  __ ldr(rscratch1, address_bad_mask_from_thread(rthread));\n+  __ tst(ref->as_register(), rscratch1);\n+}\n+\n+void XBarrierSetAssembler::generate_c1_load_barrier_stub(LIR_Assembler* ce,\n+                                                         XLoadBarrierStubC1* stub) const {\n+  \/\/ Stub entry\n+  __ bind(*stub->entry());\n+\n+  Register ref = stub->ref()->as_register();\n+  Register ref_addr = noreg;\n+  Register tmp = noreg;\n+\n+  if (stub->tmp()->is_valid()) {\n+    \/\/ Load address into tmp register\n+    ce->leal(stub->ref_addr(), stub->tmp());\n+    ref_addr = tmp = stub->tmp()->as_pointer_register();\n+  } else {\n+    \/\/ Address already in register\n+    ref_addr = stub->ref_addr()->as_address_ptr()->base()->as_pointer_register();\n+  }\n+\n+  assert_different_registers(ref, ref_addr, noreg);\n+\n+  \/\/ Save r0 unless it is the result or tmp register\n+  \/\/ Set up SP to accommodate parameters and maybe r0..\n+  if (ref != r0 && tmp != r0) {\n+    __ sub(sp, sp, 32);\n+    __ str(r0, Address(sp, 16));\n+  } else {\n+    __ sub(sp, sp, 16);\n+  }\n+\n+  \/\/ Setup arguments and call runtime stub\n+  ce->store_parameter(ref_addr, 1);\n+  ce->store_parameter(ref, 0);\n+\n+  __ far_call(stub->runtime_stub());\n+\n+  \/\/ Verify result\n+  __ verify_oop(r0);\n+\n+  \/\/ Move result into place\n+  if (ref != r0) {\n+    __ mov(ref, r0);\n+  }\n+\n+  \/\/ Restore r0 unless it is the result or tmp register\n+  if (ref != r0 && tmp != r0) {\n+    __ ldr(r0, Address(sp, 16));\n+    __ add(sp, sp, 32);\n+  } else {\n+    __ add(sp, sp, 16);\n+  }\n+\n+  \/\/ Stub exit\n+  __ b(*stub->continuation());\n+}\n+\n+#undef __\n+#define __ sasm->\n+\n+void XBarrierSetAssembler::generate_c1_load_barrier_runtime_stub(StubAssembler* sasm,\n+                                                                 DecoratorSet decorators) const {\n+  __ prologue(\"zgc_load_barrier stub\", false);\n+\n+  __ push_call_clobbered_registers_except(RegSet::of(r0));\n+\n+  \/\/ Setup arguments\n+  __ load_parameter(0, c_rarg0);\n+  __ load_parameter(1, c_rarg1);\n+\n+  __ call_VM_leaf(XBarrierSetRuntime::load_barrier_on_oop_field_preloaded_addr(decorators), 2);\n+\n+  __ pop_call_clobbered_registers_except(RegSet::of(r0));\n+\n+  __ epilogue();\n+}\n+#endif \/\/ COMPILER1\n+\n+#ifdef COMPILER2\n+\n+OptoReg::Name XBarrierSetAssembler::refine_register(const Node* node, OptoReg::Name opto_reg) {\n+  if (!OptoReg::is_reg(opto_reg)) {\n+    return OptoReg::Bad;\n+  }\n+\n+  const VMReg vm_reg = OptoReg::as_VMReg(opto_reg);\n+  if (vm_reg->is_FloatRegister()) {\n+    return opto_reg & ~1;\n+  }\n+\n+  return opto_reg;\n+}\n+\n+#undef __\n+#define __ _masm->\n+\n+class XSaveLiveRegisters {\n+private:\n+  MacroAssembler* const _masm;\n+  RegSet                _gp_regs;\n+  FloatRegSet           _fp_regs;\n+  PRegSet               _p_regs;\n+\n+public:\n+  void initialize(XLoadBarrierStubC2* stub) {\n+    \/\/ Record registers that needs to be saved\/restored\n+    RegMaskIterator rmi(stub->live());\n+    while (rmi.has_next()) {\n+      const OptoReg::Name opto_reg = rmi.next();\n+      if (OptoReg::is_reg(opto_reg)) {\n+        const VMReg vm_reg = OptoReg::as_VMReg(opto_reg);\n+        if (vm_reg->is_Register()) {\n+          _gp_regs += RegSet::of(vm_reg->as_Register());\n+        } else if (vm_reg->is_FloatRegister()) {\n+          _fp_regs += FloatRegSet::of(vm_reg->as_FloatRegister());\n+        } else if (vm_reg->is_PRegister()) {\n+          _p_regs += PRegSet::of(vm_reg->as_PRegister());\n+        } else {\n+          fatal(\"Unknown register type\");\n+        }\n+      }\n+    }\n+\n+    \/\/ Remove C-ABI SOE registers, scratch regs and _ref register that will be updated\n+    _gp_regs -= RegSet::range(r19, r30) + RegSet::of(r8, r9, stub->ref());\n+  }\n+\n+  XSaveLiveRegisters(MacroAssembler* masm, XLoadBarrierStubC2* stub) :\n+      _masm(masm),\n+      _gp_regs(),\n+      _fp_regs(),\n+      _p_regs() {\n+\n+    \/\/ Figure out what registers to save\/restore\n+    initialize(stub);\n+\n+    \/\/ Save registers\n+    __ push(_gp_regs, sp);\n+    __ push_fp(_fp_regs, sp);\n+    __ push_p(_p_regs, sp);\n+  }\n+\n+  ~XSaveLiveRegisters() {\n+    \/\/ Restore registers\n+    __ pop_p(_p_regs, sp);\n+    __ pop_fp(_fp_regs, sp);\n+\n+    \/\/ External runtime call may clobber ptrue reg\n+    __ reinitialize_ptrue();\n+\n+    __ pop(_gp_regs, sp);\n+  }\n+};\n+\n+#undef __\n+#define __ _masm->\n+\n+class XSetupArguments {\n+private:\n+  MacroAssembler* const _masm;\n+  const Register        _ref;\n+  const Address         _ref_addr;\n+\n+public:\n+  XSetupArguments(MacroAssembler* masm, XLoadBarrierStubC2* stub) :\n+      _masm(masm),\n+      _ref(stub->ref()),\n+      _ref_addr(stub->ref_addr()) {\n+\n+    \/\/ Setup arguments\n+    if (_ref_addr.base() == noreg) {\n+      \/\/ No self healing\n+      if (_ref != c_rarg0) {\n+        __ mov(c_rarg0, _ref);\n+      }\n+      __ mov(c_rarg1, 0);\n+    } else {\n+      \/\/ Self healing\n+      if (_ref == c_rarg0) {\n+        \/\/ _ref is already at correct place\n+        __ lea(c_rarg1, _ref_addr);\n+      } else if (_ref != c_rarg1) {\n+        \/\/ _ref is in wrong place, but not in c_rarg1, so fix it first\n+        __ lea(c_rarg1, _ref_addr);\n+        __ mov(c_rarg0, _ref);\n+      } else if (_ref_addr.base() != c_rarg0 && _ref_addr.index() != c_rarg0) {\n+        assert(_ref == c_rarg1, \"Mov ref first, vacating c_rarg0\");\n+        __ mov(c_rarg0, _ref);\n+        __ lea(c_rarg1, _ref_addr);\n+      } else {\n+        assert(_ref == c_rarg1, \"Need to vacate c_rarg1 and _ref_addr is using c_rarg0\");\n+        if (_ref_addr.base() == c_rarg0 || _ref_addr.index() == c_rarg0) {\n+          __ mov(rscratch2, c_rarg1);\n+          __ lea(c_rarg1, _ref_addr);\n+          __ mov(c_rarg0, rscratch2);\n+        } else {\n+          ShouldNotReachHere();\n+        }\n+      }\n+    }\n+  }\n+\n+  ~XSetupArguments() {\n+    \/\/ Transfer result\n+    if (_ref != r0) {\n+      __ mov(_ref, r0);\n+    }\n+  }\n+};\n+\n+#undef __\n+#define __ masm->\n+\n+void XBarrierSetAssembler::generate_c2_load_barrier_stub(MacroAssembler* masm, XLoadBarrierStubC2* stub) const {\n+  BLOCK_COMMENT(\"XLoadBarrierStubC2\");\n+\n+  \/\/ Stub entry\n+  __ bind(*stub->entry());\n+\n+  {\n+    XSaveLiveRegisters save_live_registers(masm, stub);\n+    XSetupArguments setup_arguments(masm, stub);\n+    __ mov(rscratch1, stub->slow_path());\n+    __ blr(rscratch1);\n+  }\n+  \/\/ Stub exit\n+  __ b(*stub->continuation());\n+}\n+\n+#endif \/\/ COMPILER2\n+\n+#undef __\n+#define __ masm->\n+\n+void XBarrierSetAssembler::check_oop(MacroAssembler* masm, Register obj, Register tmp1, Register tmp2, Label& error) {\n+  \/\/ Check if mask is good.\n+  \/\/ verifies that XAddressBadMask & r0 == 0\n+  __ ldr(tmp2, Address(rthread, XThreadLocalData::address_bad_mask_offset()));\n+  __ andr(tmp1, obj, tmp2);\n+  __ cbnz(tmp1, error);\n+\n+  BarrierSetAssembler::check_oop(masm, obj, tmp1, tmp2, error);\n+}\n+\n+#undef __\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/x\/xBarrierSetAssembler_aarch64.cpp","additions":462,"deletions":0,"binary":false,"changes":462,"status":"added"},{"patch":"@@ -0,0 +1,110 @@\n+\/*\n+ * Copyright (c) 2019, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef CPU_AARCH64_GC_X_XBARRIERSETASSEMBLER_AARCH64_HPP\n+#define CPU_AARCH64_GC_X_XBARRIERSETASSEMBLER_AARCH64_HPP\n+\n+#include \"code\/vmreg.hpp\"\n+#include \"oops\/accessDecorators.hpp\"\n+#ifdef COMPILER2\n+#include \"opto\/optoreg.hpp\"\n+#endif \/\/ COMPILER2\n+\n+#ifdef COMPILER1\n+class LIR_Assembler;\n+class LIR_Opr;\n+class StubAssembler;\n+#endif \/\/ COMPILER1\n+\n+#ifdef COMPILER2\n+class Node;\n+#endif \/\/ COMPILER2\n+\n+#ifdef COMPILER1\n+class XLoadBarrierStubC1;\n+#endif \/\/ COMPILER1\n+\n+#ifdef COMPILER2\n+class XLoadBarrierStubC2;\n+#endif \/\/ COMPILER2\n+\n+class XBarrierSetAssembler : public XBarrierSetAssemblerBase {\n+public:\n+  virtual void load_at(MacroAssembler* masm,\n+                       DecoratorSet decorators,\n+                       BasicType type,\n+                       Register dst,\n+                       Address src,\n+                       Register tmp1,\n+                       Register tmp2);\n+\n+#ifdef ASSERT\n+  virtual void store_at(MacroAssembler* masm,\n+                        DecoratorSet decorators,\n+                        BasicType type,\n+                        Address dst,\n+                        Register val,\n+                        Register tmp1,\n+                        Register tmp2,\n+                        Register tmp3);\n+#endif \/\/ ASSERT\n+\n+  virtual void arraycopy_prologue(MacroAssembler* masm,\n+                                  DecoratorSet decorators,\n+                                  bool is_oop,\n+                                  Register src,\n+                                  Register dst,\n+                                  Register count,\n+                                  RegSet saved_regs);\n+\n+  virtual void try_resolve_jobject_in_native(MacroAssembler* masm,\n+                                             Register jni_env,\n+                                             Register robj,\n+                                             Register tmp,\n+                                             Label& slowpath);\n+\n+  virtual NMethodPatchingType nmethod_patching_type() { return NMethodPatchingType::conc_data_patch; }\n+\n+#ifdef COMPILER1\n+  void generate_c1_load_barrier_test(LIR_Assembler* ce,\n+                                     LIR_Opr ref) const;\n+\n+  void generate_c1_load_barrier_stub(LIR_Assembler* ce,\n+                                     XLoadBarrierStubC1* stub) const;\n+\n+  void generate_c1_load_barrier_runtime_stub(StubAssembler* sasm,\n+                                             DecoratorSet decorators) const;\n+#endif \/\/ COMPILER1\n+\n+#ifdef COMPILER2\n+  OptoReg::Name refine_register(const Node* node,\n+                                OptoReg::Name opto_reg);\n+\n+  void generate_c2_load_barrier_stub(MacroAssembler* masm,\n+                                     XLoadBarrierStubC2* stub) const;\n+#endif \/\/ COMPILER2\n+\n+  void check_oop(MacroAssembler* masm, Register obj, Register tmp1, Register tmp2, Label& error);\n+};\n+\n+#endif \/\/ CPU_AARCH64_GC_X_XBARRIERSETASSEMBLER_AARCH64_HPP\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/x\/xBarrierSetAssembler_aarch64.hpp","additions":110,"deletions":0,"binary":false,"changes":110,"status":"added"},{"patch":"@@ -27,1 +27,1 @@\n-#include \"gc\/z\/zGlobals.hpp\"\n+#include \"gc\/x\/xGlobals.hpp\"\n@@ -199,1 +199,1 @@\n-size_t ZPlatformAddressOffsetBits() {\n+size_t XPlatformAddressOffsetBits() {\n@@ -203,1 +203,1 @@\n-  const size_t address_offset = round_up_power_of_2(MaxHeapSize * ZVirtualToPhysicalRatio);\n+  const size_t address_offset = round_up_power_of_2(MaxHeapSize * XVirtualToPhysicalRatio);\n@@ -208,2 +208,2 @@\n-size_t ZPlatformAddressMetadataShift() {\n-  return ZPlatformAddressOffsetBits();\n+size_t XPlatformAddressMetadataShift() {\n+  return XPlatformAddressOffsetBits();\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/x\/xGlobals_aarch64.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"previous_filename":"src\/hotspot\/cpu\/aarch64\/gc\/z\/zGlobals_aarch64.cpp","status":"renamed"},{"patch":"@@ -0,0 +1,33 @@\n+\/*\n+ * Copyright (c) 2015, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef CPU_AARCH64_GC_X_XGLOBALS_AARCH64_HPP\n+#define CPU_AARCH64_GC_X_XGLOBALS_AARCH64_HPP\n+\n+const size_t XPlatformHeapViews        = 3;\n+const size_t XPlatformCacheLineSize    = 64;\n+\n+size_t XPlatformAddressOffsetBits();\n+size_t XPlatformAddressMetadataShift();\n+\n+#endif \/\/ CPU_AARCH64_GC_X_XGLOBALS_AARCH64_HPP\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/x\/xGlobals_aarch64.hpp","additions":33,"deletions":0,"binary":false,"changes":33,"status":"added"},{"patch":"@@ -0,0 +1,243 @@\n+\/\/\n+\/\/ Copyright (c) 2019, 2021, Oracle and\/or its affiliates. All rights reserved.\n+\/\/ DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+\/\/\n+\/\/ This code is free software; you can redistribute it and\/or modify it\n+\/\/ under the terms of the GNU General Public License version 2 only, as\n+\/\/ published by the Free Software Foundation.\n+\/\/\n+\/\/ This code is distributed in the hope that it will be useful, but WITHOUT\n+\/\/ ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+\/\/ FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+\/\/ version 2 for more details (a copy is included in the LICENSE file that\n+\/\/ accompanied this code).\n+\/\/\n+\/\/ You should have received a copy of the GNU General Public License version\n+\/\/ 2 along with this work; if not, write to the Free Software Foundation,\n+\/\/ Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+\/\/\n+\/\/ Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+\/\/ or visit www.oracle.com if you need additional information or have any\n+\/\/ questions.\n+\/\/\n+\n+source_hpp %{\n+\n+#include \"gc\/shared\/gc_globals.hpp\"\n+#include \"gc\/x\/c2\/xBarrierSetC2.hpp\"\n+#include \"gc\/x\/xThreadLocalData.hpp\"\n+\n+%}\n+\n+source %{\n+\n+static void x_load_barrier(MacroAssembler& _masm, const MachNode* node, Address ref_addr, Register ref, Register tmp, uint8_t barrier_data) {\n+  if (barrier_data == XLoadBarrierElided) {\n+    return;\n+  }\n+  XLoadBarrierStubC2* const stub = XLoadBarrierStubC2::create(node, ref_addr, ref, tmp, barrier_data);\n+  __ ldr(tmp, Address(rthread, XThreadLocalData::address_bad_mask_offset()));\n+  __ andr(tmp, tmp, ref);\n+  __ cbnz(tmp, *stub->entry());\n+  __ bind(*stub->continuation());\n+}\n+\n+static void x_load_barrier_slow_path(MacroAssembler& _masm, const MachNode* node, Address ref_addr, Register ref, Register tmp) {\n+  XLoadBarrierStubC2* const stub = XLoadBarrierStubC2::create(node, ref_addr, ref, tmp, XLoadBarrierStrong);\n+  __ b(*stub->entry());\n+  __ bind(*stub->continuation());\n+}\n+\n+%}\n+\n+\/\/ Load Pointer\n+instruct xLoadP(iRegPNoSp dst, memory8 mem, rFlagsReg cr)\n+%{\n+  match(Set dst (LoadP mem));\n+  predicate(UseZGC && !ZGenerational && !needs_acquiring_load(n) && (n->as_Load()->barrier_data() != 0));\n+  effect(TEMP dst, KILL cr);\n+\n+  ins_cost(4 * INSN_COST);\n+\n+  format %{ \"ldr  $dst, $mem\" %}\n+\n+  ins_encode %{\n+    const Address ref_addr = mem2address($mem->opcode(), as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+    __ ldr($dst$$Register, ref_addr);\n+    x_load_barrier(_masm, this, ref_addr, $dst$$Register, rscratch2 \/* tmp *\/, barrier_data());\n+  %}\n+\n+  ins_pipe(iload_reg_mem);\n+%}\n+\n+\/\/ Load Pointer Volatile\n+instruct xLoadPVolatile(iRegPNoSp dst, indirect mem \/* sync_memory *\/, rFlagsReg cr)\n+%{\n+  match(Set dst (LoadP mem));\n+  predicate(UseZGC && !ZGenerational && needs_acquiring_load(n) && n->as_Load()->barrier_data() != 0);\n+  effect(TEMP dst, KILL cr);\n+\n+  ins_cost(VOLATILE_REF_COST);\n+\n+  format %{ \"ldar  $dst, $mem\\t\" %}\n+\n+  ins_encode %{\n+    __ ldar($dst$$Register, $mem$$Register);\n+    x_load_barrier(_masm, this, Address($mem$$Register), $dst$$Register, rscratch2 \/* tmp *\/, barrier_data());\n+  %}\n+\n+  ins_pipe(pipe_serial);\n+%}\n+\n+instruct xCompareAndSwapP(iRegINoSp res, indirect mem, iRegP oldval, iRegP newval, rFlagsReg cr) %{\n+  match(Set res (CompareAndSwapP mem (Binary oldval newval)));\n+  match(Set res (WeakCompareAndSwapP mem (Binary oldval newval)));\n+  predicate(UseZGC && !ZGenerational && !needs_acquiring_load_exclusive(n) && n->as_LoadStore()->barrier_data() == XLoadBarrierStrong);\n+  effect(KILL cr, TEMP_DEF res);\n+\n+  ins_cost(2 * VOLATILE_REF_COST);\n+\n+  format %{ \"cmpxchg $mem, $oldval, $newval\\n\\t\"\n+            \"cset    $res, EQ\" %}\n+\n+  ins_encode %{\n+    guarantee($mem$$index == -1 && $mem$$disp == 0, \"impossible encoding\");\n+    __ cmpxchg($mem$$Register, $oldval$$Register, $newval$$Register, Assembler::xword,\n+               false \/* acquire *\/, true \/* release *\/, false \/* weak *\/, rscratch2);\n+    __ cset($res$$Register, Assembler::EQ);\n+    if (barrier_data() != XLoadBarrierElided) {\n+      Label good;\n+      __ ldr(rscratch1, Address(rthread, XThreadLocalData::address_bad_mask_offset()));\n+      __ andr(rscratch1, rscratch1, rscratch2);\n+      __ cbz(rscratch1, good);\n+      x_load_barrier_slow_path(_masm, this, Address($mem$$Register), rscratch2 \/* ref *\/, rscratch1 \/* tmp *\/);\n+      __ cmpxchg($mem$$Register, $oldval$$Register, $newval$$Register, Assembler::xword,\n+                 false \/* acquire *\/, true \/* release *\/, false \/* weak *\/, rscratch2);\n+      __ cset($res$$Register, Assembler::EQ);\n+      __ bind(good);\n+    }\n+  %}\n+\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct xCompareAndSwapPAcq(iRegINoSp res, indirect mem, iRegP oldval, iRegP newval, rFlagsReg cr) %{\n+  match(Set res (CompareAndSwapP mem (Binary oldval newval)));\n+  match(Set res (WeakCompareAndSwapP mem (Binary oldval newval)));\n+  predicate(UseZGC && !ZGenerational && needs_acquiring_load_exclusive(n) && (n->as_LoadStore()->barrier_data() == XLoadBarrierStrong));\n+  effect(KILL cr, TEMP_DEF res);\n+\n+  ins_cost(2 * VOLATILE_REF_COST);\n+\n+ format %{ \"cmpxchg $mem, $oldval, $newval\\n\\t\"\n+           \"cset    $res, EQ\" %}\n+\n+  ins_encode %{\n+    guarantee($mem$$index == -1 && $mem$$disp == 0, \"impossible encoding\");\n+    __ cmpxchg($mem$$Register, $oldval$$Register, $newval$$Register, Assembler::xword,\n+               true \/* acquire *\/, true \/* release *\/, false \/* weak *\/, rscratch2);\n+    __ cset($res$$Register, Assembler::EQ);\n+    if (barrier_data() != XLoadBarrierElided) {\n+      Label good;\n+      __ ldr(rscratch1, Address(rthread, XThreadLocalData::address_bad_mask_offset()));\n+      __ andr(rscratch1, rscratch1, rscratch2);\n+      __ cbz(rscratch1, good);\n+      x_load_barrier_slow_path(_masm, this, Address($mem$$Register), rscratch2 \/* ref *\/, rscratch1 \/* tmp *\/ );\n+      __ cmpxchg($mem$$Register, $oldval$$Register, $newval$$Register, Assembler::xword,\n+                 true \/* acquire *\/, true \/* release *\/, false \/* weak *\/, rscratch2);\n+      __ cset($res$$Register, Assembler::EQ);\n+      __ bind(good);\n+    }\n+  %}\n+\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct xCompareAndExchangeP(iRegPNoSp res, indirect mem, iRegP oldval, iRegP newval, rFlagsReg cr) %{\n+  match(Set res (CompareAndExchangeP mem (Binary oldval newval)));\n+  predicate(UseZGC && !ZGenerational && !needs_acquiring_load_exclusive(n) && n->as_LoadStore()->barrier_data() == XLoadBarrierStrong);\n+  effect(TEMP_DEF res, KILL cr);\n+\n+  ins_cost(2 * VOLATILE_REF_COST);\n+\n+  format %{ \"cmpxchg $res = $mem, $oldval, $newval\" %}\n+\n+  ins_encode %{\n+    guarantee($mem$$index == -1 && $mem$$disp == 0, \"impossible encoding\");\n+    __ cmpxchg($mem$$Register, $oldval$$Register, $newval$$Register, Assembler::xword,\n+               false \/* acquire *\/, true \/* release *\/, false \/* weak *\/, $res$$Register);\n+    if (barrier_data() != XLoadBarrierElided) {\n+      Label good;\n+      __ ldr(rscratch1, Address(rthread, XThreadLocalData::address_bad_mask_offset()));\n+      __ andr(rscratch1, rscratch1, $res$$Register);\n+      __ cbz(rscratch1, good);\n+      x_load_barrier_slow_path(_masm, this, Address($mem$$Register), $res$$Register \/* ref *\/, rscratch1 \/* tmp *\/);\n+      __ cmpxchg($mem$$Register, $oldval$$Register, $newval$$Register, Assembler::xword,\n+                 false \/* acquire *\/, true \/* release *\/, false \/* weak *\/, $res$$Register);\n+      __ bind(good);\n+    }\n+  %}\n+\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct xCompareAndExchangePAcq(iRegPNoSp res, indirect mem, iRegP oldval, iRegP newval, rFlagsReg cr) %{\n+  match(Set res (CompareAndExchangeP mem (Binary oldval newval)));\n+  predicate(UseZGC && !ZGenerational && needs_acquiring_load_exclusive(n) && n->as_LoadStore()->barrier_data() == XLoadBarrierStrong);\n+  effect(TEMP_DEF res, KILL cr);\n+\n+  ins_cost(2 * VOLATILE_REF_COST);\n+\n+  format %{ \"cmpxchg $res = $mem, $oldval, $newval\" %}\n+\n+  ins_encode %{\n+    guarantee($mem$$index == -1 && $mem$$disp == 0, \"impossible encoding\");\n+    __ cmpxchg($mem$$Register, $oldval$$Register, $newval$$Register, Assembler::xword,\n+               true \/* acquire *\/, true \/* release *\/, false \/* weak *\/, $res$$Register);\n+    if (barrier_data() != XLoadBarrierElided) {\n+      Label good;\n+      __ ldr(rscratch1, Address(rthread, XThreadLocalData::address_bad_mask_offset()));\n+      __ andr(rscratch1, rscratch1, $res$$Register);\n+      __ cbz(rscratch1, good);\n+      x_load_barrier_slow_path(_masm, this, Address($mem$$Register), $res$$Register \/* ref *\/, rscratch1 \/* tmp *\/);\n+      __ cmpxchg($mem$$Register, $oldval$$Register, $newval$$Register, Assembler::xword,\n+                 true \/* acquire *\/, true \/* release *\/, false \/* weak *\/, $res$$Register);\n+      __ bind(good);\n+    }\n+  %}\n+\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct xGetAndSetP(indirect mem, iRegP newv, iRegPNoSp prev, rFlagsReg cr) %{\n+  match(Set prev (GetAndSetP mem newv));\n+  predicate(UseZGC && !ZGenerational && !needs_acquiring_load_exclusive(n) && n->as_LoadStore()->barrier_data() != 0);\n+  effect(TEMP_DEF prev, KILL cr);\n+\n+  ins_cost(2 * VOLATILE_REF_COST);\n+\n+  format %{ \"atomic_xchg  $prev, $newv, [$mem]\" %}\n+\n+  ins_encode %{\n+    __ atomic_xchg($prev$$Register, $newv$$Register, $mem$$Register);\n+    x_load_barrier(_masm, this, Address(noreg, 0), $prev$$Register, rscratch2 \/* tmp *\/, barrier_data());\n+  %}\n+\n+  ins_pipe(pipe_serial);\n+%}\n+\n+instruct xGetAndSetPAcq(indirect mem, iRegP newv, iRegPNoSp prev, rFlagsReg cr) %{\n+  match(Set prev (GetAndSetP mem newv));\n+  predicate(UseZGC && !ZGenerational && needs_acquiring_load_exclusive(n) && (n->as_LoadStore()->barrier_data() != 0));\n+  effect(TEMP_DEF prev, KILL cr);\n+\n+  ins_cost(VOLATILE_REF_COST);\n+\n+  format %{ \"atomic_xchg_acq  $prev, $newv, [$mem]\" %}\n+\n+  ins_encode %{\n+    __ atomic_xchgal($prev$$Register, $newv$$Register, $mem$$Register);\n+    x_load_barrier(_masm, this, Address(noreg, 0), $prev$$Register, rscratch2 \/* tmp *\/, barrier_data());\n+  %}\n+  ins_pipe(pipe_serial);\n+%}\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/x\/x_aarch64.ad","additions":243,"deletions":0,"binary":false,"changes":243,"status":"added"},{"patch":"@@ -0,0 +1,108 @@\n+\/*\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/shared\/gcLogPrecious.hpp\"\n+#include \"gc\/shared\/gc_globals.hpp\"\n+#include \"gc\/z\/zAddress.hpp\"\n+#include \"gc\/z\/zBarrierSetAssembler.hpp\"\n+#include \"gc\/z\/zGlobals.hpp\"\n+#include \"runtime\/globals.hpp\"\n+#include \"runtime\/os.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include \"utilities\/powerOfTwo.hpp\"\n+\n+#ifdef LINUX\n+#include <sys\/mman.h>\n+#endif \/\/ LINUX\n+\n+\/\/ Default value if probing is not implemented for a certain platform: 128TB\n+static const size_t DEFAULT_MAX_ADDRESS_BIT = 47;\n+\/\/ Minimum value returned, if probing fails: 64GB\n+static const size_t MINIMUM_MAX_ADDRESS_BIT = 36;\n+\n+static size_t probe_valid_max_address_bit() {\n+#ifdef LINUX\n+  size_t max_address_bit = 0;\n+  const size_t page_size = os::vm_page_size();\n+  for (size_t i = DEFAULT_MAX_ADDRESS_BIT; i > MINIMUM_MAX_ADDRESS_BIT; --i) {\n+    const uintptr_t base_addr = ((uintptr_t) 1U) << i;\n+    if (msync((void*)base_addr, page_size, MS_ASYNC) == 0) {\n+      \/\/ msync suceeded, the address is valid, and maybe even already mapped.\n+      max_address_bit = i;\n+      break;\n+    }\n+    if (errno != ENOMEM) {\n+      \/\/ Some error occured. This should never happen, but msync\n+      \/\/ has some undefined behavior, hence ignore this bit.\n+#ifdef ASSERT\n+      fatal(\"Received '%s' while probing the address space for the highest valid bit\", os::errno_name(errno));\n+#else \/\/ ASSERT\n+      log_warning_p(gc)(\"Received '%s' while probing the address space for the highest valid bit\", os::errno_name(errno));\n+#endif \/\/ ASSERT\n+      continue;\n+    }\n+    \/\/ Since msync failed with ENOMEM, the page might not be mapped.\n+    \/\/ Try to map it, to see if the address is valid.\n+    void* const result_addr = mmap((void*) base_addr, page_size, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS|MAP_NORESERVE, -1, 0);\n+    if (result_addr != MAP_FAILED) {\n+      munmap(result_addr, page_size);\n+    }\n+    if ((uintptr_t) result_addr == base_addr) {\n+      \/\/ address is valid\n+      max_address_bit = i;\n+      break;\n+    }\n+  }\n+  if (max_address_bit == 0) {\n+    \/\/ probing failed, allocate a very high page and take that bit as the maximum\n+    const uintptr_t high_addr = ((uintptr_t) 1U) << DEFAULT_MAX_ADDRESS_BIT;\n+    void* const result_addr = mmap((void*) high_addr, page_size, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS|MAP_NORESERVE, -1, 0);\n+    if (result_addr != MAP_FAILED) {\n+      max_address_bit = BitsPerSize_t - count_leading_zeros((size_t) result_addr) - 1;\n+      munmap(result_addr, page_size);\n+    }\n+  }\n+  log_info_p(gc, init)(\"Probing address space for the highest valid bit: \" SIZE_FORMAT, max_address_bit);\n+  return MAX2(max_address_bit, MINIMUM_MAX_ADDRESS_BIT);\n+#else \/\/ LINUX\n+  return DEFAULT_MAX_ADDRESS_BIT;\n+#endif \/\/ LINUX\n+}\n+\n+size_t ZPlatformAddressOffsetBits() {\n+  const static size_t valid_max_address_offset_bits = probe_valid_max_address_bit() + 1;\n+  const size_t max_address_offset_bits = valid_max_address_offset_bits - 3;\n+  const size_t min_address_offset_bits = max_address_offset_bits - 2;\n+  const size_t address_offset = round_up_power_of_2(MaxHeapSize * ZVirtualToPhysicalRatio);\n+  const size_t address_offset_bits = log2i_exact(address_offset);\n+  return clamp(address_offset_bits, min_address_offset_bits, max_address_offset_bits);\n+}\n+\n+size_t ZPlatformAddressHeapBaseShift() {\n+  return ZPlatformAddressOffsetBits();\n+}\n+\n+void ZGlobalsPointers::pd_set_good_masks() {\n+  BarrierSetAssembler::clear_patching_epoch();\n+}\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/z\/zAddress_aarch64.cpp","additions":108,"deletions":0,"binary":false,"changes":108,"status":"added"},{"patch":"@@ -0,0 +1,34 @@\n+\/*\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef CPU_AARCH64_GC_Z_ZADDRESS_AARCH64_HPP\n+#define CPU_AARCH64_GC_Z_ZADDRESS_AARCH64_HPP\n+\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+const size_t ZPointerLoadShift = 16;\n+\n+size_t ZPlatformAddressOffsetBits();\n+size_t ZPlatformAddressHeapBaseShift();\n+\n+#endif \/\/ CPU_AARCH64_GC_Z_ZADDRESS_AARCH64_HPP\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/z\/zAddress_aarch64.hpp","additions":34,"deletions":0,"binary":false,"changes":34,"status":"added"},{"patch":"@@ -0,0 +1,37 @@\n+\/*\n+ * Copyright (c) 2019, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef CPU_AARCH64_GC_Z_ZADDRESS_AARCH64_INLINE_HPP\n+#define CPU_AARCH64_GC_Z_ZADDRESS_AARCH64_INLINE_HPP\n+\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+inline uintptr_t ZPointer::remap_bits(uintptr_t colored) {\n+  return (colored ^ ZPointerRemappedMask) & ZPointerRemappedMask;\n+}\n+\n+inline constexpr int ZPointer::load_shift_lookup(uintptr_t value) {\n+  return ZPointerLoadShift;\n+}\n+\n+#endif \/\/ CPU_AARCH64_GC_Z_ZADDRESS_AARCH64_INLINE_HPP\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/z\/zAddress_aarch64.inline.hpp","additions":37,"deletions":0,"binary":false,"changes":37,"status":"added"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"gc\/z\/zAddress.hpp\"\n@@ -34,0 +35,3 @@\n+#include \"nativeInst_aarch64.hpp\"\n+#include \"runtime\/icache.hpp\"\n+#include \"runtime\/jniHandles.hpp\"\n@@ -43,0 +47,1 @@\n+#include \"opto\/output.hpp\"\n@@ -54,0 +59,46 @@\n+\/\/ Helper for saving and restoring registers across a runtime call that does\n+\/\/ not have any live vector registers.\n+class ZRuntimeCallSpill {\n+private:\n+  MacroAssembler* _masm;\n+  Register _result;\n+\n+  void save() {\n+    MacroAssembler* masm = _masm;\n+\n+    __ enter(true \/* strip_ret_addr *\/);\n+    if (_result != noreg) {\n+      __ push_call_clobbered_registers_except(RegSet::of(_result));\n+    } else {\n+      __ push_call_clobbered_registers();\n+    }\n+  }\n+\n+  void restore() {\n+    MacroAssembler* masm = _masm;\n+\n+    if (_result != noreg) {\n+      \/\/ Make sure _result has the return value.\n+      if (_result != r0) {\n+        __ mov(_result, r0);\n+      }\n+\n+      __ pop_call_clobbered_registers_except(RegSet::of(_result));\n+    } else {\n+      __ pop_call_clobbered_registers();\n+    }\n+    __ leave();\n+  }\n+\n+public:\n+  ZRuntimeCallSpill(MacroAssembler* masm, Register result)\n+    : _masm(masm),\n+      _result(result) {\n+    save();\n+  }\n+\n+  ~ZRuntimeCallSpill() {\n+    restore();\n+  }\n+};\n+\n@@ -67,2 +118,4 @@\n-  assert_different_registers(rscratch1, rscratch2, src.base());\n-  assert_different_registers(rscratch1, rscratch2, dst);\n+  assert_different_registers(tmp1, tmp2, src.base(), noreg);\n+  assert_different_registers(tmp1, tmp2, src.index());\n+  assert_different_registers(tmp1, tmp2, dst, noreg);\n+  assert_different_registers(tmp2, rscratch1);\n@@ -71,0 +124,1 @@\n+  Label uncolor;\n@@ -73,3 +127,12 @@\n-  __ ldr(rscratch1, address_bad_mask_from_thread(rthread));\n-  __ lea(rscratch2, src);\n-  __ ldr(dst, src);\n+  const bool on_non_strong =\n+    (decorators & ON_WEAK_OOP_REF) != 0 ||\n+    (decorators & ON_PHANTOM_OOP_REF) != 0;\n+\n+  if (on_non_strong) {\n+    __ ldr(tmp1, mark_bad_mask_from_thread(rthread));\n+  } else {\n+    __ ldr(tmp1, load_bad_mask_from_thread(rthread));\n+  }\n+\n+  __ lea(tmp2, src);\n+  __ ldr(dst, tmp2);\n@@ -78,2 +141,2 @@\n-  __ tst(dst, rscratch1);\n-  __ br(Assembler::EQ, done);\n+  __ tst(dst, tmp1);\n+  __ br(Assembler::EQ, uncolor);\n@@ -81,1 +144,3 @@\n-  __ enter(\/*strip_ret_addr*\/true);\n+  {\n+    \/\/ Call VM\n+    ZRuntimeCallSpill rcs(masm, dst);\n@@ -83,1 +148,4 @@\n-  __ push_call_clobbered_registers_except(RegSet::of(dst));\n+    if (c_rarg0 != dst) {\n+      __ mov(c_rarg0, dst);\n+    }\n+    __ mov(c_rarg1, tmp2);\n@@ -85,2 +153,1 @@\n-  if (c_rarg0 != dst) {\n-    __ mov(c_rarg0, dst);\n+    __ call_VM_leaf(ZBarrierSetRuntime::load_barrier_on_oop_field_preloaded_addr(decorators), 2);\n@@ -88,1 +155,0 @@\n-  __ mov(c_rarg1, rscratch2);\n@@ -90,1 +156,2 @@\n-  __ call_VM_leaf(ZBarrierSetRuntime::load_barrier_on_oop_field_preloaded_addr(decorators), 2);\n+  \/\/ Slow-path has already uncolored\n+  __ b(done);\n@@ -92,4 +159,1 @@\n-  \/\/ Make sure dst has the return value.\n-  if (dst != r0) {\n-    __ mov(dst, r0);\n-  }\n+  __ bind(uncolor);\n@@ -97,2 +161,2 @@\n-  __ pop_call_clobbered_registers_except(RegSet::of(dst));\n-  __ leave();\n+  \/\/ Remove the color bits\n+  __ lsr(dst, dst, ZPointerLoadShift);\n@@ -103,1 +167,142 @@\n-#ifdef ASSERT\n+void ZBarrierSetAssembler::store_barrier_fast(MacroAssembler* masm,\n+                                              Address ref_addr,\n+                                              Register rnew_zaddress,\n+                                              Register rnew_zpointer,\n+                                              Register rtmp,\n+                                              bool in_nmethod,\n+                                              bool is_atomic,\n+                                              Label& medium_path,\n+                                              Label& medium_path_continuation) const {\n+  assert_different_registers(ref_addr.base(), rnew_zpointer, rtmp);\n+  assert_different_registers(ref_addr.index(), rnew_zpointer, rtmp);\n+  assert_different_registers(rnew_zaddress, rnew_zpointer, rtmp);\n+\n+  if (in_nmethod) {\n+    if (is_atomic) {\n+      __ ldrh(rtmp, ref_addr);\n+      \/\/ Atomic operations must ensure that the contents of memory are store-good before\n+      \/\/ an atomic operation can execute.\n+      \/\/ A not relocatable object could have spurious raw null pointers in its fields after\n+      \/\/ getting promoted to the old generation.\n+      __ relocate(barrier_Relocation::spec(), ZBarrierRelocationFormatStoreGoodBeforeMov);\n+      __ movzw(rnew_zpointer, barrier_Relocation::unpatched);\n+      __ cmpw(rtmp, rnew_zpointer);\n+    } else {\n+      __ ldr(rtmp, ref_addr);\n+      \/\/ Stores on relocatable objects never need to deal with raw null pointers in fields.\n+      \/\/ Raw null pointers may only exist in the young generation, as they get pruned when\n+      \/\/ the object is relocated to old. And no pre-write barrier needs to perform any action\n+      \/\/ in the young generation.\n+      __ relocate(barrier_Relocation::spec(), ZBarrierRelocationFormatStoreBadBeforeMov);\n+      __ movzw(rnew_zpointer, barrier_Relocation::unpatched);\n+      __ tst(rtmp, rnew_zpointer);\n+    }\n+    __ br(Assembler::NE, medium_path);\n+    __ bind(medium_path_continuation);\n+    assert_different_registers(rnew_zaddress, rnew_zpointer);\n+    __ relocate(barrier_Relocation::spec(), ZBarrierRelocationFormatStoreGoodBeforeMov);\n+    __ movzw(rnew_zpointer, barrier_Relocation::unpatched);\n+    __ orr(rnew_zpointer, rnew_zpointer, rnew_zaddress, Assembler::LSL, ZPointerLoadShift);\n+  } else {\n+    assert(!is_atomic, \"atomics outside of nmethods not supported\");\n+    __ lea(rtmp, ref_addr);\n+    __ ldr(rtmp, rtmp);\n+    __ ldr(rnew_zpointer, Address(rthread, ZThreadLocalData::store_bad_mask_offset()));\n+    __ tst(rtmp, rnew_zpointer);\n+    __ br(Assembler::NE, medium_path);\n+    __ bind(medium_path_continuation);\n+    if (rnew_zaddress == noreg) {\n+      __ eor(rnew_zpointer, rnew_zpointer, rnew_zpointer);\n+    } else {\n+      __ mov(rnew_zpointer, rnew_zaddress);\n+    }\n+\n+    \/\/ Load the current good shift, and add the color bits\n+    __ lsl(rnew_zpointer, rnew_zpointer, ZPointerLoadShift);\n+    __ ldr(rtmp, Address(rthread, ZThreadLocalData::store_good_mask_offset()));\n+    __ orr(rnew_zpointer, rnew_zpointer, rtmp);\n+  }\n+}\n+\n+static void store_barrier_buffer_add(MacroAssembler* masm,\n+                                     Address ref_addr,\n+                                     Register tmp1,\n+                                     Register tmp2,\n+                                     Label& slow_path) {\n+  Address buffer(rthread, ZThreadLocalData::store_barrier_buffer_offset());\n+  assert_different_registers(ref_addr.base(), ref_addr.index(), tmp1, tmp2);\n+\n+  __ ldr(tmp1, buffer);\n+\n+  \/\/ Combined pointer bump and check if the buffer is disabled or full\n+  __ ldr(tmp2, Address(tmp1, ZStoreBarrierBuffer::current_offset()));\n+  __ cmp(tmp2, (uint8_t)0);\n+  __ br(Assembler::EQ, slow_path);\n+\n+  \/\/ Bump the pointer\n+  __ sub(tmp2, tmp2, sizeof(ZStoreBarrierEntry));\n+  __ str(tmp2, Address(tmp1, ZStoreBarrierBuffer::current_offset()));\n+\n+  \/\/ Compute the buffer entry address\n+  __ lea(tmp2, Address(tmp2, ZStoreBarrierBuffer::buffer_offset()));\n+  __ add(tmp2, tmp2, tmp1);\n+\n+  \/\/ Compute and log the store address\n+  __ lea(tmp1, ref_addr);\n+  __ str(tmp1, Address(tmp2, in_bytes(ZStoreBarrierEntry::p_offset())));\n+\n+  \/\/ Load and log the prev value\n+  __ ldr(tmp1, tmp1);\n+  __ str(tmp1, Address(tmp2, in_bytes(ZStoreBarrierEntry::prev_offset())));\n+}\n+\n+void ZBarrierSetAssembler::store_barrier_medium(MacroAssembler* masm,\n+                                                Address ref_addr,\n+                                                Register rtmp1,\n+                                                Register rtmp2,\n+                                                Register rtmp3,\n+                                                bool is_native,\n+                                                bool is_atomic,\n+                                                Label& medium_path_continuation,\n+                                                Label& slow_path,\n+                                                Label& slow_path_continuation) const {\n+  assert_different_registers(ref_addr.base(), ref_addr.index(), rtmp1, rtmp2);\n+\n+  \/\/ The reason to end up in the medium path is that the pre-value was not 'good'.\n+\n+  if (is_native) {\n+    __ b(slow_path);\n+    __ bind(slow_path_continuation);\n+    __ b(medium_path_continuation);\n+  } else if (is_atomic) {\n+    \/\/ Atomic accesses can get to the medium fast path because the value was a\n+    \/\/ raw null value. If it was not null, then there is no doubt we need to take a slow path.\n+    __ lea(rtmp2, ref_addr);\n+    __ ldr(rtmp1, rtmp2);\n+    __ cbnz(rtmp1, slow_path);\n+\n+    \/\/ If we get this far, we know there is a young raw null value in the field.\n+    __ relocate(barrier_Relocation::spec(), ZBarrierRelocationFormatStoreGoodBeforeMov);\n+    __ movzw(rtmp1, barrier_Relocation::unpatched);\n+    __ cmpxchg(rtmp2, zr, rtmp1,\n+               Assembler::xword,\n+               false \/* acquire *\/, false \/* release *\/, true \/* weak *\/,\n+               rtmp3);\n+    __ br(Assembler::NE, slow_path);\n+\n+    __ bind(slow_path_continuation);\n+    __ b(medium_path_continuation);\n+  } else {\n+    \/\/ A non-atomic relocatable object won't get to the medium fast path due to a\n+    \/\/ raw null in the young generation. We only get here because the field is bad.\n+    \/\/ In this path we don't need any self healing, so we can avoid a runtime call\n+    \/\/ most of the time by buffering the store barrier to be applied lazily.\n+    store_barrier_buffer_add(masm,\n+                             ref_addr,\n+                             rtmp1,\n+                             rtmp2,\n+                             slow_path);\n+    __ bind(slow_path_continuation);\n+    __ b(medium_path_continuation);\n+  }\n+}\n@@ -106,25 +311,21 @@\n-                                        DecoratorSet decorators,\n-                                        BasicType type,\n-                                        Address dst,\n-                                        Register val,\n-                                        Register tmp1,\n-                                        Register tmp2,\n-                                        Register tmp3) {\n-  \/\/ Verify value\n-  if (is_reference_type(type)) {\n-    \/\/ Note that src could be noreg, which means we\n-    \/\/ are storing null and can skip verification.\n-    if (val != noreg) {\n-      Label done;\n-\n-      \/\/ tmp1, tmp2 and tmp3 are often set to noreg.\n-      RegSet savedRegs = RegSet::of(rscratch1);\n-      __ push(savedRegs, sp);\n-\n-      __ ldr(rscratch1, address_bad_mask_from_thread(rthread));\n-      __ tst(val, rscratch1);\n-      __ br(Assembler::EQ, done);\n-      __ stop(\"Verify oop store failed\");\n-      __ should_not_reach_here();\n-      __ bind(done);\n-      __ pop(savedRegs, sp);\n+                                    DecoratorSet decorators,\n+                                    BasicType type,\n+                                    Address dst,\n+                                    Register val,\n+                                    Register tmp1,\n+                                    Register tmp2,\n+                                    Register tmp3) {\n+  if (!ZBarrierSet::barrier_needed(decorators, type)) {\n+    BarrierSetAssembler::store_at(masm, decorators, type, dst, val, tmp1, tmp2, tmp3);\n+    return;\n+  }\n+\n+  bool dest_uninitialized = (decorators & IS_DEST_UNINITIALIZED) != 0;\n+\n+  assert_different_registers(val, tmp1, dst.base(), dst.index());\n+\n+  if (dest_uninitialized) {\n+    if (val == noreg) {\n+      __ eor(tmp1, tmp1, tmp1);\n+    } else {\n+      __ mov(tmp1, val);\n@@ -132,0 +333,34 @@\n+    \/\/ Add the color bits\n+    __ lsl(tmp1, tmp1, ZPointerLoadShift);\n+    __ ldr(tmp2, Address(rthread, ZThreadLocalData::store_good_mask_offset()));\n+    __ orr(tmp1, tmp2, tmp1);\n+  } else {\n+    Label done;\n+    Label medium;\n+    Label medium_continuation;\n+    Label slow;\n+    Label slow_continuation;\n+    store_barrier_fast(masm, dst, val, tmp1, tmp2, false, false, medium, medium_continuation);\n+    __ b(done);\n+    __ bind(medium);\n+    store_barrier_medium(masm,\n+                         dst,\n+                         tmp1,\n+                         tmp2,\n+                         noreg \/* tmp3 *\/,\n+                         false \/* is_native *\/,\n+                         false \/* is_atomic *\/,\n+                         medium_continuation,\n+                         slow,\n+                         slow_continuation);\n+\n+    __ bind(slow);\n+    {\n+      \/\/ Call VM\n+      ZRuntimeCallSpill rcs(masm, noreg);\n+      __ lea(c_rarg0, dst);\n+      __ MacroAssembler::call_VM_leaf(ZBarrierSetRuntime::store_barrier_on_oop_field_without_healing_addr(), 1);\n+    }\n+\n+    __ b(slow_continuation);\n+    __ bind(done);\n@@ -135,1 +370,1 @@\n-  BarrierSetAssembler::store_at(masm, decorators, type, dst, val, tmp1, tmp2, noreg);\n+  BarrierSetAssembler::store_at(masm, decorators, type, dst, tmp1, tmp2, tmp3, noreg);\n@@ -138,1 +373,68 @@\n-#endif \/\/ ASSERT\n+static FloatRegister z_copy_load_bad_vreg = v17;\n+static FloatRegister z_copy_store_good_vreg = v18;\n+static FloatRegister z_copy_store_bad_vreg = v19;\n+\n+static void load_wide_arraycopy_masks(MacroAssembler* masm) {\n+  __ lea(rscratch1, ExternalAddress((address)&ZPointerVectorLoadBadMask));\n+  __ ldrq(z_copy_load_bad_vreg, Address(rscratch1, 0));\n+  __ lea(rscratch1, ExternalAddress((address)&ZPointerVectorStoreBadMask));\n+  __ ldrq(z_copy_store_bad_vreg, Address(rscratch1, 0));\n+  __ lea(rscratch1, ExternalAddress((address)&ZPointerVectorStoreGoodMask));\n+  __ ldrq(z_copy_store_good_vreg, Address(rscratch1, 0));\n+}\n+\n+class ZCopyRuntimeCallSpill {\n+private:\n+  MacroAssembler* _masm;\n+  Register _result;\n+\n+  void save() {\n+    MacroAssembler* masm = _masm;\n+\n+    __ enter(true \/* strip_ret_addr *\/);\n+    if (_result != noreg) {\n+      __ push(__ call_clobbered_gp_registers() - RegSet::of(_result), sp);\n+    } else {\n+      __ push(__ call_clobbered_gp_registers(), sp);\n+    }\n+    int neonSize = wordSize * 2;\n+    __ sub(sp, sp, 4 * neonSize);\n+    __ st1(v0, v1, v2, v3, Assembler::T16B, Address(sp, 0));\n+    __ sub(sp, sp, 4 * neonSize);\n+    __ st1(v4, v5, v6, v7, Assembler::T16B, Address(sp, 0));\n+    __ sub(sp, sp, 4 * neonSize);\n+    __ st1(v16, v17, v18, v19, Assembler::T16B, Address(sp, 0));\n+  }\n+\n+  void restore() {\n+    MacroAssembler* masm = _masm;\n+\n+    int neonSize = wordSize * 2;\n+    __ ld1(v16, v17, v18, v19, Assembler::T16B, Address(sp, 0));\n+    __ add(sp, sp, 4 * neonSize);\n+    __ ld1(v4, v5, v6, v7, Assembler::T16B, Address(sp, 0));\n+    __ add(sp, sp, 4 * neonSize);\n+    __ ld1(v0, v1, v2, v3, Assembler::T16B, Address(sp, 0));\n+    __ add(sp, sp, 4 * neonSize);\n+    if (_result != noreg) {\n+      if (_result != r0) {\n+        __ mov(_result, r0);\n+      }\n+      __ pop(__ call_clobbered_gp_registers() - RegSet::of(_result), sp);\n+    } else {\n+      __ pop(__ call_clobbered_gp_registers(), sp);\n+    }\n+    __ leave();\n+  }\n+\n+public:\n+  ZCopyRuntimeCallSpill(MacroAssembler* masm, Register result)\n+    : _masm(masm),\n+      _result(result) {\n+    save();\n+  }\n+\n+  ~ZCopyRuntimeCallSpill() {\n+    restore();\n+  }\n+};\n@@ -154,1 +456,155 @@\n-  assert_different_registers(src, count, rscratch1);\n+  load_wide_arraycopy_masks(masm);\n+\n+  BLOCK_COMMENT(\"} ZBarrierSetAssembler::arraycopy_prologue\");\n+}\n+\n+static void copy_load_barrier(MacroAssembler* masm,\n+                              Register ref,\n+                              Address src,\n+                              Register tmp) {\n+  Label done;\n+\n+  __ ldr(tmp, Address(rthread, ZThreadLocalData::load_bad_mask_offset()));\n+\n+  \/\/ Test reference against bad mask. If mask bad, then we need to fix it up.\n+  __ tst(ref, tmp);\n+  __ br(Assembler::EQ, done);\n+\n+  {\n+    \/\/ Call VM\n+    ZCopyRuntimeCallSpill rcs(masm, ref);\n+\n+    __ lea(c_rarg1, src);\n+\n+    if (c_rarg0 != ref) {\n+      __ mov(c_rarg0, ref);\n+    }\n+\n+    __ call_VM_leaf(ZBarrierSetRuntime::load_barrier_on_oop_field_preloaded_addr(IN_HEAP | ON_STRONG_OOP_REF), 2);\n+  }\n+\n+  \/\/ Slow-path has uncolored; revert\n+  __ lsl(ref, ref, ZPointerLoadShift);\n+\n+  __ bind(done);\n+}\n+\n+static void copy_load_barrier(MacroAssembler* masm,\n+                              FloatRegister ref,\n+                              Address src,\n+                              Register tmp1,\n+                              Register tmp2,\n+                              FloatRegister vec_tmp) {\n+  Label done;\n+\n+  \/\/ Test reference against bad mask. If mask bad, then we need to fix it up.\n+  __ andr(vec_tmp, Assembler::T16B, ref, z_copy_load_bad_vreg);\n+  __ umaxv(vec_tmp, Assembler::T16B, vec_tmp);\n+  __ fcmpd(vec_tmp, 0.0);\n+  __ br(Assembler::EQ, done);\n+\n+  __ umov(tmp2, ref, Assembler::D, 0);\n+  copy_load_barrier(masm, tmp2, Address(src.base(), src.offset() + 0), tmp1);\n+  __ mov(ref, __ D, 0, tmp2);\n+\n+  __ umov(tmp2, ref, Assembler::D, 1);\n+  copy_load_barrier(masm, tmp2, Address(src.base(), src.offset() + 8), tmp1);\n+  __ mov(ref, __ D, 1, tmp2);\n+\n+  __ bind(done);\n+}\n+\n+static void copy_store_barrier(MacroAssembler* masm,\n+                               Register pre_ref,\n+                               Register new_ref,\n+                               Address src,\n+                               Register tmp1,\n+                               Register tmp2) {\n+  Label done;\n+  Label slow;\n+\n+  \/\/ Test reference against bad mask. If mask bad, then we need to fix it up.\n+  __ ldr(tmp1, Address(rthread, ZThreadLocalData::store_bad_mask_offset()));\n+  __ tst(pre_ref, tmp1);\n+  __ br(Assembler::EQ, done);\n+\n+  store_barrier_buffer_add(masm, src, tmp1, tmp2, slow);\n+  __ b(done);\n+\n+  __ bind(slow);\n+  {\n+    \/\/ Call VM\n+    ZCopyRuntimeCallSpill rcs(masm, noreg);\n+\n+    __ lea(c_rarg0, src);\n+\n+    __ call_VM_leaf(ZBarrierSetRuntime::store_barrier_on_oop_field_without_healing_addr(), 1);\n+  }\n+\n+  __ bind(done);\n+\n+  if (new_ref != noreg) {\n+    \/\/ Set store-good color, replacing whatever color was there before\n+    __ ldr(tmp1, Address(rthread, ZThreadLocalData::store_good_mask_offset()));\n+    __ bfi(new_ref, tmp1, 0, 16);\n+  }\n+}\n+\n+static void copy_store_barrier(MacroAssembler* masm,\n+                               FloatRegister pre_ref,\n+                               FloatRegister new_ref,\n+                               Address src,\n+                               Register tmp1,\n+                               Register tmp2,\n+                               Register tmp3,\n+                               FloatRegister vec_tmp) {\n+  Label done;\n+\n+  \/\/ Test reference against bad mask. If mask bad, then we need to fix it up.\n+  __ andr(vec_tmp, Assembler::T16B, pre_ref, z_copy_store_bad_vreg);\n+  __ umaxv(vec_tmp, Assembler::T16B, vec_tmp);\n+  __ fcmpd(vec_tmp, 0.0);\n+  __ br(Assembler::EQ, done);\n+\n+  \/\/ Extract the 2 oops from the pre_ref vector register\n+  __ umov(tmp2, pre_ref, Assembler::D, 0);\n+  copy_store_barrier(masm, tmp2, noreg, Address(src.base(), src.offset() + 0), tmp1, tmp3);\n+\n+  __ umov(tmp2, pre_ref, Assembler::D, 1);\n+  copy_store_barrier(masm, tmp2, noreg, Address(src.base(), src.offset() + 8), tmp1, tmp3);\n+\n+  __ bind(done);\n+\n+  \/\/ Remove any bad colors\n+  __ bic(new_ref, Assembler::T16B, new_ref, z_copy_store_bad_vreg);\n+  \/\/ Add good colors\n+  __ orr(new_ref, Assembler::T16B, new_ref, z_copy_store_good_vreg);\n+}\n+\n+class ZAdjustAddress {\n+private:\n+  MacroAssembler* _masm;\n+  Address _addr;\n+  int _pre_adjustment;\n+  int _post_adjustment;\n+\n+  void pre() {\n+    if (_pre_adjustment != 0) {\n+      _masm->add(_addr.base(), _addr.base(), _addr.offset());\n+    }\n+  }\n+\n+  void post() {\n+    if (_post_adjustment != 0) {\n+      _masm->add(_addr.base(), _addr.base(), _addr.offset());\n+    }\n+  }\n+\n+public:\n+  ZAdjustAddress(MacroAssembler* masm, Address addr) :\n+      _masm(masm),\n+      _addr(addr),\n+      _pre_adjustment(addr.getMode() == Address::pre ? addr.offset() : 0),\n+      _post_adjustment(addr.getMode() == Address::post ? addr.offset() : 0) {\n+    pre();\n+  }\n@@ -156,1 +612,3 @@\n-  __ push(saved_regs, sp);\n+  ~ZAdjustAddress() {\n+    post();\n+  }\n@@ -158,6 +616,3 @@\n-  if (count == c_rarg0) {\n-    if (src == c_rarg1) {\n-      \/\/ exactly backwards!!\n-      __ mov(rscratch1, c_rarg0);\n-      __ mov(c_rarg0, c_rarg1);\n-      __ mov(c_rarg1, rscratch1);\n+  Address address() {\n+    if (_pre_adjustment != 0 || _post_adjustment != 0) {\n+      return Address(_addr.base(), 0);\n@@ -165,2 +620,1 @@\n-      __ mov(c_rarg1, count);\n-      __ mov(c_rarg0, src);\n+      return Address(_addr.base(), _addr.offset());\n@@ -168,0 +622,26 @@\n+  }\n+};\n+\n+void ZBarrierSetAssembler::copy_load_at(MacroAssembler* masm,\n+                                        DecoratorSet decorators,\n+                                        BasicType type,\n+                                        size_t bytes,\n+                                        Register dst1,\n+                                        Register dst2,\n+                                        Address src,\n+                                        Register tmp) {\n+  if (!is_reference_type(type)) {\n+    BarrierSetAssembler::copy_load_at(masm, decorators, type, bytes, dst1, dst2, src, noreg);\n+    return;\n+  }\n+\n+  ZAdjustAddress adjust(masm, src);\n+  src = adjust.address();\n+\n+  BarrierSetAssembler::copy_load_at(masm, decorators, type, bytes, dst1, dst2, src, noreg);\n+\n+  if (bytes == 8) {\n+    copy_load_barrier(masm, dst1, src, tmp);\n+  } else if (bytes == 16) {\n+    copy_load_barrier(masm, dst1, Address(src.base(), src.offset() + 0), tmp);\n+    copy_load_barrier(masm, dst2, Address(src.base(), src.offset() + 8), tmp);\n@@ -169,2 +649,20 @@\n-    __ mov(c_rarg0, src);\n-    __ mov(c_rarg1, count);\n+    ShouldNotReachHere();\n+  }\n+  if ((decorators & ARRAYCOPY_CHECKCAST) != 0) {\n+    __ lsr(dst1, dst1, ZPointerLoadShift);\n+  }\n+}\n+\n+void ZBarrierSetAssembler::copy_store_at(MacroAssembler* masm,\n+                                         DecoratorSet decorators,\n+                                         BasicType type,\n+                                         size_t bytes,\n+                                         Address dst,\n+                                         Register src1,\n+                                         Register src2,\n+                                         Register tmp1,\n+                                         Register tmp2,\n+                                         Register tmp3) {\n+  if (!is_reference_type(type)) {\n+    BarrierSetAssembler::copy_store_at(masm, decorators, type, bytes, dst, src1, src2, noreg, noreg, noreg);\n+    return;\n@@ -173,1 +671,2 @@\n-  __ call_VM_leaf(ZBarrierSetRuntime::load_barrier_on_oop_array_addr(), 2);\n+  ZAdjustAddress adjust(masm, dst);\n+  dst = adjust.address();\n@@ -175,1 +674,3 @@\n-  __ pop(saved_regs, sp);\n+  if ((decorators & ARRAYCOPY_CHECKCAST) != 0) {\n+    __ lsl(src1, src1, ZPointerLoadShift);\n+  }\n@@ -177,1 +678,110 @@\n-  BLOCK_COMMENT(\"} ZBarrierSetAssembler::arraycopy_prologue\");\n+  bool is_dest_uninitialized = (decorators & IS_DEST_UNINITIALIZED) != 0;\n+\n+  if (is_dest_uninitialized) {\n+    __ ldr(tmp1, Address(rthread, ZThreadLocalData::store_good_mask_offset()));\n+    if (bytes == 8) {\n+      __ bfi(src1, tmp1, 0, 16);\n+    } else if (bytes == 16) {\n+      __ bfi(src1, tmp1, 0, 16);\n+      __ bfi(src2, tmp1, 0, 16);\n+    } else {\n+      ShouldNotReachHere();\n+    }\n+  } else {\n+    \/\/ Store barrier pre values and color new values\n+    if (bytes == 8) {\n+      __ ldr(tmp1, dst);\n+      copy_store_barrier(masm, tmp1, src1, dst, tmp2, tmp3);\n+    } else if (bytes == 16) {\n+      Address dst1(dst.base(), dst.offset() + 0);\n+      Address dst2(dst.base(), dst.offset() + 8);\n+\n+      __ ldr(tmp1, dst1);\n+      copy_store_barrier(masm, tmp1, src1, dst1, tmp2, tmp3);\n+\n+      __ ldr(tmp1, dst2);\n+      copy_store_barrier(masm, tmp1, src2, dst2, tmp2, tmp3);\n+    } else {\n+      ShouldNotReachHere();\n+    }\n+  }\n+\n+  \/\/ Store new values\n+  BarrierSetAssembler::copy_store_at(masm, decorators, type, bytes, dst, src1, src2, noreg, noreg, noreg);\n+}\n+\n+void ZBarrierSetAssembler::copy_load_at(MacroAssembler* masm,\n+                                        DecoratorSet decorators,\n+                                        BasicType type,\n+                                        size_t bytes,\n+                                        FloatRegister dst1,\n+                                        FloatRegister dst2,\n+                                        Address src,\n+                                        Register tmp1,\n+                                        Register tmp2,\n+                                        FloatRegister vec_tmp) {\n+  if (!is_reference_type(type)) {\n+    BarrierSetAssembler::copy_load_at(masm, decorators, type, bytes, dst1, dst2, src, noreg, noreg, fnoreg);\n+    return;\n+  }\n+\n+  ZAdjustAddress adjust(masm, src);\n+  src = adjust.address();\n+\n+  BarrierSetAssembler::copy_load_at(masm, decorators, type, bytes, dst1, dst2, src, noreg, noreg, fnoreg);\n+\n+  if (bytes == 32) {\n+    copy_load_barrier(masm, dst1, Address(src.base(), src.offset() + 0), tmp1, tmp2, vec_tmp);\n+    copy_load_barrier(masm, dst2, Address(src.base(), src.offset() + 16), tmp1, tmp2, vec_tmp);\n+  } else {\n+    ShouldNotReachHere();\n+  }\n+}\n+\n+void ZBarrierSetAssembler::copy_store_at(MacroAssembler* masm,\n+                                         DecoratorSet decorators,\n+                                         BasicType type,\n+                                         size_t bytes,\n+                                         Address dst,\n+                                         FloatRegister src1,\n+                                         FloatRegister src2,\n+                                         Register tmp1,\n+                                         Register tmp2,\n+                                         Register tmp3,\n+                                         FloatRegister vec_tmp1,\n+                                         FloatRegister vec_tmp2,\n+                                         FloatRegister vec_tmp3) {\n+  if (!is_reference_type(type)) {\n+    BarrierSetAssembler::copy_store_at(masm, decorators, type, bytes, dst, src1, src2, noreg, noreg, noreg, fnoreg, fnoreg, fnoreg);\n+    return;\n+  }\n+\n+  bool is_dest_uninitialized = (decorators & IS_DEST_UNINITIALIZED) != 0;\n+\n+  ZAdjustAddress adjust(masm, dst);\n+  dst = adjust.address();\n+\n+  if (is_dest_uninitialized) {\n+    if (bytes == 32) {\n+      __ bic(src1, Assembler::T16B, src1, z_copy_store_bad_vreg);\n+      __ orr(src1, Assembler::T16B, src1, z_copy_store_good_vreg);\n+      __ bic(src2, Assembler::T16B, src2, z_copy_store_bad_vreg);\n+      __ orr(src2, Assembler::T16B, src2, z_copy_store_good_vreg);\n+    } else {\n+      ShouldNotReachHere();\n+    }\n+  } else {\n+    \/\/ Load pre values\n+    BarrierSetAssembler::copy_load_at(masm, decorators, type, bytes, vec_tmp1, vec_tmp2, dst, noreg, noreg, fnoreg);\n+\n+    \/\/ Store barrier pre values and color new values\n+    if (bytes == 32) {\n+      copy_store_barrier(masm, vec_tmp1, src1, Address(dst.base(), dst.offset() + 0), tmp1, tmp2, tmp3, vec_tmp3);\n+      copy_store_barrier(masm, vec_tmp2, src2, Address(dst.base(), dst.offset() + 16), tmp1, tmp2, tmp3, vec_tmp3);\n+    } else {\n+      ShouldNotReachHere();\n+    }\n+  }\n+\n+  \/\/ Store new values\n+  BarrierSetAssembler::copy_store_at(masm, decorators, type, bytes, dst, src1, src2, noreg, noreg, noreg, fnoreg, fnoreg, fnoreg);\n@@ -187,1 +797,5 @@\n-  assert_different_registers(jni_env, robj, tmp);\n+  Label done, tagged, weak_tagged, uncolor;\n+\n+  \/\/ Test for tag\n+  __ tst(robj, JNIHandles::tag_mask);\n+  __ br(Assembler::NE, tagged);\n@@ -189,2 +803,3 @@\n-  \/\/ Resolve jobject\n-  BarrierSetAssembler::try_resolve_jobject_in_native(masm, jni_env, robj, tmp, slowpath);\n+  \/\/ Resolve local handle\n+  __ ldr(robj, robj);\n+  __ b(done);\n@@ -192,3 +807,1 @@\n-  \/\/ The Address offset is too large to direct load - -784. Our range is +127, -128.\n-  __ mov(tmp, (int64_t)(in_bytes(ZThreadLocalData::address_bad_mask_offset()) -\n-              in_bytes(JavaThread::jni_environment_offset())));\n+  __ bind(tagged);\n@@ -196,3 +809,3 @@\n-  \/\/ Load address bad mask\n-  __ add(tmp, jni_env, tmp);\n-  __ ldr(tmp, Address(tmp));\n+  \/\/ Test for weak tag\n+  __ tst(robj, JNIHandles::TypeTag::weak_global);\n+  __ br(Assembler::NE, weak_tagged);\n@@ -200,1 +813,4 @@\n-  \/\/ Check address bad mask\n+  \/\/ Resolve global handle\n+  __ ldr(robj, Address(robj, -JNIHandles::TypeTag::global));\n+  __ lea(tmp, load_bad_mask_from_jni_env(jni_env));\n+  __ ldr(tmp, tmp);\n@@ -203,0 +819,17 @@\n+  __ b(uncolor);\n+\n+  __ bind(weak_tagged);\n+\n+  \/\/ Resolve weak handle\n+  __ ldr(robj, Address(robj, -JNIHandles::TypeTag::weak_global));\n+  __ lea(tmp, mark_bad_mask_from_jni_env(jni_env));\n+  __ ldr(tmp, tmp);\n+  __ tst(robj, tmp);\n+  __ br(Assembler::NE, slowpath);\n+\n+  __ bind(uncolor);\n+\n+  \/\/ Uncolor\n+  __ lsr(robj, robj, ZPointerLoadShift);\n+\n+  __ bind(done);\n@@ -207,0 +840,47 @@\n+static uint16_t patch_barrier_relocation_value(int format) {\n+  switch (format) {\n+  case ZBarrierRelocationFormatLoadGoodBeforeTbX:\n+    return (uint16_t)exact_log2(ZPointerRemapped);\n+\n+  case ZBarrierRelocationFormatMarkBadBeforeMov:\n+    return (uint16_t)ZPointerMarkBadMask;\n+\n+  case ZBarrierRelocationFormatStoreGoodBeforeMov:\n+    return (uint16_t)ZPointerStoreGoodMask;\n+\n+  case ZBarrierRelocationFormatStoreBadBeforeMov:\n+    return (uint16_t)ZPointerStoreBadMask;\n+\n+  default:\n+    ShouldNotReachHere();\n+    return 0;\n+  }\n+}\n+\n+static void change_immediate(uint32_t& instr, uint32_t imm, uint32_t start, uint32_t end) {\n+  uint32_t imm_mask = ((1u << start) - 1u) ^ ((1u << (end + 1)) - 1u);\n+  instr &= ~imm_mask;\n+  instr |= imm << start;\n+}\n+\n+void ZBarrierSetAssembler::patch_barrier_relocation(address addr, int format) {\n+  const uint16_t value = patch_barrier_relocation_value(format);\n+  uint32_t* const patch_addr = (uint32_t*)addr;\n+\n+  switch (format) {\n+  case ZBarrierRelocationFormatLoadGoodBeforeTbX:\n+    change_immediate(*patch_addr, value, 19, 23);\n+    break;\n+  case ZBarrierRelocationFormatStoreGoodBeforeMov:\n+  case ZBarrierRelocationFormatMarkBadBeforeMov:\n+  case ZBarrierRelocationFormatStoreBadBeforeMov:\n+    change_immediate(*patch_addr, value, 5, 20);\n+    break;\n+  default:\n+    ShouldNotReachHere();\n+  }\n+\n+  OrderAccess::fence();\n+  ICache::invalidate_word((address)patch_addr);\n+}\n+\n@@ -212,3 +892,17 @@\n-void ZBarrierSetAssembler::generate_c1_load_barrier_test(LIR_Assembler* ce,\n-                                                         LIR_Opr ref) const {\n-  assert_different_registers(rscratch1, rthread, ref->as_register());\n+static void z_uncolor(LIR_Assembler* ce, LIR_Opr ref) {\n+  __ lsr(ref->as_register(), ref->as_register(), ZPointerLoadShift);\n+}\n+\n+static void z_color(LIR_Assembler* ce, LIR_Opr ref) {\n+  __ relocate(barrier_Relocation::spec(), ZBarrierRelocationFormatStoreGoodBeforeMov);\n+  __ movzw(rscratch2, barrier_Relocation::unpatched);\n+  __ orr(ref->as_register(), rscratch2, ref->as_register(), Assembler::LSL, ZPointerLoadShift);\n+}\n+\n+void ZBarrierSetAssembler::generate_c1_uncolor(LIR_Assembler* ce, LIR_Opr ref) const {\n+  z_uncolor(ce, ref);\n+}\n+\n+void ZBarrierSetAssembler::generate_c1_color(LIR_Assembler* ce, LIR_Opr ref) const {\n+  z_color(ce, ref);\n+}\n@@ -216,2 +910,22 @@\n-  __ ldr(rscratch1, address_bad_mask_from_thread(rthread));\n-  __ tst(ref->as_register(), rscratch1);\n+void ZBarrierSetAssembler::generate_c1_load_barrier(LIR_Assembler* ce,\n+                                                    LIR_Opr ref,\n+                                                    ZLoadBarrierStubC1* stub,\n+                                                    bool on_non_strong) const {\n+\n+  if (on_non_strong) {\n+    \/\/ Test against MarkBad mask\n+    assert_different_registers(rscratch1, rthread, ref->as_register());\n+    __ relocate(barrier_Relocation::spec(), ZBarrierRelocationFormatMarkBadBeforeMov);\n+    __ movzw(rscratch1, barrier_Relocation::unpatched);\n+    __ tst(ref->as_register(), rscratch1);\n+    __ br(Assembler::NE, *stub->entry());\n+    z_uncolor(ce, ref);\n+  } else {\n+    Label good;\n+    __ relocate(barrier_Relocation::spec(), ZBarrierRelocationFormatLoadGoodBeforeTbX);\n+    __ tbz(ref->as_register(), barrier_Relocation::unpatched, good);\n+    __ b(*stub->entry());\n+    __ bind(good);\n+    z_uncolor(ce, ref);\n+  }\n+  __ bind(*stub->continuation());\n@@ -275,0 +989,51 @@\n+void ZBarrierSetAssembler::generate_c1_store_barrier(LIR_Assembler* ce,\n+                                                     LIR_Address* addr,\n+                                                     LIR_Opr new_zaddress,\n+                                                     LIR_Opr new_zpointer,\n+                                                     ZStoreBarrierStubC1* stub) const {\n+  Register rnew_zaddress = new_zaddress->as_register();\n+  Register rnew_zpointer = new_zpointer->as_register();\n+\n+  store_barrier_fast(ce->masm(),\n+                     ce->as_Address(addr),\n+                     rnew_zaddress,\n+                     rnew_zpointer,\n+                     rscratch2,\n+                     true,\n+                     stub->is_atomic(),\n+                     *stub->entry(),\n+                     *stub->continuation());\n+}\n+\n+void ZBarrierSetAssembler::generate_c1_store_barrier_stub(LIR_Assembler* ce,\n+                                                          ZStoreBarrierStubC1* stub) const {\n+  \/\/ Stub entry\n+  __ bind(*stub->entry());\n+  Label slow;\n+  Label slow_continuation;\n+  store_barrier_medium(ce->masm(),\n+                       ce->as_Address(stub->ref_addr()->as_address_ptr()),\n+                       rscratch2,\n+                       stub->new_zpointer()->as_register(),\n+                       rscratch1,\n+                       false \/* is_native *\/,\n+                       stub->is_atomic(),\n+                       *stub->continuation(),\n+                       slow,\n+                       slow_continuation);\n+\n+  __ bind(slow);\n+\n+  __ lea(stub->new_zpointer()->as_register(), ce->as_Address(stub->ref_addr()->as_address_ptr()));\n+\n+  __ sub(sp, sp, 16);\n+  \/\/ Setup arguments and call runtime stub\n+  assert(stub->new_zpointer()->is_valid(), \"invariant\");\n+  ce->store_parameter(stub->new_zpointer()->as_register(), 0);\n+  __ far_call(stub->runtime_stub());\n+  __ add(sp, sp, 16);\n+\n+  \/\/ Stub exit\n+  __ b(slow_continuation);\n+}\n+\n@@ -294,0 +1059,21 @@\n+\n+void ZBarrierSetAssembler::generate_c1_store_barrier_runtime_stub(StubAssembler* sasm,\n+                                                                  bool self_healing) const {\n+  __ prologue(\"zgc_store_barrier stub\", false);\n+\n+  __ push_call_clobbered_registers();\n+\n+  \/\/ Setup arguments\n+  __ load_parameter(0, c_rarg0);\n+\n+  if (self_healing) {\n+    __ call_VM_leaf(ZBarrierSetRuntime::store_barrier_on_oop_field_with_healing_addr(), 1);\n+  } else {\n+    __ call_VM_leaf(ZBarrierSetRuntime::store_barrier_on_oop_field_without_healing_addr(), 1);\n+  }\n+\n+  __ pop_call_clobbered_registers();\n+\n+  __ epilogue();\n+}\n+\n@@ -322,1 +1108,1 @@\n-  void initialize(ZLoadBarrierStubC2* stub) {\n+  void initialize(ZBarrierStubC2* stub) {\n@@ -342,1 +1128,5 @@\n-    _gp_regs -= RegSet::range(r19, r30) + RegSet::of(r8, r9, stub->ref());\n+    if (stub->result() != noreg) {\n+      _gp_regs -= RegSet::range(r19, r30) + RegSet::of(r8, r9, stub->result());\n+    } else {\n+      _gp_regs -= RegSet::range(r19, r30) + RegSet::of(r8, r9);\n+    }\n@@ -345,1 +1135,1 @@\n-  ZSaveLiveRegisters(MacroAssembler* masm, ZLoadBarrierStubC2* stub) :\n+  ZSaveLiveRegisters(MacroAssembler* masm, ZBarrierStubC2* stub) :\n@@ -432,0 +1222,1 @@\n+  Assembler::InlineSkippedInstructionsCounter skipped_counter(masm);\n@@ -435,1 +1226,3 @@\n-  __ bind(*stub->entry());\n+  if (!Compile::current()->output()->in_scratch_emit_size()) {\n+    __ bind(*stub->entry());\n+  }\n@@ -447,0 +1240,160 @@\n+void ZBarrierSetAssembler::generate_c2_store_barrier_stub(MacroAssembler* masm, ZStoreBarrierStubC2* stub) const {\n+  Assembler::InlineSkippedInstructionsCounter skipped_counter(masm);\n+  BLOCK_COMMENT(\"ZStoreBarrierStubC2\");\n+\n+  \/\/ Stub entry\n+  __ bind(*stub->entry());\n+\n+  Label slow;\n+  Label slow_continuation;\n+  store_barrier_medium(masm,\n+                       stub->ref_addr(),\n+                       stub->new_zpointer(),\n+                       rscratch1,\n+                       rscratch2,\n+                       stub->is_native(),\n+                       stub->is_atomic(),\n+                       *stub->continuation(),\n+                       slow,\n+                       slow_continuation);\n+\n+  __ bind(slow);\n+\n+  {\n+    ZSaveLiveRegisters save_live_registers(masm, stub);\n+    __ lea(c_rarg0, stub->ref_addr());\n+\n+    if (stub->is_native()) {\n+      __ lea(rscratch1, RuntimeAddress(ZBarrierSetRuntime::store_barrier_on_native_oop_field_without_healing_addr()));\n+    } else if (stub->is_atomic()) {\n+      __ lea(rscratch1, RuntimeAddress(ZBarrierSetRuntime::store_barrier_on_oop_field_with_healing_addr()));\n+    } else {\n+      __ lea(rscratch1, RuntimeAddress(ZBarrierSetRuntime::store_barrier_on_oop_field_without_healing_addr()));\n+    }\n+    __ blr(rscratch1);\n+  }\n+\n+  \/\/ Stub exit\n+  __ b(slow_continuation);\n+}\n+\n+\/\/ Only handles forward branch jumps, target_offset >= branch_offset\n+static bool aarch64_test_and_branch_reachable(int branch_offset, int target_offset) {\n+  assert(branch_offset >= 0, \"branch to stub offsets must be positive\");\n+  assert(target_offset >= 0, \"offset in stubs section must be positive\");\n+  assert(target_offset >= branch_offset, \"forward branches only, branch_offset -> target_offset\");\n+\n+  const int test_and_branch_delta_limit = 32 * K;\n+\n+  const int test_and_branch_to_trampoline_delta = target_offset - branch_offset;\n+\n+  return test_and_branch_to_trampoline_delta < test_and_branch_delta_limit;\n+}\n+\n+ZLoadBarrierStubC2Aarch64::ZLoadBarrierStubC2Aarch64(const MachNode* node, Address ref_addr, Register ref, int offset)\n+  : ZLoadBarrierStubC2(node, ref_addr, ref), _test_and_branch_reachable_entry(), _offset(offset), _deferred_emit(false), _test_and_branch_reachable(false) {\n+  PhaseOutput* const output = Compile::current()->output();\n+  if (output->in_scratch_emit_size()) {\n+    return;\n+  }\n+  const int code_size = output->buffer_sizing_data()->_code;\n+  const int offset_code = _offset;\n+  \/\/ Assumption that the stub can always be reached from a branch immediate. (128 M Product, 2 M Debug)\n+  \/\/ Same assumption is made in z_aarch64.ad\n+  const int trampoline_offset = trampoline_stubs_count() * NativeInstruction::instruction_size;\n+  _test_and_branch_reachable = aarch64_test_and_branch_reachable(offset_code, code_size + trampoline_offset);\n+  if (_test_and_branch_reachable) {\n+    inc_trampoline_stubs_count();\n+  }\n+}\n+\n+int ZLoadBarrierStubC2Aarch64::get_stub_size() {\n+  PhaseOutput* const output = Compile::current()->output();\n+  assert(!output->in_scratch_emit_size(), \"only used when emitting stubs\");\n+  BufferBlob* const blob = output->scratch_buffer_blob();\n+  CodeBuffer cb(blob->content_begin(), (address)output->scratch_locs_memory() - blob->content_begin());\n+  MacroAssembler masm(&cb);\n+  output->set_in_scratch_emit_size(true);\n+  ZLoadBarrierStubC2::emit_code(masm);\n+  output->set_in_scratch_emit_size(false);\n+  return cb.insts_size();\n+}\n+\n+ZLoadBarrierStubC2Aarch64* ZLoadBarrierStubC2Aarch64::create(const MachNode* node, Address ref_addr, Register ref, int offset) {\n+  ZLoadBarrierStubC2Aarch64* const stub = new (Compile::current()->comp_arena()) ZLoadBarrierStubC2Aarch64(node, ref_addr, ref, offset);\n+  register_stub(stub);\n+  return stub;\n+}\n+\n+#undef __\n+#define __ masm.\n+\n+void ZLoadBarrierStubC2Aarch64::emit_code(MacroAssembler& masm) {\n+  PhaseOutput* const output = Compile::current()->output();\n+  const int branch_offset = _offset;\n+  const int target_offset = __ offset();\n+\n+  \/\/ Deferred emission, emit actual stub\n+  if (_deferred_emit) {\n+    ZLoadBarrierStubC2::emit_code(masm);\n+    return;\n+  }\n+  _deferred_emit = true;\n+\n+  \/\/ No trampoline used, defer emission to after trampolines\n+  if (!_test_and_branch_reachable) {\n+    register_stub(this);\n+    return;\n+  }\n+\n+  \/\/ Current assumption is that the barrier stubs are the first stubs emitted after the actual code\n+  assert(stubs_start_offset() <= output->buffer_sizing_data()->_code, \"stubs are assumed to be emitted directly after code and code_size is a hard limit on where it can start\");\n+\n+  __ bind(_test_and_branch_reachable_entry);\n+\n+  \/\/ Next branch's offset is unknown, but is > branch_offset\n+  const int next_branch_offset = branch_offset + NativeInstruction::instruction_size;\n+  \/\/ If emitting the stub directly does not interfere with emission of the next trampoline then do it to avoid a double jump.\n+  if (aarch64_test_and_branch_reachable(next_branch_offset, target_offset + get_stub_size())) {\n+    \/\/ The next potential trampoline will still be reachable even if we emit the whole stub\n+    ZLoadBarrierStubC2::emit_code(masm);\n+  } else {\n+    \/\/ Emit trampoline and defer actual stub to the end\n+    assert(aarch64_test_and_branch_reachable(branch_offset, target_offset), \"trampoline should be reachable\");\n+    __ b(*ZLoadBarrierStubC2::entry());\n+    register_stub(this);\n+  }\n+}\n+\n+bool ZLoadBarrierStubC2Aarch64::is_test_and_branch_reachable() {\n+  return _test_and_branch_reachable;\n+}\n+\n+Label* ZLoadBarrierStubC2Aarch64::entry() {\n+  if (_test_and_branch_reachable) {\n+    return &_test_and_branch_reachable_entry;\n+  }\n+  return ZBarrierStubC2::entry();\n+}\n+\n+ZStoreBarrierStubC2Aarch64::ZStoreBarrierStubC2Aarch64(const MachNode* node, Address ref_addr, Register new_zaddress, Register new_zpointer, bool is_native, bool is_atomic)\n+  : ZStoreBarrierStubC2(node, ref_addr, new_zaddress, new_zpointer, is_native, is_atomic), _deferred_emit(false) {}\n+\n+ZStoreBarrierStubC2Aarch64* ZStoreBarrierStubC2Aarch64::create(const MachNode* node, Address ref_addr, Register new_zaddress, Register new_zpointer, bool is_native, bool is_atomic) {\n+  ZStoreBarrierStubC2Aarch64* const stub = new (Compile::current()->comp_arena()) ZStoreBarrierStubC2Aarch64(node, ref_addr, new_zaddress, new_zpointer, is_native, is_atomic);\n+  register_stub(stub);\n+  return stub;\n+}\n+\n+void ZStoreBarrierStubC2Aarch64::emit_code(MacroAssembler& masm) {\n+  if (_deferred_emit) {\n+    ZStoreBarrierStubC2::emit_code(masm);\n+    return;\n+  }\n+  \/\/ Defer emission of store barriers so that trampolines are emitted first\n+  _deferred_emit = true;\n+  register_stub(this);\n+}\n+\n+#undef __\n+\n@@ -453,5 +1406,47 @@\n-  \/\/ Check if mask is good.\n-  \/\/ verifies that ZAddressBadMask & r0 == 0\n-  __ ldr(tmp2, Address(rthread, ZThreadLocalData::address_bad_mask_offset()));\n-  __ andr(tmp1, obj, tmp2);\n-  __ cbnz(tmp1, error);\n+  \/\/ C1 calls verfy_oop in the middle of barriers, before they have been uncolored\n+  \/\/ and after being colored. Therefore, we must deal with colored oops as well.\n+  Label done;\n+  Label check_oop;\n+  Label check_zaddress;\n+  int color_bits = ZPointerRemappedShift + ZPointerRemappedBits;\n+\n+  uintptr_t shifted_base_start_mask = (UCONST64(1) << (ZAddressHeapBaseShift + color_bits + 1)) - 1;\n+  uintptr_t shifted_base_end_mask = (UCONST64(1) << (ZAddressHeapBaseShift + 1)) - 1;\n+  uintptr_t shifted_base_mask = shifted_base_start_mask ^ shifted_base_end_mask;\n+\n+  uintptr_t shifted_address_end_mask = (UCONST64(1) << (color_bits + 1)) - 1;\n+  uintptr_t shifted_address_mask = shifted_address_end_mask ^ (uintptr_t)CONST64(-1);\n+\n+  __ get_nzcv(tmp2);\n+\n+  \/\/ Check colored null\n+  __ mov(tmp1, shifted_address_mask);\n+  __ tst(tmp1, obj);\n+  __ br(Assembler::EQ, done);\n+\n+  \/\/ Check for zpointer\n+  __ mov(tmp1, shifted_base_mask);\n+  __ tst(tmp1, obj);\n+  __ br(Assembler::EQ, check_oop);\n+\n+  \/\/ Uncolor presumed zpointer\n+  __ lsr(obj, obj, ZPointerLoadShift);\n+\n+  __ b(check_zaddress);\n+\n+  __ bind(check_oop);\n+\n+  \/\/ make sure klass is 'reasonable', which is not zero.\n+  __ load_klass(tmp1, obj);  \/\/ get klass\n+  __ tst(tmp1, tmp1);\n+  __ br(Assembler::EQ, error); \/\/ if klass is null it is broken\n+\n+  __ bind(check_zaddress);\n+  \/\/ Check if the oop is in the right area of memory\n+  __ mov(tmp1, (intptr_t) Universe::verify_oop_mask());\n+  __ andr(tmp1, tmp1, obj);\n+  __ mov(obj, (intptr_t) Universe::verify_oop_bits());\n+  __ cmp(tmp1, obj);\n+  __ br(Assembler::NE, error);\n+\n+  __ bind(done);\n@@ -459,1 +1454,1 @@\n-  BarrierSetAssembler::check_oop(masm, obj, tmp1, tmp2, error);\n+  __ set_nzcv(tmp2);\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/z\/zBarrierSetAssembler_aarch64.cpp","additions":1082,"deletions":87,"binary":false,"changes":1169,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2019, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2019, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -30,0 +30,1 @@\n+#include \"gc\/z\/c2\/zBarrierSetC2.hpp\"\n@@ -34,0 +35,1 @@\n+class LIR_Address;\n@@ -38,0 +40,1 @@\n+class ZStoreBarrierStubC1;\n@@ -41,0 +44,1 @@\n+class MachNode;\n@@ -42,1 +46,0 @@\n-class ZLoadBarrierStubC2;\n@@ -45,0 +48,8 @@\n+\/\/ ZBarrierRelocationFormatLoadGoodBeforeTbX is used for both tbnz and tbz\n+\/\/ They are patched in the same way, their immediate value has the same\n+\/\/ structure\n+const int ZBarrierRelocationFormatLoadGoodBeforeTbX  = 0;\n+const int ZBarrierRelocationFormatMarkBadBeforeMov   = 1;\n+const int ZBarrierRelocationFormatStoreGoodBeforeMov = 2;\n+const int ZBarrierRelocationFormatStoreBadBeforeMov  = 3;\n+\n@@ -55,1 +66,21 @@\n-#ifdef ASSERT\n+  void store_barrier_fast(MacroAssembler* masm,\n+                          Address ref_addr,\n+                          Register rnew_zaddress,\n+                          Register rnew_zpointer,\n+                          Register rtmp,\n+                          bool in_nmethod,\n+                          bool is_atomic,\n+                          Label& medium_path,\n+                          Label& medium_path_continuation) const;\n+\n+  void store_barrier_medium(MacroAssembler* masm,\n+                            Address ref_addr,\n+                            Register rtmp1,\n+                            Register rtmp2,\n+                            Register rtmp3,\n+                            bool is_native,\n+                            bool is_atomic,\n+                            Label& medium_path_continuation,\n+                            Label& slow_path,\n+                            Label& slow_path_continuation) const;\n+\n@@ -64,1 +95,0 @@\n-#endif \/\/ ASSERT\n@@ -74,0 +104,45 @@\n+  virtual void copy_load_at(MacroAssembler* masm,\n+                            DecoratorSet decorators,\n+                            BasicType type,\n+                            size_t bytes,\n+                            Register dst1,\n+                            Register dst2,\n+                            Address src,\n+                            Register tmp);\n+\n+  virtual void copy_store_at(MacroAssembler* masm,\n+                             DecoratorSet decorators,\n+                             BasicType type,\n+                             size_t bytes,\n+                             Address dst,\n+                             Register src1,\n+                             Register src2,\n+                             Register tmp1,\n+                             Register tmp2,\n+                             Register tmp3);\n+\n+  virtual void copy_load_at(MacroAssembler* masm,\n+                            DecoratorSet decorators,\n+                            BasicType type,\n+                            size_t bytes,\n+                            FloatRegister dst1,\n+                            FloatRegister dst2,\n+                            Address src,\n+                            Register tmp1,\n+                            Register tmp2,\n+                            FloatRegister vec_tmp);\n+\n+  virtual void copy_store_at(MacroAssembler* masm,\n+                             DecoratorSet decorators,\n+                             BasicType type,\n+                             size_t bytes,\n+                             Address dst,\n+                             FloatRegister src1,\n+                             FloatRegister src2,\n+                             Register tmp1,\n+                             Register tmp2,\n+                             Register tmp3,\n+                             FloatRegister vec_tmp1,\n+                             FloatRegister vec_tmp2,\n+                             FloatRegister vec_tmp3);\n+\n@@ -80,1 +155,5 @@\n-  virtual NMethodPatchingType nmethod_patching_type() { return NMethodPatchingType::conc_data_patch; }\n+  virtual NMethodPatchingType nmethod_patching_type() { return NMethodPatchingType::conc_instruction_and_data_patch; }\n+\n+  void patch_barrier_relocation(address addr, int format);\n+\n+  void patch_barriers() {}\n@@ -83,2 +162,7 @@\n-  void generate_c1_load_barrier_test(LIR_Assembler* ce,\n-                                     LIR_Opr ref) const;\n+  void generate_c1_color(LIR_Assembler* ce, LIR_Opr ref) const;\n+  void generate_c1_uncolor(LIR_Assembler* ce, LIR_Opr ref) const;\n+\n+  void generate_c1_load_barrier(LIR_Assembler* ce,\n+                                LIR_Opr ref,\n+                                ZLoadBarrierStubC1* stub,\n+                                bool on_non_strong) const;\n@@ -91,0 +175,12 @@\n+\n+  void generate_c1_store_barrier(LIR_Assembler* ce,\n+                                 LIR_Address* addr,\n+                                 LIR_Opr new_zaddress,\n+                                 LIR_Opr new_zpointer,\n+                                 ZStoreBarrierStubC1* stub) const;\n+\n+  void generate_c1_store_barrier_stub(LIR_Assembler* ce,\n+                                      ZStoreBarrierStubC1* stub) const;\n+\n+  void generate_c1_store_barrier_runtime_stub(StubAssembler* sasm,\n+                                              bool self_healing) const;\n@@ -99,0 +195,2 @@\n+  void generate_c2_store_barrier_stub(MacroAssembler* masm,\n+                                      ZStoreBarrierStubC2* stub) const;\n@@ -104,0 +202,92 @@\n+#ifdef COMPILER2\n+\n+\/\/ Load barriers on aarch64 are implemented with a test-and-branch immediate instruction.\n+\/\/ This immediate has a max delta of 32K. Because of this the branch is implemented with\n+\/\/ a small jump, as follows:\n+\/\/      __ tbz(ref, barrier_Relocation::unpatched, good);\n+\/\/      __ b(*stub->entry());\n+\/\/      __ bind(good);\n+\/\/\n+\/\/ If we can guarantee that the *stub->entry() label is within 32K we can replace the above\n+\/\/ code with:\n+\/\/      __ tbnz(ref, barrier_Relocation::unpatched, *stub->entry());\n+\/\/\n+\/\/ From the branch shortening part of PhaseOutput we get a pessimistic code size that the code\n+\/\/ will not grow beyond.\n+\/\/\n+\/\/ The stubs objects are created and registered when the load barriers are emitted. The decision\n+\/\/ between emitting the long branch or the test and branch is done at this point and uses the\n+\/\/ pessimistic code size from branch shortening.\n+\/\/\n+\/\/ After the code has been emitted the barrier set will emit all the stubs. When the stubs are\n+\/\/ emitted we know the real code size. Because of this the trampoline jump can be skipped in\n+\/\/ favour of emitting the stub directly if it does not interfere with the next trampoline stub.\n+\/\/ (With respect to test and branch distance)\n+\/\/\n+\/\/ The algorithm for emitting the load barrier branches and stubs now have three versions\n+\/\/ depending on the distance between the barrier and the stub.\n+\/\/ Version 1: Not Reachable with a test-and-branch immediate\n+\/\/ Version 2: Reachable with a test-and-branch immediate via trampoline\n+\/\/ Version 3: Reachable with a test-and-branch immediate without trampoline\n+\/\/\n+\/\/     +--------------------- Code ----------------------+\n+\/\/     |                      ***                        |\n+\/\/     | b(stub1)                                        | (Version 1)\n+\/\/     |                      ***                        |\n+\/\/     | tbnz(ref, barrier_Relocation::unpatched, tramp) | (Version 2)\n+\/\/     |                      ***                        |\n+\/\/     | tbnz(ref, barrier_Relocation::unpatched, stub3) | (Version 3)\n+\/\/     |                      ***                        |\n+\/\/     +--------------------- Stub ----------------------+\n+\/\/     | tramp: b(stub2)                                 | (Trampoline slot)\n+\/\/     | stub3:                                          |\n+\/\/     |                  * Stub Code*                   |\n+\/\/     | stub1:                                          |\n+\/\/     |                  * Stub Code*                   |\n+\/\/     | stub2:                                          |\n+\/\/     |                  * Stub Code*                   |\n+\/\/     +-------------------------------------------------+\n+\/\/\n+\/\/  Version 1: Is emitted if the pessimistic distance between the branch instruction and the current\n+\/\/             trampoline slot cannot fit in a test and branch immediate.\n+\/\/\n+\/\/  Version 2: Is emitted if the distance between the branch instruction and the current trampoline\n+\/\/             slot can fit in a test and branch immediate. But emitting the stub directly would\n+\/\/             interfere with the next trampoline.\n+\/\/\n+\/\/  Version 3: Same as version two but emitting the stub directly (skipping the trampoline) does not\n+\/\/             interfere with the next trampoline.\n+\/\/\n+class ZLoadBarrierStubC2Aarch64 : public ZLoadBarrierStubC2 {\n+private:\n+  Label _test_and_branch_reachable_entry;\n+  const int _offset;\n+  bool _deferred_emit;\n+  bool _test_and_branch_reachable;\n+\n+  ZLoadBarrierStubC2Aarch64(const MachNode* node, Address ref_addr, Register ref, int offset);\n+\n+  int get_stub_size();\n+public:\n+  static ZLoadBarrierStubC2Aarch64* create(const MachNode* node, Address ref_addr, Register ref, int offset);\n+\n+  virtual void emit_code(MacroAssembler& masm);\n+  bool is_test_and_branch_reachable();\n+  Label* entry();\n+};\n+\n+\n+class ZStoreBarrierStubC2Aarch64 : public ZStoreBarrierStubC2 {\n+private:\n+  bool _deferred_emit;\n+\n+  ZStoreBarrierStubC2Aarch64(const MachNode* node, Address ref_addr, Register new_zaddress, Register new_zpointer, bool is_native, bool is_atomic);\n+\n+public:\n+  static ZStoreBarrierStubC2Aarch64* create(const MachNode* node, Address ref_addr, Register new_zaddress, Register new_zpointer, bool is_native, bool is_atomic);\n+\n+  virtual void emit_code(MacroAssembler& masm);\n+};\n+\n+#endif \/\/ COMPILER2\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/z\/zBarrierSetAssembler_aarch64.hpp","additions":197,"deletions":7,"binary":false,"changes":204,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,2 +27,1 @@\n-const size_t ZPlatformHeapViews        = 3;\n-const size_t ZPlatformCacheLineSize    = 64;\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -30,2 +29,1 @@\n-size_t ZPlatformAddressOffsetBits();\n-size_t ZPlatformAddressMetadataShift();\n+const size_t ZPlatformCacheLineSize    = 64;\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/z\/zGlobals_aarch64.hpp","additions":3,"deletions":5,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n-\/\/ Copyright (c) 2019, 2021, Oracle and\/or its affiliates. All rights reserved.\n+\/\/ Copyright (c) 2019, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -34,2 +34,31 @@\n-static void z_load_barrier(MacroAssembler& _masm, const MachNode* node, Address ref_addr, Register ref, Register tmp, uint8_t barrier_data) {\n-  if (barrier_data == ZLoadBarrierElided) {\n+#include \"gc\/z\/zBarrierSetAssembler.hpp\"\n+\n+static void z_color(MacroAssembler& _masm, const MachNode* node, Register dst, Register src) {\n+  assert_different_registers(src, dst);\n+  __ relocate(barrier_Relocation::spec(), ZBarrierRelocationFormatStoreGoodBeforeMov);\n+  __ movzw(dst, barrier_Relocation::unpatched);\n+  __ orr(dst, dst, src, Assembler::LSL, ZPointerLoadShift);\n+}\n+\n+static void z_uncolor(MacroAssembler& _masm, const MachNode* node, Register ref) {\n+  __ lsr(ref, ref, ZPointerLoadShift);\n+}\n+\n+static void z_keep_alive_load_barrier(MacroAssembler& _masm, const MachNode* node, Address ref_addr, Register ref, Register tmp) {\n+  __ relocate(barrier_Relocation::spec(), ZBarrierRelocationFormatMarkBadBeforeMov);\n+  __ movzw(tmp, barrier_Relocation::unpatched);\n+  __ tst(ref, tmp);\n+  ZLoadBarrierStubC2* const stub = ZLoadBarrierStubC2::create(node, ref_addr, ref);\n+  __ br(Assembler::NE, *stub->entry());\n+  z_uncolor(_masm, node, ref);\n+  __ bind(*stub->continuation());\n+}\n+\n+static void z_load_barrier(MacroAssembler& _masm, const MachNode* node, Address ref_addr, Register ref, Register tmp) {\n+  Assembler::InlineSkippedInstructionsCounter skipped_counter(&_masm);\n+  const bool on_non_strong =\n+      ((node->barrier_data() & ZBarrierWeak) != 0) ||\n+      ((node->barrier_data() & ZBarrierPhantom) != 0);\n+\n+  if (on_non_strong) {\n+    z_keep_alive_load_barrier(_masm, node, ref_addr, ref, tmp);\n@@ -38,4 +67,18 @@\n-  ZLoadBarrierStubC2* const stub = ZLoadBarrierStubC2::create(node, ref_addr, ref, tmp, barrier_data);\n-  __ ldr(tmp, Address(rthread, ZThreadLocalData::address_bad_mask_offset()));\n-  __ andr(tmp, tmp, ref);\n-  __ cbnz(tmp, *stub->entry());\n+\n+  if (node->barrier_data() == ZBarrierElided) {\n+    z_uncolor(_masm, node, ref);\n+    return;\n+  }\n+\n+  ZLoadBarrierStubC2Aarch64* const stub = ZLoadBarrierStubC2Aarch64::create(node, ref_addr, ref, __ offset());\n+  if (stub->is_test_and_branch_reachable()) {\n+    __ relocate(barrier_Relocation::spec(), ZBarrierRelocationFormatLoadGoodBeforeTbX);\n+    __ tbnz(ref, barrier_Relocation::unpatched, *stub->entry());\n+  } else {\n+    Label good;\n+    __ relocate(barrier_Relocation::spec(), ZBarrierRelocationFormatLoadGoodBeforeTbX);\n+    __ tbz(ref, barrier_Relocation::unpatched, good);\n+    __ b(*stub->entry());\n+    __ bind(good);\n+  }\n+  z_uncolor(_masm, node, ref);\n@@ -45,4 +88,10 @@\n-static void z_load_barrier_slow_path(MacroAssembler& _masm, const MachNode* node, Address ref_addr, Register ref, Register tmp) {\n-  ZLoadBarrierStubC2* const stub = ZLoadBarrierStubC2::create(node, ref_addr, ref, tmp, ZLoadBarrierStrong);\n-  __ b(*stub->entry());\n-  __ bind(*stub->continuation());\n+static void z_store_barrier(MacroAssembler& _masm, const MachNode* node, Address ref_addr, Register rnew_zaddress, Register rnew_zpointer, Register tmp, bool is_atomic) {\n+  Assembler::InlineSkippedInstructionsCounter skipped_counter(&_masm);\n+  if (node->barrier_data() == ZBarrierElided) {\n+    z_color(_masm, node, rnew_zpointer, rnew_zaddress);\n+  } else {\n+    bool is_native = (node->barrier_data() & ZBarrierNative) != 0;\n+    ZStoreBarrierStubC2Aarch64* const stub = ZStoreBarrierStubC2Aarch64::create(node, ref_addr, rnew_zaddress, rnew_zpointer, is_native, is_atomic);\n+    ZBarrierSetAssembler* bs_asm = ZBarrierSet::assembler();\n+    bs_asm->store_barrier_fast(&_masm, ref_addr, rnew_zaddress, rnew_zpointer, tmp, true \/* in_nmethod *\/, is_atomic, *stub->entry(), *stub->continuation());\n+  }\n@@ -57,1 +106,1 @@\n-  predicate(UseZGC && !needs_acquiring_load(n) && (n->as_Load()->barrier_data() != 0));\n+  predicate(UseZGC && ZGenerational && !needs_acquiring_load(n) && n->as_Load()->barrier_data() != 0);\n@@ -67,1 +116,1 @@\n-    z_load_barrier(_masm, this, ref_addr, $dst$$Register, rscratch2 \/* tmp *\/, barrier_data());\n+    z_load_barrier(_masm, this, ref_addr, $dst$$Register, rscratch1);\n@@ -77,1 +126,1 @@\n-  predicate(UseZGC && needs_acquiring_load(n) && n->as_Load()->barrier_data() != 0);\n+  predicate(UseZGC && ZGenerational && needs_acquiring_load(n) && n->as_Load()->barrier_data() != 0);\n@@ -85,0 +134,1 @@\n+    const Address ref_addr = Address($mem$$Register);\n@@ -86,1 +136,19 @@\n-    z_load_barrier(_masm, this, Address($mem$$Register), $dst$$Register, rscratch2 \/* tmp *\/, barrier_data());\n+    z_load_barrier(_masm, this, ref_addr, $dst$$Register, rscratch1);\n+  %}\n+\n+  ins_pipe(pipe_serial);\n+%}\n+\n+\/\/ Store Pointer\n+instruct zStoreP(memory mem, iRegP src, iRegPNoSp tmp, rFlagsReg cr)\n+%{\n+  predicate(UseZGC && ZGenerational && !needs_releasing_store(n) && n->as_Store()->barrier_data() != 0);\n+  match(Set mem (StoreP mem src));\n+  effect(TEMP tmp, KILL cr);\n+\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"movq    $mem, $src\\t# ptr\" %}\n+  ins_encode %{\n+    const Address ref_addr = mem2address($mem->opcode(), as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+    z_store_barrier(_masm, this, ref_addr, $src$$Register, $tmp$$Register, rscratch2, false \/* is_atomic *\/);\n+    __ str($tmp$$Register, ref_addr);\n@@ -88,0 +156,2 @@\n+  ins_pipe(pipe_serial);\n+%}\n@@ -89,0 +159,14 @@\n+\/\/ Store Pointer Volatile\n+instruct zStorePVolatile(indirect mem, iRegP src, iRegPNoSp tmp, rFlagsReg cr)\n+%{\n+  predicate(UseZGC && ZGenerational && needs_releasing_store(n) && n->as_Store()->barrier_data() != 0);\n+  match(Set mem (StoreP mem src));\n+  effect(TEMP tmp, KILL cr);\n+\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"movq    $mem, $src\\t# ptr\" %}\n+  ins_encode %{\n+    const Address ref_addr = Address($mem$$Register);\n+    z_store_barrier(_masm, this, ref_addr, $src$$Register, $tmp$$Register, rscratch2, false \/* is_atomic *\/);\n+    __ stlr($tmp$$Register, $mem$$Register);\n+  %}\n@@ -92,1 +176,1 @@\n-instruct zCompareAndSwapP(iRegINoSp res, indirect mem, iRegP oldval, iRegP newval, rFlagsReg cr) %{\n+instruct zCompareAndSwapP(iRegINoSp res, indirect mem, iRegP oldval, iRegP newval, iRegPNoSp oldval_tmp, iRegPNoSp newval_tmp, rFlagsReg cr) %{\n@@ -95,2 +179,2 @@\n-  predicate(UseZGC && !needs_acquiring_load_exclusive(n) && n->as_LoadStore()->barrier_data() == ZLoadBarrierStrong);\n-  effect(KILL cr, TEMP_DEF res);\n+  predicate(UseZGC && ZGenerational && !needs_acquiring_load_exclusive(n) && n->as_LoadStore()->barrier_data() != 0);\n+  effect(TEMP oldval_tmp, TEMP newval_tmp, TEMP res, KILL cr);\n@@ -105,2 +189,5 @@\n-    __ cmpxchg($mem$$Register, $oldval$$Register, $newval$$Register, Assembler::xword,\n-               false \/* acquire *\/, true \/* release *\/, false \/* weak *\/, rscratch2);\n+    Address ref_addr($mem$$Register);\n+    z_store_barrier(_masm, this, ref_addr, $newval$$Register, $newval_tmp$$Register, rscratch2, true \/* is_atomic *\/);\n+    z_color(_masm, this, $oldval_tmp$$Register, $oldval$$Register);\n+    __ cmpxchg($mem$$Register, $oldval_tmp$$Register, $newval_tmp$$Register, Assembler::xword,\n+               false \/* acquire *\/, true \/* release *\/, false \/* weak *\/, noreg);\n@@ -108,11 +195,0 @@\n-    if (barrier_data() != ZLoadBarrierElided) {\n-      Label good;\n-      __ ldr(rscratch1, Address(rthread, ZThreadLocalData::address_bad_mask_offset()));\n-      __ andr(rscratch1, rscratch1, rscratch2);\n-      __ cbz(rscratch1, good);\n-      z_load_barrier_slow_path(_masm, this, Address($mem$$Register), rscratch2 \/* ref *\/, rscratch1 \/* tmp *\/);\n-      __ cmpxchg($mem$$Register, $oldval$$Register, $newval$$Register, Assembler::xword,\n-                 false \/* acquire *\/, true \/* release *\/, false \/* weak *\/, rscratch2);\n-      __ cset($res$$Register, Assembler::EQ);\n-      __ bind(good);\n-    }\n@@ -124,1 +200,1 @@\n-instruct zCompareAndSwapPAcq(iRegINoSp res, indirect mem, iRegP oldval, iRegP newval, rFlagsReg cr) %{\n+instruct zCompareAndSwapPAcq(iRegINoSp res, indirect mem, iRegP oldval, iRegP newval, iRegPNoSp oldval_tmp, iRegPNoSp newval_tmp, rFlagsReg cr) %{\n@@ -127,2 +203,2 @@\n-  predicate(UseZGC && needs_acquiring_load_exclusive(n) && (n->as_LoadStore()->barrier_data() == ZLoadBarrierStrong));\n-  effect(KILL cr, TEMP_DEF res);\n+  predicate(UseZGC && ZGenerational && needs_acquiring_load_exclusive(n) && n->as_LoadStore()->barrier_data() != 0);\n+  effect(TEMP oldval_tmp, TEMP newval_tmp, TEMP res, KILL cr);\n@@ -132,2 +208,2 @@\n- format %{ \"cmpxchg $mem, $oldval, $newval\\n\\t\"\n-           \"cset    $res, EQ\" %}\n+  format %{ \"cmpxchg $mem, $oldval, $newval\\n\\t\"\n+            \"cset    $res, EQ\" %}\n@@ -137,2 +213,5 @@\n-    __ cmpxchg($mem$$Register, $oldval$$Register, $newval$$Register, Assembler::xword,\n-               true \/* acquire *\/, true \/* release *\/, false \/* weak *\/, rscratch2);\n+    Address ref_addr($mem$$Register);\n+    z_store_barrier(_masm, this, ref_addr, $newval$$Register, $newval_tmp$$Register, rscratch2, true \/* is_atomic *\/);\n+    z_color(_masm, this, $oldval_tmp$$Register, $oldval$$Register);\n+    __ cmpxchg($mem$$Register, $oldval_tmp$$Register, $newval_tmp$$Register, Assembler::xword,\n+               true \/* acquire *\/, true \/* release *\/, false \/* weak *\/, noreg);\n@@ -140,11 +219,0 @@\n-    if (barrier_data() != ZLoadBarrierElided) {\n-      Label good;\n-      __ ldr(rscratch1, Address(rthread, ZThreadLocalData::address_bad_mask_offset()));\n-      __ andr(rscratch1, rscratch1, rscratch2);\n-      __ cbz(rscratch1, good);\n-      z_load_barrier_slow_path(_masm, this, Address($mem$$Register), rscratch2 \/* ref *\/, rscratch1 \/* tmp *\/ );\n-      __ cmpxchg($mem$$Register, $oldval$$Register, $newval$$Register, Assembler::xword,\n-                 true \/* acquire *\/, true \/* release *\/, false \/* weak *\/, rscratch2);\n-      __ cset($res$$Register, Assembler::EQ);\n-      __ bind(good);\n-    }\n@@ -156,1 +224,2 @@\n-instruct zCompareAndExchangeP(iRegPNoSp res, indirect mem, iRegP oldval, iRegP newval, rFlagsReg cr) %{\n+\n+instruct zCompareAndExchangeP(iRegPNoSp res, indirect mem, iRegP oldval, iRegP newval, iRegPNoSp oldval_tmp, iRegPNoSp newval_tmp, rFlagsReg cr) %{\n@@ -158,2 +227,2 @@\n-  predicate(UseZGC && !needs_acquiring_load_exclusive(n) && n->as_LoadStore()->barrier_data() == ZLoadBarrierStrong);\n-  effect(TEMP_DEF res, KILL cr);\n+  predicate(UseZGC && ZGenerational && !needs_acquiring_load_exclusive(n) && n->as_LoadStore()->barrier_data() != 0);\n+  effect(TEMP oldval_tmp, TEMP newval_tmp, TEMP res, KILL cr);\n@@ -163,1 +232,2 @@\n-  format %{ \"cmpxchg $res = $mem, $oldval, $newval\" %}\n+  format %{ \"cmpxchg $mem, $oldval, $newval\\n\\t\"\n+            \"cset    $res, EQ\" %}\n@@ -167,1 +237,4 @@\n-    __ cmpxchg($mem$$Register, $oldval$$Register, $newval$$Register, Assembler::xword,\n+    Address ref_addr($mem$$Register);\n+    z_store_barrier(_masm, this, ref_addr, $newval$$Register, $newval_tmp$$Register, rscratch2, true \/* is_atomic *\/);\n+    z_color(_masm, this, $oldval_tmp$$Register, $oldval$$Register);\n+    __ cmpxchg($mem$$Register, $oldval_tmp$$Register, $newval_tmp$$Register, Assembler::xword,\n@@ -169,10 +242,1 @@\n-    if (barrier_data() != ZLoadBarrierElided) {\n-      Label good;\n-      __ ldr(rscratch1, Address(rthread, ZThreadLocalData::address_bad_mask_offset()));\n-      __ andr(rscratch1, rscratch1, $res$$Register);\n-      __ cbz(rscratch1, good);\n-      z_load_barrier_slow_path(_masm, this, Address($mem$$Register), $res$$Register \/* ref *\/, rscratch1 \/* tmp *\/);\n-      __ cmpxchg($mem$$Register, $oldval$$Register, $newval$$Register, Assembler::xword,\n-                 false \/* acquire *\/, true \/* release *\/, false \/* weak *\/, $res$$Register);\n-      __ bind(good);\n-    }\n+    z_uncolor(_masm, this, $res$$Register);\n@@ -184,1 +248,1 @@\n-instruct zCompareAndExchangePAcq(iRegPNoSp res, indirect mem, iRegP oldval, iRegP newval, rFlagsReg cr) %{\n+instruct zCompareAndExchangePAcq(iRegPNoSp res, indirect mem, iRegP oldval, iRegP newval, iRegPNoSp oldval_tmp, iRegPNoSp newval_tmp, rFlagsReg cr) %{\n@@ -186,2 +250,2 @@\n-  predicate(UseZGC && needs_acquiring_load_exclusive(n) && n->as_LoadStore()->barrier_data() == ZLoadBarrierStrong);\n-  effect(TEMP_DEF res, KILL cr);\n+  predicate(UseZGC && ZGenerational && needs_acquiring_load_exclusive(n) && n->as_LoadStore()->barrier_data() != 0);\n+  effect(TEMP oldval_tmp, TEMP newval_tmp, TEMP res, KILL cr);\n@@ -191,1 +255,2 @@\n-  format %{ \"cmpxchg $res = $mem, $oldval, $newval\" %}\n+  format %{ \"cmpxchg $mem, $oldval, $newval\\n\\t\"\n+            \"cset    $res, EQ\" %}\n@@ -195,1 +260,4 @@\n-    __ cmpxchg($mem$$Register, $oldval$$Register, $newval$$Register, Assembler::xword,\n+    Address ref_addr($mem$$Register);\n+    z_store_barrier(_masm, this, ref_addr, $newval$$Register, $newval_tmp$$Register, rscratch2, true \/* is_atomic *\/);\n+    z_color(_masm, this, $oldval_tmp$$Register, $oldval$$Register);\n+    __ cmpxchg($mem$$Register, $oldval_tmp$$Register, $newval_tmp$$Register, Assembler::xword,\n@@ -197,10 +265,1 @@\n-    if (barrier_data() != ZLoadBarrierElided) {\n-      Label good;\n-      __ ldr(rscratch1, Address(rthread, ZThreadLocalData::address_bad_mask_offset()));\n-      __ andr(rscratch1, rscratch1, $res$$Register);\n-      __ cbz(rscratch1, good);\n-      z_load_barrier_slow_path(_masm, this, Address($mem$$Register), $res$$Register \/* ref *\/, rscratch1 \/* tmp *\/);\n-      __ cmpxchg($mem$$Register, $oldval$$Register, $newval$$Register, Assembler::xword,\n-                 true \/* acquire *\/, true \/* release *\/, false \/* weak *\/, $res$$Register);\n-      __ bind(good);\n-    }\n+    z_uncolor(_masm, this, $res$$Register);\n@@ -214,2 +273,2 @@\n-  predicate(UseZGC && !needs_acquiring_load_exclusive(n) && n->as_LoadStore()->barrier_data() != 0);\n-  effect(TEMP_DEF prev, KILL cr);\n+  predicate(UseZGC && ZGenerational && !needs_acquiring_load_exclusive(n) && n->as_LoadStore()->barrier_data() != 0);\n+  effect(TEMP prev, KILL cr);\n@@ -222,2 +281,3 @@\n-    __ atomic_xchg($prev$$Register, $newv$$Register, $mem$$Register);\n-    z_load_barrier(_masm, this, Address(noreg, 0), $prev$$Register, rscratch2 \/* tmp *\/, barrier_data());\n+    z_store_barrier(_masm, this, Address($mem$$Register), $newv$$Register, $prev$$Register, rscratch2, true \/* is_atomic *\/);\n+    __ atomic_xchg($prev$$Register, $prev$$Register, $mem$$Register);\n+    z_uncolor(_masm, this, $prev$$Register);\n@@ -231,2 +291,2 @@\n-  predicate(UseZGC && needs_acquiring_load_exclusive(n) && (n->as_LoadStore()->barrier_data() != 0));\n-  effect(TEMP_DEF prev, KILL cr);\n+  predicate(UseZGC && ZGenerational && needs_acquiring_load_exclusive(n) && n->as_LoadStore()->barrier_data() != 0);\n+  effect(TEMP prev, KILL cr);\n@@ -234,1 +294,1 @@\n-  ins_cost(VOLATILE_REF_COST);\n+  ins_cost(2 * VOLATILE_REF_COST);\n@@ -236,1 +296,1 @@\n-  format %{ \"atomic_xchg_acq  $prev, $newv, [$mem]\" %}\n+  format %{ \"atomic_xchg  $prev, $newv, [$mem]\" %}\n@@ -239,2 +299,3 @@\n-    __ atomic_xchgal($prev$$Register, $newv$$Register, $mem$$Register);\n-    z_load_barrier(_masm, this, Address(noreg, 0), $prev$$Register, rscratch2 \/* tmp *\/, barrier_data());\n+    z_store_barrier(_masm, this, Address($mem$$Register), $newv$$Register, $prev$$Register, rscratch2, true \/* is_atomic *\/);\n+    __ atomic_xchgal($prev$$Register, $prev$$Register, $mem$$Register);\n+    z_uncolor(_masm, this, $prev$$Register);\n@@ -242,0 +303,1 @@\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/z\/z_aarch64.ad","additions":151,"deletions":89,"binary":false,"changes":240,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -36,1 +36,2 @@\n-    format_width       =  1\n+    \/\/ Must be at least 2 for ZGC GC barrier patching.\n+    format_width       =  2\n","filename":"src\/hotspot\/cpu\/aarch64\/relocInfo_aarch64.hpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -41,2 +41,2 @@\n-  _compiler_stubs_code_size     = 30000,\n-  _final_stubs_code_size        = 20000\n+  _compiler_stubs_code_size     = 30000 ZGC_ONLY(+10000),\n+  _final_stubs_code_size        = 20000 ZGC_ONLY(+60000)\n","filename":"src\/hotspot\/cpu\/aarch64\/stubRoutines_aarch64.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -240,0 +240,1 @@\n+    ANDI_OPCODE_MASK    = (63u << OPCODE_SHIFT),\n@@ -244,0 +245,1 @@\n+    CMPLI_OPCODE_MASK   = (63u << OPCODE_SHIFT),\n@@ -1481,0 +1483,3 @@\n+  static bool is_andi(int x) {\n+     return ANDI_OPCODE == (x & ANDI_OPCODE_MASK);\n+  }\n@@ -1505,0 +1510,3 @@\n+  static bool is_cmpli(int x) {\n+     return CMPLI_OPCODE == (x & CMPLI_OPCODE_MASK);\n+  }\n","filename":"src\/hotspot\/cpu\/ppc\/assembler_ppc.hpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -0,0 +1,585 @@\n+\/*\n+ * Copyright (c) 2021, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2021, 2022 SAP SE. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"asm\/register.hpp\"\n+#include \"precompiled.hpp\"\n+#include \"asm\/macroAssembler.inline.hpp\"\n+#include \"code\/codeBlob.hpp\"\n+#include \"code\/vmreg.inline.hpp\"\n+#include \"gc\/x\/xBarrier.inline.hpp\"\n+#include \"gc\/x\/xBarrierSet.hpp\"\n+#include \"gc\/x\/xBarrierSetAssembler.hpp\"\n+#include \"gc\/x\/xBarrierSetRuntime.hpp\"\n+#include \"gc\/x\/xThreadLocalData.hpp\"\n+#include \"memory\/resourceArea.hpp\"\n+#include \"register_ppc.hpp\"\n+#include \"runtime\/sharedRuntime.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include \"utilities\/macros.hpp\"\n+#ifdef COMPILER1\n+#include \"c1\/c1_LIRAssembler.hpp\"\n+#include \"c1\/c1_MacroAssembler.hpp\"\n+#include \"gc\/x\/c1\/xBarrierSetC1.hpp\"\n+#endif \/\/ COMPILER1\n+#ifdef COMPILER2\n+#include \"gc\/x\/c2\/xBarrierSetC2.hpp\"\n+#endif \/\/ COMPILER2\n+\n+#undef __\n+#define __ masm->\n+\n+void XBarrierSetAssembler::load_at(MacroAssembler* masm, DecoratorSet decorators, BasicType type,\n+                                   Register base, RegisterOrConstant ind_or_offs, Register dst,\n+                                   Register tmp1, Register tmp2,\n+                                   MacroAssembler::PreservationLevel preservation_level, Label *L_handle_null) {\n+  __ block_comment(\"load_at (zgc) {\");\n+\n+  \/\/ Check whether a special gc barrier is required for this particular load\n+  \/\/ (e.g. whether it's a reference load or not)\n+  if (!XBarrierSet::barrier_needed(decorators, type)) {\n+    BarrierSetAssembler::load_at(masm, decorators, type, base, ind_or_offs, dst,\n+                                 tmp1, tmp2, preservation_level, L_handle_null);\n+    return;\n+  }\n+\n+  if (ind_or_offs.is_register()) {\n+    assert_different_registers(base, ind_or_offs.as_register(), tmp1, tmp2, R0, noreg);\n+    assert_different_registers(dst, ind_or_offs.as_register(), tmp1, tmp2, R0, noreg);\n+  } else {\n+    assert_different_registers(base, tmp1, tmp2, R0, noreg);\n+    assert_different_registers(dst, tmp1, tmp2, R0, noreg);\n+  }\n+\n+  \/* ==== Load the pointer using the standard implementation for the actual heap access\n+          and the decompression of compressed pointers ==== *\/\n+  \/\/ Result of 'load_at' (standard implementation) will be written back to 'dst'.\n+  \/\/ As 'base' is required for the C-call, it must be reserved in case of a register clash.\n+  Register saved_base = base;\n+  if (base == dst) {\n+    __ mr(tmp2, base);\n+    saved_base = tmp2;\n+  }\n+\n+  BarrierSetAssembler::load_at(masm, decorators, type, base, ind_or_offs, dst,\n+                               tmp1, noreg, preservation_level, L_handle_null);\n+\n+  \/* ==== Check whether pointer is dirty ==== *\/\n+  Label skip_barrier;\n+\n+  \/\/ Load bad mask into scratch register.\n+  __ ld(tmp1, (intptr_t) XThreadLocalData::address_bad_mask_offset(), R16_thread);\n+\n+  \/\/ The color bits of the to-be-tested pointer do not have to be equivalent to the 'bad_mask' testing bits.\n+  \/\/ A pointer is classified as dirty if any of the color bits that also match the bad mask is set.\n+  \/\/ Conversely, it follows that the logical AND of the bad mask and the pointer must be zero\n+  \/\/ if the pointer is not dirty.\n+  \/\/ Only dirty pointers must be processed by this barrier, so we can skip it in case the latter condition holds true.\n+  __ and_(tmp1, tmp1, dst);\n+  __ beq(CCR0, skip_barrier);\n+\n+  \/* ==== Invoke barrier ==== *\/\n+  int nbytes_save = 0;\n+\n+  const bool needs_frame = preservation_level >= MacroAssembler::PRESERVATION_FRAME_LR;\n+  const bool preserve_gp_registers = preservation_level >= MacroAssembler::PRESERVATION_FRAME_LR_GP_REGS;\n+  const bool preserve_fp_registers = preservation_level >= MacroAssembler::PRESERVATION_FRAME_LR_GP_FP_REGS;\n+\n+  const bool preserve_R3 = dst != R3_ARG1;\n+\n+  if (needs_frame) {\n+    if (preserve_gp_registers) {\n+      nbytes_save = (preserve_fp_registers\n+                     ? MacroAssembler::num_volatile_gp_regs + MacroAssembler::num_volatile_fp_regs\n+                     : MacroAssembler::num_volatile_gp_regs) * BytesPerWord;\n+      nbytes_save -= preserve_R3 ? 0 : BytesPerWord;\n+      __ save_volatile_gprs(R1_SP, -nbytes_save, preserve_fp_registers, preserve_R3);\n+    }\n+\n+    __ save_LR_CR(tmp1);\n+    __ push_frame_reg_args(nbytes_save, tmp1);\n+  }\n+\n+  \/\/ Setup arguments\n+  if (saved_base != R3_ARG1) {\n+    __ mr_if_needed(R3_ARG1, dst);\n+    __ add(R4_ARG2, ind_or_offs, saved_base);\n+  } else if (dst != R4_ARG2) {\n+    __ add(R4_ARG2, ind_or_offs, saved_base);\n+    __ mr(R3_ARG1, dst);\n+  } else {\n+    __ add(R0, ind_or_offs, saved_base);\n+    __ mr(R3_ARG1, dst);\n+    __ mr(R4_ARG2, R0);\n+  }\n+\n+  __ call_VM_leaf(XBarrierSetRuntime::load_barrier_on_oop_field_preloaded_addr(decorators));\n+\n+  Register result = R3_RET;\n+  if (needs_frame) {\n+    __ pop_frame();\n+    __ restore_LR_CR(tmp1);\n+\n+    if (preserve_R3) {\n+      __ mr(R0, R3_RET);\n+      result = R0;\n+    }\n+\n+    if (preserve_gp_registers) {\n+      __ restore_volatile_gprs(R1_SP, -nbytes_save, preserve_fp_registers, preserve_R3);\n+    }\n+  }\n+  __ mr_if_needed(dst, result);\n+\n+  __ bind(skip_barrier);\n+  __ block_comment(\"} load_at (zgc)\");\n+}\n+\n+#ifdef ASSERT\n+\/\/ The Z store barrier only verifies the pointers it is operating on and is thus a sole debugging measure.\n+void XBarrierSetAssembler::store_at(MacroAssembler* masm, DecoratorSet decorators, BasicType type,\n+                                    Register base, RegisterOrConstant ind_or_offs, Register val,\n+                                    Register tmp1, Register tmp2, Register tmp3,\n+                                    MacroAssembler::PreservationLevel preservation_level) {\n+  __ block_comment(\"store_at (zgc) {\");\n+\n+  \/\/ If the 'val' register is 'noreg', the to-be-stored value is a null pointer.\n+  if (is_reference_type(type) && val != noreg) {\n+    __ ld(tmp1, in_bytes(XThreadLocalData::address_bad_mask_offset()), R16_thread);\n+    __ and_(tmp1, tmp1, val);\n+    __ asm_assert_eq(\"Detected dirty pointer on the heap in Z store barrier\");\n+  }\n+\n+  \/\/ Store value\n+  BarrierSetAssembler::store_at(masm, decorators, type, base, ind_or_offs, val, tmp1, tmp2, tmp3, preservation_level);\n+\n+  __ block_comment(\"} store_at (zgc)\");\n+}\n+#endif \/\/ ASSERT\n+\n+void XBarrierSetAssembler::arraycopy_prologue(MacroAssembler *masm, DecoratorSet decorators, BasicType component_type,\n+                                              Register src, Register dst, Register count,\n+                                              Register preserve1, Register preserve2) {\n+  __ block_comment(\"arraycopy_prologue (zgc) {\");\n+\n+  \/* ==== Check whether a special gc barrier is required for this particular load ==== *\/\n+  if (!is_reference_type(component_type)) {\n+    return;\n+  }\n+\n+  Label skip_barrier;\n+\n+  \/\/ Fast path: Array is of length zero\n+  __ cmpdi(CCR0, count, 0);\n+  __ beq(CCR0, skip_barrier);\n+\n+  \/* ==== Ensure register sanity ==== *\/\n+  Register tmp_R11 = R11_scratch1;\n+\n+  assert_different_registers(src, dst, count, tmp_R11, noreg);\n+  if (preserve1 != noreg) {\n+    \/\/ Not technically required, but unlikely being intended.\n+    assert_different_registers(preserve1, preserve2);\n+  }\n+\n+  \/* ==== Invoke barrier (slowpath) ==== *\/\n+  int nbytes_save = 0;\n+\n+  {\n+    assert(!noreg->is_volatile(), \"sanity\");\n+\n+    if (preserve1->is_volatile()) {\n+      __ std(preserve1, -BytesPerWord * ++nbytes_save, R1_SP);\n+    }\n+\n+    if (preserve2->is_volatile() && preserve1 != preserve2) {\n+      __ std(preserve2, -BytesPerWord * ++nbytes_save, R1_SP);\n+    }\n+\n+    __ std(src, -BytesPerWord * ++nbytes_save, R1_SP);\n+    __ std(dst, -BytesPerWord * ++nbytes_save, R1_SP);\n+    __ std(count, -BytesPerWord * ++nbytes_save, R1_SP);\n+\n+    __ save_LR_CR(tmp_R11);\n+    __ push_frame_reg_args(nbytes_save, tmp_R11);\n+  }\n+\n+  \/\/ XBarrierSetRuntime::load_barrier_on_oop_array_addr(src, count)\n+  if (count == R3_ARG1) {\n+    if (src == R4_ARG2) {\n+      \/\/ Arguments are provided in reverse order\n+      __ mr(tmp_R11, count);\n+      __ mr(R3_ARG1, src);\n+      __ mr(R4_ARG2, tmp_R11);\n+    } else {\n+      __ mr(R4_ARG2, count);\n+      __ mr(R3_ARG1, src);\n+    }\n+  } else {\n+    __ mr_if_needed(R3_ARG1, src);\n+    __ mr_if_needed(R4_ARG2, count);\n+  }\n+\n+  __ call_VM_leaf(XBarrierSetRuntime::load_barrier_on_oop_array_addr());\n+\n+  __ pop_frame();\n+  __ restore_LR_CR(tmp_R11);\n+\n+  {\n+    __ ld(count, -BytesPerWord * nbytes_save--, R1_SP);\n+    __ ld(dst, -BytesPerWord * nbytes_save--, R1_SP);\n+    __ ld(src, -BytesPerWord * nbytes_save--, R1_SP);\n+\n+    if (preserve2->is_volatile() && preserve1 != preserve2) {\n+      __ ld(preserve2, -BytesPerWord * nbytes_save--, R1_SP);\n+    }\n+\n+    if (preserve1->is_volatile()) {\n+      __ ld(preserve1, -BytesPerWord * nbytes_save--, R1_SP);\n+    }\n+  }\n+\n+  __ bind(skip_barrier);\n+\n+  __ block_comment(\"} arraycopy_prologue (zgc)\");\n+}\n+\n+void XBarrierSetAssembler::try_resolve_jobject_in_native(MacroAssembler* masm, Register dst, Register jni_env,\n+                                                         Register obj, Register tmp, Label& slowpath) {\n+  __ block_comment(\"try_resolve_jobject_in_native (zgc) {\");\n+\n+  assert_different_registers(jni_env, obj, tmp);\n+\n+  \/\/ Resolve the pointer using the standard implementation for weak tag handling and pointer verification.\n+  BarrierSetAssembler::try_resolve_jobject_in_native(masm, dst, jni_env, obj, tmp, slowpath);\n+\n+  \/\/ Check whether pointer is dirty.\n+  __ ld(tmp,\n+        in_bytes(XThreadLocalData::address_bad_mask_offset() - JavaThread::jni_environment_offset()),\n+        jni_env);\n+\n+  __ and_(tmp, obj, tmp);\n+  __ bne(CCR0, slowpath);\n+\n+  __ block_comment(\"} try_resolve_jobject_in_native (zgc)\");\n+}\n+\n+#undef __\n+\n+#ifdef COMPILER1\n+#define __ ce->masm()->\n+\n+\/\/ Code emitted by LIR node \"LIR_OpXLoadBarrierTest\" which in turn is emitted by XBarrierSetC1::load_barrier.\n+\/\/ The actual compare and branch instructions are represented as stand-alone LIR nodes.\n+void XBarrierSetAssembler::generate_c1_load_barrier_test(LIR_Assembler* ce,\n+                                                         LIR_Opr ref) const {\n+  __ block_comment(\"load_barrier_test (zgc) {\");\n+\n+  __ ld(R0, in_bytes(XThreadLocalData::address_bad_mask_offset()), R16_thread);\n+  __ andr(R0, R0, ref->as_pointer_register());\n+  __ cmpdi(CCR5 \/* as mandated by LIR node *\/, R0, 0);\n+\n+  __ block_comment(\"} load_barrier_test (zgc)\");\n+}\n+\n+\/\/ Code emitted by code stub \"XLoadBarrierStubC1\" which in turn is emitted by XBarrierSetC1::load_barrier.\n+\/\/ Invokes the runtime stub which is defined just below.\n+void XBarrierSetAssembler::generate_c1_load_barrier_stub(LIR_Assembler* ce,\n+                                                         XLoadBarrierStubC1* stub) const {\n+  __ block_comment(\"c1_load_barrier_stub (zgc) {\");\n+\n+  __ bind(*stub->entry());\n+\n+  \/* ==== Determine relevant data registers and ensure register sanity ==== *\/\n+  Register ref = stub->ref()->as_register();\n+  Register ref_addr = noreg;\n+\n+  \/\/ Determine reference address\n+  if (stub->tmp()->is_valid()) {\n+    \/\/ 'tmp' register is given, so address might have an index or a displacement.\n+    ce->leal(stub->ref_addr(), stub->tmp());\n+    ref_addr = stub->tmp()->as_pointer_register();\n+  } else {\n+    \/\/ 'tmp' register is not given, so address must have neither an index nor a displacement.\n+    \/\/ The address' base register is thus usable as-is.\n+    assert(stub->ref_addr()->as_address_ptr()->disp() == 0, \"illegal displacement\");\n+    assert(!stub->ref_addr()->as_address_ptr()->index()->is_valid(), \"illegal index\");\n+\n+    ref_addr = stub->ref_addr()->as_address_ptr()->base()->as_pointer_register();\n+  }\n+\n+  assert_different_registers(ref, ref_addr, R0, noreg);\n+\n+  \/* ==== Invoke stub ==== *\/\n+  \/\/ Pass arguments via stack. The stack pointer will be bumped by the stub.\n+  __ std(ref, (intptr_t) -1 * BytesPerWord, R1_SP);\n+  __ std(ref_addr, (intptr_t) -2 * BytesPerWord, R1_SP);\n+\n+  __ load_const_optimized(R0, stub->runtime_stub());\n+  __ call_stub(R0);\n+\n+  \/\/ The runtime stub passes the result via the R0 register, overriding the previously-loaded stub address.\n+  __ mr_if_needed(ref, R0);\n+  __ b(*stub->continuation());\n+\n+  __ block_comment(\"} c1_load_barrier_stub (zgc)\");\n+}\n+\n+#undef __\n+#define __ sasm->\n+\n+\/\/ Code emitted by runtime code stub which in turn is emitted by XBarrierSetC1::generate_c1_runtime_stubs.\n+void XBarrierSetAssembler::generate_c1_load_barrier_runtime_stub(StubAssembler* sasm,\n+                                                                 DecoratorSet decorators) const {\n+  __ block_comment(\"c1_load_barrier_runtime_stub (zgc) {\");\n+\n+  const int stack_parameters = 2;\n+  const int nbytes_save = (MacroAssembler::num_volatile_regs + stack_parameters) * BytesPerWord;\n+\n+  __ save_volatile_gprs(R1_SP, -nbytes_save);\n+  __ save_LR_CR(R0);\n+\n+  \/\/ Load arguments back again from the stack.\n+  __ ld(R3_ARG1, (intptr_t) -1 * BytesPerWord, R1_SP); \/\/ ref\n+  __ ld(R4_ARG2, (intptr_t) -2 * BytesPerWord, R1_SP); \/\/ ref_addr\n+\n+  __ push_frame_reg_args(nbytes_save, R0);\n+\n+  __ call_VM_leaf(XBarrierSetRuntime::load_barrier_on_oop_field_preloaded_addr(decorators));\n+\n+  __ verify_oop(R3_RET, \"Bad pointer after barrier invocation\");\n+  __ mr(R0, R3_RET);\n+\n+  __ pop_frame();\n+  __ restore_LR_CR(R3_RET);\n+  __ restore_volatile_gprs(R1_SP, -nbytes_save);\n+\n+  __ blr();\n+\n+  __ block_comment(\"} c1_load_barrier_runtime_stub (zgc)\");\n+}\n+\n+#undef __\n+#endif \/\/ COMPILER1\n+\n+#ifdef COMPILER2\n+\n+OptoReg::Name XBarrierSetAssembler::refine_register(const Node* node, OptoReg::Name opto_reg) const {\n+  if (!OptoReg::is_reg(opto_reg)) {\n+    return OptoReg::Bad;\n+  }\n+\n+  VMReg vm_reg = OptoReg::as_VMReg(opto_reg);\n+  if ((vm_reg->is_Register() || vm_reg ->is_FloatRegister()) && (opto_reg & 1) != 0) {\n+    return OptoReg::Bad;\n+  }\n+\n+  return opto_reg;\n+}\n+\n+#define __ _masm->\n+\n+class XSaveLiveRegisters {\n+  MacroAssembler* _masm;\n+  RegMask _reg_mask;\n+  Register _result_reg;\n+  int _frame_size;\n+\n+ public:\n+  XSaveLiveRegisters(MacroAssembler *masm, XLoadBarrierStubC2 *stub)\n+      : _masm(masm), _reg_mask(stub->live()), _result_reg(stub->ref()) {\n+\n+    const int register_save_size = iterate_over_register_mask(ACTION_COUNT_ONLY) * BytesPerWord;\n+    _frame_size = align_up(register_save_size, frame::alignment_in_bytes)\n+                  + frame::abi_reg_args_size;\n+\n+    __ save_LR_CR(R0);\n+    __ push_frame(_frame_size, R0);\n+\n+    iterate_over_register_mask(ACTION_SAVE, _frame_size);\n+  }\n+\n+  ~XSaveLiveRegisters() {\n+    iterate_over_register_mask(ACTION_RESTORE, _frame_size);\n+\n+    __ addi(R1_SP, R1_SP, _frame_size);\n+    __ restore_LR_CR(R0);\n+  }\n+\n+ private:\n+  enum IterationAction : int {\n+    ACTION_SAVE,\n+    ACTION_RESTORE,\n+    ACTION_COUNT_ONLY\n+  };\n+\n+  int iterate_over_register_mask(IterationAction action, int offset = 0) {\n+    int reg_save_index = 0;\n+    RegMaskIterator live_regs_iterator(_reg_mask);\n+\n+    while(live_regs_iterator.has_next()) {\n+      const OptoReg::Name opto_reg = live_regs_iterator.next();\n+\n+      \/\/ Filter out stack slots (spilled registers, i.e., stack-allocated registers).\n+      if (!OptoReg::is_reg(opto_reg)) {\n+        continue;\n+      }\n+\n+      const VMReg vm_reg = OptoReg::as_VMReg(opto_reg);\n+      if (vm_reg->is_Register()) {\n+        Register std_reg = vm_reg->as_Register();\n+\n+        \/\/ '_result_reg' will hold the end result of the operation. Its content must thus not be preserved.\n+        if (std_reg == _result_reg) {\n+          continue;\n+        }\n+\n+        if (std_reg->encoding() >= R2->encoding() && std_reg->encoding() <= R12->encoding()) {\n+          reg_save_index++;\n+\n+          if (action == ACTION_SAVE) {\n+            _masm->std(std_reg, offset - reg_save_index * BytesPerWord, R1_SP);\n+          } else if (action == ACTION_RESTORE) {\n+            _masm->ld(std_reg, offset - reg_save_index * BytesPerWord, R1_SP);\n+          } else {\n+            assert(action == ACTION_COUNT_ONLY, \"Sanity\");\n+          }\n+        }\n+      } else if (vm_reg->is_FloatRegister()) {\n+        FloatRegister fp_reg = vm_reg->as_FloatRegister();\n+        if (fp_reg->encoding() >= F0->encoding() && fp_reg->encoding() <= F13->encoding()) {\n+          reg_save_index++;\n+\n+          if (action == ACTION_SAVE) {\n+            _masm->stfd(fp_reg, offset - reg_save_index * BytesPerWord, R1_SP);\n+          } else if (action == ACTION_RESTORE) {\n+            _masm->lfd(fp_reg, offset - reg_save_index * BytesPerWord, R1_SP);\n+          } else {\n+            assert(action == ACTION_COUNT_ONLY, \"Sanity\");\n+          }\n+        }\n+      } else if (vm_reg->is_ConditionRegister()) {\n+        \/\/ NOP. Conditions registers are covered by save_LR_CR\n+      } else if (vm_reg->is_VectorSRegister()) {\n+        assert(SuperwordUseVSX, \"or should not reach here\");\n+        VectorSRegister vs_reg = vm_reg->as_VectorSRegister();\n+        if (vs_reg->encoding() >= VSR32->encoding() && vs_reg->encoding() <= VSR51->encoding()) {\n+          reg_save_index += 2;\n+\n+          Register spill_addr = R0;\n+          if (action == ACTION_SAVE) {\n+            _masm->addi(spill_addr, R1_SP, offset - reg_save_index * BytesPerWord);\n+            _masm->stxvd2x(vs_reg, spill_addr);\n+          } else if (action == ACTION_RESTORE) {\n+            _masm->addi(spill_addr, R1_SP, offset - reg_save_index * BytesPerWord);\n+            _masm->lxvd2x(vs_reg, spill_addr);\n+          } else {\n+            assert(action == ACTION_COUNT_ONLY, \"Sanity\");\n+          }\n+        }\n+      } else {\n+        if (vm_reg->is_SpecialRegister()) {\n+          fatal(\"Special registers are unsupported. Found register %s\", vm_reg->name());\n+        } else {\n+          fatal(\"Register type is not known\");\n+        }\n+      }\n+    }\n+\n+    return reg_save_index;\n+  }\n+};\n+\n+#undef __\n+#define __ _masm->\n+\n+class XSetupArguments {\n+  MacroAssembler* const _masm;\n+  const Register        _ref;\n+  const Address         _ref_addr;\n+\n+ public:\n+  XSetupArguments(MacroAssembler* masm, XLoadBarrierStubC2* stub) :\n+      _masm(masm),\n+      _ref(stub->ref()),\n+      _ref_addr(stub->ref_addr()) {\n+\n+    \/\/ Desired register\/argument configuration:\n+    \/\/ _ref: R3_ARG1\n+    \/\/ _ref_addr: R4_ARG2\n+\n+    \/\/ '_ref_addr' can be unspecified. In that case, the barrier will not heal the reference.\n+    if (_ref_addr.base() == noreg) {\n+      assert_different_registers(_ref, R0, noreg);\n+\n+      __ mr_if_needed(R3_ARG1, _ref);\n+      __ li(R4_ARG2, 0);\n+    } else {\n+      assert_different_registers(_ref, _ref_addr.base(), R0, noreg);\n+      assert(!_ref_addr.index()->is_valid(), \"reference addresses must not contain an index component\");\n+\n+      if (_ref != R4_ARG2) {\n+        \/\/ Calculate address first as the address' base register might clash with R4_ARG2\n+        __ addi(R4_ARG2, _ref_addr.base(), _ref_addr.disp());\n+        __ mr_if_needed(R3_ARG1, _ref);\n+      } else if (_ref_addr.base() != R3_ARG1) {\n+        __ mr(R3_ARG1, _ref);\n+        __ addi(R4_ARG2, _ref_addr.base(), _ref_addr.disp()); \/\/ Clobbering _ref\n+      } else {\n+        \/\/ Arguments are provided in inverse order (i.e. _ref == R4_ARG2, _ref_addr == R3_ARG1)\n+        __ mr(R0, _ref);\n+        __ addi(R4_ARG2, _ref_addr.base(), _ref_addr.disp());\n+        __ mr(R3_ARG1, R0);\n+      }\n+    }\n+  }\n+};\n+\n+#undef __\n+#define __ masm->\n+\n+void XBarrierSetAssembler::generate_c2_load_barrier_stub(MacroAssembler* masm, XLoadBarrierStubC2* stub) const {\n+  __ block_comment(\"generate_c2_load_barrier_stub (zgc) {\");\n+\n+  __ bind(*stub->entry());\n+\n+  Register ref = stub->ref();\n+  Address ref_addr = stub->ref_addr();\n+\n+  assert_different_registers(ref, ref_addr.base());\n+\n+  {\n+    XSaveLiveRegisters save_live_registers(masm, stub);\n+    XSetupArguments setup_arguments(masm, stub);\n+\n+    __ call_VM_leaf(stub->slow_path());\n+    __ mr_if_needed(ref, R3_RET);\n+  }\n+\n+  __ b(*stub->continuation());\n+\n+  __ block_comment(\"} generate_c2_load_barrier_stub (zgc)\");\n+}\n+\n+#undef __\n+#endif \/\/ COMPILER2\n","filename":"src\/hotspot\/cpu\/ppc\/gc\/x\/xBarrierSetAssembler_ppc.cpp","additions":585,"deletions":0,"binary":false,"changes":585,"status":"added"},{"patch":"@@ -0,0 +1,93 @@\n+\/*\n+ * Copyright (c) 2021, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2021, 2022 SAP SE. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef CPU_PPC_GC_X_XBARRIERSETASSEMBLER_PPC_HPP\n+#define CPU_PPC_GC_X_XBARRIERSETASSEMBLER_PPC_HPP\n+\n+#include \"code\/vmreg.hpp\"\n+#include \"oops\/accessDecorators.hpp\"\n+#ifdef COMPILER2\n+#include \"opto\/optoreg.hpp\"\n+#endif \/\/ COMPILER2\n+\n+#ifdef COMPILER1\n+class LIR_Assembler;\n+class LIR_Opr;\n+class StubAssembler;\n+#endif \/\/ COMPILER1\n+\n+#ifdef COMPILER2\n+class Node;\n+#endif \/\/ COMPILER2\n+\n+#ifdef COMPILER1\n+class XLoadBarrierStubC1;\n+#endif \/\/ COMPILER1\n+\n+#ifdef COMPILER2\n+class XLoadBarrierStubC2;\n+#endif \/\/ COMPILER2\n+\n+class XBarrierSetAssembler : public XBarrierSetAssemblerBase {\n+public:\n+  virtual void load_at(MacroAssembler* masm, DecoratorSet decorators, BasicType type,\n+                       Register base, RegisterOrConstant ind_or_offs, Register dst,\n+                       Register tmp1, Register tmp2,\n+                       MacroAssembler::PreservationLevel preservation_level, Label *L_handle_null = NULL);\n+\n+#ifdef ASSERT\n+  virtual void store_at(MacroAssembler* masm, DecoratorSet decorators, BasicType type,\n+                        Register base, RegisterOrConstant ind_or_offs, Register val,\n+                        Register tmp1, Register tmp2, Register tmp3,\n+                        MacroAssembler::PreservationLevel preservation_level);\n+#endif \/\/ ASSERT\n+\n+  virtual void arraycopy_prologue(MacroAssembler* masm, DecoratorSet decorators, BasicType type,\n+                                  Register src, Register dst, Register count,\n+                                  Register preserve1, Register preserve2);\n+\n+  virtual void try_resolve_jobject_in_native(MacroAssembler* masm, Register dst, Register jni_env,\n+                                             Register obj, Register tmp, Label& slowpath);\n+\n+  virtual NMethodPatchingType nmethod_patching_type() { return NMethodPatchingType::conc_data_patch; }\n+\n+#ifdef COMPILER1\n+  void generate_c1_load_barrier_test(LIR_Assembler* ce,\n+                                     LIR_Opr ref) const;\n+\n+  void generate_c1_load_barrier_stub(LIR_Assembler* ce,\n+                                     XLoadBarrierStubC1* stub) const;\n+\n+  void generate_c1_load_barrier_runtime_stub(StubAssembler* sasm,\n+                                             DecoratorSet decorators) const;\n+#endif \/\/ COMPILER1\n+\n+#ifdef COMPILER2\n+  OptoReg::Name refine_register(const Node* node, OptoReg::Name opto_reg) const;\n+\n+  void generate_c2_load_barrier_stub(MacroAssembler* masm, XLoadBarrierStubC2* stub) const;\n+#endif \/\/ COMPILER2\n+};\n+\n+#endif \/\/ CPU_PPC_GC_X_XBARRIERSETASSEMBLER_PPC_HPP\n","filename":"src\/hotspot\/cpu\/ppc\/gc\/x\/xBarrierSetAssembler_ppc.hpp","additions":93,"deletions":0,"binary":false,"changes":93,"status":"added"},{"patch":"@@ -28,1 +28,1 @@\n-#include \"gc\/z\/zGlobals.hpp\"\n+#include \"gc\/x\/xGlobals.hpp\"\n@@ -157,1 +157,1 @@\n-    \/\/ In debug builds, an assertion in 'ZPlatformAddressOffsetBits' will bail out the VM to indicate that\n+    \/\/ In debug builds, an assertion in 'XPlatformAddressOffsetBits' will bail out the VM to indicate that\n@@ -187,1 +187,1 @@\n-size_t ZPlatformAddressOffsetBits() {\n+size_t XPlatformAddressOffsetBits() {\n@@ -195,1 +195,1 @@\n-  const size_t address_offset = round_up_power_of_2(MaxHeapSize * ZVirtualToPhysicalRatio);\n+  const size_t address_offset = round_up_power_of_2(MaxHeapSize * XVirtualToPhysicalRatio);\n@@ -201,2 +201,2 @@\n-size_t ZPlatformAddressMetadataShift() {\n-  return ZPlatformAddressOffsetBits();\n+size_t XPlatformAddressMetadataShift() {\n+  return XPlatformAddressOffsetBits();\n","filename":"src\/hotspot\/cpu\/ppc\/gc\/x\/xGlobals_ppc.cpp","additions":6,"deletions":6,"binary":false,"changes":12,"previous_filename":"src\/hotspot\/cpu\/ppc\/gc\/z\/zGlobals_ppc.cpp","status":"renamed"},{"patch":"@@ -0,0 +1,36 @@\n+\/*\n+ * Copyright (c) 2021, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2021 SAP SE. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef CPU_PPC_GC_X_XGLOBALS_PPC_HPP\n+#define CPU_PPC_GC_X_XGLOBALS_PPC_HPP\n+\n+#include \"globalDefinitions_ppc.hpp\"\n+\n+const size_t XPlatformHeapViews        = 3;\n+const size_t XPlatformCacheLineSize    = DEFAULT_CACHE_LINE_SIZE;\n+\n+size_t XPlatformAddressOffsetBits();\n+size_t XPlatformAddressMetadataShift();\n+\n+#endif \/\/ CPU_PPC_GC_X_XGLOBALS_PPC_HPP\n","filename":"src\/hotspot\/cpu\/ppc\/gc\/x\/xGlobals_ppc.hpp","additions":36,"deletions":0,"binary":false,"changes":36,"status":"added"},{"patch":"@@ -0,0 +1,298 @@\n+\/\/\n+\/\/ Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+\/\/ Copyright (c) 2021 SAP SE. All rights reserved.\n+\/\/ DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+\/\/\n+\/\/ This code is free software; you can redistribute it and\/or modify it\n+\/\/ under the terms of the GNU General Public License version 2 only, as\n+\/\/ published by the Free Software Foundation.\n+\/\/\n+\/\/ This code is distributed in the hope that it will be useful, but WITHOUT\n+\/\/ ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+\/\/ FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+\/\/ version 2 for more details (a copy is included in the LICENSE file that\n+\/\/ accompanied this code).\n+\/\/\n+\/\/ You should have received a copy of the GNU General Public License version\n+\/\/ 2 along with this work; if not, write to the Free Software Foundation,\n+\/\/ Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+\/\/\n+\/\/ Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+\/\/ or visit www.oracle.com if you need additional information or have any\n+\/\/ questions.\n+\/\/\n+\n+source_hpp %{\n+\n+#include \"gc\/shared\/gc_globals.hpp\"\n+#include \"gc\/x\/c2\/xBarrierSetC2.hpp\"\n+#include \"gc\/x\/xThreadLocalData.hpp\"\n+\n+%}\n+\n+source %{\n+\n+static void x_load_barrier(MacroAssembler& _masm, const MachNode* node, Address ref_addr, Register ref,\n+                           Register tmp, uint8_t barrier_data) {\n+  if (barrier_data == XLoadBarrierElided) {\n+    return;\n+  }\n+\n+  XLoadBarrierStubC2* const stub = XLoadBarrierStubC2::create(node, ref_addr, ref, tmp, barrier_data);\n+  __ ld(tmp, in_bytes(XThreadLocalData::address_bad_mask_offset()), R16_thread);\n+  __ and_(tmp, tmp, ref);\n+  __ bne_far(CCR0, *stub->entry(), MacroAssembler::bc_far_optimize_on_relocate);\n+  __ bind(*stub->continuation());\n+}\n+\n+static void x_load_barrier_slow_path(MacroAssembler& _masm, const MachNode* node, Address ref_addr, Register ref,\n+                                     Register tmp) {\n+  XLoadBarrierStubC2* const stub = XLoadBarrierStubC2::create(node, ref_addr, ref, tmp, XLoadBarrierStrong);\n+  __ b(*stub->entry());\n+  __ bind(*stub->continuation());\n+}\n+\n+static void x_compare_and_swap(MacroAssembler& _masm, const MachNode* node,\n+                              Register res, Register mem, Register oldval, Register newval,\n+                              Register tmp_xchg, Register tmp_mask,\n+                              bool weak, bool acquire) {\n+  \/\/ z-specific load barrier requires strong CAS operations.\n+  \/\/ Weak CAS operations are thus only emitted if the barrier is elided.\n+  __ cmpxchgd(CCR0, tmp_xchg, oldval, newval, mem,\n+              MacroAssembler::MemBarNone, MacroAssembler::cmpxchgx_hint_atomic_update(), res, NULL, true,\n+              weak && node->barrier_data() == XLoadBarrierElided);\n+\n+  if (node->barrier_data() != XLoadBarrierElided) {\n+    Label skip_barrier;\n+\n+    __ ld(tmp_mask, in_bytes(XThreadLocalData::address_bad_mask_offset()), R16_thread);\n+    __ and_(tmp_mask, tmp_mask, tmp_xchg);\n+    __ beq(CCR0, skip_barrier);\n+\n+    \/\/ CAS must have failed because pointer in memory is bad.\n+    x_load_barrier_slow_path(_masm, node, Address(mem), tmp_xchg, res \/* used as tmp *\/);\n+\n+    __ cmpxchgd(CCR0, tmp_xchg, oldval, newval, mem,\n+                MacroAssembler::MemBarNone, MacroAssembler::cmpxchgx_hint_atomic_update(), res, NULL, true, weak);\n+\n+    __ bind(skip_barrier);\n+  }\n+\n+  if (acquire) {\n+    if (support_IRIW_for_not_multiple_copy_atomic_cpu) {\n+      \/\/ Uses the isync instruction as an acquire barrier.\n+      \/\/ This exploits the compare and the branch in the z load barrier (load, compare and branch, isync).\n+      __ isync();\n+    } else {\n+      __ sync();\n+    }\n+  }\n+}\n+\n+static void x_compare_and_exchange(MacroAssembler& _masm, const MachNode* node,\n+                                   Register res, Register mem, Register oldval, Register newval, Register tmp,\n+                                   bool weak, bool acquire) {\n+  \/\/ z-specific load barrier requires strong CAS operations.\n+  \/\/ Weak CAS operations are thus only emitted if the barrier is elided.\n+  __ cmpxchgd(CCR0, res, oldval, newval, mem,\n+              MacroAssembler::MemBarNone, MacroAssembler::cmpxchgx_hint_atomic_update(), noreg, NULL, true,\n+              weak && node->barrier_data() == XLoadBarrierElided);\n+\n+  if (node->barrier_data() != XLoadBarrierElided) {\n+    Label skip_barrier;\n+    __ ld(tmp, in_bytes(XThreadLocalData::address_bad_mask_offset()), R16_thread);\n+    __ and_(tmp, tmp, res);\n+    __ beq(CCR0, skip_barrier);\n+\n+    x_load_barrier_slow_path(_masm, node, Address(mem), res, tmp);\n+\n+    __ cmpxchgd(CCR0, res, oldval, newval, mem,\n+                MacroAssembler::MemBarNone, MacroAssembler::cmpxchgx_hint_atomic_update(), noreg, NULL, true, weak);\n+\n+    __ bind(skip_barrier);\n+  }\n+\n+  if (acquire) {\n+    if (support_IRIW_for_not_multiple_copy_atomic_cpu) {\n+      \/\/ Uses the isync instruction as an acquire barrier.\n+      \/\/ This exploits the compare and the branch in the z load barrier (load, compare and branch, isync).\n+      __ isync();\n+    } else {\n+      __ sync();\n+    }\n+  }\n+}\n+\n+%}\n+\n+instruct xLoadP(iRegPdst dst, memoryAlg4 mem, iRegPdst tmp, flagsRegCR0 cr0)\n+%{\n+  match(Set dst (LoadP mem));\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr0);\n+  ins_cost(MEMORY_REF_COST);\n+\n+  predicate((UseZGC && !ZGenerational && n->as_Load()->barrier_data() != 0)\n+            && (n->as_Load()->is_unordered() || followed_by_acquire(n)));\n+\n+  format %{ \"LD $dst, $mem\" %}\n+  ins_encode %{\n+    assert($mem$$index == 0, \"sanity\");\n+    __ ld($dst$$Register, $mem$$disp, $mem$$base$$Register);\n+    x_load_barrier(_masm, this, Address($mem$$base$$Register, $mem$$disp), $dst$$Register, $tmp$$Register, barrier_data());\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+\/\/ Load Pointer Volatile\n+instruct xLoadP_acq(iRegPdst dst, memoryAlg4 mem, iRegPdst tmp, flagsRegCR0 cr0)\n+%{\n+  match(Set dst (LoadP mem));\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr0);\n+  ins_cost(3 * MEMORY_REF_COST);\n+\n+  \/\/ Predicate on instruction order is implicitly present due to the predicate of the cheaper zLoadP operation\n+  predicate(UseZGC && !ZGenerational && n->as_Load()->barrier_data() != 0);\n+\n+  format %{ \"LD acq $dst, $mem\" %}\n+  ins_encode %{\n+    __ ld($dst$$Register, $mem$$disp, $mem$$base$$Register);\n+    x_load_barrier(_masm, this, Address($mem$$base$$Register, $mem$$disp), $dst$$Register, $tmp$$Register, barrier_data());\n+\n+    \/\/ Uses the isync instruction as an acquire barrier.\n+    \/\/ This exploits the compare and the branch in the z load barrier (load, compare and branch, isync).\n+    __ isync();\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct xCompareAndSwapP(iRegIdst res, iRegPdst mem, iRegPsrc oldval, iRegPsrc newval,\n+                                iRegPdst tmp_xchg, iRegPdst tmp_mask, flagsRegCR0 cr0) %{\n+  match(Set res (CompareAndSwapP mem (Binary oldval newval)));\n+  effect(TEMP_DEF res, TEMP tmp_xchg, TEMP tmp_mask, KILL cr0);\n+\n+  predicate((UseZGC && !ZGenerational && n->as_LoadStore()->barrier_data() == XLoadBarrierStrong)\n+            && (((CompareAndSwapNode*)n)->order() != MemNode::acquire && ((CompareAndSwapNode*) n)->order() != MemNode::seqcst));\n+\n+  format %{ \"CMPXCHG $res, $mem, $oldval, $newval; as bool; ptr\" %}\n+  ins_encode %{\n+    x_compare_and_swap(_masm, this,\n+                                $res$$Register, $mem$$Register, $oldval$$Register, $newval$$Register,\n+                                $tmp_xchg$$Register, $tmp_mask$$Register,\n+                                false \/* weak *\/, false \/* acquire *\/);\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct xCompareAndSwapP_acq(iRegIdst res, iRegPdst mem, iRegPsrc oldval, iRegPsrc newval,\n+                                    iRegPdst tmp_xchg, iRegPdst tmp_mask, flagsRegCR0 cr0) %{\n+  match(Set res (CompareAndSwapP mem (Binary oldval newval)));\n+  effect(TEMP_DEF res, TEMP tmp_xchg, TEMP tmp_mask, KILL cr0);\n+\n+  predicate((UseZGC && !ZGenerational && n->as_LoadStore()->barrier_data() == XLoadBarrierStrong)\n+            && (((CompareAndSwapNode*)n)->order() == MemNode::acquire || ((CompareAndSwapNode*) n)->order() == MemNode::seqcst));\n+\n+  format %{ \"CMPXCHG acq $res, $mem, $oldval, $newval; as bool; ptr\" %}\n+  ins_encode %{\n+    x_compare_and_swap(_masm, this,\n+                                $res$$Register, $mem$$Register, $oldval$$Register, $newval$$Register,\n+                                $tmp_xchg$$Register, $tmp_mask$$Register,\n+                                false \/* weak *\/, true \/* acquire *\/);\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct xCompareAndSwapPWeak(iRegIdst res, iRegPdst mem, iRegPsrc oldval, iRegPsrc newval,\n+                                    iRegPdst tmp_xchg, iRegPdst tmp_mask, flagsRegCR0 cr0) %{\n+  match(Set res (WeakCompareAndSwapP mem (Binary oldval newval)));\n+  effect(TEMP_DEF res, TEMP tmp_xchg, TEMP tmp_mask, KILL cr0);\n+\n+  predicate((UseZGC && !ZGenerational && n->as_LoadStore()->barrier_data() == XLoadBarrierStrong)\n+            && ((CompareAndSwapNode*)n)->order() != MemNode::acquire && ((CompareAndSwapNode*) n)->order() != MemNode::seqcst);\n+\n+  format %{ \"weak CMPXCHG $res, $mem, $oldval, $newval; as bool; ptr\" %}\n+  ins_encode %{\n+    x_compare_and_swap(_masm, this,\n+                                $res$$Register, $mem$$Register, $oldval$$Register, $newval$$Register,\n+                                $tmp_xchg$$Register, $tmp_mask$$Register,\n+                                true \/* weak *\/, false \/* acquire *\/);\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct xCompareAndSwapPWeak_acq(iRegIdst res, iRegPdst mem, iRegPsrc oldval, iRegPsrc newval,\n+                                        iRegPdst tmp_xchg, iRegPdst tmp_mask, flagsRegCR0 cr0) %{\n+  match(Set res (WeakCompareAndSwapP mem (Binary oldval newval)));\n+  effect(TEMP_DEF res, TEMP tmp_xchg, TEMP tmp_mask, KILL cr0);\n+\n+  predicate((UseZGC && !ZGenerational && n->as_LoadStore()->barrier_data() == XLoadBarrierStrong)\n+            && (((CompareAndSwapNode*)n)->order() == MemNode::acquire || ((CompareAndSwapNode*) n)->order() == MemNode::seqcst));\n+\n+  format %{ \"weak CMPXCHG acq $res, $mem, $oldval, $newval; as bool; ptr\" %}\n+  ins_encode %{\n+    x_compare_and_swap(_masm, this,\n+                                $res$$Register, $mem$$Register, $oldval$$Register, $newval$$Register,\n+                                $tmp_xchg$$Register, $tmp_mask$$Register,\n+                                true \/* weak *\/, true \/* acquire *\/);\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct xCompareAndExchangeP(iRegPdst res, iRegPdst mem, iRegPsrc oldval, iRegPsrc newval,\n+                              iRegPdst tmp, flagsRegCR0 cr0) %{\n+  match(Set res (CompareAndExchangeP mem (Binary oldval newval)));\n+  effect(TEMP_DEF res, TEMP tmp, KILL cr0);\n+\n+  predicate((UseZGC && !ZGenerational && n->as_LoadStore()->barrier_data() == XLoadBarrierStrong)\n+            && (\n+              ((CompareAndSwapNode*)n)->order() != MemNode::acquire\n+              && ((CompareAndSwapNode*)n)->order() != MemNode::seqcst\n+            ));\n+\n+  format %{ \"CMPXCHG $res, $mem, $oldval, $newval; as ptr; ptr\" %}\n+  ins_encode %{\n+    x_compare_and_exchange(_masm, this,\n+                                    $res$$Register, $mem$$Register, $oldval$$Register, $newval$$Register, $tmp$$Register,\n+                                    false \/* weak *\/, false \/* acquire *\/);\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct xCompareAndExchangeP_acq(iRegPdst res, iRegPdst mem, iRegPsrc oldval, iRegPsrc newval,\n+                                        iRegPdst tmp, flagsRegCR0 cr0) %{\n+  match(Set res (CompareAndExchangeP mem (Binary oldval newval)));\n+  effect(TEMP_DEF res, TEMP tmp, KILL cr0);\n+\n+  predicate((UseZGC && !ZGenerational && n->as_LoadStore()->barrier_data() == XLoadBarrierStrong)\n+            && (\n+              ((CompareAndSwapNode*)n)->order() == MemNode::acquire\n+              || ((CompareAndSwapNode*)n)->order() == MemNode::seqcst\n+            ));\n+\n+  format %{ \"CMPXCHG acq $res, $mem, $oldval, $newval; as ptr; ptr\" %}\n+  ins_encode %{\n+    x_compare_and_exchange(_masm, this,\n+                                    $res$$Register, $mem$$Register, $oldval$$Register, $newval$$Register, $tmp$$Register,\n+                                    false \/* weak *\/, true \/* acquire *\/);\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct xGetAndSetP(iRegPdst res, iRegPdst mem, iRegPsrc newval, iRegPdst tmp, flagsRegCR0 cr0) %{\n+  match(Set res (GetAndSetP mem newval));\n+  effect(TEMP_DEF res, TEMP tmp, KILL cr0);\n+\n+  predicate(UseZGC && !ZGenerational && n->as_LoadStore()->barrier_data() != 0);\n+\n+  format %{ \"GetAndSetP $res, $mem, $newval\" %}\n+  ins_encode %{\n+    __ getandsetd($res$$Register, $newval$$Register, $mem$$Register, MacroAssembler::cmpxchgx_hint_atomic_update());\n+    x_load_barrier(_masm, this, Address(noreg, (intptr_t) 0), $res$$Register, $tmp$$Register, barrier_data());\n+\n+    if (support_IRIW_for_not_multiple_copy_atomic_cpu) {\n+      __ isync();\n+    } else {\n+      __ sync();\n+    }\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n","filename":"src\/hotspot\/cpu\/ppc\/gc\/x\/x_ppc.ad","additions":298,"deletions":0,"binary":false,"changes":298,"status":"added"},{"patch":"@@ -0,0 +1,106 @@\n+\/*\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/shared\/gcLogPrecious.hpp\"\n+#include \"gc\/shared\/gc_globals.hpp\"\n+#include \"gc\/z\/zAddress.inline.hpp\"\n+#include \"gc\/z\/zGlobals.hpp\"\n+#include \"runtime\/globals.hpp\"\n+#include \"runtime\/os.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include \"utilities\/powerOfTwo.hpp\"\n+\n+#ifdef LINUX\n+#include <sys\/mman.h>\n+#endif \/\/ LINUX\n+\n+\/\/ Default value if probing is not implemented for a certain platform: 128TB\n+static const size_t DEFAULT_MAX_ADDRESS_BIT = 47;\n+\/\/ Minimum value returned, if probing fails: 64GB\n+static const size_t MINIMUM_MAX_ADDRESS_BIT = 36;\n+\n+static size_t probe_valid_max_address_bit() {\n+#ifdef LINUX\n+  size_t max_address_bit = 0;\n+  const size_t page_size = os::vm_page_size();\n+  for (size_t i = DEFAULT_MAX_ADDRESS_BIT; i > MINIMUM_MAX_ADDRESS_BIT; --i) {\n+    const uintptr_t base_addr = ((uintptr_t) 1U) << i;\n+    if (msync((void*)base_addr, page_size, MS_ASYNC) == 0) {\n+      \/\/ msync suceeded, the address is valid, and maybe even already mapped.\n+      max_address_bit = i;\n+      break;\n+    }\n+    if (errno != ENOMEM) {\n+      \/\/ Some error occured. This should never happen, but msync\n+      \/\/ has some undefined behavior, hence ignore this bit.\n+#ifdef ASSERT\n+      fatal(\"Received '%s' while probing the address space for the highest valid bit\", os::errno_name(errno));\n+#else \/\/ ASSERT\n+      log_warning_p(gc)(\"Received '%s' while probing the address space for the highest valid bit\", os::errno_name(errno));\n+#endif \/\/ ASSERT\n+      continue;\n+    }\n+    \/\/ Since msync failed with ENOMEM, the page might not be mapped.\n+    \/\/ Try to map it, to see if the address is valid.\n+    void* const result_addr = mmap((void*) base_addr, page_size, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS|MAP_NORESERVE, -1, 0);\n+    if (result_addr != MAP_FAILED) {\n+      munmap(result_addr, page_size);\n+    }\n+    if ((uintptr_t) result_addr == base_addr) {\n+      \/\/ address is valid\n+      max_address_bit = i;\n+      break;\n+    }\n+  }\n+  if (max_address_bit == 0) {\n+    \/\/ probing failed, allocate a very high page and take that bit as the maximum\n+    const uintptr_t high_addr = ((uintptr_t) 1U) << DEFAULT_MAX_ADDRESS_BIT;\n+    void* const result_addr = mmap((void*) high_addr, page_size, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS|MAP_NORESERVE, -1, 0);\n+    if (result_addr != MAP_FAILED) {\n+      max_address_bit = BitsPerSize_t - count_leading_zeros((size_t) result_addr) - 1;\n+      munmap(result_addr, page_size);\n+    }\n+  }\n+  log_info_p(gc, init)(\"Probing address space for the highest valid bit: \" SIZE_FORMAT, max_address_bit);\n+  return MAX2(max_address_bit, MINIMUM_MAX_ADDRESS_BIT);\n+#else \/\/ LINUX\n+  return DEFAULT_MAX_ADDRESS_BIT;\n+#endif \/\/ LINUX\n+}\n+\n+size_t ZPlatformAddressOffsetBits() {\n+  const static size_t valid_max_address_offset_bits = probe_valid_max_address_bit() + 1;\n+  const size_t max_address_offset_bits = valid_max_address_offset_bits - 3;\n+  const size_t min_address_offset_bits = max_address_offset_bits - 2;\n+  const size_t address_offset = round_up_power_of_2(MaxHeapSize * ZVirtualToPhysicalRatio);\n+  const size_t address_offset_bits = log2i_exact(address_offset);\n+  return clamp(address_offset_bits, min_address_offset_bits, max_address_offset_bits);\n+}\n+\n+size_t ZPlatformAddressHeapBaseShift() {\n+  return ZPlatformAddressOffsetBits();\n+}\n+\n+void ZGlobalsPointers::pd_set_good_masks() {\n+}\n","filename":"src\/hotspot\/cpu\/ppc\/gc\/z\/zAddress_ppc.cpp","additions":106,"deletions":0,"binary":false,"changes":106,"status":"added"},{"patch":"@@ -0,0 +1,34 @@\n+\/*\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef CPU_PPC_GC_Z_ZADDRESS_PPC_HPP\n+#define CPU_PPC_GC_Z_ZADDRESS_PPC_HPP\n+\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+const size_t ZPointerLoadShift = 16;\n+\n+size_t ZPlatformAddressOffsetBits();\n+size_t ZPlatformAddressHeapBaseShift();\n+\n+#endif \/\/ CPU_PPC_GC_Z_ZADDRESS_PPC_HPP\n","filename":"src\/hotspot\/cpu\/ppc\/gc\/z\/zAddress_ppc.hpp","additions":34,"deletions":0,"binary":false,"changes":34,"status":"added"},{"patch":"@@ -0,0 +1,37 @@\n+\/*\n+ * Copyright (c) 2019, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef CPU_PPC_GC_Z_ZADDRESS_PPC_INLINE_HPP\n+#define CPU_PPC_GC_Z_ZADDRESS_PPC_INLINE_HPP\n+\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+inline uintptr_t ZPointer::remap_bits(uintptr_t colored) {\n+  return colored & ZPointerRemappedMask;\n+}\n+\n+inline constexpr int ZPointer::load_shift_lookup(uintptr_t value) {\n+  return ZPointerLoadShift;\n+}\n+\n+#endif \/\/ CPU_PPC_GC_Z_ZADDRESS_PPC_INLINE_HPP\n","filename":"src\/hotspot\/cpu\/ppc\/gc\/z\/zAddress_ppc.inline.hpp","additions":37,"deletions":0,"binary":false,"changes":37,"status":"added"},{"patch":"@@ -25,1 +25,0 @@\n-#include \"asm\/register.hpp\"\n@@ -28,0 +27,1 @@\n+#include \"asm\/register.hpp\"\n@@ -30,0 +30,1 @@\n+#include \"gc\/z\/zAddress.hpp\"\n@@ -37,0 +38,1 @@\n+#include \"runtime\/jniHandles.hpp\"\n@@ -47,0 +49,1 @@\n+#include \"opto\/output.hpp\"\n@@ -52,0 +55,64 @@\n+\/\/ Helper for saving and restoring registers across a runtime call that does\n+\/\/ not have any live vector registers.\n+class ZRuntimeCallSpill {\n+  MacroAssembler* _masm;\n+  Register _result;\n+  bool _needs_frame, _preserve_gp_registers, _preserve_fp_registers;\n+  int _nbytes_save;\n+\n+  void save() {\n+    MacroAssembler* masm = _masm;\n+\n+    if (_needs_frame) {\n+      if (_preserve_gp_registers) {\n+        bool preserve_R3 = _result != R3_ARG1;\n+        _nbytes_save = (MacroAssembler::num_volatile_gp_regs\n+                        + (_preserve_fp_registers ? MacroAssembler::num_volatile_fp_regs : 0)\n+                        - (preserve_R3 ? 0 : 1)\n+                       ) * BytesPerWord;\n+        __ save_volatile_gprs(R1_SP, -_nbytes_save, _preserve_fp_registers, preserve_R3);\n+      }\n+\n+      __ save_LR_CR(R0);\n+      __ push_frame_reg_args(_nbytes_save, R0);\n+    }\n+  }\n+\n+  void restore() {\n+    MacroAssembler* masm = _masm;\n+\n+    Register result = R3_RET;\n+    if (_needs_frame) {\n+      __ pop_frame();\n+      __ restore_LR_CR(R0);\n+\n+      if (_preserve_gp_registers) {\n+        bool restore_R3 = _result != R3_ARG1;\n+        if (restore_R3 && _result != noreg) {\n+          __ mr(R0, R3_RET);\n+          result = R0;\n+        }\n+        __ restore_volatile_gprs(R1_SP, -_nbytes_save, _preserve_fp_registers, restore_R3);\n+      }\n+    }\n+    if (_result != noreg) {\n+      __ mr_if_needed(_result, result);\n+    }\n+  }\n+\n+public:\n+  ZRuntimeCallSpill(MacroAssembler* masm, Register result, MacroAssembler::PreservationLevel preservation_level)\n+    : _masm(masm),\n+      _result(result),\n+      _needs_frame(preservation_level >= MacroAssembler::PRESERVATION_FRAME_LR),\n+      _preserve_gp_registers(preservation_level >= MacroAssembler::PRESERVATION_FRAME_LR_GP_REGS),\n+      _preserve_fp_registers(preservation_level >= MacroAssembler::PRESERVATION_FRAME_LR_GP_FP_REGS),\n+      _nbytes_save(0) {\n+    save();\n+  }\n+  ~ZRuntimeCallSpill() {\n+    restore();\n+  }\n+};\n+\n+\n@@ -84,2 +151,1 @@\n-  BarrierSetAssembler::load_at(masm, decorators, type, base, ind_or_offs, dst,\n-                               tmp1, noreg, preservation_level, L_handle_null);\n+  __ ld(dst, ind_or_offs, base);\n@@ -88,1 +154,5 @@\n-  Label skip_barrier;\n+  Label done, uncolor;\n+\n+  const bool on_non_strong =\n+    (decorators & ON_WEAK_OOP_REF) != 0 ||\n+    (decorators & ON_PHANTOM_OOP_REF) != 0;\n@@ -91,1 +161,5 @@\n-  __ ld(tmp1, (intptr_t) ZThreadLocalData::address_bad_mask_offset(), R16_thread);\n+  if (on_non_strong) {\n+    __ ld(tmp1, in_bytes(ZThreadLocalData::mark_bad_mask_offset()), R16_thread);\n+  } else {\n+    __ ld(tmp1, in_bytes(ZThreadLocalData::load_bad_mask_offset()), R16_thread);\n+  }\n@@ -99,1 +173,1 @@\n-  __ beq(CCR0, skip_barrier);\n+  __ beq(CCR0, uncolor);\n@@ -102,1 +176,15 @@\n-  int nbytes_save = 0;\n+  {\n+    ZRuntimeCallSpill rcs(masm, dst, preservation_level);\n+\n+    \/\/ Setup arguments\n+    if (saved_base != R3_ARG1 && ind_or_offs.register_or_noreg() != R3_ARG1) {\n+      __ mr_if_needed(R3_ARG1, dst);\n+      __ add(R4_ARG2, ind_or_offs, saved_base);\n+    } else if (dst != R4_ARG2) {\n+      __ add(R4_ARG2, ind_or_offs, saved_base);\n+      __ mr(R3_ARG1, dst);\n+    } else {\n+      __ add(R0, ind_or_offs, saved_base);\n+      __ mr(R3_ARG1, dst);\n+      __ mr(R4_ARG2, R0);\n+    }\n@@ -104,3 +192,2 @@\n-  const bool needs_frame = preservation_level >= MacroAssembler::PRESERVATION_FRAME_LR;\n-  const bool preserve_gp_registers = preservation_level >= MacroAssembler::PRESERVATION_FRAME_LR_GP_REGS;\n-  const bool preserve_fp_registers = preservation_level >= MacroAssembler::PRESERVATION_FRAME_LR_GP_FP_REGS;\n+    __ call_VM_leaf(ZBarrierSetRuntime::load_barrier_on_oop_field_preloaded_addr(decorators));\n+  }\n@@ -108,1 +195,6 @@\n-  const bool preserve_R3 = dst != R3_ARG1;\n+  \/\/ Slow-path has already uncolored\n+  if (L_handle_null != nullptr) {\n+    __ cmpdi(CCR0, dst, 0);\n+    __ beq(CCR0, *L_handle_null);\n+  }\n+  __ b(done);\n@@ -110,8 +202,7 @@\n-  if (needs_frame) {\n-    if (preserve_gp_registers) {\n-      nbytes_save = (preserve_fp_registers\n-                     ? MacroAssembler::num_volatile_gp_regs + MacroAssembler::num_volatile_fp_regs\n-                     : MacroAssembler::num_volatile_gp_regs) * BytesPerWord;\n-      nbytes_save -= preserve_R3 ? 0 : BytesPerWord;\n-      __ save_volatile_gprs(R1_SP, -nbytes_save, preserve_fp_registers, preserve_R3);\n-    }\n+  __ bind(uncolor);\n+  if (L_handle_null == nullptr) {\n+    __ srdi(dst, dst, ZPointerLoadShift);\n+  } else {\n+    __ srdi_(dst, dst, ZPointerLoadShift);\n+    __ beq(CCR0, *L_handle_null);\n+  }\n@@ -119,2 +210,13 @@\n-    __ save_LR_CR(tmp1);\n-    __ push_frame_reg_args(nbytes_save, tmp1);\n+  __ bind(done);\n+  __ block_comment(\"} load_at (zgc)\");\n+}\n+\n+static void load_least_significant_16_oop_bits(MacroAssembler* masm, Register dst, RegisterOrConstant ind_or_offs, Register base) {\n+  assert_different_registers(dst, base);\n+#ifndef VM_LITTLE_ENDIAN\n+  const int BE_offset = 6;\n+  if (ind_or_offs.is_register()) {\n+    __ addi(dst, ind_or_offs.as_register(), BE_offset);\n+    __ lhzx(dst, base, dst);\n+  } else {\n+    __ lhz(dst, ind_or_offs.as_constant() + BE_offset, base);\n@@ -122,0 +224,4 @@\n+#else\n+  __ lhz(dst, ind_or_offs, base);\n+#endif\n+}\n@@ -123,7 +229,10 @@\n-  \/\/ Setup arguments\n-  if (saved_base != R3_ARG1) {\n-    __ mr_if_needed(R3_ARG1, dst);\n-    __ add(R4_ARG2, ind_or_offs, saved_base);\n-  } else if (dst != R4_ARG2) {\n-    __ add(R4_ARG2, ind_or_offs, saved_base);\n-    __ mr(R3_ARG1, dst);\n+static void emit_store_fast_path_check(MacroAssembler* masm, Register base, RegisterOrConstant ind_or_offs, bool is_atomic, Label& medium_path) {\n+  if (is_atomic) {\n+    assert(ZPointerLoadShift + LogMinObjAlignmentInBytes >= 16, \"or replace following code\");\n+    load_least_significant_16_oop_bits(masm, R0, ind_or_offs, base);\n+    \/\/ Atomic operations must ensure that the contents of memory are store-good before\n+    \/\/ an atomic operation can execute.\n+    \/\/ A not relocatable object could have spurious raw null pointers in its fields after\n+    \/\/ getting promoted to the old generation.\n+    __ relocate(barrier_Relocation::spec(), ZBarrierRelocationFormatStoreGoodBits);\n+    __ cmplwi(CCR0, R0, barrier_Relocation::unpatched);\n@@ -131,3 +240,7 @@\n-    __ add(R0, ind_or_offs, saved_base);\n-    __ mr(R3_ARG1, dst);\n-    __ mr(R4_ARG2, R0);\n+    __ ld(R0, ind_or_offs, base);\n+    \/\/ Stores on relocatable objects never need to deal with raw null pointers in fields.\n+    \/\/ Raw null pointers may only exist in the young generation, as they get pruned when\n+    \/\/ the object is relocated to old. And no pre-write barrier needs to perform any action\n+    \/\/ in the young generation.\n+    __ relocate(barrier_Relocation::spec(), ZBarrierRelocationFormatStoreBadMask);\n+    __ andi_(R0, R0, barrier_Relocation::unpatched);\n@@ -135,0 +248,2 @@\n+  __ bc_far_optimized(Assembler::bcondCRbiIs0, __ bi0(CCR0, Assembler::equal), medium_path);\n+}\n@@ -136,1 +251,35 @@\n-  __ call_VM_leaf(ZBarrierSetRuntime::load_barrier_on_oop_field_preloaded_addr(decorators));\n+void ZBarrierSetAssembler::store_barrier_fast(MacroAssembler* masm,\n+                                              Register ref_base,\n+                                              RegisterOrConstant ind_or_offset,\n+                                              Register rnew_zaddress,\n+                                              Register rnew_zpointer,\n+                                              bool in_nmethod,\n+                                              bool is_atomic,\n+                                              Label& medium_path,\n+                                              Label& medium_path_continuation) const {\n+  assert_different_registers(ref_base, rnew_zpointer);\n+  assert_different_registers(ind_or_offset.register_or_noreg(), rnew_zpointer);\n+  assert_different_registers(rnew_zaddress, rnew_zpointer);\n+\n+  if (in_nmethod) {\n+    emit_store_fast_path_check(masm, ref_base, ind_or_offset, is_atomic, medium_path);\n+    __ bind(medium_path_continuation);\n+    __ relocate(barrier_Relocation::spec(), ZBarrierRelocationFormatStoreGoodBits);\n+    __ li(rnew_zpointer, barrier_Relocation::unpatched); \/\/ Load color bits.\n+    if (rnew_zaddress == noreg) { \/\/ noreg encodes null.\n+      if (ZPointerLoadShift >= 16) {\n+        __ rldicl(rnew_zpointer, rnew_zpointer, 0, 64 - ZPointerLoadShift); \/\/ Clear sign extension from li.\n+      }\n+    }\n+  } else {\n+    __ ld(R0, ind_or_offset, ref_base);\n+    __ ld(rnew_zpointer, in_bytes(ZThreadLocalData::store_bad_mask_offset()), R16_thread);\n+    __ and_(R0, R0, rnew_zpointer);\n+    __ bne(CCR0, medium_path);\n+    __ bind(medium_path_continuation);\n+    __ ld(rnew_zpointer, in_bytes(ZThreadLocalData::store_good_mask_offset()), R16_thread);\n+  }\n+  if (rnew_zaddress != noreg) { \/\/ noreg encodes null.\n+    __ rldimi(rnew_zpointer, rnew_zaddress, ZPointerLoadShift, 0); \/\/ Insert shifted pointer.\n+  }\n+}\n@@ -138,4 +287,23 @@\n-  Register result = R3_RET;\n-  if (needs_frame) {\n-    __ pop_frame();\n-    __ restore_LR_CR(tmp1);\n+static void store_barrier_buffer_add(MacroAssembler* masm,\n+                                     Register ref_base,\n+                                     RegisterOrConstant ind_or_offs,\n+                                     Register tmp1,\n+                                     Label& slow_path) {\n+  __ ld(tmp1, in_bytes(ZThreadLocalData::store_barrier_buffer_offset()), R16_thread);\n+\n+  \/\/ Combined pointer bump and check if the buffer is disabled or full\n+  __ ld(R0, in_bytes(ZStoreBarrierBuffer::current_offset()), tmp1);\n+  __ addic_(R0, R0, -(int)sizeof(ZStoreBarrierEntry));\n+  __ blt(CCR0, slow_path);\n+  __ std(R0, in_bytes(ZStoreBarrierBuffer::current_offset()), tmp1);\n+\n+  \/\/ Entry is at ZStoreBarrierBuffer (tmp1) + buffer_offset + scaled index (R0)\n+  __ add(tmp1, tmp1, R0);\n+\n+  \/\/ Compute and log the store address\n+  Register store_addr = ref_base;\n+  if (!ind_or_offs.is_constant() || ind_or_offs.as_constant() != 0) {\n+    __ add(R0, ind_or_offs, ref_base);\n+    store_addr = R0;\n+  }\n+  __ std(store_addr, in_bytes(ZStoreBarrierBuffer::buffer_offset()) + in_bytes(ZStoreBarrierEntry::p_offset()), tmp1);\n@@ -143,4 +311,4 @@\n-    if (preserve_R3) {\n-      __ mr(R0, R3_RET);\n-      result = R0;\n-    }\n+  \/\/ Load and log the prev value\n+  __ ld(R0, ind_or_offs, ref_base);\n+  __ std(R0, in_bytes(ZStoreBarrierBuffer::buffer_offset()) + in_bytes(ZStoreBarrierEntry::prev_offset()), tmp1);\n+}\n@@ -148,2 +316,30 @@\n-    if (preserve_gp_registers) {\n-      __ restore_volatile_gprs(R1_SP, -nbytes_save, preserve_fp_registers, preserve_R3);\n+void ZBarrierSetAssembler::store_barrier_medium(MacroAssembler* masm,\n+                                                Register ref_base,\n+                                                RegisterOrConstant ind_or_offs,\n+                                                Register tmp,\n+                                                bool is_atomic,\n+                                                Label& medium_path_continuation,\n+                                                Label& slow_path) const {\n+  assert_different_registers(ref_base, tmp, R0);\n+\n+  \/\/ The reason to end up in the medium path is that the pre-value was not 'good'.\n+\n+  if (is_atomic) {\n+    \/\/ Atomic accesses can get to the medium fast path because the value was a\n+    \/\/ raw null value. If it was not null, then there is no doubt we need to take a slow path.\n+    __ ld(tmp, ind_or_offs, ref_base);\n+    __ cmpdi(CCR0, tmp, 0);\n+    __ bne(CCR0, slow_path);\n+\n+    \/\/ If we get this far, we know there is a young raw null value in the field.\n+    \/\/ Try to self-heal null values for atomic accesses\n+    bool need_restore = false;\n+    if (!ind_or_offs.is_constant() || ind_or_offs.as_constant() != 0) {\n+      __ add(ref_base, ind_or_offs, ref_base);\n+      need_restore = true;\n+    }\n+    __ ld(R0, in_bytes(ZThreadLocalData::store_good_mask_offset()), R16_thread);\n+    __ cmpxchgd(CCR0, tmp, (intptr_t)0, R0, ref_base,\n+                MacroAssembler::MemBarNone, MacroAssembler::cmpxchgx_hint_atomic_update());\n+    if (need_restore) {\n+      __ subf(ref_base, ind_or_offs, ref_base);\n@@ -151,0 +347,11 @@\n+    __ bne(CCR0, slow_path);\n+  } else {\n+    \/\/ A non-atomic relocatable object won't get to the medium fast path due to a\n+    \/\/ raw null in the young generation. We only get here because the field is bad.\n+    \/\/ In this path we don't need any self healing, so we can avoid a runtime call\n+    \/\/ most of the time by buffering the store barrier to be applied lazily.\n+    store_barrier_buffer_add(masm,\n+                             ref_base,\n+                             ind_or_offs,\n+                             tmp,\n+                             slow_path);\n@@ -152,4 +359,1 @@\n-  __ mr_if_needed(dst, result);\n-\n-  __ bind(skip_barrier);\n-  __ block_comment(\"} load_at (zgc)\");\n+  __ b(medium_path_continuation);\n@@ -158,1 +362,0 @@\n-#ifdef ASSERT\n@@ -166,6 +369,28 @@\n-  \/\/ If the 'val' register is 'noreg', the to-be-stored value is a null pointer.\n-  if (is_reference_type(type) && val != noreg) {\n-    __ ld(tmp1, in_bytes(ZThreadLocalData::address_bad_mask_offset()), R16_thread);\n-    __ and_(tmp1, tmp1, val);\n-    __ asm_assert_eq(\"Detected dirty pointer on the heap in Z store barrier\");\n-  }\n+  bool dest_uninitialized = (decorators & IS_DEST_UNINITIALIZED) != 0;\n+\n+  if (is_reference_type(type)) {\n+    assert_different_registers(base, val, tmp1, tmp2, tmp3);\n+\n+    if (dest_uninitialized) {\n+      \/\/ tmp1 = (val << ZPointerLoadShift) | store_good_mask\n+      __ ld(tmp1, in_bytes(ZThreadLocalData::store_good_mask_offset()), R16_thread);\n+      if (val != noreg) { \/\/ noreg encodes null.\n+        __ rldimi(tmp1, val, ZPointerLoadShift, 0);\n+      }\n+    } else {\n+      Label done;\n+      Label medium;\n+      Label medium_continuation; \/\/ bound in store_barrier_fast\n+      Label slow;\n+\n+      store_barrier_fast(masm, base, ind_or_offs, val, tmp1, false, false, medium, medium_continuation);\n+      __ b(done);\n+      __ bind(medium);\n+      store_barrier_medium(masm, base, ind_or_offs, tmp1, false, medium_continuation, slow);\n+      __ bind(slow);\n+      {\n+        ZRuntimeCallSpill rcs(masm, noreg, preservation_level);\n+        __ add(R3_ARG1, ind_or_offs, base);\n+        __ call_VM_leaf(ZBarrierSetRuntime::store_barrier_on_oop_field_without_healing_addr(), R3_ARG1);\n+      }\n+      __ b(medium_continuation);\n@@ -173,2 +398,6 @@\n-  \/\/ Store value\n-  BarrierSetAssembler::store_at(masm, decorators, type, base, ind_or_offs, val, tmp1, tmp2, tmp3, preservation_level);\n+      __ bind(done);\n+    }\n+    BarrierSetAssembler::store_at(masm, decorators, type, base, ind_or_offs, tmp1, tmp2, tmp3, noreg, preservation_level);\n+  } else {\n+    BarrierSetAssembler::store_at(masm, decorators, type, base, ind_or_offs, val, tmp1, tmp2, tmp3, preservation_level);\n+  }\n@@ -178,1 +407,0 @@\n-#endif \/\/ ASSERT\n@@ -180,1 +408,4 @@\n-void ZBarrierSetAssembler::arraycopy_prologue(MacroAssembler *masm, DecoratorSet decorators, BasicType component_type,\n+\/* arraycopy *\/\n+const Register _load_bad_mask = R6, _store_bad_mask = R7, _store_good_mask = R8;\n+\n+void ZBarrierSetAssembler::arraycopy_prologue(MacroAssembler *masm, DecoratorSet decorators, BasicType type,\n@@ -183,1 +414,2 @@\n-  __ block_comment(\"arraycopy_prologue (zgc) {\");\n+  bool is_checkcast_copy  = (decorators & ARRAYCOPY_CHECKCAST)   != 0,\n+       dest_uninitialized = (decorators & IS_DEST_UNINITIALIZED) != 0;\n@@ -185,2 +417,2 @@\n-  \/* ==== Check whether a special gc barrier is required for this particular load ==== *\/\n-  if (!is_reference_type(component_type)) {\n+  if (!ZBarrierSet::barrier_needed(decorators, type) || is_checkcast_copy) {\n+    \/\/ Barrier not needed\n@@ -190,1 +422,1 @@\n-  Label skip_barrier;\n+  __ block_comment(\"arraycopy_prologue (zgc) {\");\n@@ -192,3 +424,1 @@\n-  \/\/ Fast path: Array is of length zero\n-  __ cmpdi(CCR0, count, 0);\n-  __ beq(CCR0, skip_barrier);\n+  load_copy_masks(masm, _load_bad_mask, _store_bad_mask, _store_good_mask, dest_uninitialized);\n@@ -196,2 +426,2 @@\n-  \/* ==== Ensure register sanity ==== *\/\n-  Register tmp_R11 = R11_scratch1;\n+  __ block_comment(\"} arraycopy_prologue (zgc)\");\n+}\n@@ -199,4 +429,11 @@\n-  assert_different_registers(src, dst, count, tmp_R11, noreg);\n-  if (preserve1 != noreg) {\n-    \/\/ Not technically required, but unlikely being intended.\n-    assert_different_registers(preserve1, preserve2);\n+void ZBarrierSetAssembler::load_copy_masks(MacroAssembler* masm,\n+                                           Register load_bad_mask,\n+                                           Register store_bad_mask,\n+                                           Register store_good_mask,\n+                                           bool dest_uninitialized) const {\n+  __ ld(load_bad_mask, in_bytes(ZThreadLocalData::load_bad_mask_offset()), R16_thread);\n+  __ ld(store_good_mask, in_bytes(ZThreadLocalData::store_good_mask_offset()), R16_thread);\n+  if (dest_uninitialized) {\n+    DEBUG_ONLY( __ li(store_bad_mask, -1); )\n+  } else {\n+    __ ld(store_bad_mask, in_bytes(ZThreadLocalData::store_bad_mask_offset()), R16_thread);\n@@ -204,4 +441,21 @@\n-\n-  \/* ==== Invoke barrier (slowpath) ==== *\/\n-  int nbytes_save = 0;\n-\n+}\n+void ZBarrierSetAssembler::copy_load_at_fast(MacroAssembler* masm,\n+                                             Register zpointer,\n+                                             Register addr,\n+                                             Register load_bad_mask,\n+                                             Label& slow_path,\n+                                             Label& continuation) const {\n+  __ ldx(zpointer, addr);\n+  __ and_(R0, zpointer, load_bad_mask);\n+  __ bne(CCR0, slow_path);\n+  __ bind(continuation);\n+}\n+void ZBarrierSetAssembler::copy_load_at_slow(MacroAssembler* masm,\n+                                             Register zpointer,\n+                                             Register addr,\n+                                             Register tmp,\n+                                             Label& slow_path,\n+                                             Label& continuation) const {\n+  __ align(32);\n+  __ bind(slow_path);\n+  __ mfctr(tmp); \/\/ preserve loop counter\n@@ -209,16 +463,4 @@\n-    assert(!noreg->is_volatile(), \"sanity\");\n-\n-    if (preserve1->is_volatile()) {\n-      __ std(preserve1, -BytesPerWord * ++nbytes_save, R1_SP);\n-    }\n-\n-    if (preserve2->is_volatile() && preserve1 != preserve2) {\n-      __ std(preserve2, -BytesPerWord * ++nbytes_save, R1_SP);\n-    }\n-\n-    __ std(src, -BytesPerWord * ++nbytes_save, R1_SP);\n-    __ std(dst, -BytesPerWord * ++nbytes_save, R1_SP);\n-    __ std(count, -BytesPerWord * ++nbytes_save, R1_SP);\n-\n-    __ save_LR_CR(tmp_R11);\n-    __ push_frame_reg_args(nbytes_save, tmp_R11);\n+    ZRuntimeCallSpill rcs(masm, R0, MacroAssembler::PRESERVATION_FRAME_LR_GP_REGS);\n+    assert(zpointer != R4_ARG2, \"or change argument setup\");\n+    __ mr_if_needed(R4_ARG2, addr);\n+    __ call_VM_leaf(ZBarrierSetRuntime::load_barrier_on_oop_field_preloaded_addr(), zpointer, R4_ARG2);\n@@ -226,11 +468,37 @@\n-\n-  \/\/ ZBarrierSetRuntime::load_barrier_on_oop_array_addr(src, count)\n-  if (count == R3_ARG1) {\n-    if (src == R4_ARG2) {\n-      \/\/ Arguments are provided in reverse order\n-      __ mr(tmp_R11, count);\n-      __ mr(R3_ARG1, src);\n-      __ mr(R4_ARG2, tmp_R11);\n-    } else {\n-      __ mr(R4_ARG2, count);\n-      __ mr(R3_ARG1, src);\n+  __ sldi(zpointer, R0, ZPointerLoadShift); \/\/ Slow-path has uncolored; revert\n+  __ mtctr(tmp); \/\/ restore loop counter\n+  __ b(continuation);\n+}\n+void ZBarrierSetAssembler::copy_store_at_fast(MacroAssembler* masm,\n+                                              Register zpointer,\n+                                              Register addr,\n+                                              Register store_bad_mask,\n+                                              Register store_good_mask,\n+                                              Label& medium_path,\n+                                              Label& continuation,\n+                                              bool dest_uninitialized) const {\n+  if (!dest_uninitialized) {\n+    __ ldx(R0, addr);\n+    __ and_(R0, R0, store_bad_mask);\n+    __ bne(CCR0, medium_path);\n+    __ bind(continuation);\n+  }\n+  __ rldimi(zpointer, store_good_mask, 0, 64 - ZPointerLoadShift); \/\/ Replace color bits.\n+  __ stdx(zpointer, addr);\n+}\n+void ZBarrierSetAssembler::copy_store_at_slow(MacroAssembler* masm,\n+                                              Register addr,\n+                                              Register tmp,\n+                                              Label& medium_path,\n+                                              Label& continuation,\n+                                              bool dest_uninitialized) const {\n+  if (!dest_uninitialized) {\n+    Label slow_path;\n+    __ align(32);\n+    __ bind(medium_path);\n+    store_barrier_medium(masm, addr, (intptr_t)0, tmp, false, continuation, slow_path);\n+    __ bind(slow_path);\n+    __ mfctr(tmp); \/\/ preserve loop counter\n+    {\n+      ZRuntimeCallSpill rcs(masm, noreg, MacroAssembler::PRESERVATION_FRAME_LR_GP_REGS);\n+      __ call_VM_leaf(ZBarrierSetRuntime::store_barrier_on_oop_field_without_healing_addr(), addr);\n@@ -238,3 +506,2 @@\n-  } else {\n-    __ mr_if_needed(R3_ARG1, src);\n-    __ mr_if_needed(R4_ARG2, count);\n+    __ mtctr(tmp); \/\/ restore loop counter\n+    __ b(continuation);\n@@ -242,0 +509,1 @@\n+}\n@@ -243,4 +511,22 @@\n-  __ call_VM_leaf(ZBarrierSetRuntime::load_barrier_on_oop_array_addr());\n-\n-  __ pop_frame();\n-  __ restore_LR_CR(tmp_R11);\n+\/\/ Arguments for generated stub:\n+\/\/      from:  R3_ARG1\n+\/\/      to:    R4_ARG2\n+\/\/      count: R5_ARG3 (int >= 0)\n+void ZBarrierSetAssembler::generate_disjoint_oop_copy(MacroAssembler* masm, bool dest_uninitialized) {\n+  const Register zpointer = R2, tmp = R9;\n+  Label done, loop, load_bad, load_good, store_bad, store_good;\n+  __ cmpdi(CCR0, R5_ARG3, 0);\n+  __ beq(CCR0, done);\n+  __ mtctr(R5_ARG3);\n+\n+  __ align(32);\n+  __ bind(loop);\n+  copy_load_at_fast(masm, zpointer, R3_ARG1, _load_bad_mask, load_bad, load_good);\n+  copy_store_at_fast(masm, zpointer, R4_ARG2, _store_bad_mask, _store_good_mask, store_bad, store_good, dest_uninitialized);\n+  __ addi(R3_ARG1, R3_ARG1, 8);\n+  __ addi(R4_ARG2, R4_ARG2, 8);\n+  __ bdnz(loop);\n+\n+  __ bind(done);\n+  __ li(R3_RET, 0);\n+  __ blr();\n@@ -248,4 +534,3 @@\n-  {\n-    __ ld(count, -BytesPerWord * nbytes_save--, R1_SP);\n-    __ ld(dst, -BytesPerWord * nbytes_save--, R1_SP);\n-    __ ld(src, -BytesPerWord * nbytes_save--, R1_SP);\n+  copy_load_at_slow(masm, zpointer, R3_ARG1, tmp, load_bad, load_good);\n+  copy_store_at_slow(masm, R4_ARG2, tmp, store_bad, store_good, dest_uninitialized);\n+}\n@@ -253,3 +538,21 @@\n-    if (preserve2->is_volatile() && preserve1 != preserve2) {\n-      __ ld(preserve2, -BytesPerWord * nbytes_save--, R1_SP);\n-    }\n+void ZBarrierSetAssembler::generate_conjoint_oop_copy(MacroAssembler* masm, bool dest_uninitialized) {\n+  const Register zpointer = R2, tmp = R9;\n+  Label done, loop, load_bad, load_good, store_bad, store_good;\n+  __ sldi_(R0, R5_ARG3, 3);\n+  __ beq(CCR0, done);\n+  __ mtctr(R5_ARG3);\n+  \/\/ Point behind last elements and copy backwards.\n+  __ add(R3_ARG1, R3_ARG1, R0);\n+  __ add(R4_ARG2, R4_ARG2, R0);\n+\n+  __ align(32);\n+  __ bind(loop);\n+  __ addi(R3_ARG1, R3_ARG1, -8);\n+  __ addi(R4_ARG2, R4_ARG2, -8);\n+  copy_load_at_fast(masm, zpointer, R3_ARG1, _load_bad_mask, load_bad, load_good);\n+  copy_store_at_fast(masm, zpointer, R4_ARG2, _store_bad_mask, _store_good_mask, store_bad, store_good, dest_uninitialized);\n+  __ bdnz(loop);\n+\n+  __ bind(done);\n+  __ li(R3_RET, 0);\n+  __ blr();\n@@ -257,4 +560,3 @@\n-    if (preserve1->is_volatile()) {\n-      __ ld(preserve1, -BytesPerWord * nbytes_save--, R1_SP);\n-    }\n-  }\n+  copy_load_at_slow(masm, zpointer, R3_ARG1, tmp, load_bad, load_good);\n+  copy_store_at_slow(masm, R4_ARG2, tmp, store_bad, store_good, dest_uninitialized);\n+}\n@@ -262,1 +564,0 @@\n-  __ bind(skip_barrier);\n@@ -264,1 +565,19 @@\n-  __ block_comment(\"} arraycopy_prologue (zgc)\");\n+\/\/ Verify a colored pointer.\n+void ZBarrierSetAssembler::check_oop(MacroAssembler *masm, Register obj, const char* msg) {\n+  if (!VerifyOops) {\n+    return;\n+  }\n+  Label done, skip_uncolor;\n+  \/\/ Skip (colored) null.\n+  __ srdi_(R0, obj, ZPointerLoadShift);\n+  __ beq(CCR0, done);\n+\n+  \/\/ Check if ZAddressHeapBase << ZPointerLoadShift is set. If so, we need to uncolor.\n+  __ rldicl_(R0, obj, 64 - ZAddressHeapBaseShift - ZPointerLoadShift, 63);\n+  __ mr(R0, obj);\n+  __ beq(CCR0, skip_uncolor);\n+  __ srdi(R0, obj, ZPointerLoadShift);\n+  __ bind(skip_uncolor);\n+\n+  __ verify_oop(R0, msg);\n+  __ bind(done);\n@@ -267,0 +586,1 @@\n+\n@@ -271,1 +591,18 @@\n-  assert_different_registers(jni_env, obj, tmp);\n+  Label done, tagged, weak_tagged, check_color;\n+  Address load_bad_mask = load_bad_mask_from_jni_env(jni_env),\n+          mark_bad_mask = mark_bad_mask_from_jni_env(jni_env);\n+\n+  \/\/ Test for tag\n+  __ andi_(tmp, obj, JNIHandles::tag_mask);\n+  __ bne(CCR0, tagged);\n+\n+  \/\/ Resolve local handle\n+  __ ld(dst, 0, obj);\n+  __ b(done);\n+\n+  __ bind(tagged);\n+\n+  \/\/ Test for weak tag\n+  __ andi_(tmp, obj, JNIHandles::TypeTag::weak_global);\n+  __ clrrdi(dst, obj, JNIHandles::tag_size); \/\/ Untag.\n+  __ bne(CCR0, weak_tagged);\n@@ -273,2 +610,4 @@\n-  \/\/ Resolve the pointer using the standard implementation for weak tag handling and pointer verification.\n-  BarrierSetAssembler::try_resolve_jobject_in_native(masm, dst, jni_env, obj, tmp, slowpath);\n+  \/\/ Resolve global handle\n+  __ ld(dst, 0, dst);\n+  __ ld(tmp, load_bad_mask.disp(), load_bad_mask.base());\n+  __ b(check_color);\n@@ -276,4 +615,1 @@\n-  \/\/ Check whether pointer is dirty.\n-  __ ld(tmp,\n-        in_bytes(ZThreadLocalData::address_bad_mask_offset() - JavaThread::jni_environment_offset()),\n-        jni_env);\n+  __ bind(weak_tagged);\n@@ -281,1 +617,6 @@\n-  __ and_(tmp, obj, tmp);\n+  \/\/ Resolve weak handle\n+  __ ld(dst, 0, dst);\n+  __ ld(tmp, mark_bad_mask.disp(), mark_bad_mask.base());\n+\n+  __ bind(check_color);\n+  __ and_(tmp, tmp, dst);\n@@ -284,0 +625,5 @@\n+  \/\/ Uncolor\n+  __ srdi(dst, dst, ZPointerLoadShift);\n+\n+  __ bind(done);\n+\n@@ -292,5 +638,11 @@\n-\/\/ Code emitted by LIR node \"LIR_OpZLoadBarrierTest\" which in turn is emitted by ZBarrierSetC1::load_barrier.\n-\/\/ The actual compare and branch instructions are represented as stand-alone LIR nodes.\n-void ZBarrierSetAssembler::generate_c1_load_barrier_test(LIR_Assembler* ce,\n-                                                         LIR_Opr ref) const {\n-  __ block_comment(\"load_barrier_test (zgc) {\");\n+static void z_uncolor(LIR_Assembler* ce, LIR_Opr ref) {\n+  Register r = ref->as_register();\n+  __ srdi(r, r, ZPointerLoadShift);\n+}\n+\n+static void check_color(LIR_Assembler* ce, LIR_Opr ref, bool on_non_strong) {\n+  int relocFormat = on_non_strong ? ZBarrierRelocationFormatMarkBadMask\n+                                  : ZBarrierRelocationFormatLoadBadMask;\n+  __ relocate(barrier_Relocation::spec(), relocFormat);\n+  __ andi_(R0, ref->as_register(), barrier_Relocation::unpatched);\n+}\n@@ -298,3 +650,13 @@\n-  __ ld(R0, in_bytes(ZThreadLocalData::address_bad_mask_offset()), R16_thread);\n-  __ andr(R0, R0, ref->as_pointer_register());\n-  __ cmpdi(CCR5 \/* as mandated by LIR node *\/, R0, 0);\n+static void z_color(LIR_Assembler* ce, LIR_Opr ref) {\n+  __ sldi(ref->as_register(), ref->as_register(), ZPointerLoadShift);\n+  __ relocate(barrier_Relocation::spec(), ZBarrierRelocationFormatStoreGoodBits);\n+  __ ori(ref->as_register(), ref->as_register(), barrier_Relocation::unpatched);\n+}\n+\n+void ZBarrierSetAssembler::generate_c1_uncolor(LIR_Assembler* ce, LIR_Opr ref) const {\n+  z_uncolor(ce, ref);\n+}\n+\n+void ZBarrierSetAssembler::generate_c1_color(LIR_Assembler* ce, LIR_Opr ref) const {\n+  z_color(ce, ref);\n+}\n@@ -302,1 +664,8 @@\n-  __ block_comment(\"} load_barrier_test (zgc)\");\n+void ZBarrierSetAssembler::generate_c1_load_barrier(LIR_Assembler* ce,\n+                                                    LIR_Opr ref,\n+                                                    ZLoadBarrierStubC1* stub,\n+                                                    bool on_non_strong) const {\n+  check_color(ce, ref, on_non_strong);\n+  __ bc_far_optimized(Assembler::bcondCRbiIs0, __ bi0(CCR0, Assembler::equal), *stub->entry());\n+  z_uncolor(ce, ref);\n+  __ bind(*stub->continuation());\n@@ -335,2 +704,2 @@\n-  __ std(ref, (intptr_t) -1 * BytesPerWord, R1_SP);\n-  __ std(ref_addr, (intptr_t) -2 * BytesPerWord, R1_SP);\n+  __ std(ref, -1 * BytesPerWord, R1_SP);\n+  __ std(ref_addr, -2 * BytesPerWord, R1_SP);\n@@ -338,1 +707,1 @@\n-  __ load_const_optimized(R0, stub->runtime_stub());\n+  __ load_const_optimized(R0, stub->runtime_stub(), \/* temp *\/ ref);\n@@ -342,1 +711,1 @@\n-  __ mr_if_needed(ref, R0);\n+  __ mr(ref, R0);\n@@ -348,0 +717,58 @@\n+void ZBarrierSetAssembler::generate_c1_store_barrier(LIR_Assembler* ce,\n+                                                     LIR_Address* addr,\n+                                                     LIR_Opr new_zaddress,\n+                                                     LIR_Opr new_zpointer,\n+                                                     ZStoreBarrierStubC1* stub) const {\n+  Register rnew_zaddress = new_zaddress->as_register();\n+  Register rnew_zpointer = new_zpointer->as_register();\n+\n+  Register rbase = addr->base()->as_pointer_register();\n+  RegisterOrConstant ind_or_offs = (addr->index()->is_illegal())\n+                                 ? (RegisterOrConstant)addr->disp()\n+                                 : (RegisterOrConstant)addr->index()->as_pointer_register();\n+\n+  store_barrier_fast(ce->masm(),\n+                     rbase,\n+                     ind_or_offs,\n+                     rnew_zaddress,\n+                     rnew_zpointer,\n+                     true,\n+                     stub->is_atomic(),\n+                     *stub->entry(),\n+                     *stub->continuation());\n+}\n+\n+void ZBarrierSetAssembler::generate_c1_store_barrier_stub(LIR_Assembler* ce,\n+                                                          ZStoreBarrierStubC1* stub) const {\n+  \/\/ Stub entry\n+  __ bind(*stub->entry());\n+\n+  Label slow;\n+\n+  LIR_Address* addr = stub->ref_addr()->as_address_ptr();\n+  assert(addr->index()->is_illegal() || addr->disp() == 0, \"can't have both\");\n+  Register rbase = addr->base()->as_pointer_register();\n+  RegisterOrConstant ind_or_offs = (addr->index()->is_illegal())\n+                                 ? (RegisterOrConstant)addr->disp()\n+                                 : (RegisterOrConstant)addr->index()->as_pointer_register();\n+  Register new_zpointer = stub->new_zpointer()->as_register();\n+\n+  store_barrier_medium(ce->masm(),\n+                       rbase,\n+                       ind_or_offs,\n+                       new_zpointer, \/\/ temp\n+                       stub->is_atomic(),\n+                       *stub->continuation(),\n+                       slow);\n+\n+  __ bind(slow);\n+\n+  __ load_const_optimized(\/*stub address*\/ new_zpointer, stub->runtime_stub(), R0);\n+  __ add(R0, ind_or_offs, rbase); \/\/ pass store address in R0\n+  __ mtctr(new_zpointer);\n+  __ bctrl();\n+\n+  \/\/ Stub exit\n+  __ b(*stub->continuation());\n+}\n+\n@@ -363,2 +790,2 @@\n-  __ ld(R3_ARG1, (intptr_t) -1 * BytesPerWord, R1_SP); \/\/ ref\n-  __ ld(R4_ARG2, (intptr_t) -2 * BytesPerWord, R1_SP); \/\/ ref_addr\n+  __ ld(R3_ARG1, -1 * BytesPerWord, R1_SP); \/\/ ref\n+  __ ld(R4_ARG2, -2 * BytesPerWord, R1_SP); \/\/ ref_addr\n@@ -382,0 +809,26 @@\n+void ZBarrierSetAssembler::generate_c1_store_barrier_runtime_stub(StubAssembler* sasm,\n+                                                                  bool self_healing) const {\n+  __ block_comment(\"c1_store_barrier_runtime_stub (zgc) {\");\n+\n+  const int nbytes_save = MacroAssembler::num_volatile_regs * BytesPerWord;\n+  __ save_volatile_gprs(R1_SP, -nbytes_save);\n+  __ mr(R3_ARG1, R0); \/\/ store address\n+\n+  __ save_LR_CR(R0);\n+  __ push_frame_reg_args(nbytes_save, R0);\n+\n+  if (self_healing) {\n+    __ call_VM_leaf(ZBarrierSetRuntime::store_barrier_on_oop_field_with_healing_addr());\n+  } else {\n+    __ call_VM_leaf(ZBarrierSetRuntime::store_barrier_on_oop_field_without_healing_addr());\n+  }\n+\n+  __ pop_frame();\n+  __ restore_LR_CR(R3_RET);\n+  __ restore_volatile_gprs(R1_SP, -nbytes_save);\n+\n+  __ blr();\n+\n+  __ block_comment(\"} c1_store_barrier_runtime_stub (zgc)\");\n+}\n+\n@@ -409,2 +862,2 @@\n-  ZSaveLiveRegisters(MacroAssembler *masm, ZLoadBarrierStubC2 *stub)\n-      : _masm(masm), _reg_mask(stub->live()), _result_reg(stub->ref()) {\n+  ZSaveLiveRegisters(MacroAssembler *masm, ZBarrierStubC2 *stub)\n+      : _masm(masm), _reg_mask(stub->live()), _result_reg(stub->result()) {\n@@ -562,0 +1015,1 @@\n+  Assembler::InlineSkippedInstructionsCounter skipped_counter(masm);\n@@ -584,0 +1038,42 @@\n+void ZBarrierSetAssembler::generate_c2_store_barrier_stub(MacroAssembler* masm, ZStoreBarrierStubC2* stub) const {\n+  Assembler::InlineSkippedInstructionsCounter skipped_counter(masm);\n+  __ block_comment(\"ZStoreBarrierStubC2\");\n+\n+  \/\/ Stub entry\n+  __ bind(*stub->entry());\n+\n+  Label slow;\n+\n+  Address addr = stub->ref_addr();\n+  Register rbase = addr.base();\n+  RegisterOrConstant ind_or_offs = (addr.index() == noreg)\n+                                 ? (RegisterOrConstant)addr.disp()\n+                                 : (RegisterOrConstant)addr.index();\n+\n+  if (!stub->is_native()) {\n+    store_barrier_medium(masm,\n+                         rbase,\n+                         ind_or_offs,\n+                         stub->new_zpointer(),\n+                         stub->is_atomic(),\n+                         *stub->continuation(),\n+                         slow);\n+  }\n+\n+  __ bind(slow);\n+  {\n+    ZSaveLiveRegisters save_live_registers(masm, stub);\n+    __ add(R3_ARG1, ind_or_offs, rbase);\n+    if (stub->is_native()) {\n+      __ call_VM_leaf(ZBarrierSetRuntime::store_barrier_on_native_oop_field_without_healing_addr(), R3_ARG1);\n+    } else if (stub->is_atomic()) {\n+      __ call_VM_leaf(ZBarrierSetRuntime::store_barrier_on_oop_field_with_healing_addr(), R3_ARG1);\n+    } else {\n+      __ call_VM_leaf(ZBarrierSetRuntime::store_barrier_on_oop_field_without_healing_addr(), R3_ARG1);\n+    }\n+  }\n+\n+  \/\/ Stub exit\n+  __ b(*stub->continuation());\n+}\n+\n@@ -586,0 +1082,32 @@\n+\n+static uint16_t patch_barrier_relocation_value(int format) {\n+  switch (format) {\n+  case ZBarrierRelocationFormatLoadBadMask:\n+    return (uint16_t)ZPointerLoadBadMask;\n+  case ZBarrierRelocationFormatMarkBadMask:\n+    return (uint16_t)ZPointerMarkBadMask;\n+  case ZBarrierRelocationFormatStoreGoodBits:\n+    return (uint16_t)ZPointerStoreGoodMask;\n+  case ZBarrierRelocationFormatStoreBadMask:\n+    return (uint16_t)ZPointerStoreBadMask;\n+  default:\n+    ShouldNotReachHere();\n+    return 0;\n+  }\n+}\n+\n+void ZBarrierSetAssembler::patch_barrier_relocation(address addr, int format) {\n+#ifdef ASSERT\n+  int inst = *(int*)addr;\n+  if (format == ZBarrierRelocationFormatStoreGoodBits) {\n+    assert(Assembler::is_li(inst) || Assembler::is_ori(inst) || Assembler::is_cmpli(inst),\n+           \"unexpected instruction 0x%04x\", inst);\n+    \/\/ Note: li uses sign extend, but these bits will get cleared by rldimi.\n+  } else {\n+    assert(Assembler::is_andi(inst), \"unexpected instruction 0x%04x\", inst);\n+  }\n+#endif\n+  \/\/ Patch the signed\/unsigned 16 bit immediate field of the instruction.\n+  *(uint16_t*)(addr BIG_ENDIAN_ONLY(+2)) = patch_barrier_relocation_value(format);\n+  ICache::ppc64_flush_icache_bytes(addr, BytesPerInstWord);\n+}\n","filename":"src\/hotspot\/cpu\/ppc\/gc\/z\/zBarrierSetAssembler_ppc.cpp","additions":674,"deletions":146,"binary":false,"changes":820,"status":"modified"},{"patch":"@@ -35,0 +35,2 @@\n+class CodeStub;\n+class LIR_Address;\n@@ -39,0 +41,1 @@\n+class ZStoreBarrierStubC1;\n@@ -42,0 +45,1 @@\n+class MachNode;\n@@ -44,0 +48,1 @@\n+class ZStoreBarrierStubC2;\n@@ -46,0 +51,5 @@\n+const int ZBarrierRelocationFormatLoadBadMask = 0;\n+const int ZBarrierRelocationFormatMarkBadMask = 1;\n+const int ZBarrierRelocationFormatStoreGoodBits = 2;\n+const int ZBarrierRelocationFormatStoreBadMask = 3;\n+\n@@ -53,1 +63,0 @@\n-#ifdef ASSERT\n@@ -58,1 +67,0 @@\n-#endif \/\/ ASSERT\n@@ -67,1 +75,1 @@\n-  virtual NMethodPatchingType nmethod_patching_type() { return NMethodPatchingType::conc_data_patch; }\n+  virtual void check_oop(MacroAssembler *masm, Register obj, const char* msg);\n@@ -69,3 +77,1 @@\n-#ifdef COMPILER1\n-  void generate_c1_load_barrier_test(LIR_Assembler* ce,\n-                                     LIR_Opr ref) const;\n+  virtual NMethodPatchingType nmethod_patching_type() { return NMethodPatchingType::conc_instruction_and_data_patch; }\n@@ -73,0 +79,1 @@\n+#ifdef COMPILER1\n@@ -78,0 +85,20 @@\n+\n+  void generate_c1_color(LIR_Assembler* ce, LIR_Opr ref) const;\n+  void generate_c1_uncolor(LIR_Assembler* ce, LIR_Opr ref) const;\n+\n+  void generate_c1_load_barrier(LIR_Assembler* ce,\n+                                LIR_Opr ref,\n+                                ZLoadBarrierStubC1* stub,\n+                                bool on_non_strong) const;\n+\n+  void generate_c1_store_barrier(LIR_Assembler* ce,\n+                                 LIR_Address* addr,\n+                                 LIR_Opr new_zaddress,\n+                                 LIR_Opr new_zpointer,\n+                                 ZStoreBarrierStubC1* stub) const;\n+\n+  void generate_c1_store_barrier_stub(LIR_Assembler* ce,\n+                                      ZStoreBarrierStubC1* stub) const;\n+\n+  void generate_c1_store_barrier_runtime_stub(StubAssembler* sasm,\n+                                              bool self_healing) const;\n@@ -84,0 +111,2 @@\n+\n+  void generate_c2_store_barrier_stub(MacroAssembler* masm, ZStoreBarrierStubC2* stub) const;\n@@ -85,0 +114,57 @@\n+\n+  void store_barrier_fast(MacroAssembler* masm,\n+                          Register ref_base,\n+                          RegisterOrConstant ind_or_offset,\n+                          Register rnew_persistent,\n+                          Register rnew_transient,\n+                          bool in_nmethod,\n+                          bool is_atomic,\n+                          Label& medium_path,\n+                          Label& medium_path_continuation) const;\n+\n+  void store_barrier_medium(MacroAssembler* masm,\n+                            Register ref_base,\n+                            RegisterOrConstant ind_or_offs,\n+                            Register tmp,\n+                            bool is_atomic,\n+                            Label& medium_path_continuation,\n+                            Label& slow_path) const;\n+\n+  void load_copy_masks(MacroAssembler* masm,\n+                       Register load_bad_mask,\n+                       Register store_bad_mask,\n+                       Register store_good_mask,\n+                       bool dest_uninitialized) const;\n+  void copy_load_at_fast(MacroAssembler* masm,\n+                         Register zpointer,\n+                         Register addr,\n+                         Register load_bad_mask,\n+                         Label& slow_path,\n+                         Label& continuation) const;\n+  void copy_load_at_slow(MacroAssembler* masm,\n+                         Register zpointer,\n+                         Register addr,\n+                         Register tmp,\n+                         Label& slow_path,\n+                         Label& continuation) const;\n+  void copy_store_at_fast(MacroAssembler* masm,\n+                          Register zpointer,\n+                          Register addr,\n+                          Register store_bad_mask,\n+                          Register store_good_mask,\n+                          Label& medium_path,\n+                          Label& continuation,\n+                          bool dest_uninitialized) const;\n+  void copy_store_at_slow(MacroAssembler* masm,\n+                          Register addr,\n+                          Register tmp,\n+                          Label& medium_path,\n+                          Label& continuation,\n+                          bool dest_uninitialized) const;\n+\n+  void generate_disjoint_oop_copy(MacroAssembler* masm, bool dest_uninitialized);\n+  void generate_conjoint_oop_copy(MacroAssembler* masm, bool dest_uninitialized);\n+\n+  void patch_barrier_relocation(address addr, int format);\n+\n+  void patch_barriers() {}\n@@ -87,1 +173,1 @@\n-#endif \/\/ CPU_AARCH64_GC_Z_ZBARRIERSETASSEMBLER_AARCH64_HPP\n+#endif \/\/ CPU_PPC_GC_Z_ZBARRIERSETASSEMBLER_PPC_HPP\n","filename":"src\/hotspot\/cpu\/ppc\/gc\/z\/zBarrierSetAssembler_ppc.hpp","additions":93,"deletions":7,"binary":false,"changes":100,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2021, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2021, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -29,2 +29,0 @@\n-const size_t ZPlatformHeapViews        = 3;\n-const size_t ZPlatformCacheLineSize    = DEFAULT_CACHE_LINE_SIZE;\n@@ -32,2 +30,1 @@\n-size_t ZPlatformAddressOffsetBits();\n-size_t ZPlatformAddressMetadataShift();\n+const size_t ZPlatformCacheLineSize    = DEFAULT_CACHE_LINE_SIZE;\n","filename":"src\/hotspot\/cpu\/ppc\/gc\/z\/zGlobals_ppc.hpp","additions":2,"deletions":5,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n-\/\/ Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+\/\/ Copyright (c) 2021, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -35,5 +35,1 @@\n-static void z_load_barrier(MacroAssembler& _masm, const MachNode* node, Address ref_addr, Register ref,\n-                           Register tmp, uint8_t barrier_data) {\n-  if (barrier_data == ZLoadBarrierElided) {\n-    return;\n-  }\n+#include \"gc\/z\/zBarrierSetAssembler.hpp\"\n@@ -41,5 +37,11 @@\n-  ZLoadBarrierStubC2* const stub = ZLoadBarrierStubC2::create(node, ref_addr, ref, tmp, barrier_data);\n-  __ ld(tmp, in_bytes(ZThreadLocalData::address_bad_mask_offset()), R16_thread);\n-  __ and_(tmp, tmp, ref);\n-  __ bne_far(CCR0, *stub->entry(), MacroAssembler::bc_far_optimize_on_relocate);\n-  __ bind(*stub->continuation());\n+static void z_color(MacroAssembler& _masm, Register dst, Register src) {\n+  assert_different_registers(dst, src);\n+  __ relocate(barrier_Relocation::spec(), ZBarrierRelocationFormatStoreGoodBits);\n+  __ li(dst, barrier_Relocation::unpatched); \/\/ Load color bits.\n+  if (src == noreg) { \/\/ noreg encodes null.\n+    if (ZPointerLoadShift >= 16) {\n+      __ rldicl(dst, dst, 0, 64 - ZPointerLoadShift); \/\/ Clear sign extension from li.\n+    }\n+  } else {\n+    __ rldimi(dst, src, ZPointerLoadShift, 0); \/\/ Insert shifted pointer.\n+  }\n@@ -48,5 +50,2 @@\n-static void z_load_barrier_slow_path(MacroAssembler& _masm, const MachNode* node, Address ref_addr, Register ref,\n-                                     Register tmp) {\n-  ZLoadBarrierStubC2* const stub = ZLoadBarrierStubC2::create(node, ref_addr, ref, tmp, ZLoadBarrierStrong);\n-  __ b(*stub->entry());\n-  __ bind(*stub->continuation());\n+static void z_uncolor(MacroAssembler& _masm, Register ref) {\n+  __ srdi(ref, ref, ZPointerLoadShift);\n@@ -55,9 +54,6 @@\n-static void z_compare_and_swap(MacroAssembler& _masm, const MachNode* node,\n-                              Register res, Register mem, Register oldval, Register newval,\n-                              Register tmp_xchg, Register tmp_mask,\n-                              bool weak, bool acquire) {\n-  \/\/ z-specific load barrier requires strong CAS operations.\n-  \/\/ Weak CAS operations are thus only emitted if the barrier is elided.\n-  __ cmpxchgd(CCR0, tmp_xchg, oldval, newval, mem,\n-              MacroAssembler::MemBarNone, MacroAssembler::cmpxchgx_hint_atomic_update(), res, NULL, true,\n-              weak && node->barrier_data() == ZLoadBarrierElided);\n+static void check_color(MacroAssembler& _masm, Register ref, bool on_non_strong) {\n+  int relocFormat = on_non_strong ? ZBarrierRelocationFormatMarkBadMask\n+                                  : ZBarrierRelocationFormatLoadBadMask;\n+  __ relocate(barrier_Relocation::spec(), relocFormat);\n+  __ andi_(R0, ref, barrier_Relocation::unpatched);\n+}\n@@ -65,2 +61,8 @@\n-  if (node->barrier_data() != ZLoadBarrierElided) {\n-    Label skip_barrier;\n+static void z_load_barrier(MacroAssembler& _masm, const MachNode* node, Address ref_addr, Register ref) {\n+  Assembler::InlineSkippedInstructionsCounter skipped_counter(&_masm);\n+  if (node->barrier_data() == ZBarrierElided) {\n+    z_uncolor(_masm, ref);\n+  } else {\n+    const bool on_non_strong =\n+      ((node->barrier_data() & ZBarrierWeak) != 0) ||\n+      ((node->barrier_data() & ZBarrierPhantom) != 0);\n@@ -68,3 +70,1 @@\n-    __ ld(tmp_mask, in_bytes(ZThreadLocalData::address_bad_mask_offset()), R16_thread);\n-    __ and_(tmp_mask, tmp_mask, tmp_xchg);\n-    __ beq(CCR0, skip_barrier);\n+    check_color(_masm, ref, on_non_strong);\n@@ -72,2 +72,2 @@\n-    \/\/ CAS must have failed because pointer in memory is bad.\n-    z_load_barrier_slow_path(_masm, node, Address(mem), tmp_xchg, res \/* used as tmp *\/);\n+    ZLoadBarrierStubC2* const stub = ZLoadBarrierStubC2::create(node, ref_addr, ref);\n+    __ bne_far(CCR0, *stub->entry(), MacroAssembler::bc_far_optimize_on_relocate);\n@@ -75,2 +75,4 @@\n-    __ cmpxchgd(CCR0, tmp_xchg, oldval, newval, mem,\n-                MacroAssembler::MemBarNone, MacroAssembler::cmpxchgx_hint_atomic_update(), res, NULL, true, weak);\n+    z_uncolor(_masm, ref);\n+    __ bind(*stub->continuation());\n+  }\n+}\n@@ -78,1 +80,9 @@\n-    __ bind(skip_barrier);\n+static void z_store_barrier(MacroAssembler& _masm, const MachNode* node, Register ref_base, intptr_t disp, Register rnew_zaddress, Register rnew_zpointer, bool is_atomic) {\n+  Assembler::InlineSkippedInstructionsCounter skipped_counter(&_masm);\n+  if (node->barrier_data() == ZBarrierElided) {\n+    z_color(_masm, rnew_zpointer, rnew_zaddress);\n+  } else {\n+    bool is_native = (node->barrier_data() & ZBarrierNative) != 0;\n+    ZStoreBarrierStubC2* const stub = ZStoreBarrierStubC2::create(node, Address(ref_base, disp), rnew_zaddress, rnew_zpointer, is_native, is_atomic);\n+    ZBarrierSetAssembler* bs_asm = ZBarrierSet::assembler();\n+    bs_asm->store_barrier_fast(&_masm, ref_base, disp, rnew_zaddress, rnew_zpointer, true \/* in_nmethod *\/, is_atomic, *stub->entry(), *stub->continuation());\n@@ -80,0 +90,12 @@\n+}\n+\n+static void z_compare_and_swap(MacroAssembler& _masm, const MachNode* node,\n+                              Register res, Register mem, Register oldval, Register newval,\n+                              Register tmp1, Register tmp2, bool acquire) {\n+\n+  Register rold_zpointer = tmp1, rnew_zpointer = tmp2;\n+  z_store_barrier(_masm, node, mem, 0, newval, rnew_zpointer, true \/* is_atomic *\/);\n+  z_color(_masm, rold_zpointer, oldval);\n+  __ cmpxchgd(CCR0, R0, rold_zpointer, rnew_zpointer, mem,\n+              MacroAssembler::MemBarNone, MacroAssembler::cmpxchgx_hint_atomic_update(), res, nullptr, true,\n+              false \/* we could support weak, but benefit is questionable *\/);\n@@ -93,21 +115,10 @@\n-                                   Register res, Register mem, Register oldval, Register newval, Register tmp,\n-                                   bool weak, bool acquire) {\n-  \/\/ z-specific load barrier requires strong CAS operations.\n-  \/\/ Weak CAS operations are thus only emitted if the barrier is elided.\n-  __ cmpxchgd(CCR0, res, oldval, newval, mem,\n-              MacroAssembler::MemBarNone, MacroAssembler::cmpxchgx_hint_atomic_update(), noreg, NULL, true,\n-              weak && node->barrier_data() == ZLoadBarrierElided);\n-\n-  if (node->barrier_data() != ZLoadBarrierElided) {\n-    Label skip_barrier;\n-    __ ld(tmp, in_bytes(ZThreadLocalData::address_bad_mask_offset()), R16_thread);\n-    __ and_(tmp, tmp, res);\n-    __ beq(CCR0, skip_barrier);\n-\n-    z_load_barrier_slow_path(_masm, node, Address(mem), res, tmp);\n-\n-    __ cmpxchgd(CCR0, res, oldval, newval, mem,\n-                MacroAssembler::MemBarNone, MacroAssembler::cmpxchgx_hint_atomic_update(), noreg, NULL, true, weak);\n-\n-    __ bind(skip_barrier);\n-  }\n+                                   Register res, Register mem, Register oldval, Register newval,\n+                                   Register tmp, bool acquire) {\n+\n+  Register rold_zpointer = R0, rnew_zpointer = tmp;\n+  z_store_barrier(_masm, node, mem, 0, newval, rnew_zpointer, true \/* is_atomic *\/);\n+  z_color(_masm, rold_zpointer, oldval);\n+  __ cmpxchgd(CCR0, res, rold_zpointer, rnew_zpointer, mem,\n+              MacroAssembler::MemBarNone, MacroAssembler::cmpxchgx_hint_atomic_update(), noreg, nullptr, true,\n+              false \/* we could support weak, but benefit is questionable *\/);\n+  z_uncolor(_masm, res);\n@@ -128,1 +139,1 @@\n-instruct zLoadP(iRegPdst dst, memoryAlg4 mem, iRegPdst tmp, flagsRegCR0 cr0)\n+instruct zLoadP(iRegPdst dst, memoryAlg4 mem, flagsRegCR0 cr0)\n@@ -131,1 +142,1 @@\n-  effect(TEMP_DEF dst, TEMP tmp, KILL cr0);\n+  effect(TEMP_DEF dst, KILL cr0);\n@@ -134,1 +145,1 @@\n-  predicate((UseZGC && n->as_Load()->barrier_data() != 0)\n+  predicate((UseZGC && ZGenerational && n->as_Load()->barrier_data() != 0)\n@@ -141,1 +152,1 @@\n-    z_load_barrier(_masm, this, Address($mem$$base$$Register, $mem$$disp), $dst$$Register, $tmp$$Register, barrier_data());\n+    z_load_barrier(_masm, this, Address($mem$$base$$Register, $mem$$disp), $dst$$Register);\n@@ -147,1 +158,1 @@\n-instruct zLoadP_acq(iRegPdst dst, memoryAlg4 mem, iRegPdst tmp, flagsRegCR0 cr0)\n+instruct zLoadP_acq(iRegPdst dst, memoryAlg4 mem, flagsRegCR0 cr0)\n@@ -150,1 +161,1 @@\n-  effect(TEMP_DEF dst, TEMP tmp, KILL cr0);\n+  effect(TEMP_DEF dst, KILL cr0);\n@@ -154,1 +165,1 @@\n-  predicate(UseZGC && n->as_Load()->barrier_data() != 0);\n+  predicate(UseZGC && ZGenerational && n->as_Load()->barrier_data() != 0);\n@@ -159,1 +170,1 @@\n-    z_load_barrier(_masm, this, Address($mem$$base$$Register, $mem$$disp), $dst$$Register, $tmp$$Register, barrier_data());\n+    z_load_barrier(_masm, this, Address($mem$$base$$Register, $mem$$disp), $dst$$Register);\n@@ -163,0 +174,1 @@\n+    if (barrier_data() == ZBarrierElided) __ twi_0($dst$$Register);\n@@ -168,9 +180,8 @@\n-instruct zCompareAndSwapP(iRegIdst res, iRegPdst mem, iRegPsrc oldval, iRegPsrc newval,\n-                          iRegPdst tmp_xchg, iRegPdst tmp_mask, flagsRegCR0 cr0) %{\n-  match(Set res (CompareAndSwapP mem (Binary oldval newval)));\n-  effect(TEMP_DEF res, TEMP tmp_xchg, TEMP tmp_mask, KILL cr0);\n-\n-  predicate((UseZGC && n->as_LoadStore()->barrier_data() == ZLoadBarrierStrong)\n-            && (((CompareAndSwapNode*)n)->order() != MemNode::acquire && ((CompareAndSwapNode*) n)->order() != MemNode::seqcst));\n-\n-  format %{ \"CMPXCHG $res, $mem, $oldval, $newval; as bool; ptr\" %}\n+\/\/ Store Pointer\n+instruct zStoreP(memoryAlg4 mem, iRegPsrc src, iRegPdst tmp, flagsRegCR0 cr0)\n+%{\n+  predicate(UseZGC && ZGenerational && n->as_Store()->barrier_data() != 0);\n+  match(Set mem (StoreP mem src));\n+  effect(TEMP tmp, KILL cr0);\n+  ins_cost(2 * MEMORY_REF_COST);\n+  format %{ \"std    $mem, $src\\t# ptr\" %}\n@@ -178,4 +189,2 @@\n-    z_compare_and_swap(_masm, this,\n-                       $res$$Register, $mem$$Register, $oldval$$Register, $newval$$Register,\n-                       $tmp_xchg$$Register, $tmp_mask$$Register,\n-                       false \/* weak *\/, false \/* acquire *\/);\n+    z_store_barrier(_masm, this, $mem$$base$$Register, $mem$$disp, $src$$Register, $tmp$$Register, false \/* is_atomic *\/);\n+    __ std($tmp$$Register, $mem$$disp, $mem$$base$$Register);\n@@ -186,9 +195,7 @@\n-instruct zCompareAndSwapP_acq(iRegIdst res, iRegPdst mem, iRegPsrc oldval, iRegPsrc newval,\n-                              iRegPdst tmp_xchg, iRegPdst tmp_mask, flagsRegCR0 cr0) %{\n-  match(Set res (CompareAndSwapP mem (Binary oldval newval)));\n-  effect(TEMP_DEF res, TEMP tmp_xchg, TEMP tmp_mask, KILL cr0);\n-\n-  predicate((UseZGC && n->as_LoadStore()->barrier_data() == ZLoadBarrierStrong)\n-            && (((CompareAndSwapNode*)n)->order() == MemNode::acquire || ((CompareAndSwapNode*) n)->order() == MemNode::seqcst));\n-\n-  format %{ \"CMPXCHG acq $res, $mem, $oldval, $newval; as bool; ptr\" %}\n+instruct zStorePNull(memoryAlg4 mem, immP_0 zero, iRegPdst tmp, flagsRegCR0 cr0)\n+%{\n+  predicate(UseZGC && ZGenerational && n->as_Store()->barrier_data() != 0);\n+  match(Set mem (StoreP mem zero));\n+  effect(TEMP tmp, KILL cr0);\n+  ins_cost(MEMORY_REF_COST);\n+  format %{ \"std    $mem, null\\t# ptr\" %}\n@@ -196,4 +203,2 @@\n-    z_compare_and_swap(_masm, this,\n-                       $res$$Register, $mem$$Register, $oldval$$Register, $newval$$Register,\n-                       $tmp_xchg$$Register, $tmp_mask$$Register,\n-                       false \/* weak *\/, true \/* acquire *\/);\n+    z_store_barrier(_masm, this, $mem$$base$$Register, $mem$$disp, noreg, $tmp$$Register, false \/* is_atomic *\/);\n+    __ std($tmp$$Register, $mem$$disp, $mem$$base$$Register);\n@@ -204,2 +209,3 @@\n-instruct zCompareAndSwapPWeak(iRegIdst res, iRegPdst mem, iRegPsrc oldval, iRegPsrc newval,\n-                              iRegPdst tmp_xchg, iRegPdst tmp_mask, flagsRegCR0 cr0) %{\n+instruct zCompareAndSwapP(iRegIdst res, iRegPdst mem, iRegPsrc oldval, iRegPsrc newval,\n+                          iRegPdst tmp1, iRegPdst tmp2, flagsRegCR0 cr0) %{\n+  match(Set res (CompareAndSwapP mem (Binary oldval newval)));\n@@ -207,1 +213,1 @@\n-  effect(TEMP_DEF res, TEMP tmp_xchg, TEMP tmp_mask, KILL cr0);\n+  effect(TEMP_DEF res, TEMP tmp1, TEMP tmp2, KILL cr0);\n@@ -209,2 +215,2 @@\n-  predicate((UseZGC && n->as_LoadStore()->barrier_data() == ZLoadBarrierStrong)\n-            && ((CompareAndSwapNode*)n)->order() != MemNode::acquire && ((CompareAndSwapNode*) n)->order() != MemNode::seqcst);\n+  predicate((UseZGC && ZGenerational && n->as_LoadStore()->barrier_data() != 0)\n+            && (((CompareAndSwapNode*)n)->order() != MemNode::acquire && ((CompareAndSwapNode*) n)->order() != MemNode::seqcst));\n@@ -212,1 +218,1 @@\n-  format %{ \"weak CMPXCHG $res, $mem, $oldval, $newval; as bool; ptr\" %}\n+  format %{ \"CMPXCHG $res, $mem, $oldval, $newval; as bool; ptr\" %}\n@@ -216,2 +222,2 @@\n-                       $tmp_xchg$$Register, $tmp_mask$$Register,\n-                       true \/* weak *\/, false \/* acquire *\/);\n+                       $tmp1$$Register, $tmp2$$Register,\n+                       false \/* acquire *\/);\n@@ -222,2 +228,3 @@\n-instruct zCompareAndSwapPWeak_acq(iRegIdst res, iRegPdst mem, iRegPsrc oldval, iRegPsrc newval,\n-                                  iRegPdst tmp_xchg, iRegPdst tmp_mask, flagsRegCR0 cr0) %{\n+instruct zCompareAndSwapP_acq(iRegIdst res, iRegPdst mem, iRegPsrc oldval, iRegPsrc newval,\n+                              iRegPdst tmp1, iRegPdst tmp2, flagsRegCR0 cr0) %{\n+  match(Set res (CompareAndSwapP mem (Binary oldval newval)));\n@@ -225,1 +232,1 @@\n-  effect(TEMP_DEF res, TEMP tmp_xchg, TEMP tmp_mask, KILL cr0);\n+  effect(TEMP_DEF res, TEMP tmp1, TEMP tmp2, KILL cr0);\n@@ -227,1 +234,1 @@\n-  predicate((UseZGC && n->as_LoadStore()->barrier_data() == ZLoadBarrierStrong)\n+  predicate((UseZGC && ZGenerational && n->as_LoadStore()->barrier_data() != 0)\n@@ -230,1 +237,1 @@\n-  format %{ \"weak CMPXCHG acq $res, $mem, $oldval, $newval; as bool; ptr\" %}\n+  format %{ \"CMPXCHG acq $res, $mem, $oldval, $newval; as bool; ptr\" %}\n@@ -234,2 +241,2 @@\n-                       $tmp_xchg$$Register, $tmp_mask$$Register,\n-                       true \/* weak *\/, true \/* acquire *\/);\n+                       $tmp1$$Register, $tmp2$$Register,\n+                       true \/* acquire *\/);\n@@ -245,1 +252,1 @@\n-  predicate((UseZGC && n->as_LoadStore()->barrier_data() == ZLoadBarrierStrong)\n+  predicate((UseZGC && ZGenerational && n->as_LoadStore()->barrier_data() != 0)\n@@ -255,1 +262,1 @@\n-                           false \/* weak *\/, false \/* acquire *\/);\n+                           false \/* acquire *\/);\n@@ -265,1 +272,1 @@\n-  predicate((UseZGC && n->as_LoadStore()->barrier_data() == ZLoadBarrierStrong)\n+  predicate((UseZGC && ZGenerational && n->as_LoadStore()->barrier_data() != 0)\n@@ -275,1 +282,1 @@\n-                           false \/* weak *\/, true \/* acquire *\/);\n+                           true \/* acquire *\/);\n@@ -284,1 +291,1 @@\n-  predicate(UseZGC && n->as_LoadStore()->barrier_data() != 0);\n+  predicate(UseZGC && ZGenerational && n->as_LoadStore()->barrier_data() != 0);\n@@ -288,2 +295,4 @@\n-    __ getandsetd($res$$Register, $newval$$Register, $mem$$Register, MacroAssembler::cmpxchgx_hint_atomic_update());\n-    z_load_barrier(_masm, this, Address(noreg, (intptr_t) 0), $res$$Register, $tmp$$Register, barrier_data());\n+    Register rnew_zpointer = $tmp$$Register, result = $res$$Register;\n+    z_store_barrier(_masm, this, $mem$$Register, 0, $newval$$Register, rnew_zpointer, true \/* is_atomic *\/);\n+    __ getandsetd(result, rnew_zpointer, $mem$$Register, MacroAssembler::cmpxchgx_hint_atomic_update());\n+    z_uncolor(_masm, result);\n","filename":"src\/hotspot\/cpu\/ppc\/gc\/z\/z_ppc.ad","additions":121,"deletions":112,"binary":false,"changes":233,"status":"modified"},{"patch":"@@ -357,1 +357,1 @@\n-                         ON_UNKNOWN_OOP_REF)) == 0, \"unsupported decorator\");\n+                         ON_UNKNOWN_OOP_REF | IS_DEST_UNINITIALIZED)) == 0, \"unsupported decorator\");\n","filename":"src\/hotspot\/cpu\/ppc\/macroAssembler_ppc.inline.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -42,1 +42,2 @@\n-    format_width       =  1\n+    \/\/ Must be at least 2 for ZGC GC barrier patching.\n+    format_width       =  2\n","filename":"src\/hotspot\/cpu\/ppc\/relocInfo_ppc.hpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -50,0 +50,4 @@\n+#if INCLUDE_ZGC\n+#include \"gc\/x\/xBarrierSetAssembler.hpp\"\n+#include \"gc\/z\/zBarrierSetAssembler.hpp\"\n+#endif\n@@ -64,1 +68,1 @@\n-#define STUB_ENTRY(name) StubRoutines::name()\n+#define STUB_ENTRY(name) StubRoutines::name\n@@ -66,1 +70,1 @@\n-#define STUB_ENTRY(name) ((FunctionDescriptor*)StubRoutines::name())->entry()\n+#define STUB_ENTRY(name) ((FunctionDescriptor*)StubRoutines::name)->entry()\n@@ -1185,2 +1189,2 @@\n-      STUB_ENTRY(arrayof_jbyte_disjoint_arraycopy) :\n-      STUB_ENTRY(jbyte_disjoint_arraycopy);\n+      STUB_ENTRY(arrayof_jbyte_disjoint_arraycopy()) :\n+      STUB_ENTRY(jbyte_disjoint_arraycopy());\n@@ -1457,2 +1461,2 @@\n-      STUB_ENTRY(arrayof_jshort_disjoint_arraycopy) :\n-      STUB_ENTRY(jshort_disjoint_arraycopy);\n+      STUB_ENTRY(arrayof_jshort_disjoint_arraycopy()) :\n+      STUB_ENTRY(jshort_disjoint_arraycopy());\n@@ -1770,2 +1774,2 @@\n-      STUB_ENTRY(arrayof_jint_disjoint_arraycopy) :\n-      STUB_ENTRY(jint_disjoint_arraycopy);\n+      STUB_ENTRY(arrayof_jint_disjoint_arraycopy()) :\n+      STUB_ENTRY(jint_disjoint_arraycopy());\n@@ -2027,2 +2031,2 @@\n-      STUB_ENTRY(arrayof_jlong_disjoint_arraycopy) :\n-      STUB_ENTRY(jlong_disjoint_arraycopy);\n+      STUB_ENTRY(arrayof_jlong_disjoint_arraycopy()) :\n+      STUB_ENTRY(jlong_disjoint_arraycopy());\n@@ -2057,2 +2061,4 @@\n-      STUB_ENTRY(arrayof_oop_disjoint_arraycopy) :\n-      STUB_ENTRY(oop_disjoint_arraycopy);\n+      STUB_ENTRY(arrayof_oop_disjoint_arraycopy(dest_uninitialized)) :\n+      STUB_ENTRY(oop_disjoint_arraycopy(dest_uninitialized));\n+\n+    array_overlap_test(nooverlap_target, UseCompressedOops ? 2 : 3);\n@@ -2072,1 +2078,0 @@\n-      array_overlap_test(nooverlap_target, 2);\n@@ -2075,1 +2080,6 @@\n-      array_overlap_test(nooverlap_target, 3);\n+#if INCLUDE_ZGC\n+      if (UseZGC && ZGenerational) {\n+        ZBarrierSetAssembler *zbs = (ZBarrierSetAssembler*)bs;\n+        zbs->generate_conjoint_oop_copy(_masm, dest_uninitialized);\n+      } else\n+#endif\n@@ -2113,0 +2123,6 @@\n+#if INCLUDE_ZGC\n+      if (UseZGC && ZGenerational) {\n+        ZBarrierSetAssembler *zbs = (ZBarrierSetAssembler*)bs;\n+        zbs->generate_disjoint_oop_copy(_masm, dest_uninitialized);\n+      } else\n+#endif\n@@ -2225,0 +2241,7 @@\n+#if INCLUDE_ZGC\n+      if (UseZGC && ZGenerational) {\n+        __ store_heap_oop(R10_oop, R8_offset, R4_to, R11_scratch1, R12_tmp, noreg,\n+                          MacroAssembler::PRESERVATION_FRAME_LR_GP_REGS,\n+                          dest_uninitialized ? IS_DEST_UNINITIALIZED : 0);\n+      } else\n+#endif\n@@ -2234,0 +2257,8 @@\n+#if INCLUDE_ZGC\n+    if (UseZGC && ZGenerational) {\n+      __ load_heap_oop(R10_oop, R8_offset, R3_from,\n+                       R11_scratch1, R12_tmp,\n+                       MacroAssembler::PRESERVATION_FRAME_LR_GP_REGS,\n+                       0, &store_null);\n+    } else\n+#endif\n@@ -3139,4 +3170,4 @@\n-                                                            STUB_ENTRY(jbyte_arraycopy),\n-                                                            STUB_ENTRY(jshort_arraycopy),\n-                                                            STUB_ENTRY(jint_arraycopy),\n-                                                            STUB_ENTRY(jlong_arraycopy));\n+                                                            STUB_ENTRY(jbyte_arraycopy()),\n+                                                            STUB_ENTRY(jshort_arraycopy()),\n+                                                            STUB_ENTRY(jint_arraycopy()),\n+                                                            STUB_ENTRY(jlong_arraycopy()));\n@@ -3144,7 +3175,7 @@\n-                                                             STUB_ENTRY(jbyte_arraycopy),\n-                                                             STUB_ENTRY(jshort_arraycopy),\n-                                                             STUB_ENTRY(jint_arraycopy),\n-                                                             STUB_ENTRY(oop_arraycopy),\n-                                                             STUB_ENTRY(oop_disjoint_arraycopy),\n-                                                             STUB_ENTRY(jlong_arraycopy),\n-                                                             STUB_ENTRY(checkcast_arraycopy));\n+                                                             STUB_ENTRY(jbyte_arraycopy()),\n+                                                             STUB_ENTRY(jshort_arraycopy()),\n+                                                             STUB_ENTRY(jint_arraycopy()),\n+                                                             STUB_ENTRY(oop_arraycopy()),\n+                                                             STUB_ENTRY(oop_disjoint_arraycopy()),\n+                                                             STUB_ENTRY(jlong_arraycopy()),\n+                                                             STUB_ENTRY(checkcast_arraycopy()));\n","filename":"src\/hotspot\/cpu\/ppc\/stubGenerator_ppc.cpp","additions":56,"deletions":25,"binary":false,"changes":81,"status":"modified"},{"patch":"@@ -861,1 +861,1 @@\n-    if (!UseZGC) {\n+    if (!(UseZGC && !ZGenerational)) {\n@@ -1267,0 +1267,3 @@\n+      Register tmp2 = op->tmp2()->as_register();\n+      assert(op->tmp2()->is_valid(), \"must be\");\n+\n@@ -1269,2 +1272,2 @@\n-      __ encode_heap_oop(t1, newval);\n-      newval = t1;\n+      __ encode_heap_oop(tmp2, newval);\n+      newval = tmp2;\n@@ -1280,0 +1283,5 @@\n+\n+  if (op->result_opr()->is_valid()) {\n+    assert(op->result_opr()->is_register(), \"need a register\");\n+    __ mv(as_reg(op->result_opr()), t0); \/\/ cas result in t0, and 0 for success\n+  }\n","filename":"src\/hotspot\/cpu\/riscv\/c1_LIRAssembler_riscv.cpp","additions":11,"deletions":3,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -0,0 +1,458 @@\n+\/*\n+ * Copyright (c) 2019, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"asm\/macroAssembler.inline.hpp\"\n+#include \"code\/codeBlob.hpp\"\n+#include \"code\/vmreg.inline.hpp\"\n+#include \"gc\/x\/xBarrier.inline.hpp\"\n+#include \"gc\/x\/xBarrierSet.hpp\"\n+#include \"gc\/x\/xBarrierSetAssembler.hpp\"\n+#include \"gc\/x\/xBarrierSetRuntime.hpp\"\n+#include \"gc\/x\/xThreadLocalData.hpp\"\n+#include \"memory\/resourceArea.hpp\"\n+#include \"runtime\/sharedRuntime.hpp\"\n+#include \"utilities\/macros.hpp\"\n+#ifdef COMPILER1\n+#include \"c1\/c1_LIRAssembler.hpp\"\n+#include \"c1\/c1_MacroAssembler.hpp\"\n+#include \"gc\/x\/c1\/xBarrierSetC1.hpp\"\n+#endif \/\/ COMPILER1\n+#ifdef COMPILER2\n+#include \"gc\/x\/c2\/xBarrierSetC2.hpp\"\n+#endif \/\/ COMPILER2\n+\n+#ifdef PRODUCT\n+#define BLOCK_COMMENT(str) \/* nothing *\/\n+#else\n+#define BLOCK_COMMENT(str) __ block_comment(str)\n+#endif\n+\n+#undef __\n+#define __ masm->\n+\n+void XBarrierSetAssembler::load_at(MacroAssembler* masm,\n+                                   DecoratorSet decorators,\n+                                   BasicType type,\n+                                   Register dst,\n+                                   Address src,\n+                                   Register tmp1,\n+                                   Register tmp2) {\n+  if (!XBarrierSet::barrier_needed(decorators, type)) {\n+    \/\/ Barrier not needed\n+    BarrierSetAssembler::load_at(masm, decorators, type, dst, src, tmp1, tmp2);\n+    return;\n+  }\n+\n+  assert_different_registers(t1, src.base());\n+  assert_different_registers(t0, t1, dst);\n+\n+  Label done;\n+\n+  \/\/ Load bad mask into temp register.\n+  __ la(t0, src);\n+  __ ld(t1, address_bad_mask_from_thread(xthread));\n+  __ ld(dst, Address(t0));\n+\n+  \/\/ Test reference against bad mask. If mask bad, then we need to fix it up.\n+  __ andr(t1, dst, t1);\n+  __ beqz(t1, done);\n+\n+  __ enter();\n+\n+  __ push_call_clobbered_registers_except(RegSet::of(dst));\n+\n+  if (c_rarg0 != dst) {\n+    __ mv(c_rarg0, dst);\n+  }\n+\n+  __ mv(c_rarg1, t0);\n+\n+  __ call_VM_leaf(XBarrierSetRuntime::load_barrier_on_oop_field_preloaded_addr(decorators), 2);\n+\n+  \/\/ Make sure dst has the return value.\n+  if (dst != x10) {\n+    __ mv(dst, x10);\n+  }\n+\n+  __ pop_call_clobbered_registers_except(RegSet::of(dst));\n+  __ leave();\n+\n+  __ bind(done);\n+}\n+\n+#ifdef ASSERT\n+\n+void XBarrierSetAssembler::store_at(MacroAssembler* masm,\n+                                    DecoratorSet decorators,\n+                                    BasicType type,\n+                                    Address dst,\n+                                    Register val,\n+                                    Register tmp1,\n+                                    Register tmp2,\n+                                    Register tmp3) {\n+  \/\/ Verify value\n+  if (is_reference_type(type)) {\n+    \/\/ Note that src could be noreg, which means we\n+    \/\/ are storing null and can skip verification.\n+    if (val != noreg) {\n+      Label done;\n+\n+      \/\/ tmp1, tmp2 and tmp3 are often set to noreg.\n+      RegSet savedRegs = RegSet::of(t0);\n+      __ push_reg(savedRegs, sp);\n+\n+      __ ld(t0, address_bad_mask_from_thread(xthread));\n+      __ andr(t0, val, t0);\n+      __ beqz(t0, done);\n+      __ stop(\"Verify oop store failed\");\n+      __ should_not_reach_here();\n+      __ bind(done);\n+      __ pop_reg(savedRegs, sp);\n+    }\n+  }\n+\n+  \/\/ Store value\n+  BarrierSetAssembler::store_at(masm, decorators, type, dst, val, tmp1, tmp2, noreg);\n+}\n+\n+#endif \/\/ ASSERT\n+\n+void XBarrierSetAssembler::arraycopy_prologue(MacroAssembler* masm,\n+                                              DecoratorSet decorators,\n+                                              bool is_oop,\n+                                              Register src,\n+                                              Register dst,\n+                                              Register count,\n+                                              RegSet saved_regs) {\n+  if (!is_oop) {\n+    \/\/ Barrier not needed\n+    return;\n+  }\n+\n+  BLOCK_COMMENT(\"XBarrierSetAssembler::arraycopy_prologue {\");\n+\n+  assert_different_registers(src, count, t0);\n+\n+  __ push_reg(saved_regs, sp);\n+\n+  if (count == c_rarg0 && src == c_rarg1) {\n+    \/\/ exactly backwards!!\n+    __ xorr(c_rarg0, c_rarg0, c_rarg1);\n+    __ xorr(c_rarg1, c_rarg0, c_rarg1);\n+    __ xorr(c_rarg0, c_rarg0, c_rarg1);\n+  } else {\n+    __ mv(c_rarg0, src);\n+    __ mv(c_rarg1, count);\n+  }\n+\n+  __ call_VM_leaf(XBarrierSetRuntime::load_barrier_on_oop_array_addr(), 2);\n+\n+  __ pop_reg(saved_regs, sp);\n+\n+  BLOCK_COMMENT(\"} XBarrierSetAssembler::arraycopy_prologue\");\n+}\n+\n+void XBarrierSetAssembler::try_resolve_jobject_in_native(MacroAssembler* masm,\n+                                                         Register jni_env,\n+                                                         Register robj,\n+                                                         Register tmp,\n+                                                         Label& slowpath) {\n+  BLOCK_COMMENT(\"XBarrierSetAssembler::try_resolve_jobject_in_native {\");\n+\n+  assert_different_registers(jni_env, robj, tmp);\n+\n+  \/\/ Resolve jobject\n+  BarrierSetAssembler::try_resolve_jobject_in_native(masm, jni_env, robj, tmp, slowpath);\n+\n+  \/\/ Compute the offset of address bad mask from the field of jni_environment\n+  long int bad_mask_relative_offset = (long int) (in_bytes(XThreadLocalData::address_bad_mask_offset()) -\n+                                                  in_bytes(JavaThread::jni_environment_offset()));\n+\n+  \/\/ Load the address bad mask\n+  __ ld(tmp, Address(jni_env, bad_mask_relative_offset));\n+\n+  \/\/ Check address bad mask\n+  __ andr(tmp, robj, tmp);\n+  __ bnez(tmp, slowpath);\n+\n+  BLOCK_COMMENT(\"} XBarrierSetAssembler::try_resolve_jobject_in_native\");\n+}\n+\n+#ifdef COMPILER2\n+\n+OptoReg::Name XBarrierSetAssembler::refine_register(const Node* node, OptoReg::Name opto_reg) {\n+  if (!OptoReg::is_reg(opto_reg)) {\n+    return OptoReg::Bad;\n+  }\n+\n+  const VMReg vm_reg = OptoReg::as_VMReg(opto_reg);\n+  if (vm_reg->is_FloatRegister()) {\n+    return opto_reg & ~1;\n+  }\n+\n+  return opto_reg;\n+}\n+\n+#undef __\n+#define __ _masm->\n+\n+class XSaveLiveRegisters {\n+private:\n+  MacroAssembler* const _masm;\n+  RegSet                _gp_regs;\n+  FloatRegSet           _fp_regs;\n+  VectorRegSet          _vp_regs;\n+\n+public:\n+  void initialize(XLoadBarrierStubC2* stub) {\n+    \/\/ Record registers that needs to be saved\/restored\n+    RegMaskIterator rmi(stub->live());\n+    while (rmi.has_next()) {\n+      const OptoReg::Name opto_reg = rmi.next();\n+      if (OptoReg::is_reg(opto_reg)) {\n+        const VMReg vm_reg = OptoReg::as_VMReg(opto_reg);\n+        if (vm_reg->is_Register()) {\n+          _gp_regs += RegSet::of(vm_reg->as_Register());\n+        } else if (vm_reg->is_FloatRegister()) {\n+          _fp_regs += FloatRegSet::of(vm_reg->as_FloatRegister());\n+        } else if (vm_reg->is_VectorRegister()) {\n+          const VMReg vm_reg_base = OptoReg::as_VMReg(opto_reg & ~(VectorRegister::max_slots_per_register - 1));\n+          _vp_regs += VectorRegSet::of(vm_reg_base->as_VectorRegister());\n+        } else {\n+          fatal(\"Unknown register type\");\n+        }\n+      }\n+    }\n+\n+    \/\/ Remove C-ABI SOE registers, tmp regs and _ref register that will be updated\n+    _gp_regs -= RegSet::range(x18, x27) + RegSet::of(x2) + RegSet::of(x8, x9) + RegSet::of(x5, stub->ref());\n+  }\n+\n+  XSaveLiveRegisters(MacroAssembler* masm, XLoadBarrierStubC2* stub) :\n+      _masm(masm),\n+      _gp_regs(),\n+      _fp_regs(),\n+      _vp_regs() {\n+    \/\/ Figure out what registers to save\/restore\n+    initialize(stub);\n+\n+    \/\/ Save registers\n+    __ push_reg(_gp_regs, sp);\n+    __ push_fp(_fp_regs, sp);\n+    __ push_v(_vp_regs, sp);\n+  }\n+\n+  ~XSaveLiveRegisters() {\n+    \/\/ Restore registers\n+    __ pop_v(_vp_regs, sp);\n+    __ pop_fp(_fp_regs, sp);\n+    __ pop_reg(_gp_regs, sp);\n+  }\n+};\n+\n+class XSetupArguments {\n+private:\n+  MacroAssembler* const _masm;\n+  const Register        _ref;\n+  const Address         _ref_addr;\n+\n+public:\n+  XSetupArguments(MacroAssembler* masm, XLoadBarrierStubC2* stub) :\n+      _masm(masm),\n+      _ref(stub->ref()),\n+      _ref_addr(stub->ref_addr()) {\n+\n+    \/\/ Setup arguments\n+    if (_ref_addr.base() == noreg) {\n+      \/\/ No self healing\n+      if (_ref != c_rarg0) {\n+        __ mv(c_rarg0, _ref);\n+      }\n+      __ mv(c_rarg1, zr);\n+    } else {\n+      \/\/ Self healing\n+      if (_ref == c_rarg0) {\n+        \/\/ _ref is already at correct place\n+        __ la(c_rarg1, _ref_addr);\n+      } else if (_ref != c_rarg1) {\n+        \/\/ _ref is in wrong place, but not in c_rarg1, so fix it first\n+        __ la(c_rarg1, _ref_addr);\n+        __ mv(c_rarg0, _ref);\n+      } else if (_ref_addr.base() != c_rarg0) {\n+        assert(_ref == c_rarg1, \"Mov ref first, vacating c_rarg0\");\n+        __ mv(c_rarg0, _ref);\n+        __ la(c_rarg1, _ref_addr);\n+      } else {\n+        assert(_ref == c_rarg1, \"Need to vacate c_rarg1 and _ref_addr is using c_rarg0\");\n+        if (_ref_addr.base() == c_rarg0) {\n+          __ mv(t1, c_rarg1);\n+          __ la(c_rarg1, _ref_addr);\n+          __ mv(c_rarg0, t1);\n+        } else {\n+          ShouldNotReachHere();\n+        }\n+      }\n+    }\n+  }\n+\n+  ~XSetupArguments() {\n+    \/\/ Transfer result\n+    if (_ref != x10) {\n+      __ mv(_ref, x10);\n+    }\n+  }\n+};\n+\n+#undef __\n+#define __ masm->\n+\n+void XBarrierSetAssembler::generate_c2_load_barrier_stub(MacroAssembler* masm, XLoadBarrierStubC2* stub) const {\n+  BLOCK_COMMENT(\"XLoadBarrierStubC2\");\n+\n+  \/\/ Stub entry\n+  __ bind(*stub->entry());\n+\n+  {\n+    XSaveLiveRegisters save_live_registers(masm, stub);\n+    XSetupArguments setup_arguments(masm, stub);\n+\n+    Address target(stub->slow_path());\n+    __ relocate(target.rspec(), [&] {\n+      int32_t offset;\n+      __ la_patchable(t0, target, offset);\n+      __ jalr(x1, t0, offset);\n+    });\n+  }\n+\n+  \/\/ Stub exit\n+  __ j(*stub->continuation());\n+}\n+\n+#endif \/\/ COMPILER2\n+\n+#ifdef COMPILER1\n+#undef __\n+#define __ ce->masm()->\n+\n+void XBarrierSetAssembler::generate_c1_load_barrier_test(LIR_Assembler* ce,\n+                                                         LIR_Opr ref) const {\n+  assert_different_registers(xthread, ref->as_register(), t1);\n+  __ ld(t1, address_bad_mask_from_thread(xthread));\n+  __ andr(t1, t1, ref->as_register());\n+}\n+\n+void XBarrierSetAssembler::generate_c1_load_barrier_stub(LIR_Assembler* ce,\n+                                                         XLoadBarrierStubC1* stub) const {\n+  \/\/ Stub entry\n+  __ bind(*stub->entry());\n+\n+  Register ref = stub->ref()->as_register();\n+  Register ref_addr = noreg;\n+  Register tmp = noreg;\n+\n+  if (stub->tmp()->is_valid()) {\n+    \/\/ Load address into tmp register\n+    ce->leal(stub->ref_addr(), stub->tmp());\n+    ref_addr = tmp = stub->tmp()->as_pointer_register();\n+  } else {\n+    \/\/ Address already in register\n+    ref_addr = stub->ref_addr()->as_address_ptr()->base()->as_pointer_register();\n+  }\n+\n+  assert_different_registers(ref, ref_addr, noreg);\n+\n+  \/\/ Save x10 unless it is the result or tmp register\n+  \/\/ Set up SP to accommodate parameters and maybe x10.\n+  if (ref != x10 && tmp != x10) {\n+    __ sub(sp, sp, 32);\n+    __ sd(x10, Address(sp, 16));\n+  } else {\n+    __ sub(sp, sp, 16);\n+  }\n+\n+  \/\/ Setup arguments and call runtime stub\n+  ce->store_parameter(ref_addr, 1);\n+  ce->store_parameter(ref, 0);\n+\n+  __ far_call(stub->runtime_stub());\n+\n+  \/\/ Verify result\n+  __ verify_oop(x10);\n+\n+\n+  \/\/ Move result into place\n+  if (ref != x10) {\n+    __ mv(ref, x10);\n+  }\n+\n+  \/\/ Restore x10 unless it is the result or tmp register\n+  if (ref != x10 && tmp != x10) {\n+    __ ld(x10, Address(sp, 16));\n+    __ add(sp, sp, 32);\n+  } else {\n+    __ add(sp, sp, 16);\n+  }\n+\n+  \/\/ Stub exit\n+  __ j(*stub->continuation());\n+}\n+\n+#undef __\n+#define __ sasm->\n+\n+void XBarrierSetAssembler::generate_c1_load_barrier_runtime_stub(StubAssembler* sasm,\n+                                                                 DecoratorSet decorators) const {\n+  __ prologue(\"zgc_load_barrier stub\", false);\n+\n+  __ push_call_clobbered_registers_except(RegSet::of(x10));\n+\n+  \/\/ Setup arguments\n+  __ load_parameter(0, c_rarg0);\n+  __ load_parameter(1, c_rarg1);\n+\n+  __ call_VM_leaf(XBarrierSetRuntime::load_barrier_on_oop_field_preloaded_addr(decorators), 2);\n+\n+  __ pop_call_clobbered_registers_except(RegSet::of(x10));\n+\n+  __ epilogue();\n+}\n+\n+#endif \/\/ COMPILER1\n+\n+#undef __\n+#define __ masm->\n+\n+void XBarrierSetAssembler::check_oop(MacroAssembler* masm, Register obj, Register tmp1, Register tmp2, Label& error) {\n+  \/\/ Check if mask is good.\n+  \/\/ verifies that XAddressBadMask & obj == 0\n+  __ ld(tmp2, Address(xthread, XThreadLocalData::address_bad_mask_offset()));\n+  __ andr(tmp1, obj, tmp2);\n+  __ bnez(tmp1, error);\n+\n+  BarrierSetAssembler::check_oop(masm, obj, tmp1, tmp2, error);\n+}\n+\n+#undef __\n","filename":"src\/hotspot\/cpu\/riscv\/gc\/x\/xBarrierSetAssembler_riscv.cpp","additions":458,"deletions":0,"binary":false,"changes":458,"status":"added"},{"patch":"@@ -0,0 +1,112 @@\n+\/*\n+ * Copyright (c) 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2021, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef CPU_RISCV_GC_X_XBARRIERSETASSEMBLER_RISCV_HPP\n+#define CPU_RISCV_GC_X_XBARRIERSETASSEMBLER_RISCV_HPP\n+\n+#include \"code\/vmreg.hpp\"\n+#include \"oops\/accessDecorators.hpp\"\n+#ifdef COMPILER2\n+#include \"opto\/optoreg.hpp\"\n+#endif \/\/ COMPILER2\n+\n+#ifdef COMPILER1\n+class LIR_Assembler;\n+class LIR_Opr;\n+class StubAssembler;\n+#endif \/\/ COMPILER1\n+\n+#ifdef COMPILER2\n+class Node;\n+#endif \/\/ COMPILER2\n+\n+#ifdef COMPILER1\n+class XLoadBarrierStubC1;\n+#endif \/\/ COMPILER1\n+\n+#ifdef COMPILER2\n+class XLoadBarrierStubC2;\n+#endif \/\/ COMPILER2\n+\n+class XBarrierSetAssembler : public XBarrierSetAssemblerBase {\n+public:\n+  virtual void load_at(MacroAssembler* masm,\n+                       DecoratorSet decorators,\n+                       BasicType type,\n+                       Register dst,\n+                       Address src,\n+                       Register tmp1,\n+                       Register tmp2);\n+\n+#ifdef ASSERT\n+  virtual void store_at(MacroAssembler* masm,\n+                        DecoratorSet decorators,\n+                        BasicType type,\n+                        Address dst,\n+                        Register val,\n+                        Register tmp1,\n+                        Register tmp2,\n+                        Register tmp3);\n+#endif \/\/ ASSERT\n+\n+  virtual void arraycopy_prologue(MacroAssembler* masm,\n+                                  DecoratorSet decorators,\n+                                  bool is_oop,\n+                                  Register src,\n+                                  Register dst,\n+                                  Register count,\n+                                  RegSet saved_regs);\n+\n+  virtual void try_resolve_jobject_in_native(MacroAssembler* masm,\n+                                             Register jni_env,\n+                                             Register robj,\n+                                             Register tmp,\n+                                             Label& slowpath);\n+\n+  virtual NMethodPatchingType nmethod_patching_type() { return NMethodPatchingType::conc_data_patch; }\n+\n+#ifdef COMPILER1\n+  void generate_c1_load_barrier_test(LIR_Assembler* ce,\n+                                     LIR_Opr ref) const;\n+\n+  void generate_c1_load_barrier_stub(LIR_Assembler* ce,\n+                                     XLoadBarrierStubC1* stub) const;\n+\n+  void generate_c1_load_barrier_runtime_stub(StubAssembler* sasm,\n+                                             DecoratorSet decorators) const;\n+#endif \/\/ COMPILER1\n+\n+#ifdef COMPILER2\n+  OptoReg::Name refine_register(const Node* node,\n+                                OptoReg::Name opto_reg);\n+\n+  void generate_c2_load_barrier_stub(MacroAssembler* masm,\n+                                     XLoadBarrierStubC2* stub) const;\n+#endif \/\/ COMPILER2\n+\n+  void check_oop(MacroAssembler* masm, Register obj, Register tmp1, Register tmp2, Label& error);\n+};\n+\n+#endif \/\/ CPU_RISCV_GC_X_XBARRIERSETASSEMBLER_RISCV_HPP\n","filename":"src\/hotspot\/cpu\/riscv\/gc\/x\/xBarrierSetAssembler_riscv.hpp","additions":112,"deletions":0,"binary":false,"changes":112,"status":"added"},{"patch":"@@ -29,1 +29,1 @@\n-#include \"gc\/z\/zGlobals.hpp\"\n+#include \"gc\/x\/xGlobals.hpp\"\n@@ -201,1 +201,1 @@\n-size_t ZPlatformAddressOffsetBits() {\n+size_t XPlatformAddressOffsetBits() {\n@@ -205,1 +205,1 @@\n-  const size_t address_offset = round_up_power_of_2(MaxHeapSize * ZVirtualToPhysicalRatio);\n+  const size_t address_offset = round_up_power_of_2(MaxHeapSize * XVirtualToPhysicalRatio);\n@@ -210,2 +210,2 @@\n-size_t ZPlatformAddressMetadataShift() {\n-  return ZPlatformAddressOffsetBits();\n+size_t XPlatformAddressMetadataShift() {\n+  return XPlatformAddressOffsetBits();\n","filename":"src\/hotspot\/cpu\/riscv\/gc\/x\/xGlobals_riscv.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"previous_filename":"src\/hotspot\/cpu\/riscv\/gc\/z\/zGlobals_riscv.cpp","status":"renamed"},{"patch":"@@ -0,0 +1,35 @@\n+\/*\n+ * Copyright (c) 2015, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2021, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef CPU_RISCV_GC_X_XGLOBALS_RISCV_HPP\n+#define CPU_RISCV_GC_X_XGLOBALS_RISCV_HPP\n+\n+const size_t XPlatformHeapViews        = 3;\n+const size_t XPlatformCacheLineSize    = 64;\n+\n+size_t XPlatformAddressOffsetBits();\n+size_t XPlatformAddressMetadataShift();\n+\n+#endif \/\/ CPU_RISCV_GC_X_XGLOBALS_RISCV_HPP\n","filename":"src\/hotspot\/cpu\/riscv\/gc\/x\/xGlobals_riscv.hpp","additions":35,"deletions":0,"binary":false,"changes":35,"status":"added"},{"patch":"@@ -0,0 +1,233 @@\n+\/\/\n+\/\/ Copyright (c) 2019, 2021, Oracle and\/or its affiliates. All rights reserved.\n+\/\/ Copyright (c) 2020, 2021, Huawei Technologies Co., Ltd. All rights reserved.\n+\/\/ DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+\/\/\n+\/\/ This code is free software; you can redistribute it and\/or modify it\n+\/\/ under the terms of the GNU General Public License version 2 only, as\n+\/\/ published by the Free Software Foundation.\n+\/\/\n+\/\/ This code is distributed in the hope that it will be useful, but WITHOUT\n+\/\/ ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+\/\/ FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+\/\/ version 2 for more details (a copy is included in the LICENSE file that\n+\/\/ accompanied this code).\n+\/\/\n+\/\/ You should have received a copy of the GNU General Public License version\n+\/\/ 2 along with this work; if not, write to the Free Software Foundation,\n+\/\/ Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+\/\/\n+\/\/ Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+\/\/ or visit www.oracle.com if you need additional information or have any\n+\/\/ questions.\n+\/\/\n+\n+source_hpp %{\n+\n+#include \"gc\/shared\/gc_globals.hpp\"\n+#include \"gc\/x\/c2\/xBarrierSetC2.hpp\"\n+#include \"gc\/x\/xThreadLocalData.hpp\"\n+\n+%}\n+\n+source %{\n+\n+static void x_load_barrier(MacroAssembler& _masm, const MachNode* node, Address ref_addr, Register ref, Register tmp, int barrier_data) {\n+  if (barrier_data == XLoadBarrierElided) {\n+    return;\n+  }\n+  XLoadBarrierStubC2* const stub = XLoadBarrierStubC2::create(node, ref_addr, ref, tmp, barrier_data);\n+  __ ld(tmp, Address(xthread, XThreadLocalData::address_bad_mask_offset()));\n+  __ andr(tmp, tmp, ref);\n+  __ bnez(tmp, *stub->entry(), true \/* far *\/);\n+  __ bind(*stub->continuation());\n+}\n+\n+static void x_load_barrier_slow_path(MacroAssembler& _masm, const MachNode* node, Address ref_addr, Register ref, Register tmp) {\n+  XLoadBarrierStubC2* const stub = XLoadBarrierStubC2::create(node, ref_addr, ref, tmp, XLoadBarrierStrong);\n+  __ j(*stub->entry());\n+  __ bind(*stub->continuation());\n+}\n+\n+%}\n+\n+\/\/ Load Pointer\n+instruct xLoadP(iRegPNoSp dst, memory mem)\n+%{\n+  match(Set dst (LoadP mem));\n+  predicate(UseZGC && !ZGenerational && (n->as_Load()->barrier_data() != 0));\n+  effect(TEMP dst);\n+\n+  ins_cost(4 * DEFAULT_COST);\n+\n+  format %{ \"ld  $dst, $mem, #@zLoadP\" %}\n+\n+  ins_encode %{\n+    const Address ref_addr (as_Register($mem$$base), $mem$$disp);\n+    __ ld($dst$$Register, ref_addr);\n+    x_load_barrier(_masm, this, ref_addr, $dst$$Register, t0 \/* tmp *\/, barrier_data());\n+  %}\n+\n+  ins_pipe(iload_reg_mem);\n+%}\n+\n+instruct xCompareAndSwapP(iRegINoSp res, indirect mem, iRegP oldval, iRegP newval, rFlagsReg cr) %{\n+  match(Set res (CompareAndSwapP mem (Binary oldval newval)));\n+  match(Set res (WeakCompareAndSwapP mem (Binary oldval newval)));\n+  predicate(UseZGC && !ZGenerational && !needs_acquiring_load_reserved(n) && n->as_LoadStore()->barrier_data() == XLoadBarrierStrong);\n+  effect(KILL cr, TEMP_DEF res);\n+\n+  ins_cost(2 * VOLATILE_REF_COST);\n+\n+  format %{ \"cmpxchg $mem, $oldval, $newval, #@zCompareAndSwapP\\n\\t\"\n+            \"mv $res, $res == $oldval\" %}\n+\n+  ins_encode %{\n+    Label failed;\n+    guarantee($mem$$index == -1 && $mem$$disp == 0, \"impossible encoding\");\n+    __ cmpxchg($mem$$Register, $oldval$$Register, $newval$$Register, Assembler::int64,\n+               Assembler::relaxed \/* acquire *\/, Assembler::rl \/* release *\/, $res$$Register,\n+               true \/* result_as_bool *\/);\n+    __ beqz($res$$Register, failed);\n+    __ mv(t0, $oldval$$Register);\n+    __ bind(failed);\n+    if (barrier_data() != XLoadBarrierElided) {\n+      Label good;\n+      __ ld(t1, Address(xthread, XThreadLocalData::address_bad_mask_offset()), t1 \/* tmp *\/);\n+      __ andr(t1, t1, t0);\n+      __ beqz(t1, good);\n+      x_load_barrier_slow_path(_masm, this, Address($mem$$Register), t0 \/* ref *\/, t1 \/* tmp *\/);\n+      __ cmpxchg($mem$$Register, $oldval$$Register, $newval$$Register, Assembler::int64,\n+                 Assembler::relaxed \/* acquire *\/, Assembler::rl \/* release *\/, $res$$Register,\n+                 true \/* result_as_bool *\/);\n+      __ bind(good);\n+    }\n+  %}\n+\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct xCompareAndSwapPAcq(iRegINoSp res, indirect mem, iRegP oldval, iRegP newval, rFlagsReg cr) %{\n+  match(Set res (CompareAndSwapP mem (Binary oldval newval)));\n+  match(Set res (WeakCompareAndSwapP mem (Binary oldval newval)));\n+  predicate(UseZGC && !ZGenerational && needs_acquiring_load_reserved(n) && (n->as_LoadStore()->barrier_data() == XLoadBarrierStrong));\n+  effect(KILL cr, TEMP_DEF res);\n+\n+  ins_cost(2 * VOLATILE_REF_COST);\n+\n+  format %{ \"cmpxchg $mem, $oldval, $newval, #@zCompareAndSwapPAcq\\n\\t\"\n+            \"mv $res, $res == $oldval\" %}\n+\n+  ins_encode %{\n+    Label failed;\n+    guarantee($mem$$index == -1 && $mem$$disp == 0, \"impossible encoding\");\n+    __ cmpxchg($mem$$Register, $oldval$$Register, $newval$$Register, Assembler::int64,\n+               Assembler::aq \/* acquire *\/, Assembler::rl \/* release *\/, $res$$Register,\n+               true \/* result_as_bool *\/);\n+    __ beqz($res$$Register, failed);\n+    __ mv(t0, $oldval$$Register);\n+    __ bind(failed);\n+    if (barrier_data() != XLoadBarrierElided) {\n+      Label good;\n+      __ ld(t1, Address(xthread, XThreadLocalData::address_bad_mask_offset()), t1 \/* tmp *\/);\n+      __ andr(t1, t1, t0);\n+      __ beqz(t1, good);\n+      x_load_barrier_slow_path(_masm, this, Address($mem$$Register), t0 \/* ref *\/, t1 \/* tmp *\/);\n+      __ cmpxchg($mem$$Register, $oldval$$Register, $newval$$Register, Assembler::int64,\n+                 Assembler::aq \/* acquire *\/, Assembler::rl \/* release *\/, $res$$Register,\n+                 true \/* result_as_bool *\/);\n+      __ bind(good);\n+    }\n+  %}\n+\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct xCompareAndExchangeP(iRegPNoSp res, indirect mem, iRegP oldval, iRegP newval) %{\n+  match(Set res (CompareAndExchangeP mem (Binary oldval newval)));\n+  predicate(UseZGC && !ZGenerational && !needs_acquiring_load_reserved(n) && n->as_LoadStore()->barrier_data() == XLoadBarrierStrong);\n+  effect(TEMP_DEF res);\n+\n+  ins_cost(2 * VOLATILE_REF_COST);\n+\n+  format %{ \"cmpxchg $res = $mem, $oldval, $newval, #@zCompareAndExchangeP\" %}\n+\n+  ins_encode %{\n+    guarantee($mem$$index == -1 && $mem$$disp == 0, \"impossible encoding\");\n+    __ cmpxchg($mem$$Register, $oldval$$Register, $newval$$Register, Assembler::int64,\n+               Assembler::relaxed \/* acquire *\/, Assembler::rl \/* release *\/, $res$$Register);\n+    if (barrier_data() != XLoadBarrierElided) {\n+      Label good;\n+      __ ld(t0, Address(xthread, XThreadLocalData::address_bad_mask_offset()));\n+      __ andr(t0, t0, $res$$Register);\n+      __ beqz(t0, good);\n+      x_load_barrier_slow_path(_masm, this, Address($mem$$Register), $res$$Register \/* ref *\/, t0 \/* tmp *\/);\n+      __ cmpxchg($mem$$Register, $oldval$$Register, $newval$$Register, Assembler::int64,\n+                 Assembler::relaxed \/* acquire *\/, Assembler::rl \/* release *\/, $res$$Register);\n+      __ bind(good);\n+    }\n+  %}\n+\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct xCompareAndExchangePAcq(iRegPNoSp res, indirect mem, iRegP oldval, iRegP newval) %{\n+  match(Set res (CompareAndExchangeP mem (Binary oldval newval)));\n+  predicate(UseZGC && !ZGenerational && needs_acquiring_load_reserved(n) && n->as_LoadStore()->barrier_data() == XLoadBarrierStrong);\n+  effect(TEMP_DEF res);\n+\n+  ins_cost(2 * VOLATILE_REF_COST);\n+\n+  format %{ \"cmpxchg $res = $mem, $oldval, $newval, #@zCompareAndExchangePAcq\" %}\n+\n+  ins_encode %{\n+    guarantee($mem$$index == -1 && $mem$$disp == 0, \"impossible encoding\");\n+    __ cmpxchg($mem$$Register, $oldval$$Register, $newval$$Register, Assembler::int64,\n+               Assembler::aq \/* acquire *\/, Assembler::rl \/* release *\/, $res$$Register);\n+    if (barrier_data() != XLoadBarrierElided) {\n+      Label good;\n+      __ ld(t0, Address(xthread, XThreadLocalData::address_bad_mask_offset()));\n+      __ andr(t0, t0, $res$$Register);\n+      __ beqz(t0, good);\n+      x_load_barrier_slow_path(_masm, this, Address($mem$$Register), $res$$Register \/* ref *\/, t0 \/* tmp *\/);\n+      __ cmpxchg($mem$$Register, $oldval$$Register, $newval$$Register, Assembler::int64,\n+                 Assembler::aq \/* acquire *\/, Assembler::rl \/* release *\/, $res$$Register);\n+      __ bind(good);\n+    }\n+  %}\n+\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct xGetAndSetP(indirect mem, iRegP newv, iRegPNoSp prev, rFlagsReg cr) %{\n+  match(Set prev (GetAndSetP mem newv));\n+  predicate(UseZGC && !ZGenerational && !needs_acquiring_load_reserved(n) && n->as_LoadStore()->barrier_data() != 0);\n+  effect(TEMP_DEF prev, KILL cr);\n+\n+  ins_cost(2 * VOLATILE_REF_COST);\n+\n+  format %{ \"atomic_xchg  $prev, $newv, [$mem], #@zGetAndSetP\" %}\n+\n+  ins_encode %{\n+    __ atomic_xchg($prev$$Register, $newv$$Register, as_Register($mem$$base));\n+    x_load_barrier(_masm, this, Address(noreg, 0), $prev$$Register, t0 \/* tmp *\/, barrier_data());\n+  %}\n+\n+  ins_pipe(pipe_serial);\n+%}\n+\n+instruct xGetAndSetPAcq(indirect mem, iRegP newv, iRegPNoSp prev, rFlagsReg cr) %{\n+  match(Set prev (GetAndSetP mem newv));\n+  predicate(UseZGC && !ZGenerational && needs_acquiring_load_reserved(n) && (n->as_LoadStore()->barrier_data() != 0));\n+  effect(TEMP_DEF prev, KILL cr);\n+\n+  ins_cost(VOLATILE_REF_COST);\n+\n+  format %{ \"atomic_xchg_acq  $prev, $newv, [$mem], #@zGetAndSetPAcq\" %}\n+\n+  ins_encode %{\n+    __ atomic_xchgal($prev$$Register, $newv$$Register, as_Register($mem$$base));\n+    x_load_barrier(_masm, this, Address(noreg, 0), $prev$$Register, t0 \/* tmp *\/, barrier_data());\n+  %}\n+  ins_pipe(pipe_serial);\n+%}\n","filename":"src\/hotspot\/cpu\/riscv\/gc\/x\/x_riscv64.ad","additions":233,"deletions":0,"binary":false,"changes":233,"status":"added"},{"patch":"@@ -0,0 +1,109 @@\n+\/*\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2023, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/shared\/gcLogPrecious.hpp\"\n+#include \"gc\/shared\/gc_globals.hpp\"\n+#include \"gc\/z\/zAddress.hpp\"\n+#include \"gc\/z\/zBarrierSetAssembler.hpp\"\n+#include \"gc\/z\/zGlobals.hpp\"\n+#include \"runtime\/globals.hpp\"\n+#include \"runtime\/os.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include \"utilities\/powerOfTwo.hpp\"\n+\n+#ifdef LINUX\n+#include <sys\/mman.h>\n+#endif \/\/ LINUX\n+\n+\/\/ Default value if probe is not implemented for a certain platform: 128TB\n+static const size_t DEFAULT_MAX_ADDRESS_BIT = 47;\n+\/\/ Minimum value returned, if probing fails: 64GB\n+static const size_t MINIMUM_MAX_ADDRESS_BIT = 36;\n+\n+static size_t probe_valid_max_address_bit() {\n+#ifdef LINUX\n+  size_t max_address_bit = 0;\n+  const size_t page_size = os::vm_page_size();\n+  for (size_t i = DEFAULT_MAX_ADDRESS_BIT; i > MINIMUM_MAX_ADDRESS_BIT; --i) {\n+    const uintptr_t base_addr = ((uintptr_t) 1U) << i;\n+    if (msync((void*)base_addr, page_size, MS_ASYNC) == 0) {\n+      \/\/ msync suceeded, the address is valid, and maybe even already mapped.\n+      max_address_bit = i;\n+      break;\n+    }\n+    if (errno != ENOMEM) {\n+      \/\/ Some error occured. This should never happen, but msync\n+      \/\/ has some undefined behavior, hence ignore this bit.\n+#ifdef ASSERT\n+      fatal(\"Received '%s' while probing the address space for the highest valid bit\", os::errno_name(errno));\n+#else \/\/ ASSERT\n+      log_warning_p(gc)(\"Received '%s' while probing the address space for the highest valid bit\", os::errno_name(errno));\n+#endif \/\/ ASSERT\n+      continue;\n+    }\n+    \/\/ Since msync failed with ENOMEM, the page might not be mapped.\n+    \/\/ Try to map it, to see if the address is valid.\n+    void* const result_addr = mmap((void*) base_addr, page_size, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS|MAP_NORESERVE, -1, 0);\n+    if (result_addr != MAP_FAILED) {\n+      munmap(result_addr, page_size);\n+    }\n+    if ((uintptr_t) result_addr == base_addr) {\n+      \/\/ address is valid\n+      max_address_bit = i;\n+      break;\n+    }\n+  }\n+  if (max_address_bit == 0) {\n+    \/\/ probing failed, allocate a very high page and take that bit as the maximum\n+    const uintptr_t high_addr = ((uintptr_t) 1U) << DEFAULT_MAX_ADDRESS_BIT;\n+    void* const result_addr = mmap((void*) high_addr, page_size, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS|MAP_NORESERVE, -1, 0);\n+    if (result_addr != MAP_FAILED) {\n+      max_address_bit = BitsPerSize_t - count_leading_zeros((size_t) result_addr) - 1;\n+      munmap(result_addr, page_size);\n+    }\n+  }\n+  log_info_p(gc, init)(\"Probing address space for the highest valid bit: \" SIZE_FORMAT, max_address_bit);\n+  return MAX2(max_address_bit, MINIMUM_MAX_ADDRESS_BIT);\n+#else \/\/ LINUX\n+  return DEFAULT_MAX_ADDRESS_BIT;\n+#endif \/\/ LINUX\n+}\n+\n+size_t ZPlatformAddressOffsetBits() {\n+  const static size_t valid_max_address_offset_bits = probe_valid_max_address_bit() + 1;\n+  const size_t max_address_offset_bits = valid_max_address_offset_bits - 3;\n+  const size_t min_address_offset_bits = max_address_offset_bits - 2;\n+  const size_t address_offset = round_up_power_of_2(MaxHeapSize * ZVirtualToPhysicalRatio);\n+  const size_t address_offset_bits = log2i_exact(address_offset);\n+  return clamp(address_offset_bits, min_address_offset_bits, max_address_offset_bits);\n+}\n+\n+size_t ZPlatformAddressHeapBaseShift() {\n+  return ZPlatformAddressOffsetBits();\n+}\n+\n+void ZGlobalsPointers::pd_set_good_masks() {\n+  BarrierSetAssembler::clear_patching_epoch();\n+}\n","filename":"src\/hotspot\/cpu\/riscv\/gc\/z\/zAddress_riscv.cpp","additions":109,"deletions":0,"binary":false,"changes":109,"status":"added"},{"patch":"@@ -0,0 +1,34 @@\n+\/*\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef CPU_RISCV_GC_Z_ZADDRESS_RISCV_HPP\n+#define CPU_RISCV_GC_Z_ZADDRESS_RISCV_HPP\n+\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+const size_t ZPointerLoadShift = 16;\n+\n+size_t ZPlatformAddressOffsetBits();\n+size_t ZPlatformAddressHeapBaseShift();\n+\n+#endif \/\/ CPU_RISCV_GC_Z_ZADDRESS_RISCV_HPP\n","filename":"src\/hotspot\/cpu\/riscv\/gc\/z\/zAddress_riscv.hpp","additions":34,"deletions":0,"binary":false,"changes":34,"status":"added"},{"patch":"@@ -0,0 +1,38 @@\n+\/*\n+ * Copyright (c) 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2023, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef CPU_RISCV_GC_Z_ZADDRESS_RISCV_INLINE_HPP\n+#define CPU_RISCV_GC_Z_ZADDRESS_RISCV_INLINE_HPP\n+\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+inline uintptr_t ZPointer::remap_bits(uintptr_t colored) {\n+  return colored & ZPointerRemappedMask;\n+}\n+\n+inline constexpr int ZPointer::load_shift_lookup(uintptr_t value) {\n+  return ZPointerLoadShift;\n+}\n+\n+#endif \/\/ CPU_RISCV_GC_Z_ZADDRESS_RISCV_INLINE_HPP\n","filename":"src\/hotspot\/cpu\/riscv\/gc\/z\/zAddress_riscv.inline.hpp","additions":38,"deletions":0,"binary":false,"changes":38,"status":"added"},{"patch":"@@ -3,1 +3,1 @@\n- * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * Copyright (c) 2020, 2023, Huawei Technologies Co., Ltd. All rights reserved.\n@@ -30,0 +30,1 @@\n+#include \"gc\/z\/zAddress.hpp\"\n@@ -36,0 +37,1 @@\n+#include \"runtime\/jniHandles.hpp\"\n@@ -37,0 +39,1 @@\n+#include \"utilities\/debug.hpp\"\n@@ -45,0 +48,1 @@\n+#include \"opto\/output.hpp\"\n@@ -56,0 +60,46 @@\n+\/\/ Helper for saving and restoring registers across a runtime call that does\n+\/\/ not have any live vector registers.\n+class ZRuntimeCallSpill {\n+private:\n+  MacroAssembler* _masm;\n+  Register _result;\n+\n+  void save() {\n+    MacroAssembler* masm = _masm;\n+\n+    __ enter();\n+    if (_result != noreg) {\n+      __ push_call_clobbered_registers_except(RegSet::of(_result));\n+    } else {\n+      __ push_call_clobbered_registers();\n+    }\n+  }\n+\n+  void restore() {\n+    MacroAssembler* masm = _masm;\n+\n+    if (_result != noreg) {\n+      \/\/ Make sure _result has the return value.\n+      if (_result != x10) {\n+        __ mv(_result, x10);\n+      }\n+\n+      __ pop_call_clobbered_registers_except(RegSet::of(_result));\n+    } else {\n+      __ pop_call_clobbered_registers();\n+    }\n+    __ leave();\n+  }\n+\n+public:\n+  ZRuntimeCallSpill(MacroAssembler* masm, Register result)\n+    : _masm(masm),\n+      _result(result) {\n+    save();\n+  }\n+\n+  ~ZRuntimeCallSpill() {\n+    restore();\n+  }\n+};\n+\n@@ -69,2 +119,3 @@\n-  assert_different_registers(t1, src.base());\n-  assert_different_registers(t0, t1, dst);\n+  assert_different_registers(tmp1, tmp2, src.base(), noreg);\n+  assert_different_registers(tmp1, tmp2, dst, noreg);\n+  assert_different_registers(tmp2, t0);\n@@ -73,0 +124,6 @@\n+  Label uncolor;\n+\n+  \/\/ Load bad mask into scratch register.\n+  const bool on_non_strong =\n+    (decorators & ON_WEAK_OOP_REF) != 0 ||\n+    (decorators & ON_PHANTOM_OOP_REF) != 0;\n@@ -74,4 +131,8 @@\n-  \/\/ Load bad mask into temp register.\n-  __ la(t0, src);\n-  __ ld(t1, address_bad_mask_from_thread(xthread));\n-  __ ld(dst, Address(t0));\n+  if (on_non_strong) {\n+    __ ld(tmp1, mark_bad_mask_from_thread(xthread));\n+  } else {\n+    __ ld(tmp1, load_bad_mask_from_thread(xthread));\n+  }\n+\n+  __ la(tmp2, src);\n+  __ ld(dst, tmp2);\n@@ -80,2 +141,2 @@\n-  __ andr(t1, dst, t1);\n-  __ beqz(t1, done);\n+  __ andr(tmp1, dst, tmp1);\n+  __ beqz(tmp1, uncolor);\n@@ -83,1 +144,3 @@\n-  __ enter();\n+  {\n+    \/\/ Call VM\n+    ZRuntimeCallSpill rsc(masm, dst);\n@@ -85,1 +148,4 @@\n-  __ push_call_clobbered_registers_except(RegSet::of(dst));\n+    if (c_rarg0 != dst) {\n+      __ mv(c_rarg0, dst);\n+    }\n+    __ mv(c_rarg1, tmp2);\n@@ -87,2 +153,1 @@\n-  if (c_rarg0 != dst) {\n-    __ mv(c_rarg0, dst);\n+    __ call_VM_leaf(ZBarrierSetRuntime::load_barrier_on_oop_field_preloaded_addr(decorators), 2);\n@@ -91,1 +156,2 @@\n-  __ mv(c_rarg1, t0);\n+  \/\/ Slow-path has already uncolored\n+  __ j(done);\n@@ -93,1 +159,4 @@\n-  __ call_VM_leaf(ZBarrierSetRuntime::load_barrier_on_oop_field_preloaded_addr(decorators), 2);\n+  __ bind(uncolor);\n+\n+  \/\/ Remove the color bits\n+  __ srli(dst, dst, ZPointerLoadShift);\n@@ -95,3 +164,62 @@\n-  \/\/ Make sure dst has the return value.\n-  if (dst != x10) {\n-    __ mv(dst, x10);\n+  __ bind(done);\n+}\n+\n+void ZBarrierSetAssembler::store_barrier_fast(MacroAssembler* masm,\n+                                              Address ref_addr,\n+                                              Register rnew_zaddress,\n+                                              Register rnew_zpointer,\n+                                              Register rtmp,\n+                                              bool in_nmethod,\n+                                              bool is_atomic,\n+                                              Label& medium_path,\n+                                              Label& medium_path_continuation) const {\n+  assert_different_registers(ref_addr.base(), rnew_zpointer, rtmp);\n+  assert_different_registers(rnew_zaddress, rnew_zpointer, rtmp);\n+\n+  if (in_nmethod) {\n+    if (is_atomic) {\n+      __ lhu(rtmp, ref_addr);\n+      \/\/ Atomic operations must ensure that the contents of memory are store-good before\n+      \/\/ an atomic opertion can execute.\n+      \/\/ A non-relocatable object could have spurious raw null pointers in its fields after\n+      \/\/ getting promoted to the old generation.\n+      __ relocate(barrier_Relocation::spec(), [&] {\n+        __ li16u(rnew_zpointer, barrier_Relocation::unpatched);\n+      }, ZBarrierRelocationFormatStoreGoodBits);\n+      __ bne(rtmp, rnew_zpointer, medium_path, true \/* is_far *\/);\n+    } else {\n+      __ ld(rtmp, ref_addr);\n+      \/\/ Stores on relocatable objects never need to deal with raw null pointers in fields.\n+      \/\/ Raw null pointers may only exists in the young generation, as they get pruned when\n+      \/\/ the object is relocated to old. And no pre-write barrier needs to perform any action\n+      \/\/ in the young generation.\n+      __ relocate(barrier_Relocation::spec(), [&] {\n+        __ li16u(rnew_zpointer, barrier_Relocation::unpatched);\n+      }, ZBarrierRelocationFormatStoreBadMask);\n+      __ andr(rtmp, rtmp, rnew_zpointer);\n+      __ bnez(rtmp, medium_path, true \/* is_far *\/);\n+    }\n+    __ bind(medium_path_continuation);\n+    __ relocate(barrier_Relocation::spec(), [&] {\n+      __ li16u(rtmp, barrier_Relocation::unpatched);\n+    }, ZBarrierRelocationFormatStoreGoodBits);\n+    __ slli(rnew_zpointer, rnew_zaddress, ZPointerLoadShift);\n+    __ orr(rnew_zpointer, rnew_zpointer, rtmp);\n+  } else {\n+    assert(!is_atomic, \"atomic outside of nmethods not supported\");\n+    __ la(rtmp, ref_addr);\n+    __ ld(rtmp, rtmp);\n+    __ ld(rnew_zpointer, Address(xthread, ZThreadLocalData::store_bad_mask_offset()));\n+    __ andr(rtmp, rtmp, rnew_zpointer);\n+    __ bnez(rtmp, medium_path, true \/* is_far *\/);\n+    __ bind(medium_path_continuation);\n+    if (rnew_zaddress == noreg) {\n+      __ mv(rnew_zpointer, zr);\n+    } else {\n+      __ mv(rnew_zpointer, rnew_zaddress);\n+    }\n+\n+    \/\/ Load the current good shift, and add the color bits\n+    __ slli(rnew_zpointer, rnew_zpointer, ZPointerLoadShift);\n+    __ ld(rtmp, Address(xthread, ZThreadLocalData::store_good_mask_offset()));\n+    __ orr(rnew_zpointer, rnew_zpointer, rtmp);\n@@ -99,0 +227,1 @@\n+}\n@@ -100,2 +229,7 @@\n-  __ pop_call_clobbered_registers_except(RegSet::of(dst));\n-  __ leave();\n+static void store_barrier_buffer_add(MacroAssembler* masm,\n+                                     Address ref_addr,\n+                                     Register tmp1,\n+                                     Register tmp2,\n+                                     Label& slow_path) {\n+  Address buffer(xthread, ZThreadLocalData::store_barrier_buffer_offset());\n+  assert_different_registers(ref_addr.base(), tmp1, tmp2);\n@@ -103,1 +237,21 @@\n-  __ bind(done);\n+  __ ld(tmp1, buffer);\n+\n+  \/\/ Combined pointer bump and check if the buffer is disabled or full\n+  __ ld(tmp2, Address(tmp1, ZStoreBarrierBuffer::current_offset()));\n+  __ beqz(tmp2, slow_path);\n+\n+  \/\/ Bump the pointer\n+  __ sub(tmp2, tmp2, sizeof(ZStoreBarrierEntry));\n+  __ sd(tmp2, Address(tmp1, ZStoreBarrierBuffer::current_offset()));\n+\n+  \/\/ Compute the buffer entry address\n+  __ la(tmp2, Address(tmp2, ZStoreBarrierBuffer::buffer_offset()));\n+  __ add(tmp2, tmp2, tmp1);\n+\n+  \/\/ Compute and log the store address\n+  __ la(tmp1, ref_addr);\n+  __ sd(tmp1, Address(tmp2, in_bytes(ZStoreBarrierEntry::p_offset())));\n+\n+  \/\/ Load and log the prev value\n+  __ ld(tmp1, tmp1);\n+  __ sd(tmp1, Address(tmp2, in_bytes(ZStoreBarrierEntry::prev_offset())));\n@@ -106,1 +260,50 @@\n-#ifdef ASSERT\n+void ZBarrierSetAssembler::store_barrier_medium(MacroAssembler* masm,\n+                                                Address ref_addr,\n+                                                Register rtmp1,\n+                                                Register rtmp2,\n+                                                Register rtmp3,\n+                                                bool is_native,\n+                                                bool is_atomic,\n+                                                Label& medium_path_continuation,\n+                                                Label& slow_path,\n+                                                Label& slow_path_continuation) const {\n+  assert_different_registers(ref_addr.base(), rtmp1, rtmp2, rtmp3);\n+\n+  \/\/ The reason to end up in the medium path is that the pre-value was not 'good'.\n+  if (is_native) {\n+    __ j(slow_path);\n+    __ bind(slow_path_continuation);\n+    __ j(medium_path_continuation);\n+  } else if (is_atomic) {\n+    \/\/ Atomic accesses can get to the medium fast path because the value was a\n+    \/\/ raw null value. If it was not null, then there is no doubt we need to take a slow path.\n+\n+    __ la(rtmp2, ref_addr);\n+    __ ld(rtmp1, rtmp2);\n+    __ bnez(rtmp1, slow_path);\n+\n+    \/\/ If we get this far, we know there is a young raw null value in the field.\n+    __ relocate(barrier_Relocation::spec(), [&] {\n+      __ li16u(rtmp1, barrier_Relocation::unpatched);\n+    }, ZBarrierRelocationFormatStoreGoodBits);\n+    __ cmpxchg_weak(rtmp2, zr, rtmp1,\n+                    Assembler::int64,\n+                    Assembler::relaxed \/* acquire *\/, Assembler::relaxed \/* release *\/,\n+                    rtmp3);\n+    __ beqz(rtmp3, slow_path);\n+    __ bind(slow_path_continuation);\n+    __ j(medium_path_continuation);\n+  } else {\n+    \/\/ A non-atomic relocatable object wont't get to the medium fast path due to a\n+    \/\/ raw null in the young generation. We only get here because the field is bad.\n+    \/\/ In this path we don't need any self healing, so we can avoid a runtime call\n+    \/\/ most of the time by buffering the store barrier to be applied lazily.\n+    store_barrier_buffer_add(masm,\n+                             ref_addr,\n+                             rtmp1,\n+                             rtmp2,\n+                             slow_path);\n+    __ bind(slow_path_continuation);\n+    __ j(medium_path_continuation);\n+  }\n+}\n@@ -116,18 +319,46 @@\n-  \/\/ Verify value\n-  if (is_reference_type(type)) {\n-    \/\/ Note that src could be noreg, which means we\n-    \/\/ are storing null and can skip verification.\n-    if (val != noreg) {\n-      Label done;\n-\n-      \/\/ tmp1, tmp2 and tmp3 are often set to noreg.\n-      RegSet savedRegs = RegSet::of(t0);\n-      __ push_reg(savedRegs, sp);\n-\n-      __ ld(t0, address_bad_mask_from_thread(xthread));\n-      __ andr(t0, val, t0);\n-      __ beqz(t0, done);\n-      __ stop(\"Verify oop store failed\");\n-      __ should_not_reach_here();\n-      __ bind(done);\n-      __ pop_reg(savedRegs, sp);\n+  if (!ZBarrierSet::barrier_needed(decorators, type)) {\n+    BarrierSetAssembler::store_at(masm, decorators, type, dst, val, tmp1, tmp2, tmp3);\n+    return;\n+  }\n+\n+  bool dest_uninitialized = (decorators & IS_DEST_UNINITIALIZED) != 0;\n+\n+  assert_different_registers(val, tmp1, dst.base());\n+\n+  if (dest_uninitialized) {\n+    if (val == noreg) {\n+      __ mv(tmp1, zr);\n+    } else {\n+      __ mv(tmp1, val);\n+    }\n+    \/\/ Add the color bits\n+    __ slli(tmp1, tmp1, ZPointerLoadShift);\n+    __ ld(tmp2, Address(xthread, ZThreadLocalData::store_good_mask_offset()));\n+    __ orr(tmp1, tmp2, tmp1);\n+  } else {\n+    Label done;\n+    Label medium;\n+    Label medium_continuation;\n+    Label slow;\n+    Label slow_continuation;\n+    store_barrier_fast(masm, dst, val, tmp1, tmp2, false, false, medium, medium_continuation);\n+\n+    __ j(done);\n+    __ bind(medium);\n+    store_barrier_medium(masm,\n+                         dst,\n+                         tmp1,\n+                         tmp2,\n+                         noreg \/* tmp3 *\/,\n+                         false \/* is_native *\/,\n+                         false \/* is_atomic *\/,\n+                         medium_continuation,\n+                         slow,\n+                         slow_continuation);\n+\n+    __ bind(slow);\n+    {\n+      \/\/ Call VM\n+      ZRuntimeCallSpill rcs(masm, noreg);\n+      __ la(c_rarg0, dst);\n+      __ MacroAssembler::call_VM_leaf(ZBarrierSetRuntime::store_barrier_on_oop_field_without_healing_addr(), 1);\n@@ -135,0 +366,3 @@\n+\n+    __ j(slow_continuation);\n+    __ bind(done);\n@@ -138,1 +372,1 @@\n-  BarrierSetAssembler::store_at(masm, decorators, type, dst, val, tmp1, tmp2, noreg);\n+  BarrierSetAssembler::store_at(masm, decorators, type, dst, tmp1, tmp2, tmp3, noreg);\n@@ -141,1 +375,41 @@\n-#endif \/\/ ASSERT\n+class ZCopyRuntimeCallSpill {\n+private:\n+  MacroAssembler* _masm;\n+  Register _result;\n+\n+  void save() {\n+    MacroAssembler* masm = _masm;\n+\n+    __ enter();\n+    if (_result != noreg) {\n+      __ push_call_clobbered_registers_except(RegSet::of(_result));\n+    } else {\n+      __ push_call_clobbered_registers();\n+    }\n+  }\n+\n+  void restore() {\n+    MacroAssembler* masm = _masm;\n+\n+    if (_result != noreg) {\n+      if (_result != x10) {\n+        __ mv(_result, x10);\n+      }\n+      __ pop_call_clobbered_registers_except(RegSet::of(_result));\n+    } else {\n+      __ pop_call_clobbered_registers();\n+    }\n+    __ leave();\n+  }\n+\n+public:\n+  ZCopyRuntimeCallSpill(MacroAssembler* masm, Register result)\n+    : _masm(masm),\n+      _result(result) {\n+    save();\n+  }\n+\n+  ~ZCopyRuntimeCallSpill() {\n+    restore();\n+  }\n+};\n@@ -150,2 +424,80 @@\n-  if (!is_oop) {\n-    \/\/ Barrier not needed\n+}\n+\n+static void copy_load_barrier(MacroAssembler* masm,\n+                              Register ref,\n+                              Address src,\n+                              Register tmp) {\n+  Label done;\n+\n+  __ ld(tmp, Address(xthread, ZThreadLocalData::load_bad_mask_offset()));\n+\n+  \/\/ Test reference against bad mask. If mask bad, then we need to fix it up\n+  __ andr(tmp, ref, tmp);\n+  __ beqz(tmp, done);\n+\n+  {\n+    \/\/ Call VM\n+    ZCopyRuntimeCallSpill rsc(masm, ref);\n+\n+    __ la(c_rarg1, src);\n+\n+    if (c_rarg0 != ref) {\n+      __ mv(c_rarg0, ref);\n+    }\n+\n+    __ call_VM_leaf(ZBarrierSetRuntime::load_barrier_on_oop_field_preloaded_addr(IN_HEAP | ON_STRONG_OOP_REF), 2);\n+  }\n+\n+  \/\/ Slow-path has uncolored; revert\n+  __ slli(ref, ref, ZPointerLoadShift);\n+\n+  __ bind(done);\n+}\n+\n+static void copy_store_barrier(MacroAssembler* masm,\n+                               Register pre_ref,\n+                               Register new_ref,\n+                               Address src,\n+                               Register tmp1,\n+                               Register tmp2) {\n+  Label done;\n+  Label slow;\n+\n+  \/\/ Test reference against bad mask. If mask bad, then we need to fix it up.\n+  __ ld(tmp1, Address(xthread, ZThreadLocalData::store_bad_mask_offset()));\n+  __ andr(tmp1, pre_ref, tmp1);\n+  __ beqz(tmp1, done);\n+\n+  store_barrier_buffer_add(masm, src, tmp1, tmp2, slow);\n+  __ j(done);\n+\n+  __ bind(slow);\n+  {\n+    \/\/ Call VM\n+    ZCopyRuntimeCallSpill rcs(masm, noreg);\n+\n+    __ la(c_rarg0, src);\n+\n+    __ call_VM_leaf(ZBarrierSetRuntime::store_barrier_on_oop_field_without_healing_addr(), 1);\n+  }\n+\n+  __ bind(done);\n+\n+  if (new_ref != noreg) {\n+    \/\/ Set store-good color, replacing whatever color was there before\n+    __ ld(tmp1, Address(xthread, ZThreadLocalData::store_good_mask_offset()));\n+    __ srli(new_ref, new_ref, 16);\n+    __ slli(new_ref, new_ref, 16);\n+    __ orr(new_ref, new_ref, tmp1);\n+  }\n+}\n+\n+void ZBarrierSetAssembler::copy_load_at(MacroAssembler* masm,\n+                                        DecoratorSet decorators,\n+                                        BasicType type,\n+                                        size_t bytes,\n+                                        Register dst,\n+                                        Address src,\n+                                        Register tmp) {\n+  if (!is_reference_type(type)) {\n+    BarrierSetAssembler::copy_load_at(masm, decorators, type, bytes, dst, src, noreg);\n@@ -155,1 +507,1 @@\n-  BLOCK_COMMENT(\"ZBarrierSetAssembler::arraycopy_prologue {\");\n+  BarrierSetAssembler::copy_load_at(masm, decorators, type, bytes, dst, src, noreg);\n@@ -157,1 +509,2 @@\n-  assert_different_registers(src, count, t0);\n+  assert(bytes == 8, \"unsupported copy step\");\n+  copy_load_barrier(masm, dst, src, tmp);\n@@ -159,1 +512,4 @@\n-  __ push_reg(saved_regs, sp);\n+  if ((decorators & ARRAYCOPY_CHECKCAST) != 0) {\n+    __ srli(dst, dst, ZPointerLoadShift);\n+  }\n+}\n@@ -161,8 +517,16 @@\n-  if (count == c_rarg0 && src == c_rarg1) {\n-    \/\/ exactly backwards!!\n-    __ xorr(c_rarg0, c_rarg0, c_rarg1);\n-    __ xorr(c_rarg1, c_rarg0, c_rarg1);\n-    __ xorr(c_rarg0, c_rarg0, c_rarg1);\n-  } else {\n-    __ mv(c_rarg0, src);\n-    __ mv(c_rarg1, count);\n+void ZBarrierSetAssembler::copy_store_at(MacroAssembler* masm,\n+                                         DecoratorSet decorators,\n+                                         BasicType type,\n+                                         size_t bytes,\n+                                         Address dst,\n+                                         Register src,\n+                                         Register tmp1,\n+                                         Register tmp2,\n+                                         Register tmp3) {\n+  if (!is_reference_type(type)) {\n+    BarrierSetAssembler::copy_store_at(masm, decorators, type, bytes, dst, src, noreg, noreg, noreg);\n+    return;\n+  }\n+\n+  if ((decorators & ARRAYCOPY_CHECKCAST) != 0) {\n+    __ slli(src, src, ZPointerLoadShift);\n@@ -171,1 +535,13 @@\n-  __ call_VM_leaf(ZBarrierSetRuntime::load_barrier_on_oop_array_addr(), 2);\n+  bool is_dest_uninitialized = (decorators & IS_DEST_UNINITIALIZED) != 0;\n+\n+  assert(bytes == 8, \"unsupported copy step\");\n+  if (is_dest_uninitialized) {\n+    __ ld(tmp1, Address(xthread, ZThreadLocalData::store_good_mask_offset()));\n+    __ srli(src, src, 16);\n+    __ slli(src, src, 16);\n+    __ orr(src, src, tmp1);\n+  } else {\n+    \/\/ Store barrier pre values and color new values\n+    __ ld(tmp1, dst);\n+    copy_store_barrier(masm, tmp1, src, dst, tmp2, tmp3);\n+  }\n@@ -173,1 +549,3 @@\n-  __ pop_reg(saved_regs, sp);\n+  \/\/ Store new values\n+  BarrierSetAssembler::copy_store_at(masm, decorators, type, bytes, dst, src, noreg, noreg, noreg);\n+}\n@@ -175,1 +553,2 @@\n-  BLOCK_COMMENT(\"} ZBarrierSetAssembler::arraycopy_prologue\");\n+bool ZBarrierSetAssembler::supports_rvv_arraycopy() {\n+  return false;\n@@ -185,1 +564,5 @@\n-  assert_different_registers(jni_env, robj, tmp);\n+  Label done, tagged, weak_tagged, uncolor;\n+\n+  \/\/ Test for tag\n+  __ andi(tmp, robj, JNIHandles::tag_mask);\n+  __ bnez(tmp, tagged);\n@@ -187,2 +570,3 @@\n-  \/\/ Resolve jobject\n-  BarrierSetAssembler::try_resolve_jobject_in_native(masm, jni_env, robj, tmp, slowpath);\n+  \/\/ Resolve local handle\n+  __ ld(robj, robj);\n+  __ j(done);\n@@ -190,3 +574,1 @@\n-  \/\/ Compute the offset of address bad mask from the field of jni_environment\n-  long int bad_mask_relative_offset = (long int) (in_bytes(ZThreadLocalData::address_bad_mask_offset()) -\n-                                                  in_bytes(JavaThread::jni_environment_offset()));\n+  __ bind(tagged);\n@@ -194,2 +576,3 @@\n-  \/\/ Load the address bad mask\n-  __ ld(tmp, Address(jni_env, bad_mask_relative_offset));\n+  \/\/ Test for weak tag\n+  __ andi(tmp, robj, JNIHandles::TypeTag::weak_global);\n+  __ bnez(tmp, weak_tagged);\n@@ -197,1 +580,4 @@\n-  \/\/ Check address bad mask\n+  \/\/ Resolve global handle\n+  __ ld(robj, Address(robj, -JNIHandles::TypeTag::global));\n+  __ la(tmp, load_bad_mask_from_jni_env(jni_env));\n+  __ ld(tmp, tmp);\n@@ -200,0 +586,17 @@\n+  __ j(uncolor);\n+\n+  __ bind(weak_tagged);\n+\n+  \/\/ Resolve weak handle\n+  __ ld(robj, Address(robj, -JNIHandles::TypeTag::weak_global));\n+  __ la(tmp, mark_bad_mask_from_jni_env(jni_env));\n+  __ ld(tmp, tmp);\n+  __ andr(tmp, robj, tmp);\n+  __ bnez(tmp, slowpath);\n+\n+  __ bind(uncolor);\n+\n+  \/\/ Uncolor\n+  __ srli(robj, robj, ZPointerLoadShift);\n+\n+  __ bind(done);\n@@ -204,0 +607,37 @@\n+static uint16_t patch_barrier_relocation_value(int format) {\n+  switch (format) {\n+    case ZBarrierRelocationFormatLoadBadMask:\n+      return (uint16_t)ZPointerLoadBadMask;\n+    case ZBarrierRelocationFormatMarkBadMask:\n+      return (uint16_t)ZPointerMarkBadMask;\n+    case ZBarrierRelocationFormatStoreGoodBits:\n+      return (uint16_t)ZPointerStoreGoodMask;\n+    case ZBarrierRelocationFormatStoreBadMask:\n+      return (uint16_t)ZPointerStoreBadMask;\n+\n+    default:\n+      ShouldNotReachHere();\n+      return 0;\n+  }\n+}\n+\n+void ZBarrierSetAssembler::patch_barrier_relocation(address addr, int format) {\n+  const uint16_t value = patch_barrier_relocation_value(format);\n+\n+  int bytes;\n+  switch (format) {\n+    case ZBarrierRelocationFormatLoadBadMask:\n+    case ZBarrierRelocationFormatMarkBadMask:\n+    case ZBarrierRelocationFormatStoreGoodBits:\n+    case ZBarrierRelocationFormatStoreBadMask:\n+      assert(NativeInstruction::is_li16u_at(addr), \"invalide zgc barrier\");\n+      bytes = MacroAssembler::pd_patch_instruction_size(addr, (address)(uintptr_t)value);\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+  }\n+\n+  \/\/ A full fence is generated before icache_flush by default in invalidate_word\n+  ICache::invalidate_range(addr, bytes);\n+}\n+\n@@ -230,1 +670,1 @@\n-  void initialize(ZLoadBarrierStubC2* stub) {\n+  void initialize(ZBarrierStubC2* stub) {\n@@ -251,1 +691,5 @@\n-    _gp_regs -= RegSet::range(x18, x27) + RegSet::of(x2) + RegSet::of(x8, x9) + RegSet::of(x5, stub->ref());\n+    if (stub->result() != noreg) {\n+      _gp_regs -= RegSet::range(x18, x27) + RegSet::of(x2) + RegSet::of(x8, x9) + RegSet::of(x5, stub->result());\n+    } else {\n+      _gp_regs -= RegSet::range(x18, x27) + RegSet::of(x2, x5) + RegSet::of(x8, x9);\n+    }\n@@ -254,1 +698,1 @@\n-  ZSaveLiveRegisters(MacroAssembler* masm, ZLoadBarrierStubC2* stub) :\n+  ZSaveLiveRegisters(MacroAssembler* masm, ZBarrierStubC2* stub) :\n@@ -336,1 +780,3 @@\n-  __ bind(*stub->entry());\n+  if (!Compile::current()->output()->in_scratch_emit_size()) {\n+    __ bind(*stub->entry());\n+  }\n@@ -341,7 +787,2 @@\n-\n-    Address target(stub->slow_path());\n-    __ relocate(target.rspec(), [&] {\n-      int32_t offset;\n-      __ la_patchable(t0, target, offset);\n-      __ jalr(x1, t0, offset);\n-    });\n+    __ mv(t0, stub->slow_path());\n+    __ jalr(t0);\n@@ -354,0 +795,41 @@\n+void ZBarrierSetAssembler::generate_c2_store_barrier_stub(MacroAssembler* masm, ZStoreBarrierStubC2* stub) const {\n+  BLOCK_COMMENT(\"ZStoreBarrierStubC2\");\n+\n+  \/\/ Stub entry\n+  __ bind(*stub->entry());\n+\n+  Label slow;\n+  Label slow_continuation;\n+  store_barrier_medium(masm,\n+                       stub->ref_addr(),\n+                       stub->new_zpointer(),\n+                       t1,\n+                       t0,\n+                       stub->is_native(),\n+                       stub->is_atomic(),\n+                       *stub->continuation(),\n+                       slow,\n+                       slow_continuation);\n+\n+  __ bind(slow);\n+\n+  {\n+    ZSaveLiveRegisters save_live_registers(masm, stub);\n+    __ la(c_rarg0, stub->ref_addr());\n+\n+    if (stub->is_native()) {\n+      __ la(t0, RuntimeAddress(ZBarrierSetRuntime::store_barrier_on_native_oop_field_without_healing_addr()));\n+    } else if (stub->is_atomic()) {\n+      __ la(t0, RuntimeAddress(ZBarrierSetRuntime::store_barrier_on_oop_field_with_healing_addr()));\n+    } else {\n+      __ la(t0, RuntimeAddress(ZBarrierSetRuntime::store_barrier_on_oop_field_without_healing_addr()));\n+    }\n+    __ jalr(t0);\n+  }\n+\n+  \/\/ Stub exit\n+  __ j(slow_continuation);\n+}\n+\n+#undef __\n+\n@@ -360,5 +842,43 @@\n-void ZBarrierSetAssembler::generate_c1_load_barrier_test(LIR_Assembler* ce,\n-                                                         LIR_Opr ref) const {\n-  assert_different_registers(xthread, ref->as_register(), t1);\n-  __ ld(t1, address_bad_mask_from_thread(xthread));\n-  __ andr(t1, t1, ref->as_register());\n+static void z_color(LIR_Assembler* ce, LIR_Opr ref) {\n+  __ relocate(barrier_Relocation::spec(), [&] {\n+    __ li16u(t1, barrier_Relocation::unpatched);\n+  }, ZBarrierRelocationFormatStoreGoodBits);\n+  __ slli(ref->as_register(), ref->as_register(), ZPointerLoadShift);\n+  __ orr(ref->as_register(), ref->as_register(), t1);\n+}\n+\n+static void z_uncolor(LIR_Assembler* ce, LIR_Opr ref) {\n+  __ srli(ref->as_register(), ref->as_register(), ZPointerLoadShift);\n+}\n+\n+static void check_color(LIR_Assembler* ce, LIR_Opr ref, bool on_non_strong) {\n+  assert_different_registers(t0, xthread, ref->as_register());\n+  int format = on_non_strong ? ZBarrierRelocationFormatMarkBadMask\n+                             : ZBarrierRelocationFormatLoadBadMask;\n+  Label good;\n+  __ relocate(barrier_Relocation::spec(), [&] {\n+    __ li16u(t0, barrier_Relocation::unpatched);\n+  }, format);\n+  __ andr(t0, ref->as_register(), t0);\n+}\n+\n+void ZBarrierSetAssembler::generate_c1_color(LIR_Assembler* ce, LIR_Opr ref) const {\n+  z_color(ce, ref);\n+}\n+\n+void ZBarrierSetAssembler::generate_c1_uncolor(LIR_Assembler* ce, LIR_Opr ref) const {\n+  z_uncolor(ce, ref);\n+}\n+\n+void ZBarrierSetAssembler::generate_c1_load_barrier(LIR_Assembler* ce,\n+                                                    LIR_Opr ref,\n+                                                    ZLoadBarrierStubC1* stub,\n+                                                    bool on_non_strong) const {\n+  Label good;\n+  check_color(ce, ref, on_non_strong);\n+  __ beqz(t0, good);\n+  __ j(*stub->entry());\n+\n+  __ bind(good);\n+  z_uncolor(ce, ref);\n+  __ bind(*stub->continuation());\n@@ -385,36 +905,35 @@\n-  assert_different_registers(ref, ref_addr, noreg);\n-\n-  \/\/ Save x10 unless it is the result or tmp register\n-  \/\/ Set up SP to accommodate parameters and maybe x10.\n-  if (ref != x10 && tmp != x10) {\n-    __ sub(sp, sp, 32);\n-    __ sd(x10, Address(sp, 16));\n-  } else {\n-    __ sub(sp, sp, 16);\n-  }\n-\n-  \/\/ Setup arguments and call runtime stub\n-  ce->store_parameter(ref_addr, 1);\n-  ce->store_parameter(ref, 0);\n-\n-  __ far_call(stub->runtime_stub());\n-\n-  \/\/ Verify result\n-  __ verify_oop(x10);\n-\n-\n-  \/\/ Move result into place\n-  if (ref != x10) {\n-    __ mv(ref, x10);\n-  }\n-\n-  \/\/ Restore x10 unless it is the result or tmp register\n-  if (ref != x10 && tmp != x10) {\n-    __ ld(x10, Address(sp, 16));\n-    __ add(sp, sp, 32);\n-  } else {\n-    __ add(sp, sp, 16);\n-  }\n-\n-  \/\/ Stub exit\n-  __ j(*stub->continuation());\n+   assert_different_registers(ref, ref_addr, noreg);\n+\n+   \/\/ Save x10 unless it is the result or tmp register\n+   \/\/ Set up SP to accommdate parameters and maybe x10.\n+   if (ref != x10 && tmp != x10) {\n+     __ sub(sp, sp, 32);\n+     __ sd(x10, Address(sp, 16));\n+   } else {\n+     __ sub(sp, sp, 16);\n+   }\n+\n+   \/\/ Setup arguments and call runtime stub\n+   ce->store_parameter(ref_addr, 1);\n+   ce->store_parameter(ref, 0);\n+\n+   __ far_call(stub->runtime_stub());\n+\n+   \/\/ Verify result\n+   __ verify_oop(x10);\n+\n+   \/\/ Move result into place\n+   if (ref != x10) {\n+     __ mv(ref, x10);\n+   }\n+\n+   \/\/ Restore x10 unless it is the result or tmp register\n+   if (ref != x10 && tmp != x10) {\n+     __ ld(x10, Address(sp, 16));\n+     __ addi(sp, sp, 32);\n+   } else {\n+     __ addi(sp, sp, 16);\n+   }\n+\n+   \/\/ Stub exit\n+   __ j(*stub->continuation());\n@@ -443,0 +962,76 @@\n+void ZBarrierSetAssembler::generate_c1_store_barrier_runtime_stub(StubAssembler* sasm,\n+                                                                  bool self_healing) const {\n+  __ prologue(\"zgc_store_barrier stub\", false);\n+\n+  __ push_call_clobbered_registers();\n+\n+  \/\/ Setup arguments\n+  __ load_parameter(0, c_rarg0);\n+\n+  if (self_healing) {\n+    __ call_VM_leaf(ZBarrierSetRuntime::store_barrier_on_oop_field_with_healing_addr(), 1);\n+  } else {\n+    __ call_VM_leaf(ZBarrierSetRuntime::store_barrier_on_oop_field_without_healing_addr(), 1);\n+  }\n+\n+  __ pop_call_clobbered_registers();\n+\n+  __ epilogue();\n+}\n+\n+#undef __\n+#define __ ce->masm()->\n+\n+void ZBarrierSetAssembler::generate_c1_store_barrier(LIR_Assembler* ce,\n+                                                     LIR_Address* addr,\n+                                                     LIR_Opr new_zaddress,\n+                                                     LIR_Opr new_zpointer,\n+                                                     ZStoreBarrierStubC1* stub) const {\n+  Register rnew_zaddress = new_zaddress->as_register();\n+  Register rnew_zpointer = new_zpointer->as_register();\n+\n+  store_barrier_fast(ce->masm(),\n+                     ce->as_Address(addr),\n+                     rnew_zaddress,\n+                     rnew_zpointer,\n+                     t1,\n+                     true,\n+                     stub->is_atomic(),\n+                     *stub->entry(),\n+                     *stub->continuation());\n+}\n+\n+void ZBarrierSetAssembler::generate_c1_store_barrier_stub(LIR_Assembler* ce,\n+                                                          ZStoreBarrierStubC1* stub) const {\n+  \/\/ Stub entry\n+  __ bind(*stub->entry());\n+  Label slow;\n+  Label slow_continuation;\n+  store_barrier_medium(ce->masm(),\n+                       ce->as_Address(stub->ref_addr()->as_address_ptr()),\n+                       t1,\n+                       stub->new_zpointer()->as_register(),\n+                       stub->tmp()->as_pointer_register(),\n+                       false \/* is_native *\/,\n+                       stub->is_atomic(),\n+                       *stub->continuation(),\n+                       slow,\n+                       slow_continuation);\n+\n+  __ bind(slow);\n+\n+  __ la(stub->new_zpointer()->as_register(), ce->as_Address(stub->ref_addr()->as_address_ptr()));\n+\n+  __ sub(sp, sp, 16);\n+  \/\/Setup arguments and call runtime stub\n+  assert(stub->new_zpointer()->is_valid(), \"invariant\");\n+  ce->store_parameter(stub->new_zpointer()->as_register(), 0);\n+  __ far_call(stub->runtime_stub());\n+  __ addi(sp, sp, 16);\n+\n+  \/\/ Stub exit\n+  __ j(slow_continuation);\n+}\n+\n+#undef __\n+\n@@ -449,5 +1044,23 @@\n-  \/\/ Check if mask is good.\n-  \/\/ verifies that ZAddressBadMask & obj == 0\n-  __ ld(tmp2, Address(xthread, ZThreadLocalData::address_bad_mask_offset()));\n-  __ andr(tmp1, obj, tmp2);\n-  __ bnez(tmp1, error);\n+  \/\/ C1 calls verify_oop in the middle of barriers, before they have been uncolored\n+  \/\/ and after being colored. Therefore, we must deal with colored oops as well.\n+  Label done;\n+  Label check_oop;\n+  Label check_zaddress;\n+  int color_bits = ZPointerRemappedShift + ZPointerRemappedBits;\n+\n+  uintptr_t shifted_base_start_mask = (UCONST64(1) << (ZAddressHeapBaseShift + color_bits + 1)) - 1;\n+  uintptr_t shifted_base_end_mask = (UCONST64(1) << (ZAddressHeapBaseShift + 1)) - 1;\n+  uintptr_t shifted_base_mask = shifted_base_start_mask ^ shifted_base_end_mask;\n+\n+  uintptr_t shifted_address_end_mask = (UCONST64(1) << (color_bits + 1)) - 1;\n+  uintptr_t shifted_address_mask = shifted_base_end_mask ^ (uintptr_t)CONST64(-1);\n+\n+  \/\/ Check colored null\n+  __ mv(tmp1, shifted_address_mask);\n+  __ andr(tmp1, tmp1, obj);\n+  __ beqz(tmp1, done);\n+\n+  \/\/ Check for zpointer\n+  __ mv(tmp1, shifted_base_mask);\n+  __ andr(tmp1, tmp1, obj);\n+  __ beqz(tmp1, check_oop);\n@@ -455,1 +1068,19 @@\n-  BarrierSetAssembler::check_oop(masm, obj, tmp1, tmp2, error);\n+  \/\/ Uncolor presumed zpointer\n+  __ srli(obj, obj, ZPointerLoadShift);\n+\n+  __ j(check_zaddress);\n+\n+  __ bind(check_oop);\n+\n+  \/\/ Make sure klass is 'reasonable', which is not zero\n+  __ load_klass(tmp1, obj, tmp2);\n+  __ beqz(tmp1, error);\n+\n+  __ bind(check_zaddress);\n+  \/\/ Check if the oop is the right area of memory\n+  __ mv(tmp1, (intptr_t) Universe::verify_oop_mask());\n+  __ andr(tmp1, tmp1, obj);\n+  __ mv(obj, (intptr_t) Universe::verify_oop_bits());\n+  __ bne(tmp1, obj, error);\n+\n+  __ bind(done);\n","filename":"src\/hotspot\/cpu\/riscv\/gc\/z\/zBarrierSetAssembler_riscv.cpp","additions":756,"deletions":125,"binary":false,"changes":881,"status":"modified"},{"patch":"@@ -2,2 +2,2 @@\n- * Copyright (c) 2019, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2020, 2021, Huawei Technologies Co., Ltd. All rights reserved.\n+ * Copyright (c) 2019, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2023, Huawei Technologies Co., Ltd. All rights reserved.\n@@ -31,0 +31,3 @@\n+#ifdef COMPILER1\n+#include \"c1\/c1_LIR.hpp\"\n+#endif \/\/ COMPILER1\n@@ -32,0 +35,1 @@\n+#include \"gc\/z\/c2\/zBarrierSetC2.hpp\"\n@@ -36,0 +40,1 @@\n+class LIR_Address;\n@@ -40,0 +45,1 @@\n+class ZStoreBarrierStubC1;\n@@ -43,0 +49,1 @@\n+class MachNode;\n@@ -44,1 +51,0 @@\n-class ZLoadBarrierStubC2;\n@@ -47,0 +53,5 @@\n+const int ZBarrierRelocationFormatLoadBadMask   = 0;\n+const int ZBarrierRelocationFormatMarkBadMask   = 1;\n+const int ZBarrierRelocationFormatStoreGoodBits = 2;\n+const int ZBarrierRelocationFormatStoreBadMask  = 3;\n+\n@@ -57,1 +68,21 @@\n-#ifdef ASSERT\n+  void store_barrier_fast(MacroAssembler* masm,\n+                          Address ref_addr,\n+                          Register rnew_zaddress,\n+                          Register rnew_zpointer,\n+                          Register rtmp,\n+                          bool in_nmethod,\n+                          bool is_atomic,\n+                          Label& medium_path,\n+                          Label& medium_path_continuation) const;\n+\n+  void store_barrier_medium(MacroAssembler* masm,\n+                            Address ref_addr,\n+                            Register rtmp1,\n+                            Register rtmp2,\n+                            Register rtmp3,\n+                            bool is_native,\n+                            bool is_atomic,\n+                            Label& medium_path_continuation,\n+                            Label& slow_path,\n+                            Label& slow_path_continuation) const;\n+\n@@ -66,1 +97,0 @@\n-#endif \/\/ ASSERT\n@@ -76,0 +106,20 @@\n+  virtual void copy_load_at(MacroAssembler* masm,\n+                            DecoratorSet decorators,\n+                            BasicType type,\n+                            size_t bytes,\n+                            Register dst,\n+                            Address src,\n+                            Register tmp);\n+\n+  virtual void copy_store_at(MacroAssembler* masm,\n+                             DecoratorSet decorators,\n+                             BasicType type,\n+                             size_t bytes,\n+                             Address dst,\n+                             Register src,\n+                             Register tmp1,\n+                             Register tmp2,\n+                             Register tmp3);\n+\n+  virtual bool supports_rvv_arraycopy();\n+\n@@ -82,1 +132,5 @@\n-  virtual NMethodPatchingType nmethod_patching_type() { return NMethodPatchingType::conc_data_patch; }\n+  virtual NMethodPatchingType nmethod_patching_type() { return NMethodPatchingType::conc_instruction_and_data_patch; }\n+\n+  void patch_barrier_relocation(address addr, int format);\n+\n+  void patch_barriers() {}\n@@ -85,0 +139,3 @@\n+  void generate_c1_color(LIR_Assembler* ce, LIR_Opr ref) const;\n+  void generate_c1_uncolor(LIR_Assembler* ce, LIR_Opr ref) const;\n+\n@@ -87,0 +144,4 @@\n+  void generate_c1_load_barrier(LIR_Assembler* ce,\n+                                LIR_Opr ref,\n+                                ZLoadBarrierStubC1* stub,\n+                                bool on_non_strong) const;\n@@ -93,0 +154,12 @@\n+\n+  void generate_c1_store_barrier(LIR_Assembler* ce,\n+                                 LIR_Address* addr,\n+                                 LIR_Opr new_zaddress,\n+                                 LIR_Opr new_zpointer,\n+                                 ZStoreBarrierStubC1* stub) const;\n+\n+  void generate_c1_store_barrier_stub(LIR_Assembler* ce,\n+                                      ZStoreBarrierStubC1* stub) const;\n+\n+  void generate_c1_store_barrier_runtime_stub(StubAssembler* sasm,\n+                                              bool self_healing) const;\n@@ -101,0 +174,2 @@\n+  void generate_c2_store_barrier_stub(MacroAssembler* masm,\n+                                      ZStoreBarrierStubC2* stub) const;\n","filename":"src\/hotspot\/cpu\/riscv\/gc\/z\/zBarrierSetAssembler_riscv.hpp","additions":81,"deletions":6,"binary":false,"changes":87,"status":"modified"},{"patch":"@@ -2,2 +2,2 @@\n- * Copyright (c) 2015, 2022, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2020, 2021, Huawei Technologies Co., Ltd. All rights reserved.\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2023, Huawei Technologies Co., Ltd. All rights reserved.\n@@ -29,2 +29,1 @@\n-const size_t ZPlatformHeapViews        = 3;\n-const size_t ZPlatformCacheLineSize    = 64;\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -32,2 +31,1 @@\n-size_t ZPlatformAddressOffsetBits();\n-size_t ZPlatformAddressMetadataShift();\n+const size_t ZPlatformCacheLineSize    = 64;\n","filename":"src\/hotspot\/cpu\/riscv\/gc\/z\/zGlobals_riscv.hpp","additions":4,"deletions":6,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -2,2 +2,2 @@\n-\/\/ Copyright (c) 2019, 2021, Oracle and\/or its affiliates. All rights reserved.\n-\/\/ Copyright (c) 2020, 2021, Huawei Technologies Co., Ltd. All rights reserved.\n+\/\/ Copyright (c) 2019, 2023, Oracle and\/or its affiliates. All rights reserved.\n+\/\/ Copyright (c) 2020, 2023, Huawei Technologies Co., Ltd. All rights reserved.\n@@ -34,0 +34,1 @@\n+#include \"gc\/z\/zBarrierSetAssembler.hpp\"\n@@ -35,2 +36,30 @@\n-static void z_load_barrier(MacroAssembler& _masm, const MachNode* node, Address ref_addr, Register ref, Register tmp, int barrier_data) {\n-  if (barrier_data == ZLoadBarrierElided) {\n+static void z_color(MacroAssembler& _masm, const MachNode* node, Register dst, Register src, Register tmp) {\n+  assert_different_registers(dst, tmp);\n+\n+  __ relocate(barrier_Relocation::spec(), [&] {\n+    __ li16u(tmp, barrier_Relocation::unpatched);\n+  }, ZBarrierRelocationFormatStoreGoodBits);\n+  __ slli(dst, src, ZPointerLoadShift);\n+  __ orr(dst, dst, tmp);\n+}\n+\n+static void z_uncolor(MacroAssembler& _masm, const MachNode* node, Register ref) {\n+  __ srli(ref, ref, ZPointerLoadShift);\n+}\n+\n+static void check_color(MacroAssembler& _masm, Register ref, bool on_non_strong, Register result) {\n+  int format = on_non_strong ? ZBarrierRelocationFormatMarkBadMask\n+                             : ZBarrierRelocationFormatLoadBadMask;\n+  __ relocate(barrier_Relocation::spec(), [&] {\n+    __ li16u(result, barrier_Relocation::unpatched);\n+  }, format);\n+  __ andr(result, ref, result);\n+}\n+\n+static void z_load_barrier(MacroAssembler& _masm, const MachNode* node, Address ref_addr, Register ref, Register tmp) {\n+  const bool on_non_strong =\n+      ((node->barrier_data() & ZBarrierWeak) != 0) ||\n+      ((node->barrier_data() & ZBarrierPhantom) != 0);\n+\n+  if (node->barrier_data() == ZBarrierElided) {\n+    z_uncolor(_masm, node, ref);\n@@ -39,6 +68,0 @@\n-  ZLoadBarrierStubC2* const stub = ZLoadBarrierStubC2::create(node, ref_addr, ref, tmp, barrier_data);\n-  __ ld(tmp, Address(xthread, ZThreadLocalData::address_bad_mask_offset()));\n-  __ andr(tmp, tmp, ref);\n-  __ bnez(tmp, *stub->entry(), true \/* far *\/);\n-  __ bind(*stub->continuation());\n-}\n@@ -46,2 +69,4 @@\n-static void z_load_barrier_slow_path(MacroAssembler& _masm, const MachNode* node, Address ref_addr, Register ref, Register tmp) {\n-  ZLoadBarrierStubC2* const stub = ZLoadBarrierStubC2::create(node, ref_addr, ref, tmp, ZLoadBarrierStrong);\n+  ZLoadBarrierStubC2* const stub = ZLoadBarrierStubC2::create(node, ref_addr, ref);\n+  Label good;\n+  check_color(_masm, ref, on_non_strong, tmp);\n+  __ beqz(tmp, good);\n@@ -49,0 +74,3 @@\n+\n+  __ bind(good);\n+  z_uncolor(_masm, node, ref);\n@@ -52,0 +80,10 @@\n+static void z_store_barrier(MacroAssembler& _masm, const MachNode* node, Address ref_addr, Register rnew_zaddress, Register rnew_zpointer, Register tmp, bool is_atomic) {\n+  if (node->barrier_data() == ZBarrierElided) {\n+    z_color(_masm, node, rnew_zpointer, rnew_zaddress, t0);\n+  } else {\n+    bool is_native = (node->barrier_data() & ZBarrierNative) != 0;\n+    ZStoreBarrierStubC2* const stub = ZStoreBarrierStubC2::create(node, ref_addr, rnew_zaddress, rnew_zpointer, is_native, is_atomic);\n+    ZBarrierSetAssembler* bs_asm = ZBarrierSet::assembler();\n+    bs_asm->store_barrier_fast(&_masm, ref_addr, rnew_zaddress, rnew_zpointer, tmp, true \/* in_nmethod *\/, is_atomic, *stub->entry(), *stub->continuation());\n+  }\n+}\n@@ -58,1 +96,1 @@\n-  predicate(UseZGC && (n->as_Load()->barrier_data() != 0));\n+  predicate(UseZGC && ZGenerational && n->as_Load()->barrier_data() != 0);\n@@ -66,1 +104,1 @@\n-    const Address ref_addr (as_Register($mem$$base), $mem$$disp);\n+    const Address ref_addr(as_Register($mem$$base), $mem$$disp);\n@@ -68,1 +106,1 @@\n-    z_load_barrier(_masm, this, ref_addr, $dst$$Register, t0 \/* tmp *\/, barrier_data());\n+    z_load_barrier(_masm, this, ref_addr, $dst$$Register, t0);\n@@ -74,1 +112,18 @@\n-instruct zCompareAndSwapP(iRegINoSp res, indirect mem, iRegP oldval, iRegP newval, rFlagsReg cr) %{\n+\/\/ Store Pointer\n+instruct zStoreP(memory mem, iRegP src, iRegPNoSp tmp, rFlagsReg cr)\n+%{\n+  predicate(UseZGC && ZGenerational && n->as_Store()->barrier_data() != 0);\n+  match(Set mem (StoreP mem src));\n+  effect(TEMP tmp, KILL cr);\n+\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"sd    $mem, $src\\t# ptr\" %}\n+  ins_encode %{\n+    const Address ref_addr(as_Register($mem$$base), $mem$$disp);\n+    z_store_barrier(_masm, this, ref_addr, $src$$Register, $tmp$$Register, t1, false \/* is_atomic *\/);\n+    __ sd($tmp$$Register, ref_addr);\n+  %}\n+  ins_pipe(pipe_serial);\n+%}\n+\n+instruct zCompareAndSwapP(iRegINoSp res, indirect mem, iRegP oldval, iRegP newval, iRegPNoSp oldval_tmp, iRegPNoSp newval_tmp, rFlagsReg cr) %{\n@@ -77,2 +132,2 @@\n-  predicate(UseZGC && !needs_acquiring_load_reserved(n) && n->as_LoadStore()->barrier_data() == ZLoadBarrierStrong);\n-  effect(KILL cr, TEMP_DEF res);\n+  predicate(UseZGC && ZGenerational && !needs_acquiring_load_reserved(n) && n->as_LoadStore()->barrier_data() != 0);\n+  effect(TEMP oldval_tmp, TEMP newval_tmp, KILL cr, TEMP_DEF res);\n@@ -86,19 +141,5 @@\n-    Label failed;\n-    guarantee($mem$$index == -1 && $mem$$disp == 0, \"impossible encoding\");\n-    __ cmpxchg($mem$$Register, $oldval$$Register, $newval$$Register, Assembler::int64,\n-               Assembler::relaxed \/* acquire *\/, Assembler::rl \/* release *\/, $res$$Register,\n-               true \/* result_as_bool *\/);\n-    __ beqz($res$$Register, failed);\n-    __ mv(t0, $oldval$$Register);\n-    __ bind(failed);\n-    if (barrier_data() != ZLoadBarrierElided) {\n-      Label good;\n-      __ ld(t1, Address(xthread, ZThreadLocalData::address_bad_mask_offset()), t1 \/* tmp *\/);\n-      __ andr(t1, t1, t0);\n-      __ beqz(t1, good);\n-      z_load_barrier_slow_path(_masm, this, Address($mem$$Register), t0 \/* ref *\/, t1 \/* tmp *\/);\n-      __ cmpxchg($mem$$Register, $oldval$$Register, $newval$$Register, Assembler::int64,\n-                 Assembler::relaxed \/* acquire *\/, Assembler::rl \/* release *\/, $res$$Register,\n-                 true \/* result_as_bool *\/);\n-      __ bind(good);\n-    }\n+    guarantee($mem$$disp == 0, \"impossible encoding\");\n+    Address ref_addr($mem$$Register);\n+    z_color(_masm, this, $oldval_tmp$$Register, $oldval$$Register, t0);\n+    z_store_barrier(_masm, this, ref_addr, $newval$$Register, $newval_tmp$$Register, t1, true \/* is_atomic *\/);\n+    __ cmpxchg($mem$$Register, $oldval_tmp$$Register, $newval_tmp$$Register, Assembler::int64, Assembler::relaxed \/* acquire *\/, Assembler::rl \/* release *\/, $res$$Register, true \/* result_as_bool *\/);\n@@ -110,1 +151,1 @@\n-instruct zCompareAndSwapPAcq(iRegINoSp res, indirect mem, iRegP oldval, iRegP newval, rFlagsReg cr) %{\n+instruct zCompareAndSwapPAcq(iRegINoSp res, indirect mem, iRegP oldval, iRegP newval, iRegPNoSp oldval_tmp, iRegPNoSp newval_tmp, rFlagsReg cr) %{\n@@ -113,2 +154,2 @@\n-  predicate(UseZGC && needs_acquiring_load_reserved(n) && (n->as_LoadStore()->barrier_data() == ZLoadBarrierStrong));\n-  effect(KILL cr, TEMP_DEF res);\n+  predicate(UseZGC && ZGenerational && needs_acquiring_load_reserved(n) && n->as_LoadStore()->barrier_data() != 0);\n+  effect(TEMP oldval_tmp, TEMP newval_tmp, KILL cr, TEMP_DEF res);\n@@ -122,19 +163,5 @@\n-    Label failed;\n-    guarantee($mem$$index == -1 && $mem$$disp == 0, \"impossible encoding\");\n-    __ cmpxchg($mem$$Register, $oldval$$Register, $newval$$Register, Assembler::int64,\n-               Assembler::aq \/* acquire *\/, Assembler::rl \/* release *\/, $res$$Register,\n-               true \/* result_as_bool *\/);\n-    __ beqz($res$$Register, failed);\n-    __ mv(t0, $oldval$$Register);\n-    __ bind(failed);\n-    if (barrier_data() != ZLoadBarrierElided) {\n-      Label good;\n-      __ ld(t1, Address(xthread, ZThreadLocalData::address_bad_mask_offset()), t1 \/* tmp *\/);\n-      __ andr(t1, t1, t0);\n-      __ beqz(t1, good);\n-      z_load_barrier_slow_path(_masm, this, Address($mem$$Register), t0 \/* ref *\/, t1 \/* tmp *\/);\n-      __ cmpxchg($mem$$Register, $oldval$$Register, $newval$$Register, Assembler::int64,\n-                 Assembler::aq \/* acquire *\/, Assembler::rl \/* release *\/, $res$$Register,\n-                 true \/* result_as_bool *\/);\n-      __ bind(good);\n-    }\n+    guarantee($mem$$disp == 0, \"impossible encoding\");\n+    Address ref_addr($mem$$Register);\n+    z_color(_masm, this, $oldval_tmp$$Register, $oldval$$Register, t0);\n+    z_store_barrier(_masm, this, ref_addr, $newval$$Register, $newval_tmp$$Register, t1, true \/* is_atomic *\/);\n+    __ cmpxchg($mem$$Register, $oldval_tmp$$Register, $newval_tmp$$Register, Assembler::int64, Assembler::aq \/* acquire *\/, Assembler::rl \/* release *\/, $res$$Register, true \/* result_as_bool *\/);\n@@ -146,1 +173,1 @@\n-instruct zCompareAndExchangeP(iRegPNoSp res, indirect mem, iRegP oldval, iRegP newval) %{\n+instruct zCompareAndExchangeP(iRegPNoSp res, indirect mem, iRegP oldval, iRegP newval, iRegPNoSp oldval_tmp, iRegPNoSp newval_tmp, rFlagsReg cr) %{\n@@ -148,2 +175,2 @@\n-  predicate(UseZGC && !needs_acquiring_load_reserved(n) && n->as_LoadStore()->barrier_data() == ZLoadBarrierStrong);\n-  effect(TEMP_DEF res);\n+  predicate(UseZGC && ZGenerational && !needs_acquiring_load_reserved(n) && n->as_LoadStore()->barrier_data() != 0);\n+  effect(TEMP oldval_tmp, TEMP newval_tmp, KILL cr, TEMP_DEF res);\n@@ -156,13 +183,6 @@\n-    guarantee($mem$$index == -1 && $mem$$disp == 0, \"impossible encoding\");\n-    __ cmpxchg($mem$$Register, $oldval$$Register, $newval$$Register, Assembler::int64,\n-               Assembler::relaxed \/* acquire *\/, Assembler::rl \/* release *\/, $res$$Register);\n-    if (barrier_data() != ZLoadBarrierElided) {\n-      Label good;\n-      __ ld(t0, Address(xthread, ZThreadLocalData::address_bad_mask_offset()));\n-      __ andr(t0, t0, $res$$Register);\n-      __ beqz(t0, good);\n-      z_load_barrier_slow_path(_masm, this, Address($mem$$Register), $res$$Register \/* ref *\/, t0 \/* tmp *\/);\n-      __ cmpxchg($mem$$Register, $oldval$$Register, $newval$$Register, Assembler::int64,\n-                 Assembler::relaxed \/* acquire *\/, Assembler::rl \/* release *\/, $res$$Register);\n-      __ bind(good);\n-    }\n+    guarantee($mem$$disp == 0, \"impossible encoding\");\n+    Address ref_addr($mem$$Register);\n+    z_color(_masm, this, $oldval_tmp$$Register, $oldval$$Register, t0);\n+    z_store_barrier(_masm, this, ref_addr, $newval$$Register, $newval_tmp$$Register, t1, true \/* is_atomic *\/);\n+    __ cmpxchg($mem$$Register, $oldval_tmp$$Register, $newval_tmp$$Register, Assembler::int64, Assembler::relaxed \/* acquire *\/, Assembler::rl \/* release *\/, $res$$Register);\n+    z_uncolor(_masm, this, $res$$Register);\n@@ -174,1 +194,1 @@\n-instruct zCompareAndExchangePAcq(iRegPNoSp res, indirect mem, iRegP oldval, iRegP newval) %{\n+instruct zCompareAndExchangePAcq(iRegPNoSp res, indirect mem, iRegP oldval, iRegP newval, iRegPNoSp oldval_tmp, iRegPNoSp newval_tmp, rFlagsReg cr) %{\n@@ -176,2 +196,2 @@\n-  predicate(UseZGC && needs_acquiring_load_reserved(n) && n->as_LoadStore()->barrier_data() == ZLoadBarrierStrong);\n-  effect(TEMP_DEF res);\n+  predicate(UseZGC && ZGenerational && needs_acquiring_load_reserved(n) && n->as_LoadStore()->barrier_data() != 0);\n+  effect(TEMP oldval_tmp, TEMP newval_tmp, KILL cr, TEMP_DEF res);\n@@ -184,13 +204,6 @@\n-    guarantee($mem$$index == -1 && $mem$$disp == 0, \"impossible encoding\");\n-    __ cmpxchg($mem$$Register, $oldval$$Register, $newval$$Register, Assembler::int64,\n-               Assembler::aq \/* acquire *\/, Assembler::rl \/* release *\/, $res$$Register);\n-    if (barrier_data() != ZLoadBarrierElided) {\n-      Label good;\n-      __ ld(t0, Address(xthread, ZThreadLocalData::address_bad_mask_offset()));\n-      __ andr(t0, t0, $res$$Register);\n-      __ beqz(t0, good);\n-      z_load_barrier_slow_path(_masm, this, Address($mem$$Register), $res$$Register \/* ref *\/, t0 \/* tmp *\/);\n-      __ cmpxchg($mem$$Register, $oldval$$Register, $newval$$Register, Assembler::int64,\n-                 Assembler::aq \/* acquire *\/, Assembler::rl \/* release *\/, $res$$Register);\n-      __ bind(good);\n-    }\n+    guarantee($mem$$disp == 0, \"impossible encoding\");\n+    Address ref_addr($mem$$Register);\n+    z_color(_masm, this, $oldval_tmp$$Register, $oldval$$Register, t0);\n+    z_store_barrier(_masm, this, ref_addr, $newval$$Register, $newval_tmp$$Register, t1, true \/* is_atomic *\/);\n+    __ cmpxchg($mem$$Register, $oldval_tmp$$Register, $newval_tmp$$Register, Assembler::int64, Assembler::aq \/* acquire *\/, Assembler::rl \/* release *\/, $res$$Register);\n+    z_uncolor(_masm, this, $res$$Register);\n@@ -204,1 +217,1 @@\n-  predicate(UseZGC && !needs_acquiring_load_reserved(n) && n->as_LoadStore()->barrier_data() != 0);\n+  predicate(UseZGC && ZGenerational && !needs_acquiring_load_reserved(n) && n->as_LoadStore()->barrier_data() != 0);\n@@ -212,2 +225,3 @@\n-    __ atomic_xchg($prev$$Register, $newv$$Register, as_Register($mem$$base));\n-    z_load_barrier(_masm, this, Address(noreg, 0), $prev$$Register, t0 \/* tmp *\/, barrier_data());\n+    z_store_barrier(_masm, this, Address($mem$$Register), $newv$$Register, $prev$$Register, t1, true \/* is_atomic *\/);\n+    __ atomic_xchg($prev$$Register, $prev$$Register, $mem$$Register);\n+    z_uncolor(_masm, this, $prev$$Register);\n@@ -221,1 +235,1 @@\n-  predicate(UseZGC && needs_acquiring_load_reserved(n) && (n->as_LoadStore()->barrier_data() != 0));\n+  predicate(UseZGC && ZGenerational && needs_acquiring_load_reserved(n) && n->as_LoadStore()->barrier_data() != 0);\n@@ -224,1 +238,1 @@\n-  ins_cost(VOLATILE_REF_COST);\n+  ins_cost(2 * VOLATILE_REF_COST);\n@@ -229,2 +243,3 @@\n-    __ atomic_xchgal($prev$$Register, $newv$$Register, as_Register($mem$$base));\n-    z_load_barrier(_masm, this, Address(noreg, 0), $prev$$Register, t0 \/* tmp *\/, barrier_data());\n+    z_store_barrier(_masm, this, Address($mem$$Register), $newv$$Register, $prev$$Register, t1, true \/* is_atomic *\/);\n+    __ atomic_xchgal($prev$$Register, $prev$$Register, $mem$$Register);\n+    z_uncolor(_masm, this, $prev$$Register);\n","filename":"src\/hotspot\/cpu\/riscv\/gc\/z\/z_riscv64.ad","additions":113,"deletions":98,"binary":false,"changes":211,"status":"modified"},{"patch":"@@ -565,2 +565,2 @@\n-  andi(t0, value, JNIHandles::tag_mask);\n-  bnez(t0, tagged);\n+  andi(tmp1, value, JNIHandles::tag_mask);\n+  bnez(tmp1, tagged);\n@@ -575,2 +575,3 @@\n-  test_bit(t0, value, exact_log2(JNIHandles::TypeTag::weak_global));\n-  bnez(t0, weak_tagged);\n+  STATIC_ASSERT(JNIHandles::TypeTag::weak_global == 0b1);\n+  test_bit(tmp1, value, exact_log2(JNIHandles::TypeTag::weak_global));\n+  bnez(tmp1, weak_tagged);\n@@ -581,0 +582,1 @@\n+  verify_oop(value);\n@@ -600,0 +602,1 @@\n+    STATIC_ASSERT(JNIHandles::TypeTag::global == 0b10);\n@@ -601,2 +604,2 @@\n-    test_bit(t0, value, exact_log2(JNIHandles::TypeTag::global)); \/\/ Test for global tag.\n-    bnez(t0, valid_global_tag);\n+    test_bit(tmp1, value, exact_log2(JNIHandles::TypeTag::global)); \/\/ Test for global tag.\n+    bnez(tmp1, valid_global_tag);\n@@ -757,0 +760,5 @@\n+void MacroAssembler::li16u(Register Rd, int32_t imm) {\n+  lui(Rd, imm << 12);\n+  srli(Rd, Rd, 12);\n+}\n+\n@@ -1406,0 +1414,5 @@\n+static int patch_imm_in_li16u(address branch, int32_t target) {\n+  Assembler::patch(branch, 31, 12, target & 0xfffff); \/\/ patch lui only\n+  return NativeInstruction::instruction_size;\n+}\n+\n@@ -1495,0 +1508,3 @@\n+  } else if (NativeInstruction::is_li16u_at(branch)) {\n+    int64_t imm = (intptr_t)target;\n+    return patch_imm_in_li16u(branch, (int32_t)imm);\n@@ -2428,0 +2444,4 @@\n+  assert_different_registers(addr, tmp);\n+  assert_different_registers(newv, tmp);\n+  assert_different_registers(oldv, tmp);\n+\n@@ -2614,0 +2634,3 @@\n+  assert_different_registers(addr, t0);\n+  assert_different_registers(expected, t0);\n+  assert_different_registers(new_val, t0);\n@@ -2646,0 +2669,4 @@\n+  assert_different_registers(addr, t0);\n+  assert_different_registers(expected, t0);\n+  assert_different_registers(new_val, t0);\n+\n","filename":"src\/hotspot\/cpu\/riscv\/macroAssembler_riscv.cpp","additions":33,"deletions":6,"binary":false,"changes":39,"status":"modified"},{"patch":"@@ -692,0 +692,1 @@\n+  void li16u(Register Rd, int32_t imm);\n","filename":"src\/hotspot\/cpu\/riscv\/macroAssembler_riscv.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -4,1 +4,1 @@\n- * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * Copyright (c) 2020, 2023, Huawei Technologies Co., Ltd. All rights reserved.\n@@ -100,0 +100,6 @@\n+bool NativeInstruction::is_li16u_at(address instr) {\n+  return is_lui_at(instr) && \/\/ lui\n+         is_srli_at(instr + instruction_size) && \/\/ srli\n+         check_li16u_data_dependency(instr);\n+}\n+\n","filename":"src\/hotspot\/cpu\/riscv\/nativeInst_riscv.cpp","additions":7,"deletions":1,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -4,1 +4,1 @@\n- * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * Copyright (c) 2020, 2023, Huawei Technologies Co., Ltd. All rights reserved.\n@@ -84,0 +84,8 @@\n+\n+  static bool is_srli_at(address instr) {\n+    assert_cond(instr != nullptr);\n+    return extract_opcode(instr) == 0b0010011 &&\n+           extract_funct3(instr) == 0b101 &&\n+           Assembler::extract(((unsigned*)instr)[0], 31, 26) == 0b000000;\n+  }\n+\n@@ -156,0 +164,11 @@\n+  \/\/ the instruction sequence of li16u is as below:\n+  \/\/     lui\n+  \/\/     srli\n+  static bool check_li16u_data_dependency(address instr) {\n+    address lui = instr;\n+    address srli = lui + instruction_size;\n+\n+    return extract_rs1(srli) == extract_rd(lui) &&\n+           extract_rs1(srli) == extract_rd(srli);\n+  }\n+\n@@ -189,0 +208,1 @@\n+  static bool is_li16u_at(address instr);\n","filename":"src\/hotspot\/cpu\/riscv\/nativeInst_riscv.hpp","additions":21,"deletions":1,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -2,2 +2,2 @@\n- * Copyright (c) 1997, 2019, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2020, 2021, Huawei Technologies Co., Ltd. All rights reserved.\n+ * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2023, Huawei Technologies Co., Ltd. All rights reserved.\n@@ -35,1 +35,2 @@\n-    format_width       =  1\n+    \/\/ Must be at least 2 for ZGC GC barrier patching.\n+    format_width       =  2\n","filename":"src\/hotspot\/cpu\/riscv\/relocInfo_riscv.hpp","additions":4,"deletions":3,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -54,3 +54,0 @@\n-#if INCLUDE_ZGC\n-#include \"gc\/z\/zThreadLocalData.hpp\"\n-#endif\n@@ -960,1 +957,5 @@\n-    __ beqz(count, done);\n+    \/\/ The size of copy32_loop body increases significantly with ZGC GC barriers.\n+    \/\/ Need conditional far branches to reach a point beyond the loop in this case.\n+    bool is_far = UseZGC && ZGenerational;\n+\n+    __ beqz(count, done, is_far);\n@@ -974,1 +975,1 @@\n-      __ bgez(t0, copy8_loop);\n+      __ bgez(t0, copy8_loop, is_far);\n@@ -978,1 +979,1 @@\n-      __ blt(cnt, t0, copy_small);\n+      __ blt(cnt, t0, copy_small, is_far);\n@@ -982,1 +983,1 @@\n-      __ bnez(t0, copy_small);\n+      __ bnez(t0, copy_small, is_far);\n@@ -998,1 +999,1 @@\n-      __ beqz(cnt, done);\n+      __ beqz(cnt, done, is_far);\n@@ -1003,1 +1004,1 @@\n-      __ blt(cnt, t0, copy8_loop);\n+      __ blt(cnt, t0, copy8_loop, is_far);\n@@ -1005,0 +1006,1 @@\n+\n@@ -1011,2 +1013,2 @@\n-    bs_asm->copy_load_at(_masm, decorators, type, 8, tmp3, Address(src), gct1);\n-    bs_asm->copy_load_at(_masm, decorators, type, 8, tmp4, Address(src, 8), gct1);\n+    bs_asm->copy_load_at(_masm, decorators, type, 8, tmp3, Address(src),     gct1);\n+    bs_asm->copy_load_at(_masm, decorators, type, 8, tmp4, Address(src, 8),  gct1);\n@@ -1016,2 +1018,2 @@\n-    bs_asm->copy_store_at(_masm, decorators, type, 8, Address(dst), tmp3, gct1, gct2, gct3);\n-    bs_asm->copy_store_at(_masm, decorators, type, 8, Address(dst, 8), tmp4, gct1, gct2, gct3);\n+    bs_asm->copy_store_at(_masm, decorators, type, 8, Address(dst),     tmp3, gct1, gct2, gct3);\n+    bs_asm->copy_store_at(_masm, decorators, type, 8, Address(dst, 8),  tmp4, gct1, gct2, gct3);\n@@ -3734,1 +3736,1 @@\n-    const int insts_size = 512;\n+    const int insts_size = 1024;\n@@ -3973,1 +3975,1 @@\n-    int insts_size = 512;\n+    int insts_size = 1024;\n","filename":"src\/hotspot\/cpu\/riscv\/stubGenerator_riscv.cpp","additions":17,"deletions":15,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -4,1 +4,1 @@\n- * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * Copyright (c) 2020, 2023, Huawei Technologies Co., Ltd. All rights reserved.\n@@ -42,2 +42,2 @@\n-  _compiler_stubs_code_size     = 28000,\n-  _final_stubs_code_size        = 28000\n+  _compiler_stubs_code_size     = 128000,\n+  _final_stubs_code_size        = 128000\n","filename":"src\/hotspot\/cpu\/riscv\/stubRoutines_riscv.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -824,0 +824,1 @@\n+ public:\n@@ -845,0 +846,1 @@\n+ protected:\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1353,2 +1353,2 @@\n-    \/\/ Load barrier has not yet been applied, so ZGC can't verify the oop here\n-    if (!UseZGC) {\n+    if (!(UseZGC && !ZGenerational)) {\n+      \/\/ Load barrier has not yet been applied, so ZGC can't verify the oop here\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRAssembler_x86.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -0,0 +1,713 @@\n+\/*\n+ * Copyright (c) 2018, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"asm\/macroAssembler.inline.hpp\"\n+#include \"code\/codeBlob.hpp\"\n+#include \"code\/vmreg.inline.hpp\"\n+#include \"gc\/x\/xBarrier.inline.hpp\"\n+#include \"gc\/x\/xBarrierSet.hpp\"\n+#include \"gc\/x\/xBarrierSetAssembler.hpp\"\n+#include \"gc\/x\/xBarrierSetRuntime.hpp\"\n+#include \"gc\/x\/xThreadLocalData.hpp\"\n+#include \"memory\/resourceArea.hpp\"\n+#include \"runtime\/sharedRuntime.hpp\"\n+#include \"utilities\/macros.hpp\"\n+#ifdef COMPILER1\n+#include \"c1\/c1_LIRAssembler.hpp\"\n+#include \"c1\/c1_MacroAssembler.hpp\"\n+#include \"gc\/x\/c1\/xBarrierSetC1.hpp\"\n+#endif \/\/ COMPILER1\n+#ifdef COMPILER2\n+#include \"gc\/x\/c2\/xBarrierSetC2.hpp\"\n+#endif \/\/ COMPILER2\n+\n+#ifdef PRODUCT\n+#define BLOCK_COMMENT(str) \/* nothing *\/\n+#else\n+#define BLOCK_COMMENT(str) __ block_comment(str)\n+#endif\n+\n+#undef __\n+#define __ masm->\n+\n+static void call_vm(MacroAssembler* masm,\n+                    address entry_point,\n+                    Register arg0,\n+                    Register arg1) {\n+  \/\/ Setup arguments\n+  if (arg1 == c_rarg0) {\n+    if (arg0 == c_rarg1) {\n+      __ xchgptr(c_rarg1, c_rarg0);\n+    } else {\n+      __ movptr(c_rarg1, arg1);\n+      __ movptr(c_rarg0, arg0);\n+    }\n+  } else {\n+    if (arg0 != c_rarg0) {\n+      __ movptr(c_rarg0, arg0);\n+    }\n+    if (arg1 != c_rarg1) {\n+      __ movptr(c_rarg1, arg1);\n+    }\n+  }\n+\n+  \/\/ Call VM\n+  __ MacroAssembler::call_VM_leaf_base(entry_point, 2);\n+}\n+\n+void XBarrierSetAssembler::load_at(MacroAssembler* masm,\n+                                   DecoratorSet decorators,\n+                                   BasicType type,\n+                                   Register dst,\n+                                   Address src,\n+                                   Register tmp1,\n+                                   Register tmp_thread) {\n+  if (!XBarrierSet::barrier_needed(decorators, type)) {\n+    \/\/ Barrier not needed\n+    BarrierSetAssembler::load_at(masm, decorators, type, dst, src, tmp1, tmp_thread);\n+    return;\n+  }\n+\n+  BLOCK_COMMENT(\"XBarrierSetAssembler::load_at {\");\n+\n+  \/\/ Allocate scratch register\n+  Register scratch = tmp1;\n+  if (tmp1 == noreg) {\n+    scratch = r12;\n+    __ push(scratch);\n+  }\n+\n+  assert_different_registers(dst, scratch);\n+\n+  Label done;\n+\n+  \/\/\n+  \/\/ Fast Path\n+  \/\/\n+\n+  \/\/ Load address\n+  __ lea(scratch, src);\n+\n+  \/\/ Load oop at address\n+  __ movptr(dst, Address(scratch, 0));\n+\n+  \/\/ Test address bad mask\n+  __ testptr(dst, address_bad_mask_from_thread(r15_thread));\n+  __ jcc(Assembler::zero, done);\n+\n+  \/\/\n+  \/\/ Slow path\n+  \/\/\n+\n+  \/\/ Save registers\n+  __ push(rax);\n+  __ push(rcx);\n+  __ push(rdx);\n+  __ push(rdi);\n+  __ push(rsi);\n+  __ push(r8);\n+  __ push(r9);\n+  __ push(r10);\n+  __ push(r11);\n+\n+  \/\/ We may end up here from generate_native_wrapper, then the method may have\n+  \/\/ floats as arguments, and we must spill them before calling the VM runtime\n+  \/\/ leaf. From the interpreter all floats are passed on the stack.\n+  assert(Argument::n_float_register_parameters_j == 8, \"Assumption\");\n+  const int xmm_size = wordSize * 2;\n+  const int xmm_spill_size = xmm_size * Argument::n_float_register_parameters_j;\n+  __ subptr(rsp, xmm_spill_size);\n+  __ movdqu(Address(rsp, xmm_size * 7), xmm7);\n+  __ movdqu(Address(rsp, xmm_size * 6), xmm6);\n+  __ movdqu(Address(rsp, xmm_size * 5), xmm5);\n+  __ movdqu(Address(rsp, xmm_size * 4), xmm4);\n+  __ movdqu(Address(rsp, xmm_size * 3), xmm3);\n+  __ movdqu(Address(rsp, xmm_size * 2), xmm2);\n+  __ movdqu(Address(rsp, xmm_size * 1), xmm1);\n+  __ movdqu(Address(rsp, xmm_size * 0), xmm0);\n+\n+  \/\/ Call VM\n+  call_vm(masm, XBarrierSetRuntime::load_barrier_on_oop_field_preloaded_addr(decorators), dst, scratch);\n+\n+  __ movdqu(xmm0, Address(rsp, xmm_size * 0));\n+  __ movdqu(xmm1, Address(rsp, xmm_size * 1));\n+  __ movdqu(xmm2, Address(rsp, xmm_size * 2));\n+  __ movdqu(xmm3, Address(rsp, xmm_size * 3));\n+  __ movdqu(xmm4, Address(rsp, xmm_size * 4));\n+  __ movdqu(xmm5, Address(rsp, xmm_size * 5));\n+  __ movdqu(xmm6, Address(rsp, xmm_size * 6));\n+  __ movdqu(xmm7, Address(rsp, xmm_size * 7));\n+  __ addptr(rsp, xmm_spill_size);\n+\n+  __ pop(r11);\n+  __ pop(r10);\n+  __ pop(r9);\n+  __ pop(r8);\n+  __ pop(rsi);\n+  __ pop(rdi);\n+  __ pop(rdx);\n+  __ pop(rcx);\n+\n+  if (dst == rax) {\n+    __ addptr(rsp, wordSize);\n+  } else {\n+    __ movptr(dst, rax);\n+    __ pop(rax);\n+  }\n+\n+  __ bind(done);\n+\n+  \/\/ Restore scratch register\n+  if (tmp1 == noreg) {\n+    __ pop(scratch);\n+  }\n+\n+  BLOCK_COMMENT(\"} XBarrierSetAssembler::load_at\");\n+}\n+\n+#ifdef ASSERT\n+\n+void XBarrierSetAssembler::store_at(MacroAssembler* masm,\n+                                    DecoratorSet decorators,\n+                                    BasicType type,\n+                                    Address dst,\n+                                    Register src,\n+                                    Register tmp1,\n+                                    Register tmp2,\n+                                    Register tmp3) {\n+  BLOCK_COMMENT(\"XBarrierSetAssembler::store_at {\");\n+\n+  \/\/ Verify oop store\n+  if (is_reference_type(type)) {\n+    \/\/ Note that src could be noreg, which means we\n+    \/\/ are storing null and can skip verification.\n+    if (src != noreg) {\n+      Label done;\n+      __ testptr(src, address_bad_mask_from_thread(r15_thread));\n+      __ jcc(Assembler::zero, done);\n+      __ stop(\"Verify oop store failed\");\n+      __ should_not_reach_here();\n+      __ bind(done);\n+    }\n+  }\n+\n+  \/\/ Store value\n+  BarrierSetAssembler::store_at(masm, decorators, type, dst, src, tmp1, tmp2, tmp3);\n+\n+  BLOCK_COMMENT(\"} XBarrierSetAssembler::store_at\");\n+}\n+\n+#endif \/\/ ASSERT\n+\n+void XBarrierSetAssembler::arraycopy_prologue(MacroAssembler* masm,\n+                                              DecoratorSet decorators,\n+                                              BasicType type,\n+                                              Register src,\n+                                              Register dst,\n+                                              Register count) {\n+  if (!XBarrierSet::barrier_needed(decorators, type)) {\n+    \/\/ Barrier not needed\n+    return;\n+  }\n+\n+  BLOCK_COMMENT(\"XBarrierSetAssembler::arraycopy_prologue {\");\n+\n+  \/\/ Save registers\n+  __ pusha();\n+\n+  \/\/ Call VM\n+  call_vm(masm, XBarrierSetRuntime::load_barrier_on_oop_array_addr(), src, count);\n+\n+  \/\/ Restore registers\n+  __ popa();\n+\n+  BLOCK_COMMENT(\"} XBarrierSetAssembler::arraycopy_prologue\");\n+}\n+\n+void XBarrierSetAssembler::try_resolve_jobject_in_native(MacroAssembler* masm,\n+                                                         Register jni_env,\n+                                                         Register obj,\n+                                                         Register tmp,\n+                                                         Label& slowpath) {\n+  BLOCK_COMMENT(\"XBarrierSetAssembler::try_resolve_jobject_in_native {\");\n+\n+  \/\/ Resolve jobject\n+  BarrierSetAssembler::try_resolve_jobject_in_native(masm, jni_env, obj, tmp, slowpath);\n+\n+  \/\/ Test address bad mask\n+  __ testptr(obj, address_bad_mask_from_jni_env(jni_env));\n+  __ jcc(Assembler::notZero, slowpath);\n+\n+  BLOCK_COMMENT(\"} XBarrierSetAssembler::try_resolve_jobject_in_native\");\n+}\n+\n+#ifdef COMPILER1\n+\n+#undef __\n+#define __ ce->masm()->\n+\n+void XBarrierSetAssembler::generate_c1_load_barrier_test(LIR_Assembler* ce,\n+                                                         LIR_Opr ref) const {\n+  __ testptr(ref->as_register(), address_bad_mask_from_thread(r15_thread));\n+}\n+\n+void XBarrierSetAssembler::generate_c1_load_barrier_stub(LIR_Assembler* ce,\n+                                                         XLoadBarrierStubC1* stub) const {\n+  \/\/ Stub entry\n+  __ bind(*stub->entry());\n+\n+  Register ref = stub->ref()->as_register();\n+  Register ref_addr = noreg;\n+  Register tmp = noreg;\n+\n+  if (stub->tmp()->is_valid()) {\n+    \/\/ Load address into tmp register\n+    ce->leal(stub->ref_addr(), stub->tmp());\n+    ref_addr = tmp = stub->tmp()->as_pointer_register();\n+  } else {\n+    \/\/ Address already in register\n+    ref_addr = stub->ref_addr()->as_address_ptr()->base()->as_pointer_register();\n+  }\n+\n+  assert_different_registers(ref, ref_addr, noreg);\n+\n+  \/\/ Save rax unless it is the result or tmp register\n+  if (ref != rax && tmp != rax) {\n+    __ push(rax);\n+  }\n+\n+  \/\/ Setup arguments and call runtime stub\n+  __ subptr(rsp, 2 * BytesPerWord);\n+  ce->store_parameter(ref_addr, 1);\n+  ce->store_parameter(ref, 0);\n+  __ call(RuntimeAddress(stub->runtime_stub()));\n+  __ addptr(rsp, 2 * BytesPerWord);\n+\n+  \/\/ Verify result\n+  __ verify_oop(rax);\n+\n+  \/\/ Move result into place\n+  if (ref != rax) {\n+    __ movptr(ref, rax);\n+  }\n+\n+  \/\/ Restore rax unless it is the result or tmp register\n+  if (ref != rax && tmp != rax) {\n+    __ pop(rax);\n+  }\n+\n+  \/\/ Stub exit\n+  __ jmp(*stub->continuation());\n+}\n+\n+#undef __\n+#define __ sasm->\n+\n+void XBarrierSetAssembler::generate_c1_load_barrier_runtime_stub(StubAssembler* sasm,\n+                                                                 DecoratorSet decorators) const {\n+  \/\/ Enter and save registers\n+  __ enter();\n+  __ save_live_registers_no_oop_map(true \/* save_fpu_registers *\/);\n+\n+  \/\/ Setup arguments\n+  __ load_parameter(1, c_rarg1);\n+  __ load_parameter(0, c_rarg0);\n+\n+  \/\/ Call VM\n+  __ call_VM_leaf(XBarrierSetRuntime::load_barrier_on_oop_field_preloaded_addr(decorators), c_rarg0, c_rarg1);\n+\n+  \/\/ Restore registers and return\n+  __ restore_live_registers_except_rax(true \/* restore_fpu_registers *\/);\n+  __ leave();\n+  __ ret(0);\n+}\n+\n+#endif \/\/ COMPILER1\n+\n+#ifdef COMPILER2\n+\n+OptoReg::Name XBarrierSetAssembler::refine_register(const Node* node, OptoReg::Name opto_reg) {\n+  if (!OptoReg::is_reg(opto_reg)) {\n+    return OptoReg::Bad;\n+  }\n+\n+  const VMReg vm_reg = OptoReg::as_VMReg(opto_reg);\n+  if (vm_reg->is_XMMRegister()) {\n+    opto_reg &= ~15;\n+    switch (node->ideal_reg()) {\n+      case Op_VecX:\n+        opto_reg |= 2;\n+        break;\n+      case Op_VecY:\n+        opto_reg |= 4;\n+        break;\n+      case Op_VecZ:\n+        opto_reg |= 8;\n+        break;\n+      default:\n+        opto_reg |= 1;\n+        break;\n+    }\n+  }\n+\n+  return opto_reg;\n+}\n+\n+\/\/ We use the vec_spill_helper from the x86.ad file to avoid reinventing this wheel\n+extern void vec_spill_helper(CodeBuffer *cbuf, bool is_load,\n+                            int stack_offset, int reg, uint ireg, outputStream* st);\n+\n+#undef __\n+#define __ _masm->\n+\n+class XSaveLiveRegisters {\n+private:\n+  struct XMMRegisterData {\n+    XMMRegister _reg;\n+    int         _size;\n+\n+    \/\/ Used by GrowableArray::find()\n+    bool operator == (const XMMRegisterData& other) {\n+      return _reg == other._reg;\n+    }\n+  };\n+\n+  MacroAssembler* const          _masm;\n+  GrowableArray<Register>        _gp_registers;\n+  GrowableArray<KRegister>       _opmask_registers;\n+  GrowableArray<XMMRegisterData> _xmm_registers;\n+  int                            _spill_size;\n+  int                            _spill_offset;\n+\n+  static int xmm_compare_register_size(XMMRegisterData* left, XMMRegisterData* right) {\n+    if (left->_size == right->_size) {\n+      return 0;\n+    }\n+\n+    return (left->_size < right->_size) ? -1 : 1;\n+  }\n+\n+  static int xmm_slot_size(OptoReg::Name opto_reg) {\n+    \/\/ The low order 4 bytes denote what size of the XMM register is live\n+    return (opto_reg & 15) << 3;\n+  }\n+\n+  static uint xmm_ideal_reg_for_size(int reg_size) {\n+    switch (reg_size) {\n+    case 8:\n+      return Op_VecD;\n+    case 16:\n+      return Op_VecX;\n+    case 32:\n+      return Op_VecY;\n+    case 64:\n+      return Op_VecZ;\n+    default:\n+      fatal(\"Invalid register size %d\", reg_size);\n+      return 0;\n+    }\n+  }\n+\n+  bool xmm_needs_vzeroupper() const {\n+    return _xmm_registers.is_nonempty() && _xmm_registers.at(0)._size > 16;\n+  }\n+\n+  void xmm_register_save(const XMMRegisterData& reg_data) {\n+    const OptoReg::Name opto_reg = OptoReg::as_OptoReg(reg_data._reg->as_VMReg());\n+    const uint ideal_reg = xmm_ideal_reg_for_size(reg_data._size);\n+    _spill_offset -= reg_data._size;\n+    vec_spill_helper(__ code(), false \/* is_load *\/, _spill_offset, opto_reg, ideal_reg, tty);\n+  }\n+\n+  void xmm_register_restore(const XMMRegisterData& reg_data) {\n+    const OptoReg::Name opto_reg = OptoReg::as_OptoReg(reg_data._reg->as_VMReg());\n+    const uint ideal_reg = xmm_ideal_reg_for_size(reg_data._size);\n+    vec_spill_helper(__ code(), true \/* is_load *\/, _spill_offset, opto_reg, ideal_reg, tty);\n+    _spill_offset += reg_data._size;\n+  }\n+\n+  void gp_register_save(Register reg) {\n+    _spill_offset -= 8;\n+    __ movq(Address(rsp, _spill_offset), reg);\n+  }\n+\n+  void opmask_register_save(KRegister reg) {\n+    _spill_offset -= 8;\n+    __ kmov(Address(rsp, _spill_offset), reg);\n+  }\n+\n+  void gp_register_restore(Register reg) {\n+    __ movq(reg, Address(rsp, _spill_offset));\n+    _spill_offset += 8;\n+  }\n+\n+  void opmask_register_restore(KRegister reg) {\n+    __ kmov(reg, Address(rsp, _spill_offset));\n+    _spill_offset += 8;\n+  }\n+\n+  void initialize(XLoadBarrierStubC2* stub) {\n+    \/\/ Create mask of caller saved registers that need to\n+    \/\/ be saved\/restored if live\n+    RegMask caller_saved;\n+    caller_saved.Insert(OptoReg::as_OptoReg(rax->as_VMReg()));\n+    caller_saved.Insert(OptoReg::as_OptoReg(rcx->as_VMReg()));\n+    caller_saved.Insert(OptoReg::as_OptoReg(rdx->as_VMReg()));\n+    caller_saved.Insert(OptoReg::as_OptoReg(rsi->as_VMReg()));\n+    caller_saved.Insert(OptoReg::as_OptoReg(rdi->as_VMReg()));\n+    caller_saved.Insert(OptoReg::as_OptoReg(r8->as_VMReg()));\n+    caller_saved.Insert(OptoReg::as_OptoReg(r9->as_VMReg()));\n+    caller_saved.Insert(OptoReg::as_OptoReg(r10->as_VMReg()));\n+    caller_saved.Insert(OptoReg::as_OptoReg(r11->as_VMReg()));\n+    caller_saved.Remove(OptoReg::as_OptoReg(stub->ref()->as_VMReg()));\n+\n+    \/\/ Create mask of live registers\n+    RegMask live = stub->live();\n+    if (stub->tmp() != noreg) {\n+      live.Insert(OptoReg::as_OptoReg(stub->tmp()->as_VMReg()));\n+    }\n+\n+    int gp_spill_size = 0;\n+    int opmask_spill_size = 0;\n+    int xmm_spill_size = 0;\n+\n+    \/\/ Record registers that needs to be saved\/restored\n+    RegMaskIterator rmi(live);\n+    while (rmi.has_next()) {\n+      const OptoReg::Name opto_reg = rmi.next();\n+      const VMReg vm_reg = OptoReg::as_VMReg(opto_reg);\n+\n+      if (vm_reg->is_Register()) {\n+        if (caller_saved.Member(opto_reg)) {\n+          _gp_registers.append(vm_reg->as_Register());\n+          gp_spill_size += 8;\n+        }\n+      } else if (vm_reg->is_KRegister()) {\n+        \/\/ All opmask registers are caller saved, thus spill the ones\n+        \/\/ which are live.\n+        if (_opmask_registers.find(vm_reg->as_KRegister()) == -1) {\n+          _opmask_registers.append(vm_reg->as_KRegister());\n+          opmask_spill_size += 8;\n+        }\n+      } else if (vm_reg->is_XMMRegister()) {\n+        \/\/ We encode in the low order 4 bits of the opto_reg, how large part of the register is live\n+        const VMReg vm_reg_base = OptoReg::as_VMReg(opto_reg & ~15);\n+        const int reg_size = xmm_slot_size(opto_reg);\n+        const XMMRegisterData reg_data = { vm_reg_base->as_XMMRegister(), reg_size };\n+        const int reg_index = _xmm_registers.find(reg_data);\n+        if (reg_index == -1) {\n+          \/\/ Not previously appended\n+          _xmm_registers.append(reg_data);\n+          xmm_spill_size += reg_size;\n+        } else {\n+          \/\/ Previously appended, update size\n+          const int reg_size_prev = _xmm_registers.at(reg_index)._size;\n+          if (reg_size > reg_size_prev) {\n+            _xmm_registers.at_put(reg_index, reg_data);\n+            xmm_spill_size += reg_size - reg_size_prev;\n+          }\n+        }\n+      } else {\n+        fatal(\"Unexpected register type\");\n+      }\n+    }\n+\n+    \/\/ Sort by size, largest first\n+    _xmm_registers.sort(xmm_compare_register_size);\n+\n+    \/\/ On Windows, the caller reserves stack space for spilling register arguments\n+    const int arg_spill_size = frame::arg_reg_save_area_bytes;\n+\n+    \/\/ Stack pointer must be 16 bytes aligned for the call\n+    _spill_offset = _spill_size = align_up(xmm_spill_size + gp_spill_size + opmask_spill_size + arg_spill_size, 16);\n+  }\n+\n+public:\n+  XSaveLiveRegisters(MacroAssembler* masm, XLoadBarrierStubC2* stub) :\n+      _masm(masm),\n+      _gp_registers(),\n+      _opmask_registers(),\n+      _xmm_registers(),\n+      _spill_size(0),\n+      _spill_offset(0) {\n+\n+    \/\/\n+    \/\/ Stack layout after registers have been spilled:\n+    \/\/\n+    \/\/ | ...            | original rsp, 16 bytes aligned\n+    \/\/ ------------------\n+    \/\/ | zmm0 high      |\n+    \/\/ | ...            |\n+    \/\/ | zmm0 low       | 16 bytes aligned\n+    \/\/ | ...            |\n+    \/\/ | ymm1 high      |\n+    \/\/ | ...            |\n+    \/\/ | ymm1 low       | 16 bytes aligned\n+    \/\/ | ...            |\n+    \/\/ | xmmN high      |\n+    \/\/ | ...            |\n+    \/\/ | xmmN low       | 8 bytes aligned\n+    \/\/ | reg0           | 8 bytes aligned\n+    \/\/ | reg1           |\n+    \/\/ | ...            |\n+    \/\/ | regN           | new rsp, if 16 bytes aligned\n+    \/\/ | <padding>      | else new rsp, 16 bytes aligned\n+    \/\/ ------------------\n+    \/\/\n+\n+    \/\/ Figure out what registers to save\/restore\n+    initialize(stub);\n+\n+    \/\/ Allocate stack space\n+    if (_spill_size > 0) {\n+      __ subptr(rsp, _spill_size);\n+    }\n+\n+    \/\/ Save XMM\/YMM\/ZMM registers\n+    for (int i = 0; i < _xmm_registers.length(); i++) {\n+      xmm_register_save(_xmm_registers.at(i));\n+    }\n+\n+    if (xmm_needs_vzeroupper()) {\n+      __ vzeroupper();\n+    }\n+\n+    \/\/ Save general purpose registers\n+    for (int i = 0; i < _gp_registers.length(); i++) {\n+      gp_register_save(_gp_registers.at(i));\n+    }\n+\n+    \/\/ Save opmask registers\n+    for (int i = 0; i < _opmask_registers.length(); i++) {\n+      opmask_register_save(_opmask_registers.at(i));\n+    }\n+  }\n+\n+  ~XSaveLiveRegisters() {\n+    \/\/ Restore opmask registers\n+    for (int i = _opmask_registers.length() - 1; i >= 0; i--) {\n+      opmask_register_restore(_opmask_registers.at(i));\n+    }\n+\n+    \/\/ Restore general purpose registers\n+    for (int i = _gp_registers.length() - 1; i >= 0; i--) {\n+      gp_register_restore(_gp_registers.at(i));\n+    }\n+\n+    __ vzeroupper();\n+\n+    \/\/ Restore XMM\/YMM\/ZMM registers\n+    for (int i = _xmm_registers.length() - 1; i >= 0; i--) {\n+      xmm_register_restore(_xmm_registers.at(i));\n+    }\n+\n+    \/\/ Free stack space\n+    if (_spill_size > 0) {\n+      __ addptr(rsp, _spill_size);\n+    }\n+  }\n+};\n+\n+class XSetupArguments {\n+private:\n+  MacroAssembler* const _masm;\n+  const Register        _ref;\n+  const Address         _ref_addr;\n+\n+public:\n+  XSetupArguments(MacroAssembler* masm, XLoadBarrierStubC2* stub) :\n+      _masm(masm),\n+      _ref(stub->ref()),\n+      _ref_addr(stub->ref_addr()) {\n+\n+    \/\/ Setup arguments\n+    if (_ref_addr.base() == noreg) {\n+      \/\/ No self healing\n+      if (_ref != c_rarg0) {\n+        __ movq(c_rarg0, _ref);\n+      }\n+      __ xorq(c_rarg1, c_rarg1);\n+    } else {\n+      \/\/ Self healing\n+      if (_ref == c_rarg0) {\n+        __ lea(c_rarg1, _ref_addr);\n+      } else if (_ref != c_rarg1) {\n+        __ lea(c_rarg1, _ref_addr);\n+        __ movq(c_rarg0, _ref);\n+      } else if (_ref_addr.base() != c_rarg0 && _ref_addr.index() != c_rarg0) {\n+        __ movq(c_rarg0, _ref);\n+        __ lea(c_rarg1, _ref_addr);\n+      } else {\n+        __ xchgq(c_rarg0, c_rarg1);\n+        if (_ref_addr.base() == c_rarg0) {\n+          __ lea(c_rarg1, Address(c_rarg1, _ref_addr.index(), _ref_addr.scale(), _ref_addr.disp()));\n+        } else if (_ref_addr.index() == c_rarg0) {\n+          __ lea(c_rarg1, Address(_ref_addr.base(), c_rarg1, _ref_addr.scale(), _ref_addr.disp()));\n+        } else {\n+          ShouldNotReachHere();\n+        }\n+      }\n+    }\n+  }\n+\n+  ~XSetupArguments() {\n+    \/\/ Transfer result\n+    if (_ref != rax) {\n+      __ movq(_ref, rax);\n+    }\n+  }\n+};\n+\n+#undef __\n+#define __ masm->\n+\n+void XBarrierSetAssembler::generate_c2_load_barrier_stub(MacroAssembler* masm, XLoadBarrierStubC2* stub) const {\n+  BLOCK_COMMENT(\"XLoadBarrierStubC2\");\n+\n+  \/\/ Stub entry\n+  __ bind(*stub->entry());\n+\n+  {\n+    XSaveLiveRegisters save_live_registers(masm, stub);\n+    XSetupArguments setup_arguments(masm, stub);\n+    __ call(RuntimeAddress(stub->slow_path()));\n+  }\n+\n+  \/\/ Stub exit\n+  __ jmp(*stub->continuation());\n+}\n+\n+#endif \/\/ COMPILER2\n+\n+#undef __\n+#define __ masm->\n+\n+void XBarrierSetAssembler::check_oop(MacroAssembler* masm, Register obj, Register tmp1, Register tmp2, Label& error) {\n+  \/\/ Check if metadata bits indicate a bad oop\n+  __ testptr(obj, Address(r15_thread, XThreadLocalData::address_bad_mask_offset()));\n+  __ jcc(Assembler::notZero, error);\n+  BarrierSetAssembler::check_oop(masm, obj, tmp1, tmp2, error);\n+}\n+\n+#undef __\n","filename":"src\/hotspot\/cpu\/x86\/gc\/x\/xBarrierSetAssembler_x86.cpp","additions":713,"deletions":0,"binary":false,"changes":713,"status":"added"},{"patch":"@@ -0,0 +1,109 @@\n+\/*\n+ * Copyright (c) 2018, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef CPU_X86_GC_X_XBARRIERSETASSEMBLER_X86_HPP\n+#define CPU_X86_GC_X_XBARRIERSETASSEMBLER_X86_HPP\n+\n+#include \"code\/vmreg.hpp\"\n+#include \"oops\/accessDecorators.hpp\"\n+#ifdef COMPILER2\n+#include \"opto\/optoreg.hpp\"\n+#endif \/\/ COMPILER2\n+\n+class MacroAssembler;\n+\n+#ifdef COMPILER1\n+class LIR_Assembler;\n+class LIR_Opr;\n+class StubAssembler;\n+#endif \/\/ COMPILER1\n+\n+#ifdef COMPILER2\n+class Node;\n+#endif \/\/ COMPILER2\n+\n+#ifdef COMPILER1\n+class XLoadBarrierStubC1;\n+#endif \/\/ COMPILER1\n+\n+#ifdef COMPILER2\n+class XLoadBarrierStubC2;\n+#endif \/\/ COMPILER2\n+\n+class XBarrierSetAssembler : public XBarrierSetAssemblerBase {\n+public:\n+  virtual void load_at(MacroAssembler* masm,\n+                       DecoratorSet decorators,\n+                       BasicType type,\n+                       Register dst,\n+                       Address src,\n+                       Register tmp1,\n+                       Register tmp_thread);\n+\n+#ifdef ASSERT\n+  virtual void store_at(MacroAssembler* masm,\n+                        DecoratorSet decorators,\n+                        BasicType type,\n+                        Address dst,\n+                        Register src,\n+                        Register tmp1,\n+                        Register tmp2,\n+                        Register tmp3);\n+#endif \/\/ ASSERT\n+\n+  virtual void arraycopy_prologue(MacroAssembler* masm,\n+                                  DecoratorSet decorators,\n+                                  BasicType type,\n+                                  Register src,\n+                                  Register dst,\n+                                  Register count);\n+\n+  virtual void try_resolve_jobject_in_native(MacroAssembler* masm,\n+                                             Register jni_env,\n+                                             Register obj,\n+                                             Register tmp,\n+                                             Label& slowpath);\n+\n+#ifdef COMPILER1\n+  void generate_c1_load_barrier_test(LIR_Assembler* ce,\n+                                     LIR_Opr ref) const;\n+\n+  void generate_c1_load_barrier_stub(LIR_Assembler* ce,\n+                                     XLoadBarrierStubC1* stub) const;\n+\n+  void generate_c1_load_barrier_runtime_stub(StubAssembler* sasm,\n+                                             DecoratorSet decorators) const;\n+#endif \/\/ COMPILER1\n+\n+#ifdef COMPILER2\n+  OptoReg::Name refine_register(const Node* node,\n+                                OptoReg::Name opto_reg);\n+\n+  void generate_c2_load_barrier_stub(MacroAssembler* masm,\n+                                     XLoadBarrierStubC2* stub) const;\n+#endif \/\/ COMPILER2\n+\n+  void check_oop(MacroAssembler* masm, Register obj, Register tmp1, Register tmp2, Label& error);\n+};\n+\n+#endif \/\/ CPU_X86_GC_X_XBARRIERSETASSEMBLER_X86_HPP\n","filename":"src\/hotspot\/cpu\/x86\/gc\/x\/xBarrierSetAssembler_x86.hpp","additions":109,"deletions":0,"binary":false,"changes":109,"status":"added"},{"patch":"@@ -26,1 +26,1 @@\n-#include \"gc\/z\/zGlobals.hpp\"\n+#include \"gc\/x\/xGlobals.hpp\"\n@@ -139,1 +139,1 @@\n-size_t ZPlatformAddressOffsetBits() {\n+size_t XPlatformAddressOffsetBits() {\n@@ -142,1 +142,1 @@\n-  const size_t address_offset = round_up_power_of_2(MaxHeapSize * ZVirtualToPhysicalRatio);\n+  const size_t address_offset = round_up_power_of_2(MaxHeapSize * XVirtualToPhysicalRatio);\n@@ -147,2 +147,2 @@\n-size_t ZPlatformAddressMetadataShift() {\n-  return ZPlatformAddressOffsetBits();\n+size_t XPlatformAddressMetadataShift() {\n+  return XPlatformAddressOffsetBits();\n","filename":"src\/hotspot\/cpu\/x86\/gc\/x\/xGlobals_x86.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"previous_filename":"src\/hotspot\/cpu\/x86\/gc\/z\/zGlobals_x86.cpp","status":"renamed"},{"patch":"@@ -0,0 +1,33 @@\n+\/*\n+ * Copyright (c) 2015, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef CPU_X86_GC_X_XGLOBALS_X86_HPP\n+#define CPU_X86_GC_X_XGLOBALS_X86_HPP\n+\n+const size_t XPlatformHeapViews        = 3;\n+const size_t XPlatformCacheLineSize    = 64;\n+\n+size_t XPlatformAddressOffsetBits();\n+size_t XPlatformAddressMetadataShift();\n+\n+#endif \/\/ CPU_X86_GC_X_XGLOBALS_X86_HPP\n","filename":"src\/hotspot\/cpu\/x86\/gc\/x\/xGlobals_x86.hpp","additions":33,"deletions":0,"binary":false,"changes":33,"status":"added"},{"patch":"@@ -0,0 +1,158 @@\n+\/\/\n+\/\/ Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n+\/\/ DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+\/\/\n+\/\/ This code is free software; you can redistribute it and\/or modify it\n+\/\/ under the terms of the GNU General Public License version 2 only, as\n+\/\/ published by the Free Software Foundation.\n+\/\/\n+\/\/ This code is distributed in the hope that it will be useful, but WITHOUT\n+\/\/ ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+\/\/ FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+\/\/ version 2 for more details (a copy is included in the LICENSE file that\n+\/\/ accompanied this code).\n+\/\/\n+\/\/ You should have received a copy of the GNU General Public License version\n+\/\/ 2 along with this work; if not, write to the Free Software Foundation,\n+\/\/ Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+\/\/\n+\/\/ Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+\/\/ or visit www.oracle.com if you need additional information or have any\n+\/\/ questions.\n+\/\/\n+\n+source_hpp %{\n+\n+#include \"gc\/shared\/gc_globals.hpp\"\n+#include \"gc\/x\/c2\/xBarrierSetC2.hpp\"\n+#include \"gc\/x\/xThreadLocalData.hpp\"\n+\n+%}\n+\n+source %{\n+\n+#include \"c2_intelJccErratum_x86.hpp\"\n+\n+static void x_load_barrier(MacroAssembler& _masm, const MachNode* node, Address ref_addr, Register ref, Register tmp, uint8_t barrier_data) {\n+  if (barrier_data == XLoadBarrierElided) {\n+    return;\n+  }\n+  XLoadBarrierStubC2* const stub = XLoadBarrierStubC2::create(node, ref_addr, ref, tmp, barrier_data);\n+  {\n+    IntelJccErratumAlignment intel_alignment(_masm, 10 \/* jcc_size *\/);\n+    __ testptr(ref, Address(r15_thread, XThreadLocalData::address_bad_mask_offset()));\n+    __ jcc(Assembler::notZero, *stub->entry());\n+  }\n+  __ bind(*stub->continuation());\n+}\n+\n+static void x_load_barrier_cmpxchg(MacroAssembler& _masm, const MachNode* node, Address ref_addr, Register ref, Register tmp, Label& good) {\n+  XLoadBarrierStubC2* const stub = XLoadBarrierStubC2::create(node, ref_addr, ref, tmp, XLoadBarrierStrong);\n+  {\n+    IntelJccErratumAlignment intel_alignment(_masm, 10 \/* jcc_size *\/);\n+    __ testptr(ref, Address(r15_thread, XThreadLocalData::address_bad_mask_offset()));\n+    __ jcc(Assembler::zero, good);\n+  }\n+  {\n+    IntelJccErratumAlignment intel_alignment(_masm, 5 \/* jcc_size *\/);\n+    __ jmp(*stub->entry());\n+  }\n+  __ bind(*stub->continuation());\n+}\n+\n+static void x_cmpxchg_common(MacroAssembler& _masm, const MachNode* node, Register mem_reg, Register newval, Register tmp) {\n+  \/\/ Compare value (oldval) is in rax\n+   const Address mem = Address(mem_reg, 0);\n+\n+  if (node->barrier_data() != XLoadBarrierElided) {\n+    __ movptr(tmp, rax);\n+  }\n+\n+  __ lock();\n+  __ cmpxchgptr(newval, mem);\n+\n+  if (node->barrier_data() != XLoadBarrierElided) {\n+    Label good;\n+    x_load_barrier_cmpxchg(_masm, node, mem, rax, tmp, good);\n+    __ movptr(rax, tmp);\n+    __ lock();\n+    __ cmpxchgptr(newval, mem);\n+    __ bind(good);\n+  }\n+}\n+\n+%}\n+\n+\/\/ Load Pointer\n+instruct xLoadP(rRegP dst, memory mem, rFlagsReg cr)\n+%{\n+  predicate(UseZGC && !ZGenerational && n->as_Load()->barrier_data() != 0);\n+  match(Set dst (LoadP mem));\n+  effect(KILL cr, TEMP dst);\n+\n+  ins_cost(125);\n+\n+  format %{ \"movq     $dst, $mem\" %}\n+\n+  ins_encode %{\n+    __ movptr($dst$$Register, $mem$$Address);\n+    x_load_barrier(_masm, this, $mem$$Address, $dst$$Register, noreg \/* tmp *\/, barrier_data());\n+  %}\n+\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct xCompareAndExchangeP(indirect mem, rax_RegP oldval, rRegP newval, rRegP tmp, rFlagsReg cr) %{\n+  match(Set oldval (CompareAndExchangeP mem (Binary oldval newval)));\n+  predicate(UseZGC && !ZGenerational && n->as_LoadStore()->barrier_data() == XLoadBarrierStrong);\n+  effect(KILL cr, TEMP tmp);\n+\n+  format %{ \"lock\\n\\t\"\n+            \"cmpxchgq $newval, $mem\" %}\n+\n+  ins_encode %{\n+    precond($oldval$$Register == rax);\n+    x_cmpxchg_common(_masm, this, $mem$$Register, $newval$$Register, $tmp$$Register);\n+  %}\n+\n+  ins_pipe(pipe_cmpxchg);\n+%}\n+\n+instruct xCompareAndSwapP(rRegI res, indirect mem, rRegP newval, rRegP tmp, rFlagsReg cr, rax_RegP oldval) %{\n+  match(Set res (CompareAndSwapP mem (Binary oldval newval)));\n+  match(Set res (WeakCompareAndSwapP mem (Binary oldval newval)));\n+  predicate(UseZGC && !ZGenerational && n->as_LoadStore()->barrier_data() == XLoadBarrierStrong);\n+  effect(KILL cr, KILL oldval, TEMP tmp);\n+\n+  format %{ \"lock\\n\\t\"\n+            \"cmpxchgq $newval, $mem\\n\\t\"\n+            \"sete     $res\\n\\t\"\n+            \"movzbl   $res, $res\" %}\n+\n+  ins_encode %{\n+    precond($oldval$$Register == rax);\n+    x_cmpxchg_common(_masm, this, $mem$$Register, $newval$$Register, $tmp$$Register);\n+    if (barrier_data() != XLoadBarrierElided) {\n+      __ cmpptr($tmp$$Register, rax);\n+    }\n+    __ setb(Assembler::equal, $res$$Register);\n+    __ movzbl($res$$Register, $res$$Register);\n+  %}\n+\n+  ins_pipe(pipe_cmpxchg);\n+%}\n+\n+instruct xXChgP(indirect mem, rRegP newval, rFlagsReg cr) %{\n+  match(Set newval (GetAndSetP mem newval));\n+  predicate(UseZGC && !ZGenerational && n->as_LoadStore()->barrier_data() != 0);\n+  effect(KILL cr);\n+\n+  format %{ \"xchgq    $newval, $mem\" %}\n+\n+  ins_encode %{\n+    __ xchgptr($newval$$Register, Address($mem$$Register, 0));\n+    x_load_barrier(_masm, this, Address(noreg, 0), $newval$$Register, noreg \/* tmp *\/, barrier_data());\n+  %}\n+\n+  ins_pipe(pipe_cmpxchg);\n+%}\n","filename":"src\/hotspot\/cpu\/x86\/gc\/x\/x_x86_64.ad","additions":158,"deletions":0,"binary":false,"changes":158,"status":"added"},{"patch":"@@ -0,0 +1,47 @@\n+\/*\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/shared\/gc_globals.hpp\"\n+#include \"gc\/z\/zAddress.inline.hpp\"\n+#include \"gc\/z\/zGlobals.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include \"utilities\/powerOfTwo.hpp\"\n+\n+size_t ZPointerLoadShift;\n+\n+size_t ZPlatformAddressOffsetBits() {\n+  const size_t min_address_offset_bits = 42; \/\/ 4TB\n+  const size_t max_address_offset_bits = 44; \/\/ 16TB\n+  const size_t address_offset = round_up_power_of_2(MaxHeapSize * ZVirtualToPhysicalRatio);\n+  const size_t address_offset_bits = log2i_exact(address_offset);\n+  return clamp(address_offset_bits, min_address_offset_bits, max_address_offset_bits);\n+}\n+\n+size_t ZPlatformAddressHeapBaseShift() {\n+  return ZPlatformAddressOffsetBits();\n+}\n+\n+void ZGlobalsPointers::pd_set_good_masks() {\n+  ZPointerLoadShift = ZPointer::load_shift_lookup(ZPointerLoadGoodMask);\n+}\n","filename":"src\/hotspot\/cpu\/x86\/gc\/z\/zAddress_x86.cpp","additions":47,"deletions":0,"binary":false,"changes":47,"status":"added"},{"patch":"@@ -0,0 +1,34 @@\n+\/*\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef CPU_X86_GC_Z_ZADDRESS_X86_HPP\n+#define CPU_X86_GC_Z_ZADDRESS_X86_HPP\n+\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+extern size_t ZPointerLoadShift;\n+\n+size_t ZPlatformAddressOffsetBits();\n+size_t ZPlatformAddressHeapBaseShift();\n+\n+#endif \/\/ CPU_X86_GC_Z_ZADDRESS_X86_HPP\n","filename":"src\/hotspot\/cpu\/x86\/gc\/z\/zAddress_x86.hpp","additions":34,"deletions":0,"binary":false,"changes":34,"status":"added"},{"patch":"@@ -0,0 +1,39 @@\n+\/*\n+ * Copyright (c) 2019, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef CPU_X86_GC_Z_ZADDRESS_X86_INLINE_HPP\n+#define CPU_X86_GC_Z_ZADDRESS_X86_INLINE_HPP\n+\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+inline uintptr_t ZPointer::remap_bits(uintptr_t colored) {\n+  return colored & ZPointerRemappedMask;\n+}\n+\n+inline constexpr int ZPointer::load_shift_lookup(uintptr_t value) {\n+  const size_t index = load_shift_lookup_index(value);\n+  assert(index == 0 || is_power_of_2(index), \"Incorrect load shift: \" SIZE_FORMAT, index);\n+  return ZPointerLoadShiftTable[index];\n+}\n+\n+#endif \/\/ CPU_X86_GC_Z_ZADDRESS_X86_INLINE_HPP\n","filename":"src\/hotspot\/cpu\/x86\/gc\/z\/zAddress_x86.inline.hpp","additions":39,"deletions":0,"binary":false,"changes":39,"status":"added"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"gc\/z\/zAddress.hpp\"\n@@ -34,0 +35,1 @@\n+#include \"runtime\/jniHandles.hpp\"\n@@ -42,0 +44,1 @@\n+#include \"c2_intelJccErratum_x86.hpp\"\n@@ -43,0 +46,1 @@\n+#include \"opto\/output.hpp\"\n@@ -54,0 +58,136 @@\n+ZBarrierSetAssembler::ZBarrierSetAssembler()\n+  : _load_bad_relocations(),\n+    _store_bad_relocations(),\n+    _store_good_relocations() {\n+}\n+\n+enum class ZXMMSpillMode {\n+  none,\n+  avx128,\n+  avx256\n+};\n+\n+\/\/ Helper for saving and restoring registers across a runtime call that does\n+\/\/ not have any live vector registers.\n+class ZRuntimeCallSpill {\n+private:\n+  const ZXMMSpillMode _xmm_spill_mode;\n+  const int _xmm_size;\n+  const int _xmm_spill_size;\n+  MacroAssembler* _masm;\n+  Register _result;\n+\n+  void save() {\n+    MacroAssembler* masm = _masm;\n+    __ push(rax);\n+    __ push(rcx);\n+    __ push(rdx);\n+    __ push(rdi);\n+    __ push(rsi);\n+    __ push(r8);\n+    __ push(r9);\n+    __ push(r10);\n+    __ push(r11);\n+\n+    if (_xmm_spill_size != 0) {\n+      __ subptr(rsp, _xmm_spill_size);\n+      if (_xmm_spill_mode == ZXMMSpillMode::avx128) {\n+        __ movdqu(Address(rsp, _xmm_size * 7), xmm7);\n+        __ movdqu(Address(rsp, _xmm_size * 6), xmm6);\n+        __ movdqu(Address(rsp, _xmm_size * 5), xmm5);\n+        __ movdqu(Address(rsp, _xmm_size * 4), xmm4);\n+        __ movdqu(Address(rsp, _xmm_size * 3), xmm3);\n+        __ movdqu(Address(rsp, _xmm_size * 2), xmm2);\n+        __ movdqu(Address(rsp, _xmm_size * 1), xmm1);\n+        __ movdqu(Address(rsp, _xmm_size * 0), xmm0);\n+      } else {\n+        assert(_xmm_spill_mode == ZXMMSpillMode::avx256, \"AVX support ends at avx256\");\n+        __ vmovdqu(Address(rsp, _xmm_size * 7), xmm7);\n+        __ vmovdqu(Address(rsp, _xmm_size * 6), xmm6);\n+        __ vmovdqu(Address(rsp, _xmm_size * 5), xmm5);\n+        __ vmovdqu(Address(rsp, _xmm_size * 4), xmm4);\n+        __ vmovdqu(Address(rsp, _xmm_size * 3), xmm3);\n+        __ vmovdqu(Address(rsp, _xmm_size * 2), xmm2);\n+        __ vmovdqu(Address(rsp, _xmm_size * 1), xmm1);\n+        __ vmovdqu(Address(rsp, _xmm_size * 0), xmm0);\n+      }\n+    }\n+  }\n+\n+  void restore() {\n+    MacroAssembler* masm = _masm;\n+    if (_xmm_spill_size != 0) {\n+      if (_xmm_spill_mode == ZXMMSpillMode::avx128) {\n+        __ movdqu(xmm0, Address(rsp, _xmm_size * 0));\n+        __ movdqu(xmm1, Address(rsp, _xmm_size * 1));\n+        __ movdqu(xmm2, Address(rsp, _xmm_size * 2));\n+        __ movdqu(xmm3, Address(rsp, _xmm_size * 3));\n+        __ movdqu(xmm4, Address(rsp, _xmm_size * 4));\n+        __ movdqu(xmm5, Address(rsp, _xmm_size * 5));\n+        __ movdqu(xmm6, Address(rsp, _xmm_size * 6));\n+        __ movdqu(xmm7, Address(rsp, _xmm_size * 7));\n+      } else {\n+        assert(_xmm_spill_mode == ZXMMSpillMode::avx256, \"AVX support ends at avx256\");\n+        __ vmovdqu(xmm0, Address(rsp, _xmm_size * 0));\n+        __ vmovdqu(xmm1, Address(rsp, _xmm_size * 1));\n+        __ vmovdqu(xmm2, Address(rsp, _xmm_size * 2));\n+        __ vmovdqu(xmm3, Address(rsp, _xmm_size * 3));\n+        __ vmovdqu(xmm4, Address(rsp, _xmm_size * 4));\n+        __ vmovdqu(xmm5, Address(rsp, _xmm_size * 5));\n+        __ vmovdqu(xmm6, Address(rsp, _xmm_size * 6));\n+        __ vmovdqu(xmm7, Address(rsp, _xmm_size * 7));\n+      }\n+      __ addptr(rsp, _xmm_spill_size);\n+    }\n+\n+    __ pop(r11);\n+    __ pop(r10);\n+    __ pop(r9);\n+    __ pop(r8);\n+    __ pop(rsi);\n+    __ pop(rdi);\n+    __ pop(rdx);\n+    __ pop(rcx);\n+    if (_result == noreg) {\n+      __ pop(rax);\n+    } else if (_result == rax) {\n+      __ addptr(rsp, wordSize);\n+    } else {\n+      __ movptr(_result, rax);\n+      __ pop(rax);\n+    }\n+  }\n+\n+  static int compute_xmm_size(ZXMMSpillMode spill_mode) {\n+    switch (spill_mode) {\n+      case ZXMMSpillMode::none:\n+        return 0;\n+      case ZXMMSpillMode::avx128:\n+        return wordSize * 2;\n+      case ZXMMSpillMode::avx256:\n+        return wordSize * 4;\n+      default:\n+        ShouldNotReachHere();\n+        return 0;\n+    }\n+  }\n+\n+public:\n+  ZRuntimeCallSpill(MacroAssembler* masm, Register result, ZXMMSpillMode spill_mode)\n+    : _xmm_spill_mode(spill_mode),\n+      _xmm_size(compute_xmm_size(spill_mode)),\n+      _xmm_spill_size(_xmm_size * Argument::n_float_register_parameters_j),\n+      _masm(masm),\n+      _result(result) {\n+    \/\/ We may end up here from generate_native_wrapper, then the method may have\n+    \/\/ floats as arguments, and we must spill them before calling the VM runtime\n+    \/\/ leaf. From the interpreter all floats are passed on the stack.\n+    assert(Argument::n_float_register_parameters_j == 8, \"Assumption\");\n+    save();\n+  }\n+\n+  ~ZRuntimeCallSpill() {\n+    restore();\n+  }\n+};\n+\n@@ -104,0 +244,1 @@\n+  Label uncolor;\n@@ -115,0 +256,4 @@\n+  const bool on_non_strong =\n+      (decorators & ON_WEAK_OOP_REF) != 0 ||\n+      (decorators & ON_PHANTOM_OOP_REF) != 0;\n+\n@@ -116,2 +261,7 @@\n-  __ testptr(dst, address_bad_mask_from_thread(r15_thread));\n-  __ jcc(Assembler::zero, done);\n+  if (on_non_strong) {\n+    __ testptr(dst, mark_bad_mask_from_thread(r15_thread));\n+  } else {\n+    __ testptr(dst, load_bad_mask_from_thread(r15_thread));\n+  }\n+\n+  __ jcc(Assembler::zero, uncolor);\n@@ -123,26 +273,8 @@\n-  \/\/ Save registers\n-  __ push(rax);\n-  __ push(rcx);\n-  __ push(rdx);\n-  __ push(rdi);\n-  __ push(rsi);\n-  __ push(r8);\n-  __ push(r9);\n-  __ push(r10);\n-  __ push(r11);\n-\n-  \/\/ We may end up here from generate_native_wrapper, then the method may have\n-  \/\/ floats as arguments, and we must spill them before calling the VM runtime\n-  \/\/ leaf. From the interpreter all floats are passed on the stack.\n-  assert(Argument::n_float_register_parameters_j == 8, \"Assumption\");\n-  const int xmm_size = wordSize * 2;\n-  const int xmm_spill_size = xmm_size * Argument::n_float_register_parameters_j;\n-  __ subptr(rsp, xmm_spill_size);\n-  __ movdqu(Address(rsp, xmm_size * 7), xmm7);\n-  __ movdqu(Address(rsp, xmm_size * 6), xmm6);\n-  __ movdqu(Address(rsp, xmm_size * 5), xmm5);\n-  __ movdqu(Address(rsp, xmm_size * 4), xmm4);\n-  __ movdqu(Address(rsp, xmm_size * 3), xmm3);\n-  __ movdqu(Address(rsp, xmm_size * 2), xmm2);\n-  __ movdqu(Address(rsp, xmm_size * 1), xmm1);\n-  __ movdqu(Address(rsp, xmm_size * 0), xmm0);\n+  {\n+    \/\/ Call VM\n+    ZRuntimeCallSpill rcs(masm, dst, ZXMMSpillMode::avx128);\n+    call_vm(masm, ZBarrierSetRuntime::load_barrier_on_oop_field_preloaded_addr(decorators), dst, scratch);\n+  }\n+\n+  \/\/ Slow-path has already uncolored\n+  __ jmp(done);\n@@ -150,24 +282,7 @@\n-  \/\/ Call VM\n-  call_vm(masm, ZBarrierSetRuntime::load_barrier_on_oop_field_preloaded_addr(decorators), dst, scratch);\n-\n-  __ movdqu(xmm0, Address(rsp, xmm_size * 0));\n-  __ movdqu(xmm1, Address(rsp, xmm_size * 1));\n-  __ movdqu(xmm2, Address(rsp, xmm_size * 2));\n-  __ movdqu(xmm3, Address(rsp, xmm_size * 3));\n-  __ movdqu(xmm4, Address(rsp, xmm_size * 4));\n-  __ movdqu(xmm5, Address(rsp, xmm_size * 5));\n-  __ movdqu(xmm6, Address(rsp, xmm_size * 6));\n-  __ movdqu(xmm7, Address(rsp, xmm_size * 7));\n-  __ addptr(rsp, xmm_spill_size);\n-\n-  __ pop(r11);\n-  __ pop(r10);\n-  __ pop(r9);\n-  __ pop(r8);\n-  __ pop(rsi);\n-  __ pop(rdi);\n-  __ pop(rdx);\n-  __ pop(rcx);\n-\n-  if (dst == rax) {\n-    __ addptr(rsp, wordSize);\n+  __ bind(uncolor);\n+\n+  __ movptr(scratch, rcx); \/\/ Save rcx because shrq needs shift in rcx\n+  __ movptr(rcx, ExternalAddress((address)&ZPointerLoadShift));\n+  if (dst == rcx) {\n+    \/\/ Dst was rcx which is saved in scratch because shrq needs rcx for shift\n+    __ shrq(scratch);\n@@ -175,2 +290,1 @@\n-    __ movptr(dst, rax);\n-    __ pop(rax);\n+    __ shrq(dst);\n@@ -178,0 +292,1 @@\n+  __ movptr(rcx, scratch); \/\/ restore rcx\n@@ -189,1 +304,202 @@\n-#ifdef ASSERT\n+static void emit_store_fast_path_check(MacroAssembler* masm, Address ref_addr, bool is_atomic, Label& medium_path) {\n+  if (is_atomic) {\n+    \/\/ Atomic operations must ensure that the contents of memory are store-good before\n+    \/\/ an atomic operation can execute.\n+    \/\/ A not relocatable object could have spurious raw null pointers in its fields after\n+    \/\/ getting promoted to the old generation.\n+    __ cmpw(ref_addr, barrier_Relocation::unpatched);\n+    __ relocate(barrier_Relocation::spec(), ZBarrierRelocationFormatStoreGoodAfterCmp);\n+  } else {\n+    \/\/ Stores on relocatable objects never need to deal with raw null pointers in fields.\n+    \/\/ Raw null pointers may only exist in the young generation, as they get pruned when\n+    \/\/ the object is relocated to old. And no pre-write barrier needs to perform any action\n+    \/\/ in the young generation.\n+    __ Assembler::testl(ref_addr, barrier_Relocation::unpatched);\n+    __ relocate(barrier_Relocation::spec(), ZBarrierRelocationFormatStoreBadAfterTest);\n+  }\n+  __ jcc(Assembler::notEqual, medium_path);\n+}\n+\n+static int store_fast_path_check_size(MacroAssembler* masm, Address ref_addr, bool is_atomic, Label& medium_path) {\n+  if (!VM_Version::has_intel_jcc_erratum()) {\n+    return 0;\n+  }\n+  int size = 0;\n+#ifdef COMPILER2\n+  bool in_scratch_emit_size = masm->code_section()->scratch_emit();\n+  if (!in_scratch_emit_size) {\n+    \/\/ Temporarily register as scratch buffer so that relocations don't register\n+    masm->code_section()->set_scratch_emit();\n+  }\n+  \/\/ First emit the code, to measure its size\n+  address insts_end = masm->code_section()->end();\n+  \/\/ The dummy medium path label is bound after the code emission. This ensures\n+  \/\/ full size of the generated jcc, which is what the real barrier will have\n+  \/\/ as well, as it also binds after the emission of the barrier.\n+  Label dummy_medium_path;\n+  emit_store_fast_path_check(masm, ref_addr, is_atomic, dummy_medium_path);\n+  address emitted_end = masm->code_section()->end();\n+  size = (int)(intptr_t)(emitted_end - insts_end);\n+  __ bind(dummy_medium_path);\n+  if (!in_scratch_emit_size) {\n+    \/\/ Potentially restore scratchyness\n+    masm->code_section()->clear_scratch_emit();\n+  }\n+  \/\/ Roll back code, now that we know the size\n+  masm->code_section()->set_end(insts_end);\n+#endif\n+  return size;\n+}\n+\n+static void emit_store_fast_path_check_c2(MacroAssembler* masm, Address ref_addr, bool is_atomic, Label& medium_path) {\n+#ifdef COMPILER2\n+  \/\/ This is a JCC erratum mitigation wrapper for calling the inner check\n+  int size = store_fast_path_check_size(masm, ref_addr, is_atomic, medium_path);\n+  \/\/ Emit JCC erratum mitigation nops with the right size\n+  IntelJccErratumAlignment(*masm, size);\n+  \/\/ Emit the JCC erratum mitigation guarded code\n+  emit_store_fast_path_check(masm, ref_addr, is_atomic, medium_path);\n+#endif\n+}\n+\n+static bool is_c2_compilation() {\n+  CompileTask* task = ciEnv::current()->task();\n+  return task != nullptr && is_c2_compile(task->comp_level());\n+}\n+\n+void ZBarrierSetAssembler::store_barrier_fast(MacroAssembler* masm,\n+                                              Address ref_addr,\n+                                              Register rnew_zaddress,\n+                                              Register rnew_zpointer,\n+                                              bool in_nmethod,\n+                                              bool is_atomic,\n+                                              Label& medium_path,\n+                                              Label& medium_path_continuation) const {\n+  assert_different_registers(ref_addr.base(), rnew_zpointer);\n+  assert_different_registers(ref_addr.index(), rnew_zpointer);\n+  assert_different_registers(rnew_zaddress, rnew_zpointer);\n+\n+  if (in_nmethod) {\n+    if (is_c2_compilation()) {\n+      emit_store_fast_path_check_c2(masm, ref_addr, is_atomic, medium_path);\n+    } else {\n+      emit_store_fast_path_check(masm, ref_addr, is_atomic, medium_path);\n+    }\n+    __ bind(medium_path_continuation);\n+    if (rnew_zaddress != noreg) {\n+      \/\/ noreg means null; no need to color\n+      __ movptr(rnew_zpointer, rnew_zaddress);\n+      __ relocate(barrier_Relocation::spec(), ZBarrierRelocationFormatLoadGoodBeforeShl);\n+      __ shlq(rnew_zpointer, barrier_Relocation::unpatched);\n+      __ orq_imm32(rnew_zpointer, barrier_Relocation::unpatched);\n+      __ relocate(barrier_Relocation::spec(), ZBarrierRelocationFormatStoreGoodAfterOr);\n+    }\n+  } else {\n+    __ movzwq(rnew_zpointer, ref_addr);\n+    __ testq(rnew_zpointer, Address(r15_thread, ZThreadLocalData::store_bad_mask_offset()));\n+    __ jcc(Assembler::notEqual, medium_path);\n+    __ bind(medium_path_continuation);\n+    if (rnew_zaddress == noreg) {\n+      __ xorptr(rnew_zpointer, rnew_zpointer);\n+    } else {\n+      __ movptr(rnew_zpointer, rnew_zaddress);\n+    }\n+    assert_different_registers(rcx, rnew_zpointer);\n+    __ push(rcx);\n+    __ movptr(rcx, ExternalAddress((address)&ZPointerLoadShift));\n+    __ shlq(rnew_zpointer);\n+    __ pop(rcx);\n+    __ orq(rnew_zpointer, Address(r15_thread, ZThreadLocalData::store_good_mask_offset()));\n+  }\n+}\n+\n+static void store_barrier_buffer_add(MacroAssembler* masm,\n+                                     Address ref_addr,\n+                                     Register tmp1,\n+                                     Label& slow_path) {\n+  Address buffer(r15_thread, ZThreadLocalData::store_barrier_buffer_offset());\n+\n+  __ movptr(tmp1, buffer);\n+\n+  \/\/ Combined pointer bump and check if the buffer is disabled or full\n+  __ cmpptr(Address(tmp1, ZStoreBarrierBuffer::current_offset()), 0);\n+  __ jcc(Assembler::equal, slow_path);\n+\n+  Register tmp2 = r15_thread;\n+  __ push(tmp2);\n+\n+  \/\/ Bump the pointer\n+  __ movq(tmp2, Address(tmp1, ZStoreBarrierBuffer::current_offset()));\n+  __ subq(tmp2, sizeof(ZStoreBarrierEntry));\n+  __ movq(Address(tmp1, ZStoreBarrierBuffer::current_offset()), tmp2);\n+\n+  \/\/ Compute the buffer entry address\n+  __ lea(tmp2, Address(tmp1, tmp2, Address::times_1, ZStoreBarrierBuffer::buffer_offset()));\n+\n+  \/\/ Compute and log the store address\n+  __ lea(tmp1, ref_addr);\n+  __ movptr(Address(tmp2, in_bytes(ZStoreBarrierEntry::p_offset())), tmp1);\n+\n+  \/\/ Load and log the prev value\n+  __ movptr(tmp1, Address(tmp1, 0));\n+  __ movptr(Address(tmp2, in_bytes(ZStoreBarrierEntry::prev_offset())), tmp1);\n+\n+  __ pop(tmp2);\n+}\n+\n+void ZBarrierSetAssembler::store_barrier_medium(MacroAssembler* masm,\n+                                                Address ref_addr,\n+                                                Register tmp,\n+                                                bool is_native,\n+                                                bool is_atomic,\n+                                                Label& medium_path_continuation,\n+                                                Label& slow_path,\n+                                                Label& slow_path_continuation) const {\n+  assert_different_registers(ref_addr.base(), tmp);\n+\n+  \/\/ The reason to end up in the medium path is that the pre-value was not 'good'.\n+\n+  if (is_native) {\n+    __ jmp(slow_path);\n+    __ bind(slow_path_continuation);\n+    __ jmp(medium_path_continuation);\n+  } else if (is_atomic) {\n+    \/\/ Atomic accesses can get to the medium fast path because the value was a\n+    \/\/ raw null value. If it was not null, then there is no doubt we need to take a slow path.\n+    __ cmpptr(ref_addr, 0);\n+    __ jcc(Assembler::notEqual, slow_path);\n+\n+    \/\/ If we get this far, we know there is a young raw null value in the field.\n+    \/\/ Try to self-heal null values for atomic accesses\n+    __ push(rax);\n+    __ push(rbx);\n+    __ push(rcx);\n+\n+    __ lea(rcx, ref_addr);\n+    __ xorq(rax, rax);\n+    __ movptr(rbx, Address(r15, ZThreadLocalData::store_good_mask_offset()));\n+\n+    __ lock();\n+    __ cmpxchgq(rbx, Address(rcx, 0));\n+\n+    __ pop(rcx);\n+    __ pop(rbx);\n+    __ pop(rax);\n+\n+    __ jcc(Assembler::notEqual, slow_path);\n+\n+    __ bind(slow_path_continuation);\n+    __ jmp(medium_path_continuation);\n+  } else {\n+    \/\/ A non-atomic relocatable object won't get to the medium fast path due to a\n+    \/\/ raw null in the young generation. We only get here because the field is bad.\n+    \/\/ In this path we don't need any self healing, so we can avoid a runtime call\n+    \/\/ most of the time by buffering the store barrier to be applied lazily.\n+    store_barrier_buffer_add(masm,\n+                             ref_addr,\n+                             tmp,\n+                             slow_path);\n+    __ bind(slow_path_continuation);\n+    __ jmp(medium_path_continuation);\n+  }\n+}\n@@ -201,1 +517,2 @@\n-  \/\/ Verify oop store\n+  bool dest_uninitialized = (decorators & IS_DEST_UNINITIALIZED) != 0;\n+\n@@ -203,3 +520,15 @@\n-    \/\/ Note that src could be noreg, which means we\n-    \/\/ are storing null and can skip verification.\n-    if (src != noreg) {\n+    assert_different_registers(src, tmp1, dst.base(), dst.index());\n+\n+    if (dest_uninitialized) {\n+      assert_different_registers(rcx, tmp1);\n+      if (src == noreg) {\n+        __ xorq(tmp1, tmp1);\n+      } else {\n+        __ movptr(tmp1, src);\n+      }\n+      __ push(rcx);\n+      __ movptr(rcx, ExternalAddress((address)&ZPointerLoadShift));\n+      __ shlq(tmp1);\n+      __ pop(rcx);\n+      __ orq(tmp1, Address(r15_thread, ZThreadLocalData::store_good_mask_offset()));\n+    } else {\n@@ -207,4 +536,25 @@\n-      __ testptr(src, address_bad_mask_from_thread(r15_thread));\n-      __ jcc(Assembler::zero, done);\n-      __ stop(\"Verify oop store failed\");\n-      __ should_not_reach_here();\n+      Label medium;\n+      Label medium_continuation;\n+      Label slow;\n+      Label slow_continuation;\n+      store_barrier_fast(masm, dst, src, tmp1, false, false, medium, medium_continuation);\n+      __ jmp(done);\n+      __ bind(medium);\n+      store_barrier_medium(masm,\n+                           dst,\n+                           tmp1,\n+                           false \/* is_native *\/,\n+                           false \/* is_atomic *\/,\n+                           medium_continuation,\n+                           slow,\n+                           slow_continuation);\n+\n+      __ bind(slow);\n+      {\n+        \/\/ Call VM\n+        ZRuntimeCallSpill rcs(masm, noreg, ZXMMSpillMode::avx128);\n+        __ leaq(c_rarg0, dst);\n+        __ MacroAssembler::call_VM_leaf(ZBarrierSetRuntime::store_barrier_on_oop_field_without_healing_addr(), c_rarg0);\n+      }\n+\n+      __ jmp(slow_continuation);\n@@ -213,0 +563,32 @@\n+\n+    \/\/ Store value\n+    BarrierSetAssembler::store_at(masm, decorators, type, dst, tmp1, noreg, noreg, noreg);\n+  } else {\n+    BarrierSetAssembler::store_at(masm, decorators, type, dst, src, noreg, noreg, noreg);\n+  }\n+\n+  BLOCK_COMMENT(\"} ZBarrierSetAssembler::store_at\");\n+}\n+\n+bool ZBarrierSetAssembler::supports_avx3_masked_arraycopy() {\n+  return false;\n+}\n+\n+static void load_arraycopy_masks(MacroAssembler* masm) {\n+  \/\/ xmm2: load_bad_mask\n+  \/\/ xmm3: store_bad_mask\n+  \/\/ xmm4: store_good_mask\n+  if (UseAVX >= 2) {\n+    __ lea(r10, ExternalAddress((address)&ZPointerVectorLoadBadMask));\n+    __ vmovdqu(xmm2, Address(r10, 0));\n+    __ lea(r10, ExternalAddress((address)&ZPointerVectorStoreBadMask));\n+    __ vmovdqu(xmm3, Address(r10, 0));\n+    __ lea(r10, ExternalAddress((address)&ZPointerVectorStoreGoodMask));\n+    __ vmovdqu(xmm4, Address(r10, 0));\n+  } else {\n+    __ lea(r10, ExternalAddress((address)&ZPointerVectorLoadBadMask));\n+    __ movdqu(xmm2, Address(r10, 0));\n+    __ lea(r10, ExternalAddress((address)&ZPointerVectorStoreBadMask));\n+    __ movdqu(xmm3, Address(r10, 0));\n+    __ lea(r10, ExternalAddress((address)&ZPointerVectorStoreGoodMask));\n+    __ movdqu(xmm4, Address(r10, 0));\n@@ -214,0 +596,101 @@\n+}\n+\n+static ZXMMSpillMode compute_arraycopy_spill_mode() {\n+  if (UseAVX >= 2) {\n+    return ZXMMSpillMode::avx256;\n+  } else {\n+    return ZXMMSpillMode::avx128;\n+  }\n+}\n+\n+void ZBarrierSetAssembler::copy_load_at(MacroAssembler* masm,\n+                                        DecoratorSet decorators,\n+                                        BasicType type,\n+                                        size_t bytes,\n+                                        Register dst,\n+                                        Address src,\n+                                        Register tmp) {\n+  if (!is_reference_type(type)) {\n+    BarrierSetAssembler::copy_load_at(masm, decorators, type, bytes, dst, src, tmp);\n+    return;\n+  }\n+\n+  Label load_done;\n+\n+  \/\/ Load oop at address\n+  __ movptr(dst, src);\n+\n+  \/\/ Test address bad mask\n+  __ Assembler::testl(dst, (int32_t)(uint32_t)ZPointerLoadBadMask);\n+  _load_bad_relocations.append(__ code_section()->end());\n+  __ jcc(Assembler::zero, load_done);\n+\n+  {\n+    \/\/ Call VM\n+    ZRuntimeCallSpill rcs(masm, dst, compute_arraycopy_spill_mode());\n+    __ leaq(c_rarg1, src);\n+    call_vm(masm, ZBarrierSetRuntime::load_barrier_on_oop_field_preloaded_store_good_addr(), dst, c_rarg1);\n+  }\n+\n+  __ bind(load_done);\n+\n+  \/\/ Remove metadata bits so that the store side (vectorized or non-vectorized) can\n+  \/\/ inject the store-good color with an or instruction.\n+  __ andq(dst, _zpointer_address_mask);\n+\n+  if ((decorators & ARRAYCOPY_CHECKCAST) != 0) {\n+    \/\/ The checkcast arraycopy needs to be able to dereference the oops in order to perform a typechecks.\n+    assert(tmp != rcx, \"Surprising choice of temp register\");\n+    __ movptr(tmp, rcx);\n+    __ movptr(rcx, ExternalAddress((address)&ZPointerLoadShift));\n+    __ shrq(dst);\n+    __ movptr(rcx, tmp);\n+  }\n+}\n+\n+void ZBarrierSetAssembler::copy_store_at(MacroAssembler* masm,\n+                                         DecoratorSet decorators,\n+                                         BasicType type,\n+                                         size_t bytes,\n+                                         Address dst,\n+                                         Register src,\n+                                         Register tmp) {\n+  if (!is_reference_type(type)) {\n+    BarrierSetAssembler::copy_store_at(masm, decorators, type, bytes, dst, src, tmp);\n+    return;\n+  }\n+\n+  bool dest_uninitialized = (decorators & IS_DEST_UNINITIALIZED) != 0;\n+\n+  if (!dest_uninitialized) {\n+    Label store;\n+    Label store_bad;\n+    __ Assembler::testl(dst, (int32_t)(uint32_t)ZPointerStoreBadMask);\n+    _store_bad_relocations.append(__ code_section()->end());\n+    __ jcc(Assembler::zero, store);\n+\n+    store_barrier_buffer_add(masm, dst, tmp, store_bad);\n+    __ jmp(store);\n+\n+    __ bind(store_bad);\n+    {\n+      \/\/ Call VM\n+      ZRuntimeCallSpill rcs(masm, noreg, compute_arraycopy_spill_mode());\n+      __ leaq(c_rarg0, dst);\n+      __ MacroAssembler::call_VM_leaf(ZBarrierSetRuntime::store_barrier_on_oop_field_without_healing_addr(), c_rarg0);\n+    }\n+\n+    __ bind(store);\n+  }\n+\n+  if ((decorators & ARRAYCOPY_CHECKCAST) != 0) {\n+    assert(tmp != rcx, \"Surprising choice of temp register\");\n+    __ movptr(tmp, rcx);\n+    __ movptr(rcx, ExternalAddress((address)&ZPointerLoadShift));\n+    __ shlq(src);\n+    __ movptr(rcx, tmp);\n+  }\n+\n+  \/\/ Color\n+  __ orq_imm32(src, (int32_t)(uint32_t)ZPointerStoreGoodMask);\n+  _store_good_relocations.append(__ code_section()->end());\n@@ -216,1 +699,2 @@\n-  BarrierSetAssembler::store_at(masm, decorators, type, dst, src, tmp1, tmp2, tmp3);\n+  __ movptr(dst, src);\n+}\n@@ -218,1 +702,88 @@\n-  BLOCK_COMMENT(\"} ZBarrierSetAssembler::store_at\");\n+void ZBarrierSetAssembler::copy_load_at(MacroAssembler* masm,\n+                                        DecoratorSet decorators,\n+                                        BasicType type,\n+                                        size_t bytes,\n+                                        XMMRegister dst,\n+                                        Address src,\n+                                        Register tmp,\n+                                        XMMRegister xmm_tmp) {\n+  if (!is_reference_type(type)) {\n+    BarrierSetAssembler::copy_load_at(masm, decorators, type, bytes, dst, src, tmp, xmm_tmp);\n+    return;\n+  }\n+  Address src0(src.base(), src.index(), src.scale(), src.disp() + 0);\n+  Address src1(src.base(), src.index(), src.scale(), src.disp() + 8);\n+  Address src2(src.base(), src.index(), src.scale(), src.disp() + 16);\n+  Address src3(src.base(), src.index(), src.scale(), src.disp() + 24);\n+\n+  \/\/ Registers set up in the prologue:\n+  \/\/ xmm2: load_bad_mask\n+  \/\/ xmm3: store_bad_mask\n+  \/\/ xmm4: store_good_mask\n+\n+  if (bytes == 16) {\n+    Label done;\n+    Label fallback;\n+\n+    if (UseAVX >= 1) {\n+      \/\/ Load source vector\n+      __ movdqu(dst, src);\n+      \/\/ Check source load-good\n+      __ movdqu(xmm_tmp, dst);\n+      __ ptest(xmm_tmp, xmm2);\n+      __ jcc(Assembler::notZero, fallback);\n+\n+      \/\/ Remove bad metadata bits\n+      __ vpandn(dst, xmm3, dst, Assembler::AVX_128bit);\n+      __ jmp(done);\n+    }\n+\n+    __ bind(fallback);\n+\n+    __ subptr(rsp, wordSize * 2);\n+\n+    ZBarrierSetAssembler::copy_load_at(masm, decorators, type, 8, tmp, src0, noreg);\n+    __ movq(Address(rsp, 0), tmp);\n+    ZBarrierSetAssembler::copy_load_at(masm, decorators, type, 8, tmp, src1, noreg);\n+    __ movq(Address(rsp, 8), tmp);\n+\n+    __ movdqu(dst, Address(rsp, 0));\n+    __ addptr(rsp, wordSize * 2);\n+\n+    __ bind(done);\n+  } else if (bytes == 32) {\n+    Label done;\n+    Label fallback;\n+    assert(UseAVX >= 2, \"Assume that UseAVX >= 2\");\n+\n+    \/\/ Load source vector\n+    __ vmovdqu(dst, src);\n+    \/\/ Check source load-good\n+    __ vmovdqu(xmm_tmp, dst);\n+    __ vptest(xmm_tmp, xmm2, Assembler::AVX_256bit);\n+    __ jcc(Assembler::notZero, fallback);\n+\n+    \/\/ Remove bad metadata bits so that the store can colour the pointers with an or instruction.\n+    \/\/ This makes the fast path and slow path formats look the same, in the sense that they don't\n+    \/\/ have any of the store bad bits.\n+    __ vpandn(dst, xmm3, dst, Assembler::AVX_256bit);\n+    __ jmp(done);\n+\n+    __ bind(fallback);\n+\n+    __ subptr(rsp, wordSize * 4);\n+\n+    ZBarrierSetAssembler::copy_load_at(masm, decorators, type, 8, tmp, src0, noreg);\n+    __ movq(Address(rsp, 0), tmp);\n+    ZBarrierSetAssembler::copy_load_at(masm, decorators, type, 8, tmp, src1, noreg);\n+    __ movq(Address(rsp, 8), tmp);\n+    ZBarrierSetAssembler::copy_load_at(masm, decorators, type, 8, tmp, src2, noreg);\n+    __ movq(Address(rsp, 16), tmp);\n+    ZBarrierSetAssembler::copy_load_at(masm, decorators, type, 8, tmp, src3, noreg);\n+    __ movq(Address(rsp, 24), tmp);\n+\n+    __ vmovdqu(dst, Address(rsp, 0));\n+    __ addptr(rsp, wordSize * 4);\n+\n+    __ bind(done);\n+  }\n@@ -221,1 +792,97 @@\n-#endif \/\/ ASSERT\n+void ZBarrierSetAssembler::copy_store_at(MacroAssembler* masm,\n+                                         DecoratorSet decorators,\n+                                         BasicType type,\n+                                         size_t bytes,\n+                                         Address dst,\n+                                         XMMRegister src,\n+                                         Register tmp1,\n+                                         Register tmp2,\n+                                         XMMRegister xmm_tmp) {\n+  if (!is_reference_type(type)) {\n+    BarrierSetAssembler::copy_store_at(masm, decorators, type, bytes, dst, src, tmp1, tmp2, xmm_tmp);\n+    return;\n+  }\n+  Address dst0(dst.base(), dst.index(), dst.scale(), dst.disp() + 0);\n+  Address dst1(dst.base(), dst.index(), dst.scale(), dst.disp() + 8);\n+  Address dst2(dst.base(), dst.index(), dst.scale(), dst.disp() + 16);\n+  Address dst3(dst.base(), dst.index(), dst.scale(), dst.disp() + 24);\n+\n+  bool dest_uninitialized = (decorators & IS_DEST_UNINITIALIZED) != 0;\n+\n+  \/\/ Registers set up in the prologue:\n+  \/\/ xmm2: load_bad_mask\n+  \/\/ xmm3: store_bad_mask\n+  \/\/ xmm4: store_good_mask\n+\n+  if (bytes == 16) {\n+    Label done;\n+    Label fallback;\n+\n+    if (UseAVX >= 1) {\n+      if (!dest_uninitialized) {\n+        \/\/ Load destination vector\n+        __ movdqu(xmm_tmp, dst);\n+        \/\/ Check destination store-good\n+        __ ptest(xmm_tmp, xmm3);\n+        __ jcc(Assembler::notZero, fallback);\n+      }\n+\n+      \/\/ Color source\n+      __ por(src, xmm4);\n+      \/\/ Store source in destination\n+      __ movdqu(dst, src);\n+      __ jmp(done);\n+    }\n+\n+    __ bind(fallback);\n+\n+    __ subptr(rsp, wordSize * 2);\n+    __ movdqu(Address(rsp, 0), src);\n+\n+    __ movq(tmp1, Address(rsp, 0));\n+    ZBarrierSetAssembler::copy_store_at(masm, decorators, type, 8, dst0, tmp1, tmp2);\n+    __ movq(tmp1, Address(rsp, 8));\n+    ZBarrierSetAssembler::copy_store_at(masm, decorators, type, 8, dst1, tmp1, tmp2);\n+\n+    __ addptr(rsp, wordSize * 2);\n+\n+    __ bind(done);\n+  } else if (bytes == 32) {\n+    Label done;\n+    Label fallback;\n+    assert(UseAVX >= 2, \"Assume UseAVX >= 2\");\n+\n+    if (!dest_uninitialized) {\n+      \/\/ Load destination vector\n+      __ vmovdqu(xmm_tmp, dst);\n+      \/\/ Check destination store-good\n+      __ vptest(xmm_tmp, xmm3, Assembler::AVX_256bit);\n+      __ jcc(Assembler::notZero, fallback);\n+    }\n+\n+    \/\/ Color source\n+    __ vpor(src, src, xmm4, Assembler::AVX_256bit);\n+\n+    \/\/ Store colored source in destination\n+    __ vmovdqu(dst, src);\n+    __ jmp(done);\n+\n+    __ bind(fallback);\n+\n+    __ subptr(rsp, wordSize * 4);\n+    __ vmovdqu(Address(rsp, 0), src);\n+\n+    __ movq(tmp1, Address(rsp, 0));\n+    ZBarrierSetAssembler::copy_store_at(masm, decorators, type, 8, dst0, tmp1, tmp2);\n+    __ movq(tmp1, Address(rsp, 8));\n+    ZBarrierSetAssembler::copy_store_at(masm, decorators, type, 8, dst1, tmp1, tmp2);\n+    __ movq(tmp1, Address(rsp, 16));\n+    ZBarrierSetAssembler::copy_store_at(masm, decorators, type, 8, dst2, tmp1, tmp2);\n+    __ movq(tmp1, Address(rsp, 24));\n+    ZBarrierSetAssembler::copy_store_at(masm, decorators, type, 8, dst3, tmp1, tmp2);\n+\n+    __ addptr(rsp, wordSize * 4);\n+\n+    __ bind(done);\n+  }\n+}\n@@ -236,8 +903,1 @@\n-  \/\/ Save registers\n-  __ pusha();\n-\n-  \/\/ Call VM\n-  call_vm(masm, ZBarrierSetRuntime::load_barrier_on_oop_array_addr(), src, count);\n-\n-  \/\/ Restore registers\n-  __ popa();\n+  load_arraycopy_masks(masm);\n@@ -255,2 +915,1 @@\n-  \/\/ Resolve jobject\n-  BarrierSetAssembler::try_resolve_jobject_in_native(masm, jni_env, obj, tmp, slowpath);\n+  Label done, tagged, weak_tagged, uncolor;\n@@ -258,2 +917,17 @@\n-  \/\/ Test address bad mask\n-  __ testptr(obj, address_bad_mask_from_jni_env(jni_env));\n+  \/\/ Test for tag\n+  __ testptr(obj, JNIHandles::tag_mask);\n+  __ jcc(Assembler::notZero, tagged);\n+\n+  \/\/ Resolve local handle\n+  __ movptr(obj, Address(obj, 0));\n+  __ jmp(done);\n+\n+  __ bind(tagged);\n+\n+  \/\/ Test for weak tag\n+  __ testptr(obj, JNIHandles::TypeTag::weak_global);\n+  __ jcc(Assembler::notZero, weak_tagged);\n+\n+  \/\/ Resolve global handle\n+  __ movptr(obj, Address(obj, -JNIHandles::TypeTag::global));\n+  __ testptr(obj, load_bad_mask_from_jni_env(jni_env));\n@@ -261,0 +935,25 @@\n+  __ jmp(uncolor);\n+\n+  __ bind(weak_tagged);\n+\n+  \/\/ Resolve weak handle\n+  __ movptr(obj, Address(obj, -JNIHandles::TypeTag::weak_global));\n+  __ testptr(obj, mark_bad_mask_from_jni_env(jni_env));\n+  __ jcc(Assembler::notZero, slowpath);\n+\n+  __ bind(uncolor);\n+\n+  \/\/ Uncolor\n+  if (obj == rcx) {\n+    __ movptr(tmp, obj);\n+    __ movptr(rcx, ExternalAddress((address)&ZPointerLoadShift));\n+    __ shrq(tmp);\n+    __ movptr(obj, tmp);\n+  } else {\n+    __ push(rcx);\n+    __ movptr(rcx, ExternalAddress((address)&ZPointerLoadShift));\n+    __ shrq(obj);\n+    __ pop(rcx);\n+  }\n+\n+  __ bind(done);\n@@ -270,3 +969,39 @@\n-void ZBarrierSetAssembler::generate_c1_load_barrier_test(LIR_Assembler* ce,\n-                                                         LIR_Opr ref) const {\n-  __ testptr(ref->as_register(), address_bad_mask_from_thread(r15_thread));\n+static void z_uncolor(LIR_Assembler* ce, LIR_Opr ref) {\n+  __ relocate(barrier_Relocation::spec(), ZBarrierRelocationFormatLoadGoodBeforeShl);\n+  __ shrq(ref->as_register(), barrier_Relocation::unpatched);\n+}\n+\n+static void z_color(LIR_Assembler* ce, LIR_Opr ref) {\n+  __ relocate(barrier_Relocation::spec(), ZBarrierRelocationFormatLoadGoodBeforeShl);\n+  __ shlq(ref->as_register(), barrier_Relocation::unpatched);\n+  __ orq_imm32(ref->as_register(), barrier_Relocation::unpatched);\n+  __ relocate(barrier_Relocation::spec(), ZBarrierRelocationFormatStoreGoodAfterOr);\n+}\n+\n+void ZBarrierSetAssembler::generate_c1_uncolor(LIR_Assembler* ce, LIR_Opr ref) const {\n+  z_uncolor(ce, ref);\n+}\n+\n+void ZBarrierSetAssembler::generate_c1_color(LIR_Assembler* ce, LIR_Opr ref) const {\n+  z_color(ce, ref);\n+}\n+\n+void ZBarrierSetAssembler::generate_c1_load_barrier(LIR_Assembler* ce,\n+                                                    LIR_Opr ref,\n+                                                    ZLoadBarrierStubC1* stub,\n+                                                    bool on_non_strong) const {\n+  if (on_non_strong) {\n+    \/\/ Test against MarkBad mask\n+    __ Assembler::testl(ref->as_register(), barrier_Relocation::unpatched);\n+    __ relocate(barrier_Relocation::spec(), ZBarrierRelocationFormatMarkBadAfterTest);\n+\n+    \/\/ Slow path if not zero\n+    __ jcc(Assembler::notZero, *stub->entry());\n+    \/\/ Fast path: convert to colorless\n+    z_uncolor(ce, ref);\n+  } else {\n+    \/\/ Convert to colorless and fast path test\n+    z_uncolor(ce, ref);\n+    __ jcc(Assembler::above, *stub->entry());\n+  }\n+  __ bind(*stub->continuation());\n@@ -284,0 +1019,3 @@\n+  \/\/ The fast-path shift destroyed the oop - need to re-read it\n+  __ movptr(ref, ce->as_Address(stub->ref_addr()->as_address_ptr()));\n+\n@@ -324,0 +1062,49 @@\n+void ZBarrierSetAssembler::generate_c1_store_barrier(LIR_Assembler* ce,\n+                                                     LIR_Address* addr,\n+                                                     LIR_Opr new_zaddress,\n+                                                     LIR_Opr new_zpointer,\n+                                                     ZStoreBarrierStubC1* stub) const {\n+  Register rnew_zaddress = new_zaddress->as_register();\n+  Register rnew_zpointer = new_zpointer->as_register();\n+\n+  Register rbase = addr->base()->as_pointer_register();\n+  store_barrier_fast(ce->masm(),\n+                     ce->as_Address(addr),\n+                     rnew_zaddress,\n+                     rnew_zpointer,\n+                     true,\n+                     stub->is_atomic(),\n+                     *stub->entry(),\n+                     *stub->continuation());\n+}\n+\n+void ZBarrierSetAssembler::generate_c1_store_barrier_stub(LIR_Assembler* ce,\n+                                                          ZStoreBarrierStubC1* stub) const {\n+  \/\/ Stub entry\n+  __ bind(*stub->entry());\n+\n+  Label slow;\n+  Label slow_continuation;\n+  store_barrier_medium(ce->masm(),\n+                       ce->as_Address(stub->ref_addr()->as_address_ptr()),\n+                       rscratch1,\n+                       false \/* is_native *\/,\n+                       stub->is_atomic(),\n+                       *stub->continuation(),\n+                       slow,\n+                       slow_continuation);\n+\n+  __ bind(slow);\n+\n+  ce->leal(stub->ref_addr(), stub->new_zpointer());\n+\n+  \/\/ Setup arguments and call runtime stub\n+  __ subptr(rsp, 2 * BytesPerWord);\n+  ce->store_parameter(stub->new_zpointer()->as_pointer_register(), 0);\n+  __ call(RuntimeAddress(stub->runtime_stub()));\n+  __ addptr(rsp, 2 * BytesPerWord);\n+\n+  \/\/ Stub exit\n+  __ jmp(slow_continuation);\n+}\n+\n@@ -346,0 +1133,22 @@\n+void ZBarrierSetAssembler::generate_c1_store_barrier_runtime_stub(StubAssembler* sasm,\n+                                                                  bool self_healing) const {\n+  \/\/ Enter and save registers\n+  __ enter();\n+  __ save_live_registers_no_oop_map(true \/* save_fpu_registers *\/);\n+\n+  \/\/ Setup arguments\n+  __ load_parameter(0, c_rarg0);\n+\n+  \/\/ Call VM\n+  if (self_healing) {\n+    __ call_VM_leaf(ZBarrierSetRuntime::store_barrier_on_oop_field_with_healing_addr(), c_rarg0);\n+  } else {\n+    __ call_VM_leaf(ZBarrierSetRuntime::store_barrier_on_oop_field_without_healing_addr(), c_rarg0);\n+  }\n+\n+  \/\/ Restore registers and return\n+  __ restore_live_registers(true \/* restore_fpu_registers *\/);\n+  __ leave();\n+  __ ret(0);\n+}\n+\n@@ -470,1 +1279,1 @@\n-  void initialize(ZLoadBarrierStubC2* stub) {\n+  void initialize(ZBarrierStubC2* stub) {\n@@ -483,1 +1292,4 @@\n-    caller_saved.Remove(OptoReg::as_OptoReg(stub->ref()->as_VMReg()));\n+\n+    if (stub->result() != noreg) {\n+      caller_saved.Remove(OptoReg::as_OptoReg(stub->result()->as_VMReg()));\n+    }\n@@ -487,3 +1299,0 @@\n-    if (stub->tmp() != noreg) {\n-      live.Insert(OptoReg::as_OptoReg(stub->tmp()->as_VMReg()));\n-    }\n@@ -547,1 +1356,1 @@\n-  ZSaveLiveRegisters(MacroAssembler* masm, ZLoadBarrierStubC2* stub) :\n+  ZSaveLiveRegisters(MacroAssembler* masm, ZBarrierStubC2* stub) :\n@@ -686,0 +1495,1 @@\n+  Assembler::InlineSkippedInstructionsCounter skipped_counter(masm);\n@@ -691,0 +1501,3 @@\n+  \/\/ The fast-path shift destroyed the oop - need to re-read it\n+  __ movptr(stub->ref(), stub->ref_addr());\n+\n@@ -701,0 +1514,112 @@\n+void ZBarrierSetAssembler::generate_c2_store_barrier_stub(MacroAssembler* masm, ZStoreBarrierStubC2* stub) const {\n+  Assembler::InlineSkippedInstructionsCounter skipped_counter(masm);\n+  BLOCK_COMMENT(\"ZStoreBarrierStubC2\");\n+\n+  \/\/ Stub entry\n+  __ bind(*stub->entry());\n+\n+  Label slow;\n+  Label slow_continuation;\n+  store_barrier_medium(masm,\n+                       stub->ref_addr(),\n+                       stub->new_zpointer(),\n+                       stub->is_native(),\n+                       stub->is_atomic(),\n+                       *stub->continuation(),\n+                       slow,\n+                       slow_continuation);\n+\n+  __ bind(slow);\n+\n+  {\n+    ZSaveLiveRegisters save_live_registers(masm, stub);\n+    __ lea(c_rarg0, stub->ref_addr());\n+\n+    if (stub->is_native()) {\n+      __ call(RuntimeAddress(ZBarrierSetRuntime::store_barrier_on_native_oop_field_without_healing_addr()));\n+    } else if (stub->is_atomic()) {\n+      __ call(RuntimeAddress(ZBarrierSetRuntime::store_barrier_on_oop_field_with_healing_addr()));\n+    } else {\n+      __ call(RuntimeAddress(ZBarrierSetRuntime::store_barrier_on_oop_field_without_healing_addr()));\n+    }\n+  }\n+\n+  \/\/ Stub exit\n+  __ jmp(slow_continuation);\n+}\n+\n+#undef __\n+\n+static int patch_barrier_relocation_offset(int format) {\n+  switch (format) {\n+  case ZBarrierRelocationFormatLoadGoodBeforeShl:\n+    return 3;\n+\n+  case ZBarrierRelocationFormatStoreGoodAfterCmp:\n+    return -2;\n+\n+  case ZBarrierRelocationFormatLoadBadAfterTest:\n+  case ZBarrierRelocationFormatMarkBadAfterTest:\n+  case ZBarrierRelocationFormatStoreBadAfterTest:\n+  case ZBarrierRelocationFormatStoreGoodAfterOr:\n+    return -4;\n+  case ZBarrierRelocationFormatStoreGoodAfterMov:\n+    return -3;\n+\n+  default:\n+    ShouldNotReachHere();\n+    return 0;\n+  }\n+}\n+\n+static uint16_t patch_barrier_relocation_value(int format) {\n+  switch (format) {\n+  case ZBarrierRelocationFormatLoadGoodBeforeShl:\n+    return (uint16_t)ZPointerLoadShift;\n+\n+  case ZBarrierRelocationFormatMarkBadAfterTest:\n+    return (uint16_t)ZPointerMarkBadMask;\n+\n+  case ZBarrierRelocationFormatLoadBadAfterTest:\n+    return (uint16_t)ZPointerLoadBadMask;\n+\n+  case ZBarrierRelocationFormatStoreGoodAfterCmp:\n+  case ZBarrierRelocationFormatStoreGoodAfterOr:\n+  case ZBarrierRelocationFormatStoreGoodAfterMov:\n+    return (uint16_t)ZPointerStoreGoodMask;\n+\n+  case ZBarrierRelocationFormatStoreBadAfterTest:\n+    return (uint16_t)ZPointerStoreBadMask;\n+\n+  default:\n+    ShouldNotReachHere();\n+    return 0;\n+  }\n+}\n+\n+void ZBarrierSetAssembler::patch_barrier_relocation(address addr, int format) {\n+  const int offset = patch_barrier_relocation_offset(format);\n+  const uint16_t value = patch_barrier_relocation_value(format);\n+  uint8_t* const patch_addr = (uint8_t*)addr + offset;\n+  if (format == ZBarrierRelocationFormatLoadGoodBeforeShl) {\n+    *patch_addr = (uint8_t)value;\n+  } else {\n+    *(uint16_t*)patch_addr = value;\n+  }\n+}\n+\n+void ZBarrierSetAssembler::patch_barriers() {\n+  for (int i = 0; i < _load_bad_relocations.length(); ++i) {\n+    address addr = _load_bad_relocations.at(i);\n+    patch_barrier_relocation(addr, ZBarrierRelocationFormatLoadBadAfterTest);\n+  }\n+  for (int i = 0; i < _store_bad_relocations.length(); ++i) {\n+    address addr = _store_bad_relocations.at(i);\n+    patch_barrier_relocation(addr, ZBarrierRelocationFormatStoreBadAfterTest);\n+  }\n+  for (int i = 0; i < _store_good_relocations.length(); ++i) {\n+    address addr = _store_good_relocations.at(i);\n+    patch_barrier_relocation(addr, ZBarrierRelocationFormatStoreGoodAfterOr);\n+  }\n+}\n+\n@@ -706,0 +1631,1 @@\n+\n@@ -707,2 +1633,59 @@\n-  \/\/ Check if metadata bits indicate a bad oop\n-  __ testptr(obj, Address(r15_thread, ZThreadLocalData::address_bad_mask_offset()));\n+  \/\/ C1 calls verfy_oop in the middle of barriers, before they have been uncolored\n+  \/\/ and after being colored. Therefore, we must deal with colored oops as well.\n+  Label done;\n+  Label check_oop;\n+  Label check_zaddress;\n+  int color_bits = ZPointerRemappedShift + ZPointerRemappedBits;\n+\n+  uintptr_t shifted_base_start_mask = (UCONST64(1) << (ZAddressHeapBaseShift + color_bits + 1)) - 1;\n+  uintptr_t shifted_base_end_mask = (UCONST64(1) << (ZAddressHeapBaseShift + 1)) - 1;\n+  uintptr_t shifted_base_mask = shifted_base_start_mask ^ shifted_base_end_mask;\n+\n+  uintptr_t shifted_address_end_mask = (UCONST64(1) << (color_bits + 1)) - 1;\n+  uintptr_t shifted_address_mask = shifted_address_end_mask ^ (uintptr_t)CONST64(-1);\n+\n+  \/\/ Check colored null\n+  __ mov64(tmp1, shifted_address_mask);\n+  __ testptr(tmp1, obj);\n+  __ jcc(Assembler::zero, done);\n+\n+  \/\/ Check for zpointer\n+  __ mov64(tmp1, shifted_base_mask);\n+  __ testptr(tmp1, obj);\n+  __ jcc(Assembler::zero, check_oop);\n+\n+  \/\/ Lookup shift\n+  __ movq(tmp1, obj);\n+  __ mov64(tmp2, shifted_address_end_mask);\n+  __ andq(tmp1, tmp2);\n+  __ shrq(tmp1, ZPointerRemappedShift);\n+  __ andq(tmp1, (1 << ZPointerRemappedBits) - 1);\n+  __ lea(tmp2, ExternalAddress((address)&ZPointerLoadShiftTable));\n+\n+  \/\/ Uncolor presumed zpointer\n+  assert(obj != rcx, \"bad choice of register\");\n+  if (rcx != tmp1 && rcx != tmp2) {\n+    __ push(rcx);\n+  }\n+  __ movl(rcx, Address(tmp2, tmp1, Address::times_4, 0));\n+  __ shrq(obj);\n+  if (rcx != tmp1 && rcx != tmp2) {\n+    __ pop(rcx);\n+  }\n+\n+  __ jmp(check_zaddress);\n+\n+  __ bind(check_oop);\n+\n+  \/\/ make sure klass is 'reasonable', which is not zero.\n+  __ load_klass(tmp1, obj, tmp2);  \/\/ get klass\n+  __ testptr(tmp1, tmp1);\n+  __ jcc(Assembler::zero, error); \/\/ if klass is null it is broken\n+\n+  __ bind(check_zaddress);\n+  \/\/ Check if the oop is in the right area of memory\n+  __ movptr(tmp1, obj);\n+  __ movptr(tmp2, (intptr_t) Universe::verify_oop_mask());\n+  __ andptr(tmp1, tmp2);\n+  __ movptr(tmp2, (intptr_t) Universe::verify_oop_bits());\n+  __ cmpptr(tmp1, tmp2);\n@@ -710,1 +1693,2 @@\n-  BarrierSetAssembler::check_oop(masm, obj, tmp1, tmp2, error);\n+\n+  __ bind(done);\n","filename":"src\/hotspot\/cpu\/x86\/gc\/z\/zBarrierSetAssembler_x86.cpp","additions":1074,"deletions":90,"binary":false,"changes":1164,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -36,0 +36,2 @@\n+class CodeStub;\n+class LIR_Address;\n@@ -40,0 +42,1 @@\n+class ZStoreBarrierStubC1;\n@@ -43,0 +46,1 @@\n+class MachNode;\n@@ -45,0 +49,1 @@\n+class ZStoreBarrierStubC2;\n@@ -47,0 +52,8 @@\n+const int ZBarrierRelocationFormatLoadGoodBeforeShl = 0;\n+const int ZBarrierRelocationFormatLoadBadAfterTest  = 1;\n+const int ZBarrierRelocationFormatMarkBadAfterTest  = 2;\n+const int ZBarrierRelocationFormatStoreGoodAfterCmp = 3;\n+const int ZBarrierRelocationFormatStoreBadAfterTest = 4;\n+const int ZBarrierRelocationFormatStoreGoodAfterOr  = 5;\n+const int ZBarrierRelocationFormatStoreGoodAfterMov = 6;\n+\n@@ -48,0 +61,5 @@\n+private:\n+  GrowableArrayCHeap<address, mtGC> _load_bad_relocations;\n+  GrowableArrayCHeap<address, mtGC> _store_bad_relocations;\n+  GrowableArrayCHeap<address, mtGC> _store_good_relocations;\n+\n@@ -49,0 +67,4 @@\n+  static const int32_t _zpointer_address_mask = 0xFFFF0000;\n+\n+  ZBarrierSetAssembler();\n+\n@@ -57,1 +79,0 @@\n-#ifdef ASSERT\n@@ -66,1 +87,37 @@\n-#endif \/\/ ASSERT\n+\n+  virtual bool supports_avx3_masked_arraycopy();\n+\n+  virtual void copy_load_at(MacroAssembler* masm,\n+                            DecoratorSet decorators,\n+                            BasicType type,\n+                            size_t bytes,\n+                            Register dst,\n+                            Address src,\n+                            Register tmp);\n+\n+  virtual void copy_store_at(MacroAssembler* masm,\n+                             DecoratorSet decorators,\n+                             BasicType type,\n+                             size_t bytes,\n+                             Address dst,\n+                             Register src,\n+                             Register tmp);\n+\n+  virtual void copy_load_at(MacroAssembler* masm,\n+                            DecoratorSet decorators,\n+                            BasicType type,\n+                            size_t bytes,\n+                            XMMRegister dst,\n+                            Address src,\n+                            Register tmp,\n+                            XMMRegister xmm_tmp);\n+\n+  virtual void copy_store_at(MacroAssembler* masm,\n+                             DecoratorSet decorators,\n+                             BasicType type,\n+                             size_t bytes,\n+                             Address dst,\n+                             XMMRegister src,\n+                             Register tmp1,\n+                             Register tmp2,\n+                             XMMRegister xmm_tmp);\n@@ -82,2 +139,19 @@\n-  void generate_c1_load_barrier_test(LIR_Assembler* ce,\n-                                     LIR_Opr ref) const;\n+  void generate_c1_color(LIR_Assembler* ce, LIR_Opr ref) const;\n+  void generate_c1_uncolor(LIR_Assembler* ce, LIR_Opr ref) const;\n+\n+  void generate_c1_store_barrier(LIR_Assembler* ce,\n+                                 LIR_Address* addr,\n+                                 LIR_Opr new_zaddress,\n+                                 LIR_Opr new_zpointer,\n+                                 ZStoreBarrierStubC1* stub) const;\n+\n+  void generate_c1_store_barrier_stub(LIR_Assembler* ce,\n+                                      ZStoreBarrierStubC1* stub) const;\n+\n+  void generate_c1_store_barrier_runtime_stub(StubAssembler* sasm,\n+                                              bool self_healing) const;\n+\n+  void generate_c1_load_barrier(LIR_Assembler* ce,\n+                                LIR_Opr ref,\n+                                ZLoadBarrierStubC1* stub,\n+                                bool on_non_strong) const;\n@@ -98,0 +172,2 @@\n+  void generate_c2_store_barrier_stub(MacroAssembler* masm,\n+                                      ZStoreBarrierStubC2* stub) const;\n@@ -100,0 +176,22 @@\n+  void store_barrier_fast(MacroAssembler* masm,\n+                          Address ref_addr,\n+                          Register rnew_persistent,\n+                          Register rnew_transient,\n+                          bool in_nmethod,\n+                          bool is_atomic,\n+                          Label& medium_path,\n+                          Label& medium_path_continuation) const;\n+\n+  void store_barrier_medium(MacroAssembler* masm,\n+                            Address ref_addr,\n+                            Register tmp,\n+                            bool is_native,\n+                            bool is_atomic,\n+                            Label& medium_path_continuation,\n+                            Label& slow_path,\n+                            Label& slow_path_continuation) const;\n+\n+  void patch_barrier_relocation(address addr, int format);\n+\n+  void patch_barriers();\n+\n","filename":"src\/hotspot\/cpu\/x86\/gc\/z\/zBarrierSetAssembler_x86.hpp","additions":103,"deletions":5,"binary":false,"changes":108,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,1 +27,0 @@\n-const size_t ZPlatformHeapViews        = 3;\n@@ -30,3 +29,0 @@\n-size_t ZPlatformAddressOffsetBits();\n-size_t ZPlatformAddressMetadataShift();\n-\n","filename":"src\/hotspot\/cpu\/x86\/gc\/z\/zGlobals_x86.hpp","additions":1,"deletions":5,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n-\/\/ Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n+\/\/ Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -35,0 +35,1 @@\n+#include \"gc\/z\/zBarrierSetAssembler.hpp\"\n@@ -36,11 +37,5 @@\n-static void z_load_barrier(MacroAssembler& _masm, const MachNode* node, Address ref_addr, Register ref, Register tmp, uint8_t barrier_data) {\n-  if (barrier_data == ZLoadBarrierElided) {\n-    return;\n-  }\n-  ZLoadBarrierStubC2* const stub = ZLoadBarrierStubC2::create(node, ref_addr, ref, tmp, barrier_data);\n-  {\n-    IntelJccErratumAlignment intel_alignment(_masm, 10 \/* jcc_size *\/);\n-    __ testptr(ref, Address(r15_thread, ZThreadLocalData::address_bad_mask_offset()));\n-    __ jcc(Assembler::notZero, *stub->entry());\n-  }\n-  __ bind(*stub->continuation());\n+static void z_color(MacroAssembler& _masm, const MachNode* node, Register ref) {\n+  __ relocate(barrier_Relocation::spec(), ZBarrierRelocationFormatLoadGoodBeforeShl);\n+  __ shlq(ref, barrier_Relocation::unpatched);\n+  __ orq_imm32(ref, barrier_Relocation::unpatched);\n+  __ relocate(barrier_Relocation::spec(), ZBarrierRelocationFormatStoreGoodAfterOr);\n@@ -49,11 +44,14 @@\n-static void z_load_barrier_cmpxchg(MacroAssembler& _masm, const MachNode* node, Address ref_addr, Register ref, Register tmp, Label& good) {\n-  ZLoadBarrierStubC2* const stub = ZLoadBarrierStubC2::create(node, ref_addr, ref, tmp, ZLoadBarrierStrong);\n-  {\n-    IntelJccErratumAlignment intel_alignment(_masm, 10 \/* jcc_size *\/);\n-    __ testptr(ref, Address(r15_thread, ZThreadLocalData::address_bad_mask_offset()));\n-    __ jcc(Assembler::zero, good);\n-  }\n-  {\n-    IntelJccErratumAlignment intel_alignment(_masm, 5 \/* jcc_size *\/);\n-    __ jmp(*stub->entry());\n-  }\n+static void z_uncolor(MacroAssembler& _masm, const MachNode* node, Register ref) {\n+  __ relocate(barrier_Relocation::spec(), ZBarrierRelocationFormatLoadGoodBeforeShl);\n+  __ shrq(ref, barrier_Relocation::unpatched);\n+}\n+\n+static void z_keep_alive_load_barrier(MacroAssembler& _masm, const MachNode* node, Address ref_addr, Register ref) {\n+  __ Assembler::testl(ref, barrier_Relocation::unpatched);\n+  __ relocate(barrier_Relocation::spec(), ZBarrierRelocationFormatMarkBadAfterTest);\n+\n+  ZLoadBarrierStubC2* const stub = ZLoadBarrierStubC2::create(node, ref_addr, ref);\n+  __ jcc(Assembler::notEqual, *stub->entry());\n+\n+  z_uncolor(_masm, node, ref);\n+\n@@ -63,3 +61,5 @@\n-static void z_cmpxchg_common(MacroAssembler& _masm, const MachNode* node, Register mem_reg, Register newval, Register tmp) {\n-  \/\/ Compare value (oldval) is in rax\n-   const Address mem = Address(mem_reg, 0);\n+static void z_load_barrier(MacroAssembler& _masm, const MachNode* node, Address ref_addr, Register ref) {\n+  Assembler::InlineSkippedInstructionsCounter skipped_counter(&_masm);\n+  const bool on_non_strong =\n+      ((node->barrier_data() & ZBarrierWeak) != 0) ||\n+      ((node->barrier_data() & ZBarrierPhantom) != 0);\n@@ -67,2 +67,3 @@\n-  if (node->barrier_data() != ZLoadBarrierElided) {\n-    __ movptr(tmp, rax);\n+  if (on_non_strong) {\n+    z_keep_alive_load_barrier(_masm, node, ref_addr, ref);\n+    return;\n@@ -71,2 +72,9 @@\n-  __ lock();\n-  __ cmpxchgptr(newval, mem);\n+  z_uncolor(_masm, node, ref);\n+  if (node->barrier_data() == ZBarrierElided) {\n+    return;\n+  }\n+  ZLoadBarrierStubC2* const stub = ZLoadBarrierStubC2::create(node, ref_addr, ref);\n+  IntelJccErratumAlignment(_masm, 6);\n+  __ jcc(Assembler::above, *stub->entry());\n+  __ bind(*stub->continuation());\n+}\n@@ -74,7 +82,13 @@\n-  if (node->barrier_data() != ZLoadBarrierElided) {\n-    Label good;\n-    z_load_barrier_cmpxchg(_masm, node, mem, rax, tmp, good);\n-    __ movptr(rax, tmp);\n-    __ lock();\n-    __ cmpxchgptr(newval, mem);\n-    __ bind(good);\n+static void z_store_barrier(MacroAssembler& _masm, const MachNode* node, Address ref_addr, Register rnew_zaddress, Register rnew_zpointer, bool is_atomic) {\n+  Assembler::InlineSkippedInstructionsCounter skipped_counter(&_masm);\n+  if (node->barrier_data() == ZBarrierElided) {\n+    if (rnew_zaddress != noreg) {\n+      \/\/ noreg means null; no need to color\n+      __ movptr(rnew_zpointer, rnew_zaddress);\n+      z_color(_masm, node, rnew_zpointer);\n+    }\n+  } else {\n+    bool is_native = (node->barrier_data() & ZBarrierNative) != 0;\n+    ZStoreBarrierStubC2* const stub = ZStoreBarrierStubC2::create(node, ref_addr, rnew_zaddress, rnew_zpointer, is_native, is_atomic);\n+    ZBarrierSetAssembler* bs_asm = ZBarrierSet::assembler();\n+    bs_asm->store_barrier_fast(&_masm, ref_addr, rnew_zaddress, rnew_zpointer, true \/* in_nmethod *\/, is_atomic, *stub->entry(), *stub->continuation());\n@@ -89,1 +103,1 @@\n-  predicate(UseZGC && n->as_Load()->barrier_data() != 0);\n+  predicate(UseZGC && ZGenerational && n->as_Load()->barrier_data() != 0);\n@@ -91,1 +105,1 @@\n-  effect(KILL cr, TEMP dst);\n+  effect(TEMP dst, KILL cr);\n@@ -99,1 +113,1 @@\n-    z_load_barrier(_masm, this, $mem$$Address, $dst$$Register, noreg \/* tmp *\/, barrier_data());\n+    z_load_barrier(_masm, this, $mem$$Address, $dst$$Register);\n@@ -105,1 +119,53 @@\n-instruct zCompareAndExchangeP(indirect mem, rax_RegP oldval, rRegP newval, rRegP tmp, rFlagsReg cr) %{\n+\/\/ Load Pointer and Null Check\n+instruct zLoadPNullCheck(rFlagsReg cr, memory op, immP0 zero)\n+%{\n+  predicate(UseZGC && ZGenerational && n->in(1)->as_Load()->barrier_data() != 0);\n+  match(Set cr (CmpP (LoadP op) zero));\n+\n+  ins_cost(500); \/\/ XXX\n+  format %{ \"testq   $op, 0xffffffffffff0000\\t# ptr\" %}\n+  ins_encode %{\n+    \/\/ A null pointer will have all address bits 0. This mask sign extends\n+    \/\/ all address bits, so we can test if the address is 0.\n+    __ testq($op$$Address, ZBarrierSetAssembler::_zpointer_address_mask);\n+  %}\n+  ins_pipe(ialu_cr_reg_imm);\n+%}\n+\n+\/\/ Store Pointer\n+instruct zStoreP(memory mem, any_RegP src, rRegP tmp, rFlagsReg cr)\n+%{\n+  predicate(UseZGC && ZGenerational && n->as_Store()->barrier_data() != 0);\n+  match(Set mem (StoreP mem src));\n+  effect(TEMP tmp, KILL cr);\n+\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"movq    $mem, $src\\t# ptr\" %}\n+  ins_encode %{\n+    z_store_barrier(_masm, this, $mem$$Address, $src$$Register, $tmp$$Register, false \/* is_atomic *\/);\n+    __ movq($mem$$Address, $tmp$$Register);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+\/\/ Store Null Pointer\n+instruct zStorePNull(memory mem, immP0 zero, rRegP tmp, rFlagsReg cr)\n+%{\n+  predicate(UseZGC && ZGenerational && n->as_Store()->barrier_data() != 0);\n+  match(Set mem (StoreP mem zero));\n+  effect(TEMP tmp, KILL cr);\n+\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"movq    $mem, 0\\t# ptr\" %}\n+  ins_encode %{\n+    z_store_barrier(_masm, this, $mem$$Address, noreg, $tmp$$Register, false \/* is_atomic *\/);\n+    \/\/ Store a colored null - barrier code above does not need to color\n+    __ movq($mem$$Address, barrier_Relocation::unpatched);\n+    \/\/ The relocation cant be fully after the mov, as that is the beginning of a random subsequent\n+    \/\/ instruction, which violates assumptions made by unrelated code. Hence the end() - 1\n+    __ code_section()->relocate(__ code_section()->end() - 1, barrier_Relocation::spec(), ZBarrierRelocationFormatStoreGoodAfterMov);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+instruct zCompareAndExchangeP(indirect mem, no_rax_RegP newval, rRegP tmp, rax_RegP oldval, rFlagsReg cr) %{\n@@ -107,2 +173,2 @@\n-  predicate(UseZGC && n->as_LoadStore()->barrier_data() == ZLoadBarrierStrong);\n-  effect(KILL cr, TEMP tmp);\n+  predicate(UseZGC && ZGenerational && n->as_LoadStore()->barrier_data() != 0);\n+  effect(TEMP tmp, KILL cr);\n@@ -114,2 +180,8 @@\n-    precond($oldval$$Register == rax);\n-    z_cmpxchg_common(_masm, this, $mem$$Register, $newval$$Register, $tmp$$Register);\n+    assert_different_registers($oldval$$Register, $mem$$Register);\n+    assert_different_registers($oldval$$Register, $newval$$Register);\n+    const Address mem_addr = Address($mem$$Register, 0);\n+    z_store_barrier(_masm, this, mem_addr, $newval$$Register, $tmp$$Register, true \/* is_atomic *\/);\n+    z_color(_masm, this, $oldval$$Register);\n+    __ lock();\n+    __ cmpxchgptr($tmp$$Register, mem_addr);\n+    z_uncolor(_masm, this, $oldval$$Register);\n@@ -121,1 +193,1 @@\n-instruct zCompareAndSwapP(rRegI res, indirect mem, rRegP newval, rRegP tmp, rFlagsReg cr, rax_RegP oldval) %{\n+instruct zCompareAndSwapP(rRegI res, indirect mem, rRegP newval, rRegP tmp, rax_RegP oldval, rFlagsReg cr) %{\n@@ -124,2 +196,2 @@\n-  predicate(UseZGC && n->as_LoadStore()->barrier_data() == ZLoadBarrierStrong);\n-  effect(KILL cr, KILL oldval, TEMP tmp);\n+  predicate(UseZGC && ZGenerational && n->as_LoadStore()->barrier_data() != 0);\n+  effect(TEMP tmp, KILL oldval, KILL cr);\n@@ -133,5 +205,6 @@\n-    precond($oldval$$Register == rax);\n-    z_cmpxchg_common(_masm, this, $mem$$Register, $newval$$Register, $tmp$$Register);\n-    if (barrier_data() != ZLoadBarrierElided) {\n-      __ cmpptr($tmp$$Register, rax);\n-    }\n+    assert_different_registers($oldval$$Register, $mem$$Register);\n+    const Address mem_addr = Address($mem$$Register, 0);\n+    z_store_barrier(_masm, this, mem_addr, $newval$$Register, $tmp$$Register, true \/* is_atomic *\/);\n+    z_color(_masm, this, $oldval$$Register);\n+    __ lock();\n+    __ cmpxchgptr($tmp$$Register, mem_addr);\n@@ -145,1 +218,1 @@\n-instruct zXChgP(indirect mem, rRegP newval, rFlagsReg cr) %{\n+instruct zXChgP(indirect mem, rRegP newval, rRegP tmp, rFlagsReg cr) %{\n@@ -147,2 +220,2 @@\n-  predicate(UseZGC && n->as_LoadStore()->barrier_data() != 0);\n-  effect(KILL cr);\n+  predicate(UseZGC && ZGenerational && n->as_LoadStore()->barrier_data() != 0);\n+  effect(TEMP tmp, KILL cr);\n@@ -153,2 +226,6 @@\n-    __ xchgptr($newval$$Register, Address($mem$$Register, 0));\n-    z_load_barrier(_masm, this, Address(noreg, 0), $newval$$Register, noreg \/* tmp *\/, barrier_data());\n+    assert_different_registers($mem$$Register, $newval$$Register);\n+    const Address mem_addr = Address($mem$$Register, 0);\n+    z_store_barrier(_masm, this, mem_addr, $newval$$Register, $tmp$$Register, true \/* is_atomic *\/);\n+    __ movptr($newval$$Register, $tmp$$Register);\n+    __ xchgptr($newval$$Register, mem_addr);\n+    z_uncolor(_masm, this, $newval$$Register);\n","filename":"src\/hotspot\/cpu\/x86\/gc\/z\/z_x86_64.ad","additions":135,"deletions":58,"binary":false,"changes":193,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -38,2 +38,2 @@\n-    \/\/ vs Assembler::narrow_oop_operand.\n-    format_width       =  2\n+    \/\/ vs Assembler::narrow_oop_operand and ZGC barrier encodings.\n+    format_width       =  3\n","filename":"src\/hotspot\/cpu\/x86\/relocInfo_x86.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -47,3 +47,0 @@\n-#if INCLUDE_ZGC\n-#include \"gc\/z\/zThreadLocalData.hpp\"\n-#endif\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.cpp","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -41,1 +41,1 @@\n-  _final_stubs_code_size        = 10000 LP64_ONLY(+20000) WINDOWS_ONLY(+2000)\n+  _final_stubs_code_size        = 10000 LP64_ONLY(+20000) WINDOWS_ONLY(+2000) ZGC_ONLY(+20000)\n","filename":"src\/hotspot\/cpu\/x86\/stubRoutines_x86.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"gc\/shared\/gc_globals.hpp\"\n","filename":"src\/hotspot\/cpu\/x86\/templateTable_x86.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -0,0 +1,34 @@\n+\/*\n+ * Copyright (c) 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/x\/xLargePages.hpp\"\n+#include \"runtime\/globals.hpp\"\n+\n+void XLargePages::pd_initialize() {\n+  if (UseLargePages) {\n+    _state = Explicit;\n+  } else {\n+    _state = Disabled;\n+  }\n+}\n","filename":"src\/hotspot\/os\/bsd\/gc\/x\/xLargePages_bsd.cpp","additions":34,"deletions":0,"binary":false,"changes":34,"status":"added"},{"patch":"@@ -0,0 +1,43 @@\n+\/*\n+ * Copyright (c) 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/x\/xNUMA.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+void XNUMA::pd_initialize() {\n+  _enabled = false;\n+}\n+\n+uint32_t XNUMA::count() {\n+  return 1;\n+}\n+\n+uint32_t XNUMA::id() {\n+  return 0;\n+}\n+\n+uint32_t XNUMA::memory_id(uintptr_t addr) {\n+  \/\/ NUMA support not enabled, assume everything belongs to node zero\n+  return 0;\n+}\n","filename":"src\/hotspot\/os\/bsd\/gc\/x\/xNUMA_bsd.cpp","additions":43,"deletions":0,"binary":false,"changes":43,"status":"added"},{"patch":"@@ -0,0 +1,181 @@\n+\/*\n+ * Copyright (c) 2019, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/shared\/gcLogPrecious.hpp\"\n+#include \"gc\/x\/xErrno.hpp\"\n+#include \"gc\/x\/xGlobals.hpp\"\n+#include \"gc\/x\/xLargePages.inline.hpp\"\n+#include \"gc\/x\/xPhysicalMemory.inline.hpp\"\n+#include \"gc\/x\/xPhysicalMemoryBacking_bsd.hpp\"\n+#include \"logging\/log.hpp\"\n+#include \"runtime\/globals.hpp\"\n+#include \"runtime\/os.hpp\"\n+#include \"utilities\/align.hpp\"\n+#include \"utilities\/debug.hpp\"\n+\n+#include <mach\/mach.h>\n+#include <mach\/mach_vm.h>\n+#include <sys\/mman.h>\n+#include <sys\/types.h>\n+\n+\/\/ The backing is represented by a reserved virtual address space, in which\n+\/\/ we commit and uncommit physical memory. Multi-mapping the different heap\n+\/\/ views is done by simply remapping the backing memory using mach_vm_remap().\n+\n+static int vm_flags_superpage() {\n+  if (!XLargePages::is_explicit()) {\n+    return 0;\n+  }\n+\n+  const int page_size_in_megabytes = XGranuleSize >> 20;\n+  return page_size_in_megabytes << VM_FLAGS_SUPERPAGE_SHIFT;\n+}\n+\n+static XErrno mremap(uintptr_t from_addr, uintptr_t to_addr, size_t size) {\n+  mach_vm_address_t remap_addr = to_addr;\n+  vm_prot_t remap_cur_prot;\n+  vm_prot_t remap_max_prot;\n+\n+  \/\/ Remap memory to an additional location\n+  const kern_return_t res = mach_vm_remap(mach_task_self(),\n+                                          &remap_addr,\n+                                          size,\n+                                          0 \/* mask *\/,\n+                                          VM_FLAGS_FIXED | VM_FLAGS_OVERWRITE | vm_flags_superpage(),\n+                                          mach_task_self(),\n+                                          from_addr,\n+                                          FALSE \/* copy *\/,\n+                                          &remap_cur_prot,\n+                                          &remap_max_prot,\n+                                          VM_INHERIT_COPY);\n+\n+  return (res == KERN_SUCCESS) ? XErrno(0) : XErrno(EINVAL);\n+}\n+\n+XPhysicalMemoryBacking::XPhysicalMemoryBacking(size_t max_capacity) :\n+    _base(0),\n+    _initialized(false) {\n+\n+  \/\/ Reserve address space for backing memory\n+  _base = (uintptr_t)os::reserve_memory(max_capacity);\n+  if (_base == 0) {\n+    \/\/ Failed\n+    log_error_pd(gc)(\"Failed to reserve address space for backing memory\");\n+    return;\n+  }\n+\n+  \/\/ Successfully initialized\n+  _initialized = true;\n+}\n+\n+bool XPhysicalMemoryBacking::is_initialized() const {\n+  return _initialized;\n+}\n+\n+void XPhysicalMemoryBacking::warn_commit_limits(size_t max_capacity) const {\n+  \/\/ Does nothing\n+}\n+\n+bool XPhysicalMemoryBacking::commit_inner(size_t offset, size_t length) const {\n+  assert(is_aligned(offset, os::vm_page_size()), \"Invalid offset\");\n+  assert(is_aligned(length, os::vm_page_size()), \"Invalid length\");\n+\n+  log_trace(gc, heap)(\"Committing memory: \" SIZE_FORMAT \"M-\" SIZE_FORMAT \"M (\" SIZE_FORMAT \"M)\",\n+                      offset \/ M, (offset + length) \/ M, length \/ M);\n+\n+  const uintptr_t addr = _base + offset;\n+  const void* const res = mmap((void*)addr, length, PROT_READ | PROT_WRITE, MAP_FIXED | MAP_ANONYMOUS | MAP_PRIVATE, -1, 0);\n+  if (res == MAP_FAILED) {\n+    XErrno err;\n+    log_error(gc)(\"Failed to commit memory (%s)\", err.to_string());\n+    return false;\n+  }\n+\n+  \/\/ Success\n+  return true;\n+}\n+\n+size_t XPhysicalMemoryBacking::commit(size_t offset, size_t length) const {\n+  \/\/ Try to commit the whole region\n+  if (commit_inner(offset, length)) {\n+    \/\/ Success\n+    return length;\n+  }\n+\n+  \/\/ Failed, try to commit as much as possible\n+  size_t start = offset;\n+  size_t end = offset + length;\n+\n+  for (;;) {\n+    length = align_down((end - start) \/ 2, XGranuleSize);\n+    if (length == 0) {\n+      \/\/ Done, don't commit more\n+      return start - offset;\n+    }\n+\n+    if (commit_inner(start, length)) {\n+      \/\/ Success, try commit more\n+      start += length;\n+    } else {\n+      \/\/ Failed, try commit less\n+      end -= length;\n+    }\n+  }\n+}\n+\n+size_t XPhysicalMemoryBacking::uncommit(size_t offset, size_t length) const {\n+  assert(is_aligned(offset, os::vm_page_size()), \"Invalid offset\");\n+  assert(is_aligned(length, os::vm_page_size()), \"Invalid length\");\n+\n+  log_trace(gc, heap)(\"Uncommitting memory: \" SIZE_FORMAT \"M-\" SIZE_FORMAT \"M (\" SIZE_FORMAT \"M)\",\n+                      offset \/ M, (offset + length) \/ M, length \/ M);\n+\n+  const uintptr_t start = _base + offset;\n+  const void* const res = mmap((void*)start, length, PROT_NONE, MAP_FIXED | MAP_ANONYMOUS | MAP_PRIVATE | MAP_NORESERVE, -1, 0);\n+  if (res == MAP_FAILED) {\n+    XErrno err;\n+    log_error(gc)(\"Failed to uncommit memory (%s)\", err.to_string());\n+    return 0;\n+  }\n+\n+  return length;\n+}\n+\n+void XPhysicalMemoryBacking::map(uintptr_t addr, size_t size, uintptr_t offset) const {\n+  const XErrno err = mremap(_base + offset, addr, size);\n+  if (err) {\n+    fatal(\"Failed to remap memory (%s)\", err.to_string());\n+  }\n+}\n+\n+void XPhysicalMemoryBacking::unmap(uintptr_t addr, size_t size) const {\n+  \/\/ Note that we must keep the address space reservation intact and just detach\n+  \/\/ the backing memory. For this reason we map a new anonymous, non-accessible\n+  \/\/ and non-reserved page over the mapping instead of actually unmapping.\n+  const void* const res = mmap((void*)addr, size, PROT_NONE, MAP_FIXED | MAP_ANONYMOUS | MAP_PRIVATE | MAP_NORESERVE, -1, 0);\n+  if (res == MAP_FAILED) {\n+    XErrno err;\n+    fatal(\"Failed to map memory (%s)\", err.to_string());\n+  }\n+}\n","filename":"src\/hotspot\/os\/bsd\/gc\/x\/xPhysicalMemoryBacking_bsd.cpp","additions":181,"deletions":0,"binary":false,"changes":181,"status":"added"},{"patch":"@@ -0,0 +1,48 @@\n+\/*\n+ * Copyright (c) 2019, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef OS_BSD_GC_X_XPHYSICALMEMORYBACKING_BSD_HPP\n+#define OS_BSD_GC_X_XPHYSICALMEMORYBACKING_BSD_HPP\n+\n+class XPhysicalMemoryBacking {\n+private:\n+  uintptr_t _base;\n+  bool      _initialized;\n+\n+  bool commit_inner(size_t offset, size_t length) const;\n+\n+public:\n+  XPhysicalMemoryBacking(size_t max_capacity);\n+\n+  bool is_initialized() const;\n+\n+  void warn_commit_limits(size_t max_capacity) const;\n+\n+  size_t commit(size_t offset, size_t length) const;\n+  size_t uncommit(size_t offset, size_t length) const;\n+\n+  void map(uintptr_t addr, size_t size, uintptr_t offset) const;\n+  void unmap(uintptr_t addr, size_t size) const;\n+};\n+\n+#endif \/\/ OS_BSD_GC_X_XPHYSICALMEMORYBACKING_BSD_HPP\n","filename":"src\/hotspot\/os\/bsd\/gc\/x\/xPhysicalMemoryBacking_bsd.hpp","additions":48,"deletions":0,"binary":false,"changes":48,"status":"added"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2019, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2019, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -26,0 +26,1 @@\n+#include \"gc\/z\/zAddress.inline.hpp\"\n@@ -100,2 +101,2 @@\n-bool ZPhysicalMemoryBacking::commit_inner(size_t offset, size_t length) const {\n-  assert(is_aligned(offset, os::vm_page_size()), \"Invalid offset\");\n+bool ZPhysicalMemoryBacking::commit_inner(zoffset offset, size_t length) const {\n+  assert(is_aligned(untype(offset), os::vm_page_size()), \"Invalid offset\");\n@@ -105,1 +106,1 @@\n-                      offset \/ M, (offset + length) \/ M, length \/ M);\n+                      untype(offset) \/ M, untype(offset) + length \/ M, length \/ M);\n@@ -107,1 +108,1 @@\n-  const uintptr_t addr = _base + offset;\n+  const uintptr_t addr = _base + untype(offset);\n@@ -119,1 +120,1 @@\n-size_t ZPhysicalMemoryBacking::commit(size_t offset, size_t length) const {\n+size_t ZPhysicalMemoryBacking::commit(zoffset offset, size_t length) const {\n@@ -127,2 +128,2 @@\n-  size_t start = offset;\n-  size_t end = offset + length;\n+  zoffset start = offset;\n+  zoffset end = offset + length;\n@@ -147,2 +148,2 @@\n-size_t ZPhysicalMemoryBacking::uncommit(size_t offset, size_t length) const {\n-  assert(is_aligned(offset, os::vm_page_size()), \"Invalid offset\");\n+size_t ZPhysicalMemoryBacking::uncommit(zoffset offset, size_t length) const {\n+  assert(is_aligned(untype(offset), os::vm_page_size()), \"Invalid offset\");\n@@ -152,1 +153,1 @@\n-                      offset \/ M, (offset + length) \/ M, length \/ M);\n+                      untype(offset) \/ M, untype(offset) + length \/ M, length \/ M);\n@@ -154,1 +155,1 @@\n-  const uintptr_t start = _base + offset;\n+  const uintptr_t start = _base + untype(offset);\n@@ -165,2 +166,2 @@\n-void ZPhysicalMemoryBacking::map(uintptr_t addr, size_t size, uintptr_t offset) const {\n-  const ZErrno err = mremap(_base + offset, addr, size);\n+void ZPhysicalMemoryBacking::map(zaddress_unsafe addr, size_t size, zoffset offset) const {\n+  const ZErrno err = mremap(_base + untype(offset), untype(addr), size);\n@@ -172,1 +173,1 @@\n-void ZPhysicalMemoryBacking::unmap(uintptr_t addr, size_t size) const {\n+void ZPhysicalMemoryBacking::unmap(zaddress_unsafe addr, size_t size) const {\n@@ -176,1 +177,1 @@\n-  const void* const res = mmap((void*)addr, size, PROT_NONE, MAP_FIXED | MAP_ANONYMOUS | MAP_PRIVATE | MAP_NORESERVE, -1, 0);\n+  const void* const res = mmap((void*)untype(addr), size, PROT_NONE, MAP_FIXED | MAP_ANONYMOUS | MAP_PRIVATE | MAP_NORESERVE, -1, 0);\n","filename":"src\/hotspot\/os\/bsd\/gc\/z\/zPhysicalMemoryBacking_bsd.cpp","additions":17,"deletions":16,"binary":false,"changes":33,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2019, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2019, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,0 +27,2 @@\n+#include \"gc\/z\/zAddress.hpp\"\n+\n@@ -32,1 +34,1 @@\n-  bool commit_inner(size_t offset, size_t length) const;\n+  bool commit_inner(zoffset offset, size_t length) const;\n@@ -41,2 +43,2 @@\n-  size_t commit(size_t offset, size_t length) const;\n-  size_t uncommit(size_t offset, size_t length) const;\n+  size_t commit(zoffset offset, size_t length) const;\n+  size_t uncommit(zoffset offset, size_t length) const;\n@@ -44,2 +46,2 @@\n-  void map(uintptr_t addr, size_t size, uintptr_t offset) const;\n-  void unmap(uintptr_t addr, size_t size) const;\n+  void map(zaddress_unsafe addr, size_t size, zoffset offset) const;\n+  void unmap(zaddress_unsafe addr, size_t size) const;\n","filename":"src\/hotspot\/os\/bsd\/gc\/z\/zPhysicalMemoryBacking_bsd.hpp","additions":8,"deletions":6,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -0,0 +1,38 @@\n+\/*\n+ * Copyright (c) 2017, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/x\/xLargePages.hpp\"\n+#include \"runtime\/globals.hpp\"\n+\n+void XLargePages::pd_initialize() {\n+  if (UseLargePages) {\n+    if (UseTransparentHugePages) {\n+      _state = Transparent;\n+    } else {\n+      _state = Explicit;\n+    }\n+  } else {\n+    _state = Disabled;\n+  }\n+}\n","filename":"src\/hotspot\/os\/linux\/gc\/x\/xLargePages_linux.cpp","additions":38,"deletions":0,"binary":false,"changes":38,"status":"added"},{"patch":"@@ -0,0 +1,154 @@\n+\/*\n+ * Copyright (c) 2016, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/shared\/gcLogPrecious.hpp\"\n+#include \"gc\/x\/xArray.inline.hpp\"\n+#include \"gc\/x\/xErrno.hpp\"\n+#include \"gc\/x\/xMountPoint_linux.hpp\"\n+#include \"runtime\/globals.hpp\"\n+#include \"runtime\/os.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+#include <stdio.h>\n+#include <unistd.h>\n+\n+\/\/ Mount information, see proc(5) for more details.\n+#define PROC_SELF_MOUNTINFO        \"\/proc\/self\/mountinfo\"\n+\n+XMountPoint::XMountPoint(const char* filesystem, const char** preferred_mountpoints) {\n+  if (AllocateHeapAt != nullptr) {\n+    \/\/ Use specified path\n+    _path = os::strdup(AllocateHeapAt, mtGC);\n+  } else {\n+    \/\/ Find suitable path\n+    _path = find_mountpoint(filesystem, preferred_mountpoints);\n+  }\n+}\n+\n+XMountPoint::~XMountPoint() {\n+  os::free(_path);\n+  _path = nullptr;\n+}\n+\n+char* XMountPoint::get_mountpoint(const char* line, const char* filesystem) const {\n+  char* line_mountpoint = nullptr;\n+  char* line_filesystem = nullptr;\n+\n+  \/\/ Parse line and return a newly allocated string containing the mount point if\n+  \/\/ the line contains a matching filesystem and the mount point is accessible by\n+  \/\/ the current user.\n+  \/\/ sscanf, using %m, will return malloced memory. Need raw ::free, not os::free.\n+  if (sscanf(line, \"%*u %*u %*u:%*u %*s %ms %*[^-]- %ms\", &line_mountpoint, &line_filesystem) != 2 ||\n+      strcmp(line_filesystem, filesystem) != 0 ||\n+      access(line_mountpoint, R_OK|W_OK|X_OK) != 0) {\n+    \/\/ Not a matching or accessible filesystem\n+    ALLOW_C_FUNCTION(::free, ::free(line_mountpoint);)\n+    line_mountpoint = nullptr;\n+  }\n+\n+  ALLOW_C_FUNCTION(::free, ::free(line_filesystem);)\n+\n+  return line_mountpoint;\n+}\n+\n+void XMountPoint::get_mountpoints(const char* filesystem, XArray<char*>* mountpoints) const {\n+  FILE* fd = os::fopen(PROC_SELF_MOUNTINFO, \"r\");\n+  if (fd == nullptr) {\n+    XErrno err;\n+    log_error_p(gc)(\"Failed to open %s: %s\", PROC_SELF_MOUNTINFO, err.to_string());\n+    return;\n+  }\n+\n+  char* line = nullptr;\n+  size_t length = 0;\n+\n+  while (getline(&line, &length, fd) != -1) {\n+    char* const mountpoint = get_mountpoint(line, filesystem);\n+    if (mountpoint != nullptr) {\n+      mountpoints->append(mountpoint);\n+    }\n+  }\n+\n+  \/\/ readline will return malloced memory. Need raw ::free, not os::free.\n+  ALLOW_C_FUNCTION(::free, ::free(line);)\n+  fclose(fd);\n+}\n+\n+void XMountPoint::free_mountpoints(XArray<char*>* mountpoints) const {\n+  XArrayIterator<char*> iter(mountpoints);\n+  for (char* mountpoint; iter.next(&mountpoint);) {\n+    ALLOW_C_FUNCTION(::free, ::free(mountpoint);) \/\/ *not* os::free\n+  }\n+  mountpoints->clear();\n+}\n+\n+char* XMountPoint::find_preferred_mountpoint(const char* filesystem,\n+                                              XArray<char*>* mountpoints,\n+                                              const char** preferred_mountpoints) const {\n+  \/\/ Find preferred mount point\n+  XArrayIterator<char*> iter1(mountpoints);\n+  for (char* mountpoint; iter1.next(&mountpoint);) {\n+    for (const char** preferred = preferred_mountpoints; *preferred != nullptr; preferred++) {\n+      if (!strcmp(mountpoint, *preferred)) {\n+        \/\/ Preferred mount point found\n+        return os::strdup(mountpoint, mtGC);\n+      }\n+    }\n+  }\n+\n+  \/\/ Preferred mount point not found\n+  log_error_p(gc)(\"More than one %s filesystem found:\", filesystem);\n+  XArrayIterator<char*> iter2(mountpoints);\n+  for (char* mountpoint; iter2.next(&mountpoint);) {\n+    log_error_p(gc)(\"  %s\", mountpoint);\n+  }\n+\n+  return nullptr;\n+}\n+\n+char* XMountPoint::find_mountpoint(const char* filesystem, const char** preferred_mountpoints) const {\n+  char* path = nullptr;\n+  XArray<char*> mountpoints;\n+\n+  get_mountpoints(filesystem, &mountpoints);\n+\n+  if (mountpoints.length() == 0) {\n+    \/\/ No mount point found\n+    log_error_p(gc)(\"Failed to find an accessible %s filesystem\", filesystem);\n+  } else if (mountpoints.length() == 1) {\n+    \/\/ One mount point found\n+    path = os::strdup(mountpoints.at(0), mtGC);\n+  } else {\n+    \/\/ More than one mount point found\n+    path = find_preferred_mountpoint(filesystem, &mountpoints, preferred_mountpoints);\n+  }\n+\n+  free_mountpoints(&mountpoints);\n+\n+  return path;\n+}\n+\n+const char* XMountPoint::get() const {\n+  return _path;\n+}\n","filename":"src\/hotspot\/os\/linux\/gc\/x\/xMountPoint_linux.cpp","additions":154,"deletions":0,"binary":false,"changes":154,"status":"added"},{"patch":"@@ -0,0 +1,52 @@\n+\/*\n+ * Copyright (c) 2016, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef OS_LINUX_GC_X_XMOUNTPOINT_LINUX_HPP\n+#define OS_LINUX_GC_X_XMOUNTPOINT_LINUX_HPP\n+\n+#include \"gc\/x\/xArray.hpp\"\n+#include \"memory\/allocation.hpp\"\n+\n+class XMountPoint : public StackObj {\n+private:\n+  char* _path;\n+\n+  char* get_mountpoint(const char* line,\n+                       const char* filesystem) const;\n+  void get_mountpoints(const char* filesystem,\n+                       XArray<char*>* mountpoints) const;\n+  void free_mountpoints(XArray<char*>* mountpoints) const;\n+  char* find_preferred_mountpoint(const char* filesystem,\n+                                  XArray<char*>* mountpoints,\n+                                  const char** preferred_mountpoints) const;\n+  char* find_mountpoint(const char* filesystem,\n+                        const char** preferred_mountpoints) const;\n+\n+public:\n+  XMountPoint(const char* filesystem, const char** preferred_mountpoints);\n+  ~XMountPoint();\n+\n+  const char* get() const;\n+};\n+\n+#endif \/\/ OS_LINUX_GC_X_XMOUNTPOINT_LINUX_HPP\n","filename":"src\/hotspot\/os\/linux\/gc\/x\/xMountPoint_linux.hpp","additions":52,"deletions":0,"binary":false,"changes":52,"status":"added"},{"patch":"@@ -0,0 +1,71 @@\n+\/*\n+ * Copyright (c) 2016, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"gc\/x\/xCPU.inline.hpp\"\n+#include \"gc\/x\/xErrno.hpp\"\n+#include \"gc\/x\/xNUMA.hpp\"\n+#include \"gc\/x\/xSyscall_linux.hpp\"\n+#include \"os_linux.hpp\"\n+#include \"runtime\/globals.hpp\"\n+#include \"runtime\/os.hpp\"\n+#include \"utilities\/debug.hpp\"\n+\n+void XNUMA::pd_initialize() {\n+  _enabled = UseNUMA;\n+}\n+\n+uint32_t XNUMA::count() {\n+  if (!_enabled) {\n+    \/\/ NUMA support not enabled\n+    return 1;\n+  }\n+\n+  return os::Linux::numa_max_node() + 1;\n+}\n+\n+uint32_t XNUMA::id() {\n+  if (!_enabled) {\n+    \/\/ NUMA support not enabled\n+    return 0;\n+  }\n+\n+  return os::Linux::get_node_by_cpu(XCPU::id());\n+}\n+\n+uint32_t XNUMA::memory_id(uintptr_t addr) {\n+  if (!_enabled) {\n+    \/\/ NUMA support not enabled, assume everything belongs to node zero\n+    return 0;\n+  }\n+\n+  uint32_t id = (uint32_t)-1;\n+\n+  if (XSyscall::get_mempolicy((int*)&id, nullptr, 0, (void*)addr, MPOL_F_NODE | MPOL_F_ADDR) == -1) {\n+    XErrno err;\n+    fatal(\"Failed to get NUMA id for memory at \" PTR_FORMAT \" (%s)\", addr, err.to_string());\n+  }\n+\n+  assert(id < count(), \"Invalid NUMA id\");\n+\n+  return id;\n+}\n","filename":"src\/hotspot\/os\/linux\/gc\/x\/xNUMA_linux.cpp","additions":71,"deletions":0,"binary":false,"changes":71,"status":"added"},{"patch":"@@ -0,0 +1,724 @@\n+\/*\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/shared\/gcLogPrecious.hpp\"\n+#include \"gc\/x\/xArray.inline.hpp\"\n+#include \"gc\/x\/xErrno.hpp\"\n+#include \"gc\/x\/xGlobals.hpp\"\n+#include \"gc\/x\/xLargePages.inline.hpp\"\n+#include \"gc\/x\/xMountPoint_linux.hpp\"\n+#include \"gc\/x\/xNUMA.inline.hpp\"\n+#include \"gc\/x\/xPhysicalMemoryBacking_linux.hpp\"\n+#include \"gc\/x\/xSyscall_linux.hpp\"\n+#include \"logging\/log.hpp\"\n+#include \"os_linux.hpp\"\n+#include \"runtime\/init.hpp\"\n+#include \"runtime\/os.hpp\"\n+#include \"runtime\/safefetch.hpp\"\n+#include \"utilities\/align.hpp\"\n+#include \"utilities\/debug.hpp\"\n+#include \"utilities\/growableArray.hpp\"\n+\n+#include <fcntl.h>\n+#include <stdio.h>\n+#include <sys\/mman.h>\n+#include <sys\/stat.h>\n+#include <sys\/statfs.h>\n+#include <sys\/types.h>\n+#include <unistd.h>\n+\n+\/\/\n+\/\/ Support for building on older Linux systems\n+\/\/\n+\n+\/\/ memfd_create(2) flags\n+#ifndef MFD_CLOEXEC\n+#define MFD_CLOEXEC                      0x0001U\n+#endif\n+#ifndef MFD_HUGETLB\n+#define MFD_HUGETLB                      0x0004U\n+#endif\n+#ifndef MFD_HUGE_2MB\n+#define MFD_HUGE_2MB                     0x54000000U\n+#endif\n+\n+\/\/ open(2) flags\n+#ifndef O_CLOEXEC\n+#define O_CLOEXEC                        02000000\n+#endif\n+#ifndef O_TMPFILE\n+#define O_TMPFILE                        (020000000 | O_DIRECTORY)\n+#endif\n+\n+\/\/ fallocate(2) flags\n+#ifndef FALLOC_FL_KEEP_SIZE\n+#define FALLOC_FL_KEEP_SIZE              0x01\n+#endif\n+#ifndef FALLOC_FL_PUNCH_HOLE\n+#define FALLOC_FL_PUNCH_HOLE             0x02\n+#endif\n+\n+\/\/ Filesystem types, see statfs(2)\n+#ifndef TMPFS_MAGIC\n+#define TMPFS_MAGIC                      0x01021994\n+#endif\n+#ifndef HUGETLBFS_MAGIC\n+#define HUGETLBFS_MAGIC                  0x958458f6\n+#endif\n+\n+\/\/ Filesystem names\n+#define XFILESYSTEM_TMPFS                \"tmpfs\"\n+#define XFILESYSTEM_HUGETLBFS            \"hugetlbfs\"\n+\n+\/\/ Proc file entry for max map mount\n+#define XFILENAME_PROC_MAX_MAP_COUNT     \"\/proc\/sys\/vm\/max_map_count\"\n+\n+\/\/ Sysfs file for transparent huge page on tmpfs\n+#define XFILENAME_SHMEM_ENABLED          \"\/sys\/kernel\/mm\/transparent_hugepage\/shmem_enabled\"\n+\n+\/\/ Java heap filename\n+#define XFILENAME_HEAP                   \"java_heap\"\n+\n+\/\/ Preferred tmpfs mount points, ordered by priority\n+static const char* z_preferred_tmpfs_mountpoints[] = {\n+  \"\/dev\/shm\",\n+  \"\/run\/shm\",\n+  nullptr\n+};\n+\n+\/\/ Preferred hugetlbfs mount points, ordered by priority\n+static const char* z_preferred_hugetlbfs_mountpoints[] = {\n+  \"\/dev\/hugepages\",\n+  \"\/hugepages\",\n+  nullptr\n+};\n+\n+static int z_fallocate_hugetlbfs_attempts = 3;\n+static bool z_fallocate_supported = true;\n+\n+XPhysicalMemoryBacking::XPhysicalMemoryBacking(size_t max_capacity) :\n+    _fd(-1),\n+    _filesystem(0),\n+    _block_size(0),\n+    _available(0),\n+    _initialized(false) {\n+\n+  \/\/ Create backing file\n+  _fd = create_fd(XFILENAME_HEAP);\n+  if (_fd == -1) {\n+    return;\n+  }\n+\n+  \/\/ Truncate backing file\n+  while (ftruncate(_fd, max_capacity) == -1) {\n+    if (errno != EINTR) {\n+      XErrno err;\n+      log_error_p(gc)(\"Failed to truncate backing file (%s)\", err.to_string());\n+      return;\n+    }\n+  }\n+\n+  \/\/ Get filesystem statistics\n+  struct statfs buf;\n+  if (fstatfs(_fd, &buf) == -1) {\n+    XErrno err;\n+    log_error_p(gc)(\"Failed to determine filesystem type for backing file (%s)\", err.to_string());\n+    return;\n+  }\n+\n+  _filesystem = buf.f_type;\n+  _block_size = buf.f_bsize;\n+  _available = buf.f_bavail * _block_size;\n+\n+  log_info_p(gc, init)(\"Heap Backing Filesystem: %s (\" UINT64_FORMAT_X \")\",\n+                       is_tmpfs() ? XFILESYSTEM_TMPFS : is_hugetlbfs() ? XFILESYSTEM_HUGETLBFS : \"other\", _filesystem);\n+\n+  \/\/ Make sure the filesystem type matches requested large page type\n+  if (XLargePages::is_transparent() && !is_tmpfs()) {\n+    log_error_p(gc)(\"-XX:+UseTransparentHugePages can only be enabled when using a %s filesystem\",\n+                    XFILESYSTEM_TMPFS);\n+    return;\n+  }\n+\n+  if (XLargePages::is_transparent() && !tmpfs_supports_transparent_huge_pages()) {\n+    log_error_p(gc)(\"-XX:+UseTransparentHugePages on a %s filesystem not supported by kernel\",\n+                    XFILESYSTEM_TMPFS);\n+    return;\n+  }\n+\n+  if (XLargePages::is_explicit() && !is_hugetlbfs()) {\n+    log_error_p(gc)(\"-XX:+UseLargePages (without -XX:+UseTransparentHugePages) can only be enabled \"\n+                    \"when using a %s filesystem\", XFILESYSTEM_HUGETLBFS);\n+    return;\n+  }\n+\n+  if (!XLargePages::is_explicit() && is_hugetlbfs()) {\n+    log_error_p(gc)(\"-XX:+UseLargePages must be enabled when using a %s filesystem\",\n+                    XFILESYSTEM_HUGETLBFS);\n+    return;\n+  }\n+\n+  \/\/ Make sure the filesystem block size is compatible\n+  if (XGranuleSize % _block_size != 0) {\n+    log_error_p(gc)(\"Filesystem backing the heap has incompatible block size (\" SIZE_FORMAT \")\",\n+                    _block_size);\n+    return;\n+  }\n+\n+  if (is_hugetlbfs() && _block_size != XGranuleSize) {\n+    log_error_p(gc)(\"%s filesystem has unexpected block size \" SIZE_FORMAT \" (expected \" SIZE_FORMAT \")\",\n+                    XFILESYSTEM_HUGETLBFS, _block_size, XGranuleSize);\n+    return;\n+  }\n+\n+  \/\/ Successfully initialized\n+  _initialized = true;\n+}\n+\n+int XPhysicalMemoryBacking::create_mem_fd(const char* name) const {\n+  assert(XGranuleSize == 2 * M, \"Granule size must match MFD_HUGE_2MB\");\n+\n+  \/\/ Create file name\n+  char filename[PATH_MAX];\n+  snprintf(filename, sizeof(filename), \"%s%s\", name, XLargePages::is_explicit() ? \".hugetlb\" : \"\");\n+\n+  \/\/ Create file\n+  const int extra_flags = XLargePages::is_explicit() ? (MFD_HUGETLB | MFD_HUGE_2MB) : 0;\n+  const int fd = XSyscall::memfd_create(filename, MFD_CLOEXEC | extra_flags);\n+  if (fd == -1) {\n+    XErrno err;\n+    log_debug_p(gc, init)(\"Failed to create memfd file (%s)\",\n+                          (XLargePages::is_explicit() && (err == EINVAL || err == ENODEV)) ?\n+                          \"Hugepages (2M) not available\" : err.to_string());\n+    return -1;\n+  }\n+\n+  log_info_p(gc, init)(\"Heap Backing File: \/memfd:%s\", filename);\n+\n+  return fd;\n+}\n+\n+int XPhysicalMemoryBacking::create_file_fd(const char* name) const {\n+  const char* const filesystem = XLargePages::is_explicit()\n+                                 ? XFILESYSTEM_HUGETLBFS\n+                                 : XFILESYSTEM_TMPFS;\n+  const char** const preferred_mountpoints = XLargePages::is_explicit()\n+                                             ? z_preferred_hugetlbfs_mountpoints\n+                                             : z_preferred_tmpfs_mountpoints;\n+\n+  \/\/ Find mountpoint\n+  XMountPoint mountpoint(filesystem, preferred_mountpoints);\n+  if (mountpoint.get() == nullptr) {\n+    log_error_p(gc)(\"Use -XX:AllocateHeapAt to specify the path to a %s filesystem\", filesystem);\n+    return -1;\n+  }\n+\n+  \/\/ Try to create an anonymous file using the O_TMPFILE flag. Note that this\n+  \/\/ flag requires kernel >= 3.11. If this fails we fall back to open\/unlink.\n+  const int fd_anon = os::open(mountpoint.get(), O_TMPFILE|O_EXCL|O_RDWR|O_CLOEXEC, S_IRUSR|S_IWUSR);\n+  if (fd_anon == -1) {\n+    XErrno err;\n+    log_debug_p(gc, init)(\"Failed to create anonymous file in %s (%s)\", mountpoint.get(),\n+                          (err == EINVAL ? \"Not supported\" : err.to_string()));\n+  } else {\n+    \/\/ Get inode number for anonymous file\n+    struct stat stat_buf;\n+    if (fstat(fd_anon, &stat_buf) == -1) {\n+      XErrno err;\n+      log_error_pd(gc)(\"Failed to determine inode number for anonymous file (%s)\", err.to_string());\n+      return -1;\n+    }\n+\n+    log_info_p(gc, init)(\"Heap Backing File: %s\/#\" UINT64_FORMAT, mountpoint.get(), (uint64_t)stat_buf.st_ino);\n+\n+    return fd_anon;\n+  }\n+\n+  log_debug_p(gc, init)(\"Falling back to open\/unlink\");\n+\n+  \/\/ Create file name\n+  char filename[PATH_MAX];\n+  snprintf(filename, sizeof(filename), \"%s\/%s.%d\", mountpoint.get(), name, os::current_process_id());\n+\n+  \/\/ Create file\n+  const int fd = os::open(filename, O_CREAT|O_EXCL|O_RDWR|O_CLOEXEC, S_IRUSR|S_IWUSR);\n+  if (fd == -1) {\n+    XErrno err;\n+    log_error_p(gc)(\"Failed to create file %s (%s)\", filename, err.to_string());\n+    return -1;\n+  }\n+\n+  \/\/ Unlink file\n+  if (unlink(filename) == -1) {\n+    XErrno err;\n+    log_error_p(gc)(\"Failed to unlink file %s (%s)\", filename, err.to_string());\n+    return -1;\n+  }\n+\n+  log_info_p(gc, init)(\"Heap Backing File: %s\", filename);\n+\n+  return fd;\n+}\n+\n+int XPhysicalMemoryBacking::create_fd(const char* name) const {\n+  if (AllocateHeapAt == nullptr) {\n+    \/\/ If the path is not explicitly specified, then we first try to create a memfd file\n+    \/\/ instead of looking for a tmpfd\/hugetlbfs mount point. Note that memfd_create() might\n+    \/\/ not be supported at all (requires kernel >= 3.17), or it might not support large\n+    \/\/ pages (requires kernel >= 4.14). If memfd_create() fails, then we try to create a\n+    \/\/ file on an accessible tmpfs or hugetlbfs mount point.\n+    const int fd = create_mem_fd(name);\n+    if (fd != -1) {\n+      return fd;\n+    }\n+\n+    log_debug_p(gc)(\"Falling back to searching for an accessible mount point\");\n+  }\n+\n+  return create_file_fd(name);\n+}\n+\n+bool XPhysicalMemoryBacking::is_initialized() const {\n+  return _initialized;\n+}\n+\n+void XPhysicalMemoryBacking::warn_available_space(size_t max_capacity) const {\n+  \/\/ Note that the available space on a tmpfs or a hugetlbfs filesystem\n+  \/\/ will be zero if no size limit was specified when it was mounted.\n+  if (_available == 0) {\n+    \/\/ No size limit set, skip check\n+    log_info_p(gc, init)(\"Available space on backing filesystem: N\/A\");\n+    return;\n+  }\n+\n+  log_info_p(gc, init)(\"Available space on backing filesystem: \" SIZE_FORMAT \"M\", _available \/ M);\n+\n+  \/\/ Warn if the filesystem doesn't currently have enough space available to hold\n+  \/\/ the max heap size. The max heap size will be capped if we later hit this limit\n+  \/\/ when trying to expand the heap.\n+  if (_available < max_capacity) {\n+    log_warning_p(gc)(\"***** WARNING! INCORRECT SYSTEM CONFIGURATION DETECTED! *****\");\n+    log_warning_p(gc)(\"Not enough space available on the backing filesystem to hold the current max Java heap\");\n+    log_warning_p(gc)(\"size (\" SIZE_FORMAT \"M). Please adjust the size of the backing filesystem accordingly \"\n+                      \"(available\", max_capacity \/ M);\n+    log_warning_p(gc)(\"space is currently \" SIZE_FORMAT \"M). Continuing execution with the current filesystem \"\n+                      \"size could\", _available \/ M);\n+    log_warning_p(gc)(\"lead to a premature OutOfMemoryError being thrown, due to failure to commit memory.\");\n+  }\n+}\n+\n+void XPhysicalMemoryBacking::warn_max_map_count(size_t max_capacity) const {\n+  const char* const filename = XFILENAME_PROC_MAX_MAP_COUNT;\n+  FILE* const file = os::fopen(filename, \"r\");\n+  if (file == nullptr) {\n+    \/\/ Failed to open file, skip check\n+    log_debug_p(gc, init)(\"Failed to open %s\", filename);\n+    return;\n+  }\n+\n+  size_t actual_max_map_count = 0;\n+  const int result = fscanf(file, SIZE_FORMAT, &actual_max_map_count);\n+  fclose(file);\n+  if (result != 1) {\n+    \/\/ Failed to read file, skip check\n+    log_debug_p(gc, init)(\"Failed to read %s\", filename);\n+    return;\n+  }\n+\n+  \/\/ The required max map count is impossible to calculate exactly since subsystems\n+  \/\/ other than ZGC are also creating memory mappings, and we have no control over that.\n+  \/\/ However, ZGC tends to create the most mappings and dominate the total count.\n+  \/\/ In the worst cases, ZGC will map each granule three times, i.e. once per heap view.\n+  \/\/ We speculate that we need another 20% to allow for non-ZGC subsystems to map memory.\n+  const size_t required_max_map_count = (max_capacity \/ XGranuleSize) * 3 * 1.2;\n+  if (actual_max_map_count < required_max_map_count) {\n+    log_warning_p(gc)(\"***** WARNING! INCORRECT SYSTEM CONFIGURATION DETECTED! *****\");\n+    log_warning_p(gc)(\"The system limit on number of memory mappings per process might be too low for the given\");\n+    log_warning_p(gc)(\"max Java heap size (\" SIZE_FORMAT \"M). Please adjust %s to allow for at\",\n+                      max_capacity \/ M, filename);\n+    log_warning_p(gc)(\"least \" SIZE_FORMAT \" mappings (current limit is \" SIZE_FORMAT \"). Continuing execution \"\n+                      \"with the current\", required_max_map_count, actual_max_map_count);\n+    log_warning_p(gc)(\"limit could lead to a premature OutOfMemoryError being thrown, due to failure to map memory.\");\n+  }\n+}\n+\n+void XPhysicalMemoryBacking::warn_commit_limits(size_t max_capacity) const {\n+  \/\/ Warn if available space is too low\n+  warn_available_space(max_capacity);\n+\n+  \/\/ Warn if max map count is too low\n+  warn_max_map_count(max_capacity);\n+}\n+\n+bool XPhysicalMemoryBacking::is_tmpfs() const {\n+  return _filesystem == TMPFS_MAGIC;\n+}\n+\n+bool XPhysicalMemoryBacking::is_hugetlbfs() const {\n+  return _filesystem == HUGETLBFS_MAGIC;\n+}\n+\n+bool XPhysicalMemoryBacking::tmpfs_supports_transparent_huge_pages() const {\n+  \/\/ If the shmem_enabled file exists and is readable then we\n+  \/\/ know the kernel supports transparent huge pages for tmpfs.\n+  return access(XFILENAME_SHMEM_ENABLED, R_OK) == 0;\n+}\n+\n+XErrno XPhysicalMemoryBacking::fallocate_compat_mmap_hugetlbfs(size_t offset, size_t length, bool touch) const {\n+  \/\/ On hugetlbfs, mapping a file segment will fail immediately, without\n+  \/\/ the need to touch the mapped pages first, if there aren't enough huge\n+  \/\/ pages available to back the mapping.\n+  void* const addr = mmap(0, length, PROT_READ|PROT_WRITE, MAP_SHARED, _fd, offset);\n+  if (addr == MAP_FAILED) {\n+    \/\/ Failed\n+    return errno;\n+  }\n+\n+  \/\/ Once mapped, the huge pages are only reserved. We need to touch them\n+  \/\/ to associate them with the file segment. Note that we can not punch\n+  \/\/ hole in file segments which only have reserved pages.\n+  if (touch) {\n+    char* const start = (char*)addr;\n+    char* const end = start + length;\n+    os::pretouch_memory(start, end, _block_size);\n+  }\n+\n+  \/\/ Unmap again. From now on, the huge pages that were mapped are allocated\n+  \/\/ to this file. There's no risk of getting a SIGBUS when mapping and\n+  \/\/ touching these pages again.\n+  if (munmap(addr, length) == -1) {\n+    \/\/ Failed\n+    return errno;\n+  }\n+\n+  \/\/ Success\n+  return 0;\n+}\n+\n+static bool safe_touch_mapping(void* addr, size_t length, size_t page_size) {\n+  char* const start = (char*)addr;\n+  char* const end = start + length;\n+\n+  \/\/ Touching a mapping that can't be backed by memory will generate a\n+  \/\/ SIGBUS. By using SafeFetch32 any SIGBUS will be safely caught and\n+  \/\/ handled. On tmpfs, doing a fetch (rather than a store) is enough\n+  \/\/ to cause backing pages to be allocated (there's no zero-page to\n+  \/\/ worry about).\n+  for (char *p = start; p < end; p += page_size) {\n+    if (SafeFetch32((int*)p, -1) == -1) {\n+      \/\/ Failed\n+      return false;\n+    }\n+  }\n+\n+  \/\/ Success\n+  return true;\n+}\n+\n+XErrno XPhysicalMemoryBacking::fallocate_compat_mmap_tmpfs(size_t offset, size_t length) const {\n+  \/\/ On tmpfs, we need to touch the mapped pages to figure out\n+  \/\/ if there are enough pages available to back the mapping.\n+  void* const addr = mmap(0, length, PROT_READ|PROT_WRITE, MAP_SHARED, _fd, offset);\n+  if (addr == MAP_FAILED) {\n+    \/\/ Failed\n+    return errno;\n+  }\n+\n+  \/\/ Advise mapping to use transparent huge pages\n+  os::realign_memory((char*)addr, length, XGranuleSize);\n+\n+  \/\/ Touch the mapping (safely) to make sure it's backed by memory\n+  const bool backed = safe_touch_mapping(addr, length, _block_size);\n+\n+  \/\/ Unmap again. If successfully touched, the backing memory will\n+  \/\/ be allocated to this file. There's no risk of getting a SIGBUS\n+  \/\/ when mapping and touching these pages again.\n+  if (munmap(addr, length) == -1) {\n+    \/\/ Failed\n+    return errno;\n+  }\n+\n+  \/\/ Success\n+  return backed ? 0 : ENOMEM;\n+}\n+\n+XErrno XPhysicalMemoryBacking::fallocate_compat_pwrite(size_t offset, size_t length) const {\n+  uint8_t data = 0;\n+\n+  \/\/ Allocate backing memory by writing to each block\n+  for (size_t pos = offset; pos < offset + length; pos += _block_size) {\n+    if (pwrite(_fd, &data, sizeof(data), pos) == -1) {\n+      \/\/ Failed\n+      return errno;\n+    }\n+  }\n+\n+  \/\/ Success\n+  return 0;\n+}\n+\n+XErrno XPhysicalMemoryBacking::fallocate_fill_hole_compat(size_t offset, size_t length) const {\n+  \/\/ fallocate(2) is only supported by tmpfs since Linux 3.5, and by hugetlbfs\n+  \/\/ since Linux 4.3. When fallocate(2) is not supported we emulate it using\n+  \/\/ mmap\/munmap (for hugetlbfs and tmpfs with transparent huge pages) or pwrite\n+  \/\/ (for tmpfs without transparent huge pages and other filesystem types).\n+  if (XLargePages::is_explicit()) {\n+    return fallocate_compat_mmap_hugetlbfs(offset, length, false \/* touch *\/);\n+  } else if (XLargePages::is_transparent()) {\n+    return fallocate_compat_mmap_tmpfs(offset, length);\n+  } else {\n+    return fallocate_compat_pwrite(offset, length);\n+  }\n+}\n+\n+XErrno XPhysicalMemoryBacking::fallocate_fill_hole_syscall(size_t offset, size_t length) const {\n+  const int mode = 0; \/\/ Allocate\n+  const int res = XSyscall::fallocate(_fd, mode, offset, length);\n+  if (res == -1) {\n+    \/\/ Failed\n+    return errno;\n+  }\n+\n+  \/\/ Success\n+  return 0;\n+}\n+\n+XErrno XPhysicalMemoryBacking::fallocate_fill_hole(size_t offset, size_t length) const {\n+  \/\/ Using compat mode is more efficient when allocating space on hugetlbfs.\n+  \/\/ Note that allocating huge pages this way will only reserve them, and not\n+  \/\/ associate them with segments of the file. We must guarantee that we at\n+  \/\/ some point touch these segments, otherwise we can not punch hole in them.\n+  \/\/ Also note that we need to use compat mode when using transparent huge pages,\n+  \/\/ since we need to use madvise(2) on the mapping before the page is allocated.\n+  if (z_fallocate_supported && !XLargePages::is_enabled()) {\n+     const XErrno err = fallocate_fill_hole_syscall(offset, length);\n+     if (!err) {\n+       \/\/ Success\n+       return 0;\n+     }\n+\n+     if (err != ENOSYS && err != EOPNOTSUPP) {\n+       \/\/ Failed\n+       return err;\n+     }\n+\n+     \/\/ Not supported\n+     log_debug_p(gc)(\"Falling back to fallocate() compatibility mode\");\n+     z_fallocate_supported = false;\n+  }\n+\n+  return fallocate_fill_hole_compat(offset, length);\n+}\n+\n+XErrno XPhysicalMemoryBacking::fallocate_punch_hole(size_t offset, size_t length) const {\n+  if (XLargePages::is_explicit()) {\n+    \/\/ We can only punch hole in pages that have been touched. Non-touched\n+    \/\/ pages are only reserved, and not associated with any specific file\n+    \/\/ segment. We don't know which pages have been previously touched, so\n+    \/\/ we always touch them here to guarantee that we can punch hole.\n+    const XErrno err = fallocate_compat_mmap_hugetlbfs(offset, length, true \/* touch *\/);\n+    if (err) {\n+      \/\/ Failed\n+      return err;\n+    }\n+  }\n+\n+  const int mode = FALLOC_FL_PUNCH_HOLE|FALLOC_FL_KEEP_SIZE;\n+  if (XSyscall::fallocate(_fd, mode, offset, length) == -1) {\n+    \/\/ Failed\n+    return errno;\n+  }\n+\n+  \/\/ Success\n+  return 0;\n+}\n+\n+XErrno XPhysicalMemoryBacking::split_and_fallocate(bool punch_hole, size_t offset, size_t length) const {\n+  \/\/ Try first half\n+  const size_t offset0 = offset;\n+  const size_t length0 = align_up(length \/ 2, _block_size);\n+  const XErrno err0 = fallocate(punch_hole, offset0, length0);\n+  if (err0) {\n+    return err0;\n+  }\n+\n+  \/\/ Try second half\n+  const size_t offset1 = offset0 + length0;\n+  const size_t length1 = length - length0;\n+  const XErrno err1 = fallocate(punch_hole, offset1, length1);\n+  if (err1) {\n+    return err1;\n+  }\n+\n+  \/\/ Success\n+  return 0;\n+}\n+\n+XErrno XPhysicalMemoryBacking::fallocate(bool punch_hole, size_t offset, size_t length) const {\n+  assert(is_aligned(offset, _block_size), \"Invalid offset\");\n+  assert(is_aligned(length, _block_size), \"Invalid length\");\n+\n+  const XErrno err = punch_hole ? fallocate_punch_hole(offset, length) : fallocate_fill_hole(offset, length);\n+  if (err == EINTR && length > _block_size) {\n+    \/\/ Calling fallocate(2) with a large length can take a long time to\n+    \/\/ complete. When running profilers, such as VTune, this syscall will\n+    \/\/ be constantly interrupted by signals. Expanding the file in smaller\n+    \/\/ steps avoids this problem.\n+    return split_and_fallocate(punch_hole, offset, length);\n+  }\n+\n+  return err;\n+}\n+\n+bool XPhysicalMemoryBacking::commit_inner(size_t offset, size_t length) const {\n+  log_trace(gc, heap)(\"Committing memory: \" SIZE_FORMAT \"M-\" SIZE_FORMAT \"M (\" SIZE_FORMAT \"M)\",\n+                      offset \/ M, (offset + length) \/ M, length \/ M);\n+\n+retry:\n+  const XErrno err = fallocate(false \/* punch_hole *\/, offset, length);\n+  if (err) {\n+    if (err == ENOSPC && !is_init_completed() && XLargePages::is_explicit() && z_fallocate_hugetlbfs_attempts-- > 0) {\n+      \/\/ If we fail to allocate during initialization, due to lack of space on\n+      \/\/ the hugetlbfs filesystem, then we wait and retry a few times before\n+      \/\/ giving up. Otherwise there is a risk that running JVMs back-to-back\n+      \/\/ will fail, since there is a delay between process termination and the\n+      \/\/ huge pages owned by that process being returned to the huge page pool\n+      \/\/ and made available for new allocations.\n+      log_debug_p(gc, init)(\"Failed to commit memory (%s), retrying\", err.to_string());\n+\n+      \/\/ Wait and retry in one second, in the hope that huge pages will be\n+      \/\/ available by then.\n+      sleep(1);\n+      goto retry;\n+    }\n+\n+    \/\/ Failed\n+    log_error_p(gc)(\"Failed to commit memory (%s)\", err.to_string());\n+    return false;\n+  }\n+\n+  \/\/ Success\n+  return true;\n+}\n+\n+static int offset_to_node(size_t offset) {\n+  const GrowableArray<int>* mapping = os::Linux::numa_nindex_to_node();\n+  const size_t nindex = (offset >> XGranuleSizeShift) % mapping->length();\n+  return mapping->at((int)nindex);\n+}\n+\n+size_t XPhysicalMemoryBacking::commit_numa_interleaved(size_t offset, size_t length) const {\n+  size_t committed = 0;\n+\n+  \/\/ Commit one granule at a time, so that each granule\n+  \/\/ can be allocated from a different preferred node.\n+  while (committed < length) {\n+    const size_t granule_offset = offset + committed;\n+\n+    \/\/ Setup NUMA policy to allocate memory from a preferred node\n+    os::Linux::numa_set_preferred(offset_to_node(granule_offset));\n+\n+    if (!commit_inner(granule_offset, XGranuleSize)) {\n+      \/\/ Failed\n+      break;\n+    }\n+\n+    committed += XGranuleSize;\n+  }\n+\n+  \/\/ Restore NUMA policy\n+  os::Linux::numa_set_preferred(-1);\n+\n+  return committed;\n+}\n+\n+size_t XPhysicalMemoryBacking::commit_default(size_t offset, size_t length) const {\n+  \/\/ Try to commit the whole region\n+  if (commit_inner(offset, length)) {\n+    \/\/ Success\n+    return length;\n+  }\n+\n+  \/\/ Failed, try to commit as much as possible\n+  size_t start = offset;\n+  size_t end = offset + length;\n+\n+  for (;;) {\n+    length = align_down((end - start) \/ 2, XGranuleSize);\n+    if (length < XGranuleSize) {\n+      \/\/ Done, don't commit more\n+      return start - offset;\n+    }\n+\n+    if (commit_inner(start, length)) {\n+      \/\/ Success, try commit more\n+      start += length;\n+    } else {\n+      \/\/ Failed, try commit less\n+      end -= length;\n+    }\n+  }\n+}\n+\n+size_t XPhysicalMemoryBacking::commit(size_t offset, size_t length) const {\n+  if (XNUMA::is_enabled() && !XLargePages::is_explicit()) {\n+    \/\/ To get granule-level NUMA interleaving when using non-large pages,\n+    \/\/ we must explicitly interleave the memory at commit\/fallocate time.\n+    return commit_numa_interleaved(offset, length);\n+  }\n+\n+  return commit_default(offset, length);\n+}\n+\n+size_t XPhysicalMemoryBacking::uncommit(size_t offset, size_t length) const {\n+  log_trace(gc, heap)(\"Uncommitting memory: \" SIZE_FORMAT \"M-\" SIZE_FORMAT \"M (\" SIZE_FORMAT \"M)\",\n+                      offset \/ M, (offset + length) \/ M, length \/ M);\n+\n+  const XErrno err = fallocate(true \/* punch_hole *\/, offset, length);\n+  if (err) {\n+    log_error(gc)(\"Failed to uncommit memory (%s)\", err.to_string());\n+    return 0;\n+  }\n+\n+  return length;\n+}\n+\n+void XPhysicalMemoryBacking::map(uintptr_t addr, size_t size, uintptr_t offset) const {\n+  const void* const res = mmap((void*)addr, size, PROT_READ|PROT_WRITE, MAP_FIXED|MAP_SHARED, _fd, offset);\n+  if (res == MAP_FAILED) {\n+    XErrno err;\n+    fatal(\"Failed to map memory (%s)\", err.to_string());\n+  }\n+}\n+\n+void XPhysicalMemoryBacking::unmap(uintptr_t addr, size_t size) const {\n+  \/\/ Note that we must keep the address space reservation intact and just detach\n+  \/\/ the backing memory. For this reason we map a new anonymous, non-accessible\n+  \/\/ and non-reserved page over the mapping instead of actually unmapping.\n+  const void* const res = mmap((void*)addr, size, PROT_NONE, MAP_FIXED | MAP_ANONYMOUS | MAP_PRIVATE | MAP_NORESERVE, -1, 0);\n+  if (res == MAP_FAILED) {\n+    XErrno err;\n+    fatal(\"Failed to map memory (%s)\", err.to_string());\n+  }\n+}\n","filename":"src\/hotspot\/os\/linux\/gc\/x\/xPhysicalMemoryBacking_linux.cpp","additions":724,"deletions":0,"binary":false,"changes":724,"status":"added"},{"patch":"@@ -0,0 +1,77 @@\n+\/*\n+ * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef OS_LINUX_GC_X_XPHYSICALMEMORYBACKING_LINUX_HPP\n+#define OS_LINUX_GC_X_XPHYSICALMEMORYBACKING_LINUX_HPP\n+\n+class XErrno;\n+\n+class XPhysicalMemoryBacking {\n+private:\n+  int      _fd;\n+  size_t   _size;\n+  uint64_t _filesystem;\n+  size_t   _block_size;\n+  size_t   _available;\n+  bool     _initialized;\n+\n+  void warn_available_space(size_t max_capacity) const;\n+  void warn_max_map_count(size_t max_capacity) const;\n+\n+  int create_mem_fd(const char* name) const;\n+  int create_file_fd(const char* name) const;\n+  int create_fd(const char* name) const;\n+\n+  bool is_tmpfs() const;\n+  bool is_hugetlbfs() const;\n+  bool tmpfs_supports_transparent_huge_pages() const;\n+\n+  XErrno fallocate_compat_mmap_hugetlbfs(size_t offset, size_t length, bool touch) const;\n+  XErrno fallocate_compat_mmap_tmpfs(size_t offset, size_t length) const;\n+  XErrno fallocate_compat_pwrite(size_t offset, size_t length) const;\n+  XErrno fallocate_fill_hole_compat(size_t offset, size_t length) const;\n+  XErrno fallocate_fill_hole_syscall(size_t offset, size_t length) const;\n+  XErrno fallocate_fill_hole(size_t offset, size_t length) const;\n+  XErrno fallocate_punch_hole(size_t offset, size_t length) const;\n+  XErrno split_and_fallocate(bool punch_hole, size_t offset, size_t length) const;\n+  XErrno fallocate(bool punch_hole, size_t offset, size_t length) const;\n+\n+  bool commit_inner(size_t offset, size_t length) const;\n+  size_t commit_numa_interleaved(size_t offset, size_t length) const;\n+  size_t commit_default(size_t offset, size_t length) const;\n+\n+public:\n+  XPhysicalMemoryBacking(size_t max_capacity);\n+\n+  bool is_initialized() const;\n+\n+  void warn_commit_limits(size_t max_capacity) const;\n+\n+  size_t commit(size_t offset, size_t length) const;\n+  size_t uncommit(size_t offset, size_t length) const;\n+\n+  void map(uintptr_t addr, size_t size, uintptr_t offset) const;\n+  void unmap(uintptr_t addr, size_t size) const;\n+};\n+\n+#endif \/\/ OS_LINUX_GC_X_XPHYSICALMEMORYBACKING_LINUX_HPP\n","filename":"src\/hotspot\/os\/linux\/gc\/x\/xPhysicalMemoryBacking_linux.hpp","additions":77,"deletions":0,"binary":false,"changes":77,"status":"added"},{"patch":"@@ -0,0 +1,40 @@\n+\/*\n+ * Copyright (c) 2019, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/x\/xSyscall_linux.hpp\"\n+#include OS_CPU_HEADER(gc\/x\/xSyscall)\n+\n+#include <unistd.h>\n+\n+int XSyscall::memfd_create(const char *name, unsigned int flags) {\n+  return syscall(SYS_memfd_create, name, flags);\n+}\n+\n+int XSyscall::fallocate(int fd, int mode, size_t offset, size_t length) {\n+  return syscall(SYS_fallocate, fd, mode, offset, length);\n+}\n+\n+long XSyscall::get_mempolicy(int* mode, unsigned long* nodemask, unsigned long maxnode, void* addr, unsigned long flags) {\n+  return syscall(SYS_get_mempolicy, mode, nodemask, maxnode, addr, flags);\n+}\n","filename":"src\/hotspot\/os\/linux\/gc\/x\/xSyscall_linux.cpp","additions":40,"deletions":0,"binary":false,"changes":40,"status":"added"},{"patch":"@@ -0,0 +1,45 @@\n+\/*\n+ * Copyright (c) 2019, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef OS_LINUX_GC_X_XSYSCALL_LINUX_HPP\n+#define OS_LINUX_GC_X_XSYSCALL_LINUX_HPP\n+\n+#include \"memory\/allStatic.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+\/\/ Flags for get_mempolicy()\n+#ifndef MPOL_F_NODE\n+#define MPOL_F_NODE        (1<<0)\n+#endif\n+#ifndef MPOL_F_ADDR\n+#define MPOL_F_ADDR        (1<<1)\n+#endif\n+\n+class XSyscall : public AllStatic {\n+public:\n+  static int memfd_create(const char* name, unsigned int flags);\n+  static int fallocate(int fd, int mode, size_t offset, size_t length);\n+  static long get_mempolicy(int* mode, unsigned long* nodemask, unsigned long maxnode, void* addr, unsigned long flags);\n+};\n+\n+#endif \/\/ OS_LINUX_GC_X_XSYSCALL_LINUX_HPP\n","filename":"src\/hotspot\/os\/linux\/gc\/x\/xSyscall_linux.hpp","additions":45,"deletions":0,"binary":false,"changes":45,"status":"added"},{"patch":"@@ -26,0 +26,1 @@\n+#include \"gc\/z\/zAddress.inline.hpp\"\n@@ -388,1 +389,1 @@\n-ZErrno ZPhysicalMemoryBacking::fallocate_compat_mmap_hugetlbfs(size_t offset, size_t length, bool touch) const {\n+ZErrno ZPhysicalMemoryBacking::fallocate_compat_mmap_hugetlbfs(zoffset offset, size_t length, bool touch) const {\n@@ -392,1 +393,1 @@\n-  void* const addr = mmap(0, length, PROT_READ|PROT_WRITE, MAP_SHARED, _fd, offset);\n+  void* const addr = mmap(0, length, PROT_READ|PROT_WRITE, MAP_SHARED, _fd, untype(offset));\n@@ -439,1 +440,1 @@\n-ZErrno ZPhysicalMemoryBacking::fallocate_compat_mmap_tmpfs(size_t offset, size_t length) const {\n+ZErrno ZPhysicalMemoryBacking::fallocate_compat_mmap_tmpfs(zoffset offset, size_t length) const {\n@@ -442,1 +443,1 @@\n-  void* const addr = mmap(0, length, PROT_READ|PROT_WRITE, MAP_SHARED, _fd, offset);\n+  void* const addr = mmap(0, length, PROT_READ|PROT_WRITE, MAP_SHARED, _fd, untype(offset));\n@@ -466,1 +467,1 @@\n-ZErrno ZPhysicalMemoryBacking::fallocate_compat_pwrite(size_t offset, size_t length) const {\n+ZErrno ZPhysicalMemoryBacking::fallocate_compat_pwrite(zoffset offset, size_t length) const {\n@@ -470,2 +471,2 @@\n-  for (size_t pos = offset; pos < offset + length; pos += _block_size) {\n-    if (pwrite(_fd, &data, sizeof(data), pos) == -1) {\n+  for (zoffset pos = offset; pos < offset + length; pos += _block_size) {\n+    if (pwrite(_fd, &data, sizeof(data), untype(pos)) == -1) {\n@@ -481,1 +482,1 @@\n-ZErrno ZPhysicalMemoryBacking::fallocate_fill_hole_compat(size_t offset, size_t length) const {\n+ZErrno ZPhysicalMemoryBacking::fallocate_fill_hole_compat(zoffset offset, size_t length) const {\n@@ -495,1 +496,1 @@\n-ZErrno ZPhysicalMemoryBacking::fallocate_fill_hole_syscall(size_t offset, size_t length) const {\n+ZErrno ZPhysicalMemoryBacking::fallocate_fill_hole_syscall(zoffset offset, size_t length) const {\n@@ -497,1 +498,1 @@\n-  const int res = ZSyscall::fallocate(_fd, mode, offset, length);\n+  const int res = ZSyscall::fallocate(_fd, mode, untype(offset), length);\n@@ -507,1 +508,1 @@\n-ZErrno ZPhysicalMemoryBacking::fallocate_fill_hole(size_t offset, size_t length) const {\n+ZErrno ZPhysicalMemoryBacking::fallocate_fill_hole(zoffset offset, size_t length) const {\n@@ -534,1 +535,1 @@\n-ZErrno ZPhysicalMemoryBacking::fallocate_punch_hole(size_t offset, size_t length) const {\n+ZErrno ZPhysicalMemoryBacking::fallocate_punch_hole(zoffset offset, size_t length) const {\n@@ -548,1 +549,1 @@\n-  if (ZSyscall::fallocate(_fd, mode, offset, length) == -1) {\n+  if (ZSyscall::fallocate(_fd, mode, untype(offset), length) == -1) {\n@@ -557,1 +558,1 @@\n-ZErrno ZPhysicalMemoryBacking::split_and_fallocate(bool punch_hole, size_t offset, size_t length) const {\n+ZErrno ZPhysicalMemoryBacking::split_and_fallocate(bool punch_hole, zoffset offset, size_t length) const {\n@@ -559,1 +560,1 @@\n-  const size_t offset0 = offset;\n+  const zoffset offset0 = offset;\n@@ -567,1 +568,1 @@\n-  const size_t offset1 = offset0 + length0;\n+  const zoffset offset1 = offset0 + length0;\n@@ -578,2 +579,2 @@\n-ZErrno ZPhysicalMemoryBacking::fallocate(bool punch_hole, size_t offset, size_t length) const {\n-  assert(is_aligned(offset, _block_size), \"Invalid offset\");\n+ZErrno ZPhysicalMemoryBacking::fallocate(bool punch_hole, zoffset offset, size_t length) const {\n+  assert(is_aligned(untype(offset), _block_size), \"Invalid offset\");\n@@ -594,1 +595,1 @@\n-bool ZPhysicalMemoryBacking::commit_inner(size_t offset, size_t length) const {\n+bool ZPhysicalMemoryBacking::commit_inner(zoffset offset, size_t length) const {\n@@ -596,1 +597,1 @@\n-                      offset \/ M, (offset + length) \/ M, length \/ M);\n+                      untype(offset) \/ M, untype(offset + length) \/ M, length \/ M);\n@@ -625,1 +626,1 @@\n-static int offset_to_node(size_t offset) {\n+static int offset_to_node(zoffset offset) {\n@@ -627,1 +628,1 @@\n-  const size_t nindex = (offset >> ZGranuleSizeShift) % mapping->length();\n+  const size_t nindex = (untype(offset) >> ZGranuleSizeShift) % mapping->length();\n@@ -631,1 +632,1 @@\n-size_t ZPhysicalMemoryBacking::commit_numa_interleaved(size_t offset, size_t length) const {\n+size_t ZPhysicalMemoryBacking::commit_numa_interleaved(zoffset offset, size_t length) const {\n@@ -637,1 +638,1 @@\n-    const size_t granule_offset = offset + committed;\n+    const zoffset granule_offset = offset + committed;\n@@ -656,1 +657,1 @@\n-size_t ZPhysicalMemoryBacking::commit_default(size_t offset, size_t length) const {\n+size_t ZPhysicalMemoryBacking::commit_default(zoffset offset, size_t length) const {\n@@ -664,2 +665,2 @@\n-  size_t start = offset;\n-  size_t end = offset + length;\n+  zoffset start = offset;\n+  zoffset end = offset + length;\n@@ -684,1 +685,1 @@\n-size_t ZPhysicalMemoryBacking::commit(size_t offset, size_t length) const {\n+size_t ZPhysicalMemoryBacking::commit(zoffset offset, size_t length) const {\n@@ -694,1 +695,1 @@\n-size_t ZPhysicalMemoryBacking::uncommit(size_t offset, size_t length) const {\n+size_t ZPhysicalMemoryBacking::uncommit(zoffset offset, size_t length) const {\n@@ -696,1 +697,1 @@\n-                      offset \/ M, (offset + length) \/ M, length \/ M);\n+                      untype(offset) \/ M, untype(offset + length) \/ M, length \/ M);\n@@ -707,2 +708,2 @@\n-void ZPhysicalMemoryBacking::map(uintptr_t addr, size_t size, uintptr_t offset) const {\n-  const void* const res = mmap((void*)addr, size, PROT_READ|PROT_WRITE, MAP_FIXED|MAP_SHARED, _fd, offset);\n+void ZPhysicalMemoryBacking::map(zaddress_unsafe addr, size_t size, zoffset offset) const {\n+  const void* const res = mmap((void*)untype(addr), size, PROT_READ|PROT_WRITE, MAP_FIXED|MAP_SHARED, _fd, untype(offset));\n@@ -715,1 +716,1 @@\n-void ZPhysicalMemoryBacking::unmap(uintptr_t addr, size_t size) const {\n+void ZPhysicalMemoryBacking::unmap(zaddress_unsafe addr, size_t size) const {\n@@ -719,1 +720,1 @@\n-  const void* const res = mmap((void*)addr, size, PROT_NONE, MAP_FIXED | MAP_ANONYMOUS | MAP_PRIVATE | MAP_NORESERVE, -1, 0);\n+  const void* const res = mmap((void*)untype(addr), size, PROT_NONE, MAP_FIXED | MAP_ANONYMOUS | MAP_PRIVATE | MAP_NORESERVE, -1, 0);\n","filename":"src\/hotspot\/os\/linux\/gc\/z\/zPhysicalMemoryBacking_linux.cpp","additions":35,"deletions":34,"binary":false,"changes":69,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,0 +27,2 @@\n+#include \"gc\/z\/zAddress.hpp\"\n+\n@@ -49,9 +51,9 @@\n-  ZErrno fallocate_compat_mmap_hugetlbfs(size_t offset, size_t length, bool touch) const;\n-  ZErrno fallocate_compat_mmap_tmpfs(size_t offset, size_t length) const;\n-  ZErrno fallocate_compat_pwrite(size_t offset, size_t length) const;\n-  ZErrno fallocate_fill_hole_compat(size_t offset, size_t length) const;\n-  ZErrno fallocate_fill_hole_syscall(size_t offset, size_t length) const;\n-  ZErrno fallocate_fill_hole(size_t offset, size_t length) const;\n-  ZErrno fallocate_punch_hole(size_t offset, size_t length) const;\n-  ZErrno split_and_fallocate(bool punch_hole, size_t offset, size_t length) const;\n-  ZErrno fallocate(bool punch_hole, size_t offset, size_t length) const;\n+  ZErrno fallocate_compat_mmap_hugetlbfs(zoffset offset, size_t length, bool touch) const;\n+  ZErrno fallocate_compat_mmap_tmpfs(zoffset offset, size_t length) const;\n+  ZErrno fallocate_compat_pwrite(zoffset offset, size_t length) const;\n+  ZErrno fallocate_fill_hole_compat(zoffset offset, size_t length) const;\n+  ZErrno fallocate_fill_hole_syscall(zoffset offset, size_t length) const;\n+  ZErrno fallocate_fill_hole(zoffset offset, size_t length) const;\n+  ZErrno fallocate_punch_hole(zoffset offset, size_t length) const;\n+  ZErrno split_and_fallocate(bool punch_hole, zoffset offset, size_t length) const;\n+  ZErrno fallocate(bool punch_hole, zoffset offset, size_t length) const;\n@@ -59,3 +61,3 @@\n-  bool commit_inner(size_t offset, size_t length) const;\n-  size_t commit_numa_interleaved(size_t offset, size_t length) const;\n-  size_t commit_default(size_t offset, size_t length) const;\n+  bool commit_inner(zoffset offset, size_t length) const;\n+  size_t commit_numa_interleaved(zoffset offset, size_t length) const;\n+  size_t commit_default(zoffset offset, size_t length) const;\n@@ -70,2 +72,2 @@\n-  size_t commit(size_t offset, size_t length) const;\n-  size_t uncommit(size_t offset, size_t length) const;\n+  size_t commit(zoffset offset, size_t length) const;\n+  size_t uncommit(zoffset offset, size_t length) const;\n@@ -73,2 +75,2 @@\n-  void map(uintptr_t addr, size_t size, uintptr_t offset) const;\n-  void unmap(uintptr_t addr, size_t size) const;\n+  void map(zaddress_unsafe addr, size_t size, zoffset offset) const;\n+  void unmap(zaddress_unsafe addr, size_t size) const;\n","filename":"src\/hotspot\/os\/linux\/gc\/z\/zPhysicalMemoryBacking_linux.hpp","additions":19,"deletions":17,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -25,1 +25,1 @@\n-#include \"gc\/z\/zArguments.hpp\"\n+#include \"gc\/x\/xArguments.hpp\"\n@@ -27,1 +27,1 @@\n-bool ZArguments::is_os_supported() const {\n+bool XArguments::is_os_supported() {\n","filename":"src\/hotspot\/os\/posix\/gc\/x\/xArguments_posix.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"previous_filename":"src\/hotspot\/os\/posix\/gc\/z\/zArguments_posix.cpp","status":"copied"},{"patch":"@@ -25,1 +25,1 @@\n-#include \"gc\/z\/zArguments.hpp\"\n+#include \"gc\/x\/xInitialize.hpp\"\n@@ -27,2 +27,2 @@\n-bool ZArguments::is_os_supported() const {\n-  return true;\n+void XInitialize::pd_initialize() {\n+  \/\/ Does nothing\n","filename":"src\/hotspot\/os\/posix\/gc\/x\/xInitialize_posix.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"previous_filename":"src\/hotspot\/os\/posix\/gc\/z\/zArguments_posix.cpp","status":"copied"},{"patch":"@@ -0,0 +1,43 @@\n+\/*\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/x\/xUtils.hpp\"\n+#include \"utilities\/debug.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+#include <stdlib.h>\n+\n+uintptr_t XUtils::alloc_aligned(size_t alignment, size_t size) {\n+  void* res = nullptr;\n+\n+  \/\/ Use raw posix_memalign as long as we have no wrapper for it\n+  ALLOW_C_FUNCTION(::posix_memalign, int rc = posix_memalign(&res, alignment, size);)\n+  if (rc != 0) {\n+    fatal(\"posix_memalign() failed\");\n+  }\n+\n+  memset(res, 0, size);\n+\n+  return (uintptr_t)res;\n+}\n","filename":"src\/hotspot\/os\/posix\/gc\/x\/xUtils_posix.cpp","additions":43,"deletions":0,"binary":false,"changes":43,"status":"added"},{"patch":"@@ -0,0 +1,60 @@\n+\/*\n+ * Copyright (c) 2015, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/x\/xAddress.inline.hpp\"\n+#include \"gc\/x\/xVirtualMemory.hpp\"\n+#include \"logging\/log.hpp\"\n+\n+#include <sys\/mman.h>\n+#include <sys\/types.h>\n+\n+void XVirtualMemoryManager::pd_initialize_before_reserve() {\n+  \/\/ Does nothing\n+}\n+\n+void XVirtualMemoryManager::pd_initialize_after_reserve() {\n+  \/\/ Does nothing\n+}\n+\n+bool XVirtualMemoryManager::pd_reserve(uintptr_t addr, size_t size) {\n+  const uintptr_t res = (uintptr_t)mmap((void*)addr, size, PROT_NONE, MAP_ANONYMOUS|MAP_PRIVATE|MAP_NORESERVE, -1, 0);\n+  if (res == (uintptr_t)MAP_FAILED) {\n+    \/\/ Failed to reserve memory\n+    return false;\n+  }\n+\n+  if (res != addr) {\n+    \/\/ Failed to reserve memory at the requested address\n+    munmap((void*)res, size);\n+    return false;\n+  }\n+\n+  \/\/ Success\n+  return true;\n+}\n+\n+void XVirtualMemoryManager::pd_unreserve(uintptr_t addr, size_t size) {\n+  const int res = munmap((void*)addr, size);\n+  assert(res == 0, \"Failed to unmap memory\");\n+}\n","filename":"src\/hotspot\/os\/posix\/gc\/x\/xVirtualMemory_posix.cpp","additions":60,"deletions":0,"binary":false,"changes":60,"status":"added"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2019, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,1 +27,1 @@\n-bool ZArguments::is_os_supported() const {\n+bool ZArguments::is_os_supported() {\n","filename":"src\/hotspot\/os\/posix\/gc\/z\/zArguments_posix.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -40,3 +40,3 @@\n-bool ZVirtualMemoryManager::pd_reserve(uintptr_t addr, size_t size) {\n-  const uintptr_t res = (uintptr_t)mmap((void*)addr, size, PROT_NONE, MAP_ANONYMOUS|MAP_PRIVATE|MAP_NORESERVE, -1, 0);\n-  if (res == (uintptr_t)MAP_FAILED) {\n+bool ZVirtualMemoryManager::pd_reserve(zaddress_unsafe addr, size_t size) {\n+  void* const res = mmap((void*)untype(addr), size, PROT_NONE, MAP_ANONYMOUS|MAP_PRIVATE|MAP_NORESERVE, -1, 0);\n+  if (res == MAP_FAILED) {\n@@ -47,1 +47,1 @@\n-  if (res != addr) {\n+  if (res != (void*)untype(addr)) {\n@@ -49,1 +49,1 @@\n-    munmap((void*)res, size);\n+    munmap(res, size);\n@@ -57,2 +57,2 @@\n-void ZVirtualMemoryManager::pd_unreserve(uintptr_t addr, size_t size) {\n-  const int res = munmap((void*)addr, size);\n+void ZVirtualMemoryManager::pd_unreserve(zaddress_unsafe addr, size_t size) {\n+  const int res = munmap((void*)untype(addr), size);\n","filename":"src\/hotspot\/os\/posix\/gc\/z\/zVirtualMemory_posix.cpp","additions":8,"deletions":8,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -0,0 +1,30 @@\n+\/*\n+ * Copyright (c) 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/x\/xArguments.hpp\"\n+#include \"gc\/x\/xSyscall_windows.hpp\"\n+\n+bool XArguments::is_os_supported() {\n+  return XSyscall::is_supported();\n+}\n","filename":"src\/hotspot\/os\/windows\/gc\/x\/xArguments_windows.cpp","additions":30,"deletions":0,"binary":false,"changes":30,"status":"added"},{"patch":"@@ -0,0 +1,30 @@\n+\/*\n+ * Copyright (c) 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/x\/xInitialize.hpp\"\n+#include \"gc\/x\/xSyscall_windows.hpp\"\n+\n+void XInitialize::pd_initialize() {\n+  XSyscall::initialize();\n+}\n","filename":"src\/hotspot\/os\/windows\/gc\/x\/xInitialize_windows.cpp","additions":30,"deletions":0,"binary":false,"changes":30,"status":"added"},{"patch":"@@ -0,0 +1,40 @@\n+\/*\n+ * Copyright (c) 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/shared\/gcLogPrecious.hpp\"\n+#include \"gc\/x\/xLargePages.hpp\"\n+#include \"gc\/x\/xSyscall_windows.hpp\"\n+#include \"runtime\/globals.hpp\"\n+\n+void XLargePages::pd_initialize() {\n+  if (UseLargePages) {\n+    if (XSyscall::is_large_pages_supported()) {\n+      _state = Explicit;\n+      return;\n+    }\n+    log_info_p(gc, init)(\"Shared large pages not supported on this OS version\");\n+  }\n+\n+  _state = Disabled;\n+}\n","filename":"src\/hotspot\/os\/windows\/gc\/x\/xLargePages_windows.cpp","additions":40,"deletions":0,"binary":false,"changes":40,"status":"added"},{"patch":"@@ -0,0 +1,310 @@\n+\/*\n+ * Copyright (c) 2019, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/x\/xMapper_windows.hpp\"\n+#include \"gc\/x\/xSyscall_windows.hpp\"\n+#include \"logging\/log.hpp\"\n+#include \"utilities\/debug.hpp\"\n+\n+#include <Windows.h>\n+\n+\/\/ Memory reservation, commit, views, and placeholders.\n+\/\/\n+\/\/ To be able to up-front reserve address space for the heap views, and later\n+\/\/ multi-map the heap views to the same physical memory, without ever losing the\n+\/\/ reservation of the reserved address space, we use \"placeholders\".\n+\/\/\n+\/\/ These placeholders block out the address space from being used by other parts\n+\/\/ of the process. To commit memory in this address space, the placeholder must\n+\/\/ be replaced by anonymous memory, or replaced by mapping a view against a\n+\/\/ paging file mapping. We use the later to support multi-mapping.\n+\/\/\n+\/\/ We want to be able to dynamically commit and uncommit the physical memory of\n+\/\/ the heap (and also unmap ZPages), in granules of ZGranuleSize bytes. There is\n+\/\/ no way to grow and shrink the committed memory of a paging file mapping.\n+\/\/ Therefore, we create multiple granule-sized page file mappings. The memory is\n+\/\/ committed by creating a page file mapping, map a view against it, commit the\n+\/\/ memory, unmap the view. The memory will stay committed until all views are\n+\/\/ unmapped, and the paging file mapping handle is closed.\n+\/\/\n+\/\/ When replacing a placeholder address space reservation with a mapped view\n+\/\/ against a paging file mapping, the virtual address space must exactly match\n+\/\/ an existing placeholder's address and size. Therefore we only deal with\n+\/\/ granule-sized placeholders at this layer. Higher layers that keep track of\n+\/\/ reserved available address space can (and will) coalesce placeholders, but\n+\/\/ they will be split before being used.\n+\n+#define fatal_error(msg, addr, size)                  \\\n+  fatal(msg \": \" PTR_FORMAT \" \" SIZE_FORMAT \"M (%d)\", \\\n+        (addr), (size) \/ M, GetLastError())\n+\n+uintptr_t XMapper::reserve(uintptr_t addr, size_t size) {\n+  void* const res = XSyscall::VirtualAlloc2(\n+    GetCurrentProcess(),                   \/\/ Process\n+    (void*)addr,                           \/\/ BaseAddress\n+    size,                                  \/\/ Size\n+    MEM_RESERVE | MEM_RESERVE_PLACEHOLDER, \/\/ AllocationType\n+    PAGE_NOACCESS,                         \/\/ PageProtection\n+    nullptr,                               \/\/ ExtendedParameters\n+    0                                      \/\/ ParameterCount\n+    );\n+\n+  \/\/ Caller responsible for error handling\n+  return (uintptr_t)res;\n+}\n+\n+void XMapper::unreserve(uintptr_t addr, size_t size) {\n+  const bool res = XSyscall::VirtualFreeEx(\n+    GetCurrentProcess(), \/\/ hProcess\n+    (void*)addr,         \/\/ lpAddress\n+    size,                \/\/ dwSize\n+    MEM_RELEASE          \/\/ dwFreeType\n+    );\n+\n+  if (!res) {\n+    fatal_error(\"Failed to unreserve memory\", addr, size);\n+  }\n+}\n+\n+HANDLE XMapper::create_paging_file_mapping(size_t size) {\n+  \/\/ Create mapping with SEC_RESERVE instead of SEC_COMMIT.\n+  \/\/\n+  \/\/ We use MapViewOfFile3 for two different reasons:\n+  \/\/  1) When committing memory for the created paging file\n+  \/\/  2) When mapping a view of the memory created in (2)\n+  \/\/\n+  \/\/ The non-platform code is only setup to deal with out-of-memory\n+  \/\/ errors in (1). By using SEC_RESERVE, we prevent MapViewOfFile3\n+  \/\/ from failing because of \"commit limit\" checks. To actually commit\n+  \/\/ memory in (1), a call to VirtualAlloc2 is done.\n+\n+  HANDLE const res = XSyscall::CreateFileMappingW(\n+    INVALID_HANDLE_VALUE,         \/\/ hFile\n+    nullptr,                      \/\/ lpFileMappingAttribute\n+    PAGE_READWRITE | SEC_RESERVE, \/\/ flProtect\n+    size >> 32,                   \/\/ dwMaximumSizeHigh\n+    size & 0xFFFFFFFF,            \/\/ dwMaximumSizeLow\n+    nullptr                       \/\/ lpName\n+    );\n+\n+  \/\/ Caller responsible for error handling\n+  return res;\n+}\n+\n+bool XMapper::commit_paging_file_mapping(HANDLE file_handle, uintptr_t file_offset, size_t size) {\n+  const uintptr_t addr = map_view_no_placeholder(file_handle, file_offset, size);\n+  if (addr == 0) {\n+    log_error(gc)(\"Failed to map view of paging file mapping (%d)\", GetLastError());\n+    return false;\n+  }\n+\n+  const uintptr_t res = commit(addr, size);\n+  if (res != addr) {\n+    log_error(gc)(\"Failed to commit memory (%d)\", GetLastError());\n+  }\n+\n+  unmap_view_no_placeholder(addr, size);\n+\n+  return res == addr;\n+}\n+\n+uintptr_t XMapper::map_view_no_placeholder(HANDLE file_handle, uintptr_t file_offset, size_t size) {\n+  void* const res = XSyscall::MapViewOfFile3(\n+    file_handle,         \/\/ FileMapping\n+    GetCurrentProcess(), \/\/ ProcessHandle\n+    nullptr,             \/\/ BaseAddress\n+    file_offset,         \/\/ Offset\n+    size,                \/\/ ViewSize\n+    0,                   \/\/ AllocationType\n+    PAGE_NOACCESS,       \/\/ PageProtection\n+    nullptr,             \/\/ ExtendedParameters\n+    0                    \/\/ ParameterCount\n+    );\n+\n+  \/\/ Caller responsible for error handling\n+  return (uintptr_t)res;\n+}\n+\n+void XMapper::unmap_view_no_placeholder(uintptr_t addr, size_t size) {\n+  const bool res = XSyscall::UnmapViewOfFile2(\n+    GetCurrentProcess(), \/\/ ProcessHandle\n+    (void*)addr,         \/\/ BaseAddress\n+    0                    \/\/ UnmapFlags\n+    );\n+\n+  if (!res) {\n+    fatal_error(\"Failed to unmap memory\", addr, size);\n+  }\n+}\n+\n+uintptr_t XMapper::commit(uintptr_t addr, size_t size) {\n+  void* const res = XSyscall::VirtualAlloc2(\n+    GetCurrentProcess(), \/\/ Process\n+    (void*)addr,         \/\/ BaseAddress\n+    size,                \/\/ Size\n+    MEM_COMMIT,          \/\/ AllocationType\n+    PAGE_NOACCESS,       \/\/ PageProtection\n+    nullptr,             \/\/ ExtendedParameters\n+    0                    \/\/ ParameterCount\n+    );\n+\n+  \/\/ Caller responsible for error handling\n+  return (uintptr_t)res;\n+}\n+\n+HANDLE XMapper::create_and_commit_paging_file_mapping(size_t size) {\n+  HANDLE const file_handle = create_paging_file_mapping(size);\n+  if (file_handle == 0) {\n+    log_error(gc)(\"Failed to create paging file mapping (%d)\", GetLastError());\n+    return 0;\n+  }\n+\n+  const bool res = commit_paging_file_mapping(file_handle, 0 \/* file_offset *\/, size);\n+  if (!res) {\n+    close_paging_file_mapping(file_handle);\n+    return 0;\n+  }\n+\n+  return file_handle;\n+}\n+\n+void XMapper::close_paging_file_mapping(HANDLE file_handle) {\n+  const bool res = CloseHandle(\n+    file_handle \/\/ hObject\n+    );\n+\n+  if (!res) {\n+    fatal(\"Failed to close paging file handle (%d)\", GetLastError());\n+  }\n+}\n+\n+HANDLE XMapper::create_shared_awe_section() {\n+  MEM_EXTENDED_PARAMETER parameter = { 0 };\n+  parameter.Type = MemSectionExtendedParameterUserPhysicalFlags;\n+  parameter.ULong64 = 0;\n+\n+  HANDLE section = XSyscall::CreateFileMapping2(\n+    INVALID_HANDLE_VALUE,                 \/\/ File\n+    nullptr,                              \/\/ SecurityAttributes\n+    SECTION_MAP_READ | SECTION_MAP_WRITE, \/\/ DesiredAccess\n+    PAGE_READWRITE,                       \/\/ PageProtection\n+    SEC_RESERVE | SEC_LARGE_PAGES,        \/\/ AllocationAttributes\n+    0,                                    \/\/ MaximumSize\n+    nullptr,                              \/\/ Name\n+    &parameter,                           \/\/ ExtendedParameters\n+    1                                     \/\/ ParameterCount\n+    );\n+\n+  if (section == nullptr) {\n+    fatal(\"Could not create shared AWE section (%d)\", GetLastError());\n+  }\n+\n+  return section;\n+}\n+\n+uintptr_t XMapper::reserve_for_shared_awe(HANDLE awe_section, uintptr_t addr, size_t size) {\n+  MEM_EXTENDED_PARAMETER parameter = { 0 };\n+  parameter.Type = MemExtendedParameterUserPhysicalHandle;\n+  parameter.Handle = awe_section;\n+\n+  void* const res = XSyscall::VirtualAlloc2(\n+    GetCurrentProcess(),        \/\/ Process\n+    (void*)addr,                \/\/ BaseAddress\n+    size,                       \/\/ Size\n+    MEM_RESERVE | MEM_PHYSICAL, \/\/ AllocationType\n+    PAGE_READWRITE,             \/\/ PageProtection\n+    &parameter,                 \/\/ ExtendedParameters\n+    1                           \/\/ ParameterCount\n+    );\n+\n+  \/\/ Caller responsible for error handling\n+  return (uintptr_t)res;\n+}\n+\n+void XMapper::unreserve_for_shared_awe(uintptr_t addr, size_t size) {\n+  bool res = VirtualFree(\n+    (void*)addr, \/\/ lpAddress\n+    0,           \/\/ dwSize\n+    MEM_RELEASE  \/\/ dwFreeType\n+    );\n+\n+  if (!res) {\n+    fatal(\"Failed to unreserve memory: \" PTR_FORMAT \" \" SIZE_FORMAT \"M (%d)\",\n+          addr, size \/ M, GetLastError());\n+  }\n+}\n+\n+void XMapper::split_placeholder(uintptr_t addr, size_t size) {\n+  const bool res = VirtualFree(\n+    (void*)addr,                           \/\/ lpAddress\n+    size,                                  \/\/ dwSize\n+    MEM_RELEASE | MEM_PRESERVE_PLACEHOLDER \/\/ dwFreeType\n+    );\n+\n+  if (!res) {\n+    fatal_error(\"Failed to split placeholder\", addr, size);\n+  }\n+}\n+\n+void XMapper::coalesce_placeholders(uintptr_t addr, size_t size) {\n+  const bool res = VirtualFree(\n+    (void*)addr,                            \/\/ lpAddress\n+    size,                                   \/\/ dwSize\n+    MEM_RELEASE | MEM_COALESCE_PLACEHOLDERS \/\/ dwFreeType\n+    );\n+\n+  if (!res) {\n+    fatal_error(\"Failed to coalesce placeholders\", addr, size);\n+  }\n+}\n+\n+void XMapper::map_view_replace_placeholder(HANDLE file_handle, uintptr_t file_offset, uintptr_t addr, size_t size) {\n+  void* const res = XSyscall::MapViewOfFile3(\n+    file_handle,             \/\/ FileMapping\n+    GetCurrentProcess(),     \/\/ ProcessHandle\n+    (void*)addr,             \/\/ BaseAddress\n+    file_offset,             \/\/ Offset\n+    size,                    \/\/ ViewSize\n+    MEM_REPLACE_PLACEHOLDER, \/\/ AllocationType\n+    PAGE_READWRITE,          \/\/ PageProtection\n+    nullptr,                 \/\/ ExtendedParameters\n+    0                        \/\/ ParameterCount\n+    );\n+\n+  if (res == nullptr) {\n+    fatal_error(\"Failed to map memory\", addr, size);\n+  }\n+}\n+\n+void XMapper::unmap_view_preserve_placeholder(uintptr_t addr, size_t size) {\n+  const bool res = XSyscall::UnmapViewOfFile2(\n+    GetCurrentProcess(),     \/\/ ProcessHandle\n+    (void*)addr,             \/\/ BaseAddress\n+    MEM_PRESERVE_PLACEHOLDER \/\/ UnmapFlags\n+    );\n+\n+  if (!res) {\n+    fatal_error(\"Failed to unmap memory\", addr, size);\n+  }\n+}\n","filename":"src\/hotspot\/os\/windows\/gc\/x\/xMapper_windows.cpp","additions":310,"deletions":0,"binary":false,"changes":310,"status":"added"},{"patch":"@@ -24,2 +24,2 @@\n-#ifndef OS_WINDOWS_GC_Z_ZMAPPER_WINDOWS_HPP\n-#define OS_WINDOWS_GC_Z_ZMAPPER_WINDOWS_HPP\n+#ifndef OS_WINDOWS_GC_X_XMAPPER_WINDOWS_HPP\n+#define OS_WINDOWS_GC_X_XMAPPER_WINDOWS_HPP\n@@ -32,1 +32,1 @@\n-class ZMapper : public AllStatic {\n+class XMapper : public AllStatic {\n@@ -94,1 +94,1 @@\n-#endif \/\/ OS_WINDOWS_GC_Z_ZMAPPER_WINDOWS_HPP\n+#endif \/\/ OS_WINDOWS_GC_X_XMAPPER_WINDOWS_HPP\n","filename":"src\/hotspot\/os\/windows\/gc\/x\/xMapper_windows.hpp","additions":4,"deletions":4,"binary":false,"changes":8,"previous_filename":"src\/hotspot\/os\/windows\/gc\/z\/zMapper_windows.hpp","status":"copied"},{"patch":"@@ -0,0 +1,42 @@\n+\/*\n+ * Copyright (c) 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/x\/xNUMA.hpp\"\n+\n+void XNUMA::pd_initialize() {\n+  _enabled = false;\n+}\n+\n+uint32_t XNUMA::count() {\n+  return 1;\n+}\n+\n+uint32_t XNUMA::id() {\n+  return 0;\n+}\n+\n+uint32_t XNUMA::memory_id(uintptr_t addr) {\n+  \/\/ NUMA support not enabled, assume everything belongs to node zero\n+  return 0;\n+}\n","filename":"src\/hotspot\/os\/windows\/gc\/x\/xNUMA_windows.cpp","additions":42,"deletions":0,"binary":false,"changes":42,"status":"added"},{"patch":"@@ -0,0 +1,252 @@\n+\/*\n+ * Copyright (c) 2019, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/x\/xGlobals.hpp\"\n+#include \"gc\/x\/xGranuleMap.inline.hpp\"\n+#include \"gc\/x\/xLargePages.inline.hpp\"\n+#include \"gc\/x\/xMapper_windows.hpp\"\n+#include \"gc\/x\/xPhysicalMemoryBacking_windows.hpp\"\n+#include \"logging\/log.hpp\"\n+#include \"runtime\/globals.hpp\"\n+#include \"utilities\/debug.hpp\"\n+\n+class XPhysicalMemoryBackingImpl : public CHeapObj<mtGC> {\n+public:\n+  virtual size_t commit(size_t offset, size_t size) = 0;\n+  virtual size_t uncommit(size_t offset, size_t size) = 0;\n+  virtual void map(uintptr_t addr, size_t size, size_t offset) const = 0;\n+  virtual void unmap(uintptr_t addr, size_t size) const = 0;\n+};\n+\n+\/\/ Implements small pages (paged) support using placeholder reservation.\n+\/\/\n+\/\/ The backing commits and uncommits physical memory, that can be\n+\/\/ multi-mapped into the virtual address space. To support fine-graned\n+\/\/ committing and uncommitting, each XGranuleSize'd chunk is mapped to\n+\/\/ a separate paging file mapping.\n+\n+class XPhysicalMemoryBackingSmallPages : public XPhysicalMemoryBackingImpl {\n+private:\n+  XGranuleMap<HANDLE> _handles;\n+\n+  HANDLE get_handle(uintptr_t offset) const {\n+    HANDLE const handle = _handles.get(offset);\n+    assert(handle != 0, \"Should be set\");\n+    return handle;\n+  }\n+\n+  void put_handle(uintptr_t offset, HANDLE handle) {\n+    assert(handle != INVALID_HANDLE_VALUE, \"Invalid handle\");\n+    assert(_handles.get(offset) == 0, \"Should be cleared\");\n+    _handles.put(offset, handle);\n+  }\n+\n+  void clear_handle(uintptr_t offset) {\n+    assert(_handles.get(offset) != 0, \"Should be set\");\n+    _handles.put(offset, 0);\n+  }\n+\n+public:\n+  XPhysicalMemoryBackingSmallPages(size_t max_capacity) :\n+      XPhysicalMemoryBackingImpl(),\n+      _handles(max_capacity) {}\n+\n+  size_t commit(size_t offset, size_t size) {\n+    for (size_t i = 0; i < size; i += XGranuleSize) {\n+      HANDLE const handle = XMapper::create_and_commit_paging_file_mapping(XGranuleSize);\n+      if (handle == 0) {\n+        return i;\n+      }\n+\n+      put_handle(offset + i, handle);\n+    }\n+\n+    return size;\n+  }\n+\n+  size_t uncommit(size_t offset, size_t size) {\n+    for (size_t i = 0; i < size; i += XGranuleSize) {\n+      HANDLE const handle = get_handle(offset + i);\n+      clear_handle(offset + i);\n+      XMapper::close_paging_file_mapping(handle);\n+    }\n+\n+    return size;\n+  }\n+\n+  void map(uintptr_t addr, size_t size, size_t offset) const {\n+    assert(is_aligned(offset, XGranuleSize), \"Misaligned\");\n+    assert(is_aligned(addr, XGranuleSize), \"Misaligned\");\n+    assert(is_aligned(size, XGranuleSize), \"Misaligned\");\n+\n+    for (size_t i = 0; i < size; i += XGranuleSize) {\n+      HANDLE const handle = get_handle(offset + i);\n+      XMapper::map_view_replace_placeholder(handle, 0 \/* offset *\/, addr + i, XGranuleSize);\n+    }\n+  }\n+\n+  void unmap(uintptr_t addr, size_t size) const {\n+    assert(is_aligned(addr, XGranuleSize), \"Misaligned\");\n+    assert(is_aligned(size, XGranuleSize), \"Misaligned\");\n+\n+    for (size_t i = 0; i < size; i += XGranuleSize) {\n+      XMapper::unmap_view_preserve_placeholder(addr + i, XGranuleSize);\n+    }\n+  }\n+};\n+\n+\/\/ Implements Large Pages (locked) support using shared AWE physical memory.\n+\/\/\n+\/\/ Shared AWE physical memory also works with small pages, but it has\n+\/\/ a few drawbacks that makes it a no-go to use it at this point:\n+\/\/\n+\/\/ 1) It seems to use 8 bytes of committed memory per *reserved* memory.\n+\/\/ Given our scheme to use a large address space range this turns out to\n+\/\/ use too much memory.\n+\/\/\n+\/\/ 2) It requires memory locking privileges, even for small pages. This\n+\/\/ has always been a requirement for large pages, and would be an extra\n+\/\/ restriction for usage with small pages.\n+\/\/\n+\/\/ Note: The large pages size is tied to our XGranuleSize.\n+\n+extern HANDLE XAWESection;\n+\n+class XPhysicalMemoryBackingLargePages : public XPhysicalMemoryBackingImpl {\n+private:\n+  ULONG_PTR* const _page_array;\n+\n+  static ULONG_PTR* alloc_page_array(size_t max_capacity) {\n+    const size_t npages = max_capacity \/ XGranuleSize;\n+    const size_t array_size = npages * sizeof(ULONG_PTR);\n+\n+    return (ULONG_PTR*)os::malloc(array_size, mtGC);\n+  }\n+\n+public:\n+  XPhysicalMemoryBackingLargePages(size_t max_capacity) :\n+      XPhysicalMemoryBackingImpl(),\n+      _page_array(alloc_page_array(max_capacity)) {}\n+\n+  size_t commit(size_t offset, size_t size) {\n+    const size_t index = offset >> XGranuleSizeShift;\n+    const size_t npages = size >> XGranuleSizeShift;\n+\n+    size_t npages_res = npages;\n+    const bool res = AllocateUserPhysicalPages(XAWESection, &npages_res, &_page_array[index]);\n+    if (!res) {\n+      fatal(\"Failed to allocate physical memory \" SIZE_FORMAT \"M @ \" PTR_FORMAT \" (%d)\",\n+            size \/ M, offset, GetLastError());\n+    } else {\n+      log_debug(gc)(\"Allocated physical memory: \" SIZE_FORMAT \"M @ \" PTR_FORMAT, size \/ M, offset);\n+    }\n+\n+    \/\/ AllocateUserPhysicalPages might not be able to allocate the requested amount of memory.\n+    \/\/ The allocated number of pages are written in npages_res.\n+    return npages_res << XGranuleSizeShift;\n+  }\n+\n+  size_t uncommit(size_t offset, size_t size) {\n+    const size_t index = offset >> XGranuleSizeShift;\n+    const size_t npages = size >> XGranuleSizeShift;\n+\n+    size_t npages_res = npages;\n+    const bool res = FreeUserPhysicalPages(XAWESection, &npages_res, &_page_array[index]);\n+    if (!res) {\n+      fatal(\"Failed to uncommit physical memory \" SIZE_FORMAT \"M @ \" PTR_FORMAT \" (%d)\",\n+            size, offset, GetLastError());\n+    }\n+\n+    return npages_res << XGranuleSizeShift;\n+  }\n+\n+  void map(uintptr_t addr, size_t size, size_t offset) const {\n+    const size_t npages = size >> XGranuleSizeShift;\n+    const size_t index = offset >> XGranuleSizeShift;\n+\n+    const bool res = MapUserPhysicalPages((char*)addr, npages, &_page_array[index]);\n+    if (!res) {\n+      fatal(\"Failed to map view \" PTR_FORMAT \" \" SIZE_FORMAT \"M @ \" PTR_FORMAT \" (%d)\",\n+            addr, size \/ M, offset, GetLastError());\n+    }\n+  }\n+\n+  void unmap(uintptr_t addr, size_t size) const {\n+    const size_t npages = size >> XGranuleSizeShift;\n+\n+    const bool res = MapUserPhysicalPages((char*)addr, npages, nullptr);\n+    if (!res) {\n+      fatal(\"Failed to unmap view \" PTR_FORMAT \" \" SIZE_FORMAT \"M (%d)\",\n+            addr, size \/ M, GetLastError());\n+    }\n+  }\n+};\n+\n+static XPhysicalMemoryBackingImpl* select_impl(size_t max_capacity) {\n+  if (XLargePages::is_enabled()) {\n+    return new XPhysicalMemoryBackingLargePages(max_capacity);\n+  }\n+\n+  return new XPhysicalMemoryBackingSmallPages(max_capacity);\n+}\n+\n+XPhysicalMemoryBacking::XPhysicalMemoryBacking(size_t max_capacity) :\n+    _impl(select_impl(max_capacity)) {}\n+\n+bool XPhysicalMemoryBacking::is_initialized() const {\n+  return true;\n+}\n+\n+void XPhysicalMemoryBacking::warn_commit_limits(size_t max_capacity) const {\n+  \/\/ Does nothing\n+}\n+\n+size_t XPhysicalMemoryBacking::commit(size_t offset, size_t length) {\n+  log_trace(gc, heap)(\"Committing memory: \" SIZE_FORMAT \"M-\" SIZE_FORMAT \"M (\" SIZE_FORMAT \"M)\",\n+                      offset \/ M, (offset + length) \/ M, length \/ M);\n+\n+  return _impl->commit(offset, length);\n+}\n+\n+size_t XPhysicalMemoryBacking::uncommit(size_t offset, size_t length) {\n+  log_trace(gc, heap)(\"Uncommitting memory: \" SIZE_FORMAT \"M-\" SIZE_FORMAT \"M (\" SIZE_FORMAT \"M)\",\n+                      offset \/ M, (offset + length) \/ M, length \/ M);\n+\n+  return _impl->uncommit(offset, length);\n+}\n+\n+void XPhysicalMemoryBacking::map(uintptr_t addr, size_t size, size_t offset) const {\n+  assert(is_aligned(offset, XGranuleSize), \"Misaligned: \" PTR_FORMAT, offset);\n+  assert(is_aligned(addr, XGranuleSize), \"Misaligned: \" PTR_FORMAT, addr);\n+  assert(is_aligned(size, XGranuleSize), \"Misaligned: \" PTR_FORMAT, size);\n+\n+  _impl->map(addr, size, offset);\n+}\n+\n+void XPhysicalMemoryBacking::unmap(uintptr_t addr, size_t size) const {\n+  assert(is_aligned(addr, XGranuleSize), \"Misaligned\");\n+  assert(is_aligned(size, XGranuleSize), \"Misaligned\");\n+\n+  _impl->unmap(addr, size);\n+}\n","filename":"src\/hotspot\/os\/windows\/gc\/x\/xPhysicalMemoryBacking_windows.cpp","additions":252,"deletions":0,"binary":false,"changes":252,"status":"added"},{"patch":"@@ -0,0 +1,51 @@\n+\/*\n+ * Copyright (c) 2019, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef OS_WINDOWS_GC_X_XPHYSICALMEMORYBACKING_WINDOWS_HPP\n+#define OS_WINDOWS_GC_X_XPHYSICALMEMORYBACKING_WINDOWS_HPP\n+\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+#include <Windows.h>\n+\n+class XPhysicalMemoryBackingImpl;\n+\n+class XPhysicalMemoryBacking {\n+private:\n+  XPhysicalMemoryBackingImpl* _impl;\n+\n+public:\n+  XPhysicalMemoryBacking(size_t max_capacity);\n+\n+  bool is_initialized() const;\n+\n+  void warn_commit_limits(size_t max_capacity) const;\n+\n+  size_t commit(size_t offset, size_t length);\n+  size_t uncommit(size_t offset, size_t length);\n+\n+  void map(uintptr_t addr, size_t size, size_t offset) const;\n+  void unmap(uintptr_t addr, size_t size) const;\n+};\n+\n+#endif \/\/ OS_WINDOWS_GC_X_XPHYSICALMEMORYBACKING_WINDOWS_HPP\n","filename":"src\/hotspot\/os\/windows\/gc\/x\/xPhysicalMemoryBacking_windows.hpp","additions":51,"deletions":0,"binary":false,"changes":51,"status":"added"},{"patch":"@@ -0,0 +1,93 @@\n+\/*\n+ * Copyright (c) 2019, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/shared\/gcLogPrecious.hpp\"\n+#include \"gc\/x\/xSyscall_windows.hpp\"\n+#include \"runtime\/java.hpp\"\n+#include \"runtime\/os.hpp\"\n+\n+XSyscall::CreateFileMappingWFn XSyscall::CreateFileMappingW;\n+XSyscall::CreateFileMapping2Fn XSyscall::CreateFileMapping2;\n+XSyscall::VirtualAlloc2Fn XSyscall::VirtualAlloc2;\n+XSyscall::VirtualFreeExFn XSyscall::VirtualFreeEx;\n+XSyscall::MapViewOfFile3Fn XSyscall::MapViewOfFile3;\n+XSyscall::UnmapViewOfFile2Fn XSyscall::UnmapViewOfFile2;\n+\n+static void* lookup_kernelbase_library() {\n+  const char* const name = \"KernelBase\";\n+  char ebuf[1024];\n+  void* const handle = os::dll_load(name, ebuf, sizeof(ebuf));\n+  if (handle == nullptr) {\n+    log_error_p(gc)(\"Failed to load library: %s\", name);\n+  }\n+  return handle;\n+}\n+\n+static void* lookup_kernelbase_symbol(const char* name) {\n+  static void* const handle = lookup_kernelbase_library();\n+  if (handle == nullptr) {\n+    return nullptr;\n+  }\n+  return os::dll_lookup(handle, name);\n+}\n+\n+static bool has_kernelbase_symbol(const char* name) {\n+  return lookup_kernelbase_symbol(name) != nullptr;\n+}\n+\n+template <typename Fn>\n+static void install_kernelbase_symbol(Fn*& fn, const char* name) {\n+  fn = reinterpret_cast<Fn*>(lookup_kernelbase_symbol(name));\n+}\n+\n+template <typename Fn>\n+static void install_kernelbase_1803_symbol_or_exit(Fn*& fn, const char* name) {\n+  install_kernelbase_symbol(fn, name);\n+  if (fn == nullptr) {\n+    log_error_p(gc)(\"Failed to lookup symbol: %s\", name);\n+    vm_exit_during_initialization(\"ZGC requires Windows version 1803 or later\");\n+  }\n+}\n+\n+void XSyscall::initialize() {\n+  \/\/ Required\n+  install_kernelbase_1803_symbol_or_exit(CreateFileMappingW, \"CreateFileMappingW\");\n+  install_kernelbase_1803_symbol_or_exit(VirtualAlloc2,      \"VirtualAlloc2\");\n+  install_kernelbase_1803_symbol_or_exit(VirtualFreeEx,      \"VirtualFreeEx\");\n+  install_kernelbase_1803_symbol_or_exit(MapViewOfFile3,     \"MapViewOfFile3\");\n+  install_kernelbase_1803_symbol_or_exit(UnmapViewOfFile2,   \"UnmapViewOfFile2\");\n+\n+  \/\/ Optional - for large pages support\n+  install_kernelbase_symbol(CreateFileMapping2, \"CreateFileMapping2\");\n+}\n+\n+bool XSyscall::is_supported() {\n+  \/\/ Available in Windows version 1803 and later\n+  return has_kernelbase_symbol(\"VirtualAlloc2\");\n+}\n+\n+bool XSyscall::is_large_pages_supported() {\n+  \/\/ Available in Windows version 1809 and later\n+  return has_kernelbase_symbol(\"CreateFileMapping2\");\n+}\n","filename":"src\/hotspot\/os\/windows\/gc\/x\/xSyscall_windows.cpp","additions":93,"deletions":0,"binary":false,"changes":93,"status":"added"},{"patch":"@@ -0,0 +1,55 @@\n+\/*\n+ * Copyright (c) 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef OS_WINDOWS_GC_X_XSYSCALL_WINDOWS_HPP\n+#define OS_WINDOWS_GC_X_XSYSCALL_WINDOWS_HPP\n+\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+#include <Windows.h>\n+#include <Memoryapi.h>\n+\n+class XSyscall {\n+private:\n+  typedef HANDLE (*CreateFileMappingWFn)(HANDLE, LPSECURITY_ATTRIBUTES, DWORD, DWORD, DWORD, LPCWSTR);\n+  typedef HANDLE (*CreateFileMapping2Fn)(HANDLE, LPSECURITY_ATTRIBUTES, ULONG, ULONG, ULONG, ULONG64, PCWSTR, PMEM_EXTENDED_PARAMETER, ULONG);\n+  typedef PVOID (*VirtualAlloc2Fn)(HANDLE, PVOID, SIZE_T, ULONG, ULONG, MEM_EXTENDED_PARAMETER*, ULONG);\n+  typedef BOOL (*VirtualFreeExFn)(HANDLE, LPVOID, SIZE_T, DWORD);\n+  typedef PVOID (*MapViewOfFile3Fn)(HANDLE, HANDLE, PVOID, ULONG64, SIZE_T, ULONG, ULONG, MEM_EXTENDED_PARAMETER*, ULONG);\n+  typedef BOOL (*UnmapViewOfFile2Fn)(HANDLE, PVOID, ULONG);\n+\n+public:\n+  static CreateFileMappingWFn CreateFileMappingW;\n+  static CreateFileMapping2Fn CreateFileMapping2;\n+  static VirtualAlloc2Fn      VirtualAlloc2;\n+  static VirtualFreeExFn      VirtualFreeEx;\n+  static MapViewOfFile3Fn     MapViewOfFile3;\n+  static UnmapViewOfFile2Fn   UnmapViewOfFile2;\n+\n+  static void initialize();\n+\n+  static bool is_supported();\n+  static bool is_large_pages_supported();\n+};\n+\n+#endif \/\/ OS_WINDOWS_GC_X_XSYSCALL_WINDOWS_HPP\n","filename":"src\/hotspot\/os\/windows\/gc\/x\/xSyscall_windows.hpp","additions":55,"deletions":0,"binary":false,"changes":55,"status":"added"},{"patch":"@@ -0,0 +1,40 @@\n+\/*\n+ * Copyright (c) 2019, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/x\/xUtils.hpp\"\n+#include \"utilities\/debug.hpp\"\n+\n+#include <malloc.h>\n+\n+uintptr_t XUtils::alloc_aligned(size_t alignment, size_t size) {\n+  void* const res = _aligned_malloc(size, alignment);\n+\n+  if (res == nullptr) {\n+    fatal(\"_aligned_malloc failed\");\n+  }\n+\n+  memset(res, 0, size);\n+\n+  return (uintptr_t)res;\n+}\n","filename":"src\/hotspot\/os\/windows\/gc\/x\/xUtils_windows.cpp","additions":40,"deletions":0,"binary":false,"changes":40,"status":"added"},{"patch":"@@ -0,0 +1,195 @@\n+\/*\n+ * Copyright (c) 2019, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/x\/xAddress.inline.hpp\"\n+#include \"gc\/x\/xGlobals.hpp\"\n+#include \"gc\/x\/xLargePages.inline.hpp\"\n+#include \"gc\/x\/xMapper_windows.hpp\"\n+#include \"gc\/x\/xSyscall_windows.hpp\"\n+#include \"gc\/x\/xVirtualMemory.hpp\"\n+#include \"utilities\/align.hpp\"\n+#include \"utilities\/debug.hpp\"\n+\n+class XVirtualMemoryManagerImpl : public CHeapObj<mtGC> {\n+public:\n+  virtual void initialize_before_reserve() {}\n+  virtual void initialize_after_reserve(XMemoryManager* manager) {}\n+  virtual bool reserve(uintptr_t addr, size_t size) = 0;\n+  virtual void unreserve(uintptr_t addr, size_t size) = 0;\n+};\n+\n+\/\/ Implements small pages (paged) support using placeholder reservation.\n+class XVirtualMemoryManagerSmallPages : public XVirtualMemoryManagerImpl {\n+private:\n+  class PlaceholderCallbacks : public AllStatic {\n+  public:\n+    static void split_placeholder(uintptr_t start, size_t size) {\n+      XMapper::split_placeholder(XAddress::marked0(start), size);\n+      XMapper::split_placeholder(XAddress::marked1(start), size);\n+      XMapper::split_placeholder(XAddress::remapped(start), size);\n+    }\n+\n+    static void coalesce_placeholders(uintptr_t start, size_t size) {\n+      XMapper::coalesce_placeholders(XAddress::marked0(start), size);\n+      XMapper::coalesce_placeholders(XAddress::marked1(start), size);\n+      XMapper::coalesce_placeholders(XAddress::remapped(start), size);\n+    }\n+\n+    static void split_into_placeholder_granules(uintptr_t start, size_t size) {\n+      for (uintptr_t addr = start; addr < start + size; addr += XGranuleSize) {\n+        split_placeholder(addr, XGranuleSize);\n+      }\n+    }\n+\n+    static void coalesce_into_one_placeholder(uintptr_t start, size_t size) {\n+      assert(is_aligned(size, XGranuleSize), \"Must be granule aligned\");\n+\n+      if (size > XGranuleSize) {\n+        coalesce_placeholders(start, size);\n+      }\n+    }\n+\n+    static void create_callback(const XMemory* area) {\n+      assert(is_aligned(area->size(), XGranuleSize), \"Must be granule aligned\");\n+      coalesce_into_one_placeholder(area->start(), area->size());\n+    }\n+\n+    static void destroy_callback(const XMemory* area) {\n+      assert(is_aligned(area->size(), XGranuleSize), \"Must be granule aligned\");\n+      \/\/ Don't try split the last granule - VirtualFree will fail\n+      split_into_placeholder_granules(area->start(), area->size() - XGranuleSize);\n+    }\n+\n+    static void shrink_from_front_callback(const XMemory* area, size_t size) {\n+      assert(is_aligned(size, XGranuleSize), \"Must be granule aligned\");\n+      split_into_placeholder_granules(area->start(), size);\n+    }\n+\n+    static void shrink_from_back_callback(const XMemory* area, size_t size) {\n+      assert(is_aligned(size, XGranuleSize), \"Must be granule aligned\");\n+      \/\/ Don't try split the last granule - VirtualFree will fail\n+      split_into_placeholder_granules(area->end() - size, size - XGranuleSize);\n+    }\n+\n+    static void grow_from_front_callback(const XMemory* area, size_t size) {\n+      assert(is_aligned(area->size(), XGranuleSize), \"Must be granule aligned\");\n+      coalesce_into_one_placeholder(area->start() - size, area->size() + size);\n+    }\n+\n+    static void grow_from_back_callback(const XMemory* area, size_t size) {\n+      assert(is_aligned(area->size(), XGranuleSize), \"Must be granule aligned\");\n+      coalesce_into_one_placeholder(area->start(), area->size() + size);\n+    }\n+\n+    static void register_with(XMemoryManager* manager) {\n+      \/\/ Each reserved virtual memory address area registered in _manager is\n+      \/\/ exactly covered by a single placeholder. Callbacks are installed so\n+      \/\/ that whenever a memory area changes, the corresponding placeholder\n+      \/\/ is adjusted.\n+      \/\/\n+      \/\/ The create and grow callbacks are called when virtual memory is\n+      \/\/ returned to the memory manager. The new memory area is then covered\n+      \/\/ by a new single placeholder.\n+      \/\/\n+      \/\/ The destroy and shrink callbacks are called when virtual memory is\n+      \/\/ allocated from the memory manager. The memory area is then is split\n+      \/\/ into granule-sized placeholders.\n+      \/\/\n+      \/\/ See comment in zMapper_windows.cpp explaining why placeholders are\n+      \/\/ split into XGranuleSize sized placeholders.\n+\n+      XMemoryManager::Callbacks callbacks;\n+\n+      callbacks._create = &create_callback;\n+      callbacks._destroy = &destroy_callback;\n+      callbacks._shrink_from_front = &shrink_from_front_callback;\n+      callbacks._shrink_from_back = &shrink_from_back_callback;\n+      callbacks._grow_from_front = &grow_from_front_callback;\n+      callbacks._grow_from_back = &grow_from_back_callback;\n+\n+      manager->register_callbacks(callbacks);\n+    }\n+  };\n+\n+  virtual void initialize_after_reserve(XMemoryManager* manager) {\n+    PlaceholderCallbacks::register_with(manager);\n+  }\n+\n+  virtual bool reserve(uintptr_t addr, size_t size) {\n+    const uintptr_t res = XMapper::reserve(addr, size);\n+\n+    assert(res == addr || res == 0, \"Should not reserve other memory than requested\");\n+    return res == addr;\n+  }\n+\n+  virtual void unreserve(uintptr_t addr, size_t size) {\n+    XMapper::unreserve(addr, size);\n+  }\n+};\n+\n+\/\/ Implements Large Pages (locked) support using shared AWE physical memory.\n+\n+\/\/ XPhysicalMemory layer needs access to the section\n+HANDLE XAWESection;\n+\n+class XVirtualMemoryManagerLargePages : public XVirtualMemoryManagerImpl {\n+private:\n+  virtual void initialize_before_reserve() {\n+    XAWESection = XMapper::create_shared_awe_section();\n+  }\n+\n+  virtual bool reserve(uintptr_t addr, size_t size) {\n+    const uintptr_t res = XMapper::reserve_for_shared_awe(XAWESection, addr, size);\n+\n+    assert(res == addr || res == 0, \"Should not reserve other memory than requested\");\n+    return res == addr;\n+  }\n+\n+  virtual void unreserve(uintptr_t addr, size_t size) {\n+    XMapper::unreserve_for_shared_awe(addr, size);\n+  }\n+};\n+\n+static XVirtualMemoryManagerImpl* _impl = nullptr;\n+\n+void XVirtualMemoryManager::pd_initialize_before_reserve() {\n+  if (XLargePages::is_enabled()) {\n+    _impl = new XVirtualMemoryManagerLargePages();\n+  } else {\n+    _impl = new XVirtualMemoryManagerSmallPages();\n+  }\n+  _impl->initialize_before_reserve();\n+}\n+\n+void XVirtualMemoryManager::pd_initialize_after_reserve() {\n+  _impl->initialize_after_reserve(&_manager);\n+}\n+\n+bool XVirtualMemoryManager::pd_reserve(uintptr_t addr, size_t size) {\n+  return _impl->reserve(addr, size);\n+}\n+\n+void XVirtualMemoryManager::pd_unreserve(uintptr_t addr, size_t size) {\n+  _impl->unreserve(addr, size);\n+}\n","filename":"src\/hotspot\/os\/windows\/gc\/x\/xVirtualMemory_windows.cpp","additions":195,"deletions":0,"binary":false,"changes":195,"status":"added"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2019, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,1 +28,1 @@\n-bool ZArguments::is_os_supported() const {\n+bool ZArguments::is_os_supported() {\n","filename":"src\/hotspot\/os\/windows\/gc\/z\/zArguments_windows.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -25,0 +25,1 @@\n+#include \"gc\/z\/zAddress.inline.hpp\"\n@@ -62,1 +63,1 @@\n-uintptr_t ZMapper::reserve(uintptr_t addr, size_t size) {\n+zaddress_unsafe ZMapper::reserve(zaddress_unsafe addr, size_t size) {\n@@ -65,1 +66,1 @@\n-    (void*)addr,                           \/\/ BaseAddress\n+    (void*)untype(addr),                   \/\/ BaseAddress\n@@ -74,1 +75,1 @@\n-  return (uintptr_t)res;\n+  return to_zaddress_unsafe((uintptr_t)res);\n@@ -77,1 +78,1 @@\n-void ZMapper::unreserve(uintptr_t addr, size_t size) {\n+void ZMapper::unreserve(zaddress_unsafe addr, size_t size) {\n@@ -80,1 +81,1 @@\n-    (void*)addr,         \/\/ lpAddress\n+    (void*)untype(addr), \/\/ lpAddress\n@@ -86,1 +87,1 @@\n-    fatal_error(\"Failed to unreserve memory\", addr, size);\n+    fatal_error(\"Failed to unreserve memory\", untype(addr), size);\n@@ -226,1 +227,1 @@\n-uintptr_t ZMapper::reserve_for_shared_awe(HANDLE awe_section, uintptr_t addr, size_t size) {\n+zaddress_unsafe ZMapper::reserve_for_shared_awe(HANDLE awe_section, zaddress_unsafe addr, size_t size) {\n@@ -233,1 +234,1 @@\n-    (void*)addr,                \/\/ BaseAddress\n+    (void*)untype(addr),        \/\/ BaseAddress\n@@ -242,1 +243,1 @@\n-  return (uintptr_t)res;\n+  return to_zaddress_unsafe((uintptr_t)res);\n@@ -245,1 +246,1 @@\n-void ZMapper::unreserve_for_shared_awe(uintptr_t addr, size_t size) {\n+void ZMapper::unreserve_for_shared_awe(zaddress_unsafe addr, size_t size) {\n@@ -247,3 +248,3 @@\n-    (void*)addr, \/\/ lpAddress\n-    0,           \/\/ dwSize\n-    MEM_RELEASE  \/\/ dwFreeType\n+    (void*)untype(addr), \/\/ lpAddress\n+    0,                   \/\/ dwSize\n+    MEM_RELEASE          \/\/ dwFreeType\n@@ -254,1 +255,1 @@\n-          addr, size \/ M, GetLastError());\n+          untype(addr), size \/ M, GetLastError());\n@@ -258,1 +259,1 @@\n-void ZMapper::split_placeholder(uintptr_t addr, size_t size) {\n+void ZMapper::split_placeholder(zaddress_unsafe addr, size_t size) {\n@@ -260,1 +261,1 @@\n-    (void*)addr,                           \/\/ lpAddress\n+    (void*)untype(addr),                   \/\/ lpAddress\n@@ -270,1 +271,1 @@\n-void ZMapper::coalesce_placeholders(uintptr_t addr, size_t size) {\n+void ZMapper::coalesce_placeholders(zaddress_unsafe addr, size_t size) {\n@@ -272,1 +273,1 @@\n-    (void*)addr,                            \/\/ lpAddress\n+    (void*)untype(addr),                    \/\/ lpAddress\n@@ -282,1 +283,1 @@\n-void ZMapper::map_view_replace_placeholder(HANDLE file_handle, uintptr_t file_offset, uintptr_t addr, size_t size) {\n+void ZMapper::map_view_replace_placeholder(HANDLE file_handle, uintptr_t file_offset, zaddress_unsafe addr, size_t size) {\n@@ -286,1 +287,1 @@\n-    (void*)addr,             \/\/ BaseAddress\n+    (void*)untype(addr),     \/\/ BaseAddress\n@@ -300,1 +301,1 @@\n-void ZMapper::unmap_view_preserve_placeholder(uintptr_t addr, size_t size) {\n+void ZMapper::unmap_view_preserve_placeholder(zaddress_unsafe addr, size_t size) {\n@@ -303,1 +304,1 @@\n-    (void*)addr,             \/\/ BaseAddress\n+    (void*)untype(addr),     \/\/ BaseAddress\n","filename":"src\/hotspot\/os\/windows\/gc\/z\/zMapper_windows.cpp","additions":23,"deletions":22,"binary":false,"changes":45,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2019, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,0 +27,1 @@\n+#include \"gc\/z\/zAddress.hpp\"\n@@ -51,1 +52,1 @@\n-  static uintptr_t reserve(uintptr_t addr, size_t size);\n+  static zaddress_unsafe reserve(zaddress_unsafe addr, size_t size);\n@@ -54,1 +55,1 @@\n-  static void unreserve(uintptr_t addr, size_t size);\n+  static void unreserve(zaddress_unsafe addr, size_t size);\n@@ -66,1 +67,1 @@\n-  static uintptr_t reserve_for_shared_awe(HANDLE awe_section, uintptr_t addr, size_t size);\n+  static zaddress_unsafe reserve_for_shared_awe(HANDLE awe_section, zaddress_unsafe addr, size_t size);\n@@ -69,1 +70,1 @@\n-  static void unreserve_for_shared_awe(uintptr_t addr, size_t size);\n+  static void unreserve_for_shared_awe(zaddress_unsafe addr, size_t size);\n@@ -77,1 +78,1 @@\n-  static void split_placeholder(uintptr_t addr, size_t size);\n+  static void split_placeholder(zaddress_unsafe addr, size_t size);\n@@ -83,1 +84,1 @@\n-  static void coalesce_placeholders(uintptr_t addr, size_t size);\n+  static void coalesce_placeholders(zaddress_unsafe addr, size_t size);\n@@ -87,1 +88,1 @@\n-  static void map_view_replace_placeholder(HANDLE file_handle, uintptr_t file_offset, uintptr_t addr, size_t size);\n+  static void map_view_replace_placeholder(HANDLE file_handle, uintptr_t file_offset, zaddress_unsafe addr, size_t size);\n@@ -91,1 +92,1 @@\n-  static void unmap_view_preserve_placeholder(uintptr_t addr, size_t size);\n+  static void unmap_view_preserve_placeholder(zaddress_unsafe addr, size_t size);\n","filename":"src\/hotspot\/os\/windows\/gc\/z\/zMapper_windows.hpp","additions":10,"deletions":9,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -25,0 +25,1 @@\n+#include \"gc\/z\/zAddress.inline.hpp\"\n@@ -36,4 +37,4 @@\n-  virtual size_t commit(size_t offset, size_t size) = 0;\n-  virtual size_t uncommit(size_t offset, size_t size) = 0;\n-  virtual void map(uintptr_t addr, size_t size, size_t offset) const = 0;\n-  virtual void unmap(uintptr_t addr, size_t size) const = 0;\n+  virtual size_t commit(zoffset offset, size_t size) = 0;\n+  virtual size_t uncommit(zoffset offset, size_t size) = 0;\n+  virtual void map(zaddress_unsafe addr, size_t size, zoffset offset) const = 0;\n+  virtual void unmap(zaddress_unsafe addr, size_t size) const = 0;\n@@ -53,1 +54,1 @@\n-  HANDLE get_handle(uintptr_t offset) const {\n+  HANDLE get_handle(zoffset offset) const {\n@@ -59,1 +60,1 @@\n-  void put_handle(uintptr_t offset, HANDLE handle) {\n+  void put_handle(zoffset offset, HANDLE handle) {\n@@ -65,1 +66,1 @@\n-  void clear_handle(uintptr_t offset) {\n+  void clear_handle(zoffset offset) {\n@@ -75,1 +76,1 @@\n-  size_t commit(size_t offset, size_t size) {\n+  size_t commit(zoffset offset, size_t size) {\n@@ -88,1 +89,1 @@\n-  size_t uncommit(size_t offset, size_t size) {\n+  size_t uncommit(zoffset offset, size_t size) {\n@@ -98,3 +99,3 @@\n-  void map(uintptr_t addr, size_t size, size_t offset) const {\n-    assert(is_aligned(offset, ZGranuleSize), \"Misaligned\");\n-    assert(is_aligned(addr, ZGranuleSize), \"Misaligned\");\n+  void map(zaddress_unsafe addr, size_t size, zoffset offset) const {\n+    assert(is_aligned(untype(offset), ZGranuleSize), \"Misaligned\");\n+    assert(is_aligned(untype(addr), ZGranuleSize), \"Misaligned\");\n@@ -109,2 +110,2 @@\n-  void unmap(uintptr_t addr, size_t size) const {\n-    assert(is_aligned(addr, ZGranuleSize), \"Misaligned\");\n+  void unmap(zaddress_unsafe addr, size_t size) const {\n+    assert(is_aligned(untype(addr), ZGranuleSize), \"Misaligned\");\n@@ -114,1 +115,1 @@\n-      ZMapper::unmap_view_preserve_placeholder(addr + i, ZGranuleSize);\n+      ZMapper::unmap_view_preserve_placeholder(to_zaddress_unsafe(untype(addr) + i), ZGranuleSize);\n@@ -152,2 +153,2 @@\n-  size_t commit(size_t offset, size_t size) {\n-    const size_t index = offset >> ZGranuleSizeShift;\n+  size_t commit(zoffset offset, size_t size) {\n+    const size_t index = untype(offset) >> ZGranuleSizeShift;\n@@ -160,1 +161,1 @@\n-            size \/ M, offset, GetLastError());\n+            size \/ M, untype(offset), GetLastError());\n@@ -162,1 +163,1 @@\n-      log_debug(gc)(\"Allocated physical memory: \" SIZE_FORMAT \"M @ \" PTR_FORMAT, size \/ M, offset);\n+      log_debug(gc)(\"Allocated physical memory: \" SIZE_FORMAT \"M @ \" PTR_FORMAT, size \/ M, untype(offset));\n@@ -170,2 +171,2 @@\n-  size_t uncommit(size_t offset, size_t size) {\n-    const size_t index = offset >> ZGranuleSizeShift;\n+  size_t uncommit(zoffset offset, size_t size) {\n+    const size_t index = untype(offset) >> ZGranuleSizeShift;\n@@ -178,1 +179,1 @@\n-            size, offset, GetLastError());\n+            size, untype(offset), GetLastError());\n@@ -184,1 +185,1 @@\n-  void map(uintptr_t addr, size_t size, size_t offset) const {\n+  void map(zaddress_unsafe addr, size_t size, zoffset offset) const {\n@@ -186,1 +187,1 @@\n-    const size_t index = offset >> ZGranuleSizeShift;\n+    const size_t index = untype(offset) >> ZGranuleSizeShift;\n@@ -188,1 +189,1 @@\n-    const bool res = MapUserPhysicalPages((char*)addr, npages, &_page_array[index]);\n+    const bool res = MapUserPhysicalPages((char*)untype(addr), npages, &_page_array[index]);\n@@ -191,1 +192,1 @@\n-            addr, size \/ M, offset, GetLastError());\n+            untype(addr), size \/ M, untype(offset), GetLastError());\n@@ -195,1 +196,1 @@\n-  void unmap(uintptr_t addr, size_t size) const {\n+  void unmap(zaddress_unsafe addr, size_t size) const {\n@@ -198,1 +199,1 @@\n-    const bool res = MapUserPhysicalPages((char*)addr, npages, nullptr);\n+    const bool res = MapUserPhysicalPages((char*)untype(addr), npages, nullptr);\n@@ -225,1 +226,1 @@\n-size_t ZPhysicalMemoryBacking::commit(size_t offset, size_t length) {\n+size_t ZPhysicalMemoryBacking::commit(zoffset offset, size_t length) {\n@@ -227,1 +228,1 @@\n-                      offset \/ M, (offset + length) \/ M, length \/ M);\n+                      untype(offset) \/ M, (untype(offset) + length) \/ M, length \/ M);\n@@ -232,1 +233,1 @@\n-size_t ZPhysicalMemoryBacking::uncommit(size_t offset, size_t length) {\n+size_t ZPhysicalMemoryBacking::uncommit(zoffset offset, size_t length) {\n@@ -234,1 +235,1 @@\n-                      offset \/ M, (offset + length) \/ M, length \/ M);\n+                      untype(offset) \/ M, (untype(offset) + length) \/ M, length \/ M);\n@@ -239,3 +240,3 @@\n-void ZPhysicalMemoryBacking::map(uintptr_t addr, size_t size, size_t offset) const {\n-  assert(is_aligned(offset, ZGranuleSize), \"Misaligned: \" PTR_FORMAT, offset);\n-  assert(is_aligned(addr, ZGranuleSize), \"Misaligned: \" PTR_FORMAT, addr);\n+void ZPhysicalMemoryBacking::map(zaddress_unsafe addr, size_t size, zoffset offset) const {\n+  assert(is_aligned(untype(offset), ZGranuleSize), \"Misaligned: \" PTR_FORMAT, untype(offset));\n+  assert(is_aligned(untype(addr), ZGranuleSize), \"Misaligned: \" PTR_FORMAT, addr);\n@@ -247,2 +248,2 @@\n-void ZPhysicalMemoryBacking::unmap(uintptr_t addr, size_t size) const {\n-  assert(is_aligned(addr, ZGranuleSize), \"Misaligned\");\n+void ZPhysicalMemoryBacking::unmap(zaddress_unsafe addr, size_t size) const {\n+  assert(is_aligned(untype(addr), ZGranuleSize), \"Misaligned\");\n","filename":"src\/hotspot\/os\/windows\/gc\/z\/zPhysicalMemoryBacking_windows.cpp","additions":38,"deletions":37,"binary":false,"changes":75,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2019, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2019, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,0 +27,1 @@\n+#include \"gc\/z\/zAddress.hpp\"\n@@ -44,2 +45,2 @@\n-  size_t commit(size_t offset, size_t length);\n-  size_t uncommit(size_t offset, size_t length);\n+  size_t commit(zoffset offset, size_t length);\n+  size_t uncommit(zoffset offset, size_t length);\n@@ -47,2 +48,2 @@\n-  void map(uintptr_t addr, size_t size, size_t offset) const;\n-  void unmap(uintptr_t addr, size_t size) const;\n+  void map(zaddress_unsafe addr, size_t size, zoffset offset) const;\n+  void unmap(zaddress_unsafe addr, size_t size) const;\n","filename":"src\/hotspot\/os\/windows\/gc\/z\/zPhysicalMemoryBacking_windows.hpp","additions":6,"deletions":5,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -38,2 +38,2 @@\n-  virtual bool reserve(uintptr_t addr, size_t size) = 0;\n-  virtual void unreserve(uintptr_t addr, size_t size) = 0;\n+  virtual bool reserve(zaddress_unsafe addr, size_t size) = 0;\n+  virtual void unreserve(zaddress_unsafe addr, size_t size) = 0;\n@@ -47,4 +47,2 @@\n-    static void split_placeholder(uintptr_t start, size_t size) {\n-      ZMapper::split_placeholder(ZAddress::marked0(start), size);\n-      ZMapper::split_placeholder(ZAddress::marked1(start), size);\n-      ZMapper::split_placeholder(ZAddress::remapped(start), size);\n+    static void split_placeholder(zoffset start, size_t size) {\n+      ZMapper::split_placeholder(ZOffset::address_unsafe(start), size);\n@@ -53,4 +51,2 @@\n-    static void coalesce_placeholders(uintptr_t start, size_t size) {\n-      ZMapper::coalesce_placeholders(ZAddress::marked0(start), size);\n-      ZMapper::coalesce_placeholders(ZAddress::marked1(start), size);\n-      ZMapper::coalesce_placeholders(ZAddress::remapped(start), size);\n+    static void coalesce_placeholders(zoffset start, size_t size) {\n+      ZMapper::coalesce_placeholders(ZOffset::address_unsafe(start), size);\n@@ -59,3 +55,3 @@\n-    static void split_into_placeholder_granules(uintptr_t start, size_t size) {\n-      for (uintptr_t addr = start; addr < start + size; addr += ZGranuleSize) {\n-        split_placeholder(addr, ZGranuleSize);\n+    static void split_into_placeholder_granules(zoffset start, size_t size) {\n+      for (uintptr_t addr = untype(start); addr < untype(start) + size; addr += ZGranuleSize) {\n+        split_placeholder(to_zoffset(addr), ZGranuleSize);\n@@ -65,1 +61,1 @@\n-    static void coalesce_into_one_placeholder(uintptr_t start, size_t size) {\n+    static void coalesce_into_one_placeholder(zoffset start, size_t size) {\n@@ -92,1 +88,1 @@\n-      split_into_placeholder_granules(area->end() - size, size - ZGranuleSize);\n+      split_into_placeholder_granules(to_zoffset(untype(area->end()) - size), size - ZGranuleSize);\n@@ -97,1 +93,1 @@\n-      coalesce_into_one_placeholder(area->start() - size, area->size() + size);\n+      coalesce_into_one_placeholder(to_zoffset(untype(area->start()) - size), area->size() + size);\n@@ -139,2 +135,2 @@\n-  virtual bool reserve(uintptr_t addr, size_t size) {\n-    const uintptr_t res = ZMapper::reserve(addr, size);\n+  virtual bool reserve(zaddress_unsafe addr, size_t size) {\n+    const zaddress_unsafe res = ZMapper::reserve(addr, size);\n@@ -142,1 +138,1 @@\n-    assert(res == addr || res == 0, \"Should not reserve other memory than requested\");\n+    assert(res == addr || untype(res) == 0, \"Should not reserve other memory than requested\");\n@@ -146,1 +142,1 @@\n-  virtual void unreserve(uintptr_t addr, size_t size) {\n+  virtual void unreserve(zaddress_unsafe addr, size_t size) {\n@@ -162,2 +158,2 @@\n-  virtual bool reserve(uintptr_t addr, size_t size) {\n-    const uintptr_t res = ZMapper::reserve_for_shared_awe(ZAWESection, addr, size);\n+  virtual bool reserve(zaddress_unsafe addr, size_t size) {\n+    const zaddress_unsafe res = ZMapper::reserve_for_shared_awe(ZAWESection, addr, size);\n@@ -165,1 +161,1 @@\n-    assert(res == addr || res == 0, \"Should not reserve other memory than requested\");\n+    assert(res == addr || untype(res) == 0, \"Should not reserve other memory than requested\");\n@@ -169,1 +165,1 @@\n-  virtual void unreserve(uintptr_t addr, size_t size) {\n+  virtual void unreserve(zaddress_unsafe addr, size_t size) {\n@@ -189,1 +185,1 @@\n-bool ZVirtualMemoryManager::pd_reserve(uintptr_t addr, size_t size) {\n+bool ZVirtualMemoryManager::pd_reserve(zaddress_unsafe addr, size_t size) {\n@@ -193,1 +189,1 @@\n-void ZVirtualMemoryManager::pd_unreserve(uintptr_t addr, size_t size) {\n+void ZVirtualMemoryManager::pd_unreserve(zaddress_unsafe addr, size_t size) {\n","filename":"src\/hotspot\/os\/windows\/gc\/z\/zVirtualMemory_windows.cpp","additions":22,"deletions":26,"binary":false,"changes":48,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -40,3 +40,7 @@\n-    D res = __atomic_add_fetch(dest, add_value, __ATOMIC_RELEASE);\n-    FULL_MEM_BARRIER;\n-    return res;\n+    if (order == memory_order_relaxed) {\n+      return __atomic_add_fetch(dest, add_value, __ATOMIC_RELAXED);\n+    } else {\n+      D res = __atomic_add_fetch(dest, add_value, __ATOMIC_RELEASE);\n+      FULL_MEM_BARRIER;\n+      return res;\n+    }\n","filename":"src\/hotspot\/os_cpu\/bsd_aarch64\/atomic_bsd_aarch64.hpp","additions":8,"deletions":4,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -0,0 +1,40 @@\n+\/*\n+ * Copyright (c) 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef OS_CPU_LINUX_AARCH64_GC_X_XSYSCALL_LINUX_AARCH64_HPP\n+#define OS_CPU_LINUX_AARCH64_GC_X_XSYSCALL_LINUX_AARCH64_HPP\n+\n+#include <sys\/syscall.h>\n+\n+\/\/\n+\/\/ Support for building on older Linux systems\n+\/\/\n+\n+#ifndef SYS_memfd_create\n+#define SYS_memfd_create     279\n+#endif\n+#ifndef SYS_fallocate\n+#define SYS_fallocate        47\n+#endif\n+\n+#endif \/\/ OS_CPU_LINUX_AARCH64_GC_X_XSYSCALL_LINUX_AARCH64_HPP\n","filename":"src\/hotspot\/os_cpu\/linux_aarch64\/gc\/x\/xSyscall_linux_aarch64.hpp","additions":40,"deletions":0,"binary":false,"changes":40,"status":"added"},{"patch":"@@ -0,0 +1,42 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2021 SAP SE. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef OS_CPU_LINUX_PPC_GC_X_XSYSCALL_LINUX_PPC_HPP\n+#define OS_CPU_LINUX_PPC_GC_X_XSYSCALL_LINUX_PPC_HPP\n+\n+#include <sys\/syscall.h>\n+\n+\/\/\n+\/\/ Support for building on older Linux systems\n+\/\/\n+\n+\n+#ifndef SYS_memfd_create\n+#define SYS_memfd_create     360\n+#endif\n+#ifndef SYS_fallocate\n+#define SYS_fallocate        309\n+#endif\n+\n+#endif \/\/ OS_CPU_LINUX_PPC_GC_X_XSYSCALL_LINUX_PPC_HPP\n","filename":"src\/hotspot\/os_cpu\/linux_ppc\/gc\/x\/xSyscall_linux_ppc.hpp","additions":42,"deletions":0,"binary":false,"changes":42,"status":"added"},{"patch":"@@ -0,0 +1,42 @@\n+\/*\n+ * Copyright (c) 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2021, Huawei Technologies Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef OS_CPU_LINUX_RISCV_GC_X_XSYSCALL_LINUX_RISCV_HPP\n+#define OS_CPU_LINUX_RISCV_GC_X_XSYSCALL_LINUX_RISCV_HPP\n+\n+#include <sys\/syscall.h>\n+\n+\/\/\n+\/\/ Support for building on older Linux systems\n+\/\/\n+\n+#ifndef SYS_memfd_create\n+#define SYS_memfd_create     279\n+#endif\n+#ifndef SYS_fallocate\n+#define SYS_fallocate        47\n+#endif\n+\n+#endif \/\/ OS_CPU_LINUX_RISCV_GC_X_XSYSCALL_LINUX_RISCV_HPP\n","filename":"src\/hotspot\/os_cpu\/linux_riscv\/gc\/x\/xSyscall_linux_riscv.hpp","additions":42,"deletions":0,"binary":false,"changes":42,"status":"added"},{"patch":"@@ -0,0 +1,40 @@\n+\/*\n+ * Copyright (c) 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef OS_CPU_LINUX_X86_GC_X_XSYSCALL_LINUX_X86_HPP\n+#define OS_CPU_LINUX_X86_GC_X_XSYSCALL_LINUX_X86_HPP\n+\n+#include <sys\/syscall.h>\n+\n+\/\/\n+\/\/ Support for building on older Linux systems\n+\/\/\n+\n+#ifndef SYS_memfd_create\n+#define SYS_memfd_create     319\n+#endif\n+#ifndef SYS_fallocate\n+#define SYS_fallocate        285\n+#endif\n+\n+#endif \/\/ OS_CPU_LINUX_X86_GC_X_XSYSCALL_LINUX_X86_HPP\n","filename":"src\/hotspot\/os_cpu\/linux_x86\/gc\/x\/xSyscall_linux_x86.hpp","additions":40,"deletions":0,"binary":false,"changes":40,"status":"added"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -245,0 +245,1 @@\n+ public:\n@@ -257,0 +258,2 @@\n+\n+ protected:\n","filename":"src\/hotspot\/share\/asm\/assembler.hpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -326,1 +326,2 @@\n-           rtype == relocInfo::external_word_type,\n+           rtype == relocInfo::external_word_type||\n+           rtype == relocInfo::barrier_type,\n","filename":"src\/hotspot\/share\/asm\/codeBuffer.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -183,0 +183,1 @@\n+  void        clear_scratch_emit()  { _scratch_emit = false; }\n","filename":"src\/hotspot\/share\/asm\/codeBuffer.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -188,9 +188,0 @@\n-  switch (code()) {\n-    case lir_xchg:\n-      break;\n-\n-    default:\n-      assert(!result_opr()->is_register() || !result_opr()->is_oop_register(),\n-             \"can't produce oops from arith\");\n-  }\n-\n@@ -1146,0 +1137,6 @@\n+    case lir_cas_long:\n+    case lir_cas_obj:\n+    case lir_cas_int:\n+      _cmp_opr1 = op->as_OpCompareAndSwap()->result_opr();\n+      _cmp_opr2 = LIR_OprFact::intConst(0);\n+      break;\n@@ -1147,1 +1144,1 @@\n-    case lir_zloadbarrier_test:\n+    case lir_xloadbarrier_test:\n","filename":"src\/hotspot\/share\/c1\/c1_LIR.cpp","additions":7,"deletions":10,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -1021,3 +1021,3 @@\n-  , begin_opZLoadBarrierTest\n-    , lir_zloadbarrier_test\n-  , end_opZLoadBarrierTest\n+  , begin_opXLoadBarrierTest\n+    , lir_xloadbarrier_test\n+  , end_opXLoadBarrierTest\n","filename":"src\/hotspot\/share\/c1\/c1_LIR.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -94,0 +94,1 @@\n+ public:\n@@ -107,0 +108,1 @@\n+ private:\n","filename":"src\/hotspot\/share\/c1\/c1_LIRAssembler.hpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -163,1 +163,1 @@\n-  void set_next(ClassLoaderData* next) { _next = next; }\n+  void set_next(ClassLoaderData* next) { Atomic::store(&_next, next); }\n","filename":"src\/hotspot\/share\/classfile\/classLoaderData.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -264,0 +264,7 @@\n+inline void assert_is_safepoint_or_gc() {\n+  assert(SafepointSynchronize::is_at_safepoint() ||\n+         Thread::current()->is_ConcurrentGC_thread() ||\n+         Thread::current()->is_Worker_thread(),\n+         \"Must be called by safepoint or GC\");\n+}\n+\n@@ -265,1 +272,1 @@\n-  assert_locked_or_safepoint_weak(ClassLoaderDataGraph_lock);\n+  assert_is_safepoint_or_gc();\n@@ -275,2 +282,2 @@\n-  assert_locked_or_safepoint_weak(ClassLoaderDataGraph_lock);\n-  for (ClassLoaderData* cld = _head;  cld != nullptr; cld = cld->_next) {\n+  assert_is_safepoint_or_gc();\n+  for (ClassLoaderData* cld = Atomic::load_acquire(&_head);  cld != nullptr; cld = cld->next()) {\n@@ -282,2 +289,2 @@\n-  assert_locked_or_safepoint_weak(ClassLoaderDataGraph_lock);\n-  for (ClassLoaderData* cld = _head;  cld != nullptr; cld = cld->_next) {\n+  assert_is_safepoint_or_gc();\n+  for (ClassLoaderData* cld = Atomic::load_acquire(&_head);  cld != nullptr; cld = cld->next()) {\n@@ -292,1 +299,1 @@\n-  assert_locked_or_safepoint_weak(ClassLoaderDataGraph_lock);\n+  assert_is_safepoint_or_gc();\n@@ -523,1 +530,2 @@\n-      _head = data;\n+      \/\/ The GC might be walking this concurrently\n+      Atomic::store(&_head, data);\n","filename":"src\/hotspot\/share\/classfile\/classLoaderDataGraph.cpp","additions":15,"deletions":7,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -278,0 +278,1 @@\n+    barrier_type            = 18, \/\/ GC barrier data\n@@ -319,0 +320,1 @@\n+    visitor(barrier) \\\n@@ -832,1 +834,0 @@\n-  int      format()       const { return binding()->format(); }\n@@ -844,0 +845,2 @@\n+  int      format()       const { return binding()->format(); }\n+\n@@ -1081,0 +1084,25 @@\n+class barrier_Relocation : public DataRelocation {\n+\n+ public:\n+  \/\/ The uninitialized value used before the relocation has been patched.\n+  \/\/ Code assumes that the unpatched value is zero.\n+  static const int16_t unpatched = 0;\n+\n+  static RelocationHolder spec() {\n+    return RelocationHolder::construct<barrier_Relocation>();\n+  }\n+\n+  void copy_into(RelocationHolder& holder) const override;\n+\n+ private:\n+  friend class RelocIterator;\n+  friend class RelocationHolder;\n+  barrier_Relocation() : DataRelocation(relocInfo::barrier_type) { }\n+\n+ public:\n+  int offset() override                  { ShouldNotReachHere(); return 0; }\n+  address value() override               { ShouldNotReachHere(); return nullptr; }\n+  void set_value(address value) override { ShouldNotReachHere(); }\n+};\n+\n+\n","filename":"src\/hotspot\/share\/code\/relocInfo.hpp","additions":29,"deletions":1,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -36,0 +36,1 @@\n+  ZGC_ONLY(f(XBarrierSet))                           \\\n","filename":"src\/hotspot\/share\/gc\/shared\/barrierSetConfig.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -43,0 +43,1 @@\n+#include \"gc\/x\/xBarrierSet.inline.hpp\"\n","filename":"src\/hotspot\/share\/gc\/shared\/barrierSetConfig.inline.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -98,0 +98,1 @@\n+  friend class DisableIsGCActiveMark; \/\/ Disable current IsGCActiveMark\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -47,1 +47,1 @@\n-#include \"gc\/z\/zArguments.hpp\"\n+#include \"gc\/z\/shared\/zSharedArguments.hpp\"\n@@ -65,1 +65,1 @@\n-         ZGC_ONLY(static ZArguments          zArguments;)\n+         ZGC_ONLY(static ZSharedArguments    zArguments;)\n","filename":"src\/hotspot\/share\/gc\/shared\/gcConfig.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2012, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2012, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -26,0 +26,1 @@\n+#include \"gc\/shared\/gc_globals.hpp\"\n@@ -44,1 +45,9 @@\n-  if (UseZGC || UseShenandoahGC) {\n+  if (UseZGC) {\n+    if (ZGenerational) {\n+      return ZMinor;\n+    } else {\n+      return NA;\n+    }\n+  }\n+\n+  if (UseShenandoahGC) {\n@@ -61,1 +70,5 @@\n-    return Z;\n+    if (ZGenerational) {\n+      return ZMajor;\n+    } else {\n+      return Z;\n+    }\n","filename":"src\/hotspot\/share\/gc\/shared\/gcConfiguration.cpp","additions":16,"deletions":3,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -33,0 +33,12 @@\n+GCIdPrinter GCId::_default_printer;\n+GCIdPrinter* GCId::_printer = &_default_printer;\n+\n+size_t GCIdPrinter::print_gc_id(uint gc_id, char* buf, size_t len) {\n+  int ret = jio_snprintf(buf, len, \"GC(%u) \", gc_id);\n+  assert(ret > 0, \"Failed to print prefix. Log buffer too small?\");\n+  return (size_t)ret;\n+}\n+\n+void GCId::set_printer(GCIdPrinter* printer) {\n+  _printer = printer;\n+}\n@@ -62,3 +74,1 @@\n-      int ret = jio_snprintf(buf, len, \"GC(%u) \", gc_id);\n-      assert(ret > 0, \"Failed to print prefix. Log buffer too small?\");\n-      return (size_t)ret;\n+      return _printer->print_gc_id(gc_id, buf, len);\n@@ -70,2 +80,1 @@\n-GCIdMark::GCIdMark() {\n-  assert(currentNamedthread()->gc_id() == GCId::undefined(), \"nested\");\n+GCIdMark::GCIdMark() : _previous_gc_id(currentNamedthread()->gc_id()) {\n@@ -75,2 +84,1 @@\n-GCIdMark::GCIdMark(uint gc_id) {\n-  assert(currentNamedthread()->gc_id() == GCId::undefined(), \"nested\");\n+GCIdMark::GCIdMark(uint gc_id) : _previous_gc_id(currentNamedthread()->gc_id()) {\n@@ -81,1 +89,1 @@\n-  currentNamedthread()->set_gc_id(GCId::undefined());\n+  currentNamedthread()->set_gc_id(_previous_gc_id);\n","filename":"src\/hotspot\/share\/gc\/shared\/gcId.cpp","additions":16,"deletions":8,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2014, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -30,0 +30,6 @@\n+class GCIdPrinter : public CHeapObj<mtGC> {\n+public:\n+  virtual ~GCIdPrinter() {}\n+  virtual size_t print_gc_id(uint gc_id, char* buf, size_t len);\n+};\n+\n@@ -38,0 +44,4 @@\n+  \/\/ Default printer used unless a custom printer is set\n+  static GCIdPrinter _default_printer;\n+  static GCIdPrinter* _printer;\n+\n@@ -47,0 +57,2 @@\n+  \/\/ Set a custom GCId printer\n+  static void set_printer(GCIdPrinter* printer);\n@@ -50,0 +62,3 @@\n+private:\n+  const uint _previous_gc_id;\n+\n","filename":"src\/hotspot\/share\/gc\/shared\/gcId.hpp","additions":16,"deletions":1,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -38,1 +38,3 @@\n-  Z,\n+  ZMinor,\n+  ZMajor,\n+  Z, \/\/ Support for the legacy, single-gen mode\n@@ -55,0 +57,2 @@\n+      case ZMinor: return \"ZGC Minor\";\n+      case ZMajor: return \"ZGC Major\";\n","filename":"src\/hotspot\/share\/gc\/shared\/gcName.hpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -43,1 +43,1 @@\n-typedef uint64_t GCThreadLocalData[19]; \/\/ 152 bytes\n+typedef uint64_t GCThreadLocalData[43]; \/\/ 344 bytes\n","filename":"src\/hotspot\/share\/gc\/shared\/gcThreadLocalData.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2012, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2012, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -320,1 +320,1 @@\n-    assert(phase->level() < 2, \"There is only two levels for ConcurrentPhase\");\n+    assert(phase->level() < 3, \"There are only three levels for ConcurrentPhase\");\n@@ -325,0 +325,1 @@\n+      case 2: send_phase<EventGCPhaseConcurrentLevel2>(phase); break;\n","filename":"src\/hotspot\/share\/gc\/shared\/gcTraceSend.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -138,0 +138,11 @@\n+bool VM_GC_HeapInspection::doit_prologue() {\n+  if (_full_gc && UseZGC) {\n+    \/\/ ZGC cannot perform a synchronous GC cycle from within the VM thread.\n+    \/\/ So VM_GC_HeapInspection::collect() is a noop. To respect the _full_gc\n+    \/\/ flag a synchronous GC cycle is performed from the caller thread in the\n+    \/\/ prologue.\n+    Universe::heap()->collect(GCCause::_heap_inspection);\n+  }\n+  return VM_GC_Operation::doit_prologue();\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shared\/gcVMOperations.cpp","additions":11,"deletions":0,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -174,0 +174,1 @@\n+  virtual bool doit_prologue();\n","filename":"src\/hotspot\/share\/gc\/shared\/gcVMOperations.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -46,1 +46,1 @@\n-#include \"gc\/z\/z_globals.hpp\"\n+#include \"gc\/z\/shared\/z_shared_globals.hpp\"\n@@ -102,1 +102,1 @@\n-  ZGC_ONLY(GC_Z_FLAGS(                                                      \\\n+  ZGC_ONLY(GC_Z_SHARED_FLAGS(                                               \\\n@@ -128,0 +128,3 @@\n+  product(bool, ZGenerational, false,                                       \\\n+          \"Use the generational version of ZGC\")                            \\\n+                                                                            \\\n","filename":"src\/hotspot\/share\/gc\/shared\/gc_globals.hpp","additions":6,"deletions":3,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2002, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2002, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -45,0 +45,12 @@\n+\n+DisableIsGCActiveMark::DisableIsGCActiveMark() {\n+  CollectedHeap* heap = Universe::heap();\n+  assert(heap->is_gc_active(), \"Not reentrant\");\n+  heap->_is_gc_active = false;\n+}\n+\n+DisableIsGCActiveMark::~DisableIsGCActiveMark() {\n+  CollectedHeap* heap = Universe::heap();\n+  assert(!heap->is_gc_active(), \"Sanity\");\n+  heap->_is_gc_active = true;\n+}\n","filename":"src\/hotspot\/share\/gc\/shared\/isGCActiveMark.cpp","additions":13,"deletions":1,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2002, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2002, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -39,0 +39,6 @@\n+class DisableIsGCActiveMark : public StackObj {\n+ public:\n+  DisableIsGCActiveMark();\n+  ~DisableIsGCActiveMark();\n+};\n+\n","filename":"src\/hotspot\/share\/gc\/shared\/isGCActiveMark.hpp","additions":7,"deletions":1,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -94,1 +94,0 @@\n-  assert(Thread::current()->is_VM_thread(), \"Must be the VM thread\");\n@@ -129,1 +128,0 @@\n-  assert(Thread::current()->is_VM_thread(), \"Must be the VM thread\");\n","filename":"src\/hotspot\/share\/gc\/shared\/suspendibleThreadSet.cpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -53,1 +53,1 @@\n-#include \"gc\/z\/vmStructs_z.hpp\"\n+#include \"gc\/z\/shared\/vmStructs_z_shared.hpp\"\n@@ -75,3 +75,3 @@\n-  ZGC_ONLY(VM_STRUCTS_ZGC(nonstatic_field,                                                                                           \\\n-                          volatile_nonstatic_field,                                                                                  \\\n-                          static_field))                                                                                             \\\n+  ZGC_ONLY(VM_STRUCTS_Z_SHARED(nonstatic_field,                                                                                      \\\n+                               volatile_nonstatic_field,                                                                             \\\n+                               static_field))                                                                                        \\\n@@ -150,3 +150,3 @@\n-  ZGC_ONLY(VM_TYPES_ZGC(declare_type,                                     \\\n-                        declare_toplevel_type,                            \\\n-                        declare_integer_type))                            \\\n+  ZGC_ONLY(VM_TYPES_Z_SHARED(declare_type,                                \\\n+                             declare_toplevel_type,                       \\\n+                             declare_integer_type))                       \\\n@@ -214,2 +214,2 @@\n-  ZGC_ONLY(VM_INT_CONSTANTS_ZGC(declare_constant,                           \\\n-                                declare_constant_with_value))               \\\n+  ZGC_ONLY(VM_INT_CONSTANTS_Z_SHARED(declare_constant,                      \\\n+                                     declare_constant_with_value))          \\\n@@ -247,1 +247,1 @@\n-  ZGC_ONLY(VM_LONG_CONSTANTS_ZGC(declare_constant))\n+  ZGC_ONLY(VM_LONG_CONSTANTS_Z_SHARED(declare_constant))\n","filename":"src\/hotspot\/share\/gc\/shared\/vmStructs_gc.hpp","additions":10,"deletions":10,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -144,0 +144,30 @@\n+void WorkerThreads::set_indirectly_suspendible_threads() {\n+#ifdef ASSERT\n+  class SetIndirectlySuspendibleThreadClosure : public ThreadClosure {\n+    virtual void do_thread(Thread* thread) {\n+      thread->set_indirectly_suspendible_thread();\n+    }\n+  };\n+\n+  if (Thread::current()->is_suspendible_thread()) {\n+    SetIndirectlySuspendibleThreadClosure cl;\n+    threads_do(&cl);\n+  }\n+#endif\n+}\n+\n+void WorkerThreads::clear_indirectly_suspendible_threads() {\n+#ifdef ASSERT\n+  class ClearIndirectlySuspendibleThreadClosure : public ThreadClosure {\n+    virtual void do_thread(Thread* thread) {\n+      thread->clear_indirectly_suspendible_thread();\n+    }\n+  };\n+\n+  if (Thread::current()->is_suspendible_thread()) {\n+    ClearIndirectlySuspendibleThreadClosure cl;\n+    threads_do(&cl);\n+  }\n+#endif\n+}\n+\n@@ -145,0 +175,1 @@\n+  set_indirectly_suspendible_threads();\n@@ -146,0 +177,1 @@\n+  clear_indirectly_suspendible_threads();\n","filename":"src\/hotspot\/share\/gc\/shared\/workerThread.cpp","additions":32,"deletions":0,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2002, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2002, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -96,0 +96,3 @@\n+  void set_indirectly_suspendible_threads();\n+  void clear_indirectly_suspendible_threads();\n+\n","filename":"src\/hotspot\/share\/gc\/shared\/workerThread.hpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -0,0 +1,237 @@\n+\/*\n+ * Copyright (c) 2015, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"c1\/c1_LIR.hpp\"\n+#include \"c1\/c1_LIRGenerator.hpp\"\n+#include \"c1\/c1_CodeStubs.hpp\"\n+#include \"gc\/x\/c1\/xBarrierSetC1.hpp\"\n+#include \"gc\/x\/xBarrierSet.hpp\"\n+#include \"gc\/x\/xBarrierSetAssembler.hpp\"\n+#include \"gc\/x\/xThreadLocalData.hpp\"\n+#include \"utilities\/macros.hpp\"\n+\n+XLoadBarrierStubC1::XLoadBarrierStubC1(LIRAccess& access, LIR_Opr ref, address runtime_stub) :\n+    _decorators(access.decorators()),\n+    _ref_addr(access.resolved_addr()),\n+    _ref(ref),\n+    _tmp(LIR_OprFact::illegalOpr),\n+    _runtime_stub(runtime_stub) {\n+\n+  assert(_ref_addr->is_address(), \"Must be an address\");\n+  assert(_ref->is_register(), \"Must be a register\");\n+\n+  \/\/ Allocate tmp register if needed\n+  if (_ref_addr->as_address_ptr()->index()->is_valid() ||\n+      _ref_addr->as_address_ptr()->disp() != 0) {\n+    \/\/ Has index or displacement, need tmp register to load address into\n+    _tmp = access.gen()->new_pointer_register();\n+  }\n+\n+  FrameMap* f = Compilation::current()->frame_map();\n+  f->update_reserved_argument_area_size(2 * BytesPerWord);\n+}\n+\n+DecoratorSet XLoadBarrierStubC1::decorators() const {\n+  return _decorators;\n+}\n+\n+LIR_Opr XLoadBarrierStubC1::ref() const {\n+  return _ref;\n+}\n+\n+LIR_Opr XLoadBarrierStubC1::ref_addr() const {\n+  return _ref_addr;\n+}\n+\n+LIR_Opr XLoadBarrierStubC1::tmp() const {\n+  return _tmp;\n+}\n+\n+address XLoadBarrierStubC1::runtime_stub() const {\n+  return _runtime_stub;\n+}\n+\n+void XLoadBarrierStubC1::visit(LIR_OpVisitState* visitor) {\n+  visitor->do_slow_case();\n+  visitor->do_input(_ref_addr);\n+  visitor->do_output(_ref);\n+  if (_tmp->is_valid()) {\n+    visitor->do_temp(_tmp);\n+  }\n+}\n+\n+void XLoadBarrierStubC1::emit_code(LIR_Assembler* ce) {\n+  XBarrierSet::assembler()->generate_c1_load_barrier_stub(ce, this);\n+}\n+\n+#ifndef PRODUCT\n+void XLoadBarrierStubC1::print_name(outputStream* out) const {\n+  out->print(\"XLoadBarrierStubC1\");\n+}\n+#endif \/\/ PRODUCT\n+\n+class LIR_OpXLoadBarrierTest : public LIR_Op {\n+private:\n+  LIR_Opr _opr;\n+\n+public:\n+  LIR_OpXLoadBarrierTest(LIR_Opr opr) :\n+      LIR_Op(lir_xloadbarrier_test, LIR_OprFact::illegalOpr, NULL),\n+      _opr(opr) {}\n+\n+  virtual void visit(LIR_OpVisitState* state) {\n+    state->do_input(_opr);\n+  }\n+\n+  virtual void emit_code(LIR_Assembler* ce) {\n+    XBarrierSet::assembler()->generate_c1_load_barrier_test(ce, _opr);\n+  }\n+\n+  virtual void print_instr(outputStream* out) const {\n+    _opr->print(out);\n+    out->print(\" \");\n+  }\n+\n+#ifndef PRODUCT\n+  virtual const char* name() const {\n+    return \"lir_z_load_barrier_test\";\n+  }\n+#endif \/\/ PRODUCT\n+};\n+\n+static bool barrier_needed(LIRAccess& access) {\n+  return XBarrierSet::barrier_needed(access.decorators(), access.type());\n+}\n+\n+XBarrierSetC1::XBarrierSetC1() :\n+    _load_barrier_on_oop_field_preloaded_runtime_stub(NULL),\n+    _load_barrier_on_weak_oop_field_preloaded_runtime_stub(NULL) {}\n+\n+address XBarrierSetC1::load_barrier_on_oop_field_preloaded_runtime_stub(DecoratorSet decorators) const {\n+  assert((decorators & ON_PHANTOM_OOP_REF) == 0, \"Unsupported decorator\");\n+  \/\/assert((decorators & ON_UNKNOWN_OOP_REF) == 0, \"Unsupported decorator\");\n+\n+  if ((decorators & ON_WEAK_OOP_REF) != 0) {\n+    return _load_barrier_on_weak_oop_field_preloaded_runtime_stub;\n+  } else {\n+    return _load_barrier_on_oop_field_preloaded_runtime_stub;\n+  }\n+}\n+\n+#ifdef ASSERT\n+#define __ access.gen()->lir(__FILE__, __LINE__)->\n+#else\n+#define __ access.gen()->lir()->\n+#endif\n+\n+void XBarrierSetC1::load_barrier(LIRAccess& access, LIR_Opr result) const {\n+  \/\/ Fast path\n+  __ append(new LIR_OpXLoadBarrierTest(result));\n+\n+  \/\/ Slow path\n+  const address runtime_stub = load_barrier_on_oop_field_preloaded_runtime_stub(access.decorators());\n+  CodeStub* const stub = new XLoadBarrierStubC1(access, result, runtime_stub);\n+  __ branch(lir_cond_notEqual, stub);\n+  __ branch_destination(stub->continuation());\n+}\n+\n+LIR_Opr XBarrierSetC1::resolve_address(LIRAccess& access, bool resolve_in_register) {\n+  \/\/ We must resolve in register when patching. This is to avoid\n+  \/\/ having a patch area in the load barrier stub, since the call\n+  \/\/ into the runtime to patch will not have the proper oop map.\n+  const bool patch_before_barrier = barrier_needed(access) && (access.decorators() & C1_NEEDS_PATCHING) != 0;\n+  return BarrierSetC1::resolve_address(access, resolve_in_register || patch_before_barrier);\n+}\n+\n+#undef __\n+\n+void XBarrierSetC1::load_at_resolved(LIRAccess& access, LIR_Opr result) {\n+  BarrierSetC1::load_at_resolved(access, result);\n+\n+  if (barrier_needed(access)) {\n+    load_barrier(access, result);\n+  }\n+}\n+\n+static void pre_load_barrier(LIRAccess& access) {\n+  DecoratorSet decorators = access.decorators();\n+\n+  \/\/ Downgrade access to MO_UNORDERED\n+  decorators = (decorators & ~MO_DECORATOR_MASK) | MO_UNORDERED;\n+\n+  \/\/ Remove ACCESS_WRITE\n+  decorators = (decorators & ~ACCESS_WRITE);\n+\n+  \/\/ Generate synthetic load at\n+  access.gen()->access_load_at(decorators,\n+                               access.type(),\n+                               access.base().item(),\n+                               access.offset().opr(),\n+                               access.gen()->new_register(access.type()),\n+                               NULL \/* patch_emit_info *\/,\n+                               NULL \/* load_emit_info *\/);\n+}\n+\n+LIR_Opr XBarrierSetC1::atomic_xchg_at_resolved(LIRAccess& access, LIRItem& value) {\n+  if (barrier_needed(access)) {\n+    pre_load_barrier(access);\n+  }\n+\n+  return BarrierSetC1::atomic_xchg_at_resolved(access, value);\n+}\n+\n+LIR_Opr XBarrierSetC1::atomic_cmpxchg_at_resolved(LIRAccess& access, LIRItem& cmp_value, LIRItem& new_value) {\n+  if (barrier_needed(access)) {\n+    pre_load_barrier(access);\n+  }\n+\n+  return BarrierSetC1::atomic_cmpxchg_at_resolved(access, cmp_value, new_value);\n+}\n+\n+class XLoadBarrierRuntimeStubCodeGenClosure : public StubAssemblerCodeGenClosure {\n+private:\n+  const DecoratorSet _decorators;\n+\n+public:\n+  XLoadBarrierRuntimeStubCodeGenClosure(DecoratorSet decorators) :\n+      _decorators(decorators) {}\n+\n+  virtual OopMapSet* generate_code(StubAssembler* sasm) {\n+    XBarrierSet::assembler()->generate_c1_load_barrier_runtime_stub(sasm, _decorators);\n+    return NULL;\n+  }\n+};\n+\n+static address generate_c1_runtime_stub(BufferBlob* blob, DecoratorSet decorators, const char* name) {\n+  XLoadBarrierRuntimeStubCodeGenClosure cl(decorators);\n+  CodeBlob* const code_blob = Runtime1::generate_blob(blob, -1 \/* stub_id *\/, name, false \/* expect_oop_map*\/, &cl);\n+  return code_blob->code_begin();\n+}\n+\n+void XBarrierSetC1::generate_c1_runtime_stubs(BufferBlob* blob) {\n+  _load_barrier_on_oop_field_preloaded_runtime_stub =\n+    generate_c1_runtime_stub(blob, ON_STRONG_OOP_REF, \"load_barrier_on_oop_field_preloaded_runtime_stub\");\n+  _load_barrier_on_weak_oop_field_preloaded_runtime_stub =\n+    generate_c1_runtime_stub(blob, ON_WEAK_OOP_REF, \"load_barrier_on_weak_oop_field_preloaded_runtime_stub\");\n+}\n","filename":"src\/hotspot\/share\/gc\/x\/c1\/xBarrierSetC1.cpp","additions":237,"deletions":0,"binary":false,"changes":237,"status":"added"},{"patch":"@@ -0,0 +1,78 @@\n+\/*\n+ * Copyright (c) 2015, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_C1_XBARRIERSETC1_HPP\n+#define SHARE_GC_X_C1_XBARRIERSETC1_HPP\n+\n+#include \"c1\/c1_CodeStubs.hpp\"\n+#include \"c1\/c1_IR.hpp\"\n+#include \"c1\/c1_LIR.hpp\"\n+#include \"gc\/shared\/c1\/barrierSetC1.hpp\"\n+#include \"oops\/accessDecorators.hpp\"\n+\n+class XLoadBarrierStubC1 : public CodeStub {\n+private:\n+  DecoratorSet _decorators;\n+  LIR_Opr      _ref_addr;\n+  LIR_Opr      _ref;\n+  LIR_Opr      _tmp;\n+  address      _runtime_stub;\n+\n+public:\n+  XLoadBarrierStubC1(LIRAccess& access, LIR_Opr ref, address runtime_stub);\n+\n+  DecoratorSet decorators() const;\n+  LIR_Opr ref() const;\n+  LIR_Opr ref_addr() const;\n+  LIR_Opr tmp() const;\n+  address runtime_stub() const;\n+\n+  virtual void emit_code(LIR_Assembler* ce);\n+  virtual void visit(LIR_OpVisitState* visitor);\n+\n+#ifndef PRODUCT\n+  virtual void print_name(outputStream* out) const;\n+#endif \/\/ PRODUCT\n+};\n+\n+class XBarrierSetC1 : public BarrierSetC1 {\n+private:\n+  address _load_barrier_on_oop_field_preloaded_runtime_stub;\n+  address _load_barrier_on_weak_oop_field_preloaded_runtime_stub;\n+\n+  address load_barrier_on_oop_field_preloaded_runtime_stub(DecoratorSet decorators) const;\n+  void load_barrier(LIRAccess& access, LIR_Opr result) const;\n+\n+protected:\n+  virtual LIR_Opr resolve_address(LIRAccess& access, bool resolve_in_register);\n+  virtual void load_at_resolved(LIRAccess& access, LIR_Opr result);\n+  virtual LIR_Opr atomic_xchg_at_resolved(LIRAccess& access, LIRItem& value);\n+  virtual LIR_Opr atomic_cmpxchg_at_resolved(LIRAccess& access, LIRItem& cmp_value, LIRItem& new_value);\n+\n+public:\n+  XBarrierSetC1();\n+\n+  virtual void generate_c1_runtime_stubs(BufferBlob* blob);\n+};\n+\n+#endif \/\/ SHARE_GC_X_C1_XBARRIERSETC1_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/c1\/xBarrierSetC1.hpp","additions":78,"deletions":0,"binary":false,"changes":78,"status":"added"},{"patch":"@@ -0,0 +1,583 @@\n+\/*\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"classfile\/javaClasses.hpp\"\n+#include \"gc\/x\/c2\/xBarrierSetC2.hpp\"\n+#include \"gc\/x\/xBarrierSet.hpp\"\n+#include \"gc\/x\/xBarrierSetAssembler.hpp\"\n+#include \"gc\/x\/xBarrierSetRuntime.hpp\"\n+#include \"opto\/arraycopynode.hpp\"\n+#include \"opto\/addnode.hpp\"\n+#include \"opto\/block.hpp\"\n+#include \"opto\/compile.hpp\"\n+#include \"opto\/graphKit.hpp\"\n+#include \"opto\/machnode.hpp\"\n+#include \"opto\/macro.hpp\"\n+#include \"opto\/memnode.hpp\"\n+#include \"opto\/node.hpp\"\n+#include \"opto\/output.hpp\"\n+#include \"opto\/regalloc.hpp\"\n+#include \"opto\/rootnode.hpp\"\n+#include \"opto\/runtime.hpp\"\n+#include \"opto\/type.hpp\"\n+#include \"utilities\/growableArray.hpp\"\n+#include \"utilities\/macros.hpp\"\n+\n+class XBarrierSetC2State : public ArenaObj {\n+private:\n+  GrowableArray<XLoadBarrierStubC2*>* _stubs;\n+  Node_Array                          _live;\n+\n+public:\n+  XBarrierSetC2State(Arena* arena) :\n+    _stubs(new (arena) GrowableArray<XLoadBarrierStubC2*>(arena, 8,  0, NULL)),\n+    _live(arena) {}\n+\n+  GrowableArray<XLoadBarrierStubC2*>* stubs() {\n+    return _stubs;\n+  }\n+\n+  RegMask* live(const Node* node) {\n+    if (!node->is_Mach()) {\n+      \/\/ Don't need liveness for non-MachNodes\n+      return NULL;\n+    }\n+\n+    const MachNode* const mach = node->as_Mach();\n+    if (mach->barrier_data() == XLoadBarrierElided) {\n+      \/\/ Don't need liveness data for nodes without barriers\n+      return NULL;\n+    }\n+\n+    RegMask* live = (RegMask*)_live[node->_idx];\n+    if (live == NULL) {\n+      live = new (Compile::current()->comp_arena()->AmallocWords(sizeof(RegMask))) RegMask();\n+      _live.map(node->_idx, (Node*)live);\n+    }\n+\n+    return live;\n+  }\n+};\n+\n+static XBarrierSetC2State* barrier_set_state() {\n+  return reinterpret_cast<XBarrierSetC2State*>(Compile::current()->barrier_set_state());\n+}\n+\n+XLoadBarrierStubC2* XLoadBarrierStubC2::create(const MachNode* node, Address ref_addr, Register ref, Register tmp, uint8_t barrier_data) {\n+  XLoadBarrierStubC2* const stub = new (Compile::current()->comp_arena()) XLoadBarrierStubC2(node, ref_addr, ref, tmp, barrier_data);\n+  if (!Compile::current()->output()->in_scratch_emit_size()) {\n+    barrier_set_state()->stubs()->append(stub);\n+  }\n+\n+  return stub;\n+}\n+\n+XLoadBarrierStubC2::XLoadBarrierStubC2(const MachNode* node, Address ref_addr, Register ref, Register tmp, uint8_t barrier_data) :\n+    _node(node),\n+    _ref_addr(ref_addr),\n+    _ref(ref),\n+    _tmp(tmp),\n+    _barrier_data(barrier_data),\n+    _entry(),\n+    _continuation() {\n+  assert_different_registers(ref, ref_addr.base());\n+  assert_different_registers(ref, ref_addr.index());\n+}\n+\n+Address XLoadBarrierStubC2::ref_addr() const {\n+  return _ref_addr;\n+}\n+\n+Register XLoadBarrierStubC2::ref() const {\n+  return _ref;\n+}\n+\n+Register XLoadBarrierStubC2::tmp() const {\n+  return _tmp;\n+}\n+\n+address XLoadBarrierStubC2::slow_path() const {\n+  DecoratorSet decorators = DECORATORS_NONE;\n+  if (_barrier_data & XLoadBarrierStrong) {\n+    decorators |= ON_STRONG_OOP_REF;\n+  }\n+  if (_barrier_data & XLoadBarrierWeak) {\n+    decorators |= ON_WEAK_OOP_REF;\n+  }\n+  if (_barrier_data & XLoadBarrierPhantom) {\n+    decorators |= ON_PHANTOM_OOP_REF;\n+  }\n+  if (_barrier_data & XLoadBarrierNoKeepalive) {\n+    decorators |= AS_NO_KEEPALIVE;\n+  }\n+  return XBarrierSetRuntime::load_barrier_on_oop_field_preloaded_addr(decorators);\n+}\n+\n+RegMask& XLoadBarrierStubC2::live() const {\n+  RegMask* mask = barrier_set_state()->live(_node);\n+  assert(mask != NULL, \"must be mach-node with barrier\");\n+  return *mask;\n+}\n+\n+Label* XLoadBarrierStubC2::entry() {\n+  \/\/ The _entry will never be bound when in_scratch_emit_size() is true.\n+  \/\/ However, we still need to return a label that is not bound now, but\n+  \/\/ will eventually be bound. Any label will do, as it will only act as\n+  \/\/ a placeholder, so we return the _continuation label.\n+  return Compile::current()->output()->in_scratch_emit_size() ? &_continuation : &_entry;\n+}\n+\n+Label* XLoadBarrierStubC2::continuation() {\n+  return &_continuation;\n+}\n+\n+void* XBarrierSetC2::create_barrier_state(Arena* comp_arena) const {\n+  return new (comp_arena) XBarrierSetC2State(comp_arena);\n+}\n+\n+void XBarrierSetC2::late_barrier_analysis() const {\n+  analyze_dominating_barriers();\n+  compute_liveness_at_stubs();\n+}\n+\n+void XBarrierSetC2::emit_stubs(CodeBuffer& cb) const {\n+  MacroAssembler masm(&cb);\n+  GrowableArray<XLoadBarrierStubC2*>* const stubs = barrier_set_state()->stubs();\n+\n+  for (int i = 0; i < stubs->length(); i++) {\n+    \/\/ Make sure there is enough space in the code buffer\n+    if (cb.insts()->maybe_expand_to_ensure_remaining(PhaseOutput::MAX_inst_size) && cb.blob() == NULL) {\n+      ciEnv::current()->record_failure(\"CodeCache is full\");\n+      return;\n+    }\n+\n+    XBarrierSet::assembler()->generate_c2_load_barrier_stub(&masm, stubs->at(i));\n+  }\n+\n+  masm.flush();\n+}\n+\n+int XBarrierSetC2::estimate_stub_size() const {\n+  Compile* const C = Compile::current();\n+  BufferBlob* const blob = C->output()->scratch_buffer_blob();\n+  GrowableArray<XLoadBarrierStubC2*>* const stubs = barrier_set_state()->stubs();\n+  int size = 0;\n+\n+  for (int i = 0; i < stubs->length(); i++) {\n+    CodeBuffer cb(blob->content_begin(), (address)C->output()->scratch_locs_memory() - blob->content_begin());\n+    MacroAssembler masm(&cb);\n+    XBarrierSet::assembler()->generate_c2_load_barrier_stub(&masm, stubs->at(i));\n+    size += cb.insts_size();\n+  }\n+\n+  return size;\n+}\n+\n+static void set_barrier_data(C2Access& access) {\n+  if (XBarrierSet::barrier_needed(access.decorators(), access.type())) {\n+    uint8_t barrier_data = 0;\n+\n+    if (access.decorators() & ON_PHANTOM_OOP_REF) {\n+      barrier_data |= XLoadBarrierPhantom;\n+    } else if (access.decorators() & ON_WEAK_OOP_REF) {\n+      barrier_data |= XLoadBarrierWeak;\n+    } else {\n+      barrier_data |= XLoadBarrierStrong;\n+    }\n+\n+    if (access.decorators() & AS_NO_KEEPALIVE) {\n+      barrier_data |= XLoadBarrierNoKeepalive;\n+    }\n+\n+    access.set_barrier_data(barrier_data);\n+  }\n+}\n+\n+Node* XBarrierSetC2::load_at_resolved(C2Access& access, const Type* val_type) const {\n+  set_barrier_data(access);\n+  return BarrierSetC2::load_at_resolved(access, val_type);\n+}\n+\n+Node* XBarrierSetC2::atomic_cmpxchg_val_at_resolved(C2AtomicParseAccess& access, Node* expected_val,\n+                                                    Node* new_val, const Type* val_type) const {\n+  set_barrier_data(access);\n+  return BarrierSetC2::atomic_cmpxchg_val_at_resolved(access, expected_val, new_val, val_type);\n+}\n+\n+Node* XBarrierSetC2::atomic_cmpxchg_bool_at_resolved(C2AtomicParseAccess& access, Node* expected_val,\n+                                                     Node* new_val, const Type* value_type) const {\n+  set_barrier_data(access);\n+  return BarrierSetC2::atomic_cmpxchg_bool_at_resolved(access, expected_val, new_val, value_type);\n+}\n+\n+Node* XBarrierSetC2::atomic_xchg_at_resolved(C2AtomicParseAccess& access, Node* new_val, const Type* val_type) const {\n+  set_barrier_data(access);\n+  return BarrierSetC2::atomic_xchg_at_resolved(access, new_val, val_type);\n+}\n+\n+bool XBarrierSetC2::array_copy_requires_gc_barriers(bool tightly_coupled_alloc, BasicType type,\n+                                                    bool is_clone, bool is_clone_instance,\n+                                                    ArrayCopyPhase phase) const {\n+  if (phase == ArrayCopyPhase::Parsing) {\n+    return false;\n+  }\n+  if (phase == ArrayCopyPhase::Optimization) {\n+    return is_clone_instance;\n+  }\n+  \/\/ else ArrayCopyPhase::Expansion\n+  return type == T_OBJECT || type == T_ARRAY;\n+}\n+\n+\/\/ This TypeFunc assumes a 64bit system\n+static const TypeFunc* clone_type() {\n+  \/\/ Create input type (domain)\n+  const Type** domain_fields = TypeTuple::fields(4);\n+  domain_fields[TypeFunc::Parms + 0] = TypeInstPtr::NOTNULL;  \/\/ src\n+  domain_fields[TypeFunc::Parms + 1] = TypeInstPtr::NOTNULL;  \/\/ dst\n+  domain_fields[TypeFunc::Parms + 2] = TypeLong::LONG;        \/\/ size lower\n+  domain_fields[TypeFunc::Parms + 3] = Type::HALF;            \/\/ size upper\n+  const TypeTuple* domain = TypeTuple::make(TypeFunc::Parms + 4, domain_fields);\n+\n+  \/\/ Create result type (range)\n+  const Type** range_fields = TypeTuple::fields(0);\n+  const TypeTuple* range = TypeTuple::make(TypeFunc::Parms + 0, range_fields);\n+\n+  return TypeFunc::make(domain, range);\n+}\n+\n+#define XTOP LP64_ONLY(COMMA phase->top())\n+\n+void XBarrierSetC2::clone_at_expansion(PhaseMacroExpand* phase, ArrayCopyNode* ac) const {\n+  Node* const src = ac->in(ArrayCopyNode::Src);\n+  const TypeAryPtr* ary_ptr = src->get_ptr_type()->isa_aryptr();\n+\n+  if (ac->is_clone_array() && ary_ptr != NULL) {\n+    BasicType bt = ary_ptr->elem()->array_element_basic_type();\n+    if (is_reference_type(bt)) {\n+      \/\/ Clone object array\n+      bt = T_OBJECT;\n+    } else {\n+      \/\/ Clone primitive array\n+      bt = T_LONG;\n+    }\n+\n+    Node* ctrl = ac->in(TypeFunc::Control);\n+    Node* mem = ac->in(TypeFunc::Memory);\n+    Node* src = ac->in(ArrayCopyNode::Src);\n+    Node* src_offset = ac->in(ArrayCopyNode::SrcPos);\n+    Node* dest = ac->in(ArrayCopyNode::Dest);\n+    Node* dest_offset = ac->in(ArrayCopyNode::DestPos);\n+    Node* length = ac->in(ArrayCopyNode::Length);\n+\n+    if (bt == T_OBJECT) {\n+      \/\/ BarrierSetC2::clone sets the offsets via BarrierSetC2::arraycopy_payload_base_offset\n+      \/\/ which 8-byte aligns them to allow for word size copies. Make sure the offsets point\n+      \/\/ to the first element in the array when cloning object arrays. Otherwise, load\n+      \/\/ barriers are applied to parts of the header. Also adjust the length accordingly.\n+      assert(src_offset == dest_offset, \"should be equal\");\n+      jlong offset = src_offset->get_long();\n+      if (offset != arrayOopDesc::base_offset_in_bytes(T_OBJECT)) {\n+        assert(!UseCompressedClassPointers, \"should only happen without compressed class pointers\");\n+        assert((arrayOopDesc::base_offset_in_bytes(T_OBJECT) - offset) == BytesPerLong, \"unexpected offset\");\n+        length = phase->transform_later(new SubLNode(length, phase->longcon(1))); \/\/ Size is in longs\n+        src_offset = phase->longcon(arrayOopDesc::base_offset_in_bytes(T_OBJECT));\n+        dest_offset = src_offset;\n+      }\n+    }\n+    Node* payload_src = phase->basic_plus_adr(src, src_offset);\n+    Node* payload_dst = phase->basic_plus_adr(dest, dest_offset);\n+\n+    const char* copyfunc_name = \"arraycopy\";\n+    address     copyfunc_addr = phase->basictype2arraycopy(bt, NULL, NULL, true, copyfunc_name, true);\n+\n+    const TypePtr* raw_adr_type = TypeRawPtr::BOTTOM;\n+    const TypeFunc* call_type = OptoRuntime::fast_arraycopy_Type();\n+\n+    Node* call = phase->make_leaf_call(ctrl, mem, call_type, copyfunc_addr, copyfunc_name, raw_adr_type, payload_src, payload_dst, length XTOP);\n+    phase->transform_later(call);\n+\n+    phase->igvn().replace_node(ac, call);\n+    return;\n+  }\n+\n+  \/\/ Clone instance\n+  Node* const ctrl       = ac->in(TypeFunc::Control);\n+  Node* const mem        = ac->in(TypeFunc::Memory);\n+  Node* const dst        = ac->in(ArrayCopyNode::Dest);\n+  Node* const size       = ac->in(ArrayCopyNode::Length);\n+\n+  assert(size->bottom_type()->is_long(), \"Should be long\");\n+\n+  \/\/ The native clone we are calling here expects the instance size in words\n+  \/\/ Add header\/offset size to payload size to get instance size.\n+  Node* const base_offset = phase->longcon(arraycopy_payload_base_offset(ac->is_clone_array()) >> LogBytesPerLong);\n+  Node* const full_size = phase->transform_later(new AddLNode(size, base_offset));\n+\n+  Node* const call = phase->make_leaf_call(ctrl,\n+                                           mem,\n+                                           clone_type(),\n+                                           XBarrierSetRuntime::clone_addr(),\n+                                           \"XBarrierSetRuntime::clone\",\n+                                           TypeRawPtr::BOTTOM,\n+                                           src,\n+                                           dst,\n+                                           full_size,\n+                                           phase->top());\n+  phase->transform_later(call);\n+  phase->igvn().replace_node(ac, call);\n+}\n+\n+#undef XTOP\n+\n+\/\/ == Dominating barrier elision ==\n+\n+static bool block_has_safepoint(const Block* block, uint from, uint to) {\n+  for (uint i = from; i < to; i++) {\n+    if (block->get_node(i)->is_MachSafePoint()) {\n+      \/\/ Safepoint found\n+      return true;\n+    }\n+  }\n+\n+  \/\/ Safepoint not found\n+  return false;\n+}\n+\n+static bool block_has_safepoint(const Block* block) {\n+  return block_has_safepoint(block, 0, block->number_of_nodes());\n+}\n+\n+static uint block_index(const Block* block, const Node* node) {\n+  for (uint j = 0; j < block->number_of_nodes(); ++j) {\n+    if (block->get_node(j) == node) {\n+      return j;\n+    }\n+  }\n+  ShouldNotReachHere();\n+  return 0;\n+}\n+\n+void XBarrierSetC2::analyze_dominating_barriers() const {\n+  ResourceMark rm;\n+  Compile* const C = Compile::current();\n+  PhaseCFG* const cfg = C->cfg();\n+  Block_List worklist;\n+  Node_List mem_ops;\n+  Node_List barrier_loads;\n+\n+  \/\/ Step 1 - Find accesses, and track them in lists\n+  for (uint i = 0; i < cfg->number_of_blocks(); ++i) {\n+    const Block* const block = cfg->get_block(i);\n+    for (uint j = 0; j < block->number_of_nodes(); ++j) {\n+      const Node* const node = block->get_node(j);\n+      if (!node->is_Mach()) {\n+        continue;\n+      }\n+\n+      MachNode* const mach = node->as_Mach();\n+      switch (mach->ideal_Opcode()) {\n+      case Op_LoadP:\n+        if ((mach->barrier_data() & XLoadBarrierStrong) != 0) {\n+          barrier_loads.push(mach);\n+        }\n+        if ((mach->barrier_data() & (XLoadBarrierStrong | XLoadBarrierNoKeepalive)) ==\n+            XLoadBarrierStrong) {\n+          mem_ops.push(mach);\n+        }\n+        break;\n+      case Op_CompareAndExchangeP:\n+      case Op_CompareAndSwapP:\n+      case Op_GetAndSetP:\n+        if ((mach->barrier_data() & XLoadBarrierStrong) != 0) {\n+          barrier_loads.push(mach);\n+        }\n+      case Op_StoreP:\n+        mem_ops.push(mach);\n+        break;\n+\n+      default:\n+        break;\n+      }\n+    }\n+  }\n+\n+  \/\/ Step 2 - Find dominating accesses for each load\n+  for (uint i = 0; i < barrier_loads.size(); i++) {\n+    MachNode* const load = barrier_loads.at(i)->as_Mach();\n+    const TypePtr* load_adr_type = NULL;\n+    intptr_t load_offset = 0;\n+    const Node* const load_obj = load->get_base_and_disp(load_offset, load_adr_type);\n+    Block* const load_block = cfg->get_block_for_node(load);\n+    const uint load_index = block_index(load_block, load);\n+\n+    for (uint j = 0; j < mem_ops.size(); j++) {\n+      MachNode* mem = mem_ops.at(j)->as_Mach();\n+      const TypePtr* mem_adr_type = NULL;\n+      intptr_t mem_offset = 0;\n+      const Node* mem_obj = mem->get_base_and_disp(mem_offset, mem_adr_type);\n+      Block* mem_block = cfg->get_block_for_node(mem);\n+      uint mem_index = block_index(mem_block, mem);\n+\n+      if (load_obj == NodeSentinel || mem_obj == NodeSentinel ||\n+          load_obj == NULL || mem_obj == NULL ||\n+          load_offset < 0 || mem_offset < 0) {\n+        continue;\n+      }\n+\n+      if (mem_obj != load_obj || mem_offset != load_offset) {\n+        \/\/ Not the same addresses, not a candidate\n+        continue;\n+      }\n+\n+      if (load_block == mem_block) {\n+        \/\/ Earlier accesses in the same block\n+        if (mem_index < load_index && !block_has_safepoint(mem_block, mem_index + 1, load_index)) {\n+          load->set_barrier_data(XLoadBarrierElided);\n+        }\n+      } else if (mem_block->dominates(load_block)) {\n+        \/\/ Dominating block? Look around for safepoints\n+        ResourceMark rm;\n+        Block_List stack;\n+        VectorSet visited;\n+        stack.push(load_block);\n+        bool safepoint_found = block_has_safepoint(load_block);\n+        while (!safepoint_found && stack.size() > 0) {\n+          Block* block = stack.pop();\n+          if (visited.test_set(block->_pre_order)) {\n+            continue;\n+          }\n+          if (block_has_safepoint(block)) {\n+            safepoint_found = true;\n+            break;\n+          }\n+          if (block == mem_block) {\n+            continue;\n+          }\n+\n+          \/\/ Push predecessor blocks\n+          for (uint p = 1; p < block->num_preds(); ++p) {\n+            Block* pred = cfg->get_block_for_node(block->pred(p));\n+            stack.push(pred);\n+          }\n+        }\n+\n+        if (!safepoint_found) {\n+          load->set_barrier_data(XLoadBarrierElided);\n+        }\n+      }\n+    }\n+  }\n+}\n+\n+\/\/ == Reduced spilling optimization ==\n+\n+void XBarrierSetC2::compute_liveness_at_stubs() const {\n+  ResourceMark rm;\n+  Compile* const C = Compile::current();\n+  Arena* const A = Thread::current()->resource_area();\n+  PhaseCFG* const cfg = C->cfg();\n+  PhaseRegAlloc* const regalloc = C->regalloc();\n+  RegMask* const live = NEW_ARENA_ARRAY(A, RegMask, cfg->number_of_blocks() * sizeof(RegMask));\n+  XBarrierSetAssembler* const bs = XBarrierSet::assembler();\n+  Block_List worklist;\n+\n+  for (uint i = 0; i < cfg->number_of_blocks(); ++i) {\n+    new ((void*)(live + i)) RegMask();\n+    worklist.push(cfg->get_block(i));\n+  }\n+\n+  while (worklist.size() > 0) {\n+    const Block* const block = worklist.pop();\n+    RegMask& old_live = live[block->_pre_order];\n+    RegMask new_live;\n+\n+    \/\/ Initialize to union of successors\n+    for (uint i = 0; i < block->_num_succs; i++) {\n+      const uint succ_id = block->_succs[i]->_pre_order;\n+      new_live.OR(live[succ_id]);\n+    }\n+\n+    \/\/ Walk block backwards, computing liveness\n+    for (int i = block->number_of_nodes() - 1; i >= 0; --i) {\n+      const Node* const node = block->get_node(i);\n+\n+      \/\/ Remove def bits\n+      const OptoReg::Name first = bs->refine_register(node, regalloc->get_reg_first(node));\n+      const OptoReg::Name second = bs->refine_register(node, regalloc->get_reg_second(node));\n+      if (first != OptoReg::Bad) {\n+        new_live.Remove(first);\n+      }\n+      if (second != OptoReg::Bad) {\n+        new_live.Remove(second);\n+      }\n+\n+      \/\/ Add use bits\n+      for (uint j = 1; j < node->req(); ++j) {\n+        const Node* const use = node->in(j);\n+        const OptoReg::Name first = bs->refine_register(use, regalloc->get_reg_first(use));\n+        const OptoReg::Name second = bs->refine_register(use, regalloc->get_reg_second(use));\n+        if (first != OptoReg::Bad) {\n+          new_live.Insert(first);\n+        }\n+        if (second != OptoReg::Bad) {\n+          new_live.Insert(second);\n+        }\n+      }\n+\n+      \/\/ If this node tracks liveness, update it\n+      RegMask* const regs = barrier_set_state()->live(node);\n+      if (regs != NULL) {\n+        regs->OR(new_live);\n+      }\n+    }\n+\n+    \/\/ Now at block top, see if we have any changes\n+    new_live.SUBTRACT(old_live);\n+    if (new_live.is_NotEmpty()) {\n+      \/\/ Liveness has refined, update and propagate to prior blocks\n+      old_live.OR(new_live);\n+      for (uint i = 1; i < block->num_preds(); ++i) {\n+        Block* const pred = cfg->get_block_for_node(block->pred(i));\n+        worklist.push(pred);\n+      }\n+    }\n+  }\n+}\n+\n+#ifndef PRODUCT\n+void XBarrierSetC2::dump_barrier_data(const MachNode* mach, outputStream* st) const {\n+  if ((mach->barrier_data() & XLoadBarrierStrong) != 0) {\n+    st->print(\"strong \");\n+  }\n+  if ((mach->barrier_data() & XLoadBarrierWeak) != 0) {\n+    st->print(\"weak \");\n+  }\n+  if ((mach->barrier_data() & XLoadBarrierPhantom) != 0) {\n+    st->print(\"phantom \");\n+  }\n+  if ((mach->barrier_data() & XLoadBarrierNoKeepalive) != 0) {\n+    st->print(\"nokeepalive \");\n+  }\n+}\n+#endif \/\/ !PRODUCT\n","filename":"src\/hotspot\/share\/gc\/x\/c2\/xBarrierSetC2.cpp","additions":583,"deletions":0,"binary":false,"changes":583,"status":"added"},{"patch":"@@ -0,0 +1,100 @@\n+\/*\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_C2_XBARRIERSETC2_HPP\n+#define SHARE_GC_X_C2_XBARRIERSETC2_HPP\n+\n+#include \"gc\/shared\/c2\/barrierSetC2.hpp\"\n+#include \"memory\/allocation.hpp\"\n+#include \"opto\/node.hpp\"\n+#include \"utilities\/growableArray.hpp\"\n+\n+const uint8_t XLoadBarrierElided      = 0;\n+const uint8_t XLoadBarrierStrong      = 1;\n+const uint8_t XLoadBarrierWeak        = 2;\n+const uint8_t XLoadBarrierPhantom     = 4;\n+const uint8_t XLoadBarrierNoKeepalive = 8;\n+\n+class XLoadBarrierStubC2 : public ArenaObj {\n+private:\n+  const MachNode* _node;\n+  const Address   _ref_addr;\n+  const Register  _ref;\n+  const Register  _tmp;\n+  const uint8_t   _barrier_data;\n+  Label           _entry;\n+  Label           _continuation;\n+\n+  XLoadBarrierStubC2(const MachNode* node, Address ref_addr, Register ref, Register tmp, uint8_t barrier_data);\n+\n+public:\n+  static XLoadBarrierStubC2* create(const MachNode* node, Address ref_addr, Register ref, Register tmp, uint8_t barrier_data);\n+\n+  Address ref_addr() const;\n+  Register ref() const;\n+  Register tmp() const;\n+  address slow_path() const;\n+  RegMask& live() const;\n+  Label* entry();\n+  Label* continuation();\n+};\n+\n+class XBarrierSetC2 : public BarrierSetC2 {\n+private:\n+  void compute_liveness_at_stubs() const;\n+  void analyze_dominating_barriers() const;\n+\n+protected:\n+  virtual Node* load_at_resolved(C2Access& access, const Type* val_type) const;\n+  virtual Node* atomic_cmpxchg_val_at_resolved(C2AtomicParseAccess& access,\n+                                               Node* expected_val,\n+                                               Node* new_val,\n+                                               const Type* val_type) const;\n+  virtual Node* atomic_cmpxchg_bool_at_resolved(C2AtomicParseAccess& access,\n+                                                Node* expected_val,\n+                                                Node* new_val,\n+                                                const Type* value_type) const;\n+  virtual Node* atomic_xchg_at_resolved(C2AtomicParseAccess& access,\n+                                        Node* new_val,\n+                                        const Type* val_type) const;\n+\n+public:\n+  virtual void* create_barrier_state(Arena* comp_arena) const;\n+  virtual bool array_copy_requires_gc_barriers(bool tightly_coupled_alloc,\n+                                               BasicType type,\n+                                               bool is_clone,\n+                                               bool is_clone_instance,\n+                                               ArrayCopyPhase phase) const;\n+  virtual void clone_at_expansion(PhaseMacroExpand* phase,\n+                                  ArrayCopyNode* ac) const;\n+\n+  virtual void late_barrier_analysis() const;\n+  virtual int estimate_stub_size() const;\n+  virtual void emit_stubs(CodeBuffer& cb) const;\n+\n+#ifndef PRODUCT\n+  virtual void dump_barrier_data(const MachNode* mach, outputStream* st) const;\n+#endif\n+};\n+\n+#endif \/\/ SHARE_GC_X_C2_XBARRIERSETC2_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/c2\/xBarrierSetC2.hpp","additions":100,"deletions":0,"binary":false,"changes":100,"status":"added"},{"patch":"@@ -0,0 +1,41 @@\n+\/*\n+ * Copyright (c) 2018, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/x\/vmStructs_x.hpp\"\n+\n+XGlobalsForVMStructs::XGlobalsForVMStructs() :\n+    _XGlobalPhase(&XGlobalPhase),\n+    _XGlobalSeqNum(&XGlobalSeqNum),\n+    _XAddressOffsetMask(&XAddressOffsetMask),\n+    _XAddressMetadataMask(&XAddressMetadataMask),\n+    _XAddressMetadataFinalizable(&XAddressMetadataFinalizable),\n+    _XAddressGoodMask(&XAddressGoodMask),\n+    _XAddressBadMask(&XAddressBadMask),\n+    _XAddressWeakBadMask(&XAddressWeakBadMask),\n+    _XObjectAlignmentSmallShift(&XObjectAlignmentSmallShift),\n+    _XObjectAlignmentSmall(&XObjectAlignmentSmall) {\n+}\n+\n+XGlobalsForVMStructs XGlobalsForVMStructs::_instance;\n+XGlobalsForVMStructs* XGlobalsForVMStructs::_instance_p = &XGlobalsForVMStructs::_instance;\n","filename":"src\/hotspot\/share\/gc\/x\/vmStructs_x.cpp","additions":41,"deletions":0,"binary":false,"changes":41,"status":"added"},{"patch":"@@ -0,0 +1,143 @@\n+\/*\n+ * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_VMSTRUCTS_X_HPP\n+#define SHARE_GC_X_VMSTRUCTS_X_HPP\n+\n+#include \"gc\/x\/xAttachedArray.hpp\"\n+#include \"gc\/x\/xCollectedHeap.hpp\"\n+#include \"gc\/x\/xForwarding.hpp\"\n+#include \"gc\/x\/xGranuleMap.hpp\"\n+#include \"gc\/x\/xHeap.hpp\"\n+#include \"gc\/x\/xPageAllocator.hpp\"\n+#include \"utilities\/macros.hpp\"\n+\n+\/\/ Expose some ZGC globals to the SA agent.\n+class XGlobalsForVMStructs {\n+  static XGlobalsForVMStructs _instance;\n+\n+public:\n+  static XGlobalsForVMStructs* _instance_p;\n+\n+  XGlobalsForVMStructs();\n+\n+  uint32_t* _XGlobalPhase;\n+\n+  uint32_t* _XGlobalSeqNum;\n+\n+  uintptr_t* _XAddressOffsetMask;\n+  uintptr_t* _XAddressMetadataMask;\n+  uintptr_t* _XAddressMetadataFinalizable;\n+  uintptr_t* _XAddressGoodMask;\n+  uintptr_t* _XAddressBadMask;\n+  uintptr_t* _XAddressWeakBadMask;\n+\n+  const int* _XObjectAlignmentSmallShift;\n+  const int* _XObjectAlignmentSmall;\n+};\n+\n+typedef XGranuleMap<XPage*> XGranuleMapForPageTable;\n+typedef XGranuleMap<XForwarding*> XGranuleMapForForwarding;\n+typedef XAttachedArray<XForwarding, XForwardingEntry> XAttachedArrayForForwarding;\n+\n+#define VM_STRUCTS_X(nonstatic_field, volatile_nonstatic_field, static_field)                            \\\n+  static_field(XGlobalsForVMStructs,            _instance_p,          XGlobalsForVMStructs*)             \\\n+  nonstatic_field(XGlobalsForVMStructs,         _XGlobalPhase,        uint32_t*)                         \\\n+  nonstatic_field(XGlobalsForVMStructs,         _XGlobalSeqNum,       uint32_t*)                         \\\n+  nonstatic_field(XGlobalsForVMStructs,         _XAddressOffsetMask,  uintptr_t*)                        \\\n+  nonstatic_field(XGlobalsForVMStructs,         _XAddressMetadataMask, uintptr_t*)                       \\\n+  nonstatic_field(XGlobalsForVMStructs,         _XAddressMetadataFinalizable, uintptr_t*)                \\\n+  nonstatic_field(XGlobalsForVMStructs,         _XAddressGoodMask,    uintptr_t*)                        \\\n+  nonstatic_field(XGlobalsForVMStructs,         _XAddressBadMask,     uintptr_t*)                        \\\n+  nonstatic_field(XGlobalsForVMStructs,         _XAddressWeakBadMask, uintptr_t*)                        \\\n+  nonstatic_field(XGlobalsForVMStructs,         _XObjectAlignmentSmallShift, const int*)                 \\\n+  nonstatic_field(XGlobalsForVMStructs,         _XObjectAlignmentSmall, const int*)                      \\\n+                                                                                                         \\\n+  nonstatic_field(XCollectedHeap,               _heap,                XHeap)                             \\\n+                                                                                                         \\\n+  nonstatic_field(XHeap,                        _page_allocator,      XPageAllocator)                    \\\n+  nonstatic_field(XHeap,                        _page_table,          XPageTable)                        \\\n+  nonstatic_field(XHeap,                        _forwarding_table,    XForwardingTable)                  \\\n+  nonstatic_field(XHeap,                        _relocate,            XRelocate)                         \\\n+                                                                                                         \\\n+  nonstatic_field(XPage,                        _type,                const uint8_t)                     \\\n+  nonstatic_field(XPage,                        _seqnum,              uint32_t)                          \\\n+  nonstatic_field(XPage,                        _virtual,             const XVirtualMemory)              \\\n+  volatile_nonstatic_field(XPage,               _top,                 uintptr_t)                         \\\n+                                                                                                         \\\n+  nonstatic_field(XPageAllocator,               _max_capacity,        const size_t)                      \\\n+  volatile_nonstatic_field(XPageAllocator,      _capacity,            size_t)                            \\\n+  volatile_nonstatic_field(XPageAllocator,      _used,                size_t)                            \\\n+                                                                                                         \\\n+  nonstatic_field(XPageTable,                   _map,                 XGranuleMapForPageTable)           \\\n+                                                                                                         \\\n+  nonstatic_field(XGranuleMapForPageTable,      _map,                 XPage** const)                     \\\n+  nonstatic_field(XGranuleMapForForwarding,     _map,                 XForwarding** const)               \\\n+                                                                                                         \\\n+  nonstatic_field(XForwardingTable,             _map,                 XGranuleMapForForwarding)          \\\n+                                                                                                         \\\n+  nonstatic_field(XVirtualMemory,               _start,               const uintptr_t)                   \\\n+  nonstatic_field(XVirtualMemory,               _end,                 const uintptr_t)                   \\\n+                                                                                                         \\\n+  nonstatic_field(XForwarding,                  _virtual,             const XVirtualMemory)              \\\n+  nonstatic_field(XForwarding,                  _object_alignment_shift, const size_t)                   \\\n+  volatile_nonstatic_field(XForwarding,         _ref_count,           int)                               \\\n+  nonstatic_field(XForwarding,                  _entries,             const XAttachedArrayForForwarding) \\\n+  nonstatic_field(XForwardingEntry,             _entry,               uint64_t)                          \\\n+  nonstatic_field(XAttachedArrayForForwarding,  _length,              const size_t)\n+\n+#define VM_INT_CONSTANTS_X(declare_constant, declare_constant_with_value)                                \\\n+  declare_constant(XPhaseRelocate)                                                                       \\\n+  declare_constant(XPageTypeSmall)                                                                       \\\n+  declare_constant(XPageTypeMedium)                                                                      \\\n+  declare_constant(XPageTypeLarge)                                                                       \\\n+  declare_constant(XObjectAlignmentMediumShift)                                                          \\\n+  declare_constant(XObjectAlignmentLargeShift)\n+\n+#define VM_LONG_CONSTANTS_X(declare_constant)                                                            \\\n+  declare_constant(XGranuleSizeShift)                                                                    \\\n+  declare_constant(XPageSizeSmallShift)                                                                  \\\n+  declare_constant(XPageSizeMediumShift)                                                                 \\\n+  declare_constant(XAddressOffsetShift)                                                                  \\\n+  declare_constant(XAddressOffsetBits)                                                                   \\\n+  declare_constant(XAddressOffsetMask)                                                                   \\\n+  declare_constant(XAddressOffsetMax)\n+\n+#define VM_TYPES_X(declare_type, declare_toplevel_type, declare_integer_type)                            \\\n+  declare_toplevel_type(XGlobalsForVMStructs)                                                            \\\n+  declare_type(XCollectedHeap, CollectedHeap)                                                            \\\n+  declare_toplevel_type(XHeap)                                                                           \\\n+  declare_toplevel_type(XRelocate)                                                                       \\\n+  declare_toplevel_type(XPage)                                                                           \\\n+  declare_toplevel_type(XPageAllocator)                                                                  \\\n+  declare_toplevel_type(XPageTable)                                                                      \\\n+  declare_toplevel_type(XAttachedArrayForForwarding)                                                     \\\n+  declare_toplevel_type(XGranuleMapForPageTable)                                                         \\\n+  declare_toplevel_type(XGranuleMapForForwarding)                                                        \\\n+  declare_toplevel_type(XVirtualMemory)                                                                  \\\n+  declare_toplevel_type(XForwardingTable)                                                                \\\n+  declare_toplevel_type(XForwarding)                                                                     \\\n+  declare_toplevel_type(XForwardingEntry)                                                                \\\n+  declare_toplevel_type(XPhysicalMemoryManager)\n+\n+#endif \/\/ SHARE_GC_X_VMSTRUCTS_X_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/vmStructs_x.hpp","additions":143,"deletions":0,"binary":false,"changes":143,"status":"added"},{"patch":"@@ -25,1 +25,1 @@\n-#include \"gc\/z\/zAbort.hpp\"\n+#include \"gc\/x\/xAbort.hpp\"\n@@ -28,1 +28,1 @@\n-volatile bool ZAbort::_should_abort = false;\n+volatile bool XAbort::_should_abort = false;\n@@ -30,1 +30,1 @@\n-void ZAbort::abort() {\n+void XAbort::abort() {\n","filename":"src\/hotspot\/share\/gc\/x\/xAbort.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"previous_filename":"src\/hotspot\/share\/gc\/z\/zAbort.cpp","status":"copied"},{"patch":"@@ -24,2 +24,2 @@\n-#ifndef SHARE_GC_Z_ZABORT_HPP\n-#define SHARE_GC_Z_ZABORT_HPP\n+#ifndef SHARE_GC_X_XABORT_HPP\n+#define SHARE_GC_X_XABORT_HPP\n@@ -29,1 +29,1 @@\n-class ZAbort : public AllStatic {\n+class XAbort : public AllStatic {\n@@ -38,1 +38,1 @@\n-#endif \/\/ SHARE_GC_Z_ZABORT_HPP\n+#endif \/\/ SHARE_GC_X_XABORT_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xAbort.hpp","additions":4,"deletions":4,"binary":false,"changes":8,"previous_filename":"src\/hotspot\/share\/gc\/z\/zAbort.hpp","status":"copied"},{"patch":"@@ -0,0 +1,35 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XABORT_INLINE_HPP\n+#define SHARE_GC_X_XABORT_INLINE_HPP\n+\n+#include \"gc\/x\/xAbort.hpp\"\n+\n+#include \"runtime\/atomic.hpp\"\n+\n+inline bool XAbort::should_abort() {\n+  return Atomic::load_acquire(&_should_abort);\n+}\n+\n+#endif \/\/ SHARE_GC_X_XABORT_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xAbort.inline.hpp","additions":35,"deletions":0,"binary":false,"changes":35,"status":"added"},{"patch":"@@ -0,0 +1,58 @@\n+\/*\n+ * Copyright (c) 2015, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/x\/xAddress.hpp\"\n+#include \"gc\/x\/xGlobals.hpp\"\n+\n+void XAddress::set_good_mask(uintptr_t mask) {\n+  XAddressGoodMask = mask;\n+  XAddressBadMask = XAddressGoodMask ^ XAddressMetadataMask;\n+  XAddressWeakBadMask = (XAddressGoodMask | XAddressMetadataRemapped | XAddressMetadataFinalizable) ^ XAddressMetadataMask;\n+}\n+\n+void XAddress::initialize() {\n+  XAddressOffsetBits = XPlatformAddressOffsetBits();\n+  XAddressOffsetMask = (((uintptr_t)1 << XAddressOffsetBits) - 1) << XAddressOffsetShift;\n+  XAddressOffsetMax = (uintptr_t)1 << XAddressOffsetBits;\n+\n+  XAddressMetadataShift = XPlatformAddressMetadataShift();\n+  XAddressMetadataMask = (((uintptr_t)1 << XAddressMetadataBits) - 1) << XAddressMetadataShift;\n+\n+  XAddressMetadataMarked0 = (uintptr_t)1 << (XAddressMetadataShift + 0);\n+  XAddressMetadataMarked1 = (uintptr_t)1 << (XAddressMetadataShift + 1);\n+  XAddressMetadataRemapped = (uintptr_t)1 << (XAddressMetadataShift + 2);\n+  XAddressMetadataFinalizable = (uintptr_t)1 << (XAddressMetadataShift + 3);\n+\n+  XAddressMetadataMarked = XAddressMetadataMarked0;\n+  set_good_mask(XAddressMetadataRemapped);\n+}\n+\n+void XAddress::flip_to_marked() {\n+  XAddressMetadataMarked ^= (XAddressMetadataMarked0 | XAddressMetadataMarked1);\n+  set_good_mask(XAddressMetadataMarked);\n+}\n+\n+void XAddress::flip_to_remapped() {\n+  set_good_mask(XAddressMetadataRemapped);\n+}\n","filename":"src\/hotspot\/share\/gc\/x\/xAddress.cpp","additions":58,"deletions":0,"binary":false,"changes":58,"status":"added"},{"patch":"@@ -24,2 +24,2 @@\n-#ifndef SHARE_GC_Z_ZADDRESS_HPP\n-#define SHARE_GC_Z_ZADDRESS_HPP\n+#ifndef SHARE_GC_X_XADDRESS_HPP\n+#define SHARE_GC_X_XADDRESS_HPP\n@@ -30,2 +30,2 @@\n-class ZAddress : public AllStatic {\n-  friend class ZAddressTest;\n+class XAddress : public AllStatic {\n+  friend class XAddressTest;\n@@ -67,1 +67,1 @@\n-#endif \/\/ SHARE_GC_Z_ZADDRESS_HPP\n+#endif \/\/ SHARE_GC_X_XADDRESS_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xAddress.hpp","additions":5,"deletions":5,"binary":false,"changes":10,"previous_filename":"src\/hotspot\/share\/gc\/z\/zAddress.hpp","status":"copied"},{"patch":"@@ -0,0 +1,137 @@\n+\/*\n+ * Copyright (c) 2015, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XADDRESS_INLINE_HPP\n+#define SHARE_GC_X_XADDRESS_INLINE_HPP\n+\n+#include \"gc\/x\/xAddress.hpp\"\n+\n+#include \"gc\/x\/xGlobals.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include \"utilities\/macros.hpp\"\n+#include \"utilities\/powerOfTwo.hpp\"\n+\n+inline bool XAddress::is_null(uintptr_t value) {\n+  return value == 0;\n+}\n+\n+inline bool XAddress::is_bad(uintptr_t value) {\n+  return value & XAddressBadMask;\n+}\n+\n+inline bool XAddress::is_good(uintptr_t value) {\n+  return !is_bad(value) && !is_null(value);\n+}\n+\n+inline bool XAddress::is_good_or_null(uintptr_t value) {\n+  \/\/ Checking if an address is \"not bad\" is an optimized version of\n+  \/\/ checking if it's \"good or null\", which eliminates an explicit\n+  \/\/ null check. However, the implicit null check only checks that\n+  \/\/ the mask bits are zero, not that the entire address is zero.\n+  \/\/ This means that an address without mask bits would pass through\n+  \/\/ the barrier as if it was null. This should be harmless as such\n+  \/\/ addresses should ever be passed through the barrier.\n+  const bool result = !is_bad(value);\n+  assert((is_good(value) || is_null(value)) == result, \"Bad address\");\n+  return result;\n+}\n+\n+inline bool XAddress::is_weak_bad(uintptr_t value) {\n+  return value & XAddressWeakBadMask;\n+}\n+\n+inline bool XAddress::is_weak_good(uintptr_t value) {\n+  return !is_weak_bad(value) && !is_null(value);\n+}\n+\n+inline bool XAddress::is_weak_good_or_null(uintptr_t value) {\n+  return !is_weak_bad(value);\n+}\n+\n+inline bool XAddress::is_marked(uintptr_t value) {\n+  return value & XAddressMetadataMarked;\n+}\n+\n+inline bool XAddress::is_marked_or_null(uintptr_t value) {\n+  return is_marked(value) || is_null(value);\n+}\n+\n+inline bool XAddress::is_finalizable(uintptr_t value) {\n+  return value & XAddressMetadataFinalizable;\n+}\n+\n+inline bool XAddress::is_finalizable_good(uintptr_t value) {\n+  return is_finalizable(value) && is_good(value ^ XAddressMetadataFinalizable);\n+}\n+\n+inline bool XAddress::is_remapped(uintptr_t value) {\n+  return value & XAddressMetadataRemapped;\n+}\n+\n+inline bool XAddress::is_in(uintptr_t value) {\n+  \/\/ Check that exactly one non-offset bit is set\n+  if (!is_power_of_2(value & ~XAddressOffsetMask)) {\n+    return false;\n+  }\n+\n+  \/\/ Check that one of the non-finalizable metadata is set\n+  return value & (XAddressMetadataMask & ~XAddressMetadataFinalizable);\n+}\n+\n+inline uintptr_t XAddress::offset(uintptr_t value) {\n+  return value & XAddressOffsetMask;\n+}\n+\n+inline uintptr_t XAddress::good(uintptr_t value) {\n+  return offset(value) | XAddressGoodMask;\n+}\n+\n+inline uintptr_t XAddress::good_or_null(uintptr_t value) {\n+  return is_null(value) ? 0 : good(value);\n+}\n+\n+inline uintptr_t XAddress::finalizable_good(uintptr_t value) {\n+  return offset(value) | XAddressMetadataFinalizable | XAddressGoodMask;\n+}\n+\n+inline uintptr_t XAddress::marked(uintptr_t value) {\n+  return offset(value) | XAddressMetadataMarked;\n+}\n+\n+inline uintptr_t XAddress::marked0(uintptr_t value) {\n+  return offset(value) | XAddressMetadataMarked0;\n+}\n+\n+inline uintptr_t XAddress::marked1(uintptr_t value) {\n+  return offset(value) | XAddressMetadataMarked1;\n+}\n+\n+inline uintptr_t XAddress::remapped(uintptr_t value) {\n+  return offset(value) | XAddressMetadataRemapped;\n+}\n+\n+inline uintptr_t XAddress::remapped_or_null(uintptr_t value) {\n+  return is_null(value) ? 0 : remapped(value);\n+}\n+\n+#endif \/\/ SHARE_GC_X_XADDRESS_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xAddress.inline.hpp","additions":137,"deletions":0,"binary":false,"changes":137,"status":"added"},{"patch":"@@ -0,0 +1,53 @@\n+\/*\n+ * Copyright (c) 2019, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/shared\/gc_globals.hpp\"\n+#include \"gc\/x\/xAddressSpaceLimit.hpp\"\n+#include \"gc\/x\/xGlobals.hpp\"\n+#include \"runtime\/globals.hpp\"\n+#include \"runtime\/os.hpp\"\n+#include \"utilities\/align.hpp\"\n+\n+static size_t address_space_limit() {\n+  size_t limit = 0;\n+\n+  if (os::has_allocatable_memory_limit(&limit)) {\n+    return limit;\n+  }\n+\n+  \/\/ No limit\n+  return SIZE_MAX;\n+}\n+\n+size_t XAddressSpaceLimit::mark_stack() {\n+  \/\/ Allow mark stacks to occupy 10% of the address space\n+  const size_t limit = address_space_limit() \/ 10;\n+  return align_up(limit, XMarkStackSpaceExpandSize);\n+}\n+\n+size_t XAddressSpaceLimit::heap_view() {\n+  \/\/ Allow all heap views to occupy 50% of the address space\n+  const size_t limit = address_space_limit() \/ MaxVirtMemFraction \/ XHeapViews;\n+  return align_up(limit, XGranuleSize);\n+}\n","filename":"src\/hotspot\/share\/gc\/x\/xAddressSpaceLimit.cpp","additions":53,"deletions":0,"binary":false,"changes":53,"status":"added"},{"patch":"@@ -0,0 +1,36 @@\n+\/*\n+ * Copyright (c) 2019, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XADDRESSSPACELIMIT_HPP\n+#define SHARE_GC_X_XADDRESSSPACELIMIT_HPP\n+\n+#include \"memory\/allStatic.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+class XAddressSpaceLimit : public AllStatic {\n+public:\n+  static size_t mark_stack();\n+  static size_t heap_view();\n+};\n+\n+#endif \/\/ SHARE_GC_X_XADDRESSSPACELIMIT_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xAddressSpaceLimit.hpp","additions":36,"deletions":0,"binary":false,"changes":36,"status":"added"},{"patch":"@@ -0,0 +1,85 @@\n+\/*\n+ * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XALLOCATIONFLAGS_HPP\n+#define SHARE_GC_X_XALLOCATIONFLAGS_HPP\n+\n+#include \"gc\/x\/xBitField.hpp\"\n+#include \"memory\/allocation.hpp\"\n+\n+\/\/\n+\/\/ Allocation flags layout\n+\/\/ -----------------------\n+\/\/\n+\/\/   7     2 1 0\n+\/\/  +-----+-+-+-+\n+\/\/  |00000|1|1|1|\n+\/\/  +-----+-+-+-+\n+\/\/  |     | | |\n+\/\/  |     | | * 0-0 Non-Blocking Flag (1-bit)\n+\/\/  |     | |\n+\/\/  |     | * 1-1 Worker Relocation Flag (1-bit)\n+\/\/  |     |\n+\/\/  |     * 2-2 Low Address Flag (1-bit)\n+\/\/  |\n+\/\/  * 7-3 Unused (5-bits)\n+\/\/\n+\n+class XAllocationFlags {\n+private:\n+  typedef XBitField<uint8_t, bool, 0, 1> field_non_blocking;\n+  typedef XBitField<uint8_t, bool, 1, 1> field_worker_relocation;\n+  typedef XBitField<uint8_t, bool, 2, 1> field_low_address;\n+\n+  uint8_t _flags;\n+\n+public:\n+  XAllocationFlags() :\n+      _flags(0) {}\n+\n+  void set_non_blocking() {\n+    _flags |= field_non_blocking::encode(true);\n+  }\n+\n+  void set_worker_relocation() {\n+    _flags |= field_worker_relocation::encode(true);\n+  }\n+\n+  void set_low_address() {\n+    _flags |= field_low_address::encode(true);\n+  }\n+\n+  bool non_blocking() const {\n+    return field_non_blocking::decode(_flags);\n+  }\n+\n+  bool worker_relocation() const {\n+    return field_worker_relocation::decode(_flags);\n+  }\n+\n+  bool low_address() const {\n+    return field_low_address::decode(_flags);\n+  }\n+};\n+\n+#endif \/\/ SHARE_GC_X_XALLOCATIONFLAGS_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xAllocationFlags.hpp","additions":85,"deletions":0,"binary":false,"changes":85,"status":"added"},{"patch":"@@ -0,0 +1,123 @@\n+\/*\n+ * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/x\/xAddressSpaceLimit.hpp\"\n+#include \"gc\/x\/xArguments.hpp\"\n+#include \"gc\/x\/xCollectedHeap.hpp\"\n+#include \"gc\/x\/xGlobals.hpp\"\n+#include \"gc\/x\/xHeuristics.hpp\"\n+#include \"gc\/shared\/gcArguments.hpp\"\n+#include \"runtime\/globals.hpp\"\n+#include \"runtime\/globals_extension.hpp\"\n+#include \"runtime\/java.hpp\"\n+\n+void XArguments::initialize_alignments() {\n+  SpaceAlignment = XGranuleSize;\n+  HeapAlignment = SpaceAlignment;\n+}\n+\n+void XArguments::initialize() {\n+  \/\/ Check mark stack size\n+  const size_t mark_stack_space_limit = XAddressSpaceLimit::mark_stack();\n+  if (ZMarkStackSpaceLimit > mark_stack_space_limit) {\n+    if (!FLAG_IS_DEFAULT(ZMarkStackSpaceLimit)) {\n+      vm_exit_during_initialization(\"ZMarkStackSpaceLimit too large for limited address space\");\n+    }\n+    FLAG_SET_DEFAULT(ZMarkStackSpaceLimit, mark_stack_space_limit);\n+  }\n+\n+  \/\/ Enable NUMA by default\n+  if (FLAG_IS_DEFAULT(UseNUMA)) {\n+    FLAG_SET_DEFAULT(UseNUMA, true);\n+  }\n+\n+  if (FLAG_IS_DEFAULT(ZFragmentationLimit)) {\n+    FLAG_SET_DEFAULT(ZFragmentationLimit, 25.0);\n+  }\n+\n+  \/\/ Select number of parallel threads\n+  if (FLAG_IS_DEFAULT(ParallelGCThreads)) {\n+    FLAG_SET_DEFAULT(ParallelGCThreads, XHeuristics::nparallel_workers());\n+  }\n+\n+  if (ParallelGCThreads == 0) {\n+    vm_exit_during_initialization(\"The flag -XX:+UseZGC can not be combined with -XX:ParallelGCThreads=0\");\n+  }\n+\n+  \/\/ Select number of concurrent threads\n+  if (FLAG_IS_DEFAULT(ConcGCThreads)) {\n+    FLAG_SET_DEFAULT(ConcGCThreads, XHeuristics::nconcurrent_workers());\n+  }\n+\n+  if (ConcGCThreads == 0) {\n+    vm_exit_during_initialization(\"The flag -XX:+UseZGC can not be combined with -XX:ConcGCThreads=0\");\n+  }\n+\n+  \/\/ Large page size must match granule size\n+  if (!FLAG_IS_DEFAULT(LargePageSizeInBytes) && LargePageSizeInBytes != XGranuleSize) {\n+    vm_exit_during_initialization(err_msg(\"Incompatible -XX:LargePageSizeInBytes, only \"\n+                                          SIZE_FORMAT \"M large pages are supported by ZGC\",\n+                                          XGranuleSize \/ M));\n+  }\n+\n+  \/\/ The heuristics used when UseDynamicNumberOfGCThreads is\n+  \/\/ enabled defaults to using a ZAllocationSpikeTolerance of 1.\n+  if (UseDynamicNumberOfGCThreads && FLAG_IS_DEFAULT(ZAllocationSpikeTolerance)) {\n+    FLAG_SET_DEFAULT(ZAllocationSpikeTolerance, 1);\n+  }\n+\n+#ifdef COMPILER2\n+  \/\/ Enable loop strip mining by default\n+  if (FLAG_IS_DEFAULT(UseCountedLoopSafepoints)) {\n+    FLAG_SET_DEFAULT(UseCountedLoopSafepoints, true);\n+    if (FLAG_IS_DEFAULT(LoopStripMiningIter)) {\n+      FLAG_SET_DEFAULT(LoopStripMiningIter, 1000);\n+    }\n+  }\n+#endif\n+\n+  \/\/ CompressedOops not supported\n+  FLAG_SET_DEFAULT(UseCompressedOops, false);\n+\n+  \/\/ Verification before startup and after exit not (yet) supported\n+  FLAG_SET_DEFAULT(VerifyDuringStartup, false);\n+  FLAG_SET_DEFAULT(VerifyBeforeExit, false);\n+\n+  if (VerifyBeforeGC || VerifyDuringGC || VerifyAfterGC) {\n+    FLAG_SET_DEFAULT(ZVerifyRoots, true);\n+    FLAG_SET_DEFAULT(ZVerifyObjects, true);\n+  }\n+}\n+\n+size_t XArguments::heap_virtual_to_physical_ratio() {\n+  return XHeapViews * XVirtualToPhysicalRatio;\n+}\n+\n+CollectedHeap* XArguments::create_heap() {\n+  return new XCollectedHeap();\n+}\n+\n+bool XArguments::is_supported() {\n+  return is_os_supported();\n+}\n","filename":"src\/hotspot\/share\/gc\/x\/xArguments.cpp","additions":123,"deletions":0,"binary":false,"changes":123,"status":"added"},{"patch":"@@ -0,0 +1,43 @@\n+\/*\n+ * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XARGUMENTS_HPP\n+#define SHARE_GC_X_XARGUMENTS_HPP\n+\n+#include \"gc\/shared\/gcArguments.hpp\"\n+\n+class CollectedHeap;\n+\n+class XArguments : AllStatic {\n+public:\n+  static void initialize_alignments();\n+  static void initialize();\n+  static size_t heap_virtual_to_physical_ratio();\n+  static CollectedHeap* create_heap();\n+\n+  static bool is_supported();\n+\n+  static bool is_os_supported();\n+};\n+\n+#endif \/\/ SHARE_GC_X_XARGUMENTS_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xArguments.hpp","additions":43,"deletions":0,"binary":false,"changes":43,"status":"added"},{"patch":"@@ -0,0 +1,51 @@\n+\/*\n+ * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XARRAY_HPP\n+#define SHARE_GC_X_XARRAY_HPP\n+\n+#include \"memory\/allocation.hpp\"\n+#include \"utilities\/growableArray.hpp\"\n+\n+template <typename T> using XArray = GrowableArrayCHeap<T, mtGC>;\n+\n+template <typename T, bool Parallel>\n+class XArrayIteratorImpl : public StackObj {\n+private:\n+  const T*       _next;\n+  const T* const _end;\n+\n+  bool next_serial(T* elem);\n+  bool next_parallel(T* elem);\n+\n+public:\n+  XArrayIteratorImpl(const T* array, size_t length);\n+  XArrayIteratorImpl(const XArray<T>* array);\n+\n+  bool next(T* elem);\n+};\n+\n+template <typename T> using XArrayIterator = XArrayIteratorImpl<T, false \/* Parallel *\/>;\n+template <typename T> using XArrayParallelIterator = XArrayIteratorImpl<T, true \/* Parallel *\/>;\n+\n+#endif \/\/ SHARE_GC_X_XARRAY_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xArray.hpp","additions":51,"deletions":0,"binary":false,"changes":51,"status":"added"},{"patch":"@@ -0,0 +1,81 @@\n+\/*\n+ * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XARRAY_INLINE_HPP\n+#define SHARE_GC_X_XARRAY_INLINE_HPP\n+\n+#include \"gc\/x\/xArray.hpp\"\n+\n+#include \"runtime\/atomic.hpp\"\n+\n+template <typename T, bool Parallel>\n+inline bool XArrayIteratorImpl<T, Parallel>::next_serial(T* elem) {\n+  if (_next == _end) {\n+    return false;\n+  }\n+\n+  *elem = *_next;\n+  _next++;\n+\n+  return true;\n+}\n+\n+template <typename T, bool Parallel>\n+inline bool XArrayIteratorImpl<T, Parallel>::next_parallel(T* elem) {\n+  const T* old_next = Atomic::load(&_next);\n+\n+  for (;;) {\n+    if (old_next == _end) {\n+      return false;\n+    }\n+\n+    const T* const new_next = old_next + 1;\n+    const T* const prev_next = Atomic::cmpxchg(&_next, old_next, new_next);\n+    if (prev_next == old_next) {\n+      *elem = *old_next;\n+      return true;\n+    }\n+\n+    old_next = prev_next;\n+  }\n+}\n+\n+template <typename T, bool Parallel>\n+inline XArrayIteratorImpl<T, Parallel>::XArrayIteratorImpl(const T* array, size_t length) :\n+    _next(array),\n+    _end(array + length) {}\n+\n+template <typename T, bool Parallel>\n+inline XArrayIteratorImpl<T, Parallel>::XArrayIteratorImpl(const XArray<T>* array) :\n+    XArrayIteratorImpl<T, Parallel>(array->is_empty() ? NULL : array->adr_at(0), array->length()) {}\n+\n+template <typename T, bool Parallel>\n+inline bool XArrayIteratorImpl<T, Parallel>::next(T* elem) {\n+  if (Parallel) {\n+    return next_parallel(elem);\n+  } else {\n+    return next_serial(elem);\n+  }\n+}\n+\n+#endif \/\/ SHARE_GC_X_XARRAY_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xArray.inline.hpp","additions":81,"deletions":0,"binary":false,"changes":81,"status":"added"},{"patch":"@@ -0,0 +1,54 @@\n+\/*\n+ * Copyright (c) 2019, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XATTACHEDARRAY_HPP\n+#define SHARE_GC_X_XATTACHEDARRAY_HPP\n+\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+class VMStructs;\n+\n+template <typename ObjectT, typename ArrayT>\n+class XAttachedArray {\n+  friend class ::VMStructs;\n+\n+private:\n+  const size_t _length;\n+\n+  static size_t object_size();\n+  static size_t array_size(size_t length);\n+\n+public:\n+  template <typename Allocator>\n+  static void* alloc(Allocator* allocator, size_t length);\n+\n+  static void* alloc(size_t length);\n+  static void free(ObjectT* obj);\n+\n+  XAttachedArray(size_t length);\n+\n+  size_t length() const;\n+  ArrayT* operator()(const ObjectT* obj) const;\n+};\n+\n+#endif \/\/ SHARE_GC_X_XATTACHEDARRAY_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xAttachedArray.hpp","additions":54,"deletions":0,"binary":false,"changes":54,"status":"added"},{"patch":"@@ -0,0 +1,86 @@\n+\/*\n+ * Copyright (c) 2019, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XATTACHEDARRAY_INLINE_HPP\n+#define SHARE_GC_X_XATTACHEDARRAY_INLINE_HPP\n+\n+#include \"gc\/x\/xAttachedArray.hpp\"\n+\n+#include \"memory\/allocation.hpp\"\n+#include \"utilities\/align.hpp\"\n+\n+template <typename ObjectT, typename ArrayT>\n+inline size_t XAttachedArray<ObjectT, ArrayT>::object_size() {\n+  return align_up(sizeof(ObjectT), sizeof(ArrayT));\n+}\n+\n+template <typename ObjectT, typename ArrayT>\n+inline size_t XAttachedArray<ObjectT, ArrayT>::array_size(size_t length) {\n+  return sizeof(ArrayT) * length;\n+}\n+\n+template <typename ObjectT, typename ArrayT>\n+template <typename Allocator>\n+inline void* XAttachedArray<ObjectT, ArrayT>::alloc(Allocator* allocator, size_t length) {\n+  \/\/ Allocate memory for object and array\n+  const size_t size = object_size() + array_size(length);\n+  void* const addr = allocator->alloc(size);\n+\n+  \/\/ Placement new array\n+  void* const array_addr = reinterpret_cast<char*>(addr) + object_size();\n+  ::new (array_addr) ArrayT[length];\n+\n+  \/\/ Return pointer to object\n+  return addr;\n+}\n+\n+template <typename ObjectT, typename ArrayT>\n+inline void* XAttachedArray<ObjectT, ArrayT>::alloc(size_t length) {\n+  struct Allocator {\n+    void* alloc(size_t size) const {\n+      return AllocateHeap(size, mtGC);\n+    }\n+  } allocator;\n+  return alloc(&allocator, length);\n+}\n+\n+template <typename ObjectT, typename ArrayT>\n+inline void XAttachedArray<ObjectT, ArrayT>::free(ObjectT* obj) {\n+  FreeHeap(obj);\n+}\n+\n+template <typename ObjectT, typename ArrayT>\n+inline XAttachedArray<ObjectT, ArrayT>::XAttachedArray(size_t length) :\n+    _length(length) {}\n+\n+template <typename ObjectT, typename ArrayT>\n+inline size_t XAttachedArray<ObjectT, ArrayT>::length() const {\n+  return _length;\n+}\n+\n+template <typename ObjectT, typename ArrayT>\n+inline ArrayT* XAttachedArray<ObjectT, ArrayT>::operator()(const ObjectT* obj) const {\n+  return reinterpret_cast<ArrayT*>(reinterpret_cast<uintptr_t>(obj) + object_size());\n+}\n+\n+#endif \/\/ SHARE_GC_X_XATTACHEDARRAY_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xAttachedArray.inline.hpp","additions":86,"deletions":0,"binary":false,"changes":86,"status":"added"},{"patch":"@@ -0,0 +1,275 @@\n+\/*\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"classfile\/javaClasses.hpp\"\n+#include \"gc\/x\/xBarrier.inline.hpp\"\n+#include \"gc\/x\/xHeap.inline.hpp\"\n+#include \"gc\/x\/xOop.inline.hpp\"\n+#include \"gc\/x\/xThread.inline.hpp\"\n+#include \"memory\/iterator.inline.hpp\"\n+#include \"oops\/oop.inline.hpp\"\n+#include \"runtime\/safepoint.hpp\"\n+#include \"utilities\/debug.hpp\"\n+\n+template <bool finalizable>\n+bool XBarrier::should_mark_through(uintptr_t addr) {\n+  \/\/ Finalizable marked oops can still exists on the heap after marking\n+  \/\/ has completed, in which case we just want to convert this into a\n+  \/\/ good oop and not push it on the mark stack.\n+  if (!during_mark()) {\n+    assert(XAddress::is_marked(addr), \"Should be marked\");\n+    assert(XAddress::is_finalizable(addr), \"Should be finalizable\");\n+    return false;\n+  }\n+\n+  \/\/ During marking, we mark through already marked oops to avoid having\n+  \/\/ some large part of the object graph hidden behind a pushed, but not\n+  \/\/ yet flushed, entry on a mutator mark stack. Always marking through\n+  \/\/ allows the GC workers to proceed through the object graph even if a\n+  \/\/ mutator touched an oop first, which in turn will reduce the risk of\n+  \/\/ having to flush mark stacks multiple times to terminate marking.\n+  \/\/\n+  \/\/ However, when doing finalizable marking we don't always want to mark\n+  \/\/ through. First, marking through an already strongly marked oop would\n+  \/\/ be wasteful, since we will then proceed to do finalizable marking on\n+  \/\/ an object which is, or will be, marked strongly. Second, marking\n+  \/\/ through an already finalizable marked oop would also be wasteful,\n+  \/\/ since such oops can never end up on a mutator mark stack and can\n+  \/\/ therefore not hide some part of the object graph from GC workers.\n+  if (finalizable) {\n+    return !XAddress::is_marked(addr);\n+  }\n+\n+  \/\/ Mark through\n+  return true;\n+}\n+\n+template <bool gc_thread, bool follow, bool finalizable, bool publish>\n+uintptr_t XBarrier::mark(uintptr_t addr) {\n+  uintptr_t good_addr;\n+\n+  if (XAddress::is_marked(addr)) {\n+    \/\/ Already marked, but try to mark though anyway\n+    good_addr = XAddress::good(addr);\n+  } else if (XAddress::is_remapped(addr)) {\n+    \/\/ Already remapped, but also needs to be marked\n+    good_addr = XAddress::good(addr);\n+  } else {\n+    \/\/ Needs to be both remapped and marked\n+    good_addr = remap(addr);\n+  }\n+\n+  \/\/ Mark\n+  if (should_mark_through<finalizable>(addr)) {\n+    XHeap::heap()->mark_object<gc_thread, follow, finalizable, publish>(good_addr);\n+  }\n+\n+  if (finalizable) {\n+    \/\/ Make the oop finalizable marked\/good, instead of normal marked\/good.\n+    \/\/ This is needed because an object might first becomes finalizable\n+    \/\/ marked by the GC, and then loaded by a mutator thread. In this case,\n+    \/\/ the mutator thread must be able to tell that the object needs to be\n+    \/\/ strongly marked. The finalizable bit in the oop exists to make sure\n+    \/\/ that a load of a finalizable marked oop will fall into the barrier\n+    \/\/ slow path so that we can mark the object as strongly reachable.\n+    return XAddress::finalizable_good(good_addr);\n+  }\n+\n+  return good_addr;\n+}\n+\n+uintptr_t XBarrier::remap(uintptr_t addr) {\n+  assert(!XAddress::is_good(addr), \"Should not be good\");\n+  assert(!XAddress::is_weak_good(addr), \"Should not be weak good\");\n+  return XHeap::heap()->remap_object(addr);\n+}\n+\n+uintptr_t XBarrier::relocate(uintptr_t addr) {\n+  assert(!XAddress::is_good(addr), \"Should not be good\");\n+  assert(!XAddress::is_weak_good(addr), \"Should not be weak good\");\n+  return XHeap::heap()->relocate_object(addr);\n+}\n+\n+uintptr_t XBarrier::relocate_or_mark(uintptr_t addr) {\n+  return during_relocate() ? relocate(addr) : mark<AnyThread, Follow, Strong, Publish>(addr);\n+}\n+\n+uintptr_t XBarrier::relocate_or_mark_no_follow(uintptr_t addr) {\n+  return during_relocate() ? relocate(addr) : mark<AnyThread, DontFollow, Strong, Publish>(addr);\n+}\n+\n+uintptr_t XBarrier::relocate_or_remap(uintptr_t addr) {\n+  return during_relocate() ? relocate(addr) : remap(addr);\n+}\n+\n+\/\/\n+\/\/ Load barrier\n+\/\/\n+uintptr_t XBarrier::load_barrier_on_oop_slow_path(uintptr_t addr) {\n+  return relocate_or_mark(addr);\n+}\n+\n+uintptr_t XBarrier::load_barrier_on_invisible_root_oop_slow_path(uintptr_t addr) {\n+  return relocate_or_mark_no_follow(addr);\n+}\n+\n+void XBarrier::load_barrier_on_oop_fields(oop o) {\n+  assert(XAddress::is_good(XOop::to_address(o)), \"Should be good\");\n+  XLoadBarrierOopClosure cl;\n+  o->oop_iterate(&cl);\n+}\n+\n+\/\/\n+\/\/ Weak load barrier\n+\/\/\n+uintptr_t XBarrier::weak_load_barrier_on_oop_slow_path(uintptr_t addr) {\n+  return XAddress::is_weak_good(addr) ? XAddress::good(addr) : relocate_or_remap(addr);\n+}\n+\n+uintptr_t XBarrier::weak_load_barrier_on_weak_oop_slow_path(uintptr_t addr) {\n+  const uintptr_t good_addr = weak_load_barrier_on_oop_slow_path(addr);\n+  if (XHeap::heap()->is_object_strongly_live(good_addr)) {\n+    return good_addr;\n+  }\n+\n+  \/\/ Not strongly live\n+  return 0;\n+}\n+\n+uintptr_t XBarrier::weak_load_barrier_on_phantom_oop_slow_path(uintptr_t addr) {\n+  const uintptr_t good_addr = weak_load_barrier_on_oop_slow_path(addr);\n+  if (XHeap::heap()->is_object_live(good_addr)) {\n+    return good_addr;\n+  }\n+\n+  \/\/ Not live\n+  return 0;\n+}\n+\n+\/\/\n+\/\/ Keep alive barrier\n+\/\/\n+uintptr_t XBarrier::keep_alive_barrier_on_oop_slow_path(uintptr_t addr) {\n+  assert(during_mark(), \"Invalid phase\");\n+\n+  \/\/ Mark\n+  return mark<AnyThread, Follow, Strong, Overflow>(addr);\n+}\n+\n+uintptr_t XBarrier::keep_alive_barrier_on_weak_oop_slow_path(uintptr_t addr) {\n+  assert(XResurrection::is_blocked(), \"This operation is only valid when resurrection is blocked\");\n+  const uintptr_t good_addr = weak_load_barrier_on_oop_slow_path(addr);\n+  assert(XHeap::heap()->is_object_strongly_live(good_addr), \"Should be live\");\n+  return good_addr;\n+}\n+\n+uintptr_t XBarrier::keep_alive_barrier_on_phantom_oop_slow_path(uintptr_t addr) {\n+  assert(XResurrection::is_blocked(), \"This operation is only valid when resurrection is blocked\");\n+  const uintptr_t good_addr = weak_load_barrier_on_oop_slow_path(addr);\n+  assert(XHeap::heap()->is_object_live(good_addr), \"Should be live\");\n+  return good_addr;\n+}\n+\n+\/\/\n+\/\/ Mark barrier\n+\/\/\n+uintptr_t XBarrier::mark_barrier_on_oop_slow_path(uintptr_t addr) {\n+  assert(during_mark(), \"Invalid phase\");\n+  assert(XThread::is_worker(), \"Invalid thread\");\n+\n+  \/\/ Mark\n+  return mark<GCThread, Follow, Strong, Overflow>(addr);\n+}\n+\n+uintptr_t XBarrier::mark_barrier_on_finalizable_oop_slow_path(uintptr_t addr) {\n+  assert(during_mark(), \"Invalid phase\");\n+  assert(XThread::is_worker(), \"Invalid thread\");\n+\n+  \/\/ Mark\n+  return mark<GCThread, Follow, Finalizable, Overflow>(addr);\n+}\n+\n+\/\/\n+\/\/ Narrow oop variants, never used.\n+\/\/\n+oop XBarrier::load_barrier_on_oop_field(volatile narrowOop* p) {\n+  ShouldNotReachHere();\n+  return NULL;\n+}\n+\n+oop XBarrier::load_barrier_on_oop_field_preloaded(volatile narrowOop* p, oop o) {\n+  ShouldNotReachHere();\n+  return NULL;\n+}\n+\n+void XBarrier::load_barrier_on_oop_array(volatile narrowOop* p, size_t length) {\n+  ShouldNotReachHere();\n+}\n+\n+oop XBarrier::load_barrier_on_weak_oop_field_preloaded(volatile narrowOop* p, oop o) {\n+  ShouldNotReachHere();\n+  return NULL;\n+}\n+\n+oop XBarrier::load_barrier_on_phantom_oop_field_preloaded(volatile narrowOop* p, oop o) {\n+  ShouldNotReachHere();\n+  return NULL;\n+}\n+\n+oop XBarrier::weak_load_barrier_on_oop_field_preloaded(volatile narrowOop* p, oop o) {\n+  ShouldNotReachHere();\n+  return NULL;\n+}\n+\n+oop XBarrier::weak_load_barrier_on_weak_oop_field_preloaded(volatile narrowOop* p, oop o) {\n+  ShouldNotReachHere();\n+  return NULL;\n+}\n+\n+oop XBarrier::weak_load_barrier_on_phantom_oop_field_preloaded(volatile narrowOop* p, oop o) {\n+  ShouldNotReachHere();\n+  return NULL;\n+}\n+\n+#ifdef ASSERT\n+\n+\/\/ ON_WEAK barriers should only ever be applied to j.l.r.Reference.referents.\n+void XBarrier::verify_on_weak(volatile oop* referent_addr) {\n+  if (referent_addr != NULL) {\n+    uintptr_t base = (uintptr_t)referent_addr - java_lang_ref_Reference::referent_offset();\n+    oop obj = cast_to_oop(base);\n+    assert(oopDesc::is_oop(obj), \"Verification failed for: ref \" PTR_FORMAT \" obj: \" PTR_FORMAT, (uintptr_t)referent_addr, base);\n+    assert(java_lang_ref_Reference::is_referent_field(obj, java_lang_ref_Reference::referent_offset()), \"Sanity\");\n+  }\n+}\n+\n+#endif\n+\n+void XLoadBarrierOopClosure::do_oop(oop* p) {\n+  XBarrier::load_barrier_on_oop_field(p);\n+}\n+\n+void XLoadBarrierOopClosure::do_oop(narrowOop* p) {\n+  ShouldNotReachHere();\n+}\n","filename":"src\/hotspot\/share\/gc\/x\/xBarrier.cpp","additions":275,"deletions":0,"binary":false,"changes":275,"status":"added"},{"patch":"@@ -24,2 +24,2 @@\n-#ifndef SHARE_GC_Z_ZBARRIER_HPP\n-#define SHARE_GC_Z_ZBARRIER_HPP\n+#ifndef SHARE_GC_X_XBARRIER_HPP\n+#define SHARE_GC_X_XBARRIER_HPP\n@@ -31,2 +31,2 @@\n-typedef bool (*ZBarrierFastPath)(uintptr_t);\n-typedef uintptr_t (*ZBarrierSlowPath)(uintptr_t);\n+typedef bool (*XBarrierFastPath)(uintptr_t);\n+typedef uintptr_t (*XBarrierSlowPath)(uintptr_t);\n@@ -34,1 +34,1 @@\n-class ZBarrier : public AllStatic {\n+class XBarrier : public AllStatic {\n@@ -48,1 +48,1 @@\n-  template <ZBarrierFastPath fast_path> static void self_heal(volatile oop* p, uintptr_t addr, uintptr_t heal_addr);\n+  template <XBarrierFastPath fast_path> static void self_heal(volatile oop* p, uintptr_t addr, uintptr_t heal_addr);\n@@ -50,3 +50,3 @@\n-  template <ZBarrierFastPath fast_path, ZBarrierSlowPath slow_path> static oop barrier(volatile oop* p, oop o);\n-  template <ZBarrierFastPath fast_path, ZBarrierSlowPath slow_path> static oop weak_barrier(volatile oop* p, oop o);\n-  template <ZBarrierFastPath fast_path, ZBarrierSlowPath slow_path> static void root_barrier(oop* p, oop o);\n+  template <XBarrierFastPath fast_path, XBarrierSlowPath slow_path> static oop barrier(volatile oop* p, oop o);\n+  template <XBarrierFastPath fast_path, XBarrierSlowPath slow_path> static oop weak_barrier(volatile oop* p, oop o);\n+  template <XBarrierFastPath fast_path, XBarrierSlowPath slow_path> static void root_barrier(oop* p, oop o);\n@@ -129,1 +129,1 @@\n-class ZLoadBarrierOopClosure : public BasicOopIterateClosure {\n+class XLoadBarrierOopClosure : public BasicOopIterateClosure {\n@@ -135,1 +135,1 @@\n-#endif \/\/ SHARE_GC_Z_ZBARRIER_HPP\n+#endif \/\/ SHARE_GC_X_XBARRIER_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xBarrier.hpp","additions":11,"deletions":11,"binary":false,"changes":22,"previous_filename":"src\/hotspot\/share\/gc\/z\/zBarrier.hpp","status":"copied"},{"patch":"@@ -0,0 +1,394 @@\n+\/*\n+ * Copyright (c) 2015, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XBARRIER_INLINE_HPP\n+#define SHARE_GC_X_XBARRIER_INLINE_HPP\n+\n+#include \"gc\/x\/xBarrier.hpp\"\n+\n+#include \"code\/codeCache.hpp\"\n+#include \"gc\/x\/xAddress.inline.hpp\"\n+#include \"gc\/x\/xOop.inline.hpp\"\n+#include \"gc\/x\/xResurrection.inline.hpp\"\n+#include \"oops\/oop.hpp\"\n+#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/continuation.hpp\"\n+\n+\/\/ A self heal must always \"upgrade\" the address metadata bits in\n+\/\/ accordance with the metadata bits state machine, which has the\n+\/\/ valid state transitions as described below (where N is the GC\n+\/\/ cycle).\n+\/\/\n+\/\/ Note the subtleness of overlapping GC cycles. Specifically that\n+\/\/ oops are colored Remapped(N) starting at relocation N and ending\n+\/\/ at marking N + 1.\n+\/\/\n+\/\/              +--- Mark Start\n+\/\/              | +--- Mark End\n+\/\/              | | +--- Relocate Start\n+\/\/              | | | +--- Relocate End\n+\/\/              | | | |\n+\/\/ Marked       |---N---|--N+1--|--N+2--|----\n+\/\/ Finalizable  |---N---|--N+1--|--N+2--|----\n+\/\/ Remapped     ----|---N---|--N+1--|--N+2--|\n+\/\/\n+\/\/ VALID STATE TRANSITIONS\n+\/\/\n+\/\/   Marked(N)           -> Remapped(N)\n+\/\/                       -> Marked(N + 1)\n+\/\/                       -> Finalizable(N + 1)\n+\/\/\n+\/\/   Finalizable(N)      -> Marked(N)\n+\/\/                       -> Remapped(N)\n+\/\/                       -> Marked(N + 1)\n+\/\/                       -> Finalizable(N + 1)\n+\/\/\n+\/\/   Remapped(N)         -> Marked(N + 1)\n+\/\/                       -> Finalizable(N + 1)\n+\/\/\n+\/\/ PHASE VIEW\n+\/\/\n+\/\/ XPhaseMark\n+\/\/   Load & Mark\n+\/\/     Marked(N)         <- Marked(N - 1)\n+\/\/                       <- Finalizable(N - 1)\n+\/\/                       <- Remapped(N - 1)\n+\/\/                       <- Finalizable(N)\n+\/\/\n+\/\/   Mark(Finalizable)\n+\/\/     Finalizable(N)    <- Marked(N - 1)\n+\/\/                       <- Finalizable(N - 1)\n+\/\/                       <- Remapped(N - 1)\n+\/\/\n+\/\/   Load(AS_NO_KEEPALIVE)\n+\/\/     Remapped(N - 1)   <- Marked(N - 1)\n+\/\/                       <- Finalizable(N - 1)\n+\/\/\n+\/\/ XPhaseMarkCompleted (Resurrection blocked)\n+\/\/   Load & Load(ON_WEAK\/PHANTOM_OOP_REF | AS_NO_KEEPALIVE) & KeepAlive\n+\/\/     Marked(N)         <- Marked(N - 1)\n+\/\/                       <- Finalizable(N - 1)\n+\/\/                       <- Remapped(N - 1)\n+\/\/                       <- Finalizable(N)\n+\/\/\n+\/\/   Load(ON_STRONG_OOP_REF | AS_NO_KEEPALIVE)\n+\/\/     Remapped(N - 1)   <- Marked(N - 1)\n+\/\/                       <- Finalizable(N - 1)\n+\/\/\n+\/\/ XPhaseMarkCompleted (Resurrection unblocked)\n+\/\/   Load\n+\/\/     Marked(N)         <- Finalizable(N)\n+\/\/\n+\/\/ XPhaseRelocate\n+\/\/   Load & Load(AS_NO_KEEPALIVE)\n+\/\/     Remapped(N)       <- Marked(N)\n+\/\/                       <- Finalizable(N)\n+\n+template <XBarrierFastPath fast_path>\n+inline void XBarrier::self_heal(volatile oop* p, uintptr_t addr, uintptr_t heal_addr) {\n+  if (heal_addr == 0) {\n+    \/\/ Never heal with null since it interacts badly with reference processing.\n+    \/\/ A mutator clearing an oop would be similar to calling Reference.clear(),\n+    \/\/ which would make the reference non-discoverable or silently dropped\n+    \/\/ by the reference processor.\n+    return;\n+  }\n+\n+  assert(!fast_path(addr), \"Invalid self heal\");\n+  assert(fast_path(heal_addr), \"Invalid self heal\");\n+\n+  for (;;) {\n+    \/\/ Heal\n+    const uintptr_t prev_addr = Atomic::cmpxchg((volatile uintptr_t*)p, addr, heal_addr, memory_order_relaxed);\n+    if (prev_addr == addr) {\n+      \/\/ Success\n+      return;\n+    }\n+\n+    if (fast_path(prev_addr)) {\n+      \/\/ Must not self heal\n+      return;\n+    }\n+\n+    \/\/ The oop location was healed by another barrier, but still needs upgrading.\n+    \/\/ Re-apply healing to make sure the oop is not left with weaker (remapped or\n+    \/\/ finalizable) metadata bits than what this barrier tried to apply.\n+    assert(XAddress::offset(prev_addr) == XAddress::offset(heal_addr), \"Invalid offset\");\n+    addr = prev_addr;\n+  }\n+}\n+\n+template <XBarrierFastPath fast_path, XBarrierSlowPath slow_path>\n+inline oop XBarrier::barrier(volatile oop* p, oop o) {\n+  const uintptr_t addr = XOop::to_address(o);\n+\n+  \/\/ Fast path\n+  if (fast_path(addr)) {\n+    return XOop::from_address(addr);\n+  }\n+\n+  \/\/ Slow path\n+  const uintptr_t good_addr = slow_path(addr);\n+\n+  if (p != NULL) {\n+    self_heal<fast_path>(p, addr, good_addr);\n+  }\n+\n+  return XOop::from_address(good_addr);\n+}\n+\n+template <XBarrierFastPath fast_path, XBarrierSlowPath slow_path>\n+inline oop XBarrier::weak_barrier(volatile oop* p, oop o) {\n+  const uintptr_t addr = XOop::to_address(o);\n+\n+  \/\/ Fast path\n+  if (fast_path(addr)) {\n+    \/\/ Return the good address instead of the weak good address\n+    \/\/ to ensure that the currently active heap view is used.\n+    return XOop::from_address(XAddress::good_or_null(addr));\n+  }\n+\n+  \/\/ Slow path\n+  const uintptr_t good_addr = slow_path(addr);\n+\n+  if (p != NULL) {\n+    \/\/ The slow path returns a good\/marked address or null, but we never mark\n+    \/\/ oops in a weak load barrier so we always heal with the remapped address.\n+    self_heal<fast_path>(p, addr, XAddress::remapped_or_null(good_addr));\n+  }\n+\n+  return XOop::from_address(good_addr);\n+}\n+\n+template <XBarrierFastPath fast_path, XBarrierSlowPath slow_path>\n+inline void XBarrier::root_barrier(oop* p, oop o) {\n+  const uintptr_t addr = XOop::to_address(o);\n+\n+  \/\/ Fast path\n+  if (fast_path(addr)) {\n+    return;\n+  }\n+\n+  \/\/ Slow path\n+  const uintptr_t good_addr = slow_path(addr);\n+\n+  \/\/ Non-atomic healing helps speed up root scanning. This is safe to do\n+  \/\/ since we are always healing roots in a safepoint, or under a lock,\n+  \/\/ which ensures we are never racing with mutators modifying roots while\n+  \/\/ we are healing them. It's also safe in case multiple GC threads try\n+  \/\/ to heal the same root if it is aligned, since they would always heal\n+  \/\/ the root in the same way and it does not matter in which order it\n+  \/\/ happens. For misaligned oops, there needs to be mutual exclusion.\n+  *p = XOop::from_address(good_addr);\n+}\n+\n+inline bool XBarrier::is_good_or_null_fast_path(uintptr_t addr) {\n+  return XAddress::is_good_or_null(addr);\n+}\n+\n+inline bool XBarrier::is_weak_good_or_null_fast_path(uintptr_t addr) {\n+  return XAddress::is_weak_good_or_null(addr);\n+}\n+\n+inline bool XBarrier::is_marked_or_null_fast_path(uintptr_t addr) {\n+  return XAddress::is_marked_or_null(addr);\n+}\n+\n+inline bool XBarrier::during_mark() {\n+  return XGlobalPhase == XPhaseMark;\n+}\n+\n+inline bool XBarrier::during_relocate() {\n+  return XGlobalPhase == XPhaseRelocate;\n+}\n+\n+\/\/\n+\/\/ Load barrier\n+\/\/\n+inline oop XBarrier::load_barrier_on_oop(oop o) {\n+  return load_barrier_on_oop_field_preloaded((oop*)NULL, o);\n+}\n+\n+inline oop XBarrier::load_barrier_on_oop_field(volatile oop* p) {\n+  const oop o = Atomic::load(p);\n+  return load_barrier_on_oop_field_preloaded(p, o);\n+}\n+\n+inline oop XBarrier::load_barrier_on_oop_field_preloaded(volatile oop* p, oop o) {\n+  return barrier<is_good_or_null_fast_path, load_barrier_on_oop_slow_path>(p, o);\n+}\n+\n+inline void XBarrier::load_barrier_on_oop_array(volatile oop* p, size_t length) {\n+  for (volatile const oop* const end = p + length; p < end; p++) {\n+    load_barrier_on_oop_field(p);\n+  }\n+}\n+\n+inline oop XBarrier::load_barrier_on_weak_oop_field_preloaded(volatile oop* p, oop o) {\n+  verify_on_weak(p);\n+\n+  if (XResurrection::is_blocked()) {\n+    return barrier<is_good_or_null_fast_path, weak_load_barrier_on_weak_oop_slow_path>(p, o);\n+  }\n+\n+  return load_barrier_on_oop_field_preloaded(p, o);\n+}\n+\n+inline oop XBarrier::load_barrier_on_phantom_oop_field_preloaded(volatile oop* p, oop o) {\n+  if (XResurrection::is_blocked()) {\n+    return barrier<is_good_or_null_fast_path, weak_load_barrier_on_phantom_oop_slow_path>(p, o);\n+  }\n+\n+  return load_barrier_on_oop_field_preloaded(p, o);\n+}\n+\n+inline void XBarrier::load_barrier_on_root_oop_field(oop* p) {\n+  const oop o = *p;\n+  root_barrier<is_good_or_null_fast_path, load_barrier_on_oop_slow_path>(p, o);\n+}\n+\n+inline void XBarrier::load_barrier_on_invisible_root_oop_field(oop* p) {\n+  const oop o = *p;\n+  root_barrier<is_good_or_null_fast_path, load_barrier_on_invisible_root_oop_slow_path>(p, o);\n+}\n+\n+\/\/\n+\/\/ Weak load barrier\n+\/\/\n+inline oop XBarrier::weak_load_barrier_on_oop_field(volatile oop* p) {\n+  assert(!XResurrection::is_blocked(), \"Should not be called during resurrection blocked phase\");\n+  const oop o = Atomic::load(p);\n+  return weak_load_barrier_on_oop_field_preloaded(p, o);\n+}\n+\n+inline oop XBarrier::weak_load_barrier_on_oop_field_preloaded(volatile oop* p, oop o) {\n+  return weak_barrier<is_weak_good_or_null_fast_path, weak_load_barrier_on_oop_slow_path>(p, o);\n+}\n+\n+inline oop XBarrier::weak_load_barrier_on_weak_oop(oop o) {\n+  return weak_load_barrier_on_weak_oop_field_preloaded((oop*)NULL, o);\n+}\n+\n+inline oop XBarrier::weak_load_barrier_on_weak_oop_field_preloaded(volatile oop* p, oop o) {\n+  verify_on_weak(p);\n+\n+  if (XResurrection::is_blocked()) {\n+    return barrier<is_good_or_null_fast_path, weak_load_barrier_on_weak_oop_slow_path>(p, o);\n+  }\n+\n+  return weak_load_barrier_on_oop_field_preloaded(p, o);\n+}\n+\n+inline oop XBarrier::weak_load_barrier_on_phantom_oop(oop o) {\n+  return weak_load_barrier_on_phantom_oop_field_preloaded((oop*)NULL, o);\n+}\n+\n+inline oop XBarrier::weak_load_barrier_on_phantom_oop_field_preloaded(volatile oop* p, oop o) {\n+  if (XResurrection::is_blocked()) {\n+    return barrier<is_good_or_null_fast_path, weak_load_barrier_on_phantom_oop_slow_path>(p, o);\n+  }\n+\n+  return weak_load_barrier_on_oop_field_preloaded(p, o);\n+}\n+\n+\/\/\n+\/\/ Is alive barrier\n+\/\/\n+inline bool XBarrier::is_alive_barrier_on_weak_oop(oop o) {\n+  \/\/ Check if oop is logically non-null. This operation\n+  \/\/ is only valid when resurrection is blocked.\n+  assert(XResurrection::is_blocked(), \"Invalid phase\");\n+  return weak_load_barrier_on_weak_oop(o) != NULL;\n+}\n+\n+inline bool XBarrier::is_alive_barrier_on_phantom_oop(oop o) {\n+  \/\/ Check if oop is logically non-null. This operation\n+  \/\/ is only valid when resurrection is blocked.\n+  assert(XResurrection::is_blocked(), \"Invalid phase\");\n+  return weak_load_barrier_on_phantom_oop(o) != NULL;\n+}\n+\n+\/\/\n+\/\/ Keep alive barrier\n+\/\/\n+inline void XBarrier::keep_alive_barrier_on_weak_oop_field(volatile oop* p) {\n+  assert(XResurrection::is_blocked(), \"This operation is only valid when resurrection is blocked\");\n+  const oop o = Atomic::load(p);\n+  barrier<is_good_or_null_fast_path, keep_alive_barrier_on_weak_oop_slow_path>(p, o);\n+}\n+\n+inline void XBarrier::keep_alive_barrier_on_phantom_oop_field(volatile oop* p) {\n+  assert(XResurrection::is_blocked(), \"This operation is only valid when resurrection is blocked\");\n+  const oop o = Atomic::load(p);\n+  barrier<is_good_or_null_fast_path, keep_alive_barrier_on_phantom_oop_slow_path>(p, o);\n+}\n+\n+inline void XBarrier::keep_alive_barrier_on_phantom_root_oop_field(oop* p) {\n+  \/\/ The keep alive operation is only valid when resurrection is blocked.\n+  \/\/\n+  \/\/ Except with Loom, where we intentionally trigger arms nmethods after\n+  \/\/ unlinking, to get a sense of what nmethods are alive. This will trigger\n+  \/\/ the keep alive barriers, but the oops are healed and the slow-paths\n+  \/\/ will not trigger. We have stronger checks in the slow-paths.\n+  assert(XResurrection::is_blocked() || (CodeCache::contains((void*)p)),\n+         \"This operation is only valid when resurrection is blocked\");\n+  const oop o = *p;\n+  root_barrier<is_good_or_null_fast_path, keep_alive_barrier_on_phantom_oop_slow_path>(p, o);\n+}\n+\n+inline void XBarrier::keep_alive_barrier_on_oop(oop o) {\n+  const uintptr_t addr = XOop::to_address(o);\n+  assert(XAddress::is_good(addr), \"Invalid address\");\n+\n+  if (during_mark()) {\n+    keep_alive_barrier_on_oop_slow_path(addr);\n+  }\n+}\n+\n+\/\/\n+\/\/ Mark barrier\n+\/\/\n+inline void XBarrier::mark_barrier_on_oop_field(volatile oop* p, bool finalizable) {\n+  const oop o = Atomic::load(p);\n+\n+  if (finalizable) {\n+    barrier<is_marked_or_null_fast_path, mark_barrier_on_finalizable_oop_slow_path>(p, o);\n+  } else {\n+    const uintptr_t addr = XOop::to_address(o);\n+    if (XAddress::is_good(addr)) {\n+      \/\/ Mark through good oop\n+      mark_barrier_on_oop_slow_path(addr);\n+    } else {\n+      \/\/ Mark through bad oop\n+      barrier<is_good_or_null_fast_path, mark_barrier_on_oop_slow_path>(p, o);\n+    }\n+  }\n+}\n+\n+inline void XBarrier::mark_barrier_on_oop_array(volatile oop* p, size_t length, bool finalizable) {\n+  for (volatile const oop* const end = p + length; p < end; p++) {\n+    mark_barrier_on_oop_field(p, finalizable);\n+  }\n+}\n+\n+#endif \/\/ SHARE_GC_X_XBARRIER_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xBarrier.inline.hpp","additions":394,"deletions":0,"binary":false,"changes":394,"status":"added"},{"patch":"@@ -0,0 +1,99 @@\n+\/*\n+ * Copyright (c) 2018, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/x\/xBarrierSet.hpp\"\n+#include \"gc\/x\/xBarrierSetAssembler.hpp\"\n+#include \"gc\/x\/xBarrierSetNMethod.hpp\"\n+#include \"gc\/x\/xBarrierSetStackChunk.hpp\"\n+#include \"gc\/x\/xGlobals.hpp\"\n+#include \"gc\/x\/xHeap.inline.hpp\"\n+#include \"gc\/x\/xStackWatermark.hpp\"\n+#include \"gc\/x\/xThreadLocalData.hpp\"\n+#include \"runtime\/javaThread.hpp\"\n+#include \"utilities\/macros.hpp\"\n+#ifdef COMPILER1\n+#include \"gc\/x\/c1\/xBarrierSetC1.hpp\"\n+#endif\n+#ifdef COMPILER2\n+#include \"gc\/x\/c2\/xBarrierSetC2.hpp\"\n+#endif\n+\n+class XBarrierSetC1;\n+class XBarrierSetC2;\n+\n+XBarrierSet::XBarrierSet() :\n+    BarrierSet(make_barrier_set_assembler<XBarrierSetAssembler>(),\n+               make_barrier_set_c1<XBarrierSetC1>(),\n+               make_barrier_set_c2<XBarrierSetC2>(),\n+               new XBarrierSetNMethod(),\n+               new XBarrierSetStackChunk(),\n+               BarrierSet::FakeRtti(BarrierSet::XBarrierSet)) {}\n+\n+XBarrierSetAssembler* XBarrierSet::assembler() {\n+  BarrierSetAssembler* const bsa = BarrierSet::barrier_set()->barrier_set_assembler();\n+  return reinterpret_cast<XBarrierSetAssembler*>(bsa);\n+}\n+\n+bool XBarrierSet::barrier_needed(DecoratorSet decorators, BasicType type) {\n+  assert((decorators & AS_RAW) == 0, \"Unexpected decorator\");\n+  \/\/assert((decorators & ON_UNKNOWN_OOP_REF) == 0, \"Unexpected decorator\");\n+\n+  if (is_reference_type(type)) {\n+    assert((decorators & (IN_HEAP | IN_NATIVE)) != 0, \"Where is reference?\");\n+    \/\/ Barrier needed even when IN_NATIVE, to allow concurrent scanning.\n+    return true;\n+  }\n+\n+  \/\/ Barrier not needed\n+  return false;\n+}\n+\n+void XBarrierSet::on_thread_create(Thread* thread) {\n+  \/\/ Create thread local data\n+  XThreadLocalData::create(thread);\n+}\n+\n+void XBarrierSet::on_thread_destroy(Thread* thread) {\n+  \/\/ Destroy thread local data\n+  XThreadLocalData::destroy(thread);\n+}\n+\n+void XBarrierSet::on_thread_attach(Thread* thread) {\n+  \/\/ Set thread local address bad mask\n+  XThreadLocalData::set_address_bad_mask(thread, XAddressBadMask);\n+  if (thread->is_Java_thread()) {\n+    JavaThread* const jt = JavaThread::cast(thread);\n+    StackWatermark* const watermark = new XStackWatermark(jt);\n+    StackWatermarkSet::add_watermark(jt, watermark);\n+  }\n+}\n+\n+void XBarrierSet::on_thread_detach(Thread* thread) {\n+  \/\/ Flush and free any remaining mark stacks\n+  XHeap::heap()->mark_flush_and_free(thread);\n+}\n+\n+void XBarrierSet::print_on(outputStream* st) const {\n+  st->print_cr(\"XBarrierSet\");\n+}\n","filename":"src\/hotspot\/share\/gc\/x\/xBarrierSet.cpp","additions":99,"deletions":0,"binary":false,"changes":99,"status":"added"},{"patch":"@@ -0,0 +1,109 @@\n+\/*\n+ * Copyright (c) 2015, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XBARRIERSET_HPP\n+#define SHARE_GC_X_XBARRIERSET_HPP\n+\n+#include \"gc\/shared\/barrierSet.hpp\"\n+\n+class XBarrierSetAssembler;\n+\n+class XBarrierSet : public BarrierSet {\n+public:\n+  XBarrierSet();\n+\n+  static XBarrierSetAssembler* assembler();\n+  static bool barrier_needed(DecoratorSet decorators, BasicType type);\n+\n+  virtual void on_thread_create(Thread* thread);\n+  virtual void on_thread_destroy(Thread* thread);\n+  virtual void on_thread_attach(Thread* thread);\n+  virtual void on_thread_detach(Thread* thread);\n+\n+  virtual void print_on(outputStream* st) const;\n+\n+  template <DecoratorSet decorators, typename BarrierSetT = XBarrierSet>\n+  class AccessBarrier : public BarrierSet::AccessBarrier<decorators, BarrierSetT> {\n+  private:\n+    typedef BarrierSet::AccessBarrier<decorators, BarrierSetT> Raw;\n+\n+    template <DecoratorSet expected>\n+    static void verify_decorators_present();\n+\n+    template <DecoratorSet expected>\n+    static void verify_decorators_absent();\n+\n+    static oop* field_addr(oop base, ptrdiff_t offset);\n+\n+    template <typename T>\n+    static oop load_barrier_on_oop_field_preloaded(T* addr, oop o);\n+\n+    template <typename T>\n+    static oop load_barrier_on_unknown_oop_field_preloaded(oop base, ptrdiff_t offset, T* addr, oop o);\n+\n+  public:\n+    \/\/\n+    \/\/ In heap\n+    \/\/\n+    template <typename T>\n+    static oop oop_load_in_heap(T* addr);\n+    static oop oop_load_in_heap_at(oop base, ptrdiff_t offset);\n+\n+    template <typename T>\n+    static oop oop_atomic_cmpxchg_in_heap(T* addr, oop compare_value, oop new_value);\n+    static oop oop_atomic_cmpxchg_in_heap_at(oop base, ptrdiff_t offset, oop compare_value, oop new_value);\n+\n+    template <typename T>\n+    static oop oop_atomic_xchg_in_heap(T* addr, oop new_value);\n+    static oop oop_atomic_xchg_in_heap_at(oop base, ptrdiff_t offset, oop new_value);\n+\n+    template <typename T>\n+    static bool oop_arraycopy_in_heap(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,\n+                                      arrayOop dst_obj, size_t dst_offset_in_bytes, T* dst_raw,\n+                                      size_t length);\n+\n+    static void clone_in_heap(oop src, oop dst, size_t size);\n+\n+    \/\/\n+    \/\/ Not in heap\n+    \/\/\n+    template <typename T>\n+    static oop oop_load_not_in_heap(T* addr);\n+\n+    template <typename T>\n+    static oop oop_atomic_cmpxchg_not_in_heap(T* addr, oop compare_value, oop new_value);\n+\n+    template <typename T>\n+    static oop oop_atomic_xchg_not_in_heap(T* addr, oop new_value);\n+  };\n+};\n+\n+template<> struct BarrierSet::GetName<XBarrierSet> {\n+  static const BarrierSet::Name value = BarrierSet::XBarrierSet;\n+};\n+\n+template<> struct BarrierSet::GetType<BarrierSet::XBarrierSet> {\n+  typedef ::XBarrierSet type;\n+};\n+\n+#endif \/\/ SHARE_GC_X_XBARRIERSET_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xBarrierSet.hpp","additions":109,"deletions":0,"binary":false,"changes":109,"status":"added"},{"patch":"@@ -0,0 +1,242 @@\n+\/*\n+ * Copyright (c) 2017, 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XBARRIERSET_INLINE_HPP\n+#define SHARE_GC_X_XBARRIERSET_INLINE_HPP\n+\n+#include \"gc\/x\/xBarrierSet.hpp\"\n+\n+#include \"gc\/shared\/accessBarrierSupport.inline.hpp\"\n+#include \"gc\/x\/xBarrier.inline.hpp\"\n+#include \"utilities\/debug.hpp\"\n+\n+template <DecoratorSet decorators, typename BarrierSetT>\n+template <DecoratorSet expected>\n+inline void XBarrierSet::AccessBarrier<decorators, BarrierSetT>::verify_decorators_present() {\n+  if ((decorators & expected) == 0) {\n+    fatal(\"Using unsupported access decorators\");\n+  }\n+}\n+\n+template <DecoratorSet decorators, typename BarrierSetT>\n+template <DecoratorSet expected>\n+inline void XBarrierSet::AccessBarrier<decorators, BarrierSetT>::verify_decorators_absent() {\n+  if ((decorators & expected) != 0) {\n+    fatal(\"Using unsupported access decorators\");\n+  }\n+}\n+\n+template <DecoratorSet decorators, typename BarrierSetT>\n+inline oop* XBarrierSet::AccessBarrier<decorators, BarrierSetT>::field_addr(oop base, ptrdiff_t offset) {\n+  assert(base != NULL, \"Invalid base\");\n+  return reinterpret_cast<oop*>(reinterpret_cast<intptr_t>((void*)base) + offset);\n+}\n+\n+template <DecoratorSet decorators, typename BarrierSetT>\n+template <typename T>\n+inline oop XBarrierSet::AccessBarrier<decorators, BarrierSetT>::load_barrier_on_oop_field_preloaded(T* addr, oop o) {\n+  verify_decorators_absent<ON_UNKNOWN_OOP_REF>();\n+\n+  if (HasDecorator<decorators, AS_NO_KEEPALIVE>::value) {\n+    if (HasDecorator<decorators, ON_STRONG_OOP_REF>::value) {\n+      return XBarrier::weak_load_barrier_on_oop_field_preloaded(addr, o);\n+    } else if (HasDecorator<decorators, ON_WEAK_OOP_REF>::value) {\n+      return XBarrier::weak_load_barrier_on_weak_oop_field_preloaded(addr, o);\n+    } else {\n+      assert((HasDecorator<decorators, ON_PHANTOM_OOP_REF>::value), \"Must be\");\n+      return XBarrier::weak_load_barrier_on_phantom_oop_field_preloaded(addr, o);\n+    }\n+  } else {\n+    if (HasDecorator<decorators, ON_STRONG_OOP_REF>::value) {\n+      return XBarrier::load_barrier_on_oop_field_preloaded(addr, o);\n+    } else if (HasDecorator<decorators, ON_WEAK_OOP_REF>::value) {\n+      return XBarrier::load_barrier_on_weak_oop_field_preloaded(addr, o);\n+    } else {\n+      assert((HasDecorator<decorators, ON_PHANTOM_OOP_REF>::value), \"Must be\");\n+      return XBarrier::load_barrier_on_phantom_oop_field_preloaded(addr, o);\n+    }\n+  }\n+}\n+\n+template <DecoratorSet decorators, typename BarrierSetT>\n+template <typename T>\n+inline oop XBarrierSet::AccessBarrier<decorators, BarrierSetT>::load_barrier_on_unknown_oop_field_preloaded(oop base, ptrdiff_t offset, T* addr, oop o) {\n+  verify_decorators_present<ON_UNKNOWN_OOP_REF>();\n+\n+  const DecoratorSet decorators_known_strength =\n+    AccessBarrierSupport::resolve_possibly_unknown_oop_ref_strength<decorators>(base, offset);\n+\n+  if (HasDecorator<decorators, AS_NO_KEEPALIVE>::value) {\n+    if (decorators_known_strength & ON_STRONG_OOP_REF) {\n+      return XBarrier::weak_load_barrier_on_oop_field_preloaded(addr, o);\n+    } else if (decorators_known_strength & ON_WEAK_OOP_REF) {\n+      return XBarrier::weak_load_barrier_on_weak_oop_field_preloaded(addr, o);\n+    } else {\n+      assert(decorators_known_strength & ON_PHANTOM_OOP_REF, \"Must be\");\n+      return XBarrier::weak_load_barrier_on_phantom_oop_field_preloaded(addr, o);\n+    }\n+  } else {\n+    if (decorators_known_strength & ON_STRONG_OOP_REF) {\n+      return XBarrier::load_barrier_on_oop_field_preloaded(addr, o);\n+    } else if (decorators_known_strength & ON_WEAK_OOP_REF) {\n+      return XBarrier::load_barrier_on_weak_oop_field_preloaded(addr, o);\n+    } else {\n+      assert(decorators_known_strength & ON_PHANTOM_OOP_REF, \"Must be\");\n+      return XBarrier::load_barrier_on_phantom_oop_field_preloaded(addr, o);\n+    }\n+  }\n+}\n+\n+\/\/\n+\/\/ In heap\n+\/\/\n+template <DecoratorSet decorators, typename BarrierSetT>\n+template <typename T>\n+inline oop XBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_load_in_heap(T* addr) {\n+  verify_decorators_absent<ON_UNKNOWN_OOP_REF>();\n+\n+  const oop o = Raw::oop_load_in_heap(addr);\n+  return load_barrier_on_oop_field_preloaded(addr, o);\n+}\n+\n+template <DecoratorSet decorators, typename BarrierSetT>\n+inline oop XBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_load_in_heap_at(oop base, ptrdiff_t offset) {\n+  oop* const addr = field_addr(base, offset);\n+  const oop o = Raw::oop_load_in_heap(addr);\n+\n+  if (HasDecorator<decorators, ON_UNKNOWN_OOP_REF>::value) {\n+    return load_barrier_on_unknown_oop_field_preloaded(base, offset, addr, o);\n+  }\n+\n+  return load_barrier_on_oop_field_preloaded(addr, o);\n+}\n+\n+template <DecoratorSet decorators, typename BarrierSetT>\n+template <typename T>\n+inline oop XBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_atomic_cmpxchg_in_heap(T* addr, oop compare_value, oop new_value) {\n+  verify_decorators_present<ON_STRONG_OOP_REF>();\n+  verify_decorators_absent<AS_NO_KEEPALIVE>();\n+\n+  XBarrier::load_barrier_on_oop_field(addr);\n+  return Raw::oop_atomic_cmpxchg_in_heap(addr, compare_value, new_value);\n+}\n+\n+template <DecoratorSet decorators, typename BarrierSetT>\n+inline oop XBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_atomic_cmpxchg_in_heap_at(oop base, ptrdiff_t offset, oop compare_value, oop new_value) {\n+  verify_decorators_present<ON_STRONG_OOP_REF | ON_UNKNOWN_OOP_REF>();\n+  verify_decorators_absent<AS_NO_KEEPALIVE>();\n+\n+  \/\/ Through Unsafe.CompareAndExchangeObject()\/CompareAndSetObject() we can receive\n+  \/\/ calls with ON_UNKNOWN_OOP_REF set. However, we treat these as ON_STRONG_OOP_REF,\n+  \/\/ with the motivation that if you're doing Unsafe operations on a Reference.referent\n+  \/\/ field, then you're on your own anyway.\n+  XBarrier::load_barrier_on_oop_field(field_addr(base, offset));\n+  return Raw::oop_atomic_cmpxchg_in_heap_at(base, offset, compare_value, new_value);\n+}\n+\n+template <DecoratorSet decorators, typename BarrierSetT>\n+template <typename T>\n+inline oop XBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_atomic_xchg_in_heap(T* addr, oop new_value) {\n+  verify_decorators_present<ON_STRONG_OOP_REF>();\n+  verify_decorators_absent<AS_NO_KEEPALIVE>();\n+\n+  const oop o = Raw::oop_atomic_xchg_in_heap(addr, new_value);\n+  return XBarrier::load_barrier_on_oop(o);\n+}\n+\n+template <DecoratorSet decorators, typename BarrierSetT>\n+inline oop XBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_atomic_xchg_in_heap_at(oop base, ptrdiff_t offset, oop new_value) {\n+  verify_decorators_present<ON_STRONG_OOP_REF>();\n+  verify_decorators_absent<AS_NO_KEEPALIVE>();\n+\n+  const oop o = Raw::oop_atomic_xchg_in_heap_at(base, offset, new_value);\n+  return XBarrier::load_barrier_on_oop(o);\n+}\n+\n+template <DecoratorSet decorators, typename BarrierSetT>\n+template <typename T>\n+inline bool XBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_arraycopy_in_heap(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,\n+                                                                                       arrayOop dst_obj, size_t dst_offset_in_bytes, T* dst_raw,\n+                                                                                       size_t length) {\n+  T* src = arrayOopDesc::obj_offset_to_raw(src_obj, src_offset_in_bytes, src_raw);\n+  T* dst = arrayOopDesc::obj_offset_to_raw(dst_obj, dst_offset_in_bytes, dst_raw);\n+\n+  if (!HasDecorator<decorators, ARRAYCOPY_CHECKCAST>::value) {\n+    \/\/ No check cast, bulk barrier and bulk copy\n+    XBarrier::load_barrier_on_oop_array(src, length);\n+    return Raw::oop_arraycopy_in_heap(NULL, 0, src, NULL, 0, dst, length);\n+  }\n+\n+  \/\/ Check cast and copy each elements\n+  Klass* const dst_klass = objArrayOop(dst_obj)->element_klass();\n+  for (const T* const end = src + length; src < end; src++, dst++) {\n+    const oop elem = XBarrier::load_barrier_on_oop_field(src);\n+    if (!oopDesc::is_instanceof_or_null(elem, dst_klass)) {\n+      \/\/ Check cast failed\n+      return false;\n+    }\n+\n+    \/\/ Cast is safe, since we know it's never a narrowOop\n+    *(oop*)dst = elem;\n+  }\n+\n+  return true;\n+}\n+\n+template <DecoratorSet decorators, typename BarrierSetT>\n+inline void XBarrierSet::AccessBarrier<decorators, BarrierSetT>::clone_in_heap(oop src, oop dst, size_t size) {\n+  XBarrier::load_barrier_on_oop_fields(src);\n+  Raw::clone_in_heap(src, dst, size);\n+}\n+\n+\/\/\n+\/\/ Not in heap\n+\/\/\n+template <DecoratorSet decorators, typename BarrierSetT>\n+template <typename T>\n+inline oop XBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_load_not_in_heap(T* addr) {\n+  verify_decorators_absent<ON_UNKNOWN_OOP_REF>();\n+\n+  const oop o = Raw::oop_load_not_in_heap(addr);\n+  return load_barrier_on_oop_field_preloaded(addr, o);\n+}\n+\n+template <DecoratorSet decorators, typename BarrierSetT>\n+template <typename T>\n+inline oop XBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_atomic_cmpxchg_not_in_heap(T* addr, oop compare_value, oop new_value) {\n+  verify_decorators_present<ON_STRONG_OOP_REF>();\n+  verify_decorators_absent<AS_NO_KEEPALIVE>();\n+\n+  return Raw::oop_atomic_cmpxchg_not_in_heap(addr, compare_value, new_value);\n+}\n+\n+template <DecoratorSet decorators, typename BarrierSetT>\n+template <typename T>\n+inline oop XBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_atomic_xchg_not_in_heap(T* addr, oop new_value) {\n+  verify_decorators_present<ON_STRONG_OOP_REF>();\n+  verify_decorators_absent<AS_NO_KEEPALIVE>();\n+\n+  return Raw::oop_atomic_xchg_not_in_heap(addr, new_value);\n+}\n+\n+#endif \/\/ SHARE_GC_X_XBARRIERSET_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xBarrierSet.inline.hpp","additions":242,"deletions":0,"binary":false,"changes":242,"status":"added"},{"patch":"@@ -0,0 +1,35 @@\n+\/*\n+ * Copyright (c) 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/x\/xBarrierSetAssembler.hpp\"\n+#include \"gc\/x\/xThreadLocalData.hpp\"\n+#include \"runtime\/javaThread.hpp\"\n+\n+Address XBarrierSetAssemblerBase::address_bad_mask_from_thread(Register thread) {\n+  return Address(thread, XThreadLocalData::address_bad_mask_offset());\n+}\n+\n+Address XBarrierSetAssemblerBase::address_bad_mask_from_jni_env(Register env) {\n+  return Address(env, XThreadLocalData::address_bad_mask_offset() - JavaThread::jni_environment_offset());\n+}\n","filename":"src\/hotspot\/share\/gc\/x\/xBarrierSetAssembler.cpp","additions":35,"deletions":0,"binary":false,"changes":35,"status":"added"},{"patch":"@@ -0,0 +1,39 @@\n+\/*\n+ * Copyright (c) 2018, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XBARRIERSETASSEMBLER_HPP\n+#define SHARE_GC_X_XBARRIERSETASSEMBLER_HPP\n+\n+#include \"gc\/shared\/barrierSetAssembler.hpp\"\n+#include \"utilities\/macros.hpp\"\n+\n+class XBarrierSetAssemblerBase : public BarrierSetAssembler {\n+public:\n+  static Address address_bad_mask_from_thread(Register thread);\n+  static Address address_bad_mask_from_jni_env(Register env);\n+};\n+\n+\/\/ Needs to be included after definition of XBarrierSetAssemblerBase\n+#include CPU_HEADER(gc\/x\/xBarrierSetAssembler)\n+\n+#endif \/\/ SHARE_GC_X_XBARRIERSETASSEMBLER_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xBarrierSetAssembler.hpp","additions":39,"deletions":0,"binary":false,"changes":39,"status":"added"},{"patch":"@@ -0,0 +1,77 @@\n+\/*\n+ * Copyright (c) 2018, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"code\/nmethod.hpp\"\n+#include \"gc\/x\/xBarrierSetNMethod.hpp\"\n+#include \"gc\/x\/xGlobals.hpp\"\n+#include \"gc\/x\/xLock.inline.hpp\"\n+#include \"gc\/x\/xNMethod.hpp\"\n+#include \"gc\/x\/xThreadLocalData.hpp\"\n+#include \"logging\/log.hpp\"\n+#include \"runtime\/threadWXSetters.inline.hpp\"\n+\n+bool XBarrierSetNMethod::nmethod_entry_barrier(nmethod* nm) {\n+  XLocker<XReentrantLock> locker(XNMethod::lock_for_nmethod(nm));\n+  log_trace(nmethod, barrier)(\"Entered critical zone for %p\", nm);\n+\n+  if (!is_armed(nm)) {\n+    \/\/ Some other thread got here first and healed the oops\n+    \/\/ and disarmed the nmethod.\n+    return true;\n+  }\n+\n+  MACOS_AARCH64_ONLY(ThreadWXEnable wx(WXWrite, Thread::current()));\n+\n+  if (nm->is_unloading()) {\n+    \/\/ We don't need to take the lock when unlinking nmethods from\n+    \/\/ the Method, because it is only concurrently unlinked by\n+    \/\/ the entry barrier, which acquires the per nmethod lock.\n+    nm->unlink_from_method();\n+\n+    \/\/ We can end up calling nmethods that are unloading\n+    \/\/ since we clear compiled ICs lazily. Returning false\n+    \/\/ will re-resovle the call and update the compiled IC.\n+    return false;\n+  }\n+\n+  \/\/ Heal oops\n+  XNMethod::nmethod_oops_barrier(nm);\n+\n+\n+  \/\/ CodeCache unloading support\n+  nm->mark_as_maybe_on_stack();\n+\n+  \/\/ Disarm\n+  disarm(nm);\n+\n+  return true;\n+}\n+\n+int* XBarrierSetNMethod::disarmed_guard_value_address() const {\n+  return (int*)XAddressBadMaskHighOrderBitsAddr;\n+}\n+\n+ByteSize XBarrierSetNMethod::thread_disarmed_guard_value_offset() const {\n+  return XThreadLocalData::nmethod_disarmed_offset();\n+}\n","filename":"src\/hotspot\/share\/gc\/x\/xBarrierSetNMethod.cpp","additions":77,"deletions":0,"binary":false,"changes":77,"status":"added"},{"patch":"@@ -0,0 +1,41 @@\n+\/*\n+ * Copyright (c) 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XBARRIERSETNMETHOD_HPP\n+#define SHARE_GC_X_XBARRIERSETNMETHOD_HPP\n+\n+#include \"gc\/shared\/barrierSetNMethod.hpp\"\n+#include \"memory\/allocation.hpp\"\n+\n+class nmethod;\n+\n+class XBarrierSetNMethod : public BarrierSetNMethod {\n+protected:\n+  virtual bool nmethod_entry_barrier(nmethod* nm);\n+\n+public:\n+  virtual ByteSize thread_disarmed_guard_value_offset() const;\n+  virtual int* disarmed_guard_value_address() const;\n+};\n+\n+#endif \/\/ SHARE_GC_X_XBARRIERSETNMETHOD_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xBarrierSetNMethod.hpp","additions":41,"deletions":0,"binary":false,"changes":41,"status":"added"},{"patch":"@@ -0,0 +1,114 @@\n+\/*\n+ * Copyright (c) 2018, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/x\/xBarrier.inline.hpp\"\n+#include \"gc\/x\/xBarrierSetRuntime.hpp\"\n+#include \"oops\/access.hpp\"\n+#include \"runtime\/interfaceSupport.inline.hpp\"\n+\n+JRT_LEAF(oopDesc*, XBarrierSetRuntime::load_barrier_on_oop_field_preloaded(oopDesc* o, oop* p))\n+  return XBarrier::load_barrier_on_oop_field_preloaded(p, o);\n+JRT_END\n+\n+JRT_LEAF(oopDesc*, XBarrierSetRuntime::weak_load_barrier_on_oop_field_preloaded(oopDesc* o, oop* p))\n+  return XBarrier::weak_load_barrier_on_oop_field_preloaded(p, o);\n+JRT_END\n+\n+JRT_LEAF(oopDesc*, XBarrierSetRuntime::weak_load_barrier_on_weak_oop_field_preloaded(oopDesc* o, oop* p))\n+  return XBarrier::weak_load_barrier_on_weak_oop_field_preloaded(p, o);\n+JRT_END\n+\n+JRT_LEAF(oopDesc*, XBarrierSetRuntime::weak_load_barrier_on_phantom_oop_field_preloaded(oopDesc* o, oop* p))\n+  return XBarrier::weak_load_barrier_on_phantom_oop_field_preloaded(p, o);\n+JRT_END\n+\n+JRT_LEAF(oopDesc*, XBarrierSetRuntime::load_barrier_on_weak_oop_field_preloaded(oopDesc* o, oop* p))\n+  return XBarrier::load_barrier_on_weak_oop_field_preloaded(p, o);\n+JRT_END\n+\n+JRT_LEAF(oopDesc*, XBarrierSetRuntime::load_barrier_on_phantom_oop_field_preloaded(oopDesc* o, oop* p))\n+  return XBarrier::load_barrier_on_phantom_oop_field_preloaded(p, o);\n+JRT_END\n+\n+JRT_LEAF(void, XBarrierSetRuntime::load_barrier_on_oop_array(oop* p, size_t length))\n+  XBarrier::load_barrier_on_oop_array(p, length);\n+JRT_END\n+\n+JRT_LEAF(void, XBarrierSetRuntime::clone(oopDesc* src, oopDesc* dst, size_t size))\n+  HeapAccess<>::clone(src, dst, size);\n+JRT_END\n+\n+address XBarrierSetRuntime::load_barrier_on_oop_field_preloaded_addr(DecoratorSet decorators) {\n+  if (decorators & ON_PHANTOM_OOP_REF) {\n+    if (decorators & AS_NO_KEEPALIVE) {\n+      return weak_load_barrier_on_phantom_oop_field_preloaded_addr();\n+    } else {\n+      return load_barrier_on_phantom_oop_field_preloaded_addr();\n+    }\n+  } else if (decorators & ON_WEAK_OOP_REF) {\n+    if (decorators & AS_NO_KEEPALIVE) {\n+      return weak_load_barrier_on_weak_oop_field_preloaded_addr();\n+    } else {\n+      return load_barrier_on_weak_oop_field_preloaded_addr();\n+    }\n+  } else {\n+    if (decorators & AS_NO_KEEPALIVE) {\n+      return weak_load_barrier_on_oop_field_preloaded_addr();\n+    } else {\n+      return load_barrier_on_oop_field_preloaded_addr();\n+    }\n+  }\n+}\n+\n+address XBarrierSetRuntime::load_barrier_on_oop_field_preloaded_addr() {\n+  return reinterpret_cast<address>(load_barrier_on_oop_field_preloaded);\n+}\n+\n+address XBarrierSetRuntime::load_barrier_on_weak_oop_field_preloaded_addr() {\n+  return reinterpret_cast<address>(load_barrier_on_weak_oop_field_preloaded);\n+}\n+\n+address XBarrierSetRuntime::load_barrier_on_phantom_oop_field_preloaded_addr() {\n+  return reinterpret_cast<address>(load_barrier_on_phantom_oop_field_preloaded);\n+}\n+\n+address XBarrierSetRuntime::weak_load_barrier_on_oop_field_preloaded_addr() {\n+  return reinterpret_cast<address>(weak_load_barrier_on_oop_field_preloaded);\n+}\n+\n+address XBarrierSetRuntime::weak_load_barrier_on_weak_oop_field_preloaded_addr() {\n+  return reinterpret_cast<address>(weak_load_barrier_on_weak_oop_field_preloaded);\n+}\n+\n+address XBarrierSetRuntime::weak_load_barrier_on_phantom_oop_field_preloaded_addr() {\n+  return reinterpret_cast<address>(weak_load_barrier_on_phantom_oop_field_preloaded);\n+}\n+\n+address XBarrierSetRuntime::load_barrier_on_oop_array_addr() {\n+  return reinterpret_cast<address>(load_barrier_on_oop_array);\n+}\n+\n+address XBarrierSetRuntime::clone_addr() {\n+  return reinterpret_cast<address>(clone);\n+}\n","filename":"src\/hotspot\/share\/gc\/x\/xBarrierSetRuntime.cpp","additions":114,"deletions":0,"binary":false,"changes":114,"status":"added"},{"patch":"@@ -24,2 +24,2 @@\n-#ifndef SHARE_GC_Z_ZBARRIERSETRUNTIME_HPP\n-#define SHARE_GC_Z_ZBARRIERSETRUNTIME_HPP\n+#ifndef SHARE_GC_X_XBARRIERSETRUNTIME_HPP\n+#define SHARE_GC_X_XBARRIERSETRUNTIME_HPP\n@@ -33,1 +33,1 @@\n-class ZBarrierSetRuntime : public AllStatic {\n+class XBarrierSetRuntime : public AllStatic {\n@@ -56,1 +56,1 @@\n-#endif \/\/ SHARE_GC_Z_ZBARRIERSETRUNTIME_HPP\n+#endif \/\/ SHARE_GC_X_XBARRIERSETRUNTIME_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xBarrierSetRuntime.hpp","additions":4,"deletions":4,"binary":false,"changes":8,"previous_filename":"src\/hotspot\/share\/gc\/z\/zBarrierSetRuntime.hpp","status":"copied"},{"patch":"@@ -0,0 +1,47 @@\n+\/*\n+ * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/x\/xBarrier.inline.hpp\"\n+#include \"gc\/x\/xBarrierSetStackChunk.hpp\"\n+#include \"runtime\/atomic.hpp\"\n+#include \"utilities\/debug.hpp\"\n+\n+void XBarrierSetStackChunk::encode_gc_mode(stackChunkOop chunk, OopIterator* iterator) {\n+  \/\/ Do nothing\n+}\n+\n+void XBarrierSetStackChunk::decode_gc_mode(stackChunkOop chunk, OopIterator* iterator) {\n+  \/\/ Do nothing\n+}\n+\n+oop XBarrierSetStackChunk::load_oop(stackChunkOop chunk, oop* addr) {\n+  oop obj = Atomic::load(addr);\n+  return XBarrier::load_barrier_on_oop_field_preloaded((volatile oop*)NULL, obj);\n+}\n+\n+oop XBarrierSetStackChunk::load_oop(stackChunkOop chunk, narrowOop* addr) {\n+  ShouldNotReachHere();\n+  return NULL;\n+}\n","filename":"src\/hotspot\/share\/gc\/x\/xBarrierSetStackChunk.cpp","additions":47,"deletions":0,"binary":false,"changes":47,"status":"added"},{"patch":"@@ -0,0 +1,44 @@\n+\/*\n+ * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_X_XBARRIERSETSTACKCHUNK_HPP\n+#define SHARE_GC_X_XBARRIERSETSTACKCHUNK_HPP\n+\n+#include \"gc\/shared\/barrierSetStackChunk.hpp\"\n+#include \"memory\/iterator.hpp\"\n+#include \"oops\/oopsHierarchy.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+class OopClosure;\n+\n+class XBarrierSetStackChunk : public BarrierSetStackChunk {\n+public:\n+  virtual void encode_gc_mode(stackChunkOop chunk, OopIterator* iterator) override;\n+  virtual void decode_gc_mode(stackChunkOop chunk, OopIterator* iterator) override;\n+\n+  virtual oop load_oop(stackChunkOop chunk, oop* addr) override;\n+  virtual oop load_oop(stackChunkOop chunk, narrowOop* addr) override;\n+};\n+\n+#endif \/\/ SHARE_GC_X_XBARRIERSETSTACKCHUNK_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xBarrierSetStackChunk.hpp","additions":44,"deletions":0,"binary":false,"changes":44,"status":"added"},{"patch":"@@ -0,0 +1,81 @@\n+\/*\n+ * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XBITFIELD_HPP\n+#define SHARE_GC_X_XBITFIELD_HPP\n+\n+#include \"memory\/allStatic.hpp\"\n+#include \"utilities\/debug.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+\/\/\n+\/\/  Example\n+\/\/  -------\n+\/\/\n+\/\/  typedef XBitField<uint64_t, uint8_t,  0,  2, 3> field_word_aligned_size;\n+\/\/  typedef XBitField<uint64_t, uint32_t, 2, 30>    field_length;\n+\/\/\n+\/\/\n+\/\/   6                                 3 3\n+\/\/   3                                 2 1                               2 10\n+\/\/  +-----------------------------------+---------------------------------+--+\n+\/\/  |11111111 11111111 11111111 11111111|11111111 11111111 11111111 111111|11|\n+\/\/  +-----------------------------------+---------------------------------+--+\n+\/\/  |                                   |                                 |\n+\/\/  |       31-2 field_length (30-bits) *                                 |\n+\/\/  |                                                                     |\n+\/\/  |                                1-0 field_word_aligned_size (2-bits) *\n+\/\/  |\n+\/\/  * 63-32 Unused (32-bits)\n+\/\/\n+\/\/\n+\/\/  field_word_aligned_size::encode(16) = 2\n+\/\/  field_length::encode(2342) = 9368\n+\/\/\n+\/\/  field_word_aligned_size::decode(9368 | 2) = 16\n+\/\/  field_length::decode(9368 | 2) = 2342\n+\/\/\n+\n+template <typename ContainerType, typename ValueType, int FieldShift, int FieldBits, int ValueShift = 0>\n+class XBitField : public AllStatic {\n+private:\n+  static const int ContainerBits = sizeof(ContainerType) * BitsPerByte;\n+\n+  static_assert(FieldBits < ContainerBits, \"Field too large\");\n+  static_assert(FieldShift + FieldBits <= ContainerBits, \"Field too large\");\n+  static_assert(ValueShift + FieldBits <= ContainerBits, \"Field too large\");\n+\n+  static const ContainerType FieldMask = (((ContainerType)1 << FieldBits) - 1);\n+\n+public:\n+  static ValueType decode(ContainerType container) {\n+    return (ValueType)(((container >> FieldShift) & FieldMask) << ValueShift);\n+  }\n+\n+  static ContainerType encode(ValueType value) {\n+    assert(((ContainerType)value & (FieldMask << ValueShift)) == (ContainerType)value, \"Invalid value\");\n+    return ((ContainerType)value >> ValueShift) << FieldShift;\n+  }\n+};\n+\n+#endif \/\/ SHARE_GC_X_XBITFIELD_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xBitField.hpp","additions":81,"deletions":0,"binary":false,"changes":81,"status":"added"},{"patch":"@@ -0,0 +1,42 @@\n+\/*\n+ * Copyright (c) 2016, 2017, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XBITMAP_HPP\n+#define SHARE_GC_X_XBITMAP_HPP\n+\n+#include \"utilities\/bitMap.hpp\"\n+\n+class XBitMap : public CHeapBitMap {\n+private:\n+  static bm_word_t bit_mask_pair(idx_t bit);\n+\n+  bool par_set_bit_pair_finalizable(idx_t bit, bool& inc_live);\n+  bool par_set_bit_pair_strong(idx_t bit, bool& inc_live);\n+\n+public:\n+  XBitMap(idx_t size_in_bits);\n+\n+  bool par_set_bit_pair(idx_t bit, bool finalizable, bool& inc_live);\n+};\n+\n+#endif \/\/ SHARE_GC_X_XBITMAP_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xBitMap.hpp","additions":42,"deletions":0,"binary":false,"changes":42,"status":"added"},{"patch":"@@ -0,0 +1,80 @@\n+\/*\n+ * Copyright (c) 2016, 2017, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XBITMAP_INLINE_HPP\n+#define SHARE_GC_X_XBITMAP_INLINE_HPP\n+\n+#include \"gc\/x\/xBitMap.hpp\"\n+\n+#include \"runtime\/atomic.hpp\"\n+#include \"utilities\/bitMap.inline.hpp\"\n+#include \"utilities\/debug.hpp\"\n+\n+inline XBitMap::XBitMap(idx_t size_in_bits) :\n+    CHeapBitMap(size_in_bits, mtGC, false \/* clear *\/) {}\n+\n+inline BitMap::bm_word_t XBitMap::bit_mask_pair(idx_t bit) {\n+  assert(bit_in_word(bit) < BitsPerWord - 1, \"Invalid bit index\");\n+  return (bm_word_t)3 << bit_in_word(bit);\n+}\n+\n+inline bool XBitMap::par_set_bit_pair_finalizable(idx_t bit, bool& inc_live) {\n+  inc_live = par_set_bit(bit);\n+  return inc_live;\n+}\n+\n+inline bool XBitMap::par_set_bit_pair_strong(idx_t bit, bool& inc_live) {\n+  verify_index(bit);\n+  volatile bm_word_t* const addr = word_addr(bit);\n+  const bm_word_t pair_mask = bit_mask_pair(bit);\n+  bm_word_t old_val = *addr;\n+\n+  do {\n+    const bm_word_t new_val = old_val | pair_mask;\n+    if (new_val == old_val) {\n+      \/\/ Someone else beat us to it\n+      inc_live = false;\n+      return false;\n+    }\n+    const bm_word_t cur_val = Atomic::cmpxchg(addr, old_val, new_val);\n+    if (cur_val == old_val) {\n+      \/\/ Success\n+      const bm_word_t marked_mask = bit_mask(bit);\n+      inc_live = !(old_val & marked_mask);\n+      return true;\n+    }\n+\n+    \/\/ The value changed, retry\n+    old_val = cur_val;\n+  } while (true);\n+}\n+\n+inline bool XBitMap::par_set_bit_pair(idx_t bit, bool finalizable, bool& inc_live) {\n+  if (finalizable) {\n+    return par_set_bit_pair_finalizable(bit, inc_live);\n+  } else {\n+    return par_set_bit_pair_strong(bit, inc_live);\n+  }\n+}\n+\n+#endif \/\/ SHARE_GC_X_XBITMAP_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xBitMap.inline.hpp","additions":80,"deletions":0,"binary":false,"changes":80,"status":"added"},{"patch":"@@ -0,0 +1,63 @@\n+\/*\n+ * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/shared\/concurrentGCBreakpoints.hpp\"\n+#include \"gc\/x\/xBreakpoint.hpp\"\n+#include \"runtime\/mutexLocker.hpp\"\n+#include \"utilities\/debug.hpp\"\n+\n+bool XBreakpoint::_start_gc = false;\n+\n+void XBreakpoint::start_gc() {\n+  MonitorLocker ml(ConcurrentGCBreakpoints::monitor());\n+  assert(ConcurrentGCBreakpoints::is_controlled(), \"Invalid state\");\n+  assert(!_start_gc, \"Invalid state\");\n+  _start_gc = true;\n+  ml.notify_all();\n+}\n+\n+void XBreakpoint::at_before_gc() {\n+  MonitorLocker ml(ConcurrentGCBreakpoints::monitor(), Mutex::_no_safepoint_check_flag);\n+  while (ConcurrentGCBreakpoints::is_controlled() && !_start_gc) {\n+    ml.wait();\n+  }\n+  _start_gc = false;\n+  ConcurrentGCBreakpoints::notify_idle_to_active();\n+}\n+\n+void XBreakpoint::at_after_gc() {\n+  ConcurrentGCBreakpoints::notify_active_to_idle();\n+}\n+\n+void XBreakpoint::at_after_marking_started() {\n+  ConcurrentGCBreakpoints::at(\"AFTER MARKING STARTED\");\n+}\n+\n+void XBreakpoint::at_before_marking_completed() {\n+  ConcurrentGCBreakpoints::at(\"BEFORE MARKING COMPLETED\");\n+}\n+\n+void XBreakpoint::at_after_reference_processing_started() {\n+  ConcurrentGCBreakpoints::at(\"AFTER CONCURRENT REFERENCE PROCESSING STARTED\");\n+}\n","filename":"src\/hotspot\/share\/gc\/x\/xBreakpoint.cpp","additions":63,"deletions":0,"binary":false,"changes":63,"status":"added"},{"patch":"@@ -0,0 +1,43 @@\n+\/*\n+ * Copyright (c) 2020, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XBREAKPOINT_HPP\n+#define SHARE_GC_X_XBREAKPOINT_HPP\n+\n+#include \"memory\/allStatic.hpp\"\n+\n+class XBreakpoint : public AllStatic {\n+private:\n+  static bool _start_gc;\n+\n+public:\n+  static void start_gc();\n+\n+  static void at_before_gc();\n+  static void at_after_gc();\n+  static void at_after_marking_started();\n+  static void at_before_marking_completed();\n+  static void at_after_reference_processing_started();\n+};\n+\n+#endif \/\/ SHARE_GC_X_XBREAKPOINT_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xBreakpoint.hpp","additions":43,"deletions":0,"binary":false,"changes":43,"status":"added"},{"patch":"@@ -0,0 +1,67 @@\n+\/*\n+ * Copyright (c) 2015, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/shared\/gcLogPrecious.hpp\"\n+#include \"gc\/x\/xCPU.inline.hpp\"\n+#include \"memory\/padded.inline.hpp\"\n+#include \"runtime\/javaThread.hpp\"\n+#include \"runtime\/os.hpp\"\n+#include \"utilities\/debug.hpp\"\n+\n+#define XCPU_UNKNOWN_AFFINITY ((Thread*)-1)\n+#define XCPU_UNKNOWN_SELF     ((Thread*)-2)\n+\n+PaddedEnd<XCPU::XCPUAffinity>* XCPU::_affinity = NULL;\n+THREAD_LOCAL Thread*           XCPU::_self     = XCPU_UNKNOWN_SELF;\n+THREAD_LOCAL uint32_t          XCPU::_cpu      = 0;\n+\n+void XCPU::initialize() {\n+  assert(_affinity == NULL, \"Already initialized\");\n+  const uint32_t ncpus = count();\n+\n+  _affinity = PaddedArray<XCPUAffinity, mtGC>::create_unfreeable(ncpus);\n+\n+  for (uint32_t i = 0; i < ncpus; i++) {\n+    _affinity[i]._thread = XCPU_UNKNOWN_AFFINITY;\n+  }\n+\n+  log_info_p(gc, init)(\"CPUs: %u total, %u available\",\n+                       os::processor_count(),\n+                       os::initial_active_processor_count());\n+}\n+\n+uint32_t XCPU::id_slow() {\n+  \/\/ Set current thread\n+  if (_self == XCPU_UNKNOWN_SELF) {\n+    _self = Thread::current();\n+  }\n+\n+  \/\/ Set current CPU\n+  _cpu = os::processor_id();\n+\n+  \/\/ Update affinity table\n+  _affinity[_cpu]._thread = _self;\n+\n+  return _cpu;\n+}\n","filename":"src\/hotspot\/share\/gc\/x\/xCPU.cpp","additions":67,"deletions":0,"binary":false,"changes":67,"status":"added"},{"patch":"@@ -0,0 +1,52 @@\n+\/*\n+ * Copyright (c) 2015, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XCPU_HPP\n+#define SHARE_GC_X_XCPU_HPP\n+\n+#include \"memory\/allStatic.hpp\"\n+#include \"memory\/padded.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+class Thread;\n+\n+class XCPU : public AllStatic {\n+private:\n+  struct XCPUAffinity {\n+    Thread* _thread;\n+  };\n+\n+  static PaddedEnd<XCPUAffinity>* _affinity;\n+  static THREAD_LOCAL Thread*     _self;\n+  static THREAD_LOCAL uint32_t    _cpu;\n+\n+  static uint32_t id_slow();\n+\n+public:\n+  static void initialize();\n+\n+  static uint32_t count();\n+  static uint32_t id();\n+};\n+\n+#endif \/\/ SHARE_GC_X_XCPU_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xCPU.hpp","additions":52,"deletions":0,"binary":false,"changes":52,"status":"added"},{"patch":"@@ -0,0 +1,48 @@\n+\/*\n+ * Copyright (c) 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XCPU_INLINE_HPP\n+#define SHARE_GC_X_XCPU_INLINE_HPP\n+\n+#include \"gc\/x\/xCPU.hpp\"\n+\n+#include \"runtime\/os.hpp\"\n+#include \"utilities\/debug.hpp\"\n+\n+inline uint32_t XCPU::count() {\n+  return os::processor_count();\n+}\n+\n+inline uint32_t XCPU::id() {\n+  assert(_affinity != NULL, \"Not initialized\");\n+\n+  \/\/ Fast path\n+  if (_affinity[_cpu]._thread == _self) {\n+    return _cpu;\n+  }\n+\n+  \/\/ Slow path\n+  return id_slow();\n+}\n+\n+#endif \/\/ SHARE_GC_X_XCPU_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xCPU.inline.hpp","additions":48,"deletions":0,"binary":false,"changes":48,"status":"added"},{"patch":"@@ -0,0 +1,351 @@\n+\/*\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"classfile\/classLoaderData.hpp\"\n+#include \"gc\/shared\/gcHeapSummary.hpp\"\n+#include \"gc\/shared\/gcLocker.inline.hpp\"\n+#include \"gc\/shared\/suspendibleThreadSet.hpp\"\n+#include \"gc\/x\/xCollectedHeap.hpp\"\n+#include \"gc\/x\/xDirector.hpp\"\n+#include \"gc\/x\/xDriver.hpp\"\n+#include \"gc\/x\/xGlobals.hpp\"\n+#include \"gc\/x\/xHeap.inline.hpp\"\n+#include \"gc\/x\/xNMethod.hpp\"\n+#include \"gc\/x\/xObjArrayAllocator.hpp\"\n+#include \"gc\/x\/xOop.inline.hpp\"\n+#include \"gc\/x\/xServiceability.hpp\"\n+#include \"gc\/x\/xStat.hpp\"\n+#include \"gc\/x\/xUtils.inline.hpp\"\n+#include \"memory\/classLoaderMetaspace.hpp\"\n+#include \"memory\/iterator.hpp\"\n+#include \"memory\/metaspaceCriticalAllocation.hpp\"\n+#include \"memory\/universe.hpp\"\n+#include \"oops\/stackChunkOop.hpp\"\n+#include \"runtime\/continuationJavaClasses.hpp\"\n+#include \"utilities\/align.hpp\"\n+\n+XCollectedHeap* XCollectedHeap::heap() {\n+  return named_heap<XCollectedHeap>(CollectedHeap::Z);\n+}\n+\n+XCollectedHeap::XCollectedHeap() :\n+    _soft_ref_policy(),\n+    _barrier_set(),\n+    _initialize(&_barrier_set),\n+    _heap(),\n+    _driver(new XDriver()),\n+    _director(new XDirector(_driver)),\n+    _stat(new XStat()),\n+    _runtime_workers() {}\n+\n+CollectedHeap::Name XCollectedHeap::kind() const {\n+  return CollectedHeap::Z;\n+}\n+\n+const char* XCollectedHeap::name() const {\n+  return XName;\n+}\n+\n+jint XCollectedHeap::initialize() {\n+  if (!_heap.is_initialized()) {\n+    return JNI_ENOMEM;\n+  }\n+\n+  Universe::calculate_verify_data((HeapWord*)0, (HeapWord*)UINTPTR_MAX);\n+\n+  return JNI_OK;\n+}\n+\n+void XCollectedHeap::initialize_serviceability() {\n+  _heap.serviceability_initialize();\n+}\n+\n+class XStopConcurrentGCThreadClosure : public ThreadClosure {\n+public:\n+  virtual void do_thread(Thread* thread) {\n+    if (thread->is_ConcurrentGC_thread()) {\n+      ConcurrentGCThread::cast(thread)->stop();\n+    }\n+  }\n+};\n+\n+void XCollectedHeap::stop() {\n+  XStopConcurrentGCThreadClosure cl;\n+  gc_threads_do(&cl);\n+}\n+\n+SoftRefPolicy* XCollectedHeap::soft_ref_policy() {\n+  return &_soft_ref_policy;\n+}\n+\n+size_t XCollectedHeap::max_capacity() const {\n+  return _heap.max_capacity();\n+}\n+\n+size_t XCollectedHeap::capacity() const {\n+  return _heap.capacity();\n+}\n+\n+size_t XCollectedHeap::used() const {\n+  return _heap.used();\n+}\n+\n+size_t XCollectedHeap::unused() const {\n+  return _heap.unused();\n+}\n+\n+bool XCollectedHeap::is_maximal_no_gc() const {\n+  \/\/ Not supported\n+  ShouldNotReachHere();\n+  return false;\n+}\n+\n+bool XCollectedHeap::is_in(const void* p) const {\n+  return _heap.is_in((uintptr_t)p);\n+}\n+\n+bool XCollectedHeap::requires_barriers(stackChunkOop obj) const {\n+  uintptr_t* cont_addr = obj->field_addr<uintptr_t>(jdk_internal_vm_StackChunk::cont_offset());\n+\n+  if (!_heap.is_allocating(cast_from_oop<uintptr_t>(obj))) {\n+    \/\/ An object that isn't allocating, is visible from GC tracing. Such\n+    \/\/ stack chunks require barriers.\n+    return true;\n+  }\n+\n+  if (!XAddress::is_good_or_null(*cont_addr)) {\n+    \/\/ If a chunk is allocated after a GC started, but before relocate start\n+    \/\/ we can have an allocating chunk that isn't deeply good. That means that\n+    \/\/ the contained oops might be bad and require GC barriers.\n+    return true;\n+  }\n+\n+  \/\/ The chunk is allocating and its pointers are good. This chunk needs no\n+  \/\/ GC barriers\n+  return false;\n+}\n+\n+HeapWord* XCollectedHeap::allocate_new_tlab(size_t min_size, size_t requested_size, size_t* actual_size) {\n+  const size_t size_in_bytes = XUtils::words_to_bytes(align_object_size(requested_size));\n+  const uintptr_t addr = _heap.alloc_tlab(size_in_bytes);\n+\n+  if (addr != 0) {\n+    *actual_size = requested_size;\n+  }\n+\n+  return (HeapWord*)addr;\n+}\n+\n+oop XCollectedHeap::array_allocate(Klass* klass, size_t size, int length, bool do_zero, TRAPS) {\n+  XObjArrayAllocator allocator(klass, size, length, do_zero, THREAD);\n+  return allocator.allocate();\n+}\n+\n+HeapWord* XCollectedHeap::mem_allocate(size_t size, bool* gc_overhead_limit_was_exceeded) {\n+  const size_t size_in_bytes = XUtils::words_to_bytes(align_object_size(size));\n+  return (HeapWord*)_heap.alloc_object(size_in_bytes);\n+}\n+\n+MetaWord* XCollectedHeap::satisfy_failed_metadata_allocation(ClassLoaderData* loader_data,\n+                                                             size_t size,\n+                                                             Metaspace::MetadataType mdtype) {\n+  \/\/ Start asynchronous GC\n+  collect(GCCause::_metadata_GC_threshold);\n+\n+  \/\/ Expand and retry allocation\n+  MetaWord* const result = loader_data->metaspace_non_null()->expand_and_allocate(size, mdtype);\n+  if (result != NULL) {\n+    return result;\n+  }\n+\n+  \/\/ As a last resort, try a critical allocation, riding on a synchronous full GC\n+  return MetaspaceCriticalAllocation::allocate(loader_data, size, mdtype);\n+}\n+\n+void XCollectedHeap::collect(GCCause::Cause cause) {\n+  _driver->collect(cause);\n+}\n+\n+void XCollectedHeap::collect_as_vm_thread(GCCause::Cause cause) {\n+  \/\/ These collection requests are ignored since ZGC can't run a synchronous\n+  \/\/ GC cycle from within the VM thread. This is considered benign, since the\n+  \/\/ only GC causes coming in here should be heap dumper and heap inspector.\n+  \/\/ If the heap dumper or heap inspector explicitly requests a gc and the\n+  \/\/ caller is not the VM thread a synchronous GC cycle is performed from the\n+  \/\/ caller thread in the prologue.\n+  assert(Thread::current()->is_VM_thread(), \"Should be the VM thread\");\n+  guarantee(cause == GCCause::_heap_dump ||\n+            cause == GCCause::_heap_inspection, \"Invalid cause\");\n+}\n+\n+void XCollectedHeap::do_full_collection(bool clear_all_soft_refs) {\n+  \/\/ Not supported\n+  ShouldNotReachHere();\n+}\n+\n+size_t XCollectedHeap::tlab_capacity(Thread* ignored) const {\n+  return _heap.tlab_capacity();\n+}\n+\n+size_t XCollectedHeap::tlab_used(Thread* ignored) const {\n+  return _heap.tlab_used();\n+}\n+\n+size_t XCollectedHeap::max_tlab_size() const {\n+  return _heap.max_tlab_size();\n+}\n+\n+size_t XCollectedHeap::unsafe_max_tlab_alloc(Thread* ignored) const {\n+  return _heap.unsafe_max_tlab_alloc();\n+}\n+\n+bool XCollectedHeap::uses_stack_watermark_barrier() const {\n+  return true;\n+}\n+\n+MemoryUsage XCollectedHeap::memory_usage() {\n+  return _heap.serviceability_memory_pool()->get_memory_usage();\n+}\n+\n+GrowableArray<GCMemoryManager*> XCollectedHeap::memory_managers() {\n+  GrowableArray<GCMemoryManager*> memory_managers(2);\n+  memory_managers.append(_heap.serviceability_cycle_memory_manager());\n+  memory_managers.append(_heap.serviceability_pause_memory_manager());\n+  return memory_managers;\n+}\n+\n+GrowableArray<MemoryPool*> XCollectedHeap::memory_pools() {\n+  GrowableArray<MemoryPool*> memory_pools(1);\n+  memory_pools.append(_heap.serviceability_memory_pool());\n+  return memory_pools;\n+}\n+\n+void XCollectedHeap::object_iterate(ObjectClosure* cl) {\n+  _heap.object_iterate(cl, true \/* visit_weaks *\/);\n+}\n+\n+ParallelObjectIteratorImpl* XCollectedHeap::parallel_object_iterator(uint nworkers) {\n+  return _heap.parallel_object_iterator(nworkers, true \/* visit_weaks *\/);\n+}\n+\n+void XCollectedHeap::keep_alive(oop obj) {\n+  _heap.keep_alive(obj);\n+}\n+\n+void XCollectedHeap::register_nmethod(nmethod* nm) {\n+  XNMethod::register_nmethod(nm);\n+}\n+\n+void XCollectedHeap::unregister_nmethod(nmethod* nm) {\n+  XNMethod::unregister_nmethod(nm);\n+}\n+\n+void XCollectedHeap::verify_nmethod(nmethod* nm) {\n+  \/\/ Does nothing\n+}\n+\n+WorkerThreads* XCollectedHeap::safepoint_workers() {\n+  return _runtime_workers.workers();\n+}\n+\n+void XCollectedHeap::gc_threads_do(ThreadClosure* tc) const {\n+  tc->do_thread(_director);\n+  tc->do_thread(_driver);\n+  tc->do_thread(_stat);\n+  _heap.threads_do(tc);\n+  _runtime_workers.threads_do(tc);\n+}\n+\n+VirtualSpaceSummary XCollectedHeap::create_heap_space_summary() {\n+  return VirtualSpaceSummary((HeapWord*)0, (HeapWord*)capacity(), (HeapWord*)max_capacity());\n+}\n+\n+void XCollectedHeap::safepoint_synchronize_begin() {\n+  SuspendibleThreadSet::synchronize();\n+}\n+\n+void XCollectedHeap::safepoint_synchronize_end() {\n+  SuspendibleThreadSet::desynchronize();\n+}\n+\n+void XCollectedHeap::pin_object(JavaThread* thread, oop obj) {\n+  GCLocker::lock_critical(thread);\n+}\n+\n+void XCollectedHeap::unpin_object(JavaThread* thread, oop obj) {\n+  GCLocker::unlock_critical(thread);\n+}\n+\n+void XCollectedHeap::prepare_for_verify() {\n+  \/\/ Does nothing\n+}\n+\n+void XCollectedHeap::print_on(outputStream* st) const {\n+  _heap.print_on(st);\n+}\n+\n+void XCollectedHeap::print_on_error(outputStream* st) const {\n+  st->print_cr(\"ZGC Globals:\");\n+  st->print_cr(\" GlobalPhase:       %u (%s)\", XGlobalPhase, XGlobalPhaseToString());\n+  st->print_cr(\" GlobalSeqNum:      %u\", XGlobalSeqNum);\n+  st->print_cr(\" Offset Max:        \" SIZE_FORMAT \"%s (\" PTR_FORMAT \")\",\n+               byte_size_in_exact_unit(XAddressOffsetMax),\n+               exact_unit_for_byte_size(XAddressOffsetMax),\n+               XAddressOffsetMax);\n+  st->print_cr(\" Page Size Small:   \" SIZE_FORMAT \"M\", XPageSizeSmall \/ M);\n+  st->print_cr(\" Page Size Medium:  \" SIZE_FORMAT \"M\", XPageSizeMedium \/ M);\n+  st->cr();\n+  st->print_cr(\"ZGC Metadata Bits:\");\n+  st->print_cr(\" Good:              \" PTR_FORMAT, XAddressGoodMask);\n+  st->print_cr(\" Bad:               \" PTR_FORMAT, XAddressBadMask);\n+  st->print_cr(\" WeakBad:           \" PTR_FORMAT, XAddressWeakBadMask);\n+  st->print_cr(\" Marked:            \" PTR_FORMAT, XAddressMetadataMarked);\n+  st->print_cr(\" Remapped:          \" PTR_FORMAT, XAddressMetadataRemapped);\n+  st->cr();\n+  CollectedHeap::print_on_error(st);\n+}\n+\n+void XCollectedHeap::print_extended_on(outputStream* st) const {\n+  _heap.print_extended_on(st);\n+}\n+\n+void XCollectedHeap::print_tracing_info() const {\n+  \/\/ Does nothing\n+}\n+\n+bool XCollectedHeap::print_location(outputStream* st, void* addr) const {\n+  return _heap.print_location(st, (uintptr_t)addr);\n+}\n+\n+void XCollectedHeap::verify(VerifyOption option \/* ignored *\/) {\n+  _heap.verify();\n+}\n+\n+bool XCollectedHeap::is_oop(oop object) const {\n+  return _heap.is_oop(XOop::to_address(object));\n+}\n+\n+bool XCollectedHeap::supports_concurrent_gc_breakpoints() const {\n+  return true;\n+}\n","filename":"src\/hotspot\/share\/gc\/x\/xCollectedHeap.cpp","additions":351,"deletions":0,"binary":false,"changes":351,"status":"added"},{"patch":"@@ -0,0 +1,132 @@\n+\/*\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XCOLLECTEDHEAP_HPP\n+#define SHARE_GC_X_XCOLLECTEDHEAP_HPP\n+\n+#include \"gc\/shared\/collectedHeap.hpp\"\n+#include \"gc\/shared\/softRefPolicy.hpp\"\n+#include \"gc\/x\/xBarrierSet.hpp\"\n+#include \"gc\/x\/xHeap.hpp\"\n+#include \"gc\/x\/xInitialize.hpp\"\n+#include \"gc\/x\/xRuntimeWorkers.hpp\"\n+#include \"memory\/metaspace.hpp\"\n+#include \"services\/memoryUsage.hpp\"\n+\n+class VMStructs;\n+class XDirector;\n+class XDriver;\n+class XStat;\n+\n+class XCollectedHeap : public CollectedHeap {\n+  friend class ::VMStructs;\n+\n+private:\n+  SoftRefPolicy     _soft_ref_policy;\n+  XBarrierSet       _barrier_set;\n+  XInitialize       _initialize;\n+  XHeap             _heap;\n+  XDriver*          _driver;\n+  XDirector*        _director;\n+  XStat*            _stat;\n+  XRuntimeWorkers   _runtime_workers;\n+\n+  HeapWord* allocate_new_tlab(size_t min_size,\n+                              size_t requested_size,\n+                              size_t* actual_size) override;\n+\n+public:\n+  static XCollectedHeap* heap();\n+\n+  XCollectedHeap();\n+  Name kind() const override;\n+  const char* name() const override;\n+  jint initialize() override;\n+  void initialize_serviceability() override;\n+  void stop() override;\n+\n+  SoftRefPolicy* soft_ref_policy() override;\n+\n+  size_t max_capacity() const override;\n+  size_t capacity() const override;\n+  size_t used() const override;\n+  size_t unused() const override;\n+\n+  bool is_maximal_no_gc() const override;\n+  bool is_in(const void* p) const override;\n+  bool requires_barriers(stackChunkOop obj) const override;\n+\n+  oop array_allocate(Klass* klass, size_t size, int length, bool do_zero, TRAPS) override;\n+  HeapWord* mem_allocate(size_t size, bool* gc_overhead_limit_was_exceeded) override;\n+  MetaWord* satisfy_failed_metadata_allocation(ClassLoaderData* loader_data,\n+                                               size_t size,\n+                                               Metaspace::MetadataType mdtype) override;\n+  void collect(GCCause::Cause cause) override;\n+  void collect_as_vm_thread(GCCause::Cause cause) override;\n+  void do_full_collection(bool clear_all_soft_refs) override;\n+\n+  size_t tlab_capacity(Thread* thr) const override;\n+  size_t tlab_used(Thread* thr) const override;\n+  size_t max_tlab_size() const override;\n+  size_t unsafe_max_tlab_alloc(Thread* thr) const override;\n+\n+  bool uses_stack_watermark_barrier() const override;\n+\n+  MemoryUsage memory_usage() override;\n+  GrowableArray<GCMemoryManager*> memory_managers() override;\n+  GrowableArray<MemoryPool*> memory_pools() override;\n+\n+  void object_iterate(ObjectClosure* cl) override;\n+  ParallelObjectIteratorImpl* parallel_object_iterator(uint nworkers) override;\n+\n+  void keep_alive(oop obj) override;\n+\n+  void register_nmethod(nmethod* nm) override;\n+  void unregister_nmethod(nmethod* nm) override;\n+  void verify_nmethod(nmethod* nmethod) override;\n+\n+  WorkerThreads* safepoint_workers() override;\n+\n+  void gc_threads_do(ThreadClosure* tc) const override;\n+\n+  VirtualSpaceSummary create_heap_space_summary() override;\n+\n+  void safepoint_synchronize_begin() override;\n+  void safepoint_synchronize_end() override;\n+\n+  void pin_object(JavaThread* thread, oop obj) override;\n+  void unpin_object(JavaThread* thread, oop obj) override;\n+\n+  void print_on(outputStream* st) const override;\n+  void print_on_error(outputStream* st) const override;\n+  void print_extended_on(outputStream* st) const override;\n+  void print_tracing_info() const override;\n+  bool print_location(outputStream* st, void* addr) const override;\n+\n+  void prepare_for_verify() override;\n+  void verify(VerifyOption option \/* ignored *\/) override;\n+  bool is_oop(oop object) const override;\n+  bool supports_concurrent_gc_breakpoints() const override;\n+};\n+\n+#endif \/\/ SHARE_GC_X_XCOLLECTEDHEAP_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xCollectedHeap.hpp","additions":132,"deletions":0,"binary":false,"changes":132,"status":"added"},{"patch":"@@ -0,0 +1,148 @@\n+#\n+# GDB functions for debugging the Z Garbage Collector\n+#\n+\n+printf \"Loading zDebug.gdb\\n\"\n+\n+# Print Klass*\n+define zpk\n+    printf \"Klass: %s\\n\", (char*)((Klass*)($arg0))->_name->_body\n+end\n+\n+# Print oop\n+define zpo\n+    set $obj = (oopDesc*)($arg0)\n+\n+    printf \"Oop:   0x%016llx\\tState: \", (uintptr_t)$obj\n+    if ((uintptr_t)$obj & (uintptr_t)XAddressGoodMask)\n+        printf \"Good \"\n+        if ((uintptr_t)$obj & (uintptr_t)XAddressMetadataRemapped)\n+            printf \"(Remapped)\"\n+        else\n+            if ((uintptr_t)$obj & (uintptr_t)XAddressMetadataMarked)\n+                printf \"(Marked)\"\n+            else\n+                printf \"(Unknown)\"\n+            end\n+        end\n+    else\n+        printf \"Bad \"\n+        if ((uintptr_t)XAddressGoodMask & (uintptr_t)XAddressMetadataMarked)\n+            # Should be marked\n+            if ((uintptr_t)$obj & (uintptr_t)XAddressMetadataRemapped)\n+                printf \"(Not Marked, Remapped)\"\n+            else\n+                printf \"(Not Marked, Not Remapped)\"\n+            end\n+        else\n+            if ((uintptr_t)XAddressGoodMask & (uintptr_t)XAddressMetadataRemapped)\n+                # Should be remapped\n+                if ((uintptr_t)$obj & (uintptr_t)XAddressMetadataMarked)\n+                    printf \"(Marked, Not Remapped)\"\n+                else\n+                    printf \"(Not Marked, Not Remapped)\"\n+                end\n+            else\n+                # Unknown\n+                printf \"(Unknown)\"\n+            end\n+        end\n+    end\n+    printf \"\\t Page: %llu\\n\", ((uintptr_t)$obj & XAddressOffsetMask) >> XGranuleSizeShift\n+    x\/16gx $obj\n+    if (UseCompressedClassPointers)\n+        set $klass = (Klass*)(void*)((uintptr_t)CompressedKlassPointers::_narrow_klass._base +((uintptr_t)$obj->_metadata->_compressed_klass << CompressedKlassPointers::_narrow_klass._shift))\n+    else\n+        set $klass = $obj->_metadata->_klass\n+    end\n+    printf \"Mark:  0x%016llx\\tKlass: %s\\n\", (uintptr_t)$obj->_mark, (char*)$klass->_name->_body\n+end\n+\n+# Print heap page by page table index\n+define zpp\n+    set $page = (XPage*)((uintptr_t)XHeap::_heap._page_table._map._map[($arg0)] & ~1)\n+    printf \"Page %p\\n\", $page\n+    print *$page\n+end\n+\n+# Print page_table\n+define zpt\n+    printf \"Pagetable (first 128 slots)\\n\"\n+    x\/128gx XHeap::_heap._page_table._map._map\n+end\n+\n+# Print live map\n+define __zmarked\n+    set $livemap   = $arg0\n+    set $bit        = $arg1\n+    set $size       = $livemap._bitmap._size\n+    set $segment    = $size \/ XLiveMap::nsegments\n+    set $segment_bit = 1 << $segment\n+\n+    printf \"Segment is \"\n+    if !($livemap._segment_live_bits & $segment_bit)\n+        printf \"NOT \"\n+    end\n+    printf \"live (segment %d)\\n\", $segment\n+\n+    if $bit >= $size\n+        print \"Error: Bit %z out of bounds (bitmap size %z)\\n\", $bit, $size\n+    else\n+        set $word_index = $bit \/ 64\n+        set $bit_index  = $bit % 64\n+        set $word       = $livemap._bitmap._map[$word_index]\n+        set $live_bit   = $word & (1 << $bit_index)\n+\n+        printf \"Object is \"\n+        if $live_bit == 0\n+            printf \"NOT \"\n+        end\n+        printf \"live (word index %d, bit index %d)\\n\", $word_index, $bit_index\n+    end\n+end\n+\n+define zmarked\n+    set $addr          = $arg0\n+    set $obj           = ((uintptr_t)$addr & XAddressOffsetMask)\n+    set $page_index    = $obj >> XGranuleSizeShift\n+    set $page_entry    = (uintptr_t)XHeap::_heap._page_table._map._map[$page_index]\n+    set $page          = (XPage*)($page_entry & ~1)\n+    set $page_start    = (uintptr_t)$page._virtual._start\n+    set $page_end      = (uintptr_t)$page._virtual._end\n+    set $page_seqnum   = $page._livemap._seqnum\n+    set $global_seqnum = XGlobalSeqNum\n+\n+    if $obj < $page_start || $obj >= $page_end\n+        printf \"Error: %p not in page %p (start %p, end %p)\\n\", $obj, $page, $page_start, $page_end\n+    else\n+        printf \"Page is \"\n+        if $page_seqnum != $global_seqnum\n+            printf \"NOT \"\n+        end\n+        printf \"live (page %p, page seqnum %d, global seqnum %d)\\n\", $page, $page_seqnum, $global_seqnum\n+\n+        #if $page_seqnum == $global_seqnum\n+            set $offset = $obj - $page_start\n+            set $bit = $offset \/ 8\n+            __zmarked $page._livemap $bit\n+        #end\n+    end\n+end\n+\n+# Print heap information\n+define zph\n+    printf \"Heap\\n\"\n+    printf \"     GlobalPhase:       %u\\n\", XGlobalPhase\n+    printf \"     GlobalSeqNum:      %u\\n\", XGlobalSeqNum\n+    printf \"     Offset Max:        %-15llu (0x%llx)\\n\", XAddressOffsetMax, XAddressOffsetMax\n+    printf \"     Page Size Small:   %-15llu (0x%llx)\\n\", XPageSizeSmall, XPageSizeSmall\n+    printf \"     Page Size Medium:  %-15llu (0x%llx)\\n\", XPageSizeMedium, XPageSizeMedium\n+    printf \"Metadata Bits\\n\"\n+    printf \"     Good:              0x%016llx\\n\", XAddressGoodMask\n+    printf \"     Bad:               0x%016llx\\n\", XAddressBadMask\n+    printf \"     WeakBad:           0x%016llx\\n\", XAddressWeakBadMask\n+    printf \"     Marked:            0x%016llx\\n\", XAddressMetadataMarked\n+    printf \"     Remapped:          0x%016llx\\n\", XAddressMetadataRemapped\n+end\n+\n+# End of file\n","filename":"src\/hotspot\/share\/gc\/x\/xDebug.gdb","additions":148,"deletions":0,"binary":false,"changes":148,"status":"added"},{"patch":"@@ -0,0 +1,406 @@\n+\/*\n+ * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/shared\/gc_globals.hpp\"\n+#include \"gc\/x\/xDirector.hpp\"\n+#include \"gc\/x\/xDriver.hpp\"\n+#include \"gc\/x\/xHeap.inline.hpp\"\n+#include \"gc\/x\/xHeuristics.hpp\"\n+#include \"gc\/x\/xStat.hpp\"\n+#include \"logging\/log.hpp\"\n+\n+constexpr double one_in_1000 = 3.290527;\n+constexpr double sample_interval = 1.0 \/ XStatAllocRate::sample_hz;\n+\n+XDirector::XDirector(XDriver* driver) :\n+    _driver(driver),\n+    _metronome(XStatAllocRate::sample_hz) {\n+  set_name(\"XDirector\");\n+  create_and_start();\n+}\n+\n+static void sample_allocation_rate() {\n+  \/\/ Sample allocation rate. This is needed by rule_allocation_rate()\n+  \/\/ below to estimate the time we have until we run out of memory.\n+  const double bytes_per_second = XStatAllocRate::sample_and_reset();\n+\n+  log_debug(gc, alloc)(\"Allocation Rate: %.1fMB\/s, Predicted: %.1fMB\/s, Avg: %.1f(+\/-%.1f)MB\/s\",\n+                       bytes_per_second \/ M,\n+                       XStatAllocRate::predict() \/ M,\n+                       XStatAllocRate::avg() \/ M,\n+                       XStatAllocRate::sd() \/ M);\n+}\n+\n+static XDriverRequest rule_allocation_stall() {\n+  \/\/ Perform GC if we've observed at least one allocation stall since\n+  \/\/ the last GC started.\n+  if (!XHeap::heap()->has_alloc_stalled()) {\n+    return GCCause::_no_gc;\n+  }\n+\n+  log_debug(gc, director)(\"Rule: Allocation Stall Observed\");\n+\n+  return GCCause::_z_allocation_stall;\n+}\n+\n+static XDriverRequest rule_warmup() {\n+  if (XStatCycle::is_warm()) {\n+    \/\/ Rule disabled\n+    return GCCause::_no_gc;\n+  }\n+\n+  \/\/ Perform GC if heap usage passes 10\/20\/30% and no other GC has been\n+  \/\/ performed yet. This allows us to get some early samples of the GC\n+  \/\/ duration, which is needed by the other rules.\n+  const size_t soft_max_capacity = XHeap::heap()->soft_max_capacity();\n+  const size_t used = XHeap::heap()->used();\n+  const double used_threshold_percent = (XStatCycle::nwarmup_cycles() + 1) * 0.1;\n+  const size_t used_threshold = soft_max_capacity * used_threshold_percent;\n+\n+  log_debug(gc, director)(\"Rule: Warmup %.0f%%, Used: \" SIZE_FORMAT \"MB, UsedThreshold: \" SIZE_FORMAT \"MB\",\n+                          used_threshold_percent * 100, used \/ M, used_threshold \/ M);\n+\n+  if (used < used_threshold) {\n+    return GCCause::_no_gc;\n+  }\n+\n+  return GCCause::_z_warmup;\n+}\n+\n+static XDriverRequest rule_timer() {\n+  if (ZCollectionInterval <= 0) {\n+    \/\/ Rule disabled\n+    return GCCause::_no_gc;\n+  }\n+\n+  \/\/ Perform GC if timer has expired.\n+  const double time_since_last_gc = XStatCycle::time_since_last();\n+  const double time_until_gc = ZCollectionInterval - time_since_last_gc;\n+\n+  log_debug(gc, director)(\"Rule: Timer, Interval: %.3fs, TimeUntilGC: %.3fs\",\n+                          ZCollectionInterval, time_until_gc);\n+\n+  if (time_until_gc > 0) {\n+    return GCCause::_no_gc;\n+  }\n+\n+  return GCCause::_z_timer;\n+}\n+\n+static double estimated_gc_workers(double serial_gc_time, double parallelizable_gc_time, double time_until_deadline) {\n+  const double parallelizable_time_until_deadline = MAX2(time_until_deadline - serial_gc_time, 0.001);\n+  return parallelizable_gc_time \/ parallelizable_time_until_deadline;\n+}\n+\n+static uint discrete_gc_workers(double gc_workers) {\n+  return clamp<uint>(ceil(gc_workers), 1, ConcGCThreads);\n+}\n+\n+static double select_gc_workers(double serial_gc_time, double parallelizable_gc_time, double alloc_rate_sd_percent, double time_until_oom) {\n+  \/\/ Use all workers until we're warm\n+  if (!XStatCycle::is_warm()) {\n+    const double not_warm_gc_workers = ConcGCThreads;\n+    log_debug(gc, director)(\"Select GC Workers (Not Warm), GCWorkers: %.3f\", not_warm_gc_workers);\n+    return not_warm_gc_workers;\n+  }\n+\n+  \/\/ Calculate number of GC workers needed to avoid a long GC cycle and to avoid OOM.\n+  const double avoid_long_gc_workers = estimated_gc_workers(serial_gc_time, parallelizable_gc_time, 10 \/* seconds *\/);\n+  const double avoid_oom_gc_workers = estimated_gc_workers(serial_gc_time, parallelizable_gc_time, time_until_oom);\n+\n+  const double gc_workers = MAX2(avoid_long_gc_workers, avoid_oom_gc_workers);\n+  const uint actual_gc_workers = discrete_gc_workers(gc_workers);\n+  const uint last_gc_workers = XStatCycle::last_active_workers();\n+\n+  \/\/ More than 15% division from the average is considered unsteady\n+  if (alloc_rate_sd_percent >= 0.15) {\n+    const double half_gc_workers = ConcGCThreads \/ 2.0;\n+    const double unsteady_gc_workers = MAX3<double>(gc_workers, last_gc_workers, half_gc_workers);\n+    log_debug(gc, director)(\"Select GC Workers (Unsteady), \"\n+                            \"AvoidLongGCWorkers: %.3f, AvoidOOMGCWorkers: %.3f, LastGCWorkers: %.3f, HalfGCWorkers: %.3f, GCWorkers: %.3f\",\n+                            avoid_long_gc_workers, avoid_oom_gc_workers, (double)last_gc_workers, half_gc_workers, unsteady_gc_workers);\n+    return unsteady_gc_workers;\n+  }\n+\n+  if (actual_gc_workers < last_gc_workers) {\n+    \/\/ Before decreasing number of GC workers compared to the previous GC cycle, check if the\n+    \/\/ next GC cycle will need to increase it again. If so, use the same number of GC workers\n+    \/\/ that will be needed in the next cycle.\n+    const double gc_duration_delta = (parallelizable_gc_time \/ actual_gc_workers) - (parallelizable_gc_time \/ last_gc_workers);\n+    const double additional_time_for_allocations = XStatCycle::time_since_last() - gc_duration_delta - sample_interval;\n+    const double next_time_until_oom = time_until_oom + additional_time_for_allocations;\n+    const double next_avoid_oom_gc_workers = estimated_gc_workers(serial_gc_time, parallelizable_gc_time, next_time_until_oom);\n+\n+    \/\/ Add 0.5 to increase friction and avoid lowering too eagerly\n+    const double next_gc_workers = next_avoid_oom_gc_workers + 0.5;\n+    const double try_lowering_gc_workers = clamp<double>(next_gc_workers, actual_gc_workers, last_gc_workers);\n+\n+    log_debug(gc, director)(\"Select GC Workers (Try Lowering), \"\n+                           \"AvoidLongGCWorkers: %.3f, AvoidOOMGCWorkers: %.3f, NextAvoidOOMGCWorkers: %.3f, LastGCWorkers: %.3f, GCWorkers: %.3f\",\n+                            avoid_long_gc_workers, avoid_oom_gc_workers, next_avoid_oom_gc_workers, (double)last_gc_workers, try_lowering_gc_workers);\n+    return try_lowering_gc_workers;\n+  }\n+\n+  log_debug(gc, director)(\"Select GC Workers (Normal), \"\n+                         \"AvoidLongGCWorkers: %.3f, AvoidOOMGCWorkers: %.3f, LastGCWorkers: %.3f, GCWorkers: %.3f\",\n+                         avoid_long_gc_workers, avoid_oom_gc_workers, (double)last_gc_workers, gc_workers);\n+  return gc_workers;\n+}\n+\n+XDriverRequest rule_allocation_rate_dynamic() {\n+  if (!XStatCycle::is_time_trustable()) {\n+    \/\/ Rule disabled\n+    return GCCause::_no_gc;\n+  }\n+\n+  \/\/ Calculate amount of free memory available. Note that we take the\n+  \/\/ relocation headroom into account to avoid in-place relocation.\n+  const size_t soft_max_capacity = XHeap::heap()->soft_max_capacity();\n+  const size_t used = XHeap::heap()->used();\n+  const size_t free_including_headroom = soft_max_capacity - MIN2(soft_max_capacity, used);\n+  const size_t free = free_including_headroom - MIN2(free_including_headroom, XHeuristics::relocation_headroom());\n+\n+  \/\/ Calculate time until OOM given the max allocation rate and the amount\n+  \/\/ of free memory. The allocation rate is a moving average and we multiply\n+  \/\/ that with an allocation spike tolerance factor to guard against unforeseen\n+  \/\/ phase changes in the allocate rate. We then add ~3.3 sigma to account for\n+  \/\/ the allocation rate variance, which means the probability is 1 in 1000\n+  \/\/ that a sample is outside of the confidence interval.\n+  const double alloc_rate_predict = XStatAllocRate::predict();\n+  const double alloc_rate_avg = XStatAllocRate::avg();\n+  const double alloc_rate_sd = XStatAllocRate::sd();\n+  const double alloc_rate_sd_percent = alloc_rate_sd \/ (alloc_rate_avg + 1.0);\n+  const double alloc_rate = (MAX2(alloc_rate_predict, alloc_rate_avg) * ZAllocationSpikeTolerance) + (alloc_rate_sd * one_in_1000) + 1.0;\n+  const double time_until_oom = (free \/ alloc_rate) \/ (1.0 + alloc_rate_sd_percent);\n+\n+  \/\/ Calculate max serial\/parallel times of a GC cycle. The times are\n+  \/\/ moving averages, we add ~3.3 sigma to account for the variance.\n+  const double serial_gc_time = XStatCycle::serial_time().davg() + (XStatCycle::serial_time().dsd() * one_in_1000);\n+  const double parallelizable_gc_time = XStatCycle::parallelizable_time().davg() + (XStatCycle::parallelizable_time().dsd() * one_in_1000);\n+\n+  \/\/ Calculate number of GC workers needed to avoid OOM.\n+  const double gc_workers = select_gc_workers(serial_gc_time, parallelizable_gc_time, alloc_rate_sd_percent, time_until_oom);\n+\n+  \/\/ Convert to a discrete number of GC workers within limits.\n+  const uint actual_gc_workers = discrete_gc_workers(gc_workers);\n+\n+  \/\/ Calculate GC duration given number of GC workers needed.\n+  const double actual_gc_duration = serial_gc_time + (parallelizable_gc_time \/ actual_gc_workers);\n+  const uint last_gc_workers = XStatCycle::last_active_workers();\n+\n+  \/\/ Calculate time until GC given the time until OOM and GC duration.\n+  \/\/ We also subtract the sample interval, so that we don't overshoot the\n+  \/\/ target time and end up starting the GC too late in the next interval.\n+  const double time_until_gc = time_until_oom - actual_gc_duration - sample_interval;\n+\n+  log_debug(gc, director)(\"Rule: Allocation Rate (Dynamic GC Workers), \"\n+                          \"MaxAllocRate: %.1fMB\/s (+\/-%.1f%%), Free: \" SIZE_FORMAT \"MB, GCCPUTime: %.3f, \"\n+                          \"GCDuration: %.3fs, TimeUntilOOM: %.3fs, TimeUntilGC: %.3fs, GCWorkers: %u -> %u\",\n+                          alloc_rate \/ M,\n+                          alloc_rate_sd_percent * 100,\n+                          free \/ M,\n+                          serial_gc_time + parallelizable_gc_time,\n+                          serial_gc_time + (parallelizable_gc_time \/ actual_gc_workers),\n+                          time_until_oom,\n+                          time_until_gc,\n+                          last_gc_workers,\n+                          actual_gc_workers);\n+\n+  if (actual_gc_workers <= last_gc_workers && time_until_gc > 0) {\n+    return XDriverRequest(GCCause::_no_gc, actual_gc_workers);\n+  }\n+\n+  return XDriverRequest(GCCause::_z_allocation_rate, actual_gc_workers);\n+}\n+\n+static XDriverRequest rule_allocation_rate_static() {\n+  if (!XStatCycle::is_time_trustable()) {\n+    \/\/ Rule disabled\n+    return GCCause::_no_gc;\n+  }\n+\n+  \/\/ Perform GC if the estimated max allocation rate indicates that we\n+  \/\/ will run out of memory. The estimated max allocation rate is based\n+  \/\/ on the moving average of the sampled allocation rate plus a safety\n+  \/\/ margin based on variations in the allocation rate and unforeseen\n+  \/\/ allocation spikes.\n+\n+  \/\/ Calculate amount of free memory available. Note that we take the\n+  \/\/ relocation headroom into account to avoid in-place relocation.\n+  const size_t soft_max_capacity = XHeap::heap()->soft_max_capacity();\n+  const size_t used = XHeap::heap()->used();\n+  const size_t free_including_headroom = soft_max_capacity - MIN2(soft_max_capacity, used);\n+  const size_t free = free_including_headroom - MIN2(free_including_headroom, XHeuristics::relocation_headroom());\n+\n+  \/\/ Calculate time until OOM given the max allocation rate and the amount\n+  \/\/ of free memory. The allocation rate is a moving average and we multiply\n+  \/\/ that with an allocation spike tolerance factor to guard against unforeseen\n+  \/\/ phase changes in the allocate rate. We then add ~3.3 sigma to account for\n+  \/\/ the allocation rate variance, which means the probability is 1 in 1000\n+  \/\/ that a sample is outside of the confidence interval.\n+  const double max_alloc_rate = (XStatAllocRate::avg() * ZAllocationSpikeTolerance) + (XStatAllocRate::sd() * one_in_1000);\n+  const double time_until_oom = free \/ (max_alloc_rate + 1.0); \/\/ Plus 1.0B\/s to avoid division by zero\n+\n+  \/\/ Calculate max serial\/parallel times of a GC cycle. The times are\n+  \/\/ moving averages, we add ~3.3 sigma to account for the variance.\n+  const double serial_gc_time = XStatCycle::serial_time().davg() + (XStatCycle::serial_time().dsd() * one_in_1000);\n+  const double parallelizable_gc_time = XStatCycle::parallelizable_time().davg() + (XStatCycle::parallelizable_time().dsd() * one_in_1000);\n+\n+  \/\/ Calculate GC duration given number of GC workers needed.\n+  const double gc_duration = serial_gc_time + (parallelizable_gc_time \/ ConcGCThreads);\n+\n+  \/\/ Calculate time until GC given the time until OOM and max duration of GC.\n+  \/\/ We also deduct the sample interval, so that we don't overshoot the target\n+  \/\/ time and end up starting the GC too late in the next interval.\n+  const double time_until_gc = time_until_oom - gc_duration - sample_interval;\n+\n+  log_debug(gc, director)(\"Rule: Allocation Rate (Static GC Workers), MaxAllocRate: %.1fMB\/s, Free: \" SIZE_FORMAT \"MB, GCDuration: %.3fs, TimeUntilGC: %.3fs\",\n+                          max_alloc_rate \/ M, free \/ M, gc_duration, time_until_gc);\n+\n+  if (time_until_gc > 0) {\n+    return GCCause::_no_gc;\n+  }\n+\n+  return GCCause::_z_allocation_rate;\n+}\n+\n+static XDriverRequest rule_allocation_rate() {\n+  if (UseDynamicNumberOfGCThreads) {\n+    return rule_allocation_rate_dynamic();\n+  } else {\n+    return rule_allocation_rate_static();\n+  }\n+}\n+\n+static XDriverRequest rule_high_usage() {\n+  \/\/ Perform GC if the amount of free memory is 5% or less. This is a preventive\n+  \/\/ meassure in the case where the application has a very low allocation rate,\n+  \/\/ such that the allocation rate rule doesn't trigger, but the amount of free\n+  \/\/ memory is still slowly but surely heading towards zero. In this situation,\n+  \/\/ we start a GC cycle to avoid a potential allocation stall later.\n+\n+  \/\/ Calculate amount of free memory available. Note that we take the\n+  \/\/ relocation headroom into account to avoid in-place relocation.\n+  const size_t soft_max_capacity = XHeap::heap()->soft_max_capacity();\n+  const size_t used = XHeap::heap()->used();\n+  const size_t free_including_headroom = soft_max_capacity - MIN2(soft_max_capacity, used);\n+  const size_t free = free_including_headroom - MIN2(free_including_headroom, XHeuristics::relocation_headroom());\n+  const double free_percent = percent_of(free, soft_max_capacity);\n+\n+  log_debug(gc, director)(\"Rule: High Usage, Free: \" SIZE_FORMAT \"MB(%.1f%%)\",\n+                          free \/ M, free_percent);\n+\n+  if (free_percent > 5.0) {\n+    return GCCause::_no_gc;\n+  }\n+\n+  return GCCause::_z_high_usage;\n+}\n+\n+static XDriverRequest rule_proactive() {\n+  if (!ZProactive || !XStatCycle::is_warm()) {\n+    \/\/ Rule disabled\n+    return GCCause::_no_gc;\n+  }\n+\n+  \/\/ Perform GC if the impact of doing so, in terms of application throughput\n+  \/\/ reduction, is considered acceptable. This rule allows us to keep the heap\n+  \/\/ size down and allow reference processing to happen even when we have a lot\n+  \/\/ of free space on the heap.\n+\n+  \/\/ Only consider doing a proactive GC if the heap usage has grown by at least\n+  \/\/ 10% of the max capacity since the previous GC, or more than 5 minutes has\n+  \/\/ passed since the previous GC. This helps avoid superfluous GCs when running\n+  \/\/ applications with very low allocation rate.\n+  const size_t used_after_last_gc = XStatHeap::used_at_relocate_end();\n+  const size_t used_increase_threshold = XHeap::heap()->soft_max_capacity() * 0.10; \/\/ 10%\n+  const size_t used_threshold = used_after_last_gc + used_increase_threshold;\n+  const size_t used = XHeap::heap()->used();\n+  const double time_since_last_gc = XStatCycle::time_since_last();\n+  const double time_since_last_gc_threshold = 5 * 60; \/\/ 5 minutes\n+  if (used < used_threshold && time_since_last_gc < time_since_last_gc_threshold) {\n+    \/\/ Don't even consider doing a proactive GC\n+    log_debug(gc, director)(\"Rule: Proactive, UsedUntilEnabled: \" SIZE_FORMAT \"MB, TimeUntilEnabled: %.3fs\",\n+                            (used_threshold - used) \/ M,\n+                            time_since_last_gc_threshold - time_since_last_gc);\n+    return GCCause::_no_gc;\n+  }\n+\n+  const double assumed_throughput_drop_during_gc = 0.50; \/\/ 50%\n+  const double acceptable_throughput_drop = 0.01;        \/\/ 1%\n+  const double serial_gc_time = XStatCycle::serial_time().davg() + (XStatCycle::serial_time().dsd() * one_in_1000);\n+  const double parallelizable_gc_time = XStatCycle::parallelizable_time().davg() + (XStatCycle::parallelizable_time().dsd() * one_in_1000);\n+  const double gc_duration = serial_gc_time + (parallelizable_gc_time \/ ConcGCThreads);\n+  const double acceptable_gc_interval = gc_duration * ((assumed_throughput_drop_during_gc \/ acceptable_throughput_drop) - 1.0);\n+  const double time_until_gc = acceptable_gc_interval - time_since_last_gc;\n+\n+  log_debug(gc, director)(\"Rule: Proactive, AcceptableGCInterval: %.3fs, TimeSinceLastGC: %.3fs, TimeUntilGC: %.3fs\",\n+                          acceptable_gc_interval, time_since_last_gc, time_until_gc);\n+\n+  if (time_until_gc > 0) {\n+    return GCCause::_no_gc;\n+  }\n+\n+  return GCCause::_z_proactive;\n+}\n+\n+static XDriverRequest make_gc_decision() {\n+  \/\/ List of rules\n+  using XDirectorRule = XDriverRequest (*)();\n+  const XDirectorRule rules[] = {\n+    rule_allocation_stall,\n+    rule_warmup,\n+    rule_timer,\n+    rule_allocation_rate,\n+    rule_high_usage,\n+    rule_proactive,\n+  };\n+\n+  \/\/ Execute rules\n+  for (size_t i = 0; i < ARRAY_SIZE(rules); i++) {\n+    const XDriverRequest request = rules[i]();\n+    if (request.cause() != GCCause::_no_gc) {\n+      return request;\n+    }\n+  }\n+\n+  return GCCause::_no_gc;\n+}\n+\n+void XDirector::run_service() {\n+  \/\/ Main loop\n+  while (_metronome.wait_for_tick()) {\n+    sample_allocation_rate();\n+    if (!_driver->is_busy()) {\n+      const XDriverRequest request = make_gc_decision();\n+      if (request.cause() != GCCause::_no_gc) {\n+        _driver->collect(request);\n+      }\n+    }\n+  }\n+}\n+\n+void XDirector::stop_service() {\n+  _metronome.stop();\n+}\n","filename":"src\/hotspot\/share\/gc\/x\/xDirector.cpp","additions":406,"deletions":0,"binary":false,"changes":406,"status":"added"},{"patch":"@@ -0,0 +1,45 @@\n+\/*\n+ * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XDIRECTOR_HPP\n+#define SHARE_GC_X_XDIRECTOR_HPP\n+\n+#include \"gc\/shared\/concurrentGCThread.hpp\"\n+#include \"gc\/x\/xMetronome.hpp\"\n+\n+class XDriver;\n+\n+class XDirector : public ConcurrentGCThread {\n+private:\n+  XDriver* const _driver;\n+  XMetronome     _metronome;\n+\n+protected:\n+  virtual void run_service();\n+  virtual void stop_service();\n+\n+public:\n+  XDirector(XDriver* driver);\n+};\n+\n+#endif \/\/ SHARE_GC_X_XDIRECTOR_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xDirector.hpp","additions":45,"deletions":0,"binary":false,"changes":45,"status":"added"},{"patch":"@@ -0,0 +1,513 @@\n+\/*\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/shared\/gcId.hpp\"\n+#include \"gc\/shared\/gcLocker.hpp\"\n+#include \"gc\/shared\/gcVMOperations.hpp\"\n+#include \"gc\/shared\/isGCActiveMark.hpp\"\n+#include \"gc\/x\/xAbort.inline.hpp\"\n+#include \"gc\/x\/xBreakpoint.hpp\"\n+#include \"gc\/x\/xCollectedHeap.hpp\"\n+#include \"gc\/x\/xDriver.hpp\"\n+#include \"gc\/x\/xHeap.inline.hpp\"\n+#include \"gc\/x\/xMessagePort.inline.hpp\"\n+#include \"gc\/x\/xServiceability.hpp\"\n+#include \"gc\/x\/xStat.hpp\"\n+#include \"gc\/x\/xVerify.hpp\"\n+#include \"logging\/log.hpp\"\n+#include \"memory\/universe.hpp\"\n+#include \"runtime\/threads.hpp\"\n+#include \"runtime\/vmOperations.hpp\"\n+#include \"runtime\/vmThread.hpp\"\n+\n+static const XStatPhaseCycle      XPhaseCycle(\"Garbage Collection Cycle\");\n+static const XStatPhasePause      XPhasePauseMarkStart(\"Pause Mark Start\");\n+static const XStatPhaseConcurrent XPhaseConcurrentMark(\"Concurrent Mark\");\n+static const XStatPhaseConcurrent XPhaseConcurrentMarkContinue(\"Concurrent Mark Continue\");\n+static const XStatPhaseConcurrent XPhaseConcurrentMarkFree(\"Concurrent Mark Free\");\n+static const XStatPhasePause      XPhasePauseMarkEnd(\"Pause Mark End\");\n+static const XStatPhaseConcurrent XPhaseConcurrentProcessNonStrongReferences(\"Concurrent Process Non-Strong References\");\n+static const XStatPhaseConcurrent XPhaseConcurrentResetRelocationSet(\"Concurrent Reset Relocation Set\");\n+static const XStatPhaseConcurrent XPhaseConcurrentSelectRelocationSet(\"Concurrent Select Relocation Set\");\n+static const XStatPhasePause      XPhasePauseRelocateStart(\"Pause Relocate Start\");\n+static const XStatPhaseConcurrent XPhaseConcurrentRelocated(\"Concurrent Relocate\");\n+static const XStatCriticalPhase   XCriticalPhaseGCLockerStall(\"GC Locker Stall\", false \/* verbose *\/);\n+static const XStatSampler         XSamplerJavaThreads(\"System\", \"Java Threads\", XStatUnitThreads);\n+\n+XDriverRequest::XDriverRequest() :\n+    XDriverRequest(GCCause::_no_gc) {}\n+\n+XDriverRequest::XDriverRequest(GCCause::Cause cause) :\n+    XDriverRequest(cause, ConcGCThreads) {}\n+\n+XDriverRequest::XDriverRequest(GCCause::Cause cause, uint nworkers) :\n+    _cause(cause),\n+    _nworkers(nworkers) {}\n+\n+bool XDriverRequest::operator==(const XDriverRequest& other) const {\n+  return _cause == other._cause;\n+}\n+\n+GCCause::Cause XDriverRequest::cause() const {\n+  return _cause;\n+}\n+\n+uint XDriverRequest::nworkers() const {\n+  return _nworkers;\n+}\n+\n+class VM_XOperation : public VM_Operation {\n+private:\n+  const uint _gc_id;\n+  bool       _gc_locked;\n+  bool       _success;\n+\n+public:\n+  VM_XOperation() :\n+      _gc_id(GCId::current()),\n+      _gc_locked(false),\n+      _success(false) {}\n+\n+  virtual bool needs_inactive_gc_locker() const {\n+    \/\/ An inactive GC locker is needed in operations where we change the bad\n+    \/\/ mask or move objects. Changing the bad mask will invalidate all oops,\n+    \/\/ which makes it conceptually the same thing as moving all objects.\n+    return false;\n+  }\n+\n+  virtual bool skip_thread_oop_barriers() const {\n+    return true;\n+  }\n+\n+  virtual bool do_operation() = 0;\n+\n+  virtual bool doit_prologue() {\n+    Heap_lock->lock();\n+    return true;\n+  }\n+\n+  virtual void doit() {\n+    \/\/ Abort if GC locker state is incompatible\n+    if (needs_inactive_gc_locker() && GCLocker::check_active_before_gc()) {\n+      _gc_locked = true;\n+      return;\n+    }\n+\n+    \/\/ Setup GC id and active marker\n+    GCIdMark gc_id_mark(_gc_id);\n+    IsGCActiveMark gc_active_mark;\n+\n+    \/\/ Verify before operation\n+    XVerify::before_zoperation();\n+\n+    \/\/ Execute operation\n+    _success = do_operation();\n+\n+    \/\/ Update statistics\n+    XStatSample(XSamplerJavaThreads, Threads::number_of_threads());\n+  }\n+\n+  virtual void doit_epilogue() {\n+    Heap_lock->unlock();\n+  }\n+\n+  bool gc_locked() const {\n+    return _gc_locked;\n+  }\n+\n+  bool success() const {\n+    return _success;\n+  }\n+};\n+\n+class VM_XMarkStart : public VM_XOperation {\n+public:\n+  virtual VMOp_Type type() const {\n+    return VMOp_XMarkStart;\n+  }\n+\n+  virtual bool needs_inactive_gc_locker() const {\n+    return true;\n+  }\n+\n+  virtual bool do_operation() {\n+    XStatTimer timer(XPhasePauseMarkStart);\n+    XServiceabilityPauseTracer tracer;\n+\n+    XCollectedHeap::heap()->increment_total_collections(true \/* full *\/);\n+\n+    XHeap::heap()->mark_start();\n+    return true;\n+  }\n+};\n+\n+class VM_XMarkEnd : public VM_XOperation {\n+public:\n+  virtual VMOp_Type type() const {\n+    return VMOp_XMarkEnd;\n+  }\n+\n+  virtual bool do_operation() {\n+    XStatTimer timer(XPhasePauseMarkEnd);\n+    XServiceabilityPauseTracer tracer;\n+    return XHeap::heap()->mark_end();\n+  }\n+};\n+\n+class VM_XRelocateStart : public VM_XOperation {\n+public:\n+  virtual VMOp_Type type() const {\n+    return VMOp_XRelocateStart;\n+  }\n+\n+  virtual bool needs_inactive_gc_locker() const {\n+    return true;\n+  }\n+\n+  virtual bool do_operation() {\n+    XStatTimer timer(XPhasePauseRelocateStart);\n+    XServiceabilityPauseTracer tracer;\n+    XHeap::heap()->relocate_start();\n+    return true;\n+  }\n+};\n+\n+class VM_XVerify : public VM_Operation {\n+public:\n+  virtual VMOp_Type type() const {\n+    return VMOp_XVerify;\n+  }\n+\n+  virtual bool skip_thread_oop_barriers() const {\n+    return true;\n+  }\n+\n+  virtual void doit() {\n+    XVerify::after_weak_processing();\n+  }\n+};\n+\n+XDriver::XDriver() :\n+    _gc_cycle_port(),\n+    _gc_locker_port() {\n+  set_name(\"XDriver\");\n+  create_and_start();\n+}\n+\n+bool XDriver::is_busy() const {\n+  return _gc_cycle_port.is_busy();\n+}\n+\n+void XDriver::collect(const XDriverRequest& request) {\n+  switch (request.cause()) {\n+  case GCCause::_heap_dump:\n+  case GCCause::_heap_inspection:\n+  case GCCause::_wb_young_gc:\n+  case GCCause::_wb_full_gc:\n+  case GCCause::_dcmd_gc_run:\n+  case GCCause::_java_lang_system_gc:\n+  case GCCause::_full_gc_alot:\n+  case GCCause::_scavenge_alot:\n+  case GCCause::_jvmti_force_gc:\n+  case GCCause::_metadata_GC_clear_soft_refs:\n+  case GCCause::_codecache_GC_aggressive:\n+    \/\/ Start synchronous GC\n+    _gc_cycle_port.send_sync(request);\n+    break;\n+\n+  case GCCause::_z_timer:\n+  case GCCause::_z_warmup:\n+  case GCCause::_z_allocation_rate:\n+  case GCCause::_z_allocation_stall:\n+  case GCCause::_z_proactive:\n+  case GCCause::_z_high_usage:\n+  case GCCause::_codecache_GC_threshold:\n+  case GCCause::_metadata_GC_threshold:\n+    \/\/ Start asynchronous GC\n+    _gc_cycle_port.send_async(request);\n+    break;\n+\n+  case GCCause::_gc_locker:\n+    \/\/ Restart VM operation previously blocked by the GC locker\n+    _gc_locker_port.signal();\n+    break;\n+\n+  case GCCause::_wb_breakpoint:\n+    XBreakpoint::start_gc();\n+    _gc_cycle_port.send_async(request);\n+    break;\n+\n+  default:\n+    \/\/ Other causes not supported\n+    fatal(\"Unsupported GC cause (%s)\", GCCause::to_string(request.cause()));\n+    break;\n+  }\n+}\n+\n+template <typename T>\n+bool XDriver::pause() {\n+  for (;;) {\n+    T op;\n+    VMThread::execute(&op);\n+    if (op.gc_locked()) {\n+      \/\/ Wait for GC to become unlocked and restart the VM operation\n+      XStatTimer timer(XCriticalPhaseGCLockerStall);\n+      _gc_locker_port.wait();\n+      continue;\n+    }\n+\n+    \/\/ Notify VM operation completed\n+    _gc_locker_port.ack();\n+\n+    return op.success();\n+  }\n+}\n+\n+void XDriver::pause_mark_start() {\n+  pause<VM_XMarkStart>();\n+}\n+\n+void XDriver::concurrent_mark() {\n+  XStatTimer timer(XPhaseConcurrentMark);\n+  XBreakpoint::at_after_marking_started();\n+  XHeap::heap()->mark(true \/* initial *\/);\n+  XBreakpoint::at_before_marking_completed();\n+}\n+\n+bool XDriver::pause_mark_end() {\n+  return pause<VM_XMarkEnd>();\n+}\n+\n+void XDriver::concurrent_mark_continue() {\n+  XStatTimer timer(XPhaseConcurrentMarkContinue);\n+  XHeap::heap()->mark(false \/* initial *\/);\n+}\n+\n+void XDriver::concurrent_mark_free() {\n+  XStatTimer timer(XPhaseConcurrentMarkFree);\n+  XHeap::heap()->mark_free();\n+}\n+\n+void XDriver::concurrent_process_non_strong_references() {\n+  XStatTimer timer(XPhaseConcurrentProcessNonStrongReferences);\n+  XBreakpoint::at_after_reference_processing_started();\n+  XHeap::heap()->process_non_strong_references();\n+}\n+\n+void XDriver::concurrent_reset_relocation_set() {\n+  XStatTimer timer(XPhaseConcurrentResetRelocationSet);\n+  XHeap::heap()->reset_relocation_set();\n+}\n+\n+void XDriver::pause_verify() {\n+  if (ZVerifyRoots || ZVerifyObjects) {\n+    VM_XVerify op;\n+    VMThread::execute(&op);\n+  }\n+}\n+\n+void XDriver::concurrent_select_relocation_set() {\n+  XStatTimer timer(XPhaseConcurrentSelectRelocationSet);\n+  XHeap::heap()->select_relocation_set();\n+}\n+\n+void XDriver::pause_relocate_start() {\n+  pause<VM_XRelocateStart>();\n+}\n+\n+void XDriver::concurrent_relocate() {\n+  XStatTimer timer(XPhaseConcurrentRelocated);\n+  XHeap::heap()->relocate();\n+}\n+\n+void XDriver::check_out_of_memory() {\n+  XHeap::heap()->check_out_of_memory();\n+}\n+\n+static bool should_clear_soft_references(const XDriverRequest& request) {\n+  \/\/ Clear soft references if implied by the GC cause\n+  if (request.cause() == GCCause::_wb_full_gc ||\n+      request.cause() == GCCause::_metadata_GC_clear_soft_refs ||\n+      request.cause() == GCCause::_z_allocation_stall) {\n+    \/\/ Clear\n+    return true;\n+  }\n+\n+  \/\/ Don't clear\n+  return false;\n+}\n+\n+static uint select_active_worker_threads_dynamic(const XDriverRequest& request) {\n+  \/\/ Use requested number of worker threads\n+  return request.nworkers();\n+}\n+\n+static uint select_active_worker_threads_static(const XDriverRequest& request) {\n+  const GCCause::Cause cause = request.cause();\n+  const uint nworkers = request.nworkers();\n+\n+  \/\/ Boost number of worker threads if implied by the GC cause\n+  if (cause == GCCause::_wb_full_gc ||\n+      cause == GCCause::_java_lang_system_gc ||\n+      cause == GCCause::_metadata_GC_clear_soft_refs ||\n+      cause == GCCause::_z_allocation_stall) {\n+    \/\/ Boost\n+    const uint boosted_nworkers = MAX2(nworkers, ParallelGCThreads);\n+    return boosted_nworkers;\n+  }\n+\n+  \/\/ Use requested number of worker threads\n+  return nworkers;\n+}\n+\n+static uint select_active_worker_threads(const XDriverRequest& request) {\n+  if (UseDynamicNumberOfGCThreads) {\n+    return select_active_worker_threads_dynamic(request);\n+  } else {\n+    return select_active_worker_threads_static(request);\n+  }\n+}\n+\n+class XDriverGCScope : public StackObj {\n+private:\n+  GCIdMark                   _gc_id;\n+  GCCause::Cause             _gc_cause;\n+  GCCauseSetter              _gc_cause_setter;\n+  XStatTimer                 _timer;\n+  XServiceabilityCycleTracer _tracer;\n+\n+public:\n+  XDriverGCScope(const XDriverRequest& request) :\n+      _gc_id(),\n+      _gc_cause(request.cause()),\n+      _gc_cause_setter(XCollectedHeap::heap(), _gc_cause),\n+      _timer(XPhaseCycle),\n+      _tracer() {\n+    \/\/ Update statistics\n+    XStatCycle::at_start();\n+\n+    \/\/ Set up soft reference policy\n+    const bool clear = should_clear_soft_references(request);\n+    XHeap::heap()->set_soft_reference_policy(clear);\n+\n+    \/\/ Select number of worker threads to use\n+    const uint nworkers = select_active_worker_threads(request);\n+    XHeap::heap()->set_active_workers(nworkers);\n+  }\n+\n+  ~XDriverGCScope() {\n+    \/\/ Update statistics\n+    XStatCycle::at_end(_gc_cause, XHeap::heap()->active_workers());\n+\n+    \/\/ Update data used by soft reference policy\n+    Universe::heap()->update_capacity_and_used_at_gc();\n+\n+    \/\/ Signal that we have completed a visit to all live objects\n+    Universe::heap()->record_whole_heap_examined_timestamp();\n+  }\n+};\n+\n+\/\/ Macro to execute a termination check after a concurrent phase. Note\n+\/\/ that it's important that the termination check comes after the call\n+\/\/ to the function f, since we can't abort between pause_relocate_start()\n+\/\/ and concurrent_relocate(). We need to let concurrent_relocate() call\n+\/\/ abort_page() on the remaining entries in the relocation set.\n+#define concurrent(f)                 \\\n+  do {                                \\\n+    concurrent_##f();                 \\\n+    if (should_terminate()) {         \\\n+      return;                         \\\n+    }                                 \\\n+  } while (false)\n+\n+void XDriver::gc(const XDriverRequest& request) {\n+  XDriverGCScope scope(request);\n+\n+  \/\/ Phase 1: Pause Mark Start\n+  pause_mark_start();\n+\n+  \/\/ Phase 2: Concurrent Mark\n+  concurrent(mark);\n+\n+  \/\/ Phase 3: Pause Mark End\n+  while (!pause_mark_end()) {\n+    \/\/ Phase 3.5: Concurrent Mark Continue\n+    concurrent(mark_continue);\n+  }\n+\n+  \/\/ Phase 4: Concurrent Mark Free\n+  concurrent(mark_free);\n+\n+  \/\/ Phase 5: Concurrent Process Non-Strong References\n+  concurrent(process_non_strong_references);\n+\n+  \/\/ Phase 6: Concurrent Reset Relocation Set\n+  concurrent(reset_relocation_set);\n+\n+  \/\/ Phase 7: Pause Verify\n+  pause_verify();\n+\n+  \/\/ Phase 8: Concurrent Select Relocation Set\n+  concurrent(select_relocation_set);\n+\n+  \/\/ Phase 9: Pause Relocate Start\n+  pause_relocate_start();\n+\n+  \/\/ Phase 10: Concurrent Relocate\n+  concurrent(relocate);\n+}\n+\n+void XDriver::run_service() {\n+  \/\/ Main loop\n+  while (!should_terminate()) {\n+    \/\/ Wait for GC request\n+    const XDriverRequest request = _gc_cycle_port.receive();\n+    if (request.cause() == GCCause::_no_gc) {\n+      continue;\n+    }\n+\n+    XBreakpoint::at_before_gc();\n+\n+    \/\/ Run GC\n+    gc(request);\n+\n+    if (should_terminate()) {\n+      \/\/ Abort\n+      break;\n+    }\n+\n+    \/\/ Notify GC completed\n+    _gc_cycle_port.ack();\n+\n+    \/\/ Check for out of memory condition\n+    check_out_of_memory();\n+\n+    XBreakpoint::at_after_gc();\n+  }\n+}\n+\n+void XDriver::stop_service() {\n+  XAbort::abort();\n+  _gc_cycle_port.send_async(GCCause::_no_gc);\n+}\n","filename":"src\/hotspot\/share\/gc\/x\/xDriver.cpp","additions":513,"deletions":0,"binary":false,"changes":513,"status":"added"},{"patch":"@@ -0,0 +1,84 @@\n+\/*\n+ * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XDRIVER_HPP\n+#define SHARE_GC_X_XDRIVER_HPP\n+\n+#include \"gc\/shared\/concurrentGCThread.hpp\"\n+#include \"gc\/shared\/gcCause.hpp\"\n+#include \"gc\/x\/xMessagePort.hpp\"\n+\n+class VM_XOperation;\n+\n+class XDriverRequest {\n+private:\n+  GCCause::Cause _cause;\n+  uint           _nworkers;\n+\n+public:\n+  XDriverRequest();\n+  XDriverRequest(GCCause::Cause cause);\n+  XDriverRequest(GCCause::Cause cause, uint nworkers);\n+\n+  bool operator==(const XDriverRequest& other) const;\n+\n+  GCCause::Cause cause() const;\n+  uint nworkers() const;\n+};\n+\n+class XDriver : public ConcurrentGCThread {\n+private:\n+  XMessagePort<XDriverRequest> _gc_cycle_port;\n+  XRendezvousPort              _gc_locker_port;\n+\n+  template <typename T> bool pause();\n+\n+  void pause_mark_start();\n+  void concurrent_mark();\n+  bool pause_mark_end();\n+  void concurrent_mark_continue();\n+  void concurrent_mark_free();\n+  void concurrent_process_non_strong_references();\n+  void concurrent_reset_relocation_set();\n+  void pause_verify();\n+  void concurrent_select_relocation_set();\n+  void pause_relocate_start();\n+  void concurrent_relocate();\n+\n+  void check_out_of_memory();\n+\n+  void gc(const XDriverRequest& request);\n+\n+protected:\n+  virtual void run_service();\n+  virtual void stop_service();\n+\n+public:\n+  XDriver();\n+\n+  bool is_busy() const;\n+\n+  void collect(const XDriverRequest& request);\n+};\n+\n+#endif \/\/ SHARE_GC_X_XDRIVER_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xDriver.hpp","additions":84,"deletions":0,"binary":false,"changes":84,"status":"added"},{"patch":"@@ -0,0 +1,51 @@\n+\/*\n+ * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/x\/xErrno.hpp\"\n+#include \"runtime\/os.hpp\"\n+\n+#include <errno.h>\n+#include <string.h>\n+\n+XErrno::XErrno() :\n+    _error(errno) {}\n+\n+XErrno::XErrno(int error) :\n+    _error(error) {}\n+\n+XErrno::operator bool() const {\n+  return _error != 0;\n+}\n+\n+bool XErrno::operator==(int error) const {\n+  return _error == error;\n+}\n+\n+bool XErrno::operator!=(int error) const {\n+  return _error != error;\n+}\n+\n+const char* XErrno::to_string() const {\n+  return os::strerror(_error);\n+}\n","filename":"src\/hotspot\/share\/gc\/x\/xErrno.cpp","additions":51,"deletions":0,"binary":false,"changes":51,"status":"added"},{"patch":"@@ -0,0 +1,43 @@\n+\/*\n+ * Copyright (c) 2015, 2017, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XERRNO_HPP\n+#define SHARE_GC_X_XERRNO_HPP\n+\n+#include \"memory\/allocation.hpp\"\n+\n+class XErrno : public StackObj {\n+private:\n+  const int _error;\n+\n+public:\n+  XErrno();\n+  XErrno(int error);\n+\n+  operator bool() const;\n+  bool operator==(int error) const;\n+  bool operator!=(int error) const;\n+  const char* to_string() const;\n+};\n+\n+#endif \/\/ SHARE_GC_X_XERRNO_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xErrno.hpp","additions":43,"deletions":0,"binary":false,"changes":43,"status":"added"},{"patch":"@@ -0,0 +1,210 @@\n+\/*\n+ * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/x\/xAddress.inline.hpp\"\n+#include \"gc\/x\/xForwarding.inline.hpp\"\n+#include \"gc\/x\/xStat.hpp\"\n+#include \"gc\/x\/xUtils.inline.hpp\"\n+#include \"utilities\/align.hpp\"\n+\n+\/\/\n+\/\/ Reference count states:\n+\/\/\n+\/\/ * If the reference count is zero, it will never change again.\n+\/\/\n+\/\/ * If the reference count is positive, it can be both retained\n+\/\/   (increased) and released (decreased).\n+\/\/\n+\/\/ * If the reference count is negative, is can only be released\n+\/\/   (increased). A negative reference count means that one or more\n+\/\/   threads are waiting for one or more other threads to release\n+\/\/   their references.\n+\/\/\n+\/\/ The reference lock is used for waiting until the reference\n+\/\/ count has become zero (released) or negative one (claimed).\n+\/\/\n+\n+static const XStatCriticalPhase XCriticalPhaseRelocationStall(\"Relocation Stall\");\n+\n+bool XForwarding::retain_page() {\n+  for (;;) {\n+    const int32_t ref_count = Atomic::load_acquire(&_ref_count);\n+\n+    if (ref_count == 0) {\n+      \/\/ Released\n+      return false;\n+    }\n+\n+    if (ref_count < 0) {\n+      \/\/ Claimed\n+      const bool success = wait_page_released();\n+      assert(success, \"Should always succeed\");\n+      return false;\n+    }\n+\n+    if (Atomic::cmpxchg(&_ref_count, ref_count, ref_count + 1) == ref_count) {\n+      \/\/ Retained\n+      return true;\n+    }\n+  }\n+}\n+\n+XPage* XForwarding::claim_page() {\n+  for (;;) {\n+    const int32_t ref_count = Atomic::load(&_ref_count);\n+    assert(ref_count > 0, \"Invalid state\");\n+\n+    \/\/ Invert reference count\n+    if (Atomic::cmpxchg(&_ref_count, ref_count, -ref_count) != ref_count) {\n+      continue;\n+    }\n+\n+    \/\/ If the previous reference count was 1, then we just changed it to -1,\n+    \/\/ and we have now claimed the page. Otherwise we wait until it is claimed.\n+    if (ref_count != 1) {\n+      XLocker<XConditionLock> locker(&_ref_lock);\n+      while (Atomic::load_acquire(&_ref_count) != -1) {\n+        _ref_lock.wait();\n+      }\n+    }\n+\n+    return _page;\n+  }\n+}\n+\n+void XForwarding::release_page() {\n+  for (;;) {\n+    const int32_t ref_count = Atomic::load(&_ref_count);\n+    assert(ref_count != 0, \"Invalid state\");\n+\n+    if (ref_count > 0) {\n+      \/\/ Decrement reference count\n+      if (Atomic::cmpxchg(&_ref_count, ref_count, ref_count - 1) != ref_count) {\n+        continue;\n+      }\n+\n+      \/\/ If the previous reference count was 1, then we just decremented\n+      \/\/ it to 0 and we should signal that the page is now released.\n+      if (ref_count == 1) {\n+        \/\/ Notify released\n+        XLocker<XConditionLock> locker(&_ref_lock);\n+        _ref_lock.notify_all();\n+      }\n+    } else {\n+      \/\/ Increment reference count\n+      if (Atomic::cmpxchg(&_ref_count, ref_count, ref_count + 1) != ref_count) {\n+        continue;\n+      }\n+\n+      \/\/ If the previous reference count was -2 or -1, then we just incremented it\n+      \/\/ to -1 or 0, and we should signal the that page is now claimed or released.\n+      if (ref_count == -2 || ref_count == -1) {\n+        \/\/ Notify claimed or released\n+        XLocker<XConditionLock> locker(&_ref_lock);\n+        _ref_lock.notify_all();\n+      }\n+    }\n+\n+    return;\n+  }\n+}\n+\n+bool XForwarding::wait_page_released() const {\n+  if (Atomic::load_acquire(&_ref_count) != 0) {\n+    XStatTimer timer(XCriticalPhaseRelocationStall);\n+    XLocker<XConditionLock> locker(&_ref_lock);\n+    while (Atomic::load_acquire(&_ref_count) != 0) {\n+      if (_ref_abort) {\n+        return false;\n+      }\n+\n+      _ref_lock.wait();\n+    }\n+  }\n+\n+  return true;\n+}\n+\n+XPage* XForwarding::detach_page() {\n+  \/\/ Wait until released\n+  if (Atomic::load_acquire(&_ref_count) != 0) {\n+    XLocker<XConditionLock> locker(&_ref_lock);\n+    while (Atomic::load_acquire(&_ref_count) != 0) {\n+      _ref_lock.wait();\n+    }\n+  }\n+\n+  \/\/ Detach and return page\n+  XPage* const page = _page;\n+  _page = NULL;\n+  return page;\n+}\n+\n+void XForwarding::abort_page() {\n+  XLocker<XConditionLock> locker(&_ref_lock);\n+  assert(Atomic::load(&_ref_count) > 0, \"Invalid state\");\n+  assert(!_ref_abort, \"Invalid state\");\n+  _ref_abort = true;\n+  _ref_lock.notify_all();\n+}\n+\n+void XForwarding::verify() const {\n+  guarantee(_ref_count != 0, \"Invalid reference count\");\n+  guarantee(_page != NULL, \"Invalid page\");\n+\n+  uint32_t live_objects = 0;\n+  size_t live_bytes = 0;\n+\n+  for (XForwardingCursor i = 0; i < _entries.length(); i++) {\n+    const XForwardingEntry entry = at(&i);\n+    if (!entry.populated()) {\n+      \/\/ Skip empty entries\n+      continue;\n+    }\n+\n+    \/\/ Check from index\n+    guarantee(entry.from_index() < _page->object_max_count(), \"Invalid from index\");\n+\n+    \/\/ Check for duplicates\n+    for (XForwardingCursor j = i + 1; j < _entries.length(); j++) {\n+      const XForwardingEntry other = at(&j);\n+      if (!other.populated()) {\n+        \/\/ Skip empty entries\n+        continue;\n+      }\n+\n+      guarantee(entry.from_index() != other.from_index(), \"Duplicate from\");\n+      guarantee(entry.to_offset() != other.to_offset(), \"Duplicate to\");\n+    }\n+\n+    const uintptr_t to_addr = XAddress::good(entry.to_offset());\n+    const size_t size = XUtils::object_size(to_addr);\n+    const size_t aligned_size = align_up(size, _page->object_alignment());\n+    live_bytes += aligned_size;\n+    live_objects++;\n+  }\n+\n+  \/\/ Verify number of live objects and bytes\n+  _page->verify_live(live_objects, live_bytes);\n+}\n","filename":"src\/hotspot\/share\/gc\/x\/xForwarding.cpp","additions":210,"deletions":0,"binary":false,"changes":210,"status":"added"},{"patch":"@@ -0,0 +1,88 @@\n+\/*\n+ * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XFORWARDING_HPP\n+#define SHARE_GC_X_XFORWARDING_HPP\n+\n+#include \"gc\/x\/xAttachedArray.hpp\"\n+#include \"gc\/x\/xForwardingEntry.hpp\"\n+#include \"gc\/x\/xLock.hpp\"\n+#include \"gc\/x\/xVirtualMemory.hpp\"\n+\n+class ObjectClosure;\n+class VMStructs;\n+class XForwardingAllocator;\n+class XPage;\n+\n+typedef size_t XForwardingCursor;\n+\n+class XForwarding {\n+  friend class ::VMStructs;\n+  friend class XForwardingTest;\n+\n+private:\n+  typedef XAttachedArray<XForwarding, XForwardingEntry> AttachedArray;\n+\n+  const XVirtualMemory   _virtual;\n+  const size_t           _object_alignment_shift;\n+  const AttachedArray    _entries;\n+  XPage*                 _page;\n+  mutable XConditionLock _ref_lock;\n+  volatile int32_t       _ref_count;\n+  bool                   _ref_abort;\n+  bool                   _in_place;\n+\n+  XForwardingEntry* entries() const;\n+  XForwardingEntry at(XForwardingCursor* cursor) const;\n+  XForwardingEntry first(uintptr_t from_index, XForwardingCursor* cursor) const;\n+  XForwardingEntry next(XForwardingCursor* cursor) const;\n+\n+  XForwarding(XPage* page, size_t nentries);\n+\n+public:\n+  static uint32_t nentries(const XPage* page);\n+  static XForwarding* alloc(XForwardingAllocator* allocator, XPage* page);\n+\n+  uint8_t type() const;\n+  uintptr_t start() const;\n+  size_t size() const;\n+  size_t object_alignment_shift() const;\n+  void object_iterate(ObjectClosure *cl);\n+\n+  bool retain_page();\n+  XPage* claim_page();\n+  void release_page();\n+  bool wait_page_released() const;\n+  XPage* detach_page();\n+  void abort_page();\n+\n+  void set_in_place();\n+  bool in_place() const;\n+\n+  XForwardingEntry find(uintptr_t from_index, XForwardingCursor* cursor) const;\n+  uintptr_t insert(uintptr_t from_index, uintptr_t to_offset, XForwardingCursor* cursor);\n+\n+  void verify() const;\n+};\n+\n+#endif \/\/ SHARE_GC_X_XFORWARDING_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xForwarding.hpp","additions":88,"deletions":0,"binary":false,"changes":88,"status":"added"},{"patch":"@@ -0,0 +1,163 @@\n+\/*\n+ * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XFORWARDING_INLINE_HPP\n+#define SHARE_GC_X_XFORWARDING_INLINE_HPP\n+\n+#include \"gc\/x\/xForwarding.hpp\"\n+\n+#include \"gc\/x\/xAttachedArray.inline.hpp\"\n+#include \"gc\/x\/xForwardingAllocator.inline.hpp\"\n+#include \"gc\/x\/xHash.inline.hpp\"\n+#include \"gc\/x\/xHeap.hpp\"\n+#include \"gc\/x\/xLock.inline.hpp\"\n+#include \"gc\/x\/xPage.inline.hpp\"\n+#include \"gc\/x\/xVirtualMemory.inline.hpp\"\n+#include \"runtime\/atomic.hpp\"\n+#include \"utilities\/debug.hpp\"\n+#include \"utilities\/powerOfTwo.hpp\"\n+\n+inline uint32_t XForwarding::nentries(const XPage* page) {\n+  \/\/ The number returned by the function is used to size the hash table of\n+  \/\/ forwarding entries for this page. This hash table uses linear probing.\n+  \/\/ The size of the table must be a power of two to allow for quick and\n+  \/\/ inexpensive indexing\/masking. The table is also sized to have a load\n+  \/\/ factor of 50%, i.e. sized to have double the number of entries actually\n+  \/\/ inserted, to allow for good lookup\/insert performance.\n+  return round_up_power_of_2(page->live_objects() * 2);\n+}\n+\n+inline XForwarding* XForwarding::alloc(XForwardingAllocator* allocator, XPage* page) {\n+  const size_t nentries = XForwarding::nentries(page);\n+  void* const addr = AttachedArray::alloc(allocator, nentries);\n+  return ::new (addr) XForwarding(page, nentries);\n+}\n+\n+inline XForwarding::XForwarding(XPage* page, size_t nentries) :\n+    _virtual(page->virtual_memory()),\n+    _object_alignment_shift(page->object_alignment_shift()),\n+    _entries(nentries),\n+    _page(page),\n+    _ref_lock(),\n+    _ref_count(1),\n+    _ref_abort(false),\n+    _in_place(false) {}\n+\n+inline uint8_t XForwarding::type() const {\n+  return _page->type();\n+}\n+\n+inline uintptr_t XForwarding::start() const {\n+  return _virtual.start();\n+}\n+\n+inline size_t XForwarding::size() const {\n+  return _virtual.size();\n+}\n+\n+inline size_t XForwarding::object_alignment_shift() const {\n+  return _object_alignment_shift;\n+}\n+\n+inline void XForwarding::object_iterate(ObjectClosure *cl) {\n+  return _page->object_iterate(cl);\n+}\n+\n+inline void XForwarding::set_in_place() {\n+  _in_place = true;\n+}\n+\n+inline bool XForwarding::in_place() const {\n+  return _in_place;\n+}\n+\n+inline XForwardingEntry* XForwarding::entries() const {\n+  return _entries(this);\n+}\n+\n+inline XForwardingEntry XForwarding::at(XForwardingCursor* cursor) const {\n+  \/\/ Load acquire for correctness with regards to\n+  \/\/ accesses to the contents of the forwarded object.\n+  return Atomic::load_acquire(entries() + *cursor);\n+}\n+\n+inline XForwardingEntry XForwarding::first(uintptr_t from_index, XForwardingCursor* cursor) const {\n+  const size_t mask = _entries.length() - 1;\n+  const size_t hash = XHash::uint32_to_uint32((uint32_t)from_index);\n+  *cursor = hash & mask;\n+  return at(cursor);\n+}\n+\n+inline XForwardingEntry XForwarding::next(XForwardingCursor* cursor) const {\n+  const size_t mask = _entries.length() - 1;\n+  *cursor = (*cursor + 1) & mask;\n+  return at(cursor);\n+}\n+\n+inline XForwardingEntry XForwarding::find(uintptr_t from_index, XForwardingCursor* cursor) const {\n+  \/\/ Reading entries in the table races with the atomic CAS done for\n+  \/\/ insertion into the table. This is safe because each entry is at\n+  \/\/ most updated once (from zero to something else).\n+  XForwardingEntry entry = first(from_index, cursor);\n+  while (entry.populated()) {\n+    if (entry.from_index() == from_index) {\n+      \/\/ Match found, return matching entry\n+      return entry;\n+    }\n+\n+    entry = next(cursor);\n+  }\n+\n+  \/\/ Match not found, return empty entry\n+  return entry;\n+}\n+\n+inline uintptr_t XForwarding::insert(uintptr_t from_index, uintptr_t to_offset, XForwardingCursor* cursor) {\n+  const XForwardingEntry new_entry(from_index, to_offset);\n+  const XForwardingEntry old_entry; \/\/ Empty\n+\n+  \/\/ Make sure that object copy is finished\n+  \/\/ before forwarding table installation\n+  OrderAccess::release();\n+\n+  for (;;) {\n+    const XForwardingEntry prev_entry = Atomic::cmpxchg(entries() + *cursor, old_entry, new_entry, memory_order_relaxed);\n+    if (!prev_entry.populated()) {\n+      \/\/ Success\n+      return to_offset;\n+    }\n+\n+    \/\/ Find next empty or matching entry\n+    XForwardingEntry entry = at(cursor);\n+    while (entry.populated()) {\n+      if (entry.from_index() == from_index) {\n+        \/\/ Match found, return already inserted address\n+        return entry.to_offset();\n+      }\n+\n+      entry = next(cursor);\n+    }\n+  }\n+}\n+\n+#endif \/\/ SHARE_GC_X_XFORWARDING_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xForwarding.inline.hpp","additions":163,"deletions":0,"binary":false,"changes":163,"status":"added"},{"patch":"@@ -0,0 +1,40 @@\n+\/*\n+ * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/x\/xForwardingAllocator.hpp\"\n+#include \"memory\/allocation.inline.hpp\"\n+\n+XForwardingAllocator::XForwardingAllocator() :\n+    _start(NULL),\n+    _end(NULL),\n+    _top(NULL) {}\n+\n+XForwardingAllocator::~XForwardingAllocator() {\n+  FREE_C_HEAP_ARRAY(char, _start);\n+}\n+\n+void XForwardingAllocator::reset(size_t size) {\n+  _start = _top = REALLOC_C_HEAP_ARRAY(char, _start, size, mtGC);\n+  _end = _start + size;\n+}\n","filename":"src\/hotspot\/share\/gc\/x\/xForwardingAllocator.cpp","additions":40,"deletions":0,"binary":false,"changes":40,"status":"added"},{"patch":"@@ -0,0 +1,46 @@\n+\/*\n+ * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XFORWARDINGALLOCATOR_HPP\n+#define SHARE_GC_X_XFORWARDINGALLOCATOR_HPP\n+\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+class XForwardingAllocator {\n+private:\n+  char* _start;\n+  char* _end;\n+  char* _top;\n+\n+public:\n+  XForwardingAllocator();\n+  ~XForwardingAllocator();\n+\n+  void reset(size_t size);\n+  size_t size() const;\n+  bool is_full() const;\n+\n+  void* alloc(size_t size);\n+};\n+\n+#endif \/\/ SHARE_GC_X_XFORWARDINGALLOCATOR_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xForwardingAllocator.hpp","additions":46,"deletions":0,"binary":false,"changes":46,"status":"added"},{"patch":"@@ -0,0 +1,46 @@\n+\/*\n+ * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XFORWARDINGALLOCATOR_INLINE_HPP\n+#define SHARE_GC_X_XFORWARDINGALLOCATOR_INLINE_HPP\n+\n+#include \"gc\/x\/xForwardingAllocator.hpp\"\n+\n+#include \"runtime\/atomic.hpp\"\n+#include \"utilities\/debug.hpp\"\n+\n+inline size_t XForwardingAllocator::size() const {\n+  return _end - _start;\n+}\n+\n+inline bool XForwardingAllocator::is_full() const {\n+  return _top == _end;\n+}\n+\n+inline void* XForwardingAllocator::alloc(size_t size) {\n+  char* const addr = Atomic::fetch_and_add(&_top, size);\n+  assert(addr + size <= _end, \"Allocation should never fail\");\n+  return addr;\n+}\n+\n+#endif \/\/ SHARE_GC_X_XFORWARDINGALLOCATOR_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xForwardingAllocator.inline.hpp","additions":46,"deletions":0,"binary":false,"changes":46,"status":"added"},{"patch":"@@ -0,0 +1,102 @@\n+\/*\n+ * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XFORWARDINGENTRY_HPP\n+#define SHARE_GC_X_XFORWARDINGENTRY_HPP\n+\n+#include \"gc\/x\/xBitField.hpp\"\n+#include \"memory\/allocation.hpp\"\n+#include \"metaprogramming\/primitiveConversions.hpp\"\n+\n+#include <type_traits>\n+\n+class VMStructs;\n+\n+\/\/\n+\/\/ Forwarding entry layout\n+\/\/ -----------------------\n+\/\/\n+\/\/   6                  4 4\n+\/\/   3                  6 5                                                1 0\n+\/\/  +--------------------+--------------------------------------------------+-+\n+\/\/  |11111111 11111111 11|111111 11111111 11111111 11111111 11111111 1111111|1|\n+\/\/  +--------------------+--------------------------------------------------+-+\n+\/\/  |                    |                                                  |\n+\/\/  |                    |                      0-0 Populated Flag (1-bits) *\n+\/\/  |                    |\n+\/\/  |                    * 45-1 To Object Offset (45-bits)\n+\/\/  |\n+\/\/  * 63-46 From Object Index (18-bits)\n+\/\/\n+\n+class XForwardingEntry {\n+  friend struct PrimitiveConversions::Translate<XForwardingEntry>;\n+  friend class ::VMStructs;\n+\n+private:\n+  typedef XBitField<uint64_t, bool,   0,   1> field_populated;\n+  typedef XBitField<uint64_t, size_t, 1,  45> field_to_offset;\n+  typedef XBitField<uint64_t, size_t, 46, 18> field_from_index;\n+\n+  uint64_t _entry;\n+\n+public:\n+  XForwardingEntry() :\n+      _entry(0) {}\n+\n+  XForwardingEntry(size_t from_index, size_t to_offset) :\n+      _entry(field_populated::encode(true) |\n+             field_to_offset::encode(to_offset) |\n+             field_from_index::encode(from_index)) {}\n+\n+  bool populated() const {\n+    return field_populated::decode(_entry);\n+  }\n+\n+  size_t to_offset() const {\n+    return field_to_offset::decode(_entry);\n+  }\n+\n+  size_t from_index() const {\n+    return field_from_index::decode(_entry);\n+  }\n+};\n+\n+\/\/ Needed to allow atomic operations on XForwardingEntry\n+template <>\n+struct PrimitiveConversions::Translate<XForwardingEntry> : public std::true_type {\n+  typedef XForwardingEntry Value;\n+  typedef uint64_t                  Decayed;\n+\n+  static Decayed decay(Value v) {\n+    return v._entry;\n+  }\n+\n+  static Value recover(Decayed d) {\n+    XForwardingEntry entry;\n+    entry._entry = d;\n+    return entry;\n+  }\n+};\n+\n+#endif \/\/ SHARE_GC_X_XFORWARDINGENTRY_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xForwardingEntry.hpp","additions":102,"deletions":0,"binary":false,"changes":102,"status":"added"},{"patch":"@@ -0,0 +1,47 @@\n+\/*\n+ * Copyright (c) 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XFORWARDINGTABLE_HPP\n+#define SHARE_GC_X_XFORWARDINGTABLE_HPP\n+\n+#include \"gc\/x\/xGranuleMap.hpp\"\n+\n+class VMStructs;\n+class XForwarding;\n+\n+class XForwardingTable {\n+  friend class ::VMStructs;\n+\n+private:\n+  XGranuleMap<XForwarding*> _map;\n+\n+public:\n+  XForwardingTable();\n+\n+  XForwarding* get(uintptr_t addr) const;\n+\n+  void insert(XForwarding* forwarding);\n+  void remove(XForwarding* forwarding);\n+};\n+\n+#endif \/\/ SHARE_GC_X_XFORWARDINGTABLE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xForwardingTable.hpp","additions":47,"deletions":0,"binary":false,"changes":47,"status":"added"},{"patch":"@@ -0,0 +1,59 @@\n+\/*\n+ * Copyright (c) 2019, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XFORWARDINGTABLE_INLINE_HPP\n+#define SHARE_GC_X_XFORWARDINGTABLE_INLINE_HPP\n+\n+#include \"gc\/x\/xForwardingTable.hpp\"\n+\n+#include \"gc\/x\/xAddress.inline.hpp\"\n+#include \"gc\/x\/xForwarding.inline.hpp\"\n+#include \"gc\/x\/xGlobals.hpp\"\n+#include \"gc\/x\/xGranuleMap.inline.hpp\"\n+#include \"utilities\/debug.hpp\"\n+\n+inline XForwardingTable::XForwardingTable() :\n+    _map(XAddressOffsetMax) {}\n+\n+inline XForwarding* XForwardingTable::get(uintptr_t addr) const {\n+  assert(!XAddress::is_null(addr), \"Invalid address\");\n+  return _map.get(XAddress::offset(addr));\n+}\n+\n+inline void XForwardingTable::insert(XForwarding* forwarding) {\n+  const uintptr_t offset = forwarding->start();\n+  const size_t size = forwarding->size();\n+\n+  assert(_map.get(offset) == NULL, \"Invalid entry\");\n+  _map.put(offset, size, forwarding);\n+}\n+\n+inline void XForwardingTable::remove(XForwarding* forwarding) {\n+  const uintptr_t offset = forwarding->start();\n+  const size_t size = forwarding->size();\n+\n+  assert(_map.get(offset) == forwarding, \"Invalid entry\");\n+  _map.put(offset, size, NULL);\n+}\n+\n+#endif \/\/ SHARE_GC_X_XFORWARDINGTABLE_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xForwardingTable.inline.hpp","additions":59,"deletions":0,"binary":false,"changes":59,"status":"added"},{"patch":"@@ -0,0 +1,43 @@\n+\/*\n+ * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XFUTURE_HPP\n+#define SHARE_GC_X_XFUTURE_HPP\n+\n+#include \"memory\/allocation.hpp\"\n+#include \"runtime\/semaphore.hpp\"\n+\n+template <typename T>\n+class XFuture {\n+private:\n+  Semaphore _sema;\n+  T         _value;\n+\n+public:\n+  XFuture();\n+\n+  void set(T value);\n+  T get();\n+};\n+\n+#endif \/\/ SHARE_GC_X_XFUTURE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xFuture.hpp","additions":43,"deletions":0,"binary":false,"changes":43,"status":"added"},{"patch":"@@ -0,0 +1,59 @@\n+\/*\n+ * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XFUTURE_INLINE_HPP\n+#define SHARE_GC_X_XFUTURE_INLINE_HPP\n+\n+#include \"gc\/x\/xFuture.hpp\"\n+\n+#include \"runtime\/javaThread.hpp\"\n+#include \"runtime\/semaphore.inline.hpp\"\n+\n+template <typename T>\n+inline XFuture<T>::XFuture() :\n+    _value() {}\n+\n+template <typename T>\n+inline void XFuture<T>::set(T value) {\n+  \/\/ Set value\n+  _value = value;\n+\n+  \/\/ Notify waiter\n+  _sema.signal();\n+}\n+\n+template <typename T>\n+inline T XFuture<T>::get() {\n+  \/\/ Wait for notification\n+  Thread* const thread = Thread::current();\n+  if (thread->is_Java_thread()) {\n+    _sema.wait_with_safepoint_check(JavaThread::cast(thread));\n+  } else {\n+    _sema.wait();\n+  }\n+\n+  \/\/ Return value\n+  return _value;\n+}\n+\n+#endif \/\/ SHARE_GC_X_XFUTURE_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xFuture.inline.hpp","additions":59,"deletions":0,"binary":false,"changes":59,"status":"added"},{"patch":"@@ -0,0 +1,79 @@\n+\/*\n+ * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/x\/xGlobals.hpp\"\n+\n+uint32_t   XGlobalPhase                = XPhaseRelocate;\n+uint32_t   XGlobalSeqNum               = 1;\n+\n+size_t     XPageSizeMediumShift;\n+size_t     XPageSizeMedium;\n+\n+size_t     XObjectSizeLimitMedium;\n+\n+const int& XObjectAlignmentSmallShift  = LogMinObjAlignmentInBytes;\n+int        XObjectAlignmentMediumShift;\n+\n+const int& XObjectAlignmentSmall       = MinObjAlignmentInBytes;\n+int        XObjectAlignmentMedium;\n+\n+uintptr_t  XAddressGoodMask;\n+uintptr_t  XAddressBadMask;\n+uintptr_t  XAddressWeakBadMask;\n+\n+static uint32_t* XAddressCalculateBadMaskHighOrderBitsAddr() {\n+  const uintptr_t addr = reinterpret_cast<uintptr_t>(&XAddressBadMask);\n+  return reinterpret_cast<uint32_t*>(addr + XAddressBadMaskHighOrderBitsOffset);\n+}\n+\n+uint32_t*  XAddressBadMaskHighOrderBitsAddr = XAddressCalculateBadMaskHighOrderBitsAddr();\n+\n+size_t     XAddressOffsetBits;\n+uintptr_t  XAddressOffsetMask;\n+size_t     XAddressOffsetMax;\n+\n+size_t     XAddressMetadataShift;\n+uintptr_t  XAddressMetadataMask;\n+\n+uintptr_t  XAddressMetadataMarked;\n+uintptr_t  XAddressMetadataMarked0;\n+uintptr_t  XAddressMetadataMarked1;\n+uintptr_t  XAddressMetadataRemapped;\n+uintptr_t  XAddressMetadataFinalizable;\n+\n+const char* XGlobalPhaseToString() {\n+  switch (XGlobalPhase) {\n+  case XPhaseMark:\n+    return \"Mark\";\n+\n+  case XPhaseMarkCompleted:\n+    return \"MarkCompleted\";\n+\n+  case XPhaseRelocate:\n+    return \"Relocate\";\n+\n+  default:\n+    return \"Unknown\";\n+  }\n+}\n","filename":"src\/hotspot\/share\/gc\/x\/xGlobals.cpp","additions":79,"deletions":0,"binary":false,"changes":79,"status":"added"},{"patch":"@@ -0,0 +1,158 @@\n+\/*\n+ * Copyright (c) 2015, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XGLOBALS_HPP\n+#define SHARE_GC_X_XGLOBALS_HPP\n+\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include \"utilities\/macros.hpp\"\n+#include CPU_HEADER(gc\/x\/xGlobals)\n+\n+\/\/ Collector name\n+const char* const XName                         = \"The Z Garbage Collector\";\n+\n+\/\/ Global phase state\n+extern uint32_t   XGlobalPhase;\n+const uint32_t    XPhaseMark                    = 0;\n+const uint32_t    XPhaseMarkCompleted           = 1;\n+const uint32_t    XPhaseRelocate                = 2;\n+const char*       XGlobalPhaseToString();\n+\n+\/\/ Global sequence number\n+extern uint32_t   XGlobalSeqNum;\n+\n+\/\/ Granule shift\/size\n+const size_t      XGranuleSizeShift             = 21; \/\/ 2MB\n+const size_t      XGranuleSize                  = (size_t)1 << XGranuleSizeShift;\n+\n+\/\/ Number of heap views\n+const size_t      XHeapViews                    = XPlatformHeapViews;\n+\n+\/\/ Virtual memory to physical memory ratio\n+const size_t      XVirtualToPhysicalRatio       = 16; \/\/ 16:1\n+\n+\/\/ Page types\n+const uint8_t     XPageTypeSmall                = 0;\n+const uint8_t     XPageTypeMedium               = 1;\n+const uint8_t     XPageTypeLarge                = 2;\n+\n+\/\/ Page size shifts\n+const size_t      XPageSizeSmallShift           = XGranuleSizeShift;\n+extern size_t     XPageSizeMediumShift;\n+\n+\/\/ Page sizes\n+const size_t      XPageSizeSmall                = (size_t)1 << XPageSizeSmallShift;\n+extern size_t     XPageSizeMedium;\n+\n+\/\/ Object size limits\n+const size_t      XObjectSizeLimitSmall         = XPageSizeSmall \/ 8; \/\/ 12.5% max waste\n+extern size_t     XObjectSizeLimitMedium;\n+\n+\/\/ Object alignment shifts\n+extern const int& XObjectAlignmentSmallShift;\n+extern int        XObjectAlignmentMediumShift;\n+const int         XObjectAlignmentLargeShift    = XGranuleSizeShift;\n+\n+\/\/ Object alignments\n+extern const int& XObjectAlignmentSmall;\n+extern int        XObjectAlignmentMedium;\n+const int         XObjectAlignmentLarge         = 1 << XObjectAlignmentLargeShift;\n+\n+\/\/\n+\/\/ Good\/Bad mask states\n+\/\/ --------------------\n+\/\/\n+\/\/                 GoodMask         BadMask          WeakGoodMask     WeakBadMask\n+\/\/                 --------------------------------------------------------------\n+\/\/  Marked0        001              110              101              010\n+\/\/  Marked1        010              101              110              001\n+\/\/  Remapped       100              011              100              011\n+\/\/\n+\n+\/\/ Good\/bad masks\n+extern uintptr_t  XAddressGoodMask;\n+extern uintptr_t  XAddressBadMask;\n+extern uintptr_t  XAddressWeakBadMask;\n+\n+\/\/ The bad mask is 64 bit. Its high order 32 bits contain all possible value combinations\n+\/\/ that this mask will have. Therefore, the memory where the 32 high order bits are stored,\n+\/\/ can be used as a 32 bit GC epoch counter, that has a different bit pattern every time\n+\/\/ the bad mask is flipped. This provides a pointer to said 32 bits.\n+extern uint32_t*  XAddressBadMaskHighOrderBitsAddr;\n+const int         XAddressBadMaskHighOrderBitsOffset = LITTLE_ENDIAN_ONLY(4) BIG_ENDIAN_ONLY(0);\n+\n+\/\/ Pointer part of address\n+extern size_t     XAddressOffsetBits;\n+const  size_t     XAddressOffsetShift           = 0;\n+extern uintptr_t  XAddressOffsetMask;\n+extern size_t     XAddressOffsetMax;\n+\n+\/\/ Metadata part of address\n+const size_t      XAddressMetadataBits          = 4;\n+extern size_t     XAddressMetadataShift;\n+extern uintptr_t  XAddressMetadataMask;\n+\n+\/\/ Metadata types\n+extern uintptr_t  XAddressMetadataMarked;\n+extern uintptr_t  XAddressMetadataMarked0;\n+extern uintptr_t  XAddressMetadataMarked1;\n+extern uintptr_t  XAddressMetadataRemapped;\n+extern uintptr_t  XAddressMetadataFinalizable;\n+\n+\/\/ Cache line size\n+const size_t      XCacheLineSize                = XPlatformCacheLineSize;\n+#define           XCACHE_ALIGNED                ATTRIBUTE_ALIGNED(XCacheLineSize)\n+\n+\/\/ Mark stack space\n+extern uintptr_t  XMarkStackSpaceStart;\n+const size_t      XMarkStackSpaceExpandSize     = (size_t)1 << 25; \/\/ 32M\n+\n+\/\/ Mark stack and magazine sizes\n+const size_t      XMarkStackSizeShift           = 11; \/\/ 2K\n+const size_t      XMarkStackSize                = (size_t)1 << XMarkStackSizeShift;\n+const size_t      XMarkStackHeaderSize          = (size_t)1 << 4; \/\/ 16B\n+const size_t      XMarkStackSlots               = (XMarkStackSize - XMarkStackHeaderSize) \/ sizeof(uintptr_t);\n+const size_t      XMarkStackMagazineSize        = (size_t)1 << 15; \/\/ 32K\n+const size_t      XMarkStackMagazineSlots       = (XMarkStackMagazineSize \/ XMarkStackSize) - 1;\n+\n+\/\/ Mark stripe size\n+const size_t      XMarkStripeShift              = XGranuleSizeShift;\n+\n+\/\/ Max number of mark stripes\n+const size_t      XMarkStripesMax               = 16; \/\/ Must be a power of two\n+\n+\/\/ Mark cache size\n+const size_t      XMarkCacheSize                = 1024; \/\/ Must be a power of two\n+\n+\/\/ Partial array minimum size\n+const size_t      XMarkPartialArrayMinSizeShift = 12; \/\/ 4K\n+const size_t      XMarkPartialArrayMinSize      = (size_t)1 << XMarkPartialArrayMinSizeShift;\n+\n+\/\/ Max number of proactive\/terminate flush attempts\n+const size_t      XMarkProactiveFlushMax        = 10;\n+const size_t      XMarkTerminateFlushMax        = 3;\n+\n+\/\/ Try complete mark timeout\n+const uint64_t    XMarkCompleteTimeout          = 200; \/\/ us\n+\n+#endif \/\/ SHARE_GC_X_XGLOBALS_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xGlobals.hpp","additions":158,"deletions":0,"binary":false,"changes":158,"status":"added"},{"patch":"@@ -0,0 +1,61 @@\n+\/*\n+ * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XGRANULEMAP_HPP\n+#define SHARE_GC_X_XGRANULEMAP_HPP\n+\n+#include \"gc\/x\/xArray.hpp\"\n+#include \"memory\/allocation.hpp\"\n+\n+class VMStructs;\n+\n+template <typename T>\n+class XGranuleMap {\n+  friend class ::VMStructs;\n+  template <typename> friend class XGranuleMapIterator;\n+\n+private:\n+  const size_t _size;\n+  T* const     _map;\n+\n+  size_t index_for_offset(uintptr_t offset) const;\n+\n+public:\n+  XGranuleMap(size_t max_offset);\n+  ~XGranuleMap();\n+\n+  T get(uintptr_t offset) const;\n+  void put(uintptr_t offset, T value);\n+  void put(uintptr_t offset, size_t size, T value);\n+\n+  T get_acquire(uintptr_t offset) const;\n+  void release_put(uintptr_t offset, T value);\n+};\n+\n+template <typename T>\n+class XGranuleMapIterator : public XArrayIteratorImpl<T, false \/* Parallel *\/> {\n+public:\n+  XGranuleMapIterator(const XGranuleMap<T>* granule_map);\n+};\n+\n+#endif \/\/ SHARE_GC_X_XGRANULEMAP_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xGranuleMap.hpp","additions":61,"deletions":0,"binary":false,"changes":61,"status":"added"},{"patch":"@@ -0,0 +1,94 @@\n+\/*\n+ * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XGRANULEMAP_INLINE_HPP\n+#define SHARE_GC_X_XGRANULEMAP_INLINE_HPP\n+\n+#include \"gc\/x\/xGranuleMap.hpp\"\n+\n+#include \"gc\/x\/xArray.inline.hpp\"\n+#include \"gc\/x\/xGlobals.hpp\"\n+#include \"memory\/allocation.inline.hpp\"\n+#include \"runtime\/atomic.hpp\"\n+#include \"utilities\/align.hpp\"\n+#include \"utilities\/debug.hpp\"\n+\n+template <typename T>\n+inline XGranuleMap<T>::XGranuleMap(size_t max_offset) :\n+    _size(max_offset >> XGranuleSizeShift),\n+    _map(MmapArrayAllocator<T>::allocate(_size, mtGC)) {\n+  assert(is_aligned(max_offset, XGranuleSize), \"Misaligned\");\n+}\n+\n+template <typename T>\n+inline XGranuleMap<T>::~XGranuleMap() {\n+  MmapArrayAllocator<T>::free(_map, _size);\n+}\n+\n+template <typename T>\n+inline size_t XGranuleMap<T>::index_for_offset(uintptr_t offset) const {\n+  const size_t index = offset >> XGranuleSizeShift;\n+  assert(index < _size, \"Invalid index\");\n+  return index;\n+}\n+\n+template <typename T>\n+inline T XGranuleMap<T>::get(uintptr_t offset) const {\n+  const size_t index = index_for_offset(offset);\n+  return _map[index];\n+}\n+\n+template <typename T>\n+inline void XGranuleMap<T>::put(uintptr_t offset, T value) {\n+  const size_t index = index_for_offset(offset);\n+  _map[index] = value;\n+}\n+\n+template <typename T>\n+inline void XGranuleMap<T>::put(uintptr_t offset, size_t size, T value) {\n+  assert(is_aligned(size, XGranuleSize), \"Misaligned\");\n+\n+  const size_t start_index = index_for_offset(offset);\n+  const size_t end_index = start_index + (size >> XGranuleSizeShift);\n+  for (size_t index = start_index; index < end_index; index++) {\n+    _map[index] = value;\n+  }\n+}\n+\n+template <typename T>\n+inline T XGranuleMap<T>::get_acquire(uintptr_t offset) const {\n+  const size_t index = index_for_offset(offset);\n+  return Atomic::load_acquire(_map + index);\n+}\n+\n+template <typename T>\n+inline void XGranuleMap<T>::release_put(uintptr_t offset, T value) {\n+  const size_t index = index_for_offset(offset);\n+  Atomic::release_store(_map + index, value);\n+}\n+\n+template <typename T>\n+inline XGranuleMapIterator<T>::XGranuleMapIterator(const XGranuleMap<T>* granule_map) :\n+    XArrayIteratorImpl<T, false \/* Parallel *\/>(granule_map->_map, granule_map->_size) {}\n+\n+#endif \/\/ SHARE_GC_X_XGRANULEMAP_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xGranuleMap.inline.hpp","additions":94,"deletions":0,"binary":false,"changes":94,"status":"added"},{"patch":"@@ -24,2 +24,2 @@\n-#ifndef SHARE_GC_Z_ZHASH_HPP\n-#define SHARE_GC_Z_ZHASH_HPP\n+#ifndef SHARE_GC_X_XHASH_HPP\n+#define SHARE_GC_X_XHASH_HPP\n@@ -30,1 +30,1 @@\n-class ZHash : public AllStatic {\n+class XHash : public AllStatic {\n@@ -36,1 +36,1 @@\n-#endif \/\/ SHARE_GC_Z_ZHASH_HPP\n+#endif \/\/ SHARE_GC_X_XHASH_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xHash.hpp","additions":4,"deletions":4,"binary":false,"changes":8,"previous_filename":"src\/hotspot\/share\/gc\/z\/zHash.hpp","status":"copied"},{"patch":"@@ -56,2 +56,2 @@\n-#ifndef SHARE_GC_Z_ZHASH_INLINE_HPP\n-#define SHARE_GC_Z_ZHASH_INLINE_HPP\n+#ifndef SHARE_GC_X_XHASH_INLINE_HPP\n+#define SHARE_GC_X_XHASH_INLINE_HPP\n@@ -59,1 +59,1 @@\n-#include \"gc\/z\/zHash.hpp\"\n+#include \"gc\/x\/xHash.hpp\"\n@@ -61,1 +61,1 @@\n-#include \"gc\/z\/zAddress.inline.hpp\"\n+#include \"gc\/x\/xAddress.inline.hpp\"\n@@ -63,1 +63,1 @@\n-inline uint32_t ZHash::uint32_to_uint32(uint32_t key) {\n+inline uint32_t XHash::uint32_to_uint32(uint32_t key) {\n@@ -73,1 +73,1 @@\n-inline uint32_t ZHash::address_to_uint32(uintptr_t key) {\n+inline uint32_t XHash::address_to_uint32(uintptr_t key) {\n@@ -77,1 +77,1 @@\n-#endif \/\/ SHARE_GC_Z_ZHASH_INLINE_HPP\n+#endif \/\/ SHARE_GC_X_XHASH_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xHash.inline.hpp","additions":7,"deletions":7,"binary":false,"changes":14,"previous_filename":"src\/hotspot\/share\/gc\/z\/zHash.inline.hpp","status":"copied"},{"patch":"@@ -0,0 +1,536 @@\n+\/*\n+ * Copyright (c) 2015, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"classfile\/classLoaderDataGraph.hpp\"\n+#include \"gc\/shared\/gc_globals.hpp\"\n+#include \"gc\/shared\/locationPrinter.hpp\"\n+#include \"gc\/shared\/tlab_globals.hpp\"\n+#include \"gc\/x\/xAddress.inline.hpp\"\n+#include \"gc\/x\/xArray.inline.hpp\"\n+#include \"gc\/x\/xGlobals.hpp\"\n+#include \"gc\/x\/xHeap.inline.hpp\"\n+#include \"gc\/x\/xHeapIterator.hpp\"\n+#include \"gc\/x\/xHeuristics.hpp\"\n+#include \"gc\/x\/xMark.inline.hpp\"\n+#include \"gc\/x\/xPage.inline.hpp\"\n+#include \"gc\/x\/xPageTable.inline.hpp\"\n+#include \"gc\/x\/xRelocationSet.inline.hpp\"\n+#include \"gc\/x\/xRelocationSetSelector.inline.hpp\"\n+#include \"gc\/x\/xResurrection.hpp\"\n+#include \"gc\/x\/xStat.hpp\"\n+#include \"gc\/x\/xThread.inline.hpp\"\n+#include \"gc\/x\/xVerify.hpp\"\n+#include \"gc\/x\/xWorkers.hpp\"\n+#include \"logging\/log.hpp\"\n+#include \"memory\/iterator.hpp\"\n+#include \"memory\/metaspaceUtils.hpp\"\n+#include \"memory\/resourceArea.hpp\"\n+#include \"prims\/jvmtiTagMap.hpp\"\n+#include \"runtime\/handshake.hpp\"\n+#include \"runtime\/javaThread.hpp\"\n+#include \"runtime\/safepoint.hpp\"\n+#include \"utilities\/debug.hpp\"\n+\n+static const XStatCounter XCounterUndoPageAllocation(\"Memory\", \"Undo Page Allocation\", XStatUnitOpsPerSecond);\n+static const XStatCounter XCounterOutOfMemory(\"Memory\", \"Out Of Memory\", XStatUnitOpsPerSecond);\n+\n+XHeap* XHeap::_heap = NULL;\n+\n+XHeap::XHeap() :\n+    _workers(),\n+    _object_allocator(),\n+    _page_allocator(&_workers, MinHeapSize, InitialHeapSize, MaxHeapSize),\n+    _page_table(),\n+    _forwarding_table(),\n+    _mark(&_workers, &_page_table),\n+    _reference_processor(&_workers),\n+    _weak_roots_processor(&_workers),\n+    _relocate(&_workers),\n+    _relocation_set(&_workers),\n+    _unload(&_workers),\n+    _serviceability(min_capacity(), max_capacity()) {\n+  \/\/ Install global heap instance\n+  assert(_heap == NULL, \"Already initialized\");\n+  _heap = this;\n+\n+  \/\/ Update statistics\n+  XStatHeap::set_at_initialize(_page_allocator.stats());\n+}\n+\n+bool XHeap::is_initialized() const {\n+  return _page_allocator.is_initialized() && _mark.is_initialized();\n+}\n+\n+size_t XHeap::min_capacity() const {\n+  return _page_allocator.min_capacity();\n+}\n+\n+size_t XHeap::max_capacity() const {\n+  return _page_allocator.max_capacity();\n+}\n+\n+size_t XHeap::soft_max_capacity() const {\n+  return _page_allocator.soft_max_capacity();\n+}\n+\n+size_t XHeap::capacity() const {\n+  return _page_allocator.capacity();\n+}\n+\n+size_t XHeap::used() const {\n+  return _page_allocator.used();\n+}\n+\n+size_t XHeap::unused() const {\n+  return _page_allocator.unused();\n+}\n+\n+size_t XHeap::tlab_capacity() const {\n+  return capacity();\n+}\n+\n+size_t XHeap::tlab_used() const {\n+  return _object_allocator.used();\n+}\n+\n+size_t XHeap::max_tlab_size() const {\n+  return XObjectSizeLimitSmall;\n+}\n+\n+size_t XHeap::unsafe_max_tlab_alloc() const {\n+  size_t size = _object_allocator.remaining();\n+\n+  if (size < MinTLABSize) {\n+    \/\/ The remaining space in the allocator is not enough to\n+    \/\/ fit the smallest possible TLAB. This means that the next\n+    \/\/ TLAB allocation will force the allocator to get a new\n+    \/\/ backing page anyway, which in turn means that we can then\n+    \/\/ fit the largest possible TLAB.\n+    size = max_tlab_size();\n+  }\n+\n+  return MIN2(size, max_tlab_size());\n+}\n+\n+bool XHeap::is_in(uintptr_t addr) const {\n+  \/\/ An address is considered to be \"in the heap\" if it points into\n+  \/\/ the allocated part of a page, regardless of which heap view is\n+  \/\/ used. Note that an address with the finalizable metadata bit set\n+  \/\/ is not pointing into a heap view, and therefore not considered\n+  \/\/ to be \"in the heap\".\n+\n+  if (XAddress::is_in(addr)) {\n+    const XPage* const page = _page_table.get(addr);\n+    if (page != NULL) {\n+      return page->is_in(addr);\n+    }\n+  }\n+\n+  return false;\n+}\n+\n+uint XHeap::active_workers() const {\n+  return _workers.active_workers();\n+}\n+\n+void XHeap::set_active_workers(uint nworkers) {\n+  _workers.set_active_workers(nworkers);\n+}\n+\n+void XHeap::threads_do(ThreadClosure* tc) const {\n+  _page_allocator.threads_do(tc);\n+  _workers.threads_do(tc);\n+}\n+\n+void XHeap::out_of_memory() {\n+  ResourceMark rm;\n+\n+  XStatInc(XCounterOutOfMemory);\n+  log_info(gc)(\"Out Of Memory (%s)\", Thread::current()->name());\n+}\n+\n+XPage* XHeap::alloc_page(uint8_t type, size_t size, XAllocationFlags flags) {\n+  XPage* const page = _page_allocator.alloc_page(type, size, flags);\n+  if (page != NULL) {\n+    \/\/ Insert page table entry\n+    _page_table.insert(page);\n+  }\n+\n+  return page;\n+}\n+\n+void XHeap::undo_alloc_page(XPage* page) {\n+  assert(page->is_allocating(), \"Invalid page state\");\n+\n+  XStatInc(XCounterUndoPageAllocation);\n+  log_trace(gc)(\"Undo page allocation, thread: \" PTR_FORMAT \" (%s), page: \" PTR_FORMAT \", size: \" SIZE_FORMAT,\n+                XThread::id(), XThread::name(), p2i(page), page->size());\n+\n+  free_page(page, false \/* reclaimed *\/);\n+}\n+\n+void XHeap::free_page(XPage* page, bool reclaimed) {\n+  \/\/ Remove page table entry\n+  _page_table.remove(page);\n+\n+  \/\/ Free page\n+  _page_allocator.free_page(page, reclaimed);\n+}\n+\n+void XHeap::free_pages(const XArray<XPage*>* pages, bool reclaimed) {\n+  \/\/ Remove page table entries\n+  XArrayIterator<XPage*> iter(pages);\n+  for (XPage* page; iter.next(&page);) {\n+    _page_table.remove(page);\n+  }\n+\n+  \/\/ Free pages\n+  _page_allocator.free_pages(pages, reclaimed);\n+}\n+\n+void XHeap::flip_to_marked() {\n+  XVerifyViewsFlip flip(&_page_allocator);\n+  XAddress::flip_to_marked();\n+}\n+\n+void XHeap::flip_to_remapped() {\n+  XVerifyViewsFlip flip(&_page_allocator);\n+  XAddress::flip_to_remapped();\n+}\n+\n+void XHeap::mark_start() {\n+  assert(SafepointSynchronize::is_at_safepoint(), \"Should be at safepoint\");\n+\n+  \/\/ Verification\n+  ClassLoaderDataGraph::verify_claimed_marks_cleared(ClassLoaderData::_claim_strong);\n+\n+  if (XHeap::heap()->has_alloc_stalled()) {\n+    \/\/ If there are stalled allocations, ensure that regardless of the\n+    \/\/ cause of the GC, we have to clear soft references, as we are just\n+    \/\/ about to increment the sequence number, and all previous allocations\n+    \/\/ will throw if not presented with enough memory.\n+    XHeap::heap()->set_soft_reference_policy(true);\n+  }\n+\n+  \/\/ Flip address view\n+  flip_to_marked();\n+\n+  \/\/ Retire allocating pages\n+  _object_allocator.retire_pages();\n+\n+  \/\/ Reset allocated\/reclaimed\/used statistics\n+  _page_allocator.reset_statistics();\n+\n+  \/\/ Reset encountered\/dropped\/enqueued statistics\n+  _reference_processor.reset_statistics();\n+\n+  \/\/ Enter mark phase\n+  XGlobalPhase = XPhaseMark;\n+\n+  \/\/ Reset marking information and mark roots\n+  _mark.start();\n+\n+  \/\/ Update statistics\n+  XStatHeap::set_at_mark_start(_page_allocator.stats());\n+}\n+\n+void XHeap::mark(bool initial) {\n+  _mark.mark(initial);\n+}\n+\n+void XHeap::mark_flush_and_free(Thread* thread) {\n+  _mark.flush_and_free(thread);\n+}\n+\n+bool XHeap::mark_end() {\n+  assert(SafepointSynchronize::is_at_safepoint(), \"Should be at safepoint\");\n+\n+  \/\/ Try end marking\n+  if (!_mark.end()) {\n+    \/\/ Marking not completed, continue concurrent mark\n+    return false;\n+  }\n+\n+  \/\/ Enter mark completed phase\n+  XGlobalPhase = XPhaseMarkCompleted;\n+\n+  \/\/ Verify after mark\n+  XVerify::after_mark();\n+\n+  \/\/ Update statistics\n+  XStatHeap::set_at_mark_end(_page_allocator.stats());\n+\n+  \/\/ Block resurrection of weak\/phantom references\n+  XResurrection::block();\n+\n+  \/\/ Prepare to unload stale metadata and nmethods\n+  _unload.prepare();\n+\n+  \/\/ Notify JVMTI that some tagmap entry objects may have died.\n+  JvmtiTagMap::set_needs_cleaning();\n+\n+  return true;\n+}\n+\n+void XHeap::mark_free() {\n+  _mark.free();\n+}\n+\n+void XHeap::keep_alive(oop obj) {\n+  XBarrier::keep_alive_barrier_on_oop(obj);\n+}\n+\n+void XHeap::set_soft_reference_policy(bool clear) {\n+  _reference_processor.set_soft_reference_policy(clear);\n+}\n+\n+class XRendezvousClosure : public HandshakeClosure {\n+public:\n+  XRendezvousClosure() :\n+      HandshakeClosure(\"XRendezvous\") {}\n+\n+  void do_thread(Thread* thread) {}\n+};\n+\n+void XHeap::process_non_strong_references() {\n+  \/\/ Process Soft\/Weak\/Final\/PhantomReferences\n+  _reference_processor.process_references();\n+\n+  \/\/ Process weak roots\n+  _weak_roots_processor.process_weak_roots();\n+\n+  \/\/ Unlink stale metadata and nmethods\n+  _unload.unlink();\n+\n+  \/\/ Perform a handshake. This is needed 1) to make sure that stale\n+  \/\/ metadata and nmethods are no longer observable. And 2), to\n+  \/\/ prevent the race where a mutator first loads an oop, which is\n+  \/\/ logically null but not yet cleared. Then this oop gets cleared\n+  \/\/ by the reference processor and resurrection is unblocked. At\n+  \/\/ this point the mutator could see the unblocked state and pass\n+  \/\/ this invalid oop through the normal barrier path, which would\n+  \/\/ incorrectly try to mark the oop.\n+  XRendezvousClosure cl;\n+  Handshake::execute(&cl);\n+\n+  \/\/ Unblock resurrection of weak\/phantom references\n+  XResurrection::unblock();\n+\n+  \/\/ Purge stale metadata and nmethods that were unlinked\n+  _unload.purge();\n+\n+  \/\/ Enqueue Soft\/Weak\/Final\/PhantomReferences. Note that this\n+  \/\/ must be done after unblocking resurrection. Otherwise the\n+  \/\/ Finalizer thread could call Reference.get() on the Finalizers\n+  \/\/ that were just enqueued, which would incorrectly return null\n+  \/\/ during the resurrection block window, since such referents\n+  \/\/ are only Finalizable marked.\n+  _reference_processor.enqueue_references();\n+\n+  \/\/ Clear old markings claim bits.\n+  \/\/ Note: Clearing _claim_strong also clears _claim_finalizable.\n+  ClassLoaderDataGraph::clear_claimed_marks(ClassLoaderData::_claim_strong);\n+}\n+\n+void XHeap::free_empty_pages(XRelocationSetSelector* selector, int bulk) {\n+  \/\/ Freeing empty pages in bulk is an optimization to avoid grabbing\n+  \/\/ the page allocator lock, and trying to satisfy stalled allocations\n+  \/\/ too frequently.\n+  if (selector->should_free_empty_pages(bulk)) {\n+    free_pages(selector->empty_pages(), true \/* reclaimed *\/);\n+    selector->clear_empty_pages();\n+  }\n+}\n+\n+void XHeap::select_relocation_set() {\n+  \/\/ Do not allow pages to be deleted\n+  _page_allocator.enable_deferred_delete();\n+\n+  \/\/ Register relocatable pages with selector\n+  XRelocationSetSelector selector;\n+  XPageTableIterator pt_iter(&_page_table);\n+  for (XPage* page; pt_iter.next(&page);) {\n+    if (!page->is_relocatable()) {\n+      \/\/ Not relocatable, don't register\n+      continue;\n+    }\n+\n+    if (page->is_marked()) {\n+      \/\/ Register live page\n+      selector.register_live_page(page);\n+    } else {\n+      \/\/ Register empty page\n+      selector.register_empty_page(page);\n+\n+      \/\/ Reclaim empty pages in bulk\n+      free_empty_pages(&selector, 64 \/* bulk *\/);\n+    }\n+  }\n+\n+  \/\/ Reclaim remaining empty pages\n+  free_empty_pages(&selector, 0 \/* bulk *\/);\n+\n+  \/\/ Allow pages to be deleted\n+  _page_allocator.disable_deferred_delete();\n+\n+  \/\/ Select relocation set\n+  selector.select();\n+\n+  \/\/ Install relocation set\n+  _relocation_set.install(&selector);\n+\n+  \/\/ Setup forwarding table\n+  XRelocationSetIterator rs_iter(&_relocation_set);\n+  for (XForwarding* forwarding; rs_iter.next(&forwarding);) {\n+    _forwarding_table.insert(forwarding);\n+  }\n+\n+  \/\/ Update statistics\n+  XStatRelocation::set_at_select_relocation_set(selector.stats());\n+  XStatHeap::set_at_select_relocation_set(selector.stats());\n+}\n+\n+void XHeap::reset_relocation_set() {\n+  \/\/ Reset forwarding table\n+  XRelocationSetIterator iter(&_relocation_set);\n+  for (XForwarding* forwarding; iter.next(&forwarding);) {\n+    _forwarding_table.remove(forwarding);\n+  }\n+\n+  \/\/ Reset relocation set\n+  _relocation_set.reset();\n+}\n+\n+void XHeap::relocate_start() {\n+  assert(SafepointSynchronize::is_at_safepoint(), \"Should be at safepoint\");\n+\n+  \/\/ Finish unloading stale metadata and nmethods\n+  _unload.finish();\n+\n+  \/\/ Flip address view\n+  flip_to_remapped();\n+\n+  \/\/ Enter relocate phase\n+  XGlobalPhase = XPhaseRelocate;\n+\n+  \/\/ Update statistics\n+  XStatHeap::set_at_relocate_start(_page_allocator.stats());\n+}\n+\n+void XHeap::relocate() {\n+  \/\/ Relocate relocation set\n+  _relocate.relocate(&_relocation_set);\n+\n+  \/\/ Update statistics\n+  XStatHeap::set_at_relocate_end(_page_allocator.stats(), _object_allocator.relocated());\n+}\n+\n+bool XHeap::is_allocating(uintptr_t addr) const {\n+  const XPage* const page = _page_table.get(addr);\n+  return page->is_allocating();\n+}\n+\n+void XHeap::object_iterate(ObjectClosure* cl, bool visit_weaks) {\n+  assert(SafepointSynchronize::is_at_safepoint(), \"Should be at safepoint\");\n+  XHeapIterator iter(1 \/* nworkers *\/, visit_weaks);\n+  iter.object_iterate(cl, 0 \/* worker_id *\/);\n+}\n+\n+ParallelObjectIteratorImpl* XHeap::parallel_object_iterator(uint nworkers, bool visit_weaks) {\n+  assert(SafepointSynchronize::is_at_safepoint(), \"Should be at safepoint\");\n+  return new XHeapIterator(nworkers, visit_weaks);\n+}\n+\n+void XHeap::pages_do(XPageClosure* cl) {\n+  XPageTableIterator iter(&_page_table);\n+  for (XPage* page; iter.next(&page);) {\n+    cl->do_page(page);\n+  }\n+  _page_allocator.pages_do(cl);\n+}\n+\n+void XHeap::serviceability_initialize() {\n+  _serviceability.initialize();\n+}\n+\n+GCMemoryManager* XHeap::serviceability_cycle_memory_manager() {\n+  return _serviceability.cycle_memory_manager();\n+}\n+\n+GCMemoryManager* XHeap::serviceability_pause_memory_manager() {\n+  return _serviceability.pause_memory_manager();\n+}\n+\n+MemoryPool* XHeap::serviceability_memory_pool() {\n+  return _serviceability.memory_pool();\n+}\n+\n+XServiceabilityCounters* XHeap::serviceability_counters() {\n+  return _serviceability.counters();\n+}\n+\n+void XHeap::print_on(outputStream* st) const {\n+  st->print_cr(\" ZHeap           used \" SIZE_FORMAT \"M, capacity \" SIZE_FORMAT \"M, max capacity \" SIZE_FORMAT \"M\",\n+               used() \/ M,\n+               capacity() \/ M,\n+               max_capacity() \/ M);\n+  MetaspaceUtils::print_on(st);\n+}\n+\n+void XHeap::print_extended_on(outputStream* st) const {\n+  print_on(st);\n+  st->cr();\n+\n+  \/\/ Do not allow pages to be deleted\n+  _page_allocator.enable_deferred_delete();\n+\n+  \/\/ Print all pages\n+  st->print_cr(\"ZGC Page Table:\");\n+  XPageTableIterator iter(&_page_table);\n+  for (XPage* page; iter.next(&page);) {\n+    page->print_on(st);\n+  }\n+\n+  \/\/ Allow pages to be deleted\n+  _page_allocator.disable_deferred_delete();\n+}\n+\n+bool XHeap::print_location(outputStream* st, uintptr_t addr) const {\n+  if (LocationPrinter::is_valid_obj((void*)addr)) {\n+    st->print(PTR_FORMAT \" is a %s oop: \", addr, XAddress::is_good(addr) ? \"good\" : \"bad\");\n+    XOop::from_address(addr)->print_on(st);\n+    return true;\n+  }\n+\n+  return false;\n+}\n+\n+void XHeap::verify() {\n+  \/\/ Heap verification can only be done between mark end and\n+  \/\/ relocate start. This is the only window where all oop are\n+  \/\/ good and the whole heap is in a consistent state.\n+  guarantee(XGlobalPhase == XPhaseMarkCompleted, \"Invalid phase\");\n+\n+  XVerify::after_weak_processing();\n+}\n","filename":"src\/hotspot\/share\/gc\/x\/xHeap.cpp","additions":536,"deletions":0,"binary":false,"changes":536,"status":"added"},{"patch":"@@ -0,0 +1,167 @@\n+\/*\n+ * Copyright (c) 2015, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XHEAP_HPP\n+#define SHARE_GC_X_XHEAP_HPP\n+\n+#include \"gc\/x\/xAllocationFlags.hpp\"\n+#include \"gc\/x\/xArray.hpp\"\n+#include \"gc\/x\/xForwardingTable.hpp\"\n+#include \"gc\/x\/xMark.hpp\"\n+#include \"gc\/x\/xObjectAllocator.hpp\"\n+#include \"gc\/x\/xPageAllocator.hpp\"\n+#include \"gc\/x\/xPageTable.hpp\"\n+#include \"gc\/x\/xReferenceProcessor.hpp\"\n+#include \"gc\/x\/xRelocate.hpp\"\n+#include \"gc\/x\/xRelocationSet.hpp\"\n+#include \"gc\/x\/xWeakRootsProcessor.hpp\"\n+#include \"gc\/x\/xServiceability.hpp\"\n+#include \"gc\/x\/xUnload.hpp\"\n+#include \"gc\/x\/xWorkers.hpp\"\n+\n+class ThreadClosure;\n+class VMStructs;\n+class XPage;\n+class XRelocationSetSelector;\n+\n+class XHeap {\n+  friend class ::VMStructs;\n+\n+private:\n+  static XHeap*       _heap;\n+\n+  XWorkers            _workers;\n+  XObjectAllocator    _object_allocator;\n+  XPageAllocator      _page_allocator;\n+  XPageTable          _page_table;\n+  XForwardingTable    _forwarding_table;\n+  XMark               _mark;\n+  XReferenceProcessor _reference_processor;\n+  XWeakRootsProcessor _weak_roots_processor;\n+  XRelocate           _relocate;\n+  XRelocationSet      _relocation_set;\n+  XUnload             _unload;\n+  XServiceability     _serviceability;\n+\n+  void flip_to_marked();\n+  void flip_to_remapped();\n+\n+  void free_empty_pages(XRelocationSetSelector* selector, int bulk);\n+\n+  void out_of_memory();\n+\n+public:\n+  static XHeap* heap();\n+\n+  XHeap();\n+\n+  bool is_initialized() const;\n+\n+  \/\/ Heap metrics\n+  size_t min_capacity() const;\n+  size_t max_capacity() const;\n+  size_t soft_max_capacity() const;\n+  size_t capacity() const;\n+  size_t used() const;\n+  size_t unused() const;\n+\n+  size_t tlab_capacity() const;\n+  size_t tlab_used() const;\n+  size_t max_tlab_size() const;\n+  size_t unsafe_max_tlab_alloc() const;\n+\n+  bool is_in(uintptr_t addr) const;\n+\n+  \/\/ Threads\n+  uint active_workers() const;\n+  void set_active_workers(uint nworkers);\n+  void threads_do(ThreadClosure* tc) const;\n+\n+  \/\/ Reference processing\n+  ReferenceDiscoverer* reference_discoverer();\n+  void set_soft_reference_policy(bool clear);\n+\n+  \/\/ Non-strong reference processing\n+  void process_non_strong_references();\n+\n+  \/\/ Page allocation\n+  XPage* alloc_page(uint8_t type, size_t size, XAllocationFlags flags);\n+  void undo_alloc_page(XPage* page);\n+  void free_page(XPage* page, bool reclaimed);\n+  void free_pages(const XArray<XPage*>* pages, bool reclaimed);\n+\n+  \/\/ Object allocation\n+  uintptr_t alloc_tlab(size_t size);\n+  uintptr_t alloc_object(size_t size);\n+  uintptr_t alloc_object_for_relocation(size_t size);\n+  void undo_alloc_object_for_relocation(uintptr_t addr, size_t size);\n+  bool has_alloc_stalled() const;\n+  void check_out_of_memory();\n+\n+  \/\/ Marking\n+  bool is_object_live(uintptr_t addr) const;\n+  bool is_object_strongly_live(uintptr_t addr) const;\n+  template <bool gc_thread, bool follow, bool finalizable, bool publish> void mark_object(uintptr_t addr);\n+  void mark_start();\n+  void mark(bool initial);\n+  void mark_flush_and_free(Thread* thread);\n+  bool mark_end();\n+  void mark_free();\n+  void keep_alive(oop obj);\n+\n+  \/\/ Relocation set\n+  void select_relocation_set();\n+  void reset_relocation_set();\n+\n+  \/\/ Relocation\n+  void relocate_start();\n+  uintptr_t relocate_object(uintptr_t addr);\n+  uintptr_t remap_object(uintptr_t addr);\n+  void relocate();\n+\n+  \/\/ Continuations\n+  bool is_allocating(uintptr_t addr) const;\n+\n+  \/\/ Iteration\n+  void object_iterate(ObjectClosure* cl, bool visit_weaks);\n+  ParallelObjectIteratorImpl* parallel_object_iterator(uint nworkers, bool visit_weaks);\n+  void pages_do(XPageClosure* cl);\n+\n+  \/\/ Serviceability\n+  void serviceability_initialize();\n+  GCMemoryManager* serviceability_cycle_memory_manager();\n+  GCMemoryManager* serviceability_pause_memory_manager();\n+  MemoryPool* serviceability_memory_pool();\n+  XServiceabilityCounters* serviceability_counters();\n+\n+  \/\/ Printing\n+  void print_on(outputStream* st) const;\n+  void print_extended_on(outputStream* st) const;\n+  bool print_location(outputStream* st, uintptr_t addr) const;\n+\n+  \/\/ Verification\n+  bool is_oop(uintptr_t addr) const;\n+  void verify();\n+};\n+\n+#endif \/\/ SHARE_GC_X_XHEAP_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xHeap.hpp","additions":167,"deletions":0,"binary":false,"changes":167,"status":"added"},{"patch":"@@ -0,0 +1,127 @@\n+\/*\n+ * Copyright (c) 2015, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XHEAP_INLINE_HPP\n+#define SHARE_GC_X_XHEAP_INLINE_HPP\n+\n+#include \"gc\/x\/xHeap.hpp\"\n+\n+#include \"gc\/x\/xAddress.inline.hpp\"\n+#include \"gc\/x\/xForwardingTable.inline.hpp\"\n+#include \"gc\/x\/xMark.inline.hpp\"\n+#include \"gc\/x\/xPage.inline.hpp\"\n+#include \"gc\/x\/xPageTable.inline.hpp\"\n+#include \"utilities\/debug.hpp\"\n+\n+inline XHeap* XHeap::heap() {\n+  assert(_heap != NULL, \"Not initialized\");\n+  return _heap;\n+}\n+\n+inline ReferenceDiscoverer* XHeap::reference_discoverer() {\n+  return &_reference_processor;\n+}\n+\n+inline bool XHeap::is_object_live(uintptr_t addr) const {\n+  XPage* page = _page_table.get(addr);\n+  return page->is_object_live(addr);\n+}\n+\n+inline bool XHeap::is_object_strongly_live(uintptr_t addr) const {\n+  XPage* page = _page_table.get(addr);\n+  return page->is_object_strongly_live(addr);\n+}\n+\n+template <bool gc_thread, bool follow, bool finalizable, bool publish>\n+inline void XHeap::mark_object(uintptr_t addr) {\n+  assert(XGlobalPhase == XPhaseMark, \"Mark not allowed\");\n+  _mark.mark_object<gc_thread, follow, finalizable, publish>(addr);\n+}\n+\n+inline uintptr_t XHeap::alloc_tlab(size_t size) {\n+  guarantee(size <= max_tlab_size(), \"TLAB too large\");\n+  return _object_allocator.alloc_object(size);\n+}\n+\n+inline uintptr_t XHeap::alloc_object(size_t size) {\n+  uintptr_t addr = _object_allocator.alloc_object(size);\n+  assert(XAddress::is_good_or_null(addr), \"Bad address\");\n+\n+  if (addr == 0) {\n+    out_of_memory();\n+  }\n+\n+  return addr;\n+}\n+\n+inline uintptr_t XHeap::alloc_object_for_relocation(size_t size) {\n+  const uintptr_t addr = _object_allocator.alloc_object_for_relocation(&_page_table, size);\n+  assert(XAddress::is_good_or_null(addr), \"Bad address\");\n+  return addr;\n+}\n+\n+inline void XHeap::undo_alloc_object_for_relocation(uintptr_t addr, size_t size) {\n+  XPage* const page = _page_table.get(addr);\n+  _object_allocator.undo_alloc_object_for_relocation(page, addr, size);\n+}\n+\n+inline uintptr_t XHeap::relocate_object(uintptr_t addr) {\n+  assert(XGlobalPhase == XPhaseRelocate, \"Relocate not allowed\");\n+\n+  XForwarding* const forwarding = _forwarding_table.get(addr);\n+  if (forwarding == NULL) {\n+    \/\/ Not forwarding\n+    return XAddress::good(addr);\n+  }\n+\n+  \/\/ Relocate object\n+  return _relocate.relocate_object(forwarding, XAddress::good(addr));\n+}\n+\n+inline uintptr_t XHeap::remap_object(uintptr_t addr) {\n+  assert(XGlobalPhase == XPhaseMark ||\n+         XGlobalPhase == XPhaseMarkCompleted, \"Forward not allowed\");\n+\n+  XForwarding* const forwarding = _forwarding_table.get(addr);\n+  if (forwarding == NULL) {\n+    \/\/ Not forwarding\n+    return XAddress::good(addr);\n+  }\n+\n+  \/\/ Forward object\n+  return _relocate.forward_object(forwarding, XAddress::good(addr));\n+}\n+\n+inline bool XHeap::has_alloc_stalled() const {\n+  return _page_allocator.has_alloc_stalled();\n+}\n+\n+inline void XHeap::check_out_of_memory() {\n+  _page_allocator.check_out_of_memory();\n+}\n+\n+inline bool XHeap::is_oop(uintptr_t addr) const {\n+  return XAddress::is_good(addr) && is_object_aligned(addr) && is_in(addr);\n+}\n+\n+#endif \/\/ SHARE_GC_X_XHEAP_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xHeap.inline.hpp","additions":127,"deletions":0,"binary":false,"changes":127,"status":"added"},{"patch":"@@ -0,0 +1,439 @@\n+\/*\n+ * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"classfile\/classLoaderDataGraph.hpp\"\n+#include \"gc\/shared\/barrierSetNMethod.hpp\"\n+#include \"gc\/shared\/gc_globals.hpp\"\n+#include \"gc\/shared\/taskqueue.inline.hpp\"\n+#include \"gc\/x\/xAddress.inline.hpp\"\n+#include \"gc\/x\/xCollectedHeap.hpp\"\n+#include \"gc\/x\/xGlobals.hpp\"\n+#include \"gc\/x\/xGranuleMap.inline.hpp\"\n+#include \"gc\/x\/xHeapIterator.hpp\"\n+#include \"gc\/x\/xLock.inline.hpp\"\n+#include \"gc\/x\/xNMethod.hpp\"\n+#include \"gc\/x\/xOop.inline.hpp\"\n+#include \"memory\/iterator.inline.hpp\"\n+#include \"utilities\/bitMap.inline.hpp\"\n+\n+class XHeapIteratorBitMap : public CHeapObj<mtGC> {\n+private:\n+  CHeapBitMap _bitmap;\n+\n+public:\n+  XHeapIteratorBitMap(size_t size_in_bits) :\n+      _bitmap(size_in_bits, mtGC) {}\n+\n+  bool try_set_bit(size_t index) {\n+    return _bitmap.par_set_bit(index);\n+  }\n+};\n+\n+class XHeapIteratorContext {\n+private:\n+  XHeapIterator* const           _iter;\n+  XHeapIteratorQueue* const      _queue;\n+  XHeapIteratorArrayQueue* const _array_queue;\n+  const uint                     _worker_id;\n+  XStatTimerDisable              _timer_disable;\n+\n+public:\n+  XHeapIteratorContext(XHeapIterator* iter, uint worker_id) :\n+      _iter(iter),\n+      _queue(_iter->_queues.queue(worker_id)),\n+      _array_queue(_iter->_array_queues.queue(worker_id)),\n+      _worker_id(worker_id) {}\n+\n+  void mark_and_push(oop obj) const {\n+    if (_iter->mark_object(obj)) {\n+      _queue->push(obj);\n+    }\n+  }\n+\n+  void push_array(const ObjArrayTask& array) const {\n+    _array_queue->push(array);\n+  }\n+\n+  bool pop(oop& obj) const {\n+    return _queue->pop_overflow(obj) || _queue->pop_local(obj);\n+  }\n+\n+  bool pop_array(ObjArrayTask& array) const {\n+    return _array_queue->pop_overflow(array) || _array_queue->pop_local(array);\n+  }\n+\n+  bool steal(oop& obj) const {\n+    return _iter->_queues.steal(_worker_id, obj);\n+  }\n+\n+  bool steal_array(ObjArrayTask& array) const {\n+    return _iter->_array_queues.steal(_worker_id, array);\n+  }\n+\n+  bool is_drained() const {\n+    return _queue->is_empty() && _array_queue->is_empty();\n+  }\n+};\n+\n+template <bool Weak>\n+class XHeapIteratorRootOopClosure : public OopClosure {\n+private:\n+  const XHeapIteratorContext& _context;\n+\n+  oop load_oop(oop* p) {\n+    if (Weak) {\n+      return NativeAccess<AS_NO_KEEPALIVE | ON_PHANTOM_OOP_REF>::oop_load(p);\n+    }\n+\n+    return NativeAccess<AS_NO_KEEPALIVE>::oop_load(p);\n+  }\n+\n+public:\n+  XHeapIteratorRootOopClosure(const XHeapIteratorContext& context) :\n+      _context(context) {}\n+\n+  virtual void do_oop(oop* p) {\n+    const oop obj = load_oop(p);\n+    _context.mark_and_push(obj);\n+  }\n+\n+  virtual void do_oop(narrowOop* p) {\n+    ShouldNotReachHere();\n+  }\n+};\n+\n+template <bool VisitReferents>\n+class XHeapIteratorOopClosure : public OopIterateClosure {\n+private:\n+  const XHeapIteratorContext& _context;\n+  const oop                   _base;\n+\n+  oop load_oop(oop* p) {\n+    assert(XCollectedHeap::heap()->is_in(p), \"Should be in heap\");\n+\n+    if (VisitReferents) {\n+      return HeapAccess<AS_NO_KEEPALIVE | ON_UNKNOWN_OOP_REF>::oop_load_at(_base, _base->field_offset(p));\n+    }\n+\n+    return HeapAccess<AS_NO_KEEPALIVE>::oop_load(p);\n+  }\n+\n+public:\n+  XHeapIteratorOopClosure(const XHeapIteratorContext& context, oop base) :\n+      OopIterateClosure(),\n+      _context(context),\n+      _base(base) {}\n+\n+  virtual ReferenceIterationMode reference_iteration_mode() {\n+    return VisitReferents ? DO_FIELDS : DO_FIELDS_EXCEPT_REFERENT;\n+  }\n+\n+  virtual void do_oop(oop* p) {\n+    const oop obj = load_oop(p);\n+    _context.mark_and_push(obj);\n+  }\n+\n+  virtual void do_oop(narrowOop* p) {\n+    ShouldNotReachHere();\n+  }\n+\n+  virtual bool do_metadata() {\n+    return true;\n+  }\n+\n+  virtual void do_klass(Klass* k) {\n+    ClassLoaderData* const cld = k->class_loader_data();\n+    XHeapIteratorOopClosure::do_cld(cld);\n+  }\n+\n+  virtual void do_cld(ClassLoaderData* cld) {\n+    class NativeAccessClosure : public OopClosure {\n+    private:\n+      const XHeapIteratorContext& _context;\n+\n+    public:\n+      explicit NativeAccessClosure(const XHeapIteratorContext& context) :\n+          _context(context) {}\n+\n+      virtual void do_oop(oop* p) {\n+        assert(!XCollectedHeap::heap()->is_in(p), \"Should not be in heap\");\n+        const oop obj = NativeAccess<AS_NO_KEEPALIVE>::oop_load(p);\n+        _context.mark_and_push(obj);\n+      }\n+\n+      virtual void do_oop(narrowOop* p) {\n+        ShouldNotReachHere();\n+      }\n+    };\n+\n+    NativeAccessClosure cl(_context);\n+    cld->oops_do(&cl, ClassLoaderData::_claim_other);\n+  }\n+\n+  \/\/ Don't follow loom stack metadata; it's already followed in other ways through CLDs\n+  virtual void do_nmethod(nmethod* nm) {}\n+  virtual void do_method(Method* m) {}\n+};\n+\n+XHeapIterator::XHeapIterator(uint nworkers, bool visit_weaks) :\n+    _visit_weaks(visit_weaks),\n+    _timer_disable(),\n+    _bitmaps(XAddressOffsetMax),\n+    _bitmaps_lock(),\n+    _queues(nworkers),\n+    _array_queues(nworkers),\n+    _roots(ClassLoaderData::_claim_other),\n+    _weak_roots(),\n+    _terminator(nworkers, &_queues) {\n+\n+  \/\/ Create queues\n+  for (uint i = 0; i < _queues.size(); i++) {\n+    XHeapIteratorQueue* const queue = new XHeapIteratorQueue();\n+    _queues.register_queue(i, queue);\n+  }\n+\n+  \/\/ Create array queues\n+  for (uint i = 0; i < _array_queues.size(); i++) {\n+    XHeapIteratorArrayQueue* const array_queue = new XHeapIteratorArrayQueue();\n+    _array_queues.register_queue(i, array_queue);\n+  }\n+}\n+\n+XHeapIterator::~XHeapIterator() {\n+  \/\/ Destroy bitmaps\n+  XHeapIteratorBitMapsIterator iter(&_bitmaps);\n+  for (XHeapIteratorBitMap* bitmap; iter.next(&bitmap);) {\n+    delete bitmap;\n+  }\n+\n+  \/\/ Destroy array queues\n+  for (uint i = 0; i < _array_queues.size(); i++) {\n+    delete _array_queues.queue(i);\n+  }\n+\n+  \/\/ Destroy queues\n+  for (uint i = 0; i < _queues.size(); i++) {\n+    delete _queues.queue(i);\n+  }\n+\n+  \/\/ Clear claimed CLD bits\n+  ClassLoaderDataGraph::clear_claimed_marks(ClassLoaderData::_claim_other);\n+}\n+\n+static size_t object_index_max() {\n+  return XGranuleSize >> XObjectAlignmentSmallShift;\n+}\n+\n+static size_t object_index(oop obj) {\n+  const uintptr_t addr = XOop::to_address(obj);\n+  const uintptr_t offset = XAddress::offset(addr);\n+  const uintptr_t mask = XGranuleSize - 1;\n+  return (offset & mask) >> XObjectAlignmentSmallShift;\n+}\n+\n+XHeapIteratorBitMap* XHeapIterator::object_bitmap(oop obj) {\n+  const uintptr_t offset = XAddress::offset(XOop::to_address(obj));\n+  XHeapIteratorBitMap* bitmap = _bitmaps.get_acquire(offset);\n+  if (bitmap == NULL) {\n+    XLocker<XLock> locker(&_bitmaps_lock);\n+    bitmap = _bitmaps.get(offset);\n+    if (bitmap == NULL) {\n+      \/\/ Install new bitmap\n+      bitmap = new XHeapIteratorBitMap(object_index_max());\n+      _bitmaps.release_put(offset, bitmap);\n+    }\n+  }\n+\n+  return bitmap;\n+}\n+\n+bool XHeapIterator::mark_object(oop obj) {\n+  if (obj == NULL) {\n+    return false;\n+  }\n+\n+  XHeapIteratorBitMap* const bitmap = object_bitmap(obj);\n+  const size_t index = object_index(obj);\n+  return bitmap->try_set_bit(index);\n+}\n+\n+typedef ClaimingCLDToOopClosure<ClassLoaderData::_claim_other> XHeapIteratorCLDCLosure;\n+\n+class XHeapIteratorNMethodClosure : public NMethodClosure {\n+private:\n+  OopClosure* const        _cl;\n+  BarrierSetNMethod* const _bs_nm;\n+\n+public:\n+  XHeapIteratorNMethodClosure(OopClosure* cl) :\n+      _cl(cl),\n+      _bs_nm(BarrierSet::barrier_set()->barrier_set_nmethod()) {}\n+\n+  virtual void do_nmethod(nmethod* nm) {\n+    \/\/ If ClassUnloading is turned off, all nmethods are considered strong,\n+    \/\/ not only those on the call stacks. The heap iteration might happen\n+    \/\/ before the concurrent processign of the code cache, make sure that\n+    \/\/ all nmethods have been processed before visiting the oops.\n+    _bs_nm->nmethod_entry_barrier(nm);\n+\n+    XNMethod::nmethod_oops_do(nm, _cl);\n+  }\n+};\n+\n+class XHeapIteratorThreadClosure : public ThreadClosure {\n+private:\n+  OopClosure* const        _cl;\n+  CodeBlobToNMethodClosure _cb_cl;\n+\n+public:\n+  XHeapIteratorThreadClosure(OopClosure* cl, NMethodClosure* nm_cl) :\n+      _cl(cl),\n+      _cb_cl(nm_cl) {}\n+\n+  void do_thread(Thread* thread) {\n+    thread->oops_do(_cl, &_cb_cl);\n+  }\n+};\n+\n+void XHeapIterator::push_strong_roots(const XHeapIteratorContext& context) {\n+  XHeapIteratorRootOopClosure<false \/* Weak *\/> cl(context);\n+  XHeapIteratorCLDCLosure cld_cl(&cl);\n+  XHeapIteratorNMethodClosure nm_cl(&cl);\n+  XHeapIteratorThreadClosure thread_cl(&cl, &nm_cl);\n+\n+  _roots.apply(&cl,\n+               &cld_cl,\n+               &thread_cl,\n+               &nm_cl);\n+}\n+\n+void XHeapIterator::push_weak_roots(const XHeapIteratorContext& context) {\n+  XHeapIteratorRootOopClosure<true  \/* Weak *\/> cl(context);\n+  _weak_roots.apply(&cl);\n+}\n+\n+template <bool VisitWeaks>\n+void XHeapIterator::push_roots(const XHeapIteratorContext& context) {\n+  push_strong_roots(context);\n+  if (VisitWeaks) {\n+    push_weak_roots(context);\n+  }\n+}\n+\n+template <bool VisitReferents>\n+void XHeapIterator::follow_object(const XHeapIteratorContext& context, oop obj) {\n+  XHeapIteratorOopClosure<VisitReferents> cl(context, obj);\n+  obj->oop_iterate(&cl);\n+}\n+\n+void XHeapIterator::follow_array(const XHeapIteratorContext& context, oop obj) {\n+  \/\/ Follow klass\n+  XHeapIteratorOopClosure<false \/* VisitReferents *\/> cl(context, obj);\n+  cl.do_klass(obj->klass());\n+\n+  \/\/ Push array chunk\n+  context.push_array(ObjArrayTask(obj, 0 \/* index *\/));\n+}\n+\n+void XHeapIterator::follow_array_chunk(const XHeapIteratorContext& context, const ObjArrayTask& array) {\n+  const objArrayOop obj = objArrayOop(array.obj());\n+  const int length = obj->length();\n+  const int start = array.index();\n+  const int stride = MIN2<int>(length - start, ObjArrayMarkingStride);\n+  const int end = start + stride;\n+\n+  \/\/ Push remaining array chunk first\n+  if (end < length) {\n+    context.push_array(ObjArrayTask(obj, end));\n+  }\n+\n+  \/\/ Follow array chunk\n+  XHeapIteratorOopClosure<false \/* VisitReferents *\/> cl(context, obj);\n+  obj->oop_iterate_range(&cl, start, end);\n+}\n+\n+template <bool VisitWeaks>\n+void XHeapIterator::visit_and_follow(const XHeapIteratorContext& context, ObjectClosure* cl, oop obj) {\n+  \/\/ Visit\n+  cl->do_object(obj);\n+\n+  \/\/ Follow\n+  if (obj->is_objArray()) {\n+    follow_array(context, obj);\n+  } else {\n+    follow_object<VisitWeaks>(context, obj);\n+  }\n+}\n+\n+template <bool VisitWeaks>\n+void XHeapIterator::drain(const XHeapIteratorContext& context, ObjectClosure* cl) {\n+  ObjArrayTask array;\n+  oop obj;\n+\n+  do {\n+    while (context.pop(obj)) {\n+      visit_and_follow<VisitWeaks>(context, cl, obj);\n+    }\n+\n+    if (context.pop_array(array)) {\n+      follow_array_chunk(context, array);\n+    }\n+  } while (!context.is_drained());\n+}\n+\n+template <bool VisitWeaks>\n+void XHeapIterator::steal(const XHeapIteratorContext& context, ObjectClosure* cl) {\n+  ObjArrayTask array;\n+  oop obj;\n+\n+  if (context.steal_array(array)) {\n+    follow_array_chunk(context, array);\n+  } else if (context.steal(obj)) {\n+    visit_and_follow<VisitWeaks>(context, cl, obj);\n+  }\n+}\n+\n+template <bool VisitWeaks>\n+void XHeapIterator::drain_and_steal(const XHeapIteratorContext& context, ObjectClosure* cl) {\n+  do {\n+    drain<VisitWeaks>(context, cl);\n+    steal<VisitWeaks>(context, cl);\n+  } while (!context.is_drained() || !_terminator.offer_termination());\n+}\n+\n+template <bool VisitWeaks>\n+void XHeapIterator::object_iterate_inner(const XHeapIteratorContext& context, ObjectClosure* object_cl) {\n+  push_roots<VisitWeaks>(context);\n+  drain_and_steal<VisitWeaks>(context, object_cl);\n+}\n+\n+void XHeapIterator::object_iterate(ObjectClosure* cl, uint worker_id) {\n+  XHeapIteratorContext context(this, worker_id);\n+\n+  if (_visit_weaks) {\n+    object_iterate_inner<true \/* VisitWeaks *\/>(context, cl);\n+  } else {\n+    object_iterate_inner<false \/* VisitWeaks *\/>(context, cl);\n+  }\n+}\n","filename":"src\/hotspot\/share\/gc\/x\/xHeapIterator.cpp","additions":439,"deletions":0,"binary":false,"changes":439,"status":"added"},{"patch":"@@ -0,0 +1,97 @@\n+\/*\n+ * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XHEAPITERATOR_HPP\n+#define SHARE_GC_X_XHEAPITERATOR_HPP\n+\n+#include \"gc\/shared\/collectedHeap.hpp\"\n+#include \"gc\/shared\/taskTerminator.hpp\"\n+#include \"gc\/shared\/taskqueue.hpp\"\n+#include \"gc\/x\/xGranuleMap.hpp\"\n+#include \"gc\/x\/xLock.hpp\"\n+#include \"gc\/x\/xRootsIterator.hpp\"\n+#include \"gc\/x\/xStat.hpp\"\n+\n+class XHeapIteratorBitMap;\n+class XHeapIteratorContext;\n+\n+using XHeapIteratorBitMaps = XGranuleMap<XHeapIteratorBitMap*>;\n+using XHeapIteratorBitMapsIterator = XGranuleMapIterator<XHeapIteratorBitMap*>;\n+using XHeapIteratorQueue = OverflowTaskQueue<oop, mtGC>;\n+using XHeapIteratorQueues = GenericTaskQueueSet<XHeapIteratorQueue, mtGC>;\n+using XHeapIteratorArrayQueue = OverflowTaskQueue<ObjArrayTask, mtGC>;\n+using XHeapIteratorArrayQueues = GenericTaskQueueSet<XHeapIteratorArrayQueue, mtGC>;\n+\n+class XHeapIterator : public ParallelObjectIteratorImpl {\n+  friend class XHeapIteratorContext;\n+\n+private:\n+  const bool               _visit_weaks;\n+  XStatTimerDisable        _timer_disable;\n+  XHeapIteratorBitMaps     _bitmaps;\n+  XLock                    _bitmaps_lock;\n+  XHeapIteratorQueues      _queues;\n+  XHeapIteratorArrayQueues _array_queues;\n+  XRootsIterator           _roots;\n+  XWeakRootsIterator       _weak_roots;\n+  TaskTerminator           _terminator;\n+\n+  XHeapIteratorBitMap* object_bitmap(oop obj);\n+\n+  bool mark_object(oop obj);\n+\n+  void push_strong_roots(const XHeapIteratorContext& context);\n+  void push_weak_roots(const XHeapIteratorContext& context);\n+\n+  template <bool VisitWeaks>\n+  void push_roots(const XHeapIteratorContext& context);\n+\n+  template <bool VisitReferents>\n+  void follow_object(const XHeapIteratorContext& context, oop obj);\n+\n+  void follow_array(const XHeapIteratorContext& context, oop obj);\n+  void follow_array_chunk(const XHeapIteratorContext& context, const ObjArrayTask& array);\n+\n+  template <bool VisitWeaks>\n+  void visit_and_follow(const XHeapIteratorContext& context, ObjectClosure* cl, oop obj);\n+\n+  template <bool VisitWeaks>\n+  void drain(const XHeapIteratorContext& context, ObjectClosure* cl);\n+\n+  template <bool VisitWeaks>\n+  void steal(const XHeapIteratorContext& context, ObjectClosure* cl);\n+\n+  template <bool VisitWeaks>\n+  void drain_and_steal(const XHeapIteratorContext& context, ObjectClosure* cl);\n+\n+  template <bool VisitWeaks>\n+  void object_iterate_inner(const XHeapIteratorContext& context, ObjectClosure* cl);\n+\n+public:\n+  XHeapIterator(uint nworkers, bool visit_weaks);\n+  virtual ~XHeapIterator();\n+\n+  virtual void object_iterate(ObjectClosure* cl, uint worker_id);\n+};\n+\n+#endif \/\/ SHARE_GC_X_XHEAPITERATOR_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xHeapIterator.hpp","additions":97,"deletions":0,"binary":false,"changes":97,"status":"added"},{"patch":"@@ -0,0 +1,104 @@\n+\/*\n+ * Copyright (c) 2019, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/shared\/gcLogPrecious.hpp\"\n+#include \"gc\/shared\/gc_globals.hpp\"\n+#include \"gc\/x\/xCPU.inline.hpp\"\n+#include \"gc\/x\/xGlobals.hpp\"\n+#include \"gc\/x\/xHeuristics.hpp\"\n+#include \"runtime\/globals.hpp\"\n+#include \"runtime\/os.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include \"utilities\/powerOfTwo.hpp\"\n+\n+void XHeuristics::set_medium_page_size() {\n+  \/\/ Set XPageSizeMedium so that a medium page occupies at most 3.125% of the\n+  \/\/ max heap size. XPageSizeMedium is initially set to 0, which means medium\n+  \/\/ pages are effectively disabled. It is adjusted only if XPageSizeMedium\n+  \/\/ becomes larger than XPageSizeSmall.\n+  const size_t min = XGranuleSize;\n+  const size_t max = XGranuleSize * 16;\n+  const size_t unclamped = MaxHeapSize * 0.03125;\n+  const size_t clamped = clamp(unclamped, min, max);\n+  const size_t size = round_down_power_of_2(clamped);\n+\n+  if (size > XPageSizeSmall) {\n+    \/\/ Enable medium pages\n+    XPageSizeMedium             = size;\n+    XPageSizeMediumShift        = log2i_exact(XPageSizeMedium);\n+    XObjectSizeLimitMedium      = XPageSizeMedium \/ 8;\n+    XObjectAlignmentMediumShift = (int)XPageSizeMediumShift - 13;\n+    XObjectAlignmentMedium      = 1 << XObjectAlignmentMediumShift;\n+  }\n+}\n+\n+size_t XHeuristics::relocation_headroom() {\n+  \/\/ Calculate headroom needed to avoid in-place relocation. Each worker will try\n+  \/\/ to allocate a small page, and all workers will share a single medium page.\n+  const uint nworkers = UseDynamicNumberOfGCThreads ? ConcGCThreads : MAX2(ConcGCThreads, ParallelGCThreads);\n+  return (nworkers * XPageSizeSmall) + XPageSizeMedium;\n+}\n+\n+bool XHeuristics::use_per_cpu_shared_small_pages() {\n+  \/\/ Use per-CPU shared small pages only if these pages occupy at most 3.125%\n+  \/\/ of the max heap size. Otherwise fall back to using a single shared small\n+  \/\/ page. This is useful when using small heaps on large machines.\n+  const size_t per_cpu_share = (MaxHeapSize * 0.03125) \/ XCPU::count();\n+  return per_cpu_share >= XPageSizeSmall;\n+}\n+\n+static uint nworkers_based_on_ncpus(double cpu_share_in_percent) {\n+  return ceil(os::initial_active_processor_count() * cpu_share_in_percent \/ 100.0);\n+}\n+\n+static uint nworkers_based_on_heap_size(double heap_share_in_percent) {\n+  const int nworkers = (MaxHeapSize * (heap_share_in_percent \/ 100.0)) \/ XPageSizeSmall;\n+  return MAX2(nworkers, 1);\n+}\n+\n+static uint nworkers(double cpu_share_in_percent) {\n+  \/\/ Cap number of workers so that they don't use more than 2% of the max heap\n+  \/\/ during relocation. This is useful when using small heaps on large machines.\n+  return MIN2(nworkers_based_on_ncpus(cpu_share_in_percent),\n+              nworkers_based_on_heap_size(2.0));\n+}\n+\n+uint XHeuristics::nparallel_workers() {\n+  \/\/ Use 60% of the CPUs, rounded up. We would like to use as many threads as\n+  \/\/ possible to increase parallelism. However, using a thread count that is\n+  \/\/ close to the number of processors tends to lead to over-provisioning and\n+  \/\/ scheduling latency issues. Using 60% of the active processors appears to\n+  \/\/ be a fairly good balance.\n+  return nworkers(60.0);\n+}\n+\n+uint XHeuristics::nconcurrent_workers() {\n+  \/\/ The number of concurrent threads we would like to use heavily depends\n+  \/\/ on the type of workload we are running. Using too many threads will have\n+  \/\/ a negative impact on the application throughput, while using too few\n+  \/\/ threads will prolong the GC-cycle and we then risk being out-run by the\n+  \/\/ application. When in dynamic mode, use up to 25% of the active processors.\n+  \/\/  When in non-dynamic mode, use 12.5% of the active processors.\n+  return nworkers(UseDynamicNumberOfGCThreads ? 25.0 : 12.5);\n+}\n","filename":"src\/hotspot\/share\/gc\/x\/xHeuristics.cpp","additions":104,"deletions":0,"binary":false,"changes":104,"status":"added"},{"patch":"@@ -0,0 +1,41 @@\n+\/*\n+ * Copyright (c) 2019, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XHEURISTICS_HPP\n+#define SHARE_GC_X_XHEURISTICS_HPP\n+\n+#include \"memory\/allStatic.hpp\"\n+\n+class XHeuristics : public AllStatic {\n+public:\n+  static void set_medium_page_size();\n+\n+  static size_t relocation_headroom();\n+\n+  static bool use_per_cpu_shared_small_pages();\n+\n+  static uint nparallel_workers();\n+  static uint nconcurrent_workers();\n+};\n+\n+#endif \/\/ SHARE_GC_X_XHEURISTICS_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xHeuristics.hpp","additions":41,"deletions":0,"binary":false,"changes":41,"status":"added"},{"patch":"@@ -0,0 +1,58 @@\n+\/*\n+ * Copyright (c) 2016, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/x\/xAddress.hpp\"\n+#include \"gc\/x\/xBarrierSet.hpp\"\n+#include \"gc\/x\/xCPU.hpp\"\n+#include \"gc\/x\/xGlobals.hpp\"\n+#include \"gc\/x\/xHeuristics.hpp\"\n+#include \"gc\/x\/xInitialize.hpp\"\n+#include \"gc\/x\/xLargePages.hpp\"\n+#include \"gc\/x\/xNUMA.hpp\"\n+#include \"gc\/x\/xStat.hpp\"\n+#include \"gc\/x\/xThreadLocalAllocBuffer.hpp\"\n+#include \"gc\/x\/xTracer.hpp\"\n+#include \"logging\/log.hpp\"\n+#include \"runtime\/vm_version.hpp\"\n+\n+XInitialize::XInitialize(XBarrierSet* barrier_set) {\n+  log_info(gc, init)(\"Initializing %s\", XName);\n+  log_info(gc, init)(\"Version: %s (%s)\",\n+                     VM_Version::vm_release(),\n+                     VM_Version::jdk_debug_level());\n+  log_info(gc, init)(\"Using legacy single-generation mode\");\n+\n+  \/\/ Early initialization\n+  XAddress::initialize();\n+  XNUMA::initialize();\n+  XCPU::initialize();\n+  XStatValue::initialize();\n+  XThreadLocalAllocBuffer::initialize();\n+  XTracer::initialize();\n+  XLargePages::initialize();\n+  XHeuristics::set_medium_page_size();\n+  XBarrierSet::set_barrier_set(barrier_set);\n+\n+  pd_initialize();\n+}\n","filename":"src\/hotspot\/share\/gc\/x\/xInitialize.cpp","additions":58,"deletions":0,"binary":false,"changes":58,"status":"added"},{"patch":"@@ -0,0 +1,39 @@\n+\/*\n+ * Copyright (c) 2016, 2017, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XINITIALIZE_HPP\n+#define SHARE_GC_X_XINITIALIZE_HPP\n+\n+#include \"memory\/allocation.hpp\"\n+\n+class XBarrierSet;\n+\n+class XInitialize {\n+private:\n+  void pd_initialize();\n+\n+public:\n+  XInitialize(XBarrierSet* barrier_set);\n+};\n+\n+#endif \/\/ SHARE_GC_X_XINITIALIZE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xInitialize.hpp","additions":39,"deletions":0,"binary":false,"changes":39,"status":"added"},{"patch":"@@ -0,0 +1,49 @@\n+\/*\n+ * Copyright (c) 2017, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/shared\/gcLogPrecious.hpp\"\n+#include \"gc\/x\/xLargePages.hpp\"\n+#include \"runtime\/os.hpp\"\n+\n+XLargePages::State XLargePages::_state;\n+\n+void XLargePages::initialize() {\n+  pd_initialize();\n+\n+  log_info_p(gc, init)(\"Memory: \" JULONG_FORMAT \"M\", os::physical_memory() \/ M);\n+  log_info_p(gc, init)(\"Large Page Support: %s\", to_string());\n+}\n+\n+const char* XLargePages::to_string() {\n+  switch (_state) {\n+  case Explicit:\n+    return \"Enabled (Explicit)\";\n+\n+  case Transparent:\n+    return \"Enabled (Transparent)\";\n+\n+  default:\n+    return \"Disabled\";\n+  }\n+}\n","filename":"src\/hotspot\/share\/gc\/x\/xLargePages.cpp","additions":49,"deletions":0,"binary":false,"changes":49,"status":"added"},{"patch":"@@ -0,0 +1,51 @@\n+\/*\n+ * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XLARGEPAGES_HPP\n+#define SHARE_GC_X_XLARGEPAGES_HPP\n+\n+#include \"memory\/allStatic.hpp\"\n+\n+class XLargePages : public AllStatic {\n+private:\n+  enum State {\n+    Disabled,\n+    Explicit,\n+    Transparent\n+  };\n+\n+  static State _state;\n+\n+  static void pd_initialize();\n+\n+public:\n+  static void initialize();\n+\n+  static bool is_enabled();\n+  static bool is_explicit();\n+  static bool is_transparent();\n+\n+  static const char* to_string();\n+};\n+\n+#endif \/\/ SHARE_GC_X_XLARGEPAGES_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xLargePages.hpp","additions":51,"deletions":0,"binary":false,"changes":51,"status":"added"},{"patch":"@@ -0,0 +1,41 @@\n+\/*\n+ * Copyright (c) 2017, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XLARGEPAGES_INLINE_HPP\n+#define SHARE_GC_X_XLARGEPAGES_INLINE_HPP\n+\n+#include \"gc\/x\/xLargePages.hpp\"\n+\n+inline bool XLargePages::is_enabled() {\n+  return _state != Disabled;\n+}\n+\n+inline bool XLargePages::is_explicit() {\n+  return _state == Explicit;\n+}\n+\n+inline bool XLargePages::is_transparent() {\n+  return _state == Transparent;\n+}\n+\n+#endif \/\/ SHARE_GC_X_XLARGEPAGES_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xLargePages.inline.hpp","additions":41,"deletions":0,"binary":false,"changes":41,"status":"added"},{"patch":"@@ -0,0 +1,116 @@\n+\/*\n+ * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XLIST_HPP\n+#define SHARE_GC_X_XLIST_HPP\n+\n+#include \"memory\/allocation.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+template <typename T> class XList;\n+\n+\/\/ Element in a doubly linked list\n+template <typename T>\n+class XListNode {\n+  friend class XList<T>;\n+\n+private:\n+  XListNode<T>* _next;\n+  XListNode<T>* _prev;\n+\n+  NONCOPYABLE(XListNode);\n+\n+  void verify_links() const;\n+  void verify_links_linked() const;\n+  void verify_links_unlinked() const;\n+\n+public:\n+  XListNode();\n+  ~XListNode();\n+};\n+\n+\/\/ Doubly linked list\n+template <typename T>\n+class XList {\n+private:\n+  XListNode<T> _head;\n+  size_t       _size;\n+\n+  NONCOPYABLE(XList);\n+\n+  void verify_head() const;\n+\n+  void insert(XListNode<T>* before, XListNode<T>* node);\n+\n+  XListNode<T>* cast_to_inner(T* elem) const;\n+  T* cast_to_outer(XListNode<T>* node) const;\n+\n+public:\n+  XList();\n+\n+  size_t size() const;\n+  bool is_empty() const;\n+\n+  T* first() const;\n+  T* last() const;\n+  T* next(T* elem) const;\n+  T* prev(T* elem) const;\n+\n+  void insert_first(T* elem);\n+  void insert_last(T* elem);\n+  void insert_before(T* before, T* elem);\n+  void insert_after(T* after, T* elem);\n+\n+  void remove(T* elem);\n+  T* remove_first();\n+  T* remove_last();\n+};\n+\n+template <typename T, bool Forward>\n+class XListIteratorImpl : public StackObj {\n+private:\n+  const XList<T>* const _list;\n+  T*                    _next;\n+\n+public:\n+  XListIteratorImpl(const XList<T>* list);\n+\n+  bool next(T** elem);\n+};\n+\n+template <typename T, bool Forward>\n+class XListRemoveIteratorImpl : public StackObj {\n+private:\n+  XList<T>* const _list;\n+\n+public:\n+  XListRemoveIteratorImpl(XList<T>* list);\n+\n+  bool next(T** elem);\n+};\n+\n+template <typename T> using XListIterator = XListIteratorImpl<T, true \/* Forward *\/>;\n+template <typename T> using XListReverseIterator = XListIteratorImpl<T, false \/* Forward *\/>;\n+template <typename T> using XListRemoveIterator = XListRemoveIteratorImpl<T, true \/* Forward *\/>;\n+\n+#endif \/\/ SHARE_GC_X_XLIST_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xList.hpp","additions":116,"deletions":0,"binary":false,"changes":116,"status":"added"},{"patch":"@@ -0,0 +1,238 @@\n+\/*\n+ * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XLIST_INLINE_HPP\n+#define SHARE_GC_X_XLIST_INLINE_HPP\n+\n+#include \"gc\/x\/xList.hpp\"\n+\n+#include \"utilities\/debug.hpp\"\n+\n+template <typename T>\n+inline XListNode<T>::XListNode() :\n+    _next(this),\n+    _prev(this) {}\n+\n+template <typename T>\n+inline XListNode<T>::~XListNode() {\n+  verify_links_unlinked();\n+}\n+\n+template <typename T>\n+inline void XListNode<T>::verify_links() const {\n+  assert(_next->_prev == this, \"Corrupt list node\");\n+  assert(_prev->_next == this, \"Corrupt list node\");\n+}\n+\n+template <typename T>\n+inline void XListNode<T>::verify_links_linked() const {\n+  assert(_next != this, \"Should be in a list\");\n+  assert(_prev != this, \"Should be in a list\");\n+  verify_links();\n+}\n+\n+template <typename T>\n+inline void XListNode<T>::verify_links_unlinked() const {\n+  assert(_next == this, \"Should not be in a list\");\n+  assert(_prev == this, \"Should not be in a list\");\n+}\n+\n+template <typename T>\n+inline void XList<T>::verify_head() const {\n+  _head.verify_links();\n+}\n+\n+template <typename T>\n+inline void XList<T>::insert(XListNode<T>* before, XListNode<T>* node) {\n+  verify_head();\n+\n+  before->verify_links();\n+  node->verify_links_unlinked();\n+\n+  node->_prev = before;\n+  node->_next = before->_next;\n+  before->_next = node;\n+  node->_next->_prev = node;\n+\n+  before->verify_links_linked();\n+  node->verify_links_linked();\n+\n+  _size++;\n+}\n+\n+template <typename T>\n+inline XListNode<T>* XList<T>::cast_to_inner(T* elem) const {\n+  return &elem->_node;\n+}\n+\n+template <typename T>\n+inline T* XList<T>::cast_to_outer(XListNode<T>* node) const {\n+  return (T*)((uintptr_t)node - offset_of(T, _node));\n+}\n+\n+template <typename T>\n+inline XList<T>::XList() :\n+    _head(),\n+    _size(0) {\n+  verify_head();\n+}\n+\n+template <typename T>\n+inline size_t XList<T>::size() const {\n+  verify_head();\n+  return _size;\n+}\n+\n+template <typename T>\n+inline bool XList<T>::is_empty() const {\n+  return size() == 0;\n+}\n+\n+template <typename T>\n+inline T* XList<T>::first() const {\n+  return is_empty() ? NULL : cast_to_outer(_head._next);\n+}\n+\n+template <typename T>\n+inline T* XList<T>::last() const {\n+  return is_empty() ? NULL : cast_to_outer(_head._prev);\n+}\n+\n+template <typename T>\n+inline T* XList<T>::next(T* elem) const {\n+  verify_head();\n+\n+  XListNode<T>* const node = cast_to_inner(elem);\n+  node->verify_links_linked();\n+\n+  XListNode<T>* const next = node->_next;\n+  next->verify_links_linked();\n+\n+  return (next == &_head) ? NULL : cast_to_outer(next);\n+}\n+\n+template <typename T>\n+inline T* XList<T>::prev(T* elem) const {\n+  verify_head();\n+\n+  XListNode<T>* const node = cast_to_inner(elem);\n+  node->verify_links_linked();\n+\n+  XListNode<T>* const prev = node->_prev;\n+  prev->verify_links_linked();\n+\n+  return (prev == &_head) ? NULL : cast_to_outer(prev);\n+}\n+\n+template <typename T>\n+inline void XList<T>::insert_first(T* elem) {\n+  insert(&_head, cast_to_inner(elem));\n+}\n+\n+template <typename T>\n+inline void XList<T>::insert_last(T* elem) {\n+  insert(_head._prev, cast_to_inner(elem));\n+}\n+\n+template <typename T>\n+inline void XList<T>::insert_before(T* before, T* elem) {\n+  insert(cast_to_inner(before)->_prev, cast_to_inner(elem));\n+}\n+\n+template <typename T>\n+inline void XList<T>::insert_after(T* after, T* elem) {\n+  insert(cast_to_inner(after), cast_to_inner(elem));\n+}\n+\n+template <typename T>\n+inline void XList<T>::remove(T* elem) {\n+  verify_head();\n+\n+  XListNode<T>* const node = cast_to_inner(elem);\n+  node->verify_links_linked();\n+\n+  XListNode<T>* const next = node->_next;\n+  XListNode<T>* const prev = node->_prev;\n+  next->verify_links_linked();\n+  prev->verify_links_linked();\n+\n+  node->_next = prev->_next;\n+  node->_prev = next->_prev;\n+  node->verify_links_unlinked();\n+\n+  next->_prev = prev;\n+  prev->_next = next;\n+  next->verify_links();\n+  prev->verify_links();\n+\n+  _size--;\n+}\n+\n+template <typename T>\n+inline T* XList<T>::remove_first() {\n+  T* elem = first();\n+  if (elem != NULL) {\n+    remove(elem);\n+  }\n+\n+  return elem;\n+}\n+\n+template <typename T>\n+inline T* XList<T>::remove_last() {\n+  T* elem = last();\n+  if (elem != NULL) {\n+    remove(elem);\n+  }\n+\n+  return elem;\n+}\n+\n+template <typename T, bool Forward>\n+inline XListIteratorImpl<T, Forward>::XListIteratorImpl(const XList<T>* list) :\n+    _list(list),\n+    _next(Forward ? list->first() : list->last()) {}\n+\n+template <typename T, bool Forward>\n+inline bool XListIteratorImpl<T, Forward>::next(T** elem) {\n+  if (_next != NULL) {\n+    *elem = _next;\n+    _next = Forward ? _list->next(_next) : _list->prev(_next);\n+    return true;\n+  }\n+\n+  \/\/ No more elements\n+  return false;\n+}\n+\n+template <typename T, bool Forward>\n+inline XListRemoveIteratorImpl<T, Forward>::XListRemoveIteratorImpl(XList<T>* list) :\n+    _list(list) {}\n+\n+template <typename T, bool Forward>\n+inline bool XListRemoveIteratorImpl<T, Forward>::next(T** elem) {\n+  *elem = Forward ? _list->remove_first() : _list->remove_last();\n+  return *elem != NULL;\n+}\n+\n+#endif \/\/ SHARE_GC_X_XLIST_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xList.inline.hpp","additions":238,"deletions":0,"binary":false,"changes":238,"status":"added"},{"patch":"@@ -0,0 +1,133 @@\n+\/*\n+ * Copyright (c) 2015, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/x\/xHeap.inline.hpp\"\n+#include \"gc\/x\/xLiveMap.inline.hpp\"\n+#include \"gc\/x\/xStat.hpp\"\n+#include \"gc\/x\/xThread.inline.hpp\"\n+#include \"logging\/log.hpp\"\n+#include \"runtime\/atomic.hpp\"\n+#include \"utilities\/debug.hpp\"\n+#include \"utilities\/powerOfTwo.hpp\"\n+\n+static const XStatCounter XCounterMarkSeqNumResetContention(\"Contention\", \"Mark SeqNum Reset Contention\", XStatUnitOpsPerSecond);\n+static const XStatCounter XCounterMarkSegmentResetContention(\"Contention\", \"Mark Segment Reset Contention\", XStatUnitOpsPerSecond);\n+\n+static size_t bitmap_size(uint32_t size, size_t nsegments) {\n+  \/\/ We need at least one bit per segment\n+  return MAX2<size_t>(size, nsegments) * 2;\n+}\n+\n+XLiveMap::XLiveMap(uint32_t size) :\n+    _seqnum(0),\n+    _live_objects(0),\n+    _live_bytes(0),\n+    _segment_live_bits(0),\n+    _segment_claim_bits(0),\n+    _bitmap(bitmap_size(size, nsegments)),\n+    _segment_shift(exact_log2(segment_size())) {}\n+\n+void XLiveMap::reset(size_t index) {\n+  const uint32_t seqnum_initializing = (uint32_t)-1;\n+  bool contention = false;\n+\n+  \/\/ Multiple threads can enter here, make sure only one of them\n+  \/\/ resets the marking information while the others busy wait.\n+  for (uint32_t seqnum = Atomic::load_acquire(&_seqnum);\n+       seqnum != XGlobalSeqNum;\n+       seqnum = Atomic::load_acquire(&_seqnum)) {\n+    if ((seqnum != seqnum_initializing) &&\n+        (Atomic::cmpxchg(&_seqnum, seqnum, seqnum_initializing) == seqnum)) {\n+      \/\/ Reset marking information\n+      _live_bytes = 0;\n+      _live_objects = 0;\n+\n+      \/\/ Clear segment claimed\/live bits\n+      segment_live_bits().clear();\n+      segment_claim_bits().clear();\n+\n+      assert(_seqnum == seqnum_initializing, \"Invalid\");\n+\n+      \/\/ Make sure the newly reset marking information is ordered\n+      \/\/ before the update of the page seqnum, such that when the\n+      \/\/ up-to-date seqnum is load acquired, the bit maps will not\n+      \/\/ contain stale information.\n+      Atomic::release_store(&_seqnum, XGlobalSeqNum);\n+      break;\n+    }\n+\n+    \/\/ Mark reset contention\n+    if (!contention) {\n+      \/\/ Count contention once\n+      XStatInc(XCounterMarkSeqNumResetContention);\n+      contention = true;\n+\n+      log_trace(gc)(\"Mark seqnum reset contention, thread: \" PTR_FORMAT \" (%s), map: \" PTR_FORMAT \", bit: \" SIZE_FORMAT,\n+                    XThread::id(), XThread::name(), p2i(this), index);\n+    }\n+  }\n+}\n+\n+void XLiveMap::reset_segment(BitMap::idx_t segment) {\n+  bool contention = false;\n+\n+  if (!claim_segment(segment)) {\n+    \/\/ Already claimed, wait for live bit to be set\n+    while (!is_segment_live(segment)) {\n+      \/\/ Mark reset contention\n+      if (!contention) {\n+        \/\/ Count contention once\n+        XStatInc(XCounterMarkSegmentResetContention);\n+        contention = true;\n+\n+        log_trace(gc)(\"Mark segment reset contention, thread: \" PTR_FORMAT \" (%s), map: \" PTR_FORMAT \", segment: \" SIZE_FORMAT,\n+                      XThread::id(), XThread::name(), p2i(this), segment);\n+      }\n+    }\n+\n+    \/\/ Segment is live\n+    return;\n+  }\n+\n+  \/\/ Segment claimed, clear it\n+  const BitMap::idx_t start_index = segment_start(segment);\n+  const BitMap::idx_t end_index   = segment_end(segment);\n+  if (segment_size() \/ BitsPerWord >= 32) {\n+    _bitmap.clear_large_range(start_index, end_index);\n+  } else {\n+    _bitmap.clear_range(start_index, end_index);\n+  }\n+\n+  \/\/ Set live bit\n+  const bool success = set_segment_live(segment);\n+  assert(success, \"Should never fail\");\n+}\n+\n+void XLiveMap::resize(uint32_t size) {\n+  const size_t new_bitmap_size = bitmap_size(size, nsegments);\n+  if (_bitmap.size() != new_bitmap_size) {\n+    _bitmap.reinitialize(new_bitmap_size, false \/* clear *\/);\n+    _segment_shift = exact_log2(segment_size());\n+  }\n+}\n","filename":"src\/hotspot\/share\/gc\/x\/xLiveMap.cpp","additions":133,"deletions":0,"binary":false,"changes":133,"status":"added"},{"patch":"@@ -24,2 +24,2 @@\n-#ifndef SHARE_GC_Z_ZLIVEMAP_HPP\n-#define SHARE_GC_Z_ZLIVEMAP_HPP\n+#ifndef SHARE_GC_X_XLIVEMAP_HPP\n+#define SHARE_GC_X_XLIVEMAP_HPP\n@@ -27,1 +27,1 @@\n-#include \"gc\/z\/zBitMap.hpp\"\n+#include \"gc\/x\/xBitMap.hpp\"\n@@ -32,2 +32,2 @@\n-class ZLiveMap {\n-  friend class ZLiveMapTest;\n+class XLiveMap {\n+  friend class XLiveMapTest;\n@@ -43,1 +43,1 @@\n-  ZBitMap           _bitmap;\n+  XBitMap           _bitmap;\n@@ -72,1 +72,1 @@\n-  ZLiveMap(uint32_t size);\n+  XLiveMap(uint32_t size);\n@@ -90,1 +90,1 @@\n-#endif \/\/ SHARE_GC_Z_ZLIVEMAP_HPP\n+#endif \/\/ SHARE_GC_X_XLIVEMAP_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xLiveMap.hpp","additions":8,"deletions":8,"binary":false,"changes":16,"previous_filename":"src\/hotspot\/share\/gc\/z\/zLiveMap.hpp","status":"copied"},{"patch":"@@ -0,0 +1,175 @@\n+\/*\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XLIVEMAP_INLINE_HPP\n+#define SHARE_GC_X_XLIVEMAP_INLINE_HPP\n+\n+#include \"gc\/x\/xLiveMap.hpp\"\n+\n+#include \"gc\/x\/xBitMap.inline.hpp\"\n+#include \"gc\/x\/xMark.hpp\"\n+#include \"gc\/x\/xOop.inline.hpp\"\n+#include \"gc\/x\/xUtils.inline.hpp\"\n+#include \"runtime\/atomic.hpp\"\n+#include \"utilities\/bitMap.inline.hpp\"\n+#include \"utilities\/debug.hpp\"\n+\n+inline void XLiveMap::reset() {\n+  _seqnum = 0;\n+}\n+\n+inline bool XLiveMap::is_marked() const {\n+  return Atomic::load_acquire(&_seqnum) == XGlobalSeqNum;\n+}\n+\n+inline uint32_t XLiveMap::live_objects() const {\n+  assert(XGlobalPhase != XPhaseMark, \"Invalid phase\");\n+  return _live_objects;\n+}\n+\n+inline size_t XLiveMap::live_bytes() const {\n+  assert(XGlobalPhase != XPhaseMark, \"Invalid phase\");\n+  return _live_bytes;\n+}\n+\n+inline const BitMapView XLiveMap::segment_live_bits() const {\n+  return BitMapView(const_cast<BitMap::bm_word_t*>(&_segment_live_bits), nsegments);\n+}\n+\n+inline const BitMapView XLiveMap::segment_claim_bits() const {\n+  return BitMapView(const_cast<BitMap::bm_word_t*>(&_segment_claim_bits), nsegments);\n+}\n+\n+inline BitMapView XLiveMap::segment_live_bits() {\n+  return BitMapView(&_segment_live_bits, nsegments);\n+}\n+\n+inline BitMapView XLiveMap::segment_claim_bits() {\n+  return BitMapView(&_segment_claim_bits, nsegments);\n+}\n+\n+inline bool XLiveMap::is_segment_live(BitMap::idx_t segment) const {\n+  return segment_live_bits().par_at(segment);\n+}\n+\n+inline bool XLiveMap::set_segment_live(BitMap::idx_t segment) {\n+  return segment_live_bits().par_set_bit(segment, memory_order_release);\n+}\n+\n+inline bool XLiveMap::claim_segment(BitMap::idx_t segment) {\n+  return segment_claim_bits().par_set_bit(segment, memory_order_acq_rel);\n+}\n+\n+inline BitMap::idx_t XLiveMap::first_live_segment() const {\n+  return segment_live_bits().find_first_set_bit(0, nsegments);\n+}\n+\n+inline BitMap::idx_t XLiveMap::next_live_segment(BitMap::idx_t segment) const {\n+  return segment_live_bits().find_first_set_bit(segment + 1, nsegments);\n+}\n+\n+inline BitMap::idx_t XLiveMap::segment_size() const {\n+  return _bitmap.size() \/ nsegments;\n+}\n+\n+inline BitMap::idx_t XLiveMap::index_to_segment(BitMap::idx_t index) const {\n+  return index >> _segment_shift;\n+}\n+\n+inline bool XLiveMap::get(size_t index) const {\n+  BitMap::idx_t segment = index_to_segment(index);\n+  return is_marked() &&                               \/\/ Page is marked\n+         is_segment_live(segment) &&                  \/\/ Segment is marked\n+         _bitmap.par_at(index, memory_order_relaxed); \/\/ Object is marked\n+}\n+\n+inline bool XLiveMap::set(size_t index, bool finalizable, bool& inc_live) {\n+  if (!is_marked()) {\n+    \/\/ First object to be marked during this\n+    \/\/ cycle, reset marking information.\n+    reset(index);\n+  }\n+\n+  const BitMap::idx_t segment = index_to_segment(index);\n+  if (!is_segment_live(segment)) {\n+    \/\/ First object to be marked in this segment during\n+    \/\/ this cycle, reset segment bitmap.\n+    reset_segment(segment);\n+  }\n+\n+  return _bitmap.par_set_bit_pair(index, finalizable, inc_live);\n+}\n+\n+inline void XLiveMap::inc_live(uint32_t objects, size_t bytes) {\n+  Atomic::add(&_live_objects, objects);\n+  Atomic::add(&_live_bytes, bytes);\n+}\n+\n+inline BitMap::idx_t XLiveMap::segment_start(BitMap::idx_t segment) const {\n+  return segment_size() * segment;\n+}\n+\n+inline BitMap::idx_t XLiveMap::segment_end(BitMap::idx_t segment) const {\n+  return segment_start(segment) + segment_size();\n+}\n+\n+inline void XLiveMap::iterate_segment(ObjectClosure* cl, BitMap::idx_t segment, uintptr_t page_start, size_t page_object_alignment_shift) {\n+  assert(is_segment_live(segment), \"Must be\");\n+\n+  const BitMap::idx_t start_index = segment_start(segment);\n+  const BitMap::idx_t end_index   = segment_end(segment);\n+  BitMap::idx_t index = _bitmap.find_first_set_bit(start_index, end_index);\n+\n+  while (index < end_index) {\n+    \/\/ Calculate object address\n+    const uintptr_t addr = page_start + ((index \/ 2) << page_object_alignment_shift);\n+\n+    \/\/ Get the size of the object before calling the closure, which\n+    \/\/ might overwrite the object in case we are relocating in-place.\n+    const size_t size = XUtils::object_size(addr);\n+\n+    \/\/ Apply closure\n+    cl->do_object(XOop::from_address(addr));\n+\n+    \/\/ Find next bit after this object\n+    const uintptr_t next_addr = align_up(addr + size, 1 << page_object_alignment_shift);\n+    const BitMap::idx_t next_index = ((next_addr - page_start) >> page_object_alignment_shift) * 2;\n+    if (next_index >= end_index) {\n+      \/\/ End of live map\n+      break;\n+    }\n+\n+    index = _bitmap.find_first_set_bit(next_index, end_index);\n+  }\n+}\n+\n+inline void XLiveMap::iterate(ObjectClosure* cl, uintptr_t page_start, size_t page_object_alignment_shift) {\n+  if (is_marked()) {\n+    for (BitMap::idx_t segment = first_live_segment(); segment < nsegments; segment = next_live_segment(segment)) {\n+      \/\/ For each live segment\n+      iterate_segment(cl, segment, page_start, page_object_alignment_shift);\n+    }\n+  }\n+}\n+\n+#endif \/\/ SHARE_GC_X_XLIVEMAP_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xLiveMap.inline.hpp","additions":175,"deletions":0,"binary":false,"changes":175,"status":"added"},{"patch":"@@ -0,0 +1,79 @@\n+\/*\n+ * Copyright (c) 2015, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XLOCK_HPP\n+#define SHARE_GC_X_XLOCK_HPP\n+\n+#include \"memory\/allocation.hpp\"\n+#include \"runtime\/mutex.hpp\"\n+\n+class XLock {\n+private:\n+  PlatformMutex _lock;\n+\n+public:\n+  void lock();\n+  bool try_lock();\n+  void unlock();\n+};\n+\n+class XReentrantLock {\n+private:\n+  XLock            _lock;\n+  Thread* volatile _owner;\n+  uint64_t         _count;\n+\n+public:\n+  XReentrantLock();\n+\n+  void lock();\n+  void unlock();\n+\n+  bool is_owned() const;\n+};\n+\n+class XConditionLock {\n+private:\n+  PlatformMonitor _lock;\n+\n+public:\n+  void lock();\n+  bool try_lock();\n+  void unlock();\n+\n+  bool wait(uint64_t millis = 0);\n+  void notify();\n+  void notify_all();\n+};\n+\n+template <typename T>\n+class XLocker : public StackObj {\n+private:\n+  T* const _lock;\n+\n+public:\n+  XLocker(T* lock);\n+  ~XLocker();\n+};\n+\n+#endif \/\/ SHARE_GC_X_XLOCK_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xLock.hpp","additions":79,"deletions":0,"binary":false,"changes":79,"status":"added"},{"patch":"@@ -0,0 +1,120 @@\n+\/*\n+ * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XLOCK_INLINE_HPP\n+#define SHARE_GC_X_XLOCK_INLINE_HPP\n+\n+#include \"gc\/x\/xLock.hpp\"\n+\n+#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/javaThread.hpp\"\n+#include \"runtime\/os.inline.hpp\"\n+#include \"utilities\/debug.hpp\"\n+\n+inline void XLock::lock() {\n+  _lock.lock();\n+}\n+\n+inline bool XLock::try_lock() {\n+  return _lock.try_lock();\n+}\n+\n+inline void XLock::unlock() {\n+  _lock.unlock();\n+}\n+\n+inline XReentrantLock::XReentrantLock() :\n+    _lock(),\n+    _owner(NULL),\n+    _count(0) {}\n+\n+inline void XReentrantLock::lock() {\n+  Thread* const thread = Thread::current();\n+  Thread* const owner = Atomic::load(&_owner);\n+\n+  if (owner != thread) {\n+    _lock.lock();\n+    Atomic::store(&_owner, thread);\n+  }\n+\n+  _count++;\n+}\n+\n+inline void XReentrantLock::unlock() {\n+  assert(is_owned(), \"Invalid owner\");\n+  assert(_count > 0, \"Invalid count\");\n+\n+  _count--;\n+\n+  if (_count == 0) {\n+    Atomic::store(&_owner, (Thread*)NULL);\n+    _lock.unlock();\n+  }\n+}\n+\n+inline bool XReentrantLock::is_owned() const {\n+  Thread* const thread = Thread::current();\n+  Thread* const owner = Atomic::load(&_owner);\n+  return owner == thread;\n+}\n+\n+inline void XConditionLock::lock() {\n+  _lock.lock();\n+}\n+\n+inline bool XConditionLock::try_lock() {\n+  return _lock.try_lock();\n+}\n+\n+inline void XConditionLock::unlock() {\n+  _lock.unlock();\n+}\n+\n+inline bool XConditionLock::wait(uint64_t millis) {\n+  return _lock.wait(millis) == OS_OK;\n+}\n+\n+inline void XConditionLock::notify() {\n+  _lock.notify();\n+}\n+\n+inline void XConditionLock::notify_all() {\n+  _lock.notify_all();\n+}\n+\n+template <typename T>\n+inline XLocker<T>::XLocker(T* lock) :\n+    _lock(lock) {\n+  if (_lock != NULL) {\n+    _lock->lock();\n+  }\n+}\n+\n+template <typename T>\n+inline XLocker<T>::~XLocker() {\n+  if (_lock != NULL) {\n+    _lock->unlock();\n+  }\n+}\n+\n+#endif \/\/ SHARE_GC_X_XLOCK_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xLock.inline.hpp","additions":120,"deletions":0,"binary":false,"changes":120,"status":"added"},{"patch":"@@ -0,0 +1,875 @@\n+\/*\n+ * Copyright (c) 2015, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"classfile\/classLoaderData.hpp\"\n+#include \"classfile\/classLoaderDataGraph.hpp\"\n+#include \"classfile\/javaClasses.inline.hpp\"\n+#include \"code\/nmethod.hpp\"\n+#include \"gc\/shared\/continuationGCSupport.inline.hpp\"\n+#include \"gc\/shared\/gc_globals.hpp\"\n+#include \"gc\/shared\/stringdedup\/stringDedup.hpp\"\n+#include \"gc\/shared\/suspendibleThreadSet.hpp\"\n+#include \"gc\/x\/xAbort.inline.hpp\"\n+#include \"gc\/x\/xBarrier.inline.hpp\"\n+#include \"gc\/x\/xHeap.inline.hpp\"\n+#include \"gc\/x\/xLock.inline.hpp\"\n+#include \"gc\/x\/xMark.inline.hpp\"\n+#include \"gc\/x\/xMarkCache.inline.hpp\"\n+#include \"gc\/x\/xMarkContext.inline.hpp\"\n+#include \"gc\/x\/xMarkStack.inline.hpp\"\n+#include \"gc\/x\/xMarkTerminate.inline.hpp\"\n+#include \"gc\/x\/xNMethod.hpp\"\n+#include \"gc\/x\/xOop.inline.hpp\"\n+#include \"gc\/x\/xPage.hpp\"\n+#include \"gc\/x\/xPageTable.inline.hpp\"\n+#include \"gc\/x\/xRootsIterator.hpp\"\n+#include \"gc\/x\/xStackWatermark.hpp\"\n+#include \"gc\/x\/xStat.hpp\"\n+#include \"gc\/x\/xTask.hpp\"\n+#include \"gc\/x\/xThread.inline.hpp\"\n+#include \"gc\/x\/xThreadLocalAllocBuffer.hpp\"\n+#include \"gc\/x\/xUtils.inline.hpp\"\n+#include \"gc\/x\/xWorkers.hpp\"\n+#include \"logging\/log.hpp\"\n+#include \"memory\/iterator.inline.hpp\"\n+#include \"oops\/objArrayOop.inline.hpp\"\n+#include \"oops\/oop.inline.hpp\"\n+#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/continuation.hpp\"\n+#include \"runtime\/handshake.hpp\"\n+#include \"runtime\/javaThread.hpp\"\n+#include \"runtime\/prefetch.inline.hpp\"\n+#include \"runtime\/safepointMechanism.hpp\"\n+#include \"runtime\/stackWatermark.hpp\"\n+#include \"runtime\/stackWatermarkSet.inline.hpp\"\n+#include \"runtime\/threads.hpp\"\n+#include \"utilities\/align.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include \"utilities\/powerOfTwo.hpp\"\n+#include \"utilities\/ticks.hpp\"\n+\n+static const XStatSubPhase XSubPhaseConcurrentMark(\"Concurrent Mark\");\n+static const XStatSubPhase XSubPhaseConcurrentMarkTryFlush(\"Concurrent Mark Try Flush\");\n+static const XStatSubPhase XSubPhaseConcurrentMarkTryTerminate(\"Concurrent Mark Try Terminate\");\n+static const XStatSubPhase XSubPhaseMarkTryComplete(\"Pause Mark Try Complete\");\n+\n+XMark::XMark(XWorkers* workers, XPageTable* page_table) :\n+    _workers(workers),\n+    _page_table(page_table),\n+    _allocator(),\n+    _stripes(),\n+    _terminate(),\n+    _work_terminateflush(true),\n+    _work_nproactiveflush(0),\n+    _work_nterminateflush(0),\n+    _nproactiveflush(0),\n+    _nterminateflush(0),\n+    _ntrycomplete(0),\n+    _ncontinue(0),\n+    _nworkers(0) {}\n+\n+bool XMark::is_initialized() const {\n+  return _allocator.is_initialized();\n+}\n+\n+size_t XMark::calculate_nstripes(uint nworkers) const {\n+  \/\/ Calculate the number of stripes from the number of workers we use,\n+  \/\/ where the number of stripes must be a power of two and we want to\n+  \/\/ have at least one worker per stripe.\n+  const size_t nstripes = round_down_power_of_2(nworkers);\n+  return MIN2(nstripes, XMarkStripesMax);\n+}\n+\n+void XMark::start() {\n+  \/\/ Verification\n+  if (ZVerifyMarking) {\n+    verify_all_stacks_empty();\n+  }\n+\n+  \/\/ Increment global sequence number to invalidate\n+  \/\/ marking information for all pages.\n+  XGlobalSeqNum++;\n+\n+  \/\/ Note that we start a marking cycle.\n+  \/\/ Unlike other GCs, the color switch implicitly changes the nmethods\n+  \/\/ to be armed, and the thread-local disarm values are lazily updated\n+  \/\/ when JavaThreads wake up from safepoints.\n+  CodeCache::on_gc_marking_cycle_start();\n+\n+  \/\/ Reset flush\/continue counters\n+  _nproactiveflush = 0;\n+  _nterminateflush = 0;\n+  _ntrycomplete = 0;\n+  _ncontinue = 0;\n+\n+  \/\/ Set number of workers to use\n+  _nworkers = _workers->active_workers();\n+\n+  \/\/ Set number of mark stripes to use, based on number\n+  \/\/ of workers we will use in the concurrent mark phase.\n+  const size_t nstripes = calculate_nstripes(_nworkers);\n+  _stripes.set_nstripes(nstripes);\n+\n+  \/\/ Update statistics\n+  XStatMark::set_at_mark_start(nstripes);\n+\n+  \/\/ Print worker\/stripe distribution\n+  LogTarget(Debug, gc, marking) log;\n+  if (log.is_enabled()) {\n+    log.print(\"Mark Worker\/Stripe Distribution\");\n+    for (uint worker_id = 0; worker_id < _nworkers; worker_id++) {\n+      const XMarkStripe* const stripe = _stripes.stripe_for_worker(_nworkers, worker_id);\n+      const size_t stripe_id = _stripes.stripe_id(stripe);\n+      log.print(\"  Worker %u(%u) -> Stripe \" SIZE_FORMAT \"(\" SIZE_FORMAT \")\",\n+                worker_id, _nworkers, stripe_id, nstripes);\n+    }\n+  }\n+}\n+\n+void XMark::prepare_work() {\n+  assert(_nworkers == _workers->active_workers(), \"Invalid number of workers\");\n+\n+  \/\/ Set number of active workers\n+  _terminate.reset(_nworkers);\n+\n+  \/\/ Reset flush counters\n+  _work_nproactiveflush = _work_nterminateflush = 0;\n+  _work_terminateflush = true;\n+}\n+\n+void XMark::finish_work() {\n+  \/\/ Accumulate proactive\/terminate flush counters\n+  _nproactiveflush += _work_nproactiveflush;\n+  _nterminateflush += _work_nterminateflush;\n+}\n+\n+bool XMark::is_array(uintptr_t addr) const {\n+  return XOop::from_address(addr)->is_objArray();\n+}\n+\n+void XMark::push_partial_array(uintptr_t addr, size_t size, bool finalizable) {\n+  assert(is_aligned(addr, XMarkPartialArrayMinSize), \"Address misaligned\");\n+  XMarkThreadLocalStacks* const stacks = XThreadLocalData::stacks(Thread::current());\n+  XMarkStripe* const stripe = _stripes.stripe_for_addr(addr);\n+  const uintptr_t offset = XAddress::offset(addr) >> XMarkPartialArrayMinSizeShift;\n+  const uintptr_t length = size \/ oopSize;\n+  const XMarkStackEntry entry(offset, length, finalizable);\n+\n+  log_develop_trace(gc, marking)(\"Array push partial: \" PTR_FORMAT \" (\" SIZE_FORMAT \"), stripe: \" SIZE_FORMAT,\n+                                 addr, size, _stripes.stripe_id(stripe));\n+\n+  stacks->push(&_allocator, &_stripes, stripe, entry, false \/* publish *\/);\n+}\n+\n+void XMark::follow_small_array(uintptr_t addr, size_t size, bool finalizable) {\n+  assert(size <= XMarkPartialArrayMinSize, \"Too large, should be split\");\n+  const size_t length = size \/ oopSize;\n+\n+  log_develop_trace(gc, marking)(\"Array follow small: \" PTR_FORMAT \" (\" SIZE_FORMAT \")\", addr, size);\n+\n+  XBarrier::mark_barrier_on_oop_array((oop*)addr, length, finalizable);\n+}\n+\n+void XMark::follow_large_array(uintptr_t addr, size_t size, bool finalizable) {\n+  assert(size <= (size_t)arrayOopDesc::max_array_length(T_OBJECT) * oopSize, \"Too large\");\n+  assert(size > XMarkPartialArrayMinSize, \"Too small, should not be split\");\n+  const uintptr_t start = addr;\n+  const uintptr_t end = start + size;\n+\n+  \/\/ Calculate the aligned middle start\/end\/size, where the middle start\n+  \/\/ should always be greater than the start (hence the +1 below) to make\n+  \/\/ sure we always do some follow work, not just split the array into pieces.\n+  const uintptr_t middle_start = align_up(start + 1, XMarkPartialArrayMinSize);\n+  const size_t    middle_size = align_down(end - middle_start, XMarkPartialArrayMinSize);\n+  const uintptr_t middle_end = middle_start + middle_size;\n+\n+  log_develop_trace(gc, marking)(\"Array follow large: \" PTR_FORMAT \"-\" PTR_FORMAT\" (\" SIZE_FORMAT \"), \"\n+                                 \"middle: \" PTR_FORMAT \"-\" PTR_FORMAT \" (\" SIZE_FORMAT \")\",\n+                                 start, end, size, middle_start, middle_end, middle_size);\n+\n+  \/\/ Push unaligned trailing part\n+  if (end > middle_end) {\n+    const uintptr_t trailing_addr = middle_end;\n+    const size_t trailing_size = end - middle_end;\n+    push_partial_array(trailing_addr, trailing_size, finalizable);\n+  }\n+\n+  \/\/ Push aligned middle part(s)\n+  uintptr_t partial_addr = middle_end;\n+  while (partial_addr > middle_start) {\n+    const size_t parts = 2;\n+    const size_t partial_size = align_up((partial_addr - middle_start) \/ parts, XMarkPartialArrayMinSize);\n+    partial_addr -= partial_size;\n+    push_partial_array(partial_addr, partial_size, finalizable);\n+  }\n+\n+  \/\/ Follow leading part\n+  assert(start < middle_start, \"Miscalculated middle start\");\n+  const uintptr_t leading_addr = start;\n+  const size_t leading_size = middle_start - start;\n+  follow_small_array(leading_addr, leading_size, finalizable);\n+}\n+\n+void XMark::follow_array(uintptr_t addr, size_t size, bool finalizable) {\n+  if (size <= XMarkPartialArrayMinSize) {\n+    follow_small_array(addr, size, finalizable);\n+  } else {\n+    follow_large_array(addr, size, finalizable);\n+  }\n+}\n+\n+void XMark::follow_partial_array(XMarkStackEntry entry, bool finalizable) {\n+  const uintptr_t addr = XAddress::good(entry.partial_array_offset() << XMarkPartialArrayMinSizeShift);\n+  const size_t size = entry.partial_array_length() * oopSize;\n+\n+  follow_array(addr, size, finalizable);\n+}\n+\n+template <bool finalizable>\n+class XMarkBarrierOopClosure : public ClaimMetadataVisitingOopIterateClosure {\n+public:\n+  XMarkBarrierOopClosure() :\n+      ClaimMetadataVisitingOopIterateClosure(finalizable\n+                                                 ? ClassLoaderData::_claim_finalizable\n+                                                 : ClassLoaderData::_claim_strong,\n+                                             finalizable\n+                                                 ? NULL\n+                                                 : XHeap::heap()->reference_discoverer()) {}\n+\n+  virtual void do_oop(oop* p) {\n+    XBarrier::mark_barrier_on_oop_field(p, finalizable);\n+  }\n+\n+  virtual void do_oop(narrowOop* p) {\n+    ShouldNotReachHere();\n+  }\n+\n+  virtual void do_nmethod(nmethod* nm) {\n+    assert(!finalizable, \"Can't handle finalizable marking of nmethods\");\n+    nm->run_nmethod_entry_barrier();\n+  }\n+};\n+\n+void XMark::follow_array_object(objArrayOop obj, bool finalizable) {\n+  if (finalizable) {\n+    XMarkBarrierOopClosure<true \/* finalizable *\/> cl;\n+    cl.do_klass(obj->klass());\n+  } else {\n+    XMarkBarrierOopClosure<false \/* finalizable *\/> cl;\n+    cl.do_klass(obj->klass());\n+  }\n+\n+  const uintptr_t addr = (uintptr_t)obj->base();\n+  const size_t size = (size_t)obj->length() * oopSize;\n+\n+  follow_array(addr, size, finalizable);\n+}\n+\n+void XMark::follow_object(oop obj, bool finalizable) {\n+  if (ContinuationGCSupport::relativize_stack_chunk(obj)) {\n+    \/\/ Loom doesn't support mixing of finalizable marking and strong marking of\n+    \/\/ stack chunks. See: RelativizeDerivedOopClosure.\n+    XMarkBarrierOopClosure<false \/* finalizable *\/> cl;\n+    obj->oop_iterate(&cl);\n+    return;\n+  }\n+\n+  if (finalizable) {\n+    XMarkBarrierOopClosure<true \/* finalizable *\/> cl;\n+    obj->oop_iterate(&cl);\n+  } else {\n+    XMarkBarrierOopClosure<false \/* finalizable *\/> cl;\n+    obj->oop_iterate(&cl);\n+  }\n+}\n+\n+static void try_deduplicate(XMarkContext* context, oop obj) {\n+  if (!StringDedup::is_enabled()) {\n+    \/\/ Not enabled\n+    return;\n+  }\n+\n+  if (!java_lang_String::is_instance(obj)) {\n+    \/\/ Not a String object\n+    return;\n+  }\n+\n+  if (java_lang_String::test_and_set_deduplication_requested(obj)) {\n+    \/\/ Already requested deduplication\n+    return;\n+  }\n+\n+  \/\/ Request deduplication\n+  context->string_dedup_requests()->add(obj);\n+}\n+\n+void XMark::mark_and_follow(XMarkContext* context, XMarkStackEntry entry) {\n+  \/\/ Decode flags\n+  const bool finalizable = entry.finalizable();\n+  const bool partial_array = entry.partial_array();\n+\n+  if (partial_array) {\n+    follow_partial_array(entry, finalizable);\n+    return;\n+  }\n+\n+  \/\/ Decode object address and additional flags\n+  const uintptr_t addr = entry.object_address();\n+  const bool mark = entry.mark();\n+  bool inc_live = entry.inc_live();\n+  const bool follow = entry.follow();\n+\n+  XPage* const page = _page_table->get(addr);\n+  assert(page->is_relocatable(), \"Invalid page state\");\n+\n+  \/\/ Mark\n+  if (mark && !page->mark_object(addr, finalizable, inc_live)) {\n+    \/\/ Already marked\n+    return;\n+  }\n+\n+  \/\/ Increment live\n+  if (inc_live) {\n+    \/\/ Update live objects\/bytes for page. We use the aligned object\n+    \/\/ size since that is the actual number of bytes used on the page\n+    \/\/ and alignment paddings can never be reclaimed.\n+    const size_t size = XUtils::object_size(addr);\n+    const size_t aligned_size = align_up(size, page->object_alignment());\n+    context->cache()->inc_live(page, aligned_size);\n+  }\n+\n+  \/\/ Follow\n+  if (follow) {\n+    if (is_array(addr)) {\n+      follow_array_object(objArrayOop(XOop::from_address(addr)), finalizable);\n+    } else {\n+      const oop obj = XOop::from_address(addr);\n+      follow_object(obj, finalizable);\n+\n+      \/\/ Try deduplicate\n+      try_deduplicate(context, obj);\n+    }\n+  }\n+}\n+\n+template <typename T>\n+bool XMark::drain(XMarkContext* context, T* timeout) {\n+  XMarkStripe* const stripe = context->stripe();\n+  XMarkThreadLocalStacks* const stacks = context->stacks();\n+  XMarkStackEntry entry;\n+\n+  \/\/ Drain stripe stacks\n+  while (stacks->pop(&_allocator, &_stripes, stripe, entry)) {\n+    mark_and_follow(context, entry);\n+\n+    \/\/ Check timeout\n+    if (timeout->has_expired()) {\n+      \/\/ Timeout\n+      return false;\n+    }\n+  }\n+\n+  \/\/ Success\n+  return !timeout->has_expired();\n+}\n+\n+bool XMark::try_steal_local(XMarkContext* context) {\n+  XMarkStripe* const stripe = context->stripe();\n+  XMarkThreadLocalStacks* const stacks = context->stacks();\n+\n+  \/\/ Try to steal a local stack from another stripe\n+  for (XMarkStripe* victim_stripe = _stripes.stripe_next(stripe);\n+       victim_stripe != stripe;\n+       victim_stripe = _stripes.stripe_next(victim_stripe)) {\n+    XMarkStack* const stack = stacks->steal(&_stripes, victim_stripe);\n+    if (stack != NULL) {\n+      \/\/ Success, install the stolen stack\n+      stacks->install(&_stripes, stripe, stack);\n+      return true;\n+    }\n+  }\n+\n+  \/\/ Nothing to steal\n+  return false;\n+}\n+\n+bool XMark::try_steal_global(XMarkContext* context) {\n+  XMarkStripe* const stripe = context->stripe();\n+  XMarkThreadLocalStacks* const stacks = context->stacks();\n+\n+  \/\/ Try to steal a stack from another stripe\n+  for (XMarkStripe* victim_stripe = _stripes.stripe_next(stripe);\n+       victim_stripe != stripe;\n+       victim_stripe = _stripes.stripe_next(victim_stripe)) {\n+    XMarkStack* const stack = victim_stripe->steal_stack();\n+    if (stack != NULL) {\n+      \/\/ Success, install the stolen stack\n+      stacks->install(&_stripes, stripe, stack);\n+      return true;\n+    }\n+  }\n+\n+  \/\/ Nothing to steal\n+  return false;\n+}\n+\n+bool XMark::try_steal(XMarkContext* context) {\n+  return try_steal_local(context) || try_steal_global(context);\n+}\n+\n+void XMark::idle() const {\n+  os::naked_short_sleep(1);\n+}\n+\n+class XMarkFlushAndFreeStacksClosure : public HandshakeClosure {\n+private:\n+  XMark* const _mark;\n+  bool         _flushed;\n+\n+public:\n+  XMarkFlushAndFreeStacksClosure(XMark* mark) :\n+      HandshakeClosure(\"XMarkFlushAndFreeStacks\"),\n+      _mark(mark),\n+      _flushed(false) {}\n+\n+  void do_thread(Thread* thread) {\n+    if (_mark->flush_and_free(thread)) {\n+      _flushed = true;\n+    }\n+  }\n+\n+  bool flushed() const {\n+    return _flushed;\n+  }\n+};\n+\n+bool XMark::flush(bool at_safepoint) {\n+  XMarkFlushAndFreeStacksClosure cl(this);\n+  if (at_safepoint) {\n+    Threads::threads_do(&cl);\n+  } else {\n+    Handshake::execute(&cl);\n+  }\n+\n+  \/\/ Returns true if more work is available\n+  return cl.flushed() || !_stripes.is_empty();\n+}\n+\n+bool XMark::try_flush(volatile size_t* nflush) {\n+  Atomic::inc(nflush);\n+\n+  XStatTimer timer(XSubPhaseConcurrentMarkTryFlush);\n+  return flush(false \/* at_safepoint *\/);\n+}\n+\n+bool XMark::try_proactive_flush() {\n+  \/\/ Only do proactive flushes from worker 0\n+  if (XThread::worker_id() != 0) {\n+    return false;\n+  }\n+\n+  if (Atomic::load(&_work_nproactiveflush) == XMarkProactiveFlushMax ||\n+      Atomic::load(&_work_nterminateflush) != 0) {\n+    \/\/ Limit reached or we're trying to terminate\n+    return false;\n+  }\n+\n+  return try_flush(&_work_nproactiveflush);\n+}\n+\n+bool XMark::try_terminate() {\n+  XStatTimer timer(XSubPhaseConcurrentMarkTryTerminate);\n+\n+  if (_terminate.enter_stage0()) {\n+    \/\/ Last thread entered stage 0, flush\n+    if (Atomic::load(&_work_terminateflush) &&\n+        Atomic::load(&_work_nterminateflush) != XMarkTerminateFlushMax) {\n+      \/\/ Exit stage 0 to allow other threads to continue marking\n+      _terminate.exit_stage0();\n+\n+      \/\/ Flush before termination\n+      if (!try_flush(&_work_nterminateflush)) {\n+        \/\/ No more work available, skip further flush attempts\n+        Atomic::store(&_work_terminateflush, false);\n+      }\n+\n+      \/\/ Don't terminate, regardless of whether we successfully\n+      \/\/ flushed out more work or not. We've already exited\n+      \/\/ termination stage 0, to allow other threads to continue\n+      \/\/ marking, so this thread has to return false and also\n+      \/\/ make another round of attempted marking.\n+      return false;\n+    }\n+  }\n+\n+  for (;;) {\n+    if (_terminate.enter_stage1()) {\n+      \/\/ Last thread entered stage 1, terminate\n+      return true;\n+    }\n+\n+    \/\/ Idle to give the other threads\n+    \/\/ a chance to enter termination.\n+    idle();\n+\n+    if (!_terminate.try_exit_stage1()) {\n+      \/\/ All workers in stage 1, terminate\n+      return true;\n+    }\n+\n+    if (_terminate.try_exit_stage0()) {\n+      \/\/ More work available, don't terminate\n+      return false;\n+    }\n+  }\n+}\n+\n+class XMarkNoTimeout : public StackObj {\n+public:\n+  bool has_expired() {\n+    \/\/ No timeout, but check for signal to abort\n+    return XAbort::should_abort();\n+  }\n+};\n+\n+void XMark::work_without_timeout(XMarkContext* context) {\n+  XStatTimer timer(XSubPhaseConcurrentMark);\n+  XMarkNoTimeout no_timeout;\n+\n+  for (;;) {\n+    if (!drain(context, &no_timeout)) {\n+      \/\/ Abort\n+      break;\n+    }\n+\n+    if (try_steal(context)) {\n+      \/\/ Stole work\n+      continue;\n+    }\n+\n+    if (try_proactive_flush()) {\n+      \/\/ Work available\n+      continue;\n+    }\n+\n+    if (try_terminate()) {\n+      \/\/ Terminate\n+      break;\n+    }\n+  }\n+}\n+\n+class XMarkTimeout : public StackObj {\n+private:\n+  const Ticks    _start;\n+  const uint64_t _timeout;\n+  const uint64_t _check_interval;\n+  uint64_t       _check_at;\n+  uint64_t       _check_count;\n+  bool           _expired;\n+\n+public:\n+  XMarkTimeout(uint64_t timeout_in_micros) :\n+      _start(Ticks::now()),\n+      _timeout(_start.value() + TimeHelper::micros_to_counter(timeout_in_micros)),\n+      _check_interval(200),\n+      _check_at(_check_interval),\n+      _check_count(0),\n+      _expired(false) {}\n+\n+  ~XMarkTimeout() {\n+    const Tickspan duration = Ticks::now() - _start;\n+    log_debug(gc, marking)(\"Mark With Timeout (%s): %s, \" UINT64_FORMAT \" oops, %.3fms\",\n+                           XThread::name(), _expired ? \"Expired\" : \"Completed\",\n+                           _check_count, TimeHelper::counter_to_millis(duration.value()));\n+  }\n+\n+  bool has_expired() {\n+    if (++_check_count == _check_at) {\n+      _check_at += _check_interval;\n+      if ((uint64_t)Ticks::now().value() >= _timeout) {\n+        \/\/ Timeout\n+        _expired = true;\n+      }\n+    }\n+\n+    return _expired;\n+  }\n+};\n+\n+void XMark::work_with_timeout(XMarkContext* context, uint64_t timeout_in_micros) {\n+  XStatTimer timer(XSubPhaseMarkTryComplete);\n+  XMarkTimeout timeout(timeout_in_micros);\n+\n+  for (;;) {\n+    if (!drain(context, &timeout)) {\n+      \/\/ Timed out\n+      break;\n+    }\n+\n+    if (try_steal(context)) {\n+      \/\/ Stole work\n+      continue;\n+    }\n+\n+    \/\/ Terminate\n+    break;\n+  }\n+}\n+\n+void XMark::work(uint64_t timeout_in_micros) {\n+  XMarkStripe* const stripe = _stripes.stripe_for_worker(_nworkers, XThread::worker_id());\n+  XMarkThreadLocalStacks* const stacks = XThreadLocalData::stacks(Thread::current());\n+  XMarkContext context(_stripes.nstripes(), stripe, stacks);\n+\n+  if (timeout_in_micros == 0) {\n+    work_without_timeout(&context);\n+  } else {\n+    work_with_timeout(&context, timeout_in_micros);\n+  }\n+\n+  \/\/ Flush and publish stacks\n+  stacks->flush(&_allocator, &_stripes);\n+\n+  \/\/ Free remaining stacks\n+  stacks->free(&_allocator);\n+}\n+\n+class XMarkOopClosure : public OopClosure {\n+  virtual void do_oop(oop* p) {\n+    XBarrier::mark_barrier_on_oop_field(p, false \/* finalizable *\/);\n+  }\n+\n+  virtual void do_oop(narrowOop* p) {\n+    ShouldNotReachHere();\n+  }\n+};\n+\n+class XMarkThreadClosure : public ThreadClosure {\n+private:\n+  OopClosure* const _cl;\n+\n+public:\n+  XMarkThreadClosure(OopClosure* cl) :\n+      _cl(cl) {\n+    XThreadLocalAllocBuffer::reset_statistics();\n+  }\n+  ~XMarkThreadClosure() {\n+    XThreadLocalAllocBuffer::publish_statistics();\n+  }\n+  virtual void do_thread(Thread* thread) {\n+    JavaThread* const jt = JavaThread::cast(thread);\n+    StackWatermarkSet::finish_processing(jt, _cl, StackWatermarkKind::gc);\n+    XThreadLocalAllocBuffer::update_stats(jt);\n+  }\n+};\n+\n+class XMarkNMethodClosure : public NMethodClosure {\n+private:\n+  OopClosure* const _cl;\n+\n+public:\n+  XMarkNMethodClosure(OopClosure* cl) :\n+      _cl(cl) {}\n+\n+  virtual void do_nmethod(nmethod* nm) {\n+    XLocker<XReentrantLock> locker(XNMethod::lock_for_nmethod(nm));\n+    if (XNMethod::is_armed(nm)) {\n+      XNMethod::nmethod_oops_do_inner(nm, _cl);\n+\n+      \/\/ CodeCache unloading support\n+      nm->mark_as_maybe_on_stack();\n+\n+      XNMethod::disarm(nm);\n+    }\n+  }\n+};\n+\n+typedef ClaimingCLDToOopClosure<ClassLoaderData::_claim_strong> XMarkCLDClosure;\n+\n+class XMarkRootsTask : public XTask {\n+private:\n+  XMark* const               _mark;\n+  SuspendibleThreadSetJoiner _sts_joiner;\n+  XRootsIterator             _roots;\n+\n+  XMarkOopClosure            _cl;\n+  XMarkCLDClosure            _cld_cl;\n+  XMarkThreadClosure         _thread_cl;\n+  XMarkNMethodClosure        _nm_cl;\n+\n+public:\n+  XMarkRootsTask(XMark* mark) :\n+      XTask(\"XMarkRootsTask\"),\n+      _mark(mark),\n+      _sts_joiner(),\n+      _roots(ClassLoaderData::_claim_strong),\n+      _cl(),\n+      _cld_cl(&_cl),\n+      _thread_cl(&_cl),\n+      _nm_cl(&_cl) {\n+    ClassLoaderDataGraph_lock->lock();\n+  }\n+\n+  ~XMarkRootsTask() {\n+    ClassLoaderDataGraph_lock->unlock();\n+  }\n+\n+  virtual void work() {\n+    _roots.apply(&_cl,\n+                 &_cld_cl,\n+                 &_thread_cl,\n+                 &_nm_cl);\n+\n+    \/\/ Flush and free worker stacks. Needed here since\n+    \/\/ the set of workers executing during root scanning\n+    \/\/ can be different from the set of workers executing\n+    \/\/ during mark.\n+    _mark->flush_and_free();\n+  }\n+};\n+\n+class XMarkTask : public XTask {\n+private:\n+  XMark* const   _mark;\n+  const uint64_t _timeout_in_micros;\n+\n+public:\n+  XMarkTask(XMark* mark, uint64_t timeout_in_micros = 0) :\n+      XTask(\"XMarkTask\"),\n+      _mark(mark),\n+      _timeout_in_micros(timeout_in_micros) {\n+    _mark->prepare_work();\n+  }\n+\n+  ~XMarkTask() {\n+    _mark->finish_work();\n+  }\n+\n+  virtual void work() {\n+    _mark->work(_timeout_in_micros);\n+  }\n+};\n+\n+void XMark::mark(bool initial) {\n+  if (initial) {\n+    XMarkRootsTask task(this);\n+    _workers->run(&task);\n+  }\n+\n+  XMarkTask task(this);\n+  _workers->run(&task);\n+}\n+\n+bool XMark::try_complete() {\n+  _ntrycomplete++;\n+\n+  \/\/ Use nconcurrent number of worker threads to maintain the\n+  \/\/ worker\/stripe distribution used during concurrent mark.\n+  XMarkTask task(this, XMarkCompleteTimeout);\n+  _workers->run(&task);\n+\n+  \/\/ Successful if all stripes are empty\n+  return _stripes.is_empty();\n+}\n+\n+bool XMark::try_end() {\n+  \/\/ Flush all mark stacks\n+  if (!flush(true \/* at_safepoint *\/)) {\n+    \/\/ Mark completed\n+    return true;\n+  }\n+\n+  \/\/ Try complete marking by doing a limited\n+  \/\/ amount of mark work in this phase.\n+  return try_complete();\n+}\n+\n+bool XMark::end() {\n+  \/\/ Try end marking\n+  if (!try_end()) {\n+    \/\/ Mark not completed\n+    _ncontinue++;\n+    return false;\n+  }\n+\n+  \/\/ Verification\n+  if (ZVerifyMarking) {\n+    verify_all_stacks_empty();\n+  }\n+\n+  \/\/ Update statistics\n+  XStatMark::set_at_mark_end(_nproactiveflush, _nterminateflush, _ntrycomplete, _ncontinue);\n+\n+  \/\/ Note that we finished a marking cycle.\n+  \/\/ Unlike other GCs, we do not arm the nmethods\n+  \/\/ when marking terminates.\n+  CodeCache::on_gc_marking_cycle_finish();\n+\n+  \/\/ Mark completed\n+  return true;\n+}\n+\n+void XMark::free() {\n+  \/\/ Free any unused mark stack space\n+  _allocator.free();\n+\n+  \/\/ Update statistics\n+  XStatMark::set_at_mark_free(_allocator.size());\n+}\n+\n+void XMark::flush_and_free() {\n+  Thread* const thread = Thread::current();\n+  flush_and_free(thread);\n+}\n+\n+bool XMark::flush_and_free(Thread* thread) {\n+  XMarkThreadLocalStacks* const stacks = XThreadLocalData::stacks(thread);\n+  const bool flushed = stacks->flush(&_allocator, &_stripes);\n+  stacks->free(&_allocator);\n+  return flushed;\n+}\n+\n+class XVerifyMarkStacksEmptyClosure : public ThreadClosure {\n+private:\n+  const XMarkStripeSet* const _stripes;\n+\n+public:\n+  XVerifyMarkStacksEmptyClosure(const XMarkStripeSet* stripes) :\n+      _stripes(stripes) {}\n+\n+  void do_thread(Thread* thread) {\n+    XMarkThreadLocalStacks* const stacks = XThreadLocalData::stacks(thread);\n+    guarantee(stacks->is_empty(_stripes), \"Should be empty\");\n+  }\n+};\n+\n+void XMark::verify_all_stacks_empty() const {\n+  \/\/ Verify thread stacks\n+  XVerifyMarkStacksEmptyClosure cl(&_stripes);\n+  Threads::threads_do(&cl);\n+\n+  \/\/ Verify stripe stacks\n+  guarantee(_stripes.is_empty(), \"Should be empty\");\n+}\n","filename":"src\/hotspot\/share\/gc\/x\/xMark.cpp","additions":875,"deletions":0,"binary":false,"changes":875,"status":"added"},{"patch":"@@ -0,0 +1,106 @@\n+\/*\n+ * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XMARK_HPP\n+#define SHARE_GC_X_XMARK_HPP\n+\n+#include \"gc\/x\/xMarkStack.hpp\"\n+#include \"gc\/x\/xMarkStackAllocator.hpp\"\n+#include \"gc\/x\/xMarkStackEntry.hpp\"\n+#include \"gc\/x\/xMarkTerminate.hpp\"\n+#include \"oops\/oopsHierarchy.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+class Thread;\n+class XMarkContext;\n+class XPageTable;\n+class XWorkers;\n+\n+class XMark {\n+  friend class XMarkTask;\n+\n+private:\n+  XWorkers* const     _workers;\n+  XPageTable* const   _page_table;\n+  XMarkStackAllocator _allocator;\n+  XMarkStripeSet      _stripes;\n+  XMarkTerminate      _terminate;\n+  volatile bool       _work_terminateflush;\n+  volatile size_t     _work_nproactiveflush;\n+  volatile size_t     _work_nterminateflush;\n+  size_t              _nproactiveflush;\n+  size_t              _nterminateflush;\n+  size_t              _ntrycomplete;\n+  size_t              _ncontinue;\n+  uint                _nworkers;\n+\n+  size_t calculate_nstripes(uint nworkers) const;\n+\n+  bool is_array(uintptr_t addr) const;\n+  void push_partial_array(uintptr_t addr, size_t size, bool finalizable);\n+  void follow_small_array(uintptr_t addr, size_t size, bool finalizable);\n+  void follow_large_array(uintptr_t addr, size_t size, bool finalizable);\n+  void follow_array(uintptr_t addr, size_t size, bool finalizable);\n+  void follow_partial_array(XMarkStackEntry entry, bool finalizable);\n+  void follow_array_object(objArrayOop obj, bool finalizable);\n+  void follow_object(oop obj, bool finalizable);\n+  void mark_and_follow(XMarkContext* context, XMarkStackEntry entry);\n+\n+  template <typename T> bool drain(XMarkContext* context, T* timeout);\n+  bool try_steal_local(XMarkContext* context);\n+  bool try_steal_global(XMarkContext* context);\n+  bool try_steal(XMarkContext* context);\n+  void idle() const;\n+  bool flush(bool at_safepoint);\n+  bool try_proactive_flush();\n+  bool try_flush(volatile size_t* nflush);\n+  bool try_terminate();\n+  bool try_complete();\n+  bool try_end();\n+\n+  void prepare_work();\n+  void finish_work();\n+\n+  void work_without_timeout(XMarkContext* context);\n+  void work_with_timeout(XMarkContext* context, uint64_t timeout_in_micros);\n+  void work(uint64_t timeout_in_micros);\n+\n+  void verify_all_stacks_empty() const;\n+\n+public:\n+  XMark(XWorkers* workers, XPageTable* page_table);\n+\n+  bool is_initialized() const;\n+\n+  template <bool gc_thread, bool follow, bool finalizable, bool publish> void mark_object(uintptr_t addr);\n+\n+  void start();\n+  void mark(bool initial);\n+  bool end();\n+  void free();\n+\n+  void flush_and_free();\n+  bool flush_and_free(Thread* thread);\n+};\n+\n+#endif \/\/ SHARE_GC_X_XMARK_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xMark.hpp","additions":106,"deletions":0,"binary":false,"changes":106,"status":"added"},{"patch":"@@ -0,0 +1,80 @@\n+\/*\n+ * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XMARK_INLINE_HPP\n+#define SHARE_GC_X_XMARK_INLINE_HPP\n+\n+#include \"gc\/x\/xMark.hpp\"\n+\n+#include \"gc\/x\/xAddress.inline.hpp\"\n+#include \"gc\/x\/xMarkStack.inline.hpp\"\n+#include \"gc\/x\/xPage.inline.hpp\"\n+#include \"gc\/x\/xPageTable.inline.hpp\"\n+#include \"gc\/x\/xThreadLocalData.hpp\"\n+#include \"runtime\/javaThread.hpp\"\n+#include \"utilities\/debug.hpp\"\n+\n+\/\/ Marking before pushing helps reduce mark stack memory usage. However,\n+\/\/ we only mark before pushing in GC threads to avoid burdening Java threads\n+\/\/ with writing to, and potentially first having to clear, mark bitmaps.\n+\/\/\n+\/\/ It's also worth noting that while marking an object can be done at any\n+\/\/ time in the marking phase, following an object can only be done after\n+\/\/ root processing has called ClassLoaderDataGraph::clear_claimed_marks(),\n+\/\/ since it otherwise would interact badly with claiming of CLDs.\n+\n+template <bool gc_thread, bool follow, bool finalizable, bool publish>\n+inline void XMark::mark_object(uintptr_t addr) {\n+  assert(XAddress::is_marked(addr), \"Should be marked\");\n+\n+  XPage* const page = _page_table->get(addr);\n+  if (page->is_allocating()) {\n+    \/\/ Already implicitly marked\n+    return;\n+  }\n+\n+  const bool mark_before_push = gc_thread;\n+  bool inc_live = false;\n+\n+  if (mark_before_push) {\n+    \/\/ Try mark object\n+    if (!page->mark_object(addr, finalizable, inc_live)) {\n+      \/\/ Already marked\n+      return;\n+    }\n+  } else {\n+    \/\/ Don't push if already marked\n+    if (page->is_object_marked<finalizable>(addr)) {\n+      \/\/ Already marked\n+      return;\n+    }\n+  }\n+\n+  \/\/ Push\n+  XMarkThreadLocalStacks* const stacks = XThreadLocalData::stacks(Thread::current());\n+  XMarkStripe* const stripe = _stripes.stripe_for_addr(addr);\n+  XMarkStackEntry entry(addr, !mark_before_push, inc_live, follow, finalizable);\n+  stacks->push(&_allocator, &_stripes, stripe, entry, publish);\n+}\n+\n+#endif \/\/ SHARE_GC_X_XMARK_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xMark.inline.hpp","additions":80,"deletions":0,"binary":false,"changes":80,"status":"added"},{"patch":"@@ -0,0 +1,43 @@\n+\/*\n+ * Copyright (c) 2016, 2017, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/x\/xMarkCache.inline.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include \"utilities\/powerOfTwo.hpp\"\n+\n+XMarkCacheEntry::XMarkCacheEntry() :\n+    _page(NULL),\n+    _objects(0),\n+    _bytes(0) {}\n+\n+XMarkCache::XMarkCache(size_t nstripes) :\n+    _shift(XMarkStripeShift + exact_log2(nstripes)) {}\n+\n+XMarkCache::~XMarkCache() {\n+  \/\/ Evict all entries\n+  for (size_t i = 0; i < XMarkCacheSize; i++) {\n+    _cache[i].evict();\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/x\/xMarkCache.cpp","additions":43,"deletions":0,"binary":false,"changes":43,"status":"added"},{"patch":"@@ -0,0 +1,57 @@\n+\/*\n+ * Copyright (c) 2016, 2017, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XMARKCACHE_HPP\n+#define SHARE_GC_X_XMARKCACHE_HPP\n+\n+#include \"gc\/x\/xGlobals.hpp\"\n+#include \"memory\/allocation.hpp\"\n+\n+class XPage;\n+\n+class XMarkCacheEntry {\n+private:\n+  XPage*   _page;\n+  uint32_t _objects;\n+  size_t   _bytes;\n+\n+public:\n+  XMarkCacheEntry();\n+\n+  void inc_live(XPage* page, size_t bytes);\n+  void evict();\n+};\n+\n+class XMarkCache : public StackObj {\n+private:\n+  const size_t    _shift;\n+  XMarkCacheEntry _cache[XMarkCacheSize];\n+\n+public:\n+  XMarkCache(size_t nstripes);\n+  ~XMarkCache();\n+\n+  void inc_live(XPage* page, size_t bytes);\n+};\n+\n+#endif \/\/ SHARE_GC_X_XMARKCACHE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xMarkCache.hpp","additions":57,"deletions":0,"binary":false,"changes":57,"status":"added"},{"patch":"@@ -0,0 +1,59 @@\n+\/*\n+ * Copyright (c) 2016, 2017, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XMARKCACHE_INLINE_HPP\n+#define SHARE_GC_X_XMARKCACHE_INLINE_HPP\n+\n+#include \"gc\/x\/xMarkCache.hpp\"\n+\n+#include \"gc\/x\/xPage.inline.hpp\"\n+\n+inline void XMarkCacheEntry::inc_live(XPage* page, size_t bytes) {\n+  if (_page == page) {\n+    \/\/ Cache hit\n+    _objects++;\n+    _bytes += bytes;\n+  } else {\n+    \/\/ Cache miss\n+    evict();\n+    _page = page;\n+    _objects = 1;\n+    _bytes = bytes;\n+  }\n+}\n+\n+inline void XMarkCacheEntry::evict() {\n+  if (_page != NULL) {\n+    \/\/ Write cached data out to page\n+    _page->inc_live(_objects, _bytes);\n+    _page = NULL;\n+  }\n+}\n+\n+inline void XMarkCache::inc_live(XPage* page, size_t bytes) {\n+  const size_t mask = XMarkCacheSize - 1;\n+  const size_t index = (page->start() >> _shift) & mask;\n+  _cache[index].inc_live(page, bytes);\n+}\n+\n+#endif \/\/ SHARE_GC_X_XMARKCACHE_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xMarkCache.inline.hpp","additions":59,"deletions":0,"binary":false,"changes":59,"status":"added"},{"patch":"@@ -0,0 +1,52 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XMARKCONTEXT_HPP\n+#define SHARE_GC_X_XMARKCONTEXT_HPP\n+\n+#include \"gc\/x\/xMarkCache.hpp\"\n+#include \"gc\/shared\/stringdedup\/stringDedup.hpp\"\n+#include \"memory\/allocation.hpp\"\n+\n+class XMarkStripe;\n+class XMarkThreadLocalStacks;\n+\n+class XMarkContext : public StackObj {\n+private:\n+  XMarkCache                    _cache;\n+  XMarkStripe* const            _stripe;\n+  XMarkThreadLocalStacks* const _stacks;\n+  StringDedup::Requests         _string_dedup_requests;\n+\n+public:\n+  XMarkContext(size_t nstripes,\n+               XMarkStripe* stripe,\n+               XMarkThreadLocalStacks* stacks);\n+\n+  XMarkCache* cache();\n+  XMarkStripe* stripe();\n+  XMarkThreadLocalStacks* stacks();\n+  StringDedup::Requests* string_dedup_requests();\n+};\n+\n+#endif \/\/ SHARE_GC_X_XMARKCONTEXT_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xMarkContext.hpp","additions":52,"deletions":0,"binary":false,"changes":52,"status":"added"},{"patch":"@@ -0,0 +1,53 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XMARKCONTEXT_INLINE_HPP\n+#define SHARE_GC_X_XMARKCONTEXT_INLINE_HPP\n+\n+#include \"gc\/x\/xMarkContext.hpp\"\n+\n+inline XMarkContext::XMarkContext(size_t nstripes,\n+                                  XMarkStripe* stripe,\n+                                  XMarkThreadLocalStacks* stacks) :\n+    _cache(nstripes),\n+    _stripe(stripe),\n+    _stacks(stacks),\n+    _string_dedup_requests() {}\n+\n+inline XMarkCache* XMarkContext::cache() {\n+  return &_cache;\n+}\n+\n+inline XMarkStripe* XMarkContext::stripe() {\n+  return _stripe;\n+}\n+\n+inline XMarkThreadLocalStacks* XMarkContext::stacks() {\n+  return _stacks;\n+}\n+\n+inline StringDedup::Requests* XMarkContext::string_dedup_requests() {\n+  return &_string_dedup_requests;\n+}\n+\n+#endif \/\/ SHARE_GC_X_XMARKCACHE_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xMarkContext.inline.hpp","additions":53,"deletions":0,"binary":false,"changes":53,"status":"added"},{"patch":"@@ -0,0 +1,226 @@\n+\/*\n+ * Copyright (c) 2016, 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/x\/xMarkStack.inline.hpp\"\n+#include \"gc\/x\/xMarkStackAllocator.hpp\"\n+#include \"logging\/log.hpp\"\n+#include \"utilities\/debug.hpp\"\n+#include \"utilities\/powerOfTwo.hpp\"\n+\n+XMarkStripe::XMarkStripe() :\n+    _published(),\n+    _overflowed() {}\n+\n+XMarkStripeSet::XMarkStripeSet() :\n+    _nstripes(0),\n+    _nstripes_mask(0),\n+    _stripes() {}\n+\n+void XMarkStripeSet::set_nstripes(size_t nstripes) {\n+  assert(is_power_of_2(nstripes), \"Must be a power of two\");\n+  assert(is_power_of_2(XMarkStripesMax), \"Must be a power of two\");\n+  assert(nstripes >= 1, \"Invalid number of stripes\");\n+  assert(nstripes <= XMarkStripesMax, \"Invalid number of stripes\");\n+\n+  _nstripes = nstripes;\n+  _nstripes_mask = nstripes - 1;\n+\n+  log_debug(gc, marking)(\"Using \" SIZE_FORMAT \" mark stripes\", _nstripes);\n+}\n+\n+bool XMarkStripeSet::is_empty() const {\n+  for (size_t i = 0; i < _nstripes; i++) {\n+    if (!_stripes[i].is_empty()) {\n+      return false;\n+    }\n+  }\n+\n+  return true;\n+}\n+\n+XMarkStripe* XMarkStripeSet::stripe_for_worker(uint nworkers, uint worker_id) {\n+  const size_t spillover_limit = (nworkers \/ _nstripes) * _nstripes;\n+  size_t index;\n+\n+  if (worker_id < spillover_limit) {\n+    \/\/ Not a spillover worker, use natural stripe\n+    index = worker_id & _nstripes_mask;\n+  } else {\n+    \/\/ Distribute spillover workers evenly across stripes\n+    const size_t spillover_nworkers = nworkers - spillover_limit;\n+    const size_t spillover_worker_id = worker_id - spillover_limit;\n+    const double spillover_chunk = (double)_nstripes \/ (double)spillover_nworkers;\n+    index = spillover_worker_id * spillover_chunk;\n+  }\n+\n+  assert(index < _nstripes, \"Invalid index\");\n+  return &_stripes[index];\n+}\n+\n+XMarkThreadLocalStacks::XMarkThreadLocalStacks() :\n+    _magazine(NULL) {\n+  for (size_t i = 0; i < XMarkStripesMax; i++) {\n+    _stacks[i] = NULL;\n+  }\n+}\n+\n+bool XMarkThreadLocalStacks::is_empty(const XMarkStripeSet* stripes) const {\n+  for (size_t i = 0; i < stripes->nstripes(); i++) {\n+    XMarkStack* const stack = _stacks[i];\n+    if (stack != NULL) {\n+      return false;\n+    }\n+  }\n+\n+  return true;\n+}\n+\n+XMarkStack* XMarkThreadLocalStacks::allocate_stack(XMarkStackAllocator* allocator) {\n+  if (_magazine == NULL) {\n+    \/\/ Allocate new magazine\n+    _magazine = allocator->alloc_magazine();\n+    if (_magazine == NULL) {\n+      return NULL;\n+    }\n+  }\n+\n+  XMarkStack* stack = NULL;\n+\n+  if (!_magazine->pop(stack)) {\n+    \/\/ Magazine is empty, convert magazine into a new stack\n+    _magazine->~XMarkStackMagazine();\n+    stack = new ((void*)_magazine) XMarkStack();\n+    _magazine = NULL;\n+  }\n+\n+  return stack;\n+}\n+\n+void XMarkThreadLocalStacks::free_stack(XMarkStackAllocator* allocator, XMarkStack* stack) {\n+  for (;;) {\n+    if (_magazine == NULL) {\n+      \/\/ Convert stack into a new magazine\n+      stack->~XMarkStack();\n+      _magazine = new ((void*)stack) XMarkStackMagazine();\n+      return;\n+    }\n+\n+    if (_magazine->push(stack)) {\n+      \/\/ Success\n+      return;\n+    }\n+\n+    \/\/ Free and uninstall full magazine\n+    allocator->free_magazine(_magazine);\n+    _magazine = NULL;\n+  }\n+}\n+\n+bool XMarkThreadLocalStacks::push_slow(XMarkStackAllocator* allocator,\n+                                       XMarkStripe* stripe,\n+                                       XMarkStack** stackp,\n+                                       XMarkStackEntry entry,\n+                                       bool publish) {\n+  XMarkStack* stack = *stackp;\n+\n+  for (;;) {\n+    if (stack == NULL) {\n+      \/\/ Allocate and install new stack\n+      *stackp = stack = allocate_stack(allocator);\n+      if (stack == NULL) {\n+        \/\/ Out of mark stack memory\n+        return false;\n+      }\n+    }\n+\n+    if (stack->push(entry)) {\n+      \/\/ Success\n+      return true;\n+    }\n+\n+    \/\/ Publish\/Overflow and uninstall stack\n+    stripe->publish_stack(stack, publish);\n+    *stackp = stack = NULL;\n+  }\n+}\n+\n+bool XMarkThreadLocalStacks::pop_slow(XMarkStackAllocator* allocator,\n+                                      XMarkStripe* stripe,\n+                                      XMarkStack** stackp,\n+                                      XMarkStackEntry& entry) {\n+  XMarkStack* stack = *stackp;\n+\n+  for (;;) {\n+    if (stack == NULL) {\n+      \/\/ Try steal and install stack\n+      *stackp = stack = stripe->steal_stack();\n+      if (stack == NULL) {\n+        \/\/ Nothing to steal\n+        return false;\n+      }\n+    }\n+\n+    if (stack->pop(entry)) {\n+      \/\/ Success\n+      return true;\n+    }\n+\n+    \/\/ Free and uninstall stack\n+    free_stack(allocator, stack);\n+    *stackp = stack = NULL;\n+  }\n+}\n+\n+bool XMarkThreadLocalStacks::flush(XMarkStackAllocator* allocator, XMarkStripeSet* stripes) {\n+  bool flushed = false;\n+\n+  \/\/ Flush all stacks\n+  for (size_t i = 0; i < stripes->nstripes(); i++) {\n+    XMarkStripe* const stripe = stripes->stripe_at(i);\n+    XMarkStack** const stackp = &_stacks[i];\n+    XMarkStack* const stack = *stackp;\n+    if (stack == NULL) {\n+      continue;\n+    }\n+\n+    \/\/ Free\/Publish and uninstall stack\n+    if (stack->is_empty()) {\n+      free_stack(allocator, stack);\n+    } else {\n+      stripe->publish_stack(stack);\n+      flushed = true;\n+    }\n+    *stackp = NULL;\n+  }\n+\n+  return flushed;\n+}\n+\n+void XMarkThreadLocalStacks::free(XMarkStackAllocator* allocator) {\n+  \/\/ Free and uninstall magazine\n+  if (_magazine != NULL) {\n+    allocator->free_magazine(_magazine);\n+    _magazine = NULL;\n+  }\n+}\n","filename":"src\/hotspot\/share\/gc\/x\/xMarkStack.cpp","additions":226,"deletions":0,"binary":false,"changes":226,"status":"added"},{"patch":"@@ -0,0 +1,164 @@\n+\/*\n+ * Copyright (c) 2016, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XMARKSTACK_HPP\n+#define SHARE_GC_X_XMARKSTACK_HPP\n+\n+#include \"gc\/x\/xGlobals.hpp\"\n+#include \"gc\/x\/xMarkStackEntry.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+template <typename T, size_t S>\n+class XStack {\n+private:\n+  size_t        _top;\n+  XStack<T, S>* _next;\n+  T             _slots[S];\n+\n+  bool is_full() const;\n+\n+public:\n+  XStack();\n+\n+  bool is_empty() const;\n+\n+  bool push(T value);\n+  bool pop(T& value);\n+\n+  XStack<T, S>* next() const;\n+  XStack<T, S>** next_addr();\n+};\n+\n+template <typename T>\n+class XStackList {\n+private:\n+  T* volatile _head;\n+\n+  T* encode_versioned_pointer(const T* stack, uint32_t version) const;\n+  void decode_versioned_pointer(const T* vstack, T** stack, uint32_t* version) const;\n+\n+public:\n+  XStackList();\n+\n+  bool is_empty() const;\n+\n+  void push(T* stack);\n+  T* pop();\n+\n+  void clear();\n+};\n+\n+using XMarkStack = XStack<XMarkStackEntry, XMarkStackSlots>;\n+using XMarkStackList = XStackList<XMarkStack>;\n+using XMarkStackMagazine = XStack<XMarkStack*, XMarkStackMagazineSlots>;\n+using XMarkStackMagazineList = XStackList<XMarkStackMagazine>;\n+\n+static_assert(sizeof(XMarkStack) == XMarkStackSize, \"XMarkStack size mismatch\");\n+static_assert(sizeof(XMarkStackMagazine) <= XMarkStackSize, \"XMarkStackMagazine size too large\");\n+\n+class XMarkStripe {\n+private:\n+  XCACHE_ALIGNED XMarkStackList _published;\n+  XCACHE_ALIGNED XMarkStackList _overflowed;\n+\n+public:\n+  XMarkStripe();\n+\n+  bool is_empty() const;\n+\n+  void publish_stack(XMarkStack* stack, bool publish = true);\n+  XMarkStack* steal_stack();\n+};\n+\n+class XMarkStripeSet {\n+private:\n+  size_t      _nstripes;\n+  size_t      _nstripes_mask;\n+  XMarkStripe _stripes[XMarkStripesMax];\n+\n+public:\n+  XMarkStripeSet();\n+\n+  size_t nstripes() const;\n+  void set_nstripes(size_t nstripes);\n+\n+  bool is_empty() const;\n+\n+  size_t stripe_id(const XMarkStripe* stripe) const;\n+  XMarkStripe* stripe_at(size_t index);\n+  XMarkStripe* stripe_next(XMarkStripe* stripe);\n+  XMarkStripe* stripe_for_worker(uint nworkers, uint worker_id);\n+  XMarkStripe* stripe_for_addr(uintptr_t addr);\n+};\n+\n+class XMarkStackAllocator;\n+\n+class XMarkThreadLocalStacks {\n+private:\n+  XMarkStackMagazine* _magazine;\n+  XMarkStack*         _stacks[XMarkStripesMax];\n+\n+  XMarkStack* allocate_stack(XMarkStackAllocator* allocator);\n+  void free_stack(XMarkStackAllocator* allocator, XMarkStack* stack);\n+\n+  bool push_slow(XMarkStackAllocator* allocator,\n+                 XMarkStripe* stripe,\n+                 XMarkStack** stackp,\n+                 XMarkStackEntry entry,\n+                 bool publish);\n+\n+  bool pop_slow(XMarkStackAllocator* allocator,\n+                XMarkStripe* stripe,\n+                XMarkStack** stackp,\n+                XMarkStackEntry& entry);\n+\n+public:\n+  XMarkThreadLocalStacks();\n+\n+  bool is_empty(const XMarkStripeSet* stripes) const;\n+\n+  void install(XMarkStripeSet* stripes,\n+               XMarkStripe* stripe,\n+               XMarkStack* stack);\n+\n+  XMarkStack* steal(XMarkStripeSet* stripes,\n+                    XMarkStripe* stripe);\n+\n+  bool push(XMarkStackAllocator* allocator,\n+            XMarkStripeSet* stripes,\n+            XMarkStripe* stripe,\n+            XMarkStackEntry entry,\n+            bool publish);\n+\n+  bool pop(XMarkStackAllocator* allocator,\n+           XMarkStripeSet* stripes,\n+           XMarkStripe* stripe,\n+           XMarkStackEntry& entry);\n+\n+  bool flush(XMarkStackAllocator* allocator,\n+             XMarkStripeSet* stripes);\n+\n+  void free(XMarkStackAllocator* allocator);\n+};\n+\n+#endif \/\/ SHARE_GC_X_XMARKSTACK_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xMarkStack.hpp","additions":164,"deletions":0,"binary":false,"changes":164,"status":"added"},{"patch":"@@ -0,0 +1,266 @@\n+\/*\n+ * Copyright (c) 2016, 2017, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XMARKSTACK_INLINE_HPP\n+#define SHARE_GC_X_XMARKSTACK_INLINE_HPP\n+\n+#include \"gc\/x\/xMarkStack.hpp\"\n+\n+#include \"utilities\/debug.hpp\"\n+#include \"runtime\/atomic.hpp\"\n+\n+template <typename T, size_t S>\n+inline XStack<T, S>::XStack() :\n+    _top(0),\n+    _next(NULL) {}\n+\n+template <typename T, size_t S>\n+inline bool XStack<T, S>::is_empty() const {\n+  return _top == 0;\n+}\n+\n+template <typename T, size_t S>\n+inline bool XStack<T, S>::is_full() const {\n+  return _top == S;\n+}\n+\n+template <typename T, size_t S>\n+inline bool XStack<T, S>::push(T value) {\n+  if (is_full()) {\n+    return false;\n+  }\n+\n+  _slots[_top++] = value;\n+  return true;\n+}\n+\n+template <typename T, size_t S>\n+inline bool XStack<T, S>::pop(T& value) {\n+  if (is_empty()) {\n+    return false;\n+  }\n+\n+  value = _slots[--_top];\n+  return true;\n+}\n+\n+template <typename T, size_t S>\n+inline XStack<T, S>* XStack<T, S>::next() const {\n+  return _next;\n+}\n+\n+template <typename T, size_t S>\n+inline XStack<T, S>** XStack<T, S>::next_addr() {\n+  return &_next;\n+}\n+\n+template <typename T>\n+inline XStackList<T>::XStackList() :\n+    _head(encode_versioned_pointer(NULL, 0)) {}\n+\n+template <typename T>\n+inline T* XStackList<T>::encode_versioned_pointer(const T* stack, uint32_t version) const {\n+  uint64_t addr;\n+\n+  if (stack == NULL) {\n+    addr = (uint32_t)-1;\n+  } else {\n+    addr = ((uint64_t)stack - XMarkStackSpaceStart) >> XMarkStackSizeShift;\n+  }\n+\n+  return (T*)((addr << 32) | (uint64_t)version);\n+}\n+\n+template <typename T>\n+inline void XStackList<T>::decode_versioned_pointer(const T* vstack, T** stack, uint32_t* version) const {\n+  const uint64_t addr = (uint64_t)vstack >> 32;\n+\n+  if (addr == (uint32_t)-1) {\n+    *stack = NULL;\n+  } else {\n+    *stack = (T*)((addr << XMarkStackSizeShift) + XMarkStackSpaceStart);\n+  }\n+\n+  *version = (uint32_t)(uint64_t)vstack;\n+}\n+\n+template <typename T>\n+inline bool XStackList<T>::is_empty() const {\n+  const T* vstack = _head;\n+  T* stack = NULL;\n+  uint32_t version = 0;\n+\n+  decode_versioned_pointer(vstack, &stack, &version);\n+  return stack == NULL;\n+}\n+\n+template <typename T>\n+inline void XStackList<T>::push(T* stack) {\n+  T* vstack = _head;\n+  uint32_t version = 0;\n+\n+  for (;;) {\n+    decode_versioned_pointer(vstack, stack->next_addr(), &version);\n+    T* const new_vstack = encode_versioned_pointer(stack, version + 1);\n+    T* const prev_vstack = Atomic::cmpxchg(&_head, vstack, new_vstack);\n+    if (prev_vstack == vstack) {\n+      \/\/ Success\n+      break;\n+    }\n+\n+    \/\/ Retry\n+    vstack = prev_vstack;\n+  }\n+}\n+\n+template <typename T>\n+inline T* XStackList<T>::pop() {\n+  T* vstack = _head;\n+  T* stack = NULL;\n+  uint32_t version = 0;\n+\n+  for (;;) {\n+    decode_versioned_pointer(vstack, &stack, &version);\n+    if (stack == NULL) {\n+      return NULL;\n+    }\n+\n+    T* const new_vstack = encode_versioned_pointer(stack->next(), version + 1);\n+    T* const prev_vstack = Atomic::cmpxchg(&_head, vstack, new_vstack);\n+    if (prev_vstack == vstack) {\n+      \/\/ Success\n+      return stack;\n+    }\n+\n+    \/\/ Retry\n+    vstack = prev_vstack;\n+  }\n+}\n+\n+template <typename T>\n+inline void XStackList<T>::clear() {\n+  _head = encode_versioned_pointer(NULL, 0);\n+}\n+\n+inline bool XMarkStripe::is_empty() const {\n+  return _published.is_empty() && _overflowed.is_empty();\n+}\n+\n+inline void XMarkStripe::publish_stack(XMarkStack* stack, bool publish) {\n+  \/\/ A stack is published either on the published list or the overflowed\n+  \/\/ list. The published list is used by mutators publishing stacks for GC\n+  \/\/ workers to work on, while the overflowed list is used by GC workers\n+  \/\/ to publish stacks that overflowed. The intention here is to avoid\n+  \/\/ contention between mutators and GC workers as much as possible, while\n+  \/\/ still allowing GC workers to help out and steal work from each other.\n+  if (publish) {\n+    _published.push(stack);\n+  } else {\n+    _overflowed.push(stack);\n+  }\n+}\n+\n+inline XMarkStack* XMarkStripe::steal_stack() {\n+  \/\/ Steal overflowed stacks first, then published stacks\n+  XMarkStack* const stack = _overflowed.pop();\n+  if (stack != NULL) {\n+    return stack;\n+  }\n+\n+  return _published.pop();\n+}\n+\n+inline size_t XMarkStripeSet::nstripes() const {\n+  return _nstripes;\n+}\n+\n+inline size_t XMarkStripeSet::stripe_id(const XMarkStripe* stripe) const {\n+  const size_t index = ((uintptr_t)stripe - (uintptr_t)_stripes) \/ sizeof(XMarkStripe);\n+  assert(index < _nstripes, \"Invalid index\");\n+  return index;\n+}\n+\n+inline XMarkStripe* XMarkStripeSet::stripe_at(size_t index) {\n+  assert(index < _nstripes, \"Invalid index\");\n+  return &_stripes[index];\n+}\n+\n+inline XMarkStripe* XMarkStripeSet::stripe_next(XMarkStripe* stripe) {\n+  const size_t index = (stripe_id(stripe) + 1) & _nstripes_mask;\n+  assert(index < _nstripes, \"Invalid index\");\n+  return &_stripes[index];\n+}\n+\n+inline XMarkStripe* XMarkStripeSet::stripe_for_addr(uintptr_t addr) {\n+  const size_t index = (addr >> XMarkStripeShift) & _nstripes_mask;\n+  assert(index < _nstripes, \"Invalid index\");\n+  return &_stripes[index];\n+}\n+\n+inline void XMarkThreadLocalStacks::install(XMarkStripeSet* stripes,\n+                                            XMarkStripe* stripe,\n+                                            XMarkStack* stack) {\n+  XMarkStack** const stackp = &_stacks[stripes->stripe_id(stripe)];\n+  assert(*stackp == NULL, \"Should be empty\");\n+  *stackp = stack;\n+}\n+\n+inline XMarkStack* XMarkThreadLocalStacks::steal(XMarkStripeSet* stripes,\n+                                                 XMarkStripe* stripe) {\n+  XMarkStack** const stackp = &_stacks[stripes->stripe_id(stripe)];\n+  XMarkStack* const stack = *stackp;\n+  if (stack != NULL) {\n+    *stackp = NULL;\n+  }\n+\n+  return stack;\n+}\n+\n+inline bool XMarkThreadLocalStacks::push(XMarkStackAllocator* allocator,\n+                                         XMarkStripeSet* stripes,\n+                                         XMarkStripe* stripe,\n+                                         XMarkStackEntry entry,\n+                                         bool publish) {\n+  XMarkStack** const stackp = &_stacks[stripes->stripe_id(stripe)];\n+  XMarkStack* const stack = *stackp;\n+  if (stack != NULL && stack->push(entry)) {\n+    return true;\n+  }\n+\n+  return push_slow(allocator, stripe, stackp, entry, publish);\n+}\n+\n+inline bool XMarkThreadLocalStacks::pop(XMarkStackAllocator* allocator,\n+                                        XMarkStripeSet* stripes,\n+                                        XMarkStripe* stripe,\n+                                        XMarkStackEntry& entry) {\n+  XMarkStack** const stackp = &_stacks[stripes->stripe_id(stripe)];\n+  XMarkStack* const stack = *stackp;\n+  if (stack != NULL && stack->pop(entry)) {\n+    return true;\n+  }\n+\n+  return pop_slow(allocator, stripe, stackp, entry);\n+}\n+\n+#endif \/\/ SHARE_GC_X_XMARKSTACK_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xMarkStack.inline.hpp","additions":266,"deletions":0,"binary":false,"changes":266,"status":"added"},{"patch":"@@ -0,0 +1,221 @@\n+\/*\n+ * Copyright (c) 2016, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/shared\/gcLogPrecious.hpp\"\n+#include \"gc\/shared\/gc_globals.hpp\"\n+#include \"gc\/x\/xLock.inline.hpp\"\n+#include \"gc\/x\/xMarkStack.inline.hpp\"\n+#include \"gc\/x\/xMarkStackAllocator.hpp\"\n+#include \"logging\/log.hpp\"\n+#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/os.hpp\"\n+#include \"utilities\/debug.hpp\"\n+\n+uintptr_t XMarkStackSpaceStart;\n+\n+XMarkStackSpace::XMarkStackSpace() :\n+    _expand_lock(),\n+    _start(0),\n+    _top(0),\n+    _end(0) {\n+  assert(ZMarkStackSpaceLimit >= XMarkStackSpaceExpandSize, \"ZMarkStackSpaceLimit too small\");\n+\n+  \/\/ Reserve address space\n+  const size_t size = ZMarkStackSpaceLimit;\n+  const uintptr_t addr = (uintptr_t)os::reserve_memory(size, !ExecMem, mtGC);\n+  if (addr == 0) {\n+    log_error_pd(gc, marking)(\"Failed to reserve address space for mark stacks\");\n+    return;\n+  }\n+\n+  \/\/ Successfully initialized\n+  _start = _top = _end = addr;\n+\n+  \/\/ Register mark stack space start\n+  XMarkStackSpaceStart = _start;\n+\n+  \/\/ Prime space\n+  _end += expand_space();\n+}\n+\n+bool XMarkStackSpace::is_initialized() const {\n+  return _start != 0;\n+}\n+\n+size_t XMarkStackSpace::size() const {\n+  return _end - _start;\n+}\n+\n+size_t XMarkStackSpace::used() const {\n+  return _top - _start;\n+}\n+\n+size_t XMarkStackSpace::expand_space() {\n+  const size_t expand_size = XMarkStackSpaceExpandSize;\n+  const size_t old_size = size();\n+  const size_t new_size = old_size + expand_size;\n+\n+  if (new_size > ZMarkStackSpaceLimit) {\n+    \/\/ Expansion limit reached. This is a fatal error since we\n+    \/\/ currently can't recover from running out of mark stack space.\n+    fatal(\"Mark stack space exhausted. Use -XX:ZMarkStackSpaceLimit=<size> to increase the \"\n+          \"maximum number of bytes allocated for mark stacks. Current limit is \" SIZE_FORMAT \"M.\",\n+          ZMarkStackSpaceLimit \/ M);\n+  }\n+\n+  log_debug(gc, marking)(\"Expanding mark stack space: \" SIZE_FORMAT \"M->\" SIZE_FORMAT \"M\",\n+                         old_size \/ M, new_size \/ M);\n+\n+  \/\/ Expand\n+  os::commit_memory_or_exit((char*)_end, expand_size, false \/* executable *\/, \"Mark stack space\");\n+\n+  return expand_size;\n+}\n+\n+size_t XMarkStackSpace::shrink_space() {\n+  \/\/ Shrink to what is currently used\n+  const size_t old_size = size();\n+  const size_t new_size = align_up(used(), XMarkStackSpaceExpandSize);\n+  const size_t shrink_size = old_size - new_size;\n+\n+  if (shrink_size > 0) {\n+    \/\/ Shrink\n+    log_debug(gc, marking)(\"Shrinking mark stack space: \" SIZE_FORMAT \"M->\" SIZE_FORMAT \"M\",\n+                           old_size \/ M, new_size \/ M);\n+\n+    const uintptr_t shrink_start = _end - shrink_size;\n+    os::uncommit_memory((char*)shrink_start, shrink_size, false \/* executable *\/);\n+  }\n+\n+  return shrink_size;\n+}\n+\n+uintptr_t XMarkStackSpace::alloc_space(size_t size) {\n+  uintptr_t top = Atomic::load(&_top);\n+\n+  for (;;) {\n+    const uintptr_t end = Atomic::load(&_end);\n+    const uintptr_t new_top = top + size;\n+    if (new_top > end) {\n+      \/\/ Not enough space left\n+      return 0;\n+    }\n+\n+    const uintptr_t prev_top = Atomic::cmpxchg(&_top, top, new_top);\n+    if (prev_top == top) {\n+      \/\/ Success\n+      return top;\n+    }\n+\n+    \/\/ Retry\n+    top = prev_top;\n+  }\n+}\n+\n+uintptr_t XMarkStackSpace::expand_and_alloc_space(size_t size) {\n+  XLocker<XLock> locker(&_expand_lock);\n+\n+  \/\/ Retry allocation before expanding\n+  uintptr_t addr = alloc_space(size);\n+  if (addr != 0) {\n+    return addr;\n+  }\n+\n+  \/\/ Expand\n+  const size_t expand_size = expand_space();\n+\n+  \/\/ Increment top before end to make sure another\n+  \/\/ thread can't steal out newly expanded space.\n+  addr = Atomic::fetch_and_add(&_top, size);\n+  Atomic::add(&_end, expand_size);\n+\n+  return addr;\n+}\n+\n+uintptr_t XMarkStackSpace::alloc(size_t size) {\n+  assert(size <= XMarkStackSpaceExpandSize, \"Invalid size\");\n+\n+  const uintptr_t addr = alloc_space(size);\n+  if (addr != 0) {\n+    return addr;\n+  }\n+\n+  return expand_and_alloc_space(size);\n+}\n+\n+void XMarkStackSpace::free() {\n+  _end -= shrink_space();\n+  _top = _start;\n+}\n+\n+XMarkStackAllocator::XMarkStackAllocator() :\n+    _freelist(),\n+    _space() {}\n+\n+bool XMarkStackAllocator::is_initialized() const {\n+  return _space.is_initialized();\n+}\n+\n+size_t XMarkStackAllocator::size() const {\n+  return _space.size();\n+}\n+\n+XMarkStackMagazine* XMarkStackAllocator::create_magazine_from_space(uintptr_t addr, size_t size) {\n+  assert(is_aligned(size, XMarkStackSize), \"Invalid size\");\n+\n+  \/\/ Use first stack as magazine\n+  XMarkStackMagazine* const magazine = new ((void*)addr) XMarkStackMagazine();\n+  for (size_t i = XMarkStackSize; i < size; i += XMarkStackSize) {\n+    XMarkStack* const stack = new ((void*)(addr + i)) XMarkStack();\n+    const bool success = magazine->push(stack);\n+    assert(success, \"Magazine should never get full\");\n+  }\n+\n+  return magazine;\n+}\n+\n+XMarkStackMagazine* XMarkStackAllocator::alloc_magazine() {\n+  \/\/ Try allocating from the free list first\n+  XMarkStackMagazine* const magazine = _freelist.pop();\n+  if (magazine != NULL) {\n+    return magazine;\n+  }\n+\n+  \/\/ Allocate new magazine\n+  const uintptr_t addr = _space.alloc(XMarkStackMagazineSize);\n+  if (addr == 0) {\n+    return NULL;\n+  }\n+\n+  return create_magazine_from_space(addr, XMarkStackMagazineSize);\n+}\n+\n+void XMarkStackAllocator::free_magazine(XMarkStackMagazine* magazine) {\n+  _freelist.push(magazine);\n+}\n+\n+void XMarkStackAllocator::free() {\n+  _freelist.clear();\n+  _space.free();\n+}\n","filename":"src\/hotspot\/share\/gc\/x\/xMarkStackAllocator.cpp","additions":221,"deletions":0,"binary":false,"changes":221,"status":"added"},{"patch":"@@ -0,0 +1,77 @@\n+\/*\n+ * Copyright (c) 2016, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XMARKSTACKALLOCATOR_HPP\n+#define SHARE_GC_X_XMARKSTACKALLOCATOR_HPP\n+\n+#include \"gc\/x\/xGlobals.hpp\"\n+#include \"gc\/x\/xLock.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+class XMarkStackSpace {\n+private:\n+  XLock              _expand_lock;\n+  uintptr_t          _start;\n+  volatile uintptr_t _top;\n+  volatile uintptr_t _end;\n+\n+  size_t used() const;\n+\n+  size_t expand_space();\n+  size_t shrink_space();\n+\n+  uintptr_t alloc_space(size_t size);\n+  uintptr_t expand_and_alloc_space(size_t size);\n+\n+public:\n+  XMarkStackSpace();\n+\n+  bool is_initialized() const;\n+\n+  size_t size() const;\n+\n+  uintptr_t alloc(size_t size);\n+  void free();\n+};\n+\n+class XMarkStackAllocator {\n+private:\n+  XCACHE_ALIGNED XMarkStackMagazineList _freelist;\n+  XCACHE_ALIGNED XMarkStackSpace        _space;\n+\n+  XMarkStackMagazine* create_magazine_from_space(uintptr_t addr, size_t size);\n+\n+public:\n+  XMarkStackAllocator();\n+\n+  bool is_initialized() const;\n+\n+  size_t size() const;\n+\n+  XMarkStackMagazine* alloc_magazine();\n+  void free_magazine(XMarkStackMagazine* magazine);\n+\n+  void free();\n+};\n+\n+#endif \/\/ SHARE_GC_X_XMARKSTACKALLOCATOR_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xMarkStackAllocator.hpp","additions":77,"deletions":0,"binary":false,"changes":77,"status":"added"},{"patch":"@@ -0,0 +1,142 @@\n+\/*\n+ * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XMARKSTACKENTRY_HPP\n+#define SHARE_GC_X_XMARKSTACKENTRY_HPP\n+\n+#include \"gc\/x\/xBitField.hpp\"\n+#include \"memory\/allocation.hpp\"\n+\n+\/\/\n+\/\/ Mark stack entry layout\n+\/\/ -----------------------\n+\/\/\n+\/\/  Object entry\n+\/\/  ------------\n+\/\/\n+\/\/   6\n+\/\/   3                                                                5 4 3 2 1 0\n+\/\/  +------------------------------------------------------------------+-+-+-+-+-+\n+\/\/  |11111111 11111111 11111111 11111111 11111111 11111111 11111111 111|1|1|1|1|1|\n+\/\/  +------------------------------------------------------------------+-+-+-+-+-+\n+\/\/  |                                                                  | | | | |\n+\/\/  |                                            4-4 Mark Flag (1-bit) * | | | |\n+\/\/  |                                                                    | | | |\n+\/\/  |                                    3-3 Increment Live Flag (1-bit) * | | |\n+\/\/  |                                                                      | | |\n+\/\/  |                                              2-2 Follow Flag (1-bit) * | |\n+\/\/  |                                                                        | |\n+\/\/  |                                         1-1 Partial Array Flag (1-bit) * |\n+\/\/  |                                                                          |\n+\/\/  |                                                   0-0 Final Flag (1-bit) *\n+\/\/  |\n+\/\/  * 63-5 Object Address (59-bits)\n+\/\/\n+\/\/\n+\/\/  Partial array entry\n+\/\/  -------------------\n+\/\/\n+\/\/   6                                 3  3\n+\/\/   3                                 2  1                               2 1 0\n+\/\/  +------------------------------------+---------------------------------+-+-+\n+\/\/  |11111111 11111111 11111111 11111111 |11111111 11111111 11111111 111111|1|1|\n+\/\/  +------------------------------------+---------------------------------+-+-+\n+\/\/  |                                    |                                 | |\n+\/\/  |                                    |  1-1 Partial Array Flag (1-bit) * |\n+\/\/  |                                    |                                   |\n+\/\/  |                                    |            0-0 Final Flag (1-bit) *\n+\/\/  |                                    |\n+\/\/  |                                    * 31-2 Partial Array Length (30-bits)\n+\/\/  |\n+\/\/  * 63-32 Partial Array Address Offset (32-bits)\n+\/\/\n+\n+class XMarkStackEntry  {\n+private:\n+  typedef XBitField<uint64_t, bool,      0,  1>  field_finalizable;\n+  typedef XBitField<uint64_t, bool,      1,  1>  field_partial_array;\n+  typedef XBitField<uint64_t, bool,      2,  1>  field_follow;\n+  typedef XBitField<uint64_t, bool,      3,  1>  field_inc_live;\n+  typedef XBitField<uint64_t, bool,      4,  1>  field_mark;\n+  typedef XBitField<uint64_t, uintptr_t, 5,  59> field_object_address;\n+  typedef XBitField<uint64_t, size_t,    2,  30> field_partial_array_length;\n+  typedef XBitField<uint64_t, size_t,    32, 32> field_partial_array_offset;\n+\n+  uint64_t _entry;\n+\n+public:\n+  XMarkStackEntry() {\n+    \/\/ This constructor is intentionally left empty and does not initialize\n+    \/\/ _entry to allow it to be optimized out when instantiating XMarkStack,\n+    \/\/ which has a long array of XMarkStackEntry elements, but doesn't care\n+    \/\/ what _entry is initialized to.\n+  }\n+\n+  XMarkStackEntry(uintptr_t object_address, bool mark, bool inc_live, bool follow, bool finalizable) :\n+      _entry(field_object_address::encode(object_address) |\n+             field_mark::encode(mark) |\n+             field_inc_live::encode(inc_live) |\n+             field_follow::encode(follow) |\n+             field_partial_array::encode(false) |\n+             field_finalizable::encode(finalizable)) {}\n+\n+  XMarkStackEntry(size_t partial_array_offset, size_t partial_array_length, bool finalizable) :\n+      _entry(field_partial_array_offset::encode(partial_array_offset) |\n+             field_partial_array_length::encode(partial_array_length) |\n+             field_partial_array::encode(true) |\n+             field_finalizable::encode(finalizable)) {}\n+\n+  bool finalizable() const {\n+    return field_finalizable::decode(_entry);\n+  }\n+\n+  bool partial_array() const {\n+    return field_partial_array::decode(_entry);\n+  }\n+\n+  size_t partial_array_offset() const {\n+    return field_partial_array_offset::decode(_entry);\n+  }\n+\n+  size_t partial_array_length() const {\n+    return field_partial_array_length::decode(_entry);\n+  }\n+\n+  bool follow() const {\n+    return field_follow::decode(_entry);\n+  }\n+\n+  bool inc_live() const {\n+    return field_inc_live::decode(_entry);\n+  }\n+\n+  bool mark() const {\n+    return field_mark::decode(_entry);\n+  }\n+\n+  uintptr_t object_address() const {\n+    return field_object_address::decode(_entry);\n+  }\n+};\n+\n+#endif \/\/ SHARE_GC_X_XMARKSTACKENTRY_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xMarkStackEntry.hpp","additions":142,"deletions":0,"binary":false,"changes":142,"status":"added"},{"patch":"@@ -0,0 +1,54 @@\n+\/*\n+ * Copyright (c) 2017, 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XMARKTERMINATE_HPP\n+#define SHARE_GC_X_XMARKTERMINATE_HPP\n+\n+#include \"gc\/x\/xGlobals.hpp\"\n+#include \"memory\/allocation.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+class XMarkTerminate {\n+private:\n+  uint                         _nworkers;\n+  XCACHE_ALIGNED volatile uint _nworking_stage0;\n+  volatile uint                _nworking_stage1;\n+\n+  bool enter_stage(volatile uint* nworking_stage);\n+  void exit_stage(volatile uint* nworking_stage);\n+  bool try_exit_stage(volatile uint* nworking_stage);\n+\n+public:\n+  XMarkTerminate();\n+\n+  void reset(uint nworkers);\n+\n+  bool enter_stage0();\n+  void exit_stage0();\n+  bool try_exit_stage0();\n+\n+  bool enter_stage1();\n+  bool try_exit_stage1();\n+};\n+\n+#endif \/\/ SHARE_GC_X_XMARKTERMINATE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xMarkTerminate.hpp","additions":54,"deletions":0,"binary":false,"changes":54,"status":"added"},{"patch":"@@ -0,0 +1,88 @@\n+\/*\n+ * Copyright (c) 2017, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XMARKTERMINATE_INLINE_HPP\n+#define SHARE_GC_X_XMARKTERMINATE_INLINE_HPP\n+\n+#include \"gc\/x\/xMarkTerminate.hpp\"\n+\n+#include \"runtime\/atomic.hpp\"\n+\n+inline XMarkTerminate::XMarkTerminate() :\n+    _nworkers(0),\n+    _nworking_stage0(0),\n+    _nworking_stage1(0) {}\n+\n+inline bool XMarkTerminate::enter_stage(volatile uint* nworking_stage) {\n+  return Atomic::sub(nworking_stage, 1u) == 0;\n+}\n+\n+inline void XMarkTerminate::exit_stage(volatile uint* nworking_stage) {\n+  Atomic::add(nworking_stage, 1u);\n+}\n+\n+inline bool XMarkTerminate::try_exit_stage(volatile uint* nworking_stage) {\n+  uint nworking = Atomic::load(nworking_stage);\n+\n+  for (;;) {\n+    if (nworking == 0) {\n+      return false;\n+    }\n+\n+    const uint new_nworking = nworking + 1;\n+    const uint prev_nworking = Atomic::cmpxchg(nworking_stage, nworking, new_nworking);\n+    if (prev_nworking == nworking) {\n+      \/\/ Success\n+      return true;\n+    }\n+\n+    \/\/ Retry\n+    nworking = prev_nworking;\n+  }\n+}\n+\n+inline void XMarkTerminate::reset(uint nworkers) {\n+  _nworkers = _nworking_stage0 = _nworking_stage1 = nworkers;\n+}\n+\n+inline bool XMarkTerminate::enter_stage0() {\n+  return enter_stage(&_nworking_stage0);\n+}\n+\n+inline void XMarkTerminate::exit_stage0() {\n+  exit_stage(&_nworking_stage0);\n+}\n+\n+inline bool XMarkTerminate::try_exit_stage0() {\n+  return try_exit_stage(&_nworking_stage0);\n+}\n+\n+inline bool XMarkTerminate::enter_stage1() {\n+  return enter_stage(&_nworking_stage1);\n+}\n+\n+inline bool XMarkTerminate::try_exit_stage1() {\n+  return try_exit_stage(&_nworking_stage1);\n+}\n+\n+#endif \/\/ SHARE_GC_X_XMARKTERMINATE_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xMarkTerminate.inline.hpp","additions":88,"deletions":0,"binary":false,"changes":88,"status":"added"},{"patch":"@@ -0,0 +1,220 @@\n+\/*\n+ * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/x\/xList.inline.hpp\"\n+#include \"gc\/x\/xLock.inline.hpp\"\n+#include \"gc\/x\/xMemory.inline.hpp\"\n+\n+XMemory* XMemoryManager::create(uintptr_t start, size_t size) {\n+  XMemory* const area = new XMemory(start, size);\n+  if (_callbacks._create != NULL) {\n+    _callbacks._create(area);\n+  }\n+  return area;\n+}\n+\n+void XMemoryManager::destroy(XMemory* area) {\n+  if (_callbacks._destroy != NULL) {\n+    _callbacks._destroy(area);\n+  }\n+  delete area;\n+}\n+\n+void XMemoryManager::shrink_from_front(XMemory* area, size_t size) {\n+  if (_callbacks._shrink_from_front != NULL) {\n+    _callbacks._shrink_from_front(area, size);\n+  }\n+  area->shrink_from_front(size);\n+}\n+\n+void XMemoryManager::shrink_from_back(XMemory* area, size_t size) {\n+  if (_callbacks._shrink_from_back != NULL) {\n+    _callbacks._shrink_from_back(area, size);\n+  }\n+  area->shrink_from_back(size);\n+}\n+\n+void XMemoryManager::grow_from_front(XMemory* area, size_t size) {\n+  if (_callbacks._grow_from_front != NULL) {\n+    _callbacks._grow_from_front(area, size);\n+  }\n+  area->grow_from_front(size);\n+}\n+\n+void XMemoryManager::grow_from_back(XMemory* area, size_t size) {\n+  if (_callbacks._grow_from_back != NULL) {\n+    _callbacks._grow_from_back(area, size);\n+  }\n+  area->grow_from_back(size);\n+}\n+\n+XMemoryManager::Callbacks::Callbacks() :\n+    _create(NULL),\n+    _destroy(NULL),\n+    _shrink_from_front(NULL),\n+    _shrink_from_back(NULL),\n+    _grow_from_front(NULL),\n+    _grow_from_back(NULL) {}\n+\n+XMemoryManager::XMemoryManager() :\n+    _freelist(),\n+    _callbacks() {}\n+\n+void XMemoryManager::register_callbacks(const Callbacks& callbacks) {\n+  _callbacks = callbacks;\n+}\n+\n+uintptr_t XMemoryManager::peek_low_address() const {\n+  XLocker<XLock> locker(&_lock);\n+\n+  const XMemory* const area = _freelist.first();\n+  if (area != NULL) {\n+    return area->start();\n+  }\n+\n+  \/\/ Out of memory\n+  return UINTPTR_MAX;\n+}\n+\n+uintptr_t XMemoryManager::alloc_low_address(size_t size) {\n+  XLocker<XLock> locker(&_lock);\n+\n+  XListIterator<XMemory> iter(&_freelist);\n+  for (XMemory* area; iter.next(&area);) {\n+    if (area->size() >= size) {\n+      if (area->size() == size) {\n+        \/\/ Exact match, remove area\n+        const uintptr_t start = area->start();\n+        _freelist.remove(area);\n+        destroy(area);\n+        return start;\n+      } else {\n+        \/\/ Larger than requested, shrink area\n+        const uintptr_t start = area->start();\n+        shrink_from_front(area, size);\n+        return start;\n+      }\n+    }\n+  }\n+\n+  \/\/ Out of memory\n+  return UINTPTR_MAX;\n+}\n+\n+uintptr_t XMemoryManager::alloc_low_address_at_most(size_t size, size_t* allocated) {\n+  XLocker<XLock> locker(&_lock);\n+\n+  XMemory* area = _freelist.first();\n+  if (area != NULL) {\n+    if (area->size() <= size) {\n+      \/\/ Smaller than or equal to requested, remove area\n+      const uintptr_t start = area->start();\n+      *allocated = area->size();\n+      _freelist.remove(area);\n+      destroy(area);\n+      return start;\n+    } else {\n+      \/\/ Larger than requested, shrink area\n+      const uintptr_t start = area->start();\n+      shrink_from_front(area, size);\n+      *allocated = size;\n+      return start;\n+    }\n+  }\n+\n+  \/\/ Out of memory\n+  *allocated = 0;\n+  return UINTPTR_MAX;\n+}\n+\n+uintptr_t XMemoryManager::alloc_high_address(size_t size) {\n+  XLocker<XLock> locker(&_lock);\n+\n+  XListReverseIterator<XMemory> iter(&_freelist);\n+  for (XMemory* area; iter.next(&area);) {\n+    if (area->size() >= size) {\n+      if (area->size() == size) {\n+        \/\/ Exact match, remove area\n+        const uintptr_t start = area->start();\n+        _freelist.remove(area);\n+        destroy(area);\n+        return start;\n+      } else {\n+        \/\/ Larger than requested, shrink area\n+        shrink_from_back(area, size);\n+        return area->end();\n+      }\n+    }\n+  }\n+\n+  \/\/ Out of memory\n+  return UINTPTR_MAX;\n+}\n+\n+void XMemoryManager::free(uintptr_t start, size_t size) {\n+  assert(start != UINTPTR_MAX, \"Invalid address\");\n+  const uintptr_t end = start + size;\n+\n+  XLocker<XLock> locker(&_lock);\n+\n+  XListIterator<XMemory> iter(&_freelist);\n+  for (XMemory* area; iter.next(&area);) {\n+    if (start < area->start()) {\n+      XMemory* const prev = _freelist.prev(area);\n+      if (prev != NULL && start == prev->end()) {\n+        if (end == area->start()) {\n+          \/\/ Merge with prev and current area\n+          grow_from_back(prev, size + area->size());\n+          _freelist.remove(area);\n+          delete area;\n+        } else {\n+          \/\/ Merge with prev area\n+          grow_from_back(prev, size);\n+        }\n+      } else if (end == area->start()) {\n+        \/\/ Merge with current area\n+        grow_from_front(area, size);\n+      } else {\n+        \/\/ Insert new area before current area\n+        assert(end < area->start(), \"Areas must not overlap\");\n+        XMemory* const new_area = create(start, size);\n+        _freelist.insert_before(area, new_area);\n+      }\n+\n+      \/\/ Done\n+      return;\n+    }\n+  }\n+\n+  \/\/ Insert last\n+  XMemory* const last = _freelist.last();\n+  if (last != NULL && start == last->end()) {\n+    \/\/ Merge with last area\n+    grow_from_back(last, size);\n+  } else {\n+    \/\/ Insert new area last\n+    XMemory* const new_area = create(start, size);\n+    _freelist.insert_last(new_area);\n+  }\n+}\n","filename":"src\/hotspot\/share\/gc\/x\/xMemory.cpp","additions":220,"deletions":0,"binary":false,"changes":220,"status":"added"},{"patch":"@@ -0,0 +1,93 @@\n+\/*\n+ * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XMEMORY_HPP\n+#define SHARE_GC_X_XMEMORY_HPP\n+\n+#include \"gc\/x\/xList.hpp\"\n+#include \"gc\/x\/xLock.hpp\"\n+#include \"memory\/allocation.hpp\"\n+\n+class XMemory : public CHeapObj<mtGC> {\n+  friend class XList<XMemory>;\n+\n+private:\n+  uintptr_t          _start;\n+  uintptr_t          _end;\n+  XListNode<XMemory> _node;\n+\n+public:\n+  XMemory(uintptr_t start, size_t size);\n+\n+  uintptr_t start() const;\n+  uintptr_t end() const;\n+  size_t size() const;\n+\n+  void shrink_from_front(size_t size);\n+  void shrink_from_back(size_t size);\n+  void grow_from_front(size_t size);\n+  void grow_from_back(size_t size);\n+};\n+\n+class XMemoryManager {\n+public:\n+  typedef void (*CreateDestroyCallback)(const XMemory* area);\n+  typedef void (*ResizeCallback)(const XMemory* area, size_t size);\n+\n+  struct Callbacks {\n+    CreateDestroyCallback _create;\n+    CreateDestroyCallback _destroy;\n+    ResizeCallback        _shrink_from_front;\n+    ResizeCallback        _shrink_from_back;\n+    ResizeCallback        _grow_from_front;\n+    ResizeCallback        _grow_from_back;\n+\n+    Callbacks();\n+  };\n+\n+private:\n+  mutable XLock  _lock;\n+  XList<XMemory> _freelist;\n+  Callbacks      _callbacks;\n+\n+  XMemory* create(uintptr_t start, size_t size);\n+  void destroy(XMemory* area);\n+  void shrink_from_front(XMemory* area, size_t size);\n+  void shrink_from_back(XMemory* area, size_t size);\n+  void grow_from_front(XMemory* area, size_t size);\n+  void grow_from_back(XMemory* area, size_t size);\n+\n+public:\n+  XMemoryManager();\n+\n+  void register_callbacks(const Callbacks& callbacks);\n+\n+  uintptr_t peek_low_address() const;\n+  uintptr_t alloc_low_address(size_t size);\n+  uintptr_t alloc_low_address_at_most(size_t size, size_t* allocated);\n+  uintptr_t alloc_high_address(size_t size);\n+\n+  void free(uintptr_t start, size_t size);\n+};\n+\n+#endif \/\/ SHARE_GC_X_XMEMORY_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xMemory.hpp","additions":93,"deletions":0,"binary":false,"changes":93,"status":"added"},{"patch":"@@ -0,0 +1,67 @@\n+\/*\n+ * Copyright (c) 2015, 2017, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XMEMORY_INLINE_HPP\n+#define SHARE_GC_X_XMEMORY_INLINE_HPP\n+\n+#include \"gc\/x\/xMemory.hpp\"\n+\n+#include \"gc\/x\/xList.inline.hpp\"\n+#include \"utilities\/debug.hpp\"\n+\n+inline XMemory::XMemory(uintptr_t start, size_t size) :\n+    _start(start),\n+    _end(start + size) {}\n+\n+inline uintptr_t XMemory::start() const {\n+  return _start;\n+}\n+\n+inline uintptr_t XMemory::end() const {\n+  return _end;\n+}\n+\n+inline size_t XMemory::size() const {\n+  return end() - start();\n+}\n+\n+inline void XMemory::shrink_from_front(size_t size) {\n+  assert(this->size() > size, \"Too small\");\n+  _start += size;\n+}\n+\n+inline void XMemory::shrink_from_back(size_t size) {\n+  assert(this->size() > size, \"Too small\");\n+  _end -= size;\n+}\n+\n+inline void XMemory::grow_from_front(size_t size) {\n+  assert(start() >= size, \"Too big\");\n+  _start -= size;\n+}\n+\n+inline void XMemory::grow_from_back(size_t size) {\n+  _end += size;\n+}\n+\n+#endif \/\/ SHARE_GC_X_XMEMORY_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xMemory.inline.hpp","additions":67,"deletions":0,"binary":false,"changes":67,"status":"added"},{"patch":"@@ -0,0 +1,66 @@\n+\/*\n+ * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XMESSAGEPORT_HPP\n+#define SHARE_GC_X_XMESSAGEPORT_HPP\n+\n+#include \"gc\/x\/xFuture.hpp\"\n+#include \"gc\/x\/xList.hpp\"\n+#include \"runtime\/mutex.hpp\"\n+\n+template <typename T> class XMessageRequest;\n+\n+template <typename T>\n+class XMessagePort {\n+private:\n+  typedef XMessageRequest<T> Request;\n+\n+  mutable Monitor _monitor;\n+  bool            _has_message;\n+  T               _message;\n+  uint64_t        _seqnum;\n+  XList<Request>  _queue;\n+\n+public:\n+  XMessagePort();\n+\n+  bool is_busy() const;\n+\n+  void send_sync(const T& message);\n+  void send_async(const T& message);\n+\n+  T receive();\n+  void ack();\n+};\n+\n+class XRendezvousPort {\n+private:\n+  XMessagePort<bool> _port;\n+\n+public:\n+  void signal();\n+  void wait();\n+  void ack();\n+};\n+\n+#endif \/\/ SHARE_GC_X_XMESSAGEPORT_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xMessagePort.hpp","additions":66,"deletions":0,"binary":false,"changes":66,"status":"added"},{"patch":"@@ -0,0 +1,181 @@\n+\/*\n+ * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XMESSAGEPORT_INLINE_HPP\n+#define SHARE_GC_X_XMESSAGEPORT_INLINE_HPP\n+\n+#include \"gc\/x\/xMessagePort.hpp\"\n+\n+#include \"gc\/x\/xFuture.inline.hpp\"\n+#include \"gc\/x\/xList.inline.hpp\"\n+#include \"runtime\/mutexLocker.hpp\"\n+\n+template <typename T>\n+class XMessageRequest : public StackObj {\n+  friend class XList<XMessageRequest>;\n+\n+private:\n+  T                          _message;\n+  uint64_t                   _seqnum;\n+  XFuture<T>                 _result;\n+  XListNode<XMessageRequest> _node;\n+\n+public:\n+  void initialize(T message, uint64_t seqnum) {\n+    _message = message;\n+    _seqnum = seqnum;\n+  }\n+\n+  T message() const {\n+    return _message;\n+  }\n+\n+  uint64_t seqnum() const {\n+    return _seqnum;\n+  }\n+\n+  void wait() {\n+    const T message = _result.get();\n+    assert(message == _message, \"Message mismatch\");\n+  }\n+\n+  void satisfy(T message) {\n+    _result.set(message);\n+  }\n+};\n+\n+template <typename T>\n+inline XMessagePort<T>::XMessagePort() :\n+    _monitor(Monitor::nosafepoint, \"XMessagePort_lock\"),\n+    _has_message(false),\n+    _seqnum(0),\n+    _queue() {}\n+\n+template <typename T>\n+inline bool XMessagePort<T>::is_busy() const {\n+  MonitorLocker ml(&_monitor, Monitor::_no_safepoint_check_flag);\n+  return _has_message;\n+}\n+\n+template <typename T>\n+inline void XMessagePort<T>::send_sync(const T& message) {\n+  Request request;\n+\n+  {\n+    \/\/ Enqueue message\n+    MonitorLocker ml(&_monitor, Monitor::_no_safepoint_check_flag);\n+    request.initialize(message, _seqnum);\n+    _queue.insert_last(&request);\n+    ml.notify();\n+  }\n+\n+  \/\/ Wait for completion\n+  request.wait();\n+\n+  {\n+    \/\/ Guard deletion of underlying semaphore. This is a workaround for a\n+    \/\/ bug in sem_post() in glibc < 2.21, where it's not safe to destroy\n+    \/\/ the semaphore immediately after returning from sem_wait(). The\n+    \/\/ reason is that sem_post() can touch the semaphore after a waiting\n+    \/\/ thread have returned from sem_wait(). To avoid this race we are\n+    \/\/ forcing the waiting thread to acquire\/release the lock held by the\n+    \/\/ posting thread. https:\/\/sourceware.org\/bugzilla\/show_bug.cgi?id=12674\n+    MonitorLocker ml(&_monitor, Monitor::_no_safepoint_check_flag);\n+  }\n+}\n+\n+template <typename T>\n+inline void XMessagePort<T>::send_async(const T& message) {\n+  MonitorLocker ml(&_monitor, Monitor::_no_safepoint_check_flag);\n+  if (!_has_message) {\n+    \/\/ Post message\n+    _message = message;\n+    _has_message = true;\n+    ml.notify();\n+  }\n+}\n+\n+template <typename T>\n+inline T XMessagePort<T>::receive() {\n+  MonitorLocker ml(&_monitor, Monitor::_no_safepoint_check_flag);\n+\n+  \/\/ Wait for message\n+  while (!_has_message && _queue.is_empty()) {\n+    ml.wait();\n+  }\n+\n+  \/\/ Increment request sequence number\n+  _seqnum++;\n+\n+  if (!_has_message) {\n+    \/\/ Message available in the queue\n+    _message = _queue.first()->message();\n+    _has_message = true;\n+  }\n+\n+  return _message;\n+}\n+\n+template <typename T>\n+inline void XMessagePort<T>::ack() {\n+  MonitorLocker ml(&_monitor, Monitor::_no_safepoint_check_flag);\n+\n+  if (!_has_message) {\n+    \/\/ Nothing to ack\n+    return;\n+  }\n+\n+  \/\/ Satisfy requests (and duplicates) in queue\n+  XListIterator<Request> iter(&_queue);\n+  for (Request* request; iter.next(&request);) {\n+    if (request->message() == _message && request->seqnum() < _seqnum) {\n+      \/\/ Dequeue and satisfy request. Note that the dequeue operation must\n+      \/\/ happen first, since the request will immediately be deallocated\n+      \/\/ once it has been satisfied.\n+      _queue.remove(request);\n+      request->satisfy(_message);\n+    }\n+  }\n+\n+  if (_queue.is_empty()) {\n+    \/\/ Queue is empty\n+    _has_message = false;\n+  } else {\n+    \/\/ Post first message in queue\n+    _message = _queue.first()->message();\n+  }\n+}\n+\n+inline void XRendezvousPort::signal() {\n+  _port.send_sync(true \/* ignored *\/);\n+}\n+\n+inline void XRendezvousPort::wait() {\n+  _port.receive();\n+}\n+\n+inline void XRendezvousPort::ack() {\n+  _port.ack();\n+}\n+\n+#endif \/\/ SHARE_GC_X_XMESSAGEPORT_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xMessagePort.inline.hpp","additions":181,"deletions":0,"binary":false,"changes":181,"status":"added"},{"patch":"@@ -0,0 +1,81 @@\n+\/*\n+ * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/x\/xMetronome.hpp\"\n+#include \"runtime\/mutexLocker.hpp\"\n+#include \"runtime\/timer.hpp\"\n+#include \"utilities\/ticks.hpp\"\n+\n+XMetronome::XMetronome(uint64_t hz) :\n+    _monitor(Monitor::nosafepoint, \"XMetronome_lock\"),\n+    _interval_ms(MILLIUNITS \/ hz),\n+    _start_ms(0),\n+    _nticks(0),\n+    _stopped(false) {}\n+\n+bool XMetronome::wait_for_tick() {\n+  if (_nticks++ == 0) {\n+    \/\/ First tick, set start time\n+    const Ticks now = Ticks::now();\n+    _start_ms = TimeHelper::counter_to_millis(now.value());\n+  }\n+\n+  MonitorLocker ml(&_monitor, Monitor::_no_safepoint_check_flag);\n+\n+  while (!_stopped) {\n+    \/\/ We might wake up spuriously from wait, so always recalculate\n+    \/\/ the timeout after a wakeup to see if we need to wait again.\n+    const Ticks now = Ticks::now();\n+    const uint64_t now_ms = TimeHelper::counter_to_millis(now.value());\n+    const uint64_t next_ms = _start_ms + (_interval_ms * _nticks);\n+    const int64_t timeout_ms = next_ms - now_ms;\n+\n+    if (timeout_ms > 0) {\n+      \/\/ Wait\n+      ml.wait(timeout_ms);\n+    } else {\n+      \/\/ Tick\n+      if (timeout_ms < 0) {\n+        const uint64_t overslept = -timeout_ms;\n+        if (overslept > _interval_ms) {\n+          \/\/ Missed one or more ticks. Bump _nticks accordingly to\n+          \/\/ avoid firing a string of immediate ticks to make up\n+          \/\/ for the ones we missed.\n+          _nticks += overslept \/ _interval_ms;\n+        }\n+      }\n+\n+      return true;\n+    }\n+  }\n+\n+  \/\/ Stopped\n+  return false;\n+}\n+\n+void XMetronome::stop() {\n+  MonitorLocker ml(&_monitor, Monitor::_no_safepoint_check_flag);\n+  _stopped = true;\n+  ml.notify();\n+}\n","filename":"src\/hotspot\/share\/gc\/x\/xMetronome.cpp","additions":81,"deletions":0,"binary":false,"changes":81,"status":"added"},{"patch":"@@ -0,0 +1,45 @@\n+\/*\n+ * Copyright (c) 2015, 2017, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XMETRONOME_HPP\n+#define SHARE_GC_X_XMETRONOME_HPP\n+\n+#include \"memory\/allocation.hpp\"\n+#include \"runtime\/mutex.hpp\"\n+\n+class XMetronome : public StackObj {\n+private:\n+  Monitor        _monitor;\n+  const uint64_t _interval_ms;\n+  uint64_t       _start_ms;\n+  uint64_t       _nticks;\n+  bool           _stopped;\n+\n+public:\n+  XMetronome(uint64_t hz);\n+\n+  bool wait_for_tick();\n+  void stop();\n+};\n+\n+#endif \/\/ SHARE_GC_X_XMETRONOME_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xMetronome.hpp","additions":45,"deletions":0,"binary":false,"changes":45,"status":"added"},{"patch":"@@ -0,0 +1,366 @@\n+\/*\n+ * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"code\/relocInfo.hpp\"\n+#include \"code\/nmethod.hpp\"\n+#include \"code\/icBuffer.hpp\"\n+#include \"gc\/shared\/barrierSet.hpp\"\n+#include \"gc\/shared\/barrierSetNMethod.hpp\"\n+#include \"gc\/shared\/suspendibleThreadSet.hpp\"\n+#include \"gc\/x\/xBarrier.inline.hpp\"\n+#include \"gc\/x\/xGlobals.hpp\"\n+#include \"gc\/x\/xLock.inline.hpp\"\n+#include \"gc\/x\/xNMethod.hpp\"\n+#include \"gc\/x\/xNMethodData.hpp\"\n+#include \"gc\/x\/xNMethodTable.hpp\"\n+#include \"gc\/x\/xTask.hpp\"\n+#include \"gc\/x\/xWorkers.hpp\"\n+#include \"logging\/log.hpp\"\n+#include \"memory\/allocation.inline.hpp\"\n+#include \"memory\/iterator.hpp\"\n+#include \"memory\/resourceArea.hpp\"\n+#include \"memory\/universe.hpp\"\n+#include \"oops\/oop.inline.hpp\"\n+#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/continuation.hpp\"\n+#include \"utilities\/debug.hpp\"\n+\n+static XNMethodData* gc_data(const nmethod* nm) {\n+  return nm->gc_data<XNMethodData>();\n+}\n+\n+static void set_gc_data(nmethod* nm, XNMethodData* data) {\n+  return nm->set_gc_data<XNMethodData>(data);\n+}\n+\n+void XNMethod::attach_gc_data(nmethod* nm) {\n+  GrowableArray<oop*> immediate_oops;\n+  bool non_immediate_oops = false;\n+\n+  \/\/ Find all oop relocations\n+  RelocIterator iter(nm);\n+  while (iter.next()) {\n+    if (iter.type() != relocInfo::oop_type) {\n+      \/\/ Not an oop\n+      continue;\n+    }\n+\n+    oop_Relocation* r = iter.oop_reloc();\n+\n+    if (!r->oop_is_immediate()) {\n+      \/\/ Non-immediate oop found\n+      non_immediate_oops = true;\n+      continue;\n+    }\n+\n+    if (r->oop_value() != NULL) {\n+      \/\/ Non-NULL immediate oop found. NULL oops can safely be\n+      \/\/ ignored since the method will be re-registered if they\n+      \/\/ are later patched to be non-NULL.\n+      immediate_oops.push(r->oop_addr());\n+    }\n+  }\n+\n+  \/\/ Attach GC data to nmethod\n+  XNMethodData* data = gc_data(nm);\n+  if (data == NULL) {\n+    data = new XNMethodData();\n+    set_gc_data(nm, data);\n+  }\n+\n+  \/\/ Attach oops in GC data\n+  XNMethodDataOops* const new_oops = XNMethodDataOops::create(immediate_oops, non_immediate_oops);\n+  XNMethodDataOops* const old_oops = data->swap_oops(new_oops);\n+  XNMethodDataOops::destroy(old_oops);\n+}\n+\n+XReentrantLock* XNMethod::lock_for_nmethod(nmethod* nm) {\n+  return gc_data(nm)->lock();\n+}\n+\n+void XNMethod::log_register(const nmethod* nm) {\n+  LogTarget(Trace, gc, nmethod) log;\n+  if (!log.is_enabled()) {\n+    return;\n+  }\n+\n+  const XNMethodDataOops* const oops = gc_data(nm)->oops();\n+\n+  log.print(\"Register NMethod: %s.%s (\" PTR_FORMAT \"), \"\n+            \"Compiler: %s, Oops: %d, ImmediateOops: \" SIZE_FORMAT \", NonImmediateOops: %s\",\n+            nm->method()->method_holder()->external_name(),\n+            nm->method()->name()->as_C_string(),\n+            p2i(nm),\n+            nm->compiler_name(),\n+            nm->oops_count() - 1,\n+            oops->immediates_count(),\n+            oops->has_non_immediates() ? \"Yes\" : \"No\");\n+\n+  LogTarget(Trace, gc, nmethod, oops) log_oops;\n+  if (!log_oops.is_enabled()) {\n+    return;\n+  }\n+\n+  \/\/ Print nmethod oops table\n+  {\n+    oop* const begin = nm->oops_begin();\n+    oop* const end = nm->oops_end();\n+    for (oop* p = begin; p < end; p++) {\n+      const oop o = Atomic::load(p); \/\/ C1 PatchingStub may replace it concurrently.\n+      const char* external_name = (o == nullptr) ? \"N\/A\" : o->klass()->external_name();\n+      log_oops.print(\"           Oop[\" SIZE_FORMAT \"] \" PTR_FORMAT \" (%s)\",\n+                     (p - begin), p2i(o), external_name);\n+    }\n+  }\n+\n+  \/\/ Print nmethod immediate oops\n+  {\n+    oop** const begin = oops->immediates_begin();\n+    oop** const end = oops->immediates_end();\n+    for (oop** p = begin; p < end; p++) {\n+      log_oops.print(\"  ImmediateOop[\" SIZE_FORMAT \"] \" PTR_FORMAT \" @ \" PTR_FORMAT \" (%s)\",\n+                     (p - begin), p2i(**p), p2i(*p), (**p)->klass()->external_name());\n+    }\n+  }\n+}\n+\n+void XNMethod::log_unregister(const nmethod* nm) {\n+  LogTarget(Debug, gc, nmethod) log;\n+  if (!log.is_enabled()) {\n+    return;\n+  }\n+\n+  log.print(\"Unregister NMethod: %s.%s (\" PTR_FORMAT \")\",\n+            nm->method()->method_holder()->external_name(),\n+            nm->method()->name()->as_C_string(),\n+            p2i(nm));\n+}\n+\n+void XNMethod::register_nmethod(nmethod* nm) {\n+  ResourceMark rm;\n+\n+  \/\/ Create and attach gc data\n+  attach_gc_data(nm);\n+\n+  log_register(nm);\n+\n+  XNMethodTable::register_nmethod(nm);\n+\n+  \/\/ Disarm nmethod entry barrier\n+  disarm(nm);\n+}\n+\n+void XNMethod::unregister_nmethod(nmethod* nm) {\n+  ResourceMark rm;\n+\n+  log_unregister(nm);\n+\n+  XNMethodTable::unregister_nmethod(nm);\n+\n+  \/\/ Destroy GC data\n+  delete gc_data(nm);\n+}\n+\n+bool XNMethod::supports_entry_barrier(nmethod* nm) {\n+  BarrierSetNMethod* const bs = BarrierSet::barrier_set()->barrier_set_nmethod();\n+  return bs->supports_entry_barrier(nm);\n+}\n+\n+bool XNMethod::is_armed(nmethod* nm) {\n+  BarrierSetNMethod* const bs = BarrierSet::barrier_set()->barrier_set_nmethod();\n+  return bs->is_armed(nm);\n+}\n+\n+void XNMethod::disarm(nmethod* nm) {\n+  BarrierSetNMethod* const bs = BarrierSet::barrier_set()->barrier_set_nmethod();\n+  bs->disarm(nm);\n+}\n+\n+void XNMethod::set_guard_value(nmethod* nm, int value) {\n+  BarrierSetNMethod* const bs = BarrierSet::barrier_set()->barrier_set_nmethod();\n+  bs->set_guard_value(nm, value);\n+}\n+\n+void XNMethod::nmethod_oops_do(nmethod* nm, OopClosure* cl) {\n+  XLocker<XReentrantLock> locker(XNMethod::lock_for_nmethod(nm));\n+  XNMethod::nmethod_oops_do_inner(nm, cl);\n+}\n+\n+void XNMethod::nmethod_oops_do_inner(nmethod* nm, OopClosure* cl) {\n+  \/\/ Process oops table\n+  {\n+    oop* const begin = nm->oops_begin();\n+    oop* const end = nm->oops_end();\n+    for (oop* p = begin; p < end; p++) {\n+      if (!Universe::contains_non_oop_word(p)) {\n+        cl->do_oop(p);\n+      }\n+    }\n+  }\n+\n+  XNMethodDataOops* const oops = gc_data(nm)->oops();\n+\n+  \/\/ Process immediate oops\n+  {\n+    oop** const begin = oops->immediates_begin();\n+    oop** const end = oops->immediates_end();\n+    for (oop** p = begin; p < end; p++) {\n+      if (*p != Universe::non_oop_word()) {\n+        cl->do_oop(*p);\n+      }\n+    }\n+  }\n+\n+  \/\/ Process non-immediate oops\n+  if (oops->has_non_immediates()) {\n+    nm->fix_oop_relocations();\n+  }\n+}\n+\n+class XNMethodOopClosure : public OopClosure {\n+public:\n+  virtual void do_oop(oop* p) {\n+    if (XResurrection::is_blocked()) {\n+      XBarrier::keep_alive_barrier_on_phantom_root_oop_field(p);\n+    } else {\n+      XBarrier::load_barrier_on_root_oop_field(p);\n+    }\n+  }\n+\n+  virtual void do_oop(narrowOop* p) {\n+    ShouldNotReachHere();\n+  }\n+};\n+\n+void XNMethod::nmethod_oops_barrier(nmethod* nm) {\n+  XNMethodOopClosure cl;\n+  nmethod_oops_do_inner(nm, &cl);\n+}\n+\n+void XNMethod::nmethods_do_begin() {\n+  XNMethodTable::nmethods_do_begin();\n+}\n+\n+void XNMethod::nmethods_do_end() {\n+  XNMethodTable::nmethods_do_end();\n+}\n+\n+void XNMethod::nmethods_do(NMethodClosure* cl) {\n+  XNMethodTable::nmethods_do(cl);\n+}\n+\n+class XNMethodUnlinkClosure : public NMethodClosure {\n+private:\n+  bool          _unloading_occurred;\n+  volatile bool _failed;\n+\n+  void set_failed() {\n+    Atomic::store(&_failed, true);\n+  }\n+\n+public:\n+  XNMethodUnlinkClosure(bool unloading_occurred) :\n+      _unloading_occurred(unloading_occurred),\n+      _failed(false) {}\n+\n+  virtual void do_nmethod(nmethod* nm) {\n+    if (failed()) {\n+      return;\n+    }\n+\n+    if (nm->is_unloading()) {\n+      XLocker<XReentrantLock> locker(XNMethod::lock_for_nmethod(nm));\n+      nm->unlink();\n+      return;\n+    }\n+\n+    XLocker<XReentrantLock> locker(XNMethod::lock_for_nmethod(nm));\n+\n+    if (XNMethod::is_armed(nm)) {\n+      \/\/ Heal oops and arm phase invariantly\n+      XNMethod::nmethod_oops_barrier(nm);\n+      XNMethod::set_guard_value(nm, 0);\n+    }\n+\n+    \/\/ Clear compiled ICs and exception caches\n+    if (!nm->unload_nmethod_caches(_unloading_occurred)) {\n+      set_failed();\n+    }\n+  }\n+\n+  bool failed() const {\n+    return Atomic::load(&_failed);\n+  }\n+};\n+\n+class XNMethodUnlinkTask : public XTask {\n+private:\n+  XNMethodUnlinkClosure _cl;\n+  ICRefillVerifier*     _verifier;\n+\n+public:\n+  XNMethodUnlinkTask(bool unloading_occurred, ICRefillVerifier* verifier) :\n+      XTask(\"XNMethodUnlinkTask\"),\n+      _cl(unloading_occurred),\n+      _verifier(verifier) {\n+    XNMethodTable::nmethods_do_begin();\n+  }\n+\n+  ~XNMethodUnlinkTask() {\n+    XNMethodTable::nmethods_do_end();\n+  }\n+\n+  virtual void work() {\n+    ICRefillVerifierMark mark(_verifier);\n+    XNMethodTable::nmethods_do(&_cl);\n+  }\n+\n+  bool success() const {\n+    return !_cl.failed();\n+  }\n+};\n+\n+void XNMethod::unlink(XWorkers* workers, bool unloading_occurred) {\n+  for (;;) {\n+    ICRefillVerifier verifier;\n+\n+    {\n+      XNMethodUnlinkTask task(unloading_occurred, &verifier);\n+      workers->run(&task);\n+      if (task.success()) {\n+        return;\n+      }\n+    }\n+\n+    \/\/ Cleaning failed because we ran out of transitional IC stubs,\n+    \/\/ so we have to refill and try again. Refilling requires taking\n+    \/\/ a safepoint, so we temporarily leave the suspendible thread set.\n+    SuspendibleThreadSetLeaver sts;\n+    InlineCacheBuffer::refill_ic_stubs();\n+  }\n+}\n+\n+void XNMethod::purge() {\n+  CodeCache::flush_unlinked_nmethods();\n+}\n","filename":"src\/hotspot\/share\/gc\/x\/xNMethod.cpp","additions":366,"deletions":0,"binary":false,"changes":366,"status":"added"},{"patch":"@@ -0,0 +1,66 @@\n+\/*\n+ * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XNMETHOD_HPP\n+#define SHARE_GC_X_XNMETHOD_HPP\n+\n+#include \"memory\/allStatic.hpp\"\n+\n+class nmethod;\n+class NMethodClosure;\n+class XReentrantLock;\n+class XWorkers;\n+\n+class XNMethod : public AllStatic {\n+private:\n+  static void attach_gc_data(nmethod* nm);\n+\n+  static void log_register(const nmethod* nm);\n+  static void log_unregister(const nmethod* nm);\n+\n+public:\n+  static void register_nmethod(nmethod* nm);\n+  static void unregister_nmethod(nmethod* nm);\n+\n+  static bool supports_entry_barrier(nmethod* nm);\n+\n+  static bool is_armed(nmethod* nm);\n+  static void disarm(nmethod* nm);\n+  static void set_guard_value(nmethod* nm, int value);\n+\n+  static void nmethod_oops_do(nmethod* nm, OopClosure* cl);\n+  static void nmethod_oops_do_inner(nmethod* nm, OopClosure* cl);\n+\n+  static void nmethod_oops_barrier(nmethod* nm);\n+\n+  static void nmethods_do_begin();\n+  static void nmethods_do_end();\n+  static void nmethods_do(NMethodClosure* cl);\n+\n+  static XReentrantLock* lock_for_nmethod(nmethod* nm);\n+\n+  static void unlink(XWorkers* workers, bool unloading_occurred);\n+  static void purge();\n+};\n+\n+#endif \/\/ SHARE_GC_X_XNMETHOD_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xNMethod.hpp","additions":66,"deletions":0,"binary":false,"changes":66,"status":"added"},{"patch":"@@ -0,0 +1,88 @@\n+\/*\n+ * Copyright (c) 2017, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/x\/xAttachedArray.inline.hpp\"\n+#include \"gc\/x\/xLock.inline.hpp\"\n+#include \"gc\/x\/xNMethodData.hpp\"\n+#include \"memory\/allocation.hpp\"\n+#include \"runtime\/atomic.hpp\"\n+#include \"utilities\/align.hpp\"\n+#include \"utilities\/debug.hpp\"\n+#include \"utilities\/growableArray.hpp\"\n+\n+XNMethodDataOops* XNMethodDataOops::create(const GrowableArray<oop*>& immediates, bool has_non_immediates) {\n+  return ::new (AttachedArray::alloc(immediates.length())) XNMethodDataOops(immediates, has_non_immediates);\n+}\n+\n+void XNMethodDataOops::destroy(XNMethodDataOops* oops) {\n+  AttachedArray::free(oops);\n+}\n+\n+XNMethodDataOops::XNMethodDataOops(const GrowableArray<oop*>& immediates, bool has_non_immediates) :\n+    _immediates(immediates.length()),\n+    _has_non_immediates(has_non_immediates) {\n+  \/\/ Save all immediate oops\n+  for (size_t i = 0; i < immediates_count(); i++) {\n+    immediates_begin()[i] = immediates.at(int(i));\n+  }\n+}\n+\n+size_t XNMethodDataOops::immediates_count() const {\n+  return _immediates.length();\n+}\n+\n+oop** XNMethodDataOops::immediates_begin() const {\n+  return _immediates(this);\n+}\n+\n+oop** XNMethodDataOops::immediates_end() const {\n+  return immediates_begin() + immediates_count();\n+}\n+\n+bool XNMethodDataOops::has_non_immediates() const {\n+  return _has_non_immediates;\n+}\n+\n+XNMethodData::XNMethodData() :\n+    _lock(),\n+    _oops(NULL) {}\n+\n+XNMethodData::~XNMethodData() {\n+  XNMethodDataOops::destroy(_oops);\n+}\n+\n+XReentrantLock* XNMethodData::lock() {\n+  return &_lock;\n+}\n+\n+XNMethodDataOops* XNMethodData::oops() const {\n+  return Atomic::load_acquire(&_oops);\n+}\n+\n+XNMethodDataOops* XNMethodData::swap_oops(XNMethodDataOops* new_oops) {\n+  XLocker<XReentrantLock> locker(&_lock);\n+  XNMethodDataOops* const old_oops = _oops;\n+  _oops = new_oops;\n+  return old_oops;\n+}\n","filename":"src\/hotspot\/share\/gc\/x\/xNMethodData.cpp","additions":88,"deletions":0,"binary":false,"changes":88,"status":"added"},{"patch":"@@ -0,0 +1,71 @@\n+\/*\n+ * Copyright (c) 2017, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XNMETHODDATA_HPP\n+#define SHARE_GC_X_XNMETHODDATA_HPP\n+\n+#include \"gc\/x\/xAttachedArray.hpp\"\n+#include \"gc\/x\/xLock.hpp\"\n+#include \"memory\/allocation.hpp\"\n+#include \"oops\/oopsHierarchy.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+class nmethod;\n+template <typename T> class GrowableArray;\n+\n+class XNMethodDataOops {\n+private:\n+  typedef XAttachedArray<XNMethodDataOops, oop*> AttachedArray;\n+\n+  const AttachedArray _immediates;\n+  const bool          _has_non_immediates;\n+\n+  XNMethodDataOops(const GrowableArray<oop*>& immediates, bool has_non_immediates);\n+\n+public:\n+  static XNMethodDataOops* create(const GrowableArray<oop*>& immediates, bool has_non_immediates);\n+  static void destroy(XNMethodDataOops* oops);\n+\n+  size_t immediates_count() const;\n+  oop** immediates_begin() const;\n+  oop** immediates_end() const;\n+\n+  bool has_non_immediates() const;\n+};\n+\n+class XNMethodData : public CHeapObj<mtGC> {\n+private:\n+  XReentrantLock             _lock;\n+  XNMethodDataOops* volatile _oops;\n+\n+public:\n+  XNMethodData();\n+  ~XNMethodData();\n+\n+  XReentrantLock* lock();\n+\n+  XNMethodDataOops* oops() const;\n+  XNMethodDataOops* swap_oops(XNMethodDataOops* oops);\n+};\n+\n+#endif \/\/ SHARE_GC_X_XNMETHODDATA_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xNMethodData.hpp","additions":71,"deletions":0,"binary":false,"changes":71,"status":"added"},{"patch":"@@ -0,0 +1,235 @@\n+\/*\n+ * Copyright (c) 2017, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"code\/relocInfo.hpp\"\n+#include \"code\/nmethod.hpp\"\n+#include \"code\/icBuffer.hpp\"\n+#include \"gc\/shared\/barrierSet.hpp\"\n+#include \"gc\/shared\/barrierSetNMethod.hpp\"\n+#include \"gc\/x\/xGlobals.hpp\"\n+#include \"gc\/x\/xHash.inline.hpp\"\n+#include \"gc\/x\/xLock.inline.hpp\"\n+#include \"gc\/x\/xNMethodData.hpp\"\n+#include \"gc\/x\/xNMethodTable.hpp\"\n+#include \"gc\/x\/xNMethodTableEntry.hpp\"\n+#include \"gc\/x\/xNMethodTableIteration.hpp\"\n+#include \"gc\/x\/xSafeDelete.inline.hpp\"\n+#include \"gc\/x\/xTask.hpp\"\n+#include \"gc\/x\/xWorkers.hpp\"\n+#include \"logging\/log.hpp\"\n+#include \"memory\/allocation.hpp\"\n+#include \"memory\/iterator.hpp\"\n+#include \"memory\/resourceArea.hpp\"\n+#include \"runtime\/mutexLocker.hpp\"\n+#include \"utilities\/debug.hpp\"\n+#include \"utilities\/powerOfTwo.hpp\"\n+\n+XNMethodTableEntry* XNMethodTable::_table = NULL;\n+size_t XNMethodTable::_size = 0;\n+size_t XNMethodTable::_nregistered = 0;\n+size_t XNMethodTable::_nunregistered = 0;\n+XNMethodTableIteration XNMethodTable::_iteration;\n+XSafeDeleteNoLock<XNMethodTableEntry[]> XNMethodTable::_safe_delete;\n+\n+size_t XNMethodTable::first_index(const nmethod* nm, size_t size) {\n+  assert(is_power_of_2(size), \"Invalid size\");\n+  const size_t mask = size - 1;\n+  const size_t hash = XHash::address_to_uint32((uintptr_t)nm);\n+  return hash & mask;\n+}\n+\n+size_t XNMethodTable::next_index(size_t prev_index, size_t size) {\n+  assert(is_power_of_2(size), \"Invalid size\");\n+  const size_t mask = size - 1;\n+  return (prev_index + 1) & mask;\n+}\n+\n+bool XNMethodTable::register_entry(XNMethodTableEntry* table, size_t size, nmethod* nm) {\n+  const XNMethodTableEntry entry(nm);\n+  size_t index = first_index(nm, size);\n+\n+  for (;;) {\n+    const XNMethodTableEntry table_entry = table[index];\n+\n+    if (!table_entry.registered() && !table_entry.unregistered()) {\n+      \/\/ Insert new entry\n+      table[index] = entry;\n+      return true;\n+    }\n+\n+    if (table_entry.registered() && table_entry.method() == nm) {\n+      \/\/ Replace existing entry\n+      table[index] = entry;\n+      return false;\n+    }\n+\n+    index = next_index(index, size);\n+  }\n+}\n+\n+void XNMethodTable::unregister_entry(XNMethodTableEntry* table, size_t size, nmethod* nm) {\n+  size_t index = first_index(nm, size);\n+\n+  for (;;) {\n+    const XNMethodTableEntry table_entry = table[index];\n+    assert(table_entry.registered() || table_entry.unregistered(), \"Entry not found\");\n+\n+    if (table_entry.registered() && table_entry.method() == nm) {\n+      \/\/ Remove entry\n+      table[index] = XNMethodTableEntry(true \/* unregistered *\/);\n+      return;\n+    }\n+\n+    index = next_index(index, size);\n+  }\n+}\n+\n+void XNMethodTable::rebuild(size_t new_size) {\n+  assert(CodeCache_lock->owned_by_self(), \"Lock must be held\");\n+\n+  assert(is_power_of_2(new_size), \"Invalid size\");\n+\n+  log_debug(gc, nmethod)(\"Rebuilding NMethod Table: \"\n+                         SIZE_FORMAT \"->\" SIZE_FORMAT \" entries, \"\n+                         SIZE_FORMAT \"(%.0f%%->%.0f%%) registered, \"\n+                         SIZE_FORMAT \"(%.0f%%->%.0f%%) unregistered\",\n+                         _size, new_size,\n+                         _nregistered, percent_of(_nregistered, _size), percent_of(_nregistered, new_size),\n+                         _nunregistered, percent_of(_nunregistered, _size), 0.0);\n+\n+  \/\/ Allocate new table\n+  XNMethodTableEntry* const new_table = new XNMethodTableEntry[new_size];\n+\n+  \/\/ Transfer all registered entries\n+  for (size_t i = 0; i < _size; i++) {\n+    const XNMethodTableEntry entry = _table[i];\n+    if (entry.registered()) {\n+      register_entry(new_table, new_size, entry.method());\n+    }\n+  }\n+\n+  \/\/ Free old table\n+  _safe_delete(_table);\n+\n+  \/\/ Install new table\n+  _table = new_table;\n+  _size = new_size;\n+  _nunregistered = 0;\n+}\n+\n+void XNMethodTable::rebuild_if_needed() {\n+  \/\/ The hash table uses linear probing. To avoid wasting memory while\n+  \/\/ at the same time maintaining good hash collision behavior we want\n+  \/\/ to keep the table occupancy between 30% and 70%. The table always\n+  \/\/ grows\/shrinks by doubling\/halving its size. Pruning of unregistered\n+  \/\/ entries is done by rebuilding the table with or without resizing it.\n+  const size_t min_size = 1024;\n+  const size_t shrink_threshold = _size * 0.30;\n+  const size_t prune_threshold = _size * 0.65;\n+  const size_t grow_threshold = _size * 0.70;\n+\n+  if (_size == 0) {\n+    \/\/ Initialize table\n+    rebuild(min_size);\n+  } else if (_nregistered < shrink_threshold && _size > min_size) {\n+    \/\/ Shrink table\n+    rebuild(_size \/ 2);\n+  } else if (_nregistered + _nunregistered > grow_threshold) {\n+    \/\/ Prune or grow table\n+    if (_nregistered < prune_threshold) {\n+      \/\/ Prune table\n+      rebuild(_size);\n+    } else {\n+      \/\/ Grow table\n+      rebuild(_size * 2);\n+    }\n+  }\n+}\n+\n+size_t XNMethodTable::registered_nmethods() {\n+  return _nregistered;\n+}\n+\n+size_t XNMethodTable::unregistered_nmethods() {\n+  return _nunregistered;\n+}\n+\n+void XNMethodTable::register_nmethod(nmethod* nm) {\n+  assert(CodeCache_lock->owned_by_self(), \"Lock must be held\");\n+\n+  \/\/ Grow\/Shrink\/Prune table if needed\n+  rebuild_if_needed();\n+\n+  \/\/ Insert new entry\n+  if (register_entry(_table, _size, nm)) {\n+    \/\/ New entry registered. When register_entry() instead returns\n+    \/\/ false the nmethod was already in the table so we do not want\n+    \/\/ to increase number of registered entries in that case.\n+    _nregistered++;\n+  }\n+}\n+\n+void XNMethodTable::wait_until_iteration_done() {\n+  assert(CodeCache_lock->owned_by_self(), \"Lock must be held\");\n+\n+  while (_iteration.in_progress()) {\n+    CodeCache_lock->wait_without_safepoint_check();\n+  }\n+}\n+\n+void XNMethodTable::unregister_nmethod(nmethod* nm) {\n+  assert(CodeCache_lock->owned_by_self(), \"Lock must be held\");\n+\n+  \/\/ Remove entry\n+  unregister_entry(_table, _size, nm);\n+  _nunregistered++;\n+  _nregistered--;\n+}\n+\n+void XNMethodTable::nmethods_do_begin() {\n+  MutexLocker mu(CodeCache_lock, Mutex::_no_safepoint_check_flag);\n+\n+  \/\/ Do not allow the table to be deleted while iterating\n+  _safe_delete.enable_deferred_delete();\n+\n+  \/\/ Prepare iteration\n+  _iteration.nmethods_do_begin(_table, _size);\n+}\n+\n+void XNMethodTable::nmethods_do_end() {\n+  MutexLocker mu(CodeCache_lock, Mutex::_no_safepoint_check_flag);\n+\n+  \/\/ Finish iteration\n+  _iteration.nmethods_do_end();\n+\n+  \/\/ Allow the table to be deleted\n+  _safe_delete.disable_deferred_delete();\n+\n+  \/\/ Notify iteration done\n+  CodeCache_lock->notify_all();\n+}\n+\n+void XNMethodTable::nmethods_do(NMethodClosure* cl) {\n+  _iteration.nmethods_do(cl);\n+}\n","filename":"src\/hotspot\/share\/gc\/x\/xNMethodTable.cpp","additions":235,"deletions":0,"binary":false,"changes":235,"status":"added"},{"patch":"@@ -0,0 +1,74 @@\n+\/*\n+ * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XNMETHODTABLE_HPP\n+#define SHARE_GC_X_XNMETHODTABLE_HPP\n+\n+#include \"gc\/x\/xNMethodTableIteration.hpp\"\n+#include \"gc\/x\/xSafeDelete.hpp\"\n+#include \"memory\/allStatic.hpp\"\n+\n+class nmethod;\n+class NMethodClosure;\n+class XNMethodTableEntry;\n+class XWorkers;\n+\n+class XNMethodTable : public AllStatic {\n+private:\n+  static XNMethodTableEntry*                     _table;\n+  static size_t                                  _size;\n+  static size_t                                  _nregistered;\n+  static size_t                                  _nunregistered;\n+  static XNMethodTableIteration                  _iteration;\n+  static XSafeDeleteNoLock<XNMethodTableEntry[]> _safe_delete;\n+\n+  static XNMethodTableEntry* create(size_t size);\n+  static void destroy(XNMethodTableEntry* table);\n+\n+  static size_t first_index(const nmethod* nm, size_t size);\n+  static size_t next_index(size_t prev_index, size_t size);\n+\n+  static bool register_entry(XNMethodTableEntry* table, size_t size, nmethod* nm);\n+  static void unregister_entry(XNMethodTableEntry* table, size_t size, nmethod* nm);\n+\n+  static void rebuild(size_t new_size);\n+  static void rebuild_if_needed();\n+\n+public:\n+  static size_t registered_nmethods();\n+  static size_t unregistered_nmethods();\n+\n+  static void register_nmethod(nmethod* nm);\n+  static void unregister_nmethod(nmethod* nm);\n+\n+  static void wait_until_iteration_done();\n+\n+  static void nmethods_do_begin();\n+  static void nmethods_do_end();\n+  static void nmethods_do(NMethodClosure* cl);\n+\n+  static void unlink(XWorkers* workers, bool unloading_occurred);\n+  static void purge(XWorkers* workers);\n+};\n+\n+#endif \/\/ SHARE_GC_X_XNMETHODTABLE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xNMethodTable.hpp","additions":74,"deletions":0,"binary":false,"changes":74,"status":"added"},{"patch":"@@ -0,0 +1,81 @@\n+\/*\n+ * Copyright (c) 2017, 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XNMETHODTABLEENTRY_HPP\n+#define SHARE_GC_X_XNMETHODTABLEENTRY_HPP\n+\n+#include \"gc\/x\/xBitField.hpp\"\n+#include \"memory\/allocation.hpp\"\n+\n+class nmethod;\n+\n+\/\/\n+\/\/ NMethod table entry layout\n+\/\/ --------------------------\n+\/\/\n+\/\/   6\n+\/\/   3                                                                   2 1 0\n+\/\/  +---------------------------------------------------------------------+-+-+\n+\/\/  |11111111 11111111 11111111 11111111 11111111 11111111 11111111 111111|1|1|\n+\/\/  +---------------------------------------------------------------------+-+-+\n+\/\/  |                                                                     | |\n+\/\/  |                                      1-1 Unregistered Flag (1-bits) * |\n+\/\/  |                                                                       |\n+\/\/  |                                          0-0 Registered Flag (1-bits) *\n+\/\/  |\n+\/\/  * 63-2 NMethod Address (62-bits)\n+\/\/\n+\n+class XNMethodTableEntry : public CHeapObj<mtGC> {\n+private:\n+  typedef XBitField<uint64_t, bool,     0,  1>    field_registered;\n+  typedef XBitField<uint64_t, bool,     1,  1>    field_unregistered;\n+  typedef XBitField<uint64_t, nmethod*, 2, 62, 2> field_method;\n+\n+  uint64_t _entry;\n+\n+public:\n+  explicit XNMethodTableEntry(bool unregistered = false) :\n+      _entry(field_registered::encode(false) |\n+             field_unregistered::encode(unregistered) |\n+             field_method::encode(NULL)) {}\n+\n+  explicit XNMethodTableEntry(nmethod* method) :\n+      _entry(field_registered::encode(true) |\n+             field_unregistered::encode(false) |\n+             field_method::encode(method)) {}\n+\n+  bool registered() const {\n+    return field_registered::decode(_entry);\n+  }\n+\n+  bool unregistered() const {\n+    return field_unregistered::decode(_entry);\n+  }\n+\n+  nmethod* method() const {\n+    return field_method::decode(_entry);\n+  }\n+};\n+\n+#endif \/\/ SHARE_GC_X_XNMETHODTABLEENTRY_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xNMethodTableEntry.hpp","additions":81,"deletions":0,"binary":false,"changes":81,"status":"added"},{"patch":"@@ -0,0 +1,76 @@\n+\/*\n+ * Copyright (c) 2017, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/x\/xNMethodTableEntry.hpp\"\n+#include \"gc\/x\/xNMethodTableIteration.hpp\"\n+#include \"memory\/iterator.hpp\"\n+#include \"runtime\/atomic.hpp\"\n+#include \"utilities\/debug.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+XNMethodTableIteration::XNMethodTableIteration() :\n+    _table(NULL),\n+    _size(0),\n+    _claimed(0) {}\n+\n+bool XNMethodTableIteration::in_progress() const {\n+  return _table != NULL;\n+}\n+\n+void XNMethodTableIteration::nmethods_do_begin(XNMethodTableEntry* table, size_t size) {\n+  assert(!in_progress(), \"precondition\");\n+\n+  _table = table;\n+  _size = size;\n+  _claimed = 0;\n+}\n+\n+void XNMethodTableIteration::nmethods_do_end() {\n+  assert(_claimed >= _size, \"Failed to claim all table entries\");\n+\n+  \/\/ Finish iteration\n+  _table = NULL;\n+}\n+\n+void XNMethodTableIteration::nmethods_do(NMethodClosure* cl) {\n+  for (;;) {\n+    \/\/ Claim table partition. Each partition is currently sized to span\n+    \/\/ two cache lines. This number is just a guess, but seems to work well.\n+    const size_t partition_size = (XCacheLineSize * 2) \/ sizeof(XNMethodTableEntry);\n+    const size_t partition_start = MIN2(Atomic::fetch_and_add(&_claimed, partition_size), _size);\n+    const size_t partition_end = MIN2(partition_start + partition_size, _size);\n+    if (partition_start == partition_end) {\n+      \/\/ End of table\n+      break;\n+    }\n+\n+    \/\/ Process table partition\n+    for (size_t i = partition_start; i < partition_end; i++) {\n+      const XNMethodTableEntry entry = _table[i];\n+      if (entry.registered()) {\n+        cl->do_nmethod(entry.method());\n+      }\n+    }\n+  }\n+}\n","filename":"src\/hotspot\/share\/gc\/x\/xNMethodTableIteration.cpp","additions":76,"deletions":0,"binary":false,"changes":76,"status":"added"},{"patch":"@@ -0,0 +1,48 @@\n+\/*\n+ * Copyright (c) 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XNMETHODTABLEITERATION_HPP\n+#define SHARE_GC_X_XNMETHODTABLEITERATION_HPP\n+\n+#include \"gc\/x\/xGlobals.hpp\"\n+\n+class NMethodClosure;\n+class XNMethodTableEntry;\n+\n+class XNMethodTableIteration {\n+private:\n+  XNMethodTableEntry*            _table;\n+  size_t                         _size;\n+  XCACHE_ALIGNED volatile size_t _claimed;\n+\n+public:\n+  XNMethodTableIteration();\n+\n+  bool in_progress() const;\n+\n+  void nmethods_do_begin(XNMethodTableEntry* table, size_t size);\n+  void nmethods_do_end();\n+  void nmethods_do(NMethodClosure* cl);\n+};\n+\n+#endif \/\/ SHARE_GC_X_XNMETHODTABLEITERATION_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xNMethodTableIteration.hpp","additions":48,"deletions":0,"binary":false,"changes":48,"status":"added"},{"patch":"@@ -0,0 +1,41 @@\n+\/*\n+ * Copyright (c) 2016, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/shared\/gcLogPrecious.hpp\"\n+#include \"gc\/x\/xNUMA.hpp\"\n+\n+bool XNUMA::_enabled;\n+\n+void XNUMA::initialize() {\n+  pd_initialize();\n+\n+  log_info_p(gc, init)(\"NUMA Support: %s\", to_string());\n+  if (_enabled) {\n+    log_info_p(gc, init)(\"NUMA Nodes: %u\", count());\n+  }\n+}\n+\n+const char* XNUMA::to_string() {\n+  return _enabled ? \"Enabled\" : \"Disabled\";\n+}\n","filename":"src\/hotspot\/share\/gc\/x\/xNUMA.cpp","additions":41,"deletions":0,"binary":false,"changes":41,"status":"added"},{"patch":"@@ -0,0 +1,48 @@\n+\/*\n+ * Copyright (c) 2016, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XNUMA_HPP\n+#define SHARE_GC_X_XNUMA_HPP\n+\n+#include \"memory\/allStatic.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+class XNUMA : public AllStatic {\n+private:\n+  static bool _enabled;\n+\n+  static void pd_initialize();\n+\n+public:\n+  static void initialize();\n+  static bool is_enabled();\n+\n+  static uint32_t count();\n+  static uint32_t id();\n+\n+  static uint32_t memory_id(uintptr_t addr);\n+\n+  static const char* to_string();\n+};\n+\n+#endif \/\/ SHARE_GC_X_XNUMA_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xNUMA.hpp","additions":48,"deletions":0,"binary":false,"changes":48,"status":"added"},{"patch":"@@ -0,0 +1,33 @@\n+\/*\n+ * Copyright (c) 2019, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XNUMA_INLINE_HPP\n+#define SHARE_GC_X_XNUMA_INLINE_HPP\n+\n+#include \"gc\/x\/xNUMA.hpp\"\n+\n+inline bool XNUMA::is_enabled() {\n+  return _enabled;\n+}\n+\n+#endif \/\/ SHARE_GC_X_XNUMA_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xNUMA.inline.hpp","additions":33,"deletions":0,"binary":false,"changes":33,"status":"added"},{"patch":"@@ -0,0 +1,93 @@\n+\/*\n+ * Copyright (c) 2019, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/x\/xThreadLocalData.hpp\"\n+#include \"gc\/x\/xObjArrayAllocator.hpp\"\n+#include \"gc\/x\/xUtils.inline.hpp\"\n+#include \"oops\/arrayKlass.hpp\"\n+#include \"runtime\/interfaceSupport.inline.hpp\"\n+#include \"utilities\/debug.hpp\"\n+\n+XObjArrayAllocator::XObjArrayAllocator(Klass* klass, size_t word_size, int length, bool do_zero, Thread* thread) :\n+    ObjArrayAllocator(klass, word_size, length, do_zero, thread) {}\n+\n+void XObjArrayAllocator::yield_for_safepoint() const {\n+  ThreadBlockInVM tbivm(JavaThread::cast(_thread));\n+}\n+\n+oop XObjArrayAllocator::initialize(HeapWord* mem) const {\n+  \/\/ ZGC specializes the initialization by performing segmented clearing\n+  \/\/ to allow shorter time-to-safepoints.\n+\n+  if (!_do_zero) {\n+    \/\/ No need for ZGC specialization\n+    return ObjArrayAllocator::initialize(mem);\n+  }\n+\n+  \/\/ A max segment size of 64K was chosen because microbenchmarking\n+  \/\/ suggested that it offered a good trade-off between allocation\n+  \/\/ time and time-to-safepoint\n+  const size_t segment_max = XUtils::bytes_to_words(64 * K);\n+  const BasicType element_type = ArrayKlass::cast(_klass)->element_type();\n+  const size_t header = arrayOopDesc::header_size(element_type);\n+  const size_t payload_size = _word_size - header;\n+\n+  if (payload_size <= segment_max) {\n+    \/\/ To small to use segmented clearing\n+    return ObjArrayAllocator::initialize(mem);\n+  }\n+\n+  \/\/ Segmented clearing\n+\n+  \/\/ The array is going to be exposed before it has been completely\n+  \/\/ cleared, therefore we can't expose the header at the end of this\n+  \/\/ function. Instead explicitly initialize it according to our needs.\n+  arrayOopDesc::set_mark(mem, markWord::prototype());\n+  arrayOopDesc::release_set_klass(mem, _klass);\n+  assert(_length >= 0, \"length should be non-negative\");\n+  arrayOopDesc::set_length(mem, _length);\n+\n+  \/\/ Keep the array alive across safepoints through an invisible\n+  \/\/ root. Invisible roots are not visited by the heap itarator\n+  \/\/ and the marking logic will not attempt to follow its elements.\n+  \/\/ Relocation knows how to dodge iterating over such objects.\n+  XThreadLocalData::set_invisible_root(_thread, (oop*)&mem);\n+\n+  for (size_t processed = 0; processed < payload_size; processed += segment_max) {\n+    \/\/ Calculate segment\n+    HeapWord* const start = (HeapWord*)(mem + header + processed);\n+    const size_t remaining = payload_size - processed;\n+    const size_t segment_size = MIN2(remaining, segment_max);\n+\n+    \/\/ Clear segment\n+    Copy::zero_to_words(start, segment_size);\n+\n+    \/\/ Safepoint\n+    yield_for_safepoint();\n+  }\n+\n+  XThreadLocalData::clear_invisible_root(_thread);\n+\n+  return cast_to_oop(mem);\n+}\n","filename":"src\/hotspot\/share\/gc\/x\/xObjArrayAllocator.cpp","additions":93,"deletions":0,"binary":false,"changes":93,"status":"added"},{"patch":"@@ -0,0 +1,39 @@\n+\/*\n+ * Copyright (c) 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XOBJARRAYALLOCATOR_HPP\n+#define SHARE_GC_X_XOBJARRAYALLOCATOR_HPP\n+\n+#include \"gc\/shared\/memAllocator.hpp\"\n+\n+class XObjArrayAllocator : public ObjArrayAllocator {\n+private:\n+  virtual oop initialize(HeapWord* mem) const override;\n+\n+  void yield_for_safepoint() const;\n+\n+public:\n+  XObjArrayAllocator(Klass* klass, size_t word_size, int length, bool do_zero, Thread* thread);\n+};\n+\n+#endif \/\/ SHARE_GC_X_XOBJARRAYALLOCATOR_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xObjArrayAllocator.hpp","additions":39,"deletions":0,"binary":false,"changes":39,"status":"added"},{"patch":"@@ -0,0 +1,267 @@\n+\/*\n+ * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/x\/xGlobals.hpp\"\n+#include \"gc\/x\/xHeap.inline.hpp\"\n+#include \"gc\/x\/xHeuristics.hpp\"\n+#include \"gc\/x\/xObjectAllocator.hpp\"\n+#include \"gc\/x\/xPage.inline.hpp\"\n+#include \"gc\/x\/xPageTable.inline.hpp\"\n+#include \"gc\/x\/xStat.hpp\"\n+#include \"gc\/x\/xThread.inline.hpp\"\n+#include \"gc\/x\/xValue.inline.hpp\"\n+#include \"logging\/log.hpp\"\n+#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/safepoint.hpp\"\n+#include \"utilities\/align.hpp\"\n+#include \"utilities\/debug.hpp\"\n+\n+static const XStatCounter XCounterUndoObjectAllocationSucceeded(\"Memory\", \"Undo Object Allocation Succeeded\", XStatUnitOpsPerSecond);\n+static const XStatCounter XCounterUndoObjectAllocationFailed(\"Memory\", \"Undo Object Allocation Failed\", XStatUnitOpsPerSecond);\n+\n+XObjectAllocator::XObjectAllocator() :\n+    _use_per_cpu_shared_small_pages(XHeuristics::use_per_cpu_shared_small_pages()),\n+    _used(0),\n+    _undone(0),\n+    _alloc_for_relocation(0),\n+    _undo_alloc_for_relocation(0),\n+    _shared_medium_page(NULL),\n+    _shared_small_page(NULL) {}\n+\n+XPage** XObjectAllocator::shared_small_page_addr() {\n+  return _use_per_cpu_shared_small_pages ? _shared_small_page.addr() : _shared_small_page.addr(0);\n+}\n+\n+XPage* const* XObjectAllocator::shared_small_page_addr() const {\n+  return _use_per_cpu_shared_small_pages ? _shared_small_page.addr() : _shared_small_page.addr(0);\n+}\n+\n+void XObjectAllocator::register_alloc_for_relocation(const XPageTable* page_table, uintptr_t addr, size_t size) {\n+  const XPage* const page = page_table->get(addr);\n+  const size_t aligned_size = align_up(size, page->object_alignment());\n+  Atomic::add(_alloc_for_relocation.addr(), aligned_size);\n+}\n+\n+void XObjectAllocator::register_undo_alloc_for_relocation(const XPage* page, size_t size) {\n+  const size_t aligned_size = align_up(size, page->object_alignment());\n+  Atomic::add(_undo_alloc_for_relocation.addr(), aligned_size);\n+}\n+\n+XPage* XObjectAllocator::alloc_page(uint8_t type, size_t size, XAllocationFlags flags) {\n+  XPage* const page = XHeap::heap()->alloc_page(type, size, flags);\n+  if (page != NULL) {\n+    \/\/ Increment used bytes\n+    Atomic::add(_used.addr(), size);\n+  }\n+\n+  return page;\n+}\n+\n+void XObjectAllocator::undo_alloc_page(XPage* page) {\n+  \/\/ Increment undone bytes\n+  Atomic::add(_undone.addr(), page->size());\n+\n+  XHeap::heap()->undo_alloc_page(page);\n+}\n+\n+uintptr_t XObjectAllocator::alloc_object_in_shared_page(XPage** shared_page,\n+                                                        uint8_t page_type,\n+                                                        size_t page_size,\n+                                                        size_t size,\n+                                                        XAllocationFlags flags) {\n+  uintptr_t addr = 0;\n+  XPage* page = Atomic::load_acquire(shared_page);\n+\n+  if (page != NULL) {\n+    addr = page->alloc_object_atomic(size);\n+  }\n+\n+  if (addr == 0) {\n+    \/\/ Allocate new page\n+    XPage* const new_page = alloc_page(page_type, page_size, flags);\n+    if (new_page != NULL) {\n+      \/\/ Allocate object before installing the new page\n+      addr = new_page->alloc_object(size);\n+\n+    retry:\n+      \/\/ Install new page\n+      XPage* const prev_page = Atomic::cmpxchg(shared_page, page, new_page);\n+      if (prev_page != page) {\n+        if (prev_page == NULL) {\n+          \/\/ Previous page was retired, retry installing the new page\n+          page = prev_page;\n+          goto retry;\n+        }\n+\n+        \/\/ Another page already installed, try allocation there first\n+        const uintptr_t prev_addr = prev_page->alloc_object_atomic(size);\n+        if (prev_addr == 0) {\n+          \/\/ Allocation failed, retry installing the new page\n+          page = prev_page;\n+          goto retry;\n+        }\n+\n+        \/\/ Allocation succeeded in already installed page\n+        addr = prev_addr;\n+\n+        \/\/ Undo new page allocation\n+        undo_alloc_page(new_page);\n+      }\n+    }\n+  }\n+\n+  return addr;\n+}\n+\n+uintptr_t XObjectAllocator::alloc_large_object(size_t size, XAllocationFlags flags) {\n+  uintptr_t addr = 0;\n+\n+  \/\/ Allocate new large page\n+  const size_t page_size = align_up(size, XGranuleSize);\n+  XPage* const page = alloc_page(XPageTypeLarge, page_size, flags);\n+  if (page != NULL) {\n+    \/\/ Allocate the object\n+    addr = page->alloc_object(size);\n+  }\n+\n+  return addr;\n+}\n+\n+uintptr_t XObjectAllocator::alloc_medium_object(size_t size, XAllocationFlags flags) {\n+  return alloc_object_in_shared_page(_shared_medium_page.addr(), XPageTypeMedium, XPageSizeMedium, size, flags);\n+}\n+\n+uintptr_t XObjectAllocator::alloc_small_object(size_t size, XAllocationFlags flags) {\n+  return alloc_object_in_shared_page(shared_small_page_addr(), XPageTypeSmall, XPageSizeSmall, size, flags);\n+}\n+\n+uintptr_t XObjectAllocator::alloc_object(size_t size, XAllocationFlags flags) {\n+  if (size <= XObjectSizeLimitSmall) {\n+    \/\/ Small\n+    return alloc_small_object(size, flags);\n+  } else if (size <= XObjectSizeLimitMedium) {\n+    \/\/ Medium\n+    return alloc_medium_object(size, flags);\n+  } else {\n+    \/\/ Large\n+    return alloc_large_object(size, flags);\n+  }\n+}\n+\n+uintptr_t XObjectAllocator::alloc_object(size_t size) {\n+  XAllocationFlags flags;\n+  return alloc_object(size, flags);\n+}\n+\n+uintptr_t XObjectAllocator::alloc_object_for_relocation(const XPageTable* page_table, size_t size) {\n+  XAllocationFlags flags;\n+  flags.set_non_blocking();\n+\n+  const uintptr_t addr = alloc_object(size, flags);\n+  if (addr != 0) {\n+    register_alloc_for_relocation(page_table, addr, size);\n+  }\n+\n+  return addr;\n+}\n+\n+void XObjectAllocator::undo_alloc_object_for_relocation(XPage* page, uintptr_t addr, size_t size) {\n+  const uint8_t type = page->type();\n+\n+  if (type == XPageTypeLarge) {\n+    register_undo_alloc_for_relocation(page, size);\n+    undo_alloc_page(page);\n+    XStatInc(XCounterUndoObjectAllocationSucceeded);\n+  } else {\n+    if (page->undo_alloc_object_atomic(addr, size)) {\n+      register_undo_alloc_for_relocation(page, size);\n+      XStatInc(XCounterUndoObjectAllocationSucceeded);\n+    } else {\n+      XStatInc(XCounterUndoObjectAllocationFailed);\n+    }\n+  }\n+}\n+\n+size_t XObjectAllocator::used() const {\n+  size_t total_used = 0;\n+  size_t total_undone = 0;\n+\n+  XPerCPUConstIterator<size_t> iter_used(&_used);\n+  for (const size_t* cpu_used; iter_used.next(&cpu_used);) {\n+    total_used += *cpu_used;\n+  }\n+\n+  XPerCPUConstIterator<size_t> iter_undone(&_undone);\n+  for (const size_t* cpu_undone; iter_undone.next(&cpu_undone);) {\n+    total_undone += *cpu_undone;\n+  }\n+\n+  return total_used - total_undone;\n+}\n+\n+size_t XObjectAllocator::remaining() const {\n+  assert(XThread::is_java(), \"Should be a Java thread\");\n+\n+  const XPage* const page = Atomic::load_acquire(shared_small_page_addr());\n+  if (page != NULL) {\n+    return page->remaining();\n+  }\n+\n+  return 0;\n+}\n+\n+size_t XObjectAllocator::relocated() const {\n+  size_t total_alloc = 0;\n+  size_t total_undo_alloc = 0;\n+\n+  XPerCPUConstIterator<size_t> iter_alloc(&_alloc_for_relocation);\n+  for (const size_t* alloc; iter_alloc.next(&alloc);) {\n+    total_alloc += Atomic::load(alloc);\n+  }\n+\n+  XPerCPUConstIterator<size_t> iter_undo_alloc(&_undo_alloc_for_relocation);\n+  for (const size_t* undo_alloc; iter_undo_alloc.next(&undo_alloc);) {\n+    total_undo_alloc += Atomic::load(undo_alloc);\n+  }\n+\n+  assert(total_alloc >= total_undo_alloc, \"Mismatch\");\n+\n+  return total_alloc - total_undo_alloc;\n+}\n+\n+void XObjectAllocator::retire_pages() {\n+  assert(SafepointSynchronize::is_at_safepoint(), \"Should be at safepoint\");\n+\n+  \/\/ Reset used and undone bytes\n+  _used.set_all(0);\n+  _undone.set_all(0);\n+\n+  \/\/ Reset relocated bytes\n+  _alloc_for_relocation.set_all(0);\n+  _undo_alloc_for_relocation.set_all(0);\n+\n+  \/\/ Reset allocation pages\n+  _shared_medium_page.set(NULL);\n+  _shared_small_page.set_all(NULL);\n+}\n","filename":"src\/hotspot\/share\/gc\/x\/xObjectAllocator.cpp","additions":267,"deletions":0,"binary":false,"changes":267,"status":"added"},{"patch":"@@ -0,0 +1,79 @@\n+\/*\n+ * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XOBJECTALLOCATOR_HPP\n+#define SHARE_GC_X_XOBJECTALLOCATOR_HPP\n+\n+#include \"gc\/x\/xAllocationFlags.hpp\"\n+#include \"gc\/x\/xValue.hpp\"\n+\n+class XPage;\n+class XPageTable;\n+\n+class XObjectAllocator {\n+private:\n+  const bool         _use_per_cpu_shared_small_pages;\n+  XPerCPU<size_t>    _used;\n+  XPerCPU<size_t>    _undone;\n+  XPerCPU<size_t>    _alloc_for_relocation;\n+  XPerCPU<size_t>    _undo_alloc_for_relocation;\n+  XContended<XPage*> _shared_medium_page;\n+  XPerCPU<XPage*>    _shared_small_page;\n+\n+  XPage** shared_small_page_addr();\n+  XPage* const* shared_small_page_addr() const;\n+\n+  void register_alloc_for_relocation(const XPageTable* page_table, uintptr_t addr, size_t size);\n+  void register_undo_alloc_for_relocation(const XPage* page, size_t size);\n+\n+  XPage* alloc_page(uint8_t type, size_t size, XAllocationFlags flags);\n+  void undo_alloc_page(XPage* page);\n+\n+  \/\/ Allocate an object in a shared page. Allocate and\n+  \/\/ atomically install a new page if necessary.\n+  uintptr_t alloc_object_in_shared_page(XPage** shared_page,\n+                                        uint8_t page_type,\n+                                        size_t page_size,\n+                                        size_t size,\n+                                        XAllocationFlags flags);\n+\n+  uintptr_t alloc_large_object(size_t size, XAllocationFlags flags);\n+  uintptr_t alloc_medium_object(size_t size, XAllocationFlags flags);\n+  uintptr_t alloc_small_object(size_t size, XAllocationFlags flags);\n+  uintptr_t alloc_object(size_t size, XAllocationFlags flags);\n+\n+public:\n+  XObjectAllocator();\n+\n+  uintptr_t alloc_object(size_t size);\n+  uintptr_t alloc_object_for_relocation(const XPageTable* page_table, size_t size);\n+  void undo_alloc_object_for_relocation(XPage* page, uintptr_t addr, size_t size);\n+\n+  size_t used() const;\n+  size_t remaining() const;\n+  size_t relocated() const;\n+\n+  void retire_pages();\n+};\n+\n+#endif \/\/ SHARE_GC_X_XOBJECTALLOCATOR_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xObjectAllocator.hpp","additions":79,"deletions":0,"binary":false,"changes":79,"status":"added"},{"patch":"@@ -24,2 +24,2 @@\n-#ifndef SHARE_GC_Z_ZOOP_HPP\n-#define SHARE_GC_Z_ZOOP_HPP\n+#ifndef SHARE_GC_X_XOOP_HPP\n+#define SHARE_GC_X_XOOP_HPP\n@@ -30,1 +30,1 @@\n-class ZOop : public AllStatic {\n+class XOop : public AllStatic {\n@@ -36,1 +36,1 @@\n-#endif \/\/ SHARE_GC_Z_ZOOP_HPP\n+#endif \/\/ SHARE_GC_X_XOOP_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xOop.hpp","additions":4,"deletions":4,"binary":false,"changes":8,"previous_filename":"src\/hotspot\/share\/gc\/z\/zOop.hpp","status":"renamed"},{"patch":"@@ -0,0 +1,37 @@\n+\/*\n+ * Copyright (c) 2016, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XOOP_INLINE_HPP\n+#define SHARE_GC_X_XOOP_INLINE_HPP\n+\n+#include \"gc\/x\/xOop.hpp\"\n+\n+inline oop XOop::from_address(uintptr_t addr) {\n+  return cast_to_oop(addr);\n+}\n+\n+inline uintptr_t XOop::to_address(oop o) {\n+  return cast_from_oop<uintptr_t>(o);\n+}\n+\n+#endif \/\/ SHARE_GC_X_XOOP_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xOop.inline.hpp","additions":37,"deletions":0,"binary":false,"changes":37,"status":"added"},{"patch":"@@ -0,0 +1,135 @@\n+\/*\n+ * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/x\/xList.inline.hpp\"\n+#include \"gc\/x\/xPage.inline.hpp\"\n+#include \"gc\/x\/xPhysicalMemory.inline.hpp\"\n+#include \"gc\/x\/xVirtualMemory.inline.hpp\"\n+#include \"utilities\/align.hpp\"\n+#include \"utilities\/debug.hpp\"\n+\n+XPage::XPage(const XVirtualMemory& vmem, const XPhysicalMemory& pmem) :\n+    XPage(type_from_size(vmem.size()), vmem, pmem) {}\n+\n+XPage::XPage(uint8_t type, const XVirtualMemory& vmem, const XPhysicalMemory& pmem) :\n+    _type(type),\n+    _numa_id((uint8_t)-1),\n+    _seqnum(0),\n+    _virtual(vmem),\n+    _top(start()),\n+    _livemap(object_max_count()),\n+    _last_used(0),\n+    _physical(pmem),\n+    _node() {\n+  assert_initialized();\n+}\n+\n+XPage::~XPage() {}\n+\n+void XPage::assert_initialized() const {\n+  assert(!_virtual.is_null(), \"Should not be null\");\n+  assert(!_physical.is_null(), \"Should not be null\");\n+  assert(_virtual.size() == _physical.size(), \"Virtual\/Physical size mismatch\");\n+  assert((_type == XPageTypeSmall && size() == XPageSizeSmall) ||\n+         (_type == XPageTypeMedium && size() == XPageSizeMedium) ||\n+         (_type == XPageTypeLarge && is_aligned(size(), XGranuleSize)),\n+         \"Page type\/size mismatch\");\n+}\n+\n+void XPage::reset() {\n+  _seqnum = XGlobalSeqNum;\n+  _top = start();\n+  _livemap.reset();\n+  _last_used = 0;\n+}\n+\n+void XPage::reset_for_in_place_relocation() {\n+  _seqnum = XGlobalSeqNum;\n+  _top = start();\n+}\n+\n+XPage* XPage::retype(uint8_t type) {\n+  assert(_type != type, \"Invalid retype\");\n+  _type = type;\n+  _livemap.resize(object_max_count());\n+  return this;\n+}\n+\n+XPage* XPage::split(size_t size) {\n+  return split(type_from_size(size), size);\n+}\n+\n+XPage* XPage::split(uint8_t type, size_t size) {\n+  assert(_virtual.size() > size, \"Invalid split\");\n+\n+  \/\/ Resize this page, keep _numa_id, _seqnum, and _last_used\n+  const XVirtualMemory vmem = _virtual.split(size);\n+  const XPhysicalMemory pmem = _physical.split(size);\n+  _type = type_from_size(_virtual.size());\n+  _top = start();\n+  _livemap.resize(object_max_count());\n+\n+  \/\/ Create new page, inherit _seqnum and _last_used\n+  XPage* const page = new XPage(type, vmem, pmem);\n+  page->_seqnum = _seqnum;\n+  page->_last_used = _last_used;\n+  return page;\n+}\n+\n+XPage* XPage::split_committed() {\n+  \/\/ Split any committed part of this page into a separate page,\n+  \/\/ leaving this page with only uncommitted physical memory.\n+  const XPhysicalMemory pmem = _physical.split_committed();\n+  if (pmem.is_null()) {\n+    \/\/ Nothing committed\n+    return NULL;\n+  }\n+\n+  assert(!_physical.is_null(), \"Should not be null\");\n+\n+  \/\/ Resize this page\n+  const XVirtualMemory vmem = _virtual.split(pmem.size());\n+  _type = type_from_size(_virtual.size());\n+  _top = start();\n+  _livemap.resize(object_max_count());\n+\n+  \/\/ Create new page\n+  return new XPage(vmem, pmem);\n+}\n+\n+void XPage::print_on(outputStream* out) const {\n+  out->print_cr(\" %-6s  \" PTR_FORMAT \" \" PTR_FORMAT \" \" PTR_FORMAT \" %s%s\",\n+                type_to_string(), start(), top(), end(),\n+                is_allocating()  ? \" Allocating\"  : \"\",\n+                is_relocatable() ? \" Relocatable\" : \"\");\n+}\n+\n+void XPage::print() const {\n+  print_on(tty);\n+}\n+\n+void XPage::verify_live(uint32_t live_objects, size_t live_bytes) const {\n+  guarantee(live_objects == _livemap.live_objects(), \"Invalid number of live objects\");\n+  guarantee(live_bytes == _livemap.live_bytes(), \"Invalid number of live bytes\");\n+}\n","filename":"src\/hotspot\/share\/gc\/x\/xPage.cpp","additions":135,"deletions":0,"binary":false,"changes":135,"status":"added"},{"patch":"@@ -0,0 +1,125 @@\n+\/*\n+ * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XPAGE_HPP\n+#define SHARE_GC_X_XPAGE_HPP\n+\n+#include \"gc\/x\/xList.hpp\"\n+#include \"gc\/x\/xLiveMap.hpp\"\n+#include \"gc\/x\/xPhysicalMemory.hpp\"\n+#include \"gc\/x\/xVirtualMemory.hpp\"\n+#include \"memory\/allocation.hpp\"\n+\n+class VMStructs;\n+\n+class XPage : public CHeapObj<mtGC> {\n+  friend class ::VMStructs;\n+  friend class XList<XPage>;\n+\n+private:\n+  uint8_t            _type;\n+  uint8_t            _numa_id;\n+  uint32_t           _seqnum;\n+  XVirtualMemory     _virtual;\n+  volatile uintptr_t _top;\n+  XLiveMap           _livemap;\n+  uint64_t           _last_used;\n+  XPhysicalMemory    _physical;\n+  XListNode<XPage>   _node;\n+\n+  void assert_initialized() const;\n+\n+  uint8_t type_from_size(size_t size) const;\n+  const char* type_to_string() const;\n+\n+  bool is_object_marked(uintptr_t addr) const;\n+  bool is_object_strongly_marked(uintptr_t addr) const;\n+\n+public:\n+  XPage(const XVirtualMemory& vmem, const XPhysicalMemory& pmem);\n+  XPage(uint8_t type, const XVirtualMemory& vmem, const XPhysicalMemory& pmem);\n+  ~XPage();\n+\n+  uint32_t object_max_count() const;\n+  size_t object_alignment_shift() const;\n+  size_t object_alignment() const;\n+\n+  uint8_t type() const;\n+  uintptr_t start() const;\n+  uintptr_t end() const;\n+  size_t size() const;\n+  uintptr_t top() const;\n+  size_t remaining() const;\n+\n+  const XVirtualMemory& virtual_memory() const;\n+  const XPhysicalMemory& physical_memory() const;\n+  XPhysicalMemory& physical_memory();\n+\n+  uint8_t numa_id();\n+\n+  bool is_allocating() const;\n+  bool is_relocatable() const;\n+\n+  uint64_t last_used() const;\n+  void set_last_used();\n+\n+  void reset();\n+  void reset_for_in_place_relocation();\n+\n+  XPage* retype(uint8_t type);\n+  XPage* split(size_t size);\n+  XPage* split(uint8_t type, size_t size);\n+  XPage* split_committed();\n+\n+  bool is_in(uintptr_t addr) const;\n+\n+  bool is_marked() const;\n+  template <bool finalizable> bool is_object_marked(uintptr_t addr) const;\n+  bool is_object_live(uintptr_t addr) const;\n+  bool is_object_strongly_live(uintptr_t addr) const;\n+  bool mark_object(uintptr_t addr, bool finalizable, bool& inc_live);\n+\n+  void inc_live(uint32_t objects, size_t bytes);\n+  uint32_t live_objects() const;\n+  size_t live_bytes() const;\n+\n+  void object_iterate(ObjectClosure* cl);\n+\n+  uintptr_t alloc_object(size_t size);\n+  uintptr_t alloc_object_atomic(size_t size);\n+\n+  bool undo_alloc_object(uintptr_t addr, size_t size);\n+  bool undo_alloc_object_atomic(uintptr_t addr, size_t size);\n+\n+  void print_on(outputStream* out) const;\n+  void print() const;\n+\n+  void verify_live(uint32_t live_objects, size_t live_bytes) const;\n+};\n+\n+class XPageClosure {\n+public:\n+  virtual void do_page(const XPage* page) = 0;\n+};\n+\n+#endif \/\/ SHARE_GC_X_XPAGE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xPage.hpp","additions":125,"deletions":0,"binary":false,"changes":125,"status":"added"},{"patch":"@@ -0,0 +1,312 @@\n+\/*\n+ * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XPAGE_INLINE_HPP\n+#define SHARE_GC_X_XPAGE_INLINE_HPP\n+\n+#include \"gc\/x\/xPage.hpp\"\n+\n+#include \"gc\/x\/xAddress.inline.hpp\"\n+#include \"gc\/x\/xGlobals.hpp\"\n+#include \"gc\/x\/xLiveMap.inline.hpp\"\n+#include \"gc\/x\/xNUMA.hpp\"\n+#include \"gc\/x\/xPhysicalMemory.inline.hpp\"\n+#include \"gc\/x\/xVirtualMemory.inline.hpp\"\n+#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/os.hpp\"\n+#include \"utilities\/align.hpp\"\n+#include \"utilities\/debug.hpp\"\n+\n+inline uint8_t XPage::type_from_size(size_t size) const {\n+  if (size == XPageSizeSmall) {\n+    return XPageTypeSmall;\n+  } else if (size == XPageSizeMedium) {\n+    return XPageTypeMedium;\n+  } else {\n+    return XPageTypeLarge;\n+  }\n+}\n+\n+inline const char* XPage::type_to_string() const {\n+  switch (type()) {\n+  case XPageTypeSmall:\n+    return \"Small\";\n+\n+  case XPageTypeMedium:\n+    return \"Medium\";\n+\n+  default:\n+    assert(type() == XPageTypeLarge, \"Invalid page type\");\n+    return \"Large\";\n+  }\n+}\n+\n+inline uint32_t XPage::object_max_count() const {\n+  switch (type()) {\n+  case XPageTypeLarge:\n+    \/\/ A large page can only contain a single\n+    \/\/ object aligned to the start of the page.\n+    return 1;\n+\n+  default:\n+    return (uint32_t)(size() >> object_alignment_shift());\n+  }\n+}\n+\n+inline size_t XPage::object_alignment_shift() const {\n+  switch (type()) {\n+  case XPageTypeSmall:\n+    return XObjectAlignmentSmallShift;\n+\n+  case XPageTypeMedium:\n+    return XObjectAlignmentMediumShift;\n+\n+  default:\n+    assert(type() == XPageTypeLarge, \"Invalid page type\");\n+    return XObjectAlignmentLargeShift;\n+  }\n+}\n+\n+inline size_t XPage::object_alignment() const {\n+  switch (type()) {\n+  case XPageTypeSmall:\n+    return XObjectAlignmentSmall;\n+\n+  case XPageTypeMedium:\n+    return XObjectAlignmentMedium;\n+\n+  default:\n+    assert(type() == XPageTypeLarge, \"Invalid page type\");\n+    return XObjectAlignmentLarge;\n+  }\n+}\n+\n+inline uint8_t XPage::type() const {\n+  return _type;\n+}\n+\n+inline uintptr_t XPage::start() const {\n+  return _virtual.start();\n+}\n+\n+inline uintptr_t XPage::end() const {\n+  return _virtual.end();\n+}\n+\n+inline size_t XPage::size() const {\n+  return _virtual.size();\n+}\n+\n+inline uintptr_t XPage::top() const {\n+  return _top;\n+}\n+\n+inline size_t XPage::remaining() const {\n+  return end() - top();\n+}\n+\n+inline const XVirtualMemory& XPage::virtual_memory() const {\n+  return _virtual;\n+}\n+\n+inline const XPhysicalMemory& XPage::physical_memory() const {\n+  return _physical;\n+}\n+\n+inline XPhysicalMemory& XPage::physical_memory() {\n+  return _physical;\n+}\n+\n+inline uint8_t XPage::numa_id() {\n+  if (_numa_id == (uint8_t)-1) {\n+    _numa_id = XNUMA::memory_id(XAddress::good(start()));\n+  }\n+\n+  return _numa_id;\n+}\n+\n+inline bool XPage::is_allocating() const {\n+  return _seqnum == XGlobalSeqNum;\n+}\n+\n+inline bool XPage::is_relocatable() const {\n+  return _seqnum < XGlobalSeqNum;\n+}\n+\n+inline uint64_t XPage::last_used() const {\n+  return _last_used;\n+}\n+\n+inline void XPage::set_last_used() {\n+  _last_used = ceil(os::elapsedTime());\n+}\n+\n+inline bool XPage::is_in(uintptr_t addr) const {\n+  const uintptr_t offset = XAddress::offset(addr);\n+  return offset >= start() && offset < top();\n+}\n+\n+inline bool XPage::is_marked() const {\n+  assert(is_relocatable(), \"Invalid page state\");\n+  return _livemap.is_marked();\n+}\n+\n+inline bool XPage::is_object_marked(uintptr_t addr) const {\n+  assert(is_relocatable(), \"Invalid page state\");\n+  const size_t index = ((XAddress::offset(addr) - start()) >> object_alignment_shift()) * 2;\n+  return _livemap.get(index);\n+}\n+\n+inline bool XPage::is_object_strongly_marked(uintptr_t addr) const {\n+  assert(is_relocatable(), \"Invalid page state\");\n+  const size_t index = ((XAddress::offset(addr) - start()) >> object_alignment_shift()) * 2;\n+  return _livemap.get(index + 1);\n+}\n+\n+template <bool finalizable>\n+inline bool XPage::is_object_marked(uintptr_t addr) const {\n+  return finalizable ? is_object_marked(addr) : is_object_strongly_marked(addr);\n+}\n+\n+inline bool XPage::is_object_live(uintptr_t addr) const {\n+  return is_allocating() || is_object_marked(addr);\n+}\n+\n+inline bool XPage::is_object_strongly_live(uintptr_t addr) const {\n+  return is_allocating() || is_object_strongly_marked(addr);\n+}\n+\n+inline bool XPage::mark_object(uintptr_t addr, bool finalizable, bool& inc_live) {\n+  assert(XAddress::is_marked(addr), \"Invalid address\");\n+  assert(is_relocatable(), \"Invalid page state\");\n+  assert(is_in(addr), \"Invalid address\");\n+\n+  \/\/ Set mark bit\n+  const size_t index = ((XAddress::offset(addr) - start()) >> object_alignment_shift()) * 2;\n+  return _livemap.set(index, finalizable, inc_live);\n+}\n+\n+inline void XPage::inc_live(uint32_t objects, size_t bytes) {\n+  _livemap.inc_live(objects, bytes);\n+}\n+\n+inline uint32_t XPage::live_objects() const {\n+  assert(is_marked(), \"Should be marked\");\n+  return _livemap.live_objects();\n+}\n+\n+inline size_t XPage::live_bytes() const {\n+  assert(is_marked(), \"Should be marked\");\n+  return _livemap.live_bytes();\n+}\n+\n+inline void XPage::object_iterate(ObjectClosure* cl) {\n+  _livemap.iterate(cl, XAddress::good(start()), object_alignment_shift());\n+}\n+\n+inline uintptr_t XPage::alloc_object(size_t size) {\n+  assert(is_allocating(), \"Invalid state\");\n+\n+  const size_t aligned_size = align_up(size, object_alignment());\n+  const uintptr_t addr = top();\n+  const uintptr_t new_top = addr + aligned_size;\n+\n+  if (new_top > end()) {\n+    \/\/ Not enough space left\n+    return 0;\n+  }\n+\n+  _top = new_top;\n+\n+  return XAddress::good(addr);\n+}\n+\n+inline uintptr_t XPage::alloc_object_atomic(size_t size) {\n+  assert(is_allocating(), \"Invalid state\");\n+\n+  const size_t aligned_size = align_up(size, object_alignment());\n+  uintptr_t addr = top();\n+\n+  for (;;) {\n+    const uintptr_t new_top = addr + aligned_size;\n+    if (new_top > end()) {\n+      \/\/ Not enough space left\n+      return 0;\n+    }\n+\n+    const uintptr_t prev_top = Atomic::cmpxchg(&_top, addr, new_top);\n+    if (prev_top == addr) {\n+      \/\/ Success\n+      return XAddress::good(addr);\n+    }\n+\n+    \/\/ Retry\n+    addr = prev_top;\n+  }\n+}\n+\n+inline bool XPage::undo_alloc_object(uintptr_t addr, size_t size) {\n+  assert(is_allocating(), \"Invalid state\");\n+\n+  const uintptr_t offset = XAddress::offset(addr);\n+  const size_t aligned_size = align_up(size, object_alignment());\n+  const uintptr_t old_top = top();\n+  const uintptr_t new_top = old_top - aligned_size;\n+\n+  if (new_top != offset) {\n+    \/\/ Failed to undo allocation, not the last allocated object\n+    return false;\n+  }\n+\n+  _top = new_top;\n+\n+  \/\/ Success\n+  return true;\n+}\n+\n+inline bool XPage::undo_alloc_object_atomic(uintptr_t addr, size_t size) {\n+  assert(is_allocating(), \"Invalid state\");\n+\n+  const uintptr_t offset = XAddress::offset(addr);\n+  const size_t aligned_size = align_up(size, object_alignment());\n+  uintptr_t old_top = top();\n+\n+  for (;;) {\n+    const uintptr_t new_top = old_top - aligned_size;\n+    if (new_top != offset) {\n+      \/\/ Failed to undo allocation, not the last allocated object\n+      return false;\n+    }\n+\n+    const uintptr_t prev_top = Atomic::cmpxchg(&_top, old_top, new_top);\n+    if (prev_top == old_top) {\n+      \/\/ Success\n+      return true;\n+    }\n+\n+    \/\/ Retry\n+    old_top = prev_top;\n+  }\n+}\n+\n+#endif \/\/ SHARE_GC_X_XPAGE_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xPage.inline.hpp","additions":312,"deletions":0,"binary":false,"changes":312,"status":"added"},{"patch":"@@ -0,0 +1,870 @@\n+\/*\n+ * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/shared\/gcLogPrecious.hpp\"\n+#include \"gc\/shared\/suspendibleThreadSet.hpp\"\n+#include \"gc\/x\/xArray.inline.hpp\"\n+#include \"gc\/x\/xCollectedHeap.hpp\"\n+#include \"gc\/x\/xFuture.inline.hpp\"\n+#include \"gc\/x\/xGlobals.hpp\"\n+#include \"gc\/x\/xLock.inline.hpp\"\n+#include \"gc\/x\/xPage.inline.hpp\"\n+#include \"gc\/x\/xPageAllocator.inline.hpp\"\n+#include \"gc\/x\/xPageCache.hpp\"\n+#include \"gc\/x\/xSafeDelete.inline.hpp\"\n+#include \"gc\/x\/xStat.hpp\"\n+#include \"gc\/x\/xTask.hpp\"\n+#include \"gc\/x\/xUncommitter.hpp\"\n+#include \"gc\/x\/xUnmapper.hpp\"\n+#include \"gc\/x\/xWorkers.hpp\"\n+#include \"jfr\/jfrEvents.hpp\"\n+#include \"logging\/log.hpp\"\n+#include \"runtime\/globals.hpp\"\n+#include \"runtime\/init.hpp\"\n+#include \"runtime\/java.hpp\"\n+#include \"utilities\/debug.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+static const XStatCounter       XCounterAllocationRate(\"Memory\", \"Allocation Rate\", XStatUnitBytesPerSecond);\n+static const XStatCounter       XCounterPageCacheFlush(\"Memory\", \"Page Cache Flush\", XStatUnitBytesPerSecond);\n+static const XStatCounter       XCounterDefragment(\"Memory\", \"Defragment\", XStatUnitOpsPerSecond);\n+static const XStatCriticalPhase XCriticalPhaseAllocationStall(\"Allocation Stall\");\n+\n+enum XPageAllocationStall {\n+  XPageAllocationStallSuccess,\n+  XPageAllocationStallFailed,\n+  XPageAllocationStallStartGC\n+};\n+\n+class XPageAllocation : public StackObj {\n+  friend class XList<XPageAllocation>;\n+\n+private:\n+  const uint8_t                 _type;\n+  const size_t                  _size;\n+  const XAllocationFlags        _flags;\n+  const uint32_t                _seqnum;\n+  size_t                        _flushed;\n+  size_t                        _committed;\n+  XList<XPage>                  _pages;\n+  XListNode<XPageAllocation>    _node;\n+  XFuture<XPageAllocationStall> _stall_result;\n+\n+public:\n+  XPageAllocation(uint8_t type, size_t size, XAllocationFlags flags) :\n+      _type(type),\n+      _size(size),\n+      _flags(flags),\n+      _seqnum(XGlobalSeqNum),\n+      _flushed(0),\n+      _committed(0),\n+      _pages(),\n+      _node(),\n+      _stall_result() {}\n+\n+  uint8_t type() const {\n+    return _type;\n+  }\n+\n+  size_t size() const {\n+    return _size;\n+  }\n+\n+  XAllocationFlags flags() const {\n+    return _flags;\n+  }\n+\n+  uint32_t seqnum() const {\n+    return _seqnum;\n+  }\n+\n+  size_t flushed() const {\n+    return _flushed;\n+  }\n+\n+  void set_flushed(size_t flushed) {\n+    _flushed = flushed;\n+  }\n+\n+  size_t committed() const {\n+    return _committed;\n+  }\n+\n+  void set_committed(size_t committed) {\n+    _committed = committed;\n+  }\n+\n+  XPageAllocationStall wait() {\n+    return _stall_result.get();\n+  }\n+\n+  XList<XPage>* pages() {\n+    return &_pages;\n+  }\n+\n+  void satisfy(XPageAllocationStall result) {\n+    _stall_result.set(result);\n+  }\n+};\n+\n+XPageAllocator::XPageAllocator(XWorkers* workers,\n+                               size_t min_capacity,\n+                               size_t initial_capacity,\n+                               size_t max_capacity) :\n+    _lock(),\n+    _cache(),\n+    _virtual(max_capacity),\n+    _physical(max_capacity),\n+    _min_capacity(min_capacity),\n+    _max_capacity(max_capacity),\n+    _current_max_capacity(max_capacity),\n+    _capacity(0),\n+    _claimed(0),\n+    _used(0),\n+    _used_high(0),\n+    _used_low(0),\n+    _reclaimed(0),\n+    _stalled(),\n+    _nstalled(0),\n+    _satisfied(),\n+    _unmapper(new XUnmapper(this)),\n+    _uncommitter(new XUncommitter(this)),\n+    _safe_delete(),\n+    _initialized(false) {\n+\n+  if (!_virtual.is_initialized() || !_physical.is_initialized()) {\n+    return;\n+  }\n+\n+  log_info_p(gc, init)(\"Min Capacity: \" SIZE_FORMAT \"M\", min_capacity \/ M);\n+  log_info_p(gc, init)(\"Initial Capacity: \" SIZE_FORMAT \"M\", initial_capacity \/ M);\n+  log_info_p(gc, init)(\"Max Capacity: \" SIZE_FORMAT \"M\", max_capacity \/ M);\n+  if (XPageSizeMedium > 0) {\n+    log_info_p(gc, init)(\"Medium Page Size: \" SIZE_FORMAT \"M\", XPageSizeMedium \/ M);\n+  } else {\n+    log_info_p(gc, init)(\"Medium Page Size: N\/A\");\n+  }\n+  log_info_p(gc, init)(\"Pre-touch: %s\", AlwaysPreTouch ? \"Enabled\" : \"Disabled\");\n+\n+  \/\/ Warn if system limits could stop us from reaching max capacity\n+  _physical.warn_commit_limits(max_capacity);\n+\n+  \/\/ Check if uncommit should and can be enabled\n+  _physical.try_enable_uncommit(min_capacity, max_capacity);\n+\n+  \/\/ Pre-map initial capacity\n+  if (!prime_cache(workers, initial_capacity)) {\n+    log_error_p(gc)(\"Failed to allocate initial Java heap (\" SIZE_FORMAT \"M)\", initial_capacity \/ M);\n+    return;\n+  }\n+\n+  \/\/ Successfully initialized\n+  _initialized = true;\n+}\n+\n+class XPreTouchTask : public XTask {\n+private:\n+  const XPhysicalMemoryManager* const _physical;\n+  volatile uintptr_t                  _start;\n+  const uintptr_t                     _end;\n+\n+public:\n+  XPreTouchTask(const XPhysicalMemoryManager* physical, uintptr_t start, uintptr_t end) :\n+      XTask(\"XPreTouchTask\"),\n+      _physical(physical),\n+      _start(start),\n+      _end(end) {}\n+\n+  virtual void work() {\n+    for (;;) {\n+      \/\/ Get granule offset\n+      const size_t size = XGranuleSize;\n+      const uintptr_t offset = Atomic::fetch_and_add(&_start, size);\n+      if (offset >= _end) {\n+        \/\/ Done\n+        break;\n+      }\n+\n+      \/\/ Pre-touch granule\n+      _physical->pretouch(offset, size);\n+    }\n+  }\n+};\n+\n+bool XPageAllocator::prime_cache(XWorkers* workers, size_t size) {\n+  XAllocationFlags flags;\n+\n+  flags.set_non_blocking();\n+  flags.set_low_address();\n+\n+  XPage* const page = alloc_page(XPageTypeLarge, size, flags);\n+  if (page == NULL) {\n+    return false;\n+  }\n+\n+  if (AlwaysPreTouch) {\n+    \/\/ Pre-touch page\n+    XPreTouchTask task(&_physical, page->start(), page->end());\n+    workers->run_all(&task);\n+  }\n+\n+  free_page(page, false \/* reclaimed *\/);\n+\n+  return true;\n+}\n+\n+bool XPageAllocator::is_initialized() const {\n+  return _initialized;\n+}\n+\n+size_t XPageAllocator::min_capacity() const {\n+  return _min_capacity;\n+}\n+\n+size_t XPageAllocator::max_capacity() const {\n+  return _max_capacity;\n+}\n+\n+size_t XPageAllocator::soft_max_capacity() const {\n+  \/\/ Note that SoftMaxHeapSize is a manageable flag\n+  const size_t soft_max_capacity = Atomic::load(&SoftMaxHeapSize);\n+  const size_t current_max_capacity = Atomic::load(&_current_max_capacity);\n+  return MIN2(soft_max_capacity, current_max_capacity);\n+}\n+\n+size_t XPageAllocator::capacity() const {\n+  return Atomic::load(&_capacity);\n+}\n+\n+size_t XPageAllocator::used() const {\n+  return Atomic::load(&_used);\n+}\n+\n+size_t XPageAllocator::unused() const {\n+  const ssize_t capacity = (ssize_t)Atomic::load(&_capacity);\n+  const ssize_t used = (ssize_t)Atomic::load(&_used);\n+  const ssize_t claimed = (ssize_t)Atomic::load(&_claimed);\n+  const ssize_t unused = capacity - used - claimed;\n+  return unused > 0 ? (size_t)unused : 0;\n+}\n+\n+XPageAllocatorStats XPageAllocator::stats() const {\n+  XLocker<XLock> locker(&_lock);\n+  return XPageAllocatorStats(_min_capacity,\n+                             _max_capacity,\n+                             soft_max_capacity(),\n+                             _capacity,\n+                             _used,\n+                             _used_high,\n+                             _used_low,\n+                             _reclaimed);\n+}\n+\n+void XPageAllocator::reset_statistics() {\n+  assert(SafepointSynchronize::is_at_safepoint(), \"Should be at safepoint\");\n+  _reclaimed = 0;\n+  _used_high = _used_low = _used;\n+  _nstalled = 0;\n+}\n+\n+size_t XPageAllocator::increase_capacity(size_t size) {\n+  const size_t increased = MIN2(size, _current_max_capacity - _capacity);\n+\n+  if (increased > 0) {\n+    \/\/ Update atomically since we have concurrent readers\n+    Atomic::add(&_capacity, increased);\n+\n+    \/\/ Record time of last commit. When allocation, we prefer increasing\n+    \/\/ the capacity over flushing the cache. That means there could be\n+    \/\/ expired pages in the cache at this time. However, since we are\n+    \/\/ increasing the capacity we are obviously in need of committed\n+    \/\/ memory and should therefore not be uncommitting memory.\n+    _cache.set_last_commit();\n+  }\n+\n+  return increased;\n+}\n+\n+void XPageAllocator::decrease_capacity(size_t size, bool set_max_capacity) {\n+  \/\/ Update atomically since we have concurrent readers\n+  Atomic::sub(&_capacity, size);\n+\n+  if (set_max_capacity) {\n+    \/\/ Adjust current max capacity to avoid further attempts to increase capacity\n+    log_error_p(gc)(\"Forced to lower max Java heap size from \"\n+                    SIZE_FORMAT \"M(%.0f%%) to \" SIZE_FORMAT \"M(%.0f%%)\",\n+                    _current_max_capacity \/ M, percent_of(_current_max_capacity, _max_capacity),\n+                    _capacity \/ M, percent_of(_capacity, _max_capacity));\n+\n+    \/\/ Update atomically since we have concurrent readers\n+    Atomic::store(&_current_max_capacity, _capacity);\n+  }\n+}\n+\n+void XPageAllocator::increase_used(size_t size, bool worker_relocation) {\n+  if (worker_relocation) {\n+    \/\/ Allocating a page for the purpose of worker relocation has\n+    \/\/ a negative contribution to the number of reclaimed bytes.\n+    _reclaimed -= size;\n+  }\n+\n+  \/\/ Update atomically since we have concurrent readers\n+  const size_t used = Atomic::add(&_used, size);\n+  if (used > _used_high) {\n+    _used_high = used;\n+  }\n+}\n+\n+void XPageAllocator::decrease_used(size_t size, bool reclaimed) {\n+  \/\/ Only pages explicitly released with the reclaimed flag set\n+  \/\/ counts as reclaimed bytes. This flag is true when we release\n+  \/\/ a page after relocation, and is false when we release a page\n+  \/\/ to undo an allocation.\n+  if (reclaimed) {\n+    _reclaimed += size;\n+  }\n+\n+  \/\/ Update atomically since we have concurrent readers\n+  const size_t used = Atomic::sub(&_used, size);\n+  if (used < _used_low) {\n+    _used_low = used;\n+  }\n+}\n+\n+bool XPageAllocator::commit_page(XPage* page) {\n+  \/\/ Commit physical memory\n+  return _physical.commit(page->physical_memory());\n+}\n+\n+void XPageAllocator::uncommit_page(XPage* page) {\n+  if (!ZUncommit) {\n+    return;\n+  }\n+\n+  \/\/ Uncommit physical memory\n+  _physical.uncommit(page->physical_memory());\n+}\n+\n+void XPageAllocator::map_page(const XPage* page) const {\n+  \/\/ Map physical memory\n+  _physical.map(page->start(), page->physical_memory());\n+}\n+\n+void XPageAllocator::unmap_page(const XPage* page) const {\n+  \/\/ Unmap physical memory\n+  _physical.unmap(page->start(), page->size());\n+}\n+\n+void XPageAllocator::destroy_page(XPage* page) {\n+  \/\/ Free virtual memory\n+  _virtual.free(page->virtual_memory());\n+\n+  \/\/ Free physical memory\n+  _physical.free(page->physical_memory());\n+\n+  \/\/ Delete page safely\n+  _safe_delete(page);\n+}\n+\n+bool XPageAllocator::is_alloc_allowed(size_t size) const {\n+  const size_t available = _current_max_capacity - _used - _claimed;\n+  return available >= size;\n+}\n+\n+bool XPageAllocator::alloc_page_common_inner(uint8_t type, size_t size, XList<XPage>* pages) {\n+  if (!is_alloc_allowed(size)) {\n+    \/\/ Out of memory\n+    return false;\n+  }\n+\n+  \/\/ Try allocate from the page cache\n+  XPage* const page = _cache.alloc_page(type, size);\n+  if (page != NULL) {\n+    \/\/ Success\n+    pages->insert_last(page);\n+    return true;\n+  }\n+\n+  \/\/ Try increase capacity\n+  const size_t increased = increase_capacity(size);\n+  if (increased < size) {\n+    \/\/ Could not increase capacity enough to satisfy the allocation\n+    \/\/ completely. Flush the page cache to satisfy the remainder.\n+    const size_t remaining = size - increased;\n+    _cache.flush_for_allocation(remaining, pages);\n+  }\n+\n+  \/\/ Success\n+  return true;\n+}\n+\n+bool XPageAllocator::alloc_page_common(XPageAllocation* allocation) {\n+  const uint8_t type = allocation->type();\n+  const size_t size = allocation->size();\n+  const XAllocationFlags flags = allocation->flags();\n+  XList<XPage>* const pages = allocation->pages();\n+\n+  if (!alloc_page_common_inner(type, size, pages)) {\n+    \/\/ Out of memory\n+    return false;\n+  }\n+\n+  \/\/ Updated used statistics\n+  increase_used(size, flags.worker_relocation());\n+\n+  \/\/ Success\n+  return true;\n+}\n+\n+static void check_out_of_memory_during_initialization() {\n+  if (!is_init_completed()) {\n+    vm_exit_during_initialization(\"java.lang.OutOfMemoryError\", \"Java heap too small\");\n+  }\n+}\n+\n+bool XPageAllocator::alloc_page_stall(XPageAllocation* allocation) {\n+  XStatTimer timer(XCriticalPhaseAllocationStall);\n+  EventZAllocationStall event;\n+  XPageAllocationStall result;\n+\n+  \/\/ We can only block if the VM is fully initialized\n+  check_out_of_memory_during_initialization();\n+\n+  \/\/ Increment stalled counter\n+  Atomic::inc(&_nstalled);\n+\n+  do {\n+    \/\/ Start asynchronous GC\n+    XCollectedHeap::heap()->collect(GCCause::_z_allocation_stall);\n+\n+    \/\/ Wait for allocation to complete, fail or request a GC\n+    result = allocation->wait();\n+  } while (result == XPageAllocationStallStartGC);\n+\n+  {\n+    \/\/\n+    \/\/ We grab the lock here for two different reasons:\n+    \/\/\n+    \/\/ 1) Guard deletion of underlying semaphore. This is a workaround for\n+    \/\/ a bug in sem_post() in glibc < 2.21, where it's not safe to destroy\n+    \/\/ the semaphore immediately after returning from sem_wait(). The\n+    \/\/ reason is that sem_post() can touch the semaphore after a waiting\n+    \/\/ thread have returned from sem_wait(). To avoid this race we are\n+    \/\/ forcing the waiting thread to acquire\/release the lock held by the\n+    \/\/ posting thread. https:\/\/sourceware.org\/bugzilla\/show_bug.cgi?id=12674\n+    \/\/\n+    \/\/ 2) Guard the list of satisfied pages.\n+    \/\/\n+    XLocker<XLock> locker(&_lock);\n+    _satisfied.remove(allocation);\n+  }\n+\n+  \/\/ Send event\n+  event.commit(allocation->type(), allocation->size());\n+\n+  return (result == XPageAllocationStallSuccess);\n+}\n+\n+bool XPageAllocator::alloc_page_or_stall(XPageAllocation* allocation) {\n+  {\n+    XLocker<XLock> locker(&_lock);\n+\n+    if (alloc_page_common(allocation)) {\n+      \/\/ Success\n+      return true;\n+    }\n+\n+    \/\/ Failed\n+    if (allocation->flags().non_blocking()) {\n+      \/\/ Don't stall\n+      return false;\n+    }\n+\n+    \/\/ Enqueue allocation request\n+    _stalled.insert_last(allocation);\n+  }\n+\n+  \/\/ Stall\n+  return alloc_page_stall(allocation);\n+}\n+\n+XPage* XPageAllocator::alloc_page_create(XPageAllocation* allocation) {\n+  const size_t size = allocation->size();\n+\n+  \/\/ Allocate virtual memory. To make error handling a lot more straight\n+  \/\/ forward, we allocate virtual memory before destroying flushed pages.\n+  \/\/ Flushed pages are also unmapped and destroyed asynchronously, so we\n+  \/\/ can't immediately reuse that part of the address space anyway.\n+  const XVirtualMemory vmem = _virtual.alloc(size, allocation->flags().low_address());\n+  if (vmem.is_null()) {\n+    log_error(gc)(\"Out of address space\");\n+    return NULL;\n+  }\n+\n+  XPhysicalMemory pmem;\n+  size_t flushed = 0;\n+\n+  \/\/ Harvest physical memory from flushed pages\n+  XListRemoveIterator<XPage> iter(allocation->pages());\n+  for (XPage* page; iter.next(&page);) {\n+    flushed += page->size();\n+\n+    \/\/ Harvest flushed physical memory\n+    XPhysicalMemory& fmem = page->physical_memory();\n+    pmem.add_segments(fmem);\n+    fmem.remove_segments();\n+\n+    \/\/ Unmap and destroy page\n+    _unmapper->unmap_and_destroy_page(page);\n+  }\n+\n+  if (flushed > 0) {\n+    allocation->set_flushed(flushed);\n+\n+    \/\/ Update statistics\n+    XStatInc(XCounterPageCacheFlush, flushed);\n+    log_debug(gc, heap)(\"Page Cache Flushed: \" SIZE_FORMAT \"M\", flushed \/ M);\n+  }\n+\n+  \/\/ Allocate any remaining physical memory. Capacity and used has\n+  \/\/ already been adjusted, we just need to fetch the memory, which\n+  \/\/ is guaranteed to succeed.\n+  if (flushed < size) {\n+    const size_t remaining = size - flushed;\n+    allocation->set_committed(remaining);\n+    _physical.alloc(pmem, remaining);\n+  }\n+\n+  \/\/ Create new page\n+  return new XPage(allocation->type(), vmem, pmem);\n+}\n+\n+bool XPageAllocator::should_defragment(const XPage* page) const {\n+  \/\/ A small page can end up at a high address (second half of the address space)\n+  \/\/ if we've split a larger page or we have a constrained address space. To help\n+  \/\/ fight address space fragmentation we remap such pages to a lower address, if\n+  \/\/ a lower address is available.\n+  return page->type() == XPageTypeSmall &&\n+         page->start() >= _virtual.reserved() \/ 2 &&\n+         page->start() > _virtual.lowest_available_address();\n+}\n+\n+bool XPageAllocator::is_alloc_satisfied(XPageAllocation* allocation) const {\n+  \/\/ The allocation is immediately satisfied if the list of pages contains\n+  \/\/ exactly one page, with the type and size that was requested. However,\n+  \/\/ even if the allocation is immediately satisfied we might still want to\n+  \/\/ return false here to force the page to be remapped to fight address\n+  \/\/ space fragmentation.\n+\n+  if (allocation->pages()->size() != 1) {\n+    \/\/ Not a single page\n+    return false;\n+  }\n+\n+  const XPage* const page = allocation->pages()->first();\n+  if (page->type() != allocation->type() ||\n+      page->size() != allocation->size()) {\n+    \/\/ Wrong type or size\n+    return false;\n+  }\n+\n+  if (should_defragment(page)) {\n+    \/\/ Defragment address space\n+    XStatInc(XCounterDefragment);\n+    return false;\n+  }\n+\n+  \/\/ Allocation immediately satisfied\n+  return true;\n+}\n+\n+XPage* XPageAllocator::alloc_page_finalize(XPageAllocation* allocation) {\n+  \/\/ Fast path\n+  if (is_alloc_satisfied(allocation)) {\n+    return allocation->pages()->remove_first();\n+  }\n+\n+  \/\/ Slow path\n+  XPage* const page = alloc_page_create(allocation);\n+  if (page == NULL) {\n+    \/\/ Out of address space\n+    return NULL;\n+  }\n+\n+  \/\/ Commit page\n+  if (commit_page(page)) {\n+    \/\/ Success\n+    map_page(page);\n+    return page;\n+  }\n+\n+  \/\/ Failed or partially failed. Split of any successfully committed\n+  \/\/ part of the page into a new page and insert it into list of pages,\n+  \/\/ so that it will be re-inserted into the page cache.\n+  XPage* const committed_page = page->split_committed();\n+  destroy_page(page);\n+\n+  if (committed_page != NULL) {\n+    map_page(committed_page);\n+    allocation->pages()->insert_last(committed_page);\n+  }\n+\n+  return NULL;\n+}\n+\n+void XPageAllocator::alloc_page_failed(XPageAllocation* allocation) {\n+  XLocker<XLock> locker(&_lock);\n+\n+  size_t freed = 0;\n+\n+  \/\/ Free any allocated\/flushed pages\n+  XListRemoveIterator<XPage> iter(allocation->pages());\n+  for (XPage* page; iter.next(&page);) {\n+    freed += page->size();\n+    free_page_inner(page, false \/* reclaimed *\/);\n+  }\n+\n+  \/\/ Adjust capacity and used to reflect the failed capacity increase\n+  const size_t remaining = allocation->size() - freed;\n+  decrease_used(remaining, false \/* reclaimed *\/);\n+  decrease_capacity(remaining, true \/* set_max_capacity *\/);\n+\n+  \/\/ Try satisfy stalled allocations\n+  satisfy_stalled();\n+}\n+\n+XPage* XPageAllocator::alloc_page(uint8_t type, size_t size, XAllocationFlags flags) {\n+  EventZPageAllocation event;\n+\n+retry:\n+  XPageAllocation allocation(type, size, flags);\n+\n+  \/\/ Allocate one or more pages from the page cache. If the allocation\n+  \/\/ succeeds but the returned pages don't cover the complete allocation,\n+  \/\/ then finalize phase is allowed to allocate the remaining memory\n+  \/\/ directly from the physical memory manager. Note that this call might\n+  \/\/ block in a safepoint if the non-blocking flag is not set.\n+  if (!alloc_page_or_stall(&allocation)) {\n+    \/\/ Out of memory\n+    return NULL;\n+  }\n+\n+  XPage* const page = alloc_page_finalize(&allocation);\n+  if (page == NULL) {\n+    \/\/ Failed to commit or map. Clean up and retry, in the hope that\n+    \/\/ we can still allocate by flushing the page cache (more aggressively).\n+    alloc_page_failed(&allocation);\n+    goto retry;\n+  }\n+\n+  \/\/ Reset page. This updates the page's sequence number and must\n+  \/\/ be done after we potentially blocked in a safepoint (stalled)\n+  \/\/ where the global sequence number was updated.\n+  page->reset();\n+\n+  \/\/ Update allocation statistics. Exclude worker relocations to avoid\n+  \/\/ artificial inflation of the allocation rate during relocation.\n+  if (!flags.worker_relocation() && is_init_completed()) {\n+    \/\/ Note that there are two allocation rate counters, which have\n+    \/\/ different purposes and are sampled at different frequencies.\n+    const size_t bytes = page->size();\n+    XStatInc(XCounterAllocationRate, bytes);\n+    XStatInc(XStatAllocRate::counter(), bytes);\n+  }\n+\n+  \/\/ Send event\n+  event.commit(type, size, allocation.flushed(), allocation.committed(),\n+               page->physical_memory().nsegments(), flags.non_blocking());\n+\n+  return page;\n+}\n+\n+void XPageAllocator::satisfy_stalled() {\n+  for (;;) {\n+    XPageAllocation* const allocation = _stalled.first();\n+    if (allocation == NULL) {\n+      \/\/ Allocation queue is empty\n+      return;\n+    }\n+\n+    if (!alloc_page_common(allocation)) {\n+      \/\/ Allocation could not be satisfied, give up\n+      return;\n+    }\n+\n+    \/\/ Allocation succeeded, dequeue and satisfy allocation request.\n+    \/\/ Note that we must dequeue the allocation request first, since\n+    \/\/ it will immediately be deallocated once it has been satisfied.\n+    _stalled.remove(allocation);\n+    _satisfied.insert_last(allocation);\n+    allocation->satisfy(XPageAllocationStallSuccess);\n+  }\n+}\n+\n+void XPageAllocator::free_page_inner(XPage* page, bool reclaimed) {\n+  \/\/ Update used statistics\n+  decrease_used(page->size(), reclaimed);\n+\n+  \/\/ Set time when last used\n+  page->set_last_used();\n+\n+  \/\/ Cache page\n+  _cache.free_page(page);\n+}\n+\n+void XPageAllocator::free_page(XPage* page, bool reclaimed) {\n+  XLocker<XLock> locker(&_lock);\n+\n+  \/\/ Free page\n+  free_page_inner(page, reclaimed);\n+\n+  \/\/ Try satisfy stalled allocations\n+  satisfy_stalled();\n+}\n+\n+void XPageAllocator::free_pages(const XArray<XPage*>* pages, bool reclaimed) {\n+  XLocker<XLock> locker(&_lock);\n+\n+  \/\/ Free pages\n+  XArrayIterator<XPage*> iter(pages);\n+  for (XPage* page; iter.next(&page);) {\n+    free_page_inner(page, reclaimed);\n+  }\n+\n+  \/\/ Try satisfy stalled allocations\n+  satisfy_stalled();\n+}\n+\n+size_t XPageAllocator::uncommit(uint64_t* timeout) {\n+  \/\/ We need to join the suspendible thread set while manipulating capacity and\n+  \/\/ used, to make sure GC safepoints will have a consistent view. However, when\n+  \/\/ ZVerifyViews is enabled we need to join at a broader scope to also make sure\n+  \/\/ we don't change the address good mask after pages have been flushed, and\n+  \/\/ thereby made invisible to pages_do(), but before they have been unmapped.\n+  SuspendibleThreadSetJoiner joiner(ZVerifyViews);\n+  XList<XPage> pages;\n+  size_t flushed;\n+\n+  {\n+    SuspendibleThreadSetJoiner joiner(!ZVerifyViews);\n+    XLocker<XLock> locker(&_lock);\n+\n+    \/\/ Never uncommit below min capacity. We flush out and uncommit chunks at\n+    \/\/ a time (~0.8% of the max capacity, but at least one granule and at most\n+    \/\/ 256M), in case demand for memory increases while we are uncommitting.\n+    const size_t retain = MAX2(_used, _min_capacity);\n+    const size_t release = _capacity - retain;\n+    const size_t limit = MIN2(align_up(_current_max_capacity >> 7, XGranuleSize), 256 * M);\n+    const size_t flush = MIN2(release, limit);\n+\n+    \/\/ Flush pages to uncommit\n+    flushed = _cache.flush_for_uncommit(flush, &pages, timeout);\n+    if (flushed == 0) {\n+      \/\/ Nothing flushed\n+      return 0;\n+    }\n+\n+    \/\/ Record flushed pages as claimed\n+    Atomic::add(&_claimed, flushed);\n+  }\n+\n+  \/\/ Unmap, uncommit, and destroy flushed pages\n+  XListRemoveIterator<XPage> iter(&pages);\n+  for (XPage* page; iter.next(&page);) {\n+    unmap_page(page);\n+    uncommit_page(page);\n+    destroy_page(page);\n+  }\n+\n+  {\n+    SuspendibleThreadSetJoiner joiner(!ZVerifyViews);\n+    XLocker<XLock> locker(&_lock);\n+\n+    \/\/ Adjust claimed and capacity to reflect the uncommit\n+    Atomic::sub(&_claimed, flushed);\n+    decrease_capacity(flushed, false \/* set_max_capacity *\/);\n+  }\n+\n+  return flushed;\n+}\n+\n+void XPageAllocator::enable_deferred_delete() const {\n+  _safe_delete.enable_deferred_delete();\n+}\n+\n+void XPageAllocator::disable_deferred_delete() const {\n+  _safe_delete.disable_deferred_delete();\n+}\n+\n+void XPageAllocator::debug_map_page(const XPage* page) const {\n+  assert(SafepointSynchronize::is_at_safepoint(), \"Should be at safepoint\");\n+  _physical.debug_map(page->start(), page->physical_memory());\n+}\n+\n+void XPageAllocator::debug_unmap_page(const XPage* page) const {\n+  assert(SafepointSynchronize::is_at_safepoint(), \"Should be at safepoint\");\n+  _physical.debug_unmap(page->start(), page->size());\n+}\n+\n+void XPageAllocator::pages_do(XPageClosure* cl) const {\n+  assert(SafepointSynchronize::is_at_safepoint(), \"Should be at safepoint\");\n+\n+  XListIterator<XPageAllocation> iter_satisfied(&_satisfied);\n+  for (XPageAllocation* allocation; iter_satisfied.next(&allocation);) {\n+    XListIterator<XPage> iter_pages(allocation->pages());\n+    for (XPage* page; iter_pages.next(&page);) {\n+      cl->do_page(page);\n+    }\n+  }\n+\n+  _cache.pages_do(cl);\n+}\n+\n+bool XPageAllocator::has_alloc_stalled() const {\n+  return Atomic::load(&_nstalled) != 0;\n+}\n+\n+void XPageAllocator::check_out_of_memory() {\n+  XLocker<XLock> locker(&_lock);\n+\n+  \/\/ Fail allocation requests that were enqueued before the\n+  \/\/ last GC cycle started, otherwise start a new GC cycle.\n+  for (XPageAllocation* allocation = _stalled.first(); allocation != NULL; allocation = _stalled.first()) {\n+    if (allocation->seqnum() == XGlobalSeqNum) {\n+      \/\/ Start a new GC cycle, keep allocation requests enqueued\n+      allocation->satisfy(XPageAllocationStallStartGC);\n+      return;\n+    }\n+\n+    \/\/ Out of memory, fail allocation request\n+    _stalled.remove(allocation);\n+    _satisfied.insert_last(allocation);\n+    allocation->satisfy(XPageAllocationStallFailed);\n+  }\n+}\n+\n+void XPageAllocator::threads_do(ThreadClosure* tc) const {\n+  tc->do_thread(_unmapper);\n+  tc->do_thread(_uncommitter);\n+}\n","filename":"src\/hotspot\/share\/gc\/x\/xPageAllocator.cpp","additions":870,"deletions":0,"binary":false,"changes":870,"status":"added"},{"patch":"@@ -0,0 +1,174 @@\n+\/*\n+ * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XPAGEALLOCATOR_HPP\n+#define SHARE_GC_X_XPAGEALLOCATOR_HPP\n+\n+#include \"gc\/x\/xAllocationFlags.hpp\"\n+#include \"gc\/x\/xArray.hpp\"\n+#include \"gc\/x\/xList.hpp\"\n+#include \"gc\/x\/xLock.hpp\"\n+#include \"gc\/x\/xPageCache.hpp\"\n+#include \"gc\/x\/xPhysicalMemory.hpp\"\n+#include \"gc\/x\/xSafeDelete.hpp\"\n+#include \"gc\/x\/xVirtualMemory.hpp\"\n+\n+class ThreadClosure;\n+class VMStructs;\n+class XPageAllocation;\n+class XPageAllocatorStats;\n+class XWorkers;\n+class XUncommitter;\n+class XUnmapper;\n+\n+class XPageAllocator {\n+  friend class ::VMStructs;\n+  friend class XUnmapper;\n+  friend class XUncommitter;\n+\n+private:\n+  mutable XLock              _lock;\n+  XPageCache                 _cache;\n+  XVirtualMemoryManager      _virtual;\n+  XPhysicalMemoryManager     _physical;\n+  const size_t               _min_capacity;\n+  const size_t               _max_capacity;\n+  volatile size_t            _current_max_capacity;\n+  volatile size_t            _capacity;\n+  volatile size_t            _claimed;\n+  volatile size_t            _used;\n+  size_t                     _used_high;\n+  size_t                     _used_low;\n+  ssize_t                    _reclaimed;\n+  XList<XPageAllocation>     _stalled;\n+  volatile uint64_t          _nstalled;\n+  XList<XPageAllocation>     _satisfied;\n+  XUnmapper*                 _unmapper;\n+  XUncommitter*              _uncommitter;\n+  mutable XSafeDelete<XPage> _safe_delete;\n+  bool                       _initialized;\n+\n+  bool prime_cache(XWorkers* workers, size_t size);\n+\n+  size_t increase_capacity(size_t size);\n+  void decrease_capacity(size_t size, bool set_max_capacity);\n+\n+  void increase_used(size_t size, bool relocation);\n+  void decrease_used(size_t size, bool reclaimed);\n+\n+  bool commit_page(XPage* page);\n+  void uncommit_page(XPage* page);\n+\n+  void map_page(const XPage* page) const;\n+  void unmap_page(const XPage* page) const;\n+\n+  void destroy_page(XPage* page);\n+\n+  bool is_alloc_allowed(size_t size) const;\n+\n+  bool alloc_page_common_inner(uint8_t type, size_t size, XList<XPage>* pages);\n+  bool alloc_page_common(XPageAllocation* allocation);\n+  bool alloc_page_stall(XPageAllocation* allocation);\n+  bool alloc_page_or_stall(XPageAllocation* allocation);\n+  bool should_defragment(const XPage* page) const;\n+  bool is_alloc_satisfied(XPageAllocation* allocation) const;\n+  XPage* alloc_page_create(XPageAllocation* allocation);\n+  XPage* alloc_page_finalize(XPageAllocation* allocation);\n+  void alloc_page_failed(XPageAllocation* allocation);\n+\n+  void satisfy_stalled();\n+\n+  void free_page_inner(XPage* page, bool reclaimed);\n+\n+  size_t uncommit(uint64_t* timeout);\n+\n+public:\n+  XPageAllocator(XWorkers* workers,\n+                 size_t min_capacity,\n+                 size_t initial_capacity,\n+                 size_t max_capacity);\n+\n+  bool is_initialized() const;\n+\n+  size_t min_capacity() const;\n+  size_t max_capacity() const;\n+  size_t soft_max_capacity() const;\n+  size_t capacity() const;\n+  size_t used() const;\n+  size_t unused() const;\n+\n+  XPageAllocatorStats stats() const;\n+\n+  void reset_statistics();\n+\n+  XPage* alloc_page(uint8_t type, size_t size, XAllocationFlags flags);\n+  void free_page(XPage* page, bool reclaimed);\n+  void free_pages(const XArray<XPage*>* pages, bool reclaimed);\n+\n+  void enable_deferred_delete() const;\n+  void disable_deferred_delete() const;\n+\n+  void debug_map_page(const XPage* page) const;\n+  void debug_unmap_page(const XPage* page) const;\n+\n+  bool has_alloc_stalled() const;\n+  void check_out_of_memory();\n+\n+  void pages_do(XPageClosure* cl) const;\n+\n+  void threads_do(ThreadClosure* tc) const;\n+};\n+\n+class XPageAllocatorStats {\n+private:\n+  size_t _min_capacity;\n+  size_t _max_capacity;\n+  size_t _soft_max_capacity;\n+  size_t _current_max_capacity;\n+  size_t _capacity;\n+  size_t _used;\n+  size_t _used_high;\n+  size_t _used_low;\n+  size_t _reclaimed;\n+\n+public:\n+  XPageAllocatorStats(size_t min_capacity,\n+                      size_t max_capacity,\n+                      size_t soft_max_capacity,\n+                      size_t capacity,\n+                      size_t used,\n+                      size_t used_high,\n+                      size_t used_low,\n+                      size_t reclaimed);\n+\n+  size_t min_capacity() const;\n+  size_t max_capacity() const;\n+  size_t soft_max_capacity() const;\n+  size_t capacity() const;\n+  size_t used() const;\n+  size_t used_high() const;\n+  size_t used_low() const;\n+  size_t reclaimed() const;\n+};\n+\n+#endif \/\/ SHARE_GC_X_XPAGEALLOCATOR_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xPageAllocator.hpp","additions":174,"deletions":0,"binary":false,"changes":174,"status":"added"},{"patch":"@@ -0,0 +1,78 @@\n+\/*\n+ * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XPAGEALLOCATOR_INLINE_HPP\n+#define SHARE_GC_X_XPAGEALLOCATOR_INLINE_HPP\n+\n+#include \"gc\/x\/xPageAllocator.hpp\"\n+\n+inline XPageAllocatorStats::XPageAllocatorStats(size_t min_capacity,\n+                                                size_t max_capacity,\n+                                                size_t soft_max_capacity,\n+                                                size_t capacity,\n+                                                size_t used,\n+                                                size_t used_high,\n+                                                size_t used_low,\n+                                                size_t reclaimed) :\n+    _min_capacity(min_capacity),\n+    _max_capacity(max_capacity),\n+    _soft_max_capacity(soft_max_capacity),\n+    _capacity(capacity),\n+    _used(used),\n+    _used_high(used_high),\n+    _used_low(used_low),\n+    _reclaimed(reclaimed) {}\n+\n+inline size_t XPageAllocatorStats::min_capacity() const {\n+  return _min_capacity;\n+}\n+\n+inline size_t XPageAllocatorStats::max_capacity() const {\n+  return _max_capacity;\n+}\n+\n+inline size_t XPageAllocatorStats::soft_max_capacity() const {\n+  return _soft_max_capacity;\n+}\n+\n+inline size_t XPageAllocatorStats::capacity() const {\n+  return _capacity;\n+}\n+\n+inline size_t XPageAllocatorStats::used() const {\n+  return _used;\n+}\n+\n+inline size_t XPageAllocatorStats::used_high() const {\n+  return _used_high;\n+}\n+\n+inline size_t XPageAllocatorStats::used_low() const {\n+  return _used_low;\n+}\n+\n+inline size_t XPageAllocatorStats::reclaimed() const {\n+  return _reclaimed;\n+}\n+\n+#endif \/\/ SHARE_GC_X_XPAGEALLOCATOR_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xPageAllocator.inline.hpp","additions":78,"deletions":0,"binary":false,"changes":78,"status":"added"},{"patch":"@@ -0,0 +1,356 @@\n+\/*\n+ * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/x\/xGlobals.hpp\"\n+#include \"gc\/x\/xList.inline.hpp\"\n+#include \"gc\/x\/xNUMA.hpp\"\n+#include \"gc\/x\/xPage.inline.hpp\"\n+#include \"gc\/x\/xPageCache.hpp\"\n+#include \"gc\/x\/xStat.hpp\"\n+#include \"gc\/x\/xValue.inline.hpp\"\n+#include \"memory\/allocation.hpp\"\n+#include \"runtime\/globals.hpp\"\n+#include \"runtime\/os.hpp\"\n+\n+static const XStatCounter XCounterPageCacheHitL1(\"Memory\", \"Page Cache Hit L1\", XStatUnitOpsPerSecond);\n+static const XStatCounter XCounterPageCacheHitL2(\"Memory\", \"Page Cache Hit L2\", XStatUnitOpsPerSecond);\n+static const XStatCounter XCounterPageCacheHitL3(\"Memory\", \"Page Cache Hit L3\", XStatUnitOpsPerSecond);\n+static const XStatCounter XCounterPageCacheMiss(\"Memory\", \"Page Cache Miss\", XStatUnitOpsPerSecond);\n+\n+class XPageCacheFlushClosure : public StackObj {\n+  friend class XPageCache;\n+\n+protected:\n+  const size_t _requested;\n+  size_t       _flushed;\n+\n+public:\n+  XPageCacheFlushClosure(size_t requested);\n+  virtual bool do_page(const XPage* page) = 0;\n+};\n+\n+XPageCacheFlushClosure::XPageCacheFlushClosure(size_t requested) :\n+    _requested(requested),\n+    _flushed(0) {}\n+\n+XPageCache::XPageCache() :\n+    _small(),\n+    _medium(),\n+    _large(),\n+    _last_commit(0) {}\n+\n+XPage* XPageCache::alloc_small_page() {\n+  const uint32_t numa_id = XNUMA::id();\n+  const uint32_t numa_count = XNUMA::count();\n+\n+  \/\/ Try NUMA local page cache\n+  XPage* const l1_page = _small.get(numa_id).remove_first();\n+  if (l1_page != NULL) {\n+    XStatInc(XCounterPageCacheHitL1);\n+    return l1_page;\n+  }\n+\n+  \/\/ Try NUMA remote page cache(s)\n+  uint32_t remote_numa_id = numa_id + 1;\n+  const uint32_t remote_numa_count = numa_count - 1;\n+  for (uint32_t i = 0; i < remote_numa_count; i++) {\n+    if (remote_numa_id == numa_count) {\n+      remote_numa_id = 0;\n+    }\n+\n+    XPage* const l2_page = _small.get(remote_numa_id).remove_first();\n+    if (l2_page != NULL) {\n+      XStatInc(XCounterPageCacheHitL2);\n+      return l2_page;\n+    }\n+\n+    remote_numa_id++;\n+  }\n+\n+  return NULL;\n+}\n+\n+XPage* XPageCache::alloc_medium_page() {\n+  XPage* const page = _medium.remove_first();\n+  if (page != NULL) {\n+    XStatInc(XCounterPageCacheHitL1);\n+    return page;\n+  }\n+\n+  return NULL;\n+}\n+\n+XPage* XPageCache::alloc_large_page(size_t size) {\n+  \/\/ Find a page with the right size\n+  XListIterator<XPage> iter(&_large);\n+  for (XPage* page; iter.next(&page);) {\n+    if (size == page->size()) {\n+      \/\/ Page found\n+      _large.remove(page);\n+      XStatInc(XCounterPageCacheHitL1);\n+      return page;\n+    }\n+  }\n+\n+  return NULL;\n+}\n+\n+XPage* XPageCache::alloc_oversized_medium_page(size_t size) {\n+  if (size <= XPageSizeMedium) {\n+    return _medium.remove_first();\n+  }\n+\n+  return NULL;\n+}\n+\n+XPage* XPageCache::alloc_oversized_large_page(size_t size) {\n+  \/\/ Find a page that is large enough\n+  XListIterator<XPage> iter(&_large);\n+  for (XPage* page; iter.next(&page);) {\n+    if (size <= page->size()) {\n+      \/\/ Page found\n+      _large.remove(page);\n+      return page;\n+    }\n+  }\n+\n+  return NULL;\n+}\n+\n+XPage* XPageCache::alloc_oversized_page(size_t size) {\n+  XPage* page = alloc_oversized_large_page(size);\n+  if (page == NULL) {\n+    page = alloc_oversized_medium_page(size);\n+  }\n+\n+  if (page != NULL) {\n+    XStatInc(XCounterPageCacheHitL3);\n+  }\n+\n+  return page;\n+}\n+\n+XPage* XPageCache::alloc_page(uint8_t type, size_t size) {\n+  XPage* page;\n+\n+  \/\/ Try allocate exact page\n+  if (type == XPageTypeSmall) {\n+    page = alloc_small_page();\n+  } else if (type == XPageTypeMedium) {\n+    page = alloc_medium_page();\n+  } else {\n+    page = alloc_large_page(size);\n+  }\n+\n+  if (page == NULL) {\n+    \/\/ Try allocate potentially oversized page\n+    XPage* const oversized = alloc_oversized_page(size);\n+    if (oversized != NULL) {\n+      if (size < oversized->size()) {\n+        \/\/ Split oversized page\n+        page = oversized->split(type, size);\n+\n+        \/\/ Cache remainder\n+        free_page(oversized);\n+      } else {\n+        \/\/ Re-type correctly sized page\n+        page = oversized->retype(type);\n+      }\n+    }\n+  }\n+\n+  if (page == NULL) {\n+    XStatInc(XCounterPageCacheMiss);\n+  }\n+\n+  return page;\n+}\n+\n+void XPageCache::free_page(XPage* page) {\n+  const uint8_t type = page->type();\n+  if (type == XPageTypeSmall) {\n+    _small.get(page->numa_id()).insert_first(page);\n+  } else if (type == XPageTypeMedium) {\n+    _medium.insert_first(page);\n+  } else {\n+    _large.insert_first(page);\n+  }\n+}\n+\n+bool XPageCache::flush_list_inner(XPageCacheFlushClosure* cl, XList<XPage>* from, XList<XPage>* to) {\n+  XPage* const page = from->last();\n+  if (page == NULL || !cl->do_page(page)) {\n+    \/\/ Don't flush page\n+    return false;\n+  }\n+\n+  \/\/ Flush page\n+  from->remove(page);\n+  to->insert_last(page);\n+  return true;\n+}\n+\n+void XPageCache::flush_list(XPageCacheFlushClosure* cl, XList<XPage>* from, XList<XPage>* to) {\n+  while (flush_list_inner(cl, from, to));\n+}\n+\n+void XPageCache::flush_per_numa_lists(XPageCacheFlushClosure* cl, XPerNUMA<XList<XPage> >* from, XList<XPage>* to) {\n+  const uint32_t numa_count = XNUMA::count();\n+  uint32_t numa_done = 0;\n+  uint32_t numa_next = 0;\n+\n+  \/\/ Flush lists round-robin\n+  while (numa_done < numa_count) {\n+    XList<XPage>* numa_list = from->addr(numa_next);\n+    if (++numa_next == numa_count) {\n+      numa_next = 0;\n+    }\n+\n+    if (flush_list_inner(cl, numa_list, to)) {\n+      \/\/ Not done\n+      numa_done = 0;\n+    } else {\n+      \/\/ Done\n+      numa_done++;\n+    }\n+  }\n+}\n+\n+void XPageCache::flush(XPageCacheFlushClosure* cl, XList<XPage>* to) {\n+  \/\/ Prefer flushing large, then medium and last small pages\n+  flush_list(cl, &_large, to);\n+  flush_list(cl, &_medium, to);\n+  flush_per_numa_lists(cl, &_small, to);\n+\n+  if (cl->_flushed > cl->_requested) {\n+    \/\/ Overflushed, re-insert part of last page into the cache\n+    const size_t overflushed = cl->_flushed - cl->_requested;\n+    XPage* const reinsert = to->last()->split(overflushed);\n+    free_page(reinsert);\n+    cl->_flushed -= overflushed;\n+  }\n+}\n+\n+class XPageCacheFlushForAllocationClosure : public XPageCacheFlushClosure {\n+public:\n+  XPageCacheFlushForAllocationClosure(size_t requested) :\n+      XPageCacheFlushClosure(requested) {}\n+\n+  virtual bool do_page(const XPage* page) {\n+    if (_flushed < _requested) {\n+      \/\/ Flush page\n+      _flushed += page->size();\n+      return true;\n+    }\n+\n+    \/\/ Don't flush page\n+    return false;\n+  }\n+};\n+\n+void XPageCache::flush_for_allocation(size_t requested, XList<XPage>* to) {\n+  XPageCacheFlushForAllocationClosure cl(requested);\n+  flush(&cl, to);\n+}\n+\n+class XPageCacheFlushForUncommitClosure : public XPageCacheFlushClosure {\n+private:\n+  const uint64_t _now;\n+  uint64_t*      _timeout;\n+\n+public:\n+  XPageCacheFlushForUncommitClosure(size_t requested, uint64_t now, uint64_t* timeout) :\n+      XPageCacheFlushClosure(requested),\n+      _now(now),\n+      _timeout(timeout) {\n+    \/\/ Set initial timeout\n+    *_timeout = ZUncommitDelay;\n+  }\n+\n+  virtual bool do_page(const XPage* page) {\n+    const uint64_t expires = page->last_used() + ZUncommitDelay;\n+    if (expires > _now) {\n+      \/\/ Don't flush page, record shortest non-expired timeout\n+      *_timeout = MIN2(*_timeout, expires - _now);\n+      return false;\n+    }\n+\n+    if (_flushed >= _requested) {\n+      \/\/ Don't flush page, requested amount flushed\n+      return false;\n+    }\n+\n+    \/\/ Flush page\n+    _flushed += page->size();\n+    return true;\n+  }\n+};\n+\n+size_t XPageCache::flush_for_uncommit(size_t requested, XList<XPage>* to, uint64_t* timeout) {\n+  const uint64_t now = os::elapsedTime();\n+  const uint64_t expires = _last_commit + ZUncommitDelay;\n+  if (expires > now) {\n+    \/\/ Delay uncommit, set next timeout\n+    *timeout = expires - now;\n+    return 0;\n+  }\n+\n+  if (requested == 0) {\n+    \/\/ Nothing to flush, set next timeout\n+    *timeout = ZUncommitDelay;\n+    return 0;\n+  }\n+\n+  XPageCacheFlushForUncommitClosure cl(requested, now, timeout);\n+  flush(&cl, to);\n+\n+  return cl._flushed;\n+}\n+\n+void XPageCache::set_last_commit() {\n+  _last_commit = ceil(os::elapsedTime());\n+}\n+\n+void XPageCache::pages_do(XPageClosure* cl) const {\n+  \/\/ Small\n+  XPerNUMAConstIterator<XList<XPage> > iter_numa(&_small);\n+  for (const XList<XPage>* list; iter_numa.next(&list);) {\n+    XListIterator<XPage> iter_small(list);\n+    for (XPage* page; iter_small.next(&page);) {\n+      cl->do_page(page);\n+    }\n+  }\n+\n+  \/\/ Medium\n+  XListIterator<XPage> iter_medium(&_medium);\n+  for (XPage* page; iter_medium.next(&page);) {\n+    cl->do_page(page);\n+  }\n+\n+  \/\/ Large\n+  XListIterator<XPage> iter_large(&_large);\n+  for (XPage* page; iter_large.next(&page);) {\n+    cl->do_page(page);\n+  }\n+}\n","filename":"src\/hotspot\/share\/gc\/x\/xPageCache.cpp","additions":356,"deletions":0,"binary":false,"changes":356,"status":"added"},{"patch":"@@ -0,0 +1,67 @@\n+\/*\n+ * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XPAGECACHE_HPP\n+#define SHARE_GC_X_XPAGECACHE_HPP\n+\n+#include \"gc\/x\/xList.hpp\"\n+#include \"gc\/x\/xPage.hpp\"\n+#include \"gc\/x\/xValue.hpp\"\n+\n+class XPageCacheFlushClosure;\n+\n+class XPageCache {\n+private:\n+  XPerNUMA<XList<XPage> > _small;\n+  XList<XPage>            _medium;\n+  XList<XPage>            _large;\n+  uint64_t                _last_commit;\n+\n+  XPage* alloc_small_page();\n+  XPage* alloc_medium_page();\n+  XPage* alloc_large_page(size_t size);\n+\n+  XPage* alloc_oversized_medium_page(size_t size);\n+  XPage* alloc_oversized_large_page(size_t size);\n+  XPage* alloc_oversized_page(size_t size);\n+\n+  bool flush_list_inner(XPageCacheFlushClosure* cl, XList<XPage>* from, XList<XPage>* to);\n+  void flush_list(XPageCacheFlushClosure* cl, XList<XPage>* from, XList<XPage>* to);\n+  void flush_per_numa_lists(XPageCacheFlushClosure* cl, XPerNUMA<XList<XPage> >* from, XList<XPage>* to);\n+  void flush(XPageCacheFlushClosure* cl, XList<XPage>* to);\n+\n+public:\n+  XPageCache();\n+\n+  XPage* alloc_page(uint8_t type, size_t size);\n+  void free_page(XPage* page);\n+\n+  void flush_for_allocation(size_t requested, XList<XPage>* to);\n+  size_t flush_for_uncommit(size_t requested, XList<XPage>* to, uint64_t* timeout);\n+\n+  void set_last_commit();\n+\n+  void pages_do(XPageClosure* cl) const;\n+};\n+\n+#endif \/\/ SHARE_GC_X_XPAGECACHE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xPageCache.hpp","additions":67,"deletions":0,"binary":false,"changes":67,"status":"added"},{"patch":"@@ -0,0 +1,53 @@\n+\/*\n+ * Copyright (c) 2015, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/x\/xGlobals.hpp\"\n+#include \"gc\/x\/xGranuleMap.inline.hpp\"\n+#include \"gc\/x\/xPage.inline.hpp\"\n+#include \"gc\/x\/xPageTable.inline.hpp\"\n+#include \"runtime\/orderAccess.hpp\"\n+#include \"utilities\/debug.hpp\"\n+\n+XPageTable::XPageTable() :\n+    _map(XAddressOffsetMax) {}\n+\n+void XPageTable::insert(XPage* page) {\n+  const uintptr_t offset = page->start();\n+  const size_t size = page->size();\n+\n+  \/\/ Make sure a newly created page is\n+  \/\/ visible before updating the page table.\n+  OrderAccess::storestore();\n+\n+  assert(_map.get(offset) == NULL, \"Invalid entry\");\n+  _map.put(offset, size, page);\n+}\n+\n+void XPageTable::remove(XPage* page) {\n+  const uintptr_t offset = page->start();\n+  const size_t size = page->size();\n+\n+  assert(_map.get(offset) == page, \"Invalid entry\");\n+  _map.put(offset, size, NULL);\n+}\n","filename":"src\/hotspot\/share\/gc\/x\/xPageTable.cpp","additions":53,"deletions":0,"binary":false,"changes":53,"status":"added"},{"patch":"@@ -0,0 +1,61 @@\n+\/*\n+ * Copyright (c) 2015, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XPAGETABLE_HPP\n+#define SHARE_GC_X_XPAGETABLE_HPP\n+\n+#include \"gc\/x\/xGranuleMap.hpp\"\n+#include \"memory\/allocation.hpp\"\n+\n+class VMStructs;\n+class XPage;\n+class XPageTableIterator;\n+\n+class XPageTable {\n+  friend class ::VMStructs;\n+  friend class XPageTableIterator;\n+\n+private:\n+  XGranuleMap<XPage*> _map;\n+\n+public:\n+  XPageTable();\n+\n+  XPage* get(uintptr_t addr) const;\n+\n+  void insert(XPage* page);\n+  void remove(XPage* page);\n+};\n+\n+class XPageTableIterator : public StackObj {\n+private:\n+  XGranuleMapIterator<XPage*> _iter;\n+  XPage*                      _prev;\n+\n+public:\n+  XPageTableIterator(const XPageTable* page_table);\n+\n+  bool next(XPage** page);\n+};\n+\n+#endif \/\/ SHARE_GC_X_XPAGETABLE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xPageTable.hpp","additions":61,"deletions":0,"binary":false,"changes":61,"status":"added"},{"patch":"@@ -0,0 +1,54 @@\n+\/*\n+ * Copyright (c) 2015, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XPAGETABLE_INLINE_HPP\n+#define SHARE_GC_X_XPAGETABLE_INLINE_HPP\n+\n+#include \"gc\/x\/xPageTable.hpp\"\n+\n+#include \"gc\/x\/xAddress.inline.hpp\"\n+#include \"gc\/x\/xGranuleMap.inline.hpp\"\n+\n+inline XPage* XPageTable::get(uintptr_t addr) const {\n+  assert(!XAddress::is_null(addr), \"Invalid address\");\n+  return _map.get(XAddress::offset(addr));\n+}\n+\n+inline XPageTableIterator::XPageTableIterator(const XPageTable* page_table) :\n+    _iter(&page_table->_map),\n+    _prev(NULL) {}\n+\n+inline bool XPageTableIterator::next(XPage** page) {\n+  for (XPage* entry; _iter.next(&entry);) {\n+    if (entry != NULL && entry != _prev) {\n+      \/\/ Next page found\n+      *page = _prev = entry;\n+      return true;\n+    }\n+  }\n+\n+  \/\/ No more pages\n+  return false;\n+}\n+\n+#endif \/\/ SHARE_GC_X_XPAGETABLE_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xPageTable.inline.hpp","additions":54,"deletions":0,"binary":false,"changes":54,"status":"added"},{"patch":"@@ -0,0 +1,435 @@\n+\/*\n+ * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/shared\/gcLogPrecious.hpp\"\n+#include \"gc\/x\/xAddress.inline.hpp\"\n+#include \"gc\/x\/xArray.inline.hpp\"\n+#include \"gc\/x\/xGlobals.hpp\"\n+#include \"gc\/x\/xLargePages.inline.hpp\"\n+#include \"gc\/x\/xNUMA.inline.hpp\"\n+#include \"gc\/x\/xPhysicalMemory.inline.hpp\"\n+#include \"logging\/log.hpp\"\n+#include \"runtime\/globals.hpp\"\n+#include \"runtime\/globals_extension.hpp\"\n+#include \"runtime\/init.hpp\"\n+#include \"runtime\/os.hpp\"\n+#include \"services\/memTracker.hpp\"\n+#include \"utilities\/align.hpp\"\n+#include \"utilities\/debug.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include \"utilities\/powerOfTwo.hpp\"\n+\n+XPhysicalMemory::XPhysicalMemory() :\n+    _segments() {}\n+\n+XPhysicalMemory::XPhysicalMemory(const XPhysicalMemorySegment& segment) :\n+    _segments() {\n+  add_segment(segment);\n+}\n+\n+XPhysicalMemory::XPhysicalMemory(const XPhysicalMemory& pmem) :\n+    _segments() {\n+  add_segments(pmem);\n+}\n+\n+const XPhysicalMemory& XPhysicalMemory::operator=(const XPhysicalMemory& pmem) {\n+  \/\/ Free segments\n+  _segments.clear_and_deallocate();\n+\n+  \/\/ Copy segments\n+  add_segments(pmem);\n+\n+  return *this;\n+}\n+\n+size_t XPhysicalMemory::size() const {\n+  size_t size = 0;\n+\n+  for (int i = 0; i < _segments.length(); i++) {\n+    size += _segments.at(i).size();\n+  }\n+\n+  return size;\n+}\n+\n+void XPhysicalMemory::insert_segment(int index, uintptr_t start, size_t size, bool committed) {\n+  _segments.insert_before(index, XPhysicalMemorySegment(start, size, committed));\n+}\n+\n+void XPhysicalMemory::replace_segment(int index, uintptr_t start, size_t size, bool committed) {\n+  _segments.at_put(index, XPhysicalMemorySegment(start, size, committed));\n+}\n+\n+void XPhysicalMemory::remove_segment(int index) {\n+  _segments.remove_at(index);\n+}\n+\n+void XPhysicalMemory::add_segments(const XPhysicalMemory& pmem) {\n+  for (int i = 0; i < pmem.nsegments(); i++) {\n+    add_segment(pmem.segment(i));\n+  }\n+}\n+\n+void XPhysicalMemory::remove_segments() {\n+  _segments.clear_and_deallocate();\n+}\n+\n+static bool is_mergable(const XPhysicalMemorySegment& before, const XPhysicalMemorySegment& after) {\n+  return before.end() == after.start() && before.is_committed() == after.is_committed();\n+}\n+\n+void XPhysicalMemory::add_segment(const XPhysicalMemorySegment& segment) {\n+  \/\/ Insert segments in address order, merge segments when possible\n+  for (int i = _segments.length(); i > 0; i--) {\n+    const int current = i - 1;\n+\n+    if (_segments.at(current).end() <= segment.start()) {\n+      if (is_mergable(_segments.at(current), segment)) {\n+        if (current + 1 < _segments.length() && is_mergable(segment, _segments.at(current + 1))) {\n+          \/\/ Merge with end of current segment and start of next segment\n+          const size_t start = _segments.at(current).start();\n+          const size_t size = _segments.at(current).size() + segment.size() + _segments.at(current + 1).size();\n+          replace_segment(current, start, size, segment.is_committed());\n+          remove_segment(current + 1);\n+          return;\n+        }\n+\n+        \/\/ Merge with end of current segment\n+        const size_t start = _segments.at(current).start();\n+        const size_t size = _segments.at(current).size() + segment.size();\n+        replace_segment(current, start, size, segment.is_committed());\n+        return;\n+      } else if (current + 1 < _segments.length() && is_mergable(segment, _segments.at(current + 1))) {\n+        \/\/ Merge with start of next segment\n+        const size_t start = segment.start();\n+        const size_t size = segment.size() + _segments.at(current + 1).size();\n+        replace_segment(current + 1, start, size, segment.is_committed());\n+        return;\n+      }\n+\n+      \/\/ Insert after current segment\n+      insert_segment(current + 1, segment.start(), segment.size(), segment.is_committed());\n+      return;\n+    }\n+  }\n+\n+  if (_segments.length() > 0 && is_mergable(segment, _segments.at(0))) {\n+    \/\/ Merge with start of first segment\n+    const size_t start = segment.start();\n+    const size_t size = segment.size() + _segments.at(0).size();\n+    replace_segment(0, start, size, segment.is_committed());\n+    return;\n+  }\n+\n+  \/\/ Insert before first segment\n+  insert_segment(0, segment.start(), segment.size(), segment.is_committed());\n+}\n+\n+bool XPhysicalMemory::commit_segment(int index, size_t size) {\n+  assert(size <= _segments.at(index).size(), \"Invalid size\");\n+  assert(!_segments.at(index).is_committed(), \"Invalid state\");\n+\n+  if (size == _segments.at(index).size()) {\n+    \/\/ Completely committed\n+    _segments.at(index).set_committed(true);\n+    return true;\n+  }\n+\n+  if (size > 0) {\n+    \/\/ Partially committed, split segment\n+    insert_segment(index + 1, _segments.at(index).start() + size, _segments.at(index).size() - size, false \/* committed *\/);\n+    replace_segment(index, _segments.at(index).start(), size, true \/* committed *\/);\n+  }\n+\n+  return false;\n+}\n+\n+bool XPhysicalMemory::uncommit_segment(int index, size_t size) {\n+  assert(size <= _segments.at(index).size(), \"Invalid size\");\n+  assert(_segments.at(index).is_committed(), \"Invalid state\");\n+\n+  if (size == _segments.at(index).size()) {\n+    \/\/ Completely uncommitted\n+    _segments.at(index).set_committed(false);\n+    return true;\n+  }\n+\n+  if (size > 0) {\n+    \/\/ Partially uncommitted, split segment\n+    insert_segment(index + 1, _segments.at(index).start() + size, _segments.at(index).size() - size, true \/* committed *\/);\n+    replace_segment(index, _segments.at(index).start(), size, false \/* committed *\/);\n+  }\n+\n+  return false;\n+}\n+\n+XPhysicalMemory XPhysicalMemory::split(size_t size) {\n+  XPhysicalMemory pmem;\n+  int nsegments = 0;\n+\n+  for (int i = 0; i < _segments.length(); i++) {\n+    const XPhysicalMemorySegment& segment = _segments.at(i);\n+    if (pmem.size() < size) {\n+      if (pmem.size() + segment.size() <= size) {\n+        \/\/ Transfer segment\n+        pmem.add_segment(segment);\n+      } else {\n+        \/\/ Split segment\n+        const size_t split_size = size - pmem.size();\n+        pmem.add_segment(XPhysicalMemorySegment(segment.start(), split_size, segment.is_committed()));\n+        _segments.at_put(nsegments++, XPhysicalMemorySegment(segment.start() + split_size, segment.size() - split_size, segment.is_committed()));\n+      }\n+    } else {\n+      \/\/ Keep segment\n+      _segments.at_put(nsegments++, segment);\n+    }\n+  }\n+\n+  _segments.trunc_to(nsegments);\n+\n+  return pmem;\n+}\n+\n+XPhysicalMemory XPhysicalMemory::split_committed() {\n+  XPhysicalMemory pmem;\n+  int nsegments = 0;\n+\n+  for (int i = 0; i < _segments.length(); i++) {\n+    const XPhysicalMemorySegment& segment = _segments.at(i);\n+    if (segment.is_committed()) {\n+      \/\/ Transfer segment\n+      pmem.add_segment(segment);\n+    } else {\n+      \/\/ Keep segment\n+      _segments.at_put(nsegments++, segment);\n+    }\n+  }\n+\n+  _segments.trunc_to(nsegments);\n+\n+  return pmem;\n+}\n+\n+XPhysicalMemoryManager::XPhysicalMemoryManager(size_t max_capacity) :\n+    _backing(max_capacity) {\n+  \/\/ Make the whole range free\n+  _manager.free(0, max_capacity);\n+}\n+\n+bool XPhysicalMemoryManager::is_initialized() const {\n+  return _backing.is_initialized();\n+}\n+\n+void XPhysicalMemoryManager::warn_commit_limits(size_t max_capacity) const {\n+  _backing.warn_commit_limits(max_capacity);\n+}\n+\n+void XPhysicalMemoryManager::try_enable_uncommit(size_t min_capacity, size_t max_capacity) {\n+  assert(!is_init_completed(), \"Invalid state\");\n+\n+  \/\/ If uncommit is not explicitly disabled, max capacity is greater than\n+  \/\/ min capacity, and uncommit is supported by the platform, then uncommit\n+  \/\/ will be enabled.\n+  if (!ZUncommit) {\n+    log_info_p(gc, init)(\"Uncommit: Disabled\");\n+    return;\n+  }\n+\n+  if (max_capacity == min_capacity) {\n+    log_info_p(gc, init)(\"Uncommit: Implicitly Disabled (-Xms equals -Xmx)\");\n+    FLAG_SET_ERGO(ZUncommit, false);\n+    return;\n+  }\n+\n+  \/\/ Test if uncommit is supported by the operating system by committing\n+  \/\/ and then uncommitting a granule.\n+  XPhysicalMemory pmem(XPhysicalMemorySegment(0, XGranuleSize, false \/* committed *\/));\n+  if (!commit(pmem) || !uncommit(pmem)) {\n+    log_info_p(gc, init)(\"Uncommit: Implicitly Disabled (Not supported by operating system)\");\n+    FLAG_SET_ERGO(ZUncommit, false);\n+    return;\n+  }\n+\n+  log_info_p(gc, init)(\"Uncommit: Enabled\");\n+  log_info_p(gc, init)(\"Uncommit Delay: \" UINTX_FORMAT \"s\", ZUncommitDelay);\n+}\n+\n+void XPhysicalMemoryManager::nmt_commit(uintptr_t offset, size_t size) const {\n+  \/\/ From an NMT point of view we treat the first heap view (marked0) as committed\n+  const uintptr_t addr = XAddress::marked0(offset);\n+  MemTracker::record_virtual_memory_commit((void*)addr, size, CALLER_PC);\n+}\n+\n+void XPhysicalMemoryManager::nmt_uncommit(uintptr_t offset, size_t size) const {\n+  if (MemTracker::enabled()) {\n+    const uintptr_t addr = XAddress::marked0(offset);\n+    Tracker tracker(Tracker::uncommit);\n+    tracker.record((address)addr, size);\n+  }\n+}\n+\n+void XPhysicalMemoryManager::alloc(XPhysicalMemory& pmem, size_t size) {\n+  assert(is_aligned(size, XGranuleSize), \"Invalid size\");\n+\n+  \/\/ Allocate segments\n+  while (size > 0) {\n+    size_t allocated = 0;\n+    const uintptr_t start = _manager.alloc_low_address_at_most(size, &allocated);\n+    assert(start != UINTPTR_MAX, \"Allocation should never fail\");\n+    pmem.add_segment(XPhysicalMemorySegment(start, allocated, false \/* committed *\/));\n+    size -= allocated;\n+  }\n+}\n+\n+void XPhysicalMemoryManager::free(const XPhysicalMemory& pmem) {\n+  \/\/ Free segments\n+  for (int i = 0; i < pmem.nsegments(); i++) {\n+    const XPhysicalMemorySegment& segment = pmem.segment(i);\n+    _manager.free(segment.start(), segment.size());\n+  }\n+}\n+\n+bool XPhysicalMemoryManager::commit(XPhysicalMemory& pmem) {\n+  \/\/ Commit segments\n+  for (int i = 0; i < pmem.nsegments(); i++) {\n+    const XPhysicalMemorySegment& segment = pmem.segment(i);\n+    if (segment.is_committed()) {\n+      \/\/ Segment already committed\n+      continue;\n+    }\n+\n+    \/\/ Commit segment\n+    const size_t committed = _backing.commit(segment.start(), segment.size());\n+    if (!pmem.commit_segment(i, committed)) {\n+      \/\/ Failed or partially failed\n+      return false;\n+    }\n+  }\n+\n+  \/\/ Success\n+  return true;\n+}\n+\n+bool XPhysicalMemoryManager::uncommit(XPhysicalMemory& pmem) {\n+  \/\/ Commit segments\n+  for (int i = 0; i < pmem.nsegments(); i++) {\n+    const XPhysicalMemorySegment& segment = pmem.segment(i);\n+    if (!segment.is_committed()) {\n+      \/\/ Segment already uncommitted\n+      continue;\n+    }\n+\n+    \/\/ Uncommit segment\n+    const size_t uncommitted = _backing.uncommit(segment.start(), segment.size());\n+    if (!pmem.uncommit_segment(i, uncommitted)) {\n+      \/\/ Failed or partially failed\n+      return false;\n+    }\n+  }\n+\n+  \/\/ Success\n+  return true;\n+}\n+\n+void XPhysicalMemoryManager::pretouch_view(uintptr_t addr, size_t size) const {\n+  const size_t page_size = XLargePages::is_explicit() ? XGranuleSize : os::vm_page_size();\n+  os::pretouch_memory((void*)addr, (void*)(addr + size), page_size);\n+}\n+\n+void XPhysicalMemoryManager::map_view(uintptr_t addr, const XPhysicalMemory& pmem) const {\n+  size_t size = 0;\n+\n+  \/\/ Map segments\n+  for (int i = 0; i < pmem.nsegments(); i++) {\n+    const XPhysicalMemorySegment& segment = pmem.segment(i);\n+    _backing.map(addr + size, segment.size(), segment.start());\n+    size += segment.size();\n+  }\n+\n+  \/\/ Setup NUMA interleaving for large pages\n+  if (XNUMA::is_enabled() && XLargePages::is_explicit()) {\n+    \/\/ To get granule-level NUMA interleaving when using large pages,\n+    \/\/ we simply let the kernel interleave the memory for us at page\n+    \/\/ fault time.\n+    os::numa_make_global((char*)addr, size);\n+  }\n+}\n+\n+void XPhysicalMemoryManager::unmap_view(uintptr_t addr, size_t size) const {\n+  _backing.unmap(addr, size);\n+}\n+\n+void XPhysicalMemoryManager::pretouch(uintptr_t offset, size_t size) const {\n+  if (ZVerifyViews) {\n+    \/\/ Pre-touch good view\n+    pretouch_view(XAddress::good(offset), size);\n+  } else {\n+    \/\/ Pre-touch all views\n+    pretouch_view(XAddress::marked0(offset), size);\n+    pretouch_view(XAddress::marked1(offset), size);\n+    pretouch_view(XAddress::remapped(offset), size);\n+  }\n+}\n+\n+void XPhysicalMemoryManager::map(uintptr_t offset, const XPhysicalMemory& pmem) const {\n+  const size_t size = pmem.size();\n+\n+  if (ZVerifyViews) {\n+    \/\/ Map good view\n+    map_view(XAddress::good(offset), pmem);\n+  } else {\n+    \/\/ Map all views\n+    map_view(XAddress::marked0(offset), pmem);\n+    map_view(XAddress::marked1(offset), pmem);\n+    map_view(XAddress::remapped(offset), pmem);\n+  }\n+\n+  nmt_commit(offset, size);\n+}\n+\n+void XPhysicalMemoryManager::unmap(uintptr_t offset, size_t size) const {\n+  nmt_uncommit(offset, size);\n+\n+  if (ZVerifyViews) {\n+    \/\/ Unmap good view\n+    unmap_view(XAddress::good(offset), size);\n+  } else {\n+    \/\/ Unmap all views\n+    unmap_view(XAddress::marked0(offset), size);\n+    unmap_view(XAddress::marked1(offset), size);\n+    unmap_view(XAddress::remapped(offset), size);\n+  }\n+}\n+\n+void XPhysicalMemoryManager::debug_map(uintptr_t offset, const XPhysicalMemory& pmem) const {\n+  \/\/ Map good view\n+  assert(ZVerifyViews, \"Should be enabled\");\n+  map_view(XAddress::good(offset), pmem);\n+}\n+\n+void XPhysicalMemoryManager::debug_unmap(uintptr_t offset, size_t size) const {\n+  \/\/ Unmap good view\n+  assert(ZVerifyViews, \"Should be enabled\");\n+  unmap_view(XAddress::good(offset), size);\n+}\n","filename":"src\/hotspot\/share\/gc\/x\/xPhysicalMemory.cpp","additions":435,"deletions":0,"binary":false,"changes":435,"status":"added"},{"patch":"@@ -0,0 +1,116 @@\n+\/*\n+ * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XPHYSICALMEMORY_HPP\n+#define SHARE_GC_X_XPHYSICALMEMORY_HPP\n+\n+#include \"gc\/x\/xArray.hpp\"\n+#include \"gc\/x\/xMemory.hpp\"\n+#include \"memory\/allocation.hpp\"\n+#include OS_HEADER(gc\/x\/xPhysicalMemoryBacking)\n+\n+class XPhysicalMemorySegment : public CHeapObj<mtGC> {\n+private:\n+  uintptr_t _start;\n+  uintptr_t _end;\n+  bool      _committed;\n+\n+public:\n+  XPhysicalMemorySegment();\n+  XPhysicalMemorySegment(uintptr_t start, size_t size, bool committed);\n+\n+  uintptr_t start() const;\n+  uintptr_t end() const;\n+  size_t size() const;\n+\n+  bool is_committed() const;\n+  void set_committed(bool committed);\n+};\n+\n+class XPhysicalMemory {\n+private:\n+  XArray<XPhysicalMemorySegment> _segments;\n+\n+  void insert_segment(int index, uintptr_t start, size_t size, bool committed);\n+  void replace_segment(int index, uintptr_t start, size_t size, bool committed);\n+  void remove_segment(int index);\n+\n+public:\n+  XPhysicalMemory();\n+  XPhysicalMemory(const XPhysicalMemorySegment& segment);\n+  XPhysicalMemory(const XPhysicalMemory& pmem);\n+  const XPhysicalMemory& operator=(const XPhysicalMemory& pmem);\n+\n+  bool is_null() const;\n+  size_t size() const;\n+\n+  int nsegments() const;\n+  const XPhysicalMemorySegment& segment(int index) const;\n+\n+  void add_segments(const XPhysicalMemory& pmem);\n+  void remove_segments();\n+\n+  void add_segment(const XPhysicalMemorySegment& segment);\n+  bool commit_segment(int index, size_t size);\n+  bool uncommit_segment(int index, size_t size);\n+\n+  XPhysicalMemory split(size_t size);\n+  XPhysicalMemory split_committed();\n+};\n+\n+class XPhysicalMemoryManager {\n+private:\n+  XPhysicalMemoryBacking _backing;\n+  XMemoryManager         _manager;\n+\n+  void nmt_commit(uintptr_t offset, size_t size) const;\n+  void nmt_uncommit(uintptr_t offset, size_t size) const;\n+\n+  void pretouch_view(uintptr_t addr, size_t size) const;\n+  void map_view(uintptr_t addr, const XPhysicalMemory& pmem) const;\n+  void unmap_view(uintptr_t addr, size_t size) const;\n+\n+public:\n+  XPhysicalMemoryManager(size_t max_capacity);\n+\n+  bool is_initialized() const;\n+\n+  void warn_commit_limits(size_t max_capacity) const;\n+  void try_enable_uncommit(size_t min_capacity, size_t max_capacity);\n+\n+  void alloc(XPhysicalMemory& pmem, size_t size);\n+  void free(const XPhysicalMemory& pmem);\n+\n+  bool commit(XPhysicalMemory& pmem);\n+  bool uncommit(XPhysicalMemory& pmem);\n+\n+  void pretouch(uintptr_t offset, size_t size) const;\n+\n+  void map(uintptr_t offset, const XPhysicalMemory& pmem) const;\n+  void unmap(uintptr_t offset, size_t size) const;\n+\n+  void debug_map(uintptr_t offset, const XPhysicalMemory& pmem) const;\n+  void debug_unmap(uintptr_t offset, size_t size) const;\n+};\n+\n+#endif \/\/ SHARE_GC_X_XPHYSICALMEMORY_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xPhysicalMemory.hpp","additions":116,"deletions":0,"binary":false,"changes":116,"status":"added"},{"patch":"@@ -0,0 +1,74 @@\n+\/*\n+ * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XPHYSICALMEMORY_INLINE_HPP\n+#define SHARE_GC_X_XPHYSICALMEMORY_INLINE_HPP\n+\n+#include \"gc\/x\/xPhysicalMemory.hpp\"\n+\n+#include \"gc\/x\/xAddress.inline.hpp\"\n+#include \"utilities\/debug.hpp\"\n+\n+inline XPhysicalMemorySegment::XPhysicalMemorySegment() :\n+    _start(UINTPTR_MAX),\n+    _end(UINTPTR_MAX),\n+    _committed(false) {}\n+\n+inline XPhysicalMemorySegment::XPhysicalMemorySegment(uintptr_t start, size_t size, bool committed) :\n+    _start(start),\n+    _end(start + size),\n+    _committed(committed) {}\n+\n+inline uintptr_t XPhysicalMemorySegment::start() const {\n+  return _start;\n+}\n+\n+inline uintptr_t XPhysicalMemorySegment::end() const {\n+  return _end;\n+}\n+\n+inline size_t XPhysicalMemorySegment::size() const {\n+  return _end - _start;\n+}\n+\n+inline bool XPhysicalMemorySegment::is_committed() const {\n+  return _committed;\n+}\n+\n+inline void XPhysicalMemorySegment::set_committed(bool committed) {\n+  _committed = committed;\n+}\n+\n+inline bool XPhysicalMemory::is_null() const {\n+  return _segments.length() == 0;\n+}\n+\n+inline int XPhysicalMemory::nsegments() const {\n+  return _segments.length();\n+}\n+\n+inline const XPhysicalMemorySegment& XPhysicalMemory::segment(int index) const {\n+  return _segments.at(index);\n+}\n+\n+#endif \/\/ SHARE_GC_X_XPHYSICALMEMORY_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xPhysicalMemory.inline.hpp","additions":74,"deletions":0,"binary":false,"changes":74,"status":"added"},{"patch":"@@ -0,0 +1,459 @@\n+\/*\n+ * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"classfile\/javaClasses.inline.hpp\"\n+#include \"gc\/shared\/referencePolicy.hpp\"\n+#include \"gc\/shared\/referenceProcessorStats.hpp\"\n+#include \"gc\/x\/xHeap.inline.hpp\"\n+#include \"gc\/x\/xReferenceProcessor.hpp\"\n+#include \"gc\/x\/xStat.hpp\"\n+#include \"gc\/x\/xTask.hpp\"\n+#include \"gc\/x\/xTracer.inline.hpp\"\n+#include \"gc\/x\/xValue.inline.hpp\"\n+#include \"memory\/universe.hpp\"\n+#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/mutexLocker.hpp\"\n+#include \"runtime\/os.hpp\"\n+\n+static const XStatSubPhase XSubPhaseConcurrentReferencesProcess(\"Concurrent References Process\");\n+static const XStatSubPhase XSubPhaseConcurrentReferencesEnqueue(\"Concurrent References Enqueue\");\n+\n+static ReferenceType reference_type(oop reference) {\n+  return InstanceKlass::cast(reference->klass())->reference_type();\n+}\n+\n+static const char* reference_type_name(ReferenceType type) {\n+  switch (type) {\n+  case REF_SOFT:\n+    return \"Soft\";\n+\n+  case REF_WEAK:\n+    return \"Weak\";\n+\n+  case REF_FINAL:\n+    return \"Final\";\n+\n+  case REF_PHANTOM:\n+    return \"Phantom\";\n+\n+  default:\n+    ShouldNotReachHere();\n+    return \"Unknown\";\n+  }\n+}\n+\n+static volatile oop* reference_referent_addr(oop reference) {\n+  return (volatile oop*)java_lang_ref_Reference::referent_addr_raw(reference);\n+}\n+\n+static oop reference_referent(oop reference) {\n+  return Atomic::load(reference_referent_addr(reference));\n+}\n+\n+static void reference_clear_referent(oop reference) {\n+  java_lang_ref_Reference::clear_referent_raw(reference);\n+}\n+\n+static oop* reference_discovered_addr(oop reference) {\n+  return (oop*)java_lang_ref_Reference::discovered_addr_raw(reference);\n+}\n+\n+static oop reference_discovered(oop reference) {\n+  return *reference_discovered_addr(reference);\n+}\n+\n+static void reference_set_discovered(oop reference, oop discovered) {\n+  java_lang_ref_Reference::set_discovered_raw(reference, discovered);\n+}\n+\n+static oop* reference_next_addr(oop reference) {\n+  return (oop*)java_lang_ref_Reference::next_addr_raw(reference);\n+}\n+\n+static oop reference_next(oop reference) {\n+  return *reference_next_addr(reference);\n+}\n+\n+static void reference_set_next(oop reference, oop next) {\n+  java_lang_ref_Reference::set_next_raw(reference, next);\n+}\n+\n+static void soft_reference_update_clock() {\n+  const jlong now = os::javaTimeNanos() \/ NANOSECS_PER_MILLISEC;\n+  java_lang_ref_SoftReference::set_clock(now);\n+}\n+\n+XReferenceProcessor::XReferenceProcessor(XWorkers* workers) :\n+    _workers(workers),\n+    _soft_reference_policy(NULL),\n+    _encountered_count(),\n+    _discovered_count(),\n+    _enqueued_count(),\n+    _discovered_list(NULL),\n+    _pending_list(NULL),\n+    _pending_list_tail(_pending_list.addr()) {}\n+\n+void XReferenceProcessor::set_soft_reference_policy(bool clear) {\n+  static AlwaysClearPolicy always_clear_policy;\n+  static LRUMaxHeapPolicy lru_max_heap_policy;\n+\n+  if (clear) {\n+    log_info(gc, ref)(\"Clearing All SoftReferences\");\n+    _soft_reference_policy = &always_clear_policy;\n+  } else {\n+    _soft_reference_policy = &lru_max_heap_policy;\n+  }\n+\n+  _soft_reference_policy->setup();\n+}\n+\n+bool XReferenceProcessor::is_inactive(oop reference, oop referent, ReferenceType type) const {\n+  if (type == REF_FINAL) {\n+    \/\/ A FinalReference is inactive if its next field is non-null. An application can't\n+    \/\/ call enqueue() or clear() on a FinalReference.\n+    return reference_next(reference) != NULL;\n+  } else {\n+    \/\/ A non-FinalReference is inactive if the referent is null. The referent can only\n+    \/\/ be null if the application called Reference.enqueue() or Reference.clear().\n+    return referent == NULL;\n+  }\n+}\n+\n+bool XReferenceProcessor::is_strongly_live(oop referent) const {\n+  return XHeap::heap()->is_object_strongly_live(XOop::to_address(referent));\n+}\n+\n+bool XReferenceProcessor::is_softly_live(oop reference, ReferenceType type) const {\n+  if (type != REF_SOFT) {\n+    \/\/ Not a SoftReference\n+    return false;\n+  }\n+\n+  \/\/ Ask SoftReference policy\n+  const jlong clock = java_lang_ref_SoftReference::clock();\n+  assert(clock != 0, \"Clock not initialized\");\n+  assert(_soft_reference_policy != NULL, \"Policy not initialized\");\n+  return !_soft_reference_policy->should_clear_reference(reference, clock);\n+}\n+\n+bool XReferenceProcessor::should_discover(oop reference, ReferenceType type) const {\n+  volatile oop* const referent_addr = reference_referent_addr(reference);\n+  const oop referent = XBarrier::weak_load_barrier_on_oop_field(referent_addr);\n+\n+  if (is_inactive(reference, referent, type)) {\n+    return false;\n+  }\n+\n+  if (is_strongly_live(referent)) {\n+    return false;\n+  }\n+\n+  if (is_softly_live(reference, type)) {\n+    return false;\n+  }\n+\n+  \/\/ PhantomReferences with finalizable marked referents should technically not have\n+  \/\/ to be discovered. However, InstanceRefKlass::oop_oop_iterate_ref_processing()\n+  \/\/ does not know about the finalizable mark concept, and will therefore mark\n+  \/\/ referents in non-discovered PhantomReferences as strongly live. To prevent\n+  \/\/ this, we always discover PhantomReferences with finalizable marked referents.\n+  \/\/ They will automatically be dropped during the reference processing phase.\n+  return true;\n+}\n+\n+bool XReferenceProcessor::should_drop(oop reference, ReferenceType type) const {\n+  const oop referent = reference_referent(reference);\n+  if (referent == NULL) {\n+    \/\/ Reference has been cleared, by a call to Reference.enqueue()\n+    \/\/ or Reference.clear() from the application, which means we\n+    \/\/ should drop the reference.\n+    return true;\n+  }\n+\n+  \/\/ Check if the referent is still alive, in which case we should\n+  \/\/ drop the reference.\n+  if (type == REF_PHANTOM) {\n+    return XBarrier::is_alive_barrier_on_phantom_oop(referent);\n+  } else {\n+    return XBarrier::is_alive_barrier_on_weak_oop(referent);\n+  }\n+}\n+\n+void XReferenceProcessor::keep_alive(oop reference, ReferenceType type) const {\n+  volatile oop* const p = reference_referent_addr(reference);\n+  if (type == REF_PHANTOM) {\n+    XBarrier::keep_alive_barrier_on_phantom_oop_field(p);\n+  } else {\n+    XBarrier::keep_alive_barrier_on_weak_oop_field(p);\n+  }\n+}\n+\n+void XReferenceProcessor::make_inactive(oop reference, ReferenceType type) const {\n+  if (type == REF_FINAL) {\n+    \/\/ Don't clear referent. It is needed by the Finalizer thread to make the call\n+    \/\/ to finalize(). A FinalReference is instead made inactive by self-looping the\n+    \/\/ next field. An application can't call FinalReference.enqueue(), so there is\n+    \/\/ no race to worry about when setting the next field.\n+    assert(reference_next(reference) == NULL, \"Already inactive\");\n+    reference_set_next(reference, reference);\n+  } else {\n+    \/\/ Clear referent\n+    reference_clear_referent(reference);\n+  }\n+}\n+\n+void XReferenceProcessor::discover(oop reference, ReferenceType type) {\n+  log_trace(gc, ref)(\"Discovered Reference: \" PTR_FORMAT \" (%s)\", p2i(reference), reference_type_name(type));\n+\n+  \/\/ Update statistics\n+  _discovered_count.get()[type]++;\n+\n+  if (type == REF_FINAL) {\n+    \/\/ Mark referent (and its reachable subgraph) finalizable. This avoids\n+    \/\/ the problem of later having to mark those objects if the referent is\n+    \/\/ still final reachable during processing.\n+    volatile oop* const referent_addr = reference_referent_addr(reference);\n+    XBarrier::mark_barrier_on_oop_field(referent_addr, true \/* finalizable *\/);\n+  }\n+\n+  \/\/ Add reference to discovered list\n+  assert(reference_discovered(reference) == NULL, \"Already discovered\");\n+  oop* const list = _discovered_list.addr();\n+  reference_set_discovered(reference, *list);\n+  *list = reference;\n+}\n+\n+bool XReferenceProcessor::discover_reference(oop reference, ReferenceType type) {\n+  if (!RegisterReferences) {\n+    \/\/ Reference processing disabled\n+    return false;\n+  }\n+\n+  log_trace(gc, ref)(\"Encountered Reference: \" PTR_FORMAT \" (%s)\", p2i(reference), reference_type_name(type));\n+\n+  \/\/ Update statistics\n+  _encountered_count.get()[type]++;\n+\n+  if (!should_discover(reference, type)) {\n+    \/\/ Not discovered\n+    return false;\n+  }\n+\n+  discover(reference, type);\n+\n+  \/\/ Discovered\n+  return true;\n+}\n+\n+oop XReferenceProcessor::drop(oop reference, ReferenceType type) {\n+  log_trace(gc, ref)(\"Dropped Reference: \" PTR_FORMAT \" (%s)\", p2i(reference), reference_type_name(type));\n+\n+  \/\/ Keep referent alive\n+  keep_alive(reference, type);\n+\n+  \/\/ Unlink and return next in list\n+  const oop next = reference_discovered(reference);\n+  reference_set_discovered(reference, NULL);\n+  return next;\n+}\n+\n+oop* XReferenceProcessor::keep(oop reference, ReferenceType type) {\n+  log_trace(gc, ref)(\"Enqueued Reference: \" PTR_FORMAT \" (%s)\", p2i(reference), reference_type_name(type));\n+\n+  \/\/ Update statistics\n+  _enqueued_count.get()[type]++;\n+\n+  \/\/ Make reference inactive\n+  make_inactive(reference, type);\n+\n+  \/\/ Return next in list\n+  return reference_discovered_addr(reference);\n+}\n+\n+void XReferenceProcessor::work() {\n+  \/\/ Process discovered references\n+  oop* const list = _discovered_list.addr();\n+  oop* p = list;\n+\n+  while (*p != NULL) {\n+    const oop reference = *p;\n+    const ReferenceType type = reference_type(reference);\n+\n+    if (should_drop(reference, type)) {\n+      *p = drop(reference, type);\n+    } else {\n+      p = keep(reference, type);\n+    }\n+  }\n+\n+  \/\/ Prepend discovered references to internal pending list\n+  if (*list != NULL) {\n+    *p = Atomic::xchg(_pending_list.addr(), *list);\n+    if (*p == NULL) {\n+      \/\/ First to prepend to list, record tail\n+      _pending_list_tail = p;\n+    }\n+\n+    \/\/ Clear discovered list\n+    *list = NULL;\n+  }\n+}\n+\n+bool XReferenceProcessor::is_empty() const {\n+  XPerWorkerConstIterator<oop> iter(&_discovered_list);\n+  for (const oop* list; iter.next(&list);) {\n+    if (*list != NULL) {\n+      return false;\n+    }\n+  }\n+\n+  if (_pending_list.get() != NULL) {\n+    return false;\n+  }\n+\n+  return true;\n+}\n+\n+void XReferenceProcessor::reset_statistics() {\n+  assert(is_empty(), \"Should be empty\");\n+\n+  \/\/ Reset encountered\n+  XPerWorkerIterator<Counters> iter_encountered(&_encountered_count);\n+  for (Counters* counters; iter_encountered.next(&counters);) {\n+    for (int i = REF_SOFT; i <= REF_PHANTOM; i++) {\n+      (*counters)[i] = 0;\n+    }\n+  }\n+\n+  \/\/ Reset discovered\n+  XPerWorkerIterator<Counters> iter_discovered(&_discovered_count);\n+  for (Counters* counters; iter_discovered.next(&counters);) {\n+    for (int i = REF_SOFT; i <= REF_PHANTOM; i++) {\n+      (*counters)[i] = 0;\n+    }\n+  }\n+\n+  \/\/ Reset enqueued\n+  XPerWorkerIterator<Counters> iter_enqueued(&_enqueued_count);\n+  for (Counters* counters; iter_enqueued.next(&counters);) {\n+    for (int i = REF_SOFT; i <= REF_PHANTOM; i++) {\n+      (*counters)[i] = 0;\n+    }\n+  }\n+}\n+\n+void XReferenceProcessor::collect_statistics() {\n+  Counters encountered = {};\n+  Counters discovered = {};\n+  Counters enqueued = {};\n+\n+  \/\/ Sum encountered\n+  XPerWorkerConstIterator<Counters> iter_encountered(&_encountered_count);\n+  for (const Counters* counters; iter_encountered.next(&counters);) {\n+    for (int i = REF_SOFT; i <= REF_PHANTOM; i++) {\n+      encountered[i] += (*counters)[i];\n+    }\n+  }\n+\n+  \/\/ Sum discovered\n+  XPerWorkerConstIterator<Counters> iter_discovered(&_discovered_count);\n+  for (const Counters* counters; iter_discovered.next(&counters);) {\n+    for (int i = REF_SOFT; i <= REF_PHANTOM; i++) {\n+      discovered[i] += (*counters)[i];\n+    }\n+  }\n+\n+  \/\/ Sum enqueued\n+  XPerWorkerConstIterator<Counters> iter_enqueued(&_enqueued_count);\n+  for (const Counters* counters; iter_enqueued.next(&counters);) {\n+    for (int i = REF_SOFT; i <= REF_PHANTOM; i++) {\n+      enqueued[i] += (*counters)[i];\n+    }\n+  }\n+\n+  \/\/ Update statistics\n+  XStatReferences::set_soft(encountered[REF_SOFT], discovered[REF_SOFT], enqueued[REF_SOFT]);\n+  XStatReferences::set_weak(encountered[REF_WEAK], discovered[REF_WEAK], enqueued[REF_WEAK]);\n+  XStatReferences::set_final(encountered[REF_FINAL], discovered[REF_FINAL], enqueued[REF_FINAL]);\n+  XStatReferences::set_phantom(encountered[REF_PHANTOM], discovered[REF_PHANTOM], enqueued[REF_PHANTOM]);\n+\n+  \/\/ Trace statistics\n+  const ReferenceProcessorStats stats(discovered[REF_SOFT],\n+                                      discovered[REF_WEAK],\n+                                      discovered[REF_FINAL],\n+                                      discovered[REF_PHANTOM]);\n+  XTracer::tracer()->report_gc_reference_stats(stats);\n+}\n+\n+class XReferenceProcessorTask : public XTask {\n+private:\n+  XReferenceProcessor* const _reference_processor;\n+\n+public:\n+  XReferenceProcessorTask(XReferenceProcessor* reference_processor) :\n+      XTask(\"XReferenceProcessorTask\"),\n+      _reference_processor(reference_processor) {}\n+\n+  virtual void work() {\n+    _reference_processor->work();\n+  }\n+};\n+\n+void XReferenceProcessor::process_references() {\n+  XStatTimer timer(XSubPhaseConcurrentReferencesProcess);\n+\n+  \/\/ Process discovered lists\n+  XReferenceProcessorTask task(this);\n+  _workers->run(&task);\n+\n+  \/\/ Update SoftReference clock\n+  soft_reference_update_clock();\n+\n+  \/\/ Collect, log and trace statistics\n+  collect_statistics();\n+}\n+\n+void XReferenceProcessor::enqueue_references() {\n+  XStatTimer timer(XSubPhaseConcurrentReferencesEnqueue);\n+\n+  if (_pending_list.get() == NULL) {\n+    \/\/ Nothing to enqueue\n+    return;\n+  }\n+\n+  {\n+    \/\/ Heap_lock protects external pending list\n+    MonitorLocker ml(Heap_lock);\n+\n+    \/\/ Prepend internal pending list to external pending list\n+    *_pending_list_tail = Universe::swap_reference_pending_list(_pending_list.get());\n+\n+    \/\/ Notify ReferenceHandler thread\n+    ml.notify_all();\n+  }\n+\n+  \/\/ Reset internal pending list\n+  _pending_list.set(NULL);\n+  _pending_list_tail = _pending_list.addr();\n+}\n","filename":"src\/hotspot\/share\/gc\/x\/xReferenceProcessor.cpp","additions":459,"deletions":0,"binary":false,"changes":459,"status":"added"},{"patch":"@@ -0,0 +1,79 @@\n+\/*\n+ * Copyright (c) 2015, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XREFERENCEPROCESSOR_HPP\n+#define SHARE_GC_X_XREFERENCEPROCESSOR_HPP\n+\n+#include \"gc\/shared\/referenceDiscoverer.hpp\"\n+#include \"gc\/x\/xValue.hpp\"\n+\n+class ReferencePolicy;\n+class XWorkers;\n+\n+class XReferenceProcessor : public ReferenceDiscoverer {\n+  friend class XReferenceProcessorTask;\n+\n+private:\n+  static const size_t reference_type_count = REF_PHANTOM + 1;\n+  typedef size_t Counters[reference_type_count];\n+\n+  XWorkers* const      _workers;\n+  ReferencePolicy*     _soft_reference_policy;\n+  XPerWorker<Counters> _encountered_count;\n+  XPerWorker<Counters> _discovered_count;\n+  XPerWorker<Counters> _enqueued_count;\n+  XPerWorker<oop>      _discovered_list;\n+  XContended<oop>      _pending_list;\n+  oop*                 _pending_list_tail;\n+\n+  bool is_inactive(oop reference, oop referent, ReferenceType type) const;\n+  bool is_strongly_live(oop referent) const;\n+  bool is_softly_live(oop reference, ReferenceType type) const;\n+\n+  bool should_discover(oop reference, ReferenceType type) const;\n+  bool should_drop(oop reference, ReferenceType type) const;\n+  void keep_alive(oop reference, ReferenceType type) const;\n+  void make_inactive(oop reference, ReferenceType type) const;\n+\n+  void discover(oop reference, ReferenceType type);\n+\n+  oop drop(oop reference, ReferenceType type);\n+  oop* keep(oop reference, ReferenceType type);\n+\n+  bool is_empty() const;\n+\n+  void work();\n+  void collect_statistics();\n+\n+public:\n+  XReferenceProcessor(XWorkers* workers);\n+\n+  void set_soft_reference_policy(bool clear);\n+  void reset_statistics();\n+\n+  virtual bool discover_reference(oop reference, ReferenceType type);\n+  void process_references();\n+  void enqueue_references();\n+};\n+\n+#endif \/\/ SHARE_GC_X_XREFERENCEPROCESSOR_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xReferenceProcessor.hpp","additions":79,"deletions":0,"binary":false,"changes":79,"status":"added"},{"patch":"@@ -0,0 +1,419 @@\n+\/*\n+ * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/shared\/gc_globals.hpp\"\n+#include \"gc\/x\/xAbort.inline.hpp\"\n+#include \"gc\/x\/xAddress.inline.hpp\"\n+#include \"gc\/x\/xBarrier.inline.hpp\"\n+#include \"gc\/x\/xForwarding.inline.hpp\"\n+#include \"gc\/x\/xHeap.inline.hpp\"\n+#include \"gc\/x\/xPage.inline.hpp\"\n+#include \"gc\/x\/xRelocate.hpp\"\n+#include \"gc\/x\/xRelocationSet.inline.hpp\"\n+#include \"gc\/x\/xStat.hpp\"\n+#include \"gc\/x\/xTask.hpp\"\n+#include \"gc\/x\/xThread.inline.hpp\"\n+#include \"gc\/x\/xWorkers.hpp\"\n+#include \"prims\/jvmtiTagMap.hpp\"\n+#include \"runtime\/atomic.hpp\"\n+#include \"utilities\/debug.hpp\"\n+\n+XRelocate::XRelocate(XWorkers* workers) :\n+    _workers(workers) {}\n+\n+static uintptr_t forwarding_index(XForwarding* forwarding, uintptr_t from_addr) {\n+  const uintptr_t from_offset = XAddress::offset(from_addr);\n+  return (from_offset - forwarding->start()) >> forwarding->object_alignment_shift();\n+}\n+\n+static uintptr_t forwarding_find(XForwarding* forwarding, uintptr_t from_addr, XForwardingCursor* cursor) {\n+  const uintptr_t from_index = forwarding_index(forwarding, from_addr);\n+  const XForwardingEntry entry = forwarding->find(from_index, cursor);\n+  return entry.populated() ? XAddress::good(entry.to_offset()) : 0;\n+}\n+\n+static uintptr_t forwarding_insert(XForwarding* forwarding, uintptr_t from_addr, uintptr_t to_addr, XForwardingCursor* cursor) {\n+  const uintptr_t from_index = forwarding_index(forwarding, from_addr);\n+  const uintptr_t to_offset = XAddress::offset(to_addr);\n+  const uintptr_t to_offset_final = forwarding->insert(from_index, to_offset, cursor);\n+  return XAddress::good(to_offset_final);\n+}\n+\n+static uintptr_t relocate_object_inner(XForwarding* forwarding, uintptr_t from_addr, XForwardingCursor* cursor) {\n+  assert(XHeap::heap()->is_object_live(from_addr), \"Should be live\");\n+\n+  \/\/ Allocate object\n+  const size_t size = XUtils::object_size(from_addr);\n+  const uintptr_t to_addr = XHeap::heap()->alloc_object_for_relocation(size);\n+  if (to_addr == 0) {\n+    \/\/ Allocation failed\n+    return 0;\n+  }\n+\n+  \/\/ Copy object\n+  XUtils::object_copy_disjoint(from_addr, to_addr, size);\n+\n+  \/\/ Insert forwarding\n+  const uintptr_t to_addr_final = forwarding_insert(forwarding, from_addr, to_addr, cursor);\n+  if (to_addr_final != to_addr) {\n+    \/\/ Already relocated, try undo allocation\n+    XHeap::heap()->undo_alloc_object_for_relocation(to_addr, size);\n+  }\n+\n+  return to_addr_final;\n+}\n+\n+uintptr_t XRelocate::relocate_object(XForwarding* forwarding, uintptr_t from_addr) const {\n+  XForwardingCursor cursor;\n+\n+  \/\/ Lookup forwarding\n+  uintptr_t to_addr = forwarding_find(forwarding, from_addr, &cursor);\n+  if (to_addr != 0) {\n+    \/\/ Already relocated\n+    return to_addr;\n+  }\n+\n+  \/\/ Relocate object\n+  if (forwarding->retain_page()) {\n+    to_addr = relocate_object_inner(forwarding, from_addr, &cursor);\n+    forwarding->release_page();\n+\n+    if (to_addr != 0) {\n+      \/\/ Success\n+      return to_addr;\n+    }\n+\n+    \/\/ Failed to relocate object. Wait for a worker thread to complete\n+    \/\/ relocation of this page, and then forward the object. If the GC\n+    \/\/ aborts the relocation phase before the page has been relocated,\n+    \/\/ then wait return false and we just forward the object in-place.\n+    if (!forwarding->wait_page_released()) {\n+      \/\/ Forward object in-place\n+      return forwarding_insert(forwarding, from_addr, from_addr, &cursor);\n+    }\n+  }\n+\n+  \/\/ Forward object\n+  return forward_object(forwarding, from_addr);\n+}\n+\n+uintptr_t XRelocate::forward_object(XForwarding* forwarding, uintptr_t from_addr) const {\n+  XForwardingCursor cursor;\n+  const uintptr_t to_addr = forwarding_find(forwarding, from_addr, &cursor);\n+  assert(to_addr != 0, \"Should be forwarded\");\n+  return to_addr;\n+}\n+\n+static XPage* alloc_page(const XForwarding* forwarding) {\n+  if (ZStressRelocateInPlace) {\n+    \/\/ Simulate failure to allocate a new page. This will\n+    \/\/ cause the page being relocated to be relocated in-place.\n+    return NULL;\n+  }\n+\n+  XAllocationFlags flags;\n+  flags.set_non_blocking();\n+  flags.set_worker_relocation();\n+  return XHeap::heap()->alloc_page(forwarding->type(), forwarding->size(), flags);\n+}\n+\n+static void free_page(XPage* page) {\n+  XHeap::heap()->free_page(page, true \/* reclaimed *\/);\n+}\n+\n+static bool should_free_target_page(XPage* page) {\n+  \/\/ Free target page if it is empty. We can end up with an empty target\n+  \/\/ page if we allocated a new target page, and then lost the race to\n+  \/\/ relocate the remaining objects, leaving the target page empty when\n+  \/\/ relocation completed.\n+  return page != NULL && page->top() == page->start();\n+}\n+\n+class XRelocateSmallAllocator {\n+private:\n+  volatile size_t _in_place_count;\n+\n+public:\n+  XRelocateSmallAllocator() :\n+      _in_place_count(0) {}\n+\n+  XPage* alloc_target_page(XForwarding* forwarding, XPage* target) {\n+    XPage* const page = alloc_page(forwarding);\n+    if (page == NULL) {\n+      Atomic::inc(&_in_place_count);\n+    }\n+\n+    return page;\n+  }\n+\n+  void share_target_page(XPage* page) {\n+    \/\/ Does nothing\n+  }\n+\n+  void free_target_page(XPage* page) {\n+    if (should_free_target_page(page)) {\n+      free_page(page);\n+    }\n+  }\n+\n+  void free_relocated_page(XPage* page) {\n+    free_page(page);\n+  }\n+\n+  uintptr_t alloc_object(XPage* page, size_t size) const {\n+    return (page != NULL) ? page->alloc_object(size) : 0;\n+  }\n+\n+  void undo_alloc_object(XPage* page, uintptr_t addr, size_t size) const {\n+    page->undo_alloc_object(addr, size);\n+  }\n+\n+  const size_t in_place_count() const {\n+    return _in_place_count;\n+  }\n+};\n+\n+class XRelocateMediumAllocator {\n+private:\n+  XConditionLock      _lock;\n+  XPage*              _shared;\n+  bool                _in_place;\n+  volatile size_t     _in_place_count;\n+\n+public:\n+  XRelocateMediumAllocator() :\n+      _lock(),\n+      _shared(NULL),\n+      _in_place(false),\n+      _in_place_count(0) {}\n+\n+  ~XRelocateMediumAllocator() {\n+    if (should_free_target_page(_shared)) {\n+      free_page(_shared);\n+    }\n+  }\n+\n+  XPage* alloc_target_page(XForwarding* forwarding, XPage* target) {\n+    XLocker<XConditionLock> locker(&_lock);\n+\n+    \/\/ Wait for any ongoing in-place relocation to complete\n+    while (_in_place) {\n+      _lock.wait();\n+    }\n+\n+    \/\/ Allocate a new page only if the shared page is the same as the\n+    \/\/ current target page. The shared page will be different from the\n+    \/\/ current target page if another thread shared a page, or allocated\n+    \/\/ a new page.\n+    if (_shared == target) {\n+      _shared = alloc_page(forwarding);\n+      if (_shared == NULL) {\n+        Atomic::inc(&_in_place_count);\n+        _in_place = true;\n+      }\n+    }\n+\n+    return _shared;\n+  }\n+\n+  void share_target_page(XPage* page) {\n+    XLocker<XConditionLock> locker(&_lock);\n+\n+    assert(_in_place, \"Invalid state\");\n+    assert(_shared == NULL, \"Invalid state\");\n+    assert(page != NULL, \"Invalid page\");\n+\n+    _shared = page;\n+    _in_place = false;\n+\n+    _lock.notify_all();\n+  }\n+\n+  void free_target_page(XPage* page) {\n+    \/\/ Does nothing\n+  }\n+\n+  void free_relocated_page(XPage* page) {\n+    free_page(page);\n+  }\n+\n+  uintptr_t alloc_object(XPage* page, size_t size) const {\n+    return (page != NULL) ? page->alloc_object_atomic(size) : 0;\n+  }\n+\n+  void undo_alloc_object(XPage* page, uintptr_t addr, size_t size) const {\n+    page->undo_alloc_object_atomic(addr, size);\n+  }\n+\n+  const size_t in_place_count() const {\n+    return _in_place_count;\n+  }\n+};\n+\n+template <typename Allocator>\n+class XRelocateClosure : public ObjectClosure {\n+private:\n+  Allocator* const _allocator;\n+  XForwarding*     _forwarding;\n+  XPage*           _target;\n+\n+  bool relocate_object(uintptr_t from_addr) const {\n+    XForwardingCursor cursor;\n+\n+    \/\/ Lookup forwarding\n+    if (forwarding_find(_forwarding, from_addr, &cursor) != 0) {\n+      \/\/ Already relocated\n+      return true;\n+    }\n+\n+    \/\/ Allocate object\n+    const size_t size = XUtils::object_size(from_addr);\n+    const uintptr_t to_addr = _allocator->alloc_object(_target, size);\n+    if (to_addr == 0) {\n+      \/\/ Allocation failed\n+      return false;\n+    }\n+\n+    \/\/ Copy object. Use conjoint copying if we are relocating\n+    \/\/ in-place and the new object overlapps with the old object.\n+    if (_forwarding->in_place() && to_addr + size > from_addr) {\n+      XUtils::object_copy_conjoint(from_addr, to_addr, size);\n+    } else {\n+      XUtils::object_copy_disjoint(from_addr, to_addr, size);\n+    }\n+\n+    \/\/ Insert forwarding\n+    if (forwarding_insert(_forwarding, from_addr, to_addr, &cursor) != to_addr) {\n+      \/\/ Already relocated, undo allocation\n+      _allocator->undo_alloc_object(_target, to_addr, size);\n+    }\n+\n+    return true;\n+  }\n+\n+  virtual void do_object(oop obj) {\n+    const uintptr_t addr = XOop::to_address(obj);\n+    assert(XHeap::heap()->is_object_live(addr), \"Should be live\");\n+\n+    while (!relocate_object(addr)) {\n+      \/\/ Allocate a new target page, or if that fails, use the page being\n+      \/\/ relocated as the new target, which will cause it to be relocated\n+      \/\/ in-place.\n+      _target = _allocator->alloc_target_page(_forwarding, _target);\n+      if (_target != NULL) {\n+        continue;\n+      }\n+\n+      \/\/ Claim the page being relocated to block other threads from accessing\n+      \/\/ it, or its forwarding table, until it has been released (relocation\n+      \/\/ completed).\n+      _target = _forwarding->claim_page();\n+      _target->reset_for_in_place_relocation();\n+      _forwarding->set_in_place();\n+    }\n+  }\n+\n+public:\n+  XRelocateClosure(Allocator* allocator) :\n+      _allocator(allocator),\n+      _forwarding(NULL),\n+      _target(NULL) {}\n+\n+  ~XRelocateClosure() {\n+    _allocator->free_target_page(_target);\n+  }\n+\n+  void do_forwarding(XForwarding* forwarding) {\n+    _forwarding = forwarding;\n+\n+    \/\/ Check if we should abort\n+    if (XAbort::should_abort()) {\n+      _forwarding->abort_page();\n+      return;\n+    }\n+\n+    \/\/ Relocate objects\n+    _forwarding->object_iterate(this);\n+\n+    \/\/ Verify\n+    if (ZVerifyForwarding) {\n+      _forwarding->verify();\n+    }\n+\n+    \/\/ Release relocated page\n+    _forwarding->release_page();\n+\n+    if (_forwarding->in_place()) {\n+      \/\/ The relocated page has been relocated in-place and should not\n+      \/\/ be freed. Keep it as target page until it is full, and offer to\n+      \/\/ share it with other worker threads.\n+      _allocator->share_target_page(_target);\n+    } else {\n+      \/\/ Detach and free relocated page\n+      XPage* const page = _forwarding->detach_page();\n+      _allocator->free_relocated_page(page);\n+    }\n+  }\n+};\n+\n+class XRelocateTask : public XTask {\n+private:\n+  XRelocationSetParallelIterator _iter;\n+  XRelocateSmallAllocator        _small_allocator;\n+  XRelocateMediumAllocator       _medium_allocator;\n+\n+  static bool is_small(XForwarding* forwarding) {\n+    return forwarding->type() == XPageTypeSmall;\n+  }\n+\n+public:\n+  XRelocateTask(XRelocationSet* relocation_set) :\n+      XTask(\"XRelocateTask\"),\n+      _iter(relocation_set),\n+      _small_allocator(),\n+      _medium_allocator() {}\n+\n+  ~XRelocateTask() {\n+    XStatRelocation::set_at_relocate_end(_small_allocator.in_place_count(),\n+                                         _medium_allocator.in_place_count());\n+  }\n+\n+  virtual void work() {\n+    XRelocateClosure<XRelocateSmallAllocator> small(&_small_allocator);\n+    XRelocateClosure<XRelocateMediumAllocator> medium(&_medium_allocator);\n+\n+    for (XForwarding* forwarding; _iter.next(&forwarding);) {\n+      if (is_small(forwarding)) {\n+        small.do_forwarding(forwarding);\n+      } else {\n+        medium.do_forwarding(forwarding);\n+      }\n+    }\n+  }\n+};\n+\n+void XRelocate::relocate(XRelocationSet* relocation_set) {\n+  XRelocateTask task(relocation_set);\n+  _workers->run(&task);\n+}\n","filename":"src\/hotspot\/share\/gc\/x\/xRelocate.cpp","additions":419,"deletions":0,"binary":false,"changes":419,"status":"added"},{"patch":"@@ -0,0 +1,49 @@\n+\/*\n+ * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XRELOCATE_HPP\n+#define SHARE_GC_X_XRELOCATE_HPP\n+\n+#include \"gc\/x\/xRelocationSet.hpp\"\n+\n+class XForwarding;\n+class XWorkers;\n+\n+class XRelocate {\n+  friend class XRelocateTask;\n+\n+private:\n+  XWorkers* const _workers;\n+\n+  void work(XRelocationSetParallelIterator* iter);\n+\n+public:\n+  XRelocate(XWorkers* workers);\n+\n+  uintptr_t relocate_object(XForwarding* forwarding, uintptr_t from_addr) const;\n+  uintptr_t forward_object(XForwarding* forwarding, uintptr_t from_addr) const;\n+\n+  void relocate(XRelocationSet* relocation_set);\n+};\n+\n+#endif \/\/ SHARE_GC_X_XRELOCATE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xRelocate.hpp","additions":49,"deletions":0,"binary":false,"changes":49,"status":"added"},{"patch":"@@ -0,0 +1,135 @@\n+\/*\n+ * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/x\/xArray.inline.hpp\"\n+#include \"gc\/x\/xForwarding.inline.hpp\"\n+#include \"gc\/x\/xForwardingAllocator.inline.hpp\"\n+#include \"gc\/x\/xRelocationSet.inline.hpp\"\n+#include \"gc\/x\/xRelocationSetSelector.inline.hpp\"\n+#include \"gc\/x\/xStat.hpp\"\n+#include \"gc\/x\/xTask.hpp\"\n+#include \"gc\/x\/xWorkers.hpp\"\n+#include \"runtime\/atomic.hpp\"\n+#include \"utilities\/debug.hpp\"\n+\n+class XRelocationSetInstallTask : public XTask {\n+private:\n+  XForwardingAllocator* const    _allocator;\n+  XForwarding**                  _forwardings;\n+  const size_t                   _nforwardings;\n+  XArrayParallelIterator<XPage*> _small_iter;\n+  XArrayParallelIterator<XPage*> _medium_iter;\n+  volatile size_t                _small_next;\n+  volatile size_t                _medium_next;\n+\n+  void install(XForwarding* forwarding, volatile size_t* next) {\n+    const size_t index = Atomic::fetch_and_add(next, 1u);\n+    assert(index < _nforwardings, \"Invalid index\");\n+    _forwardings[index] = forwarding;\n+  }\n+\n+  void install_small(XForwarding* forwarding) {\n+    install(forwarding, &_small_next);\n+  }\n+\n+  void install_medium(XForwarding* forwarding) {\n+    install(forwarding, &_medium_next);\n+  }\n+\n+public:\n+  XRelocationSetInstallTask(XForwardingAllocator* allocator, const XRelocationSetSelector* selector) :\n+      XTask(\"XRelocationSetInstallTask\"),\n+      _allocator(allocator),\n+      _forwardings(NULL),\n+      _nforwardings(selector->small()->length() + selector->medium()->length()),\n+      _small_iter(selector->small()),\n+      _medium_iter(selector->medium()),\n+      _small_next(selector->medium()->length()),\n+      _medium_next(0) {\n+\n+    \/\/ Reset the allocator to have room for the relocation\n+    \/\/ set, all forwardings, and all forwarding entries.\n+    const size_t relocation_set_size = _nforwardings * sizeof(XForwarding*);\n+    const size_t forwardings_size = _nforwardings * sizeof(XForwarding);\n+    const size_t forwarding_entries_size = selector->forwarding_entries() * sizeof(XForwardingEntry);\n+    _allocator->reset(relocation_set_size + forwardings_size + forwarding_entries_size);\n+\n+    \/\/ Allocate relocation set\n+    _forwardings = new (_allocator->alloc(relocation_set_size)) XForwarding*[_nforwardings];\n+  }\n+\n+  ~XRelocationSetInstallTask() {\n+    assert(_allocator->is_full(), \"Should be full\");\n+  }\n+\n+  virtual void work() {\n+    \/\/ Allocate and install forwardings for small pages\n+    for (XPage* page; _small_iter.next(&page);) {\n+      XForwarding* const forwarding = XForwarding::alloc(_allocator, page);\n+      install_small(forwarding);\n+    }\n+\n+    \/\/ Allocate and install forwardings for medium pages\n+    for (XPage* page; _medium_iter.next(&page);) {\n+      XForwarding* const forwarding = XForwarding::alloc(_allocator, page);\n+      install_medium(forwarding);\n+    }\n+  }\n+\n+  XForwarding** forwardings() const {\n+    return _forwardings;\n+  }\n+\n+  size_t nforwardings() const {\n+    return _nforwardings;\n+  }\n+};\n+\n+XRelocationSet::XRelocationSet(XWorkers* workers) :\n+    _workers(workers),\n+    _allocator(),\n+    _forwardings(NULL),\n+    _nforwardings(0) {}\n+\n+void XRelocationSet::install(const XRelocationSetSelector* selector) {\n+  \/\/ Install relocation set\n+  XRelocationSetInstallTask task(&_allocator, selector);\n+  _workers->run(&task);\n+\n+  _forwardings = task.forwardings();\n+  _nforwardings = task.nforwardings();\n+\n+  \/\/ Update statistics\n+  XStatRelocation::set_at_install_relocation_set(_allocator.size());\n+}\n+\n+void XRelocationSet::reset() {\n+  \/\/ Destroy forwardings\n+  XRelocationSetIterator iter(this);\n+  for (XForwarding* forwarding; iter.next(&forwarding);) {\n+    forwarding->~XForwarding();\n+  }\n+\n+  _nforwardings = 0;\n+}\n","filename":"src\/hotspot\/share\/gc\/x\/xRelocationSet.cpp","additions":135,"deletions":0,"binary":false,"changes":135,"status":"added"},{"patch":"@@ -0,0 +1,59 @@\n+\/*\n+ * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XRELOCATIONSET_HPP\n+#define SHARE_GC_X_XRELOCATIONSET_HPP\n+\n+#include \"gc\/x\/xArray.hpp\"\n+#include \"gc\/x\/xForwardingAllocator.hpp\"\n+\n+class XForwarding;\n+class XRelocationSetSelector;\n+class XWorkers;\n+\n+class XRelocationSet {\n+  template <bool> friend class XRelocationSetIteratorImpl;\n+\n+private:\n+  XWorkers*            _workers;\n+  XForwardingAllocator _allocator;\n+  XForwarding**        _forwardings;\n+  size_t               _nforwardings;\n+\n+public:\n+  XRelocationSet(XWorkers* workers);\n+\n+  void install(const XRelocationSetSelector* selector);\n+  void reset();\n+};\n+\n+template <bool Parallel>\n+class XRelocationSetIteratorImpl : public XArrayIteratorImpl<XForwarding*, Parallel> {\n+public:\n+  XRelocationSetIteratorImpl(XRelocationSet* relocation_set);\n+};\n+\n+using XRelocationSetIterator = XRelocationSetIteratorImpl<false \/* Parallel *\/>;\n+using XRelocationSetParallelIterator = XRelocationSetIteratorImpl<true \/* Parallel *\/>;\n+\n+#endif \/\/ SHARE_GC_X_XRELOCATIONSET_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xRelocationSet.hpp","additions":59,"deletions":0,"binary":false,"changes":59,"status":"added"},{"patch":"@@ -0,0 +1,35 @@\n+\/*\n+ * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XRELOCATIONSET_INLINE_HPP\n+#define SHARE_GC_X_XRELOCATIONSET_INLINE_HPP\n+\n+#include \"gc\/x\/xRelocationSet.hpp\"\n+\n+#include \"gc\/x\/xArray.inline.hpp\"\n+\n+template <bool Parallel>\n+inline XRelocationSetIteratorImpl<Parallel>::XRelocationSetIteratorImpl(XRelocationSet* relocation_set) :\n+    XArrayIteratorImpl<XForwarding*, Parallel>(relocation_set->_forwardings, relocation_set->_nforwardings) {}\n+\n+#endif \/\/ SHARE_GC_X_XRELOCATIONSET_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xRelocationSet.inline.hpp","additions":35,"deletions":0,"binary":false,"changes":35,"status":"added"},{"patch":"@@ -0,0 +1,213 @@\n+\/*\n+ * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/shared\/gc_globals.hpp\"\n+#include \"gc\/x\/xArray.inline.hpp\"\n+#include \"gc\/x\/xForwarding.inline.hpp\"\n+#include \"gc\/x\/xPage.inline.hpp\"\n+#include \"gc\/x\/xRelocationSetSelector.inline.hpp\"\n+#include \"jfr\/jfrEvents.hpp\"\n+#include \"logging\/log.hpp\"\n+#include \"runtime\/globals.hpp\"\n+#include \"utilities\/debug.hpp\"\n+#include \"utilities\/powerOfTwo.hpp\"\n+\n+XRelocationSetSelectorGroupStats::XRelocationSetSelectorGroupStats() :\n+    _npages_candidates(0),\n+    _total(0),\n+    _live(0),\n+    _empty(0),\n+    _npages_selected(0),\n+    _relocate(0) {}\n+\n+XRelocationSetSelectorGroup::XRelocationSetSelectorGroup(const char* name,\n+                                                         uint8_t page_type,\n+                                                         size_t page_size,\n+                                                         size_t object_size_limit) :\n+    _name(name),\n+    _page_type(page_type),\n+    _page_size(page_size),\n+    _object_size_limit(object_size_limit),\n+    _fragmentation_limit(page_size * (ZFragmentationLimit \/ 100)),\n+    _live_pages(),\n+    _forwarding_entries(0),\n+    _stats() {}\n+\n+bool XRelocationSetSelectorGroup::is_disabled() {\n+  \/\/ Medium pages are disabled when their page size is zero\n+  return _page_type == XPageTypeMedium && _page_size == 0;\n+}\n+\n+bool XRelocationSetSelectorGroup::is_selectable() {\n+  \/\/ Large pages are not selectable\n+  return _page_type != XPageTypeLarge;\n+}\n+\n+void XRelocationSetSelectorGroup::semi_sort() {\n+  \/\/ Semi-sort live pages by number of live bytes in ascending order\n+  const size_t npartitions_shift = 11;\n+  const size_t npartitions = (size_t)1 << npartitions_shift;\n+  const size_t partition_size = _page_size >> npartitions_shift;\n+  const size_t partition_size_shift = exact_log2(partition_size);\n+\n+  \/\/ Partition slots\/fingers\n+  int partitions[npartitions] = { \/* zero initialize *\/ };\n+\n+  \/\/ Calculate partition slots\n+  XArrayIterator<XPage*> iter1(&_live_pages);\n+  for (XPage* page; iter1.next(&page);) {\n+    const size_t index = page->live_bytes() >> partition_size_shift;\n+    partitions[index]++;\n+  }\n+\n+  \/\/ Calculate partition fingers\n+  int finger = 0;\n+  for (size_t i = 0; i < npartitions; i++) {\n+    const int slots = partitions[i];\n+    partitions[i] = finger;\n+    finger += slots;\n+  }\n+\n+  \/\/ Allocate destination array\n+  const int npages = _live_pages.length();\n+  XArray<XPage*> sorted_live_pages(npages, npages, NULL);\n+\n+  \/\/ Sort pages into partitions\n+  XArrayIterator<XPage*> iter2(&_live_pages);\n+  for (XPage* page; iter2.next(&page);) {\n+    const size_t index = page->live_bytes() >> partition_size_shift;\n+    const int finger = partitions[index]++;\n+    assert(sorted_live_pages.at(finger) == NULL, \"Invalid finger\");\n+    sorted_live_pages.at_put(finger, page);\n+  }\n+\n+  _live_pages.swap(&sorted_live_pages);\n+}\n+\n+void XRelocationSetSelectorGroup::select_inner() {\n+  \/\/ Calculate the number of pages to relocate by successively including pages in\n+  \/\/ a candidate relocation set and calculate the maximum space requirement for\n+  \/\/ their live objects.\n+  const int npages = _live_pages.length();\n+  int selected_from = 0;\n+  int selected_to = 0;\n+  size_t npages_selected = 0;\n+  size_t selected_live_bytes = 0;\n+  size_t selected_forwarding_entries = 0;\n+  size_t from_live_bytes = 0;\n+  size_t from_forwarding_entries = 0;\n+\n+  semi_sort();\n+\n+  for (int from = 1; from <= npages; from++) {\n+    \/\/ Add page to the candidate relocation set\n+    XPage* const page = _live_pages.at(from - 1);\n+    from_live_bytes += page->live_bytes();\n+    from_forwarding_entries += XForwarding::nentries(page);\n+\n+    \/\/ Calculate the maximum number of pages needed by the candidate relocation set.\n+    \/\/ By subtracting the object size limit from the pages size we get the maximum\n+    \/\/ number of pages that the relocation set is guaranteed to fit in, regardless\n+    \/\/ of in which order the objects are relocated.\n+    const int to = ceil((double)(from_live_bytes) \/ (double)(_page_size - _object_size_limit));\n+\n+    \/\/ Calculate the relative difference in reclaimable space compared to our\n+    \/\/ currently selected final relocation set. If this number is larger than the\n+    \/\/ acceptable fragmentation limit, then the current candidate relocation set\n+    \/\/ becomes our new final relocation set.\n+    const int diff_from = from - selected_from;\n+    const int diff_to = to - selected_to;\n+    const double diff_reclaimable = 100 - percent_of(diff_to, diff_from);\n+    if (diff_reclaimable > ZFragmentationLimit) {\n+      selected_from = from;\n+      selected_to = to;\n+      selected_live_bytes = from_live_bytes;\n+      npages_selected += 1;\n+      selected_forwarding_entries = from_forwarding_entries;\n+    }\n+\n+    log_trace(gc, reloc)(\"Candidate Relocation Set (%s Pages): %d->%d, \"\n+                         \"%.1f%% relative defragmentation, \" SIZE_FORMAT \" forwarding entries, %s\",\n+                         _name, from, to, diff_reclaimable, from_forwarding_entries,\n+                         (selected_from == from) ? \"Selected\" : \"Rejected\");\n+  }\n+\n+  \/\/ Finalize selection\n+  _live_pages.trunc_to(selected_from);\n+  _forwarding_entries = selected_forwarding_entries;\n+\n+  \/\/ Update statistics\n+  _stats._relocate = selected_live_bytes;\n+  _stats._npages_selected = npages_selected;\n+\n+  log_trace(gc, reloc)(\"Relocation Set (%s Pages): %d->%d, %d skipped, \" SIZE_FORMAT \" forwarding entries\",\n+                       _name, selected_from, selected_to, npages - selected_from, selected_forwarding_entries);\n+}\n+\n+void XRelocationSetSelectorGroup::select() {\n+  if (is_disabled()) {\n+    return;\n+  }\n+\n+  EventZRelocationSetGroup event;\n+\n+  if (is_selectable()) {\n+    select_inner();\n+  }\n+\n+  \/\/ Send event\n+  event.commit(_page_type, _stats.npages_candidates(), _stats.total(), _stats.empty(), _stats.npages_selected(), _stats.relocate());\n+}\n+\n+XRelocationSetSelector::XRelocationSetSelector() :\n+    _small(\"Small\", XPageTypeSmall, XPageSizeSmall, XObjectSizeLimitSmall),\n+    _medium(\"Medium\", XPageTypeMedium, XPageSizeMedium, XObjectSizeLimitMedium),\n+    _large(\"Large\", XPageTypeLarge, 0 \/* page_size *\/, 0 \/* object_size_limit *\/),\n+    _empty_pages() {}\n+\n+void XRelocationSetSelector::select() {\n+  \/\/ Select pages to relocate. The resulting relocation set will be\n+  \/\/ sorted such that medium pages comes first, followed by small\n+  \/\/ pages. Pages within each page group will be semi-sorted by live\n+  \/\/ bytes in ascending order. Relocating pages in this order allows\n+  \/\/ us to start reclaiming memory more quickly.\n+\n+  EventZRelocationSet event;\n+\n+  \/\/ Select pages from each group\n+  _large.select();\n+  _medium.select();\n+  _small.select();\n+\n+  \/\/ Send event\n+  event.commit(total(), empty(), relocate());\n+}\n+\n+XRelocationSetSelectorStats XRelocationSetSelector::stats() const {\n+  XRelocationSetSelectorStats stats;\n+  stats._small = _small.stats();\n+  stats._medium = _medium.stats();\n+  stats._large = _large.stats();\n+  return stats;\n+}\n","filename":"src\/hotspot\/share\/gc\/x\/xRelocationSetSelector.cpp","additions":213,"deletions":0,"binary":false,"changes":213,"status":"added"},{"patch":"@@ -0,0 +1,134 @@\n+\/*\n+ * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XRELOCATIONSETSELECTOR_HPP\n+#define SHARE_GC_X_XRELOCATIONSETSELECTOR_HPP\n+\n+#include \"gc\/x\/xArray.hpp\"\n+#include \"memory\/allocation.hpp\"\n+\n+class XPage;\n+\n+class XRelocationSetSelectorGroupStats {\n+  friend class XRelocationSetSelectorGroup;\n+\n+private:\n+  \/\/ Candidate set\n+  size_t _npages_candidates;\n+  size_t _total;\n+  size_t _live;\n+  size_t _empty;\n+\n+  \/\/ Selected set\n+  size_t _npages_selected;\n+  size_t _relocate;\n+\n+public:\n+  XRelocationSetSelectorGroupStats();\n+\n+  size_t npages_candidates() const;\n+  size_t total() const;\n+  size_t live() const;\n+  size_t empty() const;\n+\n+  size_t npages_selected() const;\n+  size_t relocate() const;\n+};\n+\n+class XRelocationSetSelectorStats {\n+  friend class XRelocationSetSelector;\n+\n+private:\n+  XRelocationSetSelectorGroupStats _small;\n+  XRelocationSetSelectorGroupStats _medium;\n+  XRelocationSetSelectorGroupStats _large;\n+\n+public:\n+  const XRelocationSetSelectorGroupStats& small() const;\n+  const XRelocationSetSelectorGroupStats& medium() const;\n+  const XRelocationSetSelectorGroupStats& large() const;\n+};\n+\n+class XRelocationSetSelectorGroup {\n+private:\n+  const char* const                _name;\n+  const uint8_t                    _page_type;\n+  const size_t                     _page_size;\n+  const size_t                     _object_size_limit;\n+  const size_t                     _fragmentation_limit;\n+  XArray<XPage*>                   _live_pages;\n+  size_t                           _forwarding_entries;\n+  XRelocationSetSelectorGroupStats _stats;\n+\n+  bool is_disabled();\n+  bool is_selectable();\n+  void semi_sort();\n+  void select_inner();\n+\n+public:\n+  XRelocationSetSelectorGroup(const char* name,\n+                              uint8_t page_type,\n+                              size_t page_size,\n+                              size_t object_size_limit);\n+\n+  void register_live_page(XPage* page);\n+  void register_empty_page(XPage* page);\n+  void select();\n+\n+  const XArray<XPage*>* selected() const;\n+  size_t forwarding_entries() const;\n+\n+  const XRelocationSetSelectorGroupStats& stats() const;\n+};\n+\n+class XRelocationSetSelector : public StackObj {\n+private:\n+  XRelocationSetSelectorGroup _small;\n+  XRelocationSetSelectorGroup _medium;\n+  XRelocationSetSelectorGroup _large;\n+  XArray<XPage*>              _empty_pages;\n+\n+  size_t total() const;\n+  size_t empty() const;\n+  size_t relocate() const;\n+\n+public:\n+  XRelocationSetSelector();\n+\n+  void register_live_page(XPage* page);\n+  void register_empty_page(XPage* page);\n+\n+  bool should_free_empty_pages(int bulk) const;\n+  const XArray<XPage*>* empty_pages() const;\n+  void clear_empty_pages();\n+\n+  void select();\n+\n+  const XArray<XPage*>* small() const;\n+  const XArray<XPage*>* medium() const;\n+  size_t forwarding_entries() const;\n+\n+  XRelocationSetSelectorStats stats() const;\n+};\n+\n+#endif \/\/ SHARE_GC_X_XRELOCATIONSETSELECTOR_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xRelocationSetSelector.hpp","additions":134,"deletions":0,"binary":false,"changes":134,"status":"added"},{"patch":"@@ -0,0 +1,165 @@\n+\/*\n+ * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XRELOCATIONSETSELECTOR_INLINE_HPP\n+#define SHARE_GC_X_XRELOCATIONSETSELECTOR_INLINE_HPP\n+\n+#include \"gc\/x\/xRelocationSetSelector.hpp\"\n+\n+#include \"gc\/x\/xArray.inline.hpp\"\n+#include \"gc\/x\/xPage.inline.hpp\"\n+\n+inline size_t XRelocationSetSelectorGroupStats::npages_candidates() const {\n+  return _npages_candidates;\n+}\n+\n+inline size_t XRelocationSetSelectorGroupStats::total() const {\n+  return _total;\n+}\n+\n+inline size_t XRelocationSetSelectorGroupStats::live() const {\n+  return _live;\n+}\n+\n+inline size_t XRelocationSetSelectorGroupStats::empty() const {\n+  return _empty;\n+}\n+\n+inline size_t XRelocationSetSelectorGroupStats::npages_selected() const {\n+  return _npages_selected;\n+}\n+\n+inline size_t XRelocationSetSelectorGroupStats::relocate() const {\n+  return _relocate;\n+}\n+\n+inline const XRelocationSetSelectorGroupStats& XRelocationSetSelectorStats::small() const {\n+  return _small;\n+}\n+\n+inline const XRelocationSetSelectorGroupStats& XRelocationSetSelectorStats::medium() const {\n+  return _medium;\n+}\n+\n+inline const XRelocationSetSelectorGroupStats& XRelocationSetSelectorStats::large() const {\n+  return _large;\n+}\n+\n+inline void XRelocationSetSelectorGroup::register_live_page(XPage* page) {\n+  const uint8_t type = page->type();\n+  const size_t size = page->size();\n+  const size_t live = page->live_bytes();\n+  const size_t garbage = size - live;\n+\n+  if (garbage > _fragmentation_limit) {\n+    _live_pages.append(page);\n+  }\n+\n+  _stats._npages_candidates++;\n+  _stats._total += size;\n+  _stats._live += live;\n+}\n+\n+inline void XRelocationSetSelectorGroup::register_empty_page(XPage* page) {\n+  const size_t size = page->size();\n+\n+  _stats._npages_candidates++;\n+  _stats._total += size;\n+  _stats._empty += size;\n+}\n+\n+inline const XArray<XPage*>* XRelocationSetSelectorGroup::selected() const {\n+  return &_live_pages;\n+}\n+\n+inline size_t XRelocationSetSelectorGroup::forwarding_entries() const {\n+  return _forwarding_entries;\n+}\n+\n+inline const XRelocationSetSelectorGroupStats& XRelocationSetSelectorGroup::stats() const {\n+  return _stats;\n+}\n+\n+inline void XRelocationSetSelector::register_live_page(XPage* page) {\n+  const uint8_t type = page->type();\n+\n+  if (type == XPageTypeSmall) {\n+    _small.register_live_page(page);\n+  } else if (type == XPageTypeMedium) {\n+    _medium.register_live_page(page);\n+  } else {\n+    _large.register_live_page(page);\n+  }\n+}\n+\n+inline void XRelocationSetSelector::register_empty_page(XPage* page) {\n+  const uint8_t type = page->type();\n+\n+  if (type == XPageTypeSmall) {\n+    _small.register_empty_page(page);\n+  } else if (type == XPageTypeMedium) {\n+    _medium.register_empty_page(page);\n+  } else {\n+    _large.register_empty_page(page);\n+  }\n+\n+  _empty_pages.append(page);\n+}\n+\n+inline bool XRelocationSetSelector::should_free_empty_pages(int bulk) const {\n+  return _empty_pages.length() >= bulk && _empty_pages.is_nonempty();\n+}\n+\n+inline const XArray<XPage*>* XRelocationSetSelector::empty_pages() const {\n+  return &_empty_pages;\n+}\n+\n+inline void XRelocationSetSelector::clear_empty_pages() {\n+  return _empty_pages.clear();\n+}\n+\n+inline size_t XRelocationSetSelector::total() const {\n+  return _small.stats().total() + _medium.stats().total() + _large.stats().total();\n+}\n+\n+inline size_t XRelocationSetSelector::empty() const {\n+  return _small.stats().empty() + _medium.stats().empty() + _large.stats().empty();\n+}\n+\n+inline size_t XRelocationSetSelector::relocate() const {\n+  return _small.stats().relocate() + _medium.stats().relocate() + _large.stats().relocate();\n+}\n+\n+inline const XArray<XPage*>* XRelocationSetSelector::small() const {\n+  return _small.selected();\n+}\n+\n+inline const XArray<XPage*>* XRelocationSetSelector::medium() const {\n+  return _medium.selected();\n+}\n+\n+inline size_t XRelocationSetSelector::forwarding_entries() const {\n+  return _small.forwarding_entries() + _medium.forwarding_entries();\n+}\n+\n+#endif \/\/ SHARE_GC_X_XRELOCATIONSETSELECTOR_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xRelocationSetSelector.inline.hpp","additions":165,"deletions":0,"binary":false,"changes":165,"status":"added"},{"patch":"@@ -0,0 +1,42 @@\n+\/*\n+ * Copyright (c) 2016, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/x\/xResurrection.hpp\"\n+#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/safepoint.hpp\"\n+#include \"utilities\/debug.hpp\"\n+\n+volatile bool XResurrection::_blocked = false;\n+\n+void XResurrection::block() {\n+  assert(SafepointSynchronize::is_at_safepoint(), \"Should be at safepoint\");\n+  _blocked = true;\n+}\n+\n+void XResurrection::unblock() {\n+  \/\/ No need for anything stronger than a relaxed store here.\n+  \/\/ The preceding handshake makes sure that all non-strong\n+  \/\/ oops have already been healed at this point.\n+  Atomic::store(&_blocked, false);\n+}\n","filename":"src\/hotspot\/share\/gc\/x\/xResurrection.cpp","additions":42,"deletions":0,"binary":false,"changes":42,"status":"added"},{"patch":"@@ -0,0 +1,39 @@\n+\/*\n+ * Copyright (c) 2016, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XRESURRECTION_HPP\n+#define SHARE_GC_X_XRESURRECTION_HPP\n+\n+#include \"memory\/allStatic.hpp\"\n+\n+class XResurrection : public AllStatic {\n+private:\n+  static volatile bool _blocked;\n+\n+public:\n+  static bool is_blocked();\n+  static void block();\n+  static void unblock();\n+};\n+\n+#endif \/\/ SHARE_GC_X_XRESURRECTION_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xResurrection.hpp","additions":39,"deletions":0,"binary":false,"changes":39,"status":"added"},{"patch":"@@ -0,0 +1,35 @@\n+\/*\n+ * Copyright (c) 2016, 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XRESURRECTION_INLINE_HPP\n+#define SHARE_GC_X_XRESURRECTION_INLINE_HPP\n+\n+#include \"gc\/x\/xResurrection.hpp\"\n+\n+#include \"runtime\/atomic.hpp\"\n+\n+inline bool XResurrection::is_blocked() {\n+  return Atomic::load(&_blocked);\n+}\n+\n+#endif \/\/ SHARE_GC_X_XRESURRECTION_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xResurrection.inline.hpp","additions":35,"deletions":0,"binary":false,"changes":35,"status":"added"},{"patch":"@@ -0,0 +1,142 @@\n+\/*\n+ * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"classfile\/classLoaderDataGraph.hpp\"\n+#include \"gc\/shared\/oopStorageSetParState.inline.hpp\"\n+#include \"gc\/x\/xNMethod.hpp\"\n+#include \"gc\/x\/xNMethodTable.hpp\"\n+#include \"gc\/x\/xRootsIterator.hpp\"\n+#include \"gc\/x\/xStat.hpp\"\n+#include \"memory\/resourceArea.hpp\"\n+#include \"prims\/jvmtiTagMap.hpp\"\n+#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/globals.hpp\"\n+#include \"runtime\/safepoint.hpp\"\n+#include \"utilities\/debug.hpp\"\n+\n+static const XStatSubPhase XSubPhaseConcurrentRootsOopStorageSet(\"Concurrent Roots OopStorageSet\");\n+static const XStatSubPhase XSubPhaseConcurrentRootsClassLoaderDataGraph(\"Concurrent Roots ClassLoaderDataGraph\");\n+static const XStatSubPhase XSubPhaseConcurrentRootsJavaThreads(\"Concurrent Roots JavaThreads\");\n+static const XStatSubPhase XSubPhaseConcurrentRootsCodeCache(\"Concurrent Roots CodeCache\");\n+static const XStatSubPhase XSubPhaseConcurrentWeakRootsOopStorageSet(\"Concurrent Weak Roots OopStorageSet\");\n+\n+template <typename Iterator>\n+template <typename ClosureType>\n+void XParallelApply<Iterator>::apply(ClosureType* cl) {\n+  if (!Atomic::load(&_completed)) {\n+    _iter.apply(cl);\n+    if (!Atomic::load(&_completed)) {\n+      Atomic::store(&_completed, true);\n+    }\n+  }\n+}\n+\n+XStrongOopStorageSetIterator::XStrongOopStorageSetIterator() :\n+    _iter() {}\n+\n+void XStrongOopStorageSetIterator::apply(OopClosure* cl) {\n+  XStatTimer timer(XSubPhaseConcurrentRootsOopStorageSet);\n+  _iter.oops_do(cl);\n+}\n+\n+void XStrongCLDsIterator::apply(CLDClosure* cl) {\n+  XStatTimer timer(XSubPhaseConcurrentRootsClassLoaderDataGraph);\n+  ClassLoaderDataGraph::always_strong_cld_do(cl);\n+}\n+\n+XJavaThreadsIterator::XJavaThreadsIterator() :\n+    _threads(),\n+    _claimed(0) {}\n+\n+uint XJavaThreadsIterator::claim() {\n+  return Atomic::fetch_and_add(&_claimed, 1u);\n+}\n+\n+void XJavaThreadsIterator::apply(ThreadClosure* cl) {\n+  XStatTimer timer(XSubPhaseConcurrentRootsJavaThreads);\n+\n+  \/\/ The resource mark is needed because interpreter oop maps are\n+  \/\/ not reused in concurrent mode. Instead, they are temporary and\n+  \/\/ resource allocated.\n+  ResourceMark                 _rm;\n+\n+  for (uint i = claim(); i < _threads.length(); i = claim()) {\n+    cl->do_thread(_threads.thread_at(i));\n+  }\n+}\n+\n+XNMethodsIterator::XNMethodsIterator() {\n+  if (!ClassUnloading) {\n+    XNMethod::nmethods_do_begin();\n+  }\n+}\n+\n+XNMethodsIterator::~XNMethodsIterator() {\n+  if (!ClassUnloading) {\n+    XNMethod::nmethods_do_end();\n+  }\n+}\n+\n+void XNMethodsIterator::apply(NMethodClosure* cl) {\n+  XStatTimer timer(XSubPhaseConcurrentRootsCodeCache);\n+  XNMethod::nmethods_do(cl);\n+}\n+\n+XRootsIterator::XRootsIterator(int cld_claim) {\n+  if (cld_claim != ClassLoaderData::_claim_none) {\n+    ClassLoaderDataGraph::verify_claimed_marks_cleared(cld_claim);\n+  }\n+}\n+\n+void XRootsIterator::apply(OopClosure* cl,\n+                           CLDClosure* cld_cl,\n+                           ThreadClosure* thread_cl,\n+                           NMethodClosure* nm_cl) {\n+  _oop_storage_set.apply(cl);\n+  _class_loader_data_graph.apply(cld_cl);\n+  _java_threads.apply(thread_cl);\n+  if (!ClassUnloading) {\n+    _nmethods.apply(nm_cl);\n+  }\n+}\n+\n+XWeakOopStorageSetIterator::XWeakOopStorageSetIterator() :\n+    _iter() {}\n+\n+void XWeakOopStorageSetIterator::apply(OopClosure* cl) {\n+  XStatTimer timer(XSubPhaseConcurrentWeakRootsOopStorageSet);\n+  _iter.oops_do(cl);\n+}\n+\n+void XWeakOopStorageSetIterator::report_num_dead() {\n+  _iter.report_num_dead();\n+}\n+\n+void XWeakRootsIterator::report_num_dead() {\n+  _oop_storage_set.iter().report_num_dead();\n+}\n+\n+void XWeakRootsIterator::apply(OopClosure* cl) {\n+  _oop_storage_set.apply(cl);\n+}\n","filename":"src\/hotspot\/share\/gc\/x\/xRootsIterator.cpp","additions":142,"deletions":0,"binary":false,"changes":142,"status":"added"},{"patch":"@@ -0,0 +1,124 @@\n+\/*\n+ * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XROOTSITERATOR_HPP\n+#define SHARE_GC_X_XROOTSITERATOR_HPP\n+\n+#include \"gc\/shared\/oopStorageSetParState.hpp\"\n+#include \"logging\/log.hpp\"\n+#include \"memory\/iterator.hpp\"\n+#include \"runtime\/threadSMR.hpp\"\n+\n+template <typename Iterator>\n+class XParallelApply {\n+private:\n+  Iterator      _iter;\n+  volatile bool _completed;\n+\n+public:\n+  XParallelApply() :\n+      _iter(),\n+      _completed(false) {}\n+\n+  template <typename ClosureType>\n+  void apply(ClosureType* cl);\n+\n+  Iterator& iter() {\n+    return _iter;\n+  }\n+};\n+\n+class XStrongOopStorageSetIterator {\n+  OopStorageSetStrongParState<true \/* concurrent *\/, false \/* is_const *\/> _iter;\n+\n+public:\n+  XStrongOopStorageSetIterator();\n+\n+  void apply(OopClosure* cl);\n+};\n+\n+class XStrongCLDsIterator {\n+public:\n+  void apply(CLDClosure* cl);\n+};\n+\n+class XJavaThreadsIterator {\n+private:\n+  ThreadsListHandle _threads;\n+  volatile uint     _claimed;\n+\n+  uint claim();\n+\n+public:\n+  XJavaThreadsIterator();\n+\n+  void apply(ThreadClosure* cl);\n+};\n+\n+class XNMethodsIterator {\n+public:\n+  XNMethodsIterator();\n+  ~XNMethodsIterator();\n+\n+  void apply(NMethodClosure* cl);\n+};\n+\n+class XRootsIterator {\n+private:\n+  XParallelApply<XStrongOopStorageSetIterator> _oop_storage_set;\n+  XParallelApply<XStrongCLDsIterator>          _class_loader_data_graph;\n+  XParallelApply<XJavaThreadsIterator>         _java_threads;\n+  XParallelApply<XNMethodsIterator>            _nmethods;\n+\n+public:\n+  XRootsIterator(int cld_claim);\n+\n+  void apply(OopClosure* cl,\n+             CLDClosure* cld_cl,\n+             ThreadClosure* thread_cl,\n+             NMethodClosure* nm_cl);\n+};\n+\n+class XWeakOopStorageSetIterator {\n+private:\n+  OopStorageSetWeakParState<true \/* concurrent *\/, false \/* is_const *\/> _iter;\n+\n+public:\n+  XWeakOopStorageSetIterator();\n+\n+  void apply(OopClosure* cl);\n+\n+  void report_num_dead();\n+};\n+\n+class XWeakRootsIterator {\n+private:\n+  XParallelApply<XWeakOopStorageSetIterator> _oop_storage_set;\n+\n+public:\n+  void apply(OopClosure* cl);\n+\n+  void report_num_dead();\n+};\n+\n+#endif \/\/ SHARE_GC_X_XROOTSITERATOR_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xRootsIterator.hpp","additions":124,"deletions":0,"binary":false,"changes":124,"status":"added"},{"patch":"@@ -0,0 +1,85 @@\n+\/*\n+ * Copyright (c) 2018, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/shared\/gcLogPrecious.hpp\"\n+#include \"gc\/shared\/gc_globals.hpp\"\n+#include \"gc\/x\/xLock.inline.hpp\"\n+#include \"gc\/x\/xRuntimeWorkers.hpp\"\n+#include \"gc\/x\/xTask.hpp\"\n+#include \"gc\/x\/xThread.hpp\"\n+#include \"runtime\/java.hpp\"\n+\n+class XRuntimeWorkersInitializeTask : public WorkerTask {\n+private:\n+  const uint     _nworkers;\n+  uint           _started;\n+  XConditionLock _lock;\n+\n+public:\n+  XRuntimeWorkersInitializeTask(uint nworkers) :\n+      WorkerTask(\"XRuntimeWorkersInitializeTask\"),\n+      _nworkers(nworkers),\n+      _started(0),\n+      _lock() {}\n+\n+  virtual void work(uint worker_id) {\n+    \/\/ Wait for all threads to start\n+    XLocker<XConditionLock> locker(&_lock);\n+    if (++_started == _nworkers) {\n+      \/\/ All threads started\n+      _lock.notify_all();\n+    } else {\n+      while (_started != _nworkers) {\n+        _lock.wait();\n+      }\n+    }\n+  }\n+};\n+\n+XRuntimeWorkers::XRuntimeWorkers() :\n+    _workers(\"RuntimeWorker\",\n+             ParallelGCThreads) {\n+\n+  log_info_p(gc, init)(\"Runtime Workers: %u\", _workers.max_workers());\n+\n+  \/\/ Initialize worker threads\n+  _workers.initialize_workers();\n+  _workers.set_active_workers(_workers.max_workers());\n+  if (_workers.active_workers() != _workers.max_workers()) {\n+    vm_exit_during_initialization(\"Failed to create XRuntimeWorkers\");\n+  }\n+\n+  \/\/ Execute task to reduce latency in early safepoints,\n+  \/\/ which otherwise would have to take on any warmup costs.\n+  XRuntimeWorkersInitializeTask task(_workers.max_workers());\n+  _workers.run_task(&task);\n+}\n+\n+WorkerThreads* XRuntimeWorkers::workers() {\n+  return &_workers;\n+}\n+\n+void XRuntimeWorkers::threads_do(ThreadClosure* tc) const {\n+  _workers.threads_do(tc);\n+}\n","filename":"src\/hotspot\/share\/gc\/x\/xRuntimeWorkers.cpp","additions":85,"deletions":0,"binary":false,"changes":85,"status":"added"},{"patch":"@@ -0,0 +1,43 @@\n+\/*\n+ * Copyright (c) 2018, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XRUNTIMEWORKERS_HPP\n+#define SHARE_GC_X_XRUNTIMEWORKERS_HPP\n+\n+#include \"gc\/shared\/workerThread.hpp\"\n+\n+class ThreadClosure;\n+\n+class XRuntimeWorkers {\n+private:\n+  WorkerThreads _workers;\n+\n+public:\n+  XRuntimeWorkers();\n+\n+  WorkerThreads* workers();\n+\n+  void threads_do(ThreadClosure* tc) const;\n+};\n+\n+#endif \/\/ SHARE_GC_X_XRUNTIMEWORKERS_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xRuntimeWorkers.hpp","additions":43,"deletions":0,"binary":false,"changes":43,"status":"added"},{"patch":"@@ -0,0 +1,68 @@\n+\/*\n+ * Copyright (c) 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XSAFEDELETE_HPP\n+#define SHARE_GC_X_XSAFEDELETE_HPP\n+\n+#include \"gc\/x\/xArray.hpp\"\n+#include \"gc\/x\/xLock.hpp\"\n+\n+#include <type_traits>\n+\n+template <typename T>\n+class XSafeDeleteImpl {\n+private:\n+  using ItemT = std::remove_extent_t<T>;\n+\n+  XLock*         _lock;\n+  uint64_t       _enabled;\n+  XArray<ItemT*> _deferred;\n+\n+  bool deferred_delete(ItemT* item);\n+  void immediate_delete(ItemT* item);\n+\n+public:\n+  XSafeDeleteImpl(XLock* lock);\n+\n+  void enable_deferred_delete();\n+  void disable_deferred_delete();\n+\n+  void operator()(ItemT* item);\n+};\n+\n+template <typename T>\n+class XSafeDelete : public XSafeDeleteImpl<T> {\n+private:\n+  XLock _lock;\n+\n+public:\n+  XSafeDelete();\n+};\n+\n+template <typename T>\n+class XSafeDeleteNoLock : public XSafeDeleteImpl<T> {\n+public:\n+  XSafeDeleteNoLock();\n+};\n+\n+#endif \/\/ SHARE_GC_X_XSAFEDELETE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xSafeDelete.hpp","additions":68,"deletions":0,"binary":false,"changes":68,"status":"added"},{"patch":"@@ -0,0 +1,100 @@\n+\/*\n+ * Copyright (c) 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XSAFEDELETE_INLINE_HPP\n+#define SHARE_GC_X_XSAFEDELETE_INLINE_HPP\n+\n+#include \"gc\/x\/xSafeDelete.hpp\"\n+\n+#include \"gc\/x\/xArray.inline.hpp\"\n+#include \"utilities\/debug.hpp\"\n+\n+#include <type_traits>\n+\n+template <typename T>\n+XSafeDeleteImpl<T>::XSafeDeleteImpl(XLock* lock) :\n+    _lock(lock),\n+    _enabled(0),\n+    _deferred() {}\n+\n+template <typename T>\n+bool XSafeDeleteImpl<T>::deferred_delete(ItemT* item) {\n+  XLocker<XLock> locker(_lock);\n+  if (_enabled > 0) {\n+    _deferred.append(item);\n+    return true;\n+  }\n+\n+  return false;\n+}\n+\n+template <typename T>\n+void XSafeDeleteImpl<T>::immediate_delete(ItemT* item) {\n+  if (std::is_array<T>::value) {\n+    delete [] item;\n+  } else {\n+    delete item;\n+  }\n+}\n+\n+template <typename T>\n+void XSafeDeleteImpl<T>::enable_deferred_delete() {\n+  XLocker<XLock> locker(_lock);\n+  _enabled++;\n+}\n+\n+template <typename T>\n+void XSafeDeleteImpl<T>::disable_deferred_delete() {\n+  XArray<ItemT*> deferred;\n+\n+  {\n+    XLocker<XLock> locker(_lock);\n+    assert(_enabled > 0, \"Invalid state\");\n+    if (--_enabled == 0) {\n+      deferred.swap(&_deferred);\n+    }\n+  }\n+\n+  XArrayIterator<ItemT*> iter(&deferred);\n+  for (ItemT* item; iter.next(&item);) {\n+    immediate_delete(item);\n+  }\n+}\n+\n+template <typename T>\n+void XSafeDeleteImpl<T>::operator()(ItemT* item) {\n+  if (!deferred_delete(item)) {\n+    immediate_delete(item);\n+  }\n+}\n+\n+template <typename T>\n+XSafeDelete<T>::XSafeDelete() :\n+    XSafeDeleteImpl<T>(&_lock),\n+    _lock() {}\n+\n+template <typename T>\n+XSafeDeleteNoLock<T>::XSafeDeleteNoLock() :\n+    XSafeDeleteImpl<T>(NULL) {}\n+\n+#endif \/\/ SHARE_GC_X_XSAFEDELETE_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xSafeDelete.inline.hpp","additions":100,"deletions":0,"binary":false,"changes":100,"status":"added"},{"patch":"@@ -0,0 +1,176 @@\n+\/*\n+ * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/shared\/generationCounters.hpp\"\n+#include \"gc\/shared\/hSpaceCounters.hpp\"\n+#include \"gc\/x\/xCollectedHeap.hpp\"\n+#include \"gc\/x\/xHeap.inline.hpp\"\n+#include \"gc\/x\/xServiceability.hpp\"\n+#include \"memory\/metaspaceCounters.hpp\"\n+#include \"runtime\/perfData.hpp\"\n+\n+class XGenerationCounters : public GenerationCounters {\n+public:\n+  XGenerationCounters(const char* name, int ordinal, int spaces,\n+                      size_t min_capacity, size_t max_capacity, size_t curr_capacity) :\n+      GenerationCounters(name, ordinal, spaces,\n+                         min_capacity, max_capacity, curr_capacity) {}\n+\n+  void update_capacity(size_t capacity) {\n+    _current_size->set_value(capacity);\n+  }\n+};\n+\n+\/\/ Class to expose perf counters used by jstat.\n+class XServiceabilityCounters : public CHeapObj<mtGC> {\n+private:\n+  XGenerationCounters _generation_counters;\n+  HSpaceCounters      _space_counters;\n+  CollectorCounters   _collector_counters;\n+\n+public:\n+  XServiceabilityCounters(size_t min_capacity, size_t max_capacity);\n+\n+  CollectorCounters* collector_counters();\n+\n+  void update_sizes();\n+};\n+\n+XServiceabilityCounters::XServiceabilityCounters(size_t min_capacity, size_t max_capacity) :\n+    \/\/ generation.1\n+    _generation_counters(\"old\"        \/* name *\/,\n+                         1            \/* ordinal *\/,\n+                         1            \/* spaces *\/,\n+                         min_capacity \/* min_capacity *\/,\n+                         max_capacity \/* max_capacity *\/,\n+                         min_capacity \/* curr_capacity *\/),\n+    \/\/ generation.1.space.0\n+    _space_counters(_generation_counters.name_space(),\n+                    \"space\"      \/* name *\/,\n+                    0            \/* ordinal *\/,\n+                    max_capacity \/* max_capacity *\/,\n+                    min_capacity \/* init_capacity *\/),\n+    \/\/ gc.collector.2\n+    _collector_counters(\"Z concurrent cycle pauses\" \/* name *\/,\n+                        2                           \/* ordinal *\/) {}\n+\n+CollectorCounters* XServiceabilityCounters::collector_counters() {\n+  return &_collector_counters;\n+}\n+\n+void XServiceabilityCounters::update_sizes() {\n+  if (UsePerfData) {\n+    const size_t capacity = XHeap::heap()->capacity();\n+    const size_t used = MIN2(XHeap::heap()->used(), capacity);\n+\n+    _generation_counters.update_capacity(capacity);\n+    _space_counters.update_capacity(capacity);\n+    _space_counters.update_used(used);\n+\n+    MetaspaceCounters::update_performance_counters();\n+  }\n+}\n+\n+XServiceabilityMemoryPool::XServiceabilityMemoryPool(size_t min_capacity, size_t max_capacity) :\n+    CollectedMemoryPool(\"ZHeap\",\n+                        min_capacity,\n+                        max_capacity,\n+                        true \/* support_usage_threshold *\/) {}\n+\n+size_t XServiceabilityMemoryPool::used_in_bytes() {\n+  return XHeap::heap()->used();\n+}\n+\n+MemoryUsage XServiceabilityMemoryPool::get_memory_usage() {\n+  const size_t committed = XHeap::heap()->capacity();\n+  const size_t used      = MIN2(XHeap::heap()->used(), committed);\n+\n+  return MemoryUsage(initial_size(), used, committed, max_size());\n+}\n+\n+XServiceabilityMemoryManager::XServiceabilityMemoryManager(const char* name,\n+                                                           const char* end_message,\n+                                                           XServiceabilityMemoryPool* pool) :\n+    GCMemoryManager(name, end_message) {\n+  add_pool(pool);\n+}\n+\n+XServiceability::XServiceability(size_t min_capacity, size_t max_capacity) :\n+    _min_capacity(min_capacity),\n+    _max_capacity(max_capacity),\n+    _memory_pool(_min_capacity, _max_capacity),\n+    _cycle_memory_manager(\"ZGC Cycles\", \"end of GC cycle\", &_memory_pool),\n+    _pause_memory_manager(\"ZGC Pauses\", \"end of GC pause\", &_memory_pool),\n+    _counters(NULL) {}\n+\n+void XServiceability::initialize() {\n+  _counters = new XServiceabilityCounters(_min_capacity, _max_capacity);\n+}\n+\n+MemoryPool* XServiceability::memory_pool() {\n+  return &_memory_pool;\n+}\n+\n+GCMemoryManager* XServiceability::cycle_memory_manager() {\n+  return &_cycle_memory_manager;\n+}\n+\n+GCMemoryManager* XServiceability::pause_memory_manager() {\n+  return &_pause_memory_manager;\n+}\n+\n+XServiceabilityCounters* XServiceability::counters() {\n+  return _counters;\n+}\n+\n+XServiceabilityCycleTracer::XServiceabilityCycleTracer() :\n+    _memory_manager_stats(XHeap::heap()->serviceability_cycle_memory_manager(),\n+                          XCollectedHeap::heap()->gc_cause(),\n+                          true  \/* allMemoryPoolsAffected *\/,\n+                          true  \/* recordGCBeginTime *\/,\n+                          true  \/* recordPreGCUsage *\/,\n+                          true  \/* recordPeakUsage *\/,\n+                          true  \/* recordPostGCUsage *\/,\n+                          true  \/* recordAccumulatedGCTime *\/,\n+                          true  \/* recordGCEndTime *\/,\n+                          true  \/* countCollection *\/) {}\n+\n+XServiceabilityPauseTracer::XServiceabilityPauseTracer() :\n+    _svc_gc_marker(SvcGCMarker::CONCURRENT),\n+    _counters_stats(XHeap::heap()->serviceability_counters()->collector_counters()),\n+    _memory_manager_stats(XHeap::heap()->serviceability_pause_memory_manager(),\n+                          XCollectedHeap::heap()->gc_cause(),\n+                          true  \/* allMemoryPoolsAffected *\/,\n+                          true  \/* recordGCBeginTime *\/,\n+                          false \/* recordPreGCUsage *\/,\n+                          false \/* recordPeakUsage *\/,\n+                          false \/* recordPostGCUsage *\/,\n+                          true  \/* recordAccumulatedGCTime *\/,\n+                          true  \/* recordGCEndTime *\/,\n+                          true  \/* countCollection *\/) {}\n+\n+XServiceabilityPauseTracer::~XServiceabilityPauseTracer()  {\n+  XHeap::heap()->serviceability_counters()->update_sizes();\n+  MemoryService::track_memory_usage();\n+}\n","filename":"src\/hotspot\/share\/gc\/x\/xServiceability.cpp","additions":176,"deletions":0,"binary":false,"changes":176,"status":"added"},{"patch":"@@ -0,0 +1,90 @@\n+\/*\n+ * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XSERVICEABILITY_HPP\n+#define SHARE_GC_X_XSERVICEABILITY_HPP\n+\n+#include \"gc\/shared\/collectorCounters.hpp\"\n+#include \"gc\/shared\/gcVMOperations.hpp\"\n+#include \"memory\/allocation.hpp\"\n+#include \"services\/memoryManager.hpp\"\n+#include \"services\/memoryPool.hpp\"\n+#include \"services\/memoryService.hpp\"\n+\n+class XServiceabilityCounters;\n+\n+class XServiceabilityMemoryPool : public CollectedMemoryPool {\n+public:\n+  XServiceabilityMemoryPool(size_t min_capacity, size_t max_capacity);\n+\n+  virtual size_t used_in_bytes();\n+  virtual MemoryUsage get_memory_usage();\n+};\n+\n+class XServiceabilityMemoryManager : public GCMemoryManager {\n+public:\n+  XServiceabilityMemoryManager(const char* name,\n+                               const char* end_message,\n+                               XServiceabilityMemoryPool* pool);\n+};\n+\n+class XServiceability {\n+private:\n+  const size_t                 _min_capacity;\n+  const size_t                 _max_capacity;\n+  XServiceabilityMemoryPool    _memory_pool;\n+  XServiceabilityMemoryManager _cycle_memory_manager;\n+  XServiceabilityMemoryManager _pause_memory_manager;\n+  XServiceabilityCounters*     _counters;\n+\n+public:\n+  XServiceability(size_t min_capacity, size_t max_capacity);\n+\n+  void initialize();\n+\n+  MemoryPool* memory_pool();\n+  GCMemoryManager* cycle_memory_manager();\n+  GCMemoryManager* pause_memory_manager();\n+  XServiceabilityCounters* counters();\n+};\n+\n+class XServiceabilityCycleTracer : public StackObj {\n+private:\n+  TraceMemoryManagerStats _memory_manager_stats;\n+\n+public:\n+  XServiceabilityCycleTracer();\n+};\n+\n+class XServiceabilityPauseTracer : public StackObj {\n+private:\n+  SvcGCMarker             _svc_gc_marker;\n+  TraceCollectorStats     _counters_stats;\n+  TraceMemoryManagerStats _memory_manager_stats;\n+\n+public:\n+  XServiceabilityPauseTracer();\n+  ~XServiceabilityPauseTracer();\n+};\n+\n+#endif \/\/ SHARE_GC_X_XSERVICEABILITY_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xServiceability.hpp","additions":90,"deletions":0,"binary":false,"changes":90,"status":"added"},{"patch":"@@ -0,0 +1,99 @@\n+\/*\n+ * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/x\/xAddress.hpp\"\n+#include \"gc\/x\/xBarrier.inline.hpp\"\n+#include \"gc\/x\/xStackWatermark.hpp\"\n+#include \"gc\/x\/xThread.inline.hpp\"\n+#include \"gc\/x\/xThreadLocalAllocBuffer.hpp\"\n+#include \"gc\/x\/xThreadLocalData.hpp\"\n+#include \"gc\/x\/xVerify.hpp\"\n+#include \"memory\/resourceArea.inline.hpp\"\n+#include \"runtime\/frame.inline.hpp\"\n+#include \"utilities\/preserveException.hpp\"\n+\n+XOnStackCodeBlobClosure::XOnStackCodeBlobClosure() :\n+    _bs_nm(BarrierSet::barrier_set()->barrier_set_nmethod()) {}\n+\n+void XOnStackCodeBlobClosure::do_code_blob(CodeBlob* cb) {\n+  nmethod* const nm = cb->as_nmethod_or_null();\n+  if (nm != NULL) {\n+    const bool result = _bs_nm->nmethod_entry_barrier(nm);\n+    assert(result, \"NMethod on-stack must be alive\");\n+  }\n+}\n+\n+ThreadLocalAllocStats& XStackWatermark::stats() {\n+  return _stats;\n+}\n+\n+uint32_t XStackWatermark::epoch_id() const {\n+  return *XAddressBadMaskHighOrderBitsAddr;\n+}\n+\n+XStackWatermark::XStackWatermark(JavaThread* jt) :\n+    StackWatermark(jt, StackWatermarkKind::gc, *XAddressBadMaskHighOrderBitsAddr),\n+    _jt_cl(),\n+    _cb_cl(),\n+    _stats() {}\n+\n+OopClosure* XStackWatermark::closure_from_context(void* context) {\n+  if (context != NULL) {\n+    assert(XThread::is_worker(), \"Unexpected thread passing in context: \" PTR_FORMAT, p2i(context));\n+    return reinterpret_cast<OopClosure*>(context);\n+  } else {\n+    return &_jt_cl;\n+  }\n+}\n+\n+void XStackWatermark::start_processing_impl(void* context) {\n+  \/\/ Verify the head (no_frames) of the thread is bad before fixing it.\n+  XVerify::verify_thread_head_bad(_jt);\n+\n+  \/\/ Process the non-frame part of the thread\n+  _jt->oops_do_no_frames(closure_from_context(context), &_cb_cl);\n+  XThreadLocalData::do_invisible_root(_jt, XBarrier::load_barrier_on_invisible_root_oop_field);\n+\n+  \/\/ Verification of frames is done after processing of the \"head\" (no_frames).\n+  \/\/ The reason is that the exception oop is fiddled with during frame processing.\n+  XVerify::verify_thread_frames_bad(_jt);\n+\n+  \/\/ Update thread local address bad mask\n+  XThreadLocalData::set_address_bad_mask(_jt, XAddressBadMask);\n+\n+  \/\/ Retire TLAB\n+  if (XGlobalPhase == XPhaseMark) {\n+    XThreadLocalAllocBuffer::retire(_jt, &_stats);\n+  } else {\n+    XThreadLocalAllocBuffer::remap(_jt);\n+  }\n+\n+  \/\/ Publishes the processing start to concurrent threads\n+  StackWatermark::start_processing_impl(context);\n+}\n+\n+void XStackWatermark::process(const frame& fr, RegisterMap& register_map, void* context) {\n+  XVerify::verify_frame_bad(fr, register_map);\n+  fr.oops_do(closure_from_context(context), &_cb_cl, &register_map, DerivedPointerIterationMode::_directly);\n+}\n","filename":"src\/hotspot\/share\/gc\/x\/xStackWatermark.cpp","additions":99,"deletions":0,"binary":false,"changes":99,"status":"added"},{"patch":"@@ -0,0 +1,68 @@\n+\/*\n+ * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XSTACKWATERMARK_HPP\n+#define SHARE_GC_X_XSTACKWATERMARK_HPP\n+\n+#include \"gc\/shared\/barrierSet.hpp\"\n+#include \"gc\/shared\/barrierSetNMethod.hpp\"\n+#include \"gc\/shared\/threadLocalAllocBuffer.hpp\"\n+#include \"gc\/x\/xBarrier.hpp\"\n+#include \"memory\/allocation.hpp\"\n+#include \"memory\/iterator.hpp\"\n+#include \"oops\/oopsHierarchy.hpp\"\n+#include \"runtime\/stackWatermark.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+class frame;\n+class JavaThread;\n+\n+class XOnStackCodeBlobClosure : public CodeBlobClosure {\n+private:\n+  BarrierSetNMethod* _bs_nm;\n+\n+  virtual void do_code_blob(CodeBlob* cb);\n+\n+public:\n+  XOnStackCodeBlobClosure();\n+};\n+\n+class XStackWatermark : public StackWatermark {\n+private:\n+  XLoadBarrierOopClosure  _jt_cl;\n+  XOnStackCodeBlobClosure _cb_cl;\n+  ThreadLocalAllocStats   _stats;\n+\n+  OopClosure* closure_from_context(void* context);\n+\n+  virtual uint32_t epoch_id() const;\n+  virtual void start_processing_impl(void* context);\n+  virtual void process(const frame& fr, RegisterMap& register_map, void* context);\n+\n+public:\n+  XStackWatermark(JavaThread* jt);\n+\n+  ThreadLocalAllocStats& stats();\n+};\n+\n+#endif \/\/ SHARE_GC_X_XSTACKWATERMARK_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xStackWatermark.hpp","additions":68,"deletions":0,"binary":false,"changes":68,"status":"added"},{"patch":"@@ -0,0 +1,1513 @@\n+\/*\n+ * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/shared\/gc_globals.hpp\"\n+#include \"gc\/x\/xAbort.inline.hpp\"\n+#include \"gc\/x\/xCollectedHeap.hpp\"\n+#include \"gc\/x\/xCPU.inline.hpp\"\n+#include \"gc\/x\/xGlobals.hpp\"\n+#include \"gc\/x\/xNMethodTable.hpp\"\n+#include \"gc\/x\/xPageAllocator.inline.hpp\"\n+#include \"gc\/x\/xRelocationSetSelector.inline.hpp\"\n+#include \"gc\/x\/xStat.hpp\"\n+#include \"gc\/x\/xThread.inline.hpp\"\n+#include \"gc\/x\/xTracer.inline.hpp\"\n+#include \"gc\/x\/xUtils.hpp\"\n+#include \"memory\/metaspaceUtils.hpp\"\n+#include \"memory\/resourceArea.hpp\"\n+#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/os.hpp\"\n+#include \"runtime\/timer.hpp\"\n+#include \"utilities\/align.hpp\"\n+#include \"utilities\/debug.hpp\"\n+#include \"utilities\/ticks.hpp\"\n+\n+#define XSIZE_FMT                       SIZE_FORMAT \"M(%.0f%%)\"\n+#define XSIZE_ARGS_WITH_MAX(size, max)  ((size) \/ M), (percent_of(size, max))\n+#define XSIZE_ARGS(size)                XSIZE_ARGS_WITH_MAX(size, XStatHeap::max_capacity())\n+\n+#define XTABLE_ARGS_NA                  \"%9s\", \"-\"\n+#define XTABLE_ARGS(size)               SIZE_FORMAT_W(8) \"M (%.0f%%)\", \\\n+                                        ((size) \/ M), (percent_of(size, XStatHeap::max_capacity()))\n+\n+\/\/\n+\/\/ Stat sampler\/counter data\n+\/\/\n+struct XStatSamplerData {\n+  uint64_t _nsamples;\n+  uint64_t _sum;\n+  uint64_t _max;\n+\n+  XStatSamplerData() :\n+    _nsamples(0),\n+    _sum(0),\n+    _max(0) {}\n+\n+  void add(const XStatSamplerData& new_sample) {\n+    _nsamples += new_sample._nsamples;\n+    _sum += new_sample._sum;\n+    _max = MAX2(_max, new_sample._max);\n+  }\n+};\n+\n+struct XStatCounterData {\n+  uint64_t _counter;\n+\n+  XStatCounterData() :\n+    _counter(0) {}\n+};\n+\n+\/\/\n+\/\/ Stat sampler history\n+\/\/\n+template <size_t size>\n+class XStatSamplerHistoryInterval {\n+private:\n+  size_t           _next;\n+  XStatSamplerData _samples[size];\n+  XStatSamplerData _accumulated;\n+  XStatSamplerData _total;\n+\n+public:\n+  XStatSamplerHistoryInterval() :\n+      _next(0),\n+      _samples(),\n+      _accumulated(),\n+      _total() {}\n+\n+  bool add(const XStatSamplerData& new_sample) {\n+    \/\/ Insert sample\n+    const XStatSamplerData old_sample = _samples[_next];\n+    _samples[_next] = new_sample;\n+\n+    \/\/ Adjust accumulated\n+    _accumulated._nsamples += new_sample._nsamples;\n+    _accumulated._sum += new_sample._sum;\n+    _accumulated._max = MAX2(_accumulated._max, new_sample._max);\n+\n+    \/\/ Adjust total\n+    _total._nsamples -= old_sample._nsamples;\n+    _total._sum -= old_sample._sum;\n+    _total._nsamples += new_sample._nsamples;\n+    _total._sum += new_sample._sum;\n+    if (_total._max < new_sample._max) {\n+      \/\/ Found new max\n+      _total._max = new_sample._max;\n+    } else if (_total._max == old_sample._max) {\n+      \/\/ Removed old max, reset and find new max\n+      _total._max = 0;\n+      for (size_t i = 0; i < size; i++) {\n+        if (_total._max < _samples[i]._max) {\n+          _total._max = _samples[i]._max;\n+        }\n+      }\n+    }\n+\n+    \/\/ Adjust next\n+    if (++_next == size) {\n+      _next = 0;\n+\n+      \/\/ Clear accumulated\n+      const XStatSamplerData zero;\n+      _accumulated = zero;\n+\n+      \/\/ Became full\n+      return true;\n+    }\n+\n+    \/\/ Not yet full\n+    return false;\n+  }\n+\n+  const XStatSamplerData& total() const {\n+    return _total;\n+  }\n+\n+  const XStatSamplerData& accumulated() const {\n+    return _accumulated;\n+  }\n+};\n+\n+class XStatSamplerHistory : public CHeapObj<mtGC> {\n+private:\n+  XStatSamplerHistoryInterval<10> _10seconds;\n+  XStatSamplerHistoryInterval<60> _10minutes;\n+  XStatSamplerHistoryInterval<60> _10hours;\n+  XStatSamplerData                _total;\n+\n+  uint64_t avg(uint64_t sum, uint64_t nsamples) const {\n+    return (nsamples > 0) ? sum \/ nsamples : 0;\n+  }\n+\n+public:\n+  XStatSamplerHistory() :\n+      _10seconds(),\n+      _10minutes(),\n+      _10hours(),\n+      _total() {}\n+\n+  void add(const XStatSamplerData& new_sample) {\n+    if (_10seconds.add(new_sample)) {\n+      if (_10minutes.add(_10seconds.total())) {\n+        if (_10hours.add(_10minutes.total())) {\n+          _total.add(_10hours.total());\n+        }\n+      }\n+    }\n+  }\n+\n+  uint64_t avg_10_seconds() const {\n+    const uint64_t sum      = _10seconds.total()._sum;\n+    const uint64_t nsamples = _10seconds.total()._nsamples;\n+    return avg(sum, nsamples);\n+  }\n+\n+  uint64_t avg_10_minutes() const {\n+    const uint64_t sum      = _10seconds.accumulated()._sum +\n+                              _10minutes.total()._sum;\n+    const uint64_t nsamples = _10seconds.accumulated()._nsamples +\n+                              _10minutes.total()._nsamples;\n+    return avg(sum, nsamples);\n+  }\n+\n+  uint64_t avg_10_hours() const {\n+    const uint64_t sum      = _10seconds.accumulated()._sum +\n+                              _10minutes.accumulated()._sum +\n+                              _10hours.total()._sum;\n+    const uint64_t nsamples = _10seconds.accumulated()._nsamples +\n+                              _10minutes.accumulated()._nsamples +\n+                              _10hours.total()._nsamples;\n+    return avg(sum, nsamples);\n+  }\n+\n+  uint64_t avg_total() const {\n+    const uint64_t sum      = _10seconds.accumulated()._sum +\n+                              _10minutes.accumulated()._sum +\n+                              _10hours.accumulated()._sum +\n+                              _total._sum;\n+    const uint64_t nsamples = _10seconds.accumulated()._nsamples +\n+                              _10minutes.accumulated()._nsamples +\n+                              _10hours.accumulated()._nsamples +\n+                              _total._nsamples;\n+    return avg(sum, nsamples);\n+  }\n+\n+  uint64_t max_10_seconds() const {\n+    return _10seconds.total()._max;\n+  }\n+\n+  uint64_t max_10_minutes() const {\n+    return MAX2(_10seconds.accumulated()._max,\n+                _10minutes.total()._max);\n+  }\n+\n+  uint64_t max_10_hours() const {\n+    return MAX3(_10seconds.accumulated()._max,\n+                _10minutes.accumulated()._max,\n+                _10hours.total()._max);\n+  }\n+\n+  uint64_t max_total() const {\n+    return MAX4(_10seconds.accumulated()._max,\n+                _10minutes.accumulated()._max,\n+                _10hours.accumulated()._max,\n+                _total._max);\n+  }\n+};\n+\n+\/\/\n+\/\/ Stat unit printers\n+\/\/\n+void XStatUnitTime(LogTargetHandle log, const XStatSampler& sampler, const XStatSamplerHistory& history) {\n+  log.print(\" %10s: %-41s \"\n+            \"%9.3f \/ %-9.3f \"\n+            \"%9.3f \/ %-9.3f \"\n+            \"%9.3f \/ %-9.3f \"\n+            \"%9.3f \/ %-9.3f   ms\",\n+            sampler.group(),\n+            sampler.name(),\n+            TimeHelper::counter_to_millis(history.avg_10_seconds()),\n+            TimeHelper::counter_to_millis(history.max_10_seconds()),\n+            TimeHelper::counter_to_millis(history.avg_10_minutes()),\n+            TimeHelper::counter_to_millis(history.max_10_minutes()),\n+            TimeHelper::counter_to_millis(history.avg_10_hours()),\n+            TimeHelper::counter_to_millis(history.max_10_hours()),\n+            TimeHelper::counter_to_millis(history.avg_total()),\n+            TimeHelper::counter_to_millis(history.max_total()));\n+}\n+\n+void XStatUnitBytes(LogTargetHandle log, const XStatSampler& sampler, const XStatSamplerHistory& history) {\n+  log.print(\" %10s: %-41s \"\n+            UINT64_FORMAT_W(9) \" \/ \" UINT64_FORMAT_W(-9) \" \"\n+            UINT64_FORMAT_W(9) \" \/ \" UINT64_FORMAT_W(-9) \" \"\n+            UINT64_FORMAT_W(9) \" \/ \" UINT64_FORMAT_W(-9) \" \"\n+            UINT64_FORMAT_W(9) \" \/ \" UINT64_FORMAT_W(-9) \"   MB\",\n+            sampler.group(),\n+            sampler.name(),\n+            history.avg_10_seconds() \/ M,\n+            history.max_10_seconds() \/ M,\n+            history.avg_10_minutes() \/ M,\n+            history.max_10_minutes() \/ M,\n+            history.avg_10_hours() \/ M,\n+            history.max_10_hours() \/ M,\n+            history.avg_total() \/ M,\n+            history.max_total() \/ M);\n+}\n+\n+void XStatUnitThreads(LogTargetHandle log, const XStatSampler& sampler, const XStatSamplerHistory& history) {\n+  log.print(\" %10s: %-41s \"\n+            UINT64_FORMAT_W(9) \" \/ \" UINT64_FORMAT_W(-9) \" \"\n+            UINT64_FORMAT_W(9) \" \/ \" UINT64_FORMAT_W(-9) \" \"\n+            UINT64_FORMAT_W(9) \" \/ \" UINT64_FORMAT_W(-9) \" \"\n+            UINT64_FORMAT_W(9) \" \/ \" UINT64_FORMAT_W(-9) \"   threads\",\n+            sampler.group(),\n+            sampler.name(),\n+            history.avg_10_seconds(),\n+            history.max_10_seconds(),\n+            history.avg_10_minutes(),\n+            history.max_10_minutes(),\n+            history.avg_10_hours(),\n+            history.max_10_hours(),\n+            history.avg_total(),\n+            history.max_total());\n+}\n+\n+void XStatUnitBytesPerSecond(LogTargetHandle log, const XStatSampler& sampler, const XStatSamplerHistory& history) {\n+  log.print(\" %10s: %-41s \"\n+            UINT64_FORMAT_W(9) \" \/ \" UINT64_FORMAT_W(-9) \" \"\n+            UINT64_FORMAT_W(9) \" \/ \" UINT64_FORMAT_W(-9) \" \"\n+            UINT64_FORMAT_W(9) \" \/ \" UINT64_FORMAT_W(-9) \" \"\n+            UINT64_FORMAT_W(9) \" \/ \" UINT64_FORMAT_W(-9) \"   MB\/s\",\n+            sampler.group(),\n+            sampler.name(),\n+            history.avg_10_seconds() \/ M,\n+            history.max_10_seconds() \/ M,\n+            history.avg_10_minutes() \/ M,\n+            history.max_10_minutes() \/ M,\n+            history.avg_10_hours() \/ M,\n+            history.max_10_hours() \/ M,\n+            history.avg_total() \/ M,\n+            history.max_total() \/ M);\n+}\n+\n+void XStatUnitOpsPerSecond(LogTargetHandle log, const XStatSampler& sampler, const XStatSamplerHistory& history) {\n+  log.print(\" %10s: %-41s \"\n+            UINT64_FORMAT_W(9) \" \/ \" UINT64_FORMAT_W(-9) \" \"\n+            UINT64_FORMAT_W(9) \" \/ \" UINT64_FORMAT_W(-9) \" \"\n+            UINT64_FORMAT_W(9) \" \/ \" UINT64_FORMAT_W(-9) \" \"\n+            UINT64_FORMAT_W(9) \" \/ \" UINT64_FORMAT_W(-9) \"   ops\/s\",\n+            sampler.group(),\n+            sampler.name(),\n+            history.avg_10_seconds(),\n+            history.max_10_seconds(),\n+            history.avg_10_minutes(),\n+            history.max_10_minutes(),\n+            history.avg_10_hours(),\n+            history.max_10_hours(),\n+            history.avg_total(),\n+            history.max_total());\n+}\n+\n+\/\/\n+\/\/ Stat value\n+\/\/\n+uintptr_t XStatValue::_base = 0;\n+uint32_t  XStatValue::_cpu_offset = 0;\n+\n+XStatValue::XStatValue(const char* group,\n+                          const char* name,\n+                          uint32_t id,\n+                          uint32_t size) :\n+    _group(group),\n+    _name(name),\n+    _id(id),\n+    _offset(_cpu_offset) {\n+  assert(_base == 0, \"Already initialized\");\n+  _cpu_offset += size;\n+}\n+\n+template <typename T>\n+T* XStatValue::get_cpu_local(uint32_t cpu) const {\n+  assert(_base != 0, \"Not initialized\");\n+  const uintptr_t cpu_base = _base + (_cpu_offset * cpu);\n+  const uintptr_t value_addr = cpu_base + _offset;\n+  return (T*)value_addr;\n+}\n+\n+void XStatValue::initialize() {\n+  \/\/ Finalize and align CPU offset\n+  _cpu_offset = align_up(_cpu_offset, (uint32_t)XCacheLineSize);\n+\n+  \/\/ Allocation aligned memory\n+  const size_t size = _cpu_offset * XCPU::count();\n+  _base = XUtils::alloc_aligned(XCacheLineSize, size);\n+}\n+\n+const char* XStatValue::group() const {\n+  return _group;\n+}\n+\n+const char* XStatValue::name() const {\n+  return _name;\n+}\n+\n+uint32_t XStatValue::id() const {\n+  return _id;\n+}\n+\n+\/\/\n+\/\/ Stat iterable value\n+\/\/\n+\n+template <typename T>\n+XStatIterableValue<T>::XStatIterableValue(const char* group,\n+                                          const char* name,\n+                                          uint32_t size) :\n+    XStatValue(group, name, _count++, size),\n+    _next(insert()) {}\n+\n+template <typename T>\n+T* XStatIterableValue<T>::insert() const {\n+  T* const next = _first;\n+  _first = (T*)this;\n+  return next;\n+}\n+\n+template <typename T>\n+void XStatIterableValue<T>::sort() {\n+  T* first_unsorted = _first;\n+  _first = NULL;\n+\n+  while (first_unsorted != NULL) {\n+    T* const value = first_unsorted;\n+    first_unsorted = value->_next;\n+    value->_next = NULL;\n+\n+    T** current = &_first;\n+\n+    while (*current != NULL) {\n+      \/\/ First sort by group, then by name\n+      const int group_cmp = strcmp((*current)->group(), value->group());\n+      if ((group_cmp > 0) || (group_cmp == 0 && strcmp((*current)->name(), value->name()) > 0)) {\n+        break;\n+      }\n+\n+      current = &(*current)->_next;\n+    }\n+    value->_next = *current;\n+    *current = value;\n+  }\n+}\n+\n+\/\/\n+\/\/ Stat sampler\n+\/\/\n+XStatSampler::XStatSampler(const char* group, const char* name, XStatUnitPrinter printer) :\n+    XStatIterableValue<XStatSampler>(group, name, sizeof(XStatSamplerData)),\n+    _printer(printer) {}\n+\n+XStatSamplerData* XStatSampler::get() const {\n+  return get_cpu_local<XStatSamplerData>(XCPU::id());\n+}\n+\n+XStatSamplerData XStatSampler::collect_and_reset() const {\n+  XStatSamplerData all;\n+\n+  const uint32_t ncpus = XCPU::count();\n+  for (uint32_t i = 0; i < ncpus; i++) {\n+    XStatSamplerData* const cpu_data = get_cpu_local<XStatSamplerData>(i);\n+    if (cpu_data->_nsamples > 0) {\n+      const uint64_t nsamples = Atomic::xchg(&cpu_data->_nsamples, (uint64_t)0);\n+      const uint64_t sum = Atomic::xchg(&cpu_data->_sum, (uint64_t)0);\n+      const uint64_t max = Atomic::xchg(&cpu_data->_max, (uint64_t)0);\n+      all._nsamples += nsamples;\n+      all._sum += sum;\n+      if (all._max < max) {\n+        all._max = max;\n+      }\n+    }\n+  }\n+\n+  return all;\n+}\n+\n+XStatUnitPrinter XStatSampler::printer() const {\n+  return _printer;\n+}\n+\n+\/\/\n+\/\/ Stat counter\n+\/\/\n+XStatCounter::XStatCounter(const char* group, const char* name, XStatUnitPrinter printer) :\n+    XStatIterableValue<XStatCounter>(group, name, sizeof(XStatCounterData)),\n+    _sampler(group, name, printer) {}\n+\n+XStatCounterData* XStatCounter::get() const {\n+  return get_cpu_local<XStatCounterData>(XCPU::id());\n+}\n+\n+void XStatCounter::sample_and_reset() const {\n+  uint64_t counter = 0;\n+\n+  const uint32_t ncpus = XCPU::count();\n+  for (uint32_t i = 0; i < ncpus; i++) {\n+    XStatCounterData* const cpu_data = get_cpu_local<XStatCounterData>(i);\n+    counter += Atomic::xchg(&cpu_data->_counter, (uint64_t)0);\n+  }\n+\n+  XStatSample(_sampler, counter);\n+}\n+\n+\/\/\n+\/\/ Stat unsampled counter\n+\/\/\n+XStatUnsampledCounter::XStatUnsampledCounter(const char* name) :\n+    XStatIterableValue<XStatUnsampledCounter>(\"Unsampled\", name, sizeof(XStatCounterData)) {}\n+\n+XStatCounterData* XStatUnsampledCounter::get() const {\n+  return get_cpu_local<XStatCounterData>(XCPU::id());\n+}\n+\n+XStatCounterData XStatUnsampledCounter::collect_and_reset() const {\n+  XStatCounterData all;\n+\n+  const uint32_t ncpus = XCPU::count();\n+  for (uint32_t i = 0; i < ncpus; i++) {\n+    XStatCounterData* const cpu_data = get_cpu_local<XStatCounterData>(i);\n+    all._counter += Atomic::xchg(&cpu_data->_counter, (uint64_t)0);\n+  }\n+\n+  return all;\n+}\n+\n+\/\/\n+\/\/ Stat MMU (Minimum Mutator Utilization)\n+\/\/\n+XStatMMUPause::XStatMMUPause() :\n+    _start(0.0),\n+    _end(0.0) {}\n+\n+XStatMMUPause::XStatMMUPause(const Ticks& start, const Ticks& end) :\n+    _start(TimeHelper::counter_to_millis(start.value())),\n+    _end(TimeHelper::counter_to_millis(end.value())) {}\n+\n+double XStatMMUPause::end() const {\n+  return _end;\n+}\n+\n+double XStatMMUPause::overlap(double start, double end) const {\n+  const double start_max = MAX2(start, _start);\n+  const double end_min = MIN2(end, _end);\n+\n+  if (end_min > start_max) {\n+    \/\/ Overlap found\n+    return end_min - start_max;\n+  }\n+\n+  \/\/ No overlap\n+  return 0.0;\n+}\n+\n+size_t XStatMMU::_next = 0;\n+size_t XStatMMU::_npauses = 0;\n+XStatMMUPause XStatMMU::_pauses[200];\n+double XStatMMU::_mmu_2ms = 100.0;\n+double XStatMMU::_mmu_5ms = 100.0;\n+double XStatMMU::_mmu_10ms = 100.0;\n+double XStatMMU::_mmu_20ms = 100.0;\n+double XStatMMU::_mmu_50ms = 100.0;\n+double XStatMMU::_mmu_100ms = 100.0;\n+\n+const XStatMMUPause& XStatMMU::pause(size_t index) {\n+  return _pauses[(_next - index - 1) % ARRAY_SIZE(_pauses)];\n+}\n+\n+double XStatMMU::calculate_mmu(double time_slice) {\n+  const double end = pause(0).end();\n+  const double start = end - time_slice;\n+  double time_paused = 0.0;\n+\n+  \/\/ Find all overlapping pauses\n+  for (size_t i = 0; i < _npauses; i++) {\n+    const double overlap = pause(i).overlap(start, end);\n+    if (overlap == 0.0) {\n+      \/\/ No overlap\n+      break;\n+    }\n+\n+    time_paused += overlap;\n+  }\n+\n+  \/\/ Calculate MMU\n+  const double time_mutator = time_slice - time_paused;\n+  return percent_of(time_mutator, time_slice);\n+}\n+\n+void XStatMMU::register_pause(const Ticks& start, const Ticks& end) {\n+  \/\/ Add pause\n+  const size_t index = _next++ % ARRAY_SIZE(_pauses);\n+  _pauses[index] = XStatMMUPause(start, end);\n+  _npauses = MIN2(_npauses + 1, ARRAY_SIZE(_pauses));\n+\n+  \/\/ Recalculate MMUs\n+  _mmu_2ms    = MIN2(_mmu_2ms,   calculate_mmu(2));\n+  _mmu_5ms    = MIN2(_mmu_5ms,   calculate_mmu(5));\n+  _mmu_10ms   = MIN2(_mmu_10ms,  calculate_mmu(10));\n+  _mmu_20ms   = MIN2(_mmu_20ms,  calculate_mmu(20));\n+  _mmu_50ms   = MIN2(_mmu_50ms,  calculate_mmu(50));\n+  _mmu_100ms  = MIN2(_mmu_100ms, calculate_mmu(100));\n+}\n+\n+void XStatMMU::print() {\n+  log_info(gc, mmu)(\"MMU: 2ms\/%.1f%%, 5ms\/%.1f%%, 10ms\/%.1f%%, 20ms\/%.1f%%, 50ms\/%.1f%%, 100ms\/%.1f%%\",\n+                    _mmu_2ms, _mmu_5ms, _mmu_10ms, _mmu_20ms, _mmu_50ms, _mmu_100ms);\n+}\n+\n+\/\/\n+\/\/ Stat phases\n+\/\/\n+ConcurrentGCTimer XStatPhase::_timer;\n+\n+XStatPhase::XStatPhase(const char* group, const char* name) :\n+    _sampler(group, name, XStatUnitTime) {}\n+\n+void XStatPhase::log_start(LogTargetHandle log, bool thread) const {\n+  if (!log.is_enabled()) {\n+    return;\n+  }\n+\n+  if (thread) {\n+    ResourceMark rm;\n+    log.print(\"%s (%s)\", name(), Thread::current()->name());\n+  } else {\n+    log.print(\"%s\", name());\n+  }\n+}\n+\n+void XStatPhase::log_end(LogTargetHandle log, const Tickspan& duration, bool thread) const {\n+  if (!log.is_enabled()) {\n+    return;\n+  }\n+\n+  if (thread) {\n+    ResourceMark rm;\n+    log.print(\"%s (%s) %.3fms\", name(), Thread::current()->name(), TimeHelper::counter_to_millis(duration.value()));\n+  } else {\n+    log.print(\"%s %.3fms\", name(), TimeHelper::counter_to_millis(duration.value()));\n+  }\n+}\n+\n+ConcurrentGCTimer* XStatPhase::timer() {\n+  return &_timer;\n+}\n+\n+const char* XStatPhase::name() const {\n+  return _sampler.name();\n+}\n+\n+XStatPhaseCycle::XStatPhaseCycle(const char* name) :\n+    XStatPhase(\"Collector\", name) {}\n+\n+void XStatPhaseCycle::register_start(const Ticks& start) const {\n+  timer()->register_gc_start(start);\n+\n+  XTracer::tracer()->report_gc_start(XCollectedHeap::heap()->gc_cause(), start);\n+\n+  XCollectedHeap::heap()->print_heap_before_gc();\n+  XCollectedHeap::heap()->trace_heap_before_gc(XTracer::tracer());\n+\n+  log_info(gc, start)(\"Garbage Collection (%s)\",\n+                       GCCause::to_string(XCollectedHeap::heap()->gc_cause()));\n+}\n+\n+void XStatPhaseCycle::register_end(const Ticks& start, const Ticks& end) const {\n+  if (XAbort::should_abort()) {\n+    log_info(gc)(\"Garbage Collection (%s) Aborted\",\n+                 GCCause::to_string(XCollectedHeap::heap()->gc_cause()));\n+    return;\n+  }\n+\n+  timer()->register_gc_end(end);\n+\n+  XCollectedHeap::heap()->print_heap_after_gc();\n+  XCollectedHeap::heap()->trace_heap_after_gc(XTracer::tracer());\n+\n+  XTracer::tracer()->report_gc_end(end, timer()->time_partitions());\n+\n+  const Tickspan duration = end - start;\n+  XStatSample(_sampler, duration.value());\n+\n+  XStatLoad::print();\n+  XStatMMU::print();\n+  XStatMark::print();\n+  XStatNMethods::print();\n+  XStatMetaspace::print();\n+  XStatReferences::print();\n+  XStatRelocation::print();\n+  XStatHeap::print();\n+\n+  log_info(gc)(\"Garbage Collection (%s) \" XSIZE_FMT \"->\" XSIZE_FMT,\n+               GCCause::to_string(XCollectedHeap::heap()->gc_cause()),\n+               XSIZE_ARGS(XStatHeap::used_at_mark_start()),\n+               XSIZE_ARGS(XStatHeap::used_at_relocate_end()));\n+}\n+\n+Tickspan XStatPhasePause::_max;\n+\n+XStatPhasePause::XStatPhasePause(const char* name) :\n+    XStatPhase(\"Phase\", name) {}\n+\n+const Tickspan& XStatPhasePause::max() {\n+  return _max;\n+}\n+\n+void XStatPhasePause::register_start(const Ticks& start) const {\n+  timer()->register_gc_pause_start(name(), start);\n+\n+  LogTarget(Debug, gc, phases, start) log;\n+  log_start(log);\n+}\n+\n+void XStatPhasePause::register_end(const Ticks& start, const Ticks& end) const {\n+  timer()->register_gc_pause_end(end);\n+\n+  const Tickspan duration = end - start;\n+  XStatSample(_sampler, duration.value());\n+\n+  \/\/ Track max pause time\n+  if (_max < duration) {\n+    _max = duration;\n+  }\n+\n+  \/\/ Track minimum mutator utilization\n+  XStatMMU::register_pause(start, end);\n+\n+  LogTarget(Info, gc, phases) log;\n+  log_end(log, duration);\n+}\n+\n+XStatPhaseConcurrent::XStatPhaseConcurrent(const char* name) :\n+    XStatPhase(\"Phase\", name) {}\n+\n+void XStatPhaseConcurrent::register_start(const Ticks& start) const {\n+  timer()->register_gc_concurrent_start(name(), start);\n+\n+  LogTarget(Debug, gc, phases, start) log;\n+  log_start(log);\n+}\n+\n+void XStatPhaseConcurrent::register_end(const Ticks& start, const Ticks& end) const {\n+  if (XAbort::should_abort()) {\n+    return;\n+  }\n+\n+  timer()->register_gc_concurrent_end(end);\n+\n+  const Tickspan duration = end - start;\n+  XStatSample(_sampler, duration.value());\n+\n+  LogTarget(Info, gc, phases) log;\n+  log_end(log, duration);\n+}\n+\n+XStatSubPhase::XStatSubPhase(const char* name) :\n+    XStatPhase(\"Subphase\", name) {}\n+\n+void XStatSubPhase::register_start(const Ticks& start) const {\n+  if (XThread::is_worker()) {\n+    LogTarget(Trace, gc, phases, start) log;\n+    log_start(log, true \/* thread *\/);\n+  } else {\n+    LogTarget(Debug, gc, phases, start) log;\n+    log_start(log, false \/* thread *\/);\n+  }\n+}\n+\n+void XStatSubPhase::register_end(const Ticks& start, const Ticks& end) const {\n+  if (XAbort::should_abort()) {\n+    return;\n+  }\n+\n+  XTracer::tracer()->report_thread_phase(name(), start, end);\n+\n+  const Tickspan duration = end - start;\n+  XStatSample(_sampler, duration.value());\n+\n+  if (XThread::is_worker()) {\n+    LogTarget(Trace, gc, phases) log;\n+    log_end(log, duration, true \/* thread *\/);\n+  } else {\n+    LogTarget(Debug, gc, phases) log;\n+    log_end(log, duration, false \/* thread *\/);\n+  }\n+}\n+\n+XStatCriticalPhase::XStatCriticalPhase(const char* name, bool verbose) :\n+    XStatPhase(\"Critical\", name),\n+    _counter(\"Critical\", name, XStatUnitOpsPerSecond),\n+    _verbose(verbose) {}\n+\n+void XStatCriticalPhase::register_start(const Ticks& start) const {\n+  \/\/ This is called from sensitive contexts, for example before an allocation stall\n+  \/\/ has been resolved. This means we must not access any oops in here since that\n+  \/\/ could lead to infinite recursion. Without access to the thread name we can't\n+  \/\/ really log anything useful here.\n+}\n+\n+void XStatCriticalPhase::register_end(const Ticks& start, const Ticks& end) const {\n+  XTracer::tracer()->report_thread_phase(name(), start, end);\n+\n+  const Tickspan duration = end - start;\n+  XStatSample(_sampler, duration.value());\n+  XStatInc(_counter);\n+\n+  if (_verbose) {\n+    LogTarget(Info, gc) log;\n+    log_end(log, duration, true \/* thread *\/);\n+  } else {\n+    LogTarget(Debug, gc) log;\n+    log_end(log, duration, true \/* thread *\/);\n+  }\n+}\n+\n+\/\/\n+\/\/ Stat timer\n+\/\/\n+THREAD_LOCAL uint32_t XStatTimerDisable::_active = 0;\n+\n+\/\/\n+\/\/ Stat sample\/inc\n+\/\/\n+void XStatSample(const XStatSampler& sampler, uint64_t value) {\n+  XStatSamplerData* const cpu_data = sampler.get();\n+  Atomic::add(&cpu_data->_nsamples, 1u);\n+  Atomic::add(&cpu_data->_sum, value);\n+\n+  uint64_t max = cpu_data->_max;\n+  for (;;) {\n+    if (max >= value) {\n+      \/\/ Not max\n+      break;\n+    }\n+\n+    const uint64_t new_max = value;\n+    const uint64_t prev_max = Atomic::cmpxchg(&cpu_data->_max, max, new_max);\n+    if (prev_max == max) {\n+      \/\/ Success\n+      break;\n+    }\n+\n+    \/\/ Retry\n+    max = prev_max;\n+  }\n+\n+  XTracer::tracer()->report_stat_sampler(sampler, value);\n+}\n+\n+void XStatInc(const XStatCounter& counter, uint64_t increment) {\n+  XStatCounterData* const cpu_data = counter.get();\n+  const uint64_t value = Atomic::add(&cpu_data->_counter, increment);\n+\n+  XTracer::tracer()->report_stat_counter(counter, increment, value);\n+}\n+\n+void XStatInc(const XStatUnsampledCounter& counter, uint64_t increment) {\n+  XStatCounterData* const cpu_data = counter.get();\n+  Atomic::add(&cpu_data->_counter, increment);\n+}\n+\n+\/\/\n+\/\/ Stat allocation rate\n+\/\/\n+const XStatUnsampledCounter XStatAllocRate::_counter(\"Allocation Rate\");\n+TruncatedSeq                XStatAllocRate::_samples(XStatAllocRate::sample_hz);\n+TruncatedSeq                XStatAllocRate::_rate(XStatAllocRate::sample_hz);\n+\n+const XStatUnsampledCounter& XStatAllocRate::counter() {\n+  return _counter;\n+}\n+\n+uint64_t XStatAllocRate::sample_and_reset() {\n+  const XStatCounterData bytes_per_sample = _counter.collect_and_reset();\n+  _samples.add(bytes_per_sample._counter);\n+\n+  const uint64_t bytes_per_second = _samples.sum();\n+  _rate.add(bytes_per_second);\n+\n+  return bytes_per_second;\n+}\n+\n+double XStatAllocRate::predict() {\n+  return _rate.predict_next();\n+}\n+\n+double XStatAllocRate::avg() {\n+  return _rate.avg();\n+}\n+\n+double XStatAllocRate::sd() {\n+  return _rate.sd();\n+}\n+\n+\/\/\n+\/\/ Stat thread\n+\/\/\n+XStat::XStat() :\n+    _metronome(sample_hz) {\n+  set_name(\"XStat\");\n+  create_and_start();\n+}\n+\n+void XStat::sample_and_collect(XStatSamplerHistory* history) const {\n+  \/\/ Sample counters\n+  for (const XStatCounter* counter = XStatCounter::first(); counter != NULL; counter = counter->next()) {\n+    counter->sample_and_reset();\n+  }\n+\n+  \/\/ Collect samples\n+  for (const XStatSampler* sampler = XStatSampler::first(); sampler != NULL; sampler = sampler->next()) {\n+    XStatSamplerHistory& sampler_history = history[sampler->id()];\n+    sampler_history.add(sampler->collect_and_reset());\n+  }\n+}\n+\n+bool XStat::should_print(LogTargetHandle log) const {\n+  static uint64_t print_at = ZStatisticsInterval;\n+  const uint64_t now = os::elapsedTime();\n+\n+  if (now < print_at) {\n+    return false;\n+  }\n+\n+  print_at = ((now \/ ZStatisticsInterval) * ZStatisticsInterval) + ZStatisticsInterval;\n+\n+  return log.is_enabled();\n+}\n+\n+void XStat::print(LogTargetHandle log, const XStatSamplerHistory* history) const {\n+  \/\/ Print\n+  log.print(\"=== Garbage Collection Statistics =======================================================================================================================\");\n+  log.print(\"                                                             Last 10s              Last 10m              Last 10h                Total\");\n+  log.print(\"                                                             Avg \/ Max             Avg \/ Max             Avg \/ Max             Avg \/ Max\");\n+\n+  for (const XStatSampler* sampler = XStatSampler::first(); sampler != NULL; sampler = sampler->next()) {\n+    const XStatSamplerHistory& sampler_history = history[sampler->id()];\n+    const XStatUnitPrinter printer = sampler->printer();\n+    printer(log, *sampler, sampler_history);\n+  }\n+\n+  log.print(\"=========================================================================================================================================================\");\n+}\n+\n+void XStat::run_service() {\n+  XStatSamplerHistory* const history = new XStatSamplerHistory[XStatSampler::count()];\n+  LogTarget(Info, gc, stats) log;\n+\n+  XStatSampler::sort();\n+\n+  \/\/ Main loop\n+  while (_metronome.wait_for_tick()) {\n+    sample_and_collect(history);\n+    if (should_print(log)) {\n+      print(log, history);\n+    }\n+  }\n+\n+  delete [] history;\n+}\n+\n+void XStat::stop_service() {\n+  _metronome.stop();\n+}\n+\n+\/\/\n+\/\/ Stat table\n+\/\/\n+class XStatTablePrinter {\n+private:\n+  static const size_t _buffer_size = 256;\n+\n+  const size_t _column0_width;\n+  const size_t _columnN_width;\n+  char         _buffer[_buffer_size];\n+\n+public:\n+  class XColumn {\n+  private:\n+    char* const  _buffer;\n+    const size_t _position;\n+    const size_t _width;\n+    const size_t _width_next;\n+\n+    XColumn next() const {\n+      \/\/ Insert space between columns\n+      _buffer[_position + _width] = ' ';\n+      return XColumn(_buffer, _position + _width + 1, _width_next, _width_next);\n+    }\n+\n+    size_t print(size_t position, const char* fmt, va_list va) {\n+      const int res = jio_vsnprintf(_buffer + position, _buffer_size - position, fmt, va);\n+      if (res < 0) {\n+        return 0;\n+      }\n+\n+      return (size_t)res;\n+    }\n+\n+  public:\n+    XColumn(char* buffer, size_t position, size_t width, size_t width_next) :\n+        _buffer(buffer),\n+        _position(position),\n+        _width(width),\n+        _width_next(width_next) {}\n+\n+    XColumn left(const char* fmt, ...) ATTRIBUTE_PRINTF(2, 3) {\n+      va_list va;\n+\n+      va_start(va, fmt);\n+      const size_t written = print(_position, fmt, va);\n+      va_end(va);\n+\n+      if (written < _width) {\n+        \/\/ Fill empty space\n+        memset(_buffer + _position + written, ' ', _width - written);\n+      }\n+\n+      return next();\n+    }\n+\n+    XColumn right(const char* fmt, ...) ATTRIBUTE_PRINTF(2, 3) {\n+      va_list va;\n+\n+      va_start(va, fmt);\n+      const size_t written = print(_position, fmt, va);\n+      va_end(va);\n+\n+      if (written > _width) {\n+        \/\/ Line too long\n+        return fill('?');\n+      }\n+\n+      if (written < _width) {\n+        \/\/ Short line, move all to right\n+        memmove(_buffer + _position + _width - written, _buffer + _position, written);\n+\n+        \/\/ Fill empty space\n+        memset(_buffer + _position, ' ', _width - written);\n+      }\n+\n+      return next();\n+    }\n+\n+    XColumn center(const char* fmt, ...) ATTRIBUTE_PRINTF(2, 3) {\n+      va_list va;\n+\n+      va_start(va, fmt);\n+      const size_t written = print(_position, fmt, va);\n+      va_end(va);\n+\n+      if (written > _width) {\n+        \/\/ Line too long\n+        return fill('?');\n+      }\n+\n+      if (written < _width) {\n+        \/\/ Short line, move all to center\n+        const size_t start_space = (_width - written) \/ 2;\n+        const size_t end_space = _width - written - start_space;\n+        memmove(_buffer + _position + start_space, _buffer + _position, written);\n+\n+        \/\/ Fill empty spaces\n+        memset(_buffer + _position, ' ', start_space);\n+        memset(_buffer + _position + start_space + written, ' ', end_space);\n+      }\n+\n+      return next();\n+    }\n+\n+    XColumn fill(char filler = ' ') {\n+      memset(_buffer + _position, filler, _width);\n+      return next();\n+    }\n+\n+    const char* end() {\n+      _buffer[_position] = '\\0';\n+      return _buffer;\n+    }\n+  };\n+\n+public:\n+  XStatTablePrinter(size_t column0_width, size_t columnN_width) :\n+      _column0_width(column0_width),\n+      _columnN_width(columnN_width) {}\n+\n+  XColumn operator()() {\n+    return XColumn(_buffer, 0, _column0_width, _columnN_width);\n+  }\n+};\n+\n+\/\/\n+\/\/ Stat cycle\n+\/\/\n+uint64_t  XStatCycle::_nwarmup_cycles = 0;\n+Ticks     XStatCycle::_start_of_last;\n+Ticks     XStatCycle::_end_of_last;\n+NumberSeq XStatCycle::_serial_time(0.7 \/* alpha *\/);\n+NumberSeq XStatCycle::_parallelizable_time(0.7 \/* alpha *\/);\n+uint      XStatCycle::_last_active_workers = 0;\n+\n+void XStatCycle::at_start() {\n+  _start_of_last = Ticks::now();\n+}\n+\n+void XStatCycle::at_end(GCCause::Cause cause, uint active_workers) {\n+  _end_of_last = Ticks::now();\n+\n+  if (cause == GCCause::_z_warmup) {\n+    _nwarmup_cycles++;\n+  }\n+\n+  _last_active_workers = active_workers;\n+\n+  \/\/ Calculate serial and parallelizable GC cycle times\n+  const double duration = (_end_of_last - _start_of_last).seconds();\n+  const double workers_duration = XStatWorkers::get_and_reset_duration();\n+  const double serial_time = duration - workers_duration;\n+  const double parallelizable_time = workers_duration * active_workers;\n+  _serial_time.add(serial_time);\n+  _parallelizable_time.add(parallelizable_time);\n+}\n+\n+bool XStatCycle::is_warm() {\n+  return _nwarmup_cycles >= 3;\n+}\n+\n+uint64_t XStatCycle::nwarmup_cycles() {\n+  return _nwarmup_cycles;\n+}\n+\n+bool XStatCycle::is_time_trustable() {\n+  \/\/ The times are considered trustable if we\n+  \/\/ have completed at least one warmup cycle.\n+  return _nwarmup_cycles > 0;\n+}\n+\n+const AbsSeq& XStatCycle::serial_time() {\n+  return _serial_time;\n+}\n+\n+const AbsSeq& XStatCycle::parallelizable_time() {\n+  return _parallelizable_time;\n+}\n+\n+uint XStatCycle::last_active_workers() {\n+  return _last_active_workers;\n+}\n+\n+double XStatCycle::time_since_last() {\n+  if (_end_of_last.value() == 0) {\n+    \/\/ No end recorded yet, return time since VM start\n+    return os::elapsedTime();\n+  }\n+\n+  const Ticks now = Ticks::now();\n+  const Tickspan time_since_last = now - _end_of_last;\n+  return time_since_last.seconds();\n+}\n+\n+\/\/\n+\/\/ Stat workers\n+\/\/\n+Ticks XStatWorkers::_start_of_last;\n+Tickspan XStatWorkers::_accumulated_duration;\n+\n+void XStatWorkers::at_start() {\n+  _start_of_last = Ticks::now();\n+}\n+\n+void XStatWorkers::at_end() {\n+  const Ticks now = Ticks::now();\n+  const Tickspan duration = now - _start_of_last;\n+  _accumulated_duration += duration;\n+}\n+\n+double XStatWorkers::get_and_reset_duration() {\n+  const double duration = _accumulated_duration.seconds();\n+  const Ticks now = Ticks::now();\n+  _accumulated_duration = now - now;\n+  return duration;\n+}\n+\n+\/\/\n+\/\/ Stat load\n+\/\/\n+void XStatLoad::print() {\n+  double loadavg[3] = {};\n+  os::loadavg(loadavg, ARRAY_SIZE(loadavg));\n+  log_info(gc, load)(\"Load: %.2f\/%.2f\/%.2f\", loadavg[0], loadavg[1], loadavg[2]);\n+}\n+\n+\/\/\n+\/\/ Stat mark\n+\/\/\n+size_t XStatMark::_nstripes;\n+size_t XStatMark::_nproactiveflush;\n+size_t XStatMark::_nterminateflush;\n+size_t XStatMark::_ntrycomplete;\n+size_t XStatMark::_ncontinue;\n+size_t XStatMark::_mark_stack_usage;\n+\n+void XStatMark::set_at_mark_start(size_t nstripes) {\n+  _nstripes = nstripes;\n+}\n+\n+void XStatMark::set_at_mark_end(size_t nproactiveflush,\n+                                size_t nterminateflush,\n+                                size_t ntrycomplete,\n+                                size_t ncontinue) {\n+  _nproactiveflush = nproactiveflush;\n+  _nterminateflush = nterminateflush;\n+  _ntrycomplete = ntrycomplete;\n+  _ncontinue = ncontinue;\n+}\n+\n+void XStatMark::set_at_mark_free(size_t mark_stack_usage) {\n+  _mark_stack_usage = mark_stack_usage;\n+}\n+\n+void XStatMark::print() {\n+  log_info(gc, marking)(\"Mark: \"\n+                        SIZE_FORMAT \" stripe(s), \"\n+                        SIZE_FORMAT \" proactive flush(es), \"\n+                        SIZE_FORMAT \" terminate flush(es), \"\n+                        SIZE_FORMAT \" completion(s), \"\n+                        SIZE_FORMAT \" continuation(s) \",\n+                        _nstripes,\n+                        _nproactiveflush,\n+                        _nterminateflush,\n+                        _ntrycomplete,\n+                        _ncontinue);\n+\n+  log_info(gc, marking)(\"Mark Stack Usage: \" SIZE_FORMAT \"M\", _mark_stack_usage \/ M);\n+}\n+\n+\/\/\n+\/\/ Stat relocation\n+\/\/\n+XRelocationSetSelectorStats XStatRelocation::_selector_stats;\n+size_t                      XStatRelocation::_forwarding_usage;\n+size_t                      XStatRelocation::_small_in_place_count;\n+size_t                      XStatRelocation::_medium_in_place_count;\n+\n+void XStatRelocation::set_at_select_relocation_set(const XRelocationSetSelectorStats& selector_stats) {\n+  _selector_stats = selector_stats;\n+}\n+\n+void XStatRelocation::set_at_install_relocation_set(size_t forwarding_usage) {\n+  _forwarding_usage = forwarding_usage;\n+}\n+\n+void XStatRelocation::set_at_relocate_end(size_t small_in_place_count, size_t medium_in_place_count) {\n+  _small_in_place_count = small_in_place_count;\n+  _medium_in_place_count = medium_in_place_count;\n+}\n+\n+void XStatRelocation::print(const char* name,\n+                            const XRelocationSetSelectorGroupStats& selector_group,\n+                            size_t in_place_count) {\n+  log_info(gc, reloc)(\"%s Pages: \" SIZE_FORMAT \" \/ \" SIZE_FORMAT \"M, Empty: \" SIZE_FORMAT \"M, \"\n+                      \"Relocated: \" SIZE_FORMAT \"M, In-Place: \" SIZE_FORMAT,\n+                      name,\n+                      selector_group.npages_candidates(),\n+                      selector_group.total() \/ M,\n+                      selector_group.empty() \/ M,\n+                      selector_group.relocate() \/ M,\n+                      in_place_count);\n+}\n+\n+void XStatRelocation::print() {\n+  print(\"Small\", _selector_stats.small(), _small_in_place_count);\n+  if (XPageSizeMedium != 0) {\n+    print(\"Medium\", _selector_stats.medium(), _medium_in_place_count);\n+  }\n+  print(\"Large\", _selector_stats.large(), 0 \/* in_place_count *\/);\n+\n+  log_info(gc, reloc)(\"Forwarding Usage: \" SIZE_FORMAT \"M\", _forwarding_usage \/ M);\n+}\n+\n+\/\/\n+\/\/ Stat nmethods\n+\/\/\n+void XStatNMethods::print() {\n+  log_info(gc, nmethod)(\"NMethods: \" SIZE_FORMAT \" registered, \" SIZE_FORMAT \" unregistered\",\n+                        XNMethodTable::registered_nmethods(),\n+                        XNMethodTable::unregistered_nmethods());\n+}\n+\n+\/\/\n+\/\/ Stat metaspace\n+\/\/\n+void XStatMetaspace::print() {\n+  MetaspaceCombinedStats stats = MetaspaceUtils::get_combined_statistics();\n+  log_info(gc, metaspace)(\"Metaspace: \"\n+                          SIZE_FORMAT \"M used, \"\n+                          SIZE_FORMAT \"M committed, \" SIZE_FORMAT \"M reserved\",\n+                          stats.used() \/ M,\n+                          stats.committed() \/ M,\n+                          stats.reserved() \/ M);\n+}\n+\n+\/\/\n+\/\/ Stat references\n+\/\/\n+XStatReferences::XCount XStatReferences::_soft;\n+XStatReferences::XCount XStatReferences::_weak;\n+XStatReferences::XCount XStatReferences::_final;\n+XStatReferences::XCount XStatReferences::_phantom;\n+\n+void XStatReferences::set(XCount* count, size_t encountered, size_t discovered, size_t enqueued) {\n+  count->encountered = encountered;\n+  count->discovered = discovered;\n+  count->enqueued = enqueued;\n+}\n+\n+void XStatReferences::set_soft(size_t encountered, size_t discovered, size_t enqueued) {\n+  set(&_soft, encountered, discovered, enqueued);\n+}\n+\n+void XStatReferences::set_weak(size_t encountered, size_t discovered, size_t enqueued) {\n+  set(&_weak, encountered, discovered, enqueued);\n+}\n+\n+void XStatReferences::set_final(size_t encountered, size_t discovered, size_t enqueued) {\n+  set(&_final, encountered, discovered, enqueued);\n+}\n+\n+void XStatReferences::set_phantom(size_t encountered, size_t discovered, size_t enqueued) {\n+  set(&_phantom, encountered, discovered, enqueued);\n+}\n+\n+void XStatReferences::print(const char* name, const XStatReferences::XCount& ref) {\n+  log_info(gc, ref)(\"%s: \"\n+                    SIZE_FORMAT \" encountered, \"\n+                    SIZE_FORMAT \" discovered, \"\n+                    SIZE_FORMAT \" enqueued\",\n+                    name,\n+                    ref.encountered,\n+                    ref.discovered,\n+                    ref.enqueued);\n+}\n+\n+void XStatReferences::print() {\n+  print(\"Soft\", _soft);\n+  print(\"Weak\", _weak);\n+  print(\"Final\", _final);\n+  print(\"Phantom\", _phantom);\n+}\n+\n+\/\/\n+\/\/ Stat heap\n+\/\/\n+XStatHeap::XAtInitialize XStatHeap::_at_initialize;\n+XStatHeap::XAtMarkStart XStatHeap::_at_mark_start;\n+XStatHeap::XAtMarkEnd XStatHeap::_at_mark_end;\n+XStatHeap::XAtRelocateStart XStatHeap::_at_relocate_start;\n+XStatHeap::XAtRelocateEnd XStatHeap::_at_relocate_end;\n+\n+size_t XStatHeap::capacity_high() {\n+  return MAX4(_at_mark_start.capacity,\n+              _at_mark_end.capacity,\n+              _at_relocate_start.capacity,\n+              _at_relocate_end.capacity);\n+}\n+\n+size_t XStatHeap::capacity_low() {\n+  return MIN4(_at_mark_start.capacity,\n+              _at_mark_end.capacity,\n+              _at_relocate_start.capacity,\n+              _at_relocate_end.capacity);\n+}\n+\n+size_t XStatHeap::free(size_t used) {\n+  return _at_initialize.max_capacity - used;\n+}\n+\n+size_t XStatHeap::allocated(size_t used, size_t reclaimed) {\n+  \/\/ The amount of allocated memory between point A and B is used(B) - used(A).\n+  \/\/ However, we might also have reclaimed memory between point A and B. This\n+  \/\/ means the current amount of used memory must be incremented by the amount\n+  \/\/ reclaimed, so that used(B) represents the amount of used memory we would\n+  \/\/ have had if we had not reclaimed anything.\n+  return (used + reclaimed) - _at_mark_start.used;\n+}\n+\n+size_t XStatHeap::garbage(size_t reclaimed) {\n+  return _at_mark_end.garbage - reclaimed;\n+}\n+\n+void XStatHeap::set_at_initialize(const XPageAllocatorStats& stats) {\n+  _at_initialize.min_capacity = stats.min_capacity();\n+  _at_initialize.max_capacity = stats.max_capacity();\n+}\n+\n+void XStatHeap::set_at_mark_start(const XPageAllocatorStats& stats) {\n+  _at_mark_start.soft_max_capacity = stats.soft_max_capacity();\n+  _at_mark_start.capacity = stats.capacity();\n+  _at_mark_start.free = free(stats.used());\n+  _at_mark_start.used = stats.used();\n+}\n+\n+void XStatHeap::set_at_mark_end(const XPageAllocatorStats& stats) {\n+  _at_mark_end.capacity = stats.capacity();\n+  _at_mark_end.free = free(stats.used());\n+  _at_mark_end.used = stats.used();\n+  _at_mark_end.allocated = allocated(stats.used(), 0 \/* reclaimed *\/);\n+}\n+\n+void XStatHeap::set_at_select_relocation_set(const XRelocationSetSelectorStats& stats) {\n+  const size_t live = stats.small().live() + stats.medium().live() + stats.large().live();\n+  _at_mark_end.live = live;\n+  _at_mark_end.garbage = _at_mark_start.used - live;\n+}\n+\n+void XStatHeap::set_at_relocate_start(const XPageAllocatorStats& stats) {\n+  _at_relocate_start.capacity = stats.capacity();\n+  _at_relocate_start.free = free(stats.used());\n+  _at_relocate_start.used = stats.used();\n+  _at_relocate_start.allocated = allocated(stats.used(), stats.reclaimed());\n+  _at_relocate_start.garbage = garbage(stats.reclaimed());\n+  _at_relocate_start.reclaimed = stats.reclaimed();\n+}\n+\n+void XStatHeap::set_at_relocate_end(const XPageAllocatorStats& stats, size_t non_worker_relocated) {\n+  const size_t reclaimed = stats.reclaimed() - MIN2(non_worker_relocated, stats.reclaimed());\n+\n+  _at_relocate_end.capacity = stats.capacity();\n+  _at_relocate_end.capacity_high = capacity_high();\n+  _at_relocate_end.capacity_low = capacity_low();\n+  _at_relocate_end.free = free(stats.used());\n+  _at_relocate_end.free_high = free(stats.used_low());\n+  _at_relocate_end.free_low = free(stats.used_high());\n+  _at_relocate_end.used = stats.used();\n+  _at_relocate_end.used_high = stats.used_high();\n+  _at_relocate_end.used_low = stats.used_low();\n+  _at_relocate_end.allocated = allocated(stats.used(), reclaimed);\n+  _at_relocate_end.garbage = garbage(reclaimed);\n+  _at_relocate_end.reclaimed = reclaimed;\n+}\n+\n+size_t XStatHeap::max_capacity() {\n+  return _at_initialize.max_capacity;\n+}\n+\n+size_t XStatHeap::used_at_mark_start() {\n+  return _at_mark_start.used;\n+}\n+\n+size_t XStatHeap::used_at_relocate_end() {\n+  return _at_relocate_end.used;\n+}\n+\n+void XStatHeap::print() {\n+  log_info(gc, heap)(\"Min Capacity: \"\n+                     XSIZE_FMT, XSIZE_ARGS(_at_initialize.min_capacity));\n+  log_info(gc, heap)(\"Max Capacity: \"\n+                     XSIZE_FMT, XSIZE_ARGS(_at_initialize.max_capacity));\n+  log_info(gc, heap)(\"Soft Max Capacity: \"\n+                     XSIZE_FMT, XSIZE_ARGS(_at_mark_start.soft_max_capacity));\n+\n+  XStatTablePrinter table(10, 18);\n+  log_info(gc, heap)(\"%s\", table()\n+                     .fill()\n+                     .center(\"Mark Start\")\n+                     .center(\"Mark End\")\n+                     .center(\"Relocate Start\")\n+                     .center(\"Relocate End\")\n+                     .center(\"High\")\n+                     .center(\"Low\")\n+                     .end());\n+  log_info(gc, heap)(\"%s\", table()\n+                     .right(\"Capacity:\")\n+                     .left(XTABLE_ARGS(_at_mark_start.capacity))\n+                     .left(XTABLE_ARGS(_at_mark_end.capacity))\n+                     .left(XTABLE_ARGS(_at_relocate_start.capacity))\n+                     .left(XTABLE_ARGS(_at_relocate_end.capacity))\n+                     .left(XTABLE_ARGS(_at_relocate_end.capacity_high))\n+                     .left(XTABLE_ARGS(_at_relocate_end.capacity_low))\n+                     .end());\n+  log_info(gc, heap)(\"%s\", table()\n+                     .right(\"Free:\")\n+                     .left(XTABLE_ARGS(_at_mark_start.free))\n+                     .left(XTABLE_ARGS(_at_mark_end.free))\n+                     .left(XTABLE_ARGS(_at_relocate_start.free))\n+                     .left(XTABLE_ARGS(_at_relocate_end.free))\n+                     .left(XTABLE_ARGS(_at_relocate_end.free_high))\n+                     .left(XTABLE_ARGS(_at_relocate_end.free_low))\n+                     .end());\n+  log_info(gc, heap)(\"%s\", table()\n+                     .right(\"Used:\")\n+                     .left(XTABLE_ARGS(_at_mark_start.used))\n+                     .left(XTABLE_ARGS(_at_mark_end.used))\n+                     .left(XTABLE_ARGS(_at_relocate_start.used))\n+                     .left(XTABLE_ARGS(_at_relocate_end.used))\n+                     .left(XTABLE_ARGS(_at_relocate_end.used_high))\n+                     .left(XTABLE_ARGS(_at_relocate_end.used_low))\n+                     .end());\n+  log_info(gc, heap)(\"%s\", table()\n+                     .right(\"Live:\")\n+                     .left(XTABLE_ARGS_NA)\n+                     .left(XTABLE_ARGS(_at_mark_end.live))\n+                     .left(XTABLE_ARGS(_at_mark_end.live \/* Same as at mark end *\/))\n+                     .left(XTABLE_ARGS(_at_mark_end.live \/* Same as at mark end *\/))\n+                     .left(XTABLE_ARGS_NA)\n+                     .left(XTABLE_ARGS_NA)\n+                     .end());\n+  log_info(gc, heap)(\"%s\", table()\n+                     .right(\"Allocated:\")\n+                     .left(XTABLE_ARGS_NA)\n+                     .left(XTABLE_ARGS(_at_mark_end.allocated))\n+                     .left(XTABLE_ARGS(_at_relocate_start.allocated))\n+                     .left(XTABLE_ARGS(_at_relocate_end.allocated))\n+                     .left(XTABLE_ARGS_NA)\n+                     .left(XTABLE_ARGS_NA)\n+                     .end());\n+  log_info(gc, heap)(\"%s\", table()\n+                     .right(\"Garbage:\")\n+                     .left(XTABLE_ARGS_NA)\n+                     .left(XTABLE_ARGS(_at_mark_end.garbage))\n+                     .left(XTABLE_ARGS(_at_relocate_start.garbage))\n+                     .left(XTABLE_ARGS(_at_relocate_end.garbage))\n+                     .left(XTABLE_ARGS_NA)\n+                     .left(XTABLE_ARGS_NA)\n+                     .end());\n+  log_info(gc, heap)(\"%s\", table()\n+                     .right(\"Reclaimed:\")\n+                     .left(XTABLE_ARGS_NA)\n+                     .left(XTABLE_ARGS_NA)\n+                     .left(XTABLE_ARGS(_at_relocate_start.reclaimed))\n+                     .left(XTABLE_ARGS(_at_relocate_end.reclaimed))\n+                     .left(XTABLE_ARGS_NA)\n+                     .left(XTABLE_ARGS_NA)\n+                     .end());\n+}\n","filename":"src\/hotspot\/share\/gc\/x\/xStat.cpp","additions":1513,"deletions":0,"binary":false,"changes":1513,"status":"added"},{"patch":"@@ -0,0 +1,578 @@\n+\/*\n+ * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XSTAT_HPP\n+#define SHARE_GC_X_XSTAT_HPP\n+\n+#include \"gc\/shared\/concurrentGCThread.hpp\"\n+#include \"gc\/shared\/gcCause.hpp\"\n+#include \"gc\/shared\/gcTimer.hpp\"\n+#include \"gc\/x\/xMetronome.hpp\"\n+#include \"logging\/logHandle.hpp\"\n+#include \"memory\/allocation.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include \"utilities\/numberSeq.hpp\"\n+#include \"utilities\/ticks.hpp\"\n+\n+class XPage;\n+class XPageAllocatorStats;\n+class XRelocationSetSelectorGroupStats;\n+class XRelocationSetSelectorStats;\n+class XStatSampler;\n+class XStatSamplerHistory;\n+struct XStatCounterData;\n+struct XStatSamplerData;\n+\n+\/\/\n+\/\/ Stat unit printers\n+\/\/\n+typedef void (*XStatUnitPrinter)(LogTargetHandle log, const XStatSampler&, const XStatSamplerHistory&);\n+\n+void XStatUnitTime(LogTargetHandle log, const XStatSampler& sampler, const XStatSamplerHistory& history);\n+void XStatUnitBytes(LogTargetHandle log, const XStatSampler& sampler, const XStatSamplerHistory& history);\n+void XStatUnitThreads(LogTargetHandle log, const XStatSampler& sampler, const XStatSamplerHistory& history);\n+void XStatUnitBytesPerSecond(LogTargetHandle log, const XStatSampler& sampler, const XStatSamplerHistory& history);\n+void XStatUnitOpsPerSecond(LogTargetHandle log, const XStatSampler& sampler, const XStatSamplerHistory& history);\n+\n+\/\/\n+\/\/ Stat value\n+\/\/\n+class XStatValue {\n+private:\n+  static uintptr_t _base;\n+  static uint32_t  _cpu_offset;\n+\n+  const char* const _group;\n+  const char* const _name;\n+  const uint32_t    _id;\n+  const uint32_t    _offset;\n+\n+protected:\n+  XStatValue(const char* group,\n+             const char* name,\n+             uint32_t id,\n+             uint32_t size);\n+\n+  template <typename T> T* get_cpu_local(uint32_t cpu) const;\n+\n+public:\n+  static void initialize();\n+\n+  const char* group() const;\n+  const char* name() const;\n+  uint32_t id() const;\n+};\n+\n+\/\/\n+\/\/ Stat iterable value\n+\/\/\n+template <typename T>\n+class XStatIterableValue : public XStatValue {\n+private:\n+  static uint32_t _count;\n+  static T*       _first;\n+\n+  T* _next;\n+\n+  T* insert() const;\n+\n+protected:\n+  XStatIterableValue(const char* group,\n+                     const char* name,\n+                     uint32_t size);\n+\n+public:\n+  static void sort();\n+\n+  static uint32_t count() {\n+    return _count;\n+  }\n+\n+  static T* first() {\n+    return _first;\n+  }\n+\n+  T* next() const {\n+    return _next;\n+  }\n+};\n+\n+template <typename T> uint32_t XStatIterableValue<T>::_count = 0;\n+template <typename T> T*       XStatIterableValue<T>::_first = NULL;\n+\n+\/\/\n+\/\/ Stat sampler\n+\/\/\n+class XStatSampler : public XStatIterableValue<XStatSampler> {\n+private:\n+  const XStatUnitPrinter _printer;\n+\n+public:\n+  XStatSampler(const char* group,\n+               const char* name,\n+               XStatUnitPrinter printer);\n+\n+  XStatSamplerData* get() const;\n+  XStatSamplerData collect_and_reset() const;\n+\n+  XStatUnitPrinter printer() const;\n+};\n+\n+\/\/\n+\/\/ Stat counter\n+\/\/\n+class XStatCounter : public XStatIterableValue<XStatCounter> {\n+private:\n+  const XStatSampler _sampler;\n+\n+public:\n+  XStatCounter(const char* group,\n+               const char* name,\n+               XStatUnitPrinter printer);\n+\n+  XStatCounterData* get() const;\n+  void sample_and_reset() const;\n+};\n+\n+\/\/\n+\/\/ Stat unsampled counter\n+\/\/\n+class XStatUnsampledCounter : public XStatIterableValue<XStatUnsampledCounter> {\n+public:\n+  XStatUnsampledCounter(const char* name);\n+\n+  XStatCounterData* get() const;\n+  XStatCounterData collect_and_reset() const;\n+};\n+\n+\/\/\n+\/\/ Stat MMU (Minimum Mutator Utilization)\n+\/\/\n+class XStatMMUPause {\n+private:\n+  double _start;\n+  double _end;\n+\n+public:\n+  XStatMMUPause();\n+  XStatMMUPause(const Ticks& start, const Ticks& end);\n+\n+  double end() const;\n+  double overlap(double start, double end) const;\n+};\n+\n+class XStatMMU {\n+private:\n+  static size_t        _next;\n+  static size_t        _npauses;\n+  static XStatMMUPause _pauses[200]; \/\/ Record the last 200 pauses\n+\n+  static double _mmu_2ms;\n+  static double _mmu_5ms;\n+  static double _mmu_10ms;\n+  static double _mmu_20ms;\n+  static double _mmu_50ms;\n+  static double _mmu_100ms;\n+\n+  static const XStatMMUPause& pause(size_t index);\n+  static double calculate_mmu(double time_slice);\n+\n+public:\n+  static void register_pause(const Ticks& start, const Ticks& end);\n+\n+  static void print();\n+};\n+\n+\/\/\n+\/\/ Stat phases\n+\/\/\n+class XStatPhase {\n+private:\n+  static ConcurrentGCTimer _timer;\n+\n+protected:\n+  const XStatSampler _sampler;\n+\n+  XStatPhase(const char* group, const char* name);\n+\n+  void log_start(LogTargetHandle log, bool thread = false) const;\n+  void log_end(LogTargetHandle log, const Tickspan& duration, bool thread = false) const;\n+\n+public:\n+  static ConcurrentGCTimer* timer();\n+\n+  const char* name() const;\n+\n+  virtual void register_start(const Ticks& start) const = 0;\n+  virtual void register_end(const Ticks& start, const Ticks& end) const = 0;\n+};\n+\n+class XStatPhaseCycle : public XStatPhase {\n+public:\n+  XStatPhaseCycle(const char* name);\n+\n+  virtual void register_start(const Ticks& start) const;\n+  virtual void register_end(const Ticks& start, const Ticks& end) const;\n+};\n+\n+class XStatPhasePause : public XStatPhase {\n+private:\n+  static Tickspan _max; \/\/ Max pause time\n+\n+public:\n+  XStatPhasePause(const char* name);\n+\n+  static const Tickspan& max();\n+\n+  virtual void register_start(const Ticks& start) const;\n+  virtual void register_end(const Ticks& start, const Ticks& end) const;\n+};\n+\n+class XStatPhaseConcurrent : public XStatPhase {\n+public:\n+  XStatPhaseConcurrent(const char* name);\n+\n+  virtual void register_start(const Ticks& start) const;\n+  virtual void register_end(const Ticks& start, const Ticks& end) const;\n+};\n+\n+class XStatSubPhase : public XStatPhase {\n+public:\n+  XStatSubPhase(const char* name);\n+\n+  virtual void register_start(const Ticks& start) const;\n+  virtual void register_end(const Ticks& start, const Ticks& end) const;\n+};\n+\n+class XStatCriticalPhase : public XStatPhase {\n+private:\n+  const XStatCounter _counter;\n+  const bool         _verbose;\n+\n+public:\n+  XStatCriticalPhase(const char* name, bool verbose = true);\n+\n+  virtual void register_start(const Ticks& start) const;\n+  virtual void register_end(const Ticks& start, const Ticks& end) const;\n+};\n+\n+\/\/\n+\/\/ Stat timer\n+\/\/\n+class XStatTimerDisable : public StackObj {\n+private:\n+  static THREAD_LOCAL uint32_t _active;\n+\n+public:\n+  XStatTimerDisable() {\n+    _active++;\n+  }\n+\n+  ~XStatTimerDisable() {\n+    _active--;\n+  }\n+\n+  static bool is_active() {\n+    return _active > 0;\n+  }\n+};\n+\n+class XStatTimer : public StackObj {\n+private:\n+  const bool        _enabled;\n+  const XStatPhase& _phase;\n+  const Ticks       _start;\n+\n+public:\n+  XStatTimer(const XStatPhase& phase) :\n+      _enabled(!XStatTimerDisable::is_active()),\n+      _phase(phase),\n+      _start(Ticks::now()) {\n+    if (_enabled) {\n+      _phase.register_start(_start);\n+    }\n+  }\n+\n+  ~XStatTimer() {\n+    if (_enabled) {\n+      const Ticks end = Ticks::now();\n+      _phase.register_end(_start, end);\n+    }\n+  }\n+};\n+\n+\/\/\n+\/\/ Stat sample\/increment\n+\/\/\n+void XStatSample(const XStatSampler& sampler, uint64_t value);\n+void XStatInc(const XStatCounter& counter, uint64_t increment = 1);\n+void XStatInc(const XStatUnsampledCounter& counter, uint64_t increment = 1);\n+\n+\/\/\n+\/\/ Stat allocation rate\n+\/\/\n+class XStatAllocRate : public AllStatic {\n+private:\n+  static const XStatUnsampledCounter _counter;\n+  static TruncatedSeq                _samples;\n+  static TruncatedSeq                _rate;\n+\n+public:\n+  static const uint64_t sample_hz = 10;\n+\n+  static const XStatUnsampledCounter& counter();\n+  static uint64_t sample_and_reset();\n+\n+  static double predict();\n+  static double avg();\n+  static double sd();\n+};\n+\n+\/\/\n+\/\/ Stat thread\n+\/\/\n+class XStat : public ConcurrentGCThread {\n+private:\n+  static const uint64_t sample_hz = 1;\n+\n+  XMetronome _metronome;\n+\n+  void sample_and_collect(XStatSamplerHistory* history) const;\n+  bool should_print(LogTargetHandle log) const;\n+  void print(LogTargetHandle log, const XStatSamplerHistory* history) const;\n+\n+protected:\n+  virtual void run_service();\n+  virtual void stop_service();\n+\n+public:\n+  XStat();\n+};\n+\n+\/\/\n+\/\/ Stat cycle\n+\/\/\n+class XStatCycle : public AllStatic {\n+private:\n+  static uint64_t  _nwarmup_cycles;\n+  static Ticks     _start_of_last;\n+  static Ticks     _end_of_last;\n+  static NumberSeq _serial_time;\n+  static NumberSeq _parallelizable_time;\n+  static uint      _last_active_workers;\n+\n+public:\n+  static void at_start();\n+  static void at_end(GCCause::Cause cause, uint active_workers);\n+\n+  static bool is_warm();\n+  static uint64_t nwarmup_cycles();\n+\n+  static bool is_time_trustable();\n+  static const AbsSeq& serial_time();\n+  static const AbsSeq& parallelizable_time();\n+\n+  static uint last_active_workers();\n+\n+  static double time_since_last();\n+};\n+\n+\/\/\n+\/\/ Stat workers\n+\/\/\n+class XStatWorkers : public AllStatic {\n+private:\n+  static Ticks    _start_of_last;\n+  static Tickspan _accumulated_duration;\n+\n+public:\n+  static void at_start();\n+  static void at_end();\n+\n+  static double get_and_reset_duration();\n+};\n+\n+\/\/\n+\/\/ Stat load\n+\/\/\n+class XStatLoad : public AllStatic {\n+public:\n+  static void print();\n+};\n+\n+\/\/\n+\/\/ Stat mark\n+\/\/\n+class XStatMark : public AllStatic {\n+private:\n+  static size_t _nstripes;\n+  static size_t _nproactiveflush;\n+  static size_t _nterminateflush;\n+  static size_t _ntrycomplete;\n+  static size_t _ncontinue;\n+  static size_t _mark_stack_usage;\n+\n+public:\n+  static void set_at_mark_start(size_t nstripes);\n+  static void set_at_mark_end(size_t nproactiveflush,\n+                              size_t nterminateflush,\n+                              size_t ntrycomplete,\n+                              size_t ncontinue);\n+  static void set_at_mark_free(size_t mark_stack_usage);\n+\n+  static void print();\n+};\n+\n+\/\/\n+\/\/ Stat relocation\n+\/\/\n+class XStatRelocation : public AllStatic {\n+private:\n+  static XRelocationSetSelectorStats _selector_stats;\n+  static size_t                      _forwarding_usage;\n+  static size_t                      _small_in_place_count;\n+  static size_t                      _medium_in_place_count;\n+\n+  static void print(const char* name,\n+                    const XRelocationSetSelectorGroupStats& selector_group,\n+                    size_t in_place_count);\n+\n+public:\n+  static void set_at_select_relocation_set(const XRelocationSetSelectorStats& selector_stats);\n+  static void set_at_install_relocation_set(size_t forwarding_usage);\n+  static void set_at_relocate_end(size_t small_in_place_count, size_t medium_in_place_count);\n+\n+  static void print();\n+};\n+\n+\/\/\n+\/\/ Stat nmethods\n+\/\/\n+class XStatNMethods : public AllStatic {\n+public:\n+  static void print();\n+};\n+\n+\/\/\n+\/\/ Stat metaspace\n+\/\/\n+class XStatMetaspace : public AllStatic {\n+public:\n+  static void print();\n+};\n+\n+\/\/\n+\/\/ Stat references\n+\/\/\n+class XStatReferences : public AllStatic {\n+private:\n+  static struct XCount {\n+    size_t encountered;\n+    size_t discovered;\n+    size_t enqueued;\n+  } _soft, _weak, _final, _phantom;\n+\n+  static void set(XCount* count, size_t encountered, size_t discovered, size_t enqueued);\n+  static void print(const char* name, const XCount& ref);\n+\n+public:\n+  static void set_soft(size_t encountered, size_t discovered, size_t enqueued);\n+  static void set_weak(size_t encountered, size_t discovered, size_t enqueued);\n+  static void set_final(size_t encountered, size_t discovered, size_t enqueued);\n+  static void set_phantom(size_t encountered, size_t discovered, size_t enqueued);\n+\n+  static void print();\n+};\n+\n+\/\/\n+\/\/ Stat heap\n+\/\/\n+class XStatHeap : public AllStatic {\n+private:\n+  static struct XAtInitialize {\n+    size_t min_capacity;\n+    size_t max_capacity;\n+  } _at_initialize;\n+\n+  static struct XAtMarkStart {\n+    size_t soft_max_capacity;\n+    size_t capacity;\n+    size_t free;\n+    size_t used;\n+  } _at_mark_start;\n+\n+  static struct XAtMarkEnd {\n+    size_t capacity;\n+    size_t free;\n+    size_t used;\n+    size_t live;\n+    size_t allocated;\n+    size_t garbage;\n+  } _at_mark_end;\n+\n+  static struct XAtRelocateStart {\n+    size_t capacity;\n+    size_t free;\n+    size_t used;\n+    size_t allocated;\n+    size_t garbage;\n+    size_t reclaimed;\n+  } _at_relocate_start;\n+\n+  static struct XAtRelocateEnd {\n+    size_t capacity;\n+    size_t capacity_high;\n+    size_t capacity_low;\n+    size_t free;\n+    size_t free_high;\n+    size_t free_low;\n+    size_t used;\n+    size_t used_high;\n+    size_t used_low;\n+    size_t allocated;\n+    size_t garbage;\n+    size_t reclaimed;\n+  } _at_relocate_end;\n+\n+  static size_t capacity_high();\n+  static size_t capacity_low();\n+  static size_t free(size_t used);\n+  static size_t allocated(size_t used, size_t reclaimed);\n+  static size_t garbage(size_t reclaimed);\n+\n+public:\n+  static void set_at_initialize(const XPageAllocatorStats& stats);\n+  static void set_at_mark_start(const XPageAllocatorStats& stats);\n+  static void set_at_mark_end(const XPageAllocatorStats& stats);\n+  static void set_at_select_relocation_set(const XRelocationSetSelectorStats& stats);\n+  static void set_at_relocate_start(const XPageAllocatorStats& stats);\n+  static void set_at_relocate_end(const XPageAllocatorStats& stats, size_t non_worker_relocated);\n+\n+  static size_t max_capacity();\n+  static size_t used_at_mark_start();\n+  static size_t used_at_relocate_end();\n+\n+  static void print();\n+};\n+\n+#endif \/\/ SHARE_GC_X_XSTAT_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xStat.hpp","additions":578,"deletions":0,"binary":false,"changes":578,"status":"added"},{"patch":"@@ -0,0 +1,47 @@\n+\/*\n+ * Copyright (c) 2017, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/x\/xTask.hpp\"\n+#include \"gc\/x\/xThread.hpp\"\n+\n+XTask::Task::Task(XTask* task, const char* name) :\n+    WorkerTask(name),\n+    _task(task) {}\n+\n+void XTask::Task::work(uint worker_id) {\n+  XThread::set_worker_id(worker_id);\n+  _task->work();\n+  XThread::clear_worker_id();\n+}\n+\n+XTask::XTask(const char* name) :\n+    _worker_task(this, name) {}\n+\n+const char* XTask::name() const {\n+  return _worker_task.name();\n+}\n+\n+WorkerTask* XTask::worker_task() {\n+  return &_worker_task;\n+}\n","filename":"src\/hotspot\/share\/gc\/x\/xTask.cpp","additions":47,"deletions":0,"binary":false,"changes":47,"status":"added"},{"patch":"@@ -0,0 +1,53 @@\n+\/*\n+ * Copyright (c) 2017, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XTASK_HPP\n+#define SHARE_GC_X_XTASK_HPP\n+\n+#include \"gc\/shared\/workerThread.hpp\"\n+#include \"memory\/allocation.hpp\"\n+\n+class XTask : public StackObj {\n+private:\n+  class Task : public WorkerTask {\n+  private:\n+    XTask* const _task;\n+\n+  public:\n+    Task(XTask* task, const char* name);\n+\n+    virtual void work(uint worker_id);\n+  };\n+\n+  Task _worker_task;\n+\n+public:\n+  XTask(const char* name);\n+\n+  const char* name() const;\n+  WorkerTask* worker_task();\n+\n+  virtual void work() = 0;\n+};\n+\n+#endif \/\/ SHARE_GC_X_XTASK_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xTask.hpp","additions":53,"deletions":0,"binary":false,"changes":53,"status":"added"},{"patch":"@@ -0,0 +1,80 @@\n+\/*\n+ * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/x\/xThread.inline.hpp\"\n+#include \"runtime\/javaThread.hpp\"\n+#include \"runtime\/nonJavaThread.hpp\"\n+#include \"utilities\/debug.hpp\"\n+\n+THREAD_LOCAL bool      XThread::_initialized;\n+THREAD_LOCAL uintptr_t XThread::_id;\n+THREAD_LOCAL bool      XThread::_is_vm;\n+THREAD_LOCAL bool      XThread::_is_java;\n+THREAD_LOCAL bool      XThread::_is_worker;\n+THREAD_LOCAL uint      XThread::_worker_id;\n+\n+void XThread::initialize() {\n+  assert(!_initialized, \"Already initialized\");\n+  const Thread* const thread = Thread::current();\n+  _initialized = true;\n+  _id = (uintptr_t)thread;\n+  _is_vm = thread->is_VM_thread();\n+  _is_java = thread->is_Java_thread();\n+  _is_worker = false;\n+  _worker_id = (uint)-1;\n+}\n+\n+const char* XThread::name() {\n+  const Thread* const thread = Thread::current();\n+  if (thread->is_Named_thread()) {\n+    const NamedThread* const named = (const NamedThread*)thread;\n+    return named->name();\n+  } else if (thread->is_Java_thread()) {\n+    return \"Java\";\n+  }\n+\n+  return \"Unknown\";\n+}\n+\n+void XThread::set_worker() {\n+  ensure_initialized();\n+  _is_worker = true;\n+}\n+\n+bool XThread::has_worker_id() {\n+  return _initialized &&\n+         _is_worker &&\n+         _worker_id != (uint)-1;\n+}\n+\n+void XThread::set_worker_id(uint worker_id) {\n+  ensure_initialized();\n+  assert(!has_worker_id(), \"Worker id already initialized\");\n+  _worker_id = worker_id;\n+}\n+\n+void XThread::clear_worker_id() {\n+  assert(has_worker_id(), \"Worker id not initialized\");\n+  _worker_id = (uint)-1;\n+}\n","filename":"src\/hotspot\/share\/gc\/x\/xThread.cpp","additions":80,"deletions":0,"binary":false,"changes":80,"status":"added"},{"patch":"@@ -0,0 +1,61 @@\n+\/*\n+ * Copyright (c) 2015, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XTHREAD_HPP\n+#define SHARE_GC_X_XTHREAD_HPP\n+\n+#include \"memory\/allStatic.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+class XThread : public AllStatic {\n+  friend class XTask;\n+  friend class XWorkersInitializeTask;\n+  friend class XRuntimeWorkersInitializeTask;\n+\n+private:\n+  static THREAD_LOCAL bool      _initialized;\n+  static THREAD_LOCAL uintptr_t _id;\n+  static THREAD_LOCAL bool      _is_vm;\n+  static THREAD_LOCAL bool      _is_java;\n+  static THREAD_LOCAL bool      _is_worker;\n+  static THREAD_LOCAL uint      _worker_id;\n+\n+  static void initialize();\n+  static void ensure_initialized();\n+\n+  static void set_worker();\n+\n+  static bool has_worker_id();\n+  static void set_worker_id(uint worker_id);\n+  static void clear_worker_id();\n+\n+public:\n+  static const char* name();\n+  static uintptr_t id();\n+  static bool is_vm();\n+  static bool is_java();\n+  static bool is_worker();\n+  static uint worker_id();\n+};\n+\n+#endif \/\/ SHARE_GC_X_XTHREAD_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xThread.hpp","additions":61,"deletions":0,"binary":false,"changes":61,"status":"added"},{"patch":"@@ -0,0 +1,62 @@\n+\/*\n+ * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XTHREAD_INLINE_HPP\n+#define SHARE_GC_X_XTHREAD_INLINE_HPP\n+\n+#include \"gc\/x\/xThread.hpp\"\n+\n+#include \"utilities\/debug.hpp\"\n+\n+inline void XThread::ensure_initialized() {\n+  if (!_initialized) {\n+    initialize();\n+  }\n+}\n+\n+inline uintptr_t XThread::id() {\n+  ensure_initialized();\n+  return _id;\n+}\n+\n+inline bool XThread::is_vm() {\n+  ensure_initialized();\n+  return _is_vm;\n+}\n+\n+inline bool XThread::is_java() {\n+  ensure_initialized();\n+  return _is_java;\n+}\n+\n+inline bool XThread::is_worker() {\n+  ensure_initialized();\n+  return _is_worker;\n+}\n+\n+inline uint XThread::worker_id() {\n+  assert(has_worker_id(), \"Worker id not initialized\");\n+  return _worker_id;\n+}\n+\n+#endif \/\/ SHARE_GC_X_XTHREAD_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xThread.inline.hpp","additions":62,"deletions":0,"binary":false,"changes":62,"status":"added"},{"patch":"@@ -0,0 +1,92 @@\n+\/*\n+ * Copyright (c) 2018, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/shared\/tlab_globals.hpp\"\n+#include \"gc\/x\/xAddress.inline.hpp\"\n+#include \"gc\/x\/xStackWatermark.hpp\"\n+#include \"gc\/x\/xThreadLocalAllocBuffer.hpp\"\n+#include \"gc\/x\/xValue.inline.hpp\"\n+#include \"runtime\/globals.hpp\"\n+#include \"runtime\/javaThread.hpp\"\n+#include \"runtime\/stackWatermarkSet.inline.hpp\"\n+\n+XPerWorker<ThreadLocalAllocStats>* XThreadLocalAllocBuffer::_stats = NULL;\n+\n+void XThreadLocalAllocBuffer::initialize() {\n+  if (UseTLAB) {\n+    assert(_stats == NULL, \"Already initialized\");\n+    _stats = new XPerWorker<ThreadLocalAllocStats>();\n+    reset_statistics();\n+  }\n+}\n+\n+void XThreadLocalAllocBuffer::reset_statistics() {\n+  if (UseTLAB) {\n+    XPerWorkerIterator<ThreadLocalAllocStats> iter(_stats);\n+    for (ThreadLocalAllocStats* stats; iter.next(&stats);) {\n+      stats->reset();\n+    }\n+  }\n+}\n+\n+void XThreadLocalAllocBuffer::publish_statistics() {\n+  if (UseTLAB) {\n+    ThreadLocalAllocStats total;\n+\n+    XPerWorkerIterator<ThreadLocalAllocStats> iter(_stats);\n+    for (ThreadLocalAllocStats* stats; iter.next(&stats);) {\n+      total.update(*stats);\n+    }\n+\n+    total.publish();\n+  }\n+}\n+\n+static void fixup_address(HeapWord** p) {\n+  *p = (HeapWord*)XAddress::good_or_null((uintptr_t)*p);\n+}\n+\n+void XThreadLocalAllocBuffer::retire(JavaThread* thread, ThreadLocalAllocStats* stats) {\n+  if (UseTLAB) {\n+    stats->reset();\n+    thread->tlab().addresses_do(fixup_address);\n+    thread->tlab().retire(stats);\n+    if (ResizeTLAB) {\n+      thread->tlab().resize();\n+    }\n+  }\n+}\n+\n+void XThreadLocalAllocBuffer::remap(JavaThread* thread) {\n+  if (UseTLAB) {\n+    thread->tlab().addresses_do(fixup_address);\n+  }\n+}\n+\n+void XThreadLocalAllocBuffer::update_stats(JavaThread* thread) {\n+  if (UseTLAB) {\n+    XStackWatermark* const watermark = StackWatermarkSet::get<XStackWatermark>(thread, StackWatermarkKind::gc);\n+    _stats->addr()->update(watermark->stats());\n+  }\n+}\n","filename":"src\/hotspot\/share\/gc\/x\/xThreadLocalAllocBuffer.cpp","additions":92,"deletions":0,"binary":false,"changes":92,"status":"added"},{"patch":"@@ -0,0 +1,48 @@\n+\/*\n+ * Copyright (c) 2018, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XTHREADLOCALALLOCBUFFER_HPP\n+#define SHARE_GC_X_XTHREADLOCALALLOCBUFFER_HPP\n+\n+#include \"gc\/shared\/threadLocalAllocBuffer.hpp\"\n+#include \"gc\/x\/xValue.hpp\"\n+#include \"memory\/allStatic.hpp\"\n+\n+class JavaThread;\n+\n+class XThreadLocalAllocBuffer : public AllStatic {\n+private:\n+  static XPerWorker<ThreadLocalAllocStats>* _stats;\n+\n+public:\n+  static void initialize();\n+\n+  static void reset_statistics();\n+  static void publish_statistics();\n+\n+  static void retire(JavaThread* thread, ThreadLocalAllocStats* stats);\n+  static void remap(JavaThread* thread);\n+  static void update_stats(JavaThread* thread);\n+};\n+\n+#endif \/\/ SHARE_GC_X_XTHREADLOCALALLOCBUFFER_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xThreadLocalAllocBuffer.hpp","additions":48,"deletions":0,"binary":false,"changes":48,"status":"added"},{"patch":"@@ -0,0 +1,91 @@\n+\/*\n+ * Copyright (c) 2018, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XTHREADLOCALDATA_HPP\n+#define SHARE_GC_X_XTHREADLOCALDATA_HPP\n+\n+#include \"gc\/x\/xMarkStack.hpp\"\n+#include \"gc\/x\/xGlobals.hpp\"\n+#include \"runtime\/javaThread.hpp\"\n+#include \"utilities\/debug.hpp\"\n+#include \"utilities\/sizes.hpp\"\n+\n+class XThreadLocalData {\n+private:\n+  uintptr_t              _address_bad_mask;\n+  XMarkThreadLocalStacks _stacks;\n+  oop*                   _invisible_root;\n+\n+  XThreadLocalData() :\n+      _address_bad_mask(0),\n+      _stacks(),\n+      _invisible_root(NULL) {}\n+\n+  static XThreadLocalData* data(Thread* thread) {\n+    return thread->gc_data<XThreadLocalData>();\n+  }\n+\n+public:\n+  static void create(Thread* thread) {\n+    new (data(thread)) XThreadLocalData();\n+  }\n+\n+  static void destroy(Thread* thread) {\n+    data(thread)->~XThreadLocalData();\n+  }\n+\n+  static void set_address_bad_mask(Thread* thread, uintptr_t mask) {\n+    data(thread)->_address_bad_mask = mask;\n+  }\n+\n+  static XMarkThreadLocalStacks* stacks(Thread* thread) {\n+    return &data(thread)->_stacks;\n+  }\n+\n+  static void set_invisible_root(Thread* thread, oop* root) {\n+    assert(data(thread)->_invisible_root == NULL, \"Already set\");\n+    data(thread)->_invisible_root = root;\n+  }\n+\n+  static void clear_invisible_root(Thread* thread) {\n+    assert(data(thread)->_invisible_root != NULL, \"Should be set\");\n+    data(thread)->_invisible_root = NULL;\n+  }\n+\n+  template <typename T>\n+  static void do_invisible_root(Thread* thread, T f) {\n+    if (data(thread)->_invisible_root != NULL) {\n+      f(data(thread)->_invisible_root);\n+    }\n+  }\n+\n+  static ByteSize address_bad_mask_offset() {\n+    return Thread::gc_data_offset() + byte_offset_of(XThreadLocalData, _address_bad_mask);\n+  }\n+\n+  static ByteSize nmethod_disarmed_offset() {\n+    return address_bad_mask_offset() + in_ByteSize(XAddressBadMaskHighOrderBitsOffset);\n+  }\n+};\n+\n+#endif \/\/ SHARE_GC_X_XTHREADLOCALDATA_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xThreadLocalData.hpp","additions":91,"deletions":0,"binary":false,"changes":91,"status":"added"},{"patch":"@@ -0,0 +1,146 @@\n+\/*\n+ * Copyright (c) 2016, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/shared\/gcId.hpp\"\n+#include \"gc\/x\/xGlobals.hpp\"\n+#include \"gc\/x\/xStat.hpp\"\n+#include \"gc\/x\/xTracer.hpp\"\n+#include \"jfr\/jfrEvents.hpp\"\n+#include \"runtime\/safepointVerifiers.hpp\"\n+#include \"utilities\/debug.hpp\"\n+#include \"utilities\/macros.hpp\"\n+#if INCLUDE_JFR\n+#include \"jfr\/metadata\/jfrSerializer.hpp\"\n+#endif\n+\n+#if INCLUDE_JFR\n+\n+class XPageTypeConstant : public JfrSerializer {\n+public:\n+  virtual void serialize(JfrCheckpointWriter& writer) {\n+    writer.write_count(3);\n+    writer.write_key(XPageTypeSmall);\n+    writer.write(\"Small\");\n+    writer.write_key(XPageTypeMedium);\n+    writer.write(\"Medium\");\n+    writer.write_key(XPageTypeLarge);\n+    writer.write(\"Large\");\n+  }\n+};\n+\n+class XStatisticsCounterTypeConstant : public JfrSerializer {\n+public:\n+  virtual void serialize(JfrCheckpointWriter& writer) {\n+    writer.write_count(XStatCounter::count());\n+    for (XStatCounter* counter = XStatCounter::first(); counter != NULL; counter = counter->next()) {\n+      writer.write_key(counter->id());\n+      writer.write(counter->name());\n+    }\n+  }\n+};\n+\n+class XStatisticsSamplerTypeConstant : public JfrSerializer {\n+public:\n+  virtual void serialize(JfrCheckpointWriter& writer) {\n+    writer.write_count(XStatSampler::count());\n+    for (XStatSampler* sampler = XStatSampler::first(); sampler != NULL; sampler = sampler->next()) {\n+      writer.write_key(sampler->id());\n+      writer.write(sampler->name());\n+    }\n+  }\n+};\n+\n+static void register_jfr_type_serializers() {\n+  JfrSerializer::register_serializer(TYPE_ZPAGETYPETYPE,\n+                                     true \/* permit_cache *\/,\n+                                     new XPageTypeConstant());\n+  JfrSerializer::register_serializer(TYPE_ZSTATISTICSCOUNTERTYPE,\n+                                     true \/* permit_cache *\/,\n+                                     new XStatisticsCounterTypeConstant());\n+  JfrSerializer::register_serializer(TYPE_ZSTATISTICSSAMPLERTYPE,\n+                                     true \/* permit_cache *\/,\n+                                     new XStatisticsSamplerTypeConstant());\n+}\n+\n+#endif \/\/ INCLUDE_JFR\n+\n+XTracer* XTracer::_tracer = NULL;\n+\n+XTracer::XTracer() :\n+    GCTracer(Z) {}\n+\n+void XTracer::initialize() {\n+  assert(_tracer == NULL, \"Already initialized\");\n+  _tracer = new XTracer();\n+  JFR_ONLY(register_jfr_type_serializers());\n+}\n+\n+void XTracer::send_stat_counter(const XStatCounter& counter, uint64_t increment, uint64_t value) {\n+  NoSafepointVerifier nsv;\n+\n+  EventZStatisticsCounter e;\n+  if (e.should_commit()) {\n+    e.set_id(counter.id());\n+    e.set_increment(increment);\n+    e.set_value(value);\n+    e.commit();\n+  }\n+}\n+\n+void XTracer::send_stat_sampler(const XStatSampler& sampler, uint64_t value) {\n+  NoSafepointVerifier nsv;\n+\n+  EventZStatisticsSampler e;\n+  if (e.should_commit()) {\n+    e.set_id(sampler.id());\n+    e.set_value(value);\n+    e.commit();\n+  }\n+}\n+\n+void XTracer::send_thread_phase(const char* name, const Ticks& start, const Ticks& end) {\n+  NoSafepointVerifier nsv;\n+\n+  EventZThreadPhase e(UNTIMED);\n+  if (e.should_commit()) {\n+    e.set_gcId(GCId::current_or_undefined());\n+    e.set_name(name);\n+    e.set_starttime(start);\n+    e.set_endtime(end);\n+    e.commit();\n+  }\n+}\n+\n+void XTracer::send_thread_debug(const char* name, const Ticks& start, const Ticks& end) {\n+  NoSafepointVerifier nsv;\n+\n+  EventZThreadDebug e(UNTIMED);\n+  if (e.should_commit()) {\n+    e.set_gcId(GCId::current_or_undefined());\n+    e.set_name(name);\n+    e.set_starttime(start);\n+    e.set_endtime(end);\n+    e.commit();\n+  }\n+}\n","filename":"src\/hotspot\/share\/gc\/x\/xTracer.cpp","additions":146,"deletions":0,"binary":false,"changes":146,"status":"added"},{"patch":"@@ -0,0 +1,65 @@\n+\/*\n+ * Copyright (c) 2016, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XTRACER_HPP\n+#define SHARE_GC_X_XTRACER_HPP\n+\n+#include \"gc\/shared\/gcTrace.hpp\"\n+\n+class XStatCounter;\n+class XStatPhase;\n+class XStatSampler;\n+\n+class XTracer : public GCTracer, public CHeapObj<mtGC> {\n+private:\n+  static XTracer* _tracer;\n+\n+  XTracer();\n+\n+  void send_stat_counter(const XStatCounter& counter, uint64_t increment, uint64_t value);\n+  void send_stat_sampler(const XStatSampler& sampler, uint64_t value);\n+  void send_thread_phase(const char* name, const Ticks& start, const Ticks& end);\n+  void send_thread_debug(const char* name, const Ticks& start, const Ticks& end);\n+\n+public:\n+  static XTracer* tracer();\n+  static void initialize();\n+\n+  void report_stat_counter(const XStatCounter& counter, uint64_t increment, uint64_t value);\n+  void report_stat_sampler(const XStatSampler& sampler, uint64_t value);\n+  void report_thread_phase(const char* name, const Ticks& start, const Ticks& end);\n+  void report_thread_debug(const char* name, const Ticks& start, const Ticks& end);\n+};\n+\n+\/\/ For temporary latency measurements during development and debugging\n+class XTraceThreadDebug : public StackObj {\n+private:\n+  const Ticks       _start;\n+  const char* const _name;\n+\n+public:\n+  XTraceThreadDebug(const char* name);\n+  ~XTraceThreadDebug();\n+};\n+\n+#endif \/\/ SHARE_GC_X_XTRACER_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xTracer.hpp","additions":65,"deletions":0,"binary":false,"changes":65,"status":"added"},{"patch":"@@ -0,0 +1,67 @@\n+\/*\n+ * Copyright (c) 2016, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XTRACER_INLINE_HPP\n+#define SHARE_GC_X_XTRACER_INLINE_HPP\n+\n+#include \"gc\/x\/xTracer.hpp\"\n+\n+#include \"jfr\/jfrEvents.hpp\"\n+\n+inline XTracer* XTracer::tracer() {\n+  return _tracer;\n+}\n+\n+inline void XTracer::report_stat_counter(const XStatCounter& counter, uint64_t increment, uint64_t value) {\n+  if (EventZStatisticsCounter::is_enabled()) {\n+    send_stat_counter(counter, increment, value);\n+  }\n+}\n+\n+inline void XTracer::report_stat_sampler(const XStatSampler& sampler, uint64_t value) {\n+  if (EventZStatisticsSampler::is_enabled()) {\n+    send_stat_sampler(sampler, value);\n+  }\n+}\n+\n+inline void XTracer::report_thread_phase(const char* name, const Ticks& start, const Ticks& end) {\n+  if (EventZThreadPhase::is_enabled()) {\n+    send_thread_phase(name, start, end);\n+  }\n+}\n+\n+inline void XTracer::report_thread_debug(const char* name, const Ticks& start, const Ticks& end) {\n+  if (EventZThreadDebug::is_enabled()) {\n+    send_thread_debug(name, start, end);\n+  }\n+}\n+\n+inline XTraceThreadDebug::XTraceThreadDebug(const char* name) :\n+    _start(Ticks::now()),\n+    _name(name) {}\n+\n+inline XTraceThreadDebug::~XTraceThreadDebug() {\n+  XTracer::tracer()->report_thread_debug(_name, _start, Ticks::now());\n+}\n+\n+#endif \/\/ SHARE_GC_X_XTRACER_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xTracer.inline.hpp","additions":67,"deletions":0,"binary":false,"changes":67,"status":"added"},{"patch":"@@ -0,0 +1,96 @@\n+\/*\n+ * Copyright (c) 2019, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/shared\/gc_globals.hpp\"\n+#include \"gc\/x\/xHeap.inline.hpp\"\n+#include \"gc\/x\/xLock.inline.hpp\"\n+#include \"gc\/x\/xStat.hpp\"\n+#include \"gc\/x\/xUncommitter.hpp\"\n+#include \"jfr\/jfrEvents.hpp\"\n+#include \"logging\/log.hpp\"\n+\n+static const XStatCounter XCounterUncommit(\"Memory\", \"Uncommit\", XStatUnitBytesPerSecond);\n+\n+XUncommitter::XUncommitter(XPageAllocator* page_allocator) :\n+    _page_allocator(page_allocator),\n+    _lock(),\n+    _stop(false) {\n+  set_name(\"XUncommitter\");\n+  create_and_start();\n+}\n+\n+bool XUncommitter::wait(uint64_t timeout) const {\n+  XLocker<XConditionLock> locker(&_lock);\n+  while (!ZUncommit && !_stop) {\n+    _lock.wait();\n+  }\n+\n+  if (!_stop && timeout > 0) {\n+    log_debug(gc, heap)(\"Uncommit Timeout: \" UINT64_FORMAT \"s\", timeout);\n+    _lock.wait(timeout * MILLIUNITS);\n+  }\n+\n+  return !_stop;\n+}\n+\n+bool XUncommitter::should_continue() const {\n+  XLocker<XConditionLock> locker(&_lock);\n+  return !_stop;\n+}\n+\n+void XUncommitter::run_service() {\n+  uint64_t timeout = 0;\n+\n+  while (wait(timeout)) {\n+    EventZUncommit event;\n+    size_t uncommitted = 0;\n+\n+    while (should_continue()) {\n+      \/\/ Uncommit chunk\n+      const size_t flushed = _page_allocator->uncommit(&timeout);\n+      if (flushed == 0) {\n+        \/\/ Done\n+        break;\n+      }\n+\n+      uncommitted += flushed;\n+    }\n+\n+    if (uncommitted > 0) {\n+      \/\/ Update statistics\n+      XStatInc(XCounterUncommit, uncommitted);\n+      log_info(gc, heap)(\"Uncommitted: \" SIZE_FORMAT \"M(%.0f%%)\",\n+                         uncommitted \/ M, percent_of(uncommitted, XHeap::heap()->max_capacity()));\n+\n+      \/\/ Send event\n+      event.commit(uncommitted);\n+    }\n+  }\n+}\n+\n+void XUncommitter::stop_service() {\n+  XLocker<XConditionLock> locker(&_lock);\n+  _stop = true;\n+  _lock.notify_all();\n+}\n","filename":"src\/hotspot\/share\/gc\/x\/xUncommitter.cpp","additions":96,"deletions":0,"binary":false,"changes":96,"status":"added"},{"patch":"@@ -0,0 +1,49 @@\n+\/*\n+ * Copyright (c) 2019, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XUNCOMMITTER_HPP\n+#define SHARE_GC_X_XUNCOMMITTER_HPP\n+\n+#include \"gc\/shared\/concurrentGCThread.hpp\"\n+#include \"gc\/x\/xLock.hpp\"\n+\n+class XPageAllocation;\n+\n+class XUncommitter : public ConcurrentGCThread {\n+private:\n+  XPageAllocator* const  _page_allocator;\n+  mutable XConditionLock _lock;\n+  bool                   _stop;\n+\n+  bool wait(uint64_t timeout) const;\n+  bool should_continue() const;\n+\n+protected:\n+  virtual void run_service();\n+  virtual void stop_service();\n+\n+public:\n+  XUncommitter(XPageAllocator* page_allocator);\n+};\n+\n+#endif \/\/ SHARE_GC_X_XUNCOMMITTER_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xUncommitter.hpp","additions":49,"deletions":0,"binary":false,"changes":49,"status":"added"},{"patch":"@@ -0,0 +1,176 @@\n+\/*\n+ * Copyright (c) 2018, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"classfile\/classLoaderDataGraph.hpp\"\n+#include \"classfile\/systemDictionary.hpp\"\n+#include \"code\/codeBehaviours.hpp\"\n+#include \"code\/codeCache.hpp\"\n+#include \"code\/dependencyContext.hpp\"\n+#include \"gc\/shared\/gcBehaviours.hpp\"\n+#include \"gc\/shared\/suspendibleThreadSet.hpp\"\n+#include \"gc\/x\/xBarrier.inline.hpp\"\n+#include \"gc\/x\/xLock.inline.hpp\"\n+#include \"gc\/x\/xNMethod.hpp\"\n+#include \"gc\/x\/xStat.hpp\"\n+#include \"gc\/x\/xUnload.hpp\"\n+#include \"memory\/metaspaceUtils.hpp\"\n+#include \"oops\/access.inline.hpp\"\n+\n+static const XStatSubPhase XSubPhaseConcurrentClassesUnlink(\"Concurrent Classes Unlink\");\n+static const XStatSubPhase XSubPhaseConcurrentClassesPurge(\"Concurrent Classes Purge\");\n+\n+class XPhantomIsAliveObjectClosure : public BoolObjectClosure {\n+public:\n+  virtual bool do_object_b(oop o) {\n+    return XBarrier::is_alive_barrier_on_phantom_oop(o);\n+  }\n+};\n+\n+class XIsUnloadingOopClosure : public OopClosure {\n+private:\n+  XPhantomIsAliveObjectClosure _is_alive;\n+  bool                         _is_unloading;\n+\n+public:\n+  XIsUnloadingOopClosure() :\n+      _is_alive(),\n+      _is_unloading(false) {}\n+\n+  virtual void do_oop(oop* p) {\n+    const oop o = RawAccess<>::oop_load(p);\n+    if (o != NULL && !_is_alive.do_object_b(o)) {\n+      _is_unloading = true;\n+    }\n+  }\n+\n+  virtual void do_oop(narrowOop* p) {\n+    ShouldNotReachHere();\n+  }\n+\n+  bool is_unloading() const {\n+    return _is_unloading;\n+  }\n+};\n+\n+class XIsUnloadingBehaviour : public IsUnloadingBehaviour {\n+public:\n+  virtual bool has_dead_oop(CompiledMethod* method) const {\n+    nmethod* const nm = method->as_nmethod();\n+    XReentrantLock* const lock = XNMethod::lock_for_nmethod(nm);\n+    XLocker<XReentrantLock> locker(lock);\n+    XIsUnloadingOopClosure cl;\n+    XNMethod::nmethod_oops_do_inner(nm, &cl);\n+    return cl.is_unloading();\n+  }\n+};\n+\n+class XCompiledICProtectionBehaviour : public CompiledICProtectionBehaviour {\n+public:\n+  virtual bool lock(CompiledMethod* method) {\n+    nmethod* const nm = method->as_nmethod();\n+    XReentrantLock* const lock = XNMethod::lock_for_nmethod(nm);\n+    lock->lock();\n+    return true;\n+  }\n+\n+  virtual void unlock(CompiledMethod* method) {\n+    nmethod* const nm = method->as_nmethod();\n+    XReentrantLock* const lock = XNMethod::lock_for_nmethod(nm);\n+    lock->unlock();\n+  }\n+\n+  virtual bool is_safe(CompiledMethod* method) {\n+    if (SafepointSynchronize::is_at_safepoint()) {\n+      return true;\n+    }\n+\n+    nmethod* const nm = method->as_nmethod();\n+    XReentrantLock* const lock = XNMethod::lock_for_nmethod(nm);\n+    return lock->is_owned();\n+  }\n+};\n+\n+XUnload::XUnload(XWorkers* workers) :\n+    _workers(workers) {\n+\n+  if (!ClassUnloading) {\n+    return;\n+  }\n+\n+  static XIsUnloadingBehaviour is_unloading_behaviour;\n+  IsUnloadingBehaviour::set_current(&is_unloading_behaviour);\n+\n+  static XCompiledICProtectionBehaviour ic_protection_behaviour;\n+  CompiledICProtectionBehaviour::set_current(&ic_protection_behaviour);\n+}\n+\n+void XUnload::prepare() {\n+  if (!ClassUnloading) {\n+    return;\n+  }\n+\n+  CodeCache::increment_unloading_cycle();\n+  DependencyContext::cleaning_start();\n+}\n+\n+void XUnload::unlink() {\n+  if (!ClassUnloading) {\n+    return;\n+  }\n+\n+  XStatTimer timer(XSubPhaseConcurrentClassesUnlink);\n+  SuspendibleThreadSetJoiner sts;\n+  bool unloading_occurred;\n+\n+  {\n+    MutexLocker ml(ClassLoaderDataGraph_lock);\n+    unloading_occurred = SystemDictionary::do_unloading(XStatPhase::timer());\n+  }\n+\n+  Klass::clean_weak_klass_links(unloading_occurred);\n+  XNMethod::unlink(_workers, unloading_occurred);\n+  DependencyContext::cleaning_end();\n+}\n+\n+void XUnload::purge() {\n+  if (!ClassUnloading) {\n+    return;\n+  }\n+\n+  XStatTimer timer(XSubPhaseConcurrentClassesPurge);\n+\n+  {\n+    SuspendibleThreadSetJoiner sts;\n+    XNMethod::purge();\n+  }\n+\n+  ClassLoaderDataGraph::purge(\/*at_safepoint*\/false);\n+  CodeCache::purge_exception_caches();\n+}\n+\n+void XUnload::finish() {\n+  \/\/ Resize and verify metaspace\n+  MetaspaceGC::compute_new_size();\n+  DEBUG_ONLY(MetaspaceUtils::verify();)\n+}\n","filename":"src\/hotspot\/share\/gc\/x\/xUnload.cpp","additions":176,"deletions":0,"binary":false,"changes":176,"status":"added"},{"patch":"@@ -0,0 +1,42 @@\n+\/*\n+ * Copyright (c) 2018, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XUNLOAD_HPP\n+#define SHARE_GC_X_XUNLOAD_HPP\n+\n+class XWorkers;\n+\n+class XUnload {\n+private:\n+  XWorkers* const _workers;\n+\n+public:\n+  XUnload(XWorkers* workers);\n+\n+  void prepare();\n+  void unlink();\n+  void purge();\n+  void finish();\n+};\n+\n+#endif \/\/ SHARE_GC_X_XUNLOAD_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xUnload.hpp","additions":42,"deletions":0,"binary":false,"changes":42,"status":"added"},{"patch":"@@ -0,0 +1,101 @@\n+\/*\n+ * Copyright (c) 2020, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/shared\/gc_globals.hpp\"\n+#include \"gc\/x\/xList.inline.hpp\"\n+#include \"gc\/x\/xLock.inline.hpp\"\n+#include \"gc\/x\/xPage.inline.hpp\"\n+#include \"gc\/x\/xPageAllocator.hpp\"\n+#include \"gc\/x\/xUnmapper.hpp\"\n+#include \"jfr\/jfrEvents.hpp\"\n+#include \"runtime\/globals.hpp\"\n+\n+XUnmapper::XUnmapper(XPageAllocator* page_allocator) :\n+    _page_allocator(page_allocator),\n+    _lock(),\n+    _queue(),\n+    _stop(false) {\n+  set_name(\"XUnmapper\");\n+  create_and_start();\n+}\n+\n+XPage* XUnmapper::dequeue() {\n+  XLocker<XConditionLock> locker(&_lock);\n+\n+  for (;;) {\n+    if (_stop) {\n+      return NULL;\n+    }\n+\n+    XPage* const page = _queue.remove_first();\n+    if (page != NULL) {\n+      return page;\n+    }\n+\n+    _lock.wait();\n+  }\n+}\n+\n+void XUnmapper::do_unmap_and_destroy_page(XPage* page) const {\n+  EventZUnmap event;\n+  const size_t unmapped = page->size();\n+\n+  \/\/ Unmap and destroy\n+  _page_allocator->unmap_page(page);\n+  _page_allocator->destroy_page(page);\n+\n+  \/\/ Send event\n+  event.commit(unmapped);\n+}\n+\n+void XUnmapper::unmap_and_destroy_page(XPage* page) {\n+  \/\/ Asynchronous unmap and destroy is not supported with ZVerifyViews\n+  if (ZVerifyViews) {\n+    \/\/ Immediately unmap and destroy\n+    do_unmap_and_destroy_page(page);\n+  } else {\n+    \/\/ Enqueue for asynchronous unmap and destroy\n+    XLocker<XConditionLock> locker(&_lock);\n+    _queue.insert_last(page);\n+    _lock.notify_all();\n+  }\n+}\n+\n+void XUnmapper::run_service() {\n+  for (;;) {\n+    XPage* const page = dequeue();\n+    if (page == NULL) {\n+      \/\/ Stop\n+      return;\n+    }\n+\n+    do_unmap_and_destroy_page(page);\n+  }\n+}\n+\n+void XUnmapper::stop_service() {\n+  XLocker<XConditionLock> locker(&_lock);\n+  _stop = true;\n+  _lock.notify_all();\n+}\n","filename":"src\/hotspot\/share\/gc\/x\/xUnmapper.cpp","additions":101,"deletions":0,"binary":false,"changes":101,"status":"added"},{"patch":"@@ -0,0 +1,54 @@\n+\/*\n+ * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XUNMAPPER_HPP\n+#define SHARE_GC_X_XUNMAPPER_HPP\n+\n+#include \"gc\/shared\/concurrentGCThread.hpp\"\n+#include \"gc\/x\/xList.hpp\"\n+#include \"gc\/x\/xLock.hpp\"\n+\n+class XPage;\n+class XPageAllocator;\n+\n+class XUnmapper : public ConcurrentGCThread {\n+private:\n+  XPageAllocator* const _page_allocator;\n+  XConditionLock        _lock;\n+  XList<XPage>          _queue;\n+  bool                  _stop;\n+\n+  XPage* dequeue();\n+  void do_unmap_and_destroy_page(XPage* page) const;\n+\n+protected:\n+  virtual void run_service();\n+  virtual void stop_service();\n+\n+public:\n+  XUnmapper(XPageAllocator* page_allocator);\n+\n+  void unmap_and_destroy_page(XPage* page);\n+};\n+\n+#endif \/\/ SHARE_GC_X_XUNMAPPER_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xUnmapper.hpp","additions":54,"deletions":0,"binary":false,"changes":54,"status":"added"},{"patch":"@@ -24,2 +24,2 @@\n-#ifndef SHARE_GC_Z_ZUTILS_HPP\n-#define SHARE_GC_Z_ZUTILS_HPP\n+#ifndef SHARE_GC_X_XUTILS_HPP\n+#define SHARE_GC_X_XUTILS_HPP\n@@ -30,1 +30,1 @@\n-class ZUtils : public AllStatic {\n+class XUtils : public AllStatic {\n@@ -45,1 +45,1 @@\n-#endif \/\/ SHARE_GC_Z_ZUTILS_HPP\n+#endif \/\/ SHARE_GC_X_XUTILS_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xUtils.hpp","additions":4,"deletions":4,"binary":false,"changes":8,"previous_filename":"src\/hotspot\/share\/gc\/z\/zUtils.hpp","status":"copied"},{"patch":"@@ -0,0 +1,59 @@\n+\/*\n+ * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XUTILS_INLINE_HPP\n+#define SHARE_GC_X_XUTILS_INLINE_HPP\n+\n+#include \"gc\/x\/xUtils.hpp\"\n+\n+#include \"gc\/x\/xOop.inline.hpp\"\n+#include \"oops\/oop.inline.hpp\"\n+#include \"utilities\/align.hpp\"\n+#include \"utilities\/copy.hpp\"\n+#include \"utilities\/debug.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+inline size_t XUtils::bytes_to_words(size_t size_in_bytes) {\n+  assert(is_aligned(size_in_bytes, BytesPerWord), \"Size not word aligned\");\n+  return size_in_bytes >> LogBytesPerWord;\n+}\n+\n+inline size_t XUtils::words_to_bytes(size_t size_in_words) {\n+  return size_in_words << LogBytesPerWord;\n+}\n+\n+inline size_t XUtils::object_size(uintptr_t addr) {\n+  return words_to_bytes(XOop::from_address(addr)->size());\n+}\n+\n+inline void XUtils::object_copy_disjoint(uintptr_t from, uintptr_t to, size_t size) {\n+  Copy::aligned_disjoint_words((HeapWord*)from, (HeapWord*)to, bytes_to_words(size));\n+}\n+\n+inline void XUtils::object_copy_conjoint(uintptr_t from, uintptr_t to, size_t size) {\n+  if (from != to) {\n+    Copy::aligned_conjoint_words((HeapWord*)from, (HeapWord*)to, bytes_to_words(size));\n+  }\n+}\n+\n+#endif \/\/ SHARE_GC_X_XUTILS_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xUtils.inline.hpp","additions":59,"deletions":0,"binary":false,"changes":59,"status":"added"},{"patch":"@@ -0,0 +1,140 @@\n+\/*\n+ * Copyright (c) 2015, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XVALUE_HPP\n+#define SHARE_GC_X_XVALUE_HPP\n+\n+#include \"memory\/allStatic.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+\/\/\n+\/\/ Storage\n+\/\/\n+\n+template <typename S>\n+class XValueStorage : public AllStatic {\n+private:\n+  static uintptr_t _top;\n+  static uintptr_t _end;\n+\n+public:\n+  static const size_t offset = 4 * K;\n+\n+  static uintptr_t alloc(size_t size);\n+};\n+\n+class XContendedStorage : public XValueStorage<XContendedStorage> {\n+public:\n+  static size_t alignment();\n+  static uint32_t count();\n+  static uint32_t id();\n+};\n+\n+class XPerCPUStorage : public XValueStorage<XPerCPUStorage> {\n+public:\n+  static size_t alignment();\n+  static uint32_t count();\n+  static uint32_t id();\n+};\n+\n+class XPerNUMAStorage : public XValueStorage<XPerNUMAStorage> {\n+public:\n+  static size_t alignment();\n+  static uint32_t count();\n+  static uint32_t id();\n+};\n+\n+class XPerWorkerStorage : public XValueStorage<XPerWorkerStorage> {\n+public:\n+  static size_t alignment();\n+  static uint32_t count();\n+  static uint32_t id();\n+};\n+\n+\/\/\n+\/\/ Value\n+\/\/\n+\n+template <typename S, typename T>\n+class XValue : public CHeapObj<mtGC> {\n+private:\n+  const uintptr_t _addr;\n+\n+  uintptr_t value_addr(uint32_t value_id) const;\n+\n+public:\n+  XValue();\n+  XValue(const T& value);\n+\n+  const T* addr(uint32_t value_id = S::id()) const;\n+  T* addr(uint32_t value_id = S::id());\n+\n+  const T& get(uint32_t value_id = S::id()) const;\n+  T& get(uint32_t value_id = S::id());\n+\n+  void set(const T& value, uint32_t value_id = S::id());\n+  void set_all(const T& value);\n+};\n+\n+template <typename T> using XContended = XValue<XContendedStorage, T>;\n+template <typename T> using XPerCPU = XValue<XPerCPUStorage, T>;\n+template <typename T> using XPerNUMA = XValue<XPerNUMAStorage, T>;\n+template <typename T> using XPerWorker = XValue<XPerWorkerStorage, T>;\n+\n+\/\/\n+\/\/ Iterator\n+\/\/\n+\n+template <typename S, typename T>\n+class XValueIterator {\n+private:\n+  XValue<S, T>* const _value;\n+  uint32_t            _value_id;\n+\n+public:\n+  XValueIterator(XValue<S, T>* value);\n+\n+  bool next(T** value);\n+};\n+\n+template <typename T> using XPerCPUIterator = XValueIterator<XPerCPUStorage, T>;\n+template <typename T> using XPerNUMAIterator = XValueIterator<XPerNUMAStorage, T>;\n+template <typename T> using XPerWorkerIterator = XValueIterator<XPerWorkerStorage, T>;\n+\n+template <typename S, typename T>\n+class XValueConstIterator {\n+private:\n+  const XValue<S, T>* const _value;\n+  uint32_t                  _value_id;\n+\n+public:\n+  XValueConstIterator(const XValue<S, T>* value);\n+\n+  bool next(const T** value);\n+};\n+\n+template <typename T> using XPerCPUConstIterator = XValueConstIterator<XPerCPUStorage, T>;\n+template <typename T> using XPerNUMAConstIterator = XValueConstIterator<XPerNUMAStorage, T>;\n+template <typename T> using XPerWorkerConstIterator = XValueConstIterator<XPerWorkerStorage, T>;\n+\n+#endif \/\/ SHARE_GC_X_XVALUE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xValue.hpp","additions":140,"deletions":0,"binary":false,"changes":140,"status":"added"},{"patch":"@@ -0,0 +1,210 @@\n+\/*\n+ * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XVALUE_INLINE_HPP\n+#define SHARE_GC_X_XVALUE_INLINE_HPP\n+\n+#include \"gc\/x\/xValue.hpp\"\n+\n+#include \"gc\/shared\/gc_globals.hpp\"\n+#include \"gc\/x\/xCPU.inline.hpp\"\n+#include \"gc\/x\/xGlobals.hpp\"\n+#include \"gc\/x\/xNUMA.hpp\"\n+#include \"gc\/x\/xThread.inline.hpp\"\n+#include \"gc\/x\/xUtils.hpp\"\n+#include \"runtime\/globals.hpp\"\n+#include \"utilities\/align.hpp\"\n+\n+\/\/\n+\/\/ Storage\n+\/\/\n+\n+template <typename T> uintptr_t XValueStorage<T>::_end = 0;\n+template <typename T> uintptr_t XValueStorage<T>::_top = 0;\n+\n+template <typename S>\n+uintptr_t XValueStorage<S>::alloc(size_t size) {\n+  assert(size <= offset, \"Allocation too large\");\n+\n+  \/\/ Allocate entry in existing memory block\n+  const uintptr_t addr = align_up(_top, S::alignment());\n+  _top = addr + size;\n+\n+  if (_top < _end) {\n+    \/\/ Success\n+    return addr;\n+  }\n+\n+  \/\/ Allocate new block of memory\n+  const size_t block_alignment = offset;\n+  const size_t block_size = offset * S::count();\n+  _top = XUtils::alloc_aligned(block_alignment, block_size);\n+  _end = _top + offset;\n+\n+  \/\/ Retry allocation\n+  return alloc(size);\n+}\n+\n+inline size_t XContendedStorage::alignment() {\n+  return XCacheLineSize;\n+}\n+\n+inline uint32_t XContendedStorage::count() {\n+  return 1;\n+}\n+\n+inline uint32_t XContendedStorage::id() {\n+  return 0;\n+}\n+\n+inline size_t XPerCPUStorage::alignment() {\n+  return sizeof(uintptr_t);\n+}\n+\n+inline uint32_t XPerCPUStorage::count() {\n+  return XCPU::count();\n+}\n+\n+inline uint32_t XPerCPUStorage::id() {\n+  return XCPU::id();\n+}\n+\n+inline size_t XPerNUMAStorage::alignment() {\n+  return sizeof(uintptr_t);\n+}\n+\n+inline uint32_t XPerNUMAStorage::count() {\n+  return XNUMA::count();\n+}\n+\n+inline uint32_t XPerNUMAStorage::id() {\n+  return XNUMA::id();\n+}\n+\n+inline size_t XPerWorkerStorage::alignment() {\n+  return sizeof(uintptr_t);\n+}\n+\n+inline uint32_t XPerWorkerStorage::count() {\n+  return UseDynamicNumberOfGCThreads ? ConcGCThreads : MAX2(ConcGCThreads, ParallelGCThreads);\n+}\n+\n+inline uint32_t XPerWorkerStorage::id() {\n+  return XThread::worker_id();\n+}\n+\n+\/\/\n+\/\/ Value\n+\/\/\n+\n+template <typename S, typename T>\n+inline uintptr_t XValue<S, T>::value_addr(uint32_t value_id) const {\n+  return _addr + (value_id * S::offset);\n+}\n+\n+template <typename S, typename T>\n+inline XValue<S, T>::XValue() :\n+    _addr(S::alloc(sizeof(T))) {\n+  \/\/ Initialize all instances\n+  XValueIterator<S, T> iter(this);\n+  for (T* addr; iter.next(&addr);) {\n+    ::new (addr) T;\n+  }\n+}\n+\n+template <typename S, typename T>\n+inline XValue<S, T>::XValue(const T& value) :\n+    _addr(S::alloc(sizeof(T))) {\n+  \/\/ Initialize all instances\n+  XValueIterator<S, T> iter(this);\n+  for (T* addr; iter.next(&addr);) {\n+    ::new (addr) T(value);\n+  }\n+}\n+\n+template <typename S, typename T>\n+inline const T* XValue<S, T>::addr(uint32_t value_id) const {\n+  return reinterpret_cast<const T*>(value_addr(value_id));\n+}\n+\n+template <typename S, typename T>\n+inline T* XValue<S, T>::addr(uint32_t value_id) {\n+  return reinterpret_cast<T*>(value_addr(value_id));\n+}\n+\n+template <typename S, typename T>\n+inline const T& XValue<S, T>::get(uint32_t value_id) const {\n+  return *addr(value_id);\n+}\n+\n+template <typename S, typename T>\n+inline T& XValue<S, T>::get(uint32_t value_id) {\n+  return *addr(value_id);\n+}\n+\n+template <typename S, typename T>\n+inline void XValue<S, T>::set(const T& value, uint32_t value_id) {\n+  get(value_id) = value;\n+}\n+\n+template <typename S, typename T>\n+inline void XValue<S, T>::set_all(const T& value) {\n+  XValueIterator<S, T> iter(this);\n+  for (T* addr; iter.next(&addr);) {\n+    *addr = value;\n+  }\n+}\n+\n+\/\/\n+\/\/ Iterator\n+\/\/\n+\n+template <typename S, typename T>\n+inline XValueIterator<S, T>::XValueIterator(XValue<S, T>* value) :\n+    _value(value),\n+    _value_id(0) {}\n+\n+template <typename S, typename T>\n+inline bool XValueIterator<S, T>::next(T** value) {\n+  if (_value_id < S::count()) {\n+    *value = _value->addr(_value_id++);\n+    return true;\n+  }\n+  return false;\n+}\n+\n+template <typename S, typename T>\n+inline XValueConstIterator<S, T>::XValueConstIterator(const XValue<S, T>* value) :\n+    _value(value),\n+    _value_id(0) {}\n+\n+template <typename S, typename T>\n+inline bool XValueConstIterator<S, T>::next(const T** value) {\n+  if (_value_id < S::count()) {\n+    *value = _value->addr(_value_id++);\n+    return true;\n+  }\n+  return false;\n+}\n+\n+#endif \/\/ SHARE_GC_X_XVALUE_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xValue.inline.hpp","additions":210,"deletions":0,"binary":false,"changes":210,"status":"added"},{"patch":"@@ -0,0 +1,415 @@\n+\/*\n+ * Copyright (c) 2019, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"classfile\/classLoaderData.hpp\"\n+#include \"gc\/shared\/gc_globals.hpp\"\n+#include \"gc\/x\/xAddress.inline.hpp\"\n+#include \"gc\/x\/xHeap.inline.hpp\"\n+#include \"gc\/x\/xNMethod.hpp\"\n+#include \"gc\/x\/xOop.hpp\"\n+#include \"gc\/x\/xPageAllocator.hpp\"\n+#include \"gc\/x\/xResurrection.hpp\"\n+#include \"gc\/x\/xRootsIterator.hpp\"\n+#include \"gc\/x\/xStackWatermark.hpp\"\n+#include \"gc\/x\/xStat.hpp\"\n+#include \"gc\/x\/xVerify.hpp\"\n+#include \"memory\/iterator.inline.hpp\"\n+#include \"memory\/resourceArea.hpp\"\n+#include \"oops\/oop.hpp\"\n+#include \"runtime\/frame.inline.hpp\"\n+#include \"runtime\/globals.hpp\"\n+#include \"runtime\/handles.hpp\"\n+#include \"runtime\/javaThread.hpp\"\n+#include \"runtime\/safepoint.hpp\"\n+#include \"runtime\/stackFrameStream.inline.hpp\"\n+#include \"runtime\/stackWatermark.inline.hpp\"\n+#include \"runtime\/stackWatermarkSet.inline.hpp\"\n+#include \"utilities\/debug.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include \"utilities\/preserveException.hpp\"\n+\n+#define BAD_OOP_ARG(o, p)   \"Bad oop \" PTR_FORMAT \" found at \" PTR_FORMAT, p2i(o), p2i(p)\n+\n+static void z_verify_oop(oop* p) {\n+  const oop o = RawAccess<>::oop_load(p);\n+  if (o != NULL) {\n+    const uintptr_t addr = XOop::to_address(o);\n+    guarantee(XAddress::is_good(addr), BAD_OOP_ARG(o, p));\n+    guarantee(oopDesc::is_oop(XOop::from_address(addr)), BAD_OOP_ARG(o, p));\n+  }\n+}\n+\n+static void z_verify_possibly_weak_oop(oop* p) {\n+  const oop o = RawAccess<>::oop_load(p);\n+  if (o != NULL) {\n+    const uintptr_t addr = XOop::to_address(o);\n+    guarantee(XAddress::is_good(addr) || XAddress::is_finalizable_good(addr), BAD_OOP_ARG(o, p));\n+    guarantee(oopDesc::is_oop(XOop::from_address(XAddress::good(addr))), BAD_OOP_ARG(o, p));\n+  }\n+}\n+\n+class XVerifyRootClosure : public OopClosure {\n+private:\n+  const bool _verify_fixed;\n+\n+public:\n+  XVerifyRootClosure(bool verify_fixed) :\n+      _verify_fixed(verify_fixed) {}\n+\n+  virtual void do_oop(oop* p) {\n+    if (_verify_fixed) {\n+      z_verify_oop(p);\n+    } else {\n+      \/\/ Don't know the state of the oop.\n+      oop obj = *p;\n+      obj = NativeAccess<AS_NO_KEEPALIVE>::oop_load(&obj);\n+      z_verify_oop(&obj);\n+    }\n+  }\n+\n+  virtual void do_oop(narrowOop*) {\n+    ShouldNotReachHere();\n+  }\n+\n+  bool verify_fixed() const {\n+    return _verify_fixed;\n+  }\n+};\n+\n+class XVerifyCodeBlobClosure : public CodeBlobToOopClosure {\n+public:\n+  XVerifyCodeBlobClosure(XVerifyRootClosure* _cl) :\n+      CodeBlobToOopClosure(_cl, false \/* fix_relocations *\/) {}\n+\n+  virtual void do_code_blob(CodeBlob* cb) {\n+    CodeBlobToOopClosure::do_code_blob(cb);\n+  }\n+};\n+\n+class XVerifyStack : public OopClosure {\n+private:\n+  XVerifyRootClosure* const _cl;\n+  JavaThread*         const _jt;\n+  uint64_t                  _last_good;\n+  bool                      _verifying_bad_frames;\n+\n+public:\n+  XVerifyStack(XVerifyRootClosure* cl, JavaThread* jt) :\n+      _cl(cl),\n+      _jt(jt),\n+      _last_good(0),\n+      _verifying_bad_frames(false) {\n+    XStackWatermark* const stack_watermark = StackWatermarkSet::get<XStackWatermark>(jt, StackWatermarkKind::gc);\n+\n+    if (_cl->verify_fixed()) {\n+      assert(stack_watermark->processing_started(), \"Should already have been fixed\");\n+      assert(stack_watermark->processing_completed(), \"Should already have been fixed\");\n+    } else {\n+      \/\/ We don't really know the state of the stack, verify watermark.\n+      if (!stack_watermark->processing_started()) {\n+        _verifying_bad_frames = true;\n+      } else {\n+        \/\/ Not time yet to verify bad frames\n+        _last_good = stack_watermark->last_processed();\n+      }\n+    }\n+  }\n+\n+  void do_oop(oop* p) {\n+    if (_verifying_bad_frames) {\n+      const oop obj = *p;\n+      guarantee(!XAddress::is_good(XOop::to_address(obj)), BAD_OOP_ARG(obj, p));\n+    }\n+    _cl->do_oop(p);\n+  }\n+\n+  void do_oop(narrowOop* p) {\n+    ShouldNotReachHere();\n+  }\n+\n+  void prepare_next_frame(frame& frame) {\n+    if (_cl->verify_fixed()) {\n+      \/\/ All frames need to be good\n+      return;\n+    }\n+\n+    \/\/ The verification has two modes, depending on whether we have reached the\n+    \/\/ last processed frame or not. Before it is reached, we expect everything to\n+    \/\/ be good. After reaching it, we expect everything to be bad.\n+    const uintptr_t sp = reinterpret_cast<uintptr_t>(frame.sp());\n+\n+    if (!_verifying_bad_frames && sp == _last_good) {\n+      \/\/ Found the last good frame, now verify the bad ones\n+      _verifying_bad_frames = true;\n+    }\n+  }\n+\n+  void verify_frames() {\n+    XVerifyCodeBlobClosure cb_cl(_cl);\n+    for (StackFrameStream frames(_jt, true \/* update *\/, false \/* process_frames *\/);\n+         !frames.is_done();\n+         frames.next()) {\n+      frame& frame = *frames.current();\n+      frame.oops_do(this, &cb_cl, frames.register_map(), DerivedPointerIterationMode::_ignore);\n+      prepare_next_frame(frame);\n+    }\n+  }\n+};\n+\n+class XVerifyOopClosure : public ClaimMetadataVisitingOopIterateClosure {\n+private:\n+  const bool _verify_weaks;\n+\n+public:\n+  XVerifyOopClosure(bool verify_weaks) :\n+      ClaimMetadataVisitingOopIterateClosure(ClassLoaderData::_claim_other),\n+      _verify_weaks(verify_weaks) {}\n+\n+  virtual void do_oop(oop* p) {\n+    if (_verify_weaks) {\n+      z_verify_possibly_weak_oop(p);\n+    } else {\n+      \/\/ We should never encounter finalizable oops through strong\n+      \/\/ paths. This assumes we have only visited strong roots.\n+      z_verify_oop(p);\n+    }\n+  }\n+\n+  virtual void do_oop(narrowOop* p) {\n+    ShouldNotReachHere();\n+  }\n+\n+  virtual ReferenceIterationMode reference_iteration_mode() {\n+    return _verify_weaks ? DO_FIELDS : DO_FIELDS_EXCEPT_REFERENT;\n+  }\n+\n+  \/\/ Don't follow this metadata when verifying oops\n+  virtual void do_method(Method* m) {}\n+  virtual void do_nmethod(nmethod* nm) {}\n+};\n+\n+typedef ClaimingCLDToOopClosure<ClassLoaderData::_claim_none> XVerifyCLDClosure;\n+\n+class XVerifyThreadClosure : public ThreadClosure {\n+private:\n+  XVerifyRootClosure* const _cl;\n+\n+public:\n+  XVerifyThreadClosure(XVerifyRootClosure* cl) :\n+      _cl(cl) {}\n+\n+  virtual void do_thread(Thread* thread) {\n+    thread->oops_do_no_frames(_cl, NULL);\n+\n+    JavaThread* const jt = JavaThread::cast(thread);\n+    if (!jt->has_last_Java_frame()) {\n+      return;\n+    }\n+\n+    XVerifyStack verify_stack(_cl, jt);\n+    verify_stack.verify_frames();\n+  }\n+};\n+\n+class XVerifyNMethodClosure : public NMethodClosure {\n+private:\n+  OopClosure* const        _cl;\n+  BarrierSetNMethod* const _bs_nm;\n+  const bool               _verify_fixed;\n+\n+  bool trust_nmethod_state() const {\n+    \/\/ The root iterator will visit non-processed\n+    \/\/ nmethods class unloading is turned off.\n+    return ClassUnloading || _verify_fixed;\n+  }\n+\n+public:\n+  XVerifyNMethodClosure(OopClosure* cl, bool verify_fixed) :\n+      _cl(cl),\n+      _bs_nm(BarrierSet::barrier_set()->barrier_set_nmethod()),\n+      _verify_fixed(verify_fixed) {}\n+\n+  virtual void do_nmethod(nmethod* nm) {\n+    assert(!trust_nmethod_state() || !_bs_nm->is_armed(nm), \"Should not encounter any armed nmethods\");\n+\n+    XNMethod::nmethod_oops_do(nm, _cl);\n+  }\n+};\n+\n+void XVerify::roots_strong(bool verify_fixed) {\n+  assert(SafepointSynchronize::is_at_safepoint(), \"Must be at a safepoint\");\n+  assert(!XResurrection::is_blocked(), \"Invalid phase\");\n+\n+  XVerifyRootClosure cl(verify_fixed);\n+  XVerifyCLDClosure cld_cl(&cl);\n+  XVerifyThreadClosure thread_cl(&cl);\n+  XVerifyNMethodClosure nm_cl(&cl, verify_fixed);\n+\n+  XRootsIterator iter(ClassLoaderData::_claim_none);\n+  iter.apply(&cl,\n+             &cld_cl,\n+             &thread_cl,\n+             &nm_cl);\n+}\n+\n+void XVerify::roots_weak() {\n+  assert(SafepointSynchronize::is_at_safepoint(), \"Must be at a safepoint\");\n+  assert(!XResurrection::is_blocked(), \"Invalid phase\");\n+\n+  XVerifyRootClosure cl(true \/* verify_fixed *\/);\n+  XWeakRootsIterator iter;\n+  iter.apply(&cl);\n+}\n+\n+void XVerify::objects(bool verify_weaks) {\n+  assert(SafepointSynchronize::is_at_safepoint(), \"Must be at a safepoint\");\n+  assert(XGlobalPhase == XPhaseMarkCompleted, \"Invalid phase\");\n+  assert(!XResurrection::is_blocked(), \"Invalid phase\");\n+\n+  XVerifyOopClosure cl(verify_weaks);\n+  ObjectToOopClosure object_cl(&cl);\n+  XHeap::heap()->object_iterate(&object_cl, verify_weaks);\n+}\n+\n+void XVerify::before_zoperation() {\n+  \/\/ Verify strong roots\n+  XStatTimerDisable disable;\n+  if (ZVerifyRoots) {\n+    roots_strong(false \/* verify_fixed *\/);\n+  }\n+}\n+\n+void XVerify::after_mark() {\n+  \/\/ Verify all strong roots and strong references\n+  XStatTimerDisable disable;\n+  if (ZVerifyRoots) {\n+    roots_strong(true \/* verify_fixed *\/);\n+  }\n+  if (ZVerifyObjects) {\n+    objects(false \/* verify_weaks *\/);\n+  }\n+}\n+\n+void XVerify::after_weak_processing() {\n+  \/\/ Verify all roots and all references\n+  XStatTimerDisable disable;\n+  if (ZVerifyRoots) {\n+    roots_strong(true \/* verify_fixed *\/);\n+    roots_weak();\n+  }\n+  if (ZVerifyObjects) {\n+    objects(true \/* verify_weaks *\/);\n+  }\n+}\n+\n+template <bool Map>\n+class XPageDebugMapOrUnmapClosure : public XPageClosure {\n+private:\n+  const XPageAllocator* const _allocator;\n+\n+public:\n+  XPageDebugMapOrUnmapClosure(const XPageAllocator* allocator) :\n+      _allocator(allocator) {}\n+\n+  void do_page(const XPage* page) {\n+    if (Map) {\n+      _allocator->debug_map_page(page);\n+    } else {\n+      _allocator->debug_unmap_page(page);\n+    }\n+  }\n+};\n+\n+XVerifyViewsFlip::XVerifyViewsFlip(const XPageAllocator* allocator) :\n+    _allocator(allocator) {\n+  if (ZVerifyViews) {\n+    \/\/ Unmap all pages\n+    XPageDebugMapOrUnmapClosure<false \/* Map *\/> cl(_allocator);\n+    XHeap::heap()->pages_do(&cl);\n+  }\n+}\n+\n+XVerifyViewsFlip::~XVerifyViewsFlip() {\n+  if (ZVerifyViews) {\n+    \/\/ Map all pages\n+    XPageDebugMapOrUnmapClosure<true \/* Map *\/> cl(_allocator);\n+    XHeap::heap()->pages_do(&cl);\n+  }\n+}\n+\n+#ifdef ASSERT\n+\n+class XVerifyBadOopClosure : public OopClosure {\n+public:\n+  virtual void do_oop(oop* p) {\n+    const oop o = *p;\n+    assert(!XAddress::is_good(XOop::to_address(o)), \"Should not be good: \" PTR_FORMAT, p2i(o));\n+  }\n+\n+  virtual void do_oop(narrowOop* p) {\n+    ShouldNotReachHere();\n+  }\n+};\n+\n+\/\/ This class encapsulates various marks we need to deal with calling the\n+\/\/ frame iteration code from arbitrary points in the runtime. It is mostly\n+\/\/ due to problems that we might want to eventually clean up inside of the\n+\/\/ frame iteration code, such as creating random handles even though there\n+\/\/ is no safepoint to protect against, and fiddling around with exceptions.\n+class StackWatermarkProcessingMark {\n+  ResetNoHandleMark     _rnhm;\n+  HandleMark            _hm;\n+  PreserveExceptionMark _pem;\n+  ResourceMark          _rm;\n+\n+public:\n+  StackWatermarkProcessingMark(Thread* thread) :\n+      _rnhm(),\n+      _hm(thread),\n+      _pem(thread),\n+      _rm(thread) {}\n+};\n+\n+void XVerify::verify_frame_bad(const frame& fr, RegisterMap& register_map) {\n+  XVerifyBadOopClosure verify_cl;\n+  fr.oops_do(&verify_cl, NULL, &register_map, DerivedPointerIterationMode::_ignore);\n+}\n+\n+void XVerify::verify_thread_head_bad(JavaThread* jt) {\n+  XVerifyBadOopClosure verify_cl;\n+  jt->oops_do_no_frames(&verify_cl, NULL);\n+}\n+\n+void XVerify::verify_thread_frames_bad(JavaThread* jt) {\n+  if (jt->has_last_Java_frame()) {\n+    XVerifyBadOopClosure verify_cl;\n+    StackWatermarkProcessingMark swpm(Thread::current());\n+    \/\/ Traverse the execution stack\n+    for (StackFrameStream fst(jt, true \/* update *\/, false \/* process_frames *\/); !fst.is_done(); fst.next()) {\n+      fst.current()->oops_do(&verify_cl, NULL \/* code_cl *\/, fst.register_map(), DerivedPointerIterationMode::_ignore);\n+    }\n+  }\n+}\n+\n+#endif \/\/ ASSERT\n","filename":"src\/hotspot\/share\/gc\/x\/xVerify.cpp","additions":415,"deletions":0,"binary":false,"changes":415,"status":"added"},{"patch":"@@ -0,0 +1,58 @@\n+\/*\n+ * Copyright (c) 2019, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XVERIFY_HPP\n+#define SHARE_GC_X_XVERIFY_HPP\n+\n+#include \"memory\/allStatic.hpp\"\n+\n+class frame;\n+class XPageAllocator;\n+\n+class XVerify : public AllStatic {\n+private:\n+  static void roots_strong(bool verify_fixed);\n+  static void roots_weak();\n+\n+  static void objects(bool verify_weaks);\n+\n+public:\n+  static void before_zoperation();\n+  static void after_mark();\n+  static void after_weak_processing();\n+\n+  static void verify_thread_head_bad(JavaThread* thread) NOT_DEBUG_RETURN;\n+  static void verify_thread_frames_bad(JavaThread* thread) NOT_DEBUG_RETURN;\n+  static void verify_frame_bad(const frame& fr, RegisterMap& register_map) NOT_DEBUG_RETURN;\n+};\n+\n+class XVerifyViewsFlip {\n+private:\n+  const XPageAllocator* const _allocator;\n+\n+public:\n+  XVerifyViewsFlip(const XPageAllocator* allocator);\n+  ~XVerifyViewsFlip();\n+};\n+\n+#endif \/\/ SHARE_GC_X_XVERIFY_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xVerify.hpp","additions":58,"deletions":0,"binary":false,"changes":58,"status":"added"},{"patch":"@@ -0,0 +1,208 @@\n+\/*\n+ * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/shared\/gcLogPrecious.hpp\"\n+#include \"gc\/x\/xAddress.inline.hpp\"\n+#include \"gc\/x\/xAddressSpaceLimit.hpp\"\n+#include \"gc\/x\/xGlobals.hpp\"\n+#include \"gc\/x\/xVirtualMemory.inline.hpp\"\n+#include \"services\/memTracker.hpp\"\n+#include \"utilities\/align.hpp\"\n+#include \"utilities\/debug.hpp\"\n+\n+XVirtualMemoryManager::XVirtualMemoryManager(size_t max_capacity) :\n+    _manager(),\n+    _reserved(0),\n+    _initialized(false) {\n+\n+  \/\/ Check max supported heap size\n+  if (max_capacity > XAddressOffsetMax) {\n+    log_error_p(gc)(\"Java heap too large (max supported heap size is \" SIZE_FORMAT \"G)\",\n+                    XAddressOffsetMax \/ G);\n+    return;\n+  }\n+\n+  \/\/ Initialize platform specific parts before reserving address space\n+  pd_initialize_before_reserve();\n+\n+  \/\/ Reserve address space\n+  if (!reserve(max_capacity)) {\n+    log_error_pd(gc)(\"Failed to reserve enough address space for Java heap\");\n+    return;\n+  }\n+\n+  \/\/ Initialize platform specific parts after reserving address space\n+  pd_initialize_after_reserve();\n+\n+  \/\/ Successfully initialized\n+  _initialized = true;\n+}\n+\n+size_t XVirtualMemoryManager::reserve_discontiguous(uintptr_t start, size_t size, size_t min_range) {\n+  if (size < min_range) {\n+    \/\/ Too small\n+    return 0;\n+  }\n+\n+  assert(is_aligned(size, XGranuleSize), \"Misaligned\");\n+\n+  if (reserve_contiguous(start, size)) {\n+    return size;\n+  }\n+\n+  const size_t half = size \/ 2;\n+  if (half < min_range) {\n+    \/\/ Too small\n+    return 0;\n+  }\n+\n+  \/\/ Divide and conquer\n+  const size_t first_part = align_down(half, XGranuleSize);\n+  const size_t second_part = size - first_part;\n+  return reserve_discontiguous(start, first_part, min_range) +\n+         reserve_discontiguous(start + first_part, second_part, min_range);\n+}\n+\n+size_t XVirtualMemoryManager::reserve_discontiguous(size_t size) {\n+  \/\/ Don't try to reserve address ranges smaller than 1% of the requested size.\n+  \/\/ This avoids an explosion of reservation attempts in case large parts of the\n+  \/\/ address space is already occupied.\n+  const size_t min_range = align_up(size \/ 100, XGranuleSize);\n+  size_t start = 0;\n+  size_t reserved = 0;\n+\n+  \/\/ Reserve size somewhere between [0, XAddressOffsetMax)\n+  while (reserved < size && start < XAddressOffsetMax) {\n+    const size_t remaining = MIN2(size - reserved, XAddressOffsetMax - start);\n+    reserved += reserve_discontiguous(start, remaining, min_range);\n+    start += remaining;\n+  }\n+\n+  return reserved;\n+}\n+\n+bool XVirtualMemoryManager::reserve_contiguous(uintptr_t start, size_t size) {\n+  assert(is_aligned(size, XGranuleSize), \"Must be granule aligned\");\n+\n+  \/\/ Reserve address views\n+  const uintptr_t marked0 = XAddress::marked0(start);\n+  const uintptr_t marked1 = XAddress::marked1(start);\n+  const uintptr_t remapped = XAddress::remapped(start);\n+\n+  \/\/ Reserve address space\n+  if (!pd_reserve(marked0, size)) {\n+    return false;\n+  }\n+\n+  if (!pd_reserve(marked1, size)) {\n+    pd_unreserve(marked0, size);\n+    return false;\n+  }\n+\n+  if (!pd_reserve(remapped, size)) {\n+    pd_unreserve(marked0, size);\n+    pd_unreserve(marked1, size);\n+    return false;\n+  }\n+\n+  \/\/ Register address views with native memory tracker\n+  nmt_reserve(marked0, size);\n+  nmt_reserve(marked1, size);\n+  nmt_reserve(remapped, size);\n+\n+  \/\/ Make the address range free\n+  _manager.free(start, size);\n+\n+  return true;\n+}\n+\n+bool XVirtualMemoryManager::reserve_contiguous(size_t size) {\n+  \/\/ Allow at most 8192 attempts spread evenly across [0, XAddressOffsetMax)\n+  const size_t unused = XAddressOffsetMax - size;\n+  const size_t increment = MAX2(align_up(unused \/ 8192, XGranuleSize), XGranuleSize);\n+\n+  for (size_t start = 0; start + size <= XAddressOffsetMax; start += increment) {\n+    if (reserve_contiguous(start, size)) {\n+      \/\/ Success\n+      return true;\n+    }\n+  }\n+\n+  \/\/ Failed\n+  return false;\n+}\n+\n+bool XVirtualMemoryManager::reserve(size_t max_capacity) {\n+  const size_t limit = MIN2(XAddressOffsetMax, XAddressSpaceLimit::heap_view());\n+  const size_t size = MIN2(max_capacity * XVirtualToPhysicalRatio, limit);\n+\n+  size_t reserved = size;\n+  bool contiguous = true;\n+\n+  \/\/ Prefer a contiguous address space\n+  if (!reserve_contiguous(size)) {\n+    \/\/ Fall back to a discontiguous address space\n+    reserved = reserve_discontiguous(size);\n+    contiguous = false;\n+  }\n+\n+  log_info_p(gc, init)(\"Address Space Type: %s\/%s\/%s\",\n+                       (contiguous ? \"Contiguous\" : \"Discontiguous\"),\n+                       (limit == XAddressOffsetMax ? \"Unrestricted\" : \"Restricted\"),\n+                       (reserved == size ? \"Complete\" : \"Degraded\"));\n+  log_info_p(gc, init)(\"Address Space Size: \" SIZE_FORMAT \"M x \" SIZE_FORMAT \" = \" SIZE_FORMAT \"M\",\n+                       reserved \/ M, XHeapViews, (reserved * XHeapViews) \/ M);\n+\n+  \/\/ Record reserved\n+  _reserved = reserved;\n+\n+  return reserved >= max_capacity;\n+}\n+\n+void XVirtualMemoryManager::nmt_reserve(uintptr_t start, size_t size) {\n+  MemTracker::record_virtual_memory_reserve((void*)start, size, CALLER_PC);\n+  MemTracker::record_virtual_memory_type((void*)start, mtJavaHeap);\n+}\n+\n+bool XVirtualMemoryManager::is_initialized() const {\n+  return _initialized;\n+}\n+\n+XVirtualMemory XVirtualMemoryManager::alloc(size_t size, bool force_low_address) {\n+  uintptr_t start;\n+\n+  \/\/ Small pages are allocated at low addresses, while medium\/large pages\n+  \/\/ are allocated at high addresses (unless forced to be at a low address).\n+  if (force_low_address || size <= XPageSizeSmall) {\n+    start = _manager.alloc_low_address(size);\n+  } else {\n+    start = _manager.alloc_high_address(size);\n+  }\n+\n+  return XVirtualMemory(start, size);\n+}\n+\n+void XVirtualMemoryManager::free(const XVirtualMemory& vmem) {\n+  _manager.free(vmem.start(), vmem.size());\n+}\n","filename":"src\/hotspot\/share\/gc\/x\/xVirtualMemory.cpp","additions":208,"deletions":0,"binary":false,"changes":208,"status":"added"},{"patch":"@@ -0,0 +1,82 @@\n+\/*\n+ * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XVIRTUALMEMORY_HPP\n+#define SHARE_GC_X_XVIRTUALMEMORY_HPP\n+\n+#include \"gc\/x\/xMemory.hpp\"\n+\n+class VMStructs;\n+\n+class XVirtualMemory {\n+  friend class ::VMStructs;\n+\n+private:\n+  uintptr_t _start;\n+  uintptr_t _end;\n+\n+public:\n+  XVirtualMemory();\n+  XVirtualMemory(uintptr_t start, size_t size);\n+\n+  bool is_null() const;\n+  uintptr_t start() const;\n+  uintptr_t end() const;\n+  size_t size() const;\n+\n+  XVirtualMemory split(size_t size);\n+};\n+\n+class XVirtualMemoryManager {\n+private:\n+  XMemoryManager _manager;\n+  uintptr_t      _reserved;\n+  bool           _initialized;\n+\n+  \/\/ Platform specific implementation\n+  void pd_initialize_before_reserve();\n+  void pd_initialize_after_reserve();\n+  bool pd_reserve(uintptr_t addr, size_t size);\n+  void pd_unreserve(uintptr_t addr, size_t size);\n+\n+  bool reserve_contiguous(uintptr_t start, size_t size);\n+  bool reserve_contiguous(size_t size);\n+  size_t reserve_discontiguous(uintptr_t start, size_t size, size_t min_range);\n+  size_t reserve_discontiguous(size_t size);\n+  bool reserve(size_t max_capacity);\n+\n+  void nmt_reserve(uintptr_t start, size_t size);\n+\n+public:\n+  XVirtualMemoryManager(size_t max_capacity);\n+\n+  bool is_initialized() const;\n+\n+  size_t reserved() const;\n+  uintptr_t lowest_available_address() const;\n+\n+  XVirtualMemory alloc(size_t size, bool force_low_address);\n+  void free(const XVirtualMemory& vmem);\n+};\n+\n+#endif \/\/ SHARE_GC_X_XVIRTUALMEMORY_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xVirtualMemory.hpp","additions":82,"deletions":0,"binary":false,"changes":82,"status":"added"},{"patch":"@@ -0,0 +1,68 @@\n+\/*\n+ * Copyright (c) 2015, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XVIRTUALMEMORY_INLINE_HPP\n+#define SHARE_GC_X_XVIRTUALMEMORY_INLINE_HPP\n+\n+#include \"gc\/x\/xVirtualMemory.hpp\"\n+\n+#include \"gc\/x\/xMemory.inline.hpp\"\n+\n+inline XVirtualMemory::XVirtualMemory() :\n+    _start(UINTPTR_MAX),\n+    _end(UINTPTR_MAX) {}\n+\n+inline XVirtualMemory::XVirtualMemory(uintptr_t start, size_t size) :\n+    _start(start),\n+    _end(start + size) {}\n+\n+inline bool XVirtualMemory::is_null() const {\n+  return _start == UINTPTR_MAX;\n+}\n+\n+inline uintptr_t XVirtualMemory::start() const {\n+  return _start;\n+}\n+\n+inline uintptr_t XVirtualMemory::end() const {\n+  return _end;\n+}\n+\n+inline size_t XVirtualMemory::size() const {\n+  return _end - _start;\n+}\n+\n+inline XVirtualMemory XVirtualMemory::split(size_t size) {\n+  _start += size;\n+  return XVirtualMemory(_start - size, size);\n+}\n+\n+inline size_t XVirtualMemoryManager::reserved() const {\n+  return _reserved;\n+}\n+\n+inline uintptr_t XVirtualMemoryManager::lowest_available_address() const {\n+  return _manager.peek_low_address();\n+}\n+\n+#endif \/\/ SHARE_GC_X_XVIRTUALMEMORY_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xVirtualMemory.inline.hpp","additions":68,"deletions":0,"binary":false,"changes":68,"status":"added"},{"patch":"@@ -0,0 +1,79 @@\n+\/*\n+ * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/x\/xBarrier.inline.hpp\"\n+#include \"gc\/x\/xRootsIterator.hpp\"\n+#include \"gc\/x\/xTask.hpp\"\n+#include \"gc\/x\/xWeakRootsProcessor.hpp\"\n+#include \"gc\/x\/xWorkers.hpp\"\n+\n+class XPhantomCleanOopClosure : public OopClosure {\n+public:\n+  virtual void do_oop(oop* p) {\n+    \/\/ Read the oop once, to make sure the liveness check\n+    \/\/ and the later clearing uses the same value.\n+    const oop obj = Atomic::load(p);\n+    if (XBarrier::is_alive_barrier_on_phantom_oop(obj)) {\n+      XBarrier::keep_alive_barrier_on_phantom_oop_field(p);\n+    } else {\n+      \/\/ The destination could have been modified\/reused, in which case\n+      \/\/ we don't want to clear it. However, no one could write the same\n+      \/\/ oop here again (the object would be strongly live and we would\n+      \/\/ not consider clearing such oops), so therefore we don't have an\n+      \/\/ ABA problem here.\n+      Atomic::cmpxchg(p, obj, oop(NULL));\n+    }\n+  }\n+\n+  virtual void do_oop(narrowOop* p) {\n+    ShouldNotReachHere();\n+  }\n+};\n+\n+XWeakRootsProcessor::XWeakRootsProcessor(XWorkers* workers) :\n+    _workers(workers) {}\n+\n+class XProcessWeakRootsTask : public XTask {\n+private:\n+  XWeakRootsIterator _weak_roots;\n+\n+public:\n+  XProcessWeakRootsTask() :\n+      XTask(\"XProcessWeakRootsTask\"),\n+      _weak_roots() {}\n+\n+  ~XProcessWeakRootsTask() {\n+    _weak_roots.report_num_dead();\n+  }\n+\n+  virtual void work() {\n+    XPhantomCleanOopClosure cl;\n+    _weak_roots.apply(&cl);\n+  }\n+};\n+\n+void XWeakRootsProcessor::process_weak_roots() {\n+  XProcessWeakRootsTask task;\n+  _workers->run(&task);\n+}\n","filename":"src\/hotspot\/share\/gc\/x\/xWeakRootsProcessor.cpp","additions":79,"deletions":0,"binary":false,"changes":79,"status":"added"},{"patch":"@@ -0,0 +1,39 @@\n+\/*\n+ * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XWEAKROOTSPROCESSOR_HPP\n+#define SHARE_GC_X_XWEAKROOTSPROCESSOR_HPP\n+\n+class XWorkers;\n+\n+class XWeakRootsProcessor {\n+private:\n+  XWorkers* const _workers;\n+\n+public:\n+  XWeakRootsProcessor(XWorkers* workers);\n+\n+  void process_weak_roots();\n+};\n+\n+#endif \/\/ SHARE_GC_X_XWEAKROOTSPROCESSOR_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xWeakRootsProcessor.hpp","additions":39,"deletions":0,"binary":false,"changes":39,"status":"added"},{"patch":"@@ -0,0 +1,117 @@\n+\/*\n+ * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/shared\/gc_globals.hpp\"\n+#include \"gc\/shared\/gcLogPrecious.hpp\"\n+#include \"gc\/x\/xLock.inline.hpp\"\n+#include \"gc\/x\/xStat.hpp\"\n+#include \"gc\/x\/xTask.hpp\"\n+#include \"gc\/x\/xThread.hpp\"\n+#include \"gc\/x\/xWorkers.hpp\"\n+#include \"runtime\/java.hpp\"\n+\n+class XWorkersInitializeTask : public WorkerTask {\n+private:\n+  const uint     _nworkers;\n+  uint           _started;\n+  XConditionLock _lock;\n+\n+public:\n+  XWorkersInitializeTask(uint nworkers) :\n+      WorkerTask(\"XWorkersInitializeTask\"),\n+      _nworkers(nworkers),\n+      _started(0),\n+      _lock() {}\n+\n+  virtual void work(uint worker_id) {\n+    \/\/ Register as worker\n+    XThread::set_worker();\n+\n+    \/\/ Wait for all threads to start\n+    XLocker<XConditionLock> locker(&_lock);\n+    if (++_started == _nworkers) {\n+      \/\/ All threads started\n+      _lock.notify_all();\n+    } else {\n+      while (_started != _nworkers) {\n+        _lock.wait();\n+      }\n+    }\n+  }\n+};\n+\n+XWorkers::XWorkers() :\n+    _workers(\"XWorker\",\n+             UseDynamicNumberOfGCThreads ? ConcGCThreads : MAX2(ConcGCThreads, ParallelGCThreads)) {\n+\n+  if (UseDynamicNumberOfGCThreads) {\n+    log_info_p(gc, init)(\"GC Workers: %u (dynamic)\", _workers.max_workers());\n+  } else {\n+    log_info_p(gc, init)(\"GC Workers: %u\/%u (static)\", ConcGCThreads, _workers.max_workers());\n+  }\n+\n+  \/\/ Initialize worker threads\n+  _workers.initialize_workers();\n+  _workers.set_active_workers(_workers.max_workers());\n+  if (_workers.active_workers() != _workers.max_workers()) {\n+    vm_exit_during_initialization(\"Failed to create XWorkers\");\n+  }\n+\n+  \/\/ Execute task to register threads as workers\n+  XWorkersInitializeTask task(_workers.max_workers());\n+  _workers.run_task(&task);\n+}\n+\n+uint XWorkers::active_workers() const {\n+  return _workers.active_workers();\n+}\n+\n+void XWorkers::set_active_workers(uint nworkers) {\n+  log_info(gc, task)(\"Using %u workers\", nworkers);\n+  _workers.set_active_workers(nworkers);\n+}\n+\n+void XWorkers::run(XTask* task) {\n+  log_debug(gc, task)(\"Executing Task: %s, Active Workers: %u\", task->name(), active_workers());\n+  XStatWorkers::at_start();\n+  _workers.run_task(task->worker_task());\n+  XStatWorkers::at_end();\n+}\n+\n+void XWorkers::run_all(XTask* task) {\n+  \/\/ Save number of active workers\n+  const uint prev_active_workers = _workers.active_workers();\n+\n+  \/\/ Execute task using all workers\n+  _workers.set_active_workers(_workers.max_workers());\n+  log_debug(gc, task)(\"Executing Task: %s, Active Workers: %u\", task->name(), active_workers());\n+  _workers.run_task(task->worker_task());\n+\n+  \/\/ Restore number of active workers\n+  _workers.set_active_workers(prev_active_workers);\n+}\n+\n+void XWorkers::threads_do(ThreadClosure* tc) const {\n+  _workers.threads_do(tc);\n+}\n","filename":"src\/hotspot\/share\/gc\/x\/xWorkers.cpp","additions":117,"deletions":0,"binary":false,"changes":117,"status":"added"},{"patch":"@@ -0,0 +1,48 @@\n+\/*\n+ * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XWORKERS_HPP\n+#define SHARE_GC_X_XWORKERS_HPP\n+\n+#include \"gc\/shared\/workerThread.hpp\"\n+\n+class ThreadClosure;\n+class XTask;\n+\n+class XWorkers {\n+private:\n+  WorkerThreads _workers;\n+\n+public:\n+  XWorkers();\n+\n+  uint active_workers() const;\n+  void set_active_workers(uint nworkers);\n+\n+  void run(XTask* task);\n+  void run_all(XTask* task);\n+\n+  void threads_do(ThreadClosure* tc) const;\n+};\n+\n+#endif \/\/ SHARE_GC_X_XWORKERS_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xWorkers.hpp","additions":48,"deletions":0,"binary":false,"changes":48,"status":"added"},{"patch":"@@ -0,0 +1,40 @@\n+\/*\n+ * Copyright (c) 2018, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_X_GLOBALS_HPP\n+#define SHARE_GC_X_X_GLOBALS_HPP\n+\n+#define GC_X_FLAGS(develop,                                          \\\n+                   develop_pd,                                       \\\n+                   product,                                          \\\n+                   product_pd,                                       \\\n+                   notproduct,                                       \\\n+                   range,                                            \\\n+                   constraint)                                       \\\n+                                                                     \\\n+  product(bool, ZVerifyViews, false, DIAGNOSTIC,                     \\\n+          \"Verify heap view accesses\")                               \\\n+                                                                     \\\n+\/\/ end of GC_X_FLAGS\n+\n+#endif \/\/ SHARE_GC_X_X_GLOBALS_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/x_globals.hpp","additions":40,"deletions":0,"binary":false,"changes":40,"status":"added"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -25,0 +25,1 @@\n+#include \"c1\/c1_FrameMap.hpp\"\n@@ -26,0 +27,1 @@\n+#include \"c1\/c1_LIRAssembler.hpp\"\n@@ -27,0 +29,1 @@\n+#include \"c1\/c1_MacroAssembler.hpp\"\n@@ -78,0 +81,1 @@\n+  visitor->do_input(_ref);\n@@ -94,1 +98,57 @@\n-class LIR_OpZLoadBarrierTest : public LIR_Op {\n+ZStoreBarrierStubC1::ZStoreBarrierStubC1(LIRAccess& access,\n+                                         LIR_Opr new_zaddress,\n+                                         LIR_Opr new_zpointer,\n+                                         LIR_Opr tmp,\n+                                         bool is_atomic,\n+                                         address runtime_stub) :\n+    _ref_addr(access.resolved_addr()),\n+    _new_zaddress(new_zaddress),\n+    _new_zpointer(new_zpointer),\n+    _tmp(tmp),\n+    _is_atomic(is_atomic),\n+    _runtime_stub(runtime_stub) {\n+  assert(_ref_addr->is_address(), \"Must be an address\");\n+}\n+\n+LIR_Opr ZStoreBarrierStubC1::ref_addr() const {\n+  return _ref_addr;\n+}\n+\n+LIR_Opr ZStoreBarrierStubC1::new_zaddress() const {\n+  return _new_zaddress;\n+}\n+\n+LIR_Opr ZStoreBarrierStubC1::new_zpointer() const {\n+  return _new_zpointer;\n+}\n+\n+LIR_Opr ZStoreBarrierStubC1::tmp() const {\n+  return _tmp;\n+}\n+\n+bool ZStoreBarrierStubC1::is_atomic() const {\n+  return _is_atomic;\n+}\n+\n+address ZStoreBarrierStubC1::runtime_stub() const {\n+  return _runtime_stub;\n+}\n+\n+void ZStoreBarrierStubC1::visit(LIR_OpVisitState* visitor) {\n+  visitor->do_slow_case();\n+  visitor->do_input(_ref_addr);\n+  visitor->do_temp(_new_zpointer);\n+  visitor->do_temp(_tmp);\n+}\n+\n+void ZStoreBarrierStubC1::emit_code(LIR_Assembler* ce) {\n+  ZBarrierSet::assembler()->generate_c1_store_barrier_stub(ce, this);\n+}\n+\n+#ifndef PRODUCT\n+void ZStoreBarrierStubC1::print_name(outputStream* out) const {\n+  out->print(\"ZStoreBarrierStubC1\");\n+}\n+#endif \/\/ PRODUCT\n+\n+class LIR_OpZUncolor : public LIR_Op {\n@@ -99,2 +159,2 @@\n-  LIR_OpZLoadBarrierTest(LIR_Opr opr) :\n-      LIR_Op(lir_zloadbarrier_test, LIR_OprFact::illegalOpr, NULL),\n+  LIR_OpZUncolor(LIR_Opr opr) :\n+      LIR_Op(),\n@@ -105,0 +165,1 @@\n+    state->do_output(_opr);\n@@ -108,1 +169,1 @@\n-    ZBarrierSet::assembler()->generate_c1_load_barrier_test(ce, _opr);\n+    ZBarrierSet::assembler()->generate_c1_uncolor(ce, _opr);\n@@ -118,1 +179,39 @@\n-    return \"lir_z_load_barrier_test\";\n+    return \"lir_z_uncolor\";\n+  }\n+#endif \/\/ PRODUCT\n+};\n+\n+class LIR_OpZLoadBarrier : public LIR_Op {\n+private:\n+  LIR_Opr                   _opr;\n+  ZLoadBarrierStubC1* const _stub;\n+  const bool                _on_non_strong;\n+\n+public:\n+  LIR_OpZLoadBarrier(LIR_Opr opr, ZLoadBarrierStubC1* stub, bool on_non_strong) :\n+      LIR_Op(),\n+      _opr(opr),\n+      _stub(stub),\n+      _on_non_strong(on_non_strong) {\n+    assert(stub != nullptr, \"The stub is the load barrier slow path.\");\n+  }\n+\n+  virtual void visit(LIR_OpVisitState* state) {\n+    state->do_input(_opr);\n+    state->do_output(_opr);\n+    state->do_stub(_stub);\n+  }\n+\n+  virtual void emit_code(LIR_Assembler* ce) {\n+    ZBarrierSet::assembler()->generate_c1_load_barrier(ce, _opr, _stub, _on_non_strong);\n+    ce->append_code_stub(_stub);\n+  }\n+\n+  virtual void print_instr(outputStream* out) const {\n+    _opr->print(out);\n+    out->print(\" \");\n+  }\n+\n+#ifndef PRODUCT\n+  virtual const char* name() const {\n+    return \"lir_z_load_barrier\";\n@@ -128,2 +227,4 @@\n-    _load_barrier_on_oop_field_preloaded_runtime_stub(NULL),\n-    _load_barrier_on_weak_oop_field_preloaded_runtime_stub(NULL) {}\n+    _load_barrier_on_oop_field_preloaded_runtime_stub(nullptr),\n+    _load_barrier_on_weak_oop_field_preloaded_runtime_stub(nullptr),\n+    _store_barrier_on_oop_field_with_healing(nullptr),\n+    _store_barrier_on_oop_field_without_healing(nullptr) {}\n@@ -142,0 +243,105 @@\n+address ZBarrierSetC1::store_barrier_on_oop_field_runtime_stub(bool self_healing) const {\n+  if (self_healing) {\n+    return _store_barrier_on_oop_field_with_healing;\n+  } else {\n+    return _store_barrier_on_oop_field_without_healing;\n+  }\n+}\n+\n+class LIR_OpZColor : public LIR_Op {\n+ friend class LIR_OpVisitState;\n+\n+private:\n+  LIR_Opr _opr;\n+\n+public:\n+  LIR_OpZColor(LIR_Opr opr) :\n+      LIR_Op(lir_none, opr, nullptr \/* info *\/),\n+      _opr(opr) {}\n+\n+  virtual void visit(LIR_OpVisitState* state) {\n+    state->do_input(_opr);\n+    state->do_output(_opr);\n+  }\n+\n+  virtual void emit_code(LIR_Assembler* ce) {\n+    ZBarrierSet::assembler()->generate_c1_color(ce, _opr);\n+  }\n+\n+  virtual void print_instr(outputStream* out) const {\n+    _opr->print(out); out->print(\" \");\n+  }\n+\n+#ifndef PRODUCT\n+  virtual const char* name() const  {\n+    return \"lir_z_color\";\n+  }\n+#endif \/\/ PRODUCT\n+};\n+\n+class LIR_OpZStoreBarrier : public LIR_Op {\n+ friend class LIR_OpVisitState;\n+\n+private:\n+  LIR_Opr       _addr;\n+  LIR_Opr       _new_zaddress;\n+  LIR_Opr       _new_zpointer;\n+  CodeStub*     _stub;\n+  CodeEmitInfo* _info;\n+\n+public:\n+  LIR_OpZStoreBarrier(LIR_Opr addr,\n+                      LIR_Opr new_zaddress,\n+                      LIR_Opr new_zpointer,\n+                      CodeStub* stub,\n+                      CodeEmitInfo* info) :\n+      LIR_Op(lir_none, new_zpointer, nullptr \/* info *\/),\n+      _addr(addr),\n+      _new_zaddress(new_zaddress),\n+      _new_zpointer(new_zpointer),\n+      _stub(stub),\n+      _info(info) {}\n+\n+  virtual void visit(LIR_OpVisitState* state) {\n+    state->do_input(_new_zaddress);\n+    state->do_input(_addr);\n+\n+    \/\/ Use temp registers to ensure these they use different registers.\n+    state->do_temp(_addr);\n+    state->do_temp(_new_zaddress);\n+\n+    state->do_output(_new_zpointer);\n+    state->do_stub(_stub);\n+\n+    if (_info != nullptr) {\n+      state->do_info(_info);\n+    }\n+  }\n+\n+  virtual void emit_code(LIR_Assembler* ce) {\n+    const ZBarrierSetAssembler* const bs_asm =\n+        (const ZBarrierSetAssembler*)BarrierSet::barrier_set()->barrier_set_assembler();\n+    if (_info != nullptr) {\n+      ce->add_debug_info_for_null_check_here(_info);\n+    }\n+    bs_asm->generate_c1_store_barrier(ce,\n+                                      _addr->as_address_ptr(),\n+                                      _new_zaddress,\n+                                      _new_zpointer,\n+                                      (ZStoreBarrierStubC1*)_stub);\n+    ce->append_code_stub(_stub);\n+  }\n+\n+  virtual void print_instr(outputStream* out) const {\n+    _addr->print(out);         out->print(\" \");\n+    _new_zaddress->print(out); out->print(\" \");\n+    _new_zpointer->print(out); out->print(\" \");\n+  }\n+\n+#ifndef PRODUCT\n+  virtual const char* name() const  {\n+    return \"lir_z_store_barrier\";\n+  }\n+#endif \/\/ PRODUCT\n+};\n+\n@@ -148,3 +354,5 @@\n-void ZBarrierSetC1::load_barrier(LIRAccess& access, LIR_Opr result) const {\n-  \/\/ Fast path\n-  __ append(new LIR_OpZLoadBarrierTest(result));\n+LIR_Opr ZBarrierSetC1::color(LIRAccess& access, LIR_Opr ref) const {\n+  \/\/ Only used from CAS where we have control over the used register\n+  assert(ref->is_single_cpu(), \"Should be using a register\");\n+\n+  __ append(new LIR_OpZColor(ref));\n@@ -152,0 +360,4 @@\n+  return ref;\n+}\n+\n+void ZBarrierSetC1::load_barrier(LIRAccess& access, LIR_Opr result) const {\n@@ -154,3 +366,40 @@\n-  CodeStub* const stub = new ZLoadBarrierStubC1(access, result, runtime_stub);\n-  __ branch(lir_cond_notEqual, stub);\n-  __ branch_destination(stub->continuation());\n+  auto stub = new ZLoadBarrierStubC1(access, result, runtime_stub);\n+\n+  const bool on_non_strong =\n+      (access.decorators() & ON_WEAK_OOP_REF) != 0 ||\n+      (access.decorators() & ON_PHANTOM_OOP_REF) != 0;\n+\n+  __ append(new LIR_OpZLoadBarrier(result, stub, on_non_strong));\n+}\n+\n+LIR_Opr ZBarrierSetC1::store_barrier(LIRAccess& access, LIR_Opr new_zaddress, bool is_atomic) const {\n+  LIRGenerator* gen = access.gen();\n+\n+  LIR_Opr new_zaddress_reg;\n+  if (new_zaddress->is_single_cpu()) {\n+    new_zaddress_reg = new_zaddress;\n+  } else if (new_zaddress->is_constant()) {\n+    new_zaddress_reg = gen->new_register(access.type());\n+    gen->lir()->move(new_zaddress, new_zaddress_reg);\n+  } else {\n+    ShouldNotReachHere();\n+  }\n+\n+  LIR_Opr new_zpointer = gen->new_register(T_OBJECT);\n+  LIR_Opr tmp = gen->new_pointer_register();\n+  ZStoreBarrierStubC1* const stub =\n+      new ZStoreBarrierStubC1(access,\n+                              new_zaddress_reg,\n+                              new_zpointer,\n+                              tmp,\n+                              is_atomic,\n+                              store_barrier_on_oop_field_runtime_stub(is_atomic));\n+\n+  __ append(new LIR_OpZStoreBarrier(access.resolved_addr(),\n+                                    new_zaddress_reg,\n+                                    new_zpointer,\n+                                    stub,\n+                                    access.access_emit_info()));\n+  access.access_emit_info() = nullptr;\n+\n+  return new_zpointer;\n@@ -167,2 +416,0 @@\n-#undef __\n-\n@@ -170,0 +417,5 @@\n+  if (!barrier_needed(access)) {\n+    BarrierSetC1::load_at_resolved(access, result);\n+    return;\n+  }\n+\n@@ -171,0 +423,2 @@\n+  load_barrier(access, result);\n+}\n@@ -172,2 +426,4 @@\n-  if (barrier_needed(access)) {\n-    load_barrier(access, result);\n+void ZBarrierSetC1::store_at_resolved(LIRAccess& access, LIR_Opr value) {\n+  if (!barrier_needed(access)) {\n+    BarrierSetC1::store_at_resolved(access, value);\n+    return;\n@@ -175,0 +431,4 @@\n+\n+  value = store_barrier(access, value, false \/* is_atomic *\/);\n+\n+  BarrierSetC1::store_at_resolved(access, value);\n@@ -177,2 +437,4 @@\n-static void pre_load_barrier(LIRAccess& access) {\n-  DecoratorSet decorators = access.decorators();\n+LIR_Opr ZBarrierSetC1::atomic_cmpxchg_at_resolved(LIRAccess& access, LIRItem& cmp_value, LIRItem& new_value) {\n+  if (!barrier_needed(access)) {\n+    return BarrierSetC1::atomic_cmpxchg_at_resolved(access, cmp_value, new_value);\n+  }\n@@ -180,2 +442,2 @@\n-  \/\/ Downgrade access to MO_UNORDERED\n-  decorators = (decorators & ~MO_DECORATOR_MASK) | MO_UNORDERED;\n+  new_value.load_item();\n+  const LIR_Opr new_value_zpointer = store_barrier(access, new_value.result(), true \/* is_atomic *\/);\n@@ -183,2 +445,3 @@\n-  \/\/ Remove ACCESS_WRITE\n-  decorators = (decorators & ~ACCESS_WRITE);\n+  cmp_value.load_item();\n+  cmp_value.set_destroys_register();\n+  color(access, cmp_value.result());\n@@ -186,8 +449,22 @@\n-  \/\/ Generate synthetic load at\n-  access.gen()->access_load_at(decorators,\n-                               access.type(),\n-                               access.base().item(),\n-                               access.offset().opr(),\n-                               access.gen()->new_register(access.type()),\n-                               NULL \/* patch_emit_info *\/,\n-                               NULL \/* load_emit_info *\/);\n+#ifdef AMD64\n+  const LIR_Opr cmp_value_opr = FrameMap::rax_oop_opr;\n+#else\n+  const LIR_Opr cmp_value_opr = access.gen()->new_register(T_OBJECT);\n+#endif\n+  access.gen()->lir()->move(cmp_value.result(), cmp_value_opr);\n+\n+  __ cas_obj(access.resolved_addr()->as_address_ptr()->base(),\n+             cmp_value_opr,\n+             new_value_zpointer,\n+#ifdef RISCV\n+             access.gen()->new_register(T_OBJECT),\n+             access.gen()->new_register(T_OBJECT),\n+             access.gen()->new_register(T_OBJECT));\n+#else\n+             LIR_OprFact::illegalOpr, LIR_OprFact::illegalOpr);\n+#endif\n+  LIR_Opr result = access.gen()->new_register(T_INT);\n+  __ cmove(lir_cond_equal, LIR_OprFact::intConst(1), LIR_OprFact::intConst(0),\n+           result, T_INT);\n+\n+  return result;\n@@ -197,2 +474,2 @@\n-  if (barrier_needed(access)) {\n-    pre_load_barrier(access);\n+  if (!barrier_needed(access)) {\n+    return BarrierSetC1::atomic_xchg_at_resolved(access, value);\n@@ -201,2 +478,1 @@\n-  return BarrierSetC1::atomic_xchg_at_resolved(access, value);\n-}\n+  value.load_item();\n@@ -204,4 +480,1 @@\n-LIR_Opr ZBarrierSetC1::atomic_cmpxchg_at_resolved(LIRAccess& access, LIRItem& cmp_value, LIRItem& new_value) {\n-  if (barrier_needed(access)) {\n-    pre_load_barrier(access);\n-  }\n+  LIR_Opr value_zpointer = store_barrier(access, value.result(), true \/* is_atomic *\/);\n@@ -209,1 +482,10 @@\n-  return BarrierSetC1::atomic_cmpxchg_at_resolved(access, cmp_value, new_value);\n+  \/\/ The parent class expects the in-parameter and out-parameter to be the same.\n+  \/\/ Move the colored pointer to the expected register.\n+#ifdef AMD64\n+  __ xchg(access.resolved_addr(), value_zpointer, value_zpointer, LIR_OprFact::illegalOpr);\n+#else\n+  __ xchg(access.resolved_addr(), value_zpointer, value_zpointer, access.gen()->new_register(T_INT));\n+#endif\n+  __ append(new LIR_OpZUncolor(value_zpointer));\n+\n+  return value_zpointer;\n@@ -212,0 +494,2 @@\n+#undef __\n+\n@@ -222,1 +506,1 @@\n-    return NULL;\n+    return nullptr;\n@@ -226,1 +510,1 @@\n-static address generate_c1_runtime_stub(BufferBlob* blob, DecoratorSet decorators, const char* name) {\n+static address generate_c1_load_runtime_stub(BufferBlob* blob, DecoratorSet decorators, const char* name) {\n@@ -232,0 +516,20 @@\n+class ZStoreBarrierRuntimeStubCodeGenClosure : public StubAssemblerCodeGenClosure {\n+private:\n+  const bool _self_healing;\n+\n+public:\n+  ZStoreBarrierRuntimeStubCodeGenClosure(bool self_healing) :\n+      _self_healing(self_healing) {}\n+\n+  virtual OopMapSet* generate_code(StubAssembler* sasm) {\n+    ZBarrierSet::assembler()->generate_c1_store_barrier_runtime_stub(sasm, _self_healing);\n+    return nullptr;\n+  }\n+};\n+\n+static address generate_c1_store_runtime_stub(BufferBlob* blob, bool self_healing, const char* name) {\n+  ZStoreBarrierRuntimeStubCodeGenClosure cl(self_healing);\n+  CodeBlob* const code_blob = Runtime1::generate_blob(blob, -1 \/* stub_id *\/, name, false \/* expect_oop_map*\/, &cl);\n+  return code_blob->code_begin();\n+}\n+\n@@ -234,1 +538,1 @@\n-    generate_c1_runtime_stub(blob, ON_STRONG_OOP_REF, \"load_barrier_on_oop_field_preloaded_runtime_stub\");\n+    generate_c1_load_runtime_stub(blob, ON_STRONG_OOP_REF, \"load_barrier_on_oop_field_preloaded_runtime_stub\");\n@@ -236,1 +540,6 @@\n-    generate_c1_runtime_stub(blob, ON_WEAK_OOP_REF, \"load_barrier_on_weak_oop_field_preloaded_runtime_stub\");\n+    generate_c1_load_runtime_stub(blob, ON_WEAK_OOP_REF, \"load_barrier_on_weak_oop_field_preloaded_runtime_stub\");\n+\n+  _store_barrier_on_oop_field_with_healing =\n+    generate_c1_store_runtime_stub(blob, true \/* self_healing *\/, \"store_barrier_on_oop_field_with_healing\");\n+  _store_barrier_on_oop_field_without_healing =\n+    generate_c1_store_runtime_stub(blob, false \/* self_healing *\/, \"store_barrier_on_oop_field_without_healing\");\n","filename":"src\/hotspot\/share\/gc\/z\/c1\/zBarrierSetC1.cpp","additions":354,"deletions":45,"binary":false,"changes":399,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -58,0 +58,32 @@\n+class ZStoreBarrierStubC1 : public CodeStub {\n+private:\n+  LIR_Opr _ref_addr;\n+  LIR_Opr _new_zaddress;\n+  LIR_Opr _new_zpointer;\n+  LIR_Opr _tmp;\n+  bool    _is_atomic;\n+  address _runtime_stub;\n+\n+public:\n+  ZStoreBarrierStubC1(LIRAccess& access,\n+                      LIR_Opr new_zaddress,\n+                      LIR_Opr new_zpointer,\n+                      LIR_Opr tmp,\n+                      bool is_atomic,\n+                      address runtime_stub);\n+\n+  LIR_Opr ref_addr() const;\n+  LIR_Opr new_zaddress() const;\n+  LIR_Opr new_zpointer() const;\n+  LIR_Opr tmp() const;\n+  bool is_atomic() const;\n+  address runtime_stub() const;\n+\n+  virtual void emit_code(LIR_Assembler* ce);\n+  virtual void visit(LIR_OpVisitState* visitor);\n+\n+#ifndef PRODUCT\n+  virtual void print_name(outputStream* out) const;\n+#endif \/\/ PRODUCT\n+};\n+\n@@ -62,0 +94,2 @@\n+  address _store_barrier_on_oop_field_with_healing;\n+  address _store_barrier_on_oop_field_without_healing;\n@@ -64,0 +98,4 @@\n+  address store_barrier_on_oop_field_runtime_stub(bool self_healing) const;\n+\n+  LIR_Opr color(LIRAccess& access, LIR_Opr ref) const;\n+\n@@ -65,0 +103,1 @@\n+  LIR_Opr store_barrier(LIRAccess& access, LIR_Opr new_zaddress, bool is_atomic) const;\n@@ -69,1 +108,1 @@\n-  virtual LIR_Opr atomic_xchg_at_resolved(LIRAccess& access, LIRItem& value);\n+  virtual void store_at_resolved(LIRAccess& access, LIR_Opr value);\n@@ -71,0 +110,1 @@\n+  virtual LIR_Opr atomic_xchg_at_resolved(LIRAccess& access, LIRItem& value);\n","filename":"src\/hotspot\/share\/gc\/z\/c1\/zBarrierSetC1.hpp","additions":42,"deletions":2,"binary":false,"changes":44,"status":"modified"},{"patch":"@@ -25,0 +25,1 @@\n+#include \"asm\/macroAssembler.hpp\"\n@@ -47,0 +48,74 @@\n+template<typename K, typename V, size_t _table_size>\n+class ZArenaHashtable : public ResourceObj {\n+  class ZArenaHashtableEntry : public ResourceObj {\n+  public:\n+    ZArenaHashtableEntry* _next;\n+    K _key;\n+    V _value;\n+  };\n+\n+  static const size_t _table_mask = _table_size - 1;\n+\n+  Arena* _arena;\n+  ZArenaHashtableEntry* _table[_table_size];\n+\n+public:\n+  class Iterator {\n+    ZArenaHashtable* _table;\n+    ZArenaHashtableEntry* _current_entry;\n+    size_t _current_index;\n+\n+  public:\n+    Iterator(ZArenaHashtable* table) :\n+        _table(table),\n+        _current_entry(table->_table[0]),\n+        _current_index(0) {\n+      if (_current_entry == nullptr) {\n+        next();\n+      }\n+    }\n+\n+    bool has_next() { return _current_entry != nullptr; }\n+    K key()         { return _current_entry->_key; }\n+    V value()       { return _current_entry->_value; }\n+\n+    void next() {\n+      if (_current_entry != nullptr) {\n+        _current_entry = _current_entry->_next;\n+      }\n+      while (_current_entry == nullptr && ++_current_index < _table_size) {\n+        _current_entry = _table->_table[_current_index];\n+      }\n+    }\n+  };\n+\n+  ZArenaHashtable(Arena* arena) :\n+      _arena(arena),\n+      _table() {\n+    Copy::zero_to_bytes(&_table, sizeof(_table));\n+  }\n+\n+  void add(K key, V value) {\n+    ZArenaHashtableEntry* entry = new (_arena) ZArenaHashtableEntry();\n+    entry->_key = key;\n+    entry->_value = value;\n+    entry->_next = _table[key & _table_mask];\n+    _table[key & _table_mask] = entry;\n+  }\n+\n+  V* get(K key) const {\n+    for (ZArenaHashtableEntry* e = _table[key & _table_mask]; e != nullptr; e = e->_next) {\n+      if (e->_key == key) {\n+        return &(e->_value);\n+      }\n+    }\n+    return nullptr;\n+  }\n+\n+  Iterator iterator() {\n+    return Iterator(this);\n+  }\n+};\n+\n+typedef ZArenaHashtable<intptr_t, bool, 4> ZOffsetTable;\n+\n@@ -49,2 +124,4 @@\n-  GrowableArray<ZLoadBarrierStubC2*>* _stubs;\n-  Node_Array                          _live;\n+  GrowableArray<ZBarrierStubC2*>* _stubs;\n+  Node_Array                      _live;\n+  int                             _trampoline_stubs_count;\n+  int                             _stubs_start_offset;\n@@ -54,2 +131,4 @@\n-    _stubs(new (arena) GrowableArray<ZLoadBarrierStubC2*>(arena, 8,  0, NULL)),\n-    _live(arena) {}\n+      _stubs(new (arena) GrowableArray<ZBarrierStubC2*>(arena, 8,  0, nullptr)),\n+      _live(arena),\n+      _trampoline_stubs_count(0),\n+      _stubs_start_offset(0) {}\n@@ -57,1 +136,1 @@\n-  GrowableArray<ZLoadBarrierStubC2*>* stubs() {\n+  GrowableArray<ZBarrierStubC2*>* stubs() {\n@@ -64,1 +143,1 @@\n-      return NULL;\n+      return nullptr;\n@@ -68,1 +147,1 @@\n-    if (mach->barrier_data() == ZLoadBarrierElided) {\n+    if (mach->barrier_data() == ZBarrierElided) {\n@@ -70,1 +149,1 @@\n-      return NULL;\n+      return nullptr;\n@@ -74,1 +153,1 @@\n-    if (live == NULL) {\n+    if (live == nullptr) {\n@@ -81,0 +160,17 @@\n+\n+  void inc_trampoline_stubs_count() {\n+    assert(_trampoline_stubs_count != INT_MAX, \"Overflow\");\n+    ++_trampoline_stubs_count;\n+  }\n+\n+  int trampoline_stubs_count() {\n+    return _trampoline_stubs_count;\n+  }\n+\n+  void set_stubs_start_offset(int offset) {\n+    _stubs_start_offset = offset;\n+  }\n+\n+  int stubs_start_offset() {\n+    return _stubs_start_offset;\n+  }\n@@ -87,2 +183,1 @@\n-ZLoadBarrierStubC2* ZLoadBarrierStubC2::create(const MachNode* node, Address ref_addr, Register ref, Register tmp, uint8_t barrier_data) {\n-  ZLoadBarrierStubC2* const stub = new (Compile::current()->comp_arena()) ZLoadBarrierStubC2(node, ref_addr, ref, tmp, barrier_data);\n+void ZBarrierStubC2::register_stub(ZBarrierStubC2* stub) {\n@@ -92,0 +187,1 @@\n+}\n@@ -93,1 +189,8 @@\n-  return stub;\n+void ZBarrierStubC2::inc_trampoline_stubs_count() {\n+  if (!Compile::current()->output()->in_scratch_emit_size()) {\n+    barrier_set_state()->inc_trampoline_stubs_count();\n+  }\n+}\n+\n+int ZBarrierStubC2::trampoline_stubs_count() {\n+  return barrier_set_state()->trampoline_stubs_count();\n@@ -96,1 +199,5 @@\n-ZLoadBarrierStubC2::ZLoadBarrierStubC2(const MachNode* node, Address ref_addr, Register ref, Register tmp, uint8_t barrier_data) :\n+int ZBarrierStubC2::stubs_start_offset() {\n+  return barrier_set_state()->stubs_start_offset();\n+}\n+\n+ZBarrierStubC2::ZBarrierStubC2(const MachNode* node) :\n@@ -98,4 +205,0 @@\n-    _ref_addr(ref_addr),\n-    _ref(ref),\n-    _tmp(tmp),\n-    _barrier_data(barrier_data),\n@@ -103,1 +206,33 @@\n-    _continuation() {\n+    _continuation() {}\n+\n+Register ZBarrierStubC2::result() const {\n+  return noreg;\n+}\n+\n+RegMask& ZBarrierStubC2::live() const {\n+  return *barrier_set_state()->live(_node);\n+}\n+\n+Label* ZBarrierStubC2::entry() {\n+  \/\/ The _entry will never be bound when in_scratch_emit_size() is true.\n+  \/\/ However, we still need to return a label that is not bound now, but\n+  \/\/ will eventually be bound. Any eventually bound label will do, as it\n+  \/\/ will only act as a placeholder, so we return the _continuation label.\n+  return Compile::current()->output()->in_scratch_emit_size() ? &_continuation : &_entry;\n+}\n+\n+Label* ZBarrierStubC2::continuation() {\n+  return &_continuation;\n+}\n+\n+ZLoadBarrierStubC2* ZLoadBarrierStubC2::create(const MachNode* node, Address ref_addr, Register ref) {\n+  ZLoadBarrierStubC2* const stub = new (Compile::current()->comp_arena()) ZLoadBarrierStubC2(node, ref_addr, ref);\n+  register_stub(stub);\n+\n+  return stub;\n+}\n+\n+ZLoadBarrierStubC2::ZLoadBarrierStubC2(const MachNode* node, Address ref_addr, Register ref) :\n+    ZBarrierStubC2(node),\n+    _ref_addr(ref_addr),\n+    _ref(ref) {\n@@ -116,2 +251,2 @@\n-Register ZLoadBarrierStubC2::tmp() const {\n-  return _tmp;\n+Register ZLoadBarrierStubC2::result() const {\n+  return ref();\n@@ -121,0 +256,1 @@\n+  const uint8_t barrier_data = _node->barrier_data();\n@@ -122,1 +258,1 @@\n-  if (_barrier_data & ZLoadBarrierStrong) {\n+  if (barrier_data & ZBarrierStrong) {\n@@ -125,1 +261,1 @@\n-  if (_barrier_data & ZLoadBarrierWeak) {\n+  if (barrier_data & ZBarrierWeak) {\n@@ -128,1 +264,1 @@\n-  if (_barrier_data & ZLoadBarrierPhantom) {\n+  if (barrier_data & ZBarrierPhantom) {\n@@ -131,1 +267,1 @@\n-  if (_barrier_data & ZLoadBarrierNoKeepalive) {\n+  if (barrier_data & ZBarrierNoKeepalive) {\n@@ -137,4 +273,2 @@\n-RegMask& ZLoadBarrierStubC2::live() const {\n-  RegMask* mask = barrier_set_state()->live(_node);\n-  assert(mask != NULL, \"must be mach-node with barrier\");\n-  return *mask;\n+void ZLoadBarrierStubC2::emit_code(MacroAssembler& masm) {\n+  ZBarrierSet::assembler()->generate_c2_load_barrier_stub(&masm, static_cast<ZLoadBarrierStubC2*>(this));\n@@ -143,6 +277,5 @@\n-Label* ZLoadBarrierStubC2::entry() {\n-  \/\/ The _entry will never be bound when in_scratch_emit_size() is true.\n-  \/\/ However, we still need to return a label that is not bound now, but\n-  \/\/ will eventually be bound. Any label will do, as it will only act as\n-  \/\/ a placeholder, so we return the _continuation label.\n-  return Compile::current()->output()->in_scratch_emit_size() ? &_continuation : &_entry;\n+ZStoreBarrierStubC2* ZStoreBarrierStubC2::create(const MachNode* node, Address ref_addr, Register new_zaddress, Register new_zpointer, bool is_native, bool is_atomic) {\n+  ZStoreBarrierStubC2* const stub = new (Compile::current()->comp_arena()) ZStoreBarrierStubC2(node, ref_addr, new_zaddress, new_zpointer, is_native, is_atomic);\n+  register_stub(stub);\n+\n+  return stub;\n@@ -151,2 +284,35 @@\n-Label* ZLoadBarrierStubC2::continuation() {\n-  return &_continuation;\n+ZStoreBarrierStubC2::ZStoreBarrierStubC2(const MachNode* node, Address ref_addr, Register new_zaddress, Register new_zpointer, bool is_native, bool is_atomic) :\n+    ZBarrierStubC2(node),\n+    _ref_addr(ref_addr),\n+    _new_zaddress(new_zaddress),\n+    _new_zpointer(new_zpointer),\n+    _is_native(is_native),\n+    _is_atomic(is_atomic) {\n+}\n+\n+Address ZStoreBarrierStubC2::ref_addr() const {\n+  return _ref_addr;\n+}\n+\n+Register ZStoreBarrierStubC2::new_zaddress() const {\n+  return _new_zaddress;\n+}\n+\n+Register ZStoreBarrierStubC2::new_zpointer() const {\n+  return _new_zpointer;\n+}\n+\n+bool ZStoreBarrierStubC2::is_native() const {\n+  return _is_native;\n+}\n+\n+bool ZStoreBarrierStubC2::is_atomic() const {\n+  return _is_atomic;\n+}\n+\n+Register ZStoreBarrierStubC2::result() const {\n+  return noreg;\n+}\n+\n+void ZStoreBarrierStubC2::emit_code(MacroAssembler& masm) {\n+  ZBarrierSet::assembler()->generate_c2_store_barrier_stub(&masm, static_cast<ZStoreBarrierStubC2*>(this));\n@@ -160,1 +326,0 @@\n-  analyze_dominating_barriers();\n@@ -162,0 +327,1 @@\n+  analyze_dominating_barriers();\n@@ -166,1 +332,2 @@\n-  GrowableArray<ZLoadBarrierStubC2*>* const stubs = barrier_set_state()->stubs();\n+  GrowableArray<ZBarrierStubC2*>* const stubs = barrier_set_state()->stubs();\n+  barrier_set_state()->set_stubs_start_offset(masm.offset());\n@@ -170,1 +337,1 @@\n-    if (cb.insts()->maybe_expand_to_ensure_remaining(PhaseOutput::MAX_inst_size) && cb.blob() == NULL) {\n+    if (cb.insts()->maybe_expand_to_ensure_remaining(PhaseOutput::MAX_inst_size) && cb.blob() == nullptr) {\n@@ -175,1 +342,1 @@\n-    ZBarrierSet::assembler()->generate_c2_load_barrier_stub(&masm, stubs->at(i));\n+    stubs->at(i)->emit_code(masm);\n@@ -184,1 +351,1 @@\n-  GrowableArray<ZLoadBarrierStubC2*>* const stubs = barrier_set_state()->stubs();\n+  GrowableArray<ZBarrierStubC2*>* const stubs = barrier_set_state()->stubs();\n@@ -190,1 +357,1 @@\n-    ZBarrierSet::assembler()->generate_c2_load_barrier_stub(&masm, stubs->at(i));\n+    stubs->at(i)->emit_code(masm);\n@@ -198,2 +365,3 @@\n-  if (ZBarrierSet::barrier_needed(access.decorators(), access.type())) {\n-    uint8_t barrier_data = 0;\n+  if (!ZBarrierSet::barrier_needed(access.decorators(), access.type())) {\n+    return;\n+  }\n@@ -201,7 +369,4 @@\n-    if (access.decorators() & ON_PHANTOM_OOP_REF) {\n-      barrier_data |= ZLoadBarrierPhantom;\n-    } else if (access.decorators() & ON_WEAK_OOP_REF) {\n-      barrier_data |= ZLoadBarrierWeak;\n-    } else {\n-      barrier_data |= ZLoadBarrierStrong;\n-    }\n+  if (access.decorators() & C2_TIGHTLY_COUPLED_ALLOC) {\n+    access.set_barrier_data(ZBarrierElided);\n+    return;\n+  }\n@@ -209,3 +374,1 @@\n-    if (access.decorators() & AS_NO_KEEPALIVE) {\n-      barrier_data |= ZLoadBarrierNoKeepalive;\n-    }\n+  uint8_t barrier_data = 0;\n@@ -213,1 +376,6 @@\n-    access.set_barrier_data(barrier_data);\n+  if (access.decorators() & ON_PHANTOM_OOP_REF) {\n+    barrier_data |= ZBarrierPhantom;\n+  } else if (access.decorators() & ON_WEAK_OOP_REF) {\n+    barrier_data |= ZBarrierWeak;\n+  } else {\n+    barrier_data |= ZBarrierStrong;\n@@ -215,0 +383,15 @@\n+\n+  if (access.decorators() & IN_NATIVE) {\n+    barrier_data |= ZBarrierNative;\n+  }\n+\n+  if (access.decorators() & AS_NO_KEEPALIVE) {\n+    barrier_data |= ZBarrierNoKeepalive;\n+  }\n+\n+  access.set_barrier_data(barrier_data);\n+}\n+\n+Node* ZBarrierSetC2::store_at_resolved(C2Access& access, C2AccessValue& val) const {\n+  set_barrier_data(access);\n+  return BarrierSetC2::store_at_resolved(access, val);\n@@ -255,1 +438,1 @@\n-  const Type** domain_fields = TypeTuple::fields(4);\n+  const Type** const domain_fields = TypeTuple::fields(4);\n@@ -260,1 +443,1 @@\n-  const TypeTuple* domain = TypeTuple::make(TypeFunc::Parms + 4, domain_fields);\n+  const TypeTuple* const domain = TypeTuple::make(TypeFunc::Parms + 4, domain_fields);\n@@ -263,2 +446,2 @@\n-  const Type** range_fields = TypeTuple::fields(0);\n-  const TypeTuple* range = TypeTuple::make(TypeFunc::Parms + 0, range_fields);\n+  const Type** const range_fields = TypeTuple::fields(0);\n+  const TypeTuple* const range = TypeTuple::make(TypeFunc::Parms + 0, range_fields);\n@@ -273,1 +456,1 @@\n-  const TypeAryPtr* ary_ptr = src->get_ptr_type()->isa_aryptr();\n+  const TypeAryPtr* const ary_ptr = src->get_ptr_type()->isa_aryptr();\n@@ -275,1 +458,1 @@\n-  if (ac->is_clone_array() && ary_ptr != NULL) {\n+  if (ac->is_clone_array() && ary_ptr != nullptr) {\n@@ -285,3 +468,3 @@\n-    Node* ctrl = ac->in(TypeFunc::Control);\n-    Node* mem = ac->in(TypeFunc::Memory);\n-    Node* src = ac->in(ArrayCopyNode::Src);\n+    Node* const ctrl = ac->in(TypeFunc::Control);\n+    Node* const mem = ac->in(TypeFunc::Memory);\n+    Node* const src = ac->in(ArrayCopyNode::Src);\n@@ -289,1 +472,1 @@\n-    Node* dest = ac->in(ArrayCopyNode::Dest);\n+    Node* const dest = ac->in(ArrayCopyNode::Dest);\n@@ -299,1 +482,1 @@\n-      jlong offset = src_offset->get_long();\n+      const jlong offset = src_offset->get_long();\n@@ -308,2 +491,2 @@\n-    Node* payload_src = phase->basic_plus_adr(src, src_offset);\n-    Node* payload_dst = phase->basic_plus_adr(dest, dest_offset);\n+    Node* const payload_src = phase->basic_plus_adr(src, src_offset);\n+    Node* const payload_dst = phase->basic_plus_adr(dest, dest_offset);\n@@ -311,2 +494,2 @@\n-    const char* copyfunc_name = \"arraycopy\";\n-    address     copyfunc_addr = phase->basictype2arraycopy(bt, NULL, NULL, true, copyfunc_name, true);\n+    const char*   copyfunc_name = \"arraycopy\";\n+    const address copyfunc_addr = phase->basictype2arraycopy(bt, nullptr, nullptr, true, copyfunc_name, true);\n@@ -314,2 +497,2 @@\n-    const TypePtr* raw_adr_type = TypeRawPtr::BOTTOM;\n-    const TypeFunc* call_type = OptoRuntime::fast_arraycopy_Type();\n+    const TypePtr* const raw_adr_type = TypeRawPtr::BOTTOM;\n+    const TypeFunc* const call_type = OptoRuntime::fast_arraycopy_Type();\n@@ -317,1 +500,1 @@\n-    Node* call = phase->make_leaf_call(ctrl, mem, call_type, copyfunc_addr, copyfunc_name, raw_adr_type, payload_src, payload_dst, length XTOP);\n+    Node* const call = phase->make_leaf_call(ctrl, mem, call_type, copyfunc_addr, copyfunc_name, raw_adr_type, payload_src, payload_dst, length XTOP);\n@@ -381,7 +564,19 @@\n-void ZBarrierSetC2::analyze_dominating_barriers() const {\n-  ResourceMark rm;\n-  Compile* const C = Compile::current();\n-  PhaseCFG* const cfg = C->cfg();\n-  Block_List worklist;\n-  Node_List mem_ops;\n-  Node_List barrier_loads;\n+\/\/ Look through various node aliases\n+static const Node* look_through_node(const Node* node) {\n+  while (node != nullptr) {\n+    const Node* new_node = node;\n+    if (node->is_Mach()) {\n+      const MachNode* const node_mach = node->as_Mach();\n+      if (node_mach->ideal_Opcode() == Op_CheckCastPP) {\n+        new_node = node->in(1);\n+      }\n+      if (node_mach->is_SpillCopy()) {\n+        new_node = node->in(1);\n+      }\n+    }\n+    if (new_node == node || new_node == nullptr) {\n+      break;\n+    } else {\n+      node = new_node;\n+    }\n+  }\n@@ -389,6 +584,62 @@\n-  \/\/ Step 1 - Find accesses, and track them in lists\n-  for (uint i = 0; i < cfg->number_of_blocks(); ++i) {\n-    const Block* const block = cfg->get_block(i);\n-    for (uint j = 0; j < block->number_of_nodes(); ++j) {\n-      const Node* const node = block->get_node(j);\n-      if (!node->is_Mach()) {\n+  return node;\n+}\n+\n+\/\/ Whether the given offset is undefined.\n+static bool is_undefined(intptr_t offset) {\n+  return offset == Type::OffsetTop;\n+}\n+\n+\/\/ Whether the given offset is unknown.\n+static bool is_unknown(intptr_t offset) {\n+  return offset == Type::OffsetBot;\n+}\n+\n+\/\/ Whether the given offset is concrete (defined and compile-time known).\n+static bool is_concrete(intptr_t offset) {\n+  return !is_undefined(offset) && !is_unknown(offset);\n+}\n+\n+\/\/ Compute base + offset components of the memory address accessed by mach.\n+\/\/ Return a node representing the base address, or null if the base cannot be\n+\/\/ found or the offset is undefined or a concrete negative value. If a non-null\n+\/\/ base is returned, the offset is a concrete, nonnegative value or unknown.\n+static const Node* get_base_and_offset(const MachNode* mach, intptr_t& offset) {\n+  const TypePtr* adr_type = nullptr;\n+  offset = 0;\n+  const Node* base = mach->get_base_and_disp(offset, adr_type);\n+\n+  if (base == nullptr || base == NodeSentinel) {\n+    return nullptr;\n+  }\n+\n+  if (offset == 0 && base->is_Mach() && base->as_Mach()->ideal_Opcode() == Op_AddP) {\n+    \/\/ The memory address is computed by 'base' and fed to 'mach' via an\n+    \/\/ indirect memory operand (indicated by offset == 0). The ultimate base and\n+    \/\/ offset can be fetched directly from the inputs and Ideal type of 'base'.\n+    offset = base->bottom_type()->isa_oopptr()->offset();\n+    \/\/ Even if 'base' is not an Ideal AddP node anymore, Matcher::ReduceInst()\n+    \/\/ guarantees that the base address is still available at the same slot.\n+    base = base->in(AddPNode::Base);\n+    assert(base != nullptr, \"\");\n+  }\n+\n+  if (is_undefined(offset) || (is_concrete(offset) && offset < 0)) {\n+    return nullptr;\n+  }\n+\n+  return look_through_node(base);\n+}\n+\n+\/\/ Whether a phi node corresponds to an array allocation.\n+\/\/ This test is incomplete: in some edge cases, it might return false even\n+\/\/ though the node does correspond to an array allocation.\n+static bool is_array_allocation(const Node* phi) {\n+  precond(phi->is_Phi());\n+  \/\/ Check whether phi has a successor cast (CheckCastPP) to Java array pointer,\n+  \/\/ possibly below spill copies and other cast nodes. Limit the exploration to\n+  \/\/ a single path from the phi node consisting of these node types.\n+  const Node* current = phi;\n+  while (true) {\n+    const Node* next = nullptr;\n+    for (DUIterator_Fast imax, i = current->fast_outs(imax); i < imax; i++) {\n+      if (!current->fast_out(i)->isa_Mach()) {\n@@ -397,17 +648,5 @@\n-\n-      MachNode* const mach = node->as_Mach();\n-      switch (mach->ideal_Opcode()) {\n-      case Op_LoadP:\n-        if ((mach->barrier_data() & ZLoadBarrierStrong) != 0) {\n-          barrier_loads.push(mach);\n-        }\n-        if ((mach->barrier_data() & (ZLoadBarrierStrong | ZLoadBarrierNoKeepalive)) ==\n-            ZLoadBarrierStrong) {\n-          mem_ops.push(mach);\n-        }\n-        break;\n-      case Op_CompareAndExchangeP:\n-      case Op_CompareAndSwapP:\n-      case Op_GetAndSetP:\n-        if ((mach->barrier_data() & ZLoadBarrierStrong) != 0) {\n-          barrier_loads.push(mach);\n+      const MachNode* succ = current->fast_out(i)->as_Mach();\n+      if (succ->ideal_Opcode() == Op_CheckCastPP) {\n+        if (succ->get_ptr_type()->isa_aryptr()) {\n+          \/\/ Cast to Java array pointer: phi corresponds to an array allocation.\n+          return true;\n@@ -415,6 +654,5 @@\n-      case Op_StoreP:\n-        mem_ops.push(mach);\n-        break;\n-\n-      default:\n-        break;\n+        \/\/ Other cast: record as candidate for further exploration.\n+        next = succ;\n+      } else if (succ->is_SpillCopy() && next == nullptr) {\n+        \/\/ Spill copy, and no better candidate found: record as candidate.\n+        next = succ;\n@@ -423,0 +661,7 @@\n+    if (next == nullptr) {\n+      \/\/ No evidence found that phi corresponds to an array allocation, and no\n+      \/\/ candidates available to continue exploring.\n+      return false;\n+    }\n+    \/\/ Continue exploring from the best candidate found.\n+    current = next;\n@@ -424,0 +669,2 @@\n+  ShouldNotReachHere();\n+}\n@@ -425,16 +672,25 @@\n-  \/\/ Step 2 - Find dominating accesses for each load\n-  for (uint i = 0; i < barrier_loads.size(); i++) {\n-    MachNode* const load = barrier_loads.at(i)->as_Mach();\n-    const TypePtr* load_adr_type = NULL;\n-    intptr_t load_offset = 0;\n-    const Node* const load_obj = load->get_base_and_disp(load_offset, load_adr_type);\n-    Block* const load_block = cfg->get_block_for_node(load);\n-    const uint load_index = block_index(load_block, load);\n-\n-    for (uint j = 0; j < mem_ops.size(); j++) {\n-      MachNode* mem = mem_ops.at(j)->as_Mach();\n-      const TypePtr* mem_adr_type = NULL;\n-      intptr_t mem_offset = 0;\n-      const Node* mem_obj = mem->get_base_and_disp(mem_offset, mem_adr_type);\n-      Block* mem_block = cfg->get_block_for_node(mem);\n-      uint mem_index = block_index(mem_block, mem);\n+\/\/ Match the phi node that connects a TLAB allocation fast path with its slowpath\n+static bool is_allocation(const Node* node) {\n+  if (node->req() != 3) {\n+    return false;\n+  }\n+  const Node* const fast_node = node->in(2);\n+  if (!fast_node->is_Mach()) {\n+    return false;\n+  }\n+  const MachNode* const fast_mach = fast_node->as_Mach();\n+  if (fast_mach->ideal_Opcode() != Op_LoadP) {\n+    return false;\n+  }\n+  const TypePtr* const adr_type = nullptr;\n+  intptr_t offset;\n+  const Node* const base = get_base_and_offset(fast_mach, offset);\n+  if (base == nullptr || !base->is_Mach() || !is_concrete(offset)) {\n+    return false;\n+  }\n+  const MachNode* const base_mach = base->as_Mach();\n+  if (base_mach->ideal_Opcode() != Op_ThreadLocal) {\n+    return false;\n+  }\n+  return offset == in_bytes(Thread::tlab_top_offset());\n+}\n@@ -442,5 +698,3 @@\n-      if (load_obj == NodeSentinel || mem_obj == NodeSentinel ||\n-          load_obj == NULL || mem_obj == NULL ||\n-          load_offset < 0 || mem_offset < 0) {\n-        continue;\n-      }\n+static void elide_mach_barrier(MachNode* mach) {\n+  mach->set_barrier_data(ZBarrierElided);\n+}\n@@ -448,3 +702,50 @@\n-      if (mem_obj != load_obj || mem_offset != load_offset) {\n-        \/\/ Not the same addresses, not a candidate\n-        continue;\n+void ZBarrierSetC2::analyze_dominating_barriers_impl(Node_List& accesses, Node_List& access_dominators) const {\n+  Compile* const C = Compile::current();\n+  PhaseCFG* const cfg = C->cfg();\n+\n+  for (uint i = 0; i < accesses.size(); i++) {\n+    MachNode* const access = accesses.at(i)->as_Mach();\n+    intptr_t access_offset;\n+    const Node* const access_obj = get_base_and_offset(access, access_offset);\n+    Block* const access_block = cfg->get_block_for_node(access);\n+    const uint access_index = block_index(access_block, access);\n+\n+    if (access_obj == nullptr) {\n+      \/\/ No information available\n+      continue;\n+    }\n+\n+    for (uint j = 0; j < access_dominators.size(); j++) {\n+     const  Node* const mem = access_dominators.at(j);\n+      if (mem->is_Phi()) {\n+        \/\/ Allocation node\n+        if (mem != access_obj) {\n+          continue;\n+        }\n+        if (is_unknown(access_offset) && !is_array_allocation(mem)) {\n+          \/\/ The accessed address has an unknown offset, but the allocated\n+          \/\/ object cannot be determined to be an array. Avoid eliding in this\n+          \/\/ case, to be on the safe side.\n+          continue;\n+        }\n+        assert((is_concrete(access_offset) && access_offset >= 0) || (is_unknown(access_offset) && is_array_allocation(mem)),\n+               \"candidate allocation-dominated access offsets must be either concrete and nonnegative, or unknown (for array allocations only)\");\n+      } else {\n+        \/\/ Access node\n+        const MachNode* const mem_mach = mem->as_Mach();\n+        intptr_t mem_offset;\n+        const Node* const mem_obj = get_base_and_offset(mem_mach, mem_offset);\n+\n+        if (mem_obj == nullptr ||\n+            !is_concrete(access_offset) ||\n+            !is_concrete(mem_offset)) {\n+          \/\/ No information available\n+          continue;\n+        }\n+\n+        if (mem_obj != access_obj || mem_offset != access_offset) {\n+          \/\/ Not the same addresses, not a candidate\n+          continue;\n+        }\n+        assert(is_concrete(access_offset) && access_offset >= 0,\n+               \"candidate non-allocation-dominated access offsets must be concrete and nonnegative\");\n@@ -453,1 +754,4 @@\n-      if (load_block == mem_block) {\n+      Block* mem_block = cfg->get_block_for_node(mem);\n+      const uint mem_index = block_index(mem_block, mem);\n+\n+      if (access_block == mem_block) {\n@@ -455,2 +759,2 @@\n-        if (mem_index < load_index && !block_has_safepoint(mem_block, mem_index + 1, load_index)) {\n-          load->set_barrier_data(ZLoadBarrierElided);\n+        if (mem_index < access_index && !block_has_safepoint(mem_block, mem_index + 1, access_index)) {\n+          elide_mach_barrier(access);\n@@ -458,1 +762,1 @@\n-      } else if (mem_block->dominates(load_block)) {\n+      } else if (mem_block->dominates(access_block)) {\n@@ -463,2 +767,2 @@\n-        stack.push(load_block);\n-        bool safepoint_found = block_has_safepoint(load_block);\n+        stack.push(access_block);\n+        bool safepoint_found = block_has_safepoint(access_block);\n@@ -466,1 +770,1 @@\n-          Block* block = stack.pop();\n+          const Block* const block = stack.pop();\n@@ -480,1 +784,1 @@\n-            Block* pred = cfg->get_block_for_node(block->pred(p));\n+            Block* const pred = cfg->get_block_for_node(block->pred(p));\n@@ -486,1 +790,1 @@\n-          load->set_barrier_data(ZLoadBarrierElided);\n+          elide_mach_barrier(access);\n@@ -493,0 +797,74 @@\n+void ZBarrierSetC2::analyze_dominating_barriers() const {\n+  ResourceMark rm;\n+  Compile* const C = Compile::current();\n+  PhaseCFG* const cfg = C->cfg();\n+\n+  Node_List loads;\n+  Node_List load_dominators;\n+\n+  Node_List stores;\n+  Node_List store_dominators;\n+\n+  Node_List atomics;\n+  Node_List atomic_dominators;\n+\n+  \/\/ Step 1 - Find accesses and allocations, and track them in lists\n+  for (uint i = 0; i < cfg->number_of_blocks(); ++i) {\n+    const Block* const block = cfg->get_block(i);\n+    for (uint j = 0; j < block->number_of_nodes(); ++j) {\n+      Node* const node = block->get_node(j);\n+      if (node->is_Phi()) {\n+        if (is_allocation(node)) {\n+          load_dominators.push(node);\n+          store_dominators.push(node);\n+          \/\/ An allocation can't be considered to \"dominate\" an atomic operation.\n+          \/\/ For example a CAS requires the memory location to be store-good.\n+          \/\/ When you have a dominating store or atomic instruction, that is\n+          \/\/ indeed ensured to be the case. However, as for allocations, the\n+          \/\/ initialized memory location could be raw null, which isn't store-good.\n+        }\n+        continue;\n+      } else if (!node->is_Mach()) {\n+        continue;\n+      }\n+\n+      MachNode* const mach = node->as_Mach();\n+      switch (mach->ideal_Opcode()) {\n+      case Op_LoadP:\n+        if ((mach->barrier_data() & ZBarrierStrong) != 0 &&\n+            (mach->barrier_data() & ZBarrierNoKeepalive) == 0) {\n+          loads.push(mach);\n+          load_dominators.push(mach);\n+        }\n+        break;\n+      case Op_StoreP:\n+        if (mach->barrier_data() != 0) {\n+          stores.push(mach);\n+          load_dominators.push(mach);\n+          store_dominators.push(mach);\n+          atomic_dominators.push(mach);\n+        }\n+        break;\n+      case Op_CompareAndExchangeP:\n+      case Op_CompareAndSwapP:\n+      case Op_GetAndSetP:\n+        if (mach->barrier_data() != 0) {\n+          atomics.push(mach);\n+          load_dominators.push(mach);\n+          store_dominators.push(mach);\n+          atomic_dominators.push(mach);\n+        }\n+        break;\n+\n+      default:\n+        break;\n+      }\n+    }\n+  }\n+\n+  \/\/ Step 2 - Find dominating accesses or allocations for each access\n+  analyze_dominating_barriers_impl(loads, load_dominators);\n+  analyze_dominating_barriers_impl(stores, store_dominators);\n+  analyze_dominating_barriers_impl(atomics, atomic_dominators);\n+}\n+\n@@ -550,1 +928,1 @@\n-      if (regs != NULL) {\n+      if (regs != nullptr) {\n@@ -568,0 +946,14 @@\n+void ZBarrierSetC2::eliminate_gc_barrier(PhaseMacroExpand* macro, Node* node) const {\n+  eliminate_gc_barrier_data(node);\n+}\n+\n+void ZBarrierSetC2::eliminate_gc_barrier_data(Node* node) const {\n+  if (node->is_LoadStore()) {\n+    LoadStoreNode* loadstore = node->as_LoadStore();\n+    loadstore->set_barrier_data(ZBarrierElided);\n+  } else if (node->is_Mem()) {\n+    MemNode* mem = node->as_Mem();\n+    mem->set_barrier_data(ZBarrierElided);\n+  }\n+}\n+\n@@ -570,1 +962,1 @@\n-  if ((mach->barrier_data() & ZLoadBarrierStrong) != 0) {\n+  if ((mach->barrier_data() & ZBarrierStrong) != 0) {\n@@ -573,1 +965,1 @@\n-  if ((mach->barrier_data() & ZLoadBarrierWeak) != 0) {\n+  if ((mach->barrier_data() & ZBarrierWeak) != 0) {\n@@ -576,1 +968,1 @@\n-  if ((mach->barrier_data() & ZLoadBarrierPhantom) != 0) {\n+  if ((mach->barrier_data() & ZBarrierPhantom) != 0) {\n@@ -579,1 +971,1 @@\n-  if ((mach->barrier_data() & ZLoadBarrierNoKeepalive) != 0) {\n+  if ((mach->barrier_data() & ZBarrierNoKeepalive) != 0) {\n@@ -582,0 +974,6 @@\n+  if ((mach->barrier_data() & ZBarrierNative) != 0) {\n+    st->print(\"native \");\n+  }\n+  if ((mach->barrier_data() & ZBarrierElided) != 0) {\n+    st->print(\"elided \");\n+  }\n","filename":"src\/hotspot\/share\/gc\/z\/c2\/zBarrierSetC2.cpp","additions":545,"deletions":147,"binary":false,"changes":692,"status":"modified"},{"patch":"@@ -32,5 +32,6 @@\n-const uint8_t ZLoadBarrierElided      = 0;\n-const uint8_t ZLoadBarrierStrong      = 1;\n-const uint8_t ZLoadBarrierWeak        = 2;\n-const uint8_t ZLoadBarrierPhantom     = 4;\n-const uint8_t ZLoadBarrierNoKeepalive = 8;\n+const uint8_t ZBarrierStrong      =  1;\n+const uint8_t ZBarrierWeak        =  2;\n+const uint8_t ZBarrierPhantom     =  4;\n+const uint8_t ZBarrierNoKeepalive =  8;\n+const uint8_t ZBarrierNative      = 16;\n+const uint8_t ZBarrierElided      = 32;\n@@ -38,2 +39,7 @@\n-class ZLoadBarrierStubC2 : public ArenaObj {\n-private:\n+class Block;\n+class MachNode;\n+\n+class MacroAssembler;\n+\n+class ZBarrierStubC2 : public ArenaObj {\n+protected:\n@@ -41,4 +47,0 @@\n-  const Address   _ref_addr;\n-  const Register  _ref;\n-  const Register  _tmp;\n-  const uint8_t   _barrier_data;\n@@ -48,1 +50,4 @@\n-  ZLoadBarrierStubC2(const MachNode* node, Address ref_addr, Register ref, Register tmp, uint8_t barrier_data);\n+static void register_stub(ZBarrierStubC2* stub);\n+static void inc_trampoline_stubs_count();\n+static int trampoline_stubs_count();\n+static int stubs_start_offset();\n@@ -51,1 +56,1 @@\n-  static ZLoadBarrierStubC2* create(const MachNode* node, Address ref_addr, Register ref, Register tmp, uint8_t barrier_data);\n+  ZBarrierStubC2(const MachNode* node);\n@@ -53,4 +58,0 @@\n-  Address ref_addr() const;\n-  Register ref() const;\n-  Register tmp() const;\n-  address slow_path() const;\n@@ -60,0 +61,46 @@\n+\n+  virtual Register result() const = 0;\n+  virtual void emit_code(MacroAssembler& masm) = 0;\n+};\n+\n+class ZLoadBarrierStubC2 : public ZBarrierStubC2 {\n+private:\n+  const Address  _ref_addr;\n+  const Register _ref;\n+\n+protected:\n+  ZLoadBarrierStubC2(const MachNode* node, Address ref_addr, Register ref);\n+\n+public:\n+  static ZLoadBarrierStubC2* create(const MachNode* node, Address ref_addr, Register ref);\n+\n+  Address ref_addr() const;\n+  Register ref() const;\n+  address slow_path() const;\n+\n+  virtual Register result() const;\n+  virtual void emit_code(MacroAssembler& masm);\n+};\n+\n+class ZStoreBarrierStubC2 : public ZBarrierStubC2 {\n+private:\n+  const Address  _ref_addr;\n+  const Register _new_zaddress;\n+  const Register _new_zpointer;\n+  const bool     _is_native;\n+  const bool     _is_atomic;\n+\n+protected:\n+  ZStoreBarrierStubC2(const MachNode* node, Address ref_addr, Register new_zaddress, Register new_zpointer, bool is_native, bool is_atomic);\n+\n+public:\n+  static ZStoreBarrierStubC2* create(const MachNode* node, Address ref_addr, Register new_zaddress, Register new_zpointer, bool is_native, bool is_atomic);\n+\n+  Address ref_addr() const;\n+  Register new_zaddress() const;\n+  Register new_zpointer() const;\n+  bool is_native() const;\n+  bool is_atomic() const;\n+\n+  virtual Register result() const;\n+  virtual void emit_code(MacroAssembler& masm);\n@@ -65,0 +112,1 @@\n+  void analyze_dominating_barriers_impl(Node_List& accesses, Node_List& access_dominators) const;\n@@ -68,0 +116,1 @@\n+  virtual Node* store_at_resolved(C2Access& access, C2AccessValue& val) const;\n@@ -94,0 +143,2 @@\n+  virtual void eliminate_gc_barrier(PhaseMacroExpand* macro, Node* node) const;\n+  virtual void eliminate_gc_barrier_data(Node* node) const;\n","filename":"src\/hotspot\/share\/gc\/z\/c2\/zBarrierSetC2.hpp","additions":68,"deletions":17,"binary":false,"changes":85,"status":"modified"},{"patch":"@@ -0,0 +1,68 @@\n+\/*\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_Z_SHARED_VMSTRUCTS_Z_SHARED_HPP\n+#define SHARE_GC_Z_SHARED_VMSTRUCTS_Z_SHARED_HPP\n+\n+#include \"gc\/x\/vmStructs_x.hpp\"\n+#include \"gc\/z\/vmStructs_z.hpp\"\n+\n+#define VM_STRUCTS_Z_SHARED(nonstatic_field, volatile_nonstatic_field, static_field)    \\\n+  VM_STRUCTS_X(                                                                         \\\n+    nonstatic_field,                                                                    \\\n+    volatile_nonstatic_field,                                                           \\\n+    static_field)                                                                       \\\n+                                                                                        \\\n+  VM_STRUCTS_Z(                                                                         \\\n+    nonstatic_field,                                                                    \\\n+    volatile_nonstatic_field,                                                           \\\n+    static_field)\n+\n+#define VM_INT_CONSTANTS_Z_SHARED(declare_constant, declare_constant_with_value)        \\\n+  VM_INT_CONSTANTS_X(                                                                   \\\n+    declare_constant,                                                                   \\\n+    declare_constant_with_value)                                                        \\\n+                                                                                        \\\n+  VM_INT_CONSTANTS_Z(                                                                   \\\n+    declare_constant,                                                                   \\\n+    declare_constant_with_value)\n+\n+#define VM_LONG_CONSTANTS_Z_SHARED(declare_constant)                                    \\\n+  VM_LONG_CONSTANTS_X(                                                                  \\\n+    declare_constant)                                                                   \\\n+                                                                                        \\\n+  VM_LONG_CONSTANTS_Z(                                                                  \\\n+    declare_constant)\n+\n+#define VM_TYPES_Z_SHARED(declare_type, declare_toplevel_type, declare_integer_type)    \\\n+  VM_TYPES_X(                                                                           \\\n+    declare_type,                                                                       \\\n+    declare_toplevel_type,                                                              \\\n+    declare_integer_type)                                                               \\\n+                                                                                        \\\n+  VM_TYPES_Z(                                                                           \\\n+    declare_type,                                                                       \\\n+    declare_toplevel_type,                                                              \\\n+    declare_integer_type)\n+\n+#endif \/\/ SHARE_GC_Z_SHARED_VMSTRUCTS_Z_SHARED_HPP\n","filename":"src\/hotspot\/share\/gc\/z\/shared\/vmStructs_z_shared.hpp","additions":68,"deletions":0,"binary":false,"changes":68,"status":"added"},{"patch":"@@ -0,0 +1,77 @@\n+\/*\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/shared\/gcArguments.hpp\"\n+#include \"gc\/x\/xArguments.hpp\"\n+#include \"gc\/z\/shared\/zSharedArguments.hpp\"\n+#include \"gc\/z\/zArguments.hpp\"\n+#include \"runtime\/globals.hpp\"\n+#include \"runtime\/globals_extension.hpp\"\n+#include \"runtime\/java.hpp\"\n+\n+void ZSharedArguments::initialize_alignments() {\n+  if (ZGenerational) {\n+    ZArguments::initialize_alignments();\n+  } else {\n+    XArguments::initialize_alignments();\n+  }\n+}\n+\n+void ZSharedArguments::initialize() {\n+  GCArguments::initialize();\n+\n+  if (ZGenerational) {\n+    ZArguments::initialize();\n+  } else {\n+    XArguments::initialize();\n+  }\n+}\n+\n+size_t ZSharedArguments::heap_virtual_to_physical_ratio() {\n+  if (ZGenerational) {\n+    return ZArguments::heap_virtual_to_physical_ratio();\n+  } else {\n+    return XArguments::heap_virtual_to_physical_ratio();\n+  }\n+}\n+\n+size_t ZSharedArguments::conservative_max_heap_alignment() {\n+  return 0;\n+}\n+\n+CollectedHeap* ZSharedArguments::create_heap() {\n+  if (ZGenerational) {\n+    return ZArguments::create_heap();\n+  } else {\n+    return XArguments::create_heap();\n+  }\n+}\n+\n+bool ZSharedArguments::is_supported() const {\n+  if (ZGenerational) {\n+    return ZArguments::is_os_supported();\n+  } else {\n+    return XArguments::is_os_supported();\n+  }\n+}\n","filename":"src\/hotspot\/share\/gc\/z\/shared\/zSharedArguments.cpp","additions":77,"deletions":0,"binary":false,"changes":77,"status":"added"},{"patch":"@@ -0,0 +1,45 @@\n+\/*\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_Z_SHARED_ZSHAREDARGUMENTS_HPP\n+#define SHARE_GC_Z_SHARED_ZSHAREDARGUMENTS_HPP\n+\n+#include \"gc\/shared\/gcArguments.hpp\"\n+\n+class CollectedHeap;\n+\n+class ZSharedArguments : public GCArguments {\n+private:\n+  virtual void initialize_alignments();\n+\n+  virtual void initialize();\n+  virtual size_t conservative_max_heap_alignment();\n+  virtual size_t heap_virtual_to_physical_ratio();\n+  virtual CollectedHeap* create_heap();\n+\n+  virtual bool is_supported() const;\n+\n+  bool is_os_supported() const;\n+};\n+\n+#endif \/\/ SHARE_GC_Z_SHARED_ZSHAREDARGUMENTS_HPP\n","filename":"src\/hotspot\/share\/gc\/z\/shared\/zSharedArguments.hpp","additions":45,"deletions":0,"binary":false,"changes":45,"status":"added"},{"patch":"@@ -0,0 +1,103 @@\n+\/*\n+ * Copyright (c) 2018, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_Z_SHARED_Z_SHARED_GLOBALS_HPP\n+#define SHARE_GC_Z_SHARED_Z_SHARED_GLOBALS_HPP\n+\n+#include \"gc\/x\/x_globals.hpp\"\n+#include \"gc\/z\/z_globals.hpp\"\n+\n+#define GC_Z_SHARED_FLAGS(develop,                                          \\\n+                          develop_pd,                                       \\\n+                          product,                                          \\\n+                          product_pd,                                       \\\n+                          notproduct,                                       \\\n+                          range,                                            \\\n+                          constraint)                                       \\\n+                                                                            \\\n+  product(double, ZAllocationSpikeTolerance, 2.0,                           \\\n+          \"Allocation spike tolerance factor\")                              \\\n+                                                                            \\\n+  \/* Updated in arguments parsing to ZGenerational ? 5.0 : 25.0 *\/          \\\n+  product(double, ZFragmentationLimit, 0 \/* ignored *\/,                     \\\n+          \"Maximum allowed heap fragmentation\")                             \\\n+          range(0, 100)                                                     \\\n+                                                                            \\\n+  product(size_t, ZMarkStackSpaceLimit, 8*G,                                \\\n+          \"Maximum number of bytes allocated for mark stacks\")              \\\n+          range(32*M, 1024*G)                                               \\\n+                                                                            \\\n+  product(double, ZCollectionInterval, 0,                                   \\\n+          \"Force GC at a fixed time interval (in seconds). \"                \\\n+          \"Backwards compatible alias for ZCollectionIntervalMajor\")        \\\n+                                                                            \\\n+  product(bool, ZProactive, true,                                           \\\n+          \"Enable proactive GC cycles\")                                     \\\n+                                                                            \\\n+  product(bool, ZUncommit, true,                                            \\\n+          \"Uncommit unused memory\")                                         \\\n+                                                                            \\\n+  product(uintx, ZUncommitDelay, 5 * 60,                                    \\\n+          \"Uncommit memory if it has been unused for the specified \"        \\\n+          \"amount of time (in seconds)\")                                    \\\n+                                                                            \\\n+  product(uint, ZStatisticsInterval, 10, DIAGNOSTIC,                        \\\n+          \"Time between statistics print outs (in seconds)\")                \\\n+          range(1, (uint)-1)                                                \\\n+                                                                            \\\n+  product(bool, ZStressRelocateInPlace, false, DIAGNOSTIC,                  \\\n+          \"Always relocate pages in-place\")                                 \\\n+                                                                            \\\n+  product(bool, ZVerifyRoots, trueInDebug, DIAGNOSTIC,                      \\\n+          \"Verify roots\")                                                   \\\n+                                                                            \\\n+  product(bool, ZVerifyObjects, false, DIAGNOSTIC,                          \\\n+          \"Verify objects\")                                                 \\\n+                                                                            \\\n+  product(bool, ZVerifyMarking, trueInDebug, DIAGNOSTIC,                    \\\n+          \"Verify marking stacks\")                                          \\\n+                                                                            \\\n+  product(bool, ZVerifyForwarding, false, DIAGNOSTIC,                       \\\n+          \"Verify forwarding tables\")                                       \\\n+                                                                            \\\n+  GC_X_FLAGS(                                                               \\\n+    develop,                                                                \\\n+    develop_pd,                                                             \\\n+    product,                                                                \\\n+    product_pd,                                                             \\\n+    notproduct,                                                             \\\n+    range,                                                                  \\\n+    constraint)                                                             \\\n+                                                                            \\\n+  GC_Z_FLAGS(                                                               \\\n+    develop,                                                                \\\n+    develop_pd,                                                             \\\n+    product,                                                                \\\n+    product_pd,                                                             \\\n+    notproduct,                                                             \\\n+    range,                                                                  \\\n+    constraint)\n+\n+\/\/ end of GC_Z_SHARED_FLAGS\n+\n+#endif \/\/ SHARE_GC_Z_SHARED_Z_SHARED_GLOBALS_HPP\n","filename":"src\/hotspot\/share\/gc\/z\/shared\/z_shared_globals.hpp","additions":103,"deletions":0,"binary":false,"changes":103,"status":"added"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -26,0 +26,1 @@\n+#include \"gc\/z\/zAddress.hpp\"\n@@ -28,2 +29,0 @@\n-    _ZGlobalPhase(&ZGlobalPhase),\n-    _ZGlobalSeqNum(&ZGlobalSeqNum),\n@@ -31,5 +30,7 @@\n-    _ZAddressMetadataMask(&ZAddressMetadataMask),\n-    _ZAddressMetadataFinalizable(&ZAddressMetadataFinalizable),\n-    _ZAddressGoodMask(&ZAddressGoodMask),\n-    _ZAddressBadMask(&ZAddressBadMask),\n-    _ZAddressWeakBadMask(&ZAddressWeakBadMask),\n+    _ZPointerLoadGoodMask(&ZPointerLoadGoodMask),\n+    _ZPointerLoadBadMask(&ZPointerLoadBadMask),\n+    _ZPointerLoadShift(const_cast<size_t*>(&ZPointerLoadShift)),\n+    _ZPointerMarkGoodMask(&ZPointerMarkGoodMask),\n+    _ZPointerMarkBadMask(&ZPointerMarkBadMask),\n+    _ZPointerStoreGoodMask(&ZPointerStoreGoodMask),\n+    _ZPointerStoreBadMask(&ZPointerStoreBadMask),\n","filename":"src\/hotspot\/share\/gc\/z\/vmStructs_z.cpp","additions":9,"deletions":8,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -33,0 +33,1 @@\n+#include \"gc\/z\/zPageType.hpp\"\n@@ -44,1 +45,1 @@\n-  uint32_t* _ZGlobalPhase;\n+  uintptr_t* _ZAddressOffsetMask;\n@@ -46,1 +47,3 @@\n-  uint32_t* _ZGlobalSeqNum;\n+  uintptr_t* _ZPointerLoadGoodMask;\n+  uintptr_t* _ZPointerLoadBadMask;\n+  size_t*    _ZPointerLoadShift;\n@@ -48,6 +51,5 @@\n-  uintptr_t* _ZAddressOffsetMask;\n-  uintptr_t* _ZAddressMetadataMask;\n-  uintptr_t* _ZAddressMetadataFinalizable;\n-  uintptr_t* _ZAddressGoodMask;\n-  uintptr_t* _ZAddressBadMask;\n-  uintptr_t* _ZAddressWeakBadMask;\n+  uintptr_t* _ZPointerMarkGoodMask;\n+  uintptr_t* _ZPointerMarkBadMask;\n+\n+  uintptr_t* _ZPointerStoreGoodMask;\n+  uintptr_t* _ZPointerStoreBadMask;\n@@ -63,1 +65,1 @@\n-#define VM_STRUCTS_ZGC(nonstatic_field, volatile_nonstatic_field, static_field)                      \\\n+#define VM_STRUCTS_Z(nonstatic_field, volatile_nonstatic_field, static_field)                        \\\n@@ -65,8 +67,9 @@\n-  nonstatic_field(ZGlobalsForVMStructs,         _ZGlobalPhase,        uint32_t*)                     \\\n-  nonstatic_field(ZGlobalsForVMStructs,         _ZGlobalSeqNum,       uint32_t*)                     \\\n-  nonstatic_field(ZGlobalsForVMStructs,         _ZAddressOffsetMask,  uintptr_t*)                    \\\n-  nonstatic_field(ZGlobalsForVMStructs,         _ZAddressMetadataMask, uintptr_t*)                   \\\n-  nonstatic_field(ZGlobalsForVMStructs,         _ZAddressMetadataFinalizable, uintptr_t*)            \\\n-  nonstatic_field(ZGlobalsForVMStructs,         _ZAddressGoodMask,    uintptr_t*)                    \\\n-  nonstatic_field(ZGlobalsForVMStructs,         _ZAddressBadMask,     uintptr_t*)                    \\\n-  nonstatic_field(ZGlobalsForVMStructs,         _ZAddressWeakBadMask, uintptr_t*)                    \\\n+                                                                                                     \\\n+  nonstatic_field(ZGlobalsForVMStructs,         _ZAddressOffsetMask,         uintptr_t*)             \\\n+  nonstatic_field(ZGlobalsForVMStructs,         _ZPointerLoadGoodMask,       uintptr_t*)             \\\n+  nonstatic_field(ZGlobalsForVMStructs,         _ZPointerLoadBadMask,        uintptr_t*)             \\\n+  nonstatic_field(ZGlobalsForVMStructs,         _ZPointerLoadShift,          size_t*)                \\\n+  nonstatic_field(ZGlobalsForVMStructs,         _ZPointerMarkGoodMask,       uintptr_t*)             \\\n+  nonstatic_field(ZGlobalsForVMStructs,         _ZPointerMarkBadMask,        uintptr_t*)             \\\n+  nonstatic_field(ZGlobalsForVMStructs,         _ZPointerStoreGoodMask,      uintptr_t*)             \\\n+  nonstatic_field(ZGlobalsForVMStructs,         _ZPointerStoreBadMask,       uintptr_t*)             \\\n@@ -74,1 +77,1 @@\n-  nonstatic_field(ZGlobalsForVMStructs,         _ZObjectAlignmentSmall, const int*)                  \\\n+  nonstatic_field(ZGlobalsForVMStructs,         _ZObjectAlignmentSmall,      const int*)             \\\n@@ -80,2 +83,0 @@\n-  nonstatic_field(ZHeap,                        _forwarding_table,    ZForwardingTable)              \\\n-  nonstatic_field(ZHeap,                        _relocate,            ZRelocate)                     \\\n@@ -83,2 +84,2 @@\n-  nonstatic_field(ZPage,                        _type,                const uint8_t)                 \\\n-  nonstatic_field(ZPage,                        _seqnum,              uint32_t)                      \\\n+  nonstatic_field(ZPage,                        _type,                const ZPageType)               \\\n+  volatile_nonstatic_field(ZPage,               _seqnum,              uint32_t)                      \\\n@@ -86,1 +87,1 @@\n-  volatile_nonstatic_field(ZPage,               _top,                 uintptr_t)                     \\\n+  volatile_nonstatic_field(ZPage,               _top,                 zoffset_end)                   \\\n@@ -99,2 +100,2 @@\n-  nonstatic_field(ZVirtualMemory,               _start,               const uintptr_t)               \\\n-  nonstatic_field(ZVirtualMemory,               _end,                 const uintptr_t)               \\\n+  nonstatic_field(ZVirtualMemory,               _start,               const zoffset)                 \\\n+  nonstatic_field(ZVirtualMemory,               _end,                 const zoffset_end)             \\\n@@ -109,5 +110,4 @@\n-#define VM_INT_CONSTANTS_ZGC(declare_constant, declare_constant_with_value)                          \\\n-  declare_constant(ZPhaseRelocate)                                                                   \\\n-  declare_constant(ZPageTypeSmall)                                                                   \\\n-  declare_constant(ZPageTypeMedium)                                                                  \\\n-  declare_constant(ZPageTypeLarge)                                                                   \\\n+#define VM_INT_CONSTANTS_Z(declare_constant, declare_constant_with_value)                            \\\n+  declare_constant(ZPageType::small)                                                                 \\\n+  declare_constant(ZPageType::medium)                                                                \\\n+  declare_constant(ZPageType::large)                                                                 \\\n@@ -117,1 +117,1 @@\n-#define VM_LONG_CONSTANTS_ZGC(declare_constant)                                                      \\\n+#define VM_LONG_CONSTANTS_Z(declare_constant)                                                        \\\n@@ -126,1 +126,3 @@\n-#define VM_TYPES_ZGC(declare_type, declare_toplevel_type, declare_integer_type)                      \\\n+#define VM_TYPES_Z(declare_type, declare_toplevel_type, declare_integer_type)                        \\\n+  declare_toplevel_type(zoffset)                                                                     \\\n+  declare_toplevel_type(zoffset_end)                                                                 \\\n@@ -132,0 +134,1 @@\n+  declare_toplevel_type(ZPageType)                                                                   \\\n","filename":"src\/hotspot\/share\/gc\/z\/vmStructs_z.hpp","additions":36,"deletions":33,"binary":false,"changes":69,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2021, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -31,1 +31,1 @@\n-  Atomic::release_store_fence(&_should_abort, true);\n+  Atomic::store(&_should_abort, true);\n","filename":"src\/hotspot\/share\/gc\/z\/zAbort.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2021, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2021, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -38,0 +38,8 @@\n+\/\/ Macro to execute a abortion check\n+#define abortpoint()               \\\n+  do {                             \\\n+    if (ZAbort::should_abort()) {  \\\n+      return;                      \\\n+    }                              \\\n+  } while (false)\n+\n","filename":"src\/hotspot\/share\/gc\/z\/zAbort.hpp","additions":9,"deletions":1,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2021, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -32,1 +32,1 @@\n-  return Atomic::load_acquire(&_should_abort);\n+  return Atomic::load(&_should_abort);\n","filename":"src\/hotspot\/share\/gc\/z\/zAbort.inline.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -25,2 +25,7 @@\n-#include \"gc\/z\/zAddress.hpp\"\n-#include \"gc\/z\/zGlobals.hpp\"\n+#include \"gc\/shared\/barrierSet.hpp\"\n+#include \"gc\/shared\/gc_globals.hpp\"\n+#include \"gc\/z\/zAddress.inline.hpp\"\n+#include \"gc\/z\/zVerify.hpp\"\n+#include \"oops\/oopsHierarchy.hpp\"\n+#include \"runtime\/java.hpp\"\n+#include \"utilities\/formatBuffer.hpp\"\n@@ -28,4 +33,68 @@\n-void ZAddress::set_good_mask(uintptr_t mask) {\n-  ZAddressGoodMask = mask;\n-  ZAddressBadMask = ZAddressGoodMask ^ ZAddressMetadataMask;\n-  ZAddressWeakBadMask = (ZAddressGoodMask | ZAddressMetadataRemapped | ZAddressMetadataFinalizable) ^ ZAddressMetadataMask;\n+size_t     ZAddressHeapBaseShift;\n+size_t     ZAddressHeapBase;\n+\n+size_t     ZAddressOffsetBits;\n+uintptr_t  ZAddressOffsetMask;\n+size_t     ZAddressOffsetMax;\n+\n+uintptr_t  ZPointerRemapped;\n+uintptr_t  ZPointerRemappedYoungMask;\n+uintptr_t  ZPointerRemappedOldMask;\n+uintptr_t  ZPointerMarkedYoung;\n+uintptr_t  ZPointerMarkedOld;\n+uintptr_t  ZPointerFinalizable;\n+uintptr_t  ZPointerRemembered;\n+\n+uintptr_t  ZPointerLoadGoodMask;\n+uintptr_t  ZPointerLoadBadMask;\n+\n+uintptr_t  ZPointerMarkGoodMask;\n+uintptr_t  ZPointerMarkBadMask;\n+\n+uintptr_t  ZPointerStoreGoodMask;\n+uintptr_t  ZPointerStoreBadMask;\n+\n+uintptr_t  ZPointerVectorLoadBadMask[8];\n+uintptr_t  ZPointerVectorStoreBadMask[8];\n+uintptr_t  ZPointerVectorStoreGoodMask[8];\n+\n+static uint32_t* ZPointerCalculateStoreGoodMaskLowOrderBitsAddr() {\n+  const uintptr_t addr = reinterpret_cast<uintptr_t>(&ZPointerStoreGoodMask);\n+  return reinterpret_cast<uint32_t*>(addr + ZPointerStoreGoodMaskLowOrderBitsOffset);\n+}\n+\n+uint32_t*  ZPointerStoreGoodMaskLowOrderBitsAddr = ZPointerCalculateStoreGoodMaskLowOrderBitsAddr();\n+\n+static void set_vector_mask(uintptr_t vector_mask[], uintptr_t mask) {\n+  for (int i = 0; i < 8; ++i) {\n+    vector_mask[i] = mask;\n+  }\n+}\n+\n+void ZGlobalsPointers::set_good_masks() {\n+  ZPointerRemapped = ZPointerRemappedOldMask & ZPointerRemappedYoungMask;\n+\n+  ZPointerLoadGoodMask  = ZPointer::remap_bits(ZPointerRemapped);\n+  ZPointerMarkGoodMask  = ZPointerLoadGoodMask | ZPointerMarkedYoung | ZPointerMarkedOld;\n+  ZPointerStoreGoodMask = ZPointerMarkGoodMask | ZPointerRemembered;\n+\n+  ZPointerLoadBadMask  = ZPointerLoadGoodMask  ^ ZPointerLoadMetadataMask;\n+  ZPointerMarkBadMask  = ZPointerMarkGoodMask  ^ ZPointerMarkMetadataMask;\n+  ZPointerStoreBadMask = ZPointerStoreGoodMask ^ ZPointerStoreMetadataMask;\n+\n+  set_vector_mask(ZPointerVectorLoadBadMask, ZPointerLoadBadMask);\n+  set_vector_mask(ZPointerVectorStoreBadMask, ZPointerStoreBadMask);\n+  set_vector_mask(ZPointerVectorStoreGoodMask, ZPointerStoreGoodMask);\n+\n+  pd_set_good_masks();\n+}\n+\n+static void initialize_check_oop_function() {\n+#ifdef CHECK_UNHANDLED_OOPS\n+  if (ZVerifyOops) {\n+    \/\/ Enable extra verification of usages of oops in oopsHierarchy.hpp\n+    check_oop_function = [](oopDesc* obj) {\n+      (void)to_zaddress(obj);\n+    };\n+  }\n+#endif\n@@ -34,1 +103,1 @@\n-void ZAddress::initialize() {\n+void ZGlobalsPointers::initialize() {\n@@ -39,2 +108,6 @@\n-  ZAddressMetadataShift = ZPlatformAddressMetadataShift();\n-  ZAddressMetadataMask = (((uintptr_t)1 << ZAddressMetadataBits) - 1) << ZAddressMetadataShift;\n+  \/\/ Check max supported heap size\n+  if (MaxHeapSize > ZAddressOffsetMax) {\n+    vm_exit_during_initialization(\n+        err_msg(\"Java heap too large (max supported heap size is \" SIZE_FORMAT \"G)\",\n+                ZAddressOffsetMax \/ G));\n+  }\n@@ -42,4 +115,20 @@\n-  ZAddressMetadataMarked0 = (uintptr_t)1 << (ZAddressMetadataShift + 0);\n-  ZAddressMetadataMarked1 = (uintptr_t)1 << (ZAddressMetadataShift + 1);\n-  ZAddressMetadataRemapped = (uintptr_t)1 << (ZAddressMetadataShift + 2);\n-  ZAddressMetadataFinalizable = (uintptr_t)1 << (ZAddressMetadataShift + 3);\n+  ZAddressHeapBaseShift = ZPlatformAddressHeapBaseShift();\n+  ZAddressHeapBase = (uintptr_t)1 << ZAddressHeapBaseShift;\n+\n+  ZPointerRemappedYoungMask = ZPointerRemapped10 | ZPointerRemapped00;\n+  ZPointerRemappedOldMask = ZPointerRemapped01 | ZPointerRemapped00;\n+  ZPointerMarkedYoung = ZPointerMarkedYoung0;\n+  ZPointerMarkedOld = ZPointerMarkedOld0;\n+  ZPointerFinalizable = ZPointerFinalizable0;\n+  ZPointerRemembered = ZPointerRemembered0;\n+\n+  set_good_masks();\n+\n+  initialize_check_oop_function();\n+}\n+\n+void ZGlobalsPointers::flip_young_mark_start() {\n+  ZPointerMarkedYoung ^= (ZPointerMarkedYoung0 | ZPointerMarkedYoung1);\n+  ZPointerRemembered ^= (ZPointerRemembered0 | ZPointerRemembered1);\n+  set_good_masks();\n+}\n@@ -47,2 +136,3 @@\n-  ZAddressMetadataMarked = ZAddressMetadataMarked0;\n-  set_good_mask(ZAddressMetadataRemapped);\n+void ZGlobalsPointers::flip_young_relocate_start() {\n+  ZPointerRemappedYoungMask ^= ZPointerRemappedMask;\n+  set_good_masks();\n@@ -51,3 +141,4 @@\n-void ZAddress::flip_to_marked() {\n-  ZAddressMetadataMarked ^= (ZAddressMetadataMarked0 | ZAddressMetadataMarked1);\n-  set_good_mask(ZAddressMetadataMarked);\n+void ZGlobalsPointers::flip_old_mark_start() {\n+  ZPointerMarkedOld ^= (ZPointerMarkedOld0 | ZPointerMarkedOld1);\n+  ZPointerFinalizable ^= (ZPointerFinalizable0 | ZPointerFinalizable1);\n+  set_good_masks();\n@@ -56,2 +147,3 @@\n-void ZAddress::flip_to_remapped() {\n-  set_good_mask(ZAddressMetadataRemapped);\n+void ZGlobalsPointers::flip_old_relocate_start() {\n+  ZPointerRemappedOldMask ^= ZPointerRemappedMask;\n+  set_good_masks();\n","filename":"src\/hotspot\/share\/gc\/z\/zAddress.cpp","additions":113,"deletions":21,"binary":false,"changes":134,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -29,0 +29,250 @@\n+#include CPU_HEADER(gc\/z\/zAddress)\n+\n+\/\/ One bit that denotes where the heap start. All uncolored\n+\/\/ oops have this bit set, plus an offset within the heap.\n+extern uintptr_t  ZAddressHeapBase;\n+extern uintptr_t  ZAddressHeapBaseShift;\n+\n+\/\/ Describes the maximal offset inside the heap.\n+extern size_t    ZAddressOffsetBits;\n+const  size_t    ZAddressOffsetShift = 0;\n+extern uintptr_t ZAddressOffsetMask;\n+extern size_t    ZAddressOffsetMax;\n+\n+\/\/ Layout of metadata bits in colored pointer \/ zpointer.\n+\/\/\n+\/\/ A zpointer is a combination of the address bits (heap base bit + offset)\n+\/\/ and two low-order metadata bytes, with the following layout:\n+\/\/\n+\/\/ RRRRMMmmFFrr0000\n+\/\/ ****               : Used by load barrier\n+\/\/ **********         : Used by mark barrier\n+\/\/ ************       : Used by store barrier\n+\/\/             ****   : Reserved bits\n+\/\/\n+\/\/ The table below describes what each color does.\n+\/\/\n+\/\/ +-------------+-------------------+--------------------------+\n+\/\/ | Bit pattern | Description       | Included colors          |\n+\/\/ +-------------+-------------------+--------------------------+\n+\/\/ |     rr      | Remembered bits   | Remembered[0, 1]         |\n+\/\/ +-------------+-------------------+--------------------------+\n+\/\/ |     FF      | Finalizable bits  | Finalizable[0, 1]        |\n+\/\/ +-------------+-------------------+--------------------------+\n+\/\/ |     mm      | Marked young bits | MarkedYoung[0, 1]        |\n+\/\/ +-------------+-------------------+--------------------------+\n+\/\/ |     MM      | Marked old bits   | MarkedOld[0, 1]          |\n+\/\/ +-------------+-------------------+--------------------------+\n+\/\/ |    RRRR     | Remapped bits     | Remapped[00, 01, 10, 11] |\n+\/\/ +-------------+-------------------+--------------------------+\n+\/\/\n+\/\/ The low order zero address bits sometimes overlap with the high order zero metadata\n+\/\/ bits, depending on the remapped bit being set.\n+\/\/\n+\/\/             vvv- overlapping address and metadata zeros\n+\/\/    aaa...aaa0001MMmmFFrr0000 = Remapped00 zpointer\n+\/\/\n+\/\/             vv-- overlapping address and metadata zeros\n+\/\/   aaa...aaa00010MMmmFFrr0000 = Remapped01 zpointer\n+\/\/\n+\/\/             v--- overlapping address and metadata zero\n+\/\/  aaa...aaa000100MMmmFFrr0000 = Remapped10 zpointer\n+\/\/\n+\/\/             ---- no overlapping address and metadata zeros\n+\/\/ aaa...aaa0001000MMmmFFrr0000 = Remapped11 zpointer\n+\/\/\n+\/\/ The overlapping is performed because the x86 JIT-compiled load barriers expect the\n+\/\/ address bits to start right after the load-good bit. It allows combining the good\n+\/\/ bit check and unmasking into a single speculative shift instruction. On AArch64 we\n+\/\/ don't do this, and hence there are no overlapping address and  metadata zeros there.\n+\/\/\n+\/\/ The remapped bits are notably not grouped into two sets of bits, one for the young\n+\/\/ collection and one for the old collection, like the other bits. The reason is that\n+\/\/ the load barrier is only compatible with bit patterns where there is a single zero in\n+\/\/ its bits of operation (the load metadata bit mask). Instead, the single bit that we\n+\/\/ set encodes the combined state of a conceptual RemappedYoung[0, 1] and\n+\/\/ RemappedOld[0, 1] pair. The encoding scheme is that the shift of the load good bit,\n+\/\/ minus the shift of the load metadata bit start encodes the numbers 0, 1, 2 and 3.\n+\/\/ These numbers in binary correspond to 00, 01, 10 and 11. The low order bit in said\n+\/\/ numbers correspond to the simulated RemappedYoung[0, 1] value, and the high order bit\n+\/\/ corresponds to the simulated RemappedOld[0, 1] value. On AArch64, the remap bits\n+\/\/ of zpointers are the complement of this bit. So there are 3 good bits and one bad bit\n+\/\/ instead. This lends itself better to AArch64 instructions.\n+\/\/\n+\/\/ We decide the bit to be taken by having the RemappedYoungMask and RemappedOldMask\n+\/\/ variables, which alternate between what two bits they accept for their corresponding\n+\/\/ old and young phase. The Remapped bit is chosen by taking the intersection of those\n+\/\/ two variables.\n+\/\/\n+\/\/ RemappedOldMask alternates between these two bit patterns:\n+\/\/\n+\/\/  RemappedOld0 => 0011\n+\/\/  RemappedOld1 => 1100\n+\/\/\n+\/\/ RemappedYoungMask alternates between these two bit patterns:\n+\/\/\n+\/\/  RemappedYoung0 => 0101\n+\/\/  RemappedYoung1 => 1010\n+\/\/\n+\/\/ The corresponding intersections look like this:\n+\/\/\n+\/\/  RemappedOld0 & RemappedYoung0 = 0001 = Remapped00\n+\/\/  RemappedOld0 & RemappedYoung1 = 0010 = Remapped01\n+\/\/  RemappedOld1 & RemappedYoung0 = 0100 = Remapped10\n+\/\/  RemappedOld1 & RemappedYoung1 = 1000 = Remapped11\n+\n+constexpr uintptr_t z_pointer_mask(size_t shift, size_t bits) {\n+  return (((uintptr_t)1 << bits) - 1) << shift;\n+}\n+\n+constexpr uintptr_t z_pointer_bit(size_t shift, size_t offset) {\n+  return (uintptr_t)1 << (shift + offset);\n+}\n+\n+\/\/ Reserved bits\n+const size_t      ZPointerReservedShift   = 0;\n+const size_t      ZPointerReservedBits    = 4;\n+const uintptr_t   ZPointerReservedMask    = z_pointer_mask(ZPointerReservedShift, ZPointerReservedBits);\n+\n+const uintptr_t   ZPointerReserved0       = z_pointer_bit(ZPointerReservedShift, 0);\n+const uintptr_t   ZPointerReserved1       = z_pointer_bit(ZPointerReservedShift, 1);\n+const uintptr_t   ZPointerReserved2       = z_pointer_bit(ZPointerReservedShift, 2);\n+const uintptr_t   ZPointerReserved3       = z_pointer_bit(ZPointerReservedShift, 3);\n+\n+\/\/ Remembered set bits\n+const size_t      ZPointerRememberedShift = ZPointerReservedShift + ZPointerReservedBits;\n+const size_t      ZPointerRememberedBits  = 2;\n+const uintptr_t   ZPointerRememberedMask  = z_pointer_mask(ZPointerRememberedShift, ZPointerRememberedBits);\n+\n+const uintptr_t   ZPointerRemembered0     = z_pointer_bit(ZPointerRememberedShift, 0);\n+const uintptr_t   ZPointerRemembered1     = z_pointer_bit(ZPointerRememberedShift, 1);\n+\n+\/\/ Marked bits\n+const size_t      ZPointerMarkedShift     = ZPointerRememberedShift + ZPointerRememberedBits;\n+const size_t      ZPointerMarkedBits      = 6;\n+const uintptr_t   ZPointerMarkedMask      = z_pointer_mask(ZPointerMarkedShift, ZPointerMarkedBits);\n+\n+const uintptr_t   ZPointerFinalizable0    = z_pointer_bit(ZPointerMarkedShift, 0);\n+const uintptr_t   ZPointerFinalizable1    = z_pointer_bit(ZPointerMarkedShift, 1);\n+const uintptr_t   ZPointerMarkedYoung0    = z_pointer_bit(ZPointerMarkedShift, 2);\n+const uintptr_t   ZPointerMarkedYoung1    = z_pointer_bit(ZPointerMarkedShift, 3);\n+const uintptr_t   ZPointerMarkedOld0      = z_pointer_bit(ZPointerMarkedShift, 4);\n+const uintptr_t   ZPointerMarkedOld1      = z_pointer_bit(ZPointerMarkedShift, 5);\n+\n+\/\/ Remapped bits\n+const size_t      ZPointerRemappedShift   = ZPointerMarkedShift + ZPointerMarkedBits;\n+const size_t      ZPointerRemappedBits    = 4;\n+const uintptr_t   ZPointerRemappedMask    = z_pointer_mask(ZPointerRemappedShift, ZPointerRemappedBits);\n+\n+const uintptr_t   ZPointerRemapped00      = z_pointer_bit(ZPointerRemappedShift, 0);\n+const uintptr_t   ZPointerRemapped01      = z_pointer_bit(ZPointerRemappedShift, 1);\n+const uintptr_t   ZPointerRemapped10      = z_pointer_bit(ZPointerRemappedShift, 2);\n+const uintptr_t   ZPointerRemapped11      = z_pointer_bit(ZPointerRemappedShift, 3);\n+\n+\/\/ The shift table is tightly coupled with the zpointer layout given above\n+constexpr int     ZPointerLoadShiftTable[] = {\n+  ZPointerRemappedShift + ZPointerRemappedShift, \/\/ [0] Null\n+  ZPointerRemappedShift + 1,                     \/\/ [1] Remapped00\n+  ZPointerRemappedShift + 2,                     \/\/ [2] Remapped01\n+  0,\n+  ZPointerRemappedShift + 3,                     \/\/ [4] Remapped10\n+  0,\n+  0,\n+  0,\n+  ZPointerRemappedShift + 4                      \/\/ [8] Remapped11\n+};\n+\n+\/\/ Barrier metadata masks\n+const uintptr_t   ZPointerLoadMetadataMask  = ZPointerRemappedMask;\n+const uintptr_t   ZPointerMarkMetadataMask  = ZPointerLoadMetadataMask | ZPointerMarkedMask;\n+const uintptr_t   ZPointerStoreMetadataMask = ZPointerMarkMetadataMask | ZPointerRememberedMask;\n+const uintptr_t   ZPointerAllMetadataMask   = ZPointerStoreMetadataMask;\n+\n+\/\/ The current expected bit\n+extern uintptr_t  ZPointerRemapped;\n+extern uintptr_t  ZPointerMarkedOld;\n+extern uintptr_t  ZPointerMarkedYoung;\n+extern uintptr_t  ZPointerFinalizable;\n+extern uintptr_t  ZPointerRemembered;\n+\n+\/\/ The current expected remap bit for the young (or old) collection is either of two bits.\n+\/\/ The other collection alternates the bits, so we need to use a mask.\n+extern uintptr_t  ZPointerRemappedYoungMask;\n+extern uintptr_t  ZPointerRemappedOldMask;\n+\n+\/\/ Good\/bad masks\n+extern uintptr_t  ZPointerLoadGoodMask;\n+extern uintptr_t  ZPointerLoadBadMask;\n+\n+extern uintptr_t  ZPointerMarkGoodMask;\n+extern uintptr_t  ZPointerMarkBadMask;\n+\n+extern uintptr_t  ZPointerStoreGoodMask;\n+extern uintptr_t  ZPointerStoreBadMask;\n+\n+extern uintptr_t  ZPointerVectorLoadBadMask[8];\n+extern uintptr_t  ZPointerVectorStoreBadMask[8];\n+extern uintptr_t  ZPointerVectorStoreGoodMask[8];\n+\n+\/\/ The bad mask is 64 bit. Its low order 32 bits contain all possible value combinations\n+\/\/ that this mask will have. Therefore, the memory where the 32 low order bits are stored\n+\/\/ can be used as a 32 bit GC epoch counter, that has a different bit pattern every time\n+\/\/ the bad mask is flipped. This provides a pointer to such 32 bits.\n+extern uint32_t*  ZPointerStoreGoodMaskLowOrderBitsAddr;\n+const int         ZPointerStoreGoodMaskLowOrderBitsOffset = LITTLE_ENDIAN_ONLY(0) BIG_ENDIAN_ONLY(4);\n+\n+\/\/ Offsets\n+\/\/ - Virtual address range offsets\n+\/\/ - Physical memory offsets\n+enum class zoffset         : uintptr_t {};\n+\/\/ Offsets including end of offset range\n+enum class zoffset_end     : uintptr_t {};\n+\n+\/\/ Colored oop\n+enum class zpointer        : uintptr_t { null = 0 };\n+\n+\/\/ Uncolored oop - safe to dereference\n+enum class zaddress        : uintptr_t { null = 0 };\n+\n+\/\/ Uncolored oop - not safe to dereference, could point uncommitted memory\n+enum class zaddress_unsafe : uintptr_t { null = 0 };\n+\n+class ZOffset : public AllStatic {\n+public:\n+  static zaddress address(zoffset offset);\n+  static zaddress_unsafe address_unsafe(zoffset offset);\n+};\n+\n+class ZPointer : public AllStatic {\n+public:\n+  static zaddress uncolor(zpointer ptr);\n+  static zaddress uncolor_store_good(zpointer ptr);\n+  static zaddress_unsafe uncolor_unsafe(zpointer ptr);\n+  static zpointer set_remset_bits(zpointer ptr);\n+\n+  static bool is_load_bad(zpointer ptr);\n+  static bool is_load_good(zpointer ptr);\n+  static bool is_load_good_or_null(zpointer ptr);\n+\n+  static bool is_old_load_good(zpointer ptr);\n+  static bool is_young_load_good(zpointer ptr);\n+\n+  static bool is_mark_bad(zpointer ptr);\n+  static bool is_mark_good(zpointer ptr);\n+  static bool is_mark_good_or_null(zpointer ptr);\n+\n+  static bool is_store_bad(zpointer ptr);\n+  static bool is_store_good(zpointer ptr);\n+  static bool is_store_good_or_null(zpointer ptr);\n+\n+  static bool is_marked_finalizable(zpointer ptr);\n+  static bool is_marked_old(zpointer ptr);\n+  static bool is_marked_young(zpointer ptr);\n+  static bool is_marked_any_old(zpointer ptr);\n+  static bool is_remapped(zpointer ptr);\n+  static bool is_remembered_exact(zpointer ptr);\n+\n+  static constexpr int load_shift_lookup_index(uintptr_t value);\n+  static constexpr int load_shift_lookup(uintptr_t value);\n+  static uintptr_t remap_bits(uintptr_t colored);\n+};\n@@ -31,0 +281,17 @@\n+public:\n+  static zpointer color(zaddress addr, uintptr_t color);\n+  static zpointer color(zaddress_unsafe addr, uintptr_t color);\n+\n+  static zoffset offset(zaddress addr);\n+  static zoffset offset(zaddress_unsafe addr);\n+\n+  static zpointer load_good(zaddress addr, zpointer prev);\n+  static zpointer finalizable_good(zaddress addr, zpointer prev);\n+  static zpointer mark_good(zaddress addr, zpointer prev);\n+  static zpointer mark_old_good(zaddress addr, zpointer prev);\n+  static zpointer mark_young_good(zaddress addr, zpointer prev);\n+  static zpointer store_good(zaddress addr);\n+  static zpointer store_good_or_null(zaddress addr);\n+};\n+\n+class ZGlobalsPointers : public AllStatic {\n@@ -34,1 +301,2 @@\n-  static void set_good_mask(uintptr_t mask);\n+  static void set_good_masks();\n+  static void pd_set_good_masks();\n@@ -39,26 +307,4 @@\n-  static void flip_to_marked();\n-  static void flip_to_remapped();\n-\n-  static bool is_null(uintptr_t value);\n-  static bool is_bad(uintptr_t value);\n-  static bool is_good(uintptr_t value);\n-  static bool is_good_or_null(uintptr_t value);\n-  static bool is_weak_bad(uintptr_t value);\n-  static bool is_weak_good(uintptr_t value);\n-  static bool is_weak_good_or_null(uintptr_t value);\n-  static bool is_marked(uintptr_t value);\n-  static bool is_marked_or_null(uintptr_t value);\n-  static bool is_finalizable(uintptr_t value);\n-  static bool is_finalizable_good(uintptr_t value);\n-  static bool is_remapped(uintptr_t value);\n-  static bool is_in(uintptr_t value);\n-\n-  static uintptr_t offset(uintptr_t value);\n-  static uintptr_t good(uintptr_t value);\n-  static uintptr_t good_or_null(uintptr_t value);\n-  static uintptr_t finalizable_good(uintptr_t value);\n-  static uintptr_t marked(uintptr_t value);\n-  static uintptr_t marked0(uintptr_t value);\n-  static uintptr_t marked1(uintptr_t value);\n-  static uintptr_t remapped(uintptr_t value);\n-  static uintptr_t remapped_or_null(uintptr_t value);\n+  static void flip_young_mark_start();\n+  static void flip_young_relocate_start();\n+  static void flip_old_mark_start();\n+  static void flip_old_relocate_start();\n","filename":"src\/hotspot\/share\/gc\/z\/zAddress.hpp","additions":274,"deletions":28,"binary":false,"changes":302,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -29,1 +29,4 @@\n-#include \"gc\/z\/zGlobals.hpp\"\n+#include \"gc\/shared\/gc_globals.hpp\"\n+#include \"oops\/oop.hpp\"\n+#include \"oops\/oopsHierarchy.hpp\"\n+#include \"runtime\/atomic.hpp\"\n@@ -33,0 +36,1 @@\n+#include CPU_HEADER_INLINE(gc\/z\/zAddress)\n@@ -34,2 +38,76 @@\n-inline bool ZAddress::is_null(uintptr_t value) {\n-  return value == 0;\n+\/\/ zoffset functions\n+\n+inline uintptr_t untype(zoffset offset) {\n+  const uintptr_t value = static_cast<uintptr_t>(offset);\n+  assert(value < ZAddressOffsetMax, \"must have no other bits\");\n+  return value;\n+}\n+\n+inline uintptr_t untype(zoffset_end offset) {\n+  const uintptr_t value = static_cast<uintptr_t>(offset);\n+  assert(value <= ZAddressOffsetMax, \"must have no other bits\");\n+  return value;\n+}\n+\n+inline zoffset to_zoffset(uintptr_t value) {\n+  assert(value < ZAddressOffsetMax, \"must have no other bits\");\n+  return zoffset(value);\n+}\n+\n+inline zoffset to_zoffset(zoffset_end offset) {\n+  const uintptr_t value = untype(offset);\n+  return to_zoffset(value);\n+}\n+\n+inline zoffset operator+(zoffset offset, size_t size) {\n+  return to_zoffset(untype(offset) + size);\n+}\n+\n+inline zoffset& operator+=(zoffset& offset, size_t size) {\n+  offset = to_zoffset(untype(offset) + size);\n+  return offset;\n+}\n+\n+inline zoffset operator-(zoffset offset, size_t size) {\n+  const uintptr_t value = untype(offset) - size;\n+  return to_zoffset(value);\n+}\n+\n+inline size_t operator-(zoffset left, zoffset right) {\n+  const size_t diff = untype(left) - untype(right);\n+  assert(diff < ZAddressOffsetMax, \"Underflow\");\n+  return diff;\n+}\n+\n+inline zoffset& operator-=(zoffset& offset, size_t size) {\n+  offset = to_zoffset(untype(offset) - size);\n+  return offset;\n+}\n+\n+inline bool to_zoffset_end(zoffset_end* result, zoffset_end start, size_t size) {\n+  const uintptr_t value = untype(start) + size;\n+  if (value <= ZAddressOffsetMax) {\n+    *result = zoffset_end(value);\n+    return true;\n+  }\n+  return false;\n+}\n+\n+inline zoffset_end to_zoffset_end(zoffset start, size_t size) {\n+  const uintptr_t value = untype(start) + size;\n+  assert(value <= ZAddressOffsetMax, \"Overflow start: \" PTR_FORMAT \" size: \" PTR_FORMAT \" value: \" PTR_FORMAT,\n+                                     untype(start), size, value);\n+  return zoffset_end(value);\n+}\n+\n+inline zoffset_end to_zoffset_end(uintptr_t value) {\n+  assert(value <= ZAddressOffsetMax, \"Overflow\");\n+  return zoffset_end(value);\n+}\n+\n+inline zoffset_end to_zoffset_end(zoffset offset) {\n+  return zoffset_end(untype(offset));\n+}\n+\n+inline bool operator!=(zoffset first, zoffset_end second) {\n+  return untype(first) != untype(second);\n@@ -38,2 +116,2 @@\n-inline bool ZAddress::is_bad(uintptr_t value) {\n-  return value & ZAddressBadMask;\n+inline bool operator!=(zoffset_end first, zoffset second) {\n+  return untype(first) != untype(second);\n@@ -42,2 +120,2 @@\n-inline bool ZAddress::is_good(uintptr_t value) {\n-  return !is_bad(value) && !is_null(value);\n+inline bool operator==(zoffset first, zoffset_end second) {\n+  return untype(first) == untype(second);\n@@ -46,1 +124,349 @@\n-inline bool ZAddress::is_good_or_null(uintptr_t value) {\n+inline bool operator==(zoffset_end first, zoffset second) {\n+  return untype(first) == untype(second);\n+}\n+\n+inline bool operator<(zoffset_end first, zoffset second) {\n+  return untype(first) < untype(second);\n+}\n+\n+inline bool operator<(zoffset first, zoffset_end second) {\n+  return untype(first) < untype(second);\n+}\n+\n+inline bool operator>(zoffset first, zoffset_end second) {\n+  return untype(first) > untype(second);\n+}\n+\n+inline bool operator>=(zoffset first, zoffset_end second) {\n+  return untype(first) >= untype(second);\n+}\n+\n+inline size_t operator-(zoffset_end first, zoffset second) {\n+  return untype(first) - untype(second);\n+}\n+\n+inline zoffset_end operator-(zoffset_end first, size_t second) {\n+  return to_zoffset_end(untype(first) - second);\n+}\n+\n+inline size_t operator-(zoffset_end first, zoffset_end second) {\n+  return untype(first) - untype(second);\n+}\n+\n+inline zoffset_end& operator-=(zoffset_end& offset, size_t size) {\n+  offset = to_zoffset_end(untype(offset) - size);\n+  return offset;\n+}\n+\n+inline zoffset_end& operator+=(zoffset_end& offset, size_t size) {\n+  offset = to_zoffset_end(untype(offset) + size);\n+  return offset;\n+}\n+\n+\/\/ zpointer functions\n+\n+#define report_is_valid_failure(str) assert(!assert_on_failure, \"%s: \" PTR_FORMAT, str, value);\n+\n+inline bool is_valid(zpointer ptr, bool assert_on_failure = false) {\n+  if (assert_on_failure && !ZVerifyOops) {\n+    return true;\n+  }\n+\n+  const uintptr_t value = static_cast<uintptr_t>(ptr);\n+\n+  if (value == 0) {\n+    \/\/ Accept raw null\n+    return false;\n+  }\n+\n+  if ((value & ~ZPointerStoreMetadataMask) != 0) {\n+#ifndef AARCH64\n+    const int index = ZPointer::load_shift_lookup_index(value);\n+    if (index != 0 && !is_power_of_2(index)) {\n+      report_is_valid_failure(\"Invalid remap bits\");\n+      return false;\n+    }\n+#endif\n+\n+    const int shift = ZPointer::load_shift_lookup(value);\n+    if (!is_power_of_2(value & (ZAddressHeapBase << shift))) {\n+      report_is_valid_failure(\"Missing heap base\");\n+      return false;\n+    }\n+\n+    if (((value >> shift) & 7) != 0) {\n+      report_is_valid_failure(\"Alignment bits should not be set\");\n+      return false;\n+    }\n+  }\n+\n+  const uintptr_t load_metadata = ZPointer::remap_bits(value);\n+  if (!is_power_of_2(load_metadata)) {\n+    report_is_valid_failure(\"Must have exactly one load metadata bit\");\n+    return false;\n+  }\n+\n+  const uintptr_t store_metadata = (value & (ZPointerStoreMetadataMask ^ ZPointerLoadMetadataMask));\n+  const uintptr_t marked_young_metadata = store_metadata & (ZPointerMarkedYoung0 | ZPointerMarkedYoung1);\n+  const uintptr_t marked_old_metadata = store_metadata & (ZPointerMarkedOld0 | ZPointerMarkedOld1 |\n+                                                          ZPointerFinalizable0 | ZPointerFinalizable1);\n+  const uintptr_t remembered_metadata = store_metadata & (ZPointerRemembered0 | ZPointerRemembered1);\n+  if (!is_power_of_2(marked_young_metadata)) {\n+    report_is_valid_failure(\"Must have exactly one marked young metadata bit\");\n+    return false;\n+  }\n+\n+  if (!is_power_of_2(marked_old_metadata)) {\n+    report_is_valid_failure(\"Must have exactly one marked old metadata bit\");\n+    return false;\n+  }\n+\n+  if (remembered_metadata == 0) {\n+    report_is_valid_failure(\"Must have at least one remembered metadata bit set\");\n+    return false;\n+  }\n+\n+  if ((marked_young_metadata | marked_old_metadata | remembered_metadata) != store_metadata) {\n+    report_is_valid_failure(\"Must have exactly three sets of store metadata bits\");\n+    return false;\n+  }\n+\n+  if ((value & ZPointerReservedMask) != 0) {\n+    report_is_valid_failure(\"Dirty reserved bits\");\n+    return false;\n+  }\n+\n+  return true;\n+}\n+\n+inline void assert_is_valid(zpointer ptr) {\n+  DEBUG_ONLY(is_valid(ptr, true \/* assert_on_failure *\/);)\n+}\n+\n+inline uintptr_t untype(zpointer ptr) {\n+  return static_cast<uintptr_t>(ptr);\n+}\n+\n+inline zpointer to_zpointer(uintptr_t value) {\n+  assert_is_valid(zpointer(value));\n+  return zpointer(value);\n+}\n+\n+inline zpointer to_zpointer(oopDesc* o) {\n+  return ::to_zpointer(uintptr_t(o));\n+}\n+\n+\/\/ Is it exactly null?\n+inline bool is_null(zpointer ptr) {\n+  return ptr == zpointer::null;\n+}\n+\n+inline bool is_null_any(zpointer ptr) {\n+  const uintptr_t raw_addr = untype(ptr);\n+  return (raw_addr & ~ZPointerAllMetadataMask) == 0;\n+}\n+\n+\/\/ Is it null - colored or not?\n+inline bool is_null_assert_load_good(zpointer ptr) {\n+  const bool result = is_null_any(ptr);\n+  assert(!result || ZPointer::is_load_good(ptr), \"Got bad colored null\");\n+  return result;\n+}\n+\n+\/\/ zaddress functions\n+\n+inline bool is_null(zaddress addr) {\n+  return addr == zaddress::null;\n+}\n+\n+inline bool is_valid(zaddress addr, bool assert_on_failure = false) {\n+  if (assert_on_failure && !ZVerifyOops) {\n+    return true;\n+  }\n+\n+  if (is_null(addr)) {\n+    \/\/ Null is valid\n+    return true;\n+  }\n+\n+  const uintptr_t value = static_cast<uintptr_t>(addr);\n+\n+  if (value & 0x7) {\n+    \/\/ No low order bits\n+    report_is_valid_failure(\"Has low-order bits set\");\n+    return false;\n+  }\n+\n+  if ((value & ZAddressHeapBase) == 0) {\n+    \/\/ Must have a heap base bit\n+    report_is_valid_failure(\"Missing heap base\");\n+    return false;\n+  }\n+\n+  if (value >= (ZAddressHeapBase + ZAddressOffsetMax)) {\n+    \/\/ Must not point outside of the heap's virtual address range\n+    report_is_valid_failure(\"Address outside of the heap\");\n+    return false;\n+  }\n+\n+  return true;\n+}\n+\n+inline void assert_is_valid(zaddress addr) {\n+  DEBUG_ONLY(is_valid(addr, true \/* assert_on_failure *\/);)\n+}\n+\n+inline uintptr_t untype(zaddress addr) {\n+  return static_cast<uintptr_t>(addr);\n+}\n+\n+#ifdef ASSERT\n+inline void dereferenceable_test(zaddress addr) {\n+  if (ZVerifyOops && !is_null(addr)) {\n+    \/\/ Intentionally crash if the address is pointing into unmapped memory\n+    (void)Atomic::load((int*)(uintptr_t)addr);\n+  }\n+}\n+#endif\n+\n+inline zaddress to_zaddress(uintptr_t value) {\n+  const zaddress addr = zaddress(value);\n+  assert_is_valid(addr);\n+  DEBUG_ONLY(dereferenceable_test(addr));\n+  return addr;\n+}\n+\n+inline zaddress to_zaddress(oopDesc* o) {\n+  return to_zaddress(uintptr_t(o));\n+}\n+\n+inline oop to_oop(zaddress addr) {\n+  const oop obj = cast_to_oop(addr);\n+  assert(!ZVerifyOops || oopDesc::is_oop_or_null(obj), \"Broken oop: \" PTR_FORMAT \" [\" PTR_FORMAT \" \" PTR_FORMAT \" \" PTR_FORMAT \" \" PTR_FORMAT \"]\",\n+         p2i(obj),\n+         *(uintptr_t*)(untype(addr) + 0x00),\n+         *(uintptr_t*)(untype(addr) + 0x08),\n+         *(uintptr_t*)(untype(addr) + 0x10),\n+         *(uintptr_t*)(untype(addr) + 0x18));\n+  return obj;\n+}\n+\n+inline zaddress operator+(zaddress addr, size_t size) {\n+  return to_zaddress(untype(addr) + size);\n+}\n+\n+inline size_t operator-(zaddress left, zaddress right) {\n+  assert(left >= right, \"Unexpected order - left: \" PTR_FORMAT \" right: \" PTR_FORMAT, untype(left), untype(right));\n+  return untype(left) - untype(right);\n+}\n+\n+\/\/ zaddress_unsafe functions\n+\n+inline bool is_null(zaddress_unsafe addr) {\n+  return addr == zaddress_unsafe::null;\n+}\n+\n+inline bool is_valid(zaddress_unsafe addr, bool assert_on_failure = false) {\n+  return is_valid(zaddress(addr), assert_on_failure);\n+}\n+\n+inline void assert_is_valid(zaddress_unsafe addr) {\n+  DEBUG_ONLY(is_valid(addr, true \/* assert_on_failure *\/);)\n+}\n+\n+\n+inline uintptr_t untype(zaddress_unsafe addr) {\n+  return static_cast<uintptr_t>(addr);\n+}\n+\n+\/\/ The zaddress_unsafe type denotes that this\n+\/\/ memory isn't guaranteed to be dereferenceable.\n+\/\/ The containing page could have been reclaimed\n+\/\/ and\/or uncommitted.\n+\/\/\n+\/\/ The zaddress type denotes that this memory can\n+\/\/ be dereferenced (runtime verified).\n+\/\/\n+\/\/ This function can be used when the caller guarantees\n+\/\/ that addr points to dereferenceable memory. Examples\n+\/\/ of cases after which this function can be used:\n+\/\/\n+\/\/ 1) A load good check on the colored pointer that addr was created from\n+\/\/ 2) A load barrier has self-healed the pointer in addr\n+\/\/ 3) A check that the addr doesn't belong to a relocation set. Since addr\n+\/\/    could denote two different objects in the two generations, a check\n+\/\/    against the colored pointer, that addr was created from, is needed to\n+\/\/    figure out what relocation set to look in.\n+\/\/ 4) From the relocation code\n+inline zaddress safe(zaddress_unsafe addr) {\n+  return to_zaddress(untype(addr));\n+}\n+\n+inline zaddress_unsafe to_zaddress_unsafe(uintptr_t value) {\n+  const zaddress_unsafe addr = zaddress_unsafe(value);\n+  assert_is_valid(addr);\n+  return addr;\n+}\n+\n+inline zaddress_unsafe unsafe(zaddress addr) {\n+  return to_zaddress_unsafe(untype(addr));\n+}\n+\n+inline zaddress_unsafe to_zaddress_unsafe(oop o) {\n+  return to_zaddress_unsafe(cast_from_oop<uintptr_t>(o));\n+}\n+\n+inline zaddress_unsafe operator+(zaddress_unsafe offset, size_t size) {\n+  return to_zaddress_unsafe(untype(offset) + size);\n+}\n+\n+inline size_t operator-(zaddress_unsafe left, zaddress_unsafe right) {\n+  return untype(left) - untype(right);\n+}\n+\n+\/\/ ZOffset functions\n+\n+inline zaddress ZOffset::address(zoffset offset) {\n+  return to_zaddress(untype(offset) | ZAddressHeapBase);\n+}\n+\n+inline zaddress_unsafe ZOffset::address_unsafe(zoffset offset) {\n+  return to_zaddress_unsafe(untype(offset) | ZAddressHeapBase);\n+}\n+\n+\/\/ ZPointer functions\n+\n+inline zaddress ZPointer::uncolor(zpointer ptr) {\n+  assert(ZPointer::is_load_good(ptr) || is_null_any(ptr),\n+      \"Should be load good when handed out: \" PTR_FORMAT, untype(ptr));\n+  const uintptr_t raw_addr = untype(ptr);\n+  return to_zaddress(raw_addr >> ZPointer::load_shift_lookup(raw_addr));\n+}\n+\n+inline zaddress ZPointer::uncolor_store_good(zpointer ptr) {\n+  assert(ZPointer::is_store_good(ptr), \"Should be store good: \" PTR_FORMAT, untype(ptr));\n+  return uncolor(ptr);\n+}\n+\n+inline zaddress_unsafe ZPointer::uncolor_unsafe(zpointer ptr) {\n+  assert(ZPointer::is_store_bad(ptr), \"Unexpected ptr\");\n+  const uintptr_t raw_addr = untype(ptr);\n+  return to_zaddress_unsafe(raw_addr >> ZPointer::load_shift_lookup(raw_addr));\n+}\n+\n+inline zpointer ZPointer::set_remset_bits(zpointer ptr) {\n+  uintptr_t raw_addr = untype(ptr);\n+  assert(raw_addr != 0, \"raw nulls should have been purged in promotion to old gen\");\n+  raw_addr |= ZPointerRemembered0 | ZPointerRemembered1;\n+  return to_zpointer(raw_addr);\n+}\n+\n+inline bool ZPointer::is_load_bad(zpointer ptr) {\n+  return untype(ptr) & ZPointerLoadBadMask;\n+}\n+\n+inline bool ZPointer::is_load_good(zpointer ptr) {\n+  return !is_load_bad(ptr) && !is_null(ptr);\n+}\n+\n+inline bool ZPointer::is_load_good_or_null(zpointer ptr) {\n@@ -54,2 +480,2 @@\n-  const bool result = !is_bad(value);\n-  assert((is_good(value) || is_null(value)) == result, \"Bad address\");\n+  const bool result = !is_load_bad(ptr);\n+  assert((is_load_good(ptr) || is_null(ptr)) == result, \"Bad address\");\n@@ -59,2 +485,8 @@\n-inline bool ZAddress::is_weak_bad(uintptr_t value) {\n-  return value & ZAddressWeakBadMask;\n+inline bool ZPointer::is_young_load_good(zpointer ptr) {\n+  assert(!is_null(ptr), \"not supported\");\n+  return (remap_bits(untype(ptr)) & ZPointerRemappedYoungMask) != 0;\n+}\n+\n+inline bool ZPointer::is_old_load_good(zpointer ptr) {\n+  assert(!is_null(ptr), \"not supported\");\n+  return (remap_bits(untype(ptr)) & ZPointerRemappedOldMask) != 0;\n@@ -63,2 +495,2 @@\n-inline bool ZAddress::is_weak_good(uintptr_t value) {\n-  return !is_weak_bad(value) && !is_null(value);\n+inline bool ZPointer::is_mark_bad(zpointer ptr) {\n+  return untype(ptr) & ZPointerMarkBadMask;\n@@ -67,2 +499,2 @@\n-inline bool ZAddress::is_weak_good_or_null(uintptr_t value) {\n-  return !is_weak_bad(value);\n+inline bool ZPointer::is_mark_good(zpointer ptr) {\n+  return !is_mark_bad(ptr) && !is_null(ptr);\n@@ -71,2 +503,11 @@\n-inline bool ZAddress::is_marked(uintptr_t value) {\n-  return value & ZAddressMetadataMarked;\n+inline bool ZPointer::is_mark_good_or_null(zpointer ptr) {\n+  \/\/ Checking if an address is \"not bad\" is an optimized version of\n+  \/\/ checking if it's \"good or null\", which eliminates an explicit\n+  \/\/ null check. However, the implicit null check only checks that\n+  \/\/ the mask bits are zero, not that the entire address is zero.\n+  \/\/ This means that an address without mask bits would pass through\n+  \/\/ the barrier as if it was null. This should be harmless as such\n+  \/\/ addresses should ever be passed through the barrier.\n+  const bool result = !is_mark_bad(ptr);\n+  assert((is_mark_good(ptr) || is_null(ptr)) == result, \"Bad address\");\n+  return result;\n@@ -75,2 +516,2 @@\n-inline bool ZAddress::is_marked_or_null(uintptr_t value) {\n-  return is_marked(value) || is_null(value);\n+inline bool ZPointer::is_store_bad(zpointer ptr) {\n+  return untype(ptr) & ZPointerStoreBadMask;\n@@ -79,2 +520,2 @@\n-inline bool ZAddress::is_finalizable(uintptr_t value) {\n-  return value & ZAddressMetadataFinalizable;\n+inline bool ZPointer::is_store_good(zpointer ptr) {\n+  return !is_store_bad(ptr) && !is_null(ptr);\n@@ -83,2 +524,11 @@\n-inline bool ZAddress::is_finalizable_good(uintptr_t value) {\n-  return is_finalizable(value) && is_good(value ^ ZAddressMetadataFinalizable);\n+inline bool ZPointer::is_store_good_or_null(zpointer ptr) {\n+  \/\/ Checking if an address is \"not bad\" is an optimized version of\n+  \/\/ checking if it's \"good or null\", which eliminates an explicit\n+  \/\/ null check. However, the implicit null check only checks that\n+  \/\/ the mask bits are zero, not that the entire address is zero.\n+  \/\/ This means that an address without mask bits would pass through\n+  \/\/ the barrier as if it was null. This should be harmless as such\n+  \/\/ addresses should ever be passed through the barrier.\n+  const bool result = !is_store_bad(ptr);\n+  assert((is_store_good(ptr) || is_null(ptr)) == result, \"Bad address\");\n+  return result;\n@@ -87,2 +537,3 @@\n-inline bool ZAddress::is_remapped(uintptr_t value) {\n-  return value & ZAddressMetadataRemapped;\n+inline bool ZPointer::is_marked_finalizable(zpointer ptr) {\n+  assert(!is_null(ptr), \"must not be null\");\n+  return untype(ptr) & ZPointerFinalizable;\n@@ -91,5 +542,3 @@\n-inline bool ZAddress::is_in(uintptr_t value) {\n-  \/\/ Check that exactly one non-offset bit is set\n-  if (!is_power_of_2(value & ~ZAddressOffsetMask)) {\n-    return false;\n-  }\n+inline bool ZPointer::is_marked_old(zpointer ptr) {\n+  return untype(ptr) & (ZPointerMarkedOld);\n+}\n@@ -97,2 +546,2 @@\n-  \/\/ Check that one of the non-finalizable metadata is set\n-  return value & (ZAddressMetadataMask & ~ZAddressMetadataFinalizable);\n+inline bool ZPointer::is_marked_young(zpointer ptr) {\n+  return untype(ptr) & (ZPointerMarkedYoung);\n@@ -101,2 +550,3 @@\n-inline uintptr_t ZAddress::offset(uintptr_t value) {\n-  return value & ZAddressOffsetMask;\n+inline bool ZPointer::is_marked_any_old(zpointer ptr) {\n+  return untype(ptr) & (ZPointerMarkedOld |\n+                        ZPointerFinalizable);\n@@ -105,2 +555,3 @@\n-inline uintptr_t ZAddress::good(uintptr_t value) {\n-  return offset(value) | ZAddressGoodMask;\n+inline bool ZPointer::is_remapped(zpointer ptr) {\n+  assert(!is_null(ptr), \"must not be null\");\n+  return remap_bits(untype(ptr)) & ZPointerRemapped;\n@@ -109,2 +560,3 @@\n-inline uintptr_t ZAddress::good_or_null(uintptr_t value) {\n-  return is_null(value) ? 0 : good(value);\n+inline bool ZPointer::is_remembered_exact(zpointer ptr) {\n+  assert(!is_null(ptr), \"must not be null\");\n+  return (untype(ptr) & ZPointerRemembered) == ZPointerRemembered;\n@@ -113,2 +565,2 @@\n-inline uintptr_t ZAddress::finalizable_good(uintptr_t value) {\n-  return offset(value) | ZAddressMetadataFinalizable | ZAddressGoodMask;\n+inline constexpr int ZPointer::load_shift_lookup_index(uintptr_t value) {\n+  return (value >> ZPointerRemappedShift) & ((1 << ZPointerRemappedBits) - 1);\n@@ -117,2 +569,4 @@\n-inline uintptr_t ZAddress::marked(uintptr_t value) {\n-  return offset(value) | ZAddressMetadataMarked;\n+\/\/ ZAddress functions\n+\n+inline zpointer ZAddress::color(zaddress addr, uintptr_t color) {\n+  return to_zpointer((untype(addr) << ZPointer::load_shift_lookup(color)) | color);\n@@ -121,2 +575,2 @@\n-inline uintptr_t ZAddress::marked0(uintptr_t value) {\n-  return offset(value) | ZAddressMetadataMarked0;\n+inline zpointer ZAddress::color(zaddress_unsafe addr, uintptr_t color) {\n+  return to_zpointer((untype(addr) << ZPointer::load_shift_lookup(color)) | color);\n@@ -125,2 +579,66 @@\n-inline uintptr_t ZAddress::marked1(uintptr_t value) {\n-  return offset(value) | ZAddressMetadataMarked1;\n+inline zoffset ZAddress::offset(zaddress addr) {\n+  return to_zoffset(untype(addr) & ZAddressOffsetMask);\n+}\n+\n+inline zoffset ZAddress::offset(zaddress_unsafe addr) {\n+  return to_zoffset(untype(addr) & ZAddressOffsetMask);\n+}\n+\n+inline zpointer color_null() {\n+  return ZAddress::color(zaddress::null, ZPointerStoreGoodMask | ZPointerRememberedMask);\n+}\n+\n+inline zpointer ZAddress::load_good(zaddress addr, zpointer prev) {\n+  if (is_null_any(prev)) {\n+    return color_null();\n+  }\n+\n+  const uintptr_t non_load_bits_mask = ZPointerLoadMetadataMask ^ ZPointerAllMetadataMask;\n+  const uintptr_t non_load_prev_bits = untype(prev) & non_load_bits_mask;\n+  return color(addr, ZPointerLoadGoodMask | non_load_prev_bits | ZPointerRememberedMask);\n+}\n+\n+inline zpointer ZAddress::finalizable_good(zaddress addr, zpointer prev) {\n+  if (is_null_any(prev)) {\n+    return color_null();\n+  }\n+\n+  const uintptr_t non_mark_bits_mask = ZPointerMarkMetadataMask ^ ZPointerAllMetadataMask;\n+  const uintptr_t non_mark_prev_bits = untype(prev) & non_mark_bits_mask;\n+  return color(addr, ZPointerLoadGoodMask | ZPointerMarkedYoung | ZPointerFinalizable | non_mark_prev_bits | ZPointerRememberedMask);\n+}\n+\n+inline zpointer ZAddress::mark_good(zaddress addr, zpointer prev) {\n+  if (is_null_any(prev)) {\n+    return color_null();\n+  }\n+\n+  const uintptr_t non_mark_bits_mask = ZPointerMarkMetadataMask ^ ZPointerAllMetadataMask;\n+  const uintptr_t non_mark_prev_bits = untype(prev) & non_mark_bits_mask;\n+  return color(addr, ZPointerLoadGoodMask | ZPointerMarkedYoung | ZPointerMarkedOld | non_mark_prev_bits | ZPointerRememberedMask);\n+}\n+\n+inline zpointer ZAddress::mark_old_good(zaddress addr, zpointer prev) {\n+  if (is_null_any(prev)) {\n+    return color_null();\n+  }\n+\n+  const uintptr_t prev_color = untype(prev);\n+\n+  const uintptr_t young_marked_mask = ZPointerMarkedYoung0 | ZPointerMarkedYoung1;\n+  const uintptr_t young_marked = prev_color & young_marked_mask;\n+\n+  return color(addr, ZPointerLoadGoodMask | ZPointerMarkedOld | young_marked | ZPointerRememberedMask);\n+}\n+\n+inline zpointer ZAddress::mark_young_good(zaddress addr, zpointer prev) {\n+  if (is_null_any(prev)) {\n+    return color_null();\n+  }\n+\n+  const uintptr_t prev_color = untype(prev);\n+\n+  const uintptr_t old_marked_mask = ZPointerMarkedMask ^ (ZPointerMarkedYoung0 | ZPointerMarkedYoung1);\n+  const uintptr_t old_marked = prev_color & old_marked_mask;\n+\n+  return color(addr, ZPointerLoadGoodMask | ZPointerMarkedYoung | old_marked | ZPointerRememberedMask);\n@@ -129,2 +647,2 @@\n-inline uintptr_t ZAddress::remapped(uintptr_t value) {\n-  return offset(value) | ZAddressMetadataRemapped;\n+inline zpointer ZAddress::store_good(zaddress addr) {\n+  return color(addr, ZPointerStoreGoodMask);\n@@ -133,2 +651,2 @@\n-inline uintptr_t ZAddress::remapped_or_null(uintptr_t value) {\n-  return is_null(value) ? 0 : remapped(value);\n+inline zpointer ZAddress::store_good_or_null(zaddress addr) {\n+  return is_null(addr) ? zpointer::null : store_good(addr);\n","filename":"src\/hotspot\/share\/gc\/z\/zAddress.inline.hpp","additions":570,"deletions":52,"binary":false,"changes":622,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2019, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2019, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -49,3 +49,3 @@\n-size_t ZAddressSpaceLimit::heap_view() {\n-  \/\/ Allow all heap views to occupy 50% of the address space\n-  const size_t limit = address_space_limit() \/ MaxVirtMemFraction \/ ZHeapViews;\n+size_t ZAddressSpaceLimit::heap() {\n+  \/\/ Allow the heap to occupy 50% of the address space\n+  const size_t limit = address_space_limit() \/ MaxVirtMemFraction;\n","filename":"src\/hotspot\/share\/gc\/z\/zAddressSpaceLimit.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2019, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2019, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -33,1 +33,1 @@\n-  static size_t heap_view();\n+  static size_t heap();\n","filename":"src\/hotspot\/share\/gc\/z\/zAddressSpaceLimit.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -41,1 +41,1 @@\n-\/\/  |     | * 1-1 Worker Relocation Flag (1-bit)\n+\/\/  |     | * 1-1 GC Relocation Flag (1-bit)\n@@ -51,1 +51,1 @@\n-  typedef ZBitField<uint8_t, bool, 1, 1> field_worker_relocation;\n+  typedef ZBitField<uint8_t, bool, 1, 1> field_gc_relocation;\n@@ -64,2 +64,2 @@\n-  void set_worker_relocation() {\n-    _flags |= field_worker_relocation::encode(true);\n+  void set_gc_relocation() {\n+    _flags |= field_gc_relocation::encode(true);\n@@ -76,2 +76,2 @@\n-  bool worker_relocation() const {\n-    return field_worker_relocation::decode(_flags);\n+  bool gc_relocation() const {\n+    return field_gc_relocation::decode(_flags);\n","filename":"src\/hotspot\/share\/gc\/z\/zAllocationFlags.hpp","additions":7,"deletions":7,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -0,0 +1,77 @@\n+\/*\n+ * Copyright (c) 2021, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/z\/zAllocator.hpp\"\n+#include \"gc\/z\/zObjectAllocator.hpp\"\n+\n+ZAllocatorEden*          ZAllocator::_eden;\n+ZAllocatorForRelocation* ZAllocator::_relocation[ZAllocator::_relocation_allocators];\n+\n+ZAllocator::ZAllocator(ZPageAge age) :\n+    _object_allocator(age) {}\n+\n+void ZAllocator::retire_pages() {\n+  _object_allocator.retire_pages();\n+}\n+\n+ZAllocatorEden::ZAllocatorEden() :\n+    ZAllocator(ZPageAge::eden) {\n+  ZAllocator::_eden = this;\n+}\n+\n+size_t ZAllocatorEden::tlab_used() const {\n+  return _object_allocator.used();\n+}\n+\n+size_t ZAllocatorEden::remaining() const {\n+  return _object_allocator.remaining();\n+}\n+\n+ZPageAge ZAllocatorForRelocation::install() {\n+  for (uint i = 0; i < ZAllocator::_relocation_allocators; ++i) {\n+    if (_relocation[i] == nullptr) {\n+      _relocation[i] = this;\n+      return static_cast<ZPageAge>(i + 1);\n+    }\n+  }\n+\n+  ShouldNotReachHere();\n+  return ZPageAge::eden;\n+}\n+\n+ZAllocatorForRelocation::ZAllocatorForRelocation() :\n+    ZAllocator(install()) {\n+}\n+\n+zaddress ZAllocatorForRelocation::alloc_object(size_t size) {\n+  return _object_allocator.alloc_object_for_relocation(size);\n+}\n+\n+void ZAllocatorForRelocation::undo_alloc_object(zaddress addr, size_t size) {\n+  _object_allocator.undo_alloc_object_for_relocation(addr, size);\n+}\n+\n+ZPage* ZAllocatorForRelocation::alloc_page_for_relocation(ZPageType type, size_t size, ZAllocationFlags flags) {\n+  return _object_allocator.alloc_page_for_relocation(type, size, flags);\n+}\n","filename":"src\/hotspot\/share\/gc\/z\/zAllocator.cpp","additions":77,"deletions":0,"binary":false,"changes":77,"status":"added"},{"patch":"@@ -0,0 +1,87 @@\n+\/*\n+ * Copyright (c) 2021, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_Z_ZALLOCATOR_HPP\n+#define SHARE_GC_Z_ZALLOCATOR_HPP\n+\n+#include \"gc\/z\/zAllocationFlags.hpp\"\n+#include \"gc\/z\/zObjectAllocator.hpp\"\n+#include \"gc\/z\/zPageAge.hpp\"\n+#include \"gc\/z\/zPageType.hpp\"\n+\n+class ZAllocatorEden;\n+class ZAllocatorForRelocation;\n+class ZPage;\n+\n+class ZAllocator {\n+  friend class ZAllocatorEden;\n+  friend class ZAllocatorSurvivor;\n+  friend class ZAllocatorOld;\n+\n+public:\n+  static constexpr uint _relocation_allocators = static_cast<uint>(ZPageAge::old);\n+\n+protected:\n+  ZObjectAllocator _object_allocator;\n+\n+  static ZAllocatorEden*          _eden;\n+  static ZAllocatorForRelocation* _relocation[ZAllocator::_relocation_allocators];\n+\n+public:\n+  static ZAllocatorEden* eden();\n+  static ZAllocatorForRelocation* relocation(ZPageAge page_age);\n+  static ZAllocatorForRelocation* old();\n+\n+  ZAllocator(ZPageAge age);\n+\n+  void retire_pages();\n+};\n+\n+class ZAllocatorEden : public ZAllocator {\n+public:\n+  ZAllocatorEden();\n+\n+  \/\/ Mutator allocation\n+  zaddress alloc_tlab(size_t size);\n+  zaddress alloc_object(size_t size);\n+\n+  \/\/ Statistics\n+  size_t tlab_used() const;\n+  size_t remaining() const;\n+};\n+\n+class ZAllocatorForRelocation : public ZAllocator {\n+private:\n+  ZPageAge install();\n+\n+public:\n+  ZAllocatorForRelocation();\n+\n+  \/\/ Relocation\n+  zaddress alloc_object(size_t size);\n+  void undo_alloc_object(zaddress addr, size_t size);\n+\n+  ZPage* alloc_page_for_relocation(ZPageType type, size_t size, ZAllocationFlags flags);\n+};\n+\n+#endif \/\/ SHARE_GC_Z_ZALLOCATOR_HPP\n","filename":"src\/hotspot\/share\/gc\/z\/zAllocator.hpp","additions":87,"deletions":0,"binary":false,"changes":87,"status":"added"},{"patch":"@@ -0,0 +1,59 @@\n+\/*\n+ * Copyright (c) 2021, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_Z_ZALLOCATOR_INLINE_HPP\n+#define SHARE_GC_Z_ZALLOCATOR_INLINE_HPP\n+\n+#include \"gc\/z\/zAllocator.hpp\"\n+\n+#include \"gc\/z\/zAddress.inline.hpp\"\n+#include \"gc\/z\/zHeap.hpp\"\n+\n+inline ZAllocatorEden* ZAllocator::eden() {\n+  return _eden;\n+}\n+\n+inline ZAllocatorForRelocation* ZAllocator::relocation(ZPageAge page_age) {\n+  return _relocation[static_cast<uint>(page_age) - 1];\n+}\n+\n+inline ZAllocatorForRelocation* ZAllocator::old() {\n+  return relocation(ZPageAge::old);\n+}\n+\n+inline zaddress ZAllocatorEden::alloc_tlab(size_t size) {\n+  guarantee(size <= ZHeap::heap()->max_tlab_size(), \"TLAB too large\");\n+  return _object_allocator.alloc_object(size);\n+}\n+\n+inline zaddress ZAllocatorEden::alloc_object(size_t size) {\n+  const zaddress addr = _object_allocator.alloc_object(size);\n+\n+  if (is_null(addr)) {\n+    ZHeap::heap()->out_of_memory();\n+  }\n+\n+  return addr;\n+}\n+\n+#endif \/\/ SHARE_GC_Z_ZALLOCATOR_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/z\/zAllocator.inline.hpp","additions":59,"deletions":0,"binary":false,"changes":59,"status":"added"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -40,2 +40,53 @@\n-void ZArguments::initialize() {\n-  GCArguments::initialize();\n+void ZArguments::select_max_gc_threads() {\n+  \/\/ Select number of parallel threads\n+  if (FLAG_IS_DEFAULT(ParallelGCThreads)) {\n+    FLAG_SET_DEFAULT(ParallelGCThreads, ZHeuristics::nparallel_workers());\n+  }\n+\n+  if (ParallelGCThreads == 0) {\n+    vm_exit_during_initialization(\"The flag -XX:+UseZGC can not be combined with -XX:ParallelGCThreads=0\");\n+  }\n+\n+  \/\/ The max number of concurrent threads we heuristically want for a generation\n+  uint max_nworkers_generation;\n+\n+  if (FLAG_IS_DEFAULT(ConcGCThreads)) {\n+    max_nworkers_generation = ZHeuristics::nconcurrent_workers();\n+\n+    \/\/ Computed max number of GC threads at a time in the machine\n+    uint max_nworkers = max_nworkers_generation;\n+\n+    if (!FLAG_IS_DEFAULT(ZYoungGCThreads)) {\n+      max_nworkers = MAX2(max_nworkers, ZYoungGCThreads);\n+    }\n+\n+    if (!FLAG_IS_DEFAULT(ZOldGCThreads)) {\n+      max_nworkers = MAX2(max_nworkers, ZOldGCThreads);\n+    }\n+\n+    FLAG_SET_DEFAULT(ConcGCThreads, max_nworkers);\n+  } else {\n+    max_nworkers_generation = ConcGCThreads;\n+  }\n+\n+  if (FLAG_IS_DEFAULT(ZYoungGCThreads)) {\n+    if (UseDynamicNumberOfGCThreads) {\n+      FLAG_SET_ERGO(ZYoungGCThreads, max_nworkers_generation);\n+    } else {\n+      const uint static_young_threads = MAX2(uint(max_nworkers_generation * 0.9), 1u);\n+      FLAG_SET_ERGO(ZYoungGCThreads, static_young_threads);\n+    }\n+  }\n+\n+  if (FLAG_IS_DEFAULT(ZOldGCThreads)) {\n+    if (UseDynamicNumberOfGCThreads) {\n+      FLAG_SET_ERGO(ZOldGCThreads, max_nworkers_generation);\n+    } else {\n+      const uint static_old_threads = MAX2(ConcGCThreads - ZYoungGCThreads, 1u);\n+      FLAG_SET_ERGO(ZOldGCThreads, static_old_threads);\n+    }\n+  }\n+\n+  if (ConcGCThreads == 0) {\n+    vm_exit_during_initialization(\"The flag -XX:+UseZGC can not be combined with -XX:ConcGCThreads=0\");\n+  }\n@@ -43,0 +94,14 @@\n+  if (ZYoungGCThreads > ConcGCThreads) {\n+    vm_exit_during_initialization(\"The flag -XX:ZYoungGCThreads can't be higher than -XX:ConcGCThreads\");\n+  } else if (ZYoungGCThreads == 0) {\n+    vm_exit_during_initialization(\"The flag -XX:ZYoungGCThreads can't be lower than 1\");\n+  }\n+\n+  if (ZOldGCThreads > ConcGCThreads) {\n+    vm_exit_during_initialization(\"The flag -XX:ZOldGCThreads can't be higher than -XX:ConcGCThreads\");\n+  } else if (ZOldGCThreads == 0) {\n+    vm_exit_during_initialization(\"The flag -XX:ZOldGCThreads can't be lower than 1\");\n+  }\n+}\n+\n+void ZArguments::initialize() {\n@@ -57,3 +122,5 @@\n-  \/\/ Select number of parallel threads\n-  if (FLAG_IS_DEFAULT(ParallelGCThreads)) {\n-    FLAG_SET_DEFAULT(ParallelGCThreads, ZHeuristics::nparallel_workers());\n+  select_max_gc_threads();\n+\n+  \/\/ Backwards compatible alias for ZCollectionIntervalMajor\n+  if (!FLAG_IS_DEFAULT(ZCollectionInterval)) {\n+    FLAG_SET_ERGO_IF_DEFAULT(ZCollectionIntervalMajor, ZCollectionInterval);\n@@ -62,2 +129,8 @@\n-  if (ParallelGCThreads == 0) {\n-    vm_exit_during_initialization(\"The flag -XX:+UseZGC can not be combined with -XX:ParallelGCThreads=0\");\n+  if (!FLAG_IS_CMDLINE(MaxHeapSize) &&\n+      !FLAG_IS_CMDLINE(MaxRAMFraction) &&\n+      !FLAG_IS_CMDLINE(MaxRAMPercentage)) {\n+    \/\/ We are really just guessing how much memory the program needs.\n+    \/\/ When that is the case, we don't want the soft and hard limits to be the same\n+    \/\/ as it can cause flakyness in the number of GC threads used, in order to keep\n+    \/\/ to a random number we just pulled out of thin air.\n+    FLAG_SET_ERGO_IF_DEFAULT(SoftMaxHeapSize, MaxHeapSize * 90 \/ 100);\n@@ -66,3 +139,2 @@\n-  \/\/ Select number of concurrent threads\n-  if (FLAG_IS_DEFAULT(ConcGCThreads)) {\n-    FLAG_SET_DEFAULT(ConcGCThreads, ZHeuristics::nconcurrent_workers());\n+  if (FLAG_IS_DEFAULT(ZFragmentationLimit)) {\n+    FLAG_SET_DEFAULT(ZFragmentationLimit, 5.0);\n@@ -71,2 +143,26 @@\n-  if (ConcGCThreads == 0) {\n-    vm_exit_during_initialization(\"The flag -XX:+UseZGC can not be combined with -XX:ConcGCThreads=0\");\n+  if (!FLAG_IS_DEFAULT(ZTenuringThreshold) && ZTenuringThreshold != -1) {\n+    FLAG_SET_ERGO_IF_DEFAULT(MaxTenuringThreshold, ZTenuringThreshold);\n+    if (MaxTenuringThreshold == 0) {\n+      FLAG_SET_ERGO_IF_DEFAULT(AlwaysTenure, true);\n+    }\n+  }\n+\n+  if (FLAG_IS_DEFAULT(MaxTenuringThreshold)) {\n+    uint tenuring_threshold;\n+    for (tenuring_threshold = 0; tenuring_threshold < MaxTenuringThreshold; ++tenuring_threshold) {\n+      \/\/ Reduce the number of object ages, if the resulting garbage is too high\n+      const size_t medium_page_overhead = ZPageSizeMedium * tenuring_threshold;\n+      const size_t small_page_overhead = ZPageSizeSmall * ConcGCThreads * tenuring_threshold;\n+      if (small_page_overhead + medium_page_overhead >= ZHeuristics::significant_young_overhead()) {\n+        break;\n+      }\n+    }\n+    FLAG_SET_DEFAULT(MaxTenuringThreshold, tenuring_threshold);\n+    if (tenuring_threshold == 0 && FLAG_IS_DEFAULT(AlwaysTenure)) {\n+      \/\/ Some flag constraint function says AlwaysTenure must be true iff MaxTenuringThreshold == 0\n+      FLAG_SET_DEFAULT(AlwaysTenure, true);\n+    }\n+  }\n+\n+  if (!FLAG_IS_DEFAULT(ZTenuringThreshold) && NeverTenure) {\n+    vm_exit_during_initialization(err_msg(\"ZTenuringThreshold and NeverTenure are incompatible\"));\n@@ -82,4 +178,3 @@\n-  \/\/ The heuristics used when UseDynamicNumberOfGCThreads is\n-  \/\/ enabled defaults to using a ZAllocationSpikeTolerance of 1.\n-  if (UseDynamicNumberOfGCThreads && FLAG_IS_DEFAULT(ZAllocationSpikeTolerance)) {\n-    FLAG_SET_DEFAULT(ZAllocationSpikeTolerance, 1);\n+  if (!FLAG_IS_DEFAULT(ZTenuringThreshold) && ZTenuringThreshold > static_cast<int>(MaxTenuringThreshold)) {\n+    vm_exit_during_initialization(err_msg(\"ZTenuringThreshold must be be within bounds of \"\n+                                          \"MaxTenuringThreshold\"));\n@@ -101,0 +196,5 @@\n+  \/\/ More events\n+  if (FLAG_IS_DEFAULT(LogEventsBufferEntries)) {\n+    FLAG_SET_DEFAULT(LogEventsBufferEntries, 250);\n+  }\n+\n@@ -109,1 +209,0 @@\n-}\n@@ -111,2 +210,6 @@\n-size_t ZArguments::heap_virtual_to_physical_ratio() {\n-  return ZHeapViews * ZVirtualToPhysicalRatio;\n+#ifdef ASSERT\n+  \/\/ This check slows down testing too much. Turn it off for now.\n+  if (FLAG_IS_DEFAULT(VerifyDependencies)) {\n+    FLAG_SET_DEFAULT(VerifyDependencies, false);\n+  }\n+#endif\n@@ -115,2 +218,2 @@\n-size_t ZArguments::conservative_max_heap_alignment() {\n-  return 0;\n+size_t ZArguments::heap_virtual_to_physical_ratio() {\n+  return ZVirtualToPhysicalRatio;\n@@ -123,1 +226,1 @@\n-bool ZArguments::is_supported() const {\n+bool ZArguments::is_supported() {\n","filename":"src\/hotspot\/share\/gc\/z\/zArguments.cpp","additions":126,"deletions":23,"binary":false,"changes":149,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -31,1 +31,1 @@\n-class ZArguments : public GCArguments {\n+class ZArguments : AllStatic {\n@@ -33,1 +33,1 @@\n-  virtual void initialize_alignments();\n+  static void select_max_gc_threads();\n@@ -35,4 +35,5 @@\n-  virtual void initialize();\n-  virtual size_t conservative_max_heap_alignment();\n-  virtual size_t heap_virtual_to_physical_ratio();\n-  virtual CollectedHeap* create_heap();\n+public:\n+  static void initialize_alignments();\n+  static void initialize();\n+  static size_t heap_virtual_to_physical_ratio();\n+  static CollectedHeap* create_heap();\n@@ -40,1 +41,1 @@\n-  virtual bool is_supported() const;\n+  static bool is_supported();\n@@ -42,1 +43,1 @@\n-  bool is_os_supported() const;\n+  static bool is_os_supported();\n","filename":"src\/hotspot\/share\/gc\/z\/zArguments.hpp","additions":10,"deletions":9,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,0 +28,3 @@\n+#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/os.hpp\"\n+#include \"runtime\/thread.hpp\"\n@@ -30,0 +33,4 @@\n+#include <type_traits>\n+\n+class ZLock;\n+\n@@ -35,2 +42,3 @@\n-  const T*       _next;\n-  const T* const _end;\n+  size_t         _next;\n+  const size_t   _end;\n+  const T* const _array;\n@@ -38,2 +46,2 @@\n-  bool next_serial(T* elem);\n-  bool next_parallel(T* elem);\n+  bool next_serial(size_t* index);\n+  bool next_parallel(size_t* index);\n@@ -46,0 +54,3 @@\n+  bool next_index(size_t* index);\n+\n+  T index_to_elem(size_t index);\n@@ -51,0 +62,21 @@\n+template <typename T>\n+class ZActivatedArray {\n+private:\n+  typedef typename std::remove_extent<T>::type ItemT;\n+\n+  ZLock*         _lock;\n+  uint64_t       _count;\n+  ZArray<ItemT*> _array;\n+\n+public:\n+  explicit ZActivatedArray(bool locked = true);\n+  ~ZActivatedArray();\n+\n+  void activate();\n+  template <typename Function>\n+  void deactivate_and_apply(Function function);\n+\n+  bool is_activated() const;\n+  bool add_if_activated(ItemT* item);\n+};\n+\n","filename":"src\/hotspot\/share\/gc\/z\/zArray.hpp","additions":37,"deletions":5,"binary":false,"changes":42,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -29,0 +29,1 @@\n+#include \"gc\/z\/zLock.inline.hpp\"\n@@ -32,1 +33,1 @@\n-inline bool ZArrayIteratorImpl<T, Parallel>::next_serial(T* elem) {\n+inline bool ZArrayIteratorImpl<T, Parallel>::next_serial(size_t* index) {\n@@ -37,1 +38,1 @@\n-  *elem = *_next;\n+  *index = _next;\n@@ -44,2 +45,2 @@\n-inline bool ZArrayIteratorImpl<T, Parallel>::next_parallel(T* elem) {\n-  const T* old_next = Atomic::load(&_next);\n+inline bool ZArrayIteratorImpl<T, Parallel>::next_parallel(size_t* index) {\n+  const size_t claimed_index = Atomic::fetch_and_add(&_next, 1u, memory_order_relaxed);\n@@ -47,13 +48,3 @@\n-  for (;;) {\n-    if (old_next == _end) {\n-      return false;\n-    }\n-\n-    const T* const new_next = old_next + 1;\n-    const T* const prev_next = Atomic::cmpxchg(&_next, old_next, new_next);\n-    if (prev_next == old_next) {\n-      *elem = *old_next;\n-      return true;\n-    }\n-\n-    old_next = prev_next;\n+  if (claimed_index < _end) {\n+    *index = claimed_index;\n+    return true;\n@@ -61,0 +52,2 @@\n+\n+  return false;\n@@ -65,2 +58,3 @@\n-    _next(array),\n-    _end(array + length) {}\n+    _next(0),\n+    _end(length),\n+    _array(array) {}\n@@ -70,1 +64,1 @@\n-    ZArrayIteratorImpl<T, Parallel>(array->is_empty() ? NULL : array->adr_at(0), array->length()) {}\n+    ZArrayIteratorImpl<T, Parallel>(array->is_empty() ? nullptr : array->adr_at(0), array->length()) {}\n@@ -74,0 +68,11 @@\n+  size_t index;\n+  if (next_index(&index)) {\n+    *elem = index_to_elem(index);\n+    return true;\n+  }\n+\n+  return false;\n+}\n+\n+template <typename T, bool Parallel>\n+inline bool ZArrayIteratorImpl<T, Parallel>::next_index(size_t* index) {\n@@ -75,1 +80,1 @@\n-    return next_parallel(elem);\n+    return next_parallel(index);\n@@ -77,1 +82,62 @@\n-    return next_serial(elem);\n+    return next_serial(index);\n+  }\n+}\n+\n+template <typename T, bool Parallel>\n+inline T ZArrayIteratorImpl<T, Parallel>::index_to_elem(size_t index) {\n+  assert(index < _end, \"Out of bounds\");\n+  return _array[index];\n+}\n+\n+template <typename T>\n+ZActivatedArray<T>::ZActivatedArray(bool locked) :\n+    _lock(locked ? new ZLock() : nullptr),\n+    _count(0),\n+    _array() {}\n+\n+template <typename T>\n+ZActivatedArray<T>::~ZActivatedArray<T>() {\n+  FreeHeap(_lock);\n+}\n+\n+template <typename T>\n+bool ZActivatedArray<T>::is_activated() const {\n+  ZLocker<ZLock> locker(_lock);\n+  return _count > 0;\n+}\n+\n+template <typename T>\n+bool ZActivatedArray<T>::add_if_activated(ItemT* item) {\n+  ZLocker<ZLock> locker(_lock);\n+  if (_count > 0) {\n+    _array.append(item);\n+    return true;\n+  }\n+\n+  return false;\n+}\n+\n+template <typename T>\n+void ZActivatedArray<T>::activate() {\n+  ZLocker<ZLock> locker(_lock);\n+  _count++;\n+}\n+\n+template <typename T>\n+template <typename Function>\n+void ZActivatedArray<T>::deactivate_and_apply(Function function) {\n+  ZArray<ItemT*> array;\n+\n+  {\n+    ZLocker<ZLock> locker(_lock);\n+    assert(_count > 0, \"Invalid state\");\n+    if (--_count == 0u) {\n+      \/\/ Fully deactivated - remove all elements\n+      array.swap(&_array);\n+    }\n+  }\n+\n+  \/\/ Apply function to all elements - if fully deactivated\n+  ZArrayIterator<ItemT*> iter(&array);\n+  for (ItemT* item; iter.next(&item);) {\n+    function(item);\n","filename":"src\/hotspot\/share\/gc\/z\/zArray.inline.hpp","additions":89,"deletions":23,"binary":false,"changes":112,"status":"modified"},{"patch":"@@ -26,0 +26,1 @@\n+#include \"gc\/z\/zAddress.inline.hpp\"\n@@ -27,0 +28,1 @@\n+#include \"gc\/z\/zGeneration.inline.hpp\"\n@@ -28,2 +30,1 @@\n-#include \"gc\/z\/zOop.inline.hpp\"\n-#include \"gc\/z\/zThread.inline.hpp\"\n+#include \"gc\/z\/zStoreBarrierBuffer.inline.hpp\"\n@@ -35,10 +36,4 @@\n-template <bool finalizable>\n-bool ZBarrier::should_mark_through(uintptr_t addr) {\n-  \/\/ Finalizable marked oops can still exists on the heap after marking\n-  \/\/ has completed, in which case we just want to convert this into a\n-  \/\/ good oop and not push it on the mark stack.\n-  if (!during_mark()) {\n-    assert(ZAddress::is_marked(addr), \"Should be marked\");\n-    assert(ZAddress::is_finalizable(addr), \"Should be finalizable\");\n-    return false;\n-  }\n+#ifdef ASSERT\n+static bool during_young_mark() {\n+  return ZGeneration::young()->is_phase_mark();\n+}\n@@ -46,17 +41,3 @@\n-  \/\/ During marking, we mark through already marked oops to avoid having\n-  \/\/ some large part of the object graph hidden behind a pushed, but not\n-  \/\/ yet flushed, entry on a mutator mark stack. Always marking through\n-  \/\/ allows the GC workers to proceed through the object graph even if a\n-  \/\/ mutator touched an oop first, which in turn will reduce the risk of\n-  \/\/ having to flush mark stacks multiple times to terminate marking.\n-  \/\/\n-  \/\/ However, when doing finalizable marking we don't always want to mark\n-  \/\/ through. First, marking through an already strongly marked oop would\n-  \/\/ be wasteful, since we will then proceed to do finalizable marking on\n-  \/\/ an object which is, or will be, marked strongly. Second, marking\n-  \/\/ through an already finalizable marked oop would also be wasteful,\n-  \/\/ since such oops can never end up on a mutator mark stack and can\n-  \/\/ therefore not hide some part of the object graph from GC workers.\n-  if (finalizable) {\n-    return !ZAddress::is_marked(addr);\n-  }\n+static bool during_old_mark() {\n+  return ZGeneration::old()->is_phase_mark();\n+}\n@@ -64,2 +45,2 @@\n-  \/\/ Mark through\n-  return true;\n+static bool during_any_mark() {\n+  return during_young_mark() || during_old_mark();\n@@ -67,0 +48,1 @@\n+#endif\n@@ -68,3 +50,3 @@\n-template <bool gc_thread, bool follow, bool finalizable, bool publish>\n-uintptr_t ZBarrier::mark(uintptr_t addr) {\n-  uintptr_t good_addr;\n+zaddress ZBarrier::relocate_or_remap(zaddress_unsafe addr, ZGeneration* generation) {\n+  return generation->relocate_or_remap_object(addr);\n+}\n@@ -72,9 +54,11 @@\n-  if (ZAddress::is_marked(addr)) {\n-    \/\/ Already marked, but try to mark though anyway\n-    good_addr = ZAddress::good(addr);\n-  } else if (ZAddress::is_remapped(addr)) {\n-    \/\/ Already remapped, but also needs to be marked\n-    good_addr = ZAddress::good(addr);\n-  } else {\n-    \/\/ Needs to be both remapped and marked\n-    good_addr = remap(addr);\n+zaddress ZBarrier::remap(zaddress_unsafe addr, ZGeneration* generation) {\n+  return generation->remap_object(addr);\n+}\n+\n+\/\/\n+\/\/ Weak load barrier\n+\/\/\n+\n+static void keep_alive_young(zaddress addr) {\n+  if (ZGeneration::young()->is_phase_mark()) {\n+    ZBarrier::mark_young<ZMark::Resurrect, ZMark::AnyThread, ZMark::Follow>(addr);\n@@ -82,0 +66,1 @@\n+}\n@@ -83,3 +68,3 @@\n-  \/\/ Mark\n-  if (should_mark_through<finalizable>(addr)) {\n-    ZHeap::heap()->mark_object<gc_thread, follow, finalizable, publish>(good_addr);\n+zaddress ZBarrier::blocking_keep_alive_on_weak_slow_path(volatile zpointer* p, zaddress addr) {\n+  if (is_null(addr)) {\n+    return zaddress::null;\n@@ -88,9 +73,7 @@\n-  if (finalizable) {\n-    \/\/ Make the oop finalizable marked\/good, instead of normal marked\/good.\n-    \/\/ This is needed because an object might first becomes finalizable\n-    \/\/ marked by the GC, and then loaded by a mutator thread. In this case,\n-    \/\/ the mutator thread must be able to tell that the object needs to be\n-    \/\/ strongly marked. The finalizable bit in the oop exists to make sure\n-    \/\/ that a load of a finalizable marked oop will fall into the barrier\n-    \/\/ slow path so that we can mark the object as strongly reachable.\n-    return ZAddress::finalizable_good(good_addr);\n+  if (ZHeap::heap()->is_old(addr)) {\n+    if (!ZHeap::heap()->is_object_strongly_live(addr)) {\n+      return zaddress::null;\n+    }\n+  } else {\n+    \/\/ Young gen objects are never blocked, need to keep alive\n+    keep_alive_young(addr);\n@@ -99,1 +82,2 @@\n-  return good_addr;\n+  \/\/ Strongly live\n+  return addr;\n@@ -102,5 +86,4 @@\n-uintptr_t ZBarrier::remap(uintptr_t addr) {\n-  assert(!ZAddress::is_good(addr), \"Should not be good\");\n-  assert(!ZAddress::is_weak_good(addr), \"Should not be weak good\");\n-  return ZHeap::heap()->remap_object(addr);\n-}\n+zaddress ZBarrier::blocking_keep_alive_on_phantom_slow_path(volatile zpointer* p, zaddress addr) {\n+  if (is_null(addr)) {\n+    return zaddress::null;\n+  }\n@@ -108,5 +91,8 @@\n-uintptr_t ZBarrier::relocate(uintptr_t addr) {\n-  assert(!ZAddress::is_good(addr), \"Should not be good\");\n-  assert(!ZAddress::is_weak_good(addr), \"Should not be weak good\");\n-  return ZHeap::heap()->relocate_object(addr);\n-}\n+  if (ZHeap::heap()->is_old(addr)) {\n+    if (!ZHeap::heap()->is_object_live(addr)) {\n+      return zaddress::null;\n+    }\n+  } else {\n+    \/\/ Young gen objects are never blocked, need to keep alive\n+    keep_alive_young(addr);\n+  }\n@@ -114,2 +100,2 @@\n-uintptr_t ZBarrier::relocate_or_mark(uintptr_t addr) {\n-  return during_relocate() ? relocate(addr) : mark<AnyThread, Follow, Strong, Publish>(addr);\n+  \/\/ Strongly live\n+  return addr;\n@@ -118,2 +104,18 @@\n-uintptr_t ZBarrier::relocate_or_mark_no_follow(uintptr_t addr) {\n-  return during_relocate() ? relocate(addr) : mark<AnyThread, DontFollow, Strong, Publish>(addr);\n+zaddress ZBarrier::blocking_load_barrier_on_weak_slow_path(volatile zpointer* p, zaddress addr) {\n+  if (is_null(addr)) {\n+    return zaddress::null;\n+  }\n+\n+  if (ZHeap::heap()->is_old(addr)) {\n+    if (!ZHeap::heap()->is_object_strongly_live(addr)) {\n+      return zaddress::null;\n+    }\n+  } else {\n+    \/\/ Young objects are never considered non-strong\n+    \/\/ Note: Should not need to keep object alive in this operation,\n+    \/\/       but the barrier colors the pointer mark good, so we need\n+    \/\/       to mark the object accordingly.\n+    keep_alive_young(addr);\n+  }\n+\n+  return addr;\n@@ -122,2 +124,18 @@\n-uintptr_t ZBarrier::relocate_or_remap(uintptr_t addr) {\n-  return during_relocate() ? relocate(addr) : remap(addr);\n+zaddress ZBarrier::blocking_load_barrier_on_phantom_slow_path(volatile zpointer* p, zaddress addr) {\n+  if (is_null(addr)) {\n+    return zaddress::null;\n+  }\n+\n+  if (ZHeap::heap()->is_old(addr)) {\n+    if (!ZHeap::heap()->is_object_live(addr)) {\n+      return zaddress::null;\n+    }\n+  } else {\n+    \/\/ Young objects are never considered non-strong\n+    \/\/ Note: Should not need to keep object alive in this operation,\n+    \/\/       but the barrier colors the pointer mark good, so we need\n+    \/\/       to mark the object accordingly.\n+    keep_alive_young(addr);\n+  }\n+\n+  return addr;\n@@ -127,1 +145,1 @@\n-\/\/ Load barrier\n+\/\/ Clean barrier\n@@ -129,3 +147,0 @@\n-uintptr_t ZBarrier::load_barrier_on_oop_slow_path(uintptr_t addr) {\n-  return relocate_or_mark(addr);\n-}\n@@ -133,3 +148,3 @@\n-uintptr_t ZBarrier::load_barrier_on_invisible_root_oop_slow_path(uintptr_t addr) {\n-  return relocate_or_mark_no_follow(addr);\n-}\n+zaddress ZBarrier::verify_old_object_live_slow_path(zaddress addr) {\n+  \/\/ Verify that the object was indeed alive\n+  assert(ZHeap::heap()->is_young(addr) || ZHeap::heap()->is_object_live(addr), \"Should be live\");\n@@ -137,4 +152,1 @@\n-void ZBarrier::load_barrier_on_oop_fields(oop o) {\n-  assert(ZAddress::is_good(ZOop::to_address(o)), \"Should be good\");\n-  ZLoadBarrierOopClosure cl;\n-  o->oop_iterate(&cl);\n+  return addr;\n@@ -144,1 +156,1 @@\n-\/\/ Weak load barrier\n+\/\/ Mark barrier\n@@ -146,3 +158,0 @@\n-uintptr_t ZBarrier::weak_load_barrier_on_oop_slow_path(uintptr_t addr) {\n-  return ZAddress::is_weak_good(addr) ? ZAddress::good(addr) : relocate_or_remap(addr);\n-}\n@@ -150,4 +159,5 @@\n-uintptr_t ZBarrier::weak_load_barrier_on_weak_oop_slow_path(uintptr_t addr) {\n-  const uintptr_t good_addr = weak_load_barrier_on_oop_slow_path(addr);\n-  if (ZHeap::heap()->is_object_strongly_live(good_addr)) {\n-    return good_addr;\n+zaddress ZBarrier::mark_slow_path(zaddress addr) {\n+  assert(during_any_mark(), \"Invalid phase\");\n+\n+  if (is_null(addr)) {\n+    return addr;\n@@ -156,2 +166,3 @@\n-  \/\/ Not strongly live\n-  return 0;\n+  mark<ZMark::DontResurrect, ZMark::AnyThread, ZMark::Follow, ZMark::Strong>(addr);\n+\n+  return addr;\n@@ -160,4 +171,5 @@\n-uintptr_t ZBarrier::weak_load_barrier_on_phantom_oop_slow_path(uintptr_t addr) {\n-  const uintptr_t good_addr = weak_load_barrier_on_oop_slow_path(addr);\n-  if (ZHeap::heap()->is_object_live(good_addr)) {\n-    return good_addr;\n+zaddress ZBarrier::mark_from_young_slow_path(zaddress addr) {\n+  assert(during_young_mark(), \"Invalid phase\");\n+\n+  if (is_null(addr)) {\n+    return addr;\n@@ -166,3 +178,4 @@\n-  \/\/ Not live\n-  return 0;\n-}\n+  if (ZHeap::heap()->is_young(addr)) {\n+    ZGeneration::young()->mark_object<ZMark::DontResurrect, ZMark::GCThread, ZMark::Follow, ZMark::Strong>(addr);\n+    return addr;\n+  }\n@@ -170,5 +183,7 @@\n-\/\/\n-\/\/ Keep alive barrier\n-\/\/\n-uintptr_t ZBarrier::keep_alive_barrier_on_oop_slow_path(uintptr_t addr) {\n-  assert(during_mark(), \"Invalid phase\");\n+  if (ZGeneration::young()->type() == ZYoungType::major_full_roots ||\n+      ZGeneration::young()->type() == ZYoungType::major_partial_roots) {\n+    \/\/ The initial major young collection is responsible for finding roots\n+    \/\/ from the young generation to the old generation.\n+    ZGeneration::old()->mark_object<ZMark::DontResurrect, ZMark::GCThread, ZMark::Follow, ZMark::Strong>(addr);\n+    return addr;\n+  }\n@@ -176,3 +191,3 @@\n-  \/\/ Mark\n-  return mark<AnyThread, Follow, Strong, Overflow>(addr);\n-}\n+  \/\/ Don't mark pointers to the old generation for minor during major;\n+  \/\/ the initial young collection pushed the young-to-old pointers that\n+  \/\/ were part of the SATB. All other young-to-old pointers are irrelevant.\n@@ -180,5 +195,3 @@\n-uintptr_t ZBarrier::keep_alive_barrier_on_weak_oop_slow_path(uintptr_t addr) {\n-  assert(ZResurrection::is_blocked(), \"This operation is only valid when resurrection is blocked\");\n-  const uintptr_t good_addr = weak_load_barrier_on_oop_slow_path(addr);\n-  assert(ZHeap::heap()->is_object_strongly_live(good_addr), \"Should be live\");\n-  return good_addr;\n+  \/\/ We still want to heal the pointers so they become store_good, so that\n+  \/\/ after a young collection, all young pointers are store good.\n+  return addr;\n@@ -187,6 +200,2 @@\n-uintptr_t ZBarrier::keep_alive_barrier_on_phantom_oop_slow_path(uintptr_t addr) {\n-  assert(ZResurrection::is_blocked(), \"This operation is only valid when resurrection is blocked\");\n-  const uintptr_t good_addr = weak_load_barrier_on_oop_slow_path(addr);\n-  assert(ZHeap::heap()->is_object_live(good_addr), \"Should be live\");\n-  return good_addr;\n-}\n+zaddress ZBarrier::mark_from_old_slow_path(zaddress addr) {\n+  assert(during_old_mark(), \"Invalid phase\");\n@@ -194,6 +203,3 @@\n-\/\/\n-\/\/ Mark barrier\n-\/\/\n-uintptr_t ZBarrier::mark_barrier_on_oop_slow_path(uintptr_t addr) {\n-  assert(during_mark(), \"Invalid phase\");\n-  assert(ZThread::is_worker(), \"Invalid thread\");\n+  if (is_null(addr)) {\n+    return addr;\n+  }\n@@ -201,3 +207,4 @@\n-  \/\/ Mark\n-  return mark<GCThread, Follow, Strong, Overflow>(addr);\n-}\n+  if (ZHeap::heap()->is_old(addr)) {\n+    ZGeneration::old()->mark_object<ZMark::DontResurrect, ZMark::GCThread, ZMark::Follow, ZMark::Strong>(addr);\n+    return addr;\n+  }\n@@ -205,3 +212,2 @@\n-uintptr_t ZBarrier::mark_barrier_on_finalizable_oop_slow_path(uintptr_t addr) {\n-  assert(during_mark(), \"Invalid phase\");\n-  assert(ZThread::is_worker(), \"Invalid thread\");\n+  \/\/ Don't mark pointers to the young generation; they will be\n+  \/\/ processed by the remembered set scanning.\n@@ -209,2 +215,2 @@\n-  \/\/ Mark\n-  return mark<GCThread, Follow, Finalizable, Overflow>(addr);\n+  \/\/ Returning null means this location is not self healed by the caller.\n+  return zaddress::null;\n@@ -213,7 +219,6 @@\n-\/\/\n-\/\/ Narrow oop variants, never used.\n-\/\/\n-oop ZBarrier::load_barrier_on_oop_field(volatile narrowOop* p) {\n-  ShouldNotReachHere();\n-  return NULL;\n-}\n+zaddress ZBarrier::mark_young_slow_path(zaddress addr) {\n+  assert(during_young_mark(), \"Invalid phase\");\n+\n+  if (is_null(addr)) {\n+    return addr;\n+  }\n@@ -221,3 +226,3 @@\n-oop ZBarrier::load_barrier_on_oop_field_preloaded(volatile narrowOop* p, oop o) {\n-  ShouldNotReachHere();\n-  return NULL;\n+  mark_if_young<ZMark::DontResurrect, ZMark::GCThread, ZMark::Follow>(addr);\n+\n+  return addr;\n@@ -226,2 +231,14 @@\n-void ZBarrier::load_barrier_on_oop_array(volatile narrowOop* p, size_t length) {\n-  ShouldNotReachHere();\n+zaddress ZBarrier::mark_finalizable_slow_path(zaddress addr) {\n+  assert(during_any_mark(), \"Invalid phase\");\n+\n+  if (is_null(addr)) {\n+    return addr;\n+  }\n+\n+  if (ZHeap::heap()->is_old(addr)) {\n+    ZGeneration::old()->mark_object<ZMark::DontResurrect, ZMark::GCThread, ZMark::Follow, ZMark::Finalizable>(addr);\n+    return addr;\n+  }\n+\n+  ZGeneration::young()->mark_object_if_active<ZMark::DontResurrect, ZMark::GCThread, ZMark::Follow, ZMark::Strong>(addr);\n+  return addr;\n@@ -230,3 +247,17 @@\n-oop ZBarrier::load_barrier_on_weak_oop_field_preloaded(volatile narrowOop* p, oop o) {\n-  ShouldNotReachHere();\n-  return NULL;\n+zaddress ZBarrier::mark_finalizable_from_old_slow_path(zaddress addr) {\n+  assert(during_any_mark(), \"Invalid phase\");\n+\n+  if (is_null(addr)) {\n+    return addr;\n+  }\n+\n+  if (ZHeap::heap()->is_old(addr)) {\n+    ZGeneration::old()->mark_object<ZMark::DontResurrect, ZMark::GCThread, ZMark::Follow, ZMark::Finalizable>(addr);\n+    return addr;\n+  }\n+\n+  \/\/ Don't mark pointers to the young generation; they will be\n+  \/\/ processed by the remembered set scanning.\n+\n+  \/\/ Returning null means this location is not self healed by the caller.\n+  return zaddress::null;\n@@ -235,3 +266,11 @@\n-oop ZBarrier::load_barrier_on_phantom_oop_field_preloaded(volatile narrowOop* p, oop o) {\n-  ShouldNotReachHere();\n-  return NULL;\n+zaddress ZBarrier::heap_store_slow_path(volatile zpointer* p, zaddress addr, zpointer prev, bool heal) {\n+  ZStoreBarrierBuffer* buffer = ZStoreBarrierBuffer::buffer_for_store(heal);\n+\n+  if (buffer != nullptr) {\n+    \/\/ Buffer store barriers whenever possible\n+    buffer->add(p, prev);\n+  } else {\n+    mark_and_remember(p, addr);\n+  }\n+\n+  return addr;\n@@ -240,3 +279,4 @@\n-oop ZBarrier::weak_load_barrier_on_oop_field_preloaded(volatile narrowOop* p, oop o) {\n-  ShouldNotReachHere();\n-  return NULL;\n+zaddress ZBarrier::no_keep_alive_heap_store_slow_path(volatile zpointer* p, zaddress addr) {\n+  remember(p);\n+\n+  return addr;\n@@ -245,3 +285,6 @@\n-oop ZBarrier::weak_load_barrier_on_weak_oop_field_preloaded(volatile narrowOop* p, oop o) {\n-  ShouldNotReachHere();\n-  return NULL;\n+zaddress ZBarrier::native_store_slow_path(zaddress addr) {\n+  if (!is_null(addr)) {\n+    mark<ZMark::DontResurrect, ZMark::AnyThread, ZMark::Follow, ZMark::Strong>(addr);\n+  }\n+\n+  return addr;\n@@ -250,3 +293,6 @@\n-oop ZBarrier::weak_load_barrier_on_phantom_oop_field_preloaded(volatile narrowOop* p, oop o) {\n-  ShouldNotReachHere();\n-  return NULL;\n+zaddress ZBarrier::keep_alive_slow_path(zaddress addr) {\n+  if (!is_null(addr)) {\n+    mark<ZMark::Resurrect, ZMark::AnyThread, ZMark::Follow, ZMark::Strong>(addr);\n+  }\n+\n+  return addr;\n@@ -258,4 +304,4 @@\n-void ZBarrier::verify_on_weak(volatile oop* referent_addr) {\n-  if (referent_addr != NULL) {\n-    uintptr_t base = (uintptr_t)referent_addr - java_lang_ref_Reference::referent_offset();\n-    oop obj = cast_to_oop(base);\n+void ZBarrier::verify_on_weak(volatile zpointer* referent_addr) {\n+  if (referent_addr != nullptr) {\n+    const uintptr_t base = (uintptr_t)referent_addr - java_lang_ref_Reference::referent_offset();\n+    const oop obj = cast_to_oop(base);\n@@ -268,8 +314,0 @@\n-\n-void ZLoadBarrierOopClosure::do_oop(oop* p) {\n-  ZBarrier::load_barrier_on_oop_field(p);\n-}\n-\n-void ZLoadBarrierOopClosure::do_oop(narrowOop* p) {\n-  ShouldNotReachHere();\n-}\n","filename":"src\/hotspot\/share\/gc\/z\/zBarrier.cpp","additions":210,"deletions":172,"binary":false,"changes":382,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,0 +27,1 @@\n+#include \"gc\/z\/zAddress.hpp\"\n@@ -29,1 +30,0 @@\n-#include \"oops\/oop.hpp\"\n@@ -31,2 +31,44 @@\n-typedef bool (*ZBarrierFastPath)(uintptr_t);\n-typedef uintptr_t (*ZBarrierSlowPath)(uintptr_t);\n+\/\/ == Shift based load barrier ==\n+\/\/\n+\/\/ The load barriers of ZGC check if a loaded value is safe to expose or not, and\n+\/\/ then shifts the pointer to remove metadata bits, such that it points to mapped\n+\/\/ memory.\n+\/\/\n+\/\/ A pointer is safe to expose if it does not have any load-bad bits set in its\n+\/\/ metadata bits. In the C++ code and non-nmethod generated code, that is checked\n+\/\/ by testing the pointer value against a load-bad mask, checking that no bad bit\n+\/\/ is set, followed by a shift, removing the metadata bits if they were good.\n+\/\/ However, for nmethod code, the test + shift sequence is optimized in such\n+\/\/ a way that the shift both tests if the pointer is exposable or not, and removes\n+\/\/ the metadata bits, with the same instruction. This is a speculative optimization\n+\/\/ that assumes that the loaded pointer is frequently going to be load-good or null\n+\/\/ when checked. Therefore, the nmethod load barriers just apply the shift with the\n+\/\/ current \"good\" shift (which is patched with nmethod entry barriers for each GC\n+\/\/ phase). If the result of that shift was a raw null value, then the ZF flag is set.\n+\/\/ If the result is a good pointer, then the very last bit that was removed by the\n+\/\/ shift, must have been a 1, which would have set the CF flag. Therefore, the \"above\"\n+\/\/ branch condition code is used to take a slowpath only iff CF == 0 and ZF == 0.\n+\/\/ CF == 0 implies it was not a good pointer, and ZF == 0 implies the resulting address\n+\/\/ was not a null value. Then we decide that the pointer is bad. This optimization\n+\/\/ is necessary to get satisfactory performance, but does come with a few constraints:\n+\/\/\n+\/\/ 1) The load barrier can only recognize 4 different good patterns across all GC phases.\n+\/\/    The reason is that when a load barrier applies the currently good shift, then\n+\/\/    the value of said shift may differ only by 3, until we risk shifting away more\n+\/\/    than the low order three zeroes of an address, given a bad pointer, which would\n+\/\/    yield spurious false positives.\n+\/\/\n+\/\/ 2) Those bit patterns must have only a single bit set. We achieve that by moving\n+\/\/    non-relocation work to store barriers.\n+\/\/\n+\/\/ Another consequence of this speculative optimization, is that when the compiled code\n+\/\/ takes a slow path, it needs to reload the oop, because the shifted oop is now\n+\/\/ broken after being shifted with a different shift to what was used when the oop\n+\/\/ was stored.\n+\n+typedef bool (*ZBarrierFastPath)(zpointer);\n+typedef zpointer (*ZBarrierColor)(zaddress, zpointer);\n+\n+class ZGeneration;\n+\n+void z_assert_is_barrier_safe();\n@@ -35,9 +77,3 @@\n-private:\n-  static const bool GCThread    = true;\n-  static const bool AnyThread   = false;\n-\n-  static const bool Follow      = true;\n-  static const bool DontFollow  = false;\n-\n-  static const bool Strong      = false;\n-  static const bool Finalizable = true;\n+  friend class ZContinuation;\n+  friend class ZStoreBarrierBuffer;\n+  friend class ZUncoloredRoot;\n@@ -45,4 +81,54 @@\n-  static const bool Publish     = true;\n-  static const bool Overflow    = false;\n-\n-  template <ZBarrierFastPath fast_path> static void self_heal(volatile oop* p, uintptr_t addr, uintptr_t heal_addr);\n+private:\n+  static void assert_transition_monotonicity(zpointer ptr, zpointer heal_ptr);\n+  static void self_heal(ZBarrierFastPath fast_path, volatile zpointer* p, zpointer ptr, zpointer heal_ptr, bool allow_null);\n+\n+  template <typename ZBarrierSlowPath>\n+  static zaddress barrier(ZBarrierFastPath fast_path, ZBarrierSlowPath slow_path, ZBarrierColor color, volatile zpointer* p, zpointer o, bool allow_null = false);\n+\n+  static zaddress make_load_good(zpointer ptr);\n+  static zaddress make_load_good_no_relocate(zpointer ptr);\n+  static zaddress relocate_or_remap(zaddress_unsafe addr, ZGeneration* generation);\n+  static zaddress remap(zaddress_unsafe addr, ZGeneration* generation);\n+  static void remember(volatile zpointer* p);\n+  static void mark_and_remember(volatile zpointer* p, zaddress addr);\n+\n+  \/\/ Fast paths in increasing strength level\n+  static bool is_load_good_or_null_fast_path(zpointer ptr);\n+  static bool is_mark_good_fast_path(zpointer ptr);\n+  static bool is_store_good_fast_path(zpointer ptr);\n+  static bool is_store_good_or_null_fast_path(zpointer ptr);\n+  static bool is_store_good_or_null_any_fast_path(zpointer ptr);\n+\n+  static bool is_mark_young_good_fast_path(zpointer ptr);\n+  static bool is_finalizable_good_fast_path(zpointer ptr);\n+\n+  \/\/ Slow paths\n+  static zaddress blocking_keep_alive_on_weak_slow_path(volatile zpointer* p, zaddress addr);\n+  static zaddress blocking_keep_alive_on_phantom_slow_path(volatile zpointer* p, zaddress addr);\n+  static zaddress blocking_load_barrier_on_weak_slow_path(volatile zpointer* p, zaddress addr);\n+  static zaddress blocking_load_barrier_on_phantom_slow_path(volatile zpointer* p, zaddress addr);\n+\n+  static zaddress verify_old_object_live_slow_path(zaddress addr);\n+\n+  static zaddress mark_slow_path(zaddress addr);\n+  static zaddress mark_young_slow_path(zaddress addr);\n+  static zaddress mark_from_young_slow_path(zaddress addr);\n+  static zaddress mark_from_old_slow_path(zaddress addr);\n+  static zaddress mark_finalizable_slow_path(zaddress addr);\n+  static zaddress mark_finalizable_from_old_slow_path(zaddress addr);\n+\n+  static zaddress keep_alive_slow_path(zaddress addr);\n+  static zaddress heap_store_slow_path(volatile zpointer* p, zaddress addr, zpointer prev, bool heal);\n+  static zaddress native_store_slow_path(zaddress addr);\n+  static zaddress no_keep_alive_heap_store_slow_path(volatile zpointer* p, zaddress addr);\n+\n+  static zaddress promote_slow_path(zaddress addr);\n+\n+  \/\/ Helpers for non-strong oop refs barriers\n+  static zaddress blocking_keep_alive_load_barrier_on_weak_oop_field_preloaded(volatile zpointer* p, zpointer o);\n+  static zaddress blocking_keep_alive_load_barrier_on_phantom_oop_field_preloaded(volatile zpointer* p, zpointer o);\n+  static zaddress blocking_load_barrier_on_weak_oop_field_preloaded(volatile zpointer* p, zpointer o);\n+  static zaddress blocking_load_barrier_on_phantom_oop_field_preloaded(volatile zpointer* p, zpointer o);\n+\n+  \/\/ Verification\n+  static void verify_on_weak(volatile zpointer* referent_addr) NOT_DEBUG_RETURN;\n@@ -50,3 +136,1 @@\n-  template <ZBarrierFastPath fast_path, ZBarrierSlowPath slow_path> static oop barrier(volatile oop* p, oop o);\n-  template <ZBarrierFastPath fast_path, ZBarrierSlowPath slow_path> static oop weak_barrier(volatile oop* p, oop o);\n-  template <ZBarrierFastPath fast_path, ZBarrierSlowPath slow_path> static void root_barrier(oop* p, oop o);\n+public:\n@@ -54,3 +138,1 @@\n-  static bool is_good_or_null_fast_path(uintptr_t addr);\n-  static bool is_weak_good_or_null_fast_path(uintptr_t addr);\n-  static bool is_marked_or_null_fast_path(uintptr_t addr);\n+  static zpointer load_atomic(volatile zpointer* p);\n@@ -58,9 +140,3 @@\n-  static bool during_mark();\n-  static bool during_relocate();\n-  template <bool finalizable> static bool should_mark_through(uintptr_t addr);\n-  template <bool gc_thread, bool follow, bool finalizable, bool publish> static uintptr_t mark(uintptr_t addr);\n-  static uintptr_t remap(uintptr_t addr);\n-  static uintptr_t relocate(uintptr_t addr);\n-  static uintptr_t relocate_or_mark(uintptr_t addr);\n-  static uintptr_t relocate_or_mark_no_follow(uintptr_t addr);\n-  static uintptr_t relocate_or_remap(uintptr_t addr);\n+  \/\/ Helpers for relocation\n+  static ZGeneration* remap_generation(zpointer ptr);\n+  static void remap_young_relocated(volatile zpointer* p, zpointer o);\n@@ -68,2 +144,7 @@\n-  static uintptr_t load_barrier_on_oop_slow_path(uintptr_t addr);\n-  static uintptr_t load_barrier_on_invisible_root_oop_slow_path(uintptr_t addr);\n+  \/\/ Helpers for marking\n+  template <bool resurrect, bool gc_thread, bool follow, bool finalizable>\n+  static void mark(zaddress addr);\n+  template <bool resurrect, bool gc_thread, bool follow>\n+  static void mark_young(zaddress addr);\n+  template <bool resurrect, bool gc_thread, bool follow>\n+  static void mark_if_young(zaddress addr);\n@@ -71,3 +152,3 @@\n-  static uintptr_t weak_load_barrier_on_oop_slow_path(uintptr_t addr);\n-  static uintptr_t weak_load_barrier_on_weak_oop_slow_path(uintptr_t addr);\n-  static uintptr_t weak_load_barrier_on_phantom_oop_slow_path(uintptr_t addr);\n+  \/\/ Load barrier\n+  static zaddress load_barrier_on_oop_field(volatile zpointer* p);\n+  static zaddress load_barrier_on_oop_field_preloaded(volatile zpointer* p, zpointer o);\n@@ -75,3 +156,1 @@\n-  static uintptr_t keep_alive_barrier_on_oop_slow_path(uintptr_t addr);\n-  static uintptr_t keep_alive_barrier_on_weak_oop_slow_path(uintptr_t addr);\n-  static uintptr_t keep_alive_barrier_on_phantom_oop_slow_path(uintptr_t addr);\n+  static zaddress keep_alive_load_barrier_on_oop_field_preloaded(volatile zpointer* p, zpointer o);\n@@ -79,2 +158,3 @@\n-  static uintptr_t mark_barrier_on_oop_slow_path(uintptr_t addr);\n-  static uintptr_t mark_barrier_on_finalizable_oop_slow_path(uintptr_t addr);\n+  \/\/ Load barriers on non-strong oop refs\n+  static zaddress load_barrier_on_weak_oop_field_preloaded(volatile zpointer* p, zpointer o);\n+  static zaddress load_barrier_on_phantom_oop_field_preloaded(volatile zpointer* p, zpointer o);\n@@ -82,1 +162,2 @@\n-  static void verify_on_weak(volatile oop* referent_addr) NOT_DEBUG_RETURN;\n+  static zaddress no_keep_alive_load_barrier_on_weak_oop_field_preloaded(volatile zpointer* p, zpointer o);\n+  static zaddress no_keep_alive_load_barrier_on_phantom_oop_field_preloaded(volatile zpointer* p, zpointer o);\n@@ -84,29 +165,4 @@\n-public:\n-  \/\/ Load barrier\n-  static  oop load_barrier_on_oop(oop o);\n-  static  oop load_barrier_on_oop_field(volatile oop* p);\n-  static  oop load_barrier_on_oop_field_preloaded(volatile oop* p, oop o);\n-  static void load_barrier_on_oop_array(volatile oop* p, size_t length);\n-  static void load_barrier_on_oop_fields(oop o);\n-  static  oop load_barrier_on_weak_oop_field_preloaded(volatile oop* p, oop o);\n-  static  oop load_barrier_on_phantom_oop_field_preloaded(volatile oop* p, oop o);\n-  static void load_barrier_on_root_oop_field(oop* p);\n-  static void load_barrier_on_invisible_root_oop_field(oop* p);\n-\n-  \/\/ Weak load barrier\n-  static oop weak_load_barrier_on_oop_field(volatile oop* p);\n-  static oop weak_load_barrier_on_oop_field_preloaded(volatile oop* p, oop o);\n-  static oop weak_load_barrier_on_weak_oop(oop o);\n-  static oop weak_load_barrier_on_weak_oop_field_preloaded(volatile oop* p, oop o);\n-  static oop weak_load_barrier_on_phantom_oop(oop o);\n-  static oop weak_load_barrier_on_phantom_oop_field_preloaded(volatile oop* p, oop o);\n-\n-  \/\/ Is alive barrier\n-  static bool is_alive_barrier_on_weak_oop(oop o);\n-  static bool is_alive_barrier_on_phantom_oop(oop o);\n-\n-  \/\/ Keep alive barrier\n-  static void keep_alive_barrier_on_oop(oop o);\n-  static void keep_alive_barrier_on_weak_oop_field(volatile oop* p);\n-  static void keep_alive_barrier_on_phantom_oop_field(volatile oop* p);\n-  static void keep_alive_barrier_on_phantom_root_oop_field(oop* p);\n+  \/\/ Reference processor \/ weak cleaning barriers\n+  static bool clean_barrier_on_weak_oop_field(volatile zpointer* p);\n+  static bool clean_barrier_on_phantom_oop_field(volatile zpointer* p);\n+  static bool clean_barrier_on_final_oop_field(volatile zpointer* p);\n@@ -115,18 +171,12 @@\n-  static void mark_barrier_on_oop_field(volatile oop* p, bool finalizable);\n-  static void mark_barrier_on_oop_array(volatile oop* p, size_t length, bool finalizable);\n-\n-  \/\/ Narrow oop variants, never used.\n-  static oop  load_barrier_on_oop_field(volatile narrowOop* p);\n-  static oop  load_barrier_on_oop_field_preloaded(volatile narrowOop* p, oop o);\n-  static void load_barrier_on_oop_array(volatile narrowOop* p, size_t length);\n-  static oop  load_barrier_on_weak_oop_field_preloaded(volatile narrowOop* p, oop o);\n-  static oop  load_barrier_on_phantom_oop_field_preloaded(volatile narrowOop* p, oop o);\n-  static oop  weak_load_barrier_on_oop_field_preloaded(volatile narrowOop* p, oop o);\n-  static oop  weak_load_barrier_on_weak_oop_field_preloaded(volatile narrowOop* p, oop o);\n-  static oop  weak_load_barrier_on_phantom_oop_field_preloaded(volatile narrowOop* p, oop o);\n-};\n-\n-class ZLoadBarrierOopClosure : public BasicOopIterateClosure {\n-public:\n-  virtual void do_oop(oop* p);\n-  virtual void do_oop(narrowOop* p);\n+  static void mark_barrier_on_young_oop_field(volatile zpointer* p);\n+  static void mark_barrier_on_old_oop_field(volatile zpointer* p, bool finalizable);\n+  static void mark_barrier_on_oop_field(volatile zpointer* p, bool finalizable);\n+  static void mark_young_good_barrier_on_oop_field(volatile zpointer* p);\n+  static zaddress remset_barrier_on_oop_field(volatile zpointer* p);\n+  static void promote_barrier_on_young_oop_field(volatile zpointer* p);\n+\n+  \/\/ Store barrier\n+  static void store_barrier_on_heap_oop_field(volatile zpointer* p, bool heal);\n+  static void store_barrier_on_native_oop_field(volatile zpointer* p, bool heal);\n+\n+  static void no_keep_alive_store_barrier_on_heap_oop_field(volatile zpointer* p);\n","filename":"src\/hotspot\/share\/gc\/z\/zBarrier.hpp","additions":140,"deletions":90,"binary":false,"changes":230,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -31,1 +31,2 @@\n-#include \"gc\/z\/zOop.inline.hpp\"\n+#include \"gc\/z\/zGeneration.inline.hpp\"\n+#include \"gc\/z\/zHeap.inline.hpp\"\n@@ -38,72 +39,36 @@\n-\/\/ accordance with the metadata bits state machine, which has the\n-\/\/ valid state transitions as described below (where N is the GC\n-\/\/ cycle).\n-\/\/\n-\/\/ Note the subtleness of overlapping GC cycles. Specifically that\n-\/\/ oops are colored Remapped(N) starting at relocation N and ending\n-\/\/ at marking N + 1.\n-\/\/\n-\/\/              +--- Mark Start\n-\/\/              | +--- Mark End\n-\/\/              | | +--- Relocate Start\n-\/\/              | | | +--- Relocate End\n-\/\/              | | | |\n-\/\/ Marked       |---N---|--N+1--|--N+2--|----\n-\/\/ Finalizable  |---N---|--N+1--|--N+2--|----\n-\/\/ Remapped     ----|---N---|--N+1--|--N+2--|\n-\/\/\n-\/\/ VALID STATE TRANSITIONS\n-\/\/\n-\/\/   Marked(N)           -> Remapped(N)\n-\/\/                       -> Marked(N + 1)\n-\/\/                       -> Finalizable(N + 1)\n-\/\/\n-\/\/   Finalizable(N)      -> Marked(N)\n-\/\/                       -> Remapped(N)\n-\/\/                       -> Marked(N + 1)\n-\/\/                       -> Finalizable(N + 1)\n-\/\/\n-\/\/   Remapped(N)         -> Marked(N + 1)\n-\/\/                       -> Finalizable(N + 1)\n-\/\/\n-\/\/ PHASE VIEW\n-\/\/\n-\/\/ ZPhaseMark\n-\/\/   Load & Mark\n-\/\/     Marked(N)         <- Marked(N - 1)\n-\/\/                       <- Finalizable(N - 1)\n-\/\/                       <- Remapped(N - 1)\n-\/\/                       <- Finalizable(N)\n-\/\/\n-\/\/   Mark(Finalizable)\n-\/\/     Finalizable(N)    <- Marked(N - 1)\n-\/\/                       <- Finalizable(N - 1)\n-\/\/                       <- Remapped(N - 1)\n-\/\/\n-\/\/   Load(AS_NO_KEEPALIVE)\n-\/\/     Remapped(N - 1)   <- Marked(N - 1)\n-\/\/                       <- Finalizable(N - 1)\n-\/\/\n-\/\/ ZPhaseMarkCompleted (Resurrection blocked)\n-\/\/   Load & Load(ON_WEAK\/PHANTOM_OOP_REF | AS_NO_KEEPALIVE) & KeepAlive\n-\/\/     Marked(N)         <- Marked(N - 1)\n-\/\/                       <- Finalizable(N - 1)\n-\/\/                       <- Remapped(N - 1)\n-\/\/                       <- Finalizable(N)\n-\/\/\n-\/\/   Load(ON_STRONG_OOP_REF | AS_NO_KEEPALIVE)\n-\/\/     Remapped(N - 1)   <- Marked(N - 1)\n-\/\/                       <- Finalizable(N - 1)\n-\/\/\n-\/\/ ZPhaseMarkCompleted (Resurrection unblocked)\n-\/\/   Load\n-\/\/     Marked(N)         <- Finalizable(N)\n-\/\/\n-\/\/ ZPhaseRelocate\n-\/\/   Load & Load(AS_NO_KEEPALIVE)\n-\/\/     Remapped(N)       <- Marked(N)\n-\/\/                       <- Finalizable(N)\n-\n-template <ZBarrierFastPath fast_path>\n-inline void ZBarrier::self_heal(volatile oop* p, uintptr_t addr, uintptr_t heal_addr) {\n-  if (heal_addr == 0) {\n+\/\/ accordance with the metadata bits state machine. The following\n+\/\/ assert verifies the monotonicity of the transitions.\n+\n+inline void ZBarrier::assert_transition_monotonicity(zpointer old_ptr, zpointer new_ptr) {\n+  const bool old_is_load_good = ZPointer::is_load_good(old_ptr);\n+  const bool old_is_mark_good = ZPointer::is_mark_good(old_ptr);\n+  const bool old_is_store_good = ZPointer::is_store_good(old_ptr);\n+\n+  const bool new_is_load_good = ZPointer::is_load_good(new_ptr);\n+  const bool new_is_mark_good = ZPointer::is_mark_good(new_ptr);\n+  const bool new_is_store_good = ZPointer::is_store_good(new_ptr);\n+\n+  assert(!old_is_load_good || new_is_load_good, \"non-monotonic load good transition\");\n+  assert(!old_is_mark_good || new_is_mark_good, \"non-monotonic mark good transition\");\n+  assert(!old_is_store_good || new_is_store_good, \"non-monotonic store good transition\");\n+\n+  if (is_null_any(new_ptr)) {\n+    \/\/ Null is good enough at this point\n+    return;\n+  }\n+\n+  const bool old_is_marked_young = ZPointer::is_marked_young(old_ptr);\n+  const bool old_is_marked_old = ZPointer::is_marked_old(old_ptr);\n+  const bool old_is_marked_finalizable = ZPointer::is_marked_finalizable(old_ptr);\n+\n+  const bool new_is_marked_young = ZPointer::is_marked_young(new_ptr);\n+  const bool new_is_marked_old = ZPointer::is_marked_old(new_ptr);\n+  const bool new_is_marked_finalizable = ZPointer::is_marked_finalizable(new_ptr);\n+\n+  assert(!old_is_marked_young || new_is_marked_young, \"non-monotonic marked young transition\");\n+  assert(!old_is_marked_old || new_is_marked_old, \"non-monotonic marked old transition\");\n+  assert(!old_is_marked_finalizable || new_is_marked_finalizable || new_is_marked_old, \"non-monotonic marked final transition\");\n+}\n+\n+inline void ZBarrier::self_heal(ZBarrierFastPath fast_path, volatile zpointer* p, zpointer ptr, zpointer heal_ptr, bool allow_null) {\n+  if (!allow_null && is_null_assert_load_good(heal_ptr) && !is_null_any(ptr)) {\n@@ -117,2 +82,6 @@\n-  assert(!fast_path(addr), \"Invalid self heal\");\n-  assert(fast_path(heal_addr), \"Invalid self heal\");\n+  assert_is_valid(ptr);\n+  assert_is_valid(heal_ptr);\n+  assert(!fast_path(ptr), \"Invalid self heal\");\n+  assert(fast_path(heal_ptr), \"Invalid self heal\");\n+\n+  assert(ZPointer::is_remapped(heal_ptr), \"invariant\");\n@@ -121,0 +90,6 @@\n+    if (ptr == zpointer::null) {\n+      assert(!ZVerifyOops || !ZHeap::heap()->is_in(uintptr_t(p)) || !ZHeap::heap()->is_old(p), \"No raw null in old\");\n+    }\n+\n+    assert_transition_monotonicity(ptr, heal_ptr);\n+\n@@ -122,2 +97,2 @@\n-    const uintptr_t prev_addr = Atomic::cmpxchg((volatile uintptr_t*)p, addr, heal_addr, memory_order_relaxed);\n-    if (prev_addr == addr) {\n+    const zpointer prev_ptr = Atomic::cmpxchg(p, ptr, heal_ptr, memory_order_relaxed);\n+    if (prev_ptr == ptr) {\n@@ -128,1 +103,1 @@\n-    if (fast_path(prev_addr)) {\n+    if (fast_path(prev_ptr)) {\n@@ -136,2 +111,1 @@\n-    assert(ZAddress::offset(prev_addr) == ZAddress::offset(heal_addr), \"Invalid offset\");\n-    addr = prev_addr;\n+    ptr = prev_ptr;\n@@ -141,3 +115,2 @@\n-template <ZBarrierFastPath fast_path, ZBarrierSlowPath slow_path>\n-inline oop ZBarrier::barrier(volatile oop* p, oop o) {\n-  const uintptr_t addr = ZOop::to_address(o);\n+inline ZGeneration* ZBarrier::remap_generation(zpointer ptr) {\n+  assert(!ZPointer::is_load_good(ptr), \"no need to remap load-good pointer\");\n@@ -145,3 +118,2 @@\n-  \/\/ Fast path\n-  if (fast_path(addr)) {\n-    return ZOop::from_address(addr);\n+  if (ZPointer::is_old_load_good(ptr)) {\n+    return ZGeneration::young();\n@@ -150,2 +122,9 @@\n-  \/\/ Slow path\n-  const uintptr_t good_addr = slow_path(addr);\n+  if (ZPointer::is_young_load_good(ptr)) {\n+    return ZGeneration::old();\n+  }\n+\n+  \/\/ Double remap bad - the pointer is neither old load good nor\n+  \/\/ young load good. First the code ...\n+\n+  const uintptr_t remembered_bits = untype(ptr) & ZPointerRememberedMask;\n+  const bool old_to_old_ptr = remembered_bits == ZPointerRememberedMask;\n@@ -153,2 +132,10 @@\n-  if (p != NULL) {\n-    self_heal<fast_path>(p, addr, good_addr);\n+  if (old_to_old_ptr) {\n+    return ZGeneration::old();\n+  }\n+\n+  const zaddress_unsafe addr = ZPointer::uncolor_unsafe(ptr);\n+  if (ZGeneration::young()->forwarding(addr) != nullptr) {\n+    assert(ZGeneration::old()->forwarding(addr) == nullptr, \"Mutually exclusive\");\n+    return ZGeneration::young();\n+  } else {\n+    return ZGeneration::old();\n@@ -157,1 +144,153 @@\n-  return ZOop::from_address(good_addr);\n+  \/\/ ... then the explanation. Time to put your seat belt on.\n+\n+  \/\/ In this context we only have access to the ptr (colored oop), but we\n+  \/\/ don't know if this refers to a stale young gen or old gen object.\n+  \/\/ However, by being careful with when we run young and old collections,\n+  \/\/ and by explicitly remapping roots we can figure this out by looking\n+  \/\/ at the metadata bits in the pointer.\n+\n+  \/\/ *Roots (including remset)*:\n+  \/\/\n+  \/\/ will never have double remap bit errors,\n+  \/\/ and will never enter this path. The reason is that there's always a\n+  \/\/ phase that remaps all roots between all relocation phases:\n+  \/\/\n+  \/\/ 1) Young marking remaps the roots, before the young relocation runs\n+  \/\/\n+  \/\/ 2) The old roots_remap phase blocks out young collections and runs just\n+  \/\/    before old relocation starts\n+\n+  \/\/ *Heap object fields*:\n+  \/\/\n+  \/\/ could have double remap bit errors, and may enter this path. We are using\n+  \/\/ knowledge about how *remember* bits are set, to narrow down the\n+  \/\/ possibilities.\n+\n+  \/\/ Short summary:\n+  \/\/\n+  \/\/ If both remember bits are set, when we have a double\n+  \/\/ remap bit error, then we know that we are dealing with\n+  \/\/ an old-to-old pointer.\n+  \/\/\n+  \/\/ Otherwise, we are dealing with a young-to-any pointer,\n+  \/\/ and the address that contained the pointed-to object, is\n+  \/\/ guaranteed to have only been used by either the young gen\n+  \/\/ or the old gen.\n+\n+  \/\/ Longer explanation:\n+\n+  \/\/ Double remap bad pointers in young gen:\n+  \/\/\n+  \/\/ After young relocation, the young gen objects were promoted to old gen,\n+  \/\/ and we keep track of those old-to-young pointers via the remset\n+  \/\/ (described above in the roots section).\n+  \/\/\n+  \/\/ However, when young marking started, the current set of young gen objects\n+  \/\/ are snapshotted, and subsequent allocations end up in the next young\n+  \/\/ collection. Between young mark start, and young relocate start, stores\n+  \/\/ can happen to either the \"young allocating\" objects, or objects that\n+  \/\/ are about to become survivors. For both survivors and young-allocating\n+  \/\/ objects, it is true that their zpointers will be store good when\n+  \/\/ young marking finishes, and can not get demoted. These pointers will become\n+  \/\/ young remap bad after young relocate start. We don't maintain a remset\n+  \/\/ for the young allocating objects, so we don't have the same guarantee as\n+  \/\/ we have for roots (including remset). Pointers in these objects are\n+  \/\/ therefore therefore susceptible to become double remap bad.\n+  \/\/\n+  \/\/ The scenario that can happen is:\n+  \/\/   - Store in young allocating or future survivor happens between young mark\n+  \/\/     start and young relocate start\n+  \/\/   - Young relocate start makes this pointer young remap bad\n+  \/\/   - It is NOT fixed in roots_remap (it is not part of the remset or roots)\n+  \/\/   - Old relocate start makes this pointer also old remap bad\n+\n+  \/\/ Double remap bad pointers in old gen:\n+  \/\/\n+  \/\/ When an object is promoted, all oop*s are added to the remset. (Could\n+  \/\/ have either double or single remember bits at this point)\n+  \/\/\n+  \/\/ As long as we have a remset entry for the oop*, we ensure that the pointer\n+  \/\/ is not double remap bad. See the roots section.\n+  \/\/\n+  \/\/ However, at some point the GC notices that the pointer points to an old\n+  \/\/ object, and that there's no need for a remset entry. Because of that,\n+  \/\/ the young collection will not visit the pointer, and the pointer can\n+  \/\/ become double remap bad.\n+  \/\/\n+  \/\/ The scenario that can happen is:\n+  \/\/   - Old marking visits the object\n+  \/\/   - Old relocation starts and then young relocation starts\n+  \/\/      or\n+  \/\/   - Young relocation starts and then old relocation starts\n+\n+  \/\/ About double *remember* bits:\n+  \/\/\n+  \/\/ Whenever we:\n+  \/\/ - perform a store barrier, we heal with one remember bit.\n+  \/\/ - mark objects in young gen, we heal with one remember bit.\n+  \/\/ - perform a non-store barrier outside of young gen, we heal with\n+  \/\/   double remember bits.\n+  \/\/ - \"remset forget\" a pointer in an old object, we heal with double\n+  \/\/   remember bits.\n+  \/\/\n+  \/\/ Double remember bits ensures that *every* store that encounters it takes\n+  \/\/ a slow path.\n+  \/\/\n+  \/\/ If we encounter a pointer that is both double remap bad *and* has double\n+  \/\/ remember bits, we know that it can't be young and it has to be old!\n+  \/\/\n+  \/\/ Pointers in young objects:\n+  \/\/\n+  \/\/ The only double remap bad young pointers are inside \"young allocating\"\n+  \/\/ objects and survivors, as described above. When such a pointer was written\n+  \/\/ into the young allocating memory, or marked in young gen, the pointer was\n+  \/\/ remap good and the store\/young mark barrier healed with a single remember bit.\n+  \/\/ No other barrier could replace that bit, because store good is the greatest\n+  \/\/ barrier, and all other barriers will take the fast-path. This is true until\n+  \/\/ the young relocation starts.\n+  \/\/\n+  \/\/ After the young relocation has started, the pointer became young remap\n+  \/\/ bad, and maybe we even started an old relocation, and the pointer became\n+  \/\/ double remap bad. When the next load barrier triggers, it will self heal\n+  \/\/ with double remember bits, but *importantly* it will at the same time\n+  \/\/ heal with good remap bits.\n+  \/\/\n+  \/\/ So, if we have entered this \"double remap bad\" path, and the pointer was\n+  \/\/ located in young gen, then it was young allocating or a survivor, and it\n+  \/\/ must only have one remember bit set!\n+  \/\/\n+  \/\/ Pointers in old objects:\n+  \/\/\n+  \/\/ When pointers become forgotten, they are tagged with double remembered\n+  \/\/ bits. Only way to convert the pointer into having only one remembered\n+  \/\/ bit, is to perform a store. When that happens, the pointer becomes both\n+  \/\/ remap good and remembered again, and will be handled as the roots\n+  \/\/ described above.\n+\n+  \/\/ With the above information:\n+  \/\/\n+  \/\/ Iff we find a double remap bad pointer with *double remember bits*,\n+  \/\/ then we know that it is an old-to-old pointer, and we should use the\n+  \/\/ forwarding table of the old generation.\n+  \/\/\n+  \/\/ Iff we find a double remap bad pointer with a *single remember bit*,\n+  \/\/ then we know that it is a young-to-any pointer. We still don't know\n+  \/\/ if the pointed-to object is young or old.\n+\n+  \/\/ Figuring out if a double remap bad pointer in young pointed at\n+  \/\/ young or old:\n+  \/\/\n+  \/\/ The scenario that created a double remap bad pointer in the young\n+  \/\/ allocating or survivor memory is that it was written during the last\n+  \/\/ young marking before the old relocation started. At that point, the old\n+  \/\/ generation collection has already taken its marking snapshot, and\n+  \/\/ determined what pages will be marked and therefore eligible to become\n+  \/\/ part of the old relocation set. If the young generation relocated\/freed\n+  \/\/ a page (address range), and that address range was then reused for an old\n+  \/\/ page, it won't be part of the old snapshot and it therefore won't be\n+  \/\/ selected for old relocation.\n+  \/\/\n+  \/\/ Because of this, we know that the object written into the young\n+  \/\/ allocating page will at most belong to one of the two relocation sets,\n+  \/\/ and we can therefore simply check in which table we installed\n+  \/\/ ZForwarding.\n@@ -160,3 +299,4 @@\n-template <ZBarrierFastPath fast_path, ZBarrierSlowPath slow_path>\n-inline oop ZBarrier::weak_barrier(volatile oop* p, oop o) {\n-  const uintptr_t addr = ZOop::to_address(o);\n+inline zaddress ZBarrier::make_load_good(zpointer o) {\n+  if (is_null_any(o)) {\n+    return zaddress::null;\n+  }\n@@ -164,5 +304,2 @@\n-  \/\/ Fast path\n-  if (fast_path(addr)) {\n-    \/\/ Return the good address instead of the weak good address\n-    \/\/ to ensure that the currently active heap view is used.\n-    return ZOop::from_address(ZAddress::good_or_null(addr));\n+  if (ZPointer::is_load_good_or_null(o)) {\n+    return ZPointer::uncolor(o);\n@@ -171,2 +308,2 @@\n-  \/\/ Slow path\n-  const uintptr_t good_addr = slow_path(addr);\n+  return relocate_or_remap(ZPointer::uncolor_unsafe(o), remap_generation(o));\n+}\n@@ -174,4 +311,3 @@\n-  if (p != NULL) {\n-    \/\/ The slow path returns a good\/marked address or null, but we never mark\n-    \/\/ oops in a weak load barrier so we always heal with the remapped address.\n-    self_heal<fast_path>(p, addr, ZAddress::remapped_or_null(good_addr));\n+inline zaddress ZBarrier::make_load_good_no_relocate(zpointer o) {\n+  if (is_null_any(o)) {\n+    return zaddress::null;\n@@ -180,1 +316,13 @@\n-  return ZOop::from_address(good_addr);\n+  if (ZPointer::is_load_good_or_null(o)) {\n+    return ZPointer::uncolor(o);\n+  }\n+\n+  return remap(ZPointer::uncolor_unsafe(o), remap_generation(o));\n+}\n+\n+inline void z_assert_is_barrier_safe() {\n+  assert(!Thread::current()->is_ConcurrentGC_thread() ||          \/* Need extra checks for ConcurrentGCThreads *\/\n+         Thread::current()->is_suspendible_thread() ||            \/* Thread prevents safepoints *\/\n+         Thread::current()->is_indirectly_suspendible_thread() || \/* Coordinator thread prevents safepoints *\/\n+         SafepointSynchronize::is_at_safepoint(),                 \/* Is at safepoint *\/\n+         \"Shouldn't perform load barrier\");\n@@ -183,3 +331,3 @@\n-template <ZBarrierFastPath fast_path, ZBarrierSlowPath slow_path>\n-inline void ZBarrier::root_barrier(oop* p, oop o) {\n-  const uintptr_t addr = ZOop::to_address(o);\n+template <typename ZBarrierSlowPath>\n+inline zaddress ZBarrier::barrier(ZBarrierFastPath fast_path, ZBarrierSlowPath slow_path, ZBarrierColor color, volatile zpointer* p, zpointer o, bool allow_null) {\n+  z_assert_is_barrier_safe();\n@@ -188,2 +336,2 @@\n-  if (fast_path(addr)) {\n-    return;\n+  if (fast_path(o)) {\n+    return ZPointer::uncolor(o);\n@@ -192,0 +340,3 @@\n+  \/\/ Make load good\n+  const zaddress load_good_addr = make_load_good(o);\n+\n@@ -193,1 +344,24 @@\n-  const uintptr_t good_addr = slow_path(addr);\n+  const zaddress good_addr = slow_path(load_good_addr);\n+\n+  \/\/ Self heal\n+  if (p != nullptr) {\n+    \/\/ Color\n+    const zpointer good_ptr = color(good_addr, o);\n+\n+    assert(!is_null(good_ptr), \"Always block raw null\");\n+\n+    self_heal(fast_path, p, o, good_ptr, allow_null);\n+  }\n+\n+  return good_addr;\n+}\n+\n+inline void ZBarrier::remap_young_relocated(volatile zpointer* p, zpointer o) {\n+  assert(ZPointer::is_old_load_good(o), \"Should be old load good\");\n+  assert(!ZPointer::is_young_load_good(o), \"Should not be young load good\");\n+\n+  \/\/ Make load good\n+  const zaddress load_good_addr = make_load_good_no_relocate(o);\n+\n+  \/\/ Color\n+  const zpointer good_ptr = ZAddress::load_good(load_good_addr,  o);\n@@ -195,8 +369,11 @@\n-  \/\/ Non-atomic healing helps speed up root scanning. This is safe to do\n-  \/\/ since we are always healing roots in a safepoint, or under a lock,\n-  \/\/ which ensures we are never racing with mutators modifying roots while\n-  \/\/ we are healing them. It's also safe in case multiple GC threads try\n-  \/\/ to heal the same root if it is aligned, since they would always heal\n-  \/\/ the root in the same way and it does not matter in which order it\n-  \/\/ happens. For misaligned oops, there needs to be mutual exclusion.\n-  *p = ZOop::from_address(good_addr);\n+  assert(!is_null(good_ptr), \"Always block raw null\");\n+\n+  \/\/ Despite knowing good_ptr isn't null in this context, we use the\n+  \/\/ load_good_or_null fast path, because it is faster.\n+  self_heal(is_load_good_or_null_fast_path, p, o, good_ptr, false \/* allow_null *\/);\n+}\n+\n+inline zpointer ZBarrier::load_atomic(volatile zpointer* p) {\n+  const zpointer ptr = Atomic::load(p);\n+  assert_is_valid(ptr);\n+  return ptr;\n@@ -205,2 +382,6 @@\n-inline bool ZBarrier::is_good_or_null_fast_path(uintptr_t addr) {\n-  return ZAddress::is_good_or_null(addr);\n+\/\/\n+\/\/ Fast paths\n+\/\/\n+\n+inline bool ZBarrier::is_load_good_or_null_fast_path(zpointer ptr) {\n+  return ZPointer::is_load_good_or_null(ptr);\n@@ -209,2 +390,2 @@\n-inline bool ZBarrier::is_weak_good_or_null_fast_path(uintptr_t addr) {\n-  return ZAddress::is_weak_good_or_null(addr);\n+inline bool ZBarrier::is_mark_good_fast_path(zpointer ptr) {\n+  return ZPointer::is_mark_good(ptr);\n@@ -213,2 +394,2 @@\n-inline bool ZBarrier::is_marked_or_null_fast_path(uintptr_t addr) {\n-  return ZAddress::is_marked_or_null(addr);\n+inline bool ZBarrier::is_store_good_fast_path(zpointer ptr) {\n+  return ZPointer::is_store_good(ptr);\n@@ -217,2 +398,2 @@\n-inline bool ZBarrier::during_mark() {\n-  return ZGlobalPhase == ZPhaseMark;\n+inline bool ZBarrier::is_store_good_or_null_fast_path(zpointer ptr) {\n+  return ZPointer::is_store_good_or_null(ptr);\n@@ -221,2 +402,10 @@\n-inline bool ZBarrier::during_relocate() {\n-  return ZGlobalPhase == ZPhaseRelocate;\n+inline bool ZBarrier::is_store_good_or_null_any_fast_path(zpointer ptr) {\n+  return is_null_any(ptr) || !ZPointer::is_store_bad(ptr);\n+}\n+\n+inline bool ZBarrier::is_mark_young_good_fast_path(zpointer ptr) {\n+  return ZPointer::is_load_good(ptr) && ZPointer::is_marked_young(ptr);\n+}\n+\n+inline bool ZBarrier::is_finalizable_good_fast_path(zpointer ptr) {\n+  return ZPointer::is_load_good(ptr) && ZPointer::is_marked_any_old(ptr);\n@@ -226,1 +415,8 @@\n-\/\/ Load barrier\n+\/\/ Slow paths\n+\/\/\n+\n+inline zaddress ZBarrier::promote_slow_path(zaddress addr) {\n+  \/\/ No need to do anything\n+  return addr;\n+}\n+\n@@ -228,2 +424,39 @@\n-inline oop ZBarrier::load_barrier_on_oop(oop o) {\n-  return load_barrier_on_oop_field_preloaded((oop*)NULL, o);\n+\/\/ Color functions\n+\/\/\n+\n+inline zpointer color_load_good(zaddress new_addr, zpointer old_ptr) {\n+  return ZAddress::load_good(new_addr, old_ptr);\n+}\n+\n+inline zpointer color_finalizable_good(zaddress new_addr, zpointer old_ptr) {\n+  if (ZPointer::is_marked_old(old_ptr)) {\n+    \/\/ Don't down-grade pointers\n+    return ZAddress::mark_old_good(new_addr, old_ptr);\n+  } else {\n+    return ZAddress::finalizable_good(new_addr, old_ptr);\n+  }\n+}\n+\n+inline zpointer color_mark_good(zaddress new_addr, zpointer old_ptr) {\n+  return ZAddress::mark_good(new_addr, old_ptr);\n+}\n+\n+inline zpointer color_mark_young_good(zaddress new_addr, zpointer old_ptr) {\n+  return ZAddress::mark_young_good(new_addr, old_ptr);\n+}\n+\n+inline zpointer color_remset_good(zaddress new_addr, zpointer old_ptr) {\n+  if (new_addr == zaddress::null || ZHeap::heap()->is_young(new_addr)) {\n+    return ZAddress::mark_good(new_addr, old_ptr);\n+  } else {\n+    \/\/ If remembered set scanning finds an old-to-old pointer, we won't mark it\n+    \/\/ and hence only really care about setting remembered bits to 11 so that\n+    \/\/ subsequent stores trip on the store-bad bit pattern. However, the contract\n+    \/\/ with the fast path check, is that the pointer should invariantly be young\n+    \/\/ mark good at least, so we color it as such.\n+    return ZAddress::mark_young_good(new_addr, old_ptr);\n+  }\n+}\n+\n+inline zpointer color_store_good(zaddress new_addr, zpointer old_ptr) {\n+  return ZAddress::store_good(new_addr);\n@@ -232,2 +465,6 @@\n-inline oop ZBarrier::load_barrier_on_oop_field(volatile oop* p) {\n-  const oop o = Atomic::load(p);\n+\/\/\n+\/\/ Load barrier\n+\/\/\n+\n+inline zaddress ZBarrier::load_barrier_on_oop_field(volatile zpointer* p) {\n+  const zpointer o = load_atomic(p);\n@@ -237,2 +474,25 @@\n-inline oop ZBarrier::load_barrier_on_oop_field_preloaded(volatile oop* p, oop o) {\n-  return barrier<is_good_or_null_fast_path, load_barrier_on_oop_slow_path>(p, o);\n+inline zaddress ZBarrier::load_barrier_on_oop_field_preloaded(volatile zpointer* p, zpointer o) {\n+  auto slow_path = [](zaddress addr) -> zaddress {\n+    return addr;\n+  };\n+\n+  return barrier(is_load_good_or_null_fast_path, slow_path, color_load_good, p, o);\n+}\n+\n+inline zaddress ZBarrier::keep_alive_load_barrier_on_oop_field_preloaded(volatile zpointer* p, zpointer o) {\n+  assert(!ZResurrection::is_blocked(), \"This operation is only valid when resurrection is not blocked\");\n+  return barrier(is_mark_good_fast_path, keep_alive_slow_path, color_mark_good, p, o);\n+}\n+\n+\/\/\n+\/\/ Load barrier on non-strong oop refs\n+\/\/\n+\n+inline zaddress ZBarrier::load_barrier_on_weak_oop_field_preloaded(volatile zpointer* p, zpointer o) {\n+  verify_on_weak(p);\n+\n+  if (ZResurrection::is_blocked()) {\n+    return blocking_keep_alive_load_barrier_on_weak_oop_field_preloaded(p, o);\n+  }\n+\n+  return keep_alive_load_barrier_on_oop_field_preloaded(p, o);\n@@ -241,3 +501,3 @@\n-inline void ZBarrier::load_barrier_on_oop_array(volatile oop* p, size_t length) {\n-  for (volatile const oop* const end = p + length; p < end; p++) {\n-    load_barrier_on_oop_field(p);\n+inline zaddress ZBarrier::load_barrier_on_phantom_oop_field_preloaded(volatile zpointer* p, zpointer o) {\n+  if (ZResurrection::is_blocked()) {\n+    return blocking_keep_alive_load_barrier_on_phantom_oop_field_preloaded(p, o);\n@@ -245,0 +505,2 @@\n+\n+  return keep_alive_load_barrier_on_oop_field_preloaded(p, o);\n@@ -247,1 +509,1 @@\n-inline oop ZBarrier::load_barrier_on_weak_oop_field_preloaded(volatile oop* p, oop o) {\n+inline zaddress ZBarrier::no_keep_alive_load_barrier_on_weak_oop_field_preloaded(volatile zpointer* p, zpointer o) {\n@@ -251,1 +513,1 @@\n-    return barrier<is_good_or_null_fast_path, weak_load_barrier_on_weak_oop_slow_path>(p, o);\n+    return blocking_load_barrier_on_weak_oop_field_preloaded(p, o);\n@@ -254,0 +516,1 @@\n+  \/\/ Normal load barrier doesn't keep the object alive\n@@ -257,1 +520,1 @@\n-inline oop ZBarrier::load_barrier_on_phantom_oop_field_preloaded(volatile oop* p, oop o) {\n+inline zaddress ZBarrier::no_keep_alive_load_barrier_on_phantom_oop_field_preloaded(volatile zpointer* p, zpointer o) {\n@@ -259,1 +522,1 @@\n-    return barrier<is_good_or_null_fast_path, weak_load_barrier_on_phantom_oop_slow_path>(p, o);\n+    return blocking_load_barrier_on_phantom_oop_field_preloaded(p, o);\n@@ -262,0 +525,1 @@\n+  \/\/ Normal load barrier doesn't keep the object alive\n@@ -265,3 +529,19 @@\n-inline void ZBarrier::load_barrier_on_root_oop_field(oop* p) {\n-  const oop o = *p;\n-  root_barrier<is_good_or_null_fast_path, load_barrier_on_oop_slow_path>(p, o);\n+inline zaddress ZBarrier::blocking_keep_alive_load_barrier_on_weak_oop_field_preloaded(volatile zpointer* p, zpointer o) {\n+  auto slow_path = [=](zaddress addr) -> zaddress {\n+    return ZBarrier::blocking_keep_alive_on_weak_slow_path(p, addr);\n+  };\n+  return barrier(is_mark_good_fast_path, slow_path, color_mark_good, p, o);\n+}\n+\n+inline zaddress ZBarrier::blocking_keep_alive_load_barrier_on_phantom_oop_field_preloaded(volatile zpointer* p, zpointer o) {\n+  auto slow_path = [=](zaddress addr) -> zaddress {\n+    return ZBarrier::blocking_keep_alive_on_phantom_slow_path(p, addr);\n+  };\n+  return barrier(is_mark_good_fast_path, slow_path, color_mark_good, p, o);\n+}\n+\n+inline zaddress ZBarrier::blocking_load_barrier_on_weak_oop_field_preloaded(volatile zpointer* p, zpointer o) {\n+  auto slow_path = [=](zaddress addr) -> zaddress {\n+    return ZBarrier::blocking_load_barrier_on_weak_slow_path(p, addr);\n+  };\n+  return barrier(is_mark_good_fast_path, slow_path, color_mark_good, p, o);\n@@ -270,3 +550,5 @@\n-inline void ZBarrier::load_barrier_on_invisible_root_oop_field(oop* p) {\n-  const oop o = *p;\n-  root_barrier<is_good_or_null_fast_path, load_barrier_on_invisible_root_oop_slow_path>(p, o);\n+inline zaddress ZBarrier::blocking_load_barrier_on_phantom_oop_field_preloaded(volatile zpointer* p, zpointer o) {\n+  auto slow_path = [=](zaddress addr) -> zaddress {\n+    return ZBarrier::blocking_load_barrier_on_phantom_slow_path(p, addr);\n+  };\n+  return barrier(is_mark_good_fast_path, slow_path, color_mark_good, p, o);\n@@ -276,1 +558,1 @@\n-\/\/ Weak load barrier\n+\/\/ Clean barrier\n@@ -278,4 +560,8 @@\n-inline oop ZBarrier::weak_load_barrier_on_oop_field(volatile oop* p) {\n-  assert(!ZResurrection::is_blocked(), \"Should not be called during resurrection blocked phase\");\n-  const oop o = Atomic::load(p);\n-  return weak_load_barrier_on_oop_field_preloaded(p, o);\n+\n+inline bool ZBarrier::clean_barrier_on_weak_oop_field(volatile zpointer* p) {\n+  assert(ZResurrection::is_blocked(), \"This operation is only valid when resurrection is blocked\");\n+  const zpointer o = load_atomic(p);\n+  auto slow_path = [=](zaddress addr) -> zaddress {\n+    return ZBarrier::blocking_load_barrier_on_weak_slow_path(p, addr);\n+  };\n+  return is_null(barrier(is_mark_good_fast_path, slow_path, color_mark_good, p, o, true \/* allow_null *\/));\n@@ -284,2 +570,7 @@\n-inline oop ZBarrier::weak_load_barrier_on_oop_field_preloaded(volatile oop* p, oop o) {\n-  return weak_barrier<is_weak_good_or_null_fast_path, weak_load_barrier_on_oop_slow_path>(p, o);\n+inline bool ZBarrier::clean_barrier_on_phantom_oop_field(volatile zpointer* p) {\n+  assert(ZResurrection::is_blocked(), \"This operation is only valid when resurrection is blocked\");\n+  const zpointer o = load_atomic(p);\n+  auto slow_path = [=](zaddress addr) -> zaddress {\n+    return ZBarrier::blocking_load_barrier_on_phantom_slow_path(p, addr);\n+  };\n+  return is_null(barrier(is_mark_good_fast_path, slow_path, color_mark_good, p, o, true \/* allow_null *\/));\n@@ -288,2 +579,14 @@\n-inline oop ZBarrier::weak_load_barrier_on_weak_oop(oop o) {\n-  return weak_load_barrier_on_weak_oop_field_preloaded((oop*)NULL, o);\n+inline bool ZBarrier::clean_barrier_on_final_oop_field(volatile zpointer* p) {\n+  assert(ZResurrection::is_blocked(), \"Invalid phase\");\n+\n+  \/\/ The referent in a FinalReference should never be cleared by the GC. Instead\n+  \/\/ it should just be healed (as if it was a phantom oop) and this function should\n+  \/\/ return true if the object pointer to by the referent is not strongly reachable.\n+  const zpointer o = load_atomic(p);\n+  auto slow_path = [=](zaddress addr) -> zaddress {\n+    return ZBarrier::blocking_load_barrier_on_phantom_slow_path(p, addr);\n+  };\n+  const zaddress addr = barrier(is_mark_good_fast_path, slow_path, color_mark_good, p, o);\n+  assert(!is_null(addr), \"Should be finalizable marked\");\n+\n+  return is_null(blocking_load_barrier_on_weak_slow_path(p, addr));\n@@ -292,2 +595,5 @@\n-inline oop ZBarrier::weak_load_barrier_on_weak_oop_field_preloaded(volatile oop* p, oop o) {\n-  verify_on_weak(p);\n+\/\/\n+\/\/ Mark barrier\n+\/\/\n+inline void ZBarrier::mark_barrier_on_oop_field(volatile zpointer* p, bool finalizable) {\n+  const zpointer o = load_atomic(p);\n@@ -295,2 +601,29 @@\n-  if (ZResurrection::is_blocked()) {\n-    return barrier<is_good_or_null_fast_path, weak_load_barrier_on_weak_oop_slow_path>(p, o);\n+  if (finalizable) {\n+    \/\/ During marking, we mark through already marked oops to avoid having\n+    \/\/ some large part of the object graph hidden behind a pushed, but not\n+    \/\/ yet flushed, entry on a mutator mark stack. Always marking through\n+    \/\/ allows the GC workers to proceed through the object graph even if a\n+    \/\/ mutator touched an oop first, which in turn will reduce the risk of\n+    \/\/ having to flush mark stacks multiple times to terminate marking.\n+    \/\/\n+    \/\/ However, when doing finalizable marking we don't always want to mark\n+    \/\/ through. First, marking through an already strongly marked oop would\n+    \/\/ be wasteful, since we will then proceed to do finalizable marking on\n+    \/\/ an object which is, or will be, marked strongly. Second, marking\n+    \/\/ through an already finalizable marked oop would also be wasteful,\n+    \/\/ since such oops can never end up on a mutator mark stack and can\n+    \/\/ therefore not hide some part of the object graph from GC workers.\n+\n+    \/\/ Make the oop finalizable marked\/good, instead of normal marked\/good.\n+    \/\/ This is needed because an object might first becomes finalizable\n+    \/\/ marked by the GC, and then loaded by a mutator thread. In this case,\n+    \/\/ the mutator thread must be able to tell that the object needs to be\n+    \/\/ strongly marked. The finalizable bit in the oop exists to make sure\n+    \/\/ that a load of a finalizable marked oop will fall into the barrier\n+    \/\/ slow path so that we can mark the object as strongly reachable.\n+\n+    \/\/ Note: that this does not color the pointer finalizable marked if it\n+    \/\/ is already colored marked old good.\n+    barrier(is_finalizable_good_fast_path, mark_finalizable_slow_path, color_finalizable_good, p, o);\n+  } else {\n+    barrier(is_mark_good_fast_path, mark_slow_path, color_mark_good, p, o);\n@@ -298,0 +631,5 @@\n+}\n+\n+inline void ZBarrier::mark_barrier_on_old_oop_field(volatile zpointer* p, bool finalizable) {\n+  assert(ZHeap::heap()->is_old(p), \"Should be from old\");\n+  const zpointer o = load_atomic(p);\n@@ -299,1 +637,30 @@\n-  return weak_load_barrier_on_oop_field_preloaded(p, o);\n+  if (finalizable) {\n+    \/\/ During marking, we mark through already marked oops to avoid having\n+    \/\/ some large part of the object graph hidden behind a pushed, but not\n+    \/\/ yet flushed, entry on a mutator mark stack. Always marking through\n+    \/\/ allows the GC workers to proceed through the object graph even if a\n+    \/\/ mutator touched an oop first, which in turn will reduce the risk of\n+    \/\/ having to flush mark stacks multiple times to terminate marking.\n+    \/\/\n+    \/\/ However, when doing finalizable marking we don't always want to mark\n+    \/\/ through. First, marking through an already strongly marked oop would\n+    \/\/ be wasteful, since we will then proceed to do finalizable marking on\n+    \/\/ an object which is, or will be, marked strongly. Second, marking\n+    \/\/ through an already finalizable marked oop would also be wasteful,\n+    \/\/ since such oops can never end up on a mutator mark stack and can\n+    \/\/ therefore not hide some part of the object graph from GC workers.\n+\n+    \/\/ Make the oop finalizable marked\/good, instead of normal marked\/good.\n+    \/\/ This is needed because an object might first becomes finalizable\n+    \/\/ marked by the GC, and then loaded by a mutator thread. In this case,\n+    \/\/ the mutator thread must be able to tell that the object needs to be\n+    \/\/ strongly marked. The finalizable bit in the oop exists to make sure\n+    \/\/ that a load of a finalizable marked oop will fall into the barrier\n+    \/\/ slow path so that we can mark the object as strongly reachable.\n+\n+    \/\/ Note: that this does not color the pointer finalizable marked if it\n+    \/\/ is already colored marked old good.\n+    barrier(is_finalizable_good_fast_path, mark_finalizable_from_old_slow_path, color_finalizable_good, p, o);\n+  } else {\n+    barrier(is_mark_good_fast_path, mark_from_old_slow_path, color_mark_good, p, o);\n+  }\n@@ -302,2 +669,4 @@\n-inline oop ZBarrier::weak_load_barrier_on_phantom_oop(oop o) {\n-  return weak_load_barrier_on_phantom_oop_field_preloaded((oop*)NULL, o);\n+inline void ZBarrier::mark_barrier_on_young_oop_field(volatile zpointer* p) {\n+  assert(ZHeap::heap()->is_young(p), \"Should be from young\");\n+  const zpointer o = load_atomic(p);\n+  barrier(is_store_good_or_null_any_fast_path, mark_from_young_slow_path, color_store_good, p, o);\n@@ -306,4 +675,17 @@\n-inline oop ZBarrier::weak_load_barrier_on_phantom_oop_field_preloaded(volatile oop* p, oop o) {\n-  if (ZResurrection::is_blocked()) {\n-    return barrier<is_good_or_null_fast_path, weak_load_barrier_on_phantom_oop_slow_path>(p, o);\n-  }\n+inline void ZBarrier::promote_barrier_on_young_oop_field(volatile zpointer* p) {\n+  const zpointer o = load_atomic(p);\n+  \/\/ Objects that get promoted to the old generation, must invariantly contain\n+  \/\/ only store good pointers. However, the young marking code above filters\n+  \/\/ out null pointers, so we need to explicitly ensure even null pointers are\n+  \/\/ store good, before objects may get promoted (and before relocate start).\n+  \/\/ This barrier ensures that.\n+  \/\/ This could simply be ensured in the marking above, but promotion rates\n+  \/\/ are typically rather low, and fixing all null pointers strictly, when\n+  \/\/ only a few had to be store good due to promotions, is generally not favourable\n+  barrier(is_store_good_fast_path, promote_slow_path, color_store_good, p, o);\n+}\n+\n+inline zaddress ZBarrier::remset_barrier_on_oop_field(volatile zpointer* p) {\n+  const zpointer o = load_atomic(p);\n+  return barrier(is_mark_young_good_fast_path, mark_young_slow_path, color_remset_good, p, o);\n+}\n@@ -311,1 +693,3 @@\n-  return weak_load_barrier_on_oop_field_preloaded(p, o);\n+inline void ZBarrier::mark_young_good_barrier_on_oop_field(volatile zpointer* p) {\n+  const zpointer o = load_atomic(p);\n+  barrier(is_mark_young_good_fast_path, mark_young_slow_path, color_mark_young_good, p, o);\n@@ -315,1 +699,1 @@\n-\/\/ Is alive barrier\n+\/\/ Store barrier\n@@ -317,5 +701,13 @@\n-inline bool ZBarrier::is_alive_barrier_on_weak_oop(oop o) {\n-  \/\/ Check if oop is logically non-null. This operation\n-  \/\/ is only valid when resurrection is blocked.\n-  assert(ZResurrection::is_blocked(), \"Invalid phase\");\n-  return weak_load_barrier_on_weak_oop(o) != NULL;\n+\n+inline void ZBarrier::store_barrier_on_heap_oop_field(volatile zpointer* p, bool heal) {\n+  const zpointer prev = load_atomic(p);\n+\n+  auto slow_path = [=](zaddress addr) -> zaddress {\n+    return ZBarrier::heap_store_slow_path(p, addr, prev, heal);\n+  };\n+\n+  if (heal) {\n+    barrier(is_store_good_fast_path, slow_path, color_store_good, p, prev);\n+  } else {\n+    barrier(is_store_good_or_null_fast_path, slow_path, color_store_good, nullptr, prev);\n+  }\n@@ -324,5 +716,8 @@\n-inline bool ZBarrier::is_alive_barrier_on_phantom_oop(oop o) {\n-  \/\/ Check if oop is logically non-null. This operation\n-  \/\/ is only valid when resurrection is blocked.\n-  assert(ZResurrection::is_blocked(), \"Invalid phase\");\n-  return weak_load_barrier_on_phantom_oop(o) != NULL;\n+inline void ZBarrier::store_barrier_on_native_oop_field(volatile zpointer* p, bool heal) {\n+  const zpointer prev = load_atomic(p);\n+\n+  if (heal) {\n+    barrier(is_store_good_fast_path, native_store_slow_path, color_store_good, p, prev);\n+  } else {\n+    barrier(is_store_good_or_null_fast_path, native_store_slow_path, color_store_good, nullptr, prev);\n+  }\n@@ -331,7 +726,8 @@\n-\/\/\n-\/\/ Keep alive barrier\n-\/\/\n-inline void ZBarrier::keep_alive_barrier_on_weak_oop_field(volatile oop* p) {\n-  assert(ZResurrection::is_blocked(), \"This operation is only valid when resurrection is blocked\");\n-  const oop o = Atomic::load(p);\n-  barrier<is_good_or_null_fast_path, keep_alive_barrier_on_weak_oop_slow_path>(p, o);\n+inline void ZBarrier::no_keep_alive_store_barrier_on_heap_oop_field(volatile zpointer* p) {\n+  const zpointer prev = load_atomic(p);\n+\n+  auto slow_path = [=](zaddress addr) -> zaddress {\n+    return ZBarrier::no_keep_alive_heap_store_slow_path(p, addr);\n+  };\n+\n+  barrier(is_store_good_fast_path, slow_path, color_store_good, nullptr, prev);\n@@ -340,4 +736,4 @@\n-inline void ZBarrier::keep_alive_barrier_on_phantom_oop_field(volatile oop* p) {\n-  assert(ZResurrection::is_blocked(), \"This operation is only valid when resurrection is blocked\");\n-  const oop o = Atomic::load(p);\n-  barrier<is_good_or_null_fast_path, keep_alive_barrier_on_phantom_oop_slow_path>(p, o);\n+inline void ZBarrier::remember(volatile zpointer* p) {\n+  if (ZHeap::heap()->is_old(p)) {\n+    ZGeneration::young()->remember(p);\n+  }\n@@ -346,19 +742,3 @@\n-inline void ZBarrier::keep_alive_barrier_on_phantom_root_oop_field(oop* p) {\n-  \/\/ The keep alive operation is only valid when resurrection is blocked.\n-  \/\/\n-  \/\/ Except with Loom, where we intentionally trigger arms nmethods after\n-  \/\/ unlinking, to get a sense of what nmethods are alive. This will trigger\n-  \/\/ the keep alive barriers, but the oops are healed and the slow-paths\n-  \/\/ will not trigger. We have stronger checks in the slow-paths.\n-  assert(ZResurrection::is_blocked() || (CodeCache::contains((void*)p)),\n-         \"This operation is only valid when resurrection is blocked\");\n-  const oop o = *p;\n-  root_barrier<is_good_or_null_fast_path, keep_alive_barrier_on_phantom_oop_slow_path>(p, o);\n-}\n-\n-inline void ZBarrier::keep_alive_barrier_on_oop(oop o) {\n-  const uintptr_t addr = ZOop::to_address(o);\n-  assert(ZAddress::is_good(addr), \"Invalid address\");\n-\n-  if (during_mark()) {\n-    keep_alive_barrier_on_oop_slow_path(addr);\n+inline void ZBarrier::mark_and_remember(volatile zpointer* p, zaddress addr) {\n+  if (!is_null(addr)) {\n+    mark<ZMark::DontResurrect, ZMark::AnyThread, ZMark::Follow, ZMark::Strong>(addr);\n@@ -366,0 +746,1 @@\n+  remember(p);\n@@ -368,5 +749,3 @@\n-\/\/\n-\/\/ Mark barrier\n-\/\/\n-inline void ZBarrier::mark_barrier_on_oop_field(volatile oop* p, bool finalizable) {\n-  const oop o = Atomic::load(p);\n+template <bool resurrect, bool gc_thread, bool follow, bool finalizable>\n+inline void ZBarrier::mark(zaddress addr) {\n+  assert(!ZVerifyOops || oopDesc::is_oop(to_oop(addr), false), \"must be oop\");\n@@ -374,2 +753,2 @@\n-  if (finalizable) {\n-    barrier<is_marked_or_null_fast_path, mark_barrier_on_finalizable_oop_slow_path>(p, o);\n+  if (ZHeap::heap()->is_old(addr)) {\n+    ZGeneration::old()->mark_object_if_active<resurrect, gc_thread, follow, finalizable>(addr);\n@@ -377,8 +756,1 @@\n-    const uintptr_t addr = ZOop::to_address(o);\n-    if (ZAddress::is_good(addr)) {\n-      \/\/ Mark through good oop\n-      mark_barrier_on_oop_slow_path(addr);\n-    } else {\n-      \/\/ Mark through bad oop\n-      barrier<is_good_or_null_fast_path, mark_barrier_on_oop_slow_path>(p, o);\n-    }\n+    ZGeneration::young()->mark_object_if_active<resurrect, gc_thread, follow, ZMark::Strong>(addr);\n@@ -388,3 +760,13 @@\n-inline void ZBarrier::mark_barrier_on_oop_array(volatile oop* p, size_t length, bool finalizable) {\n-  for (volatile const oop* const end = p + length; p < end; p++) {\n-    mark_barrier_on_oop_field(p, finalizable);\n+template <bool resurrect, bool gc_thread, bool follow>\n+inline void ZBarrier::mark_young(zaddress addr) {\n+  assert(ZGeneration::young()->is_phase_mark(), \"Should only be called during marking\");\n+  assert(!ZVerifyOops || oopDesc::is_oop(to_oop(addr), false), \"must be oop\");\n+  assert(ZHeap::heap()->is_young(addr), \"Must be young\");\n+\n+  ZGeneration::young()->mark_object<resurrect, gc_thread, follow, ZMark::Strong>(addr);\n+}\n+\n+template <bool resurrect, bool gc_thread, bool follow>\n+inline void ZBarrier::mark_if_young(zaddress addr) {\n+  if (ZHeap::heap()->is_young(addr)) {\n+    mark_young<resurrect, gc_thread, follow>(addr);\n","filename":"src\/hotspot\/share\/gc\/z\/zBarrier.inline.hpp","additions":613,"deletions":231,"binary":false,"changes":844,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -29,0 +29,1 @@\n+#include \"gc\/z\/zGeneration.inline.hpp\"\n@@ -33,0 +34,2 @@\n+#include \"runtime\/deoptimization.hpp\"\n+#include \"runtime\/frame.inline.hpp\"\n@@ -34,0 +37,2 @@\n+#include \"runtime\/registerMap.hpp\"\n+#include \"runtime\/stackWatermarkSet.hpp\"\n@@ -83,2 +88,7 @@\n-  \/\/ Set thread local address bad mask\n-  ZThreadLocalData::set_address_bad_mask(thread, ZAddressBadMask);\n+  \/\/ Set thread local masks\n+  ZThreadLocalData::set_load_bad_mask(thread, ZPointerLoadBadMask);\n+  ZThreadLocalData::set_load_good_mask(thread, ZPointerLoadGoodMask);\n+  ZThreadLocalData::set_mark_bad_mask(thread, ZPointerMarkBadMask);\n+  ZThreadLocalData::set_store_bad_mask(thread, ZPointerStoreBadMask);\n+  ZThreadLocalData::set_store_good_mask(thread, ZPointerStoreGoodMask);\n+  ZThreadLocalData::set_nmethod_disarmed(thread, ZPointerStoreGoodMask);\n@@ -89,0 +99,1 @@\n+    ZThreadLocalData::store_barrier_buffer(jt)->initialize();\n@@ -97,0 +108,47 @@\n+static void deoptimize_allocation(JavaThread* thread) {\n+  RegisterMap reg_map(thread, RegisterMap::UpdateMap::skip,\n+                      RegisterMap::ProcessFrames::include,\n+                      RegisterMap::WalkContinuation::skip);\n+  const frame runtime_frame = thread->last_frame();\n+  assert(runtime_frame.is_runtime_frame(), \"must be runtime frame\");\n+\n+  const frame caller_frame = runtime_frame.sender(&reg_map);\n+  assert(caller_frame.is_compiled_frame(), \"must be compiled\");\n+\n+  const nmethod* const nm = caller_frame.cb()->as_nmethod();\n+  if (nm->is_compiled_by_c2() && !caller_frame.is_deoptimized_frame()) {\n+    Deoptimization::deoptimize_frame(thread, caller_frame.id());\n+  }\n+}\n+\n+void ZBarrierSet::on_slowpath_allocation_exit(JavaThread* thread, oop new_obj) {\n+  const ZPage* const page = ZHeap::heap()->page(to_zaddress(new_obj));\n+  const ZPageAge age = page->age();\n+  if (age == ZPageAge::old) {\n+    \/\/ We promised C2 that its allocations would end up in young gen. This object\n+    \/\/ breaks that promise. Take a few steps in the interpreter instead, which has\n+    \/\/ no such assumptions about where an object resides.\n+    deoptimize_allocation(thread);\n+    return;\n+  }\n+\n+  if (!ZGeneration::young()->is_phase_mark_complete()) {\n+    return;\n+  }\n+\n+  if (!page->is_relocatable()) {\n+    return;\n+  }\n+\n+  if (ZRelocate::compute_to_age(age) != ZPageAge::old) {\n+    return;\n+  }\n+\n+  \/\/ If the object is young, we have to still be careful that it isn't racingly\n+  \/\/ about to get promoted to the old generation. That causes issues when null\n+  \/\/ pointers are supposed to be coloured, but the JIT is a bit sloppy and\n+  \/\/ reinitializes memory with raw nulls. We detect this situation and detune\n+  \/\/ rather than relying on the JIT to never be sloppy with redundant initialization.\n+  deoptimize_allocation(thread);\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/z\/zBarrierSet.cpp","additions":61,"deletions":3,"binary":false,"changes":64,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,0 +28,1 @@\n+#include \"gc\/z\/zAddress.hpp\"\n@@ -32,0 +33,3 @@\n+private:\n+  static zpointer store_good(oop obj);\n+\n@@ -43,0 +47,2 @@\n+  virtual void on_slowpath_allocation_exit(JavaThread* thread, oop new_obj);\n+\n@@ -56,1 +62,20 @@\n-    static oop* field_addr(oop base, ptrdiff_t offset);\n+    static zpointer* field_addr(oop base, ptrdiff_t offset);\n+\n+    static zaddress load_barrier(zpointer* p, zpointer o);\n+    static zaddress load_barrier_on_unknown_oop_ref(oop base, ptrdiff_t offset, zpointer* p, zpointer o);\n+\n+    static void store_barrier_heap_with_healing(zpointer* p);\n+    static void store_barrier_heap_without_healing(zpointer* p);\n+    static void no_keep_alive_store_barrier_heap(zpointer* p);\n+\n+    static void store_barrier_native_with_healing(zpointer* p);\n+    static void store_barrier_native_without_healing(zpointer* p);\n+\n+    static void unsupported();\n+    static zaddress load_barrier(narrowOop* p, zpointer o) { unsupported(); return zaddress::null; }\n+    static zaddress load_barrier_on_unknown_oop_ref(oop base, ptrdiff_t offset, narrowOop* p, zpointer o) { unsupported(); return zaddress::null; }\n+    static void store_barrier_heap_with_healing(narrowOop* p) { unsupported(); }\n+    static void store_barrier_heap_without_healing(narrowOop* p)  { unsupported(); }\n+    static void no_keep_alive_store_barrier_heap(narrowOop* p)  { unsupported(); }\n+    static void store_barrier_native_with_healing(narrowOop* p)  { unsupported(); }\n+    static void store_barrier_native_without_healing(narrowOop* p)  { unsupported(); }\n@@ -58,2 +83,3 @@\n-    template <typename T>\n-    static oop load_barrier_on_oop_field_preloaded(T* addr, oop o);\n+    static zaddress oop_copy_one_barriers(zpointer* dst, zpointer* src);\n+    static bool oop_copy_one_check_cast(zpointer* dst, zpointer* src, Klass* dst_klass);\n+    static void oop_copy_one(zpointer* dst, zpointer* src);\n@@ -61,2 +87,2 @@\n-    template <typename T>\n-    static oop load_barrier_on_unknown_oop_field_preloaded(oop base, ptrdiff_t offset, T* addr, oop o);\n+    static bool oop_arraycopy_in_heap_check_cast(zpointer* dst, zpointer* src, size_t length, Klass* dst_klass);\n+    static bool oop_arraycopy_in_heap_no_check_cast(zpointer* dst, zpointer* src, size_t length);\n@@ -68,2 +94,4 @@\n-    template <typename T>\n-    static oop oop_load_in_heap(T* addr);\n+    static oop oop_load_in_heap(zpointer* p);\n+    static oop oop_load_in_heap(oop* p)       { return oop_load_in_heap((zpointer*)p); };\n+    static oop oop_load_in_heap(narrowOop* p) { unsupported(); return nullptr; }\n+\n@@ -72,2 +100,13 @@\n-    template <typename T>\n-    static oop oop_atomic_cmpxchg_in_heap(T* addr, oop compare_value, oop new_value);\n+    static void oop_store_in_heap(zpointer* p, oop value);\n+    static void oop_store_in_heap(oop* p, oop value)       { oop_store_in_heap((zpointer*)p, value); }\n+    static void oop_store_in_heap(narrowOop* p, oop value) { unsupported(); }\n+    static void oop_store_in_heap_at(oop base, ptrdiff_t offset, oop value);\n+\n+    static void oop_store_not_in_heap(zpointer* p, oop value);\n+    static void oop_store_not_in_heap(oop* p, oop value)       { oop_store_not_in_heap((zpointer*)p, value); }\n+    static void oop_store_not_in_heap(narrowOop* p, oop value) { unsupported(); }\n+    static void oop_store_not_in_heap_at(oop base, ptrdiff_t offset, oop value);\n+\n+    static oop oop_atomic_cmpxchg_in_heap(zpointer* p, oop compare_value, oop new_value);\n+    static oop oop_atomic_cmpxchg_in_heap(oop* p, oop compare_value, oop new_value)       { return oop_atomic_cmpxchg_in_heap((zpointer*)p, compare_value, new_value); }\n+    static oop oop_atomic_cmpxchg_in_heap(narrowOop* p, oop compare_value, oop new_value) { unsupported(); return nullptr; }\n@@ -76,2 +115,3 @@\n-    template <typename T>\n-    static oop oop_atomic_xchg_in_heap(T* addr, oop new_value);\n+    static oop oop_atomic_xchg_in_heap(zpointer* p, oop new_value);\n+    static oop oop_atomic_xchg_in_heap(oop* p, oop new_value)       { return oop_atomic_xchg_in_heap((zpointer*)p, new_value); }\n+    static oop oop_atomic_xchg_in_heap(narrowOop* p, oop new_value) { unsupported(); return nullptr; }\n@@ -80,3 +120,2 @@\n-    template <typename T>\n-    static bool oop_arraycopy_in_heap(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,\n-                                      arrayOop dst_obj, size_t dst_offset_in_bytes, T* dst_raw,\n+    static bool oop_arraycopy_in_heap(arrayOop src_obj, size_t src_offset_in_bytes, zpointer* src_raw,\n+                                      arrayOop dst_obj, size_t dst_offset_in_bytes, zpointer* dst_raw,\n@@ -84,0 +123,10 @@\n+    static bool oop_arraycopy_in_heap(arrayOop src_obj, size_t src_offset_in_bytes, oop* src_raw,\n+                                      arrayOop dst_obj, size_t dst_offset_in_bytes, oop* dst_raw,\n+                                      size_t length) {\n+      return oop_arraycopy_in_heap(src_obj, src_offset_in_bytes, (zpointer*)src_raw,\n+                                   dst_obj, dst_offset_in_bytes, (zpointer*)dst_raw,\n+                                   length);\n+    }\n+    static bool oop_arraycopy_in_heap(arrayOop src_obj, size_t src_offset_in_bytes, narrowOop* src_raw,\n+                                      arrayOop dst_obj, size_t dst_offset_in_bytes, narrowOop* dst_raw,\n+                                      size_t length) { unsupported(); return false; }\n@@ -90,8 +139,13 @@\n-    template <typename T>\n-    static oop oop_load_not_in_heap(T* addr);\n-\n-    template <typename T>\n-    static oop oop_atomic_cmpxchg_not_in_heap(T* addr, oop compare_value, oop new_value);\n-\n-    template <typename T>\n-    static oop oop_atomic_xchg_not_in_heap(T* addr, oop new_value);\n+    static oop oop_load_not_in_heap(zpointer* p);\n+    static oop oop_load_not_in_heap(oop* p);\n+    static oop oop_load_not_in_heap(narrowOop* p) { unsupported(); return nullptr; }\n+\n+    static oop oop_atomic_cmpxchg_not_in_heap(zpointer* p, oop compare_value, oop new_value);\n+    static oop oop_atomic_cmpxchg_not_in_heap(oop* p, oop compare_value, oop new_value) {\n+      return oop_atomic_cmpxchg_not_in_heap((zpointer*)p, compare_value, new_value);\n+    }\n+    static oop oop_atomic_cmpxchg_not_in_heap(narrowOop* addr, oop compare_value, oop new_value) { unsupported(); return nullptr; }\n+\n+    static oop oop_atomic_xchg_not_in_heap(zpointer* p, oop new_value);\n+    static oop oop_atomic_xchg_not_in_heap(oop* p, oop new_value)       { return oop_atomic_xchg_not_in_heap((zpointer*)p, new_value); }\n+    static oop oop_atomic_xchg_not_in_heap(narrowOop* p, oop new_value) { unsupported(); return nullptr; }\n","filename":"src\/hotspot\/share\/gc\/z\/zBarrierSet.hpp","additions":77,"deletions":23,"binary":false,"changes":100,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -30,0 +30,1 @@\n+#include \"gc\/z\/zAddress.inline.hpp\"\n@@ -31,0 +32,3 @@\n+#include \"gc\/z\/zIterator.inline.hpp\"\n+#include \"gc\/z\/zNMethod.hpp\"\n+#include \"memory\/iterator.inline.hpp\"\n@@ -50,3 +54,2 @@\n-inline oop* ZBarrierSet::AccessBarrier<decorators, BarrierSetT>::field_addr(oop base, ptrdiff_t offset) {\n-  assert(base != NULL, \"Invalid base\");\n-  return reinterpret_cast<oop*>(reinterpret_cast<intptr_t>((void*)base) + offset);\n+inline void ZBarrierSet::AccessBarrier<decorators, BarrierSetT>::unsupported() {\n+  ShouldNotReachHere();\n@@ -56,2 +59,7 @@\n-template <typename T>\n-inline oop ZBarrierSet::AccessBarrier<decorators, BarrierSetT>::load_barrier_on_oop_field_preloaded(T* addr, oop o) {\n+inline zpointer* ZBarrierSet::AccessBarrier<decorators, BarrierSetT>::field_addr(oop base, ptrdiff_t offset) {\n+  assert(base != nullptr, \"Invalid base\");\n+  return reinterpret_cast<zpointer*>(reinterpret_cast<intptr_t>((void*)base) + offset);\n+}\n+\n+template <DecoratorSet decorators, typename BarrierSetT>\n+inline zaddress ZBarrierSet::AccessBarrier<decorators, BarrierSetT>::load_barrier(zpointer* p, zpointer o) {\n@@ -62,1 +70,2 @@\n-      return ZBarrier::weak_load_barrier_on_oop_field_preloaded(addr, o);\n+      \/\/ Load barriers on strong oop refs don't keep objects alive\n+      return ZBarrier::load_barrier_on_oop_field_preloaded(p, o);\n@@ -64,1 +73,1 @@\n-      return ZBarrier::weak_load_barrier_on_weak_oop_field_preloaded(addr, o);\n+      return ZBarrier::no_keep_alive_load_barrier_on_weak_oop_field_preloaded(p, o);\n@@ -67,1 +76,1 @@\n-      return ZBarrier::weak_load_barrier_on_phantom_oop_field_preloaded(addr, o);\n+      return ZBarrier::no_keep_alive_load_barrier_on_phantom_oop_field_preloaded(p, o);\n@@ -71,1 +80,1 @@\n-      return ZBarrier::load_barrier_on_oop_field_preloaded(addr, o);\n+      return ZBarrier::load_barrier_on_oop_field_preloaded(p, o);\n@@ -73,1 +82,1 @@\n-      return ZBarrier::load_barrier_on_weak_oop_field_preloaded(addr, o);\n+      return ZBarrier::load_barrier_on_weak_oop_field_preloaded(p, o);\n@@ -76,1 +85,1 @@\n-      return ZBarrier::load_barrier_on_phantom_oop_field_preloaded(addr, o);\n+      return ZBarrier::load_barrier_on_phantom_oop_field_preloaded(p, o);\n@@ -82,2 +91,1 @@\n-template <typename T>\n-inline oop ZBarrierSet::AccessBarrier<decorators, BarrierSetT>::load_barrier_on_unknown_oop_field_preloaded(oop base, ptrdiff_t offset, T* addr, oop o) {\n+inline zaddress ZBarrierSet::AccessBarrier<decorators, BarrierSetT>::load_barrier_on_unknown_oop_ref(oop base, ptrdiff_t offset, zpointer* p, zpointer o) {\n@@ -91,1 +99,2 @@\n-      return ZBarrier::weak_load_barrier_on_oop_field_preloaded(addr, o);\n+      \/\/ Load barriers on strong oop refs don't keep objects alive\n+      return ZBarrier::load_barrier_on_oop_field_preloaded(p, o);\n@@ -93,1 +102,1 @@\n-      return ZBarrier::weak_load_barrier_on_weak_oop_field_preloaded(addr, o);\n+      return ZBarrier::no_keep_alive_load_barrier_on_weak_oop_field_preloaded(p, o);\n@@ -96,1 +105,1 @@\n-      return ZBarrier::weak_load_barrier_on_phantom_oop_field_preloaded(addr, o);\n+      return ZBarrier::no_keep_alive_load_barrier_on_phantom_oop_field_preloaded(p, o);\n@@ -100,1 +109,1 @@\n-      return ZBarrier::load_barrier_on_oop_field_preloaded(addr, o);\n+      return ZBarrier::load_barrier_on_oop_field_preloaded(p, o);\n@@ -102,1 +111,1 @@\n-      return ZBarrier::load_barrier_on_weak_oop_field_preloaded(addr, o);\n+      return ZBarrier::load_barrier_on_weak_oop_field_preloaded(p, o);\n@@ -105,1 +114,1 @@\n-      return ZBarrier::load_barrier_on_phantom_oop_field_preloaded(addr, o);\n+      return ZBarrier::load_barrier_on_phantom_oop_field_preloaded(p, o);\n@@ -110,0 +119,46 @@\n+inline zpointer ZBarrierSet::store_good(oop obj) {\n+  assert(ZPointerStoreGoodMask != 0, \"sanity\");\n+\n+  const zaddress addr = to_zaddress(obj);\n+  return ZAddress::store_good(addr);\n+}\n+\n+template <DecoratorSet decorators, typename BarrierSetT>\n+inline void ZBarrierSet::AccessBarrier<decorators, BarrierSetT>::store_barrier_heap_with_healing(zpointer* p) {\n+  if (!HasDecorator<decorators, IS_DEST_UNINITIALIZED>::value) {\n+    ZBarrier::store_barrier_on_heap_oop_field(p, true \/* heal *\/);\n+  } else {\n+    assert(false, \"Should not be used on uninitialized memory\");\n+  }\n+}\n+\n+template <DecoratorSet decorators, typename BarrierSetT>\n+inline void ZBarrierSet::AccessBarrier<decorators, BarrierSetT>::store_barrier_heap_without_healing(zpointer* p) {\n+  if (!HasDecorator<decorators, IS_DEST_UNINITIALIZED>::value) {\n+    ZBarrier::store_barrier_on_heap_oop_field(p, false \/* heal *\/);\n+  }\n+}\n+\n+template <DecoratorSet decorators, typename BarrierSetT>\n+inline void ZBarrierSet::AccessBarrier<decorators, BarrierSetT>::no_keep_alive_store_barrier_heap(zpointer* p) {\n+  if (!HasDecorator<decorators, IS_DEST_UNINITIALIZED>::value) {\n+    ZBarrier::no_keep_alive_store_barrier_on_heap_oop_field(p);\n+  }\n+}\n+\n+template <DecoratorSet decorators, typename BarrierSetT>\n+inline void ZBarrierSet::AccessBarrier<decorators, BarrierSetT>::store_barrier_native_with_healing(zpointer* p) {\n+  if (!HasDecorator<decorators, IS_DEST_UNINITIALIZED>::value) {\n+    ZBarrier::store_barrier_on_native_oop_field(p, true \/* heal *\/);\n+  } else {\n+    assert(false, \"Should not be used on uninitialized memory\");\n+  }\n+}\n+\n+template <DecoratorSet decorators, typename BarrierSetT>\n+inline void ZBarrierSet::AccessBarrier<decorators, BarrierSetT>::store_barrier_native_without_healing(zpointer* p) {\n+  if (!HasDecorator<decorators, IS_DEST_UNINITIALIZED>::value) {\n+    ZBarrier::store_barrier_on_native_oop_field(p, false \/* heal *\/);\n+  }\n+}\n+\n@@ -114,2 +169,1 @@\n-template <typename T>\n-inline oop ZBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_load_in_heap(T* addr) {\n+inline oop ZBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_load_in_heap(zpointer* p) {\n@@ -118,2 +172,4 @@\n-  const oop o = Raw::oop_load_in_heap(addr);\n-  return load_barrier_on_oop_field_preloaded(addr, o);\n+  const zpointer o = Raw::load_in_heap(p);\n+  assert_is_valid(o);\n+\n+  return to_oop(load_barrier(p, o));\n@@ -124,2 +180,4 @@\n-  oop* const addr = field_addr(base, offset);\n-  const oop o = Raw::oop_load_in_heap(addr);\n+  zpointer* const p = field_addr(base, offset);\n+\n+  const zpointer o = Raw::load_in_heap(p);\n+  assert_is_valid(o);\n@@ -128,1 +186,10 @@\n-    return load_barrier_on_unknown_oop_field_preloaded(base, offset, addr, o);\n+    return to_oop(load_barrier_on_unknown_oop_ref(base, offset, p, o));\n+  }\n+\n+  return to_oop(load_barrier(p, o));\n+}\n+\n+template <DecoratorSet decorators>\n+bool is_store_barrier_no_keep_alive() {\n+  if (HasDecorator<decorators, ON_STRONG_OOP_REF>::value) {\n+    return HasDecorator<decorators, AS_NO_KEEPALIVE>::value;\n@@ -131,1 +198,40 @@\n-  return load_barrier_on_oop_field_preloaded(addr, o);\n+  if (HasDecorator<decorators, ON_WEAK_OOP_REF>::value) {\n+    return true;\n+  }\n+\n+  assert((decorators & ON_PHANTOM_OOP_REF) != 0, \"Must be\");\n+  return true;\n+}\n+\n+template <DecoratorSet decorators>\n+inline bool is_store_barrier_no_keep_alive(oop base, ptrdiff_t offset) {\n+  if (!HasDecorator<decorators, ON_UNKNOWN_OOP_REF>::value) {\n+    return is_store_barrier_no_keep_alive<decorators>();\n+  }\n+\n+  const DecoratorSet decorators_known_strength =\n+      AccessBarrierSupport::resolve_possibly_unknown_oop_ref_strength<decorators>(base, offset);\n+\n+  if ((decorators_known_strength & ON_STRONG_OOP_REF) != 0) {\n+    return (decorators & AS_NO_KEEPALIVE) != 0;\n+  }\n+\n+  if ((decorators_known_strength & ON_WEAK_OOP_REF) != 0) {\n+    return true;\n+  }\n+\n+  assert((decorators_known_strength & ON_PHANTOM_OOP_REF) != 0, \"Must be\");\n+  return true;\n+}\n+\n+template <DecoratorSet decorators, typename BarrierSetT>\n+inline void ZBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_store_in_heap(zpointer* p, oop value) {\n+  verify_decorators_absent<ON_UNKNOWN_OOP_REF>();\n+\n+  if (is_store_barrier_no_keep_alive<decorators>()) {\n+    no_keep_alive_store_barrier_heap(p);\n+  } else {\n+    store_barrier_heap_without_healing(p);\n+  }\n+\n+  Raw::store_in_heap(p, store_good(value));\n@@ -135,2 +241,25 @@\n-template <typename T>\n-inline oop ZBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_atomic_cmpxchg_in_heap(T* addr, oop compare_value, oop new_value) {\n+inline void ZBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_store_in_heap_at(oop base, ptrdiff_t offset, oop value) {\n+  zpointer* const p = field_addr(base, offset);\n+\n+  if (is_store_barrier_no_keep_alive<decorators>(base, offset)) {\n+    no_keep_alive_store_barrier_heap(p);\n+  } else {\n+    store_barrier_heap_without_healing(p);\n+  }\n+\n+  Raw::store_in_heap(p, store_good(value));\n+}\n+\n+template <DecoratorSet decorators, typename BarrierSetT>\n+inline void ZBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_store_not_in_heap(zpointer* p, oop value) {\n+  verify_decorators_absent<ON_UNKNOWN_OOP_REF>();\n+\n+  if (!is_store_barrier_no_keep_alive<decorators>()) {\n+    store_barrier_native_without_healing(p);\n+  }\n+\n+  Raw::store(p, store_good(value));\n+}\n+\n+template <DecoratorSet decorators, typename BarrierSetT>\n+inline oop ZBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_atomic_cmpxchg_in_heap(zpointer* p, oop compare_value, oop new_value) {\n@@ -140,2 +269,6 @@\n-  ZBarrier::load_barrier_on_oop_field(addr);\n-  return Raw::oop_atomic_cmpxchg_in_heap(addr, compare_value, new_value);\n+  store_barrier_heap_with_healing(p);\n+\n+  const zpointer o = Raw::atomic_cmpxchg_in_heap(p, store_good(compare_value), store_good(new_value));\n+  assert_is_valid(o);\n+\n+  return to_oop(ZPointer::uncolor_store_good(o));\n@@ -153,2 +286,8 @@\n-  ZBarrier::load_barrier_on_oop_field(field_addr(base, offset));\n-  return Raw::oop_atomic_cmpxchg_in_heap_at(base, offset, compare_value, new_value);\n+  zpointer* const p = field_addr(base, offset);\n+\n+  store_barrier_heap_with_healing(p);\n+\n+  const zpointer o = Raw::atomic_cmpxchg_in_heap(p, store_good(compare_value), store_good(new_value));\n+  assert_is_valid(o);\n+\n+  return to_oop(ZPointer::uncolor_store_good(o));\n@@ -158,2 +297,1 @@\n-template <typename T>\n-inline oop ZBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_atomic_xchg_in_heap(T* addr, oop new_value) {\n+inline oop ZBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_atomic_xchg_in_heap(zpointer* p, oop new_value) {\n@@ -163,2 +301,6 @@\n-  const oop o = Raw::oop_atomic_xchg_in_heap(addr, new_value);\n-  return ZBarrier::load_barrier_on_oop(o);\n+  store_barrier_heap_with_healing(p);\n+\n+  const zpointer o = Raw::atomic_xchg_in_heap(p, store_good(new_value));\n+  assert_is_valid(o);\n+\n+  return to_oop(ZPointer::uncolor_store_good(o));\n@@ -172,2 +314,8 @@\n-  const oop o = Raw::oop_atomic_xchg_in_heap_at(base, offset, new_value);\n-  return ZBarrier::load_barrier_on_oop(o);\n+  zpointer* const p = field_addr(base, offset);\n+\n+  store_barrier_heap_with_healing(p);\n+\n+  const zpointer o = Raw::atomic_xchg_in_heap(p, store_good(new_value));\n+  assert_is_valid(o);\n+\n+  return to_oop(ZPointer::uncolor_store_good(o));\n@@ -177,6 +325,9 @@\n-template <typename T>\n-inline bool ZBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_arraycopy_in_heap(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,\n-                                                                                       arrayOop dst_obj, size_t dst_offset_in_bytes, T* dst_raw,\n-                                                                                       size_t length) {\n-  T* src = arrayOopDesc::obj_offset_to_raw(src_obj, src_offset_in_bytes, src_raw);\n-  T* dst = arrayOopDesc::obj_offset_to_raw(dst_obj, dst_offset_in_bytes, dst_raw);\n+inline zaddress ZBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_copy_one_barriers(zpointer* dst, zpointer* src) {\n+  store_barrier_heap_without_healing(dst);\n+\n+  return ZBarrier::load_barrier_on_oop_field(src);\n+}\n+\n+template <DecoratorSet decorators, typename BarrierSetT>\n+inline void ZBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_copy_one(zpointer* dst, zpointer* src) {\n+  const zaddress obj = oop_copy_one_barriers(dst, src);\n@@ -184,4 +335,10 @@\n-  if (!HasDecorator<decorators, ARRAYCOPY_CHECKCAST>::value) {\n-    \/\/ No check cast, bulk barrier and bulk copy\n-    ZBarrier::load_barrier_on_oop_array(src, length);\n-    return Raw::oop_arraycopy_in_heap(NULL, 0, src, NULL, 0, dst, length);\n+  Atomic::store(dst, ZAddress::store_good(obj));\n+}\n+\n+template <DecoratorSet decorators, typename BarrierSetT>\n+inline bool ZBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_copy_one_check_cast(zpointer* dst, zpointer* src, Klass* dst_klass) {\n+  const zaddress obj = oop_copy_one_barriers(dst, src);\n+\n+  if (!oopDesc::is_instanceof_or_null(to_oop(obj), dst_klass)) {\n+    \/\/ Check cast failed\n+    return false;\n@@ -190,0 +347,8 @@\n+  Atomic::store(dst, ZAddress::store_good(obj));\n+\n+  return true;\n+}\n+\n+\n+template <DecoratorSet decorators, typename BarrierSetT>\n+inline bool ZBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_arraycopy_in_heap_check_cast(zpointer* dst, zpointer* src, size_t length, Klass* dst_klass) {\n@@ -191,4 +356,2 @@\n-  Klass* const dst_klass = objArrayOop(dst_obj)->element_klass();\n-  for (const T* const end = src + length; src < end; src++, dst++) {\n-    const oop elem = ZBarrier::load_barrier_on_oop_field(src);\n-    if (!oopDesc::is_instanceof_or_null(elem, dst_klass)) {\n+  for (const zpointer* const end = src + length; src < end; src++, dst++) {\n+    if (!oop_copy_one_check_cast(dst, src, dst_klass)) {\n@@ -198,0 +361,8 @@\n+  }\n+\n+  return true;\n+}\n+\n+template <DecoratorSet decorators, typename BarrierSetT>\n+inline bool ZBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_arraycopy_in_heap_no_check_cast(zpointer* dst, zpointer* src, size_t length) {\n+  const bool is_disjoint = HasDecorator<decorators, ARRAYCOPY_DISJOINT>::value;\n@@ -199,2 +370,5 @@\n-    \/\/ Cast is safe, since we know it's never a narrowOop\n-    *(oop*)dst = elem;\n+  if (is_disjoint || src > dst) {\n+    for (const zpointer* const end = src + length; src < end; src++, dst++) {\n+      oop_copy_one(dst, src);\n+    }\n+    return true;\n@@ -203,0 +377,11 @@\n+  if (src < dst) {\n+    const zpointer* const end = src;\n+    src += length - 1;\n+    dst += length - 1;\n+    for ( ; src >= end; src--, dst--) {\n+      oop_copy_one(dst, src);\n+    }\n+    return true;\n+  }\n+\n+  \/\/ src and dst are the same; nothing to do\n@@ -206,0 +391,41 @@\n+template <DecoratorSet decorators, typename BarrierSetT>\n+inline bool ZBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_arraycopy_in_heap(arrayOop src_obj, size_t src_offset_in_bytes, zpointer* src_raw,\n+                                                                                       arrayOop dst_obj, size_t dst_offset_in_bytes, zpointer* dst_raw,\n+                                                                                       size_t length) {\n+  zpointer* const src = arrayOopDesc::obj_offset_to_raw(src_obj, src_offset_in_bytes, src_raw);\n+  zpointer* const dst = arrayOopDesc::obj_offset_to_raw(dst_obj, dst_offset_in_bytes, dst_raw);\n+\n+  if (HasDecorator<decorators, ARRAYCOPY_CHECKCAST>::value) {\n+    Klass* const dst_klass = objArrayOop(dst_obj)->element_klass();\n+    return oop_arraycopy_in_heap_check_cast(dst, src, length, dst_klass);\n+  }\n+\n+  return oop_arraycopy_in_heap_no_check_cast(dst, src, length);\n+}\n+\n+class ZStoreBarrierOopClosure : public BasicOopIterateClosure {\n+public:\n+  virtual void do_oop(oop* p_) {\n+    volatile zpointer* const p = (volatile zpointer*)p_;\n+    const zpointer ptr = ZBarrier::load_atomic(p);\n+    const zaddress addr = ZPointer::uncolor(ptr);\n+    ZBarrier::store_barrier_on_heap_oop_field(p, false \/* heal *\/);\n+    *p = ZAddress::store_good(addr);\n+  }\n+\n+  virtual void do_oop(narrowOop* p) {\n+    ShouldNotReachHere();\n+  }\n+};\n+\n+class ZLoadBarrierOopClosure : public BasicOopIterateClosure {\n+public:\n+  virtual void do_oop(oop* p) {\n+    ZBarrier::load_barrier_on_oop_field((zpointer*)p);\n+  }\n+\n+  virtual void do_oop(narrowOop* p) {\n+    ShouldNotReachHere();\n+  }\n+};\n+\n@@ -208,1 +434,7 @@\n-  ZBarrier::load_barrier_on_oop_fields(src);\n+  assert_is_valid(to_zaddress(src));\n+\n+  \/\/ Fix the oops\n+  ZLoadBarrierOopClosure cl;\n+  ZIterator::oop_iterate(src, &cl);\n+\n+  \/\/ Clone the object\n@@ -210,0 +442,6 @@\n+\n+  assert(ZHeap::heap()->is_young(to_zaddress(dst)), \"ZColorStoreGoodOopClosure is only valid for young objects\");\n+\n+  \/\/ Color store good before handing out\n+  ZStoreBarrierOopClosure cl_sg;\n+  ZIterator::oop_iterate(dst, &cl_sg);\n@@ -216,2 +454,10 @@\n-template <typename T>\n-inline oop ZBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_load_not_in_heap(T* addr) {\n+inline oop ZBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_load_not_in_heap(zpointer* p) {\n+  verify_decorators_absent<ON_UNKNOWN_OOP_REF>();\n+\n+  const zpointer o = Raw::template load<zpointer>(p);\n+  assert_is_valid(o);\n+  return to_oop(load_barrier(p, o));\n+}\n+\n+template <DecoratorSet decorators, typename BarrierSetT>\n+inline oop ZBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_load_not_in_heap(oop* p) {\n@@ -220,2 +466,5 @@\n-  const oop o = Raw::oop_load_not_in_heap(addr);\n-  return load_barrier_on_oop_field_preloaded(addr, o);\n+  if (HasDecorator<decorators, IN_NMETHOD>::value) {\n+    return ZNMethod::load_oop(p, decorators);\n+  } else {\n+    return oop_load_not_in_heap((zpointer*)p);\n+  }\n@@ -225,2 +474,1 @@\n-template <typename T>\n-inline oop ZBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_atomic_cmpxchg_not_in_heap(T* addr, oop compare_value, oop new_value) {\n+inline oop ZBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_atomic_cmpxchg_not_in_heap(zpointer* p, oop compare_value, oop new_value) {\n@@ -230,1 +478,6 @@\n-  return Raw::oop_atomic_cmpxchg_not_in_heap(addr, compare_value, new_value);\n+  store_barrier_native_with_healing(p);\n+\n+  const zpointer o = Raw::atomic_cmpxchg(p, store_good(compare_value), store_good(new_value));\n+  assert_is_valid(o);\n+\n+  return to_oop(ZPointer::uncolor_store_good(o));\n@@ -234,2 +487,1 @@\n-template <typename T>\n-inline oop ZBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_atomic_xchg_not_in_heap(T* addr, oop new_value) {\n+inline oop ZBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_atomic_xchg_not_in_heap(zpointer* p, oop new_value) {\n@@ -239,1 +491,6 @@\n-  return Raw::oop_atomic_xchg_not_in_heap(addr, new_value);\n+  store_barrier_native_with_healing(p);\n+\n+  const zpointer o = Raw::atomic_xchg(p, store_good(new_value));\n+  assert_is_valid(o);\n+\n+  return to_oop(ZPointer::uncolor_store_good(o));\n","filename":"src\/hotspot\/share\/gc\/z\/zBarrierSet.inline.hpp","additions":324,"deletions":67,"binary":false,"changes":391,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -29,2 +29,2 @@\n-Address ZBarrierSetAssemblerBase::address_bad_mask_from_thread(Register thread) {\n-  return Address(thread, ZThreadLocalData::address_bad_mask_offset());\n+Address ZBarrierSetAssemblerBase::load_bad_mask_from_thread(Register thread) {\n+  return Address(thread, ZThreadLocalData::load_bad_mask_offset());\n@@ -33,2 +33,10 @@\n-Address ZBarrierSetAssemblerBase::address_bad_mask_from_jni_env(Register env) {\n-  return Address(env, ZThreadLocalData::address_bad_mask_offset() - JavaThread::jni_environment_offset());\n+Address ZBarrierSetAssemblerBase::mark_bad_mask_from_thread(Register thread) {\n+  return Address(thread, ZThreadLocalData::mark_bad_mask_offset());\n+}\n+\n+Address ZBarrierSetAssemblerBase::load_bad_mask_from_jni_env(Register env) {\n+  return Address(env, ZThreadLocalData::load_bad_mask_offset() - JavaThread::jni_environment_offset());\n+}\n+\n+Address ZBarrierSetAssemblerBase::mark_bad_mask_from_jni_env(Register env) {\n+  return Address(env, ZThreadLocalData::mark_bad_mask_offset() - JavaThread::jni_environment_offset());\n","filename":"src\/hotspot\/share\/gc\/z\/zBarrierSetAssembler.cpp","additions":13,"deletions":5,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -32,2 +32,5 @@\n-  static Address address_bad_mask_from_thread(Register thread);\n-  static Address address_bad_mask_from_jni_env(Register env);\n+  static Address load_bad_mask_from_thread(Register thread);\n+  static Address mark_bad_mask_from_thread(Register thread);\n+\n+  static Address load_bad_mask_from_jni_env(Register env);\n+  static Address mark_bad_mask_from_jni_env(Register env);\n","filename":"src\/hotspot\/share\/gc\/z\/zBarrierSetAssembler.hpp","additions":6,"deletions":3,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -26,0 +26,4 @@\n+#include \"gc\/shared\/barrierSet.hpp\"\n+#include \"gc\/z\/zAddress.hpp\"\n+#include \"gc\/z\/zBarrier.inline.hpp\"\n+#include \"gc\/z\/zBarrierSetAssembler.hpp\"\n@@ -27,1 +31,0 @@\n-#include \"gc\/z\/zGlobals.hpp\"\n@@ -30,0 +33,1 @@\n+#include \"gc\/z\/zResurrection.inline.hpp\"\n@@ -31,0 +35,1 @@\n+#include \"gc\/z\/zUncoloredRoot.inline.hpp\"\n@@ -38,0 +43,2 @@\n+  log_develop_trace(gc, nmethod)(\"nmethod: \" PTR_FORMAT \" visited by entry (try)\", p2i(nm));\n+\n@@ -39,0 +46,1 @@\n+    log_develop_trace(gc, nmethod)(\"nmethod: \" PTR_FORMAT \" visited by entry (disarmed)\", p2i(nm));\n@@ -47,0 +55,1 @@\n+    log_develop_trace(gc, nmethod)(\"nmethod: \" PTR_FORMAT \" visited by entry (unloading)\", p2i(nm));\n@@ -54,1 +63,1 @@\n-    \/\/ will re-resovle the call and update the compiled IC.\n+    \/\/ will re-resolve the call and update the compiled IC.\n@@ -58,0 +67,3 @@\n+  \/\/ Heal barriers\n+  ZNMethod::nmethod_patch_barriers(nm);\n+\n@@ -59,1 +71,2 @@\n-  ZNMethod::nmethod_oops_barrier(nm);\n+  ZUncoloredRootProcessWeakOopClosure cl(ZNMethod::color(nm));\n+  ZNMethod::nmethod_oops_do_inner(nm, &cl);\n@@ -61,0 +74,3 @@\n+  const uintptr_t prev_color = ZNMethod::color(nm);\n+  const uintptr_t new_color = *(int*)ZPointerStoreGoodMaskLowOrderBitsAddr;\n+  log_develop_trace(gc, nmethod)(\"nmethod: \" PTR_FORMAT \" visited by entry (complete) [\" PTR_FORMAT \" -> \" PTR_FORMAT \"]\", p2i(nm), prev_color, new_color);\n@@ -72,1 +88,1 @@\n-  return (int*)ZAddressBadMaskHighOrderBitsAddr;\n+  return (int*)ZPointerStoreGoodMaskLowOrderBitsAddr;\n","filename":"src\/hotspot\/share\/gc\/z\/zBarrierSetNMethod.cpp","additions":21,"deletions":5,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -31,1 +31,1 @@\n-  return ZBarrier::load_barrier_on_oop_field_preloaded(p, o);\n+  return to_oop(ZBarrier::load_barrier_on_oop_field_preloaded((zpointer*)p, to_zpointer(o)));\n@@ -34,2 +34,2 @@\n-JRT_LEAF(oopDesc*, ZBarrierSetRuntime::weak_load_barrier_on_oop_field_preloaded(oopDesc* o, oop* p))\n-  return ZBarrier::weak_load_barrier_on_oop_field_preloaded(p, o);\n+JRT_LEAF(zpointer, ZBarrierSetRuntime::load_barrier_on_oop_field_preloaded_store_good(oopDesc* o, oop* p))\n+  return ZAddress::color(ZBarrier::load_barrier_on_oop_field_preloaded((zpointer*)p, to_zpointer(o)), ZPointerStoreGoodMask);\n@@ -38,2 +38,2 @@\n-JRT_LEAF(oopDesc*, ZBarrierSetRuntime::weak_load_barrier_on_weak_oop_field_preloaded(oopDesc* o, oop* p))\n-  return ZBarrier::weak_load_barrier_on_weak_oop_field_preloaded(p, o);\n+JRT_LEAF(oopDesc*, ZBarrierSetRuntime::load_barrier_on_weak_oop_field_preloaded(oopDesc* o, oop* p))\n+  return to_oop(ZBarrier::load_barrier_on_weak_oop_field_preloaded((zpointer*)p, to_zpointer(o)));\n@@ -42,2 +42,2 @@\n-JRT_LEAF(oopDesc*, ZBarrierSetRuntime::weak_load_barrier_on_phantom_oop_field_preloaded(oopDesc* o, oop* p))\n-  return ZBarrier::weak_load_barrier_on_phantom_oop_field_preloaded(p, o);\n+JRT_LEAF(oopDesc*, ZBarrierSetRuntime::load_barrier_on_phantom_oop_field_preloaded(oopDesc* o, oop* p))\n+  return to_oop(ZBarrier::load_barrier_on_phantom_oop_field_preloaded((zpointer*)p, to_zpointer(o)));\n@@ -46,2 +46,2 @@\n-JRT_LEAF(oopDesc*, ZBarrierSetRuntime::load_barrier_on_weak_oop_field_preloaded(oopDesc* o, oop* p))\n-  return ZBarrier::load_barrier_on_weak_oop_field_preloaded(p, o);\n+JRT_LEAF(oopDesc*, ZBarrierSetRuntime::no_keepalive_load_barrier_on_weak_oop_field_preloaded(oopDesc* o, oop* p))\n+  return to_oop(ZBarrier::no_keep_alive_load_barrier_on_weak_oop_field_preloaded((zpointer*)p, to_zpointer(o)));\n@@ -50,2 +50,10 @@\n-JRT_LEAF(oopDesc*, ZBarrierSetRuntime::load_barrier_on_phantom_oop_field_preloaded(oopDesc* o, oop* p))\n-  return ZBarrier::load_barrier_on_phantom_oop_field_preloaded(p, o);\n+JRT_LEAF(oopDesc*, ZBarrierSetRuntime::no_keepalive_load_barrier_on_phantom_oop_field_preloaded(oopDesc* o, oop* p))\n+  return to_oop(ZBarrier::no_keep_alive_load_barrier_on_phantom_oop_field_preloaded((zpointer*)p, to_zpointer(o)));\n+JRT_END\n+\n+JRT_LEAF(void, ZBarrierSetRuntime::store_barrier_on_oop_field_with_healing(oop* p))\n+  ZBarrier::store_barrier_on_heap_oop_field((zpointer*)p, true \/* heal *\/);\n+JRT_END\n+\n+JRT_LEAF(void, ZBarrierSetRuntime::store_barrier_on_oop_field_without_healing(oop* p))\n+  ZBarrier::store_barrier_on_heap_oop_field((zpointer*)p, false \/* heal *\/);\n@@ -54,2 +62,2 @@\n-JRT_LEAF(void, ZBarrierSetRuntime::load_barrier_on_oop_array(oop* p, size_t length))\n-  ZBarrier::load_barrier_on_oop_array(p, length);\n+JRT_LEAF(void, ZBarrierSetRuntime::store_barrier_on_native_oop_field_without_healing(oop* p))\n+  ZBarrier::store_barrier_on_native_oop_field((zpointer*)p, false \/* heal *\/);\n@@ -63,3 +71,5 @@\n-  if (decorators & ON_PHANTOM_OOP_REF) {\n-    if (decorators & AS_NO_KEEPALIVE) {\n-      return weak_load_barrier_on_phantom_oop_field_preloaded_addr();\n+  if (decorators & AS_NO_KEEPALIVE) {\n+    if (decorators & ON_PHANTOM_OOP_REF) {\n+      return no_keepalive_load_barrier_on_phantom_oop_field_preloaded_addr();\n+    } else if (decorators & ON_WEAK_OOP_REF) {\n+      return no_keepalive_load_barrier_on_weak_oop_field_preloaded_addr();\n@@ -67,7 +77,3 @@\n-      return load_barrier_on_phantom_oop_field_preloaded_addr();\n-    }\n-  } else if (decorators & ON_WEAK_OOP_REF) {\n-    if (decorators & AS_NO_KEEPALIVE) {\n-      return weak_load_barrier_on_weak_oop_field_preloaded_addr();\n-    } else {\n-      return load_barrier_on_weak_oop_field_preloaded_addr();\n+      assert((decorators & ON_STRONG_OOP_REF), \"Expected type\");\n+      \/\/ Normal loads on strong oop never keep objects alive\n+      return load_barrier_on_oop_field_preloaded_addr();\n@@ -76,2 +82,4 @@\n-    if (decorators & AS_NO_KEEPALIVE) {\n-      return weak_load_barrier_on_oop_field_preloaded_addr();\n+    if (decorators & ON_PHANTOM_OOP_REF) {\n+      return load_barrier_on_phantom_oop_field_preloaded_addr();\n+    } else if (decorators & ON_WEAK_OOP_REF) {\n+      return load_barrier_on_weak_oop_field_preloaded_addr();\n@@ -79,0 +87,1 @@\n+      assert((decorators & ON_STRONG_OOP_REF), \"Expected type\");\n@@ -88,0 +97,4 @@\n+address ZBarrierSetRuntime::load_barrier_on_oop_field_preloaded_store_good_addr() {\n+  return reinterpret_cast<address>(load_barrier_on_oop_field_preloaded_store_good);\n+}\n+\n@@ -96,2 +109,6 @@\n-address ZBarrierSetRuntime::weak_load_barrier_on_oop_field_preloaded_addr() {\n-  return reinterpret_cast<address>(weak_load_barrier_on_oop_field_preloaded);\n+address ZBarrierSetRuntime::no_keepalive_load_barrier_on_weak_oop_field_preloaded_addr() {\n+  return reinterpret_cast<address>(no_keepalive_load_barrier_on_weak_oop_field_preloaded);\n+}\n+\n+address ZBarrierSetRuntime::no_keepalive_load_barrier_on_phantom_oop_field_preloaded_addr() {\n+  return reinterpret_cast<address>(no_keepalive_load_barrier_on_phantom_oop_field_preloaded);\n@@ -100,2 +117,2 @@\n-address ZBarrierSetRuntime::weak_load_barrier_on_weak_oop_field_preloaded_addr() {\n-  return reinterpret_cast<address>(weak_load_barrier_on_weak_oop_field_preloaded);\n+address ZBarrierSetRuntime::store_barrier_on_oop_field_with_healing_addr() {\n+  return reinterpret_cast<address>(store_barrier_on_oop_field_with_healing);\n@@ -104,2 +121,2 @@\n-address ZBarrierSetRuntime::weak_load_barrier_on_phantom_oop_field_preloaded_addr() {\n-  return reinterpret_cast<address>(weak_load_barrier_on_phantom_oop_field_preloaded);\n+address ZBarrierSetRuntime::store_barrier_on_oop_field_without_healing_addr() {\n+  return reinterpret_cast<address>(store_barrier_on_oop_field_without_healing);\n@@ -108,2 +125,2 @@\n-address ZBarrierSetRuntime::load_barrier_on_oop_array_addr() {\n-  return reinterpret_cast<address>(load_barrier_on_oop_array);\n+address ZBarrierSetRuntime::store_barrier_on_native_oop_field_without_healing_addr() {\n+  return reinterpret_cast<address>(store_barrier_on_native_oop_field_without_healing);\n","filename":"src\/hotspot\/share\/gc\/z\/zBarrierSetRuntime.cpp","additions":51,"deletions":34,"binary":false,"changes":85,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,0 +27,1 @@\n+#include \"gc\/z\/zAddress.hpp\"\n@@ -29,0 +30,1 @@\n+#include \"oops\/oopsHierarchy.hpp\"\n@@ -31,2 +33,0 @@\n-class oopDesc;\n-\n@@ -36,0 +36,1 @@\n+  static zpointer load_barrier_on_oop_field_preloaded_store_good(oopDesc* o, oop* p);\n@@ -38,4 +39,5 @@\n-  static oopDesc* weak_load_barrier_on_oop_field_preloaded(oopDesc* o, oop* p);\n-  static oopDesc* weak_load_barrier_on_weak_oop_field_preloaded(oopDesc* o, oop* p);\n-  static oopDesc* weak_load_barrier_on_phantom_oop_field_preloaded(oopDesc* o, oop* p);\n-  static void load_barrier_on_oop_array(oop* p, size_t length);\n+  static oopDesc* no_keepalive_load_barrier_on_weak_oop_field_preloaded(oopDesc* o, oop* p);\n+  static oopDesc* no_keepalive_load_barrier_on_phantom_oop_field_preloaded(oopDesc* o, oop* p);\n+  static void store_barrier_on_oop_field_with_healing(oop* p);\n+  static void store_barrier_on_oop_field_without_healing(oop* p);\n+  static void store_barrier_on_native_oop_field_without_healing(oop* p);\n@@ -47,0 +49,1 @@\n+  static address load_barrier_on_oop_field_preloaded_store_good_addr();\n@@ -49,4 +52,5 @@\n-  static address weak_load_barrier_on_oop_field_preloaded_addr();\n-  static address weak_load_barrier_on_weak_oop_field_preloaded_addr();\n-  static address weak_load_barrier_on_phantom_oop_field_preloaded_addr();\n-  static address load_barrier_on_oop_array_addr();\n+  static address no_keepalive_load_barrier_on_weak_oop_field_preloaded_addr();\n+  static address no_keepalive_load_barrier_on_phantom_oop_field_preloaded_addr();\n+  static address store_barrier_on_oop_field_with_healing_addr();\n+  static address store_barrier_on_oop_field_without_healing_addr();\n+  static address store_barrier_on_native_oop_field_without_healing_addr();\n","filename":"src\/hotspot\/share\/gc\/z\/zBarrierSetRuntime.hpp","additions":15,"deletions":11,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2022, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -26,1 +26,0 @@\n-#include \"gc\/z\/zBarrier.inline.hpp\"\n@@ -28,1 +27,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"gc\/z\/zContinuation.hpp\"\n@@ -32,1 +31,2 @@\n-  \/\/ Do nothing\n+  ZContinuation::ZColorStackOopClosure cl(chunk);\n+  iterator->oops_do(&cl);\n@@ -36,1 +36,2 @@\n-  \/\/ Do nothing\n+  ZContinuation::ZUncolorStackOopClosure cl;\n+  iterator->oops_do(&cl);\n@@ -40,2 +41,1 @@\n-  oop obj = Atomic::load(addr);\n-  return ZBarrier::load_barrier_on_oop_field_preloaded((volatile oop*)NULL, obj);\n+  return ZContinuation::load_oop(chunk, addr);\n@@ -46,1 +46,1 @@\n-  return NULL;\n+  return nullptr;\n","filename":"src\/hotspot\/share\/gc\/z\/zBarrierSetStackChunk.cpp","additions":8,"deletions":8,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, 2017, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -29,0 +29,6 @@\n+class ZMovableBitMap : public CHeapBitMap {\n+public:\n+  ZMovableBitMap();\n+  ZMovableBitMap(ZMovableBitMap&& bitmap);\n+};\n+\n@@ -38,0 +44,1 @@\n+  ZBitMap(const ZBitMap& other);\n@@ -40,0 +47,17 @@\n+\n+  class ReverseIterator;\n+};\n+\n+class ZBitMap::ReverseIterator {\n+  BitMap* const _bitmap;\n+  BitMap::idx_t _beg;\n+  BitMap::idx_t _end;\n+\n+public:\n+  ReverseIterator(BitMap* bitmap);\n+  ReverseIterator(BitMap* bitmap, BitMap::idx_t beg, BitMap::idx_t end);\n+\n+  void reset(BitMap::idx_t beg, BitMap::idx_t end);\n+  void reset(BitMap::idx_t end);\n+\n+  bool next(BitMap::idx_t* index);\n","filename":"src\/hotspot\/share\/gc\/z\/zBitMap.hpp","additions":25,"deletions":1,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, 2017, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -33,0 +33,9 @@\n+inline ZMovableBitMap::ZMovableBitMap() :\n+    CHeapBitMap(mtGC) {}\n+\n+inline ZMovableBitMap::ZMovableBitMap(ZMovableBitMap&& bitmap) :\n+    CHeapBitMap(mtGC) {\n+  update(bitmap.map(), bitmap.size());\n+  bitmap.update(nullptr, 0);\n+}\n+\n@@ -36,0 +45,5 @@\n+inline ZBitMap::ZBitMap(const ZBitMap& other) :\n+    CHeapBitMap(other.size(), mtGC, false \/* clear *\/) {\n+  memcpy(map(), other.map(), size_in_bytes());\n+}\n+\n@@ -80,0 +94,30 @@\n+inline ZBitMap::ReverseIterator::ReverseIterator(BitMap* bitmap) :\n+    ZBitMap::ReverseIterator(bitmap, 0, bitmap->size()) {}\n+\n+inline ZBitMap::ReverseIterator::ReverseIterator(BitMap* bitmap, BitMap::idx_t beg, BitMap::idx_t end) :\n+    _bitmap(bitmap),\n+    _beg(beg),\n+    _end(end) {}\n+\n+inline void ZBitMap::ReverseIterator::reset(BitMap::idx_t beg, BitMap::idx_t end) {\n+  assert(beg < _bitmap->size(), \"beg index out of bounds\");\n+  assert(end >= beg && end <= _bitmap->size(), \"end index out of bounds\");\n+  _beg = beg;\n+  _end = end;\n+}\n+\n+inline void ZBitMap::ReverseIterator::reset(BitMap::idx_t end) {\n+  assert(end >= _beg && end <= _bitmap->size(), \"end index out of bounds\");\n+  _end = end;\n+}\n+\n+inline bool ZBitMap::ReverseIterator::next(BitMap::idx_t *index) {\n+  BitMap::ReverseIterator iter(*_bitmap, _beg, _end);\n+  if (iter.is_empty()) {\n+    return false;\n+  }\n+\n+  *index = _end = iter.index();\n+  return true;\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/z\/zBitMap.inline.hpp","additions":45,"deletions":1,"binary":false,"changes":46,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -35,1 +35,1 @@\n-PaddedEnd<ZCPU::ZCPUAffinity>* ZCPU::_affinity = NULL;\n+PaddedEnd<ZCPU::ZCPUAffinity>* ZCPU::_affinity = nullptr;\n@@ -40,1 +40,1 @@\n-  assert(_affinity == NULL, \"Already initialized\");\n+  assert(_affinity == nullptr, \"Already initialized\");\n","filename":"src\/hotspot\/share\/gc\/z\/zCPU.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2019, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -37,1 +37,1 @@\n-  assert(_affinity != NULL, \"Not initialized\");\n+  assert(_affinity != nullptr, \"Not initialized\");\n","filename":"src\/hotspot\/share\/gc\/z\/zCPU.inline.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -24,0 +24,1 @@\n+#include \"gc\/z\/zAddress.hpp\"\n@@ -27,1 +28,1 @@\n-#include \"gc\/shared\/gcLocker.inline.hpp\"\n+#include \"gc\/shared\/gcLogPrecious.hpp\"\n@@ -29,0 +30,3 @@\n+#include \"gc\/z\/zAbort.hpp\"\n+#include \"gc\/z\/zAddress.inline.hpp\"\n+#include \"gc\/z\/zAllocator.inline.hpp\"\n@@ -30,0 +34,1 @@\n+#include \"gc\/z\/zContinuation.inline.hpp\"\n@@ -32,0 +37,1 @@\n+#include \"gc\/z\/zGeneration.inline.hpp\"\n@@ -34,0 +40,1 @@\n+#include \"gc\/z\/zJNICritical.hpp\"\n@@ -36,1 +43,0 @@\n-#include \"gc\/z\/zOop.inline.hpp\"\n@@ -38,0 +44,1 @@\n+#include \"gc\/z\/zStackChunkGCData.inline.hpp\"\n@@ -46,0 +53,2 @@\n+#include \"runtime\/jniHandles.inline.hpp\"\n+#include \"services\/memoryUsage.hpp\"\n@@ -57,2 +66,3 @@\n-    _driver(new ZDriver()),\n-    _director(new ZDirector(_driver)),\n+    _driver_minor(new ZDriverMinor()),\n+    _driver_major(new ZDriverMajor()),\n+    _director(new ZDirector()),\n@@ -75,1 +85,1 @@\n-  Universe::calculate_verify_data((HeapWord*)0, (HeapWord*)UINTPTR_MAX);\n+  Universe::set_verify_data(~(ZAddressHeapBase - 1) | 0x7, ZAddressHeapBase);\n@@ -94,0 +104,2 @@\n+  log_info_p(gc, exit)(\"Stopping ZGC\");\n+  ZAbort::abort();\n@@ -129,18 +141,1 @@\n-  uintptr_t* cont_addr = obj->field_addr<uintptr_t>(jdk_internal_vm_StackChunk::cont_offset());\n-\n-  if (!_heap.is_allocating(cast_from_oop<uintptr_t>(obj))) {\n-    \/\/ An object that isn't allocating, is visible from GC tracing. Such\n-    \/\/ stack chunks require barriers.\n-    return true;\n-  }\n-\n-  if (!ZAddress::is_good_or_null(*cont_addr)) {\n-    \/\/ If a chunk is allocated after a GC started, but before relocate start\n-    \/\/ we can have an allocating chunk that isn't deeply good. That means that\n-    \/\/ the contained oops might be bad and require GC barriers.\n-    return true;\n-  }\n-\n-  \/\/ The chunk is allocating and its pointers are good. This chunk needs no\n-  \/\/ GC barriers\n-  return false;\n+  return ZContinuation::requires_barriers(&_heap, obj);\n@@ -151,1 +146,1 @@\n-  const uintptr_t addr = _heap.alloc_tlab(size_in_bytes);\n+  const zaddress addr = ZAllocator::eden()->alloc_tlab(size_in_bytes);\n@@ -153,1 +148,1 @@\n-  if (addr != 0) {\n+  if (!is_null(addr)) {\n@@ -157,1 +152,1 @@\n-  return (HeapWord*)addr;\n+  return (HeapWord*)untype(addr);\n@@ -161,1 +156,1 @@\n-  ZObjArrayAllocator allocator(klass, size, length, do_zero, THREAD);\n+  const ZObjArrayAllocator allocator(klass, size, length, do_zero, THREAD);\n@@ -167,1 +162,1 @@\n-  return (HeapWord*)_heap.alloc_object(size_in_bytes);\n+  return (HeapWord*)ZAllocator::eden()->alloc_object(size_in_bytes);\n@@ -178,1 +173,1 @@\n-  if (result != NULL) {\n+  if (result != nullptr) {\n@@ -187,1 +182,32 @@\n-  _driver->collect(cause);\n+  \/\/ Handle external collection requests\n+  switch (cause) {\n+  case GCCause::_wb_young_gc:\n+  case GCCause::_scavenge_alot:\n+    \/\/ Start urgent minor GC\n+    _driver_minor->collect(ZDriverRequest(cause, ZYoungGCThreads, 0));\n+    break;\n+\n+  case GCCause::_heap_dump:\n+  case GCCause::_heap_inspection:\n+  case GCCause::_wb_full_gc:\n+  case GCCause::_wb_breakpoint:\n+  case GCCause::_dcmd_gc_run:\n+  case GCCause::_java_lang_system_gc:\n+  case GCCause::_full_gc_alot:\n+  case GCCause::_jvmti_force_gc:\n+  case GCCause::_metadata_GC_clear_soft_refs:\n+  case GCCause::_codecache_GC_aggressive:\n+    \/\/ Start urgent major GC\n+    _driver_major->collect(ZDriverRequest(cause, ZYoungGCThreads, ZOldGCThreads));\n+    break;\n+\n+  case GCCause::_metadata_GC_threshold:\n+  case GCCause::_codecache_GC_threshold:\n+    \/\/ Start not urgent major GC\n+    _driver_major->collect(ZDriverRequest(cause, 1, 1));\n+    break;\n+\n+  default:\n+    fatal(\"Unsupported GC cause (%s)\", GCCause::to_string(cause));\n+    break;\n+  }\n@@ -194,4 +220,3 @@\n-  \/\/ However, neither the heap dumper nor the heap inspector really need a GC\n-  \/\/ to happen, but the result of their heap iterations might in that case be\n-  \/\/ less accurate since they might include objects that would otherwise have\n-  \/\/ been collected by a GC.\n+  \/\/ If the heap dumper or heap inspector explicitly requests a gc and the\n+  \/\/ caller is not the VM thread a synchronous GC cycle is performed from the\n+  \/\/ caller thread in the prologue.\n@@ -229,1 +254,6 @@\n-  return _heap.serviceability_memory_pool()->get_memory_usage();\n+  const size_t initial_size = ZHeap::heap()->initial_capacity();\n+  const size_t committed    = ZHeap::heap()->capacity();\n+  const size_t used         = MIN2(ZHeap::heap()->used(), committed);\n+  const size_t max_size     = ZHeap::heap()->max_capacity();\n+\n+  return MemoryUsage(initial_size, used, committed, max_size);\n@@ -233,3 +263,5 @@\n-  GrowableArray<GCMemoryManager*> memory_managers(2);\n-  memory_managers.append(_heap.serviceability_cycle_memory_manager());\n-  memory_managers.append(_heap.serviceability_pause_memory_manager());\n+  GrowableArray<GCMemoryManager*> memory_managers(4);\n+  memory_managers.append(_heap.serviceability_cycle_memory_manager(true \/* minor *\/));\n+  memory_managers.append(_heap.serviceability_cycle_memory_manager(false \/* minor *\/));\n+  memory_managers.append(_heap.serviceability_pause_memory_manager(true \/* minor *\/));\n+  memory_managers.append(_heap.serviceability_pause_memory_manager(false \/* minor *\/));\n@@ -240,2 +272,3 @@\n-  GrowableArray<MemoryPool*> memory_pools(1);\n-  memory_pools.append(_heap.serviceability_memory_pool());\n+  GrowableArray<MemoryPool*> memory_pools(2);\n+  memory_pools.append(_heap.serviceability_memory_pool(ZGenerationId::young));\n+  memory_pools.append(_heap.serviceability_memory_pool(ZGenerationId::old));\n@@ -253,0 +286,8 @@\n+void ZCollectedHeap::pin_object(JavaThread* thread, oop obj) {\n+  ZJNICritical::enter(thread);\n+}\n+\n+void ZCollectedHeap::unpin_object(JavaThread* thread, oop obj) {\n+  ZJNICritical::exit(thread);\n+}\n+\n@@ -262,1 +303,8 @@\n-  ZNMethod::unregister_nmethod(nm);\n+  \/\/ ZGC follows the 'unlink | handshake | purge', where nmethods are unlinked\n+  \/\/ from the system, threads are handshaked so that no reference to the\n+  \/\/ unlinked nmethods exist, then the nmethods are deleted in the purge phase.\n+  \/\/\n+  \/\/ CollectedHeap::unregister_nmethod is called during the flush phase, which\n+  \/\/ is too late for ZGC.\n+\n+  ZNMethod::purge_nmethod(nm);\n@@ -275,1 +323,2 @@\n-  tc->do_thread(_driver);\n+  tc->do_thread(_driver_major);\n+  tc->do_thread(_driver_minor);\n@@ -282,1 +331,13 @@\n-  return VirtualSpaceSummary((HeapWord*)0, (HeapWord*)capacity(), (HeapWord*)max_capacity());\n+  const uintptr_t start = ZAddressHeapBase;\n+\n+  \/\/ Fake values. ZGC does not commit memory contiguously in the reserved\n+  \/\/ address space, and the reserved space is larger than MaxHeapSize.\n+  const uintptr_t committed_end = ZAddressHeapBase + capacity();\n+  const uintptr_t reserved_end = ZAddressHeapBase + max_capacity();\n+\n+  return VirtualSpaceSummary((HeapWord*)start, (HeapWord*)committed_end, (HeapWord*)reserved_end);\n+}\n+\n+bool ZCollectedHeap::contains_null(const oop* p) const {\n+  const zpointer* const ptr = (const zpointer*)p;\n+  return is_null_any(*ptr);\n@@ -286,0 +347,2 @@\n+  ZGeneration::young()->synchronize_relocation();\n+  ZGeneration::old()->synchronize_relocation();\n@@ -291,8 +354,2 @@\n-}\n-\n-void ZCollectedHeap::pin_object(JavaThread* thread, oop obj) {\n-  GCLocker::lock_critical(thread);\n-}\n-\n-void ZCollectedHeap::unpin_object(JavaThread* thread, oop obj) {\n-  GCLocker::unlock_critical(thread);\n+  ZGeneration::old()->desynchronize_relocation();\n+  ZGeneration::young()->desynchronize_relocation();\n@@ -311,3 +368,3 @@\n-  st->print_cr(\" GlobalPhase:       %u (%s)\", ZGlobalPhase, ZGlobalPhaseToString());\n-  st->print_cr(\" GlobalSeqNum:      %u\", ZGlobalSeqNum);\n-  st->print_cr(\" Offset Max:        \" SIZE_FORMAT \"%s (\" PTR_FORMAT \")\",\n+  st->print_cr(\" Young Collection:   %s\/%u\", ZGeneration::young()->phase_to_string(), ZGeneration::young()->seqnum());\n+  st->print_cr(\" Old Collection:     %s\/%u\", ZGeneration::old()->phase_to_string(), ZGeneration::old()->seqnum());\n+  st->print_cr(\" Offset Max:         \" SIZE_FORMAT \"%s (\" PTR_FORMAT \")\",\n@@ -317,2 +374,2 @@\n-  st->print_cr(\" Page Size Small:   \" SIZE_FORMAT \"M\", ZPageSizeSmall \/ M);\n-  st->print_cr(\" Page Size Medium:  \" SIZE_FORMAT \"M\", ZPageSizeMedium \/ M);\n+  st->print_cr(\" Page Size Small:    \" SIZE_FORMAT \"M\", ZPageSizeSmall \/ M);\n+  st->print_cr(\" Page Size Medium:   \" SIZE_FORMAT \"M\", ZPageSizeMedium \/ M);\n@@ -321,5 +378,13 @@\n-  st->print_cr(\" Good:              \" PTR_FORMAT, ZAddressGoodMask);\n-  st->print_cr(\" Bad:               \" PTR_FORMAT, ZAddressBadMask);\n-  st->print_cr(\" WeakBad:           \" PTR_FORMAT, ZAddressWeakBadMask);\n-  st->print_cr(\" Marked:            \" PTR_FORMAT, ZAddressMetadataMarked);\n-  st->print_cr(\" Remapped:          \" PTR_FORMAT, ZAddressMetadataRemapped);\n+  st->print_cr(\" LoadGood:           \" PTR_FORMAT, ZPointerLoadGoodMask);\n+  st->print_cr(\" LoadBad:            \" PTR_FORMAT, ZPointerLoadBadMask);\n+  st->print_cr(\" MarkGood:           \" PTR_FORMAT, ZPointerMarkGoodMask);\n+  st->print_cr(\" MarkBad:            \" PTR_FORMAT, ZPointerMarkBadMask);\n+  st->print_cr(\" StoreGood:          \" PTR_FORMAT, ZPointerStoreGoodMask);\n+  st->print_cr(\" StoreBad:           \" PTR_FORMAT, ZPointerStoreBadMask);\n+  st->print_cr(\" ------------------- \");\n+  st->print_cr(\" Remapped:           \" PTR_FORMAT, ZPointerRemapped);\n+  st->print_cr(\" RemappedYoung:      \" PTR_FORMAT, ZPointerRemappedYoungMask);\n+  st->print_cr(\" RemappedOld:        \" PTR_FORMAT, ZPointerRemappedOldMask);\n+  st->print_cr(\" MarkedYoung:        \" PTR_FORMAT, ZPointerMarkedYoung);\n+  st->print_cr(\" MarkedOld:          \" PTR_FORMAT, ZPointerMarkedOld);\n+  st->print_cr(\" Remembered:         \" PTR_FORMAT, ZPointerRemembered);\n@@ -343,1 +408,1 @@\n-  _heap.verify();\n+  fatal(\"Externally triggered verification not supported\");\n@@ -347,1 +412,1 @@\n-  return _heap.is_oop(ZOop::to_address(object));\n+  return _heap.is_oop(cast_from_oop<uintptr_t>(object));\n","filename":"src\/hotspot\/share\/gc\/z\/zCollectedHeap.cpp","additions":128,"deletions":63,"binary":false,"changes":191,"status":"modified"},{"patch":"@@ -37,1 +37,2 @@\n-class ZDriver;\n+class ZDriverMajor;\n+class ZDriverMinor;\n@@ -48,1 +49,2 @@\n-  ZDriver*          _driver;\n+  ZDriverMinor*     _driver_minor;\n+  ZDriverMajor*     _driver_major;\n@@ -113,0 +115,2 @@\n+  bool contains_null(const oop* p) const override;\n+\n","filename":"src\/hotspot\/share\/gc\/z\/zCollectedHeap.hpp","additions":6,"deletions":2,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -0,0 +1,109 @@\n+\/*\n+ * Copyright (c) 2022, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/z\/zAddress.inline.hpp\"\n+#include \"gc\/z\/zBarrier.inline.hpp\"\n+#include \"gc\/z\/zContinuation.inline.hpp\"\n+#include \"gc\/z\/zStackChunkGCData.inline.hpp\"\n+#include \"runtime\/atomic.hpp\"\n+\n+static zpointer materialize_zpointer(stackChunkOop chunk, void* addr) {\n+  volatile uintptr_t* const value_addr = (volatile uintptr_t*)addr;\n+\n+  \/\/ A stack chunk has two modes:\n+  \/\/\n+  \/\/ 1) It's recently allocated and the contents is a copy of the native stack.\n+  \/\/    All oops have the format of oops in the stack. That is, they are\n+  \/\/    zaddresses, and don't have any colored metadata bits.\n+  \/\/\n+  \/\/ 2) It has lived long enough that the GC needs to visit the oops.\n+  \/\/    Before the GC visits the oops, they are converted into zpointers,\n+  \/\/    and become colored pointers.\n+  \/\/\n+  \/\/ The load_oop function supports loading oops from chunks in either of the\n+  \/\/ two modes. It even supports loading oops, while another thread is\n+  \/\/ converting the chunk to \"gc mode\" [transition from (1) to (2)]. So, we\n+  \/\/ load the oop once and perform all checks on that loaded copy.\n+\n+  \/\/ Load once\n+  const uintptr_t value = Atomic::load(value_addr);\n+\n+  if ((value & ~ZPointerAllMetadataMask) == 0) {\n+    \/\/ Must be null of some sort - either zaddress or zpointer\n+    return zpointer::null;\n+  }\n+\n+  const uintptr_t impossible_zaddress_mask = ~((ZAddressHeapBase - 1) | ZAddressHeapBase);\n+  if ((value & impossible_zaddress_mask) != 0) {\n+    \/\/ Must be a zpointer - it has bits forbidden in zaddresses\n+    return to_zpointer(value);\n+  }\n+\n+  \/\/ Must be zaddress\n+  const zaddress_unsafe zaddr = to_zaddress_unsafe(value);\n+\n+  \/\/ A zaddress means that the chunk was recently allocated, and the layout is\n+  \/\/ that of a native stack. That means that oops are uncolored (zaddress). But\n+  \/\/ the oops still have an implicit color, saved away in the chunk.\n+\n+  \/\/ Use the implicit color, and create a zpointer that is equivalent with\n+  \/\/ what we would have written if we where to eagerly create the zpointer\n+  \/\/ when the stack frames where copied into the chunk.\n+  const uintptr_t color = ZStackChunkGCData::color(chunk);\n+  return ZAddress::color(zaddr, color);\n+}\n+\n+oop ZContinuation::load_oop(stackChunkOop chunk, void* addr) {\n+  \/\/ addr could contain either a zpointer or a zaddress\n+  const zpointer zptr = materialize_zpointer(chunk, addr);\n+\n+  \/\/ Apply the load barrier, without healing the zaddress\/zpointer\n+  return to_oop(ZBarrier::load_barrier_on_oop_field_preloaded(nullptr \/* p *\/, zptr));\n+}\n+\n+ZContinuation::ZColorStackOopClosure::ZColorStackOopClosure(stackChunkOop chunk) :\n+    _color(ZStackChunkGCData::color(chunk)) {\n+}\n+\n+void ZContinuation::ZColorStackOopClosure::do_oop(oop* p) {\n+  \/\/ Convert zaddress to zpointer\n+  \/\/ TODO: Comment why this is safe and non volatile\n+  zaddress_unsafe* const p_zaddress_unsafe = (zaddress_unsafe*)p;\n+  zpointer* const p_zpointer = (zpointer*)p;\n+  *p_zpointer = ZAddress::color(*p_zaddress_unsafe, _color);\n+}\n+\n+void ZContinuation::ZColorStackOopClosure::do_oop(narrowOop* p) {\n+  ShouldNotReachHere();\n+}\n+\n+void ZContinuation::ZUncolorStackOopClosure::do_oop(oop* p) {\n+  const zpointer ptr = *(volatile zpointer*)p;\n+  const zaddress addr = ZPointer::uncolor(ptr);\n+  *(volatile zaddress*)p = addr;\n+}\n+\n+void ZContinuation::ZUncolorStackOopClosure::do_oop(narrowOop* p) {\n+  ShouldNotReachHere();\n+}\n","filename":"src\/hotspot\/share\/gc\/z\/zContinuation.cpp","additions":109,"deletions":0,"binary":false,"changes":109,"status":"added"},{"patch":"@@ -0,0 +1,57 @@\n+\/*\n+ * Copyright (c) 2022, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_Z_ZCONTINUATION_HPP\n+#define SHARE_GC_Z_ZCONTINUATION_HPP\n+\n+#include \"memory\/allStatic.hpp\"\n+#include \"memory\/iterator.hpp\"\n+#include \"oops\/oopsHierarchy.hpp\"\n+\n+class OopClosure;\n+class ZHeap;\n+\n+class ZContinuation : public AllStatic {\n+public:\n+  static bool requires_barriers(const ZHeap* heap, stackChunkOop chunk);\n+\n+  static oop load_oop(stackChunkOop chunk, void* addr);\n+\n+  class ZColorStackOopClosure : public OopClosure {\n+  private:\n+    uintptr_t _color;\n+\n+  public:\n+    ZColorStackOopClosure(stackChunkOop chunk);\n+    virtual void do_oop(oop* p) override;\n+    virtual void do_oop(narrowOop* p) override;\n+  };\n+\n+  class ZUncolorStackOopClosure : public OopClosure {\n+  public:\n+    virtual void do_oop(oop* p) override;\n+    virtual void do_oop(narrowOop* p) override;\n+  };\n+};\n+\n+#endif \/\/ SHARE_GC_Z_ZCONTINUATION_HPP\n","filename":"src\/hotspot\/share\/gc\/z\/zContinuation.hpp","additions":57,"deletions":0,"binary":false,"changes":57,"status":"added"},{"patch":"@@ -0,0 +1,54 @@\n+\/*\n+ * Copyright (c) 2022, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_Z_ZCONTINUATION_INLINE_HPP\n+#define SHARE_GC_Z_ZCONTINUATION_INLINE_HPP\n+\n+#include \"gc\/z\/zContinuation.hpp\"\n+\n+#include \"classfile\/javaClasses.hpp\"\n+#include \"gc\/z\/zAddress.inline.hpp\"\n+#include \"gc\/z\/zHeap.inline.hpp\"\n+#include \"gc\/z\/zStackChunkGCData.inline.hpp\"\n+#include \"oops\/oop.inline.hpp\"\n+\n+inline bool ZContinuation::requires_barriers(const ZHeap* heap, stackChunkOop chunk) {\n+  if (!heap->is_allocating(to_zaddress(chunk))) {\n+    \/\/ An object that isn't allocating, is visible from GC tracing. Such\n+    \/\/ stack chunks require barriers.\n+    return true;\n+  }\n+\n+  if (ZStackChunkGCData::color(chunk) != ZPointerStoreGoodMask) {\n+    \/\/ If a chunk is allocated after a GC started, but before relocate start\n+    \/\/ we can have an allocating chunk that isn't deeply good. That means that\n+    \/\/ the contained oops might be bad and require GC barriers.\n+    return true;\n+  }\n+\n+  \/\/ The chunk is allocating and its pointers are good. This chunk needs no\n+  \/\/ GC barriers\n+  return false;\n+}\n+\n+#endif \/\/ SHARE_GC_Z_ZCONTINUATION_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/z\/zContinuation.inline.hpp","additions":54,"deletions":0,"binary":false,"changes":54,"status":"added"},{"patch":"@@ -17,1 +17,1 @@\n-    if ((uintptr_t)$obj & (uintptr_t)ZAddressGoodMask)\n+    if ((uintptr_t)$obj & (uintptr_t)ZPointerStoreGoodMask)\n@@ -19,1 +19,1 @@\n-        if ((uintptr_t)$obj & (uintptr_t)ZAddressMetadataRemapped)\n+        if ((uintptr_t)$obj & (uintptr_t)ZPointerRemapped)\n@@ -22,2 +22,2 @@\n-            if ((uintptr_t)$obj & (uintptr_t)ZAddressMetadataMarked)\n-                printf \"(Marked)\"\n+            if ((uintptr_t)$obj & (uintptr_t)ZPointerMarkedOld)\n+                printf \"(MarkedOld)\"\n@@ -30,1 +30,1 @@\n-        if ((uintptr_t)ZAddressGoodMask & (uintptr_t)ZAddressMetadataMarked)\n+        if ((uintptr_t)ZPointerStoreGoodMask & (uintptr_t)ZPointerMarkedOld)\n@@ -32,1 +32,1 @@\n-            if ((uintptr_t)$obj & (uintptr_t)ZAddressMetadataRemapped)\n+            if ((uintptr_t)$obj & (uintptr_t)ZPointerRemapped)\n@@ -38,1 +38,1 @@\n-            if ((uintptr_t)ZAddressGoodMask & (uintptr_t)ZAddressMetadataRemapped)\n+            if ((uintptr_t)ZPointerStoreGoodMask & (uintptr_t)ZPointerRemapped)\n@@ -40,1 +40,1 @@\n-                if ((uintptr_t)$obj & (uintptr_t)ZAddressMetadataMarked)\n+                if ((uintptr_t)$obj & (uintptr_t)ZPointerMarkedOld)\n@@ -132,0 +132,36 @@\n+# For some reason gdb doesn't like ZGeneration::ZPhase::Mark etc.\n+# Use hard-coded values instead.\n+define z_print_phase\n+  if $arg0 == 0\n+    printf \"Mark\"\n+  else\n+    if $arg0 == 1\n+      printf \"MarkComplete\"\n+    else\n+      if $arg0 == 2\n+        printf \"Relocate\"\n+      else\n+\tprintf \"Unknown\"\n+      end\n+    end\n+  end\n+end\n+\n+define z_print_generation\n+  printf \"%u\", $arg0->_seqnum\n+  printf \"\/\"\n+  z_print_phase $arg0->_phase\n+end\n+\n+define zz\n+  printf \"Old: \"\n+  z_print_generation ZHeap::_heap->_old\n+\n+  printf \" | \"\n+\n+  printf \"Young: \"\n+  z_print_generation ZHeap::_heap->_young\n+\n+  printf \"\\n\"\n+end\n+\n@@ -135,2 +171,4 @@\n-    printf \"     GlobalPhase:       %u\\n\", ZGlobalPhase\n-    printf \"     GlobalSeqNum:      %u\\n\", ZGlobalSeqNum\n+    printf \"     Young Phase:       %u\\n\", ZHeap::_heap->_young->_phase\n+    printf \"     Old Phase:         %u\\n\", ZHeap::_heap->_old->_phase\n+    printf \"     Young SeqNum:      %u\\n\", ZHeap::_heap->_young->_seqnum\n+    printf \"     Old SeqNum:        %u\\n\", ZHeap::_heap->_old->_seqnum\n@@ -141,5 +179,46 @@\n-    printf \"     Good:              0x%016llx\\n\", ZAddressGoodMask\n-    printf \"     Bad:               0x%016llx\\n\", ZAddressBadMask\n-    printf \"     WeakBad:           0x%016llx\\n\", ZAddressWeakBadMask\n-    printf \"     Marked:            0x%016llx\\n\", ZAddressMetadataMarked\n-    printf \"     Remapped:          0x%016llx\\n\", ZAddressMetadataRemapped\n+    printf \"     Good:              0x%016llx\\n\", ZPointerStoreGoodMask\n+    printf \"     Bad:               0x%016llx\\n\", ZPointerStoreBadMask\n+    printf \"     MarkedYoung:       0x%016llx\\n\", ZPointerMarkedYoung\n+    printf \"     MarkedOld:         0x%016llx\\n\", ZPointerMarkedOld\n+    printf \"     Remapped:          0x%016llx\\n\", ZPointerRemapped\n+end\n+\n+define print_bits\n+  set $value=$arg0\n+  set $bits=$arg1\n+\n+  set $bit=0\n+    while ($bit < $bits)\n+\tset $bit_pos = (1ull << ($bits - 1 - $bit))\n+\tprintf \"%d\", ($arg0 & $bit_pos) != 0\n+  \tset $bit = $bit + 1\n+  end\n+\n+  printf \" <%lX>\", $value\n+end\n+\n+define print_bits8\n+  print_bits $arg0 8\n+end\n+\n+define print_s_bits8\n+  printf $arg0\n+  print_bits8 $arg1\n+end\n+\n+# Print metadata information\n+define zpm\n+    printf          \"Metadata Load Bits  \"\n+    print_s_bits8 \"\\n     Mask:          \" ZPointerLoadMetadataMask\n+    print_s_bits8 \"\\n     Good:          \" ZPointerLoadGoodMask\n+    print_s_bits8 \"\\n     Remapped:      \" ZPointerRemapped\n+    print_s_bits8 \"\\n     Bad:           \" ZPointerLoadBadMask\n+    printf        \"\\n                    \"\n+    printf        \"\\nMetadata Store Bits \"\n+    print_s_bits8 \"\\n     Mask:          \" ZPointerStoreMetadataMask\n+    print_s_bits8 \"\\n     Good:          \" ZPointerStoreGoodMask\n+    print_s_bits8 \"\\n     Bad:           \" ZPointerStoreBadMask\n+    print_s_bits8 \"\\n     MarkedYoung:   \" ZPointerMarkedYoung\n+    print_s_bits8 \"\\n     MarkedOld:     \" ZPointerMarkedOld\n+    print_s_bits8 \"\\n     Finalizable:   \" ZPointerFinalizable\n+    printf        \"\\n\"\n","filename":"src\/hotspot\/share\/gc\/z\/zDebug.gdb","additions":94,"deletions":15,"binary":false,"changes":109,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -26,0 +26,1 @@\n+#include \"gc\/z\/zCollectedHeap.hpp\"\n@@ -28,0 +29,1 @@\n+#include \"gc\/z\/zGeneration.inline.hpp\"\n@@ -30,0 +32,1 @@\n+#include \"gc\/z\/zLock.inline.hpp\"\n@@ -33,0 +36,2 @@\n+ZDirector* ZDirector::_director;\n+\n@@ -34,1 +39,0 @@\n-constexpr double sample_interval = 1.0 \/ ZStatAllocRate::sample_hz;\n@@ -36,3 +40,37 @@\n-ZDirector::ZDirector(ZDriver* driver) :\n-    _driver(driver),\n-    _metronome(ZStatAllocRate::sample_hz) {\n+struct ZWorkerResizeStats {\n+  bool   _is_active;\n+  double _serial_gc_time_passed;\n+  double _parallel_gc_time_passed;\n+  uint   _nworkers_current;\n+};\n+\n+struct ZDirectorHeapStats {\n+  size_t _soft_max_heap_size;\n+  size_t _used;\n+  uint   _total_collections;\n+};\n+\n+struct ZDirectorGenerationGeneralStats {\n+  size_t _used;\n+  uint   _total_collections_at_start;\n+};\n+\n+struct ZDirectorGenerationStats {\n+  ZStatCycleStats                 _cycle;\n+  ZStatWorkersStats               _workers;\n+  ZWorkerResizeStats              _resize;\n+  ZStatHeapStats                  _stat_heap;\n+  ZDirectorGenerationGeneralStats _general;\n+};\n+\n+struct ZDirectorStats {\n+  ZStatMutatorAllocRateStats _mutator_alloc_rate;\n+  ZDirectorHeapStats         _heap;\n+  ZDirectorGenerationStats   _young_stats;\n+  ZDirectorGenerationStats   _old_stats;\n+};\n+\n+ZDirector::ZDirector() :\n+    _monitor(),\n+    _stopped(false) {\n+  _director = this;\n@@ -43,20 +81,1 @@\n-static void sample_allocation_rate() {\n-  \/\/ Sample allocation rate. This is needed by rule_allocation_rate()\n-  \/\/ below to estimate the time we have until we run out of memory.\n-  const double bytes_per_second = ZStatAllocRate::sample_and_reset();\n-\n-  log_debug(gc, alloc)(\"Allocation Rate: %.1fMB\/s, Predicted: %.1fMB\/s, Avg: %.1f(+\/-%.1f)MB\/s\",\n-                       bytes_per_second \/ M,\n-                       ZStatAllocRate::predict() \/ M,\n-                       ZStatAllocRate::avg() \/ M,\n-                       ZStatAllocRate::sd() \/ M);\n-}\n-\n-static ZDriverRequest rule_allocation_stall() {\n-  \/\/ Perform GC if we've observed at least one allocation stall since\n-  \/\/ the last GC started.\n-  if (!ZHeap::heap()->has_alloc_stalled()) {\n-    return GCCause::_no_gc;\n-  }\n-\n-  log_debug(gc, director)(\"Rule: Allocation Stall Observed\");\n+\/\/ Minor GC rules\n@@ -64,5 +83,2 @@\n-  return GCCause::_z_allocation_stall;\n-}\n-\n-static ZDriverRequest rule_warmup() {\n-  if (ZStatCycle::is_warm()) {\n+static bool rule_minor_timer(const ZDirectorStats& stats) {\n+  if (ZCollectionIntervalMinor <= 0) {\n@@ -70,25 +86,1 @@\n-    return GCCause::_no_gc;\n-  }\n-\n-  \/\/ Perform GC if heap usage passes 10\/20\/30% and no other GC has been\n-  \/\/ performed yet. This allows us to get some early samples of the GC\n-  \/\/ duration, which is needed by the other rules.\n-  const size_t soft_max_capacity = ZHeap::heap()->soft_max_capacity();\n-  const size_t used = ZHeap::heap()->used();\n-  const double used_threshold_percent = (ZStatCycle::nwarmup_cycles() + 1) * 0.1;\n-  const size_t used_threshold = soft_max_capacity * used_threshold_percent;\n-\n-  log_debug(gc, director)(\"Rule: Warmup %.0f%%, Used: \" SIZE_FORMAT \"MB, UsedThreshold: \" SIZE_FORMAT \"MB\",\n-                          used_threshold_percent * 100, used \/ M, used_threshold \/ M);\n-\n-  if (used < used_threshold) {\n-    return GCCause::_no_gc;\n-  }\n-\n-  return GCCause::_z_warmup;\n-}\n-\n-static ZDriverRequest rule_timer() {\n-  if (ZCollectionInterval <= 0) {\n-    \/\/ Rule disabled\n-    return GCCause::_no_gc;\n+    return false;\n@@ -98,5 +90,2 @@\n-  const double time_since_last_gc = ZStatCycle::time_since_last();\n-  const double time_until_gc = ZCollectionInterval - time_since_last_gc;\n-\n-  log_debug(gc, director)(\"Rule: Timer, Interval: %.3fs, TimeUntilGC: %.3fs\",\n-                          ZCollectionInterval, time_until_gc);\n+  const double time_since_last_gc = stats._young_stats._cycle._time_since_last;\n+  const double time_until_gc = ZCollectionIntervalMinor - time_since_last_gc;\n@@ -104,3 +93,2 @@\n-  if (time_until_gc > 0) {\n-    return GCCause::_no_gc;\n-  }\n+  log_debug(gc, director)(\"Rule Minor: Timer, Interval: %.3fs, TimeUntilGC: %.3fs\",\n+                          ZCollectionIntervalMinor, time_until_gc);\n@@ -108,1 +96,1 @@\n-  return GCCause::_z_timer;\n+  return time_until_gc <= 0;\n@@ -116,2 +104,2 @@\n-static uint discrete_gc_workers(double gc_workers) {\n-  return clamp<uint>(ceil(gc_workers), 1, ConcGCThreads);\n+static uint discrete_young_gc_workers(double gc_workers) {\n+  return clamp<uint>(ceil(gc_workers), 1, ZYoungGCThreads);\n@@ -120,1 +108,1 @@\n-static double select_gc_workers(double serial_gc_time, double parallelizable_gc_time, double alloc_rate_sd_percent, double time_until_oom) {\n+static double select_young_gc_workers(const ZDirectorStats& stats, double serial_gc_time, double parallelizable_gc_time, double alloc_rate_sd_percent, double time_until_oom) {\n@@ -122,3 +110,3 @@\n-  if (!ZStatCycle::is_warm()) {\n-    const double not_warm_gc_workers = ConcGCThreads;\n-    log_debug(gc, director)(\"Select GC Workers (Not Warm), GCWorkers: %.3f\", not_warm_gc_workers);\n+  if (!stats._old_stats._cycle._is_warm) {\n+    const double not_warm_gc_workers = ZYoungGCThreads;\n+    log_debug(gc, director)(\"Select Minor GC Workers (Not Warm), GCWorkers: %.3f\", not_warm_gc_workers);\n@@ -128,17 +116,4 @@\n-  \/\/ Calculate number of GC workers needed to avoid a long GC cycle and to avoid OOM.\n-  const double avoid_long_gc_workers = estimated_gc_workers(serial_gc_time, parallelizable_gc_time, 10 \/* seconds *\/);\n-  const double avoid_oom_gc_workers = estimated_gc_workers(serial_gc_time, parallelizable_gc_time, time_until_oom);\n-\n-  const double gc_workers = MAX2(avoid_long_gc_workers, avoid_oom_gc_workers);\n-  const uint actual_gc_workers = discrete_gc_workers(gc_workers);\n-  const uint last_gc_workers = ZStatCycle::last_active_workers();\n-\n-  \/\/ More than 15% division from the average is considered unsteady\n-  if (alloc_rate_sd_percent >= 0.15) {\n-    const double half_gc_workers = ConcGCThreads \/ 2.0;\n-    const double unsteady_gc_workers = MAX3<double>(gc_workers, last_gc_workers, half_gc_workers);\n-    log_debug(gc, director)(\"Select GC Workers (Unsteady), \"\n-                            \"AvoidLongGCWorkers: %.3f, AvoidOOMGCWorkers: %.3f, LastGCWorkers: %.3f, HalfGCWorkers: %.3f, GCWorkers: %.3f\",\n-                            avoid_long_gc_workers, avoid_oom_gc_workers, (double)last_gc_workers, half_gc_workers, unsteady_gc_workers);\n-    return unsteady_gc_workers;\n-  }\n+  \/\/ Calculate number of GC workers needed to avoid OOM.\n+  const double gc_workers = estimated_gc_workers(serial_gc_time, parallelizable_gc_time, time_until_oom);\n+  const uint actual_gc_workers = discrete_young_gc_workers(gc_workers);\n+  const double last_gc_workers = stats._young_stats._cycle._last_active_workers;\n@@ -146,1 +121,1 @@\n-  if (actual_gc_workers < last_gc_workers) {\n+  if ((double)actual_gc_workers < last_gc_workers) {\n@@ -151,1 +126,1 @@\n-    const double additional_time_for_allocations = ZStatCycle::time_since_last() - gc_duration_delta - sample_interval;\n+    const double additional_time_for_allocations = stats._young_stats._cycle._time_since_last - gc_duration_delta;\n@@ -159,3 +134,3 @@\n-    log_debug(gc, director)(\"Select GC Workers (Try Lowering), \"\n-                           \"AvoidLongGCWorkers: %.3f, AvoidOOMGCWorkers: %.3f, NextAvoidOOMGCWorkers: %.3f, LastGCWorkers: %.3f, GCWorkers: %.3f\",\n-                            avoid_long_gc_workers, avoid_oom_gc_workers, next_avoid_oom_gc_workers, (double)last_gc_workers, try_lowering_gc_workers);\n+    log_debug(gc, director)(\"Select Minor GC Workers (Try Lowering), \"\n+                            \"AvoidOOMGCWorkers: %.3f, NextAvoidOOMGCWorkers: %.3f, LastGCWorkers: %.3f, GCWorkers: %.3f\",\n+                            gc_workers, next_avoid_oom_gc_workers, last_gc_workers, try_lowering_gc_workers);\n@@ -165,3 +140,3 @@\n-  log_debug(gc, director)(\"Select GC Workers (Normal), \"\n-                         \"AvoidLongGCWorkers: %.3f, AvoidOOMGCWorkers: %.3f, LastGCWorkers: %.3f, GCWorkers: %.3f\",\n-                         avoid_long_gc_workers, avoid_oom_gc_workers, (double)last_gc_workers, gc_workers);\n+  log_debug(gc, director)(\"Select Minor GC Workers (Normal), \"\n+                          \"AvoidOOMGCWorkers: %.3f, LastGCWorkers: %.3f, GCWorkers: %.3f\",\n+                          gc_workers, last_gc_workers, gc_workers);\n@@ -171,2 +146,6 @@\n-ZDriverRequest rule_allocation_rate_dynamic() {\n-  if (!ZStatCycle::is_time_trustable()) {\n+ZDriverRequest rule_minor_allocation_rate_dynamic(const ZDirectorStats& stats,\n+                                                  double serial_gc_time_passed,\n+                                                  double parallel_gc_time_passed,\n+                                                  bool conservative_alloc_rate,\n+                                                  size_t capacity) {\n+  if (!stats._old_stats._cycle._is_time_trustable) {\n@@ -174,1 +153,1 @@\n-    return GCCause::_no_gc;\n+    return ZDriverRequest(GCCause::_no_gc, ZYoungGCThreads, 0);\n@@ -179,3 +158,2 @@\n-  const size_t soft_max_capacity = ZHeap::heap()->soft_max_capacity();\n-  const size_t used = ZHeap::heap()->used();\n-  const size_t free_including_headroom = soft_max_capacity - MIN2(soft_max_capacity, used);\n+  const size_t used = stats._heap._used;\n+  const size_t free_including_headroom = capacity - MIN2(capacity, used);\n@@ -190,3 +168,4 @@\n-  const double alloc_rate_predict = ZStatAllocRate::predict();\n-  const double alloc_rate_avg = ZStatAllocRate::avg();\n-  const double alloc_rate_sd = ZStatAllocRate::sd();\n+  const ZStatMutatorAllocRateStats alloc_rate_stats = stats._mutator_alloc_rate;\n+  const double alloc_rate_predict = alloc_rate_stats._predict;\n+  const double alloc_rate_avg = alloc_rate_stats._avg;\n+  const double alloc_rate_sd = alloc_rate_stats._sd;\n@@ -194,1 +173,2 @@\n-  const double alloc_rate = (MAX2(alloc_rate_predict, alloc_rate_avg) * ZAllocationSpikeTolerance) + (alloc_rate_sd * one_in_1000) + 1.0;\n+  const double alloc_rate_conservative = (MAX2(alloc_rate_predict, alloc_rate_avg) * ZAllocationSpikeTolerance) + (alloc_rate_sd * one_in_1000) + 1.0;\n+  const double alloc_rate = conservative_alloc_rate ? alloc_rate_conservative : alloc_rate_stats._avg;\n@@ -199,2 +179,2 @@\n-  const double serial_gc_time = ZStatCycle::serial_time().davg() + (ZStatCycle::serial_time().dsd() * one_in_1000);\n-  const double parallelizable_gc_time = ZStatCycle::parallelizable_time().davg() + (ZStatCycle::parallelizable_time().dsd() * one_in_1000);\n+  const double serial_gc_time = fabsd(stats._young_stats._cycle._avg_serial_time + (stats._young_stats._cycle._sd_serial_time * one_in_1000) - serial_gc_time_passed);\n+  const double parallelizable_gc_time = fabsd(stats._young_stats._cycle._avg_parallelizable_time + (stats._young_stats._cycle._sd_parallelizable_time * one_in_1000) - parallel_gc_time_passed);\n@@ -203,1 +183,1 @@\n-  const double gc_workers = select_gc_workers(serial_gc_time, parallelizable_gc_time, alloc_rate_sd_percent, time_until_oom);\n+  const double gc_workers = select_young_gc_workers(stats, serial_gc_time, parallelizable_gc_time, alloc_rate_sd_percent, time_until_oom);\n@@ -206,1 +186,1 @@\n-  const uint actual_gc_workers = discrete_gc_workers(gc_workers);\n+  const uint actual_gc_workers = discrete_young_gc_workers(gc_workers);\n@@ -210,1 +190,0 @@\n-  const uint last_gc_workers = ZStatCycle::last_active_workers();\n@@ -213,3 +192,1 @@\n-  \/\/ We also subtract the sample interval, so that we don't overshoot the\n-  \/\/ target time and end up starting the GC too late in the next interval.\n-  const double time_until_gc = time_until_oom - actual_gc_duration - sample_interval;\n+  const double time_until_gc = time_until_oom - actual_gc_duration;\n@@ -217,1 +194,1 @@\n-  log_debug(gc, director)(\"Rule: Allocation Rate (Dynamic GC Workers), \"\n+  log_debug(gc, director)(\"Rule Minor: Allocation Rate (Dynamic GC Workers), \"\n@@ -219,1 +196,1 @@\n-                          \"GCDuration: %.3fs, TimeUntilOOM: %.3fs, TimeUntilGC: %.3fs, GCWorkers: %u -> %u\",\n+                          \"GCDuration: %.3fs, TimeUntilOOM: %.3fs, TimeUntilGC: %.3fs, GCWorkers: %u\",\n@@ -227,1 +204,0 @@\n-                          last_gc_workers,\n@@ -230,2 +206,6 @@\n-  if (actual_gc_workers <= last_gc_workers && time_until_gc > 0) {\n-    return ZDriverRequest(GCCause::_no_gc, actual_gc_workers);\n+  \/\/ Bail out if we are not \"close\" to needing the GC to start yet, where\n+  \/\/ close is 5% of the time left until OOM. If we don't check that we\n+  \/\/ are \"close\", then the heuristics instead add more threads and we\n+  \/\/ end up not triggering GCs until we have the max number of threads.\n+  if (time_until_gc > time_until_oom * 0.05) {\n+    return ZDriverRequest(GCCause::_no_gc, actual_gc_workers, 0);\n@@ -234,1 +214,31 @@\n-  return ZDriverRequest(GCCause::_z_allocation_rate, actual_gc_workers);\n+  return ZDriverRequest(GCCause::_z_allocation_rate, actual_gc_workers, 0);\n+}\n+\n+ZDriverRequest rule_soft_minor_allocation_rate_dynamic(const ZDirectorStats& stats,\n+                                                       double serial_gc_time_passed,\n+                                                       double parallel_gc_time_passed) {\n+    return rule_minor_allocation_rate_dynamic(stats,\n+                                              0.0 \/* serial_gc_time_passed *\/,\n+                                              0.0 \/* parallel_gc_time_passed *\/,\n+                                              false \/* conservative_alloc_rate *\/,\n+                                              stats._heap._soft_max_heap_size \/* capacity *\/);\n+}\n+\n+ZDriverRequest rule_semi_hard_minor_allocation_rate_dynamic(const ZDirectorStats& stats,\n+                                                            double serial_gc_time_passed,\n+                                                            double parallel_gc_time_passed) {\n+  return rule_minor_allocation_rate_dynamic(stats,\n+                                            0.0 \/* serial_gc_time_passed *\/,\n+                                            0.0 \/* parallel_gc_time_passed *\/,\n+                                            false \/* conservative_alloc_rate *\/,\n+                                            ZHeap::heap()->max_capacity() \/* capacity *\/);\n+}\n+\n+ZDriverRequest rule_hard_minor_allocation_rate_dynamic(const ZDirectorStats& stats,\n+                                                       double serial_gc_time_passed,\n+                                                       double parallel_gc_time_passed) {\n+  return rule_minor_allocation_rate_dynamic(stats,\n+                                            0.0 \/* serial_gc_time_passed *\/,\n+                                            0.0 \/* parallel_gc_time_passed *\/,\n+                                            true \/* conservative_alloc_rate *\/,\n+                                            ZHeap::heap()->max_capacity() \/* capacity *\/);\n@@ -237,2 +247,2 @@\n-static ZDriverRequest rule_allocation_rate_static() {\n-  if (!ZStatCycle::is_time_trustable()) {\n+static bool rule_minor_allocation_rate_static(const ZDirectorStats& stats) {\n+  if (!stats._old_stats._cycle._is_time_trustable) {\n@@ -240,1 +250,1 @@\n-    return GCCause::_no_gc;\n+    return false;\n@@ -251,2 +261,2 @@\n-  const size_t soft_max_capacity = ZHeap::heap()->soft_max_capacity();\n-  const size_t used = ZHeap::heap()->used();\n+  const size_t soft_max_capacity = stats._heap._soft_max_heap_size;\n+  const size_t used = stats._heap._used;\n@@ -262,1 +272,2 @@\n-  const double max_alloc_rate = (ZStatAllocRate::avg() * ZAllocationSpikeTolerance) + (ZStatAllocRate::sd() * one_in_1000);\n+  const ZStatMutatorAllocRateStats alloc_rate_stats = stats._mutator_alloc_rate;\n+  const double max_alloc_rate = (alloc_rate_stats._avg * ZAllocationSpikeTolerance) + (alloc_rate_stats._sd * one_in_1000);\n@@ -267,2 +278,2 @@\n-  const double serial_gc_time = ZStatCycle::serial_time().davg() + (ZStatCycle::serial_time().dsd() * one_in_1000);\n-  const double parallelizable_gc_time = ZStatCycle::parallelizable_time().davg() + (ZStatCycle::parallelizable_time().dsd() * one_in_1000);\n+  const double serial_gc_time = stats._young_stats._cycle._avg_serial_time + (stats._young_stats._cycle._sd_serial_time * one_in_1000);\n+  const double parallelizable_gc_time = stats._young_stats._cycle._avg_parallelizable_time + (stats._young_stats._cycle._sd_parallelizable_time * one_in_1000);\n@@ -271,1 +282,1 @@\n-  const double gc_duration = serial_gc_time + (parallelizable_gc_time \/ ConcGCThreads);\n+  const double gc_duration = serial_gc_time + (parallelizable_gc_time \/ ZYoungGCThreads);\n@@ -276,1 +287,1 @@\n-  const double time_until_gc = time_until_oom - gc_duration - sample_interval;\n+  const double time_until_gc = time_until_oom - gc_duration;\n@@ -278,1 +289,1 @@\n-  log_debug(gc, director)(\"Rule: Allocation Rate (Static GC Workers), MaxAllocRate: %.1fMB\/s, Free: \" SIZE_FORMAT \"MB, GCDuration: %.3fs, TimeUntilGC: %.3fs\",\n+  log_debug(gc, director)(\"Rule Minor: Allocation Rate (Static GC Workers), MaxAllocRate: %.1fMB\/s, Free: \" SIZE_FORMAT \"MB, GCDuration: %.3fs, TimeUntilGC: %.3fs\",\n@@ -281,2 +292,28 @@\n-  if (time_until_gc > 0) {\n-    return GCCause::_no_gc;\n+  return time_until_gc <= 0;\n+}\n+\n+static bool is_young_small(const ZDirectorStats& stats) {\n+  \/\/ Calculate amount of freeable memory available.\n+  const size_t soft_max_capacity = stats._heap._soft_max_heap_size;\n+  const size_t young_used = stats._young_stats._general._used;\n+\n+  const double young_used_percent = percent_of(young_used, soft_max_capacity);\n+\n+  \/\/ If the freeable memory isn't even 5% of the heap, we can't expect to free up\n+  \/\/ all that much memory, so let's not even try - it will likely be a wasted effort\n+  \/\/ that takes away CPU power to the hopefullt more profitable major colelction.\n+  return young_used_percent <= 5.0;\n+}\n+\n+template <typename PrintFn = void(*)(size_t, double)>\n+static bool is_high_usage(const ZDirectorStats& stats, PrintFn* print_function = nullptr) {\n+  \/\/ Calculate amount of free memory available. Note that we take the\n+  \/\/ relocation headroom into account to avoid in-place relocation.\n+  const size_t soft_max_capacity = stats._heap._soft_max_heap_size;\n+  const size_t used = stats._heap._used;\n+  const size_t free_including_headroom = soft_max_capacity - MIN2(soft_max_capacity, used);\n+  const size_t free = free_including_headroom - MIN2(free_including_headroom, ZHeuristics::relocation_headroom());\n+  const double free_percent = percent_of(free, soft_max_capacity);\n+\n+  if (print_function != nullptr) {\n+    (*print_function)(free, free_percent);\n@@ -285,1 +322,6 @@\n-  return GCCause::_z_allocation_rate;\n+  \/\/ The heap has high usage if there is less than 5% free memory left\n+  return free_percent <= 5.0;\n+}\n+\n+static bool is_major_urgent(const ZDirectorStats& stats) {\n+  return is_young_small(stats) && is_high_usage(stats);\n@@ -288,1 +330,15 @@\n-static ZDriverRequest rule_allocation_rate() {\n+static bool rule_minor_allocation_rate(const ZDirectorStats& stats) {\n+  if (ZCollectionIntervalOnly) {\n+    \/\/ Rule disabled\n+    return false;\n+  }\n+\n+  if (ZHeap::heap()->is_alloc_stalling_for_old()) {\n+    \/\/ Don't collect young if we have threads stalled waiting for an old collection\n+    return false;\n+  }\n+\n+  if (is_young_small(stats)) {\n+    return false;\n+  }\n+\n@@ -290,3 +346,13 @@\n-    return rule_allocation_rate_dynamic();\n-  } else {\n-    return rule_allocation_rate_static();\n+    if (rule_soft_minor_allocation_rate_dynamic(stats,\n+                                                0.0 \/* serial_gc_time_passed *\/,\n+                                                0.0 \/* parallel_gc_time_passed *\/).cause() != GCCause::_no_gc) {\n+      return true;\n+    }\n+\n+    if (rule_hard_minor_allocation_rate_dynamic(stats,\n+                                                0.0 \/* serial_gc_time_passed *\/,\n+                                                0.0 \/* parallel_gc_time_passed *\/).cause() != GCCause::_no_gc) {\n+      return true;\n+    }\n+\n+    return false;\n@@ -294,0 +360,2 @@\n+\n+  return rule_minor_allocation_rate_static(stats);\n@@ -296,3 +364,12 @@\n-static ZDriverRequest rule_high_usage() {\n-  \/\/ Perform GC if the amount of free memory is 5% or less. This is a preventive\n-  \/\/ meassure in the case where the application has a very low allocation rate,\n+static bool rule_minor_high_usage(const ZDirectorStats& stats) {\n+  if (ZCollectionIntervalOnly) {\n+    \/\/ Rule disabled\n+    return false;\n+  }\n+\n+  if (is_young_small(stats)) {\n+    return false;\n+  }\n+\n+  \/\/ Perform GC if the amount of free memory is small. This is a preventive\n+  \/\/ measure in the case where the application has a very low allocation rate,\n@@ -303,4 +380,2 @@\n-  \/\/ Calculate amount of free memory available. Note that we take the\n-  \/\/ relocation headroom into account to avoid in-place relocation.\n-  const size_t soft_max_capacity = ZHeap::heap()->soft_max_capacity();\n-  const size_t used = ZHeap::heap()->used();\n+  const size_t soft_max_capacity = stats._heap._soft_max_heap_size;\n+  const size_t used = stats._heap._used;\n@@ -311,2 +386,4 @@\n-  log_debug(gc, director)(\"Rule: High Usage, Free: \" SIZE_FORMAT \"MB(%.1f%%)\",\n-                          free \/ M, free_percent);\n+  auto print_function = [&](size_t free, double free_percent) {\n+    log_debug(gc, director)(\"Rule Minor: High Usage, Free: \" SIZE_FORMAT \"MB(%.1f%%)\",\n+                            free \/ M, free_percent);\n+  };\n@@ -314,2 +391,9 @@\n-  if (free_percent > 5.0) {\n-    return GCCause::_no_gc;\n+  return is_high_usage(stats, &print_function);\n+}\n+\n+\/\/ Major GC rules\n+\n+static bool rule_major_timer(const ZDirectorStats& stats) {\n+  if (ZCollectionIntervalMajor <= 0) {\n+    \/\/ Rule disabled\n+    return false;\n@@ -318,1 +402,8 @@\n-  return GCCause::_z_high_usage;\n+  \/\/ Perform GC if timer has expired.\n+  const double time_since_last_gc = stats._old_stats._cycle._time_since_last;\n+  const double time_until_gc = ZCollectionIntervalMajor - time_since_last_gc;\n+\n+  log_debug(gc, director)(\"Rule Major: Timer, Interval: %.3fs, TimeUntilGC: %.3fs\",\n+                          ZCollectionIntervalMajor, time_until_gc);\n+\n+  return time_until_gc <= 0;\n@@ -321,2 +412,2 @@\n-static ZDriverRequest rule_proactive() {\n-  if (!ZProactive || !ZStatCycle::is_warm()) {\n+static bool rule_major_warmup(const ZDirectorStats& stats) {\n+  if (ZCollectionIntervalOnly) {\n@@ -324,1 +415,137 @@\n-    return GCCause::_no_gc;\n+    return false;\n+  }\n+\n+  if (stats._old_stats._cycle._is_warm) {\n+    \/\/ Rule disabled\n+    return false;\n+  }\n+\n+  \/\/ Perform GC if heap usage passes 10\/20\/30% and no other GC has been\n+  \/\/ performed yet. This allows us to get some early samples of the GC\n+  \/\/ duration, which is needed by the other rules.\n+  const size_t soft_max_capacity = stats._heap._soft_max_heap_size;\n+  const size_t used = stats._heap._used;\n+  const double used_threshold_percent = (stats._old_stats._cycle._nwarmup_cycles + 1) * 0.1;\n+  const size_t used_threshold = soft_max_capacity * used_threshold_percent;\n+\n+  log_debug(gc, director)(\"Rule Major: Warmup %.0f%%, Used: \" SIZE_FORMAT \"MB, UsedThreshold: \" SIZE_FORMAT \"MB\",\n+                          used_threshold_percent * 100, used \/ M, used_threshold \/ M);\n+\n+  return used >= used_threshold;\n+}\n+\n+static double gc_time(ZDirectorGenerationStats generation_stats) {\n+  \/\/ Calculate max serial\/parallel times of a generation GC cycle. The times are\n+  \/\/ moving averages, we add ~3.3 sigma to account for the variance.\n+  const double serial_gc_time = generation_stats._cycle._avg_serial_time + (generation_stats._cycle._sd_serial_time * one_in_1000);\n+  const double parallelizable_gc_time = generation_stats._cycle._avg_parallelizable_time + (generation_stats._cycle._sd_parallelizable_time * one_in_1000);\n+\n+  \/\/ Calculate young GC time and duration given number of GC workers needed.\n+  return serial_gc_time + parallelizable_gc_time;\n+}\n+\n+static double calculate_extra_young_gc_time(const ZDirectorStats& stats) {\n+  if (!stats._old_stats._cycle._is_time_trustable) {\n+    return 0.0;\n+  }\n+\n+  \/\/ Calculate amount of free memory available. Note that we take the\n+  \/\/ relocation headroom into account to avoid in-place relocation.\n+  const size_t old_used = stats._old_stats._general._used;\n+  const size_t old_live = stats._old_stats._stat_heap._live_at_mark_end;\n+  const size_t old_garbage = old_used - old_live;\n+\n+  const double young_gc_time = gc_time(stats._young_stats);\n+\n+  \/\/ Calculate how much memory young collections are predicted to free.\n+  const size_t reclaimed_per_young_gc = stats._young_stats._stat_heap._reclaimed_avg;\n+\n+  \/\/ Calculate current YC time and predicted YC time after an old collection.\n+  const double current_young_gc_time_per_bytes_freed = double(young_gc_time) \/ double(reclaimed_per_young_gc);\n+  const double potential_young_gc_time_per_bytes_freed = double(young_gc_time) \/ double(reclaimed_per_young_gc + old_garbage);\n+\n+  \/\/ Calculate extra time per young collection inflicted by *not* doing an\n+  \/\/ old collection that frees up memory in the old generation.\n+  const double extra_young_gc_time_per_bytes_freed = current_young_gc_time_per_bytes_freed - potential_young_gc_time_per_bytes_freed;\n+  const double extra_young_gc_time = extra_young_gc_time_per_bytes_freed * (reclaimed_per_young_gc + old_garbage);\n+\n+  return extra_young_gc_time;\n+}\n+\n+static bool rule_major_allocation_rate(const ZDirectorStats& stats) {\n+  if (!stats._old_stats._cycle._is_time_trustable) {\n+    \/\/ Rule disabled\n+    return false;\n+  }\n+\n+  \/\/ Calculate GC time.\n+  const double old_gc_time = gc_time(stats._old_stats);\n+  const double young_gc_time = gc_time(stats._young_stats);\n+\n+  \/\/ Calculate how much memory collections are predicted to free.\n+  const size_t reclaimed_per_young_gc = stats._young_stats._stat_heap._reclaimed_avg;\n+  const size_t reclaimed_per_old_gc = stats._old_stats._stat_heap._reclaimed_avg;\n+\n+  \/\/ Calculate the GC cost for each reclaimed byte\n+  const double current_young_gc_time_per_bytes_freed = double(young_gc_time) \/ double(reclaimed_per_young_gc);\n+  const double current_old_gc_time_per_bytes_freed = double(old_gc_time) \/ double(reclaimed_per_old_gc);\n+\n+  \/\/ Calculate extra time per young collection inflicted by *not* doing an\n+  \/\/ old collection that frees up memory in the old generation.\n+  const double extra_young_gc_time = calculate_extra_young_gc_time(stats);\n+\n+  \/\/ Doing an old collection makes subsequent young collections more efficient.\n+  \/\/ Calculate the number of young collections ahead that we will try to amortize\n+  \/\/ the cost of doing an old collection for.\n+  const int lookahead = stats._heap._total_collections - stats._old_stats._general._total_collections_at_start;\n+\n+  \/\/ Calculate extra young collection overhead predicted for a number of future\n+  \/\/ young collections, due to not freeing up memory in the old generation.\n+  const double extra_young_gc_time_for_lookahead = extra_young_gc_time * lookahead;\n+\n+  log_debug(gc, director)(\"Rule Major: Allocation Rate, ExtraYoungGCTime: %.3fs, OldGCTime: %.3fs, Lookahead: %d, ExtraYoungGCTimeForLookahead: %.3fs\",\n+                          extra_young_gc_time, old_gc_time, lookahead, extra_young_gc_time_for_lookahead);\n+\n+  \/\/ If we continue doing as many minor collections as we already did since the\n+  \/\/ last major collection (N), without doing a major collection, then the minor\n+  \/\/ GC effort of freeing up memory for another N cycles, plus the effort of doing,\n+  \/\/ a major GC combined, is lower compared to the extra GC overhead per minor\n+  \/\/ collection, freeing an equal amount of memory, at a higher GC frequency.\n+  \/\/ In other words, the cost for minor collections of not doing a major collection\n+  \/\/ will seemingly be greater than the cost of doing a major collection and getting\n+  \/\/ cheaper minor collections for a time to come.\n+  const bool can_amortize_time_cost = extra_young_gc_time_for_lookahead > old_gc_time;\n+\n+  \/\/ If the garbage is cheaper to reap in the old generation, then it makes sense\n+  \/\/ to upgrade minor collections to major collections.\n+  const bool old_garbage_is_cheaper = current_old_gc_time_per_bytes_freed < current_young_gc_time_per_bytes_freed;\n+\n+  return can_amortize_time_cost || old_garbage_is_cheaper || is_major_urgent(stats);\n+}\n+\n+static double calculate_young_to_old_worker_ratio(const ZDirectorStats& stats) {\n+  const double young_gc_time = gc_time(stats._young_stats);\n+  const double old_gc_time = gc_time(stats._old_stats);\n+  const size_t reclaimed_per_young_gc = stats._young_stats._stat_heap._reclaimed_avg;\n+  const size_t reclaimed_per_old_gc = stats._old_stats._stat_heap._reclaimed_avg;\n+  const double current_young_bytes_freed_per_gc_time = double(reclaimed_per_young_gc) \/ double(young_gc_time);\n+  const double current_old_bytes_freed_per_gc_time = double(reclaimed_per_old_gc) \/ double(old_gc_time);\n+  const double old_vs_young_efficiency_ratio = current_old_bytes_freed_per_gc_time \/ current_young_bytes_freed_per_gc_time;\n+\n+  return old_vs_young_efficiency_ratio;\n+}\n+\n+static bool rule_major_proactive(const ZDirectorStats& stats) {\n+  if (ZCollectionIntervalOnly) {\n+    \/\/ Rule disabled\n+    return false;\n+  }\n+\n+  if (!ZProactive) {\n+    \/\/ Rule disabled\n+    return false;\n+  }\n+\n+  if (!stats._old_stats._cycle._is_warm) {\n+    \/\/ Rule disabled\n+    return false;\n@@ -336,2 +563,2 @@\n-  const size_t used_after_last_gc = ZStatHeap::used_at_relocate_end();\n-  const size_t used_increase_threshold = ZHeap::heap()->soft_max_capacity() * 0.10; \/\/ 10%\n+  const size_t used_after_last_gc = stats._old_stats._stat_heap._used_at_relocate_end;\n+  const size_t used_increase_threshold = stats._heap._soft_max_heap_size * 0.10; \/\/ 10%\n@@ -339,2 +566,2 @@\n-  const size_t used = ZHeap::heap()->used();\n-  const double time_since_last_gc = ZStatCycle::time_since_last();\n+  const size_t used = stats._heap._used;\n+  const double time_since_last_gc = stats._old_stats._cycle._time_since_last;\n@@ -344,1 +571,1 @@\n-    log_debug(gc, director)(\"Rule: Proactive, UsedUntilEnabled: \" SIZE_FORMAT \"MB, TimeUntilEnabled: %.3fs\",\n+    log_debug(gc, director)(\"Rule Major: Proactive, UsedUntilEnabled: \" SIZE_FORMAT \"MB, TimeUntilEnabled: %.3fs\",\n@@ -347,1 +574,1 @@\n-    return GCCause::_no_gc;\n+    return false;\n@@ -352,3 +579,7 @@\n-  const double serial_gc_time = ZStatCycle::serial_time().davg() + (ZStatCycle::serial_time().dsd() * one_in_1000);\n-  const double parallelizable_gc_time = ZStatCycle::parallelizable_time().davg() + (ZStatCycle::parallelizable_time().dsd() * one_in_1000);\n-  const double gc_duration = serial_gc_time + (parallelizable_gc_time \/ ConcGCThreads);\n+  const double serial_old_gc_time = stats._old_stats._cycle._avg_serial_time + (stats._old_stats._cycle._sd_serial_time * one_in_1000);\n+  const double parallelizable_old_gc_time = stats._old_stats._cycle._avg_parallelizable_time + (stats._old_stats._cycle._sd_parallelizable_time * one_in_1000);\n+  const double serial_young_gc_time = stats._young_stats._cycle._avg_serial_time + (stats._young_stats._cycle._sd_serial_time * one_in_1000);\n+  const double parallelizable_young_gc_time = stats._young_stats._cycle._avg_parallelizable_time + (stats._young_stats._cycle._sd_parallelizable_time * one_in_1000);\n+  const double serial_gc_time = serial_old_gc_time + serial_young_gc_time;\n+  const double parallelizable_gc_time = parallelizable_old_gc_time + parallelizable_young_gc_time;\n+  const double gc_duration = serial_gc_time + parallelizable_gc_time;\n@@ -358,1 +589,1 @@\n-  log_debug(gc, director)(\"Rule: Proactive, AcceptableGCInterval: %.3fs, TimeSinceLastGC: %.3fs, TimeUntilGC: %.3fs\",\n+  log_debug(gc, director)(\"Rule Major: Proactive, AcceptableGCInterval: %.3fs, TimeSinceLastGC: %.3fs, TimeUntilGC: %.3fs\",\n@@ -361,1 +592,9 @@\n-  if (time_until_gc > 0) {\n+  return time_until_gc <= 0;\n+}\n+\n+static GCCause::Cause make_minor_gc_decision(const ZDirectorStats& stats) {\n+  if (ZDriver::minor()->is_busy()) {\n+    return GCCause::_no_gc;\n+  }\n+\n+  if (ZDriver::major()->is_busy() && !stats._old_stats._resize._is_active) {\n@@ -365,1 +604,13 @@\n-  return GCCause::_z_proactive;\n+  if (rule_minor_timer(stats)) {\n+    return GCCause::_z_timer;\n+  }\n+\n+  if (rule_minor_allocation_rate(stats)) {\n+    return GCCause::_z_allocation_rate;\n+  }\n+\n+  if (rule_minor_high_usage(stats)) {\n+    return GCCause::_z_high_usage;\n+  }\n+\n+  return GCCause::_no_gc;\n@@ -368,10 +619,44 @@\n-static ZDriverRequest make_gc_decision() {\n-  \/\/ List of rules\n-  using ZDirectorRule = ZDriverRequest (*)();\n-  const ZDirectorRule rules[] = {\n-    rule_allocation_stall,\n-    rule_warmup,\n-    rule_timer,\n-    rule_allocation_rate,\n-    rule_high_usage,\n-    rule_proactive,\n+static GCCause::Cause make_major_gc_decision(const ZDirectorStats& stats) {\n+  if (ZDriver::major()->is_busy()) {\n+    return GCCause::_no_gc;\n+  }\n+\n+  if (rule_major_timer(stats)) {\n+    return GCCause::_z_timer;\n+  }\n+\n+  if (rule_major_warmup(stats)) {\n+    return GCCause::_z_warmup;\n+  }\n+\n+  if (rule_major_proactive(stats)) {\n+    return GCCause::_z_proactive;\n+  }\n+\n+  return GCCause::_no_gc;\n+}\n+\n+static ZWorkerResizeStats sample_worker_resize_stats(ZStatCycleStats& cycle_stats, ZStatWorkersStats& worker_stats, ZWorkers* workers) {\n+  ZLocker<ZLock> locker(workers->resizing_lock());\n+\n+  if (!workers->is_active()) {\n+    \/\/ If the workers are not active, it isn't safe to read stats\n+    \/\/ from the stat_cycle, so return early.\n+    return {\n+      false, \/\/ _is_active\n+      0.0,   \/\/ _serial_gc_time_passed\n+      0.0,   \/\/ _parallel_gc_time_passed\n+      0      \/\/ _nworkers_current\n+    };\n+  }\n+\n+  const double parallel_gc_duration_passed = worker_stats._accumulated_duration;\n+  const double parallel_gc_time_passed = worker_stats._accumulated_time;\n+  const double serial_gc_time_passed = cycle_stats._duration_since_start - parallel_gc_duration_passed;\n+  const uint active_nworkers = workers->active_workers();\n+\n+  return {\n+    true,                    \/\/ _is_active\n+    serial_gc_time_passed,   \/\/ _serial_gc_time_passed\n+    parallel_gc_time_passed, \/\/ _parallel_gc_time_passed\n+    active_nworkers          \/\/ _nworkers_current\n@@ -379,0 +664,1 @@\n+}\n@@ -380,5 +666,44 @@\n-  \/\/ Execute rules\n-  for (size_t i = 0; i < ARRAY_SIZE(rules); i++) {\n-    const ZDriverRequest request = rules[i]();\n-    if (request.cause() != GCCause::_no_gc) {\n-      return request;\n+\/\/ Output information for select_worker_threads\n+struct ZWorkerCounts {\n+  uint _young_workers;\n+  uint _old_workers;\n+};\n+\n+enum class ZWorkerSelectionType {\n+  start_major,\n+  minor_during_old,\n+  normal\n+};\n+\n+static ZWorkerCounts select_worker_threads(const ZDirectorStats& stats, uint young_workers, ZWorkerSelectionType type) {\n+  const uint active_young_workers = stats._young_stats._resize._nworkers_current;\n+  const uint active_old_workers = stats._old_stats._resize._nworkers_current;\n+\n+  if (ZHeap::heap()->is_alloc_stalling()) {\n+    \/\/ Boost GC threads when stalling\n+    return {ZYoungGCThreads, ZOldGCThreads};\n+  } else if (active_young_workers + active_old_workers > ConcGCThreads) {\n+    \/\/ Threads are boosted, due to stalling recently; retain that boosting\n+    return {active_young_workers, active_old_workers};\n+  }\n+\n+  const double young_to_old_ratio = calculate_young_to_old_worker_ratio(stats);\n+  uint old_workers = clamp(uint(young_workers * young_to_old_ratio), 1u, ZOldGCThreads);\n+\n+  if (type != ZWorkerSelectionType::normal && old_workers + young_workers > ConcGCThreads) {\n+    \/\/ We need to somehow clamp the GC threads so the two generations don't exceed ConcGCThreads\n+    const double old_ratio = (young_to_old_ratio \/ (1.0 + young_to_old_ratio));\n+    const double young_ratio = 1.0 - old_ratio;\n+    const uint young_workers_clamped = clamp(uint(ConcGCThreads * young_ratio), 1u, ZYoungGCThreads);\n+    const uint old_workers_clamped = clamp(ConcGCThreads - young_workers_clamped, 1u, ZOldGCThreads);\n+\n+    if (type == ZWorkerSelectionType::start_major) {\n+      \/\/ Adjust down the old workers so the next minor during major will be less sad\n+      old_workers = old_workers_clamped;\n+      \/\/ Since collecting the old generation depends on the initial young collection\n+      \/\/ finishing, we don't want it to have fewer workers than the old generation.\n+      young_workers = MAX2(old_workers, young_workers);\n+    } else if (type == ZWorkerSelectionType::minor_during_old) {\n+      \/\/ Adjust young and old workers for minor during old to fit within ConcGCThreads\n+      young_workers = young_workers_clamped;\n+      old_workers = old_workers_clamped;\n@@ -388,1 +713,189 @@\n-  return GCCause::_no_gc;\n+  return {young_workers, old_workers};\n+}\n+\n+static void adjust_gc(const ZDirectorStats& stats) {\n+  if (!UseDynamicNumberOfGCThreads) {\n+    return;\n+  }\n+\n+  const ZWorkerResizeStats young_resize_stats = stats._young_stats._resize;\n+  const ZWorkerResizeStats old_resize_stats = stats._old_stats._resize;\n+\n+  if (!young_resize_stats._is_active) {\n+    \/\/ Young generation collection is not running. We only resize the number\n+    \/\/ of threads when the young generation is running. The number of threads\n+    \/\/ for the old generation is modelled as a ratio of the number of threads\n+    \/\/ needed in the young generation. If we don't need to GC the young generation\n+    \/\/ at all, then we don't have anything to scale with, and the allocation\n+    \/\/ pressure on the GC can't be that high. If it is, a minor collection will\n+    \/\/ start, and inform us how to scale the old threads.\n+    return;\n+  }\n+\n+  const ZDriverRequest request = rule_semi_hard_minor_allocation_rate_dynamic(stats,\n+                                                                              young_resize_stats._serial_gc_time_passed,\n+                                                                              young_resize_stats._parallel_gc_time_passed);\n+  if (request.cause() == GCCause::_no_gc) {\n+    \/\/ No urgency\n+    return;\n+  }\n+\n+  uint desired_young_workers = MAX2(request.young_nworkers(), young_resize_stats._nworkers_current);\n+\n+  if (desired_young_workers > young_resize_stats._nworkers_current) {\n+    \/\/ We need to increase workers\n+    const uint needed_young_increase = desired_young_workers - young_resize_stats._nworkers_current;\n+    \/\/ We want to increase by more than the minimum amount to ensure that\n+    \/\/ there are enough margins, but also to avoid too frequent resizing.\n+    const uint desired_young_increase = needed_young_increase * 2;\n+    desired_young_workers = MIN2(young_resize_stats._nworkers_current + desired_young_increase, ZYoungGCThreads);\n+  }\n+\n+  const uint young_current_workers = young_resize_stats._nworkers_current;\n+  const uint old_current_workers = old_resize_stats._nworkers_current;\n+\n+  const bool minor_during_old = old_resize_stats._is_active;\n+  ZWorkerSelectionType type = minor_during_old ? ZWorkerSelectionType::minor_during_old\n+                                               : ZWorkerSelectionType::normal;\n+\n+  const ZWorkerCounts selection = select_worker_threads(stats, desired_young_workers, type);\n+\n+  if (old_resize_stats._is_active && old_current_workers != selection._old_workers) {\n+    ZGeneration::old()->workers()->request_resize_workers(selection._old_workers);\n+  }\n+  if (young_current_workers != selection._young_workers) {\n+    ZGeneration::young()->workers()->request_resize_workers(selection._young_workers);\n+  }\n+}\n+\n+static ZWorkerCounts initial_workers(const ZDirectorStats& stats, ZWorkerSelectionType type) {\n+  if (!UseDynamicNumberOfGCThreads) {\n+    return {ZYoungGCThreads, ZOldGCThreads};\n+  }\n+\n+  const ZDriverRequest soft_request = rule_soft_minor_allocation_rate_dynamic(stats, 0.0 \/* serial_gc_time_passed *\/, 0.0 \/* parallel_gc_time_passed *\/);\n+  const ZDriverRequest hard_request = rule_hard_minor_allocation_rate_dynamic(stats, 0.0 \/* serial_gc_time_passed *\/, 0.0 \/* parallel_gc_time_passed *\/);\n+  const uint young_workers = MAX3(1u, soft_request.young_nworkers(), hard_request.young_nworkers());\n+\n+  return select_worker_threads(stats, young_workers, type);\n+}\n+\n+static void start_major_gc(const ZDirectorStats& stats, GCCause::Cause cause) {\n+  const ZWorkerCounts selection = initial_workers(stats, ZWorkerSelectionType::start_major);\n+  const ZDriverRequest request(cause, selection._young_workers, selection._old_workers);\n+  ZDriver::major()->collect(request);\n+}\n+\n+static void start_minor_gc(const ZDirectorStats& stats, GCCause::Cause cause) {\n+  const ZWorkerSelectionType type = ZDriver::major()->is_busy()\n+      ? ZWorkerSelectionType::minor_during_old\n+      : ZWorkerSelectionType::normal;\n+  const ZWorkerCounts selection = initial_workers(stats, type);\n+\n+  if (UseDynamicNumberOfGCThreads && ZDriver::major()->is_busy()) {\n+    const ZWorkerResizeStats old_resize_stats = stats._old_stats._resize;\n+    const uint old_current_workers = old_resize_stats._nworkers_current;\n+\n+    if (old_current_workers != selection._old_workers) {\n+      ZGeneration::old()->workers()->request_resize_workers(selection._old_workers);\n+    }\n+  }\n+\n+  const ZDriverRequest request(cause, selection._young_workers, 0);\n+  ZDriver::minor()->collect(request);\n+}\n+\n+static bool start_gc(const ZDirectorStats& stats) {\n+  \/\/ Try start major collections first as they include a minor collection\n+  const GCCause::Cause major_cause = make_major_gc_decision(stats);\n+  if (major_cause != GCCause::_no_gc) {\n+    start_major_gc(stats, major_cause);\n+    return true;\n+  }\n+\n+  const GCCause::Cause minor_cause = make_minor_gc_decision(stats);\n+  if (minor_cause != GCCause::_no_gc) {\n+    if (!ZDriver::major()->is_busy() && rule_major_allocation_rate(stats)) {\n+      \/\/ Merge minor GC into major GC\n+      start_major_gc(stats, GCCause::_z_allocation_rate);\n+    } else {\n+      start_minor_gc(stats, minor_cause);\n+    }\n+\n+    return true;\n+  }\n+\n+  return false;\n+}\n+\n+void ZDirector::evaluate_rules() {\n+  ZLocker<ZConditionLock> locker(&_director->_monitor);\n+  _director->_monitor.notify();\n+}\n+\n+bool ZDirector::wait_for_tick() {\n+  const uint64_t interval_ms = MILLIUNITS \/ decision_hz;\n+\n+  ZLocker<ZConditionLock> locker(&_monitor);\n+\n+  if (_stopped) {\n+    \/\/ Stopped\n+    return false;\n+  }\n+\n+  \/\/ Wait\n+  _monitor.wait(interval_ms);\n+  return true;\n+}\n+\n+static ZDirectorHeapStats sample_heap_stats() {\n+  const ZHeap* const heap = ZHeap::heap();\n+  const ZCollectedHeap* const collected_heap = ZCollectedHeap::heap();\n+  return {\n+    heap->soft_max_capacity(),\n+    heap->used(),\n+    collected_heap->total_collections()\n+  };\n+}\n+\n+\/\/ This function samples all the stat values used by the heuristics to compute what to do.\n+\/\/ This is where synchronization code goes to ensure that the values we read are valid.\n+static ZDirectorStats sample_stats() {\n+  ZGenerationYoung* young = ZGeneration::young();\n+  ZGenerationOld* old = ZGeneration::old();\n+  const ZStatMutatorAllocRateStats mutator_alloc_rate = ZStatMutatorAllocRate::stats();\n+  const ZDirectorHeapStats heap = sample_heap_stats();\n+\n+  ZStatCycleStats young_cycle = young->stat_cycle()->stats();\n+  ZStatCycleStats old_cycle = old->stat_cycle()->stats();\n+\n+  ZStatWorkersStats young_workers = young->stat_workers()->stats();\n+  ZStatWorkersStats old_workers = old->stat_workers()->stats();\n+\n+  ZWorkerResizeStats young_resize = sample_worker_resize_stats(young_cycle, young_workers, young->workers());\n+  ZWorkerResizeStats old_resize = sample_worker_resize_stats(old_cycle, old_workers, old->workers());\n+\n+  ZStatHeapStats young_stat_heap = young->stat_heap()->stats();\n+  ZStatHeapStats old_stat_heap = old->stat_heap()->stats();\n+\n+  ZDirectorGenerationGeneralStats young_generation = { ZHeap::heap()->used_young(), 0 };\n+  ZDirectorGenerationGeneralStats old_generation = { ZHeap::heap()->used_old(), old->total_collections_at_start() };\n+\n+  return {\n+    mutator_alloc_rate,\n+    heap,\n+    {\n+      young_cycle,\n+      young_workers,\n+      young_resize,\n+      young_stat_heap,\n+      young_generation\n+    },\n+    {\n+      old_cycle,\n+      old_workers,\n+      old_resize,\n+      old_stat_heap,\n+      old_generation\n+    }\n+  };\n@@ -391,1 +904,1 @@\n-void ZDirector::run_service() {\n+void ZDirector::run_thread() {\n@@ -393,7 +906,4 @@\n-  while (_metronome.wait_for_tick()) {\n-    sample_allocation_rate();\n-    if (!_driver->is_busy()) {\n-      const ZDriverRequest request = make_gc_decision();\n-      if (request.cause() != GCCause::_no_gc) {\n-        _driver->collect(request);\n-      }\n+  while (wait_for_tick()) {\n+    ZDirectorStats stats = sample_stats();\n+    if (!start_gc(stats)) {\n+      adjust_gc(stats);\n@@ -404,2 +914,4 @@\n-void ZDirector::stop_service() {\n-  _metronome.stop();\n+void ZDirector::terminate() {\n+  ZLocker<ZConditionLock> locker(&_monitor);\n+  _stopped = true;\n+  _monitor.notify();\n","filename":"src\/hotspot\/share\/gc\/z\/zDirector.cpp","additions":702,"deletions":190,"binary":false,"changes":892,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,2 +27,2 @@\n-#include \"gc\/shared\/concurrentGCThread.hpp\"\n-#include \"gc\/z\/zMetronome.hpp\"\n+#include \"gc\/z\/zLock.hpp\"\n+#include \"gc\/z\/zThread.hpp\"\n@@ -30,3 +30,1 @@\n-class ZDriver;\n-\n-class ZDirector : public ConcurrentGCThread {\n+class ZDirector : public ZThread {\n@@ -34,2 +32,7 @@\n-  ZDriver* const _driver;\n-  ZMetronome     _metronome;\n+  static const uint64_t decision_hz = 100;\n+  static ZDirector* _director;\n+\n+  ZConditionLock _monitor;\n+  bool           _stopped;\n+\n+  bool wait_for_tick();\n@@ -38,2 +41,2 @@\n-  virtual void run_service();\n-  virtual void stop_service();\n+  virtual void run_thread();\n+  virtual void terminate();\n@@ -42,1 +45,3 @@\n-  ZDirector(ZDriver* driver);\n+  ZDirector();\n+\n+  static void evaluate_rules();\n","filename":"src\/hotspot\/share\/gc\/z\/zDirector.hpp","additions":16,"deletions":11,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -25,0 +25,2 @@\n+#include \"gc\/shared\/gc_globals.hpp\"\n+#include \"gc\/shared\/gcCause.hpp\"\n@@ -26,3 +28,0 @@\n-#include \"gc\/shared\/gcLocker.hpp\"\n-#include \"gc\/shared\/gcVMOperations.hpp\"\n-#include \"gc\/shared\/isGCActiveMark.hpp\"\n@@ -32,0 +31,1 @@\n+#include \"gc\/z\/zDirector.hpp\"\n@@ -33,0 +33,2 @@\n+#include \"gc\/z\/zGCIdPrinter.hpp\"\n+#include \"gc\/z\/zGeneration.inline.hpp\"\n@@ -34,1 +36,1 @@\n-#include \"gc\/z\/zMessagePort.inline.hpp\"\n+#include \"gc\/z\/zLock.inline.hpp\"\n@@ -37,6 +39,0 @@\n-#include \"gc\/z\/zVerify.hpp\"\n-#include \"logging\/log.hpp\"\n-#include \"memory\/universe.hpp\"\n-#include \"runtime\/threads.hpp\"\n-#include \"runtime\/vmOperations.hpp\"\n-#include \"runtime\/vmThread.hpp\"\n@@ -44,13 +40,2 @@\n-static const ZStatPhaseCycle      ZPhaseCycle(\"Garbage Collection Cycle\");\n-static const ZStatPhasePause      ZPhasePauseMarkStart(\"Pause Mark Start\");\n-static const ZStatPhaseConcurrent ZPhaseConcurrentMark(\"Concurrent Mark\");\n-static const ZStatPhaseConcurrent ZPhaseConcurrentMarkContinue(\"Concurrent Mark Continue\");\n-static const ZStatPhaseConcurrent ZPhaseConcurrentMarkFree(\"Concurrent Mark Free\");\n-static const ZStatPhasePause      ZPhasePauseMarkEnd(\"Pause Mark End\");\n-static const ZStatPhaseConcurrent ZPhaseConcurrentProcessNonStrongReferences(\"Concurrent Process Non-Strong References\");\n-static const ZStatPhaseConcurrent ZPhaseConcurrentResetRelocationSet(\"Concurrent Reset Relocation Set\");\n-static const ZStatPhaseConcurrent ZPhaseConcurrentSelectRelocationSet(\"Concurrent Select Relocation Set\");\n-static const ZStatPhasePause      ZPhasePauseRelocateStart(\"Pause Relocate Start\");\n-static const ZStatPhaseConcurrent ZPhaseConcurrentRelocated(\"Concurrent Relocate\");\n-static const ZStatCriticalPhase   ZCriticalPhaseGCLockerStall(\"GC Locker Stall\", false \/* verbose *\/);\n-static const ZStatSampler         ZSamplerJavaThreads(\"System\", \"Java Threads\", ZStatUnitThreads);\n+static const ZStatPhaseCollection ZPhaseCollectionMinor(\"Minor Collection\", true \/* minor *\/);\n+static const ZStatPhaseCollection ZPhaseCollectionMajor(\"Major Collection\", false \/* minor *\/);\n@@ -58,2 +43,4 @@\n-ZDriverRequest::ZDriverRequest() :\n-    ZDriverRequest(GCCause::_no_gc) {}\n+template <typename DriverT>\n+class ZGCCauseSetter : public GCCauseSetter {\n+private:\n+  DriverT* _driver;\n@@ -61,2 +48,6 @@\n-ZDriverRequest::ZDriverRequest(GCCause::Cause cause) :\n-    ZDriverRequest(cause, ConcGCThreads) {}\n+public:\n+  ZGCCauseSetter(DriverT* driver, GCCause::Cause cause) :\n+      GCCauseSetter(ZCollectedHeap::heap(), cause),\n+      _driver(driver) {\n+    _driver->set_gc_cause(cause);\n+  }\n@@ -64,3 +55,8 @@\n-ZDriverRequest::ZDriverRequest(GCCause::Cause cause, uint nworkers) :\n-    _cause(cause),\n-    _nworkers(nworkers) {}\n+  ~ZGCCauseSetter() {\n+    _driver->set_gc_cause(GCCause::_no_gc);\n+  }\n+};\n+\n+ZLock*        ZDriver::_lock;\n+ZDriverMinor* ZDriver::_minor;\n+ZDriverMajor* ZDriver::_major;\n@@ -68,2 +64,2 @@\n-bool ZDriverRequest::operator==(const ZDriverRequest& other) const {\n-  return _cause == other._cause;\n+void ZDriver::initialize() {\n+  _lock = new ZLock();\n@@ -72,2 +68,2 @@\n-GCCause::Cause ZDriverRequest::cause() const {\n-  return _cause;\n+void ZDriver::lock() {\n+  _lock->lock();\n@@ -76,2 +72,2 @@\n-uint ZDriverRequest::nworkers() const {\n-  return _nworkers;\n+void ZDriver::unlock() {\n+  _lock->unlock();\n@@ -80,5 +76,3 @@\n-class VM_ZOperation : public VM_Operation {\n-private:\n-  const uint _gc_id;\n-  bool       _gc_locked;\n-  bool       _success;\n+void ZDriver::set_minor(ZDriverMinor* minor) {\n+  _minor = minor;\n+}\n@@ -86,12 +80,3 @@\n-public:\n-  VM_ZOperation() :\n-      _gc_id(GCId::current()),\n-      _gc_locked(false),\n-      _success(false) {}\n-\n-  virtual bool needs_inactive_gc_locker() const {\n-    \/\/ An inactive GC locker is needed in operations where we change the bad\n-    \/\/ mask or move objects. Changing the bad mask will invalidate all oops,\n-    \/\/ which makes it conceptually the same thing as moving all objects.\n-    return false;\n-  }\n+void ZDriver::set_major(ZDriverMajor* major) {\n+  _major = major;\n+}\n@@ -99,3 +84,3 @@\n-  virtual bool skip_thread_oop_barriers() const {\n-    return true;\n-  }\n+ZDriverMinor* ZDriver::minor() {\n+  return _minor;\n+}\n@@ -103,1 +88,3 @@\n-  virtual bool do_operation() = 0;\n+ZDriverMajor* ZDriver::major() {\n+  return _major;\n+}\n@@ -105,4 +92,3 @@\n-  virtual bool doit_prologue() {\n-    Heap_lock->lock();\n-    return true;\n-  }\n+ZDriverLocker::ZDriverLocker() {\n+  ZDriver::lock();\n+}\n@@ -110,6 +96,3 @@\n-  virtual void doit() {\n-    \/\/ Abort if GC locker state is incompatible\n-    if (needs_inactive_gc_locker() && GCLocker::check_active_before_gc()) {\n-      _gc_locked = true;\n-      return;\n-    }\n+ZDriverLocker::~ZDriverLocker() {\n+  ZDriver::unlock();\n+}\n@@ -117,3 +100,3 @@\n-    \/\/ Setup GC id and active marker\n-    GCIdMark gc_id_mark(_gc_id);\n-    IsGCActiveMark gc_active_mark;\n+ZDriverUnlocker::ZDriverUnlocker() {\n+  ZDriver::unlock();\n+}\n@@ -121,2 +104,3 @@\n-    \/\/ Verify before operation\n-    ZVerify::before_zoperation();\n+ZDriverUnlocker::~ZDriverUnlocker() {\n+  ZDriver::lock();\n+}\n@@ -124,2 +108,3 @@\n-    \/\/ Execute operation\n-    _success = do_operation();\n+ZDriver::ZDriver() :\n+    _gc_cause(GCCause::_no_gc) {\n+}\n@@ -127,3 +112,3 @@\n-    \/\/ Update statistics\n-    ZStatSample(ZSamplerJavaThreads, Threads::number_of_threads());\n-  }\n+void ZDriver::set_gc_cause(GCCause::Cause cause) {\n+  _gc_cause = cause;\n+}\n@@ -131,3 +116,3 @@\n-  virtual void doit_epilogue() {\n-    Heap_lock->unlock();\n-  }\n+GCCause::Cause ZDriver::gc_cause() {\n+  return _gc_cause;\n+}\n@@ -135,3 +120,14 @@\n-  bool gc_locked() const {\n-    return _gc_locked;\n-  }\n+ZDriverMinor::ZDriverMinor() :\n+    ZDriver(),\n+    _port(),\n+    _gc_timer(),\n+    _jfr_tracer(),\n+    _used_at_start() {\n+  ZDriver::set_minor(this);\n+  set_name(\"ZDriverMinor\");\n+  create_and_start();\n+}\n+\n+bool ZDriverMinor::is_busy() const {\n+  return _port.is_busy();\n+}\n@@ -139,2 +135,19 @@\n-  bool success() const {\n-    return _success;\n+void ZDriverMinor::collect(const ZDriverRequest& request) {\n+  switch (request.cause()) {\n+  case GCCause::_wb_young_gc:\n+    \/\/ Start synchronous GC\n+    _port.send_sync(request);\n+    break;\n+\n+  case GCCause::_scavenge_alot:\n+  case GCCause::_z_timer:\n+  case GCCause::_z_allocation_rate:\n+  case GCCause::_z_allocation_stall:\n+  case GCCause::_z_high_usage:\n+    \/\/ Start asynchronous GC\n+    _port.send_async(request);\n+    break;\n+\n+  default:\n+    fatal(\"Unsupported GC cause (%s)\", GCCause::to_string(request.cause()));\n+    break;\n@@ -144,5 +157,3 @@\n-class VM_ZMarkStart : public VM_ZOperation {\n-public:\n-  virtual VMOp_Type type() const {\n-    return VMOp_ZMarkStart;\n-  }\n+GCTracer* ZDriverMinor::jfr_tracer() {\n+  return &_jfr_tracer;\n+}\n@@ -150,3 +161,3 @@\n-  virtual bool needs_inactive_gc_locker() const {\n-    return true;\n-  }\n+void ZDriverMinor::set_used_at_start(size_t used) {\n+  _used_at_start = used;\n+}\n@@ -154,3 +165,3 @@\n-  virtual bool do_operation() {\n-    ZStatTimer timer(ZPhasePauseMarkStart);\n-    ZServiceabilityPauseTracer tracer;\n+size_t ZDriverMinor::used_at_start() const {\n+  return _used_at_start;\n+}\n@@ -158,1 +169,7 @@\n-    ZCollectedHeap::heap()->increment_total_collections(true \/* full *\/);\n+class ZDriverScopeMinor : public StackObj {\n+private:\n+  GCIdMark                     _gc_id;\n+  GCCause::Cause               _gc_cause;\n+  ZGCCauseSetter<ZDriverMinor> _gc_cause_setter;\n+  ZStatTimer                   _stat_timer;\n+  ZServiceabilityCycleTracer   _tracer;\n@@ -160,2 +177,9 @@\n-    ZHeap::heap()->mark_start();\n-    return true;\n+public:\n+  ZDriverScopeMinor(const ZDriverRequest& request, ConcurrentGCTimer* gc_timer) :\n+      _gc_id(),\n+      _gc_cause(request.cause()),\n+      _gc_cause_setter(ZDriver::minor(), _gc_cause),\n+      _stat_timer(ZPhaseCollectionMinor, gc_timer),\n+      _tracer(true \/* minor *\/) {\n+    \/\/ Select number of worker threads to use\n+    ZGeneration::young()->set_active_workers(request.young_nworkers());\n@@ -165,5 +189,5 @@\n-class VM_ZMarkEnd : public VM_ZOperation {\n-public:\n-  virtual VMOp_Type type() const {\n-    return VMOp_ZMarkEnd;\n-  }\n+void ZDriverMinor::gc(const ZDriverRequest& request) {\n+  ZDriverScopeMinor scope(request, &_gc_timer);\n+  ZGCIdMinor minor_id(gc_id());\n+  ZGeneration::young()->collect(ZYoungType::minor, &_gc_timer);\n+}\n@@ -171,6 +195,3 @@\n-  virtual bool do_operation() {\n-    ZStatTimer timer(ZPhasePauseMarkEnd);\n-    ZServiceabilityPauseTracer tracer;\n-    return ZHeap::heap()->mark_end();\n-  }\n-};\n+static void handle_alloc_stalling_for_young() {\n+  ZHeap::heap()->handle_alloc_stalling_for_young();\n+}\n@@ -178,4 +199,27 @@\n-class VM_ZRelocateStart : public VM_ZOperation {\n-public:\n-  virtual VMOp_Type type() const {\n-    return VMOp_ZRelocateStart;\n+void ZDriverMinor::handle_alloc_stalls() const {\n+  handle_alloc_stalling_for_young();\n+}\n+\n+void ZDriverMinor::run_thread() {\n+  \/\/ Main loop\n+  for (;;) {\n+    \/\/ Wait for GC request\n+    const ZDriverRequest request = _port.receive();\n+\n+    ZDriverLocker locker;\n+\n+    abortpoint();\n+\n+    \/\/ Run GC\n+    gc(request);\n+\n+    abortpoint();\n+\n+    \/\/ Notify GC completed\n+    _port.ack();\n+\n+    \/\/ Handle allocation stalls\n+    handle_alloc_stalls();\n+\n+    \/\/ Good point to consider back-to-back GC\n+    ZDirector::evaluate_rules();\n@@ -183,0 +227,6 @@\n+}\n+\n+void ZDriverMinor::terminate() {\n+  const ZDriverRequest request(GCCause::_no_gc, 0, 0);\n+  _port.send_async(request);\n+}\n@@ -184,1 +234,6 @@\n-  virtual bool needs_inactive_gc_locker() const {\n+static bool should_clear_soft_references(GCCause::Cause cause) {\n+  \/\/ Clear soft references if implied by the GC cause\n+  switch (cause) {\n+  case GCCause::_wb_full_gc:\n+  case GCCause::_metadata_GC_clear_soft_refs:\n+  case GCCause::_z_allocation_stall:\n@@ -186,0 +241,20 @@\n+\n+  case GCCause::_heap_dump:\n+  case GCCause::_heap_inspection:\n+  case GCCause::_wb_breakpoint:\n+  case GCCause::_dcmd_gc_run:\n+  case GCCause::_java_lang_system_gc:\n+  case GCCause::_full_gc_alot:\n+  case GCCause::_jvmti_force_gc:\n+  case GCCause::_z_timer:\n+  case GCCause::_z_warmup:\n+  case GCCause::_z_allocation_rate:\n+  case GCCause::_z_proactive:\n+  case GCCause::_metadata_GC_threshold:\n+  case GCCause::_codecache_GC_threshold:\n+  case GCCause::_codecache_GC_aggressive:\n+    break;\n+\n+  default:\n+    fatal(\"Unsupported GC cause (%s)\", GCCause::to_string(cause));\n+    break;\n@@ -188,4 +263,2 @@\n-  virtual bool do_operation() {\n-    ZStatTimer timer(ZPhasePauseRelocateStart);\n-    ZServiceabilityPauseTracer tracer;\n-    ZHeap::heap()->relocate_start();\n+  \/\/ Clear soft references if threads are stalled waiting for an old collection\n+  if (ZHeap::heap()->is_alloc_stalling_for_old()) {\n@@ -194,1 +267,0 @@\n-};\n@@ -196,5 +268,3 @@\n-class VM_ZVerify : public VM_Operation {\n-public:\n-  virtual VMOp_Type type() const {\n-    return VMOp_ZVerify;\n-  }\n+  \/\/ Don't clear\n+  return false;\n+}\n@@ -202,1 +272,13 @@\n-  virtual bool skip_thread_oop_barriers() const {\n+static bool should_preclean_young(GCCause::Cause cause) {\n+  \/\/ Preclean young if implied by the GC cause\n+  switch (cause) {\n+  case GCCause::_heap_dump:\n+  case GCCause::_heap_inspection:\n+  case GCCause::_wb_full_gc:\n+  case GCCause::_wb_breakpoint:\n+  case GCCause::_dcmd_gc_run:\n+  case GCCause::_java_lang_system_gc:\n+  case GCCause::_full_gc_alot:\n+  case GCCause::_jvmti_force_gc:\n+  case GCCause::_metadata_GC_clear_soft_refs:\n+  case GCCause::_z_allocation_stall:\n@@ -204,0 +286,13 @@\n+\n+  case GCCause::_z_timer:\n+  case GCCause::_z_warmup:\n+  case GCCause::_z_allocation_rate:\n+  case GCCause::_z_proactive:\n+  case GCCause::_metadata_GC_threshold:\n+  case GCCause::_codecache_GC_threshold:\n+  case GCCause::_codecache_GC_aggressive:\n+    break;\n+\n+  default:\n+    fatal(\"Unsupported GC cause (%s)\", GCCause::to_string(cause));\n+    break;\n@@ -206,2 +301,3 @@\n-  virtual void doit() {\n-    ZVerify::after_weak_processing();\n+  \/\/ Preclean young if threads are stalled waiting for an old collection\n+  if (ZHeap::heap()->is_alloc_stalling_for_old()) {\n+    return true;\n@@ -209,1 +305,0 @@\n-};\n@@ -211,4 +306,12 @@\n-ZDriver::ZDriver() :\n-    _gc_cycle_port(),\n-    _gc_locker_port() {\n-  set_name(\"ZDriver\");\n+  \/\/ Preclean young if implied by configuration\n+  return ScavengeBeforeFullGC;\n+}\n+\n+ZDriverMajor::ZDriverMajor() :\n+    ZDriver(),\n+    _port(),\n+    _gc_timer(),\n+    _jfr_tracer(),\n+    _used_at_start() {\n+  ZDriver::set_major(this);\n+  set_name(\"ZDriverMajor\");\n@@ -218,2 +321,2 @@\n-bool ZDriver::is_busy() const {\n-  return _gc_cycle_port.is_busy();\n+bool ZDriverMajor::is_busy() const {\n+  return _port.is_busy();\n@@ -222,1 +325,1 @@\n-void ZDriver::collect(const ZDriverRequest& request) {\n+void ZDriverMajor::collect(const ZDriverRequest& request) {\n@@ -224,1 +327,2 @@\n-  case GCCause::_wb_young_gc:\n+  case GCCause::_heap_dump:\n+  case GCCause::_heap_inspection:\n@@ -229,1 +333,0 @@\n-  case GCCause::_scavenge_alot:\n@@ -234,1 +337,1 @@\n-    _gc_cycle_port.send_sync(request);\n+    _port.send_sync(request);\n@@ -242,1 +345,0 @@\n-  case GCCause::_z_high_usage:\n@@ -246,6 +348,1 @@\n-    _gc_cycle_port.send_async(request);\n-    break;\n-\n-  case GCCause::_gc_locker:\n-    \/\/ Restart VM operation previously blocked by the GC locker\n-    _gc_locker_port.signal();\n+    _port.send_async(request);\n@@ -256,1 +353,1 @@\n-    _gc_cycle_port.send_async(request);\n+    _port.send_async(request);\n@@ -260,1 +357,0 @@\n-    \/\/ Other causes not supported\n@@ -266,21 +362,2 @@\n-template <typename T>\n-bool ZDriver::pause() {\n-  for (;;) {\n-    T op;\n-    VMThread::execute(&op);\n-    if (op.gc_locked()) {\n-      \/\/ Wait for GC to become unlocked and restart the VM operation\n-      ZStatTimer timer(ZCriticalPhaseGCLockerStall);\n-      _gc_locker_port.wait();\n-      continue;\n-    }\n-\n-    \/\/ Notify VM operation completed\n-    _gc_locker_port.ack();\n-\n-    return op.success();\n-  }\n-}\n-\n-void ZDriver::pause_mark_start() {\n-  pause<VM_ZMarkStart>();\n+GCTracer* ZDriverMajor::jfr_tracer() {\n+  return &_jfr_tracer;\n@@ -289,5 +366,2 @@\n-void ZDriver::concurrent_mark() {\n-  ZStatTimer timer(ZPhaseConcurrentMark);\n-  ZBreakpoint::at_after_marking_started();\n-  ZHeap::heap()->mark(true \/* initial *\/);\n-  ZBreakpoint::at_before_marking_completed();\n+void ZDriverMajor::set_used_at_start(size_t used) {\n+  _used_at_start = used;\n@@ -296,2 +370,2 @@\n-bool ZDriver::pause_mark_end() {\n-  return pause<VM_ZMarkEnd>();\n+size_t ZDriverMajor::used_at_start() const {\n+  return _used_at_start;\n@@ -300,91 +374,1 @@\n-void ZDriver::concurrent_mark_continue() {\n-  ZStatTimer timer(ZPhaseConcurrentMarkContinue);\n-  ZHeap::heap()->mark(false \/* initial *\/);\n-}\n-\n-void ZDriver::concurrent_mark_free() {\n-  ZStatTimer timer(ZPhaseConcurrentMarkFree);\n-  ZHeap::heap()->mark_free();\n-}\n-\n-void ZDriver::concurrent_process_non_strong_references() {\n-  ZStatTimer timer(ZPhaseConcurrentProcessNonStrongReferences);\n-  ZBreakpoint::at_after_reference_processing_started();\n-  ZHeap::heap()->process_non_strong_references();\n-}\n-\n-void ZDriver::concurrent_reset_relocation_set() {\n-  ZStatTimer timer(ZPhaseConcurrentResetRelocationSet);\n-  ZHeap::heap()->reset_relocation_set();\n-}\n-\n-void ZDriver::pause_verify() {\n-  if (ZVerifyRoots || ZVerifyObjects) {\n-    VM_ZVerify op;\n-    VMThread::execute(&op);\n-  }\n-}\n-\n-void ZDriver::concurrent_select_relocation_set() {\n-  ZStatTimer timer(ZPhaseConcurrentSelectRelocationSet);\n-  ZHeap::heap()->select_relocation_set();\n-}\n-\n-void ZDriver::pause_relocate_start() {\n-  pause<VM_ZRelocateStart>();\n-}\n-\n-void ZDriver::concurrent_relocate() {\n-  ZStatTimer timer(ZPhaseConcurrentRelocated);\n-  ZHeap::heap()->relocate();\n-}\n-\n-void ZDriver::check_out_of_memory() {\n-  ZHeap::heap()->check_out_of_memory();\n-}\n-\n-static bool should_clear_soft_references(const ZDriverRequest& request) {\n-  \/\/ Clear soft references if implied by the GC cause\n-  if (request.cause() == GCCause::_wb_full_gc ||\n-      request.cause() == GCCause::_metadata_GC_clear_soft_refs ||\n-      request.cause() == GCCause::_z_allocation_stall) {\n-    \/\/ Clear\n-    return true;\n-  }\n-\n-  \/\/ Don't clear\n-  return false;\n-}\n-\n-static uint select_active_worker_threads_dynamic(const ZDriverRequest& request) {\n-  \/\/ Use requested number of worker threads\n-  return request.nworkers();\n-}\n-\n-static uint select_active_worker_threads_static(const ZDriverRequest& request) {\n-  const GCCause::Cause cause = request.cause();\n-  const uint nworkers = request.nworkers();\n-\n-  \/\/ Boost number of worker threads if implied by the GC cause\n-  if (cause == GCCause::_wb_full_gc ||\n-      cause == GCCause::_java_lang_system_gc ||\n-      cause == GCCause::_metadata_GC_clear_soft_refs ||\n-      cause == GCCause::_z_allocation_stall) {\n-    \/\/ Boost\n-    const uint boosted_nworkers = MAX2(nworkers, ParallelGCThreads);\n-    return boosted_nworkers;\n-  }\n-\n-  \/\/ Use requested number of worker threads\n-  return nworkers;\n-}\n-\n-static uint select_active_worker_threads(const ZDriverRequest& request) {\n-  if (UseDynamicNumberOfGCThreads) {\n-    return select_active_worker_threads_dynamic(request);\n-  } else {\n-    return select_active_worker_threads_static(request);\n-  }\n-}\n-\n-class ZDriverGCScope : public StackObj {\n+class ZDriverScopeMajor : public StackObj {\n@@ -392,5 +376,5 @@\n-  GCIdMark                   _gc_id;\n-  GCCause::Cause             _gc_cause;\n-  GCCauseSetter              _gc_cause_setter;\n-  ZStatTimer                 _timer;\n-  ZServiceabilityCycleTracer _tracer;\n+  GCIdMark                     _gc_id;\n+  GCCause::Cause               _gc_cause;\n+  ZGCCauseSetter<ZDriverMajor> _gc_cause_setter;\n+  ZStatTimer                   _stat_timer;\n+  ZServiceabilityCycleTracer   _tracer;\n@@ -399,1 +383,1 @@\n-  ZDriverGCScope(const ZDriverRequest& request) :\n+  ZDriverScopeMajor(const ZDriverRequest& request, ConcurrentGCTimer* gc_timer) :\n@@ -402,6 +386,3 @@\n-      _gc_cause_setter(ZCollectedHeap::heap(), _gc_cause),\n-      _timer(ZPhaseCycle),\n-      _tracer() {\n-    \/\/ Update statistics\n-    ZStatCycle::at_start();\n-\n+      _gc_cause_setter(ZDriver::major(), _gc_cause),\n+      _stat_timer(ZPhaseCollectionMajor, gc_timer),\n+      _tracer(false \/* minor *\/) {\n@@ -409,2 +390,2 @@\n-    const bool clear = should_clear_soft_references(request);\n-    ZHeap::heap()->set_soft_reference_policy(clear);\n+    const bool clear = should_clear_soft_references(request.cause());\n+    ZGeneration::old()->set_soft_reference_policy(clear);\n@@ -413,2 +394,2 @@\n-    const uint nworkers = select_active_worker_threads(request);\n-    ZHeap::heap()->set_active_workers(nworkers);\n+    ZGeneration::young()->set_active_workers(request.young_nworkers());\n+    ZGeneration::old()->set_active_workers(request.old_nworkers());\n@@ -417,4 +398,1 @@\n-  ~ZDriverGCScope() {\n-    \/\/ Update statistics\n-    ZStatCycle::at_end(_gc_cause, ZHeap::heap()->active_workers());\n-\n+  ~ZDriverScopeMajor() {\n@@ -422,1 +400,1 @@\n-    Universe::heap()->update_capacity_and_used_at_gc();\n+    ZCollectedHeap::heap()->update_capacity_and_used_at_gc();\n@@ -425,1 +403,1 @@\n-    Universe::heap()->record_whole_heap_examined_timestamp();\n+    ZCollectedHeap::heap()->record_whole_heap_examined_timestamp();\n@@ -429,26 +407,13 @@\n-\/\/ Macro to execute a termination check after a concurrent phase. Note\n-\/\/ that it's important that the termination check comes after the call\n-\/\/ to the function f, since we can't abort between pause_relocate_start()\n-\/\/ and concurrent_relocate(). We need to let concurrent_relocate() call\n-\/\/ abort_page() on the remaining entries in the relocation set.\n-#define concurrent(f)                 \\\n-  do {                                \\\n-    concurrent_##f();                 \\\n-    if (should_terminate()) {         \\\n-      return;                         \\\n-    }                                 \\\n-  } while (false)\n-\n-void ZDriver::gc(const ZDriverRequest& request) {\n-  ZDriverGCScope scope(request);\n-\n-  \/\/ Phase 1: Pause Mark Start\n-  pause_mark_start();\n-\n-  \/\/ Phase 2: Concurrent Mark\n-  concurrent(mark);\n-\n-  \/\/ Phase 3: Pause Mark End\n-  while (!pause_mark_end()) {\n-    \/\/ Phase 3.5: Concurrent Mark Continue\n-    concurrent(mark_continue);\n+void ZDriverMajor::collect_young(const ZDriverRequest& request) {\n+  ZGCIdMajor major_id(gc_id(), 'Y');\n+  if (should_preclean_young(request.cause())) {\n+    \/\/ Collect young generation and promote everything to old generation\n+    ZGeneration::young()->collect(ZYoungType::major_full_preclean, &_gc_timer);\n+\n+    abortpoint();\n+\n+    \/\/ Collect young generation and gather roots pointing into old generation\n+    ZGeneration::young()->collect(ZYoungType::major_full_roots, &_gc_timer);\n+  } else {\n+    \/\/ Collect young generation and gather roots pointing into old generation\n+    ZGeneration::young()->collect(ZYoungType::major_partial_roots, &_gc_timer);\n@@ -457,2 +422,10 @@\n-  \/\/ Phase 4: Concurrent Mark Free\n-  concurrent(mark_free);\n+  abortpoint();\n+\n+  \/\/ Handle allocations waiting for a young collection\n+  handle_alloc_stalling_for_young();\n+}\n+\n+void ZDriverMajor::collect_old() {\n+  ZGCIdMajor major_id(gc_id(), 'O');\n+  ZGeneration::old()->collect(&_gc_timer);\n+}\n@@ -460,2 +433,2 @@\n-  \/\/ Phase 5: Concurrent Process Non-Strong References\n-  concurrent(process_non_strong_references);\n+void ZDriverMajor::gc(const ZDriverRequest& request) {\n+  ZDriverScopeMajor scope(request, &_gc_timer);\n@@ -463,2 +436,2 @@\n-  \/\/ Phase 6: Concurrent Reset Relocation Set\n-  concurrent(reset_relocation_set);\n+  \/\/ Collect the young generation\n+  collect_young(request);\n@@ -466,2 +439,1 @@\n-  \/\/ Phase 7: Pause Verify\n-  pause_verify();\n+  abortpoint();\n@@ -469,2 +441,3 @@\n-  \/\/ Phase 8: Concurrent Select Relocation Set\n-  concurrent(select_relocation_set);\n+  \/\/ Collect the old generation\n+  collect_old();\n+}\n@@ -472,2 +445,3 @@\n-  \/\/ Phase 9: Pause Relocate Start\n-  pause_relocate_start();\n+static void handle_alloc_stalling_for_old() {\n+  ZHeap::heap()->handle_alloc_stalling_for_old();\n+}\n@@ -475,2 +449,2 @@\n-  \/\/ Phase 10: Concurrent Relocate\n-  concurrent(relocate);\n+void ZDriverMajor::handle_alloc_stalls() const {\n+  handle_alloc_stalling_for_old();\n@@ -479,1 +453,1 @@\n-void ZDriver::run_service() {\n+void ZDriverMajor::run_thread() {\n@@ -481,1 +455,1 @@\n-  while (!should_terminate()) {\n+  for (;;) {\n@@ -483,4 +457,3 @@\n-    const ZDriverRequest request = _gc_cycle_port.receive();\n-    if (request.cause() == GCCause::_no_gc) {\n-      continue;\n-    }\n+    const ZDriverRequest request = _port.receive();\n+\n+    ZDriverLocker locker;\n@@ -490,0 +463,2 @@\n+    abortpoint();\n+\n@@ -493,4 +468,1 @@\n-    if (should_terminate()) {\n-      \/\/ Abort\n-      break;\n-    }\n+    abortpoint();\n@@ -499,1 +471,1 @@\n-    _gc_cycle_port.ack();\n+    _port.ack();\n@@ -501,2 +473,2 @@\n-    \/\/ Check for out of memory condition\n-    check_out_of_memory();\n+    \/\/ Handle allocation stalls\n+    handle_alloc_stalls();\n@@ -508,3 +480,3 @@\n-void ZDriver::stop_service() {\n-  ZAbort::abort();\n-  _gc_cycle_port.send_async(GCCause::_no_gc);\n+void ZDriverMajor::terminate() {\n+  const ZDriverRequest request(GCCause::_no_gc, 0, 0);\n+  _port.send_async(request);\n","filename":"src\/hotspot\/share\/gc\/z\/zDriver.cpp","additions":317,"deletions":345,"binary":false,"changes":662,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,3 +27,4 @@\n-#include \"gc\/shared\/concurrentGCThread.hpp\"\n-#include \"gc\/shared\/gcCause.hpp\"\n-#include \"gc\/z\/zMessagePort.hpp\"\n+#include \"gc\/shared\/gcTimer.hpp\"\n+#include \"gc\/z\/zDriverPort.hpp\"\n+#include \"gc\/z\/zThread.hpp\"\n+#include \"gc\/z\/zTracer.hpp\"\n@@ -32,0 +33,7 @@\n+class ZDriverMinor;\n+class ZDriverMajor;\n+class ZLock;\n+\n+class ZDriver : public ZThread {\n+  friend class ZDriverLocker;\n+  friend class ZDriverUnlocker;\n@@ -33,1 +41,0 @@\n-class ZDriverRequest {\n@@ -35,2 +42,8 @@\n-  GCCause::Cause _cause;\n-  uint           _nworkers;\n+  static ZLock*        _lock;\n+  static ZDriverMinor* _minor;\n+  static ZDriverMajor* _major;\n+\n+  GCCause::Cause _gc_cause;\n+\n+  static void lock();\n+  static void unlock();\n@@ -39,3 +52,4 @@\n-  ZDriverRequest();\n-  ZDriverRequest(GCCause::Cause cause);\n-  ZDriverRequest(GCCause::Cause cause, uint nworkers);\n+  static void initialize();\n+\n+  static void set_minor(ZDriverMinor* minor);\n+  static void set_major(ZDriverMajor* major);\n@@ -43,1 +57,2 @@\n-  bool operator==(const ZDriverRequest& other) const;\n+  static ZDriverMinor* minor();\n+  static ZDriverMajor* major();\n@@ -45,2 +60,4 @@\n-  GCCause::Cause cause() const;\n-  uint nworkers() const;\n+  ZDriver();\n+\n+  void set_gc_cause(GCCause::Cause cause);\n+  GCCause::Cause gc_cause();\n@@ -49,1 +66,1 @@\n-class ZDriver : public ConcurrentGCThread {\n+class ZDriverMinor : public ZDriver {\n@@ -51,2 +68,16 @@\n-  ZMessagePort<ZDriverRequest> _gc_cycle_port;\n-  ZRendezvousPort              _gc_locker_port;\n+  ZDriverPort       _port;\n+  ConcurrentGCTimer _gc_timer;\n+  ZMinorTracer      _jfr_tracer;\n+  size_t            _used_at_start;\n+\n+  void gc(const ZDriverRequest& request);\n+  void handle_alloc_stalls() const;\n+\n+protected:\n+  virtual void run_thread();\n+  virtual void terminate();\n+\n+public:\n+  ZDriverMinor();\n+\n+  bool is_busy() const;\n@@ -54,1 +85,3 @@\n-  template <typename T> bool pause();\n+  void collect(const ZDriverRequest& request);\n+\n+  GCTracer* jfr_tracer();\n@@ -56,11 +89,3 @@\n-  void pause_mark_start();\n-  void concurrent_mark();\n-  bool pause_mark_end();\n-  void concurrent_mark_continue();\n-  void concurrent_mark_free();\n-  void concurrent_process_non_strong_references();\n-  void concurrent_reset_relocation_set();\n-  void pause_verify();\n-  void concurrent_select_relocation_set();\n-  void pause_relocate_start();\n-  void concurrent_relocate();\n+  void set_used_at_start(size_t used);\n+  size_t used_at_start() const;\n+};\n@@ -68,1 +93,6 @@\n-  void check_out_of_memory();\n+class ZDriverMajor : public ZDriver {\n+private:\n+  ZDriverPort       _port;\n+  ConcurrentGCTimer _gc_timer;\n+  ZMajorTracer      _jfr_tracer;\n+  size_t            _used_at_start;\n@@ -70,0 +100,3 @@\n+  void collect_young(const ZDriverRequest& request);\n+\n+  void collect_old();\n@@ -71,0 +104,1 @@\n+  void handle_alloc_stalls() const;\n@@ -73,2 +107,2 @@\n-  virtual void run_service();\n-  virtual void stop_service();\n+  virtual void run_thread();\n+  virtual void terminate();\n@@ -77,1 +111,1 @@\n-  ZDriver();\n+  ZDriverMajor();\n@@ -82,0 +116,17 @@\n+\n+  GCTracer* jfr_tracer();\n+\n+  void set_used_at_start(size_t used);\n+  size_t used_at_start() const;\n+};\n+\n+class ZDriverLocker : public StackObj {\n+public:\n+  ZDriverLocker();\n+  ~ZDriverLocker();\n+};\n+\n+class ZDriverUnlocker : public StackObj {\n+public:\n+  ZDriverUnlocker();\n+  ~ZDriverUnlocker();\n","filename":"src\/hotspot\/share\/gc\/z\/zDriver.hpp","additions":83,"deletions":32,"binary":false,"changes":115,"status":"modified"},{"patch":"@@ -0,0 +1,185 @@\n+\/*\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/z\/zDriverPort.hpp\"\n+#include \"gc\/z\/zFuture.inline.hpp\"\n+#include \"gc\/z\/zList.inline.hpp\"\n+#include \"gc\/z\/zLock.inline.hpp\"\n+#include \"utilities\/debug.hpp\"\n+\n+ZDriverRequest::ZDriverRequest() :\n+    ZDriverRequest(GCCause::_no_gc, 0, 0) {}\n+\n+ZDriverRequest::ZDriverRequest(GCCause::Cause cause, uint young_nworkers, uint old_nworkers) :\n+    _cause(cause),\n+    _young_nworkers(young_nworkers),\n+    _old_nworkers(old_nworkers) {}\n+\n+bool ZDriverRequest::operator==(const ZDriverRequest& other) const {\n+  return _cause == other._cause;\n+}\n+\n+GCCause::Cause ZDriverRequest::cause() const {\n+  return _cause;\n+}\n+\n+uint ZDriverRequest::young_nworkers() const {\n+  return _young_nworkers;\n+}\n+\n+uint ZDriverRequest::old_nworkers() const {\n+  return _old_nworkers;\n+}\n+\n+class ZDriverPortEntry {\n+  friend class ZList<ZDriverPortEntry>;\n+\n+private:\n+  const ZDriverRequest        _message;\n+  uint64_t                    _seqnum;\n+  ZFuture<ZDriverRequest>     _result;\n+  ZListNode<ZDriverPortEntry> _node;\n+\n+public:\n+  ZDriverPortEntry(const ZDriverRequest& message) :\n+      _message(message),\n+      _seqnum(0) {}\n+\n+  void set_seqnum(uint64_t seqnum) {\n+    _seqnum = seqnum;\n+  }\n+\n+  uint64_t seqnum() const {\n+    return _seqnum;\n+  }\n+\n+  ZDriverRequest message() const {\n+    return _message;\n+  }\n+\n+  void wait() {\n+    const ZDriverRequest message = _result.get();\n+    assert(message == _message, \"Message mismatch\");\n+  }\n+\n+  void satisfy(const ZDriverRequest& message) {\n+    _result.set(message);\n+  }\n+};\n+\n+ZDriverPort::ZDriverPort() :\n+    _lock(),\n+    _has_message(false),\n+    _seqnum(0),\n+    _queue() {}\n+\n+bool ZDriverPort::is_busy() const {\n+  ZLocker<ZConditionLock> locker(&_lock);\n+  return _has_message;\n+}\n+\n+void ZDriverPort::send_sync(const ZDriverRequest& message) {\n+  ZDriverPortEntry entry(message);\n+\n+  {\n+    \/\/ Enqueue message\n+    ZLocker<ZConditionLock> locker(&_lock);\n+    entry.set_seqnum(_seqnum);\n+    _queue.insert_last(&entry);\n+    _lock.notify();\n+  }\n+\n+  \/\/ Wait for completion\n+  entry.wait();\n+\n+  {\n+    \/\/ Guard deletion of underlying semaphore. This is a workaround for a\n+    \/\/ bug in sem_post() in glibc < 2.21, where it's not safe to destroy\n+    \/\/ the semaphore immediately after returning from sem_wait(). The\n+    \/\/ reason is that sem_post() can touch the semaphore after a waiting\n+    \/\/ thread have returned from sem_wait(). To avoid this race we are\n+    \/\/ forcing the waiting thread to acquire\/release the lock held by the\n+    \/\/ posting thread. https:\/\/sourceware.org\/bugzilla\/show_bug.cgi?id=12674\n+    ZLocker<ZConditionLock> locker(&_lock);\n+  }\n+}\n+\n+void ZDriverPort::send_async(const ZDriverRequest& message) {\n+  ZLocker<ZConditionLock> locker(&_lock);\n+  if (!_has_message) {\n+    \/\/ Post message\n+    _message = message;\n+    _has_message = true;\n+    _lock.notify();\n+  }\n+}\n+\n+ZDriverRequest ZDriverPort::receive() {\n+  ZLocker<ZConditionLock> locker(&_lock);\n+\n+  \/\/ Wait for message\n+  while (!_has_message && _queue.is_empty()) {\n+    _lock.wait();\n+  }\n+\n+  \/\/ Increment request sequence number\n+  _seqnum++;\n+\n+  if (!_has_message) {\n+    \/\/ Message available in the queue\n+    _message = _queue.first()->message();\n+    _has_message = true;\n+  }\n+\n+  return _message;\n+}\n+\n+void ZDriverPort::ack() {\n+  ZLocker<ZConditionLock> locker(&_lock);\n+\n+  if (!_has_message) {\n+    \/\/ Nothing to ack\n+    return;\n+  }\n+\n+  \/\/ Satisfy requests (and duplicates) in queue\n+  ZListIterator<ZDriverPortEntry> iter(&_queue);\n+  for (ZDriverPortEntry* entry; iter.next(&entry);) {\n+    if (entry->message() == _message && entry->seqnum() < _seqnum) {\n+      \/\/ Dequeue and satisfy request. Note that the dequeue operation must\n+      \/\/ happen first, since the request will immediately be deallocated\n+      \/\/ once it has been satisfied.\n+      _queue.remove(entry);\n+      entry->satisfy(_message);\n+    }\n+  }\n+\n+  if (_queue.is_empty()) {\n+    \/\/ Queue is empty\n+    _has_message = false;\n+  } else {\n+    \/\/ Post first message in queue\n+    _message = _queue.first()->message();\n+  }\n+}\n","filename":"src\/hotspot\/share\/gc\/z\/zDriverPort.cpp","additions":185,"deletions":0,"binary":false,"changes":185,"status":"added"},{"patch":"@@ -0,0 +1,72 @@\n+\/*\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_Z_ZDRIVERPORT_HPP\n+#define SHARE_GC_Z_ZDRIVERPORT_HPP\n+\n+#include \"gc\/shared\/gcCause.hpp\"\n+#include \"gc\/z\/zList.hpp\"\n+#include \"gc\/z\/zLock.hpp\"\n+\n+class ZDriverPortEntry;\n+\n+class ZDriverRequest {\n+private:\n+  GCCause::Cause _cause;\n+  uint           _young_nworkers;\n+  uint           _old_nworkers;\n+\n+public:\n+  ZDriverRequest();\n+  ZDriverRequest(GCCause::Cause cause, uint young_nworkers, uint old_nworkers);\n+\n+  bool operator==(const ZDriverRequest& other) const;\n+\n+  GCCause::Cause cause() const;\n+  uint young_nworkers() const;\n+  uint old_nworkers() const;\n+};\n+\n+class ZDriverPort {\n+private:\n+  mutable ZConditionLock  _lock;\n+  bool                    _has_message;\n+  ZDriverRequest          _message;\n+  uint64_t                _seqnum;\n+  ZList<ZDriverPortEntry> _queue;\n+\n+public:\n+  ZDriverPort();\n+\n+  bool is_busy() const;\n+\n+  \/\/ For use by sender\n+  void send_sync(const ZDriverRequest& request);\n+  void send_async(const ZDriverRequest& request);\n+\n+  \/\/ For use by receiver\n+  ZDriverRequest receive();\n+  void ack();\n+};\n+\n+#endif \/\/ SHARE_GC_Z_ZDRIVERPORT_HPP\n","filename":"src\/hotspot\/share\/gc\/z\/zDriverPort.hpp","additions":72,"deletions":0,"binary":false,"changes":72,"status":"added"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -25,0 +25,1 @@\n+#include \"gc\/shared\/gcLogPrecious.hpp\"\n@@ -26,0 +27,1 @@\n+#include \"gc\/z\/zCollectedHeap.hpp\"\n@@ -27,0 +29,1 @@\n+#include \"gc\/z\/zPage.inline.hpp\"\n@@ -29,0 +32,2 @@\n+#include \"logging\/log.hpp\"\n+#include \"runtime\/atomic.hpp\"\n@@ -48,1 +53,34 @@\n-static const ZStatCriticalPhase ZCriticalPhaseRelocationStall(\"Relocation Stall\");\n+bool ZForwarding::claim() {\n+  return Atomic::cmpxchg(&_claimed, false, true) == false;\n+}\n+\n+void ZForwarding::in_place_relocation_start(zoffset relocated_watermark) {\n+  _page->log_msg(\" In-place reloc start  - relocated to: \" PTR_FORMAT, untype(relocated_watermark));\n+\n+  _in_place = true;\n+\n+  \/\/ Support for ZHeap::is_in checks of from-space objects\n+  \/\/ in a page that is in-place relocating\n+  Atomic::store(&_in_place_thread, Thread::current());\n+  _in_place_top_at_start = _page->top();\n+}\n+\n+void ZForwarding::in_place_relocation_finish() {\n+  assert(_in_place, \"Must be an in-place relocated page\");\n+\n+  _page->log_msg(\" In-place reloc finish - top at start: \" PTR_FORMAT, untype(_in_place_top_at_start));\n+\n+  if (_from_age == ZPageAge::old || _to_age != ZPageAge::old) {\n+    \/\/ Only do this for non-promoted pages, that still need to reset live map.\n+    \/\/ Done with iterating over the \"from-page\" view, so can now drop the _livemap.\n+    _page->finalize_reset_for_in_place_relocation();\n+  }\n+\n+  \/\/ Disable relaxed ZHeap::is_in checks\n+  Atomic::store(&_in_place_thread, (Thread*)nullptr);\n+}\n+\n+bool ZForwarding::in_place_relocation_is_below_top_at_start(zoffset offset) const {\n+  \/\/ Only the relocating thread is allowed to know about the old relocation top.\n+  return Atomic::load(&_in_place_thread) == Thread::current() && offset < _in_place_top_at_start;\n+}\n@@ -50,1 +88,1 @@\n-bool ZForwarding::retain_page() {\n+bool ZForwarding::retain_page(ZRelocateQueue* queue) {\n@@ -61,2 +99,3 @@\n-      const bool success = wait_page_released();\n-      assert(success, \"Should always succeed\");\n+      queue->add_and_wait(this);\n+\n+      \/\/ Released\n@@ -73,1 +112,1 @@\n-ZPage* ZForwarding::claim_page() {\n+void ZForwarding::in_place_relocation_claim_page() {\n@@ -92,1 +131,2 @@\n-    return _page;\n+    \/\/ Done\n+    break;\n@@ -133,1 +173,2 @@\n-bool ZForwarding::wait_page_released() const {\n+ZPage* ZForwarding::detach_page() {\n+  \/\/ Wait until released\n@@ -135,1 +176,0 @@\n-    ZStatTimer timer(ZCriticalPhaseRelocationStall);\n@@ -138,4 +178,0 @@\n-      if (_ref_abort) {\n-        return false;\n-      }\n-\n@@ -146,1 +182,1 @@\n-  return true;\n+  return _page;\n@@ -149,7 +185,168 @@\n-ZPage* ZForwarding::detach_page() {\n-  \/\/ Wait until released\n-  if (Atomic::load_acquire(&_ref_count) != 0) {\n-    ZLocker<ZConditionLock> locker(&_ref_lock);\n-    while (Atomic::load_acquire(&_ref_count) != 0) {\n-      _ref_lock.wait();\n-    }\n+ZPage* ZForwarding::page() {\n+  assert(Atomic::load(&_ref_count) != 0, \"The page has been released\/detached\");\n+  return _page;\n+}\n+\n+void ZForwarding::mark_done() {\n+  Atomic::store(&_done, true);\n+}\n+\n+bool ZForwarding::is_done() const {\n+  return Atomic::load(&_done);\n+}\n+\n+\/\/\n+\/\/ The relocated_remembered_fields are used when the old generation\n+\/\/ collection is relocating objects, concurrently with the young\n+\/\/ generation collection's remembered set scanning for the marking.\n+\/\/\n+\/\/ When the OC is relocating objects, the old remembered set bits\n+\/\/ for the from-space objects need to be moved over to the to-space\n+\/\/ objects.\n+\/\/\n+\/\/ The YC doesn't want to wait for the OC, so it eagerly helps relocating\n+\/\/ objects with remembered set bits, so that it can perform marking on the\n+\/\/ to-space copy of the object fields that are associated with the remembered\n+\/\/ set bits.\n+\/\/\n+\/\/ This requires some synchronization between the OC and YC, and this is\n+\/\/ mainly done via the _relocated_remembered_fields_state in each ZForwarding.\n+\/\/ The values corresponds to:\n+\/\/\n+\/\/ none:      Starting state - neither OC nor YC has stated their intentions\n+\/\/ published: The OC has completed relocating all objects, and published an array\n+\/\/            of all to-space fields that should have a remembered set entry.\n+\/\/ reject:    The OC relocation of the page happened concurrently with the YC\n+\/\/            remset scanning. Two situations:\n+\/\/            a) The page had not been released yet: The YC eagerly relocated and\n+\/\/            scanned the to-space objects with remset entries.\n+\/\/            b) The page had been released: The YC accepts the array published in\n+\/\/            (published).\n+\/\/ accept:    The YC found that the forwarding\/page had already been relocated when\n+\/\/            the YC started.\n+\/\/\n+\/\/ Central to this logic is the ZRemembered::scan_forwarding function, where\n+\/\/ the YC tries to \"retain\" the forwarding\/page. If it succeeds it means that\n+\/\/ the OC has not finished (or maybe not even started) the relocation of all objects.\n+\/\/\n+\/\/ When the YC manages to retaining the page it will bring the state from:\n+\/\/  none      -> reject - Started collecting remembered set info\n+\/\/  published -> reject - Rejected the OC's remembered set info\n+\/\/  reject    -> reject - An earlier YC had already handled the remembered set info\n+\/\/  accept    ->        - Invalid state - will not happen\n+\/\/\n+\/\/ When the YC fails to retain the page the state transitions are:\n+\/\/ none      -> x - The page was relocated before the YC started\n+\/\/ published -> x - The OC completed relocation before YC visited this forwarding.\n+\/\/                  The YC will use the remembered set info collected by the OC.\n+\/\/ reject    -> x - A previous YC has already handled the remembered set info\n+\/\/ accept    -> x - See above\n+\/\/\n+\/\/ x is:\n+\/\/  reject        - if the relocation finished while the current YC was running\n+\/\/  accept        - if the relocation finished before the current YC started\n+\/\/\n+\/\/ Note the subtlety that even though the relocation could released the page\n+\/\/ and made it non-retainable, the relocation code might not have gotten to\n+\/\/ the point where the page is removed from the page table. It could also be\n+\/\/ the case that the relocated page became in-place relocated, and we therefore\n+\/\/ shouldn't be scanning it this YC.\n+\/\/\n+\/\/ The (reject) state is the \"dangerous\" state, where both OC and YC work on\n+\/\/ the same forwarding\/page somewhat concurrently. While (accept) denotes that\n+\/\/ that the entire relocation of a page (including freeing\/reusing it) was\n+\/\/ completed before the current YC started.\n+\/\/\n+\/\/ After all remset entries of relocated objects have been scanned, the code\n+\/\/ proceeds to visit all pages in the page table, to scan all pages not part\n+\/\/ of the OC relocation set. Pages with virtual addresses that doesn't match\n+\/\/ any of the once in the OC relocation set will be visited. Pages with\n+\/\/ virtual address that *do* have a corresponding forwarding entry has two\n+\/\/ cases:\n+\/\/\n+\/\/ a) The forwarding entry is marked with (reject). This means that the\n+\/\/    corresponding page is guaranteed to be one that has been relocated by the\n+\/\/    current OC during the active YC. Any remset entry is guaranteed to have\n+\/\/    already been scanned by the scan_forwarding code.\n+\/\/\n+\/\/ b) The forwarding entry is marked with (accept). This means that the page was\n+\/\/    *not* created by the OC relocation during this YC, which means that the\n+\/\/    page must be scanned.\n+\/\/\n+\n+void ZForwarding::relocated_remembered_fields_after_relocate() {\n+  assert(from_age() == ZPageAge::old, \"Only old pages have remsets\");\n+\n+  _relocated_remembered_fields_publish_young_seqnum = ZGeneration::young()->seqnum();\n+\n+  if (ZGeneration::young()->is_phase_mark()) {\n+    relocated_remembered_fields_publish();\n+  }\n+}\n+\n+void ZForwarding::relocated_remembered_fields_publish() {\n+  \/\/ The OC has relocated all objects and collected all fields that\n+  \/\/ used to have remembered set entries. Now publish the fields to\n+  \/\/ the YC.\n+\n+  const ZPublishState res = Atomic::cmpxchg(&_relocated_remembered_fields_state, ZPublishState::none, ZPublishState::published);\n+\n+  \/\/ none:      OK to publish\n+  \/\/ published: Not possible - this operation makes this transition\n+  \/\/ reject:    YC started scanning the \"from\" page concurrently and rejects the fields\n+  \/\/            the OC collected.\n+  \/\/ accept:    YC accepted the fields published by this function - not possible\n+  \/\/            because they weren't published before the CAS above\n+\n+  if (res == ZPublishState::none) {\n+    \/\/ fields were successfully published\n+    log_debug(gc, remset)(\"Forwarding remset published       : \" PTR_FORMAT \" \" PTR_FORMAT, untype(start()), untype(end()));\n+\n+    return;\n+  }\n+\n+  log_debug(gc, remset)(\"Forwarding remset discarded       : \" PTR_FORMAT \" \" PTR_FORMAT, untype(start()), untype(end()));\n+\n+  \/\/ reject: YC scans the remset concurrently\n+  \/\/ accept: YC accepted published remset - not possible, we just atomically published it\n+  \/\/         YC failed to retain page - not possible, since the current page is retainable\n+  assert(res == ZPublishState::reject, \"Unexpected value\");\n+\n+  \/\/ YC has rejected the stored values and will (or have already) find them them itself\n+  _relocated_remembered_fields_array.clear_and_deallocate();\n+}\n+\n+void ZForwarding::relocated_remembered_fields_notify_concurrent_scan_of() {\n+  \/\/ Invariant: The page is being retained\n+  assert(ZGeneration::young()->is_phase_mark(), \"Only called when\");\n+\n+  const ZPublishState res = Atomic::cmpxchg(&_relocated_remembered_fields_state, ZPublishState::none, ZPublishState::reject);\n+\n+  \/\/ none:      OC has not completed relocation\n+  \/\/ published: OC has completed and published all relocated remembered fields\n+  \/\/ reject:    A previous YC has already handled the field\n+  \/\/ accept:    A previous YC has determined that there's no concurrency between\n+  \/\/            OC relocation and YC remembered fields scanning - not possible\n+  \/\/            since the page has been retained (still being relocated) and\n+  \/\/            we are in the process of scanning fields\n+\n+  if (res == ZPublishState::none) {\n+    \/\/ Successfully notified and rejected any collected data from the OC\n+    log_debug(gc, remset)(\"Forwarding remset eager           : \" PTR_FORMAT \" \" PTR_FORMAT, untype(start()), untype(end()));\n+\n+    return;\n+  }\n+\n+  if (res == ZPublishState::published) {\n+    \/\/ OC relocation already collected and published fields\n+\n+    \/\/ Still notify concurrent scanning and reject the collected data from the OC\n+    const ZPublishState res2 = Atomic::cmpxchg(&_relocated_remembered_fields_state, ZPublishState::published, ZPublishState::reject);\n+    assert(res2 == ZPublishState::published, \"Should not fail\");\n+\n+    log_debug(gc, remset)(\"Forwarding remset eager and reject: \" PTR_FORMAT \" \" PTR_FORMAT, untype(start()), untype(end()));\n+\n+    \/\/ The YC rejected the publish fields and is responsible for the array\n+    \/\/ Eagerly deallocate the memory\n+    _relocated_remembered_fields_array.clear_and_deallocate();\n+    return;\n@@ -158,4 +355,4 @@\n-  \/\/ Detach and return page\n-  ZPage* const page = _page;\n-  _page = NULL;\n-  return page;\n+  log_debug(gc, remset)(\"Forwarding remset redundant       : \" PTR_FORMAT \" \" PTR_FORMAT, untype(start()), untype(end()));\n+\n+  \/\/ Previous YC already handled the remembered fields\n+  assert(res == ZPublishState::reject, \"Unexpected value\");\n@@ -164,6 +361,8 @@\n-void ZForwarding::abort_page() {\n-  ZLocker<ZConditionLock> locker(&_ref_lock);\n-  assert(Atomic::load(&_ref_count) > 0, \"Invalid state\");\n-  assert(!_ref_abort, \"Invalid state\");\n-  _ref_abort = true;\n-  _ref_lock.notify_all();\n+bool ZForwarding::relocated_remembered_fields_published_contains(volatile zpointer* p) {\n+  for (volatile zpointer* const elem : _relocated_remembered_fields_array) {\n+    if (elem == p) {\n+      return true;\n+    }\n+  }\n+\n+  return false;\n@@ -174,1 +373,1 @@\n-  guarantee(_page != NULL, \"Invalid page\");\n+  guarantee(_page != nullptr, \"Invalid page\");\n@@ -201,1 +400,1 @@\n-    const uintptr_t to_addr = ZAddress::good(entry.to_offset());\n+    const zaddress to_addr = ZOffset::address(to_zoffset(entry.to_offset()));\n@@ -209,1 +408,1 @@\n-  _page->verify_live(live_objects, live_bytes);\n+  _page->verify_live(live_objects, live_bytes, _in_place);\n","filename":"src\/hotspot\/share\/gc\/z\/zForwarding.cpp","additions":233,"deletions":34,"binary":false,"changes":267,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,0 +27,1 @@\n+#include \"gc\/z\/zArray.hpp\"\n@@ -29,0 +30,1 @@\n+#include \"gc\/z\/zGenerationId.hpp\"\n@@ -30,0 +32,2 @@\n+#include \"gc\/z\/zPageAge.hpp\"\n+#include \"gc\/z\/zPageType.hpp\"\n@@ -35,0 +39,1 @@\n+class ZRelocateQueue;\n@@ -42,0 +47,7 @@\n+  enum class ZPublishState : int8_t {\n+    none,      \/\/ No publishing done yet\n+    published, \/\/ OC published remset field info, which YC will reject or accept\n+    reject,    \/\/ YC remset scanning accepted OC published remset field info\n+    accept     \/\/ YC remset scanning rejected OC published remset field info\n+  };\n+\n@@ -44,0 +56,1 @@\n+  typedef ZArray<volatile zpointer*> PointerArray;\n@@ -48,1 +61,4 @@\n-  ZPage*                 _page;\n+  ZPage* const           _page;\n+  ZPageAge               _from_age;\n+  ZPageAge               _to_age;\n+  volatile bool          _claimed;\n@@ -51,1 +67,8 @@\n-  bool                   _ref_abort;\n+  volatile bool          _done;\n+\n+  \/\/ Relocated remembered set fields support\n+  volatile ZPublishState _relocated_remembered_fields_state;\n+  PointerArray           _relocated_remembered_fields_array;\n+  uint32_t               _relocated_remembered_fields_publish_young_seqnum;\n+\n+  \/\/ In-place relocation support\n@@ -53,0 +76,4 @@\n+  zoffset_end            _in_place_top_at_start;\n+\n+  \/\/ Debugging\n+  volatile Thread*       _in_place_thread;\n@@ -59,1 +86,4 @@\n-  ZForwarding(ZPage* page, size_t nentries);\n+  template <typename Function>\n+  void object_iterate_forwarded_via_livemap(Function function);\n+\n+  ZForwarding(ZPage* page, ZPageAge to_age, size_t nentries);\n@@ -63,1 +93,1 @@\n-  static ZForwarding* alloc(ZForwardingAllocator* allocator, ZPage* page);\n+  static ZForwarding* alloc(ZForwardingAllocator* allocator, ZPage* page, ZPageAge to_age);\n@@ -65,2 +95,5 @@\n-  uint8_t type() const;\n-  uintptr_t start() const;\n+  ZPageType type() const;\n+  ZPageAge from_age() const;\n+  ZPageAge to_age() const;\n+  zoffset start() const;\n+  zoffset_end end() const;\n@@ -69,1 +102,0 @@\n-  void object_iterate(ObjectClosure *cl);\n@@ -71,2 +103,32 @@\n-  bool retain_page();\n-  ZPage* claim_page();\n+  bool is_promotion() const;\n+\n+  \/\/ Visit from-objects\n+  template <typename Function>\n+  void object_iterate(Function function);\n+\n+  template <typename Function>\n+  void address_unsafe_iterate_via_table(Function function);\n+\n+  \/\/ Visit to-objects\n+  template <typename Function>\n+  void object_iterate_forwarded(Function function);\n+\n+  template <typename Function>\n+  void object_iterate_forwarded_via_table(Function function);\n+\n+  template <typename Function>\n+  void oops_do_in_forwarded(Function function);\n+\n+  template <typename Function>\n+  void oops_do_in_forwarded_via_table(Function function);\n+\n+  bool claim();\n+\n+  \/\/ In-place relocation support\n+  bool in_place_relocation() const;\n+  void in_place_relocation_claim_page();\n+  void in_place_relocation_start(zoffset relocated_watermark);\n+  void in_place_relocation_finish();\n+  bool in_place_relocation_is_below_top_at_start(zoffset addr) const;\n+\n+  bool retain_page(ZRelocateQueue* queue);\n@@ -74,1 +136,1 @@\n-  bool wait_page_released() const;\n+\n@@ -76,1 +138,4 @@\n-  void abort_page();\n+  ZPage* page();\n+\n+  void mark_done();\n+  bool is_done() const;\n@@ -78,2 +143,1 @@\n-  void set_in_place();\n-  bool in_place() const;\n+  zaddress find(zaddress_unsafe addr);\n@@ -82,1 +146,11 @@\n-  uintptr_t insert(uintptr_t from_index, uintptr_t to_offset, ZForwardingCursor* cursor);\n+  zoffset insert(uintptr_t from_index, zoffset to_offset, ZForwardingCursor* cursor);\n+\n+  \/\/ Relocated remembered set fields support\n+  void relocated_remembered_fields_register(volatile zpointer* p);\n+  void relocated_remembered_fields_after_relocate();\n+  void relocated_remembered_fields_publish();\n+  void relocated_remembered_fields_notify_concurrent_scan_of();\n+  bool relocated_remembered_fields_is_concurrently_scanned() const;\n+  template <typename Function>\n+  void relocated_remembered_fields_apply_to_published(Function function);\n+  bool relocated_remembered_fields_published_contains(volatile zpointer* p);\n","filename":"src\/hotspot\/share\/gc\/z\/zForwarding.hpp","additions":89,"deletions":15,"binary":false,"changes":104,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -29,0 +29,1 @@\n+#include \"gc\/z\/zAddress.inline.hpp\"\n@@ -33,0 +34,1 @@\n+#include \"gc\/z\/zIterator.inline.hpp\"\n@@ -35,0 +37,1 @@\n+#include \"gc\/z\/zUtils.inline.hpp\"\n@@ -50,1 +53,1 @@\n-inline ZForwarding* ZForwarding::alloc(ZForwardingAllocator* allocator, ZPage* page) {\n+inline ZForwarding* ZForwarding::alloc(ZForwardingAllocator* allocator, ZPage* page, ZPageAge to_age) {\n@@ -53,1 +56,1 @@\n-  return ::new (addr) ZForwarding(page, nentries);\n+  return ::new (addr) ZForwarding(page, to_age, nentries);\n@@ -56,1 +59,1 @@\n-inline ZForwarding::ZForwarding(ZPage* page, size_t nentries) :\n+inline ZForwarding::ZForwarding(ZPage* page, ZPageAge to_age, size_t nentries) :\n@@ -61,0 +64,3 @@\n+    _from_age(page->age()),\n+    _to_age(to_age),\n+    _claimed(false),\n@@ -63,2 +69,7 @@\n-    _ref_abort(false),\n-    _in_place(false) {}\n+    _done(false),\n+    _relocated_remembered_fields_state(ZPublishState::none),\n+    _relocated_remembered_fields_array(),\n+    _relocated_remembered_fields_publish_young_seqnum(0),\n+    _in_place(false),\n+    _in_place_top_at_start(),\n+    _in_place_thread(nullptr) {}\n@@ -66,1 +77,1 @@\n-inline uint8_t ZForwarding::type() const {\n+inline ZPageType ZForwarding::type() const {\n@@ -70,1 +81,9 @@\n-inline uintptr_t ZForwarding::start() const {\n+inline ZPageAge ZForwarding::from_age() const {\n+  return _from_age;\n+}\n+\n+inline ZPageAge ZForwarding::to_age() const {\n+  return _to_age;\n+}\n+\n+inline zoffset ZForwarding::start() const {\n@@ -74,0 +93,4 @@\n+inline zoffset_end ZForwarding::end() const {\n+  return _virtual.end();\n+}\n+\n@@ -82,2 +105,79 @@\n-inline void ZForwarding::object_iterate(ObjectClosure *cl) {\n-  return _page->object_iterate(cl);\n+inline bool ZForwarding::is_promotion() const {\n+  return _from_age != ZPageAge::old &&\n+         _to_age == ZPageAge::old;\n+}\n+\n+template <typename Function>\n+inline void ZForwarding::object_iterate(Function function) {\n+  ZObjectClosure<Function> cl(function);\n+  _page->object_iterate(function);\n+}\n+\n+template <typename Function>\n+inline void ZForwarding::address_unsafe_iterate_via_table(Function function) {\n+  for (ZForwardingCursor i = 0; i < _entries.length(); i++) {\n+    const ZForwardingEntry entry = at(&i);\n+    if (!entry.populated()) {\n+      \/\/ Skip empty entries\n+      continue;\n+    }\n+\n+    \/\/ Find to-object\n+\n+    const zoffset from_offset = start() + (entry.from_index() << object_alignment_shift());\n+    const zaddress_unsafe from_addr = ZOffset::address_unsafe(from_offset);\n+\n+    \/\/ Apply function\n+    function(from_addr);\n+  }\n+}\n+\n+template <typename Function>\n+inline void ZForwarding::object_iterate_forwarded_via_livemap(Function function) {\n+  assert(!in_place_relocation(), \"Not allowed to use livemap iteration\");\n+\n+  object_iterate([&](oop obj) {\n+    \/\/ Find to-object\n+    const zaddress_unsafe from_addr = to_zaddress_unsafe(obj);\n+    const zaddress to_addr = this->find(from_addr);\n+    const oop to_obj = to_oop(to_addr);\n+\n+    \/\/ Apply function\n+    function(to_obj);\n+  });\n+}\n+\n+template <typename Function>\n+inline void ZForwarding::object_iterate_forwarded_via_table(Function function) {\n+  for (ZForwardingCursor i = 0; i < _entries.length(); i++) {\n+    const ZForwardingEntry entry = at(&i);\n+    if (!entry.populated()) {\n+      \/\/ Skip empty entries\n+      continue;\n+    }\n+\n+    \/\/ Find to-object\n+    const zoffset to_offset = to_zoffset(entry.to_offset());\n+    const zaddress to_addr = ZOffset::address(to_offset);\n+    const oop to_obj = to_oop(to_addr);\n+\n+    \/\/ Apply function\n+    function(to_obj);\n+  }\n+}\n+\n+template <typename Function>\n+inline void ZForwarding::object_iterate_forwarded(Function function) {\n+  if (in_place_relocation()) {\n+    \/\/ The original objects are not available anymore, can't use the livemap\n+    object_iterate_forwarded_via_table(function);\n+  } else {\n+    object_iterate_forwarded_via_livemap(function);\n+  }\n+}\n+\n+template <typename Function>\n+void ZForwarding::oops_do_in_forwarded(Function function) {\n+  object_iterate_forwarded([&](oop to_obj) {\n+    ZIterator::basic_oop_iterate_safe(to_obj, function);\n+  });\n@@ -86,2 +186,5 @@\n-inline void ZForwarding::set_in_place() {\n-  _in_place = true;\n+template <typename Function>\n+void ZForwarding::oops_do_in_forwarded_via_table(Function function) {\n+  object_iterate_forwarded_via_table([&](oop to_obj) {\n+    ZIterator::basic_oop_iterate_safe(to_obj, function);\n+  });\n@@ -90,1 +193,2 @@\n-inline bool ZForwarding::in_place() const {\n+inline bool ZForwarding::in_place_relocation() const {\n+  assert(Atomic::load(&_ref_count) != 0, \"The page has been released\/detached\");\n@@ -117,0 +221,7 @@\n+inline zaddress ZForwarding::find(zaddress_unsafe addr) {\n+  const uintptr_t from_index = (ZAddress::offset(addr) - start()) >> object_alignment_shift();\n+  ZForwardingCursor cursor;\n+  const ZForwardingEntry entry = find(from_index, &cursor);\n+  return entry.populated() ? ZOffset::address(to_zoffset(entry.to_offset())) : zaddress::null;\n+}\n+\n@@ -135,2 +246,2 @@\n-inline uintptr_t ZForwarding::insert(uintptr_t from_index, uintptr_t to_offset, ZForwardingCursor* cursor) {\n-  const ZForwardingEntry new_entry(from_index, to_offset);\n+inline zoffset ZForwarding::insert(uintptr_t from_index, zoffset to_offset, ZForwardingCursor* cursor) {\n+  const ZForwardingEntry new_entry(from_index, untype(to_offset));\n@@ -155,1 +266,1 @@\n-        return entry.to_offset();\n+        return to_zoffset(entry.to_offset());\n@@ -163,0 +274,71 @@\n+inline void ZForwarding::relocated_remembered_fields_register(volatile zpointer* p) {\n+  \/\/ Invariant: Page is being retained\n+  assert(ZGeneration::young()->is_phase_mark(), \"Only called when\");\n+\n+  const ZPublishState res = Atomic::load(&_relocated_remembered_fields_state);\n+\n+  \/\/ none:      Gather remembered fields\n+  \/\/ published: Have already published fields - not possible since they haven't been\n+  \/\/            collected yet\n+  \/\/ reject:    YC rejected fields collected by the OC\n+  \/\/ accept:    YC has marked that there's no more concurrent scanning of relocated\n+  \/\/            fields - not possible since this code is still relocating objects\n+\n+  if (res == ZPublishState::none) {\n+    _relocated_remembered_fields_array.push(p);\n+    return;\n+  }\n+\n+  assert(res == ZPublishState::reject, \"Unexpected value\");\n+}\n+\n+\/\/ Returns true iff the page is being (or about to be) relocated by the OC\n+\/\/ while the YC gathered the remembered fields of the \"from\" page.\n+inline bool ZForwarding::relocated_remembered_fields_is_concurrently_scanned() const {\n+  return Atomic::load(&_relocated_remembered_fields_state) == ZPublishState::reject;\n+}\n+\n+template <typename Function>\n+inline void ZForwarding::relocated_remembered_fields_apply_to_published(Function function) {\n+  \/\/ Invariant: Page is not being retained\n+  assert(ZGeneration::young()->is_phase_mark(), \"Only called when\");\n+\n+  const ZPublishState res = Atomic::load_acquire(&_relocated_remembered_fields_state);\n+\n+  \/\/ none:      Nothing published - page had already been relocated before YC started\n+  \/\/ published: OC relocated and published relocated remembered fields\n+  \/\/ reject:    A previous YC concurrently scanned relocated remembered fields of the \"from\" page\n+  \/\/ accept:    A previous YC marked that it didn't do (reject)\n+\n+  if (res == ZPublishState::published) {\n+    log_debug(gc, remset)(\"Forwarding remset accept          : \" PTR_FORMAT \" \" PTR_FORMAT \" (\" PTR_FORMAT \", %s)\",\n+        untype(start()), untype(end()), p2i(this), Thread::current()->name());\n+\n+    \/\/ OC published relocated remembered fields\n+    ZArrayIterator<volatile zpointer*> iter(&_relocated_remembered_fields_array);\n+    for (volatile zpointer* to_field_addr; iter.next(&to_field_addr);) {\n+      function(to_field_addr);\n+    }\n+\n+    \/\/ YC responsible for the array - eagerly deallocate\n+    _relocated_remembered_fields_array.clear_and_deallocate();\n+  }\n+\n+  assert(_relocated_remembered_fields_publish_young_seqnum != 0, \"Must have been set\");\n+  if (_relocated_remembered_fields_publish_young_seqnum == ZGeneration::young()->seqnum()) {\n+    log_debug(gc, remset)(\"scan_forwarding failed retain unsafe \" PTR_FORMAT, untype(start()));\n+    \/\/ The page was relocated concurrently with the current young generation\n+    \/\/ collection. Mark that it is unsafe (and unnecessary) to call scan_page\n+    \/\/ on the page in the page table.\n+    assert(res != ZPublishState::accept, \"Unexpected\");\n+    Atomic::store(&_relocated_remembered_fields_state, ZPublishState::reject);\n+  } else {\n+    log_debug(gc, remset)(\"scan_forwarding failed retain safe \" PTR_FORMAT, untype(start()));\n+    \/\/ Guaranteed that the page was fully relocated and removed from page table.\n+    \/\/ Because of this we can signal to scan_page that any page found in page table\n+    \/\/ of the same slot as the current forwarding is a page that is safe to scan,\n+    \/\/ and in fact must be scanned.\n+    Atomic::store(&_relocated_remembered_fields_state, ZPublishState::accept);\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/z\/zForwarding.inline.hpp","additions":198,"deletions":16,"binary":false,"changes":214,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -29,3 +29,3 @@\n-    _start(NULL),\n-    _end(NULL),\n-    _top(NULL) {}\n+    _start(nullptr),\n+    _end(nullptr),\n+    _top(nullptr) {}\n","filename":"src\/hotspot\/share\/gc\/z\/zForwardingAllocator.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+\n","filename":"src\/hotspot\/share\/gc\/z\/zForwardingEntry.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2019, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,0 +28,1 @@\n+#include \"gc\/z\/zIndexDistributor.hpp\"\n@@ -32,0 +33,1 @@\n+  friend class ZRemsetTableIterator;\n@@ -37,0 +39,2 @@\n+  ZForwarding* at(size_t index) const;\n+\n@@ -40,1 +44,1 @@\n-  ZForwarding* get(uintptr_t addr) const;\n+  ZForwarding* get(zaddress_unsafe addr) const;\n","filename":"src\/hotspot\/share\/gc\/z\/zForwardingTable.hpp","additions":6,"deletions":2,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2019, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2019, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -33,0 +33,1 @@\n+#include \"gc\/z\/zIndexDistributor.inline.hpp\"\n@@ -38,2 +39,6 @@\n-inline ZForwarding* ZForwardingTable::get(uintptr_t addr) const {\n-  assert(!ZAddress::is_null(addr), \"Invalid address\");\n+inline ZForwarding* ZForwardingTable::at(size_t index) const {\n+  return _map.at(index);\n+}\n+\n+inline ZForwarding* ZForwardingTable::get(zaddress_unsafe addr) const {\n+  assert(!is_null(addr), \"Invalid address\");\n@@ -44,1 +49,1 @@\n-  const uintptr_t offset = forwarding->start();\n+  const zoffset offset = forwarding->start();\n@@ -47,1 +52,1 @@\n-  assert(_map.get(offset) == NULL, \"Invalid entry\");\n+  assert(_map.get(offset) == nullptr, \"Invalid entry\");\n@@ -52,1 +57,1 @@\n-  const uintptr_t offset = forwarding->start();\n+  const zoffset offset = forwarding->start();\n@@ -56,1 +61,1 @@\n-  _map.put(offset, size, NULL);\n+  _map.put(offset, size, nullptr);\n","filename":"src\/hotspot\/share\/gc\/z\/zForwardingTable.inline.hpp","additions":12,"deletions":7,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -0,0 +1,92 @@\n+\/*\n+ * Copyright (c) 2022, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/z\/zGCIdPrinter.hpp\"\n+#include \"include\/jvm.h\"\n+\n+ZGCIdPrinter* ZGCIdPrinter::_instance;\n+\n+void ZGCIdPrinter::initialize() {\n+  _instance = new ZGCIdPrinter();\n+  GCId::set_printer(_instance);\n+}\n+\n+int ZGCIdPrinter::print_gc_id_unchecked(uint gc_id, char* buf, size_t len) {\n+  if (gc_id == _minor_gc_id) {\n+    \/\/ Minor collections are always tagged with 'y'\n+    return jio_snprintf(buf, len, \"GC(%u) y: \", gc_id);\n+  }\n+\n+  if (gc_id == _major_gc_id) {\n+    \/\/ Major collections are either tagged with 'Y' or 'O',\n+    \/\/ this is controlled by _major_tag.\n+    return jio_snprintf(buf, len, \"GC(%u) %c: \", gc_id, _major_tag);\n+  }\n+\n+  \/\/ The initial log for each GC should be untagged this\n+  \/\/ is handled by not yet having set the current GC id\n+  \/\/ for that collection and thus falling through to here.\n+  return jio_snprintf(buf, len, \"GC(%u) \", gc_id);\n+}\n+\n+size_t ZGCIdPrinter::print_gc_id(uint gc_id, char* buf, size_t len) {\n+  const int ret = print_gc_id_unchecked(gc_id, buf, len);\n+  assert(ret > 0, \"Failed to print prefix. Log buffer too small?\");\n+  return (size_t)ret;\n+}\n+\n+ZGCIdPrinter::ZGCIdPrinter() :\n+    _minor_gc_id(GCId::undefined()),\n+    _major_gc_id(GCId::undefined()),\n+    _major_tag('-') { }\n+\n+void ZGCIdPrinter::set_minor_gc_id(uint id) {\n+  _minor_gc_id = id;\n+}\n+\n+void ZGCIdPrinter::set_major_gc_id(uint id) {\n+ _major_gc_id = id;\n+}\n+\n+void ZGCIdPrinter::set_major_tag(char tag) {\n+  _major_tag = tag;\n+}\n+\n+ZGCIdMinor::ZGCIdMinor(uint gc_id) {\n+  ZGCIdPrinter::_instance->set_minor_gc_id(gc_id);\n+}\n+\n+ZGCIdMinor::~ZGCIdMinor() {\n+  ZGCIdPrinter::_instance->set_minor_gc_id(GCId::undefined());\n+}\n+\n+ZGCIdMajor::ZGCIdMajor(uint gc_id, char tag) {\n+  ZGCIdPrinter::_instance->set_major_gc_id(gc_id);\n+  ZGCIdPrinter::_instance->set_major_tag(tag);\n+}\n+\n+ZGCIdMajor::~ZGCIdMajor() {\n+  ZGCIdPrinter::_instance->set_major_gc_id(GCId::undefined());\n+  ZGCIdPrinter::_instance->set_major_tag('-');\n+}\n","filename":"src\/hotspot\/share\/gc\/z\/zGCIdPrinter.cpp","additions":92,"deletions":0,"binary":false,"changes":92,"status":"added"},{"patch":"@@ -0,0 +1,67 @@\n+\/*\n+ * Copyright (c) 2022, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_Z_ZGCIDPRINTER_HPP\n+#define SHARE_GC_Z_ZGCIDPRINTER_HPP\n+\n+#include \"gc\/shared\/gcId.hpp\"\n+#include \"memory\/allocation.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+class ZGCIdPrinter : public GCIdPrinter {\n+  friend class ZGCIdMajor;\n+  friend class ZGCIdMinor;\n+\n+private:\n+  static ZGCIdPrinter* _instance;\n+\n+  uint _minor_gc_id;\n+  uint _major_gc_id;\n+  char _major_tag;\n+\n+  ZGCIdPrinter();\n+\n+  void set_minor_gc_id(uint id);\n+  void set_major_gc_id(uint id);\n+  void set_major_tag(char tag);\n+\n+  int print_gc_id_unchecked(uint gc_id, char *buf, size_t len);\n+  size_t print_gc_id(uint gc_id, char *buf, size_t len) override;\n+\n+public:\n+  static void initialize();\n+};\n+\n+class ZGCIdMinor : public StackObj {\n+public:\n+  ZGCIdMinor(uint gc_id);\n+  ~ZGCIdMinor();\n+};\n+\n+class ZGCIdMajor : public StackObj {\n+public:\n+  ZGCIdMajor(uint gc_id, char tag);\n+  ~ZGCIdMajor();\n+};\n+\n+#endif \/\/ SHARE_GC_Z_ZGCIDPRINTER_HPP\n","filename":"src\/hotspot\/share\/gc\/z\/zGCIdPrinter.hpp","additions":67,"deletions":0,"binary":false,"changes":67,"status":"added"},{"patch":"@@ -0,0 +1,1486 @@\n+\/*\n+ * Copyright (c) 2021, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"classfile\/classLoaderDataGraph.hpp\"\n+#include \"code\/nmethod.hpp\"\n+#include \"gc\/shared\/gcLocker.hpp\"\n+#include \"gc\/shared\/gcVMOperations.hpp\"\n+#include \"gc\/shared\/isGCActiveMark.hpp\"\n+#include \"gc\/shared\/suspendibleThreadSet.hpp\"\n+#include \"gc\/z\/zAllocator.inline.hpp\"\n+#include \"gc\/z\/zBarrierSet.hpp\"\n+#include \"gc\/z\/zBarrierSetAssembler.hpp\"\n+#include \"gc\/z\/zBarrierSetNMethod.hpp\"\n+#include \"gc\/z\/zBreakpoint.hpp\"\n+#include \"gc\/z\/zCollectedHeap.hpp\"\n+#include \"gc\/z\/zDriver.hpp\"\n+#include \"gc\/z\/zForwarding.hpp\"\n+#include \"gc\/z\/zForwardingTable.inline.hpp\"\n+#include \"gc\/z\/zGeneration.inline.hpp\"\n+#include \"gc\/z\/zHeap.inline.hpp\"\n+#include \"gc\/z\/zJNICritical.hpp\"\n+#include \"gc\/z\/zMark.inline.hpp\"\n+#include \"gc\/z\/zPageAllocator.hpp\"\n+#include \"gc\/z\/zRelocationSet.inline.hpp\"\n+#include \"gc\/z\/zRelocationSetSelector.inline.hpp\"\n+#include \"gc\/z\/zRemembered.hpp\"\n+#include \"gc\/z\/zRootsIterator.hpp\"\n+#include \"gc\/z\/zStat.hpp\"\n+#include \"gc\/z\/zTask.hpp\"\n+#include \"gc\/z\/zUncoloredRoot.inline.hpp\"\n+#include \"gc\/z\/zVerify.hpp\"\n+#include \"gc\/z\/zWorkers.hpp\"\n+#include \"logging\/log.hpp\"\n+#include \"memory\/universe.hpp\"\n+#include \"prims\/jvmtiTagMap.hpp\"\n+#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/continuation.hpp\"\n+#include \"runtime\/handshake.hpp\"\n+#include \"runtime\/safepoint.hpp\"\n+#include \"runtime\/threads.hpp\"\n+#include \"runtime\/vmOperations.hpp\"\n+#include \"runtime\/vmThread.hpp\"\n+#include \"utilities\/debug.hpp\"\n+#include \"utilities\/events.hpp\"\n+\n+static const ZStatPhaseGeneration ZPhaseGenerationYoung[] {\n+  ZStatPhaseGeneration(\"Young Generation\", ZGenerationId::young),\n+  ZStatPhaseGeneration(\"Young Generation (Promote All)\", ZGenerationId::young),\n+  ZStatPhaseGeneration(\"Young Generation (Collect Roots)\", ZGenerationId::young),\n+  ZStatPhaseGeneration(\"Young Generation\", ZGenerationId::young)\n+};\n+\n+static const ZStatPhaseGeneration ZPhaseGenerationOld(\"Old Generation\", ZGenerationId::old);\n+\n+static const ZStatPhasePause      ZPhasePauseMarkStartYoung(\"Pause Mark Start\", ZGenerationId::young);\n+static const ZStatPhasePause      ZPhasePauseMarkStartYoungAndOld(\"Pause Mark Start (Major)\", ZGenerationId::young);\n+static const ZStatPhaseConcurrent ZPhaseConcurrentMarkYoung(\"Concurrent Mark\", ZGenerationId::young);\n+static const ZStatPhaseConcurrent ZPhaseConcurrentMarkContinueYoung(\"Concurrent Mark Continue\", ZGenerationId::young);\n+static const ZStatPhasePause      ZPhasePauseMarkEndYoung(\"Pause Mark End\", ZGenerationId::young);\n+static const ZStatPhaseConcurrent ZPhaseConcurrentMarkFreeYoung(\"Concurrent Mark Free\", ZGenerationId::young);\n+static const ZStatPhaseConcurrent ZPhaseConcurrentResetRelocationSetYoung(\"Concurrent Reset Relocation Set\", ZGenerationId::young);\n+static const ZStatPhaseConcurrent ZPhaseConcurrentSelectRelocationSetYoung(\"Concurrent Select Relocation Set\", ZGenerationId::young);\n+static const ZStatPhasePause      ZPhasePauseRelocateStartYoung(\"Pause Relocate Start\", ZGenerationId::young);\n+static const ZStatPhaseConcurrent ZPhaseConcurrentRelocatedYoung(\"Concurrent Relocate\", ZGenerationId::young);\n+\n+static const ZStatPhaseConcurrent ZPhaseConcurrentMarkOld(\"Concurrent Mark\", ZGenerationId::old);\n+static const ZStatPhaseConcurrent ZPhaseConcurrentMarkContinueOld(\"Concurrent Mark Continue\", ZGenerationId::old);\n+static const ZStatPhasePause      ZPhasePauseMarkEndOld(\"Pause Mark End\", ZGenerationId::old);\n+static const ZStatPhaseConcurrent ZPhaseConcurrentMarkFreeOld(\"Concurrent Mark Free\", ZGenerationId::old);\n+static const ZStatPhaseConcurrent ZPhaseConcurrentProcessNonStrongOld(\"Concurrent Process Non-Strong\", ZGenerationId::old);\n+static const ZStatPhaseConcurrent ZPhaseConcurrentResetRelocationSetOld(\"Concurrent Reset Relocation Set\", ZGenerationId::old);\n+static const ZStatPhaseConcurrent ZPhaseConcurrentSelectRelocationSetOld(\"Concurrent Select Relocation Set\", ZGenerationId::old);\n+static const ZStatPhasePause      ZPhasePauseRelocateStartOld(\"Pause Relocate Start\", ZGenerationId::old);\n+static const ZStatPhaseConcurrent ZPhaseConcurrentRelocatedOld(\"Concurrent Relocate\", ZGenerationId::old);\n+static const ZStatPhaseConcurrent ZPhaseConcurrentRemapRootsOld(\"Concurrent Remap Roots\", ZGenerationId::old);\n+\n+static const ZStatSubPhase ZSubPhaseConcurrentMarkRootsYoung(\"Concurrent Mark Roots\", ZGenerationId::young);\n+static const ZStatSubPhase ZSubPhaseConcurrentMarkFollowYoung(\"Concurrent Mark Follow\", ZGenerationId::young);\n+\n+static const ZStatSubPhase ZSubPhaseConcurrentMarkRootsOld(\"Concurrent Mark Roots\", ZGenerationId::old);\n+static const ZStatSubPhase ZSubPhaseConcurrentMarkFollowOld(\"Concurrent Mark Follow\", ZGenerationId::old);\n+static const ZStatSubPhase ZSubPhaseConcurrentRemapRootsColoredOld(\"Concurrent Remap Roots Colored\", ZGenerationId::old);\n+static const ZStatSubPhase ZSubPhaseConcurrentRemapRootsUncoloredOld(\"Concurrent Remap Roots Uncolored\", ZGenerationId::old);\n+static const ZStatSubPhase ZSubPhaseConcurrentRemapRememberedOld(\"Concurrent Remap Remembered\", ZGenerationId::old);\n+\n+static const ZStatSampler ZSamplerJavaThreads(\"System\", \"Java Threads\", ZStatUnitThreads);\n+\n+ZGenerationYoung* ZGeneration::_young;\n+ZGenerationOld*   ZGeneration::_old;\n+\n+ZGeneration::ZGeneration(ZGenerationId id, ZPageTable* page_table, ZPageAllocator* page_allocator) :\n+    _id(id),\n+    _page_allocator(page_allocator),\n+    _page_table(page_table),\n+    _forwarding_table(),\n+    _workers(id, &_stat_workers),\n+    _mark(this, page_table),\n+    _relocate(this),\n+    _relocation_set(this),\n+    _freed(0),\n+    _promoted(0),\n+    _compacted(0),\n+    _phase(Phase::Relocate),\n+    _seqnum(1),\n+    _stat_heap(),\n+    _stat_cycle(),\n+    _stat_workers(),\n+    _stat_mark(),\n+    _stat_relocation(),\n+    _gc_timer(nullptr) {\n+}\n+\n+bool ZGeneration::is_initialized() const {\n+  return _mark.is_initialized();\n+}\n+\n+ZWorkers* ZGeneration::workers() {\n+  return &_workers;\n+}\n+\n+uint ZGeneration::active_workers() const {\n+  return _workers.active_workers();\n+}\n+\n+void ZGeneration::set_active_workers(uint nworkers) {\n+  _workers.set_active_workers(nworkers);\n+}\n+\n+void ZGeneration::threads_do(ThreadClosure* tc) const {\n+  _workers.threads_do(tc);\n+}\n+\n+void ZGeneration::mark_flush_and_free(Thread* thread) {\n+  _mark.flush_and_free(thread);\n+}\n+\n+void ZGeneration::mark_free() {\n+   _mark.free();\n+}\n+\n+void ZGeneration::free_empty_pages(ZRelocationSetSelector* selector, int bulk) {\n+  \/\/ Freeing empty pages in bulk is an optimization to avoid grabbing\n+  \/\/ the page allocator lock, and trying to satisfy stalled allocations\n+  \/\/ too frequently.\n+  if (selector->should_free_empty_pages(bulk)) {\n+    const size_t freed = ZHeap::heap()->free_empty_pages(selector->empty_pages());\n+    increase_freed(freed);\n+    selector->clear_empty_pages();\n+  }\n+}\n+\n+void ZGeneration::flip_age_pages(const ZRelocationSetSelector* selector) {\n+  if (is_young()) {\n+    _relocate.flip_age_pages(selector->not_selected_small());\n+    _relocate.flip_age_pages(selector->not_selected_medium());\n+    _relocate.flip_age_pages(selector->not_selected_large());\n+  }\n+}\n+\n+static double fragmentation_limit(ZGenerationId generation) {\n+  if (generation == ZGenerationId::old) {\n+    return ZFragmentationLimit;\n+  } else {\n+    return ZYoungCompactionLimit;\n+  }\n+}\n+\n+void ZGeneration::select_relocation_set(ZGenerationId generation, bool promote_all) {\n+  \/\/ Register relocatable pages with selector\n+  ZRelocationSetSelector selector(fragmentation_limit(generation));\n+  {\n+    ZGenerationPagesIterator pt_iter(_page_table, _id, _page_allocator);\n+    for (ZPage* page; pt_iter.next(&page);) {\n+      if (!page->is_relocatable()) {\n+        \/\/ Not relocatable, don't register\n+        \/\/ Note that the seqnum can change under our feet here as the page\n+        \/\/ can be concurrently freed and recycled by a concurrent generation\n+        \/\/ collection. However this property is stable across such transitions.\n+        \/\/ If it was not relocatable before recycling, then it won't be\n+        \/\/ relocatable after it gets recycled either, as the seqnum atomically\n+        \/\/ becomes allocating for the given generation. The opposite property\n+        \/\/ also holds: if the page is relocatable, then it can't have been\n+        \/\/ concurrently freed; if it was re-allocated it would not be\n+        \/\/ relocatable, and if it was not re-allocated we know that it was\n+        \/\/ allocated earlier than mark start of the current generation\n+        \/\/ collection.\n+        continue;\n+      }\n+\n+      if (page->is_marked()) {\n+        \/\/ Register live page\n+        selector.register_live_page(page);\n+      } else {\n+        \/\/ Register empty page\n+        selector.register_empty_page(page);\n+\n+        \/\/ Reclaim empty pages in bulk\n+\n+        \/\/ An active iterator blocks immediate recycle and delete of pages.\n+        \/\/ The intent it to allow the code that iterates over the pages to\n+        \/\/ safely read the properties of the pages without them being changed\n+        \/\/ by another thread. However, this function both iterates over the\n+        \/\/ pages AND frees\/recycles them. We \"yield\" the iterator, so that we\n+        \/\/ can perform immediate recycling (as long as no other thread is\n+        \/\/ iterating over the pages). The contract is that the pages that are\n+        \/\/ about to be freed are \"owned\" by this thread, and no other thread\n+        \/\/ will change their states.\n+        pt_iter.yield([&]() {\n+          free_empty_pages(&selector, 64 \/* bulk *\/);\n+        });\n+      }\n+    }\n+\n+    \/\/ Reclaim remaining empty pages\n+    free_empty_pages(&selector, 0 \/* bulk *\/);\n+  }\n+\n+  \/\/ Select relocation set\n+  selector.select();\n+\n+  \/\/ Selecting tenuring threshold must be done after select\n+  \/\/ which produces the liveness data, but before install,\n+  \/\/ which consumes the tenuring threshold.\n+  if (generation == ZGenerationId::young) {\n+    ZGeneration::young()->select_tenuring_threshold(selector.stats(), promote_all);\n+  }\n+\n+  \/\/ Install relocation set\n+  _relocation_set.install(&selector);\n+\n+  \/\/ Flip age young pages that were not selected\n+  flip_age_pages(&selector);\n+\n+  \/\/ Setup forwarding table\n+  ZRelocationSetIterator rs_iter(&_relocation_set);\n+  for (ZForwarding* forwarding; rs_iter.next(&forwarding);) {\n+    _forwarding_table.insert(forwarding);\n+  }\n+\n+  \/\/ Update statistics\n+  stat_relocation()->at_select_relocation_set(selector.stats());\n+  stat_heap()->at_select_relocation_set(selector.stats());\n+}\n+\n+ZRelocationSetParallelIterator ZGeneration::relocation_set_parallel_iterator() {\n+  return ZRelocationSetParallelIterator(&_relocation_set);\n+}\n+\n+void ZGeneration::reset_relocation_set() {\n+  \/\/ Reset forwarding table\n+  ZRelocationSetIterator iter(&_relocation_set);\n+  for (ZForwarding* forwarding; iter.next(&forwarding);) {\n+    _forwarding_table.remove(forwarding);\n+  }\n+\n+  \/\/ Reset relocation set\n+  _relocation_set.reset(_page_allocator);\n+}\n+\n+void ZGeneration::synchronize_relocation() {\n+  _relocate.synchronize();\n+}\n+\n+void ZGeneration::desynchronize_relocation() {\n+  _relocate.desynchronize();\n+}\n+\n+void ZGeneration::reset_statistics() {\n+  assert(SafepointSynchronize::is_at_safepoint(), \"Should be at safepoint\");\n+  _freed = 0;\n+  _promoted = 0;\n+  _compacted = 0;\n+  _page_allocator->reset_statistics(_id);\n+}\n+\n+ssize_t ZGeneration::freed() const {\n+  return _freed;\n+}\n+\n+void ZGeneration::increase_freed(size_t size) {\n+  Atomic::add(&_freed, size, memory_order_relaxed);\n+}\n+\n+size_t ZGeneration::promoted() const {\n+  return _promoted;\n+}\n+\n+void ZGeneration::increase_promoted(size_t size) {\n+  Atomic::add(&_promoted, size, memory_order_relaxed);\n+}\n+\n+size_t ZGeneration::compacted() const {\n+  return _compacted;\n+}\n+\n+void ZGeneration::increase_compacted(size_t size) {\n+  Atomic::add(&_compacted, size, memory_order_relaxed);\n+}\n+\n+ConcurrentGCTimer* ZGeneration::gc_timer() const {\n+  return _gc_timer;\n+}\n+\n+void ZGeneration::set_gc_timer(ConcurrentGCTimer* gc_timer) {\n+  assert(_gc_timer == nullptr, \"Incorrect scoping\");\n+  _gc_timer = gc_timer;\n+}\n+\n+void ZGeneration::clear_gc_timer() {\n+  assert(_gc_timer != nullptr, \"Incorrect scoping\");\n+  _gc_timer = nullptr;\n+}\n+\n+void ZGeneration::log_phase_switch(Phase from, Phase to) {\n+  const char* const str[] = {\n+    \"Young Mark Start\",\n+    \"Young Mark End\",\n+    \"Young Relocate Start\",\n+    \"Old Mark Start\",\n+    \"Old Mark End\",\n+    \"Old Relocate Start\"\n+  };\n+\n+  size_t index = 0;\n+\n+  if (is_old()) {\n+    index += 3;\n+  }\n+\n+  if (to == Phase::Relocate) {\n+    index += 2;\n+  }\n+\n+  if (from == Phase::Mark && to == Phase::MarkComplete) {\n+    index += 1;\n+  }\n+\n+  assert(index < ARRAY_SIZE(str), \"OOB: \" SIZE_FORMAT \" < \" SIZE_FORMAT, index, ARRAY_SIZE(str));\n+\n+  Events::log_zgc_phase_switch(\"%-21s %4u\", str[index], seqnum());\n+}\n+\n+void ZGeneration::set_phase(Phase new_phase) {\n+  log_phase_switch(_phase, new_phase);\n+\n+  _phase = new_phase;\n+}\n+\n+void ZGeneration::at_collection_start(ConcurrentGCTimer* gc_timer) {\n+  set_gc_timer(gc_timer);\n+  stat_cycle()->at_start();\n+  stat_heap()->at_collection_start(_page_allocator->stats(this));\n+  workers()->set_active();\n+}\n+\n+void ZGeneration::at_collection_end() {\n+  workers()->set_inactive();\n+  stat_cycle()->at_end(stat_workers(), should_record_stats());\n+  \/\/ The heap at collection end data is gathered at relocate end\n+  clear_gc_timer();\n+}\n+\n+const char* ZGeneration::phase_to_string() const {\n+  switch (_phase) {\n+  case Phase::Mark:\n+    return \"Mark\";\n+\n+  case Phase::MarkComplete:\n+    return \"MarkComplete\";\n+\n+  case Phase::Relocate:\n+    return \"Relocate\";\n+\n+  default:\n+    return \"Unknown\";\n+  }\n+}\n+\n+class VM_ZOperation : public VM_Operation {\n+private:\n+  const uint _gc_id;\n+  bool       _success;\n+\n+public:\n+  VM_ZOperation() :\n+      _gc_id(GCId::current()),\n+      _success(false) {}\n+\n+  virtual bool block_jni_critical() const {\n+    \/\/ Blocking JNI critical regions is needed in operations where we change\n+    \/\/ the bad mask or move objects. Changing the bad mask will invalidate all\n+    \/\/ oops, which makes it conceptually the same thing as moving all objects.\n+    return false;\n+  }\n+\n+  virtual bool skip_thread_oop_barriers() const {\n+    return true;\n+  }\n+\n+  virtual bool do_operation() = 0;\n+\n+  virtual bool doit_prologue() {\n+    Heap_lock->lock();\n+    return true;\n+  }\n+\n+  virtual void doit() {\n+    \/\/ Setup GC id and active marker\n+    GCIdMark gc_id_mark(_gc_id);\n+    IsGCActiveMark gc_active_mark;\n+\n+    \/\/ Verify before operation\n+    ZVerify::before_zoperation();\n+\n+    \/\/ Execute operation\n+    _success = do_operation();\n+\n+    \/\/ Update statistics\n+    ZStatSample(ZSamplerJavaThreads, Threads::number_of_threads());\n+  }\n+\n+  virtual void doit_epilogue() {\n+    Heap_lock->unlock();\n+  }\n+\n+  bool success() const {\n+    return _success;\n+  }\n+\n+  bool pause() {\n+    if (block_jni_critical()) {\n+      ZJNICritical::block();\n+    }\n+\n+    VMThread::execute(this);\n+\n+    if (block_jni_critical()) {\n+      ZJNICritical::unblock();\n+    }\n+\n+    return _success;\n+  }\n+};\n+\n+ZYoungTypeSetter::ZYoungTypeSetter(ZYoungType type) {\n+  assert(ZGeneration::young()->_active_type == ZYoungType::none, \"Invalid type\");\n+  ZGeneration::young()->_active_type = type;\n+}\n+\n+ZYoungTypeSetter::~ZYoungTypeSetter() {\n+  assert(ZGeneration::young()->_active_type != ZYoungType::none, \"Invalid type\");\n+  ZGeneration::young()->_active_type = ZYoungType::none;\n+}\n+\n+ZGenerationYoung::ZGenerationYoung(ZPageTable* page_table,\n+                                   const ZForwardingTable* old_forwarding_table,\n+                                   ZPageAllocator* page_allocator) :\n+    ZGeneration(ZGenerationId::young, page_table, page_allocator),\n+    _active_type(ZYoungType::none),\n+    _tenuring_threshold(0),\n+    _remembered(page_table, old_forwarding_table, page_allocator),\n+    _jfr_tracer() {\n+  ZGeneration::_young = this;\n+}\n+\n+uint ZGenerationYoung::tenuring_threshold() {\n+  return _tenuring_threshold;\n+}\n+\n+class ZGenerationCollectionScopeYoung : public StackObj {\n+private:\n+  ZYoungTypeSetter _type_setter;\n+  ZStatTimer       _stat_timer;\n+\n+public:\n+  ZGenerationCollectionScopeYoung(ZYoungType type, ConcurrentGCTimer* gc_timer) :\n+      _type_setter(type),\n+      _stat_timer(ZPhaseGenerationYoung[(int)type], gc_timer) {\n+    \/\/ Update statistics and set the GC timer\n+    ZGeneration::young()->at_collection_start(gc_timer);\n+  }\n+\n+  ~ZGenerationCollectionScopeYoung() {\n+    \/\/ Update statistics and clear the GC timer\n+    ZGeneration::young()->at_collection_end();\n+  }\n+};\n+\n+bool ZGenerationYoung::should_record_stats() {\n+  return type() == ZYoungType::minor ||\n+         type() == ZYoungType::major_partial_roots;\n+}\n+\n+void ZGenerationYoung::collect(ZYoungType type, ConcurrentGCTimer* timer) {\n+  ZGenerationCollectionScopeYoung scope(type, timer);\n+\n+  \/\/ Phase 1: Pause Mark Start\n+  pause_mark_start();\n+\n+  \/\/ Phase 2: Concurrent Mark\n+  concurrent_mark();\n+\n+  abortpoint();\n+\n+  \/\/ Phase 3: Pause Mark End\n+  while (!pause_mark_end()) {\n+    \/\/ Phase 3.5: Concurrent Mark Continue\n+    concurrent_mark_continue();\n+\n+    abortpoint();\n+  }\n+\n+  \/\/ Phase 4: Concurrent Mark Free\n+  concurrent_mark_free();\n+\n+  abortpoint();\n+\n+  \/\/ Phase 5: Concurrent Reset Relocation Set\n+  concurrent_reset_relocation_set();\n+\n+  abortpoint();\n+\n+  \/\/ Phase 6: Concurrent Select Relocation Set\n+  concurrent_select_relocation_set();\n+\n+  abortpoint();\n+\n+  \/\/ Phase 7: Pause Relocate Start\n+  pause_relocate_start();\n+\n+  \/\/ Note that we can't have an abortpoint here. We need\n+  \/\/ to let concurrent_relocate() call abort_page()\n+  \/\/ on the remaining entries in the relocation set.\n+\n+  \/\/ Phase 8: Concurrent Relocate\n+  concurrent_relocate();\n+}\n+\n+class VM_ZMarkStartYoungAndOld : public VM_ZOperation {\n+public:\n+  virtual VMOp_Type type() const {\n+    return VMOp_ZMarkStartYoungAndOld;\n+  }\n+\n+  virtual bool block_jni_critical() const {\n+    return true;\n+  }\n+\n+  virtual bool do_operation() {\n+    ZStatTimerYoung timer(ZPhasePauseMarkStartYoungAndOld);\n+    ZServiceabilityPauseTracer tracer;\n+\n+    ZCollectedHeap::heap()->increment_total_collections(true \/* full *\/);\n+    ZGeneration::young()->mark_start();\n+    ZGeneration::old()->mark_start();\n+\n+    return true;\n+  }\n+};\n+\n+class VM_ZMarkStartYoung : public VM_ZOperation {\n+public:\n+  virtual VMOp_Type type() const {\n+    return VMOp_ZMarkStartYoung;\n+  }\n+\n+  virtual bool block_jni_critical() const {\n+    return true;\n+  }\n+\n+  virtual bool do_operation() {\n+    ZStatTimerYoung timer(ZPhasePauseMarkStartYoung);\n+    ZServiceabilityPauseTracer tracer;\n+\n+    ZCollectedHeap::heap()->increment_total_collections(false \/* full *\/);\n+    ZGeneration::young()->mark_start();\n+\n+    return true;\n+  }\n+};\n+\n+void ZGenerationYoung::flip_mark_start() {\n+  ZGlobalsPointers::flip_young_mark_start();\n+  ZBarrierSet::assembler()->patch_barriers();\n+  ZVerify::on_color_flip();\n+}\n+\n+void ZGenerationYoung::flip_relocate_start() {\n+  ZGlobalsPointers::flip_young_relocate_start();\n+  ZBarrierSet::assembler()->patch_barriers();\n+  ZVerify::on_color_flip();\n+}\n+\n+void ZGenerationYoung::pause_mark_start() {\n+  if (type() == ZYoungType::major_full_roots ||\n+      type() == ZYoungType::major_partial_roots) {\n+    VM_ZMarkStartYoungAndOld().pause();\n+  } else {\n+    VM_ZMarkStartYoung().pause();\n+  }\n+}\n+\n+void ZGenerationYoung::concurrent_mark() {\n+  ZStatTimerYoung timer(ZPhaseConcurrentMarkYoung);\n+  mark_roots();\n+  mark_follow();\n+}\n+\n+class VM_ZMarkEndYoung : public VM_ZOperation {\n+public:\n+  virtual VMOp_Type type() const {\n+    return VMOp_ZMarkEndYoung;\n+  }\n+\n+  virtual bool do_operation() {\n+    ZStatTimerYoung timer(ZPhasePauseMarkEndYoung);\n+    ZServiceabilityPauseTracer tracer;\n+\n+    return ZGeneration::young()->mark_end();\n+  }\n+};\n+\n+\n+bool ZGenerationYoung::pause_mark_end() {\n+  return VM_ZMarkEndYoung().pause();\n+}\n+\n+void ZGenerationYoung::concurrent_mark_continue() {\n+  ZStatTimerYoung timer(ZPhaseConcurrentMarkContinueYoung);\n+  mark_follow();\n+}\n+\n+void ZGenerationYoung::concurrent_mark_free() {\n+  ZStatTimerYoung timer(ZPhaseConcurrentMarkFreeYoung);\n+  mark_free();\n+}\n+\n+void ZGenerationYoung::concurrent_reset_relocation_set() {\n+  ZStatTimerYoung timer(ZPhaseConcurrentResetRelocationSetYoung);\n+  reset_relocation_set();\n+}\n+\n+void ZGenerationYoung::select_tenuring_threshold(ZRelocationSetSelectorStats stats, bool promote_all) {\n+  const char* reason = \"\";\n+  if (promote_all) {\n+    _tenuring_threshold = 0;\n+    reason = \"Promote All\";\n+  } else if (ZTenuringThreshold != -1) {\n+    _tenuring_threshold = static_cast<uint>(ZTenuringThreshold);\n+    reason = \"ZTenuringThreshold\";\n+  } else {\n+    _tenuring_threshold = compute_tenuring_threshold(stats);\n+    reason = \"Computed\";\n+  }\n+  log_info(gc, reloc)(\"Using tenuring threshold: %d (%s)\", _tenuring_threshold, reason);\n+}\n+\n+uint ZGenerationYoung::compute_tenuring_threshold(ZRelocationSetSelectorStats stats) {\n+  size_t young_live_total = 0;\n+  size_t young_live_last = 0;\n+  double young_life_expectancy_sum = 0.0;\n+  uint young_life_expectancy_samples = 0;\n+  uint last_populated_age = 0;\n+  size_t last_populated_live = 0;\n+\n+  for (uint i = 0; i <= ZPageAgeMax; ++i) {\n+    const ZPageAge age = static_cast<ZPageAge>(i);\n+    const size_t young_live = stats.small(age).live() + stats.medium(age).live() + stats.large(age).live();\n+    if (young_live > 0) {\n+      last_populated_age = i;\n+      last_populated_live = young_live;\n+      if (young_live_last > 0) {\n+        young_life_expectancy_sum += double(young_live) \/ double(young_live_last);\n+        young_life_expectancy_samples++;\n+      }\n+    }\n+    young_live_total += young_live;\n+    young_live_last = young_live;\n+  }\n+\n+  if (young_live_total == 0) {\n+    return 0;\n+  }\n+\n+  const size_t young_used_at_mark_start = ZGeneration::young()->stat_heap()->used_generation_at_mark_start();\n+  const size_t young_garbage = ZGeneration::young()->stat_heap()->garbage_at_mark_end();\n+  const size_t young_allocated = ZGeneration::young()->stat_heap()->allocated_at_mark_end();\n+  const size_t soft_max_capacity = ZHeap::heap()->soft_max_capacity();\n+\n+  \/\/ The life expectancy shows by what factor on average one age changes between\n+  \/\/ two ages in the age table. Values below 1 indicate generational behaviour where\n+  \/\/ the live bytes is shrinking from age to age. Values at or above 1 indicate\n+  \/\/ anti-generational patterns where the live bytes isn't going down or grows\n+  \/\/ from age to age.\n+  const double young_life_expectancy = young_life_expectancy_samples == 0 ? 1.0 : young_life_expectancy_sum \/ young_life_expectancy_samples;\n+\n+  \/\/ The life decay factor is the reciprocal of the life expectancy. Therefore,\n+  \/\/ values at or below 1 indicate anti-generational behaviour where the live\n+  \/\/ bytes either stays the same or grows from age to age. Conversely, values\n+  \/\/ above 1 indicate generational behaviour where the live bytes shrinks from\n+  \/\/ age to age. The more it shrinks from age to age, the higher the value.\n+  \/\/ Therefore, the higher this value is, the higher we want the tenuring\n+  \/\/ threshold to be, as we exponentially avoid promotions to the old generation.\n+  const double young_life_decay_factor = 1.0 \/ young_life_expectancy;\n+\n+  \/\/ The young residency reciprocal indicates the inverse of how small the\n+  \/\/ resident part of the young generation is compared to the entire heap. Values\n+  \/\/ below 1 indicate it is relatively big. Conversely, values above 1 indicate\n+  \/\/ it is relatively small.\n+  const double young_residency_reciprocal = double(soft_max_capacity) \/ double(young_live_total);\n+\n+  \/\/ The old residency factor clamps the old residency reciprocal to\n+  \/\/ at least 1. That implies this factor is 1 unless the resident memory of\n+  \/\/ the old generation is small compared to the residency of the heap. The\n+  \/\/ smaller the old generation is, the higher this value is. The reasoning\n+  \/\/ is that the less memory that is resident in the old generation, the less\n+  \/\/ point there is in promoting objects to the old generation, as the amount\n+  \/\/ of work it removes from the young generation collections becomes less\n+  \/\/ and less valuable, the smaller the old generation is.\n+  const double young_residency_factor = MAX2(young_residency_reciprocal, 1.0);\n+\n+  \/\/ The allocated to garbage ratio, compares the ratio of newly allocated\n+  \/\/ memory since GC started to how much garbage we are freeing up. The higher\n+  \/\/ the value, the harder it is for the YC to keep up with the allocation rate.\n+  const double allocated_garbage_ratio = double(young_allocated) \/ double(young_garbage + 1);\n+\n+  \/\/ We slow down the young residency factor with a log. A larger log slows\n+  \/\/ it down faster. We select a log between 2 - 16 scaled by the allocated\n+  \/\/ to garbage factor. This selects a larger log when the GC has a harder\n+  \/\/ time keeping up, which causes more promotions to the old generation,\n+  \/\/ making the young collections faster so they can catch up.\n+  const double young_log = MAX2(MIN2(allocated_garbage_ratio, 1.0) * 16, 2.0);\n+\n+  \/\/ The young log residency is essentially the young residency factor, but slowed\n+  \/\/ down by the log_{young_log}(X) function described above.\n+  const double young_log_residency = log(young_residency_factor) \/ log(young_log);\n+\n+  \/\/ The tenuring threshold is computed as the young life decay factor times\n+  \/\/ the young residency factor. That takes into consideration that the\n+  \/\/ value should be higher the more generational the age table is, and higher\n+  \/\/ the more insignificant the footprint of young resident memory is, yet breaks\n+  \/\/ if the GC is finding it hard to keep up with the allocation rate.\n+  const double tenuring_threshold_raw = young_life_decay_factor * young_log_residency;\n+\n+  log_trace(gc, reloc)(\"Young Allocated: \" SIZE_FORMAT \"M\", young_allocated \/ M);\n+  log_trace(gc, reloc)(\"Young Garbage: \" SIZE_FORMAT \"M\", young_garbage \/ M);\n+  log_debug(gc, reloc)(\"Allocated To Garbage: %.1f\", allocated_garbage_ratio);\n+  log_trace(gc, reloc)(\"Young Log: %.1f\", young_log);\n+  log_trace(gc, reloc)(\"Young Residency Reciprocal: %.1f\", young_residency_reciprocal);\n+  log_trace(gc, reloc)(\"Young Residency Factor: %.1f\", young_residency_factor);\n+  log_debug(gc, reloc)(\"Young Log Residency: %.1f\", young_log_residency);\n+  log_debug(gc, reloc)(\"Life Decay Factor: %.1f\", young_life_decay_factor);\n+\n+  \/\/ Round to an integer as we can't have non-integral tenuring threshold.\n+  const uint upper_bound = MIN2(last_populated_age + 1u, (uint)MaxTenuringThreshold);\n+  const uint lower_bound = MIN2(1u, upper_bound);\n+  const uint tenuring_threshold = clamp((uint)round(tenuring_threshold_raw), lower_bound, upper_bound);\n+\n+  return tenuring_threshold;\n+}\n+\n+void ZGenerationYoung::concurrent_select_relocation_set() {\n+  ZStatTimerYoung timer(ZPhaseConcurrentSelectRelocationSetYoung);\n+  const bool promote_all = type() == ZYoungType::major_full_preclean;\n+  select_relocation_set(_id, promote_all);\n+}\n+\n+class VM_ZRelocateStartYoung : public VM_ZOperation {\n+public:\n+  virtual VMOp_Type type() const {\n+    return VMOp_ZRelocateStartYoung;\n+  }\n+\n+  virtual bool block_jni_critical() const {\n+    return true;\n+  }\n+\n+  virtual bool do_operation() {\n+    ZStatTimerYoung timer(ZPhasePauseRelocateStartYoung);\n+    ZServiceabilityPauseTracer tracer;\n+\n+    ZGeneration::young()->relocate_start();\n+\n+    return true;\n+  }\n+};\n+\n+void ZGenerationYoung::pause_relocate_start() {\n+  VM_ZRelocateStartYoung().pause();\n+}\n+\n+void ZGenerationYoung::concurrent_relocate() {\n+  ZStatTimerYoung timer(ZPhaseConcurrentRelocatedYoung);\n+  relocate();\n+}\n+\n+void ZGenerationYoung::mark_start() {\n+  assert(SafepointSynchronize::is_at_safepoint(), \"Should be at safepoint\");\n+\n+  \/\/ Change good colors\n+  flip_mark_start();\n+\n+  \/\/ Retire allocating pages\n+  ZAllocator::eden()->retire_pages();\n+  for (ZPageAge i = ZPageAge::survivor1; i <= ZPageAge::survivor14; i = static_cast<ZPageAge>(static_cast<uint>(i) + 1)) {\n+    ZAllocator::relocation(i)->retire_pages();\n+  }\n+\n+  \/\/ Reset allocated\/reclaimed\/used statistics\n+  reset_statistics();\n+\n+  \/\/ Increment sequence number\n+  _seqnum++;\n+\n+  \/\/ Enter mark phase\n+  set_phase(Phase::Mark);\n+\n+  \/\/ Reset marking information and mark roots\n+  _mark.start();\n+\n+  \/\/ Flip remembered set bits\n+  _remembered.flip();\n+\n+  \/\/ Update statistics\n+  stat_heap()->at_mark_start(_page_allocator->stats(this));\n+}\n+\n+void ZGenerationYoung::mark_roots() {\n+  ZStatTimerYoung timer(ZSubPhaseConcurrentMarkRootsYoung);\n+  _mark.mark_young_roots();\n+}\n+\n+void ZGenerationYoung::mark_follow() {\n+  \/\/ Combine following with scanning the remembered set\n+  ZStatTimerYoung timer(ZSubPhaseConcurrentMarkFollowYoung);\n+  _remembered.scan_and_follow(&_mark);\n+}\n+\n+bool ZGenerationYoung::mark_end() {\n+  assert(SafepointSynchronize::is_at_safepoint(), \"Should be at safepoint\");\n+\n+  \/\/ End marking\n+  if (!_mark.end()) {\n+    \/\/ Marking not completed, continue concurrent mark\n+    return false;\n+  }\n+\n+  \/\/ Enter mark completed phase\n+  set_phase(Phase::MarkComplete);\n+\n+  \/\/ Update statistics\n+  stat_heap()->at_mark_end(_page_allocator->stats(this));\n+\n+  \/\/ Notify JVMTI that some tagmap entry objects may have died.\n+  JvmtiTagMap::set_needs_cleaning();\n+\n+  return true;\n+}\n+\n+void ZGenerationYoung::relocate_start() {\n+  assert(SafepointSynchronize::is_at_safepoint(), \"Should be at safepoint\");\n+\n+  \/\/ Change good colors\n+  flip_relocate_start();\n+\n+  \/\/ Enter relocate phase\n+  set_phase(Phase::Relocate);\n+\n+  \/\/ Update statistics\n+  stat_heap()->at_relocate_start(_page_allocator->stats(this));\n+\n+  _relocate.start();\n+}\n+\n+void ZGenerationYoung::relocate() {\n+  \/\/ Relocate relocation set\n+  _relocate.relocate(&_relocation_set);\n+\n+  \/\/ Update statistics\n+  stat_heap()->at_relocate_end(_page_allocator->stats(this), should_record_stats());\n+}\n+\n+void ZGenerationYoung::flip_promote(ZPage* from_page, ZPage* to_page) {\n+  _page_table->replace(from_page, to_page);\n+\n+  \/\/ Update statistics\n+  _page_allocator->promote_used(from_page->size());\n+  increase_freed(from_page->size());\n+  increase_promoted(from_page->live_bytes());\n+}\n+\n+void ZGenerationYoung::in_place_relocate_promote(ZPage* from_page, ZPage* to_page) {\n+  _page_table->replace(from_page, to_page);\n+\n+  \/\/ Update statistics\n+  _page_allocator->promote_used(from_page->size());\n+}\n+\n+void ZGenerationYoung::register_flip_promoted(const ZArray<ZPage*>& pages) {\n+  _relocation_set.register_flip_promoted(pages);\n+}\n+\n+void ZGenerationYoung::register_in_place_relocate_promoted(ZPage* page) {\n+  _relocation_set.register_in_place_relocate_promoted(page);\n+}\n+\n+void ZGenerationYoung::register_with_remset(ZPage* page) {\n+  _remembered.register_found_old(page);\n+}\n+\n+ZGenerationTracer* ZGenerationYoung::jfr_tracer() {\n+  return &_jfr_tracer;\n+}\n+\n+ZGenerationOld::ZGenerationOld(ZPageTable* page_table, ZPageAllocator* page_allocator) :\n+    ZGeneration(ZGenerationId::old, page_table, page_allocator),\n+    _reference_processor(&_workers),\n+    _weak_roots_processor(&_workers),\n+    _unload(&_workers),\n+    _total_collections_at_start(0),\n+    _young_seqnum_at_reloc_start(0),\n+    _jfr_tracer() {\n+  ZGeneration::_old = this;\n+}\n+\n+class ZGenerationCollectionScopeOld : public StackObj {\n+private:\n+  ZStatTimer      _stat_timer;\n+  ZDriverUnlocker _unlocker;\n+\n+public:\n+  ZGenerationCollectionScopeOld(ConcurrentGCTimer* gc_timer) :\n+      _stat_timer(ZPhaseGenerationOld, gc_timer),\n+      _unlocker() {\n+    \/\/ Update statistics and set the GC timer\n+    ZGeneration::old()->at_collection_start(gc_timer);\n+  }\n+\n+  ~ZGenerationCollectionScopeOld() {\n+    \/\/ Update statistics and clear the GC timer\n+    ZGeneration::old()->at_collection_end();\n+  }\n+};\n+\n+bool ZGenerationOld::should_record_stats() {\n+  return true;\n+}\n+\n+void ZGenerationOld::collect(ConcurrentGCTimer* timer) {\n+  ZGenerationCollectionScopeOld scope(timer);\n+\n+  \/\/ Phase 1: Concurrent Mark\n+  concurrent_mark();\n+\n+  abortpoint();\n+\n+  \/\/ Phase 2: Pause Mark End\n+  while (!pause_mark_end()) {\n+    \/\/ Phase 2.5: Concurrent Mark Continue\n+    concurrent_mark_continue();\n+\n+    abortpoint();\n+  }\n+\n+  \/\/ Phase 3: Concurrent Mark Free\n+  concurrent_mark_free();\n+\n+  abortpoint();\n+\n+  \/\/ Phase 4: Concurrent Process Non-Strong References\n+  concurrent_process_non_strong_references();\n+\n+  abortpoint();\n+\n+  \/\/ Phase 5: Concurrent Reset Relocation Set\n+  concurrent_reset_relocation_set();\n+\n+  abortpoint();\n+\n+  \/\/ Phase 6: Pause Verify\n+  pause_verify();\n+\n+  \/\/ Phase 7: Concurrent Select Relocation Set\n+  concurrent_select_relocation_set();\n+\n+  abortpoint();\n+\n+  {\n+    ZDriverLocker locker;\n+\n+    \/\/ Phase 8: Concurrent Remap Roots\n+    concurrent_remap_young_roots();\n+\n+    abortpoint();\n+\n+    \/\/ Phase 9: Pause Relocate Start\n+    pause_relocate_start();\n+  }\n+\n+  \/\/ Note that we can't have an abortpoint here. We need\n+  \/\/ to let concurrent_relocate() call abort_page()\n+  \/\/ on the remaining entries in the relocation set.\n+\n+  \/\/ Phase 10: Concurrent Relocate\n+  concurrent_relocate();\n+}\n+\n+void ZGenerationOld::flip_mark_start() {\n+  ZGlobalsPointers::flip_old_mark_start();\n+  ZBarrierSet::assembler()->patch_barriers();\n+  ZVerify::on_color_flip();\n+}\n+\n+void ZGenerationOld::flip_relocate_start() {\n+  ZGlobalsPointers::flip_old_relocate_start();\n+  ZBarrierSet::assembler()->patch_barriers();\n+  ZVerify::on_color_flip();\n+}\n+\n+void ZGenerationOld::concurrent_mark() {\n+  ZStatTimerOld timer(ZPhaseConcurrentMarkOld);\n+  ZBreakpoint::at_after_marking_started();\n+  mark_roots();\n+  mark_follow();\n+  ZBreakpoint::at_before_marking_completed();\n+}\n+\n+class VM_ZMarkEndOld : public VM_ZOperation {\n+public:\n+  virtual VMOp_Type type() const {\n+    return VMOp_ZMarkEndOld;\n+  }\n+\n+  virtual bool do_operation() {\n+    ZStatTimerOld timer(ZPhasePauseMarkEndOld);\n+    ZServiceabilityPauseTracer tracer;\n+\n+    return ZGeneration::old()->mark_end();\n+  }\n+};\n+\n+bool ZGenerationOld::pause_mark_end() {\n+  return VM_ZMarkEndOld().pause();\n+}\n+\n+void ZGenerationOld::concurrent_mark_continue() {\n+  ZStatTimerOld timer(ZPhaseConcurrentMarkContinueOld);\n+  mark_follow();\n+}\n+\n+void ZGenerationOld::concurrent_mark_free() {\n+  ZStatTimerOld timer(ZPhaseConcurrentMarkFreeOld);\n+  mark_free();\n+}\n+\n+void ZGenerationOld::concurrent_process_non_strong_references() {\n+  ZStatTimerOld timer(ZPhaseConcurrentProcessNonStrongOld);\n+  ZBreakpoint::at_after_reference_processing_started();\n+  process_non_strong_references();\n+}\n+\n+void ZGenerationOld::concurrent_reset_relocation_set() {\n+  ZStatTimerOld timer(ZPhaseConcurrentResetRelocationSetOld);\n+  reset_relocation_set();\n+}\n+\n+class VM_ZVerifyOld : public VM_Operation {\n+public:\n+  virtual VMOp_Type type() const {\n+    return VMOp_ZVerifyOld;\n+  }\n+\n+  virtual bool skip_thread_oop_barriers() const {\n+    return true;\n+  }\n+\n+  virtual void doit() {\n+    ZVerify::after_weak_processing();\n+  }\n+\n+  void pause() {\n+    VMThread::execute(this);\n+  }\n+};\n+\n+void ZGenerationOld::pause_verify() {\n+  \/\/ Note that we block out concurrent young collections when performing the\n+  \/\/ verification. The verification checks that store good oops in the\n+  \/\/ old generation have a corresponding remembered set entry, or is in\n+  \/\/ a store barrier buffer (hence asynchronously creating such entries).\n+  \/\/ That lookup would otherwise race with installation of base pointers\n+  \/\/ into the store barrier buffer. We dodge that race by blocking out\n+  \/\/ young collections during this verification.\n+  if (ZVerifyRoots || ZVerifyObjects) {\n+    \/\/ Limited verification\n+    ZDriverLocker locker;\n+    VM_ZVerifyOld().pause();\n+  }\n+}\n+\n+void ZGenerationOld::concurrent_select_relocation_set() {\n+  ZStatTimerOld timer(ZPhaseConcurrentSelectRelocationSetOld);\n+  select_relocation_set(_id, false \/* promote_all *\/);\n+}\n+\n+class VM_ZRelocateStartOld : public VM_ZOperation {\n+public:\n+  virtual VMOp_Type type() const {\n+    return VMOp_ZRelocateStartOld;\n+  }\n+\n+  virtual bool block_jni_critical() const {\n+    return true;\n+  }\n+\n+  virtual bool do_operation() {\n+    ZStatTimerOld timer(ZPhasePauseRelocateStartOld);\n+    ZServiceabilityPauseTracer tracer;\n+\n+    ZGeneration::old()->relocate_start();\n+\n+    return true;\n+  }\n+};\n+\n+void ZGenerationOld::pause_relocate_start() {\n+  VM_ZRelocateStartOld().pause();\n+}\n+\n+void ZGenerationOld::concurrent_relocate() {\n+  ZStatTimerOld timer(ZPhaseConcurrentRelocatedOld);\n+  relocate();\n+}\n+\n+void ZGenerationOld::concurrent_remap_young_roots() {\n+  ZStatTimerOld timer(ZPhaseConcurrentRemapRootsOld);\n+  remap_young_roots();\n+}\n+\n+void ZGenerationOld::mark_start() {\n+  assert(SafepointSynchronize::is_at_safepoint(), \"Should be at safepoint\");\n+\n+  \/\/ Verification\n+  ClassLoaderDataGraph::verify_claimed_marks_cleared(ClassLoaderData::_claim_strong);\n+\n+  \/\/ Change good colors\n+  flip_mark_start();\n+\n+  \/\/ Retire allocating pages\n+  ZAllocator::old()->retire_pages();\n+\n+  \/\/ Reset allocated\/reclaimed\/used statistics\n+  reset_statistics();\n+\n+  \/\/ Reset encountered\/dropped\/enqueued statistics\n+  _reference_processor.reset_statistics();\n+\n+  \/\/ Increment sequence number\n+  _seqnum++;\n+\n+  \/\/ Enter mark phase\n+  set_phase(Phase::Mark);\n+\n+  \/\/ Reset marking information and mark roots\n+  _mark.start();\n+\n+  \/\/ Update statistics\n+  stat_heap()->at_mark_start(_page_allocator->stats(this));\n+\n+  \/\/ Note that we start a marking cycle.\n+  \/\/ Unlike other GCs, the color switch implicitly changes the nmethods\n+  \/\/ to be armed, and the thread-local disarm values are lazily updated\n+  \/\/ when JavaThreads wake up from safepoints.\n+  CodeCache::on_gc_marking_cycle_start();\n+\n+  _total_collections_at_start = ZCollectedHeap::heap()->total_collections();\n+}\n+\n+void ZGenerationOld::mark_roots() {\n+  ZStatTimerOld timer(ZSubPhaseConcurrentMarkRootsOld);\n+  _mark.mark_old_roots();\n+}\n+\n+void ZGenerationOld::mark_follow() {\n+  ZStatTimerOld timer(ZSubPhaseConcurrentMarkFollowOld);\n+  _mark.mark_follow();\n+}\n+\n+bool ZGenerationOld::mark_end() {\n+  assert(SafepointSynchronize::is_at_safepoint(), \"Should be at safepoint\");\n+\n+  \/\/ Try end marking\n+  if (!_mark.end()) {\n+    \/\/ Marking not completed, continue concurrent mark\n+    return false;\n+  }\n+\n+  \/\/ Enter mark completed phase\n+  set_phase(Phase::MarkComplete);\n+\n+  \/\/ Verify after mark\n+  ZVerify::after_mark();\n+\n+  \/\/ Update statistics\n+  stat_heap()->at_mark_end(_page_allocator->stats(this));\n+\n+  \/\/ Block resurrection of weak\/phantom references\n+  ZResurrection::block();\n+\n+  \/\/ Prepare to unload stale metadata and nmethods\n+  _unload.prepare();\n+\n+  \/\/ Notify JVMTI that some tagmap entry objects may have died.\n+  JvmtiTagMap::set_needs_cleaning();\n+\n+  \/\/ Note that we finished a marking cycle.\n+  \/\/ Unlike other GCs, we do not arm the nmethods\n+  \/\/ when marking terminates.\n+  CodeCache::on_gc_marking_cycle_finish();\n+\n+  return true;\n+}\n+\n+void ZGenerationOld::set_soft_reference_policy(bool clear) {\n+  _reference_processor.set_soft_reference_policy(clear);\n+}\n+\n+class ZRendezvousHandshakeClosure : public HandshakeClosure {\n+public:\n+  ZRendezvousHandshakeClosure() :\n+      HandshakeClosure(\"ZRendezvous\") {}\n+\n+  void do_thread(Thread* thread) {\n+    \/\/ Does nothing\n+  }\n+};\n+\n+class ZRendezvousGCThreads: public VM_Operation {\n+ public:\n+  VMOp_Type type() const { return VMOp_ZRendezvousGCThreads; }\n+\n+  virtual bool evaluate_at_safepoint() const {\n+    \/\/ We only care about synchronizing the GC threads.\n+    \/\/ Leave the Java threads running.\n+    return false;\n+  }\n+\n+  virtual bool skip_thread_oop_barriers() const {\n+    fatal(\"Concurrent VMOps should not call this\");\n+    return true;\n+  }\n+\n+  void doit() {\n+    \/\/ Light weight \"handshake\" of the GC threads\n+    SuspendibleThreadSet::synchronize();\n+    SuspendibleThreadSet::desynchronize();\n+  };\n+};\n+\n+\n+void ZGenerationOld::process_non_strong_references() {\n+  \/\/ Process Soft\/Weak\/Final\/PhantomReferences\n+  _reference_processor.process_references();\n+\n+  \/\/ Process weak roots\n+  _weak_roots_processor.process_weak_roots();\n+\n+  \/\/ Unlink stale metadata and nmethods\n+  _unload.unlink();\n+\n+  \/\/ Perform a handshake. This is needed 1) to make sure that stale\n+  \/\/ metadata and nmethods are no longer observable. And 2), to\n+  \/\/ prevent the race where a mutator first loads an oop, which is\n+  \/\/ logically null but not yet cleared. Then this oop gets cleared\n+  \/\/ by the reference processor and resurrection is unblocked. At\n+  \/\/ this point the mutator could see the unblocked state and pass\n+  \/\/ this invalid oop through the normal barrier path, which would\n+  \/\/ incorrectly try to mark the oop.\n+  ZRendezvousHandshakeClosure cl;\n+  Handshake::execute(&cl);\n+\n+  \/\/ GC threads are not part of the handshake above.\n+  \/\/ Explicitly \"handshake\" them.\n+  ZRendezvousGCThreads op;\n+  VMThread::execute(&op);\n+\n+  \/\/ Unblock resurrection of weak\/phantom references\n+  ZResurrection::unblock();\n+\n+  \/\/ Purge stale metadata and nmethods that were unlinked\n+  _unload.purge();\n+\n+  \/\/ Enqueue Soft\/Weak\/Final\/PhantomReferences. Note that this\n+  \/\/ must be done after unblocking resurrection. Otherwise the\n+  \/\/ Finalizer thread could call Reference.get() on the Finalizers\n+  \/\/ that were just enqueued, which would incorrectly return null\n+  \/\/ during the resurrection block window, since such referents\n+  \/\/ are only Finalizable marked.\n+  _reference_processor.enqueue_references();\n+\n+  \/\/ Clear old markings claim bits.\n+  \/\/ Note: Clearing _claim_strong also clears _claim_finalizable.\n+  ClassLoaderDataGraph::clear_claimed_marks(ClassLoaderData::_claim_strong);\n+}\n+\n+void ZGenerationOld::relocate_start() {\n+  assert(SafepointSynchronize::is_at_safepoint(), \"Should be at safepoint\");\n+\n+  \/\/ Finish unloading stale metadata and nmethods\n+  _unload.finish();\n+\n+  \/\/ Change good colors\n+  flip_relocate_start();\n+\n+  \/\/ Enter relocate phase\n+  set_phase(Phase::Relocate);\n+\n+  \/\/ Update statistics\n+  stat_heap()->at_relocate_start(_page_allocator->stats(this));\n+\n+  \/\/ Need to know the remset parity when relocating objects\n+  _young_seqnum_at_reloc_start = ZGeneration::young()->seqnum();\n+\n+  _relocate.start();\n+}\n+\n+void ZGenerationOld::relocate() {\n+  \/\/ Relocate relocation set\n+  _relocate.relocate(&_relocation_set);\n+\n+  \/\/ Update statistics\n+  stat_heap()->at_relocate_end(_page_allocator->stats(this), should_record_stats());\n+}\n+\n+class ZRemapOopClosure : public OopClosure {\n+public:\n+  virtual void do_oop(oop* p) {\n+    ZBarrier::load_barrier_on_oop_field((volatile zpointer*)p);\n+  }\n+\n+  virtual void do_oop(narrowOop* p) {\n+    ShouldNotReachHere();\n+  }\n+};\n+\n+class ZRemapThreadClosure : public ThreadClosure {\n+public:\n+  virtual void do_thread(Thread* thread) {\n+    JavaThread* const jt = JavaThread::cast(thread);\n+    StackWatermarkSet::finish_processing(jt, nullptr, StackWatermarkKind::gc);\n+  }\n+};\n+\n+class ZRemapNMethodClosure : public NMethodClosure {\n+private:\n+  ZBarrierSetNMethod* const _bs_nm;\n+\n+public:\n+  ZRemapNMethodClosure() :\n+      _bs_nm(static_cast<ZBarrierSetNMethod*>(BarrierSet::barrier_set()->barrier_set_nmethod())) {}\n+\n+  virtual void do_nmethod(nmethod* nm) {\n+    ZLocker<ZReentrantLock> locker(ZNMethod::lock_for_nmethod(nm));\n+    if (_bs_nm->is_armed(nm)) {\n+      \/\/ Heal barriers\n+      ZNMethod::nmethod_patch_barriers(nm);\n+\n+      \/\/ Heal oops\n+      ZUncoloredRootProcessOopClosure cl(ZNMethod::color(nm));\n+      ZNMethod::nmethod_oops_do_inner(nm, &cl);\n+\n+      log_trace(gc, nmethod)(\"nmethod: \" PTR_FORMAT \" visited by old remapping\", p2i(nm));\n+\n+      \/\/ Disarm\n+      _bs_nm->disarm(nm);\n+    }\n+  }\n+};\n+\n+typedef ClaimingCLDToOopClosure<ClassLoaderData::_claim_none> ZRemapCLDClosure;\n+\n+class ZRemapYoungRootsTask : public ZTask {\n+private:\n+  ZGenerationPagesParallelIterator _old_pages_parallel_iterator;\n+\n+  ZRootsIteratorAllColored         _roots_colored;\n+  ZRootsIteratorAllUncolored       _roots_uncolored;\n+\n+  ZRemapOopClosure                 _cl_colored;\n+  ZRemapCLDClosure                 _cld_cl;\n+\n+  ZRemapThreadClosure              _thread_cl;\n+  ZRemapNMethodClosure             _nm_cl;\n+\n+public:\n+  ZRemapYoungRootsTask(ZPageTable* page_table, ZPageAllocator* page_allocator) :\n+      ZTask(\"ZRemapYoungRootsTask\"),\n+      _old_pages_parallel_iterator(page_table, ZGenerationId::old, page_allocator),\n+      _roots_colored(ZGenerationIdOptional::old),\n+      _roots_uncolored(ZGenerationIdOptional::old),\n+      _cl_colored(),\n+      _cld_cl(&_cl_colored),\n+      _thread_cl(),\n+      _nm_cl() {\n+    ClassLoaderDataGraph_lock->lock();\n+  }\n+\n+  ~ZRemapYoungRootsTask() {\n+    ClassLoaderDataGraph_lock->unlock();\n+  }\n+\n+  virtual void work() {\n+    {\n+      ZStatTimerWorker timer(ZSubPhaseConcurrentRemapRootsColoredOld);\n+      _roots_colored.apply(&_cl_colored,\n+                           &_cld_cl);\n+    }\n+\n+    {\n+      ZStatTimerWorker timer(ZSubPhaseConcurrentRemapRootsUncoloredOld);\n+      _roots_uncolored.apply(&_thread_cl,\n+                             &_nm_cl);\n+    }\n+\n+    {\n+      ZStatTimerWorker timer(ZSubPhaseConcurrentRemapRememberedOld);\n+      _old_pages_parallel_iterator.do_pages([&](ZPage* page) {\n+        \/\/ Visit all object fields that potentially pointing into young generation\n+        page->oops_do_current_remembered(ZBarrier::load_barrier_on_oop_field);\n+        return true;\n+      });\n+    }\n+  }\n+};\n+\n+\/\/ This function is used by the old generation to purge roots to the young generation from\n+\/\/ young remap bit errors, before the old generation performs old relocate start. By doing\n+\/\/ that, we can know that double remap bit errors don't need to be concerned with double\n+\/\/ remap bit errors, in the young generation roots. That makes it possible to figure out\n+\/\/ which generation table to use when remapping a pointer, without needing an extra adjust\n+\/\/ phase that walks the entire heap.\n+void ZGenerationOld::remap_young_roots() {\n+  \/\/ We upgrade the number of workers to the number last used by the young generation. The\n+  \/\/ reason is that this code is run under the driver lock, which means that a young generation\n+  \/\/ collection might be waiting for this code to complete.\n+  uint prev_nworkers = _workers.active_workers();\n+  uint remap_nworkers = clamp(ZGeneration::young()->workers()->active_workers() + prev_nworkers, 1u, ZOldGCThreads);\n+  _workers.set_active_workers(remap_nworkers);\n+\n+  \/\/ TODO: The STS joiner is only needed to satisfy z_assert_is_barrier_safe that doesn't\n+  \/\/ understand the driver locker. Consider making the assert aware of the driver locker.\n+  SuspendibleThreadSetJoiner sts_joiner;\n+\n+  ZRemapYoungRootsTask task(_page_table, _page_allocator);\n+  workers()->run(&task);\n+  _workers.set_active_workers(prev_nworkers);\n+}\n+\n+uint ZGenerationOld::total_collections_at_start() const {\n+  return _total_collections_at_start;\n+}\n+\n+ZGenerationTracer* ZGenerationOld::jfr_tracer() {\n+  return &_jfr_tracer;\n+}\n","filename":"src\/hotspot\/share\/gc\/z\/zGeneration.cpp","additions":1486,"deletions":0,"binary":false,"changes":1486,"status":"added"},{"patch":"@@ -0,0 +1,322 @@\n+\/*\n+ * Copyright (c) 2021, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_Z_ZGENERATION_HPP\n+#define SHARE_GC_Z_ZGENERATION_HPP\n+\n+#include \"gc\/z\/zForwardingTable.hpp\"\n+#include \"gc\/z\/zGenerationId.hpp\"\n+#include \"gc\/z\/zMark.hpp\"\n+#include \"gc\/z\/zReferenceProcessor.hpp\"\n+#include \"gc\/z\/zRelocate.hpp\"\n+#include \"gc\/z\/zRelocationSet.hpp\"\n+#include \"gc\/z\/zRemembered.hpp\"\n+#include \"gc\/z\/zStat.hpp\"\n+#include \"gc\/z\/zTracer.hpp\"\n+#include \"gc\/z\/zUnload.hpp\"\n+#include \"gc\/z\/zWeakRootsProcessor.hpp\"\n+#include \"gc\/z\/zWorkers.hpp\"\n+#include \"memory\/allocation.hpp\"\n+\n+class ThreadClosure;\n+class ZForwardingTable;\n+class ZGenerationOld;\n+class ZGenerationYoung;\n+class ZPage;\n+class ZPageAllocator;\n+class ZPageTable;\n+class ZRelocationSetSelector;\n+\n+class ZGeneration {\n+  friend class ZForwardingTest;\n+  friend class ZLiveMapTest;\n+\n+protected:\n+  static ZGenerationYoung* _young;\n+  static ZGenerationOld*   _old;\n+\n+  enum class Phase {\n+    Mark,\n+    MarkComplete,\n+    Relocate\n+  };\n+\n+  const ZGenerationId   _id;\n+  ZPageAllocator* const _page_allocator;\n+  ZPageTable* const     _page_table;\n+  ZForwardingTable      _forwarding_table;\n+  ZWorkers              _workers;\n+  ZMark                 _mark;\n+  ZRelocate             _relocate;\n+  ZRelocationSet        _relocation_set;\n+\n+  volatile size_t       _freed;\n+  volatile size_t       _promoted;\n+  volatile size_t       _compacted;\n+\n+  Phase                 _phase;\n+  uint32_t              _seqnum;\n+\n+  ZStatHeap             _stat_heap;\n+  ZStatCycle            _stat_cycle;\n+  ZStatWorkers          _stat_workers;\n+  ZStatMark             _stat_mark;\n+  ZStatRelocation       _stat_relocation;\n+\n+  ConcurrentGCTimer*    _gc_timer;\n+\n+  void free_empty_pages(ZRelocationSetSelector* selector, int bulk);\n+  void flip_age_pages(const ZRelocationSetSelector* selector);\n+  void flip_age_pages(const ZArray<ZPage*>* pages);\n+\n+  void mark_free();\n+\n+  void select_relocation_set(ZGenerationId generation, bool promote_all);\n+  void reset_relocation_set();\n+\n+  ZGeneration(ZGenerationId id, ZPageTable* page_table, ZPageAllocator* page_allocator);\n+\n+  void log_phase_switch(Phase from, Phase to);\n+\n+public:\n+  bool is_initialized() const;\n+\n+  \/\/ GC phases\n+  void set_phase(Phase new_phase);\n+  bool is_phase_relocate() const;\n+  bool is_phase_mark() const;\n+  bool is_phase_mark_complete() const;\n+  const char* phase_to_string() const;\n+\n+  uint32_t seqnum() const;\n+\n+  ZGenerationId id() const;\n+  ZGenerationIdOptional id_optional() const;\n+  bool is_young() const;\n+  bool is_old() const;\n+\n+  static ZGenerationYoung* young();\n+  static ZGenerationOld* old();\n+  static ZGeneration* generation(ZGenerationId id);\n+\n+  \/\/ Statistics\n+  void reset_statistics();\n+  virtual bool should_record_stats() = 0;\n+  ssize_t freed() const;\n+  void increase_freed(size_t size);\n+  size_t promoted() const;\n+  void increase_promoted(size_t size);\n+  size_t compacted() const;\n+  void increase_compacted(size_t size);\n+\n+  ConcurrentGCTimer* gc_timer() const;\n+  void set_gc_timer(ConcurrentGCTimer* gc_timer);\n+  void clear_gc_timer();\n+\n+  ZStatHeap* stat_heap();\n+  ZStatCycle* stat_cycle();\n+  ZStatWorkers* stat_workers();\n+  ZStatMark* stat_mark();\n+  ZStatRelocation* stat_relocation();\n+\n+  void at_collection_start(ConcurrentGCTimer* gc_timer);\n+  void at_collection_end();\n+\n+  \/\/ Workers\n+  ZWorkers* workers();\n+  uint active_workers() const;\n+  void set_active_workers(uint nworkers);\n+\n+  \/\/ Worker resizing\n+  bool should_worker_resize();\n+\n+  ZPageTable* page_table() const;\n+  const ZForwardingTable* forwarding_table() const;\n+  ZForwarding* forwarding(zaddress_unsafe addr) const;\n+\n+  ZRelocationSetParallelIterator relocation_set_parallel_iterator();\n+\n+  \/\/ Marking\n+  template <bool resurrect, bool gc_thread, bool follow, bool finalizable>\n+  void mark_object(zaddress addr);\n+  template <bool resurrect, bool gc_thread, bool follow, bool finalizable>\n+  void mark_object_if_active(zaddress addr);\n+  void mark_flush_and_free(Thread* thread);\n+\n+  \/\/ Relocation\n+  void synchronize_relocation();\n+  void desynchronize_relocation();\n+  zaddress relocate_or_remap_object(zaddress_unsafe addr);\n+  zaddress remap_object(zaddress_unsafe addr);\n+\n+  \/\/ Threads\n+  void threads_do(ThreadClosure* tc) const;\n+};\n+\n+enum class ZYoungType {\n+  minor,\n+  major_full_preclean,\n+  major_full_roots,\n+  major_partial_roots,\n+  none\n+};\n+\n+class ZYoungTypeSetter {\n+public:\n+  ZYoungTypeSetter(ZYoungType type);\n+  ~ZYoungTypeSetter();\n+};\n+\n+class ZGenerationYoung : public ZGeneration {\n+  friend class VM_ZMarkEndYoung;\n+  friend class VM_ZMarkStartYoung;\n+  friend class VM_ZMarkStartYoungAndOld;\n+  friend class VM_ZRelocateStartYoung;\n+  friend class ZYoungTypeSetter;\n+\n+private:\n+  ZYoungType   _active_type;\n+  uint         _tenuring_threshold;\n+  ZRemembered  _remembered;\n+  ZYoungTracer _jfr_tracer;\n+\n+  void flip_mark_start();\n+  void flip_relocate_start();\n+\n+  void mark_start();\n+  void mark_roots();\n+  void mark_follow();\n+  bool mark_end();\n+  void relocate_start();\n+  void relocate();\n+\n+  void pause_mark_start();\n+  void concurrent_mark();\n+  bool pause_mark_end();\n+  void concurrent_mark_continue();\n+  void concurrent_mark_free();\n+  void concurrent_reset_relocation_set();\n+  void concurrent_select_relocation_set();\n+  void pause_relocate_start();\n+  void concurrent_relocate();\n+\n+public:\n+  ZGenerationYoung(ZPageTable* page_table,\n+                   const ZForwardingTable* old_forwarding_table,\n+                   ZPageAllocator* page_allocator);\n+\n+  ZYoungType type() const;\n+\n+  void collect(ZYoungType type, ConcurrentGCTimer* timer);\n+\n+  \/\/ Statistics\n+  bool should_record_stats();\n+\n+  \/\/ Support for promoting object to the old generation\n+  void flip_promote(ZPage* from_page, ZPage* to_page);\n+  void in_place_relocate_promote(ZPage* from_page, ZPage* to_page);\n+\n+  void register_flip_promoted(const ZArray<ZPage*>& pages);\n+  void register_in_place_relocate_promoted(ZPage* page);\n+\n+  uint tenuring_threshold();\n+  void select_tenuring_threshold(ZRelocationSetSelectorStats stats, bool promote_all);\n+  uint compute_tenuring_threshold(ZRelocationSetSelectorStats stats);\n+\n+  \/\/ Add remembered set entries\n+  void remember(volatile zpointer* p);\n+  void remember_fields(zaddress addr);\n+\n+  \/\/ Scan a remembered set entry\n+  void scan_remembered_field(volatile zpointer* p);\n+\n+  \/\/ Register old pages with remembered set\n+  void register_with_remset(ZPage* page);\n+\n+  \/\/ Serviceability\n+  ZGenerationTracer* jfr_tracer();\n+\n+  \/\/ Verification\n+  bool is_remembered(volatile zpointer* p) const;\n+};\n+\n+class ZGenerationOld : public ZGeneration {\n+  friend class VM_ZMarkEndOld;\n+  friend class VM_ZMarkStartYoungAndOld;\n+  friend class VM_ZRelocateStartOld;\n+\n+private:\n+  ZReferenceProcessor _reference_processor;\n+  ZWeakRootsProcessor _weak_roots_processor;\n+  ZUnload             _unload;\n+  uint                _total_collections_at_start;\n+  uint32_t            _young_seqnum_at_reloc_start;\n+  ZOldTracer          _jfr_tracer;\n+\n+  void flip_mark_start();\n+  void flip_relocate_start();\n+\n+  void mark_start();\n+  void mark_roots();\n+  void mark_follow();\n+  bool mark_end();\n+  void process_non_strong_references();\n+  void relocate_start();\n+  void relocate();\n+  void remap_young_roots();\n+\n+  void concurrent_mark();\n+  bool pause_mark_end();\n+  void concurrent_mark_continue();\n+  void concurrent_mark_free();\n+  void concurrent_process_non_strong_references();\n+  void concurrent_reset_relocation_set();\n+  void pause_verify();\n+  void concurrent_select_relocation_set();\n+  void pause_relocate_start();\n+  void concurrent_relocate();\n+  void concurrent_remap_young_roots();\n+\n+public:\n+  ZGenerationOld(ZPageTable* page_table, ZPageAllocator* page_allocator);\n+\n+  void collect(ConcurrentGCTimer* timer);\n+\n+  \/\/ Statistics\n+  bool should_record_stats();\n+\n+  \/\/ Reference processing\n+  ReferenceDiscoverer* reference_discoverer();\n+  void set_soft_reference_policy(bool clear);\n+\n+  uint total_collections_at_start() const;\n+\n+  bool active_remset_is_current() const;\n+\n+  ZRelocateQueue* relocate_queue();\n+\n+  \/\/ Serviceability\n+  ZGenerationTracer* jfr_tracer();\n+};\n+\n+#endif \/\/ SHARE_GC_Z_ZGENERATION_HPP\n","filename":"src\/hotspot\/share\/gc\/z\/zGeneration.hpp","additions":322,"deletions":0,"binary":false,"changes":322,"status":"added"},{"patch":"@@ -0,0 +1,188 @@\n+\/*\n+ * Copyright (c) 2021, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_Z_ZGENERATION_INLINE_HPP\n+#define SHARE_GC_Z_ZGENERATION_INLINE_HPP\n+\n+#include \"gc\/z\/zGeneration.hpp\"\n+\n+#include \"gc\/z\/zAbort.inline.hpp\"\n+#include \"gc\/z\/zHeap.inline.hpp\"\n+#include \"gc\/z\/zWorkers.inline.hpp\"\n+#include \"utilities\/debug.hpp\"\n+\n+inline bool ZGeneration::is_phase_relocate() const {\n+  return _phase == Phase::Relocate;\n+}\n+\n+inline bool ZGeneration::is_phase_mark() const {\n+  return _phase == Phase::Mark;\n+}\n+\n+inline bool ZGeneration::is_phase_mark_complete() const {\n+  return _phase == Phase::MarkComplete;\n+}\n+\n+inline uint32_t ZGeneration::seqnum() const {\n+  return _seqnum;\n+}\n+\n+inline ZGenerationId ZGeneration::id() const {\n+  return _id;\n+}\n+\n+inline ZGenerationIdOptional ZGeneration::id_optional() const {\n+  return static_cast<ZGenerationIdOptional>(_id);\n+}\n+\n+inline bool ZGeneration::is_young() const {\n+  return _id == ZGenerationId::young;\n+}\n+\n+inline bool ZGeneration::is_old() const {\n+  return _id == ZGenerationId::old;\n+}\n+\n+inline ZGenerationYoung* ZGeneration::young() {\n+  return _young;\n+}\n+\n+inline ZGenerationOld* ZGeneration::old() {\n+  return _old;\n+}\n+\n+inline ZGeneration* ZGeneration::generation(ZGenerationId id) {\n+  if (id == ZGenerationId::young) {\n+    return _young;\n+  } else {\n+    return _old;\n+  }\n+}\n+\n+inline ZForwarding* ZGeneration::forwarding(zaddress_unsafe addr) const {\n+  return _forwarding_table.get(addr);\n+}\n+\n+inline bool ZGeneration::should_worker_resize() {\n+  return _workers.should_worker_resize();\n+}\n+\n+inline ZStatHeap* ZGeneration::stat_heap() {\n+  return &_stat_heap;\n+}\n+\n+inline ZStatCycle* ZGeneration::stat_cycle() {\n+  return &_stat_cycle;\n+}\n+\n+inline ZStatWorkers* ZGeneration::stat_workers() {\n+  return &_stat_workers;\n+}\n+\n+inline ZStatMark* ZGeneration::stat_mark() {\n+  return &_stat_mark;\n+}\n+\n+inline ZStatRelocation* ZGeneration::stat_relocation() {\n+  return &_stat_relocation;\n+}\n+\n+inline ZPageTable* ZGeneration::page_table() const {\n+  return _page_table;\n+}\n+\n+inline const ZForwardingTable* ZGeneration::forwarding_table() const {\n+  return &_forwarding_table;\n+}\n+\n+template <bool resurrect, bool gc_thread, bool follow, bool finalizable>\n+inline void ZGeneration::mark_object(zaddress addr) {\n+  assert(is_phase_mark(), \"Should be marking\");\n+  _mark.mark_object<resurrect, gc_thread, follow, finalizable>(addr);\n+}\n+\n+template <bool resurrect, bool gc_thread, bool follow, bool finalizable>\n+inline void ZGeneration::mark_object_if_active(zaddress addr) {\n+  if (is_phase_mark()) {\n+    mark_object<resurrect, gc_thread, follow, finalizable>(addr);\n+  }\n+}\n+\n+inline zaddress ZGeneration::relocate_or_remap_object(zaddress_unsafe addr) {\n+  ZForwarding* const forwarding = _forwarding_table.get(addr);\n+  if (forwarding == nullptr) {\n+    \/\/ Not forwarding\n+    return safe(addr);\n+  }\n+\n+  \/\/ Relocate object\n+  return _relocate.relocate_object(forwarding, addr);\n+}\n+\n+inline zaddress ZGeneration::remap_object(zaddress_unsafe addr) {\n+  ZForwarding* const forwarding = _forwarding_table.get(addr);\n+  if (forwarding == nullptr) {\n+    \/\/ Not forwarding\n+    return safe(addr);\n+  }\n+\n+  \/\/ Remap object\n+  return _relocate.forward_object(forwarding, addr);\n+}\n+\n+inline ZYoungType ZGenerationYoung::type() const {\n+  assert(_active_type != ZYoungType::none, \"Invalid type\");\n+  return _active_type;\n+}\n+\n+inline void ZGenerationYoung::remember(volatile zpointer* p) {\n+  _remembered.remember(p);\n+}\n+\n+inline void ZGenerationYoung::scan_remembered_field(volatile zpointer* p) {\n+  _remembered.scan_field(p);\n+}\n+\n+inline bool ZGenerationYoung::is_remembered(volatile zpointer* p) const {\n+  return _remembered.is_remembered(p);\n+}\n+\n+inline ReferenceDiscoverer* ZGenerationOld::reference_discoverer() {\n+  return &_reference_processor;\n+}\n+\n+inline bool ZGenerationOld::active_remset_is_current() const {\n+  assert(_young_seqnum_at_reloc_start != 0, \"Must be set before used\");\n+\n+  \/\/ The remembered set bits flip every time a new young collection starts\n+  const uint32_t seqnum = ZGeneration::young()->seqnum();\n+  const uint32_t seqnum_diff = seqnum - _young_seqnum_at_reloc_start;\n+  const bool in_current = (seqnum_diff & 1u) == 0u;\n+  return in_current;\n+}\n+\n+inline ZRelocateQueue* ZGenerationOld::relocate_queue() {\n+  return _relocate.queue();\n+}\n+\n+#endif \/\/ SHARE_GC_Z_ZGENERATION_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/z\/zGeneration.inline.hpp","additions":188,"deletions":0,"binary":false,"changes":188,"status":"added"},{"patch":"@@ -0,0 +1,40 @@\n+\/*\n+ * Copyright (c) 2020, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_Z_ZGENERATIONID_HPP\n+#define SHARE_GC_Z_ZGENERATIONID_HPP\n+\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+enum class ZGenerationId : uint8_t {\n+  young,\n+  old\n+};\n+\n+enum class ZGenerationIdOptional : uint8_t {\n+  young,\n+  old,\n+  none\n+};\n+\n+#endif \/\/ SHARE_GC_Z_ZGENERATIONID_HPP\n","filename":"src\/hotspot\/share\/gc\/z\/zGenerationId.hpp","additions":40,"deletions":0,"binary":false,"changes":40,"status":"added"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,3 +27,0 @@\n-uint32_t   ZGlobalPhase                = ZPhaseRelocate;\n-uint32_t   ZGlobalSeqNum               = 1;\n-\n@@ -40,40 +37,0 @@\n-\n-uintptr_t  ZAddressGoodMask;\n-uintptr_t  ZAddressBadMask;\n-uintptr_t  ZAddressWeakBadMask;\n-\n-static uint32_t* ZAddressCalculateBadMaskHighOrderBitsAddr() {\n-  const uintptr_t addr = reinterpret_cast<uintptr_t>(&ZAddressBadMask);\n-  return reinterpret_cast<uint32_t*>(addr + ZAddressBadMaskHighOrderBitsOffset);\n-}\n-\n-uint32_t*  ZAddressBadMaskHighOrderBitsAddr = ZAddressCalculateBadMaskHighOrderBitsAddr();\n-\n-size_t     ZAddressOffsetBits;\n-uintptr_t  ZAddressOffsetMask;\n-size_t     ZAddressOffsetMax;\n-\n-size_t     ZAddressMetadataShift;\n-uintptr_t  ZAddressMetadataMask;\n-\n-uintptr_t  ZAddressMetadataMarked;\n-uintptr_t  ZAddressMetadataMarked0;\n-uintptr_t  ZAddressMetadataMarked1;\n-uintptr_t  ZAddressMetadataRemapped;\n-uintptr_t  ZAddressMetadataFinalizable;\n-\n-const char* ZGlobalPhaseToString() {\n-  switch (ZGlobalPhase) {\n-  case ZPhaseMark:\n-    return \"Mark\";\n-\n-  case ZPhaseMarkCompleted:\n-    return \"MarkCompleted\";\n-\n-  case ZPhaseRelocate:\n-    return \"Relocate\";\n-\n-  default:\n-    return \"Unknown\";\n-  }\n-}\n","filename":"src\/hotspot\/share\/gc\/z\/zGlobals.cpp","additions":1,"deletions":44,"binary":false,"changes":45,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -34,10 +34,0 @@\n-\/\/ Global phase state\n-extern uint32_t   ZGlobalPhase;\n-const uint32_t    ZPhaseMark                    = 0;\n-const uint32_t    ZPhaseMarkCompleted           = 1;\n-const uint32_t    ZPhaseRelocate                = 2;\n-const char*       ZGlobalPhaseToString();\n-\n-\/\/ Global sequence number\n-extern uint32_t   ZGlobalSeqNum;\n-\n@@ -48,3 +38,0 @@\n-\/\/ Number of heap views\n-const size_t      ZHeapViews                    = ZPlatformHeapViews;\n-\n@@ -54,5 +41,0 @@\n-\/\/ Page types\n-const uint8_t     ZPageTypeSmall                = 0;\n-const uint8_t     ZPageTypeMedium               = 1;\n-const uint8_t     ZPageTypeLarge                = 2;\n-\n@@ -81,41 +63,0 @@\n-\/\/\n-\/\/ Good\/Bad mask states\n-\/\/ --------------------\n-\/\/\n-\/\/                 GoodMask         BadMask          WeakGoodMask     WeakBadMask\n-\/\/                 --------------------------------------------------------------\n-\/\/  Marked0        001              110              101              010\n-\/\/  Marked1        010              101              110              001\n-\/\/  Remapped       100              011              100              011\n-\/\/\n-\n-\/\/ Good\/bad masks\n-extern uintptr_t  ZAddressGoodMask;\n-extern uintptr_t  ZAddressBadMask;\n-extern uintptr_t  ZAddressWeakBadMask;\n-\n-\/\/ The bad mask is 64 bit. Its high order 32 bits contain all possible value combinations\n-\/\/ that this mask will have. Therefore, the memory where the 32 high order bits are stored,\n-\/\/ can be used as a 32 bit GC epoch counter, that has a different bit pattern every time\n-\/\/ the bad mask is flipped. This provides a pointer to said 32 bits.\n-extern uint32_t*  ZAddressBadMaskHighOrderBitsAddr;\n-const int         ZAddressBadMaskHighOrderBitsOffset = LITTLE_ENDIAN_ONLY(4) BIG_ENDIAN_ONLY(0);\n-\n-\/\/ Pointer part of address\n-extern size_t     ZAddressOffsetBits;\n-const  size_t     ZAddressOffsetShift           = 0;\n-extern uintptr_t  ZAddressOffsetMask;\n-extern size_t     ZAddressOffsetMax;\n-\n-\/\/ Metadata part of address\n-const size_t      ZAddressMetadataBits          = 4;\n-extern size_t     ZAddressMetadataShift;\n-extern uintptr_t  ZAddressMetadataMask;\n-\n-\/\/ Metadata types\n-extern uintptr_t  ZAddressMetadataMarked;\n-extern uintptr_t  ZAddressMetadataMarked0;\n-extern uintptr_t  ZAddressMetadataMarked1;\n-extern uintptr_t  ZAddressMetadataRemapped;\n-extern uintptr_t  ZAddressMetadataFinalizable;\n-\n@@ -127,1 +68,0 @@\n-extern uintptr_t  ZMarkStackSpaceStart;\n@@ -150,0 +90,1 @@\n+const size_t      ZMarkPartialArrayMinLength    = ZMarkPartialArrayMinSize \/ oopSize;\n@@ -153,1 +94,0 @@\n-const size_t      ZMarkTerminateFlushMax        = 3;\n","filename":"src\/hotspot\/share\/gc\/z\/zGlobals.hpp","additions":2,"deletions":62,"binary":false,"changes":64,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,0 +27,1 @@\n+#include \"gc\/z\/zAddress.hpp\"\n@@ -33,1 +34,4 @@\n-  template <typename> friend class ZGranuleMapIterator;\n+  template <typename, bool> friend class ZGranuleMapIterator;\n+  friend class ZForwardingTable;\n+  friend class ZPageTable;\n+  friend class ZRemsetTableIterator;\n@@ -39,1 +43,3 @@\n-  size_t index_for_offset(uintptr_t offset) const;\n+  size_t index_for_offset(zoffset offset) const;\n+\n+  T at(size_t index) const;\n@@ -45,3 +51,3 @@\n-  T get(uintptr_t offset) const;\n-  void put(uintptr_t offset, T value);\n-  void put(uintptr_t offset, size_t size, T value);\n+  T get(zoffset offset) const;\n+  void put(zoffset offset, T value);\n+  void put(zoffset offset, size_t size, T value);\n@@ -49,2 +55,3 @@\n-  T get_acquire(uintptr_t offset) const;\n-  void release_put(uintptr_t offset, T value);\n+  T get_acquire(zoffset offset) const;\n+  void release_put(zoffset offset, T value);\n+  void release_put(zoffset offset, size_t size, T value);\n@@ -53,2 +60,2 @@\n-template <typename T>\n-class ZGranuleMapIterator : public ZArrayIteratorImpl<T, false \/* Parallel *\/> {\n+template <typename T, bool Parallel>\n+class ZGranuleMapIterator : public ZArrayIteratorImpl<T, Parallel> {\n","filename":"src\/hotspot\/share\/gc\/z\/zGranuleMap.hpp","additions":17,"deletions":10,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -29,0 +29,1 @@\n+#include \"gc\/z\/zAddress.inline.hpp\"\n@@ -49,2 +50,2 @@\n-inline size_t ZGranuleMap<T>::index_for_offset(uintptr_t offset) const {\n-  const size_t index = offset >> ZGranuleSizeShift;\n+inline size_t ZGranuleMap<T>::index_for_offset(zoffset offset) const {\n+  const size_t index = untype(offset) >> ZGranuleSizeShift;\n@@ -52,0 +53,1 @@\n+\n@@ -56,1 +58,7 @@\n-inline T ZGranuleMap<T>::get(uintptr_t offset) const {\n+inline T ZGranuleMap<T>::at(size_t index) const {\n+  assert(index < _size, \"Invalid index\");\n+  return Atomic::load(_map + index);\n+}\n+\n+template <typename T>\n+inline T ZGranuleMap<T>::get(zoffset offset) const {\n@@ -58,1 +66,1 @@\n-  return _map[index];\n+  return at(index);\n@@ -62,1 +70,1 @@\n-inline void ZGranuleMap<T>::put(uintptr_t offset, T value) {\n+inline void ZGranuleMap<T>::put(zoffset offset, T value) {\n@@ -64,1 +72,1 @@\n-  _map[index] = value;\n+  Atomic::store(_map + index, value);\n@@ -68,1 +76,1 @@\n-inline void ZGranuleMap<T>::put(uintptr_t offset, size_t size, T value) {\n+inline void ZGranuleMap<T>::put(zoffset offset, size_t size, T value) {\n@@ -74,1 +82,1 @@\n-    _map[index] = value;\n+    Atomic::store(_map + index, value);\n@@ -79,1 +87,1 @@\n-inline T ZGranuleMap<T>::get_acquire(uintptr_t offset) const {\n+inline T ZGranuleMap<T>::get_acquire(zoffset offset) const {\n@@ -85,1 +93,1 @@\n-inline void ZGranuleMap<T>::release_put(uintptr_t offset, T value) {\n+inline void ZGranuleMap<T>::release_put(zoffset offset, T value) {\n@@ -91,2 +99,8 @@\n-inline ZGranuleMapIterator<T>::ZGranuleMapIterator(const ZGranuleMap<T>* granule_map) :\n-    ZArrayIteratorImpl<T, false \/* Parallel *\/>(granule_map->_map, granule_map->_size) {}\n+inline void ZGranuleMap<T>::release_put(zoffset offset, size_t size, T value) {\n+  OrderAccess::release();\n+  put(offset, size, value);\n+}\n+\n+template <typename T, bool Parallel>\n+inline ZGranuleMapIterator<T, Parallel>::ZGranuleMapIterator(const ZGranuleMap<T>* granule_map) :\n+    ZArrayIteratorImpl<T, Parallel>(granule_map->_map, granule_map->_size) {}\n","filename":"src\/hotspot\/share\/gc\/z\/zGranuleMap.inline.hpp","additions":27,"deletions":13,"binary":false,"changes":40,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,1 +27,2 @@\n-#include \"memory\/allStatic.hpp\"\n+#include \"gc\/z\/zAddress.hpp\"\n+#include \"memory\/allocation.hpp\"\n@@ -34,0 +35,1 @@\n+  static uint32_t offset_to_uint32(zoffset key);\n","filename":"src\/hotspot\/share\/gc\/z\/zHash.hpp","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -77,0 +77,4 @@\n+inline uint32_t ZHash::offset_to_uint32(zoffset key) {\n+  return address_to_uint32(untype(key));\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/z\/zHash.inline.hpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,0 +27,1 @@\n+#include \"gc\/shared\/gcLogPrecious.hpp\"\n@@ -31,0 +32,1 @@\n+#include \"gc\/z\/zGeneration.inline.hpp\"\n@@ -35,1 +37,0 @@\n-#include \"gc\/z\/zMark.inline.hpp\"\n@@ -38,2 +39,0 @@\n-#include \"gc\/z\/zRelocationSet.inline.hpp\"\n-#include \"gc\/z\/zRelocationSetSelector.inline.hpp\"\n@@ -42,1 +41,2 @@\n-#include \"gc\/z\/zThread.inline.hpp\"\n+#include \"gc\/z\/zUncoloredRoot.inline.hpp\"\n+#include \"gc\/z\/zUtils.hpp\"\n@@ -49,2 +49,0 @@\n-#include \"prims\/jvmtiTagMap.hpp\"\n-#include \"runtime\/handshake.hpp\"\n@@ -52,1 +50,0 @@\n-#include \"runtime\/safepoint.hpp\"\n@@ -58,1 +55,1 @@\n-ZHeap* ZHeap::_heap = NULL;\n+ZHeap* ZHeap::_heap = nullptr;\n@@ -61,3 +58,1 @@\n-    _workers(),\n-    _object_allocator(),\n-    _page_allocator(&_workers, MinHeapSize, InitialHeapSize, MaxHeapSize),\n+    _page_allocator(MinHeapSize, InitialHeapSize, SoftMaxHeapSize, MaxHeapSize),\n@@ -65,8 +60,7 @@\n-    _forwarding_table(),\n-    _mark(&_workers, &_page_table),\n-    _reference_processor(&_workers),\n-    _weak_roots_processor(&_workers),\n-    _relocate(&_workers),\n-    _relocation_set(&_workers),\n-    _unload(&_workers),\n-    _serviceability(min_capacity(), max_capacity()) {\n+    _allocator_eden(),\n+    _allocator_relocation(),\n+    _serviceability(initial_capacity(), min_capacity(), max_capacity()),\n+    _old(&_page_table, &_page_allocator),\n+    _young(&_page_table, _old.forwarding_table(), &_page_allocator),\n+    _initialized(false) {\n+\n@@ -74,1 +68,1 @@\n-  assert(_heap == NULL, \"Already initialized\");\n+  assert(_heap == nullptr, \"Already initialized\");\n@@ -77,0 +71,14 @@\n+  if (!_page_allocator.is_initialized() || !_young.is_initialized() || !_old.is_initialized()) {\n+    return;\n+  }\n+\n+  \/\/ Prime cache\n+  if (!_page_allocator.prime_cache(_old.workers(), InitialHeapSize)) {\n+    log_error_p(gc)(\"Failed to allocate initial Java heap (\" SIZE_FORMAT \"M)\", InitialHeapSize \/ M);\n+    return;\n+  }\n+\n+  if (UseDynamicNumberOfGCThreads) {\n+    log_info_p(gc, init)(\"GC Workers Max: %u (dynamic)\", ConcGCThreads);\n+  }\n+\n@@ -78,1 +86,5 @@\n-  ZStatHeap::set_at_initialize(_page_allocator.stats());\n+  _young.stat_heap()->at_initialize(_page_allocator.min_capacity(), _page_allocator.max_capacity());\n+  _old.stat_heap()->at_initialize(_page_allocator.min_capacity(), _page_allocator.max_capacity());\n+\n+  \/\/ Successfully initialized\n+  _initialized = true;\n@@ -82,1 +94,5 @@\n-  return _page_allocator.is_initialized() && _mark.is_initialized();\n+  return _initialized;\n+}\n+\n+size_t ZHeap::initial_capacity() const {\n+  return _page_allocator.initial_capacity();\n@@ -105,0 +121,12 @@\n+size_t ZHeap::used_generation(ZGenerationId id) const {\n+  return _page_allocator.used_generation(id);\n+}\n+\n+size_t ZHeap::used_young() const {\n+  return _page_allocator.used_generation(ZGenerationId::young);\n+}\n+\n+size_t ZHeap::used_old() const {\n+  return _page_allocator.used_generation(ZGenerationId::old);\n+}\n+\n@@ -114,1 +142,1 @@\n-  return _object_allocator.used();\n+  return _allocator_eden.tlab_used();\n@@ -122,1 +150,1 @@\n-  size_t size = _object_allocator.remaining();\n+  size_t size = _allocator_eden.remaining();\n@@ -137,0 +165,5 @@\n+  if (addr == 0) {\n+    \/\/ Null isn't in the heap.\n+    return false;\n+  }\n+\n@@ -143,5 +176,4 @@\n-  if (ZAddress::is_in(addr)) {\n-    const ZPage* const page = _page_table.get(addr);\n-    if (page != NULL) {\n-      return page->is_in(addr);\n-    }\n+  assert(!is_valid(zpointer(addr)), \"Don't pass in colored oops\");\n+\n+  if (!is_valid(zaddress(addr))) {\n+    return false;\n@@ -150,2 +182,5 @@\n-  return false;\n-}\n+  const zaddress o = to_zaddress(addr);\n+  const ZPage* const page = _page_table.get(o);\n+  if (page == nullptr) {\n+    return false;\n+  }\n@@ -153,2 +188,1 @@\n-uint ZHeap::active_workers() const {\n-  return _workers.active_workers();\n+  return is_in_page_relaxed(page, o);\n@@ -157,2 +191,20 @@\n-void ZHeap::set_active_workers(uint nworkers) {\n-  _workers.set_active_workers(nworkers);\n+bool ZHeap::is_in_page_relaxed(const ZPage* page, zaddress addr) const {\n+  if (page->is_in(addr)) {\n+    return true;\n+  }\n+\n+  \/\/ Could still be a from-object during an in-place relocation\n+  if (_old.is_phase_relocate()) {\n+    const ZForwarding* const forwarding = _old.forwarding(unsafe(addr));\n+    if (forwarding != nullptr && forwarding->in_place_relocation_is_below_top_at_start(ZAddress::offset(addr))) {\n+      return true;\n+    }\n+  }\n+  if (_young.is_phase_relocate()) {\n+    const ZForwarding* const forwarding = _young.forwarding(unsafe(addr));\n+    if (forwarding != nullptr && forwarding->in_place_relocation_is_below_top_at_start(ZAddress::offset(addr))) {\n+      return true;\n+    }\n+  }\n+\n+  return false;\n@@ -163,1 +215,2 @@\n-  _workers.threads_do(tc);\n+  _young.threads_do(tc);\n+  _old.threads_do(tc);\n@@ -173,3 +226,3 @@\n-ZPage* ZHeap::alloc_page(uint8_t type, size_t size, ZAllocationFlags flags) {\n-  ZPage* const page = _page_allocator.alloc_page(type, size, flags);\n-  if (page != NULL) {\n+ZPage* ZHeap::alloc_page(ZPageType type, size_t size, ZAllocationFlags flags, ZPageAge age) {\n+  ZPage* const page = _page_allocator.alloc_page(type, size, flags, age);\n+  if (page != nullptr) {\n@@ -188,1 +241,1 @@\n-                ZThread::id(), ZThread::name(), p2i(page), page->size());\n+                p2i(Thread::current()), ZUtils::thread_name(), p2i(page), page->size());\n@@ -190,1 +243,1 @@\n-  free_page(page, false \/* reclaimed *\/);\n+  free_page(page);\n@@ -193,1 +246,1 @@\n-void ZHeap::free_page(ZPage* page, bool reclaimed) {\n+void ZHeap::free_page(ZPage* page) {\n@@ -197,0 +250,5 @@\n+  if (page->is_old()) {\n+    page->verify_remset_cleared_current();\n+    page->verify_remset_cleared_previous();\n+  }\n+\n@@ -198,1 +256,1 @@\n-  _page_allocator.free_page(page, reclaimed);\n+  _page_allocator.free_page(page);\n@@ -201,1 +259,2 @@\n-void ZHeap::free_pages(const ZArray<ZPage*>* pages, bool reclaimed) {\n+size_t ZHeap::free_empty_pages(const ZArray<ZPage*>* pages) {\n+  size_t freed = 0;\n@@ -205,0 +264,5 @@\n+    if (page->is_old()) {\n+      \/\/ The remset of pages should be clean when installed into the page\n+      \/\/ cache.\n+      page->remset_clear();\n+    }\n@@ -206,0 +270,1 @@\n+    freed += page->size();\n@@ -209,12 +274,1 @@\n-  _page_allocator.free_pages(pages, reclaimed);\n-}\n-\n-void ZHeap::flip_to_marked() {\n-  ZVerifyViewsFlip flip(&_page_allocator);\n-  ZAddress::flip_to_marked();\n-}\n-\n-void ZHeap::flip_to_remapped() {\n-  ZVerifyViewsFlip flip(&_page_allocator);\n-  ZAddress::flip_to_remapped();\n-}\n+  _page_allocator.free_pages(pages);\n@@ -222,76 +276,1 @@\n-void ZHeap::mark_start() {\n-  assert(SafepointSynchronize::is_at_safepoint(), \"Should be at safepoint\");\n-\n-  \/\/ Verification\n-  ClassLoaderDataGraph::verify_claimed_marks_cleared(ClassLoaderData::_claim_strong);\n-\n-  if (ZHeap::heap()->has_alloc_stalled()) {\n-    \/\/ If there are stalled allocations, ensure that regardless of the\n-    \/\/ cause of the GC, we have to clear soft references, as we are just\n-    \/\/ about to increment the sequence number, and all previous allocations\n-    \/\/ will throw if not presented with enough memory.\n-    ZHeap::heap()->set_soft_reference_policy(true);\n-  }\n-\n-  \/\/ Flip address view\n-  flip_to_marked();\n-\n-  \/\/ Retire allocating pages\n-  _object_allocator.retire_pages();\n-\n-  \/\/ Reset allocated\/reclaimed\/used statistics\n-  _page_allocator.reset_statistics();\n-\n-  \/\/ Reset encountered\/dropped\/enqueued statistics\n-  _reference_processor.reset_statistics();\n-\n-  \/\/ Enter mark phase\n-  ZGlobalPhase = ZPhaseMark;\n-\n-  \/\/ Reset marking information and mark roots\n-  _mark.start();\n-\n-  \/\/ Update statistics\n-  ZStatHeap::set_at_mark_start(_page_allocator.stats());\n-}\n-\n-void ZHeap::mark(bool initial) {\n-  _mark.mark(initial);\n-}\n-\n-void ZHeap::mark_flush_and_free(Thread* thread) {\n-  _mark.flush_and_free(thread);\n-}\n-\n-bool ZHeap::mark_end() {\n-  assert(SafepointSynchronize::is_at_safepoint(), \"Should be at safepoint\");\n-\n-  \/\/ Try end marking\n-  if (!_mark.end()) {\n-    \/\/ Marking not completed, continue concurrent mark\n-    return false;\n-  }\n-\n-  \/\/ Enter mark completed phase\n-  ZGlobalPhase = ZPhaseMarkCompleted;\n-\n-  \/\/ Verify after mark\n-  ZVerify::after_mark();\n-\n-  \/\/ Update statistics\n-  ZStatHeap::set_at_mark_end(_page_allocator.stats());\n-\n-  \/\/ Block resurrection of weak\/phantom references\n-  ZResurrection::block();\n-\n-  \/\/ Prepare to unload stale metadata and nmethods\n-  _unload.prepare();\n-\n-  \/\/ Notify JVMTI that some tagmap entry objects may have died.\n-  JvmtiTagMap::set_needs_cleaning();\n-\n-  return true;\n-}\n-\n-void ZHeap::mark_free() {\n-  _mark.free();\n+  return freed;\n@@ -301,53 +280,2 @@\n-  ZBarrier::keep_alive_barrier_on_oop(obj);\n-}\n-\n-void ZHeap::set_soft_reference_policy(bool clear) {\n-  _reference_processor.set_soft_reference_policy(clear);\n-}\n-\n-class ZRendezvousClosure : public HandshakeClosure {\n-public:\n-  ZRendezvousClosure() :\n-      HandshakeClosure(\"ZRendezvous\") {}\n-\n-  void do_thread(Thread* thread) {}\n-};\n-\n-void ZHeap::process_non_strong_references() {\n-  \/\/ Process Soft\/Weak\/Final\/PhantomReferences\n-  _reference_processor.process_references();\n-\n-  \/\/ Process weak roots\n-  _weak_roots_processor.process_weak_roots();\n-\n-  \/\/ Unlink stale metadata and nmethods\n-  _unload.unlink();\n-\n-  \/\/ Perform a handshake. This is needed 1) to make sure that stale\n-  \/\/ metadata and nmethods are no longer observable. And 2), to\n-  \/\/ prevent the race where a mutator first loads an oop, which is\n-  \/\/ logically null but not yet cleared. Then this oop gets cleared\n-  \/\/ by the reference processor and resurrection is unblocked. At\n-  \/\/ this point the mutator could see the unblocked state and pass\n-  \/\/ this invalid oop through the normal barrier path, which would\n-  \/\/ incorrectly try to mark the oop.\n-  ZRendezvousClosure cl;\n-  Handshake::execute(&cl);\n-\n-  \/\/ Unblock resurrection of weak\/phantom references\n-  ZResurrection::unblock();\n-\n-  \/\/ Purge stale metadata and nmethods that were unlinked\n-  _unload.purge();\n-\n-  \/\/ Enqueue Soft\/Weak\/Final\/PhantomReferences. Note that this\n-  \/\/ must be done after unblocking resurrection. Otherwise the\n-  \/\/ Finalizer thread could call Reference.get() on the Finalizers\n-  \/\/ that were just enqueued, which would incorrectly return null\n-  \/\/ during the resurrection block window, since such referents\n-  \/\/ are only Finalizable marked.\n-  _reference_processor.enqueue_references();\n-\n-  \/\/ Clear old markings claim bits.\n-  \/\/ Note: Clearing _claim_strong also clears _claim_finalizable.\n-  ClassLoaderDataGraph::clear_claimed_marks(ClassLoaderData::_claim_strong);\n+  const zaddress addr = to_zaddress(obj);\n+  ZBarrier::mark<ZMark::Resurrect, ZMark::AnyThread, ZMark::Follow, ZMark::Strong>(addr);\n@@ -356,56 +284,3 @@\n-void ZHeap::free_empty_pages(ZRelocationSetSelector* selector, int bulk) {\n-  \/\/ Freeing empty pages in bulk is an optimization to avoid grabbing\n-  \/\/ the page allocator lock, and trying to satisfy stalled allocations\n-  \/\/ too frequently.\n-  if (selector->should_free_empty_pages(bulk)) {\n-    free_pages(selector->empty_pages(), true \/* reclaimed *\/);\n-    selector->clear_empty_pages();\n-  }\n-}\n-\n-void ZHeap::select_relocation_set() {\n-  \/\/ Do not allow pages to be deleted\n-  _page_allocator.enable_deferred_delete();\n-\n-  \/\/ Register relocatable pages with selector\n-  ZRelocationSetSelector selector;\n-  ZPageTableIterator pt_iter(&_page_table);\n-  for (ZPage* page; pt_iter.next(&page);) {\n-    if (!page->is_relocatable()) {\n-      \/\/ Not relocatable, don't register\n-      continue;\n-    }\n-\n-    if (page->is_marked()) {\n-      \/\/ Register live page\n-      selector.register_live_page(page);\n-    } else {\n-      \/\/ Register empty page\n-      selector.register_empty_page(page);\n-\n-      \/\/ Reclaim empty pages in bulk\n-      free_empty_pages(&selector, 64 \/* bulk *\/);\n-    }\n-  }\n-\n-  \/\/ Reclaim remaining empty pages\n-  free_empty_pages(&selector, 0 \/* bulk *\/);\n-\n-  \/\/ Allow pages to be deleted\n-  _page_allocator.disable_deferred_delete();\n-\n-  \/\/ Select relocation set\n-  selector.select();\n-\n-  \/\/ Install relocation set\n-  _relocation_set.install(&selector);\n-\n-  \/\/ Setup forwarding table\n-  ZRelocationSetIterator rs_iter(&_relocation_set);\n-  for (ZForwarding* forwarding; rs_iter.next(&forwarding);) {\n-    _forwarding_table.insert(forwarding);\n-  }\n-\n-  \/\/ Update statistics\n-  ZStatRelocation::set_at_select_relocation_set(selector.stats());\n-  ZStatHeap::set_at_select_relocation_set(selector.stats());\n+void ZHeap::mark_flush_and_free(Thread* thread) {\n+  _young.mark_flush_and_free(thread);\n+  _old.mark_flush_and_free(thread);\n@@ -414,9 +289,3 @@\n-void ZHeap::reset_relocation_set() {\n-  \/\/ Reset forwarding table\n-  ZRelocationSetIterator iter(&_relocation_set);\n-  for (ZForwarding* forwarding; iter.next(&forwarding);) {\n-    _forwarding_table.remove(forwarding);\n-  }\n-\n-  \/\/ Reset relocation set\n-  _relocation_set.reset();\n+bool ZHeap::is_allocating(zaddress addr) const {\n+  const ZPage* const page = _page_table.get(addr);\n+  return page->is_allocating();\n@@ -425,1 +294,1 @@\n-void ZHeap::relocate_start() {\n+void ZHeap::object_iterate(ObjectClosure* object_cl, bool visit_weaks) {\n@@ -427,25 +296,2 @@\n-\n-  \/\/ Finish unloading stale metadata and nmethods\n-  _unload.finish();\n-\n-  \/\/ Flip address view\n-  flip_to_remapped();\n-\n-  \/\/ Enter relocate phase\n-  ZGlobalPhase = ZPhaseRelocate;\n-\n-  \/\/ Update statistics\n-  ZStatHeap::set_at_relocate_start(_page_allocator.stats());\n-}\n-\n-void ZHeap::relocate() {\n-  \/\/ Relocate relocation set\n-  _relocate.relocate(&_relocation_set);\n-\n-  \/\/ Update statistics\n-  ZStatHeap::set_at_relocate_end(_page_allocator.stats(), _object_allocator.relocated());\n-}\n-\n-bool ZHeap::is_allocating(uintptr_t addr) const {\n-  const ZPage* const page = _page_table.get(addr);\n-  return page->is_allocating();\n+  ZHeapIterator iter(1 \/* nworkers *\/, visit_weaks);\n+  iter.object_iterate(object_cl, 0 \/* worker_id *\/);\n@@ -454,1 +300,1 @@\n-void ZHeap::object_iterate(ObjectClosure* cl, bool visit_weaks) {\n+void ZHeap::object_and_field_iterate(ObjectClosure* object_cl, OopFieldClosure* field_cl, bool visit_weaks) {\n@@ -457,1 +303,1 @@\n-  iter.object_iterate(cl, 0 \/* worker_id *\/);\n+  iter.object_and_field_iterate(object_cl, field_cl, 0 \/* worker_id *\/);\n@@ -465,8 +311,0 @@\n-void ZHeap::pages_do(ZPageClosure* cl) {\n-  ZPageTableIterator iter(&_page_table);\n-  for (ZPage* page; iter.next(&page);) {\n-    cl->do_page(page);\n-  }\n-  _page_allocator.pages_do(cl);\n-}\n-\n@@ -477,2 +315,2 @@\n-GCMemoryManager* ZHeap::serviceability_cycle_memory_manager() {\n-  return _serviceability.cycle_memory_manager();\n+GCMemoryManager* ZHeap::serviceability_cycle_memory_manager(bool minor) {\n+  return _serviceability.cycle_memory_manager(minor);\n@@ -481,2 +319,2 @@\n-GCMemoryManager* ZHeap::serviceability_pause_memory_manager() {\n-  return _serviceability.pause_memory_manager();\n+GCMemoryManager* ZHeap::serviceability_pause_memory_manager(bool minor) {\n+  return _serviceability.pause_memory_manager(minor);\n@@ -485,2 +323,2 @@\n-MemoryPool* ZHeap::serviceability_memory_pool() {\n-  return _serviceability.memory_pool();\n+MemoryPool* ZHeap::serviceability_memory_pool(ZGenerationId id) {\n+  return _serviceability.memory_pool(id);\n@@ -506,1 +344,1 @@\n-  _page_allocator.enable_deferred_delete();\n+  _page_allocator.enable_safe_destroy();\n@@ -516,1 +354,1 @@\n-  _page_allocator.disable_deferred_delete();\n+  _page_allocator.disable_safe_destroy();\n@@ -520,4 +358,14 @@\n-  if (LocationPrinter::is_valid_obj((void*)addr)) {\n-    st->print(PTR_FORMAT \" is a %s oop: \", addr, ZAddress::is_good(addr) ? \"good\" : \"bad\");\n-    ZOop::from_address(addr)->print_on(st);\n-    return true;\n+  \/\/ Intentionally unchecked cast\n+  const bool uncolored = is_valid(zaddress(addr));\n+  const bool colored = is_valid(zpointer(addr));\n+  if (colored && uncolored) {\n+    \/\/ Should not reach here\n+    return false;\n+  }\n+\n+  if (colored) {\n+    return print_location(st, zpointer(addr));\n+  }\n+\n+  if (uncolored) {\n+    return print_location(st, zaddress(addr));\n@@ -529,5 +377,30 @@\n-void ZHeap::verify() {\n-  \/\/ Heap verification can only be done between mark end and\n-  \/\/ relocate start. This is the only window where all oop are\n-  \/\/ good and the whole heap is in a consistent state.\n-  guarantee(ZGlobalPhase == ZPhaseMarkCompleted, \"Invalid phase\");\n+bool ZHeap::print_location(outputStream* st, zaddress addr) const {\n+  assert(is_valid(addr), \"must be\");\n+\n+  st->print(PTR_FORMAT \" is a zaddress: \", untype(addr));\n+\n+  if (addr == zaddress::null) {\n+    st->print_raw_cr(\"NULL\");\n+    return true;\n+  }\n+\n+  if (!ZHeap::is_in(untype(addr))) {\n+    st->print_raw_cr(\"not in heap\");\n+    return false;\n+  }\n+\n+  if (LocationPrinter::is_valid_obj((void*)untype(addr))) {\n+    to_oop(addr)->print_on(st);\n+    return true;\n+  }\n+\n+  ZPage* const page = ZHeap::page(addr);\n+  zaddress_unsafe base;\n+\n+  if (page->is_relocatable() && page->is_marked() && !ZGeneration::generation(page->generation_id())->is_phase_mark()) {\n+    base = page->find_base((volatile zpointer*) addr);\n+  } else {\n+    \/\/ TODO: This part is probably broken, but register printing recovers from crashes\n+    st->print_raw(\"Unreliable \");\n+    base = page->find_base_unsafe((volatile zpointer*) addr);\n+  }\n@@ -535,1 +408,41 @@\n-  ZVerify::after_weak_processing();\n+  if (base == zaddress_unsafe::null) {\n+    st->print_raw_cr(\"Cannot find base\");\n+    return false;\n+  }\n+\n+  if (untype(base) == untype(addr)) {\n+    st->print_raw_cr(\"Bad mark info\/base\");\n+    return false;\n+  }\n+\n+  st->print_raw_cr(\"Internal address\");\n+  print_location(st, untype(base));\n+  return true;\n+}\n+\n+bool ZHeap::print_location(outputStream* st, zpointer ptr) const {\n+  assert(is_valid(ptr), \"must be\");\n+\n+  st->print(PTR_FORMAT \" is %s zpointer: \", untype(ptr),\n+            ZPointer::is_load_good(ptr) ? \"a good\" : \"a bad\");\n+\n+  if (!ZPointer::is_load_good(ptr)) {\n+    st->print_cr(\"decoded \" PTR_FORMAT, untype(ZPointer::uncolor_unsafe(ptr)));\n+    \/\/ ptr is not load good but let us still investigate the uncolored address\n+    return print_location(st, untype(ZPointer::uncolor_unsafe(ptr)));\n+  }\n+\n+  const zaddress addr =  ZPointer::uncolor(ptr);\n+\n+  if (addr == zaddress::null) {\n+    st->print_raw_cr(\"NULL\");\n+    return true;\n+  }\n+\n+  if (LocationPrinter::is_valid_obj((void*)untype(addr))) {\n+    to_oop(addr)->print_on(st);\n+    return true;\n+  }\n+\n+  st->print_cr(\"invalid object \" PTR_FORMAT,  untype(addr));\n+  return false;\n","filename":"src\/hotspot\/share\/gc\/z\/zHeap.cpp","additions":218,"deletions":305,"binary":false,"changes":523,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,0 +28,1 @@\n+#include \"gc\/z\/zAllocator.hpp\"\n@@ -29,3 +30,2 @@\n-#include \"gc\/z\/zForwardingTable.hpp\"\n-#include \"gc\/z\/zMark.hpp\"\n-#include \"gc\/z\/zObjectAllocator.hpp\"\n+#include \"gc\/z\/zGeneration.hpp\"\n+#include \"gc\/z\/zPageAge.hpp\"\n@@ -34,4 +34,1 @@\n-#include \"gc\/z\/zReferenceProcessor.hpp\"\n-#include \"gc\/z\/zRelocate.hpp\"\n-#include \"gc\/z\/zRelocationSet.hpp\"\n-#include \"gc\/z\/zWeakRootsProcessor.hpp\"\n+#include \"gc\/z\/zPageType.hpp\"\n@@ -39,2 +36,0 @@\n-#include \"gc\/z\/zUnload.hpp\"\n-#include \"gc\/z\/zWorkers.hpp\"\n@@ -42,3 +37,1 @@\n-class ThreadClosure;\n-class ZPage;\n-class ZRelocationSetSelector;\n+class OopFieldClosure;\n@@ -47,0 +40,2 @@\n+  friend class ZForwardingTest;\n+  friend class ZLiveMapTest;\n@@ -50,19 +45,1 @@\n-  static ZHeap*       _heap;\n-\n-  ZWorkers            _workers;\n-  ZObjectAllocator    _object_allocator;\n-  ZPageAllocator      _page_allocator;\n-  ZPageTable          _page_table;\n-  ZForwardingTable    _forwarding_table;\n-  ZMark               _mark;\n-  ZReferenceProcessor _reference_processor;\n-  ZWeakRootsProcessor _weak_roots_processor;\n-  ZRelocate           _relocate;\n-  ZRelocationSet      _relocation_set;\n-  ZUnload             _unload;\n-  ZServiceability     _serviceability;\n-\n-  void flip_to_marked();\n-  void flip_to_remapped();\n-\n-  void free_empty_pages(ZRelocationSetSelector* selector, int bulk);\n+  static ZHeap*           _heap;\n@@ -70,1 +47,12 @@\n-  void out_of_memory();\n+  ZPageAllocator          _page_allocator;\n+  ZPageTable              _page_table;\n+\n+  ZAllocatorEden          _allocator_eden;\n+  ZAllocatorForRelocation _allocator_relocation[ZAllocator::_relocation_allocators];\n+\n+  ZServiceability         _serviceability;\n+\n+  ZGenerationOld          _old;\n+  ZGenerationYoung        _young;\n+\n+  bool                    _initialized;\n@@ -79,0 +67,2 @@\n+  void out_of_memory();\n+\n@@ -80,0 +70,1 @@\n+  size_t initial_capacity() const;\n@@ -85,0 +76,3 @@\n+  size_t used_generation(ZGenerationId id) const;\n+  size_t used_young() const;\n+  size_t used_old() const;\n@@ -93,0 +87,1 @@\n+  bool is_in_page_relaxed(const ZPage* page, zaddress addr) const;\n@@ -94,4 +89,2 @@\n-  \/\/ Threads\n-  uint active_workers() const;\n-  void set_active_workers(uint nworkers);\n-  void threads_do(ThreadClosure* tc) const;\n+  bool is_young(zaddress addr) const;\n+  bool is_young(volatile zpointer* ptr) const;\n@@ -99,3 +92,2 @@\n-  \/\/ Reference processing\n-  ReferenceDiscoverer* reference_discoverer();\n-  void set_soft_reference_policy(bool clear);\n+  bool is_old(zaddress addr) const;\n+  bool is_old(volatile zpointer* ptr) const;\n@@ -103,2 +95,8 @@\n-  \/\/ Non-strong reference processing\n-  void process_non_strong_references();\n+  ZPage* page(zaddress addr) const;\n+  ZPage* page(volatile zpointer* addr) const;\n+\n+  \/\/ Liveness\n+  bool is_object_live(zaddress addr) const;\n+  bool is_object_strongly_live(zaddress addr) const;\n+  void keep_alive(oop obj);\n+  void mark_flush_and_free(Thread* thread);\n@@ -107,1 +105,1 @@\n-  ZPage* alloc_page(uint8_t type, size_t size, ZAllocationFlags flags);\n+  ZPage* alloc_page(ZPageType type, size_t size, ZAllocationFlags flags, ZPageAge age);\n@@ -109,2 +107,2 @@\n-  void free_page(ZPage* page, bool reclaimed);\n-  void free_pages(const ZArray<ZPage*>* pages, bool reclaimed);\n+  void free_page(ZPage* page);\n+  size_t free_empty_pages(const ZArray<ZPage*>* pages);\n@@ -113,27 +111,4 @@\n-  uintptr_t alloc_tlab(size_t size);\n-  uintptr_t alloc_object(size_t size);\n-  uintptr_t alloc_object_for_relocation(size_t size);\n-  void undo_alloc_object_for_relocation(uintptr_t addr, size_t size);\n-  bool has_alloc_stalled() const;\n-  void check_out_of_memory();\n-\n-  \/\/ Marking\n-  bool is_object_live(uintptr_t addr) const;\n-  bool is_object_strongly_live(uintptr_t addr) const;\n-  template <bool gc_thread, bool follow, bool finalizable, bool publish> void mark_object(uintptr_t addr);\n-  void mark_start();\n-  void mark(bool initial);\n-  void mark_flush_and_free(Thread* thread);\n-  bool mark_end();\n-  void mark_free();\n-  void keep_alive(oop obj);\n-\n-  \/\/ Relocation set\n-  void select_relocation_set();\n-  void reset_relocation_set();\n-\n-  \/\/ Relocation\n-  void relocate_start();\n-  uintptr_t relocate_object(uintptr_t addr);\n-  uintptr_t remap_object(uintptr_t addr);\n-  void relocate();\n+  bool is_alloc_stalling() const;\n+  bool is_alloc_stalling_for_old() const;\n+  void handle_alloc_stalling_for_young();\n+  void handle_alloc_stalling_for_old();\n@@ -142,1 +117,1 @@\n-  bool is_allocating(uintptr_t addr) const;\n+  bool is_allocating(zaddress addr) const;\n@@ -145,1 +120,2 @@\n-  void object_iterate(ObjectClosure* cl, bool visit_weaks);\n+  void object_iterate(ObjectClosure* object_cl, bool visit_weaks);\n+  void object_and_field_iterate(ObjectClosure* object_cl, OopFieldClosure* field_cl, bool visit_weaks);\n@@ -147,1 +123,2 @@\n-  void pages_do(ZPageClosure* cl);\n+\n+  void threads_do(ThreadClosure* tc) const;\n@@ -151,3 +128,3 @@\n-  GCMemoryManager* serviceability_cycle_memory_manager();\n-  GCMemoryManager* serviceability_pause_memory_manager();\n-  MemoryPool* serviceability_memory_pool();\n+  GCMemoryManager* serviceability_cycle_memory_manager(bool minor);\n+  GCMemoryManager* serviceability_pause_memory_manager(bool minor);\n+  MemoryPool* serviceability_memory_pool(ZGenerationId id);\n@@ -160,0 +137,2 @@\n+  bool print_location(outputStream* st, zaddress addr) const;\n+  bool print_location(outputStream* st, zpointer ptr) const;\n@@ -163,1 +142,0 @@\n-  void verify();\n","filename":"src\/hotspot\/share\/gc\/z\/zHeap.hpp","additions":57,"deletions":79,"binary":false,"changes":136,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -31,0 +31,1 @@\n+#include \"gc\/z\/zGenerationId.hpp\"\n@@ -34,0 +35,1 @@\n+#include \"gc\/z\/zRemembered.inline.hpp\"\n@@ -37,1 +39,1 @@\n-  assert(_heap != NULL, \"Not initialized\");\n+  assert(_heap != nullptr, \"Not initialized\");\n@@ -41,2 +43,2 @@\n-inline ReferenceDiscoverer* ZHeap::reference_discoverer() {\n-  return &_reference_processor;\n+inline bool ZHeap::is_young(zaddress addr) const {\n+  return page(addr)->is_young();\n@@ -45,3 +47,2 @@\n-inline bool ZHeap::is_object_live(uintptr_t addr) const {\n-  ZPage* page = _page_table.get(addr);\n-  return page->is_object_live(addr);\n+inline bool ZHeap::is_young(volatile zpointer* ptr) const {\n+  return page(ptr)->is_young();\n@@ -50,3 +51,2 @@\n-inline bool ZHeap::is_object_strongly_live(uintptr_t addr) const {\n-  ZPage* page = _page_table.get(addr);\n-  return page->is_object_strongly_live(addr);\n+inline bool ZHeap::is_old(zaddress addr) const {\n+  return !is_young(addr);\n@@ -55,4 +55,2 @@\n-template <bool gc_thread, bool follow, bool finalizable, bool publish>\n-inline void ZHeap::mark_object(uintptr_t addr) {\n-  assert(ZGlobalPhase == ZPhaseMark, \"Mark not allowed\");\n-  _mark.mark_object<gc_thread, follow, finalizable, publish>(addr);\n+inline bool ZHeap::is_old(volatile zpointer* ptr) const {\n+  return !is_young(ptr);\n@@ -61,3 +59,2 @@\n-inline uintptr_t ZHeap::alloc_tlab(size_t size) {\n-  guarantee(size <= max_tlab_size(), \"TLAB too large\");\n-  return _object_allocator.alloc_object(size);\n+inline ZPage* ZHeap::page(zaddress addr) const {\n+  return _page_table.get(addr);\n@@ -66,9 +63,2 @@\n-inline uintptr_t ZHeap::alloc_object(size_t size) {\n-  uintptr_t addr = _object_allocator.alloc_object(size);\n-  assert(ZAddress::is_good_or_null(addr), \"Bad address\");\n-\n-  if (addr == 0) {\n-    out_of_memory();\n-  }\n-\n-  return addr;\n+inline ZPage* ZHeap::page(volatile zpointer* ptr) const {\n+  return _page_table.get(ptr);\n@@ -77,4 +67,3 @@\n-inline uintptr_t ZHeap::alloc_object_for_relocation(size_t size) {\n-  const uintptr_t addr = _object_allocator.alloc_object_for_relocation(&_page_table, size);\n-  assert(ZAddress::is_good_or_null(addr), \"Bad address\");\n-  return addr;\n+inline bool ZHeap::is_object_live(zaddress addr) const {\n+  const ZPage* const page = _page_table.get(addr);\n+  return page->is_object_live(addr);\n@@ -83,3 +72,3 @@\n-inline void ZHeap::undo_alloc_object_for_relocation(uintptr_t addr, size_t size) {\n-  ZPage* const page = _page_table.get(addr);\n-  _object_allocator.undo_alloc_object_for_relocation(page, addr, size);\n+inline bool ZHeap::is_object_strongly_live(zaddress addr) const {\n+  const ZPage* const page = _page_table.get(addr);\n+  return page->is_object_strongly_live(addr);\n@@ -88,11 +77,2 @@\n-inline uintptr_t ZHeap::relocate_object(uintptr_t addr) {\n-  assert(ZGlobalPhase == ZPhaseRelocate, \"Relocate not allowed\");\n-\n-  ZForwarding* const forwarding = _forwarding_table.get(addr);\n-  if (forwarding == NULL) {\n-    \/\/ Not forwarding\n-    return ZAddress::good(addr);\n-  }\n-\n-  \/\/ Relocate object\n-  return _relocate.relocate_object(forwarding, ZAddress::good(addr));\n+inline bool ZHeap::is_alloc_stalling() const {\n+  return _page_allocator.is_alloc_stalling();\n@@ -101,12 +81,2 @@\n-inline uintptr_t ZHeap::remap_object(uintptr_t addr) {\n-  assert(ZGlobalPhase == ZPhaseMark ||\n-         ZGlobalPhase == ZPhaseMarkCompleted, \"Forward not allowed\");\n-\n-  ZForwarding* const forwarding = _forwarding_table.get(addr);\n-  if (forwarding == NULL) {\n-    \/\/ Not forwarding\n-    return ZAddress::good(addr);\n-  }\n-\n-  \/\/ Forward object\n-  return _relocate.forward_object(forwarding, ZAddress::good(addr));\n+inline bool ZHeap::is_alloc_stalling_for_old() const {\n+  return _page_allocator.is_alloc_stalling_for_old();\n@@ -115,2 +85,2 @@\n-inline bool ZHeap::has_alloc_stalled() const {\n-  return _page_allocator.has_alloc_stalled();\n+inline void ZHeap::handle_alloc_stalling_for_young() {\n+  _page_allocator.handle_alloc_stalling_for_young();\n@@ -119,2 +89,2 @@\n-inline void ZHeap::check_out_of_memory() {\n-  _page_allocator.check_out_of_memory();\n+inline void ZHeap::handle_alloc_stalling_for_old() {\n+  _page_allocator.handle_alloc_stalling_for_old();\n@@ -124,1 +94,1 @@\n-  return ZAddress::is_good(addr) && is_object_aligned(addr) && is_in(addr);\n+  return is_in(addr);\n","filename":"src\/hotspot\/share\/gc\/z\/zHeap.inline.hpp","additions":31,"deletions":61,"binary":false,"changes":92,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -25,0 +25,1 @@\n+#include \"classfile\/classLoaderData.hpp\"\n@@ -26,0 +27,1 @@\n+#include \"gc\/shared\/barrierSet.hpp\"\n@@ -31,0 +33,1 @@\n+#include \"gc\/z\/zGenerationId.hpp\"\n@@ -33,0 +36,1 @@\n+#include \"gc\/z\/zHeap.inline.hpp\"\n@@ -36,1 +40,0 @@\n-#include \"gc\/z\/zOop.inline.hpp\"\n@@ -59,1 +62,2 @@\n-  ZStatTimerDisable              _timer_disable;\n+  ObjectClosure*                 _object_cl;\n+  OopFieldClosure*               _field_cl;\n@@ -62,1 +66,1 @@\n-  ZHeapIteratorContext(ZHeapIterator* iter, uint worker_id) :\n+  ZHeapIteratorContext(ZHeapIterator* iter, ObjectClosure* object_cl, OopFieldClosure* field_cl, uint worker_id) :\n@@ -66,1 +70,13 @@\n-      _worker_id(worker_id) {}\n+      _worker_id(worker_id),\n+      _object_cl(object_cl),\n+      _field_cl(field_cl) {}\n+\n+  void visit_field(oop base, oop* p) const {\n+    if (_field_cl != nullptr) {\n+      _field_cl->do_field(base, p);\n+    }\n+  }\n+\n+  void visit_object(oop obj) const {\n+    _object_cl->do_object(obj);\n+  }\n@@ -70,0 +86,1 @@\n+      visit_object(obj);\n@@ -100,1 +117,1 @@\n-class ZHeapIteratorRootOopClosure : public OopClosure {\n+class ZHeapIteratorColoredRootOopClosure : public OopClosure {\n@@ -113,1 +130,1 @@\n-  ZHeapIteratorRootOopClosure(const ZHeapIteratorContext& context) :\n+  ZHeapIteratorColoredRootOopClosure(const ZHeapIteratorContext& context) :\n@@ -117,0 +134,26 @@\n+    _context.visit_field(nullptr, p);\n+    const oop obj = load_oop(p);\n+    _context.mark_and_push(obj);\n+  }\n+\n+  virtual void do_oop(narrowOop* p) {\n+    ShouldNotReachHere();\n+  }\n+};\n+\n+class ZHeapIteratorUncoloredRootOopClosure : public OopClosure {\n+private:\n+  const ZHeapIteratorContext& _context;\n+\n+  oop load_oop(oop* p) {\n+    const oop o = Atomic::load(p);\n+    assert_is_valid(to_zaddress(o));\n+    return RawAccess<>::oop_load(p);\n+  }\n+\n+public:\n+  ZHeapIteratorUncoloredRootOopClosure(const ZHeapIteratorContext& context) :\n+      _context(context) {}\n+\n+  virtual void do_oop(oop* p) {\n+    _context.visit_field(nullptr, p);\n@@ -153,0 +196,1 @@\n+    _context.visit_field(_base, p);\n@@ -201,1 +245,0 @@\n-    _timer_disable(),\n@@ -206,2 +249,3 @@\n-    _roots(ClassLoaderData::_claim_other),\n-    _weak_roots(),\n+    _roots_colored(ZGenerationIdOptional::none),\n+    _roots_uncolored(ZGenerationIdOptional::none),\n+    _roots_weak_colored(ZGenerationIdOptional::none),\n@@ -249,2 +293,2 @@\n-  const uintptr_t addr = ZOop::to_address(obj);\n-  const uintptr_t offset = ZAddress::offset(addr);\n+  const zaddress addr = to_zaddress(obj);\n+  const zoffset offset = ZAddress::offset(addr);\n@@ -252,1 +296,1 @@\n-  return (offset & mask) >> ZObjectAlignmentSmallShift;\n+  return (untype(offset) & mask) >> ZObjectAlignmentSmallShift;\n@@ -256,1 +300,1 @@\n-  const uintptr_t offset = ZAddress::offset(ZOop::to_address(obj));\n+  const zoffset offset = ZAddress::offset(to_zaddress(obj));\n@@ -258,1 +302,1 @@\n-  if (bitmap == NULL) {\n+  if (bitmap == nullptr) {\n@@ -261,1 +305,1 @@\n-    if (bitmap == NULL) {\n+    if (bitmap == nullptr) {\n@@ -272,1 +316,1 @@\n-  if (obj == NULL) {\n+  if (obj == nullptr) {\n@@ -281,1 +325,1 @@\n-typedef ClaimingCLDToOopClosure<ClassLoaderData::_claim_other> ZHeapIteratorCLDCLosure;\n+typedef ClaimingCLDToOopClosure<ClassLoaderData::_claim_other> ZHeapIteratorCLDClosure;\n@@ -320,9 +364,15 @@\n-  ZHeapIteratorRootOopClosure<false \/* Weak *\/> cl(context);\n-  ZHeapIteratorCLDCLosure cld_cl(&cl);\n-  ZHeapIteratorNMethodClosure nm_cl(&cl);\n-  ZHeapIteratorThreadClosure thread_cl(&cl, &nm_cl);\n-\n-  _roots.apply(&cl,\n-               &cld_cl,\n-               &thread_cl,\n-               &nm_cl);\n+  {\n+    ZHeapIteratorColoredRootOopClosure<false \/* Weak *\/> cl(context);\n+    ZHeapIteratorCLDClosure cld_cl(&cl);\n+\n+    _roots_colored.apply(&cl,\n+                         &cld_cl);\n+  }\n+\n+  {\n+    ZHeapIteratorUncoloredRootOopClosure cl(context);\n+    ZHeapIteratorNMethodClosure nm_cl(&cl);\n+    ZHeapIteratorThreadClosure thread_cl(&cl, &nm_cl);\n+    _roots_uncolored.apply(&thread_cl,\n+                           &nm_cl);\n+  }\n@@ -332,2 +382,2 @@\n-  ZHeapIteratorRootOopClosure<true  \/* Weak *\/> cl(context);\n-  _weak_roots.apply(&cl);\n+  ZHeapIteratorColoredRootOopClosure<true  \/* Weak *\/> cl(context);\n+  _roots_weak_colored.apply(&cl);\n@@ -347,1 +397,1 @@\n-  obj->oop_iterate(&cl);\n+  ZIterator::oop_iterate(obj, &cl);\n@@ -373,1 +423,1 @@\n-  obj->oop_iterate_range(&cl, start, end);\n+  ZIterator::oop_iterate_range(obj, &cl, start, end);\n@@ -377,4 +427,1 @@\n-void ZHeapIterator::visit_and_follow(const ZHeapIteratorContext& context, ObjectClosure* cl, oop obj) {\n-  \/\/ Visit\n-  cl->do_object(obj);\n-\n+void ZHeapIterator::follow(const ZHeapIteratorContext& context, oop obj) {\n@@ -390,1 +437,1 @@\n-void ZHeapIterator::drain(const ZHeapIteratorContext& context, ObjectClosure* cl) {\n+void ZHeapIterator::drain(const ZHeapIteratorContext& context) {\n@@ -396,1 +443,1 @@\n-      visit_and_follow<VisitWeaks>(context, cl, obj);\n+      follow<VisitWeaks>(context, obj);\n@@ -406,1 +453,1 @@\n-void ZHeapIterator::steal(const ZHeapIteratorContext& context, ObjectClosure* cl) {\n+void ZHeapIterator::steal(const ZHeapIteratorContext& context) {\n@@ -413,1 +460,1 @@\n-    visit_and_follow<VisitWeaks>(context, cl, obj);\n+    follow<VisitWeaks>(context, obj);\n@@ -418,1 +465,1 @@\n-void ZHeapIterator::drain_and_steal(const ZHeapIteratorContext& context, ObjectClosure* cl) {\n+void ZHeapIterator::drain_and_steal(const ZHeapIteratorContext& context) {\n@@ -420,2 +467,2 @@\n-    drain<VisitWeaks>(context, cl);\n-    steal<VisitWeaks>(context, cl);\n+    drain<VisitWeaks>(context);\n+    steal<VisitWeaks>(context);\n@@ -426,1 +473,1 @@\n-void ZHeapIterator::object_iterate_inner(const ZHeapIteratorContext& context, ObjectClosure* object_cl) {\n+void ZHeapIterator::object_iterate_inner(const ZHeapIteratorContext& context) {\n@@ -428,1 +475,11 @@\n-  drain_and_steal<VisitWeaks>(context, object_cl);\n+  drain_and_steal<VisitWeaks>(context);\n+}\n+\n+void ZHeapIterator::object_iterate(ObjectClosure* object_cl, uint worker_id) {\n+  const ZHeapIteratorContext context(this, object_cl, nullptr \/* field_cl *\/, worker_id);\n+\n+  if (_visit_weaks) {\n+    object_iterate_inner<true \/* VisitWeaks *\/>(context);\n+  } else {\n+    object_iterate_inner<false \/* VisitWeaks *\/>(context);\n+  }\n@@ -431,2 +488,2 @@\n-void ZHeapIterator::object_iterate(ObjectClosure* cl, uint worker_id) {\n-  ZHeapIteratorContext context(this, worker_id);\n+void ZHeapIterator::object_and_field_iterate(ObjectClosure* object_cl, OopFieldClosure* field_cl, uint worker_id) {\n+  const ZHeapIteratorContext context(this, object_cl, field_cl, worker_id);\n@@ -435,1 +492,1 @@\n-    object_iterate_inner<true \/* VisitWeaks *\/>(context, cl);\n+    object_iterate_inner<true \/* VisitWeaks *\/>(context);\n@@ -437,1 +494,1 @@\n-    object_iterate_inner<false \/* VisitWeaks *\/>(context, cl);\n+    object_iterate_inner<false \/* VisitWeaks *\/>(context);\n","filename":"src\/hotspot\/share\/gc\/z\/zHeapIterator.cpp","additions":105,"deletions":48,"binary":false,"changes":153,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -39,1 +39,1 @@\n-using ZHeapIteratorBitMapsIterator = ZGranuleMapIterator<ZHeapIteratorBitMap*>;\n+using ZHeapIteratorBitMapsIterator = ZGranuleMapIterator<ZHeapIteratorBitMap*, false \/* Parallel *\/>;\n@@ -47,0 +47,1 @@\n+  friend class ZHeapIteratorRootUncoloredOopClosure;\n@@ -49,9 +50,9 @@\n-  const bool               _visit_weaks;\n-  ZStatTimerDisable        _timer_disable;\n-  ZHeapIteratorBitMaps     _bitmaps;\n-  ZLock                    _bitmaps_lock;\n-  ZHeapIteratorQueues      _queues;\n-  ZHeapIteratorArrayQueues _array_queues;\n-  ZRootsIterator           _roots;\n-  ZWeakRootsIterator       _weak_roots;\n-  TaskTerminator           _terminator;\n+  const bool                    _visit_weaks;\n+  ZHeapIteratorBitMaps          _bitmaps;\n+  ZLock                         _bitmaps_lock;\n+  ZHeapIteratorQueues           _queues;\n+  ZHeapIteratorArrayQueues      _array_queues;\n+  ZRootsIteratorStrongColored   _roots_colored;\n+  ZRootsIteratorStrongUncolored _roots_uncolored;\n+  ZRootsIteratorWeakColored     _roots_weak_colored;\n+  TaskTerminator                _terminator;\n@@ -76,1 +77,1 @@\n-  void visit_and_follow(const ZHeapIteratorContext& context, ObjectClosure* cl, oop obj);\n+  void follow(const ZHeapIteratorContext& context, oop obj);\n@@ -79,1 +80,1 @@\n-  void drain(const ZHeapIteratorContext& context, ObjectClosure* cl);\n+  void drain(const ZHeapIteratorContext& context);\n@@ -82,1 +83,1 @@\n-  void steal(const ZHeapIteratorContext& context, ObjectClosure* cl);\n+  void steal(const ZHeapIteratorContext& context);\n@@ -85,1 +86,1 @@\n-  void drain_and_steal(const ZHeapIteratorContext& context, ObjectClosure* cl);\n+  void drain_and_steal(const ZHeapIteratorContext& context);\n@@ -88,1 +89,1 @@\n-  void object_iterate_inner(const ZHeapIteratorContext& context, ObjectClosure* cl);\n+  void object_iterate_inner(const ZHeapIteratorContext& context);\n@@ -94,1 +95,2 @@\n-  virtual void object_iterate(ObjectClosure* cl, uint worker_id);\n+  virtual void object_iterate(ObjectClosure* object_cl, uint worker_id);\n+  void object_and_field_iterate(ObjectClosure* object_cl, OopFieldClosure* field_cl, uint worker_id);\n","filename":"src\/hotspot\/share\/gc\/z\/zHeapIterator.hpp","additions":19,"deletions":17,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2019, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2019, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -25,1 +25,0 @@\n-#include \"gc\/shared\/gcLogPrecious.hpp\"\n@@ -27,0 +26,1 @@\n+#include \"gc\/shared\/gcLogPrecious.hpp\"\n@@ -59,2 +59,1 @@\n-  const uint nworkers = UseDynamicNumberOfGCThreads ? ConcGCThreads : MAX2(ConcGCThreads, ParallelGCThreads);\n-  return (nworkers * ZPageSizeSmall) + ZPageSizeMedium;\n+  return (ConcGCThreads * ZPageSizeSmall) + ZPageSizeMedium;\n@@ -67,1 +66,1 @@\n-  const size_t per_cpu_share = (MaxHeapSize * 0.03125) \/ ZCPU::count();\n+  const size_t per_cpu_share = significant_heap_overhead() \/ ZCPU::count();\n@@ -76,2 +75,1 @@\n-  const int nworkers = (MaxHeapSize * (heap_share_in_percent \/ 100.0)) \/ ZPageSizeSmall;\n-  return MAX2(nworkers, 1);\n+  return (MaxHeapSize * (heap_share_in_percent \/ 100.0)) \/ ZPageSizeSmall;\n@@ -93,1 +91,1 @@\n-  return nworkers(60.0);\n+  return MAX2(nworkers(60.0), 1u);\n@@ -100,4 +98,11 @@\n-  \/\/ threads will prolong the GC-cycle and we then risk being out-run by the\n-  \/\/ application. When in dynamic mode, use up to 25% of the active processors.\n-  \/\/  When in non-dynamic mode, use 12.5% of the active processors.\n-  return nworkers(UseDynamicNumberOfGCThreads ? 25.0 : 12.5);\n+  \/\/ threads will prolong the GC cycle and we then risk being out-run by the\n+  \/\/ application.\n+  return MAX2(nworkers(25.0), 1u);\n+}\n+\n+size_t ZHeuristics::significant_heap_overhead() {\n+  return MaxHeapSize * ZFragmentationLimit;\n+}\n+\n+size_t ZHeuristics::significant_young_overhead() {\n+  return MaxHeapSize * ZYoungCompactionLimit;\n","filename":"src\/hotspot\/share\/gc\/z\/zHeuristics.cpp","additions":17,"deletions":12,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2019, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2019, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,0 +28,1 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -39,0 +40,3 @@\n+\n+  static size_t significant_heap_overhead();\n+  static size_t significant_young_overhead();\n","filename":"src\/hotspot\/share\/gc\/z\/zHeuristics.hpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -0,0 +1,44 @@\n+\/*\n+ * Copyright (c) 2021, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_Z_ZINDEXDISTRIBUTOR_HPP\n+#define SHARE_GC_Z_ZINDEXDISTRIBUTOR_HPP\n+\n+class ZIndexDistributor {\n+private:\n+  void* _strategy;\n+\n+  template <typename Strategy>\n+  Strategy* strategy();\n+\n+  static void* create_strategy(int count);\n+\n+public:\n+  ZIndexDistributor(int count);\n+  ~ZIndexDistributor();\n+\n+  template <typename Function>\n+  void do_indices(Function function);\n+};\n+\n+#endif \/\/ SHARE_GC_Z_ZINDEXDISTRIBUTOR_HPP\n","filename":"src\/hotspot\/share\/gc\/z\/zIndexDistributor.hpp","additions":44,"deletions":0,"binary":false,"changes":44,"status":"added"},{"patch":"@@ -0,0 +1,331 @@\n+\/*\n+ * Copyright (c) 2021, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_Z_ZINDEXDISTRIBUTOR_INLINE_HPP\n+#define SHARE_GC_Z_ZINDEXDISTRIBUTOR_INLINE_HPP\n+\n+#include \"gc\/z\/zIndexDistributor.hpp\"\n+\n+#include \"gc\/shared\/gc_globals.hpp\"\n+#include \"gc\/z\/zGlobals.hpp\"\n+#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/os.hpp\"\n+#include \"runtime\/thread.hpp\"\n+#include \"utilities\/align.hpp\"\n+\n+class ZIndexDistributorStriped : public CHeapObj<mtGC> {\n+  static const int MemSize = 4096;\n+\n+  const int _max_index;\n+  \/\/ For claiming a stripe\n+  volatile int _claim_stripe;\n+  \/\/ For claiming inside a stripe\n+  char _mem[MemSize + ZCacheLineSize];\n+\n+  int claim_stripe() {\n+    return Atomic::fetch_and_add(&_claim_stripe, 1, memory_order_relaxed);\n+  }\n+\n+  volatile int* claim_addr(int index) {\n+    return (volatile int*)(align_up(_mem, ZCacheLineSize) + index * ZCacheLineSize);\n+  }\n+\n+public:\n+  ZIndexDistributorStriped(int max_index) :\n+      _max_index(max_index),\n+      _claim_stripe(0),\n+      _mem() {\n+    memset(_mem, 0, MemSize + ZCacheLineSize);\n+  }\n+\n+  template <typename Function>\n+  void do_indices(Function function) {\n+    const int count = MemSize \/ ZCacheLineSize;\n+    const int stripe_max = _max_index \/ count;\n+\n+    \/\/ Use claiming\n+    for (int i; (i = claim_stripe()) < count;) {\n+      for (int index; (index = Atomic::fetch_and_add(claim_addr(i), 1, memory_order_relaxed)) < stripe_max;) {\n+        if (!function(i * stripe_max + index)) {\n+          return;\n+        }\n+      }\n+    }\n+\n+    \/\/ Use stealing\n+    for (int i = 0; i < count; i++) {\n+      for (int index; (index = Atomic::fetch_and_add(claim_addr(i), 1, memory_order_relaxed)) < stripe_max;) {\n+        if (!function(i * stripe_max + index)) {\n+          return;\n+        }\n+      }\n+    }\n+  }\n+};\n+\n+class ZIndexDistributorClaimTree : public CHeapObj<mtGC> {\n+  friend class ZIndexDistributorTest;\n+\n+private:\n+  \/\/ The N - 1 levels are used to claim a segment in the\n+  \/\/ next level the Nth level claims an index.\n+  static constexpr int N = 4;\n+  static constexpr int ClaimLevels = N - 1;\n+\n+  \/\/ Describes the how the number of indices increases when going up from the given level\n+  static constexpr int level_multiplier(int level) {\n+    assert(level < ClaimLevels, \"Must be\");\n+    constexpr int array[ClaimLevels]{16, 16, 16};\n+    return array[level];\n+  }\n+\n+  \/\/ Number of indices in one segment at the last level\n+  const int     _last_level_segment_size_shift;\n+\n+  \/\/ For deallocation\n+  char*         _malloced;\n+\n+  \/\/ Contains the tree of claim variables\n+  volatile int* _claim_array;\n+\n+  \/\/ Claim index functions\n+\n+  \/\/ Number of claim entries at the given level\n+  static constexpr int claim_level_size(int level) {\n+    if (level == 0) {\n+      return 1;\n+    }\n+\n+    return level_multiplier(level - 1) * claim_level_size(level - 1);\n+  }\n+\n+  \/\/ The index the next level starts at\n+  static constexpr int claim_level_end_index(int level) {\n+    if (level == 0) {\n+\n+      \/\/ First level uses padding\n+      return ZCacheLineSize \/ sizeof(int);\n+    }\n+\n+    return claim_level_size(level) + claim_level_end_index(level - 1);\n+  }\n+\n+  static constexpr int claim_level_start_index(int level) {\n+    return claim_level_end_index(level - 1);\n+  }\n+\n+  \/\/ Total size used to hold all claim variables\n+  static size_t claim_variables_size() {\n+    return sizeof(int) * claim_level_end_index(ClaimLevels);\n+  }\n+\n+  \/\/ Returns the index of the start of the current segment of the current level\n+  static constexpr int claim_level_index_accumulate(int* indices, int level, int acc = 1) {\n+    if (level == 0) {\n+      return acc * indices[level];\n+    }\n+\n+    return acc * indices[level] + claim_level_index_accumulate(indices, level - 1, acc * level_multiplier(level));\n+  }\n+\n+  static constexpr int claim_level_index(int* indices, int level) {\n+    assert(level > 0, \"Must be\");\n+\n+    \/\/ The claim index for the current level is found in the previous levels\n+    return claim_level_index_accumulate(indices, level - 1);\n+  }\n+\n+  static constexpr int claim_index(int* indices, int level) {\n+    if (level == 0) {\n+      return 0;\n+    }\n+\n+    return claim_level_start_index(level) + claim_level_index(indices, level);\n+  }\n+\n+  \/\/ Claim functions\n+\n+  int claim(int index) {\n+    return Atomic::fetch_and_add(&_claim_array[index], 1, memory_order_relaxed);\n+  }\n+\n+  int claim_at(int* indices, int level) {\n+    const int index = claim_index(indices, level);\n+    const int value = claim(index);\n+#if 0\n+    if      (level == 0) { tty->print_cr(\"Claim at: %d index: %d got: %d\",             indices[0], index, value); }\n+    else if (level == 1) { tty->print_cr(\"Claim at: %d %d index: %d got: %d\",          indices[0], indices[1], index, value); }\n+    else if (level == 2) { tty->print_cr(\"Claim at: %d %d %d index: %d got: %d\",       indices[0], indices[1], indices[2], index, value); }\n+    else if (level == 3) { tty->print_cr(\"Claim at: %d %d %d %d index: %d got: %d\",    indices[0], indices[1], indices[2], indices[3], index, value); }\n+    else if (level == 4) { tty->print_cr(\"Claim at: %d %d %d %d %d index: %d got: %d\", indices[0], indices[1], indices[2], indices[3], indices[4], index, value); }\n+#endif\n+    return value;\n+  }\n+\n+  template <typename Function>\n+  void claim_and_do(Function function, int* indices, int level) {\n+    if (level < N) {\n+      \/\/ Visit ClaimLevels and the last level\n+      const int ci = claim_index(indices, level);\n+      for (indices[level] = 0; (indices[level] = claim(ci)) < level_segment_size(level);) {\n+        claim_and_do(function, indices, level + 1);\n+      }\n+      return;\n+    }\n+\n+    doit(function, indices);\n+  }\n+\n+  template <typename Function>\n+  void steal_and_do(Function function, int* indices, int level) {\n+    for (indices[level] = 0; indices[level] < level_segment_size(level); indices[level]++) {\n+      const int next_level = level + 1;\n+      \/\/ First try to claim at next level\n+      claim_and_do(function, indices, next_level);\n+      \/\/ Then steal at next level\n+      if (next_level < ClaimLevels) {\n+        steal_and_do(function, indices, next_level);\n+      }\n+    }\n+  }\n+\n+  \/\/ Functions to claimed values to an index\n+\n+  static constexpr int levels_size(int level) {\n+    if (level == 0) {\n+      return level_multiplier(0);\n+    }\n+\n+    return level_multiplier(level) * levels_size(level - 1);\n+  }\n+\n+  static int constexpr level_to_last_level_count_coverage(int level) {\n+    return levels_size(ClaimLevels - 1) \/ levels_size(level);\n+  }\n+\n+  static int constexpr calculate_last_level_count(int* indices, int level = 0) {\n+    if (level == N - 1) {\n+      return 0;\n+    }\n+\n+    return indices[level] * level_to_last_level_count_coverage(level) + calculate_last_level_count(indices, level + 1);\n+  }\n+\n+  int calculate_index(int* indices) {\n+    const int segment_start = calculate_last_level_count(indices) << _last_level_segment_size_shift;\n+    return segment_start + indices[N - 1];\n+  }\n+\n+  int level_segment_size(int level) {\n+    if (level == ClaimLevels) {\n+      return 1 << _last_level_segment_size_shift;\n+    }\n+\n+    return level_multiplier(level);\n+  }\n+\n+  template <typename Function>\n+  void doit(Function function, int* indices) {\n+    \/\/const int index = first_level * second_level_max * _third_level_max + second_level * _third_level_max + third_level;\n+    const int index = calculate_index(indices);\n+\n+#if 0\n+    tty->print_cr(\"doit Thread: \" PTR_FORMAT \": %d %d %d %d => %d\",\n+        p2i(Thread::current()),\n+        indices[0], indices[1], indices[2], indices[3], index);\n+#endif\n+\n+    function(index);\n+  }\n+\n+  static int last_level_segment_size_shift(int count) {\n+    const int last_level_size = count \/ levels_size(ClaimLevels - 1);\n+    assert(levels_size(ClaimLevels - 1) * last_level_size == count, \"Not exactly divisible\");\n+\n+    return log2i_exact(last_level_size);\n+  }\n+\n+public:\n+  ZIndexDistributorClaimTree(int count) :\n+      _last_level_segment_size_shift(last_level_segment_size_shift(count)),\n+      _malloced((char*)os::malloc(claim_variables_size() + os::vm_page_size(), mtGC)),\n+      _claim_array((volatile int*)align_up(_malloced, os::vm_page_size())) {\n+\n+    assert((levels_size(ClaimLevels - 1) << _last_level_segment_size_shift) == count, \"Incorrectly setup\");\n+\n+#if 0\n+    tty->print_cr(\"ZIndexDistributorClaimTree count: %d byte size: \" SIZE_FORMAT, count, claim_variables_size() + os::vm_page_size());\n+#endif\n+\n+    memset(_malloced, 0, claim_variables_size() + os::vm_page_size());\n+  }\n+\n+  ~ZIndexDistributorClaimTree() {\n+    os::free(_malloced);\n+  }\n+\n+  template <typename Function>\n+  void do_indices(Function function) {\n+    int indices[N];\n+    claim_and_do(function, indices, 0 \/* level *\/);\n+    steal_and_do(function, indices, 0 \/* level *\/);\n+  }\n+};\n+\n+\/\/ Using dynamically allocated objects just to be able to evaluate\n+\/\/ different strategies. Revert when one has been choosen.\n+\n+inline void* ZIndexDistributor::create_strategy(int count) {\n+  switch (ZIndexDistributorStrategy) {\n+  case 0: return new ZIndexDistributorClaimTree(count);\n+  case 1: return new ZIndexDistributorStriped(count);\n+  default: fatal(\"Unknown ZIndexDistributorStrategy\"); return nullptr;\n+  };\n+}\n+\n+inline ZIndexDistributor::ZIndexDistributor(int count) :\n+    _strategy(create_strategy(count)) {}\n+\n+inline ZIndexDistributor::~ZIndexDistributor() {\n+  switch (ZIndexDistributorStrategy) {\n+  case 0: delete static_cast<ZIndexDistributorClaimTree*>(_strategy); break;\n+  case 1: delete static_cast<ZIndexDistributorStriped*>(_strategy); break;\n+  default: fatal(\"Unknown ZIndexDistributorStrategy\"); break;\n+  };\n+}\n+\n+template <typename Strategy>\n+inline Strategy* ZIndexDistributor::strategy() {\n+  return static_cast<Strategy*>(_strategy);\n+}\n+\n+template <typename Function>\n+inline void ZIndexDistributor::do_indices(Function function) {\n+  switch (ZIndexDistributorStrategy) {\n+  case 0: strategy<ZIndexDistributorClaimTree>()->do_indices(function); break;\n+  case 1: strategy<ZIndexDistributorStriped>()->do_indices(function); break;\n+  default: fatal(\"Unknown ZIndexDistributorStrategy\");\n+  };\n+}\n+\n+#endif \/\/ SHARE_GC_Z_ZINDEXDISTRIBUTOR_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/z\/zIndexDistributor.inline.hpp","additions":331,"deletions":0,"binary":false,"changes":331,"status":"added"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,0 +28,2 @@\n+#include \"gc\/z\/zDriver.hpp\"\n+#include \"gc\/z\/zGCIdPrinter.hpp\"\n@@ -31,0 +33,1 @@\n+#include \"gc\/z\/zJNICritical.hpp\"\n@@ -32,0 +35,1 @@\n+#include \"gc\/z\/zMarkStackAllocator.hpp\"\n@@ -46,1 +50,1 @@\n-  ZAddress::initialize();\n+  ZGlobalsPointers::initialize();\n@@ -55,0 +59,3 @@\n+  ZJNICritical::initialize();\n+  ZDriver::initialize();\n+  ZGCIdPrinter::initialize();\n","filename":"src\/hotspot\/share\/gc\/z\/zInitialize.cpp","additions":9,"deletions":2,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -0,0 +1,64 @@\n+\/*\n+ * Copyright (c) 2021, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_Z_ZITERATOR_HPP\n+#define SHARE_GC_Z_ZITERATOR_HPP\n+\n+#include \"memory\/allocation.hpp\"\n+#include \"memory\/iterator.hpp\"\n+\n+class ZIterator : AllStatic {\n+private:\n+  static bool is_invisible_object(oop obj);\n+  static bool is_invisible_object_array(oop obj);\n+\n+public:\n+  \/\/ This iterator skips invisible roots\n+  template <typename OopClosureT>\n+  static void oop_iterate_safe(oop obj, OopClosureT* cl);\n+\n+  template <typename OopClosureT>\n+  static void oop_iterate(oop obj, OopClosureT* cl);\n+\n+  template <typename OopClosureT>\n+  static void oop_iterate_range(objArrayOop obj, OopClosureT* cl, int start, int end);\n+\n+  \/\/ This function skips invisible roots\n+  template <typename Function>\n+  static void basic_oop_iterate_safe(oop obj, Function function);\n+\n+  template <typename Function>\n+  static void basic_oop_iterate(oop obj, Function function);\n+};\n+\n+template <typename Function>\n+class ZObjectClosure : public ObjectClosure {\n+private:\n+  Function _function;\n+\n+public:\n+  ZObjectClosure(Function function);\n+  virtual void do_object(oop obj);\n+};\n+\n+#endif \/\/ SHARE_GC_Z_ZITERATOR_HPP\n","filename":"src\/hotspot\/share\/gc\/z\/zIterator.hpp","additions":64,"deletions":0,"binary":false,"changes":64,"status":"added"},{"patch":"@@ -0,0 +1,104 @@\n+\/*\n+ * Copyright (c) 2021, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_Z_ZITERATOR_INLINE_HPP\n+#define SHARE_GC_Z_ZITERATOR_INLINE_HPP\n+\n+#include \"gc\/z\/zIterator.hpp\"\n+\n+#include \"memory\/iterator.inline.hpp\"\n+#include \"oops\/objArrayOop.hpp\"\n+#include \"oops\/oop.inline.hpp\"\n+\n+inline bool ZIterator::is_invisible_object(oop obj) {\n+  return obj->mark_acquire().is_marked();\n+}\n+\n+inline bool ZIterator::is_invisible_object_array(oop obj) {\n+  return obj->klass()->is_objArray_klass() && is_invisible_object(obj);\n+}\n+\n+\/\/ This iterator skips invisible object arrays\n+template <typename OopClosureT>\n+void ZIterator::oop_iterate_safe(oop obj, OopClosureT* cl) {\n+  \/\/ Skip invisible object arrays - we only filter out *object* arrays,\n+  \/\/ because that check is arguably faster than the is_invisible_object\n+  \/\/ check, and primitive arrays are cheap to call oop_iterate on.\n+  if (!is_invisible_object_array(obj)) {\n+    obj->oop_iterate(cl);\n+  }\n+}\n+\n+template <typename OopClosureT>\n+void ZIterator::oop_iterate(oop obj, OopClosureT* cl) {\n+  assert(!is_invisible_object_array(obj), \"not safe\");\n+  obj->oop_iterate(cl);\n+}\n+\n+template <typename OopClosureT>\n+void ZIterator::oop_iterate_range(objArrayOop obj, OopClosureT* cl, int start, int end) {\n+  assert(!is_invisible_object_array(obj), \"not safe\");\n+  obj->oop_iterate_range(cl, start, end);\n+}\n+\n+template <typename Function>\n+class ZBasicOopIterateClosure : public BasicOopIterateClosure {\n+private:\n+  Function _function;\n+\n+public:\n+  ZBasicOopIterateClosure(Function function) :\n+      _function(function) {}\n+\n+  virtual void do_oop(oop* p) {\n+    _function((volatile zpointer*)p);\n+  }\n+\n+  virtual void do_oop(narrowOop* p_) {\n+    ShouldNotReachHere();\n+  }\n+};\n+\n+\/\/ This function skips invisible roots\n+template <typename Function>\n+void ZIterator::basic_oop_iterate_safe(oop obj, Function function) {\n+  ZBasicOopIterateClosure<Function> cl(function);\n+  oop_iterate_safe(obj, &cl);\n+}\n+\n+template <typename Function>\n+void ZIterator::basic_oop_iterate(oop obj, Function function) {\n+  ZBasicOopIterateClosure<Function> cl(function);\n+  oop_iterate(obj, &cl);\n+}\n+\n+template <typename Function>\n+ZObjectClosure<Function>::ZObjectClosure(Function function) :\n+    _function(function) {}\n+\n+template <typename Function>\n+void ZObjectClosure<Function>::do_object(oop obj) {\n+  _function(obj);\n+}\n+\n+#endif \/\/ SHARE_GC_Z_ZITERATOR_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/z\/zIterator.inline.hpp","additions":104,"deletions":0,"binary":false,"changes":104,"status":"added"},{"patch":"@@ -0,0 +1,183 @@\n+\/*\n+ * Copyright (c) 2021, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/z\/zJNICritical.hpp\"\n+#include \"gc\/z\/zLock.inline.hpp\"\n+#include \"gc\/z\/zStat.hpp\"\n+#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/interfaceSupport.inline.hpp\"\n+#include \"runtime\/thread.inline.hpp\"\n+#include \"utilities\/debug.hpp\"\n+\n+\/\/\n+\/\/ The JNI critical count reflects number of Java threads currently\n+\/\/ inside a JNI critical region.\n+\/\/\n+\/\/ * Normal (count >= 0). Java threads are allowed to enter and exit\n+\/\/   a critical region.\n+\/\/\n+\/\/ * Blocked (count == -1). No Java thread is inside a critical region,\n+\/\/   and no Java thread can enter a critical region.\n+\/\/\n+\/\/ * Block in progress (count < -1). Java threads are only allowed\n+\/\/   to exit a critical region. Attempts to enter a critical region\n+\/\/   will be blocked.\n+\/\/\n+\n+static const ZStatCriticalPhase ZCriticalPhaseJNICriticalStall(\"JNI Critical Stall\", false \/* verbose *\/);\n+\n+volatile int64_t ZJNICritical::_count;\n+ZConditionLock*  ZJNICritical::_lock;\n+\n+void ZJNICritical::initialize() {\n+  _count = 0;\n+  _lock = new ZConditionLock();\n+}\n+\n+void ZJNICritical::block() {\n+  for (;;) {\n+    const int64_t count = Atomic::load_acquire(&_count);\n+\n+    if (count < 0) {\n+      \/\/ Already blocked, wait until unblocked\n+      ZLocker<ZConditionLock> locker(_lock);\n+      while (Atomic::load_acquire(&_count) < 0) {\n+        _lock->wait();\n+      }\n+\n+      \/\/ Unblocked\n+      continue;\n+    }\n+\n+    \/\/ Increment and invert count\n+    if (Atomic::cmpxchg(&_count, count, -(count + 1)) != count) {\n+      continue;\n+    }\n+\n+    \/\/ If the previous count was 0, then we just incremented and inverted\n+    \/\/ it to -1 and we have now blocked. Otherwise we wait until all Java\n+    \/\/ threads have exited the critical region.\n+    if (count != 0) {\n+      \/\/ Wait until blocked\n+      ZLocker<ZConditionLock> locker(_lock);\n+      while (Atomic::load_acquire(&_count) != -1) {\n+        _lock->wait();\n+      }\n+    }\n+\n+    \/\/ Blocked\n+    return;\n+  }\n+}\n+\n+void ZJNICritical::unblock() {\n+  const int64_t count = Atomic::load_acquire(&_count);\n+  assert(count == -1, \"Invalid count\");\n+\n+  \/\/ Notify unblocked\n+  ZLocker<ZConditionLock> locker(_lock);\n+  Atomic::release_store(&_count, (int64_t)0);\n+  _lock->notify_all();\n+}\n+\n+void ZJNICritical::enter_inner(JavaThread* thread) {\n+  for (;;) {\n+    const int64_t count = Atomic::load_acquire(&_count);\n+\n+    if (count < 0) {\n+      \/\/ Wait until unblocked\n+      ZStatTimer timer(ZCriticalPhaseJNICriticalStall);\n+\n+      \/\/ Transition thread to blocked before locking to avoid deadlock\n+      ThreadBlockInVM tbivm(thread);\n+\n+      ZLocker<ZConditionLock> locker(_lock);\n+      while (Atomic::load_acquire(&_count) < 0) {\n+        _lock->wait();\n+      }\n+\n+      \/\/ Unblocked\n+      continue;\n+    }\n+\n+    \/\/ Increment count\n+    if (Atomic::cmpxchg(&_count, count, count + 1) != count) {\n+      continue;\n+    }\n+\n+    \/\/ Entered critical region\n+    return;\n+  }\n+}\n+\n+void ZJNICritical::enter(JavaThread* thread) {\n+  assert(thread == JavaThread::current(), \"Must be this thread\");\n+\n+  if (!thread->in_critical()) {\n+    enter_inner(thread);\n+  }\n+\n+  thread->enter_critical();\n+}\n+\n+void ZJNICritical::exit_inner() {\n+  for (;;) {\n+    const int64_t count = Atomic::load_acquire(&_count);\n+    assert(count != 0, \"Invalid count\");\n+\n+    if (count > 0) {\n+      \/\/ No block in progress, decrement count\n+      if (Atomic::cmpxchg(&_count, count, count - 1) != count) {\n+        continue;\n+      }\n+    } else {\n+      \/\/ Block in progress, increment count\n+      if (Atomic::cmpxchg(&_count, count, count + 1) != count) {\n+        continue;\n+      }\n+\n+      \/\/ If the previous count was -2, then we just incremented it to -1,\n+      \/\/ and we should signal that all Java threads have now exited the\n+      \/\/ critical region and we are now blocked.\n+      if (count == -2) {\n+        \/\/ Nofity blocked\n+        ZLocker<ZConditionLock> locker(_lock);\n+        _lock->notify_all();\n+      }\n+    }\n+\n+    \/\/ Exited critical region\n+    return;\n+  }\n+}\n+\n+void ZJNICritical::exit(JavaThread* thread) {\n+  assert(thread == JavaThread::current(), \"Must be this thread\");\n+\n+  thread->exit_critical();\n+\n+  if (!thread->in_critical()) {\n+    exit_inner();\n+  }\n+}\n","filename":"src\/hotspot\/share\/gc\/z\/zJNICritical.cpp","additions":183,"deletions":0,"binary":false,"changes":183,"status":"added"},{"patch":"@@ -0,0 +1,51 @@\n+\/*\n+ * Copyright (c) 2021, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_Z_ZJNICRITICAL_HPP\n+#define SHARE_GC_Z_ZJNICRITICAL_HPP\n+\n+#include \"memory\/allocation.hpp\"\n+\n+class JavaThread;\n+class ZConditionLock;\n+\n+class ZJNICritical : public AllStatic {\n+private:\n+  static volatile int64_t _count;\n+  static ZConditionLock*  _lock;\n+\n+  static void enter_inner(JavaThread* thread);\n+  static void exit_inner();\n+\n+public:\n+  \/\/ For use by GC\n+  static void initialize();\n+  static void block();\n+  static void unblock();\n+\n+  \/\/ For use by Java threads\n+  static void enter(JavaThread* thread);\n+  static void exit(JavaThread* thread);\n+};\n+\n+#endif \/\/ SHARE_GC_Z_ZJNICRITICAL_HPP\n","filename":"src\/hotspot\/share\/gc\/z\/zJNICritical.hpp","additions":51,"deletions":0,"binary":false,"changes":51,"status":"added"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -113,1 +113,1 @@\n-  return is_empty() ? NULL : cast_to_outer(_head._next);\n+  return is_empty() ? nullptr : cast_to_outer(_head._next);\n@@ -118,1 +118,1 @@\n-  return is_empty() ? NULL : cast_to_outer(_head._prev);\n+  return is_empty() ? nullptr : cast_to_outer(_head._prev);\n@@ -131,1 +131,1 @@\n-  return (next == &_head) ? NULL : cast_to_outer(next);\n+  return (next == &_head) ? nullptr : cast_to_outer(next);\n@@ -144,1 +144,1 @@\n-  return (prev == &_head) ? NULL : cast_to_outer(prev);\n+  return (prev == &_head) ? nullptr : cast_to_outer(prev);\n@@ -194,1 +194,1 @@\n-  if (elem != NULL) {\n+  if (elem != nullptr) {\n@@ -204,1 +204,1 @@\n-  if (elem != NULL) {\n+  if (elem != nullptr) {\n@@ -218,1 +218,1 @@\n-  if (_next != NULL) {\n+  if (_next != nullptr) {\n@@ -235,1 +235,1 @@\n-  return *elem != NULL;\n+  return *elem != nullptr;\n","filename":"src\/hotspot\/share\/gc\/z\/zList.inline.hpp","additions":9,"deletions":9,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -25,0 +25,1 @@\n+#include \"gc\/z\/zGeneration.inline.hpp\"\n@@ -28,1 +29,1 @@\n-#include \"gc\/z\/zThread.inline.hpp\"\n+#include \"gc\/z\/zUtils.hpp\"\n@@ -51,1 +52,2 @@\n-void ZLiveMap::reset(size_t index) {\n+void ZLiveMap::reset(ZGenerationId id) {\n+  ZGeneration* const generation = ZGeneration::generation(id);\n@@ -58,1 +60,1 @@\n-       seqnum != ZGlobalSeqNum;\n+       seqnum != generation->seqnum();\n@@ -76,1 +78,1 @@\n-      Atomic::release_store(&_seqnum, ZGlobalSeqNum);\n+      Atomic::release_store(&_seqnum, generation->seqnum());\n@@ -86,2 +88,2 @@\n-      log_trace(gc)(\"Mark seqnum reset contention, thread: \" PTR_FORMAT \" (%s), map: \" PTR_FORMAT \", bit: \" SIZE_FORMAT,\n-                    ZThread::id(), ZThread::name(), p2i(this), index);\n+      log_trace(gc)(\"Mark seqnum reset contention, thread: \" PTR_FORMAT \" (%s), map: \" PTR_FORMAT,\n+                    p2i(Thread::current()), ZUtils::thread_name(), p2i(this));\n@@ -105,1 +107,1 @@\n-                      ZThread::id(), ZThread::name(), p2i(this), segment);\n+                      p2i(Thread::current()), ZUtils::thread_name(), p2i(this), segment);\n","filename":"src\/hotspot\/share\/gc\/z\/zLiveMap.cpp","additions":10,"deletions":8,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,0 +27,1 @@\n+#include \"gc\/z\/zAddress.hpp\"\n@@ -28,0 +29,1 @@\n+#include \"gc\/z\/zGenerationId.hpp\"\n@@ -66,1 +68,1 @@\n-  void reset(size_t index);\n+  void reset(ZGenerationId id);\n@@ -69,1 +71,4 @@\n-  void iterate_segment(ObjectClosure* cl, BitMap::idx_t segment, uintptr_t page_start, size_t page_object_alignment_shift);\n+  size_t do_object(ObjectClosure* cl, zaddress addr) const;\n+\n+  template <typename Function>\n+  void iterate_segment(BitMap::idx_t segment, Function function);\n@@ -73,0 +78,1 @@\n+  ZLiveMap(const ZLiveMap& other) = delete;\n@@ -77,1 +83,1 @@\n-  bool is_marked() const;\n+  bool is_marked(ZGenerationId id) const;\n@@ -82,2 +88,2 @@\n-  bool get(size_t index) const;\n-  bool set(size_t index, bool finalizable, bool& inc_live);\n+  bool get(ZGenerationId id, BitMap::idx_t index) const;\n+  bool set(ZGenerationId id, BitMap::idx_t index, bool finalizable, bool& inc_live);\n@@ -87,1 +93,5 @@\n-  void iterate(ObjectClosure* cl, uintptr_t page_start, size_t page_object_alignment_shift);\n+  template <typename Function>\n+  void iterate(ZGenerationId id, Function function);\n+\n+  BitMap::idx_t find_base_bit(BitMap::idx_t index);\n+  BitMap::idx_t find_base_bit_in_segment(BitMap::idx_t start, BitMap::idx_t index);\n","filename":"src\/hotspot\/share\/gc\/z\/zLiveMap.hpp","additions":17,"deletions":7,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"gc\/z\/zAddress.inline.hpp\"\n@@ -30,0 +31,1 @@\n+#include \"gc\/z\/zGeneration.inline.hpp\"\n@@ -31,1 +33,0 @@\n-#include \"gc\/z\/zOop.inline.hpp\"\n@@ -41,2 +42,2 @@\n-inline bool ZLiveMap::is_marked() const {\n-  return Atomic::load_acquire(&_seqnum) == ZGlobalSeqNum;\n+inline bool ZLiveMap::is_marked(ZGenerationId id) const {\n+  return Atomic::load_acquire(&_seqnum) == ZGeneration::generation(id)->seqnum();\n@@ -46,1 +47,0 @@\n-  assert(ZGlobalPhase != ZPhaseMark, \"Invalid phase\");\n@@ -51,1 +51,0 @@\n-  assert(ZGlobalPhase != ZPhaseMark, \"Invalid phase\");\n@@ -99,3 +98,3 @@\n-inline bool ZLiveMap::get(size_t index) const {\n-  BitMap::idx_t segment = index_to_segment(index);\n-  return is_marked() &&                               \/\/ Page is marked\n+inline bool ZLiveMap::get(ZGenerationId id, BitMap::idx_t index) const {\n+  const BitMap::idx_t segment = index_to_segment(index);\n+  return is_marked(id) &&                             \/\/ Page is marked\n@@ -106,2 +105,2 @@\n-inline bool ZLiveMap::set(size_t index, bool finalizable, bool& inc_live) {\n-  if (!is_marked()) {\n+inline bool ZLiveMap::set(ZGenerationId id, BitMap::idx_t index, bool finalizable, bool& inc_live) {\n+  if (!is_marked(id)) {\n@@ -110,1 +109,1 @@\n-    reset(index);\n+    reset(id);\n@@ -136,1 +135,13 @@\n-inline void ZLiveMap::iterate_segment(ObjectClosure* cl, BitMap::idx_t segment, uintptr_t page_start, size_t page_object_alignment_shift) {\n+inline size_t ZLiveMap::do_object(ObjectClosure* cl, zaddress addr) const {\n+  \/\/ Get the size of the object before calling the closure, which\n+  \/\/ might overwrite the object in case we are relocating in-place.\n+  const size_t size = ZUtils::object_size(addr);\n+\n+  \/\/ Apply closure\n+  cl->do_object(to_oop(addr));\n+\n+  return size;\n+}\n+\n+template <typename Function>\n+inline void ZLiveMap::iterate_segment(BitMap::idx_t segment, Function function) {\n@@ -141,1 +152,0 @@\n-  BitMap::idx_t index = _bitmap.find_first_set_bit(start_index, end_index);\n@@ -143,7 +153,2 @@\n-  while (index < end_index) {\n-    \/\/ Calculate object address\n-    const uintptr_t addr = page_start + ((index \/ 2) << page_object_alignment_shift);\n-\n-    \/\/ Get the size of the object before calling the closure, which\n-    \/\/ might overwrite the object in case we are relocating in-place.\n-    const size_t size = ZUtils::object_size(addr);\n+  _bitmap.iterate(function, start_index, end_index);\n+}\n@@ -151,2 +156,5 @@\n-    \/\/ Apply closure\n-    cl->do_object(ZOop::from_address(addr));\n+template <typename Function>\n+inline void ZLiveMap::iterate(ZGenerationId id, Function function) {\n+  if (!is_marked(id)) {\n+    return;\n+  }\n@@ -154,6 +162,3 @@\n-    \/\/ Find next bit after this object\n-    const uintptr_t next_addr = align_up(addr + size, 1 << page_object_alignment_shift);\n-    const BitMap::idx_t next_index = ((next_addr - page_start) >> page_object_alignment_shift) * 2;\n-    if (next_index >= end_index) {\n-      \/\/ End of live map\n-      break;\n+  auto live_only = [&](BitMap::idx_t index) -> bool {\n+    if ((index & 1) == 0) {\n+      return function(index);\n@@ -161,0 +166,3 @@\n+    \/\/ Don't visit the finalizable bits\n+    return true;\n+  };\n@@ -162,1 +170,3 @@\n-    index = _bitmap.find_first_set_bit(next_index, end_index);\n+  for (BitMap::idx_t segment = first_live_segment(); segment < nsegments; segment = next_live_segment(segment)) {\n+    \/\/ For each live segment\n+    iterate_segment(segment, live_only);\n@@ -166,5 +176,17 @@\n-inline void ZLiveMap::iterate(ObjectClosure* cl, uintptr_t page_start, size_t page_object_alignment_shift) {\n-  if (is_marked()) {\n-    for (BitMap::idx_t segment = first_live_segment(); segment < nsegments; segment = next_live_segment(segment)) {\n-      \/\/ For each live segment\n-      iterate_segment(cl, segment, page_start, page_object_alignment_shift);\n+\/\/ Find the bit index that correspond the start of the object that is lower,\n+\/\/ or equal, to the given index (index is inclusive).\n+\/\/\n+\/\/ Typically used to find the start of an object when there's only a field\n+\/\/ address available. Note that it's not guaranteed that the found index\n+\/\/ corresponds to an object that spans the given index. This function just\n+\/\/ looks at the bits. The calling code is responsible to check the object\n+\/\/ at the returned index.\n+\/\/\n+\/\/ returns -1 if no bit was found\n+inline BitMap::idx_t ZLiveMap::find_base_bit(BitMap::idx_t index) {\n+  \/\/ Check first segment\n+  const BitMap::idx_t start_segment = index_to_segment(index);\n+  if (is_segment_live(start_segment)) {\n+    const BitMap::idx_t res = find_base_bit_in_segment(segment_start(start_segment), index);\n+    if (res != BitMap::idx_t(-1)) {\n+      return res;\n@@ -173,0 +195,32 @@\n+\n+  \/\/ Search earlier segments\n+  for (BitMap::idx_t segment = start_segment; segment-- > 0; ) {\n+    if (is_segment_live(segment)) {\n+      const BitMap::idx_t res = find_base_bit_in_segment(segment_start(segment), segment_end(segment) - 1);\n+      if (res != BitMap::idx_t(-1)) {\n+        return res;\n+      }\n+    }\n+  }\n+\n+  \/\/ Not found\n+  return BitMap::idx_t(-1);\n+}\n+\n+\/\/ Find the bit index that correspond the start of the object that is lower,\n+\/\/ or equal, to the given index (index is inclusive). Stopping when reaching\n+\/\/ start.\n+inline BitMap::idx_t ZLiveMap::find_base_bit_in_segment(BitMap::idx_t start, BitMap::idx_t index) {\n+  assert(index_to_segment(start) == index_to_segment(index), \"Only supports searches within segments start: %zu index: %zu\", start, index);\n+  assert(is_segment_live(index_to_segment(start)), \"Must be live\");\n+\n+  \/\/ Search backwards - + 1 to make an exclusive index.\n+  const BitMap::idx_t end = index + 1;\n+  const BitMap::idx_t bit = _bitmap.find_last_set_bit(start, end);\n+  if (bit == end) {\n+    return BitMap::idx_t(-1);\n+  }\n+\n+  \/\/ The bitmaps contain pairs of bits to deal with strongly marked vs only\n+  \/\/ finalizable marked. Align down to get the the first bit position.\n+  return bit & ~BitMap::idx_t(1);\n","filename":"src\/hotspot\/share\/gc\/z\/zLiveMap.inline.hpp","additions":88,"deletions":34,"binary":false,"changes":122,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -30,1 +30,1 @@\n-class ZLock {\n+class ZLock : public CHeapObj<mtGC> {\n@@ -55,1 +55,1 @@\n-class ZConditionLock {\n+class ZConditionLock : public CHeapObj<mtGC> {\n","filename":"src\/hotspot\/share\/gc\/z\/zLock.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -48,1 +48,1 @@\n-    _owner(NULL),\n+    _owner(nullptr),\n@@ -70,1 +70,1 @@\n-    Atomic::store(&_owner, (Thread*)NULL);\n+    Atomic::store(&_owner, (Thread*)nullptr);\n@@ -108,1 +108,1 @@\n-  if (_lock != NULL) {\n+  if (_lock != nullptr) {\n@@ -115,1 +115,1 @@\n-  if (_lock != NULL) {\n+  if (_lock != nullptr) {\n","filename":"src\/hotspot\/share\/gc\/z\/zLock.inline.hpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -33,0 +33,1 @@\n+#include \"gc\/shared\/workerThread.hpp\"\n@@ -34,0 +35,1 @@\n+#include \"gc\/z\/zAddress.inline.hpp\"\n@@ -35,0 +37,3 @@\n+#include \"gc\/z\/zBarrierSetNMethod.hpp\"\n+#include \"gc\/z\/zGeneration.inline.hpp\"\n+#include \"gc\/z\/zGenerationId.hpp\"\n@@ -43,1 +48,0 @@\n-#include \"gc\/z\/zOop.inline.hpp\"\n@@ -50,1 +54,0 @@\n-#include \"gc\/z\/zThread.inline.hpp\"\n@@ -52,0 +55,1 @@\n+#include \"gc\/z\/zUncoloredRoot.inline.hpp\"\n@@ -67,0 +71,1 @@\n+#include \"runtime\/vmThread.hpp\"\n@@ -72,4 +77,4 @@\n-static const ZStatSubPhase ZSubPhaseConcurrentMark(\"Concurrent Mark\");\n-static const ZStatSubPhase ZSubPhaseConcurrentMarkTryFlush(\"Concurrent Mark Try Flush\");\n-static const ZStatSubPhase ZSubPhaseConcurrentMarkTryTerminate(\"Concurrent Mark Try Terminate\");\n-static const ZStatSubPhase ZSubPhaseMarkTryComplete(\"Pause Mark Try Complete\");\n+static const ZStatSubPhase ZSubPhaseConcurrentMarkRootUncoloredYoung(\"Concurrent Mark Root Uncolored\", ZGenerationId::young);\n+static const ZStatSubPhase ZSubPhaseConcurrentMarkRootColoredYoung(\"Concurrent Mark Root Colored\", ZGenerationId::young);\n+static const ZStatSubPhase ZSubPhaseConcurrentMarkRootUncoloredOld(\"Concurrent Mark Root Uncolored\", ZGenerationId::old);\n+static const ZStatSubPhase ZSubPhaseConcurrentMarkRootColoredOld(\"Concurrent Mark Root Colored\", ZGenerationId::old);\n@@ -77,2 +82,2 @@\n-ZMark::ZMark(ZWorkers* workers, ZPageTable* page_table) :\n-    _workers(workers),\n+ZMark::ZMark(ZGeneration* generation, ZPageTable* page_table) :\n+    _generation(generation),\n@@ -81,1 +86,1 @@\n-    _stripes(),\n+    _stripes(_allocator.start()),\n@@ -83,1 +88,0 @@\n-    _work_terminateflush(true),\n@@ -110,10 +114,0 @@\n-  \/\/ Increment global sequence number to invalidate\n-  \/\/ marking information for all pages.\n-  ZGlobalSeqNum++;\n-\n-  \/\/ Note that we start a marking cycle.\n-  \/\/ Unlike other GCs, the color switch implicitly changes the nmethods\n-  \/\/ to be armed, and the thread-local disarm values are lazily updated\n-  \/\/ when JavaThreads wake up from safepoints.\n-  CodeCache::on_gc_marking_cycle_start();\n-\n@@ -127,1 +121,1 @@\n-  _nworkers = _workers->active_workers();\n+  _nworkers = workers()->active_workers();\n@@ -135,1 +129,1 @@\n-  ZStatMark::set_at_mark_start(nstripes);\n+  _generation->stat_mark()->at_mark_start(nstripes);\n@@ -150,0 +144,4 @@\n+ZWorkers* ZMark::workers() const {\n+  return _generation->workers();\n+}\n+\n@@ -151,1 +149,7 @@\n-  assert(_nworkers == _workers->active_workers(), \"Invalid number of workers\");\n+  \/\/ Set number of workers to use\n+  _nworkers = workers()->active_workers();\n+\n+  \/\/ Set number of mark stripes to use, based on number\n+  \/\/ of workers we will use in the concurrent mark phase.\n+  const size_t nstripes = calculate_nstripes(_nworkers);\n+  _stripes.set_nstripes(nstripes);\n@@ -158,1 +162,0 @@\n-  _work_terminateflush = true;\n@@ -167,2 +170,18 @@\n-bool ZMark::is_array(uintptr_t addr) const {\n-  return ZOop::from_address(addr)->is_objArray();\n+void ZMark::follow_work_complete() {\n+  follow_work(false \/* partial *\/);\n+}\n+\n+bool ZMark::follow_work_partial() {\n+  return follow_work(true \/* partial *\/);\n+}\n+\n+bool ZMark::is_array(zaddress addr) const {\n+  return to_oop(addr)->is_objArray();\n+}\n+\n+static uintptr_t encode_partial_array_offset(zpointer* addr) {\n+  return untype(ZAddress::offset(to_zaddress((uintptr_t)addr))) >> ZMarkPartialArrayMinSizeShift;\n+}\n+\n+static zpointer* decode_partial_array_offset(uintptr_t offset) {\n+  return (zpointer*)ZOffset::address(to_zoffset(offset << ZMarkPartialArrayMinSizeShift));\n@@ -171,1 +190,1 @@\n-void ZMark::push_partial_array(uintptr_t addr, size_t size, bool finalizable) {\n+void ZMark::push_partial_array(zpointer* addr, size_t length, bool finalizable) {\n@@ -173,4 +192,3 @@\n-  ZMarkThreadLocalStacks* const stacks = ZThreadLocalData::stacks(Thread::current());\n-  ZMarkStripe* const stripe = _stripes.stripe_for_addr(addr);\n-  const uintptr_t offset = ZAddress::offset(addr) >> ZMarkPartialArrayMinSizeShift;\n-  const uintptr_t length = size \/ oopSize;\n+  ZMarkThreadLocalStacks* const stacks = ZThreadLocalData::mark_stacks(Thread::current(), _generation->id());\n+  ZMarkStripe* const stripe = _stripes.stripe_for_addr((uintptr_t)addr);\n+  const uintptr_t offset = encode_partial_array_offset(addr);\n@@ -180,1 +198,4 @@\n-                                 addr, size, _stripes.stripe_id(stripe));\n+                                 p2i(addr), length, _stripes.stripe_id(stripe));\n+\n+  stacks->push(&_allocator, &_stripes, stripe, &_terminate, entry, false \/* publish *\/);\n+}\n@@ -182,1 +203,8 @@\n-  stacks->push(&_allocator, &_stripes, stripe, entry, false \/* publish *\/);\n+static void mark_barrier_on_oop_array(volatile zpointer* p, size_t length, bool finalizable, bool young) {\n+  for (volatile const zpointer* const end = p + length; p < end; p++) {\n+    if (young) {\n+      ZBarrier::mark_barrier_on_young_oop_field(p);\n+    } else {\n+      ZBarrier::mark_barrier_on_old_oop_field(p, finalizable);\n+    }\n+  }\n@@ -185,3 +213,2 @@\n-void ZMark::follow_small_array(uintptr_t addr, size_t size, bool finalizable) {\n-  assert(size <= ZMarkPartialArrayMinSize, \"Too large, should be split\");\n-  const size_t length = size \/ oopSize;\n+void ZMark::follow_array_elements_small(zpointer* addr, size_t length, bool finalizable) {\n+  assert(length <= ZMarkPartialArrayMinLength, \"Too large, should be split\");\n@@ -189,1 +216,1 @@\n-  log_develop_trace(gc, marking)(\"Array follow small: \" PTR_FORMAT \" (\" SIZE_FORMAT \")\", addr, size);\n+  log_develop_trace(gc, marking)(\"Array follow small: \" PTR_FORMAT \" (\" SIZE_FORMAT \")\", p2i(addr), length);\n@@ -191,1 +218,1 @@\n-  ZBarrier::mark_barrier_on_oop_array((oop*)addr, length, finalizable);\n+  mark_barrier_on_oop_array(addr, length, finalizable, _generation->is_young());\n@@ -194,5 +221,6 @@\n-void ZMark::follow_large_array(uintptr_t addr, size_t size, bool finalizable) {\n-  assert(size <= (size_t)arrayOopDesc::max_array_length(T_OBJECT) * oopSize, \"Too large\");\n-  assert(size > ZMarkPartialArrayMinSize, \"Too small, should not be split\");\n-  const uintptr_t start = addr;\n-  const uintptr_t end = start + size;\n+void ZMark::follow_array_elements_large(zpointer* addr, size_t length, bool finalizable) {\n+  assert(length <= (size_t)arrayOopDesc::max_array_length(T_OBJECT), \"Too large\");\n+  assert(length > ZMarkPartialArrayMinLength, \"Too small, should not be split\");\n+\n+  zpointer* const start = addr;\n+  zpointer* const end = start + length;\n@@ -203,3 +231,3 @@\n-  const uintptr_t middle_start = align_up(start + 1, ZMarkPartialArrayMinSize);\n-  const size_t    middle_size = align_down(end - middle_start, ZMarkPartialArrayMinSize);\n-  const uintptr_t middle_end = middle_start + middle_size;\n+  zpointer* const middle_start = align_up(start + 1, ZMarkPartialArrayMinSize);\n+  const size_t    middle_length = align_down(end - middle_start, ZMarkPartialArrayMinLength);\n+  zpointer* const middle_end = middle_start + middle_length;\n@@ -209,1 +237,1 @@\n-                                 start, end, size, middle_start, middle_end, middle_size);\n+                                 p2i(start), p2i(end), length, p2i(middle_start), p2i(middle_end), middle_length);\n@@ -213,3 +241,3 @@\n-    const uintptr_t trailing_addr = middle_end;\n-    const size_t trailing_size = end - middle_end;\n-    push_partial_array(trailing_addr, trailing_size, finalizable);\n+    zpointer* const trailing_addr = middle_end;\n+    const size_t trailing_length = end - middle_end;\n+    push_partial_array(trailing_addr, trailing_length, finalizable);\n@@ -219,1 +247,1 @@\n-  uintptr_t partial_addr = middle_end;\n+  zpointer* partial_addr = middle_end;\n@@ -222,3 +250,3 @@\n-    const size_t partial_size = align_up((partial_addr - middle_start) \/ parts, ZMarkPartialArrayMinSize);\n-    partial_addr -= partial_size;\n-    push_partial_array(partial_addr, partial_size, finalizable);\n+    const size_t partial_length = align_up((partial_addr - middle_start) \/ parts, ZMarkPartialArrayMinLength);\n+    partial_addr -= partial_length;\n+    push_partial_array(partial_addr, partial_length, finalizable);\n@@ -229,3 +257,3 @@\n-  const uintptr_t leading_addr = start;\n-  const size_t leading_size = middle_start - start;\n-  follow_small_array(leading_addr, leading_size, finalizable);\n+  zpointer* const leading_addr = start;\n+  const size_t leading_length = middle_start - start;\n+  follow_array_elements_small(leading_addr, leading_length, finalizable);\n@@ -234,3 +262,3 @@\n-void ZMark::follow_array(uintptr_t addr, size_t size, bool finalizable) {\n-  if (size <= ZMarkPartialArrayMinSize) {\n-    follow_small_array(addr, size, finalizable);\n+void ZMark::follow_array_elements(zpointer* addr, size_t length, bool finalizable) {\n+  if (length <= ZMarkPartialArrayMinLength) {\n+    follow_array_elements_small(addr, length, finalizable);\n@@ -238,1 +266,1 @@\n-    follow_large_array(addr, size, finalizable);\n+    follow_array_elements_large(addr, length, finalizable);\n@@ -243,2 +271,2 @@\n-  const uintptr_t addr = ZAddress::good(entry.partial_array_offset() << ZMarkPartialArrayMinSizeShift);\n-  const size_t size = entry.partial_array_length() * oopSize;\n+  zpointer* const addr = decode_partial_array_offset(entry.partial_array_offset());\n+  const size_t length = entry.partial_array_length();\n@@ -246,1 +274,1 @@\n-  follow_array(addr, size, finalizable);\n+  follow_array_elements(addr, length, finalizable);\n@@ -249,2 +277,23 @@\n-template <bool finalizable>\n-class ZMarkBarrierOopClosure : public ClaimMetadataVisitingOopIterateClosure {\n+template <bool finalizable, ZGenerationIdOptional generation>\n+class ZMarkBarrierFollowOopClosure : public OopIterateClosure {\n+private:\n+  static int claim_value() {\n+    return finalizable ? ClassLoaderData::_claim_finalizable\n+                       : ClassLoaderData::_claim_strong;\n+  }\n+\n+  static ReferenceDiscoverer* discoverer() {\n+    if (!finalizable) {\n+      return ZGeneration::old()->reference_discoverer();\n+    } else {\n+      return nullptr;\n+    }\n+  }\n+\n+  static bool visit_metadata() {\n+    \/\/ Only visit metadata if we're marking through the old generation\n+    return ZGeneration::old()->is_phase_mark();\n+  }\n+\n+  const bool _visit_metadata;\n+\n@@ -252,7 +301,3 @@\n-  ZMarkBarrierOopClosure() :\n-      ClaimMetadataVisitingOopIterateClosure(finalizable\n-                                                 ? ClassLoaderData::_claim_finalizable\n-                                                 : ClassLoaderData::_claim_strong,\n-                                             finalizable\n-                                                 ? NULL\n-                                                 : ZHeap::heap()->reference_discoverer()) {}\n+  ZMarkBarrierFollowOopClosure() :\n+      OopIterateClosure(discoverer()),\n+      _visit_metadata(visit_metadata()) {}\n@@ -261,1 +306,11 @@\n-    ZBarrier::mark_barrier_on_oop_field(p, finalizable);\n+    switch (generation) {\n+    case ZGenerationIdOptional::young:\n+      ZBarrier::mark_barrier_on_young_oop_field((volatile zpointer*)p);\n+      break;\n+    case ZGenerationIdOptional::old:\n+      ZBarrier::mark_barrier_on_old_oop_field((volatile zpointer*)p, finalizable);\n+      break;\n+    case ZGenerationIdOptional::none:\n+      ZBarrier::mark_barrier_on_oop_field((volatile zpointer*)p, finalizable);\n+      break;\n+    }\n@@ -268,0 +323,5 @@\n+  virtual bool do_metadata() final {\n+    \/\/ Only help out with metadata visiting\n+    return _visit_metadata;\n+  }\n+\n@@ -269,0 +329,1 @@\n+    assert(do_metadata(), \"Don't call otherwise\");\n@@ -272,0 +333,16 @@\n+\n+  virtual void do_method(Method* m) {\n+    \/\/ Mark interpreted frames for class redefinition\n+    m->record_gc_epoch();\n+  }\n+\n+  virtual void do_klass(Klass* klass) {\n+    ClassLoaderData* cld = klass->class_loader_data();\n+    ZMarkBarrierFollowOopClosure<finalizable, ZGenerationIdOptional::none> cl;\n+    cld->oops_do(&cl, claim_value());\n+  }\n+\n+  virtual void do_cld(ClassLoaderData* cld) {\n+    ZMarkBarrierFollowOopClosure<finalizable, ZGenerationIdOptional::none> cl;\n+    cld->oops_do(&cl, claim_value());\n+  }\n@@ -275,3 +352,8 @@\n-  if (finalizable) {\n-    ZMarkBarrierOopClosure<true \/* finalizable *\/> cl;\n-    cl.do_klass(obj->klass());\n+  if (_generation->is_old()) {\n+    if (finalizable) {\n+      ZMarkBarrierFollowOopClosure<true \/* finalizable *\/, ZGenerationIdOptional::old> cl;\n+      cl.do_klass(obj->klass());\n+    } else {\n+      ZMarkBarrierFollowOopClosure<false \/* finalizable *\/, ZGenerationIdOptional::old> cl;\n+      cl.do_klass(obj->klass());\n+    }\n@@ -279,2 +361,4 @@\n-    ZMarkBarrierOopClosure<false \/* finalizable *\/> cl;\n-    cl.do_klass(obj->klass());\n+    ZMarkBarrierFollowOopClosure<false \/* finalizable *\/, ZGenerationIdOptional::none> cl;\n+    if (cl.do_metadata()) {\n+      cl.do_klass(obj->klass());\n+    }\n@@ -283,2 +367,2 @@\n-  const uintptr_t addr = (uintptr_t)obj->base();\n-  const size_t size = (size_t)obj->length() * oopSize;\n+  \/\/ Should be convertible to colorless oop\n+  assert_is_valid(to_zaddress(obj));\n@@ -286,1 +370,4 @@\n-  follow_array(addr, size, finalizable);\n+  zpointer* const addr = (zpointer*)obj->base();\n+  const size_t length = (size_t)obj->length();\n+\n+  follow_array_elements(addr, length, finalizable);\n@@ -290,11 +377,12 @@\n-  if (ContinuationGCSupport::relativize_stack_chunk(obj)) {\n-    \/\/ Loom doesn't support mixing of finalizable marking and strong marking of\n-    \/\/ stack chunks. See: RelativizeDerivedOopClosure.\n-    ZMarkBarrierOopClosure<false \/* finalizable *\/> cl;\n-    obj->oop_iterate(&cl);\n-    return;\n-  }\n-\n-  if (finalizable) {\n-    ZMarkBarrierOopClosure<true \/* finalizable *\/> cl;\n-    obj->oop_iterate(&cl);\n+  if (_generation->is_old()) {\n+    if (ZHeap::heap()->is_old(to_zaddress(obj))) {\n+      if (finalizable) {\n+        ZMarkBarrierFollowOopClosure<true \/* finalizable *\/, ZGenerationIdOptional::old> cl;\n+        ZIterator::oop_iterate(obj, &cl);\n+      } else {\n+        ZMarkBarrierFollowOopClosure<false \/* finalizable *\/, ZGenerationIdOptional::old> cl;\n+        ZIterator::oop_iterate(obj, &cl);\n+      }\n+    } else {\n+      fatal(\"Catch me!\");\n+    }\n@@ -302,2 +390,3 @@\n-    ZMarkBarrierOopClosure<false \/* finalizable *\/> cl;\n-    obj->oop_iterate(&cl);\n+    \/\/ Young gen must help out with old marking\n+    ZMarkBarrierFollowOopClosure<false \/* finalizable *\/, ZGenerationIdOptional::young> cl;\n+    ZIterator::oop_iterate(obj, &cl);\n@@ -338,1 +427,1 @@\n-  const uintptr_t addr = entry.object_address();\n+  const zaddress addr = ZOffset::address(to_zoffset(entry.object_address()));\n@@ -365,1 +454,1 @@\n-      follow_array_object(objArrayOop(ZOop::from_address(addr)), finalizable);\n+      follow_array_object(objArrayOop(to_oop(addr)), finalizable);\n@@ -367,1 +456,1 @@\n-      const oop obj = ZOop::from_address(addr);\n+      const oop obj = to_oop(addr);\n@@ -376,3 +465,30 @@\n-template <typename T>\n-bool ZMark::drain(ZMarkContext* context, T* timeout) {\n-  ZMarkStripe* const stripe = context->stripe();\n+\/\/ This function returns true if we need to stop working to resize threads or\n+\/\/ abort marking\n+bool ZMark::rebalance_work(ZMarkContext* context) {\n+  const size_t assumed_nstripes = context->nstripes();\n+  const size_t nstripes = _stripes.nstripes();\n+\n+  if (assumed_nstripes != nstripes) {\n+    context->set_nstripes(nstripes);\n+  } else if (nstripes < calculate_nstripes(_nworkers) && _allocator.clear_and_get_expanded_recently()) {\n+    const size_t new_nstripes = nstripes << 1;\n+    _stripes.set_nstripes(new_nstripes);\n+    context->set_nstripes(new_nstripes);\n+  }\n+\n+  ZMarkStripe* stripe = _stripes.stripe_for_worker(_nworkers, WorkerThread::worker_id());\n+  if (context->stripe() != stripe) {\n+    \/\/ Need to switch stripe\n+    context->set_stripe(stripe);\n+    flush_and_free();\n+  } else if (!_terminate.saturated()) {\n+    \/\/ Work imbalance detected; striped marking is likely going to be in the way\n+    flush_and_free();\n+  }\n+\n+  SuspendibleThreadSet::yield();\n+\n+  return ZAbort::should_abort() || _generation->should_worker_resize();\n+}\n+\n+bool ZMark::drain(ZMarkContext* context) {\n@@ -381,0 +497,4 @@\n+  size_t processed = 0;\n+\n+  context->set_stripe(_stripes.stripe_for_worker(_nworkers, WorkerThread::worker_id()));\n+  context->set_nstripes(_stripes.nstripes());\n@@ -383,1 +503,1 @@\n-  while (stacks->pop(&_allocator, &_stripes, stripe, entry)) {\n+  while (stacks->pop(&_allocator, &_stripes, context->stripe(), entry)) {\n@@ -386,3 +506,1 @@\n-    \/\/ Check timeout\n-    if (timeout->has_expired()) {\n-      \/\/ Timeout\n+    if ((processed++ & 31) == 0 && rebalance_work(context)) {\n@@ -393,2 +511,1 @@\n-  \/\/ Success\n-  return !timeout->has_expired();\n+  return true;\n@@ -406,1 +523,1 @@\n-    if (stack != NULL) {\n+    if (stack != nullptr) {\n@@ -426,1 +543,1 @@\n-    if (stack != NULL) {\n+    if (stack != nullptr) {\n@@ -441,4 +558,0 @@\n-void ZMark::idle() const {\n-  os::naked_short_sleep(1);\n-}\n-\n@@ -459,0 +572,3 @@\n+      if (SafepointSynchronize::is_at_safepoint()) {\n+        log_debug(gc, marking)(\"Thread broke mark termination %s\", thread->name());\n+      }\n@@ -467,6 +583,20 @@\n-bool ZMark::flush(bool at_safepoint) {\n-  ZMarkFlushAndFreeStacksClosure cl(this);\n-  if (at_safepoint) {\n-    Threads::threads_do(&cl);\n-  } else {\n-    Handshake::execute(&cl);\n+class VM_ZMarkFlushOperation : public VM_Operation {\n+private:\n+  ThreadClosure* _cl;\n+\n+public:\n+  VM_ZMarkFlushOperation(ThreadClosure* cl) :\n+      _cl(cl) {}\n+\n+  virtual bool evaluate_at_safepoint() const {\n+    return false;\n+  }\n+\n+  virtual void doit() {\n+    \/\/ Flush VM thread\n+    Thread* const thread = Thread::current();\n+    _cl->do_thread(thread);\n+  }\n+\n+  virtual VMOp_Type type() const {\n+    return VMOp_ZMarkFlushOperation;\n@@ -474,0 +604,7 @@\n+};\n+\n+bool ZMark::flush() {\n+  ZMarkFlushAndFreeStacksClosure cl(this);\n+  VM_ZMarkFlushOperation vm_cl(&cl);\n+  Handshake::execute(&cl);\n+  VMThread::execute(&vm_cl);\n@@ -479,2 +616,3 @@\n-bool ZMark::try_flush(volatile size_t* nflush) {\n-  Atomic::inc(nflush);\n+bool ZMark::try_terminate_flush() {\n+  Atomic::inc(&_work_nterminateflush);\n+  _terminate.set_resurrected(false);\n@@ -482,2 +620,6 @@\n-  ZStatTimer timer(ZSubPhaseConcurrentMarkTryFlush);\n-  return flush(false \/* at_safepoint *\/);\n+  if (ZVerifyMarking) {\n+    verify_worker_stacks_empty();\n+  }\n+\n+  return flush() ||\n+         _terminate.resurrected();\n@@ -488,1 +630,1 @@\n-  if (ZThread::worker_id() != 0) {\n+  if (WorkerThread::worker_id() != 0) {\n@@ -492,2 +634,1 @@\n-  if (Atomic::load(&_work_nproactiveflush) == ZMarkProactiveFlushMax ||\n-      Atomic::load(&_work_nterminateflush) != 0) {\n+  if (Atomic::load(&_work_nproactiveflush) == ZMarkProactiveFlushMax) {\n@@ -498,33 +639,1 @@\n-  return try_flush(&_work_nproactiveflush);\n-}\n-\n-bool ZMark::try_terminate() {\n-  ZStatTimer timer(ZSubPhaseConcurrentMarkTryTerminate);\n-\n-  if (_terminate.enter_stage0()) {\n-    \/\/ Last thread entered stage 0, flush\n-    if (Atomic::load(&_work_terminateflush) &&\n-        Atomic::load(&_work_nterminateflush) != ZMarkTerminateFlushMax) {\n-      \/\/ Exit stage 0 to allow other threads to continue marking\n-      _terminate.exit_stage0();\n-\n-      \/\/ Flush before termination\n-      if (!try_flush(&_work_nterminateflush)) {\n-        \/\/ No more work available, skip further flush attempts\n-        Atomic::store(&_work_terminateflush, false);\n-      }\n-\n-      \/\/ Don't terminate, regardless of whether we successfully\n-      \/\/ flushed out more work or not. We've already exited\n-      \/\/ termination stage 0, to allow other threads to continue\n-      \/\/ marking, so this thread has to return false and also\n-      \/\/ make another round of attempted marking.\n-      return false;\n-    }\n-  }\n-\n-  for (;;) {\n-    if (_terminate.enter_stage1()) {\n-      \/\/ Last thread entered stage 1, terminate\n-      return true;\n-    }\n+  Atomic::inc(&_work_nproactiveflush);\n@@ -532,8 +641,3 @@\n-    \/\/ Idle to give the other threads\n-    \/\/ a chance to enter termination.\n-    idle();\n-\n-    if (!_terminate.try_exit_stage1()) {\n-      \/\/ All workers in stage 1, terminate\n-      return true;\n-    }\n+  SuspendibleThreadSetLeaver sts_leaver;\n+  return flush();\n+}\n@@ -541,5 +645,2 @@\n-    if (_terminate.try_exit_stage0()) {\n-      \/\/ More work available, don't terminate\n-      return false;\n-    }\n-  }\n+bool ZMark::try_terminate(ZMarkContext* context) {\n+  return _terminate.try_terminate(&_stripes, context->nstripes());\n@@ -548,7 +649,3 @@\n-class ZMarkNoTimeout : public StackObj {\n-public:\n-  bool has_expired() {\n-    \/\/ No timeout, but check for signal to abort\n-    return ZAbort::should_abort();\n-  }\n-};\n+void ZMark::leave() {\n+  _terminate.leave();\n+}\n@@ -556,3 +653,6 @@\n-void ZMark::work_without_timeout(ZMarkContext* context) {\n-  ZStatTimer timer(ZSubPhaseConcurrentMark);\n-  ZMarkNoTimeout no_timeout;\n+\/\/ Returning true means marking finished successfully after marking as far as it could.\n+\/\/ Returning false means that marking finished unsuccessfully due to abort or resizing.\n+bool ZMark::follow_work(bool partial) {\n+  ZMarkStripe* const stripe = _stripes.stripe_for_worker(_nworkers, WorkerThread::worker_id());\n+  ZMarkThreadLocalStacks* const stacks = ZThreadLocalData::mark_stacks(Thread::current(), _generation->id());\n+  ZMarkContext context(ZMarkStripesMax, stripe, stacks);\n@@ -561,3 +661,3 @@\n-    if (!drain(context, &no_timeout)) {\n-      \/\/ Abort\n-      break;\n+    if (!drain(&context)) {\n+      leave();\n+      return false;\n@@ -566,1 +666,1 @@\n-    if (try_steal(context)) {\n+    if (try_steal(&context)) {\n@@ -571,0 +671,4 @@\n+    if (partial) {\n+      return true;\n+    }\n+\n@@ -576,1 +680,1 @@\n-    if (try_terminate()) {\n+    if (try_terminate(&context)) {\n@@ -578,1 +682,1 @@\n-      break;\n+      return true;\n@@ -583,9 +687,1 @@\n-class ZMarkTimeout : public StackObj {\n-private:\n-  const Ticks    _start;\n-  const uint64_t _timeout;\n-  const uint64_t _check_interval;\n-  uint64_t       _check_at;\n-  uint64_t       _check_count;\n-  bool           _expired;\n-\n+class ZMarkOopClosure : public OopClosure {\n@@ -593,45 +689,2 @@\n-  ZMarkTimeout(uint64_t timeout_in_micros) :\n-      _start(Ticks::now()),\n-      _timeout(_start.value() + TimeHelper::micros_to_counter(timeout_in_micros)),\n-      _check_interval(200),\n-      _check_at(_check_interval),\n-      _check_count(0),\n-      _expired(false) {}\n-\n-  ~ZMarkTimeout() {\n-    const Tickspan duration = Ticks::now() - _start;\n-    log_debug(gc, marking)(\"Mark With Timeout (%s): %s, \" UINT64_FORMAT \" oops, %.3fms\",\n-                           ZThread::name(), _expired ? \"Expired\" : \"Completed\",\n-                           _check_count, TimeHelper::counter_to_millis(duration.value()));\n-  }\n-\n-  bool has_expired() {\n-    if (++_check_count == _check_at) {\n-      _check_at += _check_interval;\n-      if ((uint64_t)Ticks::now().value() >= _timeout) {\n-        \/\/ Timeout\n-        _expired = true;\n-      }\n-    }\n-\n-    return _expired;\n-  }\n-};\n-\n-void ZMark::work_with_timeout(ZMarkContext* context, uint64_t timeout_in_micros) {\n-  ZStatTimer timer(ZSubPhaseMarkTryComplete);\n-  ZMarkTimeout timeout(timeout_in_micros);\n-\n-  for (;;) {\n-    if (!drain(context, &timeout)) {\n-      \/\/ Timed out\n-      break;\n-    }\n-\n-    if (try_steal(context)) {\n-      \/\/ Stole work\n-      continue;\n-    }\n-\n-    \/\/ Terminate\n-    break;\n+  virtual void do_oop(oop* p) {\n+    ZBarrier::mark_barrier_on_oop_field((zpointer*)p, false \/* finalizable *\/);\n@@ -639,6 +692,0 @@\n-}\n-\n-void ZMark::work(uint64_t timeout_in_micros) {\n-  ZMarkStripe* const stripe = _stripes.stripe_for_worker(_nworkers, ZThread::worker_id());\n-  ZMarkThreadLocalStacks* const stacks = ZThreadLocalData::stacks(Thread::current());\n-  ZMarkContext context(_stripes.nstripes(), stripe, stacks);\n@@ -646,4 +693,2 @@\n-  if (timeout_in_micros == 0) {\n-    work_without_timeout(&context);\n-  } else {\n-    work_with_timeout(&context, timeout_in_micros);\n+  virtual void do_oop(narrowOop* p) {\n+    ShouldNotReachHere();\n@@ -651,0 +696,1 @@\n+};\n@@ -652,8 +698,2 @@\n-  \/\/ Flush and publish stacks\n-  stacks->flush(&_allocator, &_stripes);\n-\n-  \/\/ Free remaining stacks\n-  stacks->free(&_allocator);\n-}\n-\n-class ZMarkOopClosure : public OopClosure {\n+class ZMarkYoungOopClosure : public OopClosure {\n+public:\n@@ -661,1 +701,1 @@\n-    ZBarrier::mark_barrier_on_oop_field(p, false \/* finalizable *\/);\n+    ZBarrier::mark_young_good_barrier_on_oop_field((zpointer*)p);\n@@ -671,1 +711,3 @@\n-  OopClosure* const _cl;\n+  static ZUncoloredRoot::RootFunction root_function() {\n+    return ZUncoloredRoot::mark;\n+  }\n@@ -674,2 +716,1 @@\n-  ZMarkThreadClosure(OopClosure* cl) :\n-      _cl(cl) {\n+  ZMarkThreadClosure() {\n@@ -681,0 +722,1 @@\n+\n@@ -683,1 +725,2 @@\n-    StackWatermarkSet::finish_processing(jt, _cl, StackWatermarkKind::gc);\n+\n+    StackWatermarkSet::finish_processing(jt, (void*)root_function(), StackWatermarkKind::gc);\n@@ -690,1 +733,1 @@\n-  OopClosure* const _cl;\n+  ZBarrierSetNMethod* const _bs_nm;\n@@ -693,2 +736,2 @@\n-  ZMarkNMethodClosure(OopClosure* cl) :\n-      _cl(cl) {}\n+  ZMarkNMethodClosure() :\n+      _bs_nm(static_cast<ZBarrierSetNMethod*>(BarrierSet::barrier_set()->barrier_set_nmethod())) {}\n@@ -698,2 +741,7 @@\n-    if (ZNMethod::is_armed(nm)) {\n-      ZNMethod::nmethod_oops_do_inner(nm, _cl);\n+    if (_bs_nm->is_armed(nm)) {\n+      \/\/ Heal barriers\n+      ZNMethod::nmethod_patch_barriers(nm);\n+\n+      \/\/ Heal oops\n+      ZUncoloredRootMarkOopClosure cl(ZNMethod::color(nm));\n+      ZNMethod::nmethod_oops_do_inner(nm, &cl);\n@@ -704,1 +752,53 @@\n-      ZNMethod::disarm(nm);\n+      log_trace(gc, nmethod)(\"nmethod: \" PTR_FORMAT \" visited by old\", p2i(nm));\n+\n+      \/\/ Disarm\n+      _bs_nm->disarm(nm);\n+    }\n+  }\n+};\n+\n+class ZMarkYoungNMethodClosure : public NMethodClosure {\n+private:\n+  ZBarrierSetNMethod* const _bs_nm;\n+\n+public:\n+  ZMarkYoungNMethodClosure() :\n+      _bs_nm(static_cast<ZBarrierSetNMethod*>(BarrierSet::barrier_set()->barrier_set_nmethod())) {}\n+\n+  virtual void do_nmethod(nmethod* nm) {\n+    ZLocker<ZReentrantLock> locker(ZNMethod::lock_for_nmethod(nm));\n+    if (nm->is_unloading()) {\n+      return;\n+    }\n+\n+    if (_bs_nm->is_armed(nm)) {\n+      const uintptr_t prev_color = ZNMethod::color(nm);\n+\n+      \/\/ Heal oops\n+      ZUncoloredRootMarkYoungOopClosure cl(prev_color);\n+      ZNMethod::nmethod_oops_do_inner(nm, &cl);\n+\n+      \/\/ Disarm only the young marking, not any potential old marking cycle\n+\n+      const uintptr_t old_marked_mask = ZPointerMarkedMask ^ (ZPointerMarkedYoung0 | ZPointerMarkedYoung1);\n+      const uintptr_t old_marked = prev_color & old_marked_mask;\n+\n+      const zpointer new_disarm_value_ptr = ZAddress::color(zaddress::null, ZPointerLoadGoodMask | ZPointerMarkedYoung | old_marked | ZPointerRemembered);\n+\n+      \/\/ Check if disarming for young mark, completely disarms the nmethod entry barrier\n+      const bool complete_disarm = ZPointer::is_store_good(new_disarm_value_ptr);\n+\n+      if (complete_disarm) {\n+        \/\/ We are about to completely disarm the nmethod, must take responsibility to patch all barriers before disarming\n+        ZNMethod::nmethod_patch_barriers(nm);\n+      }\n+\n+      _bs_nm->set_guard_value(nm, (int)untype(new_disarm_value_ptr));\n+\n+      if (complete_disarm) {\n+        log_trace(gc, nmethod)(\"nmethod: \" PTR_FORMAT \" visited by young (complete) [\" PTR_FORMAT \" -> \" PTR_FORMAT \"]\", p2i(nm), prev_color, untype(new_disarm_value_ptr));\n+        assert(!_bs_nm->is_armed(nm), \"Must not be considered armed anymore\");\n+      } else {\n+        log_trace(gc, nmethod)(\"nmethod: \" PTR_FORMAT \" visited by young (incomplete) [\" PTR_FORMAT \" -> \" PTR_FORMAT \"]\", p2i(nm), prev_color, untype(new_disarm_value_ptr));\n+        assert(_bs_nm->is_armed(nm), \"Must be considered armed\");\n+      }\n@@ -709,1 +809,1 @@\n-typedef ClaimingCLDToOopClosure<ClassLoaderData::_claim_strong> ZMarkCLDClosure;\n+typedef ClaimingCLDToOopClosure<ClassLoaderData::_claim_strong> ZMarkOldCLDClosure;\n@@ -711,1 +811,1 @@\n-class ZMarkRootsTask : public ZTask {\n+class ZMarkOldRootsTask : public ZTask {\n@@ -713,3 +813,3 @@\n-  ZMark* const               _mark;\n-  SuspendibleThreadSetJoiner _sts_joiner;\n-  ZRootsIterator             _roots;\n+  ZMark* const                  _mark;\n+  ZRootsIteratorStrongColored   _roots_colored;\n+  ZRootsIteratorStrongUncolored _roots_uncolored;\n@@ -717,4 +817,5 @@\n-  ZMarkOopClosure            _cl;\n-  ZMarkCLDClosure            _cld_cl;\n-  ZMarkThreadClosure         _thread_cl;\n-  ZMarkNMethodClosure        _nm_cl;\n+  ZMarkOopClosure               _cl_colored;\n+  ZMarkOldCLDClosure            _cld_cl;\n+\n+  ZMarkThreadClosure            _thread_cl;\n+  ZMarkNMethodClosure           _nm_cl;\n@@ -723,2 +824,2 @@\n-  ZMarkRootsTask(ZMark* mark) :\n-      ZTask(\"ZMarkRootsTask\"),\n+  ZMarkOldRootsTask(ZMark* mark) :\n+      ZTask(\"ZMarkOldRootsTask\"),\n@@ -726,6 +827,6 @@\n-      _sts_joiner(),\n-      _roots(ClassLoaderData::_claim_strong),\n-      _cl(),\n-      _cld_cl(&_cl),\n-      _thread_cl(&_cl),\n-      _nm_cl(&_cl) {\n+      _roots_colored(ZGenerationIdOptional::old),\n+      _roots_uncolored(ZGenerationIdOptional::old),\n+      _cl_colored(),\n+      _cld_cl(&_cl_colored),\n+      _thread_cl(),\n+      _nm_cl() {\n@@ -735,1 +836,1 @@\n-  ~ZMarkRootsTask() {\n+  ~ZMarkOldRootsTask() {\n@@ -740,4 +841,11 @@\n-    _roots.apply(&_cl,\n-                 &_cld_cl,\n-                 &_thread_cl,\n-                 &_nm_cl);\n+    {\n+      ZStatTimerWorker timer(ZSubPhaseConcurrentMarkRootColoredOld);\n+      _roots_colored.apply(&_cl_colored,\n+                           &_cld_cl);\n+    }\n+\n+    {\n+      ZStatTimerWorker timer(ZSubPhaseConcurrentMarkRootUncoloredOld);\n+      _roots_uncolored.apply(&_thread_cl,\n+                             &_nm_cl);\n+    }\n@@ -749,1 +857,1 @@\n-    _mark->flush_and_free();\n+    ZHeap::heap()->mark_flush_and_free(Thread::current());\n@@ -753,1 +861,15 @@\n-class ZMarkTask : public ZTask {\n+class ZMarkYoungCLDClosure : public ClaimingCLDToOopClosure<ClassLoaderData::_claim_none> {\n+public:\n+  virtual void do_cld(ClassLoaderData* cld) {\n+    if (!cld->is_alive()) {\n+      \/\/ Skip marking through concurrently unloading CLDs\n+      return;\n+    }\n+    ClaimingCLDToOopClosure<ClassLoaderData::_claim_none>::do_cld(cld);\n+  }\n+\n+  ZMarkYoungCLDClosure(OopClosure* cl) :\n+      ClaimingCLDToOopClosure<ClassLoaderData::_claim_none>(cl) {}\n+};\n+\n+class ZMarkYoungRootsTask : public ZTask {\n@@ -755,2 +877,9 @@\n-  ZMark* const   _mark;\n-  const uint64_t _timeout_in_micros;\n+  ZMark* const               _mark;\n+  ZRootsIteratorAllColored   _roots_colored;\n+  ZRootsIteratorAllUncolored _roots_uncolored;\n+\n+  ZMarkYoungOopClosure       _cl_colored;\n+  ZMarkYoungCLDClosure       _cld_cl;\n+\n+  ZMarkThreadClosure         _thread_cl;\n+  ZMarkYoungNMethodClosure   _nm_cl;\n@@ -759,2 +888,2 @@\n-  ZMarkTask(ZMark* mark, uint64_t timeout_in_micros = 0) :\n-      ZTask(\"ZMarkTask\"),\n+  ZMarkYoungRootsTask(ZMark* mark) :\n+      ZTask(\"ZMarkYoungRootsTask\"),\n@@ -762,1 +891,36 @@\n-      _timeout_in_micros(timeout_in_micros) {\n+      _roots_colored(ZGenerationIdOptional::young),\n+      _roots_uncolored(ZGenerationIdOptional::young),\n+      _cl_colored(),\n+      _cld_cl(&_cl_colored),\n+      _thread_cl(),\n+      _nm_cl() {}\n+\n+  virtual void work() {\n+    {\n+      ZStatTimerWorker timer(ZSubPhaseConcurrentMarkRootColoredYoung);\n+      _roots_colored.apply(&_cl_colored,\n+                           &_cld_cl);\n+    }\n+\n+    {\n+      ZStatTimerWorker timer(ZSubPhaseConcurrentMarkRootUncoloredYoung);\n+      _roots_uncolored.apply(&_thread_cl,\n+                             &_nm_cl);\n+    }\n+\n+    \/\/ Flush and free worker stacks. Needed here since\n+    \/\/ the set of workers executing during root scanning\n+    \/\/ can be different from the set of workers executing\n+    \/\/ during mark.\n+    ZHeap::heap()->mark_flush_and_free(Thread::current());\n+  }\n+};\n+\n+class ZMarkTask : public ZRestartableTask {\n+private:\n+  ZMark* const _mark;\n+\n+public:\n+  ZMarkTask(ZMark* mark) :\n+      ZRestartableTask(\"ZMarkTask\"),\n+      _mark(mark) {\n@@ -771,1 +935,7 @@\n-    _mark->work(_timeout_in_micros);\n+    SuspendibleThreadSetJoiner sts_joiner;\n+    _mark->follow_work_complete();\n+    \/\/ We might have found pointers into the other generation, and then we want to\n+    \/\/ publish such marking stacks to prevent that generation from getting a mark continue.\n+    \/\/ We also flush in case of a resize where a new worker thread continues the marking\n+    \/\/ work, causing a mark continue for the collected generation.\n+    ZHeap::heap()->mark_flush_and_free(Thread::current());\n@@ -773,1 +943,0 @@\n-};\n@@ -775,4 +944,2 @@\n-void ZMark::mark(bool initial) {\n-  if (initial) {\n-    ZMarkRootsTask task(this);\n-    _workers->run(&task);\n+  virtual void resize_workers(uint nworkers) {\n+    _mark->resize_workers(nworkers);\n@@ -780,0 +947,1 @@\n+};\n@@ -781,2 +949,5 @@\n-  ZMarkTask task(this);\n-  _workers->run(&task);\n+void ZMark::resize_workers(uint nworkers) {\n+  _nworkers = nworkers;\n+  const size_t nstripes = calculate_nstripes(nworkers);\n+  _stripes.set_nstripes(nstripes);\n+  _terminate.reset(nworkers);\n@@ -785,2 +956,5 @@\n-bool ZMark::try_complete() {\n-  _ntrycomplete++;\n+void ZMark::mark_young_roots() {\n+  SuspendibleThreadSetJoiner sts_joiner;\n+  ZMarkYoungRootsTask task(this);\n+  workers()->run(&task);\n+}\n@@ -788,4 +962,5 @@\n-  \/\/ Use nconcurrent number of worker threads to maintain the\n-  \/\/ worker\/stripe distribution used during concurrent mark.\n-  ZMarkTask task(this, ZMarkCompleteTimeout);\n-  _workers->run(&task);\n+void ZMark::mark_old_roots() {\n+  SuspendibleThreadSetJoiner sts_joiner;\n+  ZMarkOldRootsTask task(this);\n+  workers()->run(&task);\n+}\n@@ -793,2 +968,8 @@\n-  \/\/ Successful if all stripes are empty\n-  return _stripes.is_empty();\n+void ZMark::mark_follow() {\n+  for (;;) {\n+    ZMarkTask task(this);\n+    workers()->run(&task);\n+    if (ZAbort::should_abort() || !try_terminate_flush()) {\n+      break;\n+    }\n+  }\n@@ -798,4 +979,12 @@\n-  \/\/ Flush all mark stacks\n-  if (!flush(true \/* at_safepoint *\/)) {\n-    \/\/ Mark completed\n-    return true;\n+  if (_terminate.resurrected()) {\n+    \/\/ An oop was resurrected after concurrent termination.\n+    return false;\n+  }\n+\n+  \/\/ Try end marking\n+  ZMarkFlushAndFreeStacksClosure cl(this);\n+  Threads::non_java_threads_do(&cl);\n+\n+  \/\/ Check if non-java threads have any pending marking\n+  if (cl.flushed() || !_stripes.is_empty()) {\n+    return false;\n@@ -804,3 +993,2 @@\n-  \/\/ Try complete marking by doing a limited\n-  \/\/ amount of mark work in this phase.\n-  return try_complete();\n+  \/\/ Mark completed\n+  return true;\n@@ -823,6 +1011,1 @@\n-  ZStatMark::set_at_mark_end(_nproactiveflush, _nterminateflush, _ntrycomplete, _ncontinue);\n-\n-  \/\/ Note that we finished a marking cycle.\n-  \/\/ Unlike other GCs, we do not arm the nmethods\n-  \/\/ when marking terminates.\n-  CodeCache::on_gc_marking_cycle_finish();\n+  _generation->stat_mark()->at_mark_end(_nproactiveflush, _nterminateflush, _ntrycomplete, _ncontinue);\n@@ -839,1 +1022,1 @@\n-  ZStatMark::set_at_mark_free(_allocator.size());\n+  _generation->stat_mark()->at_mark_free(_allocator.size());\n@@ -848,2 +1031,5 @@\n-  ZMarkThreadLocalStacks* const stacks = ZThreadLocalData::stacks(thread);\n-  const bool flushed = stacks->flush(&_allocator, &_stripes);\n+  if (thread->is_Java_thread()) {\n+    ZThreadLocalData::store_barrier_buffer(thread)->flush();\n+  }\n+  ZMarkThreadLocalStacks* const stacks = ZThreadLocalData::mark_stacks(thread, _generation->id());\n+  const bool flushed = stacks->flush(&_allocator, &_stripes, &_terminate);\n@@ -857,0 +1043,1 @@\n+  const ZGenerationId _generation_id;\n@@ -859,2 +1046,3 @@\n-  ZVerifyMarkStacksEmptyClosure(const ZMarkStripeSet* stripes) :\n-      _stripes(stripes) {}\n+  ZVerifyMarkStacksEmptyClosure(const ZMarkStripeSet* stripes, ZGenerationId id) :\n+      _stripes(stripes),\n+      _generation_id(id) {}\n@@ -863,1 +1051,1 @@\n-    ZMarkThreadLocalStacks* const stacks = ZThreadLocalData::stacks(thread);\n+    ZMarkThreadLocalStacks* const stacks = ZThreadLocalData::mark_stacks(thread, _generation_id);\n@@ -870,1 +1058,1 @@\n-  ZVerifyMarkStacksEmptyClosure cl(&_stripes);\n+  ZVerifyMarkStacksEmptyClosure cl(&_stripes, _generation->id());\n@@ -876,0 +1064,6 @@\n+\n+void ZMark::verify_worker_stacks_empty() const {\n+  \/\/ Verify thread stacks\n+  ZVerifyMarkStacksEmptyClosure cl(&_stripes, _generation->id());\n+  workers()->threads_do(&cl);\n+}\n","filename":"src\/hotspot\/share\/gc\/z\/zMark.cpp","additions":531,"deletions":337,"binary":false,"changes":868,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,0 +27,1 @@\n+#include \"gc\/z\/zAddress.hpp\"\n@@ -35,0 +36,1 @@\n+class ZGeneration;\n@@ -42,0 +44,13 @@\n+public:\n+  static const bool Resurrect     = true;\n+  static const bool DontResurrect = false;\n+\n+  static const bool GCThread      = true;\n+  static const bool AnyThread     = false;\n+\n+  static const bool Follow        = true;\n+  static const bool DontFollow    = false;\n+\n+  static const bool Strong        = false;\n+  static const bool Finalizable   = true;\n+\n@@ -43,1 +58,1 @@\n-  ZWorkers* const     _workers;\n+  ZGeneration* const  _generation;\n@@ -48,1 +63,0 @@\n-  volatile bool       _work_terminateflush;\n@@ -59,5 +73,5 @@\n-  bool is_array(uintptr_t addr) const;\n-  void push_partial_array(uintptr_t addr, size_t size, bool finalizable);\n-  void follow_small_array(uintptr_t addr, size_t size, bool finalizable);\n-  void follow_large_array(uintptr_t addr, size_t size, bool finalizable);\n-  void follow_array(uintptr_t addr, size_t size, bool finalizable);\n+  bool is_array(zaddress addr) const;\n+  void push_partial_array(zpointer* addr, size_t length, bool finalizable);\n+  void follow_array_elements_small(zpointer* addr, size_t length, bool finalizable);\n+  void follow_array_elements_large(zpointer* addr, size_t length, bool finalizable);\n+  void follow_array_elements(zpointer* addr, size_t length, bool finalizable);\n@@ -69,1 +83,2 @@\n-  template <typename T> bool drain(ZMarkContext* context, T* timeout);\n+  bool rebalance_work(ZMarkContext* context);\n+  bool drain(ZMarkContext* context);\n@@ -73,2 +88,1 @@\n-  void idle() const;\n-  bool flush(bool at_safepoint);\n+  bool flush();\n@@ -76,3 +90,2 @@\n-  bool try_flush(volatile size_t* nflush);\n-  bool try_terminate();\n-  bool try_complete();\n+  bool try_terminate(ZMarkContext* context);\n+  void leave();\n@@ -81,2 +94,1 @@\n-  void prepare_work();\n-  void finish_work();\n+  ZWorkers* workers() const;\n@@ -84,3 +96,1 @@\n-  void work_without_timeout(ZMarkContext* context);\n-  void work_with_timeout(ZMarkContext* context, uint64_t timeout_in_micros);\n-  void work(uint64_t timeout_in_micros);\n+  bool follow_work(bool partial);\n@@ -89,0 +99,1 @@\n+  void verify_worker_stacks_empty() const;\n@@ -91,1 +102,1 @@\n-  ZMark(ZWorkers* workers, ZPageTable* page_table);\n+  ZMark(ZGeneration* generation, ZPageTable* page_table);\n@@ -95,1 +106,2 @@\n-  template <bool gc_thread, bool follow, bool finalizable, bool publish> void mark_object(uintptr_t addr);\n+  template <bool resurrect, bool gc_thread, bool follow, bool finalizable>\n+  void mark_object(zaddress addr);\n@@ -98,1 +110,3 @@\n-  void mark(bool initial);\n+  void mark_young_roots();\n+  void mark_old_roots();\n+  void mark_follow();\n@@ -104,0 +118,8 @@\n+\n+  \/\/ Following work\n+  void prepare_work();\n+  void finish_work();\n+  void resize_workers(uint nworkers);\n+  void follow_work_complete();\n+  bool follow_work_partial();\n+  bool try_terminate_flush();\n","filename":"src\/hotspot\/share\/gc\/z\/zMark.hpp","additions":44,"deletions":22,"binary":false,"changes":66,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -30,0 +30,1 @@\n+#include \"gc\/z\/zGeneration.inline.hpp\"\n@@ -31,0 +32,1 @@\n+#include \"gc\/z\/zMarkTerminate.inline.hpp\"\n@@ -46,3 +48,3 @@\n-template <bool gc_thread, bool follow, bool finalizable, bool publish>\n-inline void ZMark::mark_object(uintptr_t addr) {\n-  assert(ZAddress::is_marked(addr), \"Should be marked\");\n+template <bool resurrect, bool gc_thread, bool follow, bool finalizable>\n+inline void ZMark::mark_object(zaddress addr) {\n+  assert(!ZVerifyOops || oopDesc::is_oop(to_oop(addr)), \"Should be oop\");\n@@ -67,1 +69,1 @@\n-    if (page->is_object_marked<finalizable>(addr)) {\n+    if (page->is_object_marked(addr, finalizable)) {\n@@ -73,0 +75,4 @@\n+  if (resurrect) {\n+    _terminate.set_resurrected(true);\n+  }\n+\n@@ -74,4 +80,8 @@\n-  ZMarkThreadLocalStacks* const stacks = ZThreadLocalData::stacks(Thread::current());\n-  ZMarkStripe* const stripe = _stripes.stripe_for_addr(addr);\n-  ZMarkStackEntry entry(addr, !mark_before_push, inc_live, follow, finalizable);\n-  stacks->push(&_allocator, &_stripes, stripe, entry, publish);\n+  ZMarkThreadLocalStacks* const stacks = ZThreadLocalData::mark_stacks(Thread::current(), _generation->id());\n+  ZMarkStripe* const stripe = _stripes.stripe_for_addr(untype(addr));\n+  ZMarkStackEntry entry(untype(ZAddress::offset(addr)), !mark_before_push, inc_live, follow, finalizable);\n+\n+  assert(ZHeap::heap()->is_young(addr) == _generation->is_young(), \"Phase\/object mismatch\");\n+\n+  const bool publish = !gc_thread;\n+  stacks->push(&_allocator, &_stripes, stripe, &_terminate, entry, publish);\n","filename":"src\/hotspot\/share\/gc\/z\/zMark.inline.hpp","additions":19,"deletions":9,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, 2017, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -29,0 +29,4 @@\n+static size_t shift_for_stripes(size_t nstripes) {\n+  return ZMarkStripeShift + exact_log2(nstripes);\n+}\n+\n@@ -30,1 +34,1 @@\n-    _page(NULL),\n+    _page(nullptr),\n@@ -35,1 +39,1 @@\n-    _shift(ZMarkStripeShift + exact_log2(nstripes)) {}\n+    _shift(shift_for_stripes(nstripes)) {}\n@@ -43,0 +47,4 @@\n+\n+void ZMarkCache::set_nstripes(size_t nstripes) {\n+  _shift = shift_for_stripes(nstripes);\n+}\n","filename":"src\/hotspot\/share\/gc\/z\/zMarkCache.cpp","additions":11,"deletions":3,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, 2017, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -47,1 +47,1 @@\n-  const size_t    _shift;\n+  size_t          _shift;\n@@ -54,0 +54,2 @@\n+  void set_nstripes(size_t nstripes);\n+\n","filename":"src\/hotspot\/share\/gc\/z\/zMarkCache.hpp","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, 2017, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -46,1 +46,1 @@\n-  if (_page != NULL) {\n+  if (_page != nullptr) {\n@@ -49,1 +49,1 @@\n-    _page = NULL;\n+    _page = nullptr;\n@@ -55,1 +55,1 @@\n-  const size_t index = (page->start() >> _shift) & mask;\n+  const size_t index = (untype(page->start()) >> _shift) & mask;\n","filename":"src\/hotspot\/share\/gc\/z\/zMarkCache.inline.hpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2021, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -37,1 +37,1 @@\n-  ZMarkStripe* const            _stripe;\n+  ZMarkStripe*                  _stripe;\n@@ -39,0 +39,1 @@\n+  size_t                        _nstripes;\n@@ -48,0 +49,1 @@\n+  void set_stripe(ZMarkStripe* stripe);\n@@ -50,0 +52,3 @@\n+\n+  size_t nstripes();\n+  void set_nstripes(size_t nstripes);\n","filename":"src\/hotspot\/share\/gc\/z\/zMarkContext.hpp","additions":7,"deletions":2,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2021, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -35,0 +35,1 @@\n+    _nstripes(nstripes),\n@@ -45,0 +46,4 @@\n+inline void ZMarkContext::set_stripe(ZMarkStripe* stripe) {\n+  _stripe = stripe;\n+}\n+\n@@ -53,0 +58,9 @@\n+inline size_t ZMarkContext::nstripes() {\n+  return _nstripes;\n+}\n+\n+inline void ZMarkContext::set_nstripes(size_t nstripes) {\n+  _cache.set_nstripes(nstripes);\n+  _nstripes = nstripes;\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/z\/zMarkContext.inline.hpp","additions":15,"deletions":1,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,0 +27,1 @@\n+#include \"gc\/z\/zMarkTerminate.inline.hpp\"\n@@ -28,0 +29,1 @@\n+#include \"runtime\/atomic.hpp\"\n@@ -31,3 +33,3 @@\n-ZMarkStripe::ZMarkStripe() :\n-    _published(),\n-    _overflowed() {}\n+ZMarkStripe::ZMarkStripe(uintptr_t base) :\n+    _published(base),\n+    _overflowed(base) {}\n@@ -35,2 +37,1 @@\n-ZMarkStripeSet::ZMarkStripeSet() :\n-    _nstripes(0),\n+ZMarkStripeSet::ZMarkStripeSet(uintptr_t base) :\n@@ -38,1 +39,7 @@\n-    _stripes() {}\n+    _stripes() {\n+\n+  \/\/ Re-construct array elements with the correct base\n+  for (size_t i = 0; i < ARRAY_SIZE(_stripes); i++) {\n+    _stripes[i] = ZMarkStripe(base);\n+  }\n+}\n@@ -46,2 +53,6 @@\n-  _nstripes = nstripes;\n-  _nstripes_mask = nstripes - 1;\n+  \/\/ Mutators may read these values concurrently. It doesn't matter\n+  \/\/ if they see the old or new values.\n+  Atomic::store(&_nstripes_mask, nstripes - 1);\n+\n+  log_debug(gc, marking)(\"Using \" SIZE_FORMAT \" mark stripes\", nstripes);\n+}\n@@ -49,1 +60,2 @@\n-  log_debug(gc, marking)(\"Using \" SIZE_FORMAT \" mark stripes\", _nstripes);\n+size_t ZMarkStripeSet::nstripes() const {\n+  return Atomic::load(&_nstripes_mask) + 1;\n@@ -53,1 +65,1 @@\n-  for (size_t i = 0; i < _nstripes; i++) {\n+  for (size_t i = 0; i < ZMarkStripesMax; i++) {\n@@ -63,1 +75,4 @@\n-  const size_t spillover_limit = (nworkers \/ _nstripes) * _nstripes;\n+  const size_t mask = Atomic::load(&_nstripes_mask);\n+  const size_t nstripes = mask + 1;\n+\n+  const size_t spillover_limit = (nworkers \/ nstripes) * nstripes;\n@@ -68,1 +83,1 @@\n-    index = worker_id & _nstripes_mask;\n+    index = worker_id & mask;\n@@ -73,1 +88,1 @@\n-    const double spillover_chunk = (double)_nstripes \/ (double)spillover_nworkers;\n+    const double spillover_chunk = (double)nstripes \/ (double)spillover_nworkers;\n@@ -77,1 +92,1 @@\n-  assert(index < _nstripes, \"Invalid index\");\n+  assert(index < nstripes, \"Invalid index\");\n@@ -82,1 +97,1 @@\n-    _magazine(NULL) {\n+    _magazine(nullptr) {\n@@ -84,1 +99,1 @@\n-    _stacks[i] = NULL;\n+    _stacks[i] = nullptr;\n@@ -89,1 +104,1 @@\n-  for (size_t i = 0; i < stripes->nstripes(); i++) {\n+  for (size_t i = 0; i < ZMarkStripesMax; i++) {\n@@ -91,1 +106,1 @@\n-    if (stack != NULL) {\n+    if (stack != nullptr) {\n@@ -100,1 +115,1 @@\n-  if (_magazine == NULL) {\n+  if (_magazine == nullptr) {\n@@ -103,2 +118,2 @@\n-    if (_magazine == NULL) {\n-      return NULL;\n+    if (_magazine == nullptr) {\n+      return nullptr;\n@@ -108,1 +123,1 @@\n-  ZMarkStack* stack = NULL;\n+  ZMarkStack* stack = nullptr;\n@@ -114,1 +129,1 @@\n-    _magazine = NULL;\n+    _magazine = nullptr;\n@@ -122,1 +137,1 @@\n-    if (_magazine == NULL) {\n+    if (_magazine == nullptr) {\n@@ -136,1 +151,1 @@\n-    _magazine = NULL;\n+    _magazine = nullptr;\n@@ -143,0 +158,1 @@\n+                                       ZMarkTerminate* terminate,\n@@ -148,1 +164,1 @@\n-    if (stack == NULL) {\n+    if (stack == nullptr) {\n@@ -151,1 +167,1 @@\n-      if (stack == NULL) {\n+      if (stack == nullptr) {\n@@ -163,2 +179,2 @@\n-    stripe->publish_stack(stack, publish);\n-    *stackp = stack = NULL;\n+    stripe->publish_stack(stack, terminate, publish);\n+    *stackp = stack = nullptr;\n@@ -175,1 +191,1 @@\n-    if (stack == NULL) {\n+    if (stack == nullptr) {\n@@ -178,1 +194,1 @@\n-      if (stack == NULL) {\n+      if (stack == nullptr) {\n@@ -191,1 +207,1 @@\n-    *stackp = stack = NULL;\n+    *stackp = stack = nullptr;\n@@ -195,1 +211,1 @@\n-bool ZMarkThreadLocalStacks::flush(ZMarkStackAllocator* allocator, ZMarkStripeSet* stripes) {\n+bool ZMarkThreadLocalStacks::flush(ZMarkStackAllocator* allocator, ZMarkStripeSet* stripes, ZMarkTerminate* terminate) {\n@@ -199,1 +215,1 @@\n-  for (size_t i = 0; i < stripes->nstripes(); i++) {\n+  for (size_t i = 0; i < ZMarkStripesMax; i++) {\n@@ -203,1 +219,1 @@\n-    if (stack == NULL) {\n+    if (stack == nullptr) {\n@@ -211,1 +227,1 @@\n-      stripe->publish_stack(stack);\n+      stripe->publish_stack(stack, terminate, true \/* publish *\/);\n@@ -214,1 +230,1 @@\n-    *stackp = NULL;\n+    *stackp = nullptr;\n@@ -222,1 +238,1 @@\n-  if (_magazine != NULL) {\n+  if (_magazine != nullptr) {\n@@ -224,1 +240,1 @@\n-    _magazine = NULL;\n+    _magazine = nullptr;\n","filename":"src\/hotspot\/share\/gc\/z\/zMarkStack.cpp","additions":56,"deletions":40,"binary":false,"changes":96,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -31,0 +31,2 @@\n+class ZMarkTerminate;\n+\n@@ -55,0 +57,1 @@\n+  uintptr_t   _base;\n@@ -61,1 +64,1 @@\n-  ZStackList();\n+  explicit ZStackList(uintptr_t base);\n@@ -85,1 +88,1 @@\n-  ZMarkStripe();\n+  explicit ZMarkStripe(uintptr_t base = 0);\n@@ -89,1 +92,1 @@\n-  void publish_stack(ZMarkStack* stack, bool publish = true);\n+  void publish_stack(ZMarkStack* stack, ZMarkTerminate* terminate, bool publish);\n@@ -95,1 +98,0 @@\n-  size_t      _nstripes;\n@@ -100,1 +102,1 @@\n-  ZMarkStripeSet();\n+  explicit ZMarkStripeSet(uintptr_t base);\n@@ -102,1 +104,0 @@\n-  size_t nstripes() const;\n@@ -104,0 +105,1 @@\n+  size_t nstripes() const;\n@@ -127,0 +129,1 @@\n+                 ZMarkTerminate* terminate,\n@@ -150,0 +153,1 @@\n+            ZMarkTerminate* terminate,\n@@ -159,1 +163,2 @@\n-             ZMarkStripeSet* stripes);\n+             ZMarkStripeSet* stripes,\n+             ZMarkTerminate* terminate);\n","filename":"src\/hotspot\/share\/gc\/z\/zMarkStack.hpp","additions":13,"deletions":8,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, 2017, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -29,1 +29,1 @@\n-#include \"utilities\/debug.hpp\"\n+#include \"gc\/z\/zMarkTerminate.inline.hpp\"\n@@ -31,0 +31,1 @@\n+#include \"utilities\/debug.hpp\"\n@@ -35,1 +36,1 @@\n-    _next(NULL) {}\n+    _next(nullptr) {}\n@@ -78,2 +79,3 @@\n-inline ZStackList<T>::ZStackList() :\n-    _head(encode_versioned_pointer(NULL, 0)) {}\n+inline ZStackList<T>::ZStackList(uintptr_t base) :\n+    _base(base),\n+    _head(encode_versioned_pointer(nullptr, 0)) {}\n@@ -85,1 +87,1 @@\n-  if (stack == NULL) {\n+  if (stack == nullptr) {\n@@ -88,1 +90,1 @@\n-    addr = ((uint64_t)stack - ZMarkStackSpaceStart) >> ZMarkStackSizeShift;\n+    addr = ((uint64_t)stack - _base) >> ZMarkStackSizeShift;\n@@ -99,1 +101,1 @@\n-    *stack = NULL;\n+    *stack = nullptr;\n@@ -101,1 +103,1 @@\n-    *stack = (T*)((addr << ZMarkStackSizeShift) + ZMarkStackSpaceStart);\n+    *stack = (T*)((addr << ZMarkStackSizeShift) + _base);\n@@ -110,1 +112,1 @@\n-  T* stack = NULL;\n+  T* stack = nullptr;\n@@ -114,1 +116,1 @@\n-  return stack == NULL;\n+  return stack == nullptr;\n@@ -139,1 +141,1 @@\n-  T* stack = NULL;\n+  T* stack = nullptr;\n@@ -144,2 +146,2 @@\n-    if (stack == NULL) {\n-      return NULL;\n+    if (stack == nullptr) {\n+      return nullptr;\n@@ -162,1 +164,1 @@\n-  _head = encode_versioned_pointer(NULL, 0);\n+  _head = encode_versioned_pointer(nullptr, 0);\n@@ -169,1 +171,1 @@\n-inline void ZMarkStripe::publish_stack(ZMarkStack* stack, bool publish) {\n+inline void ZMarkStripe::publish_stack(ZMarkStack* stack, ZMarkTerminate* terminate, bool publish) {\n@@ -181,0 +183,2 @@\n+\n+  terminate->wake_up();\n@@ -186,1 +190,1 @@\n-  if (stack != NULL) {\n+  if (stack != nullptr) {\n@@ -193,4 +197,0 @@\n-inline size_t ZMarkStripeSet::nstripes() const {\n-  return _nstripes;\n-}\n-\n@@ -199,1 +199,1 @@\n-  assert(index < _nstripes, \"Invalid index\");\n+  assert(index < ZMarkStripesMax, \"Invalid index\");\n@@ -204,1 +204,1 @@\n-  assert(index < _nstripes, \"Invalid index\");\n+  assert(index < ZMarkStripesMax, \"Invalid index\");\n@@ -209,2 +209,2 @@\n-  const size_t index = (stripe_id(stripe) + 1) & _nstripes_mask;\n-  assert(index < _nstripes, \"Invalid index\");\n+  const size_t index = (stripe_id(stripe) + 1) & (ZMarkStripesMax - 1);\n+  assert(index < ZMarkStripesMax, \"Invalid index\");\n@@ -215,2 +215,2 @@\n-  const size_t index = (addr >> ZMarkStripeShift) & _nstripes_mask;\n-  assert(index < _nstripes, \"Invalid index\");\n+  const size_t index = (addr >> ZMarkStripeShift) & Atomic::load(&_nstripes_mask);\n+  assert(index < ZMarkStripesMax, \"Invalid index\");\n@@ -224,1 +224,1 @@\n-  assert(*stackp == NULL, \"Should be empty\");\n+  assert(*stackp == nullptr, \"Should be empty\");\n@@ -232,2 +232,2 @@\n-  if (stack != NULL) {\n-    *stackp = NULL;\n+  if (stack != nullptr) {\n+    *stackp = nullptr;\n@@ -242,0 +242,1 @@\n+                                         ZMarkTerminate* terminate,\n@@ -246,1 +247,1 @@\n-  if (stack != NULL && stack->push(entry)) {\n+  if (stack != nullptr && stack->push(entry)) {\n@@ -250,1 +251,1 @@\n-  return push_slow(allocator, stripe, stackp, entry, publish);\n+  return push_slow(allocator, stripe, stackp, terminate, entry, publish);\n@@ -259,1 +260,1 @@\n-  if (stack != NULL && stack->pop(entry)) {\n+  if (stack != nullptr && stack->pop(entry)) {\n","filename":"src\/hotspot\/share\/gc\/z\/zMarkStack.inline.hpp","additions":34,"deletions":33,"binary":false,"changes":67,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -35,2 +35,0 @@\n-uintptr_t ZMarkStackSpaceStart;\n-\n@@ -55,3 +53,0 @@\n-  \/\/ Register mark stack space start\n-  ZMarkStackSpaceStart = _start;\n-\n@@ -66,0 +61,4 @@\n+uintptr_t ZMarkStackSpace::start() const {\n+  return _start;\n+}\n+\n@@ -173,2 +172,3 @@\n-    _freelist(),\n-    _space() {}\n+    _space(),\n+    _freelist(_space.start()),\n+    _expanded_recently(false) {}\n@@ -180,0 +180,4 @@\n+uintptr_t ZMarkStackAllocator::start() const {\n+  return _space.start();\n+}\n+\n@@ -201,1 +205,1 @@\n-  if (magazine != NULL) {\n+  if (magazine != nullptr) {\n@@ -205,0 +209,4 @@\n+  if (!Atomic::load(&_expanded_recently)) {\n+    Atomic::cmpxchg(&_expanded_recently, false, true);\n+  }\n+\n@@ -208,1 +216,1 @@\n-    return NULL;\n+    return nullptr;\n@@ -214,0 +222,8 @@\n+bool ZMarkStackAllocator::clear_and_get_expanded_recently() {\n+  if (!Atomic::load(&_expanded_recently)) {\n+    return false;\n+  }\n+\n+  return Atomic::cmpxchg(&_expanded_recently, true, false);\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/z\/zMarkStackAllocator.cpp","additions":26,"deletions":10,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -29,0 +29,1 @@\n+#include \"gc\/z\/zMarkStack.hpp\"\n@@ -37,0 +38,1 @@\n+  volatile bool      _recently_expanded;\n@@ -51,0 +53,1 @@\n+  uintptr_t start() const;\n@@ -57,1 +60,1 @@\n-class ZMarkStackAllocator {\n+class ZMarkStackAllocator : public CHeapObj<mtGC> {\n@@ -59,1 +62,0 @@\n-  ZCACHE_ALIGNED ZMarkStackMagazineList _freelist;\n@@ -61,0 +63,2 @@\n+  ZCACHE_ALIGNED ZMarkStackMagazineList _freelist;\n+  ZCACHE_ALIGNED volatile bool          _expanded_recently;\n@@ -69,0 +73,1 @@\n+  uintptr_t start() const;\n@@ -71,0 +76,2 @@\n+  bool clear_and_get_expanded_recently();\n+\n","filename":"src\/hotspot\/share\/gc\/z\/zMarkStackAllocator.hpp","additions":10,"deletions":3,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,2 +27,1 @@\n-#include \"gc\/z\/zGlobals.hpp\"\n-#include \"memory\/allocation.hpp\"\n+#include \"gc\/z\/zLock.hpp\"\n@@ -31,0 +30,2 @@\n+class ZMarkStripeSet;\n+\n@@ -33,3 +34,5 @@\n-  uint                         _nworkers;\n-  ZCACHE_ALIGNED volatile uint _nworking_stage0;\n-  volatile uint                _nworking_stage1;\n+  uint           _nworkers;\n+  volatile uint  _nworking;\n+  volatile uint  _nawakening;\n+  volatile bool  _resurrected;\n+  ZConditionLock _lock;\n@@ -37,3 +40,1 @@\n-  bool enter_stage(volatile uint* nworking_stage);\n-  void exit_stage(volatile uint* nworking_stage);\n-  bool try_exit_stage(volatile uint* nworking_stage);\n+  void maybe_reduce_stripes(ZMarkStripeSet* stripes, size_t used_nstripes);\n@@ -45,0 +46,1 @@\n+  void leave();\n@@ -46,3 +48,1 @@\n-  bool enter_stage0();\n-  void exit_stage0();\n-  bool try_exit_stage0();\n+  bool saturated() const;\n@@ -50,2 +50,4 @@\n-  bool enter_stage1();\n-  bool try_exit_stage1();\n+  void wake_up();\n+  bool try_terminate(ZMarkStripeSet* stripes, size_t used_nstripes);\n+  void set_resurrected(bool value);\n+  bool resurrected() const;\n","filename":"src\/hotspot\/share\/gc\/z\/zMarkTerminate.hpp","additions":16,"deletions":14,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -29,0 +29,3 @@\n+#include \"gc\/shared\/suspendibleThreadSet.hpp\"\n+#include \"gc\/z\/zLock.inline.hpp\"\n+#include \"logging\/log.hpp\"\n@@ -30,0 +33,2 @@\n+#include \"runtime\/osThread.hpp\"\n+#include \"runtime\/thread.inline.hpp\"\n@@ -33,2 +38,4 @@\n-    _nworking_stage0(0),\n-    _nworking_stage1(0) {}\n+    _nworking(0),\n+    _nawakening(0),\n+    _resurrected(false),\n+    _lock() {}\n@@ -36,2 +43,4 @@\n-inline bool ZMarkTerminate::enter_stage(volatile uint* nworking_stage) {\n-  return Atomic::sub(nworking_stage, 1u) == 0;\n+inline void ZMarkTerminate::reset(uint nworkers) {\n+  Atomic::store(&_nworkers, nworkers);\n+  Atomic::store(&_nworking, nworkers);\n+  _nawakening = 0;\n@@ -40,2 +49,9 @@\n-inline void ZMarkTerminate::exit_stage(volatile uint* nworking_stage) {\n-  Atomic::add(nworking_stage, 1u);\n+inline void ZMarkTerminate::leave() {\n+  SuspendibleThreadSetLeaver sts_leaver;\n+  ZLocker<ZConditionLock> locker(&_lock);\n+\n+  Atomic::store(&_nworking, _nworking - 1);\n+  if (_nworking == 0) {\n+    \/\/ Last thread leaving; notify waiters\n+    _lock.notify_all();\n+  }\n@@ -44,2 +60,7 @@\n-inline bool ZMarkTerminate::try_exit_stage(volatile uint* nworking_stage) {\n-  uint nworking = Atomic::load(nworking_stage);\n+inline void ZMarkTerminate::maybe_reduce_stripes(ZMarkStripeSet* stripes, size_t used_nstripes) {\n+  size_t nstripes = stripes->nstripes();\n+  if (used_nstripes == nstripes && nstripes > 1u) {\n+    nstripes >>= 1;\n+    stripes->set_nstripes(nstripes);\n+  }\n+}\n@@ -47,4 +68,3 @@\n-  for (;;) {\n-    if (nworking == 0) {\n-      return false;\n-    }\n+inline bool ZMarkTerminate::try_terminate(ZMarkStripeSet* stripes, size_t used_nstripes) {\n+  SuspendibleThreadSetLeaver sts_leaver;\n+  ZLocker<ZConditionLock> locker(&_lock);\n@@ -52,6 +72,11 @@\n-    const uint new_nworking = nworking + 1;\n-    const uint prev_nworking = Atomic::cmpxchg(nworking_stage, nworking, new_nworking);\n-    if (prev_nworking == nworking) {\n-      \/\/ Success\n-      return true;\n-    }\n+  Atomic::store(&_nworking, _nworking - 1);\n+  if (_nworking == 0) {\n+    \/\/ Last thread entering termination: success\n+    _lock.notify_all();\n+    return true;\n+  }\n+\n+  \/\/ If a worker runs out of work, it might be a sign that we have too many stripes\n+  \/\/ hiding work. Try to reduce the number of stripes if possible.\n+  maybe_reduce_stripes(stripes, used_nstripes);\n+  _lock.wait();\n@@ -59,2 +84,4 @@\n-    \/\/ Retry\n-    nworking = prev_nworking;\n+  \/\/ We either got notification about more work\n+  \/\/ or got a spurious wakeup; don't terminate\n+  if (_nawakening > 0) {\n+    Atomic::store(&_nawakening, _nawakening - 1);\n@@ -62,1 +89,0 @@\n-}\n@@ -64,3 +90,6 @@\n-inline void ZMarkTerminate::reset(uint nworkers) {\n-  _nworkers = _nworking_stage0 = _nworking_stage1 = nworkers;\n-}\n+  if (_nworking == 0) {\n+    \/\/ We got notified all work is done; terminate\n+    return true;\n+  }\n+\n+  Atomic::store(&_nworking, _nworking + 1);\n@@ -68,2 +97,1 @@\n-inline bool ZMarkTerminate::enter_stage0() {\n-  return enter_stage(&_nworking_stage0);\n+  return false;\n@@ -72,2 +100,19 @@\n-inline void ZMarkTerminate::exit_stage0() {\n-  exit_stage(&_nworking_stage0);\n+inline void ZMarkTerminate::wake_up() {\n+  uint nworking = Atomic::load(&_nworking);\n+  uint nawakening = Atomic::load(&_nawakening);\n+  if (nworking + nawakening == Atomic::load(&_nworkers)) {\n+    \/\/ Everyone is working or about to\n+    return;\n+  }\n+\n+  if (nworking == 0) {\n+    \/\/ Marking when marking task is not active\n+    return;\n+  }\n+\n+  ZLocker<ZConditionLock> locker(&_lock);\n+  if (_nworking + _nawakening != _nworkers) {\n+    \/\/ Everyone is not working\n+    Atomic::store(&_nawakening, _nawakening + 1);\n+    _lock.notify();\n+  }\n@@ -76,2 +121,5 @@\n-inline bool ZMarkTerminate::try_exit_stage0() {\n-  return try_exit_stage(&_nworking_stage0);\n+inline bool ZMarkTerminate::saturated() const {\n+  uint nworking = Atomic::load(&_nworking);\n+  uint nawakening = Atomic::load(&_nawakening);\n+\n+  return nworking + nawakening == Atomic::load(&_nworkers);\n@@ -80,2 +128,10 @@\n-inline bool ZMarkTerminate::enter_stage1() {\n-  return enter_stage(&_nworking_stage1);\n+inline void ZMarkTerminate::set_resurrected(bool value) {\n+  \/\/ Update resurrected if it changed\n+  if (resurrected() != value) {\n+    Atomic::store(&_resurrected, value);\n+    if (value) {\n+      log_debug(gc, marking)(\"Resurrection broke termination\");\n+    } else {\n+      log_debug(gc, marking)(\"Try terminate after resurrection\");\n+    }\n+  }\n@@ -84,2 +140,2 @@\n-inline bool ZMarkTerminate::try_exit_stage1() {\n-  return try_exit_stage(&_nworking_stage1);\n+inline bool ZMarkTerminate::resurrected() const {\n+  return Atomic::load(&_resurrected);\n","filename":"src\/hotspot\/share\/gc\/z\/zMarkTerminate.inline.hpp","additions":91,"deletions":35,"binary":false,"changes":126,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -29,1 +29,1 @@\n-ZMemory* ZMemoryManager::create(uintptr_t start, size_t size) {\n+ZMemory* ZMemoryManager::create(zoffset start, size_t size) {\n@@ -31,1 +31,1 @@\n-  if (_callbacks._create != NULL) {\n+  if (_callbacks._create != nullptr) {\n@@ -38,1 +38,1 @@\n-  if (_callbacks._destroy != NULL) {\n+  if (_callbacks._destroy != nullptr) {\n@@ -45,1 +45,1 @@\n-  if (_callbacks._shrink_from_front != NULL) {\n+  if (_callbacks._shrink_from_front != nullptr) {\n@@ -52,1 +52,1 @@\n-  if (_callbacks._shrink_from_back != NULL) {\n+  if (_callbacks._shrink_from_back != nullptr) {\n@@ -59,1 +59,1 @@\n-  if (_callbacks._grow_from_front != NULL) {\n+  if (_callbacks._grow_from_front != nullptr) {\n@@ -66,1 +66,1 @@\n-  if (_callbacks._grow_from_back != NULL) {\n+  if (_callbacks._grow_from_back != nullptr) {\n@@ -73,6 +73,6 @@\n-    _create(NULL),\n-    _destroy(NULL),\n-    _shrink_from_front(NULL),\n-    _shrink_from_back(NULL),\n-    _grow_from_front(NULL),\n-    _grow_from_back(NULL) {}\n+    _create(nullptr),\n+    _destroy(nullptr),\n+    _shrink_from_front(nullptr),\n+    _shrink_from_back(nullptr),\n+    _grow_from_front(nullptr),\n+    _grow_from_back(nullptr) {}\n@@ -88,1 +88,1 @@\n-uintptr_t ZMemoryManager::peek_low_address() const {\n+zoffset ZMemoryManager::peek_low_address() const {\n@@ -92,1 +92,1 @@\n-  if (area != NULL) {\n+  if (area != nullptr) {\n@@ -97,1 +97,1 @@\n-  return UINTPTR_MAX;\n+  return zoffset(UINTPTR_MAX);\n@@ -100,1 +100,1 @@\n-uintptr_t ZMemoryManager::alloc_low_address(size_t size) {\n+zoffset ZMemoryManager::alloc_low_address(size_t size) {\n@@ -108,1 +108,1 @@\n-        const uintptr_t start = area->start();\n+        const zoffset start = area->start();\n@@ -114,1 +114,1 @@\n-        const uintptr_t start = area->start();\n+        const zoffset start = area->start();\n@@ -122,1 +122,1 @@\n-  return UINTPTR_MAX;\n+  return zoffset(UINTPTR_MAX);\n@@ -125,1 +125,1 @@\n-uintptr_t ZMemoryManager::alloc_low_address_at_most(size_t size, size_t* allocated) {\n+zoffset ZMemoryManager::alloc_low_address_at_most(size_t size, size_t* allocated) {\n@@ -128,2 +128,2 @@\n-  ZMemory* area = _freelist.first();\n-  if (area != NULL) {\n+  ZMemory* const area = _freelist.first();\n+  if (area != nullptr) {\n@@ -132,1 +132,1 @@\n-      const uintptr_t start = area->start();\n+      const zoffset start = area->start();\n@@ -139,1 +139,1 @@\n-      const uintptr_t start = area->start();\n+      const zoffset start = area->start();\n@@ -148,1 +148,1 @@\n-  return UINTPTR_MAX;\n+  return zoffset(UINTPTR_MAX);\n@@ -151,1 +151,1 @@\n-uintptr_t ZMemoryManager::alloc_high_address(size_t size) {\n+zoffset ZMemoryManager::alloc_high_address(size_t size) {\n@@ -159,1 +159,1 @@\n-        const uintptr_t start = area->start();\n+        const zoffset start = area->start();\n@@ -166,1 +166,1 @@\n-        return area->end();\n+        return to_zoffset(area->end());\n@@ -172,1 +172,1 @@\n-  return UINTPTR_MAX;\n+  return zoffset(UINTPTR_MAX);\n@@ -175,3 +175,3 @@\n-void ZMemoryManager::free(uintptr_t start, size_t size) {\n-  assert(start != UINTPTR_MAX, \"Invalid address\");\n-  const uintptr_t end = start + size;\n+void ZMemoryManager::free(zoffset start, size_t size) {\n+  assert(start != zoffset(UINTPTR_MAX), \"Invalid address\");\n+  const zoffset_end end = to_zoffset_end(start, size);\n@@ -185,1 +185,1 @@\n-      if (prev != NULL && start == prev->end()) {\n+      if (prev != nullptr && start == prev->end()) {\n@@ -212,1 +212,1 @@\n-  if (last != NULL && start == last->end()) {\n+  if (last != nullptr && start == last->end()) {\n","filename":"src\/hotspot\/share\/gc\/z\/zMemory.cpp","additions":36,"deletions":36,"binary":false,"changes":72,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,0 +27,1 @@\n+#include \"gc\/z\/zAddress.hpp\"\n@@ -35,2 +36,2 @@\n-  uintptr_t          _start;\n-  uintptr_t          _end;\n+  zoffset            _start;\n+  zoffset_end        _end;\n@@ -40,1 +41,1 @@\n-  ZMemory(uintptr_t start, size_t size);\n+  ZMemory(zoffset start, size_t size);\n@@ -42,2 +43,2 @@\n-  uintptr_t start() const;\n-  uintptr_t end() const;\n+  zoffset start() const;\n+  zoffset_end end() const;\n@@ -73,1 +74,1 @@\n-  ZMemory* create(uintptr_t start, size_t size);\n+  ZMemory* create(zoffset start, size_t size);\n@@ -85,4 +86,4 @@\n-  uintptr_t peek_low_address() const;\n-  uintptr_t alloc_low_address(size_t size);\n-  uintptr_t alloc_low_address_at_most(size_t size, size_t* allocated);\n-  uintptr_t alloc_high_address(size_t size);\n+  zoffset peek_low_address() const;\n+  zoffset alloc_low_address(size_t size);\n+  zoffset alloc_low_address_at_most(size_t size, size_t* allocated);\n+  zoffset alloc_high_address(size_t size);\n@@ -90,1 +91,1 @@\n-  void free(uintptr_t start, size_t size);\n+  void free(zoffset start, size_t size);\n","filename":"src\/hotspot\/share\/gc\/z\/zMemory.hpp","additions":13,"deletions":12,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2017, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -29,0 +29,1 @@\n+#include \"gc\/z\/zAddress.inline.hpp\"\n@@ -32,1 +33,1 @@\n-inline ZMemory::ZMemory(uintptr_t start, size_t size) :\n+inline ZMemory::ZMemory(zoffset start, size_t size) :\n@@ -34,1 +35,1 @@\n-    _end(start + size) {}\n+    _end(to_zoffset_end(start, size)) {}\n@@ -36,1 +37,1 @@\n-inline uintptr_t ZMemory::start() const {\n+inline zoffset ZMemory::start() const {\n@@ -40,1 +41,1 @@\n-inline uintptr_t ZMemory::end() const {\n+inline zoffset_end ZMemory::end() const {\n@@ -59,1 +60,1 @@\n-  assert(start() >= size, \"Too big\");\n+  assert(size_t(start()) >= size, \"Too big\");\n","filename":"src\/hotspot\/share\/gc\/z\/zMemory.inline.hpp","additions":7,"deletions":6,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -1,66 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_Z_ZMESSAGEPORT_HPP\n-#define SHARE_GC_Z_ZMESSAGEPORT_HPP\n-\n-#include \"gc\/z\/zFuture.hpp\"\n-#include \"gc\/z\/zList.hpp\"\n-#include \"runtime\/mutex.hpp\"\n-\n-template <typename T> class ZMessageRequest;\n-\n-template <typename T>\n-class ZMessagePort {\n-private:\n-  typedef ZMessageRequest<T> Request;\n-\n-  mutable Monitor _monitor;\n-  bool            _has_message;\n-  T               _message;\n-  uint64_t        _seqnum;\n-  ZList<Request>  _queue;\n-\n-public:\n-  ZMessagePort();\n-\n-  bool is_busy() const;\n-\n-  void send_sync(const T& message);\n-  void send_async(const T& message);\n-\n-  T receive();\n-  void ack();\n-};\n-\n-class ZRendezvousPort {\n-private:\n-  ZMessagePort<bool> _port;\n-\n-public:\n-  void signal();\n-  void wait();\n-  void ack();\n-};\n-\n-#endif \/\/ SHARE_GC_Z_ZMESSAGEPORT_HPP\n","filename":"src\/hotspot\/share\/gc\/z\/zMessagePort.hpp","additions":0,"deletions":66,"binary":false,"changes":66,"status":"deleted"},{"patch":"@@ -1,181 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_Z_ZMESSAGEPORT_INLINE_HPP\n-#define SHARE_GC_Z_ZMESSAGEPORT_INLINE_HPP\n-\n-#include \"gc\/z\/zMessagePort.hpp\"\n-\n-#include \"gc\/z\/zFuture.inline.hpp\"\n-#include \"gc\/z\/zList.inline.hpp\"\n-#include \"runtime\/mutexLocker.hpp\"\n-\n-template <typename T>\n-class ZMessageRequest : public StackObj {\n-  friend class ZList<ZMessageRequest>;\n-\n-private:\n-  T                          _message;\n-  uint64_t                   _seqnum;\n-  ZFuture<T>                 _result;\n-  ZListNode<ZMessageRequest> _node;\n-\n-public:\n-  void initialize(T message, uint64_t seqnum) {\n-    _message = message;\n-    _seqnum = seqnum;\n-  }\n-\n-  T message() const {\n-    return _message;\n-  }\n-\n-  uint64_t seqnum() const {\n-    return _seqnum;\n-  }\n-\n-  void wait() {\n-    const T message = _result.get();\n-    assert(message == _message, \"Message mismatch\");\n-  }\n-\n-  void satisfy(T message) {\n-    _result.set(message);\n-  }\n-};\n-\n-template <typename T>\n-inline ZMessagePort<T>::ZMessagePort() :\n-    _monitor(Monitor::nosafepoint, \"ZMessagePort_lock\"),\n-    _has_message(false),\n-    _seqnum(0),\n-    _queue() {}\n-\n-template <typename T>\n-inline bool ZMessagePort<T>::is_busy() const {\n-  MonitorLocker ml(&_monitor, Monitor::_no_safepoint_check_flag);\n-  return _has_message;\n-}\n-\n-template <typename T>\n-inline void ZMessagePort<T>::send_sync(const T& message) {\n-  Request request;\n-\n-  {\n-    \/\/ Enqueue message\n-    MonitorLocker ml(&_monitor, Monitor::_no_safepoint_check_flag);\n-    request.initialize(message, _seqnum);\n-    _queue.insert_last(&request);\n-    ml.notify();\n-  }\n-\n-  \/\/ Wait for completion\n-  request.wait();\n-\n-  {\n-    \/\/ Guard deletion of underlying semaphore. This is a workaround for a\n-    \/\/ bug in sem_post() in glibc < 2.21, where it's not safe to destroy\n-    \/\/ the semaphore immediately after returning from sem_wait(). The\n-    \/\/ reason is that sem_post() can touch the semaphore after a waiting\n-    \/\/ thread have returned from sem_wait(). To avoid this race we are\n-    \/\/ forcing the waiting thread to acquire\/release the lock held by the\n-    \/\/ posting thread. https:\/\/sourceware.org\/bugzilla\/show_bug.cgi?id=12674\n-    MonitorLocker ml(&_monitor, Monitor::_no_safepoint_check_flag);\n-  }\n-}\n-\n-template <typename T>\n-inline void ZMessagePort<T>::send_async(const T& message) {\n-  MonitorLocker ml(&_monitor, Monitor::_no_safepoint_check_flag);\n-  if (!_has_message) {\n-    \/\/ Post message\n-    _message = message;\n-    _has_message = true;\n-    ml.notify();\n-  }\n-}\n-\n-template <typename T>\n-inline T ZMessagePort<T>::receive() {\n-  MonitorLocker ml(&_monitor, Monitor::_no_safepoint_check_flag);\n-\n-  \/\/ Wait for message\n-  while (!_has_message && _queue.is_empty()) {\n-    ml.wait();\n-  }\n-\n-  \/\/ Increment request sequence number\n-  _seqnum++;\n-\n-  if (!_has_message) {\n-    \/\/ Message available in the queue\n-    _message = _queue.first()->message();\n-    _has_message = true;\n-  }\n-\n-  return _message;\n-}\n-\n-template <typename T>\n-inline void ZMessagePort<T>::ack() {\n-  MonitorLocker ml(&_monitor, Monitor::_no_safepoint_check_flag);\n-\n-  if (!_has_message) {\n-    \/\/ Nothing to ack\n-    return;\n-  }\n-\n-  \/\/ Satisfy requests (and duplicates) in queue\n-  ZListIterator<Request> iter(&_queue);\n-  for (Request* request; iter.next(&request);) {\n-    if (request->message() == _message && request->seqnum() < _seqnum) {\n-      \/\/ Dequeue and satisfy request. Note that the dequeue operation must\n-      \/\/ happen first, since the request will immediately be deallocated\n-      \/\/ once it has been satisfied.\n-      _queue.remove(request);\n-      request->satisfy(_message);\n-    }\n-  }\n-\n-  if (_queue.is_empty()) {\n-    \/\/ Queue is empty\n-    _has_message = false;\n-  } else {\n-    \/\/ Post first message in queue\n-    _message = _queue.first()->message();\n-  }\n-}\n-\n-inline void ZRendezvousPort::signal() {\n-  _port.send_sync(true \/* ignored *\/);\n-}\n-\n-inline void ZRendezvousPort::wait() {\n-  _port.receive();\n-}\n-\n-inline void ZRendezvousPort::ack() {\n-  _port.ack();\n-}\n-\n-#endif \/\/ SHARE_GC_Z_ZMESSAGEPORT_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/z\/zMessagePort.inline.hpp","additions":0,"deletions":181,"binary":false,"changes":181,"status":"deleted"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -25,0 +25,1 @@\n+#include \"code\/codeCache.hpp\"\n@@ -31,0 +32,2 @@\n+#include \"gc\/z\/zAddress.hpp\"\n+#include \"gc\/z\/zArray.inline.hpp\"\n@@ -32,1 +35,3 @@\n-#include \"gc\/z\/zGlobals.hpp\"\n+#include \"gc\/z\/zBarrierSet.hpp\"\n+#include \"gc\/z\/zBarrierSetAssembler.hpp\"\n+#include \"gc\/z\/zBarrierSetNMethod.hpp\"\n@@ -38,0 +43,1 @@\n+#include \"gc\/z\/zUncoloredRoot.inline.hpp\"\n@@ -44,0 +50,1 @@\n+#include \"oops\/klass.inline.hpp\"\n@@ -58,2 +65,3 @@\n-  GrowableArray<oop*> immediate_oops;\n-  bool non_immediate_oops = false;\n+  ZArray<ZNMethodDataBarrier> barriers;\n+  ZArray<oop*> immediate_oops;\n+  bool has_non_immediate_oops = false;\n@@ -61,1 +69,1 @@\n-  \/\/ Find all oop relocations\n+  \/\/ Find all barrier and oop relocations\n@@ -64,18 +72,17 @@\n-    if (iter.type() != relocInfo::oop_type) {\n-      \/\/ Not an oop\n-      continue;\n-    }\n-\n-    oop_Relocation* r = iter.oop_reloc();\n-\n-    if (!r->oop_is_immediate()) {\n-      \/\/ Non-immediate oop found\n-      non_immediate_oops = true;\n-      continue;\n-    }\n-\n-    if (r->oop_value() != NULL) {\n-      \/\/ Non-NULL immediate oop found. NULL oops can safely be\n-      \/\/ ignored since the method will be re-registered if they\n-      \/\/ are later patched to be non-NULL.\n-      immediate_oops.push(r->oop_addr());\n+    if (iter.type() == relocInfo::barrier_type) {\n+      \/\/ Barrier relocation\n+      barrier_Relocation* const reloc = iter.barrier_reloc();\n+      barriers.push({ reloc->addr(), reloc->format() });\n+    } else if (iter.type() == relocInfo::oop_type) {\n+      \/\/ Oop relocation\n+      oop_Relocation* const reloc = iter.oop_reloc();\n+\n+      if (!reloc->oop_is_immediate()) {\n+        \/\/ Non-immediate oop found\n+        has_non_immediate_oops = true;\n+      } else if (reloc->oop_value() != nullptr) {\n+        \/\/ Non-null immediate oop found. null oops can safely be\n+        \/\/ ignored since the method will be re-registered if they\n+        \/\/ are later patched to be non-null.\n+        immediate_oops.push(reloc->oop_addr());\n+      }\n@@ -87,1 +94,1 @@\n-  if (data == NULL) {\n+  if (data == nullptr) {\n@@ -92,4 +99,2 @@\n-  \/\/ Attach oops in GC data\n-  ZNMethodDataOops* const new_oops = ZNMethodDataOops::create(immediate_oops, non_immediate_oops);\n-  ZNMethodDataOops* const old_oops = data->swap_oops(new_oops);\n-  ZNMethodDataOops::destroy(old_oops);\n+  \/\/ Attach barriers and oops to GC data\n+  data->swap(&barriers, &immediate_oops, has_non_immediate_oops);\n@@ -103,1 +108,1 @@\n-  LogTarget(Trace, gc, nmethod) log;\n+  LogTarget(Debug, gc, nmethod) log;\n@@ -108,1 +113,3 @@\n-  const ZNMethodDataOops* const oops = gc_data(nm)->oops();\n+  ResourceMark rm;\n+\n+  const ZNMethodData* const data = gc_data(nm);\n@@ -110,2 +117,2 @@\n-  log.print(\"Register NMethod: %s.%s (\" PTR_FORMAT \"), \"\n-            \"Compiler: %s, Oops: %d, ImmediateOops: \" SIZE_FORMAT \", NonImmediateOops: %s\",\n+  log.print(\"Register NMethod: %s.%s (\" PTR_FORMAT \") [\" PTR_FORMAT \", \" PTR_FORMAT \"] \"\n+            \"Compiler: %s, Barriers: %d, Oops: %d, ImmediateOops: %d, NonImmediateOops: %s\",\n@@ -115,0 +122,2 @@\n+            p2i(nm->code_begin()),\n+            p2i(nm->code_end()),\n@@ -116,0 +125,1 @@\n+            data->barriers()->length(),\n@@ -117,6 +127,11 @@\n-            oops->immediates_count(),\n-            oops->has_non_immediates() ? \"Yes\" : \"No\");\n-\n-  LogTarget(Trace, gc, nmethod, oops) log_oops;\n-  if (!log_oops.is_enabled()) {\n-    return;\n+            data->immediate_oops()->length(),\n+            data->has_non_immediate_oops() ? \"Yes\" : \"No\");\n+\n+  LogTarget(Trace, gc, nmethod, barrier) log_barriers;\n+  if (log_barriers.is_enabled()) {\n+    \/\/ Print nmethod barriers\n+    ZArrayIterator<ZNMethodDataBarrier> iter(data->barriers());\n+    for (ZNMethodDataBarrier b; iter.next(&b);) {\n+      log_barriers.print(\"       Barrier: %d @ \" PTR_FORMAT,\n+                         b._reloc_format, p2i(b._reloc_addr));\n+    }\n@@ -125,2 +140,3 @@\n-  \/\/ Print nmethod oops table\n-  {\n+  LogTarget(Trace, gc, nmethod, oops) log_oops;\n+  if (log_oops.is_enabled()) {\n+    \/\/ Print nmethod oops table\n@@ -131,3 +147,3 @@\n-      const char* external_name = (o == nullptr) ? \"N\/A\" : o->klass()->external_name();\n-      log_oops.print(\"           Oop[\" SIZE_FORMAT \"] \" PTR_FORMAT \" (%s)\",\n-                     (p - begin), p2i(o), external_name);\n+      const char* const external_name = (o == nullptr) ? \"N\/A\" : o->klass()->external_name();\n+      log_oops.print(\"           Oop: \" PTR_FORMAT \" (%s)\",\n+                     p2i(o), external_name);\n@@ -135,1 +151,0 @@\n-  }\n@@ -137,7 +152,5 @@\n-  \/\/ Print nmethod immediate oops\n-  {\n-    oop** const begin = oops->immediates_begin();\n-    oop** const end = oops->immediates_end();\n-    for (oop** p = begin; p < end; p++) {\n-      log_oops.print(\"  ImmediateOop[\" SIZE_FORMAT \"] \" PTR_FORMAT \" @ \" PTR_FORMAT \" (%s)\",\n-                     (p - begin), p2i(**p), p2i(*p), (**p)->klass()->external_name());\n+    \/\/ Print nmethod immediate oops\n+    ZArrayIterator<oop*> iter(data->immediate_oops());\n+    for (oop* p; iter.next(&p);) {\n+      log_oops.print(\"  ImmediateOop: \" PTR_FORMAT \" @ \" PTR_FORMAT \" (%s)\",\n+                     p2i(*p), p2i(p), (*p)->klass()->external_name());\n@@ -154,1 +167,3 @@\n-  log.print(\"Unregister NMethod: %s.%s (\" PTR_FORMAT \")\",\n+  ResourceMark rm;\n+\n+  log.print(\"Unregister NMethod: %s.%s (\" PTR_FORMAT \") [\" PTR_FORMAT \", \" PTR_FORMAT \"] \",\n@@ -157,1 +172,3 @@\n-            p2i(nm));\n+            p2i(nm),\n+            p2i(nm->code_begin()),\n+            p2i(nm->code_end()));\n@@ -160,1 +177,6 @@\n-void ZNMethod::register_nmethod(nmethod* nm) {\n+void ZNMethod::log_purge(const nmethod* nm) {\n+  LogTarget(Debug, gc, nmethod) log;\n+  if (!log.is_enabled()) {\n+    return;\n+  }\n+\n@@ -163,0 +185,9 @@\n+  log.print(\"Purge NMethod: %s.%s (\" PTR_FORMAT \") [\" PTR_FORMAT \", \" PTR_FORMAT \"] \",\n+            nm->method()->method_holder()->external_name(),\n+            nm->method()->name()->as_C_string(),\n+            p2i(nm),\n+            p2i(nm->code_begin()),\n+            p2i(nm->code_end()));\n+}\n+\n+void ZNMethod::register_nmethod(nmethod* nm) {\n@@ -166,0 +197,2 @@\n+  ZLocker<ZReentrantLock> locker(lock_for_nmethod(nm));\n+\n@@ -168,0 +201,4 @@\n+  \/\/ Patch nmethod barriers\n+  nmethod_patch_barriers(nm);\n+\n+  \/\/ Register nmethod\n@@ -175,2 +212,0 @@\n-  ResourceMark rm;\n-\n@@ -180,0 +215,4 @@\n+}\n+\n+void ZNMethod::purge_nmethod(nmethod* nm) {\n+  log_purge(nm);\n@@ -205,0 +244,8 @@\n+void ZNMethod::nmethod_patch_barriers(nmethod* nm) {\n+  ZBarrierSetAssembler* const bs_asm = ZBarrierSet::assembler();\n+  ZArrayIterator<ZNMethodDataBarrier> iter(gc_data(nm)->barriers());\n+  for (ZNMethodDataBarrier barrier; iter.next(&barrier);) {\n+    bs_asm->patch_barrier_relocation(barrier._reloc_addr, barrier._reloc_format);\n+  }\n+}\n+\n@@ -206,1 +253,1 @@\n-  ZLocker<ZReentrantLock> locker(ZNMethod::lock_for_nmethod(nm));\n+  ZLocker<ZReentrantLock> locker(lock_for_nmethod(nm));\n@@ -222,1 +269,1 @@\n-  ZNMethodDataOops* const oops = gc_data(nm)->oops();\n+  ZNMethodData* const data = gc_data(nm);\n@@ -226,5 +273,4 @@\n-    oop** const begin = oops->immediates_begin();\n-    oop** const end = oops->immediates_end();\n-    for (oop** p = begin; p < end; p++) {\n-      if (*p != Universe::non_oop_word()) {\n-        cl->do_oop(*p);\n+    ZArrayIterator<oop*> iter(data->immediate_oops());\n+    for (oop* p; iter.next(&p);) {\n+      if (!Universe::contains_non_oop_word(p)) {\n+        cl->do_oop(p);\n@@ -236,1 +282,1 @@\n-  if (oops->has_non_immediates()) {\n+  if (data->has_non_immediate_oops()) {\n@@ -241,14 +287,3 @@\n-class ZNMethodOopClosure : public OopClosure {\n-public:\n-  virtual void do_oop(oop* p) {\n-    if (ZResurrection::is_blocked()) {\n-      ZBarrier::keep_alive_barrier_on_phantom_root_oop_field(p);\n-    } else {\n-      ZBarrier::load_barrier_on_root_oop_field(p);\n-    }\n-  }\n-\n-  virtual void do_oop(narrowOop* p) {\n-    ShouldNotReachHere();\n-  }\n-};\n+void ZNMethod::nmethods_do_begin(bool secondary) {\n+  ZNMethodTable::nmethods_do_begin(secondary);\n+}\n@@ -256,3 +291,2 @@\n-void ZNMethod::nmethod_oops_barrier(nmethod* nm) {\n-  ZNMethodOopClosure cl;\n-  nmethod_oops_do_inner(nm, &cl);\n+void ZNMethod::nmethods_do_end(bool secondary) {\n+  ZNMethodTable::nmethods_do_end(secondary);\n@@ -261,2 +295,2 @@\n-void ZNMethod::nmethods_do_begin() {\n-  ZNMethodTable::nmethods_do_begin();\n+void ZNMethod::nmethods_do(bool secondary, NMethodClosure* cl) {\n+  ZNMethodTable::nmethods_do(secondary, cl);\n@@ -265,2 +299,4 @@\n-void ZNMethod::nmethods_do_end() {\n-  ZNMethodTable::nmethods_do_end();\n+uintptr_t ZNMethod::color(nmethod* nm) {\n+  BarrierSetNMethod* bs_nm = BarrierSet::barrier_set()->barrier_set_nmethod();\n+  \/\/ color is stored at low order bits of int; implicit conversion to uintptr_t is fine\n+  return bs_nm->guard_value(nm);\n@@ -269,2 +305,27 @@\n-void ZNMethod::nmethods_do(NMethodClosure* cl) {\n-  ZNMethodTable::nmethods_do(cl);\n+oop ZNMethod::load_oop(oop* p, DecoratorSet decorators) {\n+  assert((decorators & ON_WEAK_OOP_REF) == 0,\n+         \"nmethod oops have phantom strength, not weak\");\n+  nmethod* const nm = CodeCache::find_nmethod((void*)p);\n+  if (!is_armed(nm)) {\n+    \/\/ If the nmethod entry barrier isn't armed, then it has been applied\n+    \/\/ already. The implication is that the contents of the memory location\n+    \/\/ is already a valid oop, and the barrier would have kept it alive if\n+    \/\/ necessary. Therefore, no action is required, and we are allowed to\n+    \/\/ simply read the oop.\n+    return *p;\n+  }\n+\n+  const bool keep_alive = (decorators & ON_PHANTOM_OOP_REF) != 0 &&\n+                          (decorators & AS_NO_KEEPALIVE) == 0;\n+  ZLocker<ZReentrantLock> locker(ZNMethod::lock_for_nmethod(nm));\n+\n+  \/\/ Make a local root\n+  zaddress_unsafe obj = *ZUncoloredRoot::cast(p);\n+\n+  if (keep_alive) {\n+    ZUncoloredRoot::process(&obj, ZNMethod::color(nm));\n+  } else {\n+    ZUncoloredRoot::process_no_keepalive(&obj, ZNMethod::color(nm));\n+  }\n+\n+  return to_oop(safe(obj));\n@@ -293,0 +354,4 @@\n+      \/\/ Unlink from the ZNMethodTable\n+      ZNMethod::unregister_nmethod(nm);\n+\n+      \/\/ Shared unlink\n@@ -301,3 +366,19 @@\n-      \/\/ Heal oops and arm phase invariantly\n-      ZNMethod::nmethod_oops_barrier(nm);\n-      ZNMethod::set_guard_value(nm, 0);\n+      const uintptr_t prev_color = ZNMethod::color(nm);\n+      assert(prev_color != ZPointerStoreGoodMask, \"Potentially non-monotonic transition\");\n+\n+      \/\/ Heal oops and potentially mark young objects if there is a concurrent young collection.\n+      ZUncoloredRootProcessOopClosure cl(prev_color);\n+      ZNMethod::nmethod_oops_do_inner(nm, &cl);\n+\n+      \/\/ Disarm for marking and relocation, but leave the remset bits so this isn't store good.\n+      \/\/ This makes sure the mutator still takes a slow path to fill in the nmethod epoch for\n+      \/\/ the sweeper, to track continuations, if they exist in the system.\n+      const zpointer new_disarm_value_ptr = ZAddress::color(zaddress::null, ZPointerMarkGoodMask | ZPointerRememberedMask);\n+\n+      \/\/ The new disarm value is mark good, and hence never store good. Therefore, this operation\n+      \/\/ never completely disarms the nmethod. Therefore, we don't need to patch barriers yet\n+      \/\/ via ZNMethod::nmethod_patch_barriers.\n+      ZNMethod::set_guard_value(nm, (int)untype(new_disarm_value_ptr));\n+\n+      log_trace(gc, nmethod)(\"nmethod: \" PTR_FORMAT \" visited by unlinking [\" PTR_FORMAT \" -> \" PTR_FORMAT \"]\", p2i(nm), prev_color, untype(new_disarm_value_ptr));\n+      assert(ZNMethod::is_armed(nm), \"Must be considered armed\");\n@@ -327,1 +408,1 @@\n-    ZNMethodTable::nmethods_do_begin();\n+    ZNMethodTable::nmethods_do_begin(false \/* secondary *\/);\n@@ -331,1 +412,1 @@\n-    ZNMethodTable::nmethods_do_end();\n+    ZNMethodTable::nmethods_do_end(false \/* secondary *\/);\n@@ -336,1 +417,1 @@\n-    ZNMethodTable::nmethods_do(&_cl);\n+    ZNMethodTable::nmethods_do(false \/* secondary *\/, &_cl);\n@@ -359,1 +440,1 @@\n-    SuspendibleThreadSetLeaver sts;\n+    SuspendibleThreadSetLeaver sts_leaver;\n","filename":"src\/hotspot\/share\/gc\/z\/zNMethod.cpp","additions":175,"deletions":94,"binary":false,"changes":269,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,1 +27,4 @@\n-#include \"memory\/allStatic.hpp\"\n+#include \"memory\/allocation.hpp\"\n+#include \"memory\/iterator.hpp\"\n+#include \"oops\/accessDecorators.hpp\"\n+#include \"oops\/oopsHierarchy.hpp\"\n@@ -40,0 +43,1 @@\n+  static void log_purge(const nmethod* nm);\n@@ -44,0 +48,1 @@\n+  static void purge_nmethod(nmethod* nm);\n@@ -51,0 +56,2 @@\n+  static void nmethod_patch_barriers(nmethod* nm);\n+\n@@ -54,5 +61,3 @@\n-  static void nmethod_oops_barrier(nmethod* nm);\n-\n-  static void nmethods_do_begin();\n-  static void nmethods_do_end();\n-  static void nmethods_do(NMethodClosure* cl);\n+  static void nmethods_do_begin(bool secondary);\n+  static void nmethods_do_end(bool secondary);\n+  static void nmethods_do(bool secondary, NMethodClosure* cl);\n@@ -64,0 +69,3 @@\n+\n+  static uintptr_t color(nmethod* nm);\n+  static oop load_oop(oop* p, DecoratorSet decorators);\n","filename":"src\/hotspot\/share\/gc\/z\/zNMethod.hpp","additions":15,"deletions":7,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -25,1 +25,0 @@\n-#include \"gc\/z\/zAttachedArray.inline.hpp\"\n@@ -28,3 +27,0 @@\n-#include \"memory\/allocation.hpp\"\n-#include \"runtime\/atomic.hpp\"\n-#include \"utilities\/align.hpp\"\n@@ -32,34 +28,0 @@\n-#include \"utilities\/growableArray.hpp\"\n-\n-ZNMethodDataOops* ZNMethodDataOops::create(const GrowableArray<oop*>& immediates, bool has_non_immediates) {\n-  return ::new (AttachedArray::alloc(immediates.length())) ZNMethodDataOops(immediates, has_non_immediates);\n-}\n-\n-void ZNMethodDataOops::destroy(ZNMethodDataOops* oops) {\n-  AttachedArray::free(oops);\n-}\n-\n-ZNMethodDataOops::ZNMethodDataOops(const GrowableArray<oop*>& immediates, bool has_non_immediates) :\n-    _immediates(immediates.length()),\n-    _has_non_immediates(has_non_immediates) {\n-  \/\/ Save all immediate oops\n-  for (size_t i = 0; i < immediates_count(); i++) {\n-    immediates_begin()[i] = immediates.at(int(i));\n-  }\n-}\n-\n-size_t ZNMethodDataOops::immediates_count() const {\n-  return _immediates.length();\n-}\n-\n-oop** ZNMethodDataOops::immediates_begin() const {\n-  return _immediates(this);\n-}\n-\n-oop** ZNMethodDataOops::immediates_end() const {\n-  return immediates_begin() + immediates_count();\n-}\n-\n-bool ZNMethodDataOops::has_non_immediates() const {\n-  return _has_non_immediates;\n-}\n@@ -69,5 +31,3 @@\n-    _oops(NULL) {}\n-\n-ZNMethodData::~ZNMethodData() {\n-  ZNMethodDataOops::destroy(_oops);\n-}\n+    _barriers(),\n+    _immediate_oops(),\n+    _has_non_immediate_oops(false) {}\n@@ -79,2 +39,13 @@\n-ZNMethodDataOops* ZNMethodData::oops() const {\n-  return Atomic::load_acquire(&_oops);\n+const ZArray<ZNMethodDataBarrier>* ZNMethodData::barriers() const {\n+  assert(_lock.is_owned(), \"Should be owned\");\n+  return &_barriers;\n+}\n+\n+const ZArray<oop*>* ZNMethodData::immediate_oops() const {\n+  assert(_lock.is_owned(), \"Should be owned\");\n+  return &_immediate_oops;\n+}\n+\n+bool ZNMethodData::has_non_immediate_oops() const {\n+  assert(_lock.is_owned(), \"Should be owned\");\n+  return _has_non_immediate_oops;\n@@ -83,1 +54,3 @@\n-ZNMethodDataOops* ZNMethodData::swap_oops(ZNMethodDataOops* new_oops) {\n+void ZNMethodData::swap(ZArray<ZNMethodDataBarrier>* barriers,\n+                        ZArray<oop*>* immediate_oops,\n+                        bool has_non_immediate_oops) {\n@@ -85,3 +58,3 @@\n-  ZNMethodDataOops* const old_oops = _oops;\n-  _oops = new_oops;\n-  return old_oops;\n+  _barriers.swap(barriers);\n+  _immediate_oops.swap(immediate_oops);\n+  _has_non_immediate_oops = has_non_immediate_oops;\n","filename":"src\/hotspot\/share\/gc\/z\/zNMethodData.cpp","additions":23,"deletions":50,"binary":false,"changes":73,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,1 +27,1 @@\n-#include \"gc\/z\/zAttachedArray.hpp\"\n+#include \"gc\/z\/zArray.hpp\"\n@@ -33,21 +33,3 @@\n-class nmethod;\n-template <typename T> class GrowableArray;\n-\n-class ZNMethodDataOops {\n-private:\n-  typedef ZAttachedArray<ZNMethodDataOops, oop*> AttachedArray;\n-\n-  const AttachedArray _immediates;\n-  const bool          _has_non_immediates;\n-\n-  ZNMethodDataOops(const GrowableArray<oop*>& immediates, bool has_non_immediates);\n-\n-public:\n-  static ZNMethodDataOops* create(const GrowableArray<oop*>& immediates, bool has_non_immediates);\n-  static void destroy(ZNMethodDataOops* oops);\n-\n-  size_t immediates_count() const;\n-  oop** immediates_begin() const;\n-  oop** immediates_end() const;\n-\n-  bool has_non_immediates() const;\n+struct ZNMethodDataBarrier {\n+  address _reloc_addr;\n+  int     _reloc_format;\n@@ -58,2 +40,4 @@\n-  ZReentrantLock             _lock;\n-  ZNMethodDataOops* volatile _oops;\n+  ZReentrantLock              _lock;\n+  ZArray<ZNMethodDataBarrier> _barriers;\n+  ZArray<oop*>                _immediate_oops;\n+  bool                        _has_non_immediate_oops;\n@@ -63,1 +47,0 @@\n-  ~ZNMethodData();\n@@ -67,2 +50,7 @@\n-  ZNMethodDataOops* oops() const;\n-  ZNMethodDataOops* swap_oops(ZNMethodDataOops* oops);\n+  const ZArray<ZNMethodDataBarrier>* barriers() const;\n+  const ZArray<oop*>* immediate_oops() const;\n+  bool has_non_immediate_oops() const;\n+\n+  void swap(ZArray<ZNMethodDataBarrier>* barriers,\n+            ZArray<oop*>* immediate_oops,\n+            bool has_non_immediate_oops);\n","filename":"src\/hotspot\/share\/gc\/z\/zNMethodData.hpp","additions":16,"deletions":28,"binary":false,"changes":44,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -30,1 +30,0 @@\n-#include \"gc\/z\/zGlobals.hpp\"\n@@ -48,1 +47,1 @@\n-ZNMethodTableEntry* ZNMethodTable::_table = NULL;\n+ZNMethodTableEntry* ZNMethodTable::_table = nullptr;\n@@ -53,1 +52,2 @@\n-ZSafeDeleteNoLock<ZNMethodTableEntry[]> ZNMethodTable::_safe_delete;\n+ZNMethodTableIteration ZNMethodTable::_iteration_secondary;\n+ZSafeDelete<ZNMethodTableEntry[]> ZNMethodTable::_safe_delete(false \/* locked *\/);\n@@ -133,1 +133,1 @@\n-  _safe_delete(_table);\n+  _safe_delete.schedule_delete(_table);\n@@ -170,0 +170,6 @@\n+ZNMethodTableIteration* ZNMethodTable::iteration(bool secondary) {\n+  return secondary\n+      ? &_iteration_secondary\n+      : &_iteration;\n+}\n+\n@@ -196,1 +202,1 @@\n-  while (_iteration.in_progress()) {\n+  while (_iteration.in_progress() || _iteration_secondary.in_progress()) {\n@@ -202,1 +208,1 @@\n-  assert(CodeCache_lock->owned_by_self(), \"Lock must be held\");\n+  MutexLocker mu(CodeCache_lock, Mutex::_no_safepoint_check_flag);\n@@ -210,1 +216,1 @@\n-void ZNMethodTable::nmethods_do_begin() {\n+void ZNMethodTable::nmethods_do_begin(bool secondary) {\n@@ -217,1 +223,1 @@\n-  _iteration.nmethods_do_begin(_table, _size);\n+  iteration(secondary)->nmethods_do_begin(_table, _size);\n@@ -220,1 +226,1 @@\n-void ZNMethodTable::nmethods_do_end() {\n+void ZNMethodTable::nmethods_do_end(bool secondary) {\n@@ -224,1 +230,1 @@\n-  _iteration.nmethods_do_end();\n+  iteration(secondary)->nmethods_do_end();\n@@ -233,2 +239,2 @@\n-void ZNMethodTable::nmethods_do(NMethodClosure* cl) {\n-  _iteration.nmethods_do(cl);\n+void ZNMethodTable::nmethods_do(bool secondary, NMethodClosure* cl) {\n+  iteration(secondary)->nmethods_do(cl);\n","filename":"src\/hotspot\/share\/gc\/z\/zNMethodTable.cpp","additions":19,"deletions":13,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -38,6 +38,7 @@\n-  static ZNMethodTableEntry*                     _table;\n-  static size_t                                  _size;\n-  static size_t                                  _nregistered;\n-  static size_t                                  _nunregistered;\n-  static ZNMethodTableIteration                  _iteration;\n-  static ZSafeDeleteNoLock<ZNMethodTableEntry[]> _safe_delete;\n+  static ZNMethodTableEntry*               _table;\n+  static size_t                            _size;\n+  static size_t                            _nregistered;\n+  static size_t                            _nunregistered;\n+  static ZNMethodTableIteration            _iteration;\n+  static ZNMethodTableIteration            _iteration_secondary;\n+  static ZSafeDelete<ZNMethodTableEntry[]> _safe_delete;\n@@ -57,0 +58,2 @@\n+  static ZNMethodTableIteration* iteration(bool secondary);\n+\n@@ -66,3 +69,3 @@\n-  static void nmethods_do_begin();\n-  static void nmethods_do_end();\n-  static void nmethods_do(NMethodClosure* cl);\n+  static void nmethods_do_begin(bool secondary);\n+  static void nmethods_do_end(bool secondary);\n+  static void nmethods_do(bool secondary, NMethodClosure* cl);\n","filename":"src\/hotspot\/share\/gc\/z\/zNMethodTable.hpp","additions":13,"deletions":10,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -61,1 +61,1 @@\n-             field_method::encode(NULL)) {}\n+             field_method::encode(nullptr)) {}\n","filename":"src\/hotspot\/share\/gc\/z\/zNMethodTableEntry.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -33,1 +33,1 @@\n-    _table(NULL),\n+    _table(nullptr),\n@@ -38,1 +38,1 @@\n-  return _table != NULL;\n+  return _table != nullptr;\n@@ -53,1 +53,1 @@\n-  _table = NULL;\n+  _table = nullptr;\n","filename":"src\/hotspot\/share\/gc\/z\/zNMethodTableIteration.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2019, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2019, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -66,1 +66,4 @@\n-  arrayOopDesc::set_mark(mem, markWord::prototype());\n+\n+  \/\/ Signal to the ZIterator that this is an invisible root, by setting\n+  \/\/ the mark word to \"marked\". Reset to prototype() after the clearing.\n+  arrayOopDesc::set_mark(mem, markWord::prototype().set_marked());\n@@ -72,1 +75,1 @@\n-  \/\/ root. Invisible roots are not visited by the heap itarator\n+  \/\/ root. Invisible roots are not visited by the heap iterator\n@@ -74,2 +77,14 @@\n-  \/\/ Relocation knows how to dodge iterating over such objects.\n-  ZThreadLocalData::set_invisible_root(_thread, (oop*)&mem);\n+  \/\/ Relocation and remembered set code know how to dodge iterating\n+  \/\/ over such objects.\n+  ZThreadLocalData::set_invisible_root(_thread, (zaddress_unsafe*)&mem);\n+\n+  uint32_t old_seqnum_before = ZGeneration::old()->seqnum();\n+  uint32_t young_seqnum_before = ZGeneration::young()->seqnum();\n+  uintptr_t color_before = ZPointerStoreGoodMask;\n+  auto gc_safepoint_happened = [&]() {\n+    return old_seqnum_before != ZGeneration::old()->seqnum() ||\n+           young_seqnum_before != ZGeneration::young()->seqnum() ||\n+           color_before != ZPointerStoreGoodMask;\n+  };\n+\n+  bool seen_gc_safepoint = false;\n@@ -77,5 +92,22 @@\n-  for (size_t processed = 0; processed < payload_size; processed += segment_max) {\n-    \/\/ Calculate segment\n-    HeapWord* const start = (HeapWord*)(mem + header + processed);\n-    const size_t remaining = payload_size - processed;\n-    const size_t segment_size = MIN2(remaining, segment_max);\n+  auto initialize_memory = [&]() {\n+    for (size_t processed = 0; processed < payload_size; processed += segment_max) {\n+      \/\/ Clear segment\n+      uintptr_t* const start = (uintptr_t*)(mem + header + processed);\n+      const size_t remaining = payload_size - processed;\n+      const size_t segment = MIN2(remaining, segment_max);\n+      \/\/ Usually, the young marking code has the responsibility to color\n+      \/\/ raw nulls, before they end up in the old generation. However, the\n+      \/\/ invisible roots are hidden from the marking code, and therefore\n+      \/\/ we must color the nulls already here in the initialization. The\n+      \/\/ color we choose must be store bad for any subsequent stores, regardless\n+      \/\/ of how many GC flips later it will arrive. That's why we OR in 11\n+      \/\/ (ZPointerRememberedMask) in the remembered bits, similar to how\n+      \/\/ forgotten old oops also have 11, for the very same reason.\n+      \/\/ However, we opportunistically try to color without the 11 remembered\n+      \/\/ bits, hoping to not get interrupted in the middle of a GC safepoint.\n+      \/\/ Most of the time, we manage to do that, and can the avoid having GC\n+      \/\/ barriers trigger slow paths for this.\n+      const uintptr_t colored_null = seen_gc_safepoint ? (ZPointerStoreGoodMask | ZPointerRememberedMask)\n+                                                       : ZPointerStoreGoodMask;\n+      const uintptr_t fill_value = is_reference_type(element_type) ? colored_null : 0;\n+      ZUtils::fill(start, segment, fill_value);\n@@ -83,2 +115,2 @@\n-    \/\/ Clear segment\n-    Copy::zero_to_words(start, segment_size);\n+      \/\/ Safepoint\n+      yield_for_safepoint();\n@@ -86,2 +118,15 @@\n-    \/\/ Safepoint\n-    yield_for_safepoint();\n+      \/\/ Deal with safepoints\n+      if (!seen_gc_safepoint && gc_safepoint_happened()) {\n+        \/\/ The first time we observe a GC safepoint in the yield point,\n+        \/\/ we have to restart processing with 11 remembered bits.\n+        seen_gc_safepoint = true;\n+        return false;\n+      }\n+    }\n+    return true;\n+  };\n+\n+  if (!initialize_memory()) {\n+    \/\/ Re-color with 11 remset bits if we got intercepted by a GC safepoint\n+    const bool result = initialize_memory();\n+    assert(result, \"Array initialization should always succeed the second time\");\n@@ -92,0 +137,3 @@\n+  \/\/ Signal to the ZIterator that this is no longer an invisible root\n+  oopDesc::release_set_mark(mem, markWord::prototype());\n+\n","filename":"src\/hotspot\/share\/gc\/z\/zObjArrayAllocator.cpp","additions":62,"deletions":14,"binary":false,"changes":76,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -32,1 +32,0 @@\n-#include \"gc\/z\/zThread.inline.hpp\"\n@@ -37,0 +36,1 @@\n+#include \"runtime\/thread.hpp\"\n@@ -43,1 +43,2 @@\n-ZObjectAllocator::ZObjectAllocator() :\n+ZObjectAllocator::ZObjectAllocator(ZPageAge age) :\n+    _age(age),\n@@ -47,4 +48,2 @@\n-    _alloc_for_relocation(0),\n-    _undo_alloc_for_relocation(0),\n-    _shared_medium_page(NULL),\n-    _shared_small_page(NULL) {}\n+    _shared_medium_page(nullptr),\n+    _shared_small_page(nullptr) {}\n@@ -60,14 +59,3 @@\n-void ZObjectAllocator::register_alloc_for_relocation(const ZPageTable* page_table, uintptr_t addr, size_t size) {\n-  const ZPage* const page = page_table->get(addr);\n-  const size_t aligned_size = align_up(size, page->object_alignment());\n-  Atomic::add(_alloc_for_relocation.addr(), aligned_size);\n-}\n-\n-void ZObjectAllocator::register_undo_alloc_for_relocation(const ZPage* page, size_t size) {\n-  const size_t aligned_size = align_up(size, page->object_alignment());\n-  Atomic::add(_undo_alloc_for_relocation.addr(), aligned_size);\n-}\n-\n-ZPage* ZObjectAllocator::alloc_page(uint8_t type, size_t size, ZAllocationFlags flags) {\n-  ZPage* const page = ZHeap::heap()->alloc_page(type, size, flags);\n-  if (page != NULL) {\n+ZPage* ZObjectAllocator::alloc_page(ZPageType type, size_t size, ZAllocationFlags flags) {\n+  ZPage* const page = ZHeap::heap()->alloc_page(type, size, flags, _age);\n+  if (page != nullptr) {\n@@ -81,0 +69,4 @@\n+ZPage* ZObjectAllocator::alloc_page_for_relocation(ZPageType type, size_t size, ZAllocationFlags flags) {\n+  return ZHeap::heap()->alloc_page(type, size, flags, _age);\n+}\n+\n@@ -88,6 +80,6 @@\n-uintptr_t ZObjectAllocator::alloc_object_in_shared_page(ZPage** shared_page,\n-                                                        uint8_t page_type,\n-                                                        size_t page_size,\n-                                                        size_t size,\n-                                                        ZAllocationFlags flags) {\n-  uintptr_t addr = 0;\n+zaddress ZObjectAllocator::alloc_object_in_shared_page(ZPage** shared_page,\n+                                                       ZPageType page_type,\n+                                                       size_t page_size,\n+                                                       size_t size,\n+                                                       ZAllocationFlags flags) {\n+  zaddress addr = zaddress::null;\n@@ -96,1 +88,1 @@\n-  if (page != NULL) {\n+  if (page != nullptr) {\n@@ -100,1 +92,1 @@\n-  if (addr == 0) {\n+  if (is_null(addr)) {\n@@ -103,1 +95,1 @@\n-    if (new_page != NULL) {\n+    if (new_page != nullptr) {\n@@ -111,1 +103,1 @@\n-        if (prev_page == NULL) {\n+        if (prev_page == nullptr) {\n@@ -118,2 +110,2 @@\n-        const uintptr_t prev_addr = prev_page->alloc_object_atomic(size);\n-        if (prev_addr == 0) {\n+        const zaddress prev_addr = prev_page->alloc_object_atomic(size);\n+        if (is_null(prev_addr)) {\n@@ -137,2 +129,2 @@\n-uintptr_t ZObjectAllocator::alloc_large_object(size_t size, ZAllocationFlags flags) {\n-  uintptr_t addr = 0;\n+zaddress ZObjectAllocator::alloc_large_object(size_t size, ZAllocationFlags flags) {\n+  zaddress addr = zaddress::null;\n@@ -142,2 +134,2 @@\n-  ZPage* const page = alloc_page(ZPageTypeLarge, page_size, flags);\n-  if (page != NULL) {\n+  ZPage* const page = alloc_page(ZPageType::large, page_size, flags);\n+  if (page != nullptr) {\n@@ -151,2 +143,2 @@\n-uintptr_t ZObjectAllocator::alloc_medium_object(size_t size, ZAllocationFlags flags) {\n-  return alloc_object_in_shared_page(_shared_medium_page.addr(), ZPageTypeMedium, ZPageSizeMedium, size, flags);\n+zaddress ZObjectAllocator::alloc_medium_object(size_t size, ZAllocationFlags flags) {\n+  return alloc_object_in_shared_page(_shared_medium_page.addr(), ZPageType::medium, ZPageSizeMedium, size, flags);\n@@ -155,2 +147,2 @@\n-uintptr_t ZObjectAllocator::alloc_small_object(size_t size, ZAllocationFlags flags) {\n-  return alloc_object_in_shared_page(shared_small_page_addr(), ZPageTypeSmall, ZPageSizeSmall, size, flags);\n+zaddress ZObjectAllocator::alloc_small_object(size_t size, ZAllocationFlags flags) {\n+  return alloc_object_in_shared_page(shared_small_page_addr(), ZPageType::small, ZPageSizeSmall, size, flags);\n@@ -159,1 +151,1 @@\n-uintptr_t ZObjectAllocator::alloc_object(size_t size, ZAllocationFlags flags) {\n+zaddress ZObjectAllocator::alloc_object(size_t size, ZAllocationFlags flags) {\n@@ -172,2 +164,2 @@\n-uintptr_t ZObjectAllocator::alloc_object(size_t size) {\n-  ZAllocationFlags flags;\n+zaddress ZObjectAllocator::alloc_object(size_t size) {\n+  const ZAllocationFlags flags;\n@@ -177,1 +169,1 @@\n-uintptr_t ZObjectAllocator::alloc_object_for_relocation(const ZPageTable* page_table, size_t size) {\n+zaddress ZObjectAllocator::alloc_object_for_relocation(size_t size) {\n@@ -181,6 +173,1 @@\n-  const uintptr_t addr = alloc_object(size, flags);\n-  if (addr != 0) {\n-    register_alloc_for_relocation(page_table, addr, size);\n-  }\n-\n-  return addr;\n+  return alloc_object(size, flags);\n@@ -189,2 +176,2 @@\n-void ZObjectAllocator::undo_alloc_object_for_relocation(ZPage* page, uintptr_t addr, size_t size) {\n-  const uint8_t type = page->type();\n+void ZObjectAllocator::undo_alloc_object_for_relocation(zaddress addr, size_t size) {\n+  ZPage* const page = ZHeap::heap()->page(addr);\n@@ -192,2 +179,1 @@\n-  if (type == ZPageTypeLarge) {\n-    register_undo_alloc_for_relocation(page, size);\n+  if (page->is_large()) {\n@@ -198,1 +184,0 @@\n-      register_undo_alloc_for_relocation(page, size);\n@@ -206,0 +191,4 @@\n+ZPageAge ZObjectAllocator::age() const {\n+  return _age;\n+}\n+\n@@ -224,1 +213,1 @@\n-  assert(ZThread::is_java(), \"Should be a Java thread\");\n+  assert(Thread::current()->is_Java_thread(), \"Should be a Java thread\");\n@@ -227,1 +216,1 @@\n-  if (page != NULL) {\n+  if (page != nullptr) {\n@@ -234,19 +223,0 @@\n-size_t ZObjectAllocator::relocated() const {\n-  size_t total_alloc = 0;\n-  size_t total_undo_alloc = 0;\n-\n-  ZPerCPUConstIterator<size_t> iter_alloc(&_alloc_for_relocation);\n-  for (const size_t* alloc; iter_alloc.next(&alloc);) {\n-    total_alloc += Atomic::load(alloc);\n-  }\n-\n-  ZPerCPUConstIterator<size_t> iter_undo_alloc(&_undo_alloc_for_relocation);\n-  for (const size_t* undo_alloc; iter_undo_alloc.next(&undo_alloc);) {\n-    total_undo_alloc += Atomic::load(undo_alloc);\n-  }\n-\n-  assert(total_alloc >= total_undo_alloc, \"Mismatch\");\n-\n-  return total_alloc - total_undo_alloc;\n-}\n-\n@@ -260,4 +230,0 @@\n-  \/\/ Reset relocated bytes\n-  _alloc_for_relocation.set_all(0);\n-  _undo_alloc_for_relocation.set_all(0);\n-\n@@ -265,2 +231,2 @@\n-  _shared_medium_page.set(NULL);\n-  _shared_small_page.set_all(NULL);\n+  _shared_medium_page.set(nullptr);\n+  _shared_small_page.set_all(nullptr);\n","filename":"src\/hotspot\/share\/gc\/z\/zObjectAllocator.cpp","additions":49,"deletions":83,"binary":false,"changes":132,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,0 +27,1 @@\n+#include \"gc\/z\/zAddress.hpp\"\n@@ -28,0 +29,2 @@\n+#include \"gc\/z\/zPageAge.hpp\"\n+#include \"gc\/z\/zPageType.hpp\"\n@@ -35,0 +38,1 @@\n+  ZPageAge           _age;\n@@ -38,2 +42,0 @@\n-  ZPerCPU<size_t>    _alloc_for_relocation;\n-  ZPerCPU<size_t>    _undo_alloc_for_relocation;\n@@ -46,4 +48,1 @@\n-  void register_alloc_for_relocation(const ZPageTable* page_table, uintptr_t addr, size_t size);\n-  void register_undo_alloc_for_relocation(const ZPage* page, size_t size);\n-\n-  ZPage* alloc_page(uint8_t type, size_t size, ZAllocationFlags flags);\n+  ZPage* alloc_page(ZPageType type, size_t size, ZAllocationFlags flags);\n@@ -54,5 +53,5 @@\n-  uintptr_t alloc_object_in_shared_page(ZPage** shared_page,\n-                                        uint8_t page_type,\n-                                        size_t page_size,\n-                                        size_t size,\n-                                        ZAllocationFlags flags);\n+  zaddress alloc_object_in_shared_page(ZPage** shared_page,\n+                                       ZPageType page_type,\n+                                       size_t page_size,\n+                                       size_t size,\n+                                       ZAllocationFlags flags);\n@@ -60,4 +59,4 @@\n-  uintptr_t alloc_large_object(size_t size, ZAllocationFlags flags);\n-  uintptr_t alloc_medium_object(size_t size, ZAllocationFlags flags);\n-  uintptr_t alloc_small_object(size_t size, ZAllocationFlags flags);\n-  uintptr_t alloc_object(size_t size, ZAllocationFlags flags);\n+  zaddress alloc_large_object(size_t size, ZAllocationFlags flags);\n+  zaddress alloc_medium_object(size_t size, ZAllocationFlags flags);\n+  zaddress alloc_small_object(size_t size, ZAllocationFlags flags);\n+  zaddress alloc_object(size_t size, ZAllocationFlags flags);\n@@ -66,1 +65,10 @@\n-  ZObjectAllocator();\n+  ZObjectAllocator(ZPageAge age);\n+\n+  \/\/ Mutator allocation\n+  zaddress alloc_object(size_t size);\n+\n+  \/\/ Relocation\n+  zaddress alloc_object_for_relocation(size_t size);\n+  void undo_alloc_object_for_relocation(zaddress addr, size_t size);\n+\n+  ZPage* alloc_page_for_relocation(ZPageType type, size_t size, ZAllocationFlags flags);\n@@ -68,3 +76,1 @@\n-  uintptr_t alloc_object(size_t size);\n-  uintptr_t alloc_object_for_relocation(const ZPageTable* page_table, size_t size);\n-  void undo_alloc_object_for_relocation(ZPage* page, uintptr_t addr, size_t size);\n+  ZPageAge age() const;\n@@ -74,1 +80,0 @@\n-  size_t relocated() const;\n","filename":"src\/hotspot\/share\/gc\/z\/zObjectAllocator.hpp","additions":26,"deletions":21,"binary":false,"changes":47,"status":"modified"},{"patch":"@@ -1,37 +0,0 @@\n-\/*\n- * Copyright (c) 2016, 2019, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_Z_ZOOP_INLINE_HPP\n-#define SHARE_GC_Z_ZOOP_INLINE_HPP\n-\n-#include \"gc\/z\/zOop.hpp\"\n-\n-inline oop ZOop::from_address(uintptr_t addr) {\n-  return cast_to_oop(addr);\n-}\n-\n-inline uintptr_t ZOop::to_address(oop o) {\n-  return cast_from_oop<uintptr_t>(o);\n-}\n-\n-#endif \/\/ SHARE_GC_Z_ZOOP_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/z\/zOop.inline.hpp","additions":0,"deletions":37,"binary":false,"changes":37,"status":"deleted"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -25,0 +25,2 @@\n+#include \"gc\/shared\/gc_globals.hpp\"\n+#include \"gc\/z\/zGeneration.inline.hpp\"\n@@ -28,0 +30,1 @@\n+#include \"gc\/z\/zRememberedSet.inline.hpp\"\n@@ -31,0 +34,1 @@\n+#include \"utilities\/growableArray.hpp\"\n@@ -32,4 +36,1 @@\n-ZPage::ZPage(const ZVirtualMemory& vmem, const ZPhysicalMemory& pmem) :\n-    ZPage(type_from_size(vmem.size()), vmem, pmem) {}\n-\n-ZPage::ZPage(uint8_t type, const ZVirtualMemory& vmem, const ZPhysicalMemory& pmem) :\n+ZPage::ZPage(ZPageType type, const ZVirtualMemory& vmem, const ZPhysicalMemory& pmem) :\n@@ -37,0 +38,2 @@\n+    _generation_id(ZGenerationId::young),\n+    _age(ZPageAge::eden),\n@@ -39,0 +42,1 @@\n+    _seqnum_other(0),\n@@ -40,1 +44,1 @@\n-    _top(start()),\n+    _top(to_zoffset_end(start())),\n@@ -42,0 +46,1 @@\n+    _remembered_set(),\n@@ -45,6 +50,0 @@\n-  assert_initialized();\n-}\n-\n-ZPage::~ZPage() {}\n-\n-void ZPage::assert_initialized() const {\n@@ -54,3 +53,3 @@\n-  assert((_type == ZPageTypeSmall && size() == ZPageSizeSmall) ||\n-         (_type == ZPageTypeMedium && size() == ZPageSizeMedium) ||\n-         (_type == ZPageTypeLarge && is_aligned(size(), ZGranuleSize)),\n+  assert((_type == ZPageType::small && size() == ZPageSizeSmall) ||\n+         (_type == ZPageType::medium && size() == ZPageSizeMedium) ||\n+         (_type == ZPageType::large && is_aligned(size(), ZGranuleSize)),\n@@ -60,4 +59,85 @@\n-void ZPage::reset() {\n-  _seqnum = ZGlobalSeqNum;\n-  _top = start();\n-  _livemap.reset();\n+ZPage* ZPage::clone_limited() const {\n+  \/\/ Only copy type and memory layouts. Let the rest be lazily reconstructed when needed.\n+  return new ZPage(_type, _virtual, _physical);\n+}\n+\n+ZPage* ZPage::clone_limited_promote_flipped() const {\n+  ZPage* const page = new ZPage(_type, _virtual, _physical);\n+\n+  \/\/ The page is still filled with the same objects, need to retain the top pointer.\n+  page->_top = _top;\n+\n+  return page;\n+}\n+\n+ZGeneration* ZPage::generation() {\n+  return ZGeneration::generation(_generation_id);\n+}\n+\n+const ZGeneration* ZPage::generation() const {\n+  return ZGeneration::generation(_generation_id);\n+}\n+\n+void ZPage::reset_seqnum() {\n+  Atomic::store(&_seqnum, generation()->seqnum());\n+  Atomic::store(&_seqnum_other, ZGeneration::generation(_generation_id == ZGenerationId::young ? ZGenerationId::old : ZGenerationId::young)->seqnum());\n+}\n+\n+void ZPage::remset_clear() {\n+  _remembered_set.clear_all();\n+}\n+\n+void ZPage::verify_remset_after_reset(ZPageAge prev_age, ZPageResetType type) {\n+  \/\/ Young-to-old reset\n+  if (prev_age != ZPageAge::old) {\n+    verify_remset_cleared_previous();\n+    verify_remset_cleared_current();\n+    return;\n+  }\n+\n+  \/\/ Old-to-old reset\n+  switch (type) {\n+  case ZPageResetType::Splitting:\n+    \/\/ Page is on the way to be destroyed or reused, delay\n+    \/\/ clearing until the page is reset for Allocation.\n+    break;\n+\n+  case ZPageResetType::InPlaceRelocation:\n+    \/\/ Relocation failed and page is being compacted in-place.\n+    \/\/ The remset bits are flipped each young mark start, so\n+    \/\/ the verification code below needs to use the right remset.\n+    if (ZGeneration::old()->active_remset_is_current()) {\n+      verify_remset_cleared_previous();\n+    } else {\n+      verify_remset_cleared_current();\n+    }\n+    break;\n+\n+  case ZPageResetType::FlipAging:\n+    fatal(\"Should not have called this for old-to-old flipping\");\n+    break;\n+\n+  case ZPageResetType::Allocation:\n+    verify_remset_cleared_previous();\n+    verify_remset_cleared_current();\n+    break;\n+  };\n+}\n+\n+void ZPage::reset_remembered_set() {\n+  if (is_young()) {\n+    \/\/ Remset not needed\n+    return;\n+  }\n+\n+  \/\/ Clearing of remsets is done when freeing a page, so this code only\n+  \/\/ needs to ensure the remset is initialized the first time a page\n+  \/\/ becomes old.\n+  if (!_remembered_set.is_initialized()) {\n+    _remembered_set.initialize(size());\n+  }\n+}\n+\n+void ZPage::reset(ZPageAge age, ZPageResetType type) {\n+  const ZPageAge prev_age = _age;\n+  _age = age;\n@@ -65,0 +145,20 @@\n+\n+  _generation_id = age == ZPageAge::old\n+      ? ZGenerationId::old\n+      : ZGenerationId::young;\n+\n+  reset_seqnum();\n+\n+  \/\/ Flip aged pages are still filled with the same objects, need to retain the top pointer.\n+  if (type != ZPageResetType::FlipAging) {\n+    _top = to_zoffset_end(start());\n+  }\n+\n+  reset_remembered_set();\n+  verify_remset_after_reset(prev_age, type);\n+\n+  if (type != ZPageResetType::InPlaceRelocation || (prev_age != ZPageAge::old && age == ZPageAge::old)) {\n+    \/\/ Promoted in-place relocations reset the live map,\n+    \/\/ because they clone the page.\n+    _livemap.reset();\n+  }\n@@ -67,3 +167,3 @@\n-void ZPage::reset_for_in_place_relocation() {\n-  _seqnum = ZGlobalSeqNum;\n-  _top = start();\n+void ZPage::finalize_reset_for_in_place_relocation() {\n+  \/\/ Now we're done iterating over the livemaps\n+  _livemap.reset();\n@@ -72,2 +172,1 @@\n-ZPage* ZPage::retype(uint8_t type) {\n-  assert(_type != type, \"Invalid retype\");\n+void ZPage::reset_type_and_size(ZPageType type) {\n@@ -76,0 +175,6 @@\n+  _remembered_set.resize(size());\n+}\n+\n+ZPage* ZPage::retype(ZPageType type) {\n+  assert(_type != type, \"Invalid retype\");\n+  reset_type_and_size(type);\n@@ -79,2 +184,2 @@\n-ZPage* ZPage::split(size_t size) {\n-  return split(type_from_size(size), size);\n+ZPage* ZPage::split(size_t split_of_size) {\n+  return split(type_from_size(split_of_size), split_of_size);\n@@ -83,2 +188,6 @@\n-ZPage* ZPage::split(uint8_t type, size_t size) {\n-  assert(_virtual.size() > size, \"Invalid split\");\n+ZPage* ZPage::split_with_pmem(ZPageType type, const ZPhysicalMemory& pmem) {\n+  \/\/ Resize this page\n+  const ZVirtualMemory vmem = _virtual.split(pmem.size());\n+\n+  reset_type_and_size(type_from_size(_virtual.size()));\n+  reset(_age, ZPageResetType::Splitting);\n@@ -86,6 +195,1 @@\n-  \/\/ Resize this page, keep _numa_id, _seqnum, and _last_used\n-  const ZVirtualMemory vmem = _virtual.split(size);\n-  const ZPhysicalMemory pmem = _physical.split(size);\n-  _type = type_from_size(_virtual.size());\n-  _top = start();\n-  _livemap.resize(object_max_count());\n+  assert(vmem.end() == _virtual.start(), \"Should be consecutive\");\n@@ -93,5 +197,15 @@\n-  \/\/ Create new page, inherit _seqnum and _last_used\n-  ZPage* const page = new ZPage(type, vmem, pmem);\n-  page->_seqnum = _seqnum;\n-  page->_last_used = _last_used;\n-  return page;\n+  log_trace(gc, page)(\"Split page [\" PTR_FORMAT \", \" PTR_FORMAT \", \" PTR_FORMAT \"]\",\n+      untype(vmem.start()),\n+      untype(vmem.end()),\n+      untype(_virtual.end()));\n+\n+  \/\/ Create new page\n+  return new ZPage(type, vmem, pmem);\n+}\n+\n+ZPage* ZPage::split(ZPageType type, size_t split_of_size) {\n+  assert(_virtual.size() > split_of_size, \"Invalid split\");\n+\n+  const ZPhysicalMemory pmem = _physical.split(split_of_size);\n+\n+  return split_with_pmem(type, pmem);\n@@ -106,1 +220,1 @@\n-    return NULL;\n+    return nullptr;\n@@ -111,5 +225,2 @@\n-  \/\/ Resize this page\n-  const ZVirtualMemory vmem = _virtual.split(pmem.size());\n-  _type = type_from_size(_virtual.size());\n-  _top = start();\n-  _livemap.resize(object_max_count());\n+  return split_with_pmem(type_from_size(pmem.size()), pmem);\n+}\n@@ -117,2 +228,66 @@\n-  \/\/ Create new page\n-  return new ZPage(vmem, pmem);\n+class ZFindBaseOopClosure : public ObjectClosure {\n+private:\n+  volatile zpointer* _p;\n+  oop _result;\n+\n+public:\n+  ZFindBaseOopClosure(volatile zpointer* p) :\n+      _p(p),\n+      _result(nullptr) {}\n+\n+  virtual void do_object(oop obj) {\n+    const uintptr_t p_int = reinterpret_cast<uintptr_t>(_p);\n+    const uintptr_t base_int = cast_from_oop<uintptr_t>(obj);\n+    const uintptr_t end_int = base_int + wordSize * obj->size();\n+    if (p_int >= base_int && p_int < end_int) {\n+      _result = obj;\n+    }\n+  }\n+\n+  oop result() const { return _result; }\n+};\n+\n+bool ZPage::is_remset_cleared_current() const {\n+  return _remembered_set.is_cleared_current();\n+}\n+\n+bool ZPage::is_remset_cleared_previous() const {\n+  return _remembered_set.is_cleared_previous();\n+}\n+\n+void ZPage::verify_remset_cleared_current() const {\n+  if (ZVerifyRemembered && !is_remset_cleared_current()) {\n+    fatal_msg(\" current remset bits should be cleared\");\n+  }\n+}\n+\n+void ZPage::verify_remset_cleared_previous() const {\n+  if (ZVerifyRemembered && !is_remset_cleared_previous()) {\n+    fatal_msg(\" previous remset bits should be cleared\");\n+  }\n+}\n+\n+void ZPage::clear_remset_current() {\n+ _remembered_set.clear_current();\n+}\n+\n+void ZPage::clear_remset_previous() {\n+ _remembered_set.clear_previous();\n+}\n+\n+void ZPage::swap_remset_bitmaps() {\n+  _remembered_set.swap_remset_bitmaps();\n+}\n+\n+void* ZPage::remset_current() {\n+  return _remembered_set.current();\n+}\n+\n+void ZPage::print_on_msg(outputStream* out, const char* msg) const {\n+  out->print_cr(\" %-6s  \" PTR_FORMAT \" \" PTR_FORMAT \" \" PTR_FORMAT \" %s\/%-4u %s%s%s\",\n+                type_to_string(), untype(start()), untype(top()), untype(end()),\n+                is_young() ? \"Y\" : \"O\",\n+                seqnum(),\n+                is_allocating()  ? \" Allocating \" : \"\",\n+                is_relocatable() ? \" Relocatable\" : \"\",\n+                msg == nullptr ? \"\" : msg);\n@@ -122,4 +297,1 @@\n-  out->print_cr(\" %-6s  \" PTR_FORMAT \" \" PTR_FORMAT \" \" PTR_FORMAT \" %s%s\",\n-                type_to_string(), start(), top(), end(),\n-                is_allocating()  ? \" Allocating\"  : \"\",\n-                is_relocatable() ? \" Relocatable\" : \"\");\n+  print_on_msg(out, nullptr);\n@@ -132,1 +304,5 @@\n-void ZPage::verify_live(uint32_t live_objects, size_t live_bytes) const {\n+void ZPage::verify_live(uint32_t live_objects, size_t live_bytes, bool in_place) const {\n+  if (!in_place) {\n+    \/\/ In-place relocation has changed the page to allocating\n+    assert_zpage_mark_state();\n+  }\n@@ -136,0 +312,6 @@\n+\n+void ZPage::fatal_msg(const char* msg) const {\n+  stringStream ss;\n+  print_on_msg(&ss, msg);\n+  fatal(\"%s\", ss.base());\n+}\n","filename":"src\/hotspot\/share\/gc\/z\/zPage.cpp","additions":234,"deletions":52,"binary":false,"changes":286,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,0 +27,1 @@\n+#include \"gc\/z\/zGenerationId.hpp\"\n@@ -29,0 +30,2 @@\n+#include \"gc\/z\/zPageAge.hpp\"\n+#include \"gc\/z\/zPageType.hpp\"\n@@ -30,0 +33,1 @@\n+#include \"gc\/z\/zRememberedSet.hpp\"\n@@ -33,0 +37,14 @@\n+class ZGeneration;\n+\n+enum class ZPageResetType {\n+  \/\/ Normal allocation path\n+  Allocation,\n+  \/\/ Relocation failed and started to relocate in-place\n+  InPlaceRelocation,\n+  \/\/ Page was not selected for relocation, all objects\n+  \/\/ stayed, but the page aged.\n+  FlipAging,\n+  \/\/ The page was split and needs to be reset\n+  Splitting,\n+};\n+\n@@ -36,0 +54,1 @@\n+  friend class ZForwardingTest;\n@@ -38,13 +57,15 @@\n-  uint8_t            _type;\n-  uint8_t            _numa_id;\n-  uint32_t           _seqnum;\n-  ZVirtualMemory     _virtual;\n-  volatile uintptr_t _top;\n-  ZLiveMap           _livemap;\n-  uint64_t           _last_used;\n-  ZPhysicalMemory    _physical;\n-  ZListNode<ZPage>   _node;\n-\n-  void assert_initialized() const;\n-\n-  uint8_t type_from_size(size_t size) const;\n+  ZPageType            _type;\n+  ZGenerationId        _generation_id;\n+  ZPageAge             _age;\n+  uint8_t              _numa_id;\n+  uint32_t             _seqnum;\n+  uint32_t             _seqnum_other;\n+  ZVirtualMemory       _virtual;\n+  volatile zoffset_end _top;\n+  ZLiveMap             _livemap;\n+  ZRememberedSet       _remembered_set;\n+  uint64_t             _last_used;\n+  ZPhysicalMemory      _physical;\n+  ZListNode<ZPage>     _node;\n+\n+  ZPageType type_from_size(size_t size) const;\n@@ -53,2 +74,16 @@\n-  bool is_object_marked(uintptr_t addr) const;\n-  bool is_object_strongly_marked(uintptr_t addr) const;\n+  BitMap::idx_t bit_index(zaddress addr) const;\n+  zoffset offset_from_bit_index(BitMap::idx_t index) const;\n+  oop object_from_bit_index(BitMap::idx_t index) const;\n+\n+  bool is_live_bit_set(zaddress addr) const;\n+  bool is_strong_bit_set(zaddress addr) const;\n+\n+  ZGeneration* generation();\n+  const ZGeneration* generation() const;\n+\n+  void reset_seqnum();\n+  void reset_remembered_set();\n+\n+  ZPage* split_with_pmem(ZPageType type, const ZPhysicalMemory& pmem);\n+\n+  void verify_remset_after_reset(ZPageAge prev_age, ZPageResetType type);\n@@ -57,3 +92,4 @@\n-  ZPage(const ZVirtualMemory& vmem, const ZPhysicalMemory& pmem);\n-  ZPage(uint8_t type, const ZVirtualMemory& vmem, const ZPhysicalMemory& pmem);\n-  ~ZPage();\n+  ZPage(ZPageType type, const ZVirtualMemory& vmem, const ZPhysicalMemory& pmem);\n+\n+  ZPage* clone_limited() const;\n+  ZPage* clone_limited_promote_flipped() const;\n@@ -65,3 +101,11 @@\n-  uint8_t type() const;\n-  uintptr_t start() const;\n-  uintptr_t end() const;\n+  ZPageType type() const;\n+\n+  bool is_small() const;\n+  bool is_medium() const;\n+  bool is_large() const;\n+\n+  ZGenerationId generation_id() const;\n+  bool is_young() const;\n+  bool is_old() const;\n+  zoffset start() const;\n+  zoffset_end end() const;\n@@ -69,1 +113,1 @@\n-  uintptr_t top() const;\n+  zoffset_end top() const;\n@@ -71,0 +115,1 @@\n+  size_t used() const;\n@@ -77,0 +122,1 @@\n+  ZPageAge age() const;\n@@ -78,0 +124,1 @@\n+  uint32_t seqnum() const;\n@@ -84,2 +131,5 @@\n-  void reset();\n-  void reset_for_in_place_relocation();\n+  void reset(ZPageAge age, ZPageResetType type);\n+\n+  void finalize_reset_for_in_place_relocation();\n+\n+  void reset_type_and_size(ZPageType type);\n@@ -87,3 +137,3 @@\n-  ZPage* retype(uint8_t type);\n-  ZPage* split(size_t size);\n-  ZPage* split(uint8_t type, size_t size);\n+  ZPage* retype(ZPageType type);\n+  ZPage* split(size_t split_of_size);\n+  ZPage* split(ZPageType type, size_t split_of_size);\n@@ -92,1 +142,12 @@\n-  bool is_in(uintptr_t addr) const;\n+  bool is_in(zoffset offset) const;\n+  bool is_in(zaddress addr) const;\n+\n+  uintptr_t local_offset(zoffset offset) const;\n+  uintptr_t local_offset(zoffset_end offset) const;\n+  uintptr_t local_offset(zaddress addr) const;\n+  uintptr_t local_offset(zaddress_unsafe addr) const;\n+\n+  zoffset global_offset(uintptr_t local_offset) const;\n+\n+  bool is_object_live(zaddress addr) const;\n+  bool is_object_strongly_live(zaddress addr) const;\n@@ -95,4 +156,4 @@\n-  template <bool finalizable> bool is_object_marked(uintptr_t addr) const;\n-  bool is_object_live(uintptr_t addr) const;\n-  bool is_object_strongly_live(uintptr_t addr) const;\n-  bool mark_object(uintptr_t addr, bool finalizable, bool& inc_live);\n+  bool is_object_marked_live(zaddress addr) const;\n+  bool is_object_marked_strong(zaddress addr) const;\n+  bool is_object_marked(zaddress addr, bool finalizable) const;\n+  bool mark_object(zaddress addr, bool finalizable, bool& inc_live);\n@@ -104,1 +165,2 @@\n-  void object_iterate(ObjectClosure* cl);\n+  template <typename Function>\n+  void object_iterate(Function function);\n@@ -106,2 +168,1 @@\n-  uintptr_t alloc_object(size_t size);\n-  uintptr_t alloc_object_atomic(size_t size);\n+  void remember(volatile zpointer* p);\n@@ -109,2 +170,4 @@\n-  bool undo_alloc_object(uintptr_t addr, size_t size);\n-  bool undo_alloc_object_atomic(uintptr_t addr, size_t size);\n+  \/\/ In-place relocation support\n+  void clear_remset_bit_non_par_current(uintptr_t l_offset);\n+  void clear_remset_range_non_par_current(uintptr_t l_offset, size_t size);\n+  void swap_remset_bitmaps();\n@@ -112,0 +175,39 @@\n+  void remset_clear();\n+\n+  ZBitMap::ReverseIterator remset_reverse_iterator_previous();\n+  BitMap::Iterator remset_iterator_limited_current(uintptr_t l_offset, size_t size);\n+  BitMap::Iterator remset_iterator_limited_previous(uintptr_t l_offset, size_t size);\n+\n+  zaddress_unsafe find_base_unsafe(volatile zpointer* p);\n+  zaddress_unsafe find_base(volatile zpointer* p);\n+\n+  template <typename Function>\n+  void oops_do_remembered(Function function);\n+\n+  \/\/ Only visits remembered set entries for live objects\n+  template <typename Function>\n+  void oops_do_remembered_in_live(Function function);\n+\n+  template <typename Function>\n+  void oops_do_current_remembered(Function function);\n+\n+  bool is_remset_cleared_current() const;\n+  bool is_remset_cleared_previous() const;\n+\n+  void verify_remset_cleared_current() const;\n+  void verify_remset_cleared_previous() const;\n+\n+  void clear_remset_current();\n+  void clear_remset_previous();\n+\n+  void* remset_current();\n+\n+  zaddress alloc_object(size_t size);\n+  zaddress alloc_object_atomic(size_t size);\n+\n+  bool undo_alloc_object(zaddress addr, size_t size);\n+  bool undo_alloc_object_atomic(zaddress addr, size_t size);\n+\n+  void log_msg(const char* msg_format, ...) const ATTRIBUTE_PRINTF(2, 3);\n+\n+  void print_on_msg(outputStream* out, const char* msg) const;\n@@ -115,1 +217,6 @@\n-  void verify_live(uint32_t live_objects, size_t live_bytes) const;\n+  \/\/ Verification\n+  bool was_remembered(volatile zpointer* p);\n+  bool is_remembered(volatile zpointer* p);\n+  void verify_live(uint32_t live_objects, size_t live_bytes, bool in_place) const;\n+\n+  void fatal_msg(const char* msg) const;\n","filename":"src\/hotspot\/share\/gc\/z\/zPage.hpp","additions":146,"deletions":39,"binary":false,"changes":185,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -30,0 +30,1 @@\n+#include \"gc\/z\/zGeneration.inline.hpp\"\n@@ -34,0 +35,2 @@\n+#include \"gc\/z\/zRememberedSet.inline.hpp\"\n+#include \"gc\/z\/zUtils.inline.hpp\"\n@@ -35,0 +38,1 @@\n+#include \"logging\/logStream.hpp\"\n@@ -40,1 +44,1 @@\n-inline uint8_t ZPage::type_from_size(size_t size) const {\n+inline ZPageType ZPage::type_from_size(size_t size) const {\n@@ -42,1 +46,1 @@\n-    return ZPageTypeSmall;\n+    return ZPageType::small;\n@@ -44,1 +48,1 @@\n-    return ZPageTypeMedium;\n+    return ZPageType::medium;\n@@ -46,1 +50,1 @@\n-    return ZPageTypeLarge;\n+    return ZPageType::large;\n@@ -52,1 +56,1 @@\n-  case ZPageTypeSmall:\n+  case ZPageType::small:\n@@ -55,1 +59,1 @@\n-  case ZPageTypeMedium:\n+  case ZPageType::medium:\n@@ -58,2 +62,1 @@\n-  default:\n-    assert(type() == ZPageTypeLarge, \"Invalid page type\");\n+  case ZPageType::large:\n@@ -61,0 +64,4 @@\n+\n+  default:\n+    fatal(\"Unexpected page type\");\n+    return 0;\n@@ -66,1 +73,1 @@\n-  case ZPageTypeLarge:\n+  case ZPageType::large:\n@@ -78,1 +85,1 @@\n-  case ZPageTypeSmall:\n+  case ZPageType::small:\n@@ -81,1 +88,1 @@\n-  case ZPageTypeMedium:\n+  case ZPageType::medium:\n@@ -84,2 +91,1 @@\n-  default:\n-    assert(type() == ZPageTypeLarge, \"Invalid page type\");\n+  case ZPageType::large:\n@@ -87,0 +93,4 @@\n+\n+  default:\n+    fatal(\"Unexpected page type\");\n+    return 0;\n@@ -92,1 +102,1 @@\n-  case ZPageTypeSmall:\n+  case ZPageType::small:\n@@ -95,1 +105,1 @@\n-  case ZPageTypeMedium:\n+  case ZPageType::medium:\n@@ -98,2 +108,1 @@\n-  default:\n-    assert(type() == ZPageTypeLarge, \"Invalid page type\");\n+  case ZPageType::large:\n@@ -101,0 +110,4 @@\n+\n+  default:\n+    fatal(\"Unexpected page type\");\n+    return 0;\n@@ -104,1 +117,1 @@\n-inline uint8_t ZPage::type() const {\n+inline ZPageType ZPage::type() const {\n@@ -108,1 +121,25 @@\n-inline uintptr_t ZPage::start() const {\n+inline bool ZPage::is_small() const {\n+  return _type == ZPageType::small;\n+}\n+\n+inline bool ZPage::is_medium() const {\n+  return _type == ZPageType::medium;\n+}\n+\n+inline bool ZPage::is_large() const {\n+  return _type == ZPageType::large;\n+}\n+\n+inline ZGenerationId ZPage::generation_id() const {\n+  return _generation_id;\n+}\n+\n+inline bool ZPage::is_young() const {\n+  return _generation_id == ZGenerationId::young;\n+}\n+\n+inline bool ZPage::is_old() const {\n+  return _generation_id == ZGenerationId::old;\n+}\n+\n+inline zoffset ZPage::start() const {\n@@ -112,1 +149,1 @@\n-inline uintptr_t ZPage::end() const {\n+inline zoffset_end ZPage::end() const {\n@@ -120,1 +157,1 @@\n-inline uintptr_t ZPage::top() const {\n+inline zoffset_end ZPage::top() const {\n@@ -128,0 +165,4 @@\n+inline size_t ZPage::used() const {\n+  return top() - start();\n+}\n+\n@@ -142,1 +183,1 @@\n-    _numa_id = ZNUMA::memory_id(ZAddress::good(start()));\n+    _numa_id = ZNUMA::memory_id(untype(ZOffset::address(start())));\n@@ -148,0 +189,8 @@\n+inline ZPageAge ZPage::age() const {\n+  return _age;\n+}\n+\n+inline uint32_t ZPage::seqnum() const {\n+  return _seqnum;\n+}\n+\n@@ -149,1 +198,1 @@\n-  return _seqnum == ZGlobalSeqNum;\n+  return _seqnum == generation()->seqnum();\n@@ -153,1 +202,1 @@\n-  return _seqnum < ZGlobalSeqNum;\n+  return _seqnum < generation()->seqnum();\n@@ -164,2 +213,1 @@\n-inline bool ZPage::is_in(uintptr_t addr) const {\n-  const uintptr_t offset = ZAddress::offset(addr);\n+inline bool ZPage::is_in(zoffset offset) const {\n@@ -169,0 +217,31 @@\n+inline bool ZPage::is_in(zaddress addr) const {\n+  const zoffset offset = ZAddress::offset(addr);\n+  return is_in(offset);\n+}\n+\n+inline uintptr_t ZPage::local_offset(zoffset offset) const {\n+  assert(ZHeap::heap()->is_in_page_relaxed(this, ZOffset::address(offset)),\n+         \"Invalid offset \" PTR_FORMAT \" page [\" PTR_FORMAT \", \" PTR_FORMAT \", \" PTR_FORMAT \")\",\n+         untype(offset), untype(start()), untype(top()), untype(end()));\n+  return offset - start();\n+}\n+\n+inline uintptr_t ZPage::local_offset(zoffset_end offset) const {\n+  assert(offset <= end(), \"Wrong offset\");\n+  return offset - start();\n+}\n+\n+inline uintptr_t ZPage::local_offset(zaddress addr) const {\n+  const zoffset offset = ZAddress::offset(addr);\n+  return local_offset(offset);\n+}\n+\n+inline uintptr_t ZPage::local_offset(zaddress_unsafe addr) const {\n+  const zoffset offset = ZAddress::offset(addr);\n+  return local_offset(offset);\n+}\n+\n+inline zoffset ZPage::global_offset(uintptr_t local_offset) const {\n+  return start() + local_offset;\n+}\n+\n@@ -171,1 +250,15 @@\n-  return _livemap.is_marked();\n+  return _livemap.is_marked(_generation_id);\n+}\n+\n+inline BitMap::idx_t ZPage::bit_index(zaddress addr) const {\n+  return (local_offset(addr) >> object_alignment_shift()) * 2;\n+}\n+\n+inline zoffset ZPage::offset_from_bit_index(BitMap::idx_t index) const {\n+  const uintptr_t l_offset = ((index \/ 2) << object_alignment_shift());\n+  return start() + l_offset;\n+}\n+\n+inline oop ZPage::object_from_bit_index(BitMap::idx_t index) const {\n+  const zoffset offset = offset_from_bit_index(index);\n+  return to_oop(ZOffset::address(offset));\n@@ -174,1 +267,1 @@\n-inline bool ZPage::is_object_marked(uintptr_t addr) const {\n+inline bool ZPage::is_live_bit_set(zaddress addr) const {\n@@ -176,2 +269,2 @@\n-  const size_t index = ((ZAddress::offset(addr) - start()) >> object_alignment_shift()) * 2;\n-  return _livemap.get(index);\n+  const BitMap::idx_t index = bit_index(addr);\n+  return _livemap.get(_generation_id, index);\n@@ -180,1 +273,1 @@\n-inline bool ZPage::is_object_strongly_marked(uintptr_t addr) const {\n+inline bool ZPage::is_strong_bit_set(zaddress addr) const {\n@@ -182,2 +275,2 @@\n-  const size_t index = ((ZAddress::offset(addr) - start()) >> object_alignment_shift()) * 2;\n-  return _livemap.get(index + 1);\n+  const BitMap::idx_t index = bit_index(addr);\n+  return _livemap.get(_generation_id, index + 1);\n@@ -186,3 +279,2 @@\n-template <bool finalizable>\n-inline bool ZPage::is_object_marked(uintptr_t addr) const {\n-  return finalizable ? is_object_marked(addr) : is_object_strongly_marked(addr);\n+inline bool ZPage::is_object_live(zaddress addr) const {\n+  return is_allocating() || is_live_bit_set(addr);\n@@ -191,2 +283,20 @@\n-inline bool ZPage::is_object_live(uintptr_t addr) const {\n-  return is_allocating() || is_object_marked(addr);\n+inline bool ZPage::is_object_strongly_live(zaddress addr) const {\n+  return is_allocating() || is_strong_bit_set(addr);\n+}\n+\n+inline bool ZPage::is_object_marked_live(zaddress addr) const {\n+  \/\/ This function is only used by the marking code and therefore has stronger\n+  \/\/ asserts that are not always valid to ask when checking for liveness.\n+  assert(!is_old() || ZGeneration::old()->is_phase_mark(), \"Location should match phase\");\n+  assert(!is_young() || ZGeneration::young()->is_phase_mark(), \"Location should match phase\");\n+\n+  return is_object_live(addr);\n+}\n+\n+inline bool ZPage::is_object_marked_strong(zaddress addr) const {\n+  \/\/ This function is only used by the marking code and therefore has stronger\n+  \/\/ asserts that are not always valid to ask when checking for liveness.\n+  assert(!is_old() || ZGeneration::old()->is_phase_mark(), \"Location should match phase\");\n+  assert(!is_young() || ZGeneration::young()->is_phase_mark(), \"Location should match phase\");\n+\n+  return is_object_strongly_live(addr);\n@@ -195,2 +305,2 @@\n-inline bool ZPage::is_object_strongly_live(uintptr_t addr) const {\n-  return is_allocating() || is_object_strongly_marked(addr);\n+inline bool ZPage::is_object_marked(zaddress addr, bool finalizable) const {\n+  return finalizable ? is_object_marked_live(addr) : is_object_marked_strong(addr);\n@@ -199,2 +309,1 @@\n-inline bool ZPage::mark_object(uintptr_t addr, bool finalizable, bool& inc_live) {\n-  assert(ZAddress::is_marked(addr), \"Invalid address\");\n+inline bool ZPage::mark_object(zaddress addr, bool finalizable, bool& inc_live) {\n@@ -204,0 +313,3 @@\n+  \/\/ Verify oop\n+  (void)to_oop(addr);\n+\n@@ -205,2 +317,2 @@\n-  const size_t index = ((ZAddress::offset(addr) - start()) >> object_alignment_shift()) * 2;\n-  return _livemap.set(index, finalizable, inc_live);\n+  const BitMap::idx_t index = bit_index(addr);\n+  return _livemap.set(_generation_id, index, finalizable, inc_live);\n@@ -213,0 +325,7 @@\n+#define assert_zpage_mark_state()                                                  \\\n+  do {                                                                             \\\n+    assert(is_marked(), \"Should be marked\");                                       \\\n+    assert(!is_young() || !ZGeneration::young()->is_phase_mark(), \"Wrong phase\");  \\\n+    assert(!is_old() || !ZGeneration::old()->is_phase_mark(), \"Wrong phase\");      \\\n+  } while (0)\n+\n@@ -214,1 +333,2 @@\n-  assert(is_marked(), \"Should be marked\");\n+  assert_zpage_mark_state();\n+\n@@ -219,1 +339,2 @@\n-  assert(is_marked(), \"Should be marked\");\n+  assert_zpage_mark_state();\n+\n@@ -223,2 +344,98 @@\n-inline void ZPage::object_iterate(ObjectClosure* cl) {\n-  _livemap.iterate(cl, ZAddress::good(start()), object_alignment_shift());\n+template <typename Function>\n+inline void ZPage::object_iterate(Function function) {\n+  auto do_bit = [&](BitMap::idx_t index) -> bool {\n+    const oop obj = object_from_bit_index(index);\n+\n+    \/\/ Apply function\n+    function(obj);\n+\n+    return true;\n+  };\n+\n+  _livemap.iterate(_generation_id, do_bit);\n+}\n+\n+inline void ZPage::remember(volatile zpointer* p) {\n+  const zaddress addr = to_zaddress((uintptr_t)p);\n+  const uintptr_t l_offset = local_offset(addr);\n+  _remembered_set.set_current(l_offset);\n+}\n+\n+inline void ZPage::clear_remset_bit_non_par_current(uintptr_t l_offset) {\n+  _remembered_set.unset_non_par_current(l_offset);\n+}\n+\n+inline void ZPage::clear_remset_range_non_par_current(uintptr_t l_offset, size_t size) {\n+  _remembered_set.unset_range_non_par_current(l_offset, size);\n+}\n+\n+inline ZBitMap::ReverseIterator ZPage::remset_reverse_iterator_previous() {\n+  return _remembered_set.iterator_reverse_previous();\n+}\n+\n+inline BitMap::Iterator ZPage::remset_iterator_limited_current(uintptr_t l_offset, size_t size) {\n+  return _remembered_set.iterator_limited_current(l_offset, size);\n+}\n+\n+inline BitMap::Iterator ZPage::remset_iterator_limited_previous(uintptr_t l_offset, size_t size) {\n+  return _remembered_set.iterator_limited_previous(l_offset, size);\n+}\n+\n+inline bool ZPage::is_remembered(volatile zpointer* p) {\n+  const zaddress addr = to_zaddress((uintptr_t)p);\n+  const uintptr_t l_offset = local_offset(addr);\n+  return _remembered_set.at_current(l_offset);\n+}\n+\n+inline bool ZPage::was_remembered(volatile zpointer* p) {\n+  const zaddress addr = to_zaddress((uintptr_t)p);\n+  const uintptr_t l_offset = local_offset(addr);\n+  return _remembered_set.at_previous(l_offset);\n+}\n+\n+\n+inline zaddress_unsafe ZPage::find_base_unsafe(volatile zpointer* p) {\n+  if (is_large()) {\n+    return ZOffset::address_unsafe(start());\n+  }\n+\n+  \/\/ Note: when thinking about excluding looking at the index corresponding to\n+  \/\/ the field address p, it's important to note that for medium pages both p\n+  \/\/ and it's associated base could map to the same index.\n+  const BitMap::idx_t index = bit_index(zaddress(uintptr_t(p)));\n+  const BitMap::idx_t base_index = _livemap.find_base_bit(index);\n+  if (base_index == BitMap::idx_t(-1)) {\n+    return zaddress_unsafe::null;\n+  } else {\n+    return ZOffset::address_unsafe(offset_from_bit_index(base_index));\n+  }\n+}\n+\n+inline zaddress_unsafe ZPage::find_base(volatile zpointer* p) {\n+  assert_zpage_mark_state();\n+\n+  return find_base_unsafe(p);\n+}\n+\n+template <typename Function>\n+inline void ZPage::oops_do_remembered(Function function) {\n+  _remembered_set.iterate_previous([&](uintptr_t local_offset) {\n+    const zoffset offset = start() + local_offset;\n+    const zaddress addr = ZOffset::address(offset);\n+\n+    function((volatile zpointer*)addr);\n+  });\n+}\n+\n+template <typename Function>\n+inline void ZPage::oops_do_remembered_in_live(Function function) {\n+  assert(!is_allocating(), \"Must have liveness information\");\n+  assert(!ZGeneration::old()->is_phase_mark(), \"Must have liveness information\");\n+  assert(is_marked(), \"Must have liveness information\");\n+\n+  ZRememberedSetContainingInLiveIterator iter(this);\n+  for (ZRememberedSetContaining containing; iter.next(&containing);) {\n+    function((volatile zpointer*)containing._field_addr);\n+  }\n+\n+  iter.print_statistics();\n@@ -227,1 +444,11 @@\n-inline uintptr_t ZPage::alloc_object(size_t size) {\n+template <typename Function>\n+inline void ZPage::oops_do_current_remembered(Function function) {\n+  _remembered_set.iterate_current([&](uintptr_t local_offset) {\n+    const zoffset offset = start() + local_offset;\n+    const zaddress addr = ZOffset::address(offset);\n+\n+    function((volatile zpointer*)addr);\n+  });\n+}\n+\n+inline zaddress ZPage::alloc_object(size_t size) {\n@@ -231,2 +458,8 @@\n-  const uintptr_t addr = top();\n-  const uintptr_t new_top = addr + aligned_size;\n+  const zoffset_end addr = top();\n+\n+  zoffset_end new_top;\n+\n+  if (!to_zoffset_end(&new_top, addr, aligned_size)) {\n+    \/\/ Next top would be outside of the heap - bail\n+    return zaddress::null;\n+  }\n@@ -235,2 +468,2 @@\n-    \/\/ Not enough space left\n-    return 0;\n+    \/\/ Not enough space left in the page\n+    return zaddress::null;\n@@ -241,1 +474,1 @@\n-  return ZAddress::good(addr);\n+  return ZOffset::address(to_zoffset(addr));\n@@ -244,1 +477,1 @@\n-inline uintptr_t ZPage::alloc_object_atomic(size_t size) {\n+inline zaddress ZPage::alloc_object_atomic(size_t size) {\n@@ -248,1 +481,1 @@\n-  uintptr_t addr = top();\n+  zoffset_end addr = top();\n@@ -251,1 +484,7 @@\n-    const uintptr_t new_top = addr + aligned_size;\n+    zoffset_end new_top;\n+\n+    if (!to_zoffset_end(&new_top, addr, aligned_size)) {\n+      \/\/ Next top would be outside of the heap - bail\n+      return zaddress::null;\n+    }\n+\n@@ -254,1 +493,1 @@\n-      return 0;\n+      return zaddress::null;\n@@ -257,1 +496,1 @@\n-    const uintptr_t prev_top = Atomic::cmpxchg(&_top, addr, new_top);\n+    const zoffset_end prev_top = Atomic::cmpxchg(&_top, addr, new_top);\n@@ -260,1 +499,1 @@\n-      return ZAddress::good(addr);\n+      return ZOffset::address(to_zoffset(addr));\n@@ -268,1 +507,1 @@\n-inline bool ZPage::undo_alloc_object(uintptr_t addr, size_t size) {\n+inline bool ZPage::undo_alloc_object(zaddress addr, size_t size) {\n@@ -271,1 +510,1 @@\n-  const uintptr_t offset = ZAddress::offset(addr);\n+  const zoffset offset = ZAddress::offset(addr);\n@@ -273,2 +512,2 @@\n-  const uintptr_t old_top = top();\n-  const uintptr_t new_top = old_top - aligned_size;\n+  const zoffset_end old_top = top();\n+  const zoffset_end new_top = old_top - aligned_size;\n@@ -287,1 +526,1 @@\n-inline bool ZPage::undo_alloc_object_atomic(uintptr_t addr, size_t size) {\n+inline bool ZPage::undo_alloc_object_atomic(zaddress addr, size_t size) {\n@@ -290,1 +529,1 @@\n-  const uintptr_t offset = ZAddress::offset(addr);\n+  const zoffset offset = ZAddress::offset(addr);\n@@ -292,1 +531,1 @@\n-  uintptr_t old_top = top();\n+  zoffset_end old_top = top();\n@@ -295,1 +534,1 @@\n-    const uintptr_t new_top = old_top - aligned_size;\n+    const zoffset_end new_top = old_top - aligned_size;\n@@ -301,1 +540,1 @@\n-    const uintptr_t prev_top = Atomic::cmpxchg(&_top, old_top, new_top);\n+    const zoffset_end prev_top = Atomic::cmpxchg(&_top, old_top, new_top);\n@@ -312,0 +551,11 @@\n+inline void ZPage::log_msg(const char* msg_format, ...) const {\n+  LogTarget(Trace, gc, page) target;\n+  if (target.is_enabled()) {\n+    va_list argp;\n+    va_start(argp, msg_format);\n+    LogStream stream(target);\n+    print_on_msg(&stream, err_msg(FormatBufferDummy(), msg_format, argp));\n+    va_end(argp);\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/z\/zPage.inline.hpp","additions":320,"deletions":70,"binary":false,"changes":390,"status":"modified"},{"patch":"@@ -0,0 +1,50 @@\n+\/*\n+ * Copyright (c) 2021, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_Z_ZPAGEAGE_HPP\n+#define SHARE_GC_Z_ZPAGEAGE_HPP\n+\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+enum class ZPageAge : uint8_t {\n+  eden,\n+  survivor1,\n+  survivor2,\n+  survivor3,\n+  survivor4,\n+  survivor5,\n+  survivor6,\n+  survivor7,\n+  survivor8,\n+  survivor9,\n+  survivor10,\n+  survivor11,\n+  survivor12,\n+  survivor13,\n+  survivor14,\n+  old\n+};\n+\n+constexpr uint ZPageAgeMax = static_cast<uint>(ZPageAge::old);\n+\n+#endif \/\/ SHARE_GC_Z_ZPAGEAGE_HPP\n","filename":"src\/hotspot\/share\/gc\/z\/zPageAge.hpp","additions":50,"deletions":0,"binary":false,"changes":50,"status":"added"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,1 +28,1 @@\n-#include \"gc\/z\/zCollectedHeap.hpp\"\n+#include \"gc\/z\/zDriver.hpp\"\n@@ -30,0 +30,2 @@\n+#include \"gc\/z\/zGeneration.inline.hpp\"\n+#include \"gc\/z\/zGenerationId.hpp\"\n@@ -33,0 +35,1 @@\n+#include \"gc\/z\/zPageAge.hpp\"\n@@ -49,1 +52,1 @@\n-static const ZStatCounter       ZCounterAllocationRate(\"Memory\", \"Allocation Rate\", ZStatUnitBytesPerSecond);\n+static const ZStatCounter       ZCounterMutatorAllocationRate(\"Memory\", \"Allocation Rate\", ZStatUnitBytesPerSecond);\n@@ -54,5 +57,41 @@\n-enum ZPageAllocationStall {\n-  ZPageAllocationStallSuccess,\n-  ZPageAllocationStallFailed,\n-  ZPageAllocationStallStartGC\n-};\n+ZSafePageRecycle::ZSafePageRecycle(ZPageAllocator* page_allocator) :\n+    _page_allocator(page_allocator),\n+    _unsafe_to_recycle() {}\n+\n+void ZSafePageRecycle::activate() {\n+  _unsafe_to_recycle.activate();\n+}\n+\n+void ZSafePageRecycle::deactivate() {\n+  auto delete_function = [&](ZPage* page) {\n+    _page_allocator->safe_destroy_page(page);\n+  };\n+\n+  _unsafe_to_recycle.deactivate_and_apply(delete_function);\n+}\n+\n+ZPage* ZSafePageRecycle::register_and_clone_if_activated(ZPage* page) {\n+  if (!_unsafe_to_recycle.is_activated()) {\n+    \/\/ The page has no concurrent readers.\n+    \/\/ Recycle original page.\n+    return page;\n+  }\n+\n+  \/\/ The page could have concurrent readers.\n+  \/\/ It would be unsafe to recycle this page at this point.\n+\n+  \/\/ As soon as the page is added to _unsafe_to_recycle, it\n+  \/\/ must not be used again. Hence, the extra double-checked\n+  \/\/ locking to only clone the page if it is believed to be\n+  \/\/ unsafe to recycle the page.\n+  ZPage* const cloned_page = page->clone_limited();\n+  if (!_unsafe_to_recycle.add_if_activated(page)) {\n+    \/\/ It became safe to recycle the page after the is_activated check\n+    delete cloned_page;\n+    return page;\n+  }\n+\n+  \/\/ The original page has been registered to be deleted by another thread.\n+  \/\/ Recycle the cloned page.\n+  return cloned_page;\n+}\n@@ -64,9 +103,10 @@\n-  const uint8_t                 _type;\n-  const size_t                  _size;\n-  const ZAllocationFlags        _flags;\n-  const uint32_t                _seqnum;\n-  size_t                        _flushed;\n-  size_t                        _committed;\n-  ZList<ZPage>                  _pages;\n-  ZListNode<ZPageAllocation>    _node;\n-  ZFuture<ZPageAllocationStall> _stall_result;\n+  const ZPageType            _type;\n+  const size_t               _size;\n+  const ZAllocationFlags     _flags;\n+  const uint32_t             _young_seqnum;\n+  const uint32_t             _old_seqnum;\n+  size_t                     _flushed;\n+  size_t                     _committed;\n+  ZList<ZPage>               _pages;\n+  ZListNode<ZPageAllocation> _node;\n+  ZFuture<bool>              _stall_result;\n@@ -75,1 +115,1 @@\n-  ZPageAllocation(uint8_t type, size_t size, ZAllocationFlags flags) :\n+  ZPageAllocation(ZPageType type, size_t size, ZAllocationFlags flags) :\n@@ -79,1 +119,2 @@\n-      _seqnum(ZGlobalSeqNum),\n+      _young_seqnum(ZGeneration::young()->seqnum()),\n+      _old_seqnum(ZGeneration::old()->seqnum()),\n@@ -86,1 +127,1 @@\n-  uint8_t type() const {\n+  ZPageType type() const {\n@@ -98,2 +139,6 @@\n-  uint32_t seqnum() const {\n-    return _seqnum;\n+  uint32_t young_seqnum() const {\n+    return _young_seqnum;\n+  }\n+\n+  uint32_t old_seqnum() const {\n+    return _old_seqnum;\n@@ -118,1 +163,1 @@\n-  ZPageAllocationStall wait() {\n+  bool wait() {\n@@ -126,1 +171,1 @@\n-  void satisfy(ZPageAllocationStall result) {\n+  void satisfy(bool result) {\n@@ -129,0 +174,4 @@\n+\n+  bool gc_relocation() const {\n+    return _flags.gc_relocation();\n+  }\n@@ -131,2 +180,1 @@\n-ZPageAllocator::ZPageAllocator(ZWorkers* workers,\n-                               size_t min_capacity,\n+ZPageAllocator::ZPageAllocator(size_t min_capacity,\n@@ -134,0 +182,1 @@\n+                               size_t soft_max_capacity,\n@@ -140,0 +189,1 @@\n+    _initial_capacity(initial_capacity),\n@@ -145,3 +195,2 @@\n-    _used_high(0),\n-    _used_low(0),\n-    _reclaimed(0),\n+    _used_generations{0, 0},\n+    _collection_stats{{0, 0}, {0, 0}},\n@@ -149,2 +198,0 @@\n-    _nstalled(0),\n-    _satisfied(),\n@@ -153,1 +200,2 @@\n-    _safe_delete(),\n+    _safe_destroy(),\n+    _safe_recycle(this),\n@@ -163,0 +211,1 @@\n+  log_info_p(gc, init)(\"Soft Max Capacity: \" SIZE_FORMAT \"M\", soft_max_capacity \/ M);\n@@ -176,6 +225,0 @@\n-  \/\/ Pre-map initial capacity\n-  if (!prime_cache(workers, initial_capacity)) {\n-    log_error_p(gc)(\"Failed to allocate initial Java heap (\" SIZE_FORMAT \"M)\", initial_capacity \/ M);\n-    return;\n-  }\n-\n@@ -186,0 +229,4 @@\n+bool ZPageAllocator::is_initialized() const {\n+  return _initialized;\n+}\n+\n@@ -189,2 +236,2 @@\n-  volatile uintptr_t                  _start;\n-  const uintptr_t                     _end;\n+  volatile zoffset                    _start;\n+  const zoffset_end                   _end;\n@@ -193,1 +240,1 @@\n-  ZPreTouchTask(const ZPhysicalMemoryManager* physical, uintptr_t start, uintptr_t end) :\n+  ZPreTouchTask(const ZPhysicalMemoryManager* physical, zoffset start, zoffset_end end) :\n@@ -203,1 +250,1 @@\n-      const uintptr_t offset = Atomic::fetch_and_add(&_start, size);\n+      const zoffset offset = to_zoffset(Atomic::fetch_and_add((uintptr_t*)&_start, size));\n@@ -217,1 +264,0 @@\n-\n@@ -221,2 +267,2 @@\n-  ZPage* const page = alloc_page(ZPageTypeLarge, size, flags);\n-  if (page == NULL) {\n+  ZPage* const page = alloc_page(ZPageType::large, size, flags, ZPageAge::eden);\n+  if (page == nullptr) {\n@@ -232,1 +278,1 @@\n-  free_page(page, false \/* reclaimed *\/);\n+  free_page(page);\n@@ -237,2 +283,2 @@\n-bool ZPageAllocator::is_initialized() const {\n-  return _initialized;\n+size_t ZPageAllocator::initial_capacity() const {\n+  return _initial_capacity;\n@@ -264,0 +310,4 @@\n+size_t ZPageAllocator::used_generation(ZGenerationId id) const {\n+  return Atomic::load(&_used_generations[(int)id]);\n+}\n+\n@@ -272,1 +322,1 @@\n-ZPageAllocatorStats ZPageAllocator::stats() const {\n+ZPageAllocatorStats ZPageAllocator::stats(ZGeneration* generation) const {\n@@ -279,3 +329,7 @@\n-                             _used_high,\n-                             _used_low,\n-                             _reclaimed);\n+                             _collection_stats[(int)generation->id()]._used_high,\n+                             _collection_stats[(int)generation->id()]._used_low,\n+                             used_generation(generation->id()),\n+                             generation->freed(),\n+                             generation->promoted(),\n+                             generation->compacted(),\n+                             _stalled.size());\n@@ -284,1 +338,1 @@\n-void ZPageAllocator::reset_statistics() {\n+void ZPageAllocator::reset_statistics(ZGenerationId id) {\n@@ -286,3 +340,2 @@\n-  _reclaimed = 0;\n-  _used_high = _used_low = _used;\n-  _nstalled = 0;\n+  _collection_stats[(int)id]._used_high = _used;\n+  _collection_stats[(int)id]._used_low = _used;\n@@ -325,6 +378,8 @@\n-void ZPageAllocator::increase_used(size_t size, bool worker_relocation) {\n-  if (worker_relocation) {\n-    \/\/ Allocating a page for the purpose of worker relocation has\n-    \/\/ a negative contribution to the number of reclaimed bytes.\n-    _reclaimed -= size;\n-  }\n+void ZPageAllocator::increase_used(size_t size) {\n+  \/\/ We don't track generation usage here because this page\n+  \/\/ could be allocated by a thread that satisfies a stalling\n+  \/\/ allocation. The stalled thread can wake up and potentially\n+  \/\/ realize that the page alloc should be undone. If the alloc\n+  \/\/ and the undo gets separated by a safepoint, the generation\n+  \/\/ statistics could se a decreasing used value between mark\n+  \/\/ start and mark end.\n@@ -334,4 +389,0 @@\n-  if (used > _used_high) {\n-    _used_high = used;\n-  }\n-}\n@@ -339,7 +390,5 @@\n-void ZPageAllocator::decrease_used(size_t size, bool reclaimed) {\n-  \/\/ Only pages explicitly released with the reclaimed flag set\n-  \/\/ counts as reclaimed bytes. This flag is true when we release\n-  \/\/ a page after relocation, and is false when we release a page\n-  \/\/ to undo an allocation.\n-  if (reclaimed) {\n-    _reclaimed += size;\n+  \/\/ Update used high\n+  for (auto& stats : _collection_stats) {\n+    if (used > stats._used_high) {\n+      stats._used_high = used;\n+    }\n@@ -347,0 +396,1 @@\n+}\n@@ -348,0 +398,1 @@\n+void ZPageAllocator::decrease_used(size_t size) {\n@@ -350,2 +401,6 @@\n-  if (used < _used_low) {\n-    _used_low = used;\n+\n+  \/\/ Update used low\n+  for (auto& stats : _collection_stats) {\n+    if (used < stats._used_low) {\n+      stats._used_low = used;\n+    }\n@@ -355,0 +410,15 @@\n+void ZPageAllocator::increase_used_generation(ZGenerationId id, size_t size) {\n+  \/\/ Update atomically since we have concurrent readers\n+  Atomic::add(&_used_generations[(int)id], size, memory_order_relaxed);\n+}\n+\n+void ZPageAllocator::decrease_used_generation(ZGenerationId id, size_t size) {\n+  \/\/ Update atomically since we have concurrent readers\n+  Atomic::sub(&_used_generations[(int)id], size, memory_order_relaxed);\n+}\n+\n+void ZPageAllocator::promote_used(size_t size) {\n+  decrease_used_generation(ZGenerationId::young, size);\n+  increase_used_generation(ZGenerationId::old, size);\n+}\n+\n@@ -379,0 +449,5 @@\n+void ZPageAllocator::safe_destroy_page(ZPage* page) {\n+  \/\/ Destroy page safely\n+  _safe_destroy.schedule_delete(page);\n+}\n+\n@@ -386,2 +461,2 @@\n-  \/\/ Delete page safely\n-  _safe_delete(page);\n+  \/\/ Destroy page safely\n+  safe_destroy_page(page);\n@@ -395,1 +470,1 @@\n-bool ZPageAllocator::alloc_page_common_inner(uint8_t type, size_t size, ZList<ZPage>* pages) {\n+bool ZPageAllocator::alloc_page_common_inner(ZPageType type, size_t size, ZList<ZPage>* pages) {\n@@ -403,1 +478,1 @@\n-  if (page != NULL) {\n+  if (page != nullptr) {\n@@ -423,1 +498,1 @@\n-  const uint8_t type = allocation->type();\n+  const ZPageType type = allocation->type();\n@@ -434,1 +509,1 @@\n-  increase_used(size, flags.worker_relocation());\n+  increase_used(size);\n@@ -449,1 +524,0 @@\n-  ZPageAllocationStall result;\n@@ -454,2 +528,3 @@\n-  \/\/ Increment stalled counter\n-  Atomic::inc(&_nstalled);\n+  \/\/ Start asynchronous minor GC\n+  const ZDriverRequest request(GCCause::_z_allocation_stall, ZYoungGCThreads, 0);\n+  ZDriver::minor()->collect(request);\n@@ -457,7 +532,2 @@\n-  do {\n-    \/\/ Start asynchronous GC\n-    ZCollectedHeap::heap()->collect(GCCause::_z_allocation_stall);\n-\n-    \/\/ Wait for allocation to complete, fail or request a GC\n-    result = allocation->wait();\n-  } while (result == ZPageAllocationStallStartGC);\n+  \/\/ Wait for allocation to complete or fail\n+  const bool result = allocation->wait();\n@@ -466,4 +536,1 @@\n-    \/\/\n-    \/\/ We grab the lock here for two different reasons:\n-    \/\/\n-    \/\/ 1) Guard deletion of underlying semaphore. This is a workaround for\n+    \/\/ Guard deletion of underlying semaphore. This is a workaround for\n@@ -476,3 +543,0 @@\n-    \/\/\n-    \/\/ 2) Guard the list of satisfied pages.\n-    \/\/\n@@ -480,1 +544,0 @@\n-    _satisfied.remove(allocation);\n@@ -484,1 +547,1 @@\n-  event.commit(allocation->type(), allocation->size());\n+  event.commit((u8)allocation->type(), allocation->size());\n@@ -486,1 +549,1 @@\n-  return (result == ZPageAllocationStallSuccess);\n+  return result;\n@@ -522,1 +585,1 @@\n-    return NULL;\n+    return nullptr;\n@@ -568,2 +631,2 @@\n-  return page->type() == ZPageTypeSmall &&\n-         page->start() >= _virtual.reserved() \/ 2 &&\n+  return page->type() == ZPageType::small &&\n+         page->start() >= to_zoffset(_virtual.reserved() \/ 2) &&\n@@ -610,1 +673,1 @@\n-  if (page == NULL) {\n+  if (page == nullptr) {\n@@ -612,1 +675,1 @@\n-    return NULL;\n+    return nullptr;\n@@ -628,1 +691,1 @@\n-  if (committed_page != NULL) {\n+  if (committed_page != nullptr) {\n@@ -633,1 +696,1 @@\n-  return NULL;\n+  return nullptr;\n@@ -636,22 +699,1 @@\n-void ZPageAllocator::alloc_page_failed(ZPageAllocation* allocation) {\n-  ZLocker<ZLock> locker(&_lock);\n-\n-  size_t freed = 0;\n-\n-  \/\/ Free any allocated\/flushed pages\n-  ZListRemoveIterator<ZPage> iter(allocation->pages());\n-  for (ZPage* page; iter.next(&page);) {\n-    freed += page->size();\n-    free_page_inner(page, false \/* reclaimed *\/);\n-  }\n-\n-  \/\/ Adjust capacity and used to reflect the failed capacity increase\n-  const size_t remaining = allocation->size() - freed;\n-  decrease_used(remaining, false \/* reclaimed *\/);\n-  decrease_capacity(remaining, true \/* set_max_capacity *\/);\n-\n-  \/\/ Try satisfy stalled allocations\n-  satisfy_stalled();\n-}\n-\n-ZPage* ZPageAllocator::alloc_page(uint8_t type, size_t size, ZAllocationFlags flags) {\n+ZPage* ZPageAllocator::alloc_page(ZPageType type, size_t size, ZAllocationFlags flags, ZPageAge age) {\n@@ -670,1 +712,1 @@\n-    return NULL;\n+    return nullptr;\n@@ -674,1 +716,1 @@\n-  if (page == NULL) {\n+  if (page == nullptr) {\n@@ -677,1 +719,1 @@\n-    alloc_page_failed(&allocation);\n+    free_pages_alloc_failed(&allocation);\n@@ -681,0 +723,6 @@\n+  \/\/ The generation's used is tracked here when the page is handed out\n+  \/\/ to the allocating thread. The overall heap \"used\" is tracked in\n+  \/\/ the lower-level allocation code.\n+  const ZGenerationId id = age == ZPageAge::old ? ZGenerationId::old : ZGenerationId::young;\n+  increase_used_generation(id, size);\n+\n@@ -684,1 +732,1 @@\n-  page->reset();\n+  page->reset(age, ZPageResetType::Allocation);\n@@ -686,1 +734,1 @@\n-  \/\/ Update allocation statistics. Exclude worker relocations to avoid\n+  \/\/ Update allocation statistics. Exclude gc relocations to avoid\n@@ -688,1 +736,1 @@\n-  if (!flags.worker_relocation() && is_init_completed()) {\n+  if (!flags.gc_relocation() && is_init_completed()) {\n@@ -691,3 +739,2 @@\n-    const size_t bytes = page->size();\n-    ZStatInc(ZCounterAllocationRate, bytes);\n-    ZStatInc(ZStatAllocRate::counter(), bytes);\n+    ZStatInc(ZCounterMutatorAllocationRate, size);\n+    ZStatMutatorAllocRate::sample_allocation(size);\n@@ -697,1 +744,1 @@\n-  event.commit(type, size, allocation.flushed(), allocation.committed(),\n+  event.commit((u8)type, size, allocation.flushed(), allocation.committed(),\n@@ -706,1 +753,1 @@\n-    if (allocation == NULL) {\n+    if (allocation == nullptr) {\n@@ -720,2 +767,1 @@\n-    _satisfied.insert_last(allocation);\n-    allocation->satisfy(ZPageAllocationStallSuccess);\n+    allocation->satisfy(true);\n@@ -725,4 +771,1 @@\n-void ZPageAllocator::free_page_inner(ZPage* page, bool reclaimed) {\n-  \/\/ Update used statistics\n-  decrease_used(page->size(), reclaimed);\n-\n+void ZPageAllocator::recycle_page(ZPage* page) {\n@@ -736,1 +779,4 @@\n-void ZPageAllocator::free_page(ZPage* page, bool reclaimed) {\n+void ZPageAllocator::free_page(ZPage* page) {\n+  const ZGenerationId generation_id = page->generation_id();\n+  ZPage* const to_recycle = _safe_recycle.register_and_clone_if_activated(page);\n+\n@@ -739,0 +785,5 @@\n+  \/\/ Update used statistics\n+  const size_t size = to_recycle->size();\n+  decrease_used(size);\n+  decrease_used_generation(generation_id, size);\n+\n@@ -740,1 +791,1 @@\n-  free_page_inner(page, reclaimed);\n+  recycle_page(to_recycle);\n@@ -746,1 +797,16 @@\n-void ZPageAllocator::free_pages(const ZArray<ZPage*>* pages, bool reclaimed) {\n+void ZPageAllocator::free_pages(const ZArray<ZPage*>* pages) {\n+  ZArray<ZPage*> to_recycle;\n+\n+  size_t young_size = 0;\n+  size_t old_size = 0;\n+\n+  ZArrayIterator<ZPage*> pages_iter(pages);\n+  for (ZPage* page; pages_iter.next(&page);) {\n+    if (page->is_young()) {\n+      young_size += page->size();\n+    } else {\n+      old_size += page->size();\n+    }\n+    to_recycle.push(_safe_recycle.register_and_clone_if_activated(page));\n+  }\n+\n@@ -749,0 +815,5 @@\n+  \/\/ Update used statistics\n+  decrease_used(young_size + old_size);\n+  decrease_used_generation(ZGenerationId::young, young_size);\n+  decrease_used_generation(ZGenerationId::old, old_size);\n+\n@@ -750,1 +821,1 @@\n-  ZArrayIterator<ZPage*> iter(pages);\n+  ZArrayIterator<ZPage*> iter(&to_recycle);\n@@ -752,1 +823,1 @@\n-    free_page_inner(page, reclaimed);\n+    recycle_page(page);\n@@ -759,0 +830,31 @@\n+void ZPageAllocator::free_pages_alloc_failed(ZPageAllocation* allocation) {\n+  ZArray<ZPage*> to_recycle;\n+\n+  ZListRemoveIterator<ZPage> allocation_pages_iter(allocation->pages());\n+  for (ZPage* page; allocation_pages_iter.next(&page);) {\n+    to_recycle.push(_safe_recycle.register_and_clone_if_activated(page));\n+  }\n+\n+  ZLocker<ZLock> locker(&_lock);\n+\n+  \/\/ Only decrease the overall used and not the generation used,\n+  \/\/ since the allocation failed and generation used wasn't bumped.\n+  decrease_used(allocation->size());\n+\n+  size_t freed = 0;\n+\n+  \/\/ Free any allocated\/flushed pages\n+  ZArrayIterator<ZPage*> iter(&to_recycle);\n+  for (ZPage* page; iter.next(&page);) {\n+    freed += page->size();\n+    recycle_page(page);\n+  }\n+\n+  \/\/ Adjust capacity and used to reflect the failed capacity increase\n+  const size_t remaining = allocation->size() - freed;\n+  decrease_capacity(remaining, true \/* set_max_capacity *\/);\n+\n+  \/\/ Try satisfy stalled allocations\n+  satisfy_stalled();\n+}\n+\n@@ -761,5 +863,1 @@\n-  \/\/ used, to make sure GC safepoints will have a consistent view. However, when\n-  \/\/ ZVerifyViews is enabled we need to join at a broader scope to also make sure\n-  \/\/ we don't change the address good mask after pages have been flushed, and\n-  \/\/ thereby made invisible to pages_do(), but before they have been unmapped.\n-  SuspendibleThreadSetJoiner joiner(ZVerifyViews);\n+  \/\/ used, to make sure GC safepoints will have a consistent view.\n@@ -770,1 +868,1 @@\n-    SuspendibleThreadSetJoiner joiner(!ZVerifyViews);\n+    SuspendibleThreadSetJoiner sts_joiner;\n@@ -801,1 +899,1 @@\n-    SuspendibleThreadSetJoiner joiner(!ZVerifyViews);\n+    SuspendibleThreadSetJoiner sts_joiner;\n@@ -812,2 +910,2 @@\n-void ZPageAllocator::enable_deferred_delete() const {\n-  _safe_delete.enable_deferred_delete();\n+void ZPageAllocator::enable_safe_destroy() const {\n+  _safe_destroy.enable_deferred_delete();\n@@ -816,2 +914,2 @@\n-void ZPageAllocator::disable_deferred_delete() const {\n-  _safe_delete.disable_deferred_delete();\n+void ZPageAllocator::disable_safe_destroy() const {\n+  _safe_destroy.disable_deferred_delete();\n@@ -820,3 +918,2 @@\n-void ZPageAllocator::debug_map_page(const ZPage* page) const {\n-  assert(SafepointSynchronize::is_at_safepoint(), \"Should be at safepoint\");\n-  _physical.debug_map(page->start(), page->physical_memory());\n+void ZPageAllocator::enable_safe_recycle() const {\n+  _safe_recycle.activate();\n@@ -825,3 +922,2 @@\n-void ZPageAllocator::debug_unmap_page(const ZPage* page) const {\n-  assert(SafepointSynchronize::is_at_safepoint(), \"Should be at safepoint\");\n-  _physical.debug_unmap(page->start(), page->size());\n+void ZPageAllocator::disable_safe_recycle() const {\n+  _safe_recycle.deactivate();\n@@ -830,10 +926,3 @@\n-void ZPageAllocator::pages_do(ZPageClosure* cl) const {\n-  assert(SafepointSynchronize::is_at_safepoint(), \"Should be at safepoint\");\n-\n-  ZListIterator<ZPageAllocation> iter_satisfied(&_satisfied);\n-  for (ZPageAllocation* allocation; iter_satisfied.next(&allocation);) {\n-    ZListIterator<ZPage> iter_pages(allocation->pages());\n-    for (ZPage* page; iter_pages.next(&page);) {\n-      cl->do_page(page);\n-    }\n-  }\n+static bool has_alloc_seen_young(const ZPageAllocation* allocation) {\n+  return allocation->young_seqnum() != ZGeneration::young()->seqnum();\n+}\n@@ -841,1 +930,2 @@\n-  _cache.pages_do(cl);\n+static bool has_alloc_seen_old(const ZPageAllocation* allocation) {\n+  return allocation->old_seqnum() != ZGeneration::old()->seqnum();\n@@ -844,2 +934,3 @@\n-bool ZPageAllocator::has_alloc_stalled() const {\n-  return Atomic::load(&_nstalled) != 0;\n+bool ZPageAllocator::is_alloc_stalling() const {\n+  ZLocker<ZLock> locker(&_lock);\n+  return _stalled.first() != nullptr;\n@@ -848,1 +939,1 @@\n-void ZPageAllocator::check_out_of_memory() {\n+bool ZPageAllocator::is_alloc_stalling_for_old() const {\n@@ -851,6 +942,14 @@\n-  \/\/ Fail allocation requests that were enqueued before the\n-  \/\/ last GC cycle started, otherwise start a new GC cycle.\n-  for (ZPageAllocation* allocation = _stalled.first(); allocation != NULL; allocation = _stalled.first()) {\n-    if (allocation->seqnum() == ZGlobalSeqNum) {\n-      \/\/ Start a new GC cycle, keep allocation requests enqueued\n-      allocation->satisfy(ZPageAllocationStallStartGC);\n+  ZPageAllocation* const allocation = _stalled.first();\n+  if (allocation == nullptr) {\n+    \/\/ No stalled allocations\n+    return false;\n+  }\n+\n+  return has_alloc_seen_young(allocation) && !has_alloc_seen_old(allocation);\n+}\n+\n+void ZPageAllocator::notify_out_of_memory() {\n+  \/\/ Fail allocation requests that were enqueued before the last major GC started\n+  for (ZPageAllocation* allocation = _stalled.first(); allocation != nullptr; allocation = _stalled.first()) {\n+    if (!has_alloc_seen_old(allocation)) {\n+      \/\/ Not out of memory, keep remaining allocation requests enqueued\n@@ -860,1 +959,1 @@\n-    \/\/ Out of memory, fail allocation request\n+    \/\/ Out of memory, dequeue and fail allocation request\n@@ -862,2 +961,9 @@\n-    _satisfied.insert_last(allocation);\n-    allocation->satisfy(ZPageAllocationStallFailed);\n+    allocation->satisfy(false);\n+  }\n+}\n+\n+void ZPageAllocator::restart_gc() const {\n+  ZPageAllocation* const allocation = _stalled.first();\n+  if (allocation == nullptr) {\n+    \/\/ No stalled allocations\n+    return;\n@@ -865,0 +971,21 @@\n+\n+  if (!has_alloc_seen_young(allocation)) {\n+    \/\/ Start asynchronous minor GC, keep allocation requests enqueued\n+    const ZDriverRequest request(GCCause::_z_allocation_stall, ZYoungGCThreads, 0);\n+    ZDriver::minor()->collect(request);\n+  } else {\n+    \/\/ Start asynchronous major GC, keep allocation requests enqueued\n+    const ZDriverRequest request(GCCause::_z_allocation_stall, ZYoungGCThreads, ZOldGCThreads);\n+    ZDriver::major()->collect(request);\n+  }\n+}\n+\n+void ZPageAllocator::handle_alloc_stalling_for_young() {\n+  ZLocker<ZLock> locker(&_lock);\n+  restart_gc();\n+}\n+\n+void ZPageAllocator::handle_alloc_stalling_for_old() {\n+  ZLocker<ZLock> locker(&_lock);\n+  notify_out_of_memory();\n+  restart_gc();\n","filename":"src\/hotspot\/share\/gc\/z\/zPageAllocator.cpp","additions":319,"deletions":192,"binary":false,"changes":511,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -31,0 +31,1 @@\n+#include \"gc\/z\/zPageAge.hpp\"\n@@ -32,0 +33,1 @@\n+#include \"gc\/z\/zPageType.hpp\"\n@@ -37,0 +39,1 @@\n+class ZGeneration;\n@@ -38,0 +41,1 @@\n+class ZPageAllocator;\n@@ -43,0 +47,14 @@\n+class ZSafePageRecycle {\n+private:\n+  ZPageAllocator*        _page_allocator;\n+  ZActivatedArray<ZPage> _unsafe_to_recycle;\n+\n+public:\n+  ZSafePageRecycle(ZPageAllocator* page_allocator);\n+\n+  void activate();\n+  void deactivate();\n+\n+  ZPage* register_and_clone_if_activated(ZPage* page);\n+};\n+\n@@ -54,0 +72,1 @@\n+  const size_t               _initial_capacity;\n@@ -59,3 +78,5 @@\n-  size_t                     _used_high;\n-  size_t                     _used_low;\n-  ssize_t                    _reclaimed;\n+  size_t                     _used_generations[2];\n+  struct {\n+    size_t                   _used_high;\n+    size_t                   _used_low;\n+  } _collection_stats[2];\n@@ -63,2 +84,0 @@\n-  volatile uint64_t          _nstalled;\n-  ZList<ZPageAllocation>     _satisfied;\n@@ -67,1 +86,2 @@\n-  mutable ZSafeDelete<ZPage> _safe_delete;\n+  mutable ZSafeDelete<ZPage> _safe_destroy;\n+  mutable ZSafePageRecycle   _safe_recycle;\n@@ -70,2 +90,0 @@\n-  bool prime_cache(ZWorkers* workers, size_t size);\n-\n@@ -75,2 +93,5 @@\n-  void increase_used(size_t size, bool relocation);\n-  void decrease_used(size_t size, bool reclaimed);\n+  void increase_used(size_t size);\n+  void decrease_used(size_t size);\n+\n+  void increase_used_generation(ZGenerationId id, size_t size);\n+  void decrease_used_generation(ZGenerationId id, size_t size);\n@@ -88,1 +109,1 @@\n-  bool alloc_page_common_inner(uint8_t type, size_t size, ZList<ZPage>* pages);\n+  bool alloc_page_common_inner(ZPageType type, size_t size, ZList<ZPage>* pages);\n@@ -96,1 +117,1 @@\n-  void alloc_page_failed(ZPageAllocation* allocation);\n+  void free_pages_alloc_failed(ZPageAllocation* allocation);\n@@ -100,2 +121,0 @@\n-  void free_page_inner(ZPage* page, bool reclaimed);\n-\n@@ -104,0 +123,3 @@\n+  void notify_out_of_memory();\n+  void restart_gc() const;\n+\n@@ -105,2 +127,1 @@\n-  ZPageAllocator(ZWorkers* workers,\n-                 size_t min_capacity,\n+  ZPageAllocator(size_t min_capacity,\n@@ -108,0 +129,1 @@\n+                 size_t soft_max_capacity,\n@@ -112,0 +134,3 @@\n+  bool prime_cache(ZWorkers* workers, size_t size);\n+\n+  size_t initial_capacity() const;\n@@ -117,0 +142,1 @@\n+  size_t used_generation(ZGenerationId id) const;\n@@ -119,1 +145,1 @@\n-  ZPageAllocatorStats stats() const;\n+  void promote_used(size_t size);\n@@ -121,1 +147,1 @@\n-  void reset_statistics();\n+  ZPageAllocatorStats stats(ZGeneration* generation) const;\n@@ -123,3 +149,1 @@\n-  ZPage* alloc_page(uint8_t type, size_t size, ZAllocationFlags flags);\n-  void free_page(ZPage* page, bool reclaimed);\n-  void free_pages(const ZArray<ZPage*>* pages, bool reclaimed);\n+  void reset_statistics(ZGenerationId id);\n@@ -127,2 +151,5 @@\n-  void enable_deferred_delete() const;\n-  void disable_deferred_delete() const;\n+  ZPage* alloc_page(ZPageType type, size_t size, ZAllocationFlags flags, ZPageAge age);\n+  void recycle_page(ZPage* page);\n+  void safe_destroy_page(ZPage* page);\n+  void free_page(ZPage* page);\n+  void free_pages(const ZArray<ZPage*>* pages);\n@@ -130,2 +157,2 @@\n-  void debug_map_page(const ZPage* page) const;\n-  void debug_unmap_page(const ZPage* page) const;\n+  void enable_safe_destroy() const;\n+  void disable_safe_destroy() const;\n@@ -133,2 +160,2 @@\n-  bool has_alloc_stalled() const;\n-  void check_out_of_memory();\n+  void enable_safe_recycle() const;\n+  void disable_safe_recycle() const;\n@@ -136,1 +163,4 @@\n-  void pages_do(ZPageClosure* cl) const;\n+  bool is_alloc_stalling() const;\n+  bool is_alloc_stalling_for_old() const;\n+  void handle_alloc_stalling_for_young();\n+  void handle_alloc_stalling_for_old();\n@@ -146,1 +176,0 @@\n-  size_t _current_max_capacity;\n@@ -151,1 +180,5 @@\n-  size_t _reclaimed;\n+  size_t _used_generation;\n+  size_t _freed;\n+  size_t _promoted;\n+  size_t _compacted;\n+  size_t _allocation_stalls;\n@@ -161,1 +194,5 @@\n-                      size_t reclaimed);\n+                      size_t used_generation,\n+                      size_t freed,\n+                      size_t promoted,\n+                      size_t compacted,\n+                      size_t allocation_stalls);\n@@ -170,1 +207,5 @@\n-  size_t reclaimed() const;\n+  size_t used_generation() const;\n+  size_t freed() const;\n+  size_t promoted() const;\n+  size_t compacted() const;\n+  size_t allocation_stalls() const;\n","filename":"src\/hotspot\/share\/gc\/z\/zPageAllocator.hpp","additions":74,"deletions":33,"binary":false,"changes":107,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -36,1 +36,5 @@\n-                                                size_t reclaimed) :\n+                                                size_t used_generation,\n+                                                size_t freed,\n+                                                size_t promoted,\n+                                                size_t compacted,\n+                                                size_t allocation_stalls) :\n@@ -44,1 +48,5 @@\n-    _reclaimed(reclaimed) {}\n+    _used_generation(used_generation),\n+    _freed(freed),\n+    _promoted(promoted),\n+    _compacted(compacted),\n+    _allocation_stalls(allocation_stalls) {}\n@@ -74,2 +82,18 @@\n-inline size_t ZPageAllocatorStats::reclaimed() const {\n-  return _reclaimed;\n+inline size_t ZPageAllocatorStats::used_generation() const {\n+  return _used_generation;\n+}\n+\n+inline size_t ZPageAllocatorStats::freed() const {\n+  return _freed;\n+}\n+\n+inline size_t ZPageAllocatorStats::promoted() const {\n+  return _promoted;\n+}\n+\n+inline size_t ZPageAllocatorStats::compacted() const {\n+  return _compacted;\n+}\n+\n+inline size_t ZPageAllocatorStats::allocation_stalls() const {\n+  return _allocation_stalls;\n","filename":"src\/hotspot\/share\/gc\/z\/zPageAllocator.inline.hpp","additions":29,"deletions":5,"binary":false,"changes":34,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -69,1 +69,1 @@\n-  if (l1_page != NULL) {\n+  if (l1_page != nullptr) {\n@@ -83,1 +83,1 @@\n-    if (l2_page != NULL) {\n+    if (l2_page != nullptr) {\n@@ -91,1 +91,1 @@\n-  return NULL;\n+  return nullptr;\n@@ -96,1 +96,1 @@\n-  if (page != NULL) {\n+  if (page != nullptr) {\n@@ -101,1 +101,1 @@\n-  return NULL;\n+  return nullptr;\n@@ -116,1 +116,1 @@\n-  return NULL;\n+  return nullptr;\n@@ -124,1 +124,1 @@\n-  return NULL;\n+  return nullptr;\n@@ -138,1 +138,1 @@\n-  return NULL;\n+  return nullptr;\n@@ -143,1 +143,1 @@\n-  if (page == NULL) {\n+  if (page == nullptr) {\n@@ -147,1 +147,1 @@\n-  if (page != NULL) {\n+  if (page != nullptr) {\n@@ -154,1 +154,1 @@\n-ZPage* ZPageCache::alloc_page(uint8_t type, size_t size) {\n+ZPage* ZPageCache::alloc_page(ZPageType type, size_t size) {\n@@ -158,1 +158,1 @@\n-  if (type == ZPageTypeSmall) {\n+  if (type == ZPageType::small) {\n@@ -160,1 +160,1 @@\n-  } else if (type == ZPageTypeMedium) {\n+  } else if (type == ZPageType::medium) {\n@@ -166,1 +166,1 @@\n-  if (page == NULL) {\n+  if (page == nullptr) {\n@@ -169,1 +169,1 @@\n-    if (oversized != NULL) {\n+    if (oversized != nullptr) {\n@@ -183,1 +183,1 @@\n-  if (page == NULL) {\n+  if (page == nullptr) {\n@@ -191,2 +191,2 @@\n-  const uint8_t type = page->type();\n-  if (type == ZPageTypeSmall) {\n+  const ZPageType type = page->type();\n+  if (type == ZPageType::small) {\n@@ -194,1 +194,1 @@\n-  } else if (type == ZPageTypeMedium) {\n+  } else if (type == ZPageType::medium) {\n@@ -203,1 +203,1 @@\n-  if (page == NULL || !cl->do_page(page)) {\n+  if (page == nullptr || !cl->do_page(page)) {\n@@ -225,1 +225,1 @@\n-    ZList<ZPage>* numa_list = from->addr(numa_next);\n+    ZList<ZPage>* const numa_list = from->addr(numa_next);\n@@ -334,23 +334,0 @@\n-\n-void ZPageCache::pages_do(ZPageClosure* cl) const {\n-  \/\/ Small\n-  ZPerNUMAConstIterator<ZList<ZPage> > iter_numa(&_small);\n-  for (const ZList<ZPage>* list; iter_numa.next(&list);) {\n-    ZListIterator<ZPage> iter_small(list);\n-    for (ZPage* page; iter_small.next(&page);) {\n-      cl->do_page(page);\n-    }\n-  }\n-\n-  \/\/ Medium\n-  ZListIterator<ZPage> iter_medium(&_medium);\n-  for (ZPage* page; iter_medium.next(&page);) {\n-    cl->do_page(page);\n-  }\n-\n-  \/\/ Large\n-  ZListIterator<ZPage> iter_large(&_large);\n-  for (ZPage* page; iter_large.next(&page);) {\n-    cl->do_page(page);\n-  }\n-}\n","filename":"src\/hotspot\/share\/gc\/z\/zPageCache.cpp","additions":22,"deletions":45,"binary":false,"changes":67,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -29,0 +29,1 @@\n+#include \"gc\/z\/zPageType.hpp\"\n@@ -56,1 +57,1 @@\n-  ZPage* alloc_page(uint8_t type, size_t size);\n+  ZPage* alloc_page(ZPageType type, size_t size);\n@@ -63,2 +64,0 @@\n-\n-  void pages_do(ZPageClosure* cl) const;\n","filename":"src\/hotspot\/share\/gc\/z\/zPageCache.hpp","additions":3,"deletions":4,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -25,1 +25,1 @@\n-#include \"gc\/z\/zGlobals.hpp\"\n+#include \"gc\/z\/zAddress.hpp\"\n@@ -36,1 +36,1 @@\n-  const uintptr_t offset = page->start();\n+  const zoffset offset = page->start();\n@@ -43,1 +43,1 @@\n-  assert(_map.get(offset) == NULL, \"Invalid entry\");\n+  assert(_map.get(offset) == nullptr, \"Invalid entry\");\n@@ -45,0 +45,4 @@\n+\n+  if (page->is_old()) {\n+    ZGeneration::young()->register_with_remset(page);\n+  }\n@@ -48,1 +52,1 @@\n-  const uintptr_t offset = page->start();\n+  const zoffset offset = page->start();\n@@ -52,1 +56,39 @@\n-  _map.put(offset, size, NULL);\n+  _map.put(offset, size, nullptr);\n+}\n+\n+void ZPageTable::replace(ZPage* old_page, ZPage* new_page) {\n+  const zoffset offset = old_page->start();\n+  const size_t size = old_page->size();\n+\n+  assert(_map.get(offset) == old_page, \"Invalid entry\");\n+  _map.release_put(offset, size, new_page);\n+\n+  if (new_page->is_old()) {\n+    ZGeneration::young()->register_with_remset(new_page);\n+  }\n+}\n+\n+ZGenerationPagesParallelIterator::ZGenerationPagesParallelIterator(const ZPageTable* page_table, ZGenerationId id, ZPageAllocator* page_allocator) :\n+    _iterator(page_table),\n+    _generation_id(id),\n+    _page_allocator(page_allocator) {\n+  _page_allocator->enable_safe_destroy();\n+  _page_allocator->enable_safe_recycle();\n+}\n+\n+ZGenerationPagesParallelIterator::~ZGenerationPagesParallelIterator() {\n+  _page_allocator->disable_safe_recycle();\n+  _page_allocator->disable_safe_destroy();\n+}\n+\n+ZGenerationPagesIterator::ZGenerationPagesIterator(const ZPageTable* page_table, ZGenerationId id, ZPageAllocator* page_allocator) :\n+    _iterator(page_table),\n+    _generation_id(id),\n+    _page_allocator(page_allocator) {\n+  _page_allocator->enable_safe_destroy();\n+  _page_allocator->enable_safe_recycle();\n+}\n+\n+ZGenerationPagesIterator::~ZGenerationPagesIterator() {\n+  _page_allocator->disable_safe_recycle();\n+  _page_allocator->disable_safe_destroy();\n","filename":"src\/hotspot\/share\/gc\/z\/zPageTable.cpp","additions":48,"deletions":6,"binary":false,"changes":54,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,0 +27,1 @@\n+#include \"gc\/z\/zGenerationId.hpp\"\n@@ -28,0 +29,1 @@\n+#include \"gc\/z\/zIndexDistributor.hpp\"\n@@ -31,0 +33,2 @@\n+class ZPageAllocator;\n+class ZPageTable;\n@@ -33,1 +37,0 @@\n-  friend class VMStructs;\n@@ -35,0 +38,3 @@\n+  friend class ZPageTableParallelIterator;\n+  friend class ZRemsetTableIterator;\n+  friend class VMStructs;\n@@ -42,1 +48,4 @@\n-  ZPage* get(uintptr_t addr) const;\n+  ZPage* get(zaddress addr) const;\n+  ZPage* get(volatile zpointer* p) const;\n+\n+  ZPage* at(size_t index) const;\n@@ -46,0 +55,1 @@\n+  void replace(ZPage* old_page, ZPage* new_page);\n@@ -50,2 +60,25 @@\n-  ZGranuleMapIterator<ZPage*> _iter;\n-  ZPage*                      _prev;\n+  ZGranuleMapIterator<ZPage*, false \/* Parallel *\/> _iter;\n+  ZPage*                                            _prev;\n+\n+public:\n+  ZPageTableIterator(const ZPageTable* table);\n+\n+  bool next(ZPage** page);\n+};\n+\n+class ZPageTableParallelIterator : public StackObj {\n+  const ZPageTable* _table;\n+  ZIndexDistributor _index_distributor;\n+\n+public:\n+  ZPageTableParallelIterator(const ZPageTable* table);\n+\n+  template <typename Function>\n+  void do_pages(Function function);\n+};\n+\n+class ZGenerationPagesIterator : public StackObj {\n+private:\n+  ZPageTableIterator _iterator;\n+  ZGenerationId      _generation_id;\n+  ZPageAllocator*    _page_allocator;\n@@ -54,1 +87,2 @@\n-  ZPageTableIterator(const ZPageTable* page_table);\n+  ZGenerationPagesIterator(const ZPageTable* page_table, ZGenerationId id, ZPageAllocator* page_allocator);\n+  ~ZGenerationPagesIterator();\n@@ -57,0 +91,17 @@\n+\n+  template <typename Function>\n+  void yield(Function function);\n+};\n+\n+class ZGenerationPagesParallelIterator : public StackObj {\n+private:\n+  ZPageTableParallelIterator _iterator;\n+  ZGenerationId              _generation_id;\n+  ZPageAllocator*            _page_allocator;\n+\n+public:\n+  ZGenerationPagesParallelIterator(const ZPageTable* page_table, ZGenerationId id, ZPageAllocator* page_allocator);\n+  ~ZGenerationPagesParallelIterator();\n+\n+  template <typename Function>\n+  void do_pages(Function function);\n","filename":"src\/hotspot\/share\/gc\/z\/zPageTable.hpp","additions":57,"deletions":6,"binary":false,"changes":63,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -31,0 +31,3 @@\n+#include \"gc\/z\/zIndexDistributor.inline.hpp\"\n+#include \"gc\/z\/zPage.inline.hpp\"\n+#include \"gc\/z\/zPageAllocator.inline.hpp\"\n@@ -32,2 +35,2 @@\n-inline ZPage* ZPageTable::get(uintptr_t addr) const {\n-  assert(!ZAddress::is_null(addr), \"Invalid address\");\n+inline ZPage* ZPageTable::get(zaddress addr) const {\n+  assert(!is_null(addr), \"Invalid address\");\n@@ -37,3 +40,11 @@\n-inline ZPageTableIterator::ZPageTableIterator(const ZPageTable* page_table) :\n-    _iter(&page_table->_map),\n-    _prev(NULL) {}\n+inline ZPage* ZPageTable::get(volatile zpointer* p) const {\n+  return get(to_zaddress((uintptr_t)p));\n+}\n+\n+inline ZPage* ZPageTable::at(size_t index) const {\n+  return _map.at(index);\n+}\n+\n+inline ZPageTableIterator::ZPageTableIterator(const ZPageTable* table) :\n+    _iter(&table->_map),\n+    _prev(nullptr) {}\n@@ -43,1 +54,1 @@\n-    if (entry != NULL && entry != _prev) {\n+    if (entry != nullptr && entry != _prev) {\n@@ -54,0 +65,50 @@\n+inline ZPageTableParallelIterator::ZPageTableParallelIterator(const ZPageTable* table) :\n+    _table(table),\n+    _index_distributor(int(ZAddressOffsetMax >> ZGranuleSizeShift)) {}\n+\n+template <typename Function>\n+inline void ZPageTableParallelIterator::do_pages(Function function) {\n+  _index_distributor.do_indices([&](int index) {\n+    ZPage* const page = _table->at(index);\n+    if (page != nullptr) {\n+      const size_t start_index = untype(page->start()) >> ZGranuleSizeShift;\n+      if (size_t(index) == start_index) {\n+        \/\/ Next page found\n+        return function(page);\n+      }\n+    }\n+    return true;\n+  });\n+}\n+\n+inline bool ZGenerationPagesIterator::next(ZPage** page) {\n+  while (_iterator.next(page)) {\n+    if ((*page)->generation_id() == _generation_id) {\n+      return true;\n+    }\n+  }\n+\n+  return false;\n+}\n+\n+template <typename Function>\n+inline void ZGenerationPagesIterator::yield(Function function) {\n+  _page_allocator->disable_safe_destroy();\n+  _page_allocator->disable_safe_recycle();\n+\n+  function();\n+\n+  _page_allocator->enable_safe_recycle();\n+  _page_allocator->enable_safe_destroy();\n+}\n+\n+template <typename Function>\n+inline void ZGenerationPagesParallelIterator::do_pages(Function function) {\n+  _iterator.do_pages([&](ZPage* page) {\n+    if (page->generation_id() == _generation_id) {\n+      return function(page);\n+    }\n+    return true;\n+  });\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/z\/zPageTable.inline.hpp","additions":68,"deletions":7,"binary":false,"changes":75,"status":"modified"},{"patch":"@@ -0,0 +1,35 @@\n+\/*\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_Z_ZPAGETYPE_HPP\n+#define SHARE_GC_Z_ZPAGETYPE_HPP\n+\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+enum class ZPageType : uint8_t {\n+  small,\n+  medium,\n+  large\n+};\n+\n+#endif \/\/ SHARE_GC_Z_ZPAGETYPE_HPP\n","filename":"src\/hotspot\/share\/gc\/z\/zPageType.hpp","additions":35,"deletions":0,"binary":false,"changes":35,"status":"added"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -76,1 +76,1 @@\n-void ZPhysicalMemory::insert_segment(int index, uintptr_t start, size_t size, bool committed) {\n+void ZPhysicalMemory::insert_segment(int index, zoffset start, size_t size, bool committed) {\n@@ -80,1 +80,1 @@\n-void ZPhysicalMemory::replace_segment(int index, uintptr_t start, size_t size, bool committed) {\n+void ZPhysicalMemory::replace_segment(int index, zoffset start, size_t size, bool committed) {\n@@ -111,1 +111,1 @@\n-          const size_t start = _segments.at(current).start();\n+          const zoffset start = _segments.at(current).start();\n@@ -119,1 +119,1 @@\n-        const size_t start = _segments.at(current).start();\n+        const zoffset start = _segments.at(current).start();\n@@ -125,1 +125,1 @@\n-        const size_t start = segment.start();\n+        const zoffset start = segment.start();\n@@ -139,1 +139,1 @@\n-    const size_t start = segment.start();\n+    const zoffset start = segment.start();\n@@ -237,1 +237,1 @@\n-  _manager.free(0, max_capacity);\n+  _manager.free(zoffset(0), max_capacity);\n@@ -267,1 +267,1 @@\n-  ZPhysicalMemory pmem(ZPhysicalMemorySegment(0, ZGranuleSize, false \/* committed *\/));\n+  ZPhysicalMemory pmem(ZPhysicalMemorySegment(zoffset(0), ZGranuleSize, false \/* committed *\/));\n@@ -278,4 +278,3 @@\n-void ZPhysicalMemoryManager::nmt_commit(uintptr_t offset, size_t size) const {\n-  \/\/ From an NMT point of view we treat the first heap view (marked0) as committed\n-  const uintptr_t addr = ZAddress::marked0(offset);\n-  MemTracker::record_virtual_memory_commit((void*)addr, size, CALLER_PC);\n+void ZPhysicalMemoryManager::nmt_commit(zoffset offset, size_t size) const {\n+  const zaddress addr = ZOffset::address(offset);\n+  MemTracker::record_virtual_memory_commit((void*)untype(addr), size, CALLER_PC);\n@@ -284,1 +283,1 @@\n-void ZPhysicalMemoryManager::nmt_uncommit(uintptr_t offset, size_t size) const {\n+void ZPhysicalMemoryManager::nmt_uncommit(zoffset offset, size_t size) const {\n@@ -286,1 +285,1 @@\n-    const uintptr_t addr = ZAddress::marked0(offset);\n+    const zaddress addr = ZOffset::address(offset);\n@@ -288,1 +287,1 @@\n-    tracker.record((address)addr, size);\n+    tracker.record((address)untype(addr), size);\n@@ -298,2 +297,2 @@\n-    const uintptr_t start = _manager.alloc_low_address_at_most(size, &allocated);\n-    assert(start != UINTPTR_MAX, \"Allocation should never fail\");\n+    const zoffset start = _manager.alloc_low_address_at_most(size, &allocated);\n+    assert(start != zoffset(UINTPTR_MAX), \"Allocation should never fail\");\n@@ -355,1 +354,1 @@\n-void ZPhysicalMemoryManager::pretouch_view(uintptr_t addr, size_t size) const {\n+void ZPhysicalMemoryManager::pretouch_view(zaddress addr, size_t size) const {\n@@ -357,1 +356,1 @@\n-  os::pretouch_memory((void*)addr, (void*)(addr + size), page_size);\n+  os::pretouch_memory((void*)untype(addr), (void*)(untype(addr) + size), page_size);\n@@ -360,1 +359,1 @@\n-void ZPhysicalMemoryManager::map_view(uintptr_t addr, const ZPhysicalMemory& pmem) const {\n+void ZPhysicalMemoryManager::map_view(zaddress_unsafe addr, const ZPhysicalMemory& pmem) const {\n@@ -379,1 +378,1 @@\n-void ZPhysicalMemoryManager::unmap_view(uintptr_t addr, size_t size) const {\n+void ZPhysicalMemoryManager::unmap_view(zaddress_unsafe addr, size_t size) const {\n@@ -383,10 +382,3 @@\n-void ZPhysicalMemoryManager::pretouch(uintptr_t offset, size_t size) const {\n-  if (ZVerifyViews) {\n-    \/\/ Pre-touch good view\n-    pretouch_view(ZAddress::good(offset), size);\n-  } else {\n-    \/\/ Pre-touch all views\n-    pretouch_view(ZAddress::marked0(offset), size);\n-    pretouch_view(ZAddress::marked1(offset), size);\n-    pretouch_view(ZAddress::remapped(offset), size);\n-  }\n+void ZPhysicalMemoryManager::pretouch(zoffset offset, size_t size) const {\n+  \/\/ Pre-touch all views\n+  pretouch_view(ZOffset::address(offset), size);\n@@ -395,1 +387,1 @@\n-void ZPhysicalMemoryManager::map(uintptr_t offset, const ZPhysicalMemory& pmem) const {\n+void ZPhysicalMemoryManager::map(zoffset offset, const ZPhysicalMemory& pmem) const {\n@@ -398,9 +390,2 @@\n-  if (ZVerifyViews) {\n-    \/\/ Map good view\n-    map_view(ZAddress::good(offset), pmem);\n-  } else {\n-    \/\/ Map all views\n-    map_view(ZAddress::marked0(offset), pmem);\n-    map_view(ZAddress::marked1(offset), pmem);\n-    map_view(ZAddress::remapped(offset), pmem);\n-  }\n+  \/\/ Map all views\n+  map_view(ZOffset::address_unsafe(offset), pmem);\n@@ -411,1 +396,1 @@\n-void ZPhysicalMemoryManager::unmap(uintptr_t offset, size_t size) const {\n+void ZPhysicalMemoryManager::unmap(zoffset offset, size_t size) const {\n@@ -414,21 +399,2 @@\n-  if (ZVerifyViews) {\n-    \/\/ Unmap good view\n-    unmap_view(ZAddress::good(offset), size);\n-  } else {\n-    \/\/ Unmap all views\n-    unmap_view(ZAddress::marked0(offset), size);\n-    unmap_view(ZAddress::marked1(offset), size);\n-    unmap_view(ZAddress::remapped(offset), size);\n-  }\n-}\n-\n-void ZPhysicalMemoryManager::debug_map(uintptr_t offset, const ZPhysicalMemory& pmem) const {\n-  \/\/ Map good view\n-  assert(ZVerifyViews, \"Should be enabled\");\n-  map_view(ZAddress::good(offset), pmem);\n-}\n-\n-void ZPhysicalMemoryManager::debug_unmap(uintptr_t offset, size_t size) const {\n-  \/\/ Unmap good view\n-  assert(ZVerifyViews, \"Should be enabled\");\n-  unmap_view(ZAddress::good(offset), size);\n+  \/\/ Unmap all views\n+  unmap_view(ZOffset::address_unsafe(offset), size);\n","filename":"src\/hotspot\/share\/gc\/z\/zPhysicalMemory.cpp","additions":30,"deletions":64,"binary":false,"changes":94,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,0 +27,1 @@\n+#include \"gc\/z\/zAddress.hpp\"\n@@ -34,3 +35,3 @@\n-  uintptr_t _start;\n-  uintptr_t _end;\n-  bool      _committed;\n+  zoffset _start;\n+  zoffset _end;\n+  bool    _committed;\n@@ -40,1 +41,1 @@\n-  ZPhysicalMemorySegment(uintptr_t start, size_t size, bool committed);\n+  ZPhysicalMemorySegment(zoffset start, size_t size, bool committed);\n@@ -42,2 +43,2 @@\n-  uintptr_t start() const;\n-  uintptr_t end() const;\n+  zoffset start() const;\n+  zoffset end() const;\n@@ -54,2 +55,2 @@\n-  void insert_segment(int index, uintptr_t start, size_t size, bool committed);\n-  void replace_segment(int index, uintptr_t start, size_t size, bool committed);\n+  void insert_segment(int index, zoffset start, size_t size, bool committed);\n+  void replace_segment(int index, zoffset start, size_t size, bool committed);\n@@ -86,2 +87,2 @@\n-  void nmt_commit(uintptr_t offset, size_t size) const;\n-  void nmt_uncommit(uintptr_t offset, size_t size) const;\n+  void nmt_commit(zoffset offset, size_t size) const;\n+  void nmt_uncommit(zoffset offset, size_t size) const;\n@@ -89,3 +90,3 @@\n-  void pretouch_view(uintptr_t addr, size_t size) const;\n-  void map_view(uintptr_t addr, const ZPhysicalMemory& pmem) const;\n-  void unmap_view(uintptr_t addr, size_t size) const;\n+  void pretouch_view(zaddress addr, size_t size) const;\n+  void map_view(zaddress_unsafe addr, const ZPhysicalMemory& pmem) const;\n+  void unmap_view(zaddress_unsafe addr, size_t size) const;\n@@ -107,1 +108,1 @@\n-  void pretouch(uintptr_t offset, size_t size) const;\n+  void pretouch(zoffset offset, size_t size) const;\n@@ -109,5 +110,2 @@\n-  void map(uintptr_t offset, const ZPhysicalMemory& pmem) const;\n-  void unmap(uintptr_t offset, size_t size) const;\n-\n-  void debug_map(uintptr_t offset, const ZPhysicalMemory& pmem) const;\n-  void debug_unmap(uintptr_t offset, size_t size) const;\n+  void map(zoffset offset, const ZPhysicalMemory& pmem) const;\n+  void unmap(zoffset offset, size_t size) const;\n","filename":"src\/hotspot\/share\/gc\/z\/zPhysicalMemory.hpp","additions":18,"deletions":20,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -33,2 +33,2 @@\n-    _start(UINTPTR_MAX),\n-    _end(UINTPTR_MAX),\n+    _start(zoffset(UINTPTR_MAX)),\n+    _end(zoffset(UINTPTR_MAX)),\n@@ -37,1 +37,1 @@\n-inline ZPhysicalMemorySegment::ZPhysicalMemorySegment(uintptr_t start, size_t size, bool committed) :\n+inline ZPhysicalMemorySegment::ZPhysicalMemorySegment(zoffset start, size_t size, bool committed) :\n@@ -42,1 +42,1 @@\n-inline uintptr_t ZPhysicalMemorySegment::start() const {\n+inline zoffset ZPhysicalMemorySegment::start() const {\n@@ -46,1 +46,1 @@\n-inline uintptr_t ZPhysicalMemorySegment::end() const {\n+inline zoffset ZPhysicalMemorySegment::end() const {\n","filename":"src\/hotspot\/share\/gc\/z\/zPhysicalMemory.inline.hpp","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,0 +28,3 @@\n+#include \"gc\/shared\/suspendibleThreadSet.hpp\"\n+#include \"gc\/z\/zCollectedHeap.hpp\"\n+#include \"gc\/z\/zDriver.hpp\"\n@@ -35,0 +38,1 @@\n+#include \"oops\/access.inline.hpp\"\n@@ -39,2 +43,2 @@\n-static const ZStatSubPhase ZSubPhaseConcurrentReferencesProcess(\"Concurrent References Process\");\n-static const ZStatSubPhase ZSubPhaseConcurrentReferencesEnqueue(\"Concurrent References Enqueue\");\n+static const ZStatSubPhase ZSubPhaseConcurrentReferencesProcess(\"Concurrent References Process\", ZGenerationId::old);\n+static const ZStatSubPhase ZSubPhaseConcurrentReferencesEnqueue(\"Concurrent References Enqueue\", ZGenerationId::old);\n@@ -42,2 +46,2 @@\n-static ReferenceType reference_type(oop reference) {\n-  return InstanceKlass::cast(reference->klass())->reference_type();\n+static ReferenceType reference_type(zaddress reference) {\n+  return InstanceKlass::cast(to_oop(reference)->klass())->reference_type();\n@@ -66,2 +70,2 @@\n-static volatile oop* reference_referent_addr(oop reference) {\n-  return (volatile oop*)java_lang_ref_Reference::referent_addr_raw(reference);\n+static volatile zpointer* reference_referent_addr(zaddress reference) {\n+  return (volatile zpointer*)java_lang_ref_Reference::referent_addr_raw(to_oop(reference));\n@@ -70,2 +74,2 @@\n-static oop reference_referent(oop reference) {\n-  return Atomic::load(reference_referent_addr(reference));\n+static zpointer reference_referent(zaddress reference) {\n+  return ZBarrier::load_atomic(reference_referent_addr(reference));\n@@ -74,2 +78,2 @@\n-static void reference_clear_referent(oop reference) {\n-  java_lang_ref_Reference::clear_referent_raw(reference);\n+static zaddress reference_discovered(zaddress reference) {\n+  return to_zaddress(java_lang_ref_Reference::discovered(to_oop(reference)));\n@@ -78,2 +82,2 @@\n-static oop* reference_discovered_addr(oop reference) {\n-  return (oop*)java_lang_ref_Reference::discovered_addr_raw(reference);\n+static void reference_set_discovered(zaddress reference, zaddress discovered) {\n+  java_lang_ref_Reference::set_discovered(to_oop(reference), to_oop(discovered));\n@@ -82,2 +86,2 @@\n-static oop reference_discovered(oop reference) {\n-  return *reference_discovered_addr(reference);\n+static zaddress reference_next(zaddress reference) {\n+  return to_zaddress(java_lang_ref_Reference::next(to_oop(reference)));\n@@ -86,14 +90,2 @@\n-static void reference_set_discovered(oop reference, oop discovered) {\n-  java_lang_ref_Reference::set_discovered_raw(reference, discovered);\n-}\n-\n-static oop* reference_next_addr(oop reference) {\n-  return (oop*)java_lang_ref_Reference::next_addr_raw(reference);\n-}\n-\n-static oop reference_next(oop reference) {\n-  return *reference_next_addr(reference);\n-}\n-\n-static void reference_set_next(oop reference, oop next) {\n-  java_lang_ref_Reference::set_next_raw(reference, next);\n+static void reference_set_next(zaddress reference, zaddress next) {\n+  java_lang_ref_Reference::set_next(to_oop(reference), to_oop(next));\n@@ -103,0 +95,1 @@\n+  SuspendibleThreadSetJoiner sts_joiner;\n@@ -107,0 +100,13 @@\n+static void list_append(zaddress& head, zaddress& tail, zaddress reference) {\n+  if (is_null(head)) {\n+    \/\/ First append - set up the head\n+    head = reference;\n+  } else {\n+    \/\/ Not first append, link tail\n+    reference_set_discovered(tail, reference);\n+  }\n+\n+  \/\/ Always set tail\n+  tail = reference;\n+}\n+\n@@ -109,1 +115,1 @@\n-    _soft_reference_policy(NULL),\n+    _soft_reference_policy(nullptr),\n@@ -113,3 +119,3 @@\n-    _discovered_list(NULL),\n-    _pending_list(NULL),\n-    _pending_list_tail(_pending_list.addr()) {}\n+    _discovered_list(zaddress::null),\n+    _pending_list(zaddress::null),\n+    _pending_list_tail(zaddress::null) {}\n@@ -131,1 +137,1 @@\n-bool ZReferenceProcessor::is_inactive(oop reference, oop referent, ReferenceType type) const {\n+bool ZReferenceProcessor::is_inactive(zaddress reference, oop referent, ReferenceType type) const {\n@@ -135,1 +141,1 @@\n-    return reference_next(reference) != NULL;\n+    return !is_null(reference_next(reference));\n@@ -137,0 +143,3 @@\n+    \/\/ Verification\n+    (void)to_zaddress(referent);\n+\n@@ -139,1 +148,1 @@\n-    return referent == NULL;\n+    return referent == nullptr;\n@@ -144,1 +153,2 @@\n-  return ZHeap::heap()->is_object_strongly_live(ZOop::to_address(referent));\n+  const zaddress addr = to_zaddress(referent);\n+  return ZHeap::heap()->is_young(addr) || ZHeap::heap()->is_object_strongly_live(to_zaddress(referent));\n@@ -147,1 +157,1 @@\n-bool ZReferenceProcessor::is_softly_live(oop reference, ReferenceType type) const {\n+bool ZReferenceProcessor::is_softly_live(zaddress reference, ReferenceType type) const {\n@@ -156,2 +166,2 @@\n-  assert(_soft_reference_policy != NULL, \"Policy not initialized\");\n-  return !_soft_reference_policy->should_clear_reference(reference, clock);\n+  assert(_soft_reference_policy != nullptr, \"Policy not initialized\");\n+  return !_soft_reference_policy->should_clear_reference(to_oop(reference), clock);\n@@ -160,3 +170,3 @@\n-bool ZReferenceProcessor::should_discover(oop reference, ReferenceType type) const {\n-  volatile oop* const referent_addr = reference_referent_addr(reference);\n-  const oop referent = ZBarrier::weak_load_barrier_on_oop_field(referent_addr);\n+bool ZReferenceProcessor::should_discover(zaddress reference, ReferenceType type) const {\n+  volatile zpointer* const referent_addr = reference_referent_addr(reference);\n+  const oop referent = to_oop(ZBarrier::load_barrier_on_oop_field(referent_addr));\n@@ -168,0 +178,4 @@\n+  if (ZHeap::heap()->is_young(reference)) {\n+    return false;\n+  }\n+\n@@ -185,8 +199,2 @@\n-bool ZReferenceProcessor::should_drop(oop reference, ReferenceType type) const {\n-  const oop referent = reference_referent(reference);\n-  if (referent == NULL) {\n-    \/\/ Reference has been cleared, by a call to Reference.enqueue()\n-    \/\/ or Reference.clear() from the application, which means we\n-    \/\/ should drop the reference.\n-    return true;\n-  }\n+bool ZReferenceProcessor::try_make_inactive(zaddress reference, ReferenceType type) const {\n+  const zpointer referent = reference_referent(reference);\n@@ -194,6 +202,5 @@\n-  \/\/ Check if the referent is still alive, in which case we should\n-  \/\/ drop the reference.\n-  if (type == REF_PHANTOM) {\n-    return ZBarrier::is_alive_barrier_on_phantom_oop(referent);\n-  } else {\n-    return ZBarrier::is_alive_barrier_on_weak_oop(referent);\n+  if (is_null_any(referent)) {\n+    \/\/ Reference has already been cleared, by a call to Reference.enqueue()\n+    \/\/ or Reference.clear() from the application, which means it's already\n+    \/\/ inactive and we should drop the reference.\n+    return false;\n@@ -201,1 +208,0 @@\n-}\n@@ -203,4 +209,18 @@\n-void ZReferenceProcessor::keep_alive(oop reference, ReferenceType type) const {\n-  volatile oop* const p = reference_referent_addr(reference);\n-  if (type == REF_PHANTOM) {\n-    ZBarrier::keep_alive_barrier_on_phantom_oop_field(p);\n+  volatile zpointer* const referent_addr = reference_referent_addr(reference);\n+\n+  \/\/ Cleaning the referent will fail if the object it points to is\n+  \/\/ still alive, in which case we should drop the reference.\n+  if (type == REF_SOFT || type == REF_WEAK) {\n+    return ZBarrier::clean_barrier_on_weak_oop_field(referent_addr);\n+  } else if (type == REF_PHANTOM) {\n+    return ZBarrier::clean_barrier_on_phantom_oop_field(referent_addr);\n+  } else if (type == REF_FINAL) {\n+    if (ZBarrier::clean_barrier_on_final_oop_field(referent_addr)) {\n+      \/\/ The referent in a FinalReference will not be cleared, instead it is\n+      \/\/ made inactive by self-looping the next field. An application can't\n+      \/\/ call FinalReference.enqueue(), so there is no race to worry about\n+      \/\/ when setting the next field.\n+      assert(is_null(reference_next(reference)), \"Already inactive\");\n+      reference_set_next(reference, reference);\n+      return true;\n+    }\n@@ -208,1 +228,1 @@\n-    ZBarrier::keep_alive_barrier_on_weak_oop_field(p);\n+    fatal(\"Invalid referent type %d\", type);\n@@ -210,1 +230,0 @@\n-}\n@@ -212,12 +231,1 @@\n-void ZReferenceProcessor::make_inactive(oop reference, ReferenceType type) const {\n-  if (type == REF_FINAL) {\n-    \/\/ Don't clear referent. It is needed by the Finalizer thread to make the call\n-    \/\/ to finalize(). A FinalReference is instead made inactive by self-looping the\n-    \/\/ next field. An application can't call FinalReference.enqueue(), so there is\n-    \/\/ no race to worry about when setting the next field.\n-    assert(reference_next(reference) == NULL, \"Already inactive\");\n-    reference_set_next(reference, reference);\n-  } else {\n-    \/\/ Clear referent\n-    reference_clear_referent(reference);\n-  }\n+  return false;\n@@ -226,2 +234,2 @@\n-void ZReferenceProcessor::discover(oop reference, ReferenceType type) {\n-  log_trace(gc, ref)(\"Discovered Reference: \" PTR_FORMAT \" (%s)\", p2i(reference), reference_type_name(type));\n+void ZReferenceProcessor::discover(zaddress reference, ReferenceType type) {\n+  log_trace(gc, ref)(\"Discovered Reference: \" PTR_FORMAT \" (%s)\", untype(reference), reference_type_name(type));\n@@ -236,2 +244,2 @@\n-    volatile oop* const referent_addr = reference_referent_addr(reference);\n-    ZBarrier::mark_barrier_on_oop_field(referent_addr, true \/* finalizable *\/);\n+    volatile zpointer* const referent_addr = reference_referent_addr(reference);\n+    ZBarrier::mark_barrier_on_old_oop_field(referent_addr, true \/* finalizable *\/);\n@@ -241,2 +249,3 @@\n-  assert(reference_discovered(reference) == NULL, \"Already discovered\");\n-  oop* const list = _discovered_list.addr();\n+  assert(ZHeap::heap()->is_old(reference), \"Must be old\");\n+  assert(is_null(reference_discovered(reference)), \"Already discovered\");\n+  zaddress* const list = _discovered_list.addr();\n@@ -247,1 +256,1 @@\n-bool ZReferenceProcessor::discover_reference(oop reference, ReferenceType type) {\n+bool ZReferenceProcessor::discover_reference(oop reference_obj, ReferenceType type) {\n@@ -253,1 +262,3 @@\n-  log_trace(gc, ref)(\"Encountered Reference: \" PTR_FORMAT \" (%s)\", p2i(reference), reference_type_name(type));\n+  log_trace(gc, ref)(\"Encountered Reference: \" PTR_FORMAT \" (%s)\", p2i(reference_obj), reference_type_name(type));\n+\n+  const zaddress reference = to_zaddress(reference_obj);\n@@ -269,2 +280,3 @@\n-oop ZReferenceProcessor::drop(oop reference, ReferenceType type) {\n-  log_trace(gc, ref)(\"Dropped Reference: \" PTR_FORMAT \" (%s)\", p2i(reference), reference_type_name(type));\n+void ZReferenceProcessor::process_worker_discovered_list(zaddress discovered_list) {\n+  zaddress keep_head = zaddress::null;\n+  zaddress keep_tail = zaddress::null;\n@@ -272,2 +284,4 @@\n-  \/\/ Keep referent alive\n-  keep_alive(reference, type);\n+  \/\/ Iterate over the discovered list and unlink them as we go, potentially\n+  \/\/ appending them to the keep list\n+  for (zaddress reference = discovered_list; !is_null(reference); ) {\n+    assert(ZHeap::heap()->is_old(reference), \"Must be old\");\n@@ -275,5 +289,3 @@\n-  \/\/ Unlink and return next in list\n-  const oop next = reference_discovered(reference);\n-  reference_set_discovered(reference, NULL);\n-  return next;\n-}\n+    const ReferenceType type = reference_type(reference);\n+    const zaddress next = reference_discovered(reference);\n+    reference_set_discovered(reference, zaddress::null);\n@@ -281,2 +293,3 @@\n-oop* ZReferenceProcessor::keep(oop reference, ReferenceType type) {\n-  log_trace(gc, ref)(\"Enqueued Reference: \" PTR_FORMAT \" (%s)\", p2i(reference), reference_type_name(type));\n+    if (try_make_inactive(reference, type)) {\n+      \/\/ Keep reference\n+      log_trace(gc, ref)(\"Enqueued Reference: \" PTR_FORMAT \" (%s)\", untype(reference), reference_type_name(type));\n@@ -284,2 +297,2 @@\n-  \/\/ Update statistics\n-  _enqueued_count.get()[type]++;\n+      \/\/ Update statistics\n+      _enqueued_count.get()[type]++;\n@@ -287,2 +300,5 @@\n-  \/\/ Make reference inactive\n-  make_inactive(reference, type);\n+      list_append(keep_head, keep_tail, reference);\n+    } else {\n+      \/\/ Drop reference\n+      log_trace(gc, ref)(\"Dropped Reference: \" PTR_FORMAT \" (%s)\", untype(reference), reference_type_name(type));\n+    }\n@@ -290,3 +306,3 @@\n-  \/\/ Return next in list\n-  return reference_discovered_addr(reference);\n-}\n+    reference = next;\n+    SuspendibleThreadSet::yield();\n+  }\n@@ -294,4 +310,1 @@\n-void ZReferenceProcessor::work() {\n-  \/\/ Process discovered references\n-  oop* const list = _discovered_list.addr();\n-  oop* p = list;\n+  \/\/ Prepend discovered references to internal pending list\n@@ -299,3 +312,6 @@\n-  while (*p != NULL) {\n-    const oop reference = *p;\n-    const ReferenceType type = reference_type(reference);\n+  \/\/ Anything kept on the list?\n+  if (!is_null(keep_head)) {\n+    const zaddress old_pending_list = Atomic::xchg(_pending_list.addr(), keep_head);\n+\n+    \/\/ Concatenate the old list\n+    reference_set_discovered(keep_tail, old_pending_list);\n@@ -303,2 +319,3 @@\n-    if (should_drop(reference, type)) {\n-      *p = drop(reference, type);\n+    if (is_null(old_pending_list)) {\n+      \/\/ Old list was empty. First to prepend to list, record tail\n+      _pending_list_tail = keep_tail;\n@@ -306,1 +323,1 @@\n-      p = keep(reference, type);\n+      assert(ZHeap::heap()->is_old(old_pending_list), \"Must be old\");\n@@ -309,0 +326,1 @@\n+}\n@@ -310,7 +328,2 @@\n-  \/\/ Prepend discovered references to internal pending list\n-  if (*list != NULL) {\n-    *p = Atomic::xchg(_pending_list.addr(), *list);\n-    if (*p == NULL) {\n-      \/\/ First to prepend to list, record tail\n-      _pending_list_tail = p;\n-    }\n+void ZReferenceProcessor::work() {\n+  SuspendibleThreadSetJoiner sts_joiner;\n@@ -318,4 +331,3 @@\n-    \/\/ Clear discovered list\n-    *list = NULL;\n-  }\n-}\n+  ZPerWorkerIterator<zaddress> iter(&_discovered_list);\n+  for (zaddress* start; iter.next(&start);) {\n+    const zaddress discovered_list = Atomic::xchg(start, zaddress::null);\n@@ -323,5 +335,3 @@\n-bool ZReferenceProcessor::is_empty() const {\n-  ZPerWorkerConstIterator<oop> iter(&_discovered_list);\n-  for (const oop* list; iter.next(&list);) {\n-    if (*list != NULL) {\n-      return false;\n+    if (discovered_list != zaddress::null) {\n+      \/\/ Process discovered references\n+      process_worker_discovered_list(discovered_list);\n@@ -330,0 +340,1 @@\n+}\n@@ -331,2 +342,5 @@\n-  if (_pending_list.get() != NULL) {\n-    return false;\n+void ZReferenceProcessor::verify_empty() const {\n+#ifdef ASSERT\n+  ZPerWorkerConstIterator<zaddress> iter(&_discovered_list);\n+  for (const zaddress* list; iter.next(&list);) {\n+    assert(is_null(*list), \"Discovered list not empty\");\n@@ -335,1 +349,2 @@\n-  return true;\n+  assert(is_null(_pending_list.get()), \"Pending list not empty\");\n+#endif\n@@ -339,1 +354,1 @@\n-  assert(is_empty(), \"Should be empty\");\n+  verify_empty();\n@@ -406,1 +421,1 @@\n-  ZTracer::tracer()->report_gc_reference_stats(stats);\n+  ZDriver::major()->jfr_tracer()->report_gc_reference_stats(stats);\n@@ -424,1 +439,1 @@\n-  ZStatTimer timer(ZSubPhaseConcurrentReferencesProcess);\n+  ZStatTimerOld timer(ZSubPhaseConcurrentReferencesProcess);\n@@ -437,0 +452,29 @@\n+void ZReferenceProcessor::verify_pending_references() {\n+#ifdef ASSERT\n+  SuspendibleThreadSetJoiner sts_joiner;\n+\n+  assert(!is_null(_pending_list.get()), \"Should not contain colored null\");\n+\n+  for (zaddress current = _pending_list.get();\n+       !is_null(current);\n+       current = reference_discovered(current))\n+  {\n+    volatile zpointer* const referent_addr = reference_referent_addr(current);\n+    const oop referent = to_oop(ZBarrier::load_barrier_on_oop_field(referent_addr));\n+    const ReferenceType type = reference_type(current);\n+    assert(ZReferenceProcessor::is_inactive(current, referent, type), \"invariant\");\n+    if (type == REF_FINAL) {\n+      assert(ZPointer::is_marked_any_old(ZBarrier::load_atomic(referent_addr)), \"invariant\");\n+    }\n+\n+    SuspendibleThreadSet::yield();\n+  }\n+#endif\n+}\n+\n+zaddress ZReferenceProcessor::swap_pending_list(zaddress pending_list) {\n+  const oop pending_list_oop = to_oop(pending_list);\n+  const oop prev = Universe::swap_reference_pending_list(pending_list_oop);\n+  return to_zaddress(prev);\n+}\n+\n@@ -438,1 +482,1 @@\n-  ZStatTimer timer(ZSubPhaseConcurrentReferencesEnqueue);\n+  ZStatTimerOld timer(ZSubPhaseConcurrentReferencesEnqueue);\n@@ -440,1 +484,1 @@\n-  if (_pending_list.get() == NULL) {\n+  if (is_null(_pending_list.get())) {\n@@ -445,0 +489,3 @@\n+  \/\/ Verify references on internal pending list\n+  verify_pending_references();\n+\n@@ -448,0 +495,3 @@\n+    SuspendibleThreadSetJoiner sts_joiner;\n+\n+    const zaddress prev_list = swap_pending_list(_pending_list.get());\n@@ -449,2 +499,2 @@\n-    \/\/ Prepend internal pending list to external pending list\n-    *_pending_list_tail = Universe::swap_reference_pending_list(_pending_list.get());\n+    \/\/ Link together new and old list\n+    reference_set_discovered(_pending_list_tail, prev_list);\n@@ -457,2 +507,2 @@\n-  _pending_list.set(NULL);\n-  _pending_list_tail = _pending_list.addr();\n+  _pending_list.set(zaddress::null);\n+  _pending_list_tail = zaddress::null;\n","filename":"src\/hotspot\/share\/gc\/z\/zReferenceProcessor.cpp","additions":190,"deletions":140,"binary":false,"changes":330,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,0 +28,1 @@\n+#include \"gc\/z\/zAddress.hpp\"\n@@ -30,0 +31,1 @@\n+class ConcurrentGCTimer;\n@@ -45,3 +47,3 @@\n-  ZPerWorker<oop>      _discovered_list;\n-  ZContended<oop>      _pending_list;\n-  oop*                 _pending_list_tail;\n+  ZPerWorker<zaddress> _discovered_list;\n+  ZContended<zaddress> _pending_list;\n+  zaddress             _pending_list_tail;\n@@ -49,1 +51,1 @@\n-  bool is_inactive(oop reference, oop referent, ReferenceType type) const;\n+  bool is_inactive(zaddress reference, oop referent, ReferenceType type) const;\n@@ -51,1 +53,1 @@\n-  bool is_softly_live(oop reference, ReferenceType type) const;\n+  bool is_softly_live(zaddress reference, ReferenceType type) const;\n@@ -53,4 +55,2 @@\n-  bool should_discover(oop reference, ReferenceType type) const;\n-  bool should_drop(oop reference, ReferenceType type) const;\n-  void keep_alive(oop reference, ReferenceType type) const;\n-  void make_inactive(oop reference, ReferenceType type) const;\n+  bool should_discover(zaddress reference, ReferenceType type) const;\n+  bool try_make_inactive(zaddress reference, ReferenceType type) const;\n@@ -58,1 +58,1 @@\n-  void discover(oop reference, ReferenceType type);\n+  void discover(zaddress reference, ReferenceType type);\n@@ -60,4 +60,1 @@\n-  oop drop(oop reference, ReferenceType type);\n-  oop* keep(oop reference, ReferenceType type);\n-\n-  bool is_empty() const;\n+  void verify_empty() const;\n@@ -65,0 +62,1 @@\n+  void process_worker_discovered_list(zaddress discovered_list);\n@@ -68,0 +66,2 @@\n+  zaddress swap_pending_list(zaddress pending_list);\n+\n@@ -77,0 +77,2 @@\n+\n+  void verify_pending_references();\n","filename":"src\/hotspot\/share\/gc\/z\/zReferenceProcessor.hpp","additions":17,"deletions":15,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -26,0 +26,1 @@\n+#include \"gc\/shared\/suspendibleThreadSet.hpp\"\n@@ -28,0 +29,1 @@\n+#include \"gc\/z\/zAllocator.inline.hpp\"\n@@ -29,0 +31,1 @@\n+#include \"gc\/z\/zCollectedHeap.hpp\"\n@@ -30,0 +33,1 @@\n+#include \"gc\/z\/zGeneration.inline.hpp\"\n@@ -31,0 +35,2 @@\n+#include \"gc\/z\/zIndexDistributor.inline.hpp\"\n+#include \"gc\/z\/zIterator.inline.hpp\"\n@@ -32,0 +38,1 @@\n+#include \"gc\/z\/zPageAge.hpp\"\n@@ -34,0 +41,2 @@\n+#include \"gc\/z\/zRootsIterator.hpp\"\n+#include \"gc\/z\/zStackWatermark.hpp\"\n@@ -36,1 +45,2 @@\n-#include \"gc\/z\/zThread.inline.hpp\"\n+#include \"gc\/z\/zUncoloredRoot.inline.hpp\"\n+#include \"gc\/z\/zVerify.hpp\"\n@@ -42,2 +52,2 @@\n-ZRelocate::ZRelocate(ZWorkers* workers) :\n-    _workers(workers) {}\n+static const ZStatCriticalPhase ZCriticalPhaseRelocationStall(\"Relocation Stall\");\n+static const ZStatSubPhase ZSubPhaseConcurrentRelocateRememberedSetFlipPromotedYoung(\"Concurrent Relocate Remset FP\", ZGenerationId::young);\n@@ -45,2 +55,1 @@\n-static uintptr_t forwarding_index(ZForwarding* forwarding, uintptr_t from_addr) {\n-  const uintptr_t from_offset = ZAddress::offset(from_addr);\n+static uintptr_t forwarding_index(ZForwarding* forwarding, zoffset from_offset) {\n@@ -50,2 +59,2 @@\n-static uintptr_t forwarding_find(ZForwarding* forwarding, uintptr_t from_addr, ZForwardingCursor* cursor) {\n-  const uintptr_t from_index = forwarding_index(forwarding, from_addr);\n+static zaddress forwarding_find(ZForwarding* forwarding, zoffset from_offset, ZForwardingCursor* cursor) {\n+  const uintptr_t from_index = forwarding_index(forwarding, from_offset);\n@@ -53,1 +62,1 @@\n-  return entry.populated() ? ZAddress::good(entry.to_offset()) : 0;\n+  return entry.populated() ? ZOffset::address(to_zoffset(entry.to_offset())) : zaddress::null;\n@@ -56,5 +65,2 @@\n-static uintptr_t forwarding_insert(ZForwarding* forwarding, uintptr_t from_addr, uintptr_t to_addr, ZForwardingCursor* cursor) {\n-  const uintptr_t from_index = forwarding_index(forwarding, from_addr);\n-  const uintptr_t to_offset = ZAddress::offset(to_addr);\n-  const uintptr_t to_offset_final = forwarding->insert(from_index, to_offset, cursor);\n-  return ZAddress::good(to_offset_final);\n+static zaddress forwarding_find(ZForwarding* forwarding, zaddress_unsafe from_addr, ZForwardingCursor* cursor) {\n+  return forwarding_find(forwarding, ZAddress::offset(from_addr), cursor);\n@@ -63,1 +69,269 @@\n-static uintptr_t relocate_object_inner(ZForwarding* forwarding, uintptr_t from_addr, ZForwardingCursor* cursor) {\n+static zaddress forwarding_find(ZForwarding* forwarding, zaddress from_addr, ZForwardingCursor* cursor) {\n+  return forwarding_find(forwarding, ZAddress::offset(from_addr), cursor);\n+}\n+\n+static zaddress forwarding_insert(ZForwarding* forwarding, zoffset from_offset, zaddress to_addr, ZForwardingCursor* cursor) {\n+  const uintptr_t from_index = forwarding_index(forwarding, from_offset);\n+  const zoffset to_offset = ZAddress::offset(to_addr);\n+  const zoffset to_offset_final = forwarding->insert(from_index, to_offset, cursor);\n+  return ZOffset::address(to_offset_final);\n+}\n+\n+static zaddress forwarding_insert(ZForwarding* forwarding, zaddress from_addr, zaddress to_addr, ZForwardingCursor* cursor) {\n+  return forwarding_insert(forwarding, ZAddress::offset(from_addr), to_addr, cursor);\n+}\n+\n+ZRelocateQueue::ZRelocateQueue() :\n+    _lock(),\n+    _queue(),\n+    _nworkers(0),\n+    _nsynchronized(0),\n+    _synchronize(false),\n+    _needs_attention(0) {}\n+\n+bool ZRelocateQueue::needs_attention() const {\n+  return Atomic::load(&_needs_attention) != 0;\n+}\n+\n+void ZRelocateQueue::inc_needs_attention() {\n+  const int needs_attention = Atomic::add(&_needs_attention, 1);\n+  assert(needs_attention == 1 || needs_attention == 2, \"Invalid state\");\n+}\n+\n+void ZRelocateQueue::dec_needs_attention() {\n+  const int needs_attention = Atomic::sub(&_needs_attention, 1);\n+  assert(needs_attention == 0 || needs_attention == 1, \"Invalid state\");\n+}\n+\n+void ZRelocateQueue::join(uint nworkers) {\n+  assert(nworkers != 0, \"Must request at least one worker\");\n+  assert(_nworkers == 0, \"Invalid state\");\n+  assert(_nsynchronized == 0, \"Invalid state\");\n+\n+  log_debug(gc, reloc)(\"Joining workers: %u\", nworkers);\n+\n+  _nworkers = nworkers;\n+}\n+\n+void ZRelocateQueue::resize_workers(uint nworkers) {\n+  assert(nworkers != 0, \"Must request at least one worker\");\n+  assert(_nworkers == 0, \"Invalid state\");\n+  assert(_nsynchronized == 0, \"Invalid state\");\n+\n+  log_debug(gc, reloc)(\"Resize workers: %u\", nworkers);\n+\n+  ZLocker<ZConditionLock> locker(&_lock);\n+  _nworkers = nworkers;\n+}\n+\n+void ZRelocateQueue::leave() {\n+  ZLocker<ZConditionLock> locker(&_lock);\n+  _nworkers--;\n+\n+  assert(_nsynchronized <= _nworkers, \"_nsynchronized: %u _nworkers: %u\", _nsynchronized, _nworkers);\n+\n+  log_debug(gc, reloc)(\"Leaving workers: left: %u _synchronize: %d _nsynchronized: %u\", _nworkers, _synchronize, _nsynchronized);\n+\n+  \/\/ Prune done forwardings\n+  const bool forwardings_done = prune();\n+\n+  \/\/ Check if all workers synchronized\n+  const bool last_synchronized = _synchronize && _nworkers == _nsynchronized;\n+\n+  if (forwardings_done || last_synchronized) {\n+    _lock.notify_all();\n+  }\n+}\n+\n+void ZRelocateQueue::add_and_wait(ZForwarding* forwarding) {\n+  ZStatTimer timer(ZCriticalPhaseRelocationStall);\n+  ZLocker<ZConditionLock> locker(&_lock);\n+\n+  if (forwarding->is_done()) {\n+    return;\n+  }\n+\n+  _queue.append(forwarding);\n+  if (_queue.length() == 1) {\n+    \/\/ Queue became non-empty\n+    inc_needs_attention();\n+    _lock.notify_all();\n+  }\n+\n+  while (!forwarding->is_done()) {\n+    _lock.wait();\n+  }\n+}\n+\n+bool ZRelocateQueue::prune() {\n+  if (_queue.is_empty()) {\n+    return false;\n+  }\n+\n+  bool done = false;\n+\n+  for (int i = 0; i < _queue.length();) {\n+    const ZForwarding* const forwarding = _queue.at(i);\n+    if (forwarding->is_done()) {\n+      done = true;\n+\n+      _queue.delete_at(i);\n+    } else {\n+      i++;\n+    }\n+  }\n+\n+  if (_queue.is_empty()) {\n+    dec_needs_attention();\n+  }\n+\n+  return done;\n+}\n+\n+ZForwarding* ZRelocateQueue::prune_and_claim() {\n+  if (prune()) {\n+    _lock.notify_all();\n+  }\n+\n+  for (int i = 0; i < _queue.length(); i++) {\n+    ZForwarding* const forwarding = _queue.at(i);\n+    if (forwarding->claim()) {\n+      return forwarding;\n+    }\n+  }\n+\n+  return nullptr;\n+}\n+\n+class ZRelocateQueueSynchronizeThread {\n+private:\n+  ZRelocateQueue* const _queue;\n+\n+public:\n+  ZRelocateQueueSynchronizeThread(ZRelocateQueue* queue) :\n+      _queue(queue) {\n+    _queue->synchronize_thread();\n+  }\n+\n+  ~ZRelocateQueueSynchronizeThread() {\n+    _queue->desynchronize_thread();\n+  }\n+};\n+\n+void ZRelocateQueue::synchronize_thread() {\n+  _nsynchronized++;\n+\n+  log_debug(gc, reloc)(\"Synchronize worker _nsynchronized %u\", _nsynchronized);\n+\n+  assert(_nsynchronized <= _nworkers, \"_nsynchronized: %u _nworkers: %u\", _nsynchronized, _nworkers);\n+  if (_nsynchronized == _nworkers) {\n+    \/\/ All workers synchronized\n+    _lock.notify_all();\n+  }\n+}\n+\n+void ZRelocateQueue::desynchronize_thread() {\n+  _nsynchronized--;\n+\n+  log_debug(gc, reloc)(\"Desynchronize worker _nsynchronized %u\", _nsynchronized);\n+\n+  assert(_nsynchronized < _nworkers, \"_nsynchronized: %u _nworkers: %u\", _nsynchronized, _nworkers);\n+}\n+\n+ZForwarding* ZRelocateQueue::synchronize_poll() {\n+  \/\/ Fast path avoids locking\n+  if (!needs_attention()) {\n+    return nullptr;\n+  }\n+\n+  \/\/ Slow path to get the next forwarding and\/or synchronize\n+  ZLocker<ZConditionLock> locker(&_lock);\n+\n+  {\n+    ZForwarding* const forwarding = prune_and_claim();\n+    if (forwarding != nullptr) {\n+      \/\/ Don't become synchronized while there are elements in the queue\n+      return forwarding;\n+    }\n+  }\n+\n+  if (!_synchronize) {\n+    return nullptr;\n+  }\n+\n+  ZRelocateQueueSynchronizeThread rqst(this);\n+\n+  do {\n+    _lock.wait();\n+\n+    ZForwarding* const forwarding = prune_and_claim();\n+    if (forwarding != nullptr) {\n+      return forwarding;\n+    }\n+  } while (_synchronize);\n+\n+  return nullptr;\n+}\n+\n+void ZRelocateQueue::clear() {\n+  assert(_nworkers == 0, \"Invalid state\");\n+\n+  if (_queue.is_empty()) {\n+    return;\n+  }\n+\n+  ZArrayIterator<ZForwarding*> iter(&_queue);\n+  for (ZForwarding* forwarding; iter.next(&forwarding);) {\n+    assert(forwarding->is_done(), \"All should be done\");\n+  }\n+\n+  assert(false, \"Clear was not empty\");\n+\n+  _queue.clear();\n+  dec_needs_attention();\n+}\n+\n+void ZRelocateQueue::synchronize() {\n+  ZLocker<ZConditionLock> locker(&_lock);\n+  _synchronize = true;\n+\n+  inc_needs_attention();\n+\n+  log_debug(gc, reloc)(\"Synchronize all workers 1 _nworkers: %u _nsynchronized: %u\", _nworkers, _nsynchronized);\n+\n+  while (_nworkers != _nsynchronized) {\n+    _lock.wait();\n+    log_debug(gc, reloc)(\"Synchronize all workers 2 _nworkers: %u _nsynchronized: %u\", _nworkers, _nsynchronized);\n+  }\n+}\n+\n+void ZRelocateQueue::desynchronize() {\n+  ZLocker<ZConditionLock> locker(&_lock);\n+  _synchronize = false;\n+\n+  log_debug(gc, reloc)(\"Desynchronize all workers _nworkers: %u _nsynchronized: %u\", _nworkers, _nsynchronized);\n+\n+  assert(_nsynchronized <= _nworkers, \"_nsynchronized: %u _nworkers: %u\", _nsynchronized, _nworkers);\n+\n+  dec_needs_attention();\n+\n+  _lock.notify_all();\n+}\n+\n+ZRelocate::ZRelocate(ZGeneration* generation) :\n+    _generation(generation),\n+    _queue() {}\n+\n+ZWorkers* ZRelocate::workers() const {\n+  return _generation->workers();\n+}\n+\n+void ZRelocate::start() {\n+  _queue.join(workers()->active_workers());\n+}\n+\n+void ZRelocate::add_remset(volatile zpointer* p) {\n+  ZGeneration::young()->remember(p);\n+}\n+\n+static zaddress relocate_object_inner(ZForwarding* forwarding, zaddress from_addr, ZForwardingCursor* cursor) {\n@@ -68,2 +342,6 @@\n-  const uintptr_t to_addr = ZHeap::heap()->alloc_object_for_relocation(size);\n-  if (to_addr == 0) {\n+\n+  ZAllocatorForRelocation* allocator = ZAllocator::relocation(forwarding->to_age());\n+\n+  const zaddress to_addr = allocator->alloc_object(size);\n+\n+  if (is_null(to_addr)) {\n@@ -71,1 +349,1 @@\n-    return 0;\n+    return zaddress::null;\n@@ -78,1 +356,2 @@\n-  const uintptr_t to_addr_final = forwarding_insert(forwarding, from_addr, to_addr, cursor);\n+  const zaddress to_addr_final = forwarding_insert(forwarding, from_addr, to_addr, cursor);\n+\n@@ -81,1 +360,1 @@\n-    ZHeap::heap()->undo_alloc_object_for_relocation(to_addr, size);\n+    allocator->undo_alloc_object(to_addr, size);\n@@ -87,1 +366,1 @@\n-uintptr_t ZRelocate::relocate_object(ZForwarding* forwarding, uintptr_t from_addr) const {\n+zaddress ZRelocate::relocate_object(ZForwarding* forwarding, zaddress_unsafe from_addr) {\n@@ -91,2 +370,2 @@\n-  uintptr_t to_addr = forwarding_find(forwarding, from_addr, &cursor);\n-  if (to_addr != 0) {\n+  zaddress to_addr = forwarding_find(forwarding, from_addr, &cursor);\n+  if (!is_null(to_addr)) {\n@@ -98,2 +377,3 @@\n-  if (forwarding->retain_page()) {\n-    to_addr = relocate_object_inner(forwarding, from_addr, &cursor);\n+  if (forwarding->retain_page(&_queue)) {\n+    assert(_generation->is_phase_relocate(), \"Must be\");\n+    to_addr = relocate_object_inner(forwarding, safe(from_addr), &cursor);\n@@ -102,1 +382,1 @@\n-    if (to_addr != 0) {\n+    if (!is_null(to_addr)) {\n@@ -107,8 +387,3 @@\n-    \/\/ Failed to relocate object. Wait for a worker thread to complete\n-    \/\/ relocation of this page, and then forward the object. If the GC\n-    \/\/ aborts the relocation phase before the page has been relocated,\n-    \/\/ then wait return false and we just forward the object in-place.\n-    if (!forwarding->wait_page_released()) {\n-      \/\/ Forward object in-place\n-      return forwarding_insert(forwarding, from_addr, from_addr, &cursor);\n-    }\n+    \/\/ Failed to relocate object. Signal and wait for a worker thread to\n+    \/\/ complete relocation of this page, and then forward the object.\n+    _queue.add_and_wait(forwarding);\n@@ -121,1 +396,1 @@\n-uintptr_t ZRelocate::forward_object(ZForwarding* forwarding, uintptr_t from_addr) const {\n+zaddress ZRelocate::forward_object(ZForwarding* forwarding, zaddress_unsafe from_addr) {\n@@ -123,2 +398,2 @@\n-  const uintptr_t to_addr = forwarding_find(forwarding, from_addr, &cursor);\n-  assert(to_addr != 0, \"Should be forwarded\");\n+  const zaddress to_addr = forwarding_find(forwarding, from_addr, &cursor);\n+  assert(!is_null(to_addr), \"Should be forwarded: \" PTR_FORMAT, untype(from_addr));\n@@ -128,1 +403,1 @@\n-static ZPage* alloc_page(const ZForwarding* forwarding) {\n+static ZPage* alloc_page(ZAllocatorForRelocation* allocator, ZPageType type, size_t size) {\n@@ -132,1 +407,1 @@\n-    return NULL;\n+    return nullptr;\n@@ -137,3 +412,1 @@\n-  flags.set_worker_relocation();\n-  return ZHeap::heap()->alloc_page(forwarding->type(), forwarding->size(), flags);\n-}\n+  flags.set_gc_relocation();\n@@ -141,2 +414,1 @@\n-static void free_page(ZPage* page) {\n-  ZHeap::heap()->free_page(page, true \/* reclaimed *\/);\n+  return allocator->alloc_page_for_relocation(type, size, flags);\n@@ -145,1 +417,7 @@\n-static bool should_free_target_page(ZPage* page) {\n+static void retire_target_page(ZGeneration* generation, ZPage* page) {\n+  if (generation->is_young() && page->is_old()) {\n+    generation->increase_promoted(page->used());\n+  } else {\n+    generation->increase_compacted(page->used());\n+  }\n+\n@@ -150,1 +428,3 @@\n-  return page != NULL && page->top() == page->start();\n+  if (page->used() == 0) {\n+    ZHeap::heap()->free_page(page);\n+  }\n@@ -155,1 +435,2 @@\n-  volatile size_t _in_place_count;\n+  ZGeneration* const _generation;\n+  volatile size_t    _in_place_count;\n@@ -158,1 +439,2 @@\n-  ZRelocateSmallAllocator() :\n+  ZRelocateSmallAllocator(ZGeneration* generation) :\n+      _generation(generation),\n@@ -161,3 +443,4 @@\n-  ZPage* alloc_target_page(ZForwarding* forwarding, ZPage* target) {\n-    ZPage* const page = alloc_page(forwarding);\n-    if (page == NULL) {\n+  ZPage* alloc_and_retire_target_page(ZForwarding* forwarding, ZPage* target) {\n+    ZAllocatorForRelocation* const allocator = ZAllocator::relocation(forwarding->to_age());\n+    ZPage* const page = alloc_page(allocator, forwarding->type(), forwarding->size());\n+    if (page == nullptr) {\n@@ -167,0 +450,5 @@\n+    if (target != nullptr) {\n+      \/\/ Retire the old target page\n+      retire_target_page(_generation, target);\n+    }\n+\n@@ -175,2 +463,2 @@\n-    if (should_free_target_page(page)) {\n-      free_page(page);\n+    if (page != nullptr) {\n+      retire_target_page(_generation, page);\n@@ -180,6 +468,2 @@\n-  void free_relocated_page(ZPage* page) {\n-    free_page(page);\n-  }\n-\n-  uintptr_t alloc_object(ZPage* page, size_t size) const {\n-    return (page != NULL) ? page->alloc_object(size) : 0;\n+  zaddress alloc_object(ZPage* page, size_t size) const {\n+    return (page != nullptr) ? page->alloc_object(size) : zaddress::null;\n@@ -188,1 +472,1 @@\n-  void undo_alloc_object(ZPage* page, uintptr_t addr, size_t size) const {\n+  void undo_alloc_object(ZPage* page, zaddress addr, size_t size) const {\n@@ -199,4 +483,5 @@\n-  ZConditionLock      _lock;\n-  ZPage*              _shared;\n-  bool                _in_place;\n-  volatile size_t     _in_place_count;\n+  ZGeneration* const _generation;\n+  ZConditionLock     _lock;\n+  ZPage*             _shared[ZAllocator::_relocation_allocators];\n+  bool               _in_place;\n+  volatile size_t    _in_place_count;\n@@ -205,1 +490,2 @@\n-  ZRelocateMediumAllocator() :\n+  ZRelocateMediumAllocator(ZGeneration* generation) :\n+      _generation(generation),\n@@ -207,1 +493,1 @@\n-      _shared(NULL),\n+      _shared(),\n@@ -212,2 +498,4 @@\n-    if (should_free_target_page(_shared)) {\n-      free_page(_shared);\n+    for (uint i = 0; i < ZAllocator::_relocation_allocators; ++i) {\n+      if (_shared[i] != nullptr) {\n+        retire_target_page(_generation, _shared[i]);\n+      }\n@@ -217,1 +505,9 @@\n-  ZPage* alloc_target_page(ZForwarding* forwarding, ZPage* target) {\n+  ZPage* shared(ZPageAge age) {\n+    return _shared[static_cast<uint>(age) - 1];\n+  }\n+\n+  void set_shared(ZPageAge age, ZPage* page) {\n+    _shared[static_cast<uint>(age) - 1] = page;\n+  }\n+\n+  ZPage* alloc_and_retire_target_page(ZForwarding* forwarding, ZPage* target) {\n@@ -229,3 +525,6 @@\n-    if (_shared == target) {\n-      _shared = alloc_page(forwarding);\n-      if (_shared == NULL) {\n+    const ZPageAge to_age = forwarding->to_age();\n+    if (shared(to_age) == target) {\n+      ZAllocatorForRelocation* const allocator = ZAllocator::relocation(forwarding->to_age());\n+      ZPage* const to_page = alloc_page(allocator, forwarding->type(), forwarding->size());\n+      set_shared(to_age, to_page);\n+      if (to_page == nullptr) {\n@@ -235,0 +534,5 @@\n+\n+      \/\/ This thread is responsible for retiring the shared target page\n+      if (target != nullptr) {\n+        retire_target_page(_generation, target);\n+      }\n@@ -237,1 +541,1 @@\n-    return _shared;\n+    return shared(to_age);\n@@ -241,1 +545,1 @@\n-    ZLocker<ZConditionLock> locker(&_lock);\n+    const ZPageAge age = page->age();\n@@ -243,0 +547,1 @@\n+    ZLocker<ZConditionLock> locker(&_lock);\n@@ -244,2 +549,2 @@\n-    assert(_shared == NULL, \"Invalid state\");\n-    assert(page != NULL, \"Invalid page\");\n+    assert(shared(age) == nullptr, \"Invalid state\");\n+    assert(page != nullptr, \"Invalid page\");\n@@ -247,1 +552,1 @@\n-    _shared = page;\n+    set_shared(age, page);\n@@ -257,6 +562,2 @@\n-  void free_relocated_page(ZPage* page) {\n-    free_page(page);\n-  }\n-\n-  uintptr_t alloc_object(ZPage* page, size_t size) const {\n-    return (page != NULL) ? page->alloc_object_atomic(size) : 0;\n+  zaddress alloc_object(ZPage* page, size_t size) const {\n+    return (page != nullptr) ? page->alloc_object_atomic(size) : zaddress::null;\n@@ -265,1 +566,1 @@\n-  void undo_alloc_object(ZPage* page, uintptr_t addr, size_t size) const {\n+  void undo_alloc_object(ZPage* page, zaddress addr, size_t size) const {\n@@ -275,1 +576,1 @@\n-class ZRelocateClosure : public ObjectClosure {\n+class ZRelocateWork : public StackObj {\n@@ -277,3 +578,18 @@\n-  Allocator* const _allocator;\n-  ZForwarding*     _forwarding;\n-  ZPage*           _target;\n+  Allocator* const   _allocator;\n+  ZForwarding*       _forwarding;\n+  ZPage*             _target[ZAllocator::_relocation_allocators];\n+  ZGeneration* const _generation;\n+  size_t             _other_promoted;\n+  size_t             _other_compacted;\n+\n+  ZPage* target(ZPageAge age) {\n+    return _target[static_cast<uint>(age) - 1];\n+  }\n+\n+  void set_target(ZPageAge age, ZPage* page) {\n+    _target[static_cast<uint>(age) - 1] = page;\n+  }\n+\n+  size_t object_alignment() const {\n+    return (size_t)1 << _forwarding->object_alignment_shift();\n+  }\n@@ -281,1 +597,10 @@\n-  bool relocate_object(uintptr_t from_addr) const {\n+  void increase_other_forwarded(size_t unaligned_object_size) {\n+    const size_t aligned_size = align_up(unaligned_object_size, object_alignment());\n+    if (_forwarding->is_promotion()) {\n+      _other_promoted += aligned_size;\n+    } else {\n+      _other_compacted += aligned_size;\n+    }\n+  }\n+\n+  zaddress try_relocate_object_inner(zaddress from_addr) {\n@@ -284,0 +609,3 @@\n+    const size_t size = ZUtils::object_size(from_addr);\n+    ZPage* const to_page = target(_forwarding->to_age());\n+\n@@ -285,3 +613,7 @@\n-    if (forwarding_find(_forwarding, from_addr, &cursor) != 0) {\n-      \/\/ Already relocated\n-      return true;\n+    {\n+      const zaddress to_addr = forwarding_find(_forwarding, from_addr, &cursor);\n+      if (!is_null(to_addr)) {\n+        \/\/ Already relocated\n+        increase_other_forwarded(size);\n+        return to_addr;\n+      }\n@@ -291,3 +623,2 @@\n-    const size_t size = ZUtils::object_size(from_addr);\n-    const uintptr_t to_addr = _allocator->alloc_object(_target, size);\n-    if (to_addr == 0) {\n+    const zaddress allocated_addr = _allocator->alloc_object(to_page, size);\n+    if (is_null(allocated_addr)) {\n@@ -295,1 +626,1 @@\n-      return false;\n+      return zaddress::null;\n@@ -299,3 +630,3 @@\n-    \/\/ in-place and the new object overlapps with the old object.\n-    if (_forwarding->in_place() && to_addr + size > from_addr) {\n-      ZUtils::object_copy_conjoint(from_addr, to_addr, size);\n+    \/\/ in-place and the new object overlaps with the old object.\n+    if (_forwarding->in_place_relocation() && allocated_addr + size > from_addr) {\n+      ZUtils::object_copy_conjoint(from_addr, allocated_addr, size);\n@@ -303,1 +634,1 @@\n-      ZUtils::object_copy_disjoint(from_addr, to_addr, size);\n+      ZUtils::object_copy_disjoint(from_addr, allocated_addr, size);\n@@ -307,1 +638,2 @@\n-    if (forwarding_insert(_forwarding, from_addr, to_addr, &cursor) != to_addr) {\n+    const zaddress to_addr = forwarding_insert(_forwarding, from_addr, allocated_addr, &cursor);\n+    if (to_addr != allocated_addr) {\n@@ -309,1 +641,178 @@\n-      _allocator->undo_alloc_object(_target, to_addr, size);\n+      _allocator->undo_alloc_object(to_page, to_addr, size);\n+      increase_other_forwarded(size);\n+    }\n+\n+    return to_addr;\n+  }\n+\n+  void update_remset_old_to_old(zaddress from_addr, zaddress to_addr) const {\n+    \/\/ Old-to-old relocation - move existing remset bits\n+\n+    \/\/ If this is called for an in-place relocated page, then this code has the\n+    \/\/ responsibility to clear the old remset bits. Extra care is needed because:\n+    \/\/\n+    \/\/ 1) The to-object copy can overlap with the from-object copy\n+    \/\/ 2) Remset bits of old objects need to be cleared\n+    \/\/\n+    \/\/ A watermark is used to keep track of how far the old remset bits have been removed.\n+\n+    const bool in_place = _forwarding->in_place_relocation();\n+    ZPage* const from_page = _forwarding->page();\n+    const uintptr_t from_local_offset = from_page->local_offset(from_addr);\n+\n+    \/\/ Note: even with in-place relocation, the to_page could be another page\n+    ZPage* const to_page = ZHeap::heap()->page(to_addr);\n+\n+    \/\/ Uses _relaxed version to handle that in-place relocation resets _top\n+    assert(ZHeap::heap()->is_in_page_relaxed(from_page, from_addr), \"Must be\");\n+    assert(to_page->is_in(to_addr), \"Must be\");\n+\n+\n+    \/\/ Read the size from the to-object, since the from-object\n+    \/\/ could have been overwritten during in-place relocation.\n+    const size_t size = ZUtils::object_size(to_addr);\n+\n+    \/\/ If a young generation collection started while the old generation\n+    \/\/ relocated  objects, the remember set bits were flipped from \"current\"\n+    \/\/ to \"previous\".\n+    \/\/\n+    \/\/ We need to select the correct remembered sets bitmap to ensure that the\n+    \/\/ old remset bits are found.\n+    \/\/\n+    \/\/ Note that if the young generation marking (remset scanning) finishes\n+    \/\/ before the old generation relocation has relocated this page, then the\n+    \/\/ young generation will visit this page's previous remembered set bits and\n+    \/\/ moved them over to the current bitmap.\n+    \/\/\n+    \/\/ If the young generation runs multiple cycles while the old generation is\n+    \/\/ relocating, then the first cycle will have consume the the old remset,\n+    \/\/ bits and moved associated objects to a new old page. The old relocation\n+    \/\/ could find either the the two bitmaps. So, either it will find the original\n+    \/\/ remset bits for the page, or it will find an empty bitmap for the page. It\n+    \/\/ doesn't matter for correctness, because the young generation marking has\n+    \/\/ already taken care of the bits.\n+\n+    const bool active_remset_is_current = ZGeneration::old()->active_remset_is_current();\n+\n+    \/\/ When in-place relocation is done and the old remset bits are located in\n+    \/\/ the bitmap that is going to be used for the new remset bits, then we\n+    \/\/ need to clear the old bits before the new bits are inserted.\n+    const bool iterate_current_remset = active_remset_is_current && !in_place;\n+\n+    BitMap::Iterator iter = iterate_current_remset\n+        ? from_page->remset_iterator_limited_current(from_local_offset, size)\n+        : from_page->remset_iterator_limited_previous(from_local_offset, size);\n+\n+    for (BitMap::idx_t field_bit : iter) {\n+      const uintptr_t field_local_offset = ZRememberedSet::to_offset(field_bit);\n+\n+      \/\/ Add remset entry in the to-page\n+      const uintptr_t offset = field_local_offset - from_local_offset;\n+      const zaddress to_field = to_addr + offset;\n+      log_trace(gc, reloc)(\"Remember: from: \" PTR_FORMAT \" to: \" PTR_FORMAT \" current: %d marking: %d page: \" PTR_FORMAT \" remset: \" PTR_FORMAT,\n+          untype(from_page->start() + field_local_offset), untype(to_field), active_remset_is_current, ZGeneration::young()->is_phase_mark(), p2i(to_page), p2i(to_page->remset_current()));\n+\n+      volatile zpointer* const p = (volatile zpointer*)to_field;\n+\n+      if (ZGeneration::young()->is_phase_mark()) {\n+        \/\/ Young generation remembered set scanning needs to know about this\n+        \/\/ field. It will take responsibility to add a new remember set entry if needed.\n+        _forwarding->relocated_remembered_fields_register(p);\n+      } else {\n+        to_page->remember(p);\n+        if (in_place) {\n+          assert(to_page->is_remembered(p), \"p: \" PTR_FORMAT, p2i(p));\n+        }\n+      }\n+    }\n+  }\n+\n+  static bool add_remset_if_young(volatile zpointer* p, zaddress addr) {\n+    if (ZHeap::heap()->is_young(addr)) {\n+      ZRelocate::add_remset(p);\n+      return true;\n+    }\n+\n+    return false;\n+  }\n+\n+  static void update_remset_promoted_filter_and_remap_per_field(volatile zpointer* p) {\n+    const zpointer ptr = Atomic::load(p);\n+\n+    assert(ZPointer::is_old_load_good(ptr), \"Should be at least old load good: \" PTR_FORMAT, untype(ptr));\n+\n+    if (ZPointer::is_store_good(ptr)) {\n+      \/\/ Already has a remset entry\n+      return;\n+    }\n+\n+    if (ZPointer::is_load_good(ptr)) {\n+      if (!is_null_any(ptr)) {\n+        const zaddress addr = ZPointer::uncolor(ptr);\n+        add_remset_if_young(p, addr);\n+      }\n+      \/\/ No need to remap it is already load good\n+      return;\n+    }\n+\n+    if (is_null_any(ptr)) {\n+      \/\/ Eagerly remap to skip adding a remset entry just to get deferred remapping\n+      ZBarrier::remap_young_relocated(p, ptr);\n+      return;\n+    }\n+\n+    const zaddress_unsafe addr_unsafe = ZPointer::uncolor_unsafe(ptr);\n+    ZForwarding* const forwarding = ZGeneration::young()->forwarding(addr_unsafe);\n+\n+    if (forwarding == nullptr) {\n+      \/\/ Object isn't being relocated\n+      const zaddress addr = safe(addr_unsafe);\n+      if (!add_remset_if_young(p, addr)) {\n+        \/\/ Not young - eagerly remap to skip adding a remset entry just to get deferred remapping\n+        ZBarrier::remap_young_relocated(p, ptr);\n+      }\n+      return;\n+    }\n+\n+    const zaddress addr = forwarding->find(addr_unsafe);\n+\n+    if (!is_null(addr)) {\n+      \/\/ Object has already been relocated\n+      if (!add_remset_if_young(p, addr)) {\n+        \/\/ Not young - eagerly remap to skip adding a remset entry just to get deferred remapping\n+        ZBarrier::remap_young_relocated(p, ptr);\n+      }\n+      return;\n+    }\n+\n+    \/\/ Object has not been relocated yet\n+    \/\/ Don't want to eagerly relocate objects, so just add a remset\n+    ZRelocate::add_remset(p);\n+    return;\n+  }\n+\n+  void update_remset_promoted(zaddress to_addr) const {\n+    ZIterator::basic_oop_iterate(to_oop(to_addr), update_remset_promoted_filter_and_remap_per_field);\n+  }\n+\n+  void update_remset_for_fields(zaddress from_addr, zaddress to_addr) const {\n+    if (_forwarding->to_age() != ZPageAge::old) {\n+      \/\/ No remembered set in young pages\n+      return;\n+    }\n+\n+    \/\/ Need to deal with remset when moving objects to the old generation\n+    if (_forwarding->from_age() == ZPageAge::old) {\n+      update_remset_old_to_old(from_addr, to_addr);\n+      return;\n+    }\n+\n+    \/\/ Normal promotion\n+    update_remset_promoted(to_addr);\n+  }\n+\n+  bool try_relocate_object(zaddress from_addr) {\n+    const zaddress to_addr = try_relocate_object_inner(from_addr);\n+\n+    if (is_null(to_addr)) {\n+      return false;\n@@ -312,0 +821,2 @@\n+    update_remset_for_fields(from_addr, to_addr);\n+\n@@ -315,2 +826,52 @@\n-  virtual void do_object(oop obj) {\n-    const uintptr_t addr = ZOop::to_address(obj);\n+  void start_in_place_relocation_prepare_remset(ZPage* from_page) {\n+    if (_forwarding->from_age() != ZPageAge::old) {\n+      \/\/ Only old pages have use remset bits\n+      return;\n+    }\n+\n+    if (ZGeneration::old()->active_remset_is_current()) {\n+      \/\/ We want to iterate over and clear the remset bits of the from-space page,\n+      \/\/ and insert current bits in the to-space page. However, with in-place\n+      \/\/ relocation, the from-space and to-space pages are the same. Clearing\n+      \/\/ is destructive, and is difficult to perform before or during the iteration.\n+      \/\/ However, clearing of the current bits has to be done before exposing the\n+      \/\/ to-space objects in the forwarding table.\n+      \/\/\n+      \/\/ To solve this tricky dependency problem, we start by stashing away the\n+      \/\/ current bits in the previous bits, and clearing the current bits\n+      \/\/ (implemented by swapping the bits). This way, the current bits are\n+      \/\/ cleared before copying the objects (like a normal to-space page),\n+      \/\/ and the previous bits are representing a copy of the current bits\n+      \/\/ of the from-space page, and are used for iteration.\n+      from_page->swap_remset_bitmaps();\n+    }\n+  }\n+\n+  ZPage* start_in_place_relocation(zoffset relocated_watermark) {\n+    _forwarding->in_place_relocation_claim_page();\n+    _forwarding->in_place_relocation_start(relocated_watermark);\n+\n+    ZPage* const from_page = _forwarding->page();\n+\n+    const ZPageAge to_age = _forwarding->to_age();\n+    const bool promotion = _forwarding->is_promotion();\n+\n+    \/\/ Promotions happen through a new cloned page\n+    ZPage* const to_page = promotion ? from_page->clone_limited() : from_page;\n+    to_page->reset(to_age, ZPageResetType::InPlaceRelocation);\n+\n+    \/\/ Clear remset bits for all objects that were relocated\n+    \/\/ before this page became an in-place relocated page.\n+    start_in_place_relocation_prepare_remset(from_page);\n+\n+    if (promotion) {\n+      \/\/ Register the the promotion\n+      ZGeneration::young()->in_place_relocate_promote(from_page, to_page);\n+      ZGeneration::young()->register_in_place_relocate_promoted(from_page);\n+    }\n+\n+    return to_page;\n+  }\n+\n+  void relocate_object(oop obj) {\n+    const zaddress addr = to_zaddress(obj);\n@@ -319,1 +880,1 @@\n-    while (!relocate_object(addr)) {\n+    while (!try_relocate_object(addr)) {\n@@ -323,2 +884,4 @@\n-      _target = _allocator->alloc_target_page(_forwarding, _target);\n-      if (_target != NULL) {\n+      const ZPageAge to_age = _forwarding->to_age();\n+      ZPage* to_page = _allocator->alloc_and_retire_target_page(_forwarding, target(to_age));\n+      set_target(to_age, to_page);\n+      if (to_page != nullptr) {\n@@ -328,6 +891,5 @@\n-      \/\/ Claim the page being relocated to block other threads from accessing\n-      \/\/ it, or its forwarding table, until it has been released (relocation\n-      \/\/ completed).\n-      _target = _forwarding->claim_page();\n-      _target->reset_for_in_place_relocation();\n-      _forwarding->set_in_place();\n+      \/\/ Start in-place relocation to block other threads from accessing\n+      \/\/ the page, or its forwarding table, until it has been released\n+      \/\/ (relocation completed).\n+      to_page = start_in_place_relocation(ZAddress::offset(addr));\n+      set_target(to_age, to_page);\n@@ -338,1 +900,1 @@\n-  ZRelocateClosure(Allocator* allocator) :\n+  ZRelocateWork(Allocator* allocator, ZGeneration* generation) :\n@@ -340,2 +902,14 @@\n-      _forwarding(NULL),\n-      _target(NULL) {}\n+      _forwarding(nullptr),\n+      _target(),\n+      _generation(generation),\n+      _other_promoted(0),\n+      _other_compacted(0) {}\n+\n+  ~ZRelocateWork() {\n+    for (uint i = 0; i < ZAllocator::_relocation_allocators; ++i) {\n+      _allocator->free_target_page(_target[i]);\n+    }\n+    \/\/ Report statistics on-behalf of non-worker threads\n+    _generation->increase_promoted(_other_promoted);\n+    _generation->increase_compacted(_other_compacted);\n+  }\n@@ -343,2 +917,26 @@\n-  ~ZRelocateClosure() {\n-    _allocator->free_target_page(_target);\n+  bool active_remset_is_current() const {\n+    \/\/ Normal old-to-old relocation can treat the from-page remset as a\n+    \/\/ read-only copy, and then copy over the appropriate remset bits to the\n+    \/\/ cleared to-page's 'current' remset bitmap.\n+    \/\/\n+    \/\/ In-place relocation is more complicated. Since, the same page is both\n+    \/\/ a from-page and a to-page, we need to remove the old remset bits, and\n+    \/\/ add remset bits that corresponds to the new locations of the relocated\n+    \/\/ objects.\n+    \/\/\n+    \/\/ Depending on how long ago (in terms of number of young GC's and the\n+    \/\/ current young GC's phase), the page was allocated, the active\n+    \/\/ remembered set will be in either the 'current' or 'previous' bitmap.\n+    \/\/\n+    \/\/ If the active bits are in the 'previous' bitmap, we know that the\n+    \/\/ 'current' bitmap was cleared at some earlier point in time, and we can\n+    \/\/ simply set new bits in 'current' bitmap, and later when relocation has\n+    \/\/ read all the old remset bits, we could just clear the 'previous' remset\n+    \/\/ bitmap.\n+    \/\/\n+    \/\/ If, on the other hand, the active bits are in the 'current' bitmap, then\n+    \/\/ that bitmap will be used to both read the old remset bits, and the\n+    \/\/ destination for the remset bits that we copy when an object is copied\n+    \/\/ to it's new location within the page. We need to *carefully* remove all\n+    \/\/ all old remset bits, without clearing out the newly set bits.\n+    return ZGeneration::old()->active_remset_is_current();\n@@ -347,2 +945,10 @@\n-  void do_forwarding(ZForwarding* forwarding) {\n-    _forwarding = forwarding;\n+  void clear_remset_before_reuse(ZPage* page, bool in_place) {\n+    if (_forwarding->from_age() != ZPageAge::old) {\n+      \/\/ No remset bits\n+      return;\n+    }\n+\n+    if (in_place) {\n+      \/\/ Clear 'previous' remset bits. For in-place relocated pages, the previous\n+      \/\/ remset bits are always used, even when active_remset_is_current().\n+      page->clear_remset_previous();\n@@ -350,3 +956,0 @@\n-    \/\/ Check if we should abort\n-    if (ZAbort::should_abort()) {\n-      _forwarding->abort_page();\n@@ -356,0 +959,29 @@\n+    \/\/ Normal relocate\n+\n+    \/\/ Clear active remset bits\n+    if (active_remset_is_current()) {\n+      page->clear_remset_current();\n+    } else {\n+      page->clear_remset_previous();\n+    }\n+\n+    \/\/ Verify that inactive remset bits are all cleared\n+    if (active_remset_is_current()) {\n+      page->verify_remset_cleared_previous();\n+    } else {\n+      page->verify_remset_cleared_current();\n+    }\n+  }\n+\n+  void finish_in_place_relocation() {\n+    \/\/ We are done with the from_space copy of the page\n+    _forwarding->in_place_relocation_finish();\n+  }\n+\n+  void do_forwarding(ZForwarding* forwarding) {\n+    _forwarding = forwarding;\n+\n+    _forwarding->page()->log_msg(\" (relocate page)\");\n+\n+    ZVerify::before_relocation(_forwarding);\n+\n@@ -357,1 +989,3 @@\n-    _forwarding->object_iterate(this);\n+    _forwarding->object_iterate([&](oop obj) { relocate_object(obj); });\n+\n+    ZVerify::after_relocation(_forwarding);\n@@ -364,0 +998,13 @@\n+    _generation->increase_freed(_forwarding->page()->size());\n+\n+    \/\/ Deal with in-place relocation\n+    const bool in_place = _forwarding->in_place_relocation();\n+    if (in_place) {\n+      finish_in_place_relocation();\n+    }\n+\n+    \/\/ Old from-space pages need to deal with remset bits\n+    if (_forwarding->from_age() == ZPageAge::old) {\n+      _forwarding->relocated_remembered_fields_after_relocate();\n+    }\n+\n@@ -367,5 +1014,13 @@\n-    if (_forwarding->in_place()) {\n-      \/\/ The relocated page has been relocated in-place and should not\n-      \/\/ be freed. Keep it as target page until it is full, and offer to\n-      \/\/ share it with other worker threads.\n-      _allocator->share_target_page(_target);\n+    if (in_place) {\n+      \/\/ Wait for all other threads to call release_page\n+      ZPage* const page = _forwarding->detach_page();\n+\n+      \/\/ Ensure that previous remset bits are cleared\n+      clear_remset_before_reuse(page, true \/* in_place *\/);\n+\n+      page->log_msg(\" (relocate page done in-place)\");\n+\n+      \/\/ Different pages when promoting\n+      ZPage* const target_page = target(_forwarding->to_age());\n+      _allocator->share_target_page(target_page);\n+\n@@ -373,1 +1028,1 @@\n-      \/\/ Detach and free relocated page\n+      \/\/ Wait for all other threads to call release_page\n@@ -375,1 +1030,10 @@\n-      _allocator->free_relocated_page(page);\n+\n+      \/\/ Ensure that all remset bits are cleared\n+      \/\/ Note: cleared after detach_page, when we know that\n+      \/\/ the young generation isn't scanning the remset.\n+      clear_remset_before_reuse(page, false \/* in_place *\/);\n+\n+      page->log_msg(\" (relocate page done normal)\");\n+\n+      \/\/ Free page\n+      ZHeap::heap()->free_page(page);\n@@ -380,1 +1044,29 @@\n-class ZRelocateTask : public ZTask {\n+class ZRelocateStoreBufferInstallBasePointersThreadClosure : public ThreadClosure {\n+public:\n+  virtual void do_thread(Thread* thread) {\n+    JavaThread* const jt = JavaThread::cast(thread);\n+    ZStoreBarrierBuffer* buffer = ZThreadLocalData::store_barrier_buffer(jt);\n+    buffer->install_base_pointers();\n+  }\n+};\n+\n+\/\/ Installs the object base pointers (object starts), for the fields written\n+\/\/ in the store buffer. The code that searches for the object start uses that\n+\/\/ liveness information stored in the pages. That information is lost when the\n+\/\/ pages have been relocated and then destroyed.\n+class ZRelocateStoreBufferInstallBasePointersTask : public ZTask {\n+private:\n+  ZJavaThreadsIterator _threads_iter;\n+\n+public:\n+  ZRelocateStoreBufferInstallBasePointersTask(ZGeneration* generation) :\n+    ZTask(\"ZRelocateStoreBufferInstallBasePointersTask\"),\n+    _threads_iter(generation->id_optional()) {}\n+\n+  virtual void work() {\n+    ZRelocateStoreBufferInstallBasePointersThreadClosure fix_store_buffer_cl;\n+    _threads_iter.apply(&fix_store_buffer_cl);\n+  }\n+};\n+\n+class ZRelocateTask : public ZRestartableTask {\n@@ -383,0 +1075,2 @@\n+  ZGeneration* const             _generation;\n+  ZRelocateQueue* const          _queue;\n@@ -386,4 +1080,0 @@\n-  static bool is_small(ZForwarding* forwarding) {\n-    return forwarding->type() == ZPageTypeSmall;\n-  }\n-\n@@ -391,2 +1081,2 @@\n-  ZRelocateTask(ZRelocationSet* relocation_set) :\n-      ZTask(\"ZRelocateTask\"),\n+  ZRelocateTask(ZRelocationSet* relocation_set, ZRelocateQueue* queue) :\n+      ZRestartableTask(\"ZRelocateTask\"),\n@@ -394,2 +1084,4 @@\n-      _small_allocator(),\n-      _medium_allocator() {}\n+      _generation(relocation_set->generation()),\n+      _queue(queue),\n+      _small_allocator(_generation),\n+      _medium_allocator(_generation) {}\n@@ -398,2 +1090,1 @@\n-    ZStatRelocation::set_at_relocate_end(_small_allocator.in_place_count(),\n-                                         _medium_allocator.in_place_count());\n+    _generation->stat_relocation()->at_relocate_end(_small_allocator.in_place_count(), _medium_allocator.in_place_count());\n@@ -403,2 +1094,2 @@\n-    ZRelocateClosure<ZRelocateSmallAllocator> small(&_small_allocator);\n-    ZRelocateClosure<ZRelocateMediumAllocator> medium(&_medium_allocator);\n+    ZRelocateWork<ZRelocateSmallAllocator> small(&_small_allocator, _generation);\n+    ZRelocateWork<ZRelocateMediumAllocator> medium(&_medium_allocator, _generation);\n@@ -406,2 +1097,3 @@\n-    for (ZForwarding* forwarding; _iter.next(&forwarding);) {\n-      if (is_small(forwarding)) {\n+    const auto do_forwarding = [&](ZForwarding* forwarding) {\n+      ZPage* const page = forwarding->page();\n+      if (page->is_small()) {\n@@ -412,0 +1104,109 @@\n+\n+      \/\/ Absolute last thing done while relocating a page.\n+      \/\/\n+      \/\/ We don't use the SuspendibleThreadSet when relocating pages.\n+      \/\/ Instead the ZRelocateQueue is used as a pseudo STS joiner\/leaver.\n+      \/\/\n+      \/\/ After the mark_done call a safepointing could be completed and a\n+      \/\/ new GC phase could be entered.\n+      forwarding->mark_done();\n+    };\n+\n+    const auto claim_and_do_forwarding = [&](ZForwarding* forwarding) {\n+      if (forwarding->claim()) {\n+        do_forwarding(forwarding);\n+      }\n+    };\n+\n+    const auto do_forwarding_one_from_iter = [&]() {\n+      ZForwarding* forwarding;\n+\n+      if (_iter.next(&forwarding)) {\n+        claim_and_do_forwarding(forwarding);\n+        return true;\n+      }\n+\n+      return false;\n+    };\n+\n+    for (;;) {\n+      \/\/ As long as there are requests in the relocate queue, there are threads\n+      \/\/ waiting in a VM state that does not allow them to be blocked. The\n+      \/\/ worker thread needs to finish relocate these pages, and allow the\n+      \/\/ other threads to continue and proceed to a blocking state. After that,\n+      \/\/ the worker threads are allowed to safepoint synchronize.\n+      for (ZForwarding* forwarding; (forwarding = _queue->synchronize_poll()) != nullptr;) {\n+        do_forwarding(forwarding);\n+      }\n+\n+      if (!do_forwarding_one_from_iter()) {\n+        \/\/ No more work\n+        break;\n+      }\n+\n+      if (_generation->should_worker_resize()) {\n+        break;\n+      }\n+    }\n+\n+    _queue->leave();\n+  }\n+\n+  virtual void resize_workers(uint nworkers) {\n+    _queue->resize_workers(nworkers);\n+  }\n+};\n+\n+static void remap_and_maybe_add_remset(volatile zpointer* p) {\n+  const zpointer ptr = Atomic::load(p);\n+\n+  if (ZPointer::is_store_good(ptr)) {\n+    \/\/ Already has a remset entry\n+    return;\n+  }\n+\n+  \/\/ Remset entries are used for two reasons:\n+  \/\/ 1) Young marking old-to-young pointer roots\n+  \/\/ 2) Deferred remapping of stale old-to-young pointers\n+  \/\/\n+  \/\/ This load barrier will up-front perform the remapping of (2),\n+  \/\/ and the code below only has to make sure we register up-to-date\n+  \/\/ old-to-young pointers for (1).\n+  const zaddress addr = ZBarrier::load_barrier_on_oop_field_preloaded(p, ptr);\n+\n+  if (is_null(addr)) {\n+    \/\/ No need for remset entries for null pointers\n+    return;\n+  }\n+\n+  if (ZHeap::heap()->is_old(addr)) {\n+    \/\/ No need for remset entries for pointers to old gen\n+    return;\n+  }\n+\n+  ZRelocate::add_remset(p);\n+}\n+\n+class ZRelocateAddRemsetForFlipPromoted : public ZRestartableTask {\n+private:\n+  ZStatTimerYoung                _timer;\n+  ZArrayParallelIterator<ZPage*> _iter;\n+\n+public:\n+  ZRelocateAddRemsetForFlipPromoted(ZArray<ZPage*>* pages) :\n+      ZRestartableTask(\"ZRelocateAddRemsetForFlipPromoted\"),\n+      _timer(ZSubPhaseConcurrentRelocateRememberedSetFlipPromotedYoung),\n+      _iter(pages) {}\n+\n+  virtual void work() {\n+    SuspendibleThreadSetJoiner sts_joiner;\n+\n+    for (ZPage* page; _iter.next(&page);) {\n+      page->object_iterate([&](oop obj) {\n+        ZIterator::basic_oop_iterate_safe(obj, remap_and_maybe_add_remset);\n+      });\n+\n+      SuspendibleThreadSet::yield();\n+      if (ZGeneration::young()->should_worker_resize()) {\n+        return;\n+      }\n@@ -417,2 +1218,100 @@\n-  ZRelocateTask task(relocation_set);\n-  _workers->run(&task);\n+  {\n+    \/\/ Install the store buffer's base pointers before the\n+    \/\/ relocate task destroys the liveness information in\n+    \/\/ the relocated pages.\n+    ZRelocateStoreBufferInstallBasePointersTask buffer_task(_generation);\n+    workers()->run(&buffer_task);\n+  }\n+\n+  {\n+    ZRelocateTask relocate_task(relocation_set, &_queue);\n+    workers()->run(&relocate_task);\n+  }\n+\n+  if (relocation_set->generation()->is_young()) {\n+    ZRelocateAddRemsetForFlipPromoted task(relocation_set->flip_promoted_pages());\n+    workers()->run(&task);\n+  }\n+\n+  _queue.clear();\n+}\n+\n+ZPageAge ZRelocate::compute_to_age(ZPageAge from_age) {\n+  if (from_age == ZPageAge::old) {\n+    return ZPageAge::old;\n+  }\n+\n+  const uint age = static_cast<uint>(from_age);\n+  if (age >= ZGeneration::young()->tenuring_threshold()) {\n+    return ZPageAge::old;\n+  }\n+\n+  return static_cast<ZPageAge>(age + 1);\n+}\n+\n+class ZFlipAgePagesTask : public ZTask {\n+private:\n+  ZArrayParallelIterator<ZPage*> _iter;\n+\n+public:\n+  ZFlipAgePagesTask(const ZArray<ZPage*>* pages) :\n+      ZTask(\"ZPromotePagesTask\"),\n+      _iter(pages) {}\n+\n+  virtual void work() {\n+    SuspendibleThreadSetJoiner sts_joiner;\n+    ZArray<ZPage*> promoted_pages;\n+\n+    for (ZPage* prev_page; _iter.next(&prev_page);) {\n+      const ZPageAge from_age = prev_page->age();\n+      const ZPageAge to_age = ZRelocate::compute_to_age(from_age);\n+      assert(from_age != ZPageAge::old, \"invalid age for a young collection\");\n+\n+      \/\/ Figure out if this is proper promotion\n+      const bool promotion = to_age == ZPageAge::old;\n+\n+      if (promotion) {\n+        \/\/ Before promoting an object (and before relocate start), we must ensure that all\n+        \/\/ contained zpointers are store good. The marking code ensures that for non-null\n+        \/\/ pointers, but null pointers are ignored. This code ensures that even null pointers\n+        \/\/ are made store good, for the promoted objects.\n+        prev_page->object_iterate([&](oop obj) {\n+          ZIterator::basic_oop_iterate_safe(obj, ZBarrier::promote_barrier_on_young_oop_field);\n+        });\n+      }\n+\n+      \/\/ Logging\n+      prev_page->log_msg(promotion ? \" (flip promoted)\" : \" (flip survived)\");\n+\n+      \/\/ Setup to-space page\n+      ZPage* const new_page = promotion ? prev_page->clone_limited_promote_flipped() : prev_page;\n+      new_page->reset(to_age, ZPageResetType::FlipAging);\n+\n+      if (promotion) {\n+        ZGeneration::young()->flip_promote(prev_page, new_page);\n+        \/\/ Defer promoted page registration times the lock is taken\n+        promoted_pages.push(prev_page);\n+      }\n+\n+      SuspendibleThreadSet::yield();\n+    }\n+\n+    ZGeneration::young()->register_flip_promoted(promoted_pages);\n+  }\n+};\n+\n+void ZRelocate::flip_age_pages(const ZArray<ZPage*>* pages) {\n+  ZFlipAgePagesTask flip_age_task(pages);\n+  workers()->run(&flip_age_task);\n+}\n+\n+void ZRelocate::synchronize() {\n+  _queue.synchronize();\n+}\n+\n+void ZRelocate::desynchronize() {\n+  _queue.desynchronize();\n+}\n+\n+ZRelocateQueue* ZRelocate::queue() {\n+  return &_queue;\n","filename":"src\/hotspot\/share\/gc\/z\/zRelocate.cpp","additions":1047,"deletions":148,"binary":false,"changes":1195,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,0 +27,2 @@\n+#include \"gc\/z\/zAddress.hpp\"\n+#include \"gc\/z\/zPageAge.hpp\"\n@@ -30,0 +32,1 @@\n+class ZGeneration;\n@@ -32,0 +35,37 @@\n+typedef size_t ZForwardingCursor;\n+\n+class ZRelocateQueue {\n+private:\n+  ZConditionLock       _lock;\n+  ZArray<ZForwarding*> _queue;\n+  uint                 _nworkers;\n+  uint                 _nsynchronized;\n+  bool                 _synchronize;\n+  volatile int         _needs_attention;\n+\n+  bool needs_attention() const;\n+  void inc_needs_attention();\n+  void dec_needs_attention();\n+\n+  bool prune();\n+  ZForwarding* prune_and_claim();\n+\n+public:\n+  ZRelocateQueue();\n+\n+  void join(uint nworkers);\n+  void resize_workers(uint nworkers);\n+  void leave();\n+\n+  void add_and_wait(ZForwarding* forwarding);\n+\n+  ZForwarding* synchronize_poll();\n+  void synchronize_thread();\n+  void desynchronize_thread();\n+\n+  void clear();\n+\n+  void synchronize();\n+  void desynchronize();\n+};\n+\n@@ -36,1 +76,2 @@\n-  ZWorkers* const _workers;\n+  ZGeneration* const _generation;\n+  ZRelocateQueue     _queue;\n@@ -38,0 +79,1 @@\n+  ZWorkers* workers() const;\n@@ -41,1 +83,5 @@\n-  ZRelocate(ZWorkers* workers);\n+  ZRelocate(ZGeneration* generation);\n+\n+  void start();\n+\n+  static void add_remset(volatile zpointer* p);\n@@ -43,2 +89,4 @@\n-  uintptr_t relocate_object(ZForwarding* forwarding, uintptr_t from_addr) const;\n-  uintptr_t forward_object(ZForwarding* forwarding, uintptr_t from_addr) const;\n+  static ZPageAge compute_to_age(ZPageAge from_age);\n+\n+  zaddress relocate_object(ZForwarding* forwarding, zaddress_unsafe from_addr);\n+  zaddress forward_object(ZForwarding* forwarding, zaddress_unsafe from_addr);\n@@ -47,0 +95,7 @@\n+\n+  void flip_age_pages(const ZArray<ZPage*>* pages);\n+\n+  void synchronize();\n+  void desynchronize();\n+\n+  ZRelocateQueue* queue();\n","filename":"src\/hotspot\/share\/gc\/z\/zRelocate.hpp","additions":60,"deletions":5,"binary":false,"changes":65,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -26,0 +26,1 @@\n+#include \"gc\/z\/zCollectedHeap.hpp\"\n@@ -28,0 +29,3 @@\n+#include \"gc\/z\/zGeneration.inline.hpp\"\n+#include \"gc\/z\/zPage.inline.hpp\"\n+#include \"gc\/z\/zPageAllocator.hpp\"\n@@ -41,0 +45,2 @@\n+  const ZArray<ZPage*>*          _small;\n+  const ZArray<ZPage*>*          _medium;\n@@ -43,2 +49,0 @@\n-  volatile size_t                _small_next;\n-  volatile size_t                _medium_next;\n@@ -46,2 +50,1 @@\n-  void install(ZForwarding* forwarding, volatile size_t* next) {\n-    const size_t index = Atomic::fetch_and_add(next, 1u);\n+  void install(ZForwarding* forwarding, size_t index) {\n@@ -49,0 +52,5 @@\n+\n+    ZPage* const page = forwarding->page();\n+\n+    page->log_msg(\" (relocation selected)\");\n+\n@@ -50,0 +58,10 @@\n+\n+    if (forwarding->is_promotion()) {\n+      \/\/ Before promoting an object (and before relocate start), we must ensure that all\n+      \/\/ contained zpointers are store good. The marking code ensures that for non-null\n+      \/\/ pointers, but null pointers are ignored. This code ensures that even null pointers\n+      \/\/ are made store good, for the promoted objects.\n+      page->object_iterate([&](oop obj) {\n+        ZIterator::basic_oop_iterate_safe(obj, ZBarrier::promote_barrier_on_young_oop_field);\n+      });\n+    }\n@@ -52,2 +70,2 @@\n-  void install_small(ZForwarding* forwarding) {\n-    install(forwarding, &_small_next);\n+  void install_small(ZForwarding* forwarding, size_t index) {\n+    install(forwarding, index);\n@@ -56,2 +74,6 @@\n-  void install_medium(ZForwarding* forwarding) {\n-    install(forwarding, &_medium_next);\n+  void install_medium(ZForwarding* forwarding, size_t index) {\n+    install(forwarding, index);\n+  }\n+\n+  ZPageAge to_age(ZPage* page) {\n+    return ZRelocate::compute_to_age(page->age());\n@@ -64,6 +86,6 @@\n-      _forwardings(NULL),\n-      _nforwardings(selector->small()->length() + selector->medium()->length()),\n-      _small_iter(selector->small()),\n-      _medium_iter(selector->medium()),\n-      _small_next(selector->medium()->length()),\n-      _medium_next(0) {\n+      _forwardings(nullptr),\n+      _nforwardings(selector->selected_small()->length() + selector->selected_medium()->length()),\n+      _small(selector->selected_small()),\n+      _medium(selector->selected_medium()),\n+      _small_iter(selector->selected_small()),\n+      _medium_iter(selector->selected_medium()) {\n@@ -88,3 +110,4 @@\n-    for (ZPage* page; _small_iter.next(&page);) {\n-      ZForwarding* const forwarding = ZForwarding::alloc(_allocator, page);\n-      install_small(forwarding);\n+    for (size_t page_index; _small_iter.next_index(&page_index);) {\n+      ZPage* page = _small->at(int(page_index));\n+      ZForwarding* const forwarding = ZForwarding::alloc(_allocator, page, to_age(page));\n+      install_small(forwarding, _medium->length() + page_index);\n@@ -94,3 +117,4 @@\n-    for (ZPage* page; _medium_iter.next(&page);) {\n-      ZForwarding* const forwarding = ZForwarding::alloc(_allocator, page);\n-      install_medium(forwarding);\n+    for (size_t page_index; _medium_iter.next_index(&page_index);) {\n+      ZPage* page = _medium->at(int(page_index));\n+      ZForwarding* const forwarding = ZForwarding::alloc(_allocator, page, to_age(page));\n+      install_medium(forwarding, page_index);\n@@ -109,2 +133,2 @@\n-ZRelocationSet::ZRelocationSet(ZWorkers* workers) :\n-    _workers(workers),\n+ZRelocationSet::ZRelocationSet(ZGeneration* generation) :\n+    _generation(generation),\n@@ -112,2 +136,17 @@\n-    _forwardings(NULL),\n-    _nforwardings(0) {}\n+    _forwardings(nullptr),\n+    _nforwardings(0),\n+    _promotion_lock(),\n+    _flip_promoted_pages(),\n+    _in_place_relocate_promoted_pages() {}\n+\n+ZWorkers* ZRelocationSet::workers() const {\n+  return _generation->workers();\n+}\n+\n+ZGeneration* ZRelocationSet::generation() const {\n+  return _generation;\n+}\n+\n+ZArray<ZPage*>* ZRelocationSet::flip_promoted_pages() {\n+  return &_flip_promoted_pages;\n+}\n@@ -118,1 +157,1 @@\n-  _workers->run(&task);\n+  workers()->run(&task);\n@@ -124,1 +163,1 @@\n-  ZStatRelocation::set_at_install_relocation_set(_allocator.size());\n+  _generation->stat_relocation()->at_install_relocation_set(_allocator.size());\n@@ -127,1 +166,10 @@\n-void ZRelocationSet::reset() {\n+static void destroy_and_clear(ZPageAllocator* page_allocator, ZArray<ZPage*>* array) {\n+  for (int i = 0; i < array->length(); i++) {\n+    \/\/ Delete non-relocating promoted pages from last cycle\n+    ZPage* const page = array->at(i);\n+    page_allocator->safe_destroy_page(page);\n+  }\n+\n+  array->clear();\n+}\n+void ZRelocationSet::reset(ZPageAllocator* page_allocator) {\n@@ -135,0 +183,17 @@\n+\n+  destroy_and_clear(page_allocator, &_in_place_relocate_promoted_pages);\n+  destroy_and_clear(page_allocator, &_flip_promoted_pages);\n+}\n+\n+void ZRelocationSet::register_flip_promoted(const ZArray<ZPage*>& pages) {\n+  ZLocker<ZLock> locker(&_promotion_lock);\n+  for (ZPage* const page : pages) {\n+    assert(!_flip_promoted_pages.contains(page), \"no duplicates allowed\");\n+    _flip_promoted_pages.append(page);\n+  }\n+}\n+\n+void ZRelocationSet::register_in_place_relocate_promoted(ZPage* page) {\n+  ZLocker<ZLock> locker(&_promotion_lock);\n+  assert(!_in_place_relocate_promoted_pages.contains(page), \"no duplicates allowed\");\n+  _in_place_relocate_promoted_pages.append(page);\n","filename":"src\/hotspot\/share\/gc\/z\/zRelocationSet.cpp","additions":93,"deletions":28,"binary":false,"changes":121,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -29,0 +29,1 @@\n+#include \"gc\/z\/zLock.hpp\"\n@@ -31,0 +32,3 @@\n+class ZGeneration;\n+class ZPage;\n+class ZPageAllocator;\n@@ -38,1 +42,1 @@\n-  ZWorkers*            _workers;\n+  ZGeneration*         _generation;\n@@ -42,0 +46,5 @@\n+  ZLock                _promotion_lock;\n+  ZArray<ZPage*>       _flip_promoted_pages;\n+  ZArray<ZPage*>       _in_place_relocate_promoted_pages;\n+\n+  ZWorkers* workers() const;\n@@ -44,1 +53,1 @@\n-  ZRelocationSet(ZWorkers* workers);\n+  ZRelocationSet(ZGeneration* generation);\n@@ -47,1 +56,6 @@\n-  void reset();\n+  void reset(ZPageAllocator* page_allocator);\n+  ZGeneration* generation() const;\n+  ZArray<ZPage*>* flip_promoted_pages();\n+\n+  void register_flip_promoted(const ZArray<ZPage*>& pages);\n+  void register_in_place_relocate_promoted(ZPage* page);\n","filename":"src\/hotspot\/share\/gc\/z\/zRelocationSet.hpp","additions":18,"deletions":4,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -37,1 +37,1 @@\n-    _npages(0),\n+    _npages_candidates(0),\n@@ -41,0 +41,1 @@\n+    _npages_selected(0),\n@@ -44,1 +45,1 @@\n-                                                         uint8_t page_type,\n+                                                         ZPageType page_type,\n@@ -46,1 +47,2 @@\n-                                                         size_t object_size_limit) :\n+                                                         size_t object_size_limit,\n+                                                         double fragmentation_limit) :\n@@ -51,1 +53,2 @@\n-    _fragmentation_limit(page_size * (ZFragmentationLimit \/ 100)),\n+    _fragmentation_limit(fragmentation_limit),\n+    _page_fragmentation_limit(page_size * (fragmentation_limit \/ 100)),\n@@ -53,0 +56,1 @@\n+    _not_selected_pages(),\n@@ -58,1 +62,1 @@\n-  return _page_type == ZPageTypeMedium && _page_size == 0;\n+  return _page_type == ZPageType::medium && _page_size == 0;\n@@ -63,1 +67,1 @@\n-  return _page_type != ZPageTypeLarge;\n+  return _page_type != ZPageType::large;\n@@ -93,1 +97,1 @@\n-  ZArray<ZPage*> sorted_live_pages(npages, npages, NULL);\n+  ZArray<ZPage*> sorted_live_pages(npages, npages, nullptr);\n@@ -100,1 +104,1 @@\n-    assert(sorted_live_pages.at(finger) == NULL, \"Invalid finger\");\n+    assert(sorted_live_pages.at(finger) == nullptr, \"Invalid finger\");\n@@ -114,1 +118,2 @@\n-  size_t selected_live_bytes = 0;\n+  size_t npages_selected[ZPageAgeMax + 1] = { 0 };\n+  size_t selected_live_bytes[ZPageAgeMax + 1] = { 0 };\n@@ -116,0 +121,1 @@\n+\n@@ -124,1 +130,2 @@\n-    from_live_bytes += page->live_bytes();\n+    const size_t page_live_bytes = page->live_bytes();\n+    from_live_bytes += page_live_bytes;\n@@ -140,1 +147,1 @@\n-    if (diff_reclaimable > ZFragmentationLimit) {\n+    if (diff_reclaimable > _fragmentation_limit) {\n@@ -143,1 +150,2 @@\n-      selected_live_bytes = from_live_bytes;\n+      selected_live_bytes[static_cast<uint>(page->age())] += page_live_bytes;\n+      npages_selected[static_cast<uint>(page->age())] += 1;\n@@ -148,1 +156,1 @@\n-                         \"%.1f%% relative defragmentation, \" SIZE_FORMAT \" forwarding entries, %s\",\n+                         \"%.1f%% relative defragmentation, \" SIZE_FORMAT \" forwarding entries, %s, live %d\",\n@@ -150,1 +158,2 @@\n-                         (selected_from == from) ? \"Selected\" : \"Rejected\");\n+                         (selected_from == from) ? \"Selected\" : \"Rejected\",\n+                         int(page_live_bytes * 100 \/ page->size()));\n@@ -154,0 +163,6 @@\n+  for (int i = selected_from; i < _live_pages.length(); i++) {\n+    ZPage* const page = _live_pages.at(i);\n+    if (page->is_young()) {\n+      _not_selected_pages.append(page);\n+    }\n+  }\n@@ -158,1 +173,4 @@\n-  _stats._relocate = selected_live_bytes;\n+  for (uint i = 0; i <= ZPageAgeMax; ++i) {\n+    _stats[i]._relocate = selected_live_bytes[i];\n+    _stats[i]._npages_selected = npages_selected[i];\n+  }\n@@ -160,1 +178,1 @@\n-  log_trace(gc, reloc)(\"Relocation Set (%s Pages): %d->%d, %d skipped, \" SIZE_FORMAT \" forwarding entries\",\n+  log_debug(gc, reloc)(\"Relocation Set (%s Pages): %d->%d, %d skipped, \" SIZE_FORMAT \" forwarding entries\",\n@@ -173,0 +191,16 @@\n+  } else {\n+    \/\/ Mark pages as not selected\n+    const int npages = _live_pages.length();\n+    for (int from = 1; from <= npages; from++) {\n+      ZPage* const page = _live_pages.at(from - 1);\n+      _not_selected_pages.append(page);\n+    }\n+  }\n+\n+  ZRelocationSetSelectorGroupStats s{};\n+  for (uint i = 0; i <= ZPageAgeMax; ++i) {\n+    s._npages_candidates += _stats[i].npages_candidates();\n+    s._total += _stats[i].total();\n+    s._empty += _stats[i].empty();\n+    s._npages_selected += _stats[i].npages_selected();\n+    s._relocate += _stats[i].relocate();\n@@ -176,1 +210,1 @@\n-  event.commit(_page_type, _stats.npages(), _stats.total(), _stats.empty(), _stats.relocate());\n+  event.commit((u8)_page_type, s._npages_candidates, s._total, s._empty, s._npages_selected, s._relocate);\n@@ -179,4 +213,4 @@\n-ZRelocationSetSelector::ZRelocationSetSelector() :\n-    _small(\"Small\", ZPageTypeSmall, ZPageSizeSmall, ZObjectSizeLimitSmall),\n-    _medium(\"Medium\", ZPageTypeMedium, ZPageSizeMedium, ZObjectSizeLimitMedium),\n-    _large(\"Large\", ZPageTypeLarge, 0 \/* page_size *\/, 0 \/* object_size_limit *\/),\n+ZRelocationSetSelector::ZRelocationSetSelector(double fragmentation_limit) :\n+    _small(\"Small\", ZPageType::small, ZPageSizeSmall, ZObjectSizeLimitSmall, fragmentation_limit),\n+    _medium(\"Medium\", ZPageType::medium, ZPageSizeMedium, ZObjectSizeLimitMedium, fragmentation_limit),\n+    _large(\"Large\", ZPageType::large, 0 \/* page_size *\/, 0 \/* object_size_limit *\/, fragmentation_limit),\n@@ -205,3 +239,10 @@\n-  stats._small = _small.stats();\n-  stats._medium = _medium.stats();\n-  stats._large = _large.stats();\n+\n+  for (uint i = 0; i <= ZPageAgeMax; ++i) {\n+    const ZPageAge age = static_cast<ZPageAge>(i);\n+    stats._small[i] = _small.stats(age);\n+    stats._medium[i] = _medium.stats(age);\n+    stats._large[i] = _large.stats(age);\n+  }\n+\n+  stats._has_relocatable_pages = total() > 0;\n+\n","filename":"src\/hotspot\/share\/gc\/z\/zRelocationSetSelector.cpp","additions":66,"deletions":25,"binary":false,"changes":91,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,0 +28,3 @@\n+#include \"gc\/z\/zGenerationId.hpp\"\n+#include \"gc\/z\/zPageAge.hpp\"\n+#include \"gc\/z\/zPageType.hpp\"\n@@ -36,1 +39,2 @@\n-  size_t _npages;\n+  \/\/ Candidate set\n+  size_t _npages_candidates;\n@@ -40,0 +44,3 @@\n+\n+  \/\/ Selected set\n+  size_t _npages_selected;\n@@ -45,1 +52,1 @@\n-  size_t npages() const;\n+  size_t npages_candidates() const;\n@@ -49,0 +56,2 @@\n+\n+  size_t npages_selected() const;\n@@ -56,3 +65,5 @@\n-  ZRelocationSetSelectorGroupStats _small;\n-  ZRelocationSetSelectorGroupStats _medium;\n-  ZRelocationSetSelectorGroupStats _large;\n+  ZRelocationSetSelectorGroupStats _small[ZPageAgeMax + 1];\n+  ZRelocationSetSelectorGroupStats _medium[ZPageAgeMax + 1];\n+  ZRelocationSetSelectorGroupStats _large[ZPageAgeMax + 1];\n+\n+  size_t _has_relocatable_pages;\n@@ -61,3 +72,5 @@\n-  const ZRelocationSetSelectorGroupStats& small() const;\n-  const ZRelocationSetSelectorGroupStats& medium() const;\n-  const ZRelocationSetSelectorGroupStats& large() const;\n+  const ZRelocationSetSelectorGroupStats& small(ZPageAge age) const;\n+  const ZRelocationSetSelectorGroupStats& medium(ZPageAge age) const;\n+  const ZRelocationSetSelectorGroupStats& large(ZPageAge age) const;\n+\n+  bool has_relocatable_pages() const;\n@@ -69,1 +82,1 @@\n-  const uint8_t                    _page_type;\n+  const ZPageType                  _page_type;\n@@ -72,1 +85,2 @@\n-  const size_t                     _fragmentation_limit;\n+  const double                     _fragmentation_limit;\n+  const size_t                     _page_fragmentation_limit;\n@@ -74,0 +88,1 @@\n+  ZArray<ZPage*>                   _not_selected_pages;\n@@ -75,1 +90,1 @@\n-  ZRelocationSetSelectorGroupStats _stats;\n+  ZRelocationSetSelectorGroupStats _stats[ZPageAgeMax + 1];\n@@ -84,1 +99,1 @@\n-                              uint8_t page_type,\n+                              ZPageType page_type,\n@@ -86,1 +101,2 @@\n-                              size_t object_size_limit);\n+                              size_t object_size_limit,\n+                              double fragmentation_limit);\n@@ -92,1 +108,3 @@\n-  const ZArray<ZPage*>* selected() const;\n+  const ZArray<ZPage*>* live_pages() const;\n+  const ZArray<ZPage*>* selected_pages() const;\n+  const ZArray<ZPage*>* not_selected_pages() const;\n@@ -95,1 +113,1 @@\n-  const ZRelocationSetSelectorGroupStats& stats() const;\n+  const ZRelocationSetSelectorGroupStats& stats(ZPageAge age) const;\n@@ -110,1 +128,1 @@\n-  ZRelocationSetSelector();\n+  ZRelocationSetSelector(double fragmentation_limit);\n@@ -121,2 +139,6 @@\n-  const ZArray<ZPage*>* small() const;\n-  const ZArray<ZPage*>* medium() const;\n+  const ZArray<ZPage*>* selected_small() const;\n+  const ZArray<ZPage*>* selected_medium() const;\n+\n+  const ZArray<ZPage*>* not_selected_small() const;\n+  const ZArray<ZPage*>* not_selected_medium() const;\n+  const ZArray<ZPage*>* not_selected_large() const;\n","filename":"src\/hotspot\/share\/gc\/z\/zRelocationSetSelector.hpp","additions":41,"deletions":19,"binary":false,"changes":60,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -32,2 +32,2 @@\n-inline size_t ZRelocationSetSelectorGroupStats::npages() const {\n-  return _npages;\n+inline size_t ZRelocationSetSelectorGroupStats::npages_candidates() const {\n+  return _npages_candidates;\n@@ -48,0 +48,4 @@\n+inline size_t ZRelocationSetSelectorGroupStats::npages_selected() const {\n+  return _npages_selected;\n+}\n+\n@@ -52,2 +56,2 @@\n-inline const ZRelocationSetSelectorGroupStats& ZRelocationSetSelectorStats::small() const {\n-  return _small;\n+inline bool ZRelocationSetSelectorStats::has_relocatable_pages() const {\n+  return _has_relocatable_pages;\n@@ -56,2 +60,2 @@\n-inline const ZRelocationSetSelectorGroupStats& ZRelocationSetSelectorStats::medium() const {\n-  return _medium;\n+inline const ZRelocationSetSelectorGroupStats& ZRelocationSetSelectorStats::small(ZPageAge age) const {\n+  return _small[static_cast<uint>(age)];\n@@ -60,2 +64,6 @@\n-inline const ZRelocationSetSelectorGroupStats& ZRelocationSetSelectorStats::large() const {\n-  return _large;\n+inline const ZRelocationSetSelectorGroupStats& ZRelocationSetSelectorStats::medium(ZPageAge age) const {\n+  return _medium[static_cast<uint>(age)];\n+}\n+\n+inline const ZRelocationSetSelectorGroupStats& ZRelocationSetSelectorStats::large(ZPageAge age) const {\n+  return _large[static_cast<uint>(age)];\n@@ -65,1 +73,0 @@\n-  const uint8_t type = page->type();\n@@ -70,1 +77,2 @@\n-  if (garbage > _fragmentation_limit) {\n+  \/\/ Pre-filter out pages that are guaranteed to not be selected\n+  if (!page->is_large() && garbage > _page_fragmentation_limit) {\n@@ -72,0 +80,2 @@\n+  } else if (page->is_young()) {\n+    _not_selected_pages.append(page);\n@@ -74,3 +84,4 @@\n-  _stats._npages++;\n-  _stats._total += size;\n-  _stats._live += live;\n+  const uint age = static_cast<uint>(page->age());\n+  _stats[age]._npages_candidates++;\n+  _stats[age]._total += size;\n+  _stats[age]._live += live;\n@@ -82,3 +93,4 @@\n-  _stats._npages++;\n-  _stats._total += size;\n-  _stats._empty += size;\n+  const uint age = static_cast<uint>(page->age());\n+  _stats[age]._npages_candidates++;\n+  _stats[age]._total += size;\n+  _stats[age]._empty += size;\n@@ -87,1 +99,1 @@\n-inline const ZArray<ZPage*>* ZRelocationSetSelectorGroup::selected() const {\n+inline const ZArray<ZPage*>* ZRelocationSetSelectorGroup::selected_pages() const {\n@@ -91,0 +103,4 @@\n+inline const ZArray<ZPage*>* ZRelocationSetSelectorGroup::not_selected_pages() const {\n+  return &_not_selected_pages;\n+}\n+\n@@ -95,2 +111,2 @@\n-inline const ZRelocationSetSelectorGroupStats& ZRelocationSetSelectorGroup::stats() const {\n-  return _stats;\n+inline const ZRelocationSetSelectorGroupStats& ZRelocationSetSelectorGroup::stats(ZPageAge age) const {\n+  return _stats[static_cast<uint>(age)];\n@@ -100,1 +116,3 @@\n-  const uint8_t type = page->type();\n+  page->log_msg(\" (relocation candidate)\");\n+\n+  const ZPageType type = page->type();\n@@ -102,1 +120,1 @@\n-  if (type == ZPageTypeSmall) {\n+  if (type == ZPageType::small) {\n@@ -104,1 +122,1 @@\n-  } else if (type == ZPageTypeMedium) {\n+  } else if (type == ZPageType::medium) {\n@@ -112,1 +130,1 @@\n-  const uint8_t type = page->type();\n+  page->log_msg(\" (relocation empty)\");\n@@ -114,1 +132,3 @@\n-  if (type == ZPageTypeSmall) {\n+  const ZPageType type = page->type();\n+\n+  if (type == ZPageType::small) {\n@@ -116,1 +136,1 @@\n-  } else if (type == ZPageTypeMedium) {\n+  } else if (type == ZPageType::medium) {\n@@ -138,1 +158,6 @@\n-  return _small.stats().total() + _medium.stats().total() + _large.stats().total();\n+  size_t sum = 0;\n+  for (uint i = 0; i <= ZPageAgeMax; ++i) {\n+    const ZPageAge age = static_cast<ZPageAge>(i);\n+    sum += _small.stats(age).total() + _medium.stats(age).total() + _large.stats(age).total();\n+  }\n+  return sum;\n@@ -142,1 +167,6 @@\n-  return _small.stats().empty() + _medium.stats().empty() + _large.stats().empty();\n+  size_t sum = 0;\n+  for (uint i = 0; i <= ZPageAgeMax; ++i) {\n+    const ZPageAge age = static_cast<ZPageAge>(i);\n+    sum += _small.stats(age).empty() + _medium.stats(age).empty() + _large.stats(age).empty();\n+  }\n+  return sum;\n@@ -146,1 +176,18 @@\n-  return _small.stats().relocate() + _medium.stats().relocate() + _large.stats().relocate();\n+  size_t sum = 0;\n+  for (uint i = 0; i <= ZPageAgeMax; ++i) {\n+    const ZPageAge age = static_cast<ZPageAge>(i);\n+    sum += _small.stats(age).relocate() + _medium.stats(age).relocate() + _large.stats(age).relocate();\n+  }\n+  return sum;\n+}\n+\n+inline const ZArray<ZPage*>* ZRelocationSetSelector::selected_small() const {\n+  return _small.selected_pages();\n+}\n+\n+inline const ZArray<ZPage*>* ZRelocationSetSelector::selected_medium() const {\n+  return _medium.selected_pages();\n+}\n+\n+inline const ZArray<ZPage*>* ZRelocationSetSelector::not_selected_small() const {\n+  return _small.not_selected_pages();\n@@ -149,2 +196,2 @@\n-inline const ZArray<ZPage*>* ZRelocationSetSelector::small() const {\n-  return _small.selected();\n+inline const ZArray<ZPage*>* ZRelocationSetSelector::not_selected_medium() const {\n+  return _medium.not_selected_pages();\n@@ -153,2 +200,2 @@\n-inline const ZArray<ZPage*>* ZRelocationSetSelector::medium() const {\n-  return _medium.selected();\n+inline const ZArray<ZPage*>* ZRelocationSetSelector::not_selected_large() const {\n+  return _large.not_selected_pages();\n","filename":"src\/hotspot\/share\/gc\/z\/zRelocationSetSelector.inline.hpp","additions":80,"deletions":33,"binary":false,"changes":113,"status":"modified"},{"patch":"@@ -0,0 +1,591 @@\n+\/*\n+ * Copyright (c) 2021, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/z\/zAddress.inline.hpp\"\n+#include \"gc\/z\/zForwarding.inline.hpp\"\n+#include \"gc\/z\/zGeneration.inline.hpp\"\n+#include \"gc\/z\/zHeap.inline.hpp\"\n+#include \"gc\/z\/zIterator.inline.hpp\"\n+#include \"gc\/z\/zMark.hpp\"\n+#include \"gc\/z\/zPage.inline.hpp\"\n+#include \"gc\/z\/zPageTable.hpp\"\n+#include \"gc\/z\/zRemembered.inline.hpp\"\n+#include \"gc\/z\/zRememberedSet.hpp\"\n+#include \"gc\/z\/zTask.hpp\"\n+#include \"gc\/z\/zVerify.hpp\"\n+#include \"memory\/iterator.hpp\"\n+#include \"oops\/oop.inline.hpp\"\n+#include \"utilities\/bitMap.inline.hpp\"\n+#include \"utilities\/debug.hpp\"\n+\n+ZRemembered::ZRemembered(ZPageTable* page_table,\n+                         const ZForwardingTable* old_forwarding_table,\n+                         ZPageAllocator* page_allocator) :\n+    _page_table(page_table),\n+    _old_forwarding_table(old_forwarding_table),\n+    _page_allocator(page_allocator),\n+    _found_old() {\n+}\n+\n+template <typename Function>\n+void ZRemembered::oops_do_forwarded_via_containing(GrowableArrayView<ZRememberedSetContaining>* array, Function function) const {\n+  \/\/ The array contains duplicated from_addr values. Cache expensive operations.\n+  zaddress_unsafe from_addr = zaddress_unsafe::null;\n+  zaddress to_addr = zaddress::null;\n+  size_t object_size = 0;\n+\n+  for (const ZRememberedSetContaining containing: *array) {\n+    if (from_addr != containing._addr) {\n+      from_addr = containing._addr;\n+\n+      \/\/ Relocate object to new location\n+      to_addr = ZGeneration::old()->relocate_or_remap_object(from_addr);\n+\n+      \/\/ Figure out size\n+      object_size = ZUtils::object_size(to_addr);\n+    }\n+\n+    \/\/ Calculate how far into the from-object the remset entry is\n+    const uintptr_t field_offset = containing._field_addr - from_addr;\n+\n+    \/\/ The 'containing' could contain mismatched (addr, addr_field).\n+    \/\/ Need to check if the field was within the reported object.\n+    if (field_offset < object_size) {\n+      \/\/ Calculate the corresponding address in the to-object\n+      const zaddress to_addr_field = to_addr + field_offset;\n+\n+      function((volatile zpointer*)untype(to_addr_field));\n+    }\n+  }\n+}\n+\n+bool ZRemembered::should_scan_page(ZPage* page) const {\n+  if (!ZGeneration::old()->is_phase_relocate()) {\n+    \/\/ If the old generation collection is not in the relocation phase, then it\n+    \/\/ will not need any synchronization on its forwardings.\n+    return true;\n+  }\n+\n+  ZForwarding* const forwarding = ZGeneration::old()->forwarding(ZOffset::address_unsafe(page->start()));\n+\n+  if (forwarding == nullptr) {\n+    \/\/ This page was provably not part of the old relocation set\n+    return true;\n+  }\n+\n+  if (!forwarding->relocated_remembered_fields_is_concurrently_scanned()) {\n+    \/\/ Safe to scan\n+    return true;\n+  }\n+\n+  \/\/ If we get here, we know that the old collection is concurrently relocating\n+  \/\/ objects. We need to be extremely careful not to scan a page that is\n+  \/\/ concurrently being in-place relocated because it's objects and previous\n+  \/\/ bits could be concurrently be moving around.\n+  \/\/\n+  \/\/ Before calling this function ZRemembered::scan_forwarding ensures\n+  \/\/ that all forwardings that have not already been fully relocated,\n+  \/\/ will have had their \"previous\" remembered set bits scanned.\n+  \/\/\n+  \/\/ The current page we're currently scanning could either be the same page\n+  \/\/ that was found during scan_forwarding, or it could have been replaced\n+  \/\/ by a new \"allocating\" page. There are two situations we have to consider:\n+  \/\/\n+  \/\/ 1) If it is a proper new allocating page, then all objects where copied\n+  \/\/ after scan_forwarding ran, and we are guaranteed that no \"previous\"\n+  \/\/ remembered set bits are set. So, there's no need to scan this page.\n+  \/\/\n+  \/\/ 2) If this is an in-place relocated page, then the entire page could\n+  \/\/ be concurrently relocated. Meaning that both objects and previous\n+  \/\/ remembered set bits could be moving around. However, if the in-place\n+  \/\/ relocation is ongoing, we've already scanned all relevant \"previous\"\n+  \/\/ bits when calling scan_forwarding. So, this page *must* not be scanned.\n+  \/\/\n+  \/\/ Don't scan the page.\n+  return false;\n+}\n+\n+bool ZRemembered::scan_page(ZPage* page) const {\n+  const bool can_trust_live_bits =\n+      page->is_relocatable() && !ZGeneration::old()->is_phase_mark();\n+\n+  bool result = false;\n+\n+  if (!can_trust_live_bits) {\n+    \/\/ We don't have full liveness info - scan all remset entries\n+    page->log_msg(\" (scan_page_remembered)\");\n+    int count = 0;\n+    page->oops_do_remembered([&](volatile zpointer* p) {\n+      result |= scan_field(p);\n+      count++;\n+    });\n+    page->log_msg(\" (scan_page_remembered done: %d ignoring: \" PTR_FORMAT \" )\", count, p2i(page->remset_current()));\n+  } else if (page->is_marked()) {\n+    \/\/ We have full liveness info - Only scan remset entries in live objects\n+    page->log_msg(\" (scan_page_remembered_in_live)\");\n+    page->oops_do_remembered_in_live([&](volatile zpointer* p) {\n+      result |= scan_field(p);\n+    });\n+  } else {\n+    page->log_msg(\" (scan_page_remembered_dead)\");\n+    \/\/ All objects are dead - do nothing\n+  }\n+\n+  return result;\n+}\n+\n+static void fill_containing(GrowableArrayCHeap<ZRememberedSetContaining, mtGC>* array, ZPage* page) {\n+  page->log_msg(\" (fill_remembered_containing)\");\n+\n+  ZRememberedSetContainingIterator iter(page);\n+\n+  for (ZRememberedSetContaining containing; iter.next(&containing);) {\n+    array->push(containing);\n+  }\n+}\n+\n+struct ZRememberedScanForwardingContext {\n+  GrowableArrayCHeap<ZRememberedSetContaining, mtGC> _containing_array;\n+\n+  struct Where {\n+    static const int NumRecords = 10;\n+\n+    Tickspan _duration;\n+    int      _count;\n+    Tickspan _max_durations[NumRecords];\n+    int      _max_count;\n+\n+    Where() :\n+        _duration(),\n+        _count(),\n+        _max_durations(),\n+        _max_count() {}\n+\n+    void report(const Tickspan& duration) {\n+      _duration += duration;\n+      _count++;\n+\n+      \/\/ Install into max array\n+      for (int i = 0; i < NumRecords; i++) {\n+        if (duration > _max_durations[i]) {\n+          \/\/ Slid to the side\n+          for (int j = _max_count - 1; i < j; j--) {\n+            _max_durations[j] = _max_durations[j - 1];\n+          }\n+\n+          \/\/ Install\n+          _max_durations[i] = duration;\n+          if (_max_count < NumRecords) {\n+            _max_count++;\n+          }\n+          break;\n+        }\n+      }\n+    }\n+\n+    void print(const char* name) {\n+      log_debug(gc, remset)(\"Remset forwarding %s: %.3fms count: %d %s\",\n+          name, TimeHelper::counter_to_millis(_duration.value()), _count, Thread::current()->name());\n+      for (int i = 0; i < _max_count; i++) {\n+        log_debug(gc, remset)(\"  %.3fms\", TimeHelper::counter_to_millis(_max_durations[i].value()));\n+      }\n+    }\n+  };\n+\n+  Where _where[2];\n+\n+  ZRememberedScanForwardingContext() :\n+      _containing_array(),\n+      _where() {}\n+\n+  ~ZRememberedScanForwardingContext() {\n+    print();\n+  }\n+\n+  void report_retained(const Tickspan& duration) {\n+    _where[0].report(duration);\n+  }\n+\n+  void report_released(const Tickspan& duration) {\n+    _where[1].report(duration);\n+  }\n+\n+  void print() {\n+    _where[0].print(\"retained\");\n+    _where[1].print(\"released\");\n+  }\n+};\n+\n+struct ZRememberedScanForwardingMeasureRetained {\n+  ZRememberedScanForwardingContext* _context;\n+  Ticks                             _start;\n+\n+  ZRememberedScanForwardingMeasureRetained(ZRememberedScanForwardingContext* context) :\n+      _context(context),\n+      _start(Ticks::now()) {\n+  }\n+\n+  ~ZRememberedScanForwardingMeasureRetained() {\n+    const Ticks end = Ticks::now();\n+    const Tickspan duration = end - _start;\n+    _context->report_retained(duration);\n+  }\n+};\n+\n+struct ZRememberedScanForwardingMeasureReleased {\n+  ZRememberedScanForwardingContext* _context;\n+  Ticks                             _start;\n+\n+  ZRememberedScanForwardingMeasureReleased(ZRememberedScanForwardingContext* context) :\n+      _context(context),\n+      _start(Ticks::now()) {\n+  }\n+\n+  ~ZRememberedScanForwardingMeasureReleased() {\n+    const Ticks end = Ticks::now();\n+    const Tickspan duration = end - _start;\n+    _context->report_released(duration);\n+  }\n+};\n+\n+bool ZRemembered::scan_forwarding(ZForwarding* forwarding, void* context_void) const {\n+  ZRememberedScanForwardingContext* const context = (ZRememberedScanForwardingContext*)context_void;\n+  bool result = false;\n+\n+  if (forwarding->retain_page(ZGeneration::old()->relocate_queue())) {\n+    ZRememberedScanForwardingMeasureRetained measure(context);\n+    forwarding->page()->log_msg(\" (scan_forwarding)\");\n+\n+    \/\/ We don't want to wait for the old relocation to finish and publish all\n+    \/\/ relocated remembered fields. Reject its fields and collect enough data\n+    \/\/ up-front.\n+    forwarding->relocated_remembered_fields_notify_concurrent_scan_of();\n+\n+    \/\/ Collect all remset info while the page is retained\n+    GrowableArrayCHeap<ZRememberedSetContaining, mtGC>* array = &context->_containing_array;\n+    array->clear();\n+    fill_containing(array, forwarding->page());\n+    forwarding->release_page();\n+\n+    \/\/ Relocate (and mark) while page is released, to prevent\n+    \/\/ retain deadlock when relocation threads in-place relocate.\n+    oops_do_forwarded_via_containing(array, [&](volatile zpointer* p) {\n+      result |= scan_field(p);\n+    });\n+\n+  } else {\n+    ZRememberedScanForwardingMeasureReleased measure(context);\n+\n+    \/\/ The page has been released. If the page was relocated while this young\n+    \/\/ generation collection was running, the old generation relocation will\n+    \/\/ have published all addresses of fields that had a remembered set entry.\n+    forwarding->relocated_remembered_fields_apply_to_published([&](volatile zpointer* p) {\n+      result |= scan_field(p);\n+    });\n+  }\n+\n+  return result;\n+}\n+\n+\/\/ When scanning the remembered set during the young generation marking, we\n+\/\/ want to visit all old pages. And we want that to be done in parallel and\n+\/\/ fast.\n+\/\/\n+\/\/ Walking over the entire page table and letting the workers claim indices\n+\/\/ have been shown to have scalability issues.\n+\/\/\n+\/\/ So, we have the \"found old\" optimization, which allows us to perform much\n+\/\/ fewer claims (order of old pages, instead of order of slots in the page\n+\/\/ table), and it allows us to read fewer pages.\n+\/\/\n+\/\/ The set of \"found old pages\" isn't precise, and can contain stale entries\n+\/\/ referring to slots of freed pages, or even slots where young pages have\n+\/\/ been installed. However, it will not lack any of the old pages.\n+\/\/\n+\/\/ The data is maintained very similar to when and how we maintain the\n+\/\/ remembered set bits: We keep two separates sets, one for read-only access\n+\/\/ by the young marking, and a currently active set where we register new\n+\/\/ pages. When pages get relocated, or die, the page table slot for that page\n+\/\/ must be cleared. This clearing is done just like we do with the remset\n+\/\/ scanning: The old entries are not copied to the current active set, only\n+\/\/ slots that were found to actually contain old pages are registered in the\n+\/\/ active set.\n+\n+ZRemembered::FoundOld::FoundOld() :\n+    \/\/ Array initialization requires copy constructors, which CHeapBitMap\n+    \/\/ doesn't provide. Instantiate two instances, and populate an array\n+    \/\/ with pointers to the two instances.\n+    _allocated_bitmap_0{ZAddressOffsetMax >> ZGranuleSizeShift, mtGC, true \/* clear *\/},\n+    _allocated_bitmap_1{ZAddressOffsetMax >> ZGranuleSizeShift, mtGC, true \/* clear *\/},\n+    _bitmaps{&_allocated_bitmap_0, &_allocated_bitmap_1},\n+    _current{0} {}\n+\n+BitMap* ZRemembered::FoundOld::current_bitmap() {\n+  return _bitmaps[_current];\n+}\n+\n+BitMap* ZRemembered::FoundOld::previous_bitmap() {\n+  return _bitmaps[_current ^ 1];\n+}\n+\n+void ZRemembered::FoundOld::flip() {\n+  _current ^= 1;\n+}\n+\n+void ZRemembered::FoundOld::clear_previous() {\n+  previous_bitmap()->clear_large();\n+}\n+\n+void ZRemembered::FoundOld::register_page(ZPage* page) {\n+  assert(page->is_old(), \"Only register old pages\");\n+  current_bitmap()->par_set_bit(untype(page->start()) >> ZGranuleSizeShift, memory_order_relaxed);\n+}\n+\n+void ZRemembered::flip_found_old_sets() {\n+  _found_old.flip();\n+}\n+\n+void ZRemembered::clear_found_old_previous_set() {\n+  _found_old.clear_previous();\n+}\n+\n+void ZRemembered::register_found_old(ZPage* page) {\n+  assert(page->is_old(), \"Should only register old pages\");\n+  _found_old.register_page(page);\n+}\n+\n+struct ZRemsetTableEntry {\n+  ZPage* _page;\n+  ZForwarding* _forwarding;\n+};\n+\n+class ZRemsetTableIterator {\n+private:\n+  ZRemembered* const            _remembered;\n+  ZPageTable* const             _page_table;\n+  const ZForwardingTable* const _old_forwarding_table;\n+  volatile BitMap::idx_t        _claimed;\n+\n+public:\n+  ZRemsetTableIterator(ZRemembered* remembered) :\n+      _remembered(remembered),\n+      _page_table(remembered->_page_table),\n+      _old_forwarding_table(remembered->_old_forwarding_table),\n+      _claimed(0) {}\n+\n+  \/\/ This iterator uses the \"found old\" optimization.\n+  bool next(ZRemsetTableEntry* entry_addr)  {\n+    BitMap* const bm = _remembered->_found_old.previous_bitmap();\n+\n+    BitMap::idx_t prev = Atomic::load(&_claimed);\n+\n+    for (;;) {\n+      if (prev == bm->size()) {\n+        return false;\n+      }\n+\n+      const BitMap::idx_t page_index = bm->find_first_set_bit(_claimed);\n+      if (page_index == bm->size()) {\n+        Atomic::cmpxchg(&_claimed, prev, page_index, memory_order_relaxed);\n+        return false;\n+      }\n+\n+      const BitMap::idx_t res = Atomic::cmpxchg(&_claimed, prev, page_index + 1, memory_order_relaxed);\n+      if (res != prev) {\n+        \/\/ Someone else claimed\n+        prev = res;\n+        continue;\n+      }\n+\n+      \/\/ Found bit - look around for page or forwarding to scan\n+\n+      ZForwarding* forwarding = nullptr;\n+      if (ZGeneration::old()->is_phase_relocate()) {\n+        forwarding = _old_forwarding_table->at(page_index);\n+      }\n+\n+      ZPage* page = _page_table->at(page_index);\n+      if (page != nullptr && !page->is_old()) {\n+        page = nullptr;\n+      }\n+\n+      if (page == nullptr && forwarding == nullptr) {\n+        \/\/ Nothing to scan\n+        continue;\n+      }\n+\n+      \/\/ Found old page or old forwarding\n+      entry_addr->_forwarding = forwarding;\n+      entry_addr->_page = page;\n+\n+      return true;\n+    }\n+  }\n+};\n+\n+\/\/ This task scans the remembered set and follows pointers when possible.\n+\/\/ Interleaving remembered set scanning with marking makes the marking times\n+\/\/ lower and more predictable.\n+class ZRememberedScanMarkFollowTask : public ZRestartableTask {\n+private:\n+  ZRemembered* const   _remembered;\n+  ZMark* const         _mark;\n+  ZRemsetTableIterator _remset_table_iterator;\n+\n+public:\n+  ZRememberedScanMarkFollowTask(ZRemembered* remembered, ZMark* mark) :\n+      ZRestartableTask(\"ZRememberedScanMarkFollowTask\"),\n+      _remembered(remembered),\n+      _mark(mark),\n+      _remset_table_iterator(remembered)  {\n+    _mark->prepare_work();\n+    _remembered->_page_allocator->enable_safe_destroy();\n+    _remembered->_page_allocator->enable_safe_recycle();\n+  }\n+\n+  ~ZRememberedScanMarkFollowTask() {\n+    _remembered->_page_allocator->disable_safe_recycle();\n+    _remembered->_page_allocator->disable_safe_destroy();\n+    _mark->finish_work();\n+    \/\/ We are done scanning the set of old pages.\n+    \/\/ Clear the set for the next young collection.\n+    _remembered->clear_found_old_previous_set();\n+  }\n+\n+  virtual void work_inner() {\n+    ZRememberedScanForwardingContext context;\n+\n+    \/\/ Follow initial roots\n+    if (!_mark->follow_work_partial()) {\n+      \/\/ Bail\n+      return;\n+    }\n+\n+    for (ZRemsetTableEntry entry; _remset_table_iterator.next(&entry);) {\n+      bool left_marking = false;\n+      ZForwarding* forwarding = entry._forwarding;\n+      ZPage* page = entry._page;\n+\n+      \/\/ Scan forwarding\n+      if (forwarding != nullptr) {\n+        bool found_roots = _remembered->scan_forwarding(forwarding, &context);\n+        ZVerify::after_scan(forwarding);\n+        if (found_roots) {\n+          \/\/ Follow remembered set when possible\n+          left_marking = !_mark->follow_work_partial();\n+        }\n+      }\n+\n+      \/\/ Scan page\n+      if (page != nullptr) {\n+        if (_remembered->should_scan_page(page)) {\n+          \/\/ Visit all entries pointing into young gen\n+          bool found_roots = _remembered->scan_page(page);\n+\n+          \/\/ ... and as a side-effect clear the previous entries\n+          if (ZVerifyRemembered) {\n+            \/\/ Make sure self healing of pointers is ordered before clearing of\n+            \/\/ the previous bits so that ZVerify::after_scan can detect missing\n+            \/\/ remset entries accurately.\n+            OrderAccess::storestore();\n+          }\n+          page->clear_remset_previous();\n+\n+          if (found_roots && !left_marking) {\n+            \/\/ Follow remembered set when possible\n+            left_marking = !_mark->follow_work_partial();\n+          }\n+        }\n+\n+        \/\/ The remset scanning maintains the \"maybe old\" pages optimization.\n+        \/\/\n+        \/\/ We maintain two sets of old pages: The first is the currently active\n+        \/\/ set, where old pages are registered into. The second is the old\n+        \/\/ read-only copy. The two sets flip during young mark start. This\n+        \/\/ analogous to how we set and clean remembered set bits.\n+        \/\/\n+        \/\/ The iterator reads from the read-only copy, and then here, we install\n+        \/\/ entries in the current active set.\n+        _remembered->register_found_old(page);\n+      }\n+\n+      SuspendibleThreadSet::yield();\n+      if (left_marking) {\n+        \/\/ Bail\n+        return;\n+      }\n+    }\n+\n+    _mark->follow_work_complete();\n+  }\n+\n+  virtual void work() {\n+    SuspendibleThreadSetJoiner sts_joiner;\n+    work_inner();\n+    \/\/ We might have found pointers into the other generation, and then we want to\n+    \/\/ publish such marking stacks to prevent that generation from getting a mark continue.\n+    \/\/ We also flush in case of a resize where a new worker thread continues the marking\n+    \/\/ work, causing a mark continue for the collected generation.\n+    ZHeap::heap()->mark_flush_and_free(Thread::current());\n+  }\n+\n+  virtual void resize_workers(uint nworkers) {\n+    _mark->resize_workers(nworkers);\n+  }\n+};\n+\n+void ZRemembered::scan_and_follow(ZMark* mark) {\n+  {\n+    \/\/ Follow the object graph and lazily scan the remembered set\n+    ZRememberedScanMarkFollowTask task(this, mark);\n+    ZGeneration::young()->workers()->run(&task);\n+\n+    \/\/ Try to terminate after following the graph\n+    if (ZAbort::should_abort() || !mark->try_terminate_flush()) {\n+      return;\n+    }\n+  }\n+\n+  \/\/ If flushing failed, we have to restart marking again, but this time we don't need to\n+  \/\/ scan the remembered set.\n+  mark->mark_follow();\n+}\n+\n+bool ZRemembered::scan_field(volatile zpointer* p) const {\n+  assert(ZGeneration::young()->is_phase_mark(), \"Wrong phase\");\n+\n+  const zaddress addr = ZBarrier::remset_barrier_on_oop_field(p);\n+\n+  if (!is_null(addr) && ZHeap::heap()->is_young(addr)) {\n+    remember(p);\n+    return true;\n+  }\n+\n+  return false;\n+}\n+\n+void ZRemembered::flip() {\n+  ZRememberedSet::flip();\n+  flip_found_old_sets();\n+}\n","filename":"src\/hotspot\/share\/gc\/z\/zRemembered.cpp","additions":591,"deletions":0,"binary":false,"changes":591,"status":"added"},{"patch":"@@ -0,0 +1,104 @@\n+\/*\n+ * Copyright (c) 2021, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_Z_ZREMEMBERED_HPP\n+#define SHARE_GC_Z_ZREMEMBERED_HPP\n+\n+#include \"gc\/z\/zAddress.hpp\"\n+#include \"utilities\/bitMap.hpp\"\n+\n+template <typename T> class GrowableArrayView;\n+class OopClosure;\n+class ZForwarding;\n+class ZForwardingTable;\n+class ZMark;\n+class ZPage;\n+class ZPageAllocator;\n+class ZPageTable;\n+struct ZRememberedSetContaining;\n+\n+class ZRemembered {\n+  friend class ZRememberedScanMarkFollowTask;\n+  friend class ZRemsetTableIterator;\n+\n+private:\n+  ZPageTable* const             _page_table;\n+  const ZForwardingTable* const _old_forwarding_table;\n+  ZPageAllocator* const         _page_allocator;\n+\n+  \/\/ Optimization aid for faster old pages iteration\n+  struct FoundOld {\n+    CHeapBitMap   _allocated_bitmap_0;\n+    CHeapBitMap   _allocated_bitmap_1;\n+    BitMap* const _bitmaps[2];\n+    int           _current;\n+\n+    FoundOld();\n+\n+    void flip();\n+    void clear_previous();\n+\n+    void register_page(ZPage* page);\n+\n+    BitMap* current_bitmap();\n+    BitMap* previous_bitmap();\n+  } _found_old;\n+\n+  \/\/ Old pages iteration optimization aid\n+  void flip_found_old_sets();\n+  void clear_found_old_previous_set();\n+\n+  template <typename Function>\n+  void oops_do_forwarded_via_containing(GrowableArrayView<ZRememberedSetContaining>* array, Function function) const;\n+\n+  bool should_scan_page(ZPage* page) const;\n+\n+  bool scan_page(ZPage* page) const;\n+  bool scan_forwarding(ZForwarding* forwarding, void* context) const;\n+\n+public:\n+  ZRemembered(ZPageTable* page_table,\n+              const ZForwardingTable* old_forwarding_table,\n+              ZPageAllocator* page_allocator);\n+\n+  \/\/ Add to remembered set\n+  void remember(volatile zpointer* p) const;\n+\n+  \/\/ Scan all remembered sets and follow\n+  void scan_and_follow(ZMark* mark);\n+\n+  \/\/ Save the current remembered sets,\n+  \/\/ and switch over to empty remembered sets.\n+  void flip();\n+\n+  \/\/ Scan a remembered set entry\n+  bool scan_field(volatile zpointer* p) const;\n+\n+  \/\/ Verification\n+  bool is_remembered(volatile zpointer* p) const;\n+\n+  \/\/ Register pages with the remembered set\n+  void register_found_old(ZPage* page);\n+};\n+\n+#endif \/\/ SHARE_GC_Z_ZREMEMBERED_HPP\n","filename":"src\/hotspot\/share\/gc\/z\/zRemembered.hpp","additions":104,"deletions":0,"binary":false,"changes":104,"status":"added"},{"patch":"@@ -0,0 +1,46 @@\n+\/*\n+ * Copyright (c) 2021, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_Z_ZREMEMBERED_INLINE_HPP\n+#define SHARE_GC_Z_ZREMEMBERED_INLINE_HPP\n+\n+#include \"gc\/z\/zRemembered.hpp\"\n+\n+#include \"gc\/z\/zBarrier.inline.hpp\"\n+#include \"gc\/z\/zHeap.inline.hpp\"\n+#include \"gc\/z\/zPage.inline.hpp\"\n+#include \"gc\/z\/zPageTable.inline.hpp\"\n+\n+inline void ZRemembered::remember(volatile zpointer* p) const {\n+  ZPage* page = _page_table->get(p);\n+  assert(page != nullptr,  \"Page missing in page table\");\n+  page->remember(p);\n+}\n+\n+inline bool ZRemembered::is_remembered(volatile zpointer* p) const {\n+  ZPage* page = _page_table->get(p);\n+  assert(page != nullptr,  \"Page missing in page table\");\n+  return page->is_remembered(p);\n+}\n+\n+#endif \/\/ SHARE_GC_Z_ZREMEMBERED_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/z\/zRemembered.inline.hpp","additions":46,"deletions":0,"binary":false,"changes":46,"status":"added"},{"patch":"@@ -0,0 +1,227 @@\n+\/*\n+ * Copyright (c) 2021, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/z\/zAddress.inline.hpp\"\n+#include \"gc\/z\/zBitMap.inline.hpp\"\n+#include \"gc\/z\/zHeap.inline.hpp\"\n+#include \"gc\/z\/zPage.inline.hpp\"\n+#include \"gc\/z\/zRememberedSet.hpp\"\n+#include \"logging\/log.hpp\"\n+#include \"memory\/allocation.hpp\"\n+#include \"memory\/iterator.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+int ZRememberedSet::_current = 0;\n+\n+void ZRememberedSet::flip() {\n+  _current ^= 1;\n+}\n+\n+ZRememberedSet::ZRememberedSet() :\n+    _bitmap{ZMovableBitMap(), ZMovableBitMap()} {\n+  \/\/ Defer initialization of the bitmaps until the owning\n+  \/\/ page becomes old and its remembered set is initialized.\n+}\n+\n+bool ZRememberedSet::is_initialized() const {\n+  return _bitmap[0].size() > 0;\n+}\n+\n+void ZRememberedSet::initialize(size_t page_size) {\n+  assert(!is_initialized(), \"precondition\");\n+  const BitMap::idx_t size_in_bits = to_bit_size(page_size);\n+  _bitmap[0].initialize(size_in_bits, true \/* clear *\/);\n+  _bitmap[1].initialize(size_in_bits, true \/* clear *\/);\n+}\n+\n+void ZRememberedSet::resize(size_t page_size) {\n+  \/\/ The bitmaps only need to be resized if remset has been\n+  \/\/ initialized, and hence the bitmaps have been initialized.\n+  if (is_initialized()) {\n+    const BitMap::idx_t size_in_bits = to_bit_size(page_size);\n+\n+    \/\/ The bitmaps need to be cleared when free, but since this function is\n+    \/\/ only used for shrinking the clear argument is correct but not crucial.\n+    assert(size_in_bits <= _bitmap[0].size(), \"Only used for shrinking\");\n+    _bitmap[0].resize(size_in_bits, true \/* clear *\/);\n+    _bitmap[1].resize(size_in_bits, true \/* clear *\/);\n+  }\n+}\n+\n+bool ZRememberedSet::is_cleared_current() const {\n+  return current()->is_empty();\n+}\n+\n+bool ZRememberedSet::is_cleared_previous() const {\n+  return previous()->is_empty();\n+}\n+\n+void ZRememberedSet::clear_all() {\n+  clear_current();\n+  clear_previous();\n+}\n+\n+void ZRememberedSet::clear_current() {\n+  current()->clear_large();\n+}\n+\n+void ZRememberedSet::clear_previous() {\n+  previous()->clear_large();\n+}\n+\n+void ZRememberedSet::swap_remset_bitmaps() {\n+  assert(previous()->is_empty(), \"Previous remset bits should be empty when swapping\");\n+  current()->iterate([&](BitMap::idx_t index) {\n+    previous()->set_bit(index);\n+    return true;\n+  });\n+  current()->clear_large();\n+}\n+\n+ZBitMap::ReverseIterator ZRememberedSet::iterator_reverse_previous() {\n+  return ZBitMap::ReverseIterator(previous());\n+}\n+\n+BitMap::Iterator ZRememberedSet::iterator_limited_current(uintptr_t offset, size_t size) {\n+  const size_t index = to_index(offset);;\n+  const size_t bit_size = to_bit_size(size);\n+\n+  return BitMap::Iterator(*current(), index, index + bit_size);\n+}\n+\n+ZBitMap::Iterator ZRememberedSet::iterator_limited_previous(uintptr_t offset, size_t size) {\n+  const size_t index = to_index(offset);;\n+  const size_t bit_size = to_bit_size(size);\n+\n+  return BitMap::Iterator(*previous(), index, index + bit_size);\n+}\n+\n+size_t ZRememberedSetContainingIterator::to_index(zaddress_unsafe addr) {\n+  const uintptr_t local_offset = _page->local_offset(addr);\n+  return ZRememberedSet::to_index(local_offset);\n+}\n+\n+zaddress_unsafe ZRememberedSetContainingIterator::to_addr(BitMap::idx_t index) {\n+  const uintptr_t local_offset = ZRememberedSet::to_offset(index);\n+  return ZOffset::address_unsafe(_page->global_offset(local_offset));\n+}\n+\n+ZRememberedSetContainingIterator::ZRememberedSetContainingIterator(ZPage* page) :\n+    _page(page),\n+    _remset_iter(page->remset_reverse_iterator_previous()),\n+    _obj(zaddress_unsafe::null),\n+    _obj_remset_iter(page->remset_reverse_iterator_previous()) {}\n+\n+bool ZRememberedSetContainingIterator::next(ZRememberedSetContaining* containing) {\n+  \/\/ Note: to skip having to read the contents of the heap, when collecting the\n+  \/\/ containing information, this code doesn't read the size of the objects and\n+  \/\/ therefore doesn't filter out remset bits that belong to dead objects.\n+  \/\/ The (addr, addr_field) pair will contain the nearest live object, of a\n+  \/\/ given remset bit. Users of 'containing' need to do the filtering.\n+\n+  BitMap::idx_t index;\n+\n+  if (!is_null(_obj)) {\n+    \/\/ We've already found a remset bit and likely owning object in the main\n+    \/\/ iterator. Now use that information to skip having to search for the\n+    \/\/ same object multiple times.\n+\n+    if (_obj_remset_iter.next(&index)) {\n+      containing->_field_addr = to_addr(index);\n+      containing->_addr = _obj;\n+\n+      log_develop_trace(gc, remset)(\"Remset Containing Obj  index: \" PTR_FORMAT \" base: \" PTR_FORMAT \" field: \" PTR_FORMAT, index, untype(containing->_addr), untype(containing->_field_addr));\n+\n+      return true;\n+    } else {\n+      \/\/ No more remset bits in the scanned object\n+      _obj = zaddress_unsafe::null;\n+    }\n+  }\n+\n+  \/\/ At this point, we don't know where the nearest earlier object starts.\n+  \/\/ Search for the next earlier remset bit, and then search for the likely\n+  \/\/ owning object.\n+  if (_remset_iter.next(&index)) {\n+    containing->_field_addr = to_addr(index);\n+    containing->_addr = _page->find_base((volatile zpointer*)untype(containing->_field_addr));\n+\n+    if (is_null(containing->_addr)) {\n+      \/\/ Found no live object\n+      return false;\n+    }\n+\n+    \/\/ Found live object. Not necessarily the one that originally owned the remset bit.\n+    const BitMap::idx_t obj_index = to_index(containing->_addr);\n+\n+    log_develop_trace(gc, remset)(\"Remset Containing Main index: \" PTR_FORMAT \" base: \" PTR_FORMAT \" field: \" PTR_FORMAT, index, untype(containing->_addr), untype(containing->_field_addr));\n+\n+    \/\/ Don't scan inside the object in the main iterator\n+    _remset_iter.reset(obj_index);\n+\n+    \/\/ Scan inside the object iterator\n+    _obj = containing->_addr;\n+    _obj_remset_iter.reset(obj_index, index);\n+\n+    return true;\n+  }\n+\n+  return false;\n+}\n+\n+ZRememberedSetContainingInLiveIterator::ZRememberedSetContainingInLiveIterator(ZPage* page) :\n+    _iter(page),\n+    _addr(zaddress::null),\n+    _addr_size(0),\n+    _count(0),\n+    _count_skipped(0),\n+    _page(page) {}\n+\n+bool ZRememberedSetContainingInLiveIterator::next(ZRememberedSetContaining* containing) {\n+  ZRememberedSetContaining local;\n+  while (_iter.next(&local)) {\n+    const zaddress local_addr = safe(local._addr);\n+    if (local_addr != _addr) {\n+      _addr = local_addr;\n+      _addr_size = ZUtils::object_size(_addr);\n+    }\n+\n+    const size_t field_offset = safe(local._field_addr) - _addr;\n+    if (field_offset < _addr_size) {\n+      *containing = local;\n+      _count++;\n+      return true;\n+    }\n+\n+    \/\/ Skip field outside object\n+    _count_skipped++;\n+  }\n+\n+  \/\/ No more entries found\n+  return false;\n+}\n+\n+void ZRememberedSetContainingInLiveIterator::print_statistics() const {\n+  _page->log_msg(\" (remembered iter count: \" SIZE_FORMAT \" skipped: \" SIZE_FORMAT \")\", _count, _count_skipped);\n+}\n","filename":"src\/hotspot\/share\/gc\/z\/zRememberedSet.cpp","additions":227,"deletions":0,"binary":false,"changes":227,"status":"added"},{"patch":"@@ -0,0 +1,146 @@\n+\/*\n+ * Copyright (c) 2021, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_Z_ZREMEMBEREDSET_HPP\n+#define SHARE_GC_Z_ZREMEMBEREDSET_HPP\n+\n+#include \"gc\/z\/zAddress.hpp\"\n+#include \"gc\/z\/zBitMap.hpp\"\n+\n+class OopClosure;\n+class ZPage;\n+\n+struct ZRememberedSetContaining {\n+  zaddress_unsafe _field_addr;\n+  zaddress_unsafe _addr;\n+};\n+\n+\/\/ Iterates over all (object, oop fields) pairs where the field address has\n+\/\/ been marked as remembered, and fill in that information in a\n+\/\/ ZRememberedSetContaining\n+\/\/\n+\/\/ Note that it's not guaranteed that _field_addr belongs to the recorded\n+\/\/ _addr. The entry could denote a stale remembered set field and _addr could\n+\/\/ just be the nearest object. The users are responsible for filtering that\n+\/\/ out.\n+class ZRememberedSetContainingIterator {\n+private:\n+  ZPage* const             _page;\n+  ZBitMap::ReverseIterator _remset_iter;\n+\n+  zaddress_unsafe          _obj;\n+  ZBitMap::ReverseIterator _obj_remset_iter;\n+\n+  size_t to_index(zaddress_unsafe addr);\n+  zaddress_unsafe to_addr(BitMap::idx_t index);\n+\n+public:\n+  ZRememberedSetContainingIterator(ZPage* page);\n+\n+  bool next(ZRememberedSetContaining* containing);\n+};\n+\n+\/\/ Like ZRememberedSetContainingIterator, but with stale remembered set fields\n+\/\/ filtered out.\n+class ZRememberedSetContainingInLiveIterator {\n+private:\n+  ZRememberedSetContainingIterator _iter;\n+  zaddress                         _addr;\n+  size_t                           _addr_size;\n+  size_t                           _count;\n+  size_t                           _count_skipped;\n+  ZPage* const                     _page;\n+\n+public:\n+  ZRememberedSetContainingInLiveIterator(ZPage* page);\n+\n+  bool next(ZRememberedSetContaining* containing);\n+\n+  void print_statistics() const;\n+};\n+\n+\/\/ The remembered set of a ZPage.\n+\/\/\n+\/\/ There's one bit per potential object field address within the ZPage.\n+\/\/\n+\/\/ New entries are added to the \"current\" active bitmap, while the\n+\/\/ \"previous\" bitmap is used by the GC to find pointers from old\n+\/\/ gen to young gen.\n+class ZRememberedSet {\n+  friend class ZRememberedSetContainingIterator;\n+\n+public:\n+  static int _current;\n+\n+  ZMovableBitMap _bitmap[2];\n+\n+  CHeapBitMap* current();\n+  const CHeapBitMap* current() const;\n+\n+  CHeapBitMap* previous();\n+  const CHeapBitMap* previous() const;\n+\n+  template <typename Function>\n+  void iterate_bitmap(Function function, CHeapBitMap* bitmap);\n+\n+  static uintptr_t to_offset(BitMap::idx_t index);\n+  static BitMap::idx_t to_index(uintptr_t offset);\n+  static BitMap::idx_t to_bit_size(size_t size);\n+\n+public:\n+  static void flip();\n+\n+  ZRememberedSet();\n+\n+  bool is_initialized() const;\n+  void initialize(size_t page_size);\n+\n+  void resize(size_t page_size);\n+\n+  bool at_current(uintptr_t offset) const;\n+  bool at_previous(uintptr_t offset) const;\n+  bool set_current(uintptr_t offset);\n+  void unset_non_par_current(uintptr_t offset);\n+  void unset_range_non_par_current(uintptr_t offset, size_t size);\n+\n+  \/\/ Visit all set offsets.\n+  template <typename Function \/* void(uintptr_t offset) *\/>\n+  void iterate_previous(Function function);\n+\n+  template <typename Function \/* void(uintptr_t offset) *\/>\n+  void iterate_current(Function function);\n+\n+  bool is_cleared_current() const;\n+  bool is_cleared_previous() const;\n+\n+  void clear_all();\n+  void clear_current();\n+  void clear_previous();\n+  void swap_remset_bitmaps();\n+\n+  ZBitMap::ReverseIterator iterator_reverse_previous();\n+  BitMap::Iterator iterator_limited_current(uintptr_t offset, size_t size);\n+  BitMap::Iterator iterator_limited_previous(uintptr_t offset, size_t size);\n+};\n+\n+#endif \/\/ SHARE_GC_Z_ZREMEMBEREDSET_HPP\n","filename":"src\/hotspot\/share\/gc\/z\/zRememberedSet.hpp","additions":146,"deletions":0,"binary":false,"changes":146,"status":"added"},{"patch":"@@ -0,0 +1,108 @@\n+\/*\n+ * Copyright (c) 2021, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_Z_ZREMEMBEREDSET_INLINE_HPP\n+#define SHARE_GC_Z_ZREMEMBEREDSET_INLINE_HPP\n+\n+#include \"gc\/z\/zRememberedSet.hpp\"\n+\n+#include \"utilities\/bitMap.inline.hpp\"\n+\n+inline CHeapBitMap* ZRememberedSet::current() {\n+  return &_bitmap[_current];\n+}\n+\n+inline const CHeapBitMap* ZRememberedSet::current() const {\n+  return &_bitmap[_current];\n+}\n+\n+inline CHeapBitMap* ZRememberedSet::previous() {\n+  return &_bitmap[_current ^ 1];\n+}\n+\n+inline const CHeapBitMap* ZRememberedSet::previous() const {\n+  return &_bitmap[_current ^ 1];\n+}\n+\n+inline uintptr_t ZRememberedSet::to_offset(BitMap::idx_t index) {\n+  \/\/ One bit per possible oop* address\n+  return index * oopSize;\n+}\n+\n+inline BitMap::idx_t ZRememberedSet::to_index(uintptr_t offset) {\n+  \/\/ One bit per possible oop* address\n+  return offset \/ oopSize;\n+}\n+\n+inline BitMap::idx_t ZRememberedSet::to_bit_size(size_t size) {\n+  return size \/ oopSize;\n+}\n+\n+inline bool ZRememberedSet::at_current(uintptr_t offset) const {\n+  const BitMap::idx_t index = to_index(offset);\n+  return current()->at(index);\n+}\n+\n+inline bool ZRememberedSet::at_previous(uintptr_t offset) const {\n+  const BitMap::idx_t index = to_index(offset);\n+  return previous()->at(index);\n+}\n+\n+inline bool ZRememberedSet::set_current(uintptr_t offset) {\n+  const BitMap::idx_t index = to_index(offset);\n+  return current()->par_set_bit(index, memory_order_relaxed);\n+}\n+\n+inline void ZRememberedSet::unset_non_par_current(uintptr_t offset) {\n+  const BitMap::idx_t index = to_index(offset);\n+  current()->clear_bit(index);\n+}\n+\n+inline void ZRememberedSet::unset_range_non_par_current(uintptr_t offset, size_t size) {\n+  const BitMap::idx_t start_index = to_index(offset);\n+  const BitMap::idx_t end_index = to_index(offset + size);\n+  current()->clear_range(start_index, end_index);\n+}\n+\n+template <typename Function>\n+void ZRememberedSet::iterate_bitmap(Function function, CHeapBitMap* bitmap) {\n+  bitmap->iterate([&](BitMap::idx_t index) {\n+    const uintptr_t offset = to_offset(index);\n+\n+    function(offset);\n+\n+    return true;\n+  });\n+}\n+\n+template <typename Function>\n+void ZRememberedSet::iterate_previous(Function function) {\n+  iterate_bitmap(function, previous());\n+}\n+\n+template <typename Function>\n+void ZRememberedSet::iterate_current(Function function) {\n+  iterate_bitmap(function, current());\n+}\n+\n+#endif \/\/ SHARE_GC_Z_ZREMEMBEREDSET_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/z\/zRememberedSet.inline.hpp","additions":108,"deletions":0,"binary":false,"changes":108,"status":"added"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -38,5 +38,53 @@\n-static const ZStatSubPhase ZSubPhaseConcurrentRootsOopStorageSet(\"Concurrent Roots OopStorageSet\");\n-static const ZStatSubPhase ZSubPhaseConcurrentRootsClassLoaderDataGraph(\"Concurrent Roots ClassLoaderDataGraph\");\n-static const ZStatSubPhase ZSubPhaseConcurrentRootsJavaThreads(\"Concurrent Roots JavaThreads\");\n-static const ZStatSubPhase ZSubPhaseConcurrentRootsCodeCache(\"Concurrent Roots CodeCache\");\n-static const ZStatSubPhase ZSubPhaseConcurrentWeakRootsOopStorageSet(\"Concurrent Weak Roots OopStorageSet\");\n+class ZRootStatSubPhase {\n+private:\n+  ZStatSubPhase _young;\n+  ZStatSubPhase _old;\n+\n+public:\n+  ZRootStatSubPhase(const char* name) :\n+     _young(name, ZGenerationId::young),\n+     _old(name, ZGenerationId::old) {}\n+\n+  const ZStatSubPhase& young() const { return _young; }\n+  const ZStatSubPhase& old() const { return _old; }\n+};\n+\n+static const ZRootStatSubPhase ZSubPhaseConcurrentRootsOopStorageSet(\"Concurrent Roots OopStorageSet\");\n+static const ZRootStatSubPhase ZSubPhaseConcurrentRootsClassLoaderDataGraph(\"Concurrent Roots ClassLoaderDataGraph\");\n+static const ZRootStatSubPhase ZSubPhaseConcurrentRootsJavaThreads(\"Concurrent Roots JavaThreads\");\n+static const ZRootStatSubPhase ZSubPhaseConcurrentRootsCodeCache(\"Concurrent Roots CodeCache\");\n+static const ZRootStatSubPhase ZSubPhaseConcurrentWeakRootsOopStorageSet(\"Concurrent Weak Roots OopStorageSet\");\n+\n+class ZRootStatTimer {\n+private:\n+  const ZStatPhase*  _phase;\n+  const Ticks        _start;\n+\n+  ZRootStatTimer(const ZStatPhase* phase) :\n+      _phase(phase),\n+      _start(Ticks::now()) {\n+    if (phase != nullptr) {\n+      _phase->register_start(nullptr \/* timer *\/, _start);\n+    }\n+  }\n+\n+public:\n+  ~ZRootStatTimer() {\n+    if (_phase != nullptr) {\n+      const Ticks end = Ticks::now();\n+      _phase->register_end(nullptr \/* timer *\/, _start, end);\n+    }\n+  }\n+\n+  static const ZStatSubPhase* calculate_subphase(const ZGenerationIdOptional generation, const ZRootStatSubPhase& subphase) {\n+    switch (generation) {\n+      case ZGenerationIdOptional::young: return &subphase.young();\n+      case ZGenerationIdOptional::old: return &subphase.old();\n+      default: return nullptr;\n+    }\n+  }\n+\n+public:\n+  ZRootStatTimer(const ZRootStatSubPhase& subphase, const ZGenerationIdOptional generation) :\n+      ZRootStatTimer(calculate_subphase(generation, subphase)) {}\n+};\n@@ -55,5 +103,2 @@\n-ZStrongOopStorageSetIterator::ZStrongOopStorageSetIterator() :\n-    _iter() {}\n-\n-void ZStrongOopStorageSetIterator::apply(OopClosure* cl) {\n-  ZStatTimer timer(ZSubPhaseConcurrentRootsOopStorageSet);\n+void ZOopStorageSetIteratorStrong::apply(OopClosure* cl) {\n+  ZRootStatTimer timer(ZSubPhaseConcurrentWeakRootsOopStorageSet, _generation);\n@@ -63,2 +108,2 @@\n-void ZStrongCLDsIterator::apply(CLDClosure* cl) {\n-  ZStatTimer timer(ZSubPhaseConcurrentRootsClassLoaderDataGraph);\n+void ZCLDsIteratorStrong::apply(CLDClosure* cl) {\n+  ZRootStatTimer timer(ZSubPhaseConcurrentRootsClassLoaderDataGraph, _generation);\n@@ -68,3 +113,9 @@\n-ZJavaThreadsIterator::ZJavaThreadsIterator() :\n-    _threads(),\n-    _claimed(0) {}\n+void ZCLDsIteratorWeak::apply(CLDClosure* cl) {\n+  ZRootStatTimer timer(ZSubPhaseConcurrentRootsClassLoaderDataGraph, _generation);\n+  ClassLoaderDataGraph::roots_cld_do(nullptr \/* strong *\/, cl \/* weak *\/);\n+}\n+\n+void ZCLDsIteratorAll::apply(CLDClosure* cl) {\n+  ZRootStatTimer timer(ZSubPhaseConcurrentRootsClassLoaderDataGraph, _generation);\n+  ClassLoaderDataGraph::cld_do(cl);\n+}\n@@ -77,1 +128,1 @@\n-  ZStatTimer timer(ZSubPhaseConcurrentRootsJavaThreads);\n+  ZRootStatTimer timer(ZSubPhaseConcurrentRootsJavaThreads, _generation);\n@@ -82,1 +133,1 @@\n-  ResourceMark                 _rm;\n+  ResourceMark rm;\n@@ -89,3 +140,6 @@\n-ZNMethodsIterator::ZNMethodsIterator() {\n-  if (!ClassUnloading) {\n-    ZNMethod::nmethods_do_begin();\n+ZNMethodsIteratorImpl::ZNMethodsIteratorImpl(ZGenerationIdOptional generation, bool enabled, bool secondary) :\n+    _enabled(enabled),\n+    _secondary(secondary),\n+    _generation(generation) {\n+  if (_enabled) {\n+    ZNMethod::nmethods_do_begin(secondary);\n@@ -95,3 +149,3 @@\n-ZNMethodsIterator::~ZNMethodsIterator() {\n-  if (!ClassUnloading) {\n-    ZNMethod::nmethods_do_end();\n+ZNMethodsIteratorImpl::~ZNMethodsIteratorImpl() {\n+  if (_enabled) {\n+    ZNMethod::nmethods_do_end(_secondary);\n@@ -101,3 +155,3 @@\n-void ZNMethodsIterator::apply(NMethodClosure* cl) {\n-  ZStatTimer timer(ZSubPhaseConcurrentRootsCodeCache);\n-  ZNMethod::nmethods_do(cl);\n+void ZNMethodsIteratorImpl::apply(NMethodClosure* cl) {\n+  ZRootStatTimer timer(ZSubPhaseConcurrentRootsCodeCache, _generation);\n+  ZNMethod::nmethods_do(_secondary, cl);\n@@ -106,4 +160,4 @@\n-ZRootsIterator::ZRootsIterator(int cld_claim) {\n-  if (cld_claim != ClassLoaderData::_claim_none) {\n-    ClassLoaderDataGraph::verify_claimed_marks_cleared(cld_claim);\n-  }\n+void ZRootsIteratorStrongColored::apply(OopClosure* cl,\n+                                  CLDClosure* cld_cl) {\n+  _oop_storage_set_strong.apply(cl);\n+  _clds_strong.apply(cld_cl);\n@@ -112,6 +166,2 @@\n-void ZRootsIterator::apply(OopClosure* cl,\n-                           CLDClosure* cld_cl,\n-                           ThreadClosure* thread_cl,\n-                           NMethodClosure* nm_cl) {\n-  _oop_storage_set.apply(cl);\n-  _class_loader_data_graph.apply(cld_cl);\n+void ZRootsIteratorStrongUncolored::apply(ThreadClosure* thread_cl,\n+                                          NMethodClosure* nm_cl) {\n@@ -120,1 +170,1 @@\n-    _nmethods.apply(nm_cl);\n+    _nmethods_strong.apply(nm_cl);\n@@ -124,2 +174,3 @@\n-ZWeakOopStorageSetIterator::ZWeakOopStorageSetIterator() :\n-    _iter() {}\n+void ZRootsIteratorWeakUncolored::apply(NMethodClosure* nm_cl) {\n+  _nmethods_weak.apply(nm_cl);\n+}\n@@ -127,2 +178,2 @@\n-void ZWeakOopStorageSetIterator::apply(OopClosure* cl) {\n-  ZStatTimer timer(ZSubPhaseConcurrentWeakRootsOopStorageSet);\n+void ZOopStorageSetIteratorWeak::apply(OopClosure* cl) {\n+  ZRootStatTimer timer(ZSubPhaseConcurrentWeakRootsOopStorageSet, _generation);\n@@ -132,1 +183,1 @@\n-void ZWeakOopStorageSetIterator::report_num_dead() {\n+void ZOopStorageSetIteratorWeak::report_num_dead() {\n@@ -136,2 +187,2 @@\n-void ZWeakRootsIterator::report_num_dead() {\n-  _oop_storage_set.iter().report_num_dead();\n+void ZRootsIteratorWeakColored::report_num_dead() {\n+  _oop_storage_set_weak.iter().report_num_dead();\n@@ -140,2 +191,15 @@\n-void ZWeakRootsIterator::apply(OopClosure* cl) {\n-  _oop_storage_set.apply(cl);\n+void ZRootsIteratorWeakColored::apply(OopClosure* cl) {\n+  _oop_storage_set_weak.apply(cl);\n+}\n+\n+void ZRootsIteratorAllColored::apply(OopClosure* cl,\n+                                     CLDClosure* cld_cl) {\n+  _oop_storage_set_strong.apply(cl);\n+  _oop_storage_set_weak.apply(cl);\n+  _clds_all.apply(cld_cl);\n+}\n+\n+void ZRootsIteratorAllUncolored::apply(ThreadClosure* thread_cl,\n+                                       NMethodClosure* nm_cl) {\n+  _java_threads.apply(thread_cl);\n+  _nmethods_all.apply(nm_cl);\n","filename":"src\/hotspot\/share\/gc\/z\/zRootsIterator.cpp","additions":111,"deletions":47,"binary":false,"changes":158,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,0 +28,1 @@\n+#include \"gc\/z\/zGenerationId.hpp\"\n@@ -39,2 +40,2 @@\n-  ZParallelApply() :\n-      _iter(),\n+  ZParallelApply(ZGenerationIdOptional generation) :\n+      _iter(generation),\n@@ -51,1 +52,2 @@\n-class ZStrongOopStorageSetIterator {\n+class ZOopStorageSetIteratorStrong {\n+private:\n@@ -53,0 +55,14 @@\n+  const ZGenerationIdOptional _generation;\n+\n+public:\n+  ZOopStorageSetIteratorStrong(ZGenerationIdOptional generation) :\n+      _iter(),\n+      _generation(generation) {}\n+\n+  void apply(OopClosure* cl);\n+};\n+\n+class ZOopStorageSetIteratorWeak {\n+private:\n+  OopStorageSetWeakParState<true \/* concurrent *\/, false \/* is_const *\/> _iter;\n+  const ZGenerationIdOptional _generation;\n@@ -55,1 +71,3 @@\n-  ZStrongOopStorageSetIterator();\n+  ZOopStorageSetIteratorWeak(ZGenerationIdOptional generation) :\n+      _iter(),\n+      _generation(generation) {}\n@@ -58,0 +76,2 @@\n+\n+  void report_num_dead();\n@@ -60,1 +80,4 @@\n-class ZStrongCLDsIterator {\n+class ZCLDsIteratorStrong {\n+private:\n+  const ZGenerationIdOptional _generation;\n+\n@@ -62,0 +85,25 @@\n+  ZCLDsIteratorStrong(ZGenerationIdOptional generation) :\n+      _generation(generation) {}\n+\n+  void apply(CLDClosure* cl);\n+};\n+\n+class ZCLDsIteratorWeak {\n+private:\n+  const ZGenerationIdOptional _generation;\n+\n+public:\n+  ZCLDsIteratorWeak(ZGenerationIdOptional generation) :\n+      _generation(generation) {}\n+\n+  void apply(CLDClosure* cl);\n+};\n+\n+class ZCLDsIteratorAll {\n+private:\n+  const ZGenerationIdOptional _generation;\n+\n+public:\n+  ZCLDsIteratorAll(ZGenerationIdOptional generation) :\n+      _generation(generation) {}\n+\n@@ -67,2 +115,3 @@\n-  ThreadsListHandle _threads;\n-  volatile uint     _claimed;\n+  ThreadsListHandle           _threads;\n+  volatile uint               _claimed;\n+  const ZGenerationIdOptional _generation;\n@@ -73,1 +122,4 @@\n-  ZJavaThreadsIterator();\n+  ZJavaThreadsIterator(ZGenerationIdOptional generation) :\n+      _threads(),\n+      _claimed(0),\n+      _generation(generation) {}\n@@ -78,4 +130,5 @@\n-class ZNMethodsIterator {\n-public:\n-  ZNMethodsIterator();\n-  ~ZNMethodsIterator();\n+class ZNMethodsIteratorImpl {\n+private:\n+  const bool                  _enabled;\n+  const bool                  _secondary;\n+  const ZGenerationIdOptional _generation;\n@@ -83,0 +136,5 @@\n+protected:\n+  ZNMethodsIteratorImpl(ZGenerationIdOptional generation, bool enabled, bool secondary);\n+  ~ZNMethodsIteratorImpl();\n+\n+public:\n@@ -86,1 +144,19 @@\n-class ZRootsIterator {\n+class ZNMethodsIteratorStrong : public ZNMethodsIteratorImpl {\n+public:\n+  ZNMethodsIteratorStrong(ZGenerationIdOptional generation) :\n+      ZNMethodsIteratorImpl(generation, !ClassUnloading \/* enabled *\/, false \/* secondary *\/) {}\n+};\n+\n+class ZNMethodsIteratorWeak : public ZNMethodsIteratorImpl {\n+public:\n+  ZNMethodsIteratorWeak(ZGenerationIdOptional generation) :\n+      ZNMethodsIteratorImpl(generation, true \/* enabled *\/, true \/* secondary *\/) {}\n+};\n+\n+class ZNMethodsIteratorAll : public ZNMethodsIteratorImpl {\n+public:\n+  ZNMethodsIteratorAll(ZGenerationIdOptional generation) :\n+      ZNMethodsIteratorImpl(generation, true \/* enabled *\/, true \/* secondary *\/) {}\n+};\n+\n+class ZRootsIteratorStrongUncolored {\n@@ -88,4 +164,2 @@\n-  ZParallelApply<ZStrongOopStorageSetIterator> _oop_storage_set;\n-  ZParallelApply<ZStrongCLDsIterator>          _class_loader_data_graph;\n-  ZParallelApply<ZJavaThreadsIterator>         _java_threads;\n-  ZParallelApply<ZNMethodsIterator>            _nmethods;\n+  ZParallelApply<ZJavaThreadsIterator>    _java_threads;\n+  ZParallelApply<ZNMethodsIteratorStrong> _nmethods_strong;\n@@ -94,1 +168,3 @@\n-  ZRootsIterator(int cld_claim);\n+  ZRootsIteratorStrongUncolored(ZGenerationIdOptional generation) :\n+      _java_threads(generation),\n+      _nmethods_strong(generation) {}\n@@ -96,3 +172,1 @@\n-  void apply(OopClosure* cl,\n-             CLDClosure* cld_cl,\n-             ThreadClosure* thread_cl,\n+  void apply(ThreadClosure* thread_cl,\n@@ -102,1 +176,1 @@\n-class ZWeakOopStorageSetIterator {\n+class ZRootsIteratorWeakUncolored {\n@@ -104,1 +178,1 @@\n-  OopStorageSetWeakParState<true \/* concurrent *\/, false \/* is_const *\/> _iter;\n+  ZParallelApply<ZNMethodsIteratorWeak> _nmethods_weak;\n@@ -107,1 +181,2 @@\n-  ZWeakOopStorageSetIterator();\n+  ZRootsIteratorWeakUncolored(ZGenerationIdOptional generation) :\n+      _nmethods_weak(generation) {}\n@@ -109,1 +184,2 @@\n-  void apply(OopClosure* cl);\n+  void apply(NMethodClosure* nm_cl);\n+};\n@@ -111,1 +187,26 @@\n-  void report_num_dead();\n+class ZRootsIteratorAllUncolored {\n+private:\n+  ZParallelApply<ZJavaThreadsIterator> _java_threads;\n+  ZParallelApply<ZNMethodsIteratorAll> _nmethods_all;\n+\n+public:\n+  ZRootsIteratorAllUncolored(ZGenerationIdOptional generation) :\n+      _java_threads(generation),\n+      _nmethods_all(generation) {}\n+\n+  void apply(ThreadClosure* thread_cl,\n+             NMethodClosure* nm_cl);\n+};\n+\n+class ZRootsIteratorStrongColored {\n+private:\n+  ZParallelApply<ZOopStorageSetIteratorStrong> _oop_storage_set_strong;\n+  ZParallelApply<ZCLDsIteratorStrong>          _clds_strong;\n+\n+public:\n+  ZRootsIteratorStrongColored(ZGenerationIdOptional generation) :\n+      _oop_storage_set_strong(generation),\n+      _clds_strong(generation) {}\n+\n+  void apply(OopClosure* cl,\n+             CLDClosure* cld_cl);\n@@ -114,1 +215,1 @@\n-class ZWeakRootsIterator {\n+class ZRootsIteratorWeakColored {\n@@ -116,1 +217,1 @@\n-  ZParallelApply<ZWeakOopStorageSetIterator> _oop_storage_set;\n+  ZParallelApply<ZOopStorageSetIteratorWeak> _oop_storage_set_weak;\n@@ -119,0 +220,3 @@\n+  ZRootsIteratorWeakColored(ZGenerationIdOptional generation) :\n+      _oop_storage_set_weak(generation) {}\n+\n@@ -124,0 +228,16 @@\n+class ZRootsIteratorAllColored {\n+private:\n+  ZParallelApply<ZOopStorageSetIteratorStrong> _oop_storage_set_strong;\n+  ZParallelApply<ZOopStorageSetIteratorWeak>   _oop_storage_set_weak;\n+  ZParallelApply<ZCLDsIteratorAll>             _clds_all;\n+\n+public:\n+  ZRootsIteratorAllColored(ZGenerationIdOptional generation) :\n+      _oop_storage_set_strong(generation),\n+      _oop_storage_set_weak(generation),\n+      _clds_all(generation) {}\n+\n+  void apply(OopClosure* cl,\n+             CLDClosure* cld_cl);\n+};\n+\n","filename":"src\/hotspot\/share\/gc\/z\/zRootsIterator.hpp","additions":149,"deletions":29,"binary":false,"changes":178,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,1 +27,0 @@\n-#include \"gc\/z\/zLock.inline.hpp\"\n@@ -29,2 +28,0 @@\n-#include \"gc\/z\/zTask.hpp\"\n-#include \"gc\/z\/zThread.hpp\"\n@@ -33,27 +30,0 @@\n-class ZRuntimeWorkersInitializeTask : public WorkerTask {\n-private:\n-  const uint     _nworkers;\n-  uint           _started;\n-  ZConditionLock _lock;\n-\n-public:\n-  ZRuntimeWorkersInitializeTask(uint nworkers) :\n-      WorkerTask(\"ZRuntimeWorkersInitializeTask\"),\n-      _nworkers(nworkers),\n-      _started(0),\n-      _lock() {}\n-\n-  virtual void work(uint worker_id) {\n-    \/\/ Wait for all threads to start\n-    ZLocker<ZConditionLock> locker(&_lock);\n-    if (++_started == _nworkers) {\n-      \/\/ All threads started\n-      _lock.notify_all();\n-    } else {\n-      while (_started != _nworkers) {\n-        _lock.wait();\n-      }\n-    }\n-  }\n-};\n-\n@@ -72,5 +42,0 @@\n-\n-  \/\/ Execute task to reduce latency in early safepoints,\n-  \/\/ which otherwise would have to take on any warmup costs.\n-  ZRuntimeWorkersInitializeTask task(_workers.max_workers());\n-  _workers.run_task(&task);\n","filename":"src\/hotspot\/share\/gc\/z\/zRuntimeWorkers.cpp","additions":1,"deletions":36,"binary":false,"changes":37,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2019, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,1 +28,0 @@\n-#include \"gc\/z\/zLock.hpp\"\n@@ -33,1 +32,1 @@\n-class ZSafeDeleteImpl {\n+class ZSafeDelete {\n@@ -37,3 +36,1 @@\n-  ZLock*         _lock;\n-  uint64_t       _enabled;\n-  ZArray<ItemT*> _deferred;\n+  ZActivatedArray<T> _deferred;\n@@ -41,2 +38,1 @@\n-  bool deferred_delete(ItemT* item);\n-  void immediate_delete(ItemT* item);\n+  static void immediate_delete(ItemT* item);\n@@ -45,1 +41,1 @@\n-  ZSafeDeleteImpl(ZLock* lock);\n+  explicit ZSafeDelete(bool locked = true);\n@@ -50,16 +46,1 @@\n-  void operator()(ItemT* item);\n-};\n-\n-template <typename T>\n-class ZSafeDelete : public ZSafeDeleteImpl<T> {\n-private:\n-  ZLock _lock;\n-\n-public:\n-  ZSafeDelete();\n-};\n-\n-template <typename T>\n-class ZSafeDeleteNoLock : public ZSafeDeleteImpl<T> {\n-public:\n-  ZSafeDeleteNoLock();\n+  void schedule_delete(ItemT* item);\n","filename":"src\/hotspot\/share\/gc\/z\/zSafeDelete.hpp","additions":6,"deletions":25,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2019, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -30,1 +30,0 @@\n-#include \"utilities\/debug.hpp\"\n@@ -35,4 +34,2 @@\n-ZSafeDeleteImpl<T>::ZSafeDeleteImpl(ZLock* lock) :\n-    _lock(lock),\n-    _enabled(0),\n-    _deferred() {}\n+ZSafeDelete<T>::ZSafeDelete(bool locked) :\n+    _deferred(locked) {}\n@@ -41,12 +38,1 @@\n-bool ZSafeDeleteImpl<T>::deferred_delete(ItemT* item) {\n-  ZLocker<ZLock> locker(_lock);\n-  if (_enabled > 0) {\n-    _deferred.append(item);\n-    return true;\n-  }\n-\n-  return false;\n-}\n-\n-template <typename T>\n-void ZSafeDeleteImpl<T>::immediate_delete(ItemT* item) {\n+void ZSafeDelete<T>::immediate_delete(ItemT* item) {\n@@ -61,3 +47,2 @@\n-void ZSafeDeleteImpl<T>::enable_deferred_delete() {\n-  ZLocker<ZLock> locker(_lock);\n-  _enabled++;\n+void ZSafeDelete<T>::enable_deferred_delete() {\n+  _deferred.activate();\n@@ -67,15 +52,2 @@\n-void ZSafeDeleteImpl<T>::disable_deferred_delete() {\n-  ZArray<ItemT*> deferred;\n-\n-  {\n-    ZLocker<ZLock> locker(_lock);\n-    assert(_enabled > 0, \"Invalid state\");\n-    if (--_enabled == 0) {\n-      deferred.swap(&_deferred);\n-    }\n-  }\n-\n-  ZArrayIterator<ItemT*> iter(&deferred);\n-  for (ItemT* item; iter.next(&item);) {\n-    immediate_delete(item);\n-  }\n+void ZSafeDelete<T>::disable_deferred_delete() {\n+  _deferred.deactivate_and_apply(immediate_delete);\n@@ -85,2 +57,2 @@\n-void ZSafeDeleteImpl<T>::operator()(ItemT* item) {\n-  if (!deferred_delete(item)) {\n+void ZSafeDelete<T>::schedule_delete(ItemT* item) {\n+  if (!_deferred.add_if_activated(item)) {\n@@ -91,9 +63,0 @@\n-template <typename T>\n-ZSafeDelete<T>::ZSafeDelete() :\n-    ZSafeDeleteImpl<T>(&_lock),\n-    _lock() {}\n-\n-template <typename T>\n-ZSafeDeleteNoLock<T>::ZSafeDeleteNoLock() :\n-    ZSafeDeleteImpl<T>(NULL) {}\n-\n","filename":"src\/hotspot\/share\/gc\/z\/zSafeDelete.inline.hpp","additions":10,"deletions":47,"binary":false,"changes":57,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,0 +28,1 @@\n+#include \"gc\/z\/zDriver.hpp\"\n@@ -33,0 +34,20 @@\n+struct ZMemoryUsageInfo {\n+  size_t _young_used;\n+  size_t _young_capacity;\n+  size_t _old_used;\n+  size_t _old_capacity;\n+};\n+\n+static ZMemoryUsageInfo compute_memory_usage_info() {\n+  const size_t capacity = ZHeap::heap()->capacity();\n+  const size_t old_used = ZHeap::heap()->used_old();\n+  const size_t young_used = ZHeap::heap()->used_young();\n+\n+  ZMemoryUsageInfo info;\n+  info._old_used = MIN2(old_used, capacity);\n+  info._old_capacity = info._old_used;\n+  info._young_capacity = capacity - info._old_capacity;\n+  info._young_used = MIN2(young_used, info._young_capacity);\n+  return info;\n+}\n+\n@@ -48,3 +69,6 @@\n-  ZGenerationCounters _generation_counters;\n-  HSpaceCounters      _space_counters;\n-  CollectorCounters   _collector_counters;\n+  ZGenerationCounters _generation_young_counters;\n+  ZGenerationCounters _generation_old_counters;\n+  HSpaceCounters      _space_young_counters;\n+  HSpaceCounters      _space_old_counters;\n+  CollectorCounters   _minor_collection_counters;\n+  CollectorCounters   _major_collection_counters;\n@@ -53,1 +77,1 @@\n-  ZServiceabilityCounters(size_t min_capacity, size_t max_capacity);\n+  ZServiceabilityCounters(size_t initial_capacity, size_t min_capacity, size_t max_capacity);\n@@ -55,1 +79,1 @@\n-  CollectorCounters* collector_counters();\n+  CollectorCounters* collector_counters(bool minor);\n@@ -60,1 +84,9 @@\n-ZServiceabilityCounters::ZServiceabilityCounters(size_t min_capacity, size_t max_capacity) :\n+ZServiceabilityCounters::ZServiceabilityCounters(size_t initial_capacity, size_t min_capacity, size_t max_capacity) :\n+    \/\/ generation.0\n+    _generation_young_counters(\n+        \"young\"          \/* name *\/,\n+        0                \/* ordinal *\/,\n+        1                \/* spaces *\/,\n+        min_capacity     \/* min_capacity *\/,\n+        max_capacity     \/* max_capacity *\/,\n+        initial_capacity \/* curr_capacity *\/),\n@@ -62,6 +94,14 @@\n-    _generation_counters(\"old\"        \/* name *\/,\n-                         1            \/* ordinal *\/,\n-                         1            \/* spaces *\/,\n-                         min_capacity \/* min_capacity *\/,\n-                         max_capacity \/* max_capacity *\/,\n-                         min_capacity \/* curr_capacity *\/),\n+    _generation_old_counters(\n+        \"old\"        \/* name *\/,\n+        1            \/* ordinal *\/,\n+        1            \/* spaces *\/,\n+        0            \/* min_capacity *\/,\n+        max_capacity \/* max_capacity *\/,\n+        0            \/* curr_capacity *\/),\n+    \/\/ generation.0.space.0\n+    _space_young_counters(\n+        _generation_young_counters.name_space(),\n+        \"space\"          \/* name *\/,\n+        0                \/* ordinal *\/,\n+        max_capacity     \/* max_capacity *\/,\n+        initial_capacity \/* init_capacity *\/),\n@@ -69,5 +109,10 @@\n-    _space_counters(_generation_counters.name_space(),\n-                    \"space\"      \/* name *\/,\n-                    0            \/* ordinal *\/,\n-                    max_capacity \/* max_capacity *\/,\n-                    min_capacity \/* init_capacity *\/),\n+    _space_old_counters(\n+        _generation_old_counters.name_space(),\n+        \"space\"      \/* name *\/,\n+        0            \/* ordinal *\/,\n+        max_capacity \/* max_capacity *\/,\n+        0            \/* init_capacity *\/),\n+    \/\/ gc.collector.0\n+    _minor_collection_counters(\n+        \"ZGC minor collection pauses\" \/* name *\/,\n+        0                             \/* ordinal *\/),\n@@ -75,2 +120,3 @@\n-    _collector_counters(\"Z concurrent cycle pauses\" \/* name *\/,\n-                        2                           \/* ordinal *\/) {}\n+    _major_collection_counters(\n+        \"ZGC major collection pauses\" \/* name *\/,\n+        2                             \/* ordinal *\/) {}\n@@ -78,2 +124,4 @@\n-CollectorCounters* ZServiceabilityCounters::collector_counters() {\n-  return &_collector_counters;\n+CollectorCounters* ZServiceabilityCounters::collector_counters(bool minor) {\n+  return minor\n+      ? &_minor_collection_counters\n+      : &_major_collection_counters;\n@@ -84,6 +132,7 @@\n-    const size_t capacity = ZHeap::heap()->capacity();\n-    const size_t used = MIN2(ZHeap::heap()->used(), capacity);\n-\n-    _generation_counters.update_capacity(capacity);\n-    _space_counters.update_capacity(capacity);\n-    _space_counters.update_used(used);\n+    const ZMemoryUsageInfo info = compute_memory_usage_info();\n+    _generation_young_counters.update_capacity(info._young_capacity);\n+    _generation_old_counters.update_capacity(info._old_capacity);\n+    _space_young_counters.update_capacity(info._young_capacity);\n+    _space_young_counters.update_used(info._young_used);\n+    _space_old_counters.update_capacity(info._old_capacity);\n+    _space_old_counters.update_used(info._old_used);\n@@ -95,2 +144,2 @@\n-ZServiceabilityMemoryPool::ZServiceabilityMemoryPool(size_t min_capacity, size_t max_capacity) :\n-    CollectedMemoryPool(\"ZHeap\",\n+ZServiceabilityMemoryPool::ZServiceabilityMemoryPool(const char* name, ZGenerationId id, size_t min_capacity, size_t max_capacity) :\n+    CollectedMemoryPool(name,\n@@ -99,1 +148,2 @@\n-                        true \/* support_usage_threshold *\/) {}\n+                        id == ZGenerationId::old \/* support_usage_threshold *\/),\n+    _generation_id(id) {}\n@@ -102,1 +152,1 @@\n-  return ZHeap::heap()->used();\n+  return ZHeap::heap()->used_generation(_generation_id);\n@@ -106,2 +156,1 @@\n-  const size_t committed = ZHeap::heap()->capacity();\n-  const size_t used      = MIN2(ZHeap::heap()->used(), committed);\n+  const ZMemoryUsageInfo info = compute_memory_usage_info();\n@@ -109,1 +158,5 @@\n-  return MemoryUsage(initial_size(), used, committed, max_size());\n+  if (_generation_id == ZGenerationId::young) {\n+    return MemoryUsage(initial_size(), info._young_used, info._young_capacity, max_size());\n+  } else {\n+    return MemoryUsage(initial_size(), info._old_used, info._old_capacity, max_size());\n+  }\n@@ -114,1 +167,2 @@\n-                                                           ZServiceabilityMemoryPool* pool) :\n+                                                           MemoryPool* young_memory_pool,\n+                                                           MemoryPool* old_memory_pool) :\n@@ -116,1 +170,2 @@\n-  add_pool(pool);\n+  add_pool(young_memory_pool);\n+  add_pool(old_memory_pool);\n@@ -119,1 +174,4 @@\n-ZServiceability::ZServiceability(size_t min_capacity, size_t max_capacity) :\n+ZServiceability::ZServiceability(size_t initial_capacity,\n+                                 size_t min_capacity,\n+                                 size_t max_capacity) :\n+    _initial_capacity(initial_capacity),\n@@ -122,4 +180,8 @@\n-    _memory_pool(_min_capacity, _max_capacity),\n-    _cycle_memory_manager(\"ZGC Cycles\", \"end of GC cycle\", &_memory_pool),\n-    _pause_memory_manager(\"ZGC Pauses\", \"end of GC pause\", &_memory_pool),\n-    _counters(NULL) {}\n+    _young_memory_pool(\"ZGC Young Generation\", ZGenerationId::young, _min_capacity, _max_capacity),\n+    _old_memory_pool(\"ZGC Old Generation\", ZGenerationId::old, 0, _max_capacity),\n+    _minor_cycle_memory_manager(\"ZGC Minor Cycles\", \"end of GC cycle\", &_young_memory_pool, &_old_memory_pool),\n+    _major_cycle_memory_manager(\"ZGC Major Cycles\", \"end of GC cycle\", &_young_memory_pool, &_old_memory_pool),\n+    _minor_pause_memory_manager(\"ZGC Minor Pauses\", \"end of GC pause\", &_young_memory_pool, &_old_memory_pool),\n+    _major_pause_memory_manager(\"ZGC Major Pauses\", \"end of GC pause\", &_young_memory_pool, &_old_memory_pool),\n+    _counters(nullptr) {\n+}\n@@ -128,1 +190,1 @@\n-  _counters = new ZServiceabilityCounters(_min_capacity, _max_capacity);\n+  _counters = new ZServiceabilityCounters(_initial_capacity, _min_capacity, _max_capacity);\n@@ -131,2 +193,4 @@\n-MemoryPool* ZServiceability::memory_pool() {\n-  return &_memory_pool;\n+MemoryPool* ZServiceability::memory_pool(ZGenerationId id) {\n+  return id == ZGenerationId::young\n+      ? &_young_memory_pool\n+      : &_old_memory_pool;\n@@ -135,2 +199,4 @@\n-GCMemoryManager* ZServiceability::cycle_memory_manager() {\n-  return &_cycle_memory_manager;\n+GCMemoryManager* ZServiceability::cycle_memory_manager(bool minor) {\n+  return minor\n+      ? &_minor_cycle_memory_manager\n+      : &_major_cycle_memory_manager;\n@@ -139,2 +205,4 @@\n-GCMemoryManager* ZServiceability::pause_memory_manager() {\n-  return &_pause_memory_manager;\n+GCMemoryManager* ZServiceability::pause_memory_manager(bool minor) {\n+  return minor\n+      ? &_minor_pause_memory_manager\n+      : &_major_pause_memory_manager;\n@@ -147,11 +215,31 @@\n-ZServiceabilityCycleTracer::ZServiceabilityCycleTracer() :\n-    _memory_manager_stats(ZHeap::heap()->serviceability_cycle_memory_manager(),\n-                          ZCollectedHeap::heap()->gc_cause(),\n-                          true  \/* allMemoryPoolsAffected *\/,\n-                          true  \/* recordGCBeginTime *\/,\n-                          true  \/* recordPreGCUsage *\/,\n-                          true  \/* recordPeakUsage *\/,\n-                          true  \/* recordPostGCUsage *\/,\n-                          true  \/* recordAccumulatedGCTime *\/,\n-                          true  \/* recordGCEndTime *\/,\n-                          true  \/* countCollection *\/) {}\n+bool ZServiceabilityCycleTracer::_minor_is_active;\n+\n+ZServiceabilityCycleTracer::ZServiceabilityCycleTracer(bool minor) :\n+    _memory_manager_stats(ZHeap::heap()->serviceability_cycle_memory_manager(minor),\n+                          minor ? ZDriver::minor()->gc_cause() : ZDriver::major()->gc_cause(),\n+                          true \/* allMemoryPoolsAffected *\/,\n+                          true \/* recordGCBeginTime *\/,\n+                          true \/* recordPreGCUsage *\/,\n+                          true \/* recordPeakUsage *\/,\n+                          true \/* recordPostGCUsage *\/,\n+                          true \/* recordAccumulatedGCTime *\/,\n+                          true \/* recordGCEndTime *\/,\n+                          true \/* countCollection *\/) {\n+  _minor_is_active = minor;\n+}\n+\n+ZServiceabilityCycleTracer::~ZServiceabilityCycleTracer() {\n+  _minor_is_active = false;\n+}\n+\n+bool ZServiceabilityCycleTracer::minor_is_active() {\n+  return _minor_is_active;\n+}\n+\n+bool ZServiceabilityPauseTracer::minor_is_active() const {\n+  \/\/ We report pauses at the minor\/major collection level instead\n+  \/\/ of the young\/old level. At the call-site where ZServiceabilityPauseTracer\n+  \/\/ is used, we don't have that information readily available, so\n+  \/\/ we let ZServiceabilityCycleTracer keep track of that.\n+  return ZServiceabilityCycleTracer::minor_is_active();\n+}\n@@ -161,3 +249,3 @@\n-    _counters_stats(ZHeap::heap()->serviceability_counters()->collector_counters()),\n-    _memory_manager_stats(ZHeap::heap()->serviceability_pause_memory_manager(),\n-                          ZCollectedHeap::heap()->gc_cause(),\n+    _counters_stats(ZHeap::heap()->serviceability_counters()->collector_counters(minor_is_active())),\n+    _memory_manager_stats(ZHeap::heap()->serviceability_pause_memory_manager(minor_is_active()),\n+                          minor_is_active() ? ZDriver::minor()->gc_cause() : ZDriver::major()->gc_cause(),\n","filename":"src\/hotspot\/share\/gc\/z\/zServiceability.cpp","additions":151,"deletions":63,"binary":false,"changes":214,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -29,0 +29,1 @@\n+#include \"gc\/z\/zGenerationId.hpp\"\n@@ -37,0 +38,3 @@\n+private:\n+  const ZGenerationId _generation_id;\n+\n@@ -38,1 +42,1 @@\n-  ZServiceabilityMemoryPool(size_t min_capacity, size_t max_capacity);\n+  ZServiceabilityMemoryPool(const char* name, ZGenerationId id, size_t min_capacity, size_t max_capacity);\n@@ -48,1 +52,2 @@\n-                               ZServiceabilityMemoryPool* pool);\n+                               MemoryPool* young_memory_pool,\n+                               MemoryPool* old_memory_pool);\n@@ -53,0 +58,1 @@\n+  const size_t                 _initial_capacity;\n@@ -55,3 +61,6 @@\n-  ZServiceabilityMemoryPool    _memory_pool;\n-  ZServiceabilityMemoryManager _cycle_memory_manager;\n-  ZServiceabilityMemoryManager _pause_memory_manager;\n+  ZServiceabilityMemoryPool    _young_memory_pool;\n+  ZServiceabilityMemoryPool    _old_memory_pool;\n+  ZServiceabilityMemoryManager _minor_cycle_memory_manager;\n+  ZServiceabilityMemoryManager _major_cycle_memory_manager;\n+  ZServiceabilityMemoryManager _minor_pause_memory_manager;\n+  ZServiceabilityMemoryManager _major_pause_memory_manager;\n@@ -61,1 +70,3 @@\n-  ZServiceability(size_t min_capacity, size_t max_capacity);\n+  ZServiceability(size_t initial_capacity,\n+                  size_t min_capacity,\n+                  size_t max_capacity);\n@@ -65,3 +76,3 @@\n-  MemoryPool* memory_pool();\n-  GCMemoryManager* cycle_memory_manager();\n-  GCMemoryManager* pause_memory_manager();\n+  MemoryPool* memory_pool(ZGenerationId id);\n+  GCMemoryManager* cycle_memory_manager(bool minor);\n+  GCMemoryManager* pause_memory_manager(bool minor);\n@@ -73,0 +84,2 @@\n+  static bool _minor_is_active;\n+\n@@ -76,1 +89,4 @@\n-  ZServiceabilityCycleTracer();\n+  ZServiceabilityCycleTracer(bool minor);\n+  ~ZServiceabilityCycleTracer();\n+\n+  static bool minor_is_active();\n@@ -85,0 +101,2 @@\n+  bool minor_is_active() const;\n+\n","filename":"src\/hotspot\/share\/gc\/z\/zServiceability.hpp","additions":29,"deletions":11,"binary":false,"changes":40,"status":"modified"},{"patch":"@@ -0,0 +1,41 @@\n+\/*\n+ * Copyright (c) 2022, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_Z_ZSTACKCHUNKGCDATA_HPP\n+#define SHARE_GC_Z_ZSTACKCHUNKGCDATA_HPP\n+\n+#include \"oops\/oopsHierarchy.hpp\"\n+\n+class ZStackChunkGCData {\n+private:\n+  \/\/ The implicit color of all oops when the chunk was recently allocated\n+  uintptr_t _color;\n+\n+  static ZStackChunkGCData* data(stackChunkOop chunk);\n+\n+public:\n+  static void initialize(stackChunkOop chunk);\n+  static uintptr_t color(stackChunkOop chunk);\n+};\n+\n+#endif \/\/ SHARE_GC_Z_ZSTACKCHUNKGCDATA_HPP\n","filename":"src\/hotspot\/share\/gc\/z\/zStackChunkGCData.hpp","additions":41,"deletions":0,"binary":false,"changes":41,"status":"added"},{"patch":"@@ -0,0 +1,45 @@\n+\/*\n+ * Copyright (c) 2022, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_Z_ZSTACKCHUNKGCDATA_INLINE_HPP\n+#define SHARE_GC_Z_ZSTACKCHUNKGCDATA_INLINE_HPP\n+\n+#include \"gc\/z\/zStackChunkGCData.hpp\"\n+\n+#include \"gc\/z\/zAddress.inline.hpp\"\n+#include \"gc\/z\/zGlobals.hpp\"\n+#include \"oops\/stackChunkOop.inline.hpp\"\n+\n+inline ZStackChunkGCData* ZStackChunkGCData::data(stackChunkOop chunk) {\n+  return reinterpret_cast<ZStackChunkGCData*>(chunk->gc_data());\n+}\n+\n+inline void ZStackChunkGCData::initialize(stackChunkOop chunk) {\n+  data(chunk)->_color = ZPointerStoreGoodMask;\n+}\n+\n+inline uintptr_t ZStackChunkGCData::color(stackChunkOop chunk) {\n+  return data(chunk)->_color;\n+}\n+\n+#endif \/\/ SHARE_GC_Z_ZSTACKCHUNKGCDATA_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/z\/zStackChunkGCData.inline.hpp","additions":45,"deletions":0,"binary":false,"changes":45,"status":"added"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,0 +27,1 @@\n+#include \"gc\/z\/zGeneration.inline.hpp\"\n@@ -28,1 +29,1 @@\n-#include \"gc\/z\/zThread.inline.hpp\"\n+#include \"gc\/z\/zStoreBarrierBuffer.hpp\"\n@@ -31,0 +32,1 @@\n+#include \"gc\/z\/zUncoloredRoot.inline.hpp\"\n@@ -34,0 +36,2 @@\n+#include \"runtime\/stackWatermark.hpp\"\n+#include \"runtime\/thread.hpp\"\n@@ -41,1 +45,1 @@\n-  if (nm != NULL) {\n+  if (nm != nullptr) {\n@@ -52,1 +56,1 @@\n-  return *ZAddressBadMaskHighOrderBitsAddr;\n+  return *ZPointerStoreGoodMaskLowOrderBitsAddr;\n@@ -56,3 +60,4 @@\n-    StackWatermark(jt, StackWatermarkKind::gc, *ZAddressBadMaskHighOrderBitsAddr),\n-    _jt_cl(),\n-    _cb_cl(),\n+    StackWatermark(jt, StackWatermarkKind::gc, *ZPointerStoreGoodMaskLowOrderBitsAddr),\n+    \/\/ First watermark is fake and setup to be replaced at next phase shift\n+    _old_watermarks{{ZPointerStoreBadMask, 1}, {}, {}},\n+    _old_watermarks_newest(0),\n@@ -61,4 +66,66 @@\n-OopClosure* ZStackWatermark::closure_from_context(void* context) {\n-  if (context != NULL) {\n-    assert(ZThread::is_worker(), \"Unexpected thread passing in context: \" PTR_FORMAT, p2i(context));\n-    return reinterpret_cast<OopClosure*>(context);\n+bool ZColorWatermark::covers(const ZColorWatermark& other) const {\n+  if (_watermark == 0) {\n+    \/\/ This watermark was completed\n+    return true;\n+  }\n+\n+  if (other._watermark == 0) {\n+    \/\/ The other watermark was completed\n+    return false;\n+  }\n+\n+  \/\/ Compare the two\n+  return _watermark >= other._watermark;\n+}\n+\n+uintptr_t ZStackWatermark::prev_head_color() const {\n+  return _old_watermarks[_old_watermarks_newest]._color;\n+}\n+\n+uintptr_t ZStackWatermark::prev_frame_color(const frame& fr) const {\n+  for (int i = _old_watermarks_newest; i >= 0; i--) {\n+    const ZColorWatermark ow = _old_watermarks[i];\n+    if (ow._watermark == 0 || uintptr_t(fr.sp()) <= ow._watermark) {\n+      return ow._color;\n+    }\n+  }\n+\n+  fatal(\"Found no matching previous color for the frame\");\n+  return 0;\n+}\n+\n+void ZStackWatermark::save_old_watermark() {\n+  assert(StackWatermarkState::epoch(_state) != ZStackWatermark::epoch_id(), \"Shouldn't be here otherwise\");\n+\n+  \/\/ Previous color\n+  const uintptr_t prev_color = StackWatermarkState::epoch(_state);\n+\n+  \/\/ If the prev_color is still the last saved color watermark, then processing has not started.\n+  const bool prev_processing_started = prev_color != prev_head_color();\n+\n+  if (!prev_processing_started) {\n+    \/\/ Nothing was processed in the previous phase, so there's no need to save a watermark for it.\n+    \/\/ Must have been a remapped phase, the other phases are explicitly completed by the GC.\n+    assert((prev_color & ZPointerRemapped) != 0, \"Unexpected color: \" PTR_FORMAT, prev_color);\n+    return;\n+  }\n+\n+  \/\/ Previous watermark\n+  const uintptr_t prev_watermark = StackWatermarkState::is_done(_state) ? 0 : last_processed_raw();\n+\n+  \/\/ Create a new color watermark to describe the old watermark\n+  const ZColorWatermark cw = { prev_color, prev_watermark };\n+\n+  \/\/ Find the location of the oldest watermark that it covers, and thus can replace\n+  int replace = -1;\n+  for (int i = 0; i <= _old_watermarks_newest; i++) {\n+    if (cw.covers(_old_watermarks[i])) {\n+      replace = i;\n+      break;\n+    }\n+  }\n+\n+  \/\/ Update top\n+  if (replace != -1) {\n+    \/\/ Found one to replace\n+    _old_watermarks_newest = replace;\n@@ -66,1 +133,43 @@\n-    return &_jt_cl;\n+    \/\/ Found none too replace - push it to the top\n+    _old_watermarks_newest++;\n+    assert(_old_watermarks_newest < _old_watermarks_max, \"Unexpected amount of old watermarks\");\n+  }\n+\n+  \/\/ Install old watermark\n+  _old_watermarks[_old_watermarks_newest] = cw;\n+}\n+\n+class ZStackWatermarkProcessOopClosure : public ZUncoloredRootClosure {\n+private:\n+  const ZUncoloredRoot::RootFunction _function;\n+  const uintptr_t                    _color;\n+\n+  static ZUncoloredRoot::RootFunction select_function(void* context) {\n+    if (context == nullptr) {\n+      return ZUncoloredRoot::process;\n+    }\n+\n+    assert(Thread::current()->is_Worker_thread(), \"Unexpected thread passing in context: \" PTR_FORMAT, p2i(context));\n+    return reinterpret_cast<ZUncoloredRoot::RootFunction>(context);\n+  }\n+\n+public:\n+  ZStackWatermarkProcessOopClosure(void* context, uintptr_t color) :\n+      _function(select_function(context)), _color(color) {}\n+\n+  virtual void do_root(zaddress_unsafe* p) {\n+    _function(p, _color);\n+  }\n+};\n+\n+void ZStackWatermark::process_head(void* context) {\n+  const uintptr_t color = prev_head_color();\n+\n+  ZStackWatermarkProcessOopClosure cl(context, color);\n+  ZOnStackCodeBlobClosure cb_cl;\n+\n+  _jt->oops_do_no_frames(&cl, &cb_cl);\n+\n+  zaddress_unsafe* const invisible_root = ZThreadLocalData::invisible_root(_jt);\n+  if (invisible_root != nullptr) {\n+    ZUncoloredRoot::process_invisible(invisible_root, color);\n@@ -71,2 +180,1 @@\n-  \/\/ Verify the head (no_frames) of the thread is bad before fixing it.\n-  ZVerify::verify_thread_head_bad(_jt);\n+  save_old_watermark();\n@@ -75,2 +183,1 @@\n-  _jt->oops_do_no_frames(closure_from_context(context), &_cb_cl);\n-  ZThreadLocalData::do_invisible_root(_jt, ZBarrier::load_barrier_on_invisible_root_oop_field);\n+  process_head(context);\n@@ -80,1 +187,1 @@\n-  ZVerify::verify_thread_frames_bad(_jt);\n+  \/\/ ZVerify::verify_thread_frames_bad(_jt);\n@@ -82,2 +189,7 @@\n-  \/\/ Update thread local address bad mask\n-  ZThreadLocalData::set_address_bad_mask(_jt, ZAddressBadMask);\n+  \/\/ Update thread-local masks\n+  ZThreadLocalData::set_load_bad_mask(_jt, ZPointerLoadBadMask);\n+  ZThreadLocalData::set_load_good_mask(_jt, ZPointerLoadGoodMask);\n+  ZThreadLocalData::set_mark_bad_mask(_jt, ZPointerMarkBadMask);\n+  ZThreadLocalData::set_store_bad_mask(_jt, ZPointerStoreBadMask);\n+  ZThreadLocalData::set_store_good_mask(_jt, ZPointerStoreGoodMask);\n+  ZThreadLocalData::set_nmethod_disarmed(_jt, ZPointerStoreGoodMask);\n@@ -86,1 +198,1 @@\n-  if (ZGlobalPhase == ZPhaseMark) {\n+  if (ZGeneration::young()->is_phase_mark() || ZGeneration::old()->is_phase_mark()) {\n@@ -88,2 +200,0 @@\n-  } else {\n-    ZThreadLocalAllocBuffer::remap(_jt);\n@@ -92,0 +202,3 @@\n+  \/\/ Prepare store barrier buffer for new GC phase\n+  ZThreadLocalData::store_barrier_buffer(_jt)->on_new_phase();\n+\n@@ -97,2 +210,5 @@\n-  ZVerify::verify_frame_bad(fr, register_map);\n-  fr.oops_do(closure_from_context(context), &_cb_cl, &register_map, DerivedPointerIterationMode::_directly);\n+  const uintptr_t color = prev_frame_color(fr);\n+  ZStackWatermarkProcessOopClosure cl(context, color);\n+  ZOnStackCodeBlobClosure cb_cl;\n+\n+  fr.oops_do(&cl, &cb_cl, &register_map, DerivedPointerIterationMode::_directly);\n","filename":"src\/hotspot\/share\/gc\/z\/zStackWatermark.cpp","additions":140,"deletions":24,"binary":false,"changes":164,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -31,0 +31,1 @@\n+#include \"gc\/z\/zUncoloredRoot.hpp\"\n@@ -50,0 +51,7 @@\n+struct ZColorWatermark {\n+  uintptr_t _color;\n+  uintptr_t _watermark;\n+\n+  bool covers(const ZColorWatermark& other) const;\n+};\n+\n@@ -52,3 +60,11 @@\n-  ZLoadBarrierOopClosure  _jt_cl;\n-  ZOnStackCodeBlobClosure _cb_cl;\n-  ThreadLocalAllocStats   _stats;\n+  \/\/ Stores old watermarks, which describes the\n+  \/\/ colors of the non-processed part of the stack.\n+  const static int      _old_watermarks_max = 3;\n+  ZColorWatermark       _old_watermarks[_old_watermarks_max];\n+  int                   _old_watermarks_newest;\n+\n+  ThreadLocalAllocStats _stats;\n+\n+  uintptr_t prev_head_color() const;\n+  uintptr_t prev_frame_color(const frame& fr) const;\n+  void save_old_watermark();\n@@ -56,1 +72,1 @@\n-  OopClosure* closure_from_context(void* context);\n+  void process_head(void* context);\n","filename":"src\/hotspot\/share\/gc\/z\/zStackWatermark.hpp","additions":21,"deletions":5,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,0 +28,2 @@\n+#include \"gc\/z\/zDirector.hpp\"\n+#include \"gc\/z\/zDriver.hpp\"\n@@ -29,0 +31,1 @@\n+#include \"gc\/z\/zGeneration.inline.hpp\"\n@@ -34,1 +37,0 @@\n-#include \"gc\/z\/zThread.inline.hpp\"\n@@ -41,0 +43,1 @@\n+#include \"runtime\/thread.hpp\"\n@@ -243,1 +246,1 @@\n-  log.print(\" %10s: %-41s \"\n+  log.print(\" %16s: %-41s \"\n@@ -261,1 +264,1 @@\n-  log.print(\" %10s: %-41s \"\n+  log.print(\" %16s: %-41s \"\n@@ -279,1 +282,1 @@\n-  log.print(\" %10s: %-41s \"\n+  log.print(\" %16s: %-41s \"\n@@ -297,1 +300,1 @@\n-  log.print(\" %10s: %-41s \"\n+  log.print(\" %16s: %-41s \"\n@@ -315,1 +318,1 @@\n-  log.print(\" %10s: %-41s \"\n+  log.print(\" %16s: %-41s \"\n@@ -383,1 +386,1 @@\n-template <typename T> T*       ZStatIterableValue<T>::_first = NULL;\n+template <typename T> T*       ZStatIterableValue<T>::_first = nullptr;\n@@ -402,1 +405,1 @@\n-  _first = NULL;\n+  _first = nullptr;\n@@ -404,1 +407,1 @@\n-  while (first_unsorted != NULL) {\n+  while (first_unsorted != nullptr) {\n@@ -407,1 +410,1 @@\n-    value->_next = NULL;\n+    value->_next = nullptr;\n@@ -411,1 +414,1 @@\n-    while (*current != NULL) {\n+    while (*current != nullptr) {\n@@ -592,1 +595,0 @@\n-ConcurrentGCTimer ZStatPhase::_timer;\n@@ -623,4 +625,0 @@\n-ConcurrentGCTimer* ZStatPhase::timer() {\n-  return &_timer;\n-}\n-\n@@ -631,2 +629,54 @@\n-ZStatPhaseCycle::ZStatPhaseCycle(const char* name) :\n-    ZStatPhase(\"Collector\", name) {}\n+ZStatPhaseCollection::ZStatPhaseCollection(const char* name, bool minor) :\n+    ZStatPhase(minor ? \"Minor Collection\" : \"Major Collection\", name),\n+    _minor(minor) {}\n+\n+GCTracer* ZStatPhaseCollection::jfr_tracer() const {\n+  return _minor\n+      ? ZDriver::minor()->jfr_tracer()\n+      : ZDriver::major()->jfr_tracer();\n+}\n+\n+void ZStatPhaseCollection::set_used_at_start(size_t used) const {\n+  if (_minor) {\n+    ZDriver::minor()->set_used_at_start(used);\n+  } else {\n+    ZDriver::major()->set_used_at_start(used);\n+  }\n+}\n+\n+size_t ZStatPhaseCollection::used_at_start() const {\n+  return _minor\n+      ? ZDriver::minor()->used_at_start()\n+      : ZDriver::major()->used_at_start();\n+}\n+\n+void ZStatPhaseCollection::register_start(ConcurrentGCTimer* timer, const Ticks& start) const {\n+  const GCCause::Cause cause = _minor ? ZDriver::minor()->gc_cause() : ZDriver::major()->gc_cause();\n+\n+  timer->register_gc_start(start);\n+\n+  jfr_tracer()->report_gc_start(cause, start);\n+  ZCollectedHeap::heap()->trace_heap_before_gc(jfr_tracer());\n+\n+  set_used_at_start(ZHeap::heap()->used());\n+\n+  log_info(gc)(\"%s (%s)\", name(), GCCause::to_string(cause));\n+}\n+\n+void ZStatPhaseCollection::register_end(ConcurrentGCTimer* timer, const Ticks& start, const Ticks& end) const {\n+  const GCCause::Cause cause = _minor ? ZDriver::minor()->gc_cause() : ZDriver::major()->gc_cause();\n+\n+  if (ZAbort::should_abort()) {\n+    log_info(gc)(\"%s (%s) Aborted\", name(), GCCause::to_string(cause));\n+    return;\n+  }\n+\n+  timer->register_gc_end(end);\n+\n+  jfr_tracer()->report_gc_end(end, timer->time_partitions());\n+  ZCollectedHeap::heap()->trace_heap_after_gc(jfr_tracer());\n+\n+  const Tickspan duration = end - start;\n+  ZStatSample(_sampler, duration.value());\n+\n+  const size_t used_at_end = ZHeap::heap()->used();\n@@ -634,2 +684,11 @@\n-void ZStatPhaseCycle::register_start(const Ticks& start) const {\n-  timer()->register_gc_start(start);\n+  log_info(gc)(\"%s (%s) \" ZSIZE_FMT \"->\" ZSIZE_FMT \" %.3fs\",\n+               name(),\n+               GCCause::to_string(cause),\n+               ZSIZE_ARGS(used_at_start()),\n+               ZSIZE_ARGS(used_at_end),\n+               duration.seconds());\n+}\n+\n+ZStatPhaseGeneration::ZStatPhaseGeneration(const char* name, ZGenerationId id) :\n+    ZStatPhase(id == ZGenerationId::old ? \"Old Generation\" : \"Young Generation\", name),\n+    _id(id) {}\n@@ -637,1 +696,5 @@\n-  ZTracer::tracer()->report_gc_start(ZCollectedHeap::heap()->gc_cause(), start);\n+ZGenerationTracer* ZStatPhaseGeneration::jfr_tracer() const {\n+  return _id == ZGenerationId::young\n+      ? ZGeneration::young()->jfr_tracer()\n+      : ZGeneration::old()->jfr_tracer();\n+}\n@@ -639,0 +702,1 @@\n+void ZStatPhaseGeneration::register_start(ConcurrentGCTimer* timer, const Ticks& start) const {\n@@ -640,1 +704,0 @@\n-  ZCollectedHeap::heap()->trace_heap_before_gc(ZTracer::tracer());\n@@ -642,2 +705,3 @@\n-  log_info(gc, start)(\"Garbage Collection (%s)\",\n-                       GCCause::to_string(ZCollectedHeap::heap()->gc_cause()));\n+  jfr_tracer()->report_start(start);\n+\n+  log_info(gc, phases)(\"%s\", name());\n@@ -646,1 +710,1 @@\n-void ZStatPhaseCycle::register_end(const Ticks& start, const Ticks& end) const {\n+void ZStatPhaseGeneration::register_end(ConcurrentGCTimer* timer, const Ticks& start, const Ticks& end) const {\n@@ -648,2 +712,1 @@\n-    log_info(gc)(\"Garbage Collection (%s) Aborted\",\n-                 GCCause::to_string(ZCollectedHeap::heap()->gc_cause()));\n+    log_info(gc, phases)(\"%s Aborted\", name());\n@@ -653,1 +716,1 @@\n-  timer()->register_gc_end(end);\n+  jfr_tracer()->report_end(end);\n@@ -656,3 +719,0 @@\n-  ZCollectedHeap::heap()->trace_heap_after_gc(ZTracer::tracer());\n-\n-  ZTracer::tracer()->report_gc_end(end, timer()->time_partitions());\n@@ -663,0 +723,3 @@\n+  ZGeneration* const generation = ZGeneration::generation(_id);\n+\n+  generation->stat_heap()->print_stalls();\n@@ -665,1 +728,1 @@\n-  ZStatMark::print();\n+  generation->stat_mark()->print();\n@@ -668,3 +731,3 @@\n-  ZStatReferences::print();\n-  ZStatRelocation::print();\n-  ZStatHeap::print();\n+  if (generation->is_old()) {\n+    ZStatReferences::print();\n+  }\n@@ -672,4 +735,12 @@\n-  log_info(gc)(\"Garbage Collection (%s) \" ZSIZE_FMT \"->\" ZSIZE_FMT,\n-               GCCause::to_string(ZCollectedHeap::heap()->gc_cause()),\n-               ZSIZE_ARGS(ZStatHeap::used_at_mark_start()),\n-               ZSIZE_ARGS(ZStatHeap::used_at_relocate_end()));\n+  generation->stat_relocation()->print_page_summary();\n+  if (generation->is_young()) {\n+    generation->stat_relocation()->print_age_table();\n+  }\n+\n+  generation->stat_heap()->print(generation);\n+\n+  log_info(gc, phases)(\"%s \" ZSIZE_FMT \"->\" ZSIZE_FMT \" %.3fs\",\n+                       name(),\n+                       ZSIZE_ARGS(generation->stat_heap()->used_at_collection_start()),\n+                       ZSIZE_ARGS(generation->stat_heap()->used_at_collection_end()),\n+                       duration.seconds());\n@@ -680,2 +751,2 @@\n-ZStatPhasePause::ZStatPhasePause(const char* name) :\n-    ZStatPhase(\"Phase\", name) {}\n+ZStatPhasePause::ZStatPhasePause(const char* name, ZGenerationId id) :\n+    ZStatPhase(id == ZGenerationId::young ? \"Young Pause\" : \"Old Pause\", name) {}\n@@ -687,2 +758,2 @@\n-void ZStatPhasePause::register_start(const Ticks& start) const {\n-  timer()->register_gc_pause_start(name(), start);\n+void ZStatPhasePause::register_start(ConcurrentGCTimer* timer, const Ticks& start) const {\n+  timer->register_gc_pause_start(name(), start);\n@@ -694,2 +765,2 @@\n-void ZStatPhasePause::register_end(const Ticks& start, const Ticks& end) const {\n-  timer()->register_gc_pause_end(end);\n+void ZStatPhasePause::register_end(ConcurrentGCTimer* timer, const Ticks& start, const Ticks& end) const {\n+  timer->register_gc_pause_end(end);\n@@ -712,2 +783,2 @@\n-ZStatPhaseConcurrent::ZStatPhaseConcurrent(const char* name) :\n-    ZStatPhase(\"Phase\", name) {}\n+ZStatPhaseConcurrent::ZStatPhaseConcurrent(const char* name, ZGenerationId id) :\n+    ZStatPhase(id == ZGenerationId::young ? \"Young Phase\" : \"Old Phase\", name) {}\n@@ -715,2 +786,2 @@\n-void ZStatPhaseConcurrent::register_start(const Ticks& start) const {\n-  timer()->register_gc_concurrent_start(name(), start);\n+void ZStatPhaseConcurrent::register_start(ConcurrentGCTimer* timer, const Ticks& start) const {\n+  timer->register_gc_concurrent_start(name(), start);\n@@ -722,1 +793,1 @@\n-void ZStatPhaseConcurrent::register_end(const Ticks& start, const Ticks& end) const {\n+void ZStatPhaseConcurrent::register_end(ConcurrentGCTimer* timer, const Ticks& start, const Ticks& end) const {\n@@ -727,1 +798,1 @@\n-  timer()->register_gc_concurrent_end(end);\n+  timer->register_gc_concurrent_end(end);\n@@ -736,2 +807,2 @@\n-ZStatSubPhase::ZStatSubPhase(const char* name) :\n-    ZStatPhase(\"Subphase\", name) {}\n+ZStatSubPhase::ZStatSubPhase(const char* name, ZGenerationId id) :\n+    ZStatPhase(id == ZGenerationId::young ? \"Young Subphase\" : \"Old Subphase\", name) {}\n@@ -739,2 +810,7 @@\n-void ZStatSubPhase::register_start(const Ticks& start) const {\n-  if (ZThread::is_worker()) {\n+void ZStatSubPhase::register_start(ConcurrentGCTimer* timer, const Ticks& start) const {\n+  if (timer != nullptr) {\n+    assert(!Thread::current()->is_Worker_thread(), \"Unexpected timer value\");\n+    timer->register_gc_phase_start(name(), start);\n+  }\n+\n+  if (Thread::current()->is_Worker_thread()) {\n@@ -749,1 +825,1 @@\n-void ZStatSubPhase::register_end(const Ticks& start, const Ticks& end) const {\n+void ZStatSubPhase::register_end(ConcurrentGCTimer* timer, const Ticks& start, const Ticks& end) const {\n@@ -754,1 +830,6 @@\n-  ZTracer::tracer()->report_thread_phase(name(), start, end);\n+  if (timer != nullptr) {\n+    assert(!Thread::current()->is_Worker_thread(), \"Unexpected timer value\");\n+    timer->register_gc_phase_end(end);\n+  }\n+\n+  ZTracer::report_thread_phase(name(), start, end);\n@@ -759,1 +840,1 @@\n-  if (ZThread::is_worker()) {\n+  if (Thread::current()->is_Worker_thread()) {\n@@ -773,1 +854,1 @@\n-void ZStatCriticalPhase::register_start(const Ticks& start) const {\n+void ZStatCriticalPhase::register_start(ConcurrentGCTimer* timer, const Ticks& start) const {\n@@ -780,2 +861,2 @@\n-void ZStatCriticalPhase::register_end(const Ticks& start, const Ticks& end) const {\n-  ZTracer::tracer()->report_thread_phase(name(), start, end);\n+void ZStatCriticalPhase::register_end(ConcurrentGCTimer* timer, const Ticks& start, const Ticks& end) const {\n+  ZTracer::report_thread_phase(name(), start, end);\n@@ -796,4 +877,10 @@\n-\/\/\n-\/\/ Stat timer\n-\/\/\n-THREAD_LOCAL uint32_t ZStatTimerDisable::_active = 0;\n+ZStatTimerYoung::ZStatTimerYoung(const ZStatPhase& phase) :\n+    ZStatTimer(phase, ZGeneration::young()->gc_timer()) {}\n+\n+ZStatTimerOld::ZStatTimerOld(const ZStatPhase& phase) :\n+    ZStatTimer(phase, ZGeneration::old()->gc_timer()) {}\n+\n+ZStatTimerWorker::ZStatTimerWorker(const ZStatPhase& phase) :\n+    ZStatTimer(phase, nullptr \/* gc_timer *\/) {\n+  assert(Thread::current()->is_Worker_thread(), \"Should only be called by worker thread\");\n+}\n@@ -827,1 +914,1 @@\n-  ZTracer::tracer()->report_stat_sampler(sampler, value);\n+  ZTracer::report_stat_sampler(sampler, value);\n@@ -834,1 +921,1 @@\n-  ZTracer::tracer()->report_stat_counter(counter, increment, value);\n+  ZTracer::report_stat_counter(counter, increment, value);\n@@ -843,1 +930,1 @@\n-\/\/ Stat allocation rate\n+\/\/ Stat mutator allocation rate\n@@ -845,3 +932,13 @@\n-const ZStatUnsampledCounter ZStatAllocRate::_counter(\"Allocation Rate\");\n-TruncatedSeq                ZStatAllocRate::_samples(ZStatAllocRate::sample_hz);\n-TruncatedSeq                ZStatAllocRate::_rate(ZStatAllocRate::sample_hz);\n+ZLock*          ZStatMutatorAllocRate::_stat_lock;\n+jlong           ZStatMutatorAllocRate::_last_sample_time;\n+volatile size_t ZStatMutatorAllocRate::_sampling_granule;\n+volatile size_t ZStatMutatorAllocRate::_allocated_since_sample;\n+TruncatedSeq    ZStatMutatorAllocRate::_samples_time(100);\n+TruncatedSeq    ZStatMutatorAllocRate::_samples_bytes(100);\n+TruncatedSeq    ZStatMutatorAllocRate::_rate(100);\n+\n+void ZStatMutatorAllocRate::initialize() {\n+  _last_sample_time = os::elapsed_counter();\n+  _stat_lock = new ZLock();\n+  update_sampling_granule();\n+}\n@@ -849,2 +946,4 @@\n-const ZStatUnsampledCounter& ZStatAllocRate::counter() {\n-  return _counter;\n+void ZStatMutatorAllocRate::update_sampling_granule() {\n+  const size_t sampling_heap_granules = 128;\n+  const size_t soft_max_capacity = ZHeap::heap()->soft_max_capacity();\n+  _sampling_granule = align_up(soft_max_capacity \/ sampling_heap_granules, ZGranuleSize);\n@@ -853,3 +952,12 @@\n-uint64_t ZStatAllocRate::sample_and_reset() {\n-  const ZStatCounterData bytes_per_sample = _counter.collect_and_reset();\n-  _samples.add(bytes_per_sample._counter);\n+void ZStatMutatorAllocRate::sample_allocation(size_t allocation_bytes) {\n+  const size_t allocated = Atomic::add(&_allocated_since_sample, allocation_bytes);\n+\n+  if (allocated < Atomic::load(&_sampling_granule)) {\n+    \/\/ No need for sampling yet\n+    return;\n+  }\n+\n+  if (!_stat_lock->try_lock()) {\n+    \/\/ Someone beat us to it\n+    return;\n+  }\n@@ -857,1 +965,27 @@\n-  const uint64_t bytes_per_second = _samples.sum();\n+  const size_t allocated_sample = Atomic::load(&_allocated_since_sample);\n+\n+  if (allocated_sample < _sampling_granule) {\n+    \/\/ Someone beat us to it\n+    _stat_lock->unlock();\n+    return;\n+  }\n+\n+  const jlong now = os::elapsed_counter();\n+  const jlong elapsed = now - _last_sample_time;\n+\n+  if (elapsed <= 0) {\n+    \/\/ Avoid sampling nonsense allocation rates\n+    _stat_lock->unlock();\n+    return;\n+  }\n+\n+  Atomic::sub(&_allocated_since_sample, allocated_sample);\n+\n+  _samples_time.add(elapsed);\n+  _samples_bytes.add(allocated_sample);\n+\n+  const double last_sample_bytes = _samples_bytes.sum();\n+  const double elapsed_time = _samples_time.sum();\n+\n+  const double elapsed_seconds = elapsed_time \/ os::elapsed_frequency();\n+  const double bytes_per_second = double(last_sample_bytes) \/ elapsed_seconds;\n@@ -860,2 +994,1 @@\n-  return bytes_per_second;\n-}\n+  update_sampling_granule();\n@@ -863,3 +996,7 @@\n-double ZStatAllocRate::predict() {\n-  return _rate.predict_next();\n-}\n+  _last_sample_time = now;\n+\n+  log_debug(gc, alloc)(\"Mutator Allocation Rate: %.1fMB\/s Predicted: %.1fMB\/s, Avg: %.1f(+\/-%.1f)MB\/s\",\n+                       bytes_per_second \/ M,\n+                       _rate.predict_next() \/ M,\n+                       _rate.avg() \/ M,\n+                       _rate.sd() \/ M);\n@@ -867,2 +1004,3 @@\n-double ZStatAllocRate::avg() {\n-  return _rate.avg();\n+  _stat_lock->unlock();\n+\n+  ZDirector::evaluate_rules();\n@@ -871,2 +1009,3 @@\n-double ZStatAllocRate::sd() {\n-  return _rate.sd();\n+ZStatMutatorAllocRateStats ZStatMutatorAllocRate::stats() {\n+  ZLocker<ZLock> locker(_stat_lock);\n+  return {_rate.avg(), _rate.predict_next(), _rate.sd()};\n@@ -882,0 +1021,1 @@\n+  ZStatMutatorAllocRate::initialize();\n@@ -886,1 +1026,1 @@\n-  for (const ZStatCounter* counter = ZStatCounter::first(); counter != NULL; counter = counter->next()) {\n+  for (const ZStatCounter* counter = ZStatCounter::first(); counter != nullptr; counter = counter->next()) {\n@@ -891,1 +1031,1 @@\n-  for (const ZStatSampler* sampler = ZStatSampler::first(); sampler != NULL; sampler = sampler->next()) {\n+  for (const ZStatSampler* sampler = ZStatSampler::first(); sampler != nullptr; sampler = sampler->next()) {\n@@ -916,1 +1056,1 @@\n-  for (const ZStatSampler* sampler = ZStatSampler::first(); sampler != NULL; sampler = sampler->next()) {\n+  for (const ZStatSampler* sampler = ZStatSampler::first(); sampler != nullptr; sampler = sampler->next()) {\n@@ -925,1 +1065,1 @@\n-void ZStat::run_service() {\n+void ZStat::run_thread() {\n@@ -927,1 +1067,1 @@\n-  LogTarget(Info, gc, stats) log;\n+  LogTarget(Debug, gc, stats) log;\n@@ -939,0 +1079,6 @@\n+  \/\/ At exit print the final stats\n+  LogTarget(Info, gc, stats) exit_log;\n+  if (exit_log.is_enabled()) {\n+    print(exit_log, history);\n+  }\n+\n@@ -942,1 +1088,1 @@\n-void ZStat::stop_service() {\n+void ZStat::terminate() {\n@@ -1075,6 +1221,11 @@\n-uint64_t  ZStatCycle::_nwarmup_cycles = 0;\n-Ticks     ZStatCycle::_start_of_last;\n-Ticks     ZStatCycle::_end_of_last;\n-NumberSeq ZStatCycle::_serial_time(0.7 \/* alpha *\/);\n-NumberSeq ZStatCycle::_parallelizable_time(0.7 \/* alpha *\/);\n-uint      ZStatCycle::_last_active_workers = 0;\n+ZStatCycle::ZStatCycle() :\n+    _stat_lock(),\n+    _nwarmup_cycles(0),\n+    _start_of_last(),\n+    _end_of_last(),\n+    _cycle_intervals(0.7 \/* alpha *\/),\n+    _serial_time(0.7 \/* alpha *\/),\n+    _parallelizable_time(0.7 \/* alpha *\/),\n+    _parallelizable_duration(0.7 \/* alpha *\/),\n+    _last_active_workers(0.0) {\n+}\n@@ -1083,0 +1234,1 @@\n+  ZLocker<ZLock> locker(&_stat_lock);\n@@ -1086,1 +1238,3 @@\n-void ZStatCycle::at_end(GCCause::Cause cause, uint active_workers) {\n+void ZStatCycle::at_end(ZStatWorkers* stat_workers, bool record_stats) {\n+  ZLocker<ZLock> locker(&_stat_lock);\n+  const Ticks end_of_last = _end_of_last;\n@@ -1089,1 +1243,1 @@\n-  if (cause == GCCause::_z_warmup) {\n+  if (ZDriver::major()->gc_cause() == GCCause::_z_warmup && _nwarmup_cycles < 3) {\n@@ -1093,2 +1247,0 @@\n-  _last_active_workers = active_workers;\n-\n@@ -1097,1 +1249,2 @@\n-  const double workers_duration = ZStatWorkers::get_and_reset_duration();\n+  const double workers_duration = stat_workers->get_and_reset_duration();\n+  const double workers_time = stat_workers->get_and_reset_time();\n@@ -1099,3 +1252,12 @@\n-  const double parallelizable_time = workers_duration * active_workers;\n-  _serial_time.add(serial_time);\n-  _parallelizable_time.add(parallelizable_time);\n+\n+  _last_active_workers = workers_time \/ workers_duration;\n+\n+  if (record_stats) {\n+    _serial_time.add(serial_time);\n+    _parallelizable_time.add(workers_time);\n+    _parallelizable_duration.add(workers_duration);\n+    if (end_of_last.value() != 0) {\n+      const double cycle_interval = (_end_of_last - end_of_last).seconds();\n+      _cycle_intervals.add(cycle_interval);\n+    }\n+  }\n@@ -1108,4 +1270,0 @@\n-uint64_t ZStatCycle::nwarmup_cycles() {\n-  return _nwarmup_cycles;\n-}\n-\n@@ -1118,2 +1276,2 @@\n-const AbsSeq& ZStatCycle::serial_time() {\n-  return _serial_time;\n+double ZStatCycle::last_active_workers() {\n+  return _last_active_workers;\n@@ -1122,3 +1280,6 @@\n-const AbsSeq& ZStatCycle::parallelizable_time() {\n-  return _parallelizable_time;\n-}\n+double ZStatCycle::duration_since_start() {\n+  const Ticks start = _start_of_last;\n+  if (start.value() == 0) {\n+    \/\/ No end recorded yet, return time since VM start\n+    return 0.0;\n+  }\n@@ -1126,2 +1287,3 @@\n-uint ZStatCycle::last_active_workers() {\n-  return _last_active_workers;\n+  const Ticks now = Ticks::now();\n+  const Tickspan duration_since_start = now - start;\n+  return duration_since_start.seconds();\n@@ -1141,0 +1303,20 @@\n+ZStatCycleStats ZStatCycle::stats() {\n+  ZLocker<ZLock> locker(&_stat_lock);\n+\n+  return {\n+    is_warm(),\n+    _nwarmup_cycles,\n+    is_time_trustable(),\n+    time_since_last(),\n+    last_active_workers(),\n+    duration_since_start(),\n+    _cycle_intervals.davg(),\n+    _serial_time.davg(),\n+    _serial_time.dsd(),\n+    _parallelizable_time.davg(),\n+    _parallelizable_time.dsd(),\n+    _parallelizable_duration.davg(),\n+    _parallelizable_duration.dsd()\n+  };\n+}\n+\n@@ -1144,4 +1326,9 @@\n-Ticks ZStatWorkers::_start_of_last;\n-Tickspan ZStatWorkers::_accumulated_duration;\n-\n-void ZStatWorkers::at_start() {\n+ZStatWorkers::ZStatWorkers() :\n+    _stat_lock(),\n+    _active_workers(0),\n+    _start_of_last(),\n+    _accumulated_duration(),\n+    _accumulated_time() {}\n+\n+void ZStatWorkers::at_start(uint active_workers) {\n+  ZLocker<ZLock> locker(&_stat_lock);\n@@ -1149,0 +1336,1 @@\n+  _active_workers = active_workers;\n@@ -1152,0 +1340,1 @@\n+  ZLocker<ZLock> locker(&_stat_lock);\n@@ -1154,0 +1343,5 @@\n+  Tickspan time = duration;\n+  for (uint i = 1; i < _active_workers; ++i) {\n+    time += duration;\n+  }\n+  _accumulated_time += time;\n@@ -1155,0 +1349,28 @@\n+  _active_workers = 0;\n+}\n+\n+double ZStatWorkers::accumulated_time() {\n+  const uint nworkers = _active_workers;\n+  const Ticks now = Ticks::now();\n+  const Ticks start = _start_of_last;\n+  Tickspan time = _accumulated_time;\n+  if (nworkers != 0) {\n+    for (uint i = 0; i < nworkers; ++i) {\n+      time += now - start;\n+    }\n+  }\n+  return time.seconds();\n+}\n+\n+double ZStatWorkers::accumulated_duration() {\n+  const Ticks now = Ticks::now();\n+  const Ticks start = _start_of_last;\n+  Tickspan duration = _accumulated_duration;\n+  if (_active_workers != 0) {\n+    duration += now - start;\n+  }\n+  return duration.seconds();\n+}\n+\n+uint ZStatWorkers::active_workers() {\n+  return _active_workers;\n@@ -1158,0 +1380,1 @@\n+  ZLocker<ZLock> locker(&_stat_lock);\n@@ -1164,0 +1387,16 @@\n+double ZStatWorkers::get_and_reset_time() {\n+  ZLocker<ZLock> locker(&_stat_lock);\n+  const double time = _accumulated_time.seconds();\n+  const Ticks now = Ticks::now();\n+  _accumulated_time = now - now;\n+  return time;\n+}\n+\n+ZStatWorkersStats ZStatWorkers::stats() {\n+  ZLocker<ZLock> locker(&_stat_lock);\n+  return {\n+    accumulated_time(),\n+    accumulated_duration()\n+  };\n+}\n+\n@@ -1170,1 +1409,4 @@\n-  log_info(gc, load)(\"Load: %.2f\/%.2f\/%.2f\", loadavg[0], loadavg[1], loadavg[2]);\n+  log_info(gc, load)(\"Load: %.2f (%.0f%%) \/ %.2f (%.0f%%) \/ %.2f (%.0f%%)\",\n+                     loadavg[0], percent_of(loadavg[0], (double) ZCPU::count()),\n+                     loadavg[1], percent_of(loadavg[1], (double) ZCPU::count()),\n+                     loadavg[2], percent_of(loadavg[2], (double) ZCPU::count()));\n@@ -1176,8 +1418,10 @@\n-size_t ZStatMark::_nstripes;\n-size_t ZStatMark::_nproactiveflush;\n-size_t ZStatMark::_nterminateflush;\n-size_t ZStatMark::_ntrycomplete;\n-size_t ZStatMark::_ncontinue;\n-size_t ZStatMark::_mark_stack_usage;\n-\n-void ZStatMark::set_at_mark_start(size_t nstripes) {\n+ZStatMark::ZStatMark() :\n+    _nstripes(),\n+    _nproactiveflush(),\n+    _nterminateflush(),\n+    _ntrycomplete(),\n+    _ncontinue(),\n+    _mark_stack_usage() {\n+}\n+\n+void ZStatMark::at_mark_start(size_t nstripes) {\n@@ -1187,4 +1431,4 @@\n-void ZStatMark::set_at_mark_end(size_t nproactiveflush,\n-                                size_t nterminateflush,\n-                                size_t ntrycomplete,\n-                                size_t ncontinue) {\n+void ZStatMark::at_mark_end(size_t nproactiveflush,\n+                            size_t nterminateflush,\n+                            size_t ntrycomplete,\n+                            size_t ncontinue) {\n@@ -1197,1 +1441,1 @@\n-void ZStatMark::set_at_mark_free(size_t mark_stack_usage) {\n+void ZStatMark::at_mark_free(size_t mark_stack_usage) {\n@@ -1220,4 +1464,8 @@\n-ZRelocationSetSelectorStats ZStatRelocation::_selector_stats;\n-size_t                      ZStatRelocation::_forwarding_usage;\n-size_t                      ZStatRelocation::_small_in_place_count;\n-size_t                      ZStatRelocation::_medium_in_place_count;\n+ZStatRelocation::ZStatRelocation() :\n+    _selector_stats(),\n+    _forwarding_usage(),\n+    _small_selected(),\n+    _small_in_place_count(),\n+    _medium_selected(),\n+    _medium_in_place_count() {\n+}\n@@ -1225,1 +1473,1 @@\n-void ZStatRelocation::set_at_select_relocation_set(const ZRelocationSetSelectorStats& selector_stats) {\n+void ZStatRelocation::at_select_relocation_set(const ZRelocationSetSelectorStats& selector_stats) {\n@@ -1229,1 +1477,1 @@\n-void ZStatRelocation::set_at_install_relocation_set(size_t forwarding_usage) {\n+void ZStatRelocation::at_install_relocation_set(size_t forwarding_usage) {\n@@ -1233,1 +1481,1 @@\n-void ZStatRelocation::set_at_relocate_end(size_t small_in_place_count, size_t medium_in_place_count) {\n+void ZStatRelocation::at_relocate_end(size_t small_in_place_count, size_t medium_in_place_count) {\n@@ -1238,12 +1486,51 @@\n-void ZStatRelocation::print(const char* name,\n-                            const ZRelocationSetSelectorGroupStats& selector_group,\n-                            size_t in_place_count) {\n-  log_info(gc, reloc)(\"%s Pages: \" SIZE_FORMAT \" \/ \" SIZE_FORMAT \"M, Empty: \" SIZE_FORMAT \"M, \"\n-                      \"Relocated: \" SIZE_FORMAT \"M, In-Place: \" SIZE_FORMAT,\n-                      name,\n-                      selector_group.npages(),\n-                      selector_group.total() \/ M,\n-                      selector_group.empty() \/ M,\n-                      selector_group.relocate() \/ M,\n-                      in_place_count);\n-}\n+void ZStatRelocation::print_page_summary() {\n+  LogTarget(Info, gc, reloc) lt;\n+\n+  if (!_selector_stats.has_relocatable_pages() || !lt.is_enabled()) {\n+    \/\/ Nothing to log or logging not enabled.\n+    return;\n+  }\n+\n+  \/\/ Zero initialize\n+  ZStatRelocationSummary small_summary{};\n+  ZStatRelocationSummary medium_summary{};\n+  ZStatRelocationSummary large_summary{};\n+\n+  auto account_page_size = [&](ZStatRelocationSummary& summary, const ZRelocationSetSelectorGroupStats& stats) {\n+    summary.npages_candidates += stats.npages_candidates();\n+    summary.total += stats.total();\n+    summary.empty += stats.empty();\n+    summary.npages_selected += stats.npages_selected();\n+    summary.relocate += stats.relocate();\n+  };\n+\n+  for (uint i = 0; i <= ZPageAgeMax; ++i) {\n+    const ZPageAge age = static_cast<ZPageAge>(i);\n+\n+    account_page_size(small_summary, _selector_stats.small(age));\n+    account_page_size(medium_summary, _selector_stats.medium(age));\n+    account_page_size(large_summary, _selector_stats.large(age));\n+  }\n+\n+  ZStatTablePrinter pages(20, 12);\n+  lt.print(\"%s\", pages()\n+           .fill()\n+           .right(\"Candidates\")\n+           .right(\"Selected\")\n+           .right(\"In-Place\")\n+           .right(\"Size\")\n+           .right(\"Empty\")\n+           .right(\"Relocated\")\n+           .end());\n+\n+  auto print_summary = [&](const char* name, ZStatRelocationSummary& summary, size_t in_place_count) {\n+    lt.print(\"%s\", pages()\n+             .left(\"%s Pages:\", name)\n+             .right(\"%zu\", summary.npages_candidates)\n+             .right(\"%zu\", summary.npages_selected)\n+             .right(\"%zu\", in_place_count)\n+             .right(\"%zuM\", summary.total \/ M)\n+             .right(\"%zuM\", summary.empty \/ M)\n+             .right(\"%zuM\", summary.relocate \/M)\n+             .end());\n+  };\n@@ -1251,2 +1538,1 @@\n-void ZStatRelocation::print() {\n-  print(\"Small\", _selector_stats.small(), _small_in_place_count);\n+  print_summary(\"Small\", small_summary, _small_in_place_count);\n@@ -1254,1 +1540,1 @@\n-    print(\"Medium\", _selector_stats.medium(), _medium_in_place_count);\n+    print_summary(\"Medium\", medium_summary, _medium_in_place_count);\n@@ -1256,1 +1542,1 @@\n-  print(\"Large\", _selector_stats.large(), 0 \/* in_place_count *\/);\n+  print_summary(\"Large\", large_summary, 0 \/* in_place_count *\/);\n@@ -1258,1 +1544,77 @@\n-  log_info(gc, reloc)(\"Forwarding Usage: \" SIZE_FORMAT \"M\", _forwarding_usage \/ M);\n+  lt.print(\"Forwarding Usage: \" SIZE_FORMAT \"M\", _forwarding_usage \/ M);\n+}\n+\n+void ZStatRelocation::print_age_table() {\n+  LogTarget(Info, gc, reloc) lt;\n+  if (!_selector_stats.has_relocatable_pages() || !lt.is_enabled()) {\n+    \/\/ Nothing to log or logging not enabled.\n+    return;\n+  }\n+\n+  ZStatTablePrinter age_table(11, 18);\n+  lt.print(\"Age Table:\");\n+  lt.print(\"%s\", age_table()\n+           .fill()\n+           .center(\"Live\")\n+           .center(\"Garbage\")\n+           .center(\"Small\")\n+           .center(\"Medium\")\n+           .center(\"Large\")\n+           .end());\n+\n+  size_t live[ZPageAgeMax + 1] = {};\n+  size_t total[ZPageAgeMax + 1] = {};\n+\n+  uint oldest_none_empty_age = 0;\n+\n+  for (uint i = 0; i <= ZPageAgeMax; ++i) {\n+    ZPageAge age = static_cast<ZPageAge>(i);\n+    auto summarize_pages = [&](const ZRelocationSetSelectorGroupStats& stats) {\n+      live[i] += stats.live();\n+      total[i] += stats.total();\n+    };\n+\n+    summarize_pages(_selector_stats.small(age));\n+    summarize_pages(_selector_stats.medium(age));\n+    summarize_pages(_selector_stats.large(age));\n+\n+    if (total[i] != 0) {\n+      oldest_none_empty_age = i;\n+    }\n+  }\n+\n+  for (uint i = 0; i <= oldest_none_empty_age; ++i) {\n+    ZPageAge age = static_cast<ZPageAge>(i);\n+\n+    FormatBuffer<> age_str(\"\");\n+    if (age == ZPageAge::eden) {\n+      age_str.append(\"Eden\");\n+    } else if (age != ZPageAge::old) {\n+      age_str.append(\"Survivor %d\", i);\n+    }\n+\n+    auto create_age_table = [&]() {\n+      if (live[i] == 0) {\n+        return age_table()\n+              .left(\"%s\", age_str.buffer())\n+              .left(ZTABLE_ARGS_NA);\n+      } else {\n+        return age_table()\n+              .left(\"%s\", age_str.buffer())\n+              .left(ZTABLE_ARGS(live[i]));\n+      }\n+    };\n+\n+    lt.print(\"%s\", create_age_table()\n+              .left(ZTABLE_ARGS(total[i] - live[i]))\n+              .left(SIZE_FORMAT_W(7) \" \/ \" SIZE_FORMAT,\n+                    _selector_stats.small(age).npages_candidates(),\n+                    _selector_stats.small(age).npages_selected())\n+              .left(SIZE_FORMAT_W(7) \" \/ \" SIZE_FORMAT,\n+                    _selector_stats.medium(age).npages_candidates(),\n+                    _selector_stats.medium(age).npages_selected())\n+              .left(SIZE_FORMAT_W(7) \" \/ \" SIZE_FORMAT,\n+                    _selector_stats.large(age).npages_candidates(),\n+                    _selector_stats.large(age).npages_selected())\n+              .end());\n+  }\n@@ -1274,1 +1636,1 @@\n-  MetaspaceCombinedStats stats = MetaspaceUtils::get_combined_statistics();\n+  const MetaspaceCombinedStats stats = MetaspaceUtils::get_combined_statistics();\n@@ -1313,11 +1675,0 @@\n-void ZStatReferences::print(const char* name, const ZStatReferences::ZCount& ref) {\n-  log_info(gc, ref)(\"%s: \"\n-                    SIZE_FORMAT \" encountered, \"\n-                    SIZE_FORMAT \" discovered, \"\n-                    SIZE_FORMAT \" enqueued\",\n-                    name,\n-                    ref.encountered,\n-                    ref.discovered,\n-                    ref.enqueued);\n-}\n-\n@@ -1325,4 +1676,27 @@\n-  print(\"Soft\", _soft);\n-  print(\"Weak\", _weak);\n-  print(\"Final\", _final);\n-  print(\"Phantom\", _phantom);\n+  LogTarget(Info, gc, ref) lt;\n+  if (!lt.is_enabled()) {\n+    \/\/ Nothing to log\n+    return;\n+  }\n+\n+  ZStatTablePrinter refs(20, 12);\n+  lt.print(\"%s\", refs()\n+           .fill()\n+           .right(\"Encountered\")\n+           .right(\"Discovered\")\n+           .right(\"Enqueued\")\n+           .end());\n+\n+  auto ref_print = [&] (const char* name, const ZStatReferences::ZCount& ref) {\n+    lt.print(\"%s\", refs()\n+             .left(\"%s References:\", name)\n+             .right(\"%zu\", ref.encountered)\n+             .right(\"%zu\", ref.discovered)\n+             .right(\"%zu\", ref.enqueued)\n+             .end());\n+  };\n+\n+  ref_print(\"Soft\", _soft);\n+  ref_print(\"Weak\", _weak);\n+  ref_print(\"Final\", _final);\n+  ref_print(\"Phantom\", _phantom);\n@@ -1334,0 +1708,11 @@\n+\n+ZStatHeap::ZStatHeap() :\n+    _stat_lock(),\n+    _at_collection_start(),\n+    _at_mark_start(),\n+    _at_mark_end(),\n+    _at_relocate_start(),\n+    _at_relocate_end(),\n+    _reclaimed_bytes(0.7 \/* alpha *\/) {\n+}\n+\n@@ -1335,4 +1720,0 @@\n-ZStatHeap::ZAtMarkStart ZStatHeap::_at_mark_start;\n-ZStatHeap::ZAtMarkEnd ZStatHeap::_at_mark_end;\n-ZStatHeap::ZAtRelocateStart ZStatHeap::_at_relocate_start;\n-ZStatHeap::ZAtRelocateEnd ZStatHeap::_at_relocate_end;\n@@ -1340,1 +1721,1 @@\n-size_t ZStatHeap::capacity_high() {\n+size_t ZStatHeap::capacity_high() const {\n@@ -1347,1 +1728,1 @@\n-size_t ZStatHeap::capacity_low() {\n+size_t ZStatHeap::capacity_low() const {\n@@ -1354,1 +1735,1 @@\n-size_t ZStatHeap::free(size_t used) {\n+size_t ZStatHeap::free(size_t used) const {\n@@ -1358,1 +1739,1 @@\n-size_t ZStatHeap::allocated(size_t used, size_t reclaimed) {\n+size_t ZStatHeap::mutator_allocated(size_t used_generation, size_t freed, size_t relocated) const {\n@@ -1364,1 +1745,2 @@\n-  return (used + reclaimed) - _at_mark_start.used;\n+  const size_t used_generation_delta = used_generation - _at_mark_start.used_generation;\n+  return  used_generation_delta + freed - relocated;\n@@ -1367,2 +1749,2 @@\n-size_t ZStatHeap::garbage(size_t reclaimed) {\n-  return _at_mark_end.garbage - reclaimed;\n+size_t ZStatHeap::garbage(size_t freed, size_t relocated, size_t promoted) const {\n+  return _at_mark_end.garbage - (freed - promoted - relocated);\n@@ -1371,3 +1753,2 @@\n-void ZStatHeap::set_at_initialize(const ZPageAllocatorStats& stats) {\n-  _at_initialize.min_capacity = stats.min_capacity();\n-  _at_initialize.max_capacity = stats.max_capacity();\n+size_t ZStatHeap::reclaimed(size_t freed, size_t relocated, size_t promoted) const {\n+  return freed - relocated - promoted;\n@@ -1376,1 +1757,20 @@\n-void ZStatHeap::set_at_mark_start(const ZPageAllocatorStats& stats) {\n+void ZStatHeap::at_initialize(size_t min_capacity, size_t max_capacity) {\n+  ZLocker<ZLock> locker(&_stat_lock);\n+\n+  _at_initialize.min_capacity = min_capacity;\n+  _at_initialize.max_capacity = max_capacity;\n+}\n+\n+void ZStatHeap::at_collection_start(const ZPageAllocatorStats& stats) {\n+  ZLocker<ZLock> locker(&_stat_lock);\n+\n+  _at_collection_start.soft_max_capacity = stats.soft_max_capacity();\n+  _at_collection_start.capacity = stats.capacity();\n+  _at_collection_start.free = free(stats.used());\n+  _at_collection_start.used = stats.used();\n+  _at_collection_start.used_generation = stats.used_generation();\n+}\n+\n+void ZStatHeap::at_mark_start(const ZPageAllocatorStats& stats) {\n+  ZLocker<ZLock> locker(&_stat_lock);\n+\n@@ -1381,0 +1781,2 @@\n+  _at_mark_start.used_generation = stats.used_generation();\n+  _at_mark_start.allocation_stalls = stats.allocation_stalls();\n@@ -1383,1 +1785,3 @@\n-void ZStatHeap::set_at_mark_end(const ZPageAllocatorStats& stats) {\n+void ZStatHeap::at_mark_end(const ZPageAllocatorStats& stats) {\n+  ZLocker<ZLock> locker(&_stat_lock);\n+\n@@ -1387,1 +1791,3 @@\n-  _at_mark_end.allocated = allocated(stats.used(), 0 \/* reclaimed *\/);\n+  _at_mark_end.used_generation = stats.used_generation();\n+  _at_mark_end.mutator_allocated = mutator_allocated(stats.used_generation(), 0 \/* reclaimed *\/, 0 \/* relocated *\/);\n+  _at_mark_end.allocation_stalls = stats.allocation_stalls();\n@@ -1390,2 +1796,8 @@\n-void ZStatHeap::set_at_select_relocation_set(const ZRelocationSetSelectorStats& stats) {\n-  const size_t live = stats.small().live() + stats.medium().live() + stats.large().live();\n+void ZStatHeap::at_select_relocation_set(const ZRelocationSetSelectorStats& stats) {\n+  ZLocker<ZLock> locker(&_stat_lock);\n+\n+  size_t live = 0;\n+  for (uint i = 0; i <= ZPageAgeMax; ++i) {\n+    const ZPageAge age = static_cast<ZPageAge>(i);\n+    live += stats.small(age).live() + stats.medium(age).live() + stats.large(age).live();\n+  }\n@@ -1393,1 +1805,1 @@\n-  _at_mark_end.garbage = _at_mark_start.used - live;\n+  _at_mark_end.garbage = _at_mark_start.used_generation - live;\n@@ -1396,1 +1808,5 @@\n-void ZStatHeap::set_at_relocate_start(const ZPageAllocatorStats& stats) {\n+void ZStatHeap::at_relocate_start(const ZPageAllocatorStats& stats) {\n+  ZLocker<ZLock> locker(&_stat_lock);\n+\n+  assert(stats.compacted() == 0, \"Nothing should have been compacted\");\n+\n@@ -1400,3 +1816,8 @@\n-  _at_relocate_start.allocated = allocated(stats.used(), stats.reclaimed());\n-  _at_relocate_start.garbage = garbage(stats.reclaimed());\n-  _at_relocate_start.reclaimed = stats.reclaimed();\n+  _at_relocate_start.used_generation = stats.used_generation();\n+  _at_relocate_start.live = _at_mark_end.live - stats.promoted();\n+  _at_relocate_start.garbage = garbage(stats.freed(), stats.compacted(), stats.promoted());\n+  _at_relocate_start.mutator_allocated = mutator_allocated(stats.used_generation(), stats.freed(), stats.compacted());\n+  _at_relocate_start.reclaimed = reclaimed(stats.freed(), stats.compacted(), stats.promoted());\n+  _at_relocate_start.promoted = stats.promoted();\n+  _at_relocate_start.compacted = stats.compacted();\n+  _at_relocate_start.allocation_stalls = stats.allocation_stalls();\n@@ -1405,2 +1826,2 @@\n-void ZStatHeap::set_at_relocate_end(const ZPageAllocatorStats& stats, size_t non_worker_relocated) {\n-  const size_t reclaimed = stats.reclaimed() - MIN2(non_worker_relocated, stats.reclaimed());\n+void ZStatHeap::at_relocate_end(const ZPageAllocatorStats& stats, bool record_stats) {\n+  ZLocker<ZLock> locker(&_stat_lock);\n@@ -1417,3 +1838,16 @@\n-  _at_relocate_end.allocated = allocated(stats.used(), reclaimed);\n-  _at_relocate_end.garbage = garbage(reclaimed);\n-  _at_relocate_end.reclaimed = reclaimed;\n+  _at_relocate_end.used_generation = stats.used_generation();\n+  _at_relocate_end.live = _at_mark_end.live - stats.promoted();\n+  _at_relocate_end.garbage = garbage(stats.freed(), stats.compacted(), stats.promoted());\n+  _at_relocate_end.mutator_allocated = mutator_allocated(stats.used_generation(), stats.freed(), stats.compacted());\n+  _at_relocate_end.reclaimed = reclaimed(stats.freed(), stats.compacted(), stats.promoted());\n+  _at_relocate_end.promoted = stats.promoted();\n+  _at_relocate_end.compacted = stats.compacted();\n+  _at_relocate_end.allocation_stalls = stats.allocation_stalls();\n+\n+  if (record_stats) {\n+    _reclaimed_bytes.add(_at_relocate_end.reclaimed);\n+  }\n+}\n+\n+size_t ZStatHeap::reclaimed_avg() {\n+  return _reclaimed_bytes.davg();\n@@ -1426,1 +1860,5 @@\n-size_t ZStatHeap::used_at_mark_start() {\n+size_t ZStatHeap::used_at_collection_start() const {\n+  return _at_collection_start.used;\n+}\n+\n+size_t ZStatHeap::used_at_mark_start() const {\n@@ -1430,1 +1868,17 @@\n-size_t ZStatHeap::used_at_relocate_end() {\n+size_t ZStatHeap::used_generation_at_mark_start() const {\n+  return _at_mark_start.used_generation;\n+}\n+\n+size_t ZStatHeap::live_at_mark_end() const {\n+  return _at_mark_end.live;\n+}\n+\n+size_t ZStatHeap::allocated_at_mark_end() const {\n+  return _at_mark_end.mutator_allocated;\n+}\n+\n+size_t ZStatHeap::garbage_at_mark_end() const {\n+  return _at_mark_end.garbage;\n+}\n+\n+size_t ZStatHeap::used_at_relocate_end() const {\n@@ -1434,1 +1888,31 @@\n-void ZStatHeap::print() {\n+size_t ZStatHeap::used_at_collection_end() const {\n+  return used_at_relocate_end();\n+}\n+\n+size_t ZStatHeap::stalls_at_mark_start() const {\n+  return _at_mark_start.allocation_stalls;\n+}\n+\n+size_t ZStatHeap::stalls_at_mark_end() const {\n+  return _at_mark_end.allocation_stalls;\n+}\n+\n+size_t ZStatHeap::stalls_at_relocate_start() const {\n+  return _at_relocate_start.allocation_stalls;\n+}\n+\n+size_t ZStatHeap::stalls_at_relocate_end() const {\n+  return _at_relocate_end.allocation_stalls;\n+}\n+\n+ZStatHeapStats ZStatHeap::stats() {\n+  ZLocker<ZLock> locker(&_stat_lock);\n+\n+  return {\n+    live_at_mark_end(),\n+    used_at_relocate_end(),\n+    reclaimed_avg()\n+  };\n+}\n+\n+void ZStatHeap::print(const ZGeneration* generation) const {\n@@ -1442,2 +1926,3 @@\n-  ZStatTablePrinter table(10, 18);\n-  log_info(gc, heap)(\"%s\", table()\n+  log_info(gc, heap)(\"Heap Statistics:\");\n+  ZStatTablePrinter heap_table(10, 18);\n+  log_info(gc, heap)(\"%s\", heap_table()\n@@ -1452,1 +1937,1 @@\n-  log_info(gc, heap)(\"%s\", table()\n+  log_info(gc, heap)(\"%s\", heap_table()\n@@ -1461,1 +1946,1 @@\n-  log_info(gc, heap)(\"%s\", table()\n+  log_info(gc, heap)(\"%s\", heap_table()\n@@ -1470,1 +1955,1 @@\n-  log_info(gc, heap)(\"%s\", table()\n+  log_info(gc, heap)(\"%s\", heap_table()\n@@ -1479,1 +1964,18 @@\n-  log_info(gc, heap)(\"%s\", table()\n+\n+  log_info(gc, heap)(\"%s Generation Statistics:\", generation->is_young() ? \"Young\" : \"Old\");\n+  ZStatTablePrinter gen_table(10, 18);\n+  log_info(gc, heap)(\"%s\", gen_table()\n+                     .fill()\n+                     .center(\"Mark Start\")\n+                     .center(\"Mark End\")\n+                     .center(\"Relocate Start\")\n+                     .center(\"Relocate End\")\n+                     .end());\n+  log_info(gc, heap)(\"%s\", gen_table()\n+                     .right(\"Used:\")\n+                     .left(ZTABLE_ARGS(_at_mark_start.used_generation))\n+                     .left(ZTABLE_ARGS(_at_mark_end.used_generation))\n+                     .left(ZTABLE_ARGS(_at_relocate_start.used_generation))\n+                     .left(ZTABLE_ARGS(_at_relocate_end.used_generation))\n+                     .end());\n+  log_info(gc, heap)(\"%s\", gen_table()\n@@ -1483,4 +1985,2 @@\n-                     .left(ZTABLE_ARGS(_at_mark_end.live \/* Same as at mark end *\/))\n-                     .left(ZTABLE_ARGS(_at_mark_end.live \/* Same as at mark end *\/))\n-                     .left(ZTABLE_ARGS_NA)\n-                     .left(ZTABLE_ARGS_NA)\n+                     .left(ZTABLE_ARGS(_at_relocate_start.live))\n+                     .left(ZTABLE_ARGS(_at_relocate_end.live))\n@@ -1488,10 +1988,1 @@\n-  log_info(gc, heap)(\"%s\", table()\n-                     .right(\"Allocated:\")\n-                     .left(ZTABLE_ARGS_NA)\n-                     .left(ZTABLE_ARGS(_at_mark_end.allocated))\n-                     .left(ZTABLE_ARGS(_at_relocate_start.allocated))\n-                     .left(ZTABLE_ARGS(_at_relocate_end.allocated))\n-                     .left(ZTABLE_ARGS_NA)\n-                     .left(ZTABLE_ARGS_NA)\n-                     .end());\n-  log_info(gc, heap)(\"%s\", table()\n+  log_info(gc, heap)(\"%s\", gen_table()\n@@ -1503,0 +1994,3 @@\n+                     .end());\n+  log_info(gc, heap)(\"%s\", gen_table()\n+                     .right(\"Allocated:\")\n@@ -1504,1 +1998,3 @@\n-                     .left(ZTABLE_ARGS_NA)\n+                     .left(ZTABLE_ARGS(_at_mark_end.mutator_allocated))\n+                     .left(ZTABLE_ARGS(_at_relocate_start.mutator_allocated))\n+                     .left(ZTABLE_ARGS(_at_relocate_end.mutator_allocated))\n@@ -1506,1 +2002,1 @@\n-  log_info(gc, heap)(\"%s\", table()\n+  log_info(gc, heap)(\"%s\", gen_table()\n@@ -1512,0 +2008,13 @@\n+                     .end());\n+  if (generation->is_young()) {\n+    log_info(gc, heap)(\"%s\", gen_table()\n+                       .right(\"Promoted:\")\n+                       .left(ZTABLE_ARGS_NA)\n+                       .left(ZTABLE_ARGS_NA)\n+                       .left(ZTABLE_ARGS(_at_relocate_start.promoted))\n+                       .left(ZTABLE_ARGS(_at_relocate_end.promoted))\n+                       .end());\n+  }\n+  log_info(gc, heap)(\"%s\", gen_table()\n+                     .right(\"Compacted:\")\n+                     .left(ZTABLE_ARGS_NA)\n@@ -1514,0 +2023,19 @@\n+                     .left(ZTABLE_ARGS(_at_relocate_end.compacted))\n+                     .end());\n+}\n+\n+void ZStatHeap::print_stalls() const {\n+  ZStatTablePrinter stall_table(20, 16);\n+  log_info(gc, alloc)(\"%s\", stall_table()\n+                     .fill()\n+                     .center(\"Mark Start\")\n+                     .center(\"Mark End\")\n+                     .center(\"Relocate Start\")\n+                     .center(\"Relocate End\")\n+                     .end());\n+  log_info(gc, alloc)(\"%s\", stall_table()\n+                     .left(\"%s\", \"Allocation Stalls:\")\n+                     .center(\"%zu\", _at_mark_start.allocation_stalls)\n+                     .center(\"%zu\", _at_mark_end.allocation_stalls)\n+                     .center(\"%zu\", _at_relocate_start.allocation_stalls)\n+                     .center(\"%zu\", _at_relocate_end.allocation_stalls)\n","filename":"src\/hotspot\/share\/gc\/z\/zStat.cpp","additions":758,"deletions":230,"binary":false,"changes":988,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,1 +27,0 @@\n-#include \"gc\/shared\/concurrentGCThread.hpp\"\n@@ -30,0 +29,2 @@\n+#include \"gc\/z\/zGenerationId.hpp\"\n+#include \"gc\/z\/zLock.hpp\"\n@@ -31,0 +32,3 @@\n+#include \"gc\/z\/zRelocationSetSelector.hpp\"\n+#include \"gc\/z\/zThread.hpp\"\n+#include \"gc\/z\/zTracer.hpp\"\n@@ -37,0 +41,2 @@\n+class GCTracer;\n+class ZGeneration;\n@@ -40,1 +46,0 @@\n-class ZRelocationSetSelectorStats;\n@@ -43,0 +48,1 @@\n+class ZStatWorkers;\n@@ -207,3 +213,0 @@\n-private:\n-  static ConcurrentGCTimer _timer;\n-\n@@ -219,2 +222,0 @@\n-  static ConcurrentGCTimer* timer();\n-\n@@ -223,2 +224,18 @@\n-  virtual void register_start(const Ticks& start) const = 0;\n-  virtual void register_end(const Ticks& start, const Ticks& end) const = 0;\n+  virtual void register_start(ConcurrentGCTimer* timer, const Ticks& start) const = 0;\n+  virtual void register_end(ConcurrentGCTimer* timer, const Ticks& start, const Ticks& end) const = 0;\n+};\n+\n+class ZStatPhaseCollection : public ZStatPhase {\n+private:\n+  const bool _minor;\n+\n+  GCTracer* jfr_tracer() const;\n+\n+  void set_used_at_start(size_t used) const;\n+  size_t used_at_start() const;\n+\n+public:\n+  ZStatPhaseCollection(const char* name, bool minor);\n+\n+  virtual void register_start(ConcurrentGCTimer* timer, const Ticks& start) const;\n+  virtual void register_end(ConcurrentGCTimer* timer, const Ticks& start, const Ticks& end) const;\n@@ -227,1 +244,6 @@\n-class ZStatPhaseCycle : public ZStatPhase {\n+class ZStatPhaseGeneration : public ZStatPhase {\n+private:\n+  const ZGenerationId _id;\n+\n+  ZGenerationTracer* jfr_tracer() const;\n+\n@@ -229,1 +251,1 @@\n-  ZStatPhaseCycle(const char* name);\n+  ZStatPhaseGeneration(const char* name, ZGenerationId id);\n@@ -231,2 +253,2 @@\n-  virtual void register_start(const Ticks& start) const;\n-  virtual void register_end(const Ticks& start, const Ticks& end) const;\n+  virtual void register_start(ConcurrentGCTimer* timer, const Ticks& start) const;\n+  virtual void register_end(ConcurrentGCTimer* timer, const Ticks& start, const Ticks& end) const;\n@@ -240,1 +262,1 @@\n-  ZStatPhasePause(const char* name);\n+  ZStatPhasePause(const char* name, ZGenerationId id);\n@@ -244,2 +266,2 @@\n-  virtual void register_start(const Ticks& start) const;\n-  virtual void register_end(const Ticks& start, const Ticks& end) const;\n+  virtual void register_start(ConcurrentGCTimer* timer, const Ticks& start) const;\n+  virtual void register_end(ConcurrentGCTimer* timer, const Ticks& start, const Ticks& end) const;\n@@ -250,1 +272,1 @@\n-  ZStatPhaseConcurrent(const char* name);\n+  ZStatPhaseConcurrent(const char* name, ZGenerationId id);\n@@ -252,2 +274,2 @@\n-  virtual void register_start(const Ticks& start) const;\n-  virtual void register_end(const Ticks& start, const Ticks& end) const;\n+  virtual void register_start(ConcurrentGCTimer* timer, const Ticks& start) const;\n+  virtual void register_end(ConcurrentGCTimer* timer, const Ticks& start, const Ticks& end) const;\n@@ -258,1 +280,1 @@\n-  ZStatSubPhase(const char* name);\n+  ZStatSubPhase(const char* name, ZGenerationId id);\n@@ -260,2 +282,2 @@\n-  virtual void register_start(const Ticks& start) const;\n-  virtual void register_end(const Ticks& start, const Ticks& end) const;\n+  virtual void register_start(ConcurrentGCTimer* timer, const Ticks& start) const;\n+  virtual void register_end(ConcurrentGCTimer* timer, const Ticks& start, const Ticks& end) const;\n@@ -272,2 +294,2 @@\n-  virtual void register_start(const Ticks& start) const;\n-  virtual void register_end(const Ticks& start, const Ticks& end) const;\n+  virtual void register_start(ConcurrentGCTimer* timer, const Ticks& start) const;\n+  virtual void register_end(ConcurrentGCTimer* timer, const Ticks& start, const Ticks& end) const;\n@@ -279,1 +301,1 @@\n-class ZStatTimerDisable : public StackObj {\n+class ZStatTimer : public StackObj {\n@@ -281,1 +303,3 @@\n-  static THREAD_LOCAL uint32_t _active;\n+  ConcurrentGCTimer* const _gc_timer;\n+  const ZStatPhase&        _phase;\n+  const Ticks              _start;\n@@ -284,2 +308,9 @@\n-  ZStatTimerDisable() {\n-    _active++;\n+  ZStatTimer(const ZStatPhase& phase, ConcurrentGCTimer* gc_timer) :\n+      _gc_timer(gc_timer),\n+      _phase(phase),\n+      _start(Ticks::now()) {\n+    _phase.register_start(_gc_timer, _start);\n+  }\n+\n+  ZStatTimer(const ZStatSubPhase& phase) :\n+      ZStatTimer(phase, nullptr \/* timer *\/) {\n@@ -288,2 +319,2 @@\n-  ~ZStatTimerDisable() {\n-    _active--;\n+  ZStatTimer(const ZStatCriticalPhase& phase) :\n+      ZStatTimer(phase, nullptr \/* timer *\/) {\n@@ -292,2 +323,3 @@\n-  static bool is_active() {\n-    return _active > 0;\n+  ~ZStatTimer() {\n+    const Ticks end = Ticks::now();\n+    _phase.register_end(_gc_timer, _start, end);\n@@ -297,5 +329,4 @@\n-class ZStatTimer : public StackObj {\n-private:\n-  const bool        _enabled;\n-  const ZStatPhase& _phase;\n-  const Ticks       _start;\n+class ZStatTimerYoung : public ZStatTimer {\n+public:\n+  ZStatTimerYoung(const ZStatPhase& phase);\n+};\n@@ -303,0 +334,1 @@\n+class ZStatTimerOld : public ZStatTimer {\n@@ -304,8 +336,2 @@\n-  ZStatTimer(const ZStatPhase& phase) :\n-      _enabled(!ZStatTimerDisable::is_active()),\n-      _phase(phase),\n-      _start(Ticks::now()) {\n-    if (_enabled) {\n-      _phase.register_start(_start);\n-    }\n-  }\n+  ZStatTimerOld(const ZStatPhase& phase);\n+};\n@@ -313,6 +339,3 @@\n-  ~ZStatTimer() {\n-    if (_enabled) {\n-      const Ticks end = Ticks::now();\n-      _phase.register_end(_start, end);\n-    }\n-  }\n+class ZStatTimerWorker : public ZStatTimer {\n+public:\n+  ZStatTimerWorker(const ZStatPhase& phase);\n@@ -328,0 +351,6 @@\n+struct ZStatMutatorAllocRateStats {\n+  double _avg;\n+  double _predict;\n+  double _sd;\n+};\n+\n@@ -329,1 +358,1 @@\n-\/\/ Stat allocation rate\n+\/\/ Stat mutator allocation rate\n@@ -331,1 +360,1 @@\n-class ZStatAllocRate : public AllStatic {\n+class ZStatMutatorAllocRate : public AllStatic {\n@@ -333,3 +362,7 @@\n-  static const ZStatUnsampledCounter _counter;\n-  static TruncatedSeq                _samples;\n-  static TruncatedSeq                _rate;\n+  static ZLock*          _stat_lock;\n+  static jlong           _last_sample_time;\n+  static volatile size_t _sampling_granule;\n+  static volatile size_t _allocated_since_sample;\n+  static TruncatedSeq    _samples_time;\n+  static TruncatedSeq    _samples_bytes;\n+  static TruncatedSeq    _rate;\n@@ -337,2 +370,1 @@\n-public:\n-  static const uint64_t sample_hz = 10;\n+  static void update_sampling_granule();\n@@ -340,0 +372,1 @@\n+public:\n@@ -341,1 +374,3 @@\n-  static uint64_t sample_and_reset();\n+  static void sample_allocation(size_t allocation_bytes);\n+\n+  static void initialize();\n@@ -343,3 +378,1 @@\n-  static double predict();\n-  static double avg();\n-  static double sd();\n+  static ZStatMutatorAllocRateStats stats();\n@@ -351,1 +384,1 @@\n-class ZStat : public ConcurrentGCThread {\n+class ZStat : public ZThread {\n@@ -362,2 +395,2 @@\n-  virtual void run_service();\n-  virtual void stop_service();\n+  virtual void run_thread();\n+  virtual void terminate();\n@@ -369,0 +402,16 @@\n+struct ZStatCycleStats {\n+  bool _is_warm;\n+  uint64_t _nwarmup_cycles;\n+  bool _is_time_trustable;\n+  double _time_since_last;\n+  double _last_active_workers;\n+  double _duration_since_start;\n+  double _avg_cycle_interval;\n+  double _avg_serial_time;\n+  double _sd_serial_time;\n+  double _avg_parallelizable_time;\n+  double _sd_parallelizable_time;\n+  double _avg_parallelizable_duration;\n+  double _sd_parallelizable_duration;\n+};\n+\n@@ -372,1 +421,1 @@\n-class ZStatCycle : public AllStatic {\n+class ZStatCycle {\n@@ -374,6 +423,15 @@\n-  static uint64_t  _nwarmup_cycles;\n-  static Ticks     _start_of_last;\n-  static Ticks     _end_of_last;\n-  static NumberSeq _serial_time;\n-  static NumberSeq _parallelizable_time;\n-  static uint      _last_active_workers;\n+  ZLock     _stat_lock;\n+  uint64_t  _nwarmup_cycles;\n+  Ticks     _start_of_last;\n+  Ticks     _end_of_last;\n+  NumberSeq _cycle_intervals;\n+  NumberSeq _serial_time;\n+  NumberSeq _parallelizable_time;\n+  NumberSeq _parallelizable_duration;\n+  double    _last_active_workers;\n+\n+  bool is_warm();\n+  bool is_time_trustable();\n+  double last_active_workers();\n+  double duration_since_start();\n+  double time_since_last();\n@@ -382,5 +440,1 @@\n-  static void at_start();\n-  static void at_end(GCCause::Cause cause, uint active_workers);\n-\n-  static bool is_warm();\n-  static uint64_t nwarmup_cycles();\n+  ZStatCycle();\n@@ -388,3 +442,2 @@\n-  static bool is_time_trustable();\n-  static const AbsSeq& serial_time();\n-  static const AbsSeq& parallelizable_time();\n+  void at_start();\n+  void at_end(ZStatWorkers* stats_workers, bool record_stats);\n@@ -392,1 +445,2 @@\n-  static uint last_active_workers();\n+  ZStatCycleStats stats();\n+};\n@@ -394,1 +448,3 @@\n-  static double time_since_last();\n+struct ZStatWorkersStats {\n+  double _accumulated_time;\n+  double _accumulated_duration;\n@@ -400,1 +456,1 @@\n-class ZStatWorkers : public AllStatic {\n+class ZStatWorkers {\n@@ -402,2 +458,9 @@\n-  static Ticks    _start_of_last;\n-  static Tickspan _accumulated_duration;\n+  ZLock    _stat_lock;\n+  uint     _active_workers;\n+  Ticks    _start_of_last;\n+  Tickspan _accumulated_duration;\n+  Tickspan _accumulated_time;\n+\n+  double accumulated_duration();\n+  double accumulated_time();\n+  uint active_workers();\n@@ -406,2 +469,7 @@\n-  static void at_start();\n-  static void at_end();\n+  ZStatWorkers();\n+\n+  void at_start(uint active_workers);\n+  void at_end();\n+\n+  double get_and_reset_duration();\n+  double get_and_reset_time();\n@@ -409,1 +477,1 @@\n-  static double get_and_reset_duration();\n+  ZStatWorkersStats stats();\n@@ -423,1 +491,1 @@\n-class ZStatMark : public AllStatic {\n+class ZStatMark {\n@@ -425,6 +493,6 @@\n-  static size_t _nstripes;\n-  static size_t _nproactiveflush;\n-  static size_t _nterminateflush;\n-  static size_t _ntrycomplete;\n-  static size_t _ncontinue;\n-  static size_t _mark_stack_usage;\n+  size_t _nstripes;\n+  size_t _nproactiveflush;\n+  size_t _nterminateflush;\n+  size_t _ntrycomplete;\n+  size_t _ncontinue;\n+  size_t _mark_stack_usage;\n@@ -433,6 +501,1 @@\n-  static void set_at_mark_start(size_t nstripes);\n-  static void set_at_mark_end(size_t nproactiveflush,\n-                              size_t nterminateflush,\n-                              size_t ntrycomplete,\n-                              size_t ncontinue);\n-  static void set_at_mark_free(size_t mark_stack_usage);\n+  ZStatMark();\n@@ -440,1 +503,17 @@\n-  static void print();\n+  void at_mark_start(size_t nstripes);\n+  void at_mark_end(size_t nproactiveflush,\n+                   size_t nterminateflush,\n+                   size_t ntrycomplete,\n+                   size_t ncontinue);\n+  void at_mark_free(size_t mark_stack_usage);\n+\n+  void print();\n+};\n+\n+struct ZStatRelocationSummary {\n+  size_t npages_candidates;\n+  size_t total;\n+  size_t live;\n+  size_t empty;\n+  size_t npages_selected;\n+  size_t relocate;\n@@ -446,1 +525,1 @@\n-class ZStatRelocation : public AllStatic {\n+class ZStatRelocation {\n@@ -448,4 +527,6 @@\n-  static ZRelocationSetSelectorStats _selector_stats;\n-  static size_t                      _forwarding_usage;\n-  static size_t                      _small_in_place_count;\n-  static size_t                      _medium_in_place_count;\n+  ZRelocationSetSelectorStats _selector_stats;\n+  size_t                      _forwarding_usage;\n+  size_t                      _small_selected;\n+  size_t                      _small_in_place_count;\n+  size_t                      _medium_selected;\n+  size_t                      _medium_in_place_count;\n@@ -453,3 +534,3 @@\n-  static void print(const char* name,\n-                    const ZRelocationSetSelectorGroupStats& selector_group,\n-                    size_t in_place_count);\n+  void print(const char* name,\n+             ZStatRelocationSummary selector_group,\n+             size_t in_place_count);\n@@ -458,3 +539,1 @@\n-  static void set_at_select_relocation_set(const ZRelocationSetSelectorStats& selector_stats);\n-  static void set_at_install_relocation_set(size_t forwarding_usage);\n-  static void set_at_relocate_end(size_t small_in_place_count, size_t medium_in_place_count);\n+  ZStatRelocation();\n@@ -462,1 +541,6 @@\n-  static void print();\n+  void at_select_relocation_set(const ZRelocationSetSelectorStats& selector_stats);\n+  void at_install_relocation_set(size_t forwarding_usage);\n+  void at_relocate_end(size_t small_in_place_count, size_t medium_in_place_count);\n+\n+  void print_page_summary();\n+  void print_age_table();\n@@ -493,1 +577,0 @@\n-  static void print(const char* name, const ZCount& ref);\n@@ -504,0 +587,6 @@\n+struct ZStatHeapStats {\n+  size_t _live_at_mark_end;\n+  size_t _used_at_relocate_end;\n+  size_t _reclaimed_avg;\n+};\n+\n@@ -507,1 +596,1 @@\n-class ZStatHeap : public AllStatic {\n+class ZStatHeap {\n@@ -509,0 +598,2 @@\n+  ZLock _stat_lock;\n+\n@@ -514,1 +605,9 @@\n-  static struct ZAtMarkStart {\n+  struct ZAtGenerationCollectionStart {\n+    size_t soft_max_capacity;\n+    size_t capacity;\n+    size_t free;\n+    size_t used;\n+    size_t used_generation;\n+  } _at_collection_start;\n+\n+  struct ZAtMarkStart {\n@@ -519,0 +618,2 @@\n+    size_t used_generation;\n+    size_t allocation_stalls;\n@@ -521,1 +622,1 @@\n-  static struct ZAtMarkEnd {\n+  struct ZAtMarkEnd {\n@@ -525,0 +626,1 @@\n+    size_t used_generation;\n@@ -526,1 +628,0 @@\n-    size_t allocated;\n@@ -528,0 +629,2 @@\n+    size_t mutator_allocated;\n+    size_t allocation_stalls;\n@@ -530,1 +633,1 @@\n-  static struct ZAtRelocateStart {\n+  struct ZAtRelocateStart {\n@@ -534,1 +637,2 @@\n-    size_t allocated;\n+    size_t used_generation;\n+    size_t live;\n@@ -536,0 +640,1 @@\n+    size_t mutator_allocated;\n@@ -537,0 +642,3 @@\n+    size_t promoted;\n+    size_t compacted;\n+    size_t allocation_stalls;\n@@ -539,1 +647,1 @@\n-  static struct ZAtRelocateEnd {\n+  struct ZAtRelocateEnd {\n@@ -549,1 +657,2 @@\n-    size_t allocated;\n+    size_t used_generation;\n+    size_t live;\n@@ -551,0 +660,1 @@\n+    size_t mutator_allocated;\n@@ -552,0 +662,3 @@\n+    size_t promoted;\n+    size_t compacted;\n+    size_t allocation_stalls;\n@@ -554,5 +667,8 @@\n-  static size_t capacity_high();\n-  static size_t capacity_low();\n-  static size_t free(size_t used);\n-  static size_t allocated(size_t used, size_t reclaimed);\n-  static size_t garbage(size_t reclaimed);\n+  NumberSeq _reclaimed_bytes;\n+\n+  size_t capacity_high() const;\n+  size_t capacity_low() const;\n+  size_t free(size_t used) const;\n+  size_t mutator_allocated(size_t used, size_t freed, size_t relocated) const;\n+  size_t garbage(size_t freed, size_t relocated, size_t promoted) const;\n+  size_t reclaimed(size_t freed, size_t relocated, size_t promoted) const;\n@@ -561,6 +677,1 @@\n-  static void set_at_initialize(const ZPageAllocatorStats& stats);\n-  static void set_at_mark_start(const ZPageAllocatorStats& stats);\n-  static void set_at_mark_end(const ZPageAllocatorStats& stats);\n-  static void set_at_select_relocation_set(const ZRelocationSetSelectorStats& stats);\n-  static void set_at_relocate_start(const ZPageAllocatorStats& stats);\n-  static void set_at_relocate_end(const ZPageAllocatorStats& stats, size_t non_worker_relocated);\n+  ZStatHeap();\n@@ -568,3 +679,7 @@\n-  static size_t max_capacity();\n-  static size_t used_at_mark_start();\n-  static size_t used_at_relocate_end();\n+  void at_initialize(size_t min_capacity, size_t max_capacity);\n+  void at_collection_start(const ZPageAllocatorStats& stats);\n+  void at_mark_start(const ZPageAllocatorStats& stats);\n+  void at_mark_end(const ZPageAllocatorStats& stats);\n+  void at_select_relocation_set(const ZRelocationSetSelectorStats& stats);\n+  void at_relocate_start(const ZPageAllocatorStats& stats);\n+  void at_relocate_end(const ZPageAllocatorStats& stats, bool record_stats);\n@@ -572,1 +687,20 @@\n-  static void print();\n+  static size_t max_capacity();\n+  size_t used_at_collection_start() const;\n+  size_t used_at_mark_start() const;\n+  size_t used_generation_at_mark_start() const;\n+  size_t live_at_mark_end() const;\n+  size_t allocated_at_mark_end() const;\n+  size_t garbage_at_mark_end() const;\n+  size_t used_at_relocate_end() const;\n+  size_t used_at_collection_end() const;\n+  size_t stalls_at_mark_start() const;\n+  size_t stalls_at_mark_end() const;\n+  size_t stalls_at_relocate_start() const;\n+  size_t stalls_at_relocate_end() const;\n+\n+  size_t reclaimed_avg();\n+\n+  ZStatHeapStats stats();\n+\n+  void print(const ZGeneration* generation) const;\n+  void print_stalls() const;\n","filename":"src\/hotspot\/share\/gc\/z\/zStat.hpp","additions":273,"deletions":139,"binary":false,"changes":412,"status":"modified"},{"patch":"@@ -0,0 +1,320 @@\n+\/*\n+ * Copyright (c) 2021, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/shared\/gc_globals.hpp\"\n+#include \"gc\/z\/zAddress.inline.hpp\"\n+#include \"gc\/z\/zBarrier.inline.hpp\"\n+#include \"gc\/z\/zGeneration.inline.hpp\"\n+#include \"gc\/z\/zStoreBarrierBuffer.inline.hpp\"\n+#include \"gc\/z\/zUncoloredRoot.inline.hpp\"\n+#include \"memory\/resourceArea.hpp\"\n+#include \"runtime\/threadSMR.hpp\"\n+#include \"utilities\/ostream.hpp\"\n+#include \"utilities\/vmError.hpp\"\n+\n+ByteSize ZStoreBarrierEntry::p_offset() {\n+  return byte_offset_of(ZStoreBarrierEntry, _p);\n+}\n+\n+ByteSize ZStoreBarrierEntry::prev_offset() {\n+  return byte_offset_of(ZStoreBarrierEntry, _prev);\n+}\n+\n+ByteSize ZStoreBarrierBuffer::buffer_offset() {\n+  return byte_offset_of(ZStoreBarrierBuffer, _buffer);\n+}\n+\n+ByteSize ZStoreBarrierBuffer::current_offset() {\n+  return byte_offset_of(ZStoreBarrierBuffer, _current);\n+}\n+\n+ZStoreBarrierBuffer::ZStoreBarrierBuffer() :\n+    _buffer(),\n+    _last_processed_color(),\n+    _last_installed_color(),\n+    _base_pointer_lock(),\n+    _base_pointers(),\n+    _current(ZBufferStoreBarriers ? _buffer_size_bytes : 0) {\n+}\n+\n+void ZStoreBarrierBuffer::initialize() {\n+  _last_processed_color = ZPointerStoreGoodMask;\n+  _last_installed_color = ZPointerStoreGoodMask;\n+}\n+\n+void ZStoreBarrierBuffer::clear() {\n+  _current = _buffer_size_bytes;\n+}\n+\n+bool ZStoreBarrierBuffer::is_empty() const {\n+  return _current == _buffer_size_bytes;\n+}\n+\n+void ZStoreBarrierBuffer::install_base_pointers_inner() {\n+  assert(ZPointer::remap_bits(_last_installed_color) ==\n+         ZPointer::remap_bits(_last_processed_color),\n+         \"Can't deal with two pending base pointer installations\");\n+\n+  assert((ZPointer::remap_bits(_last_processed_color) & ZPointerRemappedYoungMask) == 0 ||\n+         (ZPointer::remap_bits(_last_processed_color) & ZPointerRemappedOldMask) == 0,\n+         \"Should not have double bit errors\");\n+\n+  for (int i = current(); i < (int)_buffer_length; ++i) {\n+    const ZStoreBarrierEntry& entry = _buffer[i];\n+    volatile zpointer* const p = entry._p;\n+    const zaddress_unsafe p_unsafe = to_zaddress_unsafe((uintptr_t)p);\n+\n+    \/\/ Color with the last processed color\n+    const zpointer ptr = ZAddress::color(p_unsafe, _last_processed_color);\n+\n+    \/\/ Look up the generation that thinks that this pointer is not\n+    \/\/ load good and check if the page is being relocated.\n+    ZGeneration* const remap_generation = ZBarrier::remap_generation(ptr);\n+    ZForwarding* const forwarding = remap_generation->forwarding(p_unsafe);\n+    if (forwarding != nullptr) {\n+      \/\/ Page is being relocated\n+      ZPage* const page = forwarding->page();\n+      _base_pointers[i] = page->find_base(p);\n+    } else {\n+      \/\/ Page is not being relocated\n+      _base_pointers[i] = zaddress_unsafe::null;\n+    }\n+  }\n+}\n+\n+void ZStoreBarrierBuffer::install_base_pointers() {\n+  if (!ZBufferStoreBarriers) {\n+    return;\n+  }\n+\n+  \/\/ Use a lock since both the GC and the Java thread race to install the base pointers\n+  ZLocker<ZLock> locker(&_base_pointer_lock);\n+\n+  const bool should_install_base_pointers = ZPointer::remap_bits(_last_installed_color) != ZPointerRemapped;\n+\n+  if (should_install_base_pointers) {\n+    install_base_pointers_inner();\n+  }\n+\n+  \/\/ This is used as a claim mechanism to make sure that we only install the base pointers once\n+  _last_installed_color = ZPointerStoreGoodMask;\n+}\n+\n+static volatile zpointer* make_load_good(volatile zpointer* p, zaddress_unsafe p_base, uintptr_t color) {\n+  assert(!is_null(p_base), \"need base pointer\");\n+\n+  \/\/ Calculate field offset before p_base is remapped\n+  const uintptr_t offset = (uintptr_t)p - untype(p_base);\n+\n+  \/\/ Remap local-copy of base pointer\n+  ZUncoloredRoot::process_no_keepalive(&p_base, color);\n+\n+  \/\/ Retype now that the address is known to point to the correct address\n+  const zaddress p_base_remapped = safe(p_base);\n+\n+  assert(offset < ZUtils::object_size(p_base_remapped),\n+         \"wrong base object; live bits are invalid\");\n+\n+  \/\/ Calculate remapped field address\n+  const zaddress p_remapped = to_zaddress(untype(p_base_remapped) + offset);\n+\n+  return (volatile zpointer*)p_remapped;\n+}\n+\n+void ZStoreBarrierBuffer::on_new_phase_relocate(int i) {\n+  const uintptr_t last_remap_bits = ZPointer::remap_bits(_last_processed_color);\n+  if (last_remap_bits == ZPointerRemapped) {\n+    \/\/ All pointers are already remapped\n+    return;\n+  }\n+\n+  const zaddress_unsafe p_base = _base_pointers[i];\n+  if (is_null(p_base)) {\n+    \/\/ Page is not part of the relocation set\n+    return;\n+  }\n+\n+  ZStoreBarrierEntry& entry = _buffer[i];\n+\n+  \/\/ Relocate the base object and calculate the remapped p\n+  entry._p = make_load_good(entry._p, p_base, _last_processed_color);\n+}\n+\n+void ZStoreBarrierBuffer::on_new_phase_remember(int i) {\n+  volatile zpointer* const p = _buffer[i]._p;\n+\n+  if (ZHeap::heap()->is_young(p)) {\n+    \/\/ Only need remset entries for old objects\n+    return;\n+  }\n+\n+  const uintptr_t last_mark_young_bits = _last_processed_color & (ZPointerMarkedYoung0 | ZPointerMarkedYoung1);\n+  const bool woke_up_in_young_mark = last_mark_young_bits != ZPointerMarkedYoung;\n+\n+  if (woke_up_in_young_mark) {\n+    \/\/ When young mark starts we \"flip\" the remembered sets. The remembered\n+    \/\/ sets used before the young mark start becomes read-only and used by\n+    \/\/ the GC to scan for old-to-young pointers to use as marking roots.\n+    \/\/\n+    \/\/ Entries in the store buffer that were added before the mark young start,\n+    \/\/ were supposed to be part of the remembered sets that the GC scans.\n+    \/\/ However, it is too late to add those entries at this point, so instead\n+    \/\/ we perform the GC remembered set scanning up-front here.\n+    ZGeneration::young()->scan_remembered_field(p);\n+  } else {\n+    \/\/ The remembered set wasn't flipped in this phase shift,\n+    \/\/ so just add the remembered set entry.\n+    ZGeneration::young()->remember(p);\n+  }\n+}\n+\n+bool ZStoreBarrierBuffer::is_old_mark() const {\n+  return ZGeneration::old()->is_phase_mark();\n+}\n+\n+bool ZStoreBarrierBuffer::stored_during_old_mark() const {\n+  const uintptr_t last_mark_old_bits = _last_processed_color & (ZPointerMarkedOld0 | ZPointerMarkedOld1);\n+  return last_mark_old_bits == ZPointerMarkedOld;\n+}\n+\n+void ZStoreBarrierBuffer::on_new_phase_mark(int i) {\n+  const ZStoreBarrierEntry& entry = _buffer[i];\n+  const zpointer prev = entry._prev;\n+\n+  if (is_null_any(prev)) {\n+    return;\n+  }\n+\n+  volatile zpointer* const p = entry._p;\n+\n+  \/\/ Young collections can start during old collections, but not the other\n+  \/\/ way around. Therefore, only old marking can see a collection phase\n+  \/\/ shift (resulting in a call to this function).\n+  \/\/\n+  \/\/ Stores before the marking phase started is not a part of the SATB snapshot,\n+  \/\/ and therefore shouldn't be used for marking.\n+  \/\/\n+  \/\/ Locations in the young generation are not part of the old marking.\n+  if (is_old_mark() && stored_during_old_mark() && ZHeap::heap()->is_old(p)) {\n+    const zaddress addr = ZBarrier::make_load_good(prev);\n+    ZUncoloredRoot::mark_object(addr);\n+  }\n+}\n+\n+void ZStoreBarrierBuffer::on_new_phase() {\n+  if (!ZBufferStoreBarriers) {\n+    return;\n+  }\n+\n+  \/\/ Install all base pointers for relocation\n+  install_base_pointers();\n+\n+  for (int i = current(); i < (int)_buffer_length; ++i) {\n+    on_new_phase_relocate(i);\n+    on_new_phase_remember(i);\n+    on_new_phase_mark(i);\n+  }\n+\n+  clear();\n+\n+  _last_processed_color = ZPointerStoreGoodMask;\n+  assert(_last_installed_color == _last_processed_color, \"invariant\");\n+}\n+\n+class ZStoreBarrierBuffer::OnError : public VMErrorCallback {\n+private:\n+  ZStoreBarrierBuffer* _buffer;\n+\n+public:\n+  OnError(ZStoreBarrierBuffer* buffer) :\n+      _buffer(buffer) {}\n+\n+  virtual void call(outputStream* st) {\n+    _buffer->on_error(st);\n+  }\n+};\n+\n+void ZStoreBarrierBuffer::on_error(outputStream* st) {\n+  st->print_cr(\"ZStoreBarrierBuffer: error when flushing\");\n+  st->print_cr(\" _last_processed_color: \" PTR_FORMAT, _last_processed_color);\n+  st->print_cr(\" _last_installed_color: \" PTR_FORMAT, _last_installed_color);\n+\n+  for (int i = current(); i < (int)_buffer_length; ++i) {\n+    st->print_cr(\" [%2d]: base: \" PTR_FORMAT \" p: \" PTR_FORMAT \" prev: \" PTR_FORMAT,\n+        i,\n+        untype(_base_pointers[i]),\n+        p2i(_buffer[i]._p),\n+        untype(_buffer[i]._prev));\n+  }\n+}\n+\n+void ZStoreBarrierBuffer::flush() {\n+  if (!ZBufferStoreBarriers) {\n+    return;\n+  }\n+\n+  OnError on_error(this);\n+  VMErrorCallbackMark mark(&on_error);\n+\n+  for (int i = current(); i < (int)_buffer_length; ++i) {\n+    const ZStoreBarrierEntry& entry = _buffer[i];\n+    const zaddress addr = ZBarrier::make_load_good(entry._prev);\n+    ZBarrier::mark_and_remember(entry._p, addr);\n+  }\n+\n+  clear();\n+}\n+\n+bool ZStoreBarrierBuffer::is_in(volatile zpointer* p) {\n+  if (!ZBufferStoreBarriers) {\n+    return false;\n+  }\n+\n+  for (JavaThreadIteratorWithHandle jtiwh; JavaThread * const jt = jtiwh.next(); ) {\n+    ZStoreBarrierBuffer* const buffer = ZThreadLocalData::store_barrier_buffer(jt);\n+\n+    const uintptr_t  last_remap_bits = ZPointer::remap_bits(buffer->_last_processed_color) & ZPointerRemappedMask;\n+    const bool needs_remap = last_remap_bits != ZPointerRemapped;\n+\n+    for (int i = buffer->current(); i < (int)_buffer_length; ++i) {\n+      const ZStoreBarrierEntry& entry = buffer->_buffer[i];\n+      volatile zpointer* entry_p = entry._p;\n+\n+      \/\/ Potentially remap p\n+      if (needs_remap) {\n+        const zaddress_unsafe entry_p_base = buffer->_base_pointers[i];\n+        if (!is_null(entry_p_base)) {\n+          entry_p = make_load_good(entry_p, entry_p_base, buffer->_last_processed_color);\n+        }\n+      }\n+\n+      \/\/ Check if p matches\n+      if (entry_p == p) {\n+        return true;\n+      }\n+    }\n+  }\n+\n+  return false;\n+}\n","filename":"src\/hotspot\/share\/gc\/z\/zStoreBarrierBuffer.cpp","additions":320,"deletions":0,"binary":false,"changes":320,"status":"added"},{"patch":"@@ -0,0 +1,98 @@\n+\/*\n+ * Copyright (c) 2021, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_Z_ZSTOREBARRIERBUFFER_HPP\n+#define SHARE_GC_Z_ZSTOREBARRIERBUFFER_HPP\n+\n+#include \"gc\/z\/zAddress.hpp\"\n+#include \"gc\/z\/zGenerationId.hpp\"\n+#include \"gc\/z\/zLock.hpp\"\n+#include \"memory\/allocation.hpp\"\n+#include \"utilities\/sizes.hpp\"\n+\n+struct ZStoreBarrierEntry {\n+  volatile zpointer* _p;\n+  zpointer           _prev;\n+\n+  static ByteSize p_offset();\n+  static ByteSize prev_offset();\n+};\n+\n+class ZStoreBarrierBuffer : public CHeapObj<mtGC> {\n+  friend class ZVerify;\n+\n+private:\n+  static const size_t _buffer_length     = 32;\n+  static const size_t _buffer_size_bytes = _buffer_length * sizeof(ZStoreBarrierEntry);\n+\n+  ZStoreBarrierEntry _buffer[_buffer_length];\n+\n+  \/\/ Color from previous phase this buffer was processed\n+  uintptr_t          _last_processed_color;\n+\n+  \/\/ Use as a claim mechanism for installing base pointers\n+  uintptr_t          _last_installed_color;\n+\n+  ZLock              _base_pointer_lock;\n+  zaddress_unsafe    _base_pointers[_buffer_length];\n+\n+  \/\/ sizeof(ZStoreBarrierEntry) scaled index growing downwards\n+  size_t             _current;\n+\n+  void on_new_phase_relocate(int i);\n+  void on_new_phase_remember(int i);\n+  void on_new_phase_mark(int i);\n+\n+  void clear();\n+\n+  bool is_old_mark() const;\n+  bool stored_during_old_mark() const;\n+  bool is_empty() const;\n+  intptr_t current() const;\n+\n+  void install_base_pointers_inner();\n+\n+  void on_error(outputStream* st);\n+  class OnError;\n+\n+public:\n+  ZStoreBarrierBuffer();\n+\n+  static ByteSize buffer_offset();\n+  static ByteSize current_offset();\n+\n+  static ZStoreBarrierBuffer* buffer_for_store(bool heal);\n+\n+  void initialize();\n+  void on_new_phase();\n+\n+  void install_base_pointers();\n+\n+  void flush();\n+  void add(volatile zpointer* p, zpointer prev);\n+\n+  \/\/ Check if p is contained in any store barrier buffer entry in the system\n+  static bool is_in(volatile zpointer* p);\n+};\n+\n+#endif \/\/ SHARE_GC_Z_ZSTOREBARRIERBUFFER_HPP\n","filename":"src\/hotspot\/share\/gc\/z\/zStoreBarrierBuffer.hpp","additions":98,"deletions":0,"binary":false,"changes":98,"status":"added"},{"patch":"@@ -0,0 +1,60 @@\n+\/*\n+ * Copyright (c) 2021, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_Z_ZSTOREBARRIERBUFFER_INLINE_HPP\n+#define SHARE_GC_Z_ZSTOREBARRIERBUFFER_INLINE_HPP\n+\n+#include \"gc\/z\/zStoreBarrierBuffer.hpp\"\n+\n+#include \"gc\/shared\/gc_globals.hpp\"\n+#include \"gc\/z\/zThreadLocalData.hpp\"\n+#include \"runtime\/thread.hpp\"\n+\n+inline intptr_t ZStoreBarrierBuffer::current() const {\n+  return _current \/ sizeof(ZStoreBarrierEntry);\n+}\n+\n+inline void ZStoreBarrierBuffer::add(volatile zpointer* p, zpointer prev) {\n+  assert(ZBufferStoreBarriers, \"Only buffer stores when it is enabled\");\n+  if (_current == 0) {\n+    flush();\n+  }\n+  _current -= sizeof(ZStoreBarrierEntry);\n+  _buffer[current()] = {p, prev};\n+}\n+\n+inline ZStoreBarrierBuffer* ZStoreBarrierBuffer::buffer_for_store(bool heal) {\n+  if (heal) {\n+    return nullptr;\n+  }\n+\n+  Thread* const thread = Thread::current();\n+  if (!thread->is_Java_thread()) {\n+    return nullptr;\n+  }\n+\n+  ZStoreBarrierBuffer* const buffer = ZThreadLocalData::store_barrier_buffer(JavaThread::cast(thread));\n+  return ZBufferStoreBarriers ? buffer : nullptr;\n+}\n+\n+#endif \/\/ SHARE_GC_Z_ZSTOREBARRIERBUFFER_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/z\/zStoreBarrierBuffer.inline.hpp","additions":60,"deletions":0,"binary":false,"changes":60,"status":"added"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -26,1 +26,0 @@\n-#include \"gc\/z\/zThread.hpp\"\n@@ -33,1 +32,0 @@\n-  ZThread::set_worker_id(worker_id);\n@@ -35,1 +33,0 @@\n-  ZThread::clear_worker_id();\n@@ -48,0 +45,5 @@\n+\n+ZRestartableTask::ZRestartableTask(const char* name) :\n+    ZTask(name) {}\n+\n+void ZRestartableTask::resize_workers(uint nworkers) {}\n","filename":"src\/hotspot\/share\/gc\/z\/zTask.cpp","additions":6,"deletions":4,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -53,0 +53,6 @@\n+class ZRestartableTask : public ZTask {\n+public:\n+  ZRestartableTask(const char* name);\n+  virtual void resize_workers(uint nworkers);\n+};\n+\n","filename":"src\/hotspot\/share\/gc\/z\/zTask.hpp","additions":7,"deletions":1,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -25,4 +25,2 @@\n-#include \"gc\/z\/zThread.inline.hpp\"\n-#include \"runtime\/javaThread.hpp\"\n-#include \"runtime\/nonJavaThread.hpp\"\n-#include \"utilities\/debug.hpp\"\n+#include \"gc\/z\/zThread.hpp\"\n+#include \"runtime\/mutexLocker.hpp\"\n@@ -30,6 +28,2 @@\n-THREAD_LOCAL bool      ZThread::_initialized;\n-THREAD_LOCAL uintptr_t ZThread::_id;\n-THREAD_LOCAL bool      ZThread::_is_vm;\n-THREAD_LOCAL bool      ZThread::_is_java;\n-THREAD_LOCAL bool      ZThread::_is_worker;\n-THREAD_LOCAL uint      ZThread::_worker_id;\n+void ZThread::run_service() {\n+  run_thread();\n@@ -37,10 +31,1 @@\n-void ZThread::initialize() {\n-  assert(!_initialized, \"Already initialized\");\n-  const Thread* const thread = Thread::current();\n-  _initialized = true;\n-  _id = (uintptr_t)thread;\n-  _is_vm = thread->is_VM_thread();\n-  _is_java = thread->is_Java_thread();\n-  _is_worker = false;\n-  _worker_id = (uint)-1;\n-}\n+  MonitorLocker ml(Terminator_lock, Monitor::_no_safepoint_check_flag);\n@@ -48,7 +33,3 @@\n-const char* ZThread::name() {\n-  const Thread* const thread = Thread::current();\n-  if (thread->is_Named_thread()) {\n-    const NamedThread* const named = (const NamedThread*)thread;\n-    return named->name();\n-  } else if (thread->is_Java_thread()) {\n-    return \"Java\";\n+  \/\/ Wait for signal to terminate\n+  while (!should_terminate()) {\n+    ml.wait();\n@@ -56,7 +37,0 @@\n-\n-  return \"Unknown\";\n-}\n-\n-void ZThread::set_worker() {\n-  ensure_initialized();\n-  _is_worker = true;\n@@ -65,11 +39,9 @@\n-bool ZThread::has_worker_id() {\n-  return _initialized &&\n-         _is_worker &&\n-         _worker_id != (uint)-1;\n-}\n-\n-void ZThread::set_worker_id(uint worker_id) {\n-  ensure_initialized();\n-  assert(!has_worker_id(), \"Worker id already initialized\");\n-  _worker_id = worker_id;\n-}\n+void ZThread::stop_service() {\n+  {\n+    \/\/ Signal thread to terminate\n+    \/\/ The should_terminate() flag should be true, and this notifies waiters\n+    \/\/ to wake up.\n+    MonitorLocker ml(Terminator_lock);\n+    assert(should_terminate(), \"This should be called when should_terminate has been set\");\n+    ml.notify_all();\n+  }\n@@ -77,3 +49,1 @@\n-void ZThread::clear_worker_id() {\n-  assert(has_worker_id(), \"Worker id not initialized\");\n-  _worker_id = (uint)-1;\n+  terminate();\n","filename":"src\/hotspot\/share\/gc\/z\/zThread.cpp","additions":19,"deletions":49,"binary":false,"changes":68,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,2 +27,1 @@\n-#include \"memory\/allStatic.hpp\"\n-#include \"utilities\/globalDefinitions.hpp\"\n+#include \"gc\/shared\/concurrentGCThread.hpp\"\n@@ -30,4 +29,1 @@\n-class ZThread : public AllStatic {\n-  friend class ZTask;\n-  friend class ZWorkersInitializeTask;\n-  friend class ZRuntimeWorkersInitializeTask;\n+\/\/ A ZThread is a ConcurrentGCThread with some ZGC-specific handling of GC shutdown\n@@ -35,0 +31,1 @@\n+class ZThread : public ConcurrentGCThread {\n@@ -36,15 +33,2 @@\n-  static THREAD_LOCAL bool      _initialized;\n-  static THREAD_LOCAL uintptr_t _id;\n-  static THREAD_LOCAL bool      _is_vm;\n-  static THREAD_LOCAL bool      _is_java;\n-  static THREAD_LOCAL bool      _is_worker;\n-  static THREAD_LOCAL uint      _worker_id;\n-\n-  static void initialize();\n-  static void ensure_initialized();\n-\n-  static void set_worker();\n-\n-  static bool has_worker_id();\n-  static void set_worker_id(uint worker_id);\n-  static void clear_worker_id();\n+  virtual void run_service();\n+  virtual void stop_service();\n@@ -53,6 +37,3 @@\n-  static const char* name();\n-  static uintptr_t id();\n-  static bool is_vm();\n-  static bool is_java();\n-  static bool is_worker();\n-  static uint worker_id();\n+\n+  virtual void run_thread() = 0;\n+  virtual void terminate() = 0;\n","filename":"src\/hotspot\/share\/gc\/z\/zThread.hpp","additions":9,"deletions":28,"binary":false,"changes":37,"status":"modified"},{"patch":"@@ -1,62 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_Z_ZTHREAD_INLINE_HPP\n-#define SHARE_GC_Z_ZTHREAD_INLINE_HPP\n-\n-#include \"gc\/z\/zThread.hpp\"\n-\n-#include \"utilities\/debug.hpp\"\n-\n-inline void ZThread::ensure_initialized() {\n-  if (!_initialized) {\n-    initialize();\n-  }\n-}\n-\n-inline uintptr_t ZThread::id() {\n-  ensure_initialized();\n-  return _id;\n-}\n-\n-inline bool ZThread::is_vm() {\n-  ensure_initialized();\n-  return _is_vm;\n-}\n-\n-inline bool ZThread::is_java() {\n-  ensure_initialized();\n-  return _is_java;\n-}\n-\n-inline bool ZThread::is_worker() {\n-  ensure_initialized();\n-  return _is_worker;\n-}\n-\n-inline uint ZThread::worker_id() {\n-  assert(has_worker_id(), \"Worker id not initialized\");\n-  return _worker_id;\n-}\n-\n-#endif \/\/ SHARE_GC_Z_ZTHREAD_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/z\/zThread.inline.hpp","additions":0,"deletions":62,"binary":false,"changes":62,"status":"deleted"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -34,1 +34,1 @@\n-ZPerWorker<ThreadLocalAllocStats>* ZThreadLocalAllocBuffer::_stats = NULL;\n+ZPerWorker<ThreadLocalAllocStats>* ZThreadLocalAllocBuffer::_stats = nullptr;\n@@ -38,1 +38,1 @@\n-    assert(_stats == NULL, \"Already initialized\");\n+    assert(_stats == nullptr, \"Already initialized\");\n@@ -66,4 +66,0 @@\n-static void fixup_address(HeapWord** p) {\n-  *p = (HeapWord*)ZAddress::good_or_null((uintptr_t)*p);\n-}\n-\n@@ -73,1 +69,0 @@\n-    thread->tlab().addresses_do(fixup_address);\n@@ -81,6 +76,0 @@\n-void ZThreadLocalAllocBuffer::remap(JavaThread* thread) {\n-  if (UseTLAB) {\n-    thread->tlab().addresses_do(fixup_address);\n-  }\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/z\/zThreadLocalAllocBuffer.cpp","additions":3,"deletions":14,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -44,1 +44,0 @@\n-  static void remap(JavaThread* thread);\n","filename":"src\/hotspot\/share\/gc\/z\/zThreadLocalAllocBuffer.hpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,0 +27,2 @@\n+#include \"gc\/z\/zAddress.hpp\"\n+#include \"gc\/z\/zGenerationId.hpp\"\n@@ -28,1 +30,1 @@\n-#include \"gc\/z\/zGlobals.hpp\"\n+#include \"gc\/z\/zStoreBarrierBuffer.hpp\"\n@@ -35,3 +37,10 @@\n-  uintptr_t              _address_bad_mask;\n-  ZMarkThreadLocalStacks _stacks;\n-  oop*                   _invisible_root;\n+  uintptr_t              _load_good_mask;\n+  uintptr_t              _load_bad_mask;\n+  uintptr_t              _mark_bad_mask;\n+  uintptr_t              _store_good_mask;\n+  uintptr_t              _store_bad_mask;\n+  uintptr_t              _uncolor_mask;\n+  uintptr_t              _nmethod_disarmed;\n+  ZStoreBarrierBuffer*   _store_barrier_buffer;\n+  ZMarkThreadLocalStacks _mark_stacks[2];\n+  zaddress_unsafe*       _invisible_root;\n@@ -40,3 +49,14 @@\n-      _address_bad_mask(0),\n-      _stacks(),\n-      _invisible_root(NULL) {}\n+      _load_good_mask(0),\n+      _load_bad_mask(0),\n+      _mark_bad_mask(0),\n+      _store_good_mask(0),\n+      _store_bad_mask(0),\n+      _uncolor_mask(0),\n+      _nmethod_disarmed(0),\n+      _store_barrier_buffer(new ZStoreBarrierBuffer()),\n+      _mark_stacks(),\n+      _invisible_root(nullptr) {}\n+\n+  ~ZThreadLocalData() {\n+    delete _store_barrier_buffer;\n+  }\n@@ -57,2 +77,26 @@\n-  static void set_address_bad_mask(Thread* thread, uintptr_t mask) {\n-    data(thread)->_address_bad_mask = mask;\n+  static void set_load_bad_mask(Thread* thread, uintptr_t mask) {\n+    data(thread)->_load_bad_mask = mask;\n+  }\n+\n+  static void set_mark_bad_mask(Thread* thread, uintptr_t mask) {\n+    data(thread)->_mark_bad_mask = mask;\n+  }\n+\n+  static void set_store_bad_mask(Thread* thread, uintptr_t mask) {\n+    data(thread)->_store_bad_mask = mask;\n+  }\n+\n+  static void set_load_good_mask(Thread* thread, uintptr_t mask) {\n+    data(thread)->_load_good_mask = mask;\n+  }\n+\n+  static void set_store_good_mask(Thread* thread, uintptr_t mask) {\n+    data(thread)->_store_good_mask = mask;\n+  }\n+\n+  static void set_nmethod_disarmed(Thread* thread, uintptr_t value) {\n+    data(thread)->_nmethod_disarmed = value;\n+  }\n+\n+  static ZMarkThreadLocalStacks* mark_stacks(Thread* thread, ZGenerationId id) {\n+    return &data(thread)->_mark_stacks[(int)id];\n@@ -61,2 +105,2 @@\n-  static ZMarkThreadLocalStacks* stacks(Thread* thread) {\n-    return &data(thread)->_stacks;\n+  static ZStoreBarrierBuffer* store_barrier_buffer(Thread* thread) {\n+    return data(thread)->_store_barrier_buffer;\n@@ -65,2 +109,2 @@\n-  static void set_invisible_root(Thread* thread, oop* root) {\n-    assert(data(thread)->_invisible_root == NULL, \"Already set\");\n+  static void set_invisible_root(Thread* thread, zaddress_unsafe* root) {\n+    assert(data(thread)->_invisible_root == nullptr, \"Already set\");\n@@ -71,2 +115,2 @@\n-    assert(data(thread)->_invisible_root != NULL, \"Should be set\");\n-    data(thread)->_invisible_root = NULL;\n+    assert(data(thread)->_invisible_root != nullptr, \"Should be set\");\n+    data(thread)->_invisible_root = nullptr;\n@@ -75,5 +119,2 @@\n-  template <typename T>\n-  static void do_invisible_root(Thread* thread, T f) {\n-    if (data(thread)->_invisible_root != NULL) {\n-      f(data(thread)->_invisible_root);\n-    }\n+  static zaddress_unsafe* invisible_root(Thread* thread) {\n+    return data(thread)->_invisible_root;\n@@ -82,2 +123,14 @@\n-  static ByteSize address_bad_mask_offset() {\n-    return Thread::gc_data_offset() + byte_offset_of(ZThreadLocalData, _address_bad_mask);\n+  static ByteSize load_bad_mask_offset() {\n+    return Thread::gc_data_offset() + byte_offset_of(ZThreadLocalData, _load_bad_mask);\n+  }\n+\n+  static ByteSize mark_bad_mask_offset() {\n+    return Thread::gc_data_offset() + byte_offset_of(ZThreadLocalData, _mark_bad_mask);\n+  }\n+\n+  static ByteSize store_bad_mask_offset() {\n+    return Thread::gc_data_offset() + byte_offset_of(ZThreadLocalData, _store_bad_mask);\n+  }\n+\n+  static ByteSize store_good_mask_offset() {\n+    return Thread::gc_data_offset() + byte_offset_of(ZThreadLocalData, _store_good_mask);\n@@ -87,1 +140,5 @@\n-    return address_bad_mask_offset() + in_ByteSize(ZAddressBadMaskHighOrderBitsOffset);\n+    return Thread::gc_data_offset() + byte_offset_of(ZThreadLocalData, _nmethod_disarmed);\n+  }\n+\n+  static ByteSize store_barrier_buffer_offset() {\n+    return Thread::gc_data_offset() + byte_offset_of(ZThreadLocalData, _store_barrier_buffer);\n","filename":"src\/hotspot\/share\/gc\/z\/zThreadLocalData.hpp","additions":81,"deletions":24,"binary":false,"changes":105,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,0 +27,1 @@\n+#include \"gc\/z\/zPageType.hpp\"\n@@ -43,1 +44,1 @@\n-    writer.write_key(ZPageTypeSmall);\n+    writer.write_key((u8)ZPageType::small);\n@@ -45,1 +46,1 @@\n-    writer.write_key(ZPageTypeMedium);\n+    writer.write_key((u8)ZPageType::medium);\n@@ -47,1 +48,1 @@\n-    writer.write_key(ZPageTypeLarge);\n+    writer.write_key((u8)ZPageType::large);\n@@ -56,1 +57,1 @@\n-    for (ZStatCounter* counter = ZStatCounter::first(); counter != NULL; counter = counter->next()) {\n+    for (ZStatCounter* counter = ZStatCounter::first(); counter != nullptr; counter = counter->next()) {\n@@ -67,1 +68,1 @@\n-    for (ZStatSampler* sampler = ZStatSampler::first(); sampler != NULL; sampler = sampler->next()) {\n+    for (ZStatSampler* sampler = ZStatSampler::first(); sampler != nullptr; sampler = sampler->next()) {\n@@ -88,1 +89,13 @@\n-ZTracer* ZTracer::_tracer = NULL;\n+ZMinorTracer::ZMinorTracer() :\n+    GCTracer(ZMinor) {\n+}\n+\n+ZMajorTracer::ZMajorTracer() :\n+    GCTracer(ZMajor) {}\n+\n+void ZGenerationTracer::report_start(const Ticks& timestamp) {\n+  _start = timestamp;\n+}\n+\n+void ZYoungTracer::report_end(const Ticks& timestamp) {\n+  NoSafepointVerifier nsv;\n@@ -90,2 +103,17 @@\n-ZTracer::ZTracer() :\n-    GCTracer(Z) {}\n+  EventZYoungGarbageCollection e(UNTIMED);\n+  e.set_gcId(GCId::current());\n+  e.set_tenuringThreshold(ZGeneration::young()->tenuring_threshold());\n+  e.set_starttime(_start);\n+  e.set_endtime(timestamp);\n+  e.commit();\n+}\n+\n+void ZOldTracer::report_end(const Ticks& timestamp) {\n+  NoSafepointVerifier nsv;\n+\n+  EventZOldGarbageCollection e(UNTIMED);\n+  e.set_gcId(GCId::current());\n+  e.set_starttime(_start);\n+  e.set_endtime(timestamp);\n+  e.commit();\n+}\n@@ -94,2 +122,0 @@\n-  assert(_tracer == NULL, \"Already initialized\");\n-  _tracer = new ZTracer();\n","filename":"src\/hotspot\/share\/gc\/z\/zTracer.cpp","additions":37,"deletions":11,"binary":false,"changes":48,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,0 +28,1 @@\n+#include \"gc\/z\/zGenerationId.hpp\"\n@@ -33,1 +34,1 @@\n-class ZTracer : public GCTracer, public CHeapObj<mtGC> {\n+class ZTracer : AllStatic {\n@@ -35,1 +36,4 @@\n-  static ZTracer* _tracer;\n+  static void send_stat_counter(const ZStatCounter& counter, uint64_t increment, uint64_t value);\n+  static void send_stat_sampler(const ZStatSampler& sampler, uint64_t value);\n+  static void send_thread_phase(const char* name, const Ticks& start, const Ticks& end);\n+  static void send_thread_debug(const char* name, const Ticks& start, const Ticks& end);\n@@ -37,1 +41,2 @@\n-  ZTracer();\n+public:\n+  static void initialize();\n@@ -39,4 +44,5 @@\n-  void send_stat_counter(const ZStatCounter& counter, uint64_t increment, uint64_t value);\n-  void send_stat_sampler(const ZStatSampler& sampler, uint64_t value);\n-  void send_thread_phase(const char* name, const Ticks& start, const Ticks& end);\n-  void send_thread_debug(const char* name, const Ticks& start, const Ticks& end);\n+  static void report_stat_counter(const ZStatCounter& counter, uint64_t increment, uint64_t value);\n+  static void report_stat_sampler(const ZStatSampler& sampler, uint64_t value);\n+  static void report_thread_phase(const char* name, const Ticks& start, const Ticks& end);\n+  static void report_thread_debug(const char* name, const Ticks& start, const Ticks& end);\n+};\n@@ -44,0 +50,1 @@\n+class ZMinorTracer : public GCTracer {\n@@ -45,2 +52,11 @@\n-  static ZTracer* tracer();\n-  static void initialize();\n+  ZMinorTracer();\n+};\n+\n+class ZMajorTracer : public GCTracer {\n+public:\n+  ZMajorTracer();\n+};\n+\n+class ZGenerationTracer {\n+protected:\n+  Ticks _start;\n@@ -48,4 +64,16 @@\n-  void report_stat_counter(const ZStatCounter& counter, uint64_t increment, uint64_t value);\n-  void report_stat_sampler(const ZStatSampler& sampler, uint64_t value);\n-  void report_thread_phase(const char* name, const Ticks& start, const Ticks& end);\n-  void report_thread_debug(const char* name, const Ticks& start, const Ticks& end);\n+public:\n+  ZGenerationTracer() :\n+      _start() {}\n+\n+  void report_start(const Ticks& timestamp);\n+  virtual void report_end(const Ticks& timestamp) = 0;\n+};\n+\n+class ZYoungTracer : public ZGenerationTracer {\n+public:\n+  void report_end(const Ticks& timestamp) override;\n+};\n+\n+class ZOldTracer : public ZGenerationTracer {\n+public:\n+  void report_end(const Ticks& timestamp) override;\n","filename":"src\/hotspot\/share\/gc\/z\/zTracer.hpp","additions":42,"deletions":14,"binary":false,"changes":56,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -31,4 +31,0 @@\n-inline ZTracer* ZTracer::tracer() {\n-  return _tracer;\n-}\n-\n@@ -64,1 +60,1 @@\n-  ZTracer::tracer()->report_thread_debug(_name, _start, Ticks::now());\n+  ZTracer::report_thread_debug(_name, _start, Ticks::now());\n","filename":"src\/hotspot\/share\/gc\/z\/zTracer.inline.hpp","additions":2,"deletions":6,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -0,0 +1,33 @@\n+\/*\n+ * Copyright (c) 2021, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/z\/zUncoloredRoot.inline.hpp\"\n+\n+void ZUncoloredRootClosure::do_oop(oop* p) {\n+  do_root(ZUncoloredRoot::cast(p));\n+}\n+\n+void ZUncoloredRootClosure::do_oop(narrowOop* p) {\n+  ShouldNotReachHere();\n+}\n","filename":"src\/hotspot\/share\/gc\/z\/zUncoloredRoot.cpp","additions":33,"deletions":0,"binary":false,"changes":33,"status":"added"},{"patch":"@@ -0,0 +1,145 @@\n+\/*\n+ * Copyright (c) 2020, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_Z_ZUNCOLOREDROOT_HPP\n+#define SHARE_GC_Z_ZUNCOLOREDROOT_HPP\n+\n+#include \"gc\/z\/zAddress.hpp\"\n+#include \"memory\/allStatic.hpp\"\n+#include \"memory\/iterator.hpp\"\n+#include \"oops\/oopsHierarchy.hpp\"\n+\n+\/\/ ZGC has two types of oops:\n+\/\/\n+\/\/ Colored oops (zpointer)\n+\/\/   Metadata explicitly encoded in the pointer bits.\n+\/\/   Requires normal GC barriers to use.\n+\/\/   - OopStorage oops.\n+\/\/\n+\/\/ Uncolored oops (zaddress, zaddress_unsafe)\n+\/\/   Metadata is either implicit or stored elsewhere\n+\/\/   Requires specialized GC barriers\n+\/\/   - nmethod oops - nmethod entry barriers\n+\/\/   - Thread oops - stack watermark barriers\n+\/\/\n+\/\/ Even though the uncolored roots lack the color\/metadata, ZGC still needs\n+\/\/ that information when processing the roots. Therefore, we store the color\n+\/\/ in the \"container\" object where the oop is located, and use specialized\n+\/\/ GC barriers, which accepts the external color as an extra argument. These\n+\/\/ roots are handled in this file.\n+\/\/\n+\/\/ The zaddress_unsafe type is used to hold uncolored oops that the GC needs\n+\/\/ to process before it is safe to use. E.g. the original object might have\n+\/\/ been relocated and the address needs to be updated. The zaddress type\n+\/\/ denotes that this pointer refers the the correct address of the object.\n+\n+class ZUncoloredRoot : public AllStatic {\n+private:\n+  template <typename ObjectFunctionT>\n+  static void barrier(ObjectFunctionT function, zaddress_unsafe* p, uintptr_t color);\n+\n+  static zaddress make_load_good(zaddress_unsafe addr, uintptr_t color);\n+\n+public:\n+  \/\/ Operations to be used on oops that are known to be load good\n+  static void mark_object(zaddress addr);\n+  static void mark_invisible_object(zaddress addr);\n+  static void keep_alive_object(zaddress addr);\n+  static void mark_young_object(zaddress addr);\n+\n+  \/\/ Operations on roots, with an externally provided color\n+  static void mark(zaddress_unsafe* p, uintptr_t color);\n+  static void mark_young(zaddress_unsafe* p, uintptr_t color);\n+  static void process(zaddress_unsafe* p, uintptr_t color);\n+  static void process_invisible(zaddress_unsafe* p, uintptr_t color);\n+  static void process_weak(zaddress_unsafe* p, uintptr_t color);\n+  static void process_no_keepalive(zaddress_unsafe* p, uintptr_t color);\n+\n+  \/\/ Cast needed when ZGC interfaces with the rest of the JVM,\n+  \/\/ which is agnostic to ZGC's oop type system.\n+  static zaddress_unsafe* cast(oop* p);\n+\n+  typedef void (*RootFunction)(zaddress_unsafe*, uintptr_t);\n+  typedef void (*ObjectFunction)(zaddress);\n+};\n+\n+class ZUncoloredRootClosure : public OopClosure {\n+private:\n+  void do_oop(oop* p) final;\n+  void do_oop(narrowOop* p) final;\n+\n+public:\n+  virtual void do_root(zaddress_unsafe* p) = 0;\n+};\n+\n+class ZUncoloredRootMarkOopClosure : public ZUncoloredRootClosure {\n+private:\n+  const uintptr_t _color;\n+\n+public:\n+  ZUncoloredRootMarkOopClosure(uintptr_t color);\n+\n+  virtual void do_root(zaddress_unsafe* p);\n+};\n+\n+class ZUncoloredRootMarkYoungOopClosure : public ZUncoloredRootClosure {\n+private:\n+  const uintptr_t _color;\n+\n+public:\n+  ZUncoloredRootMarkYoungOopClosure(uintptr_t color);\n+\n+  virtual void do_root(zaddress_unsafe* p);\n+};\n+\n+class ZUncoloredRootProcessOopClosure : public ZUncoloredRootClosure {\n+private:\n+  const uintptr_t _color;\n+\n+public:\n+  ZUncoloredRootProcessOopClosure(uintptr_t color);\n+\n+  virtual void do_root(zaddress_unsafe* p);\n+};\n+\n+class ZUncoloredRootProcessWeakOopClosure : public ZUncoloredRootClosure {\n+private:\n+  const uintptr_t _color;\n+\n+public:\n+  ZUncoloredRootProcessWeakOopClosure(uintptr_t color);\n+\n+  virtual void do_root(zaddress_unsafe* p);\n+};\n+\n+class ZUncoloredRootProcessNoKeepaliveOopClosure : public ZUncoloredRootClosure {\n+private:\n+  const uintptr_t _color;\n+\n+public:\n+  ZUncoloredRootProcessNoKeepaliveOopClosure(uintptr_t color);\n+\n+  virtual void do_root(zaddress_unsafe* p);\n+};\n+\n+#endif \/\/ SHARE_GC_Z_ZUNCOLOREDROOT_HPP\n","filename":"src\/hotspot\/share\/gc\/z\/zUncoloredRoot.hpp","additions":145,"deletions":0,"binary":false,"changes":145,"status":"added"},{"patch":"@@ -0,0 +1,153 @@\n+\/*\n+ * Copyright (c) 2020, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_Z_ZUNCOLOREDROOT_INLINE_HPP\n+#define SHARE_GC_Z_ZUNCOLOREDROOT_INLINE_HPP\n+\n+#include \"gc\/z\/zUncoloredRoot.hpp\"\n+\n+#include \"gc\/z\/zAddress.inline.hpp\"\n+#include \"gc\/z\/zBarrier.inline.hpp\"\n+#include \"gc\/z\/zHeap.inline.hpp\"\n+#include \"oops\/oop.hpp\"\n+\n+template <typename ObjectFunctionT>\n+inline void ZUncoloredRoot::barrier(ObjectFunctionT function, zaddress_unsafe* p, uintptr_t color) {\n+  z_assert_is_barrier_safe();\n+\n+  const zaddress_unsafe addr = Atomic::load(p);\n+  assert_is_valid(addr);\n+\n+  \/\/ Nothing to do for nulls\n+  if (is_null(addr)) {\n+    return;\n+  }\n+\n+  \/\/ Make load good\n+  const zaddress load_good_addr = make_load_good(addr, color);\n+\n+  \/\/ Apply function\n+  function(load_good_addr);\n+\n+  \/\/ Non-atomic healing helps speed up root scanning. This is safe to do\n+  \/\/ since we are always healing roots in a safepoint, or under a lock,\n+  \/\/ which ensures we are never racing with mutators modifying roots while\n+  \/\/ we are healing them. It's also safe in case multiple GC threads try\n+  \/\/ to heal the same root if it is aligned, since they would always heal\n+  \/\/ the root in the same way and it does not matter in which order it\n+  \/\/ happens. For misaligned oops, there needs to be mutual exclusion.\n+  *(zaddress*)p = load_good_addr;\n+}\n+\n+inline zaddress ZUncoloredRoot::make_load_good(zaddress_unsafe addr, uintptr_t color) {\n+  const zpointer color_ptr = ZAddress::color(zaddress::null, color);\n+  if (!ZPointer::is_load_good(color_ptr)) {\n+    return ZBarrier::relocate_or_remap(addr, ZBarrier::remap_generation(color_ptr));\n+  } else {\n+    return safe(addr);\n+  }\n+}\n+\n+inline void ZUncoloredRoot::mark_object(zaddress addr) {\n+  ZBarrier::mark<ZMark::DontResurrect, ZMark::AnyThread, ZMark::Follow, ZMark::Strong>(addr);\n+}\n+\n+inline void ZUncoloredRoot::mark_young_object(zaddress addr) {\n+  ZBarrier::mark_if_young<ZMark::DontResurrect, ZMark::GCThread, ZMark::Follow>(addr);\n+}\n+\n+inline void ZUncoloredRoot::mark_invisible_object(zaddress addr) {\n+  ZBarrier::mark<ZMark::DontResurrect, ZMark::AnyThread, ZMark::DontFollow, ZMark::Strong>(addr);\n+}\n+\n+inline void ZUncoloredRoot::keep_alive_object(zaddress addr) {\n+  ZBarrier::mark<ZMark::Resurrect, ZMark::AnyThread, ZMark::Follow, ZMark::Strong>(addr);\n+}\n+\n+inline void ZUncoloredRoot::mark(zaddress_unsafe* p, uintptr_t color) {\n+  barrier(mark_object, p, color);\n+}\n+\n+inline void ZUncoloredRoot::mark_young(zaddress_unsafe* p, uintptr_t color) {\n+  barrier(mark_young_object, p, color);\n+}\n+\n+inline void ZUncoloredRoot::process(zaddress_unsafe* p, uintptr_t color) {\n+  barrier(mark_object, p, color);\n+}\n+\n+inline void ZUncoloredRoot::process_invisible(zaddress_unsafe* p, uintptr_t color) {\n+  barrier(mark_invisible_object, p, color);\n+}\n+\n+inline void ZUncoloredRoot::process_weak(zaddress_unsafe* p, uintptr_t color) {\n+  barrier(keep_alive_object, p, color);\n+}\n+\n+inline void ZUncoloredRoot::process_no_keepalive(zaddress_unsafe* p, uintptr_t color) {\n+  auto do_nothing = [](zaddress) -> void {};\n+  barrier(do_nothing, p, color);\n+}\n+\n+inline zaddress_unsafe* ZUncoloredRoot::cast(oop* p) {\n+  zaddress_unsafe* const root = (zaddress_unsafe*)p;\n+  DEBUG_ONLY(assert_is_valid(*root);)\n+  return root;\n+}\n+\n+inline ZUncoloredRootMarkOopClosure::ZUncoloredRootMarkOopClosure(uintptr_t color) :\n+    _color(color) {}\n+\n+inline void ZUncoloredRootMarkOopClosure::do_root(zaddress_unsafe* p) {\n+  ZUncoloredRoot::mark(p, _color);\n+}\n+\n+inline ZUncoloredRootMarkYoungOopClosure::ZUncoloredRootMarkYoungOopClosure(uintptr_t color) :\n+    _color(color) {}\n+\n+inline void ZUncoloredRootMarkYoungOopClosure::do_root(zaddress_unsafe* p) {\n+  ZUncoloredRoot::mark_young(p, _color);\n+}\n+\n+inline ZUncoloredRootProcessOopClosure::ZUncoloredRootProcessOopClosure(uintptr_t color) :\n+    _color(color) {}\n+\n+inline void ZUncoloredRootProcessOopClosure::do_root(zaddress_unsafe* p) {\n+  ZUncoloredRoot::process(p, _color);\n+}\n+\n+inline ZUncoloredRootProcessWeakOopClosure::ZUncoloredRootProcessWeakOopClosure(uintptr_t color) :\n+  _color(color) {}\n+\n+inline void ZUncoloredRootProcessWeakOopClosure::do_root(zaddress_unsafe* p) {\n+  ZUncoloredRoot::process_weak(p, _color);\n+}\n+\n+inline ZUncoloredRootProcessNoKeepaliveOopClosure::ZUncoloredRootProcessNoKeepaliveOopClosure(uintptr_t color) :\n+    _color(color) {}\n+\n+inline void ZUncoloredRootProcessNoKeepaliveOopClosure::do_root(zaddress_unsafe* p) {\n+  ZUncoloredRoot::process_no_keepalive(p, _color);\n+}\n+\n+#endif \/\/ SHARE_GC_Z_ZUNCOLOREDROOT_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/z\/zUncoloredRoot.inline.hpp","additions":153,"deletions":0,"binary":false,"changes":153,"status":"added"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2019, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2019, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -62,1 +62,1 @@\n-void ZUncommitter::run_service() {\n+void ZUncommitter::run_thread() {\n@@ -92,1 +92,1 @@\n-void ZUncommitter::stop_service() {\n+void ZUncommitter::terminate() {\n","filename":"src\/hotspot\/share\/gc\/z\/zUncommitter.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2019, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2019, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,1 +27,0 @@\n-#include \"gc\/shared\/concurrentGCThread.hpp\"\n@@ -29,0 +28,1 @@\n+#include \"gc\/z\/zThread.hpp\"\n@@ -30,1 +30,1 @@\n-class ZPageAllocation;\n+class ZPageAllocator;\n@@ -32,1 +32,1 @@\n-class ZUncommitter : public ConcurrentGCThread {\n+class ZUncommitter : public ZThread {\n@@ -42,2 +42,2 @@\n-  virtual void run_service();\n-  virtual void stop_service();\n+  virtual void run_thread();\n+  virtual void terminate();\n","filename":"src\/hotspot\/share\/gc\/z\/zUncommitter.hpp","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -33,0 +33,2 @@\n+#include \"gc\/z\/zBarrierSetNMethod.hpp\"\n+#include \"gc\/z\/zGeneration.inline.hpp\"\n@@ -36,0 +38,1 @@\n+#include \"gc\/z\/zUncoloredRoot.inline.hpp\"\n@@ -40,9 +43,2 @@\n-static const ZStatSubPhase ZSubPhaseConcurrentClassesUnlink(\"Concurrent Classes Unlink\");\n-static const ZStatSubPhase ZSubPhaseConcurrentClassesPurge(\"Concurrent Classes Purge\");\n-\n-class ZPhantomIsAliveObjectClosure : public BoolObjectClosure {\n-public:\n-  virtual bool do_object_b(oop o) {\n-    return ZBarrier::is_alive_barrier_on_phantom_oop(o);\n-  }\n-};\n+static const ZStatSubPhase ZSubPhaseConcurrentClassesUnlink(\"Concurrent Classes Unlink\", ZGenerationId::old);\n+static const ZStatSubPhase ZSubPhaseConcurrentClassesPurge(\"Concurrent Classes Purge\", ZGenerationId::old);\n@@ -52,2 +48,2 @@\n-  ZPhantomIsAliveObjectClosure _is_alive;\n-  bool                         _is_unloading;\n+  const uintptr_t _color;\n+  bool            _is_unloading;\n@@ -56,2 +52,2 @@\n-  ZIsUnloadingOopClosure() :\n-      _is_alive(),\n+  ZIsUnloadingOopClosure(nmethod* nm) :\n+      _color(ZNMethod::color(nm)),\n@@ -61,2 +57,5 @@\n-    const oop o = RawAccess<>::oop_load(p);\n-    if (o != NULL && !_is_alive.do_object_b(o)) {\n+    \/\/ Create local, aligned root\n+    zaddress_unsafe addr = Atomic::load(ZUncoloredRoot::cast(p));\n+    ZUncoloredRoot::process_no_keepalive(&addr, _color);\n+\n+    if (!is_null(addr) && ZHeap::heap()->is_old(safe(addr)) && !ZHeap::heap()->is_object_live(safe(addr))) {\n@@ -82,1 +81,5 @@\n-    ZIsUnloadingOopClosure cl;\n+    if (!ZNMethod::is_armed(nm)) {\n+      \/\/ Disarmed nmethods are alive\n+      return false;\n+    }\n+    ZIsUnloadingOopClosure cl(nm);\n@@ -142,2 +145,2 @@\n-  ZStatTimer timer(ZSubPhaseConcurrentClassesUnlink);\n-  SuspendibleThreadSetJoiner sts;\n+  ZStatTimerOld timer(ZSubPhaseConcurrentClassesUnlink);\n+  SuspendibleThreadSetJoiner sts_joiner;\n@@ -148,1 +151,1 @@\n-    unloading_occurred = SystemDictionary::do_unloading(ZStatPhase::timer());\n+    unloading_occurred = SystemDictionary::do_unloading(ZGeneration::old()->gc_timer());\n@@ -161,1 +164,1 @@\n-  ZStatTimer timer(ZSubPhaseConcurrentClassesPurge);\n+  ZStatTimerOld timer(ZSubPhaseConcurrentClassesPurge);\n@@ -164,1 +167,1 @@\n-    SuspendibleThreadSetJoiner sts;\n+    SuspendibleThreadSetJoiner sts_joiner;\n","filename":"src\/hotspot\/share\/gc\/z\/zUnload.cpp","additions":25,"deletions":22,"binary":false,"changes":47,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -48,1 +48,1 @@\n-      return NULL;\n+      return nullptr;\n@@ -52,1 +52,1 @@\n-    if (page != NULL) {\n+    if (page != nullptr) {\n@@ -73,10 +73,4 @@\n-  \/\/ Asynchronous unmap and destroy is not supported with ZVerifyViews\n-  if (ZVerifyViews) {\n-    \/\/ Immediately unmap and destroy\n-    do_unmap_and_destroy_page(page);\n-  } else {\n-    \/\/ Enqueue for asynchronous unmap and destroy\n-    ZLocker<ZConditionLock> locker(&_lock);\n-    _queue.insert_last(page);\n-    _lock.notify_all();\n-  }\n+  \/\/ Enqueue for asynchronous unmap and destroy\n+  ZLocker<ZConditionLock> locker(&_lock);\n+  _queue.insert_last(page);\n+  _lock.notify_all();\n@@ -85,1 +79,1 @@\n-void ZUnmapper::run_service() {\n+void ZUnmapper::run_thread() {\n@@ -88,1 +82,1 @@\n-    if (page == NULL) {\n+    if (page == nullptr) {\n@@ -97,1 +91,1 @@\n-void ZUnmapper::stop_service() {\n+void ZUnmapper::terminate() {\n","filename":"src\/hotspot\/share\/gc\/z\/zUnmapper.cpp","additions":10,"deletions":16,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,1 +27,0 @@\n-#include \"gc\/shared\/concurrentGCThread.hpp\"\n@@ -30,0 +29,1 @@\n+#include \"gc\/z\/zThread.hpp\"\n@@ -34,1 +34,1 @@\n-class ZUnmapper : public ConcurrentGCThread {\n+class ZUnmapper : public ZThread {\n@@ -45,2 +45,2 @@\n-  virtual void run_service();\n-  virtual void stop_service();\n+  virtual void run_thread();\n+  virtual void terminate();\n","filename":"src\/hotspot\/share\/gc\/z\/zUnmapper.hpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -0,0 +1,42 @@\n+\/*\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/z\/zUtils.hpp\"\n+#include \"runtime\/nonJavaThread.hpp\"\n+\n+#include <algorithm>\n+\n+const char* ZUtils::thread_name() {\n+  const Thread* const thread = Thread::current();\n+  if (thread->is_Named_thread()) {\n+    const NamedThread* const named = (const NamedThread*)thread;\n+    return named->name();\n+  }\n+\n+  return thread->type_name();\n+}\n+\n+void ZUtils::fill(uintptr_t* addr, size_t count, uintptr_t value) {\n+  std::fill_n(addr, count, value);\n+}\n","filename":"src\/hotspot\/share\/gc\/z\/zUtils.cpp","additions":42,"deletions":0,"binary":false,"changes":42,"status":"added"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,0 +27,1 @@\n+#include \"gc\/z\/zAddress.hpp\"\n@@ -32,0 +33,3 @@\n+  \/\/ Thread\n+  static const char* thread_name();\n+\n@@ -40,3 +44,6 @@\n-  static size_t object_size(uintptr_t addr);\n-  static void object_copy_disjoint(uintptr_t from, uintptr_t to, size_t size);\n-  static void object_copy_conjoint(uintptr_t from, uintptr_t to, size_t size);\n+  static size_t object_size(zaddress addr);\n+  static void object_copy_disjoint(zaddress from, zaddress to, size_t size);\n+  static void object_copy_conjoint(zaddress from, zaddress to, size_t size);\n+\n+  \/\/ Memory\n+  static void fill(uintptr_t* addr, size_t count, uintptr_t value);\n","filename":"src\/hotspot\/share\/gc\/z\/zUtils.hpp","additions":11,"deletions":4,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -29,1 +29,1 @@\n-#include \"gc\/z\/zOop.inline.hpp\"\n+#include \"gc\/z\/zAddress.inline.hpp\"\n@@ -45,2 +45,2 @@\n-inline size_t ZUtils::object_size(uintptr_t addr) {\n-  return words_to_bytes(ZOop::from_address(addr)->size());\n+inline size_t ZUtils::object_size(zaddress addr) {\n+  return words_to_bytes(to_oop(addr)->size());\n@@ -49,2 +49,2 @@\n-inline void ZUtils::object_copy_disjoint(uintptr_t from, uintptr_t to, size_t size) {\n-  Copy::aligned_disjoint_words((HeapWord*)from, (HeapWord*)to, bytes_to_words(size));\n+inline void ZUtils::object_copy_disjoint(zaddress from, zaddress to, size_t size) {\n+  Copy::aligned_disjoint_words((HeapWord*)untype(from), (HeapWord*)untype(to), bytes_to_words(size));\n@@ -53,1 +53,1 @@\n-inline void ZUtils::object_copy_conjoint(uintptr_t from, uintptr_t to, size_t size) {\n+inline void ZUtils::object_copy_conjoint(zaddress from, zaddress to, size_t size) {\n@@ -55,1 +55,1 @@\n-    Copy::aligned_conjoint_words((HeapWord*)from, (HeapWord*)to, bytes_to_words(size));\n+    Copy::aligned_conjoint_words((HeapWord*)untype(from), (HeapWord*)untype(to), bytes_to_words(size));\n","filename":"src\/hotspot\/share\/gc\/z\/zUtils.inline.hpp","additions":8,"deletions":8,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,0 +27,1 @@\n+#include \"memory\/allocation.hpp\"\n","filename":"src\/hotspot\/share\/gc\/z\/zValue.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -30,0 +30,1 @@\n+#include \"gc\/shared\/workerThread.hpp\"\n@@ -33,1 +34,0 @@\n-#include \"gc\/z\/zThread.inline.hpp\"\n@@ -109,1 +109,1 @@\n-  return UseDynamicNumberOfGCThreads ? ConcGCThreads : MAX2(ConcGCThreads, ParallelGCThreads);\n+  return ConcGCThreads;\n@@ -113,1 +113,1 @@\n-  return ZThread::worker_id();\n+  return WorkerThread::worker_id();\n","filename":"src\/hotspot\/share\/gc\/z\/zValue.inline.hpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2019, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2019, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -24,0 +24,1 @@\n+#include \"memory\/allocation.hpp\"\n@@ -27,0 +28,1 @@\n+#include \"gc\/shared\/isGCActiveMark.hpp\"\n@@ -28,0 +30,1 @@\n+#include \"gc\/z\/zGenerationId.hpp\"\n@@ -30,1 +33,0 @@\n-#include \"gc\/z\/zOop.hpp\"\n@@ -35,0 +37,1 @@\n+#include \"gc\/z\/zStoreBarrierBuffer.inline.hpp\"\n@@ -51,0 +54,1 @@\n+#include \"utilities\/resourceHash.hpp\"\n@@ -52,1 +56,1 @@\n-#define BAD_OOP_ARG(o, p)   \"Bad oop \" PTR_FORMAT \" found at \" PTR_FORMAT, p2i(o), p2i(p)\n+#define BAD_OOP_ARG(o, p)   \"Bad oop \" PTR_FORMAT \" found at \" PTR_FORMAT, untype(o), p2i(p)\n@@ -54,6 +58,39 @@\n-static void z_verify_oop(oop* p) {\n-  const oop o = RawAccess<>::oop_load(p);\n-  if (o != NULL) {\n-    const uintptr_t addr = ZOop::to_address(o);\n-    guarantee(ZAddress::is_good(addr), BAD_OOP_ARG(o, p));\n-    guarantee(oopDesc::is_oop(ZOop::from_address(addr)), BAD_OOP_ARG(o, p));\n+static bool z_is_null_relaxed(zpointer o) {\n+  const uintptr_t color_mask = ZPointerAllMetadataMask | ZPointerReservedMask;\n+  return (untype(o) & ~color_mask) == 0;\n+}\n+\n+static void z_verify_old_oop(zpointer* p) {\n+  const zpointer o = *p;\n+  assert(o != zpointer::null, \"Old should not contain raw null\");\n+  if (!z_is_null_relaxed(o)) {\n+    if (ZPointer::is_mark_good(o)) {\n+      \/\/ Even though the pointer is mark good, we can't verify that it should\n+      \/\/ be in the remembered set in old mark end. We have to wait to the verify\n+      \/\/ safepoint after reference processing, where we hold the driver lock and\n+      \/\/ know there is no concurrent remembered set processing in the young generation.\n+      const zaddress addr = ZPointer::uncolor(o);\n+      guarantee(oopDesc::is_oop(to_oop(addr)), BAD_OOP_ARG(o, p));\n+    } else {\n+      const zaddress addr = ZBarrier::load_barrier_on_oop_field_preloaded(nullptr, o);\n+      \/\/ Old to young pointers might not be mark good if the young\n+      \/\/ marking has not finished, which is responsible for coloring\n+      \/\/ these pointers.\n+      if (ZHeap::heap()->is_old(addr) || !ZGeneration::young()->is_phase_mark()) {\n+        \/\/ Old to old pointers are allowed to have bad young bits\n+        guarantee(ZPointer::is_marked_old(o),  BAD_OOP_ARG(o, p));\n+        guarantee(ZHeap::heap()->is_old(p), BAD_OOP_ARG(o, p));\n+      }\n+    }\n+  }\n+}\n+\n+static void z_verify_young_oop(zpointer* p) {\n+  const zpointer o = *p;\n+  if (!z_is_null_relaxed(o)) {\n+    guarantee(ZHeap::heap()->is_young(p), BAD_OOP_ARG(o, p));\n+    guarantee(ZPointer::is_marked_young(o),  BAD_OOP_ARG(o, p));\n+\n+    if (ZPointer::is_load_good(o)) {\n+      guarantee(oopDesc::is_oop(to_oop(ZPointer::uncolor(o))), BAD_OOP_ARG(o, p));\n+    }\n@@ -63,6 +100,9 @@\n-static void z_verify_possibly_weak_oop(oop* p) {\n-  const oop o = RawAccess<>::oop_load(p);\n-  if (o != NULL) {\n-    const uintptr_t addr = ZOop::to_address(o);\n-    guarantee(ZAddress::is_good(addr) || ZAddress::is_finalizable_good(addr), BAD_OOP_ARG(o, p));\n-    guarantee(oopDesc::is_oop(ZOop::from_address(ZAddress::good(addr))), BAD_OOP_ARG(o, p));\n+static void z_verify_root_oop_object(zaddress o, void* p) {\n+  guarantee(oopDesc::is_oop(to_oop(o)), BAD_OOP_ARG(o, p));\n+}\n+\n+static void z_verify_uncolored_root_oop(zaddress* p) {\n+  assert(!ZHeap::heap()->is_in((uintptr_t)p), \"Roots shouldn't be in heap\");\n+  const zaddress o = *p;\n+  if (!is_null(o)) {\n+    z_verify_root_oop_object(o, p);\n@@ -72,1 +112,23 @@\n-class ZVerifyRootClosure : public OopClosure {\n+static void z_verify_possibly_weak_oop(zpointer* p) {\n+  const zpointer o = *p;\n+  if (!z_is_null_relaxed(o)) {\n+    guarantee(ZPointer::is_marked_old(o) || ZPointer::is_marked_finalizable(o), BAD_OOP_ARG(o, p));\n+\n+    const zaddress addr = ZBarrier::load_barrier_on_oop_field_preloaded(nullptr, o);\n+    guarantee(ZHeap::heap()->is_old(addr) || ZPointer::is_marked_young(o), BAD_OOP_ARG(o, p));\n+    guarantee(ZHeap::heap()->is_young(addr) || ZHeap::heap()->is_object_live(addr), BAD_OOP_ARG(o, p));\n+    guarantee(oopDesc::is_oop(to_oop(addr)), BAD_OOP_ARG(o, p));\n+\n+    \/\/ Verify no missing remset entries. We are holding the driver lock here and that\n+    \/\/ allows us to more precisely verify the remembered set, as there is no concurrent\n+    \/\/ young generation collection going on at this point.\n+    const uintptr_t remset_bits = untype(o) & ZPointerRememberedMask;\n+    const uintptr_t prev_remembered = ZPointerRemembered ^ ZPointerRememberedMask;\n+    guarantee(remset_bits != prev_remembered, BAD_OOP_ARG(o, p));\n+    guarantee(remset_bits == ZPointerRememberedMask ||\n+              ZGeneration::young()->is_remembered(p) ||\n+              ZStoreBarrierBuffer::is_in(p), BAD_OOP_ARG(o, p));\n+  }\n+}\n+\n+class ZVerifyColoredRootClosure : public OopClosure {\n@@ -74,1 +136,1 @@\n-  const bool _verify_fixed;\n+  const bool _verify_marked_old;\n@@ -77,2 +139,20 @@\n-  ZVerifyRootClosure(bool verify_fixed) :\n-      _verify_fixed(verify_fixed) {}\n+  ZVerifyColoredRootClosure(bool verify_marked_old) :\n+      OopClosure(),\n+      _verify_marked_old(verify_marked_old) {}\n+\n+  virtual void do_oop(oop* p_) {\n+    zpointer* const p = (zpointer*)p_;\n+\n+    assert(!ZHeap::heap()->is_in((uintptr_t)p), \"Roots shouldn't be in heap\");\n+\n+    const zpointer o = *p;\n+\n+    if (z_is_null_relaxed(o)) {\n+      \/\/ Skip verifying nulls\n+      return;\n+    }\n+\n+    assert(is_valid(o), \"Catch me!\");\n+\n+    if (_verify_marked_old) {\n+      guarantee(ZPointer::is_marked_old(o), BAD_OOP_ARG(o, p));\n@@ -80,3 +160,4 @@\n-  virtual void do_oop(oop* p) {\n-    if (_verify_fixed) {\n-      z_verify_oop(p);\n+      \/\/ Minor collections could have relocated the object;\n+      \/\/ use load barrier to find correct object.\n+      const zaddress addr = ZBarrier::load_barrier_on_oop_field_preloaded(nullptr, o);\n+      z_verify_root_oop_object(addr, p);\n@@ -84,4 +165,7 @@\n-      \/\/ Don't know the state of the oop.\n-      oop obj = *p;\n-      obj = NativeAccess<AS_NO_KEEPALIVE>::oop_load(&obj);\n-      z_verify_oop(&obj);\n+      \/\/ Don't know the state of the oop\n+      if (is_valid(o)) {\n+        \/\/ it looks like a valid colored oop;\n+        \/\/ use load barrier to find correct object.\n+        const zaddress addr = ZBarrier::load_barrier_on_oop_field_preloaded(nullptr, o);\n+        z_verify_root_oop_object(addr, p);\n+      }\n@@ -94,0 +178,1 @@\n+};\n@@ -95,2 +180,9 @@\n-  bool verify_fixed() const {\n-    return _verify_fixed;\n+class ZVerifyUncoloredRootClosure : public OopClosure {\n+public:\n+  virtual void do_oop(oop* p_) {\n+    zaddress* const p = (zaddress*)p_;\n+    z_verify_uncolored_root_oop(p);\n+  }\n+\n+  virtual void do_oop(narrowOop*) {\n+    ShouldNotReachHere();\n@@ -102,2 +194,2 @@\n-  ZVerifyCodeBlobClosure(ZVerifyRootClosure* _cl) :\n-      CodeBlobToOopClosure(_cl, false \/* fix_relocations *\/) {}\n+  ZVerifyCodeBlobClosure(OopClosure* cl) :\n+      CodeBlobToOopClosure(cl, false \/* fix_relocations *\/) {}\n@@ -110,1 +202,1 @@\n-class ZVerifyStack : public OopClosure {\n+class ZVerifyOldOopClosure : public BasicOopIterateClosure {\n@@ -112,4 +204,1 @@\n-  ZVerifyRootClosure* const _cl;\n-  JavaThread*         const _jt;\n-  uint64_t                  _last_good;\n-  bool                      _verifying_bad_frames;\n+  const bool _verify_weaks;\n@@ -118,20 +207,2 @@\n-  ZVerifyStack(ZVerifyRootClosure* cl, JavaThread* jt) :\n-      _cl(cl),\n-      _jt(jt),\n-      _last_good(0),\n-      _verifying_bad_frames(false) {\n-    ZStackWatermark* const stack_watermark = StackWatermarkSet::get<ZStackWatermark>(jt, StackWatermarkKind::gc);\n-\n-    if (_cl->verify_fixed()) {\n-      assert(stack_watermark->processing_started(), \"Should already have been fixed\");\n-      assert(stack_watermark->processing_completed(), \"Should already have been fixed\");\n-    } else {\n-      \/\/ We don't really know the state of the stack, verify watermark.\n-      if (!stack_watermark->processing_started()) {\n-        _verifying_bad_frames = true;\n-      } else {\n-        \/\/ Not time yet to verify bad frames\n-        _last_good = stack_watermark->last_processed();\n-      }\n-    }\n-  }\n+  ZVerifyOldOopClosure(bool verify_weaks) :\n+      _verify_weaks(verify_weaks) {}\n@@ -139,4 +210,8 @@\n-  void do_oop(oop* p) {\n-    if (_verifying_bad_frames) {\n-      const oop obj = *p;\n-      guarantee(!ZAddress::is_good(ZOop::to_address(obj)), BAD_OOP_ARG(obj, p));\n+  virtual void do_oop(oop* p_) {\n+    zpointer* const p = (zpointer*)p_;\n+    if (_verify_weaks) {\n+      z_verify_possibly_weak_oop(p);\n+    } else {\n+      \/\/ We should never encounter finalizable oops through strong\n+      \/\/ paths. This assumes we have only visited strong roots.\n+      z_verify_old_oop(p);\n@@ -144,1 +219,0 @@\n-    _cl->do_oop(p);\n@@ -147,1 +221,1 @@\n-  void do_oop(narrowOop* p) {\n+  virtual void do_oop(narrowOop* p) {\n@@ -151,26 +225,2 @@\n-  void prepare_next_frame(frame& frame) {\n-    if (_cl->verify_fixed()) {\n-      \/\/ All frames need to be good\n-      return;\n-    }\n-\n-    \/\/ The verification has two modes, depending on whether we have reached the\n-    \/\/ last processed frame or not. Before it is reached, we expect everything to\n-    \/\/ be good. After reaching it, we expect everything to be bad.\n-    const uintptr_t sp = reinterpret_cast<uintptr_t>(frame.sp());\n-\n-    if (!_verifying_bad_frames && sp == _last_good) {\n-      \/\/ Found the last good frame, now verify the bad ones\n-      _verifying_bad_frames = true;\n-    }\n-  }\n-\n-  void verify_frames() {\n-    ZVerifyCodeBlobClosure cb_cl(_cl);\n-    for (StackFrameStream frames(_jt, true \/* update *\/, false \/* process_frames *\/);\n-         !frames.is_done();\n-         frames.next()) {\n-      frame& frame = *frames.current();\n-      frame.oops_do(this, &cb_cl, frames.register_map(), DerivedPointerIterationMode::_ignore);\n-      prepare_next_frame(frame);\n-    }\n+  virtual ReferenceIterationMode reference_iteration_mode() {\n+    return _verify_weaks ? DO_FIELDS : DO_FIELDS_EXCEPT_REFERENT;\n@@ -180,1 +230,1 @@\n-class ZVerifyOopClosure : public ClaimMetadataVisitingOopIterateClosure {\n+class ZVerifyYoungOopClosure : public BasicOopIterateClosure {\n@@ -185,2 +235,1 @@\n-  ZVerifyOopClosure(bool verify_weaks) :\n-      ClaimMetadataVisitingOopIterateClosure(ClassLoaderData::_claim_other),\n+  ZVerifyYoungOopClosure(bool verify_weaks) :\n@@ -189,1 +238,2 @@\n-  virtual void do_oop(oop* p) {\n+  virtual void do_oop(oop* p_) {\n+    zpointer* const p = (zpointer*)p_;\n@@ -191,1 +241,2 @@\n-      z_verify_possibly_weak_oop(p);\n+      \/\/z_verify_possibly_weak_oop(p);\n+      z_verify_young_oop(p);\n@@ -195,1 +246,1 @@\n-      z_verify_oop(p);\n+      z_verify_young_oop(p);\n@@ -216,1 +267,1 @@\n-  ZVerifyRootClosure* const _cl;\n+  OopClosure* const _verify_cl;\n@@ -219,2 +270,2 @@\n-  ZVerifyThreadClosure(ZVerifyRootClosure* cl) :\n-      _cl(cl) {}\n+  ZVerifyThreadClosure(OopClosure* verify_cl) :\n+      _verify_cl(verify_cl) {}\n@@ -223,2 +274,0 @@\n-    thread->oops_do_no_frames(_cl, NULL);\n-\n@@ -226,3 +275,3 @@\n-    if (!jt->has_last_Java_frame()) {\n-      return;\n-    }\n+    const ZStackWatermark* const watermark = StackWatermarkSet::get<ZStackWatermark>(jt, StackWatermarkKind::gc);\n+    if (watermark->processing_started_acquire()) {\n+      thread->oops_do_no_frames(_verify_cl, nullptr);\n@@ -230,2 +279,4 @@\n-    ZVerifyStack verify_stack(_cl, jt);\n-    verify_stack.verify_frames();\n+       if (watermark->processing_completed_acquire()) {\n+         thread->oops_do_frames(_verify_cl, nullptr);\n+       }\n+    }\n@@ -239,7 +290,0 @@\n-  const bool               _verify_fixed;\n-\n-  bool trust_nmethod_state() const {\n-    \/\/ The root iterator will visit non-processed\n-    \/\/ nmethods class unloading is turned off.\n-    return ClassUnloading || _verify_fixed;\n-  }\n@@ -248,1 +292,1 @@\n-  ZVerifyNMethodClosure(OopClosure* cl, bool verify_fixed) :\n+  ZVerifyNMethodClosure(OopClosure* cl) :\n@@ -250,2 +294,1 @@\n-      _bs_nm(BarrierSet::barrier_set()->barrier_set_nmethod()),\n-      _verify_fixed(verify_fixed) {}\n+      _bs_nm(BarrierSet::barrier_set()->barrier_set_nmethod()) {}\n@@ -254,1 +297,4 @@\n-    assert(!trust_nmethod_state() || !_bs_nm->is_armed(nm), \"Should not encounter any armed nmethods\");\n+    if (_bs_nm->is_armed(nm)) {\n+      \/\/ Can't verify\n+      return;\n+    }\n@@ -260,1 +306,1 @@\n-void ZVerify::roots_strong(bool verify_fixed) {\n+void ZVerify::roots_strong(bool verify_after_old_mark) {\n@@ -262,1 +308,0 @@\n-  assert(!ZResurrection::is_blocked(), \"Invalid phase\");\n@@ -264,4 +309,8 @@\n-  ZVerifyRootClosure cl(verify_fixed);\n-  ZVerifyCLDClosure cld_cl(&cl);\n-  ZVerifyThreadClosure thread_cl(&cl);\n-  ZVerifyNMethodClosure nm_cl(&cl, verify_fixed);\n+  {\n+    ZVerifyColoredRootClosure cl(verify_after_old_mark);\n+    ZVerifyCLDClosure cld_cl(&cl);\n+\n+    ZRootsIteratorStrongColored roots_strong_colored(ZGenerationIdOptional::none);\n+    roots_strong_colored.apply(&cl,\n+                               &cld_cl);\n+  }\n@@ -269,5 +318,9 @@\n-  ZRootsIterator iter(ClassLoaderData::_claim_none);\n-  iter.apply(&cl,\n-             &cld_cl,\n-             &thread_cl,\n-             &nm_cl);\n+  {\n+    ZVerifyUncoloredRootClosure cl;\n+    ZVerifyThreadClosure thread_cl(&cl);\n+    ZVerifyNMethodClosure nm_cl(&cl);\n+\n+    ZRootsIteratorStrongUncolored roots_strong_uncolored(ZGenerationIdOptional::none);\n+    roots_strong_uncolored.apply(&thread_cl,\n+                                 &nm_cl);\n+  }\n@@ -280,3 +333,75 @@\n-  ZVerifyRootClosure cl(true \/* verify_fixed *\/);\n-  ZWeakRootsIterator iter;\n-  iter.apply(&cl);\n+  ZVerifyColoredRootClosure cl(true \/* verify_after_old_mark*\/);\n+  ZRootsIteratorWeakColored roots_weak_colored(ZGenerationIdOptional::none);\n+  roots_weak_colored.apply(&cl);\n+}\n+\n+zaddress zverify_broken_object = zaddress::null;\n+\n+class ZVerifyObjectClosure : public ObjectClosure, public OopFieldClosure {\n+private:\n+  const bool         _verify_weaks;\n+\n+  zaddress           _visited_base;\n+  volatile zpointer* _visited_p;\n+  zpointer           _visited_ptr_pre_loaded;\n+\n+public:\n+  ZVerifyObjectClosure(bool verify_weaks) :\n+      _verify_weaks(verify_weaks),\n+      _visited_base(),\n+      _visited_p(),\n+      _visited_ptr_pre_loaded() {}\n+\n+  void log_dead_object(zaddress addr) {\n+    tty->print_cr(\"ZVerify found dead object: \" PTR_FORMAT \" at p: \" PTR_FORMAT \" ptr: \" PTR_FORMAT, untype(addr), p2i((void*)_visited_p), untype(_visited_ptr_pre_loaded));\n+    to_oop(addr)->print();\n+    tty->print_cr(\"--- From --- \");\n+    if (_visited_base != zaddress::null) {\n+      to_oop(_visited_base)->print();\n+    }\n+    tty->cr();\n+\n+    if (zverify_broken_object == zaddress::null) {\n+      zverify_broken_object = addr;\n+    }\n+  }\n+\n+  void verify_live_object(oop obj) {\n+    \/\/ Verify that its pointers are sane\n+    ZVerifyOldOopClosure cl(_verify_weaks);\n+    ZIterator::oop_iterate_safe(obj, &cl);\n+  }\n+\n+  virtual void do_object(oop obj) {\n+    guarantee(oopDesc::is_oop_or_null(obj), \"Must be\");\n+\n+    const zaddress addr = to_zaddress(obj);\n+    if (ZHeap::heap()->is_old(addr)) {\n+      if (ZHeap::heap()->is_object_live(addr)) {\n+        verify_live_object(obj);\n+      } else {\n+        log_dead_object(addr);\n+      }\n+    } else {\n+      \/\/ Young object - no verification\n+    }\n+  }\n+\n+  virtual void do_field(oop base, oop* p) {\n+    _visited_base = to_zaddress(base);\n+    _visited_p = (volatile zpointer*)p;\n+    _visited_ptr_pre_loaded = Atomic::load(_visited_p);\n+  }\n+};\n+\n+void ZVerify::threads_start_processing() {\n+  class StartProcessingClosure : public ThreadClosure {\n+  public:\n+    void do_thread(Thread* thread) {\n+      StackWatermarkSet::start_processing(JavaThread::cast(thread), StackWatermarkKind::gc);\n+    }\n+  };\n+\n+  ZJavaThreadsIterator threads_iterator(ZGenerationIdOptional::none);\n+  StartProcessingClosure cl;\n+  threads_iterator.apply(&cl);\n@@ -286,0 +411,5 @@\n+  if (ZAbort::should_abort()) {\n+    \/\/ Invariants might be a bit mushy if the young generation\n+    \/\/ collection was forced to shut down. So let's be a bit forgiving here.\n+    return;\n+  }\n@@ -287,1 +417,2 @@\n-  assert(ZGlobalPhase == ZPhaseMarkCompleted, \"Invalid phase\");\n+  assert(ZGeneration::young()->is_phase_mark_complete() ||\n+         ZGeneration::old()->is_phase_mark_complete(), \"Invalid phase\");\n@@ -290,3 +421,9 @@\n-  ZVerifyOopClosure cl(verify_weaks);\n-  ObjectToOopClosure object_cl(&cl);\n-  ZHeap::heap()->object_iterate(&object_cl, verify_weaks);\n+  \/\/ Note that object verification will fix the pointers and\n+  \/\/ only verify that the resulting objects are sane.\n+\n+  \/\/ The verification VM_Operation doesn't start the thread processing.\n+  \/\/ Do it here, after the roots have been verified.\n+  threads_start_processing();\n+\n+  ZVerifyObjectClosure object_cl(verify_weaks);\n+  ZHeap::heap()->object_and_field_iterate(&object_cl, &object_cl, verify_weaks);\n@@ -297,1 +434,0 @@\n-  ZStatTimerDisable disable;\n@@ -299,1 +435,1 @@\n-    roots_strong(false \/* verify_fixed *\/);\n+    roots_strong(false \/* verify_after_old_mark *\/);\n@@ -305,1 +441,0 @@\n-  ZStatTimerDisable disable;\n@@ -307,1 +442,1 @@\n-    roots_strong(true \/* verify_fixed *\/);\n+    roots_strong(true \/* verify_after_old_mark *\/);\n@@ -310,0 +445,3 @@\n+    \/\/ Workaround OopMapCacheAlloc_lock reordering with the StackWatermark_lock\n+    DisableIsGCActiveMark mark;\n+\n@@ -311,0 +449,1 @@\n+    guarantee(zverify_broken_object == zaddress::null, \"Verification failed\");\n@@ -316,1 +455,0 @@\n-  ZStatTimerDisable disable;\n@@ -318,1 +456,1 @@\n-    roots_strong(true \/* verify_fixed *\/);\n+    roots_strong(true \/* verify_after_old_mark *\/);\n@@ -326,2 +464,14 @@\n-template <bool Map>\n-class ZPageDebugMapOrUnmapClosure : public ZPageClosure {\n+\/\/\n+\/\/ Remembered set verification\n+\/\/\n+\n+typedef ResourceHashtable<volatile zpointer*, bool, 1009, AnyObj::C_HEAP, mtGC> ZStoreBarrierBufferTable;\n+\n+static ZStoreBarrierBufferTable* z_verify_store_barrier_buffer_table = nullptr;\n+\n+#define BAD_REMSET_ARG(p, ptr, addr) \\\n+  \"Missing remembered set at \" PTR_FORMAT \" pointing at \" PTR_FORMAT \\\n+  \" (\" PTR_FORMAT \" + \" INTX_FORMAT \")\" \\\n+  , p2i(p), untype(ptr), untype(addr), p2i(p) - untype(addr)\n+\n+class ZVerifyRemsetBeforeOopClosure : public BasicOopIterateClosure {\n@@ -329,1 +479,2 @@\n-  const ZPageAllocator* const _allocator;\n+  ZForwarding*    _forwarding;\n+  zaddress_unsafe _from_addr;\n@@ -332,2 +483,3 @@\n-  ZPageDebugMapOrUnmapClosure(const ZPageAllocator* allocator) :\n-      _allocator(allocator) {}\n+  ZVerifyRemsetBeforeOopClosure(ZForwarding* forwarding) :\n+      _forwarding(forwarding),\n+      _from_addr(zaddress_unsafe::null) {}\n@@ -335,3 +487,32 @@\n-  void do_page(const ZPage* page) {\n-    if (Map) {\n-      _allocator->debug_map_page(page);\n+  void set_from_addr(zaddress_unsafe addr) {\n+    _from_addr = addr;\n+  }\n+\n+  virtual void do_oop(oop* p_) {\n+    volatile zpointer* const p = (volatile zpointer*)p_;\n+    const zpointer ptr = *p;\n+\n+    if (ZPointer::is_remembered_exact(ptr)) {\n+      \/\/ When the remembered bits are 11, it means that it is intentionally\n+      \/\/ not part of the remembered set\n+      return;\n+    }\n+\n+    if (ZBufferStoreBarriers && z_verify_store_barrier_buffer_table->get(p) != nullptr) {\n+      \/\/ If this oop location is in the store barrier buffer, we can't assume\n+      \/\/ that it should have a remset entry\n+      return;\n+    }\n+\n+    if (_forwarding->find(_from_addr) != zaddress::null) {\n+      \/\/ If the mutator has already relocated the object to to-space, we defer\n+      \/\/ and do to-space verification afterwards instead, because store barrier\n+      \/\/ buffers could have installed the remembered set entry in to-space and\n+      \/\/ then flushed the store barrier buffer, and then start young marking\n+      return;\n+    }\n+\n+    ZPage* page = _forwarding->page();\n+\n+    if (ZGeneration::old()->active_remset_is_current()) {\n+      guarantee(page->is_remembered(p), BAD_REMSET_ARG(p, ptr, _from_addr));\n@@ -339,1 +520,1 @@\n-      _allocator->debug_unmap_page(page);\n+      guarantee(page->was_remembered(p), BAD_REMSET_ARG(p, ptr, _from_addr));\n@@ -342,0 +523,8 @@\n+\n+  virtual void do_oop(narrowOop* p) {\n+    ShouldNotReachHere();\n+  }\n+\n+  virtual ReferenceIterationMode reference_iteration_mode() {\n+    return DO_FIELDS;\n+  }\n@@ -344,6 +533,20 @@\n-ZVerifyViewsFlip::ZVerifyViewsFlip(const ZPageAllocator* allocator) :\n-    _allocator(allocator) {\n-  if (ZVerifyViews) {\n-    \/\/ Unmap all pages\n-    ZPageDebugMapOrUnmapClosure<false \/* Map *\/> cl(_allocator);\n-    ZHeap::heap()->pages_do(&cl);\n+void ZVerify::on_color_flip() {\n+  if (!ZVerifyRemembered || !ZBufferStoreBarriers) {\n+    return;\n+  }\n+\n+  \/\/ Reset the table tracking the stale stores of the store barrier buffer\n+  delete z_verify_store_barrier_buffer_table;\n+  z_verify_store_barrier_buffer_table = new (mtGC) ZStoreBarrierBufferTable();\n+\n+  \/\/ Gather information from store barrier buffers as we currently can't verify\n+  \/\/ remset entries for oop locations touched by the store barrier buffer\n+\n+  for (JavaThreadIteratorWithHandle jtiwh; JavaThread* const jt = jtiwh.next(); ) {\n+    const ZStoreBarrierBuffer* const buffer = ZThreadLocalData::store_barrier_buffer(jt);\n+\n+    for (int i = buffer->current(); i < (int)ZStoreBarrierBuffer::_buffer_length; ++i) {\n+      volatile zpointer* const p = buffer->_buffer[i]._p;\n+      bool created = false;\n+      z_verify_store_barrier_buffer_table->put_if_absent(p, true, &created);\n+    }\n@@ -353,5 +556,8 @@\n-ZVerifyViewsFlip::~ZVerifyViewsFlip() {\n-  if (ZVerifyViews) {\n-    \/\/ Map all pages\n-    ZPageDebugMapOrUnmapClosure<true \/* Map *\/> cl(_allocator);\n-    ZHeap::heap()->pages_do(&cl);\n+void ZVerify::before_relocation(ZForwarding* forwarding) {\n+  if (!ZVerifyRemembered) {\n+    return;\n+  }\n+\n+  if (forwarding->from_age() != ZPageAge::old) {\n+    \/\/ Only supports verification of old-to-old relocations now\n+    return;\n@@ -359,0 +565,15 @@\n+\n+  \/\/ Verify that the inactive remset is cleared\n+  if (ZGeneration::old()->active_remset_is_current()) {\n+    forwarding->page()->verify_remset_cleared_previous();\n+  } else {\n+    forwarding->page()->verify_remset_cleared_current();\n+  }\n+\n+  ZVerifyRemsetBeforeOopClosure cl(forwarding);\n+\n+  forwarding->object_iterate([&](oop obj) {\n+    const zaddress_unsafe addr = to_zaddress_unsafe(cast_from_oop<uintptr_t>(obj));\n+    cl.set_from_addr(addr);\n+    obj->oop_iterate(&cl);\n+  });\n@@ -361,1 +582,5 @@\n-#ifdef ASSERT\n+class ZVerifyRemsetAfterOopClosure : public BasicOopIterateClosure {\n+private:\n+  ZForwarding* const _forwarding;\n+  zaddress_unsafe    _from_addr;\n+  zaddress           _to_addr;\n@@ -363,1 +588,0 @@\n-class ZVerifyBadOopClosure : public OopClosure {\n@@ -365,3 +589,70 @@\n-  virtual void do_oop(oop* p) {\n-    const oop o = *p;\n-    assert(!ZAddress::is_good(ZOop::to_address(o)), \"Should not be good: \" PTR_FORMAT, p2i(o));\n+  ZVerifyRemsetAfterOopClosure(ZForwarding* forwarding) :\n+      _forwarding(forwarding),\n+      _from_addr(zaddress_unsafe::null),\n+      _to_addr(zaddress::null) {}\n+\n+  void set_from_addr(zaddress_unsafe addr) {\n+    _from_addr = addr;\n+  }\n+\n+  void set_to_addr(zaddress addr) {\n+    _to_addr = addr;\n+  }\n+\n+  virtual void do_oop(oop* p_) {\n+    volatile zpointer* const p = (volatile zpointer*)p_;\n+    const zpointer ptr = Atomic::load(p);\n+\n+    \/\/ Order this load w.r.t. the was_remembered load which can race when\n+    \/\/ the remset scanning of the to-space object is concurrently forgetting\n+    \/\/ an entry.\n+    OrderAccess::loadload();\n+\n+    if (ZPointer::is_remembered_exact(ptr)) {\n+      \/\/ When the remembered bits are 11, it means that it is intentionally\n+      \/\/ not part of the remembered set\n+      return;\n+    }\n+\n+    if (ZPointer::is_store_good(ptr)) {\n+      \/\/ In to-space, there could be stores racing with the verification.\n+      \/\/ Such stores may not have reliably manifested in the remembered\n+      \/\/ sets yet.\n+      return;\n+    }\n+\n+    if (ZBufferStoreBarriers && z_verify_store_barrier_buffer_table->get(p) != nullptr) {\n+      \/\/ If this to-space oop location is in the store barrier buffer, we\n+      \/\/ can't assume that it should have a remset entry\n+      return;\n+    }\n+\n+    const uintptr_t p_offset = uintptr_t(p) - untype(_to_addr);\n+    volatile zpointer* const fromspace_p = (volatile zpointer*)(untype(_from_addr) + p_offset);\n+\n+    if (ZBufferStoreBarriers && z_verify_store_barrier_buffer_table->get(fromspace_p) != nullptr) {\n+      \/\/ If this from-space oop location is in the store barrier buffer, we\n+      \/\/ can't assume that it should have a remset entry\n+      return;\n+    }\n+\n+    ZPage* page = ZHeap::heap()->page(p);\n+\n+    if (page->is_remembered(p) || page->was_remembered(p)) {\n+      \/\/ No missing remembered set entry\n+      return;\n+    }\n+\n+    OrderAccess::loadload();\n+    if (Atomic::load(p) != ptr) {\n+      \/\/ Order the was_remembered bitmap load w.r.t. the reload of the zpointer.\n+      \/\/ Sometimes the was_remembered() call above races with clearing of the\n+      \/\/ previous bits, when the to-space object is concurrently forgetting\n+      \/\/ remset entries because they were not so useful. When that happens,\n+      \/\/ we have already self healed the pointers to have 11 in the remset\n+      \/\/ bits.\n+      return;\n+    }\n+\n+    guarantee(ZGeneration::young()->is_phase_mark(), \"Should be in the mark phase \" BAD_REMSET_ARG(p, ptr, _to_addr));\n+    guarantee(_forwarding->relocated_remembered_fields_published_contains(p), BAD_REMSET_ARG(p, ptr, _to_addr));\n@@ -373,12 +664,0 @@\n-};\n-\n-\/\/ This class encapsulates various marks we need to deal with calling the\n-\/\/ frame iteration code from arbitrary points in the runtime. It is mostly\n-\/\/ due to problems that we might want to eventually clean up inside of the\n-\/\/ frame iteration code, such as creating random handles even though there\n-\/\/ is no safepoint to protect against, and fiddling around with exceptions.\n-class StackWatermarkProcessingMark {\n-  ResetNoHandleMark     _rnhm;\n-  HandleMark            _hm;\n-  PreserveExceptionMark _pem;\n-  ResourceMark          _rm;\n@@ -386,6 +665,3 @@\n-public:\n-  StackWatermarkProcessingMark(Thread* thread) :\n-      _rnhm(),\n-      _hm(thread),\n-      _pem(thread),\n-      _rm(thread) {}\n+  virtual ReferenceIterationMode reference_iteration_mode() {\n+    return DO_FIELDS;\n+  }\n@@ -394,3 +670,15 @@\n-void ZVerify::verify_frame_bad(const frame& fr, RegisterMap& register_map) {\n-  ZVerifyBadOopClosure verify_cl;\n-  fr.oops_do(&verify_cl, NULL, &register_map, DerivedPointerIterationMode::_ignore);\n+void ZVerify::after_relocation_internal(ZForwarding* forwarding) {\n+  ZVerifyRemsetAfterOopClosure cl(forwarding);\n+\n+  forwarding->address_unsafe_iterate_via_table([&](zaddress_unsafe from_addr) {\n+    \/\/ If no field in this object was in the store barrier buffer\n+    \/\/ when relocation started, we should be able to verify trivially\n+    ZGeneration* const from_generation = forwarding->from_age() == ZPageAge::old ? (ZGeneration*)ZGeneration::old()\n+                                                                           : (ZGeneration*)ZGeneration::young();\n+    const zaddress to_addr = from_generation->remap_object(from_addr);\n+\n+    cl.set_from_addr(from_addr);\n+    cl.set_to_addr(to_addr);\n+    const oop to_obj = to_oop(to_addr);\n+    to_obj->oop_iterate(&cl);\n+  });\n@@ -399,4 +687,4 @@\n-void ZVerify::verify_thread_head_bad(JavaThread* jt) {\n-  ZVerifyBadOopClosure verify_cl;\n-  jt->oops_do_no_frames(&verify_cl, NULL);\n-}\n+void ZVerify::after_relocation(ZForwarding* forwarding) {\n+  if (!ZVerifyRemembered) {\n+    return;\n+  }\n@@ -404,8 +692,12 @@\n-void ZVerify::verify_thread_frames_bad(JavaThread* jt) {\n-  if (jt->has_last_Java_frame()) {\n-    ZVerifyBadOopClosure verify_cl;\n-    StackWatermarkProcessingMark swpm(Thread::current());\n-    \/\/ Traverse the execution stack\n-    for (StackFrameStream fst(jt, true \/* update *\/, false \/* process_frames *\/); !fst.is_done(); fst.next()) {\n-      fst.current()->oops_do(&verify_cl, NULL \/* code_cl *\/, fst.register_map(), DerivedPointerIterationMode::_ignore);\n-    }\n+  if (forwarding->to_age() != ZPageAge::old) {\n+    \/\/ No remsets to verify in the young gen\n+    return;\n+  }\n+\n+  if (ZGeneration::young()->is_phase_mark() &&\n+      forwarding->relocated_remembered_fields_is_concurrently_scanned()) {\n+    \/\/ Can't verify to-space objects if concurrent YC rejected published\n+    \/\/ remset information, because that data is incomplete. The YC might\n+    \/\/ not have finished scanning the forwarding, and might be about to\n+    \/\/ insert required remembered set entries.\n+    return;\n@@ -413,0 +705,2 @@\n+\n+  after_relocation_internal(forwarding);\n@@ -415,1 +709,20 @@\n-#endif \/\/ ASSERT\n+void ZVerify::after_scan(ZForwarding* forwarding) {\n+  if (!ZVerifyRemembered) {\n+    return;\n+  }\n+\n+  if (ZAbort::should_abort()) {\n+    \/\/ We can't verify remembered set accurately when shutting down the VM\n+    return;\n+  }\n+\n+  if (!ZGeneration::old()->is_phase_relocate() ||\n+      !forwarding->relocated_remembered_fields_is_concurrently_scanned()) {\n+    \/\/ Only verify remembered set from remembered set scanning, when the\n+    \/\/ remembered set scanning rejected the publishing information of concurrent\n+    \/\/ old generation relocation\n+    return;\n+  }\n+\n+  after_relocation_internal(forwarding);\n+}\n","filename":"src\/hotspot\/share\/gc\/z\/zVerify.cpp","additions":510,"deletions":197,"binary":false,"changes":707,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2019, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2019, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -30,0 +30,1 @@\n+class ZForwarding;\n@@ -34,1 +35,1 @@\n-  static void roots_strong(bool verify_fixed);\n+  static void roots_strong(bool verify_after_old_mark);\n@@ -38,0 +39,3 @@\n+  static void threads_start_processing();\n+\n+  static void after_relocation_internal(ZForwarding* forwarding);\n@@ -44,4 +48,3 @@\n-  static void verify_thread_head_bad(JavaThread* thread) NOT_DEBUG_RETURN;\n-  static void verify_thread_frames_bad(JavaThread* thread) NOT_DEBUG_RETURN;\n-  static void verify_frame_bad(const frame& fr, RegisterMap& register_map) NOT_DEBUG_RETURN;\n-};\n+  static void before_relocation(ZForwarding* forwarding);\n+  static void after_relocation(ZForwarding* forwarding);\n+  static void after_scan(ZForwarding* forwarding);\n@@ -49,7 +52,1 @@\n-class ZVerifyViewsFlip {\n-private:\n-  const ZPageAllocator* const _allocator;\n-\n-public:\n-  ZVerifyViewsFlip(const ZPageAllocator* allocator);\n-  ~ZVerifyViewsFlip();\n+  static void on_color_flip();\n","filename":"src\/hotspot\/share\/gc\/z\/zVerify.hpp","additions":10,"deletions":13,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -39,6 +39,1 @@\n-  \/\/ Check max supported heap size\n-  if (max_capacity > ZAddressOffsetMax) {\n-    log_error_p(gc)(\"Java heap too large (max supported heap size is \" SIZE_FORMAT \"G)\",\n-                    ZAddressOffsetMax \/ G);\n-    return;\n-  }\n+  assert(max_capacity <= ZAddressOffsetMax, \"Too large max_capacity\");\n@@ -62,1 +57,1 @@\n-size_t ZVirtualMemoryManager::reserve_discontiguous(uintptr_t start, size_t size, size_t min_range) {\n+size_t ZVirtualMemoryManager::reserve_discontiguous(zoffset start, size_t size, size_t min_range) {\n@@ -92,1 +87,1 @@\n-  size_t start = 0;\n+  uintptr_t start = 0;\n@@ -98,1 +93,1 @@\n-    reserved += reserve_discontiguous(start, remaining, min_range);\n+    reserved += reserve_discontiguous(to_zoffset(start), remaining, min_range);\n@@ -105,1 +100,1 @@\n-bool ZVirtualMemoryManager::reserve_contiguous(uintptr_t start, size_t size) {\n+bool ZVirtualMemoryManager::reserve_contiguous(zoffset start, size_t size) {\n@@ -109,3 +104,1 @@\n-  const uintptr_t marked0 = ZAddress::marked0(start);\n-  const uintptr_t marked1 = ZAddress::marked1(start);\n-  const uintptr_t remapped = ZAddress::remapped(start);\n+  const zaddress_unsafe addr = ZOffset::address_unsafe(start);\n@@ -114,12 +107,1 @@\n-  if (!pd_reserve(marked0, size)) {\n-    return false;\n-  }\n-\n-  if (!pd_reserve(marked1, size)) {\n-    pd_unreserve(marked0, size);\n-    return false;\n-  }\n-\n-  if (!pd_reserve(remapped, size)) {\n-    pd_unreserve(marked0, size);\n-    pd_unreserve(marked1, size);\n+  if (!pd_reserve(addr, size)) {\n@@ -130,3 +112,1 @@\n-  nmt_reserve(marked0, size);\n-  nmt_reserve(marked1, size);\n-  nmt_reserve(remapped, size);\n+  nmt_reserve(addr, size);\n@@ -145,2 +125,2 @@\n-  for (size_t start = 0; start + size <= ZAddressOffsetMax; start += increment) {\n-    if (reserve_contiguous(start, size)) {\n+  for (uintptr_t start = 0; start + size <= ZAddressOffsetMax; start += increment) {\n+    if (reserve_contiguous(to_zoffset(start), size)) {\n@@ -157,1 +137,1 @@\n-  const size_t limit = MIN2(ZAddressOffsetMax, ZAddressSpaceLimit::heap_view());\n+  const size_t limit = MIN2(ZAddressOffsetMax, ZAddressSpaceLimit::heap());\n@@ -174,2 +154,1 @@\n-  log_info_p(gc, init)(\"Address Space Size: \" SIZE_FORMAT \"M x \" SIZE_FORMAT \" = \" SIZE_FORMAT \"M\",\n-                       reserved \/ M, ZHeapViews, (reserved * ZHeapViews) \/ M);\n+  log_info_p(gc, init)(\"Address Space Size: \" SIZE_FORMAT \"M\", reserved \/ M);\n@@ -183,3 +162,3 @@\n-void ZVirtualMemoryManager::nmt_reserve(uintptr_t start, size_t size) {\n-  MemTracker::record_virtual_memory_reserve((void*)start, size, CALLER_PC);\n-  MemTracker::record_virtual_memory_type((void*)start, mtJavaHeap);\n+void ZVirtualMemoryManager::nmt_reserve(zaddress_unsafe start, size_t size) {\n+  MemTracker::record_virtual_memory_reserve((void*)untype(start), size, CALLER_PC);\n+  MemTracker::record_virtual_memory_type((void*)untype(start), mtJavaHeap);\n@@ -193,1 +172,1 @@\n-  uintptr_t start;\n+  zoffset start;\n","filename":"src\/hotspot\/share\/gc\/z\/zVirtualMemory.cpp","additions":17,"deletions":38,"binary":false,"changes":55,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -33,2 +33,2 @@\n-  uintptr_t _start;\n-  uintptr_t _end;\n+  zoffset     _start;\n+  zoffset_end _end;\n@@ -38,1 +38,1 @@\n-  ZVirtualMemory(uintptr_t start, size_t size);\n+  ZVirtualMemory(zoffset start, size_t size);\n@@ -41,2 +41,2 @@\n-  uintptr_t start() const;\n-  uintptr_t end() const;\n+  zoffset start() const;\n+  zoffset_end end() const;\n@@ -51,1 +51,1 @@\n-  uintptr_t      _reserved;\n+  size_t         _reserved;\n@@ -57,2 +57,2 @@\n-  bool pd_reserve(uintptr_t addr, size_t size);\n-  void pd_unreserve(uintptr_t addr, size_t size);\n+  bool pd_reserve(zaddress_unsafe addr, size_t size);\n+  void pd_unreserve(zaddress_unsafe addr, size_t size);\n@@ -60,1 +60,1 @@\n-  bool reserve_contiguous(uintptr_t start, size_t size);\n+  bool reserve_contiguous(zoffset start, size_t size);\n@@ -62,1 +62,1 @@\n-  size_t reserve_discontiguous(uintptr_t start, size_t size, size_t min_range);\n+  size_t reserve_discontiguous(zoffset start, size_t size, size_t min_range);\n@@ -66,1 +66,1 @@\n-  void nmt_reserve(uintptr_t start, size_t size);\n+  void nmt_reserve(zaddress_unsafe start, size_t size);\n@@ -74,1 +74,1 @@\n-  uintptr_t lowest_available_address() const;\n+  zoffset lowest_available_address() const;\n","filename":"src\/hotspot\/share\/gc\/z\/zVirtualMemory.hpp","additions":13,"deletions":13,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -32,2 +32,2 @@\n-    _start(UINTPTR_MAX),\n-    _end(UINTPTR_MAX) {}\n+    _start(zoffset(UINTPTR_MAX)),\n+    _end(zoffset_end(UINTPTR_MAX)) {}\n@@ -35,1 +35,1 @@\n-inline ZVirtualMemory::ZVirtualMemory(uintptr_t start, size_t size) :\n+inline ZVirtualMemory::ZVirtualMemory(zoffset start, size_t size) :\n@@ -37,1 +37,1 @@\n-    _end(start + size) {}\n+    _end(to_zoffset_end(start, size)) {}\n@@ -40,1 +40,1 @@\n-  return _start == UINTPTR_MAX;\n+  return _start == zoffset(UINTPTR_MAX);\n@@ -43,1 +43,1 @@\n-inline uintptr_t ZVirtualMemory::start() const {\n+inline zoffset ZVirtualMemory::start() const {\n@@ -47,1 +47,1 @@\n-inline uintptr_t ZVirtualMemory::end() const {\n+inline zoffset_end ZVirtualMemory::end() const {\n@@ -64,1 +64,1 @@\n-inline uintptr_t ZVirtualMemoryManager::lowest_available_address() const {\n+inline zoffset ZVirtualMemoryManager::lowest_available_address() const {\n","filename":"src\/hotspot\/share\/gc\/z\/zVirtualMemory.inline.hpp","additions":9,"deletions":9,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -25,0 +25,2 @@\n+#include \"gc\/shared\/suspendibleThreadSet.hpp\"\n+#include \"gc\/z\/zAddress.inline.hpp\"\n@@ -26,0 +28,1 @@\n+#include \"gc\/z\/zHeap.inline.hpp\"\n@@ -30,0 +33,3 @@\n+#include \"memory\/iterator.hpp\"\n+#include \"runtime\/atomic.hpp\"\n+#include \"utilities\/debug.hpp\"\n@@ -34,13 +40,2 @@\n-    \/\/ Read the oop once, to make sure the liveness check\n-    \/\/ and the later clearing uses the same value.\n-    const oop obj = Atomic::load(p);\n-    if (ZBarrier::is_alive_barrier_on_phantom_oop(obj)) {\n-      ZBarrier::keep_alive_barrier_on_phantom_oop_field(p);\n-    } else {\n-      \/\/ The destination could have been modified\/reused, in which case\n-      \/\/ we don't want to clear it. However, no one could write the same\n-      \/\/ oop here again (the object would be strongly live and we would\n-      \/\/ not consider clearing such oops), so therefore we don't have an\n-      \/\/ ABA problem here.\n-      Atomic::cmpxchg(p, obj, oop(NULL));\n-    }\n+    ZBarrier::clean_barrier_on_phantom_oop_field((zpointer*)p);\n+    SuspendibleThreadSet::yield();\n@@ -59,1 +54,1 @@\n-  ZWeakRootsIterator _weak_roots;\n+  ZRootsIteratorWeakColored _roots_weak_colored;\n@@ -64,1 +59,1 @@\n-      _weak_roots() {}\n+      _roots_weak_colored(ZGenerationIdOptional::old) {}\n@@ -67,1 +62,1 @@\n-    _weak_roots.report_num_dead();\n+    _roots_weak_colored.report_num_dead();\n@@ -71,0 +66,1 @@\n+    SuspendibleThreadSetJoiner sts_joiner;\n@@ -72,1 +68,1 @@\n-    _weak_roots.apply(&cl);\n+    _roots_weak_colored.apply(&cl);\n","filename":"src\/hotspot\/share\/gc\/z\/zWeakRootsProcessor.cpp","additions":14,"deletions":18,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,0 +27,1 @@\n+#include \"gc\/z\/zHeap.inline.hpp\"\n@@ -30,1 +31,0 @@\n-#include \"gc\/z\/zThread.hpp\"\n@@ -34,29 +34,3 @@\n-class ZWorkersInitializeTask : public WorkerTask {\n-private:\n-  const uint     _nworkers;\n-  uint           _started;\n-  ZConditionLock _lock;\n-\n-public:\n-  ZWorkersInitializeTask(uint nworkers) :\n-      WorkerTask(\"ZWorkersInitializeTask\"),\n-      _nworkers(nworkers),\n-      _started(0),\n-      _lock() {}\n-\n-  virtual void work(uint worker_id) {\n-    \/\/ Register as worker\n-    ZThread::set_worker();\n-\n-    \/\/ Wait for all threads to start\n-    ZLocker<ZConditionLock> locker(&_lock);\n-    if (++_started == _nworkers) {\n-      \/\/ All threads started\n-      _lock.notify_all();\n-    } else {\n-      while (_started != _nworkers) {\n-        _lock.wait();\n-      }\n-    }\n-  }\n-};\n+static const char* workers_name(ZGenerationId id) {\n+  return (id == ZGenerationId::young) ? \"ZWorkerYoung\" : \"ZWorkerOld\";\n+}\n@@ -64,3 +38,3 @@\n-ZWorkers::ZWorkers() :\n-    _workers(\"ZWorker\",\n-             UseDynamicNumberOfGCThreads ? ConcGCThreads : MAX2(ConcGCThreads, ParallelGCThreads)) {\n+static const char* generation_name(ZGenerationId id) {\n+  return (id == ZGenerationId::young) ? \"Young\" : \"Old\";\n+}\n@@ -68,5 +42,17 @@\n-  if (UseDynamicNumberOfGCThreads) {\n-    log_info_p(gc, init)(\"GC Workers: %u (dynamic)\", _workers.max_workers());\n-  } else {\n-    log_info_p(gc, init)(\"GC Workers: %u\/%u (static)\", ConcGCThreads, _workers.max_workers());\n-  }\n+static uint max_nworkers(ZGenerationId id) {\n+  return id == ZGenerationId::young ? ZYoungGCThreads : ZOldGCThreads;\n+}\n+\n+ZWorkers::ZWorkers(ZGenerationId id, ZStatWorkers* stats) :\n+    _workers(workers_name(id),\n+             max_nworkers(id)),\n+    _generation_name(generation_name(id)),\n+    _resize_lock(),\n+    _requested_nworkers(0),\n+    _is_active(false),\n+    _stats(stats) {\n+\n+  log_info_p(gc, init)(\"GC Workers for %s Generation: %u (%s)\",\n+                       _generation_name,\n+                       _workers.max_workers(),\n+                       UseDynamicNumberOfGCThreads ? \"dynamic\" : \"static\");\n@@ -80,0 +66,1 @@\n+}\n@@ -81,3 +68,2 @@\n-  \/\/ Execute task to register threads as workers\n-  ZWorkersInitializeTask task(_workers.max_workers());\n-  _workers.run_task(&task);\n+bool ZWorkers::is_active() const {\n+  return _is_active;\n@@ -91,1 +77,2 @@\n-  log_info(gc, task)(\"Using %u workers\", nworkers);\n+  log_info(gc, task)(\"Using %u Workers for %s Generation\", nworkers, _generation_name);\n+  ZLocker<ZLock> locker(&_resize_lock);\n@@ -95,0 +82,11 @@\n+void ZWorkers::set_active() {\n+  ZLocker<ZLock> locker(&_resize_lock);\n+  _is_active = true;\n+  _requested_nworkers = 0;\n+}\n+\n+void ZWorkers::set_inactive() {\n+  ZLocker<ZLock> locker(&_resize_lock);\n+  _is_active = false;\n+}\n+\n@@ -96,2 +94,7 @@\n-  log_debug(gc, task)(\"Executing Task: %s, Active Workers: %u\", task->name(), active_workers());\n-  ZStatWorkers::at_start();\n+  log_debug(gc, task)(\"Executing %s using %s with %u workers\", task->name(), _workers.name(), active_workers());\n+\n+  {\n+    ZLocker<ZLock> locker(&_resize_lock);\n+    _stats->at_start(active_workers());\n+  }\n+\n@@ -99,1 +102,23 @@\n-  ZStatWorkers::at_end();\n+\n+  {\n+    ZLocker<ZLock> locker(&_resize_lock);\n+    _stats->at_end();\n+  }\n+}\n+\n+void ZWorkers::run(ZRestartableTask* task) {\n+  for (;;) {\n+    \/\/ Run task\n+    run(static_cast<ZTask*>(task));\n+\n+    ZLocker<ZLock> locker(&_resize_lock);\n+    if (_requested_nworkers == 0) {\n+      \/\/ Task completed\n+      return;\n+    }\n+\n+    \/\/ Restart task with requested number of active workers\n+    _workers.set_active_workers(_requested_nworkers);\n+    task->resize_workers(active_workers());\n+    _requested_nworkers = 0;\n+  }\n@@ -103,1 +128,1 @@\n-  \/\/ Save number of active workers\n+  \/\/ Get and set number of active workers\n@@ -105,0 +130,1 @@\n+  _workers.set_active_workers(_workers.max_workers());\n@@ -107,2 +133,1 @@\n-  _workers.set_active_workers(_workers.max_workers());\n-  log_debug(gc, task)(\"Executing Task: %s, Active Workers: %u\", task->name(), active_workers());\n+  log_debug(gc, task)(\"Executing %s using %s with %u workers\", task->name(), _workers.name(), active_workers());\n@@ -118,0 +143,25 @@\n+\n+ZLock* ZWorkers::resizing_lock() {\n+  return &_resize_lock;\n+}\n+\n+void ZWorkers::request_resize_workers(uint nworkers) {\n+  assert(nworkers != 0, \"Never ask for zero workers\");\n+\n+  ZLocker<ZLock> locker(&_resize_lock);\n+\n+  if (_requested_nworkers == nworkers) {\n+    \/\/ Already requested\n+    return;\n+  }\n+\n+  if (_workers.active_workers() == nworkers) {\n+    \/\/ Already the right amount of threads\n+    return;\n+  }\n+\n+  log_info(gc, task)(\"Adjusting Workers for %s Generation: %u -> %u\",\n+                     _generation_name, _workers.active_workers(), nworkers);\n+\n+  _requested_nworkers = nworkers;\n+}\n","filename":"src\/hotspot\/share\/gc\/z\/zWorkers.cpp","additions":99,"deletions":49,"binary":false,"changes":148,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,0 +28,3 @@\n+#include \"gc\/z\/zGenerationId.hpp\"\n+#include \"gc\/z\/zLock.hpp\"\n+#include \"gc\/z\/zStat.hpp\"\n@@ -30,0 +33,3 @@\n+class ZRestartableTask;\n+class ZStatCycle;\n+class ZStatWorkers;\n@@ -34,1 +40,6 @@\n-  WorkerThreads _workers;\n+  WorkerThreads       _workers;\n+  const char* const   _generation_name;\n+  ZLock               _resize_lock;\n+  volatile uint       _requested_nworkers;\n+  bool                _is_active;\n+  ZStatWorkers* const _stats;\n@@ -37,1 +48,1 @@\n-  ZWorkers();\n+  ZWorkers(ZGenerationId id, ZStatWorkers* stats);\n@@ -39,0 +50,1 @@\n+  bool is_active() const;\n@@ -41,0 +53,2 @@\n+  void set_active();\n+  void set_inactive();\n@@ -43,0 +57,1 @@\n+  void run(ZRestartableTask* task);\n@@ -46,0 +61,6 @@\n+\n+  \/\/ Worker resizing\n+  ZLock* resizing_lock();\n+  void request_resize_workers(uint nworkers);\n+\n+  bool should_worker_resize();\n","filename":"src\/hotspot\/share\/gc\/z\/zWorkers.hpp","additions":24,"deletions":3,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -0,0 +1,35 @@\n+\/*\n+ * Copyright (c) 2021, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_Z_ZWORKERS_INLINE_HPP\n+#define SHARE_GC_Z_ZWORKERS_INLINE_HPP\n+\n+#include \"gc\/z\/zWorkers.hpp\"\n+\n+#include \"runtime\/atomic.hpp\"\n+\n+inline bool ZWorkers::should_worker_resize() {\n+  return Atomic::load(&_requested_nworkers) != 0;\n+}\n+\n+#endif \/\/ SHARE_GC_Z_ZWORKERS_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/z\/zWorkers.inline.hpp","additions":35,"deletions":0,"binary":false,"changes":35,"status":"added"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,0 +27,2 @@\n+#include \"zPageAge.hpp\"\n+\n@@ -35,5 +37,2 @@\n-  product(double, ZAllocationSpikeTolerance, 2.0,                           \\\n-          \"Allocation spike tolerance factor\")                              \\\n-                                                                            \\\n-  product(double, ZFragmentationLimit, 25.0,                                \\\n-          \"Maximum allowed heap fragmentation\")                             \\\n+  product(double, ZYoungCompactionLimit, 25.0,                              \\\n+          \"Maximum allowed garbage in young pages\")                         \\\n@@ -41,3 +40,2 @@\n-  product(size_t, ZMarkStackSpaceLimit, 8*G,                                \\\n-          \"Maximum number of bytes allocated for mark stacks\")              \\\n-          range(32*M, 1024*G)                                               \\\n+  product(double, ZCollectionIntervalMinor, -1,                             \\\n+          \"Force Minor GC at a fixed time interval (in seconds)\")           \\\n@@ -45,1 +43,1 @@\n-  product(double, ZCollectionInterval, 0,                                   \\\n+  product(double, ZCollectionIntervalMajor, -1,                             \\\n@@ -48,9 +46,2 @@\n-  product(bool, ZProactive, true,                                           \\\n-          \"Enable proactive GC cycles\")                                     \\\n-                                                                            \\\n-  product(bool, ZUncommit, true,                                            \\\n-          \"Uncommit unused memory\")                                         \\\n-                                                                            \\\n-  product(uintx, ZUncommitDelay, 5 * 60,                                    \\\n-          \"Uncommit memory if it has been unused for the specified \"        \\\n-          \"amount of time (in seconds)\")                                    \\\n+  product(bool, ZCollectionIntervalOnly, false,                             \\\n+          \"Only use timers for GC heuristics\")                              \\\n@@ -58,3 +49,2 @@\n-  product(uint, ZStatisticsInterval, 10, DIAGNOSTIC,                        \\\n-          \"Time between statistics print outs (in seconds)\")                \\\n-          range(1, (uint)-1)                                                \\\n+  product(bool, ZBufferStoreBarriers, true, DIAGNOSTIC,                     \\\n+          \"Buffer store barriers\")                                          \\\n@@ -62,2 +52,2 @@\n-  product(bool, ZStressRelocateInPlace, false, DIAGNOSTIC,                  \\\n-          \"Always relocate pages in-place\")                                 \\\n+  product(uint, ZYoungGCThreads, 0, DIAGNOSTIC,                             \\\n+          \"Number of GC threads for the young generation\")                  \\\n@@ -65,2 +55,2 @@\n-  product(bool, ZVerifyViews, false, DIAGNOSTIC,                            \\\n-          \"Verify heap view accesses\")                                      \\\n+  product(uint, ZOldGCThreads, 0, DIAGNOSTIC,                               \\\n+          \"Number of GC threads for the old generation\")                    \\\n@@ -68,2 +58,4 @@\n-  product(bool, ZVerifyRoots, trueInDebug, DIAGNOSTIC,                      \\\n-          \"Verify roots\")                                                   \\\n+  product(uintx, ZIndexDistributorStrategy, 0, DIAGNOSTIC,                  \\\n+          \"Strategy used to distribute indices to parallel workers \"        \\\n+          \"0: Claim tree \"                                                  \\\n+          \"1: Simple Striped \")                                             \\\n@@ -71,2 +63,2 @@\n-  product(bool, ZVerifyObjects, false, DIAGNOSTIC,                          \\\n-          \"Verify objects\")                                                 \\\n+  product(bool, ZVerifyRemembered, trueInDebug, DIAGNOSTIC,                 \\\n+          \"Verify remembered sets\")                                         \\\n@@ -74,2 +66,2 @@\n-  product(bool, ZVerifyMarking, trueInDebug, DIAGNOSTIC,                    \\\n-          \"Verify marking stacks\")                                          \\\n+  develop(bool, ZVerifyOops, false,                                         \\\n+          \"Verify accessed oops\")                                           \\\n@@ -77,2 +69,3 @@\n-  product(bool, ZVerifyForwarding, false, DIAGNOSTIC,                       \\\n-          \"Verify forwarding tables\")\n+  product(int, ZTenuringThreshold, -1, DIAGNOSTIC,                          \\\n+          \"Young generation tenuring threshold, -1 for dynamic computation\")\\\n+          range(-1, static_cast<int>(ZPageAgeMax))\n","filename":"src\/hotspot\/share\/gc\/z\/z_globals.hpp","additions":27,"deletions":34,"binary":false,"changes":61,"status":"modified"},{"patch":"@@ -336,1 +336,1 @@\n-  <Event name=\"GarbageCollection\" category=\"Java Virtual Machine, GC, Collector\" label=\"Garbage Collection\" description=\"Garbage collection performed by the JVM\">\n+  <Event name=\"GarbageCollection\" category=\"Java Virtual Machine, GC, Collector\" label=\"Garbage Collection\" description=\"Garbage collection performed by the JVM\" thread=\"true\">\n@@ -542,0 +542,5 @@\n+  <Event name=\"GCPhaseConcurrentLevel2\" category=\"Java Virtual Machine, GC, Phases\" label=\"GC Phase Concurrent Level 2\" thread=\"true\">\n+    <Field type=\"uint\" name=\"gcId\" label=\"GC Identifier\" relation=\"GcId\" \/>\n+    <Field type=\"string\" name=\"name\" label=\"Name\" \/>\n+  <\/Event>\n+\n@@ -1074,0 +1079,9 @@\n+  <Event name=\"ZYoungGarbageCollection\" category=\"Java Virtual Machine, GC, Collector\" label=\"ZGC Young Garbage Collection\" description=\"Extra information specific to ZGC Young Garbage Collections\" thread=\"true\">\n+    <Field type=\"uint\" name=\"gcId\" label=\"GC Identifier\" relation=\"GcId\" \/>\n+    <Field type=\"uint\" name=\"tenuringThreshold\" label=\"Tenuring Threshold\" \/>\n+  <\/Event>\n+\n+  <Event name=\"ZOldGarbageCollection\" category=\"Java Virtual Machine, GC, Collector\" label=\"ZGC Old Garbage Collection\" description=\"Extra information specific to ZGC Old Garbage Collections\" thread=\"true\">\n+    <Field type=\"uint\" name=\"gcId\" label=\"GC Identifier\" relation=\"GcId\" \/>\n+  <\/Event>\n+\n@@ -1096,1 +1110,1 @@\n-    <Field type=\"ulong\" name=\"pages\" label=\"Pages\" \/>\n+    <Field type=\"ulong\" name=\"candidatePages\" label=\"Candidate Pages\" \/>\n@@ -1099,0 +1113,1 @@\n+    <Field type=\"ulong\" name=\"selectedPages\" label=\"Selected Pages\" \/>\n","filename":"src\/hotspot\/share\/jfr\/metadata\/metadata.xml","additions":17,"deletions":2,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -80,0 +80,1 @@\n+  LOG_PREFIX(GCId::print_prefix, LOG_TAGS(gc, page)) \\\n","filename":"src\/hotspot\/share\/logging\/logPrefix.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -140,0 +140,1 @@\n+  LOG_TAG(page) \\\n","filename":"src\/hotspot\/share\/logging\/logTag.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -207,1 +207,0 @@\n-\n@@ -213,0 +212,5 @@\n+class OopFieldClosure {\n+public:\n+  virtual void do_field(oop base, oop* p) = 0;\n+};\n+\n","filename":"src\/hotspot\/share\/memory\/iterator.hpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -1219,0 +1219,5 @@\n+void Universe::set_verify_data(uintptr_t mask, uintptr_t bits) {\n+  _verify_oop_mask = mask;\n+  _verify_oop_bits = bits;\n+}\n+\n","filename":"src\/hotspot\/share\/memory\/universe.cpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -211,0 +211,1 @@\n+  static void set_verify_data(uintptr_t mask, uintptr_t bits) PRODUCT_RETURN;\n","filename":"src\/hotspot\/share\/memory\/universe.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -193,1 +193,2 @@\n-void oopDesc::obj_field_put_raw(int offset, oop value)                { RawAccess<>::oop_store_at(as_oop(), offset, value); }\n+void oopDesc::obj_field_put_raw(int offset, oop value)                { assert(!(UseZGC && ZGenerational), \"Generational ZGC must use store barriers\");\n+                                                                        RawAccess<>::oop_store_at(as_oop(), offset, value); }\n","filename":"src\/hotspot\/share\/oops\/oop.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -76,0 +76,1 @@\n+  static inline void release_set_mark(HeapWord* mem, markWord m);\n","filename":"src\/hotspot\/share\/oops\/oop.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -69,0 +69,4 @@\n+void oopDesc::release_set_mark(HeapWord* mem, markWord m) {\n+  Atomic::release_store((markWord*)(((char*)mem) + mark_offset_in_bytes()), m);\n+}\n+\n","filename":"src\/hotspot\/share\/oops\/oop.inline.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -33,0 +33,2 @@\n+CheckOopFunctionPointer check_oop_function = nullptr;\n+\n","filename":"src\/hotspot\/share\/oops\/oopsHierarchy.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -80,0 +80,5 @@\n+\/\/ Extra verification when creating and using oops.\n+\/\/ Used to catch broken oops as soon as possible.\n+using CheckOopFunctionPointer = void(*)(oopDesc*);\n+extern CheckOopFunctionPointer check_oop_function;\n+\n@@ -86,3 +91,6 @@\n-  void register_if_checking() {\n-    if (CheckUnhandledOops) register_oop();\n-  }\n+  \/\/ Extra verification of the oop\n+  void check_oop() const { if (check_oop_function != nullptr && _o != nullptr) check_oop_function(_o); }\n+\n+  void on_usage() const  { check_oop(); }\n+  void on_construction() { check_oop(); if (CheckUnhandledOops)   register_oop(); }\n+  void on_destruction()  {              if (CheckUnhandledOops) unregister_oop(); }\n@@ -91,3 +99,3 @@\n-  oop()             : _o(nullptr) { register_if_checking(); }\n-  oop(const oop& o) : _o(o._o)    { register_if_checking(); }\n-  oop(oopDesc* o)   : _o(o)       { register_if_checking(); }\n+  oop()             : _o(nullptr) { on_construction(); }\n+  oop(const oop& o) : _o(o._o)    { on_construction(); }\n+  oop(oopDesc* o)   : _o(o)       { on_construction(); }\n@@ -95,1 +103,1 @@\n-    if (CheckUnhandledOops) unregister_oop();\n+    on_destruction();\n@@ -98,3 +106,1 @@\n-  oopDesc* obj() const                 { return _o; }\n-  oopDesc* operator->() const          { return _o; }\n-  operator oopDesc* () const           { return _o; }\n+  oopDesc* obj() const                  { on_usage(); return _o; }\n@@ -102,2 +108,2 @@\n-  bool operator==(const oop& o) const  { return _o == o._o; }\n-  bool operator!=(const oop& o) const  { return _o != o._o; }\n+  oopDesc* operator->() const           { return obj(); }\n+  operator oopDesc* () const            { return obj(); }\n@@ -105,2 +111,2 @@\n-  bool operator==(std::nullptr_t) const     { return _o == nullptr; }\n-  bool operator!=(std::nullptr_t) const     { return _o != nullptr; }\n+  bool operator==(const oop& o) const   { return obj() == o.obj(); }\n+  bool operator!=(const oop& o) const   { return obj() != o.obj(); }\n@@ -108,1 +114,4 @@\n-  oop& operator=(const oop& o)         { _o = o._o; return *this; }\n+  bool operator==(std::nullptr_t) const { return obj() == nullptr; }\n+  bool operator!=(std::nullptr_t) const { return obj() != nullptr; }\n+\n+  oop& operator=(const oop& o)          { _o = o.obj(); return *this; }\n@@ -163,0 +172,4 @@\n+inline intptr_t p2i(narrowOop o) {\n+  return static_cast<intptr_t>(o);\n+}\n+\n","filename":"src\/hotspot\/share\/oops\/oopsHierarchy.hpp","additions":28,"deletions":15,"binary":false,"changes":43,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2021, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2021, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -33,0 +33,1 @@\n+#include \"gc\/shared\/gc_globals.hpp\"\n@@ -39,0 +40,1 @@\n+#include \"runtime\/globals.hpp\"\n@@ -89,2 +91,0 @@\n-inline oop stackChunkOopDesc::cont() const                { return UseCompressedOops ? cont<narrowOop>() : cont<oop>(); \/* jdk_internal_vm_StackChunk::cont(as_oop()); *\/ }\n-template<typename P>\n@@ -92,8 +92,12 @@\n-  \/\/ The state of the cont oop is used by ZCollectedHeap::requires_barriers,\n-  \/\/ to determine the age of the stackChunkOopDesc. For that to work, it is\n-  \/\/ only the GC that is allowed to perform a load barrier on the oop.\n-  \/\/ This function is used by non-GC code and therfore create a stack-local\n-  \/\/ copy on the oop and perform the load barrier on that copy instead.\n-  oop obj = jdk_internal_vm_StackChunk::cont_raw<P>(as_oop());\n-  obj = (oop)NativeAccess<>::oop_load(&obj);\n-  return obj;\n+  if (UseZGC && !ZGenerational) {\n+    assert(!UseCompressedOops, \"Non-generational ZGC does not support compressed oops\");\n+    \/\/ The state of the cont oop is used by XCollectedHeap::requires_barriers,\n+    \/\/ to determine the age of the stackChunkOopDesc. For that to work, it is\n+    \/\/ only the GC that is allowed to perform a load barrier on the oop.\n+    \/\/ This function is used by non-GC code and therfore create a stack-local\n+    \/\/ copy on the oop and perform the load barrier on that copy instead.\n+    oop obj = jdk_internal_vm_StackChunk::cont_raw<oop>(as_oop());\n+    obj = (oop)NativeAccess<>::oop_load(&obj);\n+    return obj;\n+  }\n+  return jdk_internal_vm_StackChunk::cont(as_oop());\n","filename":"src\/hotspot\/share\/oops\/stackChunkOop.inline.hpp","additions":15,"deletions":11,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -191,0 +191,2 @@\n+  BufferSizingData* buffer_sizing_data()        { return &_buf_sizes; }\n+\n","filename":"src\/hotspot\/share\/opto\/output.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -112,0 +112,3 @@\n+#if INCLUDE_ZGC\n+#include \"gc\/z\/zAddress.inline.hpp\"\n+#endif \/\/ INCLUDE_ZGC\n@@ -400,1 +403,5 @@\n-    return Universe::heap()->is_in(p);\n+    if (ZGenerational) {\n+      return ZHeap::heap()->is_old(to_zaddress(p));\n+    } else {\n+      return Universe::heap()->is_in(p);\n+    }\n","filename":"src\/hotspot\/share\/prims\/whitebox.cpp","additions":8,"deletions":1,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -69,0 +69,3 @@\n+#if INCLUDE_ZGC\n+#include \"gc\/z\/zStackChunkGCData.inline.hpp\"\n+#endif\n@@ -1397,1 +1400,4 @@\n- if (UseZGC) {\n+  if (UseZGC) {\n+    if (ZGenerational) {\n+      ZStackChunkGCData::initialize(chunk);\n+    }\n@@ -1403,2 +1409,1 @@\n-if (UseShenandoahGC) {\n-\n+  if (UseShenandoahGC) {\n","filename":"src\/hotspot\/share\/runtime\/continuationFreezeThaw.cpp","additions":8,"deletions":3,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2022, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -127,0 +127,1 @@\n+  static inline oop cont(oop chunk);\n","filename":"src\/hotspot\/share\/runtime\/continuationJavaClasses.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -84,0 +84,4 @@\n+inline oop jdk_internal_vm_StackChunk::cont(oop chunk) {\n+  return chunk->obj_field(_cont_offset);\n+}\n+\n","filename":"src\/hotspot\/share\/runtime\/continuationJavaClasses.inline.hpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -177,10 +177,0 @@\n-\/\/ a weaker assertion than the above\n-void assert_locked_or_safepoint_weak(const Mutex* lock) {\n-  if (DebuggingContext::is_enabled() || VMError::is_error_reported()) return;\n-  assert(lock != nullptr, \"Need non-null lock\");\n-  if (lock->is_locked()) return;\n-  if (SafepointSynchronize::is_at_safepoint()) return;\n-  if (!Universe::is_fully_initialized()) return;\n-  fatal(\"must own lock %s\", lock->name());\n-}\n-\n","filename":"src\/hotspot\/share\/runtime\/mutexLocker.cpp","additions":0,"deletions":10,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -183,1 +183,0 @@\n-void assert_locked_or_safepoint_weak(const Mutex* lock);\n@@ -187,1 +186,0 @@\n-#define assert_locked_or_safepoint_weak(lock)\n","filename":"src\/hotspot\/share\/runtime\/mutexLocker.hpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -62,0 +62,2 @@\n+  assert(sp != 0, \"Sanity check\");\n+\n@@ -281,0 +283,4 @@\n+uintptr_t StackWatermark::last_processed_raw() {\n+  return _iterator->caller();\n+}\n+\n","filename":"src\/hotspot\/share\/runtime\/stackWatermark.cpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -140,0 +140,1 @@\n+  uintptr_t last_processed_raw();\n","filename":"src\/hotspot\/share\/runtime\/stackWatermark.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -75,0 +75,1 @@\n+  DEBUG_ONLY(clear_indirectly_suspendible_thread();)\n@@ -101,0 +102,1 @@\n+  _vm_error_callbacks = nullptr;\n","filename":"src\/hotspot\/share\/runtime\/thread.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -58,0 +58,1 @@\n+class VMErrorCallback;\n@@ -107,0 +108,2 @@\n+  friend class VMError;\n+  friend class VMErrorCallbackMark;\n@@ -205,0 +208,1 @@\n+  DEBUG_ONLY(bool _indirectly_suspendible_thread;)\n@@ -216,7 +220,3 @@\n-  void set_suspendible_thread() {\n-    _suspendible_thread = true;\n-  }\n-\n-  void clear_suspendible_thread() {\n-    _suspendible_thread = false;\n-  }\n+  void set_suspendible_thread()   { _suspendible_thread = true; }\n+  void clear_suspendible_thread() { _suspendible_thread = false; }\n+  bool is_suspendible_thread()    { return _suspendible_thread; }\n@@ -224,1 +224,3 @@\n-  bool is_suspendible_thread() { return _suspendible_thread; }\n+  void set_indirectly_suspendible_thread()   { _indirectly_suspendible_thread = true; }\n+  void clear_indirectly_suspendible_thread() { _indirectly_suspendible_thread = false; }\n+  bool is_indirectly_suspendible_thread()    { return _indirectly_suspendible_thread; }\n@@ -636,0 +638,3 @@\n+\n+ private:\n+  VMErrorCallback* _vm_error_callbacks;\n","filename":"src\/hotspot\/share\/runtime\/thread.hpp","additions":13,"deletions":8,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -63,4 +63,13 @@\n-  template(ZMarkStart)                            \\\n-  template(ZMarkEnd)                              \\\n-  template(ZRelocateStart)                        \\\n-  template(ZVerify)                               \\\n+  template(ZMarkEndOld)                           \\\n+  template(ZMarkEndYoung)                         \\\n+  template(ZMarkFlushOperation)                   \\\n+  template(ZMarkStartYoung)                       \\\n+  template(ZMarkStartYoungAndOld)                 \\\n+  template(ZRelocateStartOld)                     \\\n+  template(ZRelocateStartYoung)                   \\\n+  template(ZRendezvousGCThreads)                  \\\n+  template(ZVerifyOld)                            \\\n+  template(XMarkStart)                            \\\n+  template(XMarkEnd)                              \\\n+  template(XRelocateStart)                        \\\n+  template(XVerify)                               \\\n","filename":"src\/hotspot\/share\/runtime\/vmOperation.hpp","additions":13,"deletions":4,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -1961,0 +1961,1 @@\n+  virtual bool doit_prologue();\n@@ -2138,0 +2139,11 @@\n+bool VM_HeapDumper::doit_prologue() {\n+  if (_gc_before_heap_dump && UseZGC) {\n+    \/\/ ZGC cannot perform a synchronous GC cycle from within the VM thread.\n+    \/\/ So ZCollectedHeap::collect_as_vm_thread() is a noop. To respect the\n+    \/\/ _gc_before_heap_dump flag a synchronous GC cycle is performed from\n+    \/\/ the caller thread in the prologue.\n+    Universe::heap()->collect(GCCause::_heap_dump);\n+  }\n+  return VM_GC_Operation::doit_prologue();\n+}\n+\n","filename":"src\/hotspot\/share\/services\/heapDumper.cpp","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -39,0 +39,1 @@\n+StringEventLog* Events::_zgc_phase_switch = nullptr;\n@@ -98,0 +99,1 @@\n+    _zgc_phase_switch = new StringEventLog(\"ZGC Phase Switch\", \"zgcps\");\n","filename":"src\/hotspot\/share\/utilities\/events.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -226,0 +226,3 @@\n+    \/\/ A log for ZGC phase switches\n+  static StringEventLog* _zgc_phase_switch;\n+\n@@ -261,0 +264,2 @@\n+  static void log_zgc_phase_switch(const char* format, ...) ATTRIBUTE_PRINTF(1, 2);\n+\n@@ -297,0 +302,9 @@\n+inline void Events::log_zgc_phase_switch(const char* format, ...) {\n+  if (LogEvents && _zgc_phase_switch != nullptr) {\n+    va_list ap;\n+    va_start(ap, format);\n+    _zgc_phase_switch->logv(nullptr \/* thread *\/, format, ap);\n+    va_end(ap);\n+  }\n+}\n+\n@@ -471,0 +485,1 @@\n+ public:\n","filename":"src\/hotspot\/share\/utilities\/events.hpp","additions":15,"deletions":0,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -951,0 +951,8 @@\n+  STEP_IF(\"printing registered callbacks\", _verbose && _thread != nullptr);\n+    for (VMErrorCallback* callback = _thread->_vm_error_callbacks;\n+        callback != nullptr;\n+        callback = callback->_next) {\n+      callback->call(st);\n+      st->cr();\n+    }\n+\n@@ -1899,0 +1907,11 @@\n+\n+VMErrorCallbackMark::VMErrorCallbackMark(VMErrorCallback* callback)\n+  : _thread(Thread::current()) {\n+  callback->_next = _thread->_vm_error_callbacks;\n+  _thread->_vm_error_callbacks = callback;\n+}\n+\n+VMErrorCallbackMark::~VMErrorCallbackMark() {\n+  assert(_thread->_vm_error_callbacks != nullptr, \"Popped too far\");\n+  _thread->_vm_error_callbacks = _thread->_vm_error_callbacks->_next;\n+}\n","filename":"src\/hotspot\/share\/utilities\/vmError.cpp","additions":19,"deletions":0,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -216,0 +216,23 @@\n+\n+class VMErrorCallback {\n+  friend class VMError;\n+  friend class VMErrorCallbackMark;\n+\n+  \/\/ Link through all callbacks active on a thread\n+  VMErrorCallback* _next;\n+\n+  \/\/ Called by VMError reporting\n+  virtual void call(outputStream* st) = 0;\n+\n+public:\n+  VMErrorCallback() : _next(nullptr) {}\n+};\n+\n+class VMErrorCallbackMark : public StackObj {\n+  Thread* _thread;\n+\n+public:\n+  VMErrorCallbackMark(VMErrorCallback* callback);\n+  ~VMErrorCallbackMark();\n+};\n+\n","filename":"src\/hotspot\/share\/utilities\/vmError.hpp","additions":23,"deletions":0,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -41,0 +41,1 @@\n+import sun.jvm.hotspot.gc.x.*;\n@@ -1130,0 +1131,4 @@\n+                        } else if (collHeap instanceof XCollectedHeap) {\n+                          XCollectedHeap heap = (XCollectedHeap) collHeap;\n+                          anno = \"ZHeap \";\n+                          bad = false;\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/HSDB.java","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -0,0 +1,77 @@\n+\/*\n+ * Copyright (c) 2018, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+package sun.jvm.hotspot.gc.x;\n+\n+import sun.jvm.hotspot.debugger.Address;\n+import sun.jvm.hotspot.runtime.VM;\n+\n+class XAddress {\n+    static long as_long(Address value) {\n+        if (value == null) {\n+            return 0;\n+        }\n+        return value.asLongValue();\n+    };\n+\n+    static boolean is_null(Address value) {\n+        return value == null;\n+    }\n+\n+    static boolean is_weak_bad(Address value) {\n+        return (as_long(value) & XGlobals.XAddressWeakBadMask()) != 0L;\n+    }\n+\n+    static boolean is_weak_good(Address value) {\n+        return !is_weak_bad(value) && !is_null(value);\n+    }\n+\n+    static boolean is_weak_good_or_null(Address value) {\n+        return !is_weak_bad(value);\n+    }\n+\n+    static long offset(Address address) {\n+        return as_long(address) & XGlobals.XAddressOffsetMask();\n+    }\n+\n+    static Address good(Address value) {\n+        return VM.getVM().getDebugger().newAddress(offset(value) | XGlobals.XAddressGoodMask());\n+    }\n+\n+    static Address good_or_null(Address value) {\n+        return is_null(value) ? value : good(value);\n+    }\n+\n+    private static boolean isPowerOf2(long value) {\n+        return (value != 0L) && ((value & (value - 1)) == 0L);\n+    }\n+\n+    static boolean isIn(Address addr) {\n+        long value = as_long(addr);\n+        if (!isPowerOf2(value & ~XGlobals.XAddressOffsetMask())) {\n+            return false;\n+        }\n+        return (value & (XGlobals.XAddressMetadataMask() & ~XGlobals.XAddressMetadataFinalizable())) != 0L;\n+    }\n+}\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/gc\/x\/XAddress.java","additions":77,"deletions":0,"binary":false,"changes":77,"status":"added"},{"patch":"@@ -0,0 +1,71 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2021, NTT DATA.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+package sun.jvm.hotspot.gc.x;\n+\n+import sun.jvm.hotspot.debugger.Address;\n+import sun.jvm.hotspot.runtime.VM;\n+import sun.jvm.hotspot.runtime.VMObject;\n+import sun.jvm.hotspot.runtime.VMObjectFactory;\n+import sun.jvm.hotspot.types.CIntegerField;\n+import sun.jvm.hotspot.types.Type;\n+import sun.jvm.hotspot.types.TypeDataBase;\n+\n+public class XAttachedArrayForForwarding extends VMObject {\n+    private static CIntegerField lengthField;\n+\n+    static {\n+        VM.registerVMInitializedObserver((o, d) -> initialize(VM.getVM().getTypeDataBase()));\n+    }\n+\n+    private static synchronized void initialize(TypeDataBase db) {\n+        Type type = db.lookupType(\"XAttachedArrayForForwarding\");\n+\n+        lengthField = type.getCIntegerField(\"_length\");\n+    }\n+\n+    public XAttachedArrayForForwarding(Address addr) {\n+        super(addr);\n+    }\n+\n+    public long length() {\n+        return lengthField.getValue(addr);\n+    }\n+\n+    \/\/ ObjectT: XForwarding\n+    \/\/  ArrayT: XForwardingEntry\n+    \/\/\n+    \/\/ template <typename ObjectT, typename ArrayT>\n+    \/\/ inline size_t XAttachedArray<ObjectT, ArrayT>::object_size()\n+    private long objectSize() {\n+        return XUtils.alignUp(XForwarding.getSize(), XForwardingEntry.getSize());\n+    }\n+\n+    \/\/ ArrayT* operator()(const ObjectT* obj) const\n+    public XForwardingEntry get(XForwarding obj) {\n+        Address o = obj.getAddress().addOffsetTo(objectSize());\n+        return VMObjectFactory.newObject(XForwardingEntry.class, o);\n+    }\n+}\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/gc\/x\/XAttachedArrayForForwarding.java","additions":71,"deletions":0,"binary":false,"changes":71,"status":"added"},{"patch":"@@ -0,0 +1,71 @@\n+\/*\n+ * Copyright (c) 2018, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+package sun.jvm.hotspot.gc.x;\n+\n+import sun.jvm.hotspot.debugger.Address;\n+import sun.jvm.hotspot.runtime.VM;\n+\n+class XBarrier {\n+    private static boolean is_weak_good_or_null_fast_path(Address addr) {\n+        return XAddress.is_weak_good_or_null(addr);\n+    }\n+\n+    private static Address weak_load_barrier_on_oop_slow_path(Address addr) {\n+        return XAddress.is_weak_good(addr) ? XAddress.good(addr) : relocate_or_remap(addr);\n+    }\n+\n+    private static boolean during_relocate() {\n+        return XGlobals.XGlobalPhase() == XGlobals.XPhaseRelocate;\n+    }\n+\n+    private static Address relocate(Address addr) {\n+        return zheap().relocate_object(addr);\n+    }\n+\n+    private static XHeap zheap() {\n+        XCollectedHeap zCollectedHeap = (XCollectedHeap)VM.getVM().getUniverse().heap();\n+        return zCollectedHeap.heap();\n+    }\n+\n+    private static Address remap(Address addr) {\n+        return zheap().remapObject(addr);\n+    }\n+\n+    private static Address relocate_or_remap(Address addr) {\n+        return during_relocate() ? relocate(addr) : remap(addr);\n+    }\n+\n+    static Address weak_barrier(Address o) {\n+        \/\/ Fast path\n+        if (is_weak_good_or_null_fast_path(o)) {\n+            \/\/ Return the good address instead of the weak good address\n+            \/\/ to ensure that the currently active heap view is used.\n+            return XAddress.good_or_null(o);\n+        }\n+\n+        \/\/ Slow path\n+        return weak_load_barrier_on_oop_slow_path(o);\n+    }\n+}\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/gc\/x\/XBarrier.java","additions":71,"deletions":0,"binary":false,"changes":71,"status":"added"},{"patch":"@@ -0,0 +1,139 @@\n+\/*\n+ * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+package sun.jvm.hotspot.gc.x;\n+\n+import java.io.PrintStream;\n+import java.util.Iterator;\n+\n+import sun.jvm.hotspot.debugger.Address;\n+import sun.jvm.hotspot.debugger.OopHandle;\n+import sun.jvm.hotspot.gc.shared.CollectedHeap;\n+import sun.jvm.hotspot.gc.shared.CollectedHeapName;\n+import sun.jvm.hotspot.gc.shared.LiveRegionsClosure;\n+import sun.jvm.hotspot.runtime.VM;\n+import sun.jvm.hotspot.runtime.VMObjectFactory;\n+import sun.jvm.hotspot.types.Type;\n+import sun.jvm.hotspot.types.TypeDataBase;\n+import sun.jvm.hotspot.utilities.BitMapInterface;\n+\n+\/\/ Mirror class for XCollectedHeap.\n+\n+public class XCollectedHeap extends CollectedHeap {\n+    private static long zHeapFieldOffset;\n+\n+    static {\n+        VM.registerVMInitializedObserver((o, d) -> initialize(VM.getVM().getTypeDataBase()));\n+    }\n+\n+    private static synchronized void initialize(TypeDataBase db) {\n+        Type type = db.lookupType(\"XCollectedHeap\");\n+\n+        zHeapFieldOffset = type.getAddressField(\"_heap\").getOffset();\n+    }\n+\n+    public XHeap heap() {\n+        Address heapAddr = addr.addOffsetTo(zHeapFieldOffset);\n+        return VMObjectFactory.newObject(XHeap.class, heapAddr);\n+    }\n+\n+    @Override\n+    public CollectedHeapName kind() {\n+        return CollectedHeapName.Z;\n+    }\n+\n+    @Override\n+    public void printOn(PrintStream tty) {\n+        heap().printOn(tty);\n+    }\n+\n+    public XCollectedHeap(Address addr) {\n+        super(addr);\n+    }\n+\n+    @Override\n+    public long capacity() {\n+        return heap().capacity();\n+    }\n+\n+    @Override\n+    public long used() {\n+        return heap().used();\n+    }\n+\n+    @Override\n+    public boolean isInReserved(Address a) {\n+        return heap().isIn(a);\n+    }\n+\n+    private OopHandle oop_load_barrier(Address oopAddress) {\n+        oopAddress = XBarrier.weak_barrier(oopAddress);\n+        if (oopAddress == null) {\n+            return null;\n+        }\n+\n+        return oopAddress.addOffsetToAsOopHandle(0);\n+    }\n+\n+    @Override\n+    public OopHandle oop_load_at(OopHandle handle, long offset) {\n+        assert(!VM.getVM().isCompressedOopsEnabled());\n+\n+        Address oopAddress = handle.getAddressAt(offset);\n+\n+        return oop_load_barrier(oopAddress);\n+    }\n+\n+    \/\/ addr can be either in heap or in native\n+    @Override\n+    public OopHandle oop_load_in_native(Address addr) {\n+        Address oopAddress = addr.getAddressAt(0);\n+        return oop_load_barrier(oopAddress);\n+    }\n+\n+    public String oopAddressDescription(OopHandle handle) {\n+        Address origOop = XOop.to_address(handle);\n+        Address loadBarrieredOop = XBarrier.weak_barrier(origOop);\n+        if (!origOop.equals(loadBarrieredOop)) {\n+            return origOop + \" (\" + loadBarrieredOop.toString() + \")\";\n+        } else {\n+            return handle.toString();\n+        }\n+    }\n+\n+    @Override\n+    public void liveRegionsIterate(LiveRegionsClosure closure) {\n+        Iterator<XPage> iter = heap().pageTable().activePagesIterator();\n+        while (iter.hasNext()) {\n+            XPage page = iter.next();\n+            closure.doLiveRegions(page);\n+        }\n+    }\n+\n+    @Override\n+    public BitMapInterface createBitMap(long size) {\n+        \/\/ Ignores the size\n+        return new XExternalBitMap(this);\n+    }\n+}\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/gc\/x\/XCollectedHeap.java","additions":139,"deletions":0,"binary":false,"changes":139,"status":"added"},{"patch":"@@ -0,0 +1,111 @@\n+\/*\n+ * Copyright (c) 2019, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+package sun.jvm.hotspot.gc.x;\n+\n+import java.util.HashMap;\n+\n+import sun.jvm.hotspot.runtime.VM;\n+import sun.jvm.hotspot.utilities.BitMap;\n+import sun.jvm.hotspot.utilities.BitMapInterface;\n+\n+\/** Discontiguous bitmap for ZGC. *\/\n+public class XExternalBitMap implements BitMapInterface {\n+    private XPageTable pageTable;\n+    private final long oopSize;\n+\n+    private HashMap<XPage, BitMap> pageToBitMap = new HashMap<XPage, BitMap>();\n+\n+    public XExternalBitMap(XCollectedHeap collectedHeap) {\n+        pageTable = collectedHeap.heap().pageTable();\n+        oopSize = VM.getVM().getOopSize();\n+    }\n+\n+    private XPage getPage(long zOffset) {\n+        if (zOffset > XGlobals.XAddressOffsetMask()) {\n+            throw new RuntimeException(\"Not a Z offset: \" + zOffset);\n+        }\n+\n+        XPage page = pageTable.get(XUtils.longToAddress(zOffset));\n+        if (page == null) {\n+            throw new RuntimeException(\"Address not in pageTable: \" + zOffset);\n+        }\n+        return page;\n+    }\n+\n+    private BitMap getOrAddBitMap(XPage page) {\n+        BitMap bitMap = pageToBitMap.get(page);\n+        if (bitMap == null) {\n+            long size = page.size();\n+\n+            long maxNumObjects = size >>> page.object_alignment_shift();\n+            if (maxNumObjects > Integer.MAX_VALUE) {\n+                throw new RuntimeException(\"int overflow\");\n+            }\n+            int intMaxNumObjects = (int)maxNumObjects;\n+\n+            bitMap = new BitMap(intMaxNumObjects);\n+            pageToBitMap.put(page,  bitMap);\n+        }\n+\n+        return bitMap;\n+    }\n+\n+    private int pageLocalBitMapIndex(XPage page, long zOffset) {\n+        long pageLocalZOffset = zOffset - page.start();\n+        return (int)(pageLocalZOffset >>> page.object_alignment_shift());\n+    }\n+\n+    private long convertToZOffset(long offset) {\n+        long addr = oopSize * offset;\n+        return addr & XGlobals.XAddressOffsetMask();\n+    }\n+\n+    @Override\n+    public boolean at(long offset) {\n+        long zOffset = convertToZOffset(offset);\n+        XPage page = getPage(zOffset);\n+        BitMap bitMap = getOrAddBitMap(page);\n+        int index = pageLocalBitMapIndex(page, zOffset);\n+\n+        return bitMap.at(index);\n+    }\n+\n+    @Override\n+    public void atPut(long offset, boolean value) {\n+        long zOffset = convertToZOffset(offset);\n+        XPage page = getPage(zOffset);\n+        BitMap bitMap = getOrAddBitMap(page);\n+        int index = pageLocalBitMapIndex(page, zOffset);\n+\n+        bitMap.atPut(index, value);\n+    }\n+\n+    @Override\n+    public void clear() {\n+        for (BitMap bitMap : pageToBitMap.values()) {\n+            bitMap.clear();\n+        }\n+    }\n+}\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/gc\/x\/XExternalBitMap.java","additions":111,"deletions":0,"binary":false,"changes":111,"status":"added"},{"patch":"@@ -0,0 +1,136 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2021, NTT DATA.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+package sun.jvm.hotspot.gc.x;\n+\n+import java.util.Iterator;\n+\n+import sun.jvm.hotspot.debugger.Address;\n+import sun.jvm.hotspot.runtime.VM;\n+import sun.jvm.hotspot.runtime.VMObject;\n+import sun.jvm.hotspot.runtime.VMObjectFactory;\n+import sun.jvm.hotspot.types.CIntegerField;\n+import sun.jvm.hotspot.types.Type;\n+import sun.jvm.hotspot.types.TypeDataBase;\n+\n+public class XForwarding extends VMObject {\n+    private static Type type;\n+    private static long virtualFieldOffset;\n+    private static long entriesFieldOffset;\n+    private static CIntegerField objectAlignmentShiftField;\n+    private static CIntegerField refCountField;\n+\n+    static {\n+        VM.registerVMInitializedObserver((o, d) -> initialize(VM.getVM().getTypeDataBase()));\n+    }\n+\n+    private static synchronized void initialize(TypeDataBase db) {\n+        type = db.lookupType(\"XForwarding\");\n+\n+        virtualFieldOffset = type.getField(\"_virtual\").getOffset();\n+        entriesFieldOffset = type.getField(\"_entries\").getOffset();\n+        objectAlignmentShiftField = type.getCIntegerField(\"_object_alignment_shift\");\n+        refCountField = type.getCIntegerField(\"_ref_count\");\n+    }\n+\n+    public XForwarding(Address addr) {\n+        super(addr);\n+    }\n+\n+    public static long getSize() {\n+        return type.getSize();\n+    }\n+\n+    private XVirtualMemory virtual() {\n+        return VMObjectFactory.newObject(XVirtualMemory.class, addr.addOffsetTo(virtualFieldOffset));\n+    }\n+\n+    private XAttachedArrayForForwarding entries() {\n+        return VMObjectFactory.newObject(XAttachedArrayForForwarding.class, addr.addOffsetTo(entriesFieldOffset));\n+    }\n+\n+    public long start() {\n+        return virtual().start();\n+    }\n+\n+    public int objectAlignmentShift() {\n+        return (int)objectAlignmentShiftField.getValue(addr);\n+    }\n+\n+    public boolean retainPage() {\n+        return refCountField.getValue(addr) > 0;\n+    }\n+\n+    private XForwardingEntry at(long cursor) {\n+        long offset = XForwardingEntry.getSize() * cursor;\n+        Address entryAddress = entries().get(this).getAddress().addOffsetTo(offset);\n+        return VMObjectFactory.newObject(XForwardingEntry.class, entryAddress);\n+    }\n+\n+    private class XForwardEntryIterator implements Iterator<XForwardingEntry> {\n+\n+        private long cursor;\n+\n+        private XForwardingEntry nextEntry;\n+\n+        public XForwardEntryIterator(long fromIndex) {\n+            long mask = entries().length() - 1;\n+            long hash = XHash.uint32_to_uint32(fromIndex);\n+            cursor = hash & mask;\n+            nextEntry = at(cursor);\n+        }\n+\n+        @Override\n+        public boolean hasNext() {\n+            return nextEntry.populated();\n+        }\n+\n+        @Override\n+        public XForwardingEntry next() {\n+            XForwardingEntry entry = nextEntry;\n+\n+            long mask = entries().length() - 1;\n+            cursor = (cursor + 1) & mask;\n+            nextEntry = at(cursor);\n+\n+            return entry;\n+        }\n+\n+        public XForwardingEntry peak() {\n+            return nextEntry;\n+        }\n+    }\n+\n+    public XForwardingEntry find(long fromIndex) {\n+        XForwardEntryIterator itr = new XForwardEntryIterator(fromIndex);\n+        while (itr.hasNext()) {\n+            XForwardingEntry entry = itr.next();\n+            if (entry.fromIndex() == fromIndex) {\n+                return entry;\n+            }\n+        }\n+        return itr.peak();\n+    }\n+}\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/gc\/x\/XForwarding.java","additions":136,"deletions":0,"binary":false,"changes":136,"status":"added"},{"patch":"@@ -0,0 +1,97 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2021, NTT DATA.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+package sun.jvm.hotspot.gc.x;\n+\n+import sun.jvm.hotspot.debugger.Address;\n+import sun.jvm.hotspot.runtime.VM;\n+import sun.jvm.hotspot.runtime.VMObject;\n+import sun.jvm.hotspot.runtime.VMObjectFactory;\n+import sun.jvm.hotspot.types.CIntegerField;\n+import sun.jvm.hotspot.types.Type;\n+import sun.jvm.hotspot.types.TypeDataBase;\n+\n+public class XForwardingEntry extends VMObject {\n+    private static Type type;\n+    private static CIntegerField entryField;\n+\n+    static {\n+        VM.registerVMInitializedObserver((o, d) -> initialize(VM.getVM().getTypeDataBase()));\n+    }\n+\n+    private static synchronized void initialize(TypeDataBase db) {\n+        type = db.lookupType(\"XForwardingEntry\");\n+\n+        entryField = type.getCIntegerField(\"_entry\");\n+    }\n+\n+    public static long getSize() {\n+        return type.getSize();\n+    }\n+\n+    public XForwardingEntry(Address addr) {\n+        super(addr);\n+    }\n+\n+    public long entry() {\n+        return entryField.getValue(addr);\n+    }\n+\n+    \/\/ typedef XBitField<uint64_t, bool,   0,   1> field_populated\n+    private boolean fieldPopulatedDecode(long value) {\n+        long FieldMask = (1L << 1) - 1;\n+        int FieldShift = 1;\n+        int ValueShift = 0;\n+        return (((value >>> FieldShift) & FieldMask) << ValueShift) != 0L;\n+    }\n+\n+    \/\/ typedef XBitField<uint64_t, size_t, 1,  45> field_to_offset;\n+    private long fieldToOffsetDecode(long value) {\n+        long FieldMask = (1L << 45) - 1;\n+        int FieldShift = 1;\n+        int ValueShift = 0;\n+        return ((value >>> FieldShift) & FieldMask) << ValueShift;\n+    }\n+\n+    \/\/ typedef XBitField<uint64_t, size_t, 46, 18> field_from_index;\n+    private long fieldFromIndexDecode(long value) {\n+        long FieldMask = (1L << 18) - 1;\n+        int FieldShift = 46;\n+        int ValueShift = 0;\n+        return ((value >>> FieldShift) & FieldMask) << ValueShift;\n+    }\n+\n+    public boolean populated() {\n+        return fieldPopulatedDecode(entry());\n+    }\n+\n+    public long toOffset() {\n+        return fieldToOffsetDecode(entry());\n+    }\n+\n+    public long fromIndex() {\n+        return fieldFromIndexDecode(entry());\n+    }\n+}\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/gc\/x\/XForwardingEntry.java","additions":97,"deletions":0,"binary":false,"changes":97,"status":"added"},{"patch":"@@ -0,0 +1,60 @@\n+\/*\n+ * Copyright (c) 2018, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+package sun.jvm.hotspot.gc.x;\n+\n+import sun.jvm.hotspot.debugger.Address;\n+import sun.jvm.hotspot.runtime.VM;\n+import sun.jvm.hotspot.runtime.VMObject;\n+import sun.jvm.hotspot.runtime.VMObjectFactory;\n+import sun.jvm.hotspot.types.AddressField;\n+import sun.jvm.hotspot.types.CIntegerField;\n+import sun.jvm.hotspot.types.Type;\n+import sun.jvm.hotspot.types.TypeDataBase;\n+\n+public class XForwardingTable extends VMObject {\n+    private static long mapFieldOffset;\n+\n+    static {\n+        VM.registerVMInitializedObserver((o, d) -> initialize(VM.getVM().getTypeDataBase()));\n+    }\n+\n+    private static synchronized void initialize(TypeDataBase db) {\n+        Type type = db.lookupType(\"XForwardingTable\");\n+\n+        mapFieldOffset = type.getAddressField(\"_map\").getOffset();\n+    }\n+\n+    public XForwardingTable(Address addr) {\n+        super(addr);\n+    }\n+\n+    private XGranuleMapForForwarding map() {\n+        return VMObjectFactory.newObject(XGranuleMapForForwarding.class, addr.addOffsetTo(mapFieldOffset));\n+    }\n+\n+    public XForwarding get(Address o) {\n+        return VMObjectFactory.newObject(XForwarding.class, map().get(XAddress.offset(o)));\n+    }\n+}\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/gc\/x\/XForwardingTable.java","additions":60,"deletions":0,"binary":false,"changes":60,"status":"added"},{"patch":"@@ -25,1 +25,1 @@\n-package sun.jvm.hotspot.gc.z;\n+package sun.jvm.hotspot.gc.x;\n@@ -27,1 +27,1 @@\n-class ZForwardingTableCursor {\n+class XForwardingTableCursor {\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/gc\/x\/XForwardingTableCursor.java","additions":2,"deletions":2,"binary":false,"changes":4,"previous_filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/gc\/z\/ZForwardingTableCursor.java","status":"renamed"},{"patch":"@@ -25,1 +25,1 @@\n-package sun.jvm.hotspot.gc.z;\n+package sun.jvm.hotspot.gc.x;\n@@ -29,1 +29,1 @@\n-class ZForwardingTableEntry {\n+class XForwardingTableEntry {\n@@ -32,1 +32,1 @@\n-    ZForwardingTableEntry(Address addr) {\n+    XForwardingTableEntry(Address addr) {\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/gc\/x\/XForwardingTableEntry.java","additions":3,"deletions":3,"binary":false,"changes":6,"previous_filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/gc\/z\/ZForwardingTableEntry.java","status":"renamed"},{"patch":"@@ -0,0 +1,132 @@\n+\/*\n+ * Copyright (c) 2018, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+package sun.jvm.hotspot.gc.x;\n+\n+import sun.jvm.hotspot.runtime.VM;\n+import sun.jvm.hotspot.types.Field;\n+import sun.jvm.hotspot.types.Type;\n+import sun.jvm.hotspot.types.TypeDataBase;\n+\n+public class XGlobals {\n+    private static Field instanceField;\n+\n+    \/\/ Global phase state\n+    public static int XPhaseRelocate;\n+\n+    public static byte XPageTypeSmall;\n+    public static byte XPageTypeMedium;\n+    public static byte XPageTypeLarge;\n+\n+    \/\/ Granule size shift\n+    public static long XGranuleSizeShift;\n+\n+    \/\/ Page size shifts\n+    public static long XPageSizeSmallShift;\n+    public static long XPageSizeMediumShift;\n+\n+    \/\/ Object alignment shifts\n+    public static int  XObjectAlignmentMediumShift;\n+    public static int  XObjectAlignmentLargeShift;\n+\n+    \/\/ Pointer part of address\n+    public static long XAddressOffsetShift;\n+\n+    \/\/ Pointer part of address\n+    public static long XAddressOffsetBits;\n+    public static long XAddressOffsetMax;\n+\n+    static {\n+        VM.registerVMInitializedObserver((o, d) -> initialize(VM.getVM().getTypeDataBase()));\n+    }\n+\n+    private static synchronized void initialize(TypeDataBase db) {\n+        Type type = db.lookupType(\"XGlobalsForVMStructs\");\n+\n+        instanceField = type.getField(\"_instance_p\");\n+\n+        XPhaseRelocate = db.lookupIntConstant(\"XPhaseRelocate\").intValue();\n+\n+        XPageTypeSmall = db.lookupIntConstant(\"XPageTypeSmall\").byteValue();\n+        XPageTypeMedium = db.lookupIntConstant(\"XPageTypeMedium\").byteValue();\n+        XPageTypeLarge = db.lookupIntConstant(\"XPageTypeLarge\").byteValue();\n+\n+        XGranuleSizeShift = db.lookupLongConstant(\"XGranuleSizeShift\").longValue();\n+\n+        XPageSizeSmallShift = db.lookupLongConstant(\"XPageSizeSmallShift\").longValue();\n+        XPageSizeMediumShift = db.lookupLongConstant(\"XPageSizeMediumShift\").longValue();\n+\n+        XObjectAlignmentMediumShift = db.lookupIntConstant(\"XObjectAlignmentMediumShift\").intValue();\n+        XObjectAlignmentLargeShift = db.lookupIntConstant(\"XObjectAlignmentLargeShift\").intValue();\n+\n+        XAddressOffsetShift = db.lookupLongConstant(\"XAddressOffsetShift\").longValue();\n+\n+        XAddressOffsetBits = db.lookupLongConstant(\"XAddressOffsetBits\").longValue();\n+        XAddressOffsetMax  = db.lookupLongConstant(\"XAddressOffsetMax\").longValue();\n+    }\n+\n+    private static XGlobalsForVMStructs instance() {\n+        return new XGlobalsForVMStructs(instanceField.getAddress());\n+    }\n+\n+    public static int XGlobalPhase() {\n+        return instance().XGlobalPhase();\n+    }\n+\n+    public static int XGlobalSeqNum() {\n+        return instance().XGlobalSeqNum();\n+    }\n+\n+    public static long XAddressOffsetMask() {\n+        return instance().XAddressOffsetMask();\n+    }\n+\n+    public static long XAddressMetadataMask() {\n+        return instance().XAddressMetadataMask();\n+    }\n+\n+    public static long XAddressMetadataFinalizable() {\n+        return instance().XAddressMetadataFinalizable();\n+    }\n+\n+    public static long XAddressGoodMask() {\n+        return instance().XAddressGoodMask();\n+    }\n+\n+    public static long XAddressBadMask() {\n+        return instance().XAddressBadMask();\n+    }\n+\n+    public static long XAddressWeakBadMask() {\n+        return instance().XAddressWeakBadMask();\n+    }\n+\n+    public static int XObjectAlignmentSmallShift() {\n+        return instance().XObjectAlignmentSmallShift();\n+    }\n+\n+    public static int XObjectAlignmentSmall() {\n+        return instance().XObjectAlignmentSmall();\n+    }\n+}\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/gc\/x\/XGlobals.java","additions":132,"deletions":0,"binary":false,"changes":132,"status":"added"},{"patch":"@@ -0,0 +1,108 @@\n+\/*\n+ * Copyright (c) 2018, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+package sun.jvm.hotspot.gc.x;\n+\n+import sun.jvm.hotspot.debugger.Address;\n+import sun.jvm.hotspot.runtime.VM;\n+import sun.jvm.hotspot.runtime.VMObject;\n+import sun.jvm.hotspot.types.AddressField;\n+import sun.jvm.hotspot.types.Type;\n+import sun.jvm.hotspot.types.TypeDataBase;\n+\n+class XGlobalsForVMStructs extends VMObject {\n+    private static AddressField XGlobalPhaseField;\n+    private static AddressField XGlobalSeqNumField;\n+    private static AddressField XAddressOffsetMaskField;\n+    private static AddressField XAddressMetadataMaskField;\n+    private static AddressField XAddressMetadataFinalizableField;\n+    private static AddressField XAddressGoodMaskField;\n+    private static AddressField XAddressBadMaskField;\n+    private static AddressField XAddressWeakBadMaskField;\n+    private static AddressField XObjectAlignmentSmallShiftField;\n+    private static AddressField XObjectAlignmentSmallField;\n+\n+    static {\n+        VM.registerVMInitializedObserver((o, d) -> initialize(VM.getVM().getTypeDataBase()));\n+    }\n+\n+    private static synchronized void initialize(TypeDataBase db) {\n+        Type type = db.lookupType(\"XGlobalsForVMStructs\");\n+\n+        XGlobalPhaseField = type.getAddressField(\"_XGlobalPhase\");\n+        XGlobalSeqNumField = type.getAddressField(\"_XGlobalSeqNum\");\n+        XAddressOffsetMaskField = type.getAddressField(\"_XAddressOffsetMask\");\n+        XAddressMetadataMaskField = type.getAddressField(\"_XAddressMetadataMask\");\n+        XAddressMetadataFinalizableField = type.getAddressField(\"_XAddressMetadataFinalizable\");\n+        XAddressGoodMaskField = type.getAddressField(\"_XAddressGoodMask\");\n+        XAddressBadMaskField = type.getAddressField(\"_XAddressBadMask\");\n+        XAddressWeakBadMaskField = type.getAddressField(\"_XAddressWeakBadMask\");\n+        XObjectAlignmentSmallShiftField = type.getAddressField(\"_XObjectAlignmentSmallShift\");\n+        XObjectAlignmentSmallField = type.getAddressField(\"_XObjectAlignmentSmall\");\n+    }\n+\n+    XGlobalsForVMStructs(Address addr) {\n+        super(addr);\n+    }\n+\n+    int XGlobalPhase() {\n+        return XGlobalPhaseField.getValue(addr).getJIntAt(0);\n+    }\n+\n+    int XGlobalSeqNum() {\n+        return XGlobalSeqNumField.getValue(addr).getJIntAt(0);\n+    }\n+\n+    long XAddressOffsetMask() {\n+        return XAddressOffsetMaskField.getValue(addr).getJLongAt(0);\n+    }\n+\n+    long XAddressMetadataMask() {\n+        return XAddressMetadataMaskField.getValue(addr).getJLongAt(0);\n+    }\n+\n+    long XAddressMetadataFinalizable() {\n+        return XAddressMetadataFinalizableField.getValue(addr).getJLongAt(0);\n+    }\n+\n+    long XAddressGoodMask() {\n+        return XAddressGoodMaskField.getValue(addr).getJLongAt(0);\n+    }\n+\n+    long XAddressBadMask() {\n+        return XAddressBadMaskField.getValue(addr).getJLongAt(0);\n+    }\n+\n+    long XAddressWeakBadMask() {\n+        return XAddressWeakBadMaskField.getValue(addr).getJLongAt(0);\n+    }\n+\n+    int XObjectAlignmentSmallShift() {\n+        return XObjectAlignmentSmallShiftField.getValue(addr).getJIntAt(0);\n+    }\n+\n+    int XObjectAlignmentSmall() {\n+        return XObjectAlignmentSmallField.getValue(addr).getJIntAt(0);\n+    }\n+}\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/gc\/x\/XGlobalsForVMStructs.java","additions":108,"deletions":0,"binary":false,"changes":108,"status":"added"},{"patch":"@@ -0,0 +1,90 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2021, NTT DATA.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+package sun.jvm.hotspot.gc.x;\n+\n+import sun.jvm.hotspot.debugger.Address;\n+import sun.jvm.hotspot.runtime.VM;\n+import sun.jvm.hotspot.runtime.VMObject;\n+import sun.jvm.hotspot.types.AddressField;\n+import sun.jvm.hotspot.types.Type;\n+import sun.jvm.hotspot.types.TypeDataBase;\n+\n+public class XGranuleMapForForwarding  extends VMObject {\n+    private static AddressField mapField;\n+\n+    static {\n+        VM.registerVMInitializedObserver((o, d) -> initialize(VM.getVM().getTypeDataBase()));\n+    }\n+\n+    private static synchronized void initialize(TypeDataBase db) {\n+        Type type = db.lookupType(\"XGranuleMapForForwarding\");\n+\n+        mapField = type.getAddressField(\"_map\");\n+    }\n+\n+    public XGranuleMapForForwarding(Address addr) {\n+        super(addr);\n+    }\n+\n+    private Address map() {\n+        return mapField.getValue(addr);\n+    }\n+\n+    public long size() {\n+        return XGlobals.XAddressOffsetMax >> XGlobals.XGranuleSizeShift;\n+    }\n+\n+    private long index_for_offset(long offset) {\n+        long index = offset >>> XGlobals.XGranuleSizeShift;\n+\n+        return index;\n+    }\n+\n+    Address at(long index) {\n+        return map().getAddressAt(index * VM.getVM().getAddressSize());\n+    }\n+\n+    Address get(long offset) {\n+        long index = index_for_offset(offset);\n+        return at(index);\n+    }\n+\n+    public class Iterator {\n+        private long next = 0;\n+\n+        boolean hasNext() {\n+            return next < size();\n+        }\n+\n+        Address next() {\n+            if (next >= size()) {\n+                throw new RuntimeException(\"OOIBE\");\n+            }\n+\n+            return at(next++);\n+        }\n+    }\n+}\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/gc\/x\/XGranuleMapForForwarding.java","additions":90,"deletions":0,"binary":false,"changes":90,"status":"added"},{"patch":"@@ -0,0 +1,89 @@\n+\/*\n+ * Copyright (c) 2018, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+package sun.jvm.hotspot.gc.x;\n+\n+import sun.jvm.hotspot.debugger.Address;\n+import sun.jvm.hotspot.runtime.VM;\n+import sun.jvm.hotspot.runtime.VMObject;\n+import sun.jvm.hotspot.types.AddressField;\n+import sun.jvm.hotspot.types.Type;\n+import sun.jvm.hotspot.types.TypeDataBase;\n+\n+public class XGranuleMapForPageTable  extends VMObject {\n+    private static AddressField mapField;\n+\n+    static {\n+        VM.registerVMInitializedObserver((o, d) -> initialize(VM.getVM().getTypeDataBase()));\n+    }\n+\n+    private static synchronized void initialize(TypeDataBase db) {\n+        Type type = db.lookupType(\"XGranuleMapForPageTable\");\n+\n+        mapField = type.getAddressField(\"_map\");\n+    }\n+\n+    public XGranuleMapForPageTable(Address addr) {\n+        super(addr);\n+    }\n+\n+    private Address map() {\n+        return mapField.getValue(addr);\n+    }\n+\n+    public long size() {\n+        return XGlobals.XAddressOffsetMax >> XGlobals.XGranuleSizeShift;\n+    }\n+\n+    private long index_for_addr(Address addr) {\n+        long index = XAddress.offset(addr) >> XGlobals.XGranuleSizeShift;\n+\n+        return index;\n+    }\n+\n+    Address at(long index) {\n+        return map().getAddressAt(index * VM.getVM().getBytesPerLong());\n+    }\n+\n+    Address get(Address addr) {\n+        long index = index_for_addr(addr);\n+        return at(index);\n+    }\n+\n+    public class Iterator {\n+        private long next = 0;\n+\n+        boolean hasNext() {\n+            return next < size();\n+        }\n+\n+        Address next() {\n+            if (next >= size()) {\n+                throw new RuntimeException(\"OOIBE\");\n+            }\n+\n+            return at(next++);\n+        }\n+    }\n+}\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/gc\/x\/XGranuleMapForPageTable.java","additions":89,"deletions":0,"binary":false,"changes":89,"status":"added"},{"patch":"@@ -25,1 +25,1 @@\n-package sun.jvm.hotspot.gc.z;\n+package sun.jvm.hotspot.gc.x;\n@@ -27,1 +27,1 @@\n-class ZHash {\n+class XHash {\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/gc\/x\/XHash.java","additions":2,"deletions":2,"binary":false,"changes":4,"previous_filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/gc\/z\/ZHash.java","status":"renamed"},{"patch":"@@ -0,0 +1,127 @@\n+\/*\n+ * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+package sun.jvm.hotspot.gc.x;\n+\n+import java.io.PrintStream;\n+\n+import sun.jvm.hotspot.debugger.Address;\n+import sun.jvm.hotspot.runtime.VM;\n+import sun.jvm.hotspot.runtime.VMObject;\n+import sun.jvm.hotspot.runtime.VMObjectFactory;\n+import sun.jvm.hotspot.types.Type;\n+import sun.jvm.hotspot.types.TypeDataBase;\n+\n+\/\/ Mirror class for XHeap\n+\n+public class XHeap extends VMObject {\n+\n+    private static long pageAllocatorFieldOffset;\n+    private static long pageTableFieldOffset;\n+    private static long forwardingTableFieldOffset;\n+    private static long relocateFieldOffset;\n+\n+    static {\n+        VM.registerVMInitializedObserver((o, d) -> initialize(VM.getVM().getTypeDataBase()));\n+    }\n+\n+    private static synchronized void initialize(TypeDataBase db) {\n+        Type type = db.lookupType(\"XHeap\");\n+\n+        pageAllocatorFieldOffset = type.getAddressField(\"_page_allocator\").getOffset();\n+        pageTableFieldOffset = type.getAddressField(\"_page_table\").getOffset();\n+        forwardingTableFieldOffset = type.getAddressField(\"_forwarding_table\").getOffset();\n+        relocateFieldOffset = type.getAddressField(\"_relocate\").getOffset();\n+    }\n+\n+    public XHeap(Address addr) {\n+        super(addr);\n+    }\n+\n+    private XPageAllocator pageAllocator() {\n+        Address pageAllocatorAddr = addr.addOffsetTo(pageAllocatorFieldOffset);\n+        return VMObjectFactory.newObject(XPageAllocator.class, pageAllocatorAddr);\n+    }\n+\n+    XPageTable pageTable() {\n+        return VMObjectFactory.newObject(XPageTable.class, addr.addOffsetTo(pageTableFieldOffset));\n+    }\n+\n+    XForwardingTable forwardingTable() {\n+        return VMObjectFactory.newObject(XForwardingTable.class, addr.addOffsetTo(forwardingTableFieldOffset));\n+    }\n+\n+    XRelocate relocate() {\n+        return VMObjectFactory.newObject(XRelocate.class, addr.addOffsetTo(relocateFieldOffset));\n+    }\n+\n+    public long maxCapacity() {\n+        return pageAllocator().maxCapacity();\n+    }\n+\n+    public long capacity() {\n+        return pageAllocator().capacity();\n+    }\n+\n+    public long used() {\n+        return pageAllocator().used();\n+    }\n+\n+    boolean is_relocating(Address o) {\n+        return pageTable().is_relocating(o);\n+    }\n+\n+    Address relocate_object(Address addr) {\n+        XForwarding forwarding = forwardingTable().get(addr);\n+        if (forwarding == null) {\n+            return XAddress.good(addr);\n+        }\n+        return relocate().relocateObject(forwarding, XAddress.good(addr));\n+    }\n+\n+    public boolean isIn(Address addr) {\n+        if (XAddress.isIn(addr)) {\n+            XPage page = pageTable().get(addr);\n+            if (page != null) {\n+                return page.isIn(addr);\n+            }\n+        }\n+        return false;\n+    }\n+\n+    public Address remapObject(Address o) {\n+        XForwarding forwarding = forwardingTable().get(addr);\n+        if (forwarding == null) {\n+            return XAddress.good(o);\n+        }\n+        return relocate().forwardObject(forwarding, XAddress.good(o));\n+    }\n+\n+    public void printOn(PrintStream tty) {\n+        tty.print(\" ZHeap          \");\n+        tty.print(\"used \" + (used() \/ 1024 \/ 1024) + \"M, \");\n+        tty.print(\"capacity \" + (capacity() \/ 1024 \/ 1024) + \"M, \");\n+        tty.println(\"max capacity \" + (maxCapacity() \/ 1024 \/ 1024) + \"M\");\n+    }\n+}\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/gc\/x\/XHeap.java","additions":127,"deletions":0,"binary":false,"changes":127,"status":"added"},{"patch":"@@ -25,1 +25,1 @@\n-package sun.jvm.hotspot.gc.z;\n+package sun.jvm.hotspot.gc.x;\n@@ -30,1 +30,1 @@\n-class ZOop {\n+class XOop {\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/gc\/x\/XOop.java","additions":2,"deletions":2,"binary":false,"changes":4,"previous_filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/gc\/z\/ZOop.java","status":"renamed"},{"patch":"@@ -0,0 +1,141 @@\n+\/*\n+ * Copyright (c) 2018, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+package sun.jvm.hotspot.gc.x;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+import sun.jvm.hotspot.debugger.Address;\n+import sun.jvm.hotspot.debugger.OopHandle;\n+import sun.jvm.hotspot.gc.shared.LiveRegionsProvider;\n+import sun.jvm.hotspot.memory.MemRegion;\n+import sun.jvm.hotspot.oops.Oop;\n+import sun.jvm.hotspot.oops.UnknownOopException;\n+import sun.jvm.hotspot.runtime.VM;\n+import sun.jvm.hotspot.runtime.VMObject;\n+import sun.jvm.hotspot.runtime.VMObjectFactory;\n+import sun.jvm.hotspot.types.AddressField;\n+import sun.jvm.hotspot.types.CIntegerField;\n+import sun.jvm.hotspot.types.Type;\n+import sun.jvm.hotspot.types.TypeDataBase;\n+\n+public class XPage extends VMObject implements LiveRegionsProvider {\n+    private static CIntegerField typeField;\n+    private static CIntegerField seqnumField;\n+    private static long virtualFieldOffset;\n+    private static AddressField topField;\n+\n+    static {\n+        VM.registerVMInitializedObserver((o, d) -> initialize(VM.getVM().getTypeDataBase()));\n+    }\n+\n+    private static synchronized void initialize(TypeDataBase db) {\n+        Type type = db.lookupType(\"XPage\");\n+\n+        typeField = type.getCIntegerField(\"_type\");\n+        seqnumField = type.getCIntegerField(\"_seqnum\");\n+        virtualFieldOffset = type.getField(\"_virtual\").getOffset();\n+        topField = type.getAddressField(\"_top\");\n+    }\n+\n+    public XPage(Address addr) {\n+        super(addr);\n+    }\n+\n+    private byte type() {\n+        return typeField.getJByte(addr);\n+    }\n+\n+    private int seqnum() {\n+        return seqnumField.getJInt(addr);\n+    }\n+\n+    private XVirtualMemory virtual() {\n+        return VMObjectFactory.newObject(XVirtualMemory.class, addr.addOffsetTo(virtualFieldOffset));\n+    }\n+\n+    private Address top() {\n+        return topField.getValue(addr);\n+    }\n+\n+    private boolean is_relocatable() {\n+        return seqnum() < XGlobals.XGlobalSeqNum();\n+    }\n+\n+    long start() {\n+        return virtual().start();\n+    }\n+\n+    long size() {\n+        return virtual().end() - virtual().start();\n+    }\n+\n+    long object_alignment_shift() {\n+        if (type() == XGlobals.XPageTypeSmall) {\n+            return XGlobals.XObjectAlignmentSmallShift();\n+        } else if (type() == XGlobals.XPageTypeMedium) {\n+            return XGlobals.XObjectAlignmentMediumShift;\n+        } else {\n+            assert(type() == XGlobals.XPageTypeLarge);\n+            return XGlobals.XObjectAlignmentLargeShift;\n+        }\n+    }\n+\n+    long objectAlignmentSize() {\n+        return 1 << object_alignment_shift();\n+    }\n+\n+    public boolean isIn(Address addr) {\n+        long offset = XAddress.offset(addr);\n+        \/\/ FIXME: it does not consider the sign.\n+        return (offset >= start()) && (offset < top().asLongValue());\n+    }\n+\n+    private long getObjectSize(Address good) {\n+        OopHandle handle = good.addOffsetToAsOopHandle(0);\n+        Oop obj = null;\n+\n+        try {\n+           obj = VM.getVM().getObjectHeap().newOop(handle);\n+        } catch (UnknownOopException exp) {\n+          throw new RuntimeException(\" UnknownOopException  \" + exp);\n+        }\n+\n+        return VM.getVM().alignUp(obj.getObjectSize(), objectAlignmentSize());\n+    }\n+\n+    public List<MemRegion> getLiveRegions() {\n+        Address start = XAddress.good(XUtils.longToAddress(start()));\n+\n+        \/\/ Can't convert top() to a \"good\" address because it might\n+        \/\/ be at the top of the \"offset\" range, and therefore also\n+        \/\/ looks like one of the color bits. Instead use the \"good\"\n+        \/\/ address and add the size.\n+        long size = top().asLongValue() - start();\n+        Address end = start.addOffsetTo(size);\n+\n+        return List.of(new MemRegion(start, end));\n+    }\n+}\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/gc\/x\/XPage.java","additions":141,"deletions":0,"binary":false,"changes":141,"status":"added"},{"patch":"@@ -0,0 +1,69 @@\n+\/*\n+ * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+package sun.jvm.hotspot.gc.x;\n+\n+import sun.jvm.hotspot.debugger.Address;\n+import sun.jvm.hotspot.runtime.VM;\n+import sun.jvm.hotspot.runtime.VMObject;\n+import sun.jvm.hotspot.types.CIntegerField;\n+import sun.jvm.hotspot.types.Type;\n+import sun.jvm.hotspot.types.TypeDataBase;\n+\n+\/\/ Mirror class for XPageAllocator\n+\n+public class XPageAllocator extends VMObject {\n+\n+    private static CIntegerField maxCapacityField;\n+    private static CIntegerField capacityField;\n+    private static CIntegerField usedField;\n+\n+    static {\n+        VM.registerVMInitializedObserver((o, d) -> initialize(VM.getVM().getTypeDataBase()));\n+    }\n+\n+    private static synchronized void initialize(TypeDataBase db) {\n+        Type type = db.lookupType(\"XPageAllocator\");\n+\n+        maxCapacityField = type.getCIntegerField(\"_max_capacity\");\n+        capacityField = type.getCIntegerField(\"_capacity\");\n+        usedField = type.getCIntegerField(\"_used\");\n+    }\n+\n+    public long maxCapacity() {\n+        return maxCapacityField.getValue(addr);\n+    }\n+\n+    public long capacity() {\n+        return capacityField.getValue(addr);\n+    }\n+\n+    public long used() {\n+        return usedField.getValue(addr);\n+    }\n+\n+    public XPageAllocator(Address addr) {\n+        super(addr);\n+    }\n+}\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/gc\/x\/XPageAllocator.java","additions":69,"deletions":0,"binary":false,"changes":69,"status":"added"},{"patch":"@@ -0,0 +1,176 @@\n+\/*\n+ * Copyright (c) 2018, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+package sun.jvm.hotspot.gc.x;\n+\n+import java.util.Iterator;\n+\n+import sun.jvm.hotspot.debugger.Address;\n+import sun.jvm.hotspot.runtime.VM;\n+import sun.jvm.hotspot.runtime.VMObject;\n+import sun.jvm.hotspot.runtime.VMObjectFactory;\n+import sun.jvm.hotspot.types.Type;\n+import sun.jvm.hotspot.types.TypeDataBase;\n+\n+public class XPageTable extends VMObject {\n+    private static long mapFieldOffset;\n+\n+    static {\n+        VM.registerVMInitializedObserver((o, d) -> initialize(VM.getVM().getTypeDataBase()));\n+    }\n+\n+    private static synchronized void initialize(TypeDataBase db) {\n+        Type type = db.lookupType(\"XPageTable\");\n+\n+        mapFieldOffset = type.getAddressField(\"_map\").getOffset();\n+    }\n+\n+    public XPageTable(Address addr) {\n+        super(addr);\n+    }\n+\n+    private XGranuleMapForPageTable map() {\n+        return VMObjectFactory.newObject(XGranuleMapForPageTable.class, addr.addOffsetTo(mapFieldOffset));\n+    }\n+\n+    private XPageTableEntry getEntry(Address o) {\n+        return new XPageTableEntry(map().get(o));\n+    }\n+\n+    XPage get(Address o) {\n+        return VMObjectFactory.newObject(XPage.class, map().get(VM.getVM().getDebugger().newAddress(XAddress.offset(o))));\n+    }\n+\n+    boolean is_relocating(Address o) {\n+        return getEntry(o).relocating();\n+    }\n+\n+    private class XPagesIterator implements Iterator<XPage> {\n+        private XGranuleMapForPageTable.Iterator mapIter;\n+        private XPage next;\n+\n+        XPagesIterator() {\n+            mapIter = map().new Iterator();\n+            positionToNext();\n+        }\n+\n+        private XPage positionToNext() {\n+            XPage current = next;\n+\n+            \/\/ Find next\n+            XPage found = null;\n+            while (mapIter.hasNext()) {\n+                XPageTableEntry entry = new XPageTableEntry(mapIter.next());\n+                if (!entry.isEmpty()) {\n+                    XPage page = entry.page();\n+                    \/\/ Medium pages have repeated entries for all covered slots,\n+                    \/\/ therefore we need to compare against the current page.\n+                    if (page != null && !page.equals(current)) {\n+                        found = page;\n+                        break;\n+                    }\n+                }\n+            }\n+\n+            next = found;\n+\n+            return current;\n+        }\n+\n+        @Override\n+        public boolean hasNext() {\n+            return next != null;\n+        }\n+\n+        @Override\n+        public XPage next() {\n+            return positionToNext();\n+        }\n+\n+        @Override\n+        public void remove() {\n+            \/* not supported *\/\n+        }\n+    }\n+\n+    abstract class XPageFilter {\n+        public abstract boolean accept(XPage page);\n+    }\n+\n+    class XPagesFilteredIterator implements Iterator<XPage> {\n+        private XPage next;\n+        private XPagesIterator iter = new XPagesIterator();\n+        private XPageFilter filter;\n+\n+        XPagesFilteredIterator(XPageFilter filter) {\n+            this.filter = filter;\n+            positionToNext();\n+        }\n+\n+        public XPage positionToNext() {\n+            XPage current = next;\n+\n+            \/\/ Find next\n+            XPage found = null;\n+            while (iter.hasNext()) {\n+                XPage page = iter.next();\n+                if (filter.accept(page)) {\n+                    found = page;\n+                    break;\n+                }\n+            }\n+\n+            next = found;\n+\n+            return current;\n+        }\n+\n+        @Override\n+        public boolean hasNext() {\n+            return next != null;\n+        }\n+\n+        @Override\n+        public XPage next() {\n+            return positionToNext();\n+        }\n+\n+        @Override\n+        public void remove() {\n+            \/* not supported *\/\n+        }\n+    }\n+\n+    public Iterator<XPage> iterator() {\n+        return new XPagesIterator();\n+    }\n+\n+    public Iterator<XPage> activePagesIterator() {\n+        return new XPagesFilteredIterator(new XPageFilter() {\n+            public boolean accept(XPage page) {\n+                return page != null;\n+            }\n+        });\n+    }\n+}\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/gc\/x\/XPageTable.java","additions":176,"deletions":0,"binary":false,"changes":176,"status":"added"},{"patch":"@@ -0,0 +1,52 @@\n+\/*\n+ * Copyright (c) 2018, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+package sun.jvm.hotspot.gc.x;\n+\n+import sun.jvm.hotspot.debugger.Address;\n+import sun.jvm.hotspot.runtime.VMObjectFactory;\n+\n+class XPageTableEntry {\n+    Address entry;\n+\n+    XPageTableEntry(Address address) {\n+        entry = address;\n+    }\n+\n+    XPage page() {\n+        return VMObjectFactory.newObject(XPage.class, zPageBits());\n+    }\n+\n+    private Address zPageBits() {\n+        return entry.andWithMask(~1L);\n+    }\n+\n+    boolean relocating() {\n+        return (entry.asLongValue() & 1) == 1;\n+    }\n+\n+    boolean isEmpty() {\n+        return entry == null || zPageBits() == null;\n+    }\n+}\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/gc\/x\/XPageTableEntry.java","additions":52,"deletions":0,"binary":false,"changes":52,"status":"added"},{"patch":"@@ -0,0 +1,74 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2021, NTT DATA.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+package sun.jvm.hotspot.gc.x;\n+\n+import sun.jvm.hotspot.debugger.Address;\n+import sun.jvm.hotspot.runtime.VM;\n+import sun.jvm.hotspot.runtime.VMObject;\n+import sun.jvm.hotspot.types.AddressField;\n+import sun.jvm.hotspot.types.Type;\n+import sun.jvm.hotspot.types.TypeDataBase;\n+\n+public class XRelocate  extends VMObject {\n+\n+    static {\n+        VM.registerVMInitializedObserver((o, d) -> initialize(VM.getVM().getTypeDataBase()));\n+    }\n+\n+    private static synchronized void initialize(TypeDataBase db) {\n+        Type type = db.lookupType(\"XRelocate\");\n+    }\n+\n+    public XRelocate(Address addr) {\n+        super(addr);\n+    }\n+\n+    private long forwardingIndex(XForwarding forwarding, Address from) {\n+        long fromOffset = XAddress.offset(from);\n+        return (fromOffset - forwarding.start()) >>> forwarding.objectAlignmentShift();\n+    }\n+\n+    private Address forwardingFind(XForwarding forwarding, Address from) {\n+        long fromIndex = forwardingIndex(forwarding, from);\n+        XForwardingEntry entry = forwarding.find(fromIndex);\n+        return entry.populated() ? XAddress.good(VM.getVM().getDebugger().newAddress(entry.toOffset())) : null;\n+    }\n+\n+    public Address forwardObject(XForwarding forwarding, Address from) {\n+        return forwardingFind(forwarding, from);\n+    }\n+\n+    public Address relocateObject(XForwarding forwarding, Address o) {\n+        Address toAddr = forwardingFind(forwarding, o);\n+        if (toAddr != null) {\n+            \/\/ Already relocated.\n+            return toAddr;\n+        } else {\n+            \/\/ Return original address because it is not yet relocated.\n+            return o;\n+        }\n+    }\n+}\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/gc\/x\/XRelocate.java","additions":74,"deletions":0,"binary":false,"changes":74,"status":"added"},{"patch":"@@ -25,1 +25,1 @@\n-package sun.jvm.hotspot.gc.z;\n+package sun.jvm.hotspot.gc.x;\n@@ -30,1 +30,1 @@\n-class ZUtils {\n+class XUtils {\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/gc\/x\/XUtils.java","additions":2,"deletions":2,"binary":false,"changes":4,"previous_filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/gc\/z\/ZUtils.java","status":"renamed"},{"patch":"@@ -25,1 +25,1 @@\n-package sun.jvm.hotspot.gc.z;\n+package sun.jvm.hotspot.gc.x;\n@@ -34,1 +34,1 @@\n-public class ZVirtualMemory extends VMObject {\n+public class XVirtualMemory extends VMObject {\n@@ -43,1 +43,1 @@\n-        Type type = db.lookupType(\"ZVirtualMemory\");\n+        Type type = db.lookupType(\"XVirtualMemory\");\n@@ -49,1 +49,1 @@\n-    public ZVirtualMemory(Address addr) {\n+    public XVirtualMemory(Address addr) {\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/gc\/x\/XVirtualMemory.java","additions":4,"deletions":4,"binary":false,"changes":8,"previous_filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/gc\/z\/ZVirtualMemory.java","status":"renamed"},{"patch":"@@ -1,77 +0,0 @@\n-\/*\n- * Copyright (c) 2018, 2021, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-package sun.jvm.hotspot.gc.z;\n-\n-import sun.jvm.hotspot.debugger.Address;\n-import sun.jvm.hotspot.runtime.VM;\n-\n-class ZAddress {\n-    static long as_long(Address value) {\n-        if (value == null) {\n-            return 0;\n-        }\n-        return value.asLongValue();\n-    };\n-\n-    static boolean is_null(Address value) {\n-        return value == null;\n-    }\n-\n-    static boolean is_weak_bad(Address value) {\n-        return (as_long(value) & ZGlobals.ZAddressWeakBadMask()) != 0L;\n-    }\n-\n-    static boolean is_weak_good(Address value) {\n-        return !is_weak_bad(value) && !is_null(value);\n-    }\n-\n-    static boolean is_weak_good_or_null(Address value) {\n-        return !is_weak_bad(value);\n-    }\n-\n-    static long offset(Address address) {\n-        return as_long(address) & ZGlobals.ZAddressOffsetMask();\n-    }\n-\n-    static Address good(Address value) {\n-        return VM.getVM().getDebugger().newAddress(offset(value) | ZGlobals.ZAddressGoodMask());\n-    }\n-\n-    static Address good_or_null(Address value) {\n-        return is_null(value) ? value : good(value);\n-    }\n-\n-    private static boolean isPowerOf2(long value) {\n-        return (value != 0L) && ((value & (value - 1)) == 0L);\n-    }\n-\n-    static boolean isIn(Address addr) {\n-        long value = as_long(addr);\n-        if (!isPowerOf2(value & ~ZGlobals.ZAddressOffsetMask())) {\n-            return false;\n-        }\n-        return (value & (ZGlobals.ZAddressMetadataMask() & ~ZGlobals.ZAddressMetadataFinalizable())) != 0L;\n-    }\n-}\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/gc\/z\/ZAddress.java","additions":0,"deletions":77,"binary":false,"changes":77,"status":"deleted"},{"patch":"@@ -1,71 +0,0 @@\n-\/*\n- * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2021, NTT DATA.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-package sun.jvm.hotspot.gc.z;\n-\n-import sun.jvm.hotspot.debugger.Address;\n-import sun.jvm.hotspot.runtime.VM;\n-import sun.jvm.hotspot.runtime.VMObject;\n-import sun.jvm.hotspot.runtime.VMObjectFactory;\n-import sun.jvm.hotspot.types.CIntegerField;\n-import sun.jvm.hotspot.types.Type;\n-import sun.jvm.hotspot.types.TypeDataBase;\n-\n-public class ZAttachedArrayForForwarding extends VMObject {\n-    private static CIntegerField lengthField;\n-\n-    static {\n-        VM.registerVMInitializedObserver((o, d) -> initialize(VM.getVM().getTypeDataBase()));\n-    }\n-\n-    private static synchronized void initialize(TypeDataBase db) {\n-        Type type = db.lookupType(\"ZAttachedArrayForForwarding\");\n-\n-        lengthField = type.getCIntegerField(\"_length\");\n-    }\n-\n-    public ZAttachedArrayForForwarding(Address addr) {\n-        super(addr);\n-    }\n-\n-    public long length() {\n-        return lengthField.getValue(addr);\n-    }\n-\n-    \/\/ ObjectT: ZForwarding\n-    \/\/  ArrayT: ZForwardingEntry\n-    \/\/\n-    \/\/ template <typename ObjectT, typename ArrayT>\n-    \/\/ inline size_t ZAttachedArray<ObjectT, ArrayT>::object_size()\n-    private long objectSize() {\n-        return ZUtils.alignUp(ZForwarding.getSize(), ZForwardingEntry.getSize());\n-    }\n-\n-    \/\/ ArrayT* operator()(const ObjectT* obj) const\n-    public ZForwardingEntry get(ZForwarding obj) {\n-        Address o = obj.getAddress().addOffsetTo(objectSize());\n-        return VMObjectFactory.newObject(ZForwardingEntry.class, o);\n-    }\n-}\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/gc\/z\/ZAttachedArrayForForwarding.java","additions":0,"deletions":71,"binary":false,"changes":71,"status":"deleted"},{"patch":"@@ -1,71 +0,0 @@\n-\/*\n- * Copyright (c) 2018, 2021, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-package sun.jvm.hotspot.gc.z;\n-\n-import sun.jvm.hotspot.debugger.Address;\n-import sun.jvm.hotspot.runtime.VM;\n-\n-class ZBarrier {\n-    private static boolean is_weak_good_or_null_fast_path(Address addr) {\n-        return ZAddress.is_weak_good_or_null(addr);\n-    }\n-\n-    private static Address weak_load_barrier_on_oop_slow_path(Address addr) {\n-        return ZAddress.is_weak_good(addr) ? ZAddress.good(addr) : relocate_or_remap(addr);\n-    }\n-\n-    private static boolean during_relocate() {\n-        return ZGlobals.ZGlobalPhase() == ZGlobals.ZPhaseRelocate;\n-    }\n-\n-    private static Address relocate(Address addr) {\n-        return zheap().relocate_object(addr);\n-    }\n-\n-    private static ZHeap zheap() {\n-        ZCollectedHeap zCollectedHeap = (ZCollectedHeap)VM.getVM().getUniverse().heap();\n-        return zCollectedHeap.heap();\n-    }\n-\n-    private static Address remap(Address addr) {\n-        return zheap().remapObject(addr);\n-    }\n-\n-    private static Address relocate_or_remap(Address addr) {\n-        return during_relocate() ? relocate(addr) : remap(addr);\n-    }\n-\n-    static Address weak_barrier(Address o) {\n-        \/\/ Fast path\n-        if (is_weak_good_or_null_fast_path(o)) {\n-            \/\/ Return the good address instead of the weak good address\n-            \/\/ to ensure that the currently active heap view is used.\n-            return ZAddress.good_or_null(o);\n-        }\n-\n-        \/\/ Slow path\n-        return weak_load_barrier_on_oop_slow_path(o);\n-    }\n-}\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/gc\/z\/ZBarrier.java","additions":0,"deletions":71,"binary":false,"changes":71,"status":"deleted"},{"patch":"@@ -87,1 +87,1 @@\n-        return heap().isIn(a);\n+        throw new RuntimeException(\"ZCollectedHeap.isInReserved not implemented\");\n@@ -91,6 +91,1 @@\n-        oopAddress = ZBarrier.weak_barrier(oopAddress);\n-        if (oopAddress == null) {\n-            return null;\n-        }\n-\n-        return oopAddress.addOffsetToAsOopHandle(0);\n+        throw new RuntimeException(\"ZCollectedHeap.oop_load_barrier not implemented\");\n@@ -116,7 +111,1 @@\n-        Address origOop = ZOop.to_address(handle);\n-        Address loadBarrieredOop = ZBarrier.weak_barrier(origOop);\n-        if (!origOop.equals(loadBarrieredOop)) {\n-            return origOop + \" (\" + loadBarrieredOop.toString() + \")\";\n-        } else {\n-            return handle.toString();\n-        }\n+        return handle.toString();\n@@ -127,5 +116,1 @@\n-        Iterator<ZPage> iter = heap().pageTable().activePagesIterator();\n-        while (iter.hasNext()) {\n-            ZPage page = iter.next();\n-            closure.doLiveRegions(page);\n-        }\n+        throw new RuntimeException(\"ZCollectedHeap.liveRegionsIterate not implemented\");\n@@ -136,2 +121,1 @@\n-        \/\/ Ignores the size\n-        return new ZExternalBitMap(this);\n+        throw new RuntimeException(\"ZCollectedHeap.createBitMap not implemented\");\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/gc\/z\/ZCollectedHeap.java","additions":5,"deletions":21,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -1,111 +0,0 @@\n-\/*\n- * Copyright (c) 2019, 2021, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-package sun.jvm.hotspot.gc.z;\n-\n-import java.util.HashMap;\n-\n-import sun.jvm.hotspot.runtime.VM;\n-import sun.jvm.hotspot.utilities.BitMap;\n-import sun.jvm.hotspot.utilities.BitMapInterface;\n-\n-\/** Discontiguous bitmap for ZGC. *\/\n-public class ZExternalBitMap implements BitMapInterface {\n-    private ZPageTable pageTable;\n-    private final long oopSize;\n-\n-    private HashMap<ZPage, BitMap> pageToBitMap = new HashMap<ZPage, BitMap>();\n-\n-    public ZExternalBitMap(ZCollectedHeap collectedHeap) {\n-        pageTable = collectedHeap.heap().pageTable();\n-        oopSize = VM.getVM().getOopSize();\n-    }\n-\n-    private ZPage getPage(long zOffset) {\n-        if (zOffset > ZGlobals.ZAddressOffsetMask()) {\n-            throw new RuntimeException(\"Not a Z offset: \" + zOffset);\n-        }\n-\n-        ZPage page = pageTable.get(ZUtils.longToAddress(zOffset));\n-        if (page == null) {\n-            throw new RuntimeException(\"Address not in pageTable: \" + zOffset);\n-        }\n-        return page;\n-    }\n-\n-    private BitMap getOrAddBitMap(ZPage page) {\n-        BitMap bitMap = pageToBitMap.get(page);\n-        if (bitMap == null) {\n-            long size = page.size();\n-\n-            long maxNumObjects = size >>> page.object_alignment_shift();\n-            if (maxNumObjects > Integer.MAX_VALUE) {\n-                throw new RuntimeException(\"int overflow\");\n-            }\n-            int intMaxNumObjects = (int)maxNumObjects;\n-\n-            bitMap = new BitMap(intMaxNumObjects);\n-            pageToBitMap.put(page,  bitMap);\n-        }\n-\n-        return bitMap;\n-    }\n-\n-    private int pageLocalBitMapIndex(ZPage page, long zOffset) {\n-        long pageLocalZOffset = zOffset - page.start();\n-        return (int)(pageLocalZOffset >>> page.object_alignment_shift());\n-    }\n-\n-    private long convertToZOffset(long offset) {\n-        long addr = oopSize * offset;\n-        return addr & ZGlobals.ZAddressOffsetMask();\n-    }\n-\n-    @Override\n-    public boolean at(long offset) {\n-        long zOffset = convertToZOffset(offset);\n-        ZPage page = getPage(zOffset);\n-        BitMap bitMap = getOrAddBitMap(page);\n-        int index = pageLocalBitMapIndex(page, zOffset);\n-\n-        return bitMap.at(index);\n-    }\n-\n-    @Override\n-    public void atPut(long offset, boolean value) {\n-        long zOffset = convertToZOffset(offset);\n-        ZPage page = getPage(zOffset);\n-        BitMap bitMap = getOrAddBitMap(page);\n-        int index = pageLocalBitMapIndex(page, zOffset);\n-\n-        bitMap.atPut(index, value);\n-    }\n-\n-    @Override\n-    public void clear() {\n-        for (BitMap bitMap : pageToBitMap.values()) {\n-            bitMap.clear();\n-        }\n-    }\n-}\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/gc\/z\/ZExternalBitMap.java","additions":0,"deletions":111,"binary":false,"changes":111,"status":"deleted"},{"patch":"@@ -1,136 +0,0 @@\n-\/*\n- * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2021, NTT DATA.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-package sun.jvm.hotspot.gc.z;\n-\n-import java.util.Iterator;\n-\n-import sun.jvm.hotspot.debugger.Address;\n-import sun.jvm.hotspot.runtime.VM;\n-import sun.jvm.hotspot.runtime.VMObject;\n-import sun.jvm.hotspot.runtime.VMObjectFactory;\n-import sun.jvm.hotspot.types.CIntegerField;\n-import sun.jvm.hotspot.types.Type;\n-import sun.jvm.hotspot.types.TypeDataBase;\n-\n-public class ZForwarding extends VMObject {\n-    private static Type type;\n-    private static long virtualFieldOffset;\n-    private static long entriesFieldOffset;\n-    private static CIntegerField objectAlignmentShiftField;\n-    private static CIntegerField refCountField;\n-\n-    static {\n-        VM.registerVMInitializedObserver((o, d) -> initialize(VM.getVM().getTypeDataBase()));\n-    }\n-\n-    private static synchronized void initialize(TypeDataBase db) {\n-        type = db.lookupType(\"ZForwarding\");\n-\n-        virtualFieldOffset = type.getField(\"_virtual\").getOffset();\n-        entriesFieldOffset = type.getField(\"_entries\").getOffset();\n-        objectAlignmentShiftField = type.getCIntegerField(\"_object_alignment_shift\");\n-        refCountField = type.getCIntegerField(\"_ref_count\");\n-    }\n-\n-    public ZForwarding(Address addr) {\n-        super(addr);\n-    }\n-\n-    public static long getSize() {\n-        return type.getSize();\n-    }\n-\n-    private ZVirtualMemory virtual() {\n-        return VMObjectFactory.newObject(ZVirtualMemory.class, addr.addOffsetTo(virtualFieldOffset));\n-    }\n-\n-    private ZAttachedArrayForForwarding entries() {\n-        return VMObjectFactory.newObject(ZAttachedArrayForForwarding.class, addr.addOffsetTo(entriesFieldOffset));\n-    }\n-\n-    public long start() {\n-        return virtual().start();\n-    }\n-\n-    public int objectAlignmentShift() {\n-        return (int)objectAlignmentShiftField.getValue(addr);\n-    }\n-\n-    public boolean retainPage() {\n-        return refCountField.getValue(addr) > 0;\n-    }\n-\n-    private ZForwardingEntry at(long cursor) {\n-        long offset = ZForwardingEntry.getSize() * cursor;\n-        Address entryAddress = entries().get(this).getAddress().addOffsetTo(offset);\n-        return VMObjectFactory.newObject(ZForwardingEntry.class, entryAddress);\n-    }\n-\n-    private class ZForwardEntryIterator implements Iterator<ZForwardingEntry> {\n-\n-        private long cursor;\n-\n-        private ZForwardingEntry nextEntry;\n-\n-        public ZForwardEntryIterator(long fromIndex) {\n-            long mask = entries().length() - 1;\n-            long hash = ZHash.uint32_to_uint32(fromIndex);\n-            cursor = hash & mask;\n-            nextEntry = at(cursor);\n-        }\n-\n-        @Override\n-        public boolean hasNext() {\n-            return nextEntry.populated();\n-        }\n-\n-        @Override\n-        public ZForwardingEntry next() {\n-            ZForwardingEntry entry = nextEntry;\n-\n-            long mask = entries().length() - 1;\n-            cursor = (cursor + 1) & mask;\n-            nextEntry = at(cursor);\n-\n-            return entry;\n-        }\n-\n-        public ZForwardingEntry peak() {\n-            return nextEntry;\n-        }\n-    }\n-\n-    public ZForwardingEntry find(long fromIndex) {\n-        ZForwardEntryIterator itr = new ZForwardEntryIterator(fromIndex);\n-        while (itr.hasNext()) {\n-            ZForwardingEntry entry = itr.next();\n-            if (entry.fromIndex() == fromIndex) {\n-                return entry;\n-            }\n-        }\n-        return itr.peak();\n-    }\n-}\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/gc\/z\/ZForwarding.java","additions":0,"deletions":136,"binary":false,"changes":136,"status":"deleted"},{"patch":"@@ -1,97 +0,0 @@\n-\/*\n- * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2021, NTT DATA.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-package sun.jvm.hotspot.gc.z;\n-\n-import sun.jvm.hotspot.debugger.Address;\n-import sun.jvm.hotspot.runtime.VM;\n-import sun.jvm.hotspot.runtime.VMObject;\n-import sun.jvm.hotspot.runtime.VMObjectFactory;\n-import sun.jvm.hotspot.types.CIntegerField;\n-import sun.jvm.hotspot.types.Type;\n-import sun.jvm.hotspot.types.TypeDataBase;\n-\n-public class ZForwardingEntry extends VMObject {\n-    private static Type type;\n-    private static CIntegerField entryField;\n-\n-    static {\n-        VM.registerVMInitializedObserver((o, d) -> initialize(VM.getVM().getTypeDataBase()));\n-    }\n-\n-    private static synchronized void initialize(TypeDataBase db) {\n-        type = db.lookupType(\"ZForwardingEntry\");\n-\n-        entryField = type.getCIntegerField(\"_entry\");\n-    }\n-\n-    public static long getSize() {\n-        return type.getSize();\n-    }\n-\n-    public ZForwardingEntry(Address addr) {\n-        super(addr);\n-    }\n-\n-    public long entry() {\n-        return entryField.getValue(addr);\n-    }\n-\n-    \/\/ typedef ZBitField<uint64_t, bool,   0,   1> field_populated\n-    private boolean fieldPopulatedDecode(long value) {\n-        long FieldMask = (1L << 1) - 1;\n-        int FieldShift = 1;\n-        int ValueShift = 0;\n-        return (((value >>> FieldShift) & FieldMask) << ValueShift) != 0L;\n-    }\n-\n-    \/\/ typedef ZBitField<uint64_t, size_t, 1,  45> field_to_offset;\n-    private long fieldToOffsetDecode(long value) {\n-        long FieldMask = (1L << 45) - 1;\n-        int FieldShift = 1;\n-        int ValueShift = 0;\n-        return ((value >>> FieldShift) & FieldMask) << ValueShift;\n-    }\n-\n-    \/\/ typedef ZBitField<uint64_t, size_t, 46, 18> field_from_index;\n-    private long fieldFromIndexDecode(long value) {\n-        long FieldMask = (1L << 18) - 1;\n-        int FieldShift = 46;\n-        int ValueShift = 0;\n-        return ((value >>> FieldShift) & FieldMask) << ValueShift;\n-    }\n-\n-    public boolean populated() {\n-        return fieldPopulatedDecode(entry());\n-    }\n-\n-    public long toOffset() {\n-        return fieldToOffsetDecode(entry());\n-    }\n-\n-    public long fromIndex() {\n-        return fieldFromIndexDecode(entry());\n-    }\n-}\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/gc\/z\/ZForwardingEntry.java","additions":0,"deletions":97,"binary":false,"changes":97,"status":"deleted"},{"patch":"@@ -1,60 +0,0 @@\n-\/*\n- * Copyright (c) 2018, 2021, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-package sun.jvm.hotspot.gc.z;\n-\n-import sun.jvm.hotspot.debugger.Address;\n-import sun.jvm.hotspot.runtime.VM;\n-import sun.jvm.hotspot.runtime.VMObject;\n-import sun.jvm.hotspot.runtime.VMObjectFactory;\n-import sun.jvm.hotspot.types.AddressField;\n-import sun.jvm.hotspot.types.CIntegerField;\n-import sun.jvm.hotspot.types.Type;\n-import sun.jvm.hotspot.types.TypeDataBase;\n-\n-public class ZForwardingTable extends VMObject {\n-    private static long mapFieldOffset;\n-\n-    static {\n-        VM.registerVMInitializedObserver((o, d) -> initialize(VM.getVM().getTypeDataBase()));\n-    }\n-\n-    private static synchronized void initialize(TypeDataBase db) {\n-        Type type = db.lookupType(\"ZForwardingTable\");\n-\n-        mapFieldOffset = type.getAddressField(\"_map\").getOffset();\n-    }\n-\n-    public ZForwardingTable(Address addr) {\n-        super(addr);\n-    }\n-\n-    private ZGranuleMapForForwarding map() {\n-        return VMObjectFactory.newObject(ZGranuleMapForForwarding.class, addr.addOffsetTo(mapFieldOffset));\n-    }\n-\n-    public ZForwarding get(Address o) {\n-        return VMObjectFactory.newObject(ZForwarding.class, map().get(ZAddress.offset(o)));\n-    }\n-}\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/gc\/z\/ZForwardingTable.java","additions":0,"deletions":60,"binary":false,"changes":60,"status":"deleted"},{"patch":"@@ -1,132 +0,0 @@\n-\/*\n- * Copyright (c) 2018, 2021, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-package sun.jvm.hotspot.gc.z;\n-\n-import sun.jvm.hotspot.runtime.VM;\n-import sun.jvm.hotspot.types.Field;\n-import sun.jvm.hotspot.types.Type;\n-import sun.jvm.hotspot.types.TypeDataBase;\n-\n-public class ZGlobals {\n-    private static Field instanceField;\n-\n-    \/\/ Global phase state\n-    public static int ZPhaseRelocate;\n-\n-    public static byte ZPageTypeSmall;\n-    public static byte ZPageTypeMedium;\n-    public static byte ZPageTypeLarge;\n-\n-    \/\/ Granule size shift\n-    public static long ZGranuleSizeShift;\n-\n-    \/\/ Page size shifts\n-    public static long ZPageSizeSmallShift;\n-    public static long ZPageSizeMediumShift;\n-\n-    \/\/ Object alignment shifts\n-    public static int  ZObjectAlignmentMediumShift;\n-    public static int  ZObjectAlignmentLargeShift;\n-\n-    \/\/ Pointer part of address\n-    public static long ZAddressOffsetShift;\n-\n-    \/\/ Pointer part of address\n-    public static long ZAddressOffsetBits;\n-    public static long ZAddressOffsetMax;\n-\n-    static {\n-        VM.registerVMInitializedObserver((o, d) -> initialize(VM.getVM().getTypeDataBase()));\n-    }\n-\n-    private static synchronized void initialize(TypeDataBase db) {\n-        Type type = db.lookupType(\"ZGlobalsForVMStructs\");\n-\n-        instanceField = type.getField(\"_instance_p\");\n-\n-        ZPhaseRelocate = db.lookupIntConstant(\"ZPhaseRelocate\").intValue();\n-\n-        ZPageTypeSmall = db.lookupIntConstant(\"ZPageTypeSmall\").byteValue();\n-        ZPageTypeMedium = db.lookupIntConstant(\"ZPageTypeMedium\").byteValue();\n-        ZPageTypeLarge = db.lookupIntConstant(\"ZPageTypeLarge\").byteValue();\n-\n-        ZGranuleSizeShift = db.lookupLongConstant(\"ZGranuleSizeShift\").longValue();\n-\n-        ZPageSizeSmallShift = db.lookupLongConstant(\"ZPageSizeSmallShift\").longValue();\n-        ZPageSizeMediumShift = db.lookupLongConstant(\"ZPageSizeMediumShift\").longValue();\n-\n-        ZObjectAlignmentMediumShift = db.lookupIntConstant(\"ZObjectAlignmentMediumShift\").intValue();\n-        ZObjectAlignmentLargeShift = db.lookupIntConstant(\"ZObjectAlignmentLargeShift\").intValue();\n-\n-        ZAddressOffsetShift = db.lookupLongConstant(\"ZAddressOffsetShift\").longValue();\n-\n-        ZAddressOffsetBits = db.lookupLongConstant(\"ZAddressOffsetBits\").longValue();\n-        ZAddressOffsetMax  = db.lookupLongConstant(\"ZAddressOffsetMax\").longValue();\n-    }\n-\n-    private static ZGlobalsForVMStructs instance() {\n-        return new ZGlobalsForVMStructs(instanceField.getAddress());\n-    }\n-\n-    public static int ZGlobalPhase() {\n-        return instance().ZGlobalPhase();\n-    }\n-\n-    public static int ZGlobalSeqNum() {\n-        return instance().ZGlobalSeqNum();\n-    }\n-\n-    public static long ZAddressOffsetMask() {\n-        return instance().ZAddressOffsetMask();\n-    }\n-\n-    public static long ZAddressMetadataMask() {\n-        return instance().ZAddressMetadataMask();\n-    }\n-\n-    public static long ZAddressMetadataFinalizable() {\n-        return instance().ZAddressMetadataFinalizable();\n-    }\n-\n-    public static long ZAddressGoodMask() {\n-        return instance().ZAddressGoodMask();\n-    }\n-\n-    public static long ZAddressBadMask() {\n-        return instance().ZAddressBadMask();\n-    }\n-\n-    public static long ZAddressWeakBadMask() {\n-        return instance().ZAddressWeakBadMask();\n-    }\n-\n-    public static int ZObjectAlignmentSmallShift() {\n-        return instance().ZObjectAlignmentSmallShift();\n-    }\n-\n-    public static int ZObjectAlignmentSmall() {\n-        return instance().ZObjectAlignmentSmall();\n-    }\n-}\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/gc\/z\/ZGlobals.java","additions":0,"deletions":132,"binary":false,"changes":132,"status":"deleted"},{"patch":"@@ -1,108 +0,0 @@\n-\/*\n- * Copyright (c) 2018, 2021, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-package sun.jvm.hotspot.gc.z;\n-\n-import sun.jvm.hotspot.debugger.Address;\n-import sun.jvm.hotspot.runtime.VM;\n-import sun.jvm.hotspot.runtime.VMObject;\n-import sun.jvm.hotspot.types.AddressField;\n-import sun.jvm.hotspot.types.Type;\n-import sun.jvm.hotspot.types.TypeDataBase;\n-\n-class ZGlobalsForVMStructs extends VMObject {\n-    private static AddressField ZGlobalPhaseField;\n-    private static AddressField ZGlobalSeqNumField;\n-    private static AddressField ZAddressOffsetMaskField;\n-    private static AddressField ZAddressMetadataMaskField;\n-    private static AddressField ZAddressMetadataFinalizableField;\n-    private static AddressField ZAddressGoodMaskField;\n-    private static AddressField ZAddressBadMaskField;\n-    private static AddressField ZAddressWeakBadMaskField;\n-    private static AddressField ZObjectAlignmentSmallShiftField;\n-    private static AddressField ZObjectAlignmentSmallField;\n-\n-    static {\n-        VM.registerVMInitializedObserver((o, d) -> initialize(VM.getVM().getTypeDataBase()));\n-    }\n-\n-    private static synchronized void initialize(TypeDataBase db) {\n-        Type type = db.lookupType(\"ZGlobalsForVMStructs\");\n-\n-        ZGlobalPhaseField = type.getAddressField(\"_ZGlobalPhase\");\n-        ZGlobalSeqNumField = type.getAddressField(\"_ZGlobalSeqNum\");\n-        ZAddressOffsetMaskField = type.getAddressField(\"_ZAddressOffsetMask\");\n-        ZAddressMetadataMaskField = type.getAddressField(\"_ZAddressMetadataMask\");\n-        ZAddressMetadataFinalizableField = type.getAddressField(\"_ZAddressMetadataFinalizable\");\n-        ZAddressGoodMaskField = type.getAddressField(\"_ZAddressGoodMask\");\n-        ZAddressBadMaskField = type.getAddressField(\"_ZAddressBadMask\");\n-        ZAddressWeakBadMaskField = type.getAddressField(\"_ZAddressWeakBadMask\");\n-        ZObjectAlignmentSmallShiftField = type.getAddressField(\"_ZObjectAlignmentSmallShift\");\n-        ZObjectAlignmentSmallField = type.getAddressField(\"_ZObjectAlignmentSmall\");\n-    }\n-\n-    ZGlobalsForVMStructs(Address addr) {\n-        super(addr);\n-    }\n-\n-    int ZGlobalPhase() {\n-        return ZGlobalPhaseField.getValue(addr).getJIntAt(0);\n-    }\n-\n-    int ZGlobalSeqNum() {\n-        return ZGlobalSeqNumField.getValue(addr).getJIntAt(0);\n-    }\n-\n-    long ZAddressOffsetMask() {\n-        return ZAddressOffsetMaskField.getValue(addr).getJLongAt(0);\n-    }\n-\n-    long ZAddressMetadataMask() {\n-        return ZAddressMetadataMaskField.getValue(addr).getJLongAt(0);\n-    }\n-\n-    long ZAddressMetadataFinalizable() {\n-        return ZAddressMetadataFinalizableField.getValue(addr).getJLongAt(0);\n-    }\n-\n-    long ZAddressGoodMask() {\n-        return ZAddressGoodMaskField.getValue(addr).getJLongAt(0);\n-    }\n-\n-    long ZAddressBadMask() {\n-        return ZAddressBadMaskField.getValue(addr).getJLongAt(0);\n-    }\n-\n-    long ZAddressWeakBadMask() {\n-        return ZAddressWeakBadMaskField.getValue(addr).getJLongAt(0);\n-    }\n-\n-    int ZObjectAlignmentSmallShift() {\n-        return ZObjectAlignmentSmallShiftField.getValue(addr).getJIntAt(0);\n-    }\n-\n-    int ZObjectAlignmentSmall() {\n-        return ZObjectAlignmentSmallField.getValue(addr).getJIntAt(0);\n-    }\n-}\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/gc\/z\/ZGlobalsForVMStructs.java","additions":0,"deletions":108,"binary":false,"changes":108,"status":"deleted"},{"patch":"@@ -1,90 +0,0 @@\n-\/*\n- * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2021, NTT DATA.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-package sun.jvm.hotspot.gc.z;\n-\n-import sun.jvm.hotspot.debugger.Address;\n-import sun.jvm.hotspot.runtime.VM;\n-import sun.jvm.hotspot.runtime.VMObject;\n-import sun.jvm.hotspot.types.AddressField;\n-import sun.jvm.hotspot.types.Type;\n-import sun.jvm.hotspot.types.TypeDataBase;\n-\n-public class ZGranuleMapForForwarding  extends VMObject {\n-    private static AddressField mapField;\n-\n-    static {\n-        VM.registerVMInitializedObserver((o, d) -> initialize(VM.getVM().getTypeDataBase()));\n-    }\n-\n-    private static synchronized void initialize(TypeDataBase db) {\n-        Type type = db.lookupType(\"ZGranuleMapForForwarding\");\n-\n-        mapField = type.getAddressField(\"_map\");\n-    }\n-\n-    public ZGranuleMapForForwarding(Address addr) {\n-        super(addr);\n-    }\n-\n-    private Address map() {\n-        return mapField.getValue(addr);\n-    }\n-\n-    public long size() {\n-        return ZGlobals.ZAddressOffsetMax >> ZGlobals.ZGranuleSizeShift;\n-    }\n-\n-    private long index_for_offset(long offset) {\n-        long index = offset >>> ZGlobals.ZGranuleSizeShift;\n-\n-        return index;\n-    }\n-\n-    Address at(long index) {\n-        return map().getAddressAt(index * VM.getVM().getAddressSize());\n-    }\n-\n-    Address get(long offset) {\n-        long index = index_for_offset(offset);\n-        return at(index);\n-    }\n-\n-    public class Iterator {\n-        private long next = 0;\n-\n-        boolean hasNext() {\n-            return next < size();\n-        }\n-\n-        Address next() {\n-            if (next >= size()) {\n-                throw new RuntimeException(\"OOIBE\");\n-            }\n-\n-            return at(next++);\n-        }\n-    }\n-}\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/gc\/z\/ZGranuleMapForForwarding.java","additions":0,"deletions":90,"binary":false,"changes":90,"status":"deleted"},{"patch":"@@ -1,89 +0,0 @@\n-\/*\n- * Copyright (c) 2018, 2021, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-package sun.jvm.hotspot.gc.z;\n-\n-import sun.jvm.hotspot.debugger.Address;\n-import sun.jvm.hotspot.runtime.VM;\n-import sun.jvm.hotspot.runtime.VMObject;\n-import sun.jvm.hotspot.types.AddressField;\n-import sun.jvm.hotspot.types.Type;\n-import sun.jvm.hotspot.types.TypeDataBase;\n-\n-public class ZGranuleMapForPageTable  extends VMObject {\n-    private static AddressField mapField;\n-\n-    static {\n-        VM.registerVMInitializedObserver((o, d) -> initialize(VM.getVM().getTypeDataBase()));\n-    }\n-\n-    private static synchronized void initialize(TypeDataBase db) {\n-        Type type = db.lookupType(\"ZGranuleMapForPageTable\");\n-\n-        mapField = type.getAddressField(\"_map\");\n-    }\n-\n-    public ZGranuleMapForPageTable(Address addr) {\n-        super(addr);\n-    }\n-\n-    private Address map() {\n-        return mapField.getValue(addr);\n-    }\n-\n-    public long size() {\n-        return ZGlobals.ZAddressOffsetMax >> ZGlobals.ZGranuleSizeShift;\n-    }\n-\n-    private long index_for_addr(Address addr) {\n-        long index = ZAddress.offset(addr) >> ZGlobals.ZGranuleSizeShift;\n-\n-        return index;\n-    }\n-\n-    Address at(long index) {\n-        return map().getAddressAt(index * VM.getVM().getBytesPerLong());\n-    }\n-\n-    Address get(Address addr) {\n-        long index = index_for_addr(addr);\n-        return at(index);\n-    }\n-\n-    public class Iterator {\n-        private long next = 0;\n-\n-        boolean hasNext() {\n-            return next < size();\n-        }\n-\n-        Address next() {\n-            if (next >= size()) {\n-                throw new RuntimeException(\"OOIBE\");\n-            }\n-\n-            return at(next++);\n-        }\n-    }\n-}\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/gc\/z\/ZGranuleMapForPageTable.java","additions":0,"deletions":89,"binary":false,"changes":89,"status":"deleted"},{"patch":"@@ -41,3 +41,0 @@\n-    private static long pageTableFieldOffset;\n-    private static long forwardingTableFieldOffset;\n-    private static long relocateFieldOffset;\n@@ -53,3 +50,0 @@\n-        pageTableFieldOffset = type.getAddressField(\"_page_table\").getOffset();\n-        forwardingTableFieldOffset = type.getAddressField(\"_forwarding_table\").getOffset();\n-        relocateFieldOffset = type.getAddressField(\"_relocate\").getOffset();\n@@ -61,1 +55,0 @@\n-\n@@ -67,12 +60,0 @@\n-    ZPageTable pageTable() {\n-        return VMObjectFactory.newObject(ZPageTable.class, addr.addOffsetTo(pageTableFieldOffset));\n-    }\n-\n-    ZForwardingTable forwardingTable() {\n-        return VMObjectFactory.newObject(ZForwardingTable.class, addr.addOffsetTo(forwardingTableFieldOffset));\n-    }\n-\n-    ZRelocate relocate() {\n-        return VMObjectFactory.newObject(ZRelocate.class, addr.addOffsetTo(relocateFieldOffset));\n-    }\n-\n@@ -91,30 +72,0 @@\n-    boolean is_relocating(Address o) {\n-        return pageTable().is_relocating(o);\n-    }\n-\n-    Address relocate_object(Address addr) {\n-        ZForwarding forwarding = forwardingTable().get(addr);\n-        if (forwarding == null) {\n-            return ZAddress.good(addr);\n-        }\n-        return relocate().relocateObject(forwarding, ZAddress.good(addr));\n-    }\n-\n-    public boolean isIn(Address addr) {\n-        if (ZAddress.isIn(addr)) {\n-            ZPage page = pageTable().get(addr);\n-            if (page != null) {\n-                return page.isIn(addr);\n-            }\n-        }\n-        return false;\n-    }\n-\n-    public Address remapObject(Address o) {\n-        ZForwarding forwarding = forwardingTable().get(addr);\n-        if (forwarding == null) {\n-            return ZAddress.good(o);\n-        }\n-        return relocate().forwardObject(forwarding, ZAddress.good(o));\n-    }\n-\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/gc\/z\/ZHeap.java","additions":0,"deletions":49,"binary":false,"changes":49,"status":"modified"},{"patch":"@@ -1,141 +0,0 @@\n-\/*\n- * Copyright (c) 2018, 2021, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-package sun.jvm.hotspot.gc.z;\n-\n-import java.util.ArrayList;\n-import java.util.List;\n-\n-import sun.jvm.hotspot.debugger.Address;\n-import sun.jvm.hotspot.debugger.OopHandle;\n-import sun.jvm.hotspot.gc.shared.LiveRegionsProvider;\n-import sun.jvm.hotspot.memory.MemRegion;\n-import sun.jvm.hotspot.oops.Oop;\n-import sun.jvm.hotspot.oops.UnknownOopException;\n-import sun.jvm.hotspot.runtime.VM;\n-import sun.jvm.hotspot.runtime.VMObject;\n-import sun.jvm.hotspot.runtime.VMObjectFactory;\n-import sun.jvm.hotspot.types.AddressField;\n-import sun.jvm.hotspot.types.CIntegerField;\n-import sun.jvm.hotspot.types.Type;\n-import sun.jvm.hotspot.types.TypeDataBase;\n-\n-public class ZPage extends VMObject implements LiveRegionsProvider {\n-    private static CIntegerField typeField;\n-    private static CIntegerField seqnumField;\n-    private static long virtualFieldOffset;\n-    private static AddressField topField;\n-\n-    static {\n-        VM.registerVMInitializedObserver((o, d) -> initialize(VM.getVM().getTypeDataBase()));\n-    }\n-\n-    private static synchronized void initialize(TypeDataBase db) {\n-        Type type = db.lookupType(\"ZPage\");\n-\n-        typeField = type.getCIntegerField(\"_type\");\n-        seqnumField = type.getCIntegerField(\"_seqnum\");\n-        virtualFieldOffset = type.getField(\"_virtual\").getOffset();\n-        topField = type.getAddressField(\"_top\");\n-    }\n-\n-    public ZPage(Address addr) {\n-        super(addr);\n-    }\n-\n-    private byte type() {\n-        return typeField.getJByte(addr);\n-    }\n-\n-    private int seqnum() {\n-        return seqnumField.getJInt(addr);\n-    }\n-\n-    private ZVirtualMemory virtual() {\n-        return VMObjectFactory.newObject(ZVirtualMemory.class, addr.addOffsetTo(virtualFieldOffset));\n-    }\n-\n-    private Address top() {\n-        return topField.getValue(addr);\n-    }\n-\n-    private boolean is_relocatable() {\n-        return seqnum() < ZGlobals.ZGlobalSeqNum();\n-    }\n-\n-    long start() {\n-        return virtual().start();\n-    }\n-\n-    long size() {\n-        return virtual().end() - virtual().start();\n-    }\n-\n-    long object_alignment_shift() {\n-        if (type() == ZGlobals.ZPageTypeSmall) {\n-            return ZGlobals.ZObjectAlignmentSmallShift();\n-        } else if (type() == ZGlobals.ZPageTypeMedium) {\n-            return ZGlobals.ZObjectAlignmentMediumShift;\n-        } else {\n-            assert(type() == ZGlobals.ZPageTypeLarge);\n-            return ZGlobals.ZObjectAlignmentLargeShift;\n-        }\n-    }\n-\n-    long objectAlignmentSize() {\n-        return 1 << object_alignment_shift();\n-    }\n-\n-    public boolean isIn(Address addr) {\n-        long offset = ZAddress.offset(addr);\n-        \/\/ FIXME: it does not consider the sign.\n-        return (offset >= start()) && (offset < top().asLongValue());\n-    }\n-\n-    private long getObjectSize(Address good) {\n-        OopHandle handle = good.addOffsetToAsOopHandle(0);\n-        Oop obj = null;\n-\n-        try {\n-           obj = VM.getVM().getObjectHeap().newOop(handle);\n-        } catch (UnknownOopException exp) {\n-          throw new RuntimeException(\" UnknownOopException  \" + exp);\n-        }\n-\n-        return VM.getVM().alignUp(obj.getObjectSize(), objectAlignmentSize());\n-    }\n-\n-    public List<MemRegion> getLiveRegions() {\n-        Address start = ZAddress.good(ZUtils.longToAddress(start()));\n-\n-        \/\/ Can't convert top() to a \"good\" address because it might\n-        \/\/ be at the top of the \"offset\" range, and therefore also\n-        \/\/ looks like one of the color bits. Instead use the \"good\"\n-        \/\/ address and add the size.\n-        long size = top().asLongValue() - start();\n-        Address end = start.addOffsetTo(size);\n-\n-        return List.of(new MemRegion(start, end));\n-    }\n-}\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/gc\/z\/ZPage.java","additions":0,"deletions":141,"binary":false,"changes":141,"status":"deleted"},{"patch":"@@ -1,176 +0,0 @@\n-\/*\n- * Copyright (c) 2018, 2022, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-package sun.jvm.hotspot.gc.z;\n-\n-import java.util.Iterator;\n-\n-import sun.jvm.hotspot.debugger.Address;\n-import sun.jvm.hotspot.runtime.VM;\n-import sun.jvm.hotspot.runtime.VMObject;\n-import sun.jvm.hotspot.runtime.VMObjectFactory;\n-import sun.jvm.hotspot.types.Type;\n-import sun.jvm.hotspot.types.TypeDataBase;\n-\n-public class ZPageTable extends VMObject {\n-    private static long mapFieldOffset;\n-\n-    static {\n-        VM.registerVMInitializedObserver((o, d) -> initialize(VM.getVM().getTypeDataBase()));\n-    }\n-\n-    private static synchronized void initialize(TypeDataBase db) {\n-        Type type = db.lookupType(\"ZPageTable\");\n-\n-        mapFieldOffset = type.getAddressField(\"_map\").getOffset();\n-    }\n-\n-    public ZPageTable(Address addr) {\n-        super(addr);\n-    }\n-\n-    private ZGranuleMapForPageTable map() {\n-        return VMObjectFactory.newObject(ZGranuleMapForPageTable.class, addr.addOffsetTo(mapFieldOffset));\n-    }\n-\n-    private ZPageTableEntry getEntry(Address o) {\n-        return new ZPageTableEntry(map().get(o));\n-    }\n-\n-    ZPage get(Address o) {\n-        return VMObjectFactory.newObject(ZPage.class, map().get(VM.getVM().getDebugger().newAddress(ZAddress.offset(o))));\n-    }\n-\n-    boolean is_relocating(Address o) {\n-        return getEntry(o).relocating();\n-    }\n-\n-    private class ZPagesIterator implements Iterator<ZPage> {\n-        private ZGranuleMapForPageTable.Iterator mapIter;\n-        private ZPage next;\n-\n-        ZPagesIterator() {\n-            mapIter = map().new Iterator();\n-            positionToNext();\n-        }\n-\n-        private ZPage positionToNext() {\n-            ZPage current = next;\n-\n-            \/\/ Find next\n-            ZPage found = null;\n-            while (mapIter.hasNext()) {\n-                ZPageTableEntry entry = new ZPageTableEntry(mapIter.next());\n-                if (!entry.isEmpty()) {\n-                    ZPage page = entry.page();\n-                    \/\/ Medium pages have repeated entries for all covered slots,\n-                    \/\/ therefore we need to compare against the current page.\n-                    if (page != null && !page.equals(current)) {\n-                        found = page;\n-                        break;\n-                    }\n-                }\n-            }\n-\n-            next = found;\n-\n-            return current;\n-        }\n-\n-        @Override\n-        public boolean hasNext() {\n-            return next != null;\n-        }\n-\n-        @Override\n-        public ZPage next() {\n-            return positionToNext();\n-        }\n-\n-        @Override\n-        public void remove() {\n-            \/* not supported *\/\n-        }\n-    }\n-\n-    abstract class ZPageFilter {\n-        public abstract boolean accept(ZPage page);\n-    }\n-\n-    class ZPagesFilteredIterator implements Iterator<ZPage> {\n-        private ZPage next;\n-        private ZPagesIterator iter = new ZPagesIterator();\n-        private ZPageFilter filter;\n-\n-        ZPagesFilteredIterator(ZPageFilter filter) {\n-            this.filter = filter;\n-            positionToNext();\n-        }\n-\n-        public ZPage positionToNext() {\n-            ZPage current = next;\n-\n-            \/\/ Find next\n-            ZPage found = null;\n-            while (iter.hasNext()) {\n-                ZPage page = iter.next();\n-                if (filter.accept(page)) {\n-                    found = page;\n-                    break;\n-                }\n-            }\n-\n-            next = found;\n-\n-            return current;\n-        }\n-\n-        @Override\n-        public boolean hasNext() {\n-            return next != null;\n-        }\n-\n-        @Override\n-        public ZPage next() {\n-            return positionToNext();\n-        }\n-\n-        @Override\n-        public void remove() {\n-            \/* not supported *\/\n-        }\n-    }\n-\n-    public Iterator<ZPage> iterator() {\n-        return new ZPagesIterator();\n-    }\n-\n-    public Iterator<ZPage> activePagesIterator() {\n-        return new ZPagesFilteredIterator(new ZPageFilter() {\n-            public boolean accept(ZPage page) {\n-                return page != null;\n-            }\n-        });\n-    }\n-}\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/gc\/z\/ZPageTable.java","additions":0,"deletions":176,"binary":false,"changes":176,"status":"deleted"},{"patch":"@@ -1,52 +0,0 @@\n-\/*\n- * Copyright (c) 2018, 2022, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-package sun.jvm.hotspot.gc.z;\n-\n-import sun.jvm.hotspot.debugger.Address;\n-import sun.jvm.hotspot.runtime.VMObjectFactory;\n-\n-class ZPageTableEntry {\n-    Address entry;\n-\n-    ZPageTableEntry(Address address) {\n-        entry = address;\n-    }\n-\n-    ZPage page() {\n-        return VMObjectFactory.newObject(ZPage.class, zPageBits());\n-    }\n-\n-    private Address zPageBits() {\n-        return entry.andWithMask(~1L);\n-    }\n-\n-    boolean relocating() {\n-        return (entry.asLongValue() & 1) == 1;\n-    }\n-\n-    boolean isEmpty() {\n-        return entry == null || zPageBits() == null;\n-    }\n-}\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/gc\/z\/ZPageTableEntry.java","additions":0,"deletions":52,"binary":false,"changes":52,"status":"deleted"},{"patch":"@@ -1,74 +0,0 @@\n-\/*\n- * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2021, NTT DATA.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-package sun.jvm.hotspot.gc.z;\n-\n-import sun.jvm.hotspot.debugger.Address;\n-import sun.jvm.hotspot.runtime.VM;\n-import sun.jvm.hotspot.runtime.VMObject;\n-import sun.jvm.hotspot.types.AddressField;\n-import sun.jvm.hotspot.types.Type;\n-import sun.jvm.hotspot.types.TypeDataBase;\n-\n-public class ZRelocate  extends VMObject {\n-\n-    static {\n-        VM.registerVMInitializedObserver((o, d) -> initialize(VM.getVM().getTypeDataBase()));\n-    }\n-\n-    private static synchronized void initialize(TypeDataBase db) {\n-        Type type = db.lookupType(\"ZRelocate\");\n-    }\n-\n-    public ZRelocate(Address addr) {\n-        super(addr);\n-    }\n-\n-    private long forwardingIndex(ZForwarding forwarding, Address from) {\n-        long fromOffset = ZAddress.offset(from);\n-        return (fromOffset - forwarding.start()) >>> forwarding.objectAlignmentShift();\n-    }\n-\n-    private Address forwardingFind(ZForwarding forwarding, Address from) {\n-        long fromIndex = forwardingIndex(forwarding, from);\n-        ZForwardingEntry entry = forwarding.find(fromIndex);\n-        return entry.populated() ? ZAddress.good(VM.getVM().getDebugger().newAddress(entry.toOffset())) : null;\n-    }\n-\n-    public Address forwardObject(ZForwarding forwarding, Address from) {\n-        return forwardingFind(forwarding, from);\n-    }\n-\n-    public Address relocateObject(ZForwarding forwarding, Address o) {\n-        Address toAddr = forwardingFind(forwarding, o);\n-        if (toAddr != null) {\n-            \/\/ Already relocated.\n-            return toAddr;\n-        } else {\n-            \/\/ Return original address because it is not yet relocated.\n-            return o;\n-        }\n-    }\n-}\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/gc\/z\/ZRelocate.java","additions":0,"deletions":74,"binary":false,"changes":74,"status":"deleted"},{"patch":"@@ -39,0 +39,1 @@\n+import sun.jvm.hotspot.gc.x.XCollectedHeap;\n@@ -89,0 +90,1 @@\n+    addHeapTypeIfInDB(db, XCollectedHeap.class);\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/memory\/Universe.java","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -40,1 +40,1 @@\n-import sun.jvm.hotspot.gc.z.*;\n+import sun.jvm.hotspot.gc.x.*;\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/oops\/ObjectHeap.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+import sun.jvm.hotspot.gc.x.*;\n@@ -146,0 +147,3 @@\n+      } else if (heap instanceof XCollectedHeap) {\n+         XCollectedHeap zheap = (XCollectedHeap) heap;\n+         zheap.printOn(System.out);\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/tools\/HeapSummary.java","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -38,1 +38,0 @@\n-import sun.jvm.hotspot.gc.z.ZCollectedHeap;\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/utilities\/HeapHprofBinWriter.java","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -425,0 +425,5 @@\n+    <event name=\"jdk.GCPhaseConcurrentLevel2\">\n+      <setting name=\"enabled\" control=\"gc-enabled-high\">true<\/setting>\n+      <setting name=\"threshold\">0 ms<\/setting>\n+    <\/event>\n+\n@@ -834,0 +839,10 @@\n+    <event name=\"jdk.ZYoungGarbageCollection\">\n+      <setting name=\"enabled\">true<\/setting>\n+      <setting name=\"threshold\">0 ms<\/setting>\n+    <\/event>\n+\n+    <event name=\"jdk.ZOldGarbageCollection\">\n+      <setting name=\"enabled\">true<\/setting>\n+      <setting name=\"threshold\">0 ms<\/setting>\n+    <\/event>\n+\n","filename":"src\/jdk.jfr\/share\/conf\/jfr\/default.jfc","additions":15,"deletions":0,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -425,0 +425,5 @@\n+    <event name=\"jdk.GCPhaseConcurrentLevel2\">\n+      <setting name=\"enabled\" control=\"gc-enabled-high\">true<\/setting>\n+      <setting name=\"threshold\">0 ms<\/setting>\n+    <\/event>\n+\n@@ -834,0 +839,10 @@\n+    <event name=\"jdk.ZYoungGarbageCollection\">\n+      <setting name=\"enabled\">true<\/setting>\n+      <setting name=\"threshold\">0 ms<\/setting>\n+    <\/event>\n+\n+    <event name=\"jdk.ZOldGarbageCollection\">\n+      <setting name=\"enabled\">true<\/setting>\n+      <setting name=\"threshold\">0 ms<\/setting>\n+    <\/event>\n+\n","filename":"src\/jdk.jfr\/share\/conf\/jfr\/profile.jfc","additions":15,"deletions":0,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -0,0 +1,135 @@\n+\/*\n+ * Copyright (c) 2015, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/x\/xAddress.inline.hpp\"\n+#include \"gc\/x\/xGlobals.hpp\"\n+#include \"unittest.hpp\"\n+\n+class XAddressTest : public ::testing::Test {\n+protected:\n+  static void is_good_bit(uintptr_t bit_mask) {\n+    \/\/ Setup\n+    XAddress::initialize();\n+    XAddress::set_good_mask(bit_mask);\n+\n+    \/\/ Test that a pointer with only the given bit is considered good.\n+    EXPECT_EQ(XAddress::is_good(XAddressMetadataMarked0),  (bit_mask == XAddressMetadataMarked0));\n+    EXPECT_EQ(XAddress::is_good(XAddressMetadataMarked1),  (bit_mask == XAddressMetadataMarked1));\n+    EXPECT_EQ(XAddress::is_good(XAddressMetadataRemapped), (bit_mask == XAddressMetadataRemapped));\n+\n+    \/\/ Test that a pointer with the given bit and some extra bits is considered good.\n+    EXPECT_EQ(XAddress::is_good(XAddressMetadataMarked0  | 0x8),(bit_mask == XAddressMetadataMarked0));\n+    EXPECT_EQ(XAddress::is_good(XAddressMetadataMarked1  | 0x8), (bit_mask == XAddressMetadataMarked1));\n+    EXPECT_EQ(XAddress::is_good(XAddressMetadataRemapped | 0x8), (bit_mask == XAddressMetadataRemapped));\n+\n+    \/\/ Test that null is not considered good.\n+    EXPECT_FALSE(XAddress::is_good(0));\n+  }\n+\n+  static void is_good_or_null_bit(uintptr_t bit_mask) {\n+    \/\/ Setup\n+    XAddress::initialize();\n+    XAddress::set_good_mask(bit_mask);\n+\n+    \/\/ Test that a pointer with only the given bit is considered good.\n+    EXPECT_EQ(XAddress::is_good_or_null(XAddressMetadataMarked0),  (bit_mask == XAddressMetadataMarked0));\n+    EXPECT_EQ(XAddress::is_good_or_null(XAddressMetadataMarked1),  (bit_mask == XAddressMetadataMarked1));\n+    EXPECT_EQ(XAddress::is_good_or_null(XAddressMetadataRemapped), (bit_mask == XAddressMetadataRemapped));\n+\n+    \/\/ Test that a pointer with the given bit and some extra bits is considered good.\n+    EXPECT_EQ(XAddress::is_good_or_null(XAddressMetadataMarked0  | 0x8), (bit_mask == XAddressMetadataMarked0));\n+    EXPECT_EQ(XAddress::is_good_or_null(XAddressMetadataMarked1  | 0x8), (bit_mask == XAddressMetadataMarked1));\n+    EXPECT_EQ(XAddress::is_good_or_null(XAddressMetadataRemapped | 0x8), (bit_mask == XAddressMetadataRemapped));\n+\n+    \/\/ Test that null is considered good_or_null.\n+    EXPECT_TRUE(XAddress::is_good_or_null(0));\n+  }\n+\n+  static void finalizable() {\n+    \/\/ Setup\n+    XAddress::initialize();\n+    XAddress::flip_to_marked();\n+\n+    \/\/ Test that a normal good pointer is good and weak good, but not finalizable\n+    const uintptr_t addr1 = XAddress::good(1);\n+    EXPECT_FALSE(XAddress::is_finalizable(addr1));\n+    EXPECT_TRUE(XAddress::is_marked(addr1));\n+    EXPECT_FALSE(XAddress::is_remapped(addr1));\n+    EXPECT_TRUE(XAddress::is_weak_good(addr1));\n+    EXPECT_TRUE(XAddress::is_weak_good_or_null(addr1));\n+    EXPECT_TRUE(XAddress::is_good(addr1));\n+    EXPECT_TRUE(XAddress::is_good_or_null(addr1));\n+\n+    \/\/ Test that a finalizable good pointer is finalizable and weak good, but not good\n+    const uintptr_t addr2 = XAddress::finalizable_good(1);\n+    EXPECT_TRUE(XAddress::is_finalizable(addr2));\n+    EXPECT_TRUE(XAddress::is_marked(addr2));\n+    EXPECT_FALSE(XAddress::is_remapped(addr2));\n+    EXPECT_TRUE(XAddress::is_weak_good(addr2));\n+    EXPECT_TRUE(XAddress::is_weak_good_or_null(addr2));\n+    EXPECT_FALSE(XAddress::is_good(addr2));\n+    EXPECT_FALSE(XAddress::is_good_or_null(addr2));\n+\n+    \/\/ Flip to remapped and test that it's no longer weak good\n+    XAddress::flip_to_remapped();\n+    EXPECT_TRUE(XAddress::is_finalizable(addr2));\n+    EXPECT_TRUE(XAddress::is_marked(addr2));\n+    EXPECT_FALSE(XAddress::is_remapped(addr2));\n+    EXPECT_FALSE(XAddress::is_weak_good(addr2));\n+    EXPECT_FALSE(XAddress::is_weak_good_or_null(addr2));\n+    EXPECT_FALSE(XAddress::is_good(addr2));\n+    EXPECT_FALSE(XAddress::is_good_or_null(addr2));\n+  }\n+};\n+\n+TEST_F(XAddressTest, is_good) {\n+  is_good_bit(XAddressMetadataMarked0);\n+  is_good_bit(XAddressMetadataMarked1);\n+  is_good_bit(XAddressMetadataRemapped);\n+}\n+\n+TEST_F(XAddressTest, is_good_or_null) {\n+  is_good_or_null_bit(XAddressMetadataMarked0);\n+  is_good_or_null_bit(XAddressMetadataMarked1);\n+  is_good_or_null_bit(XAddressMetadataRemapped);\n+}\n+\n+TEST_F(XAddressTest, is_weak_good_or_null) {\n+#define check_is_weak_good_or_null(value)                                        \\\n+  EXPECT_EQ(XAddress::is_weak_good_or_null(value),                               \\\n+            (XAddress::is_good_or_null(value) || XAddress::is_remapped(value)))  \\\n+    << \"is_good_or_null: \" << XAddress::is_good_or_null(value)                   \\\n+    << \" is_remaped: \" << XAddress::is_remapped(value)                           \\\n+    << \" is_good_or_null_or_remapped: \" << XAddress::is_weak_good_or_null(value)\n+\n+  check_is_weak_good_or_null((uintptr_t)NULL);\n+  check_is_weak_good_or_null(XAddressMetadataMarked0);\n+  check_is_weak_good_or_null(XAddressMetadataMarked1);\n+  check_is_weak_good_or_null(XAddressMetadataRemapped);\n+  check_is_weak_good_or_null((uintptr_t)0x123);\n+}\n+\n+TEST_F(XAddressTest, finalizable) {\n+  finalizable();\n+}\n","filename":"test\/hotspot\/gtest\/gc\/x\/test_xAddress.cpp","additions":135,"deletions":0,"binary":false,"changes":135,"status":"added"},{"patch":"@@ -0,0 +1,83 @@\n+\/*\n+ * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/x\/xArray.inline.hpp\"\n+#include \"unittest.hpp\"\n+\n+TEST(XArray, sanity) {\n+  XArray<int> a;\n+\n+  \/\/ Add elements\n+  for (int i = 0; i < 10; i++) {\n+    a.append(i);\n+  }\n+\n+  XArray<int> b;\n+\n+  b.swap(&a);\n+\n+  \/\/ Check size\n+  ASSERT_EQ(a.length(), 0);\n+  ASSERT_EQ(a.capacity(), 0);\n+  ASSERT_EQ(a.is_empty(), true);\n+\n+  ASSERT_EQ(b.length(), 10);\n+  ASSERT_GE(b.capacity(), 10);\n+  ASSERT_EQ(b.is_empty(), false);\n+\n+  \/\/ Clear elements\n+  a.clear();\n+\n+  \/\/ Check that b is unaffected\n+  ASSERT_EQ(b.length(), 10);\n+  ASSERT_GE(b.capacity(), 10);\n+  ASSERT_EQ(b.is_empty(), false);\n+\n+  a.append(1);\n+\n+  \/\/ Check that b is unaffected\n+  ASSERT_EQ(b.length(), 10);\n+  ASSERT_GE(b.capacity(), 10);\n+  ASSERT_EQ(b.is_empty(), false);\n+}\n+\n+TEST(XArray, iterator) {\n+  XArray<int> a;\n+\n+  \/\/ Add elements\n+  for (int i = 0; i < 10; i++) {\n+    a.append(i);\n+  }\n+\n+  \/\/ Iterate\n+  int count = 0;\n+  XArrayIterator<int> iter(&a);\n+  for (int value; iter.next(&value);) {\n+    ASSERT_EQ(a.at(count), count);\n+    count++;\n+  }\n+\n+  \/\/ Check count\n+  ASSERT_EQ(count, 10);\n+}\n","filename":"test\/hotspot\/gtest\/gc\/x\/test_xArray.cpp","additions":83,"deletions":0,"binary":false,"changes":83,"status":"added"},{"patch":"@@ -0,0 +1,79 @@\n+\/*\n+ * Copyright (c) 2017, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/x\/xBitField.hpp\"\n+#include \"unittest.hpp\"\n+\n+TEST(XBitFieldTest, test) {\n+  typedef XBitField<uint64_t, bool,      0,  1>    field_bool;\n+  typedef XBitField<uint64_t, uint8_t,   1,  8>    field_uint8;\n+  typedef XBitField<uint64_t, uint16_t,  2, 16>    field_uint16;\n+  typedef XBitField<uint64_t, uint32_t, 32, 32>    field_uint32;\n+  typedef XBitField<uint64_t, uint64_t,  0, 63>    field_uint64;\n+  typedef XBitField<uint64_t, void*,     1, 61, 3> field_pointer;\n+\n+  uint64_t entry;\n+\n+  {\n+    const bool value = false;\n+    entry = field_bool::encode(value);\n+    EXPECT_EQ(field_bool::decode(entry), value) << \"Should be equal\";\n+  }\n+\n+  {\n+    const bool value = true;\n+    entry = field_bool::encode(value);\n+      EXPECT_EQ(field_bool::decode(entry), value) << \"Should be equal\";\n+  }\n+\n+  {\n+    const uint8_t value = ~(uint8_t)0;\n+    entry = field_uint8::encode(value);\n+    EXPECT_EQ(field_uint8::decode(entry), value) << \"Should be equal\";\n+  }\n+\n+  {\n+    const uint16_t value = ~(uint16_t)0;\n+    entry = field_uint16::encode(value);\n+    EXPECT_EQ(field_uint16::decode(entry), value) << \"Should be equal\";\n+  }\n+\n+  {\n+    const uint32_t value = ~(uint32_t)0;\n+    entry = field_uint32::encode(value);\n+    EXPECT_EQ(field_uint32::decode(entry), value) << \"Should be equal\";\n+  }\n+\n+  {\n+    const uint64_t value = ~(uint64_t)0 >> 1;\n+    entry = field_uint64::encode(value);\n+    EXPECT_EQ(field_uint64::decode(entry), value) << \"Should be equal\";\n+  }\n+\n+  {\n+    void* const value = (void*)(~(uintptr_t)0 << 3);\n+    entry = field_pointer::encode(value);\n+    EXPECT_EQ(field_pointer::decode(entry), value) << \"Should be equal\";\n+  }\n+}\n","filename":"test\/hotspot\/gtest\/gc\/x\/test_xBitField.cpp","additions":79,"deletions":0,"binary":false,"changes":79,"status":"added"},{"patch":"@@ -0,0 +1,108 @@\n+\/*\n+ * Copyright (c) 2016, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/x\/xBitMap.inline.hpp\"\n+#include \"unittest.hpp\"\n+\n+class XBitMapTest : public ::testing::Test {\n+protected:\n+  static void test_set_pair_unset(size_t size, bool finalizable) {\n+    XBitMap bitmap(size);\n+\n+    for (BitMap::idx_t i = 0; i < size - 1; i++) {\n+      if ((i + 1) % BitsPerWord == 0) {\n+        \/\/ Can't set pairs of bits in different words.\n+        continue;\n+      }\n+\n+      \/\/ XBitMaps are not cleared when constructed.\n+      bitmap.clear();\n+\n+      bool inc_live = false;\n+\n+      bool ret = bitmap.par_set_bit_pair(i, finalizable, inc_live);\n+      EXPECT_TRUE(ret) << \"Failed to set bit\";\n+      EXPECT_TRUE(inc_live) << \"Should have set inc_live\";\n+\n+      \/\/ First bit should always be set\n+      EXPECT_TRUE(bitmap.at(i)) << \"Should be set\";\n+\n+      \/\/ Second bit should only be set when marking strong\n+      EXPECT_NE(bitmap.at(i + 1), finalizable);\n+    }\n+  }\n+\n+  static void test_set_pair_set(size_t size, bool finalizable) {\n+    XBitMap bitmap(size);\n+\n+    for (BitMap::idx_t i = 0; i < size - 1; i++) {\n+      if ((i + 1) % BitsPerWord == 0) {\n+        \/\/ Can't set pairs of bits in different words.\n+        continue;\n+      }\n+\n+      \/\/ Fill the bitmap with ones.\n+      bitmap.set_range(0, size);\n+\n+      bool inc_live = false;\n+\n+      bool ret = bitmap.par_set_bit_pair(i, finalizable, inc_live);\n+      EXPECT_FALSE(ret) << \"Should not succeed setting bit\";\n+      EXPECT_FALSE(inc_live) << \"Should not have set inc_live\";\n+\n+      \/\/ Both bits were pre-set.\n+      EXPECT_TRUE(bitmap.at(i)) << \"Should be set\";\n+      EXPECT_TRUE(bitmap.at(i + 1)) << \"Should be set\";\n+    }\n+  }\n+\n+  static void test_set_pair_set(bool finalizable) {\n+    test_set_pair_set(2,   finalizable);\n+    test_set_pair_set(62,  finalizable);\n+    test_set_pair_set(64,  finalizable);\n+    test_set_pair_set(66,  finalizable);\n+    test_set_pair_set(126, finalizable);\n+    test_set_pair_set(128, finalizable);\n+  }\n+\n+  static void test_set_pair_unset(bool finalizable) {\n+    test_set_pair_unset(2,   finalizable);\n+    test_set_pair_unset(62,  finalizable);\n+    test_set_pair_unset(64,  finalizable);\n+    test_set_pair_unset(66,  finalizable);\n+    test_set_pair_unset(126, finalizable);\n+    test_set_pair_unset(128, finalizable);\n+  }\n+\n+};\n+\n+TEST_F(XBitMapTest, test_set_pair_set) {\n+  test_set_pair_set(false);\n+  test_set_pair_set(true);\n+}\n+\n+TEST_F(XBitMapTest, test_set_pair_unset) {\n+  test_set_pair_unset(false);\n+  test_set_pair_unset(true);\n+}\n","filename":"test\/hotspot\/gtest\/gc\/x\/test_xBitMap.cpp","additions":108,"deletions":0,"binary":false,"changes":108,"status":"added"},{"patch":"@@ -0,0 +1,205 @@\n+\/*\n+ * Copyright (c) 2016, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/x\/xAddress.inline.hpp\"\n+#include \"gc\/x\/xForwarding.inline.hpp\"\n+#include \"gc\/x\/xForwardingAllocator.inline.hpp\"\n+#include \"gc\/x\/xGlobals.hpp\"\n+#include \"gc\/x\/xPage.inline.hpp\"\n+#include \"unittest.hpp\"\n+\n+using namespace testing;\n+\n+#define CAPTURE_DELIM \"\\n\"\n+#define CAPTURE1(expression) #expression << \" evaluates to \" << expression\n+#define CAPTURE2(e0, e1)                 CAPTURE1(e0) << CAPTURE_DELIM << CAPTURE1(e1)\n+\n+#define CAPTURE(expression) CAPTURE1(expression)\n+\n+class XForwardingTest : public Test {\n+public:\n+  \/\/ Helper functions\n+\n+  class SequenceToFromIndex : AllStatic {\n+  public:\n+    static uintptr_t even(size_t sequence_number) {\n+      return sequence_number * 2;\n+    }\n+    static uintptr_t odd(size_t sequence_number) {\n+      return even(sequence_number) + 1;\n+    }\n+    static uintptr_t one_to_one(size_t sequence_number) {\n+      return sequence_number;\n+    }\n+  };\n+\n+  \/\/ Test functions\n+\n+  static void setup(XForwarding* forwarding) {\n+    EXPECT_PRED1(is_power_of_2<size_t>, forwarding->_entries.length()) << CAPTURE(forwarding->_entries.length());\n+  }\n+\n+  static void find_empty(XForwarding* forwarding) {\n+    size_t size = forwarding->_entries.length();\n+    size_t entries_to_check = size * 2;\n+\n+    for (size_t i = 0; i < entries_to_check; i++) {\n+      uintptr_t from_index = SequenceToFromIndex::one_to_one(i);\n+\n+      XForwardingCursor cursor;\n+      XForwardingEntry entry = forwarding->find(from_index, &cursor);\n+      EXPECT_FALSE(entry.populated()) << CAPTURE2(from_index, size);\n+    }\n+  }\n+\n+  static void find_full(XForwarding* forwarding) {\n+    size_t size = forwarding->_entries.length();\n+    size_t entries_to_populate = size;\n+\n+    \/\/ Populate\n+    for (size_t i = 0; i < entries_to_populate; i++) {\n+      uintptr_t from_index = SequenceToFromIndex::one_to_one(i);\n+\n+      XForwardingCursor cursor;\n+      XForwardingEntry entry = forwarding->find(from_index, &cursor);\n+      ASSERT_FALSE(entry.populated()) << CAPTURE2(from_index, size);\n+\n+      forwarding->insert(from_index, from_index, &cursor);\n+    }\n+\n+    \/\/ Verify\n+    for (size_t i = 0; i < entries_to_populate; i++) {\n+      uintptr_t from_index = SequenceToFromIndex::one_to_one(i);\n+\n+      XForwardingCursor cursor;\n+      XForwardingEntry entry = forwarding->find(from_index, &cursor);\n+      ASSERT_TRUE(entry.populated()) << CAPTURE2(from_index, size);\n+\n+      ASSERT_EQ(entry.from_index(), from_index) << CAPTURE(size);\n+      ASSERT_EQ(entry.to_offset(), from_index) << CAPTURE(size);\n+    }\n+  }\n+\n+  static void find_every_other(XForwarding* forwarding) {\n+    size_t size = forwarding->_entries.length();\n+    size_t entries_to_populate = size \/ 2;\n+\n+    \/\/ Populate even from indices\n+    for (size_t i = 0; i < entries_to_populate; i++) {\n+      uintptr_t from_index = SequenceToFromIndex::even(i);\n+\n+      XForwardingCursor cursor;\n+      XForwardingEntry entry = forwarding->find(from_index, &cursor);\n+      ASSERT_FALSE(entry.populated()) << CAPTURE2(from_index, size);\n+\n+      forwarding->insert(from_index, from_index, &cursor);\n+    }\n+\n+    \/\/ Verify populated even indices\n+    for (size_t i = 0; i < entries_to_populate; i++) {\n+      uintptr_t from_index = SequenceToFromIndex::even(i);\n+\n+      XForwardingCursor cursor;\n+      XForwardingEntry entry = forwarding->find(from_index, &cursor);\n+      ASSERT_TRUE(entry.populated()) << CAPTURE2(from_index, size);\n+\n+      ASSERT_EQ(entry.from_index(), from_index) << CAPTURE(size);\n+      ASSERT_EQ(entry.to_offset(), from_index) << CAPTURE(size);\n+    }\n+\n+    \/\/ Verify empty odd indices\n+    \/\/\n+    \/\/ This check could be done on a larger range of sequence numbers,\n+    \/\/ but currently entries_to_populate is used.\n+    for (size_t i = 0; i < entries_to_populate; i++) {\n+      uintptr_t from_index = SequenceToFromIndex::odd(i);\n+\n+      XForwardingCursor cursor;\n+      XForwardingEntry entry = forwarding->find(from_index, &cursor);\n+\n+      ASSERT_FALSE(entry.populated()) << CAPTURE2(from_index, size);\n+    }\n+  }\n+\n+  static void test(void (*function)(XForwarding*), uint32_t size) {\n+    \/\/ Create page\n+    const XVirtualMemory vmem(0, XPageSizeSmall);\n+    const XPhysicalMemory pmem(XPhysicalMemorySegment(0, XPageSizeSmall, true));\n+    XPage page(XPageTypeSmall, vmem, pmem);\n+\n+    page.reset();\n+\n+    const size_t object_size = 16;\n+    const uintptr_t object = page.alloc_object(object_size);\n+\n+    XGlobalSeqNum++;\n+\n+    bool dummy = false;\n+    page.mark_object(XAddress::marked(object), dummy, dummy);\n+\n+    const uint32_t live_objects = size;\n+    const size_t live_bytes = live_objects * object_size;\n+    page.inc_live(live_objects, live_bytes);\n+\n+    \/\/ Setup allocator\n+    XForwardingAllocator allocator;\n+    const uint32_t nentries = XForwarding::nentries(&page);\n+    allocator.reset((sizeof(XForwarding)) + (nentries * sizeof(XForwardingEntry)));\n+\n+    \/\/ Setup forwarding\n+    XForwarding* const forwarding = XForwarding::alloc(&allocator, &page);\n+\n+    \/\/ Actual test function\n+    (*function)(forwarding);\n+  }\n+\n+  \/\/ Run the given function with a few different input values.\n+  static void test(void (*function)(XForwarding*)) {\n+    test(function, 1);\n+    test(function, 2);\n+    test(function, 3);\n+    test(function, 4);\n+    test(function, 7);\n+    test(function, 8);\n+    test(function, 1023);\n+    test(function, 1024);\n+    test(function, 1025);\n+  }\n+};\n+\n+TEST_F(XForwardingTest, setup) {\n+  test(&XForwardingTest::setup);\n+}\n+\n+TEST_F(XForwardingTest, find_empty) {\n+  test(&XForwardingTest::find_empty);\n+}\n+\n+TEST_F(XForwardingTest, find_full) {\n+  test(&XForwardingTest::find_full);\n+}\n+\n+TEST_F(XForwardingTest, find_every_other) {\n+  test(&XForwardingTest::find_every_other);\n+}\n","filename":"test\/hotspot\/gtest\/gc\/x\/test_xForwarding.cpp","additions":205,"deletions":0,"binary":false,"changes":205,"status":"added"},{"patch":"@@ -0,0 +1,155 @@\n+\/*\n+ * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/x\/xList.inline.hpp\"\n+#include \"unittest.hpp\"\n+\n+#ifndef PRODUCT\n+\n+class XTestEntry {\n+  friend class XList<XTestEntry>;\n+\n+private:\n+  const int             _id;\n+  XListNode<XTestEntry> _node;\n+\n+public:\n+  XTestEntry(int id) :\n+      _id(id),\n+      _node() {}\n+\n+  int id() const {\n+    return _id;\n+  }\n+};\n+\n+class XListTest : public ::testing::Test {\n+protected:\n+  static void assert_sorted(XList<XTestEntry>* list) {\n+    \/\/ Iterate forward\n+    {\n+      int count = list->first()->id();\n+      XListIterator<XTestEntry> iter(list);\n+      for (XTestEntry* entry; iter.next(&entry);) {\n+        ASSERT_EQ(entry->id(), count);\n+        count++;\n+      }\n+    }\n+\n+    \/\/ Iterate backward\n+    {\n+      int count = list->last()->id();\n+      XListReverseIterator<XTestEntry> iter(list);\n+      for (XTestEntry* entry; iter.next(&entry);) {\n+        EXPECT_EQ(entry->id(), count);\n+        count--;\n+      }\n+    }\n+  }\n+};\n+\n+TEST_F(XListTest, test_insert) {\n+  XList<XTestEntry> list;\n+  XTestEntry e0(0);\n+  XTestEntry e1(1);\n+  XTestEntry e2(2);\n+  XTestEntry e3(3);\n+  XTestEntry e4(4);\n+  XTestEntry e5(5);\n+\n+  list.insert_first(&e2);\n+  list.insert_before(&e2, &e1);\n+  list.insert_after(&e2, &e3);\n+  list.insert_last(&e4);\n+  list.insert_first(&e0);\n+  list.insert_last(&e5);\n+\n+  EXPECT_EQ(list.size(), 6u);\n+  assert_sorted(&list);\n+\n+  for (int i = 0; i < 6; i++) {\n+    XTestEntry* e = list.remove_first();\n+    EXPECT_EQ(e->id(), i);\n+  }\n+\n+  EXPECT_EQ(list.size(), 0u);\n+}\n+\n+TEST_F(XListTest, test_remove) {\n+  \/\/ Remove first\n+  {\n+    XList<XTestEntry> list;\n+    XTestEntry e0(0);\n+    XTestEntry e1(1);\n+    XTestEntry e2(2);\n+    XTestEntry e3(3);\n+    XTestEntry e4(4);\n+    XTestEntry e5(5);\n+\n+    list.insert_last(&e0);\n+    list.insert_last(&e1);\n+    list.insert_last(&e2);\n+    list.insert_last(&e3);\n+    list.insert_last(&e4);\n+    list.insert_last(&e5);\n+\n+    EXPECT_EQ(list.size(), 6u);\n+\n+    for (int i = 0; i < 6; i++) {\n+      XTestEntry* e = list.remove_first();\n+      EXPECT_EQ(e->id(), i);\n+    }\n+\n+    EXPECT_EQ(list.size(), 0u);\n+  }\n+\n+  \/\/ Remove last\n+  {\n+    XList<XTestEntry> list;\n+    XTestEntry e0(0);\n+    XTestEntry e1(1);\n+    XTestEntry e2(2);\n+    XTestEntry e3(3);\n+    XTestEntry e4(4);\n+    XTestEntry e5(5);\n+\n+    list.insert_last(&e0);\n+    list.insert_last(&e1);\n+    list.insert_last(&e2);\n+    list.insert_last(&e3);\n+    list.insert_last(&e4);\n+    list.insert_last(&e5);\n+\n+    EXPECT_EQ(list.size(), 6u);\n+\n+    for (int i = 5; i >= 0; i--) {\n+      XTestEntry* e = list.remove_last();\n+      EXPECT_EQ(e->id(), i);\n+    }\n+\n+    EXPECT_EQ(list.size(), 0u);\n+  }\n+}\n+\n+#endif \/\/ PRODUCT\n","filename":"test\/hotspot\/gtest\/gc\/x\/test_xList.cpp","additions":155,"deletions":0,"binary":false,"changes":155,"status":"added"},{"patch":"@@ -0,0 +1,55 @@\n+\/*\n+ * Copyright (c) 2017, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/x\/xLiveMap.inline.hpp\"\n+#include \"unittest.hpp\"\n+\n+class XLiveMapTest : public ::testing::Test {\n+protected:\n+  static void strongly_live_for_large_xpage() {\n+    \/\/ Large XPages only have room for one object.\n+    XLiveMap livemap(1);\n+\n+    bool inc_live;\n+    uintptr_t object = 0u;\n+\n+    \/\/ Mark the object strong.\n+    livemap.set(object, false \/* finalizable *\/, inc_live);\n+\n+    \/\/ Check that both bits are in the same segment.\n+    ASSERT_EQ(livemap.index_to_segment(0), livemap.index_to_segment(1));\n+\n+    \/\/ Check that the object was marked.\n+    ASSERT_TRUE(livemap.get(0));\n+\n+    \/\/ Check that the object was strongly marked.\n+    ASSERT_TRUE(livemap.get(1));\n+\n+    ASSERT_TRUE(inc_live);\n+  }\n+};\n+\n+TEST_F(XLiveMapTest, strongly_live_for_large_xpage) {\n+  strongly_live_for_large_xpage();\n+}\n","filename":"test\/hotspot\/gtest\/gc\/x\/test_xLiveMap.cpp","additions":55,"deletions":0,"binary":false,"changes":55,"status":"added"},{"patch":"@@ -0,0 +1,174 @@\n+\/*\n+ * Copyright (c) 2016, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/x\/xPhysicalMemory.inline.hpp\"\n+#include \"unittest.hpp\"\n+\n+TEST(XPhysicalMemoryTest, copy) {\n+  const XPhysicalMemorySegment seg0(0, 100, true);\n+  const XPhysicalMemorySegment seg1(200, 100, true);\n+\n+  XPhysicalMemory pmem0;\n+  pmem0.add_segment(seg0);\n+  EXPECT_EQ(pmem0.nsegments(), 1);\n+  EXPECT_EQ(pmem0.segment(0).size(), 100u);\n+\n+  XPhysicalMemory pmem1;\n+  pmem1.add_segment(seg0);\n+  pmem1.add_segment(seg1);\n+  EXPECT_EQ(pmem1.nsegments(), 2);\n+  EXPECT_EQ(pmem1.segment(0).size(), 100u);\n+  EXPECT_EQ(pmem1.segment(1).size(), 100u);\n+\n+  XPhysicalMemory pmem2(pmem0);\n+  EXPECT_EQ(pmem2.nsegments(), 1);\n+  EXPECT_EQ(pmem2.segment(0).size(), 100u);\n+\n+  pmem2 = pmem1;\n+  EXPECT_EQ(pmem2.nsegments(), 2);\n+  EXPECT_EQ(pmem2.segment(0).size(), 100u);\n+  EXPECT_EQ(pmem2.segment(1).size(), 100u);\n+}\n+\n+TEST(XPhysicalMemoryTest, add) {\n+  const XPhysicalMemorySegment seg0(0, 1, true);\n+  const XPhysicalMemorySegment seg1(1, 1, true);\n+  const XPhysicalMemorySegment seg2(2, 1, true);\n+  const XPhysicalMemorySegment seg3(3, 1, true);\n+  const XPhysicalMemorySegment seg4(4, 1, true);\n+  const XPhysicalMemorySegment seg5(5, 1, true);\n+  const XPhysicalMemorySegment seg6(6, 1, true);\n+\n+  XPhysicalMemory pmem0;\n+  EXPECT_EQ(pmem0.nsegments(), 0);\n+  EXPECT_EQ(pmem0.is_null(), true);\n+\n+  XPhysicalMemory pmem1;\n+  pmem1.add_segment(seg0);\n+  pmem1.add_segment(seg1);\n+  pmem1.add_segment(seg2);\n+  pmem1.add_segment(seg3);\n+  pmem1.add_segment(seg4);\n+  pmem1.add_segment(seg5);\n+  pmem1.add_segment(seg6);\n+  EXPECT_EQ(pmem1.nsegments(), 1);\n+  EXPECT_EQ(pmem1.segment(0).size(), 7u);\n+  EXPECT_EQ(pmem1.is_null(), false);\n+\n+  XPhysicalMemory pmem2;\n+  pmem2.add_segment(seg0);\n+  pmem2.add_segment(seg1);\n+  pmem2.add_segment(seg2);\n+  pmem2.add_segment(seg4);\n+  pmem2.add_segment(seg5);\n+  pmem2.add_segment(seg6);\n+  EXPECT_EQ(pmem2.nsegments(), 2);\n+  EXPECT_EQ(pmem2.segment(0).size(), 3u);\n+  EXPECT_EQ(pmem2.segment(1).size(), 3u);\n+  EXPECT_EQ(pmem2.is_null(), false);\n+\n+  XPhysicalMemory pmem3;\n+  pmem3.add_segment(seg0);\n+  pmem3.add_segment(seg2);\n+  pmem3.add_segment(seg3);\n+  pmem3.add_segment(seg4);\n+  pmem3.add_segment(seg6);\n+  EXPECT_EQ(pmem3.nsegments(), 3);\n+  EXPECT_EQ(pmem3.segment(0).size(), 1u);\n+  EXPECT_EQ(pmem3.segment(1).size(), 3u);\n+  EXPECT_EQ(pmem3.segment(2).size(), 1u);\n+  EXPECT_EQ(pmem3.is_null(), false);\n+\n+  XPhysicalMemory pmem4;\n+  pmem4.add_segment(seg0);\n+  pmem4.add_segment(seg2);\n+  pmem4.add_segment(seg4);\n+  pmem4.add_segment(seg6);\n+  EXPECT_EQ(pmem4.nsegments(), 4);\n+  EXPECT_EQ(pmem4.segment(0).size(), 1u);\n+  EXPECT_EQ(pmem4.segment(1).size(), 1u);\n+  EXPECT_EQ(pmem4.segment(2).size(), 1u);\n+  EXPECT_EQ(pmem4.segment(3).size(), 1u);\n+  EXPECT_EQ(pmem4.is_null(), false);\n+}\n+\n+TEST(XPhysicalMemoryTest, remove) {\n+  XPhysicalMemory pmem;\n+\n+  pmem.add_segment(XPhysicalMemorySegment(10, 10, true));\n+  pmem.add_segment(XPhysicalMemorySegment(30, 10, true));\n+  pmem.add_segment(XPhysicalMemorySegment(50, 10, true));\n+  EXPECT_EQ(pmem.nsegments(), 3);\n+  EXPECT_EQ(pmem.size(), 30u);\n+  EXPECT_FALSE(pmem.is_null());\n+\n+  pmem.remove_segments();\n+  EXPECT_EQ(pmem.nsegments(), 0);\n+  EXPECT_EQ(pmem.size(), 0u);\n+  EXPECT_TRUE(pmem.is_null());\n+}\n+\n+TEST(XPhysicalMemoryTest, split) {\n+  XPhysicalMemory pmem;\n+\n+  pmem.add_segment(XPhysicalMemorySegment(0, 10, true));\n+  pmem.add_segment(XPhysicalMemorySegment(10, 10, true));\n+  pmem.add_segment(XPhysicalMemorySegment(30, 10, true));\n+  EXPECT_EQ(pmem.nsegments(), 2);\n+  EXPECT_EQ(pmem.size(), 30u);\n+\n+  XPhysicalMemory pmem0 = pmem.split(1);\n+  EXPECT_EQ(pmem0.nsegments(), 1);\n+  EXPECT_EQ(pmem0.size(), 1u);\n+  EXPECT_EQ(pmem.nsegments(), 2);\n+  EXPECT_EQ(pmem.size(), 29u);\n+\n+  XPhysicalMemory pmem1 = pmem.split(25);\n+  EXPECT_EQ(pmem1.nsegments(), 2);\n+  EXPECT_EQ(pmem1.size(), 25u);\n+  EXPECT_EQ(pmem.nsegments(), 1);\n+  EXPECT_EQ(pmem.size(), 4u);\n+\n+  XPhysicalMemory pmem2 = pmem.split(4);\n+  EXPECT_EQ(pmem2.nsegments(), 1);\n+  EXPECT_EQ(pmem2.size(), 4u);\n+  EXPECT_EQ(pmem.nsegments(), 0);\n+  EXPECT_EQ(pmem.size(), 0u);\n+}\n+\n+TEST(XPhysicalMemoryTest, split_committed) {\n+  XPhysicalMemory pmem0;\n+  pmem0.add_segment(XPhysicalMemorySegment(0, 10, true));\n+  pmem0.add_segment(XPhysicalMemorySegment(10, 10, false));\n+  pmem0.add_segment(XPhysicalMemorySegment(20, 10, true));\n+  pmem0.add_segment(XPhysicalMemorySegment(30, 10, false));\n+  EXPECT_EQ(pmem0.nsegments(), 4);\n+  EXPECT_EQ(pmem0.size(), 40u);\n+\n+  XPhysicalMemory pmem1 = pmem0.split_committed();\n+  EXPECT_EQ(pmem0.nsegments(), 2);\n+  EXPECT_EQ(pmem0.size(), 20u);\n+  EXPECT_EQ(pmem1.nsegments(), 2);\n+  EXPECT_EQ(pmem1.size(), 20u);\n+}\n","filename":"test\/hotspot\/gtest\/gc\/x\/test_xPhysicalMemory.cpp","additions":174,"deletions":0,"binary":false,"changes":174,"status":"added"},{"patch":"@@ -0,0 +1,45 @@\n+\/*\n+ * Copyright (c) 2016, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/x\/xVirtualMemory.inline.hpp\"\n+#include \"unittest.hpp\"\n+\n+TEST(XVirtualMemory, split) {\n+  XVirtualMemory vmem(0, 10);\n+\n+  XVirtualMemory vmem0 = vmem.split(0);\n+  EXPECT_EQ(vmem0.size(), 0u);\n+  EXPECT_EQ(vmem.size(), 10u);\n+\n+  XVirtualMemory vmem1 = vmem.split(5);\n+  EXPECT_EQ(vmem1.size(), 5u);\n+  EXPECT_EQ(vmem.size(), 5u);\n+\n+  XVirtualMemory vmem2 = vmem.split(5);\n+  EXPECT_EQ(vmem2.size(), 5u);\n+  EXPECT_EQ(vmem.size(), 0u);\n+\n+  XVirtualMemory vmem3 = vmem.split(0);\n+  EXPECT_EQ(vmem3.size(), 0u);\n+}\n","filename":"test\/hotspot\/gtest\/gc\/x\/test_xVirtualMemory.cpp","additions":45,"deletions":0,"binary":false,"changes":45,"status":"added"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -31,4 +31,23 @@\n-  static void is_good_bit(uintptr_t bit_mask) {\n-    \/\/ Setup\n-    ZAddress::initialize();\n-    ZAddress::set_good_mask(bit_mask);\n+  static zpointer color(uintptr_t value, uintptr_t color) {\n+    return ZAddress::color(zaddress(value | ZAddressHeapBase), color);\n+  }\n+\n+  static const uintptr_t valid_value = (1 << 3 \/* LogMinObjectAlignment *\/);\n+  static const uintptr_t null_value = 0;\n+\n+  enum ZColor {\n+    Uncolored,\n+    RemappedYoung0,\n+    RemappedYoung1,\n+    RemappedOld0,\n+    RemappedOld1,\n+    MarkedYoung0,\n+    MarkedYoung1,\n+    MarkedOld0,\n+    MarkedOld1,\n+    Finalizable0,\n+    Finalizable1,\n+    Remembered0,\n+    Remembered1,\n+    Remembered11\n+  };\n@@ -36,4 +55,32 @@\n-    \/\/ Test that a pointer with only the given bit is considered good.\n-    EXPECT_EQ(ZAddress::is_good(ZAddressMetadataMarked0),  (bit_mask == ZAddressMetadataMarked0));\n-    EXPECT_EQ(ZAddress::is_good(ZAddressMetadataMarked1),  (bit_mask == ZAddressMetadataMarked1));\n-    EXPECT_EQ(ZAddress::is_good(ZAddressMetadataRemapped), (bit_mask == ZAddressMetadataRemapped));\n+  static uintptr_t make_color(ZColor remembered, ZColor remapped_young, ZColor remapped_old, ZColor marked_young, ZColor marked_old) {\n+    uintptr_t color = 0;\n+    switch (remapped_young) {\n+    case RemappedYoung0: {\n+      switch (remapped_old) {\n+      case RemappedOld0:\n+        color |= ZPointer::remap_bits(ZPointerRemapped00);\n+        break;\n+      case RemappedOld1:\n+        color |= ZPointer::remap_bits(ZPointerRemapped10);\n+        break;\n+      default:\n+        EXPECT_TRUE(false);\n+      }\n+      break;\n+    }\n+    case RemappedYoung1: {\n+      switch (remapped_old) {\n+      case RemappedOld0:\n+        color |= ZPointer::remap_bits(ZPointerRemapped01);\n+        break;\n+      case RemappedOld1:\n+        color |= ZPointer::remap_bits(ZPointerRemapped11);\n+        break;\n+      default:\n+        EXPECT_TRUE(false);\n+      }\n+      break;\n+    }\n+    default:\n+      EXPECT_TRUE(false);\n+    }\n@@ -41,4 +88,10 @@\n-    \/\/ Test that a pointer with the given bit and some extra bits is considered good.\n-    EXPECT_EQ(ZAddress::is_good(ZAddressMetadataMarked0  | 0x8),(bit_mask == ZAddressMetadataMarked0));\n-    EXPECT_EQ(ZAddress::is_good(ZAddressMetadataMarked1  | 0x8), (bit_mask == ZAddressMetadataMarked1));\n-    EXPECT_EQ(ZAddress::is_good(ZAddressMetadataRemapped | 0x8), (bit_mask == ZAddressMetadataRemapped));\n+    switch (marked_young) {\n+    case MarkedYoung0:\n+      color |= ZPointerMarkedYoung0;\n+      break;\n+    case MarkedYoung1:\n+      color |= ZPointerMarkedYoung1;\n+      break;\n+    default:\n+      EXPECT_TRUE(false);\n+    }\n@@ -46,2 +99,32 @@\n-    \/\/ Test that null is not considered good.\n-    EXPECT_FALSE(ZAddress::is_good(0));\n+    switch (marked_old) {\n+    case MarkedOld0:\n+      color |= ZPointerMarkedOld0;\n+      break;\n+    case MarkedOld1:\n+      color |= ZPointerMarkedOld1;\n+      break;\n+    case Finalizable0:\n+      color |= ZPointerFinalizable0;\n+      break;\n+    case Finalizable1:\n+      color |= ZPointerFinalizable1;\n+      break;\n+    default:\n+      EXPECT_TRUE(false);\n+    }\n+\n+    switch (remembered) {\n+    case Remembered0:\n+      color |= ZPointerRemembered0;\n+      break;\n+    case Remembered1:\n+      color |= ZPointerRemembered1;\n+      break;\n+    case Remembered11:\n+      color |= ZPointerRemembered0 | ZPointerRemembered1;\n+      break;\n+    default:\n+      EXPECT_TRUE(false);\n+    }\n+\n+    return color;\n@@ -50,4 +133,16 @@\n-  static void is_good_or_null_bit(uintptr_t bit_mask) {\n-    \/\/ Setup\n-    ZAddress::initialize();\n-    ZAddress::set_good_mask(bit_mask);\n+  static zpointer color(uintptr_t addr,\n+                        ZColor remembered,\n+                        ZColor remapped_young,\n+                        ZColor remapped_old,\n+                        ZColor marked_young,\n+                        ZColor marked_old) {\n+    if (remembered == Uncolored &&\n+        remapped_young == Uncolored &&\n+        remapped_old == Uncolored &&\n+        marked_young == Uncolored &&\n+        marked_old == Uncolored) {\n+      return zpointer(addr);\n+    } else {\n+      return color(addr, make_color(remembered, remapped_young, remapped_old, marked_young, marked_old));\n+    }\n+  }\n@@ -55,4 +150,3 @@\n-    \/\/ Test that a pointer with only the given bit is considered good.\n-    EXPECT_EQ(ZAddress::is_good_or_null(ZAddressMetadataMarked0),  (bit_mask == ZAddressMetadataMarked0));\n-    EXPECT_EQ(ZAddress::is_good_or_null(ZAddressMetadataMarked1),  (bit_mask == ZAddressMetadataMarked1));\n-    EXPECT_EQ(ZAddress::is_good_or_null(ZAddressMetadataRemapped), (bit_mask == ZAddressMetadataRemapped));\n+  static bool is_remapped_young_odd(uintptr_t bits) {\n+    return ZPointer::remap_bits(bits) & (ZPointerRemapped01 | ZPointerRemapped11);\n+  }\n@@ -60,4 +154,3 @@\n-    \/\/ Test that a pointer with the given bit and some extra bits is considered good.\n-    EXPECT_EQ(ZAddress::is_good_or_null(ZAddressMetadataMarked0  | 0x8), (bit_mask == ZAddressMetadataMarked0));\n-    EXPECT_EQ(ZAddress::is_good_or_null(ZAddressMetadataMarked1  | 0x8), (bit_mask == ZAddressMetadataMarked1));\n-    EXPECT_EQ(ZAddress::is_good_or_null(ZAddressMetadataRemapped | 0x8), (bit_mask == ZAddressMetadataRemapped));\n+  static bool is_remapped_old_odd(uintptr_t bits) {\n+    return ZPointer::remap_bits(bits) & (ZPointerRemapped10 | ZPointerRemapped11);\n+  }\n@@ -65,2 +158,2 @@\n-    \/\/ Test that null is considered good_or_null.\n-    EXPECT_TRUE(ZAddress::is_good_or_null(0));\n+  static bool is_marked_young_odd(uintptr_t bits) {\n+    return bits & ZPointerMarkedYoung1;\n@@ -69,34 +162,2 @@\n-  static void finalizable() {\n-    \/\/ Setup\n-    ZAddress::initialize();\n-    ZAddress::flip_to_marked();\n-\n-    \/\/ Test that a normal good pointer is good and weak good, but not finalizable\n-    const uintptr_t addr1 = ZAddress::good(1);\n-    EXPECT_FALSE(ZAddress::is_finalizable(addr1));\n-    EXPECT_TRUE(ZAddress::is_marked(addr1));\n-    EXPECT_FALSE(ZAddress::is_remapped(addr1));\n-    EXPECT_TRUE(ZAddress::is_weak_good(addr1));\n-    EXPECT_TRUE(ZAddress::is_weak_good_or_null(addr1));\n-    EXPECT_TRUE(ZAddress::is_good(addr1));\n-    EXPECT_TRUE(ZAddress::is_good_or_null(addr1));\n-\n-    \/\/ Test that a finalizable good pointer is finalizable and weak good, but not good\n-    const uintptr_t addr2 = ZAddress::finalizable_good(1);\n-    EXPECT_TRUE(ZAddress::is_finalizable(addr2));\n-    EXPECT_TRUE(ZAddress::is_marked(addr2));\n-    EXPECT_FALSE(ZAddress::is_remapped(addr2));\n-    EXPECT_TRUE(ZAddress::is_weak_good(addr2));\n-    EXPECT_TRUE(ZAddress::is_weak_good_or_null(addr2));\n-    EXPECT_FALSE(ZAddress::is_good(addr2));\n-    EXPECT_FALSE(ZAddress::is_good_or_null(addr2));\n-\n-    \/\/ Flip to remapped and test that it's no longer weak good\n-    ZAddress::flip_to_remapped();\n-    EXPECT_TRUE(ZAddress::is_finalizable(addr2));\n-    EXPECT_TRUE(ZAddress::is_marked(addr2));\n-    EXPECT_FALSE(ZAddress::is_remapped(addr2));\n-    EXPECT_FALSE(ZAddress::is_weak_good(addr2));\n-    EXPECT_FALSE(ZAddress::is_weak_good_or_null(addr2));\n-    EXPECT_FALSE(ZAddress::is_good(addr2));\n-    EXPECT_FALSE(ZAddress::is_good_or_null(addr2));\n+  static bool is_marked_old_odd(uintptr_t bits) {\n+    return bits & (ZPointerMarkedOld1 | ZPointerFinalizable1);\n@@ -104,1 +165,0 @@\n-};\n@@ -106,5 +166,3 @@\n-TEST_F(ZAddressTest, is_good) {\n-  is_good_bit(ZAddressMetadataMarked0);\n-  is_good_bit(ZAddressMetadataMarked1);\n-  is_good_bit(ZAddressMetadataRemapped);\n-}\n+  static bool is_remembered(uintptr_t bits) {\n+    return bits & (ZPointerRemembered0 | ZPointerRemembered1);\n+  }\n@@ -112,5 +170,3 @@\n-TEST_F(ZAddressTest, is_good_or_null) {\n-  is_good_or_null_bit(ZAddressMetadataMarked0);\n-  is_good_or_null_bit(ZAddressMetadataMarked1);\n-  is_good_or_null_bit(ZAddressMetadataRemapped);\n-}\n+  static bool is_remembered_odd(uintptr_t bits) {\n+    return bits & (ZPointerRemembered1);\n+  }\n@@ -118,14 +174,260 @@\n-TEST_F(ZAddressTest, is_weak_good_or_null) {\n-#define check_is_weak_good_or_null(value)                                        \\\n-  EXPECT_EQ(ZAddress::is_weak_good_or_null(value),                               \\\n-            (ZAddress::is_good_or_null(value) || ZAddress::is_remapped(value)))  \\\n-    << \"is_good_or_null: \" << ZAddress::is_good_or_null(value)                   \\\n-    << \" is_remaped: \" << ZAddress::is_remapped(value)                           \\\n-    << \" is_good_or_null_or_remapped: \" << ZAddress::is_weak_good_or_null(value)\n-\n-  check_is_weak_good_or_null((uintptr_t)NULL);\n-  check_is_weak_good_or_null(ZAddressMetadataMarked0);\n-  check_is_weak_good_or_null(ZAddressMetadataMarked1);\n-  check_is_weak_good_or_null(ZAddressMetadataRemapped);\n-  check_is_weak_good_or_null((uintptr_t)0x123);\n-}\n+  static bool is_remembered_even(uintptr_t bits) {\n+    return bits & (ZPointerRemembered0);\n+  }\n+\n+  static void test_is_checks_on(uintptr_t value,\n+                                ZColor remembered,\n+                                ZColor remapped_young,\n+                                ZColor remapped_old,\n+                                ZColor marked_young,\n+                                ZColor marked_old) {\n+    const zpointer ptr = color(value, remembered, remapped_young, remapped_old, marked_young, marked_old);\n+    uintptr_t ptr_raw = untype(ptr);\n+\n+    EXPECT_TRUE(ZPointerLoadGoodMask != 0);\n+    EXPECT_TRUE(ZPointerStoreGoodMask != 0);\n+\n+    bool ptr_raw_null = ptr_raw == 0;\n+    bool global_remapped_old_odd = is_remapped_old_odd(ZPointerLoadGoodMask);\n+    bool global_remapped_young_odd = is_remapped_young_odd(ZPointerLoadGoodMask);\n+    bool global_marked_old_odd = is_marked_old_odd(ZPointerStoreGoodMask);\n+    bool global_marked_young_odd = is_marked_young_odd(ZPointerStoreGoodMask);\n+    bool global_remembered_odd = is_remembered_odd(ZPointerStoreGoodMask);\n+    bool global_remembered_even = is_remembered_even(ZPointerStoreGoodMask);\n+\n+    if (ptr_raw_null) {\n+      EXPECT_FALSE(ZPointer::is_marked_any_old(ptr));\n+      EXPECT_FALSE(ZPointer::is_load_good(ptr));\n+      EXPECT_TRUE(ZPointer::is_load_good_or_null(ptr));\n+      EXPECT_FALSE(ZPointer::is_load_bad(ptr));\n+      EXPECT_FALSE(ZPointer::is_mark_good(ptr));\n+      EXPECT_TRUE(ZPointer::is_mark_good_or_null(ptr));\n+      EXPECT_FALSE(ZPointer::is_mark_bad(ptr));\n+      EXPECT_FALSE(ZPointer::is_store_good(ptr));\n+      EXPECT_TRUE(ZPointer::is_store_good_or_null(ptr));\n+      EXPECT_FALSE(ZPointer::is_store_bad(ptr));\n+    } else {\n+      bool ptr_remapped_old_odd = is_remapped_old_odd(ptr_raw);\n+      bool ptr_remapped_young_odd = is_remapped_young_odd(ptr_raw);\n+      bool ptr_marked_old_odd = is_marked_old_odd(ptr_raw);\n+      bool ptr_marked_young_odd = is_marked_young_odd(ptr_raw);\n+      bool ptr_final = ptr_raw & (ZPointerFinalizable0 | ZPointerFinalizable1);\n+      bool ptr_remembered = is_power_of_2(ptr_raw & (ZPointerRemembered0 | ZPointerRemembered1));\n+      bool ptr_remembered_odd = is_remembered_odd(ptr_raw);\n+      bool ptr_remembered_even = is_remembered_even(ptr_raw);\n+      bool ptr_colored_null = !ptr_raw_null && (ptr_raw & ~ZPointerAllMetadataMask) == 0;\n+\n+      bool same_old_marking = global_marked_old_odd == ptr_marked_old_odd;\n+      bool same_young_marking = global_marked_young_odd == ptr_marked_young_odd;\n+      bool same_old_remapping = global_remapped_old_odd == ptr_remapped_old_odd;\n+      bool same_young_remapping = global_remapped_young_odd == ptr_remapped_young_odd;\n+      bool same_remembered = ptr_remembered_even == global_remembered_even && ptr_remembered_odd == global_remembered_odd;\n+\n+      EXPECT_EQ(ZPointer::is_marked_finalizable(ptr), same_old_marking && ptr_final);\n+      EXPECT_EQ(ZPointer::is_marked_any_old(ptr), same_old_marking);\n+      EXPECT_EQ(ZPointer::is_remapped(ptr), same_old_remapping && same_young_remapping);\n+      EXPECT_EQ(ZPointer::is_load_good(ptr), same_old_remapping && same_young_remapping);\n+      EXPECT_EQ(ZPointer::is_load_good_or_null(ptr), same_old_remapping && same_young_remapping);\n+      EXPECT_EQ(ZPointer::is_load_bad(ptr), !same_old_remapping || !same_young_remapping);\n+      EXPECT_EQ(ZPointer::is_mark_good(ptr), same_young_remapping && same_old_remapping && same_young_marking && same_old_marking);\n+      EXPECT_EQ(ZPointer::is_mark_good_or_null(ptr), same_young_remapping && same_old_remapping && same_young_marking && same_old_marking);\n+      EXPECT_EQ(ZPointer::is_mark_bad(ptr), !same_young_remapping || !same_old_remapping || !same_young_marking || !same_old_marking);\n+      EXPECT_EQ(ZPointer::is_store_good(ptr), same_young_remapping && same_old_remapping && same_young_marking && same_old_marking && ptr_remembered && same_remembered);\n+      EXPECT_EQ(ZPointer::is_store_good_or_null(ptr), same_young_remapping && same_old_remapping && same_young_marking && same_old_marking && ptr_remembered && same_remembered);\n+      EXPECT_EQ(ZPointer::is_store_bad(ptr), !same_young_remapping || !same_old_remapping || !same_young_marking || !same_old_marking || !ptr_remembered || !same_remembered);\n+    }\n+  }\n+\n+  static void test_is_checks_on_all() {\n+    test_is_checks_on(valid_value, Remembered0, RemappedYoung0, RemappedOld0, MarkedYoung0, MarkedOld0);\n+    test_is_checks_on(null_value, Remembered0, RemappedYoung0, RemappedOld0, MarkedYoung0, MarkedOld0);\n+    test_is_checks_on(valid_value, Remembered0, RemappedYoung0, RemappedOld0, MarkedYoung0, MarkedOld1);\n+    test_is_checks_on(null_value, Remembered0, RemappedYoung0, RemappedOld0, MarkedYoung0, MarkedOld1);\n+    test_is_checks_on(valid_value, Remembered0, RemappedYoung0, RemappedOld0, MarkedYoung1, MarkedOld0);\n+    test_is_checks_on(null_value, Remembered0, RemappedYoung0, RemappedOld0, MarkedYoung1, MarkedOld0);\n+    test_is_checks_on(valid_value, Remembered0, RemappedYoung0, RemappedOld0, MarkedYoung1, MarkedOld1);\n+    test_is_checks_on(null_value, Remembered0, RemappedYoung0, RemappedOld0, MarkedYoung1, MarkedOld1);\n+\n+    test_is_checks_on(valid_value, Remembered0, RemappedYoung0, RemappedOld1, MarkedYoung0, MarkedOld0);\n+    test_is_checks_on(null_value, Remembered0, RemappedYoung0, RemappedOld1, MarkedYoung0, MarkedOld0);\n+    test_is_checks_on(valid_value, Remembered0, RemappedYoung0, RemappedOld1, MarkedYoung0, MarkedOld1);\n+    test_is_checks_on(null_value, Remembered0, RemappedYoung0, RemappedOld1, MarkedYoung0, MarkedOld1);\n+    test_is_checks_on(valid_value, Remembered0, RemappedYoung0, RemappedOld1, MarkedYoung1, MarkedOld0);\n+    test_is_checks_on(null_value, Remembered0, RemappedYoung0, RemappedOld1, MarkedYoung1, MarkedOld0);\n+    test_is_checks_on(valid_value, Remembered0, RemappedYoung0, RemappedOld1, MarkedYoung1, MarkedOld1);\n+    test_is_checks_on(null_value, Remembered0, RemappedYoung0, RemappedOld1, MarkedYoung1, MarkedOld1);\n+\n+    test_is_checks_on(valid_value, Remembered0, RemappedYoung1, RemappedOld0, MarkedYoung0, MarkedOld0);\n+    test_is_checks_on(null_value, Remembered0, RemappedYoung1, RemappedOld0, MarkedYoung0, MarkedOld0);\n+    test_is_checks_on(valid_value, Remembered0, RemappedYoung1, RemappedOld0, MarkedYoung0, MarkedOld1);\n+    test_is_checks_on(null_value, Remembered0, RemappedYoung1, RemappedOld0, MarkedYoung0, MarkedOld1);\n+    test_is_checks_on(valid_value, Remembered0, RemappedYoung1, RemappedOld0, MarkedYoung1, MarkedOld0);\n+    test_is_checks_on(null_value, Remembered0, RemappedYoung1, RemappedOld0, MarkedYoung1, MarkedOld0);\n+    test_is_checks_on(valid_value, Remembered0, RemappedYoung1, RemappedOld0, MarkedYoung1, MarkedOld1);\n+    test_is_checks_on(null_value, Remembered0, RemappedYoung1, RemappedOld0, MarkedYoung1, MarkedOld1);\n+\n+    test_is_checks_on(valid_value, Remembered0, RemappedYoung1, RemappedOld1, MarkedYoung0, MarkedOld0);\n+    test_is_checks_on(null_value, Remembered0, RemappedYoung1, RemappedOld1, MarkedYoung0, MarkedOld0);\n+    test_is_checks_on(valid_value, Remembered0, RemappedYoung1, RemappedOld1, MarkedYoung0, MarkedOld1);\n+    test_is_checks_on(null_value, Remembered0, RemappedYoung1, RemappedOld1, MarkedYoung0, MarkedOld1);\n+    test_is_checks_on(valid_value, Remembered0, RemappedYoung1, RemappedOld1, MarkedYoung1, MarkedOld0);\n+    test_is_checks_on(null_value, Remembered0, RemappedYoung1, RemappedOld1, MarkedYoung1, MarkedOld0);\n+    test_is_checks_on(valid_value, Remembered0, RemappedYoung1, RemappedOld1, MarkedYoung1, MarkedOld1);\n+    test_is_checks_on(null_value, Remembered0, RemappedYoung1, RemappedOld1, MarkedYoung1, MarkedOld1);\n+\n+    test_is_checks_on(valid_value, Remembered1, RemappedYoung0, RemappedOld0, MarkedYoung0, MarkedOld0);\n+    test_is_checks_on(null_value, Remembered1, RemappedYoung0, RemappedOld0, MarkedYoung0, MarkedOld0);\n+    test_is_checks_on(valid_value, Remembered1, RemappedYoung0, RemappedOld0, MarkedYoung0, MarkedOld1);\n+    test_is_checks_on(null_value, Remembered1, RemappedYoung0, RemappedOld0, MarkedYoung0, MarkedOld1);\n+    test_is_checks_on(valid_value, Remembered1, RemappedYoung0, RemappedOld0, MarkedYoung1, MarkedOld0);\n+    test_is_checks_on(null_value, Remembered1, RemappedYoung0, RemappedOld0, MarkedYoung1, MarkedOld0);\n+    test_is_checks_on(valid_value, Remembered1, RemappedYoung0, RemappedOld0, MarkedYoung1, MarkedOld1);\n+    test_is_checks_on(null_value, Remembered1, RemappedYoung0, RemappedOld0, MarkedYoung1, MarkedOld1);\n+\n+    test_is_checks_on(valid_value, Remembered1, RemappedYoung0, RemappedOld1, MarkedYoung0, MarkedOld0);\n+    test_is_checks_on(null_value, Remembered1, RemappedYoung0, RemappedOld1, MarkedYoung0, MarkedOld0);\n+    test_is_checks_on(valid_value, Remembered1, RemappedYoung0, RemappedOld1, MarkedYoung0, MarkedOld1);\n+    test_is_checks_on(null_value, Remembered1, RemappedYoung0, RemappedOld1, MarkedYoung0, MarkedOld1);\n+    test_is_checks_on(valid_value, Remembered1, RemappedYoung0, RemappedOld1, MarkedYoung1, MarkedOld0);\n+    test_is_checks_on(null_value, Remembered1, RemappedYoung0, RemappedOld1, MarkedYoung1, MarkedOld0);\n+    test_is_checks_on(valid_value, Remembered1, RemappedYoung0, RemappedOld1, MarkedYoung1, MarkedOld1);\n+    test_is_checks_on(null_value, Remembered1, RemappedYoung0, RemappedOld1, MarkedYoung1, MarkedOld1);\n+\n+    test_is_checks_on(valid_value, Remembered1, RemappedYoung1, RemappedOld0, MarkedYoung0, MarkedOld0);\n+    test_is_checks_on(null_value, Remembered1, RemappedYoung1, RemappedOld0, MarkedYoung0, MarkedOld0);\n+    test_is_checks_on(valid_value, Remembered1, RemappedYoung1, RemappedOld0, MarkedYoung0, MarkedOld1);\n+    test_is_checks_on(null_value, Remembered1, RemappedYoung1, RemappedOld0, MarkedYoung0, MarkedOld1);\n+    test_is_checks_on(valid_value, Remembered1, RemappedYoung1, RemappedOld0, MarkedYoung1, MarkedOld0);\n+    test_is_checks_on(null_value, Remembered1, RemappedYoung1, RemappedOld0, MarkedYoung1, MarkedOld0);\n+    test_is_checks_on(valid_value, Remembered1, RemappedYoung1, RemappedOld0, MarkedYoung1, MarkedOld1);\n+    test_is_checks_on(null_value, Remembered1, RemappedYoung1, RemappedOld0, MarkedYoung1, MarkedOld1);\n+\n+    test_is_checks_on(valid_value, Remembered1, RemappedYoung1, RemappedOld1, MarkedYoung0, MarkedOld0);\n+    test_is_checks_on(null_value, Remembered1, RemappedYoung1, RemappedOld1, MarkedYoung0, MarkedOld0);\n+    test_is_checks_on(valid_value, Remembered1, RemappedYoung1, RemappedOld1, MarkedYoung0, MarkedOld1);\n+    test_is_checks_on(null_value, Remembered1, RemappedYoung1, RemappedOld1, MarkedYoung0, MarkedOld1);\n+    test_is_checks_on(valid_value, Remembered1, RemappedYoung1, RemappedOld1, MarkedYoung1, MarkedOld0);\n+    test_is_checks_on(null_value, Remembered1, RemappedYoung1, RemappedOld1, MarkedYoung1, MarkedOld0);\n+    test_is_checks_on(valid_value, Remembered1, RemappedYoung1, RemappedOld1, MarkedYoung1, MarkedOld1);\n+    test_is_checks_on(null_value, Remembered1, RemappedYoung1, RemappedOld1, MarkedYoung1, MarkedOld1);\n+\n+    test_is_checks_on(valid_value, Remembered11, RemappedYoung0, RemappedOld0, MarkedYoung0, MarkedOld0);\n+    test_is_checks_on(null_value, Remembered11, RemappedYoung0, RemappedOld0, MarkedYoung0, MarkedOld0);\n+    test_is_checks_on(valid_value, Remembered11, RemappedYoung0, RemappedOld0, MarkedYoung0, MarkedOld1);\n+    test_is_checks_on(null_value, Remembered11, RemappedYoung0, RemappedOld0, MarkedYoung0, MarkedOld1);\n+    test_is_checks_on(valid_value, Remembered11, RemappedYoung0, RemappedOld0, MarkedYoung1, MarkedOld0);\n+    test_is_checks_on(null_value, Remembered11, RemappedYoung0, RemappedOld0, MarkedYoung1, MarkedOld0);\n+    test_is_checks_on(valid_value, Remembered11, RemappedYoung0, RemappedOld0, MarkedYoung1, MarkedOld1);\n+    test_is_checks_on(null_value, Remembered11, RemappedYoung0, RemappedOld0, MarkedYoung1, MarkedOld1);\n+\n+    test_is_checks_on(valid_value, Remembered11, RemappedYoung0, RemappedOld1, MarkedYoung0, MarkedOld0);\n+    test_is_checks_on(null_value, Remembered11, RemappedYoung0, RemappedOld1, MarkedYoung0, MarkedOld0);\n+    test_is_checks_on(valid_value, Remembered11, RemappedYoung0, RemappedOld1, MarkedYoung0, MarkedOld1);\n+    test_is_checks_on(null_value, Remembered11, RemappedYoung0, RemappedOld1, MarkedYoung0, MarkedOld1);\n+    test_is_checks_on(valid_value, Remembered11, RemappedYoung0, RemappedOld1, MarkedYoung1, MarkedOld0);\n+    test_is_checks_on(null_value, Remembered11, RemappedYoung0, RemappedOld1, MarkedYoung1, MarkedOld0);\n+    test_is_checks_on(valid_value, Remembered11, RemappedYoung0, RemappedOld1, MarkedYoung1, MarkedOld1);\n+    test_is_checks_on(null_value, Remembered11, RemappedYoung0, RemappedOld1, MarkedYoung1, MarkedOld1);\n+\n+    test_is_checks_on(valid_value, Remembered11, RemappedYoung1, RemappedOld0, MarkedYoung0, MarkedOld0);\n+    test_is_checks_on(null_value, Remembered11, RemappedYoung1, RemappedOld0, MarkedYoung0, MarkedOld0);\n+    test_is_checks_on(valid_value, Remembered11, RemappedYoung1, RemappedOld0, MarkedYoung0, MarkedOld1);\n+    test_is_checks_on(null_value, Remembered11, RemappedYoung1, RemappedOld0, MarkedYoung0, MarkedOld1);\n+    test_is_checks_on(valid_value, Remembered11, RemappedYoung1, RemappedOld0, MarkedYoung1, MarkedOld0);\n+    test_is_checks_on(null_value, Remembered11, RemappedYoung1, RemappedOld0, MarkedYoung1, MarkedOld0);\n+    test_is_checks_on(valid_value, Remembered11, RemappedYoung1, RemappedOld0, MarkedYoung1, MarkedOld1);\n+    test_is_checks_on(null_value, Remembered11, RemappedYoung1, RemappedOld0, MarkedYoung1, MarkedOld1);\n+\n+    test_is_checks_on(valid_value, Remembered11, RemappedYoung1, RemappedOld1, MarkedYoung0, MarkedOld0);\n+    test_is_checks_on(null_value, Remembered11, RemappedYoung1, RemappedOld1, MarkedYoung0, MarkedOld0);\n+    test_is_checks_on(valid_value, Remembered11, RemappedYoung1, RemappedOld1, MarkedYoung0, MarkedOld1);\n+    test_is_checks_on(null_value, Remembered11, RemappedYoung1, RemappedOld1, MarkedYoung0, MarkedOld1);\n+    test_is_checks_on(valid_value, Remembered11, RemappedYoung1, RemappedOld1, MarkedYoung1, MarkedOld0);\n+    test_is_checks_on(null_value, Remembered11, RemappedYoung1, RemappedOld1, MarkedYoung1, MarkedOld0);\n+    test_is_checks_on(valid_value, Remembered11, RemappedYoung1, RemappedOld1, MarkedYoung1, MarkedOld1);\n+    test_is_checks_on(null_value, Remembered11, RemappedYoung1, RemappedOld1, MarkedYoung1, MarkedOld1);\n+\n+    test_is_checks_on(null_value, Uncolored, Uncolored, Uncolored, Uncolored, Uncolored);\n+  }\n+\n+  static void advance_and_test_young_phase(int& phase, int amount) {\n+    for (int i = 0; i < amount; ++i) {\n+      if (++phase & 1) {\n+        ZGlobalsPointers::flip_young_mark_start();\n+      } else {\n+        ZGlobalsPointers::flip_young_relocate_start();\n+      }\n+      test_is_checks_on_all();\n+    }\n+  }\n+\n+  static void advance_and_test_old_phase(int& phase, int amount) {\n+    for (int i = 0; i < amount; ++i) {\n+      if (++phase & 1) {\n+        ZGlobalsPointers::flip_old_mark_start();\n+      } else {\n+        ZGlobalsPointers::flip_old_relocate_start();\n+      }\n+      test_is_checks_on_all();\n+    }\n+  }\n+\n+  static void is_checks() {\n+    int young_phase = 0;\n+    int old_phase = 0;\n+    \/\/ Setup\n+    ZGlobalsPointers::initialize();\n+    test_is_checks_on_all();\n+\n+    advance_and_test_old_phase(old_phase, 4);\n+    advance_and_test_young_phase(young_phase, 4);\n+\n+    advance_and_test_old_phase(old_phase, 1);\n+    advance_and_test_young_phase(young_phase, 4);\n+\n+    advance_and_test_old_phase(old_phase, 1);\n+    advance_and_test_young_phase(young_phase, 4);\n+\n+    advance_and_test_old_phase(old_phase, 1);\n+    advance_and_test_young_phase(young_phase, 4);\n+\n+    advance_and_test_old_phase(old_phase, 1);\n+    advance_and_test_young_phase(young_phase, 4);\n+\n+    advance_and_test_old_phase(old_phase, 1);\n+    advance_and_test_young_phase(young_phase, 3);\n+\n+    advance_and_test_old_phase(old_phase, 1);\n+    advance_and_test_young_phase(young_phase, 3);\n+\n+    advance_and_test_old_phase(old_phase, 1);\n+    advance_and_test_young_phase(young_phase, 3);\n+\n+    advance_and_test_old_phase(old_phase, 1);\n+    advance_and_test_young_phase(young_phase, 3);\n+\n+    advance_and_test_old_phase(old_phase, 1);\n+    advance_and_test_young_phase(young_phase, 2);\n+\n+    advance_and_test_old_phase(old_phase, 1);\n+    advance_and_test_young_phase(young_phase, 2);\n+\n+    advance_and_test_old_phase(old_phase, 1);\n+    advance_and_test_young_phase(young_phase, 2);\n+\n+    advance_and_test_old_phase(old_phase, 1);\n+    advance_and_test_young_phase(young_phase, 2);\n+\n+    advance_and_test_old_phase(old_phase, 1);\n+    advance_and_test_young_phase(young_phase, 1);\n+\n+    advance_and_test_old_phase(old_phase, 1);\n+    advance_and_test_young_phase(young_phase, 1);\n+\n+    advance_and_test_old_phase(old_phase, 1);\n+    advance_and_test_young_phase(young_phase, 1);\n+\n+    advance_and_test_old_phase(old_phase, 1);\n+    advance_and_test_young_phase(young_phase, 1);\n+  }\n+};\n@@ -133,2 +435,2 @@\n-TEST_F(ZAddressTest, finalizable) {\n-  finalizable();\n+TEST_F(ZAddressTest, is_checks) {\n+  is_checks();\n","filename":"test\/hotspot\/gtest\/gc\/z\/test_zAddress.cpp","additions":392,"deletions":90,"binary":false,"changes":482,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,0 +28,1 @@\n+#include \"gc\/z\/zGeneration.inline.hpp\"\n@@ -29,0 +30,1 @@\n+#include \"gc\/z\/zHeap.hpp\"\n@@ -30,0 +32,1 @@\n+#include \"runtime\/os.hpp\"\n@@ -42,0 +45,36 @@\n+  \/\/ Setup and tear down\n+  ZHeap*       _old_heap;\n+  ZGenerationOld* _old_old;\n+  ZGenerationYoung* _old_young;\n+\n+  virtual void SetUp() {\n+    ZGlobalsPointers::initialize();\n+    _old_heap = ZHeap::_heap;\n+    ZHeap::_heap = (ZHeap*)os::malloc(sizeof(ZHeap), mtTest);\n+\n+    _old_old = ZGeneration::_old;\n+    _old_young = ZGeneration::_young;\n+\n+    ZGeneration::_old = &ZHeap::_heap->_old;\n+    ZGeneration::_young = &ZHeap::_heap->_young;\n+\n+    *const_cast<ZGenerationId*>(&ZGeneration::_old->_id) = ZGenerationId::old;\n+    *const_cast<ZGenerationId*>(&ZGeneration::_young->_id) = ZGenerationId::young;\n+\n+    ZGeneration::_old->_seqnum = 1;\n+    ZGeneration::_young->_seqnum = 2;\n+\n+    bool reserved = os::attempt_reserve_memory_at((char*)ZAddressHeapBase, ZGranuleSize, false \/* executable *\/);\n+    ASSERT_TRUE(reserved);\n+    os::commit_memory((char*)ZAddressHeapBase, ZGranuleSize, false \/* executable *\/);\n+  }\n+\n+  virtual void TearDown() {\n+    os::free(ZHeap::_heap);\n+    ZHeap::_heap = _old_heap;\n+    ZGeneration::_old = _old_old;\n+    ZGeneration::_young = _old_young;\n+    os::uncommit_memory((char*)ZAddressHeapBase, ZGranuleSize, false \/* executable *\/);\n+    os::release_memory((char*)ZAddressHeapBase, ZGranuleSize);\n+  }\n+\n@@ -88,1 +127,1 @@\n-      forwarding->insert(from_index, from_index, &cursor);\n+      forwarding->insert(from_index, zoffset(from_index), &cursor);\n@@ -116,1 +155,1 @@\n-      forwarding->insert(from_index, from_index, &cursor);\n+      forwarding->insert(from_index, zoffset(from_index), &cursor);\n@@ -147,3 +186,3 @@\n-    const ZVirtualMemory vmem(0, ZPageSizeSmall);\n-    const ZPhysicalMemory pmem(ZPhysicalMemorySegment(0, ZPageSizeSmall, true));\n-    ZPage page(ZPageTypeSmall, vmem, pmem);\n+    const ZVirtualMemory vmem(zoffset(0), ZPageSizeSmall);\n+    const ZPhysicalMemory pmem(ZPhysicalMemorySegment(zoffset(0), ZPageSizeSmall, true));\n+    ZPage page(ZPageType::small, vmem, pmem);\n@@ -151,1 +190,1 @@\n-    page.reset();\n+    page.reset(ZPageAge::eden, ZPageResetType::Allocation);\n@@ -154,1 +193,3 @@\n-    const uintptr_t object = page.alloc_object(object_size);\n+    const zaddress object = page.alloc_object(object_size);\n+\n+    ZGeneration::young()->_seqnum++;\n@@ -156,1 +197,3 @@\n-    ZGlobalSeqNum++;\n+    ZGeneration::young()->set_phase(ZGeneration::Phase::Mark);\n+    ZGeneration::young()->set_phase(ZGeneration::Phase::MarkComplete);\n+    ZGeneration::young()->set_phase(ZGeneration::Phase::Relocate);\n@@ -158,2 +201,6 @@\n-    bool dummy = false;\n-    page.mark_object(ZAddress::marked(object), dummy, dummy);\n+    \/\/page.mark_object(object, dummy, dummy);\n+    {\n+      bool dummy = false;\n+      const BitMap::idx_t index = page.bit_index(object);\n+      page._livemap.set(page._generation_id, index, dummy, dummy);\n+    }\n@@ -171,1 +218,1 @@\n-    ZForwarding* const forwarding = ZForwarding::alloc(&allocator, &page);\n+    ZForwarding* const forwarding = ZForwarding::alloc(&allocator, &page, ZPageAge::survivor1);\n@@ -191,1 +238,1 @@\n-TEST_F(ZForwardingTest, setup) {\n+TEST_VM_F(ZForwardingTest, setup) {\n@@ -195,1 +242,1 @@\n-TEST_F(ZForwardingTest, find_empty) {\n+TEST_VM_F(ZForwardingTest, find_empty) {\n@@ -199,1 +246,1 @@\n-TEST_F(ZForwardingTest, find_full) {\n+TEST_VM_F(ZForwardingTest, find_full) {\n@@ -203,1 +250,1 @@\n-TEST_F(ZForwardingTest, find_every_other) {\n+TEST_VM_F(ZForwardingTest, find_every_other) {\n","filename":"test\/hotspot\/gtest\/gc\/z\/test_zForwarding.cpp","additions":63,"deletions":16,"binary":false,"changes":79,"status":"modified"},{"patch":"@@ -0,0 +1,171 @@\n+\/*\n+ * Copyright (c) 2021, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/z\/zIndexDistributor.inline.hpp\"\n+#include \"unittest.hpp\"\n+\n+class ZIndexDistributorTest : public ::testing::Test {\n+protected:\n+  static void test_claim_tree_claim_level_size() {\n+    \/\/ max_index: 16, 16, 16, rest\n+    \/\/ claim level: 1, 16, 16 * 16, 16 * 16 * 16\n+    ASSERT_EQ(ZIndexDistributorClaimTree::claim_level_size(0), 1);\n+    ASSERT_EQ(ZIndexDistributorClaimTree::claim_level_size(1), 16);\n+    ASSERT_EQ(ZIndexDistributorClaimTree::claim_level_size(2), 16 * 16);\n+    ASSERT_EQ(ZIndexDistributorClaimTree::claim_level_size(3), 16 * 16 * 16);\n+  }\n+\n+  static void test_claim_tree_claim_level_end_index() {\n+    \/\/ First level is padded\n+    ASSERT_EQ(ZIndexDistributorClaimTree::claim_level_end_index(0), 16);\n+    ASSERT_EQ(ZIndexDistributorClaimTree::claim_level_end_index(1), 16 + 16);\n+    ASSERT_EQ(ZIndexDistributorClaimTree::claim_level_end_index(2), 16 + 16 + 16 * 16);\n+    ASSERT_EQ(ZIndexDistributorClaimTree::claim_level_end_index(3), 16 + 16 + 16 * 16 + 16 * 16 * 16);\n+  }\n+\n+  static void test_claim_tree_claim_index() {\n+    \/\/ First level should always give index 0\n+    {\n+      int indices[4] = {0, 0, 0, 0};\n+      ASSERT_EQ(ZIndexDistributorClaimTree::claim_index(indices, 0), 0);\n+    }\n+    {\n+      int indices[4] = {1, 0, 0, 0};\n+      ASSERT_EQ(ZIndexDistributorClaimTree::claim_index(indices, 0), 0);\n+    }\n+    {\n+      int indices[4] = {15, 0, 0, 0};\n+      ASSERT_EQ(ZIndexDistributorClaimTree::claim_index(indices, 0), 0);\n+    }\n+    {\n+      int indices[4] = {16, 0, 0, 0};\n+      ASSERT_EQ(ZIndexDistributorClaimTree::claim_index(indices, 0), 0);\n+    }\n+\n+    \/\/ Second level should depend on first claimed index\n+\n+    \/\/ Second-level start after first-level padding\n+    const int second_level_start = 16;\n+\n+    {\n+      int indices[4] = {0, 0, 0, 0};\n+      ASSERT_EQ(ZIndexDistributorClaimTree::claim_index(indices, 1), second_level_start);\n+    }\n+    {\n+      int indices[4] = {1, 0, 0, 0};\n+      ASSERT_EQ(ZIndexDistributorClaimTree::claim_index(indices, 1), second_level_start + 1);\n+    }\n+    {\n+      int indices[4] = {15, 0, 0, 0};\n+      ASSERT_EQ(ZIndexDistributorClaimTree::claim_index(indices, 1), second_level_start + 15);\n+    }\n+\n+    \/\/ Third level\n+\n+    const int third_level_start = second_level_start + 16;\n+\n+    {\n+      int indices[4] = {0, 0, 0, 0};\n+      ASSERT_EQ(ZIndexDistributorClaimTree::claim_index(indices, 2), third_level_start);\n+    }\n+    {\n+      int indices[4] = {1, 0, 0, 0};\n+      ASSERT_EQ(ZIndexDistributorClaimTree::claim_index(indices, 2), third_level_start + 1 * 16);\n+    }\n+    {\n+      int indices[4] = {15, 0, 0, 0};\n+      ASSERT_EQ(ZIndexDistributorClaimTree::claim_index(indices, 2), third_level_start + 15 * 16);\n+    }\n+    {\n+      int indices[4] = {1, 2, 0, 0};\n+      ASSERT_EQ(ZIndexDistributorClaimTree::claim_index(indices, 2), third_level_start + 1 * 16 + 2);\n+    }\n+    {\n+      int indices[4] = {15, 14, 0, 0};\n+      ASSERT_EQ(ZIndexDistributorClaimTree::claim_index(indices, 2), third_level_start + 15 * 16 + 14);\n+    }\n+\n+  }\n+\n+  static void test_claim_tree_claim_level_index() {\n+    {\n+      int indices[4] = {0,0,0,0};\n+      ASSERT_EQ(ZIndexDistributorClaimTree::claim_level_index(indices, 1), 0);\n+    }\n+    {\n+      int indices[4] = {1,0,0,0};\n+      ASSERT_EQ(ZIndexDistributorClaimTree::claim_level_index(indices, 1), 1);\n+    }\n+\n+    {\n+      int indices[4] = {0,0,0,0};\n+      ASSERT_EQ(ZIndexDistributorClaimTree::claim_level_index(indices, 2), 0);\n+    }\n+    {\n+      int indices[4] = {1,0,0,0};\n+      ASSERT_EQ(ZIndexDistributorClaimTree::claim_level_index(indices, 2), 1 * 16);\n+    }\n+    {\n+      int indices[4] = {2,0,0,0};\n+      ASSERT_EQ(ZIndexDistributorClaimTree::claim_level_index(indices, 2), 2 * 16);\n+    }\n+    {\n+      int indices[4] = {2,1,0,0};\n+      ASSERT_EQ(ZIndexDistributorClaimTree::claim_level_index(indices, 2), 2 * 16 + 1);\n+    }\n+\n+    {\n+      int indices[4] = {0,0,0,0};\n+      ASSERT_EQ(ZIndexDistributorClaimTree::claim_level_index(indices, 3), 0);\n+    }\n+    {\n+      int indices[4] = {1,0,0,0};\n+      ASSERT_EQ(ZIndexDistributorClaimTree::claim_level_index(indices, 3), 1 * 16 * 16);\n+    }\n+    {\n+      int indices[4] = {1,2,0,0};\n+      ASSERT_EQ(ZIndexDistributorClaimTree::claim_level_index(indices, 3), 1 * 16 * 16 + 2 * 16);\n+    }\n+    {\n+      int indices[4] = {1,2,1,0};\n+      ASSERT_EQ(ZIndexDistributorClaimTree::claim_level_index(indices, 3), 1 * 16 * 16 + 2 * 16 + 1);\n+    }\n+  }\n+};\n+\n+TEST_F(ZIndexDistributorTest, test_claim_tree_claim_level_size) {\n+  test_claim_tree_claim_level_size();\n+}\n+\n+TEST_F(ZIndexDistributorTest, test_claim_tree_claim_level_end_index) {\n+  test_claim_tree_claim_level_end_index();\n+}\n+\n+TEST_F(ZIndexDistributorTest, test_claim_tree_claim_level_index) {\n+  test_claim_tree_claim_level_index();\n+}\n+\n+TEST_F(ZIndexDistributorTest, test_claim_tree_claim_index) {\n+  test_claim_tree_claim_index();\n+}\n","filename":"test\/hotspot\/gtest\/gc\/z\/test_zIndexDistributor.cpp","additions":171,"deletions":0,"binary":false,"changes":171,"status":"added"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -25,0 +25,2 @@\n+#include \"gc\/z\/zGenerationId.hpp\"\n+#include \"gc\/z\/zGlobals.hpp\"\n@@ -29,0 +31,33 @@\n+private:\n+  \/\/ Setup and tear down\n+  ZHeap*            _old_heap;\n+  ZGenerationOld*   _old_old;\n+  ZGenerationYoung* _old_young;\n+\n+public:\n+\n+  virtual void SetUp() {\n+    ZGlobalsPointers::initialize();\n+    _old_heap = ZHeap::_heap;\n+    ZHeap::_heap = (ZHeap*)os::malloc(sizeof(ZHeap), mtTest);\n+\n+    _old_old = ZGeneration::_old;\n+    _old_young = ZGeneration::_young;\n+\n+    ZGeneration::_old = &ZHeap::_heap->_old;\n+    ZGeneration::_young = &ZHeap::_heap->_young;\n+\n+    *const_cast<ZGenerationId*>(&ZGeneration::_old->_id) = ZGenerationId::old;\n+    *const_cast<ZGenerationId*>(&ZGeneration::_young->_id) = ZGenerationId::young;\n+\n+    ZGeneration::_old->_seqnum = 1;\n+    ZGeneration::_young->_seqnum = 2;\n+  }\n+\n+  virtual void TearDown() {\n+    os::free(ZHeap::_heap);\n+    ZHeap::_heap = _old_heap;\n+    ZGeneration::_old = _old_old;\n+    ZGeneration::_young = _old_young;\n+  }\n+\n@@ -35,1 +70,1 @@\n-    uintptr_t object = 0u;\n+    BitMap::idx_t object_index = BitMap::idx_t(0);\n@@ -38,1 +73,1 @@\n-    livemap.set(object, false \/* finalizable *\/, inc_live);\n+    livemap.set(ZGenerationId::old, object_index, false \/* finalizable *\/, inc_live);\n@@ -44,1 +79,1 @@\n-    ASSERT_TRUE(livemap.get(0));\n+    ASSERT_TRUE(livemap.get(ZGenerationId::old, 0));\n@@ -47,1 +82,1 @@\n-    ASSERT_TRUE(livemap.get(1));\n+    ASSERT_TRUE(livemap.get(ZGenerationId::old, 1));\n","filename":"test\/hotspot\/gtest\/gc\/z\/test_zLiveMap.cpp","additions":40,"deletions":5,"binary":false,"changes":45,"status":"modified"},{"patch":"@@ -0,0 +1,105 @@\n+\/*\n+ * Copyright (c) 2021, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/z\/zGlobals.hpp\"\n+#include \"gc\/z\/zMemory.inline.hpp\"\n+#include \"unittest.hpp\"\n+\n+class ZAddressOffsetMaxSetter {\n+private:\n+  const size_t _old_max;\n+  const size_t _old_mask;\n+\n+public:\n+  ZAddressOffsetMaxSetter() :\n+      _old_max(ZAddressOffsetMax),\n+      _old_mask(ZAddressOffsetMask) {\n+    ZAddressOffsetMax = size_t(16) * G * 1024;\n+    ZAddressOffsetMask = ZAddressOffsetMax - 1;\n+  }\n+  ~ZAddressOffsetMaxSetter() {\n+    ZAddressOffsetMax = _old_max;\n+    ZAddressOffsetMask = _old_mask;\n+  }\n+};\n+\n+TEST(ZMemory, accessors) {\n+  ZAddressOffsetMaxSetter setter;\n+\n+  {\n+    ZMemory mem(zoffset(0), ZGranuleSize);\n+\n+    EXPECT_EQ(mem.start(), zoffset(0));\n+    EXPECT_EQ(mem.end(), zoffset_end(ZGranuleSize));\n+    EXPECT_EQ(mem.size(), ZGranuleSize);\n+  }\n+\n+\n+  {\n+    ZMemory mem(zoffset(ZGranuleSize), ZGranuleSize);\n+\n+    EXPECT_EQ(mem.start(), zoffset(ZGranuleSize));\n+    EXPECT_EQ(mem.end(), zoffset_end(ZGranuleSize + ZGranuleSize));\n+    EXPECT_EQ(mem.size(), ZGranuleSize);\n+  }\n+\n+  {\n+    \/\/ Max area - check end boundary\n+    ZMemory mem(zoffset(0), ZAddressOffsetMax);\n+\n+    EXPECT_EQ(mem.start(), zoffset(0));\n+    EXPECT_EQ(mem.end(), zoffset_end(ZAddressOffsetMax));\n+    EXPECT_EQ(mem.size(), ZAddressOffsetMax);\n+  }\n+}\n+\n+TEST(ZMemory, resize) {\n+  ZAddressOffsetMaxSetter setter;\n+\n+  ZMemory mem(zoffset(ZGranuleSize * 2), ZGranuleSize * 2) ;\n+\n+  mem.shrink_from_front(ZGranuleSize);\n+  EXPECT_EQ(mem.start(),   zoffset(ZGranuleSize * 3));\n+  EXPECT_EQ(mem.end(), zoffset_end(ZGranuleSize * 4));\n+  EXPECT_EQ(mem.size(),            ZGranuleSize * 1);\n+  mem.grow_from_front(ZGranuleSize);\n+\n+  mem.shrink_from_back(ZGranuleSize);\n+  EXPECT_EQ(mem.start(),   zoffset(ZGranuleSize * 2));\n+  EXPECT_EQ(mem.end(), zoffset_end(ZGranuleSize * 3));\n+  EXPECT_EQ(mem.size(),            ZGranuleSize * 1);\n+  mem.grow_from_back(ZGranuleSize);\n+\n+  mem.grow_from_front(ZGranuleSize);\n+  EXPECT_EQ(mem.start(),   zoffset(ZGranuleSize * 1));\n+  EXPECT_EQ(mem.end(), zoffset_end(ZGranuleSize * 4));\n+  EXPECT_EQ(mem.size(),            ZGranuleSize * 3);\n+  mem.shrink_from_front(ZGranuleSize);\n+\n+  mem.grow_from_back(ZGranuleSize);\n+  EXPECT_EQ(mem.start(),   zoffset(ZGranuleSize * 2));\n+  EXPECT_EQ(mem.end(), zoffset_end(ZGranuleSize * 5));\n+  EXPECT_EQ(mem.size(),            ZGranuleSize * 3);\n+  mem.shrink_from_back(ZGranuleSize);\n+}\n","filename":"test\/hotspot\/gtest\/gc\/z\/test_zMemory.cpp","additions":105,"deletions":0,"binary":false,"changes":105,"status":"added"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -29,2 +29,2 @@\n-  const ZPhysicalMemorySegment seg0(0, 100, true);\n-  const ZPhysicalMemorySegment seg1(200, 100, true);\n+  const ZPhysicalMemorySegment seg0(zoffset(0), 100, true);\n+  const ZPhysicalMemorySegment seg1(zoffset(200), 100, true);\n@@ -55,7 +55,7 @@\n-  const ZPhysicalMemorySegment seg0(0, 1, true);\n-  const ZPhysicalMemorySegment seg1(1, 1, true);\n-  const ZPhysicalMemorySegment seg2(2, 1, true);\n-  const ZPhysicalMemorySegment seg3(3, 1, true);\n-  const ZPhysicalMemorySegment seg4(4, 1, true);\n-  const ZPhysicalMemorySegment seg5(5, 1, true);\n-  const ZPhysicalMemorySegment seg6(6, 1, true);\n+  const ZPhysicalMemorySegment seg0(zoffset(0), 1, true);\n+  const ZPhysicalMemorySegment seg1(zoffset(1), 1, true);\n+  const ZPhysicalMemorySegment seg2(zoffset(2), 1, true);\n+  const ZPhysicalMemorySegment seg3(zoffset(3), 1, true);\n+  const ZPhysicalMemorySegment seg4(zoffset(4), 1, true);\n+  const ZPhysicalMemorySegment seg5(zoffset(5), 1, true);\n+  const ZPhysicalMemorySegment seg6(zoffset(6), 1, true);\n@@ -119,3 +119,3 @@\n-  pmem.add_segment(ZPhysicalMemorySegment(10, 10, true));\n-  pmem.add_segment(ZPhysicalMemorySegment(30, 10, true));\n-  pmem.add_segment(ZPhysicalMemorySegment(50, 10, true));\n+  pmem.add_segment(ZPhysicalMemorySegment(zoffset(10), 10, true));\n+  pmem.add_segment(ZPhysicalMemorySegment(zoffset(30), 10, true));\n+  pmem.add_segment(ZPhysicalMemorySegment(zoffset(50), 10, true));\n@@ -135,3 +135,3 @@\n-  pmem.add_segment(ZPhysicalMemorySegment(0, 10, true));\n-  pmem.add_segment(ZPhysicalMemorySegment(10, 10, true));\n-  pmem.add_segment(ZPhysicalMemorySegment(30, 10, true));\n+  pmem.add_segment(ZPhysicalMemorySegment(zoffset(0), 10, true));\n+  pmem.add_segment(ZPhysicalMemorySegment(zoffset(10), 10, true));\n+  pmem.add_segment(ZPhysicalMemorySegment(zoffset(30), 10, true));\n@@ -162,4 +162,4 @@\n-  pmem0.add_segment(ZPhysicalMemorySegment(0, 10, true));\n-  pmem0.add_segment(ZPhysicalMemorySegment(10, 10, false));\n-  pmem0.add_segment(ZPhysicalMemorySegment(20, 10, true));\n-  pmem0.add_segment(ZPhysicalMemorySegment(30, 10, false));\n+  pmem0.add_segment(ZPhysicalMemorySegment(zoffset(0), 10, true));\n+  pmem0.add_segment(ZPhysicalMemorySegment(zoffset(10), 10, false));\n+  pmem0.add_segment(ZPhysicalMemorySegment(zoffset(20), 10, true));\n+  pmem0.add_segment(ZPhysicalMemorySegment(zoffset(30), 10, false));\n","filename":"test\/hotspot\/gtest\/gc\/z\/test_zPhysicalMemory.cpp","additions":20,"deletions":20,"binary":false,"changes":40,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -29,1 +29,1 @@\n-  ZVirtualMemory vmem(0, 10);\n+  ZVirtualMemory vmem(zoffset(0), 10);\n","filename":"test\/hotspot\/gtest\/gc\/z\/test_zVirtualMemory.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, 2017, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -61,1 +61,1 @@\n-              << \"Duplicate entries on indexes \" << i << \" and \" << j;\n+              << \"Duplicate entries on indexes \" << i << \" and \" << j << \" : \" << VMStructs::localHotSpotVMTypes[i].typeName;\n","filename":"test\/hotspot\/gtest\/runtime\/test_vmStructs.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -0,0 +1,117 @@\n+#\n+# Copyright (c) 2019, 2023, Oracle and\/or its affiliates. All rights reserved.\n+# DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+#\n+# This code is free software; you can redistribute it and\/or modify it\n+# under the terms of the GNU General Public License version 2 only, as\n+# published by the Free Software Foundation.\n+#\n+# This code is distributed in the hope that it will be useful, but WITHOUT\n+# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+# FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+# version 2 for more details (a copy is included in the LICENSE file that\n+# accompanied this code).\n+#\n+# You should have received a copy of the GNU General Public License version\n+# 2 along with this work; if not, write to the Free Software Foundation,\n+# Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+#\n+# Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+# or visit www.oracle.com if you need additional information or have any\n+# questions.\n+#\n+\n+#############################################################################\n+#\n+# List of quarantined tests for testing with Generational ZGC.\n+#\n+#############################################################################\n+\n+# Quiet all SA tests\n+\n+resourcehogs\/serviceability\/sa\/TestHeapDumpForLargeArray.java 8000000   generic-all\n+serviceability\/sa\/CDSJMapClstats.java                         8000000   generic-all\n+serviceability\/sa\/ClhsdbAttach.java                           8000000   generic-all\n+serviceability\/sa\/ClhsdbCDSCore.java                          8000000   generic-all\n+serviceability\/sa\/ClhsdbCDSJstackPrintAll.java                8000000   generic-all\n+serviceability\/sa\/ClhsdbClasses.java                          8000000   generic-all\n+serviceability\/sa\/ClhsdbDumpclass.java                        8000000   generic-all\n+serviceability\/sa\/ClhsdbDumpheap.java                         8000000   generic-all\n+serviceability\/sa\/ClhsdbField.java                            8000000   generic-all\n+serviceability\/sa\/ClhsdbFindPC.java#apa                       8000000   generic-all\n+serviceability\/sa\/ClhsdbFindPC.java#no-xcomp-core             8000000   generic-all\n+serviceability\/sa\/ClhsdbFindPC.java#no-xcomp-process          8000000   generic-all\n+serviceability\/sa\/ClhsdbFindPC.java#xcomp-core                8000000   generic-all\n+serviceability\/sa\/ClhsdbFindPC.java#xcomp-process             8000000   generic-all\n+serviceability\/sa\/ClhsdbFlags.java                            8000000   generic-all\n+serviceability\/sa\/ClhsdbHistory.java                          8000000   generic-all\n+serviceability\/sa\/ClhsdbInspect.java                          8000000   generic-all\n+serviceability\/sa\/ClhsdbJdis.java                             8000000   generic-all\n+serviceability\/sa\/ClhsdbJhisto.java                           8000000   generic-all\n+serviceability\/sa\/ClhsdbJstack.java#id0                       8000000   generic-all\n+serviceability\/sa\/ClhsdbJstack.java#id1                       8000000   generic-all\n+serviceability\/sa\/ClhsdbJstackXcompStress.java                8000000   generic-all\n+serviceability\/sa\/ClhsdbLauncher.java                         8000000   generic-all\n+serviceability\/sa\/ClhsdbLongConstant.java                     8000000   generic-all\n+serviceability\/sa\/ClhsdbPmap.java                             8000000   generic-all\n+serviceability\/sa\/ClhsdbPmap.java#core                        8000000   generic-all\n+serviceability\/sa\/ClhsdbPmap.java#process                     8000000   generic-all\n+serviceability\/sa\/ClhsdbPrintAll.java                         8000000   generic-all\n+serviceability\/sa\/ClhsdbPrintAs.java                          8000000   generic-all\n+serviceability\/sa\/ClhsdbPrintStatics.java                     8000000   generic-all\n+serviceability\/sa\/ClhsdbPstack.java#core                      8000000   generic-all\n+serviceability\/sa\/ClhsdbPstack.java#process                   8000000   generic-all\n+serviceability\/sa\/ClhsdbScanOops.java                         8000000   generic-all\n+serviceability\/sa\/ClhsdbSource.java                           8000000   generic-all\n+serviceability\/sa\/ClhsdbSymbol.java                           8000000   generic-all\n+serviceability\/sa\/ClhsdbThread.java                           8000000   generic-all\n+serviceability\/sa\/ClhsdbThreadContext.java                    8000000   generic-all\n+serviceability\/sa\/ClhsdbVmStructsDump.java                    8000000   generic-all\n+serviceability\/sa\/ClhsdbWhere.java                            8000000   generic-all\n+serviceability\/sa\/DeadlockDetectionTest.java                  8000000   generic-all\n+serviceability\/sa\/JhsdbThreadInfoTest.java                    8000000   generic-all\n+serviceability\/sa\/LingeredAppSysProps.java                    8000000   generic-all\n+serviceability\/sa\/LingeredAppWithDefaultMethods.java          8000000   generic-all\n+serviceability\/sa\/LingeredAppWithEnum.java                    8000000   generic-all\n+serviceability\/sa\/LingeredAppWithInterface.java               8000000   generic-all\n+serviceability\/sa\/LingeredAppWithInvokeDynamic.java           8000000   generic-all\n+serviceability\/sa\/LingeredAppWithLock.java                    8000000   generic-all\n+serviceability\/sa\/LingeredAppWithNativeMethod.java            8000000   generic-all\n+serviceability\/sa\/LingeredAppWithRecComputation.java          8000000   generic-all\n+serviceability\/sa\/TestClassDump.java                          8000000   generic-all\n+serviceability\/sa\/TestClhsdbJstackLock.java                   8000000   generic-all\n+serviceability\/sa\/TestCpoolForInvokeDynamic.java              8000000   generic-all\n+serviceability\/sa\/TestDefaultMethods.java                     8000000   generic-all\n+serviceability\/sa\/TestG1HeapRegion.java                       8000000   generic-all\n+serviceability\/sa\/TestHeapDumpForInvokeDynamic.java           8000000   generic-all\n+serviceability\/sa\/TestInstanceKlassSize.java                  8000000   generic-all\n+serviceability\/sa\/TestInstanceKlassSizeForInterface.java      8000000   generic-all\n+serviceability\/sa\/TestIntConstant.java                        8000000   generic-all\n+serviceability\/sa\/TestJhsdbJstackLineNumbers.java             8000000   generic-all\n+serviceability\/sa\/TestJhsdbJstackLock.java                    8000000   generic-all\n+serviceability\/sa\/TestJhsdbJstackMixed.java                   8000000   generic-all\n+serviceability\/sa\/TestJmapCore.java                           8000000   generic-all\n+serviceability\/sa\/TestJmapCoreMetaspace.java                  8000000   generic-all\n+serviceability\/sa\/TestObjectAlignment.java                    8000000   generic-all\n+serviceability\/sa\/TestObjectMonitorIterate.java               8000000   generic-all\n+serviceability\/sa\/TestPrintMdo.java                           8000000   generic-all\n+serviceability\/sa\/TestRevPtrsForInvokeDynamic.java            8000000   generic-all\n+serviceability\/sa\/TestSysProps.java                           8000000   generic-all\n+serviceability\/sa\/TestType.java                               8000000   generic-all\n+serviceability\/sa\/TestUniverse.java                           8000000   generic-all\n+serviceability\/sa\/UniqueVtableTest.java                       8000000   generic-all\n+serviceability\/sa\/jmap-hprof\/JMapHProfLargeHeapProc.java      8000000   generic-all\n+serviceability\/sa\/jmap-hprof\/JMapHProfLargeHeapTest.java      8000000   generic-all\n+serviceability\/sa\/sadebugd\/ClhsdbAttachToDebugServer.java     8000000   generic-all\n+serviceability\/sa\/sadebugd\/ClhsdbTestConnectArgument.java     8000000   generic-all\n+serviceability\/sa\/sadebugd\/DebugdConnectTest.java             8000000   generic-all\n+serviceability\/sa\/sadebugd\/DebugdUtils.java                   8000000   generic-all\n+serviceability\/sa\/sadebugd\/DisableRegistryTest.java           8000000   generic-all\n+serviceability\/sa\/sadebugd\/PmapOnDebugdTest.java              8000000   generic-all\n+serviceability\/sa\/sadebugd\/RunCommandOnServerTest.java        8000000   generic-all\n+serviceability\/sa\/sadebugd\/SADebugDTest.java                  8000000   generic-all\n+\n+vmTestbase\/gc\/gctests\/MemoryEaterMT\/MemoryEaterMT.java        8289582   windows-x64\n+\n+vmTestbase\/nsk\/monitoring\/MemoryPoolMBean\/isCollectionUsageThresholdExceeded\/isexceeded002\/TestDescription.java 8298302 generic-all\n+vmTestbase\/nsk\/sysdict\/vm\/stress\/chain\/chain007\/chain007.java 8298991 linux-x64\n","filename":"test\/hotspot\/jtreg\/ProblemList-generational-zgc.txt","additions":117,"deletions":0,"binary":false,"changes":117,"status":"added"},{"patch":"@@ -0,0 +1,440 @@\n+\/*\n+ * Copyright (c) 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package compiler.gcbarriers;\n+\n+import compiler.lib.ir_framework.*;\n+import java.lang.invoke.VarHandle;\n+import java.lang.invoke.MethodHandles;\n+import java.util.concurrent.ThreadLocalRandom;\n+\n+\/**\n+ * @test\n+ * @summary Test that the ZGC barrier elision optimization does not elide\n+ *          necessary barriers. The tests use volatile memory accesses and\n+ *          blackholes to prevent C2 from simply optimizing them away.\n+ * @library \/test\/lib \/\n+ * @requires vm.gc.Z & vm.opt.final.ZGenerational\n+ * @run driver compiler.gcbarriers.TestZGCBarrierElision test-correctness\n+ *\/\n+\n+\/**\n+ * @test\n+ * @summary Test that the ZGC barrier elision optimization elides unnecessary\n+ *          barriers following simple allocation and domination rules.\n+ * @library \/test\/lib \/\n+ * @requires vm.gc.Z & vm.opt.final.ZGenerational & (vm.simpleArch == \"x64\" | vm.simpleArch == \"aarch64\")\n+ * @run driver compiler.gcbarriers.TestZGCBarrierElision test-effectiveness\n+ *\/\n+\n+class Inner {}\n+\n+class Outer {\n+    volatile Inner field1;\n+    volatile Inner field2;\n+    Outer() {}\n+}\n+\n+class Common {\n+\n+    static Inner inner = new Inner();\n+    static Outer outer = new Outer();\n+    static Outer outer2 = new Outer();\n+    static Outer[] outerArray = new Outer[42];\n+\n+    static final VarHandle field1VarHandle;\n+    static final VarHandle field2VarHandle;\n+    static {\n+        MethodHandles.Lookup l = MethodHandles.lookup();\n+        try {\n+            field1VarHandle = l.findVarHandle(Outer.class, \"field1\", Inner.class);\n+            field2VarHandle = l.findVarHandle(Outer.class, \"field2\", Inner.class);\n+        } catch (Exception e) {\n+            throw new Error(e);\n+        }\n+    }\n+    static final VarHandle outerArrayVarHandle =\n+        MethodHandles.arrayElementVarHandle(Outer[].class);\n+\n+    static final String REMAINING = \"strong\";\n+    static final String ELIDED = \"elided\";\n+\n+    static void blackhole(Object o) {}\n+    static void nonInlinedMethod() {}\n+}\n+\n+public class TestZGCBarrierElision {\n+\n+    public static void main(String[] args) {\n+        if (args.length != 1) {\n+            throw new IllegalArgumentException();\n+        }\n+        Class testClass;\n+        if (args[0].equals(\"test-correctness\")) {\n+            testClass = TestZGCCorrectBarrierElision.class;\n+        } else if (args[0].equals(\"test-effectiveness\")) {\n+            testClass = TestZGCEffectiveBarrierElision.class;\n+        } else {\n+            throw new IllegalArgumentException();\n+        }\n+        String commonName = Common.class.getName();\n+        TestFramework test = new TestFramework(testClass);\n+        test.addFlags(\"-XX:+UseZGC\", \"-XX:+UnlockExperimentalVMOptions\",\n+                      \"-XX:CompileCommand=blackhole,\" + commonName + \"::blackhole\",\n+                      \"-XX:CompileCommand=dontinline,\" + commonName + \"::nonInlinedMethod\",\n+                      \"-XX:LoopMaxUnroll=0\");\n+        test.start();\n+    }\n+}\n+\n+class TestZGCCorrectBarrierElision {\n+\n+    @Test\n+    @IR(counts = { IRNode.Z_STORE_P_WITH_BARRIER_FLAG, Common.REMAINING, \"1\" }, phase = CompilePhase.FINAL_CODE)\n+    @IR(counts = { IRNode.Z_LOAD_P_WITH_BARRIER_FLAG, Common.REMAINING, \"1\" },  phase = CompilePhase.FINAL_CODE)\n+    static void testLoadThenStore(Outer o, Inner i) {\n+        Common.blackhole(o.field1);\n+        o.field1 = i;\n+    }\n+\n+    @Test\n+    @IR(counts = { IRNode.Z_LOAD_P_WITH_BARRIER_FLAG, Common.REMAINING, \"2\" }, phase = CompilePhase.FINAL_CODE)\n+    static void testLoadThenLoadAnotherField(Outer o) {\n+        Common.blackhole(o.field1);\n+        Common.blackhole(o.field2);\n+    }\n+\n+    @Test\n+    @IR(counts = { IRNode.Z_LOAD_P_WITH_BARRIER_FLAG, Common.REMAINING, \"2\" }, phase = CompilePhase.FINAL_CODE)\n+    static void testLoadThenLoadFromAnotherObject(Outer o1, Outer o2) {\n+        Common.blackhole(o1.field1);\n+        Common.blackhole(o2.field1);\n+    }\n+\n+    @Run(test = {\"testLoadThenStore\",\n+                 \"testLoadThenLoadAnotherField\",\n+                 \"testLoadThenLoadFromAnotherObject\"})\n+    void runBasicTests() {\n+        testLoadThenStore(Common.outer, Common.inner);\n+        testLoadThenLoadAnotherField(Common.outer);\n+        testLoadThenLoadFromAnotherObject(Common.outer, Common.outer2);\n+    }\n+\n+    @Test\n+    @IR(counts = { IRNode.Z_LOAD_P_WITH_BARRIER_FLAG, Common.REMAINING, \"1\" }, phase = CompilePhase.FINAL_CODE)\n+    @IR(counts = { IRNode.Z_STORE_P_WITH_BARRIER_FLAG, Common.REMAINING, \"1\" }, phase = CompilePhase.FINAL_CODE)\n+    static void testArrayLoadThenStore(Outer[] a, Outer o) {\n+        Common.blackhole(Common.outerArrayVarHandle.getVolatile(a, 0));\n+        Common.outerArrayVarHandle.setVolatile(a, 0, o);\n+    }\n+\n+    @Test\n+    @IR(counts = { IRNode.Z_LOAD_P_WITH_BARRIER_FLAG, Common.REMAINING, \"2\" }, phase = CompilePhase.FINAL_CODE)\n+    static void testArrayLoadThenLoadAnotherElement(Outer[] a) {\n+        Common.blackhole(Common.outerArrayVarHandle.getVolatile(a, 0));\n+        Common.blackhole(Common.outerArrayVarHandle.getVolatile(a, 10));\n+    }\n+\n+    @Run(test = {\"testArrayLoadThenStore\",\n+                 \"testArrayLoadThenLoadAnotherElement\"})\n+    void runArrayTests() {\n+        testArrayLoadThenStore(Common.outerArray, Common.outer);\n+        testArrayLoadThenLoadAnotherElement(Common.outerArray);\n+    }\n+\n+    @Test\n+    @IR(counts = { IRNode.Z_STORE_P_WITH_BARRIER_FLAG, Common.REMAINING, \"2\" }, phase = CompilePhase.FINAL_CODE)\n+    static void testConditionalStoreThenStore(Outer o, Inner i, int value) {\n+        if (value % 2 == 0) {\n+            o.field1 = i;\n+        }\n+        o.field1 = i;\n+    }\n+\n+    @Test\n+    @IR(counts = { IRNode.Z_STORE_P_WITH_BARRIER_FLAG, Common.REMAINING, \"2\" }, phase = CompilePhase.FINAL_CODE)\n+    static void testStoreThenCallThenStore(Outer o, Inner i) {\n+        o.field1 = i;\n+        Common.nonInlinedMethod();\n+        o.field1 = i;\n+    }\n+\n+    @Run(test = {\"testConditionalStoreThenStore\",\n+                 \"testStoreThenCallThenStore\"})\n+    void runControlFlowTests() {\n+        testConditionalStoreThenStore(Common.outer, Common.inner, ThreadLocalRandom.current().nextInt(0, 100));\n+        testStoreThenCallThenStore(Common.outer, Common.inner);\n+    }\n+\n+    @Test\n+    @IR(counts = { IRNode.Z_GET_AND_SET_P_WITH_BARRIER_FLAG, Common.REMAINING, \"1\" }, phase = CompilePhase.FINAL_CODE)\n+    static void testAllocateThenAtomic(Inner i) {\n+        Outer o = new Outer();\n+        Common.blackhole(o);\n+        Common.field1VarHandle.getAndSet​(o, i);\n+    }\n+\n+    @Test\n+    @IR(counts = { IRNode.Z_LOAD_P_WITH_BARRIER_FLAG, Common.REMAINING, \"1\" }, phase = CompilePhase.FINAL_CODE)\n+    @IR(counts = { IRNode.Z_GET_AND_SET_P_WITH_BARRIER_FLAG, Common.REMAINING, \"1\" }, phase = CompilePhase.FINAL_CODE)\n+    static void testLoadThenAtomic(Outer o, Inner i) {\n+        Common.blackhole(o.field1);\n+        Common.field1VarHandle.getAndSet​(o, i);\n+    }\n+\n+    @Test\n+    @IR(counts = { IRNode.Z_GET_AND_SET_P_WITH_BARRIER_FLAG, Common.REMAINING, \"2\" }, phase = CompilePhase.FINAL_CODE)\n+    static void testAtomicThenAtomicAnotherField(Outer o, Inner i) {\n+        Common.field1VarHandle.getAndSet​(o, i);\n+        Common.field2VarHandle.getAndSet​(o, i);\n+    }\n+\n+    @Test\n+    @IR(counts = { IRNode.Z_GET_AND_SET_P_WITH_BARRIER_FLAG, Common.REMAINING, \"1\" }, phase = CompilePhase.FINAL_CODE)\n+    static void testAllocateArrayThenAtomicAtKnownIndex(Outer o) {\n+        Outer[] a = new Outer[42];\n+        Common.blackhole(a);\n+        Common.outerArrayVarHandle.getAndSet(a, 2, o);\n+    }\n+\n+    @Test\n+    @IR(counts = { IRNode.Z_GET_AND_SET_P_WITH_BARRIER_FLAG, Common.REMAINING, \"1\" }, phase = CompilePhase.FINAL_CODE)\n+    static void testAllocateArrayThenAtomicAtUnknownIndex(Outer o, int index) {\n+        Outer[] a = new Outer[42];\n+        Common.blackhole(a);\n+        Common.outerArrayVarHandle.getAndSet(a, index, o);\n+    }\n+\n+    @Test\n+    @IR(counts = { IRNode.Z_GET_AND_SET_P_WITH_BARRIER_FLAG, Common.REMAINING, \"2\" }, phase = CompilePhase.FINAL_CODE)\n+    static void testArrayAtomicThenAtomicAtUnknownIndices(Outer[] a, Outer o, int index1, int index2) {\n+        Common.outerArrayVarHandle.getAndSet(a, index1, o);\n+        Common.outerArrayVarHandle.getAndSet(a, index2, o);\n+    }\n+\n+    @Run(test = {\"testAllocateThenAtomic\",\n+                 \"testLoadThenAtomic\",\n+                 \"testAtomicThenAtomicAnotherField\",\n+                 \"testAllocateArrayThenAtomicAtKnownIndex\",\n+                 \"testAllocateArrayThenAtomicAtUnknownIndex\",\n+                 \"testArrayAtomicThenAtomicAtUnknownIndices\"})\n+    void runAtomicOperationTests() {\n+        testAllocateThenAtomic(Common.inner);\n+        testLoadThenAtomic(Common.outer, Common.inner);\n+        testAtomicThenAtomicAnotherField(Common.outer, Common.inner);\n+        testAllocateArrayThenAtomicAtKnownIndex(Common.outer);\n+        testAllocateArrayThenAtomicAtUnknownIndex(Common.outer, 10);\n+        testArrayAtomicThenAtomicAtUnknownIndices(Common.outerArray, Common.outer, 10, 20);\n+    }\n+}\n+\n+class TestZGCEffectiveBarrierElision {\n+\n+    @Test\n+    @IR(counts = { IRNode.Z_LOAD_P_WITH_BARRIER_FLAG, Common.ELIDED, \"1\" }, phase = CompilePhase.FINAL_CODE)\n+    static void testAllocateThenLoad() {\n+        Outer o1 = new Outer();\n+        Common.blackhole(o1);\n+        \/\/ This load is directly optimized away by C2.\n+        Common.blackhole(o1.field1);\n+        Common.blackhole(o1.field1);\n+    }\n+\n+    @Test\n+    @IR(counts = { IRNode.Z_STORE_P_WITH_BARRIER_FLAG, Common.ELIDED, \"1\" }, phase = CompilePhase.FINAL_CODE)\n+    static void testAllocateThenStore(Inner i) {\n+        Outer o1 = new Outer();\n+        Common.blackhole(o1);\n+        o1.field1 = i;\n+    }\n+\n+    @Test\n+    @IR(counts = { IRNode.Z_LOAD_P_WITH_BARRIER_FLAG, Common.REMAINING, \"1\" }, phase = CompilePhase.FINAL_CODE)\n+    @IR(counts = { IRNode.Z_LOAD_P_WITH_BARRIER_FLAG, Common.ELIDED, \"1\" }, phase = CompilePhase.FINAL_CODE)\n+    static void testLoadThenLoad(Outer o) {\n+        Common.blackhole(o.field1);\n+        Common.blackhole(o.field1);\n+    }\n+\n+    @Test\n+    @IR(counts = { IRNode.Z_STORE_P_WITH_BARRIER_FLAG, Common.REMAINING, \"1\" }, phase = CompilePhase.FINAL_CODE)\n+    @IR(counts = { IRNode.Z_STORE_P_WITH_BARRIER_FLAG, Common.ELIDED, \"1\" }, phase = CompilePhase.FINAL_CODE)\n+    static void testStoreThenStore(Outer o, Inner i) {\n+        o.field1 = i;\n+        o.field1 = i;\n+    }\n+\n+    @Test\n+    @IR(counts = { IRNode.Z_STORE_P_WITH_BARRIER_FLAG, Common.REMAINING, \"1\" }, phase = CompilePhase.FINAL_CODE)\n+    @IR(counts = { IRNode.Z_LOAD_P_WITH_BARRIER_FLAG, Common.ELIDED, \"1\" },  phase = CompilePhase.FINAL_CODE)\n+    static void testStoreThenLoad(Outer o, Inner i) {\n+        o.field1 = i;\n+        Common.blackhole(o.field1);\n+    }\n+\n+    @Run(test = {\"testAllocateThenLoad\",\n+                 \"testAllocateThenStore\",\n+                 \"testLoadThenLoad\",\n+                 \"testStoreThenStore\",\n+                 \"testStoreThenLoad\"})\n+    void runBasicTests() {\n+        testAllocateThenLoad();\n+        testAllocateThenStore(Common.inner);\n+        testLoadThenLoad(Common.outer);\n+        testStoreThenStore(Common.outer, Common.inner);\n+        testStoreThenLoad(Common.outer, Common.inner);\n+    }\n+\n+    @Test\n+    @IR(counts = { IRNode.Z_STORE_P_WITH_BARRIER_FLAG, Common.ELIDED, \"1\" }, phase = CompilePhase.FINAL_CODE)\n+    static void testAllocateArrayThenStoreAtKnownIndex(Outer o) {\n+        Outer[] a = new Outer[42];\n+        Common.blackhole(a);\n+        Common.outerArrayVarHandle.setVolatile(a, 0, o);\n+    }\n+\n+    @Test\n+    @IR(counts = { IRNode.Z_STORE_P_WITH_BARRIER_FLAG, Common.ELIDED, \"1\" }, phase = CompilePhase.FINAL_CODE)\n+    static void testAllocateArrayThenStoreAtUnknownIndex(Outer o, int index) {\n+        Outer[] a = new Outer[42];\n+        Common.blackhole(a);\n+        Common.outerArrayVarHandle.setVolatile(a, index, o);\n+    }\n+\n+    @Test\n+    @IR(counts = { IRNode.Z_LOAD_P_WITH_BARRIER_FLAG, Common.REMAINING, \"1\" }, phase = CompilePhase.FINAL_CODE)\n+    @IR(counts = { IRNode.Z_LOAD_P_WITH_BARRIER_FLAG, Common.ELIDED, \"1\" }, phase = CompilePhase.FINAL_CODE)\n+    static void testArrayLoadThenLoad(Outer[] a) {\n+        Common.blackhole(Common.outerArrayVarHandle.getVolatile(a, 0));\n+        Common.blackhole(Common.outerArrayVarHandle.getVolatile(a, 0));\n+    }\n+\n+    @Test\n+    @IR(counts = { IRNode.Z_STORE_P_WITH_BARRIER_FLAG, Common.REMAINING, \"1\" }, phase = CompilePhase.FINAL_CODE)\n+    @IR(counts = { IRNode.Z_STORE_P_WITH_BARRIER_FLAG, Common.ELIDED, \"1\" }, phase = CompilePhase.FINAL_CODE)\n+    static void testArrayStoreThenStore(Outer[] a, Outer o) {\n+        Common.outerArrayVarHandle.setVolatile(a, 0, o);\n+        Common.outerArrayVarHandle.setVolatile(a, 0, o);\n+    }\n+\n+    @Test\n+    @IR(counts = { IRNode.Z_STORE_P_WITH_BARRIER_FLAG, Common.REMAINING, \"1\" }, phase = CompilePhase.FINAL_CODE)\n+    @IR(counts = { IRNode.Z_LOAD_P_WITH_BARRIER_FLAG, Common.ELIDED, \"1\" }, phase = CompilePhase.FINAL_CODE)\n+    static void testArrayStoreThenLoad(Outer[] a, Outer o) {\n+        Common.outerArrayVarHandle.setVolatile(a, 0, o);\n+        Common.blackhole(Common.outerArrayVarHandle.getVolatile(a, 0));\n+    }\n+\n+    @Run(test = {\"testAllocateArrayThenStoreAtKnownIndex\",\n+                 \"testAllocateArrayThenStoreAtUnknownIndex\",\n+                 \"testArrayLoadThenLoad\",\n+                 \"testArrayStoreThenStore\",\n+                 \"testArrayStoreThenLoad\"})\n+    void runArrayTests() {\n+        testAllocateArrayThenStoreAtKnownIndex(Common.outer);\n+        testAllocateArrayThenStoreAtUnknownIndex(Common.outer, 10);\n+        testArrayLoadThenLoad(Common.outerArray);\n+        testArrayStoreThenStore(Common.outerArray, Common.outer);\n+        testArrayStoreThenLoad(Common.outerArray, Common.outer);\n+    }\n+\n+    @Test\n+    @IR(counts = { IRNode.Z_STORE_P_WITH_BARRIER_FLAG, Common.REMAINING, \"1\" }, phase = CompilePhase.FINAL_CODE)\n+    @IR(counts = { IRNode.Z_STORE_P_WITH_BARRIER_FLAG, Common.ELIDED, \"1\" }, phase = CompilePhase.FINAL_CODE)\n+    static void testStoreThenConditionalStore(Outer o, Inner i, int value) {\n+        o.field1 = i;\n+        if (value % 2 == 0) {\n+            o.field1 = i;\n+        }\n+    }\n+\n+    @Test\n+    @IR(counts = { IRNode.Z_STORE_P_WITH_BARRIER_FLAG, Common.REMAINING, \"1\" }, phase = CompilePhase.FINAL_CODE)\n+    @IR(counts = { IRNode.Z_STORE_P_WITH_BARRIER_FLAG, Common.ELIDED, \"1\" }, phase = CompilePhase.FINAL_CODE)\n+    static void testStoreThenStoreInLoop(Outer o, Inner i) {\n+        o.field1 = i;\n+        for (int j = 0; j < 100; j++) {\n+            o.field1 = i;\n+        }\n+    }\n+\n+    @Run(test = {\"testStoreThenConditionalStore\",\n+                 \"testStoreThenStoreInLoop\"})\n+    void runControlFlowTests() {\n+        testStoreThenConditionalStore(Common.outer, Common.inner, ThreadLocalRandom.current().nextInt(0, 100));\n+        testStoreThenStoreInLoop(Common.outer, Common.inner);\n+    }\n+\n+    @Test\n+    @IR(counts = { IRNode.Z_STORE_P_WITH_BARRIER_FLAG, Common.REMAINING, \"1\" }, phase = CompilePhase.FINAL_CODE)\n+    @IR(counts = { IRNode.Z_GET_AND_SET_P_WITH_BARRIER_FLAG, Common.ELIDED, \"1\" }, phase = CompilePhase.FINAL_CODE)\n+    static void testStoreThenAtomic(Outer o, Inner i) {\n+        o.field1 = i;\n+        Common.field1VarHandle.getAndSet​(o, i);\n+    }\n+\n+    @Test\n+    @IR(counts = { IRNode.Z_GET_AND_SET_P_WITH_BARRIER_FLAG, Common.REMAINING, \"1\" }, phase = CompilePhase.FINAL_CODE)\n+    @IR(counts = { IRNode.Z_LOAD_P_WITH_BARRIER_FLAG, Common.ELIDED, \"1\" }, phase = CompilePhase.FINAL_CODE)\n+    static void testAtomicThenLoad(Outer o, Inner i) {\n+        Common.field1VarHandle.getAndSet​(o, i);\n+        Common.blackhole(o.field1);\n+    }\n+\n+    @Test\n+    @IR(counts = { IRNode.Z_GET_AND_SET_P_WITH_BARRIER_FLAG, Common.REMAINING, \"1\" }, phase = CompilePhase.FINAL_CODE)\n+    @IR(counts = { IRNode.Z_STORE_P_WITH_BARRIER_FLAG, Common.ELIDED, \"1\" }, phase = CompilePhase.FINAL_CODE)\n+    static void testAtomicThenStore(Outer o, Inner i) {\n+        Common.field1VarHandle.getAndSet​(o, i);\n+        o.field1 = i;\n+    }\n+\n+    @Test\n+    @IR(counts = { IRNode.Z_GET_AND_SET_P_WITH_BARRIER_FLAG, Common.REMAINING, \"1\" }, phase = CompilePhase.FINAL_CODE)\n+    @IR(counts = { IRNode.Z_GET_AND_SET_P_WITH_BARRIER_FLAG, Common.ELIDED, \"1\" }, phase = CompilePhase.FINAL_CODE)\n+    static void testAtomicThenAtomic(Outer o, Inner i) {\n+        Common.field1VarHandle.getAndSet​(o, i);\n+        Common.field1VarHandle.getAndSet​(o, i);\n+    }\n+\n+    @Test\n+    @IR(counts = { IRNode.Z_GET_AND_SET_P_WITH_BARRIER_FLAG, Common.REMAINING, \"1\" }, phase = CompilePhase.FINAL_CODE)\n+    @IR(counts = { IRNode.Z_GET_AND_SET_P_WITH_BARRIER_FLAG, Common.ELIDED, \"1\" }, phase = CompilePhase.FINAL_CODE)\n+    static void testArrayAtomicThenAtomic(Outer[] a, Outer o) {\n+        Common.outerArrayVarHandle.getAndSet(a, 0, o);\n+        Common.outerArrayVarHandle.getAndSet(a, 0, o);\n+    }\n+\n+    @Run(test = {\"testStoreThenAtomic\",\n+                 \"testAtomicThenLoad\",\n+                 \"testAtomicThenStore\",\n+                 \"testAtomicThenAtomic\",\n+                 \"testArrayAtomicThenAtomic\"})\n+    void runAtomicOperationTests() {\n+        testStoreThenAtomic(Common.outer, Common.inner);\n+        testAtomicThenLoad(Common.outer, Common.inner);\n+        testAtomicThenStore(Common.outer, Common.inner);\n+        testAtomicThenAtomic(Common.outer, Common.inner);\n+        testArrayAtomicThenAtomic(Common.outerArray, Common.outer);\n+    }\n+}\n","filename":"test\/hotspot\/jtreg\/compiler\/gcbarriers\/TestZGCBarrierElision.java","additions":440,"deletions":0,"binary":false,"changes":440,"status":"added"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -24,0 +24,16 @@\n+\/*\n+ * @test id=ZDebug\n+ * @key randomness\n+ * @bug 8059022 8271855\n+ * @modules java.base\/jdk.internal.misc:+open\n+ * @summary Validate barriers after Unsafe getReference, CAS and swap (GetAndSet)\n+ * @requires vm.gc.Z & vm.debug\n+ * @library \/test\/lib\n+ * @run main\/othervm -XX:+UseZGC\n+ *                   -XX:+UnlockDiagnosticVMOptions\n+ *                   -XX:+ZVerifyOops -XX:ZCollectionInterval=1\n+ *                   -XX:-CreateCoredumpOnCrash\n+ *                   -XX:CompileCommand=dontinline,*::mergeImpl*\n+ *                   compiler.gcbarriers.UnsafeIntrinsicsTest\n+ *\/\n+\n@@ -30,1 +46,1 @@\n- * @requires vm.gc.Z\n+ * @requires vm.gc.Z & !vm.debug\n@@ -34,1 +50,1 @@\n- *                   -XX:+ZVerifyViews -XX:ZCollectionInterval=1\n+ *                   -XX:ZCollectionInterval=1\n@@ -292,1 +308,1 @@\n-        UNSAFE.storeFence(); \/\/ Make all new Node fields visible to concurrent readers.\n+        UNSAFE.storeFence(); \/\/ We need the contents of the published node to be released\n","filename":"test\/hotspot\/jtreg\/compiler\/gcbarriers\/UnsafeIntrinsicsTest.java","additions":20,"deletions":4,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -1441,0 +1441,18 @@\n+    public static final String Z_LOAD_P_WITH_BARRIER_FLAG = COMPOSITE_PREFIX + \"Z_LOAD_P_WITH_BARRIER_FLAG\" + POSTFIX;\n+    static {\n+        String regex = START + \"zLoadP\\\\S*\" + MID + \"barrier\\\\(\\\\s*\" + IS_REPLACED + \"\\\\s*\\\\)\" + END;\n+        machOnly(Z_LOAD_P_WITH_BARRIER_FLAG, regex);\n+    }\n+\n+    public static final String Z_STORE_P_WITH_BARRIER_FLAG = COMPOSITE_PREFIX + \"Z_STORE_P_WITH_BARRIER_FLAG\" + POSTFIX;\n+    static {\n+        String regex = START + \"zStoreP\\\\S*\" + MID + \"barrier\\\\(\\\\s*\" + IS_REPLACED + \"\\\\s*\\\\)\" + END;\n+        machOnly(Z_STORE_P_WITH_BARRIER_FLAG, regex);\n+    }\n+\n+    public static final String Z_GET_AND_SET_P_WITH_BARRIER_FLAG = COMPOSITE_PREFIX + \"Z_GET_AND_SET_P_WITH_BARRIER_FLAG\" + POSTFIX;\n+    static {\n+        String regex = START + \"(zXChgP)|(zGetAndSetP\\\\S*)\" + MID + \"barrier\\\\(\\\\s*\" + IS_REPLACED + \"\\\\s*\\\\)\" + END;\n+        machOnly(Z_GET_AND_SET_P_WITH_BARRIER_FLAG, regex);\n+    }\n+\n","filename":"test\/hotspot\/jtreg\/compiler\/lib\/ir_framework\/IRNode.java","additions":18,"deletions":0,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2013, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2013, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -60,0 +60,9 @@\n+\/*\n+ * @test id=ZSingleGenMode\n+ * @requires vm.gc.Z & !vm.opt.final.ZGenerational\n+ * @comment ZGC will not start when LargePages cannot be allocated, therefore\n+ *          we do not run such configuration.\n+ * @summary Runs System.gc() with different flags.\n+ * @run main\/othervm -XX:+UseZGC -XX:-ZGenerational gc.TestSystemGC\n+ *\/\n+\n@@ -62,1 +71,1 @@\n- * @requires vm.gc.Z\n+ * @requires vm.gc.Z & vm.opt.final.ZGenerational\n@@ -66,1 +75,1 @@\n- * @run main\/othervm -XX:+UseZGC gc.TestSystemGC\n+ * @run main\/othervm -XX:+UseZGC -XX:+ZGenerational gc.TestSystemGC\n","filename":"test\/hotspot\/jtreg\/gc\/TestSystemGC.java","additions":12,"deletions":3,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -29,4 +29,4 @@\n- *\n- * @comment ZGC doesn't use Universe::verify and will not log the output below\n- * @requires !vm.gc.Z\n- *\n+ * @comment Generational ZGC can't use the generic Universe::verify\n+ *          because there's no guarantee that we will ever have\n+ *          a stable snapshot where all roots can be verified.\n+ * @requires vm.gc != \"Z\"\n","filename":"test\/hotspot\/jtreg\/gc\/TestVerifySubSet.java","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -33,1 +33,1 @@\n- * @requires vm.gc.Z\n+ * @requires vm.gc.Z & vm.opt.final.ZGenerational\n@@ -36,1 +36,10 @@\n- * @run main\/othervm\/timeout=200 -Xlog:gc*=info -Xmx384m -server -XX:+UseZGC gc.stress.gcbasher.TestGCBasherWithZ 120000\n+ * @run main\/othervm\/timeout=200 -Xlog:gc*=info -Xmx384m -server -XX:+UseZGC -XX:+ZGenerational  gc.stress.gcbasher.TestGCBasherWithZ 120000\n+ *\/\n+\/*\n+ * @test TestGCBasherWithZSingleGenMode\n+ * @key stress\n+ * @library \/\n+ * @requires vm.gc.Z & !vm.opt.final.ZGenerational\n+ * @requires vm.flavor == \"server\" & !vm.emulatedClient\n+ * @summary Stress ZGC\n+ * @run main\/othervm\/timeout=200 -Xlog:gc*=info -Xmx384m -server -XX:+UseZGC -XX:-ZGenerational gc.stress.gcbasher.TestGCBasherWithZ 120000\n@@ -43,1 +52,13 @@\n- * @requires vm.gc.Z\n+ * @requires vm.gc.Z & vm.opt.final.ZGenerational\n+ * @requires vm.flavor == \"server\" & !vm.emulatedClient & vm.opt.ClassUnloading != false\n+ * @summary Stress ZGC with nmethod barrier forced deoptimization enabled.\n+ * @run main\/othervm\/timeout=200 -Xlog:gc*=info,nmethod+barrier=trace -Xmx384m -server -XX:+UseZGC -XX:+ZGenerational\n+ *   -XX:+UnlockDiagnosticVMOptions -XX:+DeoptimizeNMethodBarriersALot -XX:-Inline\n+ *   gc.stress.gcbasher.TestGCBasherWithZ 120000\n+ *\/\n+\n+\/*\n+ * @test TestGCBasherDeoptWithZSingleGenMode\n+ * @key stress\n+ * @library \/\n+ * @requires vm.gc.Z & !vm.opt.final.ZGenerational\n@@ -46,1 +67,1 @@\n- * @run main\/othervm\/timeout=200 -Xlog:gc*=info,nmethod+barrier=trace -Xmx384m -server -XX:+UseZGC\n+ * @run main\/othervm\/timeout=200 -Xlog:gc*=info,nmethod+barrier=trace -Xmx384m -server -XX:+UseZGC -XX:-ZGenerational\n","filename":"test\/hotspot\/jtreg\/gc\/stress\/gcbasher\/TestGCBasherWithZ.java","additions":26,"deletions":5,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -31,1 +31,1 @@\n- * @requires vm.gc.Z\n+ * @requires vm.gc.Z & vm.opt.final.ZGenerational\n@@ -33,2 +33,12 @@\n- * @run main\/othervm -Xmx384M -XX:+UseZGC gc.stress.gcold.TestGCOldWithZ 50 1 20 10 10000\n- * @run main\/othervm -Xmx256m -XX:+UseZGC gc.stress.gcold.TestGCOldWithZ 50 5 20 1 5000\n+ * @run main\/othervm -Xmx384M -XX:+UseZGC -XX:+ZGenerational gc.stress.gcold.TestGCOldWithZ 50 1 20 10 10000\n+ * @run main\/othervm -Xmx256m -XX:+UseZGC -XX:+ZGenerational gc.stress.gcold.TestGCOldWithZ 50 5 20 1 5000\n+ *\/\n+\n+\/*\n+ * @test TestGCOldWithZSingleGenMode\n+ * @key randomness\n+ * @library \/ \/test\/lib\n+ * @requires vm.gc.Z & !vm.opt.final.ZGenerational\n+ * @summary Stress the Z\n+ * @run main\/othervm -Xmx384M -XX:+UseZGC -XX:-ZGenerational gc.stress.gcold.TestGCOldWithZ 50 1 20 10 10000\n+ * @run main\/othervm -Xmx256m -XX:+UseZGC -XX:-ZGenerational gc.stress.gcold.TestGCOldWithZ 50 5 20 1 5000\n","filename":"test\/hotspot\/jtreg\/gc\/stress\/gcold\/TestGCOldWithZ.java","additions":14,"deletions":4,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2014, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -103,1 +103,1 @@\n-                if (info.getGcName().startsWith(\"Shenandoah\") || info.getGcName().startsWith(\"ZGC\")) {\n+                if (info.getGcName().startsWith(\"Shenandoah\")) {\n@@ -107,0 +107,11 @@\n+                } else if (info.getGcName().startsWith(\"ZGC\")) {\n+                    \/\/ Generational ZGC only triggers string deduplications from major collections\n+                    if (info.getGcName().startsWith(\"ZGC Major\") && \"end of GC cycle\".equals(info.getGcAction())) {\n+                        gcCount++;\n+                    }\n+\n+                    \/\/ Single-gen ZGC\n+                    if (!info.getGcName().startsWith(\"ZGC Major\") && !info.getGcName().startsWith(\"ZGC Minor\") &&\n+                            \"end of GC cycle\".equals(info.getGcAction())) {\n+                        gcCount++;\n+                    }\n","filename":"test\/hotspot\/jtreg\/gc\/stringdedup\/TestStringDeduplicationTools.java","additions":13,"deletions":2,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -0,0 +1,56 @@\n+\/*\n+ * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package gc.x;\n+\n+\/*\n+ * @test TestAllocateHeapAt\n+ * @requires vm.gc.Z & !vm.opt.final.ZGenerational & os.family == \"linux\"\n+ * @summary Test ZGC with -XX:AllocateHeapAt\n+ * @library \/test\/lib\n+ * @run main\/othervm gc.x.TestAllocateHeapAt . true\n+ * @run main\/othervm gc.x.TestAllocateHeapAt non-existing-directory false\n+ *\/\n+\n+import jdk.test.lib.process.ProcessTools;\n+\n+public class TestAllocateHeapAt {\n+    public static void main(String[] args) throws Exception {\n+        final String directory = args[0];\n+        final boolean exists = Boolean.parseBoolean(args[1]);\n+        final String heapBackingFile = \"Heap Backing File: \" + directory;\n+        final String failedToCreateFile = \"Failed to create file \" + directory;\n+\n+        ProcessTools.executeProcess(ProcessTools.createJavaProcessBuilder(\n+                \"-XX:+UseZGC\",\n+                \"-XX:-ZGenerational\",\n+                \"-Xlog:gc*\",\n+                \"-Xms32M\",\n+                \"-Xmx32M\",\n+                \"-XX:AllocateHeapAt=\" + directory,\n+                \"-version\"))\n+            .shouldContain(exists ? heapBackingFile : failedToCreateFile)\n+            .shouldNotContain(exists ? failedToCreateFile : heapBackingFile)\n+            .shouldHaveExitValue(exists ? 0 : 1);\n+    }\n+}\n","filename":"test\/hotspot\/jtreg\/gc\/x\/TestAllocateHeapAt.java","additions":56,"deletions":0,"binary":false,"changes":56,"status":"added"},{"patch":"@@ -0,0 +1,41 @@\n+\/*\n+ * Copyright (c) 2019, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package gc.x;\n+\n+\/*\n+ * @test TestAlwaysPreTouch\n+ * @requires vm.gc.Z & !vm.opt.final.ZGenerational\n+ * @summary Test ZGC parallel pre-touch\n+ * @run main\/othervm -XX:+UseZGC -XX:-ZGenerational -Xlog:gc* -XX:-AlwaysPreTouch -Xms128M -Xmx128M gc.x.TestAlwaysPreTouch\n+ * @run main\/othervm -XX:+UseZGC -XX:-ZGenerational -Xlog:gc* -XX:+AlwaysPreTouch -XX:ParallelGCThreads=1 -Xms2M -Xmx128M gc.x.TestAlwaysPreTouch\n+ * @run main\/othervm -XX:+UseZGC -XX:-ZGenerational -Xlog:gc* -XX:+AlwaysPreTouch -XX:ParallelGCThreads=8 -Xms2M -Xmx128M gc.x.TestAlwaysPreTouch\n+ * @run main\/othervm -XX:+UseZGC -XX:-ZGenerational -Xlog:gc* -XX:+AlwaysPreTouch -XX:ParallelGCThreads=1 -Xms128M -Xmx128M gc.x.TestAlwaysPreTouch\n+ * @run main\/othervm -XX:+UseZGC -XX:-ZGenerational -Xlog:gc* -XX:+AlwaysPreTouch -XX:ParallelGCThreads=8 -Xms128M -Xmx128M gc.x.TestAlwaysPreTouch\n+ *\/\n+\n+public class TestAlwaysPreTouch {\n+    public static void main(String[] args) throws Exception {\n+        System.out.println(\"Success\");\n+    }\n+}\n","filename":"test\/hotspot\/jtreg\/gc\/x\/TestAlwaysPreTouch.java","additions":41,"deletions":0,"binary":false,"changes":41,"status":"added"},{"patch":"@@ -24,1 +24,1 @@\n-package gc.z;\n+package gc.x;\n@@ -28,1 +28,1 @@\n- * @requires vm.gc.Z\n+ * @requires vm.gc.Z & !vm.opt.final.ZGenerational\n@@ -31,2 +31,2 @@\n- * @run main\/othervm -XX:+UseZGC -Xms256M -Xmx512M -Xlog:gc gc.z.TestGarbageCollectorMXBean 256 512\n- * @run main\/othervm -XX:+UseZGC -Xms512M -Xmx512M -Xlog:gc gc.z.TestGarbageCollectorMXBean 512 512\n+ * @run main\/othervm -XX:+UseZGC -XX:-ZGenerational -Xms256M -Xmx512M -Xlog:gc gc.x.TestGarbageCollectorMXBean 256 512\n+ * @run main\/othervm -XX:+UseZGC -XX:-ZGenerational -Xms512M -Xmx512M -Xlog:gc gc.x.TestGarbageCollectorMXBean 512 512\n","filename":"test\/hotspot\/jtreg\/gc\/x\/TestGarbageCollectorMXBean.java","additions":4,"deletions":4,"binary":false,"changes":8,"previous_filename":"test\/hotspot\/jtreg\/gc\/z\/TestGarbageCollectorMXBean.java","status":"copied"},{"patch":"@@ -24,1 +24,1 @@\n-package gc.z;\n+package gc.x;\n@@ -28,1 +28,1 @@\n- * @requires vm.gc.Z\n+ * @requires vm.gc.Z & !vm.opt.final.ZGenerational\n@@ -31,1 +31,1 @@\n- * @run main\/othervm gc.z.TestHighUsage\n+ * @run main\/othervm gc.x.TestHighUsage\n@@ -89,0 +89,1 @@\n+                                    \"-XX:-ZGenerational\",\n","filename":"test\/hotspot\/jtreg\/gc\/x\/TestHighUsage.java","additions":4,"deletions":3,"binary":false,"changes":7,"previous_filename":"test\/hotspot\/jtreg\/gc\/z\/TestHighUsage.java","status":"copied"},{"patch":"@@ -0,0 +1,65 @@\n+\/*\n+ * Copyright (c) 2020, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package gc.x;\n+\n+\/**\n+ * @test TestMemoryMXBean\n+ * @requires vm.gc.Z & !vm.opt.final.ZGenerational\n+ * @summary Test ZGC heap memory MXBean\n+ * @modules java.management\n+ * @run main\/othervm -XX:+UseZGC -XX:-ZGenerational -Xms128M -Xmx256M -Xlog:gc* gc.x.TestMemoryMXBean 128 256\n+ * @run main\/othervm -XX:+UseZGC -XX:-ZGenerational -Xms256M -Xmx256M -Xlog:gc* gc.x.TestMemoryMXBean 256 256\n+ *\/\n+\n+import java.lang.management.ManagementFactory;\n+\n+public class TestMemoryMXBean {\n+    public static void main(String[] args) throws Exception {\n+        final long M = 1024 * 1024;\n+        final long expectedInitialCapacity = Long.parseLong(args[0]) * M;\n+        final long expectedMaxCapacity = Long.parseLong(args[1]) * M;\n+        final var memoryUsage = ManagementFactory.getMemoryMXBean().getHeapMemoryUsage();\n+        final long initialCapacity = memoryUsage.getInit();\n+        final long capacity = memoryUsage.getCommitted();\n+        final long maxCapacity = memoryUsage.getMax();\n+\n+        System.out.println(\"expectedInitialCapacity: \" + expectedInitialCapacity);\n+        System.out.println(\"    expectedMaxCapacity: \" + expectedMaxCapacity);\n+        System.out.println(\"        initialCapacity: \" + initialCapacity);\n+        System.out.println(\"               capacity: \" + capacity);\n+        System.out.println(\"            maxCapacity: \" + maxCapacity);\n+\n+        if (initialCapacity != expectedInitialCapacity) {\n+            throw new Exception(\"Unexpected initial capacity\");\n+        }\n+\n+        if (maxCapacity != expectedMaxCapacity) {\n+            throw new Exception(\"Unexpected max capacity\");\n+        }\n+\n+        if (capacity < initialCapacity || capacity > maxCapacity) {\n+            throw new Exception(\"Unexpected capacity\");\n+        }\n+    }\n+}\n","filename":"test\/hotspot\/jtreg\/gc\/x\/TestMemoryMXBean.java","additions":65,"deletions":0,"binary":false,"changes":65,"status":"added"},{"patch":"@@ -24,1 +24,1 @@\n-package gc.z;\n+package gc.x;\n@@ -28,1 +28,1 @@\n- * @requires vm.gc.Z\n+ * @requires vm.gc.Z & !vm.opt.final.ZGenerational\n@@ -31,1 +31,1 @@\n- * @run main\/othervm -XX:+UseZGC -Xmx128M gc.z.TestMemoryManagerMXBean\n+ * @run main\/othervm -XX:+UseZGC -XX:-ZGenerational -Xmx128M gc.x.TestMemoryManagerMXBean\n","filename":"test\/hotspot\/jtreg\/gc\/x\/TestMemoryManagerMXBean.java","additions":3,"deletions":3,"binary":false,"changes":6,"previous_filename":"test\/hotspot\/jtreg\/gc\/z\/TestMemoryManagerMXBean.java","status":"copied"},{"patch":"@@ -0,0 +1,63 @@\n+\/*\n+ * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package gc.x;\n+\n+\/*\n+ * @test TestNoUncommit\n+ * @requires vm.gc.Z  & !vm.opt.final.ZGenerational & !vm.graal.enabled\n+ * @summary Test ZGC uncommit unused memory disabled\n+ * @run main\/othervm -XX:+UseZGC -XX:-ZGenerational -Xlog:gc*,gc+heap=debug,gc+stats=off -Xms512M -Xmx512M -XX:ZUncommitDelay=1 gc.x.TestNoUncommit\n+ * @run main\/othervm -XX:+UseZGC -XX:-ZGenerational -Xlog:gc*,gc+heap=debug,gc+stats=off -Xms128M -Xmx512M -XX:ZUncommitDelay=1 -XX:-ZUncommit gc.x.TestNoUncommit\n+ *\/\n+\n+public class TestNoUncommit {\n+    private static final int allocSize = 200 * 1024 * 1024; \/\/ 200M\n+    private static volatile Object keepAlive = null;\n+\n+    private static long capacity() {\n+        return Runtime.getRuntime().totalMemory();\n+    }\n+\n+    public static void main(String[] args) throws Exception {\n+        System.out.println(\"Allocating\");\n+        keepAlive = new byte[allocSize];\n+        final var afterAlloc = capacity();\n+\n+        System.out.println(\"Reclaiming\");\n+        keepAlive = null;\n+        System.gc();\n+\n+        \/\/ Wait longer than the uncommit delay (which is 1 second)\n+        Thread.sleep(5 * 1000);\n+\n+        final var afterDelay = capacity();\n+\n+        \/\/ Verify\n+        if (afterAlloc > afterDelay) {\n+            throw new Exception(\"Should not uncommit\");\n+        }\n+\n+        System.out.println(\"Success\");\n+    }\n+}\n","filename":"test\/hotspot\/jtreg\/gc\/x\/TestNoUncommit.java","additions":63,"deletions":0,"binary":false,"changes":63,"status":"added"},{"patch":"@@ -24,1 +24,1 @@\n-package gc.z;\n+package gc.x;\n@@ -28,1 +28,1 @@\n- * @requires vm.gc.Z\n+ * @requires vm.gc.Z & !vm.opt.final.ZGenerational\n@@ -31,1 +31,1 @@\n- * @run driver gc.z.TestPageCacheFlush\n+ * @run driver gc.x.TestPageCacheFlush\n@@ -73,0 +73,1 @@\n+                                    \"-XX:-ZGenerational\",\n","filename":"test\/hotspot\/jtreg\/gc\/x\/TestPageCacheFlush.java","additions":4,"deletions":3,"binary":false,"changes":7,"previous_filename":"test\/hotspot\/jtreg\/gc\/z\/TestPageCacheFlush.java","status":"copied"},{"patch":"@@ -24,1 +24,1 @@\n-package gc.z;\n+package gc.x;\n@@ -28,1 +28,1 @@\n- * @requires vm.gc.Z\n+ * @requires vm.gc.Z & !vm.opt.final.ZGenerational\n@@ -30,1 +30,1 @@\n- * @run main\/othervm -XX:+UseZGC -Xlog:gc*,gc+stats=off -Xmx256M -XX:+UnlockDiagnosticVMOptions -XX:+ZStressRelocateInPlace gc.z.TestRelocateInPlace\n+ * @run main\/othervm -XX:+UseZGC -XX:-ZGenerational -Xlog:gc*,gc+stats=off -Xmx256M -XX:+UnlockDiagnosticVMOptions -XX:+ZStressRelocateInPlace gc.x.TestRelocateInPlace\n","filename":"test\/hotspot\/jtreg\/gc\/x\/TestRelocateInPlace.java","additions":3,"deletions":3,"binary":false,"changes":6,"previous_filename":"test\/hotspot\/jtreg\/gc\/z\/TestRelocateInPlace.java","status":"copied"},{"patch":"@@ -24,1 +24,1 @@\n-package gc.z;\n+package gc.x;\n@@ -28,1 +28,1 @@\n- * @requires vm.gc.Z\n+ * @requires vm.gc.Z & !vm.opt.final.ZGenerational\n@@ -31,1 +31,1 @@\n- * @run driver gc.z.TestSmallHeap 8M 16M 32M 64M 128M 256M 512M 1024M\n+ * @run driver gc.x.TestSmallHeap 8M 16M 32M 64M 128M 256M 512M 1024M\n@@ -58,0 +58,1 @@\n+                                        \"-XX:-ZGenerational\",\n","filename":"test\/hotspot\/jtreg\/gc\/x\/TestSmallHeap.java","additions":4,"deletions":3,"binary":false,"changes":7,"previous_filename":"test\/hotspot\/jtreg\/gc\/z\/TestSmallHeap.java","status":"copied"},{"patch":"@@ -24,1 +24,1 @@\n-package gc.z;\n+package gc.x;\n@@ -28,1 +28,1 @@\n- * @requires vm.gc.Z\n+ * @requires vm.gc.Z & !vm.opt.final.ZGenerational\n@@ -31,1 +31,1 @@\n- * @run main\/othervm -XX:+UseZGC -Xlog:gc*,gc+heap=debug,gc+stats=off -Xms128M -Xmx512M -XX:ZUncommitDelay=10 gc.z.TestUncommit\n+ * @run main\/othervm -XX:+UseZGC -XX:-ZGenerational -Xlog:gc*,gc+heap=debug,gc+stats=off -Xms128M -Xmx512M -XX:ZUncommitDelay=10 gc.x.TestUncommit\n","filename":"test\/hotspot\/jtreg\/gc\/x\/TestUncommit.java","additions":3,"deletions":3,"binary":false,"changes":6,"previous_filename":"test\/hotspot\/jtreg\/gc\/z\/TestUncommit.java","status":"copied"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,1 +28,1 @@\n- * @requires vm.gc.Z & os.family == \"linux\"\n+ * @requires vm.gc.Z & vm.opt.final.ZGenerational & os.family == \"linux\"\n@@ -46,0 +46,1 @@\n+                \"-XX:+ZGenerational\",\n","filename":"test\/hotspot\/jtreg\/gc\/z\/TestAllocateHeapAt.java","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2019, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2019, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,1 +28,1 @@\n- * @requires vm.gc.Z\n+ * @requires vm.gc.Z & vm.opt.final.ZGenerational\n@@ -30,5 +30,5 @@\n- * @run main\/othervm -XX:+UseZGC -Xlog:gc* -XX:-AlwaysPreTouch -Xms128M -Xmx128M gc.z.TestAlwaysPreTouch\n- * @run main\/othervm -XX:+UseZGC -Xlog:gc* -XX:+AlwaysPreTouch -XX:ParallelGCThreads=1 -Xms2M -Xmx128M gc.z.TestAlwaysPreTouch\n- * @run main\/othervm -XX:+UseZGC -Xlog:gc* -XX:+AlwaysPreTouch -XX:ParallelGCThreads=8 -Xms2M -Xmx128M gc.z.TestAlwaysPreTouch\n- * @run main\/othervm -XX:+UseZGC -Xlog:gc* -XX:+AlwaysPreTouch -XX:ParallelGCThreads=1 -Xms128M -Xmx128M gc.z.TestAlwaysPreTouch\n- * @run main\/othervm -XX:+UseZGC -Xlog:gc* -XX:+AlwaysPreTouch -XX:ParallelGCThreads=8 -Xms128M -Xmx128M gc.z.TestAlwaysPreTouch\n+ * @run main\/othervm -XX:+UseZGC -XX:+ZGenerational -Xlog:gc* -XX:-AlwaysPreTouch -Xms128M -Xmx128M gc.z.TestAlwaysPreTouch\n+ * @run main\/othervm -XX:+UseZGC -XX:+ZGenerational -Xlog:gc* -XX:+AlwaysPreTouch -XX:ParallelGCThreads=1 -Xms2M -Xmx128M gc.z.TestAlwaysPreTouch\n+ * @run main\/othervm -XX:+UseZGC -XX:+ZGenerational -Xlog:gc* -XX:+AlwaysPreTouch -XX:ParallelGCThreads=8 -Xms2M -Xmx128M gc.z.TestAlwaysPreTouch\n+ * @run main\/othervm -XX:+UseZGC -XX:+ZGenerational -Xlog:gc* -XX:+AlwaysPreTouch -XX:ParallelGCThreads=1 -Xms128M -Xmx128M gc.z.TestAlwaysPreTouch\n+ * @run main\/othervm -XX:+UseZGC -XX:+ZGenerational -Xlog:gc* -XX:+AlwaysPreTouch -XX:ParallelGCThreads=8 -Xms128M -Xmx128M gc.z.TestAlwaysPreTouch\n","filename":"test\/hotspot\/jtreg\/gc\/z\/TestAlwaysPreTouch.java","additions":7,"deletions":7,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,1 +28,1 @@\n- * @requires vm.gc.Z\n+ * @requires vm.gc.Z & vm.opt.final.ZGenerational\n@@ -31,2 +31,2 @@\n- * @run main\/othervm -XX:+UseZGC -Xms256M -Xmx512M -Xlog:gc gc.z.TestGarbageCollectorMXBean 256 512\n- * @run main\/othervm -XX:+UseZGC -Xms512M -Xmx512M -Xlog:gc gc.z.TestGarbageCollectorMXBean 512 512\n+ * @run main\/othervm -XX:+UseZGC -XX:+ZGenerational -Xms256M -Xmx512M -Xlog:gc gc.z.TestGarbageCollectorMXBean 256 512\n+ * @run main\/othervm -XX:+UseZGC -XX:+ZGenerational -Xms512M -Xmx512M -Xlog:gc gc.z.TestGarbageCollectorMXBean 512 512\n@@ -77,2 +77,4 @@\n-            final var memoryUsageBeforeGC = info.getGcInfo().getMemoryUsageBeforeGc().get(\"ZHeap\");\n-            final var memoryUsageAfterGC = info.getGcInfo().getMemoryUsageAfterGc().get(\"ZHeap\");\n+            final var youngMemoryUsageBeforeGC = info.getGcInfo().getMemoryUsageBeforeGc().get(\"ZGC Young Generation\");\n+            final var youngMemoryUsageAfterGC = info.getGcInfo().getMemoryUsageAfterGc().get(\"ZGC Young Generation\");\n+            final var oldMemoryUsageBeforeGC = info.getGcInfo().getMemoryUsageBeforeGc().get(\"ZGC Old Generation\");\n+            final var oldMemoryUsageAfterGC = info.getGcInfo().getMemoryUsageAfterGc().get(\"ZGC Old Generation\");\n@@ -81,8 +83,10 @@\n-            log(\"                  Id: \" + id);\n-            log(\"              Action: \" + action);\n-            log(\"               Cause: \" + cause);\n-            log(\"           StartTime: \" + startTime);\n-            log(\"             EndTime: \" + endTime);\n-            log(\"            Duration: \" + duration);\n-            log(\" MemoryUsageBeforeGC: \" + memoryUsageBeforeGC);\n-            log(\"  MemoryUsageAfterGC: \" + memoryUsageAfterGC);\n+            log(\"                        Id: \" + id);\n+            log(\"                    Action: \" + action);\n+            log(\"                     Cause: \" + cause);\n+            log(\"                 StartTime: \" + startTime);\n+            log(\"                   EndTime: \" + endTime);\n+            log(\"                  Duration: \" + duration);\n+            log(\" Young MemoryUsageBeforeGC: \" + youngMemoryUsageBeforeGC);\n+            log(\"  Young MemoryUsageAfterGC: \" + youngMemoryUsageAfterGC);\n+            log(\"   Old MemoryUsageBeforeGC: \" + oldMemoryUsageBeforeGC);\n+            log(\"    Old MemoryUsageAfterGC: \" + oldMemoryUsageAfterGC);\n@@ -91,1 +95,1 @@\n-            if (name.equals(\"ZGC Cycles\")) {\n+            if (name.equals(\"ZGC Major Cycles\")) {\n@@ -99,2 +103,2 @@\n-                if (memoryUsageBeforeGC.getInit() != initialCapacity) {\n-                    log(\"ERROR: MemoryUsageBeforeGC.init\");\n+                if (oldMemoryUsageBeforeGC.getInit() != 0) {\n+                    log(\"ERROR: Old MemoryUsageBeforeGC.init\");\n@@ -104,2 +108,2 @@\n-                if (memoryUsageBeforeGC.getUsed() > initialCapacity) {\n-                    log(\"ERROR: MemoryUsageBeforeGC.used\");\n+                if (oldMemoryUsageBeforeGC.getUsed() > initialCapacity) {\n+                    log(\"ERROR: Old MemoryUsageBeforeGC.used\");\n@@ -109,2 +113,2 @@\n-                if (memoryUsageBeforeGC.getCommitted() != initialCapacity) {\n-                    log(\"ERROR: MemoryUsageBeforeGC.committed\");\n+                if (oldMemoryUsageBeforeGC.getCommitted() != oldMemoryUsageBeforeGC.getUsed()) {\n+                    log(\"ERROR: Old MemoryUsageBeforeGC.committed\");\n@@ -114,2 +118,2 @@\n-                if (memoryUsageBeforeGC.getMax() != maxCapacity) {\n-                    log(\"ERROR: MemoryUsageBeforeGC.max\");\n+                if (oldMemoryUsageBeforeGC.getMax() != maxCapacity) {\n+                    log(\"ERROR: Old MemoryUsageBeforeGC.max\");\n@@ -118,1 +122,1 @@\n-            } else if (name.equals(\"ZGC Pauses\")) {\n+            } else if (name.equals(\"ZGC Major Pauses\")) {\n@@ -126,2 +130,2 @@\n-                if (memoryUsageBeforeGC.getInit() != 0) {\n-                    log(\"ERROR: MemoryUsageBeforeGC.init\");\n+                if (oldMemoryUsageBeforeGC.getInit() != 0) {\n+                    log(\"ERROR: Old MemoryUsageBeforeGC.init\");\n@@ -131,2 +135,2 @@\n-                if (memoryUsageBeforeGC.getUsed() != 0) {\n-                    log(\"ERROR: MemoryUsageBeforeGC.used\");\n+                if (oldMemoryUsageBeforeGC.getUsed() != 0) {\n+                    log(\"ERROR: Old MemoryUsageBeforeGC.used\");\n@@ -136,2 +140,2 @@\n-                if (memoryUsageBeforeGC.getCommitted() != 0) {\n-                    log(\"ERROR: MemoryUsageBeforeGC.committed\");\n+                if (oldMemoryUsageBeforeGC.getCommitted() != 0) {\n+                    log(\"ERROR: Old MemoryUsageBeforeGC.committed\");\n@@ -141,2 +145,56 @@\n-                if (memoryUsageBeforeGC.getMax() != 0) {\n-                    log(\"ERROR: MemoryUsageBeforeGC.max\");\n+                if (oldMemoryUsageBeforeGC.getMax() != 0) {\n+                    log(\"ERROR: Old MemoryUsageBeforeGC.max\");\n+                    errors.incrementAndGet();\n+                }\n+            } else if (name.equals(\"ZGC Minor Cycles\")) {\n+                cycles.incrementAndGet();\n+\n+                if (!action.equals(\"end of GC cycle\")) {\n+                    log(\"ERROR: Action\");\n+                    errors.incrementAndGet();\n+                }\n+\n+                if (youngMemoryUsageBeforeGC.getInit() != initialCapacity) {\n+                    log(\"ERROR: Young MemoryUsageBeforeGC.init\");\n+                    errors.incrementAndGet();\n+                }\n+\n+                if (youngMemoryUsageBeforeGC.getUsed() > youngMemoryUsageBeforeGC.getCommitted()) {\n+                    log(\"ERROR: Young MemoryUsageBeforeGC.used\");\n+                    errors.incrementAndGet();\n+                }\n+\n+                if (youngMemoryUsageBeforeGC.getCommitted() > initialCapacity) {\n+                    log(\"ERROR: Young MemoryUsageBeforeGC.committed\");\n+                    errors.incrementAndGet();\n+                }\n+\n+                if (youngMemoryUsageBeforeGC.getMax() != maxCapacity) {\n+                    log(\"ERROR: Young MemoryUsageBeforeGC.max\");\n+                    errors.incrementAndGet();\n+                }\n+            } else if (name.equals(\"ZGC Minor Pauses\")) {\n+                pauses.incrementAndGet();\n+\n+                if (!action.equals(\"end of GC pause\")) {\n+                    log(\"ERROR: Action\");\n+                    errors.incrementAndGet();\n+                }\n+\n+                if (youngMemoryUsageBeforeGC.getInit() != 0) {\n+                    log(\"ERROR: Young MemoryUsageBeforeGC.init\");\n+                    errors.incrementAndGet();\n+                }\n+\n+                if (youngMemoryUsageBeforeGC.getUsed() != 0) {\n+                    log(\"ERROR: Young MemoryUsageBeforeGC.used\");\n+                    errors.incrementAndGet();\n+                }\n+\n+                if (youngMemoryUsageBeforeGC.getCommitted() != 0) {\n+                    log(\"ERROR: Young MemoryUsageBeforeGC.committed\");\n+                    errors.incrementAndGet();\n+                }\n+\n+                if (youngMemoryUsageBeforeGC.getMax() != 0) {\n+                    log(\"ERROR: Young MemoryUsageBeforeGC.max\");\n@@ -150,4 +208,4 @@\n-            if (!cause.equals(\"System.gc()\")) {\n-                log(\"ERROR: Cause\");\n-                errors.incrementAndGet();\n-            }\n+            \/\/if (!cause.equals(\"System.gc()\")) {\n+            \/\/    log(\"ERROR: Cause\");\n+            \/\/    errors.incrementAndGet();\n+            \/\/}\n","filename":"test\/hotspot\/jtreg\/gc\/z\/TestGarbageCollectorMXBean.java","additions":94,"deletions":36,"binary":false,"changes":130,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2019, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2019, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,1 +28,1 @@\n- * @requires vm.gc.Z\n+ * @requires vm.gc.Z & vm.opt.final.ZGenerational\n@@ -89,0 +89,1 @@\n+                                    \"-XX:+ZGenerational\",\n@@ -97,1 +98,0 @@\n-                    .shouldContain(\"High Usage\")\n","filename":"test\/hotspot\/jtreg\/gc\/z\/TestHighUsage.java","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,1 +28,1 @@\n- * @requires vm.gc.Z\n+ * @requires vm.gc.Z & vm.opt.final.ZGenerational\n@@ -31,2 +31,2 @@\n- * @run main\/othervm -XX:+UseZGC -Xms128M -Xmx256M -Xlog:gc* gc.z.TestMemoryMXBean 128 256\n- * @run main\/othervm -XX:+UseZGC -Xms256M -Xmx256M -Xlog:gc* gc.z.TestMemoryMXBean 256 256\n+ * @run main\/othervm -XX:+UseZGC -XX:+ZGenerational -Xms128M -Xmx256M -Xlog:gc* gc.z.TestMemoryMXBean 128 256\n+ * @run main\/othervm -XX:+UseZGC -XX:+ZGenerational -Xms256M -Xmx256M -Xlog:gc* gc.z.TestMemoryMXBean 256 256\n","filename":"test\/hotspot\/jtreg\/gc\/z\/TestMemoryMXBean.java","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,1 +28,1 @@\n- * @requires vm.gc.Z\n+ * @requires vm.gc.Z & vm.opt.final.ZGenerational\n@@ -31,1 +31,1 @@\n- * @run main\/othervm -XX:+UseZGC -Xmx128M gc.z.TestMemoryManagerMXBean\n+ * @run main\/othervm -XX:+UseZGC -XX:+ZGenerational -Xmx128M gc.z.TestMemoryManagerMXBean\n@@ -55,1 +55,1 @@\n-            if (memoryManagerName.equals(\"ZGC Cycles\")) {\n+            if (memoryManagerName.equals(\"ZGC Minor Cycles\") || memoryManagerName.equals(\"ZGC Major Cycles\")) {\n@@ -57,1 +57,1 @@\n-            } else if (memoryManagerName.equals(\"ZGC Pauses\")) {\n+            } else if (memoryManagerName.equals(\"ZGC Minor Pauses\") || memoryManagerName.equals(\"ZGC Major Pauses\")) {\n@@ -66,2 +66,2 @@\n-                if (memoryPoolName.equals(\"ZHeap\")) {\n-                    if (memoryManagerName.equals(\"ZGC Cycles\")) {\n+                if (memoryPoolName.equals(\"ZGC Young Generation\") || memoryPoolName.equals(\"ZGC Old Generation\")) {\n+                    if (memoryManagerName.equals(\"ZGC Minor Cycles\") || memoryManagerName.equals(\"ZGC Major Cycles\")) {\n@@ -69,1 +69,1 @@\n-                    } else if (memoryManagerName.equals(\"ZGC Pauses\")) {\n+                    } else if (memoryManagerName.equals(\"ZGC Minor Pauses\") || memoryManagerName.equals(\"ZGC Major Pauses\")) {\n@@ -76,1 +76,1 @@\n-        if (zgcCyclesMemoryManagers != 1) {\n+        if (zgcCyclesMemoryManagers != 2) {\n@@ -80,1 +80,1 @@\n-        if (zgcPausesMemoryManagers != 1) {\n+        if (zgcPausesMemoryManagers != 2) {\n@@ -84,1 +84,1 @@\n-        if (zgcCyclesMemoryPools != 1) {\n+        if (zgcCyclesMemoryPools != 4) {\n@@ -88,1 +88,1 @@\n-        if (zgcPausesMemoryPools != 1) {\n+        if (zgcPausesMemoryPools != 4) {\n","filename":"test\/hotspot\/jtreg\/gc\/z\/TestMemoryManagerMXBean.java","additions":12,"deletions":12,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,1 +28,1 @@\n- * @requires vm.gc.Z & !vm.graal.enabled\n+ * @requires vm.gc.Z & vm.opt.final.ZGenerational & !vm.graal.enabled\n@@ -30,2 +30,2 @@\n- * @run main\/othervm -XX:+UseZGC -Xlog:gc*,gc+heap=debug,gc+stats=off -Xms512M -Xmx512M -XX:ZUncommitDelay=1 gc.z.TestNoUncommit\n- * @run main\/othervm -XX:+UseZGC -Xlog:gc*,gc+heap=debug,gc+stats=off -Xms128M -Xmx512M -XX:ZUncommitDelay=1 -XX:-ZUncommit gc.z.TestNoUncommit\n+ * @run main\/othervm -XX:+UseZGC -XX:+ZGenerational -Xlog:gc*,gc+heap=debug,gc+stats=off -Xms512M -Xmx512M -XX:ZUncommitDelay=1 gc.z.TestNoUncommit\n+ * @run main\/othervm -XX:+UseZGC -XX:+ZGenerational -Xlog:gc*,gc+heap=debug,gc+stats=off -Xms128M -Xmx512M -XX:ZUncommitDelay=1 -XX:-ZUncommit gc.z.TestNoUncommit\n","filename":"test\/hotspot\/jtreg\/gc\/z\/TestNoUncommit.java","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,1 +28,1 @@\n- * @requires vm.gc.Z\n+ * @requires vm.gc.Z & vm.opt.final.ZGenerational\n@@ -73,0 +73,1 @@\n+                                    \"-XX:+ZGenerational\",\n","filename":"test\/hotspot\/jtreg\/gc\/z\/TestPageCacheFlush.java","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,1 +28,1 @@\n- * @requires vm.gc.Z\n+ * @requires vm.gc.Z & vm.opt.final.ZGenerational\n@@ -30,1 +30,1 @@\n- * @run main\/othervm -XX:+UseZGC -Xlog:gc*,gc+stats=off -Xmx256M -XX:+UnlockDiagnosticVMOptions -XX:+ZStressRelocateInPlace gc.z.TestRelocateInPlace\n+ * @run main\/othervm -XX:+UseZGC -XX:+ZGenerational -Xlog:gc*,gc+stats=off -Xmx256M -XX:+UnlockDiagnosticVMOptions -XX:+ZStressRelocateInPlace gc.z.TestRelocateInPlace\n","filename":"test\/hotspot\/jtreg\/gc\/z\/TestRelocateInPlace.java","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2019, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2019, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,1 +28,1 @@\n- * @requires vm.gc.Z\n+ * @requires vm.gc.Z & vm.opt.final.ZGenerational\n@@ -58,0 +58,1 @@\n+                                        \"-XX:+ZGenerational\",\n","filename":"test\/hotspot\/jtreg\/gc\/z\/TestSmallHeap.java","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2019, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2019, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,1 +28,1 @@\n- * @requires vm.gc.Z\n+ * @requires vm.gc.Z & vm.opt.final.ZGenerational\n@@ -31,1 +31,1 @@\n- * @run main\/othervm -XX:+UseZGC -Xlog:gc*,gc+heap=debug,gc+stats=off -Xms128M -Xmx512M -XX:ZUncommitDelay=10 gc.z.TestUncommit\n+ * @run main\/othervm -XX:+UseZGC -XX:+ZGenerational -Xlog:gc*,gc+heap=debug,gc+stats=off -Xms128M -Xmx512M -XX:ZUncommitDelay=10 gc.z.TestUncommit\n","filename":"test\/hotspot\/jtreg\/gc\/z\/TestUncommit.java","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -84,3 +84,9 @@\n-    \/\/ Suffix for ZGC.\n-    private static final String zStartSuffix = \"Garbage Collection (.*)$\";\n-    private static final String zEndSuffix = \"Garbage Collection (.*) .*->.*$\";\n+    \/\/ For ZGC only major collections clean the string table. ZGC prints the\n+    \/\/ start message without using the start tag, hence the special prefix.\n+    private static final String zStartPrefix = gcPrefix + gcMiddle;\n+    private static final String zStartSuffix = \"Major Collection \\\\(.*\\\\)$\";\n+    private static final String zEndSuffix = \"Major Collection \\\\(.*\\\\) .*->.*$\";\n+\n+    \/\/ Suffix for ZGC (non generational).\n+    private static final String xStartSuffix = \"Garbage Collection (.*)$\";\n+    private static final String xEndSuffix = \"Garbage Collection (.*) .*->.*$\";\n@@ -97,1 +103,1 @@\n-            return gcStartPrefix + zStartSuffix;\n+            return \"(\" + zStartPrefix + zStartSuffix + \")|(\" + gcStartPrefix + xStartSuffix + \")\";\n@@ -111,1 +117,1 @@\n-            return gcEndPrefix + zEndSuffix;\n+            return gcEndPrefix + \"(\" + zEndSuffix +  \")|(\" + xEndSuffix + \")\";\n","filename":"test\/hotspot\/jtreg\/runtime\/stringtable\/StringTableCleaningTest.java","additions":12,"deletions":6,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -0,0 +1,30 @@\n+#\n+# Copyright (c) 2020, 2023, Oracle and\/or its affiliates. All rights reserved.\n+# DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+#\n+# This code is free software; you can redistribute it and\/or modify it\n+# under the terms of the GNU General Public License version 2 only, as\n+# published by the Free Software Foundation.\n+#\n+# This code is distributed in the hope that it will be useful, but WITHOUT\n+# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+# FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+# version 2 for more details (a copy is included in the LICENSE file that\n+# accompanied this code).\n+#\n+# You should have received a copy of the GNU General Public License version\n+# 2 along with this work; if not, write to the Free Software Foundation,\n+# Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+#\n+# Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+# or visit www.oracle.com if you need additional information or have any\n+# questions.\n+#\n+\n+#############################################################################\n+#\n+# List of quarantined tests for testing with Generational ZGC.\n+#\n+#############################################################################\n+\n+java\/util\/concurrent\/locks\/Lock\/OOMEInAQS.java 8298066 linux,windows-x64\n","filename":"test\/jdk\/ProblemList-generational-zgc.txt","additions":30,"deletions":0,"binary":false,"changes":30,"status":"added"},{"patch":"@@ -30,1 +30,1 @@\n-java\/util\/concurrent\/locks\/Lock\/OOMEInAQS.java 8298066 linux-aarch64,windows-x64\n+java\/util\/concurrent\/locks\/Lock\/OOMEInAQS.java 8298066 linux,windows-x64\n","filename":"test\/jdk\/ProblemList-zgc.txt","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -30,1 +30,1 @@\n- * @requires (vm.compMode == \"Xmixed\")\n+ * @requires (vm.compMode == \"Xmixed\") & !(vm.gc.Z & vm.opt.final.ZGenerational)\n","filename":"test\/jdk\/com\/sun\/jdi\/ThreadMemoryLeakTest.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2021, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2021, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -52,6 +52,2 @@\n- * @test id=Z\n- * @requires vm.gc.Z\n- * @bug 8277072\n- * @library \/test\/lib\/\n- * @summary ObjectStreamClass caches keep ClassLoaders alive (Z GC)\n- * @run testng\/othervm -Xmx64m -XX:+UseZGC ObjectStreamClassCaching\n+ * Disabled for ZGC Generational.\n+ * TODO: Find correct appropriate solution to the flakiness of this test.\n","filename":"test\/jdk\/java\/io\/ObjectStreamClass\/ObjectStreamClassCaching.java","additions":3,"deletions":7,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2013, 2014, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2013, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -30,0 +30,1 @@\n+ * @requires !vm.opt.final.ZGenerational\n@@ -33,0 +34,7 @@\n+\/**\n+ * @test\n+ * @comment Turn up heap size to lower amount of GCs\n+ * @requires vm.gc.Z & vm.opt.final.ZGenerational\n+ * @run main\/othervm -Xmx32M -Dtest.duration=2 CloseRace\n+ *\/\n+\n","filename":"test\/jdk\/java\/lang\/ProcessBuilder\/CloseRace.java","additions":9,"deletions":1,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -36,1 +36,1 @@\n- *     -XX:+ZVerifyViews -XX:ZCollectionInterval=0.01 -Xmx1g Skynet\n+ *     -XX:+ZVerifyOops -XX:ZCollectionInterval=0.01 -Xmx1g Skynet\n","filename":"test\/jdk\/java\/lang\/Thread\/virtual\/stress\/Skynet.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -41,1 +41,25 @@\n- * @requires vm.gc == \"Z\" | vm.gc == \"Shenandoah\"\n+ * @requires vm.gc == \"Z\" & !vm.opt.final.ZGenerational\n+ * @author  Mandy Chung\n+ *\n+ * @modules jdk.management\n+ * @run main MemoryTest 2 1\n+ *\/\n+\n+\/*\n+ * @test\n+ * @bug     4530538\n+ * @summary Basic unit test of MemoryMXBean.getMemoryPools() and\n+ *          MemoryMXBean.getMemoryManager().\n+ * @requires vm.gc == \"Z\" & vm.opt.final.ZGenerational\n+ * @author  Mandy Chung\n+ *\n+ * @modules jdk.management\n+ * @run main MemoryTest 4 2\n+ *\/\n+\n+\/*\n+ * @test\n+ * @bug     4530538\n+ * @summary Basic unit test of MemoryMXBean.getMemoryPools() and\n+ *          MemoryMXBean.getMemoryManager().\n+ * @requires vm.gc == \"Shenandoah\"\n","filename":"test\/jdk\/java\/lang\/management\/MemoryMXBean\/MemoryTest.java","additions":26,"deletions":2,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -0,0 +1,78 @@\n+\/*\n+ * Copyright (c) 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package jdk.jfr.event.gc.collection;\n+\n+import static jdk.test.lib.Asserts.assertGreaterThan;\n+import static jdk.test.lib.Asserts.assertTrue;\n+\n+import java.time.Duration;\n+import java.time.Instant;\n+\n+import jdk.jfr.Recording;\n+import jdk.jfr.consumer.RecordedEvent;\n+import jdk.test.lib.jfr.Events;\n+\n+\/**\n+ * @test\n+ * @requires vm.hasJFR & vm.gc.Z & vm.opt.final.ZGenerational\n+ * @key jfr\n+ * @library \/test\/lib \/test\/jdk\n+ * @run main\/othervm -Xmx50m -XX:+UseZGC -XX:+UnlockExperimentalVMOptions -XX:-UseFastUnorderedTimeStamps -Xlog:gc* jdk.jfr.event.gc.collection.TestGarbageCollectionEventWithZMajor\n+ *\/\n+public class TestGarbageCollectionEventWithZMajor {\n+\n+    private static final String EVENT_NAME = \"jdk.GarbageCollection\";\n+    private static final String GC_NAME = \"ZGC Major\";\n+\n+    public static void main(String[] args) throws Exception {\n+        Recording recording = new Recording();\n+        recording.enable(EVENT_NAME).withThreshold(Duration.ofMillis(0));\n+        recording.start();\n+        System.gc();\n+        recording.stop();\n+\n+        boolean isAnyFound = false;\n+        for (RecordedEvent event : Events.fromRecording(recording)) {\n+            if (!EVENT_NAME.equals(event.getEventType().getName())) {\n+                continue;\n+            }\n+            if (!GC_NAME.equals(Events.assertField(event, \"name\").getValue())) {\n+                continue;\n+            }\n+            System.out.println(\"Event: \" + event);\n+            isAnyFound = true;\n+            Events.assertField(event, \"gcId\").atLeast(0);\n+\n+            Instant startTime = event.getStartTime();\n+            Instant endTime = event.getEndTime();\n+            Duration duration = event.getDuration();\n+            assertGreaterThan(startTime, Instant.EPOCH, \"startTime should be at least 0\");\n+            assertGreaterThan(endTime, Instant.EPOCH, \"endTime should be at least 0\");\n+            assertGreaterThan(duration, Duration.ZERO, \"Duration should be above 0\");\n+            assertGreaterThan(endTime, startTime, \"End time should be after start time\");\n+        }\n+        assertTrue(isAnyFound, \"No matching event found\");\n+    }\n+\n+}\n","filename":"test\/jdk\/jdk\/jfr\/event\/gc\/collection\/TestGarbageCollectionEventWithZMajor.java","additions":78,"deletions":0,"binary":false,"changes":78,"status":"added"},{"patch":"@@ -0,0 +1,84 @@\n+\/*\n+ * Copyright (c) 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package jdk.jfr.event.gc.collection;\n+\n+import static jdk.test.lib.Asserts.assertGreaterThan;\n+import static jdk.test.lib.Asserts.assertTrue;\n+\n+import java.time.Duration;\n+import java.time.Instant;\n+\n+import java.util.LinkedList;\n+\n+import jdk.jfr.Recording;\n+import jdk.jfr.consumer.RecordedEvent;\n+import jdk.test.lib.jfr.Events;\n+\n+import jdk.test.whitebox.WhiteBox;\n+\n+\/**\n+ * @test\n+ * @key jfr\n+ * @requires vm.hasJFR & vm.gc.Z & vm.opt.final.ZGenerational\n+ * @key jfr\n+ * @library \/test\/lib \/test\/jdk\n+ * @build jdk.test.whitebox.WhiteBox\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox\n+ * @run main\/othervm -Xbootclasspath\/a:. -XX:+UseZGC -XX:+UnlockDiagnosticVMOptions -XX:+WhiteBoxAPI -XX:+UnlockExperimentalVMOptions -XX:-UseFastUnorderedTimeStamps -Xlog:gc* jdk.jfr.event.gc.collection.TestGarbageCollectionEventWithZMinor\n+ *\/\n+public class TestGarbageCollectionEventWithZMinor {\n+\n+    private static final String EVENT_NAME = \"jdk.GarbageCollection\";\n+    private static final String GC_NAME = \"ZGC Minor\";\n+\n+    public static void main(String[] args) throws Exception {\n+        Recording recording = new Recording();\n+        recording.enable(EVENT_NAME).withThreshold(Duration.ofMillis(0));\n+        recording.start();\n+        WhiteBox.getWhiteBox().youngGC();\n+        recording.stop();\n+\n+        boolean isAnyFound = false;\n+        for (RecordedEvent event : Events.fromRecording(recording)) {\n+            if (!EVENT_NAME.equals(event.getEventType().getName())) {\n+                continue;\n+            }\n+            if (!GC_NAME.equals(Events.assertField(event, \"name\").getValue())) {\n+                continue;\n+            }\n+            System.out.println(\"Event: \" + event);\n+            isAnyFound = true;\n+            Events.assertField(event, \"gcId\").atLeast(0);\n+\n+            Instant startTime = event.getStartTime();\n+            Instant endTime = event.getEndTime();\n+            Duration duration = event.getDuration();\n+            assertGreaterThan(startTime, Instant.EPOCH, \"startTime should be at least 0\");\n+            assertGreaterThan(endTime, Instant.EPOCH, \"endTime should be at least 0\");\n+            assertGreaterThan(duration, Duration.ZERO, \"Duration should be above 0\");\n+            assertGreaterThan(endTime, startTime, \"End time should be after start time\");\n+        }\n+        assertTrue(isAnyFound, \"No matching event found\");\n+    }\n+}\n","filename":"test\/jdk\/jdk\/jfr\/event\/gc\/collection\/TestGarbageCollectionEventWithZMinor.java","additions":84,"deletions":0,"binary":false,"changes":84,"status":"added"},{"patch":"@@ -0,0 +1,74 @@\n+\/*\n+ * Copyright (c) 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package jdk.jfr.event.gc.collection;\n+\n+import static jdk.test.lib.Asserts.assertGreaterThan;\n+import static jdk.test.lib.Asserts.assertTrue;\n+\n+import java.time.Duration;\n+import java.time.Instant;\n+\n+import jdk.jfr.Recording;\n+import jdk.jfr.consumer.RecordedEvent;\n+import jdk.test.lib.jfr.Events;\n+\n+\/**\n+ * @test\n+ * @requires vm.hasJFR & vm.gc.Z & vm.opt.final.ZGenerational\n+ * @key jfr\n+ * @library \/test\/lib \/test\/jdk\n+ * @run main\/othervm -Xmx50m -XX:+UseZGC -XX:+UnlockExperimentalVMOptions -XX:-UseFastUnorderedTimeStamps -Xlog:gc* jdk.jfr.event.gc.collection.TestZOldGarbageCollectionEvent\n+ *\/\n+public class TestZOldGarbageCollectionEvent {\n+\n+    private static final String EVENT_NAME = \"jdk.ZOldGarbageCollection\";\n+\n+    public static void main(String[] args) throws Exception {\n+        Recording recording = new Recording();\n+        recording.enable(EVENT_NAME).withThreshold(Duration.ofMillis(0));\n+        recording.start();\n+        System.gc();\n+        recording.stop();\n+\n+        boolean isAnyFound = false;\n+        for (RecordedEvent event : Events.fromRecording(recording)) {\n+            if (!EVENT_NAME.equals(event.getEventType().getName())) {\n+                continue;\n+            }\n+            System.out.println(\"Event: \" + event);\n+            isAnyFound = true;\n+            Events.assertField(event, \"gcId\").atLeast(0);\n+\n+            Instant startTime = event.getStartTime();\n+            Instant endTime = event.getEndTime();\n+            Duration duration = event.getDuration();\n+            assertGreaterThan(startTime, Instant.EPOCH, \"startTime should be at least 0\");\n+            assertGreaterThan(endTime, Instant.EPOCH, \"endTime should be at least 0\");\n+            assertGreaterThan(duration, Duration.ZERO, \"Duration should be above 0\");\n+            assertGreaterThan(endTime, startTime, \"End time should be after start time\");\n+        }\n+        assertTrue(isAnyFound, \"No matching event found\");\n+    }\n+\n+}\n","filename":"test\/jdk\/jdk\/jfr\/event\/gc\/collection\/TestZOldGarbageCollectionEvent.java","additions":74,"deletions":0,"binary":false,"changes":74,"status":"added"},{"patch":"@@ -0,0 +1,75 @@\n+\/*\n+ * Copyright (c) 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package jdk.jfr.event.gc.collection;\n+\n+import static jdk.test.lib.Asserts.assertGreaterThan;\n+import static jdk.test.lib.Asserts.assertTrue;\n+\n+import java.time.Duration;\n+import java.time.Instant;\n+\n+import jdk.jfr.Recording;\n+import jdk.jfr.consumer.RecordedEvent;\n+import jdk.test.lib.jfr.Events;\n+\n+\/**\n+ * @test\n+ * @requires vm.hasJFR & vm.gc.Z & vm.opt.final.ZGenerational\n+ * @key jfr\n+ * @library \/test\/lib \/test\/jdk\n+ * @run main\/othervm -Xmx50m -XX:+UseZGC -XX:+UnlockExperimentalVMOptions -XX:-UseFastUnorderedTimeStamps -Xlog:gc* jdk.jfr.event.gc.collection.TestZYoungGarbageCollectionEvent\n+ *\/\n+public class TestZYoungGarbageCollectionEvent {\n+\n+    private static final String EVENT_NAME = \"jdk.ZYoungGarbageCollection\";\n+\n+    public static void main(String[] args) throws Exception {\n+        Recording recording = new Recording();\n+        recording.enable(EVENT_NAME).withThreshold(Duration.ofMillis(0));\n+        recording.start();\n+        System.gc();\n+        recording.stop();\n+\n+        boolean isAnyFound = false;\n+        for (RecordedEvent event : Events.fromRecording(recording)) {\n+            if (!EVENT_NAME.equals(event.getEventType().getName())) {\n+                continue;\n+            }\n+            System.out.println(\"Event: \" + event);\n+            isAnyFound = true;\n+            Events.assertField(event, \"gcId\").atLeast(0);\n+            Events.assertField(event, \"tenuringThreshold\").atLeast(0);\n+\n+            Instant startTime = event.getStartTime();\n+            Instant endTime = event.getEndTime();\n+            Duration duration = event.getDuration();\n+            assertGreaterThan(startTime, Instant.EPOCH, \"startTime should be at least 0\");\n+            assertGreaterThan(endTime, Instant.EPOCH, \"endTime should be at least 0\");\n+            assertGreaterThan(duration, Duration.ZERO, \"Duration should be above 0\");\n+            assertGreaterThan(endTime, startTime, \"End time should be after start time\");\n+        }\n+        assertTrue(isAnyFound, \"No matching event found\");\n+    }\n+\n+}\n","filename":"test\/jdk\/jdk\/jfr\/event\/gc\/collection\/TestZYoungGarbageCollectionEvent.java","additions":75,"deletions":0,"binary":false,"changes":75,"status":"added"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -37,2 +37,10 @@\n- * @requires vm.hasJFR & vm.gc.Z\n- * @run main\/othervm -XX:+UseZGC -Xmx32M jdk.jfr.event.gc.detailed.TestGCPhaseConcurrent\n+ * @requires vm.hasJFR & vm.gc.Z & vm.opt.final.ZGenerational\n+ * @run main\/othervm -XX:+UseZGC -Xmx32M jdk.jfr.event.gc.detailed.TestGCPhaseConcurrent Z\n+ *\/\n+\n+\/**\n+ * @test TestGCPhaseConcurrent\n+ * @key jfr\n+ * @library \/test\/lib \/test\/jdk \/test\/hotspot\/jtreg\n+ * @requires vm.hasJFR & vm.gc.Z & !vm.opt.final.ZGenerational\n+ * @run main\/othervm -XX:+UseZGC -Xmx32M jdk.jfr.event.gc.detailed.TestGCPhaseConcurrent X\n@@ -46,1 +54,1 @@\n- * @run main\/othervm -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -Xmx32M jdk.jfr.event.gc.detailed.TestGCPhaseConcurrent\n+ * @run main\/othervm -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -Xmx32M jdk.jfr.event.gc.detailed.TestGCPhaseConcurrent Shenandoah\n@@ -50,0 +58,2 @@\n+      boolean usesZGC = args[0].equals(\"Z\");\n+\n@@ -54,0 +64,3 @@\n+            if (usesZGC) {\n+              recording.enable(EventNames.GCPhaseConcurrentLevel2);\n+            }\n@@ -66,0 +79,3 @@\n+            if (usesZGC) {\n+              Events.hasEvent(events, EventNames.GCPhaseConcurrentLevel2);\n+            }\n","filename":"test\/jdk\/jdk\/jfr\/event\/gc\/detailed\/TestGCPhaseConcurrent.java","additions":20,"deletions":4,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -349,2 +349,1 @@\n-        vmOptFinalFlag(map, \"UseCompressedOops\");\n-        vmOptFinalFlag(map, \"UseVectorizedMismatchIntrinsic\");\n+        vmOptFinalFlag(map, \"CriticalJNINatives\");\n@@ -353,0 +352,2 @@\n+        vmOptFinalFlag(map, \"UseCompressedOops\");\n+        vmOptFinalFlag(map, \"UseVectorizedMismatchIntrinsic\");\n@@ -354,0 +355,1 @@\n+        vmOptFinalFlag(map, \"ZGenerational\");\n","filename":"test\/jtreg-ext\/requires\/VMProps.java","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -145,0 +145,3 @@\n+    public static final String GCPhaseConcurrentLevel2 = PREFIX + \"GCPhaseConcurrentLevel2\";\n+    public static final String ZYoungGarbageCollection = PREFIX + \"ZYoungGarbageCollection\";\n+    public static final String ZOldGarbageCollection = PREFIX + \"ZOldGarbageCollection\";\n","filename":"test\/lib\/jdk\/test\/lib\/jfr\/EventNames.java","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"}]}