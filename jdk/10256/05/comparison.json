{"files":[{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -85,0 +85,1 @@\n+    _dirtied_cards_in_thread_buffers_seq(new TruncatedSeq(TruncatedSeqLength)),\n@@ -175,0 +176,4 @@\n+void G1Analytics::report_dirtied_cards_in_thread_buffers(size_t cards) {\n+  _dirtied_cards_in_thread_buffers_seq->add(double(cards));\n+}\n+\n@@ -259,0 +264,4 @@\n+size_t G1Analytics::predict_dirtied_cards_in_thread_buffers() const {\n+  return predict_size(_dirtied_cards_in_thread_buffers_seq);\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1Analytics.cpp","additions":10,"deletions":1,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -52,0 +52,1 @@\n+  TruncatedSeq* _dirtied_cards_in_thread_buffers_seq;\n@@ -129,0 +130,1 @@\n+  void report_dirtied_cards_in_thread_buffers(size_t num_cards);\n@@ -145,0 +147,1 @@\n+  size_t predict_dirtied_cards_in_thread_buffers() const;\n","filename":"src\/hotspot\/share\/gc\/g1\/g1Analytics.hpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -180,1 +180,7 @@\n-  if (FLAG_IS_DEFAULT(G1ConcRefinementThreads)) {\n+  if (!G1UseConcRefinement) {\n+    if (!FLAG_IS_DEFAULT(G1ConcRefinementThreads)) {\n+      log_warning(gc, ergo)(\"Ignoring -XX:G1ConcRefinementThreads \"\n+                            \"because of -XX:-G1UseConcRefinement\");\n+    }\n+    FLAG_SET_DEFAULT(G1ConcRefinementThreads, 0);\n+  } else if (FLAG_IS_DEFAULT(G1ConcRefinementThreads)) {\n","filename":"src\/hotspot\/share\/gc\/g1\/g1Arguments.cpp","additions":8,"deletions":2,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -1536,1 +1536,1 @@\n-  _cr = G1ConcurrentRefine::create(&ecode);\n+  _cr = G1ConcurrentRefine::create(policy(), &ecode);\n@@ -1716,3 +1716,0 @@\n-  \/\/ Initialize and schedule sampling task on service thread.\n-  _rem_set->initialize_sampling_task(service_thread());\n-\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectedHeap.cpp","additions":1,"deletions":4,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"gc\/g1\/g1CollectionSet.hpp\"\n@@ -30,0 +31,4 @@\n+#include \"gc\/g1\/g1Policy.hpp\"\n+#include \"gc\/g1\/heapRegion.inline.hpp\"\n+#include \"gc\/g1\/heapRegionRemSet.inline.hpp\"\n+#include \"gc\/shared\/gc_globals.hpp\"\n@@ -33,1 +38,0 @@\n-#include \"runtime\/globals_extension.hpp\"\n@@ -35,1 +39,1 @@\n-#include \"runtime\/javaThread.hpp\"\n+#include \"runtime\/mutexLocker.hpp\"\n@@ -37,1 +41,0 @@\n-#include \"utilities\/formatBuffer.hpp\"\n@@ -39,1 +42,0 @@\n-#include \"utilities\/pair.hpp\"\n@@ -43,1 +45,1 @@\n-  G1ConcurrentRefineThread* result = NULL;\n+  G1ConcurrentRefineThread* result = nullptr;\n@@ -47,1 +49,1 @@\n-  if (result == NULL || result->osthread() == NULL) {\n+  if (result == nullptr || result->osthread() == nullptr) {\n@@ -50,1 +52,5 @@\n-                    result == NULL ? \"memory\" : \"OS threads\");\n+                    result == nullptr ? \"memory\" : \"OS threads\");\n+    if (result != nullptr) {\n+      delete result;\n+      result = nullptr;\n+    }\n@@ -57,1 +63,0 @@\n-  _primary_thread(nullptr),\n@@ -59,3 +64,2 @@\n-  _num_max_threads(0)\n-{\n-}\n+  _max_num_threads(0)\n+{}\n@@ -64,4 +68,13 @@\n-  for (uint i = 0; i < _num_max_threads; i++) {\n-    G1ConcurrentRefineThread* t = _threads[i];\n-    if (t != NULL) {\n-      delete t;\n+  if (_threads != nullptr) {\n+    for (uint i = 0; i < _max_num_threads; i++) {\n+      G1ConcurrentRefineThread* t = _threads[i];\n+      if (t == nullptr) {\n+#ifdef ASSERT\n+        for (uint j = i + 1; j < _max_num_threads; ++j) {\n+          assert(_threads[j] == nullptr, \"invariant\");\n+        }\n+#endif \/\/ ASSERT\n+        break;\n+      } else {\n+        delete t;\n+      }\n@@ -69,0 +82,1 @@\n+    FREE_C_HEAP_ARRAY(G1ConcurrentRefineThread*, _threads);\n@@ -70,1 +84,0 @@\n-  FREE_C_HEAP_ARRAY(G1ConcurrentRefineThread*, _threads);\n@@ -73,1 +86,1 @@\n-jint G1ConcurrentRefineThreadControl::initialize(G1ConcurrentRefine* cr, uint num_max_threads) {\n+jint G1ConcurrentRefineThreadControl::initialize(G1ConcurrentRefine* cr, uint max_num_threads) {\n@@ -76,1 +89,1 @@\n-  _num_max_threads = num_max_threads;\n+  _max_num_threads = max_num_threads;\n@@ -78,1 +91,2 @@\n-  _threads = NEW_C_HEAP_ARRAY(G1ConcurrentRefineThread*, num_max_threads, mtGC);\n+  if (max_num_threads > 0) {\n+    _threads = NEW_C_HEAP_ARRAY(G1ConcurrentRefineThread*, max_num_threads, mtGC);\n@@ -80,3 +94,2 @@\n-  if (num_max_threads > 0) {\n-    auto primary = G1PrimaryConcurrentRefineThread::create(cr);\n-    if (primary == nullptr) {\n+    _threads[0] = create_refinement_thread(0, true);\n+    if (_threads[0] == nullptr) {\n@@ -86,1 +99,0 @@\n-    _threads[0] = _primary_thread = primary;\n@@ -88,2 +100,2 @@\n-    for (uint i = 1; i < num_max_threads; ++i) {\n-      if (UseDynamicNumberOfGCThreads) {\n+    if (UseDynamicNumberOfGCThreads) {\n+      for (uint i = 1; i < max_num_threads; ++i) {\n@@ -91,1 +103,3 @@\n-      } else {\n+      }\n+    } else {\n+      for (uint i = 1; i < max_num_threads; ++i) {\n@@ -104,8 +118,6 @@\n-void G1ConcurrentRefineThreadControl::maybe_activate_next(uint cur_worker_id) {\n-  assert(cur_worker_id < _num_max_threads,\n-         \"Activating another thread from %u not allowed since there can be at most %u\",\n-         cur_worker_id, _num_max_threads);\n-  if (cur_worker_id == (_num_max_threads - 1)) {\n-    \/\/ Already the last thread, there is no more thread to activate.\n-    return;\n-  }\n+#ifdef ASSERT\n+void G1ConcurrentRefineThreadControl::assert_current_thread_is_primary_refinement_thread() const {\n+  assert(_threads != nullptr, \"No threads\");\n+  assert(Thread::current() == _threads[0], \"Not primary thread\");\n+}\n+#endif \/\/ ASSERT\n@@ -113,1 +125,2 @@\n-  uint worker_id = cur_worker_id + 1;\n+bool G1ConcurrentRefineThreadControl::activate(uint worker_id) {\n+  assert(worker_id < _max_num_threads, \"precondition\");\n@@ -115,7 +128,6 @@\n-  if (thread_to_activate == NULL) {\n-    \/\/ Still need to create the thread...\n-    _threads[worker_id] = create_refinement_thread(worker_id, false);\n-    thread_to_activate = _threads[worker_id];\n-  }\n-  if (thread_to_activate != NULL) {\n-    thread_to_activate->activate();\n+  if (thread_to_activate == nullptr) {\n+    thread_to_activate = create_refinement_thread(worker_id, false);\n+    if (thread_to_activate == nullptr) {\n+      return false;\n+    }\n+    _threads[worker_id] = thread_to_activate;\n@@ -123,0 +135,2 @@\n+  thread_to_activate->activate();\n+  return true;\n@@ -126,1 +140,1 @@\n-  for (uint i = 0; i < _num_max_threads; i++) {\n+  for (uint i = 0; i < _max_num_threads; i++) {\n@@ -134,1 +148,1 @@\n-  for (uint i = 0; i < _num_max_threads; i++) {\n+  for (uint i = 0; i < _max_num_threads; i++) {\n@@ -141,62 +155,4 @@\n-\/\/ Arbitrary but large limits, to simplify some of the zone calculations.\n-\/\/ The general idea is to allow expressions like\n-\/\/   MIN2(x OP y, max_XXX_zone)\n-\/\/ without needing to check for overflow in \"x OP y\", because the\n-\/\/ ranges for x and y have been restricted.\n-STATIC_ASSERT(sizeof(LP64_ONLY(jint) NOT_LP64(jshort)) <= (sizeof(size_t)\/2));\n-const size_t max_yellow_zone = LP64_ONLY(max_jint) NOT_LP64(max_jshort);\n-const size_t max_green_zone = max_yellow_zone \/ 2;\n-const size_t max_red_zone = INT_MAX; \/\/ For dcqs.set_max_cards.\n-STATIC_ASSERT(max_yellow_zone <= max_red_zone);\n-\n-\/\/ Range check assertions for green zone values.\n-#define assert_zone_constraints_g(green)                        \\\n-  do {                                                          \\\n-    size_t azc_g_green = (green);                               \\\n-    assert(azc_g_green <= max_green_zone,                       \\\n-           \"green exceeds max: \" SIZE_FORMAT, azc_g_green);     \\\n-  } while (0)\n-\n-\/\/ Range check assertions for green and yellow zone values.\n-#define assert_zone_constraints_gy(green, yellow)                       \\\n-  do {                                                                  \\\n-    size_t azc_gy_green = (green);                                      \\\n-    size_t azc_gy_yellow = (yellow);                                    \\\n-    assert_zone_constraints_g(azc_gy_green);                            \\\n-    assert(azc_gy_yellow <= max_yellow_zone,                            \\\n-           \"yellow exceeds max: \" SIZE_FORMAT, azc_gy_yellow);          \\\n-    assert(azc_gy_green <= azc_gy_yellow,                               \\\n-           \"green (\" SIZE_FORMAT \") exceeds yellow (\" SIZE_FORMAT \")\",  \\\n-           azc_gy_green, azc_gy_yellow);                                \\\n-  } while (0)\n-\n-\/\/ Range check assertions for green, yellow, and red zone values.\n-#define assert_zone_constraints_gyr(green, yellow, red)                 \\\n-  do {                                                                  \\\n-    size_t azc_gyr_green = (green);                                     \\\n-    size_t azc_gyr_yellow = (yellow);                                   \\\n-    size_t azc_gyr_red = (red);                                         \\\n-    assert_zone_constraints_gy(azc_gyr_green, azc_gyr_yellow);          \\\n-    assert(azc_gyr_red <= max_red_zone,                                 \\\n-           \"red exceeds max: \" SIZE_FORMAT, azc_gyr_red);               \\\n-    assert(azc_gyr_yellow <= azc_gyr_red,                               \\\n-           \"yellow (\" SIZE_FORMAT \") exceeds red (\" SIZE_FORMAT \")\",    \\\n-           azc_gyr_yellow, azc_gyr_red);                                \\\n-  } while (0)\n-\n-\/\/ Logging tag sequence for refinement control updates.\n-#define CTRL_TAGS gc, ergo, refine\n-\n-\/\/ For logging zone values, ensuring consistency of level and tags.\n-#define LOG_ZONES(...) log_debug( CTRL_TAGS )(__VA_ARGS__)\n-\n-\/\/ Convert configuration values in units of buffers to number of cards.\n-static size_t configuration_buffers_to_cards(size_t value, const char* value_name) {\n-  if (value == 0) return 0;\n-  size_t res = value * G1UpdateBufferSize;\n-\n-  if (res \/ value != G1UpdateBufferSize) { \/\/ Check overflow\n-    vm_exit_during_initialization(err_msg(\"configuration_buffers_to_cards: \"\n-      \"(%s = \" SIZE_FORMAT \") * (G1UpdateBufferSize = \" SIZE_FORMAT \") overflow!\", value_name, value, G1UpdateBufferSize));\n-  }\n-  return res;\n+uint64_t G1ConcurrentRefine::adjust_threads_period_ms() const {\n+  \/\/ Instead of a fixed value, this could be a command line option.  But then\n+  \/\/ we might also want to allow configuration of adjust_threads_wait_ms().\n+  return 50;\n@@ -205,25 +161,3 @@\n-\/\/ Package for pair of refinement thread activation and deactivation\n-\/\/ thresholds.  The activation and deactivation levels are resp. the first\n-\/\/ and second values of the pair.\n-typedef Pair<size_t, size_t> Thresholds;\n-inline size_t activation_level(const Thresholds& t) { return t.first; }\n-inline size_t deactivation_level(const Thresholds& t) { return t.second; }\n-\n-static Thresholds calc_thresholds(size_t green_zone,\n-                                  size_t yellow_zone,\n-                                  uint worker_id) {\n-  double yellow_size = yellow_zone - green_zone;\n-  double step = yellow_size \/ G1ConcurrentRefine::max_num_threads();\n-  if (worker_id == 0) {\n-    \/\/ Potentially activate worker 0 more aggressively, to keep\n-    \/\/ available buffers near green_zone value.  When yellow_size is\n-    \/\/ large we don't want to allow a full step to accumulate before\n-    \/\/ doing any processing, as that might lead to significantly more\n-    \/\/ than green_zone buffers to be processed during pause.  So limit\n-    \/\/ to an extra half buffer per pause-time processing thread.\n-    step = MIN2(step, configuration_buffers_to_cards(ParallelGCThreads, \"ParallelGCThreads\") \/ 2.0);\n-  }\n-  size_t activate_offset = static_cast<size_t>(ceil(step * (worker_id + 1)));\n-  size_t deactivate_offset = static_cast<size_t>(floor(step * worker_id));\n-  return Thresholds(green_zone + activate_offset,\n-                    green_zone + deactivate_offset);\n+static size_t minimum_pending_cards_target() {\n+  \/\/ One buffer per thread.\n+  return ParallelGCThreads * G1UpdateBufferSize;\n@@ -232,4 +166,7 @@\n-G1ConcurrentRefine::G1ConcurrentRefine(size_t green_zone,\n-                                       size_t yellow_zone,\n-                                       size_t red_zone,\n-                                       size_t min_yellow_zone_size) :\n+G1ConcurrentRefine::G1ConcurrentRefine(G1Policy* policy) :\n+  _policy(policy),\n+  _threads_wanted(0),\n+  _pending_cards_target(PendingCardsTargetUninitialized),\n+  _last_adjust(),\n+  _needs_adjust(false),\n+  _threads_needed(policy, adjust_threads_period_ms()),\n@@ -237,7 +174,2 @@\n-  _green_zone(green_zone),\n-  _yellow_zone(yellow_zone),\n-  _red_zone(red_zone),\n-  _min_yellow_zone_size(min_yellow_zone_size)\n-{\n-  assert_zone_constraints_gyr(green_zone, yellow_zone, red_zone);\n-}\n+  _dcqs(G1BarrierSet::dirty_card_queue_set())\n+{}\n@@ -246,53 +178,1 @@\n-  jint result = _thread_control.initialize(this, max_num_threads());\n-  if (result != JNI_OK) return result;\n-\n-  G1DirtyCardQueueSet& dcqs = G1BarrierSet::dirty_card_queue_set();\n-  dcqs.set_max_cards(red_zone());\n-  if (max_num_threads() > 0) {\n-    G1PrimaryConcurrentRefineThread* primary_thread = _thread_control.primary_thread();\n-    primary_thread->update_notify_threshold(primary_activation_threshold());\n-    dcqs.set_refinement_notification_thread(primary_thread);\n-  }\n-\n-  return JNI_OK;\n-}\n-\n-static size_t calc_min_yellow_zone_size() {\n-  size_t step = configuration_buffers_to_cards(G1ConcRefinementThresholdStep, \"G1ConcRefinementThresholdStep\");\n-  uint n_workers = G1ConcurrentRefine::max_num_threads();\n-  if ((max_yellow_zone \/ step) < n_workers) {\n-    return max_yellow_zone;\n-  } else {\n-    return step * n_workers;\n-  }\n-}\n-\n-\/\/ An initial guess at the rate for pause-time card refinement for one\n-\/\/ thread, used when computing the default initial green zone value.\n-const double InitialPauseTimeCardRefinementRate = 200.0;\n-\n-static size_t calc_init_green_zone() {\n-  size_t green;\n-  if (FLAG_IS_DEFAULT(G1ConcRefinementGreenZone)) {\n-    const double rate = InitialPauseTimeCardRefinementRate * ParallelGCThreads;\n-    \/\/ The time budget for pause-time card refinement.\n-    const double ms = MaxGCPauseMillis * (G1RSetUpdatingPauseTimePercent \/ 100.0);\n-    green = rate * ms;\n-  } else {\n-    green = configuration_buffers_to_cards(G1ConcRefinementGreenZone,\n-                                           \"G1ConcRefinementGreenZone\");\n-  }\n-  return MIN2(green, max_green_zone);\n-}\n-\n-static size_t calc_init_yellow_zone(size_t green, size_t min_size) {\n-  size_t config = configuration_buffers_to_cards(G1ConcRefinementYellowZone, \"G1ConcRefinementYellowZone\");\n-  size_t size = 0;\n-  if (FLAG_IS_DEFAULT(G1ConcRefinementYellowZone)) {\n-    size = green * 2;\n-  } else if (green < config) {\n-    size = config - green;\n-  }\n-  size = MAX2(size, min_size);\n-  size = MIN2(size, max_yellow_zone);\n-  return MIN2(green + size, max_yellow_zone);\n+  return _thread_control.initialize(this, max_num_threads());\n@@ -301,28 +181,2 @@\n-static size_t calc_init_red_zone(size_t green, size_t yellow) {\n-  size_t size = yellow - green;\n-  if (!FLAG_IS_DEFAULT(G1ConcRefinementRedZone)) {\n-    size_t config = configuration_buffers_to_cards(G1ConcRefinementRedZone, \"G1ConcRefinementRedZone\");\n-    if (yellow < config) {\n-      size = MAX2(size, config - yellow);\n-    }\n-  }\n-  return MIN2(yellow + size, max_red_zone);\n-}\n-\n-G1ConcurrentRefine* G1ConcurrentRefine::create(jint* ecode) {\n-  size_t min_yellow_zone_size = calc_min_yellow_zone_size();\n-  size_t green_zone = calc_init_green_zone();\n-  size_t yellow_zone = calc_init_yellow_zone(green_zone, min_yellow_zone_size);\n-  size_t red_zone = calc_init_red_zone(green_zone, yellow_zone);\n-\n-  LOG_ZONES(\"Initial Refinement Zones: \"\n-            \"green: \" SIZE_FORMAT \", \"\n-            \"yellow: \" SIZE_FORMAT \", \"\n-            \"red: \" SIZE_FORMAT \", \"\n-            \"min yellow size: \" SIZE_FORMAT,\n-            green_zone, yellow_zone, red_zone, min_yellow_zone_size);\n-\n-  G1ConcurrentRefine* cr = new G1ConcurrentRefine(green_zone,\n-                                                  yellow_zone,\n-                                                  red_zone,\n-                                                  min_yellow_zone_size);\n+G1ConcurrentRefine* G1ConcurrentRefine::create(G1Policy* policy, jint* ecode) {\n+  G1ConcurrentRefine* cr = new G1ConcurrentRefine(policy);\n@@ -330,0 +184,4 @@\n+  if (*ecode != 0) {\n+    delete cr;\n+    cr = nullptr;\n+  }\n@@ -348,10 +206,48 @@\n-static size_t calc_new_green_zone(size_t green,\n-                                  double logged_cards_scan_time,\n-                                  size_t processed_logged_cards,\n-                                  double goal_ms) {\n-  \/\/ Adjust green zone based on whether we're meeting the time goal.\n-  \/\/ Limit to max_green_zone.\n-  const double inc_k = 1.1, dec_k = 0.9;\n-  if (logged_cards_scan_time > goal_ms) {\n-    if (green > 0) {\n-      green = static_cast<size_t>(green * dec_k);\n+void G1ConcurrentRefine::update_pending_cards_target(double logged_cards_time_ms,\n+                                                     size_t processed_logged_cards,\n+                                                     size_t predicted_thread_buffer_cards,\n+                                                     double goal_ms) {\n+  size_t minimum = minimum_pending_cards_target();\n+  if ((processed_logged_cards < minimum) || (logged_cards_time_ms == 0.0)) {\n+    log_debug(gc, ergo, refine)(\"Unchanged pending cards target: %zu\",\n+                                _pending_cards_target);\n+    return;\n+  }\n+\n+  \/\/ Base the pending cards budget on the measured rate.\n+  double rate = processed_logged_cards \/ logged_cards_time_ms;\n+  size_t budget = static_cast<size_t>(goal_ms * rate);\n+  \/\/ Deduct predicted cards in thread buffers to get target.\n+  size_t new_target = budget - MIN2(budget, predicted_thread_buffer_cards);\n+  \/\/ Add some hysteresis with previous values.\n+  if (is_pending_cards_target_initialized()) {\n+    new_target = (new_target + _pending_cards_target) \/ 2;\n+  }\n+  \/\/ Apply minimum target.\n+  new_target = MAX2(new_target, minimum_pending_cards_target());\n+  _pending_cards_target = new_target;\n+  log_debug(gc, ergo, refine)(\"New pending cards target: %zu\", new_target);\n+}\n+\n+void G1ConcurrentRefine::adjust_after_gc(double logged_cards_time_ms,\n+                                         size_t processed_logged_cards,\n+                                         size_t predicted_thread_buffer_cards,\n+                                         double goal_ms) {\n+  if (!G1UseConcRefinement) return;\n+\n+  update_pending_cards_target(logged_cards_time_ms,\n+                              processed_logged_cards,\n+                              predicted_thread_buffer_cards,\n+                              goal_ms);\n+  if (_thread_control.max_num_threads() == 0) {\n+    \/\/ If no refinement threads then the mutator threshold is the target.\n+    _dcqs.set_mutator_refinement_threshold(_pending_cards_target);\n+  } else {\n+    \/\/ Provisionally make the mutator threshold unlimited, to be updated by\n+    \/\/ the next periodic adjustment.  Because card state may have changed\n+    \/\/ drastically, record that adjustment is needed and kick the primary\n+    \/\/ thread, in case it is waiting.\n+    _dcqs.set_mutator_refinement_threshold(SIZE_MAX);\n+    _needs_adjust = true;\n+    if (is_pending_cards_target_initialized()) {\n+      _thread_control.activate(0);\n@@ -359,4 +255,0 @@\n-  } else if (logged_cards_scan_time < goal_ms &&\n-             processed_logged_cards > green) {\n-    green = static_cast<size_t>(MAX2(green * inc_k, green + 1.0));\n-    green = MIN2(green, max_green_zone);\n@@ -364,1 +256,0 @@\n-  return green;\n@@ -367,4 +258,6 @@\n-static size_t calc_new_yellow_zone(size_t green, size_t min_yellow_size) {\n-  size_t size = green * 2;\n-  size = MAX2(size, min_yellow_size);\n-  return MIN2(green + size, max_yellow_zone);\n+\/\/ Wake up the primary thread less frequently when the time available until\n+\/\/ the next GC is longer.  But don't increase the wait time too rapidly.\n+\/\/ This reduces the number of primary thread wakeups that just immediately\n+\/\/ go back to waiting, while still being responsive to behavior changes.\n+static uint64_t compute_adjust_wait_time_ms(double available_ms) {\n+  return static_cast<uint64_t>(sqrt(available_ms) * 4.0);\n@@ -373,2 +266,12 @@\n-static size_t calc_new_red_zone(size_t green, size_t yellow) {\n-  return MIN2(yellow + (yellow - green), max_red_zone);\n+uint64_t G1ConcurrentRefine::adjust_threads_wait_ms() const {\n+  assert_current_thread_is_primary_refinement_thread();\n+  if (is_pending_cards_target_initialized()) {\n+    double available_ms = _threads_needed.predicted_time_until_next_gc_ms();\n+    uint64_t wait_time_ms = compute_adjust_wait_time_ms(available_ms);\n+    return MAX2(wait_time_ms, adjust_threads_period_ms());\n+  } else {\n+    \/\/ If target not yet initialized then wait forever (until explicitly\n+    \/\/ activated).  This happens during startup, when we don't bother with\n+    \/\/ refinement.\n+    return 0;\n+  }\n@@ -377,24 +280,38 @@\n-void G1ConcurrentRefine::update_zones(double logged_cards_scan_time,\n-                                      size_t processed_logged_cards,\n-                                      double goal_ms) {\n-  log_trace( CTRL_TAGS )(\"Updating Refinement Zones: \"\n-                         \"logged cards scan time: %.3fms, \"\n-                         \"processed cards: \" SIZE_FORMAT \", \"\n-                         \"goal time: %.3fms\",\n-                         logged_cards_scan_time,\n-                         processed_logged_cards,\n-                         goal_ms);\n-\n-  _green_zone = calc_new_green_zone(_green_zone,\n-                                    logged_cards_scan_time,\n-                                    processed_logged_cards,\n-                                    goal_ms);\n-  _yellow_zone = calc_new_yellow_zone(_green_zone, _min_yellow_zone_size);\n-  _red_zone = calc_new_red_zone(_green_zone, _yellow_zone);\n-\n-  assert_zone_constraints_gyr(_green_zone, _yellow_zone, _red_zone);\n-  LOG_ZONES(\"Updated Refinement Zones: \"\n-            \"green: \" SIZE_FORMAT \", \"\n-            \"yellow: \" SIZE_FORMAT \", \"\n-            \"red: \" SIZE_FORMAT,\n-            _green_zone, _yellow_zone, _red_zone);\n+class G1ConcurrentRefine::RemSetSamplingClosure : public HeapRegionClosure {\n+  G1CollectionSet* _cset;\n+  size_t _sampled_rs_length;\n+\n+public:\n+  explicit RemSetSamplingClosure(G1CollectionSet* cset) :\n+    _cset(cset), _sampled_rs_length(0) {}\n+\n+  bool do_heap_region(HeapRegion* r) override {\n+    size_t rs_length = r->rem_set()->occupied();\n+    _sampled_rs_length += rs_length;\n+    \/\/ Update the collection set policy information for this region.\n+    _cset->update_young_region_prediction(r, rs_length);\n+    return false;\n+  }\n+\n+  size_t sampled_rs_length() const { return _sampled_rs_length; }\n+};\n+\n+\/\/ Adjust the target length (in regions) of the young gen, based on the the\n+\/\/ current length of the remembered sets.\n+\/\/\n+\/\/ At the end of the GC G1 determines the length of the young gen based on\n+\/\/ how much time the next GC can take, and when the next GC may occur\n+\/\/ according to the MMU.\n+\/\/\n+\/\/ The assumption is that a significant part of the GC is spent on scanning\n+\/\/ the remembered sets (and many other components), so this thread constantly\n+\/\/ reevaluates the prediction for the remembered set scanning costs, and potentially\n+\/\/ G1Policy resizes the young gen. This may do a premature GC or even\n+\/\/ increase the young gen size to keep pause time length goal.\n+void G1ConcurrentRefine::adjust_young_list_target_length() {\n+  if (_policy->use_adaptive_young_list_length()) {\n+    G1CollectionSet* cset = G1CollectedHeap::heap()->collection_set();\n+    RemSetSamplingClosure cl{cset};\n+    cset->iterate(&cl);\n+    _policy->revise_young_list_target_length(cl.sampled_rs_length());\n+  }\n@@ -403,4 +320,2 @@\n-void G1ConcurrentRefine::adjust(double logged_cards_scan_time,\n-                                size_t processed_logged_cards,\n-                                double goal_ms) {\n-  G1DirtyCardQueueSet& dcqs = G1BarrierSet::dirty_card_queue_set();\n+bool G1ConcurrentRefine::adjust_threads_periodically() {\n+  assert_current_thread_is_primary_refinement_thread();\n@@ -408,2 +323,7 @@\n-  if (G1UseAdaptiveConcRefinement) {\n-    update_zones(logged_cards_scan_time, processed_logged_cards, goal_ms);\n+  \/\/ Check whether it's time to do a periodic adjustment.\n+  if (!_needs_adjust) {\n+    Tickspan since_adjust = Ticks::now() - _last_adjust;\n+    if (since_adjust.milliseconds() >= adjust_threads_period_ms()) {\n+      _needs_adjust = true;\n+    }\n+  }\n@@ -411,4 +331,18 @@\n-    \/\/ Change the barrier params\n-    if (max_num_threads() > 0) {\n-      size_t threshold = primary_activation_threshold();\n-      _thread_control.primary_thread()->update_notify_threshold(threshold);\n+  \/\/ If needed, try to adjust threads wanted.\n+  if (_needs_adjust) {\n+    \/\/ Getting used young bytes requires holding Heap_lock.  But we can't use\n+    \/\/ normal lock and block until available.  Blocking on the lock could\n+    \/\/ deadlock with a GC VMOp that is holding the lock and requesting a\n+    \/\/ safepoint.  Instead try to lock, and if fail then skip adjustment for\n+    \/\/ this iteration of the thread, do some refinement work, and retry the\n+    \/\/ adjustment later.\n+    if (Heap_lock->try_lock()) {\n+      size_t used_bytes = _policy->estimate_used_young_bytes_locked();\n+      Heap_lock->unlock();\n+      adjust_young_list_target_length();\n+      size_t young_bytes = _policy->young_list_target_length() * HeapRegion::GrainBytes;\n+      size_t available_bytes = young_bytes - MIN2(young_bytes, used_bytes);\n+      adjust_threads_wanted(available_bytes);\n+      _needs_adjust = false;\n+      _last_adjust = Ticks::now();\n+      return true;\n@@ -416,1 +350,0 @@\n-    dcqs.set_max_cards(red_zone());\n@@ -419,6 +352,51 @@\n-  size_t curr_queue_size = dcqs.num_cards();\n-  if ((dcqs.max_cards() > 0) &&\n-      (curr_queue_size >= yellow_zone())) {\n-    dcqs.set_max_cards_padding(curr_queue_size);\n-  } else {\n-    dcqs.set_max_cards_padding(0);\n+  return false;\n+}\n+\n+bool G1ConcurrentRefine::is_in_last_adjustment_period() const {\n+  return _threads_needed.predicted_time_until_next_gc_ms() <= adjust_threads_period_ms();\n+}\n+\n+void G1ConcurrentRefine::adjust_threads_wanted(size_t available_bytes) {\n+  assert_current_thread_is_primary_refinement_thread();\n+  size_t num_cards = _dcqs.num_cards();\n+  size_t mutator_threshold = SIZE_MAX;\n+  uint old_wanted = Atomic::load(&_threads_wanted);\n+\n+  _threads_needed.update(old_wanted,\n+                         available_bytes,\n+                         num_cards,\n+                         _pending_cards_target);\n+  uint new_wanted = _threads_needed.threads_needed();\n+  if (new_wanted > _thread_control.max_num_threads()) {\n+    \/\/ If running all the threads can't reach goal, turn on refinement by\n+    \/\/ mutator threads.  Using target as the threshold may be stronger\n+    \/\/ than required, but will do the most to get us under goal, and we'll\n+    \/\/ reevaluate with the next adjustment.\n+    mutator_threshold = _pending_cards_target;\n+    new_wanted = _thread_control.max_num_threads();\n+  } else if (is_in_last_adjustment_period()) {\n+    \/\/ If very little time remains until GC, enable mutator refinement.  If\n+    \/\/ the target has been reached, this keeps the number of pending cards on\n+    \/\/ target even if refinement threads deactivate in the meantime.  And if\n+    \/\/ the target hasn't been reached, this prevents things from getting\n+    \/\/ worse.\n+    mutator_threshold = _pending_cards_target;\n+  }\n+  Atomic::store(&_threads_wanted, new_wanted);\n+  _dcqs.set_mutator_refinement_threshold(mutator_threshold);\n+  log_debug(gc, refine)(\"Concurrent refinement: wanted %u, cards: %zu, \"\n+                        \"predicted: %zu, time: %1.2fms\",\n+                        new_wanted,\n+                        num_cards,\n+                        _threads_needed.predicted_cards_at_next_gc(),\n+                        _threads_needed.predicted_time_until_next_gc_ms());\n+  \/\/ Activate newly wanted threads.  The current thread is the primary\n+  \/\/ refinement thread, so is already active.\n+  for (uint i = MAX2(old_wanted, 1u); i < new_wanted; ++i) {\n+    if (!_thread_control.activate(i)) {\n+      \/\/ Failed to allocate and activate thread.  Stop trying to activate, and\n+      \/\/ instead use mutator threads to make up the gap.\n+      Atomic::store(&_threads_wanted, i);\n+      _dcqs.set_mutator_refinement_threshold(_pending_cards_target);\n+      break;\n+    }\n@@ -428,0 +406,30 @@\n+void G1ConcurrentRefine::reduce_threads_wanted() {\n+  assert_current_thread_is_primary_refinement_thread();\n+  if (!_needs_adjust) {         \/\/ Defer if adjustment request is active.\n+    uint wanted = Atomic::load(&_threads_wanted);\n+    if (wanted > 0) {\n+      Atomic::store(&_threads_wanted, --wanted);\n+    }\n+    \/\/ If very little time remains until GC, enable mutator refinement.  If\n+    \/\/ the target has been reached, this keeps the number of pending cards on\n+    \/\/ target even as refinement threads deactivate in the meantime.\n+    if (is_in_last_adjustment_period()) {\n+      _dcqs.set_mutator_refinement_threshold(_pending_cards_target);\n+    }\n+  }\n+}\n+\n+bool G1ConcurrentRefine::is_thread_wanted(uint worker_id) const {\n+  return worker_id < Atomic::load(&_threads_wanted);\n+}\n+\n+bool G1ConcurrentRefine::is_thread_adjustment_needed() const {\n+  assert_current_thread_is_primary_refinement_thread();\n+  return _needs_adjust;\n+}\n+\n+void G1ConcurrentRefine::record_thread_adjustment_needed() {\n+  assert_current_thread_is_primary_refinement_thread();\n+  _needs_adjust = true;\n+}\n+\n@@ -442,15 +450,0 @@\n-size_t G1ConcurrentRefine::activation_threshold(uint worker_id) const {\n-  Thresholds thresholds = calc_thresholds(_green_zone, _yellow_zone, worker_id);\n-  return activation_level(thresholds);\n-}\n-\n-size_t G1ConcurrentRefine::deactivation_threshold(uint worker_id) const {\n-  Thresholds thresholds = calc_thresholds(_green_zone, _yellow_zone, worker_id);\n-  return deactivation_level(thresholds);\n-}\n-\n-size_t G1ConcurrentRefine::primary_activation_threshold() const {\n-  assert(max_num_threads() > 0, \"No primary refinement thread\");\n-  return activation_threshold(0);\n-}\n-\n@@ -461,23 +454,5 @@\n-void G1ConcurrentRefine::maybe_activate_more_threads(uint worker_id, size_t num_cur_cards) {\n-  if (num_cur_cards > activation_threshold(worker_id + 1)) {\n-    _thread_control.maybe_activate_next(worker_id);\n-  }\n-}\n-\n-bool G1ConcurrentRefine::do_refinement_step(uint worker_id,\n-                                            G1ConcurrentRefineStats* stats) {\n-  G1DirtyCardQueueSet& dcqs = G1BarrierSet::dirty_card_queue_set();\n-\n-  size_t curr_cards = dcqs.num_cards();\n-  \/\/ If the number of the cards falls down into the yellow zone,\n-  \/\/ that means that the transition period after the evacuation pause has ended.\n-  if (curr_cards <= yellow_zone()) {\n-    dcqs.discard_max_cards_padding();\n-  }\n-\n-  maybe_activate_more_threads(worker_id, curr_cards);\n-\n-  \/\/ Process the next buffer, if there are enough left.\n-  return dcqs.refine_completed_buffer_concurrently(worker_id + worker_id_offset(),\n-                                                   deactivation_threshold(worker_id),\n-                                                   stats);\n+bool G1ConcurrentRefine::try_refinement_step(uint worker_id,\n+                                             size_t stop_at,\n+                                             G1ConcurrentRefineStats* stats) {\n+  uint adjusted_id = worker_id + worker_id_offset();\n+  return _dcqs.refine_completed_buffer_concurrently(adjusted_id, stop_at, stats);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ConcurrentRefine.cpp","additions":298,"deletions":323,"binary":false,"changes":621,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"gc\/g1\/g1ConcurrentRefineThreadsNeeded.hpp\"\n@@ -32,0 +33,1 @@\n+#include \"utilities\/macros.hpp\"\n@@ -36,1 +38,2 @@\n-class G1PrimaryConcurrentRefineThread;\n+class G1DirtyCardQueueSet;\n+class G1Policy;\n@@ -43,1 +46,0 @@\n-  G1PrimaryConcurrentRefineThread* _primary_thread;\n@@ -45,1 +47,1 @@\n-  uint _num_max_threads;\n+  uint _max_num_threads;\n@@ -50,0 +52,3 @@\n+\n+  NONCOPYABLE(G1ConcurrentRefineThreadControl);\n+\n@@ -54,1 +59,1 @@\n-  jint initialize(G1ConcurrentRefine* cr, uint num_max_threads);\n+  jint initialize(G1ConcurrentRefine* cr, uint max_num_threads);\n@@ -56,5 +61,3 @@\n-  G1PrimaryConcurrentRefineThread* primary_thread() const {\n-    assert(_num_max_threads > 0, \"precondition\");\n-    assert(_primary_thread != nullptr, \"uninitialized\");\n-    return _primary_thread;\n-  }\n+  void assert_current_thread_is_primary_refinement_thread() const NOT_DEBUG_RETURN;\n+\n+  uint max_num_threads() const { return _max_num_threads; }\n@@ -62,3 +65,6 @@\n-  \/\/ If there is a \"successor\" thread that can be activated given the current id,\n-  \/\/ activate it.\n-  void maybe_activate_next(uint cur_worker_id);\n+  \/\/ Activate the indicated thread.  If the thread has not yet been allocated,\n+  \/\/ allocate and then activate.  If allocation is needed and fails, return\n+  \/\/ false.  Otherwise return true.\n+  \/\/ precondition: worker_id < max_num_threads().\n+  \/\/ precondition: current thread is not the designated worker.\n+  bool activate(uint worker_id);\n@@ -70,8 +76,33 @@\n-\/\/ Controls refinement threads and their activation based on the number of\n-\/\/ cards currently available in the global dirty card queue.\n-\/\/ Refinement threads obtain work from the queue (a buffer at a time) based\n-\/\/ on these thresholds. They are activated gradually based on the amount of\n-\/\/ work to do.\n-\/\/ Refinement thread n activates thread n+1 if the instance of this class determines there\n-\/\/ is enough work available. Threads deactivate themselves if the current amount of\n-\/\/ available cards falls below their individual threshold.\n+\/\/ Controls concurrent refinement.\n+\/\/\n+\/\/ Mutator threads produce dirty cards, which need to be examined for updates\n+\/\/ to the remembered sets (refinement).  There is a pause-time budget for\n+\/\/ processing these dirty cards (see -XX:G1RSetUpdatingPauseTimePercent).  The\n+\/\/ purpose of concurrent refinement is to (attempt to) ensure the number of\n+\/\/ pending dirty cards at the start of a GC can be processed within that time\n+\/\/ budget.\n+\/\/\n+\/\/ Concurrent refinement is performed by a combination of dedicated threads\n+\/\/ and by mutator threads as they produce dirty cards.  If configured to not\n+\/\/ have any dedicated threads (-XX:G1ConcRefinementThreads=0) then all\n+\/\/ concurrent refinement work is performed by mutator threads.  When there are\n+\/\/ dedicated threads, they generally do most of the concurrent refinement\n+\/\/ work, to minimize throughput impact of refinement work on mutator threads.\n+\/\/\n+\/\/ This class determines the target number of dirty cards pending for the next\n+\/\/ GC.  It also owns the dedicated refinement threads and controls their\n+\/\/ activation in order to achieve that target.\n+\/\/\n+\/\/ There are two kinds of dedicated refinement threads, a single primary\n+\/\/ thread and some number of secondary threads.  When active, all refinement\n+\/\/ threads take buffers of dirty cards from the dirty card queue and process\n+\/\/ them.  Between buffers they query this owning object to find out whether\n+\/\/ they should continue running, deactivating themselves if not.\n+\/\/\n+\/\/ The primary thread drives the control system that determines how many\n+\/\/ refinement threads should be active.  If inactive, it wakes up periodically\n+\/\/ to recalculate the number of active threads needed, and activates\n+\/\/ additional threads as necessary.  While active it also periodically\n+\/\/ recalculates the number wanted and activates more threads if needed.  It\n+\/\/ also reduces the number of wanted threads when the target has been reached,\n+\/\/ triggering deactivations.\n@@ -79,0 +110,7 @@\n+  G1Policy* _policy;\n+  volatile uint _threads_wanted;\n+  size_t _pending_cards_target;\n+  Ticks _last_adjust;\n+  Ticks _last_deactivate;\n+  bool _needs_adjust;\n+  G1ConcurrentRefineThreadsNeeded _threads_needed;\n@@ -80,31 +118,3 @@\n-  \/*\n-   * The value of the completed dirty card queue length falls into one of 3 zones:\n-   * green, yellow, red. If the value is in [0, green) nothing is\n-   * done, the buffered cards are left unprocessed to enable the caching effect of the\n-   * dirtied cards. In the yellow zone [green, yellow) the concurrent refinement\n-   * threads are gradually activated. In [yellow, red) all threads are\n-   * running. If the length becomes red (max queue length) the mutators start\n-   * processing cards too.\n-   *\n-   * There are some interesting cases (when G1UseAdaptiveConcRefinement\n-   * is turned off):\n-   * 1) green = yellow = red = 0. In this case the mutator will process all\n-   *    cards. Except for those that are created by the deferred updates\n-   *    machinery during a collection.\n-   * 2) green = 0. Means no caching. Can be a good way to minimize the\n-   *    amount of time spent updating remembered sets during a collection.\n-   *\/\n-  size_t _green_zone;\n-  size_t _yellow_zone;\n-  size_t _red_zone;\n-  size_t _min_yellow_zone_size;\n-\n-  G1ConcurrentRefine(size_t green_zone,\n-                     size_t yellow_zone,\n-                     size_t red_zone,\n-                     size_t min_yellow_zone_size);\n-\n-  \/\/ Update green\/yellow\/red zone values based on how well goals are being met.\n-  void update_zones(double logged_cards_scan_time,\n-                    size_t processed_logged_cards,\n-                    double goal_ms);\n+  G1DirtyCardQueueSet& _dcqs;\n+\n+  G1ConcurrentRefine(G1Policy* policy);\n@@ -113,1 +123,0 @@\n-  void maybe_activate_more_threads(uint worker_id, size_t num_cur_cards);\n@@ -116,0 +125,27 @@\n+\n+  void assert_current_thread_is_primary_refinement_thread() const {\n+    _thread_control.assert_current_thread_is_primary_refinement_thread();\n+  }\n+\n+  \/\/ For the first few collection cycles we don't have a target (and so don't\n+  \/\/ do any concurrent refinement), because there hasn't been enough pause\n+  \/\/ time refinement work to be done to make useful predictions.  We use\n+  \/\/ SIZE_MAX as a special marker value to indicate we're in this state.\n+  static const size_t PendingCardsTargetUninitialized = SIZE_MAX;\n+  bool is_pending_cards_target_initialized() const {\n+    return _pending_cards_target != PendingCardsTargetUninitialized;\n+  }\n+\n+  void update_pending_cards_target(double logged_cards_scan_time_ms,\n+                                   size_t processed_logged_cards,\n+                                   size_t predicted_thread_buffer_cards,\n+                                   double goal_ms);\n+\n+  uint64_t adjust_threads_period_ms() const;\n+  bool is_in_last_adjustment_period() const;\n+  class RemSetSamplingClosure;\n+  void adjust_young_list_target_length();\n+  void adjust_threads_wanted(size_t available_bytes);\n+\n+  NONCOPYABLE(G1ConcurrentRefine);\n+\n@@ -120,2 +156,2 @@\n-  \/\/ G1ConcurrentRefine instance. Otherwise, returns NULL with error code.\n-  static G1ConcurrentRefine* create(jint* ecode);\n+  \/\/ G1ConcurrentRefine instance. Otherwise, returns nullptr with error code.\n+  static G1ConcurrentRefine* create(G1Policy* policy, jint* ecode);\n@@ -123,0 +159,1 @@\n+  \/\/ Stop all the refinement threads.\n@@ -125,6 +162,40 @@\n-  \/\/ The minimum number of pending cards for activation of the primary\n-  \/\/ refinement thread.\n-  size_t primary_activation_threshold() const;\n-\n-  \/\/ Adjust refinement thresholds based on work done during the pause and the goal time.\n-  void adjust(double logged_cards_scan_time, size_t processed_logged_cards, double goal_ms);\n+  \/\/ Called at the end of a GC to prepare for refinement during the next\n+  \/\/ concurrent phase.  Updates the target for the number of pending dirty\n+  \/\/ cards.  Updates the mutator refinement threshold.  Ensures the primary\n+  \/\/ refinement thread (if it exists) is active, so it will adjust the number\n+  \/\/ of running threads.\n+  void adjust_after_gc(double logged_cards_scan_time_ms,\n+                       size_t processed_logged_cards,\n+                       size_t predicted_thread_buffer_cards,\n+                       double goal_ms);\n+\n+  \/\/ Target number of pending dirty cards at the start of the next GC.\n+  size_t pending_cards_target() const { return _pending_cards_target; }\n+\n+  \/\/ May recalculate the number of refinement threads that should be active in\n+  \/\/ order to meet the pending cards target.  Returns true if adjustment was\n+  \/\/ performed, and clears any pending request.  Returns false if the\n+  \/\/ adjustment period has not expired, or because a timed or requested\n+  \/\/ adjustment could not be performed immediately and so was deferred.\n+  \/\/ precondition: current thread is the primary refinement thread.\n+  bool adjust_threads_periodically();\n+\n+  \/\/ The amount of time (in ms) the primary refinement thread should sleep\n+  \/\/ when it is inactive.  It requests adjustment whenever it is reactivated.\n+  \/\/ precondition: current thread is the primary refinement thread.\n+  uint64_t adjust_threads_wait_ms() const;\n+\n+  \/\/ Record a request for thread adjustment as soon as possible.\n+  \/\/ precondition: current thread is the primary refinement thread.\n+  void record_thread_adjustment_needed();\n+\n+  \/\/ Test whether there is a pending request for thread adjustment.\n+  \/\/ precondition: current thread is the primary refinement thread.\n+  bool is_thread_adjustment_needed() const;\n+\n+  \/\/ Reduce the number of active threads wanted.\n+  \/\/ precondition: current thread is the primary refinement thread.\n+  void reduce_threads_wanted();\n+\n+  \/\/ Test whether the thread designated by worker_id should be active.\n+  bool is_thread_wanted(uint worker_id) const;\n@@ -136,4 +207,0 @@\n-  \/\/ Cards in the dirty card queue set.\n-  size_t activation_threshold(uint worker_id) const;\n-  size_t deactivation_threshold(uint worker_id) const;\n-\n@@ -143,1 +210,3 @@\n-  bool do_refinement_step(uint worker_id, G1ConcurrentRefineStats* stats);\n+  bool try_refinement_step(uint worker_id,\n+                           size_t stop_at,\n+                           G1ConcurrentRefineStats* stats);\n@@ -150,5 +219,0 @@\n-\n-  \/\/ Cards in the dirty card queue set.\n-  size_t green_zone() const      { return _green_zone;  }\n-  size_t yellow_zone() const     { return _yellow_zone; }\n-  size_t red_zone() const        { return _red_zone;    }\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ConcurrentRefine.hpp","additions":134,"deletions":70,"binary":false,"changes":204,"status":"modified"},{"patch":"@@ -33,3 +33,0 @@\n-#include \"runtime\/atomic.hpp\"\n-#include \"runtime\/init.hpp\"\n-#include \"runtime\/javaThread.hpp\"\n@@ -37,1 +34,6 @@\n-#include \"runtime\/safepoint.hpp\"\n+#include \"runtime\/os.hpp\"\n+#include \"runtime\/thread.hpp\"\n+#include \"utilities\/debug.hpp\"\n+#include \"utilities\/formatBuffer.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include \"utilities\/ticks.hpp\"\n@@ -43,1 +45,3 @@\n-  _refinement_stats(new G1ConcurrentRefineStats()),\n+  _notifier(Mutex::nosafepoint, FormatBuffer<>(\"G1 Refine#%d\", worker_id), true),\n+  _requested_active(false),\n+  _refinement_stats(),\n@@ -51,4 +55,0 @@\n-G1ConcurrentRefineThread::~G1ConcurrentRefineThread() {\n-  delete _refinement_stats;\n-}\n-\n@@ -59,26 +59,15 @@\n-    \/\/ For logging.\n-    G1ConcurrentRefineStats start_stats = *_refinement_stats;\n-    G1ConcurrentRefineStats total_stats; \/\/ Accumulate over activation.\n-\n-    {\n-      SuspendibleThreadSetJoiner sts_join;\n-\n-      log_debug(gc, refine)(\"Activated worker %d, on threshold: %zu, current: %zu\",\n-                            _worker_id, _cr->activation_threshold(_worker_id),\n-                            G1BarrierSet::dirty_card_queue_set().num_cards());\n-\n-      while (!should_terminate()) {\n-        if (sts_join.should_yield()) {\n-          \/\/ Accumulate changed stats before possible GC that resets stats.\n-          total_stats += *_refinement_stats - start_stats;\n-          sts_join.yield();\n-          \/\/ Reinitialize baseline stats after safepoint.\n-          start_stats = *_refinement_stats;\n-          continue;             \/\/ Re-check for termination after yield delay.\n-        }\n-\n-        if (!_cr->do_refinement_step(_worker_id, _refinement_stats)) {\n-          if (maybe_deactivate()) {\n-            break;\n-          }\n-        }\n+    SuspendibleThreadSetJoiner sts_join;\n+    G1ConcurrentRefineStats active_stats_start = _refinement_stats;\n+    report_active(\"Activated\");\n+    while (!should_terminate()) {\n+      if (sts_join.should_yield()) {\n+        report_inactive(\"Paused\", _refinement_stats - active_stats_start);\n+        sts_join.yield();\n+        \/\/ Reset after yield rather than accumulating across yields, else a\n+        \/\/ very long running thread could overflow.\n+        active_stats_start = _refinement_stats;\n+        report_active(\"Resumed\");\n+      } else if (maybe_deactivate()) {\n+        break;\n+      } else {\n+        do_refinement_step();\n@@ -87,9 +76,1 @@\n-\n-    total_stats += *_refinement_stats - start_stats;\n-    log_debug(gc, refine)(\"Deactivated worker %d, off threshold: %zu, \"\n-                          \"cards: %zu, refined %zu, rate %1.2fc\/ms\",\n-                          _worker_id, _cr->deactivation_threshold(_worker_id),\n-                          G1BarrierSet::dirty_card_queue_set().num_cards(),\n-                          total_stats.refined_cards(),\n-                          total_stats.refinement_rate_ms());\n-\n+    report_inactive(\"Deactivated\", _refinement_stats - active_stats_start);\n@@ -106,2 +87,5 @@\n-void G1ConcurrentRefineThread::stop_service() {\n-  activate();\n+void G1ConcurrentRefineThread::report_active(const char* reason) const {\n+  log_trace(gc, refine)(\"%s worker %u, current: %zu\",\n+                        reason,\n+                        _worker_id,\n+                        G1BarrierSet::dirty_card_queue_set().num_cards());\n@@ -110,8 +94,9 @@\n-G1PrimaryConcurrentRefineThread*\n-G1PrimaryConcurrentRefineThread::create(G1ConcurrentRefine* cr) {\n-  G1PrimaryConcurrentRefineThread* crt =\n-    new (std::nothrow) G1PrimaryConcurrentRefineThread(cr);\n-  if (crt != nullptr) {\n-    crt->create_and_start();\n-  }\n-  return crt;\n+void G1ConcurrentRefineThread::report_inactive(const char* reason,\n+                                               const G1ConcurrentRefineStats& stats) const {\n+  log_trace(gc, refine)\n+           (\"%s worker %u, cards: %zu, refined %zu, rate %1.2fc\/ms\",\n+            reason,\n+            _worker_id,\n+            G1BarrierSet::dirty_card_queue_set().num_cards(),\n+            stats.refined_cards(),\n+            stats.refinement_rate_ms());\n@@ -120,10 +105,7 @@\n-G1PrimaryConcurrentRefineThread::G1PrimaryConcurrentRefineThread(G1ConcurrentRefine* cr) :\n-  G1ConcurrentRefineThread(cr, 0),\n-  _notifier(0),\n-  _threshold(0)\n-{}\n-\n-void G1PrimaryConcurrentRefineThread::stop_service() {\n-  G1DirtyCardQueueSet& dcqs = G1BarrierSet::dirty_card_queue_set();\n-  dcqs.set_refinement_notification_thread(nullptr);\n-  G1ConcurrentRefineThread::stop_service();\n+void G1ConcurrentRefineThread::activate() {\n+  assert(this != Thread::current(), \"precondition\");\n+  MonitorLocker ml(&_notifier, Mutex::_no_safepoint_check_flag);\n+  if (!_requested_active || should_terminate()) {\n+    _requested_active = true;\n+    ml.notify();\n+  }\n@@ -132,15 +114,1 @@\n-\/\/ The primary refinement thread is notified when buffers\/cards are added to\n-\/\/ the dirty card queue.  That can happen in fairly arbitrary contexts.\n-\/\/ This means there may be arbitrary other locks held when notifying.  We\n-\/\/ also don't want to have to take a lock on the fairly common notification\n-\/\/ path, as contention for that lock can significantly impact performance.\n-\/\/\n-\/\/ We use a semaphore to implement waiting and unblocking, to avoid\n-\/\/ lock rank checking issues.  (We could alternatively use an\n-\/\/ arbitrarily low ranked mutex.)  The atomic variable _threshold is\n-\/\/ used to decide when to signal the semaphore.  When its value is\n-\/\/ SIZE_MAX then the thread is running.  Otherwise, the thread should\n-\/\/ be requested to run when notified that the number of cards has\n-\/\/ exceeded the threshold value.\n-\n-bool G1PrimaryConcurrentRefineThread::wait_for_completed_buffers() {\n+bool G1ConcurrentRefineThread::maybe_deactivate() {\n@@ -148,3 +116,8 @@\n-  _notifier.wait();\n-  assert(Atomic::load(&_threshold) == SIZE_MAX || should_terminate(), \"incorrect state\");\n-  return !should_terminate();\n+  if (cr()->is_thread_wanted(_worker_id)) {\n+    return false;\n+  } else {\n+    MutexLocker ml(&_notifier, Mutex::_no_safepoint_check_flag);\n+    bool requested = _requested_active;\n+    _requested_active = false;\n+    return !requested;  \/\/ Deactivate only if not recently requested active.\n+  }\n@@ -153,1 +126,1 @@\n-bool G1PrimaryConcurrentRefineThread::maybe_deactivate() {\n+bool G1ConcurrentRefineThread::try_refinement_step(size_t stop_at) {\n@@ -155,7 +128,1 @@\n-  assert(Atomic::load(&_threshold) == SIZE_MAX, \"incorrect state\");\n-  Atomic::store(&_threshold, cr()->primary_activation_threshold());\n-  \/\/ Always deactivate when no refinement work found.  New refinement\n-  \/\/ work may have arrived after we tried, but checking for that would\n-  \/\/ still be racy.  Instead, the next time additional work is made\n-  \/\/ available we'll get reactivated.\n-  return true;\n+  return _cr->try_refinement_step(_worker_id, stop_at, &_refinement_stats);\n@@ -164,11 +131,2 @@\n-void G1PrimaryConcurrentRefineThread::activate() {\n-  assert(this != Thread::current(), \"precondition\");\n-  \/\/ The thread is running when notifications are disabled, so shouldn't\n-  \/\/ signal is this case.  But there's a race between stop requests and\n-  \/\/ maybe_deactivate, so also signal if stop requested.\n-  size_t threshold = Atomic::load(&_threshold);\n-  if (((threshold != SIZE_MAX) &&\n-       (threshold == Atomic::cmpxchg(&_threshold, threshold, SIZE_MAX))) ||\n-      should_terminate()) {\n-    _notifier.signal();\n-  }\n+void G1ConcurrentRefineThread::stop_service() {\n+  activate();\n@@ -177,15 +135,21 @@\n-void G1PrimaryConcurrentRefineThread::notify(size_t num_cards) {\n-  \/\/ Only activate if the number of pending cards exceeds the activation\n-  \/\/ threshold.  Notification is disabled when the thread is running, by\n-  \/\/ setting _threshold to SIZE_MAX.  A relaxed load is sufficient; we don't\n-  \/\/ need to be precise about this.\n-  if (num_cards > Atomic::load(&_threshold)) {\n-    \/\/ Discard notifications occurring during a safepoint.  A GC safepoint\n-    \/\/ may dirty some cards (such as during reference processing), possibly\n-    \/\/ leading to notification.  End-of-GC update_notify_threshold activates\n-    \/\/ the primary thread if needed.  Non-GC safepoints are expected to\n-    \/\/ rarely (if ever) dirty cards, so defer activation to a post-safepoint\n-    \/\/ notification.\n-    if (!SafepointSynchronize::is_at_safepoint()) {\n-      activate();\n-    }\n+\/\/ The (single) primary thread drives the controller for the refinement threads.\n+class G1PrimaryConcurrentRefineThread final : public G1ConcurrentRefineThread {\n+  bool wait_for_completed_buffers() override;\n+  bool maybe_deactivate() override;\n+  void do_refinement_step() override;\n+\n+public:\n+  G1PrimaryConcurrentRefineThread(G1ConcurrentRefine* cr) :\n+    G1ConcurrentRefineThread(cr, 0)\n+  {}\n+};\n+\n+\/\/ When inactive, the primary thread periodically wakes up and requests\n+\/\/ adjustment of the number of active refinement threads.\n+bool G1PrimaryConcurrentRefineThread::wait_for_completed_buffers() {\n+  assert(this == Thread::current(), \"precondition\");\n+  MonitorLocker ml(notifier(), Mutex::_no_safepoint_check_flag);\n+  if (!requested_active() && !should_terminate()) {\n+    \/\/ Rather than trying to be smart about spurious wakeups, we just treat\n+    \/\/ them as timeouts.\n+    ml.wait(cr()->adjust_threads_wait_ms());\n@@ -193,0 +157,3 @@\n+  \/\/ Record adjustment needed whenever reactivating.\n+  cr()->record_thread_adjustment_needed();\n+  return !should_terminate();\n@@ -195,13 +162,19 @@\n-void G1PrimaryConcurrentRefineThread::update_notify_threshold(size_t threshold) {\n-#ifdef ASSERT\n-  if (is_init_completed()) {\n-    assert_at_safepoint();\n-    assert(Thread::current()->is_VM_thread(), \"precondition\");\n-  }\n-#endif \/\/ ASSERT\n-  \/\/ If _threshold is SIZE_MAX then the thread is active and the value\n-  \/\/ of _threshold shouldn't be changed.\n-  if (Atomic::load(&_threshold) != SIZE_MAX) {\n-    Atomic::store(&_threshold, threshold);\n-    if (G1BarrierSet::dirty_card_queue_set().num_cards() > threshold) {\n-      activate();\n+bool G1PrimaryConcurrentRefineThread::maybe_deactivate() {\n+  \/\/ Don't deactivate while needing to adjust the number of active threads.\n+  return !cr()->is_thread_adjustment_needed() &&\n+         G1ConcurrentRefineThread::maybe_deactivate();\n+}\n+\n+void G1PrimaryConcurrentRefineThread::do_refinement_step() {\n+  \/\/ Try adjustment first.  If it succeeds then don't do any refinement this\n+  \/\/ round.  This thread may have just woken up but no threads are currently\n+  \/\/ needed, which is common.  In this case we want to just go back to\n+  \/\/ waiting, with a minimum of fuss; in particular, don't do any \"premature\"\n+  \/\/ refinement.  However, adjustment may be pending but temporarily\n+  \/\/ blocked. In that case we *do* try refinement, rather than possibly\n+  \/\/ uselessly spinning while waiting for adjustment to succeed.\n+  if (!cr()->adjust_threads_periodically()) {\n+    \/\/ No adjustment, so try refinement, with the target as a cuttoff.\n+    if (!try_refinement_step(cr()->pending_cards_target())) {\n+      \/\/ Refinement was cut off, so proceed with fewer threads.\n+      cr()->reduce_threads_wanted();\n@@ -213,3 +186,0 @@\n-  Monitor _notifier;\n-  bool _requested_active;\n-\n@@ -217,1 +187,1 @@\n-  bool maybe_deactivate() override;\n+  void do_refinement_step() override;\n@@ -220,3 +190,5 @@\n-  G1SecondaryConcurrentRefineThread(G1ConcurrentRefine* cr, uint worker_id);\n-\n-  void activate() override;\n+  G1SecondaryConcurrentRefineThread(G1ConcurrentRefine* cr, uint worker_id) :\n+    G1ConcurrentRefineThread(cr, worker_id)\n+  {\n+    assert(worker_id > 0, \"precondition\");\n+  }\n@@ -225,9 +197,0 @@\n-G1SecondaryConcurrentRefineThread::G1SecondaryConcurrentRefineThread(G1ConcurrentRefine* cr,\n-                                                                     uint worker_id) :\n-  G1ConcurrentRefineThread(cr, worker_id),\n-  _notifier(Mutex::nosafepoint, this->name(), true),\n-  _requested_active(false)\n-{\n-  assert(worker_id > 0, \"precondition\");\n-}\n-\n@@ -236,2 +199,2 @@\n-  MonitorLocker ml(&_notifier, Mutex::_no_safepoint_check_flag);\n-  while (!_requested_active && !should_terminate()) {\n+  MonitorLocker ml(notifier(), Mutex::_no_safepoint_check_flag);\n+  while (!requested_active() && !should_terminate()) {\n@@ -243,10 +206,1 @@\n-void G1SecondaryConcurrentRefineThread::activate() {\n-  assert(this != Thread::current(), \"precondition\");\n-  MonitorLocker ml(&_notifier, Mutex::_no_safepoint_check_flag);\n-  if (!_requested_active || should_terminate()) {\n-    _requested_active = true;\n-    ml.notify();\n-  }\n-}\n-\n-bool G1SecondaryConcurrentRefineThread::maybe_deactivate() {\n+void G1SecondaryConcurrentRefineThread::do_refinement_step() {\n@@ -254,4 +208,8 @@\n-  MutexLocker ml(&_notifier, Mutex::_no_safepoint_check_flag);\n-  bool requested = _requested_active;\n-  _requested_active = false;\n-  return !requested;            \/\/ Deactivate if not recently requested active.\n+  \/\/ Secondary threads ignore the target and just drive the number of pending\n+  \/\/ dirty cards down.  The primary thread is responsible for noticing the\n+  \/\/ target has been reached and reducing the number of wanted threads.  This\n+  \/\/ makes the control of wanted threads all under the primary, while avoiding\n+  \/\/ useless spinning by secondary threads until the primary thread notices.\n+  \/\/ (Useless spinning is still possible if there are no pending cards, but\n+  \/\/ that should rarely happen.)\n+  try_refinement_step(0);\n@@ -262,3 +220,6 @@\n-  assert(worker_id > 0, \"precondition\");\n-  G1ConcurrentRefineThread* crt =\n-    new (std::nothrow) G1SecondaryConcurrentRefineThread(cr, worker_id);\n+  G1ConcurrentRefineThread* crt;\n+  if (worker_id == 0) {\n+    crt = new (std::nothrow) G1PrimaryConcurrentRefineThread(cr);\n+  } else {\n+    crt = new (std::nothrow) G1SecondaryConcurrentRefineThread(cr, worker_id);\n+  }\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ConcurrentRefineThread.cpp","additions":125,"deletions":164,"binary":false,"changes":289,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"gc\/g1\/g1ConcurrentRefineStats.hpp\"\n@@ -29,3 +30,2 @@\n-#include \"memory\/padded.hpp\"\n-#include \"runtime\/semaphore.hpp\"\n-#include \"utilities\/macros.hpp\"\n+#include \"runtime\/mutex.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -35,1 +35,0 @@\n-class G1ConcurrentRefineStats;\n@@ -46,1 +45,4 @@\n-  G1ConcurrentRefineStats* _refinement_stats;\n+  Monitor _notifier;\n+  bool _requested_active;\n+\n+  G1ConcurrentRefineStats _refinement_stats;\n@@ -57,0 +59,3 @@\n+  Monitor* notifier() { return &_notifier; }\n+  bool requested_active() const { return _requested_active; }\n+\n@@ -61,2 +66,1 @@\n-  \/\/ Called when no refinement work found for this thread.\n-  \/\/ Returns true if should deactivate.\n+  \/\/ Deactivate if appropriate.  Returns true if deactivated.\n@@ -64,1 +68,14 @@\n-  virtual bool maybe_deactivate() = 0;\n+  virtual bool maybe_deactivate();\n+\n+  \/\/ Attempt to do some refinement work.\n+  \/\/ precondition: this is the current thread.\n+  virtual void do_refinement_step() = 0;\n+\n+  \/\/ Helper for do_refinement_step implementations.  Try to perform some\n+  \/\/ refinement work, limited by stop_at.  Returns true if any refinement work\n+  \/\/ was performed, false if no work available per stop_at.\n+  \/\/ precondition: this is the current thread.\n+  bool try_refinement_step(size_t stop_at);\n+\n+  void report_active(const char* reason) const;\n+  void report_inactive(const char* reason, const G1ConcurrentRefineStats& stats) const;\n@@ -73,1 +90,3 @@\n-  virtual ~G1ConcurrentRefineThread();\n+  virtual ~G1ConcurrentRefineThread() = default;\n+\n+  uint worker_id() const { return _worker_id; }\n@@ -77,1 +96,5 @@\n-  virtual void activate() = 0;\n+  void activate();\n+\n+  G1ConcurrentRefineStats* refinement_stats() {\n+    return &_refinement_stats;\n+  }\n@@ -79,2 +102,2 @@\n-  G1ConcurrentRefineStats* refinement_stats() const {\n-    return _refinement_stats;\n+  const G1ConcurrentRefineStats* refinement_stats() const {\n+    return &_refinement_stats;\n@@ -87,33 +110,0 @@\n-\/\/ Singleton special refinement thread, registered with the dirty card queue.\n-\/\/ This thread supports notification of increases to the number of cards in\n-\/\/ the dirty card queue, which may trigger activation of this thread when it\n-\/\/ is not already running.\n-class G1PrimaryConcurrentRefineThread final : public G1ConcurrentRefineThread {\n-  \/\/ Support for activation.  The thread waits on this semaphore when idle.\n-  \/\/ Calls to activate signal it to wake the thread.\n-  Semaphore _notifier;\n-  DEFINE_PAD_MINUS_SIZE(0, DEFAULT_CACHE_LINE_SIZE, 0);\n-  \/\/ Used as both the activation threshold and also the \"is active\" state.\n-  \/\/ The value is SIZE_MAX when the thread is active, otherwise the threshold\n-  \/\/ for signaling the semaphore.\n-  volatile size_t _threshold;\n-  DEFINE_PAD_MINUS_SIZE(1, DEFAULT_CACHE_LINE_SIZE, sizeof(size_t));\n-\n-  bool wait_for_completed_buffers() override;\n-  bool maybe_deactivate() override;\n-\n-  G1PrimaryConcurrentRefineThread(G1ConcurrentRefine* cr);\n-\n-  void stop_service() override;\n-\n-public:\n-  static G1PrimaryConcurrentRefineThread* create(G1ConcurrentRefine* cr);\n-\n-  void activate() override;\n-\n-  \/\/ Used by the write barrier support to activate the thread if needed when\n-  \/\/ there are new refinement buffers.\n-  void notify(size_t num_cards);\n-  void update_notify_threshold(size_t threshold);\n-};\n-\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ConcurrentRefineThread.hpp","additions":35,"deletions":45,"binary":false,"changes":80,"status":"modified"},{"patch":"@@ -0,0 +1,143 @@\n+\/*\n+ * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/g1\/g1Analytics.hpp\"\n+#include \"gc\/g1\/g1ConcurrentRefineThreadsNeeded.hpp\"\n+#include \"gc\/g1\/heapRegion.hpp\"\n+#include \"gc\/g1\/g1Policy.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include <math.h>\n+\n+G1ConcurrentRefineThreadsNeeded::G1ConcurrentRefineThreadsNeeded(G1Policy* policy,\n+                                                                 double update_period_ms) :\n+  _policy(policy),\n+  _update_period_ms(update_period_ms),\n+  _predicted_time_until_next_gc_ms(0.0),\n+  _predicted_cards_at_next_gc(0),\n+  _threads_needed(0)\n+{}\n+\n+\/\/ Estimate how many concurrent refinement threads we need to run to achieve\n+\/\/ the target number of card by the time the next GC happens.  There are\n+\/\/ several secondary goals we'd like to achieve while meeting that goal.\n+\/\/\n+\/\/ 1. Minimize the number of refinement threads running at once.\n+\/\/\n+\/\/ 2. Minimize the number of activations and deactivations for the\n+\/\/ refinement threads that run.\n+\/\/\n+\/\/ 3. Delay performing refinement work.  Having more dirty cards waiting to\n+\/\/ be refined can be beneficial, as further writes to the same card don't\n+\/\/ create more work.\n+void G1ConcurrentRefineThreadsNeeded::update(uint active_threads,\n+                                             size_t available_bytes,\n+                                             size_t num_cards,\n+                                             size_t target_num_cards) {\n+  const G1Analytics* analytics = _policy->analytics();\n+\n+  \/\/ Estimate time until next GC, based on remaining bytes available for\n+  \/\/ allocation and the allocation rate.\n+  double alloc_region_rate = analytics->predict_alloc_rate_ms();\n+  double alloc_bytes_rate = alloc_region_rate * HeapRegion::GrainBytes;\n+  if (alloc_bytes_rate == 0.0) {\n+    \/\/ A zero rate indicates we don't yet have data to use for predictions.\n+    \/\/ Since we don't have any idea how long until the next GC, use a time of\n+    \/\/ zero.\n+    _predicted_time_until_next_gc_ms = 0.0;\n+  } else {\n+    \/\/ If the heap size is large and the allocation rate is small, we can get\n+    \/\/ a predicted time until next GC that is so large it can cause problems\n+    \/\/ (such as overflow) in other calculations.  Limit the prediction to one\n+    \/\/ hour, which is still large in this context.\n+    const double one_hour_ms = 60.0 * 60.0 * MILLIUNITS;\n+    double raw_time_ms = available_bytes \/ alloc_bytes_rate;\n+    _predicted_time_until_next_gc_ms = MIN2(raw_time_ms, one_hour_ms);\n+  }\n+\n+  \/\/ Estimate number of cards that need to be processed before next GC.  There\n+  \/\/ are no incoming cards when time is short, because in that case the\n+  \/\/ controller activates refinement by mutator threads to stay on target even\n+  \/\/ if threads deactivate in the meantime.  This also covers the case of not\n+  \/\/ having a real prediction of time until GC.\n+  size_t incoming_cards = 0;\n+  if (_predicted_time_until_next_gc_ms > _update_period_ms) {\n+    double incoming_rate = analytics->predict_dirtied_cards_rate_ms();\n+    double raw_cards = incoming_rate * _predicted_time_until_next_gc_ms;\n+    incoming_cards = static_cast<size_t>(raw_cards);\n+  }\n+  size_t total_cards = num_cards + incoming_cards;\n+  _predicted_cards_at_next_gc = total_cards;\n+\n+  \/\/ No concurrent refinement needed.\n+  if (total_cards <= target_num_cards) {\n+    \/\/ We don't expect to exceed the target before the next GC.\n+    _threads_needed = 0;\n+    return;\n+  }\n+\n+  \/\/ The calculation of the number of threads needed isn't very stable when\n+  \/\/ time is short, and can lead to starting up lots of threads for not much\n+  \/\/ profit.  If we're in the last update period, don't change the number of\n+  \/\/ threads running, other than to treat the current thread as running.  That\n+  \/\/ might not be sufficient, but hopefully we were already reasonably close.\n+  \/\/ We won't accumulate more because mutator refinement will be activated.\n+  if (_predicted_time_until_next_gc_ms <= _update_period_ms) {\n+    _threads_needed = MAX2(active_threads, 1u);\n+    return;\n+  }\n+\n+  \/\/ Estimate the number of cards that need to be refined before the next GC\n+  \/\/ to meet the goal.\n+  size_t cards_needed = total_cards - target_num_cards;\n+\n+  \/\/ Estimate the rate at which a thread can refine cards.  If we don't yet\n+  \/\/ have an estimate then only request one running thread, since we do have\n+  \/\/ excess cards to process.  Just one thread might not be sufficient, but\n+  \/\/ we don't have any idea how many we actually need.  Eventually the\n+  \/\/ prediction machinery will warm up and we'll be able to get estimates.\n+  double refine_rate = analytics->predict_concurrent_refine_rate_ms();\n+  if (refine_rate == 0.0) {\n+    _threads_needed = 1;\n+    return;\n+  }\n+\n+  \/\/ Estimate the number of refinement threads we need to run in order to\n+  \/\/ reach the goal in time.\n+  double thread_capacity = refine_rate * _predicted_time_until_next_gc_ms;\n+  double nthreads = cards_needed \/ thread_capacity;\n+\n+  \/\/ Decide how to round nthreads to an integral number of threads.  Always\n+  \/\/ rounding up is contrary to delaying refinement work.  But when we're\n+  \/\/ close to the next GC we want to drive toward the target, so round up\n+  \/\/ then.  The rest of the time we round to nearest, trying to remain near\n+  \/\/ the middle of the range.\n+  if (_predicted_time_until_next_gc_ms <= _update_period_ms * 5.0) {\n+    nthreads = ::ceil(nthreads);\n+  } else {\n+    nthreads = ::round(nthreads);\n+  }\n+\n+  _threads_needed = static_cast<uint>(MIN2<size_t>(nthreads, UINT_MAX));\n+}\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ConcurrentRefineThreadsNeeded.cpp","additions":143,"deletions":0,"binary":false,"changes":143,"status":"added"},{"patch":"@@ -0,0 +1,70 @@\n+\/*\n+ * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_G1_G1CONCURRENTREFINETHREADSNEEDED_HPP\n+#define SHARE_GC_G1_G1CONCURRENTREFINETHREADSNEEDED_HPP\n+\n+#include \"memory\/allocation.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+class G1Analytics;\n+class G1Policy;\n+\n+\/\/ Used to compute the number of refinement threads that need to be running\n+\/\/ in order to have the number of pending cards below the policy-directed\n+\/\/ goal when the next GC occurs.\n+class G1ConcurrentRefineThreadsNeeded : public CHeapObj<mtGC> {\n+  G1Policy* _policy;\n+  double _update_period_ms;\n+  double _predicted_time_until_next_gc_ms;\n+  size_t _predicted_cards_at_next_gc;\n+  uint _threads_needed;\n+\n+public:\n+  G1ConcurrentRefineThreadsNeeded(G1Policy* policy, double update_period_ms);\n+\n+  \/\/ Update the number of running refinement threads needed to reach the\n+  \/\/ target before the next GC.\n+  void update(uint active_threads,\n+              size_t available_bytes,\n+              size_t num_cards,\n+              size_t target_num_cards);\n+\n+  \/\/ Estimate of the number of active refinement threads needed to reach the\n+  \/\/ target before the next GC.\n+  uint threads_needed() const { return _threads_needed; }\n+\n+  \/\/ Estimate of the time until the next GC.\n+  double predicted_time_until_next_gc_ms() const {\n+    return _predicted_time_until_next_gc_ms;\n+  }\n+\n+  \/\/ Estimate of the number of pending cards at the next GC if no further\n+  \/\/ refinement is performed.\n+  size_t predicted_cards_at_next_gc() const {\n+    return _predicted_cards_at_next_gc;\n+  }\n+};\n+\n+#endif \/\/ SHARE_GC_G1_G1CONCURRENTREFINETHREADSNEEDED_HPP\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ConcurrentRefineThreadsNeeded.hpp","additions":70,"deletions":0,"binary":false,"changes":70,"status":"added"},{"patch":"@@ -70,1 +70,0 @@\n-  _refinement_notification_thread(nullptr),\n@@ -72,0 +71,1 @@\n+  _mutator_refinement_threshold(SIZE_MAX),\n@@ -75,2 +75,0 @@\n-  _max_cards(MaxCardsUnlimited),\n-  _padded_max_cards(MaxCardsUnlimited),\n@@ -129,11 +127,6 @@\n-  size_t new_num_cards = Atomic::add(&_num_cards, buffer_size() - cbn->index());\n-  {\n-    \/\/ Perform push in CS.  The old tail may be popped while the push is\n-    \/\/ observing it (attaching it to the new buffer).  We need to ensure it\n-    \/\/ can't be reused until the push completes, to avoid ABA problems.\n-    GlobalCounter::CriticalSection cs(Thread::current());\n-    _completed.push(*cbn);\n-  }\n-  if (_refinement_notification_thread != nullptr) {\n-    _refinement_notification_thread->notify(new_num_cards);\n-  }\n+  Atomic::add(&_num_cards, buffer_size() - cbn->index());\n+  \/\/ Perform push in CS.  The old tail may be popped while the push is\n+  \/\/ observing it (attaching it to the new buffer).  We need to ensure it\n+  \/\/ can't be reused until the push completes, to avoid ABA problems.\n+  GlobalCounter::CriticalSection cs(Thread::current());\n+  _completed.push(*cbn);\n@@ -496,1 +489,1 @@\n-  if (Atomic::load(&_num_cards) <= Atomic::load(&_padded_max_cards)) {\n+  if (Atomic::load(&_num_cards) <= Atomic::load(&_mutator_refinement_threshold)) {\n@@ -545,0 +538,3 @@\n+  \/\/ Disable mutator refinement until concurrent refinement decides otherwise.\n+  set_mutator_refinement_threshold(SIZE_MAX);\n+\n@@ -560,3 +556,0 @@\n-  \/\/ Iterate over all the threads, if we find a partial log add it to\n-  \/\/ the global list of logs.  Temporarily turn off the limit on the number\n-  \/\/ of outstanding buffers.\n@@ -564,2 +557,0 @@\n-  size_t old_limit = max_cards();\n-  set_max_cards(MaxCardsUnlimited);\n@@ -567,0 +558,5 @@\n+  \/\/ Disable mutator refinement until concurrent refinement decides otherwise.\n+  set_mutator_refinement_threshold(SIZE_MAX);\n+\n+  \/\/ Iterate over all the threads, if we find a partial log add it to\n+  \/\/ the global list of logs.\n@@ -582,1 +578,0 @@\n-  set_max_cards(old_limit);\n@@ -619,7 +614,2 @@\n-size_t G1DirtyCardQueueSet::max_cards() const {\n-  return _max_cards;\n-}\n-\n-void G1DirtyCardQueueSet::set_max_cards(size_t value) {\n-  _max_cards = value;\n-  Atomic::store(&_padded_max_cards, value);\n+size_t G1DirtyCardQueueSet::mutator_refinement_threshold() const {\n+  return Atomic::load(&_mutator_refinement_threshold);\n@@ -628,14 +618,2 @@\n-void G1DirtyCardQueueSet::set_max_cards_padding(size_t padding) {\n-  \/\/ Compute sum, clipping to max.\n-  size_t limit = _max_cards + padding;\n-  if (limit < padding) {        \/\/ Check for overflow.\n-    limit = MaxCardsUnlimited;\n-  }\n-  Atomic::store(&_padded_max_cards, limit);\n-}\n-\n-void G1DirtyCardQueueSet::discard_max_cards_padding() {\n-  \/\/ Being racy here is okay, since all threads store the same value.\n-  if (_max_cards != Atomic::load(&_padded_max_cards)) {\n-    Atomic::store(&_padded_max_cards, _max_cards);\n-  }\n+void G1DirtyCardQueueSet::set_mutator_refinement_threshold(size_t value) {\n+  Atomic::store(&_mutator_refinement_threshold, value);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1DirtyCardQueue.cpp","additions":20,"deletions":42,"binary":false,"changes":62,"status":"modified"},{"patch":"@@ -159,4 +159,1 @@\n-  \/\/ The refinement notification thread, for activation when the notification\n-  \/\/ threshold is reached.  nullptr if there aren't any refinement threads.\n-  G1PrimaryConcurrentRefineThread* _refinement_notification_thread;\n-  DEFINE_PAD_MINUS_SIZE(1, DEFAULT_CACHE_LINE_SIZE, sizeof(G1PrimaryConcurrentRefineThread*));\n+  DEFINE_PAD_MINUS_SIZE(0, DEFAULT_CACHE_LINE_SIZE, 0);\n@@ -165,0 +162,4 @@\n+  DEFINE_PAD_MINUS_SIZE(1, DEFAULT_CACHE_LINE_SIZE, sizeof(size_t));\n+  \/\/ If the queue contains more cards than configured here, the\n+  \/\/ mutator must start doing some of the concurrent refinement work.\n+  volatile size_t _mutator_refinement_threshold;\n@@ -177,6 +178,0 @@\n-  \/\/ If the queue contains more cards than configured here, the\n-  \/\/ mutator must start doing some of the concurrent refinement work.\n-  size_t _max_cards;\n-  volatile size_t _padded_max_cards;\n-  static const size_t MaxCardsUnlimited = SIZE_MAX;\n-\n@@ -230,2 +225,2 @@\n-  \/\/ are more than max_cards (possibly padded) cards in the completed\n-  \/\/ buffers.  Updates stats.\n+  \/\/ are more than mutator_refinement_threshold cards in the completed buffers.\n+  \/\/ Updates stats.\n@@ -255,6 +250,0 @@\n-  \/\/ Record the primary concurrent refinement thread.  This is the thread to\n-  \/\/ be notified when num_cards() exceeds the refinement notification threshold.\n-  void set_refinement_notification_thread(G1PrimaryConcurrentRefineThread* thread) {\n-    _refinement_notification_thread = thread;\n-  }\n-\n@@ -296,9 +285,2 @@\n-  \/\/ Threshold for mutator threads to also do refinement when there\n-  \/\/ are concurrent refinement threads.\n-  size_t max_cards() const;\n-\n-  \/\/ Set threshold for mutator threads to also do refinement.\n-  void set_max_cards(size_t value);\n-\n-  \/\/ Artificially increase mutator refinement threshold.\n-  void set_max_cards_padding(size_t padding);\n+  \/\/ Number of cards above which mutator threads should do refinement.\n+  size_t mutator_refinement_threshold() const;\n@@ -306,2 +288,2 @@\n-  \/\/ Discard artificial increase of mutator refinement threshold.\n-  void discard_max_cards_padding();\n+  \/\/ Set number of cards above which mutator threads should do refinement.\n+  void set_mutator_refinement_threshold(size_t value);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1DirtyCardQueue.hpp","additions":11,"deletions":29,"binary":false,"changes":40,"status":"modified"},{"patch":"@@ -26,0 +26,1 @@\n+#include \"gc\/g1\/g1Allocator.hpp\"\n@@ -527,0 +528,1 @@\n+  size_t thread_buffer_cards = _analytics->predict_dirtied_cards_in_thread_buffers();\n@@ -528,3 +530,2 @@\n-  \/\/ We have no measure of the number of cards in the thread buffers, assume\n-  \/\/ these are very few compared to the ones in the DCQS.\n-  update_young_length_bounds(dcqs.num_cards(), rs_length);\n+  size_t pending_cards = dcqs.num_cards() + thread_buffer_cards;\n+  update_young_length_bounds(pending_cards, rs_length);\n@@ -624,0 +625,5 @@\n+void G1Policy::record_concatenate_dirty_card_logs(Tickspan concat_time, size_t num_cards) {\n+  _analytics->report_dirtied_cards_in_thread_buffers(num_cards);\n+  phase_times()->record_concatenate_dirty_card_logs_time_ms(concat_time.seconds() * MILLIUNITS);\n+}\n+\n@@ -907,1 +913,1 @@\n-  double scan_logged_cards_time_goal_ms = _mmu_tracker->max_gc_time() * MILLIUNITS * G1RSetUpdatingPauseTimePercent \/ 100.0;\n+  double logged_cards_time_goal_ms = _mmu_tracker->max_gc_time() * MILLIUNITS * G1RSetUpdatingPauseTimePercent \/ 100.0;\n@@ -909,1 +915,1 @@\n-  if (scan_logged_cards_time_goal_ms < merge_hcc_time_ms) {\n+  if (logged_cards_time_goal_ms < merge_hcc_time_ms) {\n@@ -912,1 +918,1 @@\n-                                scan_logged_cards_time_goal_ms, merge_hcc_time_ms);\n+                                logged_cards_time_goal_ms, merge_hcc_time_ms);\n@@ -914,1 +920,1 @@\n-    scan_logged_cards_time_goal_ms = 0;\n+    logged_cards_time_goal_ms = 0;\n@@ -916,1 +922,1 @@\n-    scan_logged_cards_time_goal_ms -= merge_hcc_time_ms;\n+    logged_cards_time_goal_ms -= merge_hcc_time_ms;\n@@ -919,4 +925,10 @@\n-  double const logged_cards_time = logged_cards_processing_time();\n-\n-  log_debug(gc, ergo, refine)(\"Concurrent refinement times: Logged Cards Scan time goal: %1.2fms Logged Cards Scan time: %1.2fms HCC time: %1.2fms\",\n-                              scan_logged_cards_time_goal_ms, logged_cards_time, merge_hcc_time_ms);\n+  double const logged_cards_time_ms = logged_cards_processing_time();\n+  size_t logged_cards =\n+    phase_times()->sum_thread_work_items(G1GCPhaseTimes::MergeLB,\n+                                         G1GCPhaseTimes::MergeLBDirtyCards);\n+  size_t hcc_cards =\n+    phase_times()->sum_thread_work_items(G1GCPhaseTimes::MergeHCC,\n+                                         G1GCPhaseTimes::MergeHCCDirtyCards);\n+  bool exceeded_goal = logged_cards_time_goal_ms < logged_cards_time_ms;\n+  size_t predicted_thread_buffer_cards = _analytics->predict_dirtied_cards_in_thread_buffers();\n+  G1ConcurrentRefine* cr = _g1h->concurrent_refine();\n@@ -924,3 +936,15 @@\n-  _g1h->concurrent_refine()->adjust(logged_cards_time,\n-                                    phase_times()->sum_thread_work_items(G1GCPhaseTimes::MergeLB, G1GCPhaseTimes::MergeLBDirtyCards),\n-                                    scan_logged_cards_time_goal_ms);\n+  log_debug(gc, ergo, refine)\n+           (\"GC refinement: goal: %zu + %zu \/ %1.2fms, actual: %zu \/ %1.2fms, HCC: %zu \/ %1.2fms%s\",\n+            cr->pending_cards_target(),\n+            predicted_thread_buffer_cards,\n+            logged_cards_time_goal_ms,\n+            logged_cards,\n+            logged_cards_time_ms,\n+            hcc_cards,\n+            merge_hcc_time_ms,\n+            (exceeded_goal ? \" (exceeded goal)\" : \"\"));\n+\n+  cr->adjust_after_gc(logged_cards_time_ms,\n+                      logged_cards,\n+                      predicted_thread_buffer_cards,\n+                      logged_cards_time_goal_ms);\n@@ -1087,0 +1111,10 @@\n+size_t G1Policy::estimate_used_young_bytes_locked() const {\n+  assert_lock_strong(Heap_lock);\n+  G1Allocator* allocator = _g1h->allocator();\n+  uint used = _g1h->young_regions_count();\n+  uint alloc = allocator->num_nodes();\n+  uint full = used - MIN2(used, alloc);\n+  size_t bytes_used = full * HeapRegion::GrainBytes;\n+  return bytes_used + allocator->used_in_alloc_regions();\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1Policy.cpp","additions":49,"deletions":15,"binary":false,"changes":64,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -39,0 +39,1 @@\n+#include \"utilities\/ticks.hpp\"\n@@ -298,0 +299,2 @@\n+  void record_concatenate_dirty_card_logs(Tickspan concat_time, size_t num_cards);\n+\n@@ -392,0 +395,4 @@\n+  \/\/ Return an estimate of the number of bytes used in young gen.\n+  \/\/ precondition: holding Heap_lock\n+  size_t estimate_used_young_bytes_locked() const;\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1Policy.hpp","additions":8,"deletions":1,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -43,1 +43,0 @@\n-#include \"gc\/g1\/g1ServiceThread.hpp\"\n@@ -51,1 +50,0 @@\n-#include \"gc\/shared\/suspendibleThreadSet.hpp\"\n@@ -471,112 +469,0 @@\n-class G1YoungRemSetSamplingClosure : public HeapRegionClosure {\n-  SuspendibleThreadSetJoiner* _sts;\n-  size_t _regions_visited;\n-  size_t _sampled_rs_length;\n-public:\n-  G1YoungRemSetSamplingClosure(SuspendibleThreadSetJoiner* sts) :\n-    HeapRegionClosure(), _sts(sts), _regions_visited(0), _sampled_rs_length(0) { }\n-\n-  virtual bool do_heap_region(HeapRegion* r) {\n-    size_t rs_length = r->rem_set()->occupied();\n-    _sampled_rs_length += rs_length;\n-\n-    \/\/ Update the collection set policy information for this region\n-    G1CollectedHeap::heap()->collection_set()->update_young_region_prediction(r, rs_length);\n-\n-    _regions_visited++;\n-\n-    if (_regions_visited == 10) {\n-      if (_sts->should_yield()) {\n-        _sts->yield();\n-        \/\/ A gc may have occurred and our sampling data is stale and further\n-        \/\/ traversal of the collection set is unsafe\n-        return true;\n-      }\n-      _regions_visited = 0;\n-    }\n-    return false;\n-  }\n-\n-  size_t sampled_rs_length() const { return _sampled_rs_length; }\n-};\n-\n-\/\/ Task handling young gen remembered set sampling.\n-class G1RemSetSamplingTask : public G1ServiceTask {\n-  \/\/ Helper to account virtual time.\n-  class VTimer {\n-    double _start;\n-  public:\n-    VTimer() : _start(os::elapsedVTime()) { }\n-    double duration() { return os::elapsedVTime() - _start; }\n-  };\n-\n-  double _vtime_accum;  \/\/ Accumulated virtual time.\n-  void update_vtime_accum(double duration) {\n-    _vtime_accum += duration;\n-  }\n-\n-  \/\/ Sample the current length of remembered sets for young.\n-  \/\/\n-  \/\/ At the end of the GC G1 determines the length of the young gen based on\n-  \/\/ how much time the next GC can take, and when the next GC may occur\n-  \/\/ according to the MMU.\n-  \/\/\n-  \/\/ The assumption is that a significant part of the GC is spent on scanning\n-  \/\/ the remembered sets (and many other components), so this thread constantly\n-  \/\/ reevaluates the prediction for the remembered set scanning costs, and potentially\n-  \/\/ G1Policy resizes the young gen. This may do a premature GC or even\n-  \/\/ increase the young gen size to keep pause time length goal.\n-  void sample_young_list_rs_length(SuspendibleThreadSetJoiner* sts){\n-    G1CollectedHeap* g1h = G1CollectedHeap::heap();\n-    G1Policy* policy = g1h->policy();\n-    VTimer vtime;\n-\n-    if (policy->use_adaptive_young_list_length()) {\n-      G1YoungRemSetSamplingClosure cl(sts);\n-\n-      G1CollectionSet* g1cs = g1h->collection_set();\n-      g1cs->iterate(&cl);\n-\n-      if (cl.is_complete()) {\n-        policy->revise_young_list_target_length(cl.sampled_rs_length());\n-      }\n-    }\n-    update_vtime_accum(vtime.duration());\n-  }\n-\n-  \/\/ There is no reason to do the sampling if a GC occurred recently. We use the\n-  \/\/ G1ConcRefinementServiceIntervalMillis as the metric for recently and calculate\n-  \/\/ the diff to the last GC. If the last GC occurred longer ago than the interval\n-  \/\/ 0 is returned.\n-  jlong reschedule_delay_ms() {\n-    Tickspan since_last_gc = G1CollectedHeap::heap()->time_since_last_collection();\n-    jlong delay = (jlong) (G1ConcRefinementServiceIntervalMillis - since_last_gc.milliseconds());\n-    return MAX2<jlong>(0L, delay);\n-  }\n-\n-public:\n-  G1RemSetSamplingTask(const char* name) : G1ServiceTask(name) { }\n-  virtual void execute() {\n-    SuspendibleThreadSetJoiner sts;\n-\n-    \/\/ Reschedule if a GC happened too recently.\n-    jlong delay_ms = reschedule_delay_ms();\n-    if (delay_ms > 0) {\n-      schedule(delay_ms);\n-      return;\n-    }\n-\n-    \/\/ Do the actual sampling.\n-    sample_young_list_rs_length(&sts);\n-    schedule(G1ConcRefinementServiceIntervalMillis);\n-  }\n-\n-  double vtime_accum() {\n-    \/\/ Only report vtime if supported by the os.\n-    if (!os::supports_vtime()) {\n-      return 0.0;\n-    }\n-    return _vtime_accum;\n-  }\n-};\n-\n@@ -591,2 +477,1 @@\n-  _hot_card_cache(hot_card_cache),\n-  _sampling_task(NULL) {\n+  _hot_card_cache(hot_card_cache) {\n@@ -597,1 +482,0 @@\n-  delete _sampling_task;\n@@ -604,11 +488,0 @@\n-void G1RemSet::initialize_sampling_task(G1ServiceThread* thread) {\n-  assert(_sampling_task == NULL, \"Sampling task already initialized\");\n-  _sampling_task = new G1RemSetSamplingTask(\"Remembered Set Sampling Task\");\n-  thread->register_task(_sampling_task);\n-}\n-\n-double G1RemSet::sampling_task_vtime() {\n-  assert(_sampling_task != NULL, \"Must have been initialized\");\n-  return _sampling_task->vtime_accum();\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/g1\/g1RemSet.cpp","additions":1,"deletions":128,"binary":false,"changes":129,"status":"modified"},{"patch":"@@ -73,1 +73,0 @@\n-  G1RemSetSamplingTask*  _sampling_task;\n@@ -90,6 +89,0 @@\n-  \/\/ Initialize and schedule young remembered set sampling task.\n-  void initialize_sampling_task(G1ServiceThread* thread);\n-\n-  \/\/ Accumulated vtime used by the sampling task.\n-  double sampling_task_vtime();\n-\n","filename":"src\/hotspot\/share\/gc\/g1\/g1RemSet.hpp","additions":0,"deletions":7,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -55,2 +55,0 @@\n-\n-  set_sampling_task_vtime(g1h->rem_set()->sampling_task_vtime());\n@@ -73,2 +71,1 @@\n-  _rs_threads_vtimes(NEW_C_HEAP_ARRAY(double, _num_vtimes, mtGC)),\n-  _sampling_task_vtime(0.0f) {\n+  _rs_threads_vtimes(NEW_C_HEAP_ARRAY(double, _num_vtimes, mtGC)) {\n@@ -92,2 +89,0 @@\n-\n-  set_sampling_task_vtime(other->sampling_task_vtime());\n@@ -103,2 +98,0 @@\n-\n-  _sampling_task_vtime = other->sampling_task_vtime() - _sampling_task_vtime;\n@@ -332,2 +325,0 @@\n-    out->print_cr(\" Sampling task time (ms)\");\n-    out->print_cr(\"         %5.3f\", sampling_task_vtime() * MILLIUNITS);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1RemSetSummary.cpp","additions":1,"deletions":10,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -40,2 +40,0 @@\n-  double _sampling_task_vtime;\n-\n@@ -43,3 +41,0 @@\n-  void set_sampling_task_vtime(double value) {\n-    _sampling_task_vtime = value;\n-  }\n@@ -63,4 +58,0 @@\n-\n-  double sampling_task_vtime() const {\n-    return _sampling_task_vtime;\n-  }\n","filename":"src\/hotspot\/share\/gc\/g1\/g1RemSetSummary.hpp","additions":0,"deletions":9,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -470,0 +470,10 @@\n+void G1YoungCollector::flush_dirty_card_queues() {\n+  Ticks start = Ticks::now();\n+  G1DirtyCardQueueSet& qset = G1BarrierSet::dirty_card_queue_set();\n+  size_t old_cards = qset.num_cards();\n+  qset.concatenate_logs();\n+  size_t added_cards = qset.num_cards() - old_cards;\n+  Tickspan concat_time = Ticks::now() - start;\n+  policy()->record_concatenate_dirty_card_logs(concat_time, added_cards);\n+}\n+\n@@ -486,8 +496,3 @@\n-  {\n-    \/\/ Flush dirty card queues to qset, so later phases don't need to account\n-    \/\/ for partially filled per-thread queues and such.\n-    Ticks start = Ticks::now();\n-    G1BarrierSet::dirty_card_queue_set().concatenate_logs();\n-    Tickspan dt = Ticks::now() - start;\n-    phase_times()->record_concatenate_dirty_card_logs_time_ms(dt.seconds() * MILLIUNITS);\n-  }\n+  \/\/ Flush dirty card queues to qset, so later phases don't need to account\n+  \/\/ for partially filled per-thread queues and such.\n+  flush_dirty_card_queues();\n","filename":"src\/hotspot\/share\/gc\/g1\/g1YoungCollector.cpp","additions":13,"deletions":8,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -102,0 +102,2 @@\n+  void flush_dirty_card_queues();\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1YoungCollector.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2001, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2001, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -171,30 +171,0 @@\n-  product(size_t, G1ConcRefinementYellowZone, 0,                            \\\n-          \"Number of enqueued update buffers that will \"                    \\\n-          \"trigger concurrent processing. Will be selected ergonomically \"  \\\n-          \"by default.\")                                                    \\\n-          range(0, max_intx)                                                \\\n-                                                                            \\\n-  product(size_t, G1ConcRefinementRedZone, 0,                               \\\n-          \"Maximum number of enqueued update buffers before mutator \"       \\\n-          \"threads start processing new ones instead of enqueueing them. \"  \\\n-          \"Will be selected ergonomically by default.\")                     \\\n-          range(0, max_intx)                                                \\\n-                                                                            \\\n-  product(size_t, G1ConcRefinementGreenZone, 0,                             \\\n-          \"The number of update buffers that are left in the queue by the \" \\\n-          \"concurrent processing threads. Will be selected ergonomically \"  \\\n-          \"by default.\")                                                    \\\n-          range(0, max_intx)                                                \\\n-                                                                            \\\n-  product(uintx, G1ConcRefinementServiceIntervalMillis, 300,                \\\n-          \"The G1 service thread wakes up every specified number of \"       \\\n-          \"milliseconds to do miscellaneous work.\")                         \\\n-          range(0, max_jint)                                                \\\n-                                                                            \\\n-  product(size_t, G1ConcRefinementThresholdStep, 2,                         \\\n-          \"Each time the remembered set update queue increases by this \"    \\\n-          \"amount activate the next refinement thread if available. \"       \\\n-          \"The actual step size will be selected ergonomically by \"         \\\n-          \"default, with this value used to determine a lower bound.\")      \\\n-          range(1, SIZE_MAX)                                                \\\n-                                                                            \\\n@@ -207,3 +177,3 @@\n-  product(bool, G1UseAdaptiveConcRefinement, true,                          \\\n-          \"Select green, yellow and red zones adaptively to meet the \"      \\\n-          \"the pause requirements.\")                                        \\\n+  product(bool, G1UseConcRefinement, true, DIAGNOSTIC,                      \\\n+          \"Control whether concurrent refinement is performed. \"            \\\n+          \"Disabling effectively ignores G1RSetUpdatingPauseTimePercent\")   \\\n","filename":"src\/hotspot\/share\/gc\/g1\/g1_globals.hpp","additions":4,"deletions":34,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -555,0 +555,6 @@\n+  { \"G1ConcRefinementGreenZone\",    JDK_Version::undefined(), JDK_Version::jdk(20), JDK_Version::undefined() },\n+  { \"G1ConcRefinementYellowZone\",   JDK_Version::undefined(), JDK_Version::jdk(20), JDK_Version::undefined() },\n+  { \"G1ConcRefinementRedZone\",      JDK_Version::undefined(), JDK_Version::jdk(20), JDK_Version::undefined() },\n+  { \"G1ConcRefinementThresholdStep\", JDK_Version::undefined(), JDK_Version::jdk(20), JDK_Version::undefined() },\n+  { \"G1UseAdaptiveConcRefinement\",  JDK_Version::undefined(), JDK_Version::jdk(20), JDK_Version::undefined() },\n+\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -79,1 +79,0 @@\n-  AutoModifyRestore<uintx> f2(G1ConcRefinementServiceIntervalMillis, 100000);\n","filename":"test\/hotspot\/gtest\/gc\/g1\/test_g1ServiceThread.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -1,46 +0,0 @@\n-\/*\n- * Copyright (C) 2020 THL A29 Limited, a Tencent company. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-\/**\n- * @test\n- * @bug 8257228\n- * @library \/test\/lib\n- * @requires vm.bits == 64\n- * @build gc.g1.TestBuffersToCardsOverflow jdk.test.lib.process.*\n- * @run main gc.g1.TestBuffersToCardsOverflow\n- *\/\n-\n-package gc.g1;\n-\n-import jdk.test.lib.process.ProcessTools;\n-\n-public class TestBuffersToCardsOverflow {\n-    public static void main(String... args) throws Exception {\n-        ProcessTools.executeTestJava(\"-XX:G1ConcRefinementThresholdStep=16G\",\n-                                     \"-XX:G1UpdateBufferSize=1G\")\n-                .outputTo(System.out)\n-                .errorTo(System.out)\n-                .stdoutShouldNotContain(\"SIGFPE\")\n-                .stdoutShouldNotContain(\"hs_err\");\n-    }\n-}\n","filename":"test\/hotspot\/jtreg\/gc\/g1\/TestBuffersToCardsOverflow.java","additions":0,"deletions":46,"binary":false,"changes":46,"status":"deleted"}]}