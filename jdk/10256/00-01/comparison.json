{"files":[{"patch":"@@ -166,1 +166,1 @@\n-  _pending_cards_target(SIZE_MAX),\n+  _pending_cards_target(PendingCardsTargetUninitialized),\n@@ -220,1 +220,1 @@\n-  if (_pending_cards_target != SIZE_MAX) {\n+  if (is_pending_cards_target_initialized()) {\n@@ -248,1 +248,1 @@\n-    if (_pending_cards_target != SIZE_MAX) {\n+    if (is_pending_cards_target_initialized()) {\n@@ -264,2 +264,6 @@\n-  if (_pending_cards_target == SIZE_MAX) {\n-    \/\/ If target is \"unbounded\" then wait forever (until explicitly\n+  if (is_pending_cards_target_initialized()) {\n+    double available_ms = _threads_needed.predicted_time_until_next_gc_ms();\n+    uint64_t delay = compute_adjust_delay(available_ms);\n+    return MAX2(delay, adjust_threads_period_ms());\n+  } else {\n+    \/\/ If target not yet initialized then wait forever (until explicitly\n@@ -269,4 +273,0 @@\n-  } else {\n-    double available_ms = _threads_needed.predicted_time_ms();\n-    uint64_t delay = compute_adjust_delay(available_ms);\n-    return MAX2(delay, adjust_threads_period_ms());\n@@ -309,1 +309,1 @@\n-  return _threads_needed.predicted_time_ms() <= adjust_threads_period_ms();\n+  return _threads_needed.predicted_time_until_next_gc_ms() <= adjust_threads_period_ms();\n@@ -344,2 +344,2 @@\n-                        _threads_needed.predicted_cards(),\n-                        _threads_needed.predicted_time_ms());\n+                        _threads_needed.predicted_cards_at_next_gc(),\n+                        _threads_needed.predicted_time_until_next_gc_ms());\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ConcurrentRefine.cpp","additions":12,"deletions":12,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -130,0 +130,9 @@\n+  \/\/ For the first few collection cycles we don't have a target (and so don't\n+  \/\/ do any concurrent refinement), because there hasn't been enough pause\n+  \/\/ time refinement work to be done to make useful predictions.  We use\n+  \/\/ SIZE_MAX as a special marker value to indicate we're in this state.\n+  static const size_t PendingCardsTargetUninitialized = SIZE_MAX;\n+  bool is_pending_cards_target_initialized() const {\n+    return _pending_cards_target != PendingCardsTargetUninitialized;\n+  }\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ConcurrentRefine.hpp","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -169,2 +169,7 @@\n-  \/\/ If adjustment done, don't do any refinement this round, since this thread\n-  \/\/ might no longer be wanted active.\n+  \/\/ Try adjustment first.  If it succeeds then don't do any refinement this\n+  \/\/ round.  This thread may have just woken up but no threads are currently\n+  \/\/ needed, which is common.  In this case we want to just go back to\n+  \/\/ waiting, with a minimum of fuss; in particular, don't do any \"premature\"\n+  \/\/ refinement.  However, adjustment may be pending but temporarily\n+  \/\/ blocked. In that case we *do* try refinement, rather than possibly\n+  \/\/ uselessly spinning while waiting for adjustment to succeed.\n@@ -172,0 +177,1 @@\n+    \/\/ No adjustment, so try refinement, with the target as a cuttoff.\n@@ -173,1 +179,1 @@\n-      \/\/ Proceed with fewer threads if target reached.\n+      \/\/ Refinement was cut off, so proceed with fewer threads.\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ConcurrentRefineThread.cpp","additions":9,"deletions":3,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -37,2 +37,2 @@\n-  _predicted_time_ms(0.0),\n-  _predicted_cards(0),\n+  _predicted_time_until_next_gc_ms(0.0),\n+  _predicted_cards_at_next_gc(0),\n@@ -66,1 +66,1 @@\n-    _predicted_time_ms = 0.0;\n+    _predicted_time_until_next_gc_ms = 0.0;\n@@ -73,1 +73,2 @@\n-    _predicted_time_ms = MIN2(available_bytes \/ alloc_bytes_rate, one_hour_ms);\n+    double raw_time_ms = available_bytes \/ alloc_bytes_rate;\n+    _predicted_time_until_next_gc_ms = MIN2(raw_time_ms, one_hour_ms);\n@@ -81,1 +82,1 @@\n-  if (_predicted_time_ms > _update_period_ms) {\n+  if (_predicted_time_until_next_gc_ms > _update_period_ms) {\n@@ -83,1 +84,2 @@\n-    incoming_cards = static_cast<size_t>(incoming_rate * _predicted_time_ms);\n+    double raw_cards = incoming_rate * _predicted_time_until_next_gc_ms;\n+    incoming_cards = static_cast<size_t>(raw_cards);\n@@ -86,1 +88,1 @@\n-  _predicted_cards = total_cards;\n+  _predicted_cards_at_next_gc = total_cards;\n@@ -101,1 +103,1 @@\n-  if (_predicted_time_ms <= _update_period_ms) {\n+  if (_predicted_time_until_next_gc_ms <= _update_period_ms) {\n@@ -123,1 +125,1 @@\n-  double thread_capacity = refine_rate * _predicted_time_ms;\n+  double thread_capacity = refine_rate * _predicted_time_until_next_gc_ms;\n@@ -131,1 +133,1 @@\n-  if (_predicted_time_ms <= _update_period_ms * 5.0) {\n+  if (_predicted_time_until_next_gc_ms <= _update_period_ms * 5.0) {\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ConcurrentRefineThreadsNeeded.cpp","additions":12,"deletions":10,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -40,2 +40,2 @@\n-  double _predicted_time_ms;\n-  size_t _predicted_cards;\n+  double _predicted_time_until_next_gc_ms;\n+  size_t _predicted_cards_at_next_gc;\n@@ -58,1 +58,3 @@\n-  double predicted_time_ms() const { return _predicted_time_ms; }\n+  double predicted_time_until_next_gc_ms() const {\n+    return _predicted_time_until_next_gc_ms;\n+  }\n@@ -62,1 +64,3 @@\n-  size_t predicted_cards() const { return _predicted_cards; }\n+  size_t predicted_cards_at_next_gc() const {\n+    return _predicted_cards_at_next_gc;\n+  }\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ConcurrentRefineThreadsNeeded.hpp","additions":8,"deletions":4,"binary":false,"changes":12,"status":"modified"}]}