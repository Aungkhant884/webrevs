{"files":[{"patch":"@@ -956,12 +956,0 @@\n-  void gfmul(XMMRegister tmp0, XMMRegister t);\n-  void schoolbookAAD(int i, Register subkeyH, XMMRegister data, XMMRegister tmp0,\n-                     XMMRegister tmp1, XMMRegister tmp2, XMMRegister tmp3);\n-  void generateHtbl_one_block(Register htbl);\n-  void generateHtbl_eight_blocks(Register htbl);\n- public:\n-  void sha256_AVX2(XMMRegister msg, XMMRegister state0, XMMRegister state1, XMMRegister msgtmp0,\n-                   XMMRegister msgtmp1, XMMRegister msgtmp2, XMMRegister msgtmp3, XMMRegister msgtmp4,\n-                   Register buf, Register state, Register ofs, Register limit, Register rsp,\n-                   bool multi_block, XMMRegister shuf_mask);\n-  void avx_ghash(Register state, Register htbl, Register data, Register blocks);\n-#endif\n@@ -969,2 +957,0 @@\n-#ifdef _LP64\n- private:\n@@ -980,0 +966,4 @@\n+  void sha256_AVX2(XMMRegister msg, XMMRegister state0, XMMRegister state1, XMMRegister msgtmp0,\n+                   XMMRegister msgtmp1, XMMRegister msgtmp2, XMMRegister msgtmp3, XMMRegister msgtmp4,\n+                   Register buf, Register state, Register ofs, Register limit, Register rsp,\n+                   bool multi_block, XMMRegister shuf_mask);\n@@ -984,21 +974,1 @@\n-private:\n-  void roundEnc(XMMRegister key, int rnum);\n-  void lastroundEnc(XMMRegister key, int rnum);\n-  void roundDec(XMMRegister key, int rnum);\n-  void lastroundDec(XMMRegister key, int rnum);\n-  void ev_load_key(XMMRegister xmmdst, Register key, int offset, XMMRegister xmm_shuf_mask);\n-  void gfmul_avx512(XMMRegister ghash, XMMRegister hkey);\n-  void generateHtbl_48_block_zmm(Register htbl, Register avx512_subkeyHtbl);\n-  void ghash16_encrypt16_parallel(Register key, Register subkeyHtbl, XMMRegister ctr_blockx,\n-                                  XMMRegister aad_hashx, Register in, Register out, Register data, Register pos, bool reduction,\n-                                  XMMRegister addmask, bool no_ghash_input, Register rounds, Register ghash_pos,\n-                                  bool final_reduction, int index, XMMRegister counter_inc_mask);\n-public:\n-  void aesecb_encrypt(Register source_addr, Register dest_addr, Register key, Register len);\n-  void aesecb_decrypt(Register source_addr, Register dest_addr, Register key, Register len);\n-  void aesctr_encrypt(Register src_addr, Register dest_addr, Register key, Register counter,\n-                      Register len_reg, Register used, Register used_addr, Register saved_encCounter_start);\n-  void aesgcm_encrypt(Register in, Register len, Register ct, Register out, Register key,\n-                      Register state, Register subkeyHtbl, Register avx512_subkeyHtbl, Register counter);\n-\n-#endif\n+#endif \/\/ _LP64\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":5,"deletions":35,"binary":false,"changes":40,"status":"modified"},{"patch":"@@ -1,1892 +0,0 @@\n-\/*\n-* Copyright (c) 2019, 2021, Intel Corporation. All rights reserved.\n-*\n-* DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n-*\n-* This code is free software; you can redistribute it and\/or modify it\n-* under the terms of the GNU General Public License version 2 only, as\n-* published by the Free Software Foundation.\n-*\n-* This code is distributed in the hope that it will be useful, but WITHOUT\n-* ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n-* FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n-* version 2 for more details (a copy is included in the LICENSE file that\n-* accompanied this code).\n-*\n-* You should have received a copy of the GNU General Public License version\n-* 2 along with this work; if not, write to the Free Software Foundation,\n-* Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n-*\n-* Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n-* or visit www.oracle.com if you need additional information or have any\n-* questions.\n-*\n-*\/\n-\n-#include \"precompiled.hpp\"\n-#include \"asm\/assembler.hpp\"\n-#include \"asm\/assembler.inline.hpp\"\n-#include \"runtime\/stubRoutines.hpp\"\n-#include \"macroAssembler_x86.hpp\"\n-\n-#ifdef _LP64\n-\n-void MacroAssembler::roundEnc(XMMRegister key, int rnum) {\n-    for (int xmm_reg_no = 0; xmm_reg_no <=rnum; xmm_reg_no++) {\n-      vaesenc(as_XMMRegister(xmm_reg_no), as_XMMRegister(xmm_reg_no), key, Assembler::AVX_512bit);\n-    }\n-}\n-\n-void MacroAssembler::lastroundEnc(XMMRegister key, int rnum) {\n-    for (int xmm_reg_no = 0; xmm_reg_no <=rnum; xmm_reg_no++) {\n-      vaesenclast(as_XMMRegister(xmm_reg_no), as_XMMRegister(xmm_reg_no), key, Assembler::AVX_512bit);\n-    }\n-}\n-\n-void MacroAssembler::roundDec(XMMRegister key, int rnum) {\n-    for (int xmm_reg_no = 0; xmm_reg_no <=rnum; xmm_reg_no++) {\n-      vaesdec(as_XMMRegister(xmm_reg_no), as_XMMRegister(xmm_reg_no), key, Assembler::AVX_512bit);\n-    }\n-}\n-\n-void MacroAssembler::lastroundDec(XMMRegister key, int rnum) {\n-    for (int xmm_reg_no = 0; xmm_reg_no <=rnum; xmm_reg_no++) {\n-      vaesdeclast(as_XMMRegister(xmm_reg_no), as_XMMRegister(xmm_reg_no), key, Assembler::AVX_512bit);\n-    }\n-}\n-\n-\/\/ Load key and shuffle operation\n-void MacroAssembler::ev_load_key(XMMRegister xmmdst, Register key, int offset, XMMRegister xmm_shuf_mask) {\n-    movdqu(xmmdst, Address(key, offset));\n-    if (xmm_shuf_mask != xnoreg) {\n-        pshufb(xmmdst, xmm_shuf_mask);\n-    } else {\n-       pshufb(xmmdst, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()));\n-    }\n-   evshufi64x2(xmmdst, xmmdst, xmmdst, 0x0, Assembler::AVX_512bit);\n-}\n-\n-\/\/ AES-ECB Encrypt Operation\n-void MacroAssembler::aesecb_encrypt(Register src_addr, Register dest_addr, Register key, Register len) {\n-\n-    const Register pos = rax;\n-    const Register rounds = r12;\n-\n-    Label NO_PARTS, LOOP, Loop_start, LOOP2, AES192, END_LOOP, AES256, REMAINDER, LAST2, END, KEY_192, KEY_256, EXIT;\n-    push(r13);\n-    push(r12);\n-\n-    \/\/ For EVEX with VL and BW, provide a standard mask, VL = 128 will guide the merge\n-    \/\/ context for the registers used, where all instructions below are using 128-bit mode\n-    \/\/ On EVEX without VL and BW, these instructions will all be AVX.\n-    if (VM_Version::supports_avx512vlbw()) {\n-       movl(rax, 0xffff);\n-       kmovql(k1, rax);\n-    }\n-    push(len); \/\/ Save\n-    push(rbx);\n-\n-    vzeroupper();\n-\n-    xorptr(pos, pos);\n-\n-    \/\/ Calculate number of rounds based on key length(128, 192, 256):44 for 10-rounds, 52 for 12-rounds, 60 for 14-rounds\n-    movl(rounds, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));\n-\n-    \/\/ Load Key shuf mask\n-    const XMMRegister xmm_key_shuf_mask = xmm31;  \/\/ used temporarily to swap key bytes up front\n-    movdqu(xmm_key_shuf_mask, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()));\n-\n-    \/\/ Load and shuffle key based on number of rounds\n-    ev_load_key(xmm8, key, 0 * 16, xmm_key_shuf_mask);\n-    ev_load_key(xmm9, key, 1 * 16, xmm_key_shuf_mask);\n-    ev_load_key(xmm10, key, 2 * 16, xmm_key_shuf_mask);\n-    ev_load_key(xmm23, key, 3 * 16, xmm_key_shuf_mask);\n-    ev_load_key(xmm12, key, 4 * 16, xmm_key_shuf_mask);\n-    ev_load_key(xmm13, key, 5 * 16, xmm_key_shuf_mask);\n-    ev_load_key(xmm14, key, 6 * 16, xmm_key_shuf_mask);\n-    ev_load_key(xmm15, key, 7 * 16, xmm_key_shuf_mask);\n-    ev_load_key(xmm16, key, 8 * 16, xmm_key_shuf_mask);\n-    ev_load_key(xmm17, key, 9 * 16, xmm_key_shuf_mask);\n-    ev_load_key(xmm24, key, 10 * 16, xmm_key_shuf_mask);\n-    cmpl(rounds, 52);\n-    jcc(Assembler::greaterEqual, KEY_192);\n-    jmp(Loop_start);\n-\n-    bind(KEY_192);\n-    ev_load_key(xmm19, key, 11 * 16, xmm_key_shuf_mask);\n-    ev_load_key(xmm20, key, 12 * 16, xmm_key_shuf_mask);\n-    cmpl(rounds, 60);\n-    jcc(Assembler::equal, KEY_256);\n-    jmp(Loop_start);\n-\n-    bind(KEY_256);\n-    ev_load_key(xmm21, key, 13 * 16, xmm_key_shuf_mask);\n-    ev_load_key(xmm22, key, 14 * 16, xmm_key_shuf_mask);\n-\n-    bind(Loop_start);\n-    movq(rbx, len);\n-    \/\/ Divide length by 16 to convert it to number of blocks\n-    shrq(len, 4);\n-    shlq(rbx, 60);\n-    jcc(Assembler::equal, NO_PARTS);\n-    addq(len, 1);\n-    \/\/ Check if number of blocks is greater than or equal to 32\n-    \/\/ If true, 512 bytes are processed at a time (code marked by label LOOP)\n-    \/\/ If not, 16 bytes are processed (code marked by REMAINDER label)\n-    bind(NO_PARTS);\n-    movq(rbx, len);\n-    shrq(len, 5);\n-    jcc(Assembler::equal, REMAINDER);\n-    movl(r13, len);\n-    \/\/ Compute number of blocks that will be processed 512 bytes at a time\n-    \/\/ Subtract this from the total number of blocks which will then be processed by REMAINDER loop\n-    shlq(r13, 5);\n-    subq(rbx, r13);\n-    \/\/Begin processing 512 bytes\n-    bind(LOOP);\n-    \/\/ Move 64 bytes of PT data into a zmm register, as a result 512 bytes of PT loaded in zmm0-7\n-    evmovdquq(xmm0, Address(src_addr, pos, Address::times_1, 0 * 64), Assembler::AVX_512bit);\n-    evmovdquq(xmm1, Address(src_addr, pos, Address::times_1, 1 * 64), Assembler::AVX_512bit);\n-    evmovdquq(xmm2, Address(src_addr, pos, Address::times_1, 2 * 64), Assembler::AVX_512bit);\n-    evmovdquq(xmm3, Address(src_addr, pos, Address::times_1, 3 * 64), Assembler::AVX_512bit);\n-    evmovdquq(xmm4, Address(src_addr, pos, Address::times_1, 4 * 64), Assembler::AVX_512bit);\n-    evmovdquq(xmm5, Address(src_addr, pos, Address::times_1, 5 * 64), Assembler::AVX_512bit);\n-    evmovdquq(xmm6, Address(src_addr, pos, Address::times_1, 6 * 64), Assembler::AVX_512bit);\n-    evmovdquq(xmm7, Address(src_addr, pos, Address::times_1, 7 * 64), Assembler::AVX_512bit);\n-    \/\/ Xor with the first round key\n-    evpxorq(xmm0, xmm0, xmm8, Assembler::AVX_512bit);\n-    evpxorq(xmm1, xmm1, xmm8, Assembler::AVX_512bit);\n-    evpxorq(xmm2, xmm2, xmm8, Assembler::AVX_512bit);\n-    evpxorq(xmm3, xmm3, xmm8, Assembler::AVX_512bit);\n-    evpxorq(xmm4, xmm4, xmm8, Assembler::AVX_512bit);\n-    evpxorq(xmm5, xmm5, xmm8, Assembler::AVX_512bit);\n-    evpxorq(xmm6, xmm6, xmm8, Assembler::AVX_512bit);\n-    evpxorq(xmm7, xmm7, xmm8, Assembler::AVX_512bit);\n-    \/\/ 9 Aes encode round operations\n-    roundEnc(xmm9,  7);\n-    roundEnc(xmm10, 7);\n-    roundEnc(xmm23, 7);\n-    roundEnc(xmm12, 7);\n-    roundEnc(xmm13, 7);\n-    roundEnc(xmm14, 7);\n-    roundEnc(xmm15, 7);\n-    roundEnc(xmm16, 7);\n-    roundEnc(xmm17, 7);\n-    cmpl(rounds, 52);\n-    jcc(Assembler::aboveEqual, AES192);\n-    \/\/ Aesenclast round operation for keysize = 128\n-    lastroundEnc(xmm24, 7);\n-    jmp(END_LOOP);\n-    \/\/Additional 2 rounds of Aesenc operation for keysize = 192\n-    bind(AES192);\n-    roundEnc(xmm24, 7);\n-    roundEnc(xmm19, 7);\n-    cmpl(rounds, 60);\n-    jcc(Assembler::aboveEqual, AES256);\n-    \/\/ Aesenclast round for keysize = 192\n-    lastroundEnc(xmm20, 7);\n-    jmp(END_LOOP);\n-    \/\/ 2 rounds of Aesenc operation and Aesenclast for keysize = 256\n-    bind(AES256);\n-    roundEnc(xmm20, 7);\n-    roundEnc(xmm21, 7);\n-    lastroundEnc(xmm22, 7);\n-\n-    bind(END_LOOP);\n-    \/\/ Move 512 bytes of CT to destination\n-    evmovdquq(Address(dest_addr, pos, Address::times_1, 0 * 64), xmm0, Assembler::AVX_512bit);\n-    evmovdquq(Address(dest_addr, pos, Address::times_1, 1 * 64), xmm1, Assembler::AVX_512bit);\n-    evmovdquq(Address(dest_addr, pos, Address::times_1, 2 * 64), xmm2, Assembler::AVX_512bit);\n-    evmovdquq(Address(dest_addr, pos, Address::times_1, 3 * 64), xmm3, Assembler::AVX_512bit);\n-    evmovdquq(Address(dest_addr, pos, Address::times_1, 4 * 64), xmm4, Assembler::AVX_512bit);\n-    evmovdquq(Address(dest_addr, pos, Address::times_1, 5 * 64), xmm5, Assembler::AVX_512bit);\n-    evmovdquq(Address(dest_addr, pos, Address::times_1, 6 * 64), xmm6, Assembler::AVX_512bit);\n-    evmovdquq(Address(dest_addr, pos, Address::times_1, 7 * 64), xmm7, Assembler::AVX_512bit);\n-\n-    addq(pos, 512);\n-    decq(len);\n-    jcc(Assembler::notEqual, LOOP);\n-\n-    bind(REMAINDER);\n-    vzeroupper();\n-    cmpq(rbx, 0);\n-    jcc(Assembler::equal, END);\n-    \/\/ Process 16 bytes at a time\n-    bind(LOOP2);\n-    movdqu(xmm1, Address(src_addr, pos, Address::times_1, 0));\n-    vpxor(xmm1, xmm1, xmm8, Assembler::AVX_128bit);\n-    \/\/ xmm2 contains shuffled key for Aesenclast operation.\n-    vmovdqu(xmm2, xmm24);\n-\n-    vaesenc(xmm1, xmm1, xmm9, Assembler::AVX_128bit);\n-    vaesenc(xmm1, xmm1, xmm10, Assembler::AVX_128bit);\n-    vaesenc(xmm1, xmm1, xmm23, Assembler::AVX_128bit);\n-    vaesenc(xmm1, xmm1, xmm12, Assembler::AVX_128bit);\n-    vaesenc(xmm1, xmm1, xmm13, Assembler::AVX_128bit);\n-    vaesenc(xmm1, xmm1, xmm14, Assembler::AVX_128bit);\n-    vaesenc(xmm1, xmm1, xmm15, Assembler::AVX_128bit);\n-    vaesenc(xmm1, xmm1, xmm16, Assembler::AVX_128bit);\n-    vaesenc(xmm1, xmm1, xmm17, Assembler::AVX_128bit);\n-\n-    cmpl(rounds, 52);\n-    jcc(Assembler::below, LAST2);\n-    vmovdqu(xmm2, xmm20);\n-    vaesenc(xmm1, xmm1, xmm24, Assembler::AVX_128bit);\n-    vaesenc(xmm1, xmm1, xmm19, Assembler::AVX_128bit);\n-    cmpl(rounds, 60);\n-    jcc(Assembler::below, LAST2);\n-    vmovdqu(xmm2, xmm22);\n-    vaesenc(xmm1, xmm1, xmm20, Assembler::AVX_128bit);\n-    vaesenc(xmm1, xmm1, xmm21, Assembler::AVX_128bit);\n-\n-    bind(LAST2);\n-    \/\/ Aesenclast round\n-    vaesenclast(xmm1, xmm1, xmm2, Assembler::AVX_128bit);\n-    \/\/ Write 16 bytes of CT to destination\n-    movdqu(Address(dest_addr, pos, Address::times_1, 0), xmm1);\n-    addq(pos, 16);\n-    decq(rbx);\n-    jcc(Assembler::notEqual, LOOP2);\n-\n-    bind(END);\n-    \/\/ Zero out the round keys\n-    evpxorq(xmm8, xmm8, xmm8, Assembler::AVX_512bit);\n-    evpxorq(xmm9, xmm9, xmm9, Assembler::AVX_512bit);\n-    evpxorq(xmm10, xmm10, xmm10, Assembler::AVX_512bit);\n-    evpxorq(xmm23, xmm23, xmm23, Assembler::AVX_512bit);\n-    evpxorq(xmm12, xmm12, xmm12, Assembler::AVX_512bit);\n-    evpxorq(xmm13, xmm13, xmm13, Assembler::AVX_512bit);\n-    evpxorq(xmm14, xmm14, xmm14, Assembler::AVX_512bit);\n-    evpxorq(xmm15, xmm15, xmm15, Assembler::AVX_512bit);\n-    evpxorq(xmm16, xmm16, xmm16, Assembler::AVX_512bit);\n-    evpxorq(xmm17, xmm17, xmm17, Assembler::AVX_512bit);\n-    evpxorq(xmm24, xmm24, xmm24, Assembler::AVX_512bit);\n-    cmpl(rounds, 44);\n-    jcc(Assembler::belowEqual, EXIT);\n-    evpxorq(xmm19, xmm19, xmm19, Assembler::AVX_512bit);\n-    evpxorq(xmm20, xmm20, xmm20, Assembler::AVX_512bit);\n-    cmpl(rounds, 52);\n-    jcc(Assembler::belowEqual, EXIT);\n-    evpxorq(xmm21, xmm21, xmm21, Assembler::AVX_512bit);\n-    evpxorq(xmm22, xmm22, xmm22, Assembler::AVX_512bit);\n-    bind(EXIT);\n-    pop(rbx);\n-    pop(rax); \/\/ return length\n-    pop(r12);\n-    pop(r13);\n-}\n-\n-\/\/ AES-ECB Decrypt Operation\n-void MacroAssembler::aesecb_decrypt(Register src_addr, Register dest_addr, Register key, Register len)  {\n-\n-    Label NO_PARTS, LOOP, Loop_start, LOOP2, AES192, END_LOOP, AES256, REMAINDER, LAST2, END, KEY_192, KEY_256, EXIT;\n-    const Register pos = rax;\n-    const Register rounds = r12;\n-    push(r13);\n-    push(r12);\n-\n-    \/\/ For EVEX with VL and BW, provide a standard mask, VL = 128 will guide the merge\n-    \/\/ context for the registers used, where all instructions below are using 128-bit mode\n-    \/\/ On EVEX without VL and BW, these instructions will all be AVX.\n-    if (VM_Version::supports_avx512vlbw()) {\n-       movl(rax, 0xffff);\n-       kmovql(k1, rax);\n-    }\n-\n-    push(len); \/\/ Save\n-    push(rbx);\n-\n-    vzeroupper();\n-\n-    xorptr(pos, pos);\n-    \/\/ Calculate number of rounds i.e. based on key length(128, 192, 256):44 for 10-rounds, 52 for 12-rounds, 60 for 14-rounds\n-    movl(rounds, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));\n-\n-    \/\/ Load Key shuf mask\n-    const XMMRegister xmm_key_shuf_mask = xmm31;  \/\/ used temporarily to swap key bytes up front\n-    movdqu(xmm_key_shuf_mask, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()));\n-\n-    \/\/ Load and shuffle round keys. The java expanded key ordering is rotated one position in decryption.\n-    \/\/ So the first round key is loaded from 1*16 here and last round key is loaded from 0*16\n-    ev_load_key(xmm9,  key, 1 * 16, xmm_key_shuf_mask);\n-    ev_load_key(xmm10, key, 2 * 16, xmm_key_shuf_mask);\n-    ev_load_key(xmm11, key, 3 * 16, xmm_key_shuf_mask);\n-    ev_load_key(xmm12, key, 4 * 16, xmm_key_shuf_mask);\n-    ev_load_key(xmm13, key, 5 * 16, xmm_key_shuf_mask);\n-    ev_load_key(xmm14, key, 6 * 16, xmm_key_shuf_mask);\n-    ev_load_key(xmm15, key, 7 * 16, xmm_key_shuf_mask);\n-    ev_load_key(xmm16, key, 8 * 16, xmm_key_shuf_mask);\n-    ev_load_key(xmm17, key, 9 * 16, xmm_key_shuf_mask);\n-    ev_load_key(xmm18, key, 10 * 16, xmm_key_shuf_mask);\n-    ev_load_key(xmm27, key, 0 * 16, xmm_key_shuf_mask);\n-    cmpl(rounds, 52);\n-    jcc(Assembler::greaterEqual, KEY_192);\n-    jmp(Loop_start);\n-\n-    bind(KEY_192);\n-    ev_load_key(xmm19, key, 11 * 16, xmm_key_shuf_mask);\n-    ev_load_key(xmm20, key, 12 * 16, xmm_key_shuf_mask);\n-    cmpl(rounds, 60);\n-    jcc(Assembler::equal, KEY_256);\n-    jmp(Loop_start);\n-\n-    bind(KEY_256);\n-    ev_load_key(xmm21, key, 13 * 16, xmm_key_shuf_mask);\n-    ev_load_key(xmm22, key, 14 * 16, xmm_key_shuf_mask);\n-    bind(Loop_start);\n-    movq(rbx, len);\n-    \/\/ Convert input length to number of blocks\n-    shrq(len, 4);\n-    shlq(rbx, 60);\n-    jcc(Assembler::equal, NO_PARTS);\n-    addq(len, 1);\n-    \/\/ Check if number of blocks is greater than\/ equal to 32\n-    \/\/ If true, blocks then 512 bytes are processed at a time (code marked by label LOOP)\n-    \/\/ If not, 16 bytes are processed (code marked by label REMAINDER)\n-    bind(NO_PARTS);\n-    movq(rbx, len);\n-    shrq(len, 5);\n-    jcc(Assembler::equal, REMAINDER);\n-    movl(r13, len);\n-    \/\/ Compute number of blocks that will be processed as 512 bytes at a time\n-    \/\/ Subtract this from the total number of blocks, which will then be processed by REMAINDER loop.\n-    shlq(r13, 5);\n-    subq(rbx, r13);\n-\n-    bind(LOOP);\n-    \/\/ Move 64 bytes of CT data into a zmm register, as a result 512 bytes of CT loaded in zmm0-7\n-    evmovdquq(xmm0, Address(src_addr, pos, Address::times_1, 0 * 64), Assembler::AVX_512bit);\n-    evmovdquq(xmm1, Address(src_addr, pos, Address::times_1, 1 * 64), Assembler::AVX_512bit);\n-    evmovdquq(xmm2, Address(src_addr, pos, Address::times_1, 2 * 64), Assembler::AVX_512bit);\n-    evmovdquq(xmm3, Address(src_addr, pos, Address::times_1, 3 * 64), Assembler::AVX_512bit);\n-    evmovdquq(xmm4, Address(src_addr, pos, Address::times_1, 4 * 64), Assembler::AVX_512bit);\n-    evmovdquq(xmm5, Address(src_addr, pos, Address::times_1, 5 * 64), Assembler::AVX_512bit);\n-    evmovdquq(xmm6, Address(src_addr, pos, Address::times_1, 6 * 64), Assembler::AVX_512bit);\n-    evmovdquq(xmm7, Address(src_addr, pos, Address::times_1, 7 * 64), Assembler::AVX_512bit);\n-    \/\/ Xor with the first round key\n-    evpxorq(xmm0, xmm0, xmm9, Assembler::AVX_512bit);\n-    evpxorq(xmm1, xmm1, xmm9, Assembler::AVX_512bit);\n-    evpxorq(xmm2, xmm2, xmm9, Assembler::AVX_512bit);\n-    evpxorq(xmm3, xmm3, xmm9, Assembler::AVX_512bit);\n-    evpxorq(xmm4, xmm4, xmm9, Assembler::AVX_512bit);\n-    evpxorq(xmm5, xmm5, xmm9, Assembler::AVX_512bit);\n-    evpxorq(xmm6, xmm6, xmm9, Assembler::AVX_512bit);\n-    evpxorq(xmm7, xmm7, xmm9, Assembler::AVX_512bit);\n-    \/\/ 9 rounds of Aesdec\n-    roundDec(xmm10, 7);\n-    roundDec(xmm11, 7);\n-    roundDec(xmm12, 7);\n-    roundDec(xmm13, 7);\n-    roundDec(xmm14, 7);\n-    roundDec(xmm15, 7);\n-    roundDec(xmm16, 7);\n-    roundDec(xmm17, 7);\n-    roundDec(xmm18, 7);\n-    cmpl(rounds, 52);\n-    jcc(Assembler::aboveEqual, AES192);\n-    \/\/ Aesdeclast round for keysize = 128\n-    lastroundDec(xmm27, 7);\n-    jmp(END_LOOP);\n-\n-    bind(AES192);\n-    \/\/ 2 Additional rounds for keysize = 192\n-    roundDec(xmm19, 7);\n-    roundDec(xmm20, 7);\n-    cmpl(rounds, 60);\n-    jcc(Assembler::aboveEqual, AES256);\n-    \/\/ Aesdeclast round for keysize = 192\n-    lastroundDec(xmm27, 7);\n-    jmp(END_LOOP);\n-    bind(AES256);\n-    \/\/ 2 Additional rounds and Aesdeclast for keysize = 256\n-    roundDec(xmm21, 7);\n-    roundDec(xmm22, 7);\n-    lastroundDec(xmm27, 7);\n-\n-    bind(END_LOOP);\n-    \/\/ Write 512 bytes of PT to the destination\n-    evmovdquq(Address(dest_addr, pos, Address::times_1, 0 * 64), xmm0, Assembler::AVX_512bit);\n-    evmovdquq(Address(dest_addr, pos, Address::times_1, 1 * 64), xmm1, Assembler::AVX_512bit);\n-    evmovdquq(Address(dest_addr, pos, Address::times_1, 2 * 64), xmm2, Assembler::AVX_512bit);\n-    evmovdquq(Address(dest_addr, pos, Address::times_1, 3 * 64), xmm3, Assembler::AVX_512bit);\n-    evmovdquq(Address(dest_addr, pos, Address::times_1, 4 * 64), xmm4, Assembler::AVX_512bit);\n-    evmovdquq(Address(dest_addr, pos, Address::times_1, 5 * 64), xmm5, Assembler::AVX_512bit);\n-    evmovdquq(Address(dest_addr, pos, Address::times_1, 6 * 64), xmm6, Assembler::AVX_512bit);\n-    evmovdquq(Address(dest_addr, pos, Address::times_1, 7 * 64), xmm7, Assembler::AVX_512bit);\n-\n-    addq(pos, 512);\n-    decq(len);\n-    jcc(Assembler::notEqual, LOOP);\n-\n-    bind(REMAINDER);\n-    vzeroupper();\n-    cmpq(rbx, 0);\n-    jcc(Assembler::equal, END);\n-    \/\/ Process 16 bytes at a time\n-    bind(LOOP2);\n-    movdqu(xmm1, Address(src_addr, pos, Address::times_1, 0));\n-    vpxor(xmm1, xmm1, xmm9, Assembler::AVX_128bit);\n-    \/\/ xmm2 contains shuffled key for Aesdeclast operation.\n-    vmovdqu(xmm2, xmm27);\n-\n-    vaesdec(xmm1, xmm1, xmm10, Assembler::AVX_128bit);\n-    vaesdec(xmm1, xmm1, xmm11, Assembler::AVX_128bit);\n-    vaesdec(xmm1, xmm1, xmm12, Assembler::AVX_128bit);\n-    vaesdec(xmm1, xmm1, xmm13, Assembler::AVX_128bit);\n-    vaesdec(xmm1, xmm1, xmm14, Assembler::AVX_128bit);\n-    vaesdec(xmm1, xmm1, xmm15, Assembler::AVX_128bit);\n-    vaesdec(xmm1, xmm1, xmm16, Assembler::AVX_128bit);\n-    vaesdec(xmm1, xmm1, xmm17, Assembler::AVX_128bit);\n-    vaesdec(xmm1, xmm1, xmm18, Assembler::AVX_128bit);\n-\n-    cmpl(rounds, 52);\n-    jcc(Assembler::below, LAST2);\n-    vaesdec(xmm1, xmm1, xmm19, Assembler::AVX_128bit);\n-    vaesdec(xmm1, xmm1, xmm20, Assembler::AVX_128bit);\n-    cmpl(rounds, 60);\n-    jcc(Assembler::below, LAST2);\n-    vaesdec(xmm1, xmm1, xmm21, Assembler::AVX_128bit);\n-    vaesdec(xmm1, xmm1, xmm22, Assembler::AVX_128bit);\n-\n-    bind(LAST2);\n-    \/\/ Aesdeclast round\n-    vaesdeclast(xmm1, xmm1, xmm2, Assembler::AVX_128bit);\n-    \/\/ Write 16 bytes of PT to destination\n-    movdqu(Address(dest_addr, pos, Address::times_1, 0), xmm1);\n-    addq(pos, 16);\n-    decq(rbx);\n-    jcc(Assembler::notEqual, LOOP2);\n-\n-    bind(END);\n-    \/\/ Zero out the round keys\n-    evpxorq(xmm8, xmm8, xmm8, Assembler::AVX_512bit);\n-    evpxorq(xmm9, xmm9, xmm9, Assembler::AVX_512bit);\n-    evpxorq(xmm10, xmm10, xmm10, Assembler::AVX_512bit);\n-    evpxorq(xmm11, xmm11, xmm11, Assembler::AVX_512bit);\n-    evpxorq(xmm12, xmm12, xmm12, Assembler::AVX_512bit);\n-    evpxorq(xmm13, xmm13, xmm13, Assembler::AVX_512bit);\n-    evpxorq(xmm14, xmm14, xmm14, Assembler::AVX_512bit);\n-    evpxorq(xmm15, xmm15, xmm15, Assembler::AVX_512bit);\n-    evpxorq(xmm16, xmm16, xmm16, Assembler::AVX_512bit);\n-    evpxorq(xmm17, xmm17, xmm17, Assembler::AVX_512bit);\n-    evpxorq(xmm18, xmm18, xmm18, Assembler::AVX_512bit);\n-    evpxorq(xmm27, xmm27, xmm27, Assembler::AVX_512bit);\n-    cmpl(rounds, 44);\n-    jcc(Assembler::belowEqual, EXIT);\n-    evpxorq(xmm19, xmm19, xmm19, Assembler::AVX_512bit);\n-    evpxorq(xmm20, xmm20, xmm20, Assembler::AVX_512bit);\n-    cmpl(rounds, 52);\n-    jcc(Assembler::belowEqual, EXIT);\n-    evpxorq(xmm21, xmm21, xmm21, Assembler::AVX_512bit);\n-    evpxorq(xmm22, xmm22, xmm22, Assembler::AVX_512bit);\n-    bind(EXIT);\n-    pop(rbx);\n-    pop(rax); \/\/ return length\n-    pop(r12);\n-    pop(r13);\n-}\n-\n-\/\/ Multiply 128 x 128 bits, using 4 pclmulqdq operations\n-void MacroAssembler::schoolbookAAD(int i, Register htbl, XMMRegister data,\n-    XMMRegister tmp0, XMMRegister tmp1, XMMRegister tmp2, XMMRegister tmp3) {\n-    movdqu(xmm15, Address(htbl, i * 16));\n-    vpclmulhqlqdq(tmp3, data, xmm15); \/\/ 0x01\n-    vpxor(tmp2, tmp2, tmp3, Assembler::AVX_128bit);\n-    vpclmulldq(tmp3, data, xmm15); \/\/ 0x00\n-    vpxor(tmp0, tmp0, tmp3, Assembler::AVX_128bit);\n-    vpclmulhdq(tmp3, data, xmm15); \/\/ 0x11\n-    vpxor(tmp1, tmp1, tmp3, Assembler::AVX_128bit);\n-    vpclmullqhqdq(tmp3, data, xmm15); \/\/ 0x10\n-    vpxor(tmp2, tmp2, tmp3, Assembler::AVX_128bit);\n-}\n-\n-\/\/ Multiply two 128 bit numbers resulting in a 256 bit value\n-\/\/ Result of the multiplication followed by reduction stored in state\n-void MacroAssembler::gfmul(XMMRegister tmp0, XMMRegister state) {\n-    const XMMRegister tmp1 = xmm4;\n-    const XMMRegister tmp2 = xmm5;\n-    const XMMRegister tmp3 = xmm6;\n-    const XMMRegister tmp4 = xmm7;\n-\n-    vpclmulldq(tmp1, state, tmp0); \/\/0x00  (a0 * b0)\n-    vpclmulhdq(tmp4, state, tmp0);\/\/0x11 (a1 * b1)\n-    vpclmullqhqdq(tmp2, state, tmp0);\/\/0x10 (a1 * b0)\n-    vpclmulhqlqdq(tmp3, state, tmp0); \/\/0x01 (a0 * b1)\n-\n-    vpxor(tmp2, tmp2, tmp3, Assembler::AVX_128bit); \/\/ (a0 * b1) + (a1 * b0)\n-\n-    vpslldq(tmp3, tmp2, 8, Assembler::AVX_128bit);\n-    vpsrldq(tmp2, tmp2, 8, Assembler::AVX_128bit);\n-    vpxor(tmp1, tmp1, tmp3, Assembler::AVX_128bit); \/\/ tmp1 and tmp4 hold the result\n-    vpxor(tmp4, tmp4, tmp2, Assembler::AVX_128bit); \/\/ of carryless multiplication\n-    \/\/ Follows the reduction technique mentioned in\n-    \/\/ Shift-XOR reduction described in Gueron-Kounavis May 2010\n-    \/\/ First phase of reduction\n-    \/\/\n-    vpslld(xmm8, tmp1, 31, Assembler::AVX_128bit); \/\/ packed right shift shifting << 31\n-    vpslld(xmm9, tmp1, 30, Assembler::AVX_128bit); \/\/ packed right shift shifting << 30\n-    vpslld(xmm10, tmp1, 25, Assembler::AVX_128bit);\/\/ packed right shift shifting << 25\n-    \/\/ xor the shifted versions\n-    vpxor(xmm8, xmm8, xmm9, Assembler::AVX_128bit);\n-    vpxor(xmm8, xmm8, xmm10, Assembler::AVX_128bit);\n-    vpslldq(xmm9, xmm8, 12, Assembler::AVX_128bit);\n-    vpsrldq(xmm8, xmm8, 4, Assembler::AVX_128bit);\n-    vpxor(tmp1, tmp1, xmm9, Assembler::AVX_128bit);\/\/ first phase of the reduction complete\n-    \/\/\n-    \/\/ Second phase of the reduction\n-    \/\/\n-    vpsrld(xmm9, tmp1, 1, Assembler::AVX_128bit);\/\/ packed left shifting >> 1\n-    vpsrld(xmm10, tmp1, 2, Assembler::AVX_128bit);\/\/ packed left shifting >> 2\n-    vpsrld(xmm11, tmp1, 7, Assembler::AVX_128bit);\/\/ packed left shifting >> 7\n-    vpxor(xmm9, xmm9, xmm10, Assembler::AVX_128bit);\/\/ xor the shifted versions\n-    vpxor(xmm9, xmm9, xmm11, Assembler::AVX_128bit);\n-    vpxor(xmm9, xmm9, xmm8, Assembler::AVX_128bit);\n-    vpxor(tmp1, tmp1, xmm9, Assembler::AVX_128bit);\n-    vpxor(state, tmp4, tmp1, Assembler::AVX_128bit);\/\/ the result is in state\n-    ret(0);\n-}\n-\n-\/\/ This method takes the subkey after expansion as input and generates 1 * 16 power of subkey H.\n-\/\/ The power of H is used in reduction process for one block ghash\n-void MacroAssembler::generateHtbl_one_block(Register htbl) {\n-    const XMMRegister t = xmm13;\n-\n-    \/\/ load the original subkey hash\n-    movdqu(t, Address(htbl, 0));\n-    \/\/ shuffle using long swap mask\n-    movdqu(xmm10, ExternalAddress(StubRoutines::x86::ghash_long_swap_mask_addr()));\n-    vpshufb(t, t, xmm10, Assembler::AVX_128bit);\n-\n-    \/\/ Compute H' = GFMUL(H, 2)\n-    vpsrld(xmm3, t, 7, Assembler::AVX_128bit);\n-    movdqu(xmm4, ExternalAddress(StubRoutines::x86::ghash_shufflemask_addr()));\n-    vpshufb(xmm3, xmm3, xmm4, Assembler::AVX_128bit);\n-    movl(rax, 0xff00);\n-    movdl(xmm4, rax);\n-    vpshufb(xmm4, xmm4, xmm3, Assembler::AVX_128bit);\n-    movdqu(xmm5, ExternalAddress(StubRoutines::x86::ghash_polynomial_addr()));\n-    vpand(xmm5, xmm5, xmm4, Assembler::AVX_128bit);\n-    vpsrld(xmm3, t, 31, Assembler::AVX_128bit);\n-    vpslld(xmm4, t, 1, Assembler::AVX_128bit);\n-    vpslldq(xmm3, xmm3, 4, Assembler::AVX_128bit);\n-    vpxor(t, xmm4, xmm3, Assembler::AVX_128bit);\/\/ t holds p(x) <<1 or H * 2\n-\n-    \/\/Adding p(x)<<1 to xmm5 which holds the reduction polynomial\n-    vpxor(t, t, xmm5, Assembler::AVX_128bit);\n-    movdqu(Address(htbl, 1 * 16), t); \/\/ H * 2\n-\n-    ret(0);\n-}\n-\n-\/\/ This method takes the subkey after expansion as input and generates the remaining powers of subkey H.\n-\/\/ The power of H is used in reduction process for eight block ghash\n-void MacroAssembler::generateHtbl_eight_blocks(Register htbl) {\n-    const XMMRegister t = xmm13;\n-    const XMMRegister tmp0 = xmm1;\n-    Label GFMUL;\n-\n-    movdqu(t, Address(htbl, 1 * 16));\n-    movdqu(tmp0, t);\n-\n-    \/\/ tmp0 and t hold H. Now we compute powers of H by using GFMUL(H, H)\n-    call(GFMUL, relocInfo::none);\n-    movdqu(Address(htbl, 2 * 16), t); \/\/H ^ 2 * 2\n-    call(GFMUL, relocInfo::none);\n-    movdqu(Address(htbl, 3 * 16), t); \/\/H ^ 3 * 2\n-    call(GFMUL, relocInfo::none);\n-    movdqu(Address(htbl, 4 * 16), t); \/\/H ^ 4 * 2\n-    call(GFMUL, relocInfo::none);\n-    movdqu(Address(htbl, 5 * 16), t); \/\/H ^ 5 * 2\n-    call(GFMUL, relocInfo::none);\n-    movdqu(Address(htbl, 6 * 16), t); \/\/H ^ 6 * 2\n-    call(GFMUL, relocInfo::none);\n-    movdqu(Address(htbl, 7 * 16), t); \/\/H ^ 7 * 2\n-    call(GFMUL, relocInfo::none);\n-    movdqu(Address(htbl, 8 * 16), t); \/\/H ^ 8 * 2\n-    ret(0);\n-\n-    bind(GFMUL);\n-    gfmul(tmp0, t);\n-}\n-\n-\/\/ Multiblock and single block GHASH computation using Shift XOR reduction technique\n-void MacroAssembler::avx_ghash(Register input_state, Register htbl,\n-    Register input_data, Register blocks) {\n-\n-    \/\/ temporary variables to hold input data and input state\n-    const XMMRegister data = xmm1;\n-    const XMMRegister state = xmm0;\n-    \/\/ temporary variables to hold intermediate results\n-    const XMMRegister tmp0 = xmm3;\n-    const XMMRegister tmp1 = xmm4;\n-    const XMMRegister tmp2 = xmm5;\n-    const XMMRegister tmp3 = xmm6;\n-    \/\/ temporary variables to hold byte and long swap masks\n-    const XMMRegister bswap_mask = xmm2;\n-    const XMMRegister lswap_mask = xmm14;\n-\n-    Label GENERATE_HTBL_1_BLK, GENERATE_HTBL_8_BLKS, BEGIN_PROCESS, GFMUL, BLOCK8_REDUCTION,\n-          ONE_BLK_INIT, PROCESS_1_BLOCK, PROCESS_8_BLOCKS, SAVE_STATE, EXIT_GHASH;\n-\n-    testptr(blocks, blocks);\n-    jcc(Assembler::zero, EXIT_GHASH);\n-\n-    \/\/ Check if Hashtable (1*16) has been already generated\n-    \/\/ For anything less than 8 blocks, we generate only the first power of H.\n-    movdqu(tmp2, Address(htbl, 1 * 16));\n-    ptest(tmp2, tmp2);\n-    jcc(Assembler::notZero, BEGIN_PROCESS);\n-    call(GENERATE_HTBL_1_BLK, relocInfo::none);\n-\n-    \/\/ Shuffle the input state\n-    bind(BEGIN_PROCESS);\n-    movdqu(lswap_mask, ExternalAddress(StubRoutines::x86::ghash_long_swap_mask_addr()));\n-    movdqu(state, Address(input_state, 0));\n-    vpshufb(state, state, lswap_mask, Assembler::AVX_128bit);\n-\n-    cmpl(blocks, 8);\n-    jcc(Assembler::below, ONE_BLK_INIT);\n-    \/\/ If we have 8 blocks or more data, then generate remaining powers of H\n-    movdqu(tmp2, Address(htbl, 8 * 16));\n-    ptest(tmp2, tmp2);\n-    jcc(Assembler::notZero, PROCESS_8_BLOCKS);\n-    call(GENERATE_HTBL_8_BLKS, relocInfo::none);\n-\n-    \/\/Do 8 multiplies followed by a reduction processing 8 blocks of data at a time\n-    \/\/Each block = 16 bytes.\n-    bind(PROCESS_8_BLOCKS);\n-    subl(blocks, 8);\n-    movdqu(bswap_mask, ExternalAddress(StubRoutines::x86::ghash_byte_swap_mask_addr()));\n-    movdqu(data, Address(input_data, 16 * 7));\n-    vpshufb(data, data, bswap_mask, Assembler::AVX_128bit);\n-    \/\/Loading 1*16 as calculated powers of H required starts at that location.\n-    movdqu(xmm15, Address(htbl, 1 * 16));\n-    \/\/Perform carryless multiplication of (H*2, data block #7)\n-    vpclmulhqlqdq(tmp2, data, xmm15);\/\/a0 * b1\n-    vpclmulldq(tmp0, data, xmm15);\/\/a0 * b0\n-    vpclmulhdq(tmp1, data, xmm15);\/\/a1 * b1\n-    vpclmullqhqdq(tmp3, data, xmm15);\/\/a1* b0\n-    vpxor(tmp2, tmp2, tmp3, Assembler::AVX_128bit);\/\/ (a0 * b1) + (a1 * b0)\n-\n-    movdqu(data, Address(input_data, 16 * 6));\n-    vpshufb(data, data, bswap_mask, Assembler::AVX_128bit);\n-    \/\/ Perform carryless multiplication of (H^2 * 2, data block #6)\n-    schoolbookAAD(2, htbl, data, tmp0, tmp1, tmp2, tmp3);\n-\n-    movdqu(data, Address(input_data, 16 * 5));\n-    vpshufb(data, data, bswap_mask, Assembler::AVX_128bit);\n-    \/\/ Perform carryless multiplication of (H^3 * 2, data block #5)\n-    schoolbookAAD(3, htbl, data, tmp0, tmp1, tmp2, tmp3);\n-    movdqu(data, Address(input_data, 16 * 4));\n-    vpshufb(data, data, bswap_mask, Assembler::AVX_128bit);\n-    \/\/ Perform carryless multiplication of (H^4 * 2, data block #4)\n-    schoolbookAAD(4, htbl, data, tmp0, tmp1, tmp2, tmp3);\n-    movdqu(data, Address(input_data, 16 * 3));\n-    vpshufb(data, data, bswap_mask, Assembler::AVX_128bit);\n-    \/\/ Perform carryless multiplication of (H^5 * 2, data block #3)\n-    schoolbookAAD(5, htbl, data, tmp0, tmp1, tmp2, tmp3);\n-    movdqu(data, Address(input_data, 16 * 2));\n-    vpshufb(data, data, bswap_mask, Assembler::AVX_128bit);\n-    \/\/ Perform carryless multiplication of (H^6 * 2, data block #2)\n-    schoolbookAAD(6, htbl, data, tmp0, tmp1, tmp2, tmp3);\n-    movdqu(data, Address(input_data, 16 * 1));\n-    vpshufb(data, data, bswap_mask, Assembler::AVX_128bit);\n-    \/\/ Perform carryless multiplication of (H^7 * 2, data block #1)\n-    schoolbookAAD(7, htbl, data, tmp0, tmp1, tmp2, tmp3);\n-    movdqu(data, Address(input_data, 16 * 0));\n-    \/\/ xor data block#0 with input state before performing carry-less multiplication\n-    vpshufb(data, data, bswap_mask, Assembler::AVX_128bit);\n-    vpxor(data, data, state, Assembler::AVX_128bit);\n-    \/\/ Perform carryless multiplication of (H^8 * 2, data block #0)\n-    schoolbookAAD(8, htbl, data, tmp0, tmp1, tmp2, tmp3);\n-    vpslldq(tmp3, tmp2, 8, Assembler::AVX_128bit);\n-    vpsrldq(tmp2, tmp2, 8, Assembler::AVX_128bit);\n-    vpxor(tmp0, tmp0, tmp3, Assembler::AVX_128bit);\/\/ tmp0, tmp1 contains aggregated results of\n-    vpxor(tmp1, tmp1, tmp2, Assembler::AVX_128bit);\/\/ the multiplication operation\n-\n-    \/\/ we have the 2 128-bit partially accumulated multiplication results in tmp0:tmp1\n-    \/\/ with higher 128-bit in tmp1 and lower 128-bit in corresponding tmp0\n-    \/\/ Follows the reduction technique mentioned in\n-    \/\/ Shift-XOR reduction described in Gueron-Kounavis May 2010\n-    bind(BLOCK8_REDUCTION);\n-    \/\/ First Phase of the reduction\n-    vpslld(xmm8, tmp0, 31, Assembler::AVX_128bit); \/\/ packed right shifting << 31\n-    vpslld(xmm9, tmp0, 30, Assembler::AVX_128bit); \/\/ packed right shifting << 30\n-    vpslld(xmm10, tmp0, 25, Assembler::AVX_128bit); \/\/ packed right shifting << 25\n-    \/\/ xor the shifted versions\n-    vpxor(xmm8, xmm8, xmm10, Assembler::AVX_128bit);\n-    vpxor(xmm8, xmm8, xmm9, Assembler::AVX_128bit);\n-\n-    vpslldq(xmm9, xmm8, 12, Assembler::AVX_128bit);\n-    vpsrldq(xmm8, xmm8, 4, Assembler::AVX_128bit);\n-\n-    vpxor(tmp0, tmp0, xmm9, Assembler::AVX_128bit); \/\/ first phase of reduction is complete\n-    \/\/ second phase of the reduction\n-    vpsrld(xmm9, tmp0, 1, Assembler::AVX_128bit); \/\/ packed left shifting >> 1\n-    vpsrld(xmm10, tmp0, 2, Assembler::AVX_128bit); \/\/ packed left shifting >> 2\n-    vpsrld(tmp2, tmp0, 7, Assembler::AVX_128bit); \/\/ packed left shifting >> 7\n-    \/\/ xor the shifted versions\n-    vpxor(xmm9, xmm9, xmm10, Assembler::AVX_128bit);\n-    vpxor(xmm9, xmm9, tmp2, Assembler::AVX_128bit);\n-    vpxor(xmm9, xmm9, xmm8, Assembler::AVX_128bit);\n-    vpxor(tmp0, xmm9, tmp0, Assembler::AVX_128bit);\n-    \/\/ Final result is in state\n-    vpxor(state, tmp0, tmp1, Assembler::AVX_128bit);\n-\n-    lea(input_data, Address(input_data, 16 * 8));\n-    cmpl(blocks, 8);\n-    jcc(Assembler::below, ONE_BLK_INIT);\n-    jmp(PROCESS_8_BLOCKS);\n-\n-    \/\/ Since this is one block operation we will only use H * 2 i.e. the first power of H\n-    bind(ONE_BLK_INIT);\n-    movdqu(tmp0, Address(htbl, 1 * 16));\n-    movdqu(bswap_mask, ExternalAddress(StubRoutines::x86::ghash_byte_swap_mask_addr()));\n-\n-    \/\/Do one (128 bit x 128 bit) carry-less multiplication at a time followed by a reduction.\n-    bind(PROCESS_1_BLOCK);\n-    cmpl(blocks, 0);\n-    jcc(Assembler::equal, SAVE_STATE);\n-    subl(blocks, 1);\n-    movdqu(data, Address(input_data, 0));\n-    vpshufb(data, data, bswap_mask, Assembler::AVX_128bit);\n-    vpxor(state, state, data, Assembler::AVX_128bit);\n-    \/\/ gfmul(H*2, state)\n-    call(GFMUL, relocInfo::none);\n-    addptr(input_data, 16);\n-    jmp(PROCESS_1_BLOCK);\n-\n-    bind(SAVE_STATE);\n-    vpshufb(state, state, lswap_mask, Assembler::AVX_128bit);\n-    movdqu(Address(input_state, 0), state);\n-    jmp(EXIT_GHASH);\n-\n-    bind(GFMUL);\n-    gfmul(tmp0, state);\n-\n-    bind(GENERATE_HTBL_1_BLK);\n-    generateHtbl_one_block(htbl);\n-\n-    bind(GENERATE_HTBL_8_BLKS);\n-    generateHtbl_eight_blocks(htbl);\n-\n-    bind(EXIT_GHASH);\n-    \/\/ zero out xmm registers used for Htbl storage\n-    vpxor(xmm0, xmm0, xmm0, Assembler::AVX_128bit);\n-    vpxor(xmm1, xmm1, xmm1, Assembler::AVX_128bit);\n-    vpxor(xmm3, xmm3, xmm3, Assembler::AVX_128bit);\n-    vpxor(xmm15, xmm15, xmm15, Assembler::AVX_128bit);\n-}\n-\n-\/\/ AES Counter Mode using VAES instructions\n-void MacroAssembler::aesctr_encrypt(Register src_addr, Register dest_addr, Register key, Register counter,\n-    Register len_reg, Register used, Register used_addr, Register saved_encCounter_start) {\n-\n-    const Register rounds = rax;\n-    const Register pos = r12;\n-\n-    Label PRELOOP_START, EXIT_PRELOOP, REMAINDER, REMAINDER_16, LOOP, END, EXIT, END_LOOP,\n-    AES192, AES256, AES192_REMAINDER16, REMAINDER16_END_LOOP, AES256_REMAINDER16,\n-    REMAINDER_8, REMAINDER_4, AES192_REMAINDER8, REMAINDER_LOOP, AES256_REMINDER,\n-    AES192_REMAINDER, END_REMAINDER_LOOP, AES256_REMAINDER8, REMAINDER8_END_LOOP,\n-    AES192_REMAINDER4, AES256_REMAINDER4, AES256_REMAINDER, END_REMAINDER4, EXTRACT_TAILBYTES,\n-    EXTRACT_TAIL_4BYTES, EXTRACT_TAIL_2BYTES, EXTRACT_TAIL_1BYTE, STORE_CTR;\n-\n-    cmpl(len_reg, 0);\n-    jcc(Assembler::belowEqual, EXIT);\n-\n-    movl(pos, 0);\n-    \/\/ if the number of used encrypted counter bytes < 16,\n-    \/\/ XOR PT with saved encrypted counter to obtain CT\n-    bind(PRELOOP_START);\n-    cmpl(used, 16);\n-    jcc(Assembler::aboveEqual, EXIT_PRELOOP);\n-    movb(rbx, Address(saved_encCounter_start, used));\n-    xorb(rbx, Address(src_addr, pos));\n-    movb(Address(dest_addr, pos), rbx);\n-    addptr(pos, 1);\n-    addptr(used, 1);\n-    decrement(len_reg);\n-    jmp(PRELOOP_START);\n-\n-    bind(EXIT_PRELOOP);\n-    movl(Address(used_addr, 0), used);\n-\n-    \/\/ Calculate number of rounds i.e. 10, 12, 14,  based on key length(128, 192, 256).\n-    movl(rounds, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));\n-\n-    vpxor(xmm0, xmm0, xmm0, Assembler::AVX_128bit);\n-    \/\/ Move initial counter value in xmm0\n-    movdqu(xmm0, Address(counter, 0));\n-    \/\/ broadcast counter value to zmm8\n-    evshufi64x2(xmm8, xmm0, xmm0, 0, Assembler::AVX_512bit);\n-\n-    \/\/ load lbswap mask\n-    evmovdquq(xmm16, ExternalAddress(StubRoutines::x86::counter_mask_addr()), Assembler::AVX_512bit, r15);\n-\n-    \/\/shuffle counter using lbswap_mask\n-    vpshufb(xmm8, xmm8, xmm16, Assembler::AVX_512bit);\n-\n-    \/\/ pre-increment and propagate counter values to zmm9-zmm15 registers.\n-    \/\/ Linc0 increments the zmm8 by 1 (initial value being 0), Linc4 increments the counters zmm9-zmm15 by 4\n-    \/\/ The counter is incremented after each block i.e. 16 bytes is processed;\n-    \/\/ each zmm register has 4 counter values as its MSB\n-    \/\/ the counters are incremented in parallel\n-    vpaddd(xmm8, xmm8, ExternalAddress(StubRoutines::x86::counter_mask_addr() + 64), Assembler::AVX_512bit, r15);\/\/linc0\n-    vpaddd(xmm9, xmm8, ExternalAddress(StubRoutines::x86::counter_mask_addr() + 128), Assembler::AVX_512bit, r15);\/\/linc4(rip)\n-    vpaddd(xmm10, xmm9, ExternalAddress(StubRoutines::x86::counter_mask_addr() + 128), Assembler::AVX_512bit, r15);\/\/Linc4(rip)\n-    vpaddd(xmm11, xmm10, ExternalAddress(StubRoutines::x86::counter_mask_addr() + 128), Assembler::AVX_512bit, r15);\/\/Linc4(rip)\n-    vpaddd(xmm12, xmm11, ExternalAddress(StubRoutines::x86::counter_mask_addr() + 128), Assembler::AVX_512bit, r15);\/\/Linc4(rip)\n-    vpaddd(xmm13, xmm12, ExternalAddress(StubRoutines::x86::counter_mask_addr() + 128), Assembler::AVX_512bit, r15);\/\/Linc4(rip)\n-    vpaddd(xmm14, xmm13, ExternalAddress(StubRoutines::x86::counter_mask_addr() + 128), Assembler::AVX_512bit, r15);\/\/Linc4(rip)\n-    vpaddd(xmm15, xmm14, ExternalAddress(StubRoutines::x86::counter_mask_addr() + 128), Assembler::AVX_512bit, r15);\/\/Linc4(rip)\n-\n-    \/\/ load linc32 mask in zmm register.linc32 increments counter by 32\n-    evmovdquq(xmm19, ExternalAddress(StubRoutines::x86::counter_mask_addr() + 256), Assembler::AVX_512bit, r15);\/\/Linc32\n-\n-    \/\/ xmm31 contains the key shuffle mask.\n-    movdqu(xmm31, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()));\n-    \/\/ Load key function loads 128 bit key and shuffles it. Then we broadcast the shuffled key to convert it into a 512 bit value.\n-    \/\/ For broadcasting the values to ZMM, vshufi64 is used instead of evbroadcasti64x2 as the source in this case is ZMM register\n-    \/\/ that holds shuffled key value.\n-    ev_load_key(xmm20, key, 0, xmm31);\n-    ev_load_key(xmm21, key, 1 * 16, xmm31);\n-    ev_load_key(xmm22, key, 2 * 16, xmm31);\n-    ev_load_key(xmm23, key, 3 * 16, xmm31);\n-    ev_load_key(xmm24, key, 4 * 16, xmm31);\n-    ev_load_key(xmm25, key, 5 * 16, xmm31);\n-    ev_load_key(xmm26, key, 6 * 16, xmm31);\n-    ev_load_key(xmm27, key, 7 * 16, xmm31);\n-    ev_load_key(xmm28, key, 8 * 16, xmm31);\n-    ev_load_key(xmm29, key, 9 * 16, xmm31);\n-    ev_load_key(xmm30, key, 10 * 16, xmm31);\n-\n-    \/\/ Process 32 blocks or 512 bytes of data\n-    bind(LOOP);\n-    cmpl(len_reg, 512);\n-    jcc(Assembler::less, REMAINDER);\n-    subq(len_reg, 512);\n-    \/\/Shuffle counter and Exor it with roundkey1. Result is stored in zmm0-7\n-    vpshufb(xmm0, xmm8, xmm16, Assembler::AVX_512bit);\n-    evpxorq(xmm0, xmm0, xmm20, Assembler::AVX_512bit);\n-    vpshufb(xmm1, xmm9, xmm16, Assembler::AVX_512bit);\n-    evpxorq(xmm1, xmm1, xmm20, Assembler::AVX_512bit);\n-    vpshufb(xmm2, xmm10, xmm16, Assembler::AVX_512bit);\n-    evpxorq(xmm2, xmm2, xmm20, Assembler::AVX_512bit);\n-    vpshufb(xmm3, xmm11, xmm16, Assembler::AVX_512bit);\n-    evpxorq(xmm3, xmm3, xmm20, Assembler::AVX_512bit);\n-    vpshufb(xmm4, xmm12, xmm16, Assembler::AVX_512bit);\n-    evpxorq(xmm4, xmm4, xmm20, Assembler::AVX_512bit);\n-    vpshufb(xmm5, xmm13, xmm16, Assembler::AVX_512bit);\n-    evpxorq(xmm5, xmm5, xmm20, Assembler::AVX_512bit);\n-    vpshufb(xmm6, xmm14, xmm16, Assembler::AVX_512bit);\n-    evpxorq(xmm6, xmm6, xmm20, Assembler::AVX_512bit);\n-    vpshufb(xmm7, xmm15, xmm16, Assembler::AVX_512bit);\n-    evpxorq(xmm7, xmm7, xmm20, Assembler::AVX_512bit);\n-    \/\/ Perform AES encode operations and put results in zmm0-zmm7.\n-    \/\/ This is followed by incrementing counter values in zmm8-zmm15.\n-    \/\/ Since we will be processing 32 blocks at a time, the counter is incremented by 32.\n-    roundEnc(xmm21, 7);\n-    vpaddq(xmm8, xmm8, xmm19, Assembler::AVX_512bit);\n-    roundEnc(xmm22, 7);\n-    vpaddq(xmm9, xmm9, xmm19, Assembler::AVX_512bit);\n-    roundEnc(xmm23, 7);\n-    vpaddq(xmm10, xmm10, xmm19, Assembler::AVX_512bit);\n-    roundEnc(xmm24, 7);\n-    vpaddq(xmm11, xmm11, xmm19, Assembler::AVX_512bit);\n-    roundEnc(xmm25, 7);\n-    vpaddq(xmm12, xmm12, xmm19, Assembler::AVX_512bit);\n-    roundEnc(xmm26, 7);\n-    vpaddq(xmm13, xmm13, xmm19, Assembler::AVX_512bit);\n-    roundEnc(xmm27, 7);\n-    vpaddq(xmm14, xmm14, xmm19, Assembler::AVX_512bit);\n-    roundEnc(xmm28, 7);\n-    vpaddq(xmm15, xmm15, xmm19, Assembler::AVX_512bit);\n-    roundEnc(xmm29, 7);\n-\n-    cmpl(rounds, 52);\n-    jcc(Assembler::aboveEqual, AES192);\n-    lastroundEnc(xmm30, 7);\n-    jmp(END_LOOP);\n-\n-    bind(AES192);\n-    roundEnc(xmm30, 7);\n-    ev_load_key(xmm18, key, 11 * 16, xmm31);\n-    roundEnc(xmm18, 7);\n-    cmpl(rounds, 60);\n-    jcc(Assembler::aboveEqual, AES256);\n-    ev_load_key(xmm18, key, 12 * 16, xmm31);\n-    lastroundEnc(xmm18, 7);\n-    jmp(END_LOOP);\n-\n-    bind(AES256);\n-    ev_load_key(xmm18, key, 12 * 16, xmm31);\n-    roundEnc(xmm18, 7);\n-    ev_load_key(xmm18, key, 13 * 16, xmm31);\n-    roundEnc(xmm18, 7);\n-    ev_load_key(xmm18, key, 14 * 16, xmm31);\n-    lastroundEnc(xmm18, 7);\n-\n-    \/\/ After AES encode rounds, the encrypted block cipher lies in zmm0-zmm7\n-    \/\/ xor encrypted block cipher and input plaintext and store resultant ciphertext\n-    bind(END_LOOP);\n-    evpxorq(xmm0, xmm0, Address(src_addr, pos, Address::times_1, 0 * 64), Assembler::AVX_512bit);\n-    evmovdquq(Address(dest_addr, pos, Address::times_1, 0), xmm0, Assembler::AVX_512bit);\n-    evpxorq(xmm1, xmm1, Address(src_addr, pos, Address::times_1, 1 * 64), Assembler::AVX_512bit);\n-    evmovdquq(Address(dest_addr, pos, Address::times_1, 64), xmm1, Assembler::AVX_512bit);\n-    evpxorq(xmm2, xmm2, Address(src_addr, pos, Address::times_1, 2 * 64), Assembler::AVX_512bit);\n-    evmovdquq(Address(dest_addr, pos, Address::times_1, 2 * 64), xmm2, Assembler::AVX_512bit);\n-    evpxorq(xmm3, xmm3, Address(src_addr, pos, Address::times_1, 3 * 64), Assembler::AVX_512bit);\n-    evmovdquq(Address(dest_addr, pos, Address::times_1, 3 * 64), xmm3, Assembler::AVX_512bit);\n-    evpxorq(xmm4, xmm4, Address(src_addr, pos, Address::times_1, 4 * 64), Assembler::AVX_512bit);\n-    evmovdquq(Address(dest_addr, pos, Address::times_1, 4 * 64), xmm4, Assembler::AVX_512bit);\n-    evpxorq(xmm5, xmm5, Address(src_addr, pos, Address::times_1, 5 * 64), Assembler::AVX_512bit);\n-    evmovdquq(Address(dest_addr, pos, Address::times_1, 5 * 64), xmm5, Assembler::AVX_512bit);\n-    evpxorq(xmm6, xmm6, Address(src_addr, pos, Address::times_1, 6 * 64), Assembler::AVX_512bit);\n-    evmovdquq(Address(dest_addr, pos, Address::times_1, 6 * 64), xmm6, Assembler::AVX_512bit);\n-    evpxorq(xmm7, xmm7, Address(src_addr, pos, Address::times_1, 7 * 64), Assembler::AVX_512bit);\n-    evmovdquq(Address(dest_addr, pos, Address::times_1, 7 * 64), xmm7, Assembler::AVX_512bit);\n-    addq(pos, 512);\n-    jmp(LOOP);\n-\n-    \/\/ Encode 256, 128, 64 or 16 bytes at a time if length is less than 512 bytes\n-    bind(REMAINDER);\n-    cmpl(len_reg, 0);\n-    jcc(Assembler::equal, END);\n-    cmpl(len_reg, 256);\n-    jcc(Assembler::aboveEqual, REMAINDER_16);\n-    cmpl(len_reg, 128);\n-    jcc(Assembler::aboveEqual, REMAINDER_8);\n-    cmpl(len_reg, 64);\n-    jcc(Assembler::aboveEqual, REMAINDER_4);\n-    \/\/ At this point, we will process 16 bytes of data at a time.\n-    \/\/ So load xmm19 with counter increment value as 1\n-    evmovdquq(xmm19, ExternalAddress(StubRoutines::x86::counter_mask_addr() + 80), Assembler::AVX_128bit, r15);\n-    jmp(REMAINDER_LOOP);\n-\n-    \/\/ Each ZMM register can be used to encode 64 bytes of data, so we have 4 ZMM registers to encode 256 bytes of data\n-    bind(REMAINDER_16);\n-    subq(len_reg, 256);\n-    \/\/ As we process 16 blocks at a time, load mask for incrementing the counter value by 16\n-    evmovdquq(xmm19, ExternalAddress(StubRoutines::x86::counter_mask_addr() + 320), Assembler::AVX_512bit, r15);\/\/Linc16(rip)\n-    \/\/ shuffle counter and XOR counter with roundkey1\n-    vpshufb(xmm0, xmm8, xmm16, Assembler::AVX_512bit);\n-    evpxorq(xmm0, xmm0, xmm20, Assembler::AVX_512bit);\n-    vpshufb(xmm1, xmm9, xmm16, Assembler::AVX_512bit);\n-    evpxorq(xmm1, xmm1, xmm20, Assembler::AVX_512bit);\n-    vpshufb(xmm2, xmm10, xmm16, Assembler::AVX_512bit);\n-    evpxorq(xmm2, xmm2, xmm20, Assembler::AVX_512bit);\n-    vpshufb(xmm3, xmm11, xmm16, Assembler::AVX_512bit);\n-    evpxorq(xmm3, xmm3, xmm20, Assembler::AVX_512bit);\n-    \/\/ Increment counter values by 16\n-    vpaddq(xmm8, xmm8, xmm19, Assembler::AVX_512bit);\n-    vpaddq(xmm9, xmm9, xmm19, Assembler::AVX_512bit);\n-    \/\/ AES encode rounds\n-    roundEnc(xmm21, 3);\n-    roundEnc(xmm22, 3);\n-    roundEnc(xmm23, 3);\n-    roundEnc(xmm24, 3);\n-    roundEnc(xmm25, 3);\n-    roundEnc(xmm26, 3);\n-    roundEnc(xmm27, 3);\n-    roundEnc(xmm28, 3);\n-    roundEnc(xmm29, 3);\n-\n-    cmpl(rounds, 52);\n-    jcc(Assembler::aboveEqual, AES192_REMAINDER16);\n-    lastroundEnc(xmm30, 3);\n-    jmp(REMAINDER16_END_LOOP);\n-\n-    bind(AES192_REMAINDER16);\n-    roundEnc(xmm30, 3);\n-    ev_load_key(xmm18, key, 11 * 16, xmm31);\n-    roundEnc(xmm18, 3);\n-    ev_load_key(xmm5, key, 12 * 16, xmm31);\n-\n-    cmpl(rounds, 60);\n-    jcc(Assembler::aboveEqual, AES256_REMAINDER16);\n-    lastroundEnc(xmm5, 3);\n-    jmp(REMAINDER16_END_LOOP);\n-    bind(AES256_REMAINDER16);\n-    roundEnc(xmm5, 3);\n-    ev_load_key(xmm6, key, 13 * 16, xmm31);\n-    roundEnc(xmm6, 3);\n-    ev_load_key(xmm7, key, 14 * 16, xmm31);\n-    lastroundEnc(xmm7, 3);\n-\n-    \/\/ After AES encode rounds, the encrypted block cipher lies in zmm0-zmm3\n-    \/\/ xor 256 bytes of PT with the encrypted counters to produce CT.\n-    bind(REMAINDER16_END_LOOP);\n-    evpxorq(xmm0, xmm0, Address(src_addr, pos, Address::times_1, 0), Assembler::AVX_512bit);\n-    evmovdquq(Address(dest_addr, pos, Address::times_1, 0), xmm0, Assembler::AVX_512bit);\n-    evpxorq(xmm1, xmm1, Address(src_addr, pos, Address::times_1, 1 * 64), Assembler::AVX_512bit);\n-    evmovdquq(Address(dest_addr, pos, Address::times_1, 1 * 64), xmm1, Assembler::AVX_512bit);\n-    evpxorq(xmm2, xmm2, Address(src_addr, pos, Address::times_1, 2 * 64), Assembler::AVX_512bit);\n-    evmovdquq(Address(dest_addr, pos, Address::times_1, 2 * 64), xmm2, Assembler::AVX_512bit);\n-    evpxorq(xmm3, xmm3, Address(src_addr, pos, Address::times_1, 3 * 64), Assembler::AVX_512bit);\n-    evmovdquq(Address(dest_addr, pos, Address::times_1, 3 * 64), xmm3, Assembler::AVX_512bit);\n-    addq(pos, 256);\n-\n-    cmpl(len_reg, 128);\n-    jcc(Assembler::aboveEqual, REMAINDER_8);\n-\n-    cmpl(len_reg, 64);\n-    jcc(Assembler::aboveEqual, REMAINDER_4);\n-    \/\/load mask for incrementing the counter value by 1\n-    evmovdquq(xmm19, ExternalAddress(StubRoutines::x86::counter_mask_addr() + 80), Assembler::AVX_128bit, r15);\/\/Linc0 + 16(rip)\n-    jmp(REMAINDER_LOOP);\n-\n-    \/\/ Each ZMM register can be used to encode 64 bytes of data, so we have 2 ZMM registers to encode 128 bytes of data\n-    bind(REMAINDER_8);\n-    subq(len_reg, 128);\n-    \/\/ As we process 8 blocks at a time, load mask for incrementing the counter value by 8\n-    evmovdquq(xmm19, ExternalAddress(StubRoutines::x86::counter_mask_addr() + 192), Assembler::AVX_512bit, r15);\/\/Linc8(rip)\n-    \/\/ shuffle counters and xor with roundkey1\n-    vpshufb(xmm0, xmm8, xmm16, Assembler::AVX_512bit);\n-    evpxorq(xmm0, xmm0, xmm20, Assembler::AVX_512bit);\n-    vpshufb(xmm1, xmm9, xmm16, Assembler::AVX_512bit);\n-    evpxorq(xmm1, xmm1, xmm20, Assembler::AVX_512bit);\n-    \/\/ increment counter by 8\n-    vpaddq(xmm8, xmm8, xmm19, Assembler::AVX_512bit);\n-    \/\/ AES encode\n-    roundEnc(xmm21, 1);\n-    roundEnc(xmm22, 1);\n-    roundEnc(xmm23, 1);\n-    roundEnc(xmm24, 1);\n-    roundEnc(xmm25, 1);\n-    roundEnc(xmm26, 1);\n-    roundEnc(xmm27, 1);\n-    roundEnc(xmm28, 1);\n-    roundEnc(xmm29, 1);\n-\n-    cmpl(rounds, 52);\n-    jcc(Assembler::aboveEqual, AES192_REMAINDER8);\n-    lastroundEnc(xmm30, 1);\n-    jmp(REMAINDER8_END_LOOP);\n-\n-    bind(AES192_REMAINDER8);\n-    roundEnc(xmm30, 1);\n-    ev_load_key(xmm18, key, 11 * 16, xmm31);\n-    roundEnc(xmm18, 1);\n-    ev_load_key(xmm5, key, 12 * 16, xmm31);\n-    cmpl(rounds, 60);\n-    jcc(Assembler::aboveEqual, AES256_REMAINDER8);\n-    lastroundEnc(xmm5, 1);\n-    jmp(REMAINDER8_END_LOOP);\n-\n-    bind(AES256_REMAINDER8);\n-    roundEnc(xmm5, 1);\n-    ev_load_key(xmm6, key, 13 * 16, xmm31);\n-    roundEnc(xmm6, 1);\n-    ev_load_key(xmm7, key, 14 * 16, xmm31);\n-    lastroundEnc(xmm7, 1);\n-\n-    bind(REMAINDER8_END_LOOP);\n-    \/\/ After AES encode rounds, the encrypted block cipher lies in zmm0-zmm1\n-    \/\/ XOR PT with the encrypted counter and store as CT\n-    evpxorq(xmm0, xmm0, Address(src_addr, pos, Address::times_1, 0 * 64), Assembler::AVX_512bit);\n-    evmovdquq(Address(dest_addr, pos, Address::times_1, 0 * 64), xmm0, Assembler::AVX_512bit);\n-    evpxorq(xmm1, xmm1, Address(src_addr, pos, Address::times_1, 1 * 64), Assembler::AVX_512bit);\n-    evmovdquq(Address(dest_addr, pos, Address::times_1, 1 * 64), xmm1, Assembler::AVX_512bit);\n-    addq(pos, 128);\n-\n-    cmpl(len_reg, 64);\n-    jcc(Assembler::aboveEqual, REMAINDER_4);\n-    \/\/ load mask for incrementing the counter value by 1\n-    evmovdquq(xmm19, ExternalAddress(StubRoutines::x86::counter_mask_addr() + 80), Assembler::AVX_128bit, r15);\/\/Linc0 + 16(rip)\n-    jmp(REMAINDER_LOOP);\n-\n-    \/\/ Each ZMM register can be used to encode 64 bytes of data, so we have 1 ZMM register used in this block of code\n-    bind(REMAINDER_4);\n-    subq(len_reg, 64);\n-    \/\/ As we process 4 blocks at a time, load mask for incrementing the counter value by 4\n-    evmovdquq(xmm19, ExternalAddress(StubRoutines::x86::counter_mask_addr() + 128), Assembler::AVX_512bit, r15);\/\/Linc4(rip)\n-    \/\/ XOR counter with first roundkey\n-    vpshufb(xmm0, xmm8, xmm16, Assembler::AVX_512bit);\n-    evpxorq(xmm0, xmm0, xmm20, Assembler::AVX_512bit);\n-    \/\/ Increment counter\n-    vpaddq(xmm8, xmm8, xmm19, Assembler::AVX_512bit);\n-    vaesenc(xmm0, xmm0, xmm21, Assembler::AVX_512bit);\n-    vaesenc(xmm0, xmm0, xmm22, Assembler::AVX_512bit);\n-    vaesenc(xmm0, xmm0, xmm23, Assembler::AVX_512bit);\n-    vaesenc(xmm0, xmm0, xmm24, Assembler::AVX_512bit);\n-    vaesenc(xmm0, xmm0, xmm25, Assembler::AVX_512bit);\n-    vaesenc(xmm0, xmm0, xmm26, Assembler::AVX_512bit);\n-    vaesenc(xmm0, xmm0, xmm27, Assembler::AVX_512bit);\n-    vaesenc(xmm0, xmm0, xmm28, Assembler::AVX_512bit);\n-    vaesenc(xmm0, xmm0, xmm29, Assembler::AVX_512bit);\n-    cmpl(rounds, 52);\n-    jcc(Assembler::aboveEqual, AES192_REMAINDER4);\n-    vaesenclast(xmm0, xmm0, xmm30, Assembler::AVX_512bit);\n-    jmp(END_REMAINDER4);\n-\n-    bind(AES192_REMAINDER4);\n-    vaesenc(xmm0, xmm0, xmm30, Assembler::AVX_512bit);\n-    ev_load_key(xmm18, key, 11 * 16, xmm31);\n-    vaesenc(xmm0, xmm0, xmm18, Assembler::AVX_512bit);\n-    ev_load_key(xmm5, key, 12 * 16, xmm31);\n-\n-    cmpl(rounds, 60);\n-    jcc(Assembler::aboveEqual, AES256_REMAINDER4);\n-    vaesenclast(xmm0, xmm0, xmm5, Assembler::AVX_512bit);\n-    jmp(END_REMAINDER4);\n-\n-    bind(AES256_REMAINDER4);\n-    vaesenc(xmm0, xmm0, xmm5, Assembler::AVX_512bit);\n-    ev_load_key(xmm6, key, 13 * 16, xmm31);\n-    vaesenc(xmm0, xmm0, xmm6, Assembler::AVX_512bit);\n-    ev_load_key(xmm7, key, 14 * 16, xmm31);\n-    vaesenclast(xmm0, xmm0, xmm7, Assembler::AVX_512bit);\n-    \/\/ After AES encode rounds, the encrypted block cipher lies in zmm0.\n-    \/\/ XOR encrypted block cipher with PT and store 64 bytes of ciphertext\n-    bind(END_REMAINDER4);\n-    evpxorq(xmm0, xmm0, Address(src_addr, pos, Address::times_1, 0 * 64), Assembler::AVX_512bit);\n-    evmovdquq(Address(dest_addr, pos, Address::times_1, 0), xmm0, Assembler::AVX_512bit);\n-    addq(pos, 64);\n-    \/\/ load mask for incrementing the counter value by 1\n-    evmovdquq(xmm19, ExternalAddress(StubRoutines::x86::counter_mask_addr() + 80), Assembler::AVX_128bit, r15);\/\/Linc0 + 16(rip)\n-\n-    \/\/ For a single block, the AES rounds start here.\n-    bind(REMAINDER_LOOP);\n-    cmpl(len_reg, 0);\n-    jcc(Assembler::belowEqual, END);\n-    \/\/ XOR counter with first roundkey\n-    vpshufb(xmm0, xmm8, xmm16, Assembler::AVX_128bit);\n-    evpxorq(xmm0, xmm0, xmm20, Assembler::AVX_128bit);\n-    vaesenc(xmm0, xmm0, xmm21, Assembler::AVX_128bit);\n-    \/\/ Increment counter by 1\n-    vpaddq(xmm8, xmm8, xmm19, Assembler::AVX_128bit);\n-    vaesenc(xmm0, xmm0, xmm22, Assembler::AVX_128bit);\n-    vaesenc(xmm0, xmm0, xmm23, Assembler::AVX_128bit);\n-    vaesenc(xmm0, xmm0, xmm24, Assembler::AVX_128bit);\n-    vaesenc(xmm0, xmm0, xmm25, Assembler::AVX_128bit);\n-    vaesenc(xmm0, xmm0, xmm26, Assembler::AVX_128bit);\n-    vaesenc(xmm0, xmm0, xmm27, Assembler::AVX_128bit);\n-    vaesenc(xmm0, xmm0, xmm28, Assembler::AVX_128bit);\n-    vaesenc(xmm0, xmm0, xmm29, Assembler::AVX_128bit);\n-\n-    cmpl(rounds, 52);\n-    jcc(Assembler::aboveEqual, AES192_REMAINDER);\n-    vaesenclast(xmm0, xmm0, xmm30, Assembler::AVX_128bit);\n-    jmp(END_REMAINDER_LOOP);\n-\n-    bind(AES192_REMAINDER);\n-    vaesenc(xmm0, xmm0, xmm30, Assembler::AVX_128bit);\n-    ev_load_key(xmm18, key, 11 * 16, xmm31);\n-    vaesenc(xmm0, xmm0, xmm18, Assembler::AVX_128bit);\n-    ev_load_key(xmm5, key, 12 * 16, xmm31);\n-    cmpl(rounds, 60);\n-    jcc(Assembler::aboveEqual, AES256_REMAINDER);\n-    vaesenclast(xmm0, xmm0, xmm5, Assembler::AVX_128bit);\n-    jmp(END_REMAINDER_LOOP);\n-\n-    bind(AES256_REMAINDER);\n-    vaesenc(xmm0, xmm0, xmm5, Assembler::AVX_128bit);\n-    ev_load_key(xmm6, key, 13 * 16, xmm31);\n-    vaesenc(xmm0, xmm0, xmm6, Assembler::AVX_128bit);\n-    ev_load_key(xmm7, key, 14 * 16, xmm31);\n-    vaesenclast(xmm0, xmm0, xmm7, Assembler::AVX_128bit);\n-\n-    bind(END_REMAINDER_LOOP);\n-    \/\/ If the length register is less than the blockSize i.e. 16\n-    \/\/ then we store only those bytes of the CT to the destination\n-    \/\/ corresponding to the length register value\n-    \/\/ extracting the exact number of bytes is handled by EXTRACT_TAILBYTES\n-    cmpl(len_reg, 16);\n-    jcc(Assembler::less, EXTRACT_TAILBYTES);\n-    subl(len_reg, 16);\n-    \/\/ After AES encode rounds, the encrypted block cipher lies in xmm0.\n-    \/\/ If the length register is equal to 16 bytes, store CT in dest after XOR operation.\n-    evpxorq(xmm0, xmm0, Address(src_addr, pos, Address::times_1, 0), Assembler::AVX_128bit);\n-    evmovdquq(Address(dest_addr, pos, Address::times_1, 0), xmm0, Assembler::AVX_128bit);\n-    addl(pos, 16);\n-\n-    jmp(REMAINDER_LOOP);\n-\n-    bind(EXTRACT_TAILBYTES);\n-    \/\/ Save encrypted counter value in xmm0 for next invocation, before XOR operation\n-    movdqu(Address(saved_encCounter_start, 0), xmm0);\n-    \/\/ XOR encryted block cipher in xmm0 with PT to produce CT\n-    evpxorq(xmm0, xmm0, Address(src_addr, pos, Address::times_1, 0), Assembler::AVX_128bit);\n-    \/\/ extract up to 15 bytes of CT from xmm0 as specified by length register\n-    testptr(len_reg, 8);\n-    jcc(Assembler::zero, EXTRACT_TAIL_4BYTES);\n-    pextrq(Address(dest_addr, pos), xmm0, 0);\n-    psrldq(xmm0, 8);\n-    addl(pos, 8);\n-    bind(EXTRACT_TAIL_4BYTES);\n-    testptr(len_reg, 4);\n-    jcc(Assembler::zero, EXTRACT_TAIL_2BYTES);\n-    pextrd(Address(dest_addr, pos), xmm0, 0);\n-    psrldq(xmm0, 4);\n-    addq(pos, 4);\n-    bind(EXTRACT_TAIL_2BYTES);\n-    testptr(len_reg, 2);\n-    jcc(Assembler::zero, EXTRACT_TAIL_1BYTE);\n-    pextrw(Address(dest_addr, pos), xmm0, 0);\n-    psrldq(xmm0, 2);\n-    addl(pos, 2);\n-    bind(EXTRACT_TAIL_1BYTE);\n-    testptr(len_reg, 1);\n-    jcc(Assembler::zero, END);\n-    pextrb(Address(dest_addr, pos), xmm0, 0);\n-    addl(pos, 1);\n-\n-    bind(END);\n-    \/\/ If there are no tail bytes, store counter value and exit\n-    cmpl(len_reg, 0);\n-    jcc(Assembler::equal, STORE_CTR);\n-    movl(Address(used_addr, 0), len_reg);\n-\n-    bind(STORE_CTR);\n-    \/\/shuffle updated counter and store it\n-    vpshufb(xmm8, xmm8, xmm16, Assembler::AVX_128bit);\n-    movdqu(Address(counter, 0), xmm8);\n-    \/\/ Zero out counter and key registers\n-    evpxorq(xmm8, xmm8, xmm8, Assembler::AVX_512bit);\n-    evpxorq(xmm20, xmm20, xmm20, Assembler::AVX_512bit);\n-    evpxorq(xmm21, xmm21, xmm21, Assembler::AVX_512bit);\n-    evpxorq(xmm22, xmm22, xmm22, Assembler::AVX_512bit);\n-    evpxorq(xmm23, xmm23, xmm23, Assembler::AVX_512bit);\n-    evpxorq(xmm24, xmm24, xmm24, Assembler::AVX_512bit);\n-    evpxorq(xmm25, xmm25, xmm25, Assembler::AVX_512bit);\n-    evpxorq(xmm26, xmm26, xmm26, Assembler::AVX_512bit);\n-    evpxorq(xmm27, xmm27, xmm27, Assembler::AVX_512bit);\n-    evpxorq(xmm28, xmm28, xmm28, Assembler::AVX_512bit);\n-    evpxorq(xmm29, xmm29, xmm29, Assembler::AVX_512bit);\n-    evpxorq(xmm30, xmm30, xmm30, Assembler::AVX_512bit);\n-    cmpl(rounds, 44);\n-    jcc(Assembler::belowEqual, EXIT);\n-    evpxorq(xmm18, xmm18, xmm18, Assembler::AVX_512bit);\n-    evpxorq(xmm5, xmm5, xmm5, Assembler::AVX_512bit);\n-    cmpl(rounds, 52);\n-    jcc(Assembler::belowEqual, EXIT);\n-    evpxorq(xmm6, xmm6, xmm6, Assembler::AVX_512bit);\n-    evpxorq(xmm7, xmm7, xmm7, Assembler::AVX_512bit);\n-    bind(EXIT);\n-}\n-\n-void MacroAssembler::gfmul_avx512(XMMRegister GH, XMMRegister HK) {\n-    const XMMRegister TMP1 = xmm0;\n-    const XMMRegister TMP2 = xmm1;\n-    const XMMRegister TMP3 = xmm2;\n-\n-    evpclmulqdq(TMP1, GH, HK, 0x11, Assembler::AVX_512bit);\n-    evpclmulqdq(TMP2, GH, HK, 0x00, Assembler::AVX_512bit);\n-    evpclmulqdq(TMP3, GH, HK, 0x01, Assembler::AVX_512bit);\n-    evpclmulqdq(GH, GH, HK, 0x10, Assembler::AVX_512bit);\n-    evpxorq(GH, GH, TMP3, Assembler::AVX_512bit);\n-    vpsrldq(TMP3, GH, 8, Assembler::AVX_512bit);\n-    vpslldq(GH, GH, 8, Assembler::AVX_512bit);\n-    evpxorq(TMP1, TMP1, TMP3, Assembler::AVX_512bit);\n-    evpxorq(GH, GH, TMP2, Assembler::AVX_512bit);\n-\n-    evmovdquq(TMP3, ExternalAddress(StubRoutines::x86::ghash_polynomial512_addr()), Assembler::AVX_512bit, r15);\n-    evpclmulqdq(TMP2, TMP3, GH, 0x01, Assembler::AVX_512bit);\n-    vpslldq(TMP2, TMP2, 8, Assembler::AVX_512bit);\n-    evpxorq(GH, GH, TMP2, Assembler::AVX_512bit);\n-    evpclmulqdq(TMP2, TMP3, GH, 0x00, Assembler::AVX_512bit);\n-    vpsrldq(TMP2, TMP2, 4, Assembler::AVX_512bit);\n-    evpclmulqdq(GH, TMP3, GH, 0x10, Assembler::AVX_512bit);\n-    vpslldq(GH, GH, 4, Assembler::AVX_512bit);\n-    vpternlogq(GH, 0x96, TMP1, TMP2, Assembler::AVX_512bit);\n-}\n-\n-void MacroAssembler::generateHtbl_48_block_zmm(Register htbl, Register avx512_htbl) {\n-    const XMMRegister HK = xmm6;\n-    const XMMRegister ZT5 = xmm4;\n-    const XMMRegister ZT7 = xmm7;\n-    const XMMRegister ZT8 = xmm8;\n-\n-    Label GFMUL_AVX512;\n-\n-    movdqu(HK, Address(htbl, 0));\n-    movdqu(xmm10, ExternalAddress(StubRoutines::x86::ghash_long_swap_mask_addr()));\n-    vpshufb(HK, HK, xmm10, Assembler::AVX_128bit);\n-\n-    movdqu(xmm11, ExternalAddress(StubRoutines::x86::ghash_polynomial512_addr() + 64)); \/\/ Poly\n-    movdqu(xmm12, ExternalAddress(StubRoutines::x86::ghash_polynomial512_addr() + 80)); \/\/ Twoone\n-    \/\/ Compute H ^ 2 from the input subkeyH\n-    movdqu(xmm2, xmm6);\n-    vpsllq(xmm6, xmm6, 1, Assembler::AVX_128bit);\n-    vpsrlq(xmm2, xmm2, 63, Assembler::AVX_128bit);\n-    movdqu(xmm1, xmm2);\n-    vpslldq(xmm2, xmm2, 8, Assembler::AVX_128bit);\n-    vpsrldq(xmm1, xmm1, 8, Assembler::AVX_128bit);\n-    vpor(xmm6, xmm6, xmm2, Assembler::AVX_128bit);\n-\n-    vpshufd(xmm2, xmm1, 0x24, Assembler::AVX_128bit);\n-    vpcmpeqd(xmm2, xmm2, xmm12, AVX_128bit);\n-    vpand(xmm2, xmm2, xmm11, Assembler::AVX_128bit);\n-    vpxor(xmm6, xmm6, xmm2, Assembler::AVX_128bit);\n-    movdqu(Address(avx512_htbl, 16 * 47), xmm6); \/\/ H ^ 2\n-    \/\/ Compute the remaining three powers of H using XMM registers and all following powers using ZMM\n-    movdqu(ZT5, HK);\n-    vinserti32x4(ZT7, ZT7, HK, 3);\n-\n-    gfmul_avx512(ZT5, HK);\n-    movdqu(Address(avx512_htbl, 16 * 46), ZT5); \/\/ H ^ 2 * 2\n-    vinserti32x4(ZT7, ZT7, ZT5, 2);\n-\n-    gfmul_avx512(ZT5, HK);\n-    movdqu(Address(avx512_htbl, 16 * 45), ZT5); \/\/ H ^ 2 * 3\n-    vinserti32x4(ZT7, ZT7, ZT5, 1);\n-\n-    gfmul_avx512(ZT5, HK);\n-    movdqu(Address(avx512_htbl, 16 * 44), ZT5); \/\/ H ^ 2 * 4\n-    vinserti32x4(ZT7, ZT7, ZT5, 0);\n-\n-    evshufi64x2(ZT5, ZT5, ZT5, 0x00, Assembler::AVX_512bit);\n-    evmovdquq(ZT8, ZT7, Assembler::AVX_512bit);\n-    gfmul_avx512(ZT7, ZT5);\n-    evmovdquq(Address(avx512_htbl, 16 * 40), ZT7, Assembler::AVX_512bit);\n-    evshufi64x2(ZT5, ZT7, ZT7, 0x00, Assembler::AVX_512bit);\n-    gfmul_avx512(ZT8, ZT5);\n-    evmovdquq(Address(avx512_htbl, 16 * 36), ZT8, Assembler::AVX_512bit);\n-    gfmul_avx512(ZT7, ZT5);\n-    evmovdquq(Address(avx512_htbl, 16 * 32), ZT7, Assembler::AVX_512bit);\n-    gfmul_avx512(ZT8, ZT5);\n-    evmovdquq(Address(avx512_htbl, 16 * 28), ZT8, Assembler::AVX_512bit);\n-    gfmul_avx512(ZT7, ZT5);\n-    evmovdquq(Address(avx512_htbl, 16 * 24), ZT7, Assembler::AVX_512bit);\n-    gfmul_avx512(ZT8, ZT5);\n-    evmovdquq(Address(avx512_htbl, 16 * 20), ZT8, Assembler::AVX_512bit);\n-    gfmul_avx512(ZT7, ZT5);\n-    evmovdquq(Address(avx512_htbl, 16 * 16), ZT7, Assembler::AVX_512bit);\n-    gfmul_avx512(ZT8, ZT5);\n-    evmovdquq(Address(avx512_htbl, 16 * 12), ZT8, Assembler::AVX_512bit);\n-    gfmul_avx512(ZT7, ZT5);\n-    evmovdquq(Address(avx512_htbl, 16 * 8), ZT7, Assembler::AVX_512bit);\n-    gfmul_avx512(ZT8, ZT5);\n-    evmovdquq(Address(avx512_htbl, 16 * 4), ZT8, Assembler::AVX_512bit);\n-    gfmul_avx512(ZT7, ZT5);\n-    evmovdquq(Address(avx512_htbl, 16 * 0), ZT7, Assembler::AVX_512bit);\n-    ret(0);\n-}\n-\n-#define vclmul_reduce(out, poly, hi128, lo128, tmp0, tmp1) \\\n-evpclmulqdq(tmp0, poly, lo128, 0x01, Assembler::AVX_512bit); \\\n-vpslldq(tmp0, tmp0, 8, Assembler::AVX_512bit); \\\n-evpxorq(tmp0, lo128, tmp0, Assembler::AVX_512bit); \\\n-evpclmulqdq(tmp1, poly, tmp0, 0x00, Assembler::AVX_512bit); \\\n-vpsrldq(tmp1, tmp1, 4, Assembler::AVX_512bit); \\\n-evpclmulqdq(out, poly, tmp0, 0x10, Assembler::AVX_512bit); \\\n-vpslldq(out, out, 4, Assembler::AVX_512bit); \\\n-vpternlogq(out, 0x96, tmp1, hi128, Assembler::AVX_512bit); \\\n-\n-#define vhpxori4x128(reg, tmp) \\\n-vextracti64x4(tmp, reg, 1); \\\n-evpxorq(reg, reg, tmp, Assembler::AVX_256bit); \\\n-vextracti32x4(tmp, reg, 1); \\\n-evpxorq(reg, reg, tmp, Assembler::AVX_128bit); \\\n-\n-#define roundEncode(key, dst1, dst2, dst3, dst4) \\\n-vaesenc(dst1, dst1, key, Assembler::AVX_512bit); \\\n-vaesenc(dst2, dst2, key, Assembler::AVX_512bit); \\\n-vaesenc(dst3, dst3, key, Assembler::AVX_512bit); \\\n-vaesenc(dst4, dst4, key, Assembler::AVX_512bit); \\\n-\n-#define lastroundEncode(key, dst1, dst2, dst3, dst4) \\\n-vaesenclast(dst1, dst1, key, Assembler::AVX_512bit); \\\n-vaesenclast(dst2, dst2, key, Assembler::AVX_512bit); \\\n-vaesenclast(dst3, dst3, key, Assembler::AVX_512bit); \\\n-vaesenclast(dst4, dst4, key, Assembler::AVX_512bit); \\\n-\n-#define storeData(dst, position, src1, src2, src3, src4) \\\n-evmovdquq(Address(dst, position, Address::times_1, 0 * 64), src1, Assembler::AVX_512bit); \\\n-evmovdquq(Address(dst, position, Address::times_1, 1 * 64), src2, Assembler::AVX_512bit); \\\n-evmovdquq(Address(dst, position, Address::times_1, 2 * 64), src3, Assembler::AVX_512bit); \\\n-evmovdquq(Address(dst, position, Address::times_1, 3 * 64), src4, Assembler::AVX_512bit); \\\n-\n-#define loadData(src, position, dst1, dst2, dst3, dst4) \\\n-evmovdquq(dst1, Address(src, position, Address::times_1, 0 * 64), Assembler::AVX_512bit); \\\n-evmovdquq(dst2, Address(src, position, Address::times_1, 1 * 64), Assembler::AVX_512bit); \\\n-evmovdquq(dst3, Address(src, position, Address::times_1, 2 * 64), Assembler::AVX_512bit); \\\n-evmovdquq(dst4, Address(src, position, Address::times_1, 3 * 64), Assembler::AVX_512bit); \\\n-\n-#define carrylessMultiply(dst00, dst01, dst10, dst11, ghdata, hkey) \\\n-evpclmulqdq(dst00, ghdata, hkey, 0x00, Assembler::AVX_512bit); \\\n-evpclmulqdq(dst01, ghdata, hkey, 0x01, Assembler::AVX_512bit); \\\n-evpclmulqdq(dst10, ghdata, hkey, 0x10, Assembler::AVX_512bit); \\\n-evpclmulqdq(dst11, ghdata, hkey, 0x11, Assembler::AVX_512bit); \\\n-\n-#define shuffleExorRnd1Key(dst0, dst1, dst2, dst3, shufmask, rndkey) \\\n-vpshufb(dst0, dst0, shufmask, Assembler::AVX_512bit); \\\n-evpxorq(dst0, dst0, rndkey, Assembler::AVX_512bit); \\\n-vpshufb(dst1, dst1, shufmask, Assembler::AVX_512bit); \\\n-evpxorq(dst1, dst1, rndkey, Assembler::AVX_512bit); \\\n-vpshufb(dst2, dst2, shufmask, Assembler::AVX_512bit); \\\n-evpxorq(dst2, dst2, rndkey, Assembler::AVX_512bit); \\\n-vpshufb(dst3, dst3, shufmask, Assembler::AVX_512bit); \\\n-evpxorq(dst3, dst3, rndkey, Assembler::AVX_512bit); \\\n-\n-#define xorBeforeStore(dst0, dst1, dst2, dst3, src0, src1, src2, src3) \\\n-evpxorq(dst0, dst0, src0, Assembler::AVX_512bit); \\\n-evpxorq(dst1, dst1, src1, Assembler::AVX_512bit); \\\n-evpxorq(dst2, dst2, src2, Assembler::AVX_512bit); \\\n-evpxorq(dst3, dst3, src3, Assembler::AVX_512bit); \\\n-\n-#define xorGHASH(dst0, dst1, dst2, dst3, src02, src03, src12, src13, src22, src23, src32, src33) \\\n-vpternlogq(dst0, 0x96, src02, src03, Assembler::AVX_512bit); \\\n-vpternlogq(dst1, 0x96, src12, src13, Assembler::AVX_512bit); \\\n-vpternlogq(dst2, 0x96, src22, src23, Assembler::AVX_512bit); \\\n-vpternlogq(dst3, 0x96, src32, src33, Assembler::AVX_512bit); \\\n-\n-void MacroAssembler::ghash16_encrypt16_parallel(Register key, Register subkeyHtbl, XMMRegister ctr_blockx, XMMRegister aad_hashx,\n-    Register in, Register out, Register data, Register pos, bool first_time_reduction, XMMRegister addmask, bool ghash_input, Register rounds,\n-    Register ghash_pos, bool final_reduction, int i, XMMRegister counter_inc_mask) {\n-\n-    Label AES_192, AES_256, LAST_AES_RND;\n-    const XMMRegister ZTMP0 = xmm0;\n-    const XMMRegister ZTMP1 = xmm3;\n-    const XMMRegister ZTMP2 = xmm4;\n-    const XMMRegister ZTMP3 = xmm5;\n-    const XMMRegister ZTMP5 = xmm7;\n-    const XMMRegister ZTMP6 = xmm10;\n-    const XMMRegister ZTMP7 = xmm11;\n-    const XMMRegister ZTMP8 = xmm12;\n-    const XMMRegister ZTMP9 = xmm13;\n-    const XMMRegister ZTMP10 = xmm15;\n-    const XMMRegister ZTMP11 = xmm16;\n-    const XMMRegister ZTMP12 = xmm17;\n-\n-    const XMMRegister ZTMP13 = xmm19;\n-    const XMMRegister ZTMP14 = xmm20;\n-    const XMMRegister ZTMP15 = xmm21;\n-    const XMMRegister ZTMP16 = xmm30;\n-    const XMMRegister ZTMP17 = xmm31;\n-    const XMMRegister ZTMP18 = xmm1;\n-    const XMMRegister ZTMP19 = xmm2;\n-    const XMMRegister ZTMP20 = xmm8;\n-    const XMMRegister ZTMP21 = xmm22;\n-    const XMMRegister ZTMP22 = xmm23;\n-\n-    \/\/ Pre increment counters\n-    vpaddd(ZTMP0, ctr_blockx, counter_inc_mask, Assembler::AVX_512bit);\n-    vpaddd(ZTMP1, ZTMP0, counter_inc_mask, Assembler::AVX_512bit);\n-    vpaddd(ZTMP2, ZTMP1, counter_inc_mask, Assembler::AVX_512bit);\n-    vpaddd(ZTMP3, ZTMP2, counter_inc_mask, Assembler::AVX_512bit);\n-    \/\/ Save counter value\n-    evmovdquq(ctr_blockx, ZTMP3, Assembler::AVX_512bit);\n-\n-    \/\/ Reuse ZTMP17 \/ ZTMP18 for loading AES Keys\n-    \/\/ Pre-load AES round keys\n-    ev_load_key(ZTMP17, key, 0, xmm29);\n-    ev_load_key(ZTMP18, key, 1 * 16, xmm29);\n-\n-    \/\/ ZTMP19 & ZTMP20 used for loading hash key\n-    \/\/ Pre-load hash key\n-    evmovdquq(ZTMP19, Address(subkeyHtbl, i * 64), Assembler::AVX_512bit);\n-    evmovdquq(ZTMP20, Address(subkeyHtbl, ++i * 64), Assembler::AVX_512bit);\n-    \/\/ Load data for computing ghash\n-    evmovdquq(ZTMP21, Address(data, ghash_pos, Address::times_1, 0 * 64), Assembler::AVX_512bit);\n-    vpshufb(ZTMP21, ZTMP21, xmm24, Assembler::AVX_512bit);\n-\n-    \/\/ Xor cipher block 0 with input ghash, if available\n-    if (ghash_input) {\n-        evpxorq(ZTMP21, ZTMP21, aad_hashx, Assembler::AVX_512bit);\n-    }\n-    \/\/ Load data for computing ghash\n-    evmovdquq(ZTMP22, Address(data, ghash_pos, Address::times_1, 1 * 64), Assembler::AVX_512bit);\n-    vpshufb(ZTMP22, ZTMP22, xmm24, Assembler::AVX_512bit);\n-\n-    \/\/ stitch AES rounds with GHASH\n-    \/\/ AES round 0, xmm24 has shuffle mask\n-    shuffleExorRnd1Key(ZTMP0, ZTMP1, ZTMP2, ZTMP3, xmm24, ZTMP17);\n-    \/\/ Reuse ZTMP17 \/ ZTMP18 for loading remaining AES Keys\n-    ev_load_key(ZTMP17, key, 2 * 16, xmm29);\n-    \/\/ GHASH 4 blocks\n-    carrylessMultiply(ZTMP6, ZTMP7, ZTMP8, ZTMP5, ZTMP21, ZTMP19);\n-    \/\/ Load the next hkey and Ghash data\n-    evmovdquq(ZTMP19, Address(subkeyHtbl, ++i * 64), Assembler::AVX_512bit);\n-    evmovdquq(ZTMP21, Address(data, ghash_pos, Address::times_1, 2 * 64), Assembler::AVX_512bit);\n-    vpshufb(ZTMP21, ZTMP21, xmm24, Assembler::AVX_512bit);\n-\n-    \/\/ AES round 1\n-    roundEncode(ZTMP18, ZTMP0, ZTMP1, ZTMP2, ZTMP3);\n-    ev_load_key(ZTMP18, key, 3 * 16, xmm29);\n-\n-    \/\/ GHASH 4 blocks(11 to 8)\n-    carrylessMultiply(ZTMP10, ZTMP12, ZTMP11, ZTMP9, ZTMP22, ZTMP20);\n-    \/\/ Load the next hkey and GDATA\n-    evmovdquq(ZTMP20, Address(subkeyHtbl, ++i * 64), Assembler::AVX_512bit);\n-    evmovdquq(ZTMP22, Address(data, ghash_pos, Address::times_1, 3 * 64), Assembler::AVX_512bit);\n-    vpshufb(ZTMP22, ZTMP22, xmm24, Assembler::AVX_512bit);\n-\n-    \/\/ AES round 2\n-    roundEncode(ZTMP17, ZTMP0, ZTMP1, ZTMP2, ZTMP3);\n-    ev_load_key(ZTMP17, key, 4 * 16, xmm29);\n-\n-    \/\/ GHASH 4 blocks(7 to 4)\n-    carrylessMultiply(ZTMP14, ZTMP16, ZTMP15, ZTMP13, ZTMP21, ZTMP19);\n-    \/\/ AES rounds 3\n-    roundEncode(ZTMP18, ZTMP0, ZTMP1, ZTMP2, ZTMP3);\n-    ev_load_key(ZTMP18, key, 5 * 16, xmm29);\n-\n-    \/\/ Gather(XOR) GHASH for 12 blocks\n-    xorGHASH(ZTMP5, ZTMP6, ZTMP8, ZTMP7, ZTMP9, ZTMP13, ZTMP10, ZTMP14, ZTMP12, ZTMP16, ZTMP11, ZTMP15);\n-\n-    \/\/ AES rounds 4\n-    roundEncode(ZTMP17, ZTMP0, ZTMP1, ZTMP2, ZTMP3);\n-    ev_load_key(ZTMP17, key, 6 * 16, xmm29);\n-\n-    \/\/ load plain \/ cipher text(recycle registers)\n-    loadData(in, pos, ZTMP13, ZTMP14, ZTMP15, ZTMP16);\n-\n-    \/\/ AES rounds 5\n-    roundEncode(ZTMP18, ZTMP0, ZTMP1, ZTMP2, ZTMP3);\n-    ev_load_key(ZTMP18, key, 7 * 16, xmm29);\n-    \/\/ GHASH 4 blocks(3 to 0)\n-    carrylessMultiply(ZTMP10, ZTMP12, ZTMP11, ZTMP9, ZTMP22, ZTMP20);\n-\n-    \/\/  AES round 6\n-    roundEncode(ZTMP17, ZTMP0, ZTMP1, ZTMP2, ZTMP3);\n-    ev_load_key(ZTMP17, key, 8 * 16, xmm29);\n-\n-    \/\/ gather GHASH in ZTMP6(low) and ZTMP5(high)\n-    if (first_time_reduction) {\n-        vpternlogq(ZTMP7, 0x96, ZTMP8, ZTMP12, Assembler::AVX_512bit);\n-        evpxorq(xmm25, ZTMP7, ZTMP11, Assembler::AVX_512bit);\n-        evpxorq(xmm27, ZTMP5, ZTMP9, Assembler::AVX_512bit);\n-        evpxorq(xmm26, ZTMP6, ZTMP10, Assembler::AVX_512bit);\n-    }\n-    else if (!first_time_reduction && !final_reduction) {\n-        xorGHASH(ZTMP7, xmm25, xmm27, xmm26, ZTMP8, ZTMP12, ZTMP7, ZTMP11, ZTMP5, ZTMP9, ZTMP6, ZTMP10);\n-    }\n-\n-    if (final_reduction) {\n-        \/\/ Phase one: Add mid products together\n-        \/\/ Also load polynomial constant for reduction\n-        vpternlogq(ZTMP7, 0x96, ZTMP8, ZTMP12, Assembler::AVX_512bit);\n-        vpternlogq(ZTMP7, 0x96, xmm25, ZTMP11, Assembler::AVX_512bit);\n-        vpsrldq(ZTMP11, ZTMP7, 8, Assembler::AVX_512bit);\n-        vpslldq(ZTMP7, ZTMP7, 8, Assembler::AVX_512bit);\n-        evmovdquq(ZTMP12, ExternalAddress(StubRoutines::x86::ghash_polynomial512_addr()), Assembler::AVX_512bit, rbx);\n-    }\n-    \/\/ AES round 7\n-    roundEncode(ZTMP18, ZTMP0, ZTMP1, ZTMP2, ZTMP3);\n-    ev_load_key(ZTMP18, key, 9 * 16, xmm29);\n-    if (final_reduction) {\n-        vpternlogq(ZTMP5, 0x96, ZTMP9, ZTMP11, Assembler::AVX_512bit);\n-        evpxorq(ZTMP5, ZTMP5, xmm27, Assembler::AVX_512bit);\n-        vpternlogq(ZTMP6, 0x96, ZTMP10, ZTMP7, Assembler::AVX_512bit);\n-        evpxorq(ZTMP6, ZTMP6, xmm26, Assembler::AVX_512bit);\n-    }\n-    \/\/ AES round 8\n-    roundEncode(ZTMP17, ZTMP0, ZTMP1, ZTMP2, ZTMP3);\n-    ev_load_key(ZTMP17, key, 10 * 16, xmm29);\n-\n-    \/\/ Horizontal xor of low and high 4*128\n-    if (final_reduction) {\n-        vhpxori4x128(ZTMP5, ZTMP9);\n-        vhpxori4x128(ZTMP6, ZTMP10);\n-    }\n-    \/\/ AES round 9\n-    roundEncode(ZTMP18, ZTMP0, ZTMP1, ZTMP2, ZTMP3);\n-    \/\/ First phase of reduction\n-    if (final_reduction) {\n-        evpclmulqdq(ZTMP10, ZTMP12, ZTMP6, 0x01, Assembler::AVX_128bit);\n-        vpslldq(ZTMP10, ZTMP10, 8, Assembler::AVX_128bit);\n-        evpxorq(ZTMP10, ZTMP6, ZTMP10, Assembler::AVX_128bit);\n-    }\n-    cmpl(rounds, 52);\n-    jcc(Assembler::greaterEqual, AES_192);\n-    jmp(LAST_AES_RND);\n-    \/\/ AES rounds up to 11 (AES192) or 13 (AES256)\n-    bind(AES_192);\n-    roundEncode(ZTMP17, ZTMP0, ZTMP1, ZTMP2, ZTMP3);\n-    ev_load_key(ZTMP18, key, 11 * 16, xmm29);\n-    roundEncode(ZTMP18, ZTMP0, ZTMP1, ZTMP2, ZTMP3);\n-    ev_load_key(ZTMP17, key, 12 * 16, xmm29);\n-    cmpl(rounds, 60);\n-    jcc(Assembler::aboveEqual, AES_256);\n-    jmp(LAST_AES_RND);\n-\n-    bind(AES_256);\n-    roundEncode(ZTMP17, ZTMP0, ZTMP1, ZTMP2, ZTMP3);\n-    ev_load_key(ZTMP18, key, 13 * 16, xmm29);\n-    roundEncode(ZTMP18, ZTMP0, ZTMP1, ZTMP2, ZTMP3);\n-    ev_load_key(ZTMP17, key, 14 * 16, xmm29);\n-\n-    bind(LAST_AES_RND);\n-    \/\/ Second phase of reduction\n-    if (final_reduction) {\n-        evpclmulqdq(ZTMP9, ZTMP12, ZTMP10, 0x00, Assembler::AVX_128bit);\n-        vpsrldq(ZTMP9, ZTMP9, 4, Assembler::AVX_128bit); \/\/ Shift-R 1-DW to obtain 2-DWs shift-R\n-        evpclmulqdq(ZTMP11, ZTMP12, ZTMP10, 0x10, Assembler::AVX_128bit);\n-        vpslldq(ZTMP11, ZTMP11, 4, Assembler::AVX_128bit); \/\/ Shift-L 1-DW for result\n-        \/\/ ZTMP5 = ZTMP5 X ZTMP11 X ZTMP9\n-        vpternlogq(ZTMP5, 0x96, ZTMP11, ZTMP9, Assembler::AVX_128bit);\n-    }\n-    \/\/ Last AES round\n-    lastroundEncode(ZTMP17, ZTMP0, ZTMP1, ZTMP2, ZTMP3);\n-    \/\/ XOR against plain \/ cipher text\n-    xorBeforeStore(ZTMP0, ZTMP1, ZTMP2, ZTMP3, ZTMP13, ZTMP14, ZTMP15, ZTMP16);\n-    \/\/ store cipher \/ plain text\n-    storeData(out, pos, ZTMP0, ZTMP1, ZTMP2, ZTMP3);\n-}\n-\n-void MacroAssembler::aesgcm_encrypt(Register in, Register len, Register ct, Register out, Register key,\n-                                    Register state, Register subkeyHtbl, Register avx512_subkeyHtbl, Register counter) {\n-    Label ENC_DEC_DONE, GENERATE_HTBL_48_BLKS, AES_192, AES_256, STORE_CT, GHASH_LAST_32,\n-          AES_32_BLOCKS, GHASH_AES_PARALLEL, LOOP, ACCUMULATE, GHASH_16_AES_16;\n-    const XMMRegister CTR_BLOCKx = xmm9;\n-    const XMMRegister AAD_HASHx = xmm14;\n-    const Register pos = rax;\n-    const Register rounds = r15;\n-    Register ghash_pos;\n-#ifndef _WIN64\n-    ghash_pos = r14;\n-#else\n-    ghash_pos = r11;\n-#endif \/\/ !_WIN64\n-    const XMMRegister ZTMP0 = xmm0;\n-    const XMMRegister ZTMP1 = xmm3;\n-    const XMMRegister ZTMP2 = xmm4;\n-    const XMMRegister ZTMP3 = xmm5;\n-    const XMMRegister ZTMP4 = xmm6;\n-    const XMMRegister ZTMP5 = xmm7;\n-    const XMMRegister ZTMP6 = xmm10;\n-    const XMMRegister ZTMP7 = xmm11;\n-    const XMMRegister ZTMP8 = xmm12;\n-    const XMMRegister ZTMP9 = xmm13;\n-    const XMMRegister ZTMP10 = xmm15;\n-    const XMMRegister ZTMP11 = xmm16;\n-    const XMMRegister ZTMP12 = xmm17;\n-    const XMMRegister ZTMP13 = xmm19;\n-    const XMMRegister ZTMP14 = xmm20;\n-    const XMMRegister ZTMP15 = xmm21;\n-    const XMMRegister ZTMP16 = xmm30;\n-    const XMMRegister COUNTER_INC_MASK = xmm18;\n-\n-    movl(pos, 0); \/\/ Total length processed\n-    \/\/ Min data size processed = 768 bytes\n-    cmpl(len, 768);\n-    jcc(Assembler::less, ENC_DEC_DONE);\n-\n-    \/\/ Generate 48 constants for htbl\n-    call(GENERATE_HTBL_48_BLKS, relocInfo::none);\n-    int index = 0; \/\/ Index for choosing subkeyHtbl entry\n-    movl(ghash_pos, 0); \/\/ Pointer for ghash read and store operations\n-\n-    \/\/ Move initial counter value and STATE value into variables\n-    movdqu(CTR_BLOCKx, Address(counter, 0));\n-    movdqu(AAD_HASHx, Address(state, 0));\n-    \/\/ Load lswap mask for ghash\n-    movdqu(xmm24, ExternalAddress(StubRoutines::x86::ghash_long_swap_mask_addr()), rbx);\n-    \/\/ Shuffle input state using lswap mask\n-    vpshufb(AAD_HASHx, AAD_HASHx, xmm24, Assembler::AVX_128bit);\n-\n-    \/\/ Compute #rounds for AES based on the length of the key array\n-    movl(rounds, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));\n-\n-    \/\/ Broadcast counter value to 512 bit register\n-    evshufi64x2(CTR_BLOCKx, CTR_BLOCKx, CTR_BLOCKx, 0, Assembler::AVX_512bit);\n-    \/\/ Load counter shuffle mask\n-    evmovdquq(xmm24, ExternalAddress(StubRoutines::x86::counter_mask_addr()), Assembler::AVX_512bit, rbx);\n-    \/\/ Shuffle counter\n-    vpshufb(CTR_BLOCKx, CTR_BLOCKx, xmm24, Assembler::AVX_512bit);\n-\n-    \/\/ Load mask for incrementing counter\n-    evmovdquq(COUNTER_INC_MASK, ExternalAddress(StubRoutines::x86::counter_mask_addr() + 128), Assembler::AVX_512bit, rbx);\n-    \/\/ Pre-increment counter\n-    vpaddd(ZTMP5, CTR_BLOCKx, ExternalAddress(StubRoutines::x86::counter_mask_addr() + 64), Assembler::AVX_512bit, rbx);\n-    vpaddd(ZTMP6, ZTMP5, COUNTER_INC_MASK, Assembler::AVX_512bit);\n-    vpaddd(ZTMP7, ZTMP6, COUNTER_INC_MASK, Assembler::AVX_512bit);\n-    vpaddd(ZTMP8, ZTMP7, COUNTER_INC_MASK, Assembler::AVX_512bit);\n-\n-    \/\/ Begin 32 blocks of AES processing\n-    bind(AES_32_BLOCKS);\n-    \/\/ Save incremented counter before overwriting it with AES data\n-    evmovdquq(CTR_BLOCKx, ZTMP8, Assembler::AVX_512bit);\n-\n-    \/\/ Move 256 bytes of data\n-    loadData(in, pos, ZTMP0, ZTMP1, ZTMP2, ZTMP3);\n-    \/\/ Load key shuffle mask\n-    movdqu(xmm29, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()), rbx);\n-    \/\/ Load 0th AES round key\n-    ev_load_key(ZTMP4, key, 0, xmm29);\n-    \/\/ AES-ROUND0, xmm24 has the shuffle mask\n-    shuffleExorRnd1Key(ZTMP5, ZTMP6, ZTMP7, ZTMP8, xmm24, ZTMP4);\n-\n-    for (int j = 1; j < 10; j++) {\n-        ev_load_key(ZTMP4, key, j * 16, xmm29);\n-        roundEncode(ZTMP4, ZTMP5, ZTMP6, ZTMP7, ZTMP8);\n-    }\n-    ev_load_key(ZTMP4, key, 10 * 16, xmm29);\n-    \/\/ AES rounds up to 11 (AES192) or 13 (AES256)\n-    cmpl(rounds, 52);\n-    jcc(Assembler::greaterEqual, AES_192);\n-    lastroundEncode(ZTMP4, ZTMP5, ZTMP6, ZTMP7, ZTMP8);\n-    jmp(STORE_CT);\n-\n-    bind(AES_192);\n-    roundEncode(ZTMP4, ZTMP5, ZTMP6, ZTMP7, ZTMP8);\n-    ev_load_key(ZTMP4, key, 11 * 16, xmm29);\n-    roundEncode(ZTMP4, ZTMP5, ZTMP6, ZTMP7, ZTMP8);\n-    cmpl(rounds, 60);\n-    jcc(Assembler::aboveEqual, AES_256);\n-    ev_load_key(ZTMP4, key, 12 * 16, xmm29);\n-    lastroundEncode(ZTMP4, ZTMP5, ZTMP6, ZTMP7, ZTMP8);\n-    jmp(STORE_CT);\n-\n-    bind(AES_256);\n-    ev_load_key(ZTMP4, key, 12 * 16, xmm29);\n-    roundEncode(ZTMP4, ZTMP5, ZTMP6, ZTMP7, ZTMP8);\n-    ev_load_key(ZTMP4, key, 13 * 16, xmm29);\n-    roundEncode(ZTMP4, ZTMP5, ZTMP6, ZTMP7, ZTMP8);\n-    ev_load_key(ZTMP4, key, 14 * 16, xmm29);\n-    \/\/ Last AES round\n-    lastroundEncode(ZTMP4, ZTMP5, ZTMP6, ZTMP7, ZTMP8);\n-\n-    bind(STORE_CT);\n-    \/\/ Xor the encrypted key with PT to obtain CT\n-    xorBeforeStore(ZTMP5, ZTMP6, ZTMP7, ZTMP8, ZTMP0, ZTMP1, ZTMP2, ZTMP3);\n-    storeData(out, pos, ZTMP5, ZTMP6, ZTMP7, ZTMP8);\n-    \/\/ 16 blocks encryption completed\n-    addl(pos, 256);\n-    cmpl(pos, 512);\n-    jcc(Assembler::aboveEqual, GHASH_AES_PARALLEL);\n-    vpaddd(ZTMP5, CTR_BLOCKx, COUNTER_INC_MASK, Assembler::AVX_512bit);\n-    vpaddd(ZTMP6, ZTMP5, COUNTER_INC_MASK, Assembler::AVX_512bit);\n-    vpaddd(ZTMP7, ZTMP6, COUNTER_INC_MASK, Assembler::AVX_512bit);\n-    vpaddd(ZTMP8, ZTMP7, COUNTER_INC_MASK, Assembler::AVX_512bit);\n-    jmp(AES_32_BLOCKS);\n-\n-    bind(GHASH_AES_PARALLEL);\n-    \/\/ Ghash16_encrypt16_parallel takes place in the order with three reduction values:\n-    \/\/ 1) First time -> cipher xor input ghash\n-    \/\/ 2) No reduction -> accumulate multiplication values\n-    \/\/ 3) Final reduction post 48 blocks -> new ghash value is computed for the next round\n-    \/\/ Reduction value = first time\n-    ghash16_encrypt16_parallel(key, avx512_subkeyHtbl, CTR_BLOCKx, AAD_HASHx, in, out, ct, pos, true, xmm24, true, rounds, ghash_pos, false, index, COUNTER_INC_MASK);\n-    addl(pos, 256);\n-    addl(ghash_pos, 256);\n-    index += 4;\n-\n-    \/\/ At this point we have processed 768 bytes of AES and 256 bytes of GHASH.\n-    \/\/ If the remaining length is less than 768, process remaining 512 bytes of ghash in GHASH_LAST_32 code\n-    subl(len, 768);\n-    cmpl(len, 768);\n-    jcc(Assembler::less, GHASH_LAST_32);\n-\n-    \/\/ AES 16 blocks and GHASH 16 blocks in parallel\n-    \/\/ For multiples of 48 blocks we will do ghash16_encrypt16 interleaved multiple times\n-    \/\/ Reduction value = no reduction means that the carryless multiplication values are accumulated for further calculations\n-    \/\/ Each call uses 4 subkeyHtbl values, so increment the index by 4.\n-    bind(GHASH_16_AES_16);\n-    \/\/ Reduction value = no reduction\n-    ghash16_encrypt16_parallel(key, avx512_subkeyHtbl, CTR_BLOCKx, AAD_HASHx, in, out, ct, pos, false, xmm24, false, rounds, ghash_pos, false, index, COUNTER_INC_MASK);\n-    addl(pos, 256);\n-    addl(ghash_pos, 256);\n-    index += 4;\n-    \/\/ Reduction value = final reduction means that the accumulated values have to be reduced as we have completed 48 blocks of ghash\n-    ghash16_encrypt16_parallel(key, avx512_subkeyHtbl, CTR_BLOCKx, AAD_HASHx, in, out, ct, pos, false, xmm24, false, rounds, ghash_pos, true, index, COUNTER_INC_MASK);\n-    addl(pos, 256);\n-    addl(ghash_pos, 256);\n-    \/\/ Calculated ghash value needs to be moved to AAD_HASHX so that we can restart the ghash16-aes16 pipeline\n-    movdqu(AAD_HASHx, ZTMP5);\n-    index = 0; \/\/ Reset subkeyHtbl index\n-\n-    \/\/ Restart the pipeline\n-    \/\/ Reduction value = first time\n-    ghash16_encrypt16_parallel(key, avx512_subkeyHtbl, CTR_BLOCKx, AAD_HASHx, in, out, ct, pos, true, xmm24, true, rounds, ghash_pos, false, index, COUNTER_INC_MASK);\n-    addl(pos, 256);\n-    addl(ghash_pos, 256);\n-    index += 4;\n-\n-    subl(len, 768);\n-    cmpl(len, 768);\n-    jcc(Assembler::greaterEqual, GHASH_16_AES_16);\n-\n-    \/\/ GHASH last 32 blocks processed here\n-    \/\/ GHASH products accumulated in ZMM27, ZMM25 and ZMM26 during GHASH16-AES16 operation is used\n-    bind(GHASH_LAST_32);\n-    \/\/ Use rbx as a pointer to the htbl; For last 32 blocks of GHASH, use key# 4-11 entry in subkeyHtbl\n-    movl(rbx, 256);\n-    \/\/ Load cipher blocks\n-    evmovdquq(ZTMP13, Address(ct, ghash_pos, Address::times_1, 0 * 64), Assembler::AVX_512bit);\n-    evmovdquq(ZTMP14, Address(ct, ghash_pos, Address::times_1, 1 * 64), Assembler::AVX_512bit);\n-    vpshufb(ZTMP13, ZTMP13, xmm24, Assembler::AVX_512bit);\n-    vpshufb(ZTMP14, ZTMP14, xmm24, Assembler::AVX_512bit);\n-    \/\/ Load ghash keys\n-    evmovdquq(ZTMP15, Address(avx512_subkeyHtbl, rbx, Address::times_1, 0 * 64), Assembler::AVX_512bit);\n-    evmovdquq(ZTMP16, Address(avx512_subkeyHtbl, rbx, Address::times_1, 1 * 64), Assembler::AVX_512bit);\n-\n-    \/\/ Ghash blocks 0 - 3\n-    carrylessMultiply(ZTMP2, ZTMP3, ZTMP4, ZTMP1, ZTMP13, ZTMP15);\n-    \/\/ Ghash blocks 4 - 7\n-    carrylessMultiply(ZTMP6, ZTMP7, ZTMP8, ZTMP5, ZTMP14, ZTMP16);\n-\n-    vpternlogq(ZTMP1, 0x96, ZTMP5, xmm27, Assembler::AVX_512bit); \/\/ ZTMP1 = ZTMP1 + ZTMP5 + zmm27\n-    vpternlogq(ZTMP2, 0x96, ZTMP6, xmm26, Assembler::AVX_512bit); \/\/ ZTMP2 = ZTMP2 + ZTMP6 + zmm26\n-    vpternlogq(ZTMP3, 0x96, ZTMP7, xmm25, Assembler::AVX_512bit); \/\/ ZTMP3 = ZTMP3 + ZTMP7 + zmm25\n-    evpxorq(ZTMP4, ZTMP4, ZTMP8, Assembler::AVX_512bit);          \/\/ ZTMP4 = ZTMP4 + ZTMP8\n-\n-    addl(ghash_pos, 128);\n-    addl(rbx, 128);\n-\n-    \/\/ Ghash remaining blocks\n-    bind(LOOP);\n-    cmpl(ghash_pos, pos);\n-    jcc(Assembler::aboveEqual, ACCUMULATE);\n-    \/\/ Load next cipher blocks and corresponding ghash keys\n-    evmovdquq(ZTMP13, Address(ct, ghash_pos, Address::times_1, 0 * 64), Assembler::AVX_512bit);\n-    evmovdquq(ZTMP14, Address(ct, ghash_pos, Address::times_1, 1 * 64), Assembler::AVX_512bit);\n-    vpshufb(ZTMP13, ZTMP13, xmm24, Assembler::AVX_512bit);\n-    vpshufb(ZTMP14, ZTMP14, xmm24, Assembler::AVX_512bit);\n-    evmovdquq(ZTMP15, Address(avx512_subkeyHtbl, rbx, Address::times_1, 0 * 64), Assembler::AVX_512bit);\n-    evmovdquq(ZTMP16, Address(avx512_subkeyHtbl, rbx, Address::times_1, 1 * 64), Assembler::AVX_512bit);\n-\n-    \/\/ ghash blocks 0 - 3\n-    carrylessMultiply(ZTMP6, ZTMP7, ZTMP8, ZTMP5, ZTMP13, ZTMP15);\n-\n-    \/\/ ghash blocks 4 - 7\n-    carrylessMultiply(ZTMP10, ZTMP11, ZTMP12, ZTMP9, ZTMP14, ZTMP16);\n-\n-    \/\/ update sums\n-    \/\/ ZTMP1 = ZTMP1 + ZTMP5 + ZTMP9\n-    \/\/ ZTMP2 = ZTMP2 + ZTMP6 + ZTMP10\n-    \/\/ ZTMP3 = ZTMP3 + ZTMP7 xor ZTMP11\n-    \/\/ ZTMP4 = ZTMP4 + ZTMP8 xor ZTMP12\n-    xorGHASH(ZTMP1, ZTMP2, ZTMP3, ZTMP4, ZTMP5, ZTMP9, ZTMP6, ZTMP10, ZTMP7, ZTMP11, ZTMP8, ZTMP12);\n-    addl(ghash_pos, 128);\n-    addl(rbx, 128);\n-    jmp(LOOP);\n-\n-    \/\/ Integrate ZTMP3\/ZTMP4 into ZTMP1 and ZTMP2\n-    bind(ACCUMULATE);\n-    evpxorq(ZTMP3, ZTMP3, ZTMP4, Assembler::AVX_512bit);\n-    vpsrldq(ZTMP7, ZTMP3, 8, Assembler::AVX_512bit);\n-    vpslldq(ZTMP8, ZTMP3, 8, Assembler::AVX_512bit);\n-    evpxorq(ZTMP1, ZTMP1, ZTMP7, Assembler::AVX_512bit);\n-    evpxorq(ZTMP2, ZTMP2, ZTMP8, Assembler::AVX_512bit);\n-\n-    \/\/ Add ZTMP1 and ZTMP2 128 - bit words horizontally\n-    vhpxori4x128(ZTMP1, ZTMP11);\n-    vhpxori4x128(ZTMP2, ZTMP12);\n-    \/\/ Load reduction polynomial and compute final reduction\n-    evmovdquq(ZTMP15, ExternalAddress(StubRoutines::x86::ghash_polynomial512_addr()), Assembler::AVX_512bit, rbx);\n-    vclmul_reduce(AAD_HASHx, ZTMP15, ZTMP1, ZTMP2, ZTMP3, ZTMP4);\n-\n-    \/\/ Pre-increment counter for next operation\n-    vpaddd(CTR_BLOCKx, CTR_BLOCKx, xmm18, Assembler::AVX_128bit);\n-    \/\/ Shuffle counter and save the updated value\n-    vpshufb(CTR_BLOCKx, CTR_BLOCKx, xmm24, Assembler::AVX_512bit);\n-    movdqu(Address(counter, 0), CTR_BLOCKx);\n-    \/\/ Load ghash lswap mask\n-    movdqu(xmm24, ExternalAddress(StubRoutines::x86::ghash_long_swap_mask_addr()));\n-    \/\/ Shuffle ghash using lbswap_mask and store it\n-    vpshufb(AAD_HASHx, AAD_HASHx, xmm24, Assembler::AVX_128bit);\n-    movdqu(Address(state, 0), AAD_HASHx);\n-    jmp(ENC_DEC_DONE);\n-\n-    bind(GENERATE_HTBL_48_BLKS);\n-    generateHtbl_48_block_zmm(subkeyHtbl, avx512_subkeyHtbl);\n-\n-    bind(ENC_DEC_DONE);\n-    movq(rax, pos);\n-}\n-\n-#endif \/\/ _LP64\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86_aes.cpp","additions":0,"deletions":1892,"binary":false,"changes":1892,"status":"deleted"},{"patch":"@@ -1262,677 +1262,0 @@\n-\/\/ AES intrinsic stubs\n-\n-address StubGenerator::generate_key_shuffle_mask() {\n-  __ align(16);\n-  StubCodeMark mark(this, \"StubRoutines\", \"key_shuffle_mask\");\n-  address start = __ pc();\n-\n-  __ emit_data64( 0x0405060700010203, relocInfo::none );\n-  __ emit_data64( 0x0c0d0e0f08090a0b, relocInfo::none );\n-\n-  return start;\n-}\n-\n-address StubGenerator::generate_counter_shuffle_mask() {\n-  __ align(16);\n-  StubCodeMark mark(this, \"StubRoutines\", \"counter_shuffle_mask\");\n-  address start = __ pc();\n-\n-  __ emit_data64(0x08090a0b0c0d0e0f, relocInfo::none);\n-  __ emit_data64(0x0001020304050607, relocInfo::none);\n-\n-  return start;\n-}\n-\n-\/\/ Utility routine for loading a 128-bit key word in little endian format\n-\/\/ can optionally specify that the shuffle mask is already in an xmmregister\n-void StubGenerator::load_key(XMMRegister xmmdst, Register key, int offset, XMMRegister xmm_shuf_mask) {\n-  __ movdqu(xmmdst, Address(key, offset));\n-  if (xmm_shuf_mask != xnoreg) {\n-    __ pshufb(xmmdst, xmm_shuf_mask);\n-  } else {\n-    __ pshufb(xmmdst, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()));\n-  }\n-}\n-\n-\/\/ Utility routine for increase 128bit counter (iv in CTR mode)\n-void StubGenerator::inc_counter(Register reg, XMMRegister xmmdst, int inc_delta, Label& next_block) {\n-  __ pextrq(reg, xmmdst, 0x0);\n-  __ addq(reg, inc_delta);\n-  __ pinsrq(xmmdst, reg, 0x0);\n-  __ jcc(Assembler::carryClear, next_block); \/\/ jump if no carry\n-  __ pextrq(reg, xmmdst, 0x01); \/\/ Carry\n-  __ addq(reg, 0x01);\n-  __ pinsrq(xmmdst, reg, 0x01); \/\/Carry end\n-  __ BIND(next_block);          \/\/ next instruction\n-}\n-\n-\/\/ Arguments:\n-\/\/\n-\/\/ Inputs:\n-\/\/   c_rarg0   - source byte array address\n-\/\/   c_rarg1   - destination byte array address\n-\/\/   c_rarg2   - K (key) in little endian int array\n-\/\/\n-address StubGenerator::generate_aescrypt_encryptBlock() {\n-  assert(UseAES, \"need AES instructions and misaligned SSE support\");\n-  __ align(CodeEntryAlignment);\n-  StubCodeMark mark(this, \"StubRoutines\", \"aescrypt_encryptBlock\");\n-  Label L_doLast;\n-  address start = __ pc();\n-\n-  const Register from        = c_rarg0;  \/\/ source array address\n-  const Register to          = c_rarg1;  \/\/ destination array address\n-  const Register key         = c_rarg2;  \/\/ key array address\n-  const Register keylen      = rax;\n-\n-  const XMMRegister xmm_result = xmm0;\n-  const XMMRegister xmm_key_shuf_mask = xmm1;\n-  \/\/ On win64 xmm6-xmm15 must be preserved so don't use them.\n-  const XMMRegister xmm_temp1  = xmm2;\n-  const XMMRegister xmm_temp2  = xmm3;\n-  const XMMRegister xmm_temp3  = xmm4;\n-  const XMMRegister xmm_temp4  = xmm5;\n-\n-  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-\n-  \/\/ keylen could be only {11, 13, 15} * 4 = {44, 52, 60}\n-  __ movl(keylen, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));\n-\n-  __ movdqu(xmm_key_shuf_mask, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()));\n-  __ movdqu(xmm_result, Address(from, 0));  \/\/ get 16 bytes of input\n-\n-  \/\/ For encryption, the java expanded key ordering is just what we need\n-  \/\/ we don't know if the key is aligned, hence not using load-execute form\n-\n-  load_key(xmm_temp1, key, 0x00, xmm_key_shuf_mask);\n-  __ pxor(xmm_result, xmm_temp1);\n-\n-  load_key(xmm_temp1, key, 0x10, xmm_key_shuf_mask);\n-  load_key(xmm_temp2, key, 0x20, xmm_key_shuf_mask);\n-  load_key(xmm_temp3, key, 0x30, xmm_key_shuf_mask);\n-  load_key(xmm_temp4, key, 0x40, xmm_key_shuf_mask);\n-\n-  __ aesenc(xmm_result, xmm_temp1);\n-  __ aesenc(xmm_result, xmm_temp2);\n-  __ aesenc(xmm_result, xmm_temp3);\n-  __ aesenc(xmm_result, xmm_temp4);\n-\n-  load_key(xmm_temp1, key, 0x50, xmm_key_shuf_mask);\n-  load_key(xmm_temp2, key, 0x60, xmm_key_shuf_mask);\n-  load_key(xmm_temp3, key, 0x70, xmm_key_shuf_mask);\n-  load_key(xmm_temp4, key, 0x80, xmm_key_shuf_mask);\n-\n-  __ aesenc(xmm_result, xmm_temp1);\n-  __ aesenc(xmm_result, xmm_temp2);\n-  __ aesenc(xmm_result, xmm_temp3);\n-  __ aesenc(xmm_result, xmm_temp4);\n-\n-  load_key(xmm_temp1, key, 0x90, xmm_key_shuf_mask);\n-  load_key(xmm_temp2, key, 0xa0, xmm_key_shuf_mask);\n-\n-  __ cmpl(keylen, 44);\n-  __ jccb(Assembler::equal, L_doLast);\n-\n-  __ aesenc(xmm_result, xmm_temp1);\n-  __ aesenc(xmm_result, xmm_temp2);\n-\n-  load_key(xmm_temp1, key, 0xb0, xmm_key_shuf_mask);\n-  load_key(xmm_temp2, key, 0xc0, xmm_key_shuf_mask);\n-\n-  __ cmpl(keylen, 52);\n-  __ jccb(Assembler::equal, L_doLast);\n-\n-  __ aesenc(xmm_result, xmm_temp1);\n-  __ aesenc(xmm_result, xmm_temp2);\n-\n-  load_key(xmm_temp1, key, 0xd0, xmm_key_shuf_mask);\n-  load_key(xmm_temp2, key, 0xe0, xmm_key_shuf_mask);\n-\n-  __ BIND(L_doLast);\n-  __ aesenc(xmm_result, xmm_temp1);\n-  __ aesenclast(xmm_result, xmm_temp2);\n-  __ movdqu(Address(to, 0), xmm_result);        \/\/ store the result\n-  __ xorptr(rax, rax); \/\/ return 0\n-  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-  __ ret(0);\n-\n-  return start;\n-}\n-\n-\n-\/\/ Arguments:\n-\/\/\n-\/\/ Inputs:\n-\/\/   c_rarg0   - source byte array address\n-\/\/   c_rarg1   - destination byte array address\n-\/\/   c_rarg2   - K (key) in little endian int array\n-\/\/\n-address StubGenerator::generate_aescrypt_decryptBlock() {\n-  assert(UseAES, \"need AES instructions and misaligned SSE support\");\n-  __ align(CodeEntryAlignment);\n-  StubCodeMark mark(this, \"StubRoutines\", \"aescrypt_decryptBlock\");\n-  Label L_doLast;\n-  address start = __ pc();\n-\n-  const Register from        = c_rarg0;  \/\/ source array address\n-  const Register to          = c_rarg1;  \/\/ destination array address\n-  const Register key         = c_rarg2;  \/\/ key array address\n-  const Register keylen      = rax;\n-\n-  const XMMRegister xmm_result = xmm0;\n-  const XMMRegister xmm_key_shuf_mask = xmm1;\n-  \/\/ On win64 xmm6-xmm15 must be preserved so don't use them.\n-  const XMMRegister xmm_temp1  = xmm2;\n-  const XMMRegister xmm_temp2  = xmm3;\n-  const XMMRegister xmm_temp3  = xmm4;\n-  const XMMRegister xmm_temp4  = xmm5;\n-\n-  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-\n-  \/\/ keylen could be only {11, 13, 15} * 4 = {44, 52, 60}\n-  __ movl(keylen, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));\n-\n-  __ movdqu(xmm_key_shuf_mask, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()));\n-  __ movdqu(xmm_result, Address(from, 0));\n-\n-  \/\/ for decryption java expanded key ordering is rotated one position from what we want\n-  \/\/ so we start from 0x10 here and hit 0x00 last\n-  \/\/ we don't know if the key is aligned, hence not using load-execute form\n-  load_key(xmm_temp1, key, 0x10, xmm_key_shuf_mask);\n-  load_key(xmm_temp2, key, 0x20, xmm_key_shuf_mask);\n-  load_key(xmm_temp3, key, 0x30, xmm_key_shuf_mask);\n-  load_key(xmm_temp4, key, 0x40, xmm_key_shuf_mask);\n-\n-  __ pxor  (xmm_result, xmm_temp1);\n-  __ aesdec(xmm_result, xmm_temp2);\n-  __ aesdec(xmm_result, xmm_temp3);\n-  __ aesdec(xmm_result, xmm_temp4);\n-\n-  load_key(xmm_temp1, key, 0x50, xmm_key_shuf_mask);\n-  load_key(xmm_temp2, key, 0x60, xmm_key_shuf_mask);\n-  load_key(xmm_temp3, key, 0x70, xmm_key_shuf_mask);\n-  load_key(xmm_temp4, key, 0x80, xmm_key_shuf_mask);\n-\n-  __ aesdec(xmm_result, xmm_temp1);\n-  __ aesdec(xmm_result, xmm_temp2);\n-  __ aesdec(xmm_result, xmm_temp3);\n-  __ aesdec(xmm_result, xmm_temp4);\n-\n-  load_key(xmm_temp1, key, 0x90, xmm_key_shuf_mask);\n-  load_key(xmm_temp2, key, 0xa0, xmm_key_shuf_mask);\n-  load_key(xmm_temp3, key, 0x00, xmm_key_shuf_mask);\n-\n-  __ cmpl(keylen, 44);\n-  __ jccb(Assembler::equal, L_doLast);\n-\n-  __ aesdec(xmm_result, xmm_temp1);\n-  __ aesdec(xmm_result, xmm_temp2);\n-\n-  load_key(xmm_temp1, key, 0xb0, xmm_key_shuf_mask);\n-  load_key(xmm_temp2, key, 0xc0, xmm_key_shuf_mask);\n-\n-  __ cmpl(keylen, 52);\n-  __ jccb(Assembler::equal, L_doLast);\n-\n-  __ aesdec(xmm_result, xmm_temp1);\n-  __ aesdec(xmm_result, xmm_temp2);\n-\n-  load_key(xmm_temp1, key, 0xd0, xmm_key_shuf_mask);\n-  load_key(xmm_temp2, key, 0xe0, xmm_key_shuf_mask);\n-\n-  __ BIND(L_doLast);\n-  __ aesdec(xmm_result, xmm_temp1);\n-  __ aesdec(xmm_result, xmm_temp2);\n-\n-  \/\/ for decryption the aesdeclast operation is always on key+0x00\n-  __ aesdeclast(xmm_result, xmm_temp3);\n-  __ movdqu(Address(to, 0), xmm_result);  \/\/ store the result\n-  __ xorptr(rax, rax); \/\/ return 0\n-  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-  __ ret(0);\n-\n-  return start;\n-}\n-\n-\n-\/\/ Arguments:\n-\/\/\n-\/\/ Inputs:\n-\/\/   c_rarg0   - source byte array address\n-\/\/   c_rarg1   - destination byte array address\n-\/\/   c_rarg2   - K (key) in little endian int array\n-\/\/   c_rarg3   - r vector byte array address\n-\/\/   c_rarg4   - input length\n-\/\/\n-\/\/ Output:\n-\/\/   rax       - input length\n-\/\/\n-address StubGenerator::generate_cipherBlockChaining_encryptAESCrypt() {\n-  assert(UseAES, \"need AES instructions and misaligned SSE support\");\n-  __ align(CodeEntryAlignment);\n-  StubCodeMark mark(this, \"StubRoutines\", \"cipherBlockChaining_encryptAESCrypt\");\n-  address start = __ pc();\n-\n-  Label L_exit, L_key_192_256, L_key_256, L_loopTop_128, L_loopTop_192, L_loopTop_256;\n-  const Register from        = c_rarg0;  \/\/ source array address\n-  const Register to          = c_rarg1;  \/\/ destination array address\n-  const Register key         = c_rarg2;  \/\/ key array address\n-  const Register rvec        = c_rarg3;  \/\/ r byte array initialized from initvector array address\n-                                         \/\/ and left with the results of the last encryption block\n-#ifndef _WIN64\n-  const Register len_reg     = c_rarg4;  \/\/ src len (must be multiple of blocksize 16)\n-#else\n-  const Address  len_mem(rbp, 6 * wordSize);  \/\/ length is on stack on Win64\n-  const Register len_reg     = r11;      \/\/ pick the volatile windows register\n-#endif\n-  const Register pos         = rax;\n-\n-  \/\/ xmm register assignments for the loops below\n-  const XMMRegister xmm_result = xmm0;\n-  const XMMRegister xmm_temp   = xmm1;\n-  \/\/ keys 0-10 preloaded into xmm2-xmm12\n-  const int XMM_REG_NUM_KEY_FIRST = 2;\n-  const int XMM_REG_NUM_KEY_LAST  = 15;\n-  const XMMRegister xmm_key0   = as_XMMRegister(XMM_REG_NUM_KEY_FIRST);\n-  const XMMRegister xmm_key10  = as_XMMRegister(XMM_REG_NUM_KEY_FIRST+10);\n-  const XMMRegister xmm_key11  = as_XMMRegister(XMM_REG_NUM_KEY_FIRST+11);\n-  const XMMRegister xmm_key12  = as_XMMRegister(XMM_REG_NUM_KEY_FIRST+12);\n-  const XMMRegister xmm_key13  = as_XMMRegister(XMM_REG_NUM_KEY_FIRST+13);\n-\n-  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-\n-#ifdef _WIN64\n-  \/\/ on win64, fill len_reg from stack position\n-  __ movl(len_reg, len_mem);\n-#else\n-  __ push(len_reg); \/\/ Save\n-#endif\n-\n-  const XMMRegister xmm_key_shuf_mask = xmm_temp;  \/\/ used temporarily to swap key bytes up front\n-  __ movdqu(xmm_key_shuf_mask, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()));\n-  \/\/ load up xmm regs xmm2 thru xmm12 with key 0x00 - 0xa0\n-  for (int rnum = XMM_REG_NUM_KEY_FIRST, offset = 0x00; rnum <= XMM_REG_NUM_KEY_FIRST+10; rnum++) {\n-    load_key(as_XMMRegister(rnum), key, offset, xmm_key_shuf_mask);\n-    offset += 0x10;\n-  }\n-  __ movdqu(xmm_result, Address(rvec, 0x00));   \/\/ initialize xmm_result with r vec\n-\n-  \/\/ now split to different paths depending on the keylen (len in ints of AESCrypt.KLE array (52=192, or 60=256))\n-  __ movl(rax, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));\n-  __ cmpl(rax, 44);\n-  __ jcc(Assembler::notEqual, L_key_192_256);\n-\n-  \/\/ 128 bit code follows here\n-  __ movptr(pos, 0);\n-  __ align(OptoLoopAlignment);\n-\n-  __ BIND(L_loopTop_128);\n-  __ movdqu(xmm_temp, Address(from, pos, Address::times_1, 0));   \/\/ get next 16 bytes of input\n-  __ pxor  (xmm_result, xmm_temp);               \/\/ xor with the current r vector\n-  __ pxor  (xmm_result, xmm_key0);               \/\/ do the aes rounds\n-  for (int rnum = XMM_REG_NUM_KEY_FIRST + 1; rnum <= XMM_REG_NUM_KEY_FIRST + 9; rnum++) {\n-    __ aesenc(xmm_result, as_XMMRegister(rnum));\n-  }\n-  __ aesenclast(xmm_result, xmm_key10);\n-  __ movdqu(Address(to, pos, Address::times_1, 0), xmm_result);     \/\/ store into the next 16 bytes of output\n-  \/\/ no need to store r to memory until we exit\n-  __ addptr(pos, AESBlockSize);\n-  __ subptr(len_reg, AESBlockSize);\n-  __ jcc(Assembler::notEqual, L_loopTop_128);\n-\n-  __ BIND(L_exit);\n-  __ movdqu(Address(rvec, 0), xmm_result);     \/\/ final value of r stored in rvec of CipherBlockChaining object\n-\n-#ifdef _WIN64\n-  __ movl(rax, len_mem);\n-#else\n-  __ pop(rax); \/\/ return length\n-#endif\n-  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-  __ ret(0);\n-\n-  __ BIND(L_key_192_256);\n-  \/\/ here rax = len in ints of AESCrypt.KLE array (52=192, or 60=256)\n-  load_key(xmm_key11, key, 0xb0, xmm_key_shuf_mask);\n-  load_key(xmm_key12, key, 0xc0, xmm_key_shuf_mask);\n-  __ cmpl(rax, 52);\n-  __ jcc(Assembler::notEqual, L_key_256);\n-\n-  \/\/ 192-bit code follows here (could be changed to use more xmm registers)\n-  __ movptr(pos, 0);\n-  __ align(OptoLoopAlignment);\n-\n-  __ BIND(L_loopTop_192);\n-  __ movdqu(xmm_temp, Address(from, pos, Address::times_1, 0));   \/\/ get next 16 bytes of input\n-  __ pxor  (xmm_result, xmm_temp);               \/\/ xor with the current r vector\n-  __ pxor  (xmm_result, xmm_key0);               \/\/ do the aes rounds\n-  for (int rnum = XMM_REG_NUM_KEY_FIRST + 1; rnum  <= XMM_REG_NUM_KEY_FIRST + 11; rnum++) {\n-    __ aesenc(xmm_result, as_XMMRegister(rnum));\n-  }\n-  __ aesenclast(xmm_result, xmm_key12);\n-  __ movdqu(Address(to, pos, Address::times_1, 0), xmm_result);     \/\/ store into the next 16 bytes of output\n-  \/\/ no need to store r to memory until we exit\n-  __ addptr(pos, AESBlockSize);\n-  __ subptr(len_reg, AESBlockSize);\n-  __ jcc(Assembler::notEqual, L_loopTop_192);\n-  __ jmp(L_exit);\n-\n-  __ BIND(L_key_256);\n-  \/\/ 256-bit code follows here (could be changed to use more xmm registers)\n-  load_key(xmm_key13, key, 0xd0, xmm_key_shuf_mask);\n-  __ movptr(pos, 0);\n-  __ align(OptoLoopAlignment);\n-\n-  __ BIND(L_loopTop_256);\n-  __ movdqu(xmm_temp, Address(from, pos, Address::times_1, 0));   \/\/ get next 16 bytes of input\n-  __ pxor  (xmm_result, xmm_temp);               \/\/ xor with the current r vector\n-  __ pxor  (xmm_result, xmm_key0);               \/\/ do the aes rounds\n-  for (int rnum = XMM_REG_NUM_KEY_FIRST + 1; rnum  <= XMM_REG_NUM_KEY_FIRST + 13; rnum++) {\n-    __ aesenc(xmm_result, as_XMMRegister(rnum));\n-  }\n-  load_key(xmm_temp, key, 0xe0);\n-  __ aesenclast(xmm_result, xmm_temp);\n-  __ movdqu(Address(to, pos, Address::times_1, 0), xmm_result);     \/\/ store into the next 16 bytes of output\n-  \/\/ no need to store r to memory until we exit\n-  __ addptr(pos, AESBlockSize);\n-  __ subptr(len_reg, AESBlockSize);\n-  __ jcc(Assembler::notEqual, L_loopTop_256);\n-  __ jmp(L_exit);\n-\n-  return start;\n-}\n-\n-\/\/ This is a version of CBC\/AES Decrypt which does 4 blocks in a loop at a time\n-\/\/ to hide instruction latency\n-\/\/\n-\/\/ Arguments:\n-\/\/\n-\/\/ Inputs:\n-\/\/   c_rarg0   - source byte array address\n-\/\/   c_rarg1   - destination byte array address\n-\/\/   c_rarg2   - K (key) in little endian int array\n-\/\/   c_rarg3   - r vector byte array address\n-\/\/   c_rarg4   - input length\n-\/\/\n-\/\/ Output:\n-\/\/   rax       - input length\n-\/\/\n-address StubGenerator::generate_cipherBlockChaining_decryptAESCrypt_Parallel() {\n-  assert(UseAES, \"need AES instructions and misaligned SSE support\");\n-  __ align(CodeEntryAlignment);\n-  StubCodeMark mark(this, \"StubRoutines\", \"cipherBlockChaining_decryptAESCrypt\");\n-  address start = __ pc();\n-\n-  const Register from        = c_rarg0;  \/\/ source array address\n-  const Register to          = c_rarg1;  \/\/ destination array address\n-  const Register key         = c_rarg2;  \/\/ key array address\n-  const Register rvec        = c_rarg3;  \/\/ r byte array initialized from initvector array address\n-                                         \/\/ and left with the results of the last encryption block\n-#ifndef _WIN64\n-  const Register len_reg     = c_rarg4;  \/\/ src len (must be multiple of blocksize 16)\n-#else\n-  const Address  len_mem(rbp, 6 * wordSize);  \/\/ length is on stack on Win64\n-  const Register len_reg     = r11;      \/\/ pick the volatile windows register\n-#endif\n-  const Register pos         = rax;\n-\n-  const int PARALLEL_FACTOR = 4;\n-  const int ROUNDS[3] = { 10, 12, 14 }; \/\/ aes rounds for key128, key192, key256\n-\n-  Label L_exit;\n-  Label L_singleBlock_loopTopHead[3]; \/\/ 128, 192, 256\n-  Label L_singleBlock_loopTopHead2[3]; \/\/ 128, 192, 256\n-  Label L_singleBlock_loopTop[3]; \/\/ 128, 192, 256\n-  Label L_multiBlock_loopTopHead[3]; \/\/ 128, 192, 256\n-  Label L_multiBlock_loopTop[3]; \/\/ 128, 192, 256\n-\n-  \/\/ keys 0-10 preloaded into xmm5-xmm15\n-  const int XMM_REG_NUM_KEY_FIRST = 5;\n-  const int XMM_REG_NUM_KEY_LAST  = 15;\n-  const XMMRegister xmm_key_first = as_XMMRegister(XMM_REG_NUM_KEY_FIRST);\n-  const XMMRegister xmm_key_last  = as_XMMRegister(XMM_REG_NUM_KEY_LAST);\n-\n-  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-\n-#ifdef _WIN64\n-  \/\/ on win64, fill len_reg from stack position\n-  __ movl(len_reg, len_mem);\n-#else\n-  __ push(len_reg); \/\/ Save\n-#endif\n-  __ push(rbx);\n-  \/\/ the java expanded key ordering is rotated one position from what we want\n-  \/\/ so we start from 0x10 here and hit 0x00 last\n-  const XMMRegister xmm_key_shuf_mask = xmm1;  \/\/ used temporarily to swap key bytes up front\n-  __ movdqu(xmm_key_shuf_mask, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()));\n-  \/\/ load up xmm regs 5 thru 15 with key 0x10 - 0xa0 - 0x00\n-  for (int rnum = XMM_REG_NUM_KEY_FIRST, offset = 0x10; rnum < XMM_REG_NUM_KEY_LAST; rnum++) {\n-    load_key(as_XMMRegister(rnum), key, offset, xmm_key_shuf_mask);\n-    offset += 0x10;\n-  }\n-  load_key(xmm_key_last, key, 0x00, xmm_key_shuf_mask);\n-\n-  const XMMRegister xmm_prev_block_cipher = xmm1;  \/\/ holds cipher of previous block\n-\n-  \/\/ registers holding the four results in the parallelized loop\n-  const XMMRegister xmm_result0 = xmm0;\n-  const XMMRegister xmm_result1 = xmm2;\n-  const XMMRegister xmm_result2 = xmm3;\n-  const XMMRegister xmm_result3 = xmm4;\n-\n-  __ movdqu(xmm_prev_block_cipher, Address(rvec, 0x00));   \/\/ initialize with initial rvec\n-\n-  __ xorptr(pos, pos);\n-\n-  \/\/ now split to different paths depending on the keylen (len in ints of AESCrypt.KLE array (52=192, or 60=256))\n-  __ movl(rbx, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));\n-  __ cmpl(rbx, 52);\n-  __ jcc(Assembler::equal, L_multiBlock_loopTopHead[1]);\n-  __ cmpl(rbx, 60);\n-  __ jcc(Assembler::equal, L_multiBlock_loopTopHead[2]);\n-\n-#define DoFour(opc, src_reg)           \\\n-__ opc(xmm_result0, src_reg);         \\\n-__ opc(xmm_result1, src_reg);         \\\n-__ opc(xmm_result2, src_reg);         \\\n-__ opc(xmm_result3, src_reg);         \\\n-\n-  for (int k = 0; k < 3; ++k) {\n-    __ BIND(L_multiBlock_loopTopHead[k]);\n-    if (k != 0) {\n-      __ cmpptr(len_reg, PARALLEL_FACTOR * AESBlockSize); \/\/ see if at least 4 blocks left\n-      __ jcc(Assembler::less, L_singleBlock_loopTopHead2[k]);\n-    }\n-    if (k == 1) {\n-      __ subptr(rsp, 6 * wordSize);\n-      __ movdqu(Address(rsp, 0), xmm15); \/\/save last_key from xmm15\n-      load_key(xmm15, key, 0xb0); \/\/ 0xb0; 192-bit key goes up to 0xc0\n-      __ movdqu(Address(rsp, 2 * wordSize), xmm15);\n-      load_key(xmm1, key, 0xc0);  \/\/ 0xc0;\n-      __ movdqu(Address(rsp, 4 * wordSize), xmm1);\n-    } else if (k == 2) {\n-      __ subptr(rsp, 10 * wordSize);\n-      __ movdqu(Address(rsp, 0), xmm15); \/\/save last_key from xmm15\n-      load_key(xmm15, key, 0xd0); \/\/ 0xd0; 256-bit key goes up to 0xe0\n-      __ movdqu(Address(rsp, 6 * wordSize), xmm15);\n-      load_key(xmm1, key, 0xe0);  \/\/ 0xe0;\n-      __ movdqu(Address(rsp, 8 * wordSize), xmm1);\n-      load_key(xmm15, key, 0xb0); \/\/ 0xb0;\n-      __ movdqu(Address(rsp, 2 * wordSize), xmm15);\n-      load_key(xmm1, key, 0xc0);  \/\/ 0xc0;\n-      __ movdqu(Address(rsp, 4 * wordSize), xmm1);\n-    }\n-    __ align(OptoLoopAlignment);\n-    __ BIND(L_multiBlock_loopTop[k]);\n-    __ cmpptr(len_reg, PARALLEL_FACTOR * AESBlockSize); \/\/ see if at least 4 blocks left\n-    __ jcc(Assembler::less, L_singleBlock_loopTopHead[k]);\n-\n-    if  (k != 0) {\n-      __ movdqu(xmm15, Address(rsp, 2 * wordSize));\n-      __ movdqu(xmm1, Address(rsp, 4 * wordSize));\n-    }\n-\n-    __ movdqu(xmm_result0, Address(from, pos, Address::times_1, 0 * AESBlockSize)); \/\/ get next 4 blocks into xmmresult registers\n-    __ movdqu(xmm_result1, Address(from, pos, Address::times_1, 1 * AESBlockSize));\n-    __ movdqu(xmm_result2, Address(from, pos, Address::times_1, 2 * AESBlockSize));\n-    __ movdqu(xmm_result3, Address(from, pos, Address::times_1, 3 * AESBlockSize));\n-\n-    DoFour(pxor, xmm_key_first);\n-    if (k == 0) {\n-      for (int rnum = 1; rnum < ROUNDS[k]; rnum++) {\n-        DoFour(aesdec, as_XMMRegister(rnum + XMM_REG_NUM_KEY_FIRST));\n-      }\n-      DoFour(aesdeclast, xmm_key_last);\n-    } else if (k == 1) {\n-      for (int rnum = 1; rnum <= ROUNDS[k]-2; rnum++) {\n-        DoFour(aesdec, as_XMMRegister(rnum + XMM_REG_NUM_KEY_FIRST));\n-      }\n-      __ movdqu(xmm_key_last, Address(rsp, 0)); \/\/ xmm15 needs to be loaded again.\n-      DoFour(aesdec, xmm1);  \/\/ key : 0xc0\n-      __ movdqu(xmm_prev_block_cipher, Address(rvec, 0x00));  \/\/ xmm1 needs to be loaded again\n-      DoFour(aesdeclast, xmm_key_last);\n-    } else if (k == 2) {\n-      for (int rnum = 1; rnum <= ROUNDS[k] - 4; rnum++) {\n-        DoFour(aesdec, as_XMMRegister(rnum + XMM_REG_NUM_KEY_FIRST));\n-      }\n-      DoFour(aesdec, xmm1);  \/\/ key : 0xc0\n-      __ movdqu(xmm15, Address(rsp, 6 * wordSize));\n-      __ movdqu(xmm1, Address(rsp, 8 * wordSize));\n-      DoFour(aesdec, xmm15);  \/\/ key : 0xd0\n-      __ movdqu(xmm_key_last, Address(rsp, 0)); \/\/ xmm15 needs to be loaded again.\n-      DoFour(aesdec, xmm1);  \/\/ key : 0xe0\n-      __ movdqu(xmm_prev_block_cipher, Address(rvec, 0x00));  \/\/ xmm1 needs to be loaded again\n-      DoFour(aesdeclast, xmm_key_last);\n-    }\n-\n-    \/\/ for each result, xor with the r vector of previous cipher block\n-    __ pxor(xmm_result0, xmm_prev_block_cipher);\n-    __ movdqu(xmm_prev_block_cipher, Address(from, pos, Address::times_1, 0 * AESBlockSize));\n-    __ pxor(xmm_result1, xmm_prev_block_cipher);\n-    __ movdqu(xmm_prev_block_cipher, Address(from, pos, Address::times_1, 1 * AESBlockSize));\n-    __ pxor(xmm_result2, xmm_prev_block_cipher);\n-    __ movdqu(xmm_prev_block_cipher, Address(from, pos, Address::times_1, 2 * AESBlockSize));\n-    __ pxor(xmm_result3, xmm_prev_block_cipher);\n-    __ movdqu(xmm_prev_block_cipher, Address(from, pos, Address::times_1, 3 * AESBlockSize));   \/\/ this will carry over to next set of blocks\n-    if (k != 0) {\n-      __ movdqu(Address(rvec, 0x00), xmm_prev_block_cipher);\n-    }\n-\n-    __ movdqu(Address(to, pos, Address::times_1, 0 * AESBlockSize), xmm_result0);     \/\/ store 4 results into the next 64 bytes of output\n-    __ movdqu(Address(to, pos, Address::times_1, 1 * AESBlockSize), xmm_result1);\n-    __ movdqu(Address(to, pos, Address::times_1, 2 * AESBlockSize), xmm_result2);\n-    __ movdqu(Address(to, pos, Address::times_1, 3 * AESBlockSize), xmm_result3);\n-\n-    __ addptr(pos, PARALLEL_FACTOR * AESBlockSize);\n-    __ subptr(len_reg, PARALLEL_FACTOR * AESBlockSize);\n-    __ jmp(L_multiBlock_loopTop[k]);\n-\n-    \/\/ registers used in the non-parallelized loops\n-    \/\/ xmm register assignments for the loops below\n-    const XMMRegister xmm_result = xmm0;\n-    const XMMRegister xmm_prev_block_cipher_save = xmm2;\n-    const XMMRegister xmm_key11 = xmm3;\n-    const XMMRegister xmm_key12 = xmm4;\n-    const XMMRegister key_tmp = xmm4;\n-\n-    __ BIND(L_singleBlock_loopTopHead[k]);\n-    if (k == 1) {\n-      __ addptr(rsp, 6 * wordSize);\n-    } else if (k == 2) {\n-      __ addptr(rsp, 10 * wordSize);\n-    }\n-    __ cmpptr(len_reg, 0); \/\/ any blocks left??\n-    __ jcc(Assembler::equal, L_exit);\n-    __ BIND(L_singleBlock_loopTopHead2[k]);\n-    if (k == 1) {\n-      load_key(xmm_key11, key, 0xb0); \/\/ 0xb0; 192-bit key goes up to 0xc0\n-      load_key(xmm_key12, key, 0xc0); \/\/ 0xc0; 192-bit key goes up to 0xc0\n-    }\n-    if (k == 2) {\n-      load_key(xmm_key11, key, 0xb0); \/\/ 0xb0; 256-bit key goes up to 0xe0\n-    }\n-    __ align(OptoLoopAlignment);\n-    __ BIND(L_singleBlock_loopTop[k]);\n-    __ movdqu(xmm_result, Address(from, pos, Address::times_1, 0)); \/\/ get next 16 bytes of cipher input\n-    __ movdqa(xmm_prev_block_cipher_save, xmm_result); \/\/ save for next r vector\n-    __ pxor(xmm_result, xmm_key_first); \/\/ do the aes dec rounds\n-    for (int rnum = 1; rnum <= 9 ; rnum++) {\n-        __ aesdec(xmm_result, as_XMMRegister(rnum + XMM_REG_NUM_KEY_FIRST));\n-    }\n-    if (k == 1) {\n-      __ aesdec(xmm_result, xmm_key11);\n-      __ aesdec(xmm_result, xmm_key12);\n-    }\n-    if (k == 2) {\n-      __ aesdec(xmm_result, xmm_key11);\n-      load_key(key_tmp, key, 0xc0);\n-      __ aesdec(xmm_result, key_tmp);\n-      load_key(key_tmp, key, 0xd0);\n-      __ aesdec(xmm_result, key_tmp);\n-      load_key(key_tmp, key, 0xe0);\n-      __ aesdec(xmm_result, key_tmp);\n-    }\n-\n-    __ aesdeclast(xmm_result, xmm_key_last); \/\/ xmm15 always came from key+0\n-    __ pxor(xmm_result, xmm_prev_block_cipher); \/\/ xor with the current r vector\n-    __ movdqu(Address(to, pos, Address::times_1, 0), xmm_result); \/\/ store into the next 16 bytes of output\n-    \/\/ no need to store r to memory until we exit\n-    __ movdqa(xmm_prev_block_cipher, xmm_prev_block_cipher_save); \/\/ set up next r vector with cipher input from this block\n-    __ addptr(pos, AESBlockSize);\n-    __ subptr(len_reg, AESBlockSize);\n-    __ jcc(Assembler::notEqual, L_singleBlock_loopTop[k]);\n-    if (k != 2) {\n-      __ jmp(L_exit);\n-    }\n-  } \/\/for 128\/192\/256\n-\n-  __ BIND(L_exit);\n-  __ movdqu(Address(rvec, 0), xmm_prev_block_cipher);     \/\/ final value of r stored in rvec of CipherBlockChaining object\n-  __ pop(rbx);\n-#ifdef _WIN64\n-  __ movl(rax, len_mem);\n-#else\n-  __ pop(rax); \/\/ return length\n-#endif\n-  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-  __ ret(0);\n-\n-  return start;\n-}\n-\n-address StubGenerator::generate_electronicCodeBook_encryptAESCrypt() {\n-  __ align(CodeEntryAlignment);\n-  StubCodeMark mark(this, \"StubRoutines\", \"electronicCodeBook_encryptAESCrypt\");\n-  address start = __ pc();\n-\n-  const Register from = c_rarg0;  \/\/ source array address\n-  const Register to = c_rarg1;  \/\/ destination array address\n-  const Register key = c_rarg2;  \/\/ key array address\n-  const Register len = c_rarg3;  \/\/ src len (must be multiple of blocksize 16)\n-  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-  __ aesecb_encrypt(from, to, key, len);\n-  __ vzeroupper();\n-  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-  __ ret(0);\n-\n-  return start;\n- }\n-\n-address StubGenerator::generate_electronicCodeBook_decryptAESCrypt() {\n-  __ align(CodeEntryAlignment);\n-  StubCodeMark mark(this, \"StubRoutines\", \"electronicCodeBook_decryptAESCrypt\");\n-  address start = __ pc();\n-\n-  const Register from = c_rarg0;  \/\/ source array address\n-  const Register to = c_rarg1;  \/\/ destination array address\n-  const Register key = c_rarg2;  \/\/ key array address\n-  const Register len = c_rarg3;  \/\/ src len (must be multiple of blocksize 16)\n-  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-  __ aesecb_decrypt(from, to, key, len);\n-  __ vzeroupper();\n-  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-  __ ret(0);\n-\n-  return start;\n-}\n-\n@@ -2161,1045 +1484,0 @@\n-address StubGenerator::ghash_polynomial512_addr() {\n-  __ align(CodeEntryAlignment);\n-  StubCodeMark mark(this, \"StubRoutines\", \"_ghash_poly512_addr\");\n-  address start = __ pc();\n-\n-  __ emit_data64(0x00000001C2000000, relocInfo::none); \/\/ POLY for reduction\n-  __ emit_data64(0xC200000000000000, relocInfo::none);\n-  __ emit_data64(0x00000001C2000000, relocInfo::none);\n-  __ emit_data64(0xC200000000000000, relocInfo::none);\n-  __ emit_data64(0x00000001C2000000, relocInfo::none);\n-  __ emit_data64(0xC200000000000000, relocInfo::none);\n-  __ emit_data64(0x00000001C2000000, relocInfo::none);\n-  __ emit_data64(0xC200000000000000, relocInfo::none);\n-  __ emit_data64(0x0000000000000001, relocInfo::none); \/\/ POLY\n-  __ emit_data64(0xC200000000000000, relocInfo::none);\n-  __ emit_data64(0x0000000000000001, relocInfo::none); \/\/ TWOONE\n-  __ emit_data64(0x0000000100000000, relocInfo::none);\n-\n-  return start;\n-}\n-\n-\/\/ Vector AES Galois Counter Mode implementation.\n-\/\/\n-\/\/ Inputs:           Windows    |   Linux\n-\/\/   in         = rcx (c_rarg0) | rsi (c_rarg0)\n-\/\/   len        = rdx (c_rarg1) | rdi (c_rarg1)\n-\/\/   ct         = r8  (c_rarg2) | rdx (c_rarg2)\n-\/\/   out        = r9  (c_rarg3) | rcx (c_rarg3)\n-\/\/   key        = r10           | r8  (c_rarg4)\n-\/\/   state      = r13           | r9  (c_rarg5)\n-\/\/   subkeyHtbl = r14           | r11\n-\/\/   counter    = rsi           | r12\n-\/\/\n-\/\/ Output:\n-\/\/   rax - number of processed bytes\n-address StubGenerator::generate_galoisCounterMode_AESCrypt() {\n-  __ align(CodeEntryAlignment);\n-  StubCodeMark mark(this, \"StubRoutines\", \"galoisCounterMode_AESCrypt\");\n-  address start = __ pc();\n-\n-  const Register in = c_rarg0;\n-  const Register len = c_rarg1;\n-  const Register ct = c_rarg2;\n-  const Register out = c_rarg3;\n-  \/\/ and updated with the incremented counter in the end\n-#ifndef _WIN64\n-  const Register key = c_rarg4;\n-  const Register state = c_rarg5;\n-  const Address subkeyH_mem(rbp, 2 * wordSize);\n-  const Register subkeyHtbl = r11;\n-  const Register avx512_subkeyHtbl = r13;\n-  const Address counter_mem(rbp, 3 * wordSize);\n-  const Register counter = r12;\n-#else\n-  const Address key_mem(rbp, 6 * wordSize);\n-  const Register key = r10;\n-  const Address state_mem(rbp, 7 * wordSize);\n-  const Register state = r13;\n-  const Address subkeyH_mem(rbp, 8 * wordSize);\n-  const Register subkeyHtbl = r14;\n-  const Register avx512_subkeyHtbl = r12;\n-  const Address counter_mem(rbp, 9 * wordSize);\n-  const Register counter = rsi;\n-#endif\n-  __ enter();\n- \/\/ Save state before entering routine\n-  __ push(r12);\n-  __ push(r13);\n-  __ push(r14);\n-  __ push(r15);\n-  __ push(rbx);\n-#ifdef _WIN64\n-  \/\/ on win64, fill len_reg from stack position\n-  __ push(rsi);\n-  __ movptr(key, key_mem);\n-  __ movptr(state, state_mem);\n-#endif\n-  __ movptr(subkeyHtbl, subkeyH_mem);\n-  __ movptr(counter, counter_mem);\n-\/\/ Save rbp and rsp\n-  __ push(rbp);\n-  __ movq(rbp, rsp);\n-\/\/ Align stack\n-  __ andq(rsp, -64);\n-  __ subptr(rsp, 96 * longSize); \/\/ Create space on the stack for htbl entries\n-  __ movptr(avx512_subkeyHtbl, rsp);\n-\n-  __ aesgcm_encrypt(in, len, ct, out, key, state, subkeyHtbl, avx512_subkeyHtbl, counter);\n-  __ vzeroupper();\n-\n-  __ movq(rsp, rbp);\n-  __ pop(rbp);\n-\n-  \/\/ Restore state before leaving routine\n-#ifdef _WIN64\n-  __ pop(rsi);\n-#endif\n-  __ pop(rbx);\n-  __ pop(r15);\n-  __ pop(r14);\n-  __ pop(r13);\n-  __ pop(r12);\n-\n-  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-  __ ret(0);\n-\n-  return start;\n-}\n-\n-\/\/ This mask is used for incrementing counter value(linc0, linc4, etc.)\n-address StubGenerator::counter_mask_addr() {\n-  __ align64();\n-  StubCodeMark mark(this, \"StubRoutines\", \"counter_mask_addr\");\n-  address start = __ pc();\n-\n-  __ emit_data64(0x08090a0b0c0d0e0f, relocInfo::none);\/\/lbswapmask\n-  __ emit_data64(0x0001020304050607, relocInfo::none);\n-  __ emit_data64(0x08090a0b0c0d0e0f, relocInfo::none);\n-  __ emit_data64(0x0001020304050607, relocInfo::none);\n-  __ emit_data64(0x08090a0b0c0d0e0f, relocInfo::none);\n-  __ emit_data64(0x0001020304050607, relocInfo::none);\n-  __ emit_data64(0x08090a0b0c0d0e0f, relocInfo::none);\n-  __ emit_data64(0x0001020304050607, relocInfo::none);\n-  __ emit_data64(0x0000000000000000, relocInfo::none);\/\/linc0 = counter_mask_addr+64\n-  __ emit_data64(0x0000000000000000, relocInfo::none);\n-  __ emit_data64(0x0000000000000001, relocInfo::none);\/\/counter_mask_addr() + 80\n-  __ emit_data64(0x0000000000000000, relocInfo::none);\n-  __ emit_data64(0x0000000000000002, relocInfo::none);\n-  __ emit_data64(0x0000000000000000, relocInfo::none);\n-  __ emit_data64(0x0000000000000003, relocInfo::none);\n-  __ emit_data64(0x0000000000000000, relocInfo::none);\n-  __ emit_data64(0x0000000000000004, relocInfo::none);\/\/linc4 = counter_mask_addr() + 128\n-  __ emit_data64(0x0000000000000000, relocInfo::none);\n-  __ emit_data64(0x0000000000000004, relocInfo::none);\n-  __ emit_data64(0x0000000000000000, relocInfo::none);\n-  __ emit_data64(0x0000000000000004, relocInfo::none);\n-  __ emit_data64(0x0000000000000000, relocInfo::none);\n-  __ emit_data64(0x0000000000000004, relocInfo::none);\n-  __ emit_data64(0x0000000000000000, relocInfo::none);\n-  __ emit_data64(0x0000000000000008, relocInfo::none);\/\/linc8 = counter_mask_addr() + 192\n-  __ emit_data64(0x0000000000000000, relocInfo::none);\n-  __ emit_data64(0x0000000000000008, relocInfo::none);\n-  __ emit_data64(0x0000000000000000, relocInfo::none);\n-  __ emit_data64(0x0000000000000008, relocInfo::none);\n-  __ emit_data64(0x0000000000000000, relocInfo::none);\n-  __ emit_data64(0x0000000000000008, relocInfo::none);\n-  __ emit_data64(0x0000000000000000, relocInfo::none);\n-  __ emit_data64(0x0000000000000020, relocInfo::none);\/\/linc32 = counter_mask_addr() + 256\n-  __ emit_data64(0x0000000000000000, relocInfo::none);\n-  __ emit_data64(0x0000000000000020, relocInfo::none);\n-  __ emit_data64(0x0000000000000000, relocInfo::none);\n-  __ emit_data64(0x0000000000000020, relocInfo::none);\n-  __ emit_data64(0x0000000000000000, relocInfo::none);\n-  __ emit_data64(0x0000000000000020, relocInfo::none);\n-  __ emit_data64(0x0000000000000000, relocInfo::none);\n-  __ emit_data64(0x0000000000000010, relocInfo::none);\/\/linc16 = counter_mask_addr() + 320\n-  __ emit_data64(0x0000000000000000, relocInfo::none);\n-  __ emit_data64(0x0000000000000010, relocInfo::none);\n-  __ emit_data64(0x0000000000000000, relocInfo::none);\n-  __ emit_data64(0x0000000000000010, relocInfo::none);\n-  __ emit_data64(0x0000000000000000, relocInfo::none);\n-  __ emit_data64(0x0000000000000010, relocInfo::none);\n-  __ emit_data64(0x0000000000000000, relocInfo::none);\n-\n-  return start;\n-}\n-\n- \/\/ Vector AES Counter implementation\n-address StubGenerator::generate_counterMode_VectorAESCrypt()  {\n-  __ align(CodeEntryAlignment);\n-  StubCodeMark mark(this, \"StubRoutines\", \"counterMode_AESCrypt\");\n-  address start = __ pc();\n-\n-  const Register from = c_rarg0; \/\/ source array address\n-  const Register to = c_rarg1; \/\/ destination array address\n-  const Register key = c_rarg2; \/\/ key array address r8\n-  const Register counter = c_rarg3; \/\/ counter byte array initialized from counter array address\n-  \/\/ and updated with the incremented counter in the end\n-#ifndef _WIN64\n-  const Register len_reg = c_rarg4;\n-  const Register saved_encCounter_start = c_rarg5;\n-  const Register used_addr = r10;\n-  const Address  used_mem(rbp, 2 * wordSize);\n-  const Register used = r11;\n-#else\n-  const Address len_mem(rbp, 6 * wordSize); \/\/ length is on stack on Win64\n-  const Address saved_encCounter_mem(rbp, 7 * wordSize); \/\/ saved encrypted counter is on stack on Win64\n-  const Address used_mem(rbp, 8 * wordSize); \/\/ used length is on stack on Win64\n-  const Register len_reg = r10; \/\/ pick the first volatile windows register\n-  const Register saved_encCounter_start = r11;\n-  const Register used_addr = r13;\n-  const Register used = r14;\n-#endif\n-  __ enter();\n- \/\/ Save state before entering routine\n-  __ push(r12);\n-  __ push(r13);\n-  __ push(r14);\n-  __ push(r15);\n-#ifdef _WIN64\n-  \/\/ on win64, fill len_reg from stack position\n-  __ movl(len_reg, len_mem);\n-  __ movptr(saved_encCounter_start, saved_encCounter_mem);\n-  __ movptr(used_addr, used_mem);\n-  __ movl(used, Address(used_addr, 0));\n-#else\n-  __ push(len_reg); \/\/ Save\n-  __ movptr(used_addr, used_mem);\n-  __ movl(used, Address(used_addr, 0));\n-#endif\n-  __ push(rbx);\n-  __ aesctr_encrypt(from, to, key, counter, len_reg, used, used_addr, saved_encCounter_start);\n-  __ vzeroupper();\n-  \/\/ Restore state before leaving routine\n-  __ pop(rbx);\n-#ifdef _WIN64\n-  __ movl(rax, len_mem); \/\/ return length\n-#else\n-  __ pop(rax); \/\/ return length\n-#endif\n-  __ pop(r15);\n-  __ pop(r14);\n-  __ pop(r13);\n-  __ pop(r12);\n-\n-  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-  __ ret(0);\n-\n-  return start;\n-}\n-\n-\/\/ This is a version of CTR\/AES crypt which does 6 blocks in a loop at a time\n-\/\/ to hide instruction latency\n-\/\/\n-\/\/ Arguments:\n-\/\/\n-\/\/ Inputs:\n-\/\/   c_rarg0   - source byte array address\n-\/\/   c_rarg1   - destination byte array address\n-\/\/   c_rarg2   - K (key) in little endian int array\n-\/\/   c_rarg3   - counter vector byte array address\n-\/\/   Linux\n-\/\/     c_rarg4   -          input length\n-\/\/     c_rarg5   -          saved encryptedCounter start\n-\/\/     rbp + 6 * wordSize - saved used length\n-\/\/   Windows\n-\/\/     rbp + 6 * wordSize - input length\n-\/\/     rbp + 7 * wordSize - saved encryptedCounter start\n-\/\/     rbp + 8 * wordSize - saved used length\n-\/\/\n-\/\/ Output:\n-\/\/   rax       - input length\n-\/\/\n-address StubGenerator::generate_counterMode_AESCrypt_Parallel() {\n-  assert(UseAES, \"need AES instructions and misaligned SSE support\");\n-  __ align(CodeEntryAlignment);\n-  StubCodeMark mark(this, \"StubRoutines\", \"counterMode_AESCrypt\");\n-  address start = __ pc();\n-\n-  const Register from = c_rarg0; \/\/ source array address\n-  const Register to = c_rarg1; \/\/ destination array address\n-  const Register key = c_rarg2; \/\/ key array address\n-  const Register counter = c_rarg3; \/\/ counter byte array initialized from counter array address\n-                                    \/\/ and updated with the incremented counter in the end\n-#ifndef _WIN64\n-  const Register len_reg = c_rarg4;\n-  const Register saved_encCounter_start = c_rarg5;\n-  const Register used_addr = r10;\n-  const Address  used_mem(rbp, 2 * wordSize);\n-  const Register used = r11;\n-#else\n-  const Address len_mem(rbp, 6 * wordSize); \/\/ length is on stack on Win64\n-  const Address saved_encCounter_mem(rbp, 7 * wordSize); \/\/ length is on stack on Win64\n-  const Address used_mem(rbp, 8 * wordSize); \/\/ length is on stack on Win64\n-  const Register len_reg = r10; \/\/ pick the first volatile windows register\n-  const Register saved_encCounter_start = r11;\n-  const Register used_addr = r13;\n-  const Register used = r14;\n-#endif\n-  const Register pos = rax;\n-\n-  const int PARALLEL_FACTOR = 6;\n-  const XMMRegister xmm_counter_shuf_mask = xmm0;\n-  const XMMRegister xmm_key_shuf_mask = xmm1; \/\/ used temporarily to swap key bytes up front\n-  const XMMRegister xmm_curr_counter = xmm2;\n-\n-  const XMMRegister xmm_key_tmp0 = xmm3;\n-  const XMMRegister xmm_key_tmp1 = xmm4;\n-\n-  \/\/ registers holding the four results in the parallelized loop\n-  const XMMRegister xmm_result0 = xmm5;\n-  const XMMRegister xmm_result1 = xmm6;\n-  const XMMRegister xmm_result2 = xmm7;\n-  const XMMRegister xmm_result3 = xmm8;\n-  const XMMRegister xmm_result4 = xmm9;\n-  const XMMRegister xmm_result5 = xmm10;\n-\n-  const XMMRegister xmm_from0 = xmm11;\n-  const XMMRegister xmm_from1 = xmm12;\n-  const XMMRegister xmm_from2 = xmm13;\n-  const XMMRegister xmm_from3 = xmm14; \/\/the last one is xmm14. we have to preserve it on WIN64.\n-  const XMMRegister xmm_from4 = xmm3; \/\/reuse xmm3~4. Because xmm_key_tmp0~1 are useless when loading input text\n-  const XMMRegister xmm_from5 = xmm4;\n-\n-  \/\/for key_128, key_192, key_256\n-  const int rounds[3] = {10, 12, 14};\n-  Label L_exit_preLoop, L_preLoop_start;\n-  Label L_multiBlock_loopTop[3];\n-  Label L_singleBlockLoopTop[3];\n-  Label L__incCounter[3][6]; \/\/for 6 blocks\n-  Label L__incCounter_single[3]; \/\/for single block, key128, key192, key256\n-  Label L_processTail_insr[3], L_processTail_4_insr[3], L_processTail_2_insr[3], L_processTail_1_insr[3], L_processTail_exit_insr[3];\n-  Label L_processTail_4_extr[3], L_processTail_2_extr[3], L_processTail_1_extr[3], L_processTail_exit_extr[3];\n-\n-  Label L_exit;\n-\n-  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-\n-#ifdef _WIN64\n-  \/\/ allocate spill slots for r13, r14\n-  enum {\n-      saved_r13_offset,\n-      saved_r14_offset\n-  };\n-  __ subptr(rsp, 2 * wordSize);\n-  __ movptr(Address(rsp, saved_r13_offset * wordSize), r13);\n-  __ movptr(Address(rsp, saved_r14_offset * wordSize), r14);\n-\n-  \/\/ on win64, fill len_reg from stack position\n-  __ movl(len_reg, len_mem);\n-  __ movptr(saved_encCounter_start, saved_encCounter_mem);\n-  __ movptr(used_addr, used_mem);\n-  __ movl(used, Address(used_addr, 0));\n-#else\n-  __ push(len_reg); \/\/ Save\n-  __ movptr(used_addr, used_mem);\n-  __ movl(used, Address(used_addr, 0));\n-#endif\n-\n-  __ push(rbx); \/\/ Save RBX\n-  __ movdqu(xmm_curr_counter, Address(counter, 0x00)); \/\/ initialize counter with initial counter\n-  __ movdqu(xmm_counter_shuf_mask, ExternalAddress(StubRoutines::x86::counter_shuffle_mask_addr()), pos); \/\/ pos as scratch\n-  __ pshufb(xmm_curr_counter, xmm_counter_shuf_mask); \/\/counter is shuffled\n-  __ movptr(pos, 0);\n-\n-  \/\/ Use the partially used encrpyted counter from last invocation\n-  __ BIND(L_preLoop_start);\n-  __ cmpptr(used, 16);\n-  __ jcc(Assembler::aboveEqual, L_exit_preLoop);\n-    __ cmpptr(len_reg, 0);\n-    __ jcc(Assembler::lessEqual, L_exit_preLoop);\n-    __ movb(rbx, Address(saved_encCounter_start, used));\n-    __ xorb(rbx, Address(from, pos));\n-    __ movb(Address(to, pos), rbx);\n-    __ addptr(pos, 1);\n-    __ addptr(used, 1);\n-    __ subptr(len_reg, 1);\n-\n-  __ jmp(L_preLoop_start);\n-\n-  __ BIND(L_exit_preLoop);\n-  __ movl(Address(used_addr, 0), used);\n-\n-  \/\/ key length could be only {11, 13, 15} * 4 = {44, 52, 60}\n-  __ movdqu(xmm_key_shuf_mask, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()), rbx); \/\/ rbx as scratch\n-  __ movl(rbx, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));\n-  __ cmpl(rbx, 52);\n-  __ jcc(Assembler::equal, L_multiBlock_loopTop[1]);\n-  __ cmpl(rbx, 60);\n-  __ jcc(Assembler::equal, L_multiBlock_loopTop[2]);\n-\n-#define CTR_DoSix(opc, src_reg)                \\\n-  __ opc(xmm_result0, src_reg);              \\\n-  __ opc(xmm_result1, src_reg);              \\\n-  __ opc(xmm_result2, src_reg);              \\\n-  __ opc(xmm_result3, src_reg);              \\\n-  __ opc(xmm_result4, src_reg);              \\\n-  __ opc(xmm_result5, src_reg);\n-\n-  \/\/ k == 0 :  generate code for key_128\n-  \/\/ k == 1 :  generate code for key_192\n-  \/\/ k == 2 :  generate code for key_256\n-  for (int k = 0; k < 3; ++k) {\n-    \/\/multi blocks starts here\n-    __ align(OptoLoopAlignment);\n-    __ BIND(L_multiBlock_loopTop[k]);\n-    __ cmpptr(len_reg, PARALLEL_FACTOR * AESBlockSize); \/\/ see if at least PARALLEL_FACTOR blocks left\n-    __ jcc(Assembler::less, L_singleBlockLoopTop[k]);\n-    load_key(xmm_key_tmp0, key, 0x00, xmm_key_shuf_mask);\n-\n-    \/\/load, then increase counters\n-    CTR_DoSix(movdqa, xmm_curr_counter);\n-    inc_counter(rbx, xmm_result1, 0x01, L__incCounter[k][0]);\n-    inc_counter(rbx, xmm_result2, 0x02, L__incCounter[k][1]);\n-    inc_counter(rbx, xmm_result3, 0x03, L__incCounter[k][2]);\n-    inc_counter(rbx, xmm_result4, 0x04, L__incCounter[k][3]);\n-    inc_counter(rbx, xmm_result5,  0x05, L__incCounter[k][4]);\n-    inc_counter(rbx, xmm_curr_counter, 0x06, L__incCounter[k][5]);\n-    CTR_DoSix(pshufb, xmm_counter_shuf_mask); \/\/ after increased, shuffled counters back for PXOR\n-    CTR_DoSix(pxor, xmm_key_tmp0);   \/\/PXOR with Round 0 key\n-\n-    \/\/load two ROUND_KEYs at a time\n-    for (int i = 1; i < rounds[k]; ) {\n-      load_key(xmm_key_tmp1, key, (0x10 * i), xmm_key_shuf_mask);\n-      load_key(xmm_key_tmp0, key, (0x10 * (i+1)), xmm_key_shuf_mask);\n-      CTR_DoSix(aesenc, xmm_key_tmp1);\n-      i++;\n-      if (i != rounds[k]) {\n-        CTR_DoSix(aesenc, xmm_key_tmp0);\n-      } else {\n-        CTR_DoSix(aesenclast, xmm_key_tmp0);\n-      }\n-      i++;\n-    }\n-\n-    \/\/ get next PARALLEL_FACTOR blocks into xmm_result registers\n-    __ movdqu(xmm_from0, Address(from, pos, Address::times_1, 0 * AESBlockSize));\n-    __ movdqu(xmm_from1, Address(from, pos, Address::times_1, 1 * AESBlockSize));\n-    __ movdqu(xmm_from2, Address(from, pos, Address::times_1, 2 * AESBlockSize));\n-    __ movdqu(xmm_from3, Address(from, pos, Address::times_1, 3 * AESBlockSize));\n-    __ movdqu(xmm_from4, Address(from, pos, Address::times_1, 4 * AESBlockSize));\n-    __ movdqu(xmm_from5, Address(from, pos, Address::times_1, 5 * AESBlockSize));\n-\n-    __ pxor(xmm_result0, xmm_from0);\n-    __ pxor(xmm_result1, xmm_from1);\n-    __ pxor(xmm_result2, xmm_from2);\n-    __ pxor(xmm_result3, xmm_from3);\n-    __ pxor(xmm_result4, xmm_from4);\n-    __ pxor(xmm_result5, xmm_from5);\n-\n-    \/\/ store 6 results into the next 64 bytes of output\n-    __ movdqu(Address(to, pos, Address::times_1, 0 * AESBlockSize), xmm_result0);\n-    __ movdqu(Address(to, pos, Address::times_1, 1 * AESBlockSize), xmm_result1);\n-    __ movdqu(Address(to, pos, Address::times_1, 2 * AESBlockSize), xmm_result2);\n-    __ movdqu(Address(to, pos, Address::times_1, 3 * AESBlockSize), xmm_result3);\n-    __ movdqu(Address(to, pos, Address::times_1, 4 * AESBlockSize), xmm_result4);\n-    __ movdqu(Address(to, pos, Address::times_1, 5 * AESBlockSize), xmm_result5);\n-\n-    __ addptr(pos, PARALLEL_FACTOR * AESBlockSize); \/\/ increase the length of crypt text\n-    __ subptr(len_reg, PARALLEL_FACTOR * AESBlockSize); \/\/ decrease the remaining length\n-    __ jmp(L_multiBlock_loopTop[k]);\n-\n-    \/\/ singleBlock starts here\n-    __ align(OptoLoopAlignment);\n-    __ BIND(L_singleBlockLoopTop[k]);\n-    __ cmpptr(len_reg, 0);\n-    __ jcc(Assembler::lessEqual, L_exit);\n-    load_key(xmm_key_tmp0, key, 0x00, xmm_key_shuf_mask);\n-    __ movdqa(xmm_result0, xmm_curr_counter);\n-    inc_counter(rbx, xmm_curr_counter, 0x01, L__incCounter_single[k]);\n-    __ pshufb(xmm_result0, xmm_counter_shuf_mask);\n-    __ pxor(xmm_result0, xmm_key_tmp0);\n-    for (int i = 1; i < rounds[k]; i++) {\n-      load_key(xmm_key_tmp0, key, (0x10 * i), xmm_key_shuf_mask);\n-      __ aesenc(xmm_result0, xmm_key_tmp0);\n-    }\n-    load_key(xmm_key_tmp0, key, (rounds[k] * 0x10), xmm_key_shuf_mask);\n-    __ aesenclast(xmm_result0, xmm_key_tmp0);\n-    __ cmpptr(len_reg, AESBlockSize);\n-    __ jcc(Assembler::less, L_processTail_insr[k]);\n-      __ movdqu(xmm_from0, Address(from, pos, Address::times_1, 0 * AESBlockSize));\n-      __ pxor(xmm_result0, xmm_from0);\n-      __ movdqu(Address(to, pos, Address::times_1, 0 * AESBlockSize), xmm_result0);\n-      __ addptr(pos, AESBlockSize);\n-      __ subptr(len_reg, AESBlockSize);\n-      __ jmp(L_singleBlockLoopTop[k]);\n-    __ BIND(L_processTail_insr[k]);                               \/\/ Process the tail part of the input array\n-      __ addptr(pos, len_reg);                                    \/\/ 1. Insert bytes from src array into xmm_from0 register\n-      __ testptr(len_reg, 8);\n-      __ jcc(Assembler::zero, L_processTail_4_insr[k]);\n-        __ subptr(pos,8);\n-        __ pinsrq(xmm_from0, Address(from, pos), 0);\n-      __ BIND(L_processTail_4_insr[k]);\n-      __ testptr(len_reg, 4);\n-      __ jcc(Assembler::zero, L_processTail_2_insr[k]);\n-        __ subptr(pos,4);\n-        __ pslldq(xmm_from0, 4);\n-        __ pinsrd(xmm_from0, Address(from, pos), 0);\n-      __ BIND(L_processTail_2_insr[k]);\n-      __ testptr(len_reg, 2);\n-      __ jcc(Assembler::zero, L_processTail_1_insr[k]);\n-        __ subptr(pos, 2);\n-        __ pslldq(xmm_from0, 2);\n-        __ pinsrw(xmm_from0, Address(from, pos), 0);\n-      __ BIND(L_processTail_1_insr[k]);\n-      __ testptr(len_reg, 1);\n-      __ jcc(Assembler::zero, L_processTail_exit_insr[k]);\n-        __ subptr(pos, 1);\n-        __ pslldq(xmm_from0, 1);\n-        __ pinsrb(xmm_from0, Address(from, pos), 0);\n-      __ BIND(L_processTail_exit_insr[k]);\n-\n-      __ movdqu(Address(saved_encCounter_start, 0), xmm_result0);  \/\/ 2. Perform pxor of the encrypted counter and plaintext Bytes.\n-      __ pxor(xmm_result0, xmm_from0);                             \/\/    Also the encrypted counter is saved for next invocation.\n-\n-      __ testptr(len_reg, 8);\n-      __ jcc(Assembler::zero, L_processTail_4_extr[k]);            \/\/ 3. Extract bytes from xmm_result0 into the dest. array\n-        __ pextrq(Address(to, pos), xmm_result0, 0);\n-        __ psrldq(xmm_result0, 8);\n-        __ addptr(pos, 8);\n-      __ BIND(L_processTail_4_extr[k]);\n-      __ testptr(len_reg, 4);\n-      __ jcc(Assembler::zero, L_processTail_2_extr[k]);\n-        __ pextrd(Address(to, pos), xmm_result0, 0);\n-        __ psrldq(xmm_result0, 4);\n-        __ addptr(pos, 4);\n-      __ BIND(L_processTail_2_extr[k]);\n-      __ testptr(len_reg, 2);\n-      __ jcc(Assembler::zero, L_processTail_1_extr[k]);\n-        __ pextrw(Address(to, pos), xmm_result0, 0);\n-        __ psrldq(xmm_result0, 2);\n-        __ addptr(pos, 2);\n-      __ BIND(L_processTail_1_extr[k]);\n-      __ testptr(len_reg, 1);\n-      __ jcc(Assembler::zero, L_processTail_exit_extr[k]);\n-        __ pextrb(Address(to, pos), xmm_result0, 0);\n-\n-      __ BIND(L_processTail_exit_extr[k]);\n-      __ movl(Address(used_addr, 0), len_reg);\n-      __ jmp(L_exit);\n-  }\n-\n-  __ BIND(L_exit);\n-  __ pshufb(xmm_curr_counter, xmm_counter_shuf_mask); \/\/counter is shuffled back.\n-  __ movdqu(Address(counter, 0), xmm_curr_counter); \/\/save counter back\n-  __ pop(rbx); \/\/ pop the saved RBX.\n-#ifdef _WIN64\n-  __ movl(rax, len_mem);\n-  __ movptr(r13, Address(rsp, saved_r13_offset * wordSize));\n-  __ movptr(r14, Address(rsp, saved_r14_offset * wordSize));\n-  __ addptr(rsp, 2 * wordSize);\n-#else\n-  __ pop(rax); \/\/ return 'len'\n-#endif\n-  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-  __ ret(0);\n-\n-  return start;\n-}\n-\n-void StubGenerator::roundDec(XMMRegister xmm_reg) {\n-  __ vaesdec(xmm1, xmm1, xmm_reg, Assembler::AVX_512bit);\n-  __ vaesdec(xmm2, xmm2, xmm_reg, Assembler::AVX_512bit);\n-  __ vaesdec(xmm3, xmm3, xmm_reg, Assembler::AVX_512bit);\n-  __ vaesdec(xmm4, xmm4, xmm_reg, Assembler::AVX_512bit);\n-  __ vaesdec(xmm5, xmm5, xmm_reg, Assembler::AVX_512bit);\n-  __ vaesdec(xmm6, xmm6, xmm_reg, Assembler::AVX_512bit);\n-  __ vaesdec(xmm7, xmm7, xmm_reg, Assembler::AVX_512bit);\n-  __ vaesdec(xmm8, xmm8, xmm_reg, Assembler::AVX_512bit);\n-}\n-\n-void StubGenerator::roundDeclast(XMMRegister xmm_reg) {\n-  __ vaesdeclast(xmm1, xmm1, xmm_reg, Assembler::AVX_512bit);\n-  __ vaesdeclast(xmm2, xmm2, xmm_reg, Assembler::AVX_512bit);\n-  __ vaesdeclast(xmm3, xmm3, xmm_reg, Assembler::AVX_512bit);\n-  __ vaesdeclast(xmm4, xmm4, xmm_reg, Assembler::AVX_512bit);\n-  __ vaesdeclast(xmm5, xmm5, xmm_reg, Assembler::AVX_512bit);\n-  __ vaesdeclast(xmm6, xmm6, xmm_reg, Assembler::AVX_512bit);\n-  __ vaesdeclast(xmm7, xmm7, xmm_reg, Assembler::AVX_512bit);\n-  __ vaesdeclast(xmm8, xmm8, xmm_reg, Assembler::AVX_512bit);\n-}\n-\n-void StubGenerator::ev_load_key(XMMRegister xmmdst, Register key, int offset, XMMRegister xmm_shuf_mask) {\n-  __ movdqu(xmmdst, Address(key, offset));\n-  if (xmm_shuf_mask != xnoreg) {\n-    __ pshufb(xmmdst, xmm_shuf_mask);\n-  } else {\n-    __ pshufb(xmmdst, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()));\n-  }\n-  __ evshufi64x2(xmmdst, xmmdst, xmmdst, 0x0, Assembler::AVX_512bit);\n-}\n-\n-address StubGenerator::generate_cipherBlockChaining_decryptVectorAESCrypt() {\n-  assert(VM_Version::supports_avx512_vaes(), \"need AES instructions and misaligned SSE support\");\n-  __ align(CodeEntryAlignment);\n-  StubCodeMark mark(this, \"StubRoutines\", \"cipherBlockChaining_decryptAESCrypt\");\n-  address start = __ pc();\n-\n-  const Register from = c_rarg0;  \/\/ source array address\n-  const Register to = c_rarg1;  \/\/ destination array address\n-  const Register key = c_rarg2;  \/\/ key array address\n-  const Register rvec = c_rarg3;  \/\/ r byte array initialized from initvector array address\n-  \/\/ and left with the results of the last encryption block\n-#ifndef _WIN64\n-  const Register len_reg = c_rarg4;  \/\/ src len (must be multiple of blocksize 16)\n-#else\n-  const Address  len_mem(rbp, 6 * wordSize);  \/\/ length is on stack on Win64\n-  const Register len_reg = r11;      \/\/ pick the volatile windows register\n-#endif\n-\n-  Label Loop, Loop1, L_128, L_256, L_192, KEY_192, KEY_256, Loop2, Lcbc_dec_rem_loop,\n-        Lcbc_dec_rem_last, Lcbc_dec_ret, Lcbc_dec_rem, Lcbc_exit;\n-\n-  __ enter();\n-\n-#ifdef _WIN64\n-\/\/ on win64, fill len_reg from stack position\n-  __ movl(len_reg, len_mem);\n-#else\n-  __ push(len_reg); \/\/ Save\n-#endif\n-  __ push(rbx);\n-  __ vzeroupper();\n-\n-  \/\/ Temporary variable declaration for swapping key bytes\n-  const XMMRegister xmm_key_shuf_mask = xmm1;\n-  __ movdqu(xmm_key_shuf_mask, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()));\n-\n-  \/\/ Calculate number of rounds from key size: 44 for 10-rounds, 52 for 12-rounds, 60 for 14-rounds\n-  const Register rounds = rbx;\n-  __ movl(rounds, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));\n-\n-  const XMMRegister IV = xmm0;\n-  \/\/ Load IV and broadcast value to 512-bits\n-  __ evbroadcasti64x2(IV, Address(rvec, 0), Assembler::AVX_512bit);\n-\n-  \/\/ Temporary variables for storing round keys\n-  const XMMRegister RK0 = xmm30;\n-  const XMMRegister RK1 = xmm9;\n-  const XMMRegister RK2 = xmm18;\n-  const XMMRegister RK3 = xmm19;\n-  const XMMRegister RK4 = xmm20;\n-  const XMMRegister RK5 = xmm21;\n-  const XMMRegister RK6 = xmm22;\n-  const XMMRegister RK7 = xmm23;\n-  const XMMRegister RK8 = xmm24;\n-  const XMMRegister RK9 = xmm25;\n-  const XMMRegister RK10 = xmm26;\n-\n-  \/\/ Load and shuffle key\n-  \/\/ the java expanded key ordering is rotated one position from what we want\n-  \/\/ so we start from 1*16 here and hit 0*16 last\n-  ev_load_key(RK1, key, 1 * 16, xmm_key_shuf_mask);\n-  ev_load_key(RK2, key, 2 * 16, xmm_key_shuf_mask);\n-  ev_load_key(RK3, key, 3 * 16, xmm_key_shuf_mask);\n-  ev_load_key(RK4, key, 4 * 16, xmm_key_shuf_mask);\n-  ev_load_key(RK5, key, 5 * 16, xmm_key_shuf_mask);\n-  ev_load_key(RK6, key, 6 * 16, xmm_key_shuf_mask);\n-  ev_load_key(RK7, key, 7 * 16, xmm_key_shuf_mask);\n-  ev_load_key(RK8, key, 8 * 16, xmm_key_shuf_mask);\n-  ev_load_key(RK9, key, 9 * 16, xmm_key_shuf_mask);\n-  ev_load_key(RK10, key, 10 * 16, xmm_key_shuf_mask);\n-  ev_load_key(RK0, key, 0*16, xmm_key_shuf_mask);\n-\n-  \/\/ Variables for storing source cipher text\n-  const XMMRegister S0 = xmm10;\n-  const XMMRegister S1 = xmm11;\n-  const XMMRegister S2 = xmm12;\n-  const XMMRegister S3 = xmm13;\n-  const XMMRegister S4 = xmm14;\n-  const XMMRegister S5 = xmm15;\n-  const XMMRegister S6 = xmm16;\n-  const XMMRegister S7 = xmm17;\n-\n-  \/\/ Variables for storing decrypted text\n-  const XMMRegister B0 = xmm1;\n-  const XMMRegister B1 = xmm2;\n-  const XMMRegister B2 = xmm3;\n-  const XMMRegister B3 = xmm4;\n-  const XMMRegister B4 = xmm5;\n-  const XMMRegister B5 = xmm6;\n-  const XMMRegister B6 = xmm7;\n-  const XMMRegister B7 = xmm8;\n-\n-  __ cmpl(rounds, 44);\n-  __ jcc(Assembler::greater, KEY_192);\n-  __ jmp(Loop);\n-\n-  __ BIND(KEY_192);\n-  const XMMRegister RK11 = xmm27;\n-  const XMMRegister RK12 = xmm28;\n-  ev_load_key(RK11, key, 11*16, xmm_key_shuf_mask);\n-  ev_load_key(RK12, key, 12*16, xmm_key_shuf_mask);\n-\n-  __ cmpl(rounds, 52);\n-  __ jcc(Assembler::greater, KEY_256);\n-  __ jmp(Loop);\n-\n-  __ BIND(KEY_256);\n-  const XMMRegister RK13 = xmm29;\n-  const XMMRegister RK14 = xmm31;\n-  ev_load_key(RK13, key, 13*16, xmm_key_shuf_mask);\n-  ev_load_key(RK14, key, 14*16, xmm_key_shuf_mask);\n-\n-  __ BIND(Loop);\n-  __ cmpl(len_reg, 512);\n-  __ jcc(Assembler::below, Lcbc_dec_rem);\n-  __ BIND(Loop1);\n-  __ subl(len_reg, 512);\n-  __ evmovdquq(S0, Address(from, 0 * 64), Assembler::AVX_512bit);\n-  __ evmovdquq(S1, Address(from, 1 * 64), Assembler::AVX_512bit);\n-  __ evmovdquq(S2, Address(from, 2 * 64), Assembler::AVX_512bit);\n-  __ evmovdquq(S3, Address(from, 3 * 64), Assembler::AVX_512bit);\n-  __ evmovdquq(S4, Address(from, 4 * 64), Assembler::AVX_512bit);\n-  __ evmovdquq(S5, Address(from, 5 * 64), Assembler::AVX_512bit);\n-  __ evmovdquq(S6, Address(from, 6 * 64), Assembler::AVX_512bit);\n-  __ evmovdquq(S7, Address(from, 7 * 64), Assembler::AVX_512bit);\n-  __ leaq(from, Address(from, 8 * 64));\n-\n-  __ evpxorq(B0, S0, RK1, Assembler::AVX_512bit);\n-  __ evpxorq(B1, S1, RK1, Assembler::AVX_512bit);\n-  __ evpxorq(B2, S2, RK1, Assembler::AVX_512bit);\n-  __ evpxorq(B3, S3, RK1, Assembler::AVX_512bit);\n-  __ evpxorq(B4, S4, RK1, Assembler::AVX_512bit);\n-  __ evpxorq(B5, S5, RK1, Assembler::AVX_512bit);\n-  __ evpxorq(B6, S6, RK1, Assembler::AVX_512bit);\n-  __ evpxorq(B7, S7, RK1, Assembler::AVX_512bit);\n-\n-  __ evalignq(IV, S0, IV, 0x06);\n-  __ evalignq(S0, S1, S0, 0x06);\n-  __ evalignq(S1, S2, S1, 0x06);\n-  __ evalignq(S2, S3, S2, 0x06);\n-  __ evalignq(S3, S4, S3, 0x06);\n-  __ evalignq(S4, S5, S4, 0x06);\n-  __ evalignq(S5, S6, S5, 0x06);\n-  __ evalignq(S6, S7, S6, 0x06);\n-\n-  roundDec(RK2);\n-  roundDec(RK3);\n-  roundDec(RK4);\n-  roundDec(RK5);\n-  roundDec(RK6);\n-  roundDec(RK7);\n-  roundDec(RK8);\n-  roundDec(RK9);\n-  roundDec(RK10);\n-\n-  __ cmpl(rounds, 44);\n-  __ jcc(Assembler::belowEqual, L_128);\n-  roundDec(RK11);\n-  roundDec(RK12);\n-\n-  __ cmpl(rounds, 52);\n-  __ jcc(Assembler::belowEqual, L_192);\n-  roundDec(RK13);\n-  roundDec(RK14);\n-\n-  __ BIND(L_256);\n-  roundDeclast(RK0);\n-  __ jmp(Loop2);\n-\n-  __ BIND(L_128);\n-  roundDeclast(RK0);\n-  __ jmp(Loop2);\n-\n-  __ BIND(L_192);\n-  roundDeclast(RK0);\n-\n-  __ BIND(Loop2);\n-  __ evpxorq(B0, B0, IV, Assembler::AVX_512bit);\n-  __ evpxorq(B1, B1, S0, Assembler::AVX_512bit);\n-  __ evpxorq(B2, B2, S1, Assembler::AVX_512bit);\n-  __ evpxorq(B3, B3, S2, Assembler::AVX_512bit);\n-  __ evpxorq(B4, B4, S3, Assembler::AVX_512bit);\n-  __ evpxorq(B5, B5, S4, Assembler::AVX_512bit);\n-  __ evpxorq(B6, B6, S5, Assembler::AVX_512bit);\n-  __ evpxorq(B7, B7, S6, Assembler::AVX_512bit);\n-  __ evmovdquq(IV, S7, Assembler::AVX_512bit);\n-\n-  __ evmovdquq(Address(to, 0 * 64), B0, Assembler::AVX_512bit);\n-  __ evmovdquq(Address(to, 1 * 64), B1, Assembler::AVX_512bit);\n-  __ evmovdquq(Address(to, 2 * 64), B2, Assembler::AVX_512bit);\n-  __ evmovdquq(Address(to, 3 * 64), B3, Assembler::AVX_512bit);\n-  __ evmovdquq(Address(to, 4 * 64), B4, Assembler::AVX_512bit);\n-  __ evmovdquq(Address(to, 5 * 64), B5, Assembler::AVX_512bit);\n-  __ evmovdquq(Address(to, 6 * 64), B6, Assembler::AVX_512bit);\n-  __ evmovdquq(Address(to, 7 * 64), B7, Assembler::AVX_512bit);\n-  __ leaq(to, Address(to, 8 * 64));\n-  __ jmp(Loop);\n-\n-  __ BIND(Lcbc_dec_rem);\n-  __ evshufi64x2(IV, IV, IV, 0x03, Assembler::AVX_512bit);\n-\n-  __ BIND(Lcbc_dec_rem_loop);\n-  __ subl(len_reg, 16);\n-  __ jcc(Assembler::carrySet, Lcbc_dec_ret);\n-\n-  __ movdqu(S0, Address(from, 0));\n-  __ evpxorq(B0, S0, RK1, Assembler::AVX_512bit);\n-  __ vaesdec(B0, B0, RK2, Assembler::AVX_512bit);\n-  __ vaesdec(B0, B0, RK3, Assembler::AVX_512bit);\n-  __ vaesdec(B0, B0, RK4, Assembler::AVX_512bit);\n-  __ vaesdec(B0, B0, RK5, Assembler::AVX_512bit);\n-  __ vaesdec(B0, B0, RK6, Assembler::AVX_512bit);\n-  __ vaesdec(B0, B0, RK7, Assembler::AVX_512bit);\n-  __ vaesdec(B0, B0, RK8, Assembler::AVX_512bit);\n-  __ vaesdec(B0, B0, RK9, Assembler::AVX_512bit);\n-  __ vaesdec(B0, B0, RK10, Assembler::AVX_512bit);\n-  __ cmpl(rounds, 44);\n-  __ jcc(Assembler::belowEqual, Lcbc_dec_rem_last);\n-\n-  __ vaesdec(B0, B0, RK11, Assembler::AVX_512bit);\n-  __ vaesdec(B0, B0, RK12, Assembler::AVX_512bit);\n-  __ cmpl(rounds, 52);\n-  __ jcc(Assembler::belowEqual, Lcbc_dec_rem_last);\n-\n-  __ vaesdec(B0, B0, RK13, Assembler::AVX_512bit);\n-  __ vaesdec(B0, B0, RK14, Assembler::AVX_512bit);\n-\n-  __ BIND(Lcbc_dec_rem_last);\n-  __ vaesdeclast(B0, B0, RK0, Assembler::AVX_512bit);\n-\n-  __ evpxorq(B0, B0, IV, Assembler::AVX_512bit);\n-  __ evmovdquq(IV, S0, Assembler::AVX_512bit);\n-  __ movdqu(Address(to, 0), B0);\n-  __ leaq(from, Address(from, 16));\n-  __ leaq(to, Address(to, 16));\n-  __ jmp(Lcbc_dec_rem_loop);\n-\n-  __ BIND(Lcbc_dec_ret);\n-  __ movdqu(Address(rvec, 0), IV);\n-\n-  \/\/ Zero out the round keys\n-  __ evpxorq(RK0, RK0, RK0, Assembler::AVX_512bit);\n-  __ evpxorq(RK1, RK1, RK1, Assembler::AVX_512bit);\n-  __ evpxorq(RK2, RK2, RK2, Assembler::AVX_512bit);\n-  __ evpxorq(RK3, RK3, RK3, Assembler::AVX_512bit);\n-  __ evpxorq(RK4, RK4, RK4, Assembler::AVX_512bit);\n-  __ evpxorq(RK5, RK5, RK5, Assembler::AVX_512bit);\n-  __ evpxorq(RK6, RK6, RK6, Assembler::AVX_512bit);\n-  __ evpxorq(RK7, RK7, RK7, Assembler::AVX_512bit);\n-  __ evpxorq(RK8, RK8, RK8, Assembler::AVX_512bit);\n-  __ evpxorq(RK9, RK9, RK9, Assembler::AVX_512bit);\n-  __ evpxorq(RK10, RK10, RK10, Assembler::AVX_512bit);\n-  __ cmpl(rounds, 44);\n-  __ jcc(Assembler::belowEqual, Lcbc_exit);\n-  __ evpxorq(RK11, RK11, RK11, Assembler::AVX_512bit);\n-  __ evpxorq(RK12, RK12, RK12, Assembler::AVX_512bit);\n-  __ cmpl(rounds, 52);\n-  __ jcc(Assembler::belowEqual, Lcbc_exit);\n-  __ evpxorq(RK13, RK13, RK13, Assembler::AVX_512bit);\n-  __ evpxorq(RK14, RK14, RK14, Assembler::AVX_512bit);\n-\n-  __ BIND(Lcbc_exit);\n-  __ vzeroupper();\n-  __ pop(rbx);\n-#ifdef _WIN64\n-  __ movl(rax, len_mem);\n-#else\n-  __ pop(rax); \/\/ return length\n-#endif\n-  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-  __ ret(0);\n-\n-  return start;\n-}\n-\n-\/\/ Polynomial x^128+x^127+x^126+x^121+1\n-address StubGenerator::ghash_polynomial_addr() {\n-  __ align(CodeEntryAlignment);\n-  StubCodeMark mark(this, \"StubRoutines\", \"_ghash_poly_addr\");\n-  address start = __ pc();\n-\n-  __ emit_data64(0x0000000000000001, relocInfo::none);\n-  __ emit_data64(0xc200000000000000, relocInfo::none);\n-\n-  return start;\n-}\n-\n-address StubGenerator::ghash_shufflemask_addr() {\n-  __ align(CodeEntryAlignment);\n-  StubCodeMark mark(this, \"StubRoutines\", \"_ghash_shuffmask_addr\");\n-  address start = __ pc();\n-\n-  __ emit_data64(0x0f0f0f0f0f0f0f0f, relocInfo::none);\n-  __ emit_data64(0x0f0f0f0f0f0f0f0f, relocInfo::none);\n-\n-  return start;\n-}\n-\n-\/\/ Ghash single and multi block operations using AVX instructions\n-address StubGenerator::generate_avx_ghash_processBlocks() {\n-  __ align(CodeEntryAlignment);\n-\n-  StubCodeMark mark(this, \"StubRoutines\", \"ghash_processBlocks\");\n-  address start = __ pc();\n-\n-  \/\/ arguments\n-  const Register state = c_rarg0;\n-  const Register htbl = c_rarg1;\n-  const Register data = c_rarg2;\n-  const Register blocks = c_rarg3;\n-  __ enter();\n- \/\/ Save state before entering routine\n-  __ avx_ghash(state, htbl, data, blocks);\n-  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-  __ ret(0);\n-\n-  return start;\n-}\n-\n-\/\/ byte swap x86 long\n-address StubGenerator::generate_ghash_long_swap_mask() {\n-  __ align(CodeEntryAlignment);\n-  StubCodeMark mark(this, \"StubRoutines\", \"ghash_long_swap_mask\");\n-  address start = __ pc();\n-\n-  __ emit_data64(0x0f0e0d0c0b0a0908, relocInfo::none );\n-  __ emit_data64(0x0706050403020100, relocInfo::none );\n-\n-return start;\n-}\n-\n-\/\/ byte swap x86 byte array\n-address StubGenerator::generate_ghash_byte_swap_mask() {\n-  __ align(CodeEntryAlignment);\n-  StubCodeMark mark(this, \"StubRoutines\", \"ghash_byte_swap_mask\");\n-  address start = __ pc();\n-\n-  __ emit_data64(0x08090a0b0c0d0e0f, relocInfo::none );\n-  __ emit_data64(0x0001020304050607, relocInfo::none );\n-\n-return start;\n-}\n-\n-\/* Single and multi-block ghash operations *\/\n-address StubGenerator::generate_ghash_processBlocks() {\n-  __ align(CodeEntryAlignment);\n-  Label L_ghash_loop, L_exit;\n-  StubCodeMark mark(this, \"StubRoutines\", \"ghash_processBlocks\");\n-  address start = __ pc();\n-\n-  const Register state        = c_rarg0;\n-  const Register subkeyH      = c_rarg1;\n-  const Register data         = c_rarg2;\n-  const Register blocks       = c_rarg3;\n-\n-  const XMMRegister xmm_temp0 = xmm0;\n-  const XMMRegister xmm_temp1 = xmm1;\n-  const XMMRegister xmm_temp2 = xmm2;\n-  const XMMRegister xmm_temp3 = xmm3;\n-  const XMMRegister xmm_temp4 = xmm4;\n-  const XMMRegister xmm_temp5 = xmm5;\n-  const XMMRegister xmm_temp6 = xmm6;\n-  const XMMRegister xmm_temp7 = xmm7;\n-  const XMMRegister xmm_temp8 = xmm8;\n-  const XMMRegister xmm_temp9 = xmm9;\n-  const XMMRegister xmm_temp10 = xmm10;\n-\n-  __ enter();\n-\n-  __ movdqu(xmm_temp10, ExternalAddress(StubRoutines::x86::ghash_long_swap_mask_addr()));\n-\n-  __ movdqu(xmm_temp0, Address(state, 0));\n-  __ pshufb(xmm_temp0, xmm_temp10);\n-\n-\n-  __ BIND(L_ghash_loop);\n-  __ movdqu(xmm_temp2, Address(data, 0));\n-  __ pshufb(xmm_temp2, ExternalAddress(StubRoutines::x86::ghash_byte_swap_mask_addr()));\n-\n-  __ movdqu(xmm_temp1, Address(subkeyH, 0));\n-  __ pshufb(xmm_temp1, xmm_temp10);\n-\n-  __ pxor(xmm_temp0, xmm_temp2);\n-\n-  \/\/\n-  \/\/ Multiply with the hash key\n-  \/\/\n-  __ movdqu(xmm_temp3, xmm_temp0);\n-  __ pclmulqdq(xmm_temp3, xmm_temp1, 0);      \/\/ xmm3 holds a0*b0\n-  __ movdqu(xmm_temp4, xmm_temp0);\n-  __ pclmulqdq(xmm_temp4, xmm_temp1, 16);     \/\/ xmm4 holds a0*b1\n-\n-  __ movdqu(xmm_temp5, xmm_temp0);\n-  __ pclmulqdq(xmm_temp5, xmm_temp1, 1);      \/\/ xmm5 holds a1*b0\n-  __ movdqu(xmm_temp6, xmm_temp0);\n-  __ pclmulqdq(xmm_temp6, xmm_temp1, 17);     \/\/ xmm6 holds a1*b1\n-\n-  __ pxor(xmm_temp4, xmm_temp5);      \/\/ xmm4 holds a0*b1 + a1*b0\n-\n-  __ movdqu(xmm_temp5, xmm_temp4);    \/\/ move the contents of xmm4 to xmm5\n-  __ psrldq(xmm_temp4, 8);    \/\/ shift by xmm4 64 bits to the right\n-  __ pslldq(xmm_temp5, 8);    \/\/ shift by xmm5 64 bits to the left\n-  __ pxor(xmm_temp3, xmm_temp5);\n-  __ pxor(xmm_temp6, xmm_temp4);      \/\/ Register pair <xmm6:xmm3> holds the result\n-                                      \/\/ of the carry-less multiplication of\n-                                      \/\/ xmm0 by xmm1.\n-\n-  \/\/ We shift the result of the multiplication by one bit position\n-  \/\/ to the left to cope for the fact that the bits are reversed.\n-  __ movdqu(xmm_temp7, xmm_temp3);\n-  __ movdqu(xmm_temp8, xmm_temp6);\n-  __ pslld(xmm_temp3, 1);\n-  __ pslld(xmm_temp6, 1);\n-  __ psrld(xmm_temp7, 31);\n-  __ psrld(xmm_temp8, 31);\n-  __ movdqu(xmm_temp9, xmm_temp7);\n-  __ pslldq(xmm_temp8, 4);\n-  __ pslldq(xmm_temp7, 4);\n-  __ psrldq(xmm_temp9, 12);\n-  __ por(xmm_temp3, xmm_temp7);\n-  __ por(xmm_temp6, xmm_temp8);\n-  __ por(xmm_temp6, xmm_temp9);\n-\n-  \/\/\n-  \/\/ First phase of the reduction\n-  \/\/\n-  \/\/ Move xmm3 into xmm7, xmm8, xmm9 in order to perform the shifts\n-  \/\/ independently.\n-  __ movdqu(xmm_temp7, xmm_temp3);\n-  __ movdqu(xmm_temp8, xmm_temp3);\n-  __ movdqu(xmm_temp9, xmm_temp3);\n-  __ pslld(xmm_temp7, 31);    \/\/ packed right shift shifting << 31\n-  __ pslld(xmm_temp8, 30);    \/\/ packed right shift shifting << 30\n-  __ pslld(xmm_temp9, 25);    \/\/ packed right shift shifting << 25\n-  __ pxor(xmm_temp7, xmm_temp8);      \/\/ xor the shifted versions\n-  __ pxor(xmm_temp7, xmm_temp9);\n-  __ movdqu(xmm_temp8, xmm_temp7);\n-  __ pslldq(xmm_temp7, 12);\n-  __ psrldq(xmm_temp8, 4);\n-  __ pxor(xmm_temp3, xmm_temp7);      \/\/ first phase of the reduction complete\n-\n-  \/\/\n-  \/\/ Second phase of the reduction\n-  \/\/\n-  \/\/ Make 3 copies of xmm3 in xmm2, xmm4, xmm5 for doing these\n-  \/\/ shift operations.\n-  __ movdqu(xmm_temp2, xmm_temp3);\n-  __ movdqu(xmm_temp4, xmm_temp3);\n-  __ movdqu(xmm_temp5, xmm_temp3);\n-  __ psrld(xmm_temp2, 1);     \/\/ packed left shifting >> 1\n-  __ psrld(xmm_temp4, 2);     \/\/ packed left shifting >> 2\n-  __ psrld(xmm_temp5, 7);     \/\/ packed left shifting >> 7\n-  __ pxor(xmm_temp2, xmm_temp4);      \/\/ xor the shifted versions\n-  __ pxor(xmm_temp2, xmm_temp5);\n-  __ pxor(xmm_temp2, xmm_temp8);\n-  __ pxor(xmm_temp3, xmm_temp2);\n-  __ pxor(xmm_temp6, xmm_temp3);      \/\/ the result is in xmm6\n-\n-  __ decrement(blocks);\n-  __ jcc(Assembler::zero, L_exit);\n-  __ movdqu(xmm_temp0, xmm_temp6);\n-  __ addptr(data, 16);\n-  __ jmp(L_ghash_loop);\n-\n-  __ BIND(L_exit);\n-  __ pshufb(xmm_temp6, xmm_temp10);          \/\/ Byte swap 16-byte result\n-  __ movdqu(Address(state, 0), xmm_temp6);   \/\/ store the result\n-  __ leave();\n-  __ ret(0);\n-\n-  return start;\n-}\n-\n@@ -5480,18 +3758,1 @@\n-  \/\/ don't bother generating these AES intrinsic stubs unless global flag is set\n-  if (UseAESIntrinsics) {\n-    StubRoutines::x86::_key_shuffle_mask_addr = generate_key_shuffle_mask();  \/\/ needed by the others\n-    StubRoutines::_aescrypt_encryptBlock = generate_aescrypt_encryptBlock();\n-    StubRoutines::_aescrypt_decryptBlock = generate_aescrypt_decryptBlock();\n-    StubRoutines::_cipherBlockChaining_encryptAESCrypt = generate_cipherBlockChaining_encryptAESCrypt();\n-    if (VM_Version::supports_avx512_vaes() &&  VM_Version::supports_avx512vl() && VM_Version::supports_avx512dq() ) {\n-      StubRoutines::_cipherBlockChaining_decryptAESCrypt = generate_cipherBlockChaining_decryptVectorAESCrypt();\n-      StubRoutines::_electronicCodeBook_encryptAESCrypt = generate_electronicCodeBook_encryptAESCrypt();\n-      StubRoutines::_electronicCodeBook_decryptAESCrypt = generate_electronicCodeBook_decryptAESCrypt();\n-      StubRoutines::x86::_counter_mask_addr = counter_mask_addr();\n-      StubRoutines::x86::_ghash_poly512_addr = ghash_polynomial512_addr();\n-      StubRoutines::x86::_ghash_long_swap_mask_addr = generate_ghash_long_swap_mask();\n-      StubRoutines::_galoisCounterMode_AESCrypt = generate_galoisCounterMode_AESCrypt();\n-    } else {\n-      StubRoutines::_cipherBlockChaining_decryptAESCrypt = generate_cipherBlockChaining_decryptAESCrypt_Parallel();\n-    }\n-  }\n+  generate_aes_stubs();\n@@ -5499,11 +3760,1 @@\n-  if (UseAESCTRIntrinsics) {\n-    if (VM_Version::supports_avx512_vaes() && VM_Version::supports_avx512bw() && VM_Version::supports_avx512vl()) {\n-      if (StubRoutines::x86::_counter_mask_addr == NULL) {\n-        StubRoutines::x86::_counter_mask_addr = counter_mask_addr();\n-      }\n-      StubRoutines::_counterMode_AESCrypt = generate_counterMode_VectorAESCrypt();\n-    } else {\n-      StubRoutines::x86::_counter_shuffle_mask_addr = generate_counter_shuffle_mask();\n-      StubRoutines::_counterMode_AESCrypt = generate_counterMode_AESCrypt_Parallel();\n-    }\n-  }\n+  generate_ghash_stubs();\n@@ -5541,16 +3792,0 @@\n-  \/\/ Generate GHASH intrinsics code\n-  if (UseGHASHIntrinsics) {\n-    if (StubRoutines::x86::_ghash_long_swap_mask_addr == NULL) {\n-      StubRoutines::x86::_ghash_long_swap_mask_addr = generate_ghash_long_swap_mask();\n-    }\n-  StubRoutines::x86::_ghash_byte_swap_mask_addr = generate_ghash_byte_swap_mask();\n-    if (VM_Version::supports_avx()) {\n-      StubRoutines::x86::_ghash_shuffmask_addr = ghash_shufflemask_addr();\n-      StubRoutines::x86::_ghash_poly_addr = ghash_polynomial_addr();\n-      StubRoutines::_ghash_processBlocks = generate_avx_ghash_processBlocks();\n-    } else {\n-      StubRoutines::_ghash_processBlocks = generate_ghash_processBlocks();\n-    }\n-  }\n-\n-\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.cpp","additions":2,"deletions":1767,"binary":false,"changes":1769,"status":"modified"},{"patch":"@@ -28,1 +28,0 @@\n-#include \"code\/codeBlob.hpp\"\n@@ -274,1 +273,0 @@\n-  \/\/ AES intrinsic stubs\n@@ -276,3 +274,1 @@\n-  enum {\n-    AESBlockSize = 16\n-  };\n+  \/\/ MD5 stubs\n@@ -280,1 +276,3 @@\n-  address generate_key_shuffle_mask();\n+  \/\/ ofs and limit are use for multi-block byte array.\n+  \/\/ int com.sun.security.provider.MD5.implCompress(byte[] b, int ofs)\n+  address generate_md5_implCompress(bool multi_block, const char *name);\n@@ -282,1 +280,0 @@\n-  address generate_counter_shuffle_mask();\n@@ -284,3 +281,1 @@\n-  \/\/ Utility routine for loading a 128-bit key word in little endian format\n-  \/\/ can optionally specify that the shuffle mask is already in an xmmregister\n-  void load_key(XMMRegister xmmdst, Register key, int offset, XMMRegister xmm_shuf_mask = xnoreg);\n+  \/\/ SHA stubs\n@@ -288,2 +283,18 @@\n-  \/\/ Utility routine for increase 128bit counter (iv in CTR mode)\n-  void inc_counter(Register reg, XMMRegister xmmdst, int inc_delta, Label& next_block);\n+  \/\/ ofs and limit are use for multi-block byte array.\n+  \/\/ int com.sun.security.provider.DigestBase.implCompressMultiBlock(byte[] b, int ofs, int limit)\n+  address generate_sha1_implCompress(bool multi_block, const char *name);\n+\n+  \/\/ ofs and limit are use for multi-block byte array.\n+  \/\/ int com.sun.security.provider.DigestBase.implCompressMultiBlock(byte[] b, int ofs, int limit)\n+  address generate_sha256_implCompress(bool multi_block, const char *name);\n+  address generate_sha512_implCompress(bool multi_block, const char *name);\n+\n+  \/\/ Mask for byte-swapping a couple of qwords in an XMM register using (v)pshufb.\n+  address generate_pshuffle_byte_flip_mask_sha512();\n+\n+  address generate_upper_word_mask();\n+  address generate_shuffle_byte_flip_mask();\n+  address generate_pshuffle_byte_flip_mask();\n+\n+\n+  \/\/ AES intrinsic stubs\n@@ -303,7 +314,1 @@\n-  address generate_electronicCodeBook_decryptAESCrypt();\n-\n-  \/\/ ofs and limit are use for multi-block byte array.\n-  \/\/ int com.sun.security.provider.MD5.implCompress(byte[] b, int ofs)\n-  address generate_md5_implCompress(bool multi_block, const char *name);\n-\n-  address generate_upper_word_mask();\n+  void aesecb_encrypt(Register source_addr, Register dest_addr, Register key, Register len);\n@@ -311,15 +316,1 @@\n-  address generate_shuffle_byte_flip_mask();\n-\n-  \/\/ ofs and limit are use for multi-block byte array.\n-  \/\/ int com.sun.security.provider.DigestBase.implCompressMultiBlock(byte[] b, int ofs, int limit)\n-  address generate_sha1_implCompress(bool multi_block, const char *name);\n-\n-  address generate_pshuffle_byte_flip_mask();\n-\n-  \/\/ Mask for byte-swapping a couple of qwords in an XMM register using (v)pshufb.\n-  address generate_pshuffle_byte_flip_mask_sha512();\n-\n-  \/\/ ofs and limit are use for multi-block byte array.\n-  \/\/ int com.sun.security.provider.DigestBase.implCompressMultiBlock(byte[] b, int ofs, int limit)\n-  address generate_sha256_implCompress(bool multi_block, const char *name);\n-  address generate_sha512_implCompress(bool multi_block, const char *name);\n+  address generate_electronicCodeBook_decryptAESCrypt();\n@@ -327,1 +318,1 @@\n-  address ghash_polynomial512_addr();\n+  void aesecb_decrypt(Register source_addr, Register dest_addr, Register key, Register len);\n@@ -331,0 +322,2 @@\n+  void aesgcm_encrypt(Register in, Register len, Register ct, Register out, Register key,\n+                      Register state, Register subkeyHtbl, Register avx512_subkeyHtbl, Register counter);\n@@ -332,2 +325,0 @@\n-  \/\/ This mask is used for incrementing counter value(linc0, linc4, etc.)\n-  address counter_mask_addr();\n@@ -337,0 +328,2 @@\n+  void aesctr_encrypt(Register src_addr, Register dest_addr, Register key, Register counter,\n+                      Register len_reg, Register used, Register used_addr, Register saved_encCounter_start);\n@@ -342,1 +335,1 @@\n-  void roundDec(XMMRegister xmm_reg);\n+  address generate_cipherBlockChaining_decryptVectorAESCrypt();\n@@ -344,1 +337,6 @@\n-  void roundDeclast(XMMRegister xmm_reg);\n+  address generate_key_shuffle_mask();\n+\n+  address generate_counter_shuffle_mask();\n+\n+  \/\/ This mask is used for incrementing counter value(linc0, linc4, etc.)\n+  address generate_counter_mask_addr();\n@@ -346,0 +344,15 @@\n+  address generate_ghash_polynomial512_addr();\n+\n+  void roundDec(XMMRegister xmm_reg);\n+  void roundDeclast(XMMRegister xmm_reg);\n+  void roundEnc(XMMRegister key, int rnum);\n+  void lastroundEnc(XMMRegister key, int rnum);\n+  void roundDec(XMMRegister key, int rnum);\n+  void lastroundDec(XMMRegister key, int rnum);\n+  void gfmul_avx512(XMMRegister ghash, XMMRegister hkey);\n+  void generateHtbl_48_block_zmm(Register htbl, Register avx512_subkeyHtbl);\n+  void ghash16_encrypt16_parallel(Register key, Register subkeyHtbl, XMMRegister ctr_blockx,\n+                                  XMMRegister aad_hashx, Register in, Register out, Register data, Register pos, bool reduction,\n+                                  XMMRegister addmask, bool no_ghash_input, Register rounds, Register ghash_pos,\n+                                  bool final_reduction, int index, XMMRegister counter_inc_mask);\n+  \/\/ Load key and shuffle operation\n@@ -348,1 +361,3 @@\n-  address generate_cipherBlockChaining_decryptVectorAESCrypt();\n+  \/\/ Utility routine for loading a 128-bit key word in little endian format\n+  \/\/ can optionally specify that the shuffle mask is already in an xmmregister\n+  void load_key(XMMRegister xmmdst, Register key, int offset, XMMRegister xmm_shuf_mask = xnoreg);\n@@ -350,2 +365,2 @@\n-  \/\/ Polynomial x^128+x^127+x^126+x^121+1\n-  address ghash_polynomial_addr();\n+  \/\/ Utility routine for increase 128bit counter (iv in CTR mode)\n+  void inc_counter(Register reg, XMMRegister xmmdst, int inc_delta, Label& next_block);\n@@ -353,1 +368,1 @@\n-  address ghash_shufflemask_addr();\n+  void generate_aes_stubs();\n@@ -355,2 +370,0 @@\n-  \/\/ Ghash single and multi block operations using AVX instructions\n-  address generate_avx_ghash_processBlocks();\n@@ -358,2 +371,14 @@\n-  \/\/ byte swap x86 long\n-  address generate_ghash_long_swap_mask();\n+  \/\/ GHASH stubs\n+\n+  void generate_ghash_stubs();\n+\n+  void schoolbookAAD(int i, Register subkeyH, XMMRegister data, XMMRegister tmp0,\n+                     XMMRegister tmp1, XMMRegister tmp2, XMMRegister tmp3);\n+  void gfmul(XMMRegister tmp0, XMMRegister t);\n+  void generateHtbl_one_block(Register htbl);\n+  void generateHtbl_eight_blocks(Register htbl);\n+  void avx_ghash(Register state, Register htbl, Register data, Register blocks);\n+\n+  address generate_ghash_polynomial_addr();\n+\n+  address generate_ghash_shufflemask_addr();\n@@ -361,2 +386,3 @@\n-  \/\/ byte swap x86 byte array\n-  address generate_ghash_byte_swap_mask();\n+  address generate_ghash_long_swap_mask(); \/\/ byte swap x86 long\n+\n+  address generate_ghash_byte_swap_mask(); \/\/ byte swap x86 byte array\n@@ -367,0 +393,4 @@\n+  \/\/ Ghash single and multi block operations using AVX instructions\n+  address generate_avx_ghash_processBlocks();\n+\n+\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.hpp","additions":79,"deletions":49,"binary":false,"changes":128,"status":"modified"},{"patch":"@@ -0,0 +1,3160 @@\n+\/*\n+* Copyright (c) 2019, 2021, Intel Corporation. All rights reserved.\n+*\n+* DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+*\n+* This code is free software; you can redistribute it and\/or modify it\n+* under the terms of the GNU General Public License version 2 only, as\n+* published by the Free Software Foundation.\n+*\n+* This code is distributed in the hope that it will be useful, but WITHOUT\n+* ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+* FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+* version 2 for more details (a copy is included in the LICENSE file that\n+* accompanied this code).\n+*\n+* You should have received a copy of the GNU General Public License version\n+* 2 along with this work; if not, write to the Free Software Foundation,\n+* Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+*\n+* Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+* or visit www.oracle.com if you need additional information or have any\n+* questions.\n+*\n+*\/\n+\n+#include \"precompiled.hpp\"\n+#include \"asm\/assembler.hpp\"\n+#include \"asm\/assembler.inline.hpp\"\n+#include \"runtime\/stubRoutines.hpp\"\n+#include \"macroAssembler_x86.hpp\"\n+#include \"stubGenerator_x86_64.hpp\"\n+\n+#define __ _masm->\n+\n+#ifdef PRODUCT\n+#define BLOCK_COMMENT(str) \/* nothing *\/\n+#else\n+#define BLOCK_COMMENT(str) __ block_comment(str)\n+#endif \/\/ PRODUCT\n+\n+#define BIND(label) bind(label); BLOCK_COMMENT(#label \":\")\n+\n+\n+\/\/ AES intrinsic stubs\n+\n+const int AESBlockSize = 16;\n+\n+void StubGenerator::generate_aes_stubs() {\n+  if (UseAESIntrinsics) {\n+    StubRoutines::x86::_key_shuffle_mask_addr = generate_key_shuffle_mask();  \/\/ needed by the others\n+    StubRoutines::_aescrypt_encryptBlock = generate_aescrypt_encryptBlock();\n+    StubRoutines::_aescrypt_decryptBlock = generate_aescrypt_decryptBlock();\n+    StubRoutines::_cipherBlockChaining_encryptAESCrypt = generate_cipherBlockChaining_encryptAESCrypt();\n+    if (VM_Version::supports_avx512_vaes() &&  VM_Version::supports_avx512vl() && VM_Version::supports_avx512dq() ) {\n+      StubRoutines::_cipherBlockChaining_decryptAESCrypt = generate_cipherBlockChaining_decryptVectorAESCrypt();\n+      StubRoutines::_electronicCodeBook_encryptAESCrypt = generate_electronicCodeBook_encryptAESCrypt();\n+      StubRoutines::_electronicCodeBook_decryptAESCrypt = generate_electronicCodeBook_decryptAESCrypt();\n+      StubRoutines::x86::_counter_mask_addr = generate_counter_mask_addr();\n+      StubRoutines::x86::_ghash_poly512_addr = generate_ghash_polynomial512_addr();\n+      StubRoutines::x86::_ghash_long_swap_mask_addr = generate_ghash_long_swap_mask();\n+      StubRoutines::_galoisCounterMode_AESCrypt = generate_galoisCounterMode_AESCrypt();\n+    } else {\n+      StubRoutines::_cipherBlockChaining_decryptAESCrypt = generate_cipherBlockChaining_decryptAESCrypt_Parallel();\n+    }\n+  }\n+\n+  if (UseAESCTRIntrinsics) {\n+    if (VM_Version::supports_avx512_vaes() && VM_Version::supports_avx512bw() && VM_Version::supports_avx512vl()) {\n+      if (StubRoutines::x86::_counter_mask_addr == NULL) {\n+        StubRoutines::x86::_counter_mask_addr = generate_counter_mask_addr();\n+      }\n+      StubRoutines::_counterMode_AESCrypt = generate_counterMode_VectorAESCrypt();\n+    } else {\n+      StubRoutines::x86::_counter_shuffle_mask_addr = generate_counter_shuffle_mask();\n+      StubRoutines::_counterMode_AESCrypt = generate_counterMode_AESCrypt_Parallel();\n+    }\n+  }\n+}\n+\n+address StubGenerator::generate_key_shuffle_mask() {\n+  __ align(16);\n+  StubCodeMark mark(this, \"StubRoutines\", \"key_shuffle_mask\");\n+  address start = __ pc();\n+\n+  __ emit_data64( 0x0405060700010203, relocInfo::none );\n+  __ emit_data64( 0x0c0d0e0f08090a0b, relocInfo::none );\n+\n+  return start;\n+}\n+\n+address StubGenerator::generate_counter_shuffle_mask() {\n+  __ align(16);\n+  StubCodeMark mark(this, \"StubRoutines\", \"counter_shuffle_mask\");\n+  address start = __ pc();\n+\n+  __ emit_data64(0x08090a0b0c0d0e0f, relocInfo::none);\n+  __ emit_data64(0x0001020304050607, relocInfo::none);\n+\n+  return start;\n+}\n+\n+\/\/ This mask is used for incrementing counter value(linc0, linc4, etc.)\n+address StubGenerator::generate_counter_mask_addr() {\n+  __ align64();\n+  StubCodeMark mark(this, \"StubRoutines\", \"counter_mask_addr\");\n+  address start = __ pc();\n+\n+  __ emit_data64(0x08090a0b0c0d0e0f, relocInfo::none);\/\/lbswapmask\n+  __ emit_data64(0x0001020304050607, relocInfo::none);\n+  __ emit_data64(0x08090a0b0c0d0e0f, relocInfo::none);\n+  __ emit_data64(0x0001020304050607, relocInfo::none);\n+  __ emit_data64(0x08090a0b0c0d0e0f, relocInfo::none);\n+  __ emit_data64(0x0001020304050607, relocInfo::none);\n+  __ emit_data64(0x08090a0b0c0d0e0f, relocInfo::none);\n+  __ emit_data64(0x0001020304050607, relocInfo::none);\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\/\/linc0 = counter_mask_addr+64\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+  __ emit_data64(0x0000000000000001, relocInfo::none);\/\/counter_mask_addr() + 80\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+  __ emit_data64(0x0000000000000002, relocInfo::none);\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+  __ emit_data64(0x0000000000000003, relocInfo::none);\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+  __ emit_data64(0x0000000000000004, relocInfo::none);\/\/linc4 = counter_mask_addr() + 128\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+  __ emit_data64(0x0000000000000004, relocInfo::none);\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+  __ emit_data64(0x0000000000000004, relocInfo::none);\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+  __ emit_data64(0x0000000000000004, relocInfo::none);\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+  __ emit_data64(0x0000000000000008, relocInfo::none);\/\/linc8 = counter_mask_addr() + 192\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+  __ emit_data64(0x0000000000000008, relocInfo::none);\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+  __ emit_data64(0x0000000000000008, relocInfo::none);\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+  __ emit_data64(0x0000000000000008, relocInfo::none);\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+  __ emit_data64(0x0000000000000020, relocInfo::none);\/\/linc32 = counter_mask_addr() + 256\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+  __ emit_data64(0x0000000000000020, relocInfo::none);\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+  __ emit_data64(0x0000000000000020, relocInfo::none);\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+  __ emit_data64(0x0000000000000020, relocInfo::none);\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+  __ emit_data64(0x0000000000000010, relocInfo::none);\/\/linc16 = counter_mask_addr() + 320\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+  __ emit_data64(0x0000000000000010, relocInfo::none);\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+  __ emit_data64(0x0000000000000010, relocInfo::none);\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+  __ emit_data64(0x0000000000000010, relocInfo::none);\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+\n+  return start;\n+}\n+\n+address StubGenerator::generate_ghash_polynomial512_addr() {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"_ghash_poly512_addr\");\n+  address start = __ pc();\n+\n+  __ emit_data64(0x00000001C2000000, relocInfo::none); \/\/ POLY for reduction\n+  __ emit_data64(0xC200000000000000, relocInfo::none);\n+  __ emit_data64(0x00000001C2000000, relocInfo::none);\n+  __ emit_data64(0xC200000000000000, relocInfo::none);\n+  __ emit_data64(0x00000001C2000000, relocInfo::none);\n+  __ emit_data64(0xC200000000000000, relocInfo::none);\n+  __ emit_data64(0x00000001C2000000, relocInfo::none);\n+  __ emit_data64(0xC200000000000000, relocInfo::none);\n+  __ emit_data64(0x0000000000000001, relocInfo::none); \/\/ POLY\n+  __ emit_data64(0xC200000000000000, relocInfo::none);\n+  __ emit_data64(0x0000000000000001, relocInfo::none); \/\/ TWOONE\n+  __ emit_data64(0x0000000100000000, relocInfo::none);\n+\n+  return start;\n+}\n+\n+\/\/ Vector AES Galois Counter Mode implementation.\n+\/\/\n+\/\/ Inputs:           Windows    |   Linux\n+\/\/   in         = rcx (c_rarg0) | rsi (c_rarg0)\n+\/\/   len        = rdx (c_rarg1) | rdi (c_rarg1)\n+\/\/   ct         = r8  (c_rarg2) | rdx (c_rarg2)\n+\/\/   out        = r9  (c_rarg3) | rcx (c_rarg3)\n+\/\/   key        = r10           | r8  (c_rarg4)\n+\/\/   state      = r13           | r9  (c_rarg5)\n+\/\/   subkeyHtbl = r14           | r11\n+\/\/   counter    = rsi           | r12\n+\/\/\n+\/\/ Output:\n+\/\/   rax - number of processed bytes\n+address StubGenerator::generate_galoisCounterMode_AESCrypt() {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"galoisCounterMode_AESCrypt\");\n+  address start = __ pc();\n+\n+  const Register in = c_rarg0;\n+  const Register len = c_rarg1;\n+  const Register ct = c_rarg2;\n+  const Register out = c_rarg3;\n+  \/\/ and updated with the incremented counter in the end\n+#ifndef _WIN64\n+  const Register key = c_rarg4;\n+  const Register state = c_rarg5;\n+  const Address subkeyH_mem(rbp, 2 * wordSize);\n+  const Register subkeyHtbl = r11;\n+  const Register avx512_subkeyHtbl = r13;\n+  const Address counter_mem(rbp, 3 * wordSize);\n+  const Register counter = r12;\n+#else\n+  const Address key_mem(rbp, 6 * wordSize);\n+  const Register key = r10;\n+  const Address state_mem(rbp, 7 * wordSize);\n+  const Register state = r13;\n+  const Address subkeyH_mem(rbp, 8 * wordSize);\n+  const Register subkeyHtbl = r14;\n+  const Register avx512_subkeyHtbl = r12;\n+  const Address counter_mem(rbp, 9 * wordSize);\n+  const Register counter = rsi;\n+#endif\n+  __ enter();\n+ \/\/ Save state before entering routine\n+  __ push(r12);\n+  __ push(r13);\n+  __ push(r14);\n+  __ push(r15);\n+  __ push(rbx);\n+#ifdef _WIN64\n+  \/\/ on win64, fill len_reg from stack position\n+  __ push(rsi);\n+  __ movptr(key, key_mem);\n+  __ movptr(state, state_mem);\n+#endif\n+  __ movptr(subkeyHtbl, subkeyH_mem);\n+  __ movptr(counter, counter_mem);\n+\/\/ Save rbp and rsp\n+  __ push(rbp);\n+  __ movq(rbp, rsp);\n+\/\/ Align stack\n+  __ andq(rsp, -64);\n+  __ subptr(rsp, 96 * longSize); \/\/ Create space on the stack for htbl entries\n+  __ movptr(avx512_subkeyHtbl, rsp);\n+\n+  aesgcm_encrypt(in, len, ct, out, key, state, subkeyHtbl, avx512_subkeyHtbl, counter);\n+\n+  __ vzeroupper();\n+\n+  __ movq(rsp, rbp);\n+  __ pop(rbp);\n+\n+  \/\/ Restore state before leaving routine\n+#ifdef _WIN64\n+  __ pop(rsi);\n+#endif\n+  __ pop(rbx);\n+  __ pop(r15);\n+  __ pop(r14);\n+  __ pop(r13);\n+  __ pop(r12);\n+\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n+\n+  return start;\n+}\n+\n+\/\/ Vector AES Counter implementation\n+address StubGenerator::generate_counterMode_VectorAESCrypt()  {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"counterMode_AESCrypt\");\n+  address start = __ pc();\n+\n+  const Register from = c_rarg0; \/\/ source array address\n+  const Register to = c_rarg1; \/\/ destination array address\n+  const Register key = c_rarg2; \/\/ key array address r8\n+  const Register counter = c_rarg3; \/\/ counter byte array initialized from counter array address\n+  \/\/ and updated with the incremented counter in the end\n+#ifndef _WIN64\n+  const Register len_reg = c_rarg4;\n+  const Register saved_encCounter_start = c_rarg5;\n+  const Register used_addr = r10;\n+  const Address  used_mem(rbp, 2 * wordSize);\n+  const Register used = r11;\n+#else\n+  const Address len_mem(rbp, 6 * wordSize); \/\/ length is on stack on Win64\n+  const Address saved_encCounter_mem(rbp, 7 * wordSize); \/\/ saved encrypted counter is on stack on Win64\n+  const Address used_mem(rbp, 8 * wordSize); \/\/ used length is on stack on Win64\n+  const Register len_reg = r10; \/\/ pick the first volatile windows register\n+  const Register saved_encCounter_start = r11;\n+  const Register used_addr = r13;\n+  const Register used = r14;\n+#endif\n+  __ enter();\n+ \/\/ Save state before entering routine\n+  __ push(r12);\n+  __ push(r13);\n+  __ push(r14);\n+  __ push(r15);\n+#ifdef _WIN64\n+  \/\/ on win64, fill len_reg from stack position\n+  __ movl(len_reg, len_mem);\n+  __ movptr(saved_encCounter_start, saved_encCounter_mem);\n+  __ movptr(used_addr, used_mem);\n+  __ movl(used, Address(used_addr, 0));\n+#else\n+  __ push(len_reg); \/\/ Save\n+  __ movptr(used_addr, used_mem);\n+  __ movl(used, Address(used_addr, 0));\n+#endif\n+  __ push(rbx);\n+\n+  aesctr_encrypt(from, to, key, counter, len_reg, used, used_addr, saved_encCounter_start);\n+\n+  __ vzeroupper();\n+  \/\/ Restore state before leaving routine\n+  __ pop(rbx);\n+#ifdef _WIN64\n+  __ movl(rax, len_mem); \/\/ return length\n+#else\n+  __ pop(rax); \/\/ return length\n+#endif\n+  __ pop(r15);\n+  __ pop(r14);\n+  __ pop(r13);\n+  __ pop(r12);\n+\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n+\n+  return start;\n+}\n+\n+\/\/ This is a version of CTR\/AES crypt which does 6 blocks in a loop at a time\n+\/\/ to hide instruction latency\n+\/\/\n+\/\/ Arguments:\n+\/\/\n+\/\/ Inputs:\n+\/\/   c_rarg0   - source byte array address\n+\/\/   c_rarg1   - destination byte array address\n+\/\/   c_rarg2   - K (key) in little endian int array\n+\/\/   c_rarg3   - counter vector byte array address\n+\/\/   Linux\n+\/\/     c_rarg4   -          input length\n+\/\/     c_rarg5   -          saved encryptedCounter start\n+\/\/     rbp + 6 * wordSize - saved used length\n+\/\/   Windows\n+\/\/     rbp + 6 * wordSize - input length\n+\/\/     rbp + 7 * wordSize - saved encryptedCounter start\n+\/\/     rbp + 8 * wordSize - saved used length\n+\/\/\n+\/\/ Output:\n+\/\/   rax       - input length\n+\/\/\n+address StubGenerator::generate_counterMode_AESCrypt_Parallel() {\n+  assert(UseAES, \"need AES instructions and misaligned SSE support\");\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"counterMode_AESCrypt\");\n+  address start = __ pc();\n+\n+  const Register from = c_rarg0; \/\/ source array address\n+  const Register to = c_rarg1; \/\/ destination array address\n+  const Register key = c_rarg2; \/\/ key array address\n+  const Register counter = c_rarg3; \/\/ counter byte array initialized from counter array address\n+                                    \/\/ and updated with the incremented counter in the end\n+#ifndef _WIN64\n+  const Register len_reg = c_rarg4;\n+  const Register saved_encCounter_start = c_rarg5;\n+  const Register used_addr = r10;\n+  const Address  used_mem(rbp, 2 * wordSize);\n+  const Register used = r11;\n+#else\n+  const Address len_mem(rbp, 6 * wordSize); \/\/ length is on stack on Win64\n+  const Address saved_encCounter_mem(rbp, 7 * wordSize); \/\/ length is on stack on Win64\n+  const Address used_mem(rbp, 8 * wordSize); \/\/ length is on stack on Win64\n+  const Register len_reg = r10; \/\/ pick the first volatile windows register\n+  const Register saved_encCounter_start = r11;\n+  const Register used_addr = r13;\n+  const Register used = r14;\n+#endif\n+  const Register pos = rax;\n+\n+  const int PARALLEL_FACTOR = 6;\n+  const XMMRegister xmm_counter_shuf_mask = xmm0;\n+  const XMMRegister xmm_key_shuf_mask = xmm1; \/\/ used temporarily to swap key bytes up front\n+  const XMMRegister xmm_curr_counter = xmm2;\n+\n+  const XMMRegister xmm_key_tmp0 = xmm3;\n+  const XMMRegister xmm_key_tmp1 = xmm4;\n+\n+  \/\/ registers holding the four results in the parallelized loop\n+  const XMMRegister xmm_result0 = xmm5;\n+  const XMMRegister xmm_result1 = xmm6;\n+  const XMMRegister xmm_result2 = xmm7;\n+  const XMMRegister xmm_result3 = xmm8;\n+  const XMMRegister xmm_result4 = xmm9;\n+  const XMMRegister xmm_result5 = xmm10;\n+\n+  const XMMRegister xmm_from0 = xmm11;\n+  const XMMRegister xmm_from1 = xmm12;\n+  const XMMRegister xmm_from2 = xmm13;\n+  const XMMRegister xmm_from3 = xmm14; \/\/the last one is xmm14. we have to preserve it on WIN64.\n+  const XMMRegister xmm_from4 = xmm3; \/\/reuse xmm3~4. Because xmm_key_tmp0~1 are useless when loading input text\n+  const XMMRegister xmm_from5 = xmm4;\n+\n+  \/\/for key_128, key_192, key_256\n+  const int rounds[3] = {10, 12, 14};\n+  Label L_exit_preLoop, L_preLoop_start;\n+  Label L_multiBlock_loopTop[3];\n+  Label L_singleBlockLoopTop[3];\n+  Label L__incCounter[3][6]; \/\/for 6 blocks\n+  Label L__incCounter_single[3]; \/\/for single block, key128, key192, key256\n+  Label L_processTail_insr[3], L_processTail_4_insr[3], L_processTail_2_insr[3], L_processTail_1_insr[3], L_processTail_exit_insr[3];\n+  Label L_processTail_4_extr[3], L_processTail_2_extr[3], L_processTail_1_extr[3], L_processTail_exit_extr[3];\n+\n+  Label L_exit;\n+\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+\n+#ifdef _WIN64\n+  \/\/ allocate spill slots for r13, r14\n+  enum {\n+      saved_r13_offset,\n+      saved_r14_offset\n+  };\n+  __ subptr(rsp, 2 * wordSize);\n+  __ movptr(Address(rsp, saved_r13_offset * wordSize), r13);\n+  __ movptr(Address(rsp, saved_r14_offset * wordSize), r14);\n+\n+  \/\/ on win64, fill len_reg from stack position\n+  __ movl(len_reg, len_mem);\n+  __ movptr(saved_encCounter_start, saved_encCounter_mem);\n+  __ movptr(used_addr, used_mem);\n+  __ movl(used, Address(used_addr, 0));\n+#else\n+  __ push(len_reg); \/\/ Save\n+  __ movptr(used_addr, used_mem);\n+  __ movl(used, Address(used_addr, 0));\n+#endif\n+\n+  __ push(rbx); \/\/ Save RBX\n+  __ movdqu(xmm_curr_counter, Address(counter, 0x00)); \/\/ initialize counter with initial counter\n+  __ movdqu(xmm_counter_shuf_mask, ExternalAddress(StubRoutines::x86::counter_shuffle_mask_addr()), pos); \/\/ pos as scratch\n+  __ pshufb(xmm_curr_counter, xmm_counter_shuf_mask); \/\/counter is shuffled\n+  __ movptr(pos, 0);\n+\n+  \/\/ Use the partially used encrpyted counter from last invocation\n+  __ BIND(L_preLoop_start);\n+  __ cmpptr(used, 16);\n+  __ jcc(Assembler::aboveEqual, L_exit_preLoop);\n+    __ cmpptr(len_reg, 0);\n+    __ jcc(Assembler::lessEqual, L_exit_preLoop);\n+    __ movb(rbx, Address(saved_encCounter_start, used));\n+    __ xorb(rbx, Address(from, pos));\n+    __ movb(Address(to, pos), rbx);\n+    __ addptr(pos, 1);\n+    __ addptr(used, 1);\n+    __ subptr(len_reg, 1);\n+\n+  __ jmp(L_preLoop_start);\n+\n+  __ BIND(L_exit_preLoop);\n+  __ movl(Address(used_addr, 0), used);\n+\n+  \/\/ key length could be only {11, 13, 15} * 4 = {44, 52, 60}\n+  __ movdqu(xmm_key_shuf_mask, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()), rbx); \/\/ rbx as scratch\n+  __ movl(rbx, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));\n+  __ cmpl(rbx, 52);\n+  __ jcc(Assembler::equal, L_multiBlock_loopTop[1]);\n+  __ cmpl(rbx, 60);\n+  __ jcc(Assembler::equal, L_multiBlock_loopTop[2]);\n+\n+#define CTR_DoSix(opc, src_reg)                \\\n+  __ opc(xmm_result0, src_reg);              \\\n+  __ opc(xmm_result1, src_reg);              \\\n+  __ opc(xmm_result2, src_reg);              \\\n+  __ opc(xmm_result3, src_reg);              \\\n+  __ opc(xmm_result4, src_reg);              \\\n+  __ opc(xmm_result5, src_reg);\n+\n+  \/\/ k == 0 :  generate code for key_128\n+  \/\/ k == 1 :  generate code for key_192\n+  \/\/ k == 2 :  generate code for key_256\n+  for (int k = 0; k < 3; ++k) {\n+    \/\/multi blocks starts here\n+    __ align(OptoLoopAlignment);\n+    __ BIND(L_multiBlock_loopTop[k]);\n+    __ cmpptr(len_reg, PARALLEL_FACTOR * AESBlockSize); \/\/ see if at least PARALLEL_FACTOR blocks left\n+    __ jcc(Assembler::less, L_singleBlockLoopTop[k]);\n+    load_key(xmm_key_tmp0, key, 0x00, xmm_key_shuf_mask);\n+\n+    \/\/load, then increase counters\n+    CTR_DoSix(movdqa, xmm_curr_counter);\n+    inc_counter(rbx, xmm_result1, 0x01, L__incCounter[k][0]);\n+    inc_counter(rbx, xmm_result2, 0x02, L__incCounter[k][1]);\n+    inc_counter(rbx, xmm_result3, 0x03, L__incCounter[k][2]);\n+    inc_counter(rbx, xmm_result4, 0x04, L__incCounter[k][3]);\n+    inc_counter(rbx, xmm_result5,  0x05, L__incCounter[k][4]);\n+    inc_counter(rbx, xmm_curr_counter, 0x06, L__incCounter[k][5]);\n+    CTR_DoSix(pshufb, xmm_counter_shuf_mask); \/\/ after increased, shuffled counters back for PXOR\n+    CTR_DoSix(pxor, xmm_key_tmp0);   \/\/PXOR with Round 0 key\n+\n+    \/\/load two ROUND_KEYs at a time\n+    for (int i = 1; i < rounds[k]; ) {\n+      load_key(xmm_key_tmp1, key, (0x10 * i), xmm_key_shuf_mask);\n+      load_key(xmm_key_tmp0, key, (0x10 * (i+1)), xmm_key_shuf_mask);\n+      CTR_DoSix(aesenc, xmm_key_tmp1);\n+      i++;\n+      if (i != rounds[k]) {\n+        CTR_DoSix(aesenc, xmm_key_tmp0);\n+      } else {\n+        CTR_DoSix(aesenclast, xmm_key_tmp0);\n+      }\n+      i++;\n+    }\n+\n+    \/\/ get next PARALLEL_FACTOR blocks into xmm_result registers\n+    __ movdqu(xmm_from0, Address(from, pos, Address::times_1, 0 * AESBlockSize));\n+    __ movdqu(xmm_from1, Address(from, pos, Address::times_1, 1 * AESBlockSize));\n+    __ movdqu(xmm_from2, Address(from, pos, Address::times_1, 2 * AESBlockSize));\n+    __ movdqu(xmm_from3, Address(from, pos, Address::times_1, 3 * AESBlockSize));\n+    __ movdqu(xmm_from4, Address(from, pos, Address::times_1, 4 * AESBlockSize));\n+    __ movdqu(xmm_from5, Address(from, pos, Address::times_1, 5 * AESBlockSize));\n+\n+    __ pxor(xmm_result0, xmm_from0);\n+    __ pxor(xmm_result1, xmm_from1);\n+    __ pxor(xmm_result2, xmm_from2);\n+    __ pxor(xmm_result3, xmm_from3);\n+    __ pxor(xmm_result4, xmm_from4);\n+    __ pxor(xmm_result5, xmm_from5);\n+\n+    \/\/ store 6 results into the next 64 bytes of output\n+    __ movdqu(Address(to, pos, Address::times_1, 0 * AESBlockSize), xmm_result0);\n+    __ movdqu(Address(to, pos, Address::times_1, 1 * AESBlockSize), xmm_result1);\n+    __ movdqu(Address(to, pos, Address::times_1, 2 * AESBlockSize), xmm_result2);\n+    __ movdqu(Address(to, pos, Address::times_1, 3 * AESBlockSize), xmm_result3);\n+    __ movdqu(Address(to, pos, Address::times_1, 4 * AESBlockSize), xmm_result4);\n+    __ movdqu(Address(to, pos, Address::times_1, 5 * AESBlockSize), xmm_result5);\n+\n+    __ addptr(pos, PARALLEL_FACTOR * AESBlockSize); \/\/ increase the length of crypt text\n+    __ subptr(len_reg, PARALLEL_FACTOR * AESBlockSize); \/\/ decrease the remaining length\n+    __ jmp(L_multiBlock_loopTop[k]);\n+\n+    \/\/ singleBlock starts here\n+    __ align(OptoLoopAlignment);\n+    __ BIND(L_singleBlockLoopTop[k]);\n+    __ cmpptr(len_reg, 0);\n+    __ jcc(Assembler::lessEqual, L_exit);\n+    load_key(xmm_key_tmp0, key, 0x00, xmm_key_shuf_mask);\n+    __ movdqa(xmm_result0, xmm_curr_counter);\n+    inc_counter(rbx, xmm_curr_counter, 0x01, L__incCounter_single[k]);\n+    __ pshufb(xmm_result0, xmm_counter_shuf_mask);\n+    __ pxor(xmm_result0, xmm_key_tmp0);\n+    for (int i = 1; i < rounds[k]; i++) {\n+      load_key(xmm_key_tmp0, key, (0x10 * i), xmm_key_shuf_mask);\n+      __ aesenc(xmm_result0, xmm_key_tmp0);\n+    }\n+    load_key(xmm_key_tmp0, key, (rounds[k] * 0x10), xmm_key_shuf_mask);\n+    __ aesenclast(xmm_result0, xmm_key_tmp0);\n+    __ cmpptr(len_reg, AESBlockSize);\n+    __ jcc(Assembler::less, L_processTail_insr[k]);\n+      __ movdqu(xmm_from0, Address(from, pos, Address::times_1, 0 * AESBlockSize));\n+      __ pxor(xmm_result0, xmm_from0);\n+      __ movdqu(Address(to, pos, Address::times_1, 0 * AESBlockSize), xmm_result0);\n+      __ addptr(pos, AESBlockSize);\n+      __ subptr(len_reg, AESBlockSize);\n+      __ jmp(L_singleBlockLoopTop[k]);\n+    __ BIND(L_processTail_insr[k]);                               \/\/ Process the tail part of the input array\n+      __ addptr(pos, len_reg);                                    \/\/ 1. Insert bytes from src array into xmm_from0 register\n+      __ testptr(len_reg, 8);\n+      __ jcc(Assembler::zero, L_processTail_4_insr[k]);\n+        __ subptr(pos,8);\n+        __ pinsrq(xmm_from0, Address(from, pos), 0);\n+      __ BIND(L_processTail_4_insr[k]);\n+      __ testptr(len_reg, 4);\n+      __ jcc(Assembler::zero, L_processTail_2_insr[k]);\n+        __ subptr(pos,4);\n+        __ pslldq(xmm_from0, 4);\n+        __ pinsrd(xmm_from0, Address(from, pos), 0);\n+      __ BIND(L_processTail_2_insr[k]);\n+      __ testptr(len_reg, 2);\n+      __ jcc(Assembler::zero, L_processTail_1_insr[k]);\n+        __ subptr(pos, 2);\n+        __ pslldq(xmm_from0, 2);\n+        __ pinsrw(xmm_from0, Address(from, pos), 0);\n+      __ BIND(L_processTail_1_insr[k]);\n+      __ testptr(len_reg, 1);\n+      __ jcc(Assembler::zero, L_processTail_exit_insr[k]);\n+        __ subptr(pos, 1);\n+        __ pslldq(xmm_from0, 1);\n+        __ pinsrb(xmm_from0, Address(from, pos), 0);\n+      __ BIND(L_processTail_exit_insr[k]);\n+\n+      __ movdqu(Address(saved_encCounter_start, 0), xmm_result0);  \/\/ 2. Perform pxor of the encrypted counter and plaintext Bytes.\n+      __ pxor(xmm_result0, xmm_from0);                             \/\/    Also the encrypted counter is saved for next invocation.\n+\n+      __ testptr(len_reg, 8);\n+      __ jcc(Assembler::zero, L_processTail_4_extr[k]);            \/\/ 3. Extract bytes from xmm_result0 into the dest. array\n+        __ pextrq(Address(to, pos), xmm_result0, 0);\n+        __ psrldq(xmm_result0, 8);\n+        __ addptr(pos, 8);\n+      __ BIND(L_processTail_4_extr[k]);\n+      __ testptr(len_reg, 4);\n+      __ jcc(Assembler::zero, L_processTail_2_extr[k]);\n+        __ pextrd(Address(to, pos), xmm_result0, 0);\n+        __ psrldq(xmm_result0, 4);\n+        __ addptr(pos, 4);\n+      __ BIND(L_processTail_2_extr[k]);\n+      __ testptr(len_reg, 2);\n+      __ jcc(Assembler::zero, L_processTail_1_extr[k]);\n+        __ pextrw(Address(to, pos), xmm_result0, 0);\n+        __ psrldq(xmm_result0, 2);\n+        __ addptr(pos, 2);\n+      __ BIND(L_processTail_1_extr[k]);\n+      __ testptr(len_reg, 1);\n+      __ jcc(Assembler::zero, L_processTail_exit_extr[k]);\n+        __ pextrb(Address(to, pos), xmm_result0, 0);\n+\n+      __ BIND(L_processTail_exit_extr[k]);\n+      __ movl(Address(used_addr, 0), len_reg);\n+      __ jmp(L_exit);\n+  }\n+\n+  __ BIND(L_exit);\n+  __ pshufb(xmm_curr_counter, xmm_counter_shuf_mask); \/\/counter is shuffled back.\n+  __ movdqu(Address(counter, 0), xmm_curr_counter); \/\/save counter back\n+  __ pop(rbx); \/\/ pop the saved RBX.\n+#ifdef _WIN64\n+  __ movl(rax, len_mem);\n+  __ movptr(r13, Address(rsp, saved_r13_offset * wordSize));\n+  __ movptr(r14, Address(rsp, saved_r14_offset * wordSize));\n+  __ addptr(rsp, 2 * wordSize);\n+#else\n+  __ pop(rax); \/\/ return 'len'\n+#endif\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n+\n+  return start;\n+}\n+\n+address StubGenerator::generate_cipherBlockChaining_decryptVectorAESCrypt() {\n+  assert(VM_Version::supports_avx512_vaes(), \"need AES instructions and misaligned SSE support\");\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"cipherBlockChaining_decryptAESCrypt\");\n+  address start = __ pc();\n+\n+  const Register from = c_rarg0;  \/\/ source array address\n+  const Register to = c_rarg1;  \/\/ destination array address\n+  const Register key = c_rarg2;  \/\/ key array address\n+  const Register rvec = c_rarg3;  \/\/ r byte array initialized from initvector array address\n+  \/\/ and left with the results of the last encryption block\n+#ifndef _WIN64\n+  const Register len_reg = c_rarg4;  \/\/ src len (must be multiple of blocksize 16)\n+#else\n+  const Address  len_mem(rbp, 6 * wordSize);  \/\/ length is on stack on Win64\n+  const Register len_reg = r11;      \/\/ pick the volatile windows register\n+#endif\n+\n+  Label Loop, Loop1, L_128, L_256, L_192, KEY_192, KEY_256, Loop2, Lcbc_dec_rem_loop,\n+        Lcbc_dec_rem_last, Lcbc_dec_ret, Lcbc_dec_rem, Lcbc_exit;\n+\n+  __ enter();\n+\n+#ifdef _WIN64\n+\/\/ on win64, fill len_reg from stack position\n+  __ movl(len_reg, len_mem);\n+#else\n+  __ push(len_reg); \/\/ Save\n+#endif\n+  __ push(rbx);\n+  __ vzeroupper();\n+\n+  \/\/ Temporary variable declaration for swapping key bytes\n+  const XMMRegister xmm_key_shuf_mask = xmm1;\n+  __ movdqu(xmm_key_shuf_mask, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()));\n+\n+  \/\/ Calculate number of rounds from key size: 44 for 10-rounds, 52 for 12-rounds, 60 for 14-rounds\n+  const Register rounds = rbx;\n+  __ movl(rounds, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));\n+\n+  const XMMRegister IV = xmm0;\n+  \/\/ Load IV and broadcast value to 512-bits\n+  __ evbroadcasti64x2(IV, Address(rvec, 0), Assembler::AVX_512bit);\n+\n+  \/\/ Temporary variables for storing round keys\n+  const XMMRegister RK0 = xmm30;\n+  const XMMRegister RK1 = xmm9;\n+  const XMMRegister RK2 = xmm18;\n+  const XMMRegister RK3 = xmm19;\n+  const XMMRegister RK4 = xmm20;\n+  const XMMRegister RK5 = xmm21;\n+  const XMMRegister RK6 = xmm22;\n+  const XMMRegister RK7 = xmm23;\n+  const XMMRegister RK8 = xmm24;\n+  const XMMRegister RK9 = xmm25;\n+  const XMMRegister RK10 = xmm26;\n+\n+  \/\/ Load and shuffle key\n+  \/\/ the java expanded key ordering is rotated one position from what we want\n+  \/\/ so we start from 1*16 here and hit 0*16 last\n+  ev_load_key(RK1, key, 1 * 16, xmm_key_shuf_mask);\n+  ev_load_key(RK2, key, 2 * 16, xmm_key_shuf_mask);\n+  ev_load_key(RK3, key, 3 * 16, xmm_key_shuf_mask);\n+  ev_load_key(RK4, key, 4 * 16, xmm_key_shuf_mask);\n+  ev_load_key(RK5, key, 5 * 16, xmm_key_shuf_mask);\n+  ev_load_key(RK6, key, 6 * 16, xmm_key_shuf_mask);\n+  ev_load_key(RK7, key, 7 * 16, xmm_key_shuf_mask);\n+  ev_load_key(RK8, key, 8 * 16, xmm_key_shuf_mask);\n+  ev_load_key(RK9, key, 9 * 16, xmm_key_shuf_mask);\n+  ev_load_key(RK10, key, 10 * 16, xmm_key_shuf_mask);\n+  ev_load_key(RK0, key, 0*16, xmm_key_shuf_mask);\n+\n+  \/\/ Variables for storing source cipher text\n+  const XMMRegister S0 = xmm10;\n+  const XMMRegister S1 = xmm11;\n+  const XMMRegister S2 = xmm12;\n+  const XMMRegister S3 = xmm13;\n+  const XMMRegister S4 = xmm14;\n+  const XMMRegister S5 = xmm15;\n+  const XMMRegister S6 = xmm16;\n+  const XMMRegister S7 = xmm17;\n+\n+  \/\/ Variables for storing decrypted text\n+  const XMMRegister B0 = xmm1;\n+  const XMMRegister B1 = xmm2;\n+  const XMMRegister B2 = xmm3;\n+  const XMMRegister B3 = xmm4;\n+  const XMMRegister B4 = xmm5;\n+  const XMMRegister B5 = xmm6;\n+  const XMMRegister B6 = xmm7;\n+  const XMMRegister B7 = xmm8;\n+\n+  __ cmpl(rounds, 44);\n+  __ jcc(Assembler::greater, KEY_192);\n+  __ jmp(Loop);\n+\n+  __ BIND(KEY_192);\n+  const XMMRegister RK11 = xmm27;\n+  const XMMRegister RK12 = xmm28;\n+  ev_load_key(RK11, key, 11*16, xmm_key_shuf_mask);\n+  ev_load_key(RK12, key, 12*16, xmm_key_shuf_mask);\n+\n+  __ cmpl(rounds, 52);\n+  __ jcc(Assembler::greater, KEY_256);\n+  __ jmp(Loop);\n+\n+  __ BIND(KEY_256);\n+  const XMMRegister RK13 = xmm29;\n+  const XMMRegister RK14 = xmm31;\n+  ev_load_key(RK13, key, 13*16, xmm_key_shuf_mask);\n+  ev_load_key(RK14, key, 14*16, xmm_key_shuf_mask);\n+\n+  __ BIND(Loop);\n+  __ cmpl(len_reg, 512);\n+  __ jcc(Assembler::below, Lcbc_dec_rem);\n+  __ BIND(Loop1);\n+  __ subl(len_reg, 512);\n+  __ evmovdquq(S0, Address(from, 0 * 64), Assembler::AVX_512bit);\n+  __ evmovdquq(S1, Address(from, 1 * 64), Assembler::AVX_512bit);\n+  __ evmovdquq(S2, Address(from, 2 * 64), Assembler::AVX_512bit);\n+  __ evmovdquq(S3, Address(from, 3 * 64), Assembler::AVX_512bit);\n+  __ evmovdquq(S4, Address(from, 4 * 64), Assembler::AVX_512bit);\n+  __ evmovdquq(S5, Address(from, 5 * 64), Assembler::AVX_512bit);\n+  __ evmovdquq(S6, Address(from, 6 * 64), Assembler::AVX_512bit);\n+  __ evmovdquq(S7, Address(from, 7 * 64), Assembler::AVX_512bit);\n+  __ leaq(from, Address(from, 8 * 64));\n+\n+  __ evpxorq(B0, S0, RK1, Assembler::AVX_512bit);\n+  __ evpxorq(B1, S1, RK1, Assembler::AVX_512bit);\n+  __ evpxorq(B2, S2, RK1, Assembler::AVX_512bit);\n+  __ evpxorq(B3, S3, RK1, Assembler::AVX_512bit);\n+  __ evpxorq(B4, S4, RK1, Assembler::AVX_512bit);\n+  __ evpxorq(B5, S5, RK1, Assembler::AVX_512bit);\n+  __ evpxorq(B6, S6, RK1, Assembler::AVX_512bit);\n+  __ evpxorq(B7, S7, RK1, Assembler::AVX_512bit);\n+\n+  __ evalignq(IV, S0, IV, 0x06);\n+  __ evalignq(S0, S1, S0, 0x06);\n+  __ evalignq(S1, S2, S1, 0x06);\n+  __ evalignq(S2, S3, S2, 0x06);\n+  __ evalignq(S3, S4, S3, 0x06);\n+  __ evalignq(S4, S5, S4, 0x06);\n+  __ evalignq(S5, S6, S5, 0x06);\n+  __ evalignq(S6, S7, S6, 0x06);\n+\n+  roundDec(RK2);\n+  roundDec(RK3);\n+  roundDec(RK4);\n+  roundDec(RK5);\n+  roundDec(RK6);\n+  roundDec(RK7);\n+  roundDec(RK8);\n+  roundDec(RK9);\n+  roundDec(RK10);\n+\n+  __ cmpl(rounds, 44);\n+  __ jcc(Assembler::belowEqual, L_128);\n+  roundDec(RK11);\n+  roundDec(RK12);\n+\n+  __ cmpl(rounds, 52);\n+  __ jcc(Assembler::belowEqual, L_192);\n+  roundDec(RK13);\n+  roundDec(RK14);\n+\n+  __ BIND(L_256);\n+  roundDeclast(RK0);\n+  __ jmp(Loop2);\n+\n+  __ BIND(L_128);\n+  roundDeclast(RK0);\n+  __ jmp(Loop2);\n+\n+  __ BIND(L_192);\n+  roundDeclast(RK0);\n+\n+  __ BIND(Loop2);\n+  __ evpxorq(B0, B0, IV, Assembler::AVX_512bit);\n+  __ evpxorq(B1, B1, S0, Assembler::AVX_512bit);\n+  __ evpxorq(B2, B2, S1, Assembler::AVX_512bit);\n+  __ evpxorq(B3, B3, S2, Assembler::AVX_512bit);\n+  __ evpxorq(B4, B4, S3, Assembler::AVX_512bit);\n+  __ evpxorq(B5, B5, S4, Assembler::AVX_512bit);\n+  __ evpxorq(B6, B6, S5, Assembler::AVX_512bit);\n+  __ evpxorq(B7, B7, S6, Assembler::AVX_512bit);\n+  __ evmovdquq(IV, S7, Assembler::AVX_512bit);\n+\n+  __ evmovdquq(Address(to, 0 * 64), B0, Assembler::AVX_512bit);\n+  __ evmovdquq(Address(to, 1 * 64), B1, Assembler::AVX_512bit);\n+  __ evmovdquq(Address(to, 2 * 64), B2, Assembler::AVX_512bit);\n+  __ evmovdquq(Address(to, 3 * 64), B3, Assembler::AVX_512bit);\n+  __ evmovdquq(Address(to, 4 * 64), B4, Assembler::AVX_512bit);\n+  __ evmovdquq(Address(to, 5 * 64), B5, Assembler::AVX_512bit);\n+  __ evmovdquq(Address(to, 6 * 64), B6, Assembler::AVX_512bit);\n+  __ evmovdquq(Address(to, 7 * 64), B7, Assembler::AVX_512bit);\n+  __ leaq(to, Address(to, 8 * 64));\n+  __ jmp(Loop);\n+\n+  __ BIND(Lcbc_dec_rem);\n+  __ evshufi64x2(IV, IV, IV, 0x03, Assembler::AVX_512bit);\n+\n+  __ BIND(Lcbc_dec_rem_loop);\n+  __ subl(len_reg, 16);\n+  __ jcc(Assembler::carrySet, Lcbc_dec_ret);\n+\n+  __ movdqu(S0, Address(from, 0));\n+  __ evpxorq(B0, S0, RK1, Assembler::AVX_512bit);\n+  __ vaesdec(B0, B0, RK2, Assembler::AVX_512bit);\n+  __ vaesdec(B0, B0, RK3, Assembler::AVX_512bit);\n+  __ vaesdec(B0, B0, RK4, Assembler::AVX_512bit);\n+  __ vaesdec(B0, B0, RK5, Assembler::AVX_512bit);\n+  __ vaesdec(B0, B0, RK6, Assembler::AVX_512bit);\n+  __ vaesdec(B0, B0, RK7, Assembler::AVX_512bit);\n+  __ vaesdec(B0, B0, RK8, Assembler::AVX_512bit);\n+  __ vaesdec(B0, B0, RK9, Assembler::AVX_512bit);\n+  __ vaesdec(B0, B0, RK10, Assembler::AVX_512bit);\n+  __ cmpl(rounds, 44);\n+  __ jcc(Assembler::belowEqual, Lcbc_dec_rem_last);\n+\n+  __ vaesdec(B0, B0, RK11, Assembler::AVX_512bit);\n+  __ vaesdec(B0, B0, RK12, Assembler::AVX_512bit);\n+  __ cmpl(rounds, 52);\n+  __ jcc(Assembler::belowEqual, Lcbc_dec_rem_last);\n+\n+  __ vaesdec(B0, B0, RK13, Assembler::AVX_512bit);\n+  __ vaesdec(B0, B0, RK14, Assembler::AVX_512bit);\n+\n+  __ BIND(Lcbc_dec_rem_last);\n+  __ vaesdeclast(B0, B0, RK0, Assembler::AVX_512bit);\n+\n+  __ evpxorq(B0, B0, IV, Assembler::AVX_512bit);\n+  __ evmovdquq(IV, S0, Assembler::AVX_512bit);\n+  __ movdqu(Address(to, 0), B0);\n+  __ leaq(from, Address(from, 16));\n+  __ leaq(to, Address(to, 16));\n+  __ jmp(Lcbc_dec_rem_loop);\n+\n+  __ BIND(Lcbc_dec_ret);\n+  __ movdqu(Address(rvec, 0), IV);\n+\n+  \/\/ Zero out the round keys\n+  __ evpxorq(RK0, RK0, RK0, Assembler::AVX_512bit);\n+  __ evpxorq(RK1, RK1, RK1, Assembler::AVX_512bit);\n+  __ evpxorq(RK2, RK2, RK2, Assembler::AVX_512bit);\n+  __ evpxorq(RK3, RK3, RK3, Assembler::AVX_512bit);\n+  __ evpxorq(RK4, RK4, RK4, Assembler::AVX_512bit);\n+  __ evpxorq(RK5, RK5, RK5, Assembler::AVX_512bit);\n+  __ evpxorq(RK6, RK6, RK6, Assembler::AVX_512bit);\n+  __ evpxorq(RK7, RK7, RK7, Assembler::AVX_512bit);\n+  __ evpxorq(RK8, RK8, RK8, Assembler::AVX_512bit);\n+  __ evpxorq(RK9, RK9, RK9, Assembler::AVX_512bit);\n+  __ evpxorq(RK10, RK10, RK10, Assembler::AVX_512bit);\n+  __ cmpl(rounds, 44);\n+  __ jcc(Assembler::belowEqual, Lcbc_exit);\n+  __ evpxorq(RK11, RK11, RK11, Assembler::AVX_512bit);\n+  __ evpxorq(RK12, RK12, RK12, Assembler::AVX_512bit);\n+  __ cmpl(rounds, 52);\n+  __ jcc(Assembler::belowEqual, Lcbc_exit);\n+  __ evpxorq(RK13, RK13, RK13, Assembler::AVX_512bit);\n+  __ evpxorq(RK14, RK14, RK14, Assembler::AVX_512bit);\n+\n+  __ BIND(Lcbc_exit);\n+  __ vzeroupper();\n+  __ pop(rbx);\n+#ifdef _WIN64\n+  __ movl(rax, len_mem);\n+#else\n+  __ pop(rax); \/\/ return length\n+#endif\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n+\n+  return start;\n+}\n+\n+\/\/ Arguments:\n+\/\/\n+\/\/ Inputs:\n+\/\/   c_rarg0   - source byte array address\n+\/\/   c_rarg1   - destination byte array address\n+\/\/   c_rarg2   - K (key) in little endian int array\n+\/\/\n+address StubGenerator::generate_aescrypt_encryptBlock() {\n+  assert(UseAES, \"need AES instructions and misaligned SSE support\");\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"aescrypt_encryptBlock\");\n+  Label L_doLast;\n+  address start = __ pc();\n+\n+  const Register from        = c_rarg0;  \/\/ source array address\n+  const Register to          = c_rarg1;  \/\/ destination array address\n+  const Register key         = c_rarg2;  \/\/ key array address\n+  const Register keylen      = rax;\n+\n+  const XMMRegister xmm_result = xmm0;\n+  const XMMRegister xmm_key_shuf_mask = xmm1;\n+  \/\/ On win64 xmm6-xmm15 must be preserved so don't use them.\n+  const XMMRegister xmm_temp1  = xmm2;\n+  const XMMRegister xmm_temp2  = xmm3;\n+  const XMMRegister xmm_temp3  = xmm4;\n+  const XMMRegister xmm_temp4  = xmm5;\n+\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+\n+  \/\/ keylen could be only {11, 13, 15} * 4 = {44, 52, 60}\n+  __ movl(keylen, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));\n+\n+  __ movdqu(xmm_key_shuf_mask, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()));\n+  __ movdqu(xmm_result, Address(from, 0));  \/\/ get 16 bytes of input\n+\n+  \/\/ For encryption, the java expanded key ordering is just what we need\n+  \/\/ we don't know if the key is aligned, hence not using load-execute form\n+\n+  load_key(xmm_temp1, key, 0x00, xmm_key_shuf_mask);\n+  __ pxor(xmm_result, xmm_temp1);\n+\n+  load_key(xmm_temp1, key, 0x10, xmm_key_shuf_mask);\n+  load_key(xmm_temp2, key, 0x20, xmm_key_shuf_mask);\n+  load_key(xmm_temp3, key, 0x30, xmm_key_shuf_mask);\n+  load_key(xmm_temp4, key, 0x40, xmm_key_shuf_mask);\n+\n+  __ aesenc(xmm_result, xmm_temp1);\n+  __ aesenc(xmm_result, xmm_temp2);\n+  __ aesenc(xmm_result, xmm_temp3);\n+  __ aesenc(xmm_result, xmm_temp4);\n+\n+  load_key(xmm_temp1, key, 0x50, xmm_key_shuf_mask);\n+  load_key(xmm_temp2, key, 0x60, xmm_key_shuf_mask);\n+  load_key(xmm_temp3, key, 0x70, xmm_key_shuf_mask);\n+  load_key(xmm_temp4, key, 0x80, xmm_key_shuf_mask);\n+\n+  __ aesenc(xmm_result, xmm_temp1);\n+  __ aesenc(xmm_result, xmm_temp2);\n+  __ aesenc(xmm_result, xmm_temp3);\n+  __ aesenc(xmm_result, xmm_temp4);\n+\n+  load_key(xmm_temp1, key, 0x90, xmm_key_shuf_mask);\n+  load_key(xmm_temp2, key, 0xa0, xmm_key_shuf_mask);\n+\n+  __ cmpl(keylen, 44);\n+  __ jccb(Assembler::equal, L_doLast);\n+\n+  __ aesenc(xmm_result, xmm_temp1);\n+  __ aesenc(xmm_result, xmm_temp2);\n+\n+  load_key(xmm_temp1, key, 0xb0, xmm_key_shuf_mask);\n+  load_key(xmm_temp2, key, 0xc0, xmm_key_shuf_mask);\n+\n+  __ cmpl(keylen, 52);\n+  __ jccb(Assembler::equal, L_doLast);\n+\n+  __ aesenc(xmm_result, xmm_temp1);\n+  __ aesenc(xmm_result, xmm_temp2);\n+\n+  load_key(xmm_temp1, key, 0xd0, xmm_key_shuf_mask);\n+  load_key(xmm_temp2, key, 0xe0, xmm_key_shuf_mask);\n+\n+  __ BIND(L_doLast);\n+  __ aesenc(xmm_result, xmm_temp1);\n+  __ aesenclast(xmm_result, xmm_temp2);\n+  __ movdqu(Address(to, 0), xmm_result);        \/\/ store the result\n+  __ xorptr(rax, rax); \/\/ return 0\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n+\n+  return start;\n+}\n+\n+\/\/ Arguments:\n+\/\/\n+\/\/ Inputs:\n+\/\/   c_rarg0   - source byte array address\n+\/\/   c_rarg1   - destination byte array address\n+\/\/   c_rarg2   - K (key) in little endian int array\n+\/\/\n+address StubGenerator::generate_aescrypt_decryptBlock() {\n+  assert(UseAES, \"need AES instructions and misaligned SSE support\");\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"aescrypt_decryptBlock\");\n+  Label L_doLast;\n+  address start = __ pc();\n+\n+  const Register from        = c_rarg0;  \/\/ source array address\n+  const Register to          = c_rarg1;  \/\/ destination array address\n+  const Register key         = c_rarg2;  \/\/ key array address\n+  const Register keylen      = rax;\n+\n+  const XMMRegister xmm_result = xmm0;\n+  const XMMRegister xmm_key_shuf_mask = xmm1;\n+  \/\/ On win64 xmm6-xmm15 must be preserved so don't use them.\n+  const XMMRegister xmm_temp1  = xmm2;\n+  const XMMRegister xmm_temp2  = xmm3;\n+  const XMMRegister xmm_temp3  = xmm4;\n+  const XMMRegister xmm_temp4  = xmm5;\n+\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+\n+  \/\/ keylen could be only {11, 13, 15} * 4 = {44, 52, 60}\n+  __ movl(keylen, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));\n+\n+  __ movdqu(xmm_key_shuf_mask, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()));\n+  __ movdqu(xmm_result, Address(from, 0));\n+\n+  \/\/ for decryption java expanded key ordering is rotated one position from what we want\n+  \/\/ so we start from 0x10 here and hit 0x00 last\n+  \/\/ we don't know if the key is aligned, hence not using load-execute form\n+  load_key(xmm_temp1, key, 0x10, xmm_key_shuf_mask);\n+  load_key(xmm_temp2, key, 0x20, xmm_key_shuf_mask);\n+  load_key(xmm_temp3, key, 0x30, xmm_key_shuf_mask);\n+  load_key(xmm_temp4, key, 0x40, xmm_key_shuf_mask);\n+\n+  __ pxor  (xmm_result, xmm_temp1);\n+  __ aesdec(xmm_result, xmm_temp2);\n+  __ aesdec(xmm_result, xmm_temp3);\n+  __ aesdec(xmm_result, xmm_temp4);\n+\n+  load_key(xmm_temp1, key, 0x50, xmm_key_shuf_mask);\n+  load_key(xmm_temp2, key, 0x60, xmm_key_shuf_mask);\n+  load_key(xmm_temp3, key, 0x70, xmm_key_shuf_mask);\n+  load_key(xmm_temp4, key, 0x80, xmm_key_shuf_mask);\n+\n+  __ aesdec(xmm_result, xmm_temp1);\n+  __ aesdec(xmm_result, xmm_temp2);\n+  __ aesdec(xmm_result, xmm_temp3);\n+  __ aesdec(xmm_result, xmm_temp4);\n+\n+  load_key(xmm_temp1, key, 0x90, xmm_key_shuf_mask);\n+  load_key(xmm_temp2, key, 0xa0, xmm_key_shuf_mask);\n+  load_key(xmm_temp3, key, 0x00, xmm_key_shuf_mask);\n+\n+  __ cmpl(keylen, 44);\n+  __ jccb(Assembler::equal, L_doLast);\n+\n+  __ aesdec(xmm_result, xmm_temp1);\n+  __ aesdec(xmm_result, xmm_temp2);\n+\n+  load_key(xmm_temp1, key, 0xb0, xmm_key_shuf_mask);\n+  load_key(xmm_temp2, key, 0xc0, xmm_key_shuf_mask);\n+\n+  __ cmpl(keylen, 52);\n+  __ jccb(Assembler::equal, L_doLast);\n+\n+  __ aesdec(xmm_result, xmm_temp1);\n+  __ aesdec(xmm_result, xmm_temp2);\n+\n+  load_key(xmm_temp1, key, 0xd0, xmm_key_shuf_mask);\n+  load_key(xmm_temp2, key, 0xe0, xmm_key_shuf_mask);\n+\n+  __ BIND(L_doLast);\n+  __ aesdec(xmm_result, xmm_temp1);\n+  __ aesdec(xmm_result, xmm_temp2);\n+\n+  \/\/ for decryption the aesdeclast operation is always on key+0x00\n+  __ aesdeclast(xmm_result, xmm_temp3);\n+  __ movdqu(Address(to, 0), xmm_result);  \/\/ store the result\n+  __ xorptr(rax, rax); \/\/ return 0\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n+\n+  return start;\n+}\n+\n+\n+\/\/ Arguments:\n+\/\/\n+\/\/ Inputs:\n+\/\/   c_rarg0   - source byte array address\n+\/\/   c_rarg1   - destination byte array address\n+\/\/   c_rarg2   - K (key) in little endian int array\n+\/\/   c_rarg3   - r vector byte array address\n+\/\/   c_rarg4   - input length\n+\/\/\n+\/\/ Output:\n+\/\/   rax       - input length\n+\/\/\n+address StubGenerator::generate_cipherBlockChaining_encryptAESCrypt() {\n+  assert(UseAES, \"need AES instructions and misaligned SSE support\");\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"cipherBlockChaining_encryptAESCrypt\");\n+  address start = __ pc();\n+\n+  Label L_exit, L_key_192_256, L_key_256, L_loopTop_128, L_loopTop_192, L_loopTop_256;\n+  const Register from        = c_rarg0;  \/\/ source array address\n+  const Register to          = c_rarg1;  \/\/ destination array address\n+  const Register key         = c_rarg2;  \/\/ key array address\n+  const Register rvec        = c_rarg3;  \/\/ r byte array initialized from initvector array address\n+                                         \/\/ and left with the results of the last encryption block\n+#ifndef _WIN64\n+  const Register len_reg     = c_rarg4;  \/\/ src len (must be multiple of blocksize 16)\n+#else\n+  const Address  len_mem(rbp, 6 * wordSize);  \/\/ length is on stack on Win64\n+  const Register len_reg     = r11;      \/\/ pick the volatile windows register\n+#endif\n+  const Register pos         = rax;\n+\n+  \/\/ xmm register assignments for the loops below\n+  const XMMRegister xmm_result = xmm0;\n+  const XMMRegister xmm_temp   = xmm1;\n+  \/\/ keys 0-10 preloaded into xmm2-xmm12\n+  const int XMM_REG_NUM_KEY_FIRST = 2;\n+  const int XMM_REG_NUM_KEY_LAST  = 15;\n+  const XMMRegister xmm_key0   = as_XMMRegister(XMM_REG_NUM_KEY_FIRST);\n+  const XMMRegister xmm_key10  = as_XMMRegister(XMM_REG_NUM_KEY_FIRST+10);\n+  const XMMRegister xmm_key11  = as_XMMRegister(XMM_REG_NUM_KEY_FIRST+11);\n+  const XMMRegister xmm_key12  = as_XMMRegister(XMM_REG_NUM_KEY_FIRST+12);\n+  const XMMRegister xmm_key13  = as_XMMRegister(XMM_REG_NUM_KEY_FIRST+13);\n+\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+\n+#ifdef _WIN64\n+  \/\/ on win64, fill len_reg from stack position\n+  __ movl(len_reg, len_mem);\n+#else\n+  __ push(len_reg); \/\/ Save\n+#endif\n+\n+  const XMMRegister xmm_key_shuf_mask = xmm_temp;  \/\/ used temporarily to swap key bytes up front\n+  __ movdqu(xmm_key_shuf_mask, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()));\n+  \/\/ load up xmm regs xmm2 thru xmm12 with key 0x00 - 0xa0\n+  for (int rnum = XMM_REG_NUM_KEY_FIRST, offset = 0x00; rnum <= XMM_REG_NUM_KEY_FIRST+10; rnum++) {\n+    load_key(as_XMMRegister(rnum), key, offset, xmm_key_shuf_mask);\n+    offset += 0x10;\n+  }\n+  __ movdqu(xmm_result, Address(rvec, 0x00));   \/\/ initialize xmm_result with r vec\n+\n+  \/\/ now split to different paths depending on the keylen (len in ints of AESCrypt.KLE array (52=192, or 60=256))\n+  __ movl(rax, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));\n+  __ cmpl(rax, 44);\n+  __ jcc(Assembler::notEqual, L_key_192_256);\n+\n+  \/\/ 128 bit code follows here\n+  __ movptr(pos, 0);\n+  __ align(OptoLoopAlignment);\n+\n+  __ BIND(L_loopTop_128);\n+  __ movdqu(xmm_temp, Address(from, pos, Address::times_1, 0));   \/\/ get next 16 bytes of input\n+  __ pxor  (xmm_result, xmm_temp);               \/\/ xor with the current r vector\n+  __ pxor  (xmm_result, xmm_key0);               \/\/ do the aes rounds\n+  for (int rnum = XMM_REG_NUM_KEY_FIRST + 1; rnum <= XMM_REG_NUM_KEY_FIRST + 9; rnum++) {\n+    __ aesenc(xmm_result, as_XMMRegister(rnum));\n+  }\n+  __ aesenclast(xmm_result, xmm_key10);\n+  __ movdqu(Address(to, pos, Address::times_1, 0), xmm_result);     \/\/ store into the next 16 bytes of output\n+  \/\/ no need to store r to memory until we exit\n+  __ addptr(pos, AESBlockSize);\n+  __ subptr(len_reg, AESBlockSize);\n+  __ jcc(Assembler::notEqual, L_loopTop_128);\n+\n+  __ BIND(L_exit);\n+  __ movdqu(Address(rvec, 0), xmm_result);     \/\/ final value of r stored in rvec of CipherBlockChaining object\n+\n+#ifdef _WIN64\n+  __ movl(rax, len_mem);\n+#else\n+  __ pop(rax); \/\/ return length\n+#endif\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n+\n+  __ BIND(L_key_192_256);\n+  \/\/ here rax = len in ints of AESCrypt.KLE array (52=192, or 60=256)\n+  load_key(xmm_key11, key, 0xb0, xmm_key_shuf_mask);\n+  load_key(xmm_key12, key, 0xc0, xmm_key_shuf_mask);\n+  __ cmpl(rax, 52);\n+  __ jcc(Assembler::notEqual, L_key_256);\n+\n+  \/\/ 192-bit code follows here (could be changed to use more xmm registers)\n+  __ movptr(pos, 0);\n+  __ align(OptoLoopAlignment);\n+\n+  __ BIND(L_loopTop_192);\n+  __ movdqu(xmm_temp, Address(from, pos, Address::times_1, 0));   \/\/ get next 16 bytes of input\n+  __ pxor  (xmm_result, xmm_temp);               \/\/ xor with the current r vector\n+  __ pxor  (xmm_result, xmm_key0);               \/\/ do the aes rounds\n+  for (int rnum = XMM_REG_NUM_KEY_FIRST + 1; rnum  <= XMM_REG_NUM_KEY_FIRST + 11; rnum++) {\n+    __ aesenc(xmm_result, as_XMMRegister(rnum));\n+  }\n+  __ aesenclast(xmm_result, xmm_key12);\n+  __ movdqu(Address(to, pos, Address::times_1, 0), xmm_result);     \/\/ store into the next 16 bytes of output\n+  \/\/ no need to store r to memory until we exit\n+  __ addptr(pos, AESBlockSize);\n+  __ subptr(len_reg, AESBlockSize);\n+  __ jcc(Assembler::notEqual, L_loopTop_192);\n+  __ jmp(L_exit);\n+\n+  __ BIND(L_key_256);\n+  \/\/ 256-bit code follows here (could be changed to use more xmm registers)\n+  load_key(xmm_key13, key, 0xd0, xmm_key_shuf_mask);\n+  __ movptr(pos, 0);\n+  __ align(OptoLoopAlignment);\n+\n+  __ BIND(L_loopTop_256);\n+  __ movdqu(xmm_temp, Address(from, pos, Address::times_1, 0));   \/\/ get next 16 bytes of input\n+  __ pxor  (xmm_result, xmm_temp);               \/\/ xor with the current r vector\n+  __ pxor  (xmm_result, xmm_key0);               \/\/ do the aes rounds\n+  for (int rnum = XMM_REG_NUM_KEY_FIRST + 1; rnum  <= XMM_REG_NUM_KEY_FIRST + 13; rnum++) {\n+    __ aesenc(xmm_result, as_XMMRegister(rnum));\n+  }\n+  load_key(xmm_temp, key, 0xe0);\n+  __ aesenclast(xmm_result, xmm_temp);\n+  __ movdqu(Address(to, pos, Address::times_1, 0), xmm_result);     \/\/ store into the next 16 bytes of output\n+  \/\/ no need to store r to memory until we exit\n+  __ addptr(pos, AESBlockSize);\n+  __ subptr(len_reg, AESBlockSize);\n+  __ jcc(Assembler::notEqual, L_loopTop_256);\n+  __ jmp(L_exit);\n+\n+  return start;\n+}\n+\n+\/\/ This is a version of CBC\/AES Decrypt which does 4 blocks in a loop at a time\n+\/\/ to hide instruction latency\n+\/\/\n+\/\/ Arguments:\n+\/\/\n+\/\/ Inputs:\n+\/\/   c_rarg0   - source byte array address\n+\/\/   c_rarg1   - destination byte array address\n+\/\/   c_rarg2   - K (key) in little endian int array\n+\/\/   c_rarg3   - r vector byte array address\n+\/\/   c_rarg4   - input length\n+\/\/\n+\/\/ Output:\n+\/\/   rax       - input length\n+\/\/\n+address StubGenerator::generate_cipherBlockChaining_decryptAESCrypt_Parallel() {\n+  assert(UseAES, \"need AES instructions and misaligned SSE support\");\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"cipherBlockChaining_decryptAESCrypt\");\n+  address start = __ pc();\n+\n+  const Register from        = c_rarg0;  \/\/ source array address\n+  const Register to          = c_rarg1;  \/\/ destination array address\n+  const Register key         = c_rarg2;  \/\/ key array address\n+  const Register rvec        = c_rarg3;  \/\/ r byte array initialized from initvector array address\n+                                         \/\/ and left with the results of the last encryption block\n+#ifndef _WIN64\n+  const Register len_reg     = c_rarg4;  \/\/ src len (must be multiple of blocksize 16)\n+#else\n+  const Address  len_mem(rbp, 6 * wordSize);  \/\/ length is on stack on Win64\n+  const Register len_reg     = r11;      \/\/ pick the volatile windows register\n+#endif\n+  const Register pos         = rax;\n+\n+  const int PARALLEL_FACTOR = 4;\n+  const int ROUNDS[3] = { 10, 12, 14 }; \/\/ aes rounds for key128, key192, key256\n+\n+  Label L_exit;\n+  Label L_singleBlock_loopTopHead[3]; \/\/ 128, 192, 256\n+  Label L_singleBlock_loopTopHead2[3]; \/\/ 128, 192, 256\n+  Label L_singleBlock_loopTop[3]; \/\/ 128, 192, 256\n+  Label L_multiBlock_loopTopHead[3]; \/\/ 128, 192, 256\n+  Label L_multiBlock_loopTop[3]; \/\/ 128, 192, 256\n+\n+  \/\/ keys 0-10 preloaded into xmm5-xmm15\n+  const int XMM_REG_NUM_KEY_FIRST = 5;\n+  const int XMM_REG_NUM_KEY_LAST  = 15;\n+  const XMMRegister xmm_key_first = as_XMMRegister(XMM_REG_NUM_KEY_FIRST);\n+  const XMMRegister xmm_key_last  = as_XMMRegister(XMM_REG_NUM_KEY_LAST);\n+\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+\n+#ifdef _WIN64\n+  \/\/ on win64, fill len_reg from stack position\n+  __ movl(len_reg, len_mem);\n+#else\n+  __ push(len_reg); \/\/ Save\n+#endif\n+  __ push(rbx);\n+  \/\/ the java expanded key ordering is rotated one position from what we want\n+  \/\/ so we start from 0x10 here and hit 0x00 last\n+  const XMMRegister xmm_key_shuf_mask = xmm1;  \/\/ used temporarily to swap key bytes up front\n+  __ movdqu(xmm_key_shuf_mask, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()));\n+  \/\/ load up xmm regs 5 thru 15 with key 0x10 - 0xa0 - 0x00\n+  for (int rnum = XMM_REG_NUM_KEY_FIRST, offset = 0x10; rnum < XMM_REG_NUM_KEY_LAST; rnum++) {\n+    load_key(as_XMMRegister(rnum), key, offset, xmm_key_shuf_mask);\n+    offset += 0x10;\n+  }\n+  load_key(xmm_key_last, key, 0x00, xmm_key_shuf_mask);\n+\n+  const XMMRegister xmm_prev_block_cipher = xmm1;  \/\/ holds cipher of previous block\n+\n+  \/\/ registers holding the four results in the parallelized loop\n+  const XMMRegister xmm_result0 = xmm0;\n+  const XMMRegister xmm_result1 = xmm2;\n+  const XMMRegister xmm_result2 = xmm3;\n+  const XMMRegister xmm_result3 = xmm4;\n+\n+  __ movdqu(xmm_prev_block_cipher, Address(rvec, 0x00));   \/\/ initialize with initial rvec\n+\n+  __ xorptr(pos, pos);\n+\n+  \/\/ now split to different paths depending on the keylen (len in ints of AESCrypt.KLE array (52=192, or 60=256))\n+  __ movl(rbx, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));\n+  __ cmpl(rbx, 52);\n+  __ jcc(Assembler::equal, L_multiBlock_loopTopHead[1]);\n+  __ cmpl(rbx, 60);\n+  __ jcc(Assembler::equal, L_multiBlock_loopTopHead[2]);\n+\n+#define DoFour(opc, src_reg)           \\\n+__ opc(xmm_result0, src_reg);         \\\n+__ opc(xmm_result1, src_reg);         \\\n+__ opc(xmm_result2, src_reg);         \\\n+__ opc(xmm_result3, src_reg);         \\\n+\n+  for (int k = 0; k < 3; ++k) {\n+    __ BIND(L_multiBlock_loopTopHead[k]);\n+    if (k != 0) {\n+      __ cmpptr(len_reg, PARALLEL_FACTOR * AESBlockSize); \/\/ see if at least 4 blocks left\n+      __ jcc(Assembler::less, L_singleBlock_loopTopHead2[k]);\n+    }\n+    if (k == 1) {\n+      __ subptr(rsp, 6 * wordSize);\n+      __ movdqu(Address(rsp, 0), xmm15); \/\/save last_key from xmm15\n+      load_key(xmm15, key, 0xb0); \/\/ 0xb0; 192-bit key goes up to 0xc0\n+      __ movdqu(Address(rsp, 2 * wordSize), xmm15);\n+      load_key(xmm1, key, 0xc0);  \/\/ 0xc0;\n+      __ movdqu(Address(rsp, 4 * wordSize), xmm1);\n+    } else if (k == 2) {\n+      __ subptr(rsp, 10 * wordSize);\n+      __ movdqu(Address(rsp, 0), xmm15); \/\/save last_key from xmm15\n+      load_key(xmm15, key, 0xd0); \/\/ 0xd0; 256-bit key goes up to 0xe0\n+      __ movdqu(Address(rsp, 6 * wordSize), xmm15);\n+      load_key(xmm1, key, 0xe0);  \/\/ 0xe0;\n+      __ movdqu(Address(rsp, 8 * wordSize), xmm1);\n+      load_key(xmm15, key, 0xb0); \/\/ 0xb0;\n+      __ movdqu(Address(rsp, 2 * wordSize), xmm15);\n+      load_key(xmm1, key, 0xc0);  \/\/ 0xc0;\n+      __ movdqu(Address(rsp, 4 * wordSize), xmm1);\n+    }\n+    __ align(OptoLoopAlignment);\n+    __ BIND(L_multiBlock_loopTop[k]);\n+    __ cmpptr(len_reg, PARALLEL_FACTOR * AESBlockSize); \/\/ see if at least 4 blocks left\n+    __ jcc(Assembler::less, L_singleBlock_loopTopHead[k]);\n+\n+    if  (k != 0) {\n+      __ movdqu(xmm15, Address(rsp, 2 * wordSize));\n+      __ movdqu(xmm1, Address(rsp, 4 * wordSize));\n+    }\n+\n+    __ movdqu(xmm_result0, Address(from, pos, Address::times_1, 0 * AESBlockSize)); \/\/ get next 4 blocks into xmmresult registers\n+    __ movdqu(xmm_result1, Address(from, pos, Address::times_1, 1 * AESBlockSize));\n+    __ movdqu(xmm_result2, Address(from, pos, Address::times_1, 2 * AESBlockSize));\n+    __ movdqu(xmm_result3, Address(from, pos, Address::times_1, 3 * AESBlockSize));\n+\n+    DoFour(pxor, xmm_key_first);\n+    if (k == 0) {\n+      for (int rnum = 1; rnum < ROUNDS[k]; rnum++) {\n+        DoFour(aesdec, as_XMMRegister(rnum + XMM_REG_NUM_KEY_FIRST));\n+      }\n+      DoFour(aesdeclast, xmm_key_last);\n+    } else if (k == 1) {\n+      for (int rnum = 1; rnum <= ROUNDS[k]-2; rnum++) {\n+        DoFour(aesdec, as_XMMRegister(rnum + XMM_REG_NUM_KEY_FIRST));\n+      }\n+      __ movdqu(xmm_key_last, Address(rsp, 0)); \/\/ xmm15 needs to be loaded again.\n+      DoFour(aesdec, xmm1);  \/\/ key : 0xc0\n+      __ movdqu(xmm_prev_block_cipher, Address(rvec, 0x00));  \/\/ xmm1 needs to be loaded again\n+      DoFour(aesdeclast, xmm_key_last);\n+    } else if (k == 2) {\n+      for (int rnum = 1; rnum <= ROUNDS[k] - 4; rnum++) {\n+        DoFour(aesdec, as_XMMRegister(rnum + XMM_REG_NUM_KEY_FIRST));\n+      }\n+      DoFour(aesdec, xmm1);  \/\/ key : 0xc0\n+      __ movdqu(xmm15, Address(rsp, 6 * wordSize));\n+      __ movdqu(xmm1, Address(rsp, 8 * wordSize));\n+      DoFour(aesdec, xmm15);  \/\/ key : 0xd0\n+      __ movdqu(xmm_key_last, Address(rsp, 0)); \/\/ xmm15 needs to be loaded again.\n+      DoFour(aesdec, xmm1);  \/\/ key : 0xe0\n+      __ movdqu(xmm_prev_block_cipher, Address(rvec, 0x00));  \/\/ xmm1 needs to be loaded again\n+      DoFour(aesdeclast, xmm_key_last);\n+    }\n+\n+    \/\/ for each result, xor with the r vector of previous cipher block\n+    __ pxor(xmm_result0, xmm_prev_block_cipher);\n+    __ movdqu(xmm_prev_block_cipher, Address(from, pos, Address::times_1, 0 * AESBlockSize));\n+    __ pxor(xmm_result1, xmm_prev_block_cipher);\n+    __ movdqu(xmm_prev_block_cipher, Address(from, pos, Address::times_1, 1 * AESBlockSize));\n+    __ pxor(xmm_result2, xmm_prev_block_cipher);\n+    __ movdqu(xmm_prev_block_cipher, Address(from, pos, Address::times_1, 2 * AESBlockSize));\n+    __ pxor(xmm_result3, xmm_prev_block_cipher);\n+    __ movdqu(xmm_prev_block_cipher, Address(from, pos, Address::times_1, 3 * AESBlockSize));   \/\/ this will carry over to next set of blocks\n+    if (k != 0) {\n+      __ movdqu(Address(rvec, 0x00), xmm_prev_block_cipher);\n+    }\n+\n+    __ movdqu(Address(to, pos, Address::times_1, 0 * AESBlockSize), xmm_result0);     \/\/ store 4 results into the next 64 bytes of output\n+    __ movdqu(Address(to, pos, Address::times_1, 1 * AESBlockSize), xmm_result1);\n+    __ movdqu(Address(to, pos, Address::times_1, 2 * AESBlockSize), xmm_result2);\n+    __ movdqu(Address(to, pos, Address::times_1, 3 * AESBlockSize), xmm_result3);\n+\n+    __ addptr(pos, PARALLEL_FACTOR * AESBlockSize);\n+    __ subptr(len_reg, PARALLEL_FACTOR * AESBlockSize);\n+    __ jmp(L_multiBlock_loopTop[k]);\n+\n+    \/\/ registers used in the non-parallelized loops\n+    \/\/ xmm register assignments for the loops below\n+    const XMMRegister xmm_result = xmm0;\n+    const XMMRegister xmm_prev_block_cipher_save = xmm2;\n+    const XMMRegister xmm_key11 = xmm3;\n+    const XMMRegister xmm_key12 = xmm4;\n+    const XMMRegister key_tmp = xmm4;\n+\n+    __ BIND(L_singleBlock_loopTopHead[k]);\n+    if (k == 1) {\n+      __ addptr(rsp, 6 * wordSize);\n+    } else if (k == 2) {\n+      __ addptr(rsp, 10 * wordSize);\n+    }\n+    __ cmpptr(len_reg, 0); \/\/ any blocks left??\n+    __ jcc(Assembler::equal, L_exit);\n+    __ BIND(L_singleBlock_loopTopHead2[k]);\n+    if (k == 1) {\n+      load_key(xmm_key11, key, 0xb0); \/\/ 0xb0; 192-bit key goes up to 0xc0\n+      load_key(xmm_key12, key, 0xc0); \/\/ 0xc0; 192-bit key goes up to 0xc0\n+    }\n+    if (k == 2) {\n+      load_key(xmm_key11, key, 0xb0); \/\/ 0xb0; 256-bit key goes up to 0xe0\n+    }\n+    __ align(OptoLoopAlignment);\n+    __ BIND(L_singleBlock_loopTop[k]);\n+    __ movdqu(xmm_result, Address(from, pos, Address::times_1, 0)); \/\/ get next 16 bytes of cipher input\n+    __ movdqa(xmm_prev_block_cipher_save, xmm_result); \/\/ save for next r vector\n+    __ pxor(xmm_result, xmm_key_first); \/\/ do the aes dec rounds\n+    for (int rnum = 1; rnum <= 9 ; rnum++) {\n+        __ aesdec(xmm_result, as_XMMRegister(rnum + XMM_REG_NUM_KEY_FIRST));\n+    }\n+    if (k == 1) {\n+      __ aesdec(xmm_result, xmm_key11);\n+      __ aesdec(xmm_result, xmm_key12);\n+    }\n+    if (k == 2) {\n+      __ aesdec(xmm_result, xmm_key11);\n+      load_key(key_tmp, key, 0xc0);\n+      __ aesdec(xmm_result, key_tmp);\n+      load_key(key_tmp, key, 0xd0);\n+      __ aesdec(xmm_result, key_tmp);\n+      load_key(key_tmp, key, 0xe0);\n+      __ aesdec(xmm_result, key_tmp);\n+    }\n+\n+    __ aesdeclast(xmm_result, xmm_key_last); \/\/ xmm15 always came from key+0\n+    __ pxor(xmm_result, xmm_prev_block_cipher); \/\/ xor with the current r vector\n+    __ movdqu(Address(to, pos, Address::times_1, 0), xmm_result); \/\/ store into the next 16 bytes of output\n+    \/\/ no need to store r to memory until we exit\n+    __ movdqa(xmm_prev_block_cipher, xmm_prev_block_cipher_save); \/\/ set up next r vector with cipher input from this block\n+    __ addptr(pos, AESBlockSize);\n+    __ subptr(len_reg, AESBlockSize);\n+    __ jcc(Assembler::notEqual, L_singleBlock_loopTop[k]);\n+    if (k != 2) {\n+      __ jmp(L_exit);\n+    }\n+  } \/\/for 128\/192\/256\n+\n+  __ BIND(L_exit);\n+  __ movdqu(Address(rvec, 0), xmm_prev_block_cipher);     \/\/ final value of r stored in rvec of CipherBlockChaining object\n+  __ pop(rbx);\n+#ifdef _WIN64\n+  __ movl(rax, len_mem);\n+#else\n+  __ pop(rax); \/\/ return length\n+#endif\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n+\n+  return start;\n+}\n+\n+address StubGenerator::generate_electronicCodeBook_encryptAESCrypt() {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"electronicCodeBook_encryptAESCrypt\");\n+  address start = __ pc();\n+\n+  const Register from = c_rarg0;  \/\/ source array address\n+  const Register to = c_rarg1;  \/\/ destination array address\n+  const Register key = c_rarg2;  \/\/ key array address\n+  const Register len = c_rarg3;  \/\/ src len (must be multiple of blocksize 16)\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+\n+  aesecb_encrypt(from, to, key, len);\n+\n+  __ vzeroupper();\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n+\n+  return start;\n+ }\n+\n+address StubGenerator::generate_electronicCodeBook_decryptAESCrypt() {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"electronicCodeBook_decryptAESCrypt\");\n+  address start = __ pc();\n+\n+  const Register from = c_rarg0;  \/\/ source array address\n+  const Register to = c_rarg1;  \/\/ destination array address\n+  const Register key = c_rarg2;  \/\/ key array address\n+  const Register len = c_rarg3;  \/\/ src len (must be multiple of blocksize 16)\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+\n+  aesecb_decrypt(from, to, key, len);\n+\n+  __ vzeroupper();\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n+\n+  return start;\n+}\n+\n+\/\/ Utility routine for increase 128bit counter (iv in CTR mode)\n+void StubGenerator::inc_counter(Register reg, XMMRegister xmmdst, int inc_delta, Label& next_block) {\n+  __ pextrq(reg, xmmdst, 0x0);\n+  __ addq(reg, inc_delta);\n+  __ pinsrq(xmmdst, reg, 0x0);\n+  __ jcc(Assembler::carryClear, next_block); \/\/ jump if no carry\n+  __ pextrq(reg, xmmdst, 0x01); \/\/ Carry\n+  __ addq(reg, 0x01);\n+  __ pinsrq(xmmdst, reg, 0x01); \/\/Carry end\n+  __ BIND(next_block);          \/\/ next instruction\n+}\n+\n+\n+void StubGenerator::roundEnc(XMMRegister key, int rnum) {\n+  for (int xmm_reg_no = 0; xmm_reg_no <=rnum; xmm_reg_no++) {\n+    __ vaesenc(as_XMMRegister(xmm_reg_no), as_XMMRegister(xmm_reg_no), key, Assembler::AVX_512bit);\n+  }\n+}\n+\n+void StubGenerator::lastroundEnc(XMMRegister key, int rnum) {\n+  for (int xmm_reg_no = 0; xmm_reg_no <=rnum; xmm_reg_no++) {\n+    __ vaesenclast(as_XMMRegister(xmm_reg_no), as_XMMRegister(xmm_reg_no), key, Assembler::AVX_512bit);\n+  }\n+}\n+\n+void StubGenerator::roundDec(XMMRegister key, int rnum) {\n+  for (int xmm_reg_no = 0; xmm_reg_no <=rnum; xmm_reg_no++) {\n+    __ vaesdec(as_XMMRegister(xmm_reg_no), as_XMMRegister(xmm_reg_no), key, Assembler::AVX_512bit);\n+  }\n+}\n+\n+void StubGenerator::lastroundDec(XMMRegister key, int rnum) {\n+  for (int xmm_reg_no = 0; xmm_reg_no <=rnum; xmm_reg_no++) {\n+    __ vaesdeclast(as_XMMRegister(xmm_reg_no), as_XMMRegister(xmm_reg_no), key, Assembler::AVX_512bit);\n+  }\n+}\n+\n+void StubGenerator::roundDec(XMMRegister xmm_reg) {\n+  __ vaesdec(xmm1, xmm1, xmm_reg, Assembler::AVX_512bit);\n+  __ vaesdec(xmm2, xmm2, xmm_reg, Assembler::AVX_512bit);\n+  __ vaesdec(xmm3, xmm3, xmm_reg, Assembler::AVX_512bit);\n+  __ vaesdec(xmm4, xmm4, xmm_reg, Assembler::AVX_512bit);\n+  __ vaesdec(xmm5, xmm5, xmm_reg, Assembler::AVX_512bit);\n+  __ vaesdec(xmm6, xmm6, xmm_reg, Assembler::AVX_512bit);\n+  __ vaesdec(xmm7, xmm7, xmm_reg, Assembler::AVX_512bit);\n+  __ vaesdec(xmm8, xmm8, xmm_reg, Assembler::AVX_512bit);\n+}\n+\n+void StubGenerator::roundDeclast(XMMRegister xmm_reg) {\n+  __ vaesdeclast(xmm1, xmm1, xmm_reg, Assembler::AVX_512bit);\n+  __ vaesdeclast(xmm2, xmm2, xmm_reg, Assembler::AVX_512bit);\n+  __ vaesdeclast(xmm3, xmm3, xmm_reg, Assembler::AVX_512bit);\n+  __ vaesdeclast(xmm4, xmm4, xmm_reg, Assembler::AVX_512bit);\n+  __ vaesdeclast(xmm5, xmm5, xmm_reg, Assembler::AVX_512bit);\n+  __ vaesdeclast(xmm6, xmm6, xmm_reg, Assembler::AVX_512bit);\n+  __ vaesdeclast(xmm7, xmm7, xmm_reg, Assembler::AVX_512bit);\n+  __ vaesdeclast(xmm8, xmm8, xmm_reg, Assembler::AVX_512bit);\n+}\n+\n+\n+\/\/ Utility routine for loading a 128-bit key word in little endian format\n+\/\/ can optionally specify that the shuffle mask is already in an xmmregister\n+void StubGenerator::load_key(XMMRegister xmmdst, Register key, int offset, XMMRegister xmm_shuf_mask) {\n+  __ movdqu(xmmdst, Address(key, offset));\n+  if (xmm_shuf_mask != xnoreg) {\n+    __ pshufb(xmmdst, xmm_shuf_mask);\n+  } else {\n+    __ pshufb(xmmdst, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()));\n+  }\n+}\n+\n+void StubGenerator::ev_load_key(XMMRegister xmmdst, Register key, int offset, XMMRegister xmm_shuf_mask) {\n+  __ movdqu(xmmdst, Address(key, offset));\n+  if (xmm_shuf_mask != xnoreg) {\n+    __ pshufb(xmmdst, xmm_shuf_mask);\n+  } else {\n+    __ pshufb(xmmdst, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()));\n+  }\n+  __ evshufi64x2(xmmdst, xmmdst, xmmdst, 0x0, Assembler::AVX_512bit);\n+}\n+\n+\/\/ AES-ECB Encrypt Operation\n+void StubGenerator::aesecb_encrypt(Register src_addr, Register dest_addr, Register key, Register len) {\n+  const Register pos = rax;\n+  const Register rounds = r12;\n+\n+  Label NO_PARTS, LOOP, Loop_start, LOOP2, AES192, END_LOOP, AES256, REMAINDER, LAST2, END, KEY_192, KEY_256, EXIT;\n+  __ push(r13);\n+  __ push(r12);\n+\n+  \/\/ For EVEX with VL and BW, provide a standard mask, VL = 128 will guide the merge\n+  \/\/ context for the registers used, where all instructions below are using 128-bit mode\n+  \/\/ On EVEX without VL and BW, these instructions will all be AVX.\n+  if (VM_Version::supports_avx512vlbw()) {\n+    __ movl(rax, 0xffff);\n+    __ kmovql(k1, rax);\n+  }\n+  __ push(len); \/\/ Save\n+  __ push(rbx);\n+\n+  __ vzeroupper();\n+\n+  __ xorptr(pos, pos);\n+\n+  \/\/ Calculate number of rounds based on key length(128, 192, 256):44 for 10-rounds, 52 for 12-rounds, 60 for 14-rounds\n+  __ movl(rounds, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));\n+\n+  \/\/ Load Key shuf mask\n+  const XMMRegister xmm_key_shuf_mask = xmm31;  \/\/ used temporarily to swap key bytes up front\n+  __ movdqu(xmm_key_shuf_mask, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()));\n+\n+  \/\/ Load and shuffle key based on number of rounds\n+  ev_load_key(xmm8, key, 0 * 16, xmm_key_shuf_mask);\n+  ev_load_key(xmm9, key, 1 * 16, xmm_key_shuf_mask);\n+  ev_load_key(xmm10, key, 2 * 16, xmm_key_shuf_mask);\n+  ev_load_key(xmm23, key, 3 * 16, xmm_key_shuf_mask);\n+  ev_load_key(xmm12, key, 4 * 16, xmm_key_shuf_mask);\n+  ev_load_key(xmm13, key, 5 * 16, xmm_key_shuf_mask);\n+  ev_load_key(xmm14, key, 6 * 16, xmm_key_shuf_mask);\n+  ev_load_key(xmm15, key, 7 * 16, xmm_key_shuf_mask);\n+  ev_load_key(xmm16, key, 8 * 16, xmm_key_shuf_mask);\n+  ev_load_key(xmm17, key, 9 * 16, xmm_key_shuf_mask);\n+  ev_load_key(xmm24, key, 10 * 16, xmm_key_shuf_mask);\n+  __ cmpl(rounds, 52);\n+  __ jcc(Assembler::greaterEqual, KEY_192);\n+  __ jmp(Loop_start);\n+\n+  __ bind(KEY_192);\n+  ev_load_key(xmm19, key, 11 * 16, xmm_key_shuf_mask);\n+  ev_load_key(xmm20, key, 12 * 16, xmm_key_shuf_mask);\n+  __ cmpl(rounds, 60);\n+  __ jcc(Assembler::equal, KEY_256);\n+  __ jmp(Loop_start);\n+\n+  __ bind(KEY_256);\n+  ev_load_key(xmm21, key, 13 * 16, xmm_key_shuf_mask);\n+  ev_load_key(xmm22, key, 14 * 16, xmm_key_shuf_mask);\n+\n+  __ bind(Loop_start);\n+  __ movq(rbx, len);\n+  \/\/ Divide length by 16 to convert it to number of blocks\n+  __ shrq(len, 4);\n+  __ shlq(rbx, 60);\n+  __ jcc(Assembler::equal, NO_PARTS);\n+  __ addq(len, 1);\n+  \/\/ Check if number of blocks is greater than or equal to 32\n+  \/\/ If true, 512 bytes are processed at a time (code marked by label LOOP)\n+  \/\/ If not, 16 bytes are processed (code marked by REMAINDER label)\n+  __ bind(NO_PARTS);\n+  __ movq(rbx, len);\n+  __ shrq(len, 5);\n+  __ jcc(Assembler::equal, REMAINDER);\n+  __ movl(r13, len);\n+  \/\/ Compute number of blocks that will be processed 512 bytes at a time\n+  \/\/ Subtract this from the total number of blocks which will then be processed by REMAINDER loop\n+  __ shlq(r13, 5);\n+  __ subq(rbx, r13);\n+  \/\/Begin processing 512 bytes\n+  __ bind(LOOP);\n+  \/\/ Move 64 bytes of PT data into a zmm register, as a result 512 bytes of PT loaded in zmm0-7\n+  __ evmovdquq(xmm0, Address(src_addr, pos, Address::times_1, 0 * 64), Assembler::AVX_512bit);\n+  __ evmovdquq(xmm1, Address(src_addr, pos, Address::times_1, 1 * 64), Assembler::AVX_512bit);\n+  __ evmovdquq(xmm2, Address(src_addr, pos, Address::times_1, 2 * 64), Assembler::AVX_512bit);\n+  __ evmovdquq(xmm3, Address(src_addr, pos, Address::times_1, 3 * 64), Assembler::AVX_512bit);\n+  __ evmovdquq(xmm4, Address(src_addr, pos, Address::times_1, 4 * 64), Assembler::AVX_512bit);\n+  __ evmovdquq(xmm5, Address(src_addr, pos, Address::times_1, 5 * 64), Assembler::AVX_512bit);\n+  __ evmovdquq(xmm6, Address(src_addr, pos, Address::times_1, 6 * 64), Assembler::AVX_512bit);\n+  __ evmovdquq(xmm7, Address(src_addr, pos, Address::times_1, 7 * 64), Assembler::AVX_512bit);\n+  \/\/ Xor with the first round key\n+  __ evpxorq(xmm0, xmm0, xmm8, Assembler::AVX_512bit);\n+  __ evpxorq(xmm1, xmm1, xmm8, Assembler::AVX_512bit);\n+  __ evpxorq(xmm2, xmm2, xmm8, Assembler::AVX_512bit);\n+  __ evpxorq(xmm3, xmm3, xmm8, Assembler::AVX_512bit);\n+  __ evpxorq(xmm4, xmm4, xmm8, Assembler::AVX_512bit);\n+  __ evpxorq(xmm5, xmm5, xmm8, Assembler::AVX_512bit);\n+  __ evpxorq(xmm6, xmm6, xmm8, Assembler::AVX_512bit);\n+  __ evpxorq(xmm7, xmm7, xmm8, Assembler::AVX_512bit);\n+  \/\/ 9 Aes encode round operations\n+  roundEnc(xmm9,  7);\n+  roundEnc(xmm10, 7);\n+  roundEnc(xmm23, 7);\n+  roundEnc(xmm12, 7);\n+  roundEnc(xmm13, 7);\n+  roundEnc(xmm14, 7);\n+  roundEnc(xmm15, 7);\n+  roundEnc(xmm16, 7);\n+  roundEnc(xmm17, 7);\n+  __ cmpl(rounds, 52);\n+  __ jcc(Assembler::aboveEqual, AES192);\n+  \/\/ Aesenclast round operation for keysize = 128\n+  lastroundEnc(xmm24, 7);\n+  __ jmp(END_LOOP);\n+  \/\/Additional 2 rounds of Aesenc operation for keysize = 192\n+  __ bind(AES192);\n+  roundEnc(xmm24, 7);\n+  roundEnc(xmm19, 7);\n+  __ cmpl(rounds, 60);\n+  __ jcc(Assembler::aboveEqual, AES256);\n+  \/\/ Aesenclast round for keysize = 192\n+  lastroundEnc(xmm20, 7);\n+  __ jmp(END_LOOP);\n+  \/\/ 2 rounds of Aesenc operation and Aesenclast for keysize = 256\n+  __ bind(AES256);\n+  roundEnc(xmm20, 7);\n+  roundEnc(xmm21, 7);\n+  lastroundEnc(xmm22, 7);\n+\n+  __ bind(END_LOOP);\n+  \/\/ Move 512 bytes of CT to destination\n+  __ evmovdquq(Address(dest_addr, pos, Address::times_1, 0 * 64), xmm0, Assembler::AVX_512bit);\n+  __ evmovdquq(Address(dest_addr, pos, Address::times_1, 1 * 64), xmm1, Assembler::AVX_512bit);\n+  __ evmovdquq(Address(dest_addr, pos, Address::times_1, 2 * 64), xmm2, Assembler::AVX_512bit);\n+  __ evmovdquq(Address(dest_addr, pos, Address::times_1, 3 * 64), xmm3, Assembler::AVX_512bit);\n+  __ evmovdquq(Address(dest_addr, pos, Address::times_1, 4 * 64), xmm4, Assembler::AVX_512bit);\n+  __ evmovdquq(Address(dest_addr, pos, Address::times_1, 5 * 64), xmm5, Assembler::AVX_512bit);\n+  __ evmovdquq(Address(dest_addr, pos, Address::times_1, 6 * 64), xmm6, Assembler::AVX_512bit);\n+  __ evmovdquq(Address(dest_addr, pos, Address::times_1, 7 * 64), xmm7, Assembler::AVX_512bit);\n+\n+  __ addq(pos, 512);\n+  __ decq(len);\n+  __ jcc(Assembler::notEqual, LOOP);\n+\n+  __ bind(REMAINDER);\n+  __ vzeroupper();\n+  __ cmpq(rbx, 0);\n+  __ jcc(Assembler::equal, END);\n+  \/\/ Process 16 bytes at a time\n+  __ bind(LOOP2);\n+  __ movdqu(xmm1, Address(src_addr, pos, Address::times_1, 0));\n+  __ vpxor(xmm1, xmm1, xmm8, Assembler::AVX_128bit);\n+  \/\/ xmm2 contains shuffled key for Aesenclast operation.\n+  __ vmovdqu(xmm2, xmm24);\n+\n+  __ vaesenc(xmm1, xmm1, xmm9, Assembler::AVX_128bit);\n+  __ vaesenc(xmm1, xmm1, xmm10, Assembler::AVX_128bit);\n+  __ vaesenc(xmm1, xmm1, xmm23, Assembler::AVX_128bit);\n+  __ vaesenc(xmm1, xmm1, xmm12, Assembler::AVX_128bit);\n+  __ vaesenc(xmm1, xmm1, xmm13, Assembler::AVX_128bit);\n+  __ vaesenc(xmm1, xmm1, xmm14, Assembler::AVX_128bit);\n+  __ vaesenc(xmm1, xmm1, xmm15, Assembler::AVX_128bit);\n+  __ vaesenc(xmm1, xmm1, xmm16, Assembler::AVX_128bit);\n+  __ vaesenc(xmm1, xmm1, xmm17, Assembler::AVX_128bit);\n+\n+  __ cmpl(rounds, 52);\n+  __ jcc(Assembler::below, LAST2);\n+  __ vmovdqu(xmm2, xmm20);\n+  __ vaesenc(xmm1, xmm1, xmm24, Assembler::AVX_128bit);\n+  __ vaesenc(xmm1, xmm1, xmm19, Assembler::AVX_128bit);\n+  __ cmpl(rounds, 60);\n+  __ jcc(Assembler::below, LAST2);\n+  __ vmovdqu(xmm2, xmm22);\n+  __ vaesenc(xmm1, xmm1, xmm20, Assembler::AVX_128bit);\n+  __ vaesenc(xmm1, xmm1, xmm21, Assembler::AVX_128bit);\n+\n+  __ bind(LAST2);\n+  \/\/ Aesenclast round\n+  __ vaesenclast(xmm1, xmm1, xmm2, Assembler::AVX_128bit);\n+  \/\/ Write 16 bytes of CT to destination\n+  __ movdqu(Address(dest_addr, pos, Address::times_1, 0), xmm1);\n+  __ addq(pos, 16);\n+  __ decq(rbx);\n+  __ jcc(Assembler::notEqual, LOOP2);\n+\n+  __ bind(END);\n+  \/\/ Zero out the round keys\n+  __ evpxorq(xmm8, xmm8, xmm8, Assembler::AVX_512bit);\n+  __ evpxorq(xmm9, xmm9, xmm9, Assembler::AVX_512bit);\n+  __ evpxorq(xmm10, xmm10, xmm10, Assembler::AVX_512bit);\n+  __ evpxorq(xmm23, xmm23, xmm23, Assembler::AVX_512bit);\n+  __ evpxorq(xmm12, xmm12, xmm12, Assembler::AVX_512bit);\n+  __ evpxorq(xmm13, xmm13, xmm13, Assembler::AVX_512bit);\n+  __ evpxorq(xmm14, xmm14, xmm14, Assembler::AVX_512bit);\n+  __ evpxorq(xmm15, xmm15, xmm15, Assembler::AVX_512bit);\n+  __ evpxorq(xmm16, xmm16, xmm16, Assembler::AVX_512bit);\n+  __ evpxorq(xmm17, xmm17, xmm17, Assembler::AVX_512bit);\n+  __ evpxorq(xmm24, xmm24, xmm24, Assembler::AVX_512bit);\n+  __ cmpl(rounds, 44);\n+  __ jcc(Assembler::belowEqual, EXIT);\n+  __ evpxorq(xmm19, xmm19, xmm19, Assembler::AVX_512bit);\n+  __ evpxorq(xmm20, xmm20, xmm20, Assembler::AVX_512bit);\n+  __ cmpl(rounds, 52);\n+  __ jcc(Assembler::belowEqual, EXIT);\n+  __ evpxorq(xmm21, xmm21, xmm21, Assembler::AVX_512bit);\n+  __ evpxorq(xmm22, xmm22, xmm22, Assembler::AVX_512bit);\n+  __ bind(EXIT);\n+  __ pop(rbx);\n+  __ pop(rax); \/\/ return length\n+  __ pop(r12);\n+  __ pop(r13);\n+}\n+\n+\/\/ AES-ECB Decrypt Operation\n+void StubGenerator::aesecb_decrypt(Register src_addr, Register dest_addr, Register key, Register len)  {\n+\n+  Label NO_PARTS, LOOP, Loop_start, LOOP2, AES192, END_LOOP, AES256, REMAINDER, LAST2, END, KEY_192, KEY_256, EXIT;\n+  const Register pos = rax;\n+  const Register rounds = r12;\n+  __ push(r13);\n+  __ push(r12);\n+\n+  \/\/ For EVEX with VL and BW, provide a standard mask, VL = 128 will guide the merge\n+  \/\/ context for the registers used, where all instructions below are using 128-bit mode\n+  \/\/ On EVEX without VL and BW, these instructions will all be AVX.\n+  if (VM_Version::supports_avx512vlbw()) {\n+    __ movl(rax, 0xffff);\n+    __ kmovql(k1, rax);\n+  }\n+\n+  __ push(len); \/\/ Save\n+  __ push(rbx);\n+\n+  __ vzeroupper();\n+\n+  __ xorptr(pos, pos);\n+  \/\/ Calculate number of rounds i.e. based on key length(128, 192, 256):44 for 10-rounds, 52 for 12-rounds, 60 for 14-rounds\n+  __ movl(rounds, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));\n+\n+  \/\/ Load Key shuf mask\n+  const XMMRegister xmm_key_shuf_mask = xmm31;  \/\/ used temporarily to swap key bytes up front\n+  __ movdqu(xmm_key_shuf_mask, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()));\n+\n+  \/\/ Load and shuffle round keys. The java expanded key ordering is rotated one position in decryption.\n+  \/\/ So the first round key is loaded from 1*16 here and last round key is loaded from 0*16\n+  ev_load_key(xmm9,  key, 1 * 16, xmm_key_shuf_mask);\n+  ev_load_key(xmm10, key, 2 * 16, xmm_key_shuf_mask);\n+  ev_load_key(xmm11, key, 3 * 16, xmm_key_shuf_mask);\n+  ev_load_key(xmm12, key, 4 * 16, xmm_key_shuf_mask);\n+  ev_load_key(xmm13, key, 5 * 16, xmm_key_shuf_mask);\n+  ev_load_key(xmm14, key, 6 * 16, xmm_key_shuf_mask);\n+  ev_load_key(xmm15, key, 7 * 16, xmm_key_shuf_mask);\n+  ev_load_key(xmm16, key, 8 * 16, xmm_key_shuf_mask);\n+  ev_load_key(xmm17, key, 9 * 16, xmm_key_shuf_mask);\n+  ev_load_key(xmm18, key, 10 * 16, xmm_key_shuf_mask);\n+  ev_load_key(xmm27, key, 0 * 16, xmm_key_shuf_mask);\n+  __ cmpl(rounds, 52);\n+  __ jcc(Assembler::greaterEqual, KEY_192);\n+  __ jmp(Loop_start);\n+\n+  __ bind(KEY_192);\n+  ev_load_key(xmm19, key, 11 * 16, xmm_key_shuf_mask);\n+  ev_load_key(xmm20, key, 12 * 16, xmm_key_shuf_mask);\n+  __ cmpl(rounds, 60);\n+  __ jcc(Assembler::equal, KEY_256);\n+  __ jmp(Loop_start);\n+\n+  __ bind(KEY_256);\n+  ev_load_key(xmm21, key, 13 * 16, xmm_key_shuf_mask);\n+  ev_load_key(xmm22, key, 14 * 16, xmm_key_shuf_mask);\n+  __ bind(Loop_start);\n+  __ movq(rbx, len);\n+  \/\/ Convert input length to number of blocks\n+  __ shrq(len, 4);\n+  __ shlq(rbx, 60);\n+  __ jcc(Assembler::equal, NO_PARTS);\n+  __ addq(len, 1);\n+  \/\/ Check if number of blocks is greater than\/ equal to 32\n+  \/\/ If true, blocks then 512 bytes are processed at a time (code marked by label LOOP)\n+  \/\/ If not, 16 bytes are processed (code marked by label REMAINDER)\n+  __ bind(NO_PARTS);\n+  __ movq(rbx, len);\n+  __ shrq(len, 5);\n+  __ jcc(Assembler::equal, REMAINDER);\n+  __ movl(r13, len);\n+  \/\/ Compute number of blocks that will be processed as 512 bytes at a time\n+  \/\/ Subtract this from the total number of blocks, which will then be processed by REMAINDER loop.\n+  __ shlq(r13, 5);\n+  __ subq(rbx, r13);\n+\n+  __ bind(LOOP);\n+  \/\/ Move 64 bytes of CT data into a zmm register, as a result 512 bytes of CT loaded in zmm0-7\n+  __ evmovdquq(xmm0, Address(src_addr, pos, Address::times_1, 0 * 64), Assembler::AVX_512bit);\n+  __ evmovdquq(xmm1, Address(src_addr, pos, Address::times_1, 1 * 64), Assembler::AVX_512bit);\n+  __ evmovdquq(xmm2, Address(src_addr, pos, Address::times_1, 2 * 64), Assembler::AVX_512bit);\n+  __ evmovdquq(xmm3, Address(src_addr, pos, Address::times_1, 3 * 64), Assembler::AVX_512bit);\n+  __ evmovdquq(xmm4, Address(src_addr, pos, Address::times_1, 4 * 64), Assembler::AVX_512bit);\n+  __ evmovdquq(xmm5, Address(src_addr, pos, Address::times_1, 5 * 64), Assembler::AVX_512bit);\n+  __ evmovdquq(xmm6, Address(src_addr, pos, Address::times_1, 6 * 64), Assembler::AVX_512bit);\n+  __ evmovdquq(xmm7, Address(src_addr, pos, Address::times_1, 7 * 64), Assembler::AVX_512bit);\n+  \/\/ Xor with the first round key\n+  __ evpxorq(xmm0, xmm0, xmm9, Assembler::AVX_512bit);\n+  __ evpxorq(xmm1, xmm1, xmm9, Assembler::AVX_512bit);\n+  __ evpxorq(xmm2, xmm2, xmm9, Assembler::AVX_512bit);\n+  __ evpxorq(xmm3, xmm3, xmm9, Assembler::AVX_512bit);\n+  __ evpxorq(xmm4, xmm4, xmm9, Assembler::AVX_512bit);\n+  __ evpxorq(xmm5, xmm5, xmm9, Assembler::AVX_512bit);\n+  __ evpxorq(xmm6, xmm6, xmm9, Assembler::AVX_512bit);\n+  __ evpxorq(xmm7, xmm7, xmm9, Assembler::AVX_512bit);\n+  \/\/ 9 rounds of Aesdec\n+  roundDec(xmm10, 7);\n+  roundDec(xmm11, 7);\n+  roundDec(xmm12, 7);\n+  roundDec(xmm13, 7);\n+  roundDec(xmm14, 7);\n+  roundDec(xmm15, 7);\n+  roundDec(xmm16, 7);\n+  roundDec(xmm17, 7);\n+  roundDec(xmm18, 7);\n+  __ cmpl(rounds, 52);\n+  __ jcc(Assembler::aboveEqual, AES192);\n+  \/\/ Aesdeclast round for keysize = 128\n+  lastroundDec(xmm27, 7);\n+  __ jmp(END_LOOP);\n+\n+  __ bind(AES192);\n+  \/\/ 2 Additional rounds for keysize = 192\n+  roundDec(xmm19, 7);\n+  roundDec(xmm20, 7);\n+  __ cmpl(rounds, 60);\n+  __ jcc(Assembler::aboveEqual, AES256);\n+  \/\/ Aesdeclast round for keysize = 192\n+  lastroundDec(xmm27, 7);\n+  __ jmp(END_LOOP);\n+  __ bind(AES256);\n+  \/\/ 2 Additional rounds and Aesdeclast for keysize = 256\n+  roundDec(xmm21, 7);\n+  roundDec(xmm22, 7);\n+  lastroundDec(xmm27, 7);\n+\n+  __ bind(END_LOOP);\n+  \/\/ Write 512 bytes of PT to the destination\n+  __ evmovdquq(Address(dest_addr, pos, Address::times_1, 0 * 64), xmm0, Assembler::AVX_512bit);\n+  __ evmovdquq(Address(dest_addr, pos, Address::times_1, 1 * 64), xmm1, Assembler::AVX_512bit);\n+  __ evmovdquq(Address(dest_addr, pos, Address::times_1, 2 * 64), xmm2, Assembler::AVX_512bit);\n+  __ evmovdquq(Address(dest_addr, pos, Address::times_1, 3 * 64), xmm3, Assembler::AVX_512bit);\n+  __ evmovdquq(Address(dest_addr, pos, Address::times_1, 4 * 64), xmm4, Assembler::AVX_512bit);\n+  __ evmovdquq(Address(dest_addr, pos, Address::times_1, 5 * 64), xmm5, Assembler::AVX_512bit);\n+  __ evmovdquq(Address(dest_addr, pos, Address::times_1, 6 * 64), xmm6, Assembler::AVX_512bit);\n+  __ evmovdquq(Address(dest_addr, pos, Address::times_1, 7 * 64), xmm7, Assembler::AVX_512bit);\n+\n+  __ addq(pos, 512);\n+  __ decq(len);\n+  __ jcc(Assembler::notEqual, LOOP);\n+\n+  __ bind(REMAINDER);\n+  __ vzeroupper();\n+  __ cmpq(rbx, 0);\n+  __ jcc(Assembler::equal, END);\n+  \/\/ Process 16 bytes at a time\n+  __ bind(LOOP2);\n+  __ movdqu(xmm1, Address(src_addr, pos, Address::times_1, 0));\n+  __ vpxor(xmm1, xmm1, xmm9, Assembler::AVX_128bit);\n+  \/\/ xmm2 contains shuffled key for Aesdeclast operation.\n+  __ vmovdqu(xmm2, xmm27);\n+\n+  __ vaesdec(xmm1, xmm1, xmm10, Assembler::AVX_128bit);\n+  __ vaesdec(xmm1, xmm1, xmm11, Assembler::AVX_128bit);\n+  __ vaesdec(xmm1, xmm1, xmm12, Assembler::AVX_128bit);\n+  __ vaesdec(xmm1, xmm1, xmm13, Assembler::AVX_128bit);\n+  __ vaesdec(xmm1, xmm1, xmm14, Assembler::AVX_128bit);\n+  __ vaesdec(xmm1, xmm1, xmm15, Assembler::AVX_128bit);\n+  __ vaesdec(xmm1, xmm1, xmm16, Assembler::AVX_128bit);\n+  __ vaesdec(xmm1, xmm1, xmm17, Assembler::AVX_128bit);\n+  __ vaesdec(xmm1, xmm1, xmm18, Assembler::AVX_128bit);\n+\n+  __ cmpl(rounds, 52);\n+  __ jcc(Assembler::below, LAST2);\n+  __ vaesdec(xmm1, xmm1, xmm19, Assembler::AVX_128bit);\n+  __ vaesdec(xmm1, xmm1, xmm20, Assembler::AVX_128bit);\n+  __ cmpl(rounds, 60);\n+  __ jcc(Assembler::below, LAST2);\n+  __ vaesdec(xmm1, xmm1, xmm21, Assembler::AVX_128bit);\n+  __ vaesdec(xmm1, xmm1, xmm22, Assembler::AVX_128bit);\n+\n+  __ bind(LAST2);\n+  \/\/ Aesdeclast round\n+  __ vaesdeclast(xmm1, xmm1, xmm2, Assembler::AVX_128bit);\n+  \/\/ Write 16 bytes of PT to destination\n+  __ movdqu(Address(dest_addr, pos, Address::times_1, 0), xmm1);\n+  __ addq(pos, 16);\n+  __ decq(rbx);\n+  __ jcc(Assembler::notEqual, LOOP2);\n+\n+  __ bind(END);\n+  \/\/ Zero out the round keys\n+  __ evpxorq(xmm8, xmm8, xmm8, Assembler::AVX_512bit);\n+  __ evpxorq(xmm9, xmm9, xmm9, Assembler::AVX_512bit);\n+  __ evpxorq(xmm10, xmm10, xmm10, Assembler::AVX_512bit);\n+  __ evpxorq(xmm11, xmm11, xmm11, Assembler::AVX_512bit);\n+  __ evpxorq(xmm12, xmm12, xmm12, Assembler::AVX_512bit);\n+  __ evpxorq(xmm13, xmm13, xmm13, Assembler::AVX_512bit);\n+  __ evpxorq(xmm14, xmm14, xmm14, Assembler::AVX_512bit);\n+  __ evpxorq(xmm15, xmm15, xmm15, Assembler::AVX_512bit);\n+  __ evpxorq(xmm16, xmm16, xmm16, Assembler::AVX_512bit);\n+  __ evpxorq(xmm17, xmm17, xmm17, Assembler::AVX_512bit);\n+  __ evpxorq(xmm18, xmm18, xmm18, Assembler::AVX_512bit);\n+  __ evpxorq(xmm27, xmm27, xmm27, Assembler::AVX_512bit);\n+  __ cmpl(rounds, 44);\n+  __ jcc(Assembler::belowEqual, EXIT);\n+  __ evpxorq(xmm19, xmm19, xmm19, Assembler::AVX_512bit);\n+  __ evpxorq(xmm20, xmm20, xmm20, Assembler::AVX_512bit);\n+  __ cmpl(rounds, 52);\n+  __ jcc(Assembler::belowEqual, EXIT);\n+  __ evpxorq(xmm21, xmm21, xmm21, Assembler::AVX_512bit);\n+  __ evpxorq(xmm22, xmm22, xmm22, Assembler::AVX_512bit);\n+\n+  __ bind(EXIT);\n+  __ pop(rbx);\n+  __ pop(rax); \/\/ return length\n+  __ pop(r12);\n+  __ pop(r13);\n+}\n+\n+\n+\n+\/\/ AES Counter Mode using VAES instructions\n+void StubGenerator::aesctr_encrypt(Register src_addr, Register dest_addr, Register key, Register counter,\n+  Register len_reg, Register used, Register used_addr, Register saved_encCounter_start) {\n+\n+  const Register rounds = rax;\n+  const Register pos = r12;\n+\n+  Label PRELOOP_START, EXIT_PRELOOP, REMAINDER, REMAINDER_16, LOOP, END, EXIT, END_LOOP,\n+  AES192, AES256, AES192_REMAINDER16, REMAINDER16_END_LOOP, AES256_REMAINDER16,\n+  REMAINDER_8, REMAINDER_4, AES192_REMAINDER8, REMAINDER_LOOP, AES256_REMINDER,\n+  AES192_REMAINDER, END_REMAINDER_LOOP, AES256_REMAINDER8, REMAINDER8_END_LOOP,\n+  AES192_REMAINDER4, AES256_REMAINDER4, AES256_REMAINDER, END_REMAINDER4, EXTRACT_TAILBYTES,\n+  EXTRACT_TAIL_4BYTES, EXTRACT_TAIL_2BYTES, EXTRACT_TAIL_1BYTE, STORE_CTR;\n+\n+  __ cmpl(len_reg, 0);\n+  __ jcc(Assembler::belowEqual, EXIT);\n+\n+  __ movl(pos, 0);\n+  \/\/ if the number of used encrypted counter bytes < 16,\n+  \/\/ XOR PT with saved encrypted counter to obtain CT\n+  __ bind(PRELOOP_START);\n+  __ cmpl(used, 16);\n+  __ jcc(Assembler::aboveEqual, EXIT_PRELOOP);\n+  __ movb(rbx, Address(saved_encCounter_start, used));\n+  __ xorb(rbx, Address(src_addr, pos));\n+  __ movb(Address(dest_addr, pos), rbx);\n+  __ addptr(pos, 1);\n+  __ addptr(used, 1);\n+  __ decrement(len_reg);\n+  __ jmp(PRELOOP_START);\n+\n+  __ bind(EXIT_PRELOOP);\n+  __ movl(Address(used_addr, 0), used);\n+\n+  \/\/ Calculate number of rounds i.e. 10, 12, 14,  based on key length(128, 192, 256).\n+  __ movl(rounds, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));\n+\n+  __ vpxor(xmm0, xmm0, xmm0, Assembler::AVX_128bit);\n+  \/\/ Move initial counter value in xmm0\n+  __ movdqu(xmm0, Address(counter, 0));\n+  \/\/ broadcast counter value to zmm8\n+  __ evshufi64x2(xmm8, xmm0, xmm0, 0, Assembler::AVX_512bit);\n+\n+  \/\/ load lbswap mask\n+  __ evmovdquq(xmm16, ExternalAddress(StubRoutines::x86::counter_mask_addr()), Assembler::AVX_512bit, r15);\n+\n+  \/\/shuffle counter using lbswap_mask\n+  __ vpshufb(xmm8, xmm8, xmm16, Assembler::AVX_512bit);\n+\n+  \/\/ pre-increment and propagate counter values to zmm9-zmm15 registers.\n+  \/\/ Linc0 increments the zmm8 by 1 (initial value being 0), Linc4 increments the counters zmm9-zmm15 by 4\n+  \/\/ The counter is incremented after each block i.e. 16 bytes is processed;\n+  \/\/ each zmm register has 4 counter values as its MSB\n+  \/\/ the counters are incremented in parallel\n+  __ vpaddd(xmm8, xmm8, ExternalAddress(StubRoutines::x86::counter_mask_addr() + 64), Assembler::AVX_512bit, r15);\/\/linc0\n+  __ vpaddd(xmm9, xmm8, ExternalAddress(StubRoutines::x86::counter_mask_addr() + 128), Assembler::AVX_512bit, r15);\/\/linc4(rip)\n+  __ vpaddd(xmm10, xmm9, ExternalAddress(StubRoutines::x86::counter_mask_addr() + 128), Assembler::AVX_512bit, r15);\/\/Linc4(rip)\n+  __ vpaddd(xmm11, xmm10, ExternalAddress(StubRoutines::x86::counter_mask_addr() + 128), Assembler::AVX_512bit, r15);\/\/Linc4(rip)\n+  __ vpaddd(xmm12, xmm11, ExternalAddress(StubRoutines::x86::counter_mask_addr() + 128), Assembler::AVX_512bit, r15);\/\/Linc4(rip)\n+  __ vpaddd(xmm13, xmm12, ExternalAddress(StubRoutines::x86::counter_mask_addr() + 128), Assembler::AVX_512bit, r15);\/\/Linc4(rip)\n+  __ vpaddd(xmm14, xmm13, ExternalAddress(StubRoutines::x86::counter_mask_addr() + 128), Assembler::AVX_512bit, r15);\/\/Linc4(rip)\n+  __ vpaddd(xmm15, xmm14, ExternalAddress(StubRoutines::x86::counter_mask_addr() + 128), Assembler::AVX_512bit, r15);\/\/Linc4(rip)\n+\n+  \/\/ load linc32 mask in zmm register.linc32 increments counter by 32\n+  __ evmovdquq(xmm19, ExternalAddress(StubRoutines::x86::counter_mask_addr() + 256), Assembler::AVX_512bit, r15);\/\/Linc32\n+\n+  \/\/ xmm31 contains the key shuffle mask.\n+  __ movdqu(xmm31, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()));\n+  \/\/ Load key function loads 128 bit key and shuffles it. Then we broadcast the shuffled key to convert it into a 512 bit value.\n+  \/\/ For broadcasting the values to ZMM, vshufi64 is used instead of evbroadcasti64x2 as the source in this case is ZMM register\n+  \/\/ that holds shuffled key value.\n+  ev_load_key(xmm20, key, 0, xmm31);\n+  ev_load_key(xmm21, key, 1 * 16, xmm31);\n+  ev_load_key(xmm22, key, 2 * 16, xmm31);\n+  ev_load_key(xmm23, key, 3 * 16, xmm31);\n+  ev_load_key(xmm24, key, 4 * 16, xmm31);\n+  ev_load_key(xmm25, key, 5 * 16, xmm31);\n+  ev_load_key(xmm26, key, 6 * 16, xmm31);\n+  ev_load_key(xmm27, key, 7 * 16, xmm31);\n+  ev_load_key(xmm28, key, 8 * 16, xmm31);\n+  ev_load_key(xmm29, key, 9 * 16, xmm31);\n+  ev_load_key(xmm30, key, 10 * 16, xmm31);\n+\n+  \/\/ Process 32 blocks or 512 bytes of data\n+  __ bind(LOOP);\n+  __ cmpl(len_reg, 512);\n+  __ jcc(Assembler::less, REMAINDER);\n+  __ subq(len_reg, 512);\n+  \/\/Shuffle counter and Exor it with roundkey1. Result is stored in zmm0-7\n+  __ vpshufb(xmm0, xmm8, xmm16, Assembler::AVX_512bit);\n+  __ evpxorq(xmm0, xmm0, xmm20, Assembler::AVX_512bit);\n+  __ vpshufb(xmm1, xmm9, xmm16, Assembler::AVX_512bit);\n+  __ evpxorq(xmm1, xmm1, xmm20, Assembler::AVX_512bit);\n+  __ vpshufb(xmm2, xmm10, xmm16, Assembler::AVX_512bit);\n+  __ evpxorq(xmm2, xmm2, xmm20, Assembler::AVX_512bit);\n+  __ vpshufb(xmm3, xmm11, xmm16, Assembler::AVX_512bit);\n+  __ evpxorq(xmm3, xmm3, xmm20, Assembler::AVX_512bit);\n+  __ vpshufb(xmm4, xmm12, xmm16, Assembler::AVX_512bit);\n+  __ evpxorq(xmm4, xmm4, xmm20, Assembler::AVX_512bit);\n+  __ vpshufb(xmm5, xmm13, xmm16, Assembler::AVX_512bit);\n+  __ evpxorq(xmm5, xmm5, xmm20, Assembler::AVX_512bit);\n+  __ vpshufb(xmm6, xmm14, xmm16, Assembler::AVX_512bit);\n+  __ evpxorq(xmm6, xmm6, xmm20, Assembler::AVX_512bit);\n+  __ vpshufb(xmm7, xmm15, xmm16, Assembler::AVX_512bit);\n+  __ evpxorq(xmm7, xmm7, xmm20, Assembler::AVX_512bit);\n+  \/\/ Perform AES encode operations and put results in zmm0-zmm7.\n+  \/\/ This is followed by incrementing counter values in zmm8-zmm15.\n+  \/\/ Since we will be processing 32 blocks at a time, the counter is incremented by 32.\n+  roundEnc(xmm21, 7);\n+  __ vpaddq(xmm8, xmm8, xmm19, Assembler::AVX_512bit);\n+  roundEnc(xmm22, 7);\n+  __ vpaddq(xmm9, xmm9, xmm19, Assembler::AVX_512bit);\n+  roundEnc(xmm23, 7);\n+  __ vpaddq(xmm10, xmm10, xmm19, Assembler::AVX_512bit);\n+  roundEnc(xmm24, 7);\n+  __ vpaddq(xmm11, xmm11, xmm19, Assembler::AVX_512bit);\n+  roundEnc(xmm25, 7);\n+  __ vpaddq(xmm12, xmm12, xmm19, Assembler::AVX_512bit);\n+  roundEnc(xmm26, 7);\n+  __ vpaddq(xmm13, xmm13, xmm19, Assembler::AVX_512bit);\n+  roundEnc(xmm27, 7);\n+  __ vpaddq(xmm14, xmm14, xmm19, Assembler::AVX_512bit);\n+  roundEnc(xmm28, 7);\n+  __ vpaddq(xmm15, xmm15, xmm19, Assembler::AVX_512bit);\n+  roundEnc(xmm29, 7);\n+\n+  __ cmpl(rounds, 52);\n+  __ jcc(Assembler::aboveEqual, AES192);\n+  lastroundEnc(xmm30, 7);\n+  __ jmp(END_LOOP);\n+\n+  __ bind(AES192);\n+  roundEnc(xmm30, 7);\n+  ev_load_key(xmm18, key, 11 * 16, xmm31);\n+  roundEnc(xmm18, 7);\n+  __ cmpl(rounds, 60);\n+  __ jcc(Assembler::aboveEqual, AES256);\n+  ev_load_key(xmm18, key, 12 * 16, xmm31);\n+  lastroundEnc(xmm18, 7);\n+  __ jmp(END_LOOP);\n+\n+  __ bind(AES256);\n+  ev_load_key(xmm18, key, 12 * 16, xmm31);\n+  roundEnc(xmm18, 7);\n+  ev_load_key(xmm18, key, 13 * 16, xmm31);\n+  roundEnc(xmm18, 7);\n+  ev_load_key(xmm18, key, 14 * 16, xmm31);\n+  lastroundEnc(xmm18, 7);\n+\n+  \/\/ After AES encode rounds, the encrypted block cipher lies in zmm0-zmm7\n+  \/\/ xor encrypted block cipher and input plaintext and store resultant ciphertext\n+  __ bind(END_LOOP);\n+  __ evpxorq(xmm0, xmm0, Address(src_addr, pos, Address::times_1, 0 * 64), Assembler::AVX_512bit);\n+  __ evmovdquq(Address(dest_addr, pos, Address::times_1, 0), xmm0, Assembler::AVX_512bit);\n+  __ evpxorq(xmm1, xmm1, Address(src_addr, pos, Address::times_1, 1 * 64), Assembler::AVX_512bit);\n+  __ evmovdquq(Address(dest_addr, pos, Address::times_1, 64), xmm1, Assembler::AVX_512bit);\n+  __ evpxorq(xmm2, xmm2, Address(src_addr, pos, Address::times_1, 2 * 64), Assembler::AVX_512bit);\n+  __ evmovdquq(Address(dest_addr, pos, Address::times_1, 2 * 64), xmm2, Assembler::AVX_512bit);\n+  __ evpxorq(xmm3, xmm3, Address(src_addr, pos, Address::times_1, 3 * 64), Assembler::AVX_512bit);\n+  __ evmovdquq(Address(dest_addr, pos, Address::times_1, 3 * 64), xmm3, Assembler::AVX_512bit);\n+  __ evpxorq(xmm4, xmm4, Address(src_addr, pos, Address::times_1, 4 * 64), Assembler::AVX_512bit);\n+  __ evmovdquq(Address(dest_addr, pos, Address::times_1, 4 * 64), xmm4, Assembler::AVX_512bit);\n+  __ evpxorq(xmm5, xmm5, Address(src_addr, pos, Address::times_1, 5 * 64), Assembler::AVX_512bit);\n+  __ evmovdquq(Address(dest_addr, pos, Address::times_1, 5 * 64), xmm5, Assembler::AVX_512bit);\n+  __ evpxorq(xmm6, xmm6, Address(src_addr, pos, Address::times_1, 6 * 64), Assembler::AVX_512bit);\n+  __ evmovdquq(Address(dest_addr, pos, Address::times_1, 6 * 64), xmm6, Assembler::AVX_512bit);\n+  __ evpxorq(xmm7, xmm7, Address(src_addr, pos, Address::times_1, 7 * 64), Assembler::AVX_512bit);\n+  __ evmovdquq(Address(dest_addr, pos, Address::times_1, 7 * 64), xmm7, Assembler::AVX_512bit);\n+  __ addq(pos, 512);\n+  __ jmp(LOOP);\n+\n+  \/\/ Encode 256, 128, 64 or 16 bytes at a time if length is less than 512 bytes\n+  __ bind(REMAINDER);\n+  __ cmpl(len_reg, 0);\n+  __ jcc(Assembler::equal, END);\n+  __ cmpl(len_reg, 256);\n+  __ jcc(Assembler::aboveEqual, REMAINDER_16);\n+  __ cmpl(len_reg, 128);\n+  __ jcc(Assembler::aboveEqual, REMAINDER_8);\n+  __ cmpl(len_reg, 64);\n+  __ jcc(Assembler::aboveEqual, REMAINDER_4);\n+  \/\/ At this point, we will process 16 bytes of data at a time.\n+  \/\/ So load xmm19 with counter increment value as 1\n+  __ evmovdquq(xmm19, ExternalAddress(StubRoutines::x86::counter_mask_addr() + 80), Assembler::AVX_128bit, r15);\n+  __ jmp(REMAINDER_LOOP);\n+\n+  \/\/ Each ZMM register can be used to encode 64 bytes of data, so we have 4 ZMM registers to encode 256 bytes of data\n+  __ bind(REMAINDER_16);\n+  __ subq(len_reg, 256);\n+  \/\/ As we process 16 blocks at a time, load mask for incrementing the counter value by 16\n+  __ evmovdquq(xmm19, ExternalAddress(StubRoutines::x86::counter_mask_addr() + 320), Assembler::AVX_512bit, r15);\/\/Linc16(rip)\n+  \/\/ shuffle counter and XOR counter with roundkey1\n+  __ vpshufb(xmm0, xmm8, xmm16, Assembler::AVX_512bit);\n+  __ evpxorq(xmm0, xmm0, xmm20, Assembler::AVX_512bit);\n+  __ vpshufb(xmm1, xmm9, xmm16, Assembler::AVX_512bit);\n+  __ evpxorq(xmm1, xmm1, xmm20, Assembler::AVX_512bit);\n+  __ vpshufb(xmm2, xmm10, xmm16, Assembler::AVX_512bit);\n+  __ evpxorq(xmm2, xmm2, xmm20, Assembler::AVX_512bit);\n+  __ vpshufb(xmm3, xmm11, xmm16, Assembler::AVX_512bit);\n+  __ evpxorq(xmm3, xmm3, xmm20, Assembler::AVX_512bit);\n+  \/\/ Increment counter values by 16\n+  __ vpaddq(xmm8, xmm8, xmm19, Assembler::AVX_512bit);\n+  __ vpaddq(xmm9, xmm9, xmm19, Assembler::AVX_512bit);\n+  \/\/ AES encode rounds\n+  roundEnc(xmm21, 3);\n+  roundEnc(xmm22, 3);\n+  roundEnc(xmm23, 3);\n+  roundEnc(xmm24, 3);\n+  roundEnc(xmm25, 3);\n+  roundEnc(xmm26, 3);\n+  roundEnc(xmm27, 3);\n+  roundEnc(xmm28, 3);\n+  roundEnc(xmm29, 3);\n+\n+  __ cmpl(rounds, 52);\n+  __ jcc(Assembler::aboveEqual, AES192_REMAINDER16);\n+  lastroundEnc(xmm30, 3);\n+  __ jmp(REMAINDER16_END_LOOP);\n+\n+  __ bind(AES192_REMAINDER16);\n+  roundEnc(xmm30, 3);\n+  ev_load_key(xmm18, key, 11 * 16, xmm31);\n+  roundEnc(xmm18, 3);\n+  ev_load_key(xmm5, key, 12 * 16, xmm31);\n+\n+  __ cmpl(rounds, 60);\n+  __ jcc(Assembler::aboveEqual, AES256_REMAINDER16);\n+  lastroundEnc(xmm5, 3);\n+  __ jmp(REMAINDER16_END_LOOP);\n+  __ bind(AES256_REMAINDER16);\n+  roundEnc(xmm5, 3);\n+  ev_load_key(xmm6, key, 13 * 16, xmm31);\n+  roundEnc(xmm6, 3);\n+  ev_load_key(xmm7, key, 14 * 16, xmm31);\n+  lastroundEnc(xmm7, 3);\n+\n+  \/\/ After AES encode rounds, the encrypted block cipher lies in zmm0-zmm3\n+  \/\/ xor 256 bytes of PT with the encrypted counters to produce CT.\n+  __ bind(REMAINDER16_END_LOOP);\n+  __ evpxorq(xmm0, xmm0, Address(src_addr, pos, Address::times_1, 0), Assembler::AVX_512bit);\n+  __ evmovdquq(Address(dest_addr, pos, Address::times_1, 0), xmm0, Assembler::AVX_512bit);\n+  __ evpxorq(xmm1, xmm1, Address(src_addr, pos, Address::times_1, 1 * 64), Assembler::AVX_512bit);\n+  __ evmovdquq(Address(dest_addr, pos, Address::times_1, 1 * 64), xmm1, Assembler::AVX_512bit);\n+  __ evpxorq(xmm2, xmm2, Address(src_addr, pos, Address::times_1, 2 * 64), Assembler::AVX_512bit);\n+  __ evmovdquq(Address(dest_addr, pos, Address::times_1, 2 * 64), xmm2, Assembler::AVX_512bit);\n+  __ evpxorq(xmm3, xmm3, Address(src_addr, pos, Address::times_1, 3 * 64), Assembler::AVX_512bit);\n+  __ evmovdquq(Address(dest_addr, pos, Address::times_1, 3 * 64), xmm3, Assembler::AVX_512bit);\n+  __ addq(pos, 256);\n+\n+  __ cmpl(len_reg, 128);\n+  __ jcc(Assembler::aboveEqual, REMAINDER_8);\n+\n+  __ cmpl(len_reg, 64);\n+  __ jcc(Assembler::aboveEqual, REMAINDER_4);\n+  \/\/load mask for incrementing the counter value by 1\n+  __ evmovdquq(xmm19, ExternalAddress(StubRoutines::x86::counter_mask_addr() + 80), Assembler::AVX_128bit, r15);\/\/Linc0 + 16(rip)\n+  __ jmp(REMAINDER_LOOP);\n+\n+  \/\/ Each ZMM register can be used to encode 64 bytes of data, so we have 2 ZMM registers to encode 128 bytes of data\n+  __ bind(REMAINDER_8);\n+  __ subq(len_reg, 128);\n+  \/\/ As we process 8 blocks at a time, load mask for incrementing the counter value by 8\n+  __ evmovdquq(xmm19, ExternalAddress(StubRoutines::x86::counter_mask_addr() + 192), Assembler::AVX_512bit, r15);\/\/Linc8(rip)\n+  \/\/ shuffle counters and xor with roundkey1\n+  __ vpshufb(xmm0, xmm8, xmm16, Assembler::AVX_512bit);\n+  __ evpxorq(xmm0, xmm0, xmm20, Assembler::AVX_512bit);\n+  __ vpshufb(xmm1, xmm9, xmm16, Assembler::AVX_512bit);\n+  __ evpxorq(xmm1, xmm1, xmm20, Assembler::AVX_512bit);\n+  \/\/ increment counter by 8\n+  __ vpaddq(xmm8, xmm8, xmm19, Assembler::AVX_512bit);\n+  \/\/ AES encode\n+  roundEnc(xmm21, 1);\n+  roundEnc(xmm22, 1);\n+  roundEnc(xmm23, 1);\n+  roundEnc(xmm24, 1);\n+  roundEnc(xmm25, 1);\n+  roundEnc(xmm26, 1);\n+  roundEnc(xmm27, 1);\n+  roundEnc(xmm28, 1);\n+  roundEnc(xmm29, 1);\n+\n+  __ cmpl(rounds, 52);\n+  __ jcc(Assembler::aboveEqual, AES192_REMAINDER8);\n+  lastroundEnc(xmm30, 1);\n+  __ jmp(REMAINDER8_END_LOOP);\n+\n+  __ bind(AES192_REMAINDER8);\n+  roundEnc(xmm30, 1);\n+  ev_load_key(xmm18, key, 11 * 16, xmm31);\n+  roundEnc(xmm18, 1);\n+  ev_load_key(xmm5, key, 12 * 16, xmm31);\n+  __ cmpl(rounds, 60);\n+  __ jcc(Assembler::aboveEqual, AES256_REMAINDER8);\n+  lastroundEnc(xmm5, 1);\n+  __ jmp(REMAINDER8_END_LOOP);\n+\n+  __ bind(AES256_REMAINDER8);\n+  roundEnc(xmm5, 1);\n+  ev_load_key(xmm6, key, 13 * 16, xmm31);\n+  roundEnc(xmm6, 1);\n+  ev_load_key(xmm7, key, 14 * 16, xmm31);\n+  lastroundEnc(xmm7, 1);\n+\n+  __ bind(REMAINDER8_END_LOOP);\n+  \/\/ After AES encode rounds, the encrypted block cipher lies in zmm0-zmm1\n+  \/\/ XOR PT with the encrypted counter and store as CT\n+  __ evpxorq(xmm0, xmm0, Address(src_addr, pos, Address::times_1, 0 * 64), Assembler::AVX_512bit);\n+  __ evmovdquq(Address(dest_addr, pos, Address::times_1, 0 * 64), xmm0, Assembler::AVX_512bit);\n+  __ evpxorq(xmm1, xmm1, Address(src_addr, pos, Address::times_1, 1 * 64), Assembler::AVX_512bit);\n+  __ evmovdquq(Address(dest_addr, pos, Address::times_1, 1 * 64), xmm1, Assembler::AVX_512bit);\n+  __ addq(pos, 128);\n+\n+  __ cmpl(len_reg, 64);\n+  __ jcc(Assembler::aboveEqual, REMAINDER_4);\n+  \/\/ load mask for incrementing the counter value by 1\n+  __ evmovdquq(xmm19, ExternalAddress(StubRoutines::x86::counter_mask_addr() + 80), Assembler::AVX_128bit, r15);\/\/Linc0 + 16(rip)\n+  __ jmp(REMAINDER_LOOP);\n+\n+  \/\/ Each ZMM register can be used to encode 64 bytes of data, so we have 1 ZMM register used in this block of code\n+  __ bind(REMAINDER_4);\n+  __ subq(len_reg, 64);\n+  \/\/ As we process 4 blocks at a time, load mask for incrementing the counter value by 4\n+  __ evmovdquq(xmm19, ExternalAddress(StubRoutines::x86::counter_mask_addr() + 128), Assembler::AVX_512bit, r15);\/\/Linc4(rip)\n+  \/\/ XOR counter with first roundkey\n+  __ vpshufb(xmm0, xmm8, xmm16, Assembler::AVX_512bit);\n+  __ evpxorq(xmm0, xmm0, xmm20, Assembler::AVX_512bit);\n+  \/\/ Increment counter\n+  __ vpaddq(xmm8, xmm8, xmm19, Assembler::AVX_512bit);\n+  __ vaesenc(xmm0, xmm0, xmm21, Assembler::AVX_512bit);\n+  __ vaesenc(xmm0, xmm0, xmm22, Assembler::AVX_512bit);\n+  __ vaesenc(xmm0, xmm0, xmm23, Assembler::AVX_512bit);\n+  __ vaesenc(xmm0, xmm0, xmm24, Assembler::AVX_512bit);\n+  __ vaesenc(xmm0, xmm0, xmm25, Assembler::AVX_512bit);\n+  __ vaesenc(xmm0, xmm0, xmm26, Assembler::AVX_512bit);\n+  __ vaesenc(xmm0, xmm0, xmm27, Assembler::AVX_512bit);\n+  __ vaesenc(xmm0, xmm0, xmm28, Assembler::AVX_512bit);\n+  __ vaesenc(xmm0, xmm0, xmm29, Assembler::AVX_512bit);\n+  __ cmpl(rounds, 52);\n+  __ jcc(Assembler::aboveEqual, AES192_REMAINDER4);\n+  __ vaesenclast(xmm0, xmm0, xmm30, Assembler::AVX_512bit);\n+  __ jmp(END_REMAINDER4);\n+\n+  __ bind(AES192_REMAINDER4);\n+  __ vaesenc(xmm0, xmm0, xmm30, Assembler::AVX_512bit);\n+  ev_load_key(xmm18, key, 11 * 16, xmm31);\n+  __ vaesenc(xmm0, xmm0, xmm18, Assembler::AVX_512bit);\n+  ev_load_key(xmm5, key, 12 * 16, xmm31);\n+\n+  __ cmpl(rounds, 60);\n+  __ jcc(Assembler::aboveEqual, AES256_REMAINDER4);\n+  __ vaesenclast(xmm0, xmm0, xmm5, Assembler::AVX_512bit);\n+  __ jmp(END_REMAINDER4);\n+\n+  __ bind(AES256_REMAINDER4);\n+  __ vaesenc(xmm0, xmm0, xmm5, Assembler::AVX_512bit);\n+  ev_load_key(xmm6, key, 13 * 16, xmm31);\n+  __ vaesenc(xmm0, xmm0, xmm6, Assembler::AVX_512bit);\n+  ev_load_key(xmm7, key, 14 * 16, xmm31);\n+  __ vaesenclast(xmm0, xmm0, xmm7, Assembler::AVX_512bit);\n+  \/\/ After AES encode rounds, the encrypted block cipher lies in zmm0.\n+  \/\/ XOR encrypted block cipher with PT and store 64 bytes of ciphertext\n+  __ bind(END_REMAINDER4);\n+  __ evpxorq(xmm0, xmm0, Address(src_addr, pos, Address::times_1, 0 * 64), Assembler::AVX_512bit);\n+  __ evmovdquq(Address(dest_addr, pos, Address::times_1, 0), xmm0, Assembler::AVX_512bit);\n+  __ addq(pos, 64);\n+  \/\/ load mask for incrementing the counter value by 1\n+  __ evmovdquq(xmm19, ExternalAddress(StubRoutines::x86::counter_mask_addr() + 80), Assembler::AVX_128bit, r15);\/\/Linc0 + 16(rip)\n+\n+  \/\/ For a single block, the AES rounds start here.\n+  __ bind(REMAINDER_LOOP);\n+  __ cmpl(len_reg, 0);\n+  __ jcc(Assembler::belowEqual, END);\n+  \/\/ XOR counter with first roundkey\n+  __ vpshufb(xmm0, xmm8, xmm16, Assembler::AVX_128bit);\n+  __ evpxorq(xmm0, xmm0, xmm20, Assembler::AVX_128bit);\n+  __ vaesenc(xmm0, xmm0, xmm21, Assembler::AVX_128bit);\n+  \/\/ Increment counter by 1\n+  __ vpaddq(xmm8, xmm8, xmm19, Assembler::AVX_128bit);\n+  __ vaesenc(xmm0, xmm0, xmm22, Assembler::AVX_128bit);\n+  __ vaesenc(xmm0, xmm0, xmm23, Assembler::AVX_128bit);\n+  __ vaesenc(xmm0, xmm0, xmm24, Assembler::AVX_128bit);\n+  __ vaesenc(xmm0, xmm0, xmm25, Assembler::AVX_128bit);\n+  __ vaesenc(xmm0, xmm0, xmm26, Assembler::AVX_128bit);\n+  __ vaesenc(xmm0, xmm0, xmm27, Assembler::AVX_128bit);\n+  __ vaesenc(xmm0, xmm0, xmm28, Assembler::AVX_128bit);\n+  __ vaesenc(xmm0, xmm0, xmm29, Assembler::AVX_128bit);\n+\n+  __ cmpl(rounds, 52);\n+  __ jcc(Assembler::aboveEqual, AES192_REMAINDER);\n+  __ vaesenclast(xmm0, xmm0, xmm30, Assembler::AVX_128bit);\n+  __ jmp(END_REMAINDER_LOOP);\n+\n+  __ bind(AES192_REMAINDER);\n+  __ vaesenc(xmm0, xmm0, xmm30, Assembler::AVX_128bit);\n+  ev_load_key(xmm18, key, 11 * 16, xmm31);\n+  __ vaesenc(xmm0, xmm0, xmm18, Assembler::AVX_128bit);\n+  ev_load_key(xmm5, key, 12 * 16, xmm31);\n+  __ cmpl(rounds, 60);\n+  __ jcc(Assembler::aboveEqual, AES256_REMAINDER);\n+  __ vaesenclast(xmm0, xmm0, xmm5, Assembler::AVX_128bit);\n+  __ jmp(END_REMAINDER_LOOP);\n+\n+  __ bind(AES256_REMAINDER);\n+  __ vaesenc(xmm0, xmm0, xmm5, Assembler::AVX_128bit);\n+  ev_load_key(xmm6, key, 13 * 16, xmm31);\n+  __ vaesenc(xmm0, xmm0, xmm6, Assembler::AVX_128bit);\n+  ev_load_key(xmm7, key, 14 * 16, xmm31);\n+  __ vaesenclast(xmm0, xmm0, xmm7, Assembler::AVX_128bit);\n+\n+  __ bind(END_REMAINDER_LOOP);\n+  \/\/ If the length register is less than the blockSize i.e. 16\n+  \/\/ then we store only those bytes of the CT to the destination\n+  \/\/ corresponding to the length register value\n+  \/\/ extracting the exact number of bytes is handled by EXTRACT_TAILBYTES\n+  __ cmpl(len_reg, 16);\n+  __ jcc(Assembler::less, EXTRACT_TAILBYTES);\n+  __ subl(len_reg, 16);\n+  \/\/ After AES encode rounds, the encrypted block cipher lies in xmm0.\n+  \/\/ If the length register is equal to 16 bytes, store CT in dest after XOR operation.\n+  __ evpxorq(xmm0, xmm0, Address(src_addr, pos, Address::times_1, 0), Assembler::AVX_128bit);\n+  __ evmovdquq(Address(dest_addr, pos, Address::times_1, 0), xmm0, Assembler::AVX_128bit);\n+  __ addl(pos, 16);\n+\n+  __ jmp(REMAINDER_LOOP);\n+\n+  __ bind(EXTRACT_TAILBYTES);\n+  \/\/ Save encrypted counter value in xmm0 for next invocation, before XOR operation\n+  __ movdqu(Address(saved_encCounter_start, 0), xmm0);\n+  \/\/ XOR encryted block cipher in xmm0 with PT to produce CT\n+  __ evpxorq(xmm0, xmm0, Address(src_addr, pos, Address::times_1, 0), Assembler::AVX_128bit);\n+  \/\/ extract up to 15 bytes of CT from xmm0 as specified by length register\n+  __ testptr(len_reg, 8);\n+  __ jcc(Assembler::zero, EXTRACT_TAIL_4BYTES);\n+  __ pextrq(Address(dest_addr, pos), xmm0, 0);\n+  __ psrldq(xmm0, 8);\n+  __ addl(pos, 8);\n+  __ bind(EXTRACT_TAIL_4BYTES);\n+  __ testptr(len_reg, 4);\n+  __ jcc(Assembler::zero, EXTRACT_TAIL_2BYTES);\n+  __ pextrd(Address(dest_addr, pos), xmm0, 0);\n+  __ psrldq(xmm0, 4);\n+  __ addq(pos, 4);\n+  __ bind(EXTRACT_TAIL_2BYTES);\n+  __ testptr(len_reg, 2);\n+  __ jcc(Assembler::zero, EXTRACT_TAIL_1BYTE);\n+  __ pextrw(Address(dest_addr, pos), xmm0, 0);\n+  __ psrldq(xmm0, 2);\n+  __ addl(pos, 2);\n+  __ bind(EXTRACT_TAIL_1BYTE);\n+  __ testptr(len_reg, 1);\n+  __ jcc(Assembler::zero, END);\n+  __ pextrb(Address(dest_addr, pos), xmm0, 0);\n+  __ addl(pos, 1);\n+\n+  __ bind(END);\n+  \/\/ If there are no tail bytes, store counter value and exit\n+  __ cmpl(len_reg, 0);\n+  __ jcc(Assembler::equal, STORE_CTR);\n+  __ movl(Address(used_addr, 0), len_reg);\n+\n+  __ bind(STORE_CTR);\n+  \/\/shuffle updated counter and store it\n+  __ vpshufb(xmm8, xmm8, xmm16, Assembler::AVX_128bit);\n+  __ movdqu(Address(counter, 0), xmm8);\n+  \/\/ Zero out counter and key registers\n+  __ evpxorq(xmm8, xmm8, xmm8, Assembler::AVX_512bit);\n+  __ evpxorq(xmm20, xmm20, xmm20, Assembler::AVX_512bit);\n+  __ evpxorq(xmm21, xmm21, xmm21, Assembler::AVX_512bit);\n+  __ evpxorq(xmm22, xmm22, xmm22, Assembler::AVX_512bit);\n+  __ evpxorq(xmm23, xmm23, xmm23, Assembler::AVX_512bit);\n+  __ evpxorq(xmm24, xmm24, xmm24, Assembler::AVX_512bit);\n+  __ evpxorq(xmm25, xmm25, xmm25, Assembler::AVX_512bit);\n+  __ evpxorq(xmm26, xmm26, xmm26, Assembler::AVX_512bit);\n+  __ evpxorq(xmm27, xmm27, xmm27, Assembler::AVX_512bit);\n+  __ evpxorq(xmm28, xmm28, xmm28, Assembler::AVX_512bit);\n+  __ evpxorq(xmm29, xmm29, xmm29, Assembler::AVX_512bit);\n+  __ evpxorq(xmm30, xmm30, xmm30, Assembler::AVX_512bit);\n+  __ cmpl(rounds, 44);\n+  __ jcc(Assembler::belowEqual, EXIT);\n+  __ evpxorq(xmm18, xmm18, xmm18, Assembler::AVX_512bit);\n+  __ evpxorq(xmm5, xmm5, xmm5, Assembler::AVX_512bit);\n+  __ cmpl(rounds, 52);\n+  __ jcc(Assembler::belowEqual, EXIT);\n+  __ evpxorq(xmm6, xmm6, xmm6, Assembler::AVX_512bit);\n+  __ evpxorq(xmm7, xmm7, xmm7, Assembler::AVX_512bit);\n+  __ bind(EXIT);\n+}\n+\n+void StubGenerator::gfmul_avx512(XMMRegister GH, XMMRegister HK) {\n+  const XMMRegister TMP1 = xmm0;\n+  const XMMRegister TMP2 = xmm1;\n+  const XMMRegister TMP3 = xmm2;\n+\n+  __ evpclmulqdq(TMP1, GH, HK, 0x11, Assembler::AVX_512bit);\n+  __ evpclmulqdq(TMP2, GH, HK, 0x00, Assembler::AVX_512bit);\n+  __ evpclmulqdq(TMP3, GH, HK, 0x01, Assembler::AVX_512bit);\n+  __ evpclmulqdq(GH, GH, HK, 0x10, Assembler::AVX_512bit);\n+  __ evpxorq(GH, GH, TMP3, Assembler::AVX_512bit);\n+  __ vpsrldq(TMP3, GH, 8, Assembler::AVX_512bit);\n+  __ vpslldq(GH, GH, 8, Assembler::AVX_512bit);\n+  __ evpxorq(TMP1, TMP1, TMP3, Assembler::AVX_512bit);\n+  __ evpxorq(GH, GH, TMP2, Assembler::AVX_512bit);\n+\n+  __ evmovdquq(TMP3, ExternalAddress(StubRoutines::x86::ghash_polynomial512_addr()), Assembler::AVX_512bit, r15);\n+  __ evpclmulqdq(TMP2, TMP3, GH, 0x01, Assembler::AVX_512bit);\n+  __ vpslldq(TMP2, TMP2, 8, Assembler::AVX_512bit);\n+  __ evpxorq(GH, GH, TMP2, Assembler::AVX_512bit);\n+  __ evpclmulqdq(TMP2, TMP3, GH, 0x00, Assembler::AVX_512bit);\n+  __ vpsrldq(TMP2, TMP2, 4, Assembler::AVX_512bit);\n+  __ evpclmulqdq(GH, TMP3, GH, 0x10, Assembler::AVX_512bit);\n+  __ vpslldq(GH, GH, 4, Assembler::AVX_512bit);\n+  __ vpternlogq(GH, 0x96, TMP1, TMP2, Assembler::AVX_512bit);\n+}\n+\n+void StubGenerator::generateHtbl_48_block_zmm(Register htbl, Register avx512_htbl) {\n+  const XMMRegister HK = xmm6;\n+  const XMMRegister ZT5 = xmm4;\n+  const XMMRegister ZT7 = xmm7;\n+  const XMMRegister ZT8 = xmm8;\n+\n+  Label GFMUL_AVX512;\n+\n+  __ movdqu(HK, Address(htbl, 0));\n+  __ movdqu(xmm10, ExternalAddress(StubRoutines::x86::ghash_long_swap_mask_addr()));\n+  __ vpshufb(HK, HK, xmm10, Assembler::AVX_128bit);\n+\n+  __ movdqu(xmm11, ExternalAddress(StubRoutines::x86::ghash_polynomial512_addr() + 64)); \/\/ Poly\n+  __ movdqu(xmm12, ExternalAddress(StubRoutines::x86::ghash_polynomial512_addr() + 80)); \/\/ Twoone\n+  \/\/ Compute H ^ 2 from the input subkeyH\n+  __ movdqu(xmm2, xmm6);\n+  __ vpsllq(xmm6, xmm6, 1, Assembler::AVX_128bit);\n+  __ vpsrlq(xmm2, xmm2, 63, Assembler::AVX_128bit);\n+  __ movdqu(xmm1, xmm2);\n+  __ vpslldq(xmm2, xmm2, 8, Assembler::AVX_128bit);\n+  __ vpsrldq(xmm1, xmm1, 8, Assembler::AVX_128bit);\n+  __ vpor(xmm6, xmm6, xmm2, Assembler::AVX_128bit);\n+\n+  __ vpshufd(xmm2, xmm1, 0x24, Assembler::AVX_128bit);\n+  __ vpcmpeqd(xmm2, xmm2, xmm12, Assembler::AVX_128bit);\n+  __ vpand(xmm2, xmm2, xmm11, Assembler::AVX_128bit);\n+  __ vpxor(xmm6, xmm6, xmm2, Assembler::AVX_128bit);\n+  __ movdqu(Address(avx512_htbl, 16 * 47), xmm6); \/\/ H ^ 2\n+  \/\/ Compute the remaining three powers of H using XMM registers and all following powers using ZMM\n+  __ movdqu(ZT5, HK);\n+  __ vinserti32x4(ZT7, ZT7, HK, 3);\n+\n+  gfmul_avx512(ZT5, HK);\n+  __ movdqu(Address(avx512_htbl, 16 * 46), ZT5); \/\/ H ^ 2 * 2\n+  __ vinserti32x4(ZT7, ZT7, ZT5, 2);\n+\n+  gfmul_avx512(ZT5, HK);\n+  __ movdqu(Address(avx512_htbl, 16 * 45), ZT5); \/\/ H ^ 2 * 3\n+  __ vinserti32x4(ZT7, ZT7, ZT5, 1);\n+\n+  gfmul_avx512(ZT5, HK);\n+  __ movdqu(Address(avx512_htbl, 16 * 44), ZT5); \/\/ H ^ 2 * 4\n+  __ vinserti32x4(ZT7, ZT7, ZT5, 0);\n+\n+  __ evshufi64x2(ZT5, ZT5, ZT5, 0x00, Assembler::AVX_512bit);\n+  __ evmovdquq(ZT8, ZT7, Assembler::AVX_512bit);\n+  gfmul_avx512(ZT7, ZT5);\n+  __ evmovdquq(Address(avx512_htbl, 16 * 40), ZT7, Assembler::AVX_512bit);\n+  __ evshufi64x2(ZT5, ZT7, ZT7, 0x00, Assembler::AVX_512bit);\n+  gfmul_avx512(ZT8, ZT5);\n+  __ evmovdquq(Address(avx512_htbl, 16 * 36), ZT8, Assembler::AVX_512bit);\n+  gfmul_avx512(ZT7, ZT5);\n+  __ evmovdquq(Address(avx512_htbl, 16 * 32), ZT7, Assembler::AVX_512bit);\n+  gfmul_avx512(ZT8, ZT5);\n+  __ evmovdquq(Address(avx512_htbl, 16 * 28), ZT8, Assembler::AVX_512bit);\n+  gfmul_avx512(ZT7, ZT5);\n+  __ evmovdquq(Address(avx512_htbl, 16 * 24), ZT7, Assembler::AVX_512bit);\n+  gfmul_avx512(ZT8, ZT5);\n+  __ evmovdquq(Address(avx512_htbl, 16 * 20), ZT8, Assembler::AVX_512bit);\n+  gfmul_avx512(ZT7, ZT5);\n+  __ evmovdquq(Address(avx512_htbl, 16 * 16), ZT7, Assembler::AVX_512bit);\n+  gfmul_avx512(ZT8, ZT5);\n+  __ evmovdquq(Address(avx512_htbl, 16 * 12), ZT8, Assembler::AVX_512bit);\n+  gfmul_avx512(ZT7, ZT5);\n+  __ evmovdquq(Address(avx512_htbl, 16 * 8), ZT7, Assembler::AVX_512bit);\n+  gfmul_avx512(ZT8, ZT5);\n+  __ evmovdquq(Address(avx512_htbl, 16 * 4), ZT8, Assembler::AVX_512bit);\n+  gfmul_avx512(ZT7, ZT5);\n+  __ evmovdquq(Address(avx512_htbl, 16 * 0), ZT7, Assembler::AVX_512bit);\n+  __ ret(0);\n+}\n+\n+#define vclmul_reduce(out, poly, hi128, lo128, tmp0, tmp1)      \\\n+__ evpclmulqdq(tmp0, poly, lo128, 0x01, Assembler::AVX_512bit); \\\n+__ vpslldq(tmp0, tmp0, 8, Assembler::AVX_512bit);               \\\n+__ evpxorq(tmp0, lo128, tmp0, Assembler::AVX_512bit);           \\\n+__ evpclmulqdq(tmp1, poly, tmp0, 0x00, Assembler::AVX_512bit);  \\\n+__ vpsrldq(tmp1, tmp1, 4, Assembler::AVX_512bit);               \\\n+__ evpclmulqdq(out, poly, tmp0, 0x10, Assembler::AVX_512bit);   \\\n+__ vpslldq(out, out, 4, Assembler::AVX_512bit);                 \\\n+__ vpternlogq(out, 0x96, tmp1, hi128, Assembler::AVX_512bit);   \\\n+\n+#define vhpxori4x128(reg, tmp)                    \\\n+__ vextracti64x4(tmp, reg, 1);                    \\\n+__ evpxorq(reg, reg, tmp, Assembler::AVX_256bit); \\\n+__ vextracti32x4(tmp, reg, 1);                    \\\n+__ evpxorq(reg, reg, tmp, Assembler::AVX_128bit); \\\n+\n+#define roundEncode(key, dst1, dst2, dst3, dst4)    \\\n+__ vaesenc(dst1, dst1, key, Assembler::AVX_512bit); \\\n+__ vaesenc(dst2, dst2, key, Assembler::AVX_512bit); \\\n+__ vaesenc(dst3, dst3, key, Assembler::AVX_512bit); \\\n+__ vaesenc(dst4, dst4, key, Assembler::AVX_512bit); \\\n+\n+#define lastroundEncode(key, dst1, dst2, dst3, dst4) \\\n+__ vaesenclast(dst1, dst1, key, Assembler::AVX_512bit); \\\n+__ vaesenclast(dst2, dst2, key, Assembler::AVX_512bit); \\\n+__ vaesenclast(dst3, dst3, key, Assembler::AVX_512bit); \\\n+__ vaesenclast(dst4, dst4, key, Assembler::AVX_512bit); \\\n+\n+#define storeData(dst, position, src1, src2, src3, src4) \\\n+__ evmovdquq(Address(dst, position, Address::times_1, 0 * 64), src1, Assembler::AVX_512bit); \\\n+__ evmovdquq(Address(dst, position, Address::times_1, 1 * 64), src2, Assembler::AVX_512bit); \\\n+__ evmovdquq(Address(dst, position, Address::times_1, 2 * 64), src3, Assembler::AVX_512bit); \\\n+__ evmovdquq(Address(dst, position, Address::times_1, 3 * 64), src4, Assembler::AVX_512bit); \\\n+\n+#define loadData(src, position, dst1, dst2, dst3, dst4) \\\n+__ evmovdquq(dst1, Address(src, position, Address::times_1, 0 * 64), Assembler::AVX_512bit); \\\n+__ evmovdquq(dst2, Address(src, position, Address::times_1, 1 * 64), Assembler::AVX_512bit); \\\n+__ evmovdquq(dst3, Address(src, position, Address::times_1, 2 * 64), Assembler::AVX_512bit); \\\n+__ evmovdquq(dst4, Address(src, position, Address::times_1, 3 * 64), Assembler::AVX_512bit); \\\n+\n+#define carrylessMultiply(dst00, dst01, dst10, dst11, ghdata, hkey) \\\n+__ evpclmulqdq(dst00, ghdata, hkey, 0x00, Assembler::AVX_512bit); \\\n+__ evpclmulqdq(dst01, ghdata, hkey, 0x01, Assembler::AVX_512bit); \\\n+__ evpclmulqdq(dst10, ghdata, hkey, 0x10, Assembler::AVX_512bit); \\\n+__ evpclmulqdq(dst11, ghdata, hkey, 0x11, Assembler::AVX_512bit); \\\n+\n+#define shuffleExorRnd1Key(dst0, dst1, dst2, dst3, shufmask, rndkey) \\\n+__ vpshufb(dst0, dst0, shufmask, Assembler::AVX_512bit); \\\n+__ evpxorq(dst0, dst0, rndkey, Assembler::AVX_512bit); \\\n+__ vpshufb(dst1, dst1, shufmask, Assembler::AVX_512bit); \\\n+__ evpxorq(dst1, dst1, rndkey, Assembler::AVX_512bit); \\\n+__ vpshufb(dst2, dst2, shufmask, Assembler::AVX_512bit); \\\n+__ evpxorq(dst2, dst2, rndkey, Assembler::AVX_512bit); \\\n+__ vpshufb(dst3, dst3, shufmask, Assembler::AVX_512bit); \\\n+__ evpxorq(dst3, dst3, rndkey, Assembler::AVX_512bit); \\\n+\n+#define xorBeforeStore(dst0, dst1, dst2, dst3, src0, src1, src2, src3) \\\n+__ evpxorq(dst0, dst0, src0, Assembler::AVX_512bit); \\\n+__ evpxorq(dst1, dst1, src1, Assembler::AVX_512bit); \\\n+__ evpxorq(dst2, dst2, src2, Assembler::AVX_512bit); \\\n+__ evpxorq(dst3, dst3, src3, Assembler::AVX_512bit); \\\n+\n+#define xorGHASH(dst0, dst1, dst2, dst3, src02, src03, src12, src13, src22, src23, src32, src33) \\\n+__ vpternlogq(dst0, 0x96, src02, src03, Assembler::AVX_512bit); \\\n+__ vpternlogq(dst1, 0x96, src12, src13, Assembler::AVX_512bit); \\\n+__ vpternlogq(dst2, 0x96, src22, src23, Assembler::AVX_512bit); \\\n+__ vpternlogq(dst3, 0x96, src32, src33, Assembler::AVX_512bit); \\\n+\n+void StubGenerator::ghash16_encrypt16_parallel(Register key, Register subkeyHtbl, XMMRegister ctr_blockx, XMMRegister aad_hashx,\n+                                               Register in, Register out, Register data, Register pos, bool first_time_reduction, XMMRegister addmask, bool ghash_input, Register rounds,\n+                                               Register ghash_pos, bool final_reduction, int i, XMMRegister counter_inc_mask) {\n+  Label AES_192, AES_256, LAST_AES_RND;\n+  const XMMRegister ZTMP0 = xmm0;\n+  const XMMRegister ZTMP1 = xmm3;\n+  const XMMRegister ZTMP2 = xmm4;\n+  const XMMRegister ZTMP3 = xmm5;\n+  const XMMRegister ZTMP5 = xmm7;\n+  const XMMRegister ZTMP6 = xmm10;\n+  const XMMRegister ZTMP7 = xmm11;\n+  const XMMRegister ZTMP8 = xmm12;\n+  const XMMRegister ZTMP9 = xmm13;\n+  const XMMRegister ZTMP10 = xmm15;\n+  const XMMRegister ZTMP11 = xmm16;\n+  const XMMRegister ZTMP12 = xmm17;\n+\n+  const XMMRegister ZTMP13 = xmm19;\n+  const XMMRegister ZTMP14 = xmm20;\n+  const XMMRegister ZTMP15 = xmm21;\n+  const XMMRegister ZTMP16 = xmm30;\n+  const XMMRegister ZTMP17 = xmm31;\n+  const XMMRegister ZTMP18 = xmm1;\n+  const XMMRegister ZTMP19 = xmm2;\n+  const XMMRegister ZTMP20 = xmm8;\n+  const XMMRegister ZTMP21 = xmm22;\n+  const XMMRegister ZTMP22 = xmm23;\n+\n+  \/\/ Pre increment counters\n+  __ vpaddd(ZTMP0, ctr_blockx, counter_inc_mask, Assembler::AVX_512bit);\n+  __ vpaddd(ZTMP1, ZTMP0, counter_inc_mask, Assembler::AVX_512bit);\n+  __ vpaddd(ZTMP2, ZTMP1, counter_inc_mask, Assembler::AVX_512bit);\n+  __ vpaddd(ZTMP3, ZTMP2, counter_inc_mask, Assembler::AVX_512bit);\n+  \/\/ Save counter value\n+  __ evmovdquq(ctr_blockx, ZTMP3, Assembler::AVX_512bit);\n+\n+  \/\/ Reuse ZTMP17 \/ ZTMP18 for loading AES Keys\n+  \/\/ Pre-load AES round keys\n+  ev_load_key(ZTMP17, key, 0, xmm29);\n+  ev_load_key(ZTMP18, key, 1 * 16, xmm29);\n+\n+  \/\/ ZTMP19 & ZTMP20 used for loading hash key\n+  \/\/ Pre-load hash key\n+  __ evmovdquq(ZTMP19, Address(subkeyHtbl, i * 64), Assembler::AVX_512bit);\n+  __ evmovdquq(ZTMP20, Address(subkeyHtbl, ++i * 64), Assembler::AVX_512bit);\n+  \/\/ Load data for computing ghash\n+  __ evmovdquq(ZTMP21, Address(data, ghash_pos, Address::times_1, 0 * 64), Assembler::AVX_512bit);\n+  __ vpshufb(ZTMP21, ZTMP21, xmm24, Assembler::AVX_512bit);\n+\n+  \/\/ Xor cipher block 0 with input ghash, if available\n+  if (ghash_input) {\n+    __ evpxorq(ZTMP21, ZTMP21, aad_hashx, Assembler::AVX_512bit);\n+  }\n+  \/\/ Load data for computing ghash\n+  __ evmovdquq(ZTMP22, Address(data, ghash_pos, Address::times_1, 1 * 64), Assembler::AVX_512bit);\n+  __ vpshufb(ZTMP22, ZTMP22, xmm24, Assembler::AVX_512bit);\n+\n+  \/\/ stitch AES rounds with GHASH\n+  \/\/ AES round 0, xmm24 has shuffle mask\n+  shuffleExorRnd1Key(ZTMP0, ZTMP1, ZTMP2, ZTMP3, xmm24, ZTMP17);\n+  \/\/ Reuse ZTMP17 \/ ZTMP18 for loading remaining AES Keys\n+  ev_load_key(ZTMP17, key, 2 * 16, xmm29);\n+  \/\/ GHASH 4 blocks\n+  carrylessMultiply(ZTMP6, ZTMP7, ZTMP8, ZTMP5, ZTMP21, ZTMP19);\n+  \/\/ Load the next hkey and Ghash data\n+  __ evmovdquq(ZTMP19, Address(subkeyHtbl, ++i * 64), Assembler::AVX_512bit);\n+  __ evmovdquq(ZTMP21, Address(data, ghash_pos, Address::times_1, 2 * 64), Assembler::AVX_512bit);\n+  __ vpshufb(ZTMP21, ZTMP21, xmm24, Assembler::AVX_512bit);\n+\n+  \/\/ AES round 1\n+  roundEncode(ZTMP18, ZTMP0, ZTMP1, ZTMP2, ZTMP3);\n+  ev_load_key(ZTMP18, key, 3 * 16, xmm29);\n+\n+  \/\/ GHASH 4 blocks(11 to 8)\n+  carrylessMultiply(ZTMP10, ZTMP12, ZTMP11, ZTMP9, ZTMP22, ZTMP20);\n+  \/\/ Load the next hkey and GDATA\n+  __ evmovdquq(ZTMP20, Address(subkeyHtbl, ++i * 64), Assembler::AVX_512bit);\n+  __ evmovdquq(ZTMP22, Address(data, ghash_pos, Address::times_1, 3 * 64), Assembler::AVX_512bit);\n+  __ vpshufb(ZTMP22, ZTMP22, xmm24, Assembler::AVX_512bit);\n+\n+  \/\/ AES round 2\n+  roundEncode(ZTMP17, ZTMP0, ZTMP1, ZTMP2, ZTMP3);\n+  ev_load_key(ZTMP17, key, 4 * 16, xmm29);\n+\n+  \/\/ GHASH 4 blocks(7 to 4)\n+  carrylessMultiply(ZTMP14, ZTMP16, ZTMP15, ZTMP13, ZTMP21, ZTMP19);\n+  \/\/ AES rounds 3\n+  roundEncode(ZTMP18, ZTMP0, ZTMP1, ZTMP2, ZTMP3);\n+  ev_load_key(ZTMP18, key, 5 * 16, xmm29);\n+\n+  \/\/ Gather(XOR) GHASH for 12 blocks\n+  xorGHASH(ZTMP5, ZTMP6, ZTMP8, ZTMP7, ZTMP9, ZTMP13, ZTMP10, ZTMP14, ZTMP12, ZTMP16, ZTMP11, ZTMP15);\n+\n+  \/\/ AES rounds 4\n+  roundEncode(ZTMP17, ZTMP0, ZTMP1, ZTMP2, ZTMP3);\n+  ev_load_key(ZTMP17, key, 6 * 16, xmm29);\n+\n+  \/\/ load plain \/ cipher text(recycle registers)\n+  loadData(in, pos, ZTMP13, ZTMP14, ZTMP15, ZTMP16);\n+\n+  \/\/ AES rounds 5\n+  roundEncode(ZTMP18, ZTMP0, ZTMP1, ZTMP2, ZTMP3);\n+  ev_load_key(ZTMP18, key, 7 * 16, xmm29);\n+  \/\/ GHASH 4 blocks(3 to 0)\n+  carrylessMultiply(ZTMP10, ZTMP12, ZTMP11, ZTMP9, ZTMP22, ZTMP20);\n+\n+  \/\/  AES round 6\n+  roundEncode(ZTMP17, ZTMP0, ZTMP1, ZTMP2, ZTMP3);\n+  ev_load_key(ZTMP17, key, 8 * 16, xmm29);\n+\n+  \/\/ gather GHASH in ZTMP6(low) and ZTMP5(high)\n+  if (first_time_reduction) {\n+    __ vpternlogq(ZTMP7, 0x96, ZTMP8, ZTMP12, Assembler::AVX_512bit);\n+    __ evpxorq(xmm25, ZTMP7, ZTMP11, Assembler::AVX_512bit);\n+    __ evpxorq(xmm27, ZTMP5, ZTMP9, Assembler::AVX_512bit);\n+    __ evpxorq(xmm26, ZTMP6, ZTMP10, Assembler::AVX_512bit);\n+  } else if (!first_time_reduction && !final_reduction) {\n+    xorGHASH(ZTMP7, xmm25, xmm27, xmm26, ZTMP8, ZTMP12, ZTMP7, ZTMP11, ZTMP5, ZTMP9, ZTMP6, ZTMP10);\n+  }\n+\n+  if (final_reduction) {\n+    \/\/ Phase one: Add mid products together\n+    \/\/ Also load polynomial constant for reduction\n+    __ vpternlogq(ZTMP7, 0x96, ZTMP8, ZTMP12, Assembler::AVX_512bit);\n+    __ vpternlogq(ZTMP7, 0x96, xmm25, ZTMP11, Assembler::AVX_512bit);\n+    __ vpsrldq(ZTMP11, ZTMP7, 8, Assembler::AVX_512bit);\n+    __ vpslldq(ZTMP7, ZTMP7, 8, Assembler::AVX_512bit);\n+    __ evmovdquq(ZTMP12, ExternalAddress(StubRoutines::x86::ghash_polynomial512_addr()), Assembler::AVX_512bit, rbx);\n+  }\n+  \/\/ AES round 7\n+  roundEncode(ZTMP18, ZTMP0, ZTMP1, ZTMP2, ZTMP3);\n+  ev_load_key(ZTMP18, key, 9 * 16, xmm29);\n+  if (final_reduction) {\n+    __ vpternlogq(ZTMP5, 0x96, ZTMP9, ZTMP11, Assembler::AVX_512bit);\n+    __ evpxorq(ZTMP5, ZTMP5, xmm27, Assembler::AVX_512bit);\n+    __ vpternlogq(ZTMP6, 0x96, ZTMP10, ZTMP7, Assembler::AVX_512bit);\n+    __ evpxorq(ZTMP6, ZTMP6, xmm26, Assembler::AVX_512bit);\n+  }\n+  \/\/ AES round 8\n+  roundEncode(ZTMP17, ZTMP0, ZTMP1, ZTMP2, ZTMP3);\n+  ev_load_key(ZTMP17, key, 10 * 16, xmm29);\n+\n+  \/\/ Horizontal xor of low and high 4*128\n+  if (final_reduction) {\n+    vhpxori4x128(ZTMP5, ZTMP9);\n+    vhpxori4x128(ZTMP6, ZTMP10);\n+  }\n+  \/\/ AES round 9\n+  roundEncode(ZTMP18, ZTMP0, ZTMP1, ZTMP2, ZTMP3);\n+  \/\/ First phase of reduction\n+  if (final_reduction) {\n+      __ evpclmulqdq(ZTMP10, ZTMP12, ZTMP6, 0x01, Assembler::AVX_128bit);\n+      __ vpslldq(ZTMP10, ZTMP10, 8, Assembler::AVX_128bit);\n+      __ evpxorq(ZTMP10, ZTMP6, ZTMP10, Assembler::AVX_128bit);\n+  }\n+  __ cmpl(rounds, 52);\n+  __ jcc(Assembler::greaterEqual, AES_192);\n+  __ jmp(LAST_AES_RND);\n+  \/\/ AES rounds up to 11 (AES192) or 13 (AES256)\n+  __ bind(AES_192);\n+  roundEncode(ZTMP17, ZTMP0, ZTMP1, ZTMP2, ZTMP3);\n+  ev_load_key(ZTMP18, key, 11 * 16, xmm29);\n+  roundEncode(ZTMP18, ZTMP0, ZTMP1, ZTMP2, ZTMP3);\n+  ev_load_key(ZTMP17, key, 12 * 16, xmm29);\n+  __ cmpl(rounds, 60);\n+  __ jcc(Assembler::aboveEqual, AES_256);\n+  __ jmp(LAST_AES_RND);\n+\n+  __ bind(AES_256);\n+  roundEncode(ZTMP17, ZTMP0, ZTMP1, ZTMP2, ZTMP3);\n+  ev_load_key(ZTMP18, key, 13 * 16, xmm29);\n+  roundEncode(ZTMP18, ZTMP0, ZTMP1, ZTMP2, ZTMP3);\n+  ev_load_key(ZTMP17, key, 14 * 16, xmm29);\n+\n+  __ bind(LAST_AES_RND);\n+  \/\/ Second phase of reduction\n+  if (final_reduction) {\n+    __ evpclmulqdq(ZTMP9, ZTMP12, ZTMP10, 0x00, Assembler::AVX_128bit);\n+    __ vpsrldq(ZTMP9, ZTMP9, 4, Assembler::AVX_128bit); \/\/ Shift-R 1-DW to obtain 2-DWs shift-R\n+    __ evpclmulqdq(ZTMP11, ZTMP12, ZTMP10, 0x10, Assembler::AVX_128bit);\n+    __ vpslldq(ZTMP11, ZTMP11, 4, Assembler::AVX_128bit); \/\/ Shift-L 1-DW for result\n+    \/\/ ZTMP5 = ZTMP5 X ZTMP11 X ZTMP9\n+    __ vpternlogq(ZTMP5, 0x96, ZTMP11, ZTMP9, Assembler::AVX_128bit);\n+  }\n+  \/\/ Last AES round\n+  lastroundEncode(ZTMP17, ZTMP0, ZTMP1, ZTMP2, ZTMP3);\n+  \/\/ XOR against plain \/ cipher text\n+  xorBeforeStore(ZTMP0, ZTMP1, ZTMP2, ZTMP3, ZTMP13, ZTMP14, ZTMP15, ZTMP16);\n+  \/\/ store cipher \/ plain text\n+  storeData(out, pos, ZTMP0, ZTMP1, ZTMP2, ZTMP3);\n+}\n+\n+void StubGenerator::aesgcm_encrypt(Register in, Register len, Register ct, Register out, Register key,\n+                                   Register state, Register subkeyHtbl, Register avx512_subkeyHtbl, Register counter) {\n+  Label ENC_DEC_DONE, GENERATE_HTBL_48_BLKS, AES_192, AES_256, STORE_CT, GHASH_LAST_32,\n+        AES_32_BLOCKS, GHASH_AES_PARALLEL, LOOP, ACCUMULATE, GHASH_16_AES_16;\n+  const XMMRegister CTR_BLOCKx = xmm9;\n+  const XMMRegister AAD_HASHx = xmm14;\n+  const Register pos = rax;\n+  const Register rounds = r15;\n+  const Register ghash_pos = NOT_WIN64( r14) WIN64_ONLY( r11 );\n+  const XMMRegister ZTMP0 = xmm0;\n+  const XMMRegister ZTMP1 = xmm3;\n+  const XMMRegister ZTMP2 = xmm4;\n+  const XMMRegister ZTMP3 = xmm5;\n+  const XMMRegister ZTMP4 = xmm6;\n+  const XMMRegister ZTMP5 = xmm7;\n+  const XMMRegister ZTMP6 = xmm10;\n+  const XMMRegister ZTMP7 = xmm11;\n+  const XMMRegister ZTMP8 = xmm12;\n+  const XMMRegister ZTMP9 = xmm13;\n+  const XMMRegister ZTMP10 = xmm15;\n+  const XMMRegister ZTMP11 = xmm16;\n+  const XMMRegister ZTMP12 = xmm17;\n+  const XMMRegister ZTMP13 = xmm19;\n+  const XMMRegister ZTMP14 = xmm20;\n+  const XMMRegister ZTMP15 = xmm21;\n+  const XMMRegister ZTMP16 = xmm30;\n+  const XMMRegister COUNTER_INC_MASK = xmm18;\n+\n+  __ movl(pos, 0); \/\/ Total length processed\n+  \/\/ Min data size processed = 768 bytes\n+  __ cmpl(len, 768);\n+  __ jcc(Assembler::less, ENC_DEC_DONE);\n+\n+  \/\/ Generate 48 constants for htbl\n+  __ call(GENERATE_HTBL_48_BLKS, relocInfo::none);\n+  int index = 0; \/\/ Index for choosing subkeyHtbl entry\n+  __ movl(ghash_pos, 0); \/\/ Pointer for ghash read and store operations\n+\n+  \/\/ Move initial counter value and STATE value into variables\n+  __ movdqu(CTR_BLOCKx, Address(counter, 0));\n+  __ movdqu(AAD_HASHx, Address(state, 0));\n+  \/\/ Load lswap mask for ghash\n+  __ movdqu(xmm24, ExternalAddress(StubRoutines::x86::ghash_long_swap_mask_addr()), rbx);\n+  \/\/ Shuffle input state using lswap mask\n+  __ vpshufb(AAD_HASHx, AAD_HASHx, xmm24, Assembler::AVX_128bit);\n+\n+  \/\/ Compute #rounds for AES based on the length of the key array\n+  __ movl(rounds, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));\n+\n+  \/\/ Broadcast counter value to 512 bit register\n+  __ evshufi64x2(CTR_BLOCKx, CTR_BLOCKx, CTR_BLOCKx, 0, Assembler::AVX_512bit);\n+  \/\/ Load counter shuffle mask\n+  __ evmovdquq(xmm24, ExternalAddress(StubRoutines::x86::counter_mask_addr()), Assembler::AVX_512bit, rbx);\n+  \/\/ Shuffle counter\n+  __ vpshufb(CTR_BLOCKx, CTR_BLOCKx, xmm24, Assembler::AVX_512bit);\n+\n+  \/\/ Load mask for incrementing counter\n+  __ evmovdquq(COUNTER_INC_MASK, ExternalAddress(StubRoutines::x86::counter_mask_addr() + 128), Assembler::AVX_512bit, rbx);\n+  \/\/ Pre-increment counter\n+  __ vpaddd(ZTMP5, CTR_BLOCKx, ExternalAddress(StubRoutines::x86::counter_mask_addr() + 64), Assembler::AVX_512bit, rbx);\n+  __ vpaddd(ZTMP6, ZTMP5, COUNTER_INC_MASK, Assembler::AVX_512bit);\n+  __ vpaddd(ZTMP7, ZTMP6, COUNTER_INC_MASK, Assembler::AVX_512bit);\n+  __ vpaddd(ZTMP8, ZTMP7, COUNTER_INC_MASK, Assembler::AVX_512bit);\n+\n+  \/\/ Begin 32 blocks of AES processing\n+  __ bind(AES_32_BLOCKS);\n+  \/\/ Save incremented counter before overwriting it with AES data\n+  __ evmovdquq(CTR_BLOCKx, ZTMP8, Assembler::AVX_512bit);\n+\n+  \/\/ Move 256 bytes of data\n+  loadData(in, pos, ZTMP0, ZTMP1, ZTMP2, ZTMP3);\n+  \/\/ Load key shuffle mask\n+  __ movdqu(xmm29, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()), rbx);\n+  \/\/ Load 0th AES round key\n+  ev_load_key(ZTMP4, key, 0, xmm29);\n+  \/\/ AES-ROUND0, xmm24 has the shuffle mask\n+  shuffleExorRnd1Key(ZTMP5, ZTMP6, ZTMP7, ZTMP8, xmm24, ZTMP4);\n+\n+  for (int j = 1; j < 10; j++) {\n+      ev_load_key(ZTMP4, key, j * 16, xmm29);\n+      roundEncode(ZTMP4, ZTMP5, ZTMP6, ZTMP7, ZTMP8);\n+  }\n+  ev_load_key(ZTMP4, key, 10 * 16, xmm29);\n+  \/\/ AES rounds up to 11 (AES192) or 13 (AES256)\n+  __ cmpl(rounds, 52);\n+  __ jcc(Assembler::greaterEqual, AES_192);\n+  lastroundEncode(ZTMP4, ZTMP5, ZTMP6, ZTMP7, ZTMP8);\n+  __ jmp(STORE_CT);\n+\n+  __ bind(AES_192);\n+  roundEncode(ZTMP4, ZTMP5, ZTMP6, ZTMP7, ZTMP8);\n+  ev_load_key(ZTMP4, key, 11 * 16, xmm29);\n+  roundEncode(ZTMP4, ZTMP5, ZTMP6, ZTMP7, ZTMP8);\n+  __ cmpl(rounds, 60);\n+  __ jcc(Assembler::aboveEqual, AES_256);\n+  ev_load_key(ZTMP4, key, 12 * 16, xmm29);\n+  lastroundEncode(ZTMP4, ZTMP5, ZTMP6, ZTMP7, ZTMP8);\n+  __ jmp(STORE_CT);\n+\n+  __ bind(AES_256);\n+  ev_load_key(ZTMP4, key, 12 * 16, xmm29);\n+  roundEncode(ZTMP4, ZTMP5, ZTMP6, ZTMP7, ZTMP8);\n+  ev_load_key(ZTMP4, key, 13 * 16, xmm29);\n+  roundEncode(ZTMP4, ZTMP5, ZTMP6, ZTMP7, ZTMP8);\n+  ev_load_key(ZTMP4, key, 14 * 16, xmm29);\n+  \/\/ Last AES round\n+  lastroundEncode(ZTMP4, ZTMP5, ZTMP6, ZTMP7, ZTMP8);\n+\n+  __ bind(STORE_CT);\n+  \/\/ Xor the encrypted key with PT to obtain CT\n+  xorBeforeStore(ZTMP5, ZTMP6, ZTMP7, ZTMP8, ZTMP0, ZTMP1, ZTMP2, ZTMP3);\n+  storeData(out, pos, ZTMP5, ZTMP6, ZTMP7, ZTMP8);\n+  \/\/ 16 blocks encryption completed\n+  __ addl(pos, 256);\n+  __ cmpl(pos, 512);\n+  __ jcc(Assembler::aboveEqual, GHASH_AES_PARALLEL);\n+  __ vpaddd(ZTMP5, CTR_BLOCKx, COUNTER_INC_MASK, Assembler::AVX_512bit);\n+  __ vpaddd(ZTMP6, ZTMP5, COUNTER_INC_MASK, Assembler::AVX_512bit);\n+  __ vpaddd(ZTMP7, ZTMP6, COUNTER_INC_MASK, Assembler::AVX_512bit);\n+  __ vpaddd(ZTMP8, ZTMP7, COUNTER_INC_MASK, Assembler::AVX_512bit);\n+  __ jmp(AES_32_BLOCKS);\n+\n+  __ bind(GHASH_AES_PARALLEL);\n+  \/\/ Ghash16_encrypt16_parallel takes place in the order with three reduction values:\n+  \/\/ 1) First time -> cipher xor input ghash\n+  \/\/ 2) No reduction -> accumulate multiplication values\n+  \/\/ 3) Final reduction post 48 blocks -> new ghash value is computed for the next round\n+  \/\/ Reduction value = first time\n+  ghash16_encrypt16_parallel(key, avx512_subkeyHtbl, CTR_BLOCKx, AAD_HASHx, in, out, ct, pos, true, xmm24, true, rounds, ghash_pos, false, index, COUNTER_INC_MASK);\n+  __ addl(pos, 256);\n+  __ addl(ghash_pos, 256);\n+  index += 4;\n+\n+  \/\/ At this point we have processed 768 bytes of AES and 256 bytes of GHASH.\n+  \/\/ If the remaining length is less than 768, process remaining 512 bytes of ghash in GHASH_LAST_32 code\n+  __ subl(len, 768);\n+  __ cmpl(len, 768);\n+  __ jcc(Assembler::less, GHASH_LAST_32);\n+\n+  \/\/ AES 16 blocks and GHASH 16 blocks in parallel\n+  \/\/ For multiples of 48 blocks we will do ghash16_encrypt16 interleaved multiple times\n+  \/\/ Reduction value = no reduction means that the carryless multiplication values are accumulated for further calculations\n+  \/\/ Each call uses 4 subkeyHtbl values, so increment the index by 4.\n+  __ bind(GHASH_16_AES_16);\n+  \/\/ Reduction value = no reduction\n+  ghash16_encrypt16_parallel(key, avx512_subkeyHtbl, CTR_BLOCKx, AAD_HASHx, in, out, ct, pos, false, xmm24, false, rounds, ghash_pos, false, index, COUNTER_INC_MASK);\n+  __ addl(pos, 256);\n+  __ addl(ghash_pos, 256);\n+  index += 4;\n+  \/\/ Reduction value = final reduction means that the accumulated values have to be reduced as we have completed 48 blocks of ghash\n+  ghash16_encrypt16_parallel(key, avx512_subkeyHtbl, CTR_BLOCKx, AAD_HASHx, in, out, ct, pos, false, xmm24, false, rounds, ghash_pos, true, index, COUNTER_INC_MASK);\n+  __ addl(pos, 256);\n+  __ addl(ghash_pos, 256);\n+  \/\/ Calculated ghash value needs to be __ moved to AAD_HASHX so that we can restart the ghash16-aes16 pipeline\n+  __ movdqu(AAD_HASHx, ZTMP5);\n+  index = 0; \/\/ Reset subkeyHtbl index\n+\n+  \/\/ Restart the pipeline\n+  \/\/ Reduction value = first time\n+  ghash16_encrypt16_parallel(key, avx512_subkeyHtbl, CTR_BLOCKx, AAD_HASHx, in, out, ct, pos, true, xmm24, true, rounds, ghash_pos, false, index, COUNTER_INC_MASK);\n+  __ addl(pos, 256);\n+  __ addl(ghash_pos, 256);\n+  index += 4;\n+\n+  __ subl(len, 768);\n+  __ cmpl(len, 768);\n+  __ jcc(Assembler::greaterEqual, GHASH_16_AES_16);\n+\n+  \/\/ GHASH last 32 blocks processed here\n+  \/\/ GHASH products accumulated in ZMM27, ZMM25 and ZMM26 during GHASH16-AES16 operation is used\n+  __ bind(GHASH_LAST_32);\n+  \/\/ Use rbx as a pointer to the htbl; For last 32 blocks of GHASH, use key# 4-11 entry in subkeyHtbl\n+  __ movl(rbx, 256);\n+  \/\/ Load cipher blocks\n+  __ evmovdquq(ZTMP13, Address(ct, ghash_pos, Address::times_1, 0 * 64), Assembler::AVX_512bit);\n+  __ evmovdquq(ZTMP14, Address(ct, ghash_pos, Address::times_1, 1 * 64), Assembler::AVX_512bit);\n+  __ vpshufb(ZTMP13, ZTMP13, xmm24, Assembler::AVX_512bit);\n+  __ vpshufb(ZTMP14, ZTMP14, xmm24, Assembler::AVX_512bit);\n+  \/\/ Load ghash keys\n+  __ evmovdquq(ZTMP15, Address(avx512_subkeyHtbl, rbx, Address::times_1, 0 * 64), Assembler::AVX_512bit);\n+  __ evmovdquq(ZTMP16, Address(avx512_subkeyHtbl, rbx, Address::times_1, 1 * 64), Assembler::AVX_512bit);\n+\n+  \/\/ Ghash blocks 0 - 3\n+  carrylessMultiply(ZTMP2, ZTMP3, ZTMP4, ZTMP1, ZTMP13, ZTMP15);\n+  \/\/ Ghash blocks 4 - 7\n+  carrylessMultiply(ZTMP6, ZTMP7, ZTMP8, ZTMP5, ZTMP14, ZTMP16);\n+\n+  __ vpternlogq(ZTMP1, 0x96, ZTMP5, xmm27, Assembler::AVX_512bit); \/\/ ZTMP1 = ZTMP1 + ZTMP5 + zmm27\n+  __ vpternlogq(ZTMP2, 0x96, ZTMP6, xmm26, Assembler::AVX_512bit); \/\/ ZTMP2 = ZTMP2 + ZTMP6 + zmm26\n+  __ vpternlogq(ZTMP3, 0x96, ZTMP7, xmm25, Assembler::AVX_512bit); \/\/ ZTMP3 = ZTMP3 + ZTMP7 + zmm25\n+  __ evpxorq(ZTMP4, ZTMP4, ZTMP8, Assembler::AVX_512bit);          \/\/ ZTMP4 = ZTMP4 + ZTMP8\n+\n+  __ addl(ghash_pos, 128);\n+  __ addl(rbx, 128);\n+\n+  \/\/ Ghash remaining blocks\n+  __ bind(LOOP);\n+  __ cmpl(ghash_pos, pos);\n+  __ jcc(Assembler::aboveEqual, ACCUMULATE);\n+  \/\/ Load next cipher blocks and corresponding ghash keys\n+  __ evmovdquq(ZTMP13, Address(ct, ghash_pos, Address::times_1, 0 * 64), Assembler::AVX_512bit);\n+  __ evmovdquq(ZTMP14, Address(ct, ghash_pos, Address::times_1, 1 * 64), Assembler::AVX_512bit);\n+  __ vpshufb(ZTMP13, ZTMP13, xmm24, Assembler::AVX_512bit);\n+  __ vpshufb(ZTMP14, ZTMP14, xmm24, Assembler::AVX_512bit);\n+  __ evmovdquq(ZTMP15, Address(avx512_subkeyHtbl, rbx, Address::times_1, 0 * 64), Assembler::AVX_512bit);\n+  __ evmovdquq(ZTMP16, Address(avx512_subkeyHtbl, rbx, Address::times_1, 1 * 64), Assembler::AVX_512bit);\n+\n+  \/\/ ghash blocks 0 - 3\n+  carrylessMultiply(ZTMP6, ZTMP7, ZTMP8, ZTMP5, ZTMP13, ZTMP15);\n+\n+  \/\/ ghash blocks 4 - 7\n+  carrylessMultiply(ZTMP10, ZTMP11, ZTMP12, ZTMP9, ZTMP14, ZTMP16);\n+\n+  \/\/ update sums\n+  \/\/ ZTMP1 = ZTMP1 + ZTMP5 + ZTMP9\n+  \/\/ ZTMP2 = ZTMP2 + ZTMP6 + ZTMP10\n+  \/\/ ZTMP3 = ZTMP3 + ZTMP7 xor ZTMP11\n+  \/\/ ZTMP4 = ZTMP4 + ZTMP8 xor ZTMP12\n+  xorGHASH(ZTMP1, ZTMP2, ZTMP3, ZTMP4, ZTMP5, ZTMP9, ZTMP6, ZTMP10, ZTMP7, ZTMP11, ZTMP8, ZTMP12);\n+  __ addl(ghash_pos, 128);\n+  __ addl(rbx, 128);\n+  __ jmp(LOOP);\n+\n+  \/\/ Integrate ZTMP3\/ZTMP4 into ZTMP1 and ZTMP2\n+  __ bind(ACCUMULATE);\n+  __ evpxorq(ZTMP3, ZTMP3, ZTMP4, Assembler::AVX_512bit);\n+  __ vpsrldq(ZTMP7, ZTMP3, 8, Assembler::AVX_512bit);\n+  __ vpslldq(ZTMP8, ZTMP3, 8, Assembler::AVX_512bit);\n+  __ evpxorq(ZTMP1, ZTMP1, ZTMP7, Assembler::AVX_512bit);\n+  __ evpxorq(ZTMP2, ZTMP2, ZTMP8, Assembler::AVX_512bit);\n+\n+  \/\/ Add ZTMP1 and ZTMP2 128 - bit words horizontally\n+  vhpxori4x128(ZTMP1, ZTMP11);\n+  vhpxori4x128(ZTMP2, ZTMP12);\n+  \/\/ Load reduction polynomial and compute final reduction\n+  __ evmovdquq(ZTMP15, ExternalAddress(StubRoutines::x86::ghash_polynomial512_addr()), Assembler::AVX_512bit, rbx);\n+  vclmul_reduce(AAD_HASHx, ZTMP15, ZTMP1, ZTMP2, ZTMP3, ZTMP4);\n+\n+  \/\/ Pre-increment counter for next operation\n+  __ vpaddd(CTR_BLOCKx, CTR_BLOCKx, xmm18, Assembler::AVX_128bit);\n+  \/\/ Shuffle counter and save the updated value\n+  __ vpshufb(CTR_BLOCKx, CTR_BLOCKx, xmm24, Assembler::AVX_512bit);\n+  __ movdqu(Address(counter, 0), CTR_BLOCKx);\n+  \/\/ Load ghash lswap mask\n+  __ movdqu(xmm24, ExternalAddress(StubRoutines::x86::ghash_long_swap_mask_addr()));\n+  \/\/ Shuffle ghash using lbswap_mask and store it\n+  __ vpshufb(AAD_HASHx, AAD_HASHx, xmm24, Assembler::AVX_128bit);\n+  __ movdqu(Address(state, 0), AAD_HASHx);\n+  __ jmp(ENC_DEC_DONE);\n+\n+  __ bind(GENERATE_HTBL_48_BLKS);\n+  generateHtbl_48_block_zmm(subkeyHtbl, avx512_subkeyHtbl);\n+\n+  __ bind(ENC_DEC_DONE);\n+  __ movq(rax, pos);\n+}\n+\n+#undef __\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64_aes.cpp","additions":3160,"deletions":0,"binary":false,"changes":3160,"status":"added"},{"patch":"@@ -0,0 +1,554 @@\n+\/*\n+* Copyright (c) 2019, 2021, Intel Corporation. All rights reserved.\n+*\n+* DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+*\n+* This code is free software; you can redistribute it and\/or modify it\n+* under the terms of the GNU General Public License version 2 only, as\n+* published by the Free Software Foundation.\n+*\n+* This code is distributed in the hope that it will be useful, but WITHOUT\n+* ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+* FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+* version 2 for more details (a copy is included in the LICENSE file that\n+* accompanied this code).\n+*\n+* You should have received a copy of the GNU General Public License version\n+* 2 along with this work; if not, write to the Free Software Foundation,\n+* Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+*\n+* Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+* or visit www.oracle.com if you need additional information or have any\n+* questions.\n+*\n+*\/\n+\n+#include \"precompiled.hpp\"\n+#include \"asm\/assembler.hpp\"\n+#include \"asm\/assembler.inline.hpp\"\n+#include \"runtime\/stubRoutines.hpp\"\n+#include \"macroAssembler_x86.hpp\"\n+#include \"stubGenerator_x86_64.hpp\"\n+\n+#define __ _masm->\n+\n+\/\/ GHASH intrinsic stubs\n+\n+\n+\/\/ Polynomial x^128+x^127+x^126+x^121+1\n+address StubGenerator::generate_ghash_polynomial_addr() {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"_ghash_poly_addr\");\n+  address start = __ pc();\n+\n+  __ emit_data64(0x0000000000000001, relocInfo::none);\n+  __ emit_data64(0xc200000000000000, relocInfo::none);\n+\n+  return start;\n+}\n+\n+address StubGenerator::generate_ghash_shufflemask_addr() {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"_ghash_shuffmask_addr\");\n+  address start = __ pc();\n+\n+  __ emit_data64(0x0f0f0f0f0f0f0f0f, relocInfo::none);\n+  __ emit_data64(0x0f0f0f0f0f0f0f0f, relocInfo::none);\n+\n+  return start;\n+}\n+\n+\n+\/\/ byte swap x86 long\n+address StubGenerator::generate_ghash_long_swap_mask() {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"ghash_long_swap_mask\");\n+  address start = __ pc();\n+\n+  __ emit_data64(0x0f0e0d0c0b0a0908, relocInfo::none );\n+  __ emit_data64(0x0706050403020100, relocInfo::none );\n+\n+return start;\n+}\n+\n+\/\/ byte swap x86 byte array\n+address StubGenerator::generate_ghash_byte_swap_mask() {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"ghash_byte_swap_mask\");\n+  address start = __ pc();\n+\n+  __ emit_data64(0x08090a0b0c0d0e0f, relocInfo::none );\n+  __ emit_data64(0x0001020304050607, relocInfo::none );\n+\n+return start;\n+}\n+\n+\n+\/\/ Single and multi-block ghash operations.\n+address StubGenerator::generate_ghash_processBlocks() {\n+  __ align(CodeEntryAlignment);\n+  Label L_ghash_loop, L_exit;\n+  StubCodeMark mark(this, \"StubRoutines\", \"ghash_processBlocks\");\n+  address start = __ pc();\n+\n+  const Register state        = c_rarg0;\n+  const Register subkeyH      = c_rarg1;\n+  const Register data         = c_rarg2;\n+  const Register blocks       = c_rarg3;\n+\n+  const XMMRegister xmm_temp0 = xmm0;\n+  const XMMRegister xmm_temp1 = xmm1;\n+  const XMMRegister xmm_temp2 = xmm2;\n+  const XMMRegister xmm_temp3 = xmm3;\n+  const XMMRegister xmm_temp4 = xmm4;\n+  const XMMRegister xmm_temp5 = xmm5;\n+  const XMMRegister xmm_temp6 = xmm6;\n+  const XMMRegister xmm_temp7 = xmm7;\n+  const XMMRegister xmm_temp8 = xmm8;\n+  const XMMRegister xmm_temp9 = xmm9;\n+  const XMMRegister xmm_temp10 = xmm10;\n+\n+  __ enter();\n+\n+  __ movdqu(xmm_temp10, ExternalAddress(StubRoutines::x86::ghash_long_swap_mask_addr()));\n+\n+  __ movdqu(xmm_temp0, Address(state, 0));\n+  __ pshufb(xmm_temp0, xmm_temp10);\n+\n+\n+  __ bind(L_ghash_loop);\n+  __ movdqu(xmm_temp2, Address(data, 0));\n+  __ pshufb(xmm_temp2, ExternalAddress(StubRoutines::x86::ghash_byte_swap_mask_addr()));\n+\n+  __ movdqu(xmm_temp1, Address(subkeyH, 0));\n+  __ pshufb(xmm_temp1, xmm_temp10);\n+\n+  __ pxor(xmm_temp0, xmm_temp2);\n+\n+  \/\/\n+  \/\/ Multiply with the hash key\n+  \/\/\n+  __ movdqu(xmm_temp3, xmm_temp0);\n+  __ pclmulqdq(xmm_temp3, xmm_temp1, 0);      \/\/ xmm3 holds a0*b0\n+  __ movdqu(xmm_temp4, xmm_temp0);\n+  __ pclmulqdq(xmm_temp4, xmm_temp1, 16);     \/\/ xmm4 holds a0*b1\n+\n+  __ movdqu(xmm_temp5, xmm_temp0);\n+  __ pclmulqdq(xmm_temp5, xmm_temp1, 1);      \/\/ xmm5 holds a1*b0\n+  __ movdqu(xmm_temp6, xmm_temp0);\n+  __ pclmulqdq(xmm_temp6, xmm_temp1, 17);     \/\/ xmm6 holds a1*b1\n+\n+  __ pxor(xmm_temp4, xmm_temp5);      \/\/ xmm4 holds a0*b1 + a1*b0\n+\n+  __ movdqu(xmm_temp5, xmm_temp4);    \/\/ move the contents of xmm4 to xmm5\n+  __ psrldq(xmm_temp4, 8);    \/\/ shift by xmm4 64 bits to the right\n+  __ pslldq(xmm_temp5, 8);    \/\/ shift by xmm5 64 bits to the left\n+  __ pxor(xmm_temp3, xmm_temp5);\n+  __ pxor(xmm_temp6, xmm_temp4);      \/\/ Register pair <xmm6:xmm3> holds the result\n+                                      \/\/ of the carry-less multiplication of\n+                                      \/\/ xmm0 by xmm1.\n+\n+  \/\/ We shift the result of the multiplication by one bit position\n+  \/\/ to the left to cope for the fact that the bits are reversed.\n+  __ movdqu(xmm_temp7, xmm_temp3);\n+  __ movdqu(xmm_temp8, xmm_temp6);\n+  __ pslld(xmm_temp3, 1);\n+  __ pslld(xmm_temp6, 1);\n+  __ psrld(xmm_temp7, 31);\n+  __ psrld(xmm_temp8, 31);\n+  __ movdqu(xmm_temp9, xmm_temp7);\n+  __ pslldq(xmm_temp8, 4);\n+  __ pslldq(xmm_temp7, 4);\n+  __ psrldq(xmm_temp9, 12);\n+  __ por(xmm_temp3, xmm_temp7);\n+  __ por(xmm_temp6, xmm_temp8);\n+  __ por(xmm_temp6, xmm_temp9);\n+\n+  \/\/\n+  \/\/ First phase of the reduction\n+  \/\/\n+  \/\/ Move xmm3 into xmm7, xmm8, xmm9 in order to perform the shifts\n+  \/\/ independently.\n+  __ movdqu(xmm_temp7, xmm_temp3);\n+  __ movdqu(xmm_temp8, xmm_temp3);\n+  __ movdqu(xmm_temp9, xmm_temp3);\n+  __ pslld(xmm_temp7, 31);    \/\/ packed right shift shifting << 31\n+  __ pslld(xmm_temp8, 30);    \/\/ packed right shift shifting << 30\n+  __ pslld(xmm_temp9, 25);    \/\/ packed right shift shifting << 25\n+  __ pxor(xmm_temp7, xmm_temp8);      \/\/ xor the shifted versions\n+  __ pxor(xmm_temp7, xmm_temp9);\n+  __ movdqu(xmm_temp8, xmm_temp7);\n+  __ pslldq(xmm_temp7, 12);\n+  __ psrldq(xmm_temp8, 4);\n+  __ pxor(xmm_temp3, xmm_temp7);      \/\/ first phase of the reduction complete\n+\n+  \/\/\n+  \/\/ Second phase of the reduction\n+  \/\/\n+  \/\/ Make 3 copies of xmm3 in xmm2, xmm4, xmm5 for doing these\n+  \/\/ shift operations.\n+  __ movdqu(xmm_temp2, xmm_temp3);\n+  __ movdqu(xmm_temp4, xmm_temp3);\n+  __ movdqu(xmm_temp5, xmm_temp3);\n+  __ psrld(xmm_temp2, 1);     \/\/ packed left shifting >> 1\n+  __ psrld(xmm_temp4, 2);     \/\/ packed left shifting >> 2\n+  __ psrld(xmm_temp5, 7);     \/\/ packed left shifting >> 7\n+  __ pxor(xmm_temp2, xmm_temp4);      \/\/ xor the shifted versions\n+  __ pxor(xmm_temp2, xmm_temp5);\n+  __ pxor(xmm_temp2, xmm_temp8);\n+  __ pxor(xmm_temp3, xmm_temp2);\n+  __ pxor(xmm_temp6, xmm_temp3);      \/\/ the result is in xmm6\n+\n+  __ decrement(blocks);\n+  __ jcc(Assembler::zero, L_exit);\n+  __ movdqu(xmm_temp0, xmm_temp6);\n+  __ addptr(data, 16);\n+  __ jmp(L_ghash_loop);\n+\n+  __ bind(L_exit);\n+  __ pshufb(xmm_temp6, xmm_temp10);          \/\/ Byte swap 16-byte result\n+  __ movdqu(Address(state, 0), xmm_temp6);   \/\/ store the result\n+  __ leave();\n+  __ ret(0);\n+\n+  return start;\n+}\n+\n+\n+\/\/ Ghash single and multi block operations using AVX instructions\n+address StubGenerator::generate_avx_ghash_processBlocks() {\n+  __ align(CodeEntryAlignment);\n+\n+  StubCodeMark mark(this, \"StubRoutines\", \"ghash_processBlocks\");\n+  address start = __ pc();\n+\n+  \/\/ arguments\n+  const Register state = c_rarg0;\n+  const Register htbl = c_rarg1;\n+  const Register data = c_rarg2;\n+  const Register blocks = c_rarg3;\n+  __ enter();\n+\n+  avx_ghash(state, htbl, data, blocks);\n+\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n+\n+  return start;\n+}\n+\n+\n+\/\/ Multiblock and single block GHASH computation using Shift XOR reduction technique\n+void StubGenerator::avx_ghash(Register input_state, Register htbl,\n+                              Register input_data, Register blocks) {\n+  \/\/ temporary variables to hold input data and input state\n+  const XMMRegister data = xmm1;\n+  const XMMRegister state = xmm0;\n+  \/\/ temporary variables to hold intermediate results\n+  const XMMRegister tmp0 = xmm3;\n+  const XMMRegister tmp1 = xmm4;\n+  const XMMRegister tmp2 = xmm5;\n+  const XMMRegister tmp3 = xmm6;\n+  \/\/ temporary variables to hold byte and long swap masks\n+  const XMMRegister bswap_mask = xmm2;\n+  const XMMRegister lswap_mask = xmm14;\n+\n+  Label GENERATE_HTBL_1_BLK, GENERATE_HTBL_8_BLKS, BEGIN_PROCESS, GFMUL, BLOCK8_REDUCTION,\n+        ONE_BLK_INIT, PROCESS_1_BLOCK, PROCESS_8_BLOCKS, SAVE_STATE, EXIT_GHASH;\n+\n+  __ testptr(blocks, blocks);\n+  __ jcc(Assembler::zero, EXIT_GHASH);\n+\n+  \/\/ Check if Hashtable (1*16) has been already generated\n+  \/\/ For anything less than 8 blocks, we generate only the first power of H.\n+  __ movdqu(tmp2, Address(htbl, 1 * 16));\n+  __ ptest(tmp2, tmp2);\n+  __ jcc(Assembler::notZero, BEGIN_PROCESS);\n+  __ call(GENERATE_HTBL_1_BLK, relocInfo::none);\n+\n+  \/\/ Shuffle the input state\n+  __ bind(BEGIN_PROCESS);\n+  __ movdqu(lswap_mask, ExternalAddress(StubRoutines::x86::ghash_long_swap_mask_addr()));\n+  __ movdqu(state, Address(input_state, 0));\n+  __ vpshufb(state, state, lswap_mask, Assembler::AVX_128bit);\n+\n+  __ cmpl(blocks, 8);\n+  __ jcc(Assembler::below, ONE_BLK_INIT);\n+  \/\/ If we have 8 blocks or more data, then generate remaining powers of H\n+  __ movdqu(tmp2, Address(htbl, 8 * 16));\n+  __ ptest(tmp2, tmp2);\n+  __ jcc(Assembler::notZero, PROCESS_8_BLOCKS);\n+  __ call(GENERATE_HTBL_8_BLKS, relocInfo::none);\n+\n+  \/\/Do 8 multiplies followed by a reduction processing 8 blocks of data at a time\n+  \/\/Each block = 16 bytes.\n+  __ bind(PROCESS_8_BLOCKS);\n+  __ subl(blocks, 8);\n+  __ movdqu(bswap_mask, ExternalAddress(StubRoutines::x86::ghash_byte_swap_mask_addr()));\n+  __ movdqu(data, Address(input_data, 16 * 7));\n+  __ vpshufb(data, data, bswap_mask, Assembler::AVX_128bit);\n+  \/\/Loading 1*16 as calculated powers of H required starts at that location.\n+  __ movdqu(xmm15, Address(htbl, 1 * 16));\n+  \/\/Perform carryless multiplication of (H*2, data block #7)\n+  __ vpclmulhqlqdq(tmp2, data, xmm15);\/\/a0 * b1\n+  __ vpclmulldq(tmp0, data, xmm15);\/\/a0 * b0\n+  __ vpclmulhdq(tmp1, data, xmm15);\/\/a1 * b1\n+  __ vpclmullqhqdq(tmp3, data, xmm15);\/\/a1* b0\n+  __ vpxor(tmp2, tmp2, tmp3, Assembler::AVX_128bit);\/\/ (a0 * b1) + (a1 * b0)\n+\n+  __ movdqu(data, Address(input_data, 16 * 6));\n+  __ vpshufb(data, data, bswap_mask, Assembler::AVX_128bit);\n+  \/\/ Perform carryless multiplication of (H^2 * 2, data block #6)\n+  schoolbookAAD(2, htbl, data, tmp0, tmp1, tmp2, tmp3);\n+\n+  __ movdqu(data, Address(input_data, 16 * 5));\n+  __ vpshufb(data, data, bswap_mask, Assembler::AVX_128bit);\n+  \/\/ Perform carryless multiplication of (H^3 * 2, data block #5)\n+  schoolbookAAD(3, htbl, data, tmp0, tmp1, tmp2, tmp3);\n+  __ movdqu(data, Address(input_data, 16 * 4));\n+  __ vpshufb(data, data, bswap_mask, Assembler::AVX_128bit);\n+  \/\/ Perform carryless multiplication of (H^4 * 2, data block #4)\n+  schoolbookAAD(4, htbl, data, tmp0, tmp1, tmp2, tmp3);\n+  __ movdqu(data, Address(input_data, 16 * 3));\n+  __ vpshufb(data, data, bswap_mask, Assembler::AVX_128bit);\n+  \/\/ Perform carryless multiplication of (H^5 * 2, data block #3)\n+  schoolbookAAD(5, htbl, data, tmp0, tmp1, tmp2, tmp3);\n+  __ movdqu(data, Address(input_data, 16 * 2));\n+  __ vpshufb(data, data, bswap_mask, Assembler::AVX_128bit);\n+  \/\/ Perform carryless multiplication of (H^6 * 2, data block #2)\n+  schoolbookAAD(6, htbl, data, tmp0, tmp1, tmp2, tmp3);\n+  __ movdqu(data, Address(input_data, 16 * 1));\n+  __ vpshufb(data, data, bswap_mask, Assembler::AVX_128bit);\n+  \/\/ Perform carryless multiplication of (H^7 * 2, data block #1)\n+  schoolbookAAD(7, htbl, data, tmp0, tmp1, tmp2, tmp3);\n+  __ movdqu(data, Address(input_data, 16 * 0));\n+  \/\/ xor data block#0 with input state before performing carry-less multiplication\n+  __ vpshufb(data, data, bswap_mask, Assembler::AVX_128bit);\n+  __ vpxor(data, data, state, Assembler::AVX_128bit);\n+  \/\/ Perform carryless multiplication of (H^8 * 2, data block #0)\n+  schoolbookAAD(8, htbl, data, tmp0, tmp1, tmp2, tmp3);\n+  __ vpslldq(tmp3, tmp2, 8, Assembler::AVX_128bit);\n+  __ vpsrldq(tmp2, tmp2, 8, Assembler::AVX_128bit);\n+  __ vpxor(tmp0, tmp0, tmp3, Assembler::AVX_128bit);\/\/ tmp0, tmp1 contains aggregated results of\n+  __ vpxor(tmp1, tmp1, tmp2, Assembler::AVX_128bit);\/\/ the multiplication operation\n+\n+  \/\/ we have the 2 128-bit partially accumulated multiplication results in tmp0:tmp1\n+  \/\/ with higher 128-bit in tmp1 and lower 128-bit in corresponding tmp0\n+  \/\/ Follows the reduction technique mentioned in\n+  \/\/ Shift-XOR reduction described in Gueron-Kounavis May 2010\n+  __ bind(BLOCK8_REDUCTION);\n+  \/\/ First Phase of the reduction\n+  __ vpslld(xmm8, tmp0, 31, Assembler::AVX_128bit); \/\/ packed right shifting << 31\n+  __ vpslld(xmm9, tmp0, 30, Assembler::AVX_128bit); \/\/ packed right shifting << 30\n+  __ vpslld(xmm10, tmp0, 25, Assembler::AVX_128bit); \/\/ packed right shifting << 25\n+  \/\/ xor the shifted versions\n+  __ vpxor(xmm8, xmm8, xmm10, Assembler::AVX_128bit);\n+  __ vpxor(xmm8, xmm8, xmm9, Assembler::AVX_128bit);\n+\n+  __ vpslldq(xmm9, xmm8, 12, Assembler::AVX_128bit);\n+  __ vpsrldq(xmm8, xmm8, 4, Assembler::AVX_128bit);\n+\n+  __ vpxor(tmp0, tmp0, xmm9, Assembler::AVX_128bit); \/\/ first phase of reduction is complete\n+  \/\/ second phase of the reduction\n+  __ vpsrld(xmm9, tmp0, 1, Assembler::AVX_128bit); \/\/ packed left shifting >> 1\n+  __ vpsrld(xmm10, tmp0, 2, Assembler::AVX_128bit); \/\/ packed left shifting >> 2\n+  __ vpsrld(tmp2, tmp0, 7, Assembler::AVX_128bit); \/\/ packed left shifting >> 7\n+  \/\/ xor the shifted versions\n+  __ vpxor(xmm9, xmm9, xmm10, Assembler::AVX_128bit);\n+  __ vpxor(xmm9, xmm9, tmp2, Assembler::AVX_128bit);\n+  __ vpxor(xmm9, xmm9, xmm8, Assembler::AVX_128bit);\n+  __ vpxor(tmp0, xmm9, tmp0, Assembler::AVX_128bit);\n+  \/\/ Final result is in state\n+  __ vpxor(state, tmp0, tmp1, Assembler::AVX_128bit);\n+\n+  __ lea(input_data, Address(input_data, 16 * 8));\n+  __ cmpl(blocks, 8);\n+  __ jcc(Assembler::below, ONE_BLK_INIT);\n+  __ jmp(PROCESS_8_BLOCKS);\n+\n+  \/\/ Since this is one block operation we will only use H * 2 i.e. the first power of H\n+  __ bind(ONE_BLK_INIT);\n+  __ movdqu(tmp0, Address(htbl, 1 * 16));\n+  __ movdqu(bswap_mask, ExternalAddress(StubRoutines::x86::ghash_byte_swap_mask_addr()));\n+\n+  \/\/Do one (128 bit x 128 bit) carry-less multiplication at a time followed by a reduction.\n+  __ bind(PROCESS_1_BLOCK);\n+  __ cmpl(blocks, 0);\n+  __ jcc(Assembler::equal, SAVE_STATE);\n+  __ subl(blocks, 1);\n+  __ movdqu(data, Address(input_data, 0));\n+  __ vpshufb(data, data, bswap_mask, Assembler::AVX_128bit);\n+  __ vpxor(state, state, data, Assembler::AVX_128bit);\n+  \/\/ gfmul(H*2, state)\n+  __ call(GFMUL, relocInfo::none);\n+  __ addptr(input_data, 16);\n+  __ jmp(PROCESS_1_BLOCK);\n+\n+  __ bind(SAVE_STATE);\n+  __ vpshufb(state, state, lswap_mask, Assembler::AVX_128bit);\n+  __ movdqu(Address(input_state, 0), state);\n+  __ jmp(EXIT_GHASH);\n+\n+  __ bind(GFMUL);\n+  gfmul(tmp0, state);\n+\n+  __ bind(GENERATE_HTBL_1_BLK);\n+  generateHtbl_one_block(htbl);\n+\n+  __ bind(GENERATE_HTBL_8_BLKS);\n+  generateHtbl_eight_blocks(htbl);\n+\n+  __ bind(EXIT_GHASH);\n+  \/\/ zero out xmm registers used for Htbl storage\n+  __ vpxor(xmm0, xmm0, xmm0, Assembler::AVX_128bit);\n+  __ vpxor(xmm1, xmm1, xmm1, Assembler::AVX_128bit);\n+  __ vpxor(xmm3, xmm3, xmm3, Assembler::AVX_128bit);\n+  __ vpxor(xmm15, xmm15, xmm15, Assembler::AVX_128bit);\n+}\n+\n+\n+\/\/ Multiply two 128 bit numbers resulting in a 256 bit value\n+\/\/ Result of the multiplication followed by reduction stored in state\n+void StubGenerator::gfmul(XMMRegister tmp0, XMMRegister state) {\n+  const XMMRegister tmp1 = xmm4;\n+  const XMMRegister tmp2 = xmm5;\n+  const XMMRegister tmp3 = xmm6;\n+  const XMMRegister tmp4 = xmm7;\n+\n+  __ vpclmulldq(tmp1, state, tmp0); \/\/0x00  (a0 * b0)\n+  __ vpclmulhdq(tmp4, state, tmp0);\/\/0x11 (a1 * b1)\n+  __ vpclmullqhqdq(tmp2, state, tmp0);\/\/0x10 (a1 * b0)\n+  __ vpclmulhqlqdq(tmp3, state, tmp0); \/\/0x01 (a0 * b1)\n+\n+  __ vpxor(tmp2, tmp2, tmp3, Assembler::AVX_128bit); \/\/ (a0 * b1) + (a1 * b0)\n+\n+  __ vpslldq(tmp3, tmp2, 8, Assembler::AVX_128bit);\n+  __ vpsrldq(tmp2, tmp2, 8, Assembler::AVX_128bit);\n+  __ vpxor(tmp1, tmp1, tmp3, Assembler::AVX_128bit); \/\/ tmp1 and tmp4 hold the result\n+  __ vpxor(tmp4, tmp4, tmp2, Assembler::AVX_128bit); \/\/ of carryless multiplication\n+  \/\/ Follows the reduction technique mentioned in\n+  \/\/ Shift-XOR reduction described in Gueron-Kounavis May 2010\n+  \/\/ First phase of reduction\n+  \/\/\n+  __ vpslld(xmm8, tmp1, 31, Assembler::AVX_128bit); \/\/ packed right shift shifting << 31\n+  __ vpslld(xmm9, tmp1, 30, Assembler::AVX_128bit); \/\/ packed right shift shifting << 30\n+  __ vpslld(xmm10, tmp1, 25, Assembler::AVX_128bit);\/\/ packed right shift shifting << 25\n+  \/\/ xor the shifted versions\n+  __ vpxor(xmm8, xmm8, xmm9, Assembler::AVX_128bit);\n+  __ vpxor(xmm8, xmm8, xmm10, Assembler::AVX_128bit);\n+  __ vpslldq(xmm9, xmm8, 12, Assembler::AVX_128bit);\n+  __ vpsrldq(xmm8, xmm8, 4, Assembler::AVX_128bit);\n+  __ vpxor(tmp1, tmp1, xmm9, Assembler::AVX_128bit);\/\/ first phase of the reduction complete\n+  \/\/\n+  \/\/ Second phase of the reduction\n+  \/\/\n+  __ vpsrld(xmm9, tmp1, 1, Assembler::AVX_128bit);\/\/ packed left shifting >> 1\n+  __ vpsrld(xmm10, tmp1, 2, Assembler::AVX_128bit);\/\/ packed left shifting >> 2\n+  __ vpsrld(xmm11, tmp1, 7, Assembler::AVX_128bit);\/\/ packed left shifting >> 7\n+  __ vpxor(xmm9, xmm9, xmm10, Assembler::AVX_128bit);\/\/ xor the shifted versions\n+  __ vpxor(xmm9, xmm9, xmm11, Assembler::AVX_128bit);\n+  __ vpxor(xmm9, xmm9, xmm8, Assembler::AVX_128bit);\n+  __ vpxor(tmp1, tmp1, xmm9, Assembler::AVX_128bit);\n+  __ vpxor(state, tmp4, tmp1, Assembler::AVX_128bit);\/\/ the result is in state\n+  __ ret(0);\n+}\n+\n+\n+\/\/ Multiply 128 x 128 bits, using 4 pclmulqdq operations\n+void StubGenerator::schoolbookAAD(int i, Register htbl, XMMRegister data,\n+                                  XMMRegister tmp0, XMMRegister tmp1,\n+                                  XMMRegister tmp2, XMMRegister tmp3) {\n+  __ movdqu(xmm15, Address(htbl, i * 16));\n+  __ vpclmulhqlqdq(tmp3, data, xmm15); \/\/ 0x01\n+  __ vpxor(tmp2, tmp2, tmp3, Assembler::AVX_128bit);\n+  __ vpclmulldq(tmp3, data, xmm15); \/\/ 0x00\n+  __ vpxor(tmp0, tmp0, tmp3, Assembler::AVX_128bit);\n+  __ vpclmulhdq(tmp3, data, xmm15); \/\/ 0x11\n+  __ vpxor(tmp1, tmp1, tmp3, Assembler::AVX_128bit);\n+  __ vpclmullqhqdq(tmp3, data, xmm15); \/\/ 0x10\n+  __ vpxor(tmp2, tmp2, tmp3, Assembler::AVX_128bit);\n+}\n+\n+\n+\/\/ This method takes the subkey after expansion as input and generates 1 * 16 power of subkey H.\n+\/\/ The power of H is used in reduction process for one block ghash\n+void StubGenerator::generateHtbl_one_block(Register htbl) {\n+  const XMMRegister t = xmm13;\n+\n+  \/\/ load the original subkey hash\n+  __ movdqu(t, Address(htbl, 0));\n+  \/\/ shuffle using long swap mask\n+  __ movdqu(xmm10, ExternalAddress(StubRoutines::x86::ghash_long_swap_mask_addr()));\n+  __ vpshufb(t, t, xmm10, Assembler::AVX_128bit);\n+\n+  \/\/ Compute H' = GFMUL(H, 2)\n+  __ vpsrld(xmm3, t, 7, Assembler::AVX_128bit);\n+  __ movdqu(xmm4, ExternalAddress(StubRoutines::x86::ghash_shufflemask_addr()));\n+  __ vpshufb(xmm3, xmm3, xmm4, Assembler::AVX_128bit);\n+  __ movl(rax, 0xff00);\n+  __ movdl(xmm4, rax);\n+  __ vpshufb(xmm4, xmm4, xmm3, Assembler::AVX_128bit);\n+  __ movdqu(xmm5, ExternalAddress(StubRoutines::x86::ghash_polynomial_addr()));\n+  __ vpand(xmm5, xmm5, xmm4, Assembler::AVX_128bit);\n+  __ vpsrld(xmm3, t, 31, Assembler::AVX_128bit);\n+  __ vpslld(xmm4, t, 1, Assembler::AVX_128bit);\n+  __ vpslldq(xmm3, xmm3, 4, Assembler::AVX_128bit);\n+  __ vpxor(t, xmm4, xmm3, Assembler::AVX_128bit);\/\/ t holds p(x) <<1 or H * 2\n+\n+  \/\/Adding p(x)<<1 to xmm5 which holds the reduction polynomial\n+  __ vpxor(t, t, xmm5, Assembler::AVX_128bit);\n+  __ movdqu(Address(htbl, 1 * 16), t); \/\/ H * 2\n+\n+  __ ret(0);\n+}\n+\n+\n+\/\/ This method takes the subkey after expansion as input and generates the remaining powers of subkey H.\n+\/\/ The power of H is used in reduction process for eight block ghash\n+void StubGenerator::generateHtbl_eight_blocks(Register htbl) {\n+  const XMMRegister t = xmm13;\n+  const XMMRegister tmp0 = xmm1;\n+  Label GFMUL;\n+\n+  __ movdqu(t, Address(htbl, 1 * 16));\n+  __ movdqu(tmp0, t);\n+\n+  \/\/ tmp0 and t hold H. Now we compute powers of H by using GFMUL(H, H)\n+  __ call(GFMUL, relocInfo::none);\n+  __ movdqu(Address(htbl, 2 * 16), t); \/\/H ^ 2 * 2\n+  __ call(GFMUL, relocInfo::none);\n+  __ movdqu(Address(htbl, 3 * 16), t); \/\/H ^ 3 * 2\n+  __ call(GFMUL, relocInfo::none);\n+  __ movdqu(Address(htbl, 4 * 16), t); \/\/H ^ 4 * 2\n+  __ call(GFMUL, relocInfo::none);\n+  __ movdqu(Address(htbl, 5 * 16), t); \/\/H ^ 5 * 2\n+  __ call(GFMUL, relocInfo::none);\n+  __ movdqu(Address(htbl, 6 * 16), t); \/\/H ^ 6 * 2\n+  __ call(GFMUL, relocInfo::none);\n+  __ movdqu(Address(htbl, 7 * 16), t); \/\/H ^ 7 * 2\n+  __ call(GFMUL, relocInfo::none);\n+  __ movdqu(Address(htbl, 8 * 16), t); \/\/H ^ 8 * 2\n+  __ ret(0);\n+\n+  __ bind(GFMUL);\n+  gfmul(tmp0, t);\n+}\n+\n+\n+void StubGenerator::generate_ghash_stubs() {\n+  if (UseGHASHIntrinsics) {\n+    if (StubRoutines::x86::_ghash_long_swap_mask_addr == NULL) {\n+      StubRoutines::x86::_ghash_long_swap_mask_addr = generate_ghash_long_swap_mask();\n+    }\n+    StubRoutines::x86::_ghash_byte_swap_mask_addr = generate_ghash_byte_swap_mask();\n+    if (VM_Version::supports_avx()) {\n+      StubRoutines::x86::_ghash_shuffmask_addr = generate_ghash_shufflemask_addr();\n+      StubRoutines::x86::_ghash_poly_addr = generate_ghash_polynomial_addr();\n+      StubRoutines::_ghash_processBlocks = generate_avx_ghash_processBlocks();\n+    } else {\n+      StubRoutines::_ghash_processBlocks = generate_ghash_processBlocks();\n+    }\n+  }\n+}\n+\n+#undef __\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64_ghash.cpp","additions":554,"deletions":0,"binary":false,"changes":554,"status":"added"}]}