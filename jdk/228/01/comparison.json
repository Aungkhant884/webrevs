{"files":[{"patch":"@@ -457,6 +457,2 @@\n-  if (VMOperationQueue_lock->owned_by_self()) {\n-    VMOperationQueue_lock->unlock();\n-  }\n-\n-  if (VMOperationRequest_lock->owned_by_self()) {\n-    VMOperationRequest_lock->unlock();\n+  if (VMOperation_lock->owned_by_self()) {\n+    VMOperation_lock->unlock();\n","filename":"src\/hotspot\/share\/jfr\/recorder\/repository\/jfrEmergencyDump.cpp","additions":2,"deletions":6,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -68,2 +68,1 @@\n-Monitor* VMOperationQueue_lock        = NULL;\n-Monitor* VMOperationRequest_lock      = NULL;\n+Monitor* VMOperation_lock             = NULL;\n@@ -283,2 +282,1 @@\n-  def(VMOperationQueue_lock        , PaddedMonitor, nonleaf,     true,  _safepoint_check_never);  \/\/ VM_thread allowed to block on these\n-  def(VMOperationRequest_lock      , PaddedMonitor, nonleaf,     true,  _safepoint_check_always);\n+  def(VMOperation_lock             , PaddedMonitor, nonleaf,     true,  _safepoint_check_always);  \/\/ VM_thread allowed to block on these\n","filename":"src\/hotspot\/share\/runtime\/mutexLocker.cpp","additions":2,"deletions":4,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -61,1 +61,1 @@\n-extern Monitor* VMOperationQueue_lock;           \/\/ a lock on queue of vm_operations waiting to execute\n+extern Monitor* VMOperation_lock;                \/\/ a lock on queue of vm_operations waiting to execute\n","filename":"src\/hotspot\/share\/runtime\/mutexLocker.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -259,2 +259,0 @@\n-  _vm_operation_started_count = 0;\n-  _vm_operation_completed_count = 0;\n","filename":"src\/hotspot\/share\/runtime\/thread.cpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -404,3 +404,0 @@\n-  int   _vm_operation_started_count;            \/\/ VM_Operation support\n-  int   _vm_operation_completed_count;          \/\/ VM_Operation support\n-\n@@ -624,5 +621,0 @@\n-  \/\/ VM operation support\n-  int vm_operation_ticket()                      { return ++_vm_operation_started_count; }\n-  int vm_operation_completed_count()             { return _vm_operation_completed_count; }\n-  void increment_vm_operation_completed_count()  { _vm_operation_completed_count++; }\n-\n","filename":"src\/hotspot\/share\/runtime\/thread.hpp","additions":0,"deletions":8,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -124,2 +124,0 @@\n-  VM_Operation*   _next;\n-  VM_Operation*   _prev;\n@@ -131,1 +129,1 @@\n-  VM_Operation() : _calling_thread(NULL), _next(NULL), _prev(NULL) {}\n+  VM_Operation() : _calling_thread(NULL) {}\n@@ -151,6 +149,0 @@\n-  \/\/ Linking\n-  VM_Operation *next() const                     { return _next; }\n-  VM_Operation *prev() const                     { return _prev; }\n-  void set_next(VM_Operation *next)              { _next = next; }\n-  void set_prev(VM_Operation *prev)              { _prev = prev; }\n-\n","filename":"src\/hotspot\/share\/runtime\/vmOperations.hpp","additions":1,"deletions":9,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -54,87 +54,0 @@\n-VM_QueueHead VMOperationQueue::_queue_head[VMOperationQueue::nof_priorities];\n-\n-VMOperationQueue::VMOperationQueue() {\n-  \/\/ The queue is a circular doubled-linked list, which always contains\n-  \/\/ one element (i.e., one element means empty).\n-  for(int i = 0; i < nof_priorities; i++) {\n-    _queue_length[i] = 0;\n-    _queue_counter = 0;\n-    _queue[i] = &_queue_head[i];\n-    _queue[i]->set_next(_queue[i]);\n-    _queue[i]->set_prev(_queue[i]);\n-  }\n-}\n-\n-\n-bool VMOperationQueue::queue_empty(int prio) {\n-  \/\/ It is empty if there is exactly one element\n-  bool empty = (_queue[prio] == _queue[prio]->next());\n-  assert( (_queue_length[prio] == 0 && empty) ||\n-          (_queue_length[prio] > 0  && !empty), \"sanity check\");\n-  return _queue_length[prio] == 0;\n-}\n-\n-\/\/ Inserts an element to the right of the q element\n-void VMOperationQueue::insert(VM_Operation* q, VM_Operation* n) {\n-  assert(q->next()->prev() == q && q->prev()->next() == q, \"sanity check\");\n-  n->set_prev(q);\n-  n->set_next(q->next());\n-  q->next()->set_prev(n);\n-  q->set_next(n);\n-}\n-\n-void VMOperationQueue::queue_add(int prio, VM_Operation *op) {\n-  _queue_length[prio]++;\n-  insert(_queue[prio]->prev(), op);\n-}\n-\n-\n-void VMOperationQueue::unlink(VM_Operation* q) {\n-  assert(q->next()->prev() == q && q->prev()->next() == q, \"sanity check\");\n-  q->prev()->set_next(q->next());\n-  q->next()->set_prev(q->prev());\n-}\n-\n-VM_Operation* VMOperationQueue::queue_remove_front(int prio) {\n-  if (queue_empty(prio)) return NULL;\n-  assert(_queue_length[prio] >= 0, \"sanity check\");\n-  _queue_length[prio]--;\n-  VM_Operation* r = _queue[prio]->next();\n-  assert(r != _queue[prio], \"cannot remove base element\");\n-  unlink(r);\n-  return r;\n-}\n-\n-\/\/-----------------------------------------------------------------\n-\/\/ High-level interface\n-void VMOperationQueue::add(VM_Operation *op) {\n-\n-  HOTSPOT_VMOPS_REQUEST(\n-                   (char *) op->name(), strlen(op->name()),\n-                   op->evaluate_at_safepoint() ? 0 : 1);\n-\n-  \/\/ Encapsulates VM queue policy. Currently, that\n-  \/\/ only involves putting them on the right list\n-  queue_add(op->evaluate_at_safepoint() ? SafepointPriority : MediumPriority, op);\n-}\n-\n-VM_Operation* VMOperationQueue::remove_next() {\n-  \/\/ Assuming VMOperation queue is two-level priority queue. If there are\n-  \/\/ more than two priorities, we need a different scheduling algorithm.\n-  assert(SafepointPriority == 0 && MediumPriority == 1 && nof_priorities == 2,\n-         \"current algorithm does not work\");\n-\n-  \/\/ simple counter based scheduling to prevent starvation of lower priority\n-  \/\/ queue. -- see 4390175\n-  int high_prio, low_prio;\n-  if (_queue_counter++ < 10) {\n-      high_prio = SafepointPriority;\n-      low_prio  = MediumPriority;\n-  } else {\n-      _queue_counter = 0;\n-      high_prio = MediumPriority;\n-      low_prio  = SafepointPriority;\n-  }\n-\n-  return queue_remove_front(queue_empty(high_prio) ? low_prio : high_prio);\n-}\n@@ -172,0 +85,3 @@\n+static VM_None    safepointALot_op(\"SafepointALot\");\n+static VM_Cleanup cleanup_op;\n+\n@@ -177,1 +93,1 @@\n-VMOperationQueue* VMThread::_vm_queue           = NULL;\n+VM_Operation*     VMThread::_next_vm_operation  = &cleanup_op; \/\/ Prevent any thread from setting an operation until VM thread is ready.\n@@ -201,4 +117,0 @@\n-  \/\/ Create VM operation queue\n-  _vm_queue = new VMOperationQueue();\n-  guarantee(_vm_queue != NULL, \"just checking\");\n-\n@@ -313,1 +225,2 @@\n-  { MonitorLocker mu(VMOperationQueue_lock, Mutex::_no_safepoint_check_flag);\n+  {\n+    MonitorLocker mu(VMOperation_lock);\n@@ -315,1 +228,1 @@\n-    mu.notify();\n+    mu.notify_all();\n@@ -327,2 +240,3 @@\n-  { MonitorLocker ml(_terminate_lock, Mutex::_no_safepoint_check_flag);\n-    while(!VMThread::is_terminated()) {\n+  {\n+    MonitorLocker ml(_terminate_lock, Mutex::_no_safepoint_check_flag);\n+    while (!VMThread::is_terminated()) {\n@@ -367,2 +281,0 @@\n-  \/\/ Mark as completed\n-  op->calling_thread()->increment_vm_operation_completed_count();\n@@ -371,3 +283,0 @@\n-static VM_None    safepointALot_op(\"SafepointALot\");\n-static VM_Cleanup cleanup_op;\n-\n@@ -384,5 +293,15 @@\n-VM_Operation* VMThread::no_op_safepoint() {\n-  \/\/ Check for handshakes first since we may need to return a VMop.\n-  if (HandshakeALot) {\n-    HandshakeALotClosure hal_cl;\n-    Handshake::execute(&hal_cl);\n+bool VMThread::handshake_alot() {\n+  assert(_cur_vm_operation == NULL, \"Already have an op\");\n+  assert(_next_vm_operation == NULL, \"Already have an op\");\n+  if (!HandshakeALot) {\n+    return false;\n+  }\n+  static jlong last_halot_ms = 0;\n+  jlong now_ms = nanos_to_millis(os::javaTimeNanos());\n+  \/\/ If only HandshakeALot is set, but GuaranteedSafepointInterval is 0,\n+  \/\/ we emit a handshake if it's been more than a second since last.\n+  jlong interval = GuaranteedSafepointInterval != 0 ? GuaranteedSafepointInterval : 1000;\n+  jlong deadline_ms = interval + last_halot_ms;\n+  if (now_ms > deadline_ms) {\n+    last_halot_ms = now_ms;\n+    return true;\n@@ -390,0 +309,6 @@\n+  return false;\n+}\n+\n+void VMThread::setup_periodic_safepoint_if_needed() {\n+  assert(_cur_vm_operation  == NULL, \"Already have an op\");\n+  assert(_next_vm_operation == NULL, \"Already have an op\");\n@@ -394,2 +319,2 @@\n-  if (max_time_exceeded && SafepointSynchronize::is_cleanup_needed()) {\n-    return &cleanup_op;\n+  if (!max_time_exceeded) {\n+    return;\n@@ -397,2 +322,4 @@\n-  if (SafepointALot) {\n-    return &safepointALot_op;\n+  if (SafepointSynchronize::is_cleanup_needed()) {\n+    _next_vm_operation = &cleanup_op;\n+  } else if (SafepointALot) {\n+    _next_vm_operation = &safepointALot_op;\n@@ -400,2 +327,0 @@\n-  \/\/ Nothing to be done.\n-  return NULL;\n@@ -404,2 +329,5 @@\n-void VMThread::loop() {\n-  assert(_cur_vm_operation == NULL, \"no current one should be executing\");\n+bool VMThread::set_next_operation(VM_Operation *op) {\n+  if (_next_vm_operation != NULL) {\n+    return false;\n+  }\n+  log_debug(vmthread)(\"Adding VM operation: %s\", op->name());\n@@ -407,1 +335,1 @@\n-  SafepointSynchronize::init(_vm_thread);\n+  _next_vm_operation = op;\n@@ -409,43 +337,5 @@\n-  while(true) {\n-    \/\/\n-    \/\/ Wait for VM operation\n-    \/\/\n-    \/\/ use no_safepoint_check to get lock without attempting to \"sneak\"\n-    { MonitorLocker mu_queue(VMOperationQueue_lock,\n-                             Mutex::_no_safepoint_check_flag);\n-\n-      \/\/ Look for new operation\n-      assert(_cur_vm_operation == NULL, \"no current one should be executing\");\n-      _cur_vm_operation = _vm_queue->remove_next();\n-\n-      while (!should_terminate() && _cur_vm_operation == NULL) {\n-        \/\/ wait with a timeout to guarantee safepoints at regular intervals\n-        \/\/ (if there is cleanup work to do)\n-        (void)mu_queue.wait(GuaranteedSafepointInterval);\n-\n-        \/\/ Support for self destruction\n-        if ((SelfDestructTimer != 0) && !VMError::is_error_reported() &&\n-            (os::elapsedTime() > (double)SelfDestructTimer * 60.0)) {\n-          tty->print_cr(\"VM self-destructed\");\n-          exit(-1);\n-        }\n-\n-        \/\/ If the queue contains a safepoint VM op,\n-        \/\/ clean up will be done so we can skip this part.\n-        if (!_vm_queue->peek_at_safepoint_priority()) {\n-\n-          \/\/ Have to unlock VMOperationQueue_lock just in case no_op_safepoint()\n-          \/\/ has to do a handshake when HandshakeALot is enabled.\n-          MutexUnlocker mul(VMOperationQueue_lock, Mutex::_no_safepoint_check_flag);\n-          if ((_cur_vm_operation = VMThread::no_op_safepoint()) != NULL) {\n-            \/\/ Force a safepoint since we have not had one for at least\n-            \/\/ 'GuaranteedSafepointInterval' milliseconds and we need to clean\n-            \/\/ something. This will run all the clean-up processing that needs\n-            \/\/ to be done at a safepoint.\n-            SafepointSynchronize::begin();\n-            SafepointSynchronize::end();\n-            _cur_vm_operation = NULL;\n-          }\n-        }\n-        _cur_vm_operation = _vm_queue->remove_next();\n-      }\n+  HOTSPOT_VMOPS_REQUEST(\n+                   (char *) op->name(), strlen(op->name()),\n+                   op->evaluate_at_safepoint() ? 0 : 1);\n+  return true;\n+}\n@@ -453,2 +343,18 @@\n-      if (should_terminate()) break;\n-    } \/\/ Release mu_queue_lock\n+void VMThread::wait_until_executed(VM_Operation* op) {\n+  MonitorLocker ml(VMOperation_lock,\n+                   Thread::current()->is_Java_thread() ?\n+                     Mutex::_safepoint_check_flag :\n+                     Mutex::_no_safepoint_check_flag);\n+  while (true) {\n+    if (VMThread::vm_thread()->set_next_operation(op)) {\n+      ml.notify_all();\n+      break;\n+    }\n+    ml.wait();\n+  }\n+  \/\/ _next_vm_operation is cleared holding VMOperation_lock\n+  \/\/ after it have been executed.\n+  while (_next_vm_operation == op) {\n+    ml.wait();\n+  }\n+}\n@@ -456,4 +362,8 @@\n-    \/\/\n-    \/\/ Execute VM operation\n-    \/\/\n-    { HandleMark hm(VMThread::vm_thread());\n+static void self_destruct_if_needed() {\n+  \/\/ Support for self destruction\n+  if ((SelfDestructTimer != 0) && !VMError::is_error_reported() &&\n+      (os::elapsedTime() > (double)SelfDestructTimer * 60.0)) {\n+    tty->print_cr(\"VM self-destructed\");\n+    exit(-1);\n+  }\n+}\n@@ -461,2 +371,15 @@\n-      EventMark em(\"Executing VM operation: %s\", vm_operation()->name());\n-      assert(_cur_vm_operation != NULL, \"we should have found an operation to execute\");\n+void VMThread::inner_execute(VM_Operation* op) {\n+  Thread* current = Thread::current();\n+  assert(current->is_VM_thread(), \"must be a VM thread\");\n+\n+  VM_Operation* prev_vm_operation = vm_operation();\n+  if (prev_vm_operation != NULL) {\n+    \/\/ Check the VM operation allows nested VM operation.\n+    \/\/ This normally not the case, e.g., the compiler\n+    \/\/ does not allow nested scavenges or compiles.\n+    if (!prev_vm_operation->allow_nested_vm_operations()) {\n+      fatal(\"Nested VM operation %s requested by operation %s\",\n+            op->name(), vm_operation()->name());\n+    }\n+    op->set_calling_thread(prev_vm_operation->calling_thread());\n+  }\n@@ -464,4 +387,1 @@\n-      \/\/ If we are at a safepoint we will evaluate all the operations that\n-      \/\/ follow that also require a safepoint\n-      if (_cur_vm_operation->evaluate_at_safepoint()) {\n-        log_debug(vmthread)(\"Evaluating safepoint VM operation: %s\", _cur_vm_operation->name());\n+  _cur_vm_operation = op;\n@@ -469,1 +389,2 @@\n-        SafepointSynchronize::begin();\n+  HandleMark hm(VMThread::vm_thread());\n+  EventMark em(\"Executing %s VM operation: %s\", prev_vm_operation != NULL ? \"nested\" : \"\", op->name());\n@@ -471,3 +392,6 @@\n-        if (_timeout_task != NULL) {\n-          _timeout_task->arm();\n-        }\n+  \/\/ If we are at a safepoint we will evaluate all the operations that\n+  \/\/ follow that also require a safepoint\n+  log_debug(vmthread)(\"Evaluating %s %s VM operation: %s\",\n+                       prev_vm_operation != NULL ? \"nested\" : \"\",\n+                      _cur_vm_operation->evaluate_at_safepoint() ? \"safepoint\" : \"non-safepoint\",\n+                      _cur_vm_operation->name());\n@@ -475,2 +399,9 @@\n-        evaluate_operation(_cur_vm_operation);\n-        _cur_vm_operation = NULL;\n+  bool end_safepoint = false;\n+  if (_cur_vm_operation->evaluate_at_safepoint() &&\n+      !SafepointSynchronize::is_at_safepoint()) {\n+    SafepointSynchronize::begin();\n+    if (_timeout_task != NULL) {\n+      _timeout_task->arm();\n+    }\n+    end_safepoint = true;\n+  }\n@@ -478,3 +409,1 @@\n-        if (_timeout_task != NULL) {\n-          _timeout_task->disarm();\n-        }\n+  evaluate_operation(_cur_vm_operation);\n@@ -482,2 +411,18 @@\n-        \/\/ Complete safepoint synchronization\n-        SafepointSynchronize::end();\n+  if (end_safepoint) {\n+    if (_timeout_task != NULL) {\n+      _timeout_task->disarm();\n+    }\n+    SafepointSynchronize::end();\n+  }\n+\n+  _cur_vm_operation = prev_vm_operation;\n+}\n+\n+void VMThread::wait_for_operation() {\n+  MonitorLocker ml_op_lock(VMOperation_lock, Mutex::_no_safepoint_check_flag);\n+\n+  \/\/ Clear previous operation.\n+  \/\/ On first call this clears a dummy place-holder.\n+  _next_vm_operation = NULL;\n+  \/\/ Notify operation done and notify a next operation can be installed.\n+  ml_op_lock.notify_all();\n@@ -485,4 +430,14 @@\n-      } else {  \/\/ not a safepoint operation\n-        log_debug(vmthread)(\"Evaluating non-safepoint VM operation: %s\", _cur_vm_operation->name());\n-        evaluate_operation(_cur_vm_operation);\n-        _cur_vm_operation = NULL;\n+  while (!should_terminate()) {\n+    self_destruct_if_needed();\n+    if (_next_vm_operation != NULL) {\n+      return;\n+    }\n+    if (handshake_alot()) {\n+      {\n+        MutexUnlocker mul(VMOperation_lock);\n+        HandshakeALotClosure hal_cl;\n+        Handshake::execute(&hal_cl);\n+      }\n+      \/\/ When we unlocked above someone might have setup a new op.\n+      if (_next_vm_operation != NULL) {\n+        return;\n@@ -491,0 +446,2 @@\n+    assert(_next_vm_operation == NULL, \"Must be\");\n+    assert(_cur_vm_operation  == NULL, \"Must be\");\n@@ -492,4 +449,3 @@\n-    \/\/\n-    \/\/  Notify (potential) waiting Java thread(s)\n-    { MonitorLocker mu(VMOperationRequest_lock, Mutex::_no_safepoint_check_flag);\n-      mu.notify_all();\n+    setup_periodic_safepoint_if_needed();\n+    if (_next_vm_operation != NULL) {\n+      return;\n@@ -497,0 +453,4 @@\n+\n+    \/\/ We did find anything to execute, notify any waiter so they can install a new one.\n+    ml_op_lock.notify_all();\n+    ml_op_lock.wait(GuaranteedSafepointInterval);\n@@ -500,0 +460,21 @@\n+void VMThread::loop() {\n+  assert(_cur_vm_operation == NULL, \"no current one should be executing\");\n+\n+  SafepointSynchronize::init(_vm_thread);\n+\n+  \/\/ Need to set a calling thread for ops not passed\n+  \/\/ via the normal way.\n+  cleanup_op.set_calling_thread(_vm_thread);\n+  safepointALot_op.set_calling_thread(_vm_thread);\n+\n+  do {\n+    wait_for_operation();\n+    if (!should_terminate()) {\n+      assert(_next_vm_operation != NULL, \"Must have one\");\n+      inner_execute(_next_vm_operation);\n+    } else {\n+      break;\n+    }\n+  } while(!should_terminate());\n+}\n+\n@@ -528,11 +509,1 @@\n-  if (!t->is_VM_thread()) {\n-    SkipGCALot sgcalot(t);    \/\/ avoid re-entrant attempts to gc-a-lot\n-    \/\/ JavaThread or WatcherThread\n-    t->check_for_valid_safepoint_state();\n-\n-    \/\/ New request from Java thread, evaluate prologue\n-    if (!op->doit_prologue()) {\n-      return;   \/\/ op was cancelled\n-    }\n-\n-    \/\/ Setup VM_operations for execution\n+  if (t->is_VM_thread()) {\n@@ -540,0 +511,3 @@\n+    ((VMThread*)t)->inner_execute(op);\n+    return;\n+  }\n@@ -541,35 +515,2 @@\n-    \/\/ Get ticket number for the VM operation\n-    int ticket = t->vm_operation_ticket();\n-\n-    \/\/ Add VM operation to list of waiting threads. We are guaranteed not to block while holding the\n-    \/\/ VMOperationQueue_lock, so we can block without a safepoint check. This allows vm operation requests\n-    \/\/ to be queued up during a safepoint synchronization.\n-    {\n-      MonitorLocker ml(VMOperationQueue_lock, Mutex::_no_safepoint_check_flag);\n-      log_debug(vmthread)(\"Adding VM operation: %s\", op->name());\n-      _vm_queue->add(op);\n-      ml.notify();\n-    }\n-    {\n-      \/\/ Wait for completion of request\n-      \/\/ Note: only a JavaThread triggers the safepoint check when locking\n-      MonitorLocker ml(VMOperationRequest_lock,\n-                       t->is_Java_thread() ? Mutex::_safepoint_check_flag : Mutex::_no_safepoint_check_flag);\n-      while(t->vm_operation_completed_count() < ticket) {\n-        ml.wait();\n-      }\n-    }\n-    op->doit_epilogue();\n-  } else {\n-    \/\/ invoked by VM thread; usually nested VM operation\n-    assert(t->is_VM_thread(), \"must be a VM thread\");\n-    VM_Operation* prev_vm_operation = vm_operation();\n-    if (prev_vm_operation != NULL) {\n-      \/\/ Check the VM operation allows nested VM operation. This normally not the case, e.g., the compiler\n-      \/\/ does not allow nested scavenges or compiles.\n-      if (!prev_vm_operation->allow_nested_vm_operations()) {\n-        fatal(\"Nested VM operation %s requested by operation %s\",\n-              op->name(), vm_operation()->name());\n-      }\n-      op->set_calling_thread(prev_vm_operation->calling_thread());\n-    }\n+  \/\/ Avoid re-entrant attempts to gc-a-lot\n+  SkipGCALot sgcalot(t);\n@@ -577,1 +518,2 @@\n-    EventMark em(\"Executing %s VM operation: %s\", prev_vm_operation ? \"nested\" : \"\", op->name());\n+  \/\/ JavaThread or WatcherThread\n+  t->check_for_valid_safepoint_state();\n@@ -579,3 +521,4 @@\n-    \/\/ Release all internal handles after operation is evaluated\n-    HandleMark hm(t);\n-    _cur_vm_operation = op;\n+  \/\/ New request from Java thread, evaluate prologue\n+  if (!op->doit_prologue()) {\n+    return;   \/\/ op was cancelled\n+  }\n@@ -583,7 +526,1 @@\n-    if (op->evaluate_at_safepoint() && !SafepointSynchronize::is_at_safepoint()) {\n-      SafepointSynchronize::begin();\n-      op->evaluate();\n-      SafepointSynchronize::end();\n-    } else {\n-      op->evaluate();\n-    }\n+  op->set_calling_thread(t);\n@@ -591,2 +528,3 @@\n-    _cur_vm_operation = prev_vm_operation;\n-  }\n+  wait_until_executed(op);\n+\n+  op->doit_epilogue();\n","filename":"src\/hotspot\/share\/runtime\/vmThread.cpp","additions":186,"deletions":248,"binary":false,"changes":434,"status":"modified"},{"patch":"@@ -33,47 +33,0 @@\n-class VM_QueueHead : public VM_None {\n- public:\n-  VM_QueueHead() : VM_None(\"QueueHead\") {}\n-};\n-\n-\/\/\n-\/\/ Prioritized queue of VM operations.\n-\/\/\n-\/\/ Encapsulates both queue management and\n-\/\/ and priority policy\n-\/\/\n-class VMOperationQueue : public CHeapObj<mtInternal> {\n- private:\n-  enum Priorities {\n-     SafepointPriority, \/\/ Highest priority (operation executed at a safepoint)\n-     MediumPriority,    \/\/ Medium priority\n-     nof_priorities\n-  };\n-\n-  \/\/ We maintain a doubled linked list, with explicit count.\n-  int           _queue_length[nof_priorities];\n-  int           _queue_counter;\n-  VM_Operation* _queue       [nof_priorities];\n-\n-  static VM_QueueHead _queue_head[nof_priorities];\n-\n-  \/\/ Double-linked non-empty list insert.\n-  void insert(VM_Operation* q,VM_Operation* n);\n-  void unlink(VM_Operation* q);\n-\n-  \/\/ Basic queue manipulation\n-  bool queue_empty                (int prio);\n-  void queue_add                  (int prio, VM_Operation *op);\n-  VM_Operation* queue_remove_front(int prio);\n-  \/\/ lock-free query: may return the wrong answer but must not break\n-  bool queue_peek(int prio) { return _queue_length[prio] > 0; }\n-\n- public:\n-  VMOperationQueue();\n-\n-  \/\/ Highlevel operations. Encapsulates policy\n-  void add(VM_Operation *op);\n-  VM_Operation* remove_next();                        \/\/ Returns next or null\n-  bool peek_at_safepoint_priority() { return queue_peek(SafepointPriority); }\n-};\n-\n-\n@@ -117,1 +70,2 @@\n-  static VM_Operation* no_op_safepoint();\n+  static bool handshake_alot();\n+  static void setup_periodic_safepoint_if_needed();\n@@ -120,0 +74,2 @@\n+  void inner_execute(VM_Operation* op);\n+  void wait_for_operation();\n@@ -146,2 +102,8 @@\n-  static VM_Operation* vm_operation()             { return _cur_vm_operation; }\n-  static VM_Operation::VMOp_Type vm_op_type()     { return _cur_vm_operation->type(); }\n+  static VM_Operation* vm_operation()             {\n+    assert(Thread::current()->is_VM_thread(), \"Must be\");\n+    return _cur_vm_operation;\n+  }\n+  static VM_Operation::VMOp_Type vm_op_type()     {\n+    assert(Thread::current()->is_VM_thread(), \"Must be\");\n+    return _cur_vm_operation->type();\n+  }\n@@ -155,1 +117,3 @@\n-  static PerfCounter* perf_accumulated_vm_operation_time()               { return _perf_accumulated_vm_operation_time; }\n+  static PerfCounter* perf_accumulated_vm_operation_time() {\n+    return _perf_accumulated_vm_operation_time;\n+  }\n@@ -164,0 +128,2 @@\n+  static void wait_until_executed(VM_Operation* op);\n+\n@@ -167,1 +133,3 @@\n-  static VMOperationQueue* _vm_queue;           \/\/ Queue (w\/ policy) of VM operations\n+  static VM_Operation*     _next_vm_operation;  \/\/ Next VM operation\n+\n+  bool set_next_operation(VM_Operation *op);    \/\/ Set the _next_vm_operation if possible.\n","filename":"src\/hotspot\/share\/runtime\/vmThread.hpp","additions":20,"deletions":52,"binary":false,"changes":72,"status":"modified"}]}