{"files":[{"patch":"@@ -3910,2 +3910,2 @@\n-static void warn_on_large_pages_failure(char* req_addr, size_t bytes,\n-                                        int error) {\n+static void warn_on_reserve_special_failure(char* req_addr, size_t bytes,\n+                                            size_t page_size, int error) {\n@@ -3921,2 +3921,4 @@\n-    jio_snprintf(msg, sizeof(msg), \"Failed to reserve large pages memory req_addr: \"\n-                 PTR_FORMAT \" bytes: \" SIZE_FORMAT \" (errno = %d).\", req_addr, bytes, error);\n+    jio_snprintf(msg, sizeof(msg), \"Failed to reserve and commit memory. req_addr: \"\n+                                   PTR_FORMAT \" bytes: \" SIZE_FORMAT \" page size: \"\n+                                   SIZE_FORMAT \" (errno = %d).\",\n+                                   req_addr, bytes, page_size, error);\n@@ -3927,6 +3929,8 @@\n-char* os::Linux::reserve_memory_special_huge_tlbfs_only(size_t bytes,\n-                                                        char* req_addr,\n-                                                        bool exec) {\n-  assert(UseLargePages && UseHugeTLBFS, \"only for Huge TLBFS large pages\");\n-  assert(is_aligned(bytes, os::large_page_size()), \"Unaligned size\");\n-  assert(is_aligned(req_addr, os::large_page_size()), \"Unaligned address\");\n+char* os::Linux::commit_memory_special(size_t bytes,\n+                                       size_t page_size,\n+                                       char* req_addr,\n+                                       bool exec) {\n+  assert(UseLargePages && UseHugeTLBFS, \"Should only get here when HugeTLBFS large pages are used\");\n+  assert(is_aligned(bytes, page_size), \"Unaligned size\");\n+  assert(is_aligned(req_addr, page_size), \"Unaligned address\");\n+  assert(req_addr != NULL, \"Must have a requested address for special mappings\");\n@@ -3935,3 +3939,1 @@\n-  int flags = MAP_PRIVATE|MAP_ANONYMOUS|MAP_HUGETLB;\n-  \/\/ Ensure the correct page size flag is used when needed.\n-  flags |= hugetlbfs_page_size_flag(os::large_page_size());\n+  int flags = MAP_PRIVATE|MAP_ANONYMOUS|MAP_FIXED;\n@@ -3939,0 +3941,4 @@\n+  \/\/ For large pages additional flags are required.\n+  if (page_size > (size_t) os::vm_page_size()) {\n+    flags |= MAP_HUGETLB | hugetlbfs_page_size_flag(page_size);\n+  }\n@@ -3942,1 +3948,1 @@\n-    warn_on_large_pages_failure(req_addr, bytes, errno);\n+    warn_on_reserve_special_failure(req_addr, bytes, page_size, errno);\n@@ -3946,2 +3952,7 @@\n-  assert(is_aligned(addr, os::large_page_size()), \"Must be\");\n-\n+  log_debug(pagesize)(\"Commit special mapping: \" PTR_FORMAT \", size=\" SIZE_FORMAT \"%s, page size=\"\n+                      SIZE_FORMAT \"%s\",\n+                      p2i(addr), byte_size_in_exact_unit(bytes),\n+                      exact_unit_for_byte_size(bytes),\n+                      byte_size_in_exact_unit(page_size),\n+                      exact_unit_for_byte_size(page_size));\n+  assert(is_aligned(addr, page_size), \"Must be\");\n@@ -3951,14 +3962,5 @@\n-\/\/ Reserve memory using mmap(MAP_HUGETLB).\n-\/\/  - bytes shall be a multiple of alignment.\n-\/\/  - req_addr can be NULL. If not NULL, it must be a multiple of alignment.\n-\/\/  - alignment sets the alignment at which memory shall be allocated.\n-\/\/     It must be a multiple of allocation granularity.\n-\/\/ Returns address of memory or NULL. If req_addr was not NULL, will only return\n-\/\/  req_addr or NULL.\n-char* os::Linux::reserve_memory_special_huge_tlbfs_mixed(size_t bytes,\n-                                                         size_t alignment,\n-                                                         char* req_addr,\n-                                                         bool exec) {\n-  size_t large_page_size = os::large_page_size();\n-  assert(bytes >= large_page_size, \"Shouldn't allocate large pages for small sizes\");\n-\n+char* os::Linux::reserve_memory_special_huge_tlbfs(size_t bytes,\n+                                                   size_t alignment,\n+                                                   char* req_addr,\n+                                                   bool exec) {\n+  assert(UseLargePages && UseHugeTLBFS, \"only for Huge TLBFS large pages\");\n@@ -3966,20 +3968,4 @@\n-  assert(is_aligned(bytes, alignment), \"Must be\");\n-\n-  \/\/ First reserve - but not commit - the address range in small pages.\n-  char* const start = anon_mmap_aligned(req_addr, bytes, alignment);\n-\n-  if (start == NULL) {\n-    return NULL;\n-  }\n-\n-  assert(is_aligned(start, alignment), \"Must be\");\n-\n-  char* end = start + bytes;\n-\n-  \/\/ Find the regions of the allocated chunk that can be promoted to large pages.\n-  char* lp_start = align_up(start, large_page_size);\n-  char* lp_end   = align_down(end, large_page_size);\n-\n-  size_t lp_bytes = lp_end - lp_start;\n-\n-  assert(is_aligned(lp_bytes, large_page_size), \"Must be\");\n+  assert(is_aligned(req_addr, os::large_page_size()), \"Must be\");\n+  assert(is_aligned(alignment, os::vm_allocation_granularity()), \"Must be\");\n+  assert(is_power_of_2(os::large_page_size()), \"Must be\");\n+  assert(bytes >= os::large_page_size(), \"Shouldn't allocate large pages for small sizes\");\n@@ -3987,4 +3973,11 @@\n-  if (lp_bytes == 0) {\n-    \/\/ The mapped region doesn't even span the start and the end of a large page.\n-    \/\/ Fall back to allocate a non-special area.\n-    ::munmap(start, end - start);\n+  \/\/ We only end up here when at least 1 large page can be used.\n+  \/\/ If the size is not a multiple of the large page size, we\n+  \/\/ will mix the type of pages used, but in a decending order.\n+  \/\/ Start off by reserving a range of the given size that is\n+  \/\/ properly aligned. At this point no pages are committed. If\n+  \/\/ a requested address is given it will be used and it must be\n+  \/\/ aligned to both the large page size and the given alignment.\n+  \/\/ The larger of the two will be used.\n+  size_t required_alignment = MAX(os::large_page_size(), alignment);\n+  char* const aligned_start = anon_mmap_aligned(req_addr, bytes, required_alignment);\n+  if (aligned_start == NULL) {\n@@ -3994,3 +3987,3 @@\n-  int prot = exec ? PROT_READ|PROT_WRITE|PROT_EXEC : PROT_READ|PROT_WRITE;\n-  int flags = MAP_PRIVATE|MAP_ANONYMOUS|MAP_FIXED;\n-  void* result;\n+  \/\/ First commit using large pages.\n+  size_t large_bytes = align_down(bytes, os::large_page_size());\n+  char* large_mapping = commit_memory_special(large_bytes, os::large_page_size(), aligned_start, exec);\n@@ -3998,7 +3991,4 @@\n-  \/\/ Commit small-paged leading area.\n-  if (start != lp_start) {\n-    result = ::mmap(start, lp_start - start, prot, flags, -1, 0);\n-    if (result == MAP_FAILED) {\n-      ::munmap(lp_start, end - lp_start);\n-      return NULL;\n-    }\n+  if (bytes == large_bytes) {\n+    \/\/ The size was large page aligned so no additional work is\n+    \/\/ needed even if the commit failed.\n+    return large_mapping;\n@@ -4007,15 +3997,7 @@\n-  \/\/ Commit large-paged area.\n-  flags |= MAP_HUGETLB | hugetlbfs_page_size_flag(os::large_page_size());\n-\n-  result = ::mmap(lp_start, lp_bytes, prot, flags, -1, 0);\n-  if (result == MAP_FAILED) {\n-    warn_on_large_pages_failure(lp_start, lp_bytes, errno);\n-    \/\/ If the mmap above fails, the large pages region will be unmapped and we\n-    \/\/ have regions before and after with small pages. Release these regions.\n-    \/\/\n-    \/\/ |  mapped  |  unmapped  |  mapped  |\n-    \/\/ ^          ^            ^          ^\n-    \/\/ start      lp_start     lp_end     end\n-    \/\/\n-    ::munmap(start, lp_start - start);\n-    ::munmap(lp_end, end - lp_end);\n+  \/\/ The requested size requires some small pages as well.\n+  char* small_start = aligned_start + large_bytes;\n+  size_t small_size = bytes - large_bytes;\n+  if (large_mapping == NULL) {\n+    \/\/ Failed to commit large pages, so we need to unmap the\n+    \/\/ reminder of the orinal reservation.\n+    ::munmap(small_start, small_size);\n@@ -4025,28 +4007,6 @@\n-  \/\/ Commit small-paged trailing area.\n-  if (lp_end != end) {\n-    result = ::mmap(lp_end, end - lp_end, prot,\n-                    MAP_PRIVATE|MAP_ANONYMOUS|MAP_FIXED,\n-                    -1, 0);\n-    if (result == MAP_FAILED) {\n-      ::munmap(start, lp_end - start);\n-      return NULL;\n-    }\n-  }\n-\n-  return start;\n-}\n-\n-char* os::Linux::reserve_memory_special_huge_tlbfs(size_t bytes,\n-                                                   size_t alignment,\n-                                                   char* req_addr,\n-                                                   bool exec) {\n-  assert(UseLargePages && UseHugeTLBFS, \"only for Huge TLBFS large pages\");\n-  assert(is_aligned(req_addr, alignment), \"Must be\");\n-  assert(is_aligned(alignment, os::vm_allocation_granularity()), \"Must be\");\n-  assert(is_power_of_2(os::large_page_size()), \"Must be\");\n-  assert(bytes >= os::large_page_size(), \"Shouldn't allocate large pages for small sizes\");\n-\n-  if (is_aligned(bytes, os::large_page_size()) && alignment <= os::large_page_size()) {\n-    return reserve_memory_special_huge_tlbfs_only(bytes, req_addr, exec);\n-  } else {\n-    return reserve_memory_special_huge_tlbfs_mixed(bytes, alignment, req_addr, exec);\n+  \/\/ Commit the remaining bytes using small pages.\n+  void* small_mapping = commit_memory_special(small_size, os::vm_page_size(), small_start, exec);\n+  if (small_mapping == NULL) {\n+    \/\/ Failed to commit the remaining size, need to unmap large.\n+    ::munmap(large_mapping, large_bytes);\n+    return NULL;\n@@ -4054,0 +4014,1 @@\n+  return large_mapping;\n","filename":"src\/hotspot\/os\/linux\/os_linux.cpp","additions":68,"deletions":107,"binary":false,"changes":175,"status":"modified"},{"patch":"@@ -94,2 +94,1 @@\n-  static char* reserve_memory_special_huge_tlbfs_only(size_t bytes, char* req_addr, bool exec);\n-  static char* reserve_memory_special_huge_tlbfs_mixed(size_t bytes, size_t alignment, char* req_addr, bool exec);\n+  static char* commit_memory_special(size_t bytes, size_t page_size, char* req_addr, bool exec);\n","filename":"src\/hotspot\/os\/linux\/os_linux.hpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -50,5 +50,2 @@\n-    static char* reserve_memory_special_huge_tlbfs_only(size_t bytes, char* req_addr, bool exec) {\n-      return os::Linux::reserve_memory_special_huge_tlbfs_only(bytes, req_addr, exec);\n-    }\n-    static char* reserve_memory_special_huge_tlbfs_mixed(size_t bytes, size_t alignment, char* req_addr, bool exec) {\n-      return os::Linux::reserve_memory_special_huge_tlbfs_mixed(bytes, alignment, req_addr, exec);\n+    static char* reserve_memory_special_huge_tlbfs(size_t bytes, size_t alignment, char* req_addr, bool exec) {\n+      return os::Linux::reserve_memory_special_huge_tlbfs(bytes, alignment, req_addr, exec);\n@@ -99,1 +96,1 @@\n-TEST_VM(os_linux, reserve_memory_special_huge_tlbfs_only) {\n+TEST_VM(os_linux, reserve_memory_special_huge_tlbfs_size_aligned) {\n@@ -106,1 +103,1 @@\n-    char* addr = HugeTlbfsMemory::reserve_memory_special_huge_tlbfs_only(size, NULL, false);\n+    char* addr = HugeTlbfsMemory::reserve_memory_special_huge_tlbfs(size, lp, NULL, false);\n@@ -115,1 +112,1 @@\n-TEST_VM(os_linux, reserve_memory_special_huge_tlbfs_mixed_without_addr) {\n+TEST_VM(os_linux, reserve_memory_special_huge_tlbfs_size_not_aligned_without_addr) {\n@@ -132,1 +129,1 @@\n-      char* p = HugeTlbfsMemory::reserve_memory_special_huge_tlbfs_mixed(size, alignment, NULL, false);\n+      char* p = HugeTlbfsMemory::reserve_memory_special_huge_tlbfs(size, alignment, NULL, false);\n@@ -142,1 +139,1 @@\n-TEST_VM(os_linux, reserve_memory_special_huge_tlbfs_mixed_with_good_req_addr) {\n+TEST_VM(os_linux, reserve_memory_special_huge_tlbfs_size_not_aligned_with_good_req_addr) {\n@@ -170,2 +167,3 @@\n-      char* const req_addr = align_up(mapping, alignment);\n-      char* p = HugeTlbfsMemory::reserve_memory_special_huge_tlbfs_mixed(size, alignment, req_addr, false);\n+      \/\/ req_addr must be at least large page aligned.\n+      char* const req_addr = align_up(mapping, MAX2(alignment, lp));\n+      char* p = HugeTlbfsMemory::reserve_memory_special_huge_tlbfs(size, alignment, req_addr, false);\n@@ -182,1 +180,1 @@\n-TEST_VM(os_linux, reserve_memory_special_huge_tlbfs_mixed_with_bad_req_addr) {\n+TEST_VM(os_linux, reserve_memory_special_huge_tlbfs_size_not_aligned_with_bad_req_addr) {\n@@ -219,2 +217,3 @@\n-      char* const req_addr = align_up(mapping, alignment);\n-      char* p = HugeTlbfsMemory::reserve_memory_special_huge_tlbfs_mixed(size, alignment, req_addr, false);\n+      \/\/ req_addr must be at least large page aligned.\n+      char* const req_addr = align_up(mapping, MAX2(alignment, lp));\n+      char* p = HugeTlbfsMemory::reserve_memory_special_huge_tlbfs(size, alignment, req_addr, false);\n@@ -257,1 +256,1 @@\n-  static void test_reserve_memory_special_huge_tlbfs_only(size_t size) {\n+  static void test_reserve_memory_special_huge_tlbfs_size_aligned(size_t size, size_t alignment) {\n@@ -262,1 +261,1 @@\n-    char* addr = os::Linux::reserve_memory_special_huge_tlbfs_only(size, NULL, false);\n+    char* addr = os::Linux::reserve_memory_special_huge_tlbfs(size, alignment, NULL, false);\n@@ -271,1 +270,1 @@\n-  static void test_reserve_memory_special_huge_tlbfs_only() {\n+  static void test_reserve_memory_special_huge_tlbfs_size_aligned() {\n@@ -279,1 +278,1 @@\n-      test_reserve_memory_special_huge_tlbfs_only(size);\n+      test_reserve_memory_special_huge_tlbfs_size_aligned(size, lp);\n@@ -283,1 +282,1 @@\n-  static void test_reserve_memory_special_huge_tlbfs_mixed() {\n+  static void test_reserve_memory_special_huge_tlbfs_size_not_aligned() {\n@@ -323,1 +322,1 @@\n-        char* p = os::Linux::reserve_memory_special_huge_tlbfs_mixed(size, alignment, NULL, false);\n+        char* p = os::Linux::reserve_memory_special_huge_tlbfs(size, alignment, NULL, false);\n@@ -336,2 +335,3 @@\n-        char* const req_addr = align_up(mapping1, alignment);\n-        char* p = os::Linux::reserve_memory_special_huge_tlbfs_mixed(size, alignment, req_addr, false);\n+        \/\/ req_addr must be at least large page aligned.\n+        char* const req_addr = align_up(mapping1, MAX2(alignment, lp));\n+        char* p = os::Linux::reserve_memory_special_huge_tlbfs(size, alignment, req_addr, false);\n@@ -350,2 +350,3 @@\n-        char* const req_addr = align_up(mapping2, alignment);\n-        char* p = os::Linux::reserve_memory_special_huge_tlbfs_mixed(size, alignment, req_addr, false);\n+        \/\/ req_addr must be at least large page aligned.\n+        char* const req_addr = align_up(mapping2, MAX2(alignment, lp));\n+        char* p = os::Linux::reserve_memory_special_huge_tlbfs(size, alignment, req_addr, false);\n@@ -367,2 +368,2 @@\n-    test_reserve_memory_special_huge_tlbfs_only();\n-    test_reserve_memory_special_huge_tlbfs_mixed();\n+    test_reserve_memory_special_huge_tlbfs_size_aligned();\n+    test_reserve_memory_special_huge_tlbfs_size_not_aligned();\n","filename":"test\/hotspot\/gtest\/runtime\/test_os_linux.cpp","additions":28,"deletions":27,"binary":false,"changes":55,"status":"modified"}]}