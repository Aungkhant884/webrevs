{"files":[{"patch":"@@ -448,1 +448,1 @@\n-  \/\/     R0: the new value stored in dest\n+  \/\/     R0: the new stored in dest\n@@ -489,26 +489,0 @@\n-  \/\/ Implementation of atomic_add(jlong add_value, volatile jlong* dest)\n-  \/\/ used by Atomic::add(volatile jlong* dest, jlong add_value)\n-  \/\/\n-  \/\/ Arguments :\n-  \/\/\n-  \/\/      add_value:      R1 (High), R0 (Low)\n-  \/\/      dest:           R2\n-  \/\/\n-  \/\/ Results:\n-  \/\/\n-  \/\/     R0:R1: the new value stored in dest\n-  \/\/\n-  \/\/ Overwrites:\n-  \/\/\n-  \/\/     R1, R2, R3, R4\n-  \/\/\n-  address generate_atomic_add_long() {\n-    address start;\n-\n-    StubCodeMark mark(this, \"StubRoutines\", \"atomic_add_long\");\n-    Unimplemented();\n-    __ bx(LR);\n-\n-    return start;\n-  }\n-\n@@ -3106,8 +3080,0 @@\n-  static void should_not_call() {\n-    report_should_not_call(__FILE__, __LINE__);\n-  }\n-\n-  static address ShouldNotCallThisStub() {\n-    return (address) should_not_call;\n-  }\n-\n@@ -3139,1 +3105,0 @@\n-    StubRoutines::_atomic_add_long_entry = ShouldNotCallThisStub();\n","filename":"src\/hotspot\/cpu\/arm\/stubGenerator_arm.cpp","additions":1,"deletions":36,"binary":false,"changes":37,"status":"modified"},{"patch":"@@ -197,1 +197,0 @@\n-    StubRoutines::_atomic_add_long_entry     = ShouldNotCallThisStub();\n","filename":"src\/hotspot\/cpu\/zero\/stubGenerator_zero.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -97,1 +97,0 @@\n-\n@@ -143,1 +142,0 @@\n-  int64_t _Atomic_add_long(int64_t, volatile int64_t*);\n@@ -148,9 +146,0 @@\n-template<>\n-template<typename D, typename I>\n-inline D Atomic::PlatformAdd<8>::fetch_then_add(D volatile* dest, I add_value,\n-                                                atomic_memory_order order) const {\n-  STATIC_ASSERT(8 == sizeof(I));\n-  STATIC_ASSERT(8 == sizeof(D));\n-  return add_using_helper<int64_t>(_Atomic_add_long, dest, add_value);\n-}\n-\n","filename":"src\/hotspot\/os_cpu\/bsd_x86\/atomic_bsd_x86.hpp","additions":0,"deletions":11,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n-# Copyright (c) 2004, 2023, Oracle and\/or its affiliates. All rights reserved.\n+# Copyright (c) 2004, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -50,1 +50,0 @@\n-        .globl SYMBOL(_Atomic_add_long)\n@@ -529,24 +528,0 @@\n-        # Support for jlong Atomic::add(volatile jlong* dest,\n-        #                               jlong addend)\n-        #\n-        .p2align 4,,15\n-        ELF_TYPE(_Atomic_add_long,@function)\n-_Atomic_add_long:\n-                                   #  8(%esp) : return PC\n-        pushl    %ebx              #  4(%esp) : old %ebx\n-        pushl    %edi              #  0(%esp) : old %edi\n-        movl     12(%esp), %ebx    # 12(%esp) : add_value (low)\n-        movl     16(%esp), %ecx    # 16(%esp) : add_value (high)\n-        movl     20(%esp), %edi    # 20(%esp) : dest\n-\tmovl     0(%edi), %eax     # dest (low)\n-        movl     4(%edi), %edx     # dest (high)\n-        addl     %eax, %ebx        # result (low)\n-        adcl     %edx, %ecx        # result (high)\n-1:      lock\n-        cmpxchg8b (%edi)\n-        jne      1b\n-        popl     %edi\n-        popl     %ebx\n-        ret\n-\n-\n","filename":"src\/hotspot\/os_cpu\/bsd_x86\/bsd_x86_32.S","additions":1,"deletions":26,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -36,1 +36,0 @@\n-  typedef int64_t (*atomic_add_long_func_t)(int64_t add_value, volatile int64_t *dest);\n@@ -39,3 +38,3 @@\n-  typedef void    (*store_long_func_t)(int64_t, volatile int64_t*);\n-  typedef int32_t (*atomic_add_func_t)(int32_t add_value, volatile int32_t *dest);\n-  typedef int32_t (*atomic_xchg_func_t)(int32_t exchange_value, volatile int32_t *dest);\n+  typedef void (*store_long_func_t)(int64_t, volatile int64_t*);\n+  typedef int32_t  (*atomic_add_func_t)(int32_t add_value, volatile int32_t *dest);\n+  typedef int32_t  (*atomic_xchg_func_t)(int32_t exchange_value, volatile int32_t *dest);\n@@ -44,7 +43,6 @@\n-  static atomic_add_long_func_t _add_long_func;\n-  static cmpxchg_long_func_t    _cmpxchg_long_func;\n-  static load_long_func_t       _load_long_func;\n-  static store_long_func_t      _store_long_func;\n-  static atomic_add_func_t      _add_func;\n-  static atomic_xchg_func_t     _xchg_func;\n-  static cmpxchg_func_t         _cmpxchg_func;\n+  static cmpxchg_long_func_t  _cmpxchg_long_func;\n+  static load_long_func_t     _load_long_func;\n+  static store_long_func_t    _store_long_func;\n+  static atomic_add_func_t    _add_func;\n+  static atomic_xchg_func_t   _xchg_func;\n+  static cmpxchg_func_t       _cmpxchg_func;\n@@ -52,1 +50,0 @@\n-  static int64_t add_long_bootstrap(int64_t add_value, volatile int64_t *dest);\n@@ -54,0 +51,1 @@\n+\n@@ -55,6 +53,10 @@\n-  static void    store_long_bootstrap(int64_t, volatile int64_t*);\n-  static int32_t add_bootstrap(int32_t add_value, volatile int32_t *dest);\n-  static int32_t xchg_bootstrap(int32_t exchange_value, volatile int32_t *dest);\n-  static int32_t cmpxchg_bootstrap(int32_t compare_value,\n-                                   int32_t exchange_value,\n-                                   volatile int32_t *dest);\n+\n+  static void store_long_bootstrap(int64_t, volatile int64_t*);\n+\n+  static int32_t  add_bootstrap(int32_t add_value, volatile int32_t *dest);\n+\n+  static int32_t  xchg_bootstrap(int32_t exchange_value, volatile int32_t *dest);\n+\n+  static int32_t  cmpxchg_bootstrap(int32_t compare_value,\n+                                    int32_t exchange_value,\n+                                    volatile int32_t *dest);\n@@ -119,17 +121,0 @@\n-template<>\n-template<typename D, typename I>\n-inline D Atomic::PlatformAdd<8>::add_then_fetch(D volatile* dest, I add_value,\n-                                                atomic_memory_order order) const {\n-  STATIC_ASSERT(8 == sizeof(I));\n-  STATIC_ASSERT(8 == sizeof(D));\n-\n-#if 0\n-  \/\/ TBD assembly stub\n-  return add_using_helper<int64_t>(ARMAtomicFuncs::_add_long_func, dest, add_value);\n-#else\n-  D res = __atomic_add_fetch(dest, add_value, __ATOMIC_RELEASE);\n-  FULL_MEM_BARRIER;\n-  return res;\n-#endif\n-}\n-\n","filename":"src\/hotspot\/os_cpu\/linux_arm\/atomic_linux_arm.hpp","additions":20,"deletions":35,"binary":false,"changes":55,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2008, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2008, 2020, Oracle and\/or its affiliates. All rights reserved.\n@@ -96,1 +96,0 @@\n-#define FULL_MEM_BARRIER dmb_sy()\n","filename":"src\/hotspot\/os_cpu\/linux_arm\/orderAccess_linux_arm.hpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -506,20 +506,6 @@\n-ARMAtomicFuncs::atomic_add_long_func_t ARMAtomicFuncs::_add_long_func     = ARMAtomicFuncs::add_long_bootstrap;\n-ARMAtomicFuncs::cmpxchg_long_func_t    ARMAtomicFuncs::_cmpxchg_long_func = ARMAtomicFuncs::cmpxchg_long_bootstrap;\n-ARMAtomicFuncs::load_long_func_t       ARMAtomicFuncs::_load_long_func    = ARMAtomicFuncs::load_long_bootstrap;\n-ARMAtomicFuncs::store_long_func_t      ARMAtomicFuncs::_store_long_func   = ARMAtomicFuncs::store_long_bootstrap;\n-ARMAtomicFuncs::atomic_add_func_t      ARMAtomicFuncs::_add_func          = ARMAtomicFuncs::add_bootstrap;\n-ARMAtomicFuncs::atomic_xchg_func_t     ARMAtomicFuncs::_xchg_func         = ARMAtomicFuncs::xchg_bootstrap;\n-ARMAtomicFuncs::cmpxchg_func_t         ARMAtomicFuncs::_cmpxchg_func      = ARMAtomicFuncs::cmpxchg_bootstrap;\n-\n-int64_t ARMAtomicFuncs::add_long_bootstrap(int64_t add_value, volatile int64_t *dest) {\n-  atomic_add_long_func_t func = CAST_TO_FN_PTR(atomic_add_long_func_t,\n-                                               StubRoutines::atomic_add_long_entry());\n-  if (func != nullptr) {\n-    _add_long_func = func;\n-    return (*func)(add_value, dest);\n-  }\n-\n-  int64_t old_value = *dest;\n-  *dest = old_value + add_value;\n-  return (old_value + add_value);\n-}\n+ARMAtomicFuncs::cmpxchg_long_func_t ARMAtomicFuncs::_cmpxchg_long_func = ARMAtomicFuncs::cmpxchg_long_bootstrap;\n+ARMAtomicFuncs::load_long_func_t    ARMAtomicFuncs::_load_long_func    = ARMAtomicFuncs::load_long_bootstrap;\n+ARMAtomicFuncs::store_long_func_t   ARMAtomicFuncs::_store_long_func   = ARMAtomicFuncs::store_long_bootstrap;\n+ARMAtomicFuncs::atomic_add_func_t   ARMAtomicFuncs::_add_func          = ARMAtomicFuncs::add_bootstrap;\n+ARMAtomicFuncs::atomic_xchg_func_t  ARMAtomicFuncs::_xchg_func         = ARMAtomicFuncs::xchg_bootstrap;\n+ARMAtomicFuncs::cmpxchg_func_t      ARMAtomicFuncs::_cmpxchg_func      = ARMAtomicFuncs::cmpxchg_bootstrap;\n","filename":"src\/hotspot\/os_cpu\/linux_arm\/os_linux_arm.cpp","additions":6,"deletions":20,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -141,2 +141,1 @@\n-  \/\/ defined in linux_x86.S\n-  int64_t _Atomic_add_long(int64_t, volatile int64_t*);\n+  \/\/ defined in linux_x86.s\n@@ -147,9 +146,0 @@\n-template<>\n-template<typename D, typename I>\n-inline D Atomic::PlatformAdd<8>::fetch_then_add(D volatile* dest, I add_value,\n-                                                 atomic_memory_order order) const {\n-  STATIC_ASSERT(8 == sizeof(I));\n-  STATIC_ASSERT(8 == sizeof(D));\n-  return add_using_helper<int64_t>(_Atomic_add_long, dest, add_value);\n-}\n-\n","filename":"src\/hotspot\/os_cpu\/linux_x86\/atomic_linux_x86.hpp","additions":1,"deletions":11,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n-# Copyright (c) 2004, 2023, Oracle and\/or its affiliates. All rights reserved.\n+# Copyright (c) 2004, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -39,1 +39,0 @@\n-        .globl _Atomic_add_long\n@@ -59,1 +58,1 @@\n-        .type    _Copy_arrayof_conjoint_bytes,@function\n+\t.type    _Copy_arrayof_conjoint_bytes,@function\n@@ -147,1 +146,1 @@\n-        .type    _Copy_conjoint_jshorts_atomic,@function\n+\t.type    _Copy_conjoint_jshorts_atomic,@function\n@@ -234,1 +233,1 @@\n-        .type    _Copy_arrayof_conjoint_jshorts,@function\n+\t.type    _Copy_arrayof_conjoint_jshorts,@function\n@@ -311,2 +310,2 @@\n-        .type    _Copy_conjoint_jints_atomic,@function\n-        .type    _Copy_arrayof_conjoint_jints,@function\n+\t.type    _Copy_conjoint_jints_atomic,@function\n+\t.type    _Copy_arrayof_conjoint_jints,@function\n@@ -388,1 +387,1 @@\n-        .type    _Copy_conjoint_jlongs_atomic,@function\n+\t.type    _Copy_conjoint_jlongs_atomic,@function\n@@ -417,1 +416,1 @@\n-        .type    _mmx_Copy_arrayof_conjoint_jshorts,@function\n+\t.type    _mmx_Copy_arrayof_conjoint_jshorts,@function\n@@ -510,23 +509,0 @@\n-        # Support for jlong Atomic::add(volatile jlong* dest,\n-        #                               jlong addend)\n-        #\n-        .p2align 4,,15\n-        .type    _Atomic_add_long,@function\n-_Atomic_add_long:\n-                                   #  8(%esp) : return PC\n-        pushl    %ebx              #  4(%esp) : old %ebx\n-        pushl    %edi              #  0(%esp) : old %edi\n-        movl     12(%esp), %ebx    # 12(%esp) : add_value (low)\n-        movl     16(%esp), %ecx    # 16(%esp) : add_value (high)\n-        movl     20(%esp), %edi    # 20(%esp) : dest\n-\tmovl     0(%edi), %eax     # dest (low)\n-        movl     4(%edi), %edx     # dest (high)\n-        addl     %eax, %ebx        # result (low)\n-        adcl     %edx, %ecx        # result (high)\n-1:      lock cmpxchg8b (%edi)\n-        jne      1b\n-        popl     %edi\n-        popl     %ebx\n-        ret\n-\n-\n@@ -538,1 +514,1 @@\n-        .type    _Atomic_cmpxchg_long,@function\n+\t.type    _Atomic_cmpxchg_long,@function\n@@ -557,1 +533,1 @@\n-        .type    _Atomic_move_long,@function\n+\t.type    _Atomic_move_long,@function\n","filename":"src\/hotspot\/os_cpu\/linux_x86\/linux_x86_32.S","additions":10,"deletions":34,"binary":false,"changes":44,"status":"modified"},{"patch":"@@ -310,2 +310,2 @@\n-  \/\/ and 8 bytes are required.  The class must be default constructable,\n-  \/\/ with these requirements:\n+  \/\/ bytes and (if different) pointer size bytes are required.  The\n+  \/\/ class must be default constructable, with these requirements:\n","filename":"src\/hotspot\/share\/runtime\/atomic.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -74,1 +74,0 @@\n-address StubRoutines::_atomic_add_long_entry                    = nullptr;\n","filename":"src\/hotspot\/share\/runtime\/stubRoutines.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -149,1 +149,0 @@\n-  static address _atomic_add_long_entry;\n@@ -325,1 +324,0 @@\n-  static address atomic_add_long_entry()                   { return _atomic_add_long_entry; }\n","filename":"src\/hotspot\/share\/runtime\/stubRoutines.hpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1,56 +0,0 @@\n-\n-\/*\n- * Copyright (c) 2014, 2019, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#ifndef SHARE_SERVICES_ALLOCATIONSITE_HPP\n-#define SHARE_SERVICES_ALLOCATIONSITE_HPP\n-\n-#include \"memory\/allocation.hpp\"\n-#include \"utilities\/nativeCallStack.hpp\"\n-\n-\/\/ Allocation site represents a code path that makes a memory\n-\/\/ allocation\n-class AllocationSite {\n- private:\n-  const NativeCallStack  _call_stack;\n-  const MEMFLAGS         _flag;\n- public:\n-  AllocationSite(const NativeCallStack& stack, MEMFLAGS flag) : _call_stack(stack), _flag(flag) { }\n-\n-  bool equals(const NativeCallStack& stack) const {\n-    return _call_stack.equals(stack);\n-  }\n-\n-  bool equals(const AllocationSite& other) const {\n-    return other.equals(_call_stack);\n-  }\n-\n-  const NativeCallStack* call_stack() const {\n-    return &_call_stack;\n-  }\n-\n-  MEMFLAGS flag() const { return _flag; }\n-};\n-\n-#endif \/\/ SHARE_SERVICES_ALLOCATIONSITE_HPP\n","filename":"src\/hotspot\/share\/services\/#allocationSite.hpp#","additions":0,"deletions":56,"binary":false,"changes":56,"status":"deleted"},{"patch":"@@ -1,1 +0,0 @@\n-hohensee@dev-dsk-hohensee-1e-ccc30ec5.us-east-1.amazon.com.87679:1683141379\n\\ No newline at end of file\n","filename":"src\/hotspot\/share\/services\/.#allocationSite.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"deleted"},{"patch":"@@ -65,1 +65,1 @@\n-  \/\/ Number of heap bytes allocated by termianted threads.\n+  \/\/ Number of heap bytes allocated by terminated threads.\n@@ -111,1 +111,2 @@\n-    Atomic::add(&_exited_allocated_bytes, size, memory_order_relaxed);\n+    \/\/ No need for atomicity, method is called under the Threads_lock\n+    _exited_allocated_bytes += size;\n","filename":"src\/hotspot\/share\/services\/threadService.hpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"}]}