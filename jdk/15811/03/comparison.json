{"files":[{"patch":"@@ -176,1 +176,1 @@\n-void CodeBlob::flush() {\n+void CodeBlob::flush(bool do_unregister_nmethod) {\n","filename":"src\/hotspot\/share\/code\/codeBlob.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -144,1 +144,1 @@\n-  virtual void flush();\n+  virtual void flush(bool do_unregister_nmethod = true);\n","filename":"src\/hotspot\/share\/code\/codeBlob.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -988,1 +988,1 @@\n-void CodeCache::flush_unlinked_nmethods() {\n+void CodeCache::flush_unlinked_nmethods(bool do_unregister_nmethod) {\n@@ -995,1 +995,1 @@\n-    nm->flush();\n+    nm->flush(do_unregister_nmethod);\n","filename":"src\/hotspot\/share\/code\/codeCache.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -216,1 +216,4 @@\n-  static void flush_unlinked_nmethods();\n+  \/\/ do_unregister_nmethod controls whether CollectedHeap::unregister_nmethod is\n+  \/\/ called for every unlinked nmethod. If false, the caller is responsible to\n+  \/\/ do an equivalent operation before calling this.\n+  static void flush_unlinked_nmethods(bool do_unregister_nmethod = true);\n","filename":"src\/hotspot\/share\/code\/codeCache.hpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -177,1 +177,3 @@\n-  virtual void flush() = 0;\n+  \/\/ The parameter controls whether CollectedHeap::unregister_nmethod() is called\n+  \/\/ during flush. If not, the caller is responsible for doing this.\n+  virtual void flush(bool do_unregister_nmethod = true) = 0;\n","filename":"src\/hotspot\/share\/code\/compiledMethod.hpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1445,1 +1445,1 @@\n-void nmethod::flush() {\n+void nmethod::flush(bool do_unregister_nmethod) {\n@@ -1465,1 +1465,3 @@\n-  Universe::heap()->unregister_nmethod(this);\n+  if (do_unregister_nmethod) {\n+    Universe::heap()->unregister_nmethod(this);\n+  }\n@@ -1468,1 +1470,1 @@\n-  CodeBlob::flush();\n+  CodeBlob::flush(do_unregister_nmethod);\n","filename":"src\/hotspot\/share\/code\/nmethod.cpp","additions":5,"deletions":3,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -524,2 +524,1 @@\n-  \/\/ Deallocate this nmethod - called by the GC\n-  void flush();\n+  void flush(bool do_unregister_nmethod);\n","filename":"src\/hotspot\/share\/code\/nmethod.hpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"memory\/allocation.hpp\"\n@@ -32,0 +33,3 @@\n+#include \"runtime\/atomic.hpp\"\n+#include \"utilities\/concurrentHashTable.inline.hpp\"\n+#include \"utilities\/concurrentHashTableTasks.inline.hpp\"\n@@ -33,5 +37,164 @@\n-void G1CodeRootSet::add(nmethod* nm) {\n-  assert(_is_iterating == false, \"should not mutate while iterating the table\");\n-  bool added = false;\n-  if (_table == nullptr) {\n-    _table = new (mtGC) Table(SmallSize, LargeSize);\n+struct G1CodeRootSetHashTableValue {\n+  nmethod* _nmethod;\n+\n+  G1CodeRootSetHashTableValue(nmethod* nmethod) : _nmethod(nmethod) { }\n+};\n+\n+class G1CodeRootSetHashTableConfig : public StackObj {\n+public:\n+  using Value = G1CodeRootSetHashTableValue;\n+\n+  static uintx get_hash(Value const& value, bool* is_dead);\n+\n+  static void* allocate_node(void* context, size_t size, Value const& value) {\n+    return AllocateHeap(size, mtGC);\n+  }\n+\n+  static void free_node(void* context, void* memory, Value const& value) {\n+    FreeHeap(memory);\n+  }\n+};\n+\n+\/\/ Storage container for the code root set.\n+class G1CodeRootSetHashTable : public CHeapObj<mtGC> {\n+  using HashTable = ConcurrentHashTable<G1CodeRootSetHashTableConfig, mtGC>;\n+  using HashTableScanTask = HashTable::ScanTask;\n+\n+  \/\/ Default (log2) number of buckets; small since typically we do not expect many\n+  \/\/ entries.\n+  static const size_t Log2DefaultNumBuckets = 2;\n+  static const uint BucketClaimSize = 16;\n+\n+  HashTable _table;\n+  HashTableScanTask _table_scanner;\n+\n+  size_t volatile _num_entries;\n+\n+  bool is_empty() const { return number_of_entries() == 0; }\n+\n+  class HashTableLookUp : public StackObj {\n+    nmethod* _nmethod;\n+\n+  public:\n+    explicit HashTableLookUp(nmethod* nmethod) : _nmethod(nmethod) { }\n+    uintx get_hash() const;\n+    bool equals(G1CodeRootSetHashTableValue* value);\n+    bool is_dead(G1CodeRootSetHashTableValue* value) const { return false; }\n+  };\n+\n+  class HashTableIgnore : public StackObj {\n+  public:\n+    HashTableIgnore() { }\n+    void operator()(G1CodeRootSetHashTableValue* value) { \/* do nothing *\/ }\n+  };\n+\n+public:\n+  G1CodeRootSetHashTable() :\n+    _table(Log2DefaultNumBuckets,\n+           HashTable::DEFAULT_MAX_SIZE_LOG2),\n+    _table_scanner(&_table, BucketClaimSize), _num_entries(0) {\n+    clear();\n+  }\n+\n+  \/\/ Robert Jenkins 1996 & Thomas Wang 1997\n+  \/\/ http:\/\/web.archive.org\/web\/20071223173210\/http:\/\/www.concentric.net\/~Ttwang\/tech\/inthash.htm\n+  static uint32_t hash(uint32_t key) {\n+    key = ~key + (key << 15);\n+    key = key ^ (key >> 12);\n+    key = key + (key << 2);\n+    key = key ^ (key >> 4);\n+    key = key * 2057;\n+    key = key ^ (key >> 16);\n+    return key;\n+  }\n+\n+  static uintx get_hash(nmethod* nmethod) {\n+    uintptr_t value = (uintptr_t)nmethod;\n+    \/\/ The CHT only uses the bits smaller than HashTable::DEFAULT_MAX_SIZE_LOG2, so\n+    \/\/ try to increase the randomness by incorporating the upper bits of the\n+    \/\/ address too.\n+    STATIC_ASSERT(HashTable::DEFAULT_MAX_SIZE_LOG2 <= sizeof(uint32_t) * BitsPerByte);\n+#ifdef _LP64\n+    return hash((uint32_t)value ^ (uint32_t(value >> 32)));\n+#else\n+    return hash((uint32_t)value);\n+#endif\n+  }\n+\n+  void insert(nmethod* method) {\n+    HashTableLookUp lookup(method);\n+    G1CodeRootSetHashTableValue value(method);\n+    bool grow_hint = false;\n+    bool inserted = _table.insert(Thread::current(), lookup, value, &grow_hint);\n+    if (inserted) {\n+      Atomic::inc(&_num_entries);\n+    }\n+    if (grow_hint) {\n+      _table.grow(Thread::current());\n+    }\n+  }\n+\n+  bool remove(nmethod* method) {\n+    HashTableLookUp lookup(method);\n+    bool removed = _table.remove(Thread::current(), lookup);\n+    if (removed) {\n+      Atomic::dec(&_num_entries);\n+    }\n+    return removed;\n+  }\n+\n+  bool contains(nmethod* method) {\n+    HashTableLookUp lookup(method);\n+    HashTableIgnore ignore;\n+    return _table.get(Thread::current(), lookup, ignore);\n+  }\n+\n+  void clear() {\n+    _table.unsafe_reset();\n+    Atomic::store(&_num_entries, (size_t)0);\n+  }\n+\n+  void iterate_at_safepoint(CodeBlobClosure* blk) {\n+    assert_at_safepoint();\n+    \/\/ A lot of code root sets are typically empty.\n+    if (is_empty()) {\n+      return;\n+    }\n+\n+    auto do_value =\n+      [&] (G1CodeRootSetHashTableValue* value) {\n+        blk->do_code_blob(value->_nmethod);\n+        return true;\n+      };\n+    _table_scanner.do_safepoint_scan(do_value);\n+  }\n+\n+  \/\/ Removes entries as indicated by the given EVAL closure.\n+  template <class EVAL>\n+  void clean(EVAL& eval) {\n+    \/\/ A lot of code root sets are typically empty.\n+    if (is_empty()) {\n+      return;\n+    }\n+\n+    size_t num_deleted = 0;\n+    auto do_delete =\n+      [&] (G1CodeRootSetHashTableValue* value) {\n+        num_deleted++;\n+      };\n+    bool succeeded = _table.try_bulk_delete(Thread::current(), eval, do_delete);\n+    guarantee(succeeded, \"unable to clean table\");\n+\n+    if (num_deleted != 0) {\n+      size_t current_size = Atomic::sub(&_num_entries, num_deleted);\n+      shrink_to_match(current_size);\n+    }\n+  }\n+\n+  \/\/ Removes dead\/unlinked entries.\n+  void remove_dead_entries() {\n+    auto delete_check =\n+      [&] (G1CodeRootSetHashTableValue* value) {\n+        return value->_nmethod->unlinked_next() != nullptr;\n+      };\n+    clean(delete_check);\n@@ -39,3 +202,53 @@\n-  added = _table->put(nm, nm);\n-  if (added && _table->table_size() == SmallSize && length() == Threshold) {\n-    _table->resize(LargeSize);\n+\n+  \/\/ Calculate the log2 of the table size we want to shrink to.\n+  size_t log2_target_shrink_size(size_t current_size) const {\n+    \/\/ A table with the new size should be at most filled by this factor. Otherwise\n+    \/\/ we would grow again quickly.\n+    const float WantedLoadFactor = 0.5;\n+    size_t min_expected_size = checked_cast<size_t>(ceil(current_size \/ WantedLoadFactor));\n+\n+    size_t result = Log2DefaultNumBuckets;\n+    if (min_expected_size != 0) {\n+      size_t log2_bound = checked_cast<size_t>(log2i_exact(round_up_power_of_2(min_expected_size)));\n+      result = clamp(log2_bound, Log2DefaultNumBuckets, HashTable::DEFAULT_MAX_SIZE_LOG2);\n+    }\n+    return result;\n+  }\n+\n+  \/\/ Shrink to keep table size appropriate to the given number of entries.\n+  void shrink_to_match(size_t current_size) {\n+    size_t prev_log2size = _table.get_size_log2(Thread::current());\n+    size_t new_log2_table_size = log2_target_shrink_size(current_size);\n+    if (new_log2_table_size < prev_log2size) {\n+      _table.shrink(Thread::current(), new_log2_table_size);\n+    }\n+  }\n+\n+  void reset_table_scanner() {\n+    _table_scanner.set(&_table, BucketClaimSize);\n+  }\n+\n+  size_t mem_size() { return sizeof(*this) + _table.get_mem_size(Thread::current()); }\n+\n+  size_t number_of_entries() const { return Atomic::load(&_num_entries); }\n+};\n+\n+uintx G1CodeRootSetHashTable::HashTableLookUp::get_hash() const {\n+  return G1CodeRootSetHashTable::get_hash(_nmethod);\n+}\n+\n+bool G1CodeRootSetHashTable::HashTableLookUp::equals(G1CodeRootSetHashTableValue* value) {\n+  return value->_nmethod == _nmethod;\n+}\n+\n+uintx G1CodeRootSetHashTableConfig::get_hash(Value const& value, bool* is_dead) {\n+  *is_dead = false;\n+  return G1CodeRootSetHashTable::get_hash(value._nmethod);\n+}\n+\n+size_t G1CodeRootSet::length() const { return _table->number_of_entries(); }\n+\n+void G1CodeRootSet::add(nmethod* method) {\n+  if (!contains(method)) {\n+    assert(!_is_iterating, \"must be\");\n+    _table->insert(method);\n@@ -45,0 +258,4 @@\n+G1CodeRootSet::G1CodeRootSet() :\n+  _table(new G1CodeRootSetHashTable())\n+  DEBUG_ONLY(COMMA _is_iterating(false)) { }\n+\n@@ -50,11 +267,7 @@\n-  assert(_is_iterating == false, \"should not mutate while iterating the table\");\n-  bool removed = false;\n-  if (_table != nullptr) {\n-    removed = _table->remove(method);\n-  }\n-  if (removed) {\n-    if (length() == 0) {\n-      clear();\n-    }\n-  }\n-  return removed;\n+  assert(!_is_iterating, \"should not mutate while iterating the table\");\n+  return _table->remove(method);\n+}\n+\n+void G1CodeRootSet::remove_dead_entries() {\n+  assert(!_is_iterating, \"should not mutate while iterating the table\");\n+  _table->remove_dead_entries();\n@@ -64,4 +277,1 @@\n-  if (_table != nullptr) {\n-    return _table->contains(method);\n-  }\n-  return false;\n+  return _table->contains(method);\n@@ -71,3 +281,2 @@\n-  assert(_is_iterating == false, \"should not mutate while iterating the table\");\n-  delete _table;\n-  _table = nullptr;\n+  assert(!_is_iterating, \"should not mutate while iterating the table\");\n+  _table->clear();\n@@ -77,3 +286,5 @@\n-  return (_table == nullptr)\n-    ? sizeof(*this)\n-    : sizeof(*this) + _table->mem_size();\n+  return sizeof(*this) + _table->mem_size();\n+}\n+\n+void G1CodeRootSet::reset_table_scanner() {\n+  _table->reset_table_scanner();\n@@ -84,5 +295,1 @@\n-  if (_table != nullptr) {\n-    _table->iterate_all([&](nmethod* nm, nmethod* _) {\n-      blk->do_code_blob(nm);\n-    });\n-  }\n+  _table->iterate_at_safepoint(blk);\n@@ -94,0 +301,1 @@\n+\n@@ -96,11 +304,0 @@\n-   public:\n-    bool _points_into;\n-    PointsIntoHRDetectionClosure(HeapRegion* hr) : _hr(hr), _points_into(false) {}\n-\n-    void do_oop(narrowOop* o) {\n-      do_oop_work(o);\n-    }\n-\n-    void do_oop(oop* o) {\n-      do_oop_work(o);\n-    }\n@@ -114,0 +311,8 @@\n+\n+   public:\n+    bool _points_into;\n+    PointsIntoHRDetectionClosure(HeapRegion* hr) : _hr(hr), _points_into(false) {}\n+\n+    void do_oop(narrowOop* o) { do_oop_work(o); }\n+\n+    void do_oop(oop* o) { do_oop_work(o); }\n@@ -122,1 +327,1 @@\n-  bool do_entry(nmethod* nm, nmethod* _) {\n+  bool operator()(G1CodeRootSetHashTableValue* value) {\n@@ -124,1 +329,1 @@\n-    _blobs.do_code_blob(nm);\n+    _blobs.do_code_blob(value->_nmethod);\n@@ -130,8 +335,4 @@\n-  assert(_is_iterating == false, \"should not mutate while iterating the table\");\n-  CleanCallback should_clean(owner);\n-  if (_table != nullptr) {\n-    _table->unlink(&should_clean);\n-  }\n-  if (length() == 0) {\n-    clear();\n-  }\n+  assert(!_is_iterating, \"should not mutate while iterating the table\");\n+\n+  CleanCallback eval(owner);\n+  _table->clean(eval);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CodeRootSet.cpp","additions":256,"deletions":55,"binary":false,"changes":311,"status":"modified"},{"patch":"@@ -30,1 +30,0 @@\n-#include \"utilities\/resizeableResourceHash.hpp\"\n@@ -32,0 +31,1 @@\n+class G1CodeRootSetHashTable;\n@@ -36,1 +36,1 @@\n-\/\/ This class is not thread safe, locks are needed.\n+\/\/ This class is thread safe.\n@@ -38,10 +38,1 @@\n-  friend class G1CodeRootSetTest;\n-  friend class G1CodeRootSetTest_g1_code_cache_rem_set_vm_Test;\n-\n- private:\n-  const static size_t SmallSize = 32;\n-  const static size_t Threshold = 24;\n-  const static size_t LargeSize = 512;\n-\n-  using Table = ResizeableResourceHashtable<nmethod*, nmethod*, AnyObj::C_HEAP, mtGC>;\n-  Table* _table;\n+  G1CodeRootSetHashTable* _table;\n@@ -51,1 +42,1 @@\n-  G1CodeRootSet() : _table(nullptr) DEBUG_ONLY(COMMA _is_iterating(false)) {}\n+  G1CodeRootSet();\n@@ -58,0 +49,3 @@\n+\n+  \/\/ Prepare for MT iteration. Must be called before nmethods_do.\n+  void reset_table_scanner();\n@@ -60,1 +54,1 @@\n-  \/\/ Remove all nmethods which no longer contain pointers into our \"owner\" region\n+  \/\/ Remove all nmethods which no longer contain pointers into our \"owner\" region.\n@@ -62,0 +56,2 @@\n+  \/\/ Remove all nmethods which were unlinked after class unloading.\n+  void remove_dead_entries();\n@@ -66,1 +62,1 @@\n-  size_t length() const { return _table == nullptr ? 0 : _table->number_of_entries(); }\n+  size_t length() const;\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CodeRootSet.hpp","additions":11,"deletions":15,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -2598,0 +2598,29 @@\n+class G1RemoveDeadFromCodeRootSetsTask : public WorkerTask {\n+  HeapRegionClaimer _hrclaimer;\n+\n+  class RemoveDeadHeapRegionClosure : public HeapRegionClosure {\n+  public:\n+    RemoveDeadHeapRegionClosure() { }\n+\n+    bool do_heap_region(HeapRegion* hr) {\n+      hr->rem_set()->remove_dead_entries();\n+      return false;\n+    }\n+  } _cl;\n+\n+public:\n+  G1RemoveDeadFromCodeRootSetsTask(uint num_workers)\n+  : WorkerTask(\"G1 Remove Dead From Code Root Set Task\"),\n+    _hrclaimer(num_workers) { }\n+\n+  void work(uint worker_id) {\n+    G1CollectedHeap::heap()->heap_region_par_iterate_from_worker_offset(&_cl, &_hrclaimer, worker_id);\n+  }\n+};\n+\n+void G1CollectedHeap::remove_dead_entries_from_code_root_sets() {\n+  uint num_workers = workers()->active_workers();\n+  G1RemoveDeadFromCodeRootSetsTask remove_dead_task(num_workers);\n+  workers()->run_task(&remove_dead_task);\n+}\n+\n@@ -3012,2 +3041,2 @@\n-      \/\/ HeapRegion::add_code_root_locked() avoids adding duplicate entries.\n-      hr->add_code_root_locked(_nm);\n+      \/\/ HeapRegion::add_code_root() avoids adding duplicate entries.\n+      hr->add_code_root(_nm);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectedHeap.cpp","additions":31,"deletions":2,"binary":false,"changes":33,"status":"modified"},{"patch":"@@ -1267,0 +1267,3 @@\n+  \/\/ Removes unlinked nmethods from all code root sets after class unloading.\n+  void remove_dead_entries_from_code_root_sets();\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectedHeap.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -1700,0 +1700,1 @@\n+    _g1h->remove_dead_entries_from_code_root_sets();\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ConcurrentMark.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -326,0 +326,1 @@\n+    _heap->remove_dead_entries_from_code_root_sets();\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullCollector.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -1348,1 +1348,1 @@\n-    r->rem_set()->clear_locked(true \/* only_cardset *\/);\n+    r->rem_set()->clear(true \/* only_cardset *\/);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1Policy.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -260,1 +260,0 @@\n-    _collection_set_iter_state(nullptr),\n@@ -273,1 +272,0 @@\n-    FREE_C_HEAP_ARRAY(G1RemsetIterState, _collection_set_iter_state);\n@@ -280,1 +278,1 @@\n-    assert(_collection_set_iter_state == nullptr, \"Must not be initialized twice\");\n+    assert(_card_table_scan_state == nullptr, \"Must not be initialized twice\");\n@@ -282,1 +280,0 @@\n-    _collection_set_iter_state = NEW_C_HEAP_ARRAY(G1RemsetIterState, max_reserved_regions, mtGC);\n@@ -297,1 +294,0 @@\n-      reset_region_claim((uint)i);\n@@ -402,14 +398,0 @@\n-  void reset_region_claim(uint region_idx) {\n-    _collection_set_iter_state[region_idx] = false;\n-  }\n-\n-  \/\/ Attempt to claim the given region in the collection set for iteration. Returns true\n-  \/\/ if this call caused the transition from Unclaimed to Claimed.\n-  inline bool claim_collection_set_region(uint region) {\n-    assert(region < _max_reserved_regions, \"Tried to access invalid region %u\", region);\n-    if (_collection_set_iter_state[region]) {\n-      return false;\n-    }\n-    return !Atomic::cmpxchg(&_collection_set_iter_state[region], false, true);\n-  }\n-\n@@ -832,2 +814,0 @@\n-    uint const region_idx = r->hrm_index();\n-\n@@ -844,1 +824,2 @@\n-    if (_scan_state->claim_collection_set_region(region_idx)) {\n+    \/\/ Scan code root remembered sets.\n+    {\n@@ -1215,1 +1196,1 @@\n-      r->rem_set()->clear_locked(true \/* only_cardset *\/);\n+      r->rem_set()->clear(true \/* only_cardset *\/);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1RemSet.cpp","additions":4,"deletions":23,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -145,1 +145,1 @@\n-                                           r->rem_set()->clear_locked(true \/* only_cardset *\/);\n+                                           r->rem_set()->clear(true \/* only_cardset *\/);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1RemSetTrackingPolicy.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -108,1 +108,1 @@\n-  _rem_set->clear_locked(true \/* only_cardset *\/, retain \/* keep_tracked *\/);\n+  _rem_set->clear(true \/* only_cardset *\/, retain \/* keep_tracked *\/);\n@@ -125,1 +125,1 @@\n-  rem_set()->clear_locked();\n+  rem_set()->clear();\n@@ -208,1 +208,1 @@\n-  return _rem_set->reset_table_scanner();\n+  _rem_set->reset_table_scanner();\n@@ -278,8 +278,1 @@\n-  HeapRegionRemSet* hrrs = rem_set();\n-  hrrs->add_code_root(nm);\n-}\n-\n-void HeapRegion::add_code_root_locked(nmethod* nm) {\n-  assert_locked_or_safepoint(CodeCache_lock);\n-  HeapRegionRemSet* hrrs = rem_set();\n-  hrrs->add_code_root_locked(nm);\n+  rem_set()->add_code_root(nm);\n@@ -289,2 +282,1 @@\n-  HeapRegionRemSet* hrrs = rem_set();\n-  hrrs->remove_code_root(nm);\n+  rem_set()->remove_code_root(nm);\n@@ -294,2 +286,1 @@\n-  HeapRegionRemSet* hrrs = rem_set();\n-  hrrs->code_roots_do(blk);\n+  rem_set()->code_roots_do(blk);\n","filename":"src\/hotspot\/share\/gc\/g1\/heapRegion.cpp","additions":6,"deletions":15,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -547,1 +547,0 @@\n-  void add_code_root_locked(nmethod* nm);\n","filename":"src\/hotspot\/share\/gc\/g1\/heapRegion.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -60,1 +60,0 @@\n-  _m(Mutex::service - 1, FormatBuffer<128>(\"HeapRegionRemSet#%u_lock\", hr->hrm_index())),\n@@ -71,6 +70,1 @@\n-void HeapRegionRemSet::clear(bool only_cardset) {\n-  MutexLocker x(&_m, Mutex::_no_safepoint_check_flag);\n-  clear_locked(only_cardset);\n-}\n-\n-void HeapRegionRemSet::clear_locked(bool only_cardset, bool keep_tracked) {\n+void HeapRegionRemSet::clear(bool only_cardset, bool keep_tracked) {\n@@ -91,0 +85,1 @@\n+  _code_roots.reset_table_scanner();\n@@ -111,20 +106,1 @@\n-  assert((!CodeCache_lock->owned_by_self() || SafepointSynchronize::is_at_safepoint()),\n-          \"should call add_code_root_locked instead. CodeCache_lock->owned_by_self(): %s, is_at_safepoint(): %s\",\n-          BOOL_TO_STR(CodeCache_lock->owned_by_self()), BOOL_TO_STR(SafepointSynchronize::is_at_safepoint()));\n-\n-  MutexLocker ml(&_m, Mutex::_no_safepoint_check_flag);\n-  add_code_root_locked(nm);\n-}\n-\n-void HeapRegionRemSet::add_code_root_locked(nmethod* nm) {\n-  assert(nm != nullptr, \"sanity\");\n-  assert((CodeCache_lock->owned_by_self() ||\n-         (SafepointSynchronize::is_at_safepoint() &&\n-          (_m.owned_by_self() || Thread::current()->is_VM_thread()))),\n-          \"not safely locked. CodeCache_lock->owned_by_self(): %s, is_at_safepoint(): %s, _m.owned_by_self(): %s, Thread::current()->is_VM_thread(): %s\",\n-          BOOL_TO_STR(CodeCache_lock->owned_by_self()), BOOL_TO_STR(SafepointSynchronize::is_at_safepoint()),\n-          BOOL_TO_STR(_m.owned_by_self()), BOOL_TO_STR(Thread::current()->is_VM_thread()));\n-\n-  if (!_code_roots.contains(nm)) { \/\/ with this test, we can assert that we do not modify the hash table while iterating over it\n-    _code_roots.add(nm);\n-  }\n+  _code_roots.add(nm);\n@@ -135,1 +111,0 @@\n-  assert_locked_or_safepoint(CodeCache_lock);\n@@ -137,1 +112,0 @@\n-  ConditionalMutexLocker ml(&_m, !CodeCache_lock->owned_by_self(), Mutex::_no_safepoint_check_flag);\n@@ -152,0 +126,4 @@\n+void HeapRegionRemSet::remove_dead_entries() {\n+  _code_roots.remove_dead_entries();\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/heapRegionRemSet.cpp","additions":7,"deletions":29,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -43,1 +43,0 @@\n-  Mutex _m;\n@@ -120,2 +119,1 @@\n-  void clear(bool only_cardset = false);\n-  void clear_locked(bool only_cardset = false, bool keep_tracked = false);\n+  void clear(bool only_cardset = false, bool keep_tracked = false);\n@@ -161,0 +159,2 @@\n+  \/\/ Removes all unlinked nmethods from code roots.\n+  void remove_dead_entries();\n@@ -170,1 +170,0 @@\n-    MutexLocker ml(&_m, Mutex::_no_safepoint_check_flag);\n","filename":"src\/hotspot\/share\/gc\/g1\/heapRegionRemSet.hpp","additions":3,"deletions":4,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -234,1 +234,1 @@\n-  MUTEX_DEFN(Metaspace_lock                  , PaddedMutex  , nosafepoint-3);\n+  MUTEX_DEFN(Metaspace_lock                  , PaddedMutex  , nosafepoint-4);\n@@ -305,1 +305,1 @@\n-  MUTEX_DEFN(ContinuationRelativize_lock     , PaddedMonitor, nosafepoint-3);\n+  MUTEX_DEFN(ContinuationRelativize_lock     , PaddedMonitor, nosafepoint-4);\n@@ -307,1 +307,1 @@\n-  MUTEX_DEFN(ThreadsSMRDelete_lock           , PaddedMonitor, nosafepoint-3); \/\/ Holds ConcurrentHashTableResize_lock\n+  MUTEX_DEFN(ThreadsSMRDelete_lock           , PaddedMonitor, nosafepoint-4); \/\/ Holds ConcurrentHashTableResize_lock\n","filename":"src\/hotspot\/share\/runtime\/mutexLocker.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -1027,1 +1027,1 @@\n-    new Mutex(Mutex::nosafepoint-2, \"ConcurrentHashTableResize_lock\");\n+    new Mutex(Mutex::nosafepoint-3, \"ConcurrentHashTableResize_lock\");\n","filename":"src\/hotspot\/share\/utilities\/concurrentHashTable.inline.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -28,9 +28,1 @@\n-class G1CodeRootSetTest : public ::testing::Test {\n- public:\n-\n-  size_t threshold() {\n-    return G1CodeRootSet::Threshold;\n-  }\n-};\n-\n-TEST_VM_F(G1CodeRootSetTest, g1_code_cache_rem_set) {\n+TEST_VM(G1CodeRootSet, g1_code_cache_rem_set) {\n@@ -46,1 +38,1 @@\n-  const size_t num_to_add = (size_t) threshold() + 1;\n+  const size_t num_to_add = 1000;\n@@ -63,3 +55,0 @@\n-  ASSERT_EQ(root_set._table->table_size(), 512u)\n-          << \"should have grown to large hashtable\";\n-\n@@ -79,1 +68,1 @@\n-          << \"should have grown to large hashtable\";\n+          << \"should be empty\";\n","filename":"test\/hotspot\/gtest\/gc\/g1\/test_g1CodeRootSet.cpp","additions":3,"deletions":14,"binary":false,"changes":17,"status":"modified"}]}