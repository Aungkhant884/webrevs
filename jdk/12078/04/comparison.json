{"files":[{"patch":"@@ -1,66 +0,0 @@\n-\/*\n- * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2014, Red Hat Inc. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#ifndef CPU_AARCH64_BYTES_AARCH64_HPP\n-#define CPU_AARCH64_BYTES_AARCH64_HPP\n-\n-#include \"memory\/allStatic.hpp\"\n-\n-class Bytes: AllStatic {\n- public:\n-  \/\/ Efficient reading and writing of unaligned unsigned data in platform-specific byte ordering\n-  \/\/ (no special code is needed since x86 CPUs can access unaligned data)\n-  static inline u2   get_native_u2(address p)         { return *(u2*)p; }\n-  static inline u4   get_native_u4(address p)         { return *(u4*)p; }\n-  static inline u8   get_native_u8(address p)         { return *(u8*)p; }\n-\n-  static inline void put_native_u2(address p, u2 x)   { *(u2*)p = x; }\n-  static inline void put_native_u4(address p, u4 x)   { *(u4*)p = x; }\n-  static inline void put_native_u8(address p, u8 x)   { *(u8*)p = x; }\n-\n-\n-  \/\/ Efficient reading and writing of unaligned unsigned data in Java\n-  \/\/ byte ordering (i.e. big-endian ordering). Byte-order reversal is\n-  \/\/ needed since x86 CPUs use little-endian format.\n-  static inline u2   get_Java_u2(address p)           { return swap_u2(get_native_u2(p)); }\n-  static inline u4   get_Java_u4(address p)           { return swap_u4(get_native_u4(p)); }\n-  static inline u8   get_Java_u8(address p)           { return swap_u8(get_native_u8(p)); }\n-\n-  static inline void put_Java_u2(address p, u2 x)     { put_native_u2(p, swap_u2(x)); }\n-  static inline void put_Java_u4(address p, u4 x)     { put_native_u4(p, swap_u4(x)); }\n-  static inline void put_Java_u8(address p, u8 x)     { put_native_u8(p, swap_u8(x)); }\n-\n-\n-  \/\/ Efficient swapping of byte ordering\n-  static inline u2   swap_u2(u2 x);                   \/\/ compiler-dependent implementation\n-  static inline u4   swap_u4(u4 x);                   \/\/ compiler-dependent implementation\n-  static inline u8   swap_u8(u8 x);\n-};\n-\n-\n-\/\/ The following header contains the implementations of swap_u2, swap_u4, and swap_u8[_base]\n-#include OS_CPU_HEADER(bytes)\n-\n-#endif \/\/ CPU_AARCH64_BYTES_AARCH64_HPP\n","filename":"src\/hotspot\/cpu\/aarch64\/bytes_aarch64.hpp","additions":0,"deletions":66,"binary":false,"changes":66,"status":"deleted"},{"patch":"@@ -1,189 +0,0 @@\n-\/*\n- * Copyright (c) 2008, 2022, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#ifndef CPU_ARM_BYTES_ARM_HPP\n-#define CPU_ARM_BYTES_ARM_HPP\n-\n-#include \"memory\/allStatic.hpp\"\n-#include \"utilities\/macros.hpp\"\n-\n-#ifndef VM_LITTLE_ENDIAN\n-#define VM_LITTLE_ENDIAN  1\n-#endif\n-\n-class Bytes: AllStatic {\n-\n- public:\n-  static inline u2 get_Java_u2(address p) {\n-    return (u2(p[0]) << 8) | u2(p[1]);\n-  }\n-\n-  static inline u4 get_Java_u4(address p) {\n-    return u4(p[0]) << 24 |\n-           u4(p[1]) << 16 |\n-           u4(p[2]) <<  8 |\n-           u4(p[3]);\n-  }\n-\n-  static inline u8 get_Java_u8(address p) {\n-    return u8(p[0]) << 56 |\n-           u8(p[1]) << 48 |\n-           u8(p[2]) << 40 |\n-           u8(p[3]) << 32 |\n-           u8(p[4]) << 24 |\n-           u8(p[5]) << 16 |\n-           u8(p[6]) <<  8 |\n-           u8(p[7]);\n-  }\n-\n-  static inline void put_Java_u2(address p, u2 x) {\n-    p[0] = x >> 8;\n-    p[1] = x;\n-  }\n-\n-  static inline void put_Java_u4(address p, u4 x) {\n-    ((u1*)p)[0] = x >> 24;\n-    ((u1*)p)[1] = x >> 16;\n-    ((u1*)p)[2] = x >>  8;\n-    ((u1*)p)[3] = x;\n-  }\n-\n-  static inline void put_Java_u8(address p, u8 x) {\n-    ((u1*)p)[0] = x >> 56;\n-    ((u1*)p)[1] = x >> 48;\n-    ((u1*)p)[2] = x >> 40;\n-    ((u1*)p)[3] = x >> 32;\n-    ((u1*)p)[4] = x >> 24;\n-    ((u1*)p)[5] = x >> 16;\n-    ((u1*)p)[6] = x >>  8;\n-    ((u1*)p)[7] = x;\n-  }\n-\n-#ifdef VM_LITTLE_ENDIAN\n-\n-  static inline u2 get_native_u2(address p) {\n-    return (intptr_t(p) & 1) == 0 ? *(u2*)p : u2(p[0]) | (u2(p[1]) << 8);\n-  }\n-\n-  static inline u4 get_native_u4(address p) {\n-    switch (intptr_t(p) & 3) {\n-      case 0:  return *(u4*)p;\n-      case 2:  return u4(((u2*)p)[0]) |\n-                      u4(((u2*)p)[1]) << 16;\n-      default: return u4(p[0])       |\n-                      u4(p[1]) <<  8 |\n-                      u4(p[2]) << 16 |\n-                      u4(p[3]) << 24;\n-    }\n-  }\n-\n-  static inline u8 get_native_u8(address p) {\n-    switch (intptr_t(p) & 7) {\n-      case 0:  return *(u8*)p;\n-      case 4:  return u8(((u4*)p)[0]) |\n-                      u8(((u4*)p)[1]) << 32;\n-      case 2:  return u8(((u2*)p)[0])       |\n-                      u8(((u2*)p)[1]) << 16 |\n-                      u8(((u2*)p)[2]) << 32 |\n-                      u8(((u2*)p)[3]) << 48;\n-      default: return u8(p[0])       |\n-                      u8(p[1]) <<  8 |\n-                      u8(p[2]) << 16 |\n-                      u8(p[3]) << 24 |\n-                      u8(p[4]) << 32 |\n-                      u8(p[5]) << 40 |\n-                      u8(p[6]) << 48 |\n-                      u8(p[7]) << 56;\n-    }\n-  }\n-\n-  static inline void put_native_u2(address p, u2 x) {\n-    if ((intptr_t(p) & 1) == 0) {\n-      *(u2*)p = x;\n-    } else {\n-      p[0] = x;\n-      p[1] = x >> 8;\n-    }\n-  }\n-\n-  static inline void put_native_u4(address p, u4 x) {\n-    switch (intptr_t(p) & 3) {\n-      case 0:  *(u4*)p = x;\n-               break;\n-      case 2:  ((u2*)p)[0] = x;\n-               ((u2*)p)[1] = x >> 16;\n-               break;\n-      default: ((u1*)p)[0] = x;\n-               ((u1*)p)[1] = x >>  8;\n-               ((u1*)p)[2] = x >> 16;\n-               ((u1*)p)[3] = x >> 24;\n-               break;\n-    }\n-  }\n-\n-  static inline void put_native_u8(address p, u8 x) {\n-    switch (intptr_t(p) & 7) {\n-      case 0:  *(u8*)p = x;\n-               break;\n-      case 4:  ((u4*)p)[0] = x;\n-               ((u4*)p)[1] = x >> 32;\n-               break;\n-      case 2:  ((u2*)p)[0] = x;\n-               ((u2*)p)[1] = x >> 16;\n-               ((u2*)p)[2] = x >> 32;\n-               ((u2*)p)[3] = x >> 48;\n-               break;\n-      default: ((u1*)p)[0] = x;\n-               ((u1*)p)[1] = x >>  8;\n-               ((u1*)p)[2] = x >> 16;\n-               ((u1*)p)[3] = x >> 24;\n-               ((u1*)p)[4] = x >> 32;\n-               ((u1*)p)[5] = x >> 40;\n-               ((u1*)p)[6] = x >> 48;\n-               ((u1*)p)[7] = x >> 56;\n-    }\n-  }\n-\n-#else\n-\n-  static inline u2 get_native_u2(address p) { return get_Java_u2(p); }\n-  static inline u4 get_native_u4(address p) { return get_Java_u4(p); }\n-  static inline u8 get_native_u8(address p) { return get_Java_u8(p); }\n-  static inline void put_native_u2(address p, u2 x) { put_Java_u2(p, x); }\n-  static inline void put_native_u4(address p, u4 x) { put_Java_u4(p, x); }\n-  static inline void put_native_u8(address p, u8 x) { put_Java_u8(p, x); }\n-\n-#endif \/\/ VM_LITTLE_ENDIAN\n-\n-  \/\/ Efficient swapping of byte ordering\n-  static inline u2 swap_u2(u2 x);\n-  static inline u4 swap_u4(u4 x);\n-  static inline u8 swap_u8(u8 x);\n-};\n-\n-\n-\/\/ The following header contains the implementations of swap_u2, swap_u4, and swap_u8\n-#include OS_CPU_HEADER(bytes)\n-\n-#endif \/\/ CPU_ARM_BYTES_ARM_HPP\n","filename":"src\/hotspot\/cpu\/arm\/bytes_arm.hpp","additions":0,"deletions":189,"binary":false,"changes":189,"status":"deleted"},{"patch":"@@ -1,271 +0,0 @@\n-\/*\n- * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2012, 2022 SAP SE. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#ifndef CPU_PPC_BYTES_PPC_HPP\n-#define CPU_PPC_BYTES_PPC_HPP\n-\n-#include \"memory\/allStatic.hpp\"\n-\n-class Bytes: AllStatic {\n- public:\n-  \/\/ Efficient reading and writing of unaligned unsigned data in platform-specific byte ordering\n-  \/\/ PowerPC needs to check for alignment.\n-\n-  \/\/ Can I count on address always being a pointer to an unsigned char? Yes.\n-\n-#if defined(VM_LITTLE_ENDIAN)\n-\n-  \/\/ Forward declarations of the compiler-dependent implementation\n-  static inline u2 swap_u2(u2 x);\n-  static inline u4 swap_u4(u4 x);\n-  static inline u8 swap_u8(u8 x);\n-\n-  static inline u2   get_native_u2(address p) {\n-    return (intptr_t(p) & 1) == 0\n-             ?   *(u2*)p\n-             :   ( u2(p[1]) << 8 )\n-               | ( u2(p[0])      );\n-  }\n-\n-  static inline u4   get_native_u4(address p) {\n-    switch (intptr_t(p) & 3) {\n-     case 0:  return *(u4*)p;\n-\n-     case 2:  return (  u4( ((u2*)p)[1] ) << 16  )\n-                   | (  u4( ((u2*)p)[0] )        );\n-\n-    default:  return ( u4(p[3]) << 24 )\n-                   | ( u4(p[2]) << 16 )\n-                   | ( u4(p[1]) <<  8 )\n-                   |   u4(p[0]);\n-    }\n-  }\n-\n-  static inline u8   get_native_u8(address p) {\n-    switch (intptr_t(p) & 7) {\n-      case 0:  return *(u8*)p;\n-\n-      case 4:  return (  u8( ((u4*)p)[1] ) << 32  )\n-                    | (  u8( ((u4*)p)[0] )        );\n-\n-      case 2:  return (  u8( ((u2*)p)[3] ) << 48  )\n-                    | (  u8( ((u2*)p)[2] ) << 32  )\n-                    | (  u8( ((u2*)p)[1] ) << 16  )\n-                    | (  u8( ((u2*)p)[0] )        );\n-\n-     default:  return ( u8(p[7]) << 56 )\n-                    | ( u8(p[6]) << 48 )\n-                    | ( u8(p[5]) << 40 )\n-                    | ( u8(p[4]) << 32 )\n-                    | ( u8(p[3]) << 24 )\n-                    | ( u8(p[2]) << 16 )\n-                    | ( u8(p[1]) <<  8 )\n-                    |   u8(p[0]);\n-    }\n-  }\n-\n-\n-\n-  static inline void put_native_u2(address p, u2 x) {\n-    if ( (intptr_t(p) & 1) == 0 )  *(u2*)p = x;\n-    else {\n-      p[1] = x >> 8;\n-      p[0] = x;\n-    }\n-  }\n-\n-  static inline void put_native_u4(address p, u4 x) {\n-    switch ( intptr_t(p) & 3 ) {\n-    case 0:  *(u4*)p = x;\n-              break;\n-\n-    case 2:  ((u2*)p)[1] = x >> 16;\n-             ((u2*)p)[0] = x;\n-             break;\n-\n-    default: ((u1*)p)[3] = x >> 24;\n-             ((u1*)p)[2] = x >> 16;\n-             ((u1*)p)[1] = x >>  8;\n-             ((u1*)p)[0] = x;\n-             break;\n-    }\n-  }\n-\n-  static inline void put_native_u8(address p, u8 x) {\n-    switch ( intptr_t(p) & 7 ) {\n-    case 0:  *(u8*)p = x;\n-             break;\n-\n-    case 4:  ((u4*)p)[1] = x >> 32;\n-             ((u4*)p)[0] = x;\n-             break;\n-\n-    case 2:  ((u2*)p)[3] = x >> 48;\n-             ((u2*)p)[2] = x >> 32;\n-             ((u2*)p)[1] = x >> 16;\n-             ((u2*)p)[0] = x;\n-             break;\n-\n-    default: ((u1*)p)[7] = x >> 56;\n-             ((u1*)p)[6] = x >> 48;\n-             ((u1*)p)[5] = x >> 40;\n-             ((u1*)p)[4] = x >> 32;\n-             ((u1*)p)[3] = x >> 24;\n-             ((u1*)p)[2] = x >> 16;\n-             ((u1*)p)[1] = x >>  8;\n-             ((u1*)p)[0] = x;\n-    }\n-  }\n-\n-  \/\/ Efficient reading and writing of unaligned unsigned data in Java byte ordering (i.e. big-endian ordering)\n-  \/\/ (no byte-order reversal is needed since Power CPUs are big-endian oriented).\n-  static inline u2   get_Java_u2(address p) { return swap_u2(get_native_u2(p)); }\n-  static inline u4   get_Java_u4(address p) { return swap_u4(get_native_u4(p)); }\n-  static inline u8   get_Java_u8(address p) { return swap_u8(get_native_u8(p)); }\n-\n-  static inline void put_Java_u2(address p, u2 x)     { put_native_u2(p, swap_u2(x)); }\n-  static inline void put_Java_u4(address p, u4 x)     { put_native_u4(p, swap_u4(x)); }\n-  static inline void put_Java_u8(address p, u8 x)     { put_native_u8(p, swap_u8(x)); }\n-\n-#else \/\/ !defined(VM_LITTLE_ENDIAN)\n-\n-  \/\/ Thus, a swap between native and Java ordering is always a no-op:\n-  static inline u2   swap_u2(u2 x)  { return x; }\n-  static inline u4   swap_u4(u4 x)  { return x; }\n-  static inline u8   swap_u8(u8 x)  { return x; }\n-\n-  static inline u2   get_native_u2(address p) {\n-    return (intptr_t(p) & 1) == 0\n-             ?   *(u2*)p\n-             :   ( u2(p[0]) << 8 )\n-               | ( u2(p[1])      );\n-  }\n-\n-  static inline u4   get_native_u4(address p) {\n-    switch (intptr_t(p) & 3) {\n-     case 0:  return *(u4*)p;\n-\n-     case 2:  return (  u4( ((u2*)p)[0] ) << 16  )\n-                   | (  u4( ((u2*)p)[1] )        );\n-\n-    default:  return ( u4(p[0]) << 24 )\n-                   | ( u4(p[1]) << 16 )\n-                   | ( u4(p[2]) <<  8 )\n-                   |   u4(p[3]);\n-    }\n-  }\n-\n-  static inline u8   get_native_u8(address p) {\n-    switch (intptr_t(p) & 7) {\n-      case 0:  return *(u8*)p;\n-\n-      case 4:  return (  u8( ((u4*)p)[0] ) << 32  )\n-                    | (  u8( ((u4*)p)[1] )        );\n-\n-      case 2:  return (  u8( ((u2*)p)[0] ) << 48  )\n-                    | (  u8( ((u2*)p)[1] ) << 32  )\n-                    | (  u8( ((u2*)p)[2] ) << 16  )\n-                    | (  u8( ((u2*)p)[3] )        );\n-\n-     default:  return ( u8(p[0]) << 56 )\n-                    | ( u8(p[1]) << 48 )\n-                    | ( u8(p[2]) << 40 )\n-                    | ( u8(p[3]) << 32 )\n-                    | ( u8(p[4]) << 24 )\n-                    | ( u8(p[5]) << 16 )\n-                    | ( u8(p[6]) <<  8 )\n-                    |   u8(p[7]);\n-    }\n-  }\n-\n-\n-\n-  static inline void put_native_u2(address p, u2 x) {\n-    if ( (intptr_t(p) & 1) == 0 ) { *(u2*)p = x; }\n-    else {\n-      p[0] = x >> 8;\n-      p[1] = x;\n-    }\n-  }\n-\n-  static inline void put_native_u4(address p, u4 x) {\n-    switch ( intptr_t(p) & 3 ) {\n-    case 0:  *(u4*)p = x;\n-              break;\n-\n-    case 2:  ((u2*)p)[0] = x >> 16;\n-             ((u2*)p)[1] = x;\n-             break;\n-\n-    default: ((u1*)p)[0] = x >> 24;\n-             ((u1*)p)[1] = x >> 16;\n-             ((u1*)p)[2] = x >>  8;\n-             ((u1*)p)[3] = x;\n-             break;\n-    }\n-  }\n-\n-  static inline void put_native_u8(address p, u8 x) {\n-    switch ( intptr_t(p) & 7 ) {\n-    case 0:  *(u8*)p = x;\n-             break;\n-\n-    case 4:  ((u4*)p)[0] = x >> 32;\n-             ((u4*)p)[1] = x;\n-             break;\n-\n-    case 2:  ((u2*)p)[0] = x >> 48;\n-             ((u2*)p)[1] = x >> 32;\n-             ((u2*)p)[2] = x >> 16;\n-             ((u2*)p)[3] = x;\n-             break;\n-\n-    default: ((u1*)p)[0] = x >> 56;\n-             ((u1*)p)[1] = x >> 48;\n-             ((u1*)p)[2] = x >> 40;\n-             ((u1*)p)[3] = x >> 32;\n-             ((u1*)p)[4] = x >> 24;\n-             ((u1*)p)[5] = x >> 16;\n-             ((u1*)p)[6] = x >>  8;\n-             ((u1*)p)[7] = x;\n-    }\n-  }\n-\n-  \/\/ Efficient reading and writing of unaligned unsigned data in Java byte ordering (i.e. big-endian ordering)\n-  \/\/ (no byte-order reversal is needed since Power CPUs are big-endian oriented).\n-  static inline u2   get_Java_u2(address p) { return get_native_u2(p); }\n-  static inline u4   get_Java_u4(address p) { return get_native_u4(p); }\n-  static inline u8   get_Java_u8(address p) { return get_native_u8(p); }\n-\n-  static inline void put_Java_u2(address p, u2 x)     { put_native_u2(p, x); }\n-  static inline void put_Java_u4(address p, u4 x)     { put_native_u4(p, x); }\n-  static inline void put_Java_u8(address p, u8 x)     { put_native_u8(p, x); }\n-\n-#endif \/\/ VM_LITTLE_ENDIAN\n-};\n-\n-#include OS_CPU_HEADER(bytes)\n-\n-#endif \/\/ CPU_PPC_BYTES_PPC_HPP\n","filename":"src\/hotspot\/cpu\/ppc\/bytes_ppc.hpp","additions":0,"deletions":271,"binary":false,"changes":271,"status":"deleted"},{"patch":"@@ -1,169 +0,0 @@\n-\/*\n- * Copyright (c) 1997, 2019, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2012, 2016 SAP SE. All rights reserved.\n- * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#ifndef CPU_RISCV_BYTES_RISCV_HPP\n-#define CPU_RISCV_BYTES_RISCV_HPP\n-\n-#include \"memory\/allStatic.hpp\"\n-\n-class Bytes: AllStatic {\n- public:\n-  \/\/ Efficient reading and writing of unaligned unsigned data in platform-specific byte ordering\n-  \/\/ RISCV needs to check for alignment.\n-\n-  \/\/ Forward declarations of the compiler-dependent implementation\n-  static inline u2 swap_u2(u2 x);\n-  static inline u4 swap_u4(u4 x);\n-  static inline u8 swap_u8(u8 x);\n-\n-  static inline u2 get_native_u2(address p) {\n-    if ((intptr_t(p) & 1) == 0) {\n-      return *(u2*)p;\n-    } else {\n-      return ((u2)(p[1]) << 8) |\n-             ((u2)(p[0]));\n-    }\n-  }\n-\n-  static inline u4 get_native_u4(address p) {\n-    switch (intptr_t(p) & 3) {\n-      case 0:\n-        return *(u4*)p;\n-\n-      case 2:\n-        return ((u4)(((u2*)p)[1]) << 16) |\n-               ((u4)(((u2*)p)[0]));\n-\n-      default:\n-        return ((u4)(p[3]) << 24) |\n-               ((u4)(p[2]) << 16) |\n-               ((u4)(p[1]) <<  8) |\n-               ((u4)(p[0]));\n-    }\n-  }\n-\n-  static inline u8 get_native_u8(address p) {\n-    switch (intptr_t(p) & 7) {\n-      case 0:\n-        return *(u8*)p;\n-\n-      case 4:\n-        return ((u8)(((u4*)p)[1]) << 32) |\n-               ((u8)(((u4*)p)[0]));\n-\n-      case 2:\n-      case 6:\n-        return ((u8)(((u2*)p)[3]) << 48) |\n-               ((u8)(((u2*)p)[2]) << 32) |\n-               ((u8)(((u2*)p)[1]) << 16) |\n-               ((u8)(((u2*)p)[0]));\n-\n-      default:\n-        return ((u8)(p[7]) << 56) |\n-               ((u8)(p[6]) << 48) |\n-               ((u8)(p[5]) << 40) |\n-               ((u8)(p[4]) << 32) |\n-               ((u8)(p[3]) << 24) |\n-               ((u8)(p[2]) << 16) |\n-               ((u8)(p[1]) <<  8) |\n-               ((u8)(p[0]));\n-    }\n-  }\n-\n-  static inline void put_native_u2(address p, u2 x) {\n-    if ((intptr_t(p) & 1) == 0) {\n-      *(u2*)p = x;\n-    } else {\n-      p[1] = x >> 8;\n-      p[0] = x;\n-    }\n-  }\n-\n-  static inline void put_native_u4(address p, u4 x) {\n-    switch (intptr_t(p) & 3) {\n-      case 0:\n-        *(u4*)p = x;\n-        break;\n-\n-      case 2:\n-        ((u2*)p)[1] = x >> 16;\n-        ((u2*)p)[0] = x;\n-        break;\n-\n-      default:\n-        ((u1*)p)[3] = x >> 24;\n-        ((u1*)p)[2] = x >> 16;\n-        ((u1*)p)[1] = x >>  8;\n-        ((u1*)p)[0] = x;\n-        break;\n-    }\n-  }\n-\n-  static inline void put_native_u8(address p, u8 x) {\n-    switch (intptr_t(p) & 7) {\n-      case 0:\n-        *(u8*)p = x;\n-        break;\n-\n-      case 4:\n-        ((u4*)p)[1] = x >> 32;\n-        ((u4*)p)[0] = x;\n-        break;\n-\n-      case 2:\n-      case 6:\n-        ((u2*)p)[3] = x >> 48;\n-        ((u2*)p)[2] = x >> 32;\n-        ((u2*)p)[1] = x >> 16;\n-        ((u2*)p)[0] = x;\n-        break;\n-\n-      default:\n-        ((u1*)p)[7] = x >> 56;\n-        ((u1*)p)[6] = x >> 48;\n-        ((u1*)p)[5] = x >> 40;\n-        ((u1*)p)[4] = x >> 32;\n-        ((u1*)p)[3] = x >> 24;\n-        ((u1*)p)[2] = x >> 16;\n-        ((u1*)p)[1] = x >>  8;\n-        ((u1*)p)[0] = x;\n-        break;\n-    }\n-  }\n-\n-  \/\/ Efficient reading and writing of unaligned unsigned data in Java byte ordering (i.e. big-endian ordering)\n-  static inline u2 get_Java_u2(address p) { return swap_u2(get_native_u2(p)); }\n-  static inline u4 get_Java_u4(address p) { return swap_u4(get_native_u4(p)); }\n-  static inline u8 get_Java_u8(address p) { return swap_u8(get_native_u8(p)); }\n-\n-  static inline void put_Java_u2(address p, u2 x) { put_native_u2(p, swap_u2(x)); }\n-  static inline void put_Java_u4(address p, u4 x) { put_native_u4(p, swap_u4(x)); }\n-  static inline void put_Java_u8(address p, u8 x) { put_native_u8(p, swap_u8(x)); }\n-};\n-\n-#include OS_CPU_HEADER(bytes)\n-\n-#endif \/\/ CPU_RISCV_BYTES_RISCV_HPP\n","filename":"src\/hotspot\/cpu\/riscv\/bytes_riscv.hpp","additions":0,"deletions":169,"binary":false,"changes":169,"status":"deleted"},{"patch":"@@ -1,66 +0,0 @@\n-\/*\n- * Copyright (c) 2016, 2022, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2016, 2022 SAP SE. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#ifndef CPU_S390_BYTES_S390_HPP\n-#define CPU_S390_BYTES_S390_HPP\n-\n-#include \"memory\/allStatic.hpp\"\n-\n-class Bytes: AllStatic {\n- public:\n-  \/\/ Efficient reading and writing of unaligned unsigned data in\n-  \/\/ platform-specific byte ordering.\n-\n-  \/\/ Use regular load and store for unaligned access.\n-  \/\/\n-  \/\/ On z\/Architecture, unaligned loads and stores are supported when using the\n-  \/\/ \"traditional\" load (LH, L\/LY, LG) and store (STH, ST\/STY, STG) instructions.\n-  \/\/ The penalty for unaligned access is just very few (two or three) ticks,\n-  \/\/ plus another few (two or three) ticks if the access crosses a cache line boundary.\n-  \/\/\n-  \/\/ In short, it makes no sense on z\/Architecture to piecemeal get or put unaligned data.\n-\n-  static inline u2   get_native_u2(address p) { return *(u2*)p; }\n-  static inline u4   get_native_u4(address p) { return *(u4*)p; }\n-  static inline u8   get_native_u8(address p) { return *(u8*)p; }\n-\n-  static inline void put_native_u2(address p, u2 x) { *(u2*)p = x; }\n-  static inline void put_native_u4(address p, u4 x) { *(u4*)p = x; }\n-  static inline void put_native_u8(address p, u8 x) { *(u8*)p = x; }\n-\n-  \/\/ The following header contains the implementations of swap_u2, swap_u4, and swap_u8.\n-#include OS_CPU_HEADER(bytes)\n-\n-  \/\/ Efficient reading and writing of unaligned unsigned data in Java byte ordering (i.e. big-endian ordering)\n-  static inline u2   get_Java_u2(address p) { return get_native_u2(p); }\n-  static inline u4   get_Java_u4(address p) { return get_native_u4(p); }\n-  static inline u8   get_Java_u8(address p) { return get_native_u8(p); }\n-\n-  static inline void put_Java_u2(address p, u2 x) { put_native_u2(p, x); }\n-  static inline void put_Java_u4(address p, u4 x) { put_native_u4(p, x); }\n-  static inline void put_Java_u8(address p, u8 x) { put_native_u8(p, x); }\n-};\n-\n-#endif \/\/ CPU_S390_BYTES_S390_HPP\n","filename":"src\/hotspot\/cpu\/s390\/bytes_s390.hpp","additions":0,"deletions":66,"binary":false,"changes":66,"status":"deleted"},{"patch":"@@ -1,127 +0,0 @@\n-\/*\n- * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#ifndef CPU_X86_BYTES_X86_HPP\n-#define CPU_X86_BYTES_X86_HPP\n-\n-#include \"memory\/allStatic.hpp\"\n-#include \"utilities\/align.hpp\"\n-#include \"utilities\/macros.hpp\"\n-\n-class Bytes: AllStatic {\n- private:\n-#ifndef AMD64\n-  \/\/ Helper function for swap_u8\n-  static inline u8   swap_u8_base(u4 x, u4 y);        \/\/ compiler-dependent implementation\n-#endif \/\/ AMD64\n-\n- public:\n-  \/\/ Efficient reading and writing of unaligned unsigned data in platform-specific byte ordering\n-  template <typename T>\n-  static inline T get_native(const void* p) {\n-    assert(p != NULL, \"null pointer\");\n-\n-    T x;\n-\n-    if (is_aligned(p, sizeof(T))) {\n-      x = *(T*)p;\n-    } else {\n-      memcpy(&x, p, sizeof(T));\n-    }\n-\n-    return x;\n-  }\n-\n-  template <typename T>\n-  static inline void put_native(void* p, T x) {\n-    assert(p != NULL, \"null pointer\");\n-\n-    if (is_aligned(p, sizeof(T))) {\n-      *(T*)p = x;\n-    } else {\n-      memcpy(p, &x, sizeof(T));\n-    }\n-  }\n-\n-  static inline u2   get_native_u2(address p)         { return get_native<u2>((void*)p); }\n-  static inline u4   get_native_u4(address p)         { return get_native<u4>((void*)p); }\n-  static inline u8   get_native_u8(address p)         { return get_native<u8>((void*)p); }\n-  static inline void put_native_u2(address p, u2 x)   { put_native<u2>((void*)p, x); }\n-  static inline void put_native_u4(address p, u4 x)   { put_native<u4>((void*)p, x); }\n-  static inline void put_native_u8(address p, u8 x)   { put_native<u8>((void*)p, x); }\n-\n-  \/\/ Efficient reading and writing of unaligned unsigned data in Java\n-  \/\/ byte ordering (i.e. big-endian ordering). Byte-order reversal is\n-  \/\/ needed since x86 CPUs use little-endian format.\n-  template <typename T>\n-  static inline T get_Java(const address p) {\n-    T x = get_native<T>(p);\n-\n-    if (Endian::is_Java_byte_ordering_different()) {\n-      x = swap<T>(x);\n-    }\n-\n-    return x;\n-  }\n-\n-  template <typename T>\n-  static inline void put_Java(address p, T x) {\n-    if (Endian::is_Java_byte_ordering_different()) {\n-      x = swap<T>(x);\n-    }\n-\n-    put_native<T>(p, x);\n-  }\n-\n-  static inline u2   get_Java_u2(address p)           { return get_Java<u2>(p); }\n-  static inline u4   get_Java_u4(address p)           { return get_Java<u4>(p); }\n-  static inline u8   get_Java_u8(address p)           { return get_Java<u8>(p); }\n-\n-  static inline void put_Java_u2(address p, u2 x)     { put_Java<u2>(p, x); }\n-  static inline void put_Java_u4(address p, u4 x)     { put_Java<u4>(p, x); }\n-  static inline void put_Java_u8(address p, u8 x)     { put_Java<u8>(p, x); }\n-\n-  \/\/ Efficient swapping of byte ordering\n-  template <typename T>\n-  static T swap(T x) {\n-    switch (sizeof(T)) {\n-    case sizeof(u1): return x;\n-    case sizeof(u2): return swap_u2(x);\n-    case sizeof(u4): return swap_u4(x);\n-    case sizeof(u8): return swap_u8(x);\n-    default:\n-      guarantee(false, \"invalid size: \" SIZE_FORMAT \"\\n\", sizeof(T));\n-      return 0;\n-    }\n-  }\n-\n-  static inline u2   swap_u2(u2 x);                   \/\/ compiler-dependent implementation\n-  static inline u4   swap_u4(u4 x);                   \/\/ compiler-dependent implementation\n-  static inline u8   swap_u8(u8 x);\n-};\n-\n-\/\/ The following header contains the implementations of swap_u2, swap_u4, and swap_u8[_base]\n-#include OS_CPU_HEADER(bytes)\n-\n-#endif \/\/ CPU_X86_BYTES_X86_HPP\n","filename":"src\/hotspot\/cpu\/x86\/bytes_x86.hpp","additions":0,"deletions":127,"binary":false,"changes":127,"status":"deleted"},{"patch":"@@ -1,163 +0,0 @@\n-\/*\n- * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright 2007, 2008, 2009 Red Hat, Inc.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#ifndef CPU_ZERO_BYTES_ZERO_HPP\n-#define CPU_ZERO_BYTES_ZERO_HPP\n-\n-#include \"memory\/allStatic.hpp\"\n-\n-typedef union unaligned {\n-  u4 u;\n-  u2 us;\n-  u8 ul;\n-} __attribute__((packed)) unaligned;\n-\n-class Bytes: AllStatic {\n- public:\n-  \/\/ Efficient reading and writing of unaligned unsigned data in\n-  \/\/ platform-specific byte ordering.\n-  static inline u2 get_native_u2(address p){\n-    unaligned *up = (unaligned *) p;\n-    return up->us;\n-  }\n-\n-  static inline u4 get_native_u4(address p) {\n-    unaligned *up = (unaligned *) p;\n-    return up->u;\n-  }\n-\n-  static inline u8 get_native_u8(address p) {\n-    unaligned *up = (unaligned *) p;\n-    return up->ul;\n-  }\n-\n-  static inline void put_native_u2(address p, u2 x) {\n-    unaligned *up = (unaligned *) p;\n-    up->us = x;\n-  }\n-\n-  static inline void put_native_u4(address p, u4 x) {\n-    unaligned *up = (unaligned *) p;\n-    up->u = x;\n-  }\n-\n-  static inline void put_native_u8(address p, u8 x) {\n-    unaligned *up = (unaligned *) p;\n-    up->ul = x;\n-  }\n-\n-  \/\/ Efficient reading and writing of unaligned unsigned data in Java\n-  \/\/ byte ordering (i.e. big-endian ordering).\n-#ifdef VM_LITTLE_ENDIAN\n-  \/\/ Byte-order reversal is needed\n-  static inline u2 get_Java_u2(address p) {\n-    return (u2(p[0]) << 8) |\n-           (u2(p[1])     );\n-  }\n-  static inline u4 get_Java_u4(address p) {\n-    return (u4(p[0]) << 24) |\n-           (u4(p[1]) << 16) |\n-           (u4(p[2]) <<  8) |\n-           (u4(p[3])      );\n-  }\n-  static inline u8 get_Java_u8(address p) {\n-    u4 hi, lo;\n-    hi = (u4(p[0]) << 24) |\n-         (u4(p[1]) << 16) |\n-         (u4(p[2]) <<  8) |\n-         (u4(p[3])      );\n-    lo = (u4(p[4]) << 24) |\n-         (u4(p[5]) << 16) |\n-         (u4(p[6]) <<  8) |\n-         (u4(p[7])      );\n-    return u8(lo) | (u8(hi) << 32);\n-  }\n-\n-  static inline void put_Java_u2(address p, u2 x) {\n-    p[0] = x >> 8;\n-    p[1] = x;\n-  }\n-  static inline void put_Java_u4(address p, u4 x) {\n-    p[0] = x >> 24;\n-    p[1] = x >> 16;\n-    p[2] = x >> 8;\n-    p[3] = x;\n-  }\n-  static inline void put_Java_u8(address p, u8 x) {\n-    u4 hi, lo;\n-    lo = x;\n-    hi = x >> 32;\n-    p[0] = hi >> 24;\n-    p[1] = hi >> 16;\n-    p[2] = hi >> 8;\n-    p[3] = hi;\n-    p[4] = lo >> 24;\n-    p[5] = lo >> 16;\n-    p[6] = lo >> 8;\n-    p[7] = lo;\n-  }\n-\n-  \/\/ Efficient swapping of byte ordering\n-  static inline u2 swap_u2(u2 x);\n-  static inline u4 swap_u4(u4 x);\n-  static inline u8 swap_u8(u8 x);\n-#else\n-  \/\/ No byte-order reversal is needed\n-  static inline u2 get_Java_u2(address p) {\n-    return get_native_u2(p);\n-  }\n-  static inline u4 get_Java_u4(address p) {\n-    return get_native_u4(p);\n-  }\n-  static inline u8 get_Java_u8(address p) {\n-    return get_native_u8(p);\n-  }\n-\n-  static inline void put_Java_u2(address p, u2 x) {\n-    put_native_u2(p, x);\n-  }\n-  static inline void put_Java_u4(address p, u4 x) {\n-    put_native_u4(p, x);\n-  }\n-  static inline void put_Java_u8(address p, u8 x) {\n-    put_native_u8(p, x);\n-  }\n-\n-  \/\/ No byte-order reversal is needed\n-  static inline u2 swap_u2(u2 x) { return x; }\n-  static inline u4 swap_u4(u4 x) { return x; }\n-  static inline u8 swap_u8(u8 x) { return x; }\n-#endif \/\/ VM_LITTLE_ENDIAN\n-};\n-\n-#ifdef VM_LITTLE_ENDIAN\n-\/\/ The following header contains the implementations of swap_u2,\n-\/\/ swap_u4, and swap_u8\n-\n-#include OS_CPU_HEADER(bytes)\n-\n-#endif \/\/ VM_LITTLE_ENDIAN\n-\n-#endif \/\/ CPU_ZERO_BYTES_ZERO_HPP\n","filename":"src\/hotspot\/cpu\/zero\/bytes_zero.hpp","additions":0,"deletions":163,"binary":false,"changes":163,"status":"deleted"},{"patch":"@@ -1,32 +0,0 @@\n-\/*\n- * Copyright (c) 2016, 2019, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#ifndef OS_CPU_AIX_PPC_BYTES_AIX_PPC_HPP\n-#define OS_CPU_AIX_PPC_BYTES_AIX_PPC_HPP\n-\n-#if defined(VM_LITTLE_ENDIAN)\n-\/\/ Aix is not little endian.\n-#endif \/\/ VM_LITTLE_ENDIAN\n-\n-#endif \/\/ OS_CPU_AIX_PPC_BYTES_AIX_PPC_HPP\n","filename":"src\/hotspot\/os_cpu\/aix_ppc\/bytes_aix_ppc.hpp","additions":0,"deletions":32,"binary":false,"changes":32,"status":"deleted"},{"patch":"@@ -1,56 +0,0 @@\n-\/*\n- * Copyright (c) 1999, 2021, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2014, Red Hat Inc. All rights reserved.\n- * Copyright (c) 2021, Azul Systems, Inc. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#ifndef OS_CPU_BSD_AARCH64_BYTES_BSD_AARCH64_HPP\n-#define OS_CPU_BSD_AARCH64_BYTES_BSD_AARCH64_HPP\n-\n-#ifdef __APPLE__\n-#include <libkern\/OSByteOrder.h>\n-#endif\n-\n-#if defined(__APPLE__)\n-#  define bswap_16(x) OSSwapInt16(x)\n-#  define bswap_32(x) OSSwapInt32(x)\n-#  define bswap_64(x) OSSwapInt64(x)\n-#else\n-#  error \"Unimplemented\"\n-#endif\n-\n-\/\/ Efficient swapping of data bytes from Java byte\n-\/\/ ordering to native byte ordering and vice versa.\n-inline u2   Bytes::swap_u2(u2 x) {\n-  return bswap_16(x);\n-}\n-\n-inline u4   Bytes::swap_u4(u4 x) {\n-  return bswap_32(x);\n-}\n-\n-inline u8 Bytes::swap_u8(u8 x) {\n-  return bswap_64(x);\n-}\n-\n-#endif \/\/ OS_CPU_BSD_AARCH64_BYTES_BSD_AARCH64_HPP\n","filename":"src\/hotspot\/os_cpu\/bsd_aarch64\/bytes_bsd_aarch64.hpp","additions":0,"deletions":56,"binary":false,"changes":56,"status":"deleted"},{"patch":"@@ -1,101 +0,0 @@\n-\/*\n- * Copyright (c) 1999, 2020, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#ifndef OS_CPU_BSD_X86_BYTES_BSD_X86_HPP\n-#define OS_CPU_BSD_X86_BYTES_BSD_X86_HPP\n-\n-#ifdef __APPLE__\n-#include <libkern\/OSByteOrder.h>\n-#endif\n-\n-#if defined(AMD64)\n-#  if defined(__APPLE__)\n-#    define bswap_16(x) OSSwapInt16(x)\n-#    define bswap_32(x) OSSwapInt32(x)\n-#    define bswap_64(x) OSSwapInt64(x)\n-#  elif defined(__OpenBSD__)\n-#    define bswap_16(x) swap16(x)\n-#    define bswap_32(x) swap32(x)\n-#    define bswap_64(x) swap64(x)\n-#  elif defined(__NetBSD__)\n-#    define bswap_16(x) bswap16(x)\n-#    define bswap_32(x) bswap32(x)\n-#    define bswap_64(x) bswap64(x)\n-#  else\n-#    define bswap_16(x) __bswap16(x)\n-#    define bswap_32(x) __bswap32(x)\n-#    define bswap_64(x) __bswap64(x)\n-#  endif\n-#endif\n-\n-\/\/ Efficient swapping of data bytes from Java byte\n-\/\/ ordering to native byte ordering and vice versa.\n-inline u2   Bytes::swap_u2(u2 x) {\n-#ifdef AMD64\n-  return bswap_16(x);\n-#else\n-  u2 ret;\n-  __asm__ __volatile__ (\n-    \"movw %0, %%ax;\"\n-    \"xchg %%al, %%ah;\"\n-    \"movw %%ax, %0\"\n-    :\"=r\" (ret)      \/\/ output : register 0 => ret\n-    :\"0\"  (x)        \/\/ input  : x => register 0\n-    :\"ax\", \"0\"       \/\/ clobbered registers\n-  );\n-  return ret;\n-#endif \/\/ AMD64\n-}\n-\n-inline u4   Bytes::swap_u4(u4 x) {\n-#ifdef AMD64\n-  return bswap_32(x);\n-#else\n-  u4 ret;\n-  __asm__ __volatile__ (\n-    \"bswap %0\"\n-    :\"=r\" (ret)      \/\/ output : register 0 => ret\n-    :\"0\"  (x)        \/\/ input  : x => register 0\n-    :\"0\"             \/\/ clobbered register\n-  );\n-  return ret;\n-#endif \/\/ AMD64\n-}\n-\n-#ifdef AMD64\n-inline u8 Bytes::swap_u8(u8 x) {\n-  return bswap_64(x);\n-}\n-#else\n-\/\/ Helper function for swap_u8\n-inline u8   Bytes::swap_u8_base(u4 x, u4 y) {\n-  return (((u8)swap_u4(x))<<32) | swap_u4(y);\n-}\n-\n-inline u8 Bytes::swap_u8(u8 x) {\n-  return swap_u8_base(*(u4*)&x, *(((u4*)&x)+1));\n-}\n-#endif \/\/ !AMD64\n-\n-#endif \/\/ OS_CPU_BSD_X86_BYTES_BSD_X86_HPP\n","filename":"src\/hotspot\/os_cpu\/bsd_x86\/bytes_bsd_x86.hpp","additions":0,"deletions":101,"binary":false,"changes":101,"status":"deleted"},{"patch":"@@ -1,67 +0,0 @@\n-\/*\n- * Copyright (c) 2003, 2019, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#ifndef OS_CPU_BSD_ZERO_BYTES_BSD_ZERO_HPP\n-#define OS_CPU_BSD_ZERO_BYTES_BSD_ZERO_HPP\n-\n-\/\/ Efficient swapping of data bytes from Java byte\n-\/\/ ordering to native byte ordering and vice versa.\n-\n-#ifdef __APPLE__\n-#  include <libkern\/OSByteOrder.h>\n-#else\n-#  include <sys\/endian.h>\n-#endif\n-\n-#if defined(__APPLE__)\n-#  define bswap_16(x)   OSSwapInt16(x)\n-#  define bswap_32(x)   OSSwapInt32(x)\n-#  define bswap_64(x)   OSSwapInt64(x)\n-#elif defined(__OpenBSD__)\n-#  define bswap_16(x)   swap16(x)\n-#  define bswap_32(x)   swap32(x)\n-#  define bswap_64(x)   swap64(x)\n-#elif defined(__NetBSD__)\n-#  define bswap_16(x)   bswap16(x)\n-#  define bswap_32(x)   bswap32(x)\n-#  define bswap_64(x)   bswap64(x)\n-#else\n-#  define bswap_16(x) __bswap16(x)\n-#  define bswap_32(x) __bswap32(x)\n-#  define bswap_64(x) __bswap64(x)\n-#endif\n-\n-inline u2 Bytes::swap_u2(u2 x) {\n-  return bswap_16(x);\n-}\n-\n-inline u4 Bytes::swap_u4(u4 x) {\n-  return bswap_32(x);\n-}\n-\n-inline u8 Bytes::swap_u8(u8 x) {\n-  return bswap_64(x);\n-}\n-\n-#endif \/\/ OS_CPU_BSD_ZERO_BYTES_BSD_ZERO_HPP\n","filename":"src\/hotspot\/os_cpu\/bsd_zero\/bytes_bsd_zero.hpp","additions":0,"deletions":67,"binary":false,"changes":67,"status":"deleted"},{"patch":"@@ -1,45 +0,0 @@\n-\/*\n- * Copyright (c) 1999, 2019, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2014, Red Hat Inc. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#ifndef OS_CPU_LINUX_AARCH64_BYTES_LINUX_AARCH64_HPP\n-#define OS_CPU_LINUX_AARCH64_BYTES_LINUX_AARCH64_HPP\n-\n-#include <byteswap.h>\n-\n-\/\/ Efficient swapping of data bytes from Java byte\n-\/\/ ordering to native byte ordering and vice versa.\n-inline u2   Bytes::swap_u2(u2 x) {\n-  return bswap_16(x);\n-}\n-\n-inline u4   Bytes::swap_u4(u4 x) {\n-  return bswap_32(x);\n-}\n-\n-inline u8 Bytes::swap_u8(u8 x) {\n-  return bswap_64(x);\n-}\n-\n-#endif \/\/ OS_CPU_LINUX_AARCH64_BYTES_LINUX_AARCH64_HPP\n","filename":"src\/hotspot\/os_cpu\/linux_aarch64\/bytes_linux_aarch64.hpp","additions":0,"deletions":45,"binary":false,"changes":45,"status":"deleted"},{"patch":"@@ -1,47 +0,0 @@\n-\/*\n- * Copyright (c) 2008, 2019, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#ifndef OS_CPU_LINUX_ARM_BYTES_LINUX_ARM_HPP\n-#define OS_CPU_LINUX_ARM_BYTES_LINUX_ARM_HPP\n-\n-#include <byteswap.h>\n-\n-\/\/ Efficient swapping of data bytes from Java byte\n-\/\/ ordering to native byte ordering and vice versa.\n-inline u2 Bytes::swap_u2(u2 x) {\n-  \/\/ TODO: ARM - optimize\n-  return bswap_16(x);\n-}\n-\n-inline u4 Bytes::swap_u4(u4 x) {\n-  \/\/ TODO: ARM - optimize\n-  return bswap_32(x);\n-}\n-\n-inline u8 Bytes::swap_u8(u8 x) {\n-  \/\/ TODO: ARM - optimize\n-  return bswap_64(x);\n-}\n-\n-#endif \/\/ OS_CPU_LINUX_ARM_BYTES_LINUX_ARM_HPP\n","filename":"src\/hotspot\/os_cpu\/linux_arm\/bytes_linux_arm.hpp","additions":0,"deletions":47,"binary":false,"changes":47,"status":"deleted"},{"patch":"@@ -1,39 +0,0 @@\n-\/*\n- * Copyright (c) 2002, 2019, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright 2014 Google Inc.  All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#ifndef OS_CPU_LINUX_PPC_BYTES_LINUX_PPC_HPP\n-#define OS_CPU_LINUX_PPC_BYTES_LINUX_PPC_HPP\n-\n-#if defined(VM_LITTLE_ENDIAN)\n-#include <byteswap.h>\n-\n-\/\/ Efficient swapping of data bytes from Java byte\n-\/\/ ordering to native byte ordering and vice versa.\n-inline u2 Bytes::swap_u2(u2 x) { return bswap_16(x); }\n-inline u4 Bytes::swap_u4(u4 x) { return bswap_32(x); }\n-inline u8 Bytes::swap_u8(u8 x) { return bswap_64(x); }\n-#endif \/\/ VM_LITTLE_ENDIAN\n-\n-#endif \/\/ OS_CPU_LINUX_PPC_BYTES_LINUX_PPC_HPP\n","filename":"src\/hotspot\/os_cpu\/linux_ppc\/bytes_linux_ppc.hpp","additions":0,"deletions":39,"binary":false,"changes":39,"status":"deleted"},{"patch":"@@ -1,45 +0,0 @@\n-\/*\n- * Copyright (c) 1999, 2019, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2020, 2021, Huawei Technologies Co., Ltd. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#ifndef OS_CPU_LINUX_RISCV_BYTES_LINUX_RISCV_HPP\n-#define OS_CPU_LINUX_RISCV_BYTES_LINUX_RISCV_HPP\n-\n-#include <byteswap.h>\n-\n-\/\/ Efficient swapping of data bytes from Java byte\n-\/\/ ordering to native byte ordering and vice versa.\n-inline u2   Bytes::swap_u2(u2 x) {\n-  return bswap_16(x);\n-}\n-\n-inline u4   Bytes::swap_u4(u4 x) {\n-  return bswap_32(x);\n-}\n-\n-inline u8 Bytes::swap_u8(u8 x) {\n-  return bswap_64(x);\n-}\n-\n-#endif \/\/ OS_CPU_LINUX_RISCV_BYTES_LINUX_RISCV_HPP\n","filename":"src\/hotspot\/os_cpu\/linux_riscv\/bytes_linux_riscv.hpp","additions":0,"deletions":45,"binary":false,"changes":45,"status":"deleted"},{"patch":"@@ -1,46 +0,0 @@\n-\/*\n- * Copyright (c) 2016, 2019, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2016 SAP SE. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#ifndef OS_CPU_LINUX_S390_BYTES_LINUX_S390_HPP\n-#define OS_CPU_LINUX_S390_BYTES_LINUX_S390_HPP\n-\n-\/\/ Efficient swapping of data bytes from Java byte\n-\/\/ ordering to native byte ordering and vice versa.\n-\n-#include <byteswap.h>\n-\n-inline u2 swap_u2(u2 x) {\n-  return bswap_16(x);\n-}\n-\n-inline u4 swap_u4(u4 x) {\n-  return bswap_32(x);\n-}\n-\n-inline u8 swap_u8(u8 x) {\n-  return bswap_64(x);\n-}\n-\n-#endif \/\/ OS_CPU_LINUX_S390_BYTES_LINUX_S390_HPP\n","filename":"src\/hotspot\/os_cpu\/linux_s390\/bytes_linux_s390.hpp","additions":0,"deletions":46,"binary":false,"changes":46,"status":"deleted"},{"patch":"@@ -1,79 +0,0 @@\n-\/*\n- * Copyright (c) 1999, 2020, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#ifndef OS_CPU_LINUX_X86_BYTES_LINUX_X86_HPP\n-#define OS_CPU_LINUX_X86_BYTES_LINUX_X86_HPP\n-\n-#include <byteswap.h>\n-\n-\/\/ Efficient swapping of data bytes from Java byte\n-\/\/ ordering to native byte ordering and vice versa.\n-inline u2   Bytes::swap_u2(u2 x) {\n-#ifdef AMD64\n-  return bswap_16(x);\n-#else\n-  u2 ret;\n-  __asm__ __volatile__ (\n-    \"movw %0, %%ax;\"\n-    \"xchg %%al, %%ah;\"\n-    \"movw %%ax, %0\"\n-    :\"=r\" (ret)      \/\/ output : register 0 => ret\n-    :\"0\"  (x)        \/\/ input  : x => register 0\n-    :\"ax\", \"0\"       \/\/ clobbered registers\n-  );\n-  return ret;\n-#endif \/\/ AMD64\n-}\n-\n-inline u4   Bytes::swap_u4(u4 x) {\n-#ifdef AMD64\n-  return bswap_32(x);\n-#else\n-  u4 ret;\n-  __asm__ __volatile__ (\n-    \"bswap %0\"\n-    :\"=r\" (ret)      \/\/ output : register 0 => ret\n-    :\"0\"  (x)        \/\/ input  : x => register 0\n-    :\"0\"             \/\/ clobbered register\n-  );\n-  return ret;\n-#endif \/\/ AMD64\n-}\n-\n-#ifdef AMD64\n-inline u8 Bytes::swap_u8(u8 x) {\n-  return bswap_64(x);\n-}\n-#else\n-\/\/ Helper function for swap_u8\n-inline u8   Bytes::swap_u8_base(u4 x, u4 y) {\n-  return (((u8)swap_u4(x))<<32) | swap_u4(y);\n-}\n-\n-inline u8 Bytes::swap_u8(u8 x) {\n-  return swap_u8_base(*(u4*)&x, *(((u4*)&x)+1));\n-}\n-#endif \/\/ !AMD64\n-\n-#endif \/\/ OS_CPU_LINUX_X86_BYTES_LINUX_X86_HPP\n","filename":"src\/hotspot\/os_cpu\/linux_x86\/bytes_linux_x86.hpp","additions":0,"deletions":79,"binary":false,"changes":79,"status":"deleted"},{"patch":"@@ -1,45 +0,0 @@\n-\/*\n- * Copyright (c) 2003, 2019, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#ifndef OS_CPU_LINUX_ZERO_BYTES_LINUX_ZERO_HPP\n-#define OS_CPU_LINUX_ZERO_BYTES_LINUX_ZERO_HPP\n-\n-\/\/ Efficient swapping of data bytes from Java byte\n-\/\/ ordering to native byte ordering and vice versa.\n-\n-#include <byteswap.h>\n-\n-inline u2 Bytes::swap_u2(u2 x) {\n-  return bswap_16(x);\n-}\n-\n-inline u4 Bytes::swap_u4(u4 x) {\n-  return bswap_32(x);\n-}\n-\n-inline u8 Bytes::swap_u8(u8 x) {\n-  return bswap_64(x);\n-}\n-\n-#endif \/\/ OS_CPU_LINUX_ZERO_BYTES_LINUX_ZERO_HPP\n","filename":"src\/hotspot\/os_cpu\/linux_zero\/bytes_linux_zero.hpp","additions":0,"deletions":45,"binary":false,"changes":45,"status":"deleted"},{"patch":"@@ -1,46 +0,0 @@\n-\/*\n- * Copyright (c) 2020, Microsoft Corporation. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#ifndef OS_CPU_WINDOWS_AARCH64_BYTES_WINDOWS_AARCH64_HPP\n-#define OS_CPU_WINDOWS_AARCH64_BYTES_WINDOWS_AARCH64_HPP\n-\n-#include <stdlib.h>\n-\n-\/\/ Efficient swapping of data bytes from Java byte\n-\/\/ ordering to native byte ordering and vice versa.\n-inline u2   Bytes::swap_u2(u2 x) {\n-  return _byteswap_ushort(x);\n-}\n-\n-inline u4   Bytes::swap_u4(u4 x) {\n-  return _byteswap_ulong(x);\n-}\n-\n-inline u8 Bytes::swap_u8(u8 x) {\n-  return _byteswap_uint64(x);\n-}\n-\n-#pragma warning(default: 4035) \/\/ Enable warning 4035: no return value\n-\n-#endif \/\/ OS_CPU_WINDOWS_AARCH64_BYTES_WINDOWS_AARCH64_HPP\n","filename":"src\/hotspot\/os_cpu\/windows_aarch64\/bytes_windows_aarch64.hpp","additions":0,"deletions":46,"binary":false,"changes":46,"status":"deleted"},{"patch":"@@ -1,44 +0,0 @@\n-\/*\n- * Copyright (c) 1998, 2019, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#ifndef OS_CPU_WINDOWS_X86_BYTES_WINDOWS_X86_HPP\n-#define OS_CPU_WINDOWS_X86_BYTES_WINDOWS_X86_HPP\n-\n-#include <stdlib.h>\n-\n-\/\/ Efficient swapping of data bytes from Java byte\n-\/\/ ordering to native byte ordering and vice versa.\n-inline u2 Bytes::swap_u2(u2 x) {\n-  return (u2) _byteswap_ushort((unsigned short) x);\n-}\n-\n-inline u4 Bytes::swap_u4(u4 x) {\n-  return (u4) _byteswap_ulong((unsigned long) x);\n-}\n-\n-inline u8 Bytes::swap_u8(u8 x) {\n-  return (u8) _byteswap_uint64((unsigned __int64) x);\n-}\n-\n-#endif \/\/ OS_CPU_WINDOWS_X86_BYTES_WINDOWS_X86_HPP\n","filename":"src\/hotspot\/os_cpu\/windows_x86\/bytes_windows_x86.hpp","additions":0,"deletions":44,"binary":false,"changes":44,"status":"deleted"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2014, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, 2019, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,0 +28,3 @@\n+#include \"memory\/allStatic.hpp\"\n+#include \"utilities\/byteswap.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -29,0 +32,1 @@\n+#include \"utilities\/unalignedAccess.hpp\"\n@@ -30,2 +34,2 @@\n-class Endian : AllStatic {\n-public:\n+class Endian final : public AllStatic {\n+ public:\n@@ -46,1 +50,1 @@\n-  static inline bool is_Java_byte_ordering_different() {\n+  static constexpr bool is_Java_byte_ordering_different() {\n@@ -51,1 +55,39 @@\n-#include CPU_HEADER(bytes)\n+template <typename T, bool D = Endian::is_Java_byte_ordering_different()>\n+struct BytesSwapImpl;\n+\n+template <typename T>\n+struct BytesSwapImpl<T, false> final {\n+  ALWAYSINLINE T operator()(T x) const {\n+    return x;\n+  }\n+};\n+\n+template <typename T>\n+struct BytesSwapImpl<T, true> final {\n+  ALWAYSINLINE T operator()(T x) const {\n+    return byteswap<T>(x);\n+  }\n+};\n+\n+class Bytes final : public AllStatic {\n+ public:\n+  static ALWAYSINLINE u2 get_native_u2(const void* p) { return UnalignedAccess::load<u2>(p); }\n+  static ALWAYSINLINE u4 get_native_u4(const void* p) { return UnalignedAccess::load<u4>(p); }\n+  static ALWAYSINLINE u8 get_native_u8(const void* p) { return UnalignedAccess::load<u8>(p); }\n+\n+  static ALWAYSINLINE void put_native_u2(void* p, u2 x) { UnalignedAccess::store<u2>(p, x); }\n+  static ALWAYSINLINE void put_native_u4(void* p, u4 x) { UnalignedAccess::store<u4>(p, x); }\n+  static ALWAYSINLINE void put_native_u8(void* p, u8 x) { UnalignedAccess::store<u8>(p, x); }\n+\n+  static ALWAYSINLINE u2 get_Java_u2(const void* p) { return BytesSwapImpl<u2>{}(get_native_u2(p)); }\n+  static ALWAYSINLINE u4 get_Java_u4(const void* p) { return BytesSwapImpl<u4>{}(get_native_u4(p)); }\n+  static ALWAYSINLINE u8 get_Java_u8(const void* p) { return BytesSwapImpl<u8>{}(get_native_u8(p)); }\n+\n+  static ALWAYSINLINE void put_Java_u2(void* p, u2 x) { put_native_u2(p, BytesSwapImpl<u2>{}(x)); }\n+  static ALWAYSINLINE void put_Java_u4(void* p, u4 x) { put_native_u4(p, BytesSwapImpl<u4>{}(x)); }\n+  static ALWAYSINLINE void put_Java_u8(void* p, u8 x) { put_native_u8(p, BytesSwapImpl<u8>{}(x)); }\n+\n+  static ALWAYSINLINE u2 swap_u2(u2 x) { return byteswap<u2>(x); }\n+  static ALWAYSINLINE u4 swap_u4(u4 x) { return byteswap<u4>(x); }\n+  static ALWAYSINLINE u8 swap_u8(u8 x) { return byteswap<u8>(x); }\n+};\n","filename":"src\/hotspot\/share\/utilities\/bytes.hpp","additions":47,"deletions":5,"binary":false,"changes":52,"status":"modified"},{"patch":"@@ -0,0 +1,254 @@\n+\/*\n+ * Copyright (c) 2023, Google and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_UTILITIES_BYTESWAP_HPP\n+#define SHARE_UTILITIES_BYTESWAP_HPP\n+\n+\/\/ Byte swapping for 8-bit, 16-bit, 32-bit, and 64-bit integers.\n+\n+\/\/ byteswap<T>()\n+\/\/\n+\/\/ Reverses the bytes for the value of the integer type T. Partially compatible with std::byteswap\n+\/\/ introduced in C++23.\n+\n+#include \"metaprogramming\/enableIf.hpp\"\n+#include \"utilities\/debug.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include \"utilities\/macros.hpp\"\n+\n+#include <cstddef>\n+#include <cstdint>\n+#include <type_traits>\n+\n+template <typename T>\n+struct CanByteswapImpl final\n+    : public std::integral_constant<bool, (std::is_integral<T>::value && sizeof(T) <= 8)> {};\n+\n+template <typename T, size_t N = sizeof(T)>\n+struct ByteswapImpl;\n+\n+template <typename T, ENABLE_IF(CanByteswapImpl<T>::value)>\n+ALWAYSINLINE T byteswap(T x) {\n+  using U = std::make_unsigned_t<T>;\n+  STATIC_ASSERT(sizeof(T) == sizeof(U));\n+  return static_cast<T>(ByteswapImpl<U>{}(static_cast<U>(x)));\n+}\n+\n+\/*****************************************************************************\n+ * Fallback\n+ *****************************************************************************\/\n+\n+template <typename T, size_t N = sizeof(T)>\n+struct ByteswapFallbackImpl;\n+\n+\/\/ We support 8-bit integer types to be compatible with C++23's std::byteswap.\n+template <typename T>\n+struct ByteswapFallbackImpl<T, 1> {\n+  STATIC_ASSERT(CanByteswapImpl<T>::value);\n+  STATIC_ASSERT(sizeof(T) == 1);\n+\n+  ALWAYSINLINE T operator()(T x) const {\n+    return x;\n+  }\n+};\n+\n+template <typename T>\n+struct ByteswapFallbackImpl<T, 2> {\n+  STATIC_ASSERT(CanByteswapImpl<T>::value);\n+  STATIC_ASSERT(sizeof(T) == 2);\n+\n+  ALWAYSINLINE uint16_t operator()(uint16_t x) const {\n+    return (((x & UINT16_C(0x00ff)) << 8) | ((x & UINT16_C(0xff00)) >> 8));\n+  }\n+};\n+\n+template <typename T>\n+struct ByteswapFallbackImpl<T, 4> {\n+  STATIC_ASSERT(CanByteswapImpl<T>::value);\n+  STATIC_ASSERT(sizeof(T) == 4);\n+\n+  ALWAYSINLINE uint32_t operator()(uint32_t x) const {\n+    return (((x & UINT32_C(0x000000ff)) << 24) | ((x & UINT32_C(0x0000ff00)) << 8) |\n+            ((x & UINT32_C(0x00ff0000)) >> 8) | ((x & UINT32_C(0xff000000)) >> 24));\n+  }\n+};\n+\n+template <typename T>\n+struct ByteswapFallbackImpl<T, 8> {\n+  STATIC_ASSERT(CanByteswapImpl<T>::value);\n+  STATIC_ASSERT(sizeof(T) == 8);\n+\n+  ALWAYSINLINE uint64_t operator()(uint64_t x) const {\n+    return (((x & UINT64_C(0x00000000000000ff)) << 56) | ((x & UINT64_C(0x000000000000ff00)) << 40) |\n+            ((x & UINT64_C(0x0000000000ff0000)) << 24) | ((x & UINT64_C(0x00000000ff000000)) << 8) |\n+            ((x & UINT64_C(0x000000ff00000000)) >> 8) | ((x & UINT64_C(0x0000ff0000000000)) >> 24) |\n+            ((x & UINT64_C(0x00ff000000000000)) >> 40) | ((x & UINT64_C(0xff00000000000000)) >> 56));\n+  }\n+};\n+\n+\/*****************************************************************************\n+ * GCC and compatible (including Clang)\n+ *****************************************************************************\/\n+#if defined(TARGET_COMPILER_gcc)\n+\n+#if defined(__clang__)\n+\n+\/\/ Unlike GCC, Clang is willing to inline the generic implementation of __builtin_bswap when\n+\/\/ architecture support is unavailable in -O2. This ensures we avoid the function call to libgcc.\n+\/\/ Clang is able to recognize the fallback implementation as byteswapping, but not on every\n+\/\/ architecture unlike GCC. This suggests the optimization pass for GCC that recognizes byteswapping\n+\/\/ is architecture agnostic, while for Clang it is not.\n+\n+\/\/ We support 8-bit integer types to be compatible with C++23's std::byteswap.\n+template <typename T>\n+struct ByteswapImpl<T, 1> final {\n+  STATIC_ASSERT(CanByteswapImpl<T>::value);\n+  STATIC_ASSERT(sizeof(T) == 1);\n+\n+  ALWAYSINLINE T operator()(T x) const {\n+    return x;\n+  }\n+};\n+\n+template <typename T>\n+struct ByteswapImpl<T, 2> final {\n+  STATIC_ASSERT(CanByteswapImpl<T>::value);\n+  STATIC_ASSERT(sizeof(T) == sizeof(uint16_t));\n+\n+  ALWAYSINLINE T operator()(T x) const {\n+    return static_cast<T>(__builtin_bswap16(static_cast<uint16_t>(x)));\n+  }\n+};\n+\n+template <typename T>\n+struct ByteswapImpl<T, 4> final {\n+  STATIC_ASSERT(CanByteswapImpl<T>::value);\n+  STATIC_ASSERT(sizeof(T) == sizeof(uint32_t));\n+\n+  ALWAYSINLINE T operator()(T x) const {\n+    return static_cast<T>(__builtin_bswap32(static_cast<uint32_t>(x)));\n+  }\n+};\n+\n+template <typename T>\n+struct ByteswapImpl<T, 8> final {\n+  STATIC_ASSERT(CanByteswapImpl<T>::value);\n+  STATIC_ASSERT(sizeof(T) == sizeof(uint64_t));\n+\n+  ALWAYSINLINE T operator()(T x) const {\n+    return static_cast<T>(__builtin_bswap64(static_cast<uint64_t>(x)));\n+  }\n+};\n+\n+#else\n+\n+\/\/ We do not use __builtin_bswap and friends for GCC. Unfortunately on architectures that do not\n+\/\/ have a byteswap instruction (i.e. RISC-V), GCC emits a function call to libgcc regardless of\n+\/\/ optimization options, even when the generic implementation is, for example, less than 20\n+\/\/ instructions. GCC is however able to recognize the fallback as byteswapping regardless of\n+\/\/ architecture and appropriately replaces the code in -O2 with the appropriate\n+\/\/ architecture-specific byteswap instruction, if available. If it is not available, GCC emits the\n+\/\/ exact same implementation that underpins its __builtin_bswap in libgcc as there is really only\n+\/\/ one way to implement it, as we have in fallback.\n+\n+template <typename T, size_t N>\n+struct ByteswapImpl final : public ByteswapFallbackImpl<T, N> {};\n+\n+#endif\n+\n+\/*****************************************************************************\n+ * Microsoft Visual Studio\n+ *****************************************************************************\/\n+#elif defined(TARGET_COMPILER_visCPP)\n+\n+#include <stdlib.h>\n+\n+#pragma intrinsic(_byteswap_ushort)\n+#pragma intrinsic(_byteswap_ulong)\n+#pragma intrinsic(_byteswap_uint64)\n+\n+\/\/ We support 8-bit integer types to be compatible with C++23's std::byteswap.\n+template <typename T>\n+struct ByteswapImpl<T, 1> final {\n+  STATIC_ASSERT(CanByteswapImpl<T>::value);\n+  STATIC_ASSERT(sizeof(T) == 1);\n+\n+  ALWAYSINLINE T operator()(T x) const {\n+    return x;\n+  }\n+};\n+\n+template <typename T>\n+struct ByteswapImpl<T, 2> final {\n+  STATIC_ASSERT(CanByteswapImpl<T>::value);\n+  STATIC_ASSERT(sizeof(unsigned short) == sizeof(2));\n+  STATIC_ASSERT(sizeof(T) == sizeof(unsigned short));\n+\n+  ALWAYSINLINE T operator()(T x) const {\n+    return static_cast<T>(_byteswap_ushort(static_cast<unsigned short>(x)));\n+  }\n+};\n+\n+template <typename T>\n+struct ByteswapImpl<T, 4> final {\n+  STATIC_ASSERT(CanByteswapImpl<T>::value);\n+  STATIC_ASSERT(sizeof(unsigned long) == sizeof(4));\n+  STATIC_ASSERT(sizeof(T) == sizeof(unsigned long));\n+\n+  ALWAYSINLINE T operator()(T x) const {\n+    return static_cast<T>(_byteswap_ulong(static_cast<unsigned long>(x)));\n+  }\n+};\n+\n+template <typename T>\n+struct ByteswapImpl<T, 8> final {\n+  STATIC_ASSERT(CanByteswapImpl<T>::value);\n+  STATIC_ASSERT(sizeof(unsigned __int64) == sizeof(8));\n+  STATIC_ASSERT(sizeof(T) == sizeof(unsigned __int64));\n+\n+  ALWAYSINLINE T operator()(T x) const {\n+    return static_cast<T>(_byteswap_uint64(static_cast<unsigned __int64>(x)));\n+  }\n+};\n+\n+\/*****************************************************************************\n+ * IBM XL C\/C++\n+ *****************************************************************************\/\n+#elif defined(TARGET_COMPILER_xlc)\n+\n+\/\/ To our knowledge XL C\/C++ does not have a compiler intrinsic for byteswapping.\n+\n+template <typename T, size_t N>\n+struct ByteswapImpl final : public ByteswapFallbackImpl<T, N> {};\n+\n+\/*****************************************************************************\n+ * Unknown toolchain\n+ *****************************************************************************\/\n+#else\n+\n+#error Unknown toolchain.\n+\n+#endif\n+\n+#endif \/\/ SHARE_UTILITIES_BYTESWAP_HPP\n","filename":"src\/hotspot\/share\/utilities\/byteswap.hpp","additions":254,"deletions":0,"binary":false,"changes":254,"status":"added"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"utilities\/byteswap.hpp\"\n@@ -87,27 +88,0 @@\n-  \/**\n-   * Byte swap a 16-bit value\n-   *\/\n-  static uint16_t byte_swap(uint16_t x) {\n-    return (x << 8) | (x >> 8);\n-  }\n-\n-  \/**\n-   * Byte swap a 32-bit value\n-   *\/\n-  static uint32_t byte_swap(uint32_t x) {\n-    uint16_t lo = (uint16_t)x;\n-    uint16_t hi = (uint16_t)(x >> 16);\n-\n-    return ((uint32_t)byte_swap(lo) << 16) | (uint32_t)byte_swap(hi);\n-  }\n-\n-  \/**\n-   * Byte swap a 64-bit value\n-   *\/\n-  static uint64_t byte_swap(uint64_t x) {\n-    uint32_t lo = (uint32_t)x;\n-    uint32_t hi = (uint32_t)(x >> 32);\n-\n-    return ((uint64_t)byte_swap(lo) << 32) | (uint64_t)byte_swap(hi);\n-  }\n-\n@@ -157,1 +131,1 @@\n-        tmp = byte_swap(tmp);\n+        tmp = byteswap<T>(tmp);\n","filename":"src\/hotspot\/share\/utilities\/copy.cpp","additions":2,"deletions":28,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -638,0 +638,7 @@\n+\/\/ Macro which can be used to detect the availability of builtins. Supports GCC 10+ and Clang.\n+#ifdef __has_builtin\n+#define HAS_BUILTIN(x) __has_builtin(x)\n+#else\n+#define HAS_BUILTIN(x) 0\n+#endif\n+\n","filename":"src\/hotspot\/share\/utilities\/macros.hpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -0,0 +1,244 @@\n+\/*\n+ * Copyright (c) 2023, Google and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_UTILITIES_UNALIGNED_ACCESS_HPP\n+#define SHARE_UTILITIES_UNALIGNED_ACCESS_HPP\n+\n+\/\/ Support for well defined potentially unaligned memory access, regardless of underlying\n+\/\/ architecture support.\n+\/\/\n+\/\/ Unaligned access is undefined behavior according to the standard. Some architectures support\n+\/\/ aligned and unaligned memory accesses via the same instructions (i.e. x86, AArch64) while some do\n+\/\/ not (i.e. RISC-V). Compilers are free to assume that all memory accesses of a type T are done at\n+\/\/ a suitably aligned address for type T, that is an address aligned to alignof(T). This is not\n+\/\/ always the case, as there are use cases where we may want to access type T at a non-suitably\n+\/\/ aligned address. For example, when serializing scalar types to a buffer without padding.\n+\n+\/\/ UnalignedAccess<T>::load()\n+\/\/\n+\/\/ Loads the bits of the value of type T from the specified address. The address may or may not be\n+\/\/ suitably aligned for type T. T must be trivially copyable and must be default constructible.\n+\n+\/\/ UnalignedAccess<T>::store()\n+\/\/\n+\/\/ Stores the bits of the value of type T at the specified address. The address may or may not be\n+\/\/ suitably aligned for type T. T must be trivially copyable and must be default constructible.\n+\n+#include \"memory\/allStatic.hpp\"\n+#include \"metaprogramming\/enableIf.hpp\"\n+#include \"utilities\/debug.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include \"utilities\/macros.hpp\"\n+\n+#include <cstddef>\n+#include <cstdint>\n+#include <cstring>\n+#include <type_traits>\n+\n+#if defined(ADDRESS_SANITIZER)\n+#include <sanitizer\/common_interface_defs.h>\n+#endif\n+\n+template <typename T>\n+struct CanAccessUnalignedImpl final\n+    : public std::integral_constant<bool, (std::is_trivially_copyable<T>::value &&\n+                                           std::is_default_constructible<T>::value)> {};\n+\n+template <typename T, size_t S = sizeof(T), size_t A = alignof(T)>\n+struct UnalignedLoadImpl final {\n+  ALWAYSINLINE T operator()(const void* p) const {\n+    STATIC_ASSERT(CanAccessUnalignedImpl<T>::value);\n+    STATIC_ASSERT(sizeof(T) == S);\n+    STATIC_ASSERT(alignof(T) == A);\n+#if (defined(__GNUC__) && !defined(__clang__)) || HAS_BUILTIN(__builtin_memcpy)\n+    \/\/ When available, explicitly prefer the builtin memcpy variant. This ensures GCC\/Clang will\n+    \/\/ do its best at generating optimal machine code regardless of build options. For architectures\n+    \/\/ which support unaligned access, this typically results in a single instruction. For other\n+    \/\/ architectures, GCC\/Clang will attempt to determine if the access is aligned first at compile\n+    \/\/ time and generate a single instruction otherwise it will fallback to a more general approach.\n+    T x;\n+    __builtin_memcpy(&x, p, S);\n+    return x;\n+#elif defined(_MSC_VER)\n+    return *static_cast<__unaligned const T*>(p);\n+#else\n+    \/\/ Most compilers will generate optimal machine code.\n+    T x;\n+    std::memcpy(&x, p, S);\n+    return x;\n+#endif\n+  }\n+};\n+\n+template <typename T, size_t S = sizeof(T), size_t A = alignof(T)>\n+struct UnalignedStoreImpl final {\n+  ALWAYSINLINE void operator()(void* p, T x) const {\n+    STATIC_ASSERT(CanAccessUnalignedImpl<T>::value);\n+    STATIC_ASSERT(sizeof(T) == S);\n+    STATIC_ASSERT(alignof(T) == A);\n+#if (defined(__GNUC__) && !defined(__clang__)) || HAS_BUILTIN(__builtin_memcpy)\n+    \/\/ When available, explicitly prefer the builtin memcpy variant. This ensures GCC\/Clang will\n+    \/\/ do its best at generating optimal machine code regardless of build options. For architectures\n+    \/\/ which support unaligned access, this typically results in a single instruction. For other\n+    \/\/ architectures, GCC\/Clang will attempt to determine if the access is aligned first at compile\n+    \/\/ time and generate a single instruction otherwise it will fallback to a more general approach.\n+    __builtin_memcpy(p, &x, S);\n+#elif defined(_MSC_VER)\n+    *static_cast<__unaligned T*>(p) = x;\n+#else\n+    \/\/ Most compilers will generate optimal machine code.\n+    std::memcpy(p, &x, S);\n+#endif\n+  }\n+};\n+\n+\/\/ Loads for types with an alignment of 1 byte are always aligned, but for simplicity of\n+\/\/ metaprogramming we accept them in UnalignedAccess.\n+template <typename T, size_t S>\n+struct UnalignedLoadImpl<T, S, 1> final {\n+  ALWAYSINLINE T operator()(const void* p) const {\n+    STATIC_ASSERT(CanAccessUnalignedImpl<T>::value);\n+    STATIC_ASSERT(alignof(T) == 1);\n+    return *static_cast<const T*>(p);\n+  }\n+};\n+\n+\/\/ Stores for types with an alignment of 1 byte are always aligned, but for simplicity of\n+\/\/ metaprogramming we accept them in UnalignedAccess.\n+template <typename T, size_t S>\n+struct UnalignedStoreImpl<T, S, 1> final {\n+  ALWAYSINLINE void operator()(void* p, T x) const {\n+    STATIC_ASSERT(CanAccessUnalignedImpl<T>::value);\n+    STATIC_ASSERT(alignof(T) == 1);\n+    *static_cast<T*>(p) = x;\n+  }\n+};\n+\n+template <typename To>\n+struct UnalignedBitCastImpl final {\n+  template <typename From>\n+  ALWAYSINLINE To operator()(const From& from) const {\n+    STATIC_ASSERT(CanAccessUnalignedImpl<From>::value);\n+    STATIC_ASSERT(CanAccessUnalignedImpl<To>::value);\n+    STATIC_ASSERT(sizeof(To) == sizeof(From));\n+#if HAS_BUILTIN(__builtin_bit_cast)\n+    return __builtin_bit_cast(To, from);\n+#elif (defined(__GNUC__) && !defined(__clang__)) || HAS_BUILTIN(__builtin_memcpy)\n+    To to;\n+    __builtin_memcpy(&to, &from, sizeof(To));\n+    return to;\n+#else\n+    To to;\n+    std::memcpy(&to, &from, sizeof(To));\n+    return to;\n+#endif\n+  }\n+};\n+\n+#if defined(ADDRESS_SANITIZER)\n+\/\/ Intercept unaligned accesses of size 2, 4, and 8 for ASan which can miss some bugs related to\n+\/\/ unaligned accesses if these are not used.\n+\/\/\n+\/\/ NOTE: these should also be enabled for MSan and TSan as well when\/if we use those.\n+\n+template <typename T>\n+struct UnalignedLoadImpl<T, 2, 2> final {\n+  ALWAYSINLINE T operator()(const void* p) const {\n+    STATIC_ASSERT(CanAccessUnalignedImpl<T>::value);\n+    STATIC_ASSERT(sizeof(T) == 2);\n+    STATIC_ASSERT(alignof(T) == 2);\n+    return UnalignedBitCastImpl<T>{}(__sanitizer_unaligned_load16(p));\n+  }\n+};\n+\n+template <typename T>\n+struct UnalignedStoreImpl<T, 2, 2> final {\n+  ALWAYSINLINE void operator()(void* p, T x) const {\n+    STATIC_ASSERT(CanAccessUnalignedImpl<T>::value);\n+    STATIC_ASSERT(sizeof(T) == 2);\n+    STATIC_ASSERT(alignof(T) == 2);\n+    __sanitizer_unaligned_store16(p, UnalignedBitCastImpl<uint16_t>{}(x));\n+  }\n+};\n+\n+template <typename T>\n+struct UnalignedLoadImpl<T, 4, 4> final {\n+  ALWAYSINLINE T operator()(const void* p) const {\n+    STATIC_ASSERT(CanAccessUnalignedImpl<T>::value);\n+    STATIC_ASSERT(sizeof(T) == 4);\n+    STATIC_ASSERT(alignof(T) == 4);\n+    return UnalignedBitCastImpl<T>{}(__sanitizer_unaligned_load32(p));\n+  }\n+};\n+\n+template <typename T>\n+struct UnalignedStoreImpl<T, 4, 4> final {\n+  ALWAYSINLINE void operator()(void* p, T x) const {\n+    STATIC_ASSERT(CanAccessUnalignedImpl<T>::value);\n+    STATIC_ASSERT(sizeof(T) == 4);\n+    STATIC_ASSERT(alignof(T) == 4);\n+    __sanitizer_unaligned_store32(p, UnalignedBitCastImpl<uint32_t>{}(x));\n+  }\n+};\n+\n+template <typename T>\n+struct UnalignedLoadImpl<T, 8, 8> final {\n+  ALWAYSINLINE T operator()(const void* p) const {\n+    STATIC_ASSERT(CanAccessUnalignedImpl<T>::value);\n+    STATIC_ASSERT(sizeof(T) == 8);\n+    STATIC_ASSERT(alignof(T) == 8);\n+    return UnalignedBitCastImpl<T>{}(__sanitizer_unaligned_load64(p));\n+  }\n+};\n+\n+template <typename T>\n+struct UnalignedStoreImpl<T, 8, 8> final {\n+  ALWAYSINLINE void operator()(void* p, T x) const {\n+    STATIC_ASSERT(CanAccessUnalignedImpl<T>::value);\n+    STATIC_ASSERT(sizeof(T) == 8);\n+    STATIC_ASSERT(alignof(T) == 8);\n+    __sanitizer_unaligned_store64(p, UnalignedBitCastImpl<uint64_t>{}(x));\n+  }\n+};\n+#endif\n+\n+class UnalignedAccess final : public AllStatic {\n+ public:\n+  \/\/ Load the bits of the value x of type T from the specified address p.\n+  template <typename T, ENABLE_IF(CanAccessUnalignedImpl<T>::value)>\n+  static ALWAYSINLINE T load(const void* p) {\n+    return UnalignedLoadImpl<T>{}(p);\n+  }\n+\n+  \/\/ Store the bits of the value x of type U, which must be implicitly convertible to type T, at the\n+  \/\/ specified address p. This approach requires explicitly specifying type T for readability,\n+  \/\/ rather than deriving type T from the argument.\n+  template <typename T, typename U = T, ENABLE_IF(CanAccessUnalignedImpl<T>::value &&\n+                                                  std::is_convertible<U, T>::value)>\n+  static ALWAYSINLINE void store(void* p, U x) {\n+    UnalignedStoreImpl<T>{}(p, x);\n+  }\n+};\n+\n+#endif \/\/ SHARE_UTILITIES_UNALIGNED_ACCESS_HPP\n","filename":"src\/hotspot\/share\/utilities\/unalignedAccess.hpp","additions":244,"deletions":0,"binary":false,"changes":244,"status":"added"}]}