{"files":[{"patch":"@@ -1334,0 +1334,5 @@\n+void Assembler::addb(Register dst, int imm8) {\n+  (void) prefix_and_encode(dst->encoding(), true);\n+  emit_arith_b(0x80, 0xC0, dst, imm8);\n+}\n+\n@@ -5321,0 +5326,12 @@\n+void Assembler::vpshufb(XMMRegister dst, XMMRegister nds, Address src, int vector_len) {\n+  assert(vector_len == AVX_128bit ? VM_Version::supports_avx() :\n+         vector_len == AVX_256bit ? VM_Version::supports_avx2() :\n+         vector_len == AVX_512bit ? VM_Version::supports_avx512bw() : 0, \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* rex_w *\/ false, \/* legacy_mode *\/ _legacy_mode_bw, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FVM, \/* input_size_in_bits *\/ EVEX_NObit);\n+  simd_prefix(dst, nds, src, VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8(0x00);\n+  emit_operand(dst, src, 0);\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.cpp","additions":17,"deletions":0,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -988,0 +988,1 @@\n+  void addb(Register dst, int imm8);\n@@ -1955,0 +1956,1 @@\n+  void vpshufb(XMMRegister dst, XMMRegister nds, Address src, int vector_len);\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -9282,0 +9282,11 @@\n+void MacroAssembler::vpshufb(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n+  if (reachable(src)) {\n+    vpshufb(dst, nds, as_Address(src), vector_len);\n+  } else {\n+    lea(rscratch, src);\n+    vpshufb(dst, nds, Address(rscratch, 0), vector_len);\n+  }\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":11,"deletions":0,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -1797,0 +1797,3 @@\n+  using Assembler::vpshufb;\n+  void vpshufb(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register rscratch = noreg);\n+\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -330,0 +330,4 @@\n+  \/\/ AVX2 AES Galois Counter Mode implementation\n+  address generate_avx2_galoisCounterMode_AESCrypt();\n+  void aesgcm_avx2(Register in, Register len, Register ct, Register out, Register key,\n+                   Register state, Register subkeyHtbl, Register counter);\n@@ -356,0 +360,11 @@\n+  \/\/ AVX2 AES-GCM related functions\n+  void initial_blocks_avx2(XMMRegister ctr, Register rounds, Register key, Register len,\n+                           Register in, Register out, Register ct, XMMRegister aad_hashx, Register pos);\n+  void gfmul_avx2(XMMRegister GH, XMMRegister HK);\n+  void generateHtbl_8_block_avx2(Register htbl);\n+  void ghash8_encrypt8_parallel_avx2(Register key, Register subkeyHtbl, XMMRegister ctr_blockx, Register in,\n+                                     Register out, Register ct, Register pos, bool out_order, Register rounds,\n+                                     XMMRegister xmm1, XMMRegister xmm2, XMMRegister xmm3, XMMRegister xmm4,\n+                                     XMMRegister xmm5, XMMRegister xmm6, XMMRegister xmm7, XMMRegister xmm8);\n+  void ghash_last_8_avx2(Register subkeyHtbl);\n+\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.hpp","additions":15,"deletions":0,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n-* Copyright (c) 2019, 2021, Intel Corporation. All rights reserved.\n+* Copyright (c) 2019, 2023, Intel Corporation. All rights reserved.\n@@ -84,0 +84,24 @@\n+ATTRIBUTE_ALIGNED(16) uint64_t COUNTER_MASK_LINC1F[] = {\n+    0x0000000000000000UL, 0x0100000000000000UL,\n+};\n+\n+static address counter_mask_linc1f_addr() {\n+  return (address)COUNTER_MASK_LINC1F;\n+}\n+\n+ATTRIBUTE_ALIGNED(16) uint64_t COUNTER_MASK_LINC2[] = {\n+    0x0000000000000002UL, 0x0000000000000000UL,\n+};\n+\n+static address counter_mask_linc2_addr() {\n+  return (address)COUNTER_MASK_LINC2;\n+}\n+\n+ATTRIBUTE_ALIGNED(16) uint64_t COUNTER_MASK_LINC2F[] = {\n+    0x0000000000000000UL, 0x0200000000000000UL,\n+};\n+\n+static address counter_mask_linc2f_addr() {\n+  return (address)COUNTER_MASK_LINC2F;\n+}\n+\n@@ -166,0 +190,3 @@\n+      if (VM_Version::supports_avx2()) {\n+          StubRoutines::_galoisCounterMode_AESCrypt = generate_avx2_galoisCounterMode_AESCrypt();\n+      }\n@@ -267,0 +294,84 @@\n+\/\/ AVX2 Vector AES Galois Counter Mode implementation.\n+\/\/\n+\/\/ Inputs:           Windows    |   Linux\n+\/\/   in         = rcx (c_rarg0) | rsi (c_rarg0)\n+\/\/   len        = rdx (c_rarg1) | rdi (c_rarg1)\n+\/\/   ct         = r8  (c_rarg2) | rdx (c_rarg2)\n+\/\/   out        = r9  (c_rarg3) | rcx (c_rarg3)\n+\/\/   key        = rdi           | r8  (c_rarg4)\n+\/\/   state      = r13           | r9  (c_rarg5)\n+\/\/   subkeyHtbl = r11           | r11\n+\/\/   counter    = rsi           | r12\n+\/\/\n+\/\/ Output:\n+\/\/   rax - number of processed bytes\n+address StubGenerator::generate_avx2_galoisCounterMode_AESCrypt() {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"galoisCounterMode_AESCrypt\");\n+  address start = __ pc();\n+\n+  const Register in = c_rarg0;\n+  const Register len = c_rarg1;\n+  const Register ct = c_rarg2;\n+  const Register out = c_rarg3;\n+  \/\/ and updated with the incremented counter in the end\n+ #ifndef _WIN64\n+  const Register key = c_rarg4;\n+  const Register state = c_rarg5;\n+  const Address subkeyH_mem(rbp, 2 * wordSize);\n+  const Register subkeyHtbl = r11;\n+  const Address counter_mem(rbp, 3 * wordSize);\n+  const Register counter = r12;\n+ #else\n+  const Address key_mem(rbp, 6 * wordSize);\n+  const Register key = rdi;\n+  const Address state_mem(rbp, 7 * wordSize);\n+  const Register state = r13;\n+  const Address subkeyH_mem(rbp, 8 * wordSize);\n+  const Register subkeyHtbl = r11;\n+  const Address counter_mem(rbp, 9 * wordSize);\n+  const Register counter = rsi;\n+ #endif\n+  __ enter();\n+  \/\/ Save state before entering routine\n+  __ push(r12);\n+  __ push(r13);\n+  __ push(r14);\n+  __ push(r15);\n+  __ push(rbx);\n+#ifdef _WIN64\n+  \/\/ on win64, fill len_reg from stack position\n+  __ push(rsi);\n+  __ push(rdi);\n+  __ movptr(key, key_mem);\n+  __ movptr(state, state_mem);\n+#endif\n+  __ movptr(subkeyHtbl, subkeyH_mem);\n+  __ movptr(counter, counter_mem);\n+\n+  \/\/ Save rsp\n+  __ movq(r14, rsp);\n+  \/\/ Align stack\n+  __ andq(rsp, -64);\n+  __ subptr(rsp, 16 * longSize); \/\/ Create space on the stack for saving AES entries\n+\n+  aesgcm_avx2(in, len, ct, out, key, state, subkeyHtbl, counter);\n+  __ vzeroupper();\n+  __ movq(rsp, r14);\n+  \/\/ Restore state before leaving routine\n+ #ifdef _WIN64\n+  __ pop(rdi);\n+  __ pop(rsi);\n+ #endif\n+  __ pop(rbx);\n+  __ pop(r15);\n+  __ pop(r14);\n+  __ pop(r13);\n+  __ pop(r12);\n+\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n+\n+  return start;\n+}\n+\n@@ -3180,0 +3291,540 @@\n+\/\/Implements data * hashkey mod (128, 127, 126, 121, 0)\n+\/\/Inputs:\n+\/\/GH and HK - 128 bits each\n+\/\/Output:\n+\/\/GH = GH * Hashkey mod poly\n+\/\/Temp registers: xmm1, xmm2, xmm3, r15\n+void StubGenerator::gfmul_avx2(XMMRegister GH, XMMRegister HK) {\n+  const XMMRegister T1 = xmm1;\n+  const XMMRegister T2 = xmm2;\n+  const XMMRegister T3 = xmm3;\n+\n+  __ vpclmulqdq(T1, GH, HK, 0x11); \/\/ %%T1 = a1*b1\n+  __ vpclmulqdq(T2, GH, HK, 0x00); \/\/ %%T2 = a0*b0\n+  __ vpclmulqdq(T3, GH, HK, 0x01); \/\/ %%T3 = a1*b0\n+  __ vpclmulqdq(GH, GH, HK, 0x10); \/\/ %%GH = a0*b1\n+  __ vpxor(GH, GH, T3, Assembler::AVX_128bit);\n+\n+  __ vpsrldq(T3, GH, 8, Assembler::AVX_128bit); \/\/ shift-R %%GH 2 DWs\n+  __ vpslldq(GH, GH, 8, Assembler::AVX_128bit); \/\/ shift-L %%GH 2 DWs\n+\n+  __ vpxor(T1, T1, T3, Assembler::AVX_128bit);\n+  __ vpxor(GH, GH, T2, Assembler::AVX_128bit);\n+\n+  \/\/first phase of the reduction\n+  __ movdqu(T3, ExternalAddress(ghash_polynomial_reduction_addr()), r15 \/*rscratch*\/);\n+  __ vpclmulqdq(T2, T3, GH, 0x01);\n+  __ vpslldq(T2, T2, 8, Assembler::AVX_128bit); \/\/ shift-L %%T2 2 DWs\n+\n+  __ vpxor(GH, GH, T2, Assembler::AVX_128bit); \/\/ first phase of the reduction complete\n+  \/\/second phase of the reduction\n+  __ vpclmulqdq(T2, T3, GH, 0x00);\n+  __ vpsrldq(T2, T2, 4, Assembler::AVX_128bit); \/\/ shift-R %%T2 1 DW (Shift-R only 1-DW to obtain 2-DWs shift-R)\n+\n+  __ vpclmulqdq(GH, T3, GH, 0x10);\n+  __ vpslldq(GH, GH, 4, Assembler::AVX_128bit); \/\/ shift-L %%GH 1 DW (Shift-L 1-DW to obtain result with no shifts)\n+\n+  __ vpxor(GH, GH, T2, Assembler::AVX_128bit); \/\/ second phase of the reduction complete\n+  __ vpxor(GH, GH, T1, Assembler::AVX_128bit); \/\/ the result is in %%GH\n+}\n+\n+\/\/Generate 8 constants from the given subkeyH.\n+\/\/Input:\n+\/\/htbl - table containing the initial subkeyH\n+\/\/Output:\n+\/\/htbl - containing 8 H constants\n+\/\/Temp registers: xmm0, xmm1, xmm2, xmm3, xmm6, xmm11, xmm12, r15, rbx\n+void StubGenerator::generateHtbl_8_block_avx2(Register htbl) {\n+  const XMMRegister HK = xmm6;\n+\n+  __ movdqu(HK, Address(htbl, 0));\n+  __ movdqu(xmm1, ExternalAddress(ghash_long_swap_mask_addr()), rbx \/*rscratch*\/);\n+  __ vpshufb(HK, HK, xmm1, Assembler::AVX_128bit);\n+\n+  __ movdqu(xmm11, ExternalAddress(ghash_polynomial_addr()), rbx \/*rscratch*\/);\n+  __ movdqu(xmm12, ExternalAddress(ghash_polynomial_two_one_addr()), rbx \/*rscratch*\/);\n+  \/\/ Compute H ^ 2 from the input subkeyH\n+  __ vpsrlq(xmm1, xmm6, 63, Assembler::AVX_128bit);\n+  __ vpsllq(xmm6, xmm6, 1, Assembler::AVX_128bit);\n+  __ vpslldq(xmm2, xmm1, 8, Assembler::AVX_128bit);\n+  __ vpsrldq(xmm1, xmm1, 8, Assembler::AVX_128bit);\n+\n+  __ vpor(xmm6, xmm6, xmm2, Assembler::AVX_128bit);\n+\n+  __ vpshufd(xmm2, xmm1, 0x24, Assembler::AVX_128bit);\n+  __ vpcmpeqd(xmm2, xmm2, xmm12, Assembler::AVX_128bit);\n+  __ vpand(xmm2, xmm2, xmm11, Assembler::AVX_128bit);\n+  __ vpxor(xmm6, xmm6, xmm2, Assembler::AVX_128bit);\n+  __ movdqu(Address(htbl, 1 * 16), xmm6); \/\/ H * 2\n+  __ movdqu(xmm0, xmm6);\n+  for (int i = 2; i < 9; i++) {\n+    gfmul_avx2(xmm6, xmm0);\n+    __ movdqu(Address(htbl, i * 16), xmm6);\n+  }\n+}\n+\n+#define aesenc_step_avx2(t_key)\\\n+__ aesenc(xmm1, t_key);\\\n+__ aesenc(xmm2, t_key);\\\n+__ aesenc(xmm3, t_key);\\\n+__ aesenc(xmm4, t_key);\\\n+__ aesenc(xmm5, t_key);\\\n+__ aesenc(xmm6, t_key);\\\n+__ aesenc(xmm7, t_key);\\\n+__ aesenc(xmm8, t_key);\\\n+\n+#define ghash_step_avx2(ghdata, hkey) \\\n+__ vpclmulqdq(xmm11, ghdata, hkey, 0x11);\\\n+__ vpxor(xmm12, xmm12, xmm11, Assembler::AVX_128bit);\\\n+__ vpclmulqdq(xmm11, ghdata, hkey, 0x00);\\\n+__ vpxor(xmm15, xmm15, xmm11, Assembler::AVX_128bit);\\\n+__ vpclmulqdq(xmm11, ghdata, hkey, 0x01);\\\n+__ vpxor(xmm14, xmm14, xmm11, Assembler::AVX_128bit);\\\n+__ vpclmulqdq(xmm11, ghdata, hkey, 0x10);\\\n+__ vpxor(xmm14, xmm14, xmm11, Assembler::AVX_128bit);\\\n+\n+\/\/Encrypts and hashes 8 blocks in an interleaved fashion.\n+\/\/Inputs:\n+\/\/key - key for aes operations\n+\/\/subkeyHtbl - table containing H constants\n+\/\/ctr_blockx - counter for aes operations\n+\/\/in - input buffer\n+\/\/out - output buffer\n+\/\/ct - ciphertext buffer\n+\/\/pos - holds the length processed in this method\n+\/\/in_order - boolean that indicates if incrementing counter without shuffling is needed\n+\/\/rounds - number of aes rounds calculated based on key length\n+\/\/xmm1-xmm8 - holds encrypted counter values\n+\/\/Outputs:\n+\/\/xmm1-xmm8 - updated encrypted counter values\n+\/\/ctr_blockx - updated counter value\n+\/\/out - updated output buffer\n+\/\/Temp registers: xmm0, xmm10, xmm11, xmm12, xmm13, xmm14, xmm15, rbx\n+void StubGenerator::ghash8_encrypt8_parallel_avx2(Register key, Register subkeyHtbl, XMMRegister ctr_blockx, Register in,\n+                                                  Register out, Register ct, Register pos, bool in_order, Register rounds,\n+                                                  XMMRegister xmm1, XMMRegister xmm2, XMMRegister xmm3, XMMRegister xmm4,\n+                                                  XMMRegister xmm5, XMMRegister xmm6, XMMRegister xmm7, XMMRegister xmm8) {\n+  const XMMRegister t1 = xmm0;\n+  const XMMRegister t2 = xmm10;\n+  const XMMRegister t3 = xmm11;\n+  const XMMRegister t4 = xmm12;\n+  const XMMRegister t5 = xmm13;\n+  const XMMRegister t6 = xmm14;\n+  const XMMRegister t7 = xmm15;\n+  Label skip_reload, last_aes_rnd, aes_192, aes_256;\n+\n+  __ movdqu(t2, xmm1);\n+  for (int i = 0; i <= 6; i++) {\n+    __ movdqu(Address(rsp, 16 * i), as_XMMRegister(i + 2));\n+  }\n+\n+  if (in_order) {\n+    __ vpaddd(xmm1, ctr_blockx, ExternalAddress(counter_mask_linc1_addr()), Assembler::AVX_128bit, rbx \/*rscratch*\/); \/\/Increment counter by 1\n+    __ movdqu(t5, ExternalAddress(counter_mask_linc2_addr()), rbx \/*rscratch*\/);\n+    __ vpaddd(xmm2, ctr_blockx, t5, Assembler::AVX_128bit);\n+    for (int rnum = 1; rnum <= 6; rnum++) {\n+      __ vpaddd(as_XMMRegister(rnum + 2), as_XMMRegister(rnum), t5, Assembler::AVX_128bit);\n+    }\n+    __ movdqu(ctr_blockx, xmm8);\n+\n+    __ movdqu(t5, ExternalAddress(counter_shuffle_mask_addr()), rbx \/*rscratch*\/);\n+    for (int rnum = 1; rnum <= 8; rnum++) {\n+      __ vpshufb(as_XMMRegister(rnum), as_XMMRegister(rnum), t5, Assembler::AVX_128bit); \/\/perform a 16Byte swap\n+    }\n+  } else {\n+    __ vpaddd(xmm1, ctr_blockx, ExternalAddress(counter_mask_linc1f_addr()), Assembler::AVX_128bit, rbx \/*rscratch*\/); \/\/Increment counter by 1\n+    __ vmovdqu(t5, ExternalAddress(counter_mask_linc2f_addr()), Assembler::AVX_128bit, rbx \/*rscratch*\/);\n+    __ vpaddd(xmm2, ctr_blockx, t5, Assembler::AVX_128bit);\n+    for (int rnum = 1; rnum <= 6; rnum++) {\n+      __ vpaddd(as_XMMRegister(rnum + 2), as_XMMRegister(rnum), t5, Assembler::AVX_128bit);\n+    }\n+    __ movdqu(ctr_blockx, xmm8);\n+  }\n+\n+  load_key(t1, key, 16 * 0, rbx \/*rscratch*\/);\n+  for (int rnum = 1; rnum <= 8; rnum++) {\n+    __ vpxor(as_XMMRegister(rnum), as_XMMRegister(rnum), t1, Assembler::AVX_128bit);\n+  }\n+\n+  load_key(t1, key, 16 * 1, rbx \/*rscratch*\/);\n+  aesenc_step_avx2(t1);\n+\n+  load_key(t1, key, 16 * 2, rbx \/*rscratch*\/);\n+  aesenc_step_avx2(t1);\n+\n+  __ movdqu(t5, (Address(subkeyHtbl, 8 * 16)));\n+  __ vpclmulqdq(t4, t2, t5, 0x11); \/\/t4 = a1*b1\n+  __ vpclmulqdq(t7, t2, t5, 0x00); \/\/t7 = a0*b0\n+  __ vpclmulqdq(t6, t2, t5, 0x01); \/\/t6 = a1*b0\n+  __ vpclmulqdq(t5, t2, t5, 0x10); \/\/t5 = a0*b1\n+  __ vpxor(t6, t6, t5, Assembler::AVX_128bit);\n+\n+  for (int i = 3, j = 0; i <= 8; i++, j++) {\n+    load_key(t1, key, 16 * i, rbx \/*rscratch*\/);\n+    aesenc_step_avx2(t1);\n+    __ movdqu(t1, Address(rsp, 16 * j));\n+    __ movdqu(t5, (Address(subkeyHtbl, (7 - j) * 16)));\n+    ghash_step_avx2(t1, t5);\n+  }\n+\n+  load_key(t1, key, 16 * 9, rbx \/*rscratch*\/);\n+  aesenc_step_avx2(t1);\n+\n+  __ movdqu(t1, Address(rsp, 16 * 6));\n+  __ movdqu(t5, (Address(subkeyHtbl, 1 * 16)));\n+\n+  __ vpclmulqdq(t3, t1, t5, 0x00);\n+  __ vpxor(t7, t7, t3, Assembler::AVX_128bit);\n+\n+  __ vpclmulqdq(t3, t1, t5, 0x01);\n+  __ vpxor(t6, t6, t3, Assembler::AVX_128bit);\n+\n+  __ vpclmulqdq(t3, t1, t5, 0x10);\n+  __ vpxor(t6, t6, t3, Assembler::AVX_128bit);\n+\n+  __ vpclmulqdq(t3, t1, t5, 0x11);\n+  __ vpxor(t1, t4, t3, Assembler::AVX_128bit);\n+\n+  __ vpslldq(t3, t6, 8, Assembler::AVX_128bit); \/\/shift-L t3 2 DWs\n+  __ vpsrldq(t6, t6, 8, Assembler::AVX_128bit); \/\/shift-R t2 2 DWs\n+  __ vpxor(t7, t7, t3, Assembler::AVX_128bit);\n+  __ vpxor(t1, t1, t6, Assembler::AVX_128bit); \/\/ accumulate the results in t1:t7\n+\n+  load_key(t5, key, 16 * 10, rbx \/*rscratch*\/);\n+  __ cmpl(rounds, 52);\n+  __ jcc(Assembler::less, last_aes_rnd);\n+\n+  __ bind(aes_192);\n+  aesenc_step_avx2(t5);\n+  load_key(t5, key, 16 * 11, rbx \/*rscratch*\/);\n+  aesenc_step_avx2(t5);\n+  load_key(t5, key, 16 * 12, rbx \/*rscratch*\/);\n+  __ cmpl(rounds, 60);\n+  __ jcc(Assembler::less, last_aes_rnd);\n+\n+  __ bind(aes_256);\n+  aesenc_step_avx2(t5);\n+  load_key(t5, key, 16 * 13, rbx \/*rscratch*\/);\n+  aesenc_step_avx2(t5);\n+  load_key(t5, key, 16 * 14, rbx \/*rscratch*\/);\n+  __ bind(last_aes_rnd);\n+  for (int rnum = 1; rnum <= 8; rnum++) {\n+    __ aesenclast(as_XMMRegister(rnum), t5);\n+  }\n+\n+  for (int i = 0; i <= 7; i++) {\n+    __ movdqu(t2, Address(in, pos, Address::times_1, 16 * i));\n+    __ vpxor(as_XMMRegister(i + 1), as_XMMRegister(i + 1), t2, Assembler::AVX_128bit);\n+  }\n+\n+  \/\/first phase of the reduction\n+  __ vmovdqu(t3, ExternalAddress(ghash_polynomial_reduction_addr()), Assembler::AVX_128bit, rbx \/*rscratch*\/);\n+\n+  __ vpclmulqdq(t2, t3, t7, 0x01);\n+  __ vpslldq(t2, t2, 8, Assembler::AVX_128bit); \/\/shift-L xmm2 2 DWs\n+\n+  __ vpxor(t7, t7, t2, Assembler::AVX_128bit); \/\/first phase of the reduction complete\n+\n+  \/\/Write to the Ciphertext buffer\n+  for (int i = 0; i <= 7; i++) {\n+    __ movdqu(Address(out, pos, Address::times_1, 16 * i), as_XMMRegister(i + 1));\n+  }\n+\n+  __ cmpptr(ct, out);\n+  __ jcc(Assembler::equal, skip_reload);\n+  for (int i = 0; i <= 7; i++) {\n+    __ movdqu(as_XMMRegister(i + 1), Address(in, pos, Address::times_1, 16 * i));\n+  }\n+\n+  __ bind(skip_reload);\n+  \/\/second phase of the reduction\n+  __ vpclmulqdq(t2, t3, t7, 0x00);\n+  __ vpsrldq(t2, t2, 4, Assembler::AVX_128bit); \/\/shift-R t2 1 DW (Shift-R only 1-DW to obtain 2-DWs shift-R)\n+\n+  __ vpclmulqdq(t4, t3, t7, 0x10);\n+  __ vpslldq(t4, t4, 4, Assembler::AVX_128bit); \/\/shift-L t4 1 DW (Shift-L 1-DW to obtain result with no shifts)\n+  __ vpxor(t4, t4, t2, Assembler::AVX_128bit); \/\/second phase of the reduction complete\n+  __ vpxor(t1, t1, t4, Assembler::AVX_128bit); \/\/the result is in t1\n+\n+  \/\/perform a 16Byte swap\n+  __ movdqu(t7, ExternalAddress(counter_shuffle_mask_addr()), rbx \/*rscratch*\/);\n+  for (int rnum = 1; rnum <= 8; rnum++) {\n+    __ vpshufb(as_XMMRegister(rnum), as_XMMRegister(rnum), t7, Assembler::AVX_128bit);\n+  }\n+  __ vpxor(xmm1, xmm1, t1, Assembler::AVX_128bit);\n+}\n+\n+\/\/GHASH the last 8 ciphertext blocks.\n+\/\/Input:\n+\/\/subkeyHtbl - table containing H constants\n+\/\/Output:\n+\/\/xmm14 - calculated aad hash\n+\/\/Temp registers: xmm0, xmm10, xmm11, xmm12, xmm13, xmm15, rbx\n+void StubGenerator::ghash_last_8_avx2(Register subkeyHtbl) {\n+  const XMMRegister t1 = xmm0;\n+  const XMMRegister t2 = xmm10;\n+  const XMMRegister t3 = xmm11;\n+  const XMMRegister t4 = xmm12;\n+  const XMMRegister t5 = xmm13;\n+  const XMMRegister t6 = xmm14;\n+  const XMMRegister t7 = xmm15;\n+\n+  \/\/Karatsuba Method\n+  __ movdqu(t5, Address(subkeyHtbl, 8 * 16));\n+\n+  __ vpshufd(t2, xmm1, 78, Assembler::AVX_128bit);\n+  __ vpshufd(t3, t5, 78, Assembler::AVX_128bit);\n+  __ vpxor(t2, t2, xmm1, Assembler::AVX_128bit);\n+  __ vpxor(t3, t3, t5, Assembler::AVX_128bit);\n+\n+  __ vpclmulqdq(t6, xmm1, t5, 0x11);\n+  __ vpclmulqdq(t7, xmm1, t5, 0x00);\n+\n+  __ vpclmulqdq(xmm1, t2, t3, 0x00);\n+\n+  for (int i = 7, rnum = 2; rnum <= 8; i--, rnum++) {\n+    __ movdqu(t5, Address(subkeyHtbl, i * 16));\n+    __ vpshufd(t2, as_XMMRegister(rnum), 78, Assembler::AVX_128bit);\n+    __ vpshufd(t3, t5, 78, Assembler::AVX_128bit);\n+    __ vpxor(t2, t2, as_XMMRegister(rnum), Assembler::AVX_128bit);\n+    __ vpxor(t3, t3, t5, Assembler::AVX_128bit);\n+    __ vpclmulqdq(t4, as_XMMRegister(rnum), t5, 0x11);\n+    __ vpxor(t6, t6, t4, Assembler::AVX_128bit);\n+    __ vpclmulqdq(t4, as_XMMRegister(rnum), t5, 0x00);\n+    __ vpxor(t7, t7, t4, Assembler::AVX_128bit);\n+    __ vpclmulqdq(t2, t2, t3, 0x00);\n+    __ vpxor(xmm1, xmm1, t2, Assembler::AVX_128bit);\n+  }\n+\n+  __ vpxor(xmm1, xmm1, t6, Assembler::AVX_128bit);\n+  __ vpxor(t2, xmm1, t7, Assembler::AVX_128bit);\n+\n+  __ vpslldq(t4, t2, 8, Assembler::AVX_128bit);\n+  __ vpsrldq(t2, t2, 8, Assembler::AVX_128bit);\n+\n+  __ vpxor(t7, t7, t4, Assembler::AVX_128bit);\n+  __ vpxor(t6, t6, t2, Assembler::AVX_128bit); \/\/<t6:t7> holds the result of the accumulated carry-less multiplications\n+\n+  \/\/first phase of the reduction\n+  __ movdqu(t3, ExternalAddress(ghash_polynomial_reduction_addr()), rbx \/*rscratch*\/);\n+\n+  __ vpclmulqdq(t2, t3, t7, 0x01);\n+  __ vpslldq(t2, t2, 8, Assembler::AVX_128bit); \/\/ shift-L t2 2 DWs\n+\n+  __ vpxor(t7, t7, t2, Assembler::AVX_128bit);\/\/first phase of the reduction complete\n+\n+  \/\/second phase of the reduction\n+  __ vpclmulqdq(t2, t3, t7, 0x00);\n+  __ vpsrldq(t2, t2, 4, Assembler::AVX_128bit); \/\/shift-R t2 1 DW (Shift-R only 1-DW to obtain 2-DWs shift-R)\n+\n+  __ vpclmulqdq(t4, t3, t7, 0x10);\n+  __ vpslldq(t4, t4, 4, Assembler::AVX_128bit); \/\/shift-L t4 1 DW (Shift-L 1-DW to obtain result with no shifts)\n+  __ vpxor(t4, t4, t2, Assembler::AVX_128bit); \/\/second phase of the reduction complete\n+  __ vpxor(t6, t6, t4, Assembler::AVX_128bit); \/\/the result is in t6\n+}\n+\n+\/\/Encrypt initial number of 8 blocks\n+\/\/Inputs:\n+\/\/ctr - counter for aes operations\n+\/\/rounds - number of aes rounds calculated based on key length\n+\/\/key - key for aes operations\n+\/\/len - input length to be processed\n+\/\/in - input buffer\n+\/\/out - output buffer\n+\/\/ct - ciphertext buffer\n+\/\/aad_hashx - input aad hash\n+\/\/pos - holds the length processed in this method\n+\/\/Outputs:\n+\/\/xmm1-xmm8 - holds updated encrypted counter values\n+\/\/ctr - updated counter value\n+\/\/pos - updated position\n+\/\/len - updated length\n+\/\/out - updated output buffer\n+\/\/Temp registers: xmm0, xmm10, xmm11, xmm12, xmm13, xmm14, xmm15\n+void StubGenerator::initial_blocks_avx2(XMMRegister ctr, Register rounds, Register key, Register len, Register in,\n+                                        Register out, Register ct, XMMRegister aad_hashx, Register pos) {\n+  const XMMRegister t1 = xmm12;\n+  const XMMRegister t2 = xmm13;\n+  const XMMRegister t3 = xmm14;\n+  const XMMRegister t4 = xmm15;\n+  const XMMRegister t5 = xmm11;\n+  const XMMRegister t6 = xmm10;\n+  const XMMRegister t_key = xmm0;\n+\n+  Label skip_reload, last_aes_rnd, aes_192, aes_256;\n+  \/\/Move AAD_HASH to temp reg t3\n+  __ movdqu(t3, aad_hashx);\n+  \/\/Prepare 8 counter blocks and perform rounds of AES cipher on\n+  \/\/them, load plain\/cipher text and store cipher\/plain text.\n+  __ movdqu(xmm1, ctr);\n+  __ movdqu(t5, ExternalAddress(counter_mask_linc1_addr()), rbx \/*rscratch*\/);\n+  __ movdqu(t6, ExternalAddress(counter_mask_linc2_addr()), rbx \/*rscratch*\/ );\n+  __ vpaddd(xmm2, xmm1, t5, Assembler::AVX_128bit);\n+  for (int rnum = 1; rnum <= 6; rnum++) {\n+    __ vpaddd(as_XMMRegister(rnum + 2), as_XMMRegister(rnum), t6, Assembler::AVX_128bit);\n+  }\n+  __ movdqu(ctr, xmm8);\n+\n+  __ movdqu(t5, ExternalAddress(counter_shuffle_mask_addr()), rbx \/*rscratch*\/);\n+  for (int rnum = 1; rnum <= 8; rnum++) {\n+    __ vpshufb(as_XMMRegister(rnum), as_XMMRegister(rnum), t5, Assembler::AVX_128bit); \/\/perform a 16Byte swap\n+  }\n+\n+  load_key(t_key, key, 16 * 0, rbx \/*rscratch*\/);\n+  for (int rnum = 1; rnum <= 8; rnum++) {\n+    __ vpxor(as_XMMRegister(rnum), as_XMMRegister(rnum), t_key, Assembler::AVX_128bit);\n+  }\n+\n+  for (int i = 1; i <= 9; i++) {\n+    load_key(t_key, key, 16 * i, rbx \/*rscratch*\/);\n+    aesenc_step_avx2(t_key);\n+  }\n+\n+  load_key(t_key, key, 16 * 10, rbx \/*rscratch*\/);\n+  __ cmpl(rounds, 52);\n+  __ jcc(Assembler::less, last_aes_rnd);\n+\n+  __ bind(aes_192);\n+  aesenc_step_avx2(t_key);\n+  load_key(t_key, key, 16 * 11, rbx \/*rscratch*\/);\n+  aesenc_step_avx2(t_key);\n+  load_key(t_key, key, 16 * 12, rbx \/*rscratch*\/);\n+  __ cmpl(rounds, 60);\n+  __ jcc(Assembler::less, last_aes_rnd);\n+\n+  __ bind(aes_256);\n+  aesenc_step_avx2(t_key);\n+  load_key(t_key, key, 16 * 13, rbx \/*rscratch*\/);\n+  aesenc_step_avx2(t_key);\n+  load_key(t_key, key, 16 * 14, rbx \/*rscratch*\/);\n+\n+  __ bind(last_aes_rnd);\n+  for (int rnum = 1; rnum <= 8; rnum++) {\n+    __ aesenclast(as_XMMRegister(rnum), t_key);\n+  }\n+\n+  \/\/XOR and store data\n+  for (int i = 0; i <= 7; i++) {\n+    __ movdqu(t1, Address(in, pos, Address::times_1, 16 * i));\n+    __ vpxor(as_XMMRegister(i + 1), as_XMMRegister(i + 1), t1, Assembler::AVX_128bit);\n+    __ movdqu(Address(out, pos, Address::times_1, 16 * i), as_XMMRegister(i + 1));\n+  }\n+\n+  __ cmpptr(ct, out);\n+  __ jcc(Assembler::equal, skip_reload);\n+  for (int i = 0; i <= 7; i++) {\n+    __ movdqu(as_XMMRegister(i + 1), Address(in, pos, Address::times_1, 16 * i));\n+  }\n+\n+  __ bind(skip_reload);\n+  \/\/Update len with the number of blocks processed\n+  __ subl(len, 128);\n+  __ addl(pos, 128);\n+\n+  __ movdqu(t4, ExternalAddress(counter_shuffle_mask_addr()), rbx \/*rscratch*\/);\n+  for (int rnum = 1; rnum <= 8; rnum++) {\n+    __ vpshufb(as_XMMRegister(rnum), as_XMMRegister(rnum), t4, Assembler::AVX_128bit);\n+  }\n+  \/\/ Combine GHASHed value with the corresponding ciphertext\n+  __ vpxor(xmm1, xmm1, t3, Assembler::AVX_128bit);\n+}\n+\n+\/\/AES-GCM interleaved implementation\n+\/\/Inputs:\n+\/\/in - input buffer\n+\/\/len- message length to be processed\n+\/\/ct - cipher text buffer\n+\/\/out - output buffer\n+\/\/key - key for aes operations\n+\/\/state - address of aad hash for ghash computation\n+\/\/subkeyHtbl- table consisting of H constants\n+\/\/counter - address of counter for aes operations\n+\/\/Output:\n+\/\/(counter) - updated in memory counter value\n+\/\/(state) - updated in memory aad hash\n+\/\/rax - length processed\n+\/\/(out) - output buffer updated\n+\/\/len - updated length\n+\/\/Temp registers: xmm0-xmm15, r10, r15, rbx\n+void StubGenerator::aesgcm_avx2(Register in, Register len, Register ct, Register out, Register key,\n+                                Register state, Register subkeyHtbl, Register counter) {\n+  const Register pos = rax;\n+  const Register rounds = r10;\n+  const XMMRegister ctr_blockx = xmm9;\n+  const XMMRegister aad_hashx = xmm8;\n+  Label encrypt_done, encrypt_by_8_new, encrypt_by_8;\n+\n+  \/\/This routine should be called only for message sizes of 128 bytes or more.\n+  \/\/Macro flow:\n+  \/\/process 8 16 byte blocks in initial_num_blocks.\n+  \/\/process 8 16 byte blocks at a time until all are done 'encrypt_by_8_new  followed by ghash_last_8'\n+  __ xorl(pos, pos);\n+\n+  \/\/Generate 8 constants for htbl\n+  generateHtbl_8_block_avx2(subkeyHtbl);\n+\n+  \/\/Compute #rounds for AES based on the length of the key array\n+  __ movl(rounds, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));\n+\n+  \/\/Load and shuffle state and counter values\n+  __ movdqu(ctr_blockx, Address(counter, 0));\n+  __ movdqu(aad_hashx, Address(state, 0));\n+  __ vpshufb(ctr_blockx, ctr_blockx, ExternalAddress(counter_shuffle_mask_addr()), Assembler::AVX_128bit, rbx \/*rscratch*\/);\n+  __ vpshufb(aad_hashx, aad_hashx, ExternalAddress(ghash_long_swap_mask_addr()), Assembler::AVX_128bit, rbx \/*rscratch*\/);\n+\n+  initial_blocks_avx2(ctr_blockx, rounds, key, len, in, out, ct, aad_hashx, pos);\n+\n+  \/\/We need at least 128 bytes to proceed further.\n+  __ cmpl(len, 128);\n+  __ jcc(Assembler::less, encrypt_done);\n+\n+  \/\/in_order vs. out_order is an optimization to increment the counter without shuffling\n+  \/\/it back into little endian. r15d keeps track of when we need to increment in order so\n+  \/\/that the carry is handled correctly.\n+  __ movdl(r15, ctr_blockx);\n+  __ andl(r15, 255);\n+  __ vpshufb(ctr_blockx, ctr_blockx, ExternalAddress(counter_shuffle_mask_addr()), Assembler::AVX_128bit, rbx \/*rscratch*\/);\n+\n+  __ bind(encrypt_by_8_new);\n+  __ cmpl(r15, 255 - 8);\n+  __ jcc(Assembler::greater, encrypt_by_8);\n+\n+  __ addb(r15, 8);\n+  ghash8_encrypt8_parallel_avx2(key, subkeyHtbl, ctr_blockx, in, out, ct, pos, false, rounds,\n+                                xmm1, xmm2, xmm3, xmm4, xmm5, xmm6, xmm7, xmm8);\n+  __ addl(pos, 128);\n+  __ subl(len, 128);\n+  __ cmpl(len, 128);\n+  __ jcc(Assembler::greaterEqual, encrypt_by_8_new);\n+\n+  __ vpshufb(ctr_blockx, ctr_blockx, ExternalAddress(counter_shuffle_mask_addr()), Assembler::AVX_128bit, rbx \/*rscratch*\/);\n+  __ jmp(encrypt_done);\n+\n+  __ bind(encrypt_by_8);\n+  __ vpshufb(ctr_blockx, ctr_blockx, ExternalAddress(counter_shuffle_mask_addr()), Assembler::AVX_128bit, rbx \/*rscratch*\/);\n+\n+  __ addb(r15, 8);\n+  ghash8_encrypt8_parallel_avx2(key, subkeyHtbl, ctr_blockx, in, out, ct, pos, true, rounds,\n+                                xmm1, xmm2, xmm3, xmm4, xmm5, xmm6, xmm7, xmm8);\n+\n+  __ vpshufb(ctr_blockx, ctr_blockx, ExternalAddress(counter_shuffle_mask_addr()), Assembler::AVX_128bit, rbx \/*rscratch*\/);\n+  __ addl(pos, 128);\n+  __ subl(len, 128);\n+  __ cmpl(len, 128);\n+  __ jcc(Assembler::greaterEqual, encrypt_by_8_new);\n+  __ vpshufb(ctr_blockx, ctr_blockx, ExternalAddress(counter_shuffle_mask_addr()), Assembler::AVX_128bit, rbx \/*rscratch*\/);\n+\n+  __ bind(encrypt_done);\n+  ghash_last_8_avx2(subkeyHtbl);\n+\n+  __ vpaddd(ctr_blockx, ctr_blockx, ExternalAddress(counter_mask_linc1_addr()), Assembler::AVX_128bit, rbx \/*rscratch*\/);\n+  __ vpshufb(ctr_blockx, ctr_blockx, ExternalAddress(counter_shuffle_mask_addr()), Assembler::AVX_128bit, rbx \/*rscratch*\/);\n+  __ movdqu(Address(counter, 0), ctr_blockx); \/\/current_counter = xmm9\n+  __ vpshufb(xmm14, xmm14, ExternalAddress(ghash_long_swap_mask_addr()), Assembler::AVX_128bit, rbx \/*rscratch*\/);\n+  __ movdqu(Address(state, 0), xmm14); \/\/aad hash = xmm14\n+  \/\/Xor out round keys\n+  __ vpxor(xmm0, xmm0, xmm0, Assembler::AVX_128bit);\n+  __ vpxor(xmm13, xmm13, xmm13, Assembler::AVX_128bit);\n+\n+ }\n+\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64_aes.cpp","additions":652,"deletions":1,"binary":false,"changes":653,"status":"modified"},{"patch":"@@ -40,1 +40,1 @@\n-  _compiler_stubs_code_size     = 20000 LP64_ONLY(+30000) WINDOWS_ONLY(+2000),\n+  _compiler_stubs_code_size     = 20000 LP64_ONLY(+32000) WINDOWS_ONLY(+2000),\n","filename":"src\/hotspot\/cpu\/x86\/stubRoutines_x86.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -621,3 +621,3 @@\n-     * Requires 768 bytes (48 AES blocks) to efficiently use the intrinsic.\n-     * inLen that is less than 768 size block sizes, before or after this\n-     * intrinsic is used, will be done by the calling method\n+     * Requires PARALLEN_LEN bytes to efficiently use the intrinsic.\n+     * The intrinsic returns the number of bytes processed.\n+     * The remaining bytes will be processed by the calling method.\n@@ -626,2 +626,2 @@\n-     * Only Intel processors with AVX512 that support vaes, vpclmulqdq,\n-     * avx512dq, and avx512vl trigger this intrinsic.\n+     * Intel processors with AVX2 support and above trigger this intrinsic.\n+     * Some AARCH64 processors also trigger this intrinsic.\n","filename":"src\/java.base\/share\/classes\/com\/sun\/crypto\/provider\/GaloisCounterMode.java","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"}]}