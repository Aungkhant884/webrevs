{"files":[{"patch":"@@ -1334,0 +1334,5 @@\n+void Assembler::addb(Register dst, int imm8) {\n+  prefix(dst);\n+  emit_arith_b(0x80, 0xC0, dst, imm8);\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.cpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -988,0 +988,1 @@\n+  void addb(Register dst, int imm8);\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -361,1 +361,1 @@\n-  void initial_blocks(XMMRegister ctr, Register rounds, Register key, Register len,\n+  void initial_blocks_avx2(XMMRegister ctr, Register rounds, Register key, Register len,\n@@ -365,1 +365,1 @@\n-  void ghash8_encrypt8_parallel(Register key, Register subkeyHtbl, XMMRegister ctr_blockx, XMMRegister aad_hashx,\n+  void ghash8_encrypt8_parallel_avx2(Register key, Register subkeyHtbl, XMMRegister ctr_blockx, XMMRegister aad_hashx,\n@@ -367,1 +367,1 @@\n-  void ghash_last_8(Register subkeyHtbl);\n+  void ghash_last_8_avx2(Register subkeyHtbl);\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n-* Copyright (c) 2019, 2021, Intel Corporation. All rights reserved.\n+* Copyright (c) 2019, 2023, Intel Corporation. All rights reserved.\n@@ -183,3 +183,0 @@\n-    if (VM_Version::supports_avx2()) {\n-        StubRoutines::_galoisCounterMode_AESCrypt = generate_avx2_galoisCounterMode_AESCrypt();\n-    }\n@@ -193,0 +190,3 @@\n+      if (VM_Version::supports_avx2()) {\n+          StubRoutines::_galoisCounterMode_AESCrypt = generate_avx2_galoisCounterMode_AESCrypt();\n+      }\n@@ -3352,2 +3352,2 @@\n-      gfmul_avx2(xmm6, xmm0);\n-      __ movdqu(Address(htbl, i * 16), xmm6);\n+    gfmul_avx2(xmm6, xmm0);\n+    __ movdqu(Address(htbl, i * 16), xmm6);\n@@ -3358,2 +3358,22 @@\n-void StubGenerator::ghash8_encrypt8_parallel(Register key, Register subkeyHtbl, XMMRegister ctr_blockx, XMMRegister aad_hashx,\n-                                             Register in, Register out, Register ct, Register pos, bool in_order, Register rounds) {\n+#define aesenc_step_avx2(t_key)\\\n+__ aesenc(xmm1, t_key);\\\n+__ aesenc(xmm2, t_key);\\\n+__ aesenc(xmm3, t_key);\\\n+__ aesenc(xmm4, t_key);\\\n+__ aesenc(xmm5, t_key);\\\n+__ aesenc(xmm6, t_key);\\\n+__ aesenc(xmm7, t_key);\\\n+__ aesenc(xmm8, t_key);\\\n+\n+#define ghash_step_avx2(ghdata, hkey) \\\n+__ vpclmulqdq(xmm11, ghdata, hkey, 0x11);\\\n+__ vpxor(xmm12, xmm12, xmm11, Assembler::AVX_128bit);\\\n+__ vpclmulqdq(xmm11, ghdata, hkey, 0x00);\\\n+__ vpxor(xmm15, xmm15, xmm11, Assembler::AVX_128bit);\\\n+__ vpclmulqdq(xmm11, ghdata, hkey, 0x01);\\\n+__ vpxor(xmm14, xmm14, xmm11, Assembler::AVX_128bit);\\\n+__ vpclmulqdq(xmm11, ghdata, hkey, 0x10);\\\n+__ vpxor(xmm14, xmm14, xmm11, Assembler::AVX_128bit);\\\n+\n+void StubGenerator::ghash8_encrypt8_parallel_avx2(Register key, Register subkeyHtbl, XMMRegister ctr_blockx, XMMRegister aad_hashx,\n+                                                  Register in, Register out, Register ct, Register pos, bool in_order, Register rounds) {\n@@ -3392,8 +3412,3 @@\n-    __ vpshufb(xmm1, xmm1, t5, Assembler::AVX_128bit); \/\/perform a 16Byte swap\n-    __ vpshufb(xmm2, xmm2, t5, Assembler::AVX_128bit); \/\/perform a 16Byte swap\n-    __ vpshufb(xmm3, xmm3, t5, Assembler::AVX_128bit); \/\/perform a 16Byte swap\n-    __ vpshufb(xmm4, xmm4, t5, Assembler::AVX_128bit); \/\/perform a 16Byte swap\n-    __ vpshufb(xmm5, xmm5, t5, Assembler::AVX_128bit); \/\/perform a 16Byte swap\n-    __ vpshufb(xmm6, xmm6, t5, Assembler::AVX_128bit); \/\/perform a 16Byte swap\n-    __ vpshufb(xmm7, xmm7, t5, Assembler::AVX_128bit); \/\/perform a 16Byte swap\n-    __ vpshufb(xmm8, xmm8, t5, Assembler::AVX_128bit); \/\/perform a 16Byte swap\n+    for (int rnum = 1; rnum <= 8; rnum++) {\n+      __ vpshufb(as_XMMRegister(rnum), as_XMMRegister(rnum), t5, Assembler::AVX_128bit); \/\/perform a 16Byte swap\n+    }\n@@ -3414,8 +3429,3 @@\n-  __ vpxor(xmm1, xmm1, t1, Assembler::AVX_128bit);\n-  __ vpxor(xmm2, xmm2, t1, Assembler::AVX_128bit);\n-  __ vpxor(xmm3, xmm3, t1, Assembler::AVX_128bit);\n-  __ vpxor(xmm4, xmm4, t1, Assembler::AVX_128bit);\n-  __ vpxor(xmm5, xmm5, t1, Assembler::AVX_128bit);\n-  __ vpxor(xmm6, xmm6, t1, Assembler::AVX_128bit);\n-  __ vpxor(xmm7, xmm7, t1, Assembler::AVX_128bit);\n-  __ vpxor(xmm8, xmm8, t1, Assembler::AVX_128bit);\n+  for (int rnum = 1; rnum <= 8; rnum++) {\n+    __ vpxor(as_XMMRegister(rnum), as_XMMRegister(rnum), t1, Assembler::AVX_128bit);\n+  }\n@@ -3424,8 +3434,1 @@\n-  __ aesenc(xmm1, t1);\n-  __ aesenc(xmm2, t1);\n-  __ aesenc(xmm3, t1);\n-  __ aesenc(xmm4, t1);\n-  __ aesenc(xmm5, t1);\n-  __ aesenc(xmm6, t1);\n-  __ aesenc(xmm7, t1);\n-  __ aesenc(xmm8, t1);\n+  aesenc_step_avx2(t1);\n@@ -3434,8 +3437,1 @@\n-  __ aesenc(xmm1, t1);\n-  __ aesenc(xmm2, t1);\n-  __ aesenc(xmm3, t1);\n-  __ aesenc(xmm4, t1);\n-  __ aesenc(xmm5, t1);\n-  __ aesenc(xmm6, t1);\n-  __ aesenc(xmm7, t1);\n-  __ aesenc(xmm8, t1);\n+  aesenc_step_avx2(t1);\n@@ -3450,143 +3446,7 @@\n-  load_key(t1, key, 16 * 3, rbx \/*rscratch*\/);\n-  __ aesenc(xmm1, t1);\n-  __ aesenc(xmm2, t1);\n-  __ aesenc(xmm3, t1);\n-  __ aesenc(xmm4, t1);\n-  __ aesenc(xmm5, t1);\n-  __ aesenc(xmm6, t1);\n-  __ aesenc(xmm7, t1);\n-  __ aesenc(xmm8, t1);\n-\n-  __ movdqu(t1, Address(rsp, 16 * 0));\n-  __ movdqu(t5, (Address(subkeyHtbl, 7 * 16)));\n-  __ vpclmulqdq(t3, t1, t5, 0x11);\n-  __ vpxor(t4, t4, t3, Assembler::AVX_128bit);\n-\n-  __ vpclmulqdq(t3, t1, t5, 0x00);\n-  __ vpxor(t7, t7, t3, Assembler::AVX_128bit);\n-\n-  __ vpclmulqdq(t3, t1, t5, 0x01);\n-  __ vpxor(t6, t6, t3, Assembler::AVX_128bit);\n-\n-  __ vpclmulqdq(t3, t1, t5, 0x10);\n-  __ vpxor(t6, t6, t3, Assembler::AVX_128bit);\n-\n-  load_key(t1, key, 16 * 4, rbx \/*rscratch*\/);\n-  __ aesenc(xmm1, t1);\n-  __ aesenc(xmm2, t1);\n-  __ aesenc(xmm3, t1);\n-  __ aesenc(xmm4, t1);\n-  __ aesenc(xmm5, t1);\n-  __ aesenc(xmm6, t1);\n-  __ aesenc(xmm7, t1);\n-  __ aesenc(xmm8, t1);\n-\n-  __ movdqu(t1, Address(rsp, 16 * 1));\n-  __ movdqu(t5, (Address(subkeyHtbl, 6 * 16)));\n-  __ vpclmulqdq(t3, t1, t5, 0x11);\n-  __ vpxor(t4, t4, t3, Assembler::AVX_128bit);\n-\n-  __ vpclmulqdq(t3, t1, t5, 0x00);\n-  __ vpxor(t7, t7, t3, Assembler::AVX_128bit);\n-\n-  __ vpclmulqdq(t3, t1, t5, 0x01);\n-  __ vpxor(t6, t6, t3, Assembler::AVX_128bit);\n-\n-  __ vpclmulqdq(t3, t1, t5, 0x10);\n-  __ vpxor(t6, t6, t3, Assembler::AVX_128bit);\n-\n-  load_key(t1, key, 16 * 5, rbx \/*rscratch*\/);\n-  __ aesenc(xmm1, t1);\n-  __ aesenc(xmm2, t1);\n-  __ aesenc(xmm3, t1);\n-  __ aesenc(xmm4, t1);\n-  __ aesenc(xmm5, t1);\n-  __ aesenc(xmm6, t1);\n-  __ aesenc(xmm7, t1);\n-  __ aesenc(xmm8, t1);\n-\n-  __ movdqu(t1, Address(rsp, 16 * 2));\n-  __ movdqu(t5, (Address(subkeyHtbl, 5 * 16)));\n-  __ vpclmulqdq(t3, t1, t5, 0x11);\n-  __ vpxor(t4, t4, t3, Assembler::AVX_128bit);\n-\n-  __ vpclmulqdq(t3, t1, t5, 0x00);\n-  __ vpxor(t7, t7, t3, Assembler::AVX_128bit);\n-\n-  __ vpclmulqdq(t3, t1, t5, 0x01);\n-  __ vpxor(t6, t6, t3, Assembler::AVX_128bit);\n-\n-  __ vpclmulqdq(t3, t1, t5, 0x10);\n-  __ vpxor(t6, t6, t3, Assembler::AVX_128bit);\n-\n-  load_key(t1, key, 16 * 6, rbx \/*rscratch*\/);\n-  __ aesenc(xmm1, t1);\n-  __ aesenc(xmm2, t1);\n-  __ aesenc(xmm3, t1);\n-  __ aesenc(xmm4, t1);\n-  __ aesenc(xmm5, t1);\n-  __ aesenc(xmm6, t1);\n-  __ aesenc(xmm7, t1);\n-  __ aesenc(xmm8, t1);\n-\n-  __ movdqu(t1, Address(rsp, 16 * 3));\n-  __ movdqu(t5, (Address(subkeyHtbl, 4 * 16)));\n-  __ vpclmulqdq(t3, t1, t5, 0x11);\n-  __ vpxor(t4, t4, t3, Assembler::AVX_128bit);\n-\n-  __ vpclmulqdq(t3, t1, t5, 0x00);\n-  __ vpxor(t7, t7, t3, Assembler::AVX_128bit);\n-\n-  __ vpclmulqdq(t3, t1, t5, 0x01);\n-  __ vpxor(t6, t6, t3, Assembler::AVX_128bit);\n-\n-  __ vpclmulqdq(t3, t1, t5, 0x10);\n-  __ vpxor(t6, t6, t3, Assembler::AVX_128bit);\n-\n-  load_key(t1, key, 16 * 7, rbx \/*rscratch*\/);\n-  __ aesenc(xmm1, t1);\n-  __ aesenc(xmm2, t1);\n-  __ aesenc(xmm3, t1);\n-  __ aesenc(xmm4, t1);\n-  __ aesenc(xmm5, t1);\n-  __ aesenc(xmm6, t1);\n-  __ aesenc(xmm7, t1);\n-  __ aesenc(xmm8, t1);\n-\n-  __ movdqu(t1, Address(rsp, 16 * 4));\n-  __ movdqu(t5, (Address(subkeyHtbl, 3 * 16)));\n-  __ vpclmulqdq(t3, t1, t5, 0x11);\n-  __ vpxor(t4, t4, t3, Assembler::AVX_128bit);\n-\n-  __ vpclmulqdq(t3, t1, t5, 0x00);\n-  __ vpxor(t7, t7, t3, Assembler::AVX_128bit);\n-\n-  __ vpclmulqdq(t3, t1, t5, 0x01);\n-  __ vpxor(t6, t6, t3, Assembler::AVX_128bit);\n-\n-  __ vpclmulqdq(t3, t1, t5, 0x10);\n-  __ vpxor(t6, t6, t3, Assembler::AVX_128bit);\n-\n-  load_key(t1, key, 16 * 8, rbx \/*rscratch*\/);\n-  __ aesenc(xmm1, t1);\n-  __ aesenc(xmm2, t1);\n-  __ aesenc(xmm3, t1);\n-  __ aesenc(xmm4, t1);\n-  __ aesenc(xmm5, t1);\n-  __ aesenc(xmm6, t1);\n-  __ aesenc(xmm7, t1);\n-  __ aesenc(xmm8, t1);\n-\n-  __ movdqu(t1, Address(rsp, 16 * 5));\n-  __ movdqu(t5, (Address(subkeyHtbl, 2 * 16)));\n-  __ vpclmulqdq(t3, t1, t5, 0x11);\n-  __ vpxor(t4, t4, t3, Assembler::AVX_128bit);\n-\n-  __ vpclmulqdq(t3, t1, t5, 0x00);\n-  __ vpxor(t7, t7, t3, Assembler::AVX_128bit);\n-\n-  __ vpclmulqdq(t3, t1, t5, 0x01);\n-  __ vpxor(t6, t6, t3, Assembler::AVX_128bit);\n-\n-  __ vpclmulqdq(t3, t1, t5, 0x10);\n-  __ vpxor(t6, t6, t3, Assembler::AVX_128bit);\n+  for (int i = 3, j = 0; i <= 8; i++, j++) {\n+    load_key(t1, key, 16 * i, rbx \/*rscratch*\/);\n+    aesenc_step_avx2(t1);\n+    __ movdqu(t1, Address(rsp, 16 * j));\n+    __ movdqu(t5, (Address(subkeyHtbl, (7 - j) * 16)));\n+    ghash_step_avx2(t1, t5);\n+  }\n@@ -3595,8 +3455,1 @@\n-  __ aesenc(xmm1, t1);\n-  __ aesenc(xmm2, t1);\n-  __ aesenc(xmm3, t1);\n-  __ aesenc(xmm4, t1);\n-  __ aesenc(xmm5, t1);\n-  __ aesenc(xmm6, t1);\n-  __ aesenc(xmm7, t1);\n-  __ aesenc(xmm8, t1);\n+  aesenc_step_avx2(t1);\n@@ -3626,2 +3479,2 @@\n-  __ jcc(Assembler::greaterEqual, aes_192);\n-  __ jmp(last_aes_rnd);\n+  __ jcc(Assembler::less, last_aes_rnd);\n+\n@@ -3629,8 +3482,1 @@\n-  __ aesenc(xmm1, t5);\n-  __ aesenc(xmm2, t5);\n-  __ aesenc(xmm3, t5);\n-  __ aesenc(xmm4, t5);\n-  __ aesenc(xmm5, t5);\n-  __ aesenc(xmm6, t5);\n-  __ aesenc(xmm7, t5);\n-  __ aesenc(xmm8, t5);\n+  aesenc_step_avx2(t5);\n@@ -3638,8 +3484,1 @@\n-  __ aesenc(xmm1, t5);\n-  __ aesenc(xmm2, t5);\n-  __ aesenc(xmm3, t5);\n-  __ aesenc(xmm4, t5);\n-  __ aesenc(xmm5, t5);\n-  __ aesenc(xmm6, t5);\n-  __ aesenc(xmm7, t5);\n-  __ aesenc(xmm8, t5);\n+  aesenc_step_avx2(t5);\n@@ -3648,2 +3487,1 @@\n-  __ jcc(Assembler::aboveEqual, aes_256);\n-  __ jmp(last_aes_rnd);\n+  __ jcc(Assembler::less, last_aes_rnd);\n@@ -3652,8 +3490,1 @@\n-  __ aesenc(xmm1, t5);\n-  __ aesenc(xmm2, t5);\n-  __ aesenc(xmm3, t5);\n-  __ aesenc(xmm4, t5);\n-  __ aesenc(xmm5, t5);\n-  __ aesenc(xmm6, t5);\n-  __ aesenc(xmm7, t5);\n-  __ aesenc(xmm8, t5);\n+  aesenc_step_avx2(t5);\n@@ -3661,8 +3492,1 @@\n-  __ aesenc(xmm1, t5);\n-  __ aesenc(xmm2, t5);\n-  __ aesenc(xmm3, t5);\n-  __ aesenc(xmm4, t5);\n-  __ aesenc(xmm5, t5);\n-  __ aesenc(xmm6, t5);\n-  __ aesenc(xmm7, t5);\n-  __ aesenc(xmm8, t5);\n+  aesenc_step_avx2(t5);\n@@ -3671,25 +3495,8 @@\n-  __ aesenclast(xmm1, t5);\n-  __ aesenclast(xmm2, t5);\n-  __ aesenclast(xmm3, t5);\n-  __ aesenclast(xmm4, t5);\n-  __ aesenclast(xmm5, t5);\n-  __ aesenclast(xmm6, t5);\n-  __ aesenclast(xmm7, t5);\n-  __ aesenclast(xmm8, t5);\n-\n-  __ movdqu(t2, Address(in, pos, Address::times_1, 16 * 0));\n-  __ vpxor(xmm1, xmm1, t2, Assembler::AVX_128bit);\n-  __ movdqu(t2, Address(in, pos, Address::times_1, 16 * 1));\n-  __ vpxor(xmm2, xmm2, t2, Assembler::AVX_128bit);\n-  __ movdqu(t2, Address(in, pos, Address::times_1, 16 * 2));\n-  __ vpxor(xmm3, xmm3, t2, Assembler::AVX_128bit);\n-  __ movdqu(t2, Address(in, pos, Address::times_1, 16 * 3));\n-  __ vpxor(xmm4, xmm4, t2, Assembler::AVX_128bit);\n-  __ movdqu(t2, Address(in, pos, Address::times_1, 16 * 4));\n-  __ vpxor(xmm5, xmm5, t2, Assembler::AVX_128bit);\n-  __ movdqu(t2, Address(in, pos, Address::times_1, 16 * 5));\n-  __ vpxor(xmm6, xmm6, t2, Assembler::AVX_128bit);\n-  __ movdqu(t2, Address(in, pos, Address::times_1, 16 * 6));\n-  __ vpxor(xmm7, xmm7, t2, Assembler::AVX_128bit);\n-  __ movdqu(t2, Address(in, pos, Address::times_1, 16 * 7));\n-  __ vpxor(xmm8, xmm8, t2, Assembler::AVX_128bit);\n+  for (int rnum = 1; rnum <= 8; rnum++) {\n+    __ aesenclast(as_XMMRegister(rnum), t5);\n+  }\n+\n+  for (int i = 0; i <= 7; i++) {\n+    __ movdqu(t2, Address(in, pos, Address::times_1, 16 * i));\n+    __ vpxor(as_XMMRegister(i + 1), as_XMMRegister(i + 1), t2, Assembler::AVX_128bit);\n+  }\n@@ -3706,8 +3513,3 @@\n-  __ movdqu(Address(out, pos, Address::times_1, 16 * 0), xmm1);\n-  __ movdqu(Address(out, pos, Address::times_1, 16 * 1), xmm2);\n-  __ movdqu(Address(out, pos, Address::times_1, 16 * 2), xmm3);\n-  __ movdqu(Address(out, pos, Address::times_1, 16 * 3), xmm4);\n-  __ movdqu(Address(out, pos, Address::times_1, 16 * 4), xmm5);\n-  __ movdqu(Address(out, pos, Address::times_1, 16 * 5), xmm6);\n-  __ movdqu(Address(out, pos, Address::times_1, 16 * 6), xmm7);\n-  __ movdqu(Address(out, pos, Address::times_1, 16 * 7), xmm8);\n+  for (int i = 0; i <= 7; i++) {\n+    __ movdqu(Address(out, pos, Address::times_1, 16 * i), as_XMMRegister(i + 1));\n+  }\n@@ -3717,8 +3519,3 @@\n-  __ movdqu(xmm1, Address(in, pos, Address::times_1, 16 * 0));\n-  __ movdqu(xmm2, Address(in, pos, Address::times_1, 16 * 1));\n-  __ movdqu(xmm3, Address(in, pos, Address::times_1, 16 * 2));\n-  __ movdqu(xmm4, Address(in, pos, Address::times_1, 16 * 3));\n-  __ movdqu(xmm5, Address(in, pos, Address::times_1, 16 * 4));\n-  __ movdqu(xmm6, Address(in, pos, Address::times_1, 16 * 5));\n-  __ movdqu(xmm7, Address(in, pos, Address::times_1, 16 * 6));\n-  __ movdqu(xmm8, Address(in, pos, Address::times_1, 16 * 7));\n+  for (int i = 0; i <= 7; i++) {\n+    __ movdqu(as_XMMRegister(i + 1), Address(in, pos, Address::times_1, 16 * i));\n+  }\n@@ -3737,8 +3534,3 @@\n-  __ vpshufb(xmm1, xmm1, ExternalAddress(counter_shuffle_mask_addr()), Assembler::AVX_128bit, rbx \/*rscratch*\/);\n-  __ vpshufb(xmm2, xmm2, ExternalAddress(counter_shuffle_mask_addr()), Assembler::AVX_128bit, rbx \/*rscratch*\/);\n-  __ vpshufb(xmm3, xmm3, ExternalAddress(counter_shuffle_mask_addr()), Assembler::AVX_128bit, rbx \/*rscratch*\/);\n-  __ vpshufb(xmm4, xmm4, ExternalAddress(counter_shuffle_mask_addr()), Assembler::AVX_128bit, rbx \/*rscratch*\/);\n-  __ vpshufb(xmm5, xmm5, ExternalAddress(counter_shuffle_mask_addr()), Assembler::AVX_128bit, rbx \/*rscratch*\/);\n-  __ vpshufb(xmm6, xmm6, ExternalAddress(counter_shuffle_mask_addr()), Assembler::AVX_128bit, rbx \/*rscratch*\/);\n-  __ vpshufb(xmm7, xmm7, ExternalAddress(counter_shuffle_mask_addr()), Assembler::AVX_128bit, rbx \/*rscratch*\/);\n-  __ vpshufb(xmm8, xmm8, ExternalAddress(counter_shuffle_mask_addr()), Assembler::AVX_128bit, rbx \/*rscratch*\/);\n+  for (int rnum = 1; rnum <= 8; rnum++) {\n+    __ vpshufb(as_XMMRegister(rnum), as_XMMRegister(rnum), ExternalAddress(counter_shuffle_mask_addr()), Assembler::AVX_128bit, rbx \/*rscratch*\/);\n+  }\n@@ -3749,1 +3541,1 @@\n-void StubGenerator::ghash_last_8(Register subkeyHtbl) {\n+void StubGenerator::ghash_last_8_avx2(Register subkeyHtbl) {\n@@ -3771,106 +3563,13 @@\n-  __ movdqu(t5, Address(subkeyHtbl, 7 * 16));\n-  __ vpshufd(t2, xmm2, 78, Assembler::AVX_128bit);\n-  __ vpshufd(t3, t5, 78, Assembler::AVX_128bit);\n-  __ vpxor(t2, t2, xmm2, Assembler::AVX_128bit);\n-  __ vpxor(t3, t3, t5, Assembler::AVX_128bit);\n-\n-  __ vpclmulqdq(t4, xmm2, t5, 0x11);\n-  __ vpxor(t6, t6, t4, Assembler::AVX_128bit);\n-\n-  __ vpclmulqdq(t4, xmm2, t5, 0x00);\n-  __ vpxor(t7, t7, t4, Assembler::AVX_128bit);\n-\n-  __ vpclmulqdq(t2, t2, t3, 0x00);\n-\n-  __ vpxor(xmm1, xmm1, t2, Assembler::AVX_128bit);\n-\n-  __ movdqu(t5, Address(subkeyHtbl, 6 * 16));\n-  __ vpshufd(t2, xmm3, 78, Assembler::AVX_128bit);\n-  __ vpshufd(t3, t5, 78, Assembler::AVX_128bit);\n-  __ vpxor(t2, t2, xmm3, Assembler::AVX_128bit);\n-  __ vpxor(t3, t3, t5, Assembler::AVX_128bit);\n-\n-  __ vpclmulqdq(t4, xmm3, t5, 0x11);\n-  __ vpxor(t6, t6, t4, Assembler::AVX_128bit);\n-\n-  __ vpclmulqdq(t4, xmm3, t5, 0x00);\n-  __ vpxor(t7, t7, t4, Assembler::AVX_128bit);\n-  __ vpclmulqdq(t2, t2, t3, 0x00);\n-\n-  __ vpxor(xmm1, xmm1, t2, Assembler::AVX_128bit);\n-\n-  __ movdqu(t5, Address(subkeyHtbl, 5 * 16));\n-  __ vpshufd(t2, xmm4, 78, Assembler::AVX_128bit);\n-  __ vpshufd(t3, t5, 78, Assembler::AVX_128bit);\n-  __ vpxor(t2, t2, xmm4, Assembler::AVX_128bit);\n-  __ vpxor(t3, t3, t5, Assembler::AVX_128bit);\n-\n-  __ vpclmulqdq(t4, xmm4, t5, 0x11);\n-  __ vpxor(t6, t6, t4, Assembler::AVX_128bit);\n-\n-  __ vpclmulqdq(t4, xmm4, t5, 0x00);\n-  __ vpxor(t7, t7, t4, Assembler::AVX_128bit);\n-\n-  __ vpclmulqdq(t2, t2, t3, 0x00);\n-\n-  __ vpxor(xmm1, xmm1, t2, Assembler::AVX_128bit);\n-  __ movdqu(t5, Address(subkeyHtbl, 4 * 16));\n-  __ vpshufd(t2, xmm5, 78, Assembler::AVX_128bit);\n-  __ vpshufd(t3, t5, 78, Assembler::AVX_128bit);\n-  __ vpxor(t2, t2, xmm5, Assembler::AVX_128bit);\n-  __ vpxor(t3, t3, t5, Assembler::AVX_128bit);\n-\n-  __ vpclmulqdq(t4, xmm5, t5, 0x11);\n-  __ vpxor(t6, t6, t4, Assembler::AVX_128bit);\n-\n-  __ vpclmulqdq(t4, xmm5, t5, 0x00);\n-  __ vpxor(t7, t7, t4, Assembler::AVX_128bit);\n-\n-  __ vpclmulqdq(t2, t2, t3, 0x00);\n-\n-  __ vpxor(xmm1, xmm1, t2, Assembler::AVX_128bit);\n-\n-  __ movdqu(t5, Address(subkeyHtbl, 3 * 16));\n-  __ vpshufd(t2, xmm6, 78, Assembler::AVX_128bit);\n-  __ vpshufd(t3, t5, 78, Assembler::AVX_128bit);\n-  __ vpxor(t2, t2, xmm6, Assembler::AVX_128bit);\n-  __ vpxor(t3, t3, t5, Assembler::AVX_128bit);\n-\n-  __ vpclmulqdq(t4, xmm6, t5, 0x11);\n-  __ vpxor(t6, t6, t4, Assembler::AVX_128bit);\n-\n-  __ vpclmulqdq(t4, xmm6, t5, 0x00);\n-  __ vpxor(t7, t7, t4, Assembler::AVX_128bit);\n-\n-  __ vpclmulqdq(t2, t2, t3, 0x00);\n-  __ vpxor(xmm1, xmm1, t2, Assembler::AVX_128bit);\n-\n-  __ movdqu(t5, Address(subkeyHtbl, 2 * 16));\n-  __ vpshufd(t2, xmm7, 78, Assembler::AVX_128bit);\n-  __ vpshufd(t3, t5, 78, Assembler::AVX_128bit);\n-  __ vpxor(t2, t2, xmm7, Assembler::AVX_128bit);\n-  __ vpxor(t3, t3, t5, Assembler::AVX_128bit);\n-\n-  __ vpclmulqdq(t4, xmm7, t5, 0x11);\n-  __ vpxor(t6, t6, t4, Assembler::AVX_128bit);\n-\n-  __ vpclmulqdq(t4, xmm7, t5, 0x00);\n-  __ vpxor(t7, t7, t4, Assembler::AVX_128bit);\n-\n-  __ vpclmulqdq(t2, t2, t3, 0x00);\n-\n-  __ vpxor(xmm1, xmm1, t2, Assembler::AVX_128bit);\n-\n-  __ movdqu(t5, Address(subkeyHtbl, 1 * 16));\n-  __ vpshufd(t2, xmm8, 78, Assembler::AVX_128bit);\n-  __ vpshufd(t3, t5, 78, Assembler::AVX_128bit);\n-  __ vpxor(t2, t2, xmm8, Assembler::AVX_128bit);\n-  __ vpxor(t3, t3, t5, Assembler::AVX_128bit);\n-\n-  __ vpclmulqdq(t4, xmm8, t5, 0x11);\n-  __ vpxor(t6, t6, t4, Assembler::AVX_128bit);\n-\n-  __ vpclmulqdq(t4, xmm8, t5, 0x00);\n-  __ vpxor(t7, t7, t4, Assembler::AVX_128bit);\n-\n-  __ vpclmulqdq(t2, t2, t3, 0x00);\n+  for (int i = 7, rnum = 2; rnum <= 8; i--, rnum++) {\n+    __ movdqu(t5, Address(subkeyHtbl, i * 16));\n+    __ vpshufd(t2, as_XMMRegister(rnum), 78, Assembler::AVX_128bit);\n+    __ vpshufd(t3, t5, 78, Assembler::AVX_128bit);\n+    __ vpxor(t2, t2, as_XMMRegister(rnum), Assembler::AVX_128bit);\n+    __ vpxor(t3, t3, t5, Assembler::AVX_128bit);\n+    __ vpclmulqdq(t4, as_XMMRegister(rnum), t5, 0x11);\n+    __ vpxor(t6, t6, t4, Assembler::AVX_128bit);\n+    __ vpclmulqdq(t4, as_XMMRegister(rnum), t5, 0x00);\n+    __ vpxor(t7, t7, t4, Assembler::AVX_128bit);\n+    __ vpclmulqdq(t2, t2, t3, 0x00);\n+    __ vpxor(xmm1, xmm1, t2, Assembler::AVX_128bit);\n+  }\n@@ -3878,1 +3577,0 @@\n-  __ vpxor(xmm1, xmm1, t2, Assembler::AVX_128bit);\n@@ -3906,2 +3604,2 @@\n-void StubGenerator::initial_blocks(XMMRegister ctr, Register rounds, Register key,\n-  Register len, Register in, Register out, Register ct, Register subkeyHtbl, Register pos) {\n+void StubGenerator::initial_blocks_avx2(XMMRegister ctr, Register rounds, Register key, Register len, Register in,\n+                                        Register out, Register ct, Register subkeyHtbl, Register pos) {\n@@ -3921,1 +3619,1 @@\n-  __ movdqu(xmm1, xmm9);\n+  __ movdqu(xmm1, ctr);\n@@ -3933,8 +3631,3 @@\n-  __ vpshufb(xmm1, xmm1, t5, Assembler::AVX_128bit); \/\/perform a 16Byte swap\n-  __ vpshufb(xmm2, xmm2, t5, Assembler::AVX_128bit); \/\/perform a 16Byte swap\n-  __ vpshufb(xmm3, xmm3, t5, Assembler::AVX_128bit); \/\/perform a 16Byte swap\n-  __ vpshufb(xmm4, xmm4, t5, Assembler::AVX_128bit); \/\/perform a 16Byte swap\n-  __ vpshufb(xmm5, xmm5, t5, Assembler::AVX_128bit); \/\/perform a 16Byte swap\n-  __ vpshufb(xmm6, xmm6, t5, Assembler::AVX_128bit); \/\/perform a 16Byte swap\n-  __ vpshufb(xmm7, xmm7, t5, Assembler::AVX_128bit); \/\/perform a 16Byte swap\n-  __ vpshufb(xmm8, xmm8, t5, Assembler::AVX_128bit); \/\/perform a 16Byte swap\n+  for (int rnum = 1; rnum <= 8; rnum++) {\n+    __ vpshufb(as_XMMRegister(rnum), as_XMMRegister(rnum), t5, Assembler::AVX_128bit); \/\/perform a 16Byte swap\n+  }\n@@ -3943,98 +3636,8 @@\n-  __ vpxor(xmm1, xmm1, t_key, Assembler::AVX_128bit);\n-  __ vpxor(xmm2, xmm2, t_key, Assembler::AVX_128bit);\n-  __ vpxor(xmm3, xmm3, t_key, Assembler::AVX_128bit);\n-  __ vpxor(xmm4, xmm4, t_key, Assembler::AVX_128bit);\n-  __ vpxor(xmm5, xmm5, t_key, Assembler::AVX_128bit);\n-  __ vpxor(xmm6, xmm6, t_key, Assembler::AVX_128bit);\n-  __ vpxor(xmm7, xmm7, t_key, Assembler::AVX_128bit);\n-  __ vpxor(xmm8, xmm8, t_key, Assembler::AVX_128bit);\n-\n-  load_key(t_key, key, 16 * 1, rbx \/*rscratch*\/);\n-  __ aesenc(xmm1, t_key);\n-  __ aesenc(xmm2, t_key);\n-  __ aesenc(xmm3, t_key);\n-  __ aesenc(xmm4, t_key);\n-  __ aesenc(xmm5, t_key);\n-  __ aesenc(xmm6, t_key);\n-  __ aesenc(xmm7, t_key);\n-  __ aesenc(xmm8, t_key);\n-\n-  load_key(t_key, key, 16 * 2, rbx \/*rscratch*\/);\n-  __ aesenc(xmm1, t_key);\n-  __ aesenc(xmm2, t_key);\n-  __ aesenc(xmm3, t_key);\n-  __ aesenc(xmm4, t_key);\n-  __ aesenc(xmm5, t_key);\n-  __ aesenc(xmm6, t_key);\n-  __ aesenc(xmm7, t_key);\n-  __ aesenc(xmm8, t_key);\n-\n-  load_key(t_key, key, 16 * 3, rbx \/*rscratch*\/);\n-  __ aesenc(xmm1, t_key);\n-  __ aesenc(xmm2, t_key);\n-  __ aesenc(xmm3, t_key);\n-  __ aesenc(xmm4, t_key);\n-  __ aesenc(xmm5, t_key);\n-  __ aesenc(xmm6, t_key);\n-  __ aesenc(xmm7, t_key);\n-  __ aesenc(xmm8, t_key);\n-\n-  load_key(t_key, key, 16 * 4, rbx \/*rscratch*\/);\n-  __ aesenc(xmm1, t_key);\n-  __ aesenc(xmm2, t_key);\n-  __ aesenc(xmm3, t_key);\n-  __ aesenc(xmm4, t_key);\n-  __ aesenc(xmm5, t_key);\n-  __ aesenc(xmm6, t_key);\n-  __ aesenc(xmm7, t_key);\n-  __ aesenc(xmm8, t_key);\n-\n-  load_key(t_key, key, 16 * 5, rbx \/*rscratch*\/);\n-  __ aesenc(xmm1, t_key);\n-  __ aesenc(xmm2, t_key);\n-  __ aesenc(xmm3, t_key);\n-  __ aesenc(xmm4, t_key);\n-  __ aesenc(xmm5, t_key);\n-  __ aesenc(xmm6, t_key);\n-  __ aesenc(xmm7, t_key);\n-  __ aesenc(xmm8, t_key);\n-\n-  load_key(t_key, key, 16 * 6, rbx \/*rscratch*\/);\n-  __ aesenc(xmm1, t_key);\n-  __ aesenc(xmm2, t_key);\n-  __ aesenc(xmm3, t_key);\n-  __ aesenc(xmm4, t_key);\n-  __ aesenc(xmm5, t_key);\n-  __ aesenc(xmm6, t_key);\n-  __ aesenc(xmm7, t_key);\n-  __ aesenc(xmm8, t_key);\n-\n-  load_key(t_key, key, 16 * 7, rbx \/*rscratch*\/);\n-  __ aesenc(xmm1, t_key);\n-  __ aesenc(xmm2, t_key);\n-  __ aesenc(xmm3, t_key);\n-  __ aesenc(xmm4, t_key);\n-  __ aesenc(xmm5, t_key);\n-  __ aesenc(xmm6, t_key);\n-  __ aesenc(xmm7, t_key);\n-  __ aesenc(xmm8, t_key);\n-\n-  load_key(t_key, key, 16 * 8, rbx \/*rscratch*\/);\n-  __ aesenc(xmm1, t_key);\n-  __ aesenc(xmm2, t_key);\n-  __ aesenc(xmm3, t_key);\n-  __ aesenc(xmm4, t_key);\n-  __ aesenc(xmm5, t_key);\n-  __ aesenc(xmm6, t_key);\n-  __ aesenc(xmm7, t_key);\n-  __ aesenc(xmm8, t_key);\n-\n-  load_key(t_key, key, 16 * 9, rbx \/*rscratch*\/);\n-  __ aesenc(xmm1, t_key);\n-  __ aesenc(xmm2, t_key);\n-  __ aesenc(xmm3, t_key);\n-  __ aesenc(xmm4, t_key);\n-  __ aesenc(xmm5, t_key);\n-  __ aesenc(xmm6, t_key);\n-  __ aesenc(xmm7, t_key);\n-  __ aesenc(xmm8, t_key);\n+  for (int rnum = 1; rnum <= 8; rnum++) {\n+    __ vpxor(as_XMMRegister(rnum), as_XMMRegister(rnum), t_key, Assembler::AVX_128bit);\n+  }\n+\n+  for (int i = 1; i <= 9; i++) {\n+    load_key(t_key, key, 16 * i, rbx \/*rscratch*\/);\n+    aesenc_step_avx2(t_key);\n+  }\n@@ -4044,2 +3647,1 @@\n-  __ jcc(Assembler::greaterEqual, aes_192);\n-  __ jmp(last_aes_rnd);\n+  __ jcc(Assembler::less, last_aes_rnd);\n@@ -4048,8 +3650,1 @@\n-  __ aesenc(xmm1, t_key);\n-  __ aesenc(xmm2, t_key);\n-  __ aesenc(xmm3, t_key);\n-  __ aesenc(xmm4, t_key);\n-  __ aesenc(xmm5, t_key);\n-  __ aesenc(xmm6, t_key);\n-  __ aesenc(xmm7, t_key);\n-  __ aesenc(xmm8, t_key);\n+  aesenc_step_avx2(t_key);\n@@ -4057,8 +3652,1 @@\n-  __ aesenc(xmm1, t_key);\n-  __ aesenc(xmm2, t_key);\n-  __ aesenc(xmm3, t_key);\n-  __ aesenc(xmm4, t_key);\n-  __ aesenc(xmm5, t_key);\n-  __ aesenc(xmm6, t_key);\n-  __ aesenc(xmm7, t_key);\n-  __ aesenc(xmm8, t_key);\n+  aesenc_step_avx2(t_key);\n@@ -4067,2 +3655,1 @@\n-  __ jcc(Assembler::aboveEqual, aes_256);\n-  __ jmp(last_aes_rnd);\n+  __ jcc(Assembler::less, last_aes_rnd);\n@@ -4071,8 +3658,1 @@\n-  __ aesenc(xmm1, t_key);\n-  __ aesenc(xmm2, t_key);\n-  __ aesenc(xmm3, t_key);\n-  __ aesenc(xmm4, t_key);\n-  __ aesenc(xmm5, t_key);\n-  __ aesenc(xmm6, t_key);\n-  __ aesenc(xmm7, t_key);\n-  __ aesenc(xmm8, t_key);\n+  aesenc_step_avx2(t_key);\n@@ -4080,8 +3660,1 @@\n-  __ aesenc(xmm1, t_key);\n-  __ aesenc(xmm2, t_key);\n-  __ aesenc(xmm3, t_key);\n-  __ aesenc(xmm4, t_key);\n-  __ aesenc(xmm5, t_key);\n-  __ aesenc(xmm6, t_key);\n-  __ aesenc(xmm7, t_key);\n-  __ aesenc(xmm8, t_key);\n+  aesenc_step_avx2(t_key);\n@@ -4091,8 +3664,3 @@\n-  __ aesenclast(xmm1, t_key);\n-  __ aesenclast(xmm2, t_key);\n-  __ aesenclast(xmm3, t_key);\n-  __ aesenclast(xmm4, t_key);\n-  __ aesenclast(xmm5, t_key);\n-  __ aesenclast(xmm6, t_key);\n-  __ aesenclast(xmm7, t_key);\n-  __ aesenclast(xmm8, t_key);\n+  for (int rnum = 1; rnum <= 8; rnum++) {\n+    __ aesenclast(as_XMMRegister(rnum), t_key);\n+  }\n@@ -4103,31 +3671,5 @@\n-  __ movdqu(t1, Address(in, pos, Address::times_1, 16 * 0));\n-  __ vpxor(xmm1, xmm1, t1, Assembler::AVX_128bit);\n-  __ movdqu(Address(out, pos, Address::times_1, 16 * 0), xmm1);\n-\n-  __ movdqu(t1, Address(in, pos, Address::times_1, 16 * 1));\n-  __ vpxor(xmm2, xmm2, t1, Assembler::AVX_128bit);\n-  __ movdqu(Address(out, pos, Address::times_1, 16 * 1), xmm2);\n-\n-  __ movdqu(t1, Address(in, pos, Address::times_1, 16 * 2));\n-  __ vpxor(xmm3, xmm3, t1, Assembler::AVX_128bit);\n-  __ movdqu(Address(out, pos, Address::times_1, 16 * 2), xmm3);\n-\n-  __ movdqu(t1, Address(in, pos, Address::times_1, 16 * 3));\n-  __ vpxor(xmm4, xmm4, t1, Assembler::AVX_128bit);\n-  __ movdqu(Address(out, pos, Address::times_1, 16 * 3), xmm4);\n-\n-  __ movdqu(t1, Address(in, pos, Address::times_1, 16 * 4));\n-  __ vpxor(xmm5, xmm5, t1, Assembler::AVX_128bit);\n-  __ movdqu(Address(out, pos, Address::times_1, 16 * 4), xmm5);\n-\n-  __ movdqu(t1, Address(in, pos, Address::times_1, 16 * 5));\n-  __ vpxor(xmm6, xmm6, t1, Assembler::AVX_128bit);\n-  __ movdqu(Address(out, pos, Address::times_1, 16 * 5), xmm6);\n-\n-  __ movdqu(t1, Address(in, pos, Address::times_1, 16 * 6));\n-  __ vpxor(xmm7, xmm7, t1, Assembler::AVX_128bit);\n-  __ movdqu(Address(out, pos, Address::times_1, 16 * 6), xmm7);\n-\n-  __ movdqu(t1, Address(in, pos, Address::times_1, 16 * 7));\n-  __ vpxor(xmm8, xmm8, t1, Assembler::AVX_128bit);\n-  __ movdqu(Address(out, pos, Address::times_1, 16 * 7), xmm8);\n+  for (int i = 0; i <= 7; i++) {\n+    __ movdqu(t1, Address(in, pos, Address::times_1, 16 * i));\n+    __ vpxor(as_XMMRegister(i + 1), as_XMMRegister(i + 1), t1, Assembler::AVX_128bit);\n+    __ movdqu(Address(out, pos, Address::times_1, 16 * i), as_XMMRegister(i + 1));\n+  }\n@@ -4137,8 +3679,3 @@\n-  __ movdqu(xmm1, Address(in, pos, Address::times_1, 16 * 0));\n-  __ movdqu(xmm2, Address(in, pos, Address::times_1, 16 * 1));\n-  __ movdqu(xmm3, Address(in, pos, Address::times_1, 16 * 2));\n-  __ movdqu(xmm4, Address(in, pos, Address::times_1, 16 * 3));\n-  __ movdqu(xmm5, Address(in, pos, Address::times_1, 16 * 4));\n-  __ movdqu(xmm6, Address(in, pos, Address::times_1, 16 * 5));\n-  __ movdqu(xmm7, Address(in, pos, Address::times_1, 16 * 6));\n-  __ movdqu(xmm8, Address(in, pos, Address::times_1, 16 * 7));\n+  for (int i = 0; i <= 7; i++) {\n+    __ movdqu(as_XMMRegister(i + 1), Address(in, pos, Address::times_1, 16 * i));\n+  }\n@@ -4151,1 +3688,3 @@\n-  __ vpshufb(xmm1, xmm1, ExternalAddress(counter_shuffle_mask_addr()), Assembler::AVX_128bit, rbx \/*rscratch*\/);\n+  for (int rnum = 1; rnum <= 8; rnum++) {\n+    __ vpshufb(as_XMMRegister(rnum), as_XMMRegister(rnum), ExternalAddress(counter_shuffle_mask_addr()), Assembler::AVX_128bit, rbx \/*rscratch*\/);\n+  }\n@@ -4154,7 +3693,0 @@\n-  __ vpshufb(xmm2, xmm2, ExternalAddress(counter_shuffle_mask_addr()), Assembler::AVX_128bit, rbx \/*rscratch*\/);\n-  __ vpshufb(xmm3, xmm3, ExternalAddress(counter_shuffle_mask_addr()), Assembler::AVX_128bit, rbx \/*rscratch*\/);\n-  __ vpshufb(xmm4, xmm4, ExternalAddress(counter_shuffle_mask_addr()), Assembler::AVX_128bit, rbx \/*rscratch*\/);\n-  __ vpshufb(xmm5, xmm5, ExternalAddress(counter_shuffle_mask_addr()), Assembler::AVX_128bit, rbx \/*rscratch*\/);\n-  __ vpshufb(xmm6, xmm6, ExternalAddress(counter_shuffle_mask_addr()), Assembler::AVX_128bit, rbx \/*rscratch*\/);\n-  __ vpshufb(xmm7, xmm7, ExternalAddress(counter_shuffle_mask_addr()), Assembler::AVX_128bit, rbx \/*rscratch*\/);\n-  __ vpshufb(xmm8, xmm8, ExternalAddress(counter_shuffle_mask_addr()), Assembler::AVX_128bit, rbx \/*rscratch*\/);\n@@ -4169,2 +3701,1 @@\n-  Label ghash_done, encrypt_done, encrypt_by_8_parallel, encrypt_by_8_new, encrypt_by_8, encrypt_by_8_parallel_done,\n-        hash_last_8, enc_dec_done, generate_htbl_8_blks;\n+  Label encrypt_done, encrypt_by_8_parallel, encrypt_by_8_new, encrypt_by_8, hash_last_8, enc_dec_done, generate_htbl_8_blks;\n@@ -4172,0 +3703,1 @@\n+  \/\/This routine should be called only for message sizes of 128 bytes or more.\n@@ -4173,2 +3705,1 @@\n-  \/\/calculate the number of 16byte blocks in the message\n-  \/\/process 8 16 byte blocks in initial_num_blocks.'\n+  \/\/process 8 16 byte blocks in initial_num_blocks.\n@@ -4177,2 +3708,0 @@\n-  __ cmpl(len, 768);\n-  __ jcc(Assembler::less, enc_dec_done);\n@@ -4187,4 +3716,4 @@\n-  __ movdqu(xmm9, Address(counter, 0));\n-  __ movdqu(xmm8, Address(state, 0));\n-  __ vpshufb(xmm9, xmm9, ExternalAddress(counter_shuffle_mask_addr()), Assembler::AVX_128bit, rbx \/*rscratch*\/);\n-  __ vpshufb(xmm8, xmm8, ExternalAddress(ghash_long_swap_mask_addr()), Assembler::AVX_128bit, rbx \/*rscratch*\/);\n+  __ movdqu(ctr_blockx, Address(counter, 0));\n+  __ movdqu(aad_hashx, Address(state, 0));\n+  __ vpshufb(ctr_blockx, ctr_blockx, ExternalAddress(counter_shuffle_mask_addr()), Assembler::AVX_128bit, rbx \/*rscratch*\/);\n+  __ vpshufb(aad_hashx, aad_hashx, ExternalAddress(ghash_long_swap_mask_addr()), Assembler::AVX_128bit, rbx \/*rscratch*\/);\n@@ -4192,2 +3721,1 @@\n-  \/\/Save the amount of data left to process in r14\n-  __ mov(r14, len);\n+  initial_blocks_avx2(ctr_blockx, rounds, key, len, in, out, ct, subkeyHtbl, pos);\n@@ -4195,5 +3723,3 @@\n-  initial_blocks(xmm9, rounds, key, r14, in, out, ct, subkeyHtbl, pos);\n-\n-  \/\/The entire message was encrypted processed in initial and now need to be hashed\n-  __ cmpl(len, 0);\n-  __ jcc(Assembler::equal, encrypt_done);\n+  \/\/We need at least 128 bytes to proceed further.\n+  __ cmpl(len, 128);\n+  __ jcc(Assembler::less, encrypt_done);\n@@ -4205,1 +3731,1 @@\n-  __ movdl(r15, xmm9);\n+  __ movdl(r15, ctr_blockx);\n@@ -4207,1 +3733,1 @@\n-  __ vpshufb(xmm9, xmm9, ExternalAddress(counter_shuffle_mask_addr()), Assembler::AVX_128bit, rbx \/*rscratch*\/);\n+  __ vpshufb(ctr_blockx, ctr_blockx, ExternalAddress(counter_shuffle_mask_addr()), Assembler::AVX_128bit, rbx \/*rscratch*\/);\n@@ -4213,2 +3739,2 @@\n-  __ addl(r15, 8);\n-  ghash8_encrypt8_parallel(key, subkeyHtbl, ctr_blockx, aad_hashx, in, out, ct, pos, false, rounds);\n+  __ addb(r15, 8);\n+  ghash8_encrypt8_parallel_avx2(key, subkeyHtbl, ctr_blockx, aad_hashx, in, out, ct, pos, false, rounds);\n@@ -4216,2 +3742,2 @@\n-  __ subl(r14, 128);\n-  __ cmpl(r14, 128);\n+  __ subl(len, 128);\n+  __ cmpl(len, 128);\n@@ -4220,2 +3746,2 @@\n-  __ vpshufb(xmm9, xmm9, ExternalAddress(counter_shuffle_mask_addr()), Assembler::AVX_128bit, rbx \/*rscratch*\/);\n-  __ jmp(encrypt_by_8_parallel_done);\n+  __ vpshufb(ctr_blockx, ctr_blockx, ExternalAddress(counter_shuffle_mask_addr()), Assembler::AVX_128bit, rbx \/*rscratch*\/);\n+  __ jmp(encrypt_done);\n@@ -4224,1 +3750,1 @@\n-  __ vpshufb(xmm9, xmm9, ExternalAddress(counter_shuffle_mask_addr()), Assembler::AVX_128bit, rbx \/*rscratch*\/);\n+  __ vpshufb(ctr_blockx, ctr_blockx, ExternalAddress(counter_shuffle_mask_addr()), Assembler::AVX_128bit, rbx \/*rscratch*\/);\n@@ -4226,2 +3752,2 @@\n-  __ addl(r15, 8);\n-  ghash8_encrypt8_parallel(key, subkeyHtbl, ctr_blockx, aad_hashx, in, out, ct, pos, true, rounds);\n+  __ addb(r15, 8);\n+  ghash8_encrypt8_parallel_avx2(key, subkeyHtbl, ctr_blockx, aad_hashx, in, out, ct, pos, true, rounds);\n@@ -4229,1 +3755,1 @@\n-  __ vpshufb(xmm9, xmm9, ExternalAddress(counter_shuffle_mask_addr()), Assembler::AVX_128bit, rbx \/*rscratch*\/);\n+  __ vpshufb(ctr_blockx, ctr_blockx, ExternalAddress(counter_shuffle_mask_addr()), Assembler::AVX_128bit, rbx \/*rscratch*\/);\n@@ -4231,2 +3757,2 @@\n-  __ subl(r14, 128);\n-  __ cmpl(r14, 128);\n+  __ subl(len, 128);\n+  __ cmpl(len, 128);\n@@ -4234,6 +3760,1 @@\n-  __ vpshufb(xmm9, xmm9, ExternalAddress(counter_shuffle_mask_addr()), Assembler::AVX_128bit, rbx \/*rscratch*\/);\n-\n-  __ bind(encrypt_by_8_parallel_done);\n-  \/\/ At this point bytes remaining should be either zero or between 113-127.\n-  __ cmpl(r14, 0);\n-  __ jcc(Assembler::equal, encrypt_done);\n+  __ vpshufb(ctr_blockx, ctr_blockx, ExternalAddress(counter_shuffle_mask_addr()), Assembler::AVX_128bit, rbx \/*rscratch*\/);\n@@ -4242,1 +3763,1 @@\n-  ghash_last_8(subkeyHtbl);\n+  ghash_last_8_avx2(subkeyHtbl);\n@@ -4244,5 +3765,3 @@\n-  __ bind(ghash_done);\n-  __ movdqu(xmm15, ExternalAddress(counter_mask_linc1_addr()), rbx \/*rscratch*\/);\n-  __ vpaddd(xmm9, xmm9, xmm15, Assembler::AVX_128bit);\n-  __ vpshufb(xmm9, xmm9, ExternalAddress(counter_shuffle_mask_addr()), Assembler::AVX_128bit, rbx \/*rscratch*\/);\n-  __ movdqu(Address(counter, 0), xmm9); \/\/current_counter = xmm9\n+  __ vpaddd(ctr_blockx, ctr_blockx, ExternalAddress(counter_mask_linc1_addr()), Assembler::AVX_128bit, rbx \/*rscratch*\/);\n+  __ vpshufb(ctr_blockx, ctr_blockx, ExternalAddress(counter_shuffle_mask_addr()), Assembler::AVX_128bit, rbx \/*rscratch*\/);\n+  __ movdqu(Address(counter, 0), ctr_blockx); \/\/current_counter = xmm9\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64_aes.cpp","additions":146,"deletions":627,"binary":false,"changes":773,"status":"modified"},{"patch":"@@ -40,1 +40,1 @@\n-  _compiler_stubs_code_size     = 30000 LP64_ONLY(+30000) WINDOWS_ONLY(+2000),\n+  _compiler_stubs_code_size     = 20000 LP64_ONLY(+32000) WINDOWS_ONLY(+2000),\n","filename":"src\/hotspot\/cpu\/x86\/stubRoutines_x86.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"}]}