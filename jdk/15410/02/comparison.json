{"files":[{"patch":"@@ -1334,0 +1334,5 @@\n+void Assembler::addb(Register dst, int imm8) {\n+  prefix(dst);\n+  emit_arith_b(0x80, 0xC0, dst, imm8);\n+}\n+\n@@ -5321,0 +5326,12 @@\n+void Assembler::vpshufb(XMMRegister dst, XMMRegister nds, Address src, int vector_len) {\n+  assert(vector_len == AVX_128bit ? VM_Version::supports_avx() :\n+         vector_len == AVX_256bit ? VM_Version::supports_avx2() :\n+         vector_len == AVX_512bit ? VM_Version::supports_avx512bw() : 0, \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* rex_w *\/ false, \/* legacy_mode *\/ _legacy_mode_bw, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FVM, \/* input_size_in_bits *\/ EVEX_NObit);\n+  simd_prefix(dst, nds, src, VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8(0x00);\n+  emit_operand(dst, src, 0);\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.cpp","additions":17,"deletions":0,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -988,0 +988,1 @@\n+  void addb(Register dst, int imm8);\n@@ -1955,0 +1956,1 @@\n+  void vpshufb(XMMRegister dst, XMMRegister nds, Address src, int vector_len);\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -9282,0 +9282,11 @@\n+void MacroAssembler::vpshufb(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n+  if (reachable(src)) {\n+    vpshufb(dst, nds, as_Address(src), vector_len);\n+  } else {\n+    lea(rscratch, src);\n+    vpshufb(dst, nds, Address(rscratch, 0), vector_len);\n+  }\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":11,"deletions":0,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -1797,0 +1797,3 @@\n+  using Assembler::vpshufb;\n+  void vpshufb(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register rscratch = noreg);\n+\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -330,0 +330,4 @@\n+  \/\/ AVX2 AES Galois Counter Mode implementation\n+  address generate_avx2_galoisCounterMode_AESCrypt();\n+  void aesgcm_avx2(Register in, Register len, Register ct, Register out, Register key,\n+                   Register state, Register subkeyHtbl, Register counter);\n@@ -356,0 +360,9 @@\n+  \/\/ AVX2 AES-GCM related functions\n+  void initial_blocks_avx2(XMMRegister ctr, Register rounds, Register key, Register len,\n+                      Register in, Register out, Register ct, Register subkeyHtbl, Register pos);\n+  void gfmul_avx2(XMMRegister GH, XMMRegister HK);\n+  void generateHtbl_8_block_avx2(Register htbl, Register rscratch);\n+  void ghash8_encrypt8_parallel_avx2(Register key, Register subkeyHtbl, XMMRegister ctr_blockx, XMMRegister aad_hashx,\n+                                Register in, Register out, Register ct, Register pos, bool out_order, Register rounds);\n+  void ghash_last_8_avx2(Register subkeyHtbl);\n+\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.hpp","additions":13,"deletions":0,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n-* Copyright (c) 2019, 2021, Intel Corporation. All rights reserved.\n+* Copyright (c) 2019, 2023, Intel Corporation. All rights reserved.\n@@ -84,0 +84,24 @@\n+ATTRIBUTE_ALIGNED(16) uint64_t COUNTER_MASK_LINC1F[] = {\n+    0x0000000000000000UL, 0x0100000000000000UL,\n+};\n+\n+static address counter_mask_linc1f_addr() {\n+  return (address)COUNTER_MASK_LINC1F;\n+}\n+\n+ATTRIBUTE_ALIGNED(16) uint64_t COUNTER_MASK_LINC2[] = {\n+    0x0000000000000002UL, 0x0000000000000000UL,\n+};\n+\n+static address counter_mask_linc2_addr() {\n+  return (address)COUNTER_MASK_LINC2;\n+}\n+\n+ATTRIBUTE_ALIGNED(16) uint64_t COUNTER_MASK_LINC2F[] = {\n+    0x0000000000000000UL, 0x0200000000000000UL,\n+};\n+\n+static address counter_mask_linc2f_addr() {\n+  return (address)COUNTER_MASK_LINC2F;\n+}\n+\n@@ -166,0 +190,3 @@\n+      if (VM_Version::supports_avx2()) {\n+          StubRoutines::_galoisCounterMode_AESCrypt = generate_avx2_galoisCounterMode_AESCrypt();\n+      }\n@@ -267,0 +294,86 @@\n+\/\/ AVX2 Vector AES Galois Counter Mode implementation.\n+\/\/\n+\/\/ Inputs:           Windows    |   Linux\n+\/\/   in         = rcx (c_rarg0) | rsi (c_rarg0)\n+\/\/   len        = rdx (c_rarg1) | rdi (c_rarg1)\n+\/\/   ct         = r8  (c_rarg2) | rdx (c_rarg2)\n+\/\/   out        = r9  (c_rarg3) | rcx (c_rarg3)\n+\/\/   key        = rdi           | r8  (c_rarg4)\n+\/\/   state      = r13           | r9  (c_rarg5)\n+\/\/   subkeyHtbl = r11           | r11\n+\/\/   counter    = rsi           | r12\n+\/\/\n+\/\/ Output:\n+\/\/   rax - number of processed bytes\n+address StubGenerator::generate_avx2_galoisCounterMode_AESCrypt() {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"galoisCounterMode_AESCrypt\");\n+  address start = __ pc();\n+\n+  const Register in = c_rarg0;\n+  const Register len = c_rarg1;\n+  const Register ct = c_rarg2;\n+  const Register out = c_rarg3;\n+  \/\/ and updated with the incremented counter in the end\n+ #ifndef _WIN64\n+  const Register key = c_rarg4;\n+  const Register state = c_rarg5;\n+  const Address subkeyH_mem(rbp, 2 * wordSize);\n+  const Register subkeyHtbl = r11;\n+  const Address counter_mem(rbp, 3 * wordSize);\n+  const Register counter = r12;\n+ #else\n+  const Address key_mem(rbp, 6 * wordSize);\n+  const Register key = rdi;\n+  const Address state_mem(rbp, 7 * wordSize);\n+  const Register state = r13;\n+  const Address subkeyH_mem(rbp, 8 * wordSize);\n+  const Register subkeyHtbl = r11;\n+  const Address counter_mem(rbp, 9 * wordSize);\n+  const Register counter = rsi;\n+ #endif\n+  __ enter();\n+  \/\/ Save state before entering routine\n+  __ push(r12);\n+  __ push(r13);\n+  __ push(r14);\n+  __ push(r15);\n+  __ push(rbx);\n+#ifdef _WIN64\n+  \/\/ on win64, fill len_reg from stack position\n+  __ push(rsi);\n+  __ push(rdi);\n+  __ movptr(key, key_mem);\n+  __ movptr(state, state_mem);\n+#endif\n+  __ movptr(subkeyHtbl, subkeyH_mem);\n+  __ movptr(counter, counter_mem);\n+\n+  \/\/ Save rbp and rsp\n+  __ push(rbp);\n+  __ movq(rbp, rsp);\n+  \/\/ Align stack\n+  __ andq(rsp, -64);\n+  __ subptr(rsp, 16 * longSize); \/\/ Create space on the stack for saving AES entries\n+\n+  aesgcm_avx2(in, len, ct, out, key, state, subkeyHtbl, counter);\n+  __ vzeroupper();\n+  __ movq(rsp, rbp);\n+  __ pop(rbp);\n+  \/\/ Restore state before leaving routine\n+ #ifdef _WIN64\n+  __ pop(rdi);\n+  __ pop(rsi);\n+ #endif\n+  __ pop(rbx);\n+  __ pop(r15);\n+  __ pop(r14);\n+  __ pop(r13);\n+  __ pop(r12);\n+\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n+\n+  return start;\n+}\n+\n@@ -3180,0 +3293,489 @@\n+void StubGenerator::gfmul_avx2(XMMRegister GH, XMMRegister HK) {\n+  const XMMRegister T1 = xmm1;\n+  const XMMRegister T2 = xmm2;\n+  const XMMRegister T3 = xmm3;\n+\n+  __ vpclmulqdq(T1, GH, HK, 0x11); \/\/ %%T1 = a1*b1\n+  __ vpclmulqdq(T2, GH, HK, 0x00); \/\/ %%T2 = a0*b0\n+  __ vpclmulqdq(T3, GH, HK, 0x01); \/\/ %%T3 = a1*b0\n+  __ vpclmulqdq(GH, GH, HK, 0x10); \/\/ %%GH = a0*b1\n+  __ vpxor(GH, GH, T3, Assembler::AVX_128bit);\n+\n+  __ vpsrldq(T3, GH, 8, Assembler::AVX_128bit); \/\/ shift-R %%GH 2 DWs\n+  __ vpslldq(GH, GH, 8, Assembler::AVX_128bit); \/\/ shift-L %%GH 2 DWs\n+\n+  __ vpxor(T1, T1, T3, Assembler::AVX_128bit);\n+  __ vpxor(GH, GH, T2, Assembler::AVX_128bit);\n+\n+  \/\/first phase of the reduction\n+  __ movdqu(T3, ExternalAddress(ghash_polynomial_reduction_addr()), r15 \/*rscratch*\/);\n+  __ vpclmulqdq(T2, T3, GH, 0x01);\n+  __ vpslldq(T2, T2, 8, Assembler::AVX_128bit); \/\/ shift-L %%T2 2 DWs\n+\n+  __ vpxor(GH, GH, T2, Assembler::AVX_128bit); \/\/ first phase of the reduction complete\n+  \/\/second phase of the reduction\n+  __ vpclmulqdq(T2, T3, GH, 0x00);\n+  __ vpsrldq(T2, T2, 4, Assembler::AVX_128bit); \/\/ shift-R %%T2 1 DW (Shift-R only 1-DW to obtain 2-DWs shift-R)\n+\n+  __ vpclmulqdq(GH, T3, GH, 0x10);\n+  __ vpslldq(GH, GH, 4, Assembler::AVX_128bit); \/\/ shift-L %%GH 1 DW (Shift-L 1-DW to obtain result with no shifts)\n+\n+  __ vpxor(GH, GH, T2, Assembler::AVX_128bit); \/\/ second phase of the reduction complete\n+  __ vpxor(GH, GH, T1, Assembler::AVX_128bit); \/\/ the result is in %%GH\n+}\n+\n+void StubGenerator::generateHtbl_8_block_avx2(Register htbl, Register rscratch) {\n+  const XMMRegister HK = xmm6;\n+\n+  __ movdqu(HK, Address(htbl, 0));\n+  __ movdqu(xmm1, ExternalAddress(ghash_long_swap_mask_addr()), rscratch);\n+  __ vpshufb(HK, HK, xmm1, Assembler::AVX_128bit);\n+\n+  __ movdqu(xmm11, ExternalAddress(ghash_polynomial_addr()), rscratch);\n+  __ movdqu(xmm12, ExternalAddress(ghash_polynomial_two_one_addr()), rscratch);\n+  \/\/ Compute H ^ 2 from the input subkeyH\n+  __ movdqu(xmm2, xmm6);\n+  __ vpsllq(xmm6, xmm6, 1, Assembler::AVX_128bit);\n+  __ vpsrlq(xmm2, xmm2, 63, Assembler::AVX_128bit);\n+  __ movdqu(xmm1, xmm2);\n+  __ vpslldq(xmm2, xmm2, 8, Assembler::AVX_128bit);\n+  __ vpsrldq(xmm1, xmm1, 8, Assembler::AVX_128bit);\n+  __ vpor(xmm6, xmm6, xmm2, Assembler::AVX_128bit);\n+\n+  __ vpshufd(xmm2, xmm1, 0x24, Assembler::AVX_128bit);\n+  __ vpcmpeqd(xmm2, xmm2, xmm12, Assembler::AVX_128bit);\n+  __ vpand(xmm2, xmm2, xmm11, Assembler::AVX_128bit);\n+  __ vpxor(xmm6, xmm6, xmm2, Assembler::AVX_128bit);\n+  __ movdqu(Address(htbl, 1 * 16), xmm6); \/\/ H * 2\n+  __ movdqu(xmm0, xmm6);\n+  for (int i = 2; i < 9; i++) {\n+    gfmul_avx2(xmm6, xmm0);\n+    __ movdqu(Address(htbl, i * 16), xmm6);\n+  }\n+  __ ret(0);\n+}\n+\n+#define aesenc_step_avx2(t_key)\\\n+__ aesenc(xmm1, t_key);\\\n+__ aesenc(xmm2, t_key);\\\n+__ aesenc(xmm3, t_key);\\\n+__ aesenc(xmm4, t_key);\\\n+__ aesenc(xmm5, t_key);\\\n+__ aesenc(xmm6, t_key);\\\n+__ aesenc(xmm7, t_key);\\\n+__ aesenc(xmm8, t_key);\\\n+\n+#define ghash_step_avx2(ghdata, hkey) \\\n+__ vpclmulqdq(xmm11, ghdata, hkey, 0x11);\\\n+__ vpxor(xmm12, xmm12, xmm11, Assembler::AVX_128bit);\\\n+__ vpclmulqdq(xmm11, ghdata, hkey, 0x00);\\\n+__ vpxor(xmm15, xmm15, xmm11, Assembler::AVX_128bit);\\\n+__ vpclmulqdq(xmm11, ghdata, hkey, 0x01);\\\n+__ vpxor(xmm14, xmm14, xmm11, Assembler::AVX_128bit);\\\n+__ vpclmulqdq(xmm11, ghdata, hkey, 0x10);\\\n+__ vpxor(xmm14, xmm14, xmm11, Assembler::AVX_128bit);\\\n+\n+void StubGenerator::ghash8_encrypt8_parallel_avx2(Register key, Register subkeyHtbl, XMMRegister ctr_blockx, XMMRegister aad_hashx,\n+                                                  Register in, Register out, Register ct, Register pos, bool in_order, Register rounds) {\n+\n+  const XMMRegister t1 = xmm0;\n+  const XMMRegister t2 = xmm10;\n+  const XMMRegister t3 = xmm11;\n+  const XMMRegister t4 = xmm12;\n+  const XMMRegister t5 = xmm13;\n+  const XMMRegister t6 = xmm14;\n+  const XMMRegister t7 = xmm15;\n+  Label skip_reload, last_aes_rnd, aes_192, aes_256;\n+\n+  __ movdqu(t2, xmm1);\n+  __ movdqu(Address(rsp, 16 * 0), xmm2);\n+  __ movdqu(Address(rsp, 16 * 1), xmm3);\n+  __ movdqu(Address(rsp, 16 * 2), xmm4);\n+  __ movdqu(Address(rsp, 16 * 3), xmm5);\n+  __ movdqu(Address(rsp, 16 * 4), xmm6);\n+  __ movdqu(Address(rsp, 16 * 5), xmm7);\n+  __ movdqu(Address(rsp, 16 * 6), xmm8);\n+\n+  if (in_order) {\n+    __ vpaddd(xmm1, ctr_blockx, ExternalAddress(counter_mask_linc1_addr()), Assembler::AVX_128bit, rbx \/*rscratch*\/); \/\/Increment counter by 1\n+    __ movdqu(t5, ExternalAddress(counter_mask_linc2_addr()), rbx \/*rscratch*\/);\n+    __ vpaddd(xmm2, ctr_blockx, t5, Assembler::AVX_128bit);\n+    __ vpaddd(xmm3, xmm1, t5, Assembler::AVX_128bit);\n+    __ vpaddd(xmm4, xmm2, t5, Assembler::AVX_128bit);\n+    __ vpaddd(xmm5, xmm3, t5, Assembler::AVX_128bit);\n+    __ vpaddd(xmm6, xmm4, t5, Assembler::AVX_128bit);\n+    __ vpaddd(xmm7, xmm5, t5, Assembler::AVX_128bit);\n+    __ vpaddd(xmm8, xmm6, t5, Assembler::AVX_128bit);\n+    __ movdqu(ctr_blockx, xmm8);\n+\n+    __ movdqu(t5, ExternalAddress(counter_shuffle_mask_addr()), rbx \/*rscratch*\/);\n+    for (int rnum = 1; rnum <= 8; rnum++) {\n+      __ vpshufb(as_XMMRegister(rnum), as_XMMRegister(rnum), t5, Assembler::AVX_128bit); \/\/perform a 16Byte swap\n+    }\n+  } else {\n+    __ vpaddd(xmm1, ctr_blockx, ExternalAddress(counter_mask_linc1f_addr()), Assembler::AVX_128bit, rbx \/*rscratch*\/); \/\/Increment counter by 1\n+    __ vmovdqu(t5, ExternalAddress(counter_mask_linc2f_addr()), Assembler::AVX_128bit, rbx \/*rscratch*\/);\n+    __ vpaddd(xmm2, ctr_blockx, t5, Assembler::AVX_128bit);\n+    __ vpaddd(xmm3, xmm1, t5, Assembler::AVX_128bit);\n+    __ vpaddd(xmm4, xmm2, t5, Assembler::AVX_128bit);\n+    __ vpaddd(xmm5, xmm3, t5, Assembler::AVX_128bit);\n+    __ vpaddd(xmm6, xmm4, t5, Assembler::AVX_128bit);\n+    __ vpaddd(xmm7, xmm5, t5, Assembler::AVX_128bit);\n+    __ vpaddd(xmm8, xmm6, t5, Assembler::AVX_128bit);\n+    __ movdqu(ctr_blockx, xmm8);\n+  }\n+\n+  load_key(t1, key, 16 * 0, rbx \/*rscratch*\/);\n+  for (int rnum = 1; rnum <= 8; rnum++) {\n+    __ vpxor(as_XMMRegister(rnum), as_XMMRegister(rnum), t1, Assembler::AVX_128bit);\n+  }\n+\n+  load_key(t1, key, 16 * 1, rbx \/*rscratch*\/);\n+  aesenc_step_avx2(t1);\n+\n+  load_key(t1, key, 16 * 2, rbx \/*rscratch*\/);\n+  aesenc_step_avx2(t1);\n+\n+  __ movdqu(t5, (Address(subkeyHtbl, 8 * 16)));\n+  __ vpclmulqdq(t4, t2, t5, 0x11); \/\/t4 = a1*b1\n+  __ vpclmulqdq(t7, t2, t5, 0x00); \/\/t7 = a0*b0\n+  __ vpclmulqdq(t6, t2, t5, 0x01); \/\/t6 = a1*b0\n+  __ vpclmulqdq(t5, t2, t5, 0x10); \/\/t5 = a0*b1\n+  __ vpxor(t6, t6, t5, Assembler::AVX_128bit);\n+\n+  for (int i = 3, j = 0; i <= 8; i++, j++) {\n+    load_key(t1, key, 16 * i, rbx \/*rscratch*\/);\n+    aesenc_step_avx2(t1);\n+    __ movdqu(t1, Address(rsp, 16 * j));\n+    __ movdqu(t5, (Address(subkeyHtbl, (7 - j) * 16)));\n+    ghash_step_avx2(t1, t5);\n+  }\n+\n+  load_key(t1, key, 16 * 9, rbx \/*rscratch*\/);\n+  aesenc_step_avx2(t1);\n+\n+  __ movdqu(t1, Address(rsp, 16 * 6));\n+  __ movdqu(t5, (Address(subkeyHtbl, 1 * 16)));\n+\n+  __ vpclmulqdq(t3, t1, t5, 0x00);\n+  __ vpxor(t7, t7, t3, Assembler::AVX_128bit);\n+\n+  __ vpclmulqdq(t3, t1, t5, 0x01);\n+  __ vpxor(t6, t6, t3, Assembler::AVX_128bit);\n+\n+  __ vpclmulqdq(t3, t1, t5, 0x10);\n+  __ vpxor(t6, t6, t3, Assembler::AVX_128bit);\n+\n+  __ vpclmulqdq(t3, t1, t5, 0x11);\n+  __ vpxor(t1, t4, t3, Assembler::AVX_128bit);\n+\n+  __ vpslldq(t3, t6, 8, Assembler::AVX_128bit); \/\/shift-L t3 2 DWs\n+  __ vpsrldq(t6, t6, 8, Assembler::AVX_128bit); \/\/shift-R t2 2 DWs\n+  __ vpxor(t7, t7, t3, Assembler::AVX_128bit);\n+  __ vpxor(t1, t1, t6, Assembler::AVX_128bit); \/\/ accumulate the results in t1:t7\n+\n+  load_key(t5, key, 16 * 10, rbx \/*rscratch*\/);\n+  __ cmpl(rounds, 52);\n+  __ jcc(Assembler::less, last_aes_rnd);\n+\n+  __ bind(aes_192);\n+  aesenc_step_avx2(t5);\n+  load_key(t5, key, 16 * 11, rbx \/*rscratch*\/);\n+  aesenc_step_avx2(t5);\n+  load_key(t5, key, 16 * 12, rbx \/*rscratch*\/);\n+  __ cmpl(rounds, 60);\n+  __ jcc(Assembler::less, last_aes_rnd);\n+\n+  __ bind(aes_256);\n+  aesenc_step_avx2(t5);\n+  load_key(t5, key, 16 * 13, rbx \/*rscratch*\/);\n+  aesenc_step_avx2(t5);\n+  load_key(t5, key, 16 * 14, rbx \/*rscratch*\/);\n+  __ bind(last_aes_rnd);\n+  for (int rnum = 1; rnum <= 8; rnum++) {\n+    __ aesenclast(as_XMMRegister(rnum), t5);\n+  }\n+\n+  for (int i = 0; i <= 7; i++) {\n+    __ movdqu(t2, Address(in, pos, Address::times_1, 16 * i));\n+    __ vpxor(as_XMMRegister(i + 1), as_XMMRegister(i + 1), t2, Assembler::AVX_128bit);\n+  }\n+\n+  \/\/first phase of the reduction\n+  __ vmovdqu(t3, ExternalAddress(ghash_polynomial_reduction_addr()), Assembler::AVX_128bit, rbx \/*rscratch*\/);\n+\n+  __ vpclmulqdq(t2, t3, t7, 0x01);\n+  __ vpslldq(t2, t2, 8, Assembler::AVX_128bit); \/\/shift-L xmm2 2 DWs\n+\n+  __ vpxor(t7, t7, t2, Assembler::AVX_128bit); \/\/first phase of the reduction complete\n+\n+  \/\/Write to the Ciphertext buffer\n+  for (int i = 0; i <= 7; i++) {\n+    __ movdqu(Address(out, pos, Address::times_1, 16 * i), as_XMMRegister(i + 1));\n+  }\n+\n+  __ cmpptr(ct, out);\n+  __ jcc(Assembler::equal, skip_reload);\n+  for (int i = 0; i <= 7; i++) {\n+    __ movdqu(as_XMMRegister(i + 1), Address(in, pos, Address::times_1, 16 * i));\n+  }\n+\n+  __ bind(skip_reload);\n+  \/\/second phase of the reduction\n+  __ vpclmulqdq(t2, t3, t7, 0x00);\n+  __ vpsrldq(t2, t2, 4, Assembler::AVX_128bit); \/\/shift-R t2 1 DW (Shift-R only 1-DW to obtain 2-DWs shift-R)\n+\n+  __ vpclmulqdq(t4, t3, t7, 0x10);\n+  __ vpslldq(t4, t4, 4, Assembler::AVX_128bit); \/\/shift-L t4 1 DW (Shift-L 1-DW to obtain result with no shifts)\n+  __ vpxor(t4, t4, t2, Assembler::AVX_128bit); \/\/second phase of the reduction complete\n+  __ vpxor(t1, t1, t4, Assembler::AVX_128bit); \/\/the result is in t1\n+\n+  \/\/perform a 16Byte swap\n+  for (int rnum = 1; rnum <= 8; rnum++) {\n+    __ vpshufb(as_XMMRegister(rnum), as_XMMRegister(rnum), ExternalAddress(counter_shuffle_mask_addr()), Assembler::AVX_128bit, rbx \/*rscratch*\/);\n+  }\n+  __ vpxor(xmm1, xmm1, t1, Assembler::AVX_128bit);\n+}\n+\n+\/\/GHASH the last 8 ciphertext blocks.\n+void StubGenerator::ghash_last_8_avx2(Register subkeyHtbl) {\n+  const XMMRegister t1 = xmm0;\n+  const XMMRegister t2 = xmm10;\n+  const XMMRegister t3 = xmm11;\n+  const XMMRegister t4 = xmm12;\n+  const XMMRegister t5 = xmm13;\n+  const XMMRegister t6 = xmm14;\n+  const XMMRegister t7 = xmm15;\n+\n+  \/\/Karatsuba Method\n+  __ movdqu(t5, Address(subkeyHtbl, 8 * 16));\n+\n+  __ vpshufd(t2, xmm1, 78, Assembler::AVX_128bit);\n+  __ vpshufd(t3, t5, 78, Assembler::AVX_128bit);\n+  __ vpxor(t2, t2, xmm1, Assembler::AVX_128bit);\n+  __ vpxor(t3, t3, t5, Assembler::AVX_128bit);\n+\n+  __ vpclmulqdq(t6, xmm1, t5, 0x11);\n+  __ vpclmulqdq(t7, xmm1, t5, 0x00);\n+\n+  __ vpclmulqdq(xmm1, t2, t3, 0x00);\n+\n+  for (int i = 7, rnum = 2; rnum <= 8; i--, rnum++) {\n+    __ movdqu(t5, Address(subkeyHtbl, i * 16));\n+    __ vpshufd(t2, as_XMMRegister(rnum), 78, Assembler::AVX_128bit);\n+    __ vpshufd(t3, t5, 78, Assembler::AVX_128bit);\n+    __ vpxor(t2, t2, as_XMMRegister(rnum), Assembler::AVX_128bit);\n+    __ vpxor(t3, t3, t5, Assembler::AVX_128bit);\n+    __ vpclmulqdq(t4, as_XMMRegister(rnum), t5, 0x11);\n+    __ vpxor(t6, t6, t4, Assembler::AVX_128bit);\n+    __ vpclmulqdq(t4, as_XMMRegister(rnum), t5, 0x00);\n+    __ vpxor(t7, t7, t4, Assembler::AVX_128bit);\n+    __ vpclmulqdq(t2, t2, t3, 0x00);\n+    __ vpxor(xmm1, xmm1, t2, Assembler::AVX_128bit);\n+  }\n+\n+  __ vpxor(xmm1, xmm1, t6, Assembler::AVX_128bit);\n+  __ vpxor(t2, xmm1, t7, Assembler::AVX_128bit);\n+\n+  __ vpslldq(t4, t2, 8, Assembler::AVX_128bit);\n+  __ vpsrldq(t2, t2, 8, Assembler::AVX_128bit);\n+\n+  __ vpxor(t7, t7, t4, Assembler::AVX_128bit);\n+  __ vpxor(t6, t6, t2, Assembler::AVX_128bit); \/\/<t6:t7> holds the result of the accumulated carry-less multiplications\n+\n+  \/\/first phase of the reduction\n+  __ movdqu(t3, ExternalAddress(ghash_polynomial_reduction_addr()), rbx \/*rscratch*\/);\n+\n+  __ vpclmulqdq(t2, t3, t7, 0x01);\n+  __ vpslldq(t2, t2, 8, Assembler::AVX_128bit); \/\/ shift-L t2 2 DWs\n+\n+  __ vpxor(t7, t7, t2, Assembler::AVX_128bit);\/\/first phase of the reduction complete\n+\n+  \/\/second phase of the reduction\n+  __ vpclmulqdq(t2, t3, t7, 0x00);\n+  __ vpsrldq(t2, t2, 4, Assembler::AVX_128bit); \/\/shift-R t2 1 DW (Shift-R only 1-DW to obtain 2-DWs shift-R)\n+\n+  __ vpclmulqdq(t4, t3, t7, 0x10);\n+  __ vpslldq(t4, t4, 4, Assembler::AVX_128bit); \/\/shift-L t4 1 DW (Shift-L 1-DW to obtain result with no shifts)\n+  __ vpxor(t4, t4, t2, Assembler::AVX_128bit); \/\/second phase of the reduction complete\n+  __ vpxor(t6, t6, t4, Assembler::AVX_128bit); \/\/the result is in t6\n+}\n+\n+void StubGenerator::initial_blocks_avx2(XMMRegister ctr, Register rounds, Register key, Register len, Register in,\n+                                        Register out, Register ct, Register subkeyHtbl, Register pos) {\n+  const XMMRegister t1 = xmm12;\n+  const XMMRegister t2 = xmm13;\n+  const XMMRegister t3 = xmm14;\n+  const XMMRegister t4 = xmm15;\n+  const XMMRegister t5 = xmm11;\n+  const XMMRegister t6 = xmm10;\n+  const XMMRegister t_key = xmm0;\n+\n+  Label skip_reload, last_aes_rnd, aes_192, aes_256;\n+  \/\/Move AAD_HASH to temp reg\n+  __ movdqu(t2, xmm8);\n+  \/\/Prepare 8 counter blocks and perform rounds of AES cipher on\n+  \/\/them, load plain\/cipher text and store cipher\/plain text.\n+  __ movdqu(xmm1, ctr);\n+  __ movdqu(t5, ExternalAddress(counter_mask_linc1_addr()), rbx \/*rscratch*\/);\n+  __ vpaddd(xmm2, xmm1, t5, Assembler::AVX_128bit);\n+  __ vpaddd(xmm3, xmm2, t5, Assembler::AVX_128bit);\n+  __ vpaddd(xmm4, xmm3, t5, Assembler::AVX_128bit);\n+  __ vpaddd(xmm5, xmm4, t5, Assembler::AVX_128bit);\n+  __ vpaddd(xmm6, xmm5, t5, Assembler::AVX_128bit);\n+  __ vpaddd(xmm7, xmm6, t5, Assembler::AVX_128bit);\n+  __ vpaddd(xmm8, xmm7, t5, Assembler::AVX_128bit);\n+  __ movdqu(ctr, xmm8);\n+\n+  __ movdqu(t5, ExternalAddress(counter_shuffle_mask_addr()), rbx \/*rscratch*\/);\n+  for (int rnum = 1; rnum <= 8; rnum++) {\n+    __ vpshufb(as_XMMRegister(rnum), as_XMMRegister(rnum), t5, Assembler::AVX_128bit); \/\/perform a 16Byte swap\n+  }\n+\n+  load_key(t_key, key, 16 * 0, rbx \/*rscratch*\/);\n+  for (int rnum = 1; rnum <= 8; rnum++) {\n+    __ vpxor(as_XMMRegister(rnum), as_XMMRegister(rnum), t_key, Assembler::AVX_128bit);\n+  }\n+\n+  for (int i = 1; i <= 9; i++) {\n+    load_key(t_key, key, 16 * i, rbx \/*rscratch*\/);\n+    aesenc_step_avx2(t_key);\n+  }\n+\n+  load_key(t_key, key, 16 * 10, rbx \/*rscratch*\/);\n+  __ cmpl(rounds, 52);\n+  __ jcc(Assembler::less, last_aes_rnd);\n+\n+  __ bind(aes_192);\n+  aesenc_step_avx2(t_key);\n+  load_key(t_key, key, 16 * 11, rbx \/*rscratch*\/);\n+  aesenc_step_avx2(t_key);\n+  load_key(t_key, key, 16 * 12, rbx \/*rscratch*\/);\n+  __ cmpl(rounds, 60);\n+  __ jcc(Assembler::less, last_aes_rnd);\n+\n+  __ bind(aes_256);\n+  aesenc_step_avx2(t_key);\n+  load_key(t_key, key, 16 * 13, rbx \/*rscratch*\/);\n+  aesenc_step_avx2(t_key);\n+  load_key(t_key, key, 16 * 14, rbx \/*rscratch*\/);\n+\n+  __ bind(last_aes_rnd);\n+  for (int rnum = 1; rnum <= 8; rnum++) {\n+    __ aesenclast(as_XMMRegister(rnum), t_key);\n+  }\n+\n+  \/\/The hash should end up in T3\n+  __ movdqu(t3, t2);\n+  \/\/ XOR and store data\n+  for (int i = 0; i <= 7; i++) {\n+    __ movdqu(t1, Address(in, pos, Address::times_1, 16 * i));\n+    __ vpxor(as_XMMRegister(i + 1), as_XMMRegister(i + 1), t1, Assembler::AVX_128bit);\n+    __ movdqu(Address(out, pos, Address::times_1, 16 * i), as_XMMRegister(i + 1));\n+  }\n+\n+  __ cmpptr(ct, out);\n+  __ jcc(Assembler::equal, skip_reload);\n+  for (int i = 0; i <= 7; i++) {\n+    __ movdqu(as_XMMRegister(i + 1), Address(in, pos, Address::times_1, 16 * i));\n+  }\n+\n+  __ bind(skip_reload);\n+  \/\/Update len (r14) with the number of blocks processed\n+  __ subl(len, 128);\n+  __ addl(pos, 128);\n+\n+  for (int rnum = 1; rnum <= 8; rnum++) {\n+    __ vpshufb(as_XMMRegister(rnum), as_XMMRegister(rnum), ExternalAddress(counter_shuffle_mask_addr()), Assembler::AVX_128bit, rbx \/*rscratch*\/);\n+  }\n+  \/\/ Combine GHASHed value with the corresponding ciphertext\n+  __ vpxor(xmm1, xmm1, t3, Assembler::AVX_128bit);\n+}\n+\n+void StubGenerator::aesgcm_avx2(Register in, Register len, Register ct, Register out, Register key,\n+                                Register state, Register subkeyHtbl, Register counter) {\n+  const Register pos = rax;\n+  const Register rounds = r10;\n+  const XMMRegister ctr_blockx = xmm9;\n+  const XMMRegister aad_hashx = xmm8;\n+  Label encrypt_done, encrypt_by_8_parallel, encrypt_by_8_new, encrypt_by_8, hash_last_8, enc_dec_done, generate_htbl_8_blks;\n+\n+  \/\/This routine should be called only for message sizes of 128 bytes or more.\n+  \/\/Macro flow:\n+  \/\/process 8 16 byte blocks in initial_num_blocks.\n+  \/\/process 8 16 byte blocks at a time until all are done 'encrypt_by_8_new  followed by ghash_last_8'\n+  __ xorl(pos, pos);\n+\n+  \/\/ Generate 8 constants for htbl\n+  __ call(generate_htbl_8_blks, relocInfo::none);\n+\n+  \/\/ Compute #rounds for AES based on the length of the key array\n+  __ movl(rounds, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));\n+\n+  \/\/ Load and shuffle state and counter values\n+  __ movdqu(ctr_blockx, Address(counter, 0));\n+  __ movdqu(aad_hashx, Address(state, 0));\n+  __ vpshufb(ctr_blockx, ctr_blockx, ExternalAddress(counter_shuffle_mask_addr()), Assembler::AVX_128bit, rbx \/*rscratch*\/);\n+  __ vpshufb(aad_hashx, aad_hashx, ExternalAddress(ghash_long_swap_mask_addr()), Assembler::AVX_128bit, rbx \/*rscratch*\/);\n+\n+  initial_blocks_avx2(ctr_blockx, rounds, key, len, in, out, ct, subkeyHtbl, pos);\n+\n+  \/\/We need at least 128 bytes to proceed further.\n+  __ cmpl(len, 128);\n+  __ jcc(Assembler::less, encrypt_done);\n+\n+  __ bind(encrypt_by_8_parallel);\n+  \/\/in_order vs. out_order is an optimization to increment the counter without shuffling\n+  \/\/it back into little endian. r15d keeps track of when we need to increment in order so\n+  \/\/that the carry is handled correctly.\n+  __ movdl(r15, ctr_blockx);\n+  __ andl(r15, 255);\n+  __ vpshufb(ctr_blockx, ctr_blockx, ExternalAddress(counter_shuffle_mask_addr()), Assembler::AVX_128bit, rbx \/*rscratch*\/);\n+\n+  __ bind(encrypt_by_8_new);\n+  __ cmpl(r15, 255 - 8);\n+  __ jcc(Assembler::greater, encrypt_by_8);\n+\n+  __ addb(r15, 8);\n+  ghash8_encrypt8_parallel_avx2(key, subkeyHtbl, ctr_blockx, aad_hashx, in, out, ct, pos, false, rounds);\n+  __ addl(pos, 128);\n+  __ subl(len, 128);\n+  __ cmpl(len, 128);\n+  __ jcc(Assembler::greaterEqual, encrypt_by_8_new);\n+\n+  __ vpshufb(ctr_blockx, ctr_blockx, ExternalAddress(counter_shuffle_mask_addr()), Assembler::AVX_128bit, rbx \/*rscratch*\/);\n+  __ jmp(encrypt_done);\n+\n+  __ bind(encrypt_by_8);\n+  __ vpshufb(ctr_blockx, ctr_blockx, ExternalAddress(counter_shuffle_mask_addr()), Assembler::AVX_128bit, rbx \/*rscratch*\/);\n+\n+  __ addb(r15, 8);\n+  ghash8_encrypt8_parallel_avx2(key, subkeyHtbl, ctr_blockx, aad_hashx, in, out, ct, pos, true, rounds);\n+\n+  __ vpshufb(ctr_blockx, ctr_blockx, ExternalAddress(counter_shuffle_mask_addr()), Assembler::AVX_128bit, rbx \/*rscratch*\/);\n+  __ addl(pos, 128);\n+  __ subl(len, 128);\n+  __ cmpl(len, 128);\n+  __ jcc(Assembler::greaterEqual, encrypt_by_8_new);\n+  __ vpshufb(ctr_blockx, ctr_blockx, ExternalAddress(counter_shuffle_mask_addr()), Assembler::AVX_128bit, rbx \/*rscratch*\/);\n+\n+  __ bind(encrypt_done);\n+  ghash_last_8_avx2(subkeyHtbl);\n+\n+  __ vpaddd(ctr_blockx, ctr_blockx, ExternalAddress(counter_mask_linc1_addr()), Assembler::AVX_128bit, rbx \/*rscratch*\/);\n+  __ vpshufb(ctr_blockx, ctr_blockx, ExternalAddress(counter_shuffle_mask_addr()), Assembler::AVX_128bit, rbx \/*rscratch*\/);\n+  __ movdqu(Address(counter, 0), ctr_blockx); \/\/current_counter = xmm9\n+  __ vpshufb(xmm14, xmm14, ExternalAddress(ghash_long_swap_mask_addr()), Assembler::AVX_128bit, rbx \/*rscratch*\/);\n+  __ movdqu(Address(state, 0), xmm14); \/\/aad hash = xmm14\n+  \/\/ xor out round keys\n+  __ vpxor(xmm0, xmm0, xmm0, Assembler::AVX_128bit);\n+  __ vpxor(xmm13, xmm13, xmm13, Assembler::AVX_128bit);\n+  __ jmp(enc_dec_done);\n+\n+  __ bind(generate_htbl_8_blks);\n+  generateHtbl_8_block_avx2(subkeyHtbl, rbx \/*rscratch*\/);\n+\n+  __ bind(enc_dec_done);\n+  __ movq(rax, pos);\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64_aes.cpp","additions":603,"deletions":1,"binary":false,"changes":604,"status":"modified"},{"patch":"@@ -40,1 +40,1 @@\n-  _compiler_stubs_code_size     = 20000 LP64_ONLY(+30000) WINDOWS_ONLY(+2000),\n+  _compiler_stubs_code_size     = 20000 LP64_ONLY(+32000) WINDOWS_ONLY(+2000),\n","filename":"src\/hotspot\/cpu\/x86\/stubRoutines_x86.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"}]}