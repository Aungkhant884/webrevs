{"files":[{"patch":"@@ -167,6 +167,0 @@\n-  \/\/ Second-level allocation: Should be called while holding a\n-  \/\/ lock. It will try to first allocate lock-free out of the active\n-  \/\/ region or, if it's unable to, it will try to replace the active\n-  \/\/ alloc region with a new one. We require that the caller takes the\n-  \/\/ appropriate lock before calling this so that it is easier to make\n-  \/\/ it conform to its locking protocol.\n@@ -174,4 +168,6 @@\n-  \/\/ Same as attempt_allocation_locked(size_t, bool), but allowing specification\n-  \/\/ of minimum word size of the block in min_word_size, and the maximum word\n-  \/\/ size of the allocation in desired_word_size. The actual size of the block is\n-  \/\/ returned in actual_word_size.\n+  \/\/ Second-level allocation: Should be called while holding a\n+  \/\/ lock. We require that the caller takes the appropriate lock\n+  \/\/ before calling this so that it is easier to make it conform\n+  \/\/ to the locking protocol. The min and desired word size allow\n+  \/\/ specifying a minimum and maximum size of the allocation. The\n+  \/\/ actual size of allocation is returned in actual_word_size.\n@@ -182,0 +178,5 @@\n+  \/\/ Perform an allocation out of a new allocation region, retiring the current one.\n+  inline HeapWord* attempt_allocation_using_new_region(size_t min_word_size,\n+                                                       size_t desired_word_size,\n+                                                       size_t* actual_word_size);\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1AllocRegion.hpp","additions":11,"deletions":10,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -101,3 +101,0 @@\n-  \/\/ First we have to redo the allocation, assuming we're holding the\n-  \/\/ appropriate lock, in case another thread changed the region while\n-  \/\/ we were waiting to get the lock.\n@@ -109,0 +106,6 @@\n+  return attempt_allocation_using_new_region(min_word_size, desired_word_size, actual_word_size);\n+}\n+\n+inline HeapWord* G1AllocRegion::attempt_allocation_using_new_region(size_t min_word_size,\n+                                                                    size_t desired_word_size,\n+                                                                    size_t* actual_word_size) {\n@@ -110,1 +113,1 @@\n-  result = new_alloc_region_and_allocate(desired_word_size, false \/* force *\/);\n+  HeapWord* result = new_alloc_region_and_allocate(desired_word_size, false \/* force *\/);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1AllocRegion.inline.hpp","additions":7,"deletions":4,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -115,0 +115,1 @@\n+  \/\/ Attempt allocation in the current alloc region.\n@@ -118,0 +119,7 @@\n+\n+  \/\/ Attempt allocation, retiring the current region and allocating a new one. It is\n+  \/\/ assumed that attempt_allocation() has been tried and failed already first.\n+  inline HeapWord* attempt_allocation_using_new_region(size_t word_size);\n+\n+  \/\/ This is to be called when holding an appropriate lock. It first tries in the\n+  \/\/ current allocation region, and then attempts an allocation using a new region.\n@@ -119,0 +127,1 @@\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1Allocator.hpp","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -55,0 +55,1 @@\n+\n@@ -59,0 +60,1 @@\n+\n@@ -62,0 +64,10 @@\n+inline HeapWord* G1Allocator::attempt_allocation_using_new_region(size_t word_size) {\n+  uint node_index = current_node_index();\n+  size_t temp;\n+  HeapWord* result = mutator_alloc_region(node_index)->attempt_allocation_using_new_region(word_size, word_size, &temp);\n+  assert(result != NULL || mutator_alloc_region(node_index)->get() == NULL,\n+         \"Must not have a mutator alloc region if there is no memory, but is \" PTR_FORMAT,\n+         p2i(mutator_alloc_region(node_index)->get()));\n+  return result;\n+}\n+\n@@ -65,0 +77,1 @@\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1Allocator.inline.hpp","additions":13,"deletions":0,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -426,0 +426,1 @@\n+    bool preventive_collection_required = false;\n@@ -430,1 +431,5 @@\n-      result = _allocator->attempt_allocation_locked(word_size);\n+\n+      \/\/ Now that we have the lock, we first retry the allocation in case another\n+      \/\/ thread changed the region while we were waiting to acquire the lock.\n+      size_t actual_size;\n+      result = _allocator->attempt_allocation(word_size, word_size, &actual_size);\n@@ -435,7 +440,5 @@\n-      \/\/ If the GCLocker is active and we are bound for a GC, try expanding young gen.\n-      \/\/ This is different to when only GCLocker::needs_gc() is set: try to avoid\n-      \/\/ waiting because the GCLocker is active to not wait too long.\n-      if (GCLocker::is_active_and_needs_gc() && policy()->can_expand_young_list()) {\n-        \/\/ No need for an ergo message here, can_expand_young_list() does this when\n-        \/\/ it returns true.\n-        result = _allocator->attempt_allocation_force(word_size);\n+      preventive_collection_required = policy()->preventive_collection_required(1);\n+      if (!preventive_collection_required) {\n+        \/\/ We've already attempted a lock-free allocation above, so we don't want to\n+        \/\/ do it again. Let's jump straight to replacing the active region.\n+        result = _allocator->attempt_allocation_using_new_region(word_size);\n@@ -445,0 +448,12 @@\n+\n+        \/\/ If the GCLocker is active and we are bound for a GC, try expanding young gen.\n+        \/\/ This is different to when only GCLocker::needs_gc() is set: try to avoid\n+        \/\/ waiting because the GCLocker is active to not wait too long.\n+        if (GCLocker::is_active_and_needs_gc() && policy()->can_expand_young_list()) {\n+          \/\/ No need for an ergo message here, can_expand_young_list() does this when\n+          \/\/ it returns true.\n+          result = _allocator->attempt_allocation_force(word_size);\n+          if (result != NULL) {\n+            return result;\n+          }\n+        }\n@@ -446,0 +461,1 @@\n+\n@@ -455,0 +471,2 @@\n+      GCCause::Cause gc_cause = preventive_collection_required ? GCCause::_g1_preventive_collection\n+                                                              : GCCause::_g1_inc_collection_pause;\n@@ -456,2 +474,1 @@\n-      result = do_collection_pause(word_size, gc_count_before, &succeeded,\n-                                   GCCause::_g1_inc_collection_pause);\n+      result = do_collection_pause(word_size, gc_count_before, &succeeded, gc_cause);\n@@ -851,0 +868,1 @@\n+    bool preventive_collection_required = false;\n@@ -857,9 +875,12 @@\n-      \/\/ Given that humongous objects are not allocated in young\n-      \/\/ regions, we'll first try to do the allocation without doing a\n-      \/\/ collection hoping that there's enough space in the heap.\n-      result = humongous_obj_allocate(word_size);\n-      if (result != NULL) {\n-        size_t size_in_regions = humongous_obj_size_in_regions(word_size);\n-        policy()->old_gen_alloc_tracker()->\n-          add_allocated_humongous_bytes_since_last_gc(size_in_regions * HeapRegion::GrainBytes);\n-        return result;\n+      size_t size_in_regions = humongous_obj_size_in_regions(word_size);\n+      preventive_collection_required = policy()->preventive_collection_required((uint)size_in_regions);\n+      if (!preventive_collection_required) {\n+        \/\/ Given that humongous objects are not allocated in young\n+        \/\/ regions, we'll first try to do the allocation without doing a\n+        \/\/ collection hoping that there's enough space in the heap.\n+        result = humongous_obj_allocate(word_size);\n+        if (result != NULL) {\n+          policy()->old_gen_alloc_tracker()->\n+            add_allocated_humongous_bytes_since_last_gc(size_in_regions * HeapRegion::GrainBytes);\n+          return result;\n+        }\n@@ -877,0 +898,2 @@\n+      GCCause::Cause gc_cause = preventive_collection_required ? GCCause::_g1_preventive_collection\n+                                                              : GCCause::_g1_humongous_allocation;\n@@ -878,2 +901,1 @@\n-      result = do_collection_pause(word_size, gc_count_before, &succeeded,\n-                                   GCCause::_g1_humongous_allocation);\n+      result = do_collection_pause(word_size, gc_count_before, &succeeded, gc_cause);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectedHeap.cpp","additions":43,"deletions":21,"binary":false,"changes":64,"status":"modified"},{"patch":"@@ -114,0 +114,4 @@\n+bool G1CollectionSet::has_candidates() {\n+  return _candidates != NULL && !_candidates->is_empty();\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectionSet.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -275,0 +275,1 @@\n+  bool has_candidates();\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectionSet.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -71,0 +71,2 @@\n+  _predicted_surviving_bytes_from_survivor(0),\n+  _predicted_surviving_bytes_from_old(0),\n@@ -453,0 +455,1 @@\n+  update_survival_estimates_for_next_collection();\n@@ -782,0 +785,1 @@\n+  update_survival_estimates_for_next_collection();\n@@ -1403,0 +1407,80 @@\n+\/\/ Number of regions required to store the given number of bytes, taking\n+\/\/ into account the target amount of wasted space in PLABs.\n+static size_t get_num_regions_adjust_for_plab_waste(size_t byte_count) {\n+  size_t byte_count_adjusted = byte_count * (size_t)(100 + TargetPLABWastePct) \/ 100.0;\n+\n+  \/\/ Round up the region count\n+  return (byte_count_adjusted + HeapRegion::GrainBytes - 1) \/ HeapRegion::GrainBytes;\n+}\n+\n+bool G1Policy::preventive_collection_required(uint alloc_region_count) {\n+  if (!G1AllowPreventiveGC || !Universe::is_fully_initialized()) {\n+    \/\/ Don't attempt any preventive GC's if the feature is disabled,\n+    \/\/ or before initialization is complete.\n+    return false;\n+  }\n+\n+  if (_g1h->young_regions_count() == 0 && !_collection_set->has_candidates()) {\n+    return false;\n+  }\n+\n+  uint eden_count = _g1h->eden_regions_count();\n+  size_t const eden_surv_bytes_pred = _eden_surv_rate_group->accum_surv_rate_pred(eden_count) * HeapRegion::GrainBytes;\n+  size_t const total_young_predicted_surviving_bytes = eden_surv_bytes_pred + _predicted_surviving_bytes_from_survivor;\n+\n+  uint required_regions = (uint)(get_num_regions_adjust_for_plab_waste(total_young_predicted_surviving_bytes) +\n+                                get_num_regions_adjust_for_plab_waste(_predicted_surviving_bytes_from_old));\n+\n+  if (required_regions > _g1h->num_free_regions() - alloc_region_count) {\n+    log_debug(gc, ergo, cset)(\"Preventive GC, insufficient free regions. Predicted need %u. Curr Eden %u (Pred %u). Curr Survivor %u (Pred %u). Curr Old %u (Pred %u) Free %u Alloc %u\",\n+            required_regions,\n+            eden_count,\n+            (uint)get_num_regions_adjust_for_plab_waste(eden_surv_bytes_pred),\n+            _g1h->survivor_regions_count(),\n+            (uint)get_num_regions_adjust_for_plab_waste(_predicted_surviving_bytes_from_survivor),\n+            _g1h->old_regions_count(),\n+            (uint)get_num_regions_adjust_for_plab_waste(_predicted_surviving_bytes_from_old),\n+            _g1h->num_free_regions(),\n+            alloc_region_count);\n+\n+    return true;\n+  }\n+\n+  return false;\n+}\n+\n+void G1Policy::update_survival_estimates_for_next_collection() {\n+  \/\/ Predict the number of bytes of surviving objects from survivor and old\n+  \/\/ regions and update the associated members.\n+\n+  \/\/ Survivor regions\n+  size_t survivor_bytes = 0;\n+  const GrowableArray<HeapRegion*>* survivor_regions = _g1h->survivor()->regions();\n+  for (GrowableArrayIterator<HeapRegion*> it = survivor_regions->begin();\n+       it != survivor_regions->end();\n+       ++it) {\n+    survivor_bytes += predict_bytes_to_copy(*it);\n+  }\n+\n+  _predicted_surviving_bytes_from_survivor = survivor_bytes;\n+\n+  \/\/ Old regions\n+  if (!_collection_set->has_candidates()) {\n+    _predicted_surviving_bytes_from_old = 0;\n+    return;\n+  }\n+\n+  \/\/ Use the minimum old gen collection set as conservative estimate for the number\n+  \/\/ of regions to take for this calculation.\n+  G1CollectionSetCandidates *candidates = _collection_set->candidates();\n+  uint iterate_count = MIN2(candidates->num_remaining(), calc_min_old_cset_length(candidates));\n+  uint current_index = candidates->cur_idx();\n+  size_t old_bytes = 0;\n+  for (uint i = 0; i < iterate_count; i++) {\n+    HeapRegion *region = candidates->at(current_index + i);\n+    old_bytes += predict_bytes_to_copy(region);\n+  }\n+\n+  _predicted_surviving_bytes_from_old = old_bytes;\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1Policy.cpp","additions":84,"deletions":0,"binary":false,"changes":84,"status":"modified"},{"patch":"@@ -101,0 +101,5 @@\n+  \/\/ These values are predictions of how much we think will survive in each\n+  \/\/ section of the heap.\n+  size_t _predicted_surviving_bytes_from_survivor;\n+  size_t _predicted_surviving_bytes_from_old;\n+\n@@ -348,0 +353,5 @@\n+  \/\/ Returns whether a collection should be done proactively, taking into\n+  \/\/ account the current number of free regions and the expected survival\n+  \/\/ rates in each section of the heap.\n+  bool preventive_collection_required(uint region_count);\n+\n@@ -349,0 +359,5 @@\n+\n+  \/\/ Predict the number of bytes of surviving objects from survivor and old\n+  \/\/ regions and update the associated members.\n+  void update_survival_estimates_for_next_collection();\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1Policy.hpp","additions":15,"deletions":0,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -122,0 +122,5 @@\n+bool VM_G1CollectForAllocation::should_try_allocation_before_gc() {\n+  \/\/ Don't allocate before a preventive GC.\n+  return _gc_cause != GCCause::_g1_preventive_collection;\n+}\n+\n@@ -125,1 +130,1 @@\n-  if (_word_size > 0) {\n+  if (should_try_allocation_before_gc() && _word_size > 0) {\n","filename":"src\/hotspot\/share\/gc\/g1\/g1VMOperations.cpp","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -81,0 +81,3 @@\n+\n+private:\n+  bool should_try_allocation_before_gc();\n","filename":"src\/hotspot\/share\/gc\/g1\/g1VMOperations.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -305,1 +305,6 @@\n-          range(0.0, (double)max_uintx)\n+          range(0.0, (double)max_uintx)                                     \\\n+                                                                            \\\n+  product(bool, G1AllowPreventiveGC, true, DIAGNOSTIC,                       \\\n+          \"Allows collections to be triggered proactively based on the      \\\n+           number of free regions and the expected survival rates in each   \\\n+           section of the heap.\")\n","filename":"src\/hotspot\/share\/gc\/g1\/g1_globals.hpp","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -102,0 +102,3 @@\n+    case _g1_preventive_collection:\n+      return \"G1 Preventive Collection\";\n+\n","filename":"src\/hotspot\/share\/gc\/shared\/gcCause.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -76,0 +76,1 @@\n+    _g1_preventive_collection,\n","filename":"src\/hotspot\/share\/gc\/shared\/gcCause.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -253,0 +253,2 @@\n+                                                                  \"-XX:+UnlockDiagnosticVMOptions\",\n+                                                                  \"-XX:-G1AllowPreventiveGC\",\n@@ -264,0 +266,2 @@\n+                                                   \"-XX:+UnlockDiagnosticVMOptions\",\n+                                                   \"-XX:-G1AllowPreventiveGC\",\n","filename":"test\/hotspot\/jtreg\/gc\/g1\/TestGCLogMessages.java","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"}]}