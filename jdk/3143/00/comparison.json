{"files":[{"patch":"@@ -168,6 +168,9 @@\n-  \/\/ lock. It will try to first allocate lock-free out of the active\n-  \/\/ region or, if it's unable to, it will try to replace the active\n-  \/\/ alloc region with a new one. We require that the caller takes the\n-  \/\/ appropriate lock before calling this so that it is easier to make\n-  \/\/ it conform to its locking protocol.\n-  inline HeapWord* attempt_allocation_locked(size_t word_size);\n+  \/\/ lock. We require that the caller takes the appropriate lock\n+  \/\/ before calling this so that it is easier to make it conform\n+  \/\/ to the locking protocol.\n+  \/\/ If attempt_lock_free_first is set, it will try to first\n+  \/\/ allocate lock-free out of the active region. If it's unable\n+  \/\/ to (or if attempt_lock_free_first is false), it will try to\n+  \/\/ replace the active alloc region with a new one.\n+  inline HeapWord* attempt_allocation_locked(size_t word_size,\n+                                             bool attempt_lock_free_first);\n@@ -180,1 +183,2 @@\n-                                             size_t* actual_word_size);\n+                                             size_t* actual_word_size,\n+                                             bool attempt_lock_free_first);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1AllocRegion.hpp","additions":11,"deletions":7,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -93,1 +93,1 @@\n-inline HeapWord* G1AllocRegion::attempt_allocation_locked(size_t word_size) {\n+inline HeapWord* G1AllocRegion::attempt_allocation_locked(size_t word_size, bool attempt_lock_free_first) {\n@@ -95,1 +95,1 @@\n-  return attempt_allocation_locked(word_size, word_size, &temp);\n+  return attempt_allocation_locked(word_size, word_size, &temp, attempt_lock_free_first);\n@@ -100,7 +100,9 @@\n-                                                          size_t* actual_word_size) {\n-  \/\/ First we have to redo the allocation, assuming we're holding the\n-  \/\/ appropriate lock, in case another thread changed the region while\n-  \/\/ we were waiting to get the lock.\n-  HeapWord* result = attempt_allocation(min_word_size, desired_word_size, actual_word_size);\n-  if (result != NULL) {\n-    return result;\n+                                                          size_t* actual_word_size,\n+                                                          bool attempt_lock_free_first) {\n+  HeapWord* result;\n+\n+  if (attempt_lock_free_first) {\n+    result = attempt_allocation(min_word_size, desired_word_size, actual_word_size);\n+    if (result != NULL) {\n+      return result;\n+    }\n","filename":"src\/hotspot\/share\/gc\/g1\/g1AllocRegion.inline.hpp","additions":11,"deletions":9,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -252,1 +252,2 @@\n-                                                                             actual_word_size);\n+                                                                             actual_word_size,\n+                                                                             true);\n@@ -276,1 +277,2 @@\n-                                                              actual_word_size);\n+                                                              actual_word_size,\n+                                                              true);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1Allocator.cpp","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -115,0 +115,3 @@\n+  \/\/ Attempt allocation in the current alloc region. If use_retained_region_if_available\n+  \/\/ is set and a retained region is available, the allocation will first be tried in the\n+  \/\/ retained region.\n@@ -117,2 +120,10 @@\n-                                      size_t* actual_word_size);\n-  inline HeapWord* attempt_allocation_locked(size_t word_size);\n+                                      size_t* actual_word_size,\n+                                      bool use_retained_region_if_available);\n+\n+  \/\/ This is to be called when holding a lock. The attempt_lock_free_first parameter\n+  \/\/ is provided as a convenience to callers who acquire the lock just before calling\n+  \/\/ this method. Things may have changed while they were waiting for the lock such\n+  \/\/ that they no longer need to go through the slow path. Setting this parameter\n+  \/\/ to true will make this function optimistically try the normal attempt_allocation\n+  \/\/ path first on entry.\n+  inline HeapWord* attempt_allocation_locked(size_t word_size, bool attempt_lock_free_first);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1Allocator.hpp","additions":13,"deletions":2,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -53,1 +53,2 @@\n-                                                 size_t* actual_word_size) {\n+                                                 size_t* actual_word_size,\n+                                                 bool use_retained_region_if_available) {\n@@ -55,3 +56,6 @@\n-  HeapWord* result = mutator_alloc_region(node_index)->attempt_retained_allocation(min_word_size, desired_word_size, actual_word_size);\n-  if (result != NULL) {\n-    return result;\n+\n+  if (use_retained_region_if_available) {\n+    HeapWord* result = mutator_alloc_region(node_index)->attempt_retained_allocation(min_word_size, desired_word_size, actual_word_size);\n+    if (result != NULL) {\n+      return result;\n+    }\n@@ -59,0 +63,1 @@\n+\n@@ -62,1 +67,1 @@\n-inline HeapWord* G1Allocator::attempt_allocation_locked(size_t word_size) {\n+inline HeapWord* G1Allocator::attempt_allocation_locked(size_t word_size, bool attempt_lock_free_first) {\n@@ -64,1 +69,1 @@\n-  HeapWord* result = mutator_alloc_region(node_index)->attempt_allocation_locked(word_size);\n+  HeapWord* result = mutator_alloc_region(node_index)->attempt_allocation_locked(word_size, attempt_lock_free_first);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1Allocator.inline.hpp","additions":11,"deletions":6,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -423,0 +423,1 @@\n+    bool proactive_collection_required = false;\n@@ -427,1 +428,5 @@\n-      result = _allocator->attempt_allocation_locked(word_size);\n+\n+      \/\/ Now that we have the lock, we first retry the allocation in case another\n+      \/\/ thread changed the region while we were waiting to acquire the lock.\n+      size_t actual_size;\n+      result = _allocator->attempt_allocation(word_size, word_size, &actual_size, false);\n@@ -432,7 +437,5 @@\n-      \/\/ If the GCLocker is active and we are bound for a GC, try expanding young gen.\n-      \/\/ This is different to when only GCLocker::needs_gc() is set: try to avoid\n-      \/\/ waiting because the GCLocker is active to not wait too long.\n-      if (GCLocker::is_active_and_needs_gc() && policy()->can_expand_young_list()) {\n-        \/\/ No need for an ergo message here, can_expand_young_list() does this when\n-        \/\/ it returns true.\n-        result = _allocator->attempt_allocation_force(word_size);\n+      proactive_collection_required = policy()->proactive_collection_required(1);\n+      if (!proactive_collection_required) {\n+        \/\/ We've already attempted a lock-free allocation above, so we don't want to\n+        \/\/ do it again. Let's jump straight to replacing the active region.\n+        result = _allocator->attempt_allocation_locked(word_size, false \/* attempt_lock_free_first *\/);\n@@ -442,0 +445,12 @@\n+\n+        \/\/ If the GCLocker is active and we are bound for a GC, try expanding young gen.\n+        \/\/ This is different to when only GCLocker::needs_gc() is set: try to avoid\n+        \/\/ waiting because the GCLocker is active to not wait too long.\n+        if (GCLocker::is_active_and_needs_gc() && policy()->can_expand_young_list()) {\n+          \/\/ No need for an ergo message here, can_expand_young_list() does this when\n+          \/\/ it returns true.\n+          result = _allocator->attempt_allocation_force(word_size);\n+          if (result != NULL) {\n+            return result;\n+          }\n+        }\n@@ -443,0 +458,1 @@\n+\n@@ -452,0 +468,2 @@\n+      GCCause::Cause gc_cause = proactive_collection_required ? GCCause::_g1_proactive_collection\n+                                                              : GCCause::_g1_inc_collection_pause;\n@@ -453,2 +471,1 @@\n-      result = do_collection_pause(word_size, gc_count_before, &succeeded,\n-                                   GCCause::_g1_inc_collection_pause);\n+      result = do_collection_pause(word_size, gc_count_before, &succeeded, gc_cause);\n@@ -495,1 +512,1 @@\n-    result = _allocator->attempt_allocation(word_size, word_size, &dummy);\n+    result = _allocator->attempt_allocation(word_size, word_size, &dummy, true);\n@@ -722,1 +739,1 @@\n-  HeapWord* result = _allocator->attempt_allocation(min_word_size, desired_word_size, actual_word_size);\n+  HeapWord* result = _allocator->attempt_allocation(min_word_size, desired_word_size, actual_word_size, true);\n@@ -847,0 +864,1 @@\n+    bool proactive_collection_required = false;\n@@ -853,9 +871,12 @@\n-      \/\/ Given that humongous objects are not allocated in young\n-      \/\/ regions, we'll first try to do the allocation without doing a\n-      \/\/ collection hoping that there's enough space in the heap.\n-      result = humongous_obj_allocate(word_size);\n-      if (result != NULL) {\n-        size_t size_in_regions = humongous_obj_size_in_regions(word_size);\n-        policy()->old_gen_alloc_tracker()->\n-          add_allocated_humongous_bytes_since_last_gc(size_in_regions * HeapRegion::GrainBytes);\n-        return result;\n+      size_t size_in_regions = humongous_obj_size_in_regions(word_size);\n+      proactive_collection_required = policy()->proactive_collection_required((uint)size_in_regions);\n+      if (!proactive_collection_required) {\n+        \/\/ Given that humongous objects are not allocated in young\n+        \/\/ regions, we'll first try to do the allocation without doing a\n+        \/\/ collection hoping that there's enough space in the heap.\n+        result = humongous_obj_allocate(word_size);\n+        if (result != NULL) {\n+          policy()->old_gen_alloc_tracker()->\n+            add_allocated_humongous_bytes_since_last_gc(size_in_regions * HeapRegion::GrainBytes);\n+          return result;\n+        }\n@@ -873,0 +894,2 @@\n+      GCCause::Cause gc_cause = proactive_collection_required ? GCCause::_g1_proactive_collection\n+                                                              : GCCause::_g1_humongous_allocation;\n@@ -874,2 +897,1 @@\n-      result = do_collection_pause(word_size, gc_count_before, &succeeded,\n-                                   GCCause::_g1_humongous_allocation);\n+      result = do_collection_pause(word_size, gc_count_before, &succeeded, gc_cause);\n@@ -938,1 +960,1 @@\n-    return _allocator->attempt_allocation_locked(word_size);\n+    return _allocator->attempt_allocation_locked(word_size, true \/* attempt_lock_free_first *\/);\n@@ -1403,0 +1425,1 @@\n+  zzinit_complete(false),\n@@ -1757,0 +1780,3 @@\n+\n+  \/\/ TODO: temporary... is there a better way to do this?\n+  zzinit_complete = true;\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectedHeap.cpp","additions":50,"deletions":24,"binary":false,"changes":74,"status":"modified"},{"patch":"@@ -184,0 +184,2 @@\n+    bool zzinit_complete;\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectedHeap.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -72,0 +72,2 @@\n+  _predicted_surviving_bytes_from_survivor(0),\n+  _predicted_surviving_bytes_from_old(0),\n@@ -454,0 +456,1 @@\n+  update_survival_estimates_for_next_collection();\n@@ -785,0 +788,3 @@\n+  \/\/ Is this the right place? Should it be in the below?\n+  update_survival_estimates_for_next_collection();\n+\n@@ -1444,0 +1450,81 @@\n+\/\/ Number of regions required to store the given number of bytes, taking\n+\/\/ into account the target amount of wasted space in PLABs.\n+static uint get_num_regions_adjust_for_plab_waste(size_t byte_count) {\n+  size_t byte_count_adjusted = byte_count * (100 + TargetPLABWastePct) \/ 100.0;\n+\n+  \/\/ Round up the region count\n+  return (byte_count_adjusted + HeapRegion::GrainBytes - 1) \/ HeapRegion::GrainBytes;\n+}\n+\n+bool G1Policy::proactive_collection_required(uint alloc_region_count) {\n+  if (!_g1h->zzinit_complete) {\n+    \/\/ Don't attempt any proactive GC's before initialization is complete.\n+    \/\/ TODO: Is there a better way to do this without adding a new variable to hold\n+    \/\/ init state?\n+    return false;\n+  }\n+\n+  if (_g1h->young_regions_count() == 0 && _collection_set->candidates() != NULL && _collection_set->candidates()->is_empty()) {\n+    return false;\n+  }\n+\n+  uint eden_count = _g1h->eden_regions_count();\n+  size_t const eden_surv_bytes_pred = _eden_surv_rate_group->accum_surv_rate_pred(eden_count) * HeapRegion::GrainBytes;\n+  size_t const total_young_predicted_surviving_bytes = eden_surv_bytes_pred + _predicted_surviving_bytes_from_survivor;\n+\n+  uint required_regions = get_num_regions_adjust_for_plab_waste(total_young_predicted_surviving_bytes) +\n+                          get_num_regions_adjust_for_plab_waste(_predicted_surviving_bytes_from_old);\n+\n+  if (required_regions > _g1h->num_free_regions() - alloc_region_count) {\n+    log_debug(gc, ergo, cset)(\"Proactive GC, insufficient free regions. Predicted need %u. Curr Eden %u (Pred %u). Curr Survivor %u (Pred %u). Curr Old %u (Pred %u) Free %u Alloc %u\",\n+            required_regions,\n+            eden_count,\n+            get_num_regions_adjust_for_plab_waste(eden_surv_bytes_pred),\n+            _g1h->survivor_regions_count(),\n+            get_num_regions_adjust_for_plab_waste(_predicted_surviving_bytes_from_survivor),\n+            _g1h->old_regions_count(),\n+            get_num_regions_adjust_for_plab_waste(_predicted_surviving_bytes_from_old),\n+            _g1h->num_free_regions(),\n+            alloc_region_count);\n+\n+    return true;\n+  }\n+\n+  return false;\n+}\n+\n+void G1Policy::update_survival_estimates_for_next_collection() {\n+  \/\/ Predict the number of bytes of surviving objects from survivor and old\n+  \/\/ regions and update the associated members.\n+\n+  \/\/ Survivor regions\n+  size_t survivor_bytes = 0;\n+  const GrowableArray<HeapRegion*>* survivor_regions = _g1h->survivor()->regions();\n+  for (GrowableArrayIterator<HeapRegion*> it = survivor_regions->begin();\n+       it != survivor_regions->end();\n+       ++it) {\n+    survivor_bytes += predict_bytes_to_copy(*it);\n+  }\n+\n+  _predicted_surviving_bytes_from_survivor = survivor_bytes;\n+\n+  \/\/ Old regions\n+  G1CollectionSetCandidates *candidates = _collection_set->candidates();\n+  if (candidates == NULL || candidates->is_empty()) {\n+    _predicted_surviving_bytes_from_old = 0;\n+    return;\n+  }\n+\n+  \/\/ Use the minimum old gen collection set as conservative estimate for the number\n+  \/\/ of regions to take for this calculation.\n+  uint iterate_count = MIN(candidates->num_remaining(), calc_min_old_cset_length());\n+  uint current_index = candidates->cur_idx();\n+  size_t old_bytes = 0;\n+  for (uint i = 0; i < iterate_count; i++) {\n+    HeapRegion *region = candidates->at(current_index + i);\n+    old_bytes += predict_bytes_to_copy(region);\n+  }\n+\n+  _predicted_surviving_bytes_from_old = old_bytes;\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1Policy.cpp","additions":87,"deletions":0,"binary":false,"changes":87,"status":"modified"},{"patch":"@@ -101,0 +101,5 @@\n+  \/\/ These values are predictions of how much we think will survive in each\n+  \/\/ section of the heap.\n+  size_t _predicted_surviving_bytes_from_survivor;\n+  size_t _predicted_surviving_bytes_from_old;\n+\n@@ -365,0 +370,5 @@\n+  \/\/ Returns whether a collection should be done proactively, taking into\n+  \/\/ account the current number of free regions and the expected survival\n+  \/\/ rates in each section of the heap.\n+  bool proactive_collection_required(uint region_count);\n+\n@@ -366,0 +376,5 @@\n+\n+  \/\/ Predict the number of bytes of surviving objects from survivor and old\n+  \/\/ regions and update the associated members.\n+  void update_survival_estimates_for_next_collection();\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1Policy.hpp","additions":15,"deletions":0,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -123,1 +123,1 @@\n-  if (_word_size > 0) {\n+  if (_word_size > 0 && _gc_cause != GCCause::_g1_proactive_collection) {\n","filename":"src\/hotspot\/share\/gc\/g1\/g1VMOperations.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -102,0 +102,3 @@\n+    case _g1_proactive_collection:\n+      return \"G1 Proactive Collection\";\n+\n","filename":"src\/hotspot\/share\/gc\/shared\/gcCause.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -76,0 +76,1 @@\n+    _g1_proactive_collection,\n","filename":"src\/hotspot\/share\/gc\/shared\/gcCause.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -316,1 +316,1 @@\n-        private static Object[] holder = new Object[200]; \/\/ Must be larger than G1EvacuationFailureALotCount\n+        private static Object[] holder = new Object[800]; \/\/ Must be larger than G1EvacuationFailureALotCount\n@@ -321,1 +321,1 @@\n-            \/\/ Create 16 MB of garbage. This should result in at least one GC,\n+            \/\/ Create 64 MB of garbage. This should result in at least one GC,\n@@ -323,1 +323,2 @@\n-            \/\/ which is larger than G1EvacuationFailureALotInterval.\n+            \/\/ which is larger than G1EvacuationFailureALotInterval and enough\n+            \/\/ will survive to cause the evacuation failure.\n@@ -325,1 +326,1 @@\n-                holder[i % holder.length] = new byte[1024];\n+                holder[i % holder.length] = new byte[4096];\n","filename":"test\/hotspot\/jtreg\/gc\/g1\/TestGCLogMessages.java","additions":5,"deletions":4,"binary":false,"changes":9,"status":"modified"}]}