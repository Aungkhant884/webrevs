{"files":[{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2005, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2005, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -1414,1 +1414,1 @@\n-  if (!is_c1_or_interpreter_only()) {\n+  if (!CompilerConfig::is_c1_or_interpreter_only_no_aot_or_jvmci()) {\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIRGenerator_aarch64.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -35,1 +35,1 @@\n-#ifndef TIERED\n+#ifndef COMPILER2\n@@ -59,1 +59,1 @@\n-#endif \/\/ !TIERED\n+#endif \/\/ !COMPILER2\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_globals_aarch64.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -42,1 +42,1 @@\n-define_pd_global(bool, TieredCompilation,            trueInTiered);\n+define_pd_global(bool, TieredCompilation,            COMPILER1_PRESENT(true) NOT_COMPILER1(false));\n","filename":"src\/hotspot\/cpu\/aarch64\/c2_globals_aarch64.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -54,1 +54,1 @@\n-  if (is_c1_or_interpreter_only()) {\n+  if (CompilerConfig::is_c1_or_interpreter_only_no_aot_or_jvmci()) {\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/shenandoah\/c1\/shenandoahBarrierSetC1_aarch64.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -39,1 +39,1 @@\n-define_pd_global(uintx, CodeCacheSegmentSize,    64 TIERED_ONLY(+64)); \/\/ Tiered compilation has large code-entry alignment.\n+define_pd_global(uintx, CodeCacheSegmentSize,    64 COMPILER1_AND_COMPILER2_PRESENT(+64)); \/\/ Tiered compilation has large code-entry alignment.\n","filename":"src\/hotspot\/cpu\/aarch64\/globals_aarch64.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -592,4 +592,1 @@\n-void TemplateInterpreterGenerator::generate_counter_incr(\n-        Label* overflow,\n-        Label* profile_method,\n-        Label* profile_method_continue) {\n+void TemplateInterpreterGenerator::generate_counter_incr(Label* overflow) {\n@@ -598,69 +595,12 @@\n-  if (TieredCompilation) {\n-    int increment = InvocationCounter::count_increment;\n-    Label no_mdo;\n-    if (ProfileInterpreter) {\n-      \/\/ Are we profiling?\n-      __ ldr(r0, Address(rmethod, Method::method_data_offset()));\n-      __ cbz(r0, no_mdo);\n-      \/\/ Increment counter in the MDO\n-      const Address mdo_invocation_counter(r0, in_bytes(MethodData::invocation_counter_offset()) +\n-                                                in_bytes(InvocationCounter::counter_offset()));\n-      const Address mask(r0, in_bytes(MethodData::invoke_mask_offset()));\n-      __ increment_mask_and_jump(mdo_invocation_counter, increment, mask, rscratch1, rscratch2, false, Assembler::EQ, overflow);\n-      __ b(done);\n-    }\n-    __ bind(no_mdo);\n-    \/\/ Increment counter in MethodCounters\n-    const Address invocation_counter(rscratch2,\n-                  MethodCounters::invocation_counter_offset() +\n-                  InvocationCounter::counter_offset());\n-    __ get_method_counters(rmethod, rscratch2, done);\n-    const Address mask(rscratch2, in_bytes(MethodCounters::invoke_mask_offset()));\n-    __ increment_mask_and_jump(invocation_counter, increment, mask, rscratch1, r1, false, Assembler::EQ, overflow);\n-    __ bind(done);\n-  } else { \/\/ not TieredCompilation\n-    const Address backedge_counter(rscratch2,\n-                  MethodCounters::backedge_counter_offset() +\n-                  InvocationCounter::counter_offset());\n-    const Address invocation_counter(rscratch2,\n-                  MethodCounters::invocation_counter_offset() +\n-                  InvocationCounter::counter_offset());\n-\n-    __ get_method_counters(rmethod, rscratch2, done);\n-\n-    if (ProfileInterpreter) { \/\/ %%% Merge this into MethodData*\n-      __ ldrw(r1, Address(rscratch2, MethodCounters::interpreter_invocation_counter_offset()));\n-      __ addw(r1, r1, 1);\n-      __ strw(r1, Address(rscratch2, MethodCounters::interpreter_invocation_counter_offset()));\n-    }\n-    \/\/ Update standard invocation counters\n-    __ ldrw(r1, invocation_counter);\n-    __ ldrw(r0, backedge_counter);\n-\n-    __ addw(r1, r1, InvocationCounter::count_increment);\n-    __ andw(r0, r0, InvocationCounter::count_mask_value);\n-\n-    __ strw(r1, invocation_counter);\n-    __ addw(r0, r0, r1);                \/\/ add both counters\n-\n-    \/\/ profile_method is non-null only for interpreted method so\n-    \/\/ profile_method != NULL == !native_call\n-\n-    if (ProfileInterpreter && profile_method != NULL) {\n-      \/\/ Test to see if we should create a method data oop\n-      __ ldr(rscratch2, Address(rmethod, Method::method_counters_offset()));\n-      __ ldrw(rscratch2, Address(rscratch2, in_bytes(MethodCounters::interpreter_profile_limit_offset())));\n-      __ cmpw(r0, rscratch2);\n-      __ br(Assembler::LT, *profile_method_continue);\n-\n-      \/\/ if no method data exists, go to profile_method\n-      __ test_method_data_pointer(rscratch2, *profile_method);\n-    }\n-\n-    {\n-      __ ldr(rscratch2, Address(rmethod, Method::method_counters_offset()));\n-      __ ldrw(rscratch2, Address(rscratch2, in_bytes(MethodCounters::interpreter_invocation_limit_offset())));\n-      __ cmpw(r0, rscratch2);\n-      __ br(Assembler::HS, *overflow);\n-    }\n-    __ bind(done);\n+  int increment = InvocationCounter::count_increment;\n+  Label no_mdo;\n+  if (ProfileInterpreter) {\n+    \/\/ Are we profiling?\n+    __ ldr(r0, Address(rmethod, Method::method_data_offset()));\n+    __ cbz(r0, no_mdo);\n+    \/\/ Increment counter in the MDO\n+    const Address mdo_invocation_counter(r0, in_bytes(MethodData::invocation_counter_offset()) +\n+                                              in_bytes(InvocationCounter::counter_offset()));\n+    const Address mask(r0, in_bytes(MethodData::invoke_mask_offset()));\n+    __ increment_mask_and_jump(mdo_invocation_counter, increment, mask, rscratch1, rscratch2, false, Assembler::EQ, overflow);\n+    __ b(done);\n@@ -668,0 +608,9 @@\n+  __ bind(no_mdo);\n+  \/\/ Increment counter in MethodCounters\n+  const Address invocation_counter(rscratch2,\n+                MethodCounters::invocation_counter_offset() +\n+                InvocationCounter::counter_offset());\n+  __ get_method_counters(rmethod, rscratch2, done);\n+  const Address mask(rscratch2, in_bytes(MethodCounters::invoke_mask_offset()));\n+  __ increment_mask_and_jump(invocation_counter, increment, mask, rscratch1, r1, false, Assembler::EQ, overflow);\n+  __ bind(done);\n@@ -1210,1 +1159,1 @@\n-    generate_counter_incr(&invocation_counter_overflow, NULL, NULL);\n+    generate_counter_incr(&invocation_counter_overflow);\n@@ -1654,2 +1603,0 @@\n-  Label profile_method;\n-  Label profile_method_continue;\n@@ -1657,6 +1604,1 @@\n-    generate_counter_incr(&invocation_counter_overflow,\n-                          &profile_method,\n-                          &profile_method_continue);\n-    if (ProfileInterpreter) {\n-      __ bind(profile_method_continue);\n-    }\n+    generate_counter_incr(&invocation_counter_overflow);\n@@ -1714,9 +1656,0 @@\n-    if (ProfileInterpreter) {\n-      \/\/ We have decided to profile this method in the interpreter\n-      __ bind(profile_method);\n-      __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::profile_method));\n-      __ set_method_data_pointer_for_bcp();\n-      \/\/ don't think we need this\n-      __ get_method(r1);\n-      __ b(profile_method_continue);\n-    }\n","filename":"src\/hotspot\/cpu\/aarch64\/templateInterpreterGenerator_aarch64.cpp","additions":25,"deletions":92,"binary":false,"changes":117,"status":"modified"},{"patch":"@@ -1802,1 +1802,0 @@\n-  Label profile_method;\n@@ -1829,22 +1828,12 @@\n-    if (TieredCompilation) {\n-      Label no_mdo;\n-      int increment = InvocationCounter::count_increment;\n-      if (ProfileInterpreter) {\n-        \/\/ Are we profiling?\n-        __ ldr(r1, Address(rmethod, in_bytes(Method::method_data_offset())));\n-        __ cbz(r1, no_mdo);\n-        \/\/ Increment the MDO backedge counter\n-        const Address mdo_backedge_counter(r1, in_bytes(MethodData::backedge_counter_offset()) +\n-                                           in_bytes(InvocationCounter::counter_offset()));\n-        const Address mask(r1, in_bytes(MethodData::backedge_mask_offset()));\n-        __ increment_mask_and_jump(mdo_backedge_counter, increment, mask,\n-                                   r0, rscratch1, false, Assembler::EQ,\n-                                   UseOnStackReplacement ? &backedge_counter_overflow : &dispatch);\n-        __ b(dispatch);\n-      }\n-      __ bind(no_mdo);\n-      \/\/ Increment backedge counter in MethodCounters*\n-      __ ldr(rscratch1, Address(rmethod, Method::method_counters_offset()));\n-      const Address mask(rscratch1, in_bytes(MethodCounters::backedge_mask_offset()));\n-      __ increment_mask_and_jump(Address(rscratch1, be_offset), increment, mask,\n-                                 r0, rscratch2, false, Assembler::EQ,\n+    Label no_mdo;\n+    int increment = InvocationCounter::count_increment;\n+    if (ProfileInterpreter) {\n+      \/\/ Are we profiling?\n+      __ ldr(r1, Address(rmethod, in_bytes(Method::method_data_offset())));\n+      __ cbz(r1, no_mdo);\n+      \/\/ Increment the MDO backedge counter\n+      const Address mdo_backedge_counter(r1, in_bytes(MethodData::backedge_counter_offset()) +\n+                                         in_bytes(InvocationCounter::counter_offset()));\n+      const Address mask(r1, in_bytes(MethodData::backedge_mask_offset()));\n+      __ increment_mask_and_jump(mdo_backedge_counter, increment, mask,\n+                                 r0, rscratch1, false, Assembler::EQ,\n@@ -1852,46 +1841,1 @@\n-    } else { \/\/ not TieredCompilation\n-      \/\/ increment counter\n-      __ ldr(rscratch2, Address(rmethod, Method::method_counters_offset()));\n-      __ ldrw(r0, Address(rscratch2, be_offset));        \/\/ load backedge counter\n-      __ addw(rscratch1, r0, InvocationCounter::count_increment); \/\/ increment counter\n-      __ strw(rscratch1, Address(rscratch2, be_offset));        \/\/ store counter\n-\n-      __ ldrw(r0, Address(rscratch2, inv_offset));    \/\/ load invocation counter\n-      __ andw(r0, r0, (unsigned)InvocationCounter::count_mask_value); \/\/ and the status bits\n-      __ addw(r0, r0, rscratch1);        \/\/ add both counters\n-\n-      if (ProfileInterpreter) {\n-        \/\/ Test to see if we should create a method data oop\n-        __ ldrw(rscratch1, Address(rscratch2, in_bytes(MethodCounters::interpreter_profile_limit_offset())));\n-        __ cmpw(r0, rscratch1);\n-        __ br(Assembler::LT, dispatch);\n-\n-        \/\/ if no method data exists, go to profile method\n-        __ test_method_data_pointer(r0, profile_method);\n-\n-        if (UseOnStackReplacement) {\n-          \/\/ check for overflow against w1 which is the MDO taken count\n-          __ ldrw(rscratch1, Address(rscratch2, in_bytes(MethodCounters::interpreter_backward_branch_limit_offset())));\n-          __ cmpw(r1, rscratch1);\n-          __ br(Assembler::LO, dispatch); \/\/ Intel == Assembler::below\n-\n-          \/\/ When ProfileInterpreter is on, the backedge_count comes\n-          \/\/ from the MethodData*, which value does not get reset on\n-          \/\/ the call to frequency_counter_overflow().  To avoid\n-          \/\/ excessive calls to the overflow routine while the method is\n-          \/\/ being compiled, add a second test to make sure the overflow\n-          \/\/ function is called only once every overflow_frequency.\n-          const int overflow_frequency = 1024;\n-          __ andsw(r1, r1, overflow_frequency - 1);\n-          __ br(Assembler::EQ, backedge_counter_overflow);\n-\n-        }\n-      } else {\n-        if (UseOnStackReplacement) {\n-          \/\/ check for overflow against w0, which is the sum of the\n-          \/\/ counters\n-          __ ldrw(rscratch1, Address(rscratch2, in_bytes(MethodCounters::interpreter_backward_branch_limit_offset())));\n-          __ cmpw(r0, rscratch1);\n-          __ br(Assembler::HS, backedge_counter_overflow); \/\/ Intel == Assembler::aboveEqual\n-        }\n-      }\n+      __ b(dispatch);\n@@ -1899,0 +1843,7 @@\n+    __ bind(no_mdo);\n+    \/\/ Increment backedge counter in MethodCounters*\n+    __ ldr(rscratch1, Address(rmethod, Method::method_counters_offset()));\n+    const Address mask(rscratch1, in_bytes(MethodCounters::backedge_mask_offset()));\n+    __ increment_mask_and_jump(Address(rscratch1, be_offset), increment, mask,\n+                               r0, rscratch2, false, Assembler::EQ,\n+                               UseOnStackReplacement ? &backedge_counter_overflow : &dispatch);\n@@ -1910,9 +1861,11 @@\n-  if (UseLoopCounter) {\n-    if (ProfileInterpreter && !TieredCompilation) {\n-      \/\/ Out-of-line code to allocate method data oop.\n-      __ bind(profile_method);\n-      __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::profile_method));\n-      __ load_unsigned_byte(r1, Address(rbcp, 0));  \/\/ restore target bytecode\n-      __ set_method_data_pointer_for_bcp();\n-      __ b(dispatch);\n-    }\n+  if (UseLoopCounter && UseOnStackReplacement) {\n+    \/\/ invocation counter overflow\n+    __ bind(backedge_counter_overflow);\n+    __ neg(r2, r2);\n+    __ add(r2, r2, rbcp);     \/\/ branch bcp\n+    \/\/ IcoResult frequency_counter_overflow([JavaThread*], address branch_bcp)\n+    __ call_VM(noreg,\n+               CAST_FROM_FN_PTR(address,\n+                                InterpreterRuntime::frequency_counter_overflow),\n+               r2);\n+    __ load_unsigned_byte(r1, Address(rbcp, 0));  \/\/ restore target bytecode\n@@ -1920,46 +1873,33 @@\n-    if (UseOnStackReplacement) {\n-      \/\/ invocation counter overflow\n-      __ bind(backedge_counter_overflow);\n-      __ neg(r2, r2);\n-      __ add(r2, r2, rbcp);     \/\/ branch bcp\n-      \/\/ IcoResult frequency_counter_overflow([JavaThread*], address branch_bcp)\n-      __ call_VM(noreg,\n-                 CAST_FROM_FN_PTR(address,\n-                                  InterpreterRuntime::frequency_counter_overflow),\n-                 r2);\n-      __ load_unsigned_byte(r1, Address(rbcp, 0));  \/\/ restore target bytecode\n-\n-      \/\/ r0: osr nmethod (osr ok) or NULL (osr not possible)\n-      \/\/ w1: target bytecode\n-      \/\/ r2: scratch\n-      __ cbz(r0, dispatch);     \/\/ test result -- no osr if null\n-      \/\/ nmethod may have been invalidated (VM may block upon call_VM return)\n-      __ ldrb(r2, Address(r0, nmethod::state_offset()));\n-      if (nmethod::in_use != 0)\n-        __ sub(r2, r2, nmethod::in_use);\n-      __ cbnz(r2, dispatch);\n-\n-      \/\/ We have the address of an on stack replacement routine in r0\n-      \/\/ We need to prepare to execute the OSR method. First we must\n-      \/\/ migrate the locals and monitors off of the stack.\n-\n-      __ mov(r19, r0);                             \/\/ save the nmethod\n-\n-      call_VM(noreg, CAST_FROM_FN_PTR(address, SharedRuntime::OSR_migration_begin));\n-\n-      \/\/ r0 is OSR buffer, move it to expected parameter location\n-      __ mov(j_rarg0, r0);\n-\n-      \/\/ remove activation\n-      \/\/ get sender esp\n-      __ ldr(esp,\n-          Address(rfp, frame::interpreter_frame_sender_sp_offset * wordSize));\n-      \/\/ remove frame anchor\n-      __ leave();\n-      \/\/ Ensure compiled code always sees stack at proper alignment\n-      __ andr(sp, esp, -16);\n-\n-      \/\/ and begin the OSR nmethod\n-      __ ldr(rscratch1, Address(r19, nmethod::osr_entry_point_offset()));\n-      __ br(rscratch1);\n-    }\n+    \/\/ r0: osr nmethod (osr ok) or NULL (osr not possible)\n+    \/\/ w1: target bytecode\n+    \/\/ r2: scratch\n+    __ cbz(r0, dispatch);     \/\/ test result -- no osr if null\n+    \/\/ nmethod may have been invalidated (VM may block upon call_VM return)\n+    __ ldrb(r2, Address(r0, nmethod::state_offset()));\n+    if (nmethod::in_use != 0)\n+      __ sub(r2, r2, nmethod::in_use);\n+    __ cbnz(r2, dispatch);\n+\n+    \/\/ We have the address of an on stack replacement routine in r0\n+    \/\/ We need to prepare to execute the OSR method. First we must\n+    \/\/ migrate the locals and monitors off of the stack.\n+\n+    __ mov(r19, r0);                             \/\/ save the nmethod\n+\n+    call_VM(noreg, CAST_FROM_FN_PTR(address, SharedRuntime::OSR_migration_begin));\n+\n+    \/\/ r0 is OSR buffer, move it to expected parameter location\n+    __ mov(j_rarg0, r0);\n+\n+    \/\/ remove activation\n+    \/\/ get sender esp\n+    __ ldr(esp,\n+        Address(rfp, frame::interpreter_frame_sender_sp_offset * wordSize));\n+    \/\/ remove frame anchor\n+    __ leave();\n+    \/\/ Ensure compiled code always sees stack at proper alignment\n+    __ andr(sp, esp, -16);\n+\n+    \/\/ and begin the OSR nmethod\n+    __ ldr(rscratch1, Address(r19, nmethod::osr_entry_point_offset()));\n+    __ br(rscratch1);\n@@ -2487,1 +2427,1 @@\n-  if (!is_c1_or_interpreter_only()){\n+  if (!CompilerConfig::is_c1_or_interpreter_only_no_aot_or_jvmci()){\n@@ -3090,1 +3030,1 @@\n-  if (!is_c1_or_interpreter_only()) {\n+  if (!CompilerConfig::is_c1_or_interpreter_only_no_aot_or_jvmci()) {\n@@ -3152,1 +3092,1 @@\n-  if (!is_c1_or_interpreter_only()) {\n+  if (!CompilerConfig::is_c1_or_interpreter_only_no_aot_or_jvmci()) {\n","filename":"src\/hotspot\/cpu\/aarch64\/templateTable_aarch64.cpp","additions":67,"deletions":127,"binary":false,"changes":194,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2008, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2008, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -37,1 +37,1 @@\n-define_pd_global(uintx, CodeCacheSegmentSize, 64 TIERED_ONLY(+64)); \/\/ Tiered compilation has large code-entry alignment.\n+define_pd_global(uintx, CodeCacheSegmentSize, 64 COMPILER1_AND_COMPILER2_PRESENT(+64)); \/\/ Tiered compilation has large code-entry alignment.\n","filename":"src\/hotspot\/cpu\/arm\/globals_arm.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2008, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2008, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -367,3 +367,1 @@\n-void TemplateInterpreterGenerator::generate_counter_incr(Label* overflow,\n-                                                 Label* profile_method,\n-                                                 Label* profile_method_continue) {\n+void TemplateInterpreterGenerator::generate_counter_incr(Label* overflow) {\n@@ -378,72 +376,13 @@\n-  if (TieredCompilation) {\n-    int increment = InvocationCounter::count_increment;\n-    Label no_mdo;\n-    if (ProfileInterpreter) {\n-      \/\/ Are we profiling?\n-      __ ldr(R1_tmp, Address(Rmethod, Method::method_data_offset()));\n-      __ cbz(R1_tmp, no_mdo);\n-      \/\/ Increment counter in the MDO\n-      const Address mdo_invocation_counter(R1_tmp,\n-                    in_bytes(MethodData::invocation_counter_offset()) +\n-                    in_bytes(InvocationCounter::counter_offset()));\n-      const Address mask(R1_tmp, in_bytes(MethodData::invoke_mask_offset()));\n-      __ increment_mask_and_jump(mdo_invocation_counter, increment, mask, R0_tmp, Rtemp, eq, overflow);\n-      __ b(done);\n-    }\n-    __ bind(no_mdo);\n-    __ get_method_counters(Rmethod, Rcounters, done);\n-    const Address mask(Rcounters, in_bytes(MethodCounters::invoke_mask_offset()));\n-    __ increment_mask_and_jump(invocation_counter, increment, mask, R0_tmp, R1_tmp, eq, overflow);\n-    __ bind(done);\n-  } else { \/\/ not TieredCompilation\n-    const Address backedge_counter(Rcounters,\n-                  MethodCounters::backedge_counter_offset() +\n-                  InvocationCounter::counter_offset());\n-\n-    const Register Ricnt = R0_tmp;  \/\/ invocation counter\n-    const Register Rbcnt = R1_tmp;  \/\/ backedge counter\n-\n-    __ get_method_counters(Rmethod, Rcounters, done);\n-\n-    if (ProfileInterpreter) {\n-      const Register Riic = R1_tmp;\n-      __ ldr_s32(Riic, Address(Rcounters, MethodCounters::interpreter_invocation_counter_offset()));\n-      __ add(Riic, Riic, 1);\n-      __ str_32(Riic, Address(Rcounters, MethodCounters::interpreter_invocation_counter_offset()));\n-    }\n-\n-    \/\/ Update standard invocation counters\n-\n-    __ ldr_u32(Ricnt, invocation_counter);\n-    __ ldr_u32(Rbcnt, backedge_counter);\n-\n-    __ add(Ricnt, Ricnt, InvocationCounter::count_increment);\n-\n-    __ bic(Rbcnt, Rbcnt, ~InvocationCounter::count_mask_value); \/\/ mask out the status bits\n-\n-    __ str_32(Ricnt, invocation_counter);            \/\/ save invocation count\n-    __ add(Ricnt, Ricnt, Rbcnt);                     \/\/ add both counters\n-\n-    \/\/ profile_method is non-null only for interpreted method so\n-    \/\/ profile_method != NULL == !native_call\n-    \/\/ BytecodeInterpreter only calls for native so code is elided.\n-\n-    if (ProfileInterpreter && profile_method != NULL) {\n-      assert(profile_method_continue != NULL, \"should be non-null\");\n-\n-      \/\/ Test to see if we should create a method data oop\n-      \/\/ Reuse R1_tmp as we don't need backedge counters anymore.\n-      Address profile_limit(Rcounters, in_bytes(MethodCounters::interpreter_profile_limit_offset()));\n-      __ ldr_s32(R1_tmp, profile_limit);\n-      __ cmp_32(Ricnt, R1_tmp);\n-      __ b(*profile_method_continue, lt);\n-\n-      \/\/ if no method data exists, go to profile_method\n-      __ test_method_data_pointer(R1_tmp, *profile_method);\n-    }\n-\n-    Address invoke_limit(Rcounters, in_bytes(MethodCounters::interpreter_invocation_limit_offset()));\n-    __ ldr_s32(R1_tmp, invoke_limit);\n-    __ cmp_32(Ricnt, R1_tmp);\n-    __ b(*overflow, hs);\n-    __ bind(done);\n+  int increment = InvocationCounter::count_increment;\n+  Label no_mdo;\n+  if (ProfileInterpreter) {\n+    \/\/ Are we profiling?\n+    __ ldr(R1_tmp, Address(Rmethod, Method::method_data_offset()));\n+    __ cbz(R1_tmp, no_mdo);\n+    \/\/ Increment counter in the MDO\n+    const Address mdo_invocation_counter(R1_tmp,\n+                  in_bytes(MethodData::invocation_counter_offset()) +\n+                  in_bytes(InvocationCounter::counter_offset()));\n+    const Address mask(R1_tmp, in_bytes(MethodData::invoke_mask_offset()));\n+    __ increment_mask_and_jump(mdo_invocation_counter, increment, mask, R0_tmp, Rtemp, eq, overflow);\n+    __ b(done);\n@@ -451,0 +390,5 @@\n+  __ bind(no_mdo);\n+  __ get_method_counters(Rmethod, Rcounters, done);\n+  const Address mask(Rcounters, in_bytes(MethodCounters::invoke_mask_offset()));\n+  __ increment_mask_and_jump(invocation_counter, increment, mask, R0_tmp, R1_tmp, eq, overflow);\n+  __ bind(done);\n@@ -812,1 +756,1 @@\n-    generate_counter_incr(&invocation_counter_overflow, NULL, NULL);\n+    generate_counter_incr(&invocation_counter_overflow);\n@@ -1157,2 +1101,0 @@\n-  Label profile_method;\n-  Label profile_method_continue;\n@@ -1165,4 +1107,1 @@\n-    generate_counter_incr(&invocation_counter_overflow, &profile_method, &profile_method_continue);\n-    if (ProfileInterpreter) {\n-      __ bind(profile_method_continue);\n-    }\n+    generate_counter_incr(&invocation_counter_overflow);\n@@ -1221,10 +1160,0 @@\n-    if (ProfileInterpreter) {\n-      \/\/ We have decided to profile this method in the interpreter\n-      __ bind(profile_method);\n-\n-      __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::profile_method));\n-      __ set_method_data_pointer_for_bcp();\n-\n-      __ b(profile_method_continue);\n-    }\n-\n","filename":"src\/hotspot\/cpu\/arm\/templateInterpreterGenerator_arm.cpp","additions":22,"deletions":93,"binary":false,"changes":115,"status":"modified"},{"patch":"@@ -2069,1 +2069,0 @@\n-  Label profile_method;\n@@ -2083,23 +2082,11 @@\n-    if (TieredCompilation) {\n-      Label no_mdo;\n-      int increment = InvocationCounter::count_increment;\n-      if (ProfileInterpreter) {\n-        \/\/ Are we profiling?\n-        __ ldr(Rtemp, Address(Rmethod, Method::method_data_offset()));\n-        __ cbz(Rtemp, no_mdo);\n-        \/\/ Increment the MDO backedge counter\n-        const Address mdo_backedge_counter(Rtemp, in_bytes(MethodData::backedge_counter_offset()) +\n-                                                  in_bytes(InvocationCounter::counter_offset()));\n-        const Address mask(Rtemp, in_bytes(MethodData::backedge_mask_offset()));\n-        __ increment_mask_and_jump(mdo_backedge_counter, increment, mask,\n-                                   Rcnt, R4_tmp, eq, &backedge_counter_overflow);\n-        __ b(dispatch);\n-      }\n-      __ bind(no_mdo);\n-      \/\/ Increment backedge counter in MethodCounters*\n-      \/\/ Note Rbumped_taken_count is a callee saved registers for ARM32\n-      __ get_method_counters(Rmethod, Rcounters, dispatch, true \/*saveRegs*\/,\n-                             Rdisp, R3_bytecode,\n-                             noreg);\n-      const Address mask(Rcounters, in_bytes(MethodCounters::backedge_mask_offset()));\n-      __ increment_mask_and_jump(Address(Rcounters, be_offset), increment, mask,\n+    Label no_mdo;\n+    int increment = InvocationCounter::count_increment;\n+    if (ProfileInterpreter) {\n+      \/\/ Are we profiling?\n+      __ ldr(Rtemp, Address(Rmethod, Method::method_data_offset()));\n+      __ cbz(Rtemp, no_mdo);\n+      \/\/ Increment the MDO backedge counter\n+      const Address mdo_backedge_counter(Rtemp, in_bytes(MethodData::backedge_counter_offset()) +\n+                                                in_bytes(InvocationCounter::counter_offset()));\n+      const Address mask(Rtemp, in_bytes(MethodData::backedge_mask_offset()));\n+      __ increment_mask_and_jump(mdo_backedge_counter, increment, mask,\n@@ -2107,53 +2094,1 @@\n-    } else { \/\/ not TieredCompilation\n-      \/\/ Increment backedge counter in MethodCounters*\n-      __ get_method_counters(Rmethod, Rcounters, dispatch, true \/*saveRegs*\/,\n-                             Rdisp, R3_bytecode,\n-                             noreg);\n-      __ ldr_u32(Rtemp, Address(Rcounters, be_offset));           \/\/ load backedge counter\n-      __ add(Rtemp, Rtemp, InvocationCounter::count_increment);   \/\/ increment counter\n-      __ str_32(Rtemp, Address(Rcounters, be_offset));            \/\/ store counter\n-\n-      __ ldr_u32(Rcnt, Address(Rcounters, inv_offset));           \/\/ load invocation counter\n-      __ bic(Rcnt, Rcnt, ~InvocationCounter::count_mask_value);  \/\/ and the status bits\n-      __ add(Rcnt, Rcnt, Rtemp);                                 \/\/ add both counters\n-\n-      if (ProfileInterpreter) {\n-        \/\/ Test to see if we should create a method data oop\n-        const Address profile_limit(Rcounters, in_bytes(MethodCounters::interpreter_profile_limit_offset()));\n-        __ ldr_s32(Rtemp, profile_limit);\n-        __ cmp_32(Rcnt, Rtemp);\n-        __ b(dispatch, lt);\n-\n-        \/\/ if no method data exists, go to profile method\n-        __ test_method_data_pointer(R4_tmp, profile_method);\n-\n-        if (UseOnStackReplacement) {\n-          \/\/ check for overflow against Rbumped_taken_count, which is the MDO taken count\n-          const Address backward_branch_limit(Rcounters, in_bytes(MethodCounters::interpreter_backward_branch_limit_offset()));\n-          __ ldr_s32(Rtemp, backward_branch_limit);\n-          __ cmp(Rbumped_taken_count, Rtemp);\n-          __ b(dispatch, lo);\n-\n-          \/\/ When ProfileInterpreter is on, the backedge_count comes from the\n-          \/\/ MethodData*, which value does not get reset on the call to\n-          \/\/ frequency_counter_overflow().  To avoid excessive calls to the overflow\n-          \/\/ routine while the method is being compiled, add a second test to make\n-          \/\/ sure the overflow function is called only once every overflow_frequency.\n-          const int overflow_frequency = 1024;\n-\n-          \/\/ was '__ andrs(...,overflow_frequency-1)', testing if lowest 10 bits are 0\n-          assert(overflow_frequency == (1 << 10),\"shift by 22 not correct for expected frequency\");\n-          __ movs(Rbumped_taken_count, AsmOperand(Rbumped_taken_count, lsl, 22));\n-\n-          __ b(backedge_counter_overflow, eq);\n-        }\n-      } else {\n-        if (UseOnStackReplacement) {\n-          \/\/ check for overflow against Rcnt, which is the sum of the counters\n-          const Address backward_branch_limit(Rcounters, in_bytes(MethodCounters::interpreter_backward_branch_limit_offset()));\n-          __ ldr_s32(Rtemp, backward_branch_limit);\n-          __ cmp_32(Rcnt, Rtemp);\n-          __ b(backedge_counter_overflow, hs);\n-\n-        }\n-      }\n+      __ b(dispatch);\n@@ -2161,0 +2096,9 @@\n+    __ bind(no_mdo);\n+    \/\/ Increment backedge counter in MethodCounters*\n+    \/\/ Note Rbumped_taken_count is a callee saved registers for ARM32\n+    __ get_method_counters(Rmethod, Rcounters, dispatch, true \/*saveRegs*\/,\n+                           Rdisp, R3_bytecode,\n+                           noreg);\n+    const Address mask(Rcounters, in_bytes(MethodCounters::backedge_mask_offset()));\n+    __ increment_mask_and_jump(Address(Rcounters, be_offset), increment, mask,\n+                               Rcnt, R4_tmp, eq, &backedge_counter_overflow);\n@@ -2171,15 +2115,3 @@\n-  if (UseLoopCounter) {\n-    if (ProfileInterpreter && !TieredCompilation) {\n-      \/\/ Out-of-line code to allocate method data oop.\n-      __ bind(profile_method);\n-\n-      __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::profile_method));\n-      __ set_method_data_pointer_for_bcp();\n-      \/\/ reload next bytecode\n-      __ ldrb(R3_bytecode, Address(Rbcp));\n-      __ b(dispatch);\n-    }\n-\n-    if (UseOnStackReplacement) {\n-      \/\/ invocation counter overflow\n-      __ bind(backedge_counter_overflow);\n+  if (UseLoopCounter && UseOnStackReplacement) {\n+    \/\/ invocation counter overflow\n+    __ bind(backedge_counter_overflow);\n@@ -2187,2 +2119,2 @@\n-      __ sub(R1, Rbcp, Rdisp);                   \/\/ branch bcp\n-      call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::frequency_counter_overflow), R1);\n+    __ sub(R1, Rbcp, Rdisp);                   \/\/ branch bcp\n+    call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::frequency_counter_overflow), R1);\n@@ -2190,2 +2122,2 @@\n-      \/\/ R0: osr nmethod (osr ok) or NULL (osr not possible)\n-      const Register Rnmethod = R0;\n+    \/\/ R0: osr nmethod (osr ok) or NULL (osr not possible)\n+    const Register Rnmethod = R0;\n@@ -2193,1 +2125,1 @@\n-      __ ldrb(R3_bytecode, Address(Rbcp));       \/\/ reload next bytecode\n+    __ ldrb(R3_bytecode, Address(Rbcp));       \/\/ reload next bytecode\n@@ -2195,1 +2127,1 @@\n-      __ cbz(Rnmethod, dispatch);                \/\/ test result, no osr if null\n+    __ cbz(Rnmethod, dispatch);                \/\/ test result, no osr if null\n@@ -2197,4 +2129,4 @@\n-      \/\/ nmethod may have been invalidated (VM may block upon call_VM return)\n-      __ ldrb(R1_tmp, Address(Rnmethod, nmethod::state_offset()));\n-      __ cmp(R1_tmp, nmethod::in_use);\n-      __ b(dispatch, ne);\n+    \/\/ nmethod may have been invalidated (VM may block upon call_VM return)\n+    __ ldrb(R1_tmp, Address(Rnmethod, nmethod::state_offset()));\n+    __ cmp(R1_tmp, nmethod::in_use);\n+    __ b(dispatch, ne);\n@@ -2202,3 +2134,3 @@\n-      \/\/ We have the address of an on stack replacement routine in Rnmethod,\n-      \/\/ We need to prepare to execute the OSR method. First we must\n-      \/\/ migrate the locals and monitors off of the stack.\n+    \/\/ We have the address of an on stack replacement routine in Rnmethod,\n+    \/\/ We need to prepare to execute the OSR method. First we must\n+    \/\/ migrate the locals and monitors off of the stack.\n@@ -2206,1 +2138,1 @@\n-      __ mov(Rtmp_save0, Rnmethod);                      \/\/ save the nmethod\n+    __ mov(Rtmp_save0, Rnmethod);                      \/\/ save the nmethod\n@@ -2208,1 +2140,1 @@\n-      call_VM(noreg, CAST_FROM_FN_PTR(address, SharedRuntime::OSR_migration_begin));\n+    call_VM(noreg, CAST_FROM_FN_PTR(address, SharedRuntime::OSR_migration_begin));\n@@ -2210,1 +2142,1 @@\n-      \/\/ R0 is OSR buffer\n+    \/\/ R0 is OSR buffer\n@@ -2212,2 +2144,2 @@\n-      __ ldr(R1_tmp, Address(Rtmp_save0, nmethod::osr_entry_point_offset()));\n-      __ ldr(Rtemp, Address(FP, frame::interpreter_frame_sender_sp_offset * wordSize));\n+    __ ldr(R1_tmp, Address(Rtmp_save0, nmethod::osr_entry_point_offset()));\n+    __ ldr(Rtemp, Address(FP, frame::interpreter_frame_sender_sp_offset * wordSize));\n@@ -2215,2 +2147,2 @@\n-      __ ldmia(FP, RegisterSet(FP) | RegisterSet(LR));\n-      __ bic(SP, Rtemp, StackAlignmentInBytes - 1);     \/\/ Remove frame and align stack\n+    __ ldmia(FP, RegisterSet(FP) | RegisterSet(LR));\n+    __ bic(SP, Rtemp, StackAlignmentInBytes - 1);     \/\/ Remove frame and align stack\n@@ -2218,2 +2150,1 @@\n-      __ jump(R1_tmp);\n-    }\n+    __ jump(R1_tmp);\n","filename":"src\/hotspot\/cpu\/arm\/templateTable_arm.cpp","additions":45,"deletions":114,"binary":false,"changes":159,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -35,1 +35,1 @@\n-#ifndef TIERED\n+#ifndef COMPILER2\n@@ -59,1 +59,1 @@\n-#endif \/\/ !TIERED\n+#endif \/\/ !COMPILER2\n","filename":"src\/hotspot\/cpu\/ppc\/c1_globals_ppc.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -42,1 +42,1 @@\n-define_pd_global(bool, TieredCompilation,            trueInTiered);\n+define_pd_global(bool, TieredCompilation,            COMPILER1_PRESENT(true) NOT_COMPILER1(false));\n","filename":"src\/hotspot\/cpu\/ppc\/c2_globals_ppc.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2002, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2002, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -204,1 +204,0 @@\n-  void test_backedge_count_for_osr(Register backedge_count, Register method_counters, Register target_bcp, Register disp, Register Rtmp);\n@@ -220,1 +219,0 @@\n-  void test_invocation_counter_for_mdp(Register invocation_count, Register method_counters, Register Rscratch, Label &profile_continue);\n","filename":"src\/hotspot\/cpu\/ppc\/interp_masm_ppc.hpp","additions":1,"deletions":3,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -1198,89 +1198,0 @@\n-void InterpreterMacroAssembler::test_invocation_counter_for_mdp(Register invocation_count,\n-                                                                Register method_counters,\n-                                                                Register Rscratch,\n-                                                                Label &profile_continue) {\n-  assert(ProfileInterpreter, \"must be profiling interpreter\");\n-  \/\/ Control will flow to \"profile_continue\" if the counter is less than the\n-  \/\/ limit or if we call profile_method().\n-  Label done;\n-\n-  \/\/ If no method data exists, and the counter is high enough, make one.\n-  lwz(Rscratch, in_bytes(MethodCounters::interpreter_profile_limit_offset()), method_counters);\n-\n-  cmpdi(CCR0, R28_mdx, 0);\n-  \/\/ Test to see if we should create a method data oop.\n-  cmpd(CCR1, Rscratch, invocation_count);\n-  bne(CCR0, done);\n-  bge(CCR1, profile_continue);\n-\n-  \/\/ Build it now.\n-  call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::profile_method));\n-  set_method_data_pointer_for_bcp();\n-  b(profile_continue);\n-\n-  align(32, 12);\n-  bind(done);\n-}\n-\n-void InterpreterMacroAssembler::test_backedge_count_for_osr(Register backedge_count, Register method_counters,\n-                                                            Register target_bcp, Register disp, Register Rtmp) {\n-  assert_different_registers(backedge_count, target_bcp, disp, Rtmp, R4_ARG2);\n-  assert(UseOnStackReplacement,\"Must UseOnStackReplacement to test_backedge_count_for_osr\");\n-\n-  Label did_not_overflow;\n-  Label overflow_with_error;\n-\n-  lwz(Rtmp, in_bytes(MethodCounters::interpreter_backward_branch_limit_offset()), method_counters);\n-  cmpw(CCR0, backedge_count, Rtmp);\n-\n-  blt(CCR0, did_not_overflow);\n-\n-  \/\/ When ProfileInterpreter is on, the backedge_count comes from the\n-  \/\/ methodDataOop, which value does not get reset on the call to\n-  \/\/ frequency_counter_overflow(). To avoid excessive calls to the overflow\n-  \/\/ routine while the method is being compiled, add a second test to make sure\n-  \/\/ the overflow function is called only once every overflow_frequency.\n-  if (ProfileInterpreter) {\n-    const int overflow_frequency = 1024;\n-    andi_(Rtmp, backedge_count, overflow_frequency-1);\n-    bne(CCR0, did_not_overflow);\n-  }\n-\n-  \/\/ Overflow in loop, pass branch bytecode.\n-  subf(R4_ARG2, disp, target_bcp); \/\/ Compute branch bytecode (previous bcp).\n-  call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::frequency_counter_overflow), R4_ARG2, true);\n-\n-  \/\/ Was an OSR adapter generated?\n-  cmpdi(CCR0, R3_RET, 0);\n-  beq(CCR0, overflow_with_error);\n-\n-  \/\/ Has the nmethod been invalidated already?\n-  lbz(Rtmp, nmethod::state_offset(), R3_RET);\n-  cmpwi(CCR0, Rtmp, nmethod::in_use);\n-  bne(CCR0, overflow_with_error);\n-\n-  \/\/ Migrate the interpreter frame off of the stack.\n-  \/\/ We can use all registers because we will not return to interpreter from this point.\n-\n-  \/\/ Save nmethod.\n-  const Register osr_nmethod = R31;\n-  mr(osr_nmethod, R3_RET);\n-  set_top_ijava_frame_at_SP_as_last_Java_frame(R1_SP, R11_scratch1);\n-  call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::OSR_migration_begin), R16_thread);\n-  reset_last_Java_frame();\n-  \/\/ OSR buffer is in ARG1\n-\n-  \/\/ Remove the interpreter frame.\n-  merge_frames(\/*top_frame_sp*\/ R21_sender_SP, \/*return_pc*\/ R0, R11_scratch1, R12_scratch2);\n-\n-  \/\/ Jump to the osr code.\n-  ld(R11_scratch1, nmethod::osr_entry_point_offset(), osr_nmethod);\n-  mtlr(R0);\n-  mtctr(R11_scratch1);\n-  bctr();\n-\n-  align(32, 12);\n-  bind(overflow_with_error);\n-  bind(did_not_overflow);\n-}\n-\n","filename":"src\/hotspot\/cpu\/ppc\/interp_masm_ppc_64.cpp","additions":1,"deletions":90,"binary":false,"changes":91,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2014, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -714,1 +714,1 @@\n-void TemplateInterpreterGenerator::generate_counter_incr(Label* overflow, Label* profile_method, Label* profile_method_continue) {\n+void TemplateInterpreterGenerator::generate_counter_incr(Label* overflow) {\n@@ -721,27 +721,12 @@\n-  if (TieredCompilation) {\n-    const int increment = InvocationCounter::count_increment;\n-    Label no_mdo;\n-    if (ProfileInterpreter) {\n-      const Register Rmdo = R3_counters;\n-      \/\/ If no method data exists, go to profile_continue.\n-      __ ld(Rmdo, in_bytes(Method::method_data_offset()), R19_method);\n-      __ cmpdi(CCR0, Rmdo, 0);\n-      __ beq(CCR0, no_mdo);\n-\n-      \/\/ Increment invocation counter in the MDO.\n-      const int mdo_ic_offs = in_bytes(MethodData::invocation_counter_offset()) + in_bytes(InvocationCounter::counter_offset());\n-      __ lwz(Rscratch2, mdo_ic_offs, Rmdo);\n-      __ lwz(Rscratch1, in_bytes(MethodData::invoke_mask_offset()), Rmdo);\n-      __ addi(Rscratch2, Rscratch2, increment);\n-      __ stw(Rscratch2, mdo_ic_offs, Rmdo);\n-      __ and_(Rscratch1, Rscratch2, Rscratch1);\n-      __ bne(CCR0, done);\n-      __ b(*overflow);\n-    }\n-\n-    \/\/ Increment counter in MethodCounters*.\n-    const int mo_ic_offs = in_bytes(MethodCounters::invocation_counter_offset()) + in_bytes(InvocationCounter::counter_offset());\n-    __ bind(no_mdo);\n-    __ get_method_counters(R19_method, R3_counters, done);\n-    __ lwz(Rscratch2, mo_ic_offs, R3_counters);\n-    __ lwz(Rscratch1, in_bytes(MethodCounters::invoke_mask_offset()), R3_counters);\n+  const int increment = InvocationCounter::count_increment;\n+  Label no_mdo;\n+  if (ProfileInterpreter) {\n+    const Register Rmdo = R3_counters;\n+    __ ld(Rmdo, in_bytes(Method::method_data_offset()), R19_method);\n+    __ cmpdi(CCR0, Rmdo, 0);\n+    __ beq(CCR0, no_mdo);\n+\n+    \/\/ Increment invocation counter in the MDO.\n+    const int mdo_ic_offs = in_bytes(MethodData::invocation_counter_offset()) + in_bytes(InvocationCounter::counter_offset());\n+    __ lwz(Rscratch2, mdo_ic_offs, Rmdo);\n+    __ lwz(Rscratch1, in_bytes(MethodData::invoke_mask_offset()), Rmdo);\n@@ -749,1 +734,1 @@\n-    __ stw(Rscratch2, mo_ic_offs, R3_counters);\n+    __ stw(Rscratch2, mdo_ic_offs, Rmdo);\n@@ -751,5 +736,3 @@\n-    __ beq(CCR0, *overflow);\n-\n-    __ bind(done);\n-\n-  } else {\n+    __ bne(CCR0, done);\n+    __ b(*overflow);\n+  }\n@@ -757,27 +740,10 @@\n-    \/\/ Update standard invocation counters.\n-    Register Rsum_ivc_bec = R4_ARG2;\n-    __ get_method_counters(R19_method, R3_counters, done);\n-    __ increment_invocation_counter(R3_counters, Rsum_ivc_bec, R12_scratch2);\n-    \/\/ Increment interpreter invocation counter.\n-    if (ProfileInterpreter) {  \/\/ %%% Merge this into methodDataOop.\n-      __ lwz(R12_scratch2, in_bytes(MethodCounters::interpreter_invocation_counter_offset()), R3_counters);\n-      __ addi(R12_scratch2, R12_scratch2, 1);\n-      __ stw(R12_scratch2, in_bytes(MethodCounters::interpreter_invocation_counter_offset()), R3_counters);\n-    }\n-    \/\/ Check if we must create a method data obj.\n-    if (ProfileInterpreter && profile_method != NULL) {\n-      const Register profile_limit = Rscratch1;\n-      __ lwz(profile_limit, in_bytes(MethodCounters::interpreter_profile_limit_offset()), R3_counters);\n-      \/\/ Test to see if we should create a method data oop.\n-      __ cmpw(CCR0, Rsum_ivc_bec, profile_limit);\n-      __ blt(CCR0, *profile_method_continue);\n-      \/\/ If no method data exists, go to profile_method.\n-      __ test_method_data_pointer(*profile_method);\n-    }\n-    \/\/ Finally check for counter overflow.\n-    if (overflow) {\n-      const Register invocation_limit = Rscratch1;\n-      __ lwz(invocation_limit, in_bytes(MethodCounters::interpreter_invocation_limit_offset()), R3_counters);\n-      __ cmpw(CCR0, Rsum_ivc_bec, invocation_limit);\n-      __ bge(CCR0, *overflow);\n-    }\n+  \/\/ Increment counter in MethodCounters*.\n+  const int mo_ic_offs = in_bytes(MethodCounters::invocation_counter_offset()) + in_bytes(InvocationCounter::counter_offset());\n+  __ bind(no_mdo);\n+  __ get_method_counters(R19_method, R3_counters, done);\n+  __ lwz(Rscratch2, mo_ic_offs, R3_counters);\n+  __ lwz(Rscratch1, in_bytes(MethodCounters::invoke_mask_offset()), R3_counters);\n+  __ addi(Rscratch2, Rscratch2, increment);\n+  __ stw(Rscratch2, mo_ic_offs, R3_counters);\n+  __ and_(Rscratch1, Rscratch2, Rscratch1);\n+  __ beq(CCR0, *overflow);\n@@ -785,2 +751,1 @@\n-    __ bind(done);\n-  }\n+  __ bind(done);\n@@ -1270,1 +1235,1 @@\n-    generate_counter_incr(&invocation_counter_overflow, NULL, NULL);\n+    generate_counter_incr(&invocation_counter_overflow);\n@@ -1674,3 +1639,2 @@\n-  Label invocation_counter_overflow,\n-        profile_method,\n-        profile_method_continue;\n+  Label invocation_counter_overflow;\n+  Label continue_after_compile;\n@@ -1698,1 +1662,1 @@\n-      generate_counter_incr(&invocation_counter_overflow, &profile_method, &profile_method_continue);\n+      generate_counter_incr(&invocation_counter_overflow);\n@@ -1701,1 +1665,1 @@\n-    __ bind(profile_method_continue);\n+    __ bind(continue_after_compile);\n@@ -1741,9 +1705,0 @@\n-  \/\/ Out of line counter overflow and MDO creation code.\n-  if (ProfileInterpreter) {\n-    \/\/ We have decided to profile this method in the interpreter.\n-    __ bind(profile_method);\n-    __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::profile_method));\n-    __ set_method_data_pointer_for_bcp();\n-    __ b(profile_method_continue);\n-  }\n-\n@@ -1753,1 +1708,1 @@\n-    generate_counter_overflow(profile_method_continue);\n+    generate_counter_overflow(continue_after_compile);\n","filename":"src\/hotspot\/cpu\/ppc\/templateInterpreterGenerator_ppc.cpp","additions":35,"deletions":80,"binary":false,"changes":115,"status":"modified"},{"patch":"@@ -1692,31 +1692,14 @@\n-    if (TieredCompilation) {\n-      Label Lno_mdo, Loverflow;\n-      const int increment = InvocationCounter::count_increment;\n-      if (ProfileInterpreter) {\n-        Register Rmdo = Rscratch1;\n-\n-        \/\/ If no method data exists, go to profile_continue.\n-        __ ld(Rmdo, in_bytes(Method::method_data_offset()), R19_method);\n-        __ cmpdi(CCR0, Rmdo, 0);\n-        __ beq(CCR0, Lno_mdo);\n-\n-        \/\/ Increment backedge counter in the MDO.\n-        const int mdo_bc_offs = in_bytes(MethodData::backedge_counter_offset()) + in_bytes(InvocationCounter::counter_offset());\n-        __ lwz(Rscratch2, mdo_bc_offs, Rmdo);\n-        __ lwz(Rscratch3, in_bytes(MethodData::backedge_mask_offset()), Rmdo);\n-        __ addi(Rscratch2, Rscratch2, increment);\n-        __ stw(Rscratch2, mdo_bc_offs, Rmdo);\n-        if (UseOnStackReplacement) {\n-          __ and_(Rscratch3, Rscratch2, Rscratch3);\n-          __ bne(CCR0, Lforward);\n-          __ b(Loverflow);\n-        } else {\n-          __ b(Lforward);\n-        }\n-      }\n-\n-      \/\/ If there's no MDO, increment counter in method.\n-      const int mo_bc_offs = in_bytes(MethodCounters::backedge_counter_offset()) + in_bytes(InvocationCounter::counter_offset());\n-      __ bind(Lno_mdo);\n-      __ lwz(Rscratch2, mo_bc_offs, R4_counters);\n-      __ lwz(Rscratch3, in_bytes(MethodCounters::backedge_mask_offset()), R4_counters);\n+    Label Lno_mdo, Loverflow;\n+    const int increment = InvocationCounter::count_increment;\n+    if (ProfileInterpreter) {\n+      Register Rmdo = Rscratch1;\n+\n+      \/\/ If no method data exists, go to profile_continue.\n+      __ ld(Rmdo, in_bytes(Method::method_data_offset()), R19_method);\n+      __ cmpdi(CCR0, Rmdo, 0);\n+      __ beq(CCR0, Lno_mdo);\n+\n+      \/\/ Increment backedge counter in the MDO.\n+      const int mdo_bc_offs = in_bytes(MethodData::backedge_counter_offset()) + in_bytes(InvocationCounter::counter_offset());\n+      __ lwz(Rscratch2, mdo_bc_offs, Rmdo);\n+      __ lwz(Rscratch3, in_bytes(MethodData::backedge_mask_offset()), Rmdo);\n@@ -1724,1 +1707,1 @@\n-      __ stw(Rscratch2, mo_bc_offs, R4_counters);\n+      __ stw(Rscratch2, mdo_bc_offs, Rmdo);\n@@ -1728,0 +1711,1 @@\n+        __ b(Loverflow);\n@@ -1731,9 +1715,1 @@\n-      __ bind(Loverflow);\n-\n-      \/\/ Notify point for loop, pass branch bytecode.\n-      __ subf(R4_ARG2, Rdisp, R14_bcp); \/\/ Compute branch bytecode (previous bcp).\n-      __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::frequency_counter_overflow), R4_ARG2, true);\n-\n-      \/\/ Was an OSR adapter generated?\n-      __ cmpdi(CCR0, R3_RET, 0);\n-      __ beq(CCR0, Lforward);\n+    }\n@@ -1741,3 +1717,9 @@\n-      \/\/ Has the nmethod been invalidated already?\n-      __ lbz(R0, nmethod::state_offset(), R3_RET);\n-      __ cmpwi(CCR0, R0, nmethod::in_use);\n+    \/\/ If there's no MDO, increment counter in method.\n+    const int mo_bc_offs = in_bytes(MethodCounters::backedge_counter_offset()) + in_bytes(InvocationCounter::counter_offset());\n+    __ bind(Lno_mdo);\n+    __ lwz(Rscratch2, mo_bc_offs, R4_counters);\n+    __ lwz(Rscratch3, in_bytes(MethodCounters::backedge_mask_offset()), R4_counters);\n+    __ addi(Rscratch2, Rscratch2, increment);\n+    __ stw(Rscratch2, mo_bc_offs, R4_counters);\n+    if (UseOnStackReplacement) {\n+      __ and_(Rscratch3, Rscratch2, Rscratch3);\n@@ -1745,21 +1727,0 @@\n-\n-      \/\/ Migrate the interpreter frame off of the stack.\n-      \/\/ We can use all registers because we will not return to interpreter from this point.\n-\n-      \/\/ Save nmethod.\n-      const Register osr_nmethod = R31;\n-      __ mr(osr_nmethod, R3_RET);\n-      __ set_top_ijava_frame_at_SP_as_last_Java_frame(R1_SP, R11_scratch1);\n-      __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::OSR_migration_begin), R16_thread);\n-      __ reset_last_Java_frame();\n-      \/\/ OSR buffer is in ARG1.\n-\n-      \/\/ Remove the interpreter frame.\n-      __ merge_frames(\/*top_frame_sp*\/ R21_sender_SP, \/*return_pc*\/ R0, R11_scratch1, R12_scratch2);\n-\n-      \/\/ Jump to the osr code.\n-      __ ld(R11_scratch1, nmethod::osr_entry_point_offset(), osr_nmethod);\n-      __ mtlr(R0);\n-      __ mtctr(R11_scratch1);\n-      __ bctr();\n-\n@@ -1767,15 +1728,1 @@\n-\n-      const Register invoke_ctr = Rscratch1;\n-      \/\/ Update Backedge branch separately from invocations.\n-      __ increment_backedge_counter(R4_counters, invoke_ctr, Rscratch2, Rscratch3);\n-\n-      if (ProfileInterpreter) {\n-        __ test_invocation_counter_for_mdp(invoke_ctr, R4_counters, Rscratch2, Lforward);\n-        if (UseOnStackReplacement) {\n-          __ test_backedge_count_for_osr(bumped_count, R4_counters, R14_bcp, Rdisp, Rscratch2);\n-        }\n-      } else {\n-        if (UseOnStackReplacement) {\n-          __ test_backedge_count_for_osr(invoke_ctr, R4_counters, R14_bcp, Rdisp, Rscratch2);\n-        }\n-      }\n+      __ b(Lforward);\n@@ -1783,0 +1730,34 @@\n+    __ bind(Loverflow);\n+\n+    \/\/ Notify point for loop, pass branch bytecode.\n+    __ subf(R4_ARG2, Rdisp, R14_bcp); \/\/ Compute branch bytecode (previous bcp).\n+    __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::frequency_counter_overflow), R4_ARG2, true);\n+\n+    \/\/ Was an OSR adapter generated?\n+    __ cmpdi(CCR0, R3_RET, 0);\n+    __ beq(CCR0, Lforward);\n+\n+    \/\/ Has the nmethod been invalidated already?\n+    __ lbz(R0, nmethod::state_offset(), R3_RET);\n+    __ cmpwi(CCR0, R0, nmethod::in_use);\n+    __ bne(CCR0, Lforward);\n+\n+    \/\/ Migrate the interpreter frame off of the stack.\n+    \/\/ We can use all registers because we will not return to interpreter from this point.\n+\n+    \/\/ Save nmethod.\n+    const Register osr_nmethod = R31;\n+    __ mr(osr_nmethod, R3_RET);\n+    __ set_top_ijava_frame_at_SP_as_last_Java_frame(R1_SP, R11_scratch1);\n+    __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::OSR_migration_begin), R16_thread);\n+    __ reset_last_Java_frame();\n+    \/\/ OSR buffer is in ARG1.\n+\n+    \/\/ Remove the interpreter frame.\n+    __ merge_frames(\/*top_frame_sp*\/ R21_sender_SP, \/*return_pc*\/ R0, R11_scratch1, R12_scratch2);\n+\n+    \/\/ Jump to the osr code.\n+    __ ld(R11_scratch1, nmethod::osr_entry_point_offset(), osr_nmethod);\n+    __ mtlr(R0);\n+    __ mtctr(R11_scratch1);\n+    __ bctr();\n","filename":"src\/hotspot\/cpu\/ppc\/templateTable_ppc_64.cpp","additions":61,"deletions":80,"binary":false,"changes":141,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -35,1 +35,1 @@\n-#ifndef TIERED\n+#ifndef COMPILER2\n@@ -59,1 +59,1 @@\n-#endif \/\/ !TIERED\n+#endif \/\/ !COMPILER2\n","filename":"src\/hotspot\/cpu\/s390\/c1_globals_s390.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -42,1 +42,1 @@\n-define_pd_global(bool, TieredCompilation,            trueInTiered);\n+define_pd_global(bool, TieredCompilation,            COMPILER1_PRESENT(true) NOT_COMPILER1(false));\n","filename":"src\/hotspot\/cpu\/s390\/c2_globals_s390.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -728,1 +728,1 @@\n-void TemplateInterpreterGenerator::generate_counter_incr(Label* overflow, Label* profile_method, Label* profile_method_continue) {\n+void TemplateInterpreterGenerator::generate_counter_incr(Label* overflow) {\n@@ -737,28 +737,12 @@\n-  if (TieredCompilation) {\n-    int increment = InvocationCounter::count_increment;\n-    if (ProfileInterpreter) {\n-      NearLabel no_mdo;\n-      Register mdo = m_counters;\n-      \/\/ Are we profiling?\n-      __ load_and_test_long(mdo, method2_(method, method_data));\n-      __ branch_optimized(Assembler::bcondZero, no_mdo);\n-      \/\/ Increment counter in the MDO.\n-      const Address mdo_invocation_counter(mdo, MethodData::invocation_counter_offset() +\n-                                           InvocationCounter::counter_offset());\n-      const Address mask(mdo, MethodData::invoke_mask_offset());\n-      __ increment_mask_and_jump(mdo_invocation_counter, increment, mask,\n-                                 Z_R1_scratch, false, Assembler::bcondZero,\n-                                 overflow);\n-      __ z_bru(done);\n-      __ bind(no_mdo);\n-    }\n-\n-    \/\/ Increment counter in MethodCounters.\n-    const Address invocation_counter(m_counters,\n-                                     MethodCounters::invocation_counter_offset() +\n-                                     InvocationCounter::counter_offset());\n-    \/\/ Get address of MethodCounters object.\n-    __ get_method_counters(method, m_counters, done);\n-    const Address mask(m_counters, MethodCounters::invoke_mask_offset());\n-    __ increment_mask_and_jump(invocation_counter,\n-                               increment, mask,\n+  int increment = InvocationCounter::count_increment;\n+  if (ProfileInterpreter) {\n+    NearLabel no_mdo;\n+    Register mdo = m_counters;\n+    \/\/ Are we profiling?\n+    __ load_and_test_long(mdo, method2_(method, method_data));\n+    __ branch_optimized(Assembler::bcondZero, no_mdo);\n+    \/\/ Increment counter in the MDO.\n+    const Address mdo_invocation_counter(mdo, MethodData::invocation_counter_offset() +\n+                                         InvocationCounter::counter_offset());\n+    const Address mask(mdo, MethodData::invoke_mask_offset());\n+    __ increment_mask_and_jump(mdo_invocation_counter, increment, mask,\n@@ -767,31 +751,2 @@\n-  } else {\n-    Register counter_sum = Z_ARG3; \/\/ The result of this piece of code.\n-    Register tmp         = Z_R1_scratch;\n-#ifdef ASSERT\n-    {\n-      NearLabel ok;\n-      __ get_method(tmp);\n-      __ compare64_and_branch(method, tmp, Assembler::bcondEqual, ok);\n-      __ z_illtrap(0x66);\n-      __ bind(ok);\n-    }\n-#endif\n-\n-    \/\/ Get address of MethodCounters object.\n-    __ get_method_counters(method, m_counters, done);\n-    \/\/ Update standard invocation counters.\n-    __ increment_invocation_counter(m_counters, counter_sum);\n-    if (ProfileInterpreter) {\n-      __ add2mem_32(Address(m_counters, MethodCounters::interpreter_invocation_counter_offset()), 1, tmp);\n-      if (profile_method != NULL) {\n-        const Address profile_limit(m_counters, MethodCounters::interpreter_profile_limit_offset());\n-        __ z_cl(counter_sum, profile_limit);\n-        __ branch_optimized(Assembler::bcondLow, *profile_method_continue);\n-        \/\/ If no method data exists, go to profile_method.\n-        __ test_method_data_pointer(tmp, *profile_method);\n-      }\n-    }\n-\n-    const Address invocation_limit(m_counters, MethodCounters::interpreter_invocation_limit_offset());\n-    __ z_cl(counter_sum, invocation_limit);\n-    __ branch_optimized(Assembler::bcondNotLow, *overflow);\n+    __ z_bru(done);\n+    __ bind(no_mdo);\n@@ -800,0 +755,12 @@\n+  \/\/ Increment counter in MethodCounters.\n+  const Address invocation_counter(m_counters,\n+                                   MethodCounters::invocation_counter_offset() +\n+                                   InvocationCounter::counter_offset());\n+  \/\/ Get address of MethodCounters object.\n+  __ get_method_counters(method, m_counters, done);\n+  const Address mask(m_counters, MethodCounters::invoke_mask_offset());\n+  __ increment_mask_and_jump(invocation_counter,\n+                             increment, mask,\n+                             Z_R1_scratch, false, Assembler::bcondZero,\n+                             overflow);\n+\n@@ -1406,1 +1373,1 @@\n-    generate_counter_incr(&invocation_counter_overflow, NULL, NULL);\n+    generate_counter_incr(&invocation_counter_overflow);\n@@ -1778,2 +1745,0 @@\n-  NearLabel profile_method;\n-  NearLabel profile_method_continue;\n@@ -1782,4 +1747,1 @@\n-    generate_counter_incr(&invocation_counter_overflow, &profile_method, &profile_method_continue);\n-    if (ProfileInterpreter) {\n-      __ bind(profile_method_continue);\n-    }\n+    generate_counter_incr(&invocation_counter_overflow);\n@@ -1830,9 +1792,0 @@\n-    if (ProfileInterpreter) {\n-      \/\/ We have decided to profile this method in the interpreter.\n-      __ bind(profile_method);\n-\n-      __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::profile_method));\n-      __ set_method_data_pointer_for_bcp();\n-      __ z_bru(profile_method_continue);\n-    }\n-\n","filename":"src\/hotspot\/cpu\/s390\/templateInterpreterGenerator_s390.cpp","additions":30,"deletions":77,"binary":false,"changes":107,"status":"modified"},{"patch":"@@ -1914,1 +1914,0 @@\n-  NearLabel profile_method;\n@@ -1927,2 +1926,0 @@\n-    if (TieredCompilation) {\n-      Label noCounters;\n@@ -1930,2 +1927,2 @@\n-      if (ProfileInterpreter) {\n-        NearLabel   no_mdo;\n+    if (ProfileInterpreter) {\n+      NearLabel   no_mdo;\n@@ -1933,3 +1930,3 @@\n-        \/\/ Are we profiling?\n-        __ load_and_test_long(mdo, Address(method, Method::method_data_offset()));\n-        __ branch_optimized(Assembler::bcondZero, no_mdo);\n+      \/\/ Are we profiling?\n+      __ load_and_test_long(mdo, Address(method, Method::method_data_offset()));\n+      __ branch_optimized(Assembler::bcondZero, no_mdo);\n@@ -1937,2 +1934,2 @@\n-        \/\/ Increment the MDO backedge counter.\n-        const Address mdo_backedge_counter(mdo, MethodData::backedge_counter_offset() + InvocationCounter::counter_offset());\n+      \/\/ Increment the MDO backedge counter.\n+      const Address mdo_backedge_counter(mdo, MethodData::backedge_counter_offset() + InvocationCounter::counter_offset());\n@@ -1940,13 +1937,2 @@\n-        const Address mask(mdo, MethodData::backedge_mask_offset());\n-        __ increment_mask_and_jump(mdo_backedge_counter, increment, mask,\n-                                   Z_ARG2, false, Assembler::bcondZero,\n-                                   UseOnStackReplacement ? &backedge_counter_overflow : NULL);\n-        __ z_bru(dispatch);\n-        __ bind(no_mdo);\n-      }\n-\n-      \/\/ Increment backedge counter in MethodCounters*.\n-      __ get_method_counters(method, m_counters, noCounters);\n-      const Address mask(m_counters, MethodCounters::backedge_mask_offset());\n-      __ increment_mask_and_jump(Address(m_counters, be_offset),\n-                                 increment, mask,\n+      const Address mask(mdo, MethodData::backedge_mask_offset());\n+      __ increment_mask_and_jump(mdo_backedge_counter, increment, mask,\n@@ -1955,42 +1941,2 @@\n-      __ bind(noCounters);\n-    } else {\n-      Register counter = Z_tos;\n-      Label    noCounters;\n-      \/\/ Get address of MethodCounters object.\n-      __ get_method_counters(method, m_counters, noCounters);\n-      \/\/ Increment backedge counter.\n-      __ increment_backedge_counter(m_counters, counter);\n-\n-      if (ProfileInterpreter) {\n-        \/\/ Test to see if we should create a method data obj.\n-        __ z_cl(counter, Address(m_counters, MethodCounters::interpreter_profile_limit_offset()));\n-        __ z_brl(dispatch);\n-\n-        \/\/ If no method data exists, go to profile method.\n-        __ test_method_data_pointer(Z_ARG4\/*result unused*\/, profile_method);\n-\n-        if (UseOnStackReplacement) {\n-          \/\/ Check for overflow against 'bumped_count' which is the MDO taken count.\n-          __ z_cl(bumped_count, Address(m_counters, MethodCounters::interpreter_backward_branch_limit_offset()));\n-          __ z_brl(dispatch);\n-\n-          \/\/ When ProfileInterpreter is on, the backedge_count comes\n-          \/\/ from the methodDataOop, which value does not get reset on\n-          \/\/ the call to frequency_counter_overflow(). To avoid\n-          \/\/ excessive calls to the overflow routine while the method is\n-          \/\/ being compiled, add a second test to make sure the overflow\n-          \/\/ function is called only once every overflow_frequency.\n-          const int overflow_frequency = 1024;\n-          __ and_imm(bumped_count, overflow_frequency - 1);\n-          __ z_brz(backedge_counter_overflow);\n-\n-        }\n-      } else {\n-        if (UseOnStackReplacement) {\n-          \/\/ Check for overflow against 'counter', which is the sum of the\n-          \/\/ counters.\n-          __ z_cl(counter, Address(m_counters, MethodCounters::interpreter_backward_branch_limit_offset()));\n-          __ z_brh(backedge_counter_overflow);\n-        }\n-      }\n-      __ bind(noCounters);\n+      __ z_bru(dispatch);\n+      __ bind(no_mdo);\n@@ -1999,0 +1945,7 @@\n+    \/\/ Increment backedge counter in MethodCounters*.\n+    __ get_method_counters(method, m_counters, dispatch);\n+    const Address mask(m_counters, MethodCounters::backedge_mask_offset());\n+    __ increment_mask_and_jump(Address(m_counters, be_offset),\n+                               increment, mask,\n+                               Z_ARG2, false, Assembler::bcondZero,\n+                               UseOnStackReplacement ? &backedge_counter_overflow : NULL);\n@@ -2012,13 +1965,3 @@\n-  if (UseLoopCounter) {\n-    if (ProfileInterpreter && !TieredCompilation) {\n-      \/\/ Out-of-line code to allocate method data oop.\n-      __ bind(profile_method);\n-\n-      __ call_VM(noreg,\n-                 CAST_FROM_FN_PTR(address, InterpreterRuntime::profile_method));\n-      __ z_llgc(Z_bytecode, Address(Z_bcp, (intptr_t) 0));  \/\/ Restore target bytecode.\n-      __ set_method_data_pointer_for_bcp();\n-      __ z_bru(dispatch);\n-    }\n-\n-    if (UseOnStackReplacement) {\n+  if (UseLoopCounter && UseOnStackReplacement) {\n+    \/\/ invocation counter overflow\n+    __ bind(backedge_counter_overflow);\n@@ -2026,2 +1969,5 @@\n-      \/\/ invocation counter overflow\n-      __ bind(backedge_counter_overflow);\n+    __ z_lcgr(Z_ARG2, disp); \/\/ Z_ARG2 := -disp\n+    __ z_agr(Z_ARG2, Z_bcp); \/\/ Z_ARG2 := branch target bcp - disp == branch bcp\n+    __ call_VM(noreg,\n+               CAST_FROM_FN_PTR(address, InterpreterRuntime::frequency_counter_overflow),\n+               Z_ARG2);\n@@ -2029,5 +1975,2 @@\n-      __ z_lcgr(Z_ARG2, disp); \/\/ Z_ARG2 := -disp\n-      __ z_agr(Z_ARG2, Z_bcp); \/\/ Z_ARG2 := branch target bcp - disp == branch bcp\n-      __ call_VM(noreg,\n-                 CAST_FROM_FN_PTR(address, InterpreterRuntime::frequency_counter_overflow),\n-                 Z_ARG2);\n+    \/\/ Z_RET: osr nmethod (osr ok) or NULL (osr not possible).\n+    __ compare64_and_branch(Z_RET, (intptr_t) 0, Assembler::bcondEqual, dispatch);\n@@ -2035,2 +1978,3 @@\n-      \/\/ Z_RET: osr nmethod (osr ok) or NULL (osr not possible).\n-      __ compare64_and_branch(Z_RET, (intptr_t) 0, Assembler::bcondEqual, dispatch);\n+    \/\/ Nmethod may have been invalidated (VM may block upon call_VM return).\n+    __ z_cliy(nmethod::state_offset(), Z_RET, nmethod::in_use);\n+    __ z_brne(dispatch);\n@@ -2038,3 +1982,1 @@\n-      \/\/ Nmethod may have been invalidated (VM may block upon call_VM return).\n-      __ z_cliy(nmethod::state_offset(), Z_RET, nmethod::in_use);\n-      __ z_brne(dispatch);\n+    \/\/ Migrate the interpreter frame off of the stack.\n@@ -2042,1 +1984,1 @@\n-      \/\/ Migrate the interpreter frame off of the stack.\n+    __ z_lgr(Z_tmp_1, Z_RET); \/\/ Save the nmethod.\n@@ -2044,1 +1986,2 @@\n-      __ z_lgr(Z_tmp_1, Z_RET); \/\/ Save the nmethod.\n+    call_VM(noreg,\n+            CAST_FROM_FN_PTR(address, SharedRuntime::OSR_migration_begin));\n@@ -2046,2 +1989,2 @@\n-      call_VM(noreg,\n-              CAST_FROM_FN_PTR(address, SharedRuntime::OSR_migration_begin));\n+    \/\/ Z_RET is OSR buffer, move it to expected parameter location.\n+    __ lgr_if_needed(Z_ARG1, Z_RET);\n@@ -2049,2 +1992,2 @@\n-      \/\/ Z_RET is OSR buffer, move it to expected parameter location.\n-      __ lgr_if_needed(Z_ARG1, Z_RET);\n+    \/\/ Pop the interpreter frame ...\n+    __ pop_interpreter_frame(Z_R14, Z_ARG2\/*tmp1*\/, Z_ARG3\/*tmp2*\/);\n@@ -2052,7 +1995,3 @@\n-      \/\/ Pop the interpreter frame ...\n-      __ pop_interpreter_frame(Z_R14, Z_ARG2\/*tmp1*\/, Z_ARG3\/*tmp2*\/);\n-\n-      \/\/ ... and begin the OSR nmethod.\n-      __ z_lg(Z_R1_scratch, Address(Z_tmp_1, nmethod::osr_entry_point_offset()));\n-      __ z_br(Z_R1_scratch);\n-    }\n+    \/\/ ... and begin the OSR nmethod.\n+    __ z_lg(Z_R1_scratch, Address(Z_tmp_1, nmethod::osr_entry_point_offset()));\n+    __ z_br(Z_R1_scratch);\n","filename":"src\/hotspot\/cpu\/s390\/templateTable_s390.cpp","additions":42,"deletions":103,"binary":false,"changes":145,"status":"modified"},{"patch":"@@ -339,2 +339,2 @@\n-#if !defined(_LP64) && defined(TIERED)\n-  if (UseSSE < 2 ) {\n+#if !defined(_LP64) && defined(COMPILER2)\n+  if (UseSSE < 2 && !CompilerConfig::is_c1_only_no_aot_or_jvmci()) {\n@@ -344,1 +344,1 @@\n-#endif \/\/ !_LP64 && TIERED\n+#endif \/\/ !_LP64 && COMPILER2\n","filename":"src\/hotspot\/cpu\/x86\/c1_MacroAssembler_x86.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -723,2 +723,2 @@\n-#if !defined(_LP64) && defined(TIERED)\n-  if (UseSSE < 2) {\n+#if !defined(_LP64) && defined(COMPILER2)\n+  if (UseSSE < 2 && !CompilerConfig::is_c1_only_no_aot_or_jvmci()) {\n@@ -728,1 +728,1 @@\n-#endif \/\/ !_LP64 && TIERED\n+#endif \/\/ !_LP64 && COMPILER2\n","filename":"src\/hotspot\/cpu\/x86\/c1_Runtime1_x86.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -34,1 +34,1 @@\n-#ifndef TIERED\n+#ifndef COMPILER2\n@@ -58,1 +58,1 @@\n-#endif \/\/ !TIERED\n+#endif \/\/ !COMPILER2\n","filename":"src\/hotspot\/cpu\/x86\/c1_globals_x86.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -40,1 +40,1 @@\n-define_pd_global(bool, TieredCompilation,            trueInTiered);\n+define_pd_global(bool, TieredCompilation,            COMPILER1_PRESENT(true) NOT_COMPILER1(false));\n","filename":"src\/hotspot\/cpu\/x86\/c2_globals_x86.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -39,1 +39,1 @@\n-#if defined(TIERED)\n+#if COMPILER1_AND_COMPILER2\n","filename":"src\/hotspot\/cpu\/x86\/globalDefinitions_x86.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -38,1 +38,1 @@\n-define_pd_global(uintx, CodeCacheSegmentSize,    64 TIERED_ONLY(+64)); \/\/ Tiered compilation has large code-entry alignment.\n+define_pd_global(uintx, CodeCacheSegmentSize,    64 COMPILER1_AND_COMPILER2_PRESENT(+64)); \/\/ Tiered compilation has large code-entry alignment.\n","filename":"src\/hotspot\/cpu\/x86\/globals_x86.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -388,4 +388,1 @@\n-void TemplateInterpreterGenerator::generate_counter_incr(\n-        Label* overflow,\n-        Label* profile_method,\n-        Label* profile_method_continue) {\n+void TemplateInterpreterGenerator::generate_counter_incr(Label* overflow) {\n@@ -394,66 +391,13 @@\n-  if (TieredCompilation) {\n-    int increment = InvocationCounter::count_increment;\n-    Label no_mdo;\n-    if (ProfileInterpreter) {\n-      \/\/ Are we profiling?\n-      __ movptr(rax, Address(rbx, Method::method_data_offset()));\n-      __ testptr(rax, rax);\n-      __ jccb(Assembler::zero, no_mdo);\n-      \/\/ Increment counter in the MDO\n-      const Address mdo_invocation_counter(rax, in_bytes(MethodData::invocation_counter_offset()) +\n-                                                in_bytes(InvocationCounter::counter_offset()));\n-      const Address mask(rax, in_bytes(MethodData::invoke_mask_offset()));\n-      __ increment_mask_and_jump(mdo_invocation_counter, increment, mask, rcx, false, Assembler::zero, overflow);\n-      __ jmp(done);\n-    }\n-    __ bind(no_mdo);\n-    \/\/ Increment counter in MethodCounters\n-    const Address invocation_counter(rax,\n-                  MethodCounters::invocation_counter_offset() +\n-                  InvocationCounter::counter_offset());\n-    __ get_method_counters(rbx, rax, done);\n-    const Address mask(rax, in_bytes(MethodCounters::invoke_mask_offset()));\n-    __ increment_mask_and_jump(invocation_counter, increment, mask, rcx,\n-                               false, Assembler::zero, overflow);\n-    __ bind(done);\n-  } else { \/\/ not TieredCompilation\n-    const Address backedge_counter(rax,\n-                  MethodCounters::backedge_counter_offset() +\n-                  InvocationCounter::counter_offset());\n-    const Address invocation_counter(rax,\n-                  MethodCounters::invocation_counter_offset() +\n-                  InvocationCounter::counter_offset());\n-\n-    __ get_method_counters(rbx, rax, done);\n-\n-    if (ProfileInterpreter) {\n-      __ incrementl(Address(rax,\n-              MethodCounters::interpreter_invocation_counter_offset()));\n-    }\n-    \/\/ Update standard invocation counters\n-    __ movl(rcx, invocation_counter);\n-    __ incrementl(rcx, InvocationCounter::count_increment);\n-    __ movl(invocation_counter, rcx); \/\/ save invocation count\n-\n-    __ movl(rax, backedge_counter);   \/\/ load backedge counter\n-    __ andl(rax, InvocationCounter::count_mask_value); \/\/ mask out the status bits\n-\n-    __ addl(rcx, rax);                \/\/ add both counters\n-\n-    \/\/ profile_method is non-null only for interpreted method so\n-    \/\/ profile_method != NULL == !native_call\n-\n-    if (ProfileInterpreter && profile_method != NULL) {\n-      \/\/ Test to see if we should create a method data oop\n-      __ movptr(rax, Address(rbx, Method::method_counters_offset()));\n-      __ cmp32(rcx, Address(rax, in_bytes(MethodCounters::interpreter_profile_limit_offset())));\n-      __ jcc(Assembler::less, *profile_method_continue);\n-\n-      \/\/ if no method data exists, go to profile_method\n-      __ test_method_data_pointer(rax, *profile_method);\n-    }\n-\n-    __ movptr(rax, Address(rbx, Method::method_counters_offset()));\n-    __ cmp32(rcx, Address(rax, in_bytes(MethodCounters::interpreter_invocation_limit_offset())));\n-    __ jcc(Assembler::aboveEqual, *overflow);\n-    __ bind(done);\n+  int increment = InvocationCounter::count_increment;\n+  Label no_mdo;\n+  if (ProfileInterpreter) {\n+    \/\/ Are we profiling?\n+    __ movptr(rax, Address(rbx, Method::method_data_offset()));\n+    __ testptr(rax, rax);\n+    __ jccb(Assembler::zero, no_mdo);\n+    \/\/ Increment counter in the MDO\n+    const Address mdo_invocation_counter(rax, in_bytes(MethodData::invocation_counter_offset()) +\n+        in_bytes(InvocationCounter::counter_offset()));\n+    const Address mask(rax, in_bytes(MethodData::invoke_mask_offset()));\n+    __ increment_mask_and_jump(mdo_invocation_counter, increment, mask, rcx, false, Assembler::zero, overflow);\n+    __ jmp(done);\n@@ -461,0 +405,10 @@\n+  __ bind(no_mdo);\n+  \/\/ Increment counter in MethodCounters\n+  const Address invocation_counter(rax,\n+      MethodCounters::invocation_counter_offset() +\n+      InvocationCounter::counter_offset());\n+  __ get_method_counters(rbx, rax, done);\n+  const Address mask(rax, in_bytes(MethodCounters::invoke_mask_offset()));\n+  __ increment_mask_and_jump(invocation_counter, increment, mask, rcx,\n+      false, Assembler::zero, overflow);\n+  __ bind(done);\n@@ -864,1 +818,1 @@\n-    generate_counter_incr(&invocation_counter_overflow, NULL, NULL);\n+    generate_counter_incr(&invocation_counter_overflow);\n@@ -1414,2 +1368,0 @@\n-  Label profile_method;\n-  Label profile_method_continue;\n@@ -1417,6 +1369,1 @@\n-    generate_counter_incr(&invocation_counter_overflow,\n-                          &profile_method,\n-                          &profile_method_continue);\n-    if (ProfileInterpreter) {\n-      __ bind(profile_method_continue);\n-    }\n+    generate_counter_incr(&invocation_counter_overflow);\n@@ -1476,8 +1423,0 @@\n-    if (ProfileInterpreter) {\n-      \/\/ We have decided to profile this method in the interpreter\n-      __ bind(profile_method);\n-      __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::profile_method));\n-      __ set_method_data_pointer_for_bcp();\n-      __ get_method(rbx);\n-      __ jmp(profile_method_continue);\n-    }\n","filename":"src\/hotspot\/cpu\/x86\/templateInterpreterGenerator_x86.cpp","additions":27,"deletions":88,"binary":false,"changes":115,"status":"modified"},{"patch":"@@ -2190,1 +2190,0 @@\n-  Label profile_method;\n@@ -2219,68 +2218,14 @@\n-    if (TieredCompilation) {\n-      Label no_mdo;\n-      int increment = InvocationCounter::count_increment;\n-      if (ProfileInterpreter) {\n-        \/\/ Are we profiling?\n-        __ movptr(rbx, Address(rcx, in_bytes(Method::method_data_offset())));\n-        __ testptr(rbx, rbx);\n-        __ jccb(Assembler::zero, no_mdo);\n-        \/\/ Increment the MDO backedge counter\n-        const Address mdo_backedge_counter(rbx, in_bytes(MethodData::backedge_counter_offset()) +\n-                                           in_bytes(InvocationCounter::counter_offset()));\n-        const Address mask(rbx, in_bytes(MethodData::backedge_mask_offset()));\n-        __ increment_mask_and_jump(mdo_backedge_counter, increment, mask, rax, false, Assembler::zero,\n-                                   UseOnStackReplacement ? &backedge_counter_overflow : NULL);\n-        __ jmp(dispatch);\n-      }\n-      __ bind(no_mdo);\n-      \/\/ Increment backedge counter in MethodCounters*\n-      __ movptr(rcx, Address(rcx, Method::method_counters_offset()));\n-      const Address mask(rcx, in_bytes(MethodCounters::backedge_mask_offset()));\n-      __ increment_mask_and_jump(Address(rcx, be_offset), increment, mask,\n-                                 rax, false, Assembler::zero,\n-                                 UseOnStackReplacement ? &backedge_counter_overflow : NULL);\n-    } else { \/\/ not TieredCompilation\n-      \/\/ increment counter\n-      __ movptr(rcx, Address(rcx, Method::method_counters_offset()));\n-      __ movl(rax, Address(rcx, be_offset));        \/\/ load backedge counter\n-      __ incrementl(rax, InvocationCounter::count_increment); \/\/ increment counter\n-      __ movl(Address(rcx, be_offset), rax);        \/\/ store counter\n-\n-      __ movl(rax, Address(rcx, inv_offset));    \/\/ load invocation counter\n-\n-      __ andl(rax, InvocationCounter::count_mask_value); \/\/ and the status bits\n-      __ addl(rax, Address(rcx, be_offset));        \/\/ add both counters\n-\n-      if (ProfileInterpreter) {\n-        \/\/ Test to see if we should create a method data oop\n-        __ cmp32(rax, Address(rcx, in_bytes(MethodCounters::interpreter_profile_limit_offset())));\n-        __ jcc(Assembler::less, dispatch);\n-\n-        \/\/ if no method data exists, go to profile method\n-        __ test_method_data_pointer(rax, profile_method);\n-\n-        if (UseOnStackReplacement) {\n-          \/\/ check for overflow against rbx which is the MDO taken count\n-          __ cmp32(rbx, Address(rcx, in_bytes(MethodCounters::interpreter_backward_branch_limit_offset())));\n-          __ jcc(Assembler::below, dispatch);\n-\n-          \/\/ When ProfileInterpreter is on, the backedge_count comes\n-          \/\/ from the MethodData*, which value does not get reset on\n-          \/\/ the call to frequency_counter_overflow().  To avoid\n-          \/\/ excessive calls to the overflow routine while the method is\n-          \/\/ being compiled, add a second test to make sure the overflow\n-          \/\/ function is called only once every overflow_frequency.\n-          const int overflow_frequency = 1024;\n-          __ andl(rbx, overflow_frequency - 1);\n-          __ jcc(Assembler::zero, backedge_counter_overflow);\n-\n-        }\n-      } else {\n-        if (UseOnStackReplacement) {\n-          \/\/ check for overflow against rax, which is the sum of the\n-          \/\/ counters\n-          __ cmp32(rax, Address(rcx, in_bytes(MethodCounters::interpreter_backward_branch_limit_offset())));\n-          __ jcc(Assembler::aboveEqual, backedge_counter_overflow);\n-\n-        }\n-      }\n+    Label no_mdo;\n+    int increment = InvocationCounter::count_increment;\n+    if (ProfileInterpreter) {\n+      \/\/ Are we profiling?\n+      __ movptr(rbx, Address(rcx, in_bytes(Method::method_data_offset())));\n+      __ testptr(rbx, rbx);\n+      __ jccb(Assembler::zero, no_mdo);\n+      \/\/ Increment the MDO backedge counter\n+      const Address mdo_backedge_counter(rbx, in_bytes(MethodData::backedge_counter_offset()) +\n+          in_bytes(InvocationCounter::counter_offset()));\n+      const Address mask(rbx, in_bytes(MethodData::backedge_mask_offset()));\n+      __ increment_mask_and_jump(mdo_backedge_counter, increment, mask, rax, false, Assembler::zero,\n+          UseOnStackReplacement ? &backedge_counter_overflow : NULL);\n+      __ jmp(dispatch);\n@@ -2288,0 +2233,6 @@\n+    __ bind(no_mdo);\n+    \/\/ Increment backedge counter in MethodCounters*\n+    __ movptr(rcx, Address(rcx, Method::method_counters_offset()));\n+    const Address mask(rcx, in_bytes(MethodCounters::backedge_mask_offset()));\n+    __ increment_mask_and_jump(Address(rcx, be_offset), increment, mask,\n+        rax, false, Assembler::zero, UseOnStackReplacement ? &backedge_counter_overflow : NULL);\n@@ -2301,8 +2252,0 @@\n-    if (ProfileInterpreter && !TieredCompilation) {\n-      \/\/ Out-of-line code to allocate method data oop.\n-      __ bind(profile_method);\n-      __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::profile_method));\n-      __ set_method_data_pointer_for_bcp();\n-      __ jmp(dispatch);\n-    }\n-\n@@ -2310,0 +2253,1 @@\n+      Label set_mdp;\n","filename":"src\/hotspot\/cpu\/x86\/templateTable_x86.cpp","additions":21,"deletions":77,"binary":false,"changes":98,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -1009,1 +1009,1 @@\n-    if (is_client_compilation_mode_vm()) {\n+    if (!CompilerConfig::is_c2_available()) {\n","filename":"src\/hotspot\/cpu\/x86\/vm_version_x86.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -39,1 +39,1 @@\n-define_pd_global(uintx, CodeCacheSegmentSize, 64 TIERED_ONLY(+64)); \/\/ Tiered compilation has large code-entry alignment.\n+define_pd_global(uintx, CodeCacheSegmentSize, 64 COMPILER1_AND_COMPILER2_PRESENT(+64)); \/\/ Tiered compilation has large code-entry alignment.\n","filename":"src\/hotspot\/cpu\/zero\/globals_zero.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -191,4 +191,0 @@\n-  if (!TieredCompilation && _config->_tieredAOT) {\n-    handle_config_error(\"Shared file %s error: Expected to run with tiered compilation on\", _name);\n-  }\n-\n@@ -356,1 +352,1 @@\n-#ifdef TIERED\n+#if COMPILER1_OR_COMPILER2\n@@ -772,1 +768,1 @@\n-  vmassert(aot->method()->code() != aot TIERED_ONLY( && aot->method()->aot_code() == NULL), \"method still active\");\n+  vmassert(aot->method()->code() != aot COMPILER1_OR_COMPILER2_PRESENT( && aot->method()->aot_code() == NULL), \"method still active\");\n","filename":"src\/hotspot\/share\/aot\/aotCodeHeap.cpp","additions":2,"deletions":6,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -183,1 +183,1 @@\n-#ifdef TIERED\n+#if COMPILER1_OR_COMPILER2\n@@ -188,1 +188,1 @@\n-#endif\n+#endif \/\/ COMPILER1_OR_COMPILER2\n@@ -206,1 +206,0 @@\n-#ifdef TIERED\n@@ -208,0 +207,1 @@\n+#if COMPILER1_OR_COMPILER2\n@@ -236,0 +236,3 @@\n+#else\n+  return false;\n+#endif \/\/ COMPILER1_OR_COMPILER2\n@@ -237,1 +240,0 @@\n-#endif \/\/ TIERED\n","filename":"src\/hotspot\/share\/aot\/aotCompiledMethod.cpp","additions":7,"deletions":5,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -206,1 +206,1 @@\n-  virtual bool make_entrant() NOT_TIERED({ ShouldNotReachHere(); return false; });\n+  virtual bool make_entrant();\n","filename":"src\/hotspot\/share\/aot\/aotCompiledMethod.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2016, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -586,1 +586,1 @@\n-    _env->record_method_not_compilable(bailout_msg(), !TieredCompilation);\n+    _env->record_method_not_compilable(bailout_msg());\n","filename":"src\/hotspot\/share\/c1\/c1_Compilation.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -268,2 +268,2 @@\n-  bool is_optimistic() const                             {\n-    return !TieredCompilation &&\n+  bool is_optimistic() {\n+    return CompilerConfig::is_c1_only_no_aot_or_jvmci() && !is_profiling() &&\n","filename":"src\/hotspot\/share\/c1\/c1_Compilation.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -3753,1 +3753,1 @@\n-  if (CompilationPolicy::policy()->should_not_inline(compilation()->env(), callee)) {\n+  if (CompilationPolicy::should_not_inline(compilation()->env(), callee)) {\n","filename":"src\/hotspot\/share\/c1\/c1_GraphBuilder.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -484,1 +484,1 @@\n-#if defined(IA32) && defined(TIERED)\n+#if defined(IA32) && defined(COMPILER2)\n@@ -486,1 +486,1 @@\n-  if (UseSSE < 2) {\n+  if (UseSSE < 2 && !CompilerConfig::is_c1_only_no_aot_or_jvmci()) {\n@@ -495,1 +495,1 @@\n-#endif \/\/ X86 && TIERED\n+#endif \/\/ IA32 && COMPILER2\n","filename":"src\/hotspot\/share\/c1\/c1_LIRAssembler.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2005, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2005, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -462,1 +462,1 @@\n-  \/* C2 relies on constant pool entries being resolved (ciTypeFlow), so if TieredCompilation\n+  \/* C2 relies on constant pool entries being resolved (ciTypeFlow), so if tiered compilation\n@@ -465,1 +465,1 @@\n-  if ((TieredCompilation && need_resolve) || !obj->is_loaded() || PatchALot) {\n+  if ((!CompilerConfig::is_c1_only_no_aot_or_jvmci() && need_resolve) || !obj->is_loaded() || PatchALot) {\n@@ -665,1 +665,1 @@\n-  } else if (PrintNotLoaded && (TieredCompilation && new_instance->is_unresolved())) {\n+  } else if (PrintNotLoaded && (!CompilerConfig::is_c1_only_no_aot_or_jvmci() && new_instance->is_unresolved())) {\n","filename":"src\/hotspot\/share\/c1\/c1_LIRGenerator.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -472,1 +472,1 @@\n-  osr_nm = CompilationPolicy::policy()->event(enclosing_method, method, branch_bci, bci, level, nm, THREAD);\n+  osr_nm = CompilationPolicy::event(enclosing_method, method, branch_bci, bci, level, nm, THREAD);\n@@ -1408,2 +1408,0 @@\n-  assert(!TieredCompilation, \"incompatible with tiered compilation\");\n-\n","filename":"src\/hotspot\/share\/c1\/c1_Runtime1.cpp","additions":2,"deletions":4,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -331,1 +331,1 @@\n-  product(bool, C1UpdateMethodData, trueInTiered,                           \\\n+  product(bool, C1UpdateMethodData, true,                                   \\\n","filename":"src\/hotspot\/share\/c1\/c1_globals.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -43,0 +43,1 @@\n+#include \"compiler\/compilationPolicy.hpp\"\n@@ -1120,1 +1121,1 @@\n-  if (task() == NULL)  return CompLevel_highest_tier;\n+  if (task() == NULL)  return CompilationPolicy::highest_compile_level();\n","filename":"src\/hotspot\/share\/ci\/ciEnv.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -456,1 +456,1 @@\n-  void record_method_not_compilable(const char* reason, bool all_tiers = true);\n+  void record_method_not_compilable(const char* reason, bool all_tiers = false);\n","filename":"src\/hotspot\/share\/ci\/ciEnv.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -142,1 +142,1 @@\n-  if (ProfileInterpreter || TieredCompilation) {\n+  if (ProfileInterpreter || CompilerConfig::is_c1_profiling()) {\n@@ -482,9 +482,7 @@\n-        if (TieredCompilation) {\n-          \/\/ For a call, it is assumed that either the type of the receiver(s)\n-          \/\/ is recorded or an associated counter is incremented, but not both. With\n-          \/\/ tiered compilation, however, both can happen due to the interpreter and\n-          \/\/ C1 profiling invocations differently. Address that inconsistency here.\n-          if (morphism == 1 && count > 0) {\n-            epsilon = count;\n-            count = 0;\n-          }\n+        \/\/ For a call, it is assumed that either the type of the receiver(s)\n+        \/\/ is recorded or an associated counter is incremented, but not both. With\n+        \/\/ tiered compilation, however, both can happen due to the interpreter and\n+        \/\/ C1 profiling invocations differently. Address that inconsistency here.\n+        if (morphism == 1 && count > 0) {\n+          epsilon = count;\n+          count = 0;\n@@ -884,8 +882,2 @@\n-    if (TieredCompilation) {\n-      \/\/ In tiered the MDO's life is measured directly, so just use the snapshotted counters\n-      counter_life = MAX2(method_data()->invocation_count(), method_data()->backedge_count());\n-    } else {\n-      int current_mileage = method_data()->current_mileage();\n-      int creation_mileage = method_data()->creation_mileage();\n-      counter_life = current_mileage - creation_mileage;\n-    }\n+    \/\/ In tiered the MDO's life is measured directly, so just use the snapshotted counters\n+    counter_life = MAX2(method_data()->invocation_count(), method_data()->backedge_count());\n","filename":"src\/hotspot\/share\/ci\/ciMethod.cpp","additions":11,"deletions":19,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2013, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2013, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -34,0 +34,1 @@\n+#include \"compiler\/compilationPolicy.hpp\"\n@@ -480,1 +481,1 @@\n-    } else if (!TieredCompilation && (comp_level != CompLevel_highest_tier)) {\n+    } else if (is_c1_compile(comp_level) && !CompilerConfig::is_c1_available()) {\n@@ -482,10 +483,4 @@\n-      switch (comp_level) {\n-        case CompLevel_simple:\n-          jio_snprintf(msg, msg_len, \"compilation level %d requires Client VM or TieredCompilation\", comp_level);\n-          break;\n-        case CompLevel_full_optimization:\n-          jio_snprintf(msg, msg_len, \"compilation level %d requires Server VM\", comp_level);\n-          break;\n-        default:\n-          jio_snprintf(msg, msg_len, \"compilation level %d requires TieredCompilation\", comp_level);\n-      }\n+      jio_snprintf(msg, msg_len, \"compilation level %d requires C1\", comp_level);\n+    } else if (is_c2_compile(comp_level) && !CompilerConfig::is_c2_available()) {\n+      msg = NEW_RESOURCE_ARRAY(char, msg_len);\n+      jio_snprintf(msg, msg_len, \"compilation level %d requires C2\", comp_level);\n@@ -540,5 +535,1 @@\n-      if (TieredCompilation) {\n-        comp_level = TieredStopAtLevel;\n-      } else {\n-        comp_level = CompLevel_highest_tier;\n-      }\n+      comp_level = CompilationPolicy::highest_compile_level();\n","filename":"src\/hotspot\/share\/ci\/ciReplay.cpp","additions":8,"deletions":17,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -198,1 +198,1 @@\n-  const int c1_count = CompilationPolicy::policy()->compiler_count(CompLevel_simple);\n+  const int c1_count = CompilationPolicy::c1_count();\n@@ -203,1 +203,1 @@\n-  const int c2_count = CompilationPolicy::policy()->compiler_count(CompLevel_full_optimization);\n+  const int c2_count = CompilationPolicy::c2_count();\n@@ -357,1 +357,1 @@\n-  } else if (TieredCompilation && (TieredStopAtLevel > CompLevel_simple)) {\n+  } else if (CompilerConfig::is_c1_profiling()) {\n","filename":"src\/hotspot\/share\/code\/codeCache.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -55,1 +55,1 @@\n-\/\/ Depending on the availability of compilers and TieredCompilation there\n+\/\/ Depending on the availability of compilers and compilation mode there\n@@ -63,1 +63,1 @@\n-\/\/ code cache segmentation is turned on if TieredCompilation is enabled and\n+\/\/ code cache segmentation is turned on if tiered mode is enabled and\n","filename":"src\/hotspot\/share\/code\/codeCache.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2010, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -26,3 +26,0 @@\n-#include \"classfile\/classLoaderDataGraph.inline.hpp\"\n-#include \"code\/compiledIC.hpp\"\n-#include \"code\/nmethod.hpp\"\n@@ -31,2 +28,2 @@\n-#include \"compiler\/tieredThresholdPolicy.hpp\"\n-#include \"interpreter\/interpreter.hpp\"\n+#include \"compiler\/compileBroker.hpp\"\n+#include \"compiler\/compilerOracle.hpp\"\n@@ -39,0 +36,2 @@\n+#include \"runtime\/arguments.hpp\"\n+#include \"runtime\/deoptimization.hpp\"\n@@ -40,0 +39,1 @@\n+#include \"runtime\/frame.inline.hpp\"\n@@ -42,6 +42,6 @@\n-#include \"runtime\/stubRoutines.hpp\"\n-#include \"runtime\/thread.hpp\"\n-#include \"runtime\/vframe.hpp\"\n-#include \"runtime\/vmOperations.hpp\"\n-#include \"utilities\/events.hpp\"\n-#include \"utilities\/globalDefinitions.hpp\"\n+#include \"runtime\/safepoint.hpp\"\n+#include \"runtime\/safepointVerifiers.hpp\"\n+\n+#if INCLUDE_JVMCI\n+#include \"jvmci\/jvmci.hpp\"\n+#endif\n@@ -52,0 +52,1 @@\n+\n@@ -56,1 +57,4 @@\n-CompilationPolicy* CompilationPolicy::_policy;\n+jlong CompilationPolicy::_start_time = 0;\n+int CompilationPolicy::_c1_count = 0;\n+int CompilationPolicy::_c2_count = 0;\n+double CompilationPolicy::_increase_threshold_at_ratio = 0;\n@@ -58,1 +62,0 @@\n-\/\/ Determine compilation policy based on command line argument\n@@ -60,9 +63,2 @@\n-  #ifdef TIERED\n-  if (TieredCompilation) {\n-    CompilationPolicy::set_policy(new TieredThresholdPolicy());\n-  } else {\n-    CompilationPolicy::set_policy(new SimpleCompPolicy());\n-  }\n-  #else\n-  CompilationPolicy::set_policy(new SimpleCompPolicy());\n-  #endif\n+  CompilationPolicy::initialize();\n+}\n@@ -70,1 +66,7 @@\n-  CompilationPolicy::policy()->initialize();\n+int CompilationPolicy::compiler_count(CompLevel comp_level) {\n+  if (is_c1_compile(comp_level)) {\n+    return c1_count();\n+  } else if (is_c2_compile(comp_level)) {\n+    return c2_count();\n+  }\n+  return 0;\n@@ -91,3 +93,0 @@\n-    \/\/ Note: with several active threads, the must_be_compiled may be true\n-    \/\/       while can_be_compiled is false; remove assert\n-    \/\/ assert(CompilationPolicy::can_be_compiled(selected_method), \"cannot compile\");\n@@ -109,1 +108,1 @@\n-        CompilationPolicy::policy()->initial_compile_level(selected_method),\n+        CompilationPolicy::initial_compile_level(selected_method),\n@@ -114,0 +113,11 @@\n+static inline CompLevel adjust_level_for_compilability_query(CompLevel comp_level) {\n+  if (comp_level == CompLevel_all) {\n+     if (CompilerConfig::is_c1_only()) {\n+       comp_level = CompLevel_simple;\n+     } else if (CompilerConfig::is_c2_or_jvmci_compiler_only()) {\n+       comp_level = CompLevel_full_optimization;\n+     }\n+  }\n+  return comp_level;\n+}\n+\n@@ -131,9 +141,2 @@\n-  if (comp_level == CompLevel_all) {\n-    if (TieredCompilation) {\n-      \/\/ enough to be compilable at any level for tiered\n-      return !m->is_not_compilable(CompLevel_simple) || !m->is_not_compilable(CompLevel_full_optimization);\n-    } else {\n-      \/\/ must be compilable at available level for non-tiered\n-      return !m->is_not_compilable(CompLevel_highest_tier);\n-    }\n-  } else if (is_compile(comp_level)) {\n+  comp_level = adjust_level_for_compilability_query((CompLevel) comp_level);\n+  if (comp_level == CompLevel_all || is_compile(comp_level)) {\n@@ -148,9 +151,2 @@\n-  if (comp_level == CompLevel_all) {\n-    if (TieredCompilation) {\n-      \/\/ enough to be osr compilable at any level for tiered\n-      result = !m->is_not_osr_compilable(CompLevel_simple) || !m->is_not_osr_compilable(CompLevel_full_optimization);\n-    } else {\n-      \/\/ must be osr compilable at available level for non-tiered\n-      result = !m->is_not_osr_compilable(CompLevel_highest_tier);\n-    }\n-  } else if (is_compile(comp_level)) {\n+  comp_level = adjust_level_for_compilability_query((CompLevel) comp_level);\n+  if (comp_level == CompLevel_all || is_compile(comp_level)) {\n@@ -195,0 +191,9 @@\n+\/\/ Simple methods are as good being compiled with C1 as C2.\n+\/\/ Determine if a given method is such a case.\n+bool CompilationPolicy::is_trivial(Method* method) {\n+  if (method->is_accessor() ||\n+      method->is_constant_getter()) {\n+    return true;\n+  }\n+  return false;\n+}\n@@ -196,12 +201,8 @@\n-\/\/\n-\/\/ CounterDecay for SimpleCompPolicy\n-\/\/\n-\/\/ Iterates through invocation counters and decrements them. This\n-\/\/ is done at each safepoint.\n-\/\/\n-class CounterDecay : public AllStatic {\n-  static jlong _last_timestamp;\n-  static void do_method(Method* m) {\n-    MethodCounters* mcs = m->method_counters();\n-    if (mcs != NULL) {\n-      mcs->invocation_counter()->decay();\n+bool CompilationPolicy::force_comp_at_level_simple(const methodHandle& method) {\n+  if (CompilationModeFlag::quick_internal()) {\n+#if INCLUDE_JVMCI\n+    if (UseJVMCICompiler) {\n+      AbstractCompiler* comp = CompileBroker::compiler(CompLevel_full_optimization);\n+      if (comp != NULL && comp->is_jvmci() && ((JVMCICompiler*) comp)->force_comp_at_level_simple(method)) {\n+        return true;\n+      }\n@@ -209,0 +210,1 @@\n+#endif\n@@ -210,0 +212,18 @@\n+  return false;\n+}\n+\n+CompLevel CompilationPolicy::comp_level(Method* method) {\n+  CompiledMethod *nm = method->code();\n+  if (nm != NULL && nm->is_in_use()) {\n+    return (CompLevel)nm->comp_level();\n+  }\n+  return CompLevel_none;\n+}\n+\n+\/\/ Call and loop predicates determine whether a transition to a higher\n+\/\/ compilation level should be performed (pointers to predicate functions\n+\/\/ are passed to common()).\n+\/\/ Tier?LoadFeedback is basically a coefficient that determines of\n+\/\/ how many methods per compiler thread can be in the queue before\n+\/\/ the threshold values double.\n+class LoopPredicate : AllStatic {\n@@ -211,3 +231,39 @@\n-  static void decay();\n-  static bool is_decay_needed() {\n-    return nanos_to_millis(os::javaTimeNanos() - _last_timestamp) > CounterDecayMinIntervalLength;\n+  static bool apply_scaled(const methodHandle& method, CompLevel cur_level, int i, int b, double scale) {\n+    double threshold_scaling;\n+    if (CompilerOracle::has_option_value(method, CompileCommand::CompileThresholdScaling, threshold_scaling)) {\n+      scale *= threshold_scaling;\n+    }\n+    switch(cur_level) {\n+    case CompLevel_aot:\n+      return b >= Tier3AOTBackEdgeThreshold * scale;\n+    case CompLevel_none:\n+    case CompLevel_limited_profile:\n+      return b >= Tier3BackEdgeThreshold * scale;\n+    case CompLevel_full_profile:\n+      return b >= Tier4BackEdgeThreshold * scale;\n+    default:\n+      return true;\n+    }\n+  }\n+\n+  static bool apply(int i, int b, CompLevel cur_level, const methodHandle& method) {\n+    double k = 1;\n+    switch(cur_level) {\n+    case CompLevel_aot: {\n+      k = CompilationModeFlag::disable_intermediate() ? 1 : CompilationPolicy::threshold_scale(CompLevel_full_profile, Tier3LoadFeedback);\n+      break;\n+    }\n+    case CompLevel_none:\n+    \/\/ Fall through\n+    case CompLevel_limited_profile: {\n+      k = CompilationPolicy::threshold_scale(CompLevel_full_profile, Tier3LoadFeedback);\n+      break;\n+    }\n+    case CompLevel_full_profile: {\n+      k = CompilationPolicy::threshold_scale(CompLevel_full_optimization, Tier4LoadFeedback);\n+      break;\n+    }\n+    default:\n+      return true;\n+    }\n+    return apply_scaled(method, cur_level, i, b, k);\n@@ -215,1 +271,0 @@\n-  static void update_last_timestamp() { _last_timestamp = os::javaTimeNanos(); }\n@@ -218,1 +273,22 @@\n-jlong CounterDecay::_last_timestamp = 0;\n+class CallPredicate : AllStatic {\n+public:\n+  static bool apply_scaled(const methodHandle& method, CompLevel cur_level, int i, int b, double scale) {\n+    double threshold_scaling;\n+    if (CompilerOracle::has_option_value(method, CompileCommand::CompileThresholdScaling, threshold_scaling)) {\n+      scale *= threshold_scaling;\n+    }\n+    switch(cur_level) {\n+    case CompLevel_aot:\n+      return (i >= Tier3AOTInvocationThreshold * scale) ||\n+             (i >= Tier3AOTMinInvocationThreshold * scale && i + b >= Tier3AOTCompileThreshold * scale);\n+    case CompLevel_none:\n+    case CompLevel_limited_profile:\n+      return (i >= Tier3InvocationThreshold * scale) ||\n+             (i >= Tier3MinInvocationThreshold * scale && i + b >= Tier3CompileThreshold * scale);\n+    case CompLevel_full_profile:\n+      return (i >= Tier4InvocationThreshold * scale) ||\n+             (i >= Tier4MinInvocationThreshold * scale && i + b >= Tier4CompileThreshold * scale);\n+    default:\n+     return true;\n+    }\n+  }\n@@ -220,2 +296,22 @@\n-void CounterDecay::decay() {\n-  update_last_timestamp();\n+  static bool apply(int i, int b, CompLevel cur_level, const methodHandle& method) {\n+    double k = 1;\n+    switch(cur_level) {\n+    case CompLevel_aot: {\n+      k = CompilationModeFlag::disable_intermediate() ? 1 : CompilationPolicy::threshold_scale(CompLevel_full_profile, Tier3LoadFeedback);\n+      break;\n+    }\n+    case CompLevel_none:\n+    case CompLevel_limited_profile: {\n+      k = CompilationPolicy::threshold_scale(CompLevel_full_profile, Tier3LoadFeedback);\n+      break;\n+    }\n+    case CompLevel_full_profile: {\n+      k = CompilationPolicy::threshold_scale(CompLevel_full_optimization, Tier4LoadFeedback);\n+      break;\n+    }\n+    default:\n+      return true;\n+    }\n+    return apply_scaled(method, cur_level, i, b, k);\n+  }\n+};\n@@ -223,11 +319,15 @@\n-  \/\/ This operation is going to be performed only at the end of a safepoint\n-  \/\/ and hence GC's will not be going on, all Java mutators are suspended\n-  \/\/ at this point and hence SystemDictionary_lock is also not needed.\n-  assert(SafepointSynchronize::is_at_safepoint(), \"can only be executed at a safepoint\");\n-  size_t nclasses = ClassLoaderDataGraph::num_instance_classes();\n-  size_t classes_per_tick = nclasses * (CounterDecayMinIntervalLength * 1e-3 \/\n-                                        CounterHalfLifeTime);\n-  for (size_t i = 0; i < classes_per_tick; i++) {\n-    InstanceKlass* k = ClassLoaderDataGraph::try_get_next_class();\n-    if (k != NULL) {\n-      k->methods_do(do_method);\n+double CompilationPolicy::threshold_scale(CompLevel level, int feedback_k) {\n+  int comp_count = compiler_count(level);\n+  if (comp_count > 0) {\n+    double queue_size = CompileBroker::queue_size(level);\n+    double k = queue_size \/ (feedback_k * comp_count) + 1;\n+\n+    \/\/ Increase C1 compile threshold when the code cache is filled more\n+    \/\/ than specified by IncreaseFirstTierCompileThresholdAt percentage.\n+    \/\/ The main intention is to keep enough free space for C2 compiled code\n+    \/\/ to achieve peak performance if the code cache is under stress.\n+    if (!CompilationModeFlag::disable_intermediate() && TieredStopAtLevel == CompLevel_full_optimization && level != CompLevel_full_optimization)  {\n+      double current_reverse_free_ratio = CodeCache::reverse_free_ratio(CodeCache::get_code_blob_type(level));\n+      if (current_reverse_free_ratio > _increase_threshold_at_ratio) {\n+        k *= exp(current_reverse_free_ratio - _increase_threshold_at_ratio);\n+      }\n@@ -235,0 +335,1 @@\n+    return k;\n@@ -236,0 +337,1 @@\n+  return 1;\n@@ -238,0 +340,24 @@\n+void CompilationPolicy::print_counters(const char* prefix, Method* m) {\n+  int invocation_count = m->invocation_count();\n+  int backedge_count = m->backedge_count();\n+  MethodData* mdh = m->method_data();\n+  int mdo_invocations = 0, mdo_backedges = 0;\n+  int mdo_invocations_start = 0, mdo_backedges_start = 0;\n+  if (mdh != NULL) {\n+    mdo_invocations = mdh->invocation_count();\n+    mdo_backedges = mdh->backedge_count();\n+    mdo_invocations_start = mdh->invocation_count_start();\n+    mdo_backedges_start = mdh->backedge_count_start();\n+  }\n+  tty->print(\" %stotal=%d,%d %smdo=%d(%d),%d(%d)\", prefix,\n+      invocation_count, backedge_count, prefix,\n+      mdo_invocations, mdo_invocations_start,\n+      mdo_backedges, mdo_backedges_start);\n+  tty->print(\" %smax levels=%d,%d\", prefix,\n+      m->highest_comp_level(), m->highest_osr_comp_level());\n+}\n+\n+\/\/ Print an event.\n+void CompilationPolicy::print_event(EventType type, Method* m, Method* im,\n+                                        int bci, CompLevel level) {\n+  bool inlinee_event = m != im;\n@@ -239,5 +365,27 @@\n-#ifndef PRODUCT\n-void SimpleCompPolicy::trace_osr_completion(nmethod* osr_nm) {\n-  if (TraceOnStackReplacement) {\n-    if (osr_nm == NULL) tty->print_cr(\"compilation failed\");\n-    else tty->print_cr(\"nmethod \" INTPTR_FORMAT, p2i(osr_nm));\n+  ttyLocker tty_lock;\n+  tty->print(\"%lf: [\", os::elapsedTime());\n+\n+  switch(type) {\n+  case CALL:\n+    tty->print(\"call\");\n+    break;\n+  case LOOP:\n+    tty->print(\"loop\");\n+    break;\n+  case COMPILE:\n+    tty->print(\"compile\");\n+    break;\n+  case REMOVE_FROM_QUEUE:\n+    tty->print(\"remove-from-queue\");\n+    break;\n+  case UPDATE_IN_QUEUE:\n+    tty->print(\"update-in-queue\");\n+    break;\n+  case REPROFILE:\n+    tty->print(\"reprofile\");\n+    break;\n+  case MAKE_NOT_ENTRANT:\n+    tty->print(\"make-not-entrant\");\n+    break;\n+  default:\n+    tty->print(\"unknown\");\n@@ -245,0 +393,52 @@\n+\n+  tty->print(\" level=%d \", level);\n+\n+  ResourceMark rm;\n+  char *method_name = m->name_and_sig_as_C_string();\n+  tty->print(\"[%s\", method_name);\n+  if (inlinee_event) {\n+    char *inlinee_name = im->name_and_sig_as_C_string();\n+    tty->print(\" [%s]] \", inlinee_name);\n+  }\n+  else tty->print(\"] \");\n+  tty->print(\"@%d queues=%d,%d\", bci, CompileBroker::queue_size(CompLevel_full_profile),\n+                                      CompileBroker::queue_size(CompLevel_full_optimization));\n+\n+  tty->print(\" rate=\");\n+  if (m->prev_time() == 0) tty->print(\"n\/a\");\n+  else tty->print(\"%f\", m->rate());\n+\n+  tty->print(\" k=%.2lf,%.2lf\", threshold_scale(CompLevel_full_profile, Tier3LoadFeedback),\n+                               threshold_scale(CompLevel_full_optimization, Tier4LoadFeedback));\n+\n+  if (type != COMPILE) {\n+    print_counters(\"\", m);\n+    if (inlinee_event) {\n+      print_counters(\"inlinee \", im);\n+    }\n+    tty->print(\" compilable=\");\n+    bool need_comma = false;\n+    if (!m->is_not_compilable(CompLevel_full_profile)) {\n+      tty->print(\"c1\");\n+      need_comma = true;\n+    }\n+    if (!m->is_not_osr_compilable(CompLevel_full_profile)) {\n+      if (need_comma) tty->print(\",\");\n+      tty->print(\"c1-osr\");\n+      need_comma = true;\n+    }\n+    if (!m->is_not_compilable(CompLevel_full_optimization)) {\n+      if (need_comma) tty->print(\",\");\n+      tty->print(\"c2\");\n+      need_comma = true;\n+    }\n+    if (!m->is_not_osr_compilable(CompLevel_full_optimization)) {\n+      if (need_comma) tty->print(\",\");\n+      tty->print(\"c2-osr\");\n+    }\n+    tty->print(\" status=\");\n+    if (m->queued_for_compilation()) {\n+      tty->print(\"in-queue\");\n+    } else tty->print(\"idle\");\n+  }\n+  tty->print_cr(\"]\");\n@@ -246,1 +446,0 @@\n-#endif \/\/ !PRODUCT\n@@ -248,9 +447,18 @@\n-void SimpleCompPolicy::initialize() {\n-  \/\/ Setup the compiler thread numbers\n-  if (CICompilerCountPerCPU) {\n-    \/\/ Example: if CICompilerCountPerCPU is true, then we get\n-    \/\/ max(log2(8)-1,1) = 2 compiler threads on an 8-way machine.\n-    \/\/ May help big-app startup time.\n-    _compiler_count = MAX2(log2i_graceful(os::active_processor_count()) - 1, 1);\n-    \/\/ Make sure there is enough space in the code cache to hold all the compiler buffers\n-    size_t buffer_size = 1;\n+void CompilationPolicy::initialize() {\n+  if (!CompilerConfig::is_interpreter_only()) {\n+    int count = CICompilerCount;\n+    bool c1_only = CompilerConfig::is_c1_only();\n+    bool c2_only = CompilerConfig::is_c2_or_jvmci_compiler_only();\n+\n+#ifdef _LP64\n+    \/\/ Turn on ergonomic compiler count selection\n+    if (FLAG_IS_DEFAULT(CICompilerCountPerCPU) && FLAG_IS_DEFAULT(CICompilerCount)) {\n+      FLAG_SET_DEFAULT(CICompilerCountPerCPU, true);\n+    }\n+    if (CICompilerCountPerCPU) {\n+      \/\/ Simple log n seems to grow too slowly for tiered, try something faster: log n * log log n\n+      int log_cpu = log2i(os::active_processor_count());\n+      int loglog_cpu = log2i(MAX2(log_cpu, 1));\n+      count = MAX2(log_cpu * loglog_cpu * 3 \/ 2, 2);\n+      \/\/ Make sure there is enough space in the code cache to hold all the compiler buffers\n+      size_t c1_size = 0;\n@@ -258,1 +466,1 @@\n-    buffer_size = is_client_compilation_mode_vm() ? Compiler::code_buffer_size() : buffer_size;\n+      c1_size = Compiler::code_buffer_size();\n@@ -260,0 +468,1 @@\n+      size_t c2_size = 0;\n@@ -261,1 +470,1 @@\n-    buffer_size = is_server_compilation_mode_vm() ? C2Compiler::initial_code_buffer_size() : buffer_size;\n+      c2_size = C2Compiler::initial_code_buffer_size();\n@@ -263,4 +472,7 @@\n-    int max_count = (ReservedCodeCacheSize - (CodeCacheMinimumUseSpace DEBUG_ONLY(* 3))) \/ (int)buffer_size;\n-    if (_compiler_count > max_count) {\n-      \/\/ Lower the compiler count such that all buffers fit into the code cache\n-      _compiler_count = MAX2(max_count, 1);\n+      size_t buffer_size = c1_only ? c1_size : (c1_size\/3 + 2*c2_size\/3);\n+      int max_count = (ReservedCodeCacheSize - (CodeCacheMinimumUseSpace DEBUG_ONLY(* 3))) \/ (int)buffer_size;\n+      if (count > max_count) {\n+        \/\/ Lower the compiler count such that all buffers fit into the code cache\n+        count = MAX2(max_count, c1_only ? 1 : 2);\n+      }\n+      FLAG_SET_ERGO(CICompilerCount, count);\n@@ -268,6 +480,13 @@\n-    FLAG_SET_ERGO(CICompilerCount, _compiler_count);\n-  } else {\n-    _compiler_count = CICompilerCount;\n-  }\n-  CounterDecay::update_last_timestamp();\n-}\n+#else\n+    \/\/ On 32-bit systems, the number of compiler threads is limited to 3.\n+    \/\/ On these systems, the virtual address space available to the JVM\n+    \/\/ is usually limited to 2-4 GB (the exact value depends on the platform).\n+    \/\/ As the compilers (especially C2) can consume a large amount of\n+    \/\/ memory, scaling the number of compiler threads with the number of\n+    \/\/ available cores can result in the exhaustion of the address space\n+    \/\/\/ available to the VM and thus cause the VM to crash.\n+    if (FLAG_IS_DEFAULT(CICompilerCount)) {\n+      count = 3;\n+      FLAG_SET_ERGO(CICompilerCount, count);\n+    }\n+#endif\n@@ -275,13 +494,11 @@\n-\/\/ Note: this policy is used ONLY if TieredCompilation is off.\n-\/\/ compiler_count() behaves the following way:\n-\/\/ - with TIERED build (with both COMPILER1 and COMPILER2 defined) it should return\n-\/\/   zero for the c1 compilation levels in server compilation mode runs\n-\/\/   and c2 compilation levels in client compilation mode runs.\n-\/\/ - with COMPILER2 not defined it should return zero for c2 compilation levels.\n-\/\/ - with COMPILER1 not defined it should return zero for c1 compilation levels.\n-\/\/ - if neither is defined - always return zero.\n-int SimpleCompPolicy::compiler_count(CompLevel comp_level) {\n-  assert(!TieredCompilation, \"This policy should not be used with TieredCompilation\");\n-  if (COMPILER2_PRESENT(is_server_compilation_mode_vm() && is_c2_compile(comp_level) ||)\n-      is_client_compilation_mode_vm() && is_c1_compile(comp_level)) {\n-    return _compiler_count;\n+    if (c1_only) {\n+      \/\/ No C2 compiler thread required\n+      set_c1_count(count);\n+    } else if (c2_only) {\n+      set_c2_count(count);\n+    } else {\n+      set_c1_count(MAX2(count \/ 3, 1));\n+      set_c2_count(MAX2(count - c1_count(), 1));\n+    }\n+    assert(count == c1_count() + c2_count(), \"inconsistent compiler thread count\");\n+    set_increase_threshold_at_ratio();\n@@ -289,1 +506,1 @@\n-  return 0;\n+  set_start_time(nanos_to_millis(os::javaTimeNanos()));\n@@ -292,10 +509,0 @@\n-void SimpleCompPolicy::reset_counter_for_invocation_event(const methodHandle& m) {\n-  \/\/ Make sure invocation and backedge counter doesn't overflow again right away\n-  \/\/ as would be the case for native methods.\n-\n-  \/\/ BUT also make sure the method doesn't look like it was never executed.\n-  \/\/ Set carry bit and reduce counter's value to min(count, CompileThreshold\/2).\n-  MethodCounters* mcs = m->method_counters();\n-  assert(mcs != NULL, \"MethodCounters cannot be NULL for profiling\");\n-  mcs->invocation_counter()->set_carry_and_reduce();\n-  mcs->backedge_counter()->set_carry_and_reduce();\n@@ -303,1 +510,16 @@\n-  assert(!m->was_never_executed(), \"don't reset to 0 -- could be mistaken for never-executed\");\n+#ifdef ASSERT\n+bool CompilationPolicy::verify_level(CompLevel level) {\n+  \/\/ AOT and interpreter levels are always valid.\n+  if (level == CompLevel_aot || level == CompLevel_none) {\n+    return true;\n+  }\n+  if (CompilationModeFlag::normal()) {\n+    return true;\n+  } else if (CompilationModeFlag::quick_only()) {\n+    return level == CompLevel_simple;\n+  } else if (CompilationModeFlag::high_only()) {\n+    return level == CompLevel_full_optimization;\n+  } else if (CompilationModeFlag::high_only_quick_internal()) {\n+    return level == CompLevel_full_optimization || level == CompLevel_simple;\n+  }\n+  return false;\n@@ -305,0 +527,1 @@\n+#endif\n@@ -306,7 +529,0 @@\n-void SimpleCompPolicy::reset_counter_for_back_branch_event(const methodHandle& m) {\n-  \/\/ Delay next back-branch event but pump up invocation counter to trigger\n-  \/\/ whole method compilation.\n-  MethodCounters* mcs = m->method_counters();\n-  assert(mcs != NULL, \"MethodCounters cannot be NULL for profiling\");\n-  InvocationCounter* i = mcs->invocation_counter();\n-  InvocationCounter* b = mcs->backedge_counter();\n@@ -314,6 +530,15 @@\n-  \/\/ Don't set invocation_counter's value too low otherwise the method will\n-  \/\/ look like immature (ic < ~5300) which prevents the inlining based on\n-  \/\/ the type profiling.\n-  i->set(CompileThreshold);\n-  \/\/ Don't reset counter too low - it is used to check if OSR method is ready.\n-  b->set(CompileThreshold \/ 2);\n+CompLevel CompilationPolicy::highest_compile_level() {\n+  CompLevel max_level = CompLevel_none;\n+  if (!CompilerConfig::is_interpreter_only()) {\n+    if (CompilerConfig::is_c2_or_jvmci_compiler_available()) {\n+      max_level = CompLevel_full_optimization;\n+    } else if (CompilerConfig::is_c1_available()) {\n+      if (CompilerConfig::is_c1_simple_only()) {\n+        max_level = CompLevel_simple;\n+      } else {\n+        max_level = CompLevel_full_profile;\n+      }\n+    }\n+    max_level = MAX2(max_level, (CompLevel) TieredStopAtLevel);\n+  }\n+  return max_level;\n@@ -322,4 +547,19 @@\n-\/\/ Called at the end of the safepoint\n-void SimpleCompPolicy::do_safepoint_work() {\n-  if(UseCounterDecay && CounterDecay::is_decay_needed()) {\n-    CounterDecay::decay();\n+CompLevel CompilationPolicy::limit_level(CompLevel level) {\n+  if (CompilationModeFlag::quick_only()) {\n+    level = MIN2(level, CompLevel_simple);\n+  }\n+  assert(verify_level(level), \"Invalid compilation level %d\", level);\n+  if (level <= TieredStopAtLevel) {\n+    return level;\n+  }\n+  \/\/ Some compilation levels are not valid depending on a compilation mode:\n+  \/\/ a) quick_only - levels 2,3,4 are invalid; levels -1,0,1 are valid;\n+  \/\/ b) high_only - levels 1,2,3 are invalid; levels -1,0,4 are valid;\n+  \/\/ c) high_only_quick_internal - levels 2,3 are invalid; levels -1,0,1,4 are valid.\n+  \/\/ The invalid levels are actually sequential so a single comparison is sufficient.\n+  \/\/ Down here we already have (level > TieredStopAtLevel), which also implies that\n+  \/\/ (TieredStopAtLevel < Highest Possible Level), so we need to return a level that is:\n+  \/\/ a) a max level that is strictly less than the highest for a given compilation mode\n+  \/\/ b) less or equal to TieredStopAtLevel\n+  if (CompilationModeFlag::normal() || CompilationModeFlag::quick_only()) {\n+    return (CompLevel)TieredStopAtLevel;\n@@ -327,1 +567,0 @@\n-}\n@@ -329,10 +568,2 @@\n-void SimpleCompPolicy::reprofile(ScopeDesc* trap_scope, bool is_osr) {\n-  ScopeDesc* sd = trap_scope;\n-  MethodCounters* mcs;\n-  InvocationCounter* c;\n-  for (; !sd->is_top(); sd = sd->sender()) {\n-    mcs = sd->method()->method_counters();\n-    if (mcs != NULL) {\n-      \/\/ Reset ICs of inlined methods, since they can trigger compilations also.\n-      mcs->invocation_counter()->reset();\n-    }\n+  if (CompilationModeFlag::high_only() || CompilationModeFlag::high_only_quick_internal()) {\n+    return MIN2(CompLevel_none, (CompLevel)TieredStopAtLevel);\n@@ -340,6 +571,16 @@\n-  mcs = sd->method()->method_counters();\n-  if (mcs != NULL) {\n-    c = mcs->invocation_counter();\n-    if (is_osr) {\n-      \/\/ It was an OSR method, so bump the count higher.\n-      c->set(CompileThreshold);\n+\n+  ShouldNotReachHere();\n+  return CompLevel_any;\n+}\n+\n+CompLevel CompilationPolicy::initial_compile_level(const methodHandle& method) {\n+  CompLevel level = CompLevel_any;\n+  if (CompilationModeFlag::normal()) {\n+    level = CompLevel_full_profile;\n+  } else if (CompilationModeFlag::quick_only()) {\n+    level = CompLevel_simple;\n+  } else if (CompilationModeFlag::high_only()) {\n+    level = CompLevel_full_optimization;\n+  } else if (CompilationModeFlag::high_only_quick_internal()) {\n+    if (force_comp_at_level_simple(method)) {\n+      level = CompLevel_simple;\n@@ -347,1 +588,1 @@\n-      c->reset();\n+      level = CompLevel_full_optimization;\n@@ -349,1 +590,0 @@\n-    mcs->backedge_counter()->reset();\n@@ -351,0 +591,2 @@\n+  assert(level != CompLevel_any, \"Unhandled compilation mode\");\n+  return limit_level(level);\n@@ -353,4 +595,3 @@\n-\/\/ This method can be called by any component of the runtime to notify the policy\n-\/\/ that it's recommended to delay the compilation of this method.\n-void SimpleCompPolicy::delay_compilation(Method* method) {\n-  MethodCounters* mcs = method->method_counters();\n+\/\/ Set carry flags on the counters if necessary\n+void CompilationPolicy::handle_counter_overflow(Method* method) {\n+  MethodCounters *mcs = method->method_counters();\n@@ -358,2 +599,7 @@\n-    mcs->invocation_counter()->decay();\n-    mcs->backedge_counter()->decay();\n+    mcs->invocation_counter()->set_carry_on_overflow();\n+    mcs->backedge_counter()->set_carry_on_overflow();\n+  }\n+  MethodData* mdo = method->method_data();\n+  if (mdo != NULL) {\n+    mdo->invocation_counter()->set_carry_on_overflow();\n+    mdo->backedge_counter()->set_carry_on_overflow();\n@@ -363,3 +609,5 @@\n-CompileTask* SimpleCompPolicy::select_task(CompileQueue* compile_queue) {\n-  return select_task_helper(compile_queue);\n-}\n+\/\/ Called with the queue locked and with at least one element\n+CompileTask* CompilationPolicy::select_task(CompileQueue* compile_queue) {\n+  CompileTask *max_blocking_task = NULL;\n+  CompileTask *max_task = NULL;\n+  Method* max_method = NULL;\n@@ -367,28 +615,57 @@\n-bool SimpleCompPolicy::is_mature(Method* method) {\n-  MethodData* mdo = method->method_data();\n-  assert(mdo != NULL, \"Should be\");\n-  uint current = mdo->mileage_of(method);\n-  uint initial = mdo->creation_mileage();\n-  if (current < initial)\n-    return true;  \/\/ some sort of overflow\n-  uint target;\n-  if (ProfileMaturityPercentage <= 0)\n-    target = (uint) -ProfileMaturityPercentage;  \/\/ absolute value\n-  else\n-    target = (uint)( (ProfileMaturityPercentage * CompileThreshold) \/ 100 );\n-  return (current >= initial + target);\n-}\n-\n-nmethod* SimpleCompPolicy::event(const methodHandle& method, const methodHandle& inlinee, int branch_bci,\n-                                    int bci, CompLevel comp_level, CompiledMethod* nm, TRAPS) {\n-  assert(comp_level == CompLevel_none, \"This should be only called from the interpreter\");\n-  NOT_PRODUCT(trace_frequency_counter_overflow(method, branch_bci, bci));\n-  if (JvmtiExport::can_post_interpreter_events() && THREAD->as_Java_thread()->is_interp_only_mode()) {\n-    \/\/ If certain JVMTI events (e.g. frame pop event) are requested then the\n-    \/\/ thread is forced to remain in interpreted code. This is\n-    \/\/ implemented partly by a check in the run_compiled_code\n-    \/\/ section of the interpreter whether we should skip running\n-    \/\/ compiled code, and partly by skipping OSR compiles for\n-    \/\/ interpreted-only threads.\n-    if (bci != InvocationEntryBci) {\n-      reset_counter_for_back_branch_event(method);\n+  jlong t = nanos_to_millis(os::javaTimeNanos());\n+  \/\/ Iterate through the queue and find a method with a maximum rate.\n+  for (CompileTask* task = compile_queue->first(); task != NULL;) {\n+    CompileTask* next_task = task->next();\n+    Method* method = task->method();\n+    \/\/ If a method was unloaded or has been stale for some time, remove it from the queue.\n+    \/\/ Blocking tasks and tasks submitted from whitebox API don't become stale\n+    if (task->is_unloaded() || (task->can_become_stale() && is_stale(t, TieredCompileTaskTimeout, method) && !is_old(method))) {\n+      if (!task->is_unloaded()) {\n+        if (PrintTieredEvents) {\n+          print_event(REMOVE_FROM_QUEUE, method, method, task->osr_bci(), (CompLevel) task->comp_level());\n+        }\n+        method->clear_queued_for_compilation();\n+      }\n+      compile_queue->remove_and_mark_stale(task);\n+      task = next_task;\n+      continue;\n+    }\n+    update_rate(t, method);\n+    if (max_task == NULL || compare_methods(method, max_method)) {\n+      \/\/ Select a method with the highest rate\n+      max_task = task;\n+      max_method = method;\n+    }\n+\n+    if (task->is_blocking()) {\n+      if (max_blocking_task == NULL || compare_methods(method, max_blocking_task->method())) {\n+        max_blocking_task = task;\n+      }\n+    }\n+\n+    task = next_task;\n+  }\n+\n+  if (max_blocking_task != NULL) {\n+    \/\/ In blocking compilation mode, the CompileBroker will make\n+    \/\/ compilations submitted by a JVMCI compiler thread non-blocking. These\n+    \/\/ compilations should be scheduled after all blocking compilations\n+    \/\/ to service non-compiler related compilations sooner and reduce the\n+    \/\/ chance of such compilations timing out.\n+    max_task = max_blocking_task;\n+    max_method = max_task->method();\n+  }\n+\n+  methodHandle max_method_h(Thread::current(), max_method);\n+\n+  if (max_task != NULL && max_task->comp_level() == CompLevel_full_profile &&\n+      TieredStopAtLevel > CompLevel_full_profile &&\n+      max_method != NULL && is_method_profiled(max_method_h)) {\n+    max_task->set_comp_level(CompLevel_limited_profile);\n+\n+    if (CompileBroker::compilation_is_complete(max_method_h, max_task->osr_bci(), CompLevel_limited_profile)) {\n+      if (PrintTieredEvents) {\n+        print_event(REMOVE_FROM_QUEUE, max_method, max_method, max_task->osr_bci(), (CompLevel)max_task->comp_level());\n+      }\n+      compile_queue->remove_and_mark_stale(max_task);\n+      max_method->clear_queued_for_compilation();\n@@ -397,0 +674,32 @@\n+\n+    if (PrintTieredEvents) {\n+      print_event(UPDATE_IN_QUEUE, max_method, max_method, max_task->osr_bci(), (CompLevel)max_task->comp_level());\n+    }\n+  }\n+\n+  return max_task;\n+}\n+\n+void CompilationPolicy::reprofile(ScopeDesc* trap_scope, bool is_osr) {\n+  for (ScopeDesc* sd = trap_scope;; sd = sd->sender()) {\n+    if (PrintTieredEvents) {\n+      print_event(REPROFILE, sd->method(), sd->method(), InvocationEntryBci, CompLevel_none);\n+    }\n+    MethodData* mdo = sd->method()->method_data();\n+    if (mdo != NULL) {\n+      mdo->reset_start_counters();\n+    }\n+    if (sd->is_top()) break;\n+  }\n+}\n+\n+nmethod* CompilationPolicy::event(const methodHandle& method, const methodHandle& inlinee,\n+                                      int branch_bci, int bci, CompLevel comp_level, CompiledMethod* nm, TRAPS) {\n+  if (PrintTieredEvents) {\n+    print_event(bci == InvocationEntryBci ? CALL : LOOP, method(), inlinee(), bci, comp_level);\n+  }\n+\n+  if (comp_level == CompLevel_none &&\n+      JvmtiExport::can_post_interpreter_events() &&\n+      THREAD->as_Java_thread()->is_interp_only_mode()) {\n+    return NULL;\n@@ -400,5 +709,0 @@\n-    if (bci == InvocationEntryBci) {\n-      reset_counter_for_invocation_event(method);\n-    } else {\n-      reset_counter_for_back_branch_event(method);\n-    }\n@@ -408,0 +712,5 @@\n+  handle_counter_overflow(method());\n+  if (method() != inlinee()) {\n+    handle_counter_overflow(inlinee());\n+  }\n+\n@@ -409,13 +718,1 @@\n-    \/\/ when code cache is full, compilation gets switched off, UseCompiler\n-    \/\/ is set to false\n-    if (!method->has_compiled_code() && UseCompiler) {\n-      method_invocation_event(method, THREAD);\n-    } else {\n-      \/\/ Force counter overflow on method entry, even if no compilation\n-      \/\/ happened.  (The method_invocation_event call does this also.)\n-      reset_counter_for_invocation_event(method);\n-    }\n-    \/\/ compilation at an invocation overflow no longer goes and retries test for\n-    \/\/ compiled method. We always run the loser of the race as interpreted.\n-    \/\/ so return NULL\n-    return NULL;\n+    method_invocation_event(method, inlinee, comp_level, nm, THREAD);\n@@ -423,11 +720,16 @@\n-    \/\/ counter overflow in a loop => try to do on-stack-replacement\n-    nmethod* osr_nm = method->lookup_osr_nmethod_for(bci, CompLevel_highest_tier, true);\n-    NOT_PRODUCT(trace_osr_request(method, osr_nm, bci));\n-    \/\/ when code cache is full, we should not compile any more...\n-    if (osr_nm == NULL && UseCompiler) {\n-      method_back_branch_event(method, bci, THREAD);\n-      osr_nm = method->lookup_osr_nmethod_for(bci, CompLevel_highest_tier, true);\n-    }\n-    if (osr_nm == NULL) {\n-      reset_counter_for_back_branch_event(method);\n-      return NULL;\n+    \/\/ method == inlinee if the event originated in the main method\n+    method_back_branch_event(method, inlinee, bci, comp_level, nm, THREAD);\n+    \/\/ Check if event led to a higher level OSR compilation\n+    CompLevel expected_comp_level = MIN2(CompLevel_full_optimization, static_cast<CompLevel>(comp_level + 1));\n+    if (!CompilationModeFlag::disable_intermediate() && inlinee->is_not_osr_compilable(expected_comp_level)) {\n+      \/\/ It's not possble to reach the expected level so fall back to simple.\n+      expected_comp_level = CompLevel_simple;\n+    }\n+    CompLevel max_osr_level = static_cast<CompLevel>(inlinee->highest_osr_comp_level());\n+    if (max_osr_level >= expected_comp_level) { \/\/ fast check to avoid locking in a typical scenario\n+      nmethod* osr_nm = inlinee->lookup_osr_nmethod_for(bci, expected_comp_level, false);\n+      assert(osr_nm == NULL || osr_nm->comp_level() >= expected_comp_level, \"lookup_osr_nmethod_for is broken\");\n+      if (osr_nm != NULL && osr_nm->comp_level() != comp_level) {\n+        \/\/ Perform OSR with new nmethod\n+        return osr_nm;\n+      }\n@@ -435,1 +737,0 @@\n-    return osr_nm;\n@@ -440,10 +741,103 @@\n-#ifndef PRODUCT\n-void SimpleCompPolicy::trace_frequency_counter_overflow(const methodHandle& m, int branch_bci, int bci) {\n-  if (TraceInvocationCounterOverflow) {\n-    MethodCounters* mcs = m->method_counters();\n-    assert(mcs != NULL, \"MethodCounters cannot be NULL for profiling\");\n-    InvocationCounter* ic = mcs->invocation_counter();\n-    InvocationCounter* bc = mcs->backedge_counter();\n-    ResourceMark rm;\n-    if (bci == InvocationEntryBci) {\n-      tty->print(\"comp-policy cntr ovfl @ %d in entry of \", bci);\n+\/\/ Check if the method can be compiled, change level if necessary\n+void CompilationPolicy::compile(const methodHandle& mh, int bci, CompLevel level, TRAPS) {\n+  assert(verify_level(level) && level <= TieredStopAtLevel, \"Invalid compilation level %d\", level);\n+\n+  if (level == CompLevel_none) {\n+    if (mh->has_compiled_code()) {\n+      \/\/ Happens when we switch from AOT to interpreter to profile.\n+      MutexLocker ml(Compile_lock);\n+      NoSafepointVerifier nsv;\n+      if (mh->has_compiled_code()) {\n+        mh->code()->make_not_used();\n+      }\n+      \/\/ Deoptimize immediately (we don't have to wait for a compile).\n+      JavaThread* jt = THREAD->as_Java_thread();\n+      RegisterMap map(jt, false);\n+      frame fr = jt->last_frame().sender(&map);\n+      Deoptimization::deoptimize_frame(jt, fr.id());\n+    }\n+    return;\n+  }\n+  if (level == CompLevel_aot) {\n+    if (mh->has_aot_code()) {\n+      if (PrintTieredEvents) {\n+        print_event(COMPILE, mh(), mh(), bci, level);\n+      }\n+      MutexLocker ml(Compile_lock);\n+      NoSafepointVerifier nsv;\n+      if (mh->has_aot_code() && mh->code() != mh->aot_code()) {\n+        mh->aot_code()->make_entrant();\n+        if (mh->has_compiled_code()) {\n+          mh->code()->make_not_entrant();\n+        }\n+        MutexLocker pl(CompiledMethod_lock, Mutex::_no_safepoint_check_flag);\n+        Method::set_code(mh, mh->aot_code());\n+      }\n+    }\n+    return;\n+  }\n+\n+  if (!CompilationModeFlag::disable_intermediate()) {\n+    \/\/ Check if the method can be compiled. If it cannot be compiled with C1, continue profiling\n+    \/\/ in the interpreter and then compile with C2 (the transition function will request that,\n+    \/\/ see common() ). If the method cannot be compiled with C2 but still can with C1, compile it with\n+    \/\/ pure C1.\n+    if ((bci == InvocationEntryBci && !can_be_compiled(mh, level))) {\n+      if (level == CompLevel_full_optimization && can_be_compiled(mh, CompLevel_simple)) {\n+        compile(mh, bci, CompLevel_simple, THREAD);\n+      }\n+      return;\n+    }\n+    if ((bci != InvocationEntryBci && !can_be_osr_compiled(mh, level))) {\n+      if (level == CompLevel_full_optimization && can_be_osr_compiled(mh, CompLevel_simple)) {\n+        nmethod* osr_nm = mh->lookup_osr_nmethod_for(bci, CompLevel_simple, false);\n+        if (osr_nm != NULL && osr_nm->comp_level() > CompLevel_simple) {\n+          \/\/ Invalidate the existing OSR nmethod so that a compile at CompLevel_simple is permitted.\n+          osr_nm->make_not_entrant();\n+        }\n+        compile(mh, bci, CompLevel_simple, THREAD);\n+      }\n+      return;\n+    }\n+  }\n+  if (bci != InvocationEntryBci && mh->is_not_osr_compilable(level)) {\n+    return;\n+  }\n+  if (!CompileBroker::compilation_is_in_queue(mh)) {\n+    if (PrintTieredEvents) {\n+      print_event(COMPILE, mh(), mh(), bci, level);\n+    }\n+    int hot_count = (bci == InvocationEntryBci) ? mh->invocation_count() : mh->backedge_count();\n+    update_rate(nanos_to_millis(os::javaTimeNanos()), mh());\n+    CompileBroker::compile_method(mh, bci, level, mh, hot_count, CompileTask::Reason_Tiered, THREAD);\n+  }\n+}\n+\n+\/\/ update_rate() is called from select_task() while holding a compile queue lock.\n+void CompilationPolicy::update_rate(jlong t, Method* m) {\n+  \/\/ Skip update if counters are absent.\n+  \/\/ Can't allocate them since we are holding compile queue lock.\n+  if (m->method_counters() == NULL)  return;\n+\n+  if (is_old(m)) {\n+    \/\/ We don't remove old methods from the queue,\n+    \/\/ so we can just zero the rate.\n+    m->set_rate(0);\n+    return;\n+  }\n+\n+  \/\/ We don't update the rate if we've just came out of a safepoint.\n+  \/\/ delta_s is the time since last safepoint in milliseconds.\n+  jlong delta_s = t - SafepointTracing::end_of_last_safepoint_ms();\n+  jlong delta_t = t - (m->prev_time() != 0 ? m->prev_time() : start_time()); \/\/ milliseconds since the last measurement\n+  \/\/ How many events were there since the last time?\n+  int event_count = m->invocation_count() + m->backedge_count();\n+  int delta_e = event_count - m->prev_event_count();\n+\n+  \/\/ We should be running for at least 1ms.\n+  if (delta_s >= TieredRateUpdateMinTime) {\n+    \/\/ And we must've taken the previous point at least 1ms before.\n+    if (delta_t >= TieredRateUpdateMinTime && delta_e > 0) {\n+      m->set_prev_time(t);\n+      m->set_prev_event_count(event_count);\n+      m->set_rate((float)delta_e \/ (float)delta_t); \/\/ Rate is events per millisecond\n@@ -451,13 +845,242 @@\n-      tty->print(\"comp-policy cntr ovfl @ %d in loop of \", bci);\n-    }\n-    m->print_value();\n-    tty->cr();\n-    ic->print();\n-    bc->print();\n-    if (ProfileInterpreter) {\n-      if (bci != InvocationEntryBci) {\n-        MethodData* mdo = m->method_data();\n-        if (mdo != NULL) {\n-          ProfileData *pd = mdo->bci_to_data(branch_bci);\n-          if (pd == NULL) {\n-            tty->print_cr(\"back branch count = N\/A (missing ProfileData)\");\n+      if (delta_t > TieredRateUpdateMaxTime && delta_e == 0) {\n+        \/\/ If nothing happened for 25ms, zero the rate. Don't modify prev values.\n+        m->set_rate(0);\n+      }\n+    }\n+  }\n+}\n+\n+\/\/ Check if this method has been stale for a given number of milliseconds.\n+\/\/ See select_task().\n+bool CompilationPolicy::is_stale(jlong t, jlong timeout, Method* m) {\n+  jlong delta_s = t - SafepointTracing::end_of_last_safepoint_ms();\n+  jlong delta_t = t - m->prev_time();\n+  if (delta_t > timeout && delta_s > timeout) {\n+    int event_count = m->invocation_count() + m->backedge_count();\n+    int delta_e = event_count - m->prev_event_count();\n+    \/\/ Return true if there were no events.\n+    return delta_e == 0;\n+  }\n+  return false;\n+}\n+\n+\/\/ We don't remove old methods from the compile queue even if they have\n+\/\/ very low activity. See select_task().\n+bool CompilationPolicy::is_old(Method* method) {\n+  return method->invocation_count() > 50000 || method->backedge_count() > 500000;\n+}\n+\n+double CompilationPolicy::weight(Method* method) {\n+  return (double)(method->rate() + 1) *\n+    (method->invocation_count() + 1) * (method->backedge_count() + 1);\n+}\n+\n+\/\/ Apply heuristics and return true if x should be compiled before y\n+bool CompilationPolicy::compare_methods(Method* x, Method* y) {\n+  if (x->highest_comp_level() > y->highest_comp_level()) {\n+    \/\/ recompilation after deopt\n+    return true;\n+  } else\n+    if (x->highest_comp_level() == y->highest_comp_level()) {\n+      if (weight(x) > weight(y)) {\n+        return true;\n+      }\n+    }\n+  return false;\n+}\n+\n+\/\/ Is method profiled enough?\n+bool CompilationPolicy::is_method_profiled(const methodHandle& method) {\n+  MethodData* mdo = method->method_data();\n+  if (mdo != NULL) {\n+    int i = mdo->invocation_count_delta();\n+    int b = mdo->backedge_count_delta();\n+    return CallPredicate::apply_scaled(method, CompLevel_full_profile, i, b, 1);\n+  }\n+  return false;\n+}\n+\n+\n+\/\/ Determine is a method is mature.\n+bool CompilationPolicy::is_mature(Method* method) {\n+  methodHandle mh(Thread::current(), method);\n+  MethodData* mdo = method->method_data();\n+  if (mdo != NULL) {\n+    int i = mdo->invocation_count();\n+    int b = mdo->backedge_count();\n+    double k = ProfileMaturityPercentage \/ 100.0;\n+    return CallPredicate::apply_scaled(mh, CompLevel_full_profile, i, b, k) || LoopPredicate::apply_scaled(mh, CompLevel_full_profile, i, b, k);\n+  }\n+  return false;\n+}\n+\n+\/\/ If a method is old enough and is still in the interpreter we would want to\n+\/\/ start profiling without waiting for the compiled method to arrive.\n+\/\/ We also take the load on compilers into the account.\n+bool CompilationPolicy::should_create_mdo(const methodHandle& method, CompLevel cur_level) {\n+  if (cur_level != CompLevel_none || force_comp_at_level_simple(method) || CompilationModeFlag::quick_only() || !ProfileInterpreter) {\n+    return false;\n+  }\n+  int i = method->invocation_count();\n+  int b = method->backedge_count();\n+  double k = Tier0ProfilingStartPercentage \/ 100.0;\n+\n+  \/\/ If the top level compiler is not keeping up, delay profiling.\n+  if (CompileBroker::queue_size(CompLevel_full_optimization) <= Tier0Delay * compiler_count(CompLevel_full_optimization)) {\n+    return CallPredicate::apply_scaled(method, CompLevel_full_profile, i, b, k) || LoopPredicate::apply_scaled(method, CompLevel_full_profile, i, b, k);\n+  }\n+  return false;\n+}\n+\n+\/\/ Inlining control: if we're compiling a profiled method with C1 and the callee\n+\/\/ is known to have OSRed in a C2 version, don't inline it.\n+bool CompilationPolicy::should_not_inline(ciEnv* env, ciMethod* callee) {\n+  CompLevel comp_level = (CompLevel)env->comp_level();\n+  if (comp_level == CompLevel_full_profile ||\n+      comp_level == CompLevel_limited_profile) {\n+    return callee->highest_osr_comp_level() == CompLevel_full_optimization;\n+  }\n+  return false;\n+}\n+\n+\/\/ Create MDO if necessary.\n+void CompilationPolicy::create_mdo(const methodHandle& mh, Thread* THREAD) {\n+  if (mh->is_native() ||\n+      mh->is_abstract() ||\n+      mh->is_accessor() ||\n+      mh->is_constant_getter()) {\n+    return;\n+  }\n+  if (mh->method_data() == NULL) {\n+    Method::build_interpreter_method_data(mh, CHECK_AND_CLEAR);\n+  }\n+  if (ProfileInterpreter) {\n+    MethodData* mdo = mh->method_data();\n+    if (mdo != NULL) {\n+      JavaThread* jt = THREAD->as_Java_thread();\n+      frame last_frame = jt->last_frame();\n+      if (last_frame.is_interpreted_frame() && mh == last_frame.interpreter_frame_method()) {\n+        int bci = last_frame.interpreter_frame_bci();\n+        address dp = mdo->bci_to_dp(bci);\n+        last_frame.interpreter_frame_set_mdp(dp);\n+      }\n+    }\n+  }\n+}\n+\n+\n+\n+\/*\n+ * Method states:\n+ *   0 - interpreter (CompLevel_none)\n+ *   1 - pure C1 (CompLevel_simple)\n+ *   2 - C1 with invocation and backedge counting (CompLevel_limited_profile)\n+ *   3 - C1 with full profiling (CompLevel_full_profile)\n+ *   4 - C2 or Graal (CompLevel_full_optimization)\n+ *\n+ * Common state transition patterns:\n+ * a. 0 -> 3 -> 4.\n+ *    The most common path. But note that even in this straightforward case\n+ *    profiling can start at level 0 and finish at level 3.\n+ *\n+ * b. 0 -> 2 -> 3 -> 4.\n+ *    This case occurs when the load on C2 is deemed too high. So, instead of transitioning\n+ *    into state 3 directly and over-profiling while a method is in the C2 queue we transition to\n+ *    level 2 and wait until the load on C2 decreases. This path is disabled for OSRs.\n+ *\n+ * c. 0 -> (3->2) -> 4.\n+ *    In this case we enqueue a method for compilation at level 3, but the C1 queue is long enough\n+ *    to enable the profiling to fully occur at level 0. In this case we change the compilation level\n+ *    of the method to 2 while the request is still in-queue, because it'll allow it to run much faster\n+ *    without full profiling while c2 is compiling.\n+ *\n+ * d. 0 -> 3 -> 1 or 0 -> 2 -> 1.\n+ *    After a method was once compiled with C1 it can be identified as trivial and be compiled to\n+ *    level 1. These transition can also occur if a method can't be compiled with C2 but can with C1.\n+ *\n+ * e. 0 -> 4.\n+ *    This can happen if a method fails C1 compilation (it will still be profiled in the interpreter)\n+ *    or because of a deopt that didn't require reprofiling (compilation won't happen in this case because\n+ *    the compiled version already exists).\n+ *\n+ * Note that since state 0 can be reached from any other state via deoptimization different loops\n+ * are possible.\n+ *\n+ *\/\n+\n+\/\/ Common transition function. Given a predicate determines if a method should transition to another level.\n+template<typename Predicate>\n+CompLevel CompilationPolicy::common(const methodHandle& method, CompLevel cur_level, bool disable_feedback) {\n+  CompLevel next_level = cur_level;\n+  int i = method->invocation_count();\n+  int b = method->backedge_count();\n+\n+  if (force_comp_at_level_simple(method)) {\n+    next_level = CompLevel_simple;\n+  } else {\n+    if (is_trivial(method())) {\n+      next_level = CompilationModeFlag::disable_intermediate() ? CompLevel_full_optimization : CompLevel_simple;\n+    } else {\n+      switch(cur_level) {\n+      default: break;\n+      case CompLevel_aot:\n+        \/\/ If we were at full profile level, would we switch to full opt?\n+        if (common<Predicate>(method, CompLevel_full_profile, disable_feedback) == CompLevel_full_optimization) {\n+          next_level = CompLevel_full_optimization;\n+        } else if (disable_feedback || (CompileBroker::queue_size(CompLevel_full_optimization) <=\n+                                        Tier3DelayOff * compiler_count(CompLevel_full_optimization) &&\n+                                       Predicate::apply(i, b, cur_level, method))) {\n+            next_level = CompilationModeFlag::disable_intermediate() ? CompLevel_none : CompLevel_full_profile;\n+        }\n+        break;\n+      case CompLevel_none:\n+        \/\/ If we were at full profile level, would we switch to full opt?\n+        if (common<Predicate>(method, CompLevel_full_profile, disable_feedback) == CompLevel_full_optimization) {\n+          next_level = CompLevel_full_optimization;\n+        } else if (!CompilationModeFlag::disable_intermediate() && Predicate::apply(i, b, cur_level, method)) {\n+#if INCLUDE_JVMCI\n+          if (EnableJVMCI && UseJVMCICompiler) {\n+            \/\/ Since JVMCI takes a while to warm up, its queue inevitably backs up during\n+            \/\/ early VM execution. As of 2014-06-13, JVMCI's inliner assumes that the root\n+            \/\/ compilation method and all potential inlinees have mature profiles (which\n+            \/\/ includes type profiling). If it sees immature profiles, JVMCI's inliner\n+            \/\/ can perform pathologically bad (e.g., causing OutOfMemoryErrors due to\n+            \/\/ exploring\/inlining too many graphs). Since a rewrite of the inliner is\n+            \/\/ in progress, we simply disable the dialing back heuristic for now and will\n+            \/\/ revisit this decision once the new inliner is completed.\n+            next_level = CompLevel_full_profile;\n+          } else\n+#endif\n+          {\n+            \/\/ C1-generated fully profiled code is about 30% slower than the limited profile\n+            \/\/ code that has only invocation and backedge counters. The observation is that\n+            \/\/ if C2 queue is large enough we can spend too much time in the fully profiled code\n+            \/\/ while waiting for C2 to pick the method from the queue. To alleviate this problem\n+            \/\/ we introduce a feedback on the C2 queue size. If the C2 queue is sufficiently long\n+            \/\/ we choose to compile a limited profiled version and then recompile with full profiling\n+            \/\/ when the load on C2 goes down.\n+            if (!disable_feedback && CompileBroker::queue_size(CompLevel_full_optimization) >\n+                Tier3DelayOn * compiler_count(CompLevel_full_optimization)) {\n+              next_level = CompLevel_limited_profile;\n+            } else {\n+              next_level = CompLevel_full_profile;\n+            }\n+          }\n+        }\n+        break;\n+      case CompLevel_limited_profile:\n+        if (is_method_profiled(method)) {\n+          \/\/ Special case: we got here because this method was fully profiled in the interpreter.\n+          next_level = CompLevel_full_optimization;\n+        } else {\n+          MethodData* mdo = method->method_data();\n+          if (mdo != NULL) {\n+            if (mdo->would_profile()) {\n+              if (disable_feedback || (CompileBroker::queue_size(CompLevel_full_optimization) <=\n+                                       Tier3DelayOff * compiler_count(CompLevel_full_optimization) &&\n+                                       Predicate::apply(i, b, cur_level, method))) {\n+                next_level = CompLevel_full_profile;\n+              }\n+            } else {\n+              next_level = CompLevel_full_optimization;\n+            }\n@@ -465,1 +1088,22 @@\n-            tty->print_cr(\"back branch count = %d\", pd->as_JumpData()->taken());\n+            \/\/ If there is no MDO we need to profile\n+            if (disable_feedback || (CompileBroker::queue_size(CompLevel_full_optimization) <=\n+                                     Tier3DelayOff * compiler_count(CompLevel_full_optimization) &&\n+                                     Predicate::apply(i, b, cur_level, method))) {\n+              next_level = CompLevel_full_profile;\n+            }\n+          }\n+        }\n+        break;\n+      case CompLevel_full_profile:\n+        {\n+          MethodData* mdo = method->method_data();\n+          if (mdo != NULL) {\n+            if (mdo->would_profile() || CompilationModeFlag::disable_intermediate()) {\n+              int mdo_i = mdo->invocation_count_delta();\n+              int mdo_b = mdo->backedge_count_delta();\n+              if (Predicate::apply(mdo_i, mdo_b, cur_level, method)) {\n+                next_level = CompLevel_full_optimization;\n+              }\n+            } else {\n+              next_level = CompLevel_full_optimization;\n+            }\n@@ -468,0 +1112,1 @@\n+        break;\n@@ -471,0 +1116,1 @@\n+  return (next_level != cur_level) ? limit_level(next_level) : next_level;\n@@ -473,6 +1119,18 @@\n-void SimpleCompPolicy::trace_osr_request(const methodHandle& method, nmethod* osr, int bci) {\n-  if (TraceOnStackReplacement) {\n-    ResourceMark rm;\n-    tty->print(osr != NULL ? \"Reused OSR entry for \" : \"Requesting OSR entry for \");\n-    method->print_short_name(tty);\n-    tty->print_cr(\" at bci %d\", bci);\n+\n+\n+\/\/ Determine if a method should be compiled with a normal entry point at a different level.\n+CompLevel CompilationPolicy::call_event(const methodHandle& method, CompLevel cur_level, Thread* thread) {\n+  CompLevel osr_level = MIN2((CompLevel) method->highest_osr_comp_level(), common<LoopPredicate>(method, cur_level, true));\n+  CompLevel next_level = common<CallPredicate>(method, cur_level);\n+\n+  \/\/ If OSR method level is greater than the regular method level, the levels should be\n+  \/\/ equalized by raising the regular method level in order to avoid OSRs during each\n+  \/\/ invocation of the method.\n+  if (osr_level == CompLevel_full_optimization && cur_level == CompLevel_full_profile) {\n+    MethodData* mdo = method->method_data();\n+    guarantee(mdo != NULL, \"MDO should not be NULL\");\n+    if (mdo->invocation_count() >= 1) {\n+      next_level = CompLevel_full_optimization;\n+    }\n+  } else {\n+    next_level = MAX2(osr_level, next_level);\n@@ -480,0 +1138,1 @@\n+  return next_level;\n@@ -481,1 +1140,0 @@\n-#endif \/\/ !PRODUCT\n@@ -483,4 +1141,13 @@\n-void SimpleCompPolicy::method_invocation_event(const methodHandle& m, TRAPS) {\n-  const int comp_level = CompLevel_highest_tier;\n-  const int hot_count = m->invocation_count();\n-  reset_counter_for_invocation_event(m);\n+\/\/ Determine if we should do an OSR compilation of a given method.\n+CompLevel CompilationPolicy::loop_event(const methodHandle& method, CompLevel cur_level, Thread* thread) {\n+  CompLevel next_level = common<LoopPredicate>(method, cur_level, true);\n+  if (cur_level == CompLevel_none) {\n+    \/\/ If there is a live OSR method that means that we deopted to the interpreter\n+    \/\/ for the transition.\n+    CompLevel osr_level = MIN2((CompLevel)method->highest_osr_comp_level(), next_level);\n+    if (osr_level > CompLevel_none) {\n+      return osr_level;\n+    }\n+  }\n+  return next_level;\n+}\n@@ -488,4 +1155,14 @@\n-  if (is_compilation_enabled() && can_be_compiled(m, comp_level)) {\n-    CompiledMethod* nm = m->code();\n-    if (nm == NULL ) {\n-      CompileBroker::compile_method(m, InvocationEntryBci, comp_level, m, hot_count, CompileTask::Reason_InvocationCount, THREAD);\n+bool CompilationPolicy::maybe_switch_to_aot(const methodHandle& mh, CompLevel cur_level, CompLevel next_level, Thread* thread) {\n+  if (UseAOT) {\n+    if (cur_level == CompLevel_full_profile || cur_level == CompLevel_none) {\n+      \/\/ If the current level is full profile or interpreter and we're switching to any other level,\n+      \/\/ activate the AOT code back first so that we won't waste time overprofiling.\n+      compile(mh, InvocationEntryBci, CompLevel_aot, thread);\n+      \/\/ Fall through for JIT compilation.\n+    }\n+    if (next_level == CompLevel_limited_profile && cur_level != CompLevel_aot && mh->has_aot_code()) {\n+      \/\/ If the next level is limited profile, use the aot code (if there is any),\n+      \/\/ since it's essentially the same thing.\n+      compile(mh, InvocationEntryBci, CompLevel_aot, thread);\n+      \/\/ Not need to JIT, we're done.\n+      return true;\n@@ -494,0 +1171,1 @@\n+  return false;\n@@ -496,3 +1174,0 @@\n-void SimpleCompPolicy::method_back_branch_event(const methodHandle& m, int bci, TRAPS) {\n-  const int comp_level = CompLevel_highest_tier;\n-  const int hot_count = m->backedge_count();\n@@ -500,3 +1175,15 @@\n-  if (is_compilation_enabled() && can_be_osr_compiled(m, comp_level)) {\n-    CompileBroker::compile_method(m, bci, comp_level, m, hot_count, CompileTask::Reason_BackedgeCount, THREAD);\n-    NOT_PRODUCT(trace_osr_completion(m->lookup_osr_nmethod_for(bci, comp_level, true));)\n+\/\/ Handle the invocation event.\n+void CompilationPolicy::method_invocation_event(const methodHandle& mh, const methodHandle& imh,\n+                                                      CompLevel level, CompiledMethod* nm, TRAPS) {\n+  if (should_create_mdo(mh, level)) {\n+    create_mdo(mh, THREAD);\n+  }\n+  CompLevel next_level = call_event(mh, level, THREAD);\n+  if (next_level != level) {\n+    if (maybe_switch_to_aot(mh, level, next_level, THREAD)) {\n+      \/\/ No JITting necessary\n+      return;\n+    }\n+    if (is_compilation_enabled() && !CompileBroker::compilation_is_in_queue(mh)) {\n+      compile(mh, InvocationEntryBci, next_level, THREAD);\n+    }\n@@ -505,0 +1192,81 @@\n+\n+\/\/ Handle the back branch event. Notice that we can compile the method\n+\/\/ with a regular entry from here.\n+void CompilationPolicy::method_back_branch_event(const methodHandle& mh, const methodHandle& imh,\n+                                                     int bci, CompLevel level, CompiledMethod* nm, TRAPS) {\n+  if (should_create_mdo(mh, level)) {\n+    create_mdo(mh, THREAD);\n+  }\n+  \/\/ Check if MDO should be created for the inlined method\n+  if (should_create_mdo(imh, level)) {\n+    create_mdo(imh, THREAD);\n+  }\n+\n+  if (is_compilation_enabled()) {\n+    CompLevel next_osr_level = loop_event(imh, level, THREAD);\n+    CompLevel max_osr_level = (CompLevel)imh->highest_osr_comp_level();\n+    \/\/ At the very least compile the OSR version\n+    if (!CompileBroker::compilation_is_in_queue(imh) && (next_osr_level != level)) {\n+      compile(imh, bci, next_osr_level, CHECK);\n+    }\n+\n+    \/\/ Use loop event as an opportunity to also check if there's been\n+    \/\/ enough calls.\n+    CompLevel cur_level, next_level;\n+    if (mh() != imh()) { \/\/ If there is an enclosing method\n+      if (level == CompLevel_aot) {\n+        \/\/ Recompile the enclosing method to prevent infinite OSRs. Stay at AOT level while it's compiling.\n+        if (max_osr_level != CompLevel_none && !CompileBroker::compilation_is_in_queue(mh)) {\n+          CompLevel enclosing_level = limit_level(CompLevel_full_profile);\n+          compile(mh, InvocationEntryBci, enclosing_level, THREAD);\n+        }\n+      } else {\n+        \/\/ Current loop event level is not AOT\n+        guarantee(nm != NULL, \"Should have nmethod here\");\n+        cur_level = comp_level(mh());\n+        next_level = call_event(mh, cur_level, THREAD);\n+\n+        if (max_osr_level == CompLevel_full_optimization) {\n+          \/\/ The inlinee OSRed to full opt, we need to modify the enclosing method to avoid deopts\n+          bool make_not_entrant = false;\n+          if (nm->is_osr_method()) {\n+            \/\/ This is an osr method, just make it not entrant and recompile later if needed\n+            make_not_entrant = true;\n+          } else {\n+            if (next_level != CompLevel_full_optimization) {\n+              \/\/ next_level is not full opt, so we need to recompile the\n+              \/\/ enclosing method without the inlinee\n+              cur_level = CompLevel_none;\n+              make_not_entrant = true;\n+            }\n+          }\n+          if (make_not_entrant) {\n+            if (PrintTieredEvents) {\n+              int osr_bci = nm->is_osr_method() ? nm->osr_entry_bci() : InvocationEntryBci;\n+              print_event(MAKE_NOT_ENTRANT, mh(), mh(), osr_bci, level);\n+            }\n+            nm->make_not_entrant();\n+          }\n+        }\n+        \/\/ Fix up next_level if necessary to avoid deopts\n+        if (next_level == CompLevel_limited_profile && max_osr_level == CompLevel_full_profile) {\n+          next_level = CompLevel_full_profile;\n+        }\n+        if (cur_level != next_level) {\n+          if (!maybe_switch_to_aot(mh, cur_level, next_level, THREAD) && !CompileBroker::compilation_is_in_queue(mh)) {\n+            compile(mh, InvocationEntryBci, next_level, THREAD);\n+          }\n+        }\n+      }\n+    } else {\n+      cur_level = comp_level(mh());\n+      next_level = call_event(mh, cur_level, THREAD);\n+      if (next_level != cur_level) {\n+        if (!maybe_switch_to_aot(mh, cur_level, next_level, THREAD) && !CompileBroker::compilation_is_in_queue(mh)) {\n+          compile(mh, InvocationEntryBci, next_level, THREAD);\n+        }\n+      }\n+    }\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/compiler\/compilationPolicy.cpp","additions":1044,"deletions":276,"binary":false,"changes":1320,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2010, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -30,3 +30,2 @@\n-#include \"memory\/allocation.hpp\"\n-#include \"runtime\/vmOperations.hpp\"\n-#include \"utilities\/growableArray.hpp\"\n+#include \"oops\/methodData.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -34,3 +33,0 @@\n-\/\/ The CompilationPolicy selects which method (if any) should be compiled.\n-\/\/ It also decides which methods must always be compiled (i.e., are never\n-\/\/ interpreted).\n@@ -39,0 +35,47 @@\n+\/*\n+ *  The system supports 5 execution levels:\n+ *  * level 0 - interpreter\n+ *  * level 1 - C1 with full optimization (no profiling)\n+ *  * level 2 - C1 with invocation and backedge counters\n+ *  * level 3 - C1 with full profiling (level 2 + MDO)\n+ *  * level 4 - C2\n+ *\n+ * Levels 0, 2 and 3 periodically notify the runtime about the current value of the counters\n+ * (invocation counters and backedge counters). The frequency of these notifications is\n+ * different at each level. These notifications are used by the policy to decide what transition\n+ * to make.\n+ *\n+ * Execution starts at level 0 (interpreter), then the policy can decide either to compile the\n+ * method at level 3 or level 2. The decision is based on the following factors:\n+ *    1. The length of the C2 queue determines the next level. The observation is that level 2\n+ * is generally faster than level 3 by about 30%, therefore we would want to minimize the time\n+ * a method spends at level 3. We should only spend the time at level 3 that is necessary to get\n+ * adequate profiling. So, if the C2 queue is long enough it is more beneficial to go first to\n+ * level 2, because if we transitioned to level 3 we would be stuck there until our C2 compile\n+ * request makes its way through the long queue. When the load on C2 recedes we are going to\n+ * recompile at level 3 and start gathering profiling information.\n+ *    2. The length of C1 queue is used to dynamically adjust the thresholds, so as to introduce\n+ * additional filtering if the compiler is overloaded. The rationale is that by the time a\n+ * method gets compiled it can become unused, so it doesn't make sense to put too much onto the\n+ * queue.\n+ *\n+ * After profiling is completed at level 3 the transition is made to level 4. Again, the length\n+ * of the C2 queue is used as a feedback to adjust the thresholds.\n+ *\n+ * After the first C1 compile some basic information is determined about the code like the number\n+ * of the blocks and the number of the loops. Based on that it can be decided that a method\n+ * is trivial and compiling it with C1 will yield the same code. In this case the method is\n+ * compiled at level 1 instead of 4.\n+ *\n+ * We also support profiling at level 0. If C1 is slow enough to produce the level 3 version of\n+ * the code and the C2 queue is sufficiently small we can decide to start profiling in the\n+ * interpreter (and continue profiling in the compiled code once the level 3 version arrives).\n+ * If the profiling at level 0 is fully completed before level 3 version is produced, a level 2\n+ * version is compiled instead in order to run faster waiting for a level 4 version.\n+ *\n+ * Compile queues are implemented as priority queues - for each method in the queue we compute\n+ * the event rate (the number of invocation and backedge counter increments per unit of time).\n+ * When getting an element off the queue we pick the one with the largest rate. Maintaining the\n+ * rate also allows us to remove stale methods (the ones that got on the queue but stopped\n+ * being used shortly after that).\n+*\/\n@@ -40,2 +83,155 @@\n-class CompilationPolicy : public CHeapObj<mtCompiler> {\n-  static CompilationPolicy* _policy;\n+\/* Command line options:\n+ * - Tier?InvokeNotifyFreqLog and Tier?BackedgeNotifyFreqLog control the frequency of method\n+ *   invocation and backedge notifications. Basically every n-th invocation or backedge a mutator thread\n+ *   makes a call into the runtime.\n+ *\n+ * - Tier?InvocationThreshold, Tier?CompileThreshold, Tier?BackEdgeThreshold, Tier?MinInvocationThreshold control\n+ *   compilation thresholds.\n+ *   Level 2 thresholds are not used and are provided for option-compatibility and potential future use.\n+ *   Other thresholds work as follows:\n+ *\n+ *   Transition from interpreter (level 0) to C1 with full profiling (level 3) happens when\n+ *   the following predicate is true (X is the level):\n+ *\n+ *   i > TierXInvocationThreshold * s || (i > TierXMinInvocationThreshold * s  && i + b > TierXCompileThreshold * s),\n+ *\n+ *   where $i$ is the number of method invocations, $b$ number of backedges and $s$ is the scaling\n+ *   coefficient that will be discussed further.\n+ *   The intuition is to equalize the time that is spend profiling each method.\n+ *   The same predicate is used to control the transition from level 3 to level 4 (C2). It should be\n+ *   noted though that the thresholds are relative. Moreover i and b for the 0->3 transition come\n+ *   from Method* and for 3->4 transition they come from MDO (since profiled invocations are\n+ *   counted separately). Finally, if a method does not contain anything worth profiling, a transition\n+ *   from level 3 to level 4 occurs without considering thresholds (e.g., with fewer invocations than\n+ *   what is specified by Tier4InvocationThreshold).\n+ *\n+ *   OSR transitions are controlled simply with b > TierXBackEdgeThreshold * s predicates.\n+ *\n+ * - Tier?LoadFeedback options are used to automatically scale the predicates described above depending\n+ *   on the compiler load. The scaling coefficients are computed as follows:\n+ *\n+ *   s = queue_size_X \/ (TierXLoadFeedback * compiler_count_X) + 1,\n+ *\n+ *   where queue_size_X is the current size of the compiler queue of level X, and compiler_count_X\n+ *   is the number of level X compiler threads.\n+ *\n+ *   Basically these parameters describe how many methods should be in the compile queue\n+ *   per compiler thread before the scaling coefficient increases by one.\n+ *\n+ *   This feedback provides the mechanism to automatically control the flow of compilation requests\n+ *   depending on the machine speed, mutator load and other external factors.\n+ *\n+ * - Tier3DelayOn and Tier3DelayOff parameters control another important feedback loop.\n+ *   Consider the following observation: a method compiled with full profiling (level 3)\n+ *   is about 30% slower than a method at level 2 (just invocation and backedge counters, no MDO).\n+ *   Normally, the following transitions will occur: 0->3->4. The problem arises when the C2 queue\n+ *   gets congested and the 3->4 transition is delayed. While the method is the C2 queue it continues\n+ *   executing at level 3 for much longer time than is required by the predicate and at suboptimal speed.\n+ *   The idea is to dynamically change the behavior of the system in such a way that if a substantial\n+ *   load on C2 is detected we would first do the 0->2 transition allowing a method to run faster.\n+ *   And then when the load decreases to allow 2->3 transitions.\n+ *\n+ *   Tier3Delay* parameters control this switching mechanism.\n+ *   Tier3DelayOn is the number of methods in the C2 queue per compiler thread after which the policy\n+ *   no longer does 0->3 transitions but does 0->2 transitions instead.\n+ *   Tier3DelayOff switches the original behavior back when the number of methods in the C2 queue\n+ *   per compiler thread falls below the specified amount.\n+ *   The hysteresis is necessary to avoid jitter.\n+ *\n+ * - TieredCompileTaskTimeout is the amount of time an idle method can spend in the compile queue.\n+ *   Basically, since we use the event rate d(i + b)\/dt as a value of priority when selecting a method to\n+ *   compile from the compile queue, we also can detect stale methods for which the rate has been\n+ *   0 for some time in the same iteration. Stale methods can appear in the queue when an application\n+ *   abruptly changes its behavior.\n+ *\n+ * - TieredStopAtLevel, is used mostly for testing. It allows to bypass the policy logic and stick\n+ *   to a given level. For example it's useful to set TieredStopAtLevel = 1 in order to compile everything\n+ *   with pure c1.\n+ *\n+ * - Tier0ProfilingStartPercentage allows the interpreter to start profiling when the inequalities in the\n+ *   0->3 predicate are already exceeded by the given percentage but the level 3 version of the\n+ *   method is still not ready. We can even go directly from level 0 to 4 if c1 doesn't produce a compiled\n+ *   version in time. This reduces the overall transition to level 4 and decreases the startup time.\n+ *   Note that this behavior is also guarded by the Tier3Delay mechanism: when the c2 queue is too long\n+ *   these is not reason to start profiling prematurely.\n+ *\n+ * - TieredRateUpdateMinTime and TieredRateUpdateMaxTime are parameters of the rate computation.\n+ *   Basically, the rate is not computed more frequently than TieredRateUpdateMinTime and is considered\n+ *   to be zero if no events occurred in TieredRateUpdateMaxTime.\n+ *\/\n+\n+class CompilationPolicy : AllStatic {\n+  friend class CallPredicate;\n+  friend class LoopPredicate;\n+\n+  static jlong _start_time;\n+  static int _c1_count, _c2_count;\n+  static double _increase_threshold_at_ratio;\n+\n+  \/\/ Set carry flags in the counters (in Method* and MDO).\n+  inline static void handle_counter_overflow(Method* method);\n+  \/\/ Verify that a level is consistent with the compilation mode\n+  static bool verify_level(CompLevel level);\n+  \/\/ Clamp the request level according to various constraints.\n+  inline static CompLevel limit_level(CompLevel level);\n+  \/\/ Common transition function. Given a predicate determines if a method should transition to another level.\n+  template<typename Predicate>\n+  static CompLevel common(const methodHandle& method, CompLevel cur_level, bool disable_feedback = false);\n+  \/\/ Transition functions.\n+  \/\/ call_event determines if a method should be compiled at a different\n+  \/\/ level with a regular invocation entry.\n+  static CompLevel call_event(const methodHandle& method, CompLevel cur_level, Thread* thread);\n+  \/\/ loop_event checks if a method should be OSR compiled at a different\n+  \/\/ level.\n+  static CompLevel loop_event(const methodHandle& method, CompLevel cur_level, Thread* thread);\n+  static void print_counters(const char* prefix, Method* m);\n+  \/\/ Has a method been long around?\n+  \/\/ We don't remove old methods from the compile queue even if they have\n+  \/\/ very low activity (see select_task()).\n+  inline static bool is_old(Method* method);\n+  \/\/ Was a given method inactive for a given number of milliseconds.\n+  \/\/ If it is, we would remove it from the queue (see select_task()).\n+  inline static bool is_stale(jlong t, jlong timeout, Method* m);\n+  \/\/ Compute the weight of the method for the compilation scheduling\n+  inline static double weight(Method* method);\n+  \/\/ Apply heuristics and return true if x should be compiled before y\n+  inline static bool compare_methods(Method* x, Method* y);\n+  \/\/ Compute event rate for a given method. The rate is the number of event (invocations + backedges)\n+  \/\/ per millisecond.\n+  inline static void update_rate(jlong t, Method* m);\n+  \/\/ Compute threshold scaling coefficient\n+  inline static double threshold_scale(CompLevel level, int feedback_k);\n+  \/\/ If a method is old enough and is still in the interpreter we would want to\n+  \/\/ start profiling without waiting for the compiled method to arrive. This function\n+  \/\/ determines whether we should do that.\n+  inline static bool should_create_mdo(const methodHandle& method, CompLevel cur_level);\n+  \/\/ Create MDO if necessary.\n+  static void create_mdo(const methodHandle& mh, Thread* thread);\n+  \/\/ Is method profiled enough?\n+  static bool is_method_profiled(const methodHandle& method);\n+\n+  static bool maybe_switch_to_aot(const methodHandle& mh, CompLevel cur_level, CompLevel next_level, Thread* thread);\n+\n+  static void set_c1_count(int x) { _c1_count = x;    }\n+  static void set_c2_count(int x) { _c2_count = x;    }\n+\n+  enum EventType { CALL, LOOP, COMPILE, REMOVE_FROM_QUEUE, UPDATE_IN_QUEUE, REPROFILE, MAKE_NOT_ENTRANT };\n+  static void print_event(EventType type, Method* m, Method* im, int bci, CompLevel level);\n+  \/\/ Check if the method can be compiled, change level if necessary\n+  static void compile(const methodHandle& mh, int bci, CompLevel level, TRAPS);\n+  \/\/ Simple methods are as good being compiled with C1 as C2.\n+  \/\/ This function tells if it's such a function.\n+  inline static bool is_trivial(Method* method);\n+  \/\/ Force method to be compiled at CompLevel_simple?\n+  inline static bool force_comp_at_level_simple(const methodHandle& method);\n+\n+  \/\/ Get a compilation level for a given method.\n+  static CompLevel comp_level(Method* method);\n+  static void method_invocation_event(const methodHandle& method, const methodHandle& inlinee,\n+                               CompLevel level, CompiledMethod* nm, TRAPS);\n+  static void method_back_branch_event(const methodHandle& method, const methodHandle& inlinee,\n+                                int bci, CompLevel level, CompiledMethod* nm, TRAPS);\n+\n+  static void set_increase_threshold_at_ratio() { _increase_threshold_at_ratio = 100 \/ (100 - (double)IncreaseFirstTierCompileThresholdAt); }\n+  static void set_start_time(jlong t) { _start_time = t;    }\n+  static jlong start_time()           { return _start_time; }\n@@ -45,1 +241,0 @@\n-\n@@ -47,0 +242,4 @@\n+  static int c1_count() { return _c1_count; }\n+  static int c2_count() { return _c2_count; }\n+  static int compiler_count(CompLevel comp_level);\n+\n@@ -56,2 +255,0 @@\n-  static void set_policy(CompilationPolicy* policy) { _policy = policy; }\n-  static CompilationPolicy* policy()                { return _policy; }\n@@ -59,0 +256,1 @@\n+  static void do_safepoint_work() { }\n@@ -60,17 +258,6 @@\n-\n-  \/\/ Return initial compile level that is used with Xcomp\n-  virtual CompLevel initial_compile_level(const methodHandle& method) = 0;\n-  virtual int compiler_count(CompLevel comp_level) = 0;\n-  \/\/ main notification entry, return a pointer to an nmethod if the OSR is required,\n-  \/\/ returns NULL otherwise.\n-  virtual nmethod* event(const methodHandle& method, const methodHandle& inlinee, int branch_bci, int bci, CompLevel comp_level, CompiledMethod* nm, TRAPS) = 0;\n-  \/\/ safepoint() is called at the end of the safepoint\n-  virtual void do_safepoint_work() = 0;\n-  \/\/ reprofile request\n-  virtual void reprofile(ScopeDesc* trap_scope, bool is_osr) = 0;\n-  \/\/ delay_compilation(method) can be called by any component of the runtime to notify the policy\n-  \/\/ that it's recommended to delay the compilation of this method.\n-  virtual void delay_compilation(Method* method) = 0;\n-  \/\/ Select task is called by CompileBroker. The queue is guaranteed to have at least one\n-  \/\/ element and is locked. The function should select one and return it.\n-  virtual CompileTask* select_task(CompileQueue* compile_queue) = 0;\n+  \/\/ Return initial compile level to use with Xcomp (depends on compilation mode).\n+  static void reprofile(ScopeDesc* trap_scope, bool is_osr);\n+  static nmethod* event(const methodHandle& method, const methodHandle& inlinee,\n+                 int branch_bci, int bci, CompLevel comp_level, CompiledMethod* nm, TRAPS);\n+  \/\/ Select task is called by CompileBroker. We should return a task or NULL.\n+  static CompileTask* select_task(CompileQueue* compile_queue);\n@@ -78,5 +265,4 @@\n-  virtual bool is_mature(Method* method) = 0;\n-  \/\/ Do policy initialization\n-  virtual void initialize() = 0;\n-  virtual bool should_not_inline(ciEnv* env, ciMethod* method) { return false; }\n-};\n+  static bool is_mature(Method* method);\n+  \/\/ Initialize: set compiler thread count\n+  static void initialize();\n+  static bool should_not_inline(ciEnv* env, ciMethod* callee);\n@@ -84,22 +270,4 @@\n-\/\/ A simple compilation policy.\n-class SimpleCompPolicy : public CompilationPolicy {\n-  int _compiler_count;\n- private:\n-  static void trace_frequency_counter_overflow(const methodHandle& m, int branch_bci, int bci);\n-  static void trace_osr_request(const methodHandle& method, nmethod* osr, int bci);\n-  static void trace_osr_completion(nmethod* osr_nm);\n-  void reset_counter_for_invocation_event(const methodHandle& method);\n-  void reset_counter_for_back_branch_event(const methodHandle& method);\n-  void method_invocation_event(const methodHandle& m, TRAPS);\n-  void method_back_branch_event(const methodHandle& m, int bci, TRAPS);\n- public:\n-  SimpleCompPolicy() : _compiler_count(0) { }\n-  virtual CompLevel initial_compile_level(const methodHandle& m) { return CompLevel_highest_tier; }\n-  virtual int compiler_count(CompLevel comp_level);\n-  virtual void do_safepoint_work();\n-  virtual void reprofile(ScopeDesc* trap_scope, bool is_osr);\n-  virtual void delay_compilation(Method* method);\n-  virtual bool is_mature(Method* method);\n-  virtual void initialize();\n-  virtual CompileTask* select_task(CompileQueue* compile_queue);\n-  virtual nmethod* event(const methodHandle& method, const methodHandle& inlinee, int branch_bci, int bci, CompLevel comp_level, CompiledMethod* nm, TRAPS);\n+  \/\/ Return desired initial compilation level for Xcomp\n+  static CompLevel initial_compile_level(const methodHandle& method);\n+  \/\/ Return highest level possible\n+  static CompLevel highest_compile_level();\n@@ -108,1 +276,0 @@\n-\n","filename":"src\/hotspot\/share\/compiler\/compilationPolicy.hpp","additions":224,"deletions":57,"binary":false,"changes":281,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -461,1 +461,1 @@\n-    task = CompilationPolicy::policy()->select_task(this);\n+    task = CompilationPolicy::select_task(this);\n@@ -634,2 +634,2 @@\n-  _c1_count = CompilationPolicy::policy()->compiler_count(CompLevel_simple);\n-  _c2_count = CompilationPolicy::policy()->compiler_count(CompLevel_full_optimization);\n+  _c1_count = CompilationPolicy::c1_count();\n+  _c2_count = CompilationPolicy::c2_count();\n@@ -970,3 +970,1 @@\n-#if !defined(ZERO)\n-  assert(_c2_count > 0 || _c1_count > 0, \"No compilers?\");\n-#endif \/\/ !ZERO\n+\n@@ -1227,5 +1225,3 @@\n-  if (TieredCompilation) {\n-    \/\/ Tiered policy requires MethodCounters to exist before adding a method to\n-    \/\/ the queue. Create if we don't have them yet.\n-    method->get_method_counters(thread);\n-  }\n+  \/\/ Tiered policy requires MethodCounters to exist before adding a method to\n+  \/\/ the queue. Create if we don't have them yet.\n+  method->get_method_counters(thread);\n@@ -1381,3 +1377,0 @@\n-  assert(!TieredCompilation || comp_level <= TieredStopAtLevel, \"Invalid compilation level\");\n-  \/\/ allow any levels for WhiteBox\n-  assert(WhiteBoxAPI || TieredCompilation || comp_level == CompLevel_highest_tier, \"only CompLevel_highest_tier must be used in non-tiered\");\n@@ -1413,5 +1406,0 @@\n-#ifndef TIERED\n-    \/\/ seems like an assert of dubious value\n-    assert(comp_level == CompLevel_highest_tier,\n-           \"all OSR compiles are assumed to be at a single compilation level\");\n-#endif \/\/ TIERED\n@@ -1503,1 +1491,0 @@\n-      CompilationPolicy::policy()->delay_compilation(method());\n@@ -2312,1 +2299,1 @@\n-      ci_env.record_method_not_compilable(\"no compiler\", !TieredCompilation);\n+      ci_env.record_method_not_compilable(\"no compiler\");\n@@ -2335,1 +2322,1 @@\n-      ci_env.record_method_not_compilable(\"compile failed\", !TieredCompilation);\n+      ci_env.record_method_not_compilable(\"compile failed\");\n@@ -2717,1 +2704,1 @@\n-    for (int tier = CompLevel_simple; tier <= CompLevel_highest_tier; tier++) {\n+    for (int tier = CompLevel_simple; tier <= CompilationPolicy::highest_compile_level(); tier++) {\n","filename":"src\/hotspot\/share\/compiler\/compileBroker.cpp","additions":11,"deletions":24,"binary":false,"changes":35,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -26,0 +26,1 @@\n+#include \"compiler\/compilationPolicy.hpp\"\n@@ -342,1 +343,1 @@\n-  if (_comp_level != CompLevel_highest_tier) {\n+  if (_comp_level != CompilationPolicy::highest_compile_level()) {\n","filename":"src\/hotspot\/share\/compiler\/compileTask.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -41,4 +41,1 @@\n-#ifdef TIERED\n-bool CompilationModeFlag::_quick_only = false;\n-bool CompilationModeFlag::_high_only = false;\n-bool CompilationModeFlag::_high_only_quick_internal = false;\n+CompilationModeFlag::Mode CompilationModeFlag::_mode = CompilationModeFlag::Mode::NORMAL;\n@@ -46,0 +43,3 @@\n+static void print_mode_unavailable(const char* mode_name, const char* reason) {\n+  warning(\"%s compilation mode unavailable because %s.\", mode_name, reason);\n+}\n@@ -48,0 +48,1 @@\n+  _mode = Mode::NORMAL;\n@@ -49,2 +50,2 @@\n-    if (strcmp(CompilationMode, \"default\") == 0) {\n-      \/\/ Do nothing, just support the \"default\" keyword.\n+    if (strcmp(CompilationMode, \"default\") == 0 || strcmp(CompilationMode, \"normal\") == 0) {\n+      assert(_mode == Mode::NORMAL, \"Precondition\");\n@@ -52,1 +53,5 @@\n-      _quick_only = true;\n+      if (!CompilerConfig::has_c1()) {\n+        print_mode_unavailable(\"quick-only\", \"there is no c1 present\");\n+      } else {\n+        _mode = Mode::QUICK_ONLY;\n+      }\n@@ -54,1 +59,5 @@\n-      _high_only = true;\n+      if (!CompilerConfig::has_c2() && !CompilerConfig::is_jvmci_compiler()) {\n+        print_mode_unavailable(\"high-only\", \"there is no c2 or jvmci compiler present\");\n+      } else {\n+        _mode = Mode::HIGH_ONLY;\n+      }\n@@ -56,1 +65,5 @@\n-      _high_only_quick_internal = true;\n+      if (!CompilerConfig::has_c1() || !CompilerConfig::is_jvmci_compiler()) {\n+        print_mode_unavailable(\"high-only-quick-internal\", \"there is no c1 and jvmci compiler present\");\n+      } else {\n+        _mode = Mode::HIGH_ONLY_QUICK_INTERNAL;\n+      }\n@@ -58,1 +71,1 @@\n-      jio_fprintf(defaultStream::error_stream(), \"Unsupported compilation mode '%s', supported modes are: quick-only, high-only, high-only-quick-internal\\n\", CompilationMode);\n+      print_error();\n@@ -62,0 +75,12 @@\n+\n+  if (normal()) {\n+    if (CompilerConfig::is_c1_only()) {\n+      _mode = Mode::QUICK_ONLY;\n+    } else if (CompilerConfig::is_c2_or_jvmci_compiler_only()) {\n+      _mode = Mode::HIGH_ONLY;\n+    } else if (CompilerConfig::is_jvmci_compiler() && !TieredCompilation) {\n+      warning(\"Disabling tiered compilation with non-native JVMCI compiler is not recommended, \"\n+              \"disabling intermediate compilation levels instead. \");\n+      _mode = Mode::HIGH_ONLY_QUICK_INTERNAL;\n+    }\n+  }\n@@ -65,17 +90,17 @@\n-#endif\n-\n-#if defined(COMPILER2)\n-CompLevel  CompLevel_highest_tier      = CompLevel_full_optimization;  \/\/ pure C2 and tiered or JVMCI and tiered\n-#elif defined(COMPILER1)\n-CompLevel  CompLevel_highest_tier      = CompLevel_simple;             \/\/ pure C1 or JVMCI\n-#else\n-CompLevel  CompLevel_highest_tier      = CompLevel_none;\n-#endif\n-\n-#if defined(COMPILER2)\n-CompMode  Compilation_mode             = CompMode_server;\n-#elif defined(COMPILER1)\n-CompMode  Compilation_mode             = CompMode_client;\n-#else\n-CompMode  Compilation_mode             = CompMode_none;\n-#endif\n+void CompilationModeFlag::print_error() {\n+  jio_fprintf(defaultStream::error_stream(), \"Unsupported compilation mode '%s', available modes are:\", CompilationMode);\n+  bool comma = false;\n+  if (CompilerConfig::has_c1()) {\n+    jio_fprintf(defaultStream::error_stream(), \"%s quick-only\", comma ? \",\" : \"\");\n+    comma = true;\n+  }\n+  if (CompilerConfig::has_c2() || CompilerConfig::has_jvmci()) {\n+    jio_fprintf(defaultStream::error_stream(), \"%s high-only\", comma ? \",\" : \"\");\n+    comma = true;\n+  }\n+  if (CompilerConfig::has_c1() && CompilerConfig::has_jvmci()) {\n+    jio_fprintf(defaultStream::error_stream(), \"%s high-only-quick-internal\", comma ? \",\" : \"\");\n+    comma = true;\n+  }\n+  jio_fprintf(defaultStream::error_stream(), \"\\n\");\n+}\n@@ -131,5 +156,3 @@\n-#ifdef TIERED\n-void set_client_compilation_mode() {\n-  Compilation_mode = CompMode_client;\n-  CompLevel_highest_tier = CompLevel_simple;\n-  FLAG_SET_ERGO(TieredCompilation, false);\n+void set_client_emulation_mode_flags() {\n+  CompilationModeFlag::set_quick_only();\n+\n@@ -173,6 +196,0 @@\n-  if (FLAG_IS_DEFAULT(CompileThreshold)) {\n-    FLAG_SET_ERGO(CompileThreshold, 1500);\n-  }\n-  if (FLAG_IS_DEFAULT(OnStackReplacePercentage)) {\n-    FLAG_SET_ERGO(OnStackReplacePercentage, 933);\n-  }\n@@ -184,1 +201,1 @@\n-bool compilation_mode_selected() {\n+bool CompilerConfig::is_compilation_mode_selected() {\n@@ -187,1 +204,2 @@\n-         !FLAG_IS_DEFAULT(UseAOT)\n+         !FLAG_IS_DEFAULT(UseAOT)            ||\n+         !FLAG_IS_DEFAULT(CompilationMode)\n@@ -192,5 +210,49 @@\n-void select_compilation_mode_ergonomically() {\n-#if defined(_WINDOWS) && !defined(_LP64)\n-  if (FLAG_IS_DEFAULT(NeverActAsServerClassMachine)) {\n-    FLAG_SET_ERGO(NeverActAsServerClassMachine, true);\n-  }\n+\n+void CompilerConfig::set_legacy_emulation_flags() {\n+  \/\/ Any legacy flags set?\n+  if (!FLAG_IS_DEFAULT(CompileThreshold)         ||\n+      !FLAG_IS_DEFAULT(OnStackReplacePercentage) ||\n+      !FLAG_IS_DEFAULT(InterpreterProfilePercentage)) {\n+\n+    \/\/ Note, we do not scale CompileThreshold before this because the tiered flags are\n+    \/\/ all going to be scaled further in set_compilation_policy_flags().\n+    const int threshold = CompileThreshold;\n+    const int profile_threshold = threshold * InterpreterProfilePercentage \/ 100;\n+    const int osr_threshold = threshold * OnStackReplacePercentage \/ 100;\n+    const int osr_profile_threshold = osr_threshold * InterpreterProfilePercentage \/ 100;\n+\n+    if (CompilerConfig::is_c1_only() || CompilerConfig::is_c2_or_jvmci_compiler_only()) {\n+      const int threshold_log = log2i_graceful(CompilerConfig::is_c1_only() ? threshold : profile_threshold);\n+      const int osr_threshold_log = log2i_graceful(CompilerConfig::is_c1_only() ? osr_threshold : osr_profile_threshold);\n+\n+      if (Tier0InvokeNotifyFreqLog > threshold_log) {\n+        FLAG_SET_ERGO(Tier0InvokeNotifyFreqLog, MAX2(0, threshold_log));\n+      }\n+\n+      \/\/ Note: Emulation oddity. The legacy policy limited the amount of callbacks from the\n+      \/\/ interpreter for backedge events to once every 1024 counter increments.\n+      \/\/ We simulate this behavior by limiting the backedge notification frequency to be\n+      \/\/ at least 2^10.\n+      if (Tier0BackedgeNotifyFreqLog > osr_threshold_log) {\n+        FLAG_SET_ERGO(Tier0BackedgeNotifyFreqLog, MAX2(10, osr_threshold_log));\n+      }\n+      \/\/ Adjust the tiered policy flags to approximate the legacy behavior.\n+      if (CompilerConfig::is_c1_only()) {\n+        FLAG_SET_ERGO(Tier3InvocationThreshold, threshold);\n+        FLAG_SET_ERGO(Tier3MinInvocationThreshold, threshold);\n+        FLAG_SET_ERGO(Tier3CompileThreshold, threshold);\n+        FLAG_SET_ERGO(Tier3BackEdgeThreshold, osr_threshold);\n+      } else {\n+        FLAG_SET_ERGO(Tier4InvocationThreshold, threshold);\n+        FLAG_SET_ERGO(Tier4MinInvocationThreshold, threshold);\n+        FLAG_SET_ERGO(Tier4CompileThreshold, threshold);\n+        FLAG_SET_ERGO(Tier4BackEdgeThreshold, osr_threshold);\n+        FLAG_SET_ERGO(Tier0ProfilingStartPercentage, InterpreterProfilePercentage);\n+      }\n+#if INCLUDE_AOT\n+      if (UseAOT) {\n+        FLAG_SET_ERGO(Tier3AOTInvocationThreshold, threshold);\n+        FLAG_SET_ERGO(Tier3AOTMinInvocationThreshold, threshold);\n+        FLAG_SET_ERGO(Tier3AOTCompileThreshold, threshold);\n+        FLAG_SET_ERGO(Tier3AOTBackEdgeThreshold, CompilerConfig::is_c1_only() ? osr_threshold : osr_profile_threshold);\n+      }\n@@ -198,2 +260,8 @@\n-  if (NeverActAsServerClassMachine) {\n-    set_client_compilation_mode();\n+    } else {\n+      \/\/ Normal tiered mode, ignore legacy flags\n+    }\n+  }\n+  \/\/ Scale CompileThreshold\n+  \/\/ CompileThresholdScaling == 0.0 is equivalent to -Xint and leaves CompileThreshold unchanged.\n+  if (!FLAG_IS_DEFAULT(CompileThresholdScaling) && CompileThresholdScaling > 0.0 && CompileThreshold > 0) {\n+    FLAG_SET_ERGO(CompileThreshold, scaled_compile_threshold(CompileThreshold));\n@@ -204,11 +272,13 @@\n-void CompilerConfig::set_tiered_flags() {\n-  \/\/ Increase the code cache size - tiered compiles a lot more.\n-  if (FLAG_IS_DEFAULT(ReservedCodeCacheSize)) {\n-    FLAG_SET_ERGO(ReservedCodeCacheSize,\n-                  MIN2(CODE_CACHE_DEFAULT_LIMIT, (size_t)ReservedCodeCacheSize * 5));\n-  }\n-  \/\/ Enable SegmentedCodeCache if TieredCompilation is enabled, ReservedCodeCacheSize >= 240M\n-  \/\/ and the code cache contains at least 8 pages (segmentation disables advantage of huge pages).\n-  if (FLAG_IS_DEFAULT(SegmentedCodeCache) && ReservedCodeCacheSize >= 240*M &&\n-      8 * CodeCache::page_size() <= ReservedCodeCacheSize) {\n-    FLAG_SET_ERGO(SegmentedCodeCache, true);\n+void CompilerConfig::set_compilation_policy_flags() {\n+  if (is_tiered()) {\n+    \/\/ Increase the code cache size - tiered compiles a lot more.\n+    if (FLAG_IS_DEFAULT(ReservedCodeCacheSize)) {\n+      FLAG_SET_ERGO(ReservedCodeCacheSize,\n+                    MIN2(CODE_CACHE_DEFAULT_LIMIT, (size_t)ReservedCodeCacheSize * 5));\n+    }\n+    \/\/ Enable SegmentedCodeCache if tiered compilation is enabled, ReservedCodeCacheSize >= 240M\n+    \/\/ and the code cache contains at least 8 pages (segmentation disables advantage of huge pages).\n+    if (FLAG_IS_DEFAULT(SegmentedCodeCache) && ReservedCodeCacheSize >= 240*M &&\n+        8 * CodeCache::page_size() <= ReservedCodeCacheSize) {\n+      FLAG_SET_ERGO(SegmentedCodeCache, true);\n+    }\n@@ -216,0 +286,1 @@\n+\n@@ -229,0 +300,30 @@\n+\n+#if INCLUDE_AOT\n+    if (UseAOT) {\n+      if (FLAG_IS_DEFAULT(Tier3AOTInvocationThreshold)) {\n+        FLAG_SET_DEFAULT(Tier3AOTInvocationThreshold, 200);\n+      }\n+      if (FLAG_IS_DEFAULT(Tier3AOTMinInvocationThreshold)) {\n+        FLAG_SET_DEFAULT(Tier3AOTMinInvocationThreshold, 100);\n+      }\n+      if (FLAG_IS_DEFAULT(Tier3AOTCompileThreshold)) {\n+        FLAG_SET_DEFAULT(Tier3AOTCompileThreshold, 2000);\n+      }\n+      if (FLAG_IS_DEFAULT(Tier3AOTBackEdgeThreshold)) {\n+        FLAG_SET_DEFAULT(Tier3AOTBackEdgeThreshold, 2000);\n+      }\n+    }\n+#endif\n+\n+    if (FLAG_IS_DEFAULT(Tier4InvocationThreshold)) {\n+      FLAG_SET_DEFAULT(Tier4InvocationThreshold, 5000);\n+    }\n+    if (FLAG_IS_DEFAULT(Tier4MinInvocationThreshold)) {\n+      FLAG_SET_DEFAULT(Tier4MinInvocationThreshold, 600);\n+    }\n+    if (FLAG_IS_DEFAULT(Tier4CompileThreshold)) {\n+      FLAG_SET_DEFAULT(Tier4CompileThreshold, 10000);\n+    }\n+    if (FLAG_IS_DEFAULT(Tier4BackEdgeThreshold)) {\n+      FLAG_SET_DEFAULT(Tier4BackEdgeThreshold, 15000);\n+    }\n@@ -257,23 +358,0 @@\n-\n-    if (CompilationModeFlag::disable_intermediate()) {\n-      FLAG_SET_ERGO(Tier40InvocationThreshold, scaled_compile_threshold(Tier40InvocationThreshold));\n-      FLAG_SET_ERGO(Tier40MinInvocationThreshold, scaled_compile_threshold(Tier40MinInvocationThreshold));\n-      FLAG_SET_ERGO(Tier40CompileThreshold, scaled_compile_threshold(Tier40CompileThreshold));\n-      FLAG_SET_ERGO(Tier40BackEdgeThreshold, scaled_compile_threshold(Tier40BackEdgeThreshold));\n-    }\n-\n-#if INCLUDE_AOT\n-    if (UseAOT) {\n-      FLAG_SET_ERGO(Tier3AOTInvocationThreshold, scaled_compile_threshold(Tier3AOTInvocationThreshold));\n-      FLAG_SET_ERGO(Tier3AOTMinInvocationThreshold, scaled_compile_threshold(Tier3AOTMinInvocationThreshold));\n-      FLAG_SET_ERGO(Tier3AOTCompileThreshold, scaled_compile_threshold(Tier3AOTCompileThreshold));\n-      FLAG_SET_ERGO(Tier3AOTBackEdgeThreshold, scaled_compile_threshold(Tier3AOTBackEdgeThreshold));\n-\n-      if (CompilationModeFlag::disable_intermediate()) {\n-        FLAG_SET_ERGO(Tier0AOTInvocationThreshold, scaled_compile_threshold(Tier0AOTInvocationThreshold));\n-        FLAG_SET_ERGO(Tier0AOTMinInvocationThreshold, scaled_compile_threshold(Tier0AOTMinInvocationThreshold));\n-        FLAG_SET_ERGO(Tier0AOTCompileThreshold, scaled_compile_threshold(Tier0AOTCompileThreshold));\n-        FLAG_SET_ERGO(Tier0AOTBackEdgeThreshold, scaled_compile_threshold(Tier0AOTBackEdgeThreshold));\n-      }\n-    }\n-#endif \/\/ INCLUDE_AOT\n@@ -282,0 +360,1 @@\n+#ifdef COMPILER1\n@@ -285,1 +364,1 @@\n-      TieredStopAtLevel == CompLevel_full_optimization && !CompilationModeFlag::quick_only()) {\n+      TieredStopAtLevel == CompLevel_full_optimization && !CompilerConfig::is_c1_only()) {\n@@ -288,1 +367,18 @@\n-}\n+#endif\n+\n+  if (CompilerConfig::is_tiered() && CompilerConfig::is_c2_available()) {\n+#ifdef COMPILER2\n+    \/\/ Some inlining tuning\n+#ifdef X86\n+    if (FLAG_IS_DEFAULT(InlineSmallCode)) {\n+      FLAG_SET_DEFAULT(InlineSmallCode, 2500);\n+    }\n+#endif\n+\n+#if defined AARCH64\n+    if (FLAG_IS_DEFAULT(InlineSmallCode)) {\n+      FLAG_SET_DEFAULT(InlineSmallCode, 2500);\n+    }\n+#endif\n+#endif \/\/ COMPILER2\n+  }\n@@ -290,1 +386,1 @@\n-#endif \/\/ TIERED\n+}\n@@ -293,1 +389,1 @@\n-void set_jvmci_specific_flags() {\n+void CompilerConfig::set_jvmci_specific_flags() {\n@@ -295,2 +391,0 @@\n-    Compilation_mode = CompMode_server;\n-\n@@ -320,20 +414,0 @@\n-#ifdef TIERED\n-      if (!TieredCompilation) {\n-         warning(\"Disabling tiered compilation with non-native JVMCI compiler is not recommended. \"\n-                 \"Turning on tiered compilation and disabling intermediate compilation levels instead. \");\n-         FLAG_SET_ERGO(TieredCompilation, true);\n-         if (CompilationModeFlag::normal()) {\n-           CompilationModeFlag::set_high_only_quick_internal(true);\n-         }\n-         if (CICompilerCount < 2 && CompilationModeFlag::quick_internal()) {\n-            warning(\"Increasing number of compiler threads for JVMCI compiler.\");\n-            FLAG_SET_ERGO(CICompilerCount, 2);\n-         }\n-      }\n-#else \/\/ TIERED\n-      \/\/ Adjust the on stack replacement percentage to avoid early\n-      \/\/ OSR compilations while JVMCI itself is warming up\n-      if (FLAG_IS_DEFAULT(OnStackReplacePercentage)) {\n-        FLAG_SET_DEFAULT(OnStackReplacePercentage, 933);\n-      }\n-#endif \/\/ !TIERED\n@@ -440,0 +514,1 @@\n+\n@@ -444,3 +519,3 @@\n-  if (Arguments::is_interpreter_only()) {\n-    return; \/\/ Nothing to do.\n-  }\n+#if !COMPILER1_OR_COMPILER2\n+  return;\n+#endif\n@@ -448,4 +523,5 @@\n-#ifdef TIERED\n-  if (!compilation_mode_selected()) {\n-    select_compilation_mode_ergonomically();\n-  }\n+  if (!is_compilation_mode_selected()) {\n+#if defined(_WINDOWS) && !defined(_LP64)\n+    if (FLAG_IS_DEFAULT(NeverActAsServerClassMachine)) {\n+      FLAG_SET_ERGO(NeverActAsServerClassMachine, true);\n+    }\n@@ -453,0 +529,7 @@\n+    if (NeverActAsServerClassMachine) {\n+      set_client_emulation_mode_flags();\n+    }\n+  }\n+\n+  set_legacy_emulation_flags();\n+  set_compilation_policy_flags();\n@@ -463,13 +546,0 @@\n-#ifdef TIERED\n-  if (TieredCompilation) {\n-    set_tiered_flags();\n-  } else\n-#endif\n-  {\n-    \/\/ Scale CompileThreshold\n-    \/\/ CompileThresholdScaling == 0.0 is equivalent to -Xint and leaves CompileThreshold unchanged.\n-    if (!FLAG_IS_DEFAULT(CompileThresholdScaling) && CompileThresholdScaling > 0.0) {\n-      FLAG_SET_ERGO(CompileThreshold, scaled_compile_threshold(CompileThreshold));\n-    }\n-  }\n-\n@@ -488,0 +558,7 @@\n+  if (ProfileInterpreter && CompilerConfig::is_c1_simple_only()) {\n+    if (!FLAG_IS_DEFAULT(ProfileInterpreter)) {\n+        warning(\"ProfileInterpreter disabled due to client emulation mode\");\n+    }\n+    FLAG_SET_CMDLINE(ProfileInterpreter, false);\n+  }\n+\n@@ -519,38 +596,0 @@\n-static CompLevel highest_compile_level() {\n-  return TieredCompilation ? MIN2((CompLevel) TieredStopAtLevel, CompLevel_highest_tier) : CompLevel_highest_tier;\n-}\n-\n-bool is_c1_or_interpreter_only() {\n-  if (Arguments::is_interpreter_only()) {\n-    return true;\n-  }\n-\n-#if INCLUDE_AOT\n-  if (UseAOT) {\n-    return false;\n-  }\n-#endif\n-\n-  if (highest_compile_level() < CompLevel_full_optimization) {\n-#if INCLUDE_JVMCI\n-    if (TieredCompilation) {\n-       return true;\n-    }\n-    \/\/ This happens on jvm variant with C2 disabled and JVMCI\n-    \/\/ enabled.\n-    return !UseJVMCICompiler;\n-#else\n-    return true;\n-#endif\n-  }\n-\n-#ifdef TIERED\n-  \/\/ The quick-only compilation mode is c1 only. However,\n-  \/\/ CompilationModeFlag only takes effect with TieredCompilation\n-  \/\/ enabled.\n-  if (TieredCompilation && CompilationModeFlag::quick_only()) {\n-    return true;\n-  }\n-#endif\n-  return false;\n-}\n","filename":"src\/hotspot\/share\/compiler\/compilerDefinitions.cpp","additions":205,"deletions":166,"binary":false,"changes":371,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,0 +28,2 @@\n+#include \"compiler\/compiler_globals.hpp\"\n+#include \"jvmci\/jvmci_globals.hpp\"\n@@ -29,0 +31,1 @@\n+#include \"runtime\/arguments.hpp\"\n@@ -65,1 +68,0 @@\n-#ifdef TIERED\n@@ -67,4 +69,8 @@\n-  static bool _quick_only;\n-  static bool _high_only;\n-  static bool _high_only_quick_internal;\n-\n+  enum class Mode {\n+    NORMAL,\n+    QUICK_ONLY,\n+    HIGH_ONLY,\n+    HIGH_ONLY_QUICK_INTERNAL\n+  };\n+  static Mode _mode;\n+  static void print_error();\n@@ -73,4 +79,4 @@\n-  static bool normal()                   { return !quick_only() && !high_only() && !high_only_quick_internal(); }\n-  static bool quick_only()               { return _quick_only;               }\n-  static bool high_only()                { return _high_only;                }\n-  static bool high_only_quick_internal() { return _high_only_quick_internal; }\n+  static bool normal()                   { return _mode == Mode::NORMAL;                   }\n+  static bool quick_only()               { return _mode == Mode::QUICK_ONLY;               }\n+  static bool high_only()                { return _mode == Mode::HIGH_ONLY;                }\n+  static bool high_only_quick_internal() { return _mode == Mode::HIGH_ONLY_QUICK_INTERNAL; }\n@@ -81,1 +87,3 @@\n-  static void set_high_only_quick_internal(bool x) { _high_only_quick_internal = x; }\n+  static void set_high_only_quick_internal() { _mode = Mode::HIGH_ONLY_QUICK_INTERNAL; }\n+  static void set_quick_only()               { _mode = Mode::QUICK_ONLY;               }\n+  static void set_high_only()                { _mode = Mode::HIGH_ONLY;                }\n@@ -83,19 +91,0 @@\n-#endif\n-\n-extern CompLevel CompLevel_highest_tier;\n-\n-enum CompMode {\n-  CompMode_none = 0,\n-  CompMode_client = 1,\n-  CompMode_server = 2\n-};\n-\n-extern CompMode Compilation_mode;\n-\n-inline bool is_server_compilation_mode_vm() {\n-  return Compilation_mode == CompMode_server;\n-}\n-\n-inline bool is_client_compilation_mode_vm() {\n-  return Compilation_mode == CompMode_client;\n-}\n@@ -111,4 +100,0 @@\n-inline bool is_highest_tier_compile(int comp_level) {\n-  return comp_level == CompLevel_highest_tier;\n-}\n-\n@@ -119,1 +104,0 @@\n-bool is_c1_or_interpreter_only();\n@@ -152,0 +136,109 @@\n+  \/\/ Which compilers are baked in?\n+  constexpr static bool has_c1()     { return COMPILER1_PRESENT(true) NOT_COMPILER1(false); }\n+  constexpr static bool has_c2()     { return COMPILER2_PRESENT(true) NOT_COMPILER2(false); }\n+  constexpr static bool has_jvmci()  { return JVMCI_ONLY(true) NOT_JVMCI(false);            }\n+  constexpr static bool has_tiered() { return has_c1() && (has_c2() || has_jvmci());        }\n+  constexpr static bool has_aot()    { return AOT_ONLY(true) NOT_AOT(false);                }\n+\n+  \/\/ is_*_only() functions describe situations in which the JVM is in one way or another\n+  \/\/ forced to use a particular compiler or their combination. The constraint functions\n+  \/\/ deliberately ignore the fact that there may also be AOT methods and methods installed\n+  \/\/ through JVMCI (where the JVMCI compiler was invoked not through the broker). Be sure\n+  \/\/ to check for those (using is_jvmci() and is_aot()) in situations where it matters.\n+  \/\/\n+\n+  \/\/ Is the JVM in a configuration that permits only c1-compiled methods (level 1,2,3)?\n+  static bool is_c1_only() {\n+    if (!is_interpreter_only() && has_c1()) {\n+      const bool c1_only = !has_c2() && !is_jvmci_compiler();\n+      const bool tiered_degraded_to_c1_only = TieredStopAtLevel >= CompLevel_simple && TieredStopAtLevel < CompLevel_full_optimization;\n+      const bool c1_only_compilation_mode = CompilationModeFlag::quick_only();\n+      return c1_only || tiered_degraded_to_c1_only || c1_only_compilation_mode;\n+    }\n+    return false;\n+  }\n+\n+  \/\/ Is the JVM in a configuration that permits only c1-compiled methods at level 1?\n+  static bool is_c1_simple_only() {\n+    if (is_c1_only()) {\n+      const bool tiered_degraded_to_level_1 = TieredStopAtLevel == CompLevel_simple;\n+      const bool c1_only_compilation_mode = CompilationModeFlag::quick_only();\n+      return tiered_degraded_to_level_1 || c1_only_compilation_mode;\n+    }\n+    return false;\n+  }\n+\n+  static bool is_c2_available() {\n+    return has_c2() && !is_interpreter_only() && !is_c1_only() && !is_jvmci_compiler();\n+  }\n+\n+  static bool is_jvmci_compiler_available() {\n+    return is_jvmci_compiler() && !is_interpreter_only() && !is_c1_only();\n+  }\n+  \/\/ Is the JVM in a configuration that permits only c2-compiled methods?\n+  \/\/ JVMCI compiler replaces C2.\n+  static bool is_c2_only() {\n+    if (is_c2_available()) {\n+      const bool c2_only = !has_c1();\n+      \/\/ There is no JVMCI compiler to replace C2 in the broker, and the user (or ergonomics)\n+      \/\/ is forcing C1 off.\n+      const bool c2_only_compilation_mode = CompilationModeFlag::high_only();\n+      const bool tiered_off = !TieredCompilation;\n+      return c2_only || c2_only_compilation_mode || tiered_off;\n+    }\n+    return false;\n+  }\n+\n+  \/\/ Is the JVM in a configuration that permits only jvmci-compiled methods?\n+  static bool is_jvmci_compiler_only() {\n+    if (is_jvmci_compiler_available()) {\n+      const bool jvmci_compiler_only = !has_c1();\n+      \/\/ JVMCI compiler replaced C2 and the user (or ergonomics) is forcing C1 off.\n+      const bool jvmci_only_compilation_mode = CompilationModeFlag::high_only();\n+      const bool tiered_off = !TieredCompilation;\n+      return jvmci_compiler_only || jvmci_only_compilation_mode || tiered_off;\n+    }\n+    return false;\n+  }\n+\n+  static bool is_c2_or_jvmci_compiler_only() {\n+    return is_c2_only() || is_jvmci_compiler_only();\n+  }\n+\n+  \/\/ Tiered is basically C1 & (C2 | JVMCI) minus all the odd cases with restrictions.\n+  static bool is_tiered() {\n+    assert(is_c1_simple_only() && is_c1_only() || !is_c1_simple_only(), \"c1 simple mode must imply c1-only mode\");\n+    return has_tiered() && !is_interpreter_only() && !is_c1_only() && !is_c2_or_jvmci_compiler_only();\n+  }\n+\n+  static bool is_c1_available() {\n+    return has_c1() && !is_interpreter_only() && !is_c2_or_jvmci_compiler_only();\n+  }\n+\n+  static bool is_c1_profiling() {\n+    const bool c1_only_profiling = is_c1_only() && !is_c1_simple_only();\n+    const bool tiered = is_tiered();\n+    return c1_only_profiling || tiered;\n+  }\n+\n+\n+  static bool is_c2_or_jvmci_compiler_available() {\n+    return is_c2_available() || is_jvmci_compiler_available();\n+  }\n+\n+  static bool is_aot()               { return AOT_ONLY(has_aot() && UseAOT) NOT_AOT(false);                 }\n+  static bool is_jvmci_compiler()    { return JVMCI_ONLY(has_jvmci() && UseJVMCICompiler) NOT_JVMCI(false); }\n+  static bool is_jvmci()             { return JVMCI_ONLY(has_jvmci() && EnableJVMCI) NOT_JVMCI(false);      }\n+  static bool is_interpreter_only() {\n+    return Arguments::is_interpreter_only() || TieredStopAtLevel == CompLevel_none;\n+  }\n+\n+  static bool is_c1_or_interpreter_only_no_aot_or_jvmci() {\n+    assert(is_jvmci_compiler() && is_jvmci() || !is_jvmci_compiler(), \"JVMCI compiler implies enabled JVMCI\");\n+    return !is_aot() && !is_jvmci() && (is_interpreter_only() || is_c1_only());\n+  }\n+\n+  static bool is_c1_only_no_aot_or_jvmci() {\n+    return is_c1_only() && !is_aot() && !is_jvmci();\n+  }\n+\n@@ -153,1 +246,4 @@\n-  TIERED_ONLY(static void set_tiered_flags();)\n+  static bool is_compilation_mode_selected();\n+  static void set_compilation_policy_flags();\n+  static void set_jvmci_specific_flags();\n+  static void set_legacy_emulation_flags();\n","filename":"src\/hotspot\/share\/compiler\/compilerDefinitions.hpp","additions":132,"deletions":36,"binary":false,"changes":168,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -545,1 +545,1 @@\n-  if (is_server_compilation_mode_vm()) {\n+  if (CompilerConfig::is_c2_available()) {\n","filename":"src\/hotspot\/share\/compiler\/compilerDirectives.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -195,1 +195,3 @@\n-          \"threshold if coming from AOT\")                                   \\\n+          \"threshold if coming from AOT;\"                                   \\\n+          \"with CompilationMode=high-only|high-only-quick-internal)\"        \\\n+          \"determines when to transition from AOT to interpreter\")          \\\n@@ -199,1 +201,3 @@\n-          \"Minimum invocation to compile at tier 3 if coming from AOT\")     \\\n+          \"Minimum invocation to compile at tier 3 if coming from AOT;\"     \\\n+          \"with CompilationMode=high-only|high-only-quick-internal)\"        \\\n+          \"determines when to transition from AOT to interpreter\")          \\\n@@ -204,1 +208,3 @@\n-          \"minimum must be satisfied) if coming from AOT\")                  \\\n+          \"minimum must be satisfied) if coming from AOT;\"                  \\\n+          \"with CompilationMode=high-only|high-only-quick-internal)\"        \\\n+          \"determines when to transition from AOT to interpreter\")          \\\n@@ -209,30 +215,3 @@\n-          \"if coming from AOT\")                                             \\\n-          range(0, max_jint)                                                \\\n-                                                                            \\\n-  product(intx, Tier0AOTInvocationThreshold, 200, DIAGNOSTIC,               \\\n-          \"Switch to interpreter to profile if the number of method \"       \\\n-          \"invocations crosses this threshold if coming from AOT \"          \\\n-          \"(applicable only with \"                                          \\\n-          \"CompilationMode=high-only|high-only-quick-internal)\")            \\\n-          range(0, max_jint)                                                \\\n-                                                                            \\\n-  product(intx, Tier0AOTMinInvocationThreshold, 100, DIAGNOSTIC,            \\\n-          \"Minimum number of invocations to switch to interpreter \"         \\\n-          \"to profile if coming from AOT \"                                  \\\n-          \"(applicable only with \"                                          \\\n-          \"CompilationMode=high-only|high-only-quick-internal)\")            \\\n-          range(0, max_jint)                                                \\\n-                                                                            \\\n-  product(intx, Tier0AOTCompileThreshold, 2000, DIAGNOSTIC,                 \\\n-          \"Threshold at which to switch to interpreter to profile \"         \\\n-          \"if coming from AOT \"                                             \\\n-          \"(invocation minimum must be satisfied, \"                         \\\n-          \"applicable only with \"                                           \\\n-          \"CompilationMode=high-only|high-only-quick-internal)\")            \\\n-          range(0, max_jint)                                                \\\n-                                                                            \\\n-  product(intx, Tier0AOTBackEdgeThreshold,  60000, DIAGNOSTIC,              \\\n-          \"Back edge threshold at which to switch to interpreter \"          \\\n-          \"to profile if coming from AOT \"                                  \\\n-          \"(applicable only with \"                                          \\\n-          \"CompilationMode=high-only|high-only-quick-internal)\")            \\\n+          \"if coming from AOT;\"                                             \\\n+          \"with CompilationMode=high-only|high-only-quick-internal)\"        \\\n+          \"determines when to transition from AOT to interpreter\")          \\\n@@ -259,25 +238,1 @@\n-  product(intx, Tier40InvocationThreshold, 5000, DIAGNOSTIC,                \\\n-          \"Compile if number of method invocations crosses this \"           \\\n-          \"threshold (applicable only with \"                                \\\n-          \"CompilationMode=high-only|high-only-quick-internal)\")            \\\n-          range(0, max_jint)                                                \\\n-                                                                            \\\n-  product(intx, Tier40MinInvocationThreshold, 600, DIAGNOSTIC,              \\\n-          \"Minimum number of invocations to compile at tier 4 \"             \\\n-          \"(applicable only with \"                                          \\\n-          \"CompilationMode=high-only|high-only-quick-internal)\")            \\\n-          range(0, max_jint)                                                \\\n-                                                                            \\\n-  product(intx, Tier40CompileThreshold, 10000, DIAGNOSTIC,                  \\\n-          \"Threshold at which tier 4 compilation is invoked (invocation \"   \\\n-          \"minimum must be satisfied, applicable only with \"                \\\n-          \"CompilationMode=high-only|high-only-quick-internal)\")            \\\n-          range(0, max_jint)                                                \\\n-                                                                            \\\n-  product(intx, Tier40BackEdgeThreshold, 15000, DIAGNOSTIC,                 \\\n-          \"Back edge threshold at which tier 4 OSR compilation is invoked \" \\\n-          \"(applicable only with \"                                          \\\n-          \"CompilationMode=high-only|high-only-quick-internal)\")            \\\n-          range(0, max_jint)                                                \\\n-                                                                            \\\n-  product(intx, Tier0Delay, 5, DIAGNOSTIC,                                  \\\n+  product(intx, Tier0Delay, 20, DIAGNOSTIC,                                 \\\n@@ -285,3 +240,1 @@\n-          \"do not start profiling in the interpreter \"                      \\\n-          \"(applicable only with \"                                          \\\n-          \"CompilationMode=high-only|high-only-quick-internal)\")            \\\n+          \"do not start profiling in the interpreter\")                      \\\n@@ -320,4 +273,3 @@\n-          \"Start profiling in interpreter if the counters exceed tier 3 \"   \\\n-          \"thresholds (tier 4 thresholds with \"                             \\\n-          \"CompilationMode=high-only|high-only-quick-internal)\"             \\\n-          \"by the specified percentage\")                                    \\\n+          \"Start profiling in interpreter if the counters exceed the \"      \\\n+          \"specified percentage of tier 3 thresholds (tier 4 thresholds \"   \\\n+          \"with CompilationMode=high-only|high-only-quick-internal)\")       \\\n","filename":"src\/hotspot\/share\/compiler\/compiler_globals.hpp","additions":17,"deletions":65,"binary":false,"changes":82,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -279,1 +279,1 @@\n-#ifndef TIERED\n+#ifndef COMPILER2\n@@ -286,1 +286,1 @@\n-#endif \/\/ !TIERED\n+#endif \/\/ !COMPILER2\n","filename":"src\/hotspot\/share\/compiler\/oopMap.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -1,1164 +0,0 @@\n-\/*\n- * Copyright (c) 2010, 2020, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"compiler\/compileBroker.hpp\"\n-#include \"compiler\/compilerOracle.hpp\"\n-#include \"compiler\/tieredThresholdPolicy.hpp\"\n-#include \"memory\/resourceArea.hpp\"\n-#include \"prims\/jvmtiExport.hpp\"\n-#include \"runtime\/arguments.hpp\"\n-#include \"runtime\/frame.inline.hpp\"\n-#include \"runtime\/globals_extension.hpp\"\n-#include \"runtime\/handles.inline.hpp\"\n-#include \"runtime\/safepoint.hpp\"\n-#include \"runtime\/safepointVerifiers.hpp\"\n-#include \"code\/scopeDesc.hpp\"\n-#include \"oops\/method.inline.hpp\"\n-#if INCLUDE_JVMCI\n-#include \"jvmci\/jvmci.hpp\"\n-#endif\n-\n-#ifdef TIERED\n-\n-#include \"c1\/c1_Compiler.hpp\"\n-#include \"opto\/c2compiler.hpp\"\n-\n-bool TieredThresholdPolicy::call_predicate_helper(const methodHandle& method, CompLevel cur_level, int i, int b, double scale) {\n-  double threshold_scaling;\n-  if (CompilerOracle::has_option_value(method, CompileCommand::CompileThresholdScaling, threshold_scaling)) {\n-    scale *= threshold_scaling;\n-  }\n-  switch(cur_level) {\n-  case CompLevel_aot:\n-    if (CompilationModeFlag::disable_intermediate()) {\n-      return (i >= Tier0AOTInvocationThreshold * scale) ||\n-             (i >= Tier0AOTMinInvocationThreshold * scale && i + b >= Tier0AOTCompileThreshold * scale);\n-    } else {\n-      return (i >= Tier3AOTInvocationThreshold * scale) ||\n-             (i >= Tier3AOTMinInvocationThreshold * scale && i + b >= Tier3AOTCompileThreshold * scale);\n-    }\n-  case CompLevel_none:\n-    if (CompilationModeFlag::disable_intermediate()) {\n-      return (i >= Tier40InvocationThreshold * scale) ||\n-             (i >= Tier40MinInvocationThreshold * scale && i + b >= Tier40CompileThreshold * scale);\n-    }\n-    \/\/ Fall through\n-  case CompLevel_limited_profile:\n-    return (i >= Tier3InvocationThreshold * scale) ||\n-           (i >= Tier3MinInvocationThreshold * scale && i + b >= Tier3CompileThreshold * scale);\n-  case CompLevel_full_profile:\n-   return (i >= Tier4InvocationThreshold * scale) ||\n-          (i >= Tier4MinInvocationThreshold * scale && i + b >= Tier4CompileThreshold * scale);\n-  default:\n-   return true;\n-  }\n-}\n-\n-bool TieredThresholdPolicy::loop_predicate_helper(const methodHandle& method, CompLevel cur_level, int i, int b, double scale) {\n-  double threshold_scaling;\n-  if (CompilerOracle::has_option_value(method, CompileCommand::CompileThresholdScaling, threshold_scaling)) {\n-    scale *= threshold_scaling;\n-  }\n-  switch(cur_level) {\n-  case CompLevel_aot:\n-    if (CompilationModeFlag::disable_intermediate()) {\n-      return b >= Tier0AOTBackEdgeThreshold * scale;\n-    } else {\n-      return b >= Tier3AOTBackEdgeThreshold * scale;\n-    }\n-  case CompLevel_none:\n-    if (CompilationModeFlag::disable_intermediate()) {\n-      return b >= Tier40BackEdgeThreshold * scale;\n-    }\n-    \/\/ Fall through\n-  case CompLevel_limited_profile:\n-    return b >= Tier3BackEdgeThreshold * scale;\n-  case CompLevel_full_profile:\n-    return b >= Tier4BackEdgeThreshold * scale;\n-  default:\n-    return true;\n-  }\n-}\n-\n-\/\/ Simple methods are as good being compiled with C1 as C2.\n-\/\/ Determine if a given method is such a case.\n-bool TieredThresholdPolicy::is_trivial(Method* method) {\n-  if (method->is_accessor() ||\n-      method->is_constant_getter()) {\n-    return true;\n-  }\n-  return false;\n-}\n-\n-bool TieredThresholdPolicy::force_comp_at_level_simple(const methodHandle& method) {\n-  if (CompilationModeFlag::quick_internal()) {\n-#if INCLUDE_JVMCI\n-    if (UseJVMCICompiler) {\n-      AbstractCompiler* comp = CompileBroker::compiler(CompLevel_full_optimization);\n-      if (comp != NULL && comp->is_jvmci() && ((JVMCICompiler*) comp)->force_comp_at_level_simple(method)) {\n-        return true;\n-      }\n-    }\n-#endif\n-  }\n-  return false;\n-}\n-\n-CompLevel TieredThresholdPolicy::comp_level(Method* method) {\n-  CompiledMethod *nm = method->code();\n-  if (nm != NULL && nm->is_in_use()) {\n-    return (CompLevel)nm->comp_level();\n-  }\n-  return CompLevel_none;\n-}\n-\n-void TieredThresholdPolicy::print_counters(const char* prefix, Method* m) {\n-  int invocation_count = m->invocation_count();\n-  int backedge_count = m->backedge_count();\n-  MethodData* mdh = m->method_data();\n-  int mdo_invocations = 0, mdo_backedges = 0;\n-  int mdo_invocations_start = 0, mdo_backedges_start = 0;\n-  if (mdh != NULL) {\n-    mdo_invocations = mdh->invocation_count();\n-    mdo_backedges = mdh->backedge_count();\n-    mdo_invocations_start = mdh->invocation_count_start();\n-    mdo_backedges_start = mdh->backedge_count_start();\n-  }\n-  tty->print(\" %stotal=%d,%d %smdo=%d(%d),%d(%d)\", prefix,\n-      invocation_count, backedge_count, prefix,\n-      mdo_invocations, mdo_invocations_start,\n-      mdo_backedges, mdo_backedges_start);\n-  tty->print(\" %smax levels=%d,%d\", prefix,\n-      m->highest_comp_level(), m->highest_osr_comp_level());\n-}\n-\n-\/\/ Print an event.\n-void TieredThresholdPolicy::print_event(EventType type, Method* m, Method* im,\n-                                        int bci, CompLevel level) {\n-  bool inlinee_event = m != im;\n-\n-  ttyLocker tty_lock;\n-  tty->print(\"%lf: [\", os::elapsedTime());\n-\n-  switch(type) {\n-  case CALL:\n-    tty->print(\"call\");\n-    break;\n-  case LOOP:\n-    tty->print(\"loop\");\n-    break;\n-  case COMPILE:\n-    tty->print(\"compile\");\n-    break;\n-  case REMOVE_FROM_QUEUE:\n-    tty->print(\"remove-from-queue\");\n-    break;\n-  case UPDATE_IN_QUEUE:\n-    tty->print(\"update-in-queue\");\n-    break;\n-  case REPROFILE:\n-    tty->print(\"reprofile\");\n-    break;\n-  case MAKE_NOT_ENTRANT:\n-    tty->print(\"make-not-entrant\");\n-    break;\n-  default:\n-    tty->print(\"unknown\");\n-  }\n-\n-  tty->print(\" level=%d \", level);\n-\n-  ResourceMark rm;\n-  char *method_name = m->name_and_sig_as_C_string();\n-  tty->print(\"[%s\", method_name);\n-  if (inlinee_event) {\n-    char *inlinee_name = im->name_and_sig_as_C_string();\n-    tty->print(\" [%s]] \", inlinee_name);\n-  }\n-  else tty->print(\"] \");\n-  tty->print(\"@%d queues=%d,%d\", bci, CompileBroker::queue_size(CompLevel_full_profile),\n-                                      CompileBroker::queue_size(CompLevel_full_optimization));\n-\n-  tty->print(\" rate=\");\n-  if (m->prev_time() == 0) tty->print(\"n\/a\");\n-  else tty->print(\"%f\", m->rate());\n-\n-  tty->print(\" k=%.2lf,%.2lf\", threshold_scale(CompLevel_full_profile, Tier3LoadFeedback),\n-                               threshold_scale(CompLevel_full_optimization, Tier4LoadFeedback));\n-\n-  if (type != COMPILE) {\n-    print_counters(\"\", m);\n-    if (inlinee_event) {\n-      print_counters(\"inlinee \", im);\n-    }\n-    tty->print(\" compilable=\");\n-    bool need_comma = false;\n-    if (!m->is_not_compilable(CompLevel_full_profile)) {\n-      tty->print(\"c1\");\n-      need_comma = true;\n-    }\n-    if (!m->is_not_osr_compilable(CompLevel_full_profile)) {\n-      if (need_comma) tty->print(\",\");\n-      tty->print(\"c1-osr\");\n-      need_comma = true;\n-    }\n-    if (!m->is_not_compilable(CompLevel_full_optimization)) {\n-      if (need_comma) tty->print(\",\");\n-      tty->print(\"c2\");\n-      need_comma = true;\n-    }\n-    if (!m->is_not_osr_compilable(CompLevel_full_optimization)) {\n-      if (need_comma) tty->print(\",\");\n-      tty->print(\"c2-osr\");\n-    }\n-    tty->print(\" status=\");\n-    if (m->queued_for_compilation()) {\n-      tty->print(\"in-queue\");\n-    } else tty->print(\"idle\");\n-  }\n-  tty->print_cr(\"]\");\n-}\n-\n-\n-void TieredThresholdPolicy::initialize() {\n-  int count = CICompilerCount;\n-  bool c1_only = TieredStopAtLevel < CompLevel_full_optimization || CompilationModeFlag::quick_only();\n-  bool c2_only = CompilationModeFlag::high_only();\n-#ifdef _LP64\n-  \/\/ Turn on ergonomic compiler count selection\n-  if (FLAG_IS_DEFAULT(CICompilerCountPerCPU) && FLAG_IS_DEFAULT(CICompilerCount)) {\n-    FLAG_SET_DEFAULT(CICompilerCountPerCPU, true);\n-  }\n-  if (CICompilerCountPerCPU) {\n-    \/\/ Simple log n seems to grow too slowly for tiered, try something faster: log n * log log n\n-    int log_cpu = log2i(os::active_processor_count());\n-    int loglog_cpu = log2i(MAX2(log_cpu, 1));\n-    count = MAX2(log_cpu * loglog_cpu * 3 \/ 2, 2);\n-    \/\/ Make sure there is enough space in the code cache to hold all the compiler buffers\n-    size_t c1_size = Compiler::code_buffer_size();\n-    size_t c2_size = C2Compiler::initial_code_buffer_size();\n-    size_t buffer_size = c1_only ? c1_size : (c1_size\/3 + 2*c2_size\/3);\n-    int max_count = (ReservedCodeCacheSize - (CodeCacheMinimumUseSpace DEBUG_ONLY(* 3))) \/ (int)buffer_size;\n-    if (count > max_count) {\n-      \/\/ Lower the compiler count such that all buffers fit into the code cache\n-      count = MAX2(max_count, c1_only ? 1 : 2);\n-    }\n-    FLAG_SET_ERGO(CICompilerCount, count);\n-  }\n-#else\n-  \/\/ On 32-bit systems, the number of compiler threads is limited to 3.\n-  \/\/ On these systems, the virtual address space available to the JVM\n-  \/\/ is usually limited to 2-4 GB (the exact value depends on the platform).\n-  \/\/ As the compilers (especially C2) can consume a large amount of\n-  \/\/ memory, scaling the number of compiler threads with the number of\n-  \/\/ available cores can result in the exhaustion of the address space\n-  \/\/\/ available to the VM and thus cause the VM to crash.\n-  if (FLAG_IS_DEFAULT(CICompilerCount)) {\n-    count = 3;\n-    FLAG_SET_ERGO(CICompilerCount, count);\n-  }\n-#endif\n-\n-  if (c1_only) {\n-    \/\/ No C2 compiler thread required\n-    set_c1_count(count);\n-  } else if (c2_only) {\n-    set_c2_count(count);\n-  } else {\n-    set_c1_count(MAX2(count \/ 3, 1));\n-    set_c2_count(MAX2(count - c1_count(), 1));\n-  }\n-  assert(count == c1_count() + c2_count(), \"inconsistent compiler thread count\");\n-\n-  \/\/ Some inlining tuning\n-#ifdef X86\n-  if (FLAG_IS_DEFAULT(InlineSmallCode)) {\n-    FLAG_SET_DEFAULT(InlineSmallCode, 2500);\n-  }\n-#endif\n-\n-#if defined AARCH64\n-  if (FLAG_IS_DEFAULT(InlineSmallCode)) {\n-    FLAG_SET_DEFAULT(InlineSmallCode, 2500);\n-  }\n-#endif\n-\n-  set_increase_threshold_at_ratio();\n-  set_start_time(nanos_to_millis(os::javaTimeNanos()));\n-}\n-\n-\n-#ifdef ASSERT\n-bool TieredThresholdPolicy::verify_level(CompLevel level) {\n-  \/\/ AOT and interpreter levels are always valid.\n-  if (level == CompLevel_aot || level == CompLevel_none) {\n-    return true;\n-  }\n-  if (CompilationModeFlag::normal()) {\n-    return true;\n-  } else if (CompilationModeFlag::quick_only()) {\n-    return level == CompLevel_simple;\n-  } else if (CompilationModeFlag::high_only()) {\n-    return level == CompLevel_full_optimization;\n-  } else if (CompilationModeFlag::high_only_quick_internal()) {\n-    return level == CompLevel_full_optimization || level == CompLevel_simple;\n-  }\n-  return false;\n-}\n-#endif\n-\n-\n-CompLevel TieredThresholdPolicy::limit_level(CompLevel level) {\n-  if (CompilationModeFlag::quick_only()) {\n-    level = MIN2(level, CompLevel_simple);\n-  }\n-  assert(verify_level(level), \"Invalid compilation level %d\", level);\n-  if (level <= TieredStopAtLevel) {\n-    return level;\n-  }\n-  \/\/ Some compilation levels are not valid depending on a compilation mode:\n-  \/\/ a) quick_only - levels 2,3,4 are invalid; levels -1,0,1 are valid;\n-  \/\/ b) high_only - levels 1,2,3 are invalid; levels -1,0,4 are valid;\n-  \/\/ c) high_only_quick_internal - levels 2,3 are invalid; levels -1,0,1,4 are valid.\n-  \/\/ The invalid levels are actually sequential so a single comparison is sufficient.\n-  \/\/ Down here we already have (level > TieredStopAtLevel), which also implies that\n-  \/\/ (TieredStopAtLevel < Highest Possible Level), so we need to return a level that is:\n-  \/\/ a) a max level that is strictly less than the highest for a given compilation mode\n-  \/\/ b) less or equal to TieredStopAtLevel\n-  if (CompilationModeFlag::normal() || CompilationModeFlag::quick_only()) {\n-    return (CompLevel)TieredStopAtLevel;\n-  }\n-\n-  if (CompilationModeFlag::high_only() || CompilationModeFlag::high_only_quick_internal()) {\n-    return MIN2(CompLevel_none, (CompLevel)TieredStopAtLevel);\n-  }\n-\n-  ShouldNotReachHere();\n-  return CompLevel_any;\n-}\n-\n-CompLevel TieredThresholdPolicy::initial_compile_level_helper(const methodHandle& method) {\n-  if (CompilationModeFlag::normal()) {\n-    return CompLevel_full_profile;\n-  } else if (CompilationModeFlag::quick_only()) {\n-    return CompLevel_simple;\n-  } else if (CompilationModeFlag::high_only()) {\n-    return CompLevel_full_optimization;\n-  } else if (CompilationModeFlag::high_only_quick_internal()) {\n-    if (force_comp_at_level_simple(method)) {\n-      return CompLevel_simple;\n-    } else {\n-      return CompLevel_full_optimization;\n-    }\n-  }\n-  ShouldNotReachHere();\n-  return CompLevel_any;\n-}\n-\n-CompLevel TieredThresholdPolicy::initial_compile_level(const methodHandle& method) {\n-  return limit_level(initial_compile_level_helper(method));\n-}\n-\n-\/\/ Set carry flags on the counters if necessary\n-void TieredThresholdPolicy::handle_counter_overflow(Method* method) {\n-  MethodCounters *mcs = method->method_counters();\n-  if (mcs != NULL) {\n-    mcs->invocation_counter()->set_carry_on_overflow();\n-    mcs->backedge_counter()->set_carry_on_overflow();\n-  }\n-  MethodData* mdo = method->method_data();\n-  if (mdo != NULL) {\n-    mdo->invocation_counter()->set_carry_on_overflow();\n-    mdo->backedge_counter()->set_carry_on_overflow();\n-  }\n-}\n-\n-\/\/ Called with the queue locked and with at least one element\n-CompileTask* TieredThresholdPolicy::select_task(CompileQueue* compile_queue) {\n-  CompileTask *max_blocking_task = NULL;\n-  CompileTask *max_task = NULL;\n-  Method* max_method = NULL;\n-  jlong t = nanos_to_millis(os::javaTimeNanos());\n-  \/\/ Iterate through the queue and find a method with a maximum rate.\n-  for (CompileTask* task = compile_queue->first(); task != NULL;) {\n-    CompileTask* next_task = task->next();\n-    Method* method = task->method();\n-    \/\/ If a method was unloaded or has been stale for some time, remove it from the queue.\n-    \/\/ Blocking tasks and tasks submitted from whitebox API don't become stale\n-    if (task->is_unloaded() || (task->can_become_stale() && is_stale(t, TieredCompileTaskTimeout, method) && !is_old(method))) {\n-      if (!task->is_unloaded()) {\n-        if (PrintTieredEvents) {\n-          print_event(REMOVE_FROM_QUEUE, method, method, task->osr_bci(), (CompLevel) task->comp_level());\n-        }\n-        method->clear_queued_for_compilation();\n-      }\n-      compile_queue->remove_and_mark_stale(task);\n-      task = next_task;\n-      continue;\n-    }\n-    update_rate(t, method);\n-    if (max_task == NULL || compare_methods(method, max_method)) {\n-      \/\/ Select a method with the highest rate\n-      max_task = task;\n-      max_method = method;\n-    }\n-\n-    if (task->is_blocking()) {\n-      if (max_blocking_task == NULL || compare_methods(method, max_blocking_task->method())) {\n-        max_blocking_task = task;\n-      }\n-    }\n-\n-    task = next_task;\n-  }\n-\n-  if (max_blocking_task != NULL) {\n-    \/\/ In blocking compilation mode, the CompileBroker will make\n-    \/\/ compilations submitted by a JVMCI compiler thread non-blocking. These\n-    \/\/ compilations should be scheduled after all blocking compilations\n-    \/\/ to service non-compiler related compilations sooner and reduce the\n-    \/\/ chance of such compilations timing out.\n-    max_task = max_blocking_task;\n-    max_method = max_task->method();\n-  }\n-\n-  methodHandle max_method_h(Thread::current(), max_method);\n-\n-  if (max_task != NULL && max_task->comp_level() == CompLevel_full_profile &&\n-      TieredStopAtLevel > CompLevel_full_profile &&\n-      max_method != NULL && is_method_profiled(max_method_h)) {\n-    max_task->set_comp_level(CompLevel_limited_profile);\n-\n-    if (CompileBroker::compilation_is_complete(max_method_h, max_task->osr_bci(), CompLevel_limited_profile)) {\n-      if (PrintTieredEvents) {\n-        print_event(REMOVE_FROM_QUEUE, max_method, max_method, max_task->osr_bci(), (CompLevel)max_task->comp_level());\n-      }\n-      compile_queue->remove_and_mark_stale(max_task);\n-      max_method->clear_queued_for_compilation();\n-      return NULL;\n-    }\n-\n-    if (PrintTieredEvents) {\n-      print_event(UPDATE_IN_QUEUE, max_method, max_method, max_task->osr_bci(), (CompLevel)max_task->comp_level());\n-    }\n-  }\n-\n-  return max_task;\n-}\n-\n-void TieredThresholdPolicy::reprofile(ScopeDesc* trap_scope, bool is_osr) {\n-  for (ScopeDesc* sd = trap_scope;; sd = sd->sender()) {\n-    if (PrintTieredEvents) {\n-      print_event(REPROFILE, sd->method(), sd->method(), InvocationEntryBci, CompLevel_none);\n-    }\n-    MethodData* mdo = sd->method()->method_data();\n-    if (mdo != NULL) {\n-      mdo->reset_start_counters();\n-    }\n-    if (sd->is_top()) break;\n-  }\n-}\n-\n-nmethod* TieredThresholdPolicy::event(const methodHandle& method, const methodHandle& inlinee,\n-                                      int branch_bci, int bci, CompLevel comp_level, CompiledMethod* nm, TRAPS) {\n-  if (comp_level == CompLevel_none &&\n-      JvmtiExport::can_post_interpreter_events() &&\n-      THREAD->as_Java_thread()->is_interp_only_mode()) {\n-    return NULL;\n-  }\n-  if (ReplayCompiles) {\n-    \/\/ Don't trigger other compiles in testing mode\n-    return NULL;\n-  }\n-\n-  handle_counter_overflow(method());\n-  if (method() != inlinee()) {\n-    handle_counter_overflow(inlinee());\n-  }\n-\n-  if (PrintTieredEvents) {\n-    print_event(bci == InvocationEntryBci ? CALL : LOOP, method(), inlinee(), bci, comp_level);\n-  }\n-\n-  if (bci == InvocationEntryBci) {\n-    method_invocation_event(method, inlinee, comp_level, nm, THREAD);\n-  } else {\n-    \/\/ method == inlinee if the event originated in the main method\n-    method_back_branch_event(method, inlinee, bci, comp_level, nm, THREAD);\n-    \/\/ Check if event led to a higher level OSR compilation\n-    CompLevel expected_comp_level = MIN2(CompLevel_full_optimization, static_cast<CompLevel>(comp_level + 1));\n-    if (!CompilationModeFlag::disable_intermediate() && inlinee->is_not_osr_compilable(expected_comp_level)) {\n-      \/\/ It's not possble to reach the expected level so fall back to simple.\n-      expected_comp_level = CompLevel_simple;\n-    }\n-    CompLevel max_osr_level = static_cast<CompLevel>(inlinee->highest_osr_comp_level());\n-    if (max_osr_level >= expected_comp_level) { \/\/ fast check to avoid locking in a typical scenario\n-      nmethod* osr_nm = inlinee->lookup_osr_nmethod_for(bci, expected_comp_level, false);\n-      assert(osr_nm == NULL || osr_nm->comp_level() >= expected_comp_level, \"lookup_osr_nmethod_for is broken\");\n-      if (osr_nm != NULL && osr_nm->comp_level() != comp_level) {\n-        \/\/ Perform OSR with new nmethod\n-        return osr_nm;\n-      }\n-    }\n-  }\n-  return NULL;\n-}\n-\n-\/\/ Check if the method can be compiled, change level if necessary\n-void TieredThresholdPolicy::compile(const methodHandle& mh, int bci, CompLevel level, TRAPS) {\n-  assert(verify_level(level) && level <= TieredStopAtLevel, \"Invalid compilation level %d\", level);\n-\n-  if (level == CompLevel_none) {\n-    if (mh->has_compiled_code()) {\n-      \/\/ Happens when we switch from AOT to interpreter to profile.\n-      MutexLocker ml(Compile_lock);\n-      NoSafepointVerifier nsv;\n-      if (mh->has_compiled_code()) {\n-        mh->code()->make_not_used();\n-      }\n-      \/\/ Deoptimize immediately (we don't have to wait for a compile).\n-      JavaThread* jt = THREAD->as_Java_thread();\n-      RegisterMap map(jt, false);\n-      frame fr = jt->last_frame().sender(&map);\n-      Deoptimization::deoptimize_frame(jt, fr.id());\n-    }\n-    return;\n-  }\n-  if (level == CompLevel_aot) {\n-    if (mh->has_aot_code()) {\n-      if (PrintTieredEvents) {\n-        print_event(COMPILE, mh(), mh(), bci, level);\n-      }\n-      MutexLocker ml(Compile_lock);\n-      NoSafepointVerifier nsv;\n-      if (mh->has_aot_code() && mh->code() != mh->aot_code()) {\n-        mh->aot_code()->make_entrant();\n-        if (mh->has_compiled_code()) {\n-          mh->code()->make_not_entrant();\n-        }\n-        MutexLocker pl(CompiledMethod_lock, Mutex::_no_safepoint_check_flag);\n-        Method::set_code(mh, mh->aot_code());\n-      }\n-    }\n-    return;\n-  }\n-\n-  if (!CompilationModeFlag::disable_intermediate()) {\n-    \/\/ Check if the method can be compiled. If it cannot be compiled with C1, continue profiling\n-    \/\/ in the interpreter and then compile with C2 (the transition function will request that,\n-    \/\/ see common() ). If the method cannot be compiled with C2 but still can with C1, compile it with\n-    \/\/ pure C1.\n-    if ((bci == InvocationEntryBci && !can_be_compiled(mh, level))) {\n-      if (level == CompLevel_full_optimization && can_be_compiled(mh, CompLevel_simple)) {\n-        compile(mh, bci, CompLevel_simple, THREAD);\n-      }\n-      return;\n-    }\n-    if ((bci != InvocationEntryBci && !can_be_osr_compiled(mh, level))) {\n-      if (level == CompLevel_full_optimization && can_be_osr_compiled(mh, CompLevel_simple)) {\n-        nmethod* osr_nm = mh->lookup_osr_nmethod_for(bci, CompLevel_simple, false);\n-        if (osr_nm != NULL && osr_nm->comp_level() > CompLevel_simple) {\n-          \/\/ Invalidate the existing OSR nmethod so that a compile at CompLevel_simple is permitted.\n-          osr_nm->make_not_entrant();\n-        }\n-        compile(mh, bci, CompLevel_simple, THREAD);\n-      }\n-      return;\n-    }\n-  }\n-  if (bci != InvocationEntryBci && mh->is_not_osr_compilable(level)) {\n-    return;\n-  }\n-  if (!CompileBroker::compilation_is_in_queue(mh)) {\n-    if (PrintTieredEvents) {\n-      print_event(COMPILE, mh(), mh(), bci, level);\n-    }\n-    int hot_count = (bci == InvocationEntryBci) ? mh->invocation_count() : mh->backedge_count();\n-    update_rate(nanos_to_millis(os::javaTimeNanos()), mh());\n-    CompileBroker::compile_method(mh, bci, level, mh, hot_count, CompileTask::Reason_Tiered, THREAD);\n-  }\n-}\n-\n-\/\/ update_rate() is called from select_task() while holding a compile queue lock.\n-void TieredThresholdPolicy::update_rate(jlong t, Method* m) {\n-  \/\/ Skip update if counters are absent.\n-  \/\/ Can't allocate them since we are holding compile queue lock.\n-  if (m->method_counters() == NULL)  return;\n-\n-  if (is_old(m)) {\n-    \/\/ We don't remove old methods from the queue,\n-    \/\/ so we can just zero the rate.\n-    m->set_rate(0);\n-    return;\n-  }\n-\n-  \/\/ We don't update the rate if we've just came out of a safepoint.\n-  \/\/ delta_s is the time since last safepoint in milliseconds.\n-  jlong delta_s = t - SafepointTracing::end_of_last_safepoint_ms();\n-  jlong delta_t = t - (m->prev_time() != 0 ? m->prev_time() : start_time()); \/\/ milliseconds since the last measurement\n-  \/\/ How many events were there since the last time?\n-  int event_count = m->invocation_count() + m->backedge_count();\n-  int delta_e = event_count - m->prev_event_count();\n-\n-  \/\/ We should be running for at least 1ms.\n-  if (delta_s >= TieredRateUpdateMinTime) {\n-    \/\/ And we must've taken the previous point at least 1ms before.\n-    if (delta_t >= TieredRateUpdateMinTime && delta_e > 0) {\n-      m->set_prev_time(t);\n-      m->set_prev_event_count(event_count);\n-      m->set_rate((float)delta_e \/ (float)delta_t); \/\/ Rate is events per millisecond\n-    } else {\n-      if (delta_t > TieredRateUpdateMaxTime && delta_e == 0) {\n-        \/\/ If nothing happened for 25ms, zero the rate. Don't modify prev values.\n-        m->set_rate(0);\n-      }\n-    }\n-  }\n-}\n-\n-\/\/ Check if this method has been stale for a given number of milliseconds.\n-\/\/ See select_task().\n-bool TieredThresholdPolicy::is_stale(jlong t, jlong timeout, Method* m) {\n-  jlong delta_s = t - SafepointTracing::end_of_last_safepoint_ms();\n-  jlong delta_t = t - m->prev_time();\n-  if (delta_t > timeout && delta_s > timeout) {\n-    int event_count = m->invocation_count() + m->backedge_count();\n-    int delta_e = event_count - m->prev_event_count();\n-    \/\/ Return true if there were no events.\n-    return delta_e == 0;\n-  }\n-  return false;\n-}\n-\n-\/\/ We don't remove old methods from the compile queue even if they have\n-\/\/ very low activity. See select_task().\n-bool TieredThresholdPolicy::is_old(Method* method) {\n-  return method->invocation_count() > 50000 || method->backedge_count() > 500000;\n-}\n-\n-double TieredThresholdPolicy::weight(Method* method) {\n-  return (double)(method->rate() + 1) *\n-    (method->invocation_count() + 1) * (method->backedge_count() + 1);\n-}\n-\n-\/\/ Apply heuristics and return true if x should be compiled before y\n-bool TieredThresholdPolicy::compare_methods(Method* x, Method* y) {\n-  if (x->highest_comp_level() > y->highest_comp_level()) {\n-    \/\/ recompilation after deopt\n-    return true;\n-  } else\n-    if (x->highest_comp_level() == y->highest_comp_level()) {\n-      if (weight(x) > weight(y)) {\n-        return true;\n-      }\n-    }\n-  return false;\n-}\n-\n-\/\/ Is method profiled enough?\n-bool TieredThresholdPolicy::is_method_profiled(const methodHandle& method) {\n-  MethodData* mdo = method->method_data();\n-  if (mdo != NULL) {\n-    int i = mdo->invocation_count_delta();\n-    int b = mdo->backedge_count_delta();\n-    return call_predicate_helper(method, CompilationModeFlag::disable_intermediate() ? CompLevel_none : CompLevel_full_profile, i, b, 1);\n-  }\n-  return false;\n-}\n-\n-double TieredThresholdPolicy::threshold_scale(CompLevel level, int feedback_k) {\n-  int comp_count = compiler_count(level);\n-  if (comp_count > 0) {\n-    double queue_size = CompileBroker::queue_size(level);\n-    double k = queue_size \/ (feedback_k * comp_count) + 1;\n-\n-    \/\/ Increase C1 compile threshold when the code cache is filled more\n-    \/\/ than specified by IncreaseFirstTierCompileThresholdAt percentage.\n-    \/\/ The main intention is to keep enough free space for C2 compiled code\n-    \/\/ to achieve peak performance if the code cache is under stress.\n-    if (!CompilationModeFlag::disable_intermediate() && TieredStopAtLevel == CompLevel_full_optimization && level != CompLevel_full_optimization)  {\n-      double current_reverse_free_ratio = CodeCache::reverse_free_ratio(CodeCache::get_code_blob_type(level));\n-      if (current_reverse_free_ratio > _increase_threshold_at_ratio) {\n-        k *= exp(current_reverse_free_ratio - _increase_threshold_at_ratio);\n-      }\n-    }\n-    return k;\n-  }\n-  return 1;\n-}\n-\n-\/\/ Call and loop predicates determine whether a transition to a higher\n-\/\/ compilation level should be performed (pointers to predicate functions\n-\/\/ are passed to common()).\n-\/\/ Tier?LoadFeedback is basically a coefficient that determines of\n-\/\/ how many methods per compiler thread can be in the queue before\n-\/\/ the threshold values double.\n-bool TieredThresholdPolicy::loop_predicate(int i, int b, CompLevel cur_level, const methodHandle& method) {\n-  double k = 1;\n-  switch(cur_level) {\n-  case CompLevel_aot: {\n-    k = CompilationModeFlag::disable_intermediate() ? 1 : threshold_scale(CompLevel_full_profile, Tier3LoadFeedback);\n-    break;\n-  }\n-  case CompLevel_none: {\n-    if (CompilationModeFlag::disable_intermediate()) {\n-      k = threshold_scale(CompLevel_full_optimization, Tier4LoadFeedback);\n-      break;\n-    }\n-  }\n-  \/\/ Fall through\n-  case CompLevel_limited_profile: {\n-    k = threshold_scale(CompLevel_full_profile, Tier3LoadFeedback);\n-    break;\n-  }\n-  case CompLevel_full_profile: {\n-    k = threshold_scale(CompLevel_full_optimization, Tier4LoadFeedback);\n-    break;\n-  }\n-  default:\n-    return true;\n-  }\n-  return loop_predicate_helper(method, cur_level, i, b, k);\n-}\n-\n-bool TieredThresholdPolicy::call_predicate(int i, int b, CompLevel cur_level, const methodHandle& method) {\n-  double k = 1;\n-  switch(cur_level) {\n-  case CompLevel_aot: {\n-    k = CompilationModeFlag::disable_intermediate() ? 1 : threshold_scale(CompLevel_full_profile, Tier3LoadFeedback);\n-    break;\n-  }\n-  case CompLevel_none: {\n-    if (CompilationModeFlag::disable_intermediate()) {\n-      k = threshold_scale(CompLevel_full_optimization, Tier4LoadFeedback);\n-      break;\n-    }\n-  }\n-  \/\/ Fall through\n-  case CompLevel_limited_profile: {\n-    k = threshold_scale(CompLevel_full_profile, Tier3LoadFeedback);\n-    break;\n-  }\n-  case CompLevel_full_profile: {\n-    k = threshold_scale(CompLevel_full_optimization, Tier4LoadFeedback);\n-    break;\n-  }\n-  default:\n-    return true;\n-  }\n-  return call_predicate_helper(method, cur_level, i, b, k);\n-}\n-\n-\/\/ Determine is a method is mature.\n-bool TieredThresholdPolicy::is_mature(Method* method) {\n-  methodHandle mh(Thread::current(), method);\n-  if (is_trivial(method) || force_comp_at_level_simple(mh)) return true;\n-  MethodData* mdo = method->method_data();\n-  if (mdo != NULL) {\n-    int i = mdo->invocation_count();\n-    int b = mdo->backedge_count();\n-    double k = ProfileMaturityPercentage \/ 100.0;\n-    CompLevel main_profile_level = CompilationModeFlag::disable_intermediate() ? CompLevel_none : CompLevel_full_profile;\n-    return call_predicate_helper(mh, main_profile_level, i, b, k) || loop_predicate_helper(mh, main_profile_level, i, b, k);\n-  }\n-  return false;\n-}\n-\n-\/\/ If a method is old enough and is still in the interpreter we would want to\n-\/\/ start profiling without waiting for the compiled method to arrive.\n-\/\/ We also take the load on compilers into the account.\n-bool TieredThresholdPolicy::should_create_mdo(const methodHandle& method, CompLevel cur_level) {\n-  if (cur_level != CompLevel_none || force_comp_at_level_simple(method) || !ProfileInterpreter) {\n-    return false;\n-  }\n-  int i = method->invocation_count();\n-  int b = method->backedge_count();\n-  double k = Tier0ProfilingStartPercentage \/ 100.0;\n-\n-  \/\/ If the top level compiler is not keeping up, delay profiling.\n-  if (CompileBroker::queue_size(CompLevel_full_optimization) <=  (CompilationModeFlag::disable_intermediate() ? Tier0Delay : Tier3DelayOn) * compiler_count(CompLevel_full_optimization)) {\n-    return call_predicate_helper(method, CompLevel_none, i, b, k) || loop_predicate_helper(method, CompLevel_none, i, b, k);\n-  }\n-  return false;\n-}\n-\n-\/\/ Inlining control: if we're compiling a profiled method with C1 and the callee\n-\/\/ is known to have OSRed in a C2 version, don't inline it.\n-bool TieredThresholdPolicy::should_not_inline(ciEnv* env, ciMethod* callee) {\n-  CompLevel comp_level = (CompLevel)env->comp_level();\n-  if (comp_level == CompLevel_full_profile ||\n-      comp_level == CompLevel_limited_profile) {\n-    return callee->highest_osr_comp_level() == CompLevel_full_optimization;\n-  }\n-  return false;\n-}\n-\n-\/\/ Create MDO if necessary.\n-void TieredThresholdPolicy::create_mdo(const methodHandle& mh, Thread* THREAD) {\n-  if (mh->is_native() ||\n-      mh->is_abstract() ||\n-      mh->is_accessor() ||\n-      mh->is_constant_getter()) {\n-    return;\n-  }\n-  if (mh->method_data() == NULL) {\n-    Method::build_interpreter_method_data(mh, CHECK_AND_CLEAR);\n-  }\n-  if (ProfileInterpreter) {\n-    MethodData* mdo = mh->method_data();\n-    if (mdo != NULL) {\n-      JavaThread* jt = THREAD->as_Java_thread();\n-      frame last_frame = jt->last_frame();\n-      if (last_frame.is_interpreted_frame() && mh == last_frame.interpreter_frame_method()) {\n-        int bci = last_frame.interpreter_frame_bci();\n-        address dp = mdo->bci_to_dp(bci);\n-        last_frame.interpreter_frame_set_mdp(dp);\n-      }\n-    }\n-  }\n-}\n-\n-\n-\/*\n- * Method states:\n- *   0 - interpreter (CompLevel_none)\n- *   1 - pure C1 (CompLevel_simple)\n- *   2 - C1 with invocation and backedge counting (CompLevel_limited_profile)\n- *   3 - C1 with full profiling (CompLevel_full_profile)\n- *   4 - C2 or Graal (CompLevel_full_optimization)\n- *\n- * Common state transition patterns:\n- * a. 0 -> 3 -> 4.\n- *    The most common path. But note that even in this straightforward case\n- *    profiling can start at level 0 and finish at level 3.\n- *\n- * b. 0 -> 2 -> 3 -> 4.\n- *    This case occurs when the load on C2 is deemed too high. So, instead of transitioning\n- *    into state 3 directly and over-profiling while a method is in the C2 queue we transition to\n- *    level 2 and wait until the load on C2 decreases. This path is disabled for OSRs.\n- *\n- * c. 0 -> (3->2) -> 4.\n- *    In this case we enqueue a method for compilation at level 3, but the C1 queue is long enough\n- *    to enable the profiling to fully occur at level 0. In this case we change the compilation level\n- *    of the method to 2 while the request is still in-queue, because it'll allow it to run much faster\n- *    without full profiling while c2 is compiling.\n- *\n- * d. 0 -> 3 -> 1 or 0 -> 2 -> 1.\n- *    After a method was once compiled with C1 it can be identified as trivial and be compiled to\n- *    level 1. These transition can also occur if a method can't be compiled with C2 but can with C1.\n- *\n- * e. 0 -> 4.\n- *    This can happen if a method fails C1 compilation (it will still be profiled in the interpreter)\n- *    or because of a deopt that didn't require reprofiling (compilation won't happen in this case because\n- *    the compiled version already exists).\n- *\n- * Note that since state 0 can be reached from any other state via deoptimization different loops\n- * are possible.\n- *\n- *\/\n-\n-\/\/ Common transition function. Given a predicate determines if a method should transition to another level.\n-CompLevel TieredThresholdPolicy::common(Predicate p, const methodHandle& method, CompLevel cur_level, bool disable_feedback) {\n-  CompLevel next_level = cur_level;\n-  int i = method->invocation_count();\n-  int b = method->backedge_count();\n-\n-  if (force_comp_at_level_simple(method)) {\n-    next_level = CompLevel_simple;\n-  } else {\n-    if (!CompilationModeFlag::disable_intermediate() && is_trivial(method())) {\n-      next_level = CompLevel_simple;\n-    } else {\n-      switch(cur_level) {\n-      default: break;\n-      case CompLevel_aot:\n-        if (CompilationModeFlag::disable_intermediate()) {\n-          if (disable_feedback || (CompileBroker::queue_size(CompLevel_full_optimization) <=\n-                                   Tier0Delay * compiler_count(CompLevel_full_optimization) &&\n-                                  (this->*p)(i, b, cur_level, method))) {\n-            next_level = CompLevel_none;\n-          }\n-        } else {\n-          \/\/ If we were at full profile level, would we switch to full opt?\n-          if (common(p, method, CompLevel_full_profile, disable_feedback) == CompLevel_full_optimization) {\n-            next_level = CompLevel_full_optimization;\n-          } else if (disable_feedback || (CompileBroker::queue_size(CompLevel_full_optimization) <=\n-                                          Tier3DelayOff * compiler_count(CompLevel_full_optimization) &&\n-                                         (this->*p)(i, b, cur_level, method))) {\n-            next_level = CompLevel_full_profile;\n-          }\n-        }\n-        break;\n-      case CompLevel_none:\n-        if (CompilationModeFlag::disable_intermediate()) {\n-          MethodData* mdo = method->method_data();\n-          if (mdo != NULL) {\n-            \/\/ If mdo exists that means we are in a normal profiling mode.\n-            int mdo_i = mdo->invocation_count_delta();\n-            int mdo_b = mdo->backedge_count_delta();\n-            if ((this->*p)(mdo_i, mdo_b, cur_level, method)) {\n-              next_level = CompLevel_full_optimization;\n-            }\n-          }\n-        } else {\n-          \/\/ If we were at full profile level, would we switch to full opt?\n-          if (common(p, method, CompLevel_full_profile, disable_feedback) == CompLevel_full_optimization) {\n-            next_level = CompLevel_full_optimization;\n-          } else if ((this->*p)(i, b, cur_level, method)) {\n-  #if INCLUDE_JVMCI\n-            if (EnableJVMCI && UseJVMCICompiler) {\n-              \/\/ Since JVMCI takes a while to warm up, its queue inevitably backs up during\n-              \/\/ early VM execution. As of 2014-06-13, JVMCI's inliner assumes that the root\n-              \/\/ compilation method and all potential inlinees have mature profiles (which\n-              \/\/ includes type profiling). If it sees immature profiles, JVMCI's inliner\n-              \/\/ can perform pathologically bad (e.g., causing OutOfMemoryErrors due to\n-              \/\/ exploring\/inlining too many graphs). Since a rewrite of the inliner is\n-              \/\/ in progress, we simply disable the dialing back heuristic for now and will\n-              \/\/ revisit this decision once the new inliner is completed.\n-              next_level = CompLevel_full_profile;\n-            } else\n-  #endif\n-            {\n-              \/\/ C1-generated fully profiled code is about 30% slower than the limited profile\n-              \/\/ code that has only invocation and backedge counters. The observation is that\n-              \/\/ if C2 queue is large enough we can spend too much time in the fully profiled code\n-              \/\/ while waiting for C2 to pick the method from the queue. To alleviate this problem\n-              \/\/ we introduce a feedback on the C2 queue size. If the C2 queue is sufficiently long\n-              \/\/ we choose to compile a limited profiled version and then recompile with full profiling\n-              \/\/ when the load on C2 goes down.\n-              if (!disable_feedback && CompileBroker::queue_size(CompLevel_full_optimization) >\n-                  Tier3DelayOn * compiler_count(CompLevel_full_optimization)) {\n-                next_level = CompLevel_limited_profile;\n-              } else {\n-                next_level = CompLevel_full_profile;\n-              }\n-            }\n-          }\n-        }\n-        break;\n-      case CompLevel_limited_profile:\n-        if (is_method_profiled(method)) {\n-          \/\/ Special case: we got here because this method was fully profiled in the interpreter.\n-          next_level = CompLevel_full_optimization;\n-        } else {\n-          MethodData* mdo = method->method_data();\n-          if (mdo != NULL) {\n-            if (mdo->would_profile()) {\n-              if (disable_feedback || (CompileBroker::queue_size(CompLevel_full_optimization) <=\n-                                       Tier3DelayOff * compiler_count(CompLevel_full_optimization) &&\n-                                       (this->*p)(i, b, cur_level, method))) {\n-                next_level = CompLevel_full_profile;\n-              }\n-            } else {\n-              next_level = CompLevel_full_optimization;\n-            }\n-          } else {\n-            \/\/ If there is no MDO we need to profile\n-            if (disable_feedback || (CompileBroker::queue_size(CompLevel_full_optimization) <=\n-                                     Tier3DelayOff * compiler_count(CompLevel_full_optimization) &&\n-                                     (this->*p)(i, b, cur_level, method))) {\n-              next_level = CompLevel_full_profile;\n-            }\n-          }\n-        }\n-        break;\n-      case CompLevel_full_profile:\n-        {\n-          MethodData* mdo = method->method_data();\n-          if (mdo != NULL) {\n-            if (mdo->would_profile()) {\n-              int mdo_i = mdo->invocation_count_delta();\n-              int mdo_b = mdo->backedge_count_delta();\n-              if ((this->*p)(mdo_i, mdo_b, cur_level, method)) {\n-                next_level = CompLevel_full_optimization;\n-              }\n-            } else {\n-              next_level = CompLevel_full_optimization;\n-            }\n-          }\n-        }\n-        break;\n-      }\n-    }\n-  }\n-  return limit_level(next_level);\n-}\n-\n-\n-\n-\/\/ Determine if a method should be compiled with a normal entry point at a different level.\n-CompLevel TieredThresholdPolicy::call_event(const methodHandle& method, CompLevel cur_level, Thread* thread) {\n-  CompLevel osr_level = MIN2((CompLevel) method->highest_osr_comp_level(),\n-                             common(&TieredThresholdPolicy::loop_predicate, method, cur_level, true));\n-  CompLevel next_level = common(&TieredThresholdPolicy::call_predicate, method, cur_level);\n-\n-  \/\/ If OSR method level is greater than the regular method level, the levels should be\n-  \/\/ equalized by raising the regular method level in order to avoid OSRs during each\n-  \/\/ invocation of the method.\n-  if (osr_level == CompLevel_full_optimization && cur_level == CompLevel_full_profile) {\n-    MethodData* mdo = method->method_data();\n-    guarantee(mdo != NULL, \"MDO should not be NULL\");\n-    if (mdo->invocation_count() >= 1) {\n-      next_level = CompLevel_full_optimization;\n-    }\n-  } else {\n-    next_level = MAX2(osr_level, next_level);\n-  }\n-  return next_level;\n-}\n-\n-\/\/ Determine if we should do an OSR compilation of a given method.\n-CompLevel TieredThresholdPolicy::loop_event(const methodHandle& method, CompLevel cur_level, Thread* thread) {\n-  CompLevel next_level = common(&TieredThresholdPolicy::loop_predicate, method, cur_level, true);\n-  if (cur_level == CompLevel_none) {\n-    \/\/ If there is a live OSR method that means that we deopted to the interpreter\n-    \/\/ for the transition.\n-    CompLevel osr_level = MIN2((CompLevel)method->highest_osr_comp_level(), next_level);\n-    if (osr_level > CompLevel_none) {\n-      return osr_level;\n-    }\n-  }\n-  return next_level;\n-}\n-\n-bool TieredThresholdPolicy::maybe_switch_to_aot(const methodHandle& mh, CompLevel cur_level, CompLevel next_level, Thread* thread) {\n-  if (UseAOT) {\n-    if (cur_level == CompLevel_full_profile || cur_level == CompLevel_none) {\n-      \/\/ If the current level is full profile or interpreter and we're switching to any other level,\n-      \/\/ activate the AOT code back first so that we won't waste time overprofiling.\n-      compile(mh, InvocationEntryBci, CompLevel_aot, thread);\n-      \/\/ Fall through for JIT compilation.\n-    }\n-    if (next_level == CompLevel_limited_profile && cur_level != CompLevel_aot && mh->has_aot_code()) {\n-      \/\/ If the next level is limited profile, use the aot code (if there is any),\n-      \/\/ since it's essentially the same thing.\n-      compile(mh, InvocationEntryBci, CompLevel_aot, thread);\n-      \/\/ Not need to JIT, we're done.\n-      return true;\n-    }\n-  }\n-  return false;\n-}\n-\n-\n-\/\/ Handle the invocation event.\n-void TieredThresholdPolicy::method_invocation_event(const methodHandle& mh, const methodHandle& imh,\n-                                                      CompLevel level, CompiledMethod* nm, TRAPS) {\n-  if (should_create_mdo(mh, level)) {\n-    create_mdo(mh, THREAD);\n-  }\n-  CompLevel next_level = call_event(mh, level, THREAD);\n-  if (next_level != level) {\n-    if (maybe_switch_to_aot(mh, level, next_level, THREAD)) {\n-      \/\/ No JITting necessary\n-      return;\n-    }\n-    if (is_compilation_enabled() && !CompileBroker::compilation_is_in_queue(mh)) {\n-      compile(mh, InvocationEntryBci, next_level, THREAD);\n-    }\n-  }\n-}\n-\n-\/\/ Handle the back branch event. Notice that we can compile the method\n-\/\/ with a regular entry from here.\n-void TieredThresholdPolicy::method_back_branch_event(const methodHandle& mh, const methodHandle& imh,\n-                                                     int bci, CompLevel level, CompiledMethod* nm, TRAPS) {\n-  if (should_create_mdo(mh, level)) {\n-    create_mdo(mh, THREAD);\n-  }\n-  \/\/ Check if MDO should be created for the inlined method\n-  if (should_create_mdo(imh, level)) {\n-    create_mdo(imh, THREAD);\n-  }\n-\n-  if (is_compilation_enabled()) {\n-    CompLevel next_osr_level = loop_event(imh, level, THREAD);\n-    CompLevel max_osr_level = (CompLevel)imh->highest_osr_comp_level();\n-    \/\/ At the very least compile the OSR version\n-    if (!CompileBroker::compilation_is_in_queue(imh) && (next_osr_level != level)) {\n-      compile(imh, bci, next_osr_level, CHECK);\n-    }\n-\n-    \/\/ Use loop event as an opportunity to also check if there's been\n-    \/\/ enough calls.\n-    CompLevel cur_level, next_level;\n-    if (mh() != imh()) { \/\/ If there is an enclosing method\n-      if (level == CompLevel_aot) {\n-        \/\/ Recompile the enclosing method to prevent infinite OSRs. Stay at AOT level while it's compiling.\n-        if (max_osr_level != CompLevel_none && !CompileBroker::compilation_is_in_queue(mh)) {\n-          CompLevel enclosing_level = limit_level(CompLevel_full_profile);\n-          compile(mh, InvocationEntryBci, enclosing_level, THREAD);\n-        }\n-      } else {\n-        \/\/ Current loop event level is not AOT\n-        guarantee(nm != NULL, \"Should have nmethod here\");\n-        cur_level = comp_level(mh());\n-        next_level = call_event(mh, cur_level, THREAD);\n-\n-        if (max_osr_level == CompLevel_full_optimization) {\n-          \/\/ The inlinee OSRed to full opt, we need to modify the enclosing method to avoid deopts\n-          bool make_not_entrant = false;\n-          if (nm->is_osr_method()) {\n-            \/\/ This is an osr method, just make it not entrant and recompile later if needed\n-            make_not_entrant = true;\n-          } else {\n-            if (next_level != CompLevel_full_optimization) {\n-              \/\/ next_level is not full opt, so we need to recompile the\n-              \/\/ enclosing method without the inlinee\n-              cur_level = CompLevel_none;\n-              make_not_entrant = true;\n-            }\n-          }\n-          if (make_not_entrant) {\n-            if (PrintTieredEvents) {\n-              int osr_bci = nm->is_osr_method() ? nm->osr_entry_bci() : InvocationEntryBci;\n-              print_event(MAKE_NOT_ENTRANT, mh(), mh(), osr_bci, level);\n-            }\n-            nm->make_not_entrant();\n-          }\n-        }\n-        \/\/ Fix up next_level if necessary to avoid deopts\n-        if (next_level == CompLevel_limited_profile && max_osr_level == CompLevel_full_profile) {\n-          next_level = CompLevel_full_profile;\n-        }\n-        if (cur_level != next_level) {\n-          if (!maybe_switch_to_aot(mh, cur_level, next_level, THREAD) && !CompileBroker::compilation_is_in_queue(mh)) {\n-            compile(mh, InvocationEntryBci, next_level, THREAD);\n-          }\n-        }\n-      }\n-    } else {\n-      cur_level = comp_level(mh());\n-      next_level = call_event(mh, cur_level, THREAD);\n-      if (next_level != cur_level) {\n-        if (!maybe_switch_to_aot(mh, cur_level, next_level, THREAD) && !CompileBroker::compilation_is_in_queue(mh)) {\n-          compile(mh, InvocationEntryBci, next_level, THREAD);\n-        }\n-      }\n-    }\n-  }\n-}\n-\n-#endif\n","filename":"src\/hotspot\/share\/compiler\/tieredThresholdPolicy.cpp","additions":0,"deletions":1164,"binary":false,"changes":1164,"status":"deleted"},{"patch":"@@ -1,279 +0,0 @@\n-\/*\n- * Copyright (c) 2010, 2019, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#ifndef SHARE_COMPILER_TIEREDTHRESHOLDPOLICY_HPP\n-#define SHARE_COMPILER_TIEREDTHRESHOLDPOLICY_HPP\n-\n-#include \"code\/nmethod.hpp\"\n-#include \"compiler\/compilationPolicy.hpp\"\n-#include \"oops\/methodData.hpp\"\n-#include \"utilities\/globalDefinitions.hpp\"\n-\n-#ifdef TIERED\n-\n-class CompileTask;\n-class CompileQueue;\n-\/*\n- *  The system supports 5 execution levels:\n- *  * level 0 - interpreter\n- *  * level 1 - C1 with full optimization (no profiling)\n- *  * level 2 - C1 with invocation and backedge counters\n- *  * level 3 - C1 with full profiling (level 2 + MDO)\n- *  * level 4 - C2\n- *\n- * Levels 0, 2 and 3 periodically notify the runtime about the current value of the counters\n- * (invocation counters and backedge counters). The frequency of these notifications is\n- * different at each level. These notifications are used by the policy to decide what transition\n- * to make.\n- *\n- * Execution starts at level 0 (interpreter), then the policy can decide either to compile the\n- * method at level 3 or level 2. The decision is based on the following factors:\n- *    1. The length of the C2 queue determines the next level. The observation is that level 2\n- * is generally faster than level 3 by about 30%, therefore we would want to minimize the time\n- * a method spends at level 3. We should only spend the time at level 3 that is necessary to get\n- * adequate profiling. So, if the C2 queue is long enough it is more beneficial to go first to\n- * level 2, because if we transitioned to level 3 we would be stuck there until our C2 compile\n- * request makes its way through the long queue. When the load on C2 recedes we are going to\n- * recompile at level 3 and start gathering profiling information.\n- *    2. The length of C1 queue is used to dynamically adjust the thresholds, so as to introduce\n- * additional filtering if the compiler is overloaded. The rationale is that by the time a\n- * method gets compiled it can become unused, so it doesn't make sense to put too much onto the\n- * queue.\n- *\n- * After profiling is completed at level 3 the transition is made to level 4. Again, the length\n- * of the C2 queue is used as a feedback to adjust the thresholds.\n- *\n- * After the first C1 compile some basic information is determined about the code like the number\n- * of the blocks and the number of the loops. Based on that it can be decided that a method\n- * is trivial and compiling it with C1 will yield the same code. In this case the method is\n- * compiled at level 1 instead of 4.\n- *\n- * We also support profiling at level 0. If C1 is slow enough to produce the level 3 version of\n- * the code and the C2 queue is sufficiently small we can decide to start profiling in the\n- * interpreter (and continue profiling in the compiled code once the level 3 version arrives).\n- * If the profiling at level 0 is fully completed before level 3 version is produced, a level 2\n- * version is compiled instead in order to run faster waiting for a level 4 version.\n- *\n- * Compile queues are implemented as priority queues - for each method in the queue we compute\n- * the event rate (the number of invocation and backedge counter increments per unit of time).\n- * When getting an element off the queue we pick the one with the largest rate. Maintaining the\n- * rate also allows us to remove stale methods (the ones that got on the queue but stopped\n- * being used shortly after that).\n-*\/\n-\n-\/* Command line options:\n- * - Tier?InvokeNotifyFreqLog and Tier?BackedgeNotifyFreqLog control the frequency of method\n- *   invocation and backedge notifications. Basically every n-th invocation or backedge a mutator thread\n- *   makes a call into the runtime.\n- *\n- * - Tier?InvocationThreshold, Tier?CompileThreshold, Tier?BackEdgeThreshold, Tier?MinInvocationThreshold control\n- *   compilation thresholds.\n- *   Level 2 thresholds are not used and are provided for option-compatibility and potential future use.\n- *   Other thresholds work as follows:\n- *\n- *   Transition from interpreter (level 0) to C1 with full profiling (level 3) happens when\n- *   the following predicate is true (X is the level):\n- *\n- *   i > TierXInvocationThreshold * s || (i > TierXMinInvocationThreshold * s  && i + b > TierXCompileThreshold * s),\n- *\n- *   where $i$ is the number of method invocations, $b$ number of backedges and $s$ is the scaling\n- *   coefficient that will be discussed further.\n- *   The intuition is to equalize the time that is spend profiling each method.\n- *   The same predicate is used to control the transition from level 3 to level 4 (C2). It should be\n- *   noted though that the thresholds are relative. Moreover i and b for the 0->3 transition come\n- *   from Method* and for 3->4 transition they come from MDO (since profiled invocations are\n- *   counted separately). Finally, if a method does not contain anything worth profiling, a transition\n- *   from level 3 to level 4 occurs without considering thresholds (e.g., with fewer invocations than\n- *   what is specified by Tier4InvocationThreshold).\n- *\n- *   OSR transitions are controlled simply with b > TierXBackEdgeThreshold * s predicates.\n- *\n- * - Tier?LoadFeedback options are used to automatically scale the predicates described above depending\n- *   on the compiler load. The scaling coefficients are computed as follows:\n- *\n- *   s = queue_size_X \/ (TierXLoadFeedback * compiler_count_X) + 1,\n- *\n- *   where queue_size_X is the current size of the compiler queue of level X, and compiler_count_X\n- *   is the number of level X compiler threads.\n- *\n- *   Basically these parameters describe how many methods should be in the compile queue\n- *   per compiler thread before the scaling coefficient increases by one.\n- *\n- *   This feedback provides the mechanism to automatically control the flow of compilation requests\n- *   depending on the machine speed, mutator load and other external factors.\n- *\n- * - Tier3DelayOn and Tier3DelayOff parameters control another important feedback loop.\n- *   Consider the following observation: a method compiled with full profiling (level 3)\n- *   is about 30% slower than a method at level 2 (just invocation and backedge counters, no MDO).\n- *   Normally, the following transitions will occur: 0->3->4. The problem arises when the C2 queue\n- *   gets congested and the 3->4 transition is delayed. While the method is the C2 queue it continues\n- *   executing at level 3 for much longer time than is required by the predicate and at suboptimal speed.\n- *   The idea is to dynamically change the behavior of the system in such a way that if a substantial\n- *   load on C2 is detected we would first do the 0->2 transition allowing a method to run faster.\n- *   And then when the load decreases to allow 2->3 transitions.\n- *\n- *   Tier3Delay* parameters control this switching mechanism.\n- *   Tier3DelayOn is the number of methods in the C2 queue per compiler thread after which the policy\n- *   no longer does 0->3 transitions but does 0->2 transitions instead.\n- *   Tier3DelayOff switches the original behavior back when the number of methods in the C2 queue\n- *   per compiler thread falls below the specified amount.\n- *   The hysteresis is necessary to avoid jitter.\n- *\n- * - TieredCompileTaskTimeout is the amount of time an idle method can spend in the compile queue.\n- *   Basically, since we use the event rate d(i + b)\/dt as a value of priority when selecting a method to\n- *   compile from the compile queue, we also can detect stale methods for which the rate has been\n- *   0 for some time in the same iteration. Stale methods can appear in the queue when an application\n- *   abruptly changes its behavior.\n- *\n- * - TieredStopAtLevel, is used mostly for testing. It allows to bypass the policy logic and stick\n- *   to a given level. For example it's useful to set TieredStopAtLevel = 1 in order to compile everything\n- *   with pure c1.\n- *\n- * - Tier0ProfilingStartPercentage allows the interpreter to start profiling when the inequalities in the\n- *   0->3 predicate are already exceeded by the given percentage but the level 3 version of the\n- *   method is still not ready. We can even go directly from level 0 to 4 if c1 doesn't produce a compiled\n- *   version in time. This reduces the overall transition to level 4 and decreases the startup time.\n- *   Note that this behavior is also guarded by the Tier3Delay mechanism: when the c2 queue is too long\n- *   these is not reason to start profiling prematurely.\n- *\n- * - TieredRateUpdateMinTime and TieredRateUpdateMaxTime are parameters of the rate computation.\n- *   Basically, the rate is not computed more frequently than TieredRateUpdateMinTime and is considered\n- *   to be zero if no events occurred in TieredRateUpdateMaxTime.\n- *\/\n-\n-class TieredThresholdPolicy : public CompilationPolicy {\n-  jlong _start_time;\n-  int _c1_count, _c2_count;\n-\n-  \/\/ Set carry flags in the counters (in Method* and MDO).\n-  inline void handle_counter_overflow(Method* method);\n-  \/\/ Verify that a level is consistent with the compilation mode\n-  bool verify_level(CompLevel level);\n-  \/\/ Clamp the request level according to various constraints.\n-  inline CompLevel limit_level(CompLevel level);\n-  \/\/ Return desired initial compilation level for Xcomp\n-  CompLevel initial_compile_level_helper(const methodHandle& method);\n-  \/\/ Call and loop predicates determine whether a transition to a higher compilation\n-  \/\/ level should be performed (pointers to predicate functions are passed to common().\n-  \/\/ Predicates also take compiler load into account.\n-  typedef bool (TieredThresholdPolicy::*Predicate)(int i, int b, CompLevel cur_level, const methodHandle& method);\n-  bool call_predicate(int i, int b, CompLevel cur_level, const methodHandle& method);\n-  bool loop_predicate(int i, int b, CompLevel cur_level, const methodHandle& method);\n-  \/\/ Common transition function. Given a predicate determines if a method should transition to another level.\n-  CompLevel common(Predicate p, const methodHandle& method, CompLevel cur_level, bool disable_feedback = false);\n-  \/\/ Transition functions.\n-  \/\/ call_event determines if a method should be compiled at a different\n-  \/\/ level with a regular invocation entry.\n-  CompLevel call_event(const methodHandle& method, CompLevel cur_level, Thread* thread);\n-  \/\/ loop_event checks if a method should be OSR compiled at a different\n-  \/\/ level.\n-  CompLevel loop_event(const methodHandle& method, CompLevel cur_level, Thread* thread);\n-  void print_counters(const char* prefix, Method* m);\n-  \/\/ Has a method been long around?\n-  \/\/ We don't remove old methods from the compile queue even if they have\n-  \/\/ very low activity (see select_task()).\n-  inline bool is_old(Method* method);\n-  \/\/ Was a given method inactive for a given number of milliseconds.\n-  \/\/ If it is, we would remove it from the queue (see select_task()).\n-  inline bool is_stale(jlong t, jlong timeout, Method* m);\n-  \/\/ Compute the weight of the method for the compilation scheduling\n-  inline double weight(Method* method);\n-  \/\/ Apply heuristics and return true if x should be compiled before y\n-  inline bool compare_methods(Method* x, Method* y);\n-  \/\/ Compute event rate for a given method. The rate is the number of event (invocations + backedges)\n-  \/\/ per millisecond.\n-  inline void update_rate(jlong t, Method* m);\n-  \/\/ Compute threshold scaling coefficient\n-  inline double threshold_scale(CompLevel level, int feedback_k);\n-  \/\/ If a method is old enough and is still in the interpreter we would want to\n-  \/\/ start profiling without waiting for the compiled method to arrive. This function\n-  \/\/ determines whether we should do that.\n-  inline bool should_create_mdo(const methodHandle& method, CompLevel cur_level);\n-  \/\/ Create MDO if necessary.\n-  void create_mdo(const methodHandle& mh, Thread* thread);\n-  \/\/ Is method profiled enough?\n-  bool is_method_profiled(const methodHandle& method);\n-\n-  double _increase_threshold_at_ratio;\n-\n-  bool maybe_switch_to_aot(const methodHandle& mh, CompLevel cur_level, CompLevel next_level, Thread* thread);\n-\n-  int c1_count() const     { return _c1_count; }\n-  int c2_count() const     { return _c2_count; }\n-  void set_c1_count(int x) { _c1_count = x;    }\n-  void set_c2_count(int x) { _c2_count = x;    }\n-\n-  enum EventType { CALL, LOOP, COMPILE, REMOVE_FROM_QUEUE, UPDATE_IN_QUEUE, REPROFILE, MAKE_NOT_ENTRANT };\n-  void print_event(EventType type, Method* m, Method* im, int bci, CompLevel level);\n-  \/\/ Check if the method can be compiled, change level if necessary\n-  void compile(const methodHandle& mh, int bci, CompLevel level, TRAPS);\n-  \/\/ Simple methods are as good being compiled with C1 as C2.\n-  \/\/ This function tells if it's such a function.\n-  inline static bool is_trivial(Method* method);\n-  \/\/ Force method to be compiled at CompLevel_simple?\n-  inline bool force_comp_at_level_simple(const methodHandle& method);\n-\n-  \/\/ Predicate helpers are used by .*_predicate() methods as well as others.\n-  \/\/ They check the given counter values, multiplied by the scale against the thresholds.\n-  inline bool call_predicate_helper(const methodHandle& method, CompLevel cur_level, int i, int b, double scale);\n-  inline bool loop_predicate_helper(const methodHandle& method, CompLevel cur_level, int i, int b, double scale);\n-\n-  \/\/ Get a compilation level for a given method.\n-  static CompLevel comp_level(Method* method);\n-  void method_invocation_event(const methodHandle& method, const methodHandle& inlinee,\n-                               CompLevel level, CompiledMethod* nm, TRAPS);\n-  void method_back_branch_event(const methodHandle& method, const methodHandle& inlinee,\n-                                int bci, CompLevel level, CompiledMethod* nm, TRAPS);\n-\n-  void set_increase_threshold_at_ratio() { _increase_threshold_at_ratio = 100 \/ (100 - (double)IncreaseFirstTierCompileThresholdAt); }\n-  void set_start_time(jlong t) { _start_time = t;    }\n-  jlong start_time() const     { return _start_time; }\n-\n-public:\n-  TieredThresholdPolicy() : _start_time(0), _c1_count(0), _c2_count(0) { }\n-  virtual int compiler_count(CompLevel comp_level) {\n-    if (is_c1_compile(comp_level)) return c1_count();\n-    if (is_c2_compile(comp_level)) return c2_count();\n-    return 0;\n-  }\n-  \/\/ Return initial compile level to use with Xcomp (depends on compilation mode).\n-  virtual CompLevel initial_compile_level(const methodHandle& method);\n-  virtual void do_safepoint_work() { }\n-  virtual void delay_compilation(Method* method) { }\n-  virtual void disable_compilation(Method* method) { }\n-  virtual void reprofile(ScopeDesc* trap_scope, bool is_osr);\n-  virtual nmethod* event(const methodHandle& method, const methodHandle& inlinee,\n-                         int branch_bci, int bci, CompLevel comp_level, CompiledMethod* nm, TRAPS);\n-  \/\/ Select task is called by CompileBroker. We should return a task or NULL.\n-  virtual CompileTask* select_task(CompileQueue* compile_queue);\n-  \/\/ Tell the runtime if we think a given method is adequately profiled.\n-  virtual bool is_mature(Method* method);\n-  \/\/ Initialize: set compiler thread count\n-  virtual void initialize();\n-  virtual bool should_not_inline(ciEnv* env, ciMethod* callee);\n-};\n-\n-#endif \/\/ TIERED\n-\n-#endif \/\/ SHARE_COMPILER_TIEREDTHRESHOLDPOLICY_HPP\n","filename":"src\/hotspot\/share\/compiler\/tieredThresholdPolicy.hpp","additions":0,"deletions":279,"binary":false,"changes":279,"status":"deleted"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -153,1 +153,1 @@\n-  _defer_initial_card_mark = is_server_compilation_mode_vm() && ReduceInitialCardMarks\n+  _defer_initial_card_mark = CompilerConfig::is_c2_or_jvmci_compiler_available() && ReduceInitialCardMarks\n","filename":"src\/hotspot\/share\/gc\/shared\/cardTableBarrierSet.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1262,1 +1262,1 @@\n-  guarantee(is_client_compilation_mode_vm() || actual_gap > (size_t)FastAllocateSizeLimit, \"inline allocation wraps\");\n+  guarantee(!CompilerConfig::is_c2_or_jvmci_compiler_available() || actual_gap > (size_t)FastAllocateSizeLimit, \"inline allocation wraps\");\n","filename":"src\/hotspot\/share\/gc\/shared\/genCollectedHeap.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -63,1 +63,1 @@\n-  if (is_server_compilation_mode_vm()) {\n+  if (CompilerConfig::is_c2_or_jvmci_compiler_available()) {\n","filename":"src\/hotspot\/share\/gc\/shared\/referenceProcessor.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -253,1 +253,1 @@\n-  if (is_server_compilation_mode_vm()) {\n+  if (CompilerConfig::is_c2_or_jvmci_compiler_available()) {\n","filename":"src\/hotspot\/share\/gc\/shared\/threadLocalAllocBuffer.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -1021,1 +1021,1 @@\n-  nmethod* osr_nm = CompilationPolicy::policy()->event(method, method, branch_bci, bci, CompLevel_none, NULL, THREAD);\n+  nmethod* osr_nm = CompilationPolicy::event(method, method, branch_bci, bci, CompLevel_none, NULL, THREAD);\n@@ -1061,19 +1061,0 @@\n-JRT_ENTRY(void, InterpreterRuntime::profile_method(JavaThread* thread))\n-  \/\/ use UnlockFlagSaver to clear and restore the _do_not_unlock_if_synchronized\n-  \/\/ flag, in case this method triggers classloading which will call into Java.\n-  UnlockFlagSaver fs(thread);\n-\n-  assert(ProfileInterpreter, \"must be profiling interpreter\");\n-  LastFrameAccessor last_frame(thread);\n-  assert(last_frame.is_interpreted_frame(), \"must come from interpreter\");\n-  methodHandle method(thread, last_frame.method());\n-  Method::build_interpreter_method_data(method, THREAD);\n-  if (HAS_PENDING_EXCEPTION) {\n-    \/\/ Only metaspace OOM is expected. No Java code executed.\n-    assert((PENDING_EXCEPTION->is_a(SystemDictionary::OutOfMemoryError_klass())), \"we expect only an OOM error here\");\n-    CLEAR_PENDING_EXCEPTION;\n-    \/\/ and fall through...\n-  }\n-JRT_END\n-\n-\n","filename":"src\/hotspot\/share\/interpreter\/interpreterRuntime.cpp","additions":2,"deletions":21,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -154,1 +154,0 @@\n-  static void    profile_method(JavaThread* thread);\n","filename":"src\/hotspot\/share\/interpreter\/interpreterRuntime.hpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -52,12 +52,0 @@\n-void InvocationCounter::set_carry_and_reduce() {\n-  uint counter = raw_counter();\n-  \/\/ The carry bit now indicates that this counter had achieved a very\n-  \/\/ large value.  Now reduce the value, so that the method can be\n-  \/\/ executed many more times before re-entering the VM.\n-  uint old_count = extract_count(counter);\n-  uint new_count = MIN2(old_count, (uint)(CompileThreshold \/ 2));\n-  \/\/ prevent from going to zero, to distinguish from never-executed methods\n-  if (new_count == 0)  new_count = 1;\n-  if (old_count != new_count)  set(new_count, carry_mask);\n-}\n-\n","filename":"src\/hotspot\/share\/interpreter\/invocationCounter.cpp","additions":1,"deletions":13,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -64,1 +64,0 @@\n-  void set_carry_and_reduce();                   \/\/ set the sticky carry bit\n","filename":"src\/hotspot\/share\/interpreter\/invocationCounter.hpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -108,1 +108,1 @@\n-  void generate_counter_incr(Label* overflow, Label* profile_method, Label* profile_method_continue);\n+  void generate_counter_incr(Label* overflow);\n","filename":"src\/hotspot\/share\/interpreter\/templateInterpreterGenerator.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -250,4 +250,0 @@\n-  if (!TieredCompilation) {\n-    \/\/ Ignore the event if tiered is off\n-    return;\n-  }\n@@ -262,1 +258,1 @@\n-    CompilationPolicy::policy()->event(emh, mh, InvocationEntryBci, InvocationEntryBci, CompLevel_aot, cm, THREAD);\n+    CompilationPolicy::event(emh, mh, InvocationEntryBci, InvocationEntryBci, CompLevel_aot, cm, THREAD);\n@@ -267,4 +263,0 @@\n-  if (!TieredCompilation) {\n-    \/\/ Ignore the event if tiered is off\n-    return;\n-  }\n@@ -282,1 +274,1 @@\n-    nmethod* osr_nm = CompilationPolicy::policy()->event(emh, mh, branch_bci, target_bci, CompLevel_aot, cm, THREAD);\n+    nmethod* osr_nm = CompilationPolicy::event(emh, mh, branch_bci, target_bci, CompLevel_aot, cm, THREAD);\n","filename":"src\/hotspot\/share\/jvmci\/compilerRuntime.cpp","additions":3,"deletions":11,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2011, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2011, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -59,1 +59,1 @@\n-  assert(!is_c1_or_interpreter_only(), \"JVMCI is launched, it's not c1\/interpreter only mode\");\n+  assert(!CompilerConfig::is_c1_or_interpreter_only_no_aot_or_jvmci(), \"JVMCI is launched, it's not c1\/interpreter only mode\");\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompiler.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2012, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2012, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -1655,11 +1655,9 @@\n-            if (TieredCompilation) {\n-              \/\/ If there is an old version we're done with it\n-              CompiledMethod* old = method->code();\n-              if (TraceMethodReplacement && old != NULL) {\n-                ResourceMark rm;\n-                char *method_name = method->name_and_sig_as_C_string();\n-                tty->print_cr(\"Replacing method %s\", method_name);\n-              }\n-              if (old != NULL ) {\n-                old->make_not_entrant();\n-              }\n+            \/\/ If there is an old version we're done with it\n+            CompiledMethod* old = method->code();\n+            if (TraceMethodReplacement && old != NULL) {\n+              ResourceMark rm;\n+              char *method_name = method->name_and_sig_as_C_string();\n+              tty->print_cr(\"Replacing method %s\", method_name);\n+            }\n+            if (old != NULL ) {\n+              old->make_not_entrant();\n","filename":"src\/hotspot\/share\/jvmci\/jvmciRuntime.cpp","additions":10,"deletions":12,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -224,3 +224,0 @@\n-  nonstatic_field(MethodCounters,              _interpreter_invocation_limit,                 int)                                   \\\n-  nonstatic_field(MethodCounters,              _interpreter_backward_branch_limit,            int)                                   \\\n-  nonstatic_field(MethodCounters,              _interpreter_profile_limit,                    int)                                   \\\n@@ -229,1 +226,0 @@\n-  nonstatic_field(MethodCounters,              _interpreter_invocation_count,                 int)                                   \\\n","filename":"src\/hotspot\/share\/jvmci\/vmStructs_jvmci.cpp","additions":1,"deletions":5,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -42,0 +42,1 @@\n+#include \"compiler\/compilationPolicy.hpp\"\n@@ -3205,5 +3206,3 @@\n-  if (TieredCompilation) {\n-    nmethod* prev = lookup_osr_nmethod(n->method(), n->osr_entry_bci(), n->comp_level(), true);\n-    assert(prev == NULL || !prev->is_in_use() COMPILER2_PRESENT(|| StressRecompilation),\n-           \"redundant OSR recompilation detected. memory leak in CodeCache!\");\n-  }\n+  nmethod* prev = lookup_osr_nmethod(n->method(), n->osr_entry_bci(), n->comp_level(), true);\n+  assert(prev == NULL || !prev->is_in_use() COMPILER2_PRESENT(|| StressRecompilation),\n+      \"redundant OSR recompilation detected. memory leak in CodeCache!\");\n@@ -3212,10 +3211,5 @@\n-  {\n-    assert(n->is_osr_method(), \"wrong kind of nmethod\");\n-    n->set_osr_link(osr_nmethods_head());\n-    set_osr_nmethods_head(n);\n-    \/\/ Raise the highest osr level if necessary\n-    if (TieredCompilation) {\n-      Method* m = n->method();\n-      m->set_highest_osr_comp_level(MAX2(m->highest_osr_comp_level(), n->comp_level()));\n-    }\n-  }\n+  assert(n->is_osr_method(), \"wrong kind of nmethod\");\n+  n->set_osr_link(osr_nmethods_head());\n+  set_osr_nmethods_head(n);\n+  \/\/ Raise the highest osr level if necessary\n+  n->method()->set_highest_osr_comp_level(MAX2(n->method()->highest_osr_comp_level(), n->comp_level()));\n@@ -3224,6 +3218,4 @@\n-  if (TieredCompilation) {\n-    for (int l = CompLevel_limited_profile; l < n->comp_level(); l++) {\n-      nmethod *inv = lookup_osr_nmethod(n->method(), n->osr_entry_bci(), l, true);\n-      if (inv != NULL && inv->is_in_use()) {\n-        inv->make_not_entrant();\n-      }\n+  for (int l = CompLevel_limited_profile; l < n->comp_level(); l++) {\n+    nmethod *inv = lookup_osr_nmethod(n->method(), n->osr_entry_bci(), l, true);\n+    if (inv != NULL && inv->is_in_use()) {\n+      inv->make_not_entrant();\n@@ -3247,1 +3239,1 @@\n-    if (TieredCompilation && m == cur->method()) {\n+    if (m == cur->method()) {\n@@ -3266,8 +3258,5 @@\n-  if (TieredCompilation) {\n-    cur = next;\n-    while (cur != NULL) {\n-      \/\/ Find max level after n\n-      if (m == cur->method()) {\n-        max_level = MAX2(max_level, cur->comp_level());\n-      }\n-      cur = cur->osr_link();\n+  cur = next;\n+  while (cur != NULL) {\n+    \/\/ Find max level after n\n+    if (m == cur->method()) {\n+      max_level = MAX2(max_level, cur->comp_level());\n@@ -3275,1 +3264,1 @@\n-    m->set_highest_osr_comp_level(max_level);\n+    cur = cur->osr_link();\n@@ -3277,0 +3266,1 @@\n+  m->set_highest_osr_comp_level(max_level);\n@@ -3318,1 +3308,1 @@\n-          if (osr->comp_level() == CompLevel_highest_tier) {\n+          if (osr->comp_level() == CompilationPolicy::highest_compile_level()) {\n","filename":"src\/hotspot\/share\/oops\/instanceKlass.cpp","additions":23,"deletions":33,"binary":false,"changes":56,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -1013,1 +1013,1 @@\n-    return is_not_c1_compilable() || is_not_c2_compilable();\n+    return is_not_c1_compilable() && is_not_c2_compilable();\n@@ -1044,1 +1044,1 @@\n-    return is_not_c1_osr_compilable() || is_not_c2_osr_compilable();\n+    return is_not_c1_osr_compilable() && is_not_c2_osr_compilable();\n@@ -1962,10 +1962,5 @@\n-  MethodCounters *mcs = method_counters();\n-  if (TieredCompilation) {\n-    MethodData* const mdo = method_data();\n-    if (((mcs != NULL) ? mcs->invocation_counter()->carry() : false) ||\n-        ((mdo != NULL) ? mdo->invocation_counter()->carry() : false)) {\n-      return InvocationCounter::count_limit;\n-    } else {\n-      return ((mcs != NULL) ? mcs->invocation_counter()->count() : 0) +\n-             ((mdo != NULL) ? mdo->invocation_counter()->count() : 0);\n-    }\n+  MethodCounters* mcs = method_counters();\n+  MethodData* mdo = method_data();\n+  if (((mcs != NULL) ? mcs->invocation_counter()->carry() : false) ||\n+      ((mdo != NULL) ? mdo->invocation_counter()->carry() : false)) {\n+    return InvocationCounter::count_limit;\n@@ -1973,1 +1968,2 @@\n-    return (mcs == NULL) ? 0 : mcs->invocation_counter()->count();\n+    return ((mcs != NULL) ? mcs->invocation_counter()->count() : 0) +\n+           ((mdo != NULL) ? mdo->invocation_counter()->count() : 0);\n@@ -1978,10 +1974,5 @@\n-  MethodCounters *mcs = method_counters();\n-  if (TieredCompilation) {\n-    MethodData* const mdo = method_data();\n-    if (((mcs != NULL) ? mcs->backedge_counter()->carry() : false) ||\n-        ((mdo != NULL) ? mdo->backedge_counter()->carry() : false)) {\n-      return InvocationCounter::count_limit;\n-    } else {\n-      return ((mcs != NULL) ? mcs->backedge_counter()->count() : 0) +\n-             ((mdo != NULL) ? mdo->backedge_counter()->count() : 0);\n-    }\n+  MethodCounters* mcs = method_counters();\n+  MethodData* mdo = method_data();\n+  if (((mcs != NULL) ? mcs->backedge_counter()->carry() : false) ||\n+      ((mdo != NULL) ? mdo->backedge_counter()->carry() : false)) {\n+    return InvocationCounter::count_limit;\n@@ -1989,1 +1980,2 @@\n-    return (mcs == NULL) ? 0 : mcs->backedge_counter()->count();\n+    return ((mcs != NULL) ? mcs->backedge_counter()->count() : 0) +\n+           ((mdo != NULL) ? mdo->backedge_counter()->count() : 0);\n","filename":"src\/hotspot\/share\/oops\/method.cpp","additions":17,"deletions":25,"binary":false,"changes":42,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -116,1 +116,1 @@\n-#if INCLUDE_AOT && defined(TIERED)\n+#if INCLUDE_AOT\n@@ -375,9 +375,3 @@\n-#ifdef TIERED\n-  \/\/ We are reusing interpreter_invocation_count as a holder for the previous event count!\n-  \/\/ We can do that since interpreter_invocation_count is not used in tiered.\n-  int prev_event_count() const                   {\n-    if (method_counters() == NULL) {\n-      return 0;\n-    } else {\n-      return method_counters()->interpreter_invocation_count();\n-    }\n+  int prev_event_count() const {\n+    MethodCounters* mcs = method_counters();\n+    return mcs == NULL ? 0 : mcs->prev_event_count();\n@@ -388,1 +382,1 @@\n-      mcs->set_interpreter_invocation_count(count);\n+      mcs->set_prev_event_count(count);\n@@ -391,1 +385,1 @@\n-  jlong prev_time() const                        {\n+  jlong prev_time() const {\n@@ -401,1 +395,1 @@\n-  float rate() const                             {\n+  float rate() const {\n@@ -423,1 +417,0 @@\n-#endif \/\/ TIERED\n@@ -437,1 +430,1 @@\n-  bool was_never_executed()                      { return !was_executed_more_than(0); }\n+  bool was_never_executed()                     { return !was_executed_more_than(0);  }\n@@ -443,15 +436,1 @@\n-  int interpreter_invocation_count() {\n-    if (TieredCompilation) {\n-      return invocation_count();\n-    } else {\n-      MethodCounters* mcs = method_counters();\n-      return (mcs == NULL) ? 0 : mcs->interpreter_invocation_count();\n-    }\n-  }\n-#if COMPILER2_OR_JVMCI\n-  int increment_interpreter_invocation_count(TRAPS) {\n-    if (TieredCompilation) ShouldNotReachHere();\n-    MethodCounters* mcs = get_method_counters(CHECK_0);\n-    return (mcs == NULL) ? 0 : mcs->increment_interpreter_invocation_count();\n-  }\n-#endif\n+  int interpreter_invocation_count()            { return invocation_count();          }\n@@ -460,2 +439,2 @@\n-  int  compiled_invocation_count() const         { return _compiled_invocation_count;  }\n-  void set_compiled_invocation_count(int count)  { _compiled_invocation_count = count; }\n+  int  compiled_invocation_count() const        { return _compiled_invocation_count;  }\n+  void set_compiled_invocation_count(int count) { _compiled_invocation_count = count; }\n@@ -464,1 +443,1 @@\n-  int  compiled_invocation_count() const         { return 0;  }\n+  int  compiled_invocation_count() const        { return 0;  }\n@@ -706,1 +685,0 @@\n-#ifdef TIERED\n@@ -708,1 +686,0 @@\n-#endif\n","filename":"src\/hotspot\/share\/oops\/method.hpp","additions":13,"deletions":36,"binary":false,"changes":49,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2013, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2013, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -34,3 +34,0 @@\n-  _nmethod_age(INT_MAX)\n-#ifdef TIERED\n-  , _rate(0),\n@@ -38,0 +35,2 @@\n+  _rate(0),\n+  _nmethod_age(INT_MAX),\n@@ -40,1 +39,0 @@\n-#endif\n@@ -42,1 +40,0 @@\n-  set_interpreter_invocation_count(0);\n@@ -56,11 +53,0 @@\n-  int compile_threshold = CompilerConfig::scaled_compile_threshold(CompileThreshold, scale);\n-  _interpreter_invocation_limit = compile_threshold << InvocationCounter::count_shift;\n-  if (ProfileInterpreter) {\n-    \/\/ If interpreter profiling is enabled, the backward branch limit\n-    \/\/ is compared against the method data counter rather than an invocation\n-    \/\/ counter, therefore no shifting of bits is required.\n-    _interpreter_backward_branch_limit = (int)((int64_t)compile_threshold * (OnStackReplacePercentage - InterpreterProfilePercentage) \/ 100);\n-  } else {\n-    _interpreter_backward_branch_limit = (int)(((int64_t)compile_threshold * OnStackReplacePercentage \/ 100) << InvocationCounter::count_shift);\n-  }\n-  _interpreter_profile_limit = ((compile_threshold * InterpreterProfilePercentage) \/ 100) << InvocationCounter::count_shift;\n@@ -80,1 +66,0 @@\n-  set_interpreter_invocation_count(0);\n@@ -82,1 +67,0 @@\n-#ifdef TIERED\n@@ -84,0 +68,1 @@\n+  set_prev_event_count(0);\n@@ -87,30 +72,0 @@\n-#endif\n-}\n-\n-\n-int MethodCounters::highest_comp_level() const {\n-#ifdef TIERED\n-  return _highest_comp_level;\n-#else\n-  return CompLevel_none;\n-#endif\n-}\n-\n-void MethodCounters::set_highest_comp_level(int level) {\n-#ifdef TIERED\n-  _highest_comp_level = level;\n-#endif\n-}\n-\n-int MethodCounters::highest_osr_comp_level() const {\n-#ifdef TIERED\n-  return _highest_osr_comp_level;\n-#else\n-  return CompLevel_none;\n-#endif\n-}\n-\n-void MethodCounters::set_highest_osr_comp_level(int level) {\n-#ifdef TIERED\n-  _highest_osr_comp_level = level;\n-#endif\n","filename":"src\/hotspot\/share\/oops\/methodCounters.cpp","additions":4,"deletions":49,"binary":false,"changes":53,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2013, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2013, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -38,0 +38,2 @@\n+  InvocationCounter _invocation_counter;         \/\/ Incremented before each activation of the method - used to trigger frequency-based optimizations\n+  InvocationCounter _backedge_counter;           \/\/ Incremented before each backedge taken - used to trigger frequency-based optimizations\n@@ -43,0 +45,6 @@\n+  jlong             _prev_time;                   \/\/ Previous time the rate was acquired\n+  float             _rate;                        \/\/ Events (invocation and backedge counter increments) per millisecond\n+  int               _nmethod_age;\n+  int               _invoke_mask;                 \/\/ per-method Tier0InvokeNotifyFreqLog\n+  int               _backedge_mask;               \/\/ per-method Tier0BackedgeNotifyFreqLog\n+  int               _prev_event_count;            \/\/ Total number of events saved at previous callback\n@@ -44,1 +52,0 @@\n-  int               _interpreter_invocation_count; \/\/ Count of times invoked (reused as prev_event_count in tiered)\n@@ -50,2 +57,0 @@\n-  InvocationCounter _invocation_counter;         \/\/ Incremented before each activation of the method - used to trigger frequency-based optimizations\n-  InvocationCounter _backedge_counter;           \/\/ Incremented before each backedge taken - used to trigger frequencey-based optimizations\n@@ -60,9 +65,0 @@\n-  int               _nmethod_age;\n-  int               _interpreter_invocation_limit;        \/\/ per-method InterpreterInvocationLimit\n-  int               _interpreter_backward_branch_limit;   \/\/ per-method InterpreterBackwardBranchLimit\n-  int               _interpreter_profile_limit;           \/\/ per-method InterpreterProfileLimit\n-  int               _invoke_mask;                         \/\/ per-method Tier0InvokeNotifyFreqLog\n-  int               _backedge_mask;                       \/\/ per-method Tier0BackedgeNotifyFreqLog\n-#ifdef TIERED\n-  float             _rate;                        \/\/ Events (invocation and backedge counter increments) per millisecond\n-  jlong             _prev_time;                   \/\/ Previous time the rate was acquired\n@@ -71,1 +67,0 @@\n-#endif\n@@ -74,1 +69,0 @@\n-\n@@ -95,11 +89,0 @@\n-\n-  int interpreter_invocation_count() {\n-    return _interpreter_invocation_count;\n-  }\n-  void set_interpreter_invocation_count(int count) {\n-    _interpreter_invocation_count = count;\n-  }\n-  int increment_interpreter_invocation_count() {\n-    return ++_interpreter_invocation_count;\n-  }\n-\n@@ -117,1 +100,0 @@\n-\n@@ -119,8 +101,0 @@\n-\n-  int interpreter_invocation_count() {\n-    return 0;\n-  }\n-  void set_interpreter_invocation_count(int count) {\n-    assert(count == 0, \"count must be 0\");\n-  }\n-\n@@ -133,1 +107,0 @@\n-\n@@ -143,1 +116,2 @@\n-#ifdef TIERED\n+  int prev_event_count() const                   { return _prev_event_count;  }\n+  void set_prev_event_count(int count)           { _prev_event_count = count; }\n@@ -148,1 +122,0 @@\n-#endif\n@@ -150,4 +123,4 @@\n-  int highest_comp_level() const;\n-  void set_highest_comp_level(int level);\n-  int highest_osr_comp_level() const;\n-  void set_highest_osr_comp_level(int level);\n+  int highest_comp_level() const                 { return _highest_comp_level;  }\n+  void set_highest_comp_level(int level)         { _highest_comp_level = level; }\n+  int highest_osr_comp_level() const             { return _highest_osr_comp_level;  }\n+  void set_highest_osr_comp_level(int level)     { _highest_osr_comp_level = level; }\n@@ -177,19 +150,0 @@\n-#if COMPILER2_OR_JVMCI\n-\n-  static ByteSize interpreter_invocation_counter_offset() {\n-    return byte_offset_of(MethodCounters, _interpreter_invocation_count);\n-  }\n-\n-  static int interpreter_invocation_counter_offset_in_bytes() {\n-    return offset_of(MethodCounters, _interpreter_invocation_count);\n-  }\n-\n-#else \/\/ COMPILER2_OR_JVMCI\n-\n-  static ByteSize interpreter_invocation_counter_offset() {\n-    ShouldNotReachHere();\n-    return in_ByteSize(0);\n-  }\n-\n-#endif \/\/ COMPILER2_OR_JVMCI\n-\n@@ -204,12 +158,0 @@\n-  static ByteSize interpreter_invocation_limit_offset() {\n-    return byte_offset_of(MethodCounters, _interpreter_invocation_limit);\n-  }\n-\n-  static ByteSize interpreter_backward_branch_limit_offset() {\n-    return byte_offset_of(MethodCounters, _interpreter_backward_branch_limit);\n-  }\n-\n-  static ByteSize interpreter_profile_limit_offset() {\n-    return byte_offset_of(MethodCounters, _interpreter_profile_limit);\n-  }\n-\n","filename":"src\/hotspot\/share\/oops\/methodCounters.hpp","additions":15,"deletions":73,"binary":false,"changes":88,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -665,1 +665,1 @@\n-  if (is_client_compilation_mode_vm()) {\n+  if (CompilerConfig::is_c1_simple_only() && !ProfileInterpreter) {\n@@ -788,1 +788,1 @@\n-    if (is_server_compilation_mode_vm()) {\n+    if (CompilerConfig::is_c2_available()) {\n@@ -972,1 +972,1 @@\n-  if (is_client_compilation_mode_vm()) {\n+  if (CompilerConfig::is_c1_simple_only() && !ProfileInterpreter) {\n@@ -1330,19 +1330,1 @@\n-  int mileage = 0;\n-  if (TieredCompilation) {\n-    mileage = MAX2(method->invocation_count(), method->backedge_count());\n-  } else {\n-    int iic = method->interpreter_invocation_count();\n-    if (mileage < iic)  mileage = iic;\n-    MethodCounters* mcs = method->method_counters();\n-    if (mcs != NULL) {\n-      InvocationCounter* ic = mcs->invocation_counter();\n-      InvocationCounter* bc = mcs->backedge_counter();\n-      int icval = ic->count();\n-      if (ic->carry()) icval += CompileThreshold;\n-      if (mileage < icval)  mileage = icval;\n-      int bcval = bc->count();\n-      if (bc->carry()) bcval += CompileThreshold;\n-      if (mileage < bcval)  mileage = bcval;\n-    }\n-  }\n-  return mileage;\n+  return MAX2(method->invocation_count(), method->backedge_count());\n@@ -1352,1 +1334,1 @@\n-  return CompilationPolicy::policy()->is_mature(_method);\n+  return CompilationPolicy::is_mature(_method);\n","filename":"src\/hotspot\/share\/oops\/methodData.cpp","additions":6,"deletions":24,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -78,1 +78,1 @@\n-  assert(!is_c1_or_interpreter_only(), \"C2 compiler is launched, it's not c1\/interpreter only mode\");\n+  assert(!CompilerConfig::is_c1_or_interpreter_only_no_aot_or_jvmci(), \"C2 compiler is launched, it's not c1\/interpreter only mode\");\n","filename":"src\/hotspot\/share\/opto\/c2compiler.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -729,2 +729,1 @@\n-    \/\/ Bailouts cover \"all_tiers\" when TieredCompilation is off.\n-    env()->record_method_not_compilable(reason, !TieredCompilation);\n+    env()->record_method_not_compilable(reason);\n","filename":"src\/hotspot\/share\/opto\/compile.hpp","additions":2,"deletions":3,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -431,1 +431,1 @@\n-#ifdef TIERED\n+#if COMPILER1_AND_COMPILER2\n@@ -439,1 +439,1 @@\n-    #error \"INCLUDE_JVMCI should imply TIERED\"\n+    #error \"INCLUDE_JVMCI should imply COMPILER1_OR_COMPILER2\"\n@@ -443,1 +443,1 @@\n-#endif \/\/ TIERED\n+#endif \/\/ COMPILER1_AND_COMPILER2\n","filename":"src\/hotspot\/share\/prims\/jvm.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2012, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2012, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -38,1 +38,0 @@\n-#include \"compiler\/methodMatcher.hpp\"\n@@ -40,0 +39,1 @@\n+#include \"compiler\/methodMatcher.hpp\"\n@@ -764,4 +764,0 @@\n-static CompLevel highestCompLevel() {\n-  return TieredCompilation ? MIN2((CompLevel) TieredStopAtLevel, CompLevel_highest_tier) : CompLevel_highest_tier;\n-}\n-\n@@ -854,1 +850,1 @@\n-  if (method == NULL || comp_level > highestCompLevel()) {\n+  if (method == NULL || comp_level > CompilationPolicy::highest_compile_level()) {\n@@ -877,1 +873,1 @@\n-  if (compLevel < CompLevel_none || compLevel > highestCompLevel()) {\n+  if (compLevel < CompLevel_none || compLevel > CompilationPolicy::highest_compile_level()) {\n@@ -975,1 +971,1 @@\n-  if (comp_level > highestCompLevel()) {\n+  if (comp_level > CompilationPolicy::highest_compile_level()) {\n@@ -1101,1 +1097,1 @@\n-  \/\/ set i-counter according to TieredThresholdPolicy::is_method_profiled\n+  \/\/ set i-counter according to CompilationPolicy::is_method_profiled\n@@ -1130,10 +1126,1 @@\n-    mcs->backedge_counter()->init();\n-    mcs->invocation_counter()->init();\n-    mcs->set_interpreter_invocation_count(0);\n-    mcs->set_interpreter_throwout_count(0);\n-\n-#ifdef TIERED\n-    mcs->set_rate(0.0F);\n-    mh->set_prev_event_count(0);\n-    mh->set_prev_time(0);\n-#endif\n+    mcs->clear_counters();\n","filename":"src\/hotspot\/share\/prims\/whitebox.cpp","additions":7,"deletions":20,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -93,1 +93,1 @@\n-  #ifdef TIERED\n+  #if COMPILER1_AND_COMPILER2\n@@ -95,1 +95,1 @@\n-  #else \/\/ TIERED\n+  #else \/\/ COMPILER1_AND_COMPILER2\n@@ -102,1 +102,1 @@\n-  #endif \/\/ TIERED\n+  #endif \/\/ COMPILER1_AND_COMPILER2\n@@ -132,2 +132,1 @@\n-#ifdef TIERED\n-        } else if(is_client_compilation_mode_vm()) {\n+        } else if (CompilationModeFlag::quick_only()) {\n@@ -135,1 +134,0 @@\n-#endif\n@@ -142,2 +140,1 @@\n-#ifdef TIERED\n-        } else if(is_client_compilation_mode_vm()) {\n+        } else if (CompilationModeFlag::quick_only()) {\n@@ -145,1 +142,0 @@\n-#endif\n@@ -151,2 +147,1 @@\n-#ifdef TIERED\n-      if (is_client_compilation_mode_vm()) {\n+      if (CompilationModeFlag::quick_only()) {\n@@ -155,3 +150,2 @@\n-#endif\n-      return UseSharedSpaces ? \"compiled mode, sharing\"    : \"compiled mode\";\n-  };\n+      return UseSharedSpaces ? \"compiled mode, sharing\" : \"compiled mode\";\n+  }\n","filename":"src\/hotspot\/share\/runtime\/abstract_vm_version.cpp","additions":9,"deletions":15,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -1459,7 +1459,5 @@\n-  if (TieredCompilation) {\n-    if (FLAG_IS_DEFAULT(Tier3InvokeNotifyFreqLog)) {\n-      Tier3InvokeNotifyFreqLog = Arguments::_Tier3InvokeNotifyFreqLog;\n-    }\n-    if (FLAG_IS_DEFAULT(Tier4InvocationThreshold)) {\n-      Tier4InvocationThreshold = Arguments::_Tier4InvocationThreshold;\n-    }\n+  if (FLAG_IS_DEFAULT(Tier3InvokeNotifyFreqLog)) {\n+    Tier3InvokeNotifyFreqLog = Arguments::_Tier3InvokeNotifyFreqLog;\n+  }\n+  if (FLAG_IS_DEFAULT(Tier4InvocationThreshold)) {\n+    Tier4InvocationThreshold = Arguments::_Tier4InvocationThreshold;\n@@ -1489,1 +1487,1 @@\n-    if (TieredCompilation) {\n+    if (CompilerConfig::is_c2_or_jvmci_compiler_available()) {\n@@ -2139,4 +2137,2 @@\n-  if (TieredCompilation) {\n-    Arguments::_Tier3InvokeNotifyFreqLog = Tier3InvokeNotifyFreqLog;\n-    Arguments::_Tier4InvocationThreshold = Tier4InvocationThreshold;\n-  }\n+  Arguments::_Tier3InvokeNotifyFreqLog = Tier3InvokeNotifyFreqLog;\n+  Arguments::_Tier4InvocationThreshold = Tier4InvocationThreshold;\n@@ -3100,2 +3096,0 @@\n-\n-#ifdef TIERED\n@@ -3106,4 +3100,0 @@\n-#else\n-  \/\/ Tiered compilation is undefined.\n-  UNSUPPORTED_OPTION(TieredCompilation);\n-#endif\n@@ -3158,4 +3148,0 @@\n-  UNSUPPORTED_OPTION_INIT(Tier0AOTInvocationThreshold, 0);\n-  UNSUPPORTED_OPTION_INIT(Tier0AOTMinInvocationThreshold, 0);\n-  UNSUPPORTED_OPTION_INIT(Tier0AOTCompileThreshold, 0);\n-  UNSUPPORTED_OPTION_INIT(Tier0AOTBackEdgeThreshold, 0);\n@@ -3994,6 +3980,0 @@\n-#ifndef TIERED\n-  if (FLAG_IS_CMDLINE(CompilationMode)) {\n-    warning(\"CompilationMode has no effect in non-tiered VMs\");\n-  }\n-#endif\n-\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":8,"deletions":28,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -2216,1 +2216,1 @@\n-    if (ProfileTraps && !is_client_compilation_mode_vm() && update_trap_state && trap_mdo != NULL) {\n+    if (ProfileTraps && CompilerConfig::is_c2_or_jvmci_compiler_available() && update_trap_state && trap_mdo != NULL) {\n@@ -2338,1 +2338,1 @@\n-      CompilationPolicy::policy()->reprofile(trap_scope, nm->is_osr_method());\n+      CompilationPolicy::reprofile(trap_scope, nm->is_osr_method());\n","filename":"src\/hotspot\/share\/runtime\/deoptimization.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -52,15 +52,1 @@\n- * Validate the minimum number of compiler threads needed to run the\n- * JVM. The following configurations are possible.\n- *\n- * 1) The JVM is build using an interpreter only. As a result, the minimum number of\n- *    compiler threads is 0.\n- * 2) The JVM is build using the compiler(s) and tiered compilation is disabled. As\n- *    a result, either C1 or C2 is used, so the minimum number of compiler threads is 1.\n- * 3) The JVM is build using the compiler(s) and tiered compilation is enabled. However,\n- *    the option \"TieredStopAtLevel < CompLevel_full_optimization\". As a result, only\n- *    C1 can be used, so the minimum number of compiler threads is 1.\n- * 4) The JVM is build using the compilers and tiered compilation is enabled. The option\n- *    'TieredStopAtLevel = CompLevel_full_optimization' (the default value). As a result,\n- *    the minimum number of compiler threads is 2.\n- * 5) Non-tiered emulation mode is on. CompilationModeFlag::disable_intermediate() == true.\n- *    The minimum number of threads is 2. But if CompilationModeFlag::quick_internal() == false, then it's 1.\n+ * Validate the minimum number of compiler threads needed to run the JVM.\n@@ -70,16 +56,3 @@\n-#if !defined(COMPILER1) && !defined(COMPILER2) && !INCLUDE_JVMCI\n-  \/\/ case 1\n-#elif defined(TIERED)\n-  if (TieredCompilation) {\n-    if (TieredStopAtLevel < CompLevel_full_optimization || CompilationModeFlag::quick_only()) {\n-      min_number_of_compiler_threads = 1; \/\/ case 3\n-    } else if (CompilationModeFlag::disable_intermediate()) {\n-      \/\/ case 5\n-      if (CompilationModeFlag::quick_internal()) {\n-        min_number_of_compiler_threads = 2;\n-      } else {\n-        min_number_of_compiler_threads = 1;\n-      }\n-    } else {\n-      min_number_of_compiler_threads = 2;   \/\/ case 4 (tiered)\n-    }\n+#if COMPILER1_OR_COMPILER2\n+  if (CompilerConfig::is_tiered()) {\n+    min_number_of_compiler_threads = 2;\n@@ -87,1 +60,1 @@\n-    min_number_of_compiler_threads = 1; \/\/ case 2\n+    min_number_of_compiler_threads = 1;\n@@ -90,1 +63,6 @@\n-  min_number_of_compiler_threads = 1; \/\/ case 2\n+  if (value > 0) {\n+    JVMFlag::printError(verbose,\n+                        \"CICompilerCount (\" INTX_FORMAT \") cannot be \"\n+                        \"greater than 0 because there are no compilers\\n\", value);\n+    return JVMFlag::VIOLATES_CONSTRAINT;\n+  }\n@@ -93,6 +71,0 @@\n-  \/\/ The default CICompilerCount's value is CI_COMPILER_COUNT.\n-  \/\/ With a client VM, -XX:+TieredCompilation causes TieredCompilation\n-  \/\/ to be true here (the option is validated later) and\n-  \/\/ min_number_of_compiler_threads to exceed CI_COMPILER_COUNT.\n-  min_number_of_compiler_threads = MIN2(min_number_of_compiler_threads, CI_COMPILER_COUNT);\n-\n@@ -440,1 +412,2 @@\n-}\n\\ No newline at end of file\n+}\n+\n","filename":"src\/hotspot\/share\/runtime\/flags\/jvmFlagConstraintsCompiler.cpp","additions":14,"deletions":41,"binary":false,"changes":55,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -45,10 +45,0 @@\n-\/\/ use this for flags that are true per default in the tiered build\n-\/\/ but false in non-tiered builds, and vice versa\n-#ifdef TIERED\n-#define  trueInTiered true\n-#define falseInTiered false\n-#else\n-#define  trueInTiered false\n-#define falseInTiered true\n-#endif\n-\n","filename":"src\/hotspot\/share\/runtime\/globals_shared.hpp","additions":1,"deletions":11,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -562,1 +562,1 @@\n-      CompilationPolicy::policy()->do_safepoint_work();\n+      CompilationPolicy::do_safepoint_work();\n","filename":"src\/hotspot\/share\/runtime\/safepoint.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -291,3 +291,0 @@\n-  nonstatic_field(MethodCounters,              _interpreter_invocation_limit,                 int)                                   \\\n-  nonstatic_field(MethodCounters,              _interpreter_backward_branch_limit,            int)                                   \\\n-  nonstatic_field(MethodCounters,              _interpreter_profile_limit,                    int)                                   \\\n@@ -296,1 +293,0 @@\n-  COMPILER2_OR_JVMCI_PRESENT(nonstatic_field(MethodCounters, _interpreter_invocation_count,   int))                                  \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":1,"deletions":5,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -306,3 +306,0 @@\n-#ifdef COMPILER2\n-  #define TIERED\n-#endif\n@@ -340,7 +337,17 @@\n-#ifdef TIERED\n-#define TIERED_ONLY(code) code\n-#define NOT_TIERED(code)\n-#else \/\/ TIERED\n-#define TIERED_ONLY(code)\n-#define NOT_TIERED(code) code\n-#endif \/\/ TIERED\n+\/\/ COMPILER1 and COMPILER2\n+#if defined(COMPILER1) && defined(COMPILER2)\n+#define COMPILER1_AND_COMPILER2 1\n+#define COMPILER1_AND_COMPILER2_PRESENT(code) code\n+#else\n+#define COMPILER1_AND_COMPILER2 0\n+#define COMPILER1_AND_COMPILER2_PRESENT(code)\n+#endif\n+\n+\/\/ COMPILER1 or COMPILER2\n+#if defined(COMPILER1) || defined(COMPILER2)\n+#define COMPILER1_OR_COMPILER2 1\n+#define COMPILER1_OR_COMPILER2_PRESENT(code) code\n+#else\n+#define COMPILER1_OR_COMPILER2 0\n+#define COMPILER1_OR_COMPILER2_PRESENT(code)\n+#endif\n","filename":"src\/hotspot\/share\/utilities\/macros.hpp","additions":18,"deletions":11,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n-# Copyright (c) 2018, 2019, Oracle and\/or its affiliates. All rights reserved.\n+# Copyright (c) 2018, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -1787,1 +1787,0 @@\n-  vmTestbase\/jit\/tiered\/Test.java \\\n","filename":"test\/hotspot\/jtreg\/TEST.quick-groups","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -144,3 +144,1 @@\n-        COMPILE_THRESHOLD = WB.getBooleanVMFlag(\"TieredCompilation\")\n-                ? CompilerWhiteBoxTest.THRESHOLD\n-                : CompilerWhiteBoxTest.THRESHOLD * 2;\n+        COMPILE_THRESHOLD = CompilerWhiteBoxTest.THRESHOLD;\n","filename":"test\/hotspot\/jtreg\/compiler\/jvmci\/compilerToVM\/MaterializeVirtualObjectTest.java","additions":2,"deletions":4,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2013, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2013, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -80,28 +80,0 @@\n-\n-        if (testCase.isOsr()) {\n-            \/\/ part test isn't applicable for OSR test case\n-            return;\n-        }\n-        if (!TIERED_COMPILATION) {\n-            WHITE_BOX.clearMethodState(method);\n-            compile(COMPILE_THRESHOLD);\n-            checkCompiled();\n-\n-            deoptimize();\n-            checkNotCompiled();\n-            WHITE_BOX.clearMethodState(method);\n-\n-            \/\/ invoke method one less time than needed to compile\n-            if (COMPILE_THRESHOLD > 1) {\n-                compile(COMPILE_THRESHOLD - 1);\n-                checkNotCompiled();\n-            } else {\n-                System.err.println(\"Warning: 'CompileThreshold' <= 1\");\n-            }\n-\n-            compile(1);\n-            checkCompiled();\n-        } else {\n-            System.err.println(\n-                    \"Warning: part of test is not applicable in Tiered\");\n-        }\n","filename":"test\/hotspot\/jtreg\/compiler\/whitebox\/ClearMethodStateTest.java","additions":1,"deletions":29,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2013, 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2013, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -84,7 +84,1 @@\n-        if (TIERED_COMPILATION) {\n-            BACKEDGE_THRESHOLD = THRESHOLD = 150000;\n-        } else {\n-            THRESHOLD = COMPILE_THRESHOLD;\n-            BACKEDGE_THRESHOLD = Math.max(10000, COMPILE_THRESHOLD *\n-                    Long.parseLong(getVMOption(\"OnStackReplacePercentage\")));\n-        }\n+        BACKEDGE_THRESHOLD = THRESHOLD = 150000;\n","filename":"test\/hotspot\/jtreg\/compiler\/whitebox\/CompilerWhiteBoxTest.java","additions":2,"deletions":8,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -1,79 +0,0 @@\n-\/*\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-\n-\/*\n- * @test\n- *\n- * @summary converted from VM Testbase jit\/tiered.\n- * VM Testbase keywords: [jit, quick]\n- * VM Testbase readme:\n- * Description\n- *     The test verifies that JVM prints tiered events with -XX:+PrintTieredEvents\n- *     for tiered compilation explicitly enabled with -XX:+TieredCompilation.\n- *     If tiered compilation is explicitly disabled the test verifies that there are no\n- *     output from PrintTieredEvents.\n- *\n- * @comment the test can't be run w\/ jvmci compiler enabled as it enforces tiered compilation\n- * @requires vm.opt.UseJVMCICompiler != true\n- *\n- * @library \/vmTestbase\n- *          \/test\/lib\n- * @run driver vmTestbase.jit.tiered.Test\n- *\/\n-\n-package vmTestbase.jit.tiered;\n-\n-import jtreg.SkippedException;\n-import jdk.test.lib.process.OutputAnalyzer;\n-import jdk.test.lib.process.ProcessTools;\n-\n-public class Test {\n-    private static String UNSUPPORTED_OPTION_MESSAGE = \"-XX:+TieredCompilation not supported in this VM\";\n-    private static String REGEXP = \"^[0-9.]+: \\\\[compile level=\\\\d\";\n-    public static void main(String[] args) throws Exception {\n-        {\n-            System.out.println(\"TieredCompilation is enabled\");\n-            var pb = ProcessTools.createTestJvm(\n-                    \"-XX:+TieredCompilation\",\n-                    \"-XX:+PrintTieredEvents\",\n-                    \"-version\");\n-            var output = new OutputAnalyzer(pb.start());\n-            if (output.getStderr().contains(UNSUPPORTED_OPTION_MESSAGE)) {\n-                throw new SkippedException(UNSUPPORTED_OPTION_MESSAGE);\n-            }\n-            output.shouldHaveExitValue(0)\n-                  .stdoutShouldMatch(REGEXP);\n-        }\n-        {\n-            System.out.println(\"TieredCompilation is disabled\");\n-            var pb = ProcessTools.createTestJvm(\n-                    \"-XX:-TieredCompilation\",\n-                    \"-XX:+PrintTieredEvents\",\n-                    \"-version\");\n-            var output = new OutputAnalyzer(pb.start())\n-                    .shouldHaveExitValue(0)\n-                    .stdoutShouldNotMatch(REGEXP);\n-        }\n-    }\n-}\n","filename":"test\/hotspot\/jtreg\/vmTestbase\/jit\/tiered\/Test.java","additions":0,"deletions":79,"binary":false,"changes":79,"status":"deleted"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2007, 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2007, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -31,1 +31,1 @@\n-    public final static int run_for = 1000;\n+    public final static int run_for = 10000;\n","filename":"test\/hotspot\/jtreg\/vmTestbase\/nsk\/jvmti\/scenarios\/hotswap\/HS203\/hs203t004\/MyThread.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2007, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2007, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -33,1 +33,1 @@\n- *     While running the thread, it calls a method (doTask2() ) for number of times (1000).\n+ *     While running the thread, it calls a method (doTask2() ) for number of times (10000).\n","filename":"test\/hotspot\/jtreg\/vmTestbase\/nsk\/jvmti\/scenarios\/hotswap\/HS203\/hs203t004\/hs203t004.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"}]}