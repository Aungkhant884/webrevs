{"files":[{"patch":"@@ -1084,14 +1084,0 @@\n-\n-\/\/ Check GCLocker::needs_gc and enter the runtime if it's true.  This\n-\/\/ keeps a new JNI critical region from starting until a GC has been\n-\/\/ forced.  Save down any oops in registers and describe them in an\n-\/\/ OopMap.\n-static void check_needs_gc_for_critical_native(MacroAssembler* masm,\n-                                               int stack_slots,\n-                                               int total_c_args,\n-                                               int total_in_args,\n-                                               int arg_save_area,\n-                                               OopMapSet* oop_maps,\n-                                               VMRegPair* in_regs,\n-                                               BasicType* in_sig_bt) { Unimplemented(); }\n-\n@@ -1263,4 +1249,3 @@\n-\/\/ passing them to the callee and perform checks before and after the\n-\/\/ native call to ensure that they GCLocker\n-\/\/ lock_critical\/unlock_critical semantics are followed.  Some other\n-\/\/ parts of JNI setup are skipped like the tear down of the JNI handle\n+\/\/ passing them to the callee. Critical native functions leave the state _in_Java,\n+\/\/ since they block out GC.\n+\/\/ Some other parts of JNI setup are skipped like the tear down of the JNI handle\n@@ -1270,12 +1255,0 @@\n-\/\/ They are roughly structured like this:\n-\/\/    if (GCLocker::needs_gc())\n-\/\/      SharedRuntime::block_for_jni_critical();\n-\/\/    tranistion to thread_in_native\n-\/\/    unpack arrray arguments and call native entry point\n-\/\/    check for safepoint in progress\n-\/\/    check if any thread suspend flags are set\n-\/\/      call into JVM and possible unlock the JNI critical\n-\/\/      if a GC was suppressed while in the critical native.\n-\/\/    transition back to thread_in_Java\n-\/\/    return to caller\n-\/\/\n@@ -1549,5 +1522,0 @@\n-  if (is_critical_native) {\n-    check_needs_gc_for_critical_native(masm, stack_slots, total_c_args, total_in_args,\n-                                       oop_handle_offset, oop_maps, in_regs, in_sig_bt);\n-  }\n-\n@@ -1826,1 +1794,0 @@\n-  }\n@@ -1828,4 +1795,5 @@\n-  \/\/ Now set thread in native\n-  __ mov(rscratch1, _thread_in_native);\n-  __ lea(rscratch2, Address(rthread, JavaThread::thread_state_offset()));\n-  __ stlrw(rscratch1, rscratch2);\n+    \/\/ Now set thread in native\n+    __ mov(rscratch1, _thread_in_native);\n+    __ lea(rscratch2, Address(rthread, JavaThread::thread_state_offset()));\n+    __ stlrw(rscratch1, rscratch2);\n+  }\n@@ -1859,0 +1827,15 @@\n+  Label safepoint_in_progress, safepoint_in_progress_done;\n+  Label after_transition;\n+\n+  \/\/ If this is a critical native, check for a safepoint or suspend request after the call.\n+  \/\/ If a safepoint is needed, transition to native, then to native_trans to handle\n+  \/\/ safepoints like the native methods that are not critical natives.\n+  if (is_critical_native) {\n+    Label needs_safepoint;\n+    __ safepoint_poll(needs_safepoint, false \/* at_return *\/, true \/* acquire *\/, false \/* in_nmethod *\/);\n+    __ ldrw(rscratch1, Address(rthread, JavaThread::suspend_flags_offset()));\n+    __ cbnzw(rscratch1, needs_safepoint);\n+    __ b(after_transition);\n+    __ bind(needs_safepoint);\n+  }\n+\n@@ -1879,1 +1862,0 @@\n-  Label safepoint_in_progress, safepoint_in_progress_done;\n@@ -1897,1 +1879,0 @@\n-  Label after_transition;\n@@ -2102,5 +2083,1 @@\n-    if (!is_critical_native) {\n-      __ lea(rscratch1, RuntimeAddress(CAST_FROM_FN_PTR(address, JavaThread::check_special_condition_for_native_trans)));\n-    } else {\n-      __ lea(rscratch1, RuntimeAddress(CAST_FROM_FN_PTR(address, JavaThread::check_special_condition_for_native_trans_and_transition)));\n-    }\n+    __ lea(rscratch1, RuntimeAddress(CAST_FROM_FN_PTR(address, JavaThread::check_special_condition_for_native_trans)));\n@@ -2112,6 +2089,0 @@\n-    if (is_critical_native) {\n-      \/\/ The call above performed the transition to thread_in_Java so\n-      \/\/ skip the transition logic above.\n-      __ b(after_transition);\n-    }\n-\n@@ -2166,4 +2137,0 @@\n-  if (is_critical_native) {\n-    nm->set_lazy_critical_native(true);\n-  }\n-\n@@ -2171,1 +2138,0 @@\n-\n","filename":"src\/hotspot\/cpu\/aarch64\/sharedRuntime_aarch64.cpp","additions":24,"deletions":58,"binary":false,"changes":82,"status":"modified"},{"patch":"@@ -1533,150 +1533,0 @@\n-static void save_or_restore_arguments(MacroAssembler* masm,\n-                                      const int stack_slots,\n-                                      const int total_in_args,\n-                                      const int arg_save_area,\n-                                      OopMap* map,\n-                                      VMRegPair* in_regs,\n-                                      BasicType* in_sig_bt) {\n-  \/\/ If map is non-NULL then the code should store the values,\n-  \/\/ otherwise it should load them.\n-  int slot = arg_save_area;\n-  \/\/ Save down double word first.\n-  for (int i = 0; i < total_in_args; i++) {\n-    if (in_regs[i].first()->is_FloatRegister() && in_sig_bt[i] == T_DOUBLE) {\n-      int offset = slot * VMRegImpl::stack_slot_size;\n-      slot += VMRegImpl::slots_per_word;\n-      assert(slot <= stack_slots, \"overflow (after DOUBLE stack slot)\");\n-      if (map != NULL) {\n-        __ stfd(in_regs[i].first()->as_FloatRegister(), offset, R1_SP);\n-      } else {\n-        __ lfd(in_regs[i].first()->as_FloatRegister(), offset, R1_SP);\n-      }\n-    } else if (in_regs[i].first()->is_Register() &&\n-        (in_sig_bt[i] == T_LONG || in_sig_bt[i] == T_ARRAY)) {\n-      int offset = slot * VMRegImpl::stack_slot_size;\n-      if (map != NULL) {\n-        __ std(in_regs[i].first()->as_Register(), offset, R1_SP);\n-        if (in_sig_bt[i] == T_ARRAY) {\n-          map->set_oop(VMRegImpl::stack2reg(slot));\n-        }\n-      } else {\n-        __ ld(in_regs[i].first()->as_Register(), offset, R1_SP);\n-      }\n-      slot += VMRegImpl::slots_per_word;\n-      assert(slot <= stack_slots, \"overflow (after LONG\/ARRAY stack slot)\");\n-    }\n-  }\n-  \/\/ Save or restore single word registers.\n-  for (int i = 0; i < total_in_args; i++) {\n-    if (in_regs[i].first()->is_Register()) {\n-      int offset = slot * VMRegImpl::stack_slot_size;\n-      \/\/ Value lives in an input register. Save it on stack.\n-      switch (in_sig_bt[i]) {\n-        case T_BOOLEAN:\n-        case T_CHAR:\n-        case T_BYTE:\n-        case T_SHORT:\n-        case T_INT:\n-          if (map != NULL) {\n-            __ stw(in_regs[i].first()->as_Register(), offset, R1_SP);\n-          } else {\n-            __ lwa(in_regs[i].first()->as_Register(), offset, R1_SP);\n-          }\n-          slot++;\n-          assert(slot <= stack_slots, \"overflow (after INT or smaller stack slot)\");\n-          break;\n-        case T_ARRAY:\n-        case T_LONG:\n-          \/\/ handled above\n-          break;\n-        case T_OBJECT:\n-        default: ShouldNotReachHere();\n-      }\n-    } else if (in_regs[i].first()->is_FloatRegister()) {\n-      if (in_sig_bt[i] == T_FLOAT) {\n-        int offset = slot * VMRegImpl::stack_slot_size;\n-        slot++;\n-        assert(slot <= stack_slots, \"overflow (after FLOAT stack slot)\");\n-        if (map != NULL) {\n-          __ stfs(in_regs[i].first()->as_FloatRegister(), offset, R1_SP);\n-        } else {\n-          __ lfs(in_regs[i].first()->as_FloatRegister(), offset, R1_SP);\n-        }\n-      }\n-    } else if (in_regs[i].first()->is_stack()) {\n-      if (in_sig_bt[i] == T_ARRAY && map != NULL) {\n-        int offset_in_older_frame = in_regs[i].first()->reg2stack() + SharedRuntime::out_preserve_stack_slots();\n-        map->set_oop(VMRegImpl::stack2reg(offset_in_older_frame + stack_slots));\n-      }\n-    }\n-  }\n-}\n-\n-\/\/ Check GCLocker::needs_gc and enter the runtime if it's true. This\n-\/\/ keeps a new JNI critical region from starting until a GC has been\n-\/\/ forced. Save down any oops in registers and describe them in an\n-\/\/ OopMap.\n-static void check_needs_gc_for_critical_native(MacroAssembler* masm,\n-                                               const int stack_slots,\n-                                               const int total_in_args,\n-                                               const int arg_save_area,\n-                                               OopMapSet* oop_maps,\n-                                               VMRegPair* in_regs,\n-                                               BasicType* in_sig_bt,\n-                                               Register tmp_reg ) {\n-  __ block_comment(\"check GCLocker::needs_gc\");\n-  Label cont;\n-  __ lbz(tmp_reg, (RegisterOrConstant)(intptr_t)GCLocker::needs_gc_address());\n-  __ cmplwi(CCR0, tmp_reg, 0);\n-  __ beq(CCR0, cont);\n-\n-  \/\/ Save down any values that are live in registers and call into the\n-  \/\/ runtime to halt for a GC.\n-  OopMap* map = new OopMap(stack_slots * 2, 0 \/* arg_slots*\/);\n-  save_or_restore_arguments(masm, stack_slots, total_in_args,\n-                            arg_save_area, map, in_regs, in_sig_bt);\n-\n-  __ mr(R3_ARG1, R16_thread);\n-  __ set_last_Java_frame(R1_SP, noreg);\n-\n-  __ block_comment(\"block_for_jni_critical\");\n-  address entry_point = CAST_FROM_FN_PTR(address, SharedRuntime::block_for_jni_critical);\n-#if defined(ABI_ELFv2)\n-  __ call_c(entry_point, relocInfo::runtime_call_type);\n-#else\n-  __ call_c(CAST_FROM_FN_PTR(FunctionDescriptor*, entry_point), relocInfo::runtime_call_type);\n-#endif\n-  address start           = __ pc() - __ offset(),\n-          calls_return_pc = __ last_calls_return_pc();\n-  oop_maps->add_gc_map(calls_return_pc - start, map);\n-\n-  __ reset_last_Java_frame();\n-\n-  \/\/ Reload all the register arguments.\n-  save_or_restore_arguments(masm, stack_slots, total_in_args,\n-                            arg_save_area, NULL, in_regs, in_sig_bt);\n-\n-  __ BIND(cont);\n-\n-#ifdef ASSERT\n-  if (StressCriticalJNINatives) {\n-    \/\/ Stress register saving.\n-    OopMap* map = new OopMap(stack_slots * 2, 0 \/* arg_slots*\/);\n-    save_or_restore_arguments(masm, stack_slots, total_in_args,\n-                              arg_save_area, map, in_regs, in_sig_bt);\n-    \/\/ Destroy argument registers.\n-    for (int i = 0; i < total_in_args; i++) {\n-      if (in_regs[i].first()->is_Register()) {\n-        const Register reg = in_regs[i].first()->as_Register();\n-        __ neg(reg, reg);\n-      } else if (in_regs[i].first()->is_FloatRegister()) {\n-        __ fneg(in_regs[i].first()->as_FloatRegister(), in_regs[i].first()->as_FloatRegister());\n-      }\n-    }\n-\n-    save_or_restore_arguments(masm, stack_slots, total_in_args,\n-                              arg_save_area, NULL, in_regs, in_sig_bt);\n-  }\n-#endif\n-}\n-\n@@ -1824,4 +1674,3 @@\n-\/\/ passing them to the callee and perform checks before and after the\n-\/\/ native call to ensure that they GCLocker\n-\/\/ lock_critical\/unlock_critical semantics are followed.  Some other\n-\/\/ parts of JNI setup are skipped like the tear down of the JNI handle\n+\/\/ passing them to the callee. Critical native functions leave the state _in_Java,\n+\/\/ since they cannot stop for GC.\n+\/\/ Some other parts of JNI setup are skipped like the tear down of the JNI handle\n@@ -1831,12 +1680,0 @@\n-\/\/ They are roughly structured like this:\n-\/\/   if (GCLocker::needs_gc())\n-\/\/     SharedRuntime::block_for_jni_critical();\n-\/\/   tranistion to thread_in_native\n-\/\/   unpack arrray arguments and call native entry point\n-\/\/   check for safepoint in progress\n-\/\/   check if any thread suspend flags are set\n-\/\/     call into JVM and possible unlock the JNI critical\n-\/\/     if a GC was suppressed while in the critical native.\n-\/\/   transition back to thread_in_Java\n-\/\/   return to caller\n-\/\/\n@@ -2149,5 +1986,0 @@\n-  if (is_critical_native) {\n-    check_needs_gc_for_critical_native(masm, stack_slots, total_in_args, oop_handle_slot_offset,\n-                                       oop_maps, in_regs, in_sig_bt, r_temp_1);\n-  }\n-\n@@ -2354,4 +2186,0 @@\n-\n-  \/\/ Publish thread state\n-  \/\/ --------------------------------------------------------------------------\n-\n@@ -2361,5 +2189,10 @@\n-  \/\/ Transition from _thread_in_Java to _thread_in_native.\n-  __ li(R0, _thread_in_native);\n-  __ release();\n-  \/\/ TODO: PPC port assert(4 == JavaThread::sz_thread_state(), \"unexpected field size\");\n-  __ stw(R0, thread_(thread_state));\n+  if (!is_critical_native) {\n+    \/\/ Publish thread state\n+    \/\/ --------------------------------------------------------------------------\n+\n+    \/\/ Transition from _thread_in_Java to _thread_in_native.\n+    __ li(R0, _thread_in_native);\n+    __ release();\n+    \/\/ TODO: PPC port assert(4 == JavaThread::sz_thread_state(), \"unexpected field size\");\n+    __ stw(R0, thread_(thread_state));\n+  }\n@@ -2425,0 +2258,16 @@\n+  Label after_transition;\n+\n+  \/\/ If this is a critical native, check for a safepoint or suspend request after the call.\n+  \/\/ If a safepoint is needed, transition to native, then to native_trans to handle\n+  \/\/ safepoints like the native methods that are not critical natives.\n+  if (is_critical_native) {\n+    Label needs_safepoint;\n+    Register sync_state      = r_temp_5;\n+    __ safepoint_poll(needs_safepoint, sync_state);\n+\n+    Register suspend_flags   = r_temp_6;\n+    __ lwz(suspend_flags, thread_(suspend_flags));\n+    __ cmpwi(CCR1, suspend_flags, 0);\n+    __ beq(CCR1, after_transition);\n+    __ bind(needs_safepoint);\n+  }\n@@ -2452,1 +2301,0 @@\n-  Label after_transition;\n@@ -2480,3 +2328,2 @@\n-    address entry_point = is_critical_native\n-      ? CAST_FROM_FN_PTR(address, JavaThread::check_special_condition_for_native_trans_and_transition)\n-      : CAST_FROM_FN_PTR(address, JavaThread::check_special_condition_for_native_trans);\n+    address entry_point =\n+      CAST_FROM_FN_PTR(address, JavaThread::check_special_condition_for_native_trans);\n@@ -2487,3 +2334,0 @@\n-    if (is_critical_native) {\n-      __ b(after_transition); \/\/ No thread state transition here.\n-    }\n@@ -2491,1 +2335,0 @@\n-  }\n@@ -2493,2 +2336,2 @@\n-  \/\/ Publish thread state.\n-  \/\/ --------------------------------------------------------------------------\n+    \/\/ Publish thread state.\n+    \/\/ --------------------------------------------------------------------------\n@@ -2496,2 +2339,2 @@\n-  \/\/ Thread state is thread_in_native_trans. Any safepoint blocking has\n-  \/\/ already happened so we can now change state to _thread_in_Java.\n+    \/\/ Thread state is thread_in_native_trans. Any safepoint blocking has\n+    \/\/ already happened so we can now change state to _thread_in_Java.\n@@ -2499,6 +2342,7 @@\n-  \/\/ Transition from _thread_in_native_trans to _thread_in_Java.\n-  __ li(R0, _thread_in_Java);\n-  __ lwsync(); \/\/ Acquire safepoint and suspend state, release thread state.\n-  \/\/ TODO: PPC port assert(4 == JavaThread::sz_thread_state(), \"unexpected field size\");\n-  __ stw(R0, thread_(thread_state));\n-  __ bind(after_transition);\n+    \/\/ Transition from _thread_in_native_trans to _thread_in_Java.\n+    __ li(R0, _thread_in_Java);\n+    __ lwsync(); \/\/ Acquire safepoint and suspend state, release thread state.\n+    \/\/ TODO: PPC port assert(4 == JavaThread::sz_thread_state(), \"unexpected field size\");\n+    __ stw(R0, thread_(thread_state));\n+    __ bind(after_transition);\n+  }\n@@ -2661,4 +2505,0 @@\n-  if (is_critical_native) {\n-    nm->set_lazy_critical_native(true);\n-  }\n-\n","filename":"src\/hotspot\/cpu\/ppc\/sharedRuntime_ppc.cpp","additions":42,"deletions":202,"binary":false,"changes":244,"status":"modified"},{"patch":"@@ -1288,157 +1288,0 @@\n-static void save_or_restore_arguments(MacroAssembler *masm,\n-                                      const int stack_slots,\n-                                      const int total_in_args,\n-                                      const int arg_save_area,\n-                                      OopMap *map,\n-                                      VMRegPair *in_regs,\n-                                      BasicType *in_sig_bt) {\n-\n-  \/\/ If map is non-NULL then the code should store the values,\n-  \/\/ otherwise it should load them.\n-  int slot = arg_save_area;\n-  \/\/ Handle double words first.\n-  for (int i = 0; i < total_in_args; i++) {\n-    if (in_regs[i].first()->is_FloatRegister() && in_sig_bt[i] == T_DOUBLE) {\n-      int offset = slot * VMRegImpl::stack_slot_size;\n-      slot += VMRegImpl::slots_per_word;\n-      assert(slot <= stack_slots, \"overflow (after DOUBLE stack slot)\");\n-      const FloatRegister   freg = in_regs[i].first()->as_FloatRegister();\n-      Address   stackaddr(Z_SP, offset);\n-      if (map != NULL) {\n-        __ freg2mem_opt(freg, stackaddr);\n-      } else {\n-        __ mem2freg_opt(freg, stackaddr);\n-      }\n-    } else if (in_regs[i].first()->is_Register() &&\n-               (in_sig_bt[i] == T_LONG || in_sig_bt[i] == T_ARRAY)) {\n-      int offset = slot * VMRegImpl::stack_slot_size;\n-      const Register   reg = in_regs[i].first()->as_Register();\n-      if (map != NULL) {\n-        __ z_stg(reg, offset, Z_SP);\n-        if (in_sig_bt[i] == T_ARRAY) {\n-          map->set_oop(VMRegImpl::stack2reg(slot));\n-        }\n-      } else {\n-        __ z_lg(reg, offset, Z_SP);\n-      }\n-      slot += VMRegImpl::slots_per_word;\n-      assert(slot <= stack_slots, \"overflow (after LONG\/ARRAY stack slot)\");\n-    }\n-  }\n-\n-  \/\/ Save or restore single word registers.\n-  for (int i = 0; i < total_in_args; i++) {\n-    if (in_regs[i].first()->is_Register()) {\n-      int offset = slot * VMRegImpl::stack_slot_size;\n-      \/\/ Value lives in an input register. Save it on stack.\n-      switch (in_sig_bt[i]) {\n-        case T_BOOLEAN:\n-        case T_CHAR:\n-        case T_BYTE:\n-        case T_SHORT:\n-        case T_INT: {\n-          const Register   reg = in_regs[i].first()->as_Register();\n-          Address   stackaddr(Z_SP, offset);\n-          if (map != NULL) {\n-            __ z_st(reg, stackaddr);\n-          } else {\n-            __ z_lgf(reg, stackaddr);\n-          }\n-          slot++;\n-          assert(slot <= stack_slots, \"overflow (after INT or smaller stack slot)\");\n-          break;\n-        }\n-        case T_ARRAY:\n-        case T_LONG:\n-          \/\/ handled above\n-          break;\n-        case T_OBJECT:\n-        default: ShouldNotReachHere();\n-      }\n-    } else if (in_regs[i].first()->is_FloatRegister()) {\n-      if (in_sig_bt[i] == T_FLOAT) {\n-        int offset = slot * VMRegImpl::stack_slot_size;\n-        slot++;\n-        assert(slot <= stack_slots, \"overflow (after FLOAT stack slot)\");\n-        const FloatRegister   freg = in_regs[i].first()->as_FloatRegister();\n-        Address   stackaddr(Z_SP, offset);\n-        if (map != NULL) {\n-          __ freg2mem_opt(freg, stackaddr, false);\n-        } else {\n-          __ mem2freg_opt(freg, stackaddr, false);\n-        }\n-      }\n-    } else if (in_regs[i].first()->is_stack() &&\n-               in_sig_bt[i] == T_ARRAY && map != NULL) {\n-      int offset_in_older_frame = in_regs[i].first()->reg2stack() + SharedRuntime::out_preserve_stack_slots();\n-      map->set_oop(VMRegImpl::stack2reg(offset_in_older_frame + stack_slots));\n-    }\n-  }\n-}\n-\n-\/\/ Check GCLocker::needs_gc and enter the runtime if it's true. This\n-\/\/ keeps a new JNI critical region from starting until a GC has been\n-\/\/ forced. Save down any oops in registers and describe them in an OopMap.\n-static void check_needs_gc_for_critical_native(MacroAssembler   *masm,\n-                                                const int stack_slots,\n-                                                const int total_in_args,\n-                                                const int arg_save_area,\n-                                                OopMapSet *oop_maps,\n-                                                VMRegPair *in_regs,\n-                                                BasicType *in_sig_bt) {\n-  __ block_comment(\"check GCLocker::needs_gc\");\n-  Label cont;\n-\n-  \/\/ Check GCLocker::_needs_gc flag.\n-  __ load_const_optimized(Z_R1_scratch, (long) GCLocker::needs_gc_address());\n-  __ z_cli(0, Z_R1_scratch, 0);\n-  __ z_bre(cont);\n-\n-  \/\/ Save down any values that are live in registers and call into the\n-  \/\/ runtime to halt for a GC.\n-  OopMap *map = new OopMap(stack_slots * 2, 0 \/* arg_slots*\/);\n-\n-  save_or_restore_arguments(masm, stack_slots, total_in_args,\n-                            arg_save_area, map, in_regs, in_sig_bt);\n-  address the_pc = __ pc();\n-  __ set_last_Java_frame(Z_SP, noreg);\n-\n-  __ block_comment(\"block_for_jni_critical\");\n-  __ z_lgr(Z_ARG1, Z_thread);\n-\n-  address entry_point = CAST_FROM_FN_PTR(address, SharedRuntime::block_for_jni_critical);\n-  __ call_c(entry_point);\n-  oop_maps->add_gc_map(__ offset(), map);\n-\n-  __ reset_last_Java_frame();\n-\n-  \/\/ Reload all the register arguments.\n-  save_or_restore_arguments(masm, stack_slots, total_in_args,\n-                            arg_save_area, NULL, in_regs, in_sig_bt);\n-\n-  __ bind(cont);\n-\n-  if (StressCriticalJNINatives) {\n-    \/\/ Stress register saving\n-    OopMap *map = new OopMap(stack_slots * 2, 0 \/* arg_slots*\/);\n-    save_or_restore_arguments(masm, stack_slots, total_in_args,\n-                              arg_save_area, map, in_regs, in_sig_bt);\n-\n-    \/\/ Destroy argument registers.\n-    for (int i = 0; i < total_in_args; i++) {\n-      if (in_regs[i].first()->is_Register()) {\n-        \/\/ Don't set CC.\n-        __ clear_reg(in_regs[i].first()->as_Register(), true, false);\n-      } else {\n-        if (in_regs[i].first()->is_FloatRegister()) {\n-          FloatRegister fr = in_regs[i].first()->as_FloatRegister();\n-          __ z_lcdbr(fr, fr);\n-        }\n-      }\n-    }\n-\n-    save_or_restore_arguments(masm, stack_slots, total_in_args,\n-                              arg_save_area, NULL, in_regs, in_sig_bt);\n-  }\n-}\n-\n@@ -1861,6 +1704,0 @@\n-  if (is_critical_native) {\n-    check_needs_gc_for_critical_native(masm, stack_slots, total_in_args,\n-                                       oop_handle_slot_offset, oop_maps, in_regs, in_sig_bt);\n-  }\n-\n-\n@@ -2095,3 +1932,4 @@\n-  \/\/ Transition from _thread_in_Java to _thread_in_native.\n-  __ set_thread_state(_thread_in_native);\n-\n+  if (!is_critical_native) {\n+    \/\/ Transition from _thread_in_Java to _thread_in_native.\n+    __ set_thread_state(_thread_in_native);\n+  }\n@@ -2143,0 +1981,13 @@\n+  Label after_transition;\n+\n+  \/\/ If this is a critical native, check for a safepoint or suspend request after the call.\n+  \/\/ If a safepoint is needed, transition to native, then to native_trans to handle\n+  \/\/ safepoints like the native methods that are not critical natives.\n+  if (is_critical_native) {\n+    Label needs_safepoint;\n+    \/\/ Does this need to save_native_result and fences?\n+    __ safepoint_poll(needs_safepoint, Z_R1);\n+    __ load_and_test_int(Z_R0, Address(Z_thread, JavaThread::suspend_flags_offset()));\n+    __ z_bre(after_transition);\n+    __ bind(needs_safepoint);\n+  }\n@@ -2162,1 +2013,0 @@\n-  Label after_transition;\n@@ -2184,2 +2034,1 @@\n-    address entry_point = is_critical_native ? CAST_FROM_FN_PTR(address, JavaThread::check_special_condition_for_native_trans_and_transition)\n-                                             : CAST_FROM_FN_PTR(address, JavaThread::check_special_condition_for_native_trans);\n+    address entry_point = CAST_FROM_FN_PTR(address, JavaThread::check_special_condition_for_native_trans);\n@@ -2189,4 +2038,0 @@\n-    if (is_critical_native) {\n-      restore_native_result(masm, ret_type, workspace_slot_offset);\n-      __ z_bru(after_transition); \/\/ No thread state transition here.\n-    }\n@@ -2205,1 +2050,0 @@\n-\n@@ -2388,4 +2232,0 @@\n-  if (is_critical_native) {\n-    nm->set_lazy_critical_native(true);\n-  }\n-\n","filename":"src\/hotspot\/cpu\/s390\/sharedRuntime_s390.cpp","additions":18,"deletions":178,"binary":false,"changes":196,"status":"modified"},{"patch":"@@ -1217,259 +1217,0 @@\n-\n-static void save_or_restore_arguments(MacroAssembler* masm,\n-                                      const int stack_slots,\n-                                      const int total_in_args,\n-                                      const int arg_save_area,\n-                                      OopMap* map,\n-                                      VMRegPair* in_regs,\n-                                      BasicType* in_sig_bt) {\n-  \/\/ if map is non-NULL then the code should store the values,\n-  \/\/ otherwise it should load them.\n-  int handle_index = 0;\n-  \/\/ Save down double word first\n-  for ( int i = 0; i < total_in_args; i++) {\n-    if (in_regs[i].first()->is_XMMRegister() && in_sig_bt[i] == T_DOUBLE) {\n-      int slot = handle_index * VMRegImpl::slots_per_word + arg_save_area;\n-      int offset = slot * VMRegImpl::stack_slot_size;\n-      handle_index += 2;\n-      assert(handle_index <= stack_slots, \"overflow\");\n-      if (map != NULL) {\n-        __ movdbl(Address(rsp, offset), in_regs[i].first()->as_XMMRegister());\n-      } else {\n-        __ movdbl(in_regs[i].first()->as_XMMRegister(), Address(rsp, offset));\n-      }\n-    }\n-    if (in_regs[i].first()->is_Register() && in_sig_bt[i] == T_LONG) {\n-      int slot = handle_index * VMRegImpl::slots_per_word + arg_save_area;\n-      int offset = slot * VMRegImpl::stack_slot_size;\n-      handle_index += 2;\n-      assert(handle_index <= stack_slots, \"overflow\");\n-      if (map != NULL) {\n-        __ movl(Address(rsp, offset), in_regs[i].first()->as_Register());\n-        if (in_regs[i].second()->is_Register()) {\n-          __ movl(Address(rsp, offset + 4), in_regs[i].second()->as_Register());\n-        }\n-      } else {\n-        __ movl(in_regs[i].first()->as_Register(), Address(rsp, offset));\n-        if (in_regs[i].second()->is_Register()) {\n-          __ movl(in_regs[i].second()->as_Register(), Address(rsp, offset + 4));\n-        }\n-      }\n-    }\n-  }\n-  \/\/ Save or restore single word registers\n-  for ( int i = 0; i < total_in_args; i++) {\n-    if (in_regs[i].first()->is_Register()) {\n-      int slot = handle_index++ * VMRegImpl::slots_per_word + arg_save_area;\n-      int offset = slot * VMRegImpl::stack_slot_size;\n-      assert(handle_index <= stack_slots, \"overflow\");\n-      if (in_sig_bt[i] == T_ARRAY && map != NULL) {\n-        map->set_oop(VMRegImpl::stack2reg(slot));;\n-      }\n-\n-      \/\/ Value is in an input register pass we must flush it to the stack\n-      const Register reg = in_regs[i].first()->as_Register();\n-      switch (in_sig_bt[i]) {\n-        case T_ARRAY:\n-          if (map != NULL) {\n-            __ movptr(Address(rsp, offset), reg);\n-          } else {\n-            __ movptr(reg, Address(rsp, offset));\n-          }\n-          break;\n-        case T_BOOLEAN:\n-        case T_CHAR:\n-        case T_BYTE:\n-        case T_SHORT:\n-        case T_INT:\n-          if (map != NULL) {\n-            __ movl(Address(rsp, offset), reg);\n-          } else {\n-            __ movl(reg, Address(rsp, offset));\n-          }\n-          break;\n-        case T_OBJECT:\n-        default: ShouldNotReachHere();\n-      }\n-    } else if (in_regs[i].first()->is_XMMRegister()) {\n-      if (in_sig_bt[i] == T_FLOAT) {\n-        int slot = handle_index++ * VMRegImpl::slots_per_word + arg_save_area;\n-        int offset = slot * VMRegImpl::stack_slot_size;\n-        assert(handle_index <= stack_slots, \"overflow\");\n-        if (map != NULL) {\n-          __ movflt(Address(rsp, offset), in_regs[i].first()->as_XMMRegister());\n-        } else {\n-          __ movflt(in_regs[i].first()->as_XMMRegister(), Address(rsp, offset));\n-        }\n-      }\n-    } else if (in_regs[i].first()->is_stack()) {\n-      if (in_sig_bt[i] == T_ARRAY && map != NULL) {\n-        int offset_in_older_frame = in_regs[i].first()->reg2stack() + SharedRuntime::out_preserve_stack_slots();\n-        map->set_oop(VMRegImpl::stack2reg(offset_in_older_frame + stack_slots));\n-      }\n-    }\n-  }\n-}\n-\n-\/\/ Registers need to be saved for runtime call\n-static Register caller_saved_registers[] = {\n-  rcx, rdx, rsi, rdi\n-};\n-\n-\/\/ Save caller saved registers except r1 and r2\n-static void save_registers_except(MacroAssembler* masm, Register r1, Register r2) {\n-  int reg_len = (int)(sizeof(caller_saved_registers) \/ sizeof(Register));\n-  for (int index = 0; index < reg_len; index ++) {\n-    Register this_reg = caller_saved_registers[index];\n-    if (this_reg != r1 && this_reg != r2) {\n-      __ push(this_reg);\n-    }\n-  }\n-}\n-\n-\/\/ Restore caller saved registers except r1 and r2\n-static void restore_registers_except(MacroAssembler* masm, Register r1, Register r2) {\n-  int reg_len = (int)(sizeof(caller_saved_registers) \/ sizeof(Register));\n-  for (int index = reg_len - 1; index >= 0; index --) {\n-    Register this_reg = caller_saved_registers[index];\n-    if (this_reg != r1 && this_reg != r2) {\n-      __ pop(this_reg);\n-    }\n-  }\n-}\n-\n-\/\/ Pin object, return pinned object or null in rax\n-static void gen_pin_object(MacroAssembler* masm,\n-                           Register thread, VMRegPair reg) {\n-  __ block_comment(\"gen_pin_object {\");\n-\n-  Label is_null;\n-  Register tmp_reg = rax;\n-  VMRegPair tmp(tmp_reg->as_VMReg());\n-  if (reg.first()->is_stack()) {\n-    \/\/ Load the arg up from the stack\n-    simple_move32(masm, reg, tmp);\n-    reg = tmp;\n-  } else {\n-    __ movl(tmp_reg, reg.first()->as_Register());\n-  }\n-  __ testptr(reg.first()->as_Register(), reg.first()->as_Register());\n-  __ jccb(Assembler::equal, is_null);\n-\n-  \/\/ Save registers that may be used by runtime call\n-  Register arg = reg.first()->is_Register() ? reg.first()->as_Register() : noreg;\n-  save_registers_except(masm, arg, thread);\n-\n-  __ call_VM_leaf(\n-    CAST_FROM_FN_PTR(address, SharedRuntime::pin_object),\n-    thread, reg.first()->as_Register());\n-\n-  \/\/ Restore saved registers\n-  restore_registers_except(masm, arg, thread);\n-\n-  __ bind(is_null);\n-  __ block_comment(\"} gen_pin_object\");\n-}\n-\n-\/\/ Unpin object\n-static void gen_unpin_object(MacroAssembler* masm,\n-                             Register thread, VMRegPair reg) {\n-  __ block_comment(\"gen_unpin_object {\");\n-  Label is_null;\n-\n-  \/\/ temp register\n-  __ push(rax);\n-  Register tmp_reg = rax;\n-  VMRegPair tmp(tmp_reg->as_VMReg());\n-\n-  simple_move32(masm, reg, tmp);\n-\n-  __ testptr(rax, rax);\n-  __ jccb(Assembler::equal, is_null);\n-\n-  \/\/ Save registers that may be used by runtime call\n-  Register arg = reg.first()->is_Register() ? reg.first()->as_Register() : noreg;\n-  save_registers_except(masm, arg, thread);\n-\n-  __ call_VM_leaf(\n-    CAST_FROM_FN_PTR(address, SharedRuntime::unpin_object),\n-    thread, rax);\n-\n-  \/\/ Restore saved registers\n-  restore_registers_except(masm, arg, thread);\n-  __ bind(is_null);\n-  __ pop(rax);\n-  __ block_comment(\"} gen_unpin_object\");\n-}\n-\n-\/\/ Check GCLocker::needs_gc and enter the runtime if it's true.  This\n-\/\/ keeps a new JNI critical region from starting until a GC has been\n-\/\/ forced.  Save down any oops in registers and describe them in an\n-\/\/ OopMap.\n-static void check_needs_gc_for_critical_native(MacroAssembler* masm,\n-                                               Register thread,\n-                                               int stack_slots,\n-                                               int total_c_args,\n-                                               int total_in_args,\n-                                               int arg_save_area,\n-                                               OopMapSet* oop_maps,\n-                                               VMRegPair* in_regs,\n-                                               BasicType* in_sig_bt) {\n-  __ block_comment(\"check GCLocker::needs_gc\");\n-  Label cont;\n-  __ cmp8(ExternalAddress((address)GCLocker::needs_gc_address()), false);\n-  __ jcc(Assembler::equal, cont);\n-\n-  \/\/ Save down any incoming oops and call into the runtime to halt for a GC\n-\n-  OopMap* map = new OopMap(stack_slots * 2, 0 \/* arg_slots*\/);\n-\n-  save_or_restore_arguments(masm, stack_slots, total_in_args,\n-                            arg_save_area, map, in_regs, in_sig_bt);\n-\n-  address the_pc = __ pc();\n-  oop_maps->add_gc_map( __ offset(), map);\n-  __ set_last_Java_frame(thread, rsp, noreg, the_pc);\n-\n-  __ block_comment(\"block_for_jni_critical\");\n-  __ push(thread);\n-  __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, SharedRuntime::block_for_jni_critical)));\n-  __ increment(rsp, wordSize);\n-\n-  __ get_thread(thread);\n-  __ reset_last_Java_frame(thread, false);\n-\n-  save_or_restore_arguments(masm, stack_slots, total_in_args,\n-                            arg_save_area, NULL, in_regs, in_sig_bt);\n-\n-  __ bind(cont);\n-#ifdef ASSERT\n-  if (StressCriticalJNINatives) {\n-    \/\/ Stress register saving\n-    OopMap* map = new OopMap(stack_slots * 2, 0 \/* arg_slots*\/);\n-    save_or_restore_arguments(masm, stack_slots, total_in_args,\n-                              arg_save_area, map, in_regs, in_sig_bt);\n-    \/\/ Destroy argument registers\n-    for (int i = 0; i < total_in_args - 1; i++) {\n-      if (in_regs[i].first()->is_Register()) {\n-        const Register reg = in_regs[i].first()->as_Register();\n-        __ xorptr(reg, reg);\n-      } else if (in_regs[i].first()->is_XMMRegister()) {\n-        __ xorpd(in_regs[i].first()->as_XMMRegister(), in_regs[i].first()->as_XMMRegister());\n-      } else if (in_regs[i].first()->is_FloatRegister()) {\n-        ShouldNotReachHere();\n-      } else if (in_regs[i].first()->is_stack()) {\n-        \/\/ Nothing to do\n-      } else {\n-        ShouldNotReachHere();\n-      }\n-      if (in_sig_bt[i] == T_LONG || in_sig_bt[i] == T_DOUBLE) {\n-        i++;\n-      }\n-    }\n-\n-    save_or_restore_arguments(masm, stack_slots, total_in_args,\n-                              arg_save_area, NULL, in_regs, in_sig_bt);\n-  }\n-#endif\n-}\n-\n@@ -1600,4 +1341,3 @@\n-\/\/ passing them to the callee and perform checks before and after the\n-\/\/ native call to ensure that they GCLocker\n-\/\/ lock_critical\/unlock_critical semantics are followed.  Some other\n-\/\/ parts of JNI setup are skipped like the tear down of the JNI handle\n+\/\/ passing them to the callee. Critical native functions leave the state _in_Java,\n+\/\/ since they cannot stop for GC.\n+\/\/ Some other parts of JNI setup are skipped like the tear down of the JNI handle\n@@ -1607,11 +1347,0 @@\n-\/\/ They are roughly structured like this:\n-\/\/    if (GCLocker::needs_gc())\n-\/\/      SharedRuntime::block_for_jni_critical();\n-\/\/    tranistion to thread_in_native\n-\/\/    unpack arrray arguments and call native entry point\n-\/\/    check for safepoint in progress\n-\/\/    check if any thread suspend flags are set\n-\/\/      call into JVM and possible unlock the JNI critical\n-\/\/      if a GC was suppressed while in the critical native.\n-\/\/    transition back to thread_in_Java\n-\/\/    return to caller\n@@ -1929,5 +1658,0 @@\n-  if (is_critical_native && !Universe::heap()->supports_object_pinning()) {\n-    check_needs_gc_for_critical_native(masm, thread, stack_slots, total_c_args, total_in_args,\n-                                       oop_handle_offset, oop_maps, in_regs, in_sig_bt);\n-  }\n-\n@@ -1967,5 +1691,0 @@\n-  \/\/ Inbound arguments that need to be pinned for critical natives\n-  GrowableArray<int> pinned_args(total_in_args);\n-  \/\/ Current stack slot for storing register based array argument\n-  int pinned_slot = oop_handle_offset;\n-\n@@ -1984,20 +1703,0 @@\n-          if (Universe::heap()->supports_object_pinning()) {\n-            \/\/ gen_pin_object handles save and restore\n-            \/\/ of any clobbered registers\n-            gen_pin_object(masm, thread, in_arg);\n-            pinned_args.append(i);\n-\n-            \/\/ rax has pinned array\n-            VMRegPair result_reg(rax->as_VMReg());\n-            if (!in_arg.first()->is_stack()) {\n-              assert(pinned_slot <= stack_slots, \"overflow\");\n-              simple_move32(masm, result_reg, VMRegImpl::stack2reg(pinned_slot));\n-              pinned_slot += VMRegImpl::slots_per_word;\n-            } else {\n-              \/\/ Write back pinned value, it will be used to unpin this argument\n-              __ movptr(Address(rbp, reg2offset_in(in_arg.first())), result_reg.first()->as_Register());\n-            }\n-            \/\/ We have the array in register, use it\n-            in_arg = result_reg;\n-          }\n-\n@@ -2158,1 +1857,0 @@\n-\n@@ -2163,1 +1861,0 @@\n-  }\n@@ -2165,2 +1862,3 @@\n-  \/\/ Now set thread in native\n-  __ movl(Address(thread, JavaThread::thread_state_offset()), _thread_in_native);\n+    \/\/ Now set thread in native\n+    __ movl(Address(thread, JavaThread::thread_state_offset()), _thread_in_native);\n+  }\n@@ -2197,18 +1895,11 @@\n-  \/\/ unpin pinned arguments\n-  pinned_slot = oop_handle_offset;\n-  if (pinned_args.length() > 0) {\n-    \/\/ save return value that may be overwritten otherwise.\n-    save_native_result(masm, ret_type, stack_slots);\n-    for (int index = 0; index < pinned_args.length(); index ++) {\n-      int i = pinned_args.at(index);\n-      assert(pinned_slot <= stack_slots, \"overflow\");\n-      if (!in_regs[i].first()->is_stack()) {\n-        int offset = pinned_slot * VMRegImpl::stack_slot_size;\n-        __ movl(in_regs[i].first()->as_Register(), Address(rsp, offset));\n-        pinned_slot += VMRegImpl::slots_per_word;\n-      }\n-      \/\/ gen_pin_object handles save and restore\n-      \/\/ of any other clobbered registers\n-      gen_unpin_object(masm, thread, in_regs[i]);\n-    }\n-    restore_native_result(masm, ret_type, stack_slots);\n+  Label after_transition;\n+\n+  \/\/ If this is a critical native, check for a safepoint or suspend request after the call.\n+  \/\/ If a safepoint is needed, transition to native, then to native_trans to handle\n+  \/\/ safepoints like the native methods that are not critical natives.\n+  if (is_critical_native) {\n+    Label needs_safepoint;\n+    __ safepoint_poll(needs_safepoint, thread, false \/* at_return *\/, false \/* in_nmethod *\/);\n+    __ cmpl(Address(thread, JavaThread::suspend_flags_offset()), 0);\n+    __ jcc(Assembler::equal, after_transition);\n+    __ bind(needs_safepoint);\n@@ -2236,2 +1927,0 @@\n-  Label after_transition;\n-\n@@ -2257,2 +1946,1 @@\n-    if (!is_critical_native) {\n-      __ call(RuntimeAddress(CAST_FROM_FN_PTR(address,\n+    __ call(RuntimeAddress(CAST_FROM_FN_PTR(address,\n@@ -2260,4 +1948,0 @@\n-    } else {\n-      __ call(RuntimeAddress(CAST_FROM_FN_PTR(address,\n-                                              JavaThread::check_special_condition_for_native_trans_and_transition)));\n-    }\n@@ -2267,7 +1951,0 @@\n-\n-    if (is_critical_native) {\n-      \/\/ The call above performed the transition to thread_in_Java so\n-      \/\/ skip the transition logic below.\n-      __ jmpb(after_transition);\n-    }\n-\n@@ -2514,4 +2191,0 @@\n-  if (is_critical_native) {\n-    nm->set_lazy_critical_native(true);\n-  }\n-\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_32.cpp","additions":18,"deletions":345,"binary":false,"changes":363,"status":"modified"},{"patch":"@@ -1381,216 +1381,0 @@\n-\n-static void save_or_restore_arguments(MacroAssembler* masm,\n-                                      const int stack_slots,\n-                                      const int total_in_args,\n-                                      const int arg_save_area,\n-                                      OopMap* map,\n-                                      VMRegPair* in_regs,\n-                                      BasicType* in_sig_bt) {\n-  \/\/ if map is non-NULL then the code should store the values,\n-  \/\/ otherwise it should load them.\n-  int slot = arg_save_area;\n-  \/\/ Save down double word first\n-  for ( int i = 0; i < total_in_args; i++) {\n-    if (in_regs[i].first()->is_XMMRegister() && in_sig_bt[i] == T_DOUBLE) {\n-      int offset = slot * VMRegImpl::stack_slot_size;\n-      slot += VMRegImpl::slots_per_word;\n-      assert(slot <= stack_slots, \"overflow\");\n-      if (map != NULL) {\n-        __ movdbl(Address(rsp, offset), in_regs[i].first()->as_XMMRegister());\n-      } else {\n-        __ movdbl(in_regs[i].first()->as_XMMRegister(), Address(rsp, offset));\n-      }\n-    }\n-    if (in_regs[i].first()->is_Register() &&\n-        (in_sig_bt[i] == T_LONG || in_sig_bt[i] == T_ARRAY)) {\n-      int offset = slot * VMRegImpl::stack_slot_size;\n-      if (map != NULL) {\n-        __ movq(Address(rsp, offset), in_regs[i].first()->as_Register());\n-        if (in_sig_bt[i] == T_ARRAY) {\n-          map->set_oop(VMRegImpl::stack2reg(slot));;\n-        }\n-      } else {\n-        __ movq(in_regs[i].first()->as_Register(), Address(rsp, offset));\n-      }\n-      slot += VMRegImpl::slots_per_word;\n-    }\n-  }\n-  \/\/ Save or restore single word registers\n-  for ( int i = 0; i < total_in_args; i++) {\n-    if (in_regs[i].first()->is_Register()) {\n-      int offset = slot * VMRegImpl::stack_slot_size;\n-      slot++;\n-      assert(slot <= stack_slots, \"overflow\");\n-\n-      \/\/ Value is in an input register pass we must flush it to the stack\n-      const Register reg = in_regs[i].first()->as_Register();\n-      switch (in_sig_bt[i]) {\n-        case T_BOOLEAN:\n-        case T_CHAR:\n-        case T_BYTE:\n-        case T_SHORT:\n-        case T_INT:\n-          if (map != NULL) {\n-            __ movl(Address(rsp, offset), reg);\n-          } else {\n-            __ movl(reg, Address(rsp, offset));\n-          }\n-          break;\n-        case T_ARRAY:\n-        case T_LONG:\n-          \/\/ handled above\n-          break;\n-        case T_OBJECT:\n-        default: ShouldNotReachHere();\n-      }\n-    } else if (in_regs[i].first()->is_XMMRegister()) {\n-      if (in_sig_bt[i] == T_FLOAT) {\n-        int offset = slot * VMRegImpl::stack_slot_size;\n-        slot++;\n-        assert(slot <= stack_slots, \"overflow\");\n-        if (map != NULL) {\n-          __ movflt(Address(rsp, offset), in_regs[i].first()->as_XMMRegister());\n-        } else {\n-          __ movflt(in_regs[i].first()->as_XMMRegister(), Address(rsp, offset));\n-        }\n-      }\n-    } else if (in_regs[i].first()->is_stack()) {\n-      if (in_sig_bt[i] == T_ARRAY && map != NULL) {\n-        int offset_in_older_frame = in_regs[i].first()->reg2stack() + SharedRuntime::out_preserve_stack_slots();\n-        map->set_oop(VMRegImpl::stack2reg(offset_in_older_frame + stack_slots));\n-      }\n-    }\n-  }\n-}\n-\n-\/\/ Pin object, return pinned object or null in rax\n-static void gen_pin_object(MacroAssembler* masm,\n-                           VMRegPair reg) {\n-  __ block_comment(\"gen_pin_object {\");\n-\n-  \/\/ rax always contains oop, either incoming or\n-  \/\/ pinned.\n-  Register tmp_reg = rax;\n-\n-  Label is_null;\n-  VMRegPair tmp;\n-  VMRegPair in_reg = reg;\n-\n-  tmp.set_ptr(tmp_reg->as_VMReg());\n-  if (reg.first()->is_stack()) {\n-    \/\/ Load the arg up from the stack\n-    move_ptr(masm, reg, tmp);\n-    reg = tmp;\n-  } else {\n-    __ movptr(rax, reg.first()->as_Register());\n-  }\n-  __ testptr(reg.first()->as_Register(), reg.first()->as_Register());\n-  __ jccb(Assembler::equal, is_null);\n-\n-  if (reg.first()->as_Register() != c_rarg1) {\n-    __ movptr(c_rarg1, reg.first()->as_Register());\n-  }\n-\n-  __ call_VM_leaf(\n-    CAST_FROM_FN_PTR(address, SharedRuntime::pin_object),\n-    r15_thread, c_rarg1);\n-\n-  __ bind(is_null);\n-  __ block_comment(\"} gen_pin_object\");\n-}\n-\n-\/\/ Unpin object\n-static void gen_unpin_object(MacroAssembler* masm,\n-                             VMRegPair reg) {\n-  __ block_comment(\"gen_unpin_object {\");\n-  Label is_null;\n-\n-  if (reg.first()->is_stack()) {\n-    __ movptr(c_rarg1, Address(rbp, reg2offset_in(reg.first())));\n-  } else if (reg.first()->as_Register() != c_rarg1) {\n-    __ movptr(c_rarg1, reg.first()->as_Register());\n-  }\n-\n-  __ testptr(c_rarg1, c_rarg1);\n-  __ jccb(Assembler::equal, is_null);\n-\n-  __ call_VM_leaf(\n-    CAST_FROM_FN_PTR(address, SharedRuntime::unpin_object),\n-    r15_thread, c_rarg1);\n-\n-  __ bind(is_null);\n-  __ block_comment(\"} gen_unpin_object\");\n-}\n-\n-\/\/ Check GCLocker::needs_gc and enter the runtime if it's true.  This\n-\/\/ keeps a new JNI critical region from starting until a GC has been\n-\/\/ forced.  Save down any oops in registers and describe them in an\n-\/\/ OopMap.\n-static void check_needs_gc_for_critical_native(MacroAssembler* masm,\n-                                               int stack_slots,\n-                                               int total_c_args,\n-                                               int total_in_args,\n-                                               int arg_save_area,\n-                                               OopMapSet* oop_maps,\n-                                               VMRegPair* in_regs,\n-                                               BasicType* in_sig_bt) {\n-  __ block_comment(\"check GCLocker::needs_gc\");\n-  Label cont;\n-  __ cmp8(ExternalAddress((address)GCLocker::needs_gc_address()), false);\n-  __ jcc(Assembler::equal, cont);\n-\n-  \/\/ Save down any incoming oops and call into the runtime to halt for a GC\n-\n-  OopMap* map = new OopMap(stack_slots * 2, 0 \/* arg_slots*\/);\n-  save_or_restore_arguments(masm, stack_slots, total_in_args,\n-                            arg_save_area, map, in_regs, in_sig_bt);\n-\n-  address the_pc = __ pc();\n-  oop_maps->add_gc_map( __ offset(), map);\n-  __ set_last_Java_frame(rsp, noreg, the_pc);\n-\n-  __ block_comment(\"block_for_jni_critical\");\n-  __ movptr(c_rarg0, r15_thread);\n-  __ mov(r12, rsp); \/\/ remember sp\n-  __ subptr(rsp, frame::arg_reg_save_area_bytes); \/\/ windows\n-  __ andptr(rsp, -16); \/\/ align stack as required by ABI\n-  __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, SharedRuntime::block_for_jni_critical)));\n-  __ mov(rsp, r12); \/\/ restore sp\n-  __ reinit_heapbase();\n-\n-  __ reset_last_Java_frame(false);\n-\n-  save_or_restore_arguments(masm, stack_slots, total_in_args,\n-                            arg_save_area, NULL, in_regs, in_sig_bt);\n-  __ bind(cont);\n-#ifdef ASSERT\n-  if (StressCriticalJNINatives) {\n-    \/\/ Stress register saving\n-    OopMap* map = new OopMap(stack_slots * 2, 0 \/* arg_slots*\/);\n-    save_or_restore_arguments(masm, stack_slots, total_in_args,\n-                              arg_save_area, map, in_regs, in_sig_bt);\n-    \/\/ Destroy argument registers\n-    for (int i = 0; i < total_in_args - 1; i++) {\n-      if (in_regs[i].first()->is_Register()) {\n-        const Register reg = in_regs[i].first()->as_Register();\n-        __ xorptr(reg, reg);\n-      } else if (in_regs[i].first()->is_XMMRegister()) {\n-        __ xorpd(in_regs[i].first()->as_XMMRegister(), in_regs[i].first()->as_XMMRegister());\n-      } else if (in_regs[i].first()->is_FloatRegister()) {\n-        ShouldNotReachHere();\n-      } else if (in_regs[i].first()->is_stack()) {\n-        \/\/ Nothing to do\n-      } else {\n-        ShouldNotReachHere();\n-      }\n-      if (in_sig_bt[i] == T_LONG || in_sig_bt[i] == T_DOUBLE) {\n-        i++;\n-      }\n-    }\n-\n-    save_or_restore_arguments(masm, stack_slots, total_in_args,\n-                              arg_save_area, NULL, in_regs, in_sig_bt);\n-  }\n-#endif\n-}\n-\n@@ -1901,4 +1685,3 @@\n-\/\/ passing them to the callee and perform checks before and after the\n-\/\/ native call to ensure that they GCLocker\n-\/\/ lock_critical\/unlock_critical semantics are followed.  Some other\n-\/\/ parts of JNI setup are skipped like the tear down of the JNI handle\n+\/\/ passing them to the callee. Critical native functions leave the state _in_Java,\n+\/\/ since they cannot stop for GC.\n+\/\/ Some other parts of JNI setup are skipped like the tear down of the JNI handle\n@@ -1908,12 +1691,0 @@\n-\/\/ They are roughly structured like this:\n-\/\/    if (GCLocker::needs_gc())\n-\/\/      SharedRuntime::block_for_jni_critical();\n-\/\/    tranistion to thread_in_native\n-\/\/    unpack arrray arguments and call native entry point\n-\/\/    check for safepoint in progress\n-\/\/    check if any thread suspend flags are set\n-\/\/      call into JVM and possible unlock the JNI critical\n-\/\/      if a GC was suppressed while in the critical native.\n-\/\/    transition back to thread_in_Java\n-\/\/    return to caller\n-\/\/\n@@ -2220,5 +1991,0 @@\n-  if (is_critical_native && !Universe::heap()->supports_object_pinning()) {\n-    check_needs_gc_for_critical_native(masm, stack_slots, total_c_args, total_in_args,\n-                                       oop_handle_offset, oop_maps, in_regs, in_sig_bt);\n-  }\n-\n@@ -2277,4 +2043,0 @@\n-  \/\/ Inbound arguments that need to be pinned for critical natives\n-  GrowableArray<int> pinned_args(total_in_args);\n-  \/\/ Current stack slot for storing register based array argument\n-  int pinned_slot = oop_handle_offset;\n@@ -2329,17 +2091,0 @@\n-          \/\/ pin before unpack\n-          if (Universe::heap()->supports_object_pinning()) {\n-            save_args(masm, total_c_args, 0, out_regs);\n-            gen_pin_object(masm, in_regs[i]);\n-            pinned_args.append(i);\n-            restore_args(masm, total_c_args, 0, out_regs);\n-\n-            \/\/ rax has pinned array\n-            VMRegPair result_reg;\n-            result_reg.set_ptr(rax->as_VMReg());\n-            move_ptr(masm, result_reg, in_regs[i]);\n-            if (!in_regs[i].first()->is_stack()) {\n-              assert(pinned_slot <= stack_slots, \"overflow\");\n-              move_ptr(masm, result_reg, VMRegImpl::stack2reg(pinned_slot));\n-              pinned_slot += VMRegImpl::slots_per_word;\n-            }\n-          }\n@@ -2524,1 +2269,0 @@\n-\n@@ -2527,1 +2271,0 @@\n-\n@@ -2531,1 +2274,0 @@\n-  }\n@@ -2533,2 +2275,3 @@\n-  \/\/ Now set thread in native\n-  __ movl(Address(r15_thread, JavaThread::thread_state_offset()), _thread_in_native);\n+    \/\/ Now set thread in native\n+    __ movl(Address(r15_thread, JavaThread::thread_state_offset()), _thread_in_native);\n+  }\n@@ -2560,16 +2303,11 @@\n-  \/\/ unpin pinned arguments\n-  pinned_slot = oop_handle_offset;\n-  if (pinned_args.length() > 0) {\n-    \/\/ save return value that may be overwritten otherwise.\n-    save_native_result(masm, ret_type, stack_slots);\n-    for (int index = 0; index < pinned_args.length(); index ++) {\n-      int i = pinned_args.at(index);\n-      assert(pinned_slot <= stack_slots, \"overflow\");\n-      if (!in_regs[i].first()->is_stack()) {\n-        int offset = pinned_slot * VMRegImpl::stack_slot_size;\n-        __ movq(in_regs[i].first()->as_Register(), Address(rsp, offset));\n-        pinned_slot += VMRegImpl::slots_per_word;\n-      }\n-      gen_unpin_object(masm, in_regs[i]);\n-    }\n-    restore_native_result(masm, ret_type, stack_slots);\n+  Label after_transition;\n+\n+  \/\/ If this is a critical native, check for a safepoint or suspend request after the call.\n+  \/\/ If a safepoint is needed, transition to native, then to native_trans to handle\n+  \/\/ safepoints like the native methods that are not critical natives.\n+  if (is_critical_native) {\n+    Label needs_safepoint;\n+    __ safepoint_poll(needs_safepoint, r15_thread, false \/* at_return *\/, false \/* in_nmethod *\/);\n+    __ cmpl(Address(r15_thread, JavaThread::suspend_flags_offset()), 0);\n+    __ jcc(Assembler::equal, after_transition);\n+    __ bind(needs_safepoint);\n@@ -2592,2 +2330,0 @@\n-  Label after_transition;\n-\n@@ -2617,5 +2353,1 @@\n-    if (!is_critical_native) {\n-      __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, JavaThread::check_special_condition_for_native_trans)));\n-    } else {\n-      __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, JavaThread::check_special_condition_for_native_trans_and_transition)));\n-    }\n+    __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, JavaThread::check_special_condition_for_native_trans)));\n@@ -2626,7 +2358,0 @@\n-\n-    if (is_critical_native) {\n-      \/\/ The call above performed the transition to thread_in_Java so\n-      \/\/ skip the transition logic below.\n-      __ jmpb(after_transition);\n-    }\n-\n@@ -2856,4 +2581,0 @@\n-  if (is_critical_native) {\n-    nm->set_lazy_critical_native(true);\n-  }\n-\n@@ -2861,1 +2582,0 @@\n-\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_64.cpp","additions":18,"deletions":298,"binary":false,"changes":316,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n@@ -81,1 +81,0 @@\n-  _lazy_critical_native       = 0;\n","filename":"src\/hotspot\/share\/code\/compiledMethod.cpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -160,1 +160,0 @@\n-  unsigned int _lazy_critical_native:1;      \/\/ Lazy JNI critical native\n@@ -199,3 +198,0 @@\n-  bool  is_lazy_critical_native() const           { return _lazy_critical_native; }\n-  void  set_lazy_critical_native(bool z)          { _lazy_critical_native = z; }\n-\n","filename":"src\/hotspot\/share\/code\/compiledMethod.hpp","additions":0,"deletions":4,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n@@ -152,2 +152,0 @@\n-\n-  static address needs_gc_address() { return (address) &_needs_gc; }\n","filename":"src\/hotspot\/share\/gc\/shared\/gcLocker.hpp","additions":1,"deletions":3,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -534,0 +534,1 @@\n+  { \"CriticalJNINatives\",                  JDK_Version::jdk(16), JDK_Version::jdk(17), JDK_Version::jdk(18) },\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -324,5 +324,2 @@\n-  product(bool, CriticalJNINatives, true,                                   \\\n-          \"Check for critical JNI entry points\")                            \\\n-                                                                            \\\n-  notproduct(bool, StressCriticalJNINatives, false,                         \\\n-          \"Exercise register saving code in critical natives\")              \\\n+  product(bool, CriticalJNINatives, false,                                  \\\n+          \"(Deprecated) Check for critical JNI entry points\")               \\\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":2,"deletions":5,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -676,33 +676,0 @@\n-\/\/ See if the thread is running inside a lazy critical native and\n-\/\/ update the thread critical count if so.  Also set a suspend flag to\n-\/\/ cause the native wrapper to return into the JVM to do the unlock\n-\/\/ once the native finishes.\n-static void check_for_lazy_critical_native(JavaThread *thread, JavaThreadState state) {\n-  if (state == _thread_in_native &&\n-      thread->has_last_Java_frame() &&\n-      thread->frame_anchor()->walkable()) {\n-    \/\/ This thread might be in a critical native nmethod so look at\n-    \/\/ the top of the stack and increment the critical count if it\n-    \/\/ is.\n-    frame wrapper_frame = thread->last_frame();\n-    CodeBlob* stub_cb = wrapper_frame.cb();\n-    if (stub_cb != NULL &&\n-        stub_cb->is_nmethod() &&\n-        stub_cb->as_nmethod_or_null()->is_lazy_critical_native()) {\n-      \/\/ A thread could potentially be in a critical native across\n-      \/\/ more than one safepoint, so only update the critical state on\n-      \/\/ the first one.  When it returns it will perform the unlock.\n-      if (!thread->do_critical_native_unlock()) {\n-#ifdef ASSERT\n-        if (!thread->in_critical()) {\n-          GCLocker::increment_debug_jni_lock_count();\n-        }\n-#endif\n-        thread->enter_critical();\n-        \/\/ Make sure the native wrapper calls back on return to\n-        \/\/ perform the needed critical unlock.\n-        thread->set_critical_native_unlock();\n-      }\n-    }\n-  }\n-}\n@@ -902,1 +869,0 @@\n-    check_for_lazy_critical_native(_thread, stable_state);\n","filename":"src\/hotspot\/share\/runtime\/safepoint.cpp","additions":0,"deletions":34,"binary":false,"changes":34,"status":"modified"},{"patch":"@@ -2941,30 +2941,0 @@\n-JRT_ENTRY_NO_ASYNC(void, SharedRuntime::block_for_jni_critical(JavaThread* thread))\n-  assert(thread == JavaThread::current(), \"must be\");\n-  \/\/ The code is about to enter a JNI lazy critical native method and\n-  \/\/ _needs_gc is true, so if this thread is already in a critical\n-  \/\/ section then just return, otherwise this thread should block\n-  \/\/ until needs_gc has been cleared.\n-  if (thread->in_critical()) {\n-    return;\n-  }\n-  \/\/ Lock and unlock a critical section to give the system a chance to block\n-  GCLocker::lock_critical(thread);\n-  GCLocker::unlock_critical(thread);\n-JRT_END\n-\n-JRT_LEAF(oopDesc*, SharedRuntime::pin_object(JavaThread* thread, oopDesc* obj))\n-  assert(Universe::heap()->supports_object_pinning(), \"Why we are here?\");\n-  assert(obj != NULL, \"Should not be null\");\n-  oop o(obj);\n-  o = Universe::heap()->pin_object(thread, o);\n-  assert(o != NULL, \"Should not be null\");\n-  return o;\n-JRT_END\n-\n-JRT_LEAF(void, SharedRuntime::unpin_object(JavaThread* thread, oopDesc* obj))\n-  assert(Universe::heap()->supports_object_pinning(), \"Why we are here?\");\n-  assert(obj != NULL, \"Should not be null\");\n-  oop o(obj);\n-  Universe::heap()->unpin_object(thread, o);\n-JRT_END\n-\n","filename":"src\/hotspot\/share\/runtime\/sharedRuntime.cpp","additions":0,"deletions":30,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -489,7 +489,0 @@\n-  \/\/ Block before entering a JNI critical method\n-  static void block_for_jni_critical(JavaThread* thread);\n-\n-  \/\/ Pin\/Unpin object\n-  static oopDesc* pin_object(JavaThread* thread, oopDesc* obj);\n-  static void unpin_object(JavaThread* thread, oopDesc* obj);\n-\n","filename":"src\/hotspot\/share\/runtime\/sharedRuntime.hpp","additions":0,"deletions":7,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -2670,20 +2670,0 @@\n-\/\/ This is a variant of the normal\n-\/\/ check_special_condition_for_native_trans with slightly different\n-\/\/ semantics for use by critical native wrappers.  It does all the\n-\/\/ normal checks but also performs the transition back into\n-\/\/ thread_in_Java state.  This is required so that critical natives\n-\/\/ can potentially block and perform a GC if they are the last thread\n-\/\/ exiting the GCLocker.\n-void JavaThread::check_special_condition_for_native_trans_and_transition(JavaThread *thread) {\n-  check_special_condition_for_native_trans(thread);\n-\n-  \/\/ Finish the transition\n-  thread->set_thread_state(_thread_in_Java);\n-\n-  if (thread->do_critical_native_unlock()) {\n-    ThreadInVMfromJavaNoAsyncException tiv(thread);\n-    GCLocker::unlock_critical(thread);\n-    thread->clear_critical_native_unlock();\n-  }\n-}\n-\n","filename":"src\/hotspot\/share\/runtime\/thread.cpp","additions":0,"deletions":20,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -304,1 +304,0 @@\n-    _critical_native_unlock = 0x00000002U, \/\/ Must call back to unlock JNI critical lock\n@@ -545,5 +544,0 @@\n-  bool do_critical_native_unlock() const { return (_suspend_flags & _critical_native_unlock) != 0; }\n-\n-  inline void set_critical_native_unlock();\n-  inline void clear_critical_native_unlock();\n-\n@@ -1383,5 +1377,0 @@\n-  \/\/ Same as check_special_condition_for_native_trans but finishes the\n-  \/\/ transition into thread_in_Java mode so that it can potentially\n-  \/\/ block.\n-  static void check_special_condition_for_native_trans_and_transition(JavaThread *thread);\n-\n","filename":"src\/hotspot\/share\/runtime\/thread.hpp","additions":0,"deletions":11,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -56,6 +56,0 @@\n-inline void Thread::set_critical_native_unlock() {\n-  set_suspend_flag(_critical_native_unlock);\n-}\n-inline void Thread::clear_critical_native_unlock() {\n-  clear_suspend_flag(_critical_native_unlock);\n-}\n","filename":"src\/hotspot\/share\/runtime\/thread.inline.hpp","additions":0,"deletions":6,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n@@ -53,0 +54,9 @@\n+\n+\/*\n+ * @test CriticalNativeStress\n+ * @bug 8199868 8233343\n+ * @library \/\n+ * @requires os.arch ==\"x86_64\" | os.arch == \"amd64\" | os.arch==\"x86\" | os.arch==\"i386\" | os.arch==\"ppc64\" | os.arch==\"ppc64le\" | os.arch==\"s390x\"\n+ * @summary test argument unpacking nmethod wrapper of critical native method\n+ * @run main\/othervm\/native -Xcomp -Xmx512M -XX:+CriticalJNINatives gc.CriticalNativeArgs\n+ *\/\n","filename":"test\/hotspot\/jtreg\/gc\/CriticalNativeArgs.java","additions":10,"deletions":0,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n@@ -58,0 +59,11 @@\n+\n+\/*\n+ * @test CriticalNativeStress\n+ * @key randomness\n+ * @bug 8199868 8233343\n+ * @library \/ \/test\/lib\n+ * @requires os.arch ==\"x86_64\" | os.arch == \"amd64\" | os.arch==\"x86\" | os.arch==\"i386\" | os.arch==\"ppc64\" | os.arch==\"ppc64le\" | os.arch==\"s390x\"\n+ * @summary test argument unpacking nmethod wrapper of critical native method\n+ * @run main\/othervm\/native -Xcomp -Xmx512M -XX:+CriticalJNINatives gc.stress.CriticalNativeStress\n+ *\/\n+\n","filename":"test\/hotspot\/jtreg\/gc\/stress\/CriticalNativeStress.java","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"}]}