{"files":[{"patch":"@@ -15140,0 +15140,24 @@\n+instruct round_double_reg(iRegLNoSp dst, vRegD src, vRegD ftmp, rFlagsReg cr)\n+%{\n+  match(Set dst (RoundD src));\n+  effect(TEMP_DEF dst, TEMP ftmp, KILL cr);\n+  format %{ \"java_round_double $dst,$src\"%}\n+  ins_encode %{\n+    __ java_round_double($dst$$Register, as_FloatRegister($src$$reg),\n+                         as_FloatRegister($ftmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct round_float_reg(iRegINoSp dst, vRegF src, vRegF ftmp, rFlagsReg cr)\n+%{\n+  match(Set dst (RoundF src));\n+  effect(TEMP_DEF dst, TEMP ftmp, KILL cr);\n+  format %{ \"java_round_float $dst,$src\"%}\n+  ins_encode %{\n+    __ java_round_float($dst$$Register, as_FloatRegister($src$$reg),\n+                        as_FloatRegister($ftmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":24,"deletions":0,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -573,0 +573,46 @@\n+\n+instruct vroundvecD2Fto2I(vecD dst, vecD src, vecD tmp1, vecD tmp2, vecD tmp3)\n+%{\n+  predicate(UseSVE == 0 &&\n+            n->as_Vector()->length() == 2 && n->bottom_type()->is_vect()->element_basic_type() == T_INT);\n+  match(Set dst (RoundVF src));\n+  effect(TEMP_DEF dst, TEMP tmp1, TEMP tmp2, TEMP tmp3);\n+  format %{ \"vround  $dst, T2S, $src\\t# round vecD 2F to 2I vector\" %}\n+  ins_encode %{\n+    __ vector_round_neon(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg),\n+                         as_FloatRegister($tmp1$$reg), as_FloatRegister($tmp2$$reg),\n+                         as_FloatRegister($tmp3$$reg), __ T2S);\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct vroundvecX4Fto4I(vecX dst, vecX src, vecX tmp1, vecX tmp2, vecX tmp3)\n+%{\n+  predicate(UseSVE == 0 &&\n+            n->as_Vector()->length() == 4 && n->bottom_type()->is_vect()->element_basic_type() == T_INT);\n+  match(Set dst (RoundVF src));\n+  effect(TEMP_DEF dst, TEMP tmp1, TEMP tmp2, TEMP tmp3);\n+  format %{ \"vround  $dst, T4S, $src\\t# round vecX 4F to 4I vector\" %}\n+  ins_encode %{\n+    __ vector_round_neon(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg),\n+                         as_FloatRegister($tmp1$$reg), as_FloatRegister($tmp2$$reg),\n+                         as_FloatRegister($tmp3$$reg), __ T4S);\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct vroundvecX2Dto2L(vecX dst, vecX src, vecX tmp1, vecX tmp2, vecX tmp3)\n+%{\n+  predicate(UseSVE == 0 &&\n+            n->as_Vector()->length() == 2 && n->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n+  match(Set dst (RoundVD src));\n+  effect(TEMP_DEF dst, TEMP tmp1, TEMP tmp2, TEMP tmp3);\n+  format %{ \"vround  $dst, T2D, $src\\t# round vecX 2D to 2L vector\" %}\n+  ins_encode %{\n+    __ vector_round_neon(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg),\n+                         as_FloatRegister($tmp1$$reg), as_FloatRegister($tmp2$$reg),\n+                         as_FloatRegister($tmp3$$reg), __ T2D);\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64_neon.ad","additions":46,"deletions":0,"binary":false,"changes":46,"status":"modified"},{"patch":"@@ -352,0 +352,19 @@\n+define(`VECTOR_JAVA_FROUND', `\n+instruct vround$7$2to$5$3($7 dst, $7 src, $7 tmp1, $7 tmp2, $7 tmp3)\n+%{\n+  predicate(UseSVE == 0 &&\n+            n->as_Vector()->length() == $5 && n->bottom_type()->is_vect()->element_basic_type() == T_$6);\n+  match(Set dst (RoundV$1 src));\n+  effect(TEMP_DEF dst, TEMP tmp1, TEMP tmp2, TEMP tmp3);\n+  format %{ \"vround  $dst, $4, $src\\t# round $7 $2 to $5$3 vector\" %}\n+  ins_encode %{\n+    __ vector_round_neon(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg),\n+                         as_FloatRegister($tmp1$$reg), as_FloatRegister($tmp2$$reg),\n+                         as_FloatRegister($tmp3$$reg), __ $4);\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}')dnl           $1  $2  $3   $4 $5    $6    $7\n+VECTOR_JAVA_FROUND(F, 2F,  I, T2S, 2,  INT, vecD)\n+VECTOR_JAVA_FROUND(F, 4F,  I, T4S, 4,  INT, vecX)\n+VECTOR_JAVA_FROUND(D, 2D,  L, T2D, 2, LONG, vecX)\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64_neon_ad.m4","additions":19,"deletions":0,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -165,1 +165,0 @@\n-\n@@ -3855,0 +3854,48 @@\n+instruct vroundFtoI(vReg dst, vReg src, vReg tmp1, vReg tmp2, vReg tmp3, pRegGov ptmp)\n+%{\n+  predicate(UseSVE > 0);\n+  match(Set dst (RoundVF src));\n+  effect(TEMP_DEF dst, TEMP tmp1, TEMP tmp2, TEMP tmp3, TEMP ptmp);\n+  format %{ \"sve_vround  $dst, S, $src\\t# round F to I vector\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int vlen = Matcher::vector_length_in_bytes(this);\n+    if (vlen > 16) {\n+      __ vector_round_sve(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg),\n+                          as_FloatRegister($tmp1$$reg), as_FloatRegister($tmp2$$reg),\n+                          as_PRegister($ptmp$$reg), __ S);\n+    } else {\n+      __ vector_round_neon(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg),\n+                           as_FloatRegister($tmp1$$reg), as_FloatRegister($tmp2$$reg),\n+                           as_FloatRegister($tmp3$$reg),\n+                           __ esize2arrangement(type2aelembytes(bt),\n+                              \/*isQ*\/ vlen == 16));\n+    }\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct vroundDtoL(vReg dst, vReg src, vReg tmp1, vReg tmp2, vReg tmp3, pRegGov ptmp)\n+%{\n+  predicate(UseSVE > 0);\n+  match(Set dst (RoundVD src));\n+  effect(TEMP_DEF dst, TEMP tmp1, TEMP tmp2, TEMP tmp3, TEMP ptmp);\n+  format %{ \"sve_vround  $dst, D, $src\\t# round D to L vector\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int vlen = Matcher::vector_length_in_bytes(this);\n+    if (vlen > 16) {\n+      __ vector_round_sve(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg),\n+                          as_FloatRegister($tmp1$$reg), as_FloatRegister($tmp2$$reg),\n+                          as_PRegister($ptmp$$reg), __ D);\n+    } else {\n+      __ vector_round_neon(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg),\n+                           as_FloatRegister($tmp1$$reg), as_FloatRegister($tmp2$$reg),\n+                           as_FloatRegister($tmp3$$reg),\n+                           __ esize2arrangement(type2aelembytes(bt),\n+                              \/*isQ*\/ vlen == 16));\n+    }\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64_sve.ad","additions":48,"deletions":1,"binary":false,"changes":49,"status":"modified"},{"patch":"@@ -160,1 +160,0 @@\n-\n@@ -2126,0 +2125,26 @@\n+define(`VECTOR_JAVA_FROUND', `\n+instruct vround$1to$3($7 dst, $7 src, $7 tmp1, $7 tmp2, $7 tmp3, pRegGov ptmp)\n+%{\n+  predicate(UseSVE > 0);\n+  match(Set dst (RoundV$1 src));\n+  effect(TEMP_DEF dst, TEMP tmp1, TEMP tmp2, TEMP tmp3, TEMP ptmp);\n+  format %{ \"sve_vround  $dst, $4, $src\\t# round $1 to $3 vector\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int vlen = Matcher::vector_length_in_bytes(this);\n+    if (vlen > 16) {\n+      __ vector_round_sve(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg),\n+                          as_FloatRegister($tmp1$$reg), as_FloatRegister($tmp2$$reg),\n+                          as_PRegister($ptmp$$reg), __ $4);\n+    } else {\n+      __ vector_round_neon(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg),\n+                           as_FloatRegister($tmp1$$reg), as_FloatRegister($tmp2$$reg),\n+                           as_FloatRegister($tmp3$$reg),\n+                           __ esize2arrangement(type2aelembytes(bt),\n+                              \/*isQ*\/ vlen == 16));\n+    }\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}')dnl           $1  $2  $3 $4 $5    $6    $7\n+VECTOR_JAVA_FROUND(F, 8F,  I, S, 8,  INT, vReg)\n+VECTOR_JAVA_FROUND(D, 4D,  L, D, 4, LONG, vReg)\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64_sve_ad.m4","additions":26,"deletions":1,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -309,8 +309,0 @@\n-\n-  void fixed(unsigned value, unsigned mask) {\n-    assert_cond ((mask & bits) == 0);\n-#ifdef ASSERT\n-    bits |= mask;\n-#endif\n-    insn |= value;\n-  }\n@@ -701,1 +693,0 @@\n-#define fixed current_insn.fixed\n@@ -1088,1 +1079,1 @@\n-    \/\/ We can use ISH for a barrier because the ARM ARM says \"This\n+    \/\/ We can use ISH for a barrier because the Arm ARM says \"This\n@@ -2085,1 +2076,1 @@\n-  void float_int_convert(unsigned op31, unsigned type,\n+  void float_int_convert(unsigned sflag, unsigned ftype,\n@@ -2089,1 +2080,2 @@\n-    f(op31, 31, 29);\n+    f(sflag, 31);\n+    f(0b00, 30, 29);\n@@ -2091,1 +2083,1 @@\n-    f(type, 23, 22), f(1, 21), f(rmode, 20, 19);\n+    f(ftype, 23, 22), f(1, 21), f(rmode, 20, 19);\n@@ -2096,3 +2088,3 @@\n-#define INSN(NAME, op31, type, rmode, opcode)                           \\\n-  void NAME(Register Rd, FloatRegister Vn) {                            \\\n-    float_int_convert(op31, type, rmode, opcode, Rd, as_Register(Vn));  \\\n+#define INSN(NAME, sflag, ftype, rmode, opcode)                          \\\n+  void NAME(Register Rd, FloatRegister Vn) {                             \\\n+    float_int_convert(sflag, ftype, rmode, opcode, Rd, as_Register(Vn)); \\\n@@ -2101,4 +2093,8 @@\n-  INSN(fcvtzsw, 0b000, 0b00, 0b11, 0b000);\n-  INSN(fcvtzs,  0b100, 0b00, 0b11, 0b000);\n-  INSN(fcvtzdw, 0b000, 0b01, 0b11, 0b000);\n-  INSN(fcvtzd,  0b100, 0b01, 0b11, 0b000);\n+  INSN(fcvtzsw, 0b0, 0b00, 0b11, 0b000);\n+  INSN(fcvtzs,  0b1, 0b00, 0b11, 0b000);\n+  INSN(fcvtzdw, 0b0, 0b01, 0b11, 0b000);\n+  INSN(fcvtzd,  0b1, 0b01, 0b11, 0b000);\n+\n+  \/\/ RoundToNearestTiesAway\n+  INSN(fcvtassw, 0b0, 0b00, 0b00, 0b100);  \/\/ float -> signed word\n+  INSN(fcvtasd,  0b1, 0b01, 0b00, 0b100);  \/\/ double -> signed xword\n@@ -2106,2 +2102,3 @@\n-  INSN(fmovs, 0b000, 0b00, 0b00, 0b110);\n-  INSN(fmovd, 0b100, 0b01, 0b00, 0b110);\n+  \/\/ RoundTowardsNegative\n+  INSN(fcvtmssw, 0b0, 0b00, 0b10, 0b000);  \/\/ float -> signed word\n+  INSN(fcvtmsd,  0b1, 0b01, 0b10, 0b000);  \/\/ double -> signed xword\n@@ -2109,1 +2106,4 @@\n-  INSN(fmovhid, 0b100, 0b10, 0b01, 0b110);\n+  INSN(fmovs, 0b0, 0b00, 0b00, 0b110);\n+  INSN(fmovd, 0b1, 0b01, 0b00, 0b110);\n+\n+  INSN(fmovhid, 0b1, 0b10, 0b01, 0b110);\n@@ -2113,1 +2113,1 @@\n-#define INSN(NAME, op31, type, rmode, opcode)                           \\\n+#define INSN(NAME, sflag, type, rmode, opcode)                           \\\n@@ -2115,1 +2115,1 @@\n-    float_int_convert(op31, type, rmode, opcode, as_Register(Vd), Rn);  \\\n+    float_int_convert(sflag, type, rmode, opcode, as_Register(Vd), Rn);  \\\n@@ -2118,2 +2118,2 @@\n-  INSN(fmovs, 0b000, 0b00, 0b00, 0b111);\n-  INSN(fmovd, 0b100, 0b01, 0b00, 0b111);\n+  INSN(fmovs, 0b0, 0b00, 0b00, 0b111);\n+  INSN(fmovd, 0b1, 0b01, 0b00, 0b111);\n@@ -2121,4 +2121,4 @@\n-  INSN(scvtfws, 0b000, 0b00, 0b00, 0b010);\n-  INSN(scvtfs,  0b100, 0b00, 0b00, 0b010);\n-  INSN(scvtfwd, 0b000, 0b01, 0b00, 0b010);\n-  INSN(scvtfd,  0b100, 0b01, 0b00, 0b010);\n+  INSN(scvtfws, 0b0, 0b00, 0b00, 0b010);\n+  INSN(scvtfs,  0b1, 0b00, 0b00, 0b010);\n+  INSN(scvtfwd, 0b0, 0b01, 0b00, 0b010);\n+  INSN(scvtfd,  0b1, 0b01, 0b00, 0b010);\n@@ -2513,0 +2513,1 @@\n+\/\/ Advanced SIMD modified immediate\n@@ -2540,1 +2541,16 @@\n-#define INSN(NAME, op1, op2, op3) \\\n+#define INSN(NAME, op, cmode)                                           \\\n+  void NAME(FloatRegister Vd, SIMD_Arrangement T, double imm) {         \\\n+    unsigned imm8 = pack(imm);                                          \\\n+    starti;                                                             \\\n+    f(0, 31), f((int)T & 1, 30), f(op, 29), f(0b0111100000, 28, 19);    \\\n+    f(imm8 >> 5, 18, 16), f(cmode, 15, 12), f(0x01, 11, 10), f(imm8 & 0b11111, 9, 5); \\\n+    rf(Vd, 0);                                                          \\\n+  }\n+\n+  INSN(fmovs, 0, 0b1111);\n+  INSN(fmovd, 1, 0b1111);\n+\n+#undef INSN\n+\n+\/\/ Advanced SIMD three same\n+#define INSN(NAME, op1, op2, op3)                                                       \\\n@@ -2987,0 +3003,1 @@\n+  INSN(fcvtas, 0, 0b00, 0b01, 0b11100);\n@@ -2988,0 +3005,1 @@\n+  INSN(fcvtms, 0, 0b00, 0b01, 0b11011);\n@@ -3157,0 +3175,1 @@\n+  INSN(sve_frinta,  0b01100101, 0b000100101); \/\/ floating-point round to integral value, nearest with ties to away\n@@ -3452,2 +3471,3 @@\n-  \/\/ SVE copy signed integer immediate to vector elements (predicated)\n-  void sve_cpy(FloatRegister Zd, SIMD_RegVariant T, PRegister Pg, int imm8, bool isMerge) {\n+private:\n+  void sve_cpy(FloatRegister Zd, SIMD_RegVariant T, PRegister Pg, int imm8,\n+               bool isMerge, bool isFloat) {\n@@ -3467,1 +3487,11 @@\n-    prf(Pg, 16), f(0b0, 15), f(m, 14), f(sh, 13), sf(imm8, 12, 5), rf(Zd, 0);\n+    prf(Pg, 16), f(isFloat ? 1 : 0, 15), f(m, 14), f(sh, 13), sf(imm8, 12, 5), rf(Zd, 0);\n+  }\n+\n+public:\n+  \/\/ SVE copy signed integer immediate to vector elements (predicated)\n+  void sve_cpy(FloatRegister Zd, SIMD_RegVariant T, PRegister Pg, int imm8, bool isMerge) {\n+    sve_cpy(Zd, T, Pg, imm8, isMerge, \/*isFloat*\/false);\n+  }\n+  \/\/ SVE copy floating-point immediate to vector elements (predicated)\n+  void sve_cpy(FloatRegister Zd, SIMD_RegVariant T, PRegister Pg, double d) {\n+    sve_cpy(Zd, T, Pg, checked_cast<int8_t>(pack(d)), \/*isMerge*\/true, \/*isFloat*\/true);\n@@ -3531,0 +3561,23 @@\n+\/\/ SVE Floating-point compare vector with zero\n+void sve_fcm(Condition cond, PRegister Pd, SIMD_RegVariant T,\n+             PRegister Pg, FloatRegister Zn, double d) {\n+  starti;\n+  assert(T != Q, \"invalid size\");\n+  guarantee(d == 0.0, \"invalid immediate\");\n+  int cond_op;\n+  switch(cond) {\n+    case EQ: cond_op = 0b100; break;\n+    case GT: cond_op = 0b001; break;\n+    case GE: cond_op = 0b000; break;\n+    case LT: cond_op = 0b010; break;\n+    case LE: cond_op = 0b011; break;\n+    case NE: cond_op = 0b110; break;\n+    default:\n+      ShouldNotReachHere();\n+  }\n+  f(0b01100101, 31, 24), f(T, 23, 22), f(0b0100, 21, 18),\n+  f((cond_op >> 1) & 0x3, 17, 16), f(0b001, 15, 13),\n+  pgrf(Pg, 10), rf(Zn, 5);\n+  f(cond_op & 0x1, 4), prf(Pd, 0);\n+}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/assembler_aarch64.hpp","additions":88,"deletions":35,"binary":false,"changes":123,"status":"modified"},{"patch":"@@ -1270,0 +1270,71 @@\n+\n+\/\/ java.lang.Math::round intrinsics\n+\n+void C2_MacroAssembler::vector_round_neon(FloatRegister dst, FloatRegister src, FloatRegister tmp1,\n+                                       FloatRegister tmp2, FloatRegister tmp3, SIMD_Arrangement T) {\n+  assert_different_registers(tmp1, tmp2, tmp3, src, dst);\n+  switch (T) {\n+    case T2S:\n+    case T4S:\n+      fmovs(tmp1, T, 0.5f);\n+      mov(rscratch1, jint_cast(0x1.0p23f));\n+      break;\n+    case T2D:\n+      fmovd(tmp1, T, 0.5);\n+      mov(rscratch1, julong_cast(0x1.0p52));\n+      break;\n+    default:\n+      assert(T == T2S || T == T4S || T == T2D, \"invalid arrangement\");\n+  }\n+  fadd(tmp1, T, tmp1, src);\n+  fcvtms(tmp1, T, tmp1);\n+  \/\/ tmp1 = floor(src + 0.5, ties to even)\n+\n+  fcvtas(dst, T, src);\n+  \/\/ dst = round(src), ties to away\n+\n+  fneg(tmp3, T, src);\n+  dup(tmp2, T, rscratch1);\n+  cmhs(tmp3, T, tmp3, tmp2);\n+  \/\/ tmp3 is now a set of flags\n+\n+  bif(dst, T16B, tmp1, tmp3);\n+  \/\/ result in dst\n+}\n+\n+void C2_MacroAssembler::vector_round_sve(FloatRegister dst, FloatRegister src, FloatRegister tmp1,\n+                                      FloatRegister tmp2, PRegister ptmp, SIMD_RegVariant T) {\n+  assert_different_registers(tmp1, tmp2, src, dst);\n+\n+  switch (T) {\n+    case S:\n+      mov(rscratch1, jint_cast(0x1.0p23f));\n+      break;\n+    case D:\n+      mov(rscratch1, julong_cast(0x1.0p52));\n+      break;\n+    default:\n+      assert(T == S || T == D, \"invalid arrangement\");\n+  }\n+\n+  sve_frinta(dst, T, ptrue, src);\n+  \/\/ dst = round(src), ties to away\n+\n+  Label none;\n+\n+  sve_fneg(tmp1, T, ptrue, src);\n+  sve_dup(tmp2, T, rscratch1);\n+  sve_cmp(HS, ptmp, T, ptrue, tmp2, tmp1);\n+  br(EQ, none);\n+  {\n+    sve_cpy(tmp1, T, ptmp, 0.5);\n+    sve_fadd(tmp1, T, ptmp, src);\n+    sve_frintm(dst, T, ptmp, tmp1);\n+    \/\/ dst = floor(src + 0.5, ties to even)\n+  }\n+  bind(none);\n+\n+  sve_fcvtzs(dst, T, ptrue, dst, T);\n+  \/\/ result in dst\n+}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/c2_MacroAssembler_aarch64.cpp","additions":71,"deletions":0,"binary":false,"changes":71,"status":"modified"},{"patch":"@@ -106,0 +106,8 @@\n+  \/\/ java.lang.Math::round intrinsics\n+  void vector_round_neon(FloatRegister dst, FloatRegister src, FloatRegister tmp1,\n+                         FloatRegister tmp2, FloatRegister tmp3,\n+                         SIMD_Arrangement T);\n+  void vector_round_sve(FloatRegister dst, FloatRegister src, FloatRegister tmp1,\n+                        FloatRegister tmp2, PRegister ptmp,\n+                        SIMD_RegVariant T);\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/c2_MacroAssembler_aarch64.hpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -5184,0 +5184,50 @@\n+}\n+\n+\/\/ java.math.round(double a)\n+\/\/ Returns the closest long to the argument, with ties rounding to\n+\/\/ positive infinity.  This requires some fiddling for corner\n+\/\/ cases. We take care to avoid double rounding in e.g. (jlong)(a + 0.5).\n+void MacroAssembler::java_round_double(Register dst, FloatRegister src,\n+                                       FloatRegister ftmp) {\n+  Label DONE;\n+  BLOCK_COMMENT(\"java_round_float: { \");\n+  fmovd(rscratch1, src);\n+  \/\/ Use RoundToNearestTiesAway unless src small and -ve.\n+  fcvtasd(dst, src);\n+  \/\/ Test if src >= 0 || abs(src) >= 0x1.0p52\n+  eor(rscratch1, rscratch1, UCONST64(1) << 63); \/\/ flip sign bit\n+  mov(rscratch2, julong_cast(0x1.0p52));\n+  cmp(rscratch1, rscratch2);\n+  br(HS, DONE); {\n+    \/\/ src < 0 && abs(src) < 0x1.0p52\n+    \/\/ src may have a fractional part, so add 0.5\n+    fmovd(ftmp, 0.5);\n+    faddd(ftmp, src, ftmp);\n+    \/\/ Convert double to jlong, use RoundTowardsNegative\n+    fcvtmsd(dst, ftmp);\n+  }\n+  bind(DONE);\n+  BLOCK_COMMENT(\"} java_round_double\");\n+}\n+\n+void MacroAssembler::java_round_float(Register dst, FloatRegister src,\n+                                      FloatRegister ftmp) {\n+  Label DONE;\n+  BLOCK_COMMENT(\"java_round_float: { \");\n+  fmovs(rscratch1, src);\n+  \/\/ Use RoundToNearestTiesAway unless src small and -ve.\n+  fcvtassw(dst, src);\n+  \/\/ Test if src >= 0 || abs(src) >= 0x1.0p23\n+  eor(rscratch1, rscratch1, 0x80000000); \/\/ flip sign bit\n+  mov(rscratch2, jint_cast(0x1.0p23f));\n+  cmp(rscratch1, rscratch2);\n+  br(HS, DONE); {\n+    \/\/ src < 0 && |src| < 0x1.0p23\n+    \/\/ src may have a fractional part, so add 0.5\n+    fmovs(ftmp, 0.5f);\n+    fadds(ftmp, src, ftmp);\n+    \/\/ Convert float to jint, use RoundTowardsNegative\n+    fcvtmssw(dst, ftmp);\n+  }\n+  bind(DONE);\n+  BLOCK_COMMENT(\"} java_round_float\");\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":50,"deletions":0,"binary":false,"changes":50,"status":"modified"},{"patch":"@@ -880,0 +880,4 @@\n+  \/\/ java.lang.Math::round intrinsics\n+  void java_round_double(Register dst, FloatRegister src, FloatRegister ftmp);\n+  void java_round_float(Register dst, FloatRegister src, FloatRegister ftmp);\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -167,2 +167,10 @@\n-  static int vector_op_pre_select_sz_estimate(int vopc, BasicType ety, int vlen) {\n-    return 0;\n+  static int op_pre_select_sz_estimate(int vopc, BasicType ety, int vlen) {\n+    switch(vopc) {\n+      default: return 0;\n+      case Op_RoundF: \/\/ fall through\n+      case Op_RoundD: \/\/ fall through\n+      case Op_RoundVF: \/\/ fall through\n+      case Op_RoundVD: {\n+        return 15;\n+      }\n+    }\n@@ -171,1 +179,0 @@\n-\n","filename":"src\/hotspot\/cpu\/aarch64\/matcher_aarch64.hpp","additions":10,"deletions":3,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -159,1 +159,1 @@\n-  static int vector_op_pre_select_sz_estimate(int vopc, BasicType ety, int vlen) {\n+  static int op_pre_select_sz_estimate(int vopc, BasicType ety, int vlen) {\n","filename":"src\/hotspot\/cpu\/arm\/matcher_arm.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -168,1 +168,1 @@\n-  static int vector_op_pre_select_sz_estimate(int vopc, BasicType ety, int vlen) {\n+  static int op_pre_select_sz_estimate(int vopc, BasicType ety, int vlen) {\n","filename":"src\/hotspot\/cpu\/ppc\/matcher_ppc.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -157,1 +157,1 @@\n-  static int vector_op_pre_select_sz_estimate(int vopc, BasicType ety, int vlen) {\n+  static int op_pre_select_sz_estimate(int vopc, BasicType ety, int vlen) {\n","filename":"src\/hotspot\/cpu\/s390\/matcher_s390.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -187,1 +187,1 @@\n-  static int vector_op_pre_select_sz_estimate(int vopc, BasicType ety, int vlen) {\n+  static int op_pre_select_sz_estimate(int vopc, BasicType ety, int vlen) {\n@@ -192,0 +192,6 @@\n+      case Op_RoundF: \/\/ fall through\n+      case Op_RoundD: \/\/ fall through\n+      case Op_RoundVF: \/\/ fall through\n+      case Op_RoundVD: {\n+        return 30;\n+      }\n","filename":"src\/hotspot\/cpu\/x86\/matcher_x86.hpp","additions":7,"deletions":1,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -973,4 +973,4 @@\n-      case Op_RoundF: body_size += 30; break;\n-      case Op_RoundD: body_size += 30; break;\n-      case Op_RoundVF: body_size += 30; break;\n-      case Op_RoundVD: body_size += 30; break;\n+      case Op_RoundF:\n+      case Op_RoundD:\n+      case Op_RoundVF:\n+      case Op_RoundVD:\n@@ -979,2 +979,6 @@\n-        const TypeVect* vt = n->bottom_type()->is_vect();\n-        body_size += Matcher::vector_op_pre_select_sz_estimate(n->Opcode(), vt->element_basic_type(), vt->length());\n+        const TypeVect* vt = n->bottom_type()->isa_vect();\n+        if (vt != NULL) {\n+          body_size += Matcher::op_pre_select_sz_estimate(n->Opcode(), vt->element_basic_type(), vt->length());\n+        } else {\n+          body_size += Matcher::op_pre_select_sz_estimate(n->Opcode(), n->bottom_type()->basic_type(), 1);\n+        }\n","filename":"src\/hotspot\/share\/opto\/loopTransform.cpp","additions":10,"deletions":6,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -1447,0 +1447,2 @@\n+                          [\"fcvtassw\", \"fcvtas\", \"ws\"], [\"fcvtasd\", \"fcvtas\", \"xd\"],\n+                          [\"fcvtmssw\", \"fcvtms\", \"ws\"], [\"fcvtmsd\", \"fcvtms\", \"xd\"],\n@@ -1616,0 +1618,2 @@\n+                        [\"fmov\",   \"__ fmovs(v9, __ T2S, 0.5f);\",                        \"fmov\\tv9.2s, 0.5\"],\n+                        [\"fmov\",   \"__ fmovd(v14, __ T2D, 0.5f);\",                       \"fmov\\tv14.2d, 0.5\"],\n@@ -1617,1 +1621,3 @@\n-                        [\"fcvtzs\", \"__ fcvtzs(v0, __ T4S, v1);\",                         \"fcvtzs\\tv0.4s, v1.4s\"],\n+                        [\"fcvtzs\", \"__ fcvtzs(v0, __ T2S, v1);\",                         \"fcvtzs\\tv0.2s, v1.2s\"],\n+                        [\"fcvtas\", \"__ fcvtas(v2, __ T4S, v3);\",                         \"fcvtas\\tv2.4s, v3.4s\"],\n+                        [\"fcvtms\", \"__ fcvtms(v4, __ T2D, v5);\",                         \"fcvtms\\tv4.2d, v5.2d\"],\n@@ -1718,0 +1724,1 @@\n+                        [\"fcmge\",   \"__ sve_fcm(Assembler::GE, p1, __ D, p3, z6, 0.0);\",  \"fcmge\\tp1.d, p3\/z, z6.d, 0.0\"],\n","filename":"test\/hotspot\/gtest\/aarch64\/aarch64-asmtest.py","additions":8,"deletions":1,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -511,4 +511,8 @@\n-    __ fmovs(r7, v14);                                 \/\/       fmov    w7, s14\n-    __ fmovd(r7, v8);                                  \/\/       fmov    x7, d8\n-    __ fmovs(v20, r17);                                \/\/       fmov    s20, w17\n-    __ fmovd(v28, r30);                                \/\/       fmov    d28, x30\n+    __ fcvtassw(r7, v14);                              \/\/       fcvtas  w7, s14\n+    __ fcvtasd(r7, v8);                                \/\/       fcvtas  x7, d8\n+    __ fcvtmssw(r20, v17);                             \/\/       fcvtms  w20, s17\n+    __ fcvtmsd(r28, v30);                              \/\/       fcvtms  x28, d30\n+    __ fmovs(r16, v2);                                 \/\/       fmov    w16, s2\n+    __ fmovd(r9, v16);                                 \/\/       fmov    x9, d16\n+    __ fmovs(v20, r29);                                \/\/       fmov    s20, w29\n+    __ fmovd(v4, r1);                                  \/\/       fmov    d4, x1\n@@ -517,4 +521,4 @@\n-    __ fcmps(v16, v2);                                 \/\/       fcmp    s16, s2\n-    __ fcmpd(v9, v16);                                 \/\/       fcmp    d9, d16\n-    __ fcmps(v20, 0.0);                                \/\/       fcmp    s20, #0.0\n-    __ fcmpd(v29, 0.0);                                \/\/       fcmp    d29, #0.0\n+    __ fcmps(v26, v24);                                \/\/       fcmp    s26, s24\n+    __ fcmpd(v23, v14);                                \/\/       fcmp    d23, d14\n+    __ fcmps(v21, 0.0);                                \/\/       fcmp    s21, #0.0\n+    __ fcmpd(v12, 0.0);                                \/\/       fcmp    d12, #0.0\n@@ -523,5 +527,5 @@\n-    __ stpw(r1, r26, Address(r24, -208));              \/\/       stp     w1, w26, [x24, #-208]\n-    __ ldpw(r5, r11, Address(r12, 48));                \/\/       ldp     w5, w11, [x12, #48]\n-    __ ldpsw(r21, r15, Address(r27, 48));              \/\/       ldpsw   x21, x15, [x27, #48]\n-    __ stp(r5, r28, Address(r22, 32));                 \/\/       stp     x5, x28, [x22, #32]\n-    __ ldp(r27, r17, Address(r19, -32));               \/\/       ldp     x27, x17, [x19, #-32]\n+    __ stpw(r12, r24, Address(r24, -192));             \/\/       stp     w12, w24, [x24, #-192]\n+    __ ldpw(r22, r5, Address(r16, 128));               \/\/       ldp     w22, w5, [x16, #128]\n+    __ ldpsw(r20, r19, Address(r13, 112));             \/\/       ldpsw   x20, x19, [x13, #112]\n+    __ stp(r17, r6, Address(r13, 96));                 \/\/       stp     x17, x6, [x13, #96]\n+    __ ldp(r5, r1, Address(r17, -160));                \/\/       ldp     x5, x1, [x17, #-160]\n@@ -530,5 +534,5 @@\n-    __ stpw(r13, r7, Address(__ pre(r26, -176)));      \/\/       stp     w13, w7, [x26, #-176]!\n-    __ ldpw(r13, r21, Address(__ pre(r6, -48)));       \/\/       ldp     w13, w21, [x6, #-48]!\n-    __ ldpsw(r20, r30, Address(__ pre(r27, 16)));      \/\/       ldpsw   x20, x30, [x27, #16]!\n-    __ stp(r21, r5, Address(__ pre(r10, -128)));       \/\/       stp     x21, x5, [x10, #-128]!\n-    __ ldp(r14, r4, Address(__ pre(r23, -96)));        \/\/       ldp     x14, x4, [x23, #-96]!\n+    __ stpw(r13, r20, Address(__ pre(r22, -208)));     \/\/       stp     w13, w20, [x22, #-208]!\n+    __ ldpw(r30, r27, Address(__ pre(r10, 80)));       \/\/       ldp     w30, w27, [x10, #80]!\n+    __ ldpsw(r13, r20, Address(__ pre(r26, 16)));      \/\/       ldpsw   x13, x20, [x26, #16]!\n+    __ stp(r4, r23, Address(__ pre(r29, -80)));        \/\/       stp     x4, x23, [x29, #-80]!\n+    __ ldp(r22, r0, Address(__ pre(r6, -112)));        \/\/       ldp     x22, x0, [x6, #-112]!\n@@ -537,5 +541,5 @@\n-    __ stpw(r29, r12, Address(__ post(r16, 32)));      \/\/       stp     w29, w12, [x16], #32\n-    __ ldpw(r26, r17, Address(__ post(r27, 96)));      \/\/       ldp     w26, w17, [x27], #96\n-    __ ldpsw(r4, r20, Address(__ post(r14, -96)));     \/\/       ldpsw   x4, x20, [x14], #-96\n-    __ stp(r16, r2, Address(__ post(r14, -112)));      \/\/       stp     x16, x2, [x14], #-112\n-    __ ldp(r23, r24, Address(__ post(r7, -256)));      \/\/       ldp     x23, x24, [x7], #-256\n+    __ stpw(r17, r27, Address(__ post(r5, 80)));       \/\/       stp     w17, w27, [x5], #80\n+    __ ldpw(r14, r11, Address(__ post(r16, -256)));    \/\/       ldp     w14, w11, [x16], #-256\n+    __ ldpsw(r12, r23, Address(__ post(r9, -240)));    \/\/       ldpsw   x12, x23, [x9], #-240\n+    __ stp(r23, r7, Address(__ post(r0, 32)));         \/\/       stp     x23, x7, [x0], #32\n+    __ ldp(r17, r8, Address(__ post(r26, 80)));        \/\/       ldp     x17, x8, [x26], #80\n@@ -544,4 +548,4 @@\n-    __ stnpw(r0, r26, Address(r15, 128));              \/\/       stnp    w0, w26, [x15, #128]\n-    __ ldnpw(r26, r6, Address(r8, -208));              \/\/       ldnp    w26, w6, [x8, #-208]\n-    __ stnp(r15, r10, Address(r25, -112));             \/\/       stnp    x15, x10, [x25, #-112]\n-    __ ldnp(r16, r1, Address(r19, -160));              \/\/       ldnp    x16, x1, [x19, #-160]\n+    __ stnpw(r11, r15, Address(r10, -176));            \/\/       stnp    w11, w15, [x10, #-176]\n+    __ ldnpw(r19, r16, Address(r4, 64));               \/\/       ldnp    w19, w16, [x4, #64]\n+    __ stnp(r30, r14, Address(r9, -240));              \/\/       stnp    x30, x14, [x9, #-240]\n+    __ ldnp(r29, r23, Address(r20, 32));               \/\/       ldnp    x29, x23, [x20, #32]\n@@ -550,22 +554,22 @@\n-    __ ld1(v27, __ T8B, Address(r30));                 \/\/       ld1     {v27.8B}, [x30]\n-    __ ld1(v25, v26, __ T16B, Address(__ post(r3, 32))); \/\/     ld1     {v25.16B, v26.16B}, [x3], 32\n-    __ ld1(v30, v31, v0, __ T1D, Address(__ post(r16, r10))); \/\/        ld1     {v30.1D, v31.1D, v0.1D}, [x16], x10\n-    __ ld1(v16, v17, v18, v19, __ T8H, Address(__ post(r19, 64))); \/\/   ld1     {v16.8H, v17.8H, v18.8H, v19.8H}, [x19], 64\n-    __ ld1r(v23, __ T8B, Address(r24));                \/\/       ld1r    {v23.8B}, [x24]\n-    __ ld1r(v8, __ T4S, Address(__ post(r10, 4)));     \/\/       ld1r    {v8.4S}, [x10], 4\n-    __ ld1r(v9, __ T1D, Address(__ post(r20, r23)));   \/\/       ld1r    {v9.1D}, [x20], x23\n-    __ ld2(v2, v3, __ T2D, Address(r3));               \/\/       ld2     {v2.2D, v3.2D}, [x3]\n-    __ ld2(v8, v9, __ T4H, Address(__ post(r30, 16))); \/\/       ld2     {v8.4H, v9.4H}, [x30], 16\n-    __ ld2r(v4, v5, __ T16B, Address(r26));            \/\/       ld2r    {v4.16B, v5.16B}, [x26]\n-    __ ld2r(v3, v4, __ T2S, Address(__ post(r17, 8))); \/\/       ld2r    {v3.2S, v4.2S}, [x17], 8\n-    __ ld2r(v29, v30, __ T2D, Address(__ post(r11, r16))); \/\/   ld2r    {v29.2D, v30.2D}, [x11], x16\n-    __ ld3(v1, v2, v3, __ T4S, Address(__ post(r0, r23))); \/\/   ld3     {v1.4S, v2.4S, v3.4S}, [x0], x23\n-    __ ld3(v0, v1, v2, __ T2S, Address(r21));          \/\/       ld3     {v0.2S, v1.2S, v2.2S}, [x21]\n-    __ ld3r(v5, v6, v7, __ T8H, Address(r7));          \/\/       ld3r    {v5.8H, v6.8H, v7.8H}, [x7]\n-    __ ld3r(v1, v2, v3, __ T4S, Address(__ post(r7, 12))); \/\/   ld3r    {v1.4S, v2.4S, v3.4S}, [x7], 12\n-    __ ld3r(v2, v3, v4, __ T1D, Address(__ post(r5, r15))); \/\/  ld3r    {v2.1D, v3.1D, v4.1D}, [x5], x15\n-    __ ld4(v27, v28, v29, v30, __ T8H, Address(__ post(r29, 64))); \/\/   ld4     {v27.8H, v28.8H, v29.8H, v30.8H}, [x29], 64\n-    __ ld4(v24, v25, v26, v27, __ T8B, Address(__ post(r4, r7))); \/\/    ld4     {v24.8B, v25.8B, v26.8B, v27.8B}, [x4], x7\n-    __ ld4r(v15, v16, v17, v18, __ T8B, Address(r23)); \/\/       ld4r    {v15.8B, v16.8B, v17.8B, v18.8B}, [x23]\n-    __ ld4r(v14, v15, v16, v17, __ T4H, Address(__ post(r21, 8))); \/\/   ld4r    {v14.4H, v15.4H, v16.4H, v17.4H}, [x21], 8\n-    __ ld4r(v20, v21, v22, v23, __ T2S, Address(__ post(r9, r25))); \/\/  ld4r    {v20.2S, v21.2S, v22.2S, v23.2S}, [x9], x25\n+    __ ld1(v5, __ T8B, Address(r27));                  \/\/       ld1     {v5.8B}, [x27]\n+    __ ld1(v10, v11, __ T16B, Address(__ post(r25, 32))); \/\/    ld1     {v10.16B, v11.16B}, [x25], 32\n+    __ ld1(v15, v16, v17, __ T1D, Address(__ post(r30, r19))); \/\/       ld1     {v15.1D, v16.1D, v17.1D}, [x30], x19\n+    __ ld1(v17, v18, v19, v20, __ T8H, Address(__ post(r16, 64))); \/\/   ld1     {v17.8H, v18.8H, v19.8H, v20.8H}, [x16], 64\n+    __ ld1r(v30, __ T8B, Address(r23));                \/\/       ld1r    {v30.8B}, [x23]\n+    __ ld1r(v17, __ T4S, Address(__ post(r8, 4)));     \/\/       ld1r    {v17.4S}, [x8], 4\n+    __ ld1r(v12, __ T1D, Address(__ post(r9, r3)));    \/\/       ld1r    {v12.1D}, [x9], x3\n+    __ ld2(v19, v20, __ T2D, Address(r2));             \/\/       ld2     {v19.2D, v20.2D}, [x2]\n+    __ ld2(v21, v22, __ T4H, Address(__ post(r8, 16))); \/\/      ld2     {v21.4H, v22.4H}, [x8], 16\n+    __ ld2r(v13, v14, __ T16B, Address(r4));           \/\/       ld2r    {v13.16B, v14.16B}, [x4]\n+    __ ld2r(v28, v29, __ T2S, Address(__ post(r3, 8))); \/\/      ld2r    {v28.2S, v29.2S}, [x3], 8\n+    __ ld2r(v29, v30, __ T2D, Address(__ post(r29, r0))); \/\/    ld2r    {v29.2D, v30.2D}, [x29], x0\n+    __ ld3(v7, v8, v9, __ T4S, Address(__ post(r1, r21))); \/\/   ld3     {v7.4S, v8.4S, v9.4S}, [x1], x21\n+    __ ld3(v17, v18, v19, __ T2S, Address(r0));        \/\/       ld3     {v17.2S, v18.2S, v19.2S}, [x0]\n+    __ ld3r(v26, v27, v28, __ T8H, Address(r5));       \/\/       ld3r    {v26.8H, v27.8H, v28.8H}, [x5]\n+    __ ld3r(v25, v26, v27, __ T4S, Address(__ post(r1, 12))); \/\/        ld3r    {v25.4S, v26.4S, v27.4S}, [x1], 12\n+    __ ld3r(v22, v23, v24, __ T1D, Address(__ post(r2, r29))); \/\/       ld3r    {v22.1D, v23.1D, v24.1D}, [x2], x29\n+    __ ld4(v13, v14, v15, v16, __ T8H, Address(__ post(r27, 64))); \/\/   ld4     {v13.8H, v14.8H, v15.8H, v16.8H}, [x27], 64\n+    __ ld4(v29, v30, v31, v0, __ T8B, Address(__ post(r24, r23))); \/\/   ld4     {v29.8B, v30.8B, v31.8B, v0.8B}, [x24], x23\n+    __ ld4r(v13, v14, v15, v16, __ T8B, Address(r15)); \/\/       ld4r    {v13.8B, v14.8B, v15.8B, v16.8B}, [x15]\n+    __ ld4r(v15, v16, v17, v18, __ T4H, Address(__ post(r14, 8))); \/\/   ld4r    {v15.4H, v16.4H, v17.4H, v18.4H}, [x14], 8\n+    __ ld4r(v27, v28, v29, v30, __ T2S, Address(__ post(r20, r23))); \/\/ ld4r    {v27.2S, v28.2S, v29.2S, v30.2S}, [x20], x23\n@@ -574,26 +578,26 @@\n-    __ addv(v23, __ T8B, v24);                         \/\/       addv    b23, v24.8B\n-    __ addv(v26, __ T16B, v27);                        \/\/       addv    b26, v27.16B\n-    __ addv(v5, __ T4H, v6);                           \/\/       addv    h5, v6.4H\n-    __ addv(v6, __ T8H, v7);                           \/\/       addv    h6, v7.8H\n-    __ addv(v15, __ T4S, v16);                         \/\/       addv    s15, v16.4S\n-    __ smaxv(v15, __ T8B, v16);                        \/\/       smaxv   b15, v16.8B\n-    __ smaxv(v25, __ T16B, v26);                       \/\/       smaxv   b25, v26.16B\n-    __ smaxv(v16, __ T4H, v17);                        \/\/       smaxv   h16, v17.4H\n-    __ smaxv(v27, __ T8H, v28);                        \/\/       smaxv   h27, v28.8H\n-    __ smaxv(v24, __ T4S, v25);                        \/\/       smaxv   s24, v25.4S\n-    __ fmaxv(v15, __ T4S, v16);                        \/\/       fmaxv   s15, v16.4S\n-    __ sminv(v25, __ T8B, v26);                        \/\/       sminv   b25, v26.8B\n-    __ uminv(v14, __ T8B, v15);                        \/\/       uminv   b14, v15.8B\n-    __ sminv(v10, __ T16B, v11);                       \/\/       sminv   b10, v11.16B\n-    __ uminv(v13, __ T16B, v14);                       \/\/       uminv   b13, v14.16B\n-    __ sminv(v14, __ T4H, v15);                        \/\/       sminv   h14, v15.4H\n-    __ uminv(v20, __ T4H, v21);                        \/\/       uminv   h20, v21.4H\n-    __ sminv(v1, __ T8H, v2);                          \/\/       sminv   h1, v2.8H\n-    __ uminv(v22, __ T8H, v23);                        \/\/       uminv   h22, v23.8H\n-    __ sminv(v30, __ T4S, v31);                        \/\/       sminv   s30, v31.4S\n-    __ uminv(v14, __ T4S, v15);                        \/\/       uminv   s14, v15.4S\n-    __ fminv(v2, __ T4S, v3);                          \/\/       fminv   s2, v3.4S\n-    __ fmaxp(v6, v7, __ S);                            \/\/       fmaxp   s6, v7.2S\n-    __ fmaxp(v3, v4, __ D);                            \/\/       fmaxp   d3, v4.2D\n-    __ fminp(v7, v8, __ S);                            \/\/       fminp   s7, v8.2S\n-    __ fminp(v24, v25, __ D);                          \/\/       fminp   d24, v25.2D\n+    __ addv(v24, __ T8B, v25);                         \/\/       addv    b24, v25.8B\n+    __ addv(v15, __ T16B, v16);                        \/\/       addv    b15, v16.16B\n+    __ addv(v25, __ T4H, v26);                         \/\/       addv    h25, v26.4H\n+    __ addv(v14, __ T8H, v15);                         \/\/       addv    h14, v15.8H\n+    __ addv(v10, __ T4S, v11);                         \/\/       addv    s10, v11.4S\n+    __ smaxv(v13, __ T8B, v14);                        \/\/       smaxv   b13, v14.8B\n+    __ smaxv(v14, __ T16B, v15);                       \/\/       smaxv   b14, v15.16B\n+    __ smaxv(v20, __ T4H, v21);                        \/\/       smaxv   h20, v21.4H\n+    __ smaxv(v1, __ T8H, v2);                          \/\/       smaxv   h1, v2.8H\n+    __ smaxv(v22, __ T4S, v23);                        \/\/       smaxv   s22, v23.4S\n+    __ fmaxv(v30, __ T4S, v31);                        \/\/       fmaxv   s30, v31.4S\n+    __ sminv(v14, __ T8B, v15);                        \/\/       sminv   b14, v15.8B\n+    __ uminv(v2, __ T8B, v3);                          \/\/       uminv   b2, v3.8B\n+    __ sminv(v6, __ T16B, v7);                         \/\/       sminv   b6, v7.16B\n+    __ uminv(v3, __ T16B, v4);                         \/\/       uminv   b3, v4.16B\n+    __ sminv(v7, __ T4H, v8);                          \/\/       sminv   h7, v8.4H\n+    __ uminv(v24, __ T4H, v25);                        \/\/       uminv   h24, v25.4H\n+    __ sminv(v0, __ T8H, v1);                          \/\/       sminv   h0, v1.8H\n+    __ uminv(v27, __ T8H, v28);                        \/\/       uminv   h27, v28.8H\n+    __ sminv(v29, __ T4S, v30);                        \/\/       sminv   s29, v30.4S\n+    __ uminv(v5, __ T4S, v6);                          \/\/       uminv   s5, v6.4S\n+    __ fminv(v5, __ T4S, v6);                          \/\/       fminv   s5, v6.4S\n+    __ fmaxp(v29, v30, __ S);                          \/\/       fmaxp   s29, v30.2S\n+    __ fmaxp(v11, v12, __ D);                          \/\/       fmaxp   d11, v12.2D\n+    __ fminp(v25, v26, __ S);                          \/\/       fminp   s25, v26.2S\n+    __ fminp(v0, v1, __ D);                            \/\/       fminp   d0, v1.2D\n@@ -602,11 +606,11 @@\n-    __ absr(v0, __ T8B, v1);                           \/\/       abs     v0.8B, v1.8B\n-    __ absr(v27, __ T16B, v28);                        \/\/       abs     v27.16B, v28.16B\n-    __ absr(v29, __ T4H, v30);                         \/\/       abs     v29.4H, v30.4H\n-    __ absr(v5, __ T8H, v6);                           \/\/       abs     v5.8H, v6.8H\n-    __ absr(v5, __ T2S, v6);                           \/\/       abs     v5.2S, v6.2S\n-    __ absr(v29, __ T4S, v30);                         \/\/       abs     v29.4S, v30.4S\n-    __ absr(v11, __ T2D, v12);                         \/\/       abs     v11.2D, v12.2D\n-    __ fabs(v25, __ T2S, v26);                         \/\/       fabs    v25.2S, v26.2S\n-    __ fabs(v0, __ T4S, v1);                           \/\/       fabs    v0.4S, v1.4S\n-    __ fabs(v30, __ T2D, v31);                         \/\/       fabs    v30.2D, v31.2D\n-    __ fneg(v0, __ T2S, v1);                           \/\/       fneg    v0.2S, v1.2S\n+    __ absr(v30, __ T8B, v31);                         \/\/       abs     v30.8B, v31.8B\n+    __ absr(v0, __ T16B, v1);                          \/\/       abs     v0.16B, v1.16B\n+    __ absr(v17, __ T4H, v18);                         \/\/       abs     v17.4H, v18.4H\n+    __ absr(v28, __ T8H, v29);                         \/\/       abs     v28.8H, v29.8H\n+    __ absr(v25, __ T2S, v26);                         \/\/       abs     v25.2S, v26.2S\n+    __ absr(v9, __ T4S, v10);                          \/\/       abs     v9.4S, v10.4S\n+    __ absr(v25, __ T2D, v26);                         \/\/       abs     v25.2D, v26.2D\n+    __ fabs(v12, __ T2S, v13);                         \/\/       fabs    v12.2S, v13.2S\n+    __ fabs(v15, __ T4S, v16);                         \/\/       fabs    v15.4S, v16.4S\n+    __ fabs(v11, __ T2D, v12);                         \/\/       fabs    v11.2D, v12.2D\n+    __ fneg(v10, __ T2S, v11);                         \/\/       fneg    v10.2S, v11.2S\n@@ -614,6 +618,6 @@\n-    __ fneg(v28, __ T2D, v29);                         \/\/       fneg    v28.2D, v29.2D\n-    __ fsqrt(v25, __ T2S, v26);                        \/\/       fsqrt   v25.2S, v26.2S\n-    __ fsqrt(v9, __ T4S, v10);                         \/\/       fsqrt   v9.4S, v10.4S\n-    __ fsqrt(v25, __ T2D, v26);                        \/\/       fsqrt   v25.2D, v26.2D\n-    __ notr(v12, __ T8B, v13);                         \/\/       not     v12.8B, v13.8B\n-    __ notr(v15, __ T16B, v16);                        \/\/       not     v15.16B, v16.16B\n+    __ fneg(v24, __ T2D, v25);                         \/\/       fneg    v24.2D, v25.2D\n+    __ fsqrt(v21, __ T2S, v22);                        \/\/       fsqrt   v21.2S, v22.2S\n+    __ fsqrt(v23, __ T4S, v24);                        \/\/       fsqrt   v23.4S, v24.4S\n+    __ fsqrt(v0, __ T2D, v1);                          \/\/       fsqrt   v0.2D, v1.2D\n+    __ notr(v16, __ T8B, v17);                         \/\/       not     v16.8B, v17.8B\n+    __ notr(v10, __ T16B, v11);                        \/\/       not     v10.16B, v11.16B\n@@ -622,20 +626,20 @@\n-    __ andr(v11, __ T8B, v12, v13);                    \/\/       and     v11.8B, v12.8B, v13.8B\n-    __ andr(v10, __ T16B, v11, v12);                   \/\/       and     v10.16B, v11.16B, v12.16B\n-    __ orr(v17, __ T8B, v18, v19);                     \/\/       orr     v17.8B, v18.8B, v19.8B\n-    __ orr(v24, __ T16B, v25, v26);                    \/\/       orr     v24.16B, v25.16B, v26.16B\n-    __ eor(v21, __ T8B, v22, v23);                     \/\/       eor     v21.8B, v22.8B, v23.8B\n-    __ eor(v23, __ T16B, v24, v25);                    \/\/       eor     v23.16B, v24.16B, v25.16B\n-    __ addv(v0, __ T8B, v1, v2);                       \/\/       add     v0.8B, v1.8B, v2.8B\n-    __ addv(v16, __ T16B, v17, v18);                   \/\/       add     v16.16B, v17.16B, v18.16B\n-    __ addv(v10, __ T4H, v11, v12);                    \/\/       add     v10.4H, v11.4H, v12.4H\n-    __ addv(v6, __ T8H, v7, v8);                       \/\/       add     v6.8H, v7.8H, v8.8H\n-    __ addv(v28, __ T2S, v29, v30);                    \/\/       add     v28.2S, v29.2S, v30.2S\n-    __ addv(v6, __ T4S, v7, v8);                       \/\/       add     v6.4S, v7.4S, v8.4S\n-    __ addv(v5, __ T2D, v6, v7);                       \/\/       add     v5.2D, v6.2D, v7.2D\n-    __ fadd(v5, __ T2S, v6, v7);                       \/\/       fadd    v5.2S, v6.2S, v7.2S\n-    __ fadd(v20, __ T4S, v21, v22);                    \/\/       fadd    v20.4S, v21.4S, v22.4S\n-    __ fadd(v17, __ T2D, v18, v19);                    \/\/       fadd    v17.2D, v18.2D, v19.2D\n-    __ subv(v15, __ T8B, v16, v17);                    \/\/       sub     v15.8B, v16.8B, v17.8B\n-    __ subv(v17, __ T16B, v18, v19);                   \/\/       sub     v17.16B, v18.16B, v19.16B\n-    __ subv(v29, __ T4H, v30, v31);                    \/\/       sub     v29.4H, v30.4H, v31.4H\n-    __ subv(v26, __ T8H, v27, v28);                    \/\/       sub     v26.8H, v27.8H, v28.8H\n+    __ andr(v6, __ T8B, v7, v8);                       \/\/       and     v6.8B, v7.8B, v8.8B\n+    __ andr(v28, __ T16B, v29, v30);                   \/\/       and     v28.16B, v29.16B, v30.16B\n+    __ orr(v6, __ T8B, v7, v8);                        \/\/       orr     v6.8B, v7.8B, v8.8B\n+    __ orr(v5, __ T16B, v6, v7);                       \/\/       orr     v5.16B, v6.16B, v7.16B\n+    __ eor(v5, __ T8B, v6, v7);                        \/\/       eor     v5.8B, v6.8B, v7.8B\n+    __ eor(v20, __ T16B, v21, v22);                    \/\/       eor     v20.16B, v21.16B, v22.16B\n+    __ addv(v17, __ T8B, v18, v19);                    \/\/       add     v17.8B, v18.8B, v19.8B\n+    __ addv(v15, __ T16B, v16, v17);                   \/\/       add     v15.16B, v16.16B, v17.16B\n+    __ addv(v17, __ T4H, v18, v19);                    \/\/       add     v17.4H, v18.4H, v19.4H\n+    __ addv(v29, __ T8H, v30, v31);                    \/\/       add     v29.8H, v30.8H, v31.8H\n+    __ addv(v26, __ T2S, v27, v28);                    \/\/       add     v26.2S, v27.2S, v28.2S\n+    __ addv(v28, __ T4S, v29, v30);                    \/\/       add     v28.4S, v29.4S, v30.4S\n+    __ addv(v1, __ T2D, v2, v3);                       \/\/       add     v1.2D, v2.2D, v3.2D\n+    __ fadd(v27, __ T2S, v28, v29);                    \/\/       fadd    v27.2S, v28.2S, v29.2S\n+    __ fadd(v0, __ T4S, v1, v2);                       \/\/       fadd    v0.4S, v1.4S, v2.4S\n+    __ fadd(v20, __ T2D, v21, v22);                    \/\/       fadd    v20.2D, v21.2D, v22.2D\n+    __ subv(v28, __ T8B, v29, v30);                    \/\/       sub     v28.8B, v29.8B, v30.8B\n+    __ subv(v15, __ T16B, v16, v17);                   \/\/       sub     v15.16B, v16.16B, v17.16B\n+    __ subv(v12, __ T4H, v13, v14);                    \/\/       sub     v12.4H, v13.4H, v14.4H\n+    __ subv(v10, __ T8H, v11, v12);                    \/\/       sub     v10.8H, v11.8H, v12.8H\n@@ -643,16 +647,16 @@\n-    __ subv(v1, __ T4S, v2, v3);                       \/\/       sub     v1.4S, v2.4S, v3.4S\n-    __ subv(v27, __ T2D, v28, v29);                    \/\/       sub     v27.2D, v28.2D, v29.2D\n-    __ fsub(v0, __ T2S, v1, v2);                       \/\/       fsub    v0.2S, v1.2S, v2.2S\n-    __ fsub(v20, __ T4S, v21, v22);                    \/\/       fsub    v20.4S, v21.4S, v22.4S\n-    __ fsub(v28, __ T2D, v29, v30);                    \/\/       fsub    v28.2D, v29.2D, v30.2D\n-    __ mulv(v15, __ T8B, v16, v17);                    \/\/       mul     v15.8B, v16.8B, v17.8B\n-    __ mulv(v12, __ T16B, v13, v14);                   \/\/       mul     v12.16B, v13.16B, v14.16B\n-    __ mulv(v10, __ T4H, v11, v12);                    \/\/       mul     v10.4H, v11.4H, v12.4H\n-    __ mulv(v28, __ T8H, v29, v30);                    \/\/       mul     v28.8H, v29.8H, v30.8H\n-    __ mulv(v28, __ T2S, v29, v30);                    \/\/       mul     v28.2S, v29.2S, v30.2S\n-    __ mulv(v19, __ T4S, v20, v21);                    \/\/       mul     v19.4S, v20.4S, v21.4S\n-    __ fabd(v22, __ T2S, v23, v24);                    \/\/       fabd    v22.2S, v23.2S, v24.2S\n-    __ fabd(v10, __ T4S, v11, v12);                    \/\/       fabd    v10.4S, v11.4S, v12.4S\n-    __ fabd(v4, __ T2D, v5, v6);                       \/\/       fabd    v4.2D, v5.2D, v6.2D\n-    __ fmul(v30, __ T2S, v31, v0);                     \/\/       fmul    v30.2S, v31.2S, v0.2S\n-    __ fmul(v20, __ T4S, v21, v22);                    \/\/       fmul    v20.4S, v21.4S, v22.4S\n+    __ subv(v28, __ T4S, v29, v30);                    \/\/       sub     v28.4S, v29.4S, v30.4S\n+    __ subv(v19, __ T2D, v20, v21);                    \/\/       sub     v19.2D, v20.2D, v21.2D\n+    __ fsub(v22, __ T2S, v23, v24);                    \/\/       fsub    v22.2S, v23.2S, v24.2S\n+    __ fsub(v10, __ T4S, v11, v12);                    \/\/       fsub    v10.4S, v11.4S, v12.4S\n+    __ fsub(v4, __ T2D, v5, v6);                       \/\/       fsub    v4.2D, v5.2D, v6.2D\n+    __ mulv(v30, __ T8B, v31, v0);                     \/\/       mul     v30.8B, v31.8B, v0.8B\n+    __ mulv(v20, __ T16B, v21, v22);                   \/\/       mul     v20.16B, v21.16B, v22.16B\n+    __ mulv(v8, __ T4H, v9, v10);                      \/\/       mul     v8.4H, v9.4H, v10.4H\n+    __ mulv(v30, __ T8H, v31, v0);                     \/\/       mul     v30.8H, v31.8H, v0.8H\n+    __ mulv(v17, __ T2S, v18, v19);                    \/\/       mul     v17.2S, v18.2S, v19.2S\n+    __ mulv(v10, __ T4S, v11, v12);                    \/\/       mul     v10.4S, v11.4S, v12.4S\n+    __ fabd(v27, __ T2S, v28, v29);                    \/\/       fabd    v27.2S, v28.2S, v29.2S\n+    __ fabd(v2, __ T4S, v3, v4);                       \/\/       fabd    v2.4S, v3.4S, v4.4S\n+    __ fabd(v24, __ T2D, v25, v26);                    \/\/       fabd    v24.2D, v25.2D, v26.2D\n+    __ fmul(v4, __ T2S, v5, v6);                       \/\/       fmul    v4.2S, v5.2S, v6.2S\n+    __ fmul(v3, __ T4S, v4, v5);                       \/\/       fmul    v3.4S, v4.4S, v5.4S\n@@ -660,1 +664,1 @@\n-    __ mlav(v30, __ T4H, v31, v0);                     \/\/       mla     v30.4H, v31.4H, v0.4H\n+    __ mlav(v22, __ T4H, v23, v24);                    \/\/       mla     v22.4H, v23.4H, v24.4H\n@@ -662,54 +666,54 @@\n-    __ mlav(v10, __ T2S, v11, v12);                    \/\/       mla     v10.2S, v11.2S, v12.2S\n-    __ mlav(v27, __ T4S, v28, v29);                    \/\/       mla     v27.4S, v28.4S, v29.4S\n-    __ fmla(v2, __ T2S, v3, v4);                       \/\/       fmla    v2.2S, v3.2S, v4.2S\n-    __ fmla(v24, __ T4S, v25, v26);                    \/\/       fmla    v24.4S, v25.4S, v26.4S\n-    __ fmla(v4, __ T2D, v5, v6);                       \/\/       fmla    v4.2D, v5.2D, v6.2D\n-    __ mlsv(v3, __ T4H, v4, v5);                       \/\/       mls     v3.4H, v4.4H, v5.4H\n-    __ mlsv(v8, __ T8H, v9, v10);                      \/\/       mls     v8.8H, v9.8H, v10.8H\n-    __ mlsv(v22, __ T2S, v23, v24);                    \/\/       mls     v22.2S, v23.2S, v24.2S\n-    __ mlsv(v17, __ T4S, v18, v19);                    \/\/       mls     v17.4S, v18.4S, v19.4S\n-    __ fmls(v13, __ T2S, v14, v15);                    \/\/       fmls    v13.2S, v14.2S, v15.2S\n-    __ fmls(v4, __ T4S, v5, v6);                       \/\/       fmls    v4.4S, v5.4S, v6.4S\n-    __ fmls(v28, __ T2D, v29, v30);                    \/\/       fmls    v28.2D, v29.2D, v30.2D\n-    __ fdiv(v23, __ T2S, v24, v25);                    \/\/       fdiv    v23.2S, v24.2S, v25.2S\n-    __ fdiv(v21, __ T4S, v22, v23);                    \/\/       fdiv    v21.4S, v22.4S, v23.4S\n-    __ fdiv(v25, __ T2D, v26, v27);                    \/\/       fdiv    v25.2D, v26.2D, v27.2D\n-    __ maxv(v24, __ T8B, v25, v26);                    \/\/       smax    v24.8B, v25.8B, v26.8B\n-    __ maxv(v3, __ T16B, v4, v5);                      \/\/       smax    v3.16B, v4.16B, v5.16B\n-    __ maxv(v23, __ T4H, v24, v25);                    \/\/       smax    v23.4H, v24.4H, v25.4H\n-    __ maxv(v26, __ T8H, v27, v28);                    \/\/       smax    v26.8H, v27.8H, v28.8H\n-    __ maxv(v23, __ T2S, v24, v25);                    \/\/       smax    v23.2S, v24.2S, v25.2S\n-    __ maxv(v14, __ T4S, v15, v16);                    \/\/       smax    v14.4S, v15.4S, v16.4S\n-    __ smaxp(v21, __ T8B, v22, v23);                   \/\/       smaxp   v21.8B, v22.8B, v23.8B\n-    __ smaxp(v3, __ T16B, v4, v5);                     \/\/       smaxp   v3.16B, v4.16B, v5.16B\n-    __ smaxp(v23, __ T4H, v24, v25);                   \/\/       smaxp   v23.4H, v24.4H, v25.4H\n-    __ smaxp(v8, __ T8H, v9, v10);                     \/\/       smaxp   v8.8H, v9.8H, v10.8H\n-    __ smaxp(v24, __ T2S, v25, v26);                   \/\/       smaxp   v24.2S, v25.2S, v26.2S\n-    __ smaxp(v19, __ T4S, v20, v21);                   \/\/       smaxp   v19.4S, v20.4S, v21.4S\n-    __ fmax(v15, __ T2S, v16, v17);                    \/\/       fmax    v15.2S, v16.2S, v17.2S\n-    __ fmax(v16, __ T4S, v17, v18);                    \/\/       fmax    v16.4S, v17.4S, v18.4S\n-    __ fmax(v2, __ T2D, v3, v4);                       \/\/       fmax    v2.2D, v3.2D, v4.2D\n-    __ minv(v1, __ T8B, v2, v3);                       \/\/       smin    v1.8B, v2.8B, v3.8B\n-    __ minv(v0, __ T16B, v1, v2);                      \/\/       smin    v0.16B, v1.16B, v2.16B\n-    __ minv(v24, __ T4H, v25, v26);                    \/\/       smin    v24.4H, v25.4H, v26.4H\n-    __ minv(v4, __ T8H, v5, v6);                       \/\/       smin    v4.8H, v5.8H, v6.8H\n-    __ minv(v3, __ T2S, v4, v5);                       \/\/       smin    v3.2S, v4.2S, v5.2S\n-    __ minv(v11, __ T4S, v12, v13);                    \/\/       smin    v11.4S, v12.4S, v13.4S\n-    __ sminp(v30, __ T8B, v31, v0);                    \/\/       sminp   v30.8B, v31.8B, v0.8B\n-    __ sminp(v27, __ T16B, v28, v29);                  \/\/       sminp   v27.16B, v28.16B, v29.16B\n-    __ sminp(v9, __ T4H, v10, v11);                    \/\/       sminp   v9.4H, v10.4H, v11.4H\n-    __ sminp(v25, __ T8H, v26, v27);                   \/\/       sminp   v25.8H, v26.8H, v27.8H\n-    __ sminp(v2, __ T2S, v3, v4);                      \/\/       sminp   v2.2S, v3.2S, v4.2S\n-    __ sminp(v12, __ T4S, v13, v14);                   \/\/       sminp   v12.4S, v13.4S, v14.4S\n-    __ fmin(v17, __ T2S, v18, v19);                    \/\/       fmin    v17.2S, v18.2S, v19.2S\n-    __ fmin(v30, __ T4S, v31, v0);                     \/\/       fmin    v30.4S, v31.4S, v0.4S\n-    __ fmin(v1, __ T2D, v2, v3);                       \/\/       fmin    v1.2D, v2.2D, v3.2D\n-    __ cmeq(v12, __ T8B, v13, v14);                    \/\/       cmeq    v12.8B, v13.8B, v14.8B\n-    __ cmeq(v28, __ T16B, v29, v30);                   \/\/       cmeq    v28.16B, v29.16B, v30.16B\n-    __ cmeq(v0, __ T4H, v1, v2);                       \/\/       cmeq    v0.4H, v1.4H, v2.4H\n-    __ cmeq(v17, __ T8H, v18, v19);                    \/\/       cmeq    v17.8H, v18.8H, v19.8H\n-    __ cmeq(v12, __ T2S, v13, v14);                    \/\/       cmeq    v12.2S, v13.2S, v14.2S\n-    __ cmeq(v17, __ T4S, v18, v19);                    \/\/       cmeq    v17.4S, v18.4S, v19.4S\n-    __ cmeq(v21, __ T2D, v22, v23);                    \/\/       cmeq    v21.2D, v22.2D, v23.2D\n-    __ fcmeq(v12, __ T2S, v13, v14);                   \/\/       fcmeq   v12.2S, v13.2S, v14.2S\n-    __ fcmeq(v27, __ T4S, v28, v29);                   \/\/       fcmeq   v27.4S, v28.4S, v29.4S\n+    __ mlav(v13, __ T2S, v14, v15);                    \/\/       mla     v13.2S, v14.2S, v15.2S\n+    __ mlav(v4, __ T4S, v5, v6);                       \/\/       mla     v4.4S, v5.4S, v6.4S\n+    __ fmla(v28, __ T2S, v29, v30);                    \/\/       fmla    v28.2S, v29.2S, v30.2S\n+    __ fmla(v23, __ T4S, v24, v25);                    \/\/       fmla    v23.4S, v24.4S, v25.4S\n+    __ fmla(v21, __ T2D, v22, v23);                    \/\/       fmla    v21.2D, v22.2D, v23.2D\n+    __ mlsv(v25, __ T4H, v26, v27);                    \/\/       mls     v25.4H, v26.4H, v27.4H\n+    __ mlsv(v24, __ T8H, v25, v26);                    \/\/       mls     v24.8H, v25.8H, v26.8H\n+    __ mlsv(v3, __ T2S, v4, v5);                       \/\/       mls     v3.2S, v4.2S, v5.2S\n+    __ mlsv(v23, __ T4S, v24, v25);                    \/\/       mls     v23.4S, v24.4S, v25.4S\n+    __ fmls(v26, __ T2S, v27, v28);                    \/\/       fmls    v26.2S, v27.2S, v28.2S\n+    __ fmls(v23, __ T4S, v24, v25);                    \/\/       fmls    v23.4S, v24.4S, v25.4S\n+    __ fmls(v14, __ T2D, v15, v16);                    \/\/       fmls    v14.2D, v15.2D, v16.2D\n+    __ fdiv(v21, __ T2S, v22, v23);                    \/\/       fdiv    v21.2S, v22.2S, v23.2S\n+    __ fdiv(v3, __ T4S, v4, v5);                       \/\/       fdiv    v3.4S, v4.4S, v5.4S\n+    __ fdiv(v23, __ T2D, v24, v25);                    \/\/       fdiv    v23.2D, v24.2D, v25.2D\n+    __ maxv(v8, __ T8B, v9, v10);                      \/\/       smax    v8.8B, v9.8B, v10.8B\n+    __ maxv(v24, __ T16B, v25, v26);                   \/\/       smax    v24.16B, v25.16B, v26.16B\n+    __ maxv(v19, __ T4H, v20, v21);                    \/\/       smax    v19.4H, v20.4H, v21.4H\n+    __ maxv(v15, __ T8H, v16, v17);                    \/\/       smax    v15.8H, v16.8H, v17.8H\n+    __ maxv(v16, __ T2S, v17, v18);                    \/\/       smax    v16.2S, v17.2S, v18.2S\n+    __ maxv(v2, __ T4S, v3, v4);                       \/\/       smax    v2.4S, v3.4S, v4.4S\n+    __ smaxp(v1, __ T8B, v2, v3);                      \/\/       smaxp   v1.8B, v2.8B, v3.8B\n+    __ smaxp(v0, __ T16B, v1, v2);                     \/\/       smaxp   v0.16B, v1.16B, v2.16B\n+    __ smaxp(v24, __ T4H, v25, v26);                   \/\/       smaxp   v24.4H, v25.4H, v26.4H\n+    __ smaxp(v4, __ T8H, v5, v6);                      \/\/       smaxp   v4.8H, v5.8H, v6.8H\n+    __ smaxp(v3, __ T2S, v4, v5);                      \/\/       smaxp   v3.2S, v4.2S, v5.2S\n+    __ smaxp(v11, __ T4S, v12, v13);                   \/\/       smaxp   v11.4S, v12.4S, v13.4S\n+    __ fmax(v30, __ T2S, v31, v0);                     \/\/       fmax    v30.2S, v31.2S, v0.2S\n+    __ fmax(v27, __ T4S, v28, v29);                    \/\/       fmax    v27.4S, v28.4S, v29.4S\n+    __ fmax(v9, __ T2D, v10, v11);                     \/\/       fmax    v9.2D, v10.2D, v11.2D\n+    __ minv(v25, __ T8B, v26, v27);                    \/\/       smin    v25.8B, v26.8B, v27.8B\n+    __ minv(v2, __ T16B, v3, v4);                      \/\/       smin    v2.16B, v3.16B, v4.16B\n+    __ minv(v12, __ T4H, v13, v14);                    \/\/       smin    v12.4H, v13.4H, v14.4H\n+    __ minv(v17, __ T8H, v18, v19);                    \/\/       smin    v17.8H, v18.8H, v19.8H\n+    __ minv(v30, __ T2S, v31, v0);                     \/\/       smin    v30.2S, v31.2S, v0.2S\n+    __ minv(v1, __ T4S, v2, v3);                       \/\/       smin    v1.4S, v2.4S, v3.4S\n+    __ sminp(v12, __ T8B, v13, v14);                   \/\/       sminp   v12.8B, v13.8B, v14.8B\n+    __ sminp(v28, __ T16B, v29, v30);                  \/\/       sminp   v28.16B, v29.16B, v30.16B\n+    __ sminp(v0, __ T4H, v1, v2);                      \/\/       sminp   v0.4H, v1.4H, v2.4H\n+    __ sminp(v17, __ T8H, v18, v19);                   \/\/       sminp   v17.8H, v18.8H, v19.8H\n+    __ sminp(v12, __ T2S, v13, v14);                   \/\/       sminp   v12.2S, v13.2S, v14.2S\n+    __ sminp(v17, __ T4S, v18, v19);                   \/\/       sminp   v17.4S, v18.4S, v19.4S\n+    __ fmin(v21, __ T2S, v22, v23);                    \/\/       fmin    v21.2S, v22.2S, v23.2S\n+    __ fmin(v12, __ T4S, v13, v14);                    \/\/       fmin    v12.4S, v13.4S, v14.4S\n+    __ fmin(v27, __ T2D, v28, v29);                    \/\/       fmin    v27.2D, v28.2D, v29.2D\n+    __ cmeq(v29, __ T8B, v30, v31);                    \/\/       cmeq    v29.8B, v30.8B, v31.8B\n+    __ cmeq(v30, __ T16B, v31, v0);                    \/\/       cmeq    v30.16B, v31.16B, v0.16B\n+    __ cmeq(v1, __ T4H, v2, v3);                       \/\/       cmeq    v1.4H, v2.4H, v3.4H\n+    __ cmeq(v25, __ T8H, v26, v27);                    \/\/       cmeq    v25.8H, v26.8H, v27.8H\n+    __ cmeq(v27, __ T2S, v28, v29);                    \/\/       cmeq    v27.2S, v28.2S, v29.2S\n+    __ cmeq(v4, __ T4S, v5, v6);                       \/\/       cmeq    v4.4S, v5.4S, v6.4S\n+    __ cmeq(v29, __ T2D, v30, v31);                    \/\/       cmeq    v29.2D, v30.2D, v31.2D\n+    __ fcmeq(v3, __ T2S, v4, v5);                      \/\/       fcmeq   v3.2S, v4.2S, v5.2S\n+    __ fcmeq(v6, __ T4S, v7, v8);                      \/\/       fcmeq   v6.4S, v7.4S, v8.4S\n@@ -717,11 +721,11 @@\n-    __ cmgt(v30, __ T8B, v31, v0);                     \/\/       cmgt    v30.8B, v31.8B, v0.8B\n-    __ cmgt(v1, __ T16B, v2, v3);                      \/\/       cmgt    v1.16B, v2.16B, v3.16B\n-    __ cmgt(v25, __ T4H, v26, v27);                    \/\/       cmgt    v25.4H, v26.4H, v27.4H\n-    __ cmgt(v27, __ T8H, v28, v29);                    \/\/       cmgt    v27.8H, v28.8H, v29.8H\n-    __ cmgt(v4, __ T2S, v5, v6);                       \/\/       cmgt    v4.2S, v5.2S, v6.2S\n-    __ cmgt(v29, __ T4S, v30, v31);                    \/\/       cmgt    v29.4S, v30.4S, v31.4S\n-    __ cmgt(v3, __ T2D, v4, v5);                       \/\/       cmgt    v3.2D, v4.2D, v5.2D\n-    __ cmhi(v6, __ T8B, v7, v8);                       \/\/       cmhi    v6.8B, v7.8B, v8.8B\n-    __ cmhi(v29, __ T16B, v30, v31);                   \/\/       cmhi    v29.16B, v30.16B, v31.16B\n-    __ cmhi(v25, __ T4H, v26, v27);                    \/\/       cmhi    v25.4H, v26.4H, v27.4H\n-    __ cmhi(v17, __ T8H, v18, v19);                    \/\/       cmhi    v17.8H, v18.8H, v19.8H\n+    __ cmgt(v25, __ T8B, v26, v27);                    \/\/       cmgt    v25.8B, v26.8B, v27.8B\n+    __ cmgt(v17, __ T16B, v18, v19);                   \/\/       cmgt    v17.16B, v18.16B, v19.16B\n+    __ cmgt(v8, __ T4H, v9, v10);                      \/\/       cmgt    v8.4H, v9.4H, v10.4H\n+    __ cmgt(v7, __ T8H, v8, v9);                       \/\/       cmgt    v7.8H, v8.8H, v9.8H\n+    __ cmgt(v12, __ T2S, v13, v14);                    \/\/       cmgt    v12.2S, v13.2S, v14.2S\n+    __ cmgt(v0, __ T4S, v1, v2);                       \/\/       cmgt    v0.4S, v1.4S, v2.4S\n+    __ cmgt(v19, __ T2D, v20, v21);                    \/\/       cmgt    v19.2D, v20.2D, v21.2D\n+    __ cmhi(v1, __ T8B, v2, v3);                       \/\/       cmhi    v1.8B, v2.8B, v3.8B\n+    __ cmhi(v23, __ T16B, v24, v25);                   \/\/       cmhi    v23.16B, v24.16B, v25.16B\n+    __ cmhi(v2, __ T4H, v3, v4);                       \/\/       cmhi    v2.4H, v3.4H, v4.4H\n+    __ cmhi(v0, __ T8H, v1, v2);                       \/\/       cmhi    v0.8H, v1.8H, v2.8H\n@@ -729,22 +733,22 @@\n-    __ cmhi(v7, __ T4S, v8, v9);                       \/\/       cmhi    v7.4S, v8.4S, v9.4S\n-    __ cmhi(v12, __ T2D, v13, v14);                    \/\/       cmhi    v12.2D, v13.2D, v14.2D\n-    __ cmhs(v0, __ T8B, v1, v2);                       \/\/       cmhs    v0.8B, v1.8B, v2.8B\n-    __ cmhs(v19, __ T16B, v20, v21);                   \/\/       cmhs    v19.16B, v20.16B, v21.16B\n-    __ cmhs(v1, __ T4H, v2, v3);                       \/\/       cmhs    v1.4H, v2.4H, v3.4H\n-    __ cmhs(v23, __ T8H, v24, v25);                    \/\/       cmhs    v23.8H, v24.8H, v25.8H\n-    __ cmhs(v2, __ T2S, v3, v4);                       \/\/       cmhs    v2.2S, v3.2S, v4.2S\n-    __ cmhs(v0, __ T4S, v1, v2);                       \/\/       cmhs    v0.4S, v1.4S, v2.4S\n-    __ cmhs(v8, __ T2D, v9, v10);                      \/\/       cmhs    v8.2D, v9.2D, v10.2D\n-    __ fcmgt(v23, __ T2S, v24, v25);                   \/\/       fcmgt   v23.2S, v24.2S, v25.2S\n-    __ fcmgt(v25, __ T4S, v26, v27);                   \/\/       fcmgt   v25.4S, v26.4S, v27.4S\n-    __ fcmgt(v15, __ T2D, v16, v17);                   \/\/       fcmgt   v15.2D, v16.2D, v17.2D\n-    __ cmge(v29, __ T8B, v30, v31);                    \/\/       cmge    v29.8B, v30.8B, v31.8B\n-    __ cmge(v3, __ T16B, v4, v5);                      \/\/       cmge    v3.16B, v4.16B, v5.16B\n-    __ cmge(v10, __ T4H, v11, v12);                    \/\/       cmge    v10.4H, v11.4H, v12.4H\n-    __ cmge(v22, __ T8H, v23, v24);                    \/\/       cmge    v22.8H, v23.8H, v24.8H\n-    __ cmge(v10, __ T2S, v11, v12);                    \/\/       cmge    v10.2S, v11.2S, v12.2S\n-    __ cmge(v4, __ T4S, v5, v6);                       \/\/       cmge    v4.4S, v5.4S, v6.4S\n-    __ cmge(v17, __ T2D, v18, v19);                    \/\/       cmge    v17.2D, v18.2D, v19.2D\n-    __ fcmge(v1, __ T2S, v2, v3);                      \/\/       fcmge   v1.2S, v2.2S, v3.2S\n-    __ fcmge(v11, __ T4S, v12, v13);                   \/\/       fcmge   v11.4S, v12.4S, v13.4S\n-    __ fcmge(v7, __ T2D, v8, v9);                      \/\/       fcmge   v7.2D, v8.2D, v9.2D\n+    __ cmhi(v23, __ T4S, v24, v25);                    \/\/       cmhi    v23.4S, v24.4S, v25.4S\n+    __ cmhi(v25, __ T2D, v26, v27);                    \/\/       cmhi    v25.2D, v26.2D, v27.2D\n+    __ cmhs(v15, __ T8B, v16, v17);                    \/\/       cmhs    v15.8B, v16.8B, v17.8B\n+    __ cmhs(v29, __ T16B, v30, v31);                   \/\/       cmhs    v29.16B, v30.16B, v31.16B\n+    __ cmhs(v3, __ T4H, v4, v5);                       \/\/       cmhs    v3.4H, v4.4H, v5.4H\n+    __ cmhs(v10, __ T8H, v11, v12);                    \/\/       cmhs    v10.8H, v11.8H, v12.8H\n+    __ cmhs(v22, __ T2S, v23, v24);                    \/\/       cmhs    v22.2S, v23.2S, v24.2S\n+    __ cmhs(v10, __ T4S, v11, v12);                    \/\/       cmhs    v10.4S, v11.4S, v12.4S\n+    __ cmhs(v4, __ T2D, v5, v6);                       \/\/       cmhs    v4.2D, v5.2D, v6.2D\n+    __ fcmgt(v17, __ T2S, v18, v19);                   \/\/       fcmgt   v17.2S, v18.2S, v19.2S\n+    __ fcmgt(v1, __ T4S, v2, v3);                      \/\/       fcmgt   v1.4S, v2.4S, v3.4S\n+    __ fcmgt(v11, __ T2D, v12, v13);                   \/\/       fcmgt   v11.2D, v12.2D, v13.2D\n+    __ cmge(v7, __ T8B, v8, v9);                       \/\/       cmge    v7.8B, v8.8B, v9.8B\n+    __ cmge(v10, __ T16B, v11, v12);                   \/\/       cmge    v10.16B, v11.16B, v12.16B\n+    __ cmge(v15, __ T4H, v16, v17);                    \/\/       cmge    v15.4H, v16.4H, v17.4H\n+    __ cmge(v16, __ T8H, v17, v18);                    \/\/       cmge    v16.8H, v17.8H, v18.8H\n+    __ cmge(v2, __ T2S, v3, v4);                       \/\/       cmge    v2.2S, v3.2S, v4.2S\n+    __ cmge(v9, __ T4S, v10, v11);                     \/\/       cmge    v9.4S, v10.4S, v11.4S\n+    __ cmge(v11, __ T2D, v12, v13);                    \/\/       cmge    v11.2D, v12.2D, v13.2D\n+    __ fcmge(v12, __ T2S, v13, v14);                   \/\/       fcmge   v12.2S, v13.2S, v14.2S\n+    __ fcmge(v14, __ T4S, v15, v16);                   \/\/       fcmge   v14.4S, v15.4S, v16.4S\n+    __ fcmge(v13, __ T2D, v14, v15);                   \/\/       fcmge   v13.2D, v14.2D, v15.2D\n@@ -776,0 +780,2 @@\n+    __ fmovs(v9, __ T2S, 0.5f);                        \/\/       fmov    v9.2s, 0.5\n+    __ fmovd(v14, __ T2D, 0.5f);                       \/\/       fmov    v14.2d, 0.5\n@@ -777,1 +783,3 @@\n-    __ fcvtzs(v0, __ T4S, v1);                         \/\/       fcvtzs  v0.4s, v1.4s\n+    __ fcvtzs(v0, __ T2S, v1);                         \/\/       fcvtzs  v0.2s, v1.2s\n+    __ fcvtas(v2, __ T4S, v3);                         \/\/       fcvtas  v2.4s, v3.4s\n+    __ fcvtms(v4, __ T2D, v5);                         \/\/       fcvtms  v4.2d, v5.2d\n@@ -877,0 +885,1 @@\n+    __ sve_fcm(Assembler::GE, p1, __ D, p3, z6, 0.0);  \/\/       fcmge   p1.d, p3\/z, z6.d, 0.0\n@@ -978,9 +987,9 @@\n-    __ swp(Assembler::xword, r10, r15, r17);           \/\/       swp     x10, x15, [x17]\n-    __ ldadd(Assembler::xword, r2, r10, r12);          \/\/       ldadd   x2, x10, [x12]\n-    __ ldbic(Assembler::xword, r12, r15, r13);         \/\/       ldclr   x12, x15, [x13]\n-    __ ldeor(Assembler::xword, r2, r7, r20);           \/\/       ldeor   x2, x7, [x20]\n-    __ ldorr(Assembler::xword, r26, r16, r4);          \/\/       ldset   x26, x16, [x4]\n-    __ ldsmin(Assembler::xword, r2, r4, r12);          \/\/       ldsmin  x2, x4, [x12]\n-    __ ldsmax(Assembler::xword, r16, r21, r16);        \/\/       ldsmax  x16, x21, [x16]\n-    __ ldumin(Assembler::xword, r16, r11, r21);        \/\/       ldumin  x16, x11, [x21]\n-    __ ldumax(Assembler::xword, r23, r12, r26);        \/\/       ldumax  x23, x12, [x26]\n+    __ swp(Assembler::xword, r2, r7, r20);             \/\/       swp     x2, x7, [x20]\n+    __ ldadd(Assembler::xword, r26, r16, r4);          \/\/       ldadd   x26, x16, [x4]\n+    __ ldbic(Assembler::xword, r2, r4, r12);           \/\/       ldclr   x2, x4, [x12]\n+    __ ldeor(Assembler::xword, r16, r21, r16);         \/\/       ldeor   x16, x21, [x16]\n+    __ ldorr(Assembler::xword, r16, r11, r21);         \/\/       ldset   x16, x11, [x21]\n+    __ ldsmin(Assembler::xword, r23, r12, r26);        \/\/       ldsmin  x23, x12, [x26]\n+    __ ldsmax(Assembler::xword, r23, r28, r14);        \/\/       ldsmax  x23, x28, [x14]\n+    __ ldumin(Assembler::xword, r11, r24, r1);         \/\/       ldumin  x11, x24, [x1]\n+    __ ldumax(Assembler::xword, r12, zr, r10);         \/\/       ldumax  x12, xzr, [x10]\n@@ -989,9 +998,9 @@\n-    __ swpa(Assembler::xword, r23, r28, r14);          \/\/       swpa    x23, x28, [x14]\n-    __ ldadda(Assembler::xword, r11, r24, r1);         \/\/       ldadda  x11, x24, [x1]\n-    __ ldbica(Assembler::xword, r12, zr, r10);         \/\/       ldclra  x12, xzr, [x10]\n-    __ ldeora(Assembler::xword, r16, r7, r2);          \/\/       ldeora  x16, x7, [x2]\n-    __ ldorra(Assembler::xword, r3, r13, r19);         \/\/       ldseta  x3, x13, [x19]\n-    __ ldsmina(Assembler::xword, r17, r16, r3);        \/\/       ldsmina x17, x16, [x3]\n-    __ ldsmaxa(Assembler::xword, r1, r11, r30);        \/\/       ldsmaxa x1, x11, [x30]\n-    __ ldumina(Assembler::xword, r5, r8, r15);         \/\/       ldumina x5, x8, [x15]\n-    __ ldumaxa(Assembler::xword, r29, r30, r0);        \/\/       ldumaxa x29, x30, [x0]\n+    __ swpa(Assembler::xword, r16, r7, r2);            \/\/       swpa    x16, x7, [x2]\n+    __ ldadda(Assembler::xword, r3, r13, r19);         \/\/       ldadda  x3, x13, [x19]\n+    __ ldbica(Assembler::xword, r17, r16, r3);         \/\/       ldclra  x17, x16, [x3]\n+    __ ldeora(Assembler::xword, r1, r11, r30);         \/\/       ldeora  x1, x11, [x30]\n+    __ ldorra(Assembler::xword, r5, r8, r15);          \/\/       ldseta  x5, x8, [x15]\n+    __ ldsmina(Assembler::xword, r29, r30, r0);        \/\/       ldsmina x29, x30, [x0]\n+    __ ldsmaxa(Assembler::xword, r20, r7, r20);        \/\/       ldsmaxa x20, x7, [x20]\n+    __ ldumina(Assembler::xword, r23, r28, r21);       \/\/       ldumina x23, x28, [x21]\n+    __ ldumaxa(Assembler::xword, r27, r25, r5);        \/\/       ldumaxa x27, x25, [x5]\n@@ -1000,9 +1009,9 @@\n-    __ swpal(Assembler::xword, r20, r7, r20);          \/\/       swpal   x20, x7, [x20]\n-    __ ldaddal(Assembler::xword, r23, r28, r21);       \/\/       ldaddal x23, x28, [x21]\n-    __ ldbical(Assembler::xword, r27, r25, r5);        \/\/       ldclral x27, x25, [x5]\n-    __ ldeoral(Assembler::xword, r1, r23, r16);        \/\/       ldeoral x1, x23, [x16]\n-    __ ldorral(Assembler::xword, zr, r5, r12);         \/\/       ldsetal xzr, x5, [x12]\n-    __ ldsminal(Assembler::xword, r9, r28, r15);       \/\/       ldsminal        x9, x28, [x15]\n-    __ ldsmaxal(Assembler::xword, r29, r22, sp);       \/\/       ldsmaxal        x29, x22, [sp]\n-    __ lduminal(Assembler::xword, r19, zr, r5);        \/\/       lduminal        x19, xzr, [x5]\n-    __ ldumaxal(Assembler::xword, r14, r16, sp);       \/\/       ldumaxal        x14, x16, [sp]\n+    __ swpal(Assembler::xword, r1, r23, r16);          \/\/       swpal   x1, x23, [x16]\n+    __ ldaddal(Assembler::xword, zr, r5, r12);         \/\/       ldaddal xzr, x5, [x12]\n+    __ ldbical(Assembler::xword, r9, r28, r15);        \/\/       ldclral x9, x28, [x15]\n+    __ ldeoral(Assembler::xword, r29, r22, sp);        \/\/       ldeoral x29, x22, [sp]\n+    __ ldorral(Assembler::xword, r19, zr, r5);         \/\/       ldsetal x19, xzr, [x5]\n+    __ ldsminal(Assembler::xword, r14, r16, sp);       \/\/       ldsminal        x14, x16, [sp]\n+    __ ldsmaxal(Assembler::xword, r16, r27, r20);      \/\/       ldsmaxal        x16, x27, [x20]\n+    __ lduminal(Assembler::xword, r16, r12, r11);      \/\/       lduminal        x16, x12, [x11]\n+    __ ldumaxal(Assembler::xword, r9, r6, r30);        \/\/       ldumaxal        x9, x6, [x30]\n@@ -1011,9 +1020,9 @@\n-    __ swpl(Assembler::xword, r16, r27, r20);          \/\/       swpl    x16, x27, [x20]\n-    __ ldaddl(Assembler::xword, r16, r12, r11);        \/\/       ldaddl  x16, x12, [x11]\n-    __ ldbicl(Assembler::xword, r9, r6, r30);          \/\/       ldclrl  x9, x6, [x30]\n-    __ ldeorl(Assembler::xword, r17, r27, r28);        \/\/       ldeorl  x17, x27, [x28]\n-    __ ldorrl(Assembler::xword, r30, r7, r10);         \/\/       ldsetl  x30, x7, [x10]\n-    __ ldsminl(Assembler::xword, r20, r10, r4);        \/\/       ldsminl x20, x10, [x4]\n-    __ ldsmaxl(Assembler::xword, r24, r17, r17);       \/\/       ldsmaxl x24, x17, [x17]\n-    __ lduminl(Assembler::xword, r22, r3, r29);        \/\/       lduminl x22, x3, [x29]\n-    __ ldumaxl(Assembler::xword, r15, r22, r19);       \/\/       ldumaxl x15, x22, [x19]\n+    __ swpl(Assembler::xword, r17, r27, r28);          \/\/       swpl    x17, x27, [x28]\n+    __ ldaddl(Assembler::xword, r30, r7, r10);         \/\/       ldaddl  x30, x7, [x10]\n+    __ ldbicl(Assembler::xword, r20, r10, r4);         \/\/       ldclrl  x20, x10, [x4]\n+    __ ldeorl(Assembler::xword, r24, r17, r17);        \/\/       ldeorl  x24, x17, [x17]\n+    __ ldorrl(Assembler::xword, r22, r3, r29);         \/\/       ldsetl  x22, x3, [x29]\n+    __ ldsminl(Assembler::xword, r15, r22, r19);       \/\/       ldsminl x15, x22, [x19]\n+    __ ldsmaxl(Assembler::xword, r19, r22, r2);        \/\/       ldsmaxl x19, x22, [x2]\n+    __ lduminl(Assembler::xword, r15, r6, r12);        \/\/       lduminl x15, x6, [x12]\n+    __ ldumaxl(Assembler::xword, r16, r11, r13);       \/\/       ldumaxl x16, x11, [x13]\n@@ -1022,9 +1031,9 @@\n-    __ swp(Assembler::word, r19, r22, r2);             \/\/       swp     w19, w22, [x2]\n-    __ ldadd(Assembler::word, r15, r6, r12);           \/\/       ldadd   w15, w6, [x12]\n-    __ ldbic(Assembler::word, r16, r11, r13);          \/\/       ldclr   w16, w11, [x13]\n-    __ ldeor(Assembler::word, r23, r1, r30);           \/\/       ldeor   w23, w1, [x30]\n-    __ ldorr(Assembler::word, r19, r5, r17);           \/\/       ldset   w19, w5, [x17]\n-    __ ldsmin(Assembler::word, r2, r16, r22);          \/\/       ldsmin  w2, w16, [x22]\n-    __ ldsmax(Assembler::word, r13, r10, r21);         \/\/       ldsmax  w13, w10, [x21]\n-    __ ldumin(Assembler::word, r29, r27, r12);         \/\/       ldumin  w29, w27, [x12]\n-    __ ldumax(Assembler::word, r27, r3, r1);           \/\/       ldumax  w27, w3, [x1]\n+    __ swp(Assembler::word, r23, r1, r30);             \/\/       swp     w23, w1, [x30]\n+    __ ldadd(Assembler::word, r19, r5, r17);           \/\/       ldadd   w19, w5, [x17]\n+    __ ldbic(Assembler::word, r2, r16, r22);           \/\/       ldclr   w2, w16, [x22]\n+    __ ldeor(Assembler::word, r13, r10, r21);          \/\/       ldeor   w13, w10, [x21]\n+    __ ldorr(Assembler::word, r29, r27, r12);          \/\/       ldset   w29, w27, [x12]\n+    __ ldsmin(Assembler::word, r27, r3, r1);           \/\/       ldsmin  w27, w3, [x1]\n+    __ ldsmax(Assembler::word, zr, r24, r19);          \/\/       ldsmax  wzr, w24, [x19]\n+    __ ldumin(Assembler::word, r17, r9, r28);          \/\/       ldumin  w17, w9, [x28]\n+    __ ldumax(Assembler::word, r27, r15, r7);          \/\/       ldumax  w27, w15, [x7]\n@@ -1033,9 +1042,9 @@\n-    __ swpa(Assembler::word, zr, r24, r19);            \/\/       swpa    wzr, w24, [x19]\n-    __ ldadda(Assembler::word, r17, r9, r28);          \/\/       ldadda  w17, w9, [x28]\n-    __ ldbica(Assembler::word, r27, r15, r7);          \/\/       ldclra  w27, w15, [x7]\n-    __ ldeora(Assembler::word, r21, r23, sp);          \/\/       ldeora  w21, w23, [sp]\n-    __ ldorra(Assembler::word, r25, r2, sp);           \/\/       ldseta  w25, w2, [sp]\n-    __ ldsmina(Assembler::word, r27, r16, r10);        \/\/       ldsmina w27, w16, [x10]\n-    __ ldsmaxa(Assembler::word, r23, r19, r3);         \/\/       ldsmaxa w23, w19, [x3]\n-    __ ldumina(Assembler::word, r16, r0, r25);         \/\/       ldumina w16, w0, [x25]\n-    __ ldumaxa(Assembler::word, r26, r23, r2);         \/\/       ldumaxa w26, w23, [x2]\n+    __ swpa(Assembler::word, r21, r23, sp);            \/\/       swpa    w21, w23, [sp]\n+    __ ldadda(Assembler::word, r25, r2, sp);           \/\/       ldadda  w25, w2, [sp]\n+    __ ldbica(Assembler::word, r27, r16, r10);         \/\/       ldclra  w27, w16, [x10]\n+    __ ldeora(Assembler::word, r23, r19, r3);          \/\/       ldeora  w23, w19, [x3]\n+    __ ldorra(Assembler::word, r16, r0, r25);          \/\/       ldseta  w16, w0, [x25]\n+    __ ldsmina(Assembler::word, r26, r23, r2);         \/\/       ldsmina w26, w23, [x2]\n+    __ ldsmaxa(Assembler::word, r16, r12, r4);         \/\/       ldsmaxa w16, w12, [x4]\n+    __ ldumina(Assembler::word, r28, r30, r29);        \/\/       ldumina w28, w30, [x29]\n+    __ ldumaxa(Assembler::word, r16, r27, r6);         \/\/       ldumaxa w16, w27, [x6]\n@@ -1044,9 +1053,9 @@\n-    __ swpal(Assembler::word, r16, r12, r4);           \/\/       swpal   w16, w12, [x4]\n-    __ ldaddal(Assembler::word, r28, r30, r29);        \/\/       ldaddal w28, w30, [x29]\n-    __ ldbical(Assembler::word, r16, r27, r6);         \/\/       ldclral w16, w27, [x6]\n-    __ ldeoral(Assembler::word, r9, r29, r15);         \/\/       ldeoral w9, w29, [x15]\n-    __ ldorral(Assembler::word, r7, r4, r7);           \/\/       ldsetal w7, w4, [x7]\n-    __ ldsminal(Assembler::word, r15, r9, r23);        \/\/       ldsminal        w15, w9, [x23]\n-    __ ldsmaxal(Assembler::word, r8, r2, r28);         \/\/       ldsmaxal        w8, w2, [x28]\n-    __ lduminal(Assembler::word, r21, zr, r5);         \/\/       lduminal        w21, wzr, [x5]\n-    __ ldumaxal(Assembler::word, r27, r0, r17);        \/\/       ldumaxal        w27, w0, [x17]\n+    __ swpal(Assembler::word, r9, r29, r15);           \/\/       swpal   w9, w29, [x15]\n+    __ ldaddal(Assembler::word, r7, r4, r7);           \/\/       ldaddal w7, w4, [x7]\n+    __ ldbical(Assembler::word, r15, r9, r23);         \/\/       ldclral w15, w9, [x23]\n+    __ ldeoral(Assembler::word, r8, r2, r28);          \/\/       ldeoral w8, w2, [x28]\n+    __ ldorral(Assembler::word, r21, zr, r5);          \/\/       ldsetal w21, wzr, [x5]\n+    __ ldsminal(Assembler::word, r27, r0, r17);        \/\/       ldsminal        w27, w0, [x17]\n+    __ ldsmaxal(Assembler::word, r15, r4, r26);        \/\/       ldsmaxal        w15, w4, [x26]\n+    __ lduminal(Assembler::word, r8, r28, r22);        \/\/       lduminal        w8, w28, [x22]\n+    __ ldumaxal(Assembler::word, r27, r27, r25);       \/\/       ldumaxal        w27, w27, [x25]\n@@ -1055,9 +1064,9 @@\n-    __ swpl(Assembler::word, r15, r4, r26);            \/\/       swpl    w15, w4, [x26]\n-    __ ldaddl(Assembler::word, r8, r28, r22);          \/\/       ldaddl  w8, w28, [x22]\n-    __ ldbicl(Assembler::word, r27, r27, r25);         \/\/       ldclrl  w27, w27, [x25]\n-    __ ldeorl(Assembler::word, r23, r0, r4);           \/\/       ldeorl  w23, w0, [x4]\n-    __ ldorrl(Assembler::word, r6, r16, r0);           \/\/       ldsetl  w6, w16, [x0]\n-    __ ldsminl(Assembler::word, r4, r15, r1);          \/\/       ldsminl w4, w15, [x1]\n-    __ ldsmaxl(Assembler::word, r10, r7, r5);          \/\/       ldsmaxl w10, w7, [x5]\n-    __ lduminl(Assembler::word, r10, r28, r7);         \/\/       lduminl w10, w28, [x7]\n-    __ ldumaxl(Assembler::word, r20, r23, r21);        \/\/       ldumaxl w20, w23, [x21]\n+    __ swpl(Assembler::word, r23, r0, r4);             \/\/       swpl    w23, w0, [x4]\n+    __ ldaddl(Assembler::word, r6, r16, r0);           \/\/       ldaddl  w6, w16, [x0]\n+    __ ldbicl(Assembler::word, r4, r15, r1);           \/\/       ldclrl  w4, w15, [x1]\n+    __ ldeorl(Assembler::word, r10, r7, r5);           \/\/       ldeorl  w10, w7, [x5]\n+    __ ldorrl(Assembler::word, r10, r28, r7);          \/\/       ldsetl  w10, w28, [x7]\n+    __ ldsminl(Assembler::word, r20, r23, r21);        \/\/       ldsminl w20, w23, [x21]\n+    __ ldsmaxl(Assembler::word, r6, r11, r8);          \/\/       ldsmaxl w6, w11, [x8]\n+    __ lduminl(Assembler::word, r17, zr, r6);          \/\/       lduminl w17, wzr, [x6]\n+    __ ldumaxl(Assembler::word, r17, r2, r12);         \/\/       ldumaxl w17, w2, [x12]\n@@ -1066,4 +1075,4 @@\n-    __ bcax(v5, __ T16B, v10, v8, v16);                \/\/       bcax            v5.16B, v10.16B, v8.16B, v16.16B\n-    __ eor3(v30, __ T16B, v6, v17, v2);                \/\/       eor3            v30.16B, v6.16B, v17.16B, v2.16B\n-    __ rax1(v11, __ T2D, v29, v28);                    \/\/       rax1            v11.2D, v29.2D, v28.2D\n-    __ xar(v2, __ T2D, v26, v22, 58);                  \/\/       xar             v2.2D, v26.2D, v22.2D, #58\n+    __ bcax(v29, __ T16B, v28, v2, v26);               \/\/       bcax            v29.16B, v28.16B, v2.16B, v26.16B\n+    __ eor3(v22, __ T16B, v28, v14, v13);              \/\/       eor3            v22.16B, v28.16B, v14.16B, v13.16B\n+    __ rax1(v27, __ T2D, v16, v23);                    \/\/       rax1            v27.2D, v16.2D, v23.2D\n+    __ xar(v5, __ T2D, v2, v13, 20);                   \/\/       xar             v5.2D, v2.2D, v13.2D, #20\n@@ -1072,4 +1081,4 @@\n-    __ sha512h(v14, __ T2D, v13, v27);                 \/\/       sha512h         q14, q13, v27.2D\n-    __ sha512h2(v16, __ T2D, v23, v5);                 \/\/       sha512h2                q16, q23, v5.2D\n-    __ sha512su0(v2, __ T2D, v13);                     \/\/       sha512su0               v2.2D, v13.2D\n-    __ sha512su1(v10, __ T2D, v15, v10);               \/\/       sha512su1               v10.2D, v15.2D, v10.2D\n+    __ sha512h(v15, __ T2D, v10, v26);                 \/\/       sha512h         q15, q10, v26.2D\n+    __ sha512h2(v22, __ T2D, v11, v3);                 \/\/       sha512h2                q22, q11, v3.2D\n+    __ sha512su0(v22, __ T2D, v16);                    \/\/       sha512su0               v22.2D, v16.2D\n+    __ sha512su1(v4, __ T2D, v1, v19);                 \/\/       sha512su1               v4.2D, v1.2D, v19.2D\n@@ -1078,5 +1087,5 @@\n-    __ sve_add(z26, __ S, 98u);                        \/\/       add     z26.s, z26.s, #0x62\n-    __ sve_sub(z3, __ S, 138u);                        \/\/       sub     z3.s, z3.s, #0x8a\n-    __ sve_and(z4, __ B, 131u);                        \/\/       and     z4.b, z4.b, #0x83\n-    __ sve_eor(z17, __ H, 16368u);                     \/\/       eor     z17.h, z17.h, #0x3ff0\n-    __ sve_orr(z2, __ S, 4164941887u);                 \/\/       orr     z2.s, z2.s, #0xf83ff83f\n+    __ sve_add(z16, __ S, 106u);                       \/\/       add     z16.s, z16.s, #0x6a\n+    __ sve_sub(z14, __ H, 22u);                        \/\/       sub     z14.h, z14.h, #0x16\n+    __ sve_and(z16, __ B, 191u);                       \/\/       and     z16.b, z16.b, #0xbf\n+    __ sve_eor(z5, __ B, 225u);                        \/\/       eor     z5.b, z5.b, #0xe1\n+    __ sve_orr(z12, __ D, 9241386433220968447u);       \/\/       orr     z12.d, z12.d, #0x803fffff803fffff\n@@ -1085,5 +1094,5 @@\n-    __ sve_add(z23, __ B, 51u);                        \/\/       add     z23.b, z23.b, #0x33\n-    __ sve_sub(z7, __ S, 104u);                        \/\/       sub     z7.s, z7.s, #0x68\n-    __ sve_and(z27, __ S, 7864320u);                   \/\/       and     z27.s, z27.s, #0x780000\n-    __ sve_eor(z2, __ D, 68719476224u);                \/\/       eor     z2.d, z2.d, #0xffffffe00\n-    __ sve_orr(z6, __ S, 1056980736u);                 \/\/       orr     z6.s, z6.s, #0x3f003f00\n+    __ sve_add(z6, __ B, 236u);                        \/\/       add     z6.b, z6.b, #0xec\n+    __ sve_sub(z3, __ B, 49u);                         \/\/       sub     z3.b, z3.b, #0x31\n+    __ sve_and(z17, __ S, 536608768u);                 \/\/       and     z17.s, z17.s, #0x1ffc0000\n+    __ sve_eor(z19, __ H, 51199u);                     \/\/       eor     z19.h, z19.h, #0xc7ff\n+    __ sve_orr(z17, __ B, 239u);                       \/\/       orr     z17.b, z17.b, #0xef\n@@ -1092,5 +1101,5 @@\n-    __ sve_add(z12, __ S, 67u);                        \/\/       add     z12.s, z12.s, #0x43\n-    __ sve_sub(z24, __ S, 154u);                       \/\/       sub     z24.s, z24.s, #0x9a\n-    __ sve_and(z0, __ H, 511u);                        \/\/       and     z0.h, z0.h, #0x1ff\n-    __ sve_eor(z19, __ D, 9241386433220968447u);       \/\/       eor     z19.d, z19.d, #0x803fffff803fffff\n-    __ sve_orr(z6, __ B, 128u);                        \/\/       orr     z6.b, z6.b, #0x80\n+    __ sve_add(z6, __ S, 249u);                        \/\/       add     z6.s, z6.s, #0xf9\n+    __ sve_sub(z13, __ S, 54u);                        \/\/       sub     z13.s, z13.s, #0x36\n+    __ sve_and(z0, __ B, 225u);                        \/\/       and     z0.b, z0.b, #0xe1\n+    __ sve_eor(z25, __ H, 57407u);                     \/\/       eor     z25.h, z25.h, #0xe03f\n+    __ sve_orr(z20, __ S, 16368u);                     \/\/       orr     z20.s, z20.s, #0x3ff0\n@@ -1099,5 +1108,5 @@\n-    __ sve_add(z17, __ D, 74u);                        \/\/       add     z17.d, z17.d, #0x4a\n-    __ sve_sub(z10, __ S, 170u);                       \/\/       sub     z10.s, z10.s, #0xaa\n-    __ sve_and(z22, __ D, 17179852800u);               \/\/       and     z22.d, z22.d, #0x3ffffc000\n-    __ sve_eor(z15, __ S, 8388600u);                   \/\/       eor     z15.s, z15.s, #0x7ffff8\n-    __ sve_orr(z4, __ D, 8064u);                       \/\/       orr     z4.d, z4.d, #0x1f80\n+    __ sve_add(z3, __ H, 156u);                        \/\/       add     z3.h, z3.h, #0x9c\n+    __ sve_sub(z20, __ B, 34u);                        \/\/       sub     z20.b, z20.b, #0x22\n+    __ sve_and(z28, __ B, 96u);                        \/\/       and     z28.b, z28.b, #0x60\n+    __ sve_eor(z16, __ S, 491520u);                    \/\/       eor     z16.s, z16.s, #0x78000\n+    __ sve_orr(z15, __ H, 8064u);                      \/\/       orr     z15.h, z15.h, #0x1f80\n@@ -1106,5 +1115,5 @@\n-    __ sve_add(z8, __ S, 162u);                        \/\/       add     z8.s, z8.s, #0xa2\n-    __ sve_sub(z22, __ B, 130u);                       \/\/       sub     z22.b, z22.b, #0x82\n-    __ sve_and(z9, __ S, 4292870159u);                 \/\/       and     z9.s, z9.s, #0xffe0000f\n-    __ sve_eor(z5, __ D, 1150687262887383032u);        \/\/       eor     z5.d, z5.d, #0xff80ff80ff80ff8\n-    __ sve_orr(z22, __ H, 32256u);                     \/\/       orr     z22.h, z22.h, #0x7e00\n+    __ sve_add(z25, __ B, 254u);                       \/\/       add     z25.b, z25.b, #0xfe\n+    __ sve_sub(z16, __ H, 185u);                       \/\/       sub     z16.h, z16.h, #0xb9\n+    __ sve_and(z11, __ B, 96u);                        \/\/       and     z11.b, z11.b, #0x60\n+    __ sve_eor(z20, __ S, 2147482624u);                \/\/       eor     z20.s, z20.s, #0x7ffffc00\n+    __ sve_orr(z4, __ B, 96u);                         \/\/       orr     z4.b, z4.b, #0x60\n@@ -1113,5 +1122,5 @@\n-    __ sve_add(z8, __ S, 134u);                        \/\/       add     z8.s, z8.s, #0x86\n-    __ sve_sub(z25, __ H, 39u);                        \/\/       sub     z25.h, z25.h, #0x27\n-    __ sve_and(z4, __ S, 4186112u);                    \/\/       and     z4.s, z4.s, #0x3fe000\n-    __ sve_eor(z29, __ B, 131u);                       \/\/       eor     z29.b, z29.b, #0x83\n-    __ sve_orr(z29, __ D, 4611685469745315712u);       \/\/       orr     z29.d, z29.d, #0x3fffff803fffff80\n+    __ sve_add(z6, __ D, 35u);                         \/\/       add     z6.d, z6.d, #0x23\n+    __ sve_sub(z28, __ S, 239u);                       \/\/       sub     z28.s, z28.s, #0xef\n+    __ sve_and(z26, __ H, 126u);                       \/\/       and     z26.h, z26.h, #0x7e\n+    __ sve_eor(z11, __ D, 211106232532992u);           \/\/       eor     z11.d, z11.d, #0xc00000000000\n+    __ sve_orr(z1, __ D, 17296056810822168583u);       \/\/       orr     z1.d, z1.d, #0xf007f007f007f007\n@@ -1120,49 +1129,49 @@\n-    __ sve_add(z2, __ B, z11, z28);                    \/\/       add     z2.b, z11.b, z28.b\n-    __ sve_sub(z7, __ S, z1, z26);                     \/\/       sub     z7.s, z1.s, z26.s\n-    __ sve_fadd(z17, __ D, z14, z8);                   \/\/       fadd    z17.d, z14.d, z8.d\n-    __ sve_fmul(z21, __ D, z24, z5);                   \/\/       fmul    z21.d, z24.d, z5.d\n-    __ sve_fsub(z21, __ D, z17, z22);                  \/\/       fsub    z21.d, z17.d, z22.d\n-    __ sve_abs(z29, __ B, p5, z19);                    \/\/       abs     z29.b, p5\/m, z19.b\n-    __ sve_add(z4, __ B, p4, z23);                     \/\/       add     z4.b, p4\/m, z4.b, z23.b\n-    __ sve_and(z19, __ D, p1, z23);                    \/\/       and     z19.d, p1\/m, z19.d, z23.d\n-    __ sve_asr(z19, __ H, p0, z8);                     \/\/       asr     z19.h, p0\/m, z19.h, z8.h\n-    __ sve_bic(z14, __ D, p6, z17);                    \/\/       bic     z14.d, p6\/m, z14.d, z17.d\n-    __ sve_cnt(z21, __ B, p1, z30);                    \/\/       cnt     z21.b, p1\/m, z30.b\n-    __ sve_eor(z10, __ B, p5, z12);                    \/\/       eor     z10.b, p5\/m, z10.b, z12.b\n-    __ sve_lsl(z9, __ S, p1, z24);                     \/\/       lsl     z9.s, p1\/m, z9.s, z24.s\n-    __ sve_lsr(z4, __ H, p6, z6);                      \/\/       lsr     z4.h, p6\/m, z4.h, z6.h\n-    __ sve_mul(z27, __ S, p6, z13);                    \/\/       mul     z27.s, p6\/m, z27.s, z13.s\n-    __ sve_neg(z30, __ S, p5, z22);                    \/\/       neg     z30.s, p5\/m, z22.s\n-    __ sve_not(z30, __ H, p7, z9);                     \/\/       not     z30.h, p7\/m, z9.h\n-    __ sve_orr(z19, __ D, p1, z20);                    \/\/       orr     z19.d, p1\/m, z19.d, z20.d\n-    __ sve_smax(z9, __ H, p2, z13);                    \/\/       smax    z9.h, p2\/m, z9.h, z13.h\n-    __ sve_smin(z19, __ H, p0, z24);                   \/\/       smin    z19.h, p0\/m, z19.h, z24.h\n-    __ sve_sub(z19, __ S, p3, z17);                    \/\/       sub     z19.s, p3\/m, z19.s, z17.s\n-    __ sve_fabs(z16, __ S, p1, z0);                    \/\/       fabs    z16.s, p1\/m, z0.s\n-    __ sve_fadd(z11, __ S, p2, z15);                   \/\/       fadd    z11.s, p2\/m, z11.s, z15.s\n-    __ sve_fdiv(z15, __ D, p1, z15);                   \/\/       fdiv    z15.d, p1\/m, z15.d, z15.d\n-    __ sve_fmax(z5, __ D, p0, z10);                    \/\/       fmax    z5.d, p0\/m, z5.d, z10.d\n-    __ sve_fmin(z26, __ S, p0, z0);                    \/\/       fmin    z26.s, p0\/m, z26.s, z0.s\n-    __ sve_fmul(z19, __ D, p7, z10);                   \/\/       fmul    z19.d, p7\/m, z19.d, z10.d\n-    __ sve_fneg(z3, __ D, p5, z7);                     \/\/       fneg    z3.d, p5\/m, z7.d\n-    __ sve_frintm(z28, __ S, p3, z21);                 \/\/       frintm  z28.s, p3\/m, z21.s\n-    __ sve_frintn(z26, __ D, p3, z17);                 \/\/       frintn  z26.d, p3\/m, z17.d\n-    __ sve_frintp(z17, __ D, p3, z2);                  \/\/       frintp  z17.d, p3\/m, z2.d\n-    __ sve_fsqrt(z16, __ S, p5, z20);                  \/\/       fsqrt   z16.s, p5\/m, z20.s\n-    __ sve_fsub(z19, __ D, p0, z1);                    \/\/       fsub    z19.d, p0\/m, z19.d, z1.d\n-    __ sve_fmad(z17, __ D, p2, z16, z17);              \/\/       fmad    z17.d, p2\/m, z16.d, z17.d\n-    __ sve_fmla(z0, __ S, p1, z2, z23);                \/\/       fmla    z0.s, p1\/m, z2.s, z23.s\n-    __ sve_fmls(z6, __ D, p2, z20, z14);               \/\/       fmls    z6.d, p2\/m, z20.d, z14.d\n-    __ sve_fmsb(z29, __ D, p3, z3, z3);                \/\/       fmsb    z29.d, p3\/m, z3.d, z3.d\n-    __ sve_fnmad(z9, __ S, p0, z24, z27);              \/\/       fnmad   z9.s, p0\/m, z24.s, z27.s\n-    __ sve_fnmsb(z19, __ D, p5, z7, z25);              \/\/       fnmsb   z19.d, p5\/m, z7.d, z25.d\n-    __ sve_fnmla(z13, __ S, p1, z7, z25);              \/\/       fnmla   z13.s, p1\/m, z7.s, z25.s\n-    __ sve_fnmls(z21, __ S, p4, z17, z0);              \/\/       fnmls   z21.s, p4\/m, z17.s, z0.s\n-    __ sve_mla(z9, __ H, p5, z11, z7);                 \/\/       mla     z9.h, p5\/m, z11.h, z7.h\n-    __ sve_mls(z14, __ H, p4, z17, z11);               \/\/       mls     z14.h, p4\/m, z17.h, z11.h\n-    __ sve_and(z24, z17, z30);                         \/\/       and     z24.d, z17.d, z30.d\n-    __ sve_eor(z8, z15, z14);                          \/\/       eor     z8.d, z15.d, z14.d\n-    __ sve_orr(z22, z27, z22);                         \/\/       orr     z22.d, z27.d, z22.d\n-    __ sve_bic(z8, z5, z27);                           \/\/       bic     z8.d, z5.d, z27.d\n-    __ sve_uzp1(z10, __ D, z0, z14);                   \/\/       uzp1    z10.d, z0.d, z14.d\n-    __ sve_uzp2(z21, __ B, z20, z0);                   \/\/       uzp2    z21.b, z20.b, z0.b\n+    __ sve_add(z14, __ S, z8, z17);                    \/\/       add     z14.s, z8.s, z17.s\n+    __ sve_sub(z24, __ S, z5, z19);                    \/\/       sub     z24.s, z5.s, z19.s\n+    __ sve_fadd(z17, __ D, z22, z16);                  \/\/       fadd    z17.d, z22.d, z16.d\n+    __ sve_fmul(z20, __ S, z19, z0);                   \/\/       fmul    z20.s, z19.s, z0.s\n+    __ sve_fsub(z17, __ D, z23, z4);                   \/\/       fsub    z17.d, z23.d, z4.d\n+    __ sve_abs(z4, __ S, p6, z25);                     \/\/       abs     z4.s, p6\/m, z25.s\n+    __ sve_add(z2, __ H, p2, z8);                      \/\/       add     z2.h, p2\/m, z2.h, z8.h\n+    __ sve_and(z24, __ S, p4, z30);                    \/\/       and     z24.s, p4\/m, z24.s, z30.s\n+    __ sve_asr(z4, __ H, p7, z1);                      \/\/       asr     z4.h, p7\/m, z4.h, z1.h\n+    __ sve_bic(z19, __ H, p3, z0);                     \/\/       bic     z19.h, p3\/m, z19.h, z0.h\n+    __ sve_cnt(z7, __ B, p6, z17);                     \/\/       cnt     z7.b, p6\/m, z17.b\n+    __ sve_eor(z27, __ D, p1, z9);                     \/\/       eor     z27.d, p1\/m, z27.d, z9.d\n+    __ sve_lsl(z23, __ D, p3, z16);                    \/\/       lsl     z23.d, p3\/m, z23.d, z16.d\n+    __ sve_lsr(z22, __ D, p5, z20);                    \/\/       lsr     z22.d, p5\/m, z22.d, z20.d\n+    __ sve_mul(z28, __ S, p2, z13);                    \/\/       mul     z28.s, p2\/m, z28.s, z13.s\n+    __ sve_neg(z7, __ H, p5, z28);                     \/\/       neg     z7.h, p5\/m, z28.h\n+    __ sve_not(z11, __ S, p3, z11);                    \/\/       not     z11.s, p3\/m, z11.s\n+    __ sve_orr(z1, __ S, p6, z8);                      \/\/       orr     z1.s, p6\/m, z1.s, z8.s\n+    __ sve_smax(z13, __ S, p4, z17);                   \/\/       smax    z13.s, p4\/m, z13.s, z17.s\n+    __ sve_smin(z4, __ H, p0, z3);                     \/\/       smin    z4.h, p0\/m, z4.h, z3.h\n+    __ sve_sub(z7, __ S, p3, z14);                     \/\/       sub     z7.s, p3\/m, z7.s, z14.s\n+    __ sve_fabs(z4, __ S, p3, z29);                    \/\/       fabs    z4.s, p3\/m, z29.s\n+    __ sve_fadd(z0, __ D, p2, z21);                    \/\/       fadd    z0.d, p2\/m, z0.d, z21.d\n+    __ sve_fdiv(z3, __ D, p0, z9);                     \/\/       fdiv    z3.d, p0\/m, z3.d, z9.d\n+    __ sve_fmax(z28, __ S, p2, z24);                   \/\/       fmax    z28.s, p2\/m, z28.s, z24.s\n+    __ sve_fmin(z19, __ D, p1, z23);                   \/\/       fmin    z19.d, p1\/m, z19.d, z23.d\n+    __ sve_fmul(z13, __ D, p5, z10);                   \/\/       fmul    z13.d, p5\/m, z13.d, z10.d\n+    __ sve_fneg(z12, __ D, p4, z30);                   \/\/       fneg    z12.d, p4\/m, z30.d\n+    __ sve_frintm(z14, __ D, p0, z29);                 \/\/       frintm  z14.d, p0\/m, z29.d\n+    __ sve_frintn(z21, __ D, p5, z7);                  \/\/       frintn  z21.d, p5\/m, z7.d\n+    __ sve_frintp(z2, __ D, p0, z26);                  \/\/       frintp  z2.d, p0\/m, z26.d\n+    __ sve_fsqrt(z9, __ D, p4, z17);                   \/\/       fsqrt   z9.d, p4\/m, z17.d\n+    __ sve_fsub(z0, __ D, p1, z2);                     \/\/       fsub    z0.d, p1\/m, z0.d, z2.d\n+    __ sve_fmad(z14, __ S, p1, z11, z20);              \/\/       fmad    z14.s, p1\/m, z11.s, z20.s\n+    __ sve_fmla(z16, __ S, p7, z12, z3);               \/\/       fmla    z16.s, p7\/m, z12.s, z3.s\n+    __ sve_fmls(z22, __ D, p2, z3, z24);               \/\/       fmls    z22.d, p2\/m, z3.d, z24.d\n+    __ sve_fmsb(z3, __ D, p4, z22, z7);                \/\/       fmsb    z3.d, p4\/m, z22.d, z7.d\n+    __ sve_fnmad(z21, __ D, p3, z5, z7);               \/\/       fnmad   z21.d, p3\/m, z5.d, z7.d\n+    __ sve_fnmsb(z5, __ S, p5, z17, z17);              \/\/       fnmsb   z5.s, p5\/m, z17.s, z17.s\n+    __ sve_fnmla(z3, __ S, p2, z19, z11);              \/\/       fnmla   z3.s, p2\/m, z19.s, z11.s\n+    __ sve_fnmls(z11, __ S, p3, z17, z17);             \/\/       fnmls   z11.s, p3\/m, z17.s, z17.s\n+    __ sve_mla(z13, __ S, p6, z17, z30);               \/\/       mla     z13.s, p6\/m, z17.s, z30.s\n+    __ sve_mls(z8, __ S, p4, z14, z26);                \/\/       mls     z8.s, p4\/m, z14.s, z26.s\n+    __ sve_and(z27, z22, z7);                          \/\/       and     z27.d, z22.d, z7.d\n+    __ sve_eor(z5, z27, z27);                          \/\/       eor     z5.d, z27.d, z27.d\n+    __ sve_orr(z0, z14, z24);                          \/\/       orr     z0.d, z14.d, z24.d\n+    __ sve_bic(z20, z0, z3);                           \/\/       bic     z20.d, z0.d, z3.d\n+    __ sve_uzp1(z25, __ D, z5, z25);                   \/\/       uzp1    z25.d, z5.d, z25.d\n+    __ sve_uzp2(z17, __ H, z17, z1);                   \/\/       uzp2    z17.h, z17.h, z1.h\n@@ -1171,9 +1180,9 @@\n-    __ sve_andv(v22, __ D, p6, z5);                    \/\/       andv d22, p6, z5.d\n-    __ sve_orv(v29, __ B, p4, z17);                    \/\/       orv b29, p4, z17.b\n-    __ sve_eorv(v12, __ H, p3, z29);                   \/\/       eorv h12, p3, z29.h\n-    __ sve_smaxv(v0, __ D, p4, z2);                    \/\/       smaxv d0, p4, z2.d\n-    __ sve_sminv(v20, __ D, p5, z21);                  \/\/       sminv d20, p5, z21.d\n-    __ sve_fminv(v12, __ S, p2, z2);                   \/\/       fminv s12, p2, z2.s\n-    __ sve_fmaxv(v14, __ S, p5, z22);                  \/\/       fmaxv s14, p5, z22.s\n-    __ sve_fadda(v19, __ D, p6, z26);                  \/\/       fadda d19, p6, d19, z26.d\n-    __ sve_uaddv(v12, __ B, p5, z21);                  \/\/       uaddv d12, p5, z21.b\n+    __ sve_andv(v14, __ B, p7, z13);                   \/\/       andv b14, p7, z13.b\n+    __ sve_orv(v17, __ S, p0, z30);                    \/\/       orv s17, p0, z30.s\n+    __ sve_eorv(v22, __ H, p5, z29);                   \/\/       eorv h22, p5, z29.h\n+    __ sve_smaxv(v8, __ H, p0, z0);                    \/\/       smaxv h8, p0, z0.h\n+    __ sve_sminv(v23, __ S, p5, z0);                   \/\/       sminv s23, p5, z0.s\n+    __ sve_fminv(v25, __ S, p6, z23);                  \/\/       fminv s25, p6, z23.s\n+    __ sve_fmaxv(v21, __ S, p5, z1);                   \/\/       fmaxv s21, p5, z1.s\n+    __ sve_fadda(v10, __ D, p5, z11);                  \/\/       fadda d10, p5, d10, z11.d\n+    __ sve_uaddv(v23, __ D, p6, z8);                   \/\/       uaddv d23, p6, z8.d\n@@ -1198,7 +1207,7 @@\n-    0x14000000,     0x17ffffd7,     0x140003d5,     0x94000000,\n-    0x97ffffd4,     0x940003d2,     0x3400000a,     0x34fffa2a,\n-    0x340079ea,     0x35000008,     0x35fff9c8,     0x35007988,\n-    0xb400000b,     0xb4fff96b,     0xb400792b,     0xb500001d,\n-    0xb5fff91d,     0xb50078dd,     0x10000013,     0x10fff8b3,\n-    0x10007873,     0x90000013,     0x36300016,     0x3637f836,\n-    0x363077f6,     0x3758000c,     0x375ff7cc,     0x3758778c,\n+    0x14000000,     0x17ffffd7,     0x140003de,     0x94000000,\n+    0x97ffffd4,     0x940003db,     0x3400000a,     0x34fffa2a,\n+    0x34007b0a,     0x35000008,     0x35fff9c8,     0x35007aa8,\n+    0xb400000b,     0xb4fff96b,     0xb4007a4b,     0xb500001d,\n+    0xb5fff91d,     0xb50079fd,     0x10000013,     0x10fff8b3,\n+    0x10007993,     0x90000013,     0x36300016,     0x3637f836,\n+    0x36307916,     0x3758000c,     0x375ff7cc,     0x375878ac,\n@@ -1209,13 +1218,13 @@\n-    0x54007560,     0x54000001,     0x54fff541,     0x54007501,\n-    0x54000002,     0x54fff4e2,     0x540074a2,     0x54000002,\n-    0x54fff482,     0x54007442,     0x54000003,     0x54fff423,\n-    0x540073e3,     0x54000003,     0x54fff3c3,     0x54007383,\n-    0x54000004,     0x54fff364,     0x54007324,     0x54000005,\n-    0x54fff305,     0x540072c5,     0x54000006,     0x54fff2a6,\n-    0x54007266,     0x54000007,     0x54fff247,     0x54007207,\n-    0x54000008,     0x54fff1e8,     0x540071a8,     0x54000009,\n-    0x54fff189,     0x54007149,     0x5400000a,     0x54fff12a,\n-    0x540070ea,     0x5400000b,     0x54fff0cb,     0x5400708b,\n-    0x5400000c,     0x54fff06c,     0x5400702c,     0x5400000d,\n-    0x54fff00d,     0x54006fcd,     0x5400000e,     0x54ffefae,\n-    0x54006f6e,     0x5400000f,     0x54ffef4f,     0x54006f0f,\n+    0x54007680,     0x54000001,     0x54fff541,     0x54007621,\n+    0x54000002,     0x54fff4e2,     0x540075c2,     0x54000002,\n+    0x54fff482,     0x54007562,     0x54000003,     0x54fff423,\n+    0x54007503,     0x54000003,     0x54fff3c3,     0x540074a3,\n+    0x54000004,     0x54fff364,     0x54007444,     0x54000005,\n+    0x54fff305,     0x540073e5,     0x54000006,     0x54fff2a6,\n+    0x54007386,     0x54000007,     0x54fff247,     0x54007327,\n+    0x54000008,     0x54fff1e8,     0x540072c8,     0x54000009,\n+    0x54fff189,     0x54007269,     0x5400000a,     0x54fff12a,\n+    0x5400720a,     0x5400000b,     0x54fff0cb,     0x540071ab,\n+    0x5400000c,     0x54fff06c,     0x5400714c,     0x5400000d,\n+    0x54fff00d,     0x540070ed,     0x5400000e,     0x54ffefae,\n+    0x5400708e,     0x5400000f,     0x54ffef4f,     0x5400702f,\n@@ -1291,56 +1300,57 @@\n-    0x1e2601c7,     0x9e660107,     0x1e270234,     0x9e6703dc,\n-    0x1e222200,     0x1e702120,     0x1e202288,     0x1e6023a8,\n-    0x29266b01,     0x29462d85,     0x69463f75,     0xa90272c5,\n-    0xa97e467b,     0x29aa1f4d,     0x29fa54cd,     0x69c27b74,\n-    0xa9b81555,     0xa9fa12ee,     0x2884321d,     0x28cc477a,\n-    0x68f451c4,     0xa8b909d0,     0xa8f060f7,     0x281069e0,\n-    0x2866191a,     0xa8392b2f,     0xa8760670,     0x0c4073db,\n-    0x4cdfa079,     0x0cca6e1e,     0x4cdf2670,     0x0d40c317,\n-    0x4ddfc948,     0x0dd7ce89,     0x4c408c62,     0x0cdf87c8,\n-    0x4d60c344,     0x0dffca23,     0x4df0cd7d,     0x4cd74801,\n-    0x0c404aa0,     0x4d40e4e5,     0x4ddfe8e1,     0x0dcfeca2,\n-    0x4cdf07bb,     0x0cc70098,     0x0d60e2ef,     0x0dffe6ae,\n-    0x0df9e934,     0x0e31bb17,     0x4e31bb7a,     0x0e71b8c5,\n-    0x4e71b8e6,     0x4eb1ba0f,     0x0e30aa0f,     0x4e30ab59,\n-    0x0e70aa30,     0x4e70ab9b,     0x4eb0ab38,     0x6e30fa0f,\n-    0x0e31ab59,     0x2e31a9ee,     0x4e31a96a,     0x6e31a9cd,\n-    0x0e71a9ee,     0x2e71aab4,     0x4e71a841,     0x6e71aaf6,\n-    0x4eb1abfe,     0x6eb1a9ee,     0x6eb0f862,     0x7e30f8e6,\n-    0x7e70f883,     0x7eb0f907,     0x7ef0fb38,     0x0e20b820,\n-    0x4e20bb9b,     0x0e60bbdd,     0x4e60b8c5,     0x0ea0b8c5,\n-    0x4ea0bbdd,     0x4ee0b98b,     0x0ea0fb59,     0x4ea0f820,\n-    0x4ee0fbfe,     0x2ea0f820,     0x6ea0fa51,     0x6ee0fbbc,\n-    0x2ea1fb59,     0x6ea1f949,     0x6ee1fb59,     0x2e2059ac,\n-    0x6e205a0f,     0x0e2d1d8b,     0x4e2c1d6a,     0x0eb31e51,\n-    0x4eba1f38,     0x2e371ed5,     0x6e391f17,     0x0e228420,\n-    0x4e328630,     0x0e6c856a,     0x4e6884e6,     0x0ebe87bc,\n-    0x4ea884e6,     0x4ee784c5,     0x0e27d4c5,     0x4e36d6b4,\n-    0x4e73d651,     0x2e31860f,     0x6e338651,     0x2e7f87dd,\n-    0x6e7c877a,     0x2ebe87bc,     0x6ea38441,     0x6efd879b,\n-    0x0ea2d420,     0x4eb6d6b4,     0x4efed7bc,     0x0e319e0f,\n-    0x4e2e9dac,     0x0e6c9d6a,     0x4e7e9fbc,     0x0ebe9fbc,\n-    0x4eb59e93,     0x2eb8d6f6,     0x6eacd56a,     0x6ee6d4a4,\n-    0x2e20dffe,     0x6e36deb4,     0x6e6add28,     0x0e6097fe,\n-    0x4e739651,     0x0eac956a,     0x4ebd979b,     0x0e24cc62,\n-    0x4e3acf38,     0x4e66cca4,     0x2e659483,     0x6e6a9528,\n-    0x2eb896f6,     0x6eb39651,     0x0eafcdcd,     0x4ea6cca4,\n-    0x4efecfbc,     0x2e39ff17,     0x6e37fed5,     0x6e7bff59,\n-    0x0e3a6738,     0x4e256483,     0x0e796717,     0x4e7c677a,\n-    0x0eb96717,     0x4eb065ee,     0x0e37a6d5,     0x4e25a483,\n-    0x0e79a717,     0x4e6aa528,     0x0ebaa738,     0x4eb5a693,\n-    0x0e31f60f,     0x4e32f630,     0x4e64f462,     0x0e236c41,\n-    0x4e226c20,     0x0e7a6f38,     0x4e666ca4,     0x0ea56c83,\n-    0x4ead6d8b,     0x0e20affe,     0x4e3daf9b,     0x0e6bad49,\n-    0x4e7baf59,     0x0ea4ac62,     0x4eaeadac,     0x0eb3f651,\n-    0x4ea0f7fe,     0x4ee3f441,     0x2e2e8dac,     0x6e3e8fbc,\n-    0x2e628c20,     0x6e738e51,     0x2eae8dac,     0x6eb38e51,\n-    0x6ef78ed5,     0x0e2ee5ac,     0x4e3de79b,     0x4e7fe7dd,\n-    0x0e2037fe,     0x4e233441,     0x0e7b3759,     0x4e7d379b,\n-    0x0ea634a4,     0x4ebf37dd,     0x4ee53483,     0x2e2834e6,\n-    0x6e3f37dd,     0x2e7b3759,     0x6e733651,     0x2eaa3528,\n-    0x6ea93507,     0x6eee35ac,     0x2e223c20,     0x6e353e93,\n-    0x2e633c41,     0x6e793f17,     0x2ea43c62,     0x6ea23c20,\n-    0x6eea3d28,     0x2eb9e717,     0x6ebbe759,     0x6ef1e60f,\n-    0x0e3f3fdd,     0x4e253c83,     0x0e6c3d6a,     0x4e783ef6,\n-    0x0eac3d6a,     0x4ea63ca4,     0x4ef33e51,     0x2e23e441,\n-    0x6e2de58b,     0x6e69e507,     0xba5fd3e3,     0x3a5f03e5,\n+    0x1e2401c7,     0x9e640107,     0x1e300234,     0x9e7003dc,\n+    0x1e260050,     0x9e660209,     0x1e2703b4,     0x9e670024,\n+    0x1e382340,     0x1e6e22e0,     0x1e2022a8,     0x1e602188,\n+    0x2928630c,     0x29501616,     0x694e4db4,     0xa90619b1,\n+    0xa9760625,     0x29a652cd,     0x29ca6d5e,     0x69c2534d,\n+    0xa9bb5fa4,     0xa9f900d6,     0x288a6cb1,     0x28e02e0e,\n+    0x68e25d2c,     0xa8821c17,     0xa8c52351,     0x282a3d4b,\n+    0x28484093,     0xa831393e,     0xa8425e9d,     0x0c407365,\n+    0x4cdfa32a,     0x0cd36fcf,     0x4cdf2611,     0x0d40c2fe,\n+    0x4ddfc911,     0x0dc3cd2c,     0x4c408c53,     0x0cdf8515,\n+    0x4d60c08d,     0x0dffc87c,     0x4de0cfbd,     0x4cd54827,\n+    0x0c404811,     0x4d40e4ba,     0x4ddfe839,     0x0dddec56,\n+    0x4cdf076d,     0x0cd7031d,     0x0d60e1ed,     0x0dffe5cf,\n+    0x0df7ea9b,     0x0e31bb38,     0x4e31ba0f,     0x0e71bb59,\n+    0x4e71b9ee,     0x4eb1b96a,     0x0e30a9cd,     0x4e30a9ee,\n+    0x0e70aab4,     0x4e70a841,     0x4eb0aaf6,     0x6e30fbfe,\n+    0x0e31a9ee,     0x2e31a862,     0x4e31a8e6,     0x6e31a883,\n+    0x0e71a907,     0x2e71ab38,     0x4e71a820,     0x6e71ab9b,\n+    0x4eb1abdd,     0x6eb1a8c5,     0x6eb0f8c5,     0x7e30fbdd,\n+    0x7e70f98b,     0x7eb0fb59,     0x7ef0f820,     0x0e20bbfe,\n+    0x4e20b820,     0x0e60ba51,     0x4e60bbbc,     0x0ea0bb59,\n+    0x4ea0b949,     0x4ee0bb59,     0x0ea0f9ac,     0x4ea0fa0f,\n+    0x4ee0f98b,     0x2ea0f96a,     0x6ea0fa51,     0x6ee0fb38,\n+    0x2ea1fad5,     0x6ea1fb17,     0x6ee1f820,     0x2e205a30,\n+    0x6e20596a,     0x0e281ce6,     0x4e3e1fbc,     0x0ea81ce6,\n+    0x4ea71cc5,     0x2e271cc5,     0x6e361eb4,     0x0e338651,\n+    0x4e31860f,     0x0e738651,     0x4e7f87dd,     0x0ebc877a,\n+    0x4ebe87bc,     0x4ee38441,     0x0e3dd79b,     0x4e22d420,\n+    0x4e76d6b4,     0x2e3e87bc,     0x6e31860f,     0x2e6e85ac,\n+    0x6e6c856a,     0x2ebe87bc,     0x6ebe87bc,     0x6ef58693,\n+    0x0eb8d6f6,     0x4eacd56a,     0x4ee6d4a4,     0x0e209ffe,\n+    0x4e369eb4,     0x0e6a9d28,     0x4e609ffe,     0x0eb39e51,\n+    0x4eac9d6a,     0x2ebdd79b,     0x6ea4d462,     0x6efad738,\n+    0x2e26dca4,     0x6e25dc83,     0x6e6add28,     0x0e7896f6,\n+    0x4e739651,     0x0eaf95cd,     0x4ea694a4,     0x0e3ecfbc,\n+    0x4e39cf17,     0x4e77ced5,     0x2e7b9759,     0x6e7a9738,\n+    0x2ea59483,     0x6eb99717,     0x0ebccf7a,     0x4eb9cf17,\n+    0x4ef0cdee,     0x2e37fed5,     0x6e25fc83,     0x6e79ff17,\n+    0x0e2a6528,     0x4e3a6738,     0x0e756693,     0x4e71660f,\n+    0x0eb26630,     0x4ea46462,     0x0e23a441,     0x4e22a420,\n+    0x0e7aa738,     0x4e66a4a4,     0x0ea5a483,     0x4eada58b,\n+    0x0e20f7fe,     0x4e3df79b,     0x4e6bf549,     0x0e3b6f59,\n+    0x4e246c62,     0x0e6e6dac,     0x4e736e51,     0x0ea06ffe,\n+    0x4ea36c41,     0x0e2eadac,     0x4e3eafbc,     0x0e62ac20,\n+    0x4e73ae51,     0x0eaeadac,     0x4eb3ae51,     0x0eb7f6d5,\n+    0x4eaef5ac,     0x4efdf79b,     0x2e3f8fdd,     0x6e208ffe,\n+    0x2e638c41,     0x6e7b8f59,     0x2ebd8f9b,     0x6ea68ca4,\n+    0x6eff8fdd,     0x0e25e483,     0x4e28e4e6,     0x4e7fe7dd,\n+    0x0e3b3759,     0x4e333651,     0x0e6a3528,     0x4e693507,\n+    0x0eae35ac,     0x4ea23420,     0x4ef53693,     0x2e233441,\n+    0x6e393717,     0x2e643462,     0x6e623420,     0x2eaa3528,\n+    0x6eb93717,     0x6efb3759,     0x2e313e0f,     0x6e3f3fdd,\n+    0x2e653c83,     0x6e6c3d6a,     0x2eb83ef6,     0x6eac3d6a,\n+    0x6ee63ca4,     0x2eb3e651,     0x6ea3e441,     0x6eede58b,\n+    0x0e293d07,     0x4e2c3d6a,     0x0e713e0f,     0x4e723e30,\n+    0x0ea43c62,     0x4eab3d49,     0x4eed3d8b,     0x2e2ee5ac,\n+    0x6e30e5ee,     0x6e6fe5cd,     0xba5fd3e3,     0x3a5f03e5,\n@@ -1352,1 +1362,2 @@\n-    0x9eae0020,     0x4cc0ac3f,     0x4ea1b820,     0x05a08020,\n+    0x9eae0020,     0x0f03f409,     0x6f03f40e,     0x4cc0ac3f,\n+    0x0ea1b820,     0x4e21c862,     0x4e61b8a4,     0x05a08020,\n@@ -1377,67 +1388,68 @@\n-    0x65854891,     0x65c74cc1,     0x05733820,     0x05b238a4,\n-    0x05f138e6,     0x0570396a,     0x65d0a001,     0x65d6a443,\n-    0x65d4a826,     0x6594ac26,     0x6554ac26,     0x6556ac26,\n-    0x6552ac26,     0x65cbac85,     0x65caac01,     0x65dea833,\n-    0x659ca509,     0x65d8a801,     0x65dcac01,     0x655cb241,\n-    0x0520a1e0,     0x0521a601,     0x052281e0,     0x05238601,\n-    0x04a14026,     0x0568aca7,     0x05b23230,     0x853040af,\n-    0xc5b040af,     0xe57080af,     0xe5b080af,     0x25034440,\n-    0x254054c4,     0x25034640,     0x25415a05,     0x25834440,\n-    0x25c54489,     0x250b5d3a,     0x2550dc20,     0x2518e3e1,\n-    0x2518e021,     0x2518e0a1,     0x2518e121,     0x2518e1a1,\n-    0x2558e3e2,     0x2558e042,     0x2558e0c2,     0x2558e142,\n-    0x2598e3e3,     0x2598e063,     0x2598e0e3,     0x2598e163,\n-    0x25d8e3e4,     0x25d8e084,     0x25d8e104,     0x25d8e184,\n-    0x2518e407,     0x05214800,     0x05614800,     0x05a14800,\n-    0x05e14800,     0x05214c00,     0x05614c00,     0x05a14c00,\n-    0x05e14c00,     0x05304001,     0x05314001,     0x1e601000,\n-    0x1e603000,     0x1e621000,     0x1e623000,     0x1e641000,\n-    0x1e643000,     0x1e661000,     0x1e663000,     0x1e681000,\n-    0x1e683000,     0x1e6a1000,     0x1e6a3000,     0x1e6c1000,\n-    0x1e6c3000,     0x1e6e1000,     0x1e6e3000,     0x1e701000,\n-    0x1e703000,     0x1e721000,     0x1e723000,     0x1e741000,\n-    0x1e743000,     0x1e761000,     0x1e763000,     0x1e781000,\n-    0x1e783000,     0x1e7a1000,     0x1e7a3000,     0x1e7c1000,\n-    0x1e7c3000,     0x1e7e1000,     0x1e7e3000,     0xf82a822f,\n-    0xf822018a,     0xf82c11af,     0xf8222287,     0xf83a3090,\n-    0xf8225184,     0xf8304215,     0xf83072ab,     0xf837634c,\n-    0xf8b781dc,     0xf8ab0038,     0xf8ac115f,     0xf8b02047,\n-    0xf8a3326d,     0xf8b15070,     0xf8a143cb,     0xf8a571e8,\n-    0xf8bd601e,     0xf8f48287,     0xf8f702bc,     0xf8fb10b9,\n-    0xf8e12217,     0xf8ff3185,     0xf8e951fc,     0xf8fd43f6,\n-    0xf8f370bf,     0xf8ee63f0,     0xf870829b,     0xf870016c,\n-    0xf86913c6,     0xf871239b,     0xf87e3147,     0xf874508a,\n-    0xf8784231,     0xf87673a3,     0xf86f6276,     0xb8338056,\n-    0xb82f0186,     0xb83011ab,     0xb83723c1,     0xb8333225,\n-    0xb82252d0,     0xb82d42aa,     0xb83d719b,     0xb83b6023,\n-    0xb8bf8278,     0xb8b10389,     0xb8bb10ef,     0xb8b523f7,\n-    0xb8b933e2,     0xb8bb5150,     0xb8b74073,     0xb8b07320,\n-    0xb8ba6057,     0xb8f0808c,     0xb8fc03be,     0xb8f010db,\n-    0xb8e921fd,     0xb8e730e4,     0xb8ef52e9,     0xb8e84382,\n-    0xb8f570bf,     0xb8fb6220,     0xb86f8344,     0xb86802dc,\n-    0xb87b133b,     0xb8772080,     0xb8663010,     0xb864502f,\n-    0xb86a40a7,     0xb86a70fc,     0xb87462b7,     0xce284145,\n-    0xce1108de,     0xce7c8fab,     0xce96eb42,     0xce7b81ae,\n-    0xce6586f0,     0xcec081a2,     0xce6a89ea,     0x25a0cc5a,\n-    0x25a1d143,     0x05800e44,     0x05406531,     0x05002d42,\n-    0x2520c677,     0x25a1cd07,     0x0580687b,     0x0543bb42,\n-    0x050044a6,     0x25a0c86c,     0x25a1d358,     0x05800500,\n-    0x05400ad3,     0x05000e06,     0x25e0c951,     0x25a1d54a,\n-    0x05839276,     0x0540ea6f,     0x0503c8a4,     0x25a0d448,\n-    0x2521d056,     0x058059c9,     0x05406d05,     0x05003cb6,\n-    0x25a0d0c8,     0x2561c4f9,     0x05809904,     0x05400e5d,\n-    0x0500cadd,     0x043c0162,     0x04ba0427,     0x65c801d1,\n-    0x65c50b15,     0x65d60635,     0x0416b67d,     0x040012e4,\n-    0x04da06f3,     0x04508113,     0x04db1a2e,     0x041aa7d5,\n-    0x0419158a,     0x04938709,     0x045198c4,     0x049019bb,\n-    0x0497b6de,     0x045ebd3e,     0x04d80693,     0x044809a9,\n-    0x044a0313,     0x04810e33,     0x049ca410,     0x658089eb,\n-    0x65cd85ef,     0x65c68145,     0x6587801a,     0x65c29d53,\n-    0x04ddb4e3,     0x6582aebc,     0x65c0ae3a,     0x65c1ac51,\n-    0x658db690,     0x65c18033,     0x65f18a11,     0x65b70440,\n-    0x65ee2a86,     0x65e3ac7d,     0x65bbc309,     0x65f9f4f3,\n-    0x65b944ed,     0x65a07235,     0x04475569,     0x044b722e,\n-    0x043e3238,     0x04ae31e8,     0x04763376,     0x04fb30a8,\n-    0x05ee680a,     0x05206e95,     0x04da38b6,     0x0418323d,\n-    0x04592fac,     0x04c83040,     0x04ca36b4,     0x6587284c,\n-    0x658636ce,     0x65d83b53,     0x040136ac,\n+    0x65854891,     0x65c74cc1,     0x65d02cc1,     0x05733820,\n+    0x05b238a4,     0x05f138e6,     0x0570396a,     0x65d0a001,\n+    0x65d6a443,     0x65d4a826,     0x6594ac26,     0x6554ac26,\n+    0x6556ac26,     0x6552ac26,     0x65cbac85,     0x65caac01,\n+    0x65dea833,     0x659ca509,     0x65d8a801,     0x65dcac01,\n+    0x655cb241,     0x0520a1e0,     0x0521a601,     0x052281e0,\n+    0x05238601,     0x04a14026,     0x0568aca7,     0x05b23230,\n+    0x853040af,     0xc5b040af,     0xe57080af,     0xe5b080af,\n+    0x25034440,     0x254054c4,     0x25034640,     0x25415a05,\n+    0x25834440,     0x25c54489,     0x250b5d3a,     0x2550dc20,\n+    0x2518e3e1,     0x2518e021,     0x2518e0a1,     0x2518e121,\n+    0x2518e1a1,     0x2558e3e2,     0x2558e042,     0x2558e0c2,\n+    0x2558e142,     0x2598e3e3,     0x2598e063,     0x2598e0e3,\n+    0x2598e163,     0x25d8e3e4,     0x25d8e084,     0x25d8e104,\n+    0x25d8e184,     0x2518e407,     0x05214800,     0x05614800,\n+    0x05a14800,     0x05e14800,     0x05214c00,     0x05614c00,\n+    0x05a14c00,     0x05e14c00,     0x05304001,     0x05314001,\n+    0x1e601000,     0x1e603000,     0x1e621000,     0x1e623000,\n+    0x1e641000,     0x1e643000,     0x1e661000,     0x1e663000,\n+    0x1e681000,     0x1e683000,     0x1e6a1000,     0x1e6a3000,\n+    0x1e6c1000,     0x1e6c3000,     0x1e6e1000,     0x1e6e3000,\n+    0x1e701000,     0x1e703000,     0x1e721000,     0x1e723000,\n+    0x1e741000,     0x1e743000,     0x1e761000,     0x1e763000,\n+    0x1e781000,     0x1e783000,     0x1e7a1000,     0x1e7a3000,\n+    0x1e7c1000,     0x1e7c3000,     0x1e7e1000,     0x1e7e3000,\n+    0xf8228287,     0xf83a0090,     0xf8221184,     0xf8302215,\n+    0xf83032ab,     0xf837534c,     0xf83741dc,     0xf82b7038,\n+    0xf82c615f,     0xf8b08047,     0xf8a3026d,     0xf8b11070,\n+    0xf8a123cb,     0xf8a531e8,     0xf8bd501e,     0xf8b44287,\n+    0xf8b772bc,     0xf8bb60b9,     0xf8e18217,     0xf8ff0185,\n+    0xf8e911fc,     0xf8fd23f6,     0xf8f330bf,     0xf8ee53f0,\n+    0xf8f0429b,     0xf8f0716c,     0xf8e963c6,     0xf871839b,\n+    0xf87e0147,     0xf874108a,     0xf8782231,     0xf87633a3,\n+    0xf86f5276,     0xf8734056,     0xf86f7186,     0xf87061ab,\n+    0xb83783c1,     0xb8330225,     0xb82212d0,     0xb82d22aa,\n+    0xb83d319b,     0xb83b5023,     0xb83f4278,     0xb8317389,\n+    0xb83b60ef,     0xb8b583f7,     0xb8b903e2,     0xb8bb1150,\n+    0xb8b72073,     0xb8b03320,     0xb8ba5057,     0xb8b0408c,\n+    0xb8bc73be,     0xb8b060db,     0xb8e981fd,     0xb8e700e4,\n+    0xb8ef12e9,     0xb8e82382,     0xb8f530bf,     0xb8fb5220,\n+    0xb8ef4344,     0xb8e872dc,     0xb8fb633b,     0xb8778080,\n+    0xb8660010,     0xb864102f,     0xb86a20a7,     0xb86a30fc,\n+    0xb87452b7,     0xb866410b,     0xb87170df,     0xb8716182,\n+    0xce226b9d,     0xce0e3796,     0xce778e1b,     0xce8d5045,\n+    0xce7a814f,     0xce638576,     0xcec08216,     0xce738824,\n+    0x25a0cd50,     0x2561c2ce,     0x05800ed0,     0x05401e65,\n+    0x05000acc,     0x2520dd86,     0x2521c623,     0x05807151,\n+    0x05401593,     0x05001ed1,     0x25a0df26,     0x25a1c6cd,\n+    0x05801e60,     0x05401d19,     0x0500e134,     0x2560d383,\n+    0x2521c454,     0x05801e3c,     0x05408870,     0x05004caf,\n+    0x2520dfd9,     0x2561d730,     0x05801e2b,     0x0540b294,\n+    0x05001e24,     0x25e0c466,     0x25a1ddfc,     0x05807cba,\n+    0x0542902b,     0x050024c1,     0x04b1010e,     0x04b304b8,\n+    0x65d002d1,     0x65800a74,     0x65c406f1,     0x0496bb24,\n+    0x04400902,     0x049a13d8,     0x04509c24,     0x045b0c13,\n+    0x041aba27,     0x04d9053b,     0x04d38e17,     0x04d19696,\n+    0x049009bc,     0x0457b787,     0x049ead6b,     0x04981901,\n+    0x0488122d,     0x044a0064,     0x04810dc7,     0x049cafa4,\n+    0x65c08aa0,     0x65cd8123,     0x65868b1c,     0x65c786f3,\n+    0x65c2954d,     0x04ddb3cc,     0x65c2a3ae,     0x65c0b4f5,\n+    0x65c1a342,     0x65cdb229,     0x65c18440,     0x65b4856e,\n+    0x65a31d90,     0x65f82876,     0x65e7b2c3,     0x65e7ccb5,\n+    0x65b1f625,     0x65ab4a63,     0x65b16e2b,     0x049e5a2d,\n+    0x049a71c8,     0x042732db,     0x04bb3365,     0x047831c0,\n+    0x04e33014,     0x05f968b9,     0x05616e31,     0x041a3dae,\n+    0x049823d1,     0x045937b6,     0x04482008,     0x048a3417,\n+    0x65873af9,     0x65863435,     0x65d8356a,     0x04c13917,\n+\n","filename":"test\/hotspot\/gtest\/aarch64\/asmtest.out.h","additions":541,"deletions":529,"binary":false,"changes":1070,"status":"modified"},{"patch":"@@ -0,0 +1,94 @@\n+\/*\n+ * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+\/**\n+ * @test\n+ * @bug 8282541\n+ * @summary Auto-vectorize Math.round API\n+ * @requires vm.compiler2.enabled\n+ * @requires os.simpleArch == \"aarch64\"\n+ * @library \/test\/lib \/\n+ * @run driver compiler.vectorization.TestRoundVectAArch64\n+ *\/\n+\n+package compiler.vectorization;\n+\n+import compiler.lib.ir_framework.*;\n+\n+public class TestRoundVectAArch64 {\n+  private static final int ARRLEN = 1024;\n+  private static final int ITERS  = 11000;\n+\n+  private static double [] dinp;\n+  private static long   [] lout;\n+  private static float  [] finp;\n+  private static int    [] iout;\n+\n+  public static void main(String args[]) {\n+      if (System.getProperty(\"os.arch\").equals(\"aarch64\")) {\n+          TestFramework.runWithFlags(\"-XX:-TieredCompilation\",\n+                                     \"-XX:CompileThresholdScaling=0.3\");\n+      }\n+      System.out.println(\"PASSED\");\n+  }\n+\n+  @Test\n+  @IR(counts = {\"RoundVD\" , \" > 0 \"})\n+  public void test_round_double(long[] lout, double[] dinp) {\n+      for (int i = 0; i < lout.length; i+=1) {\n+          lout[i] = Math.round(dinp[i]);\n+      }\n+  }\n+\n+  @Run(test = {\"test_round_double\"}, mode = RunMode.STANDALONE)\n+  public void kernel_test_round_double() {\n+      dinp = new double[ARRLEN];\n+      lout = new long[ARRLEN];\n+      for(int i = 0 ; i < ARRLEN; i++) {\n+          dinp[i] = (double)i*1.4;\n+      }\n+      for (int i = 0; i < ITERS; i++) {\n+          test_round_double(lout , dinp);\n+      }\n+  }\n+\n+  @Test\n+  @IR(counts = {\"RoundVF\" , \" > 0 \"})\n+  public void test_round_float(int[] iout, float[] finp) {\n+      for (int i = 0; i < finp.length; i+=1) {\n+          iout[i] = Math.round(finp[i]);\n+      }\n+  }\n+\n+  @Run(test = {\"test_round_float\"}, mode = RunMode.STANDALONE)\n+  public void kernel_test_round() {\n+      finp = new float[ARRLEN];\n+      iout = new int[ARRLEN];\n+      for(int i = 0 ; i < ARRLEN; i++) {\n+          finp[i] = (float)i*1.4f;\n+      }\n+      for (int i = 0; i < ITERS; i++) {\n+          test_round_float(iout , finp);\n+      }\n+  }\n+}\n","filename":"test\/hotspot\/jtreg\/compiler\/vectorization\/TestRoundVectAArch64.java","additions":94,"deletions":0,"binary":false,"changes":94,"status":"added"}]}