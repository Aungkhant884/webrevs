{"files":[{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -93,7 +93,2 @@\n-      oop o = *p;\n-      oop new_obj;\n-      if (o->is_forwarded()) {\n-        new_obj = o->forwardee();\n-      } else {\n-        new_obj = _pm->copy_to_survivor_space<\/*promote_immediately=*\/false>(o);\n-      }\n+      oop o = RawAccess<IS_NOT_NULL>::oop_load(p);\n+      oop new_obj = _pm->copy_to_survivor_space<\/*promote_immediately=*\/false>(o);\n","filename":"src\/hotspot\/share\/gc\/parallel\/psClosure.inline.hpp","additions":3,"deletions":8,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -113,0 +113,3 @@\n+  template<bool promote_immediately>\n+  oop copy_unmarked_to_survivor_space(oop o, markWord m);\n+\n","filename":"src\/hotspot\/share\/gc\/parallel\/psPromotionManager.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -41,0 +41,1 @@\n+#include \"runtime\/orderAccess.hpp\"\n@@ -129,0 +130,21 @@\n+\n+template<bool promote_immediately>\n+inline oop PSPromotionManager::copy_to_survivor_space(oop o) {\n+  assert(should_scavenge(&o), \"Sanity\");\n+\n+  \/\/ NOTE! We must be very careful with any methods that access the mark\n+  \/\/ in o. There may be multiple threads racing on it, and it may be forwarded\n+  \/\/ at any time.\n+  markWord m = o->mark();\n+  if (!m.is_marked()) {\n+    return copy_unmarked_to_survivor_space<promote_immediately>(o, m);\n+  } else {\n+    \/\/ Ensure any loads from the forwardee follow all changes that precede\n+    \/\/ the release-cmpxchg that performed the forwarding, possibly in some\n+    \/\/ other thread.\n+    OrderAccess::acquire();\n+    \/\/ Return the already installed forwardee.\n+    return cast_to_oop(m.decode_pointer());\n+  }\n+}\n+\n@@ -135,1 +157,2 @@\n-inline oop PSPromotionManager::copy_to_survivor_space(oop o) {\n+inline oop PSPromotionManager::copy_unmarked_to_survivor_space(oop o,\n+                                                               markWord test_mark) {\n@@ -139,0 +162,2 @@\n+  bool new_obj_is_tenured = false;\n+  size_t new_obj_size = o->size();\n@@ -140,12 +165,2 @@\n-  \/\/ NOTE! We must be very careful with any methods that access the mark\n-  \/\/ in o. There may be multiple threads racing on it, and it may be forwarded\n-  \/\/ at any time. Do not use oop methods for accessing the mark!\n-  markWord test_mark = o->mark();\n-\n-  \/\/ The same test as \"o->is_forwarded()\"\n-  if (!test_mark.is_marked()) {\n-    bool new_obj_is_tenured = false;\n-    size_t new_obj_size = o->size();\n-\n-    \/\/ Find the objects age, MT safe.\n-    uint age = (test_mark.has_displaced_mark_helper() \/* o->has_displaced_mark() *\/) ?\n+  \/\/ Find the objects age, MT safe.\n+  uint age = (test_mark.has_displaced_mark_helper() \/* o->has_displaced_mark() *\/) ?\n@@ -154,10 +169,20 @@\n-    if (!promote_immediately) {\n-      \/\/ Try allocating obj in to-space (unless too old)\n-      if (age < PSScavenge::tenuring_threshold()) {\n-        new_obj = cast_to_oop(_young_lab.allocate(new_obj_size));\n-        if (new_obj == NULL && !_young_gen_is_full) {\n-          \/\/ Do we allocate directly, or flush and refill?\n-          if (new_obj_size > (YoungPLABSize \/ 2)) {\n-            \/\/ Allocate this object directly\n-            new_obj = cast_to_oop(young_space()->cas_allocate(new_obj_size));\n-            promotion_trace_event(new_obj, o, new_obj_size, age, false, NULL);\n+  if (!promote_immediately) {\n+    \/\/ Try allocating obj in to-space (unless too old)\n+    if (age < PSScavenge::tenuring_threshold()) {\n+      new_obj = cast_to_oop(_young_lab.allocate(new_obj_size));\n+      if (new_obj == NULL && !_young_gen_is_full) {\n+        \/\/ Do we allocate directly, or flush and refill?\n+        if (new_obj_size > (YoungPLABSize \/ 2)) {\n+          \/\/ Allocate this object directly\n+          new_obj = cast_to_oop(young_space()->cas_allocate(new_obj_size));\n+          promotion_trace_event(new_obj, o, new_obj_size, age, false, NULL);\n+        } else {\n+          \/\/ Flush and fill\n+          _young_lab.flush();\n+\n+          HeapWord* lab_base = young_space()->cas_allocate(YoungPLABSize);\n+          if (lab_base != NULL) {\n+            _young_lab.initialize(MemRegion(lab_base, YoungPLABSize));\n+            \/\/ Try the young lab allocation again.\n+            new_obj = cast_to_oop(_young_lab.allocate(new_obj_size));\n+            promotion_trace_event(new_obj, o, new_obj_size, age, false, &_young_lab);\n@@ -165,12 +190,1 @@\n-            \/\/ Flush and fill\n-            _young_lab.flush();\n-\n-            HeapWord* lab_base = young_space()->cas_allocate(YoungPLABSize);\n-            if (lab_base != NULL) {\n-              _young_lab.initialize(MemRegion(lab_base, YoungPLABSize));\n-              \/\/ Try the young lab allocation again.\n-              new_obj = cast_to_oop(_young_lab.allocate(new_obj_size));\n-              promotion_trace_event(new_obj, o, new_obj_size, age, false, &_young_lab);\n-            } else {\n-              _young_gen_is_full = true;\n-            }\n+            _young_gen_is_full = true;\n@@ -181,0 +195,1 @@\n+  }\n@@ -182,2 +197,2 @@\n-    \/\/ Otherwise try allocating obj tenured\n-    if (new_obj == NULL) {\n+  \/\/ Otherwise try allocating obj tenured\n+  if (new_obj == NULL) {\n@@ -185,3 +200,3 @@\n-      if (ParallelScavengeHeap::heap()->promotion_should_fail()) {\n-        return oop_promotion_failed(o, test_mark);\n-      }\n+    if (ParallelScavengeHeap::heap()->promotion_should_fail()) {\n+      return oop_promotion_failed(o, test_mark);\n+    }\n@@ -190,13 +205,2 @@\n-      new_obj = cast_to_oop(_old_lab.allocate(new_obj_size));\n-      new_obj_is_tenured = true;\n-\n-      if (new_obj == NULL) {\n-        if (!_old_gen_is_full) {\n-          \/\/ Do we allocate directly, or flush and refill?\n-          if (new_obj_size > (OldPLABSize \/ 2)) {\n-            \/\/ Allocate this object directly\n-            new_obj = cast_to_oop(old_gen()->allocate(new_obj_size));\n-            promotion_trace_event(new_obj, o, new_obj_size, age, true, NULL);\n-          } else {\n-            \/\/ Flush and fill\n-            _old_lab.flush();\n+    new_obj = cast_to_oop(_old_lab.allocate(new_obj_size));\n+    new_obj_is_tenured = true;\n@@ -204,2 +208,13 @@\n-            HeapWord* lab_base = old_gen()->allocate(OldPLABSize);\n-            if(lab_base != NULL) {\n+    if (new_obj == NULL) {\n+      if (!_old_gen_is_full) {\n+        \/\/ Do we allocate directly, or flush and refill?\n+        if (new_obj_size > (OldPLABSize \/ 2)) {\n+          \/\/ Allocate this object directly\n+          new_obj = cast_to_oop(old_gen()->allocate(new_obj_size));\n+          promotion_trace_event(new_obj, o, new_obj_size, age, true, NULL);\n+        } else {\n+          \/\/ Flush and fill\n+          _old_lab.flush();\n+\n+          HeapWord* lab_base = old_gen()->allocate(OldPLABSize);\n+          if(lab_base != NULL) {\n@@ -207,10 +222,4 @@\n-              \/\/ Delay the initialization of the promotion lab (plab).\n-              \/\/ This exposes uninitialized plabs to card table processing.\n-              if (GCWorkerDelayMillis > 0) {\n-                os::naked_sleep(GCWorkerDelayMillis);\n-              }\n-#endif\n-              _old_lab.initialize(MemRegion(lab_base, OldPLABSize));\n-              \/\/ Try the old lab allocation again.\n-              new_obj = cast_to_oop(_old_lab.allocate(new_obj_size));\n-              promotion_trace_event(new_obj, o, new_obj_size, age, true, &_old_lab);\n+            \/\/ Delay the initialization of the promotion lab (plab).\n+            \/\/ This exposes uninitialized plabs to card table processing.\n+            if (GCWorkerDelayMillis > 0) {\n+              os::naked_sleep(GCWorkerDelayMillis);\n@@ -218,0 +227,5 @@\n+#endif\n+            _old_lab.initialize(MemRegion(lab_base, OldPLABSize));\n+            \/\/ Try the old lab allocation again.\n+            new_obj = cast_to_oop(_old_lab.allocate(new_obj_size));\n+            promotion_trace_event(new_obj, o, new_obj_size, age, true, &_old_lab);\n@@ -220,0 +234,1 @@\n+      }\n@@ -221,5 +236,5 @@\n-        \/\/ This is the promotion failed test, and code handling.\n-        \/\/ The code belongs here for two reasons. It is slightly\n-        \/\/ different than the code below, and cannot share the\n-        \/\/ CAS testing code. Keeping the code here also minimizes\n-        \/\/ the impact on the common case fast path code.\n+      \/\/ This is the promotion failed test, and code handling.\n+      \/\/ The code belongs here for two reasons. It is slightly\n+      \/\/ different than the code below, and cannot share the\n+      \/\/ CAS testing code. Keeping the code here also minimizes\n+      \/\/ the impact on the common case fast path code.\n@@ -227,4 +242,3 @@\n-        if (new_obj == NULL) {\n-          _old_gen_is_full = true;\n-          return oop_promotion_failed(o, test_mark);\n-        }\n+      if (new_obj == NULL) {\n+        _old_gen_is_full = true;\n+        return oop_promotion_failed(o, test_mark);\n@@ -233,0 +247,1 @@\n+  }\n@@ -234,1 +249,1 @@\n-    assert(new_obj != NULL, \"allocation should have succeeded\");\n+  assert(new_obj != NULL, \"allocation should have succeeded\");\n@@ -236,2 +251,2 @@\n-    \/\/ Copy obj\n-    Copy::aligned_disjoint_words(cast_from_oop<HeapWord*>(o), cast_from_oop<HeapWord*>(new_obj), new_obj_size);\n+  \/\/ Copy obj\n+  Copy::aligned_disjoint_words(cast_from_oop<HeapWord*>(o), cast_from_oop<HeapWord*>(new_obj), new_obj_size);\n@@ -239,5 +254,6 @@\n-    \/\/ Now we have to CAS in the header.\n-    \/\/ Make copy visible to threads reading the forwardee.\n-    if (o->cas_forward_to(new_obj, test_mark, memory_order_release)) {\n-      \/\/ We won any races, we \"own\" this object.\n-      assert(new_obj == o->forwardee(), \"Sanity\");\n+  \/\/ Now we have to CAS in the header.\n+  \/\/ Make copy visible to threads reading the forwardee.\n+  oop forwardee = o->forward_to_atomic(new_obj, test_mark, memory_order_release);\n+  if (forwardee == NULL) {  \/\/ forwardee is NULL when forwarding is successful\n+    \/\/ We won any races, we \"own\" this object.\n+    assert(new_obj == o->forwardee(), \"Sanity\");\n@@ -245,7 +261,7 @@\n-      \/\/ Increment age if obj still in new generation. Now that\n-      \/\/ we're dealing with a markWord that cannot change, it is\n-      \/\/ okay to use the non mt safe oop methods.\n-      if (!new_obj_is_tenured) {\n-        new_obj->incr_age();\n-        assert(young_space()->contains(new_obj), \"Attempt to push non-promoted obj\");\n-      }\n+    \/\/ Increment age if obj still in new generation. Now that\n+    \/\/ we're dealing with a markWord that cannot change, it is\n+    \/\/ okay to use the non mt safe oop methods.\n+    if (!new_obj_is_tenured) {\n+      new_obj->incr_age();\n+      assert(young_space()->contains(new_obj), \"Attempt to push non-promoted obj\");\n+    }\n@@ -253,26 +269,34 @@\n-      \/\/ Do the size comparison first with new_obj_size, which we\n-      \/\/ already have. Hopefully, only a few objects are larger than\n-      \/\/ _min_array_size_for_chunking, and most of them will be arrays.\n-      \/\/ So, the is->objArray() test would be very infrequent.\n-      if (new_obj_size > _min_array_size_for_chunking &&\n-          new_obj->is_objArray() &&\n-          PSChunkLargeArrays) {\n-        \/\/ we'll chunk it\n-        push_depth(ScannerTask(PartialArrayScanTask(o)));\n-        TASKQUEUE_STATS_ONLY(++_arrays_chunked; ++_array_chunk_pushes);\n-      } else {\n-        \/\/ we'll just push its contents\n-        push_contents(new_obj);\n-      }\n-    }  else {\n-      \/\/ We lost, someone else \"owns\" this object\n-      guarantee(o->is_forwarded(), \"Object must be forwarded if the cas failed.\");\n-\n-      \/\/ Try to deallocate the space.  If it was directly allocated we cannot\n-      \/\/ deallocate it, so we have to test.  If the deallocation fails,\n-      \/\/ overwrite with a filler object.\n-      if (new_obj_is_tenured) {\n-        if (!_old_lab.unallocate_object(cast_from_oop<HeapWord*>(new_obj), new_obj_size)) {\n-          CollectedHeap::fill_with_object(cast_from_oop<HeapWord*>(new_obj), new_obj_size);\n-        }\n-      } else if (!_young_lab.unallocate_object(cast_from_oop<HeapWord*>(new_obj), new_obj_size)) {\n+    log_develop_trace(gc, scavenge)(\"{%s %s \" PTR_FORMAT \" -> \" PTR_FORMAT \" (%d)}\",\n+                                    new_obj_is_tenured ? \"copying\" : \"tenuring\",\n+                                    new_obj->klass()->internal_name(),\n+                                    p2i((void *)o), p2i((void *)new_obj), new_obj->size());\n+\n+    \/\/ Do the size comparison first with new_obj_size, which we\n+    \/\/ already have. Hopefully, only a few objects are larger than\n+    \/\/ _min_array_size_for_chunking, and most of them will be arrays.\n+    \/\/ So, the is->objArray() test would be very infrequent.\n+    if (new_obj_size > _min_array_size_for_chunking &&\n+        new_obj->is_objArray() &&\n+        PSChunkLargeArrays) {\n+      \/\/ we'll chunk it\n+      push_depth(ScannerTask(PartialArrayScanTask(o)));\n+      TASKQUEUE_STATS_ONLY(++_arrays_chunked; ++_array_chunk_pushes);\n+    } else {\n+      \/\/ we'll just push its contents\n+      push_contents(new_obj);\n+    }\n+    return new_obj;\n+  } else {\n+    \/\/ We lost, someone else \"owns\" this object.\n+    \/\/ Ensure loads from the forwardee follow all changes that preceeded the\n+    \/\/ release-cmpxchg that performed the forwarding in another thread.\n+    OrderAccess::acquire();\n+\n+    assert(o->is_forwarded(), \"Object must be forwarded if the cas failed.\");\n+    assert(o->forwardee() == forwardee, \"invariant\");\n+\n+    \/\/ Try to deallocate the space.  If it was directly allocated we cannot\n+    \/\/ deallocate it, so we have to test.  If the deallocation fails,\n+    \/\/ overwrite with a filler object.\n+    if (new_obj_is_tenured) {\n+      if (!_old_lab.unallocate_object(cast_from_oop<HeapWord*>(new_obj), new_obj_size)) {\n@@ -281,4 +305,2 @@\n-\n-      \/\/ don't update this before the unallocation!\n-      \/\/ Using acquire though consume would be accurate for accessing new_obj.\n-      new_obj = o->forwardee_acquire();\n+    } else if (!_young_lab.unallocate_object(cast_from_oop<HeapWord*>(new_obj), new_obj_size)) {\n+      CollectedHeap::fill_with_object(cast_from_oop<HeapWord*>(new_obj), new_obj_size);\n@@ -286,3 +308,1 @@\n-  } else {\n-    assert(o->is_forwarded(), \"Sanity\");\n-    new_obj = o->forwardee_acquire();\n+    return forwardee;\n@@ -290,8 +310,0 @@\n-\n-  \/\/ This code must come after the CAS test, or it will print incorrect\n-  \/\/ information.\n-  log_develop_trace(gc, scavenge)(\"{%s %s \" PTR_FORMAT \" -> \" PTR_FORMAT \" (%d)}\",\n-                                  should_scavenge(&new_obj) ? \"copying\" : \"tenuring\",\n-                                  new_obj->klass()->internal_name(), p2i((void *)o), p2i((void *)new_obj), new_obj->size());\n-\n-  return new_obj;\n@@ -308,12 +320,1 @@\n-  oop new_obj = o->is_forwarded()\n-        ? o->forwardee()\n-        : copy_to_survivor_space<promote_immediately>(o);\n-\n-  \/\/ This code must come after the CAS test, or it will print incorrect\n-  \/\/ information.\n-  if (log_develop_is_enabled(Trace, gc, scavenge) && o->is_forwarded()) {\n-    log_develop_trace(gc, scavenge)(\"{%s %s \" PTR_FORMAT \" -> \" PTR_FORMAT \" (%d)}\",\n-                      \"forwarding\",\n-                      new_obj->klass()->internal_name(), p2i((void *)o), p2i((void *)new_obj), new_obj->size());\n-  }\n-\n+  oop new_obj = copy_to_survivor_space<promote_immediately>(o);\n","filename":"src\/hotspot\/share\/gc\/parallel\/psPromotionManager.inline.hpp","additions":143,"deletions":142,"binary":false,"changes":285,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2002, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2002, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -136,2 +136,0 @@\n-  static void copy_and_push_safe_barrier_from_klass(PSPromotionManager* pm, oop* p);\n-\n","filename":"src\/hotspot\/share\/gc\/parallel\/psScavenge.hpp","additions":1,"deletions":3,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -260,1 +260,0 @@\n-  inline oop forwardee_acquire() const;\n","filename":"src\/hotspot\/share\/oops\/oop.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -308,7 +308,0 @@\n-\/\/ Note that the forwardee is not the same thing as the displaced_mark.\n-\/\/ The forwardee is used when copying during scavenge and mark-sweep.\n-\/\/ It does need to clear the low two locking- and GC-related bits.\n-oop oopDesc::forwardee_acquire() const {\n-  return cast_to_oop(Atomic::load_acquire(&_mark).decode_pointer());\n-}\n-\n","filename":"src\/hotspot\/share\/oops\/oop.inline.hpp","additions":0,"deletions":7,"binary":false,"changes":7,"status":"modified"}]}