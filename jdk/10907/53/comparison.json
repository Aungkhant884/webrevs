{"files":[{"patch":"@@ -3825,1 +3825,1 @@\n-    Label no_count;\n+    Label count, no_count;\n@@ -3843,27 +3843,33 @@\n-      \/\/ Set tmp to be (markWord of object | UNLOCK_VALUE).\n-      __ orr(tmp, disp_hdr, markWord::unlocked_value);\n-\n-      \/\/ Initialize the box. (Must happen before we update the object mark!)\n-      __ str(tmp, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n-\n-      \/\/ Compare object markWord with an unlocked value (tmp) and if\n-      \/\/ equal exchange the stack address of our box with object markWord.\n-      \/\/ On failure disp_hdr contains the possibly locked markWord.\n-      __ cmpxchg(oop, tmp, box, Assembler::xword, \/*acquire*\/ true,\n-                 \/*release*\/ true, \/*weak*\/ false, disp_hdr);\n-      __ br(Assembler::EQ, cont);\n-\n-      assert(oopDesc::mark_offset_in_bytes() == 0, \"offset of _mark is not 0\");\n-\n-      \/\/ If the compare-and-exchange succeeded, then we found an unlocked\n-      \/\/ object, will have now locked it will continue at label cont\n-\n-      \/\/ Check if the owner is self by comparing the value in the\n-      \/\/ markWord of object (disp_hdr) with the stack pointer.\n-      __ mov(rscratch1, sp);\n-      __ sub(disp_hdr, disp_hdr, rscratch1);\n-      __ mov(tmp, (address) (~(os::vm_page_size()-1) | markWord::lock_mask_in_place));\n-      \/\/ If condition is true we are cont and hence we can store 0 as the\n-      \/\/ displaced header in the box, which indicates that it is a recursive lock.\n-      __ ands(tmp\/*==0?*\/, disp_hdr, tmp);   \/\/ Sets flags for result\n-      __ str(tmp\/*==0, perhaps*\/, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n+      if (LockingMode == LIGHTWEIGHT) {\n+        __ fast_lock(oop, disp_hdr, tmp, rscratch1, no_count);\n+        __ b(count);\n+      } else if (LockingMode == LEGACY) {\n+        \/\/ Set tmp to be (markWord of object | UNLOCK_VALUE).\n+        __ orr(tmp, disp_hdr, markWord::unlocked_value);\n+\n+        \/\/ Initialize the box. (Must happen before we update the object mark!)\n+        __ str(tmp, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n+\n+        \/\/ Compare object markWord with an unlocked value (tmp) and if\n+        \/\/ equal exchange the stack address of our box with object markWord.\n+        \/\/ On failure disp_hdr contains the possibly locked markWord.\n+        __ cmpxchg(oop, tmp, box, Assembler::xword, \/*acquire*\/ true,\n+                   \/*release*\/ true, \/*weak*\/ false, disp_hdr);\n+        __ br(Assembler::EQ, cont);\n+\n+        assert(oopDesc::mark_offset_in_bytes() == 0, \"offset of _mark is not 0\");\n+\n+        \/\/ If the compare-and-exchange succeeded, then we found an unlocked\n+        \/\/ object, will have now locked it will continue at label cont\n+\n+        \/\/ Check if the owner is self by comparing the value in the\n+        \/\/ markWord of object (disp_hdr) with the stack pointer.\n+        __ mov(rscratch1, sp);\n+        __ sub(disp_hdr, disp_hdr, rscratch1);\n+        __ mov(tmp, (address) (~(os::vm_page_size()-1) | markWord::lock_mask_in_place));\n+        \/\/ If condition is true we are cont and hence we can store 0 as the\n+        \/\/ displaced header in the box, which indicates that it is a recursive lock.\n+        __ ands(tmp\/*==0?*\/, disp_hdr, tmp);   \/\/ Sets flags for result\n+        __ str(tmp\/*==0, perhaps*\/, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n+        __ b(cont);\n+      }\n@@ -3872,0 +3878,1 @@\n+      __ b(cont);\n@@ -3873,1 +3880,0 @@\n-    __ b(cont);\n@@ -3886,7 +3892,8 @@\n-    \/\/ Store a non-null value into the box to avoid looking like a re-entrant\n-    \/\/ lock. The fast-path monitor unlock code checks for\n-    \/\/ markWord::monitor_value so use markWord::unused_mark which has the\n-    \/\/ relevant bit set, and also matches ObjectSynchronizer::enter.\n-    __ mov(tmp, (address)markWord::unused_mark().value());\n-    __ str(tmp, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n-\n+    if (LockingMode != LIGHTWEIGHT) {\n+      \/\/ Store a non-null value into the box to avoid looking like a re-entrant\n+      \/\/ lock. The fast-path monitor unlock code checks for\n+      \/\/ markWord::monitor_value so use markWord::unused_mark which has the\n+      \/\/ relevant bit set, and also matches ObjectSynchronizer::enter.\n+      __ mov(tmp, (address)markWord::unused_mark().value());\n+      __ str(tmp, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n+    }\n@@ -3907,0 +3914,1 @@\n+    __ bind(count);\n@@ -3920,1 +3928,1 @@\n-    Label no_count;\n+    Label count, no_count;\n@@ -3924,1 +3932,1 @@\n-    if (!UseHeavyMonitors) {\n+    if (LockingMode == LEGACY) {\n@@ -3938,6 +3946,12 @@\n-      \/\/ Check if it is still a light weight lock, this is is true if we\n-      \/\/ see the stack address of the basicLock in the markWord of the\n-      \/\/ object.\n-\n-      __ cmpxchg(oop, box, disp_hdr, Assembler::xword, \/*acquire*\/ false,\n-                 \/*release*\/ true, \/*weak*\/ false, tmp);\n+      if (LockingMode == LIGHTWEIGHT) {\n+        __ fast_unlock(oop, tmp, box, disp_hdr, no_count);\n+        __ b(count);\n+      } else if (LockingMode == LEGACY) {\n+        \/\/ Check if it is still a light weight lock, this is is true if we\n+        \/\/ see the stack address of the basicLock in the markWord of the\n+        \/\/ object.\n+\n+        __ cmpxchg(oop, box, disp_hdr, Assembler::xword, \/*acquire*\/ false,\n+                   \/*release*\/ true, \/*weak*\/ false, tmp);\n+        __ b(cont);\n+      }\n@@ -3946,0 +3960,1 @@\n+      __ b(cont);\n@@ -3947,1 +3962,0 @@\n-    __ b(cont);\n@@ -3955,0 +3969,14 @@\n+\n+    if (LockingMode == LIGHTWEIGHT) {\n+      \/\/ If the owner is anonymous, we need to fix it -- in an outline stub.\n+      Register tmp2 = disp_hdr;\n+      __ ldr(tmp2, Address(tmp, ObjectMonitor::owner_offset_in_bytes()));\n+      \/\/ We cannot use tbnz here, the target might be too far away and cannot\n+      \/\/ be encoded.\n+      __ tst(tmp2, (uint64_t)ObjectMonitor::ANONYMOUS_OWNER);\n+      C2HandleAnonOMOwnerStub* stub = new (Compile::current()->comp_arena()) C2HandleAnonOMOwnerStub(tmp, tmp2);\n+      Compile::current()->output()->add_stub(stub);\n+      __ br(Assembler::NE, stub->entry());\n+      __ bind(stub->continuation());\n+    }\n+\n@@ -3981,0 +4009,1 @@\n+    __ bind(count);\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":74,"deletions":45,"binary":false,"changes":119,"status":"modified"},{"patch":"@@ -66,2 +66,1 @@\n-  assert(hdr != obj && hdr != disp_hdr && obj != disp_hdr, \"registers must be different\");\n-  Label done;\n+  assert_different_registers(hdr, obj, disp_hdr);\n@@ -86,33 +85,38 @@\n-  \/\/ and mark it as unlocked\n-  orr(hdr, hdr, markWord::unlocked_value);\n-  \/\/ save unlocked object header into the displaced header location on the stack\n-  str(hdr, Address(disp_hdr, 0));\n-  \/\/ test if object header is still the same (i.e. unlocked), and if so, store the\n-  \/\/ displaced header address in the object header - if it is not the same, get the\n-  \/\/ object header instead\n-  lea(rscratch2, Address(obj, hdr_offset));\n-  cmpxchgptr(hdr, disp_hdr, rscratch2, rscratch1, done, \/*fallthough*\/NULL);\n-  \/\/ if the object header was the same, we're done\n-  \/\/ if the object header was not the same, it is now in the hdr register\n-  \/\/ => test if it is a stack pointer into the same stack (recursive locking), i.e.:\n-  \/\/\n-  \/\/ 1) (hdr & aligned_mask) == 0\n-  \/\/ 2) sp <= hdr\n-  \/\/ 3) hdr <= sp + page_size\n-  \/\/\n-  \/\/ these 3 tests can be done by evaluating the following expression:\n-  \/\/\n-  \/\/ (hdr - sp) & (aligned_mask - page_size)\n-  \/\/\n-  \/\/ assuming both the stack pointer and page_size have their least\n-  \/\/ significant 2 bits cleared and page_size is a power of 2\n-  mov(rscratch1, sp);\n-  sub(hdr, hdr, rscratch1);\n-  ands(hdr, hdr, aligned_mask - (int)os::vm_page_size());\n-  \/\/ for recursive locking, the result is zero => save it in the displaced header\n-  \/\/ location (NULL in the displaced hdr location indicates recursive locking)\n-  str(hdr, Address(disp_hdr, 0));\n-  \/\/ otherwise we don't care about the result and handle locking via runtime call\n-  cbnz(hdr, slow_case);\n-  \/\/ done\n-  bind(done);\n+  if (LockingMode == LIGHTWEIGHT) {\n+    fast_lock(obj, hdr, rscratch1, rscratch2, slow_case);\n+  } else if (LockingMode == LEGACY) {\n+    Label done;\n+    \/\/ and mark it as unlocked\n+    orr(hdr, hdr, markWord::unlocked_value);\n+    \/\/ save unlocked object header into the displaced header location on the stack\n+    str(hdr, Address(disp_hdr, 0));\n+    \/\/ test if object header is still the same (i.e. unlocked), and if so, store the\n+    \/\/ displaced header address in the object header - if it is not the same, get the\n+    \/\/ object header instead\n+    lea(rscratch2, Address(obj, hdr_offset));\n+    cmpxchgptr(hdr, disp_hdr, rscratch2, rscratch1, done, \/*fallthough*\/nullptr);\n+    \/\/ if the object header was the same, we're done\n+    \/\/ if the object header was not the same, it is now in the hdr register\n+    \/\/ => test if it is a stack pointer into the same stack (recursive locking), i.e.:\n+    \/\/\n+    \/\/ 1) (hdr & aligned_mask) == 0\n+    \/\/ 2) sp <= hdr\n+    \/\/ 3) hdr <= sp + page_size\n+    \/\/\n+    \/\/ these 3 tests can be done by evaluating the following expression:\n+    \/\/\n+    \/\/ (hdr - sp) & (aligned_mask - page_size)\n+    \/\/\n+    \/\/ assuming both the stack pointer and page_size have their least\n+    \/\/ significant 2 bits cleared and page_size is a power of 2\n+    mov(rscratch1, sp);\n+    sub(hdr, hdr, rscratch1);\n+    ands(hdr, hdr, aligned_mask - (int)os::vm_page_size());\n+    \/\/ for recursive locking, the result is zero => save it in the displaced header\n+    \/\/ location (null in the displaced hdr location indicates recursive locking)\n+    str(hdr, Address(disp_hdr, 0));\n+    \/\/ otherwise we don't care about the result and handle locking via runtime call\n+    cbnz(hdr, slow_case);\n+    \/\/ done\n+    bind(done);\n+  }\n@@ -130,5 +134,8 @@\n-  \/\/ load displaced header\n-  ldr(hdr, Address(disp_hdr, 0));\n-  \/\/ if the loaded hdr is NULL we had recursive locking\n-  \/\/ if we had recursive locking, we are done\n-  cbz(hdr, done);\n+  if (LockingMode != LIGHTWEIGHT) {\n+    \/\/ load displaced header\n+    ldr(hdr, Address(disp_hdr, 0));\n+    \/\/ if the loaded hdr is null we had recursive locking\n+    \/\/ if we had recursive locking, we are done\n+    cbz(hdr, done);\n+  }\n+\n@@ -138,10 +145,22 @@\n-  \/\/ test if object header is pointing to the displaced header, and if so, restore\n-  \/\/ the displaced header in the object - if the object header is not pointing to\n-  \/\/ the displaced header, get the object header instead\n-  \/\/ if the object header was not pointing to the displaced header,\n-  \/\/ we do unlocking via runtime call\n-  if (hdr_offset) {\n-    lea(rscratch1, Address(obj, hdr_offset));\n-    cmpxchgptr(disp_hdr, hdr, rscratch1, rscratch2, done, &slow_case);\n-  } else {\n-    cmpxchgptr(disp_hdr, hdr, obj, rscratch2, done, &slow_case);\n+\n+  if (LockingMode == LIGHTWEIGHT) {\n+    ldr(hdr, Address(obj, oopDesc::mark_offset_in_bytes()));\n+    \/\/ We cannot use tbnz here, the target might be too far away and cannot\n+    \/\/ be encoded.\n+    tst(hdr, markWord::monitor_value);\n+    br(Assembler::NE, slow_case);\n+    fast_unlock(obj, hdr, rscratch1, rscratch2, slow_case);\n+  } else if (LockingMode == LEGACY) {\n+    \/\/ test if object header is pointing to the displaced header, and if so, restore\n+    \/\/ the displaced header in the object - if the object header is not pointing to\n+    \/\/ the displaced header, get the object header instead\n+    \/\/ if the object header was not pointing to the displaced header,\n+    \/\/ we do unlocking via runtime call\n+    if (hdr_offset) {\n+      lea(rscratch1, Address(obj, hdr_offset));\n+      cmpxchgptr(disp_hdr, hdr, rscratch1, rscratch2, done, &slow_case);\n+    } else {\n+      cmpxchgptr(disp_hdr, hdr, obj, rscratch2, done, &slow_case);\n+    }\n+    \/\/ done\n+    bind(done);\n@@ -149,2 +168,0 @@\n-  \/\/ done\n-  bind(done);\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_MacroAssembler_aarch64.cpp","additions":69,"deletions":52,"binary":false,"changes":121,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"runtime\/objectMonitor.hpp\"\n@@ -66,0 +67,27 @@\n+int C2HandleAnonOMOwnerStub::max_size() const {\n+  \/\/ Max size of stub has been determined by testing with 0, in which case\n+  \/\/ C2CodeStubList::emit() will throw an assertion and report the actual size that\n+  \/\/ is needed.\n+  return 24;\n+}\n+\n+void C2HandleAnonOMOwnerStub::emit(C2_MacroAssembler& masm) {\n+  __ bind(entry());\n+  Register mon = monitor();\n+  Register t = tmp();\n+  assert(t != noreg, \"need tmp register\");\n+\n+  \/\/ Fix owner to be the current thread.\n+  __ str(rthread, Address(mon, ObjectMonitor::owner_offset_in_bytes()));\n+\n+  \/\/ Pop owner object from lock-stack.\n+  __ ldrw(t, Address(rthread, JavaThread::lock_stack_top_offset()));\n+  __ subw(t, t, oopSize);\n+#ifdef ASSERT\n+  __ str(zr, Address(rthread, t));\n+#endif\n+  __ strw(t, Address(rthread, JavaThread::lock_stack_top_offset()));\n+\n+  __ b(continuation());\n+}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/c2_CodeStubs_aarch64.cpp","additions":28,"deletions":0,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -761,50 +761,55 @@\n-    \/\/ Load (object->mark() | 1) into swap_reg\n-    ldr(rscratch1, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-    orr(swap_reg, rscratch1, 1);\n-\n-    \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n-    str(swap_reg, Address(lock_reg, mark_offset));\n-\n-    assert(lock_offset == 0,\n-           \"displached header must be first word in BasicObjectLock\");\n-\n-    Label fail;\n-    cmpxchg_obj_header(swap_reg, lock_reg, obj_reg, rscratch1, count, \/*fallthrough*\/NULL);\n-\n-    \/\/ Fast check for recursive lock.\n-    \/\/\n-    \/\/ Can apply the optimization only if this is a stack lock\n-    \/\/ allocated in this thread. For efficiency, we can focus on\n-    \/\/ recently allocated stack locks (instead of reading the stack\n-    \/\/ base and checking whether 'mark' points inside the current\n-    \/\/ thread stack):\n-    \/\/  1) (mark & 7) == 0, and\n-    \/\/  2) sp <= mark < mark + os::pagesize()\n-    \/\/\n-    \/\/ Warning: sp + os::pagesize can overflow the stack base. We must\n-    \/\/ neither apply the optimization for an inflated lock allocated\n-    \/\/ just above the thread stack (this is why condition 1 matters)\n-    \/\/ nor apply the optimization if the stack lock is inside the stack\n-    \/\/ of another thread. The latter is avoided even in case of overflow\n-    \/\/ because we have guard pages at the end of all stacks. Hence, if\n-    \/\/ we go over the stack base and hit the stack of another thread,\n-    \/\/ this should not be in a writeable area that could contain a\n-    \/\/ stack lock allocated by that thread. As a consequence, a stack\n-    \/\/ lock less than page size away from sp is guaranteed to be\n-    \/\/ owned by the current thread.\n-    \/\/\n-    \/\/ These 3 tests can be done by evaluating the following\n-    \/\/ expression: ((mark - sp) & (7 - os::vm_page_size())),\n-    \/\/ assuming both stack pointer and pagesize have their\n-    \/\/ least significant 3 bits clear.\n-    \/\/ NOTE: the mark is in swap_reg %r0 as the result of cmpxchg\n-    \/\/ NOTE2: aarch64 does not like to subtract sp from rn so take a\n-    \/\/ copy\n-    mov(rscratch1, sp);\n-    sub(swap_reg, swap_reg, rscratch1);\n-    ands(swap_reg, swap_reg, (uint64_t)(7 - (int)os::vm_page_size()));\n-\n-    \/\/ Save the test result, for recursive case, the result is zero\n-    str(swap_reg, Address(lock_reg, mark_offset));\n-    br(Assembler::EQ, count);\n-\n+    if (LockingMode == LIGHTWEIGHT) {\n+      ldr(tmp, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      fast_lock(obj_reg, tmp, rscratch1, rscratch2, slow_case);\n+      b(count);\n+    } else if (LockingMode == LEGACY) {\n+      \/\/ Load (object->mark() | 1) into swap_reg\n+      ldr(rscratch1, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      orr(swap_reg, rscratch1, 1);\n+\n+      \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n+      str(swap_reg, Address(lock_reg, mark_offset));\n+\n+      assert(lock_offset == 0,\n+             \"displached header must be first word in BasicObjectLock\");\n+\n+      Label fail;\n+      cmpxchg_obj_header(swap_reg, lock_reg, obj_reg, rscratch1, count, \/*fallthrough*\/nullptr);\n+\n+      \/\/ Fast check for recursive lock.\n+      \/\/\n+      \/\/ Can apply the optimization only if this is a stack lock\n+      \/\/ allocated in this thread. For efficiency, we can focus on\n+      \/\/ recently allocated stack locks (instead of reading the stack\n+      \/\/ base and checking whether 'mark' points inside the current\n+      \/\/ thread stack):\n+      \/\/  1) (mark & 7) == 0, and\n+      \/\/  2) sp <= mark < mark + os::pagesize()\n+      \/\/\n+      \/\/ Warning: sp + os::pagesize can overflow the stack base. We must\n+      \/\/ neither apply the optimization for an inflated lock allocated\n+      \/\/ just above the thread stack (this is why condition 1 matters)\n+      \/\/ nor apply the optimization if the stack lock is inside the stack\n+      \/\/ of another thread. The latter is avoided even in case of overflow\n+      \/\/ because we have guard pages at the end of all stacks. Hence, if\n+      \/\/ we go over the stack base and hit the stack of another thread,\n+      \/\/ this should not be in a writeable area that could contain a\n+      \/\/ stack lock allocated by that thread. As a consequence, a stack\n+      \/\/ lock less than page size away from sp is guaranteed to be\n+      \/\/ owned by the current thread.\n+      \/\/\n+      \/\/ These 3 tests can be done by evaluating the following\n+      \/\/ expression: ((mark - sp) & (7 - os::vm_page_size())),\n+      \/\/ assuming both stack pointer and pagesize have their\n+      \/\/ least significant 3 bits clear.\n+      \/\/ NOTE: the mark is in swap_reg %r0 as the result of cmpxchg\n+      \/\/ NOTE2: aarch64 does not like to subtract sp from rn so take a\n+      \/\/ copy\n+      mov(rscratch1, sp);\n+      sub(swap_reg, swap_reg, rscratch1);\n+      ands(swap_reg, swap_reg, (uint64_t)(7 - (int)os::vm_page_size()));\n+\n+      \/\/ Save the test result, for recursive case, the result is zero\n+      str(swap_reg, Address(lock_reg, mark_offset));\n+      br(Assembler::EQ, count);\n+    }\n@@ -814,3 +819,9 @@\n-    call_VM(noreg,\n-            CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter),\n-            lock_reg);\n+    if (LockingMode == LIGHTWEIGHT) {\n+      call_VM(noreg,\n+              CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter_obj),\n+              obj_reg);\n+    } else {\n+      call_VM(noreg,\n+              CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter),\n+              lock_reg);\n+    }\n@@ -853,3 +864,5 @@\n-    \/\/ Convert from BasicObjectLock structure to object and BasicLock\n-    \/\/ structure Store the BasicLock address into %r0\n-    lea(swap_reg, Address(lock_reg, BasicObjectLock::lock_offset_in_bytes()));\n+    if (LockingMode != LIGHTWEIGHT) {\n+      \/\/ Convert from BasicObjectLock structure to object and BasicLock\n+      \/\/ structure Store the BasicLock address into %r0\n+      lea(swap_reg, Address(lock_reg, BasicObjectLock::lock_offset_in_bytes()));\n+    }\n@@ -863,9 +876,15 @@\n-    \/\/ Load the old header from BasicLock structure\n-    ldr(header_reg, Address(swap_reg,\n-                            BasicLock::displaced_header_offset_in_bytes()));\n-\n-    \/\/ Test for recursion\n-    cbz(header_reg, count);\n-\n-    \/\/ Atomic swap back the old header\n-    cmpxchg_obj_header(swap_reg, header_reg, obj_reg, rscratch1, count, \/*fallthrough*\/NULL);\n+    if (LockingMode == LIGHTWEIGHT) {\n+      Label slow_case;\n+\n+      \/\/ Check for non-symmetric locking. This is allowed by the spec and the interpreter\n+      \/\/ must handle it.\n+      Register tmp = rscratch1;\n+      \/\/ First check for lock-stack underflow.\n+      ldrw(tmp, Address(rthread, JavaThread::lock_stack_top_offset()));\n+      cmpw(tmp, (unsigned)LockStack::start_offset());\n+      br(Assembler::LE, slow_case);\n+      \/\/ Then check if the top of the lock-stack matches the unlocked object.\n+      subw(tmp, tmp, oopSize);\n+      ldr(tmp, Address(rthread, tmp));\n+      cmpoop(tmp, obj_reg);\n+      br(Assembler::NE, slow_case);\n@@ -873,0 +892,16 @@\n+      ldr(header_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      tbnz(header_reg, exact_log2(markWord::monitor_value), slow_case);\n+      fast_unlock(obj_reg, header_reg, swap_reg, rscratch1, slow_case);\n+      b(count);\n+      bind(slow_case);\n+    } else if (LockingMode == LEGACY) {\n+      \/\/ Load the old header from BasicLock structure\n+      ldr(header_reg, Address(swap_reg,\n+                              BasicLock::displaced_header_offset_in_bytes()));\n+\n+      \/\/ Test for recursion\n+      cbz(header_reg, count);\n+\n+      \/\/ Atomic swap back the old header\n+      cmpxchg_obj_header(swap_reg, header_reg, obj_reg, rscratch1, count, \/*fallthrough*\/nullptr);\n+    }\n","filename":"src\/hotspot\/cpu\/aarch64\/interp_masm_aarch64.cpp","additions":100,"deletions":65,"binary":false,"changes":165,"status":"modified"},{"patch":"@@ -6214,0 +6214,94 @@\n+\n+\/\/ Implements fast-locking.\n+\/\/ Branches to slow upon failure to lock the object, with ZF cleared.\n+\/\/ Falls through upon success with ZF set.\n+\/\/\n+\/\/  - obj: the object to be locked\n+\/\/  - hdr: the header, already loaded from obj, will be destroyed\n+\/\/  - t1, t2: temporary registers, will be destroyed\n+void MacroAssembler::fast_lock(Register obj, Register hdr, Register t1, Register t2, Label& slow) {\n+  assert(LockingMode == LIGHTWEIGHT, \"only used with new lightweight locking\");\n+  assert_different_registers(obj, hdr, t1, t2);\n+\n+  \/\/ Check if we would have space on lock-stack for the object.\n+  ldrw(t1, Address(rthread, JavaThread::lock_stack_top_offset()));\n+  cmpw(t1, (unsigned)LockStack::end_offset() - 1);\n+  br(Assembler::GT, slow);\n+\n+  \/\/ Load (object->mark() | 1) into hdr\n+  orr(hdr, hdr, markWord::unlocked_value);\n+  \/\/ Clear lock-bits, into t2\n+  eor(t2, hdr, markWord::unlocked_value);\n+  \/\/ Try to swing header from unlocked to locked\n+  cmpxchg(\/*addr*\/ obj, \/*expected*\/ hdr, \/*new*\/ t2, Assembler::xword,\n+          \/*acquire*\/ true, \/*release*\/ true, \/*weak*\/ false, t1);\n+  br(Assembler::NE, slow);\n+\n+  \/\/ After successful lock, push object on lock-stack\n+  ldrw(t1, Address(rthread, JavaThread::lock_stack_top_offset()));\n+  str(obj, Address(rthread, t1));\n+  addw(t1, t1, oopSize);\n+  strw(t1, Address(rthread, JavaThread::lock_stack_top_offset()));\n+}\n+\n+\/\/ Implements fast-unlocking.\n+\/\/ Branches to slow upon failure, with ZF cleared.\n+\/\/ Falls through upon success, with ZF set.\n+\/\/\n+\/\/ - obj: the object to be unlocked\n+\/\/ - hdr: the (pre-loaded) header of the object\n+\/\/ - t1, t2: temporary registers\n+void MacroAssembler::fast_unlock(Register obj, Register hdr, Register t1, Register t2, Label& slow) {\n+  assert(LockingMode == LIGHTWEIGHT, \"only used with new lightweight locking\");\n+  assert_different_registers(obj, hdr, t1, t2);\n+\n+#ifdef ASSERT\n+  {\n+    \/\/ The following checks rely on the fact that LockStack is only ever modified by\n+    \/\/ its owning stack, even if the lock got inflated concurrently; removal of LockStack\n+    \/\/ entries after inflation will happen delayed in that case.\n+\n+    \/\/ Check for lock-stack underflow.\n+    Label stack_ok;\n+    ldrw(t1, Address(rthread, JavaThread::lock_stack_top_offset()));\n+    cmpw(t1, (unsigned)LockStack::start_offset());\n+    br(Assembler::GT, stack_ok);\n+    STOP(\"Lock-stack underflow\");\n+    bind(stack_ok);\n+  }\n+  {\n+    \/\/ Check if the top of the lock-stack matches the unlocked object.\n+    Label tos_ok;\n+    subw(t1, t1, oopSize);\n+    ldr(t1, Address(rthread, t1));\n+    cmpoop(t1, obj);\n+    br(Assembler::EQ, tos_ok);\n+    STOP(\"Top of lock-stack does not match the unlocked object\");\n+    bind(tos_ok);\n+  }\n+  {\n+    \/\/ Check that hdr is fast-locked.\n+    Label hdr_ok;\n+    tst(hdr, markWord::lock_mask_in_place);\n+    br(Assembler::EQ, hdr_ok);\n+    STOP(\"Header is not fast-locked\");\n+    bind(hdr_ok);\n+  }\n+#endif\n+\n+  \/\/ Load the new header (unlocked) into t1\n+  orr(t1, hdr, markWord::unlocked_value);\n+\n+  \/\/ Try to swing header from locked to unlocked\n+  cmpxchg(obj, hdr, t1, Assembler::xword,\n+          \/*acquire*\/ true, \/*release*\/ true, \/*weak*\/ false, t2);\n+  br(Assembler::NE, slow);\n+\n+  \/\/ After successful unlock, pop object from lock-stack\n+  ldrw(t1, Address(rthread, JavaThread::lock_stack_top_offset()));\n+  subw(t1, t1, oopSize);\n+#ifdef ASSERT\n+  str(zr, Address(rthread, t1));\n+#endif\n+  strw(t1, Address(rthread, JavaThread::lock_stack_top_offset()));\n+}\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":94,"deletions":0,"binary":false,"changes":94,"status":"modified"},{"patch":"@@ -1584,0 +1584,3 @@\n+  void fast_lock(Register obj, Register hdr, Register t1, Register t2, Label& slow);\n+  void fast_unlock(Register obj, Register hdr, Register t1, Register t2, Label& slow);\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -1782,28 +1782,33 @@\n-      \/\/ Load (object->mark() | 1) into swap_reg %r0\n-      __ ldr(rscratch1, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-      __ orr(swap_reg, rscratch1, 1);\n-\n-      \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n-      __ str(swap_reg, Address(lock_reg, mark_word_offset));\n-\n-      \/\/ src -> dest iff dest == r0 else r0 <- dest\n-      __ cmpxchg_obj_header(r0, lock_reg, obj_reg, rscratch1, count, \/*fallthrough*\/NULL);\n-\n-      \/\/ Hmm should this move to the slow path code area???\n-\n-      \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n-      \/\/  1) (mark & 3) == 0, and\n-      \/\/  2) sp <= mark < mark + os::pagesize()\n-      \/\/ These 3 tests can be done by evaluating the following\n-      \/\/ expression: ((mark - sp) & (3 - os::vm_page_size())),\n-      \/\/ assuming both stack pointer and pagesize have their\n-      \/\/ least significant 2 bits clear.\n-      \/\/ NOTE: the oopMark is in swap_reg %r0 as the result of cmpxchg\n-\n-      __ sub(swap_reg, sp, swap_reg);\n-      __ neg(swap_reg, swap_reg);\n-      __ ands(swap_reg, swap_reg, 3 - (int)os::vm_page_size());\n-\n-      \/\/ Save the test result, for recursive case, the result is zero\n-      __ str(swap_reg, Address(lock_reg, mark_word_offset));\n-      __ br(Assembler::NE, slow_path_lock);\n+      if (LockingMode == LIGHTWEIGHT) {\n+        __ ldr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+        __ fast_lock(obj_reg, swap_reg, tmp, rscratch1, slow_path_lock);\n+      } else if (LockingMode == LEGACY) {\n+        \/\/ Load (object->mark() | 1) into swap_reg %r0\n+        __ ldr(rscratch1, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+        __ orr(swap_reg, rscratch1, 1);\n+\n+        \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n+        __ str(swap_reg, Address(lock_reg, mark_word_offset));\n+\n+        \/\/ src -> dest iff dest == r0 else r0 <- dest\n+        __ cmpxchg_obj_header(r0, lock_reg, obj_reg, rscratch1, count, \/*fallthrough*\/nullptr);\n+\n+        \/\/ Hmm should this move to the slow path code area???\n+\n+        \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n+        \/\/  1) (mark & 3) == 0, and\n+        \/\/  2) sp <= mark < mark + os::pagesize()\n+        \/\/ These 3 tests can be done by evaluating the following\n+        \/\/ expression: ((mark - sp) & (3 - os::vm_page_size())),\n+        \/\/ assuming both stack pointer and pagesize have their\n+        \/\/ least significant 2 bits clear.\n+        \/\/ NOTE: the oopMark is in swap_reg %r0 as the result of cmpxchg\n+\n+        __ sub(swap_reg, sp, swap_reg);\n+        __ neg(swap_reg, swap_reg);\n+        __ ands(swap_reg, swap_reg, 3 - (int)os::vm_page_size());\n+\n+        \/\/ Save the test result, for recursive case, the result is zero\n+        __ str(swap_reg, Address(lock_reg, mark_word_offset));\n+        __ br(Assembler::NE, slow_path_lock);\n+      }\n@@ -1920,1 +1925,1 @@\n-    if (!UseHeavyMonitors) {\n+    if (LockingMode == LEGACY) {\n@@ -1936,9 +1941,15 @@\n-      \/\/ get address of the stack lock\n-      __ lea(r0, Address(sp, lock_slot_offset * VMRegImpl::stack_slot_size));\n-      \/\/  get old displaced header\n-      __ ldr(old_hdr, Address(r0, 0));\n-\n-      \/\/ Atomic swap old header if oop still contains the stack lock\n-      Label count;\n-      __ cmpxchg_obj_header(r0, old_hdr, obj_reg, rscratch1, count, &slow_path_unlock);\n-      __ bind(count);\n+      if (LockingMode == LIGHTWEIGHT) {\n+        __ ldr(old_hdr, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+        __ tbnz(old_hdr, exact_log2(markWord::monitor_value), slow_path_unlock);\n+        __ fast_unlock(obj_reg, old_hdr, swap_reg, rscratch1, slow_path_unlock);\n+      } else if (LockingMode == LEGACY) {\n+        \/\/ get address of the stack lock\n+        __ lea(r0, Address(sp, lock_slot_offset * VMRegImpl::stack_slot_size));\n+        \/\/  get old displaced header\n+        __ ldr(old_hdr, Address(r0, 0));\n+\n+        \/\/ Atomic swap old header if oop still contains the stack lock\n+        Label count;\n+        __ cmpxchg_obj_header(r0, old_hdr, obj_reg, rscratch1, count, &slow_path_unlock);\n+        __ bind(count);\n+      }\n","filename":"src\/hotspot\/cpu\/aarch64\/sharedRuntime_aarch64.cpp","additions":49,"deletions":38,"binary":false,"changes":87,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"logging\/log.hpp\"\n@@ -202,0 +203,1 @@\n+  \/\/ save object being locked into the BasicObjectLock\n@@ -215,2 +217,2 @@\n-  \/\/ On MP platforms the next load could return a 'stale' value if the memory location has been modified by another thread.\n-  \/\/ That would be acceptable as ether CAS or slow case path is taken in that case.\n+  if (LockingMode == LIGHTWEIGHT) {\n+    log_trace(fastlock2)(\"C1_MacroAssembler::lock fast\");\n@@ -218,2 +220,3 @@\n-  \/\/ Must be the first instruction here, because implicit null check relies on it\n-  ldr(hdr, Address(obj, oopDesc::mark_offset_in_bytes()));\n+    Register t1 = disp_hdr; \/\/ Needs saving, probably\n+    Register t2 = hdr;      \/\/ blow\n+    Register t3 = Rtemp;    \/\/ blow\n@@ -221,2 +224,2 @@\n-  tst(hdr, markWord::unlocked_value);\n-  b(fast_lock, ne);\n+    fast_lock_2(obj \/* obj *\/, t1, t2, t3, 1 \/* savemask - save t1 *\/, slow_case);\n+    \/\/ Success: fall through\n@@ -224,14 +227,1 @@\n-  \/\/ Check for recursive locking\n-  \/\/ See comments in InterpreterMacroAssembler::lock_object for\n-  \/\/ explanations on the fast recursive locking check.\n-  \/\/ -1- test low 2 bits\n-  movs(tmp2, AsmOperand(hdr, lsl, 30));\n-  \/\/ -2- test (hdr - SP) if the low two bits are 0\n-  sub(tmp2, hdr, SP, eq);\n-  movs(tmp2, AsmOperand(tmp2, lsr, exact_log2(os::vm_page_size())), eq);\n-  \/\/ If still 'eq' then recursive locking OK\n-  \/\/ set to zero if recursive lock, set to non zero otherwise (see discussion in JDK-8267042)\n-  str(tmp2, Address(disp_hdr, mark_offset));\n-  b(fast_lock_done, eq);\n-  \/\/ else need slow case\n-  b(slow_case);\n+  } else if (LockingMode == LEGACY) {\n@@ -239,0 +229,2 @@\n+    \/\/ On MP platforms the next load could return a 'stale' value if the memory location has been modified by another thread.\n+    \/\/ That would be acceptable as ether CAS or slow case path is taken in that case.\n@@ -240,3 +232,2 @@\n-  bind(fast_lock);\n-  \/\/ Save previous object header in BasicLock structure and update the header\n-  str(hdr, Address(disp_hdr, mark_offset));\n+    \/\/ Must be the first instruction here, because implicit null check relies on it\n+    ldr(hdr, Address(obj, oopDesc::mark_offset_in_bytes()));\n@@ -244,1 +235,2 @@\n-  cas_for_lock_acquire(hdr, disp_hdr, obj, tmp2, slow_case);\n+    tst(hdr, markWord::unlocked_value);\n+    b(fast_lock, ne);\n@@ -246,1 +238,24 @@\n-  bind(fast_lock_done);\n+    \/\/ Check for recursive locking\n+    \/\/ See comments in InterpreterMacroAssembler::lock_object for\n+    \/\/ explanations on the fast recursive locking check.\n+    \/\/ -1- test low 2 bits\n+    movs(tmp2, AsmOperand(hdr, lsl, 30));\n+    \/\/ -2- test (hdr - SP) if the low two bits are 0\n+    sub(tmp2, hdr, SP, eq);\n+    movs(tmp2, AsmOperand(tmp2, lsr, exact_log2(os::vm_page_size())), eq);\n+    \/\/ If still 'eq' then recursive locking OK\n+    \/\/ set to zero if recursive lock, set to non zero otherwise (see discussion in JDK-8267042)\n+    str(tmp2, Address(disp_hdr, mark_offset));\n+    b(fast_lock_done, eq);\n+    \/\/ else need slow case\n+    b(slow_case);\n+\n+\n+    bind(fast_lock);\n+    \/\/ Save previous object header in BasicLock structure and update the header\n+    str(hdr, Address(disp_hdr, mark_offset));\n+\n+    cas_for_lock_acquire(hdr, disp_hdr, obj, tmp2, slow_case);\n+\n+    bind(fast_lock_done);\n+  }\n@@ -264,4 +279,2 @@\n-  \/\/ Load displaced header and object from the lock\n-  ldr(hdr, Address(disp_hdr, mark_offset));\n-  \/\/ If hdr is null, we've got recursive locking and there's nothing more to do\n-  cbz(hdr, done);\n+  if (LockingMode == LIGHTWEIGHT) {\n+    log_trace(fastlock2)(\"C1_MacroAssembler::unlock fast\");\n@@ -269,2 +282,1 @@\n-  \/\/ load object\n-  ldr(obj, Address(disp_hdr, obj_offset));\n+    ldr(obj, Address(disp_hdr, obj_offset));\n@@ -272,2 +284,3 @@\n-  \/\/ Restore the object header\n-  cas_for_lock_release(disp_hdr, hdr, obj, tmp2, slow_case);\n+    Register t1 = disp_hdr; \/\/ Needs saving, probably\n+    Register t2 = hdr;      \/\/ blow\n+    Register t3 = Rtemp;    \/\/ blow\n@@ -275,0 +288,17 @@\n+    fast_unlock_2(obj \/* object *\/, t1, t2, t3, 1 \/* savemask (save t1) *\/,\n+                    slow_case);\n+    \/\/ Success: Fall through\n+\n+  } else if (LockingMode == LEGACY) {\n+\n+    \/\/ Load displaced header and object from the lock\n+    ldr(hdr, Address(disp_hdr, mark_offset));\n+    \/\/ If hdr is null, we've got recursive locking and there's nothing more to do\n+    cbz(hdr, done);\n+\n+    \/\/ load object\n+    ldr(obj, Address(disp_hdr, obj_offset));\n+\n+    \/\/ Restore the object header\n+    cas_for_lock_release(disp_hdr, hdr, obj, tmp2, slow_case);\n+  }\n@@ -278,1 +308,0 @@\n-\n","filename":"src\/hotspot\/cpu\/arm\/c1_MacroAssembler_arm.cpp","additions":63,"deletions":34,"binary":false,"changes":97,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"logging\/log.hpp\"\n@@ -83,7 +84,1 @@\n-\n-  Register Rmark      = Rscratch2;\n-\n-  assert(Roop != Rscratch, \"\");\n-  assert(Roop != Rmark, \"\");\n-  assert(Rbox != Rscratch, \"\");\n-  assert(Rbox != Rmark, \"\");\n+  assert_different_registers(Roop, Rbox, Rscratch, Rscratch2);\n@@ -100,23 +95,38 @@\n-  ldr(Rmark, Address(Roop, oopDesc::mark_offset_in_bytes()));\n-  tst(Rmark, markWord::unlocked_value);\n-  b(fast_lock, ne);\n-\n-  \/\/ Check for recursive lock\n-  \/\/ See comments in InterpreterMacroAssembler::lock_object for\n-  \/\/ explanations on the fast recursive locking check.\n-  \/\/ -1- test low 2 bits\n-  movs(Rscratch, AsmOperand(Rmark, lsl, 30));\n-  \/\/ -2- test (hdr - SP) if the low two bits are 0\n-  sub(Rscratch, Rmark, SP, eq);\n-  movs(Rscratch, AsmOperand(Rscratch, lsr, exact_log2(os::vm_page_size())), eq);\n-  \/\/ If still 'eq' then recursive locking OK\n-  \/\/ set to zero if recursive lock, set to non zero otherwise (see discussion in JDK-8153107)\n-  str(Rscratch, Address(Rbox, BasicLock::displaced_header_offset_in_bytes()));\n-  b(done);\n-\n-  bind(fast_lock);\n-  str(Rmark, Address(Rbox, BasicLock::displaced_header_offset_in_bytes()));\n-\n-  bool allow_fallthrough_on_failure = true;\n-  bool one_shot = true;\n-  cas_for_lock_acquire(Rmark, Rbox, Roop, Rscratch, done, allow_fallthrough_on_failure, one_shot);\n+  if (LockingMode == LIGHTWEIGHT) {\n+    log_trace(fastlock2)(\"C2_MacroAssembler::lock fast\");\n+\n+    fast_lock_2(Roop \/* obj *\/, Rbox \/* t1 *\/, Rscratch \/* t2 *\/, Rscratch2 \/* t3 *\/,\n+                  1 \/* savemask (save t1) *\/,\n+                  done);\n+\n+    \/\/ Success: set Z\n+    cmp(Roop, Roop);\n+\n+  } else if (LockingMode == LEGACY) {\n+\n+    Register Rmark      = Rscratch2;\n+\n+    ldr(Rmark, Address(Roop, oopDesc::mark_offset_in_bytes()));\n+    tst(Rmark, markWord::unlocked_value);\n+    b(fast_lock, ne);\n+\n+    \/\/ Check for recursive lock\n+    \/\/ See comments in InterpreterMacroAssembler::lock_object for\n+    \/\/ explanations on the fast recursive locking check.\n+    \/\/ -1- test low 2 bits\n+    movs(Rscratch, AsmOperand(Rmark, lsl, 30));\n+    \/\/ -2- test (hdr - SP) if the low two bits are 0\n+    sub(Rscratch, Rmark, SP, eq);\n+    movs(Rscratch, AsmOperand(Rscratch, lsr, exact_log2(os::vm_page_size())), eq);\n+    \/\/ If still 'eq' then recursive locking OK\n+    \/\/ set to zero if recursive lock, set to non zero otherwise (see discussion in JDK-8153107)\n+    str(Rscratch, Address(Rbox, BasicLock::displaced_header_offset_in_bytes()));\n+    b(done);\n+\n+    bind(fast_lock);\n+    str(Rmark, Address(Rbox, BasicLock::displaced_header_offset_in_bytes()));\n+\n+    bool allow_fallthrough_on_failure = true;\n+    bool one_shot = true;\n+    cas_for_lock_acquire(Rmark, Rbox, Roop, Rscratch, done, allow_fallthrough_on_failure, one_shot);\n+  }\n@@ -133,0 +143,1 @@\n+  assert_different_registers(Roop, Rbox, Rscratch, Rscratch2);\n@@ -134,1 +145,1 @@\n-  Register Rmark      = Rscratch2;\n+  Label done;\n@@ -136,4 +147,2 @@\n-  assert(Roop != Rscratch, \"\");\n-  assert(Roop != Rmark, \"\");\n-  assert(Rbox != Rscratch, \"\");\n-  assert(Rbox != Rmark, \"\");\n+  if (LockingMode == LIGHTWEIGHT) {\n+    log_trace(fastlock2)(\"C2_MacroAssembler::unlock fast\");\n@@ -141,1 +150,0 @@\n-  Label done;\n@@ -143,4 +151,6 @@\n-  ldr(Rmark, Address(Rbox, BasicLock::displaced_header_offset_in_bytes()));\n-  \/\/ If hdr is null, we've got recursive locking and there's nothing more to do\n-  cmp(Rmark, 0);\n-  b(done, eq);\n+    fast_unlock_2(Roop \/* obj *\/, Rbox \/* t1 *\/, Rscratch \/* t2 *\/, Rscratch2 \/* t3 *\/,\n+                    1 \/* savemask (save t1) *\/,\n+                    done);\n+\n+    cmp(Roop, Roop); \/\/ Success: Set Z\n+    \/\/ Fall through\n@@ -148,4 +158,1 @@\n-  \/\/ Restore the object header\n-  bool allow_fallthrough_on_failure = true;\n-  bool one_shot = true;\n-  cas_for_lock_release(Rmark, Rbox, Roop, Rscratch, done, allow_fallthrough_on_failure, one_shot);\n+  } else if (LockingMode == LEGACY) {\n@@ -153,0 +160,13 @@\n+    Register Rmark      = Rscratch2;\n+\n+    \/\/ Find the lock address and load the displaced header from the stack.\n+    ldr(Rmark, Address(Rbox, BasicLock::displaced_header_offset_in_bytes()));\n+    \/\/ If hdr is null, we've got recursive locking and there's nothing more to do\n+    cmp(Rmark, 0);\n+    b(done, eq);\n+\n+    \/\/ Restore the object header\n+    bool allow_fallthrough_on_failure = true;\n+    bool one_shot = true;\n+    cas_for_lock_release(Rmark, Rbox, Roop, Rscratch, done, allow_fallthrough_on_failure, one_shot);\n+  }\n@@ -154,1 +174,0 @@\n-}\n@@ -156,0 +175,4 @@\n+  \/\/ At this point flags are set as follows:\n+  \/\/  EQ -> Success\n+  \/\/  NE -> Failure, branch to slow path\n+}\n","filename":"src\/hotspot\/cpu\/arm\/c2_MacroAssembler_arm.cpp","additions":68,"deletions":45,"binary":false,"changes":113,"status":"modified"},{"patch":"@@ -892,62 +892,73 @@\n-    \/\/ On MP platforms the next load could return a 'stale' value if the memory location has been modified by another thread.\n-    \/\/ That would be acceptable as ether CAS or slow case path is taken in that case.\n-    \/\/ Exception to that is if the object is locked by the calling thread, then the recursive test will pass (guaranteed as\n-    \/\/ loads are satisfied from a store queue if performed on the same processor).\n-\n-    assert(oopDesc::mark_offset_in_bytes() == 0, \"must be\");\n-    ldr(Rmark, Address(Robj, oopDesc::mark_offset_in_bytes()));\n-\n-    \/\/ Test if object is already locked\n-    tst(Rmark, markWord::unlocked_value);\n-    b(already_locked, eq);\n-\n-    \/\/ Save old object->mark() into BasicLock's displaced header\n-    str(Rmark, Address(Rlock, mark_offset));\n-\n-    cas_for_lock_acquire(Rmark, Rlock, Robj, Rtemp, slow_case);\n-\n-    b(done);\n-\n-    \/\/ If we got here that means the object is locked by ether calling thread or another thread.\n-    bind(already_locked);\n-    \/\/ Handling of locked objects: recursive locks and slow case.\n-\n-    \/\/ Fast check for recursive lock.\n-    \/\/\n-    \/\/ Can apply the optimization only if this is a stack lock\n-    \/\/ allocated in this thread. For efficiency, we can focus on\n-    \/\/ recently allocated stack locks (instead of reading the stack\n-    \/\/ base and checking whether 'mark' points inside the current\n-    \/\/ thread stack):\n-    \/\/  1) (mark & 3) == 0\n-    \/\/  2) SP <= mark < SP + os::pagesize()\n-    \/\/\n-    \/\/ Warning: SP + os::pagesize can overflow the stack base. We must\n-    \/\/ neither apply the optimization for an inflated lock allocated\n-    \/\/ just above the thread stack (this is why condition 1 matters)\n-    \/\/ nor apply the optimization if the stack lock is inside the stack\n-    \/\/ of another thread. The latter is avoided even in case of overflow\n-    \/\/ because we have guard pages at the end of all stacks. Hence, if\n-    \/\/ we go over the stack base and hit the stack of another thread,\n-    \/\/ this should not be in a writeable area that could contain a\n-    \/\/ stack lock allocated by that thread. As a consequence, a stack\n-    \/\/ lock less than page size away from SP is guaranteed to be\n-    \/\/ owned by the current thread.\n-    \/\/\n-    \/\/ Note: assuming SP is aligned, we can check the low bits of\n-    \/\/ (mark-SP) instead of the low bits of mark. In that case,\n-    \/\/ assuming page size is a power of 2, we can merge the two\n-    \/\/ conditions into a single test:\n-    \/\/ => ((mark - SP) & (3 - os::pagesize())) == 0\n-\n-    \/\/ (3 - os::pagesize()) cannot be encoded as an ARM immediate operand.\n-    \/\/ Check independently the low bits and the distance to SP.\n-    \/\/ -1- test low 2 bits\n-    movs(R0, AsmOperand(Rmark, lsl, 30));\n-    \/\/ -2- test (mark - SP) if the low two bits are 0\n-    sub(R0, Rmark, SP, eq);\n-    movs(R0, AsmOperand(R0, lsr, exact_log2(os::vm_page_size())), eq);\n-    \/\/ If still 'eq' then recursive locking OK: store 0 into lock record\n-    str(R0, Address(Rlock, mark_offset), eq);\n-\n-    b(done, eq);\n+    if (LockingMode == LIGHTWEIGHT) {\n+\n+      log_trace(fastlock2)(\"InterpreterMacroAssembler lock fast\");\n+\n+      fast_lock_2(Robj, R0 \/* t1 *\/, Rmark \/* t2 *\/, Rtemp \/* t3 *\/, 0 \/* savemask *\/, slow_case);\n+\n+      b(done);\n+\n+    } else if (LockingMode == LEGACY) {\n+\n+      \/\/ On MP platforms the next load could return a 'stale' value if the memory location has been modified by another thread.\n+      \/\/ That would be acceptable as ether CAS or slow case path is taken in that case.\n+      \/\/ Exception to that is if the object is locked by the calling thread, then the recursive test will pass (guaranteed as\n+      \/\/ loads are satisfied from a store queue if performed on the same processor).\n+\n+      assert(oopDesc::mark_offset_in_bytes() == 0, \"must be\");\n+      ldr(Rmark, Address(Robj, oopDesc::mark_offset_in_bytes()));\n+\n+      \/\/ Test if object is already locked\n+      tst(Rmark, markWord::unlocked_value);\n+      b(already_locked, eq);\n+\n+      \/\/ Save old object->mark() into BasicLock's displaced header\n+      str(Rmark, Address(Rlock, mark_offset));\n+\n+      cas_for_lock_acquire(Rmark, Rlock, Robj, Rtemp, slow_case);\n+\n+      b(done);\n+\n+      \/\/ If we got here that means the object is locked by ether calling thread or another thread.\n+      bind(already_locked);\n+      \/\/ Handling of locked objects: recursive locks and slow case.\n+\n+      \/\/ Fast check for recursive lock.\n+      \/\/\n+      \/\/ Can apply the optimization only if this is a stack lock\n+      \/\/ allocated in this thread. For efficiency, we can focus on\n+      \/\/ recently allocated stack locks (instead of reading the stack\n+      \/\/ base and checking whether 'mark' points inside the current\n+      \/\/ thread stack):\n+      \/\/  1) (mark & 3) == 0\n+      \/\/  2) SP <= mark < SP + os::pagesize()\n+      \/\/\n+      \/\/ Warning: SP + os::pagesize can overflow the stack base. We must\n+      \/\/ neither apply the optimization for an inflated lock allocated\n+      \/\/ just above the thread stack (this is why condition 1 matters)\n+      \/\/ nor apply the optimization if the stack lock is inside the stack\n+      \/\/ of another thread. The latter is avoided even in case of overflow\n+      \/\/ because we have guard pages at the end of all stacks. Hence, if\n+      \/\/ we go over the stack base and hit the stack of another thread,\n+      \/\/ this should not be in a writeable area that could contain a\n+      \/\/ stack lock allocated by that thread. As a consequence, a stack\n+      \/\/ lock less than page size away from SP is guaranteed to be\n+      \/\/ owned by the current thread.\n+      \/\/\n+      \/\/ Note: assuming SP is aligned, we can check the low bits of\n+      \/\/ (mark-SP) instead of the low bits of mark. In that case,\n+      \/\/ assuming page size is a power of 2, we can merge the two\n+      \/\/ conditions into a single test:\n+      \/\/ => ((mark - SP) & (3 - os::pagesize())) == 0\n+\n+      \/\/ (3 - os::pagesize()) cannot be encoded as an ARM immediate operand.\n+      \/\/ Check independently the low bits and the distance to SP.\n+      \/\/ -1- test low 2 bits\n+      movs(R0, AsmOperand(Rmark, lsl, 30));\n+      \/\/ -2- test (mark - SP) if the low two bits are 0\n+      sub(R0, Rmark, SP, eq);\n+      movs(R0, AsmOperand(R0, lsr, exact_log2(os::vm_page_size())), eq);\n+      \/\/ If still 'eq' then recursive locking OK: store 0 into lock record\n+      str(R0, Address(Rlock, mark_offset), eq);\n+\n+      b(done, eq);\n+    }\n@@ -958,2 +969,9 @@\n-    call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter), Rlock);\n-\n+    if (LockingMode == LIGHTWEIGHT) {\n+      \/\/ Pass oop, not lock, in fast lock case. call_VM wants R1 though.\n+      push(R1);\n+      mov(R1, Robj);\n+      call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter_obj), R1);\n+      pop(R1);\n+    } else {\n+      call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter), Rlock);\n+    }\n@@ -964,1 +982,0 @@\n-\n@@ -994,2 +1011,1 @@\n-    \/\/ Load the old header from BasicLock structure\n-    ldr(Rmark, Address(Rlock, mark_offset));\n+    if (LockingMode == LIGHTWEIGHT) {\n@@ -997,2 +1013,9 @@\n-    \/\/ Test for recursion (zero mark in BasicLock)\n-    cbz(Rmark, done);\n+      log_trace(fastlock2)(\"InterpreterMacroAssembler unlock fast\");\n+\n+      \/\/ Check for non-symmetric locking. This is allowed by the spec and the interpreter\n+      \/\/ must handle it.\n+      ldr(Rtemp, Address(Rthread, JavaThread::lock_stack_top_offset()));\n+      sub(Rtemp, Rtemp, oopSize);\n+      ldr(Rtemp, Address(Rthread, Rtemp));\n+      cmpoop(Rtemp, Robj);\n+      b(slow_case, ne);\n@@ -1000,1 +1023,3 @@\n-    bool allow_fallthrough_on_failure = true;\n+      fast_unlock_2(Robj \/* obj *\/, Rlock \/* t1 *\/, Rmark \/* t2 *\/, Rtemp \/* t3 *\/,\n+                      1 \/* savemask (save t1) *\/,\n+                      slow_case);\n@@ -1002,1 +1027,1 @@\n-    cas_for_lock_release(Rlock, Rmark, Robj, Rtemp, slow_case, allow_fallthrough_on_failure);\n+      b(done);\n@@ -1004,1 +1029,1 @@\n-    b(done, eq);\n+    } else if (LockingMode == LEGACY) {\n@@ -1006,0 +1031,13 @@\n+      \/\/ Load the old header from BasicLock structure\n+      ldr(Rmark, Address(Rlock, mark_offset));\n+\n+      \/\/ Test for recursion (zero mark in BasicLock)\n+      cbz(Rmark, done);\n+\n+      bool allow_fallthrough_on_failure = true;\n+\n+      cas_for_lock_release(Rlock, Rmark, Robj, Rtemp, slow_case, allow_fallthrough_on_failure);\n+\n+      b(done, eq);\n+\n+    }\n@@ -1016,1 +1054,0 @@\n-\n","filename":"src\/hotspot\/cpu\/arm\/interp_masm_arm.cpp","additions":110,"deletions":73,"binary":false,"changes":183,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright (c) 2023, Red Hat, Inc.\n@@ -44,0 +45,1 @@\n+#include \"runtime\/javaThread.hpp\"\n@@ -1200,0 +1202,2 @@\n+  \/\/ Here, on success, EQ is set, NE otherwise\n+\n@@ -1205,0 +1209,2 @@\n+  \/\/ Note: we preserve flags here.\n+  \/\/ Todo: Do we really need this also for the CAS fail case?\n@@ -1215,1 +1221,0 @@\n-\n@@ -1724,0 +1729,142 @@\n+#define PUSH_REG(mask, bit, Reg)      \\\n+  if (mask & ((unsigned)1 << bit)) {  \\\n+    push(Reg);                        \\\n+  }\n+\n+#define POP_REG(mask, bit, Reg, condition)   \\\n+  if (mask & ((unsigned)1 << bit)) {         \\\n+    pop(Reg, condition);                     \\\n+  }\n+\n+#define PUSH_REGS(mask, R1, R2, R3) \\\n+  PUSH_REG(mask, 0, R1)             \\\n+  PUSH_REG(mask, 1, R2)             \\\n+  PUSH_REG(mask, 2, R3)\n+\n+#define POP_REGS(mask, R1, R2, R3, condition)   \\\n+  POP_REG(mask, 0, R1, condition)               \\\n+  POP_REG(mask, 1, R2, condition)               \\\n+  POP_REG(mask, 2, R3, condition)\n+\n+#define POISON_REG(mask, bit, Reg, poison)      \\\n+  if (mask & ((unsigned)1 << bit)) {            \\\n+    mov(Reg, poison);                           \\\n+  }\n+\n+#define POISON_REGS(mask, R1, R2, R3, poison)   \\\n+  POISON_REG(mask, 0, R1, poison)               \\\n+  POISON_REG(mask, 1, R2, poison)               \\\n+  POISON_REG(mask, 2, R3, poison)\n+\n+\/\/ Attempt to fast-lock an object\n+\/\/ Registers:\n+\/\/  - obj: the object to be locked\n+\/\/  - t1, t2, t3: temp registers. If corresponding bit in savemask is set, they get saved, otherwise blown.\n+\/\/ Result:\n+\/\/  - Success: fallthrough\n+\/\/  - Error:   break to slow, Z cleared.\n+void MacroAssembler::fast_lock_2(Register obj, Register t1, Register t2, Register t3, unsigned savemask, Label& slow) {\n+  assert(LockingMode == LIGHTWEIGHT, \"only used with new lightweight locking\");\n+  assert_different_registers(obj, t1, t2, t3);\n+\n+#ifdef ASSERT\n+  \/\/ Poison scratch regs\n+  POISON_REGS((~savemask), t1, t2, t3, 0x10000001);\n+#endif\n+\n+  PUSH_REGS(savemask, t1, t2, t3);\n+\n+  \/\/ Check if we would have space on lock-stack for the object.\n+  ldr(t1, Address(Rthread, JavaThread::lock_stack_top_offset()));\n+  \/\/ cmp(t1, (unsigned)LockStack::end_offset()); \/\/  too complicated constant: 1132 (46c)\n+  movw(t2, LockStack::end_offset() - 1);\n+  cmp(t1, t2);\n+  POP_REGS(savemask, t1, t2, t3, gt);\n+  b(slow, gt); \/\/ Z is cleared\n+\n+  \/\/ Prepare old, new header\n+  Register old_hdr = t1;\n+  Register new_hdr = t2;\n+  ldr(new_hdr, Address(obj, oopDesc::mark_offset_in_bytes()));\n+  bic(new_hdr, new_hdr, markWord::lock_mask_in_place);  \/\/ new header (00)\n+  orr(old_hdr, new_hdr, markWord::unlocked_value);      \/\/ old header (01)\n+\n+  Label dummy;\n+\n+  cas_for_lock_acquire(old_hdr \/* old *\/, new_hdr \/* new *\/,\n+      obj \/* location *\/, t3 \/* scratch *\/, dummy,\n+      true \/* allow_fallthrough_on_failure *\/, true \/* one_shot *\/);\n+\n+  POP_REGS(savemask, t1, t2, t3, ne); \/\/ Cas failed -> slow\n+  b(slow, ne);                        \/\/ Cas failed -> slow\n+\n+  \/\/ After successful lock, push object onto lock-stack\n+  ldr(t1, Address(Rthread, JavaThread::lock_stack_top_offset()));\n+  str(obj, Address(Rthread, t1));\n+  add(t1, t1, oopSize);\n+  str(t1, Address(Rthread, JavaThread::lock_stack_top_offset()));\n+\n+  POP_REGS(savemask, t1, t2, t3, al);\n+\n+#ifdef ASSERT\n+  \/\/ Poison scratch regs\n+  POISON_REGS((~savemask), t1, t2, t3, 0x10000001);\n+#endif\n+\n+  \/\/ Success: fall through\n+}\n+\n+\/\/ Attempt to fast-unlock an object\n+\/\/ Registers:\n+\/\/  - obj: the object to be locked\n+\/\/  - t1, t2, t3: temp registers. If corresponding bit in savemask is set, they get saved, otherwise blown.\n+\/\/ Result:\n+\/\/  - Success: fallthrough\n+\/\/  - Error:   break to slow, Z cleared.\n+void MacroAssembler::fast_unlock_2(Register obj, Register t1, Register t2, Register t3, unsigned savemask, Label& slow) {\n+  assert(LockingMode == LIGHTWEIGHT, \"only used with new lightweight locking\");\n+  assert_different_registers(obj, t1, t2, t3);\n+\n+#ifdef ASSERT\n+  \/\/ Poison scratch regs\n+  POISON_REGS((~savemask), t1, t2, t3, 0x30000003);\n+#endif\n+\n+  PUSH_REGS(savemask, t1, t2, t3);\n+\n+  \/\/ Prepare old, new header\n+  Register old_hdr = t1;\n+  Register new_hdr = t2;\n+  ldr(old_hdr, Address(obj, oopDesc::mark_offset_in_bytes()));\n+  bic(old_hdr, old_hdr, markWord::lock_mask_in_place);    \/\/ old header (00)\n+  orr(new_hdr, old_hdr, markWord::unlocked_value);        \/\/ new header (01)\n+\n+  \/\/ Try to swing header from locked to unlocked\n+  Label dummy;\n+  cas_for_lock_release(old_hdr \/* old *\/, new_hdr \/* new *\/,\n+      obj \/* location *\/, t3 \/* scratch *\/, dummy,\n+      true \/* allow_fallthrough_on_failure *\/, true \/* one_shot *\/);\n+\n+  POP_REGS(savemask, t1, t2, t3, ne); \/\/ Cas failed -> slow\n+  b(slow, ne);                        \/\/ Cas failed -> slow\n+\n+  \/\/ After successful unlock, pop object from lock-stack\n+  ldr(t1, Address(Rthread, JavaThread::lock_stack_top_offset()));\n+  sub(t1, t1, oopSize);\n+  str(t1, Address(Rthread, JavaThread::lock_stack_top_offset()));\n+\n+#ifdef ASSERT\n+  \/\/ zero out popped slot\n+  mov(t2, 0);\n+  str(t2, Address(Rthread, t1));\n+#endif\n+\n+  POP_REGS(savemask, t1, t2, t3, al);\n+\n+#ifdef ASSERT\n+  \/\/ Poison scratch regs\n+  POISON_REGS((~savemask), t1, t2, t3, 0x40000004);\n+#endif\n+\n+  \/\/ Fallthrough: success\n+}\n","filename":"src\/hotspot\/cpu\/arm\/macroAssembler_arm.cpp","additions":148,"deletions":1,"binary":false,"changes":149,"status":"modified"},{"patch":"@@ -1013,0 +1013,18 @@\n+  \/\/ Attempt to fast-lock an object\n+  \/\/ Registers:\n+  \/\/  - obj: the object to be locked\n+  \/\/  - t1, t2, t3: temp registers. If corresponding bit in savemask is set, they get saved, otherwise blown.\n+  \/\/ Result:\n+  \/\/  - Success: fallthrough\n+  \/\/  - Error:   break to slow, Z cleared.\n+  void fast_lock_2(Register obj, Register t1, Register t2, Register t3, unsigned savemask, Label& slow);\n+\n+  \/\/ Attempt to fast-unlock an object\n+  \/\/ Registers:\n+  \/\/  - obj: the object to be locked\n+  \/\/  - t1, t2, t3: temp registers. If corresponding bit in savemask is set, they get saved, otherwise blown.\n+  \/\/ Result:\n+  \/\/  - Success: fallthrough\n+  \/\/  - Error:   break to slow, Z cleared.\n+  void fast_unlock_2(Register obj, Register t1, Register t2, Register t3, unsigned savemask, Label& slow);\n+\n","filename":"src\/hotspot\/cpu\/arm\/macroAssembler_arm.hpp","additions":18,"deletions":0,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -648,0 +648,1 @@\n+  __ flush();\n@@ -1156,29 +1157,35 @@\n-    const Register mark = tmp;\n-    \/\/ On MP platforms the next load could return a 'stale' value if the memory location has been modified by another thread.\n-    \/\/ That would be acceptable as either CAS or slow case path is taken in that case\n-\n-    __ ldr(mark, Address(sync_obj, oopDesc::mark_offset_in_bytes()));\n-    __ sub(disp_hdr, FP, lock_slot_fp_offset);\n-    __ tst(mark, markWord::unlocked_value);\n-    __ b(fast_lock, ne);\n-\n-    \/\/ Check for recursive lock\n-    \/\/ See comments in InterpreterMacroAssembler::lock_object for\n-    \/\/ explanations on the fast recursive locking check.\n-    \/\/ Check independently the low bits and the distance to SP\n-    \/\/ -1- test low 2 bits\n-    __ movs(Rtemp, AsmOperand(mark, lsl, 30));\n-    \/\/ -2- test (hdr - SP) if the low two bits are 0\n-    __ sub(Rtemp, mark, SP, eq);\n-    __ movs(Rtemp, AsmOperand(Rtemp, lsr, exact_log2(os::vm_page_size())), eq);\n-    \/\/ If still 'eq' then recursive locking OK\n-    \/\/ set to zero if recursive lock, set to non zero otherwise (see discussion in JDK-8267042)\n-    __ str(Rtemp, Address(disp_hdr, BasicLock::displaced_header_offset_in_bytes()));\n-    __ b(lock_done, eq);\n-    __ b(slow_lock);\n-\n-    __ bind(fast_lock);\n-    __ str(mark, Address(disp_hdr, BasicLock::displaced_header_offset_in_bytes()));\n-\n-    __ cas_for_lock_acquire(mark, disp_hdr, sync_obj, Rtemp, slow_lock);\n-\n+    if (LockingMode == LIGHTWEIGHT) {\n+      log_trace(fastlock2)(\"SharedRuntime lock fast\");\n+      __ fast_lock_2(sync_obj \/* object *\/, disp_hdr \/* t1 *\/, tmp \/* t2 *\/, Rtemp \/* t3 *\/,\n+                     0x7 \/* savemask *\/, slow_lock);\n+      \/\/ Fall through to lock_done\n+    } else if (LockingMode == LEGACY) {\n+      const Register mark = tmp;\n+      \/\/ On MP platforms the next load could return a 'stale' value if the memory location has been modified by another thread.\n+      \/\/ That would be acceptable as either CAS or slow case path is taken in that case\n+\n+      __ ldr(mark, Address(sync_obj, oopDesc::mark_offset_in_bytes()));\n+      __ sub(disp_hdr, FP, lock_slot_fp_offset);\n+      __ tst(mark, markWord::unlocked_value);\n+      __ b(fast_lock, ne);\n+\n+      \/\/ Check for recursive lock\n+      \/\/ See comments in InterpreterMacroAssembler::lock_object for\n+      \/\/ explanations on the fast recursive locking check.\n+      \/\/ Check independently the low bits and the distance to SP\n+      \/\/ -1- test low 2 bits\n+      __ movs(Rtemp, AsmOperand(mark, lsl, 30));\n+      \/\/ -2- test (hdr - SP) if the low two bits are 0\n+      __ sub(Rtemp, mark, SP, eq);\n+      __ movs(Rtemp, AsmOperand(Rtemp, lsr, exact_log2(os::vm_page_size())), eq);\n+      \/\/ If still 'eq' then recursive locking OK\n+      \/\/ set to zero if recursive lock, set to non zero otherwise (see discussion in JDK-8267042)\n+      __ str(Rtemp, Address(disp_hdr, BasicLock::displaced_header_offset_in_bytes()));\n+      __ b(lock_done, eq);\n+      __ b(slow_lock);\n+\n+      __ bind(fast_lock);\n+      __ str(mark, Address(disp_hdr, BasicLock::displaced_header_offset_in_bytes()));\n+\n+      __ cas_for_lock_acquire(mark, disp_hdr, sync_obj, Rtemp, slow_lock);\n+    }\n@@ -1235,8 +1242,14 @@\n-    __ ldr(sync_obj, Address(sync_handle));\n-\n-    \/\/ See C1_MacroAssembler::unlock_object() for more comments\n-    __ ldr(R2, Address(disp_hdr, BasicLock::displaced_header_offset_in_bytes()));\n-    __ cbz(R2, unlock_done);\n-\n-    __ cas_for_lock_release(disp_hdr, R2, sync_obj, Rtemp, slow_unlock);\n-\n+    if (LockingMode == LIGHTWEIGHT) {\n+      log_trace(fastlock2)(\"SharedRuntime unlock fast\");\n+      __ fast_unlock_2(sync_obj, R2, tmp, Rtemp, 7, slow_unlock);\n+      \/\/ Fall through\n+    } else if (LockingMode == LEGACY) {\n+      \/\/ See C1_MacroAssembler::unlock_object() for more comments\n+      __ ldr(sync_obj, Address(sync_handle));\n+\n+      \/\/ See C1_MacroAssembler::unlock_object() for more comments\n+      __ ldr(R2, Address(disp_hdr, BasicLock::displaced_header_offset_in_bytes()));\n+      __ cbz(R2, unlock_done);\n+\n+      __ cas_for_lock_release(disp_hdr, R2, sync_obj, Rtemp, slow_unlock);\n+    }\n","filename":"src\/hotspot\/cpu\/arm\/sharedRuntime_arm.cpp","additions":50,"deletions":37,"binary":false,"changes":87,"status":"modified"},{"patch":"@@ -55,2 +55,1 @@\n-  assert(hdr != obj && hdr != disp_hdr && obj != disp_hdr, \"registers must be different\");\n-  Label done;\n+  assert_different_registers(hdr, obj, disp_hdr);\n@@ -70,1 +69,1 @@\n-    bnez(t0, slow_case, true \/* is_far *\/);\n+    bnez(t0, slow_case, \/* is_far *\/ true);\n@@ -75,33 +74,39 @@\n-  \/\/ and mark it as unlocked\n-  ori(hdr, hdr, markWord::unlocked_value);\n-  \/\/ save unlocked object header into the displaced header location on the stack\n-  sd(hdr, Address(disp_hdr, 0));\n-  \/\/ test if object header is still the same (i.e. unlocked), and if so, store the\n-  \/\/ displaced header address in the object header - if it is not the same, get the\n-  \/\/ object header instead\n-  la(t1, Address(obj, hdr_offset));\n-  cmpxchgptr(hdr, disp_hdr, t1, t0, done, \/*fallthough*\/NULL);\n-  \/\/ if the object header was the same, we're done\n-  \/\/ if the object header was not the same, it is now in the hdr register\n-  \/\/ => test if it is a stack pointer into the same stack (recursive locking), i.e.:\n-  \/\/\n-  \/\/ 1) (hdr & aligned_mask) == 0\n-  \/\/ 2) sp <= hdr\n-  \/\/ 3) hdr <= sp + page_size\n-  \/\/\n-  \/\/ these 3 tests can be done by evaluating the following expression:\n-  \/\/\n-  \/\/ (hdr -sp) & (aligned_mask - page_size)\n-  \/\/\n-  \/\/ assuming both the stack pointer and page_size have their least\n-  \/\/ significant 2 bits cleared and page_size is a power of 2\n-  sub(hdr, hdr, sp);\n-  mv(t0, aligned_mask - (int)os::vm_page_size());\n-  andr(hdr, hdr, t0);\n-  \/\/ for recursive locking, the result is zero => save it in the displaced header\n-  \/\/ location (NULL in the displaced hdr location indicates recursive locking)\n-  sd(hdr, Address(disp_hdr, 0));\n-  \/\/ otherwise we don't care about the result and handle locking via runtime call\n-  bnez(hdr, slow_case, \/* is_far *\/ true);\n-  \/\/ done\n-  bind(done);\n+\n+  if (LockingMode == LIGHTWEIGHT) {\n+    fast_lock(obj, hdr, t0, t1, slow_case);\n+  } else if (LockingMode == LEGACY) {\n+    Label done;\n+    \/\/ and mark it as unlocked\n+    ori(hdr, hdr, markWord::unlocked_value);\n+    \/\/ save unlocked object header into the displaced header location on the stack\n+    sd(hdr, Address(disp_hdr, 0));\n+    \/\/ test if object header is still the same (i.e. unlocked), and if so, store the\n+    \/\/ displaced header address in the object header - if it is not the same, get the\n+    \/\/ object header instead\n+    la(t1, Address(obj, hdr_offset));\n+    cmpxchgptr(hdr, disp_hdr, t1, t0, done, \/*fallthough*\/nullptr);\n+    \/\/ if the object header was the same, we're done\n+    \/\/ if the object header was not the same, it is now in the hdr register\n+    \/\/ => test if it is a stack pointer into the same stack (recursive locking), i.e.:\n+    \/\/\n+    \/\/ 1) (hdr & aligned_mask) == 0\n+    \/\/ 2) sp <= hdr\n+    \/\/ 3) hdr <= sp + page_size\n+    \/\/\n+    \/\/ these 3 tests can be done by evaluating the following expression:\n+    \/\/\n+    \/\/ (hdr -sp) & (aligned_mask - page_size)\n+    \/\/\n+    \/\/ assuming both the stack pointer and page_size have their least\n+    \/\/ significant 2 bits cleared and page_size is a power of 2\n+    sub(hdr, hdr, sp);\n+    mv(t0, aligned_mask - (int)os::vm_page_size());\n+    andr(hdr, hdr, t0);\n+    \/\/ for recursive locking, the result is zero => save it in the displaced header\n+    \/\/ location (null in the displaced hdr location indicates recursive locking)\n+    sd(hdr, Address(disp_hdr, 0));\n+    \/\/ otherwise we don't care about the result and handle locking via runtime call\n+    bnez(hdr, slow_case, \/* is_far *\/ true);\n+    \/\/ done\n+    bind(done);\n+  }\n@@ -118,5 +123,8 @@\n-  \/\/ load displaced header\n-  ld(hdr, Address(disp_hdr, 0));\n-  \/\/ if the loaded hdr is NULL we had recursive locking\n-  \/\/ if we had recursive locking, we are done\n-  beqz(hdr, done);\n+  if (LockingMode != LIGHTWEIGHT) {\n+    \/\/ load displaced header\n+    ld(hdr, Address(disp_hdr, 0));\n+    \/\/ if the loaded hdr is null we had recursive locking\n+    \/\/ if we had recursive locking, we are done\n+    beqz(hdr, done);\n+  }\n+\n@@ -126,10 +134,18 @@\n-  \/\/ test if object header is pointing to the displaced header, and if so, restore\n-  \/\/ the displaced header in the object - if the object header is not pointing to\n-  \/\/ the displaced header, get the object header instead\n-  \/\/ if the object header was not pointing to the displaced header,\n-  \/\/ we do unlocking via runtime call\n-  if (hdr_offset) {\n-    la(t0, Address(obj, hdr_offset));\n-    cmpxchgptr(disp_hdr, hdr, t0, t1, done, &slow_case);\n-  } else {\n-    cmpxchgptr(disp_hdr, hdr, obj, t1, done, &slow_case);\n+\n+  if (LockingMode == LIGHTWEIGHT) {\n+    ld(hdr, Address(obj, oopDesc::mark_offset_in_bytes()));\n+    fast_unlock(obj, hdr, t0, t1, slow_case);\n+  } else if (LockingMode == LEGACY) {\n+    \/\/ test if object header is pointing to the displaced header, and if so, restore\n+    \/\/ the displaced header in the object - if the object header is not pointing to\n+    \/\/ the displaced header, get the object header instead\n+    \/\/ if the object header was not pointing to the displaced header,\n+    \/\/ we do unlocking via runtime call\n+    if (hdr_offset) {\n+      la(t0, Address(obj, hdr_offset));\n+      cmpxchgptr(disp_hdr, hdr, t0, t1, done, &slow_case);\n+    } else {\n+      cmpxchgptr(disp_hdr, hdr, obj, t1, done, &slow_case);\n+    }\n+    \/\/ done\n+    bind(done);\n@@ -137,2 +153,0 @@\n-  \/\/ done\n-  bind(done);\n","filename":"src\/hotspot\/cpu\/riscv\/c1_MacroAssembler_riscv.cpp","additions":67,"deletions":53,"binary":false,"changes":120,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"runtime\/objectMonitor.hpp\"\n@@ -75,0 +76,24 @@\n+int C2HandleAnonOMOwnerStub::max_size() const {\n+  \/\/ Max size of stub has been determined by testing with 0 without using RISC-V compressed\n+  \/\/ instruction-set extension, in which case C2CodeStubList::emit() will throw an assertion\n+  \/\/ and report the actual size that is needed.\n+  return 20;\n+}\n+\n+void C2HandleAnonOMOwnerStub::emit(C2_MacroAssembler& masm) {\n+  __ bind(entry());\n+  Register mon = monitor();\n+  Register t = tmp();\n+  assert(t != noreg, \"need tmp register\");\n+\n+  \/\/ Fix owner to be the current thread.\n+  __ sd(xthread, Address(mon, ObjectMonitor::owner_offset_in_bytes()));\n+\n+  \/\/ Pop owner object from lock-stack.\n+  __ lwu(t, Address(xthread, JavaThread::lock_stack_top_offset()));\n+  __ subw(t, t, oopSize);\n+  __ sw(t, Address(xthread, JavaThread::lock_stack_top_offset()));\n+\n+  __ j(continuation());\n+}\n+\n","filename":"src\/hotspot\/cpu\/riscv\/c2_CodeStubs_riscv.cpp","additions":25,"deletions":0,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -812,28 +812,34 @@\n-    \/\/ Load (object->mark() | 1) into swap_reg\n-    ld(t0, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-    ori(swap_reg, t0, 1);\n-\n-    \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n-    sd(swap_reg, Address(lock_reg, mark_offset));\n-\n-    assert(lock_offset == 0,\n-           \"displached header must be first word in BasicObjectLock\");\n-\n-    cmpxchg_obj_header(swap_reg, lock_reg, obj_reg, t0, count, \/*fallthrough*\/NULL);\n-\n-    \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n-    \/\/  1) (mark & 7) == 0, and\n-    \/\/  2) sp <= mark < mark + os::pagesize()\n-    \/\/\n-    \/\/ These 3 tests can be done by evaluating the following\n-    \/\/ expression: ((mark - sp) & (7 - os::vm_page_size())),\n-    \/\/ assuming both stack pointer and pagesize have their\n-    \/\/ least significant 3 bits clear.\n-    \/\/ NOTE: the oopMark is in swap_reg x10 as the result of cmpxchg\n-    sub(swap_reg, swap_reg, sp);\n-    mv(t0, (int64_t)(7 - (int)os::vm_page_size()));\n-    andr(swap_reg, swap_reg, t0);\n-\n-    \/\/ Save the test result, for recursive case, the result is zero\n-    sd(swap_reg, Address(lock_reg, mark_offset));\n-    beqz(swap_reg, count);\n+    if (LockingMode == LIGHTWEIGHT) {\n+      ld(tmp, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      fast_lock(obj_reg, tmp, t0, t1, slow_case);\n+      j(count);\n+    } else if (LockingMode == LEGACY) {\n+      \/\/ Load (object->mark() | 1) into swap_reg\n+      ld(t0, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      ori(swap_reg, t0, 1);\n+\n+      \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n+      sd(swap_reg, Address(lock_reg, mark_offset));\n+\n+      assert(lock_offset == 0,\n+             \"displached header must be first word in BasicObjectLock\");\n+\n+      cmpxchg_obj_header(swap_reg, lock_reg, obj_reg, t0, count, \/*fallthrough*\/nullptr);\n+\n+      \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n+      \/\/  1) (mark & 7) == 0, and\n+      \/\/  2) sp <= mark < mark + os::pagesize()\n+      \/\/\n+      \/\/ These 3 tests can be done by evaluating the following\n+      \/\/ expression: ((mark - sp) & (7 - os::vm_page_size())),\n+      \/\/ assuming both stack pointer and pagesize have their\n+      \/\/ least significant 3 bits clear.\n+      \/\/ NOTE: the oopMark is in swap_reg x10 as the result of cmpxchg\n+      sub(swap_reg, swap_reg, sp);\n+      mv(t0, (int64_t)(7 - (int)os::vm_page_size()));\n+      andr(swap_reg, swap_reg, t0);\n+\n+      \/\/ Save the test result, for recursive case, the result is zero\n+      sd(swap_reg, Address(lock_reg, mark_offset));\n+      beqz(swap_reg, count);\n+    }\n@@ -844,4 +850,9 @@\n-    call_VM(noreg,\n-            CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter),\n-            lock_reg);\n-\n+    if (LockingMode == LIGHTWEIGHT) {\n+      call_VM(noreg,\n+              CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter_obj),\n+              obj_reg);\n+    } else {\n+      call_VM(noreg,\n+              CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter),\n+              lock_reg);\n+    }\n@@ -884,3 +895,5 @@\n-    \/\/ Convert from BasicObjectLock structure to object and BasicLock\n-    \/\/ structure Store the BasicLock address into x10\n-    la(swap_reg, Address(lock_reg, BasicObjectLock::lock_offset_in_bytes()));\n+    if (LockingMode != LIGHTWEIGHT) {\n+      \/\/ Convert from BasicObjectLock structure to object and BasicLock\n+      \/\/ structure Store the BasicLock address into x10\n+      la(swap_reg, Address(lock_reg, BasicObjectLock::lock_offset_in_bytes()));\n+    }\n@@ -894,9 +907,28 @@\n-    \/\/ Load the old header from BasicLock structure\n-    ld(header_reg, Address(swap_reg,\n-                           BasicLock::displaced_header_offset_in_bytes()));\n-\n-    \/\/ Test for recursion\n-    beqz(header_reg, count);\n-\n-    \/\/ Atomic swap back the old header\n-    cmpxchg_obj_header(swap_reg, header_reg, obj_reg, t0, count, \/*fallthrough*\/NULL);\n+    if (LockingMode == LIGHTWEIGHT) {\n+      Label slow_case;\n+\n+      \/\/ Check for non-symmetric locking. This is allowed by the spec and the interpreter\n+      \/\/ must handle it.\n+      Register tmp = header_reg;\n+      lwu(tmp, Address(xthread, JavaThread::lock_stack_top_offset()));\n+      subw(tmp, tmp, oopSize);\n+      add(tmp, xthread, tmp);\n+      ld(tmp, Address(tmp, 0));\n+      bne(tmp, obj_reg, slow_case);\n+\n+      ld(header_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      fast_unlock(obj_reg, header_reg, swap_reg, t0, slow_case);\n+      j(count);\n+\n+      bind(slow_case);\n+    } else if (LockingMode == LEGACY) {\n+      \/\/ Load the old header from BasicLock structure\n+      ld(header_reg, Address(swap_reg,\n+                             BasicLock::displaced_header_offset_in_bytes()));\n+\n+      \/\/ Test for recursion\n+      beqz(header_reg, count);\n+\n+      \/\/ Atomic swap back the old header\n+      cmpxchg_obj_header(swap_reg, header_reg, obj_reg, t0, count, \/*fallthrough*\/nullptr);\n+    }\n","filename":"src\/hotspot\/cpu\/riscv\/interp_masm_riscv.cpp","additions":76,"deletions":44,"binary":false,"changes":120,"status":"modified"},{"patch":"@@ -2423,1 +2423,1 @@\n-    bgtu(in_nmethod ? sp : fp, t0, slow_path, true \/* is_far *\/);\n+    bgtu(in_nmethod ? sp : fp, t0, slow_path, \/* is_far *\/ true);\n@@ -2426,1 +2426,1 @@\n-    bnez(t0, slow_path, true \/* is_far *\/);\n+    bnez(t0, slow_path, \/* is_far *\/ true);\n@@ -4485,0 +4485,55 @@\n+\n+\/\/ Attempt to fast-lock an object. Fall-through on success, branch to slow label\n+\/\/ on failure.\n+\/\/ Registers:\n+\/\/  - obj: the object to be locked\n+\/\/  - hdr: the header, already loaded from obj, will be destroyed\n+\/\/  - tmp1, tmp2: temporary registers, will be destroyed\n+void MacroAssembler::fast_lock(Register obj, Register hdr, Register tmp1, Register tmp2, Label& slow) {\n+  assert(LockingMode == LIGHTWEIGHT, \"only used with new lightweight locking\");\n+  assert_different_registers(obj, hdr, tmp1, tmp2);\n+\n+  \/\/ Check if we would have space on lock-stack for the object.\n+  lwu(tmp1, Address(xthread, JavaThread::lock_stack_top_offset()));\n+  mv(tmp2, (unsigned)LockStack::end_offset());\n+  bge(tmp1, tmp2, slow, \/* is_far *\/ true);\n+\n+  \/\/ Load (object->mark() | 1) into hdr\n+  ori(hdr, hdr, markWord::unlocked_value);\n+  \/\/ Clear lock-bits, into tmp2\n+  xori(tmp2, hdr, markWord::unlocked_value);\n+\n+  \/\/ Try to swing header from unlocked to locked\n+  Label success;\n+  cmpxchgptr(hdr, tmp2, obj, tmp1, success, &slow);\n+  bind(success);\n+\n+  \/\/ After successful lock, push object on lock-stack\n+  lwu(tmp1, Address(xthread, JavaThread::lock_stack_top_offset()));\n+  add(tmp2, xthread, tmp1);\n+  sd(obj, Address(tmp2, 0));\n+  addw(tmp1, tmp1, oopSize);\n+  sw(tmp1, Address(xthread, JavaThread::lock_stack_top_offset()));\n+}\n+\n+void MacroAssembler::fast_unlock(Register obj, Register hdr, Register tmp1, Register tmp2, Label& slow) {\n+  assert(LockingMode == LIGHTWEIGHT, \"only used with new lightweight locking\");\n+  assert_different_registers(obj, hdr, tmp1, tmp2);\n+\n+  \/\/ Load the expected old header (lock-bits cleared to indicate 'locked') into hdr\n+  mv(tmp1, ~markWord::lock_mask_in_place);\n+  andr(hdr, hdr, tmp1);\n+\n+  \/\/ Load the new header (unlocked) into tmp1\n+  ori(tmp1, hdr, markWord::unlocked_value);\n+\n+  \/\/ Try to swing header from locked to unlocked\n+  Label success;\n+  cmpxchgptr(hdr, tmp1, obj, tmp2, success, &slow);\n+  bind(success);\n+\n+  \/\/ After successful unlock, pop object from lock-stack\n+  lwu(tmp1, Address(xthread, JavaThread::lock_stack_top_offset()));\n+  subw(tmp1, tmp1, oopSize);\n+  sw(tmp1, Address(xthread, JavaThread::lock_stack_top_offset()));\n+}\n","filename":"src\/hotspot\/cpu\/riscv\/macroAssembler_riscv.cpp","additions":57,"deletions":2,"binary":false,"changes":59,"status":"modified"},{"patch":"@@ -1386,0 +1386,4 @@\n+\n+public:\n+  void fast_lock(Register obj, Register hdr, Register tmp1, Register tmp2, Label& slow);\n+  void fast_unlock(Register obj, Register hdr, Register tmp1, Register tmp2, Label& slow);\n","filename":"src\/hotspot\/cpu\/riscv\/macroAssembler_riscv.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2391,1 +2391,1 @@\n-  \/\/ using the cr register as the bool result: 0 for success; others failed.\n+  \/\/ Use cr register to indicate the fast_lock result: zero for success; non-zero for failure.\n@@ -2401,1 +2401,1 @@\n-    Label no_count;\n+    Label count, no_count;\n@@ -2409,4 +2409,4 @@\n-      __ load_klass(flag, oop);\n-      __ lwu(flag, Address(flag, Klass::access_flags_offset()));\n-      __ andi(flag, flag, JVM_ACC_IS_VALUE_BASED_CLASS, tmp \/* tmp *\/);\n-      __ bnez(flag, cont, true \/* is_far *\/);\n+      __ load_klass(tmp, oop);\n+      __ lwu(tmp, Address(tmp, Klass::access_flags_offset()));\n+      __ andi(flag, tmp, JVM_ACC_IS_VALUE_BASED_CLASS);\n+      __ bnez(flag, cont, \/* is_far *\/ true);\n@@ -2420,30 +2420,43 @@\n-      \/\/ Set tmp to be (markWord of object | UNLOCK_VALUE).\n-      __ ori(tmp, disp_hdr, markWord::unlocked_value);\n-\n-      \/\/ Initialize the box. (Must happen before we update the object mark!)\n-      __ sd(tmp, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n-\n-      \/\/ Compare object markWord with an unlocked value (tmp) and if\n-      \/\/ equal exchange the stack address of our box with object markWord.\n-      \/\/ On failure disp_hdr contains the possibly locked markWord.\n-      __ cmpxchg(\/*memory address*\/oop, \/*expected value*\/tmp, \/*new value*\/box, Assembler::int64, Assembler::aq,\n-                 Assembler::rl, \/*result*\/disp_hdr);\n-      __ mv(flag, zr);\n-      __ beq(disp_hdr, tmp, cont); \/\/ prepare zero flag and goto cont if we won the cas\n-\n-      assert(oopDesc::mark_offset_in_bytes() == 0, \"offset of _mark is not 0\");\n-\n-      \/\/ If the compare-and-exchange succeeded, then we found an unlocked\n-      \/\/ object, will have now locked it will continue at label cont\n-      \/\/ We did not see an unlocked object so try the fast recursive case.\n-\n-      \/\/ Check if the owner is self by comparing the value in the\n-      \/\/ markWord of object (disp_hdr) with the stack pointer.\n-      __ sub(disp_hdr, disp_hdr, sp);\n-      __ mv(tmp, (intptr_t) (~(os::vm_page_size()-1) | (uintptr_t)markWord::lock_mask_in_place));\n-      \/\/ If (mark & lock_mask) == 0 and mark - sp < page_size, we are stack-locking and goto cont,\n-      \/\/ hence we can store 0 as the displaced header in the box, which indicates that it is a\n-      \/\/ recursive lock.\n-      __ andr(tmp\/*==0?*\/, disp_hdr, tmp);\n-      __ sd(tmp\/*==0, perhaps*\/, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n-      __ mv(flag, tmp); \/\/ we can use the value of tmp as the result here\n+      if (LockingMode == LIGHTWEIGHT) {\n+        Label slow;\n+        __ fast_lock(oop, disp_hdr, tmp, t0, slow);\n+\n+        \/\/ Indicate success on completion.\n+        __ mv(flag, zr);\n+        __ j(count);\n+        __ bind(slow);\n+        __ mv(flag, 1); \/\/ Set non-zero flag to indicate 'failure' -> take slow-path\n+        __ j(no_count);\n+      } else if (LockingMode == LEGACY) {\n+        \/\/ Set tmp to be (markWord of object | UNLOCK_VALUE).\n+        __ ori(tmp, disp_hdr, markWord::unlocked_value);\n+\n+        \/\/ Initialize the box. (Must happen before we update the object mark!)\n+        __ sd(tmp, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n+\n+        \/\/ Compare object markWord with an unlocked value (tmp) and if\n+        \/\/ equal exchange the stack address of our box with object markWord.\n+        \/\/ On failure disp_hdr contains the possibly locked markWord.\n+        __ cmpxchg(\/*memory address*\/oop, \/*expected value*\/tmp, \/*new value*\/box, Assembler::int64, Assembler::aq,\n+                   Assembler::rl, \/*result*\/disp_hdr);\n+        __ mv(flag, zr);\n+        __ beq(disp_hdr, tmp, cont); \/\/ prepare zero flag and goto cont if we won the cas\n+\n+        assert(oopDesc::mark_offset_in_bytes() == 0, \"offset of _mark is not 0\");\n+\n+        \/\/ If the compare-and-exchange succeeded, then we found an unlocked\n+        \/\/ object, will have now locked it will continue at label cont\n+        \/\/ We did not see an unlocked object so try the fast recursive case.\n+\n+        \/\/ Check if the owner is self by comparing the value in the\n+        \/\/ markWord of object (disp_hdr) with the stack pointer.\n+        __ sub(disp_hdr, disp_hdr, sp);\n+        __ mv(tmp, (intptr_t) (~(os::vm_page_size()-1) | (uintptr_t)markWord::lock_mask_in_place));\n+        \/\/ If (mark & lock_mask) == 0 and mark - sp < page_size, we are stack-locking and goto cont,\n+        \/\/ hence we can store 0 as the displaced header in the box, which indicates that it is a\n+        \/\/ recursive lock.\n+        __ andr(tmp\/*==0?*\/, disp_hdr, tmp);\n+        __ sd(tmp\/*==0, perhaps*\/, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n+        __ mv(flag, tmp); \/\/ we can use the value of tmp as the result here\n+      }\n+      __ j(cont);\n@@ -2452,0 +2465,1 @@\n+      __ j(cont);\n@@ -2454,2 +2468,0 @@\n-    __ j(cont);\n-\n@@ -2466,6 +2478,8 @@\n-    \/\/ Store a non-null value into the box to avoid looking like a re-entrant\n-    \/\/ lock. The fast-path monitor unlock code checks for\n-    \/\/ markWord::monitor_value so use markWord::unused_mark which has the\n-    \/\/ relevant bit set, and also matches ObjectSynchronizer::slow_enter.\n-    __ mv(tmp, (address)markWord::unused_mark().value());\n-    __ sd(tmp, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n+    if (LockingMode != LIGHTWEIGHT) {\n+      \/\/ Store a non-null value into the box to avoid looking like a re-entrant\n+      \/\/ lock. The fast-path monitor unlock code checks for\n+      \/\/ markWord::monitor_value so use markWord::unused_mark which has the\n+      \/\/ relevant bit set, and also matches ObjectSynchronizer::slow_enter.\n+      __ mv(tmp, (address)markWord::unused_mark().value());\n+      __ sd(tmp, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n+    }\n@@ -2482,1 +2496,2 @@\n-\n+    \/\/ zero flag indicates success\n+    \/\/ non-zero flag indicates failure\n@@ -2485,0 +2500,1 @@\n+    __ bind(count);\n@@ -2490,1 +2506,1 @@\n-  \/\/ using cr flag to indicate the fast_unlock result: 0 for success; others failed.\n+  \/\/ Use cr register to indicate the fast_unlock result: zero for success; non-zero for failure.\n@@ -2500,1 +2516,1 @@\n-    Label no_count;\n+    Label count, no_count;\n@@ -2504,1 +2520,1 @@\n-    if (!UseHeavyMonitors) {\n+    if (LockingMode == LEGACY) {\n@@ -2519,7 +2535,20 @@\n-      \/\/ Check if it is still a light weight lock, this is true if we\n-      \/\/ see the stack address of the basicLock in the markWord of the\n-      \/\/ object.\n-\n-      __ cmpxchg(\/*memory address*\/oop, \/*expected value*\/box, \/*new value*\/disp_hdr, Assembler::int64, Assembler::relaxed,\n-                 Assembler::rl, \/*result*\/tmp);\n-      __ xorr(flag, box, tmp); \/\/ box == tmp if cas succeeds\n+      if (LockingMode == LIGHTWEIGHT) {\n+        Label slow;\n+        __ fast_unlock(oop, tmp, box, disp_hdr, slow);\n+\n+        \/\/ Indicate success on completion.\n+        __ mv(flag, zr);\n+        __ j(count);\n+        __ bind(slow);\n+        __ mv(flag, 1); \/\/ Set non-zero flag to indicate 'failure' -> take slow path\n+        __ j(no_count);\n+      } else if (LockingMode == LEGACY) {\n+        \/\/ Check if it is still a light weight lock, this is true if we\n+        \/\/ see the stack address of the basicLock in the markWord of the\n+        \/\/ object.\n+\n+        __ cmpxchg(\/*memory address*\/oop, \/*expected value*\/box, \/*new value*\/disp_hdr, Assembler::int64, Assembler::relaxed,\n+                   Assembler::rl, \/*result*\/tmp);\n+        __ xorr(flag, box, tmp); \/\/ box == tmp if cas succeeds\n+        __ j(cont);\n+      }\n@@ -2528,0 +2557,1 @@\n+      __ j(cont);\n@@ -2529,1 +2559,0 @@\n-    __ j(cont);\n@@ -2537,0 +2566,12 @@\n+\n+    if (LockingMode == LIGHTWEIGHT) {\n+      \/\/ If the owner is anonymous, we need to fix it -- in an outline stub.\n+      Register tmp2 = disp_hdr;\n+      __ ld(tmp2, Address(tmp, ObjectMonitor::owner_offset_in_bytes()));\n+      __ andi(t0, tmp2, (int64_t)ObjectMonitor::ANONYMOUS_OWNER);\n+      C2HandleAnonOMOwnerStub* stub = new (Compile::current()->comp_arena()) C2HandleAnonOMOwnerStub(tmp, tmp2);\n+      Compile::current()->output()->add_stub(stub);\n+      __ bnez(t0, stub->entry(), \/* is_far *\/ true);\n+      __ bind(stub->continuation());\n+    }\n+\n@@ -2559,1 +2600,2 @@\n-\n+    \/\/ zero flag indicates success\n+    \/\/ non-zero flag indicates failure\n@@ -2562,0 +2604,1 @@\n+    __ bind(count);\n","filename":"src\/hotspot\/cpu\/riscv\/riscv.ad","additions":100,"deletions":57,"binary":false,"changes":157,"status":"modified"},{"patch":"@@ -1674,25 +1674,30 @@\n-      \/\/ Load (object->mark() | 1) into swap_reg % x10\n-      __ ld(t0, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-      __ ori(swap_reg, t0, 1);\n-\n-      \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n-      __ sd(swap_reg, Address(lock_reg, mark_word_offset));\n-\n-      \/\/ src -> dest if dest == x10 else x10 <- dest\n-      __ cmpxchg_obj_header(x10, lock_reg, obj_reg, t0, count, \/*fallthrough*\/NULL);\n-\n-      \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n-      \/\/  1) (mark & 3) == 0, and\n-      \/\/  2) sp <= mark < mark + os::pagesize()\n-      \/\/ These 3 tests can be done by evaluating the following\n-      \/\/ expression: ((mark - sp) & (3 - os::vm_page_size())),\n-      \/\/ assuming both stack pointer and pagesize have their\n-      \/\/ least significant 2 bits clear.\n-      \/\/ NOTE: the oopMark is in swap_reg % 10 as the result of cmpxchg\n-\n-      __ sub(swap_reg, swap_reg, sp);\n-      __ andi(swap_reg, swap_reg, 3 - (int)os::vm_page_size());\n-\n-      \/\/ Save the test result, for recursive case, the result is zero\n-      __ sd(swap_reg, Address(lock_reg, mark_word_offset));\n-      __ bnez(swap_reg, slow_path_lock);\n+      if (LockingMode == LIGHTWEIGHT) {\n+        __ ld(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+        __ fast_lock(obj_reg, swap_reg, tmp, t0, slow_path_lock);\n+      } else if (LockingMode == LEGACY) {\n+        \/\/ Load (object->mark() | 1) into swap_reg % x10\n+        __ ld(t0, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+        __ ori(swap_reg, t0, 1);\n+\n+        \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n+        __ sd(swap_reg, Address(lock_reg, mark_word_offset));\n+\n+        \/\/ src -> dest if dest == x10 else x10 <- dest\n+        __ cmpxchg_obj_header(x10, lock_reg, obj_reg, t0, count, \/*fallthrough*\/nullptr);\n+\n+        \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n+        \/\/  1) (mark & 3) == 0, and\n+        \/\/  2) sp <= mark < mark + os::pagesize()\n+        \/\/ These 3 tests can be done by evaluating the following\n+        \/\/ expression: ((mark - sp) & (3 - os::vm_page_size())),\n+        \/\/ assuming both stack pointer and pagesize have their\n+        \/\/ least significant 2 bits clear.\n+        \/\/ NOTE: the oopMark is in swap_reg % 10 as the result of cmpxchg\n+\n+        __ sub(swap_reg, swap_reg, sp);\n+        __ andi(swap_reg, swap_reg, 3 - (int)os::vm_page_size());\n+\n+        \/\/ Save the test result, for recursive case, the result is zero\n+        __ sd(swap_reg, Address(lock_reg, mark_word_offset));\n+        __ bnez(swap_reg, slow_path_lock);\n+      }\n@@ -1793,1 +1798,1 @@\n-    if (!UseHeavyMonitors) {\n+    if (LockingMode == LEGACY) {\n@@ -1809,9 +1814,14 @@\n-      \/\/ get address of the stack lock\n-      __ la(x10, Address(sp, lock_slot_offset * VMRegImpl::stack_slot_size));\n-      \/\/  get old displaced header\n-      __ ld(old_hdr, Address(x10, 0));\n-\n-      \/\/ Atomic swap old header if oop still contains the stack lock\n-      Label count;\n-      __ cmpxchg_obj_header(x10, old_hdr, obj_reg, t0, count, &slow_path_unlock);\n-      __ bind(count);\n+      if (LockingMode == LIGHTWEIGHT) {\n+        __ ld(old_hdr, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+        __ fast_unlock(obj_reg, old_hdr, swap_reg, t0, slow_path_unlock);\n+      } else if (LockingMode == LEGACY) {\n+        \/\/ get address of the stack lock\n+        __ la(x10, Address(sp, lock_slot_offset * VMRegImpl::stack_slot_size));\n+        \/\/  get old displaced header\n+        __ ld(old_hdr, Address(x10, 0));\n+\n+        \/\/ Atomic swap old header if oop still contains the stack lock\n+        Label count;\n+        __ cmpxchg_obj_header(x10, old_hdr, obj_reg, t0, count, &slow_path_unlock);\n+        __ bind(count);\n+      }\n","filename":"src\/hotspot\/cpu\/riscv\/sharedRuntime_riscv.cpp","additions":45,"deletions":35,"binary":false,"changes":80,"status":"modified"},{"patch":"@@ -3511,0 +3511,1 @@\n+    Register tmp = LockingMode == LIGHTWEIGHT ? op->scratch_opr()->as_register() : noreg;\n@@ -3512,1 +3513,1 @@\n-    int null_check_offset = __ lock_object(hdr, obj, lock, *op->stub()->entry());\n+    int null_check_offset = __ lock_object(hdr, obj, lock, tmp, *op->stub()->entry());\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRAssembler_x86.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -322,1 +322,2 @@\n-  monitor_enter(obj.result(), lock, syncTempOpr(), LIR_OprFact::illegalOpr,\n+  LIR_Opr tmp = LockingMode == LIGHTWEIGHT ? new_register(T_ADDRESS) : LIR_OprFact::illegalOpr;\n+  monitor_enter(obj.result(), lock, syncTempOpr(), tmp,\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRGenerator_x86.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -41,1 +41,1 @@\n-int C1_MacroAssembler::lock_object(Register hdr, Register obj, Register disp_hdr, Label& slow_case) {\n+int C1_MacroAssembler::lock_object(Register hdr, Register obj, Register disp_hdr, Register tmp, Label& slow_case) {\n@@ -45,2 +45,1 @@\n-  assert(hdr != obj && hdr != disp_hdr && obj != disp_hdr, \"registers must be different\");\n-  Label done;\n+  assert_different_registers(hdr, obj, disp_hdr, tmp);\n@@ -65,33 +64,45 @@\n-  \/\/ and mark it as unlocked\n-  orptr(hdr, markWord::unlocked_value);\n-  \/\/ save unlocked object header into the displaced header location on the stack\n-  movptr(Address(disp_hdr, 0), hdr);\n-  \/\/ test if object header is still the same (i.e. unlocked), and if so, store the\n-  \/\/ displaced header address in the object header - if it is not the same, get the\n-  \/\/ object header instead\n-  MacroAssembler::lock(); \/\/ must be immediately before cmpxchg!\n-  cmpxchgptr(disp_hdr, Address(obj, hdr_offset));\n-  \/\/ if the object header was the same, we're done\n-  jcc(Assembler::equal, done);\n-  \/\/ if the object header was not the same, it is now in the hdr register\n-  \/\/ => test if it is a stack pointer into the same stack (recursive locking), i.e.:\n-  \/\/\n-  \/\/ 1) (hdr & aligned_mask) == 0\n-  \/\/ 2) rsp <= hdr\n-  \/\/ 3) hdr <= rsp + page_size\n-  \/\/\n-  \/\/ these 3 tests can be done by evaluating the following expression:\n-  \/\/\n-  \/\/ (hdr - rsp) & (aligned_mask - page_size)\n-  \/\/\n-  \/\/ assuming both the stack pointer and page_size have their least\n-  \/\/ significant 2 bits cleared and page_size is a power of 2\n-  subptr(hdr, rsp);\n-  andptr(hdr, aligned_mask - (int)os::vm_page_size());\n-  \/\/ for recursive locking, the result is zero => save it in the displaced header\n-  \/\/ location (null in the displaced hdr location indicates recursive locking)\n-  movptr(Address(disp_hdr, 0), hdr);\n-  \/\/ otherwise we don't care about the result and handle locking via runtime call\n-  jcc(Assembler::notZero, slow_case);\n-  \/\/ done\n-  bind(done);\n+\n+  if (LockingMode == LIGHTWEIGHT) {\n+#ifdef _LP64\n+    const Register thread = r15_thread;\n+#else\n+    const Register thread = disp_hdr;\n+    get_thread(thread);\n+#endif\n+    fast_lock_impl(obj, hdr, thread, tmp, slow_case);\n+  } else  if (LockingMode == LEGACY) {\n+    Label done;\n+    \/\/ and mark it as unlocked\n+    orptr(hdr, markWord::unlocked_value);\n+    \/\/ save unlocked object header into the displaced header location on the stack\n+    movptr(Address(disp_hdr, 0), hdr);\n+    \/\/ test if object header is still the same (i.e. unlocked), and if so, store the\n+    \/\/ displaced header address in the object header - if it is not the same, get the\n+    \/\/ object header instead\n+    MacroAssembler::lock(); \/\/ must be immediately before cmpxchg!\n+    cmpxchgptr(disp_hdr, Address(obj, hdr_offset));\n+    \/\/ if the object header was the same, we're done\n+    jcc(Assembler::equal, done);\n+    \/\/ if the object header was not the same, it is now in the hdr register\n+    \/\/ => test if it is a stack pointer into the same stack (recursive locking), i.e.:\n+    \/\/\n+    \/\/ 1) (hdr & aligned_mask) == 0\n+    \/\/ 2) rsp <= hdr\n+    \/\/ 3) hdr <= rsp + page_size\n+    \/\/\n+    \/\/ these 3 tests can be done by evaluating the following expression:\n+    \/\/\n+    \/\/ (hdr - rsp) & (aligned_mask - page_size)\n+    \/\/\n+    \/\/ assuming both the stack pointer and page_size have their least\n+    \/\/ significant 2 bits cleared and page_size is a power of 2\n+    subptr(hdr, rsp);\n+    andptr(hdr, aligned_mask - (int)os::vm_page_size());\n+    \/\/ for recursive locking, the result is zero => save it in the displaced header\n+    \/\/ location (null in the displaced hdr location indicates recursive locking)\n+    movptr(Address(disp_hdr, 0), hdr);\n+    \/\/ otherwise we don't care about the result and handle locking via runtime call\n+    jcc(Assembler::notZero, slow_case);\n+    \/\/ done\n+    bind(done);\n+  }\n@@ -111,6 +122,9 @@\n-  \/\/ load displaced header\n-  movptr(hdr, Address(disp_hdr, 0));\n-  \/\/ if the loaded hdr is null we had recursive locking\n-  testptr(hdr, hdr);\n-  \/\/ if we had recursive locking, we are done\n-  jcc(Assembler::zero, done);\n+  if (LockingMode != LIGHTWEIGHT) {\n+    \/\/ load displaced header\n+    movptr(hdr, Address(disp_hdr, 0));\n+    \/\/ if the loaded hdr is null we had recursive locking\n+    testptr(hdr, hdr);\n+    \/\/ if we had recursive locking, we are done\n+    jcc(Assembler::zero, done);\n+  }\n+\n@@ -119,1 +133,0 @@\n-\n@@ -121,10 +134,0 @@\n-  \/\/ test if object header is pointing to the displaced header, and if so, restore\n-  \/\/ the displaced header in the object - if the object header is not pointing to\n-  \/\/ the displaced header, get the object header instead\n-  MacroAssembler::lock(); \/\/ must be immediately before cmpxchg!\n-  cmpxchgptr(hdr, Address(obj, hdr_offset));\n-  \/\/ if the object header was not pointing to the displaced header,\n-  \/\/ we do unlocking via runtime call\n-  jcc(Assembler::notEqual, slow_case);\n-  \/\/ done\n-  bind(done);\n@@ -132,0 +135,16 @@\n+  if (LockingMode == LIGHTWEIGHT) {\n+    movptr(disp_hdr, Address(obj, hdr_offset));\n+    andptr(disp_hdr, ~(int32_t)markWord::lock_mask_in_place);\n+    fast_unlock_impl(obj, disp_hdr, hdr, slow_case);\n+  } else if (LockingMode == LEGACY) {\n+    \/\/ test if object header is pointing to the displaced header, and if so, restore\n+    \/\/ the displaced header in the object - if the object header is not pointing to\n+    \/\/ the displaced header, get the object header instead\n+    MacroAssembler::lock(); \/\/ must be immediately before cmpxchg!\n+    cmpxchgptr(hdr, Address(obj, hdr_offset));\n+    \/\/ if the object header was not pointing to the displaced header,\n+    \/\/ we do unlocking via runtime call\n+    jcc(Assembler::notEqual, slow_case);\n+    \/\/ done\n+  }\n+  bind(done);\n","filename":"src\/hotspot\/cpu\/x86\/c1_MacroAssembler_x86.cpp","additions":72,"deletions":53,"binary":false,"changes":125,"status":"modified"},{"patch":"@@ -53,1 +53,1 @@\n-  int lock_object  (Register swap, Register obj, Register disp_hdr, Label& slow_case);\n+  int lock_object  (Register swap, Register obj, Register disp_hdr, Register tmp, Label& slow_case);\n","filename":"src\/hotspot\/cpu\/x86\/c1_MacroAssembler_x86.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"runtime\/objectMonitor.hpp\"\n@@ -75,0 +76,22 @@\n+#ifdef _LP64\n+int C2HandleAnonOMOwnerStub::max_size() const {\n+  \/\/ Max size of stub has been determined by testing with 0, in which case\n+  \/\/ C2CodeStubList::emit() will throw an assertion and report the actual size that\n+  \/\/ is needed.\n+  return 33;\n+}\n+\n+void C2HandleAnonOMOwnerStub::emit(C2_MacroAssembler& masm) {\n+  __ bind(entry());\n+  Register mon = monitor();\n+  Register t = tmp();\n+  __ movptr(Address(mon, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)), r15_thread);\n+  __ subl(Address(r15_thread, JavaThread::lock_stack_top_offset()), oopSize);\n+#ifdef ASSERT\n+  __ movl(t, Address(r15_thread, JavaThread::lock_stack_top_offset()));\n+  __ movptr(Address(r15_thread, t), 0);\n+#endif\n+  __ jmp(continuation());\n+}\n+#endif\n+\n","filename":"src\/hotspot\/cpu\/x86\/c2_CodeStubs_x86.cpp","additions":23,"deletions":0,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -551,1 +551,1 @@\n-                                 Register scrReg, Register cx1Reg, Register cx2Reg,\n+                                 Register scrReg, Register cx1Reg, Register cx2Reg, Register thread,\n@@ -602,1 +602,1 @@\n-  jccb(Assembler::notZero, IsInflated);\n+  jcc(Assembler::notZero, IsInflated);\n@@ -605,14 +605,19 @@\n-    \/\/ Attempt stack-locking ...\n-    orptr (tmpReg, markWord::unlocked_value);\n-    movptr(Address(boxReg, 0), tmpReg);          \/\/ Anticipate successful CAS\n-    lock();\n-    cmpxchgptr(boxReg, Address(objReg, oopDesc::mark_offset_in_bytes()));      \/\/ Updates tmpReg\n-    jcc(Assembler::equal, COUNT);           \/\/ Success\n-\n-    \/\/ Recursive locking.\n-    \/\/ The object is stack-locked: markword contains stack pointer to BasicLock.\n-    \/\/ Locked by current thread if difference with current SP is less than one page.\n-    subptr(tmpReg, rsp);\n-    \/\/ Next instruction set ZFlag == 1 (Success) if difference is less then one page.\n-    andptr(tmpReg, (int32_t) (NOT_LP64(0xFFFFF003) LP64_ONLY(7 - (int)os::vm_page_size())) );\n-    movptr(Address(boxReg, 0), tmpReg);\n+    if (LockingMode == LIGHTWEIGHT) {\n+      fast_lock_impl(objReg, tmpReg, thread, scrReg, NO_COUNT);\n+      jmp(COUNT);\n+    } else if (LockingMode == LEGACY) {\n+      \/\/ Attempt stack-locking ...\n+      orptr (tmpReg, markWord::unlocked_value);\n+      movptr(Address(boxReg, 0), tmpReg);          \/\/ Anticipate successful CAS\n+      lock();\n+      cmpxchgptr(boxReg, Address(objReg, oopDesc::mark_offset_in_bytes()));      \/\/ Updates tmpReg\n+      jcc(Assembler::equal, COUNT);           \/\/ Success\n+\n+      \/\/ Recursive locking.\n+      \/\/ The object is stack-locked: markword contains stack pointer to BasicLock.\n+      \/\/ Locked by current thread if difference with current SP is less than one page.\n+      subptr(tmpReg, rsp);\n+      \/\/ Next instruction set ZFlag == 1 (Success) if difference is less then one page.\n+      andptr(tmpReg, (int32_t) (NOT_LP64(0xFFFFF003) LP64_ONLY(7 - (int)os::vm_page_size())) );\n+      movptr(Address(boxReg, 0), tmpReg);\n+    }\n@@ -650,2 +655,1 @@\n-  \/\/ Optimistic form: consider XORL tmpReg,tmpReg\n-  movptr(tmpReg, NULL_WORD);\n+  xorptr(tmpReg, tmpReg);\n@@ -653,7 +657,1 @@\n-  \/\/ Appears unlocked - try to swing _owner from null to non-null.\n-  \/\/ Ideally, I'd manifest \"Self\" with get_thread and then attempt\n-  \/\/ to CAS the register containing Self into m->Owner.\n-  \/\/ But we don't have enough registers, so instead we can either try to CAS\n-  \/\/ rsp or the address of the box (in scr) into &m->owner.  If the CAS succeeds\n-  \/\/ we later store \"Self\" into m->Owner.  Transiently storing a stack address\n-  \/\/ (rsp or the address of the box) into  m->owner is harmless.\n+  \/\/ Appears unlocked - try to swing _owner from null to current thread.\n@@ -662,1 +660,1 @@\n-  cmpxchgptr(scrReg, Address(boxReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)));\n+  cmpxchgptr(thread, Address(boxReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)));\n@@ -684,1 +682,1 @@\n-  cmpxchgptr(r15_thread, Address(scrReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)));\n+  cmpxchgptr(thread, Address(scrReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)));\n@@ -691,1 +689,1 @@\n-  cmpptr(r15_thread, rax);                \/\/ Check if we are already the owner (recursive lock)\n+  cmpptr(thread, rax);                \/\/ Check if we are already the owner (recursive lock)\n@@ -707,6 +705,1 @@\n-#ifndef _LP64\n-  get_thread(tmpReg);\n-  incrementl(Address(tmpReg, JavaThread::held_monitor_count_offset()));\n-#else \/\/ _LP64\n-  incrementq(Address(r15_thread, JavaThread::held_monitor_count_offset()));\n-#endif\n+  increment(Address(thread, JavaThread::held_monitor_count_offset()));\n@@ -776,1 +769,1 @@\n-  if (!UseHeavyMonitors) {\n+  if (LockingMode == LEGACY) {\n@@ -783,1 +776,15 @@\n-    jccb   (Assembler::zero, Stacked);\n+    jcc(Assembler::zero, Stacked);\n+    if (LockingMode == LIGHTWEIGHT) {\n+      \/\/ If the owner is ANONYMOUS, we need to fix it -  in an outline stub.\n+      testb(Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)), (int32_t) ObjectMonitor::ANONYMOUS_OWNER);\n+#ifdef _LP64\n+      C2HandleAnonOMOwnerStub* stub = new (Compile::current()->comp_arena()) C2HandleAnonOMOwnerStub(tmpReg, boxReg);\n+      Compile::current()->output()->add_stub(stub);\n+      jcc(Assembler::notEqual, stub->entry());\n+      bind(stub->continuation());\n+#else\n+      \/\/ We can't easily implement this optimization on 32 bit because we don't have a thread register.\n+      \/\/ Call the slow-path instead.\n+      jcc(Assembler::notEqual, NO_COUNT);\n+#endif\n+    }\n@@ -795,1 +802,1 @@\n-    jmpb(DONE_LABEL);\n+    jmp(DONE_LABEL);\n@@ -909,3 +916,9 @@\n-    movptr(tmpReg, Address (boxReg, 0));      \/\/ re-fetch\n-    lock();\n-    cmpxchgptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes())); \/\/ Uses RAX which is box\n+    if (LockingMode == LIGHTWEIGHT) {\n+      mov(boxReg, tmpReg);\n+      fast_unlock_impl(objReg, boxReg, tmpReg, NO_COUNT);\n+      jmp(COUNT);\n+    } else if (LockingMode == LEGACY) {\n+      movptr(tmpReg, Address (boxReg, 0));      \/\/ re-fetch\n+      lock();\n+      cmpxchgptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes())); \/\/ Uses RAX which is box\n+    }\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":53,"deletions":40,"binary":false,"changes":93,"status":"modified"},{"patch":"@@ -39,1 +39,1 @@\n-                 Register scr, Register cx1, Register cx2,\n+                 Register scr, Register cx1, Register cx2, Register thread,\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1226,53 +1226,65 @@\n-    \/\/ Load immediate 1 into swap_reg %rax\n-    movl(swap_reg, 1);\n-\n-    \/\/ Load (object->mark() | 1) into swap_reg %rax\n-    orptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-\n-    \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n-    movptr(Address(lock_reg, mark_offset), swap_reg);\n-\n-    assert(lock_offset == 0,\n-           \"displaced header must be first word in BasicObjectLock\");\n-\n-    lock();\n-    cmpxchgptr(lock_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-    jcc(Assembler::zero, count_locking);\n-\n-    const int zero_bits = LP64_ONLY(7) NOT_LP64(3);\n-\n-    \/\/ Fast check for recursive lock.\n-    \/\/\n-    \/\/ Can apply the optimization only if this is a stack lock\n-    \/\/ allocated in this thread. For efficiency, we can focus on\n-    \/\/ recently allocated stack locks (instead of reading the stack\n-    \/\/ base and checking whether 'mark' points inside the current\n-    \/\/ thread stack):\n-    \/\/  1) (mark & zero_bits) == 0, and\n-    \/\/  2) rsp <= mark < mark + os::pagesize()\n-    \/\/\n-    \/\/ Warning: rsp + os::pagesize can overflow the stack base. We must\n-    \/\/ neither apply the optimization for an inflated lock allocated\n-    \/\/ just above the thread stack (this is why condition 1 matters)\n-    \/\/ nor apply the optimization if the stack lock is inside the stack\n-    \/\/ of another thread. The latter is avoided even in case of overflow\n-    \/\/ because we have guard pages at the end of all stacks. Hence, if\n-    \/\/ we go over the stack base and hit the stack of another thread,\n-    \/\/ this should not be in a writeable area that could contain a\n-    \/\/ stack lock allocated by that thread. As a consequence, a stack\n-    \/\/ lock less than page size away from rsp is guaranteed to be\n-    \/\/ owned by the current thread.\n-    \/\/\n-    \/\/ These 3 tests can be done by evaluating the following\n-    \/\/ expression: ((mark - rsp) & (zero_bits - os::vm_page_size())),\n-    \/\/ assuming both stack pointer and pagesize have their\n-    \/\/ least significant bits clear.\n-    \/\/ NOTE: the mark is in swap_reg %rax as the result of cmpxchg\n-    subptr(swap_reg, rsp);\n-    andptr(swap_reg, zero_bits - (int)os::vm_page_size());\n-\n-    \/\/ Save the test result, for recursive case, the result is zero\n-    movptr(Address(lock_reg, mark_offset), swap_reg);\n-    jcc(Assembler::notZero, slow_case);\n-\n-    bind(count_locking);\n+    if (LockingMode == LIGHTWEIGHT) {\n+#ifdef _LP64\n+      const Register thread = r15_thread;\n+#else\n+      const Register thread = lock_reg;\n+      get_thread(thread);\n+#endif\n+      \/\/ Load object header, prepare for CAS from unlocked to locked.\n+      movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      fast_lock_impl(obj_reg, swap_reg, thread, tmp_reg, slow_case);\n+    } else if (LockingMode == LEGACY) {\n+      \/\/ Load immediate 1 into swap_reg %rax\n+      movl(swap_reg, 1);\n+\n+      \/\/ Load (object->mark() | 1) into swap_reg %rax\n+      orptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+\n+      \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n+      movptr(Address(lock_reg, mark_offset), swap_reg);\n+\n+      assert(lock_offset == 0,\n+             \"displaced header must be first word in BasicObjectLock\");\n+\n+      lock();\n+      cmpxchgptr(lock_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      jcc(Assembler::zero, count_locking);\n+\n+      const int zero_bits = LP64_ONLY(7) NOT_LP64(3);\n+\n+      \/\/ Fast check for recursive lock.\n+      \/\/\n+      \/\/ Can apply the optimization only if this is a stack lock\n+      \/\/ allocated in this thread. For efficiency, we can focus on\n+      \/\/ recently allocated stack locks (instead of reading the stack\n+      \/\/ base and checking whether 'mark' points inside the current\n+      \/\/ thread stack):\n+      \/\/  1) (mark & zero_bits) == 0, and\n+      \/\/  2) rsp <= mark < mark + os::pagesize()\n+      \/\/\n+      \/\/ Warning: rsp + os::pagesize can overflow the stack base. We must\n+      \/\/ neither apply the optimization for an inflated lock allocated\n+      \/\/ just above the thread stack (this is why condition 1 matters)\n+      \/\/ nor apply the optimization if the stack lock is inside the stack\n+      \/\/ of another thread. The latter is avoided even in case of overflow\n+      \/\/ because we have guard pages at the end of all stacks. Hence, if\n+      \/\/ we go over the stack base and hit the stack of another thread,\n+      \/\/ this should not be in a writeable area that could contain a\n+      \/\/ stack lock allocated by that thread. As a consequence, a stack\n+      \/\/ lock less than page size away from rsp is guaranteed to be\n+      \/\/ owned by the current thread.\n+      \/\/\n+      \/\/ These 3 tests can be done by evaluating the following\n+      \/\/ expression: ((mark - rsp) & (zero_bits - os::vm_page_size())),\n+      \/\/ assuming both stack pointer and pagesize have their\n+      \/\/ least significant bits clear.\n+      \/\/ NOTE: the mark is in swap_reg %rax as the result of cmpxchg\n+      subptr(swap_reg, rsp);\n+      andptr(swap_reg, zero_bits - (int)os::vm_page_size());\n+\n+      \/\/ Save the test result, for recursive case, the result is zero\n+      movptr(Address(lock_reg, mark_offset), swap_reg);\n+      jcc(Assembler::notZero, slow_case);\n+\n+      bind(count_locking);\n+    }\n@@ -1285,4 +1297,9 @@\n-    call_VM(noreg,\n-            CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter),\n-            lock_reg);\n-\n+    if (LockingMode == LIGHTWEIGHT) {\n+      call_VM(noreg,\n+              CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter_obj),\n+              obj_reg);\n+    } else {\n+      call_VM(noreg,\n+              CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter),\n+              lock_reg);\n+    }\n@@ -1321,3 +1338,5 @@\n-    \/\/ Convert from BasicObjectLock structure to object and BasicLock\n-    \/\/ structure Store the BasicLock address into %rax\n-    lea(swap_reg, Address(lock_reg, BasicObjectLock::lock_offset_in_bytes()));\n+    if (LockingMode != LIGHTWEIGHT) {\n+      \/\/ Convert from BasicObjectLock structure to object and BasicLock\n+      \/\/ structure Store the BasicLock address into %rax\n+      lea(swap_reg, Address(lock_reg, BasicObjectLock::lock_offset_in_bytes()));\n+    }\n@@ -1331,16 +1350,33 @@\n-    \/\/ Load the old header from BasicLock structure\n-    movptr(header_reg, Address(swap_reg,\n-                               BasicLock::displaced_header_offset_in_bytes()));\n-\n-    \/\/ Test for recursion\n-    testptr(header_reg, header_reg);\n-\n-    \/\/ zero for recursive case\n-    jcc(Assembler::zero, count_locking);\n-\n-    \/\/ Atomic swap back the old header\n-    lock();\n-    cmpxchgptr(header_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-\n-    \/\/ zero for simple unlock of a stack-lock case\n-    jcc(Assembler::notZero, slow_case);\n+    if (LockingMode == LIGHTWEIGHT) {\n+#ifdef _LP64\n+      const Register thread = r15_thread;\n+#else\n+      const Register thread = header_reg;\n+      get_thread(thread);\n+#endif\n+      \/\/ Handle unstructured locking.\n+      Register tmp = swap_reg;\n+      movl(tmp, Address(thread, JavaThread::lock_stack_top_offset()));\n+      cmpptr(obj_reg, Address(thread, tmp, Address::times_1,  -oopSize));\n+      jcc(Assembler::notEqual, slow_case);\n+      \/\/ Try to swing header from locked to unlocked.\n+      movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      andptr(swap_reg, ~(int32_t)markWord::lock_mask_in_place);\n+      fast_unlock_impl(obj_reg, swap_reg, header_reg, slow_case);\n+    } else if (LockingMode == LEGACY) {\n+      \/\/ Load the old header from BasicLock structure\n+      movptr(header_reg, Address(swap_reg,\n+                                 BasicLock::displaced_header_offset_in_bytes()));\n+\n+      \/\/ Test for recursion\n+      testptr(header_reg, header_reg);\n+\n+      \/\/ zero for recursive case\n+      jcc(Assembler::zero, count_locking);\n+\n+      \/\/ Atomic swap back the old header\n+      lock();\n+      cmpxchgptr(header_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+\n+      \/\/ zero for simple unlock of a stack-lock case\n+      jcc(Assembler::notZero, slow_case);\n@@ -1348,1 +1384,2 @@\n-    bind(count_locking);\n+      bind(count_locking);\n+    }\n","filename":"src\/hotspot\/cpu\/x86\/interp_masm_x86.cpp","additions":114,"deletions":77,"binary":false,"changes":191,"status":"modified"},{"patch":"@@ -9678,0 +9678,67 @@\n+\n+\/\/ Implements fast-locking.\n+\/\/ Branches to slow upon failure to lock the object, with ZF cleared.\n+\/\/ Falls through upon success with unspecified ZF.\n+\/\/\n+\/\/ obj: the object to be locked\n+\/\/ hdr: the (pre-loaded) header of the object, must be rax\n+\/\/ thread: the thread which attempts to lock obj\n+\/\/ tmp: a temporary register\n+void MacroAssembler::fast_lock_impl(Register obj, Register hdr, Register thread, Register tmp, Label& slow) {\n+  assert(hdr == rax, \"header must be in rax for cmpxchg\");\n+  assert_different_registers(obj, hdr, thread, tmp);\n+\n+  \/\/ First we need to check if the lock-stack has room for pushing the object reference.\n+  \/\/ Note: we subtract 1 from the end-offset so that we can do a 'greater' comparison, instead\n+  \/\/ of 'greaterEqual' below, which readily clears the ZF. This makes C2 code a little simpler and\n+  \/\/ avoids one branch.\n+  cmpl(Address(thread, JavaThread::lock_stack_top_offset()), LockStack::end_offset() - 1);\n+  jcc(Assembler::greater, slow);\n+\n+  \/\/ Now we attempt to take the fast-lock.\n+  \/\/ Clear lowest two header bits (locked state).\n+  andptr(hdr, ~(int32_t)markWord::lock_mask_in_place);\n+  movptr(tmp, hdr);\n+  \/\/ Set lowest bit (unlocked state).\n+  orptr(hdr, markWord::unlocked_value);\n+  lock();\n+  cmpxchgptr(tmp, Address(obj, oopDesc::mark_offset_in_bytes()));\n+  jcc(Assembler::notEqual, slow);\n+\n+  \/\/ If successful, push object to lock-stack.\n+  movl(tmp, Address(thread, JavaThread::lock_stack_top_offset()));\n+  movptr(Address(thread, tmp), obj);\n+  incrementl(tmp, oopSize);\n+  movl(Address(thread, JavaThread::lock_stack_top_offset()), tmp);\n+}\n+\n+\/\/ Implements fast-unlocking.\n+\/\/ Branches to slow upon failure, with ZF cleared.\n+\/\/ Falls through upon success, with unspecified ZF.\n+\/\/\n+\/\/ obj: the object to be unlocked\n+\/\/ hdr: the (pre-loaded) header of the object, must be rax\n+\/\/ tmp: a temporary register\n+void MacroAssembler::fast_unlock_impl(Register obj, Register hdr, Register tmp, Label& slow) {\n+  assert(hdr == rax, \"header must be in rax for cmpxchg\");\n+  assert_different_registers(obj, hdr, tmp);\n+\n+  \/\/ Mark-word must be 00 now, try to swing it back to 01 (unlocked)\n+  movptr(tmp, hdr); \/\/ The expected old value\n+  orptr(tmp, markWord::unlocked_value);\n+  lock();\n+  cmpxchgptr(tmp, Address(obj, oopDesc::mark_offset_in_bytes()));\n+  jcc(Assembler::notEqual, slow);\n+  \/\/ Pop the lock object from the lock-stack.\n+#ifdef _LP64\n+  const Register thread = r15_thread;\n+#else\n+  const Register thread = rax;\n+  get_thread(thread);\n+#endif\n+  subl(Address(thread, JavaThread::lock_stack_top_offset()), oopSize);\n+#ifdef ASSERT\n+  movl(tmp, Address(thread, JavaThread::lock_stack_top_offset()));\n+  movptr(Address(thread, tmp), 0);\n+#endif\n+}\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":67,"deletions":0,"binary":false,"changes":67,"status":"modified"},{"patch":"@@ -152,0 +152,2 @@\n+  void increment(Address dst, int value = 1)  { LP64_ONLY(incrementq(dst, value)) NOT_LP64(incrementl(dst, value)) ; }\n+  void decrement(Address dst, int value = 1)  { LP64_ONLY(decrementq(dst, value)) NOT_LP64(decrementl(dst, value)) ; }\n@@ -2011,0 +2013,2 @@\n+  void fast_lock_impl(Register obj, Register hdr, Register thread, Register tmp, Label& slow);\n+  void fast_unlock_impl(Register obj, Register hdr, Register tmp, Label& slow);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1684,30 +1684,36 @@\n-      \/\/ Load immediate 1 into swap_reg %rax,\n-      __ movptr(swap_reg, 1);\n-\n-      \/\/ Load (object->mark() | 1) into swap_reg %rax,\n-      __ orptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-\n-      \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n-      __ movptr(Address(lock_reg, mark_word_offset), swap_reg);\n-\n-      \/\/ src -> dest iff dest == rax, else rax, <- dest\n-      \/\/ *obj_reg = lock_reg iff *obj_reg == rax, else rax, = *(obj_reg)\n-      __ lock();\n-      __ cmpxchgptr(lock_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-      __ jcc(Assembler::equal, count_mon);\n-\n-      \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n-      \/\/  1) (mark & 3) == 0, and\n-      \/\/  2) rsp <= mark < mark + os::pagesize()\n-      \/\/ These 3 tests can be done by evaluating the following\n-      \/\/ expression: ((mark - rsp) & (3 - os::vm_page_size())),\n-      \/\/ assuming both stack pointer and pagesize have their\n-      \/\/ least significant 2 bits clear.\n-      \/\/ NOTE: the oopMark is in swap_reg %rax, as the result of cmpxchg\n-\n-      __ subptr(swap_reg, rsp);\n-      __ andptr(swap_reg, 3 - (int)os::vm_page_size());\n-\n-      \/\/ Save the test result, for recursive case, the result is zero\n-      __ movptr(Address(lock_reg, mark_word_offset), swap_reg);\n-      __ jcc(Assembler::notEqual, slow_path_lock);\n+      if (LockingMode == LIGHTWEIGHT) {\n+        \/\/ Load object header\n+        __ movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+        __ fast_lock_impl(obj_reg, swap_reg, thread, lock_reg, slow_path_lock);\n+      } else if (LockingMode == LEGACY) {\n+        \/\/ Load immediate 1 into swap_reg %rax,\n+        __ movptr(swap_reg, 1);\n+\n+        \/\/ Load (object->mark() | 1) into swap_reg %rax,\n+        __ orptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+\n+        \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n+        __ movptr(Address(lock_reg, mark_word_offset), swap_reg);\n+\n+        \/\/ src -> dest iff dest == rax, else rax, <- dest\n+        \/\/ *obj_reg = lock_reg iff *obj_reg == rax, else rax, = *(obj_reg)\n+        __ lock();\n+        __ cmpxchgptr(lock_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+        __ jcc(Assembler::equal, count_mon);\n+\n+        \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n+        \/\/  1) (mark & 3) == 0, and\n+        \/\/  2) rsp <= mark < mark + os::pagesize()\n+        \/\/ These 3 tests can be done by evaluating the following\n+        \/\/ expression: ((mark - rsp) & (3 - os::vm_page_size())),\n+        \/\/ assuming both stack pointer and pagesize have their\n+        \/\/ least significant 2 bits clear.\n+        \/\/ NOTE: the oopMark is in swap_reg %rax, as the result of cmpxchg\n+\n+        __ subptr(swap_reg, rsp);\n+        __ andptr(swap_reg, 3 - (int)os::vm_page_size());\n+\n+        \/\/ Save the test result, for recursive case, the result is zero\n+        __ movptr(Address(lock_reg, mark_word_offset), swap_reg);\n+        __ jcc(Assembler::notEqual, slow_path_lock);\n+      }\n@@ -1837,1 +1843,1 @@\n-    if (!UseHeavyMonitors) {\n+    if (LockingMode == LEGACY) {\n@@ -1853,12 +1859,18 @@\n-      \/\/  get old displaced header\n-      __ movptr(rbx, Address(rbp, lock_slot_rbp_offset));\n-\n-      \/\/ get address of the stack lock\n-      __ lea(rax, Address(rbp, lock_slot_rbp_offset));\n-\n-      \/\/ Atomic swap old header if oop still contains the stack lock\n-      \/\/ src -> dest iff dest == rax, else rax, <- dest\n-      \/\/ *obj_reg = rbx, iff *obj_reg == rax, else rax, = *(obj_reg)\n-      __ lock();\n-      __ cmpxchgptr(rbx, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-      __ jcc(Assembler::notEqual, slow_path_unlock);\n+      if (LockingMode == LIGHTWEIGHT) {\n+        __ movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+        __ andptr(swap_reg, ~(int32_t)markWord::lock_mask_in_place);\n+        __ fast_unlock_impl(obj_reg, swap_reg, lock_reg, slow_path_unlock);\n+      } else if (LockingMode == LEGACY) {\n+        \/\/  get old displaced header\n+        __ movptr(rbx, Address(rbp, lock_slot_rbp_offset));\n+\n+        \/\/ get address of the stack lock\n+        __ lea(rax, Address(rbp, lock_slot_rbp_offset));\n+\n+        \/\/ Atomic swap old header if oop still contains the stack lock\n+        \/\/ src -> dest iff dest == rax, else rax, <- dest\n+        \/\/ *obj_reg = rbx, iff *obj_reg == rax, else rax, = *(obj_reg)\n+        __ lock();\n+        __ cmpxchgptr(rbx, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+        __ jcc(Assembler::notEqual, slow_path_unlock);\n+      }\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_32.cpp","additions":55,"deletions":43,"binary":false,"changes":98,"status":"modified"},{"patch":"@@ -2153,32 +2153,37 @@\n-\n-      \/\/ Load immediate 1 into swap_reg %rax\n-      __ movl(swap_reg, 1);\n-\n-      \/\/ Load (object->mark() | 1) into swap_reg %rax\n-      __ orptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-\n-      \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n-      __ movptr(Address(lock_reg, mark_word_offset), swap_reg);\n-\n-      \/\/ src -> dest iff dest == rax else rax <- dest\n-      __ lock();\n-      __ cmpxchgptr(lock_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-      __ jcc(Assembler::equal, count_mon);\n-\n-      \/\/ Hmm should this move to the slow path code area???\n-\n-      \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n-      \/\/  1) (mark & 3) == 0, and\n-      \/\/  2) rsp <= mark < mark + os::pagesize()\n-      \/\/ These 3 tests can be done by evaluating the following\n-      \/\/ expression: ((mark - rsp) & (3 - os::vm_page_size())),\n-      \/\/ assuming both stack pointer and pagesize have their\n-      \/\/ least significant 2 bits clear.\n-      \/\/ NOTE: the oopMark is in swap_reg %rax as the result of cmpxchg\n-\n-      __ subptr(swap_reg, rsp);\n-      __ andptr(swap_reg, 3 - (int)os::vm_page_size());\n-\n-      \/\/ Save the test result, for recursive case, the result is zero\n-      __ movptr(Address(lock_reg, mark_word_offset), swap_reg);\n-      __ jcc(Assembler::notEqual, slow_path_lock);\n+      if (LockingMode == LIGHTWEIGHT) {\n+        \/\/ Load object header\n+        __ movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+        __ fast_lock_impl(obj_reg, swap_reg, r15_thread, rscratch1, slow_path_lock);\n+      } else if (LockingMode == LEGACY) {\n+        \/\/ Load immediate 1 into swap_reg %rax\n+        __ movl(swap_reg, 1);\n+\n+        \/\/ Load (object->mark() | 1) into swap_reg %rax\n+        __ orptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+\n+        \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n+        __ movptr(Address(lock_reg, mark_word_offset), swap_reg);\n+\n+        \/\/ src -> dest iff dest == rax else rax <- dest\n+        __ lock();\n+        __ cmpxchgptr(lock_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+        __ jcc(Assembler::equal, count_mon);\n+\n+        \/\/ Hmm should this move to the slow path code area???\n+\n+        \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n+        \/\/  1) (mark & 3) == 0, and\n+        \/\/  2) rsp <= mark < mark + os::pagesize()\n+        \/\/ These 3 tests can be done by evaluating the following\n+        \/\/ expression: ((mark - rsp) & (3 - os::vm_page_size())),\n+        \/\/ assuming both stack pointer and pagesize have their\n+        \/\/ least significant 2 bits clear.\n+        \/\/ NOTE: the oopMark is in swap_reg %rax as the result of cmpxchg\n+\n+        __ subptr(swap_reg, rsp);\n+        __ andptr(swap_reg, 3 - (int)os::vm_page_size());\n+\n+        \/\/ Save the test result, for recursive case, the result is zero\n+        __ movptr(Address(lock_reg, mark_word_offset), swap_reg);\n+        __ jcc(Assembler::notEqual, slow_path_lock);\n+      }\n@@ -2298,1 +2303,1 @@\n-    if (!UseHeavyMonitors) {\n+    if (LockingMode == LEGACY) {\n@@ -2314,9 +2319,15 @@\n-      \/\/ get address of the stack lock\n-      __ lea(rax, Address(rsp, lock_slot_offset * VMRegImpl::stack_slot_size));\n-      \/\/  get old displaced header\n-      __ movptr(old_hdr, Address(rax, 0));\n-\n-      \/\/ Atomic swap old header if oop still contains the stack lock\n-      __ lock();\n-      __ cmpxchgptr(old_hdr, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-      __ jcc(Assembler::notEqual, slow_path_unlock);\n+      if (LockingMode == LIGHTWEIGHT) {\n+        __ movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+        __ andptr(swap_reg, ~(int32_t)markWord::lock_mask_in_place);\n+        __ fast_unlock_impl(obj_reg, swap_reg, lock_reg, slow_path_unlock);\n+      } else if (LockingMode == LEGACY) {\n+        \/\/ get address of the stack lock\n+        __ lea(rax, Address(rsp, lock_slot_offset * VMRegImpl::stack_slot_size));\n+        \/\/  get old displaced header\n+        __ movptr(old_hdr, Address(rax, 0));\n+\n+        \/\/ Atomic swap old header if oop still contains the stack lock\n+        __ lock();\n+        __ cmpxchgptr(old_hdr, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+        __ jcc(Assembler::notEqual, slow_path_unlock);\n+      }\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_64.cpp","additions":53,"deletions":42,"binary":false,"changes":95,"status":"modified"},{"patch":"@@ -13745,1 +13745,1 @@\n-instruct cmpFastLockRTM(eFlagsReg cr, eRegP object, eBXRegP box, eAXRegI tmp, eDXRegI scr, rRegI cx1, rRegI cx2) %{\n+instruct cmpFastLockRTM(eFlagsReg cr, eRegP object, eBXRegP box, eAXRegI tmp, eDXRegI scr, rRegI cx1, rRegI cx2, eRegP thread) %{\n@@ -13748,1 +13748,1 @@\n-  effect(TEMP tmp, TEMP scr, TEMP cx1, TEMP cx2, USE_KILL box);\n+  effect(TEMP tmp, TEMP scr, TEMP cx1, TEMP cx2, USE_KILL box, TEMP thread);\n@@ -13752,0 +13752,1 @@\n+    __ get_thread($thread$$Register);\n@@ -13753,1 +13754,1 @@\n-                 $scr$$Register, $cx1$$Register, $cx2$$Register,\n+                 $scr$$Register, $cx1$$Register, $cx2$$Register, $thread$$Register,\n@@ -13761,1 +13762,1 @@\n-instruct cmpFastLock(eFlagsReg cr, eRegP object, eBXRegP box, eAXRegI tmp, eRegP scr) %{\n+instruct cmpFastLock(eFlagsReg cr, eRegP object, eBXRegP box, eAXRegI tmp, eRegP scr, eRegP thread) %{\n@@ -13764,1 +13765,1 @@\n-  effect(TEMP tmp, TEMP scr, USE_KILL box);\n+  effect(TEMP tmp, TEMP scr, USE_KILL box, TEMP thread);\n@@ -13768,0 +13769,1 @@\n+    __ get_thread($thread$$Register);\n@@ -13769,1 +13771,1 @@\n-                 $scr$$Register, noreg, noreg, NULL, NULL, NULL, false, false);\n+                 $scr$$Register, noreg, noreg, $thread$$Register, nullptr, nullptr, nullptr, false, false);\n","filename":"src\/hotspot\/cpu\/x86\/x86_32.ad","additions":8,"deletions":6,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -13304,1 +13304,1 @@\n-                 $scr$$Register, $cx1$$Register, $cx2$$Register,\n+                 $scr$$Register, $cx1$$Register, $cx2$$Register, r15_thread,\n@@ -13320,1 +13320,1 @@\n-                 $scr$$Register, noreg, noreg, NULL, NULL, NULL, false, false);\n+                 $scr$$Register, noreg, noreg, r15_thread, nullptr, nullptr, nullptr, false, false);\n","filename":"src\/hotspot\/cpu\/x86\/x86_64.ad","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -760,2 +760,2 @@\n-  assert(obj == lock->obj(), \"must match\");\n-  SharedRuntime::monitor_enter_helper(obj, lock->lock(), current);\n+  assert(LockingMode == LIGHTWEIGHT || obj == lock->obj(), \"must match\");\n+  SharedRuntime::monitor_enter_helper(obj, LockingMode == LIGHTWEIGHT ? nullptr : lock->lock(), current);\n","filename":"src\/hotspot\/share\/c1\/c1_Runtime1.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -738,0 +738,1 @@\n+  assert(LockingMode != LIGHTWEIGHT, \"Should call monitorenter_obj() when using the new lightweight locking\");\n@@ -752,0 +753,16 @@\n+\/\/ NOTE: We provide a separate implementation for the new lightweight locking to workaround a limitation\n+\/\/ of registers in x86_32. This entry point accepts an oop instead of a BasicObjectLock*.\n+\/\/ The problem is that we would need to preserve the register that holds the BasicObjectLock,\n+\/\/ but we are using that register to hold the thread. We don't have enough registers to\n+\/\/ also keep the BasicObjectLock, but we don't really need it anyway, we only need\n+\/\/ the object. See also InterpreterMacroAssembler::lock_object().\n+\/\/ As soon as traditional stack-locking goes away we could remove the other monitorenter() entry\n+\/\/ point, and only use oop-accepting entries (same for monitorexit() below).\n+JRT_ENTRY_NO_ASYNC(void, InterpreterRuntime::monitorenter_obj(JavaThread* current, oopDesc* obj))\n+  assert(LockingMode == LIGHTWEIGHT, \"Should call monitorenter() when not using the new lightweight locking\");\n+  Handle h_obj(current, cast_to_oop(obj));\n+  assert(Universe::heap()->is_in_or_null(h_obj()),\n+         \"must be null or an object\");\n+  ObjectSynchronizer::enter(h_obj, nullptr, current);\n+  return;\n+JRT_END\n","filename":"src\/hotspot\/share\/interpreter\/interpreterRuntime.cpp","additions":17,"deletions":0,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -106,0 +106,1 @@\n+  static void    monitorenter_obj(JavaThread* current, oopDesc* obj);\n","filename":"src\/hotspot\/share\/interpreter\/interpreterRuntime.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -80,0 +80,1 @@\n+  LOG_TAG(fastlock2) \\\n","filename":"src\/hotspot\/share\/logging\/logTag.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -53,1 +53,2 @@\n-\/\/    [ptr             | 00]  locked             ptr points to real header on stack\n+\/\/    [ptr             | 00]  locked             ptr points to real header on stack (stack-locking in use)\n+\/\/    [header          | 00]  locked             locked regular object header (fast-locking in use)\n@@ -55,1 +56,1 @@\n-\/\/    [ptr             | 10]  monitor            inflated lock (header is wapped out)\n+\/\/    [ptr             | 10]  monitor            inflated lock (header is swapped out)\n@@ -57,1 +58,1 @@\n-\/\/    [0 ............ 0| 00]  inflating          inflation in progress\n+\/\/    [0 ............ 0| 00]  inflating          inflation in progress (stack-locking in use)\n@@ -159,0 +160,1 @@\n+  \/\/ Fast-locking does not use INFLATING.\n@@ -173,1 +175,2 @@\n-    return ((value() & lock_mask_in_place) == locked_value);\n+    assert(LockingMode == LEGACY, \"should only be called with traditional stack locking\");\n+    return (value() & lock_mask_in_place) == locked_value;\n@@ -179,0 +182,10 @@\n+\n+  bool is_fast_locked() const {\n+    assert(LockingMode == LIGHTWEIGHT, \"should only be called with new lightweight locking\");\n+    return (value() & lock_mask_in_place) == locked_value;\n+  }\n+  markWord set_fast_locked() const {\n+    \/\/ Clear the lock_mask_in_place bits to set locked_value:\n+    return markWord(value() & ~lock_mask_in_place);\n+  }\n+\n@@ -188,1 +201,3 @@\n-    return ((value() & unlocked_value) == 0);\n+    intptr_t lockbits = value() & lock_mask_in_place;\n+    return LockingMode == LIGHTWEIGHT  ? lockbits == monitor_value   \/\/ monitor?\n+                                       : (lockbits & unlocked_value) == 0; \/\/ monitor | stack-locked?\n","filename":"src\/hotspot\/share\/oops\/markWord.hpp","additions":20,"deletions":5,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -122,1 +122,1 @@\n-  \/\/ at a safepoint, it must not be zero.\n+  \/\/ at a safepoint, it must not be zero, except when using the new lightweight locking.\n@@ -131,1 +131,1 @@\n-  return !SafepointSynchronize::is_at_safepoint();\n+  return LockingMode == LIGHTWEIGHT || !SafepointSynchronize::is_at_safepoint();\n","filename":"src\/hotspot\/share\/oops\/oop.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -89,0 +89,15 @@\n+#ifdef _LP64\n+class C2HandleAnonOMOwnerStub : public C2CodeStub {\n+private:\n+  Register _monitor;\n+  Register _tmp;\n+public:\n+  C2HandleAnonOMOwnerStub(Register monitor, Register tmp = noreg) : C2CodeStub(),\n+    _monitor(monitor), _tmp(tmp) {}\n+  Register monitor() { return _monitor; }\n+  Register tmp() { return _tmp; }\n+  int max_size() const;\n+  void emit(C2_MacroAssembler& masm);\n+};\n+#endif\n+\n","filename":"src\/hotspot\/share\/opto\/c2_CodeStubs.hpp","additions":15,"deletions":0,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -2004,0 +2004,10 @@\n+#if !defined(X86) && !defined(AARCH64) && !defined(RISCV64) && !defined(ARM)\n+  if (LockingMode == LIGHTWEIGHT) {\n+    FLAG_SET_CMDLINE(LockingMode, LEGACY);\n+    warning(\"New lightweight locking not supported on this platform\");\n+  }\n+#endif\n+  if (UseHeavyMonitors) {\n+    FLAG_SET_CMDLINE(LockingMode, 0);\n+  }\n+\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":10,"deletions":0,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -1607,1 +1607,1 @@\n-          if (mark.has_locker() && fr.sp() > (intptr_t*)mark.locker()) {\n+          if (LockingMode == LEGACY && mark.has_locker() && fr.sp() > (intptr_t*)mark.locker()) {\n","filename":"src\/hotspot\/share\/runtime\/deoptimization.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1977,0 +1977,7 @@\n+                                                                            \\\n+  product(int, LockingMode, LEGACY, EXPERIMENTAL,                           \\\n+          \"Select locking mode: \"                                           \\\n+          \"0: monitors only, \"                                              \\\n+          \"1: monitors & traditional stack-locking (default), \"             \\\n+          \"2: monitors & new lightweight locking\")                          \\\n+          range(0, 2)                                                       \\\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -73,0 +73,1 @@\n+#include \"runtime\/lockStack.inline.hpp\"\n@@ -493,2 +494,3 @@\n-  _SleepEvent(ParkEvent::Allocate(this))\n-{\n+  _SleepEvent(ParkEvent::Allocate(this)),\n+\n+  _lock_stack(this) {\n@@ -991,0 +993,1 @@\n+  assert(LockingMode != LIGHTWEIGHT, \"should not be called with new lightweight locking\");\n@@ -1384,0 +1387,4 @@\n+\n+  if (LockingMode == LIGHTWEIGHT) {\n+    lock_stack().oops_do(f);\n+  }\n","filename":"src\/hotspot\/share\/runtime\/javaThread.cpp","additions":9,"deletions":2,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -37,0 +37,1 @@\n+#include \"runtime\/lockStack.hpp\"\n@@ -1148,0 +1149,11 @@\n+private:\n+  LockStack _lock_stack;\n+\n+public:\n+  LockStack& lock_stack() { return _lock_stack; }\n+\n+  static ByteSize lock_stack_offset()      { return byte_offset_of(JavaThread, _lock_stack); }\n+  static ByteSize lock_stack_top_offset()  { return lock_stack_offset() + LockStack::top_offset(); }\n+  static ByteSize lock_stack_base_offset() { return lock_stack_offset() + LockStack::base_offset(); }\n+\n+\n","filename":"src\/hotspot\/share\/runtime\/javaThread.hpp","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -0,0 +1,77 @@\n+\/*\n+ * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"memory\/allocation.hpp\"\n+#include \"runtime\/lockStack.inline.hpp\"\n+#include \"runtime\/safepoint.hpp\"\n+#include \"runtime\/stackWatermark.hpp\"\n+#include \"runtime\/stackWatermarkSet.inline.hpp\"\n+#include \"runtime\/thread.hpp\"\n+#include \"utilities\/copy.hpp\"\n+#include \"utilities\/ostream.hpp\"\n+\n+const int LockStack::lock_stack_offset =      in_bytes(JavaThread::lock_stack_offset());\n+const int LockStack::lock_stack_top_offset =  in_bytes(JavaThread::lock_stack_top_offset());\n+const int LockStack::lock_stack_base_offset = in_bytes(JavaThread::lock_stack_base_offset());\n+\n+LockStack::LockStack(JavaThread* jt) :\n+  _top(lock_stack_base_offset), _base()\n+{\n+#ifdef ASSERT\n+  for (int i = 0; i < CAPACITY; i++) {\n+    _base[i] = nullptr;\n+  }\n+#endif\n+}\n+\n+uint32_t LockStack::start_offset() {\n+  int offset = lock_stack_base_offset;\n+  assert(offset > 0, \"must be positive offset\");\n+  return static_cast<uint32_t>(offset);\n+}\n+\n+uint32_t LockStack::end_offset() {\n+  int offset = lock_stack_base_offset + CAPACITY * oopSize;\n+  assert(offset > 0, \"must be positive offset\");\n+  return static_cast<uint32_t>(offset);\n+}\n+\n+#ifndef PRODUCT\n+void LockStack::verify(const char* msg) const {\n+  assert(LockingMode == LIGHTWEIGHT, \"never use lock-stack when fast-locking is disabled\");\n+  assert((_top <=  end_offset()), \"lockstack overflow: _top %d end_offset %d\", _top, end_offset());\n+  assert((_top >= start_offset()), \"lockstack underflow: _topt %d end_offset %d\", _top, start_offset());\n+  int top = to_index(_top);\n+  for (int i = 0; i < top; i++) {\n+    assert(_base[i] != nullptr, \"no zapped before top\");\n+    for (int j = i + 1; j < top; j++) {\n+      assert(_base[i] != _base[j], \"entries must be unique: %s\", msg);\n+    }\n+  }\n+  for (int i = top; i < CAPACITY; i++) {\n+    assert(_base[i] == nullptr, \"only zapped entries after top: i: %d, top: %d, entry: \" PTR_FORMAT, i, top, p2i(_base[i]));\n+  }\n+}\n+#endif\n","filename":"src\/hotspot\/share\/runtime\/lockStack.cpp","additions":77,"deletions":0,"binary":false,"changes":77,"status":"added"},{"patch":"@@ -0,0 +1,80 @@\n+\/*\n+ * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_RUNTIME_LOCKSTACK_HPP\n+#define SHARE_RUNTIME_LOCKSTACK_HPP\n+\n+#include \"oops\/oopsHierarchy.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include \"utilities\/sizes.hpp\"\n+\n+class Thread;\n+class OopClosure;\n+\n+class LockStack {\n+  friend class VMStructs;\n+private:\n+  static const int CAPACITY = 8;\n+\n+  \/\/ TODO: It would be very useful if JavaThread::lock_stack_offset() and friends were constexpr,\n+  \/\/ but this is currently not the case because we're using offset_of() which is non-constexpr,\n+  \/\/ GCC would warn about non-standard-layout types if we were using offsetof() (which *is* constexpr).\n+  static const int lock_stack_offset;\n+  static const int lock_stack_top_offset;\n+  static const int lock_stack_base_offset;\n+\n+  \/\/ The offset of the next element, in bytes, relative to the JavaThread structure.\n+  \/\/ We do this instead of a simple index into the array because this allows for\n+  \/\/ efficient addressing in generated code.\n+  uint32_t _top;\n+  oop _base[CAPACITY];\n+\n+  inline JavaThread* get_thread() const;\n+\n+  bool is_self() const;\n+  void verify(const char* msg) const PRODUCT_RETURN;\n+\n+  static inline int to_index(uint32_t offset);\n+\n+public:\n+  static ByteSize top_offset()  { return byte_offset_of(LockStack, _top); }\n+  static ByteSize base_offset() { return byte_offset_of(LockStack, _base); }\n+\n+  LockStack(JavaThread* jt);\n+\n+  static uint32_t start_offset();\n+  static uint32_t end_offset();\n+  inline bool can_push() const;\n+  inline void push(oop o);\n+  inline oop pop();\n+  inline void remove(oop o);\n+\n+  inline bool contains(oop o) const;\n+\n+  \/\/ GC support\n+  inline void oops_do(OopClosure* cl);\n+\n+};\n+\n+#endif \/\/ SHARE_RUNTIME_LOCKSTACK_HPP\n","filename":"src\/hotspot\/share\/runtime\/lockStack.hpp","additions":80,"deletions":0,"binary":false,"changes":80,"status":"added"},{"patch":"@@ -0,0 +1,129 @@\n+\/*\n+ * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_RUNTIME_LOCKSTACK_INLINE_HPP\n+#define SHARE_RUNTIME_LOCKSTACK_INLINE_HPP\n+\n+#include \"memory\/iterator.hpp\"\n+#include \"oops\/access.hpp\"\n+#include \"runtime\/javaThread.hpp\"\n+#include \"runtime\/lockStack.hpp\"\n+#include \"runtime\/safepoint.hpp\"\n+#include \"runtime\/stackWatermark.hpp\"\n+#include \"runtime\/stackWatermarkSet.inline.hpp\"\n+\n+inline int LockStack::to_index(uint32_t offset) {\n+  return (offset - lock_stack_base_offset) \/ oopSize;\n+}\n+\n+JavaThread* LockStack::get_thread() const {\n+  char* addr = reinterpret_cast<char*>(const_cast<LockStack*>(this));\n+  return reinterpret_cast<JavaThread*>(addr - lock_stack_offset);\n+}\n+\n+inline bool LockStack::can_push() const {\n+  return to_index(_top) < CAPACITY;\n+}\n+\n+inline bool LockStack::is_self() const {\n+  Thread* thread = Thread::current();\n+  bool is_self = &JavaThread::cast(thread)->lock_stack() == this;\n+  assert(is_self == (get_thread() == thread), \"is_self sanity\");\n+  return is_self;\n+}\n+\n+inline void LockStack::push(oop o) {\n+  verify(\"pre-push\");\n+  assert(oopDesc::is_oop(o), \"must be\");\n+  assert(!contains(o), \"entries must be unique\");\n+  assert(can_push(), \"must have room\");\n+  assert(_base[to_index(_top)] == nullptr, \"expect zapped entry\");\n+  _base[to_index(_top)] = o;\n+  _top += oopSize;\n+  verify(\"post-push\");\n+}\n+\n+inline oop LockStack::pop() {\n+  verify(\"pre-pop\");\n+  assert(to_index(_top) > 0, \"underflow, probably unbalanced push\/pop\");\n+  _top -= oopSize;\n+  oop o = _base[to_index(_top)];\n+#ifdef ASSERT\n+  _base[to_index(_top)] = nullptr;\n+#endif\n+  assert(!contains(o), \"entries must be unique\");\n+  verify(\"post-pop\");\n+  return o;\n+}\n+\n+inline void LockStack::remove(oop o) {\n+  verify(\"pre-remove\");\n+  assert(contains(o), \"entry must be present\");\n+  int end = to_index(_top);\n+  for (int i = 0; i < end; i++) {\n+    if (_base[i] == o) {\n+      int last = end - 1;\n+      for (; i < last; i++) {\n+        _base[i] = _base[i + 1];\n+      }\n+      _top -= oopSize;\n+#ifdef ASSERT\n+      _base[to_index(_top)] = nullptr;\n+#endif\n+      break;\n+    }\n+  }\n+  assert(!contains(o), \"entries must be unique: \" PTR_FORMAT, p2i(o));\n+  verify(\"post-remove\");\n+}\n+\n+inline bool LockStack::contains(oop o) const {\n+  verify(\"pre-contains\");\n+  if (!SafepointSynchronize::is_at_safepoint() && !is_self()) {\n+    StackWatermark* watermark = StackWatermarkSet::get(get_thread(), StackWatermarkKind::gc);\n+    if (watermark != nullptr) {\n+      watermark->start_processing();\n+    }\n+  }\n+  int end = to_index(_top);\n+  for (int i = end - 1; i >= 0; i--) {\n+    if (NativeAccess<>::oop_load(&_base[i]) == o) {\n+      verify(\"post-contains\");\n+      return true;\n+    }\n+  }\n+  verify(\"post-contains\");\n+  return false;\n+}\n+\n+inline void LockStack::oops_do(OopClosure* cl) {\n+  verify(\"pre-oops-do\");\n+  int end = to_index(_top);\n+  for (int i = 0; i < end; i++) {\n+    cl->do_oop(&_base[i]);\n+  }\n+  verify(\"post-oops-do\");\n+}\n+\n+#endif \/\/ SHARE_RUNTIME_LOCKSTACK_INLINE_HPP\n","filename":"src\/hotspot\/share\/runtime\/lockStack.inline.hpp","additions":129,"deletions":0,"binary":false,"changes":129,"status":"added"},{"patch":"@@ -337,1 +337,1 @@\n-  if (current->is_lock_owned((address)cur)) {\n+  if (LockingMode != LIGHTWEIGHT && current->is_lock_owned((address)cur)) {\n@@ -1138,1 +1138,1 @@\n-    if (current->is_lock_owned((address)cur)) {\n+    if (LockingMode != LIGHTWEIGHT && current->is_lock_owned((address)cur)) {\n@@ -1353,1 +1353,1 @@\n-    if (current->is_lock_owned((address)cur)) {\n+    if (LockingMode != LIGHTWEIGHT && current->is_lock_owned((address)cur)) {\n@@ -1388,0 +1388,1 @@\n+  assert(cur != anon_owner_ptr(), \"no anon owner here\");\n@@ -1391,1 +1392,1 @@\n-  if (current->is_lock_owned((address)cur)) {\n+  if (LockingMode != LIGHTWEIGHT && current->is_lock_owned((address)cur)) {\n","filename":"src\/hotspot\/share\/runtime\/objectMonitor.cpp","additions":5,"deletions":4,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -147,2 +147,16 @@\n-  \/\/ Used by async deflation as a marker in the _owner field:\n-  #define DEFLATER_MARKER reinterpret_cast<void*>(-1)\n+  \/\/ Used by async deflation as a marker in the _owner field.\n+  \/\/ Note that the choice of the two markers is peculiar:\n+  \/\/ - They need to represent values that cannot be pointers. In particular,\n+  \/\/   we achieve this by using the lowest two bits.\n+  \/\/ - ANONYMOUS_OWNER should be a small value, it is used in generated code\n+  \/\/   and small values encode much better.\n+  \/\/ - We test for anonymous owner by testing for the lowest bit, therefore\n+  \/\/   DEFLATER_MARKER must *not* have that bit set.\n+  #define DEFLATER_MARKER reinterpret_cast<void*>(2)\n+public:\n+  \/\/ NOTE: Typed as uintptr_t so that we can pick it up in SA, via vmStructs.\n+  static const uintptr_t ANONYMOUS_OWNER = 1;\n+\n+private:\n+  static void* anon_owner_ptr() { return reinterpret_cast<void*>(ANONYMOUS_OWNER); }\n+\n@@ -181,0 +195,1 @@\n+\n@@ -245,1 +260,1 @@\n-  intptr_t  is_entered(JavaThread* current) const;\n+  bool is_entered(JavaThread* current) const;\n@@ -266,0 +281,12 @@\n+  void set_owner_anonymous() {\n+    set_owner_from(nullptr, anon_owner_ptr());\n+  }\n+\n+  bool is_owner_anonymous() const {\n+    return owner_raw() == anon_owner_ptr();\n+  }\n+\n+  void set_owner_from_anonymous(Thread* owner) {\n+    set_owner_from(anon_owner_ptr(), owner);\n+  }\n+\n","filename":"src\/hotspot\/share\/runtime\/objectMonitor.hpp","additions":30,"deletions":3,"binary":false,"changes":33,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"runtime\/lockStack.inline.hpp\"\n@@ -35,4 +36,12 @@\n-inline intptr_t ObjectMonitor::is_entered(JavaThread* current) const {\n-  void* owner = owner_raw();\n-  if (current == owner || current->is_lock_owned((address)owner)) {\n-    return 1;\n+inline bool ObjectMonitor::is_entered(JavaThread* current) const {\n+  if (LockingMode == LIGHTWEIGHT) {\n+    if (is_owner_anonymous()) {\n+      return current->lock_stack().contains(object());\n+    } else {\n+      return current == owner_raw();\n+    }\n+  } else {\n+    void* owner = owner_raw();\n+    if (current == owner || current->is_lock_owned((address)owner)) {\n+      return true;\n+    }\n@@ -40,1 +49,1 @@\n-  return 0;\n+  return false;\n","filename":"src\/hotspot\/share\/runtime\/objectMonitor.inline.hpp","additions":14,"deletions":5,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -42,0 +42,1 @@\n+#include \"runtime\/lockStack.inline.hpp\"\n@@ -314,4 +315,12 @@\n-  if (mark.has_locker() && current->is_lock_owned((address)mark.locker())) {\n-    \/\/ Degenerate notify\n-    \/\/ stack-locked by caller so by definition the implied waitset is empty.\n-    return true;\n+  if (LockingMode == LIGHTWEIGHT) {\n+    if (mark.is_fast_locked() && current->lock_stack().contains(cast_to_oop(obj))) {\n+      \/\/ Degenerate notify\n+      \/\/ fast-locked by caller so by definition the implied waitset is empty.\n+      return true;\n+    }\n+  } else if (LockingMode == LEGACY) {\n+    if (mark.has_locker() && current->is_lock_owned((address)mark.locker())) {\n+      \/\/ Degenerate notify\n+      \/\/ stack-locked by caller so by definition the implied waitset is empty.\n+      return true;\n+    }\n@@ -388,10 +397,12 @@\n-    \/\/ This Java Monitor is inflated so obj's header will never be\n-    \/\/ displaced to this thread's BasicLock. Make the displaced header\n-    \/\/ non-null so this BasicLock is not seen as recursive nor as\n-    \/\/ being locked. We do this unconditionally so that this thread's\n-    \/\/ BasicLock cannot be mis-interpreted by any stack walkers. For\n-    \/\/ performance reasons, stack walkers generally first check for\n-    \/\/ stack-locking in the object's header, the second check is for\n-    \/\/ recursive stack-locking in the displaced header in the BasicLock,\n-    \/\/ and last are the inflated Java Monitor (ObjectMonitor) checks.\n-    lock->set_displaced_header(markWord::unused_mark());\n+    if (LockingMode != LIGHTWEIGHT) {\n+      \/\/ This Java Monitor is inflated so obj's header will never be\n+      \/\/ displaced to this thread's BasicLock. Make the displaced header\n+      \/\/ non-null so this BasicLock is not seen as recursive nor as\n+      \/\/ being locked. We do this unconditionally so that this thread's\n+      \/\/ BasicLock cannot be mis-interpreted by any stack walkers. For\n+      \/\/ performance reasons, stack walkers generally first check for\n+      \/\/ stack-locking in the object's header, the second check is for\n+      \/\/ recursive stack-locking in the displaced header in the BasicLock,\n+      \/\/ and last are the inflated Java Monitor (ObjectMonitor) checks.\n+      lock->set_displaced_header(markWord::unused_mark());\n+    }\n@@ -486,6 +497,33 @@\n-    markWord mark = obj->mark();\n-    if (mark.is_neutral()) {\n-      \/\/ Anticipate successful CAS -- the ST of the displaced mark must\n-      \/\/ be visible <= the ST performed by the CAS.\n-      lock->set_displaced_header(mark);\n-      if (mark == obj()->cas_set_mark(markWord::from_pointer(lock), mark)) {\n+    if (LockingMode == LIGHTWEIGHT) {\n+      \/\/ Fast-locking does not use the 'lock' argument..\n+      LockStack& lock_stack = current->lock_stack();\n+      if (lock_stack.can_push()) {\n+        markWord mark = obj()->mark_acquire();\n+        if (mark.is_neutral()) {\n+          assert(!lock_stack.contains(obj()), \"thread must not already hold the lock\");\n+          \/\/ Try to swing into 'fast-locked' state.\n+          markWord locked_mark = mark.set_fast_locked();\n+          markWord old_mark = obj()->cas_set_mark(locked_mark, mark);\n+          if (old_mark == mark) {\n+            \/\/ Successfully fast-locked, push object to lock-stack and return.\n+            lock_stack.push(obj());\n+            return;\n+          }\n+        }\n+      }\n+      \/\/ All other paths fall-through to inflate-enter.\n+    } else if (LockingMode == LEGACY) {\n+      markWord mark = obj->mark();\n+      if (mark.is_neutral()) {\n+        \/\/ Anticipate successful CAS -- the ST of the displaced mark must\n+        \/\/ be visible <= the ST performed by the CAS.\n+        lock->set_displaced_header(mark);\n+        if (mark == obj()->cas_set_mark(markWord::from_pointer(lock), mark)) {\n+          return;\n+        }\n+        \/\/ Fall through to inflate() ...\n+      } else if (mark.has_locker() &&\n+                 current->is_lock_owned((address) mark.locker())) {\n+        assert(lock != mark.locker(), \"must not re-lock the same lock\");\n+        assert(lock != (BasicLock*) obj->mark().value(), \"don't relock with same BasicLock\");\n+        lock->set_displaced_header(markWord::from_pointer(nullptr));\n@@ -494,8 +532,0 @@\n-      \/\/ Fall through to inflate() ...\n-    } else if (mark.has_locker() &&\n-               current->is_lock_owned((address)mark.locker())) {\n-      assert(lock != mark.locker(), \"must not re-lock the same lock\");\n-      assert(lock != (BasicLock*)obj->mark().value(), \"don't relock with same BasicLock\");\n-      lock->set_displaced_header(markWord::from_pointer(nullptr));\n-      return;\n-    }\n@@ -503,5 +533,6 @@\n-    \/\/ The object header will never be displaced to this lock,\n-    \/\/ so it does not matter what the value is, except that it\n-    \/\/ must be non-zero to avoid looking like a re-entrant lock,\n-    \/\/ and must not look locked either.\n-    lock->set_displaced_header(markWord::unused_mark());\n+      \/\/ The object header will never be displaced to this lock,\n+      \/\/ so it does not matter what the value is, except that it\n+      \/\/ must be non-zero to avoid looking like a re-entrant lock,\n+      \/\/ and must not look locked either.\n+      lock->set_displaced_header(markWord::unused_mark());\n+    }\n@@ -509,1 +540,1 @@\n-    guarantee(!obj->mark().has_locker(), \"must not be stack-locked\");\n+    guarantee((obj->mark().value() & markWord::lock_mask_in_place) != markWord::locked_value, \"must not be lightweight\/stack-locked\");\n@@ -528,25 +559,15 @@\n-\n-    markWord dhw = lock->displaced_header();\n-    if (dhw.value() == 0) {\n-      \/\/ If the displaced header is null, then this exit matches up with\n-      \/\/ a recursive enter. No real work to do here except for diagnostics.\n-#ifndef PRODUCT\n-      if (mark != markWord::INFLATING()) {\n-        \/\/ Only do diagnostics if we are not racing an inflation. Simply\n-        \/\/ exiting a recursive enter of a Java Monitor that is being\n-        \/\/ inflated is safe; see the has_monitor() comment below.\n-        assert(!mark.is_neutral(), \"invariant\");\n-        assert(!mark.has_locker() ||\n-        current->is_lock_owned((address)mark.locker()), \"invariant\");\n-        if (mark.has_monitor()) {\n-          \/\/ The BasicLock's displaced_header is marked as a recursive\n-          \/\/ enter and we have an inflated Java Monitor (ObjectMonitor).\n-          \/\/ This is a special case where the Java Monitor was inflated\n-          \/\/ after this thread entered the stack-lock recursively. When a\n-          \/\/ Java Monitor is inflated, we cannot safely walk the Java\n-          \/\/ Monitor owner's stack and update the BasicLocks because a\n-          \/\/ Java Monitor can be asynchronously inflated by a thread that\n-          \/\/ does not own the Java Monitor.\n-          ObjectMonitor* m = mark.monitor();\n-          assert(m->object()->mark() == mark, \"invariant\");\n-          assert(m->is_entered(current), \"invariant\");\n+    if (LockingMode == LIGHTWEIGHT) {\n+      \/\/ Fast-locking does not use the 'lock' argument.\n+      if (mark.is_fast_locked()) {\n+        markWord unlocked_mark = mark.set_unlocked();\n+        markWord old_mark = object->cas_set_mark(unlocked_mark, mark);\n+        if (old_mark != mark) {\n+          \/\/ Another thread won the CAS, it must have inflated the monitor.\n+          \/\/ It can only have installed an anonymously locked monitor at this point.\n+          \/\/ Fetch that monitor, set owner correctly to this thread, and\n+          \/\/ exit it (allowing waiting threads to enter).\n+          assert(old_mark.has_monitor(), \"must have monitor\");\n+          ObjectMonitor* monitor = old_mark.monitor();\n+          assert(monitor->is_owner_anonymous(), \"must be anonymous owner\");\n+          monitor->set_owner_from_anonymous(current);\n+          monitor->exit(current);\n@@ -554,0 +575,3 @@\n+        LockStack& lock_stack = current->lock_stack();\n+        lock_stack.remove(object);\n+        return;\n@@ -555,0 +579,27 @@\n+    } else if (LockingMode == LEGACY) {\n+      markWord dhw = lock->displaced_header();\n+      if (dhw.value() == 0) {\n+        \/\/ If the displaced header is null, then this exit matches up with\n+        \/\/ a recursive enter. No real work to do here except for diagnostics.\n+#ifndef PRODUCT\n+        if (mark != markWord::INFLATING()) {\n+          \/\/ Only do diagnostics if we are not racing an inflation. Simply\n+          \/\/ exiting a recursive enter of a Java Monitor that is being\n+          \/\/ inflated is safe; see the has_monitor() comment below.\n+          assert(!mark.is_neutral(), \"invariant\");\n+          assert(!mark.has_locker() ||\n+                 current->is_lock_owned((address)mark.locker()), \"invariant\");\n+          if (mark.has_monitor()) {\n+            \/\/ The BasicLock's displaced_header is marked as a recursive\n+            \/\/ enter and we have an inflated Java Monitor (ObjectMonitor).\n+            \/\/ This is a special case where the Java Monitor was inflated\n+            \/\/ after this thread entered the stack-lock recursively. When a\n+            \/\/ Java Monitor is inflated, we cannot safely walk the Java\n+            \/\/ Monitor owner's stack and update the BasicLocks because a\n+            \/\/ Java Monitor can be asynchronously inflated by a thread that\n+            \/\/ does not own the Java Monitor.\n+            ObjectMonitor* m = mark.monitor();\n+            assert(m->object()->mark() == mark, \"invariant\");\n+            assert(m->is_entered(current), \"invariant\");\n+          }\n+        }\n@@ -556,8 +607,0 @@\n-      return;\n-    }\n-\n-    if (mark == markWord::from_pointer(lock)) {\n-      \/\/ If the object is stack-locked by the current thread, try to\n-      \/\/ swing the displaced header from the BasicLock back to the mark.\n-      assert(dhw.is_neutral(), \"invariant\");\n-      if (object->cas_set_mark(dhw, mark) == mark) {\n@@ -566,0 +609,9 @@\n+\n+      if (mark == markWord::from_pointer(lock)) {\n+        \/\/ If the object is stack-locked by the current thread, try to\n+        \/\/ swing the displaced header from the BasicLock back to the mark.\n+        assert(dhw.is_neutral(), \"invariant\");\n+        if (object->cas_set_mark(dhw, mark) == mark) {\n+          return;\n+        }\n+      }\n@@ -568,1 +620,1 @@\n-    guarantee(!object->mark().has_locker(), \"must not be stack-locked\");\n+    guarantee((object->mark().value() & markWord::lock_mask_in_place) != markWord::locked_value, \"must not be lightweight\/stack-locked\");\n@@ -575,0 +627,7 @@\n+  if (LockingMode == LIGHTWEIGHT && monitor->is_owner_anonymous()) {\n+    \/\/ It must be owned by us. Pop lock object from lock stack.\n+    LockStack& lock_stack = current->lock_stack();\n+    oop popped = lock_stack.pop();\n+    assert(popped == object, \"must be owned by this thread\");\n+    monitor->set_owner_from_anonymous(current);\n+  }\n@@ -665,3 +724,10 @@\n-  if (mark.has_locker() && current->is_lock_owned((address)mark.locker())) {\n-    \/\/ Not inflated so there can't be any waiters to notify.\n-    return;\n+  if (LockingMode == LIGHTWEIGHT) {\n+    if ((mark.is_fast_locked() && current->lock_stack().contains(obj()))) {\n+      \/\/ Not inflated so there can't be any waiters to notify.\n+      return;\n+    }\n+  } else if (LockingMode == LEGACY) {\n+    if (mark.has_locker() && current->is_lock_owned((address)mark.locker())) {\n+      \/\/ Not inflated so there can't be any waiters to notify.\n+      return;\n+    }\n@@ -680,3 +746,10 @@\n-  if (mark.has_locker() && current->is_lock_owned((address)mark.locker())) {\n-    \/\/ Not inflated so there can't be any waiters to notify.\n-    return;\n+  if (LockingMode == LIGHTWEIGHT) {\n+    if ((mark.is_fast_locked() && current->lock_stack().contains(obj()))) {\n+      \/\/ Not inflated so there can't be any waiters to notify.\n+      return;\n+    }\n+  } else if (LockingMode == LEGACY) {\n+    if (mark.has_locker() && current->is_lock_owned((address)mark.locker())) {\n+      \/\/ Not inflated so there can't be any waiters to notify.\n+      return;\n+    }\n@@ -708,1 +781,2 @@\n-  if (!mark.is_being_inflated()) {\n+  if (!mark.is_being_inflated() || LockingMode == LIGHTWEIGHT) {\n+    \/\/ New lightweight locking does not use the markWord::INFLATING() protocol.\n@@ -823,0 +897,5 @@\n+static bool is_lock_owned(Thread* thread, oop obj) {\n+  assert(LockingMode == LIGHTWEIGHT, \"only call this with new lightweight locking enabled\");\n+  return JavaThread::cast(thread)->lock_stack().contains(obj);\n+}\n+\n@@ -832,1 +911,1 @@\n-      guarantee(!mark.has_locker(), \"must not be stack locked\");\n+      guarantee((obj->mark().value() & markWord::lock_mask_in_place) != markWord::locked_value, \"must not be lightweight\/stack-locked\");\n@@ -877,2 +956,9 @@\n-    } else if (current->is_lock_owned((address)mark.locker())) {\n-      \/\/ This is a stack lock owned by the calling thread so fetch the\n+    } else if (LockingMode == LIGHTWEIGHT && mark.is_fast_locked() && is_lock_owned(current, obj)) {\n+      \/\/ This is a fast-lock owned by the calling thread so use the\n+      \/\/ markWord from the object.\n+      hash = mark.hash();\n+      if (hash != 0) {                  \/\/ if it has a hash, just return it\n+        return hash;\n+      }\n+    } else if (LockingMode == LEGACY && mark.has_locker() && current->is_lock_owned((address)mark.locker())) {\n+      \/\/ This is a stack-lock owned by the calling thread so fetch the\n@@ -889,1 +975,1 @@\n-      \/\/ So we have to inflate the stack lock into an ObjectMonitor\n+      \/\/ So we have to inflate the stack-lock into an ObjectMonitor\n@@ -942,2 +1028,2 @@\n-  \/\/ Uncontended case, header points to stack\n-  if (mark.has_locker()) {\n+  if (LockingMode == LEGACY && mark.has_locker()) {\n+    \/\/ stack-locked case, header points into owner's stack\n@@ -946,1 +1032,6 @@\n-  \/\/ Contended case, header points to ObjectMonitor (tagged pointer)\n+\n+  if (LockingMode == LIGHTWEIGHT && mark.is_fast_locked()) {\n+    \/\/ fast-locking case, see if lock is in current's lock stack\n+    return current->lock_stack().contains(h_obj());\n+  }\n+\n@@ -948,0 +1039,1 @@\n+    \/\/ inflated monitor so header points to ObjectMonitor (tagged pointer).\n@@ -960,2 +1052,0 @@\n-  address owner = nullptr;\n-\n@@ -964,3 +1054,4 @@\n-  \/\/ Uncontended case, header points to stack\n-  if (mark.has_locker()) {\n-    owner = (address) mark.locker();\n+  if (LockingMode == LEGACY && mark.has_locker()) {\n+    \/\/ stack-locked so header points into owner's stack.\n+    \/\/ owning_thread_from_monitor_owner() may also return null here:\n+    return Threads::owning_thread_from_monitor_owner(t_list, (address) mark.locker());\n@@ -969,2 +1060,8 @@\n-  \/\/ Contended case, header points to ObjectMonitor (tagged pointer)\n-  else if (mark.has_monitor()) {\n+  if (LockingMode == LIGHTWEIGHT && mark.is_fast_locked()) {\n+    \/\/ fast-locked so get owner from the object.\n+    \/\/ owning_thread_from_object() may also return null here:\n+    return Threads::owning_thread_from_object(t_list, h_obj());\n+  }\n+\n+  if (mark.has_monitor()) {\n+    \/\/ inflated monitor so header points to ObjectMonitor (tagged pointer).\n@@ -975,6 +1072,2 @@\n-    owner = (address) monitor->owner();\n-  }\n-\n-  if (owner != nullptr) {\n-    \/\/ owning_thread_from_monitor_owner() may also return null here\n-    return Threads::owning_thread_from_monitor_owner(t_list, owner);\n+    \/\/ owning_thread_from_monitor() may also return null here:\n+    return Threads::owning_thread_from_monitor(t_list, monitor);\n@@ -983,5 +1076,0 @@\n-  \/\/ Unlocked case, header in place\n-  \/\/ Cannot have assertion since this object may have been\n-  \/\/ locked by another thread when reaching here.\n-  \/\/ assert(mark.is_neutral(), \"sanity check\");\n-\n@@ -994,1 +1082,1 @@\n-\/\/ ObjectMonitors where owner is set to a stack lock address in thread.\n+\/\/ ObjectMonitors where owner is set to a stack-lock address in thread.\n@@ -1004,1 +1092,1 @@\n-      \/\/ is set to a stack lock address in the target thread.\n+      \/\/ is set to a stack-lock address in the target thread.\n@@ -1030,1 +1118,1 @@\n-    \/\/ Owner set to a stack lock address in thread should never be seen here:\n+    \/\/ Owner set to a stack-lock address in thread should never be seen here:\n@@ -1175,4 +1263,11 @@\n-    \/\/ *  Inflated     - just return\n-    \/\/ *  Stack-locked - coerce it to inflated\n-    \/\/ *  INFLATING    - busy wait for conversion to complete\n-    \/\/ *  Neutral      - aggressively inflate the object.\n+    \/\/ *  inflated     - Just return if using stack-locking.\n+    \/\/                   If using fast-locking and the ObjectMonitor owner\n+    \/\/                   is anonymous and the current thread owns the\n+    \/\/                   object lock, then we make the current thread the\n+    \/\/                   ObjectMonitor owner and remove the lock from the\n+    \/\/                   current thread's lock stack.\n+    \/\/ *  fast-locked  - Coerce it to inflated from fast-locked.\n+    \/\/ *  stack-locked - Coerce it to inflated from stack-locked.\n+    \/\/ *  INFLATING    - Busy wait for conversion from stack-locked to\n+    \/\/                   inflated.\n+    \/\/ *  neutral      - Aggressively inflate the object.\n@@ -1185,0 +1280,4 @@\n+      if (LockingMode == LIGHTWEIGHT && inf->is_owner_anonymous() && is_lock_owned(current, object)) {\n+        inf->set_owner_from_anonymous(current);\n+        JavaThread::cast(current)->lock_stack().remove(object);\n+      }\n@@ -1188,9 +1287,64 @@\n-    \/\/ CASE: inflation in progress - inflating over a stack-lock.\n-    \/\/ Some other thread is converting from stack-locked to inflated.\n-    \/\/ Only that thread can complete inflation -- other threads must wait.\n-    \/\/ The INFLATING value is transient.\n-    \/\/ Currently, we spin\/yield\/park and poll the markword, waiting for inflation to finish.\n-    \/\/ We could always eliminate polling by parking the thread on some auxiliary list.\n-    if (mark == markWord::INFLATING()) {\n-      read_stable_mark(object);\n-      continue;\n+    if (LockingMode != LIGHTWEIGHT) {\n+      \/\/ New lightweight locking does not use INFLATING.\n+      \/\/ CASE: inflation in progress - inflating over a stack-lock.\n+      \/\/ Some other thread is converting from stack-locked to inflated.\n+      \/\/ Only that thread can complete inflation -- other threads must wait.\n+      \/\/ The INFLATING value is transient.\n+      \/\/ Currently, we spin\/yield\/park and poll the markword, waiting for inflation to finish.\n+      \/\/ We could always eliminate polling by parking the thread on some auxiliary list.\n+      if (mark == markWord::INFLATING()) {\n+        read_stable_mark(object);\n+        continue;\n+      }\n+    }\n+\n+    \/\/ CASE: fast-locked\n+    \/\/ Could be fast-locked either by current or by some other thread.\n+    \/\/\n+    \/\/ Note that we allocate the ObjectMonitor speculatively, _before_\n+    \/\/ attempting to set the object's mark to the new ObjectMonitor. If\n+    \/\/ this thread owns the monitor, then we set the ObjectMonitor's\n+    \/\/ owner to this thread. Otherwise, we set the ObjectMonitor's owner\n+    \/\/ to anonymous. If we lose the race to set the object's mark to the\n+    \/\/ new ObjectMonitor, then we just delete it and loop around again.\n+    \/\/\n+    LogStreamHandle(Trace, monitorinflation) lsh;\n+    if (LockingMode == LIGHTWEIGHT && mark.is_fast_locked()) {\n+      ObjectMonitor* monitor = new ObjectMonitor(object);\n+      monitor->set_header(mark.set_unlocked());\n+      bool own = is_lock_owned(current, object);\n+      if (own) {\n+        \/\/ Owned by us.\n+        monitor->set_owner_from(nullptr, current);\n+      } else {\n+        \/\/ Owned by somebody else.\n+        monitor->set_owner_anonymous();\n+      }\n+      markWord monitor_mark = markWord::encode(monitor);\n+      markWord old_mark = object->cas_set_mark(monitor_mark, mark);\n+      if (old_mark == mark) {\n+        \/\/ Success! Return inflated monitor.\n+        if (own) {\n+          JavaThread::cast(current)->lock_stack().remove(object);\n+        }\n+        \/\/ Once the ObjectMonitor is configured and object is associated\n+        \/\/ with the ObjectMonitor, it is safe to allow async deflation:\n+        _in_use_list.add(monitor);\n+\n+        \/\/ Hopefully the performance counters are allocated on distinct\n+        \/\/ cache lines to avoid false sharing on MP systems ...\n+        OM_PERFDATA_OP(Inflations, inc());\n+        if (log_is_enabled(Trace, monitorinflation)) {\n+          ResourceMark rm(current);\n+          lsh.print_cr(\"inflate(has_locker): object=\" INTPTR_FORMAT \", mark=\"\n+                       INTPTR_FORMAT \", type='%s'\", p2i(object),\n+                       object->mark().value(), object->klass()->external_name());\n+        }\n+        if (event.should_commit()) {\n+          post_monitor_inflate_event(&event, object, cause);\n+        }\n+        return monitor;\n+      } else {\n+        delete monitor;\n+        continue;  \/\/ Interference -- just retry\n+      }\n@@ -1200,1 +1354,1 @@\n-    \/\/ Could be stack-locked either by this thread or by some other thread.\n+    \/\/ Could be stack-locked either by current or by some other thread.\n@@ -1207,5 +1361,5 @@\n-    \/\/ the odds of inflation contention.\n-\n-    LogStreamHandle(Trace, monitorinflation) lsh;\n-\n-    if (mark.has_locker()) {\n+    \/\/ the odds of inflation contention. If we lose the race to set INFLATING,\n+    \/\/ then we just delete the ObjectMonitor and loop around again.\n+    \/\/\n+    if (LockingMode == LEGACY && mark.has_locker()) {\n+      assert(LockingMode != LIGHTWEIGHT, \"cannot happen with new lightweight locking\");\n@@ -1373,2 +1527,2 @@\n-\/\/ is set to a stack lock address are NOT associated with the JavaThread\n-\/\/ that holds that stack lock. All of the current consumers of\n+\/\/ is set to a stack-lock address are NOT associated with the JavaThread\n+\/\/ that holds that stack-lock. All of the current consumers of\n@@ -1376,1 +1530,1 @@\n-\/\/ those do not have the owner set to a stack lock address.\n+\/\/ those do not have the owner set to a stack-lock address.\n@@ -1393,1 +1547,1 @@\n-      \/\/ not include when owner is set to a stack lock address in thread.\n+      \/\/ not include when owner is set to a stack-lock address in thread.\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.cpp","additions":280,"deletions":126,"binary":false,"changes":406,"status":"modified"},{"patch":"@@ -536,0 +536,1 @@\n+  assert(LockingMode != LIGHTWEIGHT, \"should not be called with new lightweight locking\");\n","filename":"src\/hotspot\/share\/runtime\/thread.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -71,0 +71,1 @@\n+#include \"runtime\/lockStack.inline.hpp\"\n@@ -1393,0 +1394,1 @@\n+  assert(LockingMode != LIGHTWEIGHT, \"Not with new lightweight locking\");\n@@ -1422,0 +1424,10 @@\n+JavaThread* Threads::owning_thread_from_object(ThreadsList * t_list, oop obj) {\n+  assert(LockingMode == LIGHTWEIGHT, \"Only with new lightweight locking\");\n+  for (JavaThread* q : *t_list) {\n+    if (q->lock_stack().contains(obj)) {\n+      return q;\n+    }\n+  }\n+  return nullptr;\n+}\n+\n@@ -1423,2 +1435,12 @@\n-  address owner = (address)monitor->owner();\n-  return owning_thread_from_monitor_owner(t_list, owner);\n+  if (LockingMode == LIGHTWEIGHT) {\n+    if (monitor->is_owner_anonymous()) {\n+      return owning_thread_from_object(t_list, monitor->object());\n+    } else {\n+      Thread* owner = reinterpret_cast<Thread*>(monitor->owner());\n+      assert(owner == nullptr || owner->is_Java_thread(), \"only JavaThreads own monitors\");\n+      return reinterpret_cast<JavaThread*>(owner);\n+    }\n+  } else {\n+    address owner = (address)monitor->owner();\n+    return owning_thread_from_monitor_owner(t_list, owner);\n+  }\n","filename":"src\/hotspot\/share\/runtime\/threads.cpp","additions":24,"deletions":2,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -141,0 +141,1 @@\n+  static JavaThread* owning_thread_from_object(ThreadsList* t_list, oop obj);\n","filename":"src\/hotspot\/share\/runtime\/threads.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -706,0 +706,3 @@\n+  nonstatic_field(JavaThread,                  _lock_stack,                                   LockStack)                             \\\n+  nonstatic_field(LockStack,                   _top,                                          uint32_t)                              \\\n+  nonstatic_field(LockStack,                   _base[0],                                      oop)                                   \\\n@@ -1320,0 +1323,1 @@\n+  declare_toplevel_type(LockStack)                                        \\\n@@ -2622,2 +2626,4 @@\n-  declare_constant(InvocationCounter::count_shift)\n-\n+  declare_constant(InvocationCounter::count_shift)                        \\\n+                                                                          \\\n+  \/* ObjectMonitor constants *\/                                           \\\n+  declare_constant(ObjectMonitor::ANONYMOUS_OWNER)                        \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":8,"deletions":2,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -1124,0 +1124,5 @@\n+  \/\/ When using the new lightweight locking, we need to take\n+  \/\/ a safepoint here, because the thread snapshot code calls\n+  \/\/ ObjectSynchronizer::get_lock_owner() and it would potentially\n+  \/\/ give wrong results when Java threads are running and\n+  \/\/ entering\/leaving locks while we inspect the thread stacks.\n","filename":"src\/hotspot\/share\/services\/management.cpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -1045,0 +1045,9 @@\n+enum LockingMode {\n+  \/\/ Use only heavy monitors for locking\n+  MONITOR     = 0,\n+  \/\/ Legacy stack-locking, with monitors as 2nd tier\n+  LEGACY      = 1,\n+  \/\/ New lightweight locing, with monitors as 2nd tier\n+  LIGHTWEIGHT = 2\n+};\n+\n","filename":"src\/hotspot\/share\/utilities\/globalDefinitions.hpp","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -47,0 +47,2 @@\n+  private static long          lockStackTopOffset;\n+  private static long          lockStackBaseOffset;\n@@ -56,0 +58,1 @@\n+  private static long oopPtrSize;\n@@ -88,0 +91,1 @@\n+    Type typeLockStack = db.lookupType(\"LockStack\");\n@@ -101,0 +105,4 @@\n+    lockStackTopOffset = type.getField(\"_lock_stack\").getOffset() + typeLockStack.getField(\"_top\").getOffset();\n+    lockStackBaseOffset = type.getField(\"_lock_stack\").getOffset() + typeLockStack.getField(\"_base[0]\").getOffset();\n+    oopPtrSize = VM.getVM().getAddressSize();\n+\n@@ -397,0 +405,17 @@\n+  public boolean isLockOwned(OopHandle obj) {\n+    long current = lockStackBaseOffset;\n+    long end = addr.getJIntAt(lockStackTopOffset);\n+    if (Assert.ASSERTS_ENABLED) {\n+      Assert.that(current <= end, \"current stack offset must be above base offset\");\n+    }\n+\n+    while (current < end) {\n+      Address oop = addr.getAddressAt(current);\n+      if (oop.equals(obj)) {\n+        return true;\n+      }\n+      current += oopPtrSize;\n+    }\n+    return false;\n+  }\n+\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/runtime\/JavaThread.java","additions":25,"deletions":0,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -85,0 +85,4 @@\n+          \/\/ Owned anonymously means that we are not the owner of\n+          \/\/ the monitor and must be waiting for the owner to\n+          \/\/ exit it.\n+          mark.monitor().isOwnedAnonymous() ||\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/runtime\/JavaVFrame.java","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -47,0 +47,1 @@\n+\n@@ -58,0 +59,2 @@\n+\n+    ANONYMOUS_OWNER = db.lookupLongConstant(\"ObjectMonitor::ANONYMOUS_OWNER\").longValue();\n@@ -82,0 +85,4 @@\n+  public boolean isOwnedAnonymous() {\n+    return addr.getAddressAt(ownerFieldOffset).asLongValue() == ANONYMOUS_OWNER;\n+  }\n+\n@@ -117,0 +124,2 @@\n+  private static long          ANONYMOUS_OWNER;\n+\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/runtime\/ObjectMonitor.java","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -213,0 +213,1 @@\n+        assert(VM.getVM().getCommandLineFlag(\"LockingMode\").getInt() != 2);\n@@ -230,1 +231,18 @@\n-        return owningThreadFromMonitor(monitor.owner());\n+        if (VM.getVM().getCommandLineFlag(\"LockingMode\").getInt() == 2) {\n+            if (monitor.isOwnedAnonymous()) {\n+                OopHandle object = monitor.object();\n+                for (int i = 0; i < getNumberOfThreads(); i++) {\n+                    JavaThread thread = getJavaThreadAt(i);\n+                    if (thread.isLockOwned(object)) {\n+                        return thread;\n+                     }\n+                }\n+                throw new InternalError(\"We should have found a thread that owns the anonymous lock\");\n+            }\n+            \/\/ Owner can only be threads at this point.\n+            Address o = monitor.owner();\n+            if (o == null) return null;\n+            return new JavaThread(o);\n+        } else {\n+            return owningThreadFromMonitor(monitor.owner());\n+        }\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/runtime\/Threads.java","additions":19,"deletions":1,"binary":false,"changes":20,"status":"modified"}]}