{"files":[{"patch":"@@ -3825,1 +3825,1 @@\n-    Label no_count;\n+    Label count, no_count;\n@@ -3842,1 +3842,4 @@\n-    if (!UseHeavyMonitors) {\n+    if (LockingMode == LM_MONITOR) {\n+      __ tst(oop, oop); \/\/ Set NE to indicate 'failure' -> take slow-path. We know that oop != 0.\n+      __ b(cont);\n+    } else if (LockingMode == LM_LEGACY) {\n@@ -3870,0 +3873,1 @@\n+      __ b(cont);\n@@ -3871,1 +3875,3 @@\n-      __ tst(oop, oop); \/\/ Set NE to indicate 'failure' -> take slow-path. We know that oop != 0.\n+      assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n+      __ fast_lock(oop, disp_hdr, tmp, rscratch1, no_count);\n+      __ b(count);\n@@ -3873,1 +3879,0 @@\n-    __ b(cont);\n@@ -3886,7 +3891,8 @@\n-    \/\/ Store a non-null value into the box to avoid looking like a re-entrant\n-    \/\/ lock. The fast-path monitor unlock code checks for\n-    \/\/ markWord::monitor_value so use markWord::unused_mark which has the\n-    \/\/ relevant bit set, and also matches ObjectSynchronizer::enter.\n-    __ mov(tmp, (address)markWord::unused_mark().value());\n-    __ str(tmp, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n-\n+    if (LockingMode != LM_LIGHTWEIGHT) {\n+      \/\/ Store a non-null value into the box to avoid looking like a re-entrant\n+      \/\/ lock. The fast-path monitor unlock code checks for\n+      \/\/ markWord::monitor_value so use markWord::unused_mark which has the\n+      \/\/ relevant bit set, and also matches ObjectSynchronizer::enter.\n+      __ mov(tmp, (address)markWord::unused_mark().value());\n+      __ str(tmp, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n+    }\n@@ -3907,0 +3913,1 @@\n+    __ bind(count);\n@@ -3920,1 +3927,1 @@\n-    Label no_count;\n+    Label count, no_count;\n@@ -3924,1 +3931,1 @@\n-    if (!UseHeavyMonitors) {\n+    if (LockingMode == LM_LEGACY) {\n@@ -3937,1 +3944,4 @@\n-    if (!UseHeavyMonitors) {\n+    if (LockingMode == LM_MONITOR) {\n+      __ tst(oop, oop); \/\/ Set NE to indicate 'failure' -> take slow-path. We know that oop != 0.\n+      __ b(cont);\n+    } else if (LockingMode == LM_LEGACY) {\n@@ -3944,0 +3954,1 @@\n+      __ b(cont);\n@@ -3945,1 +3956,3 @@\n-      __ tst(oop, oop); \/\/ Set NE to indicate 'failure' -> take slow-path. We know that oop != 0.\n+      assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n+      __ fast_unlock(oop, tmp, box, disp_hdr, no_count);\n+      __ b(count);\n@@ -3947,1 +3960,0 @@\n-    __ b(cont);\n@@ -3955,0 +3967,14 @@\n+\n+    if (LockingMode == LM_LIGHTWEIGHT) {\n+      \/\/ If the owner is anonymous, we need to fix it -- in an outline stub.\n+      Register tmp2 = disp_hdr;\n+      __ ldr(tmp2, Address(tmp, ObjectMonitor::owner_offset_in_bytes()));\n+      \/\/ We cannot use tbnz here, the target might be too far away and cannot\n+      \/\/ be encoded.\n+      __ tst(tmp2, (uint64_t)ObjectMonitor::ANONYMOUS_OWNER);\n+      C2HandleAnonOMOwnerStub* stub = new (Compile::current()->comp_arena()) C2HandleAnonOMOwnerStub(tmp, tmp2);\n+      Compile::current()->output()->add_stub(stub);\n+      __ br(Assembler::NE, stub->entry());\n+      __ bind(stub->continuation());\n+    }\n+\n@@ -3981,0 +4007,1 @@\n+    __ bind(count);\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":43,"deletions":16,"binary":false,"changes":59,"status":"modified"},{"patch":"@@ -434,1 +434,1 @@\n-    if (UseHeavyMonitors) {\n+    if (LockingMode == LM_MONITOR) {\n@@ -2561,1 +2561,1 @@\n-  if (UseHeavyMonitors) {\n+  if (LockingMode == LM_MONITOR) {\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIRAssembler_aarch64.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -66,2 +66,1 @@\n-  assert(hdr != obj && hdr != disp_hdr && obj != disp_hdr, \"registers must be different\");\n-  Label done;\n+  assert_different_registers(hdr, obj, disp_hdr);\n@@ -86,33 +85,38 @@\n-  \/\/ and mark it as unlocked\n-  orr(hdr, hdr, markWord::unlocked_value);\n-  \/\/ save unlocked object header into the displaced header location on the stack\n-  str(hdr, Address(disp_hdr, 0));\n-  \/\/ test if object header is still the same (i.e. unlocked), and if so, store the\n-  \/\/ displaced header address in the object header - if it is not the same, get the\n-  \/\/ object header instead\n-  lea(rscratch2, Address(obj, hdr_offset));\n-  cmpxchgptr(hdr, disp_hdr, rscratch2, rscratch1, done, \/*fallthough*\/NULL);\n-  \/\/ if the object header was the same, we're done\n-  \/\/ if the object header was not the same, it is now in the hdr register\n-  \/\/ => test if it is a stack pointer into the same stack (recursive locking), i.e.:\n-  \/\/\n-  \/\/ 1) (hdr & aligned_mask) == 0\n-  \/\/ 2) sp <= hdr\n-  \/\/ 3) hdr <= sp + page_size\n-  \/\/\n-  \/\/ these 3 tests can be done by evaluating the following expression:\n-  \/\/\n-  \/\/ (hdr - sp) & (aligned_mask - page_size)\n-  \/\/\n-  \/\/ assuming both the stack pointer and page_size have their least\n-  \/\/ significant 2 bits cleared and page_size is a power of 2\n-  mov(rscratch1, sp);\n-  sub(hdr, hdr, rscratch1);\n-  ands(hdr, hdr, aligned_mask - (int)os::vm_page_size());\n-  \/\/ for recursive locking, the result is zero => save it in the displaced header\n-  \/\/ location (NULL in the displaced hdr location indicates recursive locking)\n-  str(hdr, Address(disp_hdr, 0));\n-  \/\/ otherwise we don't care about the result and handle locking via runtime call\n-  cbnz(hdr, slow_case);\n-  \/\/ done\n-  bind(done);\n+  if (LockingMode == LM_LIGHTWEIGHT) {\n+    fast_lock(obj, hdr, rscratch1, rscratch2, slow_case);\n+  } else if (LockingMode == LM_LEGACY) {\n+    Label done;\n+    \/\/ and mark it as unlocked\n+    orr(hdr, hdr, markWord::unlocked_value);\n+    \/\/ save unlocked object header into the displaced header location on the stack\n+    str(hdr, Address(disp_hdr, 0));\n+    \/\/ test if object header is still the same (i.e. unlocked), and if so, store the\n+    \/\/ displaced header address in the object header - if it is not the same, get the\n+    \/\/ object header instead\n+    lea(rscratch2, Address(obj, hdr_offset));\n+    cmpxchgptr(hdr, disp_hdr, rscratch2, rscratch1, done, \/*fallthough*\/nullptr);\n+    \/\/ if the object header was the same, we're done\n+    \/\/ if the object header was not the same, it is now in the hdr register\n+    \/\/ => test if it is a stack pointer into the same stack (recursive locking), i.e.:\n+    \/\/\n+    \/\/ 1) (hdr & aligned_mask) == 0\n+    \/\/ 2) sp <= hdr\n+    \/\/ 3) hdr <= sp + page_size\n+    \/\/\n+    \/\/ these 3 tests can be done by evaluating the following expression:\n+    \/\/\n+    \/\/ (hdr - sp) & (aligned_mask - page_size)\n+    \/\/\n+    \/\/ assuming both the stack pointer and page_size have their least\n+    \/\/ significant 2 bits cleared and page_size is a power of 2\n+    mov(rscratch1, sp);\n+    sub(hdr, hdr, rscratch1);\n+    ands(hdr, hdr, aligned_mask - (int)os::vm_page_size());\n+    \/\/ for recursive locking, the result is zero => save it in the displaced header\n+    \/\/ location (null in the displaced hdr location indicates recursive locking)\n+    str(hdr, Address(disp_hdr, 0));\n+    \/\/ otherwise we don't care about the result and handle locking via runtime call\n+    cbnz(hdr, slow_case);\n+    \/\/ done\n+    bind(done);\n+  }\n@@ -130,5 +134,8 @@\n-  \/\/ load displaced header\n-  ldr(hdr, Address(disp_hdr, 0));\n-  \/\/ if the loaded hdr is NULL we had recursive locking\n-  \/\/ if we had recursive locking, we are done\n-  cbz(hdr, done);\n+  if (LockingMode != LM_LIGHTWEIGHT) {\n+    \/\/ load displaced header\n+    ldr(hdr, Address(disp_hdr, 0));\n+    \/\/ if the loaded hdr is null we had recursive locking\n+    \/\/ if we had recursive locking, we are done\n+    cbz(hdr, done);\n+  }\n+\n@@ -138,10 +145,22 @@\n-  \/\/ test if object header is pointing to the displaced header, and if so, restore\n-  \/\/ the displaced header in the object - if the object header is not pointing to\n-  \/\/ the displaced header, get the object header instead\n-  \/\/ if the object header was not pointing to the displaced header,\n-  \/\/ we do unlocking via runtime call\n-  if (hdr_offset) {\n-    lea(rscratch1, Address(obj, hdr_offset));\n-    cmpxchgptr(disp_hdr, hdr, rscratch1, rscratch2, done, &slow_case);\n-  } else {\n-    cmpxchgptr(disp_hdr, hdr, obj, rscratch2, done, &slow_case);\n+\n+  if (LockingMode == LM_LIGHTWEIGHT) {\n+    ldr(hdr, Address(obj, oopDesc::mark_offset_in_bytes()));\n+    \/\/ We cannot use tbnz here, the target might be too far away and cannot\n+    \/\/ be encoded.\n+    tst(hdr, markWord::monitor_value);\n+    br(Assembler::NE, slow_case);\n+    fast_unlock(obj, hdr, rscratch1, rscratch2, slow_case);\n+  } else if (LockingMode == LM_LEGACY) {\n+    \/\/ test if object header is pointing to the displaced header, and if so, restore\n+    \/\/ the displaced header in the object - if the object header is not pointing to\n+    \/\/ the displaced header, get the object header instead\n+    \/\/ if the object header was not pointing to the displaced header,\n+    \/\/ we do unlocking via runtime call\n+    if (hdr_offset) {\n+      lea(rscratch1, Address(obj, hdr_offset));\n+      cmpxchgptr(disp_hdr, hdr, rscratch1, rscratch2, done, &slow_case);\n+    } else {\n+      cmpxchgptr(disp_hdr, hdr, obj, rscratch2, done, &slow_case);\n+    }\n+    \/\/ done\n+    bind(done);\n@@ -149,2 +168,0 @@\n-  \/\/ done\n-  bind(done);\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_MacroAssembler_aarch64.cpp","additions":69,"deletions":52,"binary":false,"changes":121,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"runtime\/objectMonitor.hpp\"\n@@ -66,0 +67,27 @@\n+int C2HandleAnonOMOwnerStub::max_size() const {\n+  \/\/ Max size of stub has been determined by testing with 0, in which case\n+  \/\/ C2CodeStubList::emit() will throw an assertion and report the actual size that\n+  \/\/ is needed.\n+  return 24;\n+}\n+\n+void C2HandleAnonOMOwnerStub::emit(C2_MacroAssembler& masm) {\n+  __ bind(entry());\n+  Register mon = monitor();\n+  Register t = tmp();\n+  assert(t != noreg, \"need tmp register\");\n+\n+  \/\/ Fix owner to be the current thread.\n+  __ str(rthread, Address(mon, ObjectMonitor::owner_offset_in_bytes()));\n+\n+  \/\/ Pop owner object from lock-stack.\n+  __ ldrw(t, Address(rthread, JavaThread::lock_stack_top_offset()));\n+  __ subw(t, t, oopSize);\n+#ifdef ASSERT\n+  __ str(zr, Address(rthread, t));\n+#endif\n+  __ strw(t, Address(rthread, JavaThread::lock_stack_top_offset()));\n+\n+  __ b(continuation());\n+}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/c2_CodeStubs_aarch64.cpp","additions":28,"deletions":0,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -733,1 +733,1 @@\n-  if (UseHeavyMonitors) {\n+  if (LockingMode == LM_MONITOR) {\n@@ -761,50 +761,55 @@\n-    \/\/ Load (object->mark() | 1) into swap_reg\n-    ldr(rscratch1, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-    orr(swap_reg, rscratch1, 1);\n-\n-    \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n-    str(swap_reg, Address(lock_reg, mark_offset));\n-\n-    assert(lock_offset == 0,\n-           \"displached header must be first word in BasicObjectLock\");\n-\n-    Label fail;\n-    cmpxchg_obj_header(swap_reg, lock_reg, obj_reg, rscratch1, count, \/*fallthrough*\/NULL);\n-\n-    \/\/ Fast check for recursive lock.\n-    \/\/\n-    \/\/ Can apply the optimization only if this is a stack lock\n-    \/\/ allocated in this thread. For efficiency, we can focus on\n-    \/\/ recently allocated stack locks (instead of reading the stack\n-    \/\/ base and checking whether 'mark' points inside the current\n-    \/\/ thread stack):\n-    \/\/  1) (mark & 7) == 0, and\n-    \/\/  2) sp <= mark < mark + os::pagesize()\n-    \/\/\n-    \/\/ Warning: sp + os::pagesize can overflow the stack base. We must\n-    \/\/ neither apply the optimization for an inflated lock allocated\n-    \/\/ just above the thread stack (this is why condition 1 matters)\n-    \/\/ nor apply the optimization if the stack lock is inside the stack\n-    \/\/ of another thread. The latter is avoided even in case of overflow\n-    \/\/ because we have guard pages at the end of all stacks. Hence, if\n-    \/\/ we go over the stack base and hit the stack of another thread,\n-    \/\/ this should not be in a writeable area that could contain a\n-    \/\/ stack lock allocated by that thread. As a consequence, a stack\n-    \/\/ lock less than page size away from sp is guaranteed to be\n-    \/\/ owned by the current thread.\n-    \/\/\n-    \/\/ These 3 tests can be done by evaluating the following\n-    \/\/ expression: ((mark - sp) & (7 - os::vm_page_size())),\n-    \/\/ assuming both stack pointer and pagesize have their\n-    \/\/ least significant 3 bits clear.\n-    \/\/ NOTE: the mark is in swap_reg %r0 as the result of cmpxchg\n-    \/\/ NOTE2: aarch64 does not like to subtract sp from rn so take a\n-    \/\/ copy\n-    mov(rscratch1, sp);\n-    sub(swap_reg, swap_reg, rscratch1);\n-    ands(swap_reg, swap_reg, (uint64_t)(7 - (int)os::vm_page_size()));\n-\n-    \/\/ Save the test result, for recursive case, the result is zero\n-    str(swap_reg, Address(lock_reg, mark_offset));\n-    br(Assembler::EQ, count);\n-\n+    if (LockingMode == LM_LIGHTWEIGHT) {\n+      ldr(tmp, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      fast_lock(obj_reg, tmp, rscratch1, rscratch2, slow_case);\n+      b(count);\n+    } else if (LockingMode == LM_LEGACY) {\n+      \/\/ Load (object->mark() | 1) into swap_reg\n+      ldr(rscratch1, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      orr(swap_reg, rscratch1, 1);\n+\n+      \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n+      str(swap_reg, Address(lock_reg, mark_offset));\n+\n+      assert(lock_offset == 0,\n+             \"displached header must be first word in BasicObjectLock\");\n+\n+      Label fail;\n+      cmpxchg_obj_header(swap_reg, lock_reg, obj_reg, rscratch1, count, \/*fallthrough*\/nullptr);\n+\n+      \/\/ Fast check for recursive lock.\n+      \/\/\n+      \/\/ Can apply the optimization only if this is a stack lock\n+      \/\/ allocated in this thread. For efficiency, we can focus on\n+      \/\/ recently allocated stack locks (instead of reading the stack\n+      \/\/ base and checking whether 'mark' points inside the current\n+      \/\/ thread stack):\n+      \/\/  1) (mark & 7) == 0, and\n+      \/\/  2) sp <= mark < mark + os::pagesize()\n+      \/\/\n+      \/\/ Warning: sp + os::pagesize can overflow the stack base. We must\n+      \/\/ neither apply the optimization for an inflated lock allocated\n+      \/\/ just above the thread stack (this is why condition 1 matters)\n+      \/\/ nor apply the optimization if the stack lock is inside the stack\n+      \/\/ of another thread. The latter is avoided even in case of overflow\n+      \/\/ because we have guard pages at the end of all stacks. Hence, if\n+      \/\/ we go over the stack base and hit the stack of another thread,\n+      \/\/ this should not be in a writeable area that could contain a\n+      \/\/ stack lock allocated by that thread. As a consequence, a stack\n+      \/\/ lock less than page size away from sp is guaranteed to be\n+      \/\/ owned by the current thread.\n+      \/\/\n+      \/\/ These 3 tests can be done by evaluating the following\n+      \/\/ expression: ((mark - sp) & (7 - os::vm_page_size())),\n+      \/\/ assuming both stack pointer and pagesize have their\n+      \/\/ least significant 3 bits clear.\n+      \/\/ NOTE: the mark is in swap_reg %r0 as the result of cmpxchg\n+      \/\/ NOTE2: aarch64 does not like to subtract sp from rn so take a\n+      \/\/ copy\n+      mov(rscratch1, sp);\n+      sub(swap_reg, swap_reg, rscratch1);\n+      ands(swap_reg, swap_reg, (uint64_t)(7 - (int)os::vm_page_size()));\n+\n+      \/\/ Save the test result, for recursive case, the result is zero\n+      str(swap_reg, Address(lock_reg, mark_offset));\n+      br(Assembler::EQ, count);\n+    }\n@@ -814,3 +819,9 @@\n-    call_VM(noreg,\n-            CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter),\n-            lock_reg);\n+    if (LockingMode == LM_LIGHTWEIGHT) {\n+      call_VM(noreg,\n+              CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter_obj),\n+              obj_reg);\n+    } else {\n+      call_VM(noreg,\n+              CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter),\n+              lock_reg);\n+    }\n@@ -842,1 +853,1 @@\n-  if (UseHeavyMonitors) {\n+  if (LockingMode == LM_MONITOR) {\n@@ -853,3 +864,5 @@\n-    \/\/ Convert from BasicObjectLock structure to object and BasicLock\n-    \/\/ structure Store the BasicLock address into %r0\n-    lea(swap_reg, Address(lock_reg, BasicObjectLock::lock_offset_in_bytes()));\n+    if (LockingMode != LM_LIGHTWEIGHT) {\n+      \/\/ Convert from BasicObjectLock structure to object and BasicLock\n+      \/\/ structure Store the BasicLock address into %r0\n+      lea(swap_reg, Address(lock_reg, BasicObjectLock::lock_offset_in_bytes()));\n+    }\n@@ -863,9 +876,15 @@\n-    \/\/ Load the old header from BasicLock structure\n-    ldr(header_reg, Address(swap_reg,\n-                            BasicLock::displaced_header_offset_in_bytes()));\n-\n-    \/\/ Test for recursion\n-    cbz(header_reg, count);\n-\n-    \/\/ Atomic swap back the old header\n-    cmpxchg_obj_header(swap_reg, header_reg, obj_reg, rscratch1, count, \/*fallthrough*\/NULL);\n+    if (LockingMode == LM_LIGHTWEIGHT) {\n+      Label slow_case;\n+\n+      \/\/ Check for non-symmetric locking. This is allowed by the spec and the interpreter\n+      \/\/ must handle it.\n+      Register tmp = rscratch1;\n+      \/\/ First check for lock-stack underflow.\n+      ldrw(tmp, Address(rthread, JavaThread::lock_stack_top_offset()));\n+      cmpw(tmp, (unsigned)LockStack::start_offset());\n+      br(Assembler::LE, slow_case);\n+      \/\/ Then check if the top of the lock-stack matches the unlocked object.\n+      subw(tmp, tmp, oopSize);\n+      ldr(tmp, Address(rthread, tmp));\n+      cmpoop(tmp, obj_reg);\n+      br(Assembler::NE, slow_case);\n@@ -873,0 +892,16 @@\n+      ldr(header_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      tbnz(header_reg, exact_log2(markWord::monitor_value), slow_case);\n+      fast_unlock(obj_reg, header_reg, swap_reg, rscratch1, slow_case);\n+      b(count);\n+      bind(slow_case);\n+    } else if (LockingMode == LM_LEGACY) {\n+      \/\/ Load the old header from BasicLock structure\n+      ldr(header_reg, Address(swap_reg,\n+                              BasicLock::displaced_header_offset_in_bytes()));\n+\n+      \/\/ Test for recursion\n+      cbz(header_reg, count);\n+\n+      \/\/ Atomic swap back the old header\n+      cmpxchg_obj_header(swap_reg, header_reg, obj_reg, rscratch1, count, \/*fallthrough*\/nullptr);\n+    }\n","filename":"src\/hotspot\/cpu\/aarch64\/interp_masm_aarch64.cpp","additions":102,"deletions":67,"binary":false,"changes":169,"status":"modified"},{"patch":"@@ -6209,0 +6209,94 @@\n+\n+\/\/ Implements fast-locking.\n+\/\/ Branches to slow upon failure to lock the object, with ZF cleared.\n+\/\/ Falls through upon success with ZF set.\n+\/\/\n+\/\/  - obj: the object to be locked\n+\/\/  - hdr: the header, already loaded from obj, will be destroyed\n+\/\/  - t1, t2: temporary registers, will be destroyed\n+void MacroAssembler::fast_lock(Register obj, Register hdr, Register t1, Register t2, Label& slow) {\n+  assert(LockingMode == LM_LIGHTWEIGHT, \"only used with new lightweight locking\");\n+  assert_different_registers(obj, hdr, t1, t2);\n+\n+  \/\/ Check if we would have space on lock-stack for the object.\n+  ldrw(t1, Address(rthread, JavaThread::lock_stack_top_offset()));\n+  cmpw(t1, (unsigned)LockStack::end_offset() - 1);\n+  br(Assembler::GT, slow);\n+\n+  \/\/ Load (object->mark() | 1) into hdr\n+  orr(hdr, hdr, markWord::unlocked_value);\n+  \/\/ Clear lock-bits, into t2\n+  eor(t2, hdr, markWord::unlocked_value);\n+  \/\/ Try to swing header from unlocked to locked\n+  cmpxchg(\/*addr*\/ obj, \/*expected*\/ hdr, \/*new*\/ t2, Assembler::xword,\n+          \/*acquire*\/ true, \/*release*\/ true, \/*weak*\/ false, t1);\n+  br(Assembler::NE, slow);\n+\n+  \/\/ After successful lock, push object on lock-stack\n+  ldrw(t1, Address(rthread, JavaThread::lock_stack_top_offset()));\n+  str(obj, Address(rthread, t1));\n+  addw(t1, t1, oopSize);\n+  strw(t1, Address(rthread, JavaThread::lock_stack_top_offset()));\n+}\n+\n+\/\/ Implements fast-unlocking.\n+\/\/ Branches to slow upon failure, with ZF cleared.\n+\/\/ Falls through upon success, with ZF set.\n+\/\/\n+\/\/ - obj: the object to be unlocked\n+\/\/ - hdr: the (pre-loaded) header of the object\n+\/\/ - t1, t2: temporary registers\n+void MacroAssembler::fast_unlock(Register obj, Register hdr, Register t1, Register t2, Label& slow) {\n+  assert(LockingMode == LM_LIGHTWEIGHT, \"only used with new lightweight locking\");\n+  assert_different_registers(obj, hdr, t1, t2);\n+\n+#ifdef ASSERT\n+  {\n+    \/\/ The following checks rely on the fact that LockStack is only ever modified by\n+    \/\/ its owning thread, even if the lock got inflated concurrently; removal of LockStack\n+    \/\/ entries after inflation will happen delayed in that case.\n+\n+    \/\/ Check for lock-stack underflow.\n+    Label stack_ok;\n+    ldrw(t1, Address(rthread, JavaThread::lock_stack_top_offset()));\n+    cmpw(t1, (unsigned)LockStack::start_offset());\n+    br(Assembler::GT, stack_ok);\n+    STOP(\"Lock-stack underflow\");\n+    bind(stack_ok);\n+  }\n+  {\n+    \/\/ Check if the top of the lock-stack matches the unlocked object.\n+    Label tos_ok;\n+    subw(t1, t1, oopSize);\n+    ldr(t1, Address(rthread, t1));\n+    cmpoop(t1, obj);\n+    br(Assembler::EQ, tos_ok);\n+    STOP(\"Top of lock-stack does not match the unlocked object\");\n+    bind(tos_ok);\n+  }\n+  {\n+    \/\/ Check that hdr is fast-locked.\n+    Label hdr_ok;\n+    tst(hdr, markWord::lock_mask_in_place);\n+    br(Assembler::EQ, hdr_ok);\n+    STOP(\"Header is not fast-locked\");\n+    bind(hdr_ok);\n+  }\n+#endif\n+\n+  \/\/ Load the new header (unlocked) into t1\n+  orr(t1, hdr, markWord::unlocked_value);\n+\n+  \/\/ Try to swing header from locked to unlocked\n+  cmpxchg(obj, hdr, t1, Assembler::xword,\n+          \/*acquire*\/ true, \/*release*\/ true, \/*weak*\/ false, t2);\n+  br(Assembler::NE, slow);\n+\n+  \/\/ After successful unlock, pop object from lock-stack\n+  ldrw(t1, Address(rthread, JavaThread::lock_stack_top_offset()));\n+  subw(t1, t1, oopSize);\n+#ifdef ASSERT\n+  str(zr, Address(rthread, t1));\n+#endif\n+  strw(t1, Address(rthread, JavaThread::lock_stack_top_offset()));\n+}\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":94,"deletions":0,"binary":false,"changes":94,"status":"modified"},{"patch":"@@ -1583,0 +1583,3 @@\n+  void fast_lock(Register obj, Register hdr, Register t1, Register t2, Label& slow);\n+  void fast_unlock(Register obj, Register hdr, Register t1, Register t2, Label& slow);\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -1781,1 +1781,3 @@\n-    if (!UseHeavyMonitors) {\n+    if (LockingMode == LM_MONITOR) {\n+      __ b(slow_path_lock);\n+    } else if (LockingMode == LM_LEGACY) {\n@@ -1790,1 +1792,1 @@\n-      __ cmpxchg_obj_header(r0, lock_reg, obj_reg, rscratch1, count, \/*fallthrough*\/NULL);\n+      __ cmpxchg_obj_header(r0, lock_reg, obj_reg, rscratch1, count, \/*fallthrough*\/nullptr);\n@@ -1811,1 +1813,3 @@\n-      __ b(slow_path_lock);\n+      assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n+      __ ldr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      __ fast_lock(obj_reg, swap_reg, tmp, rscratch1, slow_path_lock);\n@@ -1920,1 +1924,1 @@\n-    if (!UseHeavyMonitors) {\n+    if (LockingMode == LM_LEGACY) {\n@@ -1935,1 +1939,3 @@\n-    if (!UseHeavyMonitors) {\n+    if (LockingMode == LM_MONITOR) {\n+      __ b(slow_path_unlock);\n+    } else if (LockingMode == LM_LEGACY) {\n@@ -1947,1 +1953,5 @@\n-      __ b(slow_path_unlock);\n+      assert(LockingMode == LM_LIGHTWEIGHT, \"\");\n+      __ ldr(old_hdr, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      __ tbnz(old_hdr, exact_log2(markWord::monitor_value), slow_path_unlock);\n+      __ fast_unlock(obj_reg, old_hdr, swap_reg, rscratch1, slow_path_unlock);\n+      __ decrement(Address(rthread, JavaThread::held_monitor_count_offset()));\n","filename":"src\/hotspot\/cpu\/aarch64\/sharedRuntime_aarch64.cpp","additions":16,"deletions":6,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -2434,1 +2434,1 @@\n-  if (UseHeavyMonitors) {\n+  if (LockingMode == LM_MONITOR) {\n","filename":"src\/hotspot\/cpu\/arm\/c1_LIRAssembler_arm.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"logging\/log.hpp\"\n@@ -202,0 +203,1 @@\n+  \/\/ save object being locked into the BasicObjectLock\n@@ -215,2 +217,2 @@\n-  \/\/ On MP platforms the next load could return a 'stale' value if the memory location has been modified by another thread.\n-  \/\/ That would be acceptable as ether CAS or slow case path is taken in that case.\n+  if (LockingMode == LM_LIGHTWEIGHT) {\n+    log_trace(fastlock)(\"C1_MacroAssembler::lock fast\");\n@@ -218,2 +220,3 @@\n-  \/\/ Must be the first instruction here, because implicit null check relies on it\n-  ldr(hdr, Address(obj, oopDesc::mark_offset_in_bytes()));\n+    Register t1 = disp_hdr; \/\/ Needs saving, probably\n+    Register t2 = hdr;      \/\/ blow\n+    Register t3 = Rtemp;    \/\/ blow\n@@ -221,2 +224,2 @@\n-  tst(hdr, markWord::unlocked_value);\n-  b(fast_lock, ne);\n+    fast_lock_2(obj \/* obj *\/, t1, t2, t3, 1 \/* savemask - save t1 *\/, slow_case);\n+    \/\/ Success: fall through\n@@ -224,14 +227,1 @@\n-  \/\/ Check for recursive locking\n-  \/\/ See comments in InterpreterMacroAssembler::lock_object for\n-  \/\/ explanations on the fast recursive locking check.\n-  \/\/ -1- test low 2 bits\n-  movs(tmp2, AsmOperand(hdr, lsl, 30));\n-  \/\/ -2- test (hdr - SP) if the low two bits are 0\n-  sub(tmp2, hdr, SP, eq);\n-  movs(tmp2, AsmOperand(tmp2, lsr, exact_log2(os::vm_page_size())), eq);\n-  \/\/ If still 'eq' then recursive locking OK\n-  \/\/ set to zero if recursive lock, set to non zero otherwise (see discussion in JDK-8267042)\n-  str(tmp2, Address(disp_hdr, mark_offset));\n-  b(fast_lock_done, eq);\n-  \/\/ else need slow case\n-  b(slow_case);\n+  } else if (LockingMode == LM_LEGACY) {\n@@ -239,0 +229,2 @@\n+    \/\/ On MP platforms the next load could return a 'stale' value if the memory location has been modified by another thread.\n+    \/\/ That would be acceptable as ether CAS or slow case path is taken in that case.\n@@ -240,3 +232,2 @@\n-  bind(fast_lock);\n-  \/\/ Save previous object header in BasicLock structure and update the header\n-  str(hdr, Address(disp_hdr, mark_offset));\n+    \/\/ Must be the first instruction here, because implicit null check relies on it\n+    ldr(hdr, Address(obj, oopDesc::mark_offset_in_bytes()));\n@@ -244,1 +235,2 @@\n-  cas_for_lock_acquire(hdr, disp_hdr, obj, tmp2, slow_case);\n+    tst(hdr, markWord::unlocked_value);\n+    b(fast_lock, ne);\n@@ -246,1 +238,24 @@\n-  bind(fast_lock_done);\n+    \/\/ Check for recursive locking\n+    \/\/ See comments in InterpreterMacroAssembler::lock_object for\n+    \/\/ explanations on the fast recursive locking check.\n+    \/\/ -1- test low 2 bits\n+    movs(tmp2, AsmOperand(hdr, lsl, 30));\n+    \/\/ -2- test (hdr - SP) if the low two bits are 0\n+    sub(tmp2, hdr, SP, eq);\n+    movs(tmp2, AsmOperand(tmp2, lsr, exact_log2(os::vm_page_size())), eq);\n+    \/\/ If still 'eq' then recursive locking OK\n+    \/\/ set to zero if recursive lock, set to non zero otherwise (see discussion in JDK-8267042)\n+    str(tmp2, Address(disp_hdr, mark_offset));\n+    b(fast_lock_done, eq);\n+    \/\/ else need slow case\n+    b(slow_case);\n+\n+\n+    bind(fast_lock);\n+    \/\/ Save previous object header in BasicLock structure and update the header\n+    str(hdr, Address(disp_hdr, mark_offset));\n+\n+    cas_for_lock_acquire(hdr, disp_hdr, obj, tmp2, slow_case);\n+\n+    bind(fast_lock_done);\n+  }\n@@ -264,4 +279,2 @@\n-  \/\/ Load displaced header and object from the lock\n-  ldr(hdr, Address(disp_hdr, mark_offset));\n-  \/\/ If hdr is null, we've got recursive locking and there's nothing more to do\n-  cbz(hdr, done);\n+  if (LockingMode == LM_LIGHTWEIGHT) {\n+    log_trace(fastlock)(\"C1_MacroAssembler::unlock fast\");\n@@ -269,2 +282,1 @@\n-  \/\/ load object\n-  ldr(obj, Address(disp_hdr, obj_offset));\n+    ldr(obj, Address(disp_hdr, obj_offset));\n@@ -272,2 +284,3 @@\n-  \/\/ Restore the object header\n-  cas_for_lock_release(disp_hdr, hdr, obj, tmp2, slow_case);\n+    Register t1 = disp_hdr; \/\/ Needs saving, probably\n+    Register t2 = hdr;      \/\/ blow\n+    Register t3 = Rtemp;    \/\/ blow\n@@ -275,0 +288,17 @@\n+    fast_unlock_2(obj \/* object *\/, t1, t2, t3, 1 \/* savemask (save t1) *\/,\n+                    slow_case);\n+    \/\/ Success: Fall through\n+\n+  } else if (LockingMode == LM_LEGACY) {\n+\n+    \/\/ Load displaced header and object from the lock\n+    ldr(hdr, Address(disp_hdr, mark_offset));\n+    \/\/ If hdr is null, we've got recursive locking and there's nothing more to do\n+    cbz(hdr, done);\n+\n+    \/\/ load object\n+    ldr(obj, Address(disp_hdr, obj_offset));\n+\n+    \/\/ Restore the object header\n+    cas_for_lock_release(disp_hdr, hdr, obj, tmp2, slow_case);\n+  }\n@@ -278,1 +308,0 @@\n-\n","filename":"src\/hotspot\/cpu\/arm\/c1_MacroAssembler_arm.cpp","additions":63,"deletions":34,"binary":false,"changes":97,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"logging\/log.hpp\"\n@@ -83,7 +84,1 @@\n-\n-  Register Rmark      = Rscratch2;\n-\n-  assert(Roop != Rscratch, \"\");\n-  assert(Roop != Rmark, \"\");\n-  assert(Rbox != Rscratch, \"\");\n-  assert(Rbox != Rmark, \"\");\n+  assert_different_registers(Roop, Rbox, Rscratch, Rscratch2);\n@@ -100,23 +95,37 @@\n-  ldr(Rmark, Address(Roop, oopDesc::mark_offset_in_bytes()));\n-  tst(Rmark, markWord::unlocked_value);\n-  b(fast_lock, ne);\n-\n-  \/\/ Check for recursive lock\n-  \/\/ See comments in InterpreterMacroAssembler::lock_object for\n-  \/\/ explanations on the fast recursive locking check.\n-  \/\/ -1- test low 2 bits\n-  movs(Rscratch, AsmOperand(Rmark, lsl, 30));\n-  \/\/ -2- test (hdr - SP) if the low two bits are 0\n-  sub(Rscratch, Rmark, SP, eq);\n-  movs(Rscratch, AsmOperand(Rscratch, lsr, exact_log2(os::vm_page_size())), eq);\n-  \/\/ If still 'eq' then recursive locking OK\n-  \/\/ set to zero if recursive lock, set to non zero otherwise (see discussion in JDK-8153107)\n-  str(Rscratch, Address(Rbox, BasicLock::displaced_header_offset_in_bytes()));\n-  b(done);\n-\n-  bind(fast_lock);\n-  str(Rmark, Address(Rbox, BasicLock::displaced_header_offset_in_bytes()));\n-\n-  bool allow_fallthrough_on_failure = true;\n-  bool one_shot = true;\n-  cas_for_lock_acquire(Rmark, Rbox, Roop, Rscratch, done, allow_fallthrough_on_failure, one_shot);\n+  if (LockingMode == LM_LIGHTWEIGHT) {\n+    log_trace(fastlock)(\"C2_MacroAssembler::lock fast\");\n+\n+    fast_lock_2(Roop \/* obj *\/, Rbox \/* t1 *\/, Rscratch \/* t2 *\/, Rscratch2 \/* t3 *\/,\n+                1 \/* savemask (save t1) *\/, done);\n+\n+    \/\/ Success: set Z\n+    cmp(Roop, Roop);\n+\n+  } else if (LockingMode == LM_LEGACY) {\n+\n+    Register Rmark      = Rscratch2;\n+\n+    ldr(Rmark, Address(Roop, oopDesc::mark_offset_in_bytes()));\n+    tst(Rmark, markWord::unlocked_value);\n+    b(fast_lock, ne);\n+\n+    \/\/ Check for recursive lock\n+    \/\/ See comments in InterpreterMacroAssembler::lock_object for\n+    \/\/ explanations on the fast recursive locking check.\n+    \/\/ -1- test low 2 bits\n+    movs(Rscratch, AsmOperand(Rmark, lsl, 30));\n+    \/\/ -2- test (hdr - SP) if the low two bits are 0\n+    sub(Rscratch, Rmark, SP, eq);\n+    movs(Rscratch, AsmOperand(Rscratch, lsr, exact_log2(os::vm_page_size())), eq);\n+    \/\/ If still 'eq' then recursive locking OK\n+    \/\/ set to zero if recursive lock, set to non zero otherwise (see discussion in JDK-8153107)\n+    str(Rscratch, Address(Rbox, BasicLock::displaced_header_offset_in_bytes()));\n+    b(done);\n+\n+    bind(fast_lock);\n+    str(Rmark, Address(Rbox, BasicLock::displaced_header_offset_in_bytes()));\n+\n+    bool allow_fallthrough_on_failure = true;\n+    bool one_shot = true;\n+    cas_for_lock_acquire(Rmark, Rbox, Roop, Rscratch, done, allow_fallthrough_on_failure, one_shot);\n+  }\n@@ -133,0 +142,1 @@\n+  assert_different_registers(Roop, Rbox, Rscratch, Rscratch2);\n@@ -134,1 +144,1 @@\n-  Register Rmark      = Rscratch2;\n+  Label done;\n@@ -136,4 +146,2 @@\n-  assert(Roop != Rscratch, \"\");\n-  assert(Roop != Rmark, \"\");\n-  assert(Rbox != Rscratch, \"\");\n-  assert(Rbox != Rmark, \"\");\n+  if (LockingMode == LM_LIGHTWEIGHT) {\n+    log_trace(fastlock)(\"C2_MacroAssembler::unlock fast\");\n@@ -141,1 +149,5 @@\n-  Label done;\n+    fast_unlock_2(Roop \/* obj *\/, Rbox \/* t1 *\/, Rscratch \/* t2 *\/, Rscratch2 \/* t3 *\/,\n+                  1 \/* savemask (save t1) *\/, done);\n+\n+    cmp(Roop, Roop); \/\/ Success: Set Z\n+    \/\/ Fall through\n@@ -143,4 +155,1 @@\n-  ldr(Rmark, Address(Rbox, BasicLock::displaced_header_offset_in_bytes()));\n-  \/\/ If hdr is null, we've got recursive locking and there's nothing more to do\n-  cmp(Rmark, 0);\n-  b(done, eq);\n+  } else if (LockingMode == LM_LEGACY) {\n@@ -148,4 +157,1 @@\n-  \/\/ Restore the object header\n-  bool allow_fallthrough_on_failure = true;\n-  bool one_shot = true;\n-  cas_for_lock_release(Rbox, Rmark, Roop, Rscratch, done, allow_fallthrough_on_failure, one_shot);\n+    Register Rmark      = Rscratch2;\n@@ -153,0 +159,11 @@\n+    \/\/ Find the lock address and load the displaced header from the stack.\n+    ldr(Rmark, Address(Rbox, BasicLock::displaced_header_offset_in_bytes()));\n+    \/\/ If hdr is null, we've got recursive locking and there's nothing more to do\n+    cmp(Rmark, 0);\n+    b(done, eq);\n+\n+    \/\/ Restore the object header\n+    bool allow_fallthrough_on_failure = true;\n+    bool one_shot = true;\n+    cas_for_lock_release(Rbox, Rmark, Roop, Rscratch, done, allow_fallthrough_on_failure, one_shot);\n+  }\n@@ -154,1 +171,0 @@\n-}\n@@ -156,0 +172,4 @@\n+  \/\/ At this point flags are set as follows:\n+  \/\/  EQ -> Success\n+  \/\/  NE -> Failure, branch to slow path\n+}\n","filename":"src\/hotspot\/cpu\/arm\/c2_MacroAssembler_arm.cpp","additions":65,"deletions":45,"binary":false,"changes":110,"status":"modified"},{"patch":"@@ -867,1 +867,1 @@\n-  if (UseHeavyMonitors) {\n+  if (LockingMode == LM_MONITOR) {\n@@ -892,62 +892,68 @@\n-    \/\/ On MP platforms the next load could return a 'stale' value if the memory location has been modified by another thread.\n-    \/\/ That would be acceptable as ether CAS or slow case path is taken in that case.\n-    \/\/ Exception to that is if the object is locked by the calling thread, then the recursive test will pass (guaranteed as\n-    \/\/ loads are satisfied from a store queue if performed on the same processor).\n-\n-    assert(oopDesc::mark_offset_in_bytes() == 0, \"must be\");\n-    ldr(Rmark, Address(Robj, oopDesc::mark_offset_in_bytes()));\n-\n-    \/\/ Test if object is already locked\n-    tst(Rmark, markWord::unlocked_value);\n-    b(already_locked, eq);\n-\n-    \/\/ Save old object->mark() into BasicLock's displaced header\n-    str(Rmark, Address(Rlock, mark_offset));\n-\n-    cas_for_lock_acquire(Rmark, Rlock, Robj, Rtemp, slow_case);\n-\n-    b(done);\n-\n-    \/\/ If we got here that means the object is locked by ether calling thread or another thread.\n-    bind(already_locked);\n-    \/\/ Handling of locked objects: recursive locks and slow case.\n-\n-    \/\/ Fast check for recursive lock.\n-    \/\/\n-    \/\/ Can apply the optimization only if this is a stack lock\n-    \/\/ allocated in this thread. For efficiency, we can focus on\n-    \/\/ recently allocated stack locks (instead of reading the stack\n-    \/\/ base and checking whether 'mark' points inside the current\n-    \/\/ thread stack):\n-    \/\/  1) (mark & 3) == 0\n-    \/\/  2) SP <= mark < SP + os::pagesize()\n-    \/\/\n-    \/\/ Warning: SP + os::pagesize can overflow the stack base. We must\n-    \/\/ neither apply the optimization for an inflated lock allocated\n-    \/\/ just above the thread stack (this is why condition 1 matters)\n-    \/\/ nor apply the optimization if the stack lock is inside the stack\n-    \/\/ of another thread. The latter is avoided even in case of overflow\n-    \/\/ because we have guard pages at the end of all stacks. Hence, if\n-    \/\/ we go over the stack base and hit the stack of another thread,\n-    \/\/ this should not be in a writeable area that could contain a\n-    \/\/ stack lock allocated by that thread. As a consequence, a stack\n-    \/\/ lock less than page size away from SP is guaranteed to be\n-    \/\/ owned by the current thread.\n-    \/\/\n-    \/\/ Note: assuming SP is aligned, we can check the low bits of\n-    \/\/ (mark-SP) instead of the low bits of mark. In that case,\n-    \/\/ assuming page size is a power of 2, we can merge the two\n-    \/\/ conditions into a single test:\n-    \/\/ => ((mark - SP) & (3 - os::pagesize())) == 0\n-\n-    \/\/ (3 - os::pagesize()) cannot be encoded as an ARM immediate operand.\n-    \/\/ Check independently the low bits and the distance to SP.\n-    \/\/ -1- test low 2 bits\n-    movs(R0, AsmOperand(Rmark, lsl, 30));\n-    \/\/ -2- test (mark - SP) if the low two bits are 0\n-    sub(R0, Rmark, SP, eq);\n-    movs(R0, AsmOperand(R0, lsr, exact_log2(os::vm_page_size())), eq);\n-    \/\/ If still 'eq' then recursive locking OK: store 0 into lock record\n-    str(R0, Address(Rlock, mark_offset), eq);\n-\n-    b(done, eq);\n+    if (LockingMode == LM_LIGHTWEIGHT) {\n+      log_trace(fastlock)(\"InterpreterMacroAssembler lock fast\");\n+      fast_lock_2(Robj, R0 \/* t1 *\/, Rmark \/* t2 *\/, Rtemp \/* t3 *\/, 0 \/* savemask *\/, slow_case);\n+      b(done);\n+    } else if (LockingMode == LM_LEGACY) {\n+      \/\/ On MP platforms the next load could return a 'stale' value if the memory location has been modified by another thread.\n+      \/\/ That would be acceptable as ether CAS or slow case path is taken in that case.\n+      \/\/ Exception to that is if the object is locked by the calling thread, then the recursive test will pass (guaranteed as\n+      \/\/ loads are satisfied from a store queue if performed on the same processor).\n+\n+      assert(oopDesc::mark_offset_in_bytes() == 0, \"must be\");\n+      ldr(Rmark, Address(Robj, oopDesc::mark_offset_in_bytes()));\n+\n+      \/\/ Test if object is already locked\n+      tst(Rmark, markWord::unlocked_value);\n+      b(already_locked, eq);\n+\n+      \/\/ Save old object->mark() into BasicLock's displaced header\n+      str(Rmark, Address(Rlock, mark_offset));\n+\n+      cas_for_lock_acquire(Rmark, Rlock, Robj, Rtemp, slow_case);\n+\n+      b(done);\n+\n+      \/\/ If we got here that means the object is locked by ether calling thread or another thread.\n+      bind(already_locked);\n+      \/\/ Handling of locked objects: recursive locks and slow case.\n+\n+      \/\/ Fast check for recursive lock.\n+      \/\/\n+      \/\/ Can apply the optimization only if this is a stack lock\n+      \/\/ allocated in this thread. For efficiency, we can focus on\n+      \/\/ recently allocated stack locks (instead of reading the stack\n+      \/\/ base and checking whether 'mark' points inside the current\n+      \/\/ thread stack):\n+      \/\/  1) (mark & 3) == 0\n+      \/\/  2) SP <= mark < SP + os::pagesize()\n+      \/\/\n+      \/\/ Warning: SP + os::pagesize can overflow the stack base. We must\n+      \/\/ neither apply the optimization for an inflated lock allocated\n+      \/\/ just above the thread stack (this is why condition 1 matters)\n+      \/\/ nor apply the optimization if the stack lock is inside the stack\n+      \/\/ of another thread. The latter is avoided even in case of overflow\n+      \/\/ because we have guard pages at the end of all stacks. Hence, if\n+      \/\/ we go over the stack base and hit the stack of another thread,\n+      \/\/ this should not be in a writeable area that could contain a\n+      \/\/ stack lock allocated by that thread. As a consequence, a stack\n+      \/\/ lock less than page size away from SP is guaranteed to be\n+      \/\/ owned by the current thread.\n+      \/\/\n+      \/\/ Note: assuming SP is aligned, we can check the low bits of\n+      \/\/ (mark-SP) instead of the low bits of mark. In that case,\n+      \/\/ assuming page size is a power of 2, we can merge the two\n+      \/\/ conditions into a single test:\n+      \/\/ => ((mark - SP) & (3 - os::pagesize())) == 0\n+\n+      \/\/ (3 - os::pagesize()) cannot be encoded as an ARM immediate operand.\n+      \/\/ Check independently the low bits and the distance to SP.\n+      \/\/ -1- test low 2 bits\n+      movs(R0, AsmOperand(Rmark, lsl, 30));\n+      \/\/ -2- test (mark - SP) if the low two bits are 0\n+      sub(R0, Rmark, SP, eq);\n+      movs(R0, AsmOperand(R0, lsr, exact_log2(os::vm_page_size())), eq);\n+      \/\/ If still 'eq' then recursive locking OK: store 0 into lock record\n+      str(R0, Address(Rlock, mark_offset), eq);\n+\n+      b(done, eq);\n+    }\n@@ -958,2 +964,9 @@\n-    call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter), Rlock);\n-\n+    if (LockingMode == LM_LIGHTWEIGHT) {\n+      \/\/ Pass oop, not lock, in fast lock case. call_VM wants R1 though.\n+      push(R1);\n+      mov(R1, Robj);\n+      call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter_obj), R1);\n+      pop(R1);\n+    } else {\n+      call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter), Rlock);\n+    }\n@@ -964,1 +977,0 @@\n-\n@@ -973,1 +985,1 @@\n-  if (UseHeavyMonitors) {\n+  if (LockingMode == LM_MONITOR) {\n@@ -994,2 +1006,1 @@\n-    \/\/ Load the old header from BasicLock structure\n-    ldr(Rmark, Address(Rlock, mark_offset));\n+    if (LockingMode == LM_LIGHTWEIGHT) {\n@@ -997,2 +1008,9 @@\n-    \/\/ Test for recursion (zero mark in BasicLock)\n-    cbz(Rmark, done);\n+      log_trace(fastlock)(\"InterpreterMacroAssembler unlock fast\");\n+\n+      \/\/ Check for non-symmetric locking. This is allowed by the spec and the interpreter\n+      \/\/ must handle it.\n+      ldr(Rtemp, Address(Rthread, JavaThread::lock_stack_top_offset()));\n+      sub(Rtemp, Rtemp, oopSize);\n+      ldr(Rtemp, Address(Rthread, Rtemp));\n+      cmpoop(Rtemp, Robj);\n+      b(slow_case, ne);\n@@ -1000,1 +1018,2 @@\n-    bool allow_fallthrough_on_failure = true;\n+      fast_unlock_2(Robj \/* obj *\/, Rlock \/* t1 *\/, Rmark \/* t2 *\/, Rtemp \/* t3 *\/,\n+                    1 \/* savemask (save t1) *\/, slow_case);\n@@ -1002,1 +1021,1 @@\n-    cas_for_lock_release(Rlock, Rmark, Robj, Rtemp, slow_case, allow_fallthrough_on_failure);\n+      b(done);\n@@ -1004,1 +1023,1 @@\n-    b(done, eq);\n+    } else if (LockingMode == LM_LEGACY) {\n@@ -1006,0 +1025,13 @@\n+      \/\/ Load the old header from BasicLock structure\n+      ldr(Rmark, Address(Rlock, mark_offset));\n+\n+      \/\/ Test for recursion (zero mark in BasicLock)\n+      cbz(Rmark, done);\n+\n+      bool allow_fallthrough_on_failure = true;\n+\n+      cas_for_lock_release(Rlock, Rmark, Robj, Rtemp, slow_case, allow_fallthrough_on_failure);\n+\n+      b(done, eq);\n+\n+    }\n@@ -1016,1 +1048,0 @@\n-\n","filename":"src\/hotspot\/cpu\/arm\/interp_masm_arm.cpp","additions":106,"deletions":75,"binary":false,"changes":181,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright (c) 2023, Red Hat, Inc.\n@@ -45,0 +46,1 @@\n+#include \"runtime\/javaThread.hpp\"\n@@ -1197,0 +1199,2 @@\n+  \/\/ Here, on success, EQ is set, NE otherwise\n+\n@@ -1202,0 +1206,2 @@\n+  \/\/ Note: we preserve flags here.\n+  \/\/ Todo: Do we really need this also for the CAS fail case?\n@@ -1212,1 +1218,0 @@\n-\n@@ -1716,0 +1721,142 @@\n+#define PUSH_REG(mask, bit, Reg)      \\\n+  if (mask & ((unsigned)1 << bit)) {  \\\n+    push(Reg);                        \\\n+  }\n+\n+#define POP_REG(mask, bit, Reg, condition)   \\\n+  if (mask & ((unsigned)1 << bit)) {         \\\n+    pop(Reg, condition);                     \\\n+  }\n+\n+#define PUSH_REGS(mask, R1, R2, R3) \\\n+  PUSH_REG(mask, 0, R1)             \\\n+  PUSH_REG(mask, 1, R2)             \\\n+  PUSH_REG(mask, 2, R3)\n+\n+#define POP_REGS(mask, R1, R2, R3, condition)   \\\n+  POP_REG(mask, 0, R1, condition)               \\\n+  POP_REG(mask, 1, R2, condition)               \\\n+  POP_REG(mask, 2, R3, condition)\n+\n+#define POISON_REG(mask, bit, Reg, poison)      \\\n+  if (mask & ((unsigned)1 << bit)) {            \\\n+    mov(Reg, poison);                           \\\n+  }\n+\n+#define POISON_REGS(mask, R1, R2, R3, poison)   \\\n+  POISON_REG(mask, 0, R1, poison)               \\\n+  POISON_REG(mask, 1, R2, poison)               \\\n+  POISON_REG(mask, 2, R3, poison)\n+\n+\/\/ Attempt to fast-lock an object\n+\/\/ Registers:\n+\/\/  - obj: the object to be locked\n+\/\/  - t1, t2, t3: temp registers. If corresponding bit in savemask is set, they get saved, otherwise blown.\n+\/\/ Result:\n+\/\/  - Success: fallthrough\n+\/\/  - Error:   break to slow, Z cleared.\n+void MacroAssembler::fast_lock_2(Register obj, Register t1, Register t2, Register t3, unsigned savemask, Label& slow) {\n+  assert(LockingMode == LM_LIGHTWEIGHT, \"only used with new lightweight locking\");\n+  assert_different_registers(obj, t1, t2, t3);\n+\n+#ifdef ASSERT\n+  \/\/ Poison scratch regs\n+  POISON_REGS((~savemask), t1, t2, t3, 0x10000001);\n+#endif\n+\n+  PUSH_REGS(savemask, t1, t2, t3);\n+\n+  \/\/ Check if we would have space on lock-stack for the object.\n+  ldr(t1, Address(Rthread, JavaThread::lock_stack_top_offset()));\n+  \/\/ cmp(t1, (unsigned)LockStack::end_offset()); \/\/  too complicated constant: 1132 (46c)\n+  movw(t2, LockStack::end_offset() - 1);\n+  cmp(t1, t2);\n+  POP_REGS(savemask, t1, t2, t3, gt);\n+  b(slow, gt); \/\/ Z is cleared\n+\n+  \/\/ Prepare old, new header\n+  Register old_hdr = t1;\n+  Register new_hdr = t2;\n+  ldr(new_hdr, Address(obj, oopDesc::mark_offset_in_bytes()));\n+  bic(new_hdr, new_hdr, markWord::lock_mask_in_place);  \/\/ new header (00)\n+  orr(old_hdr, new_hdr, markWord::unlocked_value);      \/\/ old header (01)\n+\n+  Label dummy;\n+\n+  cas_for_lock_acquire(old_hdr \/* old *\/, new_hdr \/* new *\/,\n+      obj \/* location *\/, t3 \/* scratch *\/, dummy,\n+      true \/* allow_fallthrough_on_failure *\/, true \/* one_shot *\/);\n+\n+  POP_REGS(savemask, t1, t2, t3, ne); \/\/ Cas failed -> slow\n+  b(slow, ne);                        \/\/ Cas failed -> slow\n+\n+  \/\/ After successful lock, push object onto lock-stack\n+  ldr(t1, Address(Rthread, JavaThread::lock_stack_top_offset()));\n+  str(obj, Address(Rthread, t1));\n+  add(t1, t1, oopSize);\n+  str(t1, Address(Rthread, JavaThread::lock_stack_top_offset()));\n+\n+  POP_REGS(savemask, t1, t2, t3, al);\n+\n+#ifdef ASSERT\n+  \/\/ Poison scratch regs\n+  POISON_REGS((~savemask), t1, t2, t3, 0x20000002);\n+#endif\n+\n+  \/\/ Success: fall through\n+}\n+\n+\/\/ Attempt to fast-unlock an object\n+\/\/ Registers:\n+\/\/  - obj: the object to be unlocked\n+\/\/  - t1, t2, t3: temp registers. If corresponding bit in savemask is set, they get saved, otherwise blown.\n+\/\/ Result:\n+\/\/  - Success: fallthrough\n+\/\/  - Error:   break to slow, Z cleared.\n+void MacroAssembler::fast_unlock_2(Register obj, Register t1, Register t2, Register t3, unsigned savemask, Label& slow) {\n+  assert(LockingMode == LM_LIGHTWEIGHT, \"only used with new lightweight locking\");\n+  assert_different_registers(obj, t1, t2, t3);\n+\n+#ifdef ASSERT\n+  \/\/ Poison scratch regs\n+  POISON_REGS((~savemask), t1, t2, t3, 0x30000003);\n+#endif\n+\n+  PUSH_REGS(savemask, t1, t2, t3);\n+\n+  \/\/ Prepare old, new header\n+  Register old_hdr = t1;\n+  Register new_hdr = t2;\n+  ldr(old_hdr, Address(obj, oopDesc::mark_offset_in_bytes()));\n+  bic(old_hdr, old_hdr, markWord::lock_mask_in_place);    \/\/ old header (00)\n+  orr(new_hdr, old_hdr, markWord::unlocked_value);        \/\/ new header (01)\n+\n+  \/\/ Try to swing header from locked to unlocked\n+  Label dummy;\n+  cas_for_lock_release(old_hdr \/* old *\/, new_hdr \/* new *\/,\n+      obj \/* location *\/, t3 \/* scratch *\/, dummy,\n+      true \/* allow_fallthrough_on_failure *\/, true \/* one_shot *\/);\n+\n+  POP_REGS(savemask, t1, t2, t3, ne); \/\/ Cas failed -> slow\n+  b(slow, ne);                        \/\/ Cas failed -> slow\n+\n+  \/\/ After successful unlock, pop object from lock-stack\n+  ldr(t1, Address(Rthread, JavaThread::lock_stack_top_offset()));\n+  sub(t1, t1, oopSize);\n+  str(t1, Address(Rthread, JavaThread::lock_stack_top_offset()));\n+\n+#ifdef ASSERT\n+  \/\/ zero out popped slot\n+  mov(t2, 0);\n+  str(t2, Address(Rthread, t1));\n+#endif\n+\n+  POP_REGS(savemask, t1, t2, t3, al);\n+\n+#ifdef ASSERT\n+  \/\/ Poison scratch regs\n+  POISON_REGS((~savemask), t1, t2, t3, 0x40000004);\n+#endif\n+\n+  \/\/ Fallthrough: success\n+}\n","filename":"src\/hotspot\/cpu\/arm\/macroAssembler_arm.cpp","additions":148,"deletions":1,"binary":false,"changes":149,"status":"modified"},{"patch":"@@ -1012,0 +1012,18 @@\n+  \/\/ Attempt to fast-lock an object\n+  \/\/ Registers:\n+  \/\/  - obj: the object to be locked\n+  \/\/  - t1, t2, t3: temp registers. If corresponding bit in savemask is set, they get saved, otherwise blown.\n+  \/\/ Result:\n+  \/\/  - Success: fallthrough\n+  \/\/  - Error:   break to slow, Z cleared.\n+  void fast_lock_2(Register obj, Register t1, Register t2, Register t3, unsigned savemask, Label& slow);\n+\n+  \/\/ Attempt to fast-unlock an object\n+  \/\/ Registers:\n+  \/\/  - obj: the object to be unlocked\n+  \/\/  - t1, t2, t3: temp registers. If corresponding bit in savemask is set, they get saved, otherwise blown.\n+  \/\/ Result:\n+  \/\/  - Success: fallthrough\n+  \/\/  - Error:   break to slow, Z cleared.\n+  void fast_unlock_2(Register obj, Register t1, Register t2, Register t3, unsigned savemask, Label& slow);\n+\n","filename":"src\/hotspot\/cpu\/arm\/macroAssembler_arm.hpp","additions":18,"deletions":0,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -1156,29 +1156,35 @@\n-    const Register mark = tmp;\n-    \/\/ On MP platforms the next load could return a 'stale' value if the memory location has been modified by another thread.\n-    \/\/ That would be acceptable as either CAS or slow case path is taken in that case\n-\n-    __ ldr(mark, Address(sync_obj, oopDesc::mark_offset_in_bytes()));\n-    __ sub(disp_hdr, FP, lock_slot_fp_offset);\n-    __ tst(mark, markWord::unlocked_value);\n-    __ b(fast_lock, ne);\n-\n-    \/\/ Check for recursive lock\n-    \/\/ See comments in InterpreterMacroAssembler::lock_object for\n-    \/\/ explanations on the fast recursive locking check.\n-    \/\/ Check independently the low bits and the distance to SP\n-    \/\/ -1- test low 2 bits\n-    __ movs(Rtemp, AsmOperand(mark, lsl, 30));\n-    \/\/ -2- test (hdr - SP) if the low two bits are 0\n-    __ sub(Rtemp, mark, SP, eq);\n-    __ movs(Rtemp, AsmOperand(Rtemp, lsr, exact_log2(os::vm_page_size())), eq);\n-    \/\/ If still 'eq' then recursive locking OK\n-    \/\/ set to zero if recursive lock, set to non zero otherwise (see discussion in JDK-8267042)\n-    __ str(Rtemp, Address(disp_hdr, BasicLock::displaced_header_offset_in_bytes()));\n-    __ b(lock_done, eq);\n-    __ b(slow_lock);\n-\n-    __ bind(fast_lock);\n-    __ str(mark, Address(disp_hdr, BasicLock::displaced_header_offset_in_bytes()));\n-\n-    __ cas_for_lock_acquire(mark, disp_hdr, sync_obj, Rtemp, slow_lock);\n-\n+    if (LockingMode == LM_LIGHTWEIGHT) {\n+      log_trace(fastlock)(\"SharedRuntime lock fast\");\n+      __ fast_lock_2(sync_obj \/* object *\/, disp_hdr \/* t1 *\/, tmp \/* t2 *\/, Rtemp \/* t3 *\/,\n+                     0x7 \/* savemask *\/, slow_lock);\n+      \/\/ Fall through to lock_done\n+    } else if (LockingMode == LM_LEGACY) {\n+      const Register mark = tmp;\n+      \/\/ On MP platforms the next load could return a 'stale' value if the memory location has been modified by another thread.\n+      \/\/ That would be acceptable as either CAS or slow case path is taken in that case\n+\n+      __ ldr(mark, Address(sync_obj, oopDesc::mark_offset_in_bytes()));\n+      __ sub(disp_hdr, FP, lock_slot_fp_offset);\n+      __ tst(mark, markWord::unlocked_value);\n+      __ b(fast_lock, ne);\n+\n+      \/\/ Check for recursive lock\n+      \/\/ See comments in InterpreterMacroAssembler::lock_object for\n+      \/\/ explanations on the fast recursive locking check.\n+      \/\/ Check independently the low bits and the distance to SP\n+      \/\/ -1- test low 2 bits\n+      __ movs(Rtemp, AsmOperand(mark, lsl, 30));\n+      \/\/ -2- test (hdr - SP) if the low two bits are 0\n+      __ sub(Rtemp, mark, SP, eq);\n+      __ movs(Rtemp, AsmOperand(Rtemp, lsr, exact_log2(os::vm_page_size())), eq);\n+      \/\/ If still 'eq' then recursive locking OK\n+      \/\/ set to zero if recursive lock, set to non zero otherwise (see discussion in JDK-8267042)\n+      __ str(Rtemp, Address(disp_hdr, BasicLock::displaced_header_offset_in_bytes()));\n+      __ b(lock_done, eq);\n+      __ b(slow_lock);\n+\n+      __ bind(fast_lock);\n+      __ str(mark, Address(disp_hdr, BasicLock::displaced_header_offset_in_bytes()));\n+\n+      __ cas_for_lock_acquire(mark, disp_hdr, sync_obj, Rtemp, slow_lock);\n+    }\n@@ -1237,8 +1243,15 @@\n-    __ ldr(sync_obj, Address(sync_handle));\n-\n-    \/\/ See C1_MacroAssembler::unlock_object() for more comments\n-    __ ldr(R2, Address(disp_hdr, BasicLock::displaced_header_offset_in_bytes()));\n-    __ cbz(R2, unlock_done);\n-\n-    __ cas_for_lock_release(disp_hdr, R2, sync_obj, Rtemp, slow_unlock);\n-\n+    if (LockingMode == LM_LIGHTWEIGHT) {\n+      log_trace(fastlock)(\"SharedRuntime unlock fast\");\n+      __ fast_unlock_2(sync_obj, R2 \/* t1 *\/, tmp \/* t2 *\/, Rtemp \/* t3 *\/,\n+                       7 \/* savemask *\/, slow_unlock);\n+      \/\/ Fall through\n+    } else if (LockingMode == LM_LEGACY) {\n+      \/\/ See C1_MacroAssembler::unlock_object() for more comments\n+      __ ldr(sync_obj, Address(sync_handle));\n+\n+      \/\/ See C1_MacroAssembler::unlock_object() for more comments\n+      __ ldr(R2, Address(disp_hdr, BasicLock::displaced_header_offset_in_bytes()));\n+      __ cbz(R2, unlock_done);\n+\n+      __ cas_for_lock_release(disp_hdr, R2, sync_obj, Rtemp, slow_unlock);\n+    }\n","filename":"src\/hotspot\/cpu\/arm\/sharedRuntime_arm.cpp","additions":50,"deletions":37,"binary":false,"changes":87,"status":"modified"},{"patch":"@@ -363,1 +363,1 @@\n-    if (UseHeavyMonitors) {\n+    if (LockingMode == LM_MONITOR) {\n@@ -1502,1 +1502,1 @@\n-  if (UseHeavyMonitors) {\n+  if (LockingMode == LM_MONITOR) {\n","filename":"src\/hotspot\/cpu\/riscv\/c1_LIRAssembler_riscv.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -55,2 +55,1 @@\n-  assert(hdr != obj && hdr != disp_hdr && obj != disp_hdr, \"registers must be different\");\n-  Label done;\n+  assert_different_registers(hdr, obj, disp_hdr);\n@@ -75,33 +74,40 @@\n-  \/\/ and mark it as unlocked\n-  ori(hdr, hdr, markWord::unlocked_value);\n-  \/\/ save unlocked object header into the displaced header location on the stack\n-  sd(hdr, Address(disp_hdr, 0));\n-  \/\/ test if object header is still the same (i.e. unlocked), and if so, store the\n-  \/\/ displaced header address in the object header - if it is not the same, get the\n-  \/\/ object header instead\n-  la(t1, Address(obj, hdr_offset));\n-  cmpxchgptr(hdr, disp_hdr, t1, t0, done, \/*fallthough*\/nullptr);\n-  \/\/ if the object header was the same, we're done\n-  \/\/ if the object header was not the same, it is now in the hdr register\n-  \/\/ => test if it is a stack pointer into the same stack (recursive locking), i.e.:\n-  \/\/\n-  \/\/ 1) (hdr & aligned_mask) == 0\n-  \/\/ 2) sp <= hdr\n-  \/\/ 3) hdr <= sp + page_size\n-  \/\/\n-  \/\/ these 3 tests can be done by evaluating the following expression:\n-  \/\/\n-  \/\/ (hdr -sp) & (aligned_mask - page_size)\n-  \/\/\n-  \/\/ assuming both the stack pointer and page_size have their least\n-  \/\/ significant 2 bits cleared and page_size is a power of 2\n-  sub(hdr, hdr, sp);\n-  mv(t0, aligned_mask - (int)os::vm_page_size());\n-  andr(hdr, hdr, t0);\n-  \/\/ for recursive locking, the result is zero => save it in the displaced header\n-  \/\/ location (null in the displaced hdr location indicates recursive locking)\n-  sd(hdr, Address(disp_hdr, 0));\n-  \/\/ otherwise we don't care about the result and handle locking via runtime call\n-  bnez(hdr, slow_case, \/* is_far *\/ true);\n-  \/\/ done\n-  bind(done);\n+\n+  if (LockingMode == LM_LIGHTWEIGHT) {\n+    fast_lock(obj, hdr, t0, t1, slow_case);\n+  } else if (LockingMode == LM_LEGACY) {\n+    Label done;\n+    \/\/ and mark it as unlocked\n+    ori(hdr, hdr, markWord::unlocked_value);\n+    \/\/ save unlocked object header into the displaced header location on the stack\n+    sd(hdr, Address(disp_hdr, 0));\n+    \/\/ test if object header is still the same (i.e. unlocked), and if so, store the\n+    \/\/ displaced header address in the object header - if it is not the same, get the\n+    \/\/ object header instead\n+    la(t1, Address(obj, hdr_offset));\n+    cmpxchgptr(hdr, disp_hdr, t1, t0, done, \/*fallthough*\/nullptr);\n+    \/\/ if the object header was the same, we're done\n+    \/\/ if the object header was not the same, it is now in the hdr register\n+    \/\/ => test if it is a stack pointer into the same stack (recursive locking), i.e.:\n+    \/\/\n+    \/\/ 1) (hdr & aligned_mask) == 0\n+    \/\/ 2) sp <= hdr\n+    \/\/ 3) hdr <= sp + page_size\n+    \/\/\n+    \/\/ these 3 tests can be done by evaluating the following expression:\n+    \/\/\n+    \/\/ (hdr -sp) & (aligned_mask - page_size)\n+    \/\/\n+    \/\/ assuming both the stack pointer and page_size have their least\n+    \/\/ significant 2 bits cleared and page_size is a power of 2\n+    sub(hdr, hdr, sp);\n+    mv(t0, aligned_mask - (int)os::vm_page_size());\n+    andr(hdr, hdr, t0);\n+    \/\/ for recursive locking, the result is zero => save it in the displaced header\n+    \/\/ location (null in the displaced hdr location indicates recursive locking)\n+    sd(hdr, Address(disp_hdr, 0));\n+    \/\/ otherwise we don't care about the result and handle locking via runtime call\n+    bnez(hdr, slow_case, \/* is_far *\/ true);\n+    \/\/ done\n+    bind(done);\n+  }\n+\n@@ -118,5 +124,8 @@\n-  \/\/ load displaced header\n-  ld(hdr, Address(disp_hdr, 0));\n-  \/\/ if the loaded hdr is null we had recursive locking\n-  \/\/ if we had recursive locking, we are done\n-  beqz(hdr, done);\n+  if (LockingMode != LM_LIGHTWEIGHT) {\n+    \/\/ load displaced header\n+    ld(hdr, Address(disp_hdr, 0));\n+    \/\/ if the loaded hdr is null we had recursive locking\n+    \/\/ if we had recursive locking, we are done\n+    beqz(hdr, done);\n+  }\n+\n@@ -126,10 +135,20 @@\n-  \/\/ test if object header is pointing to the displaced header, and if so, restore\n-  \/\/ the displaced header in the object - if the object header is not pointing to\n-  \/\/ the displaced header, get the object header instead\n-  \/\/ if the object header was not pointing to the displaced header,\n-  \/\/ we do unlocking via runtime call\n-  if (hdr_offset) {\n-    la(t0, Address(obj, hdr_offset));\n-    cmpxchgptr(disp_hdr, hdr, t0, t1, done, &slow_case);\n-  } else {\n-    cmpxchgptr(disp_hdr, hdr, obj, t1, done, &slow_case);\n+\n+  if (LockingMode == LM_LIGHTWEIGHT) {\n+    ld(hdr, Address(obj, oopDesc::mark_offset_in_bytes()));\n+    andi(t0, hdr, markWord::monitor_value);\n+    bnez(t0, slow_case, \/* is_far *\/ true);\n+    fast_unlock(obj, hdr, t0, t1, slow_case);\n+  } else if (LockingMode == LM_LEGACY) {\n+    \/\/ test if object header is pointing to the displaced header, and if so, restore\n+    \/\/ the displaced header in the object - if the object header is not pointing to\n+    \/\/ the displaced header, get the object header instead\n+    \/\/ if the object header was not pointing to the displaced header,\n+    \/\/ we do unlocking via runtime call\n+    if (hdr_offset) {\n+      la(t0, Address(obj, hdr_offset));\n+      cmpxchgptr(disp_hdr, hdr, t0, t1, done, &slow_case);\n+    } else {\n+      cmpxchgptr(disp_hdr, hdr, obj, t1, done, &slow_case);\n+    }\n+    \/\/ done\n+    bind(done);\n@@ -137,2 +156,1 @@\n-  \/\/ done\n-  bind(done);\n+\n","filename":"src\/hotspot\/cpu\/riscv\/c1_MacroAssembler_riscv.cpp","additions":70,"deletions":52,"binary":false,"changes":122,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"runtime\/objectMonitor.hpp\"\n@@ -75,0 +76,28 @@\n+int C2HandleAnonOMOwnerStub::max_size() const {\n+  \/\/ Max size of stub has been determined by testing with 0 without using RISC-V compressed\n+  \/\/ instruction-set extension, in which case C2CodeStubList::emit() will throw an assertion\n+  \/\/ and report the actual size that is needed.\n+  return 20 DEBUG_ONLY(+8);\n+}\n+\n+void C2HandleAnonOMOwnerStub::emit(C2_MacroAssembler& masm) {\n+  __ bind(entry());\n+  Register mon = monitor();\n+  Register t = tmp();\n+  assert(t != noreg, \"need tmp register\");\n+\n+  \/\/ Fix owner to be the current thread.\n+  __ sd(xthread, Address(mon, ObjectMonitor::owner_offset_in_bytes()));\n+\n+  \/\/ Pop owner object from lock-stack.\n+  __ lwu(t, Address(xthread, JavaThread::lock_stack_top_offset()));\n+  __ subw(t, t, oopSize);\n+#ifdef ASSERT\n+  __ add(t0, xthread, t);\n+  __ sd(zr, Address(t0, 0));\n+#endif\n+  __ sw(t, Address(xthread, JavaThread::lock_stack_top_offset()));\n+\n+  __ j(continuation());\n+}\n+\n","filename":"src\/hotspot\/cpu\/riscv\/c2_CodeStubs_riscv.cpp","additions":29,"deletions":0,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -784,1 +784,1 @@\n-  if (UseHeavyMonitors) {\n+  if (LockingMode == LM_MONITOR) {\n@@ -812,28 +812,34 @@\n-    \/\/ Load (object->mark() | 1) into swap_reg\n-    ld(t0, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-    ori(swap_reg, t0, 1);\n-\n-    \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n-    sd(swap_reg, Address(lock_reg, mark_offset));\n-\n-    assert(lock_offset == 0,\n-           \"displached header must be first word in BasicObjectLock\");\n-\n-    cmpxchg_obj_header(swap_reg, lock_reg, obj_reg, t0, count, \/*fallthrough*\/nullptr);\n-\n-    \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n-    \/\/  1) (mark & 7) == 0, and\n-    \/\/  2) sp <= mark < mark + os::pagesize()\n-    \/\/\n-    \/\/ These 3 tests can be done by evaluating the following\n-    \/\/ expression: ((mark - sp) & (7 - os::vm_page_size())),\n-    \/\/ assuming both stack pointer and pagesize have their\n-    \/\/ least significant 3 bits clear.\n-    \/\/ NOTE: the oopMark is in swap_reg x10 as the result of cmpxchg\n-    sub(swap_reg, swap_reg, sp);\n-    mv(t0, (int64_t)(7 - (int)os::vm_page_size()));\n-    andr(swap_reg, swap_reg, t0);\n-\n-    \/\/ Save the test result, for recursive case, the result is zero\n-    sd(swap_reg, Address(lock_reg, mark_offset));\n-    beqz(swap_reg, count);\n+    if (LockingMode == LM_LIGHTWEIGHT) {\n+      ld(tmp, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      fast_lock(obj_reg, tmp, t0, t1, slow_case);\n+      j(count);\n+    } else if (LockingMode == LM_LEGACY) {\n+      \/\/ Load (object->mark() | 1) into swap_reg\n+      ld(t0, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      ori(swap_reg, t0, 1);\n+\n+      \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n+      sd(swap_reg, Address(lock_reg, mark_offset));\n+\n+      assert(lock_offset == 0,\n+             \"displached header must be first word in BasicObjectLock\");\n+\n+      cmpxchg_obj_header(swap_reg, lock_reg, obj_reg, t0, count, \/*fallthrough*\/nullptr);\n+\n+      \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n+      \/\/  1) (mark & 7) == 0, and\n+      \/\/  2) sp <= mark < mark + os::pagesize()\n+      \/\/\n+      \/\/ These 3 tests can be done by evaluating the following\n+      \/\/ expression: ((mark - sp) & (7 - os::vm_page_size())),\n+      \/\/ assuming both stack pointer and pagesize have their\n+      \/\/ least significant 3 bits clear.\n+      \/\/ NOTE: the oopMark is in swap_reg x10 as the result of cmpxchg\n+      sub(swap_reg, swap_reg, sp);\n+      mv(t0, (int64_t)(7 - (int)os::vm_page_size()));\n+      andr(swap_reg, swap_reg, t0);\n+\n+      \/\/ Save the test result, for recursive case, the result is zero\n+      sd(swap_reg, Address(lock_reg, mark_offset));\n+      beqz(swap_reg, count);\n+    }\n@@ -844,4 +850,9 @@\n-    call_VM(noreg,\n-            CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter),\n-            lock_reg);\n-\n+    if (LockingMode == LM_LIGHTWEIGHT) {\n+      call_VM(noreg,\n+              CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter_obj),\n+              obj_reg);\n+    } else {\n+      call_VM(noreg,\n+              CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter),\n+              lock_reg);\n+    }\n@@ -873,1 +884,1 @@\n-  if (UseHeavyMonitors) {\n+  if (LockingMode == LM_MONITOR) {\n@@ -884,3 +895,5 @@\n-    \/\/ Convert from BasicObjectLock structure to object and BasicLock\n-    \/\/ structure Store the BasicLock address into x10\n-    la(swap_reg, Address(lock_reg, BasicObjectLock::lock_offset_in_bytes()));\n+    if (LockingMode != LM_LIGHTWEIGHT) {\n+      \/\/ Convert from BasicObjectLock structure to object and BasicLock\n+      \/\/ structure Store the BasicLock address into x10\n+      la(swap_reg, Address(lock_reg, BasicObjectLock::lock_offset_in_bytes()));\n+    }\n@@ -894,9 +907,35 @@\n-    \/\/ Load the old header from BasicLock structure\n-    ld(header_reg, Address(swap_reg,\n-                           BasicLock::displaced_header_offset_in_bytes()));\n-\n-    \/\/ Test for recursion\n-    beqz(header_reg, count);\n-\n-    \/\/ Atomic swap back the old header\n-    cmpxchg_obj_header(swap_reg, header_reg, obj_reg, t0, count, \/*fallthrough*\/nullptr);\n+    if (LockingMode == LM_LIGHTWEIGHT) {\n+      Label slow_case;\n+\n+      \/\/ Check for non-symmetric locking. This is allowed by the spec and the interpreter\n+      \/\/ must handle it.\n+      Register tmp1 = t0;\n+      Register tmp2 = header_reg;\n+      \/\/ First check for lock-stack underflow.\n+      lwu(tmp1, Address(xthread, JavaThread::lock_stack_top_offset()));\n+      mv(tmp2, (unsigned)LockStack::start_offset());\n+      ble(tmp1, tmp2, slow_case);\n+      \/\/ Then check if the top of the lock-stack matches the unlocked object.\n+      subw(tmp1, tmp1, oopSize);\n+      add(tmp1, xthread, tmp1);\n+      ld(tmp1, Address(tmp1, 0));\n+      bne(tmp1, obj_reg, slow_case);\n+\n+      ld(header_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      andi(t0, header_reg, markWord::monitor_value);\n+      bnez(t0, slow_case);\n+      fast_unlock(obj_reg, header_reg, swap_reg, t0, slow_case);\n+      j(count);\n+\n+      bind(slow_case);\n+    } else if (LockingMode == LM_LEGACY) {\n+      \/\/ Load the old header from BasicLock structure\n+      ld(header_reg, Address(swap_reg,\n+                             BasicLock::displaced_header_offset_in_bytes()));\n+\n+      \/\/ Test for recursion\n+      beqz(header_reg, count);\n+\n+      \/\/ Atomic swap back the old header\n+      cmpxchg_obj_header(swap_reg, header_reg, obj_reg, t0, count, \/*fallthrough*\/nullptr);\n+    }\n","filename":"src\/hotspot\/cpu\/riscv\/interp_masm_riscv.cpp","additions":85,"deletions":46,"binary":false,"changes":131,"status":"modified"},{"patch":"@@ -62,0 +62,1 @@\n+#define STOP(str) stop(str);\n@@ -2419,1 +2420,1 @@\n-    bgtu(in_nmethod ? sp : fp, t0, slow_path, true \/* is_far *\/);\n+    bgtu(in_nmethod ? sp : fp, t0, slow_path, \/* is_far *\/ true);\n@@ -4490,0 +4491,97 @@\n+\n+\/\/ Implements fast-locking.\n+\/\/ Branches to slow upon failure to lock the object.\n+\/\/ Falls through upon success.\n+\/\/\n+\/\/  - obj: the object to be locked\n+\/\/  - hdr: the header, already loaded from obj, will be destroyed\n+\/\/  - tmp1, tmp2: temporary registers, will be destroyed\n+void MacroAssembler::fast_lock(Register obj, Register hdr, Register tmp1, Register tmp2, Label& slow) {\n+  assert(LockingMode == LM_LIGHTWEIGHT, \"only used with new lightweight locking\");\n+  assert_different_registers(obj, hdr, tmp1, tmp2);\n+\n+  \/\/ Check if we would have space on lock-stack for the object.\n+  lwu(tmp1, Address(xthread, JavaThread::lock_stack_top_offset()));\n+  mv(tmp2, (unsigned)LockStack::end_offset());\n+  bge(tmp1, tmp2, slow, \/* is_far *\/ true);\n+\n+  \/\/ Load (object->mark() | 1) into hdr\n+  ori(hdr, hdr, markWord::unlocked_value);\n+  \/\/ Clear lock-bits, into tmp2\n+  xori(tmp2, hdr, markWord::unlocked_value);\n+\n+  \/\/ Try to swing header from unlocked to locked\n+  Label success;\n+  cmpxchgptr(hdr, tmp2, obj, tmp1, success, &slow);\n+  bind(success);\n+\n+  \/\/ After successful lock, push object on lock-stack\n+  lwu(tmp1, Address(xthread, JavaThread::lock_stack_top_offset()));\n+  add(tmp2, xthread, tmp1);\n+  sd(obj, Address(tmp2, 0));\n+  addw(tmp1, tmp1, oopSize);\n+  sw(tmp1, Address(xthread, JavaThread::lock_stack_top_offset()));\n+}\n+\n+\/\/ Implements fast-unlocking.\n+\/\/ Branches to slow upon failure.\n+\/\/ Falls through upon success.\n+\/\/\n+\/\/ - obj: the object to be unlocked\n+\/\/ - hdr: the (pre-loaded) header of the object\n+\/\/ - tmp1, tmp2: temporary registers\n+void MacroAssembler::fast_unlock(Register obj, Register hdr, Register tmp1, Register tmp2, Label& slow) {\n+  assert(LockingMode == LM_LIGHTWEIGHT, \"only used with new lightweight locking\");\n+  assert_different_registers(obj, hdr, tmp1, tmp2);\n+\n+#ifdef ASSERT\n+  {\n+    \/\/ The following checks rely on the fact that LockStack is only ever modified by\n+    \/\/ its owning thread, even if the lock got inflated concurrently; removal of LockStack\n+    \/\/ entries after inflation will happen delayed in that case.\n+\n+    \/\/ Check for lock-stack underflow.\n+    Label stack_ok;\n+    lwu(tmp1, Address(xthread, JavaThread::lock_stack_top_offset()));\n+    mv(tmp2, (unsigned)LockStack::start_offset());\n+    bgt(tmp1, tmp2, stack_ok);\n+    STOP(\"Lock-stack underflow\");\n+    bind(stack_ok);\n+  }\n+  {\n+    \/\/ Check if the top of the lock-stack matches the unlocked object.\n+    Label tos_ok;\n+    subw(tmp1, tmp1, oopSize);\n+    add(tmp1, xthread, tmp1);\n+    ld(tmp1, Address(tmp1, 0));\n+    beq(tmp1, obj, tos_ok);\n+    STOP(\"Top of lock-stack does not match the unlocked object\");\n+    bind(tos_ok);\n+  }\n+  {\n+    \/\/ Check that hdr is fast-locked.\n+   Label hdr_ok;\n+    andi(tmp1, hdr, markWord::lock_mask_in_place);\n+    beqz(tmp1, hdr_ok);\n+    STOP(\"Header is not fast-locked\");\n+    bind(hdr_ok);\n+  }\n+#endif\n+\n+  \/\/ Load the new header (unlocked) into tmp1\n+  ori(tmp1, hdr, markWord::unlocked_value);\n+\n+  \/\/ Try to swing header from locked to unlocked\n+  Label success;\n+  cmpxchgptr(hdr, tmp1, obj, tmp2, success, &slow);\n+  bind(success);\n+\n+  \/\/ After successful unlock, pop object from lock-stack\n+  lwu(tmp1, Address(xthread, JavaThread::lock_stack_top_offset()));\n+  subw(tmp1, tmp1, oopSize);\n+#ifdef ASSERT\n+  add(tmp2, xthread, tmp1);\n+  sd(zr, Address(tmp2, 0));\n+#endif\n+  sw(tmp1, Address(xthread, JavaThread::lock_stack_top_offset()));\n+}\n","filename":"src\/hotspot\/cpu\/riscv\/macroAssembler_riscv.cpp","additions":99,"deletions":1,"binary":false,"changes":100,"status":"modified"},{"patch":"@@ -1421,0 +1421,4 @@\n+\n+public:\n+  void fast_lock(Register obj, Register hdr, Register tmp1, Register tmp2, Label& slow);\n+  void fast_unlock(Register obj, Register hdr, Register tmp1, Register tmp2, Label& slow);\n","filename":"src\/hotspot\/cpu\/riscv\/macroAssembler_riscv.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2484,1 +2484,1 @@\n-  \/\/ using the cr register as the bool result: 0 for success; others failed.\n+  \/\/ Use cr register to indicate the fast_lock result: zero for success; non-zero for failure.\n@@ -2494,1 +2494,1 @@\n-    Label no_count;\n+    Label count, no_count;\n@@ -2512,1 +2512,4 @@\n-    if (!UseHeavyMonitors) {\n+    if (LockingMode == LM_MONITOR) {\n+      __ mv(flag, 1); \/\/ Set non-zero flag to indicate 'failure' -> take slow-path\n+      __ j(cont);\n+    } else if (LockingMode == LM_LEGACY) {\n@@ -2543,0 +2546,1 @@\n+      __ j(cont);\n@@ -2544,0 +2548,8 @@\n+      assert(LockingMode == LM_LIGHTWEIGHT, \"\");\n+      Label slow;\n+      __ fast_lock(oop, disp_hdr, tmp, t0, slow);\n+\n+      \/\/ Indicate success on completion.\n+      __ mv(flag, zr);\n+      __ j(count);\n+      __ bind(slow);\n@@ -2545,0 +2557,1 @@\n+      __ j(no_count);\n@@ -2547,2 +2560,0 @@\n-    __ j(cont);\n-\n@@ -2559,6 +2570,8 @@\n-    \/\/ Store a non-null value into the box to avoid looking like a re-entrant\n-    \/\/ lock. The fast-path monitor unlock code checks for\n-    \/\/ markWord::monitor_value so use markWord::unused_mark which has the\n-    \/\/ relevant bit set, and also matches ObjectSynchronizer::slow_enter.\n-    __ mv(tmp, (address)markWord::unused_mark().value());\n-    __ sd(tmp, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n+    if (LockingMode != LM_LIGHTWEIGHT) {\n+      \/\/ Store a non-null value into the box to avoid looking like a re-entrant\n+      \/\/ lock. The fast-path monitor unlock code checks for\n+      \/\/ markWord::monitor_value so use markWord::unused_mark which has the\n+      \/\/ relevant bit set, and also matches ObjectSynchronizer::slow_enter.\n+      __ mv(tmp, (address)markWord::unused_mark().value());\n+      __ sd(tmp, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n+    }\n@@ -2575,1 +2588,2 @@\n-\n+    \/\/ zero flag indicates success\n+    \/\/ non-zero flag indicates failure\n@@ -2578,0 +2592,1 @@\n+    __ bind(count);\n@@ -2583,1 +2598,1 @@\n-  \/\/ using cr flag to indicate the fast_unlock result: 0 for success; others failed.\n+  \/\/ Use cr register to indicate the fast_unlock result: zero for success; non-zero for failure.\n@@ -2593,1 +2608,1 @@\n-    Label no_count;\n+    Label count, no_count;\n@@ -2597,1 +2612,1 @@\n-    if (!UseHeavyMonitors) {\n+    if (LockingMode == LM_LEGACY) {\n@@ -2611,1 +2626,4 @@\n-    if (!UseHeavyMonitors) {\n+    if (LockingMode == LM_MONITOR) {\n+      __ mv(flag, 1); \/\/ Set non-zero flag to indicate 'failure' -> take slow path\n+      __ j(cont);\n+    } else if (LockingMode == LM_LEGACY) {\n@@ -2619,0 +2637,1 @@\n+      __ j(cont);\n@@ -2620,0 +2639,8 @@\n+      assert(LockingMode == LM_LIGHTWEIGHT, \"\");\n+      Label slow;\n+      __ fast_unlock(oop, tmp, box, disp_hdr, slow);\n+\n+      \/\/ Indicate success on completion.\n+      __ mv(flag, zr);\n+      __ j(count);\n+      __ bind(slow);\n@@ -2621,0 +2648,1 @@\n+      __ j(no_count);\n@@ -2622,1 +2650,0 @@\n-    __ j(cont);\n@@ -2630,0 +2657,12 @@\n+\n+    if (LockingMode == LM_LIGHTWEIGHT) {\n+      \/\/ If the owner is anonymous, we need to fix it -- in an outline stub.\n+      Register tmp2 = disp_hdr;\n+      __ ld(tmp2, Address(tmp, ObjectMonitor::owner_offset_in_bytes()));\n+      __ andi(t0, tmp2, (int64_t)ObjectMonitor::ANONYMOUS_OWNER);\n+      C2HandleAnonOMOwnerStub* stub = new (Compile::current()->comp_arena()) C2HandleAnonOMOwnerStub(tmp, tmp2);\n+      Compile::current()->output()->add_stub(stub);\n+      __ bnez(t0, stub->entry(), \/* is_far *\/ true);\n+      __ bind(stub->continuation());\n+    }\n+\n@@ -2652,1 +2691,2 @@\n-\n+    \/\/ zero flag indicates success\n+    \/\/ non-zero flag indicates failure\n@@ -2655,0 +2695,1 @@\n+    __ bind(count);\n","filename":"src\/hotspot\/cpu\/riscv\/riscv.ad","additions":59,"deletions":18,"binary":false,"changes":77,"status":"modified"},{"patch":"@@ -1674,1 +1674,3 @@\n-    if (!UseHeavyMonitors) {\n+    if (LockingMode == LM_MONITOR) {\n+      __ j(slow_path_lock);\n+    } else if (LockingMode == LM_LEGACY) {\n@@ -1701,1 +1703,3 @@\n-      __ j(slow_path_lock);\n+      assert(LockingMode == LM_LIGHTWEIGHT, \"\");\n+      __ ld(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      __ fast_lock(obj_reg, swap_reg, tmp, t0, slow_path_lock);\n@@ -1796,1 +1800,1 @@\n-    if (!UseHeavyMonitors) {\n+    if (LockingMode == LM_LEGACY) {\n@@ -1811,1 +1815,3 @@\n-    if (!UseHeavyMonitors) {\n+    if (LockingMode == LM_MONITOR) {\n+      __ j(slow_path_unlock);\n+    } else if (LockingMode == LM_LEGACY) {\n@@ -1823,1 +1829,6 @@\n-      __ j(slow_path_unlock);\n+      assert(LockingMode == LM_LIGHTWEIGHT, \"\");\n+      __ ld(old_hdr, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      __ andi(t0, old_hdr, markWord::monitor_value);\n+      __ bnez(t0, slow_path_unlock);\n+      __ fast_unlock(obj_reg, old_hdr, swap_reg, t0, slow_path_unlock);\n+      __ decrement(Address(xthread, JavaThread::held_monitor_count_offset()));\n","filename":"src\/hotspot\/cpu\/riscv\/sharedRuntime_riscv.cpp","additions":16,"deletions":5,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -457,1 +457,1 @@\n-    if (UseHeavyMonitors) {\n+    if (LockingMode == LM_MONITOR) {\n@@ -3503,1 +3503,1 @@\n-  if (UseHeavyMonitors) {\n+  if (LockingMode == LM_MONITOR) {\n@@ -3511,0 +3511,1 @@\n+    Register tmp = LockingMode == LM_LIGHTWEIGHT ? op->scratch_opr()->as_register() : noreg;\n@@ -3512,1 +3513,1 @@\n-    int null_check_offset = __ lock_object(hdr, obj, lock, *op->stub()->entry());\n+    int null_check_offset = __ lock_object(hdr, obj, lock, tmp, *op->stub()->entry());\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRAssembler_x86.cpp","additions":4,"deletions":3,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -322,1 +322,2 @@\n-  monitor_enter(obj.result(), lock, syncTempOpr(), LIR_OprFact::illegalOpr,\n+  LIR_Opr tmp = LockingMode == LM_LIGHTWEIGHT ? new_register(T_ADDRESS) : LIR_OprFact::illegalOpr;\n+  monitor_enter(obj.result(), lock, syncTempOpr(), tmp,\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRGenerator_x86.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -41,1 +41,1 @@\n-int C1_MacroAssembler::lock_object(Register hdr, Register obj, Register disp_hdr, Label& slow_case) {\n+int C1_MacroAssembler::lock_object(Register hdr, Register obj, Register disp_hdr, Register tmp, Label& slow_case) {\n@@ -45,2 +45,1 @@\n-  assert(hdr != obj && hdr != disp_hdr && obj != disp_hdr, \"registers must be different\");\n-  Label done;\n+  assert_different_registers(hdr, obj, disp_hdr, tmp);\n@@ -65,33 +64,45 @@\n-  \/\/ and mark it as unlocked\n-  orptr(hdr, markWord::unlocked_value);\n-  \/\/ save unlocked object header into the displaced header location on the stack\n-  movptr(Address(disp_hdr, 0), hdr);\n-  \/\/ test if object header is still the same (i.e. unlocked), and if so, store the\n-  \/\/ displaced header address in the object header - if it is not the same, get the\n-  \/\/ object header instead\n-  MacroAssembler::lock(); \/\/ must be immediately before cmpxchg!\n-  cmpxchgptr(disp_hdr, Address(obj, hdr_offset));\n-  \/\/ if the object header was the same, we're done\n-  jcc(Assembler::equal, done);\n-  \/\/ if the object header was not the same, it is now in the hdr register\n-  \/\/ => test if it is a stack pointer into the same stack (recursive locking), i.e.:\n-  \/\/\n-  \/\/ 1) (hdr & aligned_mask) == 0\n-  \/\/ 2) rsp <= hdr\n-  \/\/ 3) hdr <= rsp + page_size\n-  \/\/\n-  \/\/ these 3 tests can be done by evaluating the following expression:\n-  \/\/\n-  \/\/ (hdr - rsp) & (aligned_mask - page_size)\n-  \/\/\n-  \/\/ assuming both the stack pointer and page_size have their least\n-  \/\/ significant 2 bits cleared and page_size is a power of 2\n-  subptr(hdr, rsp);\n-  andptr(hdr, aligned_mask - (int)os::vm_page_size());\n-  \/\/ for recursive locking, the result is zero => save it in the displaced header\n-  \/\/ location (null in the displaced hdr location indicates recursive locking)\n-  movptr(Address(disp_hdr, 0), hdr);\n-  \/\/ otherwise we don't care about the result and handle locking via runtime call\n-  jcc(Assembler::notZero, slow_case);\n-  \/\/ done\n-  bind(done);\n+\n+  if (LockingMode == LM_LIGHTWEIGHT) {\n+#ifdef _LP64\n+    const Register thread = r15_thread;\n+#else\n+    const Register thread = disp_hdr;\n+    get_thread(thread);\n+#endif\n+    fast_lock_impl(obj, hdr, thread, tmp, slow_case);\n+  } else  if (LockingMode == LM_LEGACY) {\n+    Label done;\n+    \/\/ and mark it as unlocked\n+    orptr(hdr, markWord::unlocked_value);\n+    \/\/ save unlocked object header into the displaced header location on the stack\n+    movptr(Address(disp_hdr, 0), hdr);\n+    \/\/ test if object header is still the same (i.e. unlocked), and if so, store the\n+    \/\/ displaced header address in the object header - if it is not the same, get the\n+    \/\/ object header instead\n+    MacroAssembler::lock(); \/\/ must be immediately before cmpxchg!\n+    cmpxchgptr(disp_hdr, Address(obj, hdr_offset));\n+    \/\/ if the object header was the same, we're done\n+    jcc(Assembler::equal, done);\n+    \/\/ if the object header was not the same, it is now in the hdr register\n+    \/\/ => test if it is a stack pointer into the same stack (recursive locking), i.e.:\n+    \/\/\n+    \/\/ 1) (hdr & aligned_mask) == 0\n+    \/\/ 2) rsp <= hdr\n+    \/\/ 3) hdr <= rsp + page_size\n+    \/\/\n+    \/\/ these 3 tests can be done by evaluating the following expression:\n+    \/\/\n+    \/\/ (hdr - rsp) & (aligned_mask - page_size)\n+    \/\/\n+    \/\/ assuming both the stack pointer and page_size have their least\n+    \/\/ significant 2 bits cleared and page_size is a power of 2\n+    subptr(hdr, rsp);\n+    andptr(hdr, aligned_mask - (int)os::vm_page_size());\n+    \/\/ for recursive locking, the result is zero => save it in the displaced header\n+    \/\/ location (null in the displaced hdr location indicates recursive locking)\n+    movptr(Address(disp_hdr, 0), hdr);\n+    \/\/ otherwise we don't care about the result and handle locking via runtime call\n+    jcc(Assembler::notZero, slow_case);\n+    \/\/ done\n+    bind(done);\n+  }\n@@ -111,6 +122,9 @@\n-  \/\/ load displaced header\n-  movptr(hdr, Address(disp_hdr, 0));\n-  \/\/ if the loaded hdr is null we had recursive locking\n-  testptr(hdr, hdr);\n-  \/\/ if we had recursive locking, we are done\n-  jcc(Assembler::zero, done);\n+  if (LockingMode != LM_LIGHTWEIGHT) {\n+    \/\/ load displaced header\n+    movptr(hdr, Address(disp_hdr, 0));\n+    \/\/ if the loaded hdr is null we had recursive locking\n+    testptr(hdr, hdr);\n+    \/\/ if we had recursive locking, we are done\n+    jcc(Assembler::zero, done);\n+  }\n+\n@@ -119,1 +133,0 @@\n-\n@@ -121,10 +134,0 @@\n-  \/\/ test if object header is pointing to the displaced header, and if so, restore\n-  \/\/ the displaced header in the object - if the object header is not pointing to\n-  \/\/ the displaced header, get the object header instead\n-  MacroAssembler::lock(); \/\/ must be immediately before cmpxchg!\n-  cmpxchgptr(hdr, Address(obj, hdr_offset));\n-  \/\/ if the object header was not pointing to the displaced header,\n-  \/\/ we do unlocking via runtime call\n-  jcc(Assembler::notEqual, slow_case);\n-  \/\/ done\n-  bind(done);\n@@ -132,0 +135,16 @@\n+  if (LockingMode == LM_LIGHTWEIGHT) {\n+    movptr(disp_hdr, Address(obj, hdr_offset));\n+    andptr(disp_hdr, ~(int32_t)markWord::lock_mask_in_place);\n+    fast_unlock_impl(obj, disp_hdr, hdr, slow_case);\n+  } else if (LockingMode == LM_LEGACY) {\n+    \/\/ test if object header is pointing to the displaced header, and if so, restore\n+    \/\/ the displaced header in the object - if the object header is not pointing to\n+    \/\/ the displaced header, get the object header instead\n+    MacroAssembler::lock(); \/\/ must be immediately before cmpxchg!\n+    cmpxchgptr(hdr, Address(obj, hdr_offset));\n+    \/\/ if the object header was not pointing to the displaced header,\n+    \/\/ we do unlocking via runtime call\n+    jcc(Assembler::notEqual, slow_case);\n+    \/\/ done\n+  }\n+  bind(done);\n","filename":"src\/hotspot\/cpu\/x86\/c1_MacroAssembler_x86.cpp","additions":72,"deletions":53,"binary":false,"changes":125,"status":"modified"},{"patch":"@@ -53,1 +53,1 @@\n-  int lock_object  (Register swap, Register obj, Register disp_hdr, Label& slow_case);\n+  int lock_object  (Register swap, Register obj, Register disp_hdr, Register tmp, Label& slow_case);\n","filename":"src\/hotspot\/cpu\/x86\/c1_MacroAssembler_x86.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"runtime\/objectMonitor.hpp\"\n@@ -75,0 +76,22 @@\n+#ifdef _LP64\n+int C2HandleAnonOMOwnerStub::max_size() const {\n+  \/\/ Max size of stub has been determined by testing with 0, in which case\n+  \/\/ C2CodeStubList::emit() will throw an assertion and report the actual size that\n+  \/\/ is needed.\n+  return DEBUG_ONLY(36) NOT_DEBUG(21);\n+}\n+\n+void C2HandleAnonOMOwnerStub::emit(C2_MacroAssembler& masm) {\n+  __ bind(entry());\n+  Register mon = monitor();\n+  Register t = tmp();\n+  __ movptr(Address(mon, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)), r15_thread);\n+  __ subl(Address(r15_thread, JavaThread::lock_stack_top_offset()), oopSize);\n+#ifdef ASSERT\n+  __ movl(t, Address(r15_thread, JavaThread::lock_stack_top_offset()));\n+  __ movptr(Address(r15_thread, t), 0);\n+#endif\n+  __ jmp(continuation());\n+}\n+#endif\n+\n","filename":"src\/hotspot\/cpu\/x86\/c2_CodeStubs_x86.cpp","additions":23,"deletions":0,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -551,1 +551,1 @@\n-                                 Register scrReg, Register cx1Reg, Register cx2Reg,\n+                                 Register scrReg, Register cx1Reg, Register cx2Reg, Register thread,\n@@ -593,1 +593,1 @@\n-    assert(!UseHeavyMonitors, \"+UseHeavyMonitors and +UseRTMForStackLocks are mutually exclusive\");\n+    assert(LockingMode != LM_MONITOR, \"LockingMode == 0 (LM_MONITOR) and +UseRTMForStackLocks are mutually exclusive\");\n@@ -602,1 +602,1 @@\n-  jccb(Assembler::notZero, IsInflated);\n+  jcc(Assembler::notZero, IsInflated);\n@@ -604,1 +604,4 @@\n-  if (!UseHeavyMonitors) {\n+  if (LockingMode == LM_MONITOR) {\n+    \/\/ Clear ZF so that we take the slow path at the DONE label. objReg is known to be not 0.\n+    testptr(objReg, objReg);\n+  } else if (LockingMode == LM_LEGACY) {\n@@ -620,2 +623,3 @@\n-    \/\/ Clear ZF so that we take the slow path at the DONE label. objReg is known to be not 0.\n-    testptr(objReg, objReg);\n+    assert(LockingMode == LM_LIGHTWEIGHT, \"\");\n+    fast_lock_impl(objReg, tmpReg, thread, scrReg, NO_COUNT);\n+    jmp(COUNT);\n@@ -650,2 +654,1 @@\n-  \/\/ Optimistic form: consider XORL tmpReg,tmpReg\n-  movptr(tmpReg, NULL_WORD);\n+  xorptr(tmpReg, tmpReg);\n@@ -653,7 +656,1 @@\n-  \/\/ Appears unlocked - try to swing _owner from null to non-null.\n-  \/\/ Ideally, I'd manifest \"Self\" with get_thread and then attempt\n-  \/\/ to CAS the register containing Self into m->Owner.\n-  \/\/ But we don't have enough registers, so instead we can either try to CAS\n-  \/\/ rsp or the address of the box (in scr) into &m->owner.  If the CAS succeeds\n-  \/\/ we later store \"Self\" into m->Owner.  Transiently storing a stack address\n-  \/\/ (rsp or the address of the box) into  m->owner is harmless.\n+  \/\/ Appears unlocked - try to swing _owner from null to current thread.\n@@ -662,1 +659,1 @@\n-  cmpxchgptr(scrReg, Address(boxReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)));\n+  cmpxchgptr(thread, Address(boxReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)));\n@@ -684,1 +681,1 @@\n-  cmpxchgptr(r15_thread, Address(scrReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)));\n+  cmpxchgptr(thread, Address(scrReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)));\n@@ -691,1 +688,1 @@\n-  cmpptr(r15_thread, rax);                \/\/ Check if we are already the owner (recursive lock)\n+  cmpptr(thread, rax);                \/\/ Check if we are already the owner (recursive lock)\n@@ -707,6 +704,1 @@\n-#ifndef _LP64\n-  get_thread(tmpReg);\n-  incrementl(Address(tmpReg, JavaThread::held_monitor_count_offset()));\n-#else \/\/ _LP64\n-  incrementq(Address(r15_thread, JavaThread::held_monitor_count_offset()));\n-#endif\n+  increment(Address(thread, JavaThread::held_monitor_count_offset()));\n@@ -764,1 +756,1 @@\n-    assert(!UseHeavyMonitors, \"+UseHeavyMonitors and +UseRTMForStackLocks are mutually exclusive\");\n+    assert(LockingMode != LM_MONITOR, \"LockingMode == 0 (LM_MONITOR) and +UseRTMForStackLocks are mutually exclusive\");\n@@ -776,1 +768,1 @@\n-  if (!UseHeavyMonitors) {\n+  if (LockingMode == LM_LEGACY) {\n@@ -781,1 +773,1 @@\n-  if (!UseHeavyMonitors) {\n+  if (LockingMode != LM_MONITOR) {\n@@ -783,1 +775,1 @@\n-    jccb   (Assembler::zero, Stacked);\n+    jcc(Assembler::zero, Stacked);\n@@ -787,0 +779,18 @@\n+  if (LockingMode == LM_LIGHTWEIGHT) {\n+    \/\/ If the owner is ANONYMOUS, we need to fix it -  in an outline stub.\n+    testb(Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)), (int32_t) ObjectMonitor::ANONYMOUS_OWNER);\n+#ifdef _LP64\n+    if (!Compile::current()->output()->in_scratch_emit_size()) {\n+      C2HandleAnonOMOwnerStub* stub = new (Compile::current()->comp_arena()) C2HandleAnonOMOwnerStub(tmpReg, boxReg);\n+      Compile::current()->output()->add_stub(stub);\n+      jcc(Assembler::notEqual, stub->entry());\n+      bind(stub->continuation());\n+    } else\n+#endif\n+    {\n+      \/\/ We can't easily implement this optimization on 32 bit because we don't have a thread register.\n+      \/\/ Call the slow-path instead.\n+      jcc(Assembler::notEqual, NO_COUNT);\n+    }\n+  }\n+\n@@ -795,1 +805,1 @@\n-    jmpb(DONE_LABEL);\n+    jmp(DONE_LABEL);\n@@ -907,1 +917,1 @@\n-  if (!UseHeavyMonitors) {\n+  if (LockingMode != LM_MONITOR) {\n@@ -909,3 +919,9 @@\n-    movptr(tmpReg, Address (boxReg, 0));      \/\/ re-fetch\n-    lock();\n-    cmpxchgptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes())); \/\/ Uses RAX which is box\n+    if (LockingMode == LM_LIGHTWEIGHT) {\n+      mov(boxReg, tmpReg);\n+      fast_unlock_impl(objReg, boxReg, tmpReg, NO_COUNT);\n+      jmp(COUNT);\n+    } else if (LockingMode == LM_LEGACY) {\n+      movptr(tmpReg, Address (boxReg, 0));      \/\/ re-fetch\n+      lock();\n+      cmpxchgptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes())); \/\/ Uses RAX which is box\n+    }\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":49,"deletions":33,"binary":false,"changes":82,"status":"modified"},{"patch":"@@ -39,1 +39,1 @@\n-                 Register scr, Register cx1, Register cx2,\n+                 Register scr, Register cx1, Register cx2, Register thread,\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1199,1 +1199,1 @@\n-  if (UseHeavyMonitors) {\n+  if (LockingMode == LM_MONITOR) {\n@@ -1226,53 +1226,65 @@\n-    \/\/ Load immediate 1 into swap_reg %rax\n-    movl(swap_reg, 1);\n-\n-    \/\/ Load (object->mark() | 1) into swap_reg %rax\n-    orptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-\n-    \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n-    movptr(Address(lock_reg, mark_offset), swap_reg);\n-\n-    assert(lock_offset == 0,\n-           \"displaced header must be first word in BasicObjectLock\");\n-\n-    lock();\n-    cmpxchgptr(lock_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-    jcc(Assembler::zero, count_locking);\n-\n-    const int zero_bits = LP64_ONLY(7) NOT_LP64(3);\n-\n-    \/\/ Fast check for recursive lock.\n-    \/\/\n-    \/\/ Can apply the optimization only if this is a stack lock\n-    \/\/ allocated in this thread. For efficiency, we can focus on\n-    \/\/ recently allocated stack locks (instead of reading the stack\n-    \/\/ base and checking whether 'mark' points inside the current\n-    \/\/ thread stack):\n-    \/\/  1) (mark & zero_bits) == 0, and\n-    \/\/  2) rsp <= mark < mark + os::pagesize()\n-    \/\/\n-    \/\/ Warning: rsp + os::pagesize can overflow the stack base. We must\n-    \/\/ neither apply the optimization for an inflated lock allocated\n-    \/\/ just above the thread stack (this is why condition 1 matters)\n-    \/\/ nor apply the optimization if the stack lock is inside the stack\n-    \/\/ of another thread. The latter is avoided even in case of overflow\n-    \/\/ because we have guard pages at the end of all stacks. Hence, if\n-    \/\/ we go over the stack base and hit the stack of another thread,\n-    \/\/ this should not be in a writeable area that could contain a\n-    \/\/ stack lock allocated by that thread. As a consequence, a stack\n-    \/\/ lock less than page size away from rsp is guaranteed to be\n-    \/\/ owned by the current thread.\n-    \/\/\n-    \/\/ These 3 tests can be done by evaluating the following\n-    \/\/ expression: ((mark - rsp) & (zero_bits - os::vm_page_size())),\n-    \/\/ assuming both stack pointer and pagesize have their\n-    \/\/ least significant bits clear.\n-    \/\/ NOTE: the mark is in swap_reg %rax as the result of cmpxchg\n-    subptr(swap_reg, rsp);\n-    andptr(swap_reg, zero_bits - (int)os::vm_page_size());\n-\n-    \/\/ Save the test result, for recursive case, the result is zero\n-    movptr(Address(lock_reg, mark_offset), swap_reg);\n-    jcc(Assembler::notZero, slow_case);\n-\n-    bind(count_locking);\n+    if (LockingMode == LM_LIGHTWEIGHT) {\n+#ifdef _LP64\n+      const Register thread = r15_thread;\n+#else\n+      const Register thread = lock_reg;\n+      get_thread(thread);\n+#endif\n+      \/\/ Load object header, prepare for CAS from unlocked to locked.\n+      movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      fast_lock_impl(obj_reg, swap_reg, thread, tmp_reg, slow_case);\n+    } else if (LockingMode == LM_LEGACY) {\n+      \/\/ Load immediate 1 into swap_reg %rax\n+      movl(swap_reg, 1);\n+\n+      \/\/ Load (object->mark() | 1) into swap_reg %rax\n+      orptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+\n+      \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n+      movptr(Address(lock_reg, mark_offset), swap_reg);\n+\n+      assert(lock_offset == 0,\n+             \"displaced header must be first word in BasicObjectLock\");\n+\n+      lock();\n+      cmpxchgptr(lock_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      jcc(Assembler::zero, count_locking);\n+\n+      const int zero_bits = LP64_ONLY(7) NOT_LP64(3);\n+\n+      \/\/ Fast check for recursive lock.\n+      \/\/\n+      \/\/ Can apply the optimization only if this is a stack lock\n+      \/\/ allocated in this thread. For efficiency, we can focus on\n+      \/\/ recently allocated stack locks (instead of reading the stack\n+      \/\/ base and checking whether 'mark' points inside the current\n+      \/\/ thread stack):\n+      \/\/  1) (mark & zero_bits) == 0, and\n+      \/\/  2) rsp <= mark < mark + os::pagesize()\n+      \/\/\n+      \/\/ Warning: rsp + os::pagesize can overflow the stack base. We must\n+      \/\/ neither apply the optimization for an inflated lock allocated\n+      \/\/ just above the thread stack (this is why condition 1 matters)\n+      \/\/ nor apply the optimization if the stack lock is inside the stack\n+      \/\/ of another thread. The latter is avoided even in case of overflow\n+      \/\/ because we have guard pages at the end of all stacks. Hence, if\n+      \/\/ we go over the stack base and hit the stack of another thread,\n+      \/\/ this should not be in a writeable area that could contain a\n+      \/\/ stack lock allocated by that thread. As a consequence, a stack\n+      \/\/ lock less than page size away from rsp is guaranteed to be\n+      \/\/ owned by the current thread.\n+      \/\/\n+      \/\/ These 3 tests can be done by evaluating the following\n+      \/\/ expression: ((mark - rsp) & (zero_bits - os::vm_page_size())),\n+      \/\/ assuming both stack pointer and pagesize have their\n+      \/\/ least significant bits clear.\n+      \/\/ NOTE: the mark is in swap_reg %rax as the result of cmpxchg\n+      subptr(swap_reg, rsp);\n+      andptr(swap_reg, zero_bits - (int)os::vm_page_size());\n+\n+      \/\/ Save the test result, for recursive case, the result is zero\n+      movptr(Address(lock_reg, mark_offset), swap_reg);\n+      jcc(Assembler::notZero, slow_case);\n+\n+      bind(count_locking);\n+    }\n@@ -1285,4 +1297,9 @@\n-    call_VM(noreg,\n-            CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter),\n-            lock_reg);\n-\n+    if (LockingMode == LM_LIGHTWEIGHT) {\n+      call_VM(noreg,\n+              CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter_obj),\n+              obj_reg);\n+    } else {\n+      call_VM(noreg,\n+              CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter),\n+              lock_reg);\n+    }\n@@ -1310,1 +1327,1 @@\n-  if (UseHeavyMonitors) {\n+  if (LockingMode == LM_MONITOR) {\n@@ -1321,3 +1338,5 @@\n-    \/\/ Convert from BasicObjectLock structure to object and BasicLock\n-    \/\/ structure Store the BasicLock address into %rax\n-    lea(swap_reg, Address(lock_reg, BasicObjectLock::lock_offset_in_bytes()));\n+    if (LockingMode != LM_LIGHTWEIGHT) {\n+      \/\/ Convert from BasicObjectLock structure to object and BasicLock\n+      \/\/ structure Store the BasicLock address into %rax\n+      lea(swap_reg, Address(lock_reg, BasicObjectLock::lock_offset_in_bytes()));\n+    }\n@@ -1331,16 +1350,33 @@\n-    \/\/ Load the old header from BasicLock structure\n-    movptr(header_reg, Address(swap_reg,\n-                               BasicLock::displaced_header_offset_in_bytes()));\n-\n-    \/\/ Test for recursion\n-    testptr(header_reg, header_reg);\n-\n-    \/\/ zero for recursive case\n-    jcc(Assembler::zero, count_locking);\n-\n-    \/\/ Atomic swap back the old header\n-    lock();\n-    cmpxchgptr(header_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-\n-    \/\/ zero for simple unlock of a stack-lock case\n-    jcc(Assembler::notZero, slow_case);\n+    if (LockingMode == LM_LIGHTWEIGHT) {\n+#ifdef _LP64\n+      const Register thread = r15_thread;\n+#else\n+      const Register thread = header_reg;\n+      get_thread(thread);\n+#endif\n+      \/\/ Handle unstructured locking.\n+      Register tmp = swap_reg;\n+      movl(tmp, Address(thread, JavaThread::lock_stack_top_offset()));\n+      cmpptr(obj_reg, Address(thread, tmp, Address::times_1,  -oopSize));\n+      jcc(Assembler::notEqual, slow_case);\n+      \/\/ Try to swing header from locked to unlocked.\n+      movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      andptr(swap_reg, ~(int32_t)markWord::lock_mask_in_place);\n+      fast_unlock_impl(obj_reg, swap_reg, header_reg, slow_case);\n+    } else if (LockingMode == LM_LEGACY) {\n+      \/\/ Load the old header from BasicLock structure\n+      movptr(header_reg, Address(swap_reg,\n+                                 BasicLock::displaced_header_offset_in_bytes()));\n+\n+      \/\/ Test for recursion\n+      testptr(header_reg, header_reg);\n+\n+      \/\/ zero for recursive case\n+      jcc(Assembler::zero, count_locking);\n+\n+      \/\/ Atomic swap back the old header\n+      lock();\n+      cmpxchgptr(header_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+\n+      \/\/ zero for simple unlock of a stack-lock case\n+      jcc(Assembler::notZero, slow_case);\n@@ -1348,1 +1384,2 @@\n-    bind(count_locking);\n+      bind(count_locking);\n+    }\n","filename":"src\/hotspot\/cpu\/x86\/interp_masm_x86.cpp","additions":116,"deletions":79,"binary":false,"changes":195,"status":"modified"},{"patch":"@@ -9673,0 +9673,67 @@\n+\n+\/\/ Implements fast-locking.\n+\/\/ Branches to slow upon failure to lock the object, with ZF cleared.\n+\/\/ Falls through upon success with unspecified ZF.\n+\/\/\n+\/\/ obj: the object to be locked\n+\/\/ hdr: the (pre-loaded) header of the object, must be rax\n+\/\/ thread: the thread which attempts to lock obj\n+\/\/ tmp: a temporary register\n+void MacroAssembler::fast_lock_impl(Register obj, Register hdr, Register thread, Register tmp, Label& slow) {\n+  assert(hdr == rax, \"header must be in rax for cmpxchg\");\n+  assert_different_registers(obj, hdr, thread, tmp);\n+\n+  \/\/ First we need to check if the lock-stack has room for pushing the object reference.\n+  \/\/ Note: we subtract 1 from the end-offset so that we can do a 'greater' comparison, instead\n+  \/\/ of 'greaterEqual' below, which readily clears the ZF. This makes C2 code a little simpler and\n+  \/\/ avoids one branch.\n+  cmpl(Address(thread, JavaThread::lock_stack_top_offset()), LockStack::end_offset() - 1);\n+  jcc(Assembler::greater, slow);\n+\n+  \/\/ Now we attempt to take the fast-lock.\n+  \/\/ Clear lock_mask bits (locked state).\n+  andptr(hdr, ~(int32_t)markWord::lock_mask_in_place);\n+  movptr(tmp, hdr);\n+  \/\/ Set unlocked_value bit.\n+  orptr(hdr, markWord::unlocked_value);\n+  lock();\n+  cmpxchgptr(tmp, Address(obj, oopDesc::mark_offset_in_bytes()));\n+  jcc(Assembler::notEqual, slow);\n+\n+  \/\/ If successful, push object to lock-stack.\n+  movl(tmp, Address(thread, JavaThread::lock_stack_top_offset()));\n+  movptr(Address(thread, tmp), obj);\n+  incrementl(tmp, oopSize);\n+  movl(Address(thread, JavaThread::lock_stack_top_offset()), tmp);\n+}\n+\n+\/\/ Implements fast-unlocking.\n+\/\/ Branches to slow upon failure, with ZF cleared.\n+\/\/ Falls through upon success, with unspecified ZF.\n+\/\/\n+\/\/ obj: the object to be unlocked\n+\/\/ hdr: the (pre-loaded) header of the object, must be rax\n+\/\/ tmp: a temporary register\n+void MacroAssembler::fast_unlock_impl(Register obj, Register hdr, Register tmp, Label& slow) {\n+  assert(hdr == rax, \"header must be in rax for cmpxchg\");\n+  assert_different_registers(obj, hdr, tmp);\n+\n+  \/\/ Mark-word must be lock_mask now, try to swing it back to unlocked_value.\n+  movptr(tmp, hdr); \/\/ The expected old value\n+  orptr(tmp, markWord::unlocked_value);\n+  lock();\n+  cmpxchgptr(tmp, Address(obj, oopDesc::mark_offset_in_bytes()));\n+  jcc(Assembler::notEqual, slow);\n+  \/\/ Pop the lock object from the lock-stack.\n+#ifdef _LP64\n+  const Register thread = r15_thread;\n+#else\n+  const Register thread = rax;\n+  get_thread(thread);\n+#endif\n+  subl(Address(thread, JavaThread::lock_stack_top_offset()), oopSize);\n+#ifdef ASSERT\n+  movl(tmp, Address(thread, JavaThread::lock_stack_top_offset()));\n+  movptr(Address(thread, tmp), 0);\n+#endif\n+}\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":67,"deletions":0,"binary":false,"changes":67,"status":"modified"},{"patch":"@@ -152,0 +152,2 @@\n+  void increment(Address dst, int value = 1)  { LP64_ONLY(incrementq(dst, value)) NOT_LP64(incrementl(dst, value)) ; }\n+  void decrement(Address dst, int value = 1)  { LP64_ONLY(decrementq(dst, value)) NOT_LP64(decrementl(dst, value)) ; }\n@@ -2010,0 +2012,2 @@\n+  void fast_lock_impl(Register obj, Register hdr, Register thread, Register tmp, Label& slow);\n+  void fast_unlock_impl(Register obj, Register hdr, Register tmp, Label& slow);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1683,1 +1683,3 @@\n-    if (!UseHeavyMonitors) {\n+    if (LockingMode == LM_MONITOR) {\n+      __ jmp(slow_path_lock);\n+    } else if (LockingMode == LM_LEGACY) {\n@@ -1715,1 +1717,4 @@\n-      __ jmp(slow_path_lock);\n+      assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n+      \/\/ Load object header\n+      __ movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      __ fast_lock_impl(obj_reg, swap_reg, thread, lock_reg, slow_path_lock);\n@@ -1839,1 +1844,1 @@\n-    if (!UseHeavyMonitors) {\n+    if (LockingMode == LM_LEGACY) {\n@@ -1854,1 +1859,3 @@\n-    if (!UseHeavyMonitors) {\n+    if (LockingMode == LM_MONITOR) {\n+      __ jmp(slow_path_unlock);\n+    } else if (LockingMode == LM_LEGACY) {\n@@ -1869,1 +1876,5 @@\n-      __ jmp(slow_path_unlock);\n+      assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n+      __ movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      __ andptr(swap_reg, ~(int32_t)markWord::lock_mask_in_place);\n+      __ fast_unlock_impl(obj_reg, swap_reg, lock_reg, slow_path_unlock);\n+      __ dec_held_monitor_count();\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_32.cpp","additions":16,"deletions":5,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -2152,2 +2152,3 @@\n-    if (!UseHeavyMonitors) {\n-\n+    if (LockingMode == LM_MONITOR) {\n+      __ jmp(slow_path_lock);\n+    } else if (LockingMode == LM_LEGACY) {\n@@ -2186,1 +2187,4 @@\n-      __ jmp(slow_path_lock);\n+      assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n+      \/\/ Load object header\n+      __ movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      __ fast_lock_impl(obj_reg, swap_reg, r15_thread, rscratch1, slow_path_lock);\n@@ -2298,1 +2302,1 @@\n-    if (!UseHeavyMonitors) {\n+    if (LockingMode == LM_LEGACY) {\n@@ -2313,1 +2317,3 @@\n-    if (!UseHeavyMonitors) {\n+    if (LockingMode == LM_MONITOR) {\n+      __ jmp(slow_path_unlock);\n+    } else if (LockingMode == LM_LEGACY) {\n@@ -2325,1 +2331,5 @@\n-      __ jmp(slow_path_unlock);\n+      assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n+      __ movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      __ andptr(swap_reg, ~(int32_t)markWord::lock_mask_in_place);\n+      __ fast_unlock_impl(obj_reg, swap_reg, lock_reg, slow_path_unlock);\n+      __ dec_held_monitor_count();\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_64.cpp","additions":16,"deletions":6,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -13763,1 +13763,1 @@\n-instruct cmpFastLockRTM(eFlagsReg cr, eRegP object, eBXRegP box, eAXRegI tmp, eDXRegI scr, rRegI cx1, rRegI cx2) %{\n+instruct cmpFastLockRTM(eFlagsReg cr, eRegP object, eBXRegP box, eAXRegI tmp, eDXRegI scr, rRegI cx1, rRegI cx2, eRegP thread) %{\n@@ -13766,1 +13766,1 @@\n-  effect(TEMP tmp, TEMP scr, TEMP cx1, TEMP cx2, USE_KILL box);\n+  effect(TEMP tmp, TEMP scr, TEMP cx1, TEMP cx2, USE_KILL box, TEMP thread);\n@@ -13770,0 +13770,1 @@\n+    __ get_thread($thread$$Register);\n@@ -13771,1 +13772,1 @@\n-                 $scr$$Register, $cx1$$Register, $cx2$$Register,\n+                 $scr$$Register, $cx1$$Register, $cx2$$Register, $thread$$Register,\n@@ -13779,1 +13780,1 @@\n-instruct cmpFastLock(eFlagsReg cr, eRegP object, eBXRegP box, eAXRegI tmp, eRegP scr) %{\n+instruct cmpFastLock(eFlagsReg cr, eRegP object, eBXRegP box, eAXRegI tmp, eRegP scr, eRegP thread) %{\n@@ -13782,1 +13783,1 @@\n-  effect(TEMP tmp, TEMP scr, USE_KILL box);\n+  effect(TEMP tmp, TEMP scr, USE_KILL box, TEMP thread);\n@@ -13786,0 +13787,1 @@\n+    __ get_thread($thread$$Register);\n@@ -13787,1 +13789,1 @@\n-                 $scr$$Register, noreg, noreg, NULL, NULL, NULL, false, false);\n+                 $scr$$Register, noreg, noreg, $thread$$Register, nullptr, nullptr, nullptr, false, false);\n","filename":"src\/hotspot\/cpu\/x86\/x86_32.ad","additions":8,"deletions":6,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -13294,1 +13294,1 @@\n-                 $scr$$Register, $cx1$$Register, $cx2$$Register,\n+                 $scr$$Register, $cx1$$Register, $cx2$$Register, r15_thread,\n@@ -13310,1 +13310,1 @@\n-                 $scr$$Register, noreg, noreg, NULL, NULL, NULL, false, false);\n+                 $scr$$Register, noreg, noreg, r15_thread, nullptr, nullptr, nullptr, false, false);\n","filename":"src\/hotspot\/cpu\/x86\/x86_64.ad","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -336,1 +336,1 @@\n-    bool call_vm = UseHeavyMonitors;\n+    bool call_vm = (LockingMode == LM_MONITOR);\n","filename":"src\/hotspot\/cpu\/zero\/zeroInterpreter_zero.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -624,1 +624,1 @@\n-  CodeStub* slow_path = new MonitorExitStub(lock, !UseHeavyMonitors, monitor_no);\n+  CodeStub* slow_path = new MonitorExitStub(lock, LockingMode != LM_MONITOR, monitor_no);\n","filename":"src\/hotspot\/share\/c1\/c1_LIRGenerator.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -757,1 +757,1 @@\n-  if (UseHeavyMonitors) {\n+  if (LockingMode == LM_MONITOR) {\n@@ -760,2 +760,2 @@\n-  assert(obj == lock->obj(), \"must match\");\n-  SharedRuntime::monitor_enter_helper(obj, lock->lock(), current);\n+  assert(LockingMode == LM_LIGHTWEIGHT || obj == lock->obj(), \"must match\");\n+  SharedRuntime::monitor_enter_helper(obj, LockingMode == LM_LIGHTWEIGHT ? nullptr : lock->lock(), current);\n","filename":"src\/hotspot\/share\/c1\/c1_Runtime1.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -738,0 +738,1 @@\n+  assert(LockingMode != LM_LIGHTWEIGHT, \"Should call monitorenter_obj() when using the new lightweight locking\");\n@@ -752,0 +753,16 @@\n+\/\/ NOTE: We provide a separate implementation for the new lightweight locking to workaround a limitation\n+\/\/ of registers in x86_32. This entry point accepts an oop instead of a BasicObjectLock*.\n+\/\/ The problem is that we would need to preserve the register that holds the BasicObjectLock,\n+\/\/ but we are using that register to hold the thread. We don't have enough registers to\n+\/\/ also keep the BasicObjectLock, but we don't really need it anyway, we only need\n+\/\/ the object. See also InterpreterMacroAssembler::lock_object().\n+\/\/ As soon as legacy stack-locking goes away we could remove the other monitorenter() entry\n+\/\/ point, and only use oop-accepting entries (same for monitorexit() below).\n+JRT_ENTRY_NO_ASYNC(void, InterpreterRuntime::monitorenter_obj(JavaThread* current, oopDesc* obj))\n+  assert(LockingMode == LM_LIGHTWEIGHT, \"Should call monitorenter() when not using the new lightweight locking\");\n+  Handle h_obj(current, cast_to_oop(obj));\n+  assert(Universe::heap()->is_in_or_null(h_obj()),\n+         \"must be null or an object\");\n+  ObjectSynchronizer::enter(h_obj, nullptr, current);\n+  return;\n+JRT_END\n","filename":"src\/hotspot\/share\/interpreter\/interpreterRuntime.cpp","additions":17,"deletions":0,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -106,0 +106,1 @@\n+  static void    monitorenter_obj(JavaThread* current, oopDesc* obj);\n","filename":"src\/hotspot\/share\/interpreter\/interpreterRuntime.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -627,1 +627,1 @@\n-        bool call_vm = UseHeavyMonitors;\n+        bool call_vm = (LockingMode == LM_MONITOR);\n@@ -726,1 +726,1 @@\n-      bool call_vm = UseHeavyMonitors;\n+      bool call_vm = (LockingMode == LM_MONITOR);\n@@ -1656,1 +1656,1 @@\n-          bool call_vm = UseHeavyMonitors;\n+          bool call_vm = (LockingMode == LM_MONITOR);\n@@ -1692,1 +1692,1 @@\n-            bool call_vm = UseHeavyMonitors;\n+            bool call_vm = (LockingMode == LM_MONITOR);\n@@ -3192,1 +3192,1 @@\n-          } else if (UseHeavyMonitors) {\n+          } else if (LockingMode == LM_MONITOR) {\n","filename":"src\/hotspot\/share\/interpreter\/zero\/bytecodeInterpreter.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -80,0 +80,1 @@\n+  LOG_TAG(fastlock) \\\n","filename":"src\/hotspot\/share\/logging\/logTag.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -53,1 +53,2 @@\n-\/\/    [ptr             | 00]  locked             ptr points to real header on stack\n+\/\/    [ptr             | 00]  locked             ptr points to real header on stack (stack-locking in use)\n+\/\/    [header          | 00]  locked             locked regular object header (fast-locking in use)\n@@ -55,1 +56,1 @@\n-\/\/    [ptr             | 10]  monitor            inflated lock (header is wapped out)\n+\/\/    [ptr             | 10]  monitor            inflated lock (header is swapped out)\n@@ -57,1 +58,1 @@\n-\/\/    [0 ............ 0| 00]  inflating          inflation in progress\n+\/\/    [0 ............ 0| 00]  inflating          inflation in progress (stack-locking in use)\n@@ -159,0 +160,1 @@\n+  \/\/ Fast-locking does not use INFLATING.\n@@ -173,1 +175,2 @@\n-    return ((value() & lock_mask_in_place) == locked_value);\n+    assert(LockingMode == LM_LEGACY, \"should only be called with legacy stack locking\");\n+    return (value() & lock_mask_in_place) == locked_value;\n@@ -179,0 +182,10 @@\n+\n+  bool is_fast_locked() const {\n+    assert(LockingMode == LM_LIGHTWEIGHT, \"should only be called with new lightweight locking\");\n+    return (value() & lock_mask_in_place) == locked_value;\n+  }\n+  markWord set_fast_locked() const {\n+    \/\/ Clear the lock_mask_in_place bits to set locked_value:\n+    return markWord(value() & ~lock_mask_in_place);\n+  }\n+\n@@ -188,1 +201,3 @@\n-    return ((value() & unlocked_value) == 0);\n+    intptr_t lockbits = value() & lock_mask_in_place;\n+    return LockingMode == LM_LIGHTWEIGHT  ? lockbits == monitor_value   \/\/ monitor?\n+                                          : (lockbits & unlocked_value) == 0; \/\/ monitor | stack-locked?\n","filename":"src\/hotspot\/share\/oops\/markWord.hpp","additions":20,"deletions":5,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -122,1 +122,1 @@\n-  \/\/ at a safepoint, it must not be zero.\n+  \/\/ at a safepoint, it must not be zero, except when using the new lightweight locking.\n@@ -131,1 +131,1 @@\n-  return !SafepointSynchronize::is_at_safepoint();\n+  return LockingMode == LM_LIGHTWEIGHT || !SafepointSynchronize::is_at_safepoint();\n","filename":"src\/hotspot\/share\/oops\/oop.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -89,0 +89,15 @@\n+#ifdef _LP64\n+class C2HandleAnonOMOwnerStub : public C2CodeStub {\n+private:\n+  Register _monitor;\n+  Register _tmp;\n+public:\n+  C2HandleAnonOMOwnerStub(Register monitor, Register tmp = noreg) : C2CodeStub(),\n+    _monitor(monitor), _tmp(tmp) {}\n+  Register monitor() { return _monitor; }\n+  Register tmp() { return _tmp; }\n+  int max_size() const;\n+  void emit(C2_MacroAssembler& masm);\n+};\n+#endif\n+\n","filename":"src\/hotspot\/share\/opto\/c2_CodeStubs.hpp","additions":15,"deletions":0,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -1941,1 +1941,8 @@\n-#if !defined(X86) && !defined(AARCH64) && !defined(PPC64) && !defined(RISCV64)\n+\n+#if !defined(X86) && !defined(AARCH64) && !defined(RISCV64) && !defined(ARM)\n+  if (LockingMode == LM_LIGHTWEIGHT) {\n+    FLAG_SET_CMDLINE(LockingMode, LM_LEGACY);\n+    warning(\"New lightweight locking not supported on this platform\");\n+  }\n+#endif\n+\n@@ -1943,0 +1950,10 @@\n+    if (FLAG_IS_CMDLINE(LockingMode) && LockingMode != LM_MONITOR) {\n+      jio_fprintf(defaultStream::error_stream(),\n+                  \"Conflicting -XX:+UseHeavyMonitors and -XX:LockingMode=%d flags\", LockingMode);\n+      return false;\n+    }\n+    FLAG_SET_CMDLINE(LockingMode, LM_MONITOR);\n+  }\n+\n+#if !defined(X86) && !defined(AARCH64) && !defined(PPC64) && !defined(RISCV64)\n+  if (LockingMode == LM_MONITOR) {\n@@ -1944,1 +1961,1 @@\n-                \"UseHeavyMonitors is not fully implemented on this architecture\");\n+                \"LockingMode == 0 (LM_MONITOR) is not fully implemented on this architecture\");\n@@ -1949,1 +1966,1 @@\n-  if (UseHeavyMonitors && UseRTMForStackLocks) {\n+  if (LockingMode == LM_MONITOR && UseRTMForStackLocks) {\n@@ -1951,1 +1968,1 @@\n-                \"-XX:+UseHeavyMonitors and -XX:+UseRTMForStackLocks are mutually exclusive\");\n+                \"LockingMode == 0 (LM_MONITOR) and -XX:+UseRTMForStackLocks are mutually exclusive\");\n@@ -1956,1 +1973,1 @@\n-  if (VerifyHeavyMonitors && !UseHeavyMonitors) {\n+  if (VerifyHeavyMonitors && LockingMode != LM_MONITOR) {\n@@ -1958,1 +1975,1 @@\n-                \"-XX:+VerifyHeavyMonitors requires -XX:+UseHeavyMonitors\");\n+                \"-XX:+VerifyHeavyMonitors requires LockingMode == 0 (LM_MONITOR)\");\n@@ -1961,1 +1978,0 @@\n-\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":23,"deletions":7,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -1633,1 +1633,1 @@\n-          if (mark.has_locker() && fr.sp() > (intptr_t*)mark.locker()) {\n+          if (LockingMode == LM_LEGACY && mark.has_locker() && fr.sp() > (intptr_t*)mark.locker()) {\n","filename":"src\/hotspot\/share\/runtime\/deoptimization.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1981,0 +1981,7 @@\n+                                                                            \\\n+  product(int, LockingMode, LM_LEGACY, EXPERIMENTAL,                        \\\n+          \"Select locking mode: \"                                           \\\n+          \"0: monitors only, \"                                              \\\n+          \"1: monitors & legacy stack-locking (default), \"                  \\\n+          \"2: monitors & new lightweight locking\")                          \\\n+          range(0, 2)                                                       \\\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -73,0 +73,1 @@\n+#include \"runtime\/lockStack.inline.hpp\"\n@@ -493,2 +494,3 @@\n-  _SleepEvent(ParkEvent::Allocate(this))\n-{\n+  _SleepEvent(ParkEvent::Allocate(this)),\n+\n+  _lock_stack(this) {\n@@ -997,0 +999,1 @@\n+  assert(LockingMode != LM_LIGHTWEIGHT, \"should not be called with new lightweight locking\");\n@@ -1390,0 +1393,4 @@\n+\n+  if (LockingMode == LM_LIGHTWEIGHT) {\n+    lock_stack().oops_do(f);\n+  }\n","filename":"src\/hotspot\/share\/runtime\/javaThread.cpp","additions":9,"deletions":2,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -37,0 +37,1 @@\n+#include \"runtime\/lockStack.hpp\"\n@@ -1148,0 +1149,10 @@\n+private:\n+  LockStack _lock_stack;\n+\n+public:\n+  LockStack& lock_stack() { return _lock_stack; }\n+\n+  static ByteSize lock_stack_offset()      { return byte_offset_of(JavaThread, _lock_stack); }\n+  static ByteSize lock_stack_top_offset()  { return lock_stack_offset() + LockStack::top_offset(); }\n+  static ByteSize lock_stack_base_offset() { return lock_stack_offset() + LockStack::base_offset(); }\n+\n","filename":"src\/hotspot\/share\/runtime\/javaThread.hpp","additions":11,"deletions":0,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -0,0 +1,76 @@\n+\/*\n+ * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"memory\/allocation.hpp\"\n+#include \"runtime\/lockStack.inline.hpp\"\n+#include \"runtime\/safepoint.hpp\"\n+#include \"runtime\/stackWatermark.hpp\"\n+#include \"runtime\/stackWatermarkSet.inline.hpp\"\n+#include \"runtime\/thread.hpp\"\n+#include \"utilities\/copy.hpp\"\n+#include \"utilities\/ostream.hpp\"\n+\n+const int LockStack::lock_stack_offset =      in_bytes(JavaThread::lock_stack_offset());\n+const int LockStack::lock_stack_top_offset =  in_bytes(JavaThread::lock_stack_top_offset());\n+const int LockStack::lock_stack_base_offset = in_bytes(JavaThread::lock_stack_base_offset());\n+\n+LockStack::LockStack(JavaThread* jt) :\n+  _top(lock_stack_base_offset), _base() {\n+#ifdef ASSERT\n+  for (int i = 0; i < CAPACITY; i++) {\n+    _base[i] = nullptr;\n+  }\n+#endif\n+}\n+\n+uint32_t LockStack::start_offset() {\n+  int offset = lock_stack_base_offset;\n+  assert(offset > 0, \"must be positive offset\");\n+  return static_cast<uint32_t>(offset);\n+}\n+\n+uint32_t LockStack::end_offset() {\n+  int offset = lock_stack_base_offset + CAPACITY * oopSize;\n+  assert(offset > 0, \"must be positive offset\");\n+  return static_cast<uint32_t>(offset);\n+}\n+\n+#ifndef PRODUCT\n+void LockStack::verify(const char* msg) const {\n+  assert(LockingMode == LM_LIGHTWEIGHT, \"never use lock-stack when light weight locking is disabled\");\n+  assert((_top <= end_offset()), \"lockstack overflow: _top %d end_offset %d\", _top, end_offset());\n+  assert((_top >= start_offset()), \"lockstack underflow: _top %d end_offset %d\", _top, start_offset());\n+  int top = to_index(_top);\n+  for (int i = 0; i < top; i++) {\n+    assert(_base[i] != nullptr, \"no zapped before top\");\n+    for (int j = i + 1; j < top; j++) {\n+      assert(_base[i] != _base[j], \"entries must be unique: %s\", msg);\n+    }\n+  }\n+  for (int i = top; i < CAPACITY; i++) {\n+    assert(_base[i] == nullptr, \"only zapped entries after top: i: %d, top: %d, entry: \" PTR_FORMAT, i, top, p2i(_base[i]));\n+  }\n+}\n+#endif\n","filename":"src\/hotspot\/share\/runtime\/lockStack.cpp","additions":76,"deletions":0,"binary":false,"changes":76,"status":"added"},{"patch":"@@ -0,0 +1,80 @@\n+\/*\n+ * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_RUNTIME_LOCKSTACK_HPP\n+#define SHARE_RUNTIME_LOCKSTACK_HPP\n+\n+#include \"oops\/oopsHierarchy.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include \"utilities\/sizes.hpp\"\n+\n+class Thread;\n+class OopClosure;\n+\n+class LockStack {\n+  friend class VMStructs;\n+private:\n+  static const int CAPACITY = 8;\n+\n+  \/\/ TODO: It would be very useful if JavaThread::lock_stack_offset() and friends were constexpr,\n+  \/\/ but this is currently not the case because we're using offset_of() which is non-constexpr,\n+  \/\/ GCC would warn about non-standard-layout types if we were using offsetof() (which *is* constexpr).\n+  static const int lock_stack_offset;\n+  static const int lock_stack_top_offset;\n+  static const int lock_stack_base_offset;\n+\n+  \/\/ The offset of the next element, in bytes, relative to the JavaThread structure.\n+  \/\/ We do this instead of a simple index into the array because this allows for\n+  \/\/ efficient addressing in generated code.\n+  uint32_t _top;\n+  oop _base[CAPACITY];\n+\n+  inline JavaThread* get_thread() const;\n+\n+  bool is_self() const;\n+  void verify(const char* msg) const PRODUCT_RETURN;\n+\n+  static inline int to_index(uint32_t offset);\n+\n+public:\n+  static ByteSize top_offset()  { return byte_offset_of(LockStack, _top); }\n+  static ByteSize base_offset() { return byte_offset_of(LockStack, _base); }\n+\n+  LockStack(JavaThread* jt);\n+\n+  static uint32_t start_offset();\n+  static uint32_t end_offset();\n+  inline bool can_push() const;\n+  inline void push(oop o);\n+  inline oop pop();\n+  inline void remove(oop o);\n+\n+  inline bool contains(oop o) const;\n+\n+  \/\/ GC support\n+  inline void oops_do(OopClosure* cl);\n+\n+};\n+\n+#endif \/\/ SHARE_RUNTIME_LOCKSTACK_HPP\n","filename":"src\/hotspot\/share\/runtime\/lockStack.hpp","additions":80,"deletions":0,"binary":false,"changes":80,"status":"added"},{"patch":"@@ -0,0 +1,128 @@\n+\/*\n+ * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_RUNTIME_LOCKSTACK_INLINE_HPP\n+#define SHARE_RUNTIME_LOCKSTACK_INLINE_HPP\n+\n+#include \"memory\/iterator.hpp\"\n+#include \"runtime\/javaThread.hpp\"\n+#include \"runtime\/lockStack.hpp\"\n+#include \"runtime\/safepoint.hpp\"\n+#include \"runtime\/stackWatermark.hpp\"\n+#include \"runtime\/stackWatermarkSet.inline.hpp\"\n+\n+inline int LockStack::to_index(uint32_t offset) {\n+  return (offset - lock_stack_base_offset) \/ oopSize;\n+}\n+\n+JavaThread* LockStack::get_thread() const {\n+  char* addr = reinterpret_cast<char*>(const_cast<LockStack*>(this));\n+  return reinterpret_cast<JavaThread*>(addr - lock_stack_offset);\n+}\n+\n+inline bool LockStack::can_push() const {\n+  return to_index(_top) < CAPACITY;\n+}\n+\n+inline bool LockStack::is_self() const {\n+  Thread* thread = Thread::current();\n+  bool is_self = &JavaThread::cast(thread)->lock_stack() == this;\n+  assert(is_self == (get_thread() == thread), \"is_self sanity\");\n+  return is_self;\n+}\n+\n+inline void LockStack::push(oop o) {\n+  verify(\"pre-push\");\n+  assert(oopDesc::is_oop(o), \"must be\");\n+  assert(!contains(o), \"entries must be unique\");\n+  assert(can_push(), \"must have room\");\n+  assert(_base[to_index(_top)] == nullptr, \"expect zapped entry\");\n+  _base[to_index(_top)] = o;\n+  _top += oopSize;\n+  verify(\"post-push\");\n+}\n+\n+inline oop LockStack::pop() {\n+  verify(\"pre-pop\");\n+  assert(to_index(_top) > 0, \"underflow, probably unbalanced push\/pop\");\n+  _top -= oopSize;\n+  oop o = _base[to_index(_top)];\n+#ifdef ASSERT\n+  _base[to_index(_top)] = nullptr;\n+#endif\n+  assert(!contains(o), \"entries must be unique: \" PTR_FORMAT, p2i(o));\n+  verify(\"post-pop\");\n+  return o;\n+}\n+\n+inline void LockStack::remove(oop o) {\n+  verify(\"pre-remove\");\n+  assert(contains(o), \"entry must be present: \" PTR_FORMAT, p2i(o));\n+  int end = to_index(_top);\n+  for (int i = 0; i < end; i++) {\n+    if (_base[i] == o) {\n+      int last = end - 1;\n+      for (; i < last; i++) {\n+        _base[i] = _base[i + 1];\n+      }\n+      _top -= oopSize;\n+#ifdef ASSERT\n+      _base[to_index(_top)] = nullptr;\n+#endif\n+      break;\n+    }\n+  }\n+  assert(!contains(o), \"entries must be unique: \" PTR_FORMAT, p2i(o));\n+  verify(\"post-remove\");\n+}\n+\n+inline bool LockStack::contains(oop o) const {\n+  verify(\"pre-contains\");\n+  if (!SafepointSynchronize::is_at_safepoint() && !is_self()) {\n+    StackWatermark* watermark = StackWatermarkSet::get(get_thread(), StackWatermarkKind::gc);\n+    if (watermark != nullptr) {\n+      watermark->start_processing();\n+    }\n+  }\n+  int end = to_index(_top);\n+  for (int i = end - 1; i >= 0; i--) {\n+    if (_base[i] == o) {\n+      verify(\"post-contains\");\n+      return true;\n+    }\n+  }\n+  verify(\"post-contains\");\n+  return false;\n+}\n+\n+inline void LockStack::oops_do(OopClosure* cl) {\n+  verify(\"pre-oops-do\");\n+  int end = to_index(_top);\n+  for (int i = 0; i < end; i++) {\n+    cl->do_oop(&_base[i]);\n+  }\n+  verify(\"post-oops-do\");\n+}\n+\n+#endif \/\/ SHARE_RUNTIME_LOCKSTACK_INLINE_HPP\n","filename":"src\/hotspot\/share\/runtime\/lockStack.inline.hpp","additions":128,"deletions":0,"binary":false,"changes":128,"status":"added"},{"patch":"@@ -337,1 +337,1 @@\n-  if (current->is_lock_owned((address)cur)) {\n+  if (LockingMode != LM_LIGHTWEIGHT && current->is_lock_owned((address)cur)) {\n@@ -1138,1 +1138,1 @@\n-    if (current->is_lock_owned((address)cur)) {\n+    if (LockingMode != LM_LIGHTWEIGHT && current->is_lock_owned((address)cur)) {\n@@ -1353,1 +1353,1 @@\n-    if (current->is_lock_owned((address)cur)) {\n+    if (LockingMode != LM_LIGHTWEIGHT && current->is_lock_owned((address)cur)) {\n@@ -1388,0 +1388,1 @@\n+  assert(cur != anon_owner_ptr(), \"no anon owner here\");\n@@ -1391,1 +1392,1 @@\n-  if (current->is_lock_owned((address)cur)) {\n+  if (LockingMode != LM_LIGHTWEIGHT && current->is_lock_owned((address)cur)) {\n","filename":"src\/hotspot\/share\/runtime\/objectMonitor.cpp","additions":5,"deletions":4,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -147,2 +147,16 @@\n-  \/\/ Used by async deflation as a marker in the _owner field:\n-  #define DEFLATER_MARKER reinterpret_cast<void*>(-1)\n+  \/\/ Used by async deflation as a marker in the _owner field.\n+  \/\/ Note that the choice of the two markers is peculiar:\n+  \/\/ - They need to represent values that cannot be pointers. In particular,\n+  \/\/   we achieve this by using the lowest two bits.\n+  \/\/ - ANONYMOUS_OWNER should be a small value, it is used in generated code\n+  \/\/   and small values encode much better.\n+  \/\/ - We test for anonymous owner by testing for the lowest bit, therefore\n+  \/\/   DEFLATER_MARKER must *not* have that bit set.\n+  #define DEFLATER_MARKER reinterpret_cast<void*>(2)\n+public:\n+  \/\/ NOTE: Typed as uintptr_t so that we can pick it up in SA, via vmStructs.\n+  static const uintptr_t ANONYMOUS_OWNER = 1;\n+\n+private:\n+  static void* anon_owner_ptr() { return reinterpret_cast<void*>(ANONYMOUS_OWNER); }\n+\n@@ -181,0 +195,1 @@\n+\n@@ -245,1 +260,1 @@\n-  intptr_t  is_entered(JavaThread* current) const;\n+  bool is_entered(JavaThread* current) const;\n@@ -266,0 +281,12 @@\n+  void set_owner_anonymous() {\n+    set_owner_from(nullptr, anon_owner_ptr());\n+  }\n+\n+  bool is_owner_anonymous() const {\n+    return owner_raw() == anon_owner_ptr();\n+  }\n+\n+  void set_owner_from_anonymous(Thread* owner) {\n+    set_owner_from(anon_owner_ptr(), owner);\n+  }\n+\n","filename":"src\/hotspot\/share\/runtime\/objectMonitor.hpp","additions":30,"deletions":3,"binary":false,"changes":33,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"runtime\/lockStack.inline.hpp\"\n@@ -35,4 +36,12 @@\n-inline intptr_t ObjectMonitor::is_entered(JavaThread* current) const {\n-  void* owner = owner_raw();\n-  if (current == owner || current->is_lock_owned((address)owner)) {\n-    return 1;\n+inline bool ObjectMonitor::is_entered(JavaThread* current) const {\n+  if (LockingMode == LM_LIGHTWEIGHT) {\n+    if (is_owner_anonymous()) {\n+      return current->lock_stack().contains(object());\n+    } else {\n+      return current == owner_raw();\n+    }\n+  } else {\n+    void* owner = owner_raw();\n+    if (current == owner || current->is_lock_owned((address)owner)) {\n+      return true;\n+    }\n@@ -40,1 +49,1 @@\n-  return 0;\n+  return false;\n","filename":"src\/hotspot\/share\/runtime\/objectMonitor.inline.hpp","additions":14,"deletions":5,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -42,0 +42,1 @@\n+#include \"runtime\/lockStack.inline.hpp\"\n@@ -326,4 +327,12 @@\n-  if (mark.has_locker() && current->is_lock_owned((address)mark.locker())) {\n-    \/\/ Degenerate notify\n-    \/\/ stack-locked by caller so by definition the implied waitset is empty.\n-    return true;\n+  if (LockingMode == LM_LIGHTWEIGHT) {\n+    if (mark.is_fast_locked() && current->lock_stack().contains(cast_to_oop(obj))) {\n+      \/\/ Degenerate notify\n+      \/\/ fast-locked by caller so by definition the implied waitset is empty.\n+      return true;\n+    }\n+  } else if (LockingMode == LM_LEGACY) {\n+    if (mark.has_locker() && current->is_lock_owned((address)mark.locker())) {\n+      \/\/ Degenerate notify\n+      \/\/ stack-locked by caller so by definition the implied waitset is empty.\n+      return true;\n+    }\n@@ -400,10 +409,12 @@\n-    \/\/ This Java Monitor is inflated so obj's header will never be\n-    \/\/ displaced to this thread's BasicLock. Make the displaced header\n-    \/\/ non-null so this BasicLock is not seen as recursive nor as\n-    \/\/ being locked. We do this unconditionally so that this thread's\n-    \/\/ BasicLock cannot be mis-interpreted by any stack walkers. For\n-    \/\/ performance reasons, stack walkers generally first check for\n-    \/\/ stack-locking in the object's header, the second check is for\n-    \/\/ recursive stack-locking in the displaced header in the BasicLock,\n-    \/\/ and last are the inflated Java Monitor (ObjectMonitor) checks.\n-    lock->set_displaced_header(markWord::unused_mark());\n+    if (LockingMode != LM_LIGHTWEIGHT) {\n+      \/\/ This Java Monitor is inflated so obj's header will never be\n+      \/\/ displaced to this thread's BasicLock. Make the displaced header\n+      \/\/ non-null so this BasicLock is not seen as recursive nor as\n+      \/\/ being locked. We do this unconditionally so that this thread's\n+      \/\/ BasicLock cannot be mis-interpreted by any stack walkers. For\n+      \/\/ performance reasons, stack walkers generally first check for\n+      \/\/ stack-locking in the object's header, the second check is for\n+      \/\/ recursive stack-locking in the displaced header in the BasicLock,\n+      \/\/ and last are the inflated Java Monitor (ObjectMonitor) checks.\n+      lock->set_displaced_header(markWord::unused_mark());\n+    }\n@@ -478,1 +489,1 @@\n-  return UseHeavyMonitors;\n+  return LockingMode == LM_MONITOR;\n@@ -498,6 +509,33 @@\n-    markWord mark = obj->mark();\n-    if (mark.is_neutral()) {\n-      \/\/ Anticipate successful CAS -- the ST of the displaced mark must\n-      \/\/ be visible <= the ST performed by the CAS.\n-      lock->set_displaced_header(mark);\n-      if (mark == obj()->cas_set_mark(markWord::from_pointer(lock), mark)) {\n+    if (LockingMode == LM_LIGHTWEIGHT) {\n+      \/\/ Fast-locking does not use the 'lock' argument.\n+      LockStack& lock_stack = current->lock_stack();\n+      if (lock_stack.can_push()) {\n+        markWord mark = obj()->mark_acquire();\n+        if (mark.is_neutral()) {\n+          assert(!lock_stack.contains(obj()), \"thread must not already hold the lock\");\n+          \/\/ Try to swing into 'fast-locked' state.\n+          markWord locked_mark = mark.set_fast_locked();\n+          markWord old_mark = obj()->cas_set_mark(locked_mark, mark);\n+          if (old_mark == mark) {\n+            \/\/ Successfully fast-locked, push object to lock-stack and return.\n+            lock_stack.push(obj());\n+            return;\n+          }\n+        }\n+      }\n+      \/\/ All other paths fall-through to inflate-enter.\n+    } else if (LockingMode == LM_LEGACY) {\n+      markWord mark = obj->mark();\n+      if (mark.is_neutral()) {\n+        \/\/ Anticipate successful CAS -- the ST of the displaced mark must\n+        \/\/ be visible <= the ST performed by the CAS.\n+        lock->set_displaced_header(mark);\n+        if (mark == obj()->cas_set_mark(markWord::from_pointer(lock), mark)) {\n+          return;\n+        }\n+        \/\/ Fall through to inflate() ...\n+      } else if (mark.has_locker() &&\n+                 current->is_lock_owned((address) mark.locker())) {\n+        assert(lock != mark.locker(), \"must not re-lock the same lock\");\n+        assert(lock != (BasicLock*) obj->mark().value(), \"don't relock with same BasicLock\");\n+        lock->set_displaced_header(markWord::from_pointer(nullptr));\n@@ -506,8 +544,0 @@\n-      \/\/ Fall through to inflate() ...\n-    } else if (mark.has_locker() &&\n-               current->is_lock_owned((address)mark.locker())) {\n-      assert(lock != mark.locker(), \"must not re-lock the same lock\");\n-      assert(lock != (BasicLock*)obj->mark().value(), \"don't relock with same BasicLock\");\n-      lock->set_displaced_header(markWord::from_pointer(nullptr));\n-      return;\n-    }\n@@ -515,5 +545,6 @@\n-    \/\/ The object header will never be displaced to this lock,\n-    \/\/ so it does not matter what the value is, except that it\n-    \/\/ must be non-zero to avoid looking like a re-entrant lock,\n-    \/\/ and must not look locked either.\n-    lock->set_displaced_header(markWord::unused_mark());\n+      \/\/ The object header will never be displaced to this lock,\n+      \/\/ so it does not matter what the value is, except that it\n+      \/\/ must be non-zero to avoid looking like a re-entrant lock,\n+      \/\/ and must not look locked either.\n+      lock->set_displaced_header(markWord::unused_mark());\n+    }\n@@ -521,1 +552,1 @@\n-    guarantee(!obj->mark().has_locker(), \"must not be stack-locked\");\n+    guarantee((obj->mark().value() & markWord::lock_mask_in_place) != markWord::locked_value, \"must not be lightweight\/stack-locked\");\n@@ -540,25 +571,15 @@\n-\n-    markWord dhw = lock->displaced_header();\n-    if (dhw.value() == 0) {\n-      \/\/ If the displaced header is null, then this exit matches up with\n-      \/\/ a recursive enter. No real work to do here except for diagnostics.\n-#ifndef PRODUCT\n-      if (mark != markWord::INFLATING()) {\n-        \/\/ Only do diagnostics if we are not racing an inflation. Simply\n-        \/\/ exiting a recursive enter of a Java Monitor that is being\n-        \/\/ inflated is safe; see the has_monitor() comment below.\n-        assert(!mark.is_neutral(), \"invariant\");\n-        assert(!mark.has_locker() ||\n-        current->is_lock_owned((address)mark.locker()), \"invariant\");\n-        if (mark.has_monitor()) {\n-          \/\/ The BasicLock's displaced_header is marked as a recursive\n-          \/\/ enter and we have an inflated Java Monitor (ObjectMonitor).\n-          \/\/ This is a special case where the Java Monitor was inflated\n-          \/\/ after this thread entered the stack-lock recursively. When a\n-          \/\/ Java Monitor is inflated, we cannot safely walk the Java\n-          \/\/ Monitor owner's stack and update the BasicLocks because a\n-          \/\/ Java Monitor can be asynchronously inflated by a thread that\n-          \/\/ does not own the Java Monitor.\n-          ObjectMonitor* m = mark.monitor();\n-          assert(m->object()->mark() == mark, \"invariant\");\n-          assert(m->is_entered(current), \"invariant\");\n+    if (LockingMode == LM_LIGHTWEIGHT) {\n+      \/\/ Fast-locking does not use the 'lock' argument.\n+      if (mark.is_fast_locked()) {\n+        markWord unlocked_mark = mark.set_unlocked();\n+        markWord old_mark = object->cas_set_mark(unlocked_mark, mark);\n+        if (old_mark != mark) {\n+          \/\/ Another thread won the CAS, it must have inflated the monitor.\n+          \/\/ It can only have installed an anonymously locked monitor at this point.\n+          \/\/ Fetch that monitor, set owner correctly to this thread, and\n+          \/\/ exit it (allowing waiting threads to enter).\n+          assert(old_mark.has_monitor(), \"must have monitor\");\n+          ObjectMonitor* monitor = old_mark.monitor();\n+          assert(monitor->is_owner_anonymous(), \"must be anonymous owner\");\n+          monitor->set_owner_from_anonymous(current);\n+          monitor->exit(current);\n@@ -566,0 +587,3 @@\n+        LockStack& lock_stack = current->lock_stack();\n+        lock_stack.remove(object);\n+        return;\n@@ -567,0 +591,27 @@\n+    } else if (LockingMode == LM_LEGACY) {\n+      markWord dhw = lock->displaced_header();\n+      if (dhw.value() == 0) {\n+        \/\/ If the displaced header is null, then this exit matches up with\n+        \/\/ a recursive enter. No real work to do here except for diagnostics.\n+#ifndef PRODUCT\n+        if (mark != markWord::INFLATING()) {\n+          \/\/ Only do diagnostics if we are not racing an inflation. Simply\n+          \/\/ exiting a recursive enter of a Java Monitor that is being\n+          \/\/ inflated is safe; see the has_monitor() comment below.\n+          assert(!mark.is_neutral(), \"invariant\");\n+          assert(!mark.has_locker() ||\n+                 current->is_lock_owned((address)mark.locker()), \"invariant\");\n+          if (mark.has_monitor()) {\n+            \/\/ The BasicLock's displaced_header is marked as a recursive\n+            \/\/ enter and we have an inflated Java Monitor (ObjectMonitor).\n+            \/\/ This is a special case where the Java Monitor was inflated\n+            \/\/ after this thread entered the stack-lock recursively. When a\n+            \/\/ Java Monitor is inflated, we cannot safely walk the Java\n+            \/\/ Monitor owner's stack and update the BasicLocks because a\n+            \/\/ Java Monitor can be asynchronously inflated by a thread that\n+            \/\/ does not own the Java Monitor.\n+            ObjectMonitor* m = mark.monitor();\n+            assert(m->object()->mark() == mark, \"invariant\");\n+            assert(m->is_entered(current), \"invariant\");\n+          }\n+        }\n@@ -568,8 +619,0 @@\n-      return;\n-    }\n-\n-    if (mark == markWord::from_pointer(lock)) {\n-      \/\/ If the object is stack-locked by the current thread, try to\n-      \/\/ swing the displaced header from the BasicLock back to the mark.\n-      assert(dhw.is_neutral(), \"invariant\");\n-      if (object->cas_set_mark(dhw, mark) == mark) {\n@@ -578,0 +621,9 @@\n+\n+      if (mark == markWord::from_pointer(lock)) {\n+        \/\/ If the object is stack-locked by the current thread, try to\n+        \/\/ swing the displaced header from the BasicLock back to the mark.\n+        assert(dhw.is_neutral(), \"invariant\");\n+        if (object->cas_set_mark(dhw, mark) == mark) {\n+          return;\n+        }\n+      }\n@@ -580,1 +632,1 @@\n-    guarantee(!object->mark().has_locker(), \"must not be stack-locked\");\n+    guarantee((object->mark().value() & markWord::lock_mask_in_place) != markWord::locked_value, \"must not be lightweight\/stack-locked\");\n@@ -587,0 +639,7 @@\n+  if (LockingMode == LM_LIGHTWEIGHT && monitor->is_owner_anonymous()) {\n+    \/\/ It must be owned by us. Pop lock object from lock stack.\n+    LockStack& lock_stack = current->lock_stack();\n+    oop popped = lock_stack.pop();\n+    assert(popped == object, \"must be owned by this thread\");\n+    monitor->set_owner_from_anonymous(current);\n+  }\n@@ -677,3 +736,10 @@\n-  if (mark.has_locker() && current->is_lock_owned((address)mark.locker())) {\n-    \/\/ Not inflated so there can't be any waiters to notify.\n-    return;\n+  if (LockingMode == LM_LIGHTWEIGHT) {\n+    if ((mark.is_fast_locked() && current->lock_stack().contains(obj()))) {\n+      \/\/ Not inflated so there can't be any waiters to notify.\n+      return;\n+    }\n+  } else if (LockingMode == LM_LEGACY) {\n+    if (mark.has_locker() && current->is_lock_owned((address)mark.locker())) {\n+      \/\/ Not inflated so there can't be any waiters to notify.\n+      return;\n+    }\n@@ -692,3 +758,10 @@\n-  if (mark.has_locker() && current->is_lock_owned((address)mark.locker())) {\n-    \/\/ Not inflated so there can't be any waiters to notify.\n-    return;\n+  if (LockingMode == LM_LIGHTWEIGHT) {\n+    if ((mark.is_fast_locked() && current->lock_stack().contains(obj()))) {\n+      \/\/ Not inflated so there can't be any waiters to notify.\n+      return;\n+    }\n+  } else if (LockingMode == LM_LEGACY) {\n+    if (mark.has_locker() && current->is_lock_owned((address)mark.locker())) {\n+      \/\/ Not inflated so there can't be any waiters to notify.\n+      return;\n+    }\n@@ -720,1 +793,2 @@\n-  if (!mark.is_being_inflated()) {\n+  if (!mark.is_being_inflated() || LockingMode == LM_LIGHTWEIGHT) {\n+    \/\/ New lightweight locking does not use the markWord::INFLATING() protocol.\n@@ -835,0 +909,7 @@\n+\/\/ Can be called from non JavaThreads (e.g., VMThread) for FastHashCode\n+\/\/ calculations as part of JVM\/TI tagging.\n+static bool is_lock_owned(Thread* thread, oop obj) {\n+  assert(LockingMode == LM_LIGHTWEIGHT, \"only call this with new lightweight locking enabled\");\n+  return thread->is_Java_thread() ? JavaThread::cast(thread)->lock_stack().contains(obj) : false;\n+}\n+\n@@ -843,2 +924,2 @@\n-      assert(UseHeavyMonitors, \"+VerifyHeavyMonitors requires +UseHeavyMonitors\");\n-      guarantee(!mark.has_locker(), \"must not be stack locked\");\n+      assert(LockingMode == LM_MONITOR, \"+VerifyHeavyMonitors requires LockingMode == 0 (LM_MONITOR)\");\n+      guarantee((obj->mark().value() & markWord::lock_mask_in_place) != markWord::locked_value, \"must not be lightweight\/stack-locked\");\n@@ -889,2 +970,9 @@\n-    } else if (current->is_lock_owned((address)mark.locker())) {\n-      \/\/ This is a stack lock owned by the calling thread so fetch the\n+    } else if (LockingMode == LM_LIGHTWEIGHT && mark.is_fast_locked() && is_lock_owned(current, obj)) {\n+      \/\/ This is a fast-lock owned by the calling thread so use the\n+      \/\/ markWord from the object.\n+      hash = mark.hash();\n+      if (hash != 0) {                  \/\/ if it has a hash, just return it\n+        return hash;\n+      }\n+    } else if (LockingMode == LM_LEGACY && mark.has_locker() && current->is_lock_owned((address)mark.locker())) {\n+      \/\/ This is a stack-lock owned by the calling thread so fetch the\n@@ -901,1 +989,1 @@\n-      \/\/ So we have to inflate the stack lock into an ObjectMonitor\n+      \/\/ So we have to inflate the stack-lock into an ObjectMonitor\n@@ -954,2 +1042,2 @@\n-  \/\/ Uncontended case, header points to stack\n-  if (mark.has_locker()) {\n+  if (LockingMode == LM_LEGACY && mark.has_locker()) {\n+    \/\/ stack-locked case, header points into owner's stack\n@@ -958,1 +1046,6 @@\n-  \/\/ Contended case, header points to ObjectMonitor (tagged pointer)\n+\n+  if (LockingMode == LM_LIGHTWEIGHT && mark.is_fast_locked()) {\n+    \/\/ fast-locking case, see if lock is in current's lock stack\n+    return current->lock_stack().contains(h_obj());\n+  }\n+\n@@ -960,0 +1053,1 @@\n+    \/\/ Inflated monitor so header points to ObjectMonitor (tagged pointer).\n@@ -972,2 +1066,0 @@\n-  address owner = nullptr;\n-\n@@ -976,3 +1068,10 @@\n-  \/\/ Uncontended case, header points to stack\n-  if (mark.has_locker()) {\n-    owner = (address) mark.locker();\n+  if (LockingMode == LM_LEGACY && mark.has_locker()) {\n+    \/\/ stack-locked so header points into owner's stack.\n+    \/\/ owning_thread_from_monitor_owner() may also return null here:\n+    return Threads::owning_thread_from_monitor_owner(t_list, (address) mark.locker());\n+  }\n+\n+  if (LockingMode == LM_LIGHTWEIGHT && mark.is_fast_locked()) {\n+    \/\/ fast-locked so get owner from the object.\n+    \/\/ owning_thread_from_object() may also return null here:\n+    return Threads::owning_thread_from_object(t_list, h_obj());\n@@ -981,2 +1080,2 @@\n-  \/\/ Contended case, header points to ObjectMonitor (tagged pointer)\n-  else if (mark.has_monitor()) {\n+  if (mark.has_monitor()) {\n+    \/\/ Inflated monitor so header points to ObjectMonitor (tagged pointer).\n@@ -987,6 +1086,2 @@\n-    owner = (address) monitor->owner();\n-  }\n-\n-  if (owner != nullptr) {\n-    \/\/ owning_thread_from_monitor_owner() may also return null here\n-    return Threads::owning_thread_from_monitor_owner(t_list, owner);\n+    \/\/ owning_thread_from_monitor() may also return null here:\n+    return Threads::owning_thread_from_monitor(t_list, monitor);\n@@ -1006,1 +1101,1 @@\n-\/\/ ObjectMonitors where owner is set to a stack lock address in thread.\n+\/\/ ObjectMonitors where owner is set to a stack-lock address in thread.\n@@ -1016,1 +1111,1 @@\n-      \/\/ is set to a stack lock address in the target thread.\n+      \/\/ is set to a stack-lock address in the target thread.\n@@ -1042,1 +1137,1 @@\n-    \/\/ Owner set to a stack lock address in thread should never be seen here:\n+    \/\/ Owner set to a stack-lock address in thread should never be seen here:\n@@ -1226,4 +1321,11 @@\n-    \/\/ *  Inflated     - just return\n-    \/\/ *  Stack-locked - coerce it to inflated\n-    \/\/ *  INFLATING    - busy wait for conversion to complete\n-    \/\/ *  Neutral      - aggressively inflate the object.\n+    \/\/ *  inflated     - Just return if using stack-locking.\n+    \/\/                   If using fast-locking and the ObjectMonitor owner\n+    \/\/                   is anonymous and the current thread owns the\n+    \/\/                   object lock, then we make the current thread the\n+    \/\/                   ObjectMonitor owner and remove the lock from the\n+    \/\/                   current thread's lock stack.\n+    \/\/ *  fast-locked  - Coerce it to inflated from fast-locked.\n+    \/\/ *  stack-locked - Coerce it to inflated from stack-locked.\n+    \/\/ *  INFLATING    - Busy wait for conversion from stack-locked to\n+    \/\/                   inflated.\n+    \/\/ *  neutral      - Aggressively inflate the object.\n@@ -1236,0 +1338,4 @@\n+      if (LockingMode == LM_LIGHTWEIGHT && inf->is_owner_anonymous() && is_lock_owned(current, object)) {\n+        inf->set_owner_from_anonymous(current);\n+        JavaThread::cast(current)->lock_stack().remove(object);\n+      }\n@@ -1239,9 +1345,64 @@\n-    \/\/ CASE: inflation in progress - inflating over a stack-lock.\n-    \/\/ Some other thread is converting from stack-locked to inflated.\n-    \/\/ Only that thread can complete inflation -- other threads must wait.\n-    \/\/ The INFLATING value is transient.\n-    \/\/ Currently, we spin\/yield\/park and poll the markword, waiting for inflation to finish.\n-    \/\/ We could always eliminate polling by parking the thread on some auxiliary list.\n-    if (mark == markWord::INFLATING()) {\n-      read_stable_mark(object);\n-      continue;\n+    if (LockingMode != LM_LIGHTWEIGHT) {\n+      \/\/ New lightweight locking does not use INFLATING.\n+      \/\/ CASE: inflation in progress - inflating over a stack-lock.\n+      \/\/ Some other thread is converting from stack-locked to inflated.\n+      \/\/ Only that thread can complete inflation -- other threads must wait.\n+      \/\/ The INFLATING value is transient.\n+      \/\/ Currently, we spin\/yield\/park and poll the markword, waiting for inflation to finish.\n+      \/\/ We could always eliminate polling by parking the thread on some auxiliary list.\n+      if (mark == markWord::INFLATING()) {\n+        read_stable_mark(object);\n+        continue;\n+      }\n+    }\n+\n+    \/\/ CASE: fast-locked\n+    \/\/ Could be fast-locked either by current or by some other thread.\n+    \/\/\n+    \/\/ Note that we allocate the ObjectMonitor speculatively, _before_\n+    \/\/ attempting to set the object's mark to the new ObjectMonitor. If\n+    \/\/ this thread owns the monitor, then we set the ObjectMonitor's\n+    \/\/ owner to this thread. Otherwise, we set the ObjectMonitor's owner\n+    \/\/ to anonymous. If we lose the race to set the object's mark to the\n+    \/\/ new ObjectMonitor, then we just delete it and loop around again.\n+    \/\/\n+    LogStreamHandle(Trace, monitorinflation) lsh;\n+    if (LockingMode == LM_LIGHTWEIGHT && mark.is_fast_locked()) {\n+      ObjectMonitor* monitor = new ObjectMonitor(object);\n+      monitor->set_header(mark.set_unlocked());\n+      bool own = is_lock_owned(current, object);\n+      if (own) {\n+        \/\/ Owned by us.\n+        monitor->set_owner_from(nullptr, current);\n+      } else {\n+        \/\/ Owned by somebody else.\n+        monitor->set_owner_anonymous();\n+      }\n+      markWord monitor_mark = markWord::encode(monitor);\n+      markWord old_mark = object->cas_set_mark(monitor_mark, mark);\n+      if (old_mark == mark) {\n+        \/\/ Success! Return inflated monitor.\n+        if (own) {\n+          JavaThread::cast(current)->lock_stack().remove(object);\n+        }\n+        \/\/ Once the ObjectMonitor is configured and object is associated\n+        \/\/ with the ObjectMonitor, it is safe to allow async deflation:\n+        _in_use_list.add(monitor);\n+\n+        \/\/ Hopefully the performance counters are allocated on distinct\n+        \/\/ cache lines to avoid false sharing on MP systems ...\n+        OM_PERFDATA_OP(Inflations, inc());\n+        if (log_is_enabled(Trace, monitorinflation)) {\n+          ResourceMark rm(current);\n+          lsh.print_cr(\"inflate(has_locker): object=\" INTPTR_FORMAT \", mark=\"\n+                       INTPTR_FORMAT \", type='%s'\", p2i(object),\n+                       object->mark().value(), object->klass()->external_name());\n+        }\n+        if (event.should_commit()) {\n+          post_monitor_inflate_event(&event, object, cause);\n+        }\n+        return monitor;\n+      } else {\n+        delete monitor;\n+        continue;  \/\/ Interference -- just retry\n+      }\n@@ -1251,1 +1412,1 @@\n-    \/\/ Could be stack-locked either by this thread or by some other thread.\n+    \/\/ Could be stack-locked either by current or by some other thread.\n@@ -1258,5 +1419,5 @@\n-    \/\/ the odds of inflation contention.\n-\n-    LogStreamHandle(Trace, monitorinflation) lsh;\n-\n-    if (mark.has_locker()) {\n+    \/\/ the odds of inflation contention. If we lose the race to set INFLATING,\n+    \/\/ then we just delete the ObjectMonitor and loop around again.\n+    \/\/\n+    if (LockingMode == LM_LEGACY && mark.has_locker()) {\n+      assert(LockingMode != LM_LIGHTWEIGHT, \"cannot happen with new lightweight locking\");\n@@ -1424,2 +1585,2 @@\n-\/\/ is set to a stack lock address are NOT associated with the JavaThread\n-\/\/ that holds that stack lock. All of the current consumers of\n+\/\/ is set to a stack-lock address are NOT associated with the JavaThread\n+\/\/ that holds that stack-lock. All of the current consumers of\n@@ -1427,1 +1588,1 @@\n-\/\/ those do not have the owner set to a stack lock address.\n+\/\/ those do not have the owner set to a stack-lock address.\n@@ -1444,1 +1605,1 @@\n-      \/\/ not include when owner is set to a stack lock address in thread.\n+      \/\/ not include when owner is set to a stack-lock address in thread.\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.cpp","additions":284,"deletions":123,"binary":false,"changes":407,"status":"modified"},{"patch":"@@ -527,0 +527,1 @@\n+  assert(LockingMode != LM_LIGHTWEIGHT, \"should not be called with new lightweight locking\");\n","filename":"src\/hotspot\/share\/runtime\/thread.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -72,0 +72,1 @@\n+#include \"runtime\/lockStack.inline.hpp\"\n@@ -1180,0 +1181,1 @@\n+  assert(LockingMode != LM_LIGHTWEIGHT, \"Not with new lightweight locking\");\n@@ -1191,1 +1193,1 @@\n-  if (UseHeavyMonitors) return nullptr;\n+  if (LockingMode == LM_MONITOR) return nullptr;\n@@ -1209,0 +1211,10 @@\n+JavaThread* Threads::owning_thread_from_object(ThreadsList * t_list, oop obj) {\n+  assert(LockingMode == LM_LIGHTWEIGHT, \"Only with new lightweight locking\");\n+  for (JavaThread* q : *t_list) {\n+    if (q->lock_stack().contains(obj)) {\n+      return q;\n+    }\n+  }\n+  return nullptr;\n+}\n+\n@@ -1210,2 +1222,12 @@\n-  address owner = (address)monitor->owner();\n-  return owning_thread_from_monitor_owner(t_list, owner);\n+  if (LockingMode == LM_LIGHTWEIGHT) {\n+    if (monitor->is_owner_anonymous()) {\n+      return owning_thread_from_object(t_list, monitor->object());\n+    } else {\n+      Thread* owner = reinterpret_cast<Thread*>(monitor->owner());\n+      assert(owner == nullptr || owner->is_Java_thread(), \"only JavaThreads own monitors\");\n+      return reinterpret_cast<JavaThread*>(owner);\n+    }\n+  } else {\n+    address owner = (address)monitor->owner();\n+    return owning_thread_from_monitor_owner(t_list, owner);\n+  }\n","filename":"src\/hotspot\/share\/runtime\/threads.cpp","additions":25,"deletions":3,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -141,0 +141,1 @@\n+  static JavaThread* owning_thread_from_object(ThreadsList* t_list, oop obj);\n","filename":"src\/hotspot\/share\/runtime\/threads.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -706,0 +706,3 @@\n+  nonstatic_field(JavaThread,                  _lock_stack,                                   LockStack)                             \\\n+  nonstatic_field(LockStack,                   _top,                                          uint32_t)                              \\\n+  nonstatic_field(LockStack,                   _base[0],                                      oop)                                   \\\n@@ -1321,0 +1324,1 @@\n+  declare_toplevel_type(LockStack)                                        \\\n@@ -2618,2 +2622,4 @@\n-  declare_constant(InvocationCounter::count_shift)\n-\n+  declare_constant(InvocationCounter::count_shift)                        \\\n+                                                                          \\\n+  \/* ObjectMonitor constants *\/                                           \\\n+  declare_constant(ObjectMonitor::ANONYMOUS_OWNER)                        \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":8,"deletions":2,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -1045,0 +1045,9 @@\n+enum LockingMode {\n+  \/\/ Use only heavy monitors for locking\n+  LM_MONITOR     = 0,\n+  \/\/ Legacy stack-locking, with monitors as 2nd tier\n+  LM_LEGACY      = 1,\n+  \/\/ New lightweight locking, with monitors as 2nd tier\n+  LM_LIGHTWEIGHT = 2\n+};\n+\n","filename":"src\/hotspot\/share\/utilities\/globalDefinitions.hpp","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -47,0 +47,2 @@\n+  private static long          lockStackTopOffset;\n+  private static long          lockStackBaseOffset;\n@@ -56,0 +58,1 @@\n+  private static long oopPtrSize;\n@@ -88,0 +91,1 @@\n+    Type typeLockStack = db.lookupType(\"LockStack\");\n@@ -101,0 +105,4 @@\n+    lockStackTopOffset = type.getField(\"_lock_stack\").getOffset() + typeLockStack.getField(\"_top\").getOffset();\n+    lockStackBaseOffset = type.getField(\"_lock_stack\").getOffset() + typeLockStack.getField(\"_base[0]\").getOffset();\n+    oopPtrSize = VM.getVM().getAddressSize();\n+\n@@ -397,0 +405,17 @@\n+  public boolean isLockOwned(OopHandle obj) {\n+    long current = lockStackBaseOffset;\n+    long end = addr.getJIntAt(lockStackTopOffset);\n+    if (Assert.ASSERTS_ENABLED) {\n+      Assert.that(current <= end, \"current stack offset must be above base offset\");\n+    }\n+\n+    while (current < end) {\n+      Address oop = addr.getAddressAt(current);\n+      if (oop.equals(obj)) {\n+        return true;\n+      }\n+      current += oopPtrSize;\n+    }\n+    return false;\n+  }\n+\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/runtime\/JavaThread.java","additions":25,"deletions":0,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -85,0 +85,4 @@\n+          \/\/ Owned anonymously means that we are not the owner of\n+          \/\/ the monitor and must be waiting for the owner to\n+          \/\/ exit it.\n+          mark.monitor().isOwnedAnonymous() ||\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/runtime\/JavaVFrame.java","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -47,0 +47,1 @@\n+\n@@ -58,0 +59,2 @@\n+\n+    ANONYMOUS_OWNER = db.lookupLongConstant(\"ObjectMonitor::ANONYMOUS_OWNER\").longValue();\n@@ -82,0 +85,4 @@\n+  public boolean isOwnedAnonymous() {\n+    return addr.getAddressAt(ownerFieldOffset).asLongValue() == ANONYMOUS_OWNER;\n+  }\n+\n@@ -117,0 +124,2 @@\n+  private static long          ANONYMOUS_OWNER;\n+\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/runtime\/ObjectMonitor.java","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -214,0 +214,1 @@\n+        assert(VM.getVM().getCommandLineFlag(\"LockingMode\").getInt() != 2 \/* LM_LIGHTWEIGHT *\/);\n@@ -231,1 +232,18 @@\n-        return owningThreadFromMonitor(monitor.owner());\n+        if (VM.getVM().getCommandLineFlag(\"LockingMode\").getInt() == 2 \/* LM_LIGHTWEIGHT *\/) {\n+            if (monitor.isOwnedAnonymous()) {\n+                OopHandle object = monitor.object();\n+                for (int i = 0; i < getNumberOfThreads(); i++) {\n+                    JavaThread thread = getJavaThreadAt(i);\n+                    if (thread.isLockOwned(object)) {\n+                        return thread;\n+                     }\n+                }\n+                throw new InternalError(\"We should have found a thread that owns the anonymous lock\");\n+            }\n+            \/\/ Owner can only be threads at this point.\n+            Address o = monitor.owner();\n+            if (o == null) return null;\n+            return new JavaThread(o);\n+        } else {\n+            return owningThreadFromMonitor(monitor.owner());\n+        }\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/runtime\/Threads.java","additions":19,"deletions":1,"binary":false,"changes":20,"status":"modified"}]}