{"files":[{"patch":"@@ -3843,1 +3843,1 @@\n-      if (UseFastLocking) {\n+      if (LockingMode == 2) {\n@@ -3846,1 +3846,1 @@\n-      } else {\n+      } else if (LockingMode == 1) {\n@@ -3892,1 +3892,1 @@\n-    if (!UseFastLocking) {\n+    if (LockingMode != 2) {\n@@ -3932,1 +3932,1 @@\n-    if (!UseHeavyMonitors && !UseFastLocking) {\n+    if (LockingMode == 1) {\n@@ -3946,1 +3946,1 @@\n-      if (UseFastLocking) {\n+      if (LockingMode == 2) {\n@@ -3949,1 +3949,1 @@\n-      } else {\n+      } else if (LockingMode == 1) {\n@@ -3970,1 +3970,1 @@\n-    if (UseFastLocking) {\n+    if (LockingMode == 2) {\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":7,"deletions":7,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -85,1 +85,1 @@\n-  if (UseFastLocking) {\n+  if (LockingMode == 2) {\n@@ -87,1 +87,1 @@\n-  } else {\n+  } else if (LockingMode == 1) {\n@@ -134,1 +134,1 @@\n-  if (!UseFastLocking) {\n+  if (LockingMode != 2) {\n@@ -146,1 +146,1 @@\n-  if (UseFastLocking) {\n+  if (LockingMode == 2) {\n@@ -153,1 +153,1 @@\n-  } else {\n+  } else if (LockingMode == 1) {\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_MacroAssembler_aarch64.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -761,1 +761,1 @@\n-    if (UseFastLocking) {\n+    if (LockingMode == 2) {\n@@ -765,1 +765,1 @@\n-    } else {\n+    } else if (LockingMode == 1) {\n@@ -819,1 +819,1 @@\n-    if (UseFastLocking) {\n+    if (LockingMode == 2) {\n@@ -864,1 +864,1 @@\n-    if (!UseFastLocking) {\n+    if (LockingMode != 2) {\n@@ -876,1 +876,1 @@\n-    if (UseFastLocking) {\n+    if (LockingMode == 2) {\n@@ -897,1 +897,1 @@\n-    } else {\n+    } else if (LockingMode == 1) {\n","filename":"src\/hotspot\/cpu\/aarch64\/interp_masm_aarch64.cpp","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -6223,1 +6223,1 @@\n-  assert(UseFastLocking, \"only used with fast-locking\");\n+  assert(LockingMode == 2, \"only used with new lightweight locking\");\n@@ -6255,1 +6255,1 @@\n-  assert(UseFastLocking, \"only used with fast-locking\");\n+  assert(LockingMode == 2, \"only used with new lightweight locking\");\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1782,1 +1782,1 @@\n-      if (UseFastLocking) {\n+      if (LockingMode == 2) {\n@@ -1785,1 +1785,1 @@\n-      } else {\n+      } else if (LockingMode == 1) {\n@@ -1925,1 +1925,1 @@\n-    if (!UseHeavyMonitors && !UseFastLocking) {\n+    if (LockingMode == 1) {\n@@ -1941,1 +1941,1 @@\n-      if (UseFastLocking) {\n+      if (LockingMode == 2) {\n@@ -1945,1 +1945,1 @@\n-      } else {\n+      } else if (LockingMode == 1) {\n","filename":"src\/hotspot\/cpu\/aarch64\/sharedRuntime_aarch64.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"logging\/log.hpp\"\n@@ -202,0 +203,1 @@\n+  \/\/ save object being locked into the BasicObjectLock\n@@ -215,2 +217,2 @@\n-  \/\/ On MP platforms the next load could return a 'stale' value if the memory location has been modified by another thread.\n-  \/\/ That would be acceptable as ether CAS or slow case path is taken in that case.\n+  if (LockingMode == 2) {\n+    log_trace(fastlock2)(\"C1_MacroAssembler::lock fast\");\n@@ -218,2 +220,3 @@\n-  \/\/ Must be the first instruction here, because implicit null check relies on it\n-  ldr(hdr, Address(obj, oopDesc::mark_offset_in_bytes()));\n+    Register t1 = disp_hdr; \/\/ Needs saving, probably\n+    Register t2 = hdr;      \/\/ blow\n+    Register t3 = Rtemp;    \/\/ blow\n@@ -221,2 +224,2 @@\n-  tst(hdr, markWord::unlocked_value);\n-  b(fast_lock, ne);\n+    fast_lock_2(obj \/* obj *\/, t1, t2, t3, 1 \/* savemask - save t1 *\/, slow_case);\n+    \/\/ Success: fall through\n@@ -224,14 +227,1 @@\n-  \/\/ Check for recursive locking\n-  \/\/ See comments in InterpreterMacroAssembler::lock_object for\n-  \/\/ explanations on the fast recursive locking check.\n-  \/\/ -1- test low 2 bits\n-  movs(tmp2, AsmOperand(hdr, lsl, 30));\n-  \/\/ -2- test (hdr - SP) if the low two bits are 0\n-  sub(tmp2, hdr, SP, eq);\n-  movs(tmp2, AsmOperand(tmp2, lsr, exact_log2(os::vm_page_size())), eq);\n-  \/\/ If still 'eq' then recursive locking OK\n-  \/\/ set to zero if recursive lock, set to non zero otherwise (see discussion in JDK-8267042)\n-  str(tmp2, Address(disp_hdr, mark_offset));\n-  b(fast_lock_done, eq);\n-  \/\/ else need slow case\n-  b(slow_case);\n+  } else if (LockingMode == 1) {\n@@ -239,0 +229,2 @@\n+    \/\/ On MP platforms the next load could return a 'stale' value if the memory location has been modified by another thread.\n+    \/\/ That would be acceptable as ether CAS or slow case path is taken in that case.\n@@ -240,3 +232,2 @@\n-  bind(fast_lock);\n-  \/\/ Save previous object header in BasicLock structure and update the header\n-  str(hdr, Address(disp_hdr, mark_offset));\n+    \/\/ Must be the first instruction here, because implicit null check relies on it\n+    ldr(hdr, Address(obj, oopDesc::mark_offset_in_bytes()));\n@@ -244,1 +235,2 @@\n-  cas_for_lock_acquire(hdr, disp_hdr, obj, tmp2, slow_case);\n+    tst(hdr, markWord::unlocked_value);\n+    b(fast_lock, ne);\n@@ -246,1 +238,24 @@\n-  bind(fast_lock_done);\n+    \/\/ Check for recursive locking\n+    \/\/ See comments in InterpreterMacroAssembler::lock_object for\n+    \/\/ explanations on the fast recursive locking check.\n+    \/\/ -1- test low 2 bits\n+    movs(tmp2, AsmOperand(hdr, lsl, 30));\n+    \/\/ -2- test (hdr - SP) if the low two bits are 0\n+    sub(tmp2, hdr, SP, eq);\n+    movs(tmp2, AsmOperand(tmp2, lsr, exact_log2(os::vm_page_size())), eq);\n+    \/\/ If still 'eq' then recursive locking OK\n+    \/\/ set to zero if recursive lock, set to non zero otherwise (see discussion in JDK-8267042)\n+    str(tmp2, Address(disp_hdr, mark_offset));\n+    b(fast_lock_done, eq);\n+    \/\/ else need slow case\n+    b(slow_case);\n+\n+\n+    bind(fast_lock);\n+    \/\/ Save previous object header in BasicLock structure and update the header\n+    str(hdr, Address(disp_hdr, mark_offset));\n+\n+    cas_for_lock_acquire(hdr, disp_hdr, obj, tmp2, slow_case);\n+\n+    bind(fast_lock_done);\n+  }\n@@ -264,4 +279,2 @@\n-  \/\/ Load displaced header and object from the lock\n-  ldr(hdr, Address(disp_hdr, mark_offset));\n-  \/\/ If hdr is null, we've got recursive locking and there's nothing more to do\n-  cbz(hdr, done);\n+  if (LockingMode == 2) {\n+    log_trace(fastlock2)(\"C1_MacroAssembler::unlock fast\");\n@@ -269,2 +282,1 @@\n-  \/\/ load object\n-  ldr(obj, Address(disp_hdr, obj_offset));\n+    ldr(obj, Address(disp_hdr, obj_offset));\n@@ -272,2 +284,3 @@\n-  \/\/ Restore the object header\n-  cas_for_lock_release(disp_hdr, hdr, obj, tmp2, slow_case);\n+    Register t1 = disp_hdr; \/\/ Needs saving, probably\n+    Register t2 = hdr;      \/\/ blow\n+    Register t3 = Rtemp;    \/\/ blow\n@@ -275,0 +288,17 @@\n+    fast_unlock_2(obj \/* object *\/, t1, t2, t3, 1 \/* savemask (save t1) *\/,\n+                    slow_case);\n+    \/\/ Success: Fall through\n+\n+  } else if (LockingMode == 1) {\n+\n+    \/\/ Load displaced header and object from the lock\n+    ldr(hdr, Address(disp_hdr, mark_offset));\n+    \/\/ If hdr is null, we've got recursive locking and there's nothing more to do\n+    cbz(hdr, done);\n+\n+    \/\/ load object\n+    ldr(obj, Address(disp_hdr, obj_offset));\n+\n+    \/\/ Restore the object header\n+    cas_for_lock_release(disp_hdr, hdr, obj, tmp2, slow_case);\n+  }\n@@ -278,1 +308,0 @@\n-\n","filename":"src\/hotspot\/cpu\/arm\/c1_MacroAssembler_arm.cpp","additions":63,"deletions":34,"binary":false,"changes":97,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"logging\/log.hpp\"\n@@ -83,7 +84,1 @@\n-\n-  Register Rmark      = Rscratch2;\n-\n-  assert(Roop != Rscratch, \"\");\n-  assert(Roop != Rmark, \"\");\n-  assert(Rbox != Rscratch, \"\");\n-  assert(Rbox != Rmark, \"\");\n+  assert_different_registers(Roop, Rbox, Rscratch, Rscratch2);\n@@ -100,23 +95,38 @@\n-  ldr(Rmark, Address(Roop, oopDesc::mark_offset_in_bytes()));\n-  tst(Rmark, markWord::unlocked_value);\n-  b(fast_lock, ne);\n-\n-  \/\/ Check for recursive lock\n-  \/\/ See comments in InterpreterMacroAssembler::lock_object for\n-  \/\/ explanations on the fast recursive locking check.\n-  \/\/ -1- test low 2 bits\n-  movs(Rscratch, AsmOperand(Rmark, lsl, 30));\n-  \/\/ -2- test (hdr - SP) if the low two bits are 0\n-  sub(Rscratch, Rmark, SP, eq);\n-  movs(Rscratch, AsmOperand(Rscratch, lsr, exact_log2(os::vm_page_size())), eq);\n-  \/\/ If still 'eq' then recursive locking OK\n-  \/\/ set to zero if recursive lock, set to non zero otherwise (see discussion in JDK-8153107)\n-  str(Rscratch, Address(Rbox, BasicLock::displaced_header_offset_in_bytes()));\n-  b(done);\n-\n-  bind(fast_lock);\n-  str(Rmark, Address(Rbox, BasicLock::displaced_header_offset_in_bytes()));\n-\n-  bool allow_fallthrough_on_failure = true;\n-  bool one_shot = true;\n-  cas_for_lock_acquire(Rmark, Rbox, Roop, Rscratch, done, allow_fallthrough_on_failure, one_shot);\n+  if (LockingMode == 2) {\n+    log_trace(fastlock2)(\"C2_MacroAssembler::lock fast\");\n+\n+    fast_lock_2(Roop \/* obj *\/, Rbox \/* t1 *\/, Rscratch \/* t2 *\/, Rscratch2 \/* t3 *\/,\n+                  1 \/* savemask (save t1) *\/,\n+                  done);\n+\n+    \/\/ Success: set Z\n+    cmp(Roop, Roop);\n+\n+  } else if (LockingMode == 1) {\n+\n+    Register Rmark      = Rscratch2;\n+\n+    ldr(Rmark, Address(Roop, oopDesc::mark_offset_in_bytes()));\n+    tst(Rmark, markWord::unlocked_value);\n+    b(fast_lock, ne);\n+\n+    \/\/ Check for recursive lock\n+    \/\/ See comments in InterpreterMacroAssembler::lock_object for\n+    \/\/ explanations on the fast recursive locking check.\n+    \/\/ -1- test low 2 bits\n+    movs(Rscratch, AsmOperand(Rmark, lsl, 30));\n+    \/\/ -2- test (hdr - SP) if the low two bits are 0\n+    sub(Rscratch, Rmark, SP, eq);\n+    movs(Rscratch, AsmOperand(Rscratch, lsr, exact_log2(os::vm_page_size())), eq);\n+    \/\/ If still 'eq' then recursive locking OK\n+    \/\/ set to zero if recursive lock, set to non zero otherwise (see discussion in JDK-8153107)\n+    str(Rscratch, Address(Rbox, BasicLock::displaced_header_offset_in_bytes()));\n+    b(done);\n+\n+    bind(fast_lock);\n+    str(Rmark, Address(Rbox, BasicLock::displaced_header_offset_in_bytes()));\n+\n+    bool allow_fallthrough_on_failure = true;\n+    bool one_shot = true;\n+    cas_for_lock_acquire(Rmark, Rbox, Roop, Rscratch, done, allow_fallthrough_on_failure, one_shot);\n+  }\n@@ -133,0 +143,1 @@\n+  assert_different_registers(Roop, Rbox, Rscratch, Rscratch2);\n@@ -134,1 +145,1 @@\n-  Register Rmark      = Rscratch2;\n+  Label done;\n@@ -136,4 +147,2 @@\n-  assert(Roop != Rscratch, \"\");\n-  assert(Roop != Rmark, \"\");\n-  assert(Rbox != Rscratch, \"\");\n-  assert(Rbox != Rmark, \"\");\n+  if (LockingMode == 2) {\n+    log_trace(fastlock2)(\"C2_MacroAssembler::unlock fast\");\n@@ -141,1 +150,0 @@\n-  Label done;\n@@ -143,4 +151,6 @@\n-  ldr(Rmark, Address(Rbox, BasicLock::displaced_header_offset_in_bytes()));\n-  \/\/ If hdr is null, we've got recursive locking and there's nothing more to do\n-  cmp(Rmark, 0);\n-  b(done, eq);\n+    fast_unlock_2(Roop \/* obj *\/, Rbox \/* t1 *\/, Rscratch \/* t2 *\/, Rscratch2 \/* t3 *\/,\n+                    1 \/* savemask (save t1) *\/,\n+                    done);\n+\n+    cmp(Roop, Roop); \/\/ Success: Set Z\n+    \/\/ Fall through\n@@ -148,4 +158,1 @@\n-  \/\/ Restore the object header\n-  bool allow_fallthrough_on_failure = true;\n-  bool one_shot = true;\n-  cas_for_lock_release(Rmark, Rbox, Roop, Rscratch, done, allow_fallthrough_on_failure, one_shot);\n+  } else if (LockingMode == 1) {\n@@ -153,0 +160,13 @@\n+    Register Rmark      = Rscratch2;\n+\n+    \/\/ Find the lock address and load the displaced header from the stack.\n+    ldr(Rmark, Address(Rbox, BasicLock::displaced_header_offset_in_bytes()));\n+    \/\/ If hdr is null, we've got recursive locking and there's nothing more to do\n+    cmp(Rmark, 0);\n+    b(done, eq);\n+\n+    \/\/ Restore the object header\n+    bool allow_fallthrough_on_failure = true;\n+    bool one_shot = true;\n+    cas_for_lock_release(Rmark, Rbox, Roop, Rscratch, done, allow_fallthrough_on_failure, one_shot);\n+  }\n@@ -154,1 +174,0 @@\n-}\n@@ -156,0 +175,4 @@\n+  \/\/ At this point flags are set as follows:\n+  \/\/  EQ -> Success\n+  \/\/  NE -> Failure, branch to slow path\n+}\n","filename":"src\/hotspot\/cpu\/arm\/c2_MacroAssembler_arm.cpp","additions":68,"deletions":45,"binary":false,"changes":113,"status":"modified"},{"patch":"@@ -892,62 +892,73 @@\n-    \/\/ On MP platforms the next load could return a 'stale' value if the memory location has been modified by another thread.\n-    \/\/ That would be acceptable as ether CAS or slow case path is taken in that case.\n-    \/\/ Exception to that is if the object is locked by the calling thread, then the recursive test will pass (guaranteed as\n-    \/\/ loads are satisfied from a store queue if performed on the same processor).\n-\n-    assert(oopDesc::mark_offset_in_bytes() == 0, \"must be\");\n-    ldr(Rmark, Address(Robj, oopDesc::mark_offset_in_bytes()));\n-\n-    \/\/ Test if object is already locked\n-    tst(Rmark, markWord::unlocked_value);\n-    b(already_locked, eq);\n-\n-    \/\/ Save old object->mark() into BasicLock's displaced header\n-    str(Rmark, Address(Rlock, mark_offset));\n-\n-    cas_for_lock_acquire(Rmark, Rlock, Robj, Rtemp, slow_case);\n-\n-    b(done);\n-\n-    \/\/ If we got here that means the object is locked by ether calling thread or another thread.\n-    bind(already_locked);\n-    \/\/ Handling of locked objects: recursive locks and slow case.\n-\n-    \/\/ Fast check for recursive lock.\n-    \/\/\n-    \/\/ Can apply the optimization only if this is a stack lock\n-    \/\/ allocated in this thread. For efficiency, we can focus on\n-    \/\/ recently allocated stack locks (instead of reading the stack\n-    \/\/ base and checking whether 'mark' points inside the current\n-    \/\/ thread stack):\n-    \/\/  1) (mark & 3) == 0\n-    \/\/  2) SP <= mark < SP + os::pagesize()\n-    \/\/\n-    \/\/ Warning: SP + os::pagesize can overflow the stack base. We must\n-    \/\/ neither apply the optimization for an inflated lock allocated\n-    \/\/ just above the thread stack (this is why condition 1 matters)\n-    \/\/ nor apply the optimization if the stack lock is inside the stack\n-    \/\/ of another thread. The latter is avoided even in case of overflow\n-    \/\/ because we have guard pages at the end of all stacks. Hence, if\n-    \/\/ we go over the stack base and hit the stack of another thread,\n-    \/\/ this should not be in a writeable area that could contain a\n-    \/\/ stack lock allocated by that thread. As a consequence, a stack\n-    \/\/ lock less than page size away from SP is guaranteed to be\n-    \/\/ owned by the current thread.\n-    \/\/\n-    \/\/ Note: assuming SP is aligned, we can check the low bits of\n-    \/\/ (mark-SP) instead of the low bits of mark. In that case,\n-    \/\/ assuming page size is a power of 2, we can merge the two\n-    \/\/ conditions into a single test:\n-    \/\/ => ((mark - SP) & (3 - os::pagesize())) == 0\n-\n-    \/\/ (3 - os::pagesize()) cannot be encoded as an ARM immediate operand.\n-    \/\/ Check independently the low bits and the distance to SP.\n-    \/\/ -1- test low 2 bits\n-    movs(R0, AsmOperand(Rmark, lsl, 30));\n-    \/\/ -2- test (mark - SP) if the low two bits are 0\n-    sub(R0, Rmark, SP, eq);\n-    movs(R0, AsmOperand(R0, lsr, exact_log2(os::vm_page_size())), eq);\n-    \/\/ If still 'eq' then recursive locking OK: store 0 into lock record\n-    str(R0, Address(Rlock, mark_offset), eq);\n-\n-    b(done, eq);\n+    if (LockingMode == 2) {\n+\n+      log_trace(fastlock2)(\"InterpreterMacroAssembler lock fast\");\n+\n+      fast_lock_2(Robj, R0 \/* t1 *\/, Rmark \/* t2 *\/, Rtemp \/* t3 *\/, 0 \/* savemask *\/, slow_case);\n+\n+      b(done);\n+\n+    } else if (LockingMode == 1) {\n+\n+      \/\/ On MP platforms the next load could return a 'stale' value if the memory location has been modified by another thread.\n+      \/\/ That would be acceptable as ether CAS or slow case path is taken in that case.\n+      \/\/ Exception to that is if the object is locked by the calling thread, then the recursive test will pass (guaranteed as\n+      \/\/ loads are satisfied from a store queue if performed on the same processor).\n+\n+      assert(oopDesc::mark_offset_in_bytes() == 0, \"must be\");\n+      ldr(Rmark, Address(Robj, oopDesc::mark_offset_in_bytes()));\n+\n+      \/\/ Test if object is already locked\n+      tst(Rmark, markWord::unlocked_value);\n+      b(already_locked, eq);\n+\n+      \/\/ Save old object->mark() into BasicLock's displaced header\n+      str(Rmark, Address(Rlock, mark_offset));\n+\n+      cas_for_lock_acquire(Rmark, Rlock, Robj, Rtemp, slow_case);\n+\n+      b(done);\n+\n+      \/\/ If we got here that means the object is locked by ether calling thread or another thread.\n+      bind(already_locked);\n+      \/\/ Handling of locked objects: recursive locks and slow case.\n+\n+      \/\/ Fast check for recursive lock.\n+      \/\/\n+      \/\/ Can apply the optimization only if this is a stack lock\n+      \/\/ allocated in this thread. For efficiency, we can focus on\n+      \/\/ recently allocated stack locks (instead of reading the stack\n+      \/\/ base and checking whether 'mark' points inside the current\n+      \/\/ thread stack):\n+      \/\/  1) (mark & 3) == 0\n+      \/\/  2) SP <= mark < SP + os::pagesize()\n+      \/\/\n+      \/\/ Warning: SP + os::pagesize can overflow the stack base. We must\n+      \/\/ neither apply the optimization for an inflated lock allocated\n+      \/\/ just above the thread stack (this is why condition 1 matters)\n+      \/\/ nor apply the optimization if the stack lock is inside the stack\n+      \/\/ of another thread. The latter is avoided even in case of overflow\n+      \/\/ because we have guard pages at the end of all stacks. Hence, if\n+      \/\/ we go over the stack base and hit the stack of another thread,\n+      \/\/ this should not be in a writeable area that could contain a\n+      \/\/ stack lock allocated by that thread. As a consequence, a stack\n+      \/\/ lock less than page size away from SP is guaranteed to be\n+      \/\/ owned by the current thread.\n+      \/\/\n+      \/\/ Note: assuming SP is aligned, we can check the low bits of\n+      \/\/ (mark-SP) instead of the low bits of mark. In that case,\n+      \/\/ assuming page size is a power of 2, we can merge the two\n+      \/\/ conditions into a single test:\n+      \/\/ => ((mark - SP) & (3 - os::pagesize())) == 0\n+\n+      \/\/ (3 - os::pagesize()) cannot be encoded as an ARM immediate operand.\n+      \/\/ Check independently the low bits and the distance to SP.\n+      \/\/ -1- test low 2 bits\n+      movs(R0, AsmOperand(Rmark, lsl, 30));\n+      \/\/ -2- test (mark - SP) if the low two bits are 0\n+      sub(R0, Rmark, SP, eq);\n+      movs(R0, AsmOperand(R0, lsr, exact_log2(os::vm_page_size())), eq);\n+      \/\/ If still 'eq' then recursive locking OK: store 0 into lock record\n+      str(R0, Address(Rlock, mark_offset), eq);\n+\n+      b(done, eq);\n+    }\n@@ -958,2 +969,9 @@\n-    call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter), Rlock);\n-\n+    if (LockingMode == 2) {\n+      \/\/ Pass oop, not lock, in fast lock case. call_VM wants R1 though.\n+      push(R1);\n+      mov(R1, Robj);\n+      call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter_obj), R1);\n+      pop(R1);\n+    } else {\n+      call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter), Rlock);\n+    }\n@@ -964,1 +982,0 @@\n-\n@@ -994,2 +1011,1 @@\n-    \/\/ Load the old header from BasicLock structure\n-    ldr(Rmark, Address(Rlock, mark_offset));\n+    if (LockingMode == 2) {\n@@ -997,2 +1013,9 @@\n-    \/\/ Test for recursion (zero mark in BasicLock)\n-    cbz(Rmark, done);\n+      log_trace(fastlock2)(\"InterpreterMacroAssembler unlock fast\");\n+\n+      \/\/ Check for non-symmetric locking. This is allowed by the spec and the interpreter\n+      \/\/ must handle it.\n+      ldr(Rtemp, Address(Rthread, JavaThread::lock_stack_top_offset()));\n+      sub(Rtemp, Rtemp, oopSize);\n+      ldr(Rtemp, Address(Rthread, Rtemp));\n+      cmpoop(Rtemp, Robj);\n+      b(slow_case, ne);\n@@ -1000,1 +1023,3 @@\n-    bool allow_fallthrough_on_failure = true;\n+      fast_unlock_2(Robj \/* obj *\/, Rlock \/* t1 *\/, Rmark \/* t2 *\/, Rtemp \/* t3 *\/,\n+                      1 \/* savemask (save t1) *\/,\n+                      slow_case);\n@@ -1002,1 +1027,1 @@\n-    cas_for_lock_release(Rlock, Rmark, Robj, Rtemp, slow_case, allow_fallthrough_on_failure);\n+      b(done);\n@@ -1004,1 +1029,1 @@\n-    b(done, eq);\n+    } else if (LockingMode == 1) {\n@@ -1006,0 +1031,13 @@\n+      \/\/ Load the old header from BasicLock structure\n+      ldr(Rmark, Address(Rlock, mark_offset));\n+\n+      \/\/ Test for recursion (zero mark in BasicLock)\n+      cbz(Rmark, done);\n+\n+      bool allow_fallthrough_on_failure = true;\n+\n+      cas_for_lock_release(Rlock, Rmark, Robj, Rtemp, slow_case, allow_fallthrough_on_failure);\n+\n+      b(done, eq);\n+\n+    }\n@@ -1016,1 +1054,0 @@\n-\n","filename":"src\/hotspot\/cpu\/arm\/interp_masm_arm.cpp","additions":110,"deletions":73,"binary":false,"changes":183,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright (c) 2023, Red Hat, Inc.\n@@ -44,0 +45,1 @@\n+#include \"runtime\/javaThread.hpp\"\n@@ -1200,0 +1202,2 @@\n+  \/\/ Here, on success, EQ is set, NE otherwise\n+\n@@ -1205,0 +1209,2 @@\n+  \/\/ Note: we preserve flags here.\n+  \/\/ Todo: Do we really need this also for the CAS fail case?\n@@ -1215,1 +1221,0 @@\n-\n@@ -1724,0 +1729,142 @@\n+#define PUSH_REG(mask, bit, Reg)      \\\n+  if (mask & ((unsigned)1 << bit)) {  \\\n+    push(Reg);                        \\\n+  }\n+\n+#define POP_REG(mask, bit, Reg, condition)   \\\n+  if (mask & ((unsigned)1 << bit)) {         \\\n+    pop(Reg, condition);                     \\\n+  }\n+\n+#define PUSH_REGS(mask, R1, R2, R3) \\\n+  PUSH_REG(mask, 0, R1)             \\\n+  PUSH_REG(mask, 1, R2)             \\\n+  PUSH_REG(mask, 2, R3)\n+\n+#define POP_REGS(mask, R1, R2, R3, condition)   \\\n+  POP_REG(mask, 0, R1, condition)               \\\n+  POP_REG(mask, 1, R2, condition)               \\\n+  POP_REG(mask, 2, R3, condition)\n+\n+#define POISON_REG(mask, bit, Reg, poison)      \\\n+  if (mask & ((unsigned)1 << bit)) {            \\\n+    mov(Reg, poison);                           \\\n+  }\n+\n+#define POISON_REGS(mask, R1, R2, R3, poison)   \\\n+  POISON_REG(mask, 0, R1, poison)               \\\n+  POISON_REG(mask, 1, R2, poison)               \\\n+  POISON_REG(mask, 2, R3, poison)\n+\n+\/\/ Attempt to fast-lock an object\n+\/\/ Registers:\n+\/\/  - obj: the object to be locked\n+\/\/  - t1, t2, t3: temp registers. If corresponding bit in savemask is set, they get saved, otherwise blown.\n+\/\/ Result:\n+\/\/  - Success: fallthrough\n+\/\/  - Error:   break to slow, Z cleared.\n+void MacroAssembler::fast_lock_2(Register obj, Register t1, Register t2, Register t3, unsigned savemask, Label& slow) {\n+  assert(LockingMode == 2, \"only used with new lightweight locking\");\n+  assert_different_registers(obj, t1, t2, t3);\n+\n+#ifdef ASSERT\n+  \/\/ Poison scratch regs\n+  POISON_REGS((~savemask), t1, t2, t3, 0x10000001);\n+#endif\n+\n+  PUSH_REGS(savemask, t1, t2, t3);\n+\n+  \/\/ Check if we would have space on lock-stack for the object.\n+  ldr(t1, Address(Rthread, JavaThread::lock_stack_top_offset()));\n+  \/\/ cmp(t1, (unsigned)LockStack::end_offset()); \/\/  too complicated constant: 1132 (46c)\n+  movw(t2, LockStack::end_offset() - 1);\n+  cmp(t1, t2);\n+  POP_REGS(savemask, t1, t2, t3, gt);\n+  b(slow, gt); \/\/ Z is cleared\n+\n+  \/\/ Prepare old, new header\n+  Register old_hdr = t1;\n+  Register new_hdr = t2;\n+  ldr(new_hdr, Address(obj, oopDesc::mark_offset_in_bytes()));\n+  bic(new_hdr, new_hdr, markWord::lock_mask_in_place);  \/\/ new header (00)\n+  orr(old_hdr, new_hdr, markWord::unlocked_value);      \/\/ old header (01)\n+\n+  Label dummy;\n+\n+  cas_for_lock_acquire(old_hdr \/* old *\/, new_hdr \/* new *\/,\n+      obj \/* location *\/, t3 \/* scratch *\/, dummy,\n+      true \/* allow_fallthrough_on_failure *\/, true \/* one_shot *\/);\n+\n+  POP_REGS(savemask, t1, t2, t3, ne); \/\/ Cas failed -> slow\n+  b(slow, ne);                        \/\/ Cas failed -> slow\n+\n+  \/\/ After successful lock, push object onto lock-stack\n+  ldr(t1, Address(Rthread, JavaThread::lock_stack_top_offset()));\n+  str(obj, Address(Rthread, t1));\n+  add(t1, t1, oopSize);\n+  str(t1, Address(Rthread, JavaThread::lock_stack_top_offset()));\n+\n+  POP_REGS(savemask, t1, t2, t3, al);\n+\n+#ifdef ASSERT\n+  \/\/ Poison scratch regs\n+  POISON_REGS((~savemask), t1, t2, t3, 0x10000001);\n+#endif\n+\n+  \/\/ Success: fall through\n+}\n+\n+\/\/ Attempt to fast-unlock an object\n+\/\/ Registers:\n+\/\/  - obj: the object to be locked\n+\/\/  - t1, t2, t3: temp registers. If corresponding bit in savemask is set, they get saved, otherwise blown.\n+\/\/ Result:\n+\/\/  - Success: fallthrough\n+\/\/  - Error:   break to slow, Z cleared.\n+void MacroAssembler::fast_unlock_2(Register obj, Register t1, Register t2, Register t3, unsigned savemask, Label& slow) {\n+  assert(LockingMode == 2, \"only used with new lightweight locking\");\n+  assert_different_registers(obj, t1, t2, t3);\n+\n+#ifdef ASSERT\n+  \/\/ Poison scratch regs\n+  POISON_REGS((~savemask), t1, t2, t3, 0x30000003);\n+#endif\n+\n+  PUSH_REGS(savemask, t1, t2, t3);\n+\n+  \/\/ Prepare old, new header\n+  Register old_hdr = t1;\n+  Register new_hdr = t2;\n+  ldr(old_hdr, Address(obj, oopDesc::mark_offset_in_bytes()));\n+  bic(old_hdr, old_hdr, markWord::lock_mask_in_place);    \/\/ old header (00)\n+  orr(new_hdr, old_hdr, markWord::unlocked_value);        \/\/ new header (01)\n+\n+  \/\/ Try to swing header from locked to unlocked\n+  Label dummy;\n+  cas_for_lock_release(old_hdr \/* old *\/, new_hdr \/* new *\/,\n+      obj \/* location *\/, t3 \/* scratch *\/, dummy,\n+      true \/* allow_fallthrough_on_failure *\/, true \/* one_shot *\/);\n+\n+  POP_REGS(savemask, t1, t2, t3, ne); \/\/ Cas failed -> slow\n+  b(slow, ne);                        \/\/ Cas failed -> slow\n+\n+  \/\/ After successful unlock, pop object from lock-stack\n+  ldr(t1, Address(Rthread, JavaThread::lock_stack_top_offset()));\n+  sub(t1, t1, oopSize);\n+  str(t1, Address(Rthread, JavaThread::lock_stack_top_offset()));\n+\n+#ifdef ASSERT\n+  \/\/ zero out popped slot\n+  mov(t2, 0);\n+  str(t2, Address(Rthread, t1));\n+#endif\n+\n+  POP_REGS(savemask, t1, t2, t3, al);\n+\n+#ifdef ASSERT\n+  \/\/ Poison scratch regs\n+  POISON_REGS((~savemask), t1, t2, t3, 0x40000004);\n+#endif\n+\n+  \/\/ Fallthrough: success\n+}\n","filename":"src\/hotspot\/cpu\/arm\/macroAssembler_arm.cpp","additions":148,"deletions":1,"binary":false,"changes":149,"status":"modified"},{"patch":"@@ -1013,0 +1013,18 @@\n+  \/\/ Attempt to fast-lock an object\n+  \/\/ Registers:\n+  \/\/  - obj: the object to be locked\n+  \/\/  - t1, t2, t3: temp registers. If corresponding bit in savemask is set, they get saved, otherwise blown.\n+  \/\/ Result:\n+  \/\/  - Success: fallthrough\n+  \/\/  - Error:   break to slow, Z cleared.\n+  void fast_lock_2(Register obj, Register t1, Register t2, Register t3, unsigned savemask, Label& slow);\n+\n+  \/\/ Attempt to fast-unlock an object\n+  \/\/ Registers:\n+  \/\/  - obj: the object to be locked\n+  \/\/  - t1, t2, t3: temp registers. If corresponding bit in savemask is set, they get saved, otherwise blown.\n+  \/\/ Result:\n+  \/\/  - Success: fallthrough\n+  \/\/  - Error:   break to slow, Z cleared.\n+  void fast_unlock_2(Register obj, Register t1, Register t2, Register t3, unsigned savemask, Label& slow);\n+\n","filename":"src\/hotspot\/cpu\/arm\/macroAssembler_arm.hpp","additions":18,"deletions":0,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -648,0 +648,1 @@\n+  __ flush();\n@@ -1156,29 +1157,35 @@\n-    const Register mark = tmp;\n-    \/\/ On MP platforms the next load could return a 'stale' value if the memory location has been modified by another thread.\n-    \/\/ That would be acceptable as either CAS or slow case path is taken in that case\n-\n-    __ ldr(mark, Address(sync_obj, oopDesc::mark_offset_in_bytes()));\n-    __ sub(disp_hdr, FP, lock_slot_fp_offset);\n-    __ tst(mark, markWord::unlocked_value);\n-    __ b(fast_lock, ne);\n-\n-    \/\/ Check for recursive lock\n-    \/\/ See comments in InterpreterMacroAssembler::lock_object for\n-    \/\/ explanations on the fast recursive locking check.\n-    \/\/ Check independently the low bits and the distance to SP\n-    \/\/ -1- test low 2 bits\n-    __ movs(Rtemp, AsmOperand(mark, lsl, 30));\n-    \/\/ -2- test (hdr - SP) if the low two bits are 0\n-    __ sub(Rtemp, mark, SP, eq);\n-    __ movs(Rtemp, AsmOperand(Rtemp, lsr, exact_log2(os::vm_page_size())), eq);\n-    \/\/ If still 'eq' then recursive locking OK\n-    \/\/ set to zero if recursive lock, set to non zero otherwise (see discussion in JDK-8267042)\n-    __ str(Rtemp, Address(disp_hdr, BasicLock::displaced_header_offset_in_bytes()));\n-    __ b(lock_done, eq);\n-    __ b(slow_lock);\n-\n-    __ bind(fast_lock);\n-    __ str(mark, Address(disp_hdr, BasicLock::displaced_header_offset_in_bytes()));\n-\n-    __ cas_for_lock_acquire(mark, disp_hdr, sync_obj, Rtemp, slow_lock);\n-\n+    if (LockingMode == 2) {\n+      log_trace(fastlock2)(\"SharedRuntime lock fast\");\n+      __ fast_lock_2(sync_obj \/* object *\/, disp_hdr \/* t1 *\/, tmp \/* t2 *\/, Rtemp \/* t3 *\/,\n+                     0x7 \/* savemask *\/, slow_lock);\n+      \/\/ Fall through to lock_done\n+    } else if (LockingMode == 1) {\n+      const Register mark = tmp;\n+      \/\/ On MP platforms the next load could return a 'stale' value if the memory location has been modified by another thread.\n+      \/\/ That would be acceptable as either CAS or slow case path is taken in that case\n+\n+      __ ldr(mark, Address(sync_obj, oopDesc::mark_offset_in_bytes()));\n+      __ sub(disp_hdr, FP, lock_slot_fp_offset);\n+      __ tst(mark, markWord::unlocked_value);\n+      __ b(fast_lock, ne);\n+\n+      \/\/ Check for recursive lock\n+      \/\/ See comments in InterpreterMacroAssembler::lock_object for\n+      \/\/ explanations on the fast recursive locking check.\n+      \/\/ Check independently the low bits and the distance to SP\n+      \/\/ -1- test low 2 bits\n+      __ movs(Rtemp, AsmOperand(mark, lsl, 30));\n+      \/\/ -2- test (hdr - SP) if the low two bits are 0\n+      __ sub(Rtemp, mark, SP, eq);\n+      __ movs(Rtemp, AsmOperand(Rtemp, lsr, exact_log2(os::vm_page_size())), eq);\n+      \/\/ If still 'eq' then recursive locking OK\n+      \/\/ set to zero if recursive lock, set to non zero otherwise (see discussion in JDK-8267042)\n+      __ str(Rtemp, Address(disp_hdr, BasicLock::displaced_header_offset_in_bytes()));\n+      __ b(lock_done, eq);\n+      __ b(slow_lock);\n+\n+      __ bind(fast_lock);\n+      __ str(mark, Address(disp_hdr, BasicLock::displaced_header_offset_in_bytes()));\n+\n+      __ cas_for_lock_acquire(mark, disp_hdr, sync_obj, Rtemp, slow_lock);\n+    }\n@@ -1235,8 +1242,14 @@\n-    __ ldr(sync_obj, Address(sync_handle));\n-\n-    \/\/ See C1_MacroAssembler::unlock_object() for more comments\n-    __ ldr(R2, Address(disp_hdr, BasicLock::displaced_header_offset_in_bytes()));\n-    __ cbz(R2, unlock_done);\n-\n-    __ cas_for_lock_release(disp_hdr, R2, sync_obj, Rtemp, slow_unlock);\n-\n+    if (LockingMode == 2) {\n+      log_trace(fastlock2)(\"SharedRuntime unlock fast\");\n+      __ fast_unlock_2(sync_obj, R2, tmp, Rtemp, 7, slow_unlock);\n+      \/\/ Fall through\n+    } else if (LockingMode == 1) {\n+      \/\/ See C1_MacroAssembler::unlock_object() for more comments\n+      __ ldr(sync_obj, Address(sync_handle));\n+\n+      \/\/ See C1_MacroAssembler::unlock_object() for more comments\n+      __ ldr(R2, Address(disp_hdr, BasicLock::displaced_header_offset_in_bytes()));\n+      __ cbz(R2, unlock_done);\n+\n+      __ cas_for_lock_release(disp_hdr, R2, sync_obj, Rtemp, slow_unlock);\n+    }\n","filename":"src\/hotspot\/cpu\/arm\/sharedRuntime_arm.cpp","additions":50,"deletions":37,"binary":false,"changes":87,"status":"modified"},{"patch":"@@ -75,1 +75,1 @@\n-  if (UseFastLocking) {\n+  if (LockingMode == 2) {\n@@ -77,1 +77,1 @@\n-  } else {\n+  } else if (LockingMode == 1) {\n@@ -123,1 +123,1 @@\n-  if (!UseFastLocking) {\n+  if (LockingMode != 2) {\n@@ -135,1 +135,1 @@\n-  if (UseFastLocking) {\n+  if (LockingMode == 2) {\n@@ -138,1 +138,1 @@\n-  } else {\n+  } else if (LockingMode == 1) {\n","filename":"src\/hotspot\/cpu\/riscv\/c1_MacroAssembler_riscv.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -812,1 +812,1 @@\n-    if (UseFastLocking) {\n+    if (LockingMode == 2) {\n@@ -816,1 +816,1 @@\n-    } else {\n+    } else if (LockingMode == 1) {\n@@ -850,4 +850,9 @@\n-    call_VM(noreg,\n-            CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter),\n-            UseFastLocking ? obj_reg : lock_reg);\n-\n+    if (LockingMode == 2) {\n+      call_VM(noreg,\n+              CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter_obj),\n+              obj_reg);\n+    } else {\n+      call_VM(noreg,\n+              CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter),\n+              lock_reg);\n+    }\n@@ -890,1 +895,1 @@\n-    if (!UseFastLocking) {\n+    if (LockingMode != 2) {\n@@ -902,1 +907,1 @@\n-    if (UseFastLocking) {\n+    if (LockingMode == 2) {\n@@ -919,1 +924,1 @@\n-    } else {\n+    } else if (LockingMode == 1) {\n","filename":"src\/hotspot\/cpu\/riscv\/interp_masm_riscv.cpp","additions":14,"deletions":9,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -4493,1 +4493,1 @@\n-  assert(UseFastLocking, \"only used with fast-locking\");\n+  assert(LockingMode == 2, \"only used with new lightweight locking\");\n@@ -4520,1 +4520,1 @@\n-  assert(UseFastLocking, \"only used with fast-locking\");\n+  assert(LockingMode == 2, \"only used with new lightweight locking\");\n","filename":"src\/hotspot\/cpu\/riscv\/macroAssembler_riscv.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2420,1 +2420,1 @@\n-      if (UseFastLocking) {\n+      if (LockingMode == 2) {\n@@ -2430,1 +2430,1 @@\n-      } else {\n+      } else if (LockingMode == 1) {\n@@ -2478,1 +2478,1 @@\n-    if (!UseFastLocking) {\n+    if (LockingMode != 2) {\n@@ -2520,1 +2520,1 @@\n-    if (!UseHeavyMonitors && !UseFastLocking) {\n+    if (LockingMode == 1) {\n@@ -2535,1 +2535,1 @@\n-      if (UseFastLocking) {\n+      if (LockingMode == 2) {\n@@ -2545,1 +2545,1 @@\n-      } else {\n+      } else if (LockingMode == 1) {\n@@ -2567,1 +2567,1 @@\n-    if (UseFastLocking) {\n+    if (LockingMode == 2) {\n","filename":"src\/hotspot\/cpu\/riscv\/riscv.ad","additions":7,"deletions":7,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -1674,1 +1674,1 @@\n-      if (UseFastLocking) {\n+      if (LockingMode == 2) {\n@@ -1677,1 +1677,1 @@\n-      } else {\n+      } else if (LockingMode == 1) {\n@@ -1798,1 +1798,1 @@\n-    if (!UseHeavyMonitors && !UseFastLocking) {\n+    if (LockingMode == 1) {\n@@ -1814,1 +1814,1 @@\n-      if (UseFastLocking) {\n+      if (LockingMode == 2) {\n@@ -1817,1 +1817,1 @@\n-      } else {\n+      } else if (LockingMode == 1) {\n","filename":"src\/hotspot\/cpu\/riscv\/sharedRuntime_riscv.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -3511,1 +3511,1 @@\n-    Register tmp = UseFastLocking ? op->scratch_opr()->as_register() : noreg;\n+    Register tmp = LockingMode == 2 ? op->scratch_opr()->as_register() : noreg;\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRAssembler_x86.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -322,1 +322,1 @@\n-  LIR_Opr tmp = UseFastLocking ? new_register(T_ADDRESS) : LIR_OprFact::illegalOpr;\n+  LIR_Opr tmp = LockingMode == 2 ? new_register(T_ADDRESS) : LIR_OprFact::illegalOpr;\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRGenerator_x86.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -65,1 +65,1 @@\n-  if (UseFastLocking) {\n+  if (LockingMode == 2) {\n@@ -73,1 +73,1 @@\n-  } else {\n+  } else  if (LockingMode == 1) {\n@@ -122,1 +122,1 @@\n-  if (!UseFastLocking) {\n+  if (LockingMode != 2) {\n@@ -135,1 +135,1 @@\n-  if (UseFastLocking) {\n+  if (LockingMode == 2) {\n@@ -139,1 +139,1 @@\n-  } else {\n+  } else if (LockingMode == 1) {\n","filename":"src\/hotspot\/cpu\/x86\/c1_MacroAssembler_x86.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -605,1 +605,1 @@\n-    if (UseFastLocking) {\n+    if (LockingMode == 2) {\n@@ -608,1 +608,1 @@\n-    } else {\n+    } else if (LockingMode == 1) {\n@@ -769,1 +769,1 @@\n-  if (!UseHeavyMonitors && !UseFastLocking) {\n+  if (LockingMode == 1) {\n@@ -777,1 +777,1 @@\n-    if (UseFastLocking) {\n+    if (LockingMode == 2) {\n@@ -916,1 +916,1 @@\n-    if (UseFastLocking) {\n+    if (LockingMode == 2) {\n@@ -920,1 +920,1 @@\n-    } else {\n+    } else if (LockingMode == 1) {\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -1226,1 +1226,1 @@\n-    if (UseFastLocking) {\n+    if (LockingMode == 2) {\n@@ -1236,1 +1236,1 @@\n-    } else {\n+    } else if (LockingMode == 1) {\n@@ -1297,1 +1297,1 @@\n-    if (UseFastLocking) {\n+    if (LockingMode == 2) {\n@@ -1338,1 +1338,1 @@\n-    if (!UseFastLocking) {\n+    if (LockingMode != 2) {\n@@ -1350,1 +1350,1 @@\n-    if (UseFastLocking) {\n+    if (LockingMode == 2) {\n@@ -1366,1 +1366,1 @@\n-    } else {\n+    } else if (LockingMode == 1) {\n","filename":"src\/hotspot\/cpu\/x86\/interp_masm_x86.cpp","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -1684,1 +1684,1 @@\n-      if (UseFastLocking) {\n+      if (LockingMode == 2) {\n@@ -1688,1 +1688,1 @@\n-      } else {\n+      } else if (LockingMode == 1) {\n@@ -1843,1 +1843,1 @@\n-    if (!UseHeavyMonitors && !UseFastLocking) {\n+    if (LockingMode == 1) {\n@@ -1859,1 +1859,1 @@\n-      if (UseFastLocking) {\n+      if (LockingMode == 2) {\n@@ -1863,1 +1863,1 @@\n-      } else {\n+      } else if (LockingMode == 1) {\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_32.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -2153,1 +2153,1 @@\n-      if (UseFastLocking) {\n+      if (LockingMode == 2) {\n@@ -2157,1 +2157,1 @@\n-      } else {\n+      } else if (LockingMode == 1) {\n@@ -2303,1 +2303,1 @@\n-    if (!UseHeavyMonitors && !UseFastLocking) {\n+    if (LockingMode == 1) {\n@@ -2319,1 +2319,1 @@\n-      if (UseFastLocking) {\n+      if (LockingMode == 2) {\n@@ -2323,1 +2323,1 @@\n-      } else {\n+      } else if (LockingMode == 1) {\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_64.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -760,2 +760,2 @@\n-  assert(UseFastLocking || obj == lock->obj(), \"must match\");\n-  SharedRuntime::monitor_enter_helper(obj, UseFastLocking ? nullptr : lock->lock(), current);\n+  assert(LockingMode == 2 || obj == lock->obj(), \"must match\");\n+  SharedRuntime::monitor_enter_helper(obj, LockingMode == 2 ? nullptr : lock->lock(), current);\n","filename":"src\/hotspot\/share\/c1\/c1_Runtime1.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -738,1 +738,1 @@\n-  assert(!UseFastLocking || UseHeavyMonitors, \"Should call monitorenter_obj() when using UseFastLocking\");\n+  assert(LockingMode != 2, \"Should call monitorenter_obj() when using the new lightweight locking\");\n@@ -753,1 +753,1 @@\n-\/\/ NOTE: We provide a separate implementation for UseFastLocking to workaround a limitation\n+\/\/ NOTE: We provide a separate implementation for the new lightweight locking to workaround a limitation\n@@ -762,1 +762,1 @@\n-  assert(!UseHeavyMonitors && UseFastLocking, \"Should call monitorenter() when not using UseFastLocking\");\n+  assert(LockingMode == 2, \"Should call monitorenter() when not using the new lightweight locking\");\n","filename":"src\/hotspot\/share\/interpreter\/interpreterRuntime.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -80,0 +80,1 @@\n+  LOG_TAG(fastlock2) \\\n","filename":"src\/hotspot\/share\/logging\/logTag.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -175,1 +175,1 @@\n-    return !UseFastLocking && ((value() & lock_mask_in_place) == locked_value);\n+    return LockingMode == 1 && ((value() & lock_mask_in_place) == locked_value);\n@@ -183,1 +183,1 @@\n-    return UseFastLocking && ((value() & lock_mask_in_place) == locked_value);\n+    return LockingMode == 2 && ((value() & lock_mask_in_place) == locked_value);\n@@ -200,2 +200,2 @@\n-    return UseFastLocking ? lockbits == monitor_value   \/\/ monitor?\n-                    : (lockbits & unlocked_value) == 0; \/\/ monitor | stack-locked?\n+    return LockingMode == 2 ? lockbits == monitor_value   \/\/ monitor?\n+                            : (lockbits & unlocked_value) == 0; \/\/ monitor | stack-locked?\n","filename":"src\/hotspot\/share\/oops\/markWord.hpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -122,1 +122,1 @@\n-  \/\/ at a safepoint, it must not be zero, except when UseFastLocking is turned on.\n+  \/\/ at a safepoint, it must not be zero, except when using the new lightweight locking.\n@@ -131,1 +131,1 @@\n-  return  UseFastLocking || !SafepointSynchronize::is_at_safepoint();\n+  return LockingMode == 2 || !SafepointSynchronize::is_at_safepoint();\n","filename":"src\/hotspot\/share\/oops\/oop.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2003,0 +2003,11 @@\n+\n+#if !defined(X86) && !defined(AARCH64) && !defined(RISCV64) && !defined(ARM)\n+  if (LockingMode == 2) {\n+    FLAG_SET_CMDLINE(LockingMode, 1);\n+    warning(\"New lightweight locking not supported on this platform\");\n+  }\n+#endif\n+  if (UseHeavyMonitors) {\n+    FLAG_SET_CMDLINE(LockingMode, 0);\n+  }\n+\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":11,"deletions":0,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -1978,2 +1978,6 @@\n-  product(bool, UseFastLocking, false, EXPERIMENTAL,                        \\\n-                \"Use fast-locking instead of stack-locking\")                \\\n+  product(int, LockingMode, 1, EXPERIMENTAL,                                \\\n+          \"Select locking mode: \"                                           \\\n+          \"0: monitors only, \"                                              \\\n+          \"1: monitors & traditional stack-locking (default), \"             \\\n+          \"2: monitors & new lightweight locking\")                          \\\n+          range(0, 2)                                                       \\\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":6,"deletions":2,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -993,1 +993,1 @@\n-  assert(!UseFastLocking, \"should not be called with fast-locking\");\n+  assert(!LockingMode != 2, \"should not be called with new lightweight locking\");\n@@ -1388,1 +1388,1 @@\n-  if (!UseHeavyMonitors && UseFastLocking) {\n+  if (!UseHeavyMonitors && LockingMode == 2) {\n","filename":"src\/hotspot\/share\/runtime\/javaThread.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -77,1 +77,1 @@\n-  assert(UseFastLocking && !UseHeavyMonitors, \"never use lock-stack when fast-locking is disabled\");\n+  assert(LockingMode == 2 && !UseHeavyMonitors, \"never use lock-stack when fast-locking is disabled\");\n","filename":"src\/hotspot\/share\/runtime\/lockStack.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -337,1 +337,1 @@\n-  if (!UseFastLocking && current->is_lock_owned((address)cur)) {\n+  if (LockingMode != 2 && current->is_lock_owned((address)cur)) {\n@@ -1138,1 +1138,1 @@\n-    if (!UseFastLocking && current->is_lock_owned((address)cur)) {\n+    if (LockingMode != 2 && current->is_lock_owned((address)cur)) {\n@@ -1353,1 +1353,1 @@\n-    if (!UseFastLocking && current->is_lock_owned((address)cur)) {\n+    if (LockingMode != 2 && current->is_lock_owned((address)cur)) {\n@@ -1392,1 +1392,1 @@\n-  if (!UseFastLocking && current->is_lock_owned((address)cur)) {\n+  if (LockingMode != 2 && current->is_lock_owned((address)cur)) {\n","filename":"src\/hotspot\/share\/runtime\/objectMonitor.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -37,1 +37,1 @@\n-  if (UseFastLocking) {\n+  if (LockingMode == 2) {\n","filename":"src\/hotspot\/share\/runtime\/objectMonitor.inline.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -315,1 +315,1 @@\n-  if (UseFastLocking) {\n+  if (LockingMode == 2) {\n@@ -318,1 +318,1 @@\n-      \/\/ fast-locked or stack-locked by caller so by definition the implied waitset is empty.\n+      \/\/ fast-locked by caller so by definition the implied waitset is empty.\n@@ -321,1 +321,1 @@\n-  } else {\n+  } else if (LockingMode == 1) {\n@@ -324,1 +324,1 @@\n-      \/\/ fast-locked or stack-locked by caller so by definition the implied waitset is empty.\n+      \/\/ stack-locked by caller so by definition the implied waitset is empty.\n@@ -397,1 +397,1 @@\n-    if (!UseFastLocking) {\n+    if (LockingMode != 2) {\n@@ -497,1 +497,1 @@\n-    if (UseFastLocking) {\n+    if (LockingMode == 2) {\n@@ -515,1 +515,1 @@\n-    } else {\n+    } else if (LockingMode == 1) {\n@@ -559,1 +559,1 @@\n-    if (UseFastLocking) {\n+    if (LockingMode == 2) {\n@@ -579,1 +579,1 @@\n-    } else {\n+    } else if (LockingMode == 1) {\n@@ -627,1 +627,1 @@\n-  if (UseFastLocking && monitor->is_owner_anonymous()) {\n+  if (LockingMode == 2 && monitor->is_owner_anonymous()) {\n@@ -724,1 +724,1 @@\n-  if (UseFastLocking) {\n+  if (LockingMode == 2) {\n@@ -729,1 +729,1 @@\n-  } else {\n+  } else if (LockingMode == 1) {\n@@ -746,1 +746,1 @@\n-  if (UseFastLocking) {\n+  if (LockingMode == 2) {\n@@ -751,1 +751,1 @@\n-  } else {\n+  } else if (LockingMode == 1) {\n@@ -781,2 +781,2 @@\n-  if (!mark.is_being_inflated() || UseFastLocking) {\n-    \/\/ fast-locking does not use the markWord::INFLATING() protocol.\n+  if (!mark.is_being_inflated() || LockingMode == 2) {\n+    \/\/ New lightweight locking does not use the markWord::INFLATING() protocol.\n@@ -898,1 +898,1 @@\n-  assert(UseFastLocking, \"only call this with fast-locking enabled\");\n+  assert(LockingMode == 2, \"only call this with new lightweight locking enabled\");\n@@ -1280,1 +1280,1 @@\n-      if (UseFastLocking && inf->is_owner_anonymous() && is_lock_owned(current, object)) {\n+      if (LockingMode == 2 && inf->is_owner_anonymous() && is_lock_owned(current, object)) {\n@@ -1288,2 +1288,2 @@\n-    if (!UseFastLocking) {\n-      \/\/ Fast-locking does not use INFLATING.\n+    if (LockingMode != 2) {\n+      \/\/ New lightweight locking does not use INFLATING.\n@@ -1314,1 +1314,1 @@\n-      assert(UseFastLocking, \"can only happen with fast-locking\");\n+      assert(LockingMode == 2, \"can only happen with new lightweight locking\");\n@@ -1368,1 +1368,1 @@\n-      assert(!UseFastLocking, \"cannot happen with fast-locking\");\n+      assert(LockingMode != 2, \"cannot happen with new lightweight locking\");\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.cpp","additions":22,"deletions":22,"binary":false,"changes":44,"status":"modified"},{"patch":"@@ -536,1 +536,1 @@\n-  assert(!UseFastLocking, \"should not be called with fast-locking\");\n+  assert(LockingMode != 2, \"should not be called with new lightweight locking\");\n","filename":"src\/hotspot\/share\/runtime\/thread.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1394,1 +1394,1 @@\n-  assert(!UseFastLocking, \"only with stack-locking\");\n+  assert(LockingMode != 2, \"Not with new lightweight locking\");\n@@ -1426,1 +1426,1 @@\n-  assert(UseFastLocking, \"Only with fast-locking\");\n+  assert(LockingMode == 2, \"Only with new lightweight locking\");\n@@ -1436,1 +1436,1 @@\n-  if (UseFastLocking) {\n+  if (LockingMode == 2) {\n","filename":"src\/hotspot\/share\/runtime\/threads.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -1124,1 +1124,6 @@\n-  if (!UseFastLocking && maxDepth == 0) {\n+  \/\/ When using the new lightweight locking, we need to take\n+  \/\/ a safepoint here, because the thread snapshot code calls\n+  \/\/ ObjectSynchronizer::get_lock_owner() and it would potentially\n+  \/\/ give wrong results when Java threads are running and\n+  \/\/ entering\/leaving locks while we inspect the thread stacks.\n+  if (LockingMode != 2 && maxDepth == 0) {\n","filename":"src\/hotspot\/share\/services\/management.cpp","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -213,1 +213,1 @@\n-        assert(!VM.getVM().getCommandLineBooleanFlag(\"UseFastLocking\"));\n+        assert(VM.getVM().getCommandLineFlag(\"LockingMode\").getInt() != 2);\n@@ -231,1 +231,1 @@\n-        if (VM.getVM().getCommandLineBooleanFlag(\"UseFastLocking\")) {\n+        if (VM.getVM().getCommandLineFlag(\"LockingMode\").getInt() == 2) {\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/runtime\/Threads.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"}]}