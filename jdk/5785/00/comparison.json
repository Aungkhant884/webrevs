{"files":[{"patch":"@@ -40,0 +40,2 @@\n+extern aarch64_atomic_stub_t aarch64_atomic_fetch_add_4_relaxed_impl;\n+extern aarch64_atomic_stub_t aarch64_atomic_fetch_add_8_relaxed_impl;\n","filename":"src\/hotspot\/cpu\/aarch64\/atomic_aarch64.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -6208,1 +6208,1 @@\n-  void gen_ldaddal_entry(Assembler::operand_size size) {\n+  void gen_ldadd_entry(Assembler::operand_size size, atomic_memory_order order) {\n@@ -6210,2 +6210,8 @@\n-    __ ldaddal(size, incr, prev, addr);\n-    __ membar(Assembler::StoreStore|Assembler::StoreLoad);\n+    \/\/ If not relaxed, then default to conservative.  Relaxed is the only\n+    \/\/ case we use enough to be worth specializing.\n+    if (order == memory_order_relaxed) {\n+      __ ldadd(size, incr, prev, addr);\n+    } else {\n+      __ ldaddal(size, incr, prev, addr);\n+      __ membar(Assembler::StoreStore|Assembler::StoreLoad);\n+    }\n@@ -6241,1 +6247,1 @@\n-    \/\/ All memory_order_conservative\n+    \/\/ ADD, memory_order_conservative\n@@ -6243,1 +6249,1 @@\n-    gen_ldaddal_entry(Assembler::word);\n+    gen_ldadd_entry(Assembler::word, memory_order_conservative);\n@@ -6245,1 +6251,9 @@\n-    gen_ldaddal_entry(Assembler::xword);\n+    gen_ldadd_entry(Assembler::xword, memory_order_conservative);\n+\n+    \/\/ ADD, memory_order_relaxed\n+    AtomicStubMark mark_fetch_add_4_relaxed\n+      (_masm, &aarch64_atomic_fetch_add_4_relaxed_impl);\n+    gen_ldadd_entry(MacroAssembler::word, memory_order_relaxed);\n+    AtomicStubMark mark_fetch_add_8_relaxed\n+      (_masm, &aarch64_atomic_fetch_add_8_relaxed_impl);\n+    gen_ldadd_entry(MacroAssembler::xword, memory_order_relaxed);\n@@ -6247,0 +6261,1 @@\n+    \/\/ XCHG, memory_order_conservative\n@@ -7446,0 +7461,2 @@\n+DEFAULT_ATOMIC_OP(fetch_add, 4, _relaxed)\n+DEFAULT_ATOMIC_OP(fetch_add, 8, _relaxed)\n","filename":"src\/hotspot\/cpu\/aarch64\/stubGenerator_aarch64.cpp","additions":23,"deletions":6,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -50,0 +50,22 @@\n+        .global aarch64_atomic_fetch_add_8_relaxed_default_impl\n+        .align 5\n+aarch64_atomic_fetch_add_8_relaxed_default_impl:\n+        prfm    pstl1strm, [x0]\n+0:      ldxr    x2, [x0]\n+        add     x8, x2, x1\n+        stxr    w9, x8, [x0]\n+        cbnz    w9, 0b\n+        mov     x0, x2\n+        ret\n+\n+        .global aarch64_atomic_fetch_add_4_relaxed_default_impl\n+        .align 5\n+aarch64_atomic_fetch_add_4_relaxed_default_impl:\n+        prfm    pstl1strm, [x0]\n+0:      ldxr    w2, [x0]\n+        add     w8, w2, w1\n+        stxr    w9, w8, [x0]\n+        cbnz    w9, 0b\n+        mov     w0, w2\n+        ret\n+\n","filename":"src\/hotspot\/os_cpu\/linux_aarch64\/atomic_linux_aarch64.S","additions":22,"deletions":0,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -90,3 +90,8 @@\n-  D old_value\n-    = atomic_fastcall(aarch64_atomic_fetch_add_4_impl, dest, add_value);\n-  return old_value;\n+  aarch64_atomic_stub_t stub;\n+  switch (order) {\n+  case memory_order_relaxed:\n+    stub = aarch64_atomic_fetch_add_4_relaxed_impl; break;\n+  default:\n+    stub = aarch64_atomic_fetch_add_4_impl; break;\n+  }\n+  return atomic_fastcall(stub, dest, add_value);\n@@ -101,3 +106,8 @@\n-  D old_value\n-    = atomic_fastcall(aarch64_atomic_fetch_add_8_impl, dest, add_value);\n-  return old_value;\n+  aarch64_atomic_stub_t stub;\n+  switch (order) {\n+  case memory_order_relaxed:\n+    stub = aarch64_atomic_fetch_add_8_relaxed_impl; break;\n+  default:\n+    stub = aarch64_atomic_fetch_add_8_impl; break;\n+  }\n+  return atomic_fastcall(stub, dest, add_value);\n","filename":"src\/hotspot\/os_cpu\/linux_aarch64\/atomic_linux_aarch64.hpp","additions":17,"deletions":7,"binary":false,"changes":24,"status":"modified"}]}