{"files":[{"patch":"@@ -917,1 +917,1 @@\n-reg_class pr_reg (\n+reg_class vectmask_reg (\n@@ -1913,1 +1913,1 @@\n-  return &_PR_REG_mask;\n+  return &_VECTMASK_REG_mask;\n@@ -3540,1 +3540,3 @@\n-operand pRegGov()\n+\/\/ The mask value used to control execution of a masked\n+\/\/ vector instruction is always supplied by vector register v0.\n+operand vReg_V0()\n@@ -3542,1 +3544,1 @@\n-  constraint(ALLOC_IN_RC(pr_reg));\n+  constraint(ALLOC_IN_RC(vectmask_reg));\n","filename":"src\/hotspot\/cpu\/riscv\/riscv.ad","additions":6,"deletions":4,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -125,3 +125,5 @@\n-instruct vloadmask(pRegGov dst, vReg src, rFlagsReg cr) %{\n-  match(Set dst (VectorLoadMask src));\n-  format %{ \"vloadmask $dst, $src\\t# KILL cr\" %}\n+\/\/ vector load mask\n+\n+instruct vloadmask(vReg_V0 v0, vReg src, rFlagsReg cr) %{\n+  match(Set v0 (VectorLoadMask src));\n+  format %{ \"vloadmask V0, $src\\t# KILL cr\" %}\n@@ -130,2 +132,1 @@\n-    Assembler::SEW sew = Assembler::elemtype_to_sew(bt);\n-    __ vsetvli(t0, x0, sew);\n+    __ rvv_vsetvli(bt, Matcher::vector_length_in_bytes(this, $src));\n@@ -299,2 +300,2 @@\n-instruct vaddB_masked(vReg dst_src1, vReg src2, pRegGov mask) %{\n-  match(Set dst_src1 (AddVB (Binary dst_src1 src2) mask));\n+instruct vadd_masked(vReg dst_src1, vReg src2, vReg_V0 v0) %{\n+  match(Set dst_src1 (AddVB (Binary dst_src1 src2) v0));\n@@ -302,1 +303,1 @@\n-  format %{ \"vadd.vv $dst_src1, $src2, $mask\\t#@vaddB_masked\" %}\n+  format %{ \"vadd.vv $dst_src1, $src2, V0\\t#@vadd_masked\" %}\n@@ -312,2 +313,2 @@\n-instruct vaddS_masked(vReg dst_src1, vReg src2, pRegGov mask) %{\n-  match(Set dst_src1 (AddVS (Binary dst_src1 src2) mask));\n+instruct vaddS_masked(vReg dst_src1, vReg src2, vReg_V0 v0) %{\n+  match(Set dst_src1 (AddVS (Binary dst_src1 src2) v0));\n@@ -315,1 +316,1 @@\n-  format %{ \"vadd.vv $dst_src1, $src2, $mask\\t#@vaddS_masked\" %}\n+  format %{ \"vadd.vv $dst_src1, $src2, V0\\t#@vaddS_masked\" %}\n@@ -325,2 +326,2 @@\n-instruct vaddI_masked(vReg dst_src1, vReg src2, pRegGov mask) %{\n-  match(Set dst_src1 (AddVI (Binary dst_src1 src2) mask));\n+instruct vaddI_masked(vReg dst_src1, vReg src2, vReg_V0 v0) %{\n+  match(Set dst_src1 (AddVI (Binary dst_src1 src2) v0));\n@@ -328,1 +329,1 @@\n-  format %{ \"vadd.vv $dst_src1, $src2, $mask\\t#@vaddI_masked\" %}\n+  format %{ \"vadd.vv $dst_src1, $src2, V0\\t#@vaddI_masked\" %}\n@@ -338,2 +339,2 @@\n-instruct vaddL_masked(vReg dst_src1, vReg src2, pRegGov mask) %{\n-  match(Set dst_src1 (AddVL (Binary dst_src1 src2) mask));\n+instruct vaddL_masked(vReg dst_src1, vReg src2, vReg_V0 v0) %{\n+  match(Set dst_src1 (AddVL (Binary dst_src1 src2) v0));\n@@ -341,1 +342,1 @@\n-  format %{ \"vadd.vv $dst_src1, $src2, $mask\\t#@vaddL_masked\" %}\n+  format %{ \"vadd.vv $dst_src1, $src2, V0\\t#@vaddL_masked\" %}\n@@ -351,2 +352,2 @@\n-instruct vaddF_masked(vReg dst_src1, vReg src2, pRegGov mask) %{\n-  match(Set dst_src1 (AddVF (Binary dst_src1 src2) mask));\n+instruct vaddF_masked(vReg dst_src1, vReg src2, vReg_V0 v0) %{\n+  match(Set dst_src1 (AddVF (Binary dst_src1 src2) v0));\n@@ -354,1 +355,1 @@\n-  format %{ \"vfadd.vv $dst_src1, $src2, $mask\\t#@vaddF_masked\" %}\n+  format %{ \"vfadd.vv $dst_src1, $src2, V0\\t#@vaddF_masked\" %}\n@@ -364,2 +365,2 @@\n-instruct vaddD_masked(vReg dst_src1, vReg src2, pRegGov mask) %{\n-  match(Set dst_src1 (AddVD (Binary dst_src1 src2) mask));\n+instruct vaddD_masked(vReg dst_src1, vReg src2, vReg_V0 v0) %{\n+  match(Set dst_src1 (AddVD (Binary dst_src1 src2) v0));\n@@ -367,1 +368,1 @@\n-  format %{ \"vfadd.vv $dst_src1, $src2, $mask\\t#@vaddD_masked\" %}\n+  format %{ \"vfadd.vv $dst_src1, $src2, V0\\t#@vaddD_masked\" %}\n@@ -452,2 +453,2 @@\n-instruct vdivF_masked(vReg dst_src1, vReg src2, pRegGov mask) %{\n-  match(Set dst_src1 (DivVF (Binary dst_src1 src2) mask));\n+instruct vdivF_masked(vReg dst_src1, vReg src2, vReg_V0 v0) %{\n+  match(Set dst_src1 (DivVF (Binary dst_src1 src2) v0));\n@@ -455,1 +456,1 @@\n-  format %{ \"vfdiv.vv  $dst_src1, $src2, $mask\\t#@vdivF_masked\" %}\n+  format %{ \"vfdiv.vv  $dst_src1, $src2, V0\\t#@vdivF_masked\" %}\n@@ -465,2 +466,2 @@\n-instruct vdivD_masked(vReg dst_src1, vReg src2, pRegGov mask) %{\n-  match(Set dst_src1 (DivVD (Binary dst_src1 src2) mask));\n+instruct vdivD_masked(vReg dst_src1, vReg src2, vReg_V0 v0) %{\n+  match(Set dst_src1 (DivVD (Binary dst_src1 src2) v0));\n@@ -468,1 +469,1 @@\n-  format %{ \"vfdiv.vv  $dst_src1, $src2, $mask\\t#@vdivD_masked\" %}\n+  format %{ \"vfdiv.vv  $dst_src1, $src2, V0\\t#@vdivD_masked\" %}\n@@ -880,2 +881,2 @@\n-instruct vmulB_masked(vReg dst_src1, vReg src2, pRegGov mask) %{\n-  match(Set dst_src1 (MulVB (Binary dst_src1 src2) mask));\n+instruct vmulB_masked(vReg dst_src1, vReg src2, vReg_V0 v0) %{\n+  match(Set dst_src1 (MulVB (Binary dst_src1 src2) v0));\n@@ -883,1 +884,1 @@\n-  format %{ \"vmul.vv $dst_src1, $src2, $mask\\t#@vmulB_masked\" %}\n+  format %{ \"vmul.vv $dst_src1, $src2, V0\\t#@vmulB_masked\" %}\n@@ -892,2 +893,2 @@\n-instruct vmulS_masked(vReg dst_src1, vReg src2, pRegGov mask) %{\n-  match(Set dst_src1 (MulVS (Binary dst_src1 src2) mask));\n+instruct vmulS_masked(vReg dst_src1, vReg src2, vReg_V0 v0) %{\n+  match(Set dst_src1 (MulVS (Binary dst_src1 src2) v0));\n@@ -895,1 +896,1 @@\n-  format %{ \"vmul.vv $dst_src1, $src2, $mask\\t#@vmulS_masked\" %}\n+  format %{ \"vmul.vv $dst_src1, $src2, V0\\t#@vmulS_masked\" %}\n@@ -904,2 +905,2 @@\n-instruct vmulI_masked(vReg dst_src1, vReg src2, pRegGov mask) %{\n-  match(Set dst_src1 (MulVI (Binary dst_src1 src2) mask));\n+instruct vmulI_masked(vReg dst_src1, vReg src2, vReg_V0 v0) %{\n+  match(Set dst_src1 (MulVI (Binary dst_src1 src2) v0));\n@@ -907,1 +908,1 @@\n-  format %{ \"vmul.vv $dst_src1, $src2, $mask\\t#@vmulI_masked\" %}\n+  format %{ \"vmul.vv $dst_src1, $src2, V0\\t#@vmulI_masked\" %}\n@@ -916,2 +917,2 @@\n-instruct vmulL_masked(vReg dst_src1, vReg src2, pRegGov mask) %{\n-  match(Set dst_src1 (MulVL (Binary dst_src1 src2) mask));\n+instruct vmulL_masked(vReg dst_src1, vReg src2, vReg_V0 v0) %{\n+  match(Set dst_src1 (MulVL (Binary dst_src1 src2) v0));\n@@ -919,1 +920,1 @@\n-  format %{ \"vmul.vv $dst_src1, $src2, $mask\\t#@vmulL_masked\" %}\n+  format %{ \"vmul.vv $dst_src1, $src2, V0\\t#@vmulL_masked\" %}\n@@ -928,2 +929,2 @@\n-instruct vmulF_masked(vReg dst_src1, vReg src2, pRegGov mask) %{\n-  match(Set dst_src1 (MulVF (Binary dst_src1 src2) mask));\n+instruct vmulF_masked(vReg dst_src1, vReg src2, vReg_V0 v0) %{\n+  match(Set dst_src1 (MulVF (Binary dst_src1 src2) v0));\n@@ -931,1 +932,1 @@\n-  format %{ \"vmul.vv $dst_src1, $src2, $mask\\t#@vmulF_masked\" %}\n+  format %{ \"vmul.vv $dst_src1, $src2, V0\\t#@vmulF_masked\" %}\n@@ -940,2 +941,2 @@\n-instruct vmulD_masked(vReg dst_src1, vReg src2, pRegGov mask) %{\n-  match(Set dst_src1 (MulVD (Binary dst_src1 src2) mask));\n+instruct vmulD_masked(vReg dst_src1, vReg src2, vReg_V0 v0) %{\n+  match(Set dst_src1 (MulVD (Binary dst_src1 src2) v0));\n@@ -943,1 +944,1 @@\n-  format %{ \"vmul.vv $dst_src1, $src2, $mask\\t#@vmulD_masked\" %}\n+  format %{ \"vmul.vv $dst_src1, $src2, V0\\t#@vmulD_masked\" %}\n@@ -2025,2 +2026,2 @@\n-instruct vsubB_masked(vReg dst_src1, vReg src2, pRegGov mask) %{\n-  match(Set dst_src1 (SubVB (Binary dst_src1 src2) mask));\n+instruct vsubB_masked(vReg dst_src1, vReg src2, vReg_V0 v0) %{\n+  match(Set dst_src1 (SubVB (Binary dst_src1 src2) v0));\n@@ -2028,1 +2029,1 @@\n-  format %{ \"vsub.vv $dst_src1, $src2, $mask\\t#@vsubB_masked\" %}\n+  format %{ \"vsub.vv $dst_src1, $src2, V0\\t#@vsubB_masked\" %}\n@@ -2037,2 +2038,2 @@\n-instruct vsubS_masked(vReg dst_src1, vReg src2, pRegGov mask) %{\n-  match(Set dst_src1 (SubVS (Binary dst_src1 src2) mask));\n+instruct vsubS_masked(vReg dst_src1, vReg src2, vReg_V0 v0) %{\n+  match(Set dst_src1 (SubVS (Binary dst_src1 src2) v0));\n@@ -2040,1 +2041,1 @@\n-  format %{ \"vsub.vv $dst_src1, $src2, $mask\\t#@vsubS_masked\" %}\n+  format %{ \"vsub.vv $dst_src1, $src2, V0\\t#@vsubS_masked\" %}\n@@ -2049,2 +2050,2 @@\n-instruct vsubI_masked(vReg dst_src1, vReg src2, pRegGov mask) %{\n-  match(Set dst_src1 (SubVI (Binary dst_src1 src2) mask));\n+instruct vsubI_masked(vReg dst_src1, vReg src2, vReg_V0 v0) %{\n+  match(Set dst_src1 (SubVI (Binary dst_src1 src2) v0));\n@@ -2052,1 +2053,1 @@\n-  format %{ \"vsub.vv $dst_src1, $src2, $mask\\t#@vsubI_masked\" %}\n+  format %{ \"vsub.vv $dst_src1, $src2, V0\\t#@vsubI_masked\" %}\n@@ -2061,2 +2062,2 @@\n-instruct vsubL_masked(vReg dst_src1, vReg src2, pRegGov mask) %{\n-  match(Set dst_src1 (SubVL (Binary dst_src1 src2) mask));\n+instruct vsubL_masked(vReg dst_src1, vReg src2, vReg_V0 v0) %{\n+  match(Set dst_src1 (SubVL (Binary dst_src1 src2) v0));\n@@ -2064,1 +2065,1 @@\n-  format %{ \"vsub.vv $dst_src1, $src2, $mask\\t#@vsubL_masked\" %}\n+  format %{ \"vsub.vv $dst_src1, $src2, V0\\t#@vsubL_masked\" %}\n@@ -2073,2 +2074,2 @@\n-instruct vsubF_masked(vReg dst_src1, vReg src2, pRegGov mask) %{\n-  match(Set dst_src1 (SubVF (Binary dst_src1 src2) mask));\n+instruct vsubF_masked(vReg dst_src1, vReg src2, vReg_V0 v0) %{\n+  match(Set dst_src1 (SubVF (Binary dst_src1 src2) v0));\n@@ -2076,1 +2077,1 @@\n-  format %{ \"vfsub.vv $dst_src1, $src2, $mask\\t#@vsubF_masked\" %}\n+  format %{ \"vfsub.vv $dst_src1, $src2, V0\\t#@vsubF_masked\" %}\n@@ -2085,2 +2086,2 @@\n-instruct vsubD_masked(vReg dst_src1, vReg src2, pRegGov mask) %{\n-  match(Set dst_src1 (SubVD (Binary dst_src1 src2) mask));\n+instruct vsubD_masked(vReg dst_src1, vReg src2, vReg_V0 v0) %{\n+  match(Set dst_src1 (SubVD (Binary dst_src1 src2) v0));\n@@ -2088,1 +2089,1 @@\n-  format %{ \"vfsub.vv $dst_src1, $src2, $mask\\t#@vsubD_masked\" %}\n+  format %{ \"vfsub.vv $dst_src1, $src2, V0\\t#@vsubD_masked\" %}\n","filename":"src\/hotspot\/cpu\/riscv\/riscv_v.ad","additions":66,"deletions":65,"binary":false,"changes":131,"status":"modified"}]}