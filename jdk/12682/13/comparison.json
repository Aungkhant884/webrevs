{"files":[{"patch":"@@ -1491,0 +1491,10 @@\n+#define INSN(NAME, op, funct3, vm, funct6)                                                         \\\n+  void NAME(VectorRegister Vd, VectorRegister Vs2, Register Rs1) {                                 \\\n+    patch_VArith(op, Vd, funct3, Rs1->raw_encoding(), Vs2, vm, funct6);                            \\\n+  }\n+\n+  \/\/ Vector Integer Merge Instructions\n+  INSN(vmerge_vxm,  0b1010111, 0b100, 0b0, 0b010111);\n+\n+#undef INSN\n+\n@@ -1545,0 +1555,11 @@\n+#define INSN(NAME, op, funct3, vm, funct6)                                    \\\n+  void NAME(VectorRegister Vd, VectorRegister Vs2, int32_t imm) {             \\\n+    guarantee(is_imm_in_range(imm, 5, 0), \"imm is invalid\");                  \\\n+    patch_VArith(op, Vd, funct3, (uint32_t)(imm & 0x1f), Vs2, vm, funct6);    \\\n+  }\n+\n+  \/\/ Vector Integer Merge Instructions\n+  INSN(vmerge_vim,  0b1010111, 0b011, 0b0, 0b010111);\n+\n+#undef INSN\n+\n@@ -1563,0 +1584,3 @@\n+  \/\/ Vector Integer Merge Instructions\n+  INSN(vmerge_vvm,  0b1010111, 0b000, 0b0, 0b010111);\n+\n","filename":"src\/hotspot\/cpu\/riscv\/assembler_riscv.hpp","additions":24,"deletions":0,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -1725,0 +1725,31 @@\n+\n+void C2_MacroAssembler::rvv_compare(VectorRegister vd, BasicType bt, int length_in_bytes, VectorRegister src1, VectorRegister src2, int cond, VectorMask vm) {\n+  rvv_vsetvli(bt, length_in_bytes);\n+  vmxor_mm(vd, vd, vd);\n+  if (bt == T_FLOAT || bt == T_DOUBLE) {\n+    switch (cond) {\n+      case BoolTest::eq: vmfeq_vv(vd, src1, src2, vm); break;\n+      case BoolTest::ne: vmfne_vv(vd, src1, src2, vm); break;\n+      case BoolTest::le: vmfle_vv(vd, src1, src2, vm); break;\n+      case BoolTest::ge: vmfle_vv(vd, src2, src1, vm); break;\n+      case BoolTest::lt: vmflt_vv(vd, src1, src2, vm); break;\n+      case BoolTest::gt: vmflt_vv(vd, src2, src1, vm); break;\n+      default:\n+        assert(false, \"unsupported compare condition\");\n+        ShouldNotReachHere();\n+    }\n+  } else {\n+    assert(is_integral_type(bt), \"unsupported element type\");\n+    switch (cond) {\n+      case BoolTest::eq: vmseq_vv(vd, src1, src2, vm); break;\n+      case BoolTest::ne: vmsne_vv(vd, src1, src2, vm); break;\n+      case BoolTest::le: vmsle_vv(vd, src1, src2, vm); break;\n+      case BoolTest::ge: vmsle_vv(vd, src2, src1, vm); break;\n+      case BoolTest::lt: vmslt_vv(vd, src1, src2, vm); break;\n+      case BoolTest::gt: vmslt_vv(vd, src2, src1, vm); break;\n+      default:\n+        assert(false, \"unsupported compare condition\");\n+        ShouldNotReachHere();\n+    }\n+  }\n+}\n\\ No newline at end of file\n","filename":"src\/hotspot\/cpu\/riscv\/c2_MacroAssembler_riscv.cpp","additions":31,"deletions":0,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -140,4 +140,6 @@\n-  void spill_copy_vector_stack_to_stack(int src_offset, int dst_offset, int vec_reg_size_in_bytes) {\n-    assert(vec_reg_size_in_bytes % 16 == 0, \"unexpected vector reg size\");\n-    unspill(v0, src_offset);\n-    spill(v0, dst_offset);\n+  void spill_copy_vector_stack_to_stack(int src_offset, int dst_offset, int vector_length_in_bytes) {\n+    assert(vector_length_in_bytes % 16 == 0, \"unexpected vector reg size\");\n+    for (int i = 0; i < vector_length_in_bytes \/ 8; i++) {\n+      unspill(t0, true, src_offset + (i * 8));\n+      spill(t0, true, dst_offset + (i * 8));\n+    }\n@@ -201,0 +203,26 @@\n+ void rvv_compare(VectorRegister dst, BasicType bt, int length_in_bytes,\n+                  VectorRegister src1, VectorRegister src2, int cond, VectorMask vm = Assembler::unmasked);\n+\n+ \/\/ In Matcher::scalable_predicate_reg_slots,\n+ \/\/ we assume each predicate register is one-eighth of the size of\n+ \/\/ scalable vector register, one mask bit per vector byte.\n+ void spill_vmask(VectorRegister v, int offset){\n+   rvv_vsetvli(T_BYTE, MaxVectorSize >> 3);\n+   add(t0, sp, offset);\n+   vse8_v(v, t0);\n+ }\n+\n+ void unspill_vmask(VectorRegister v, int offset){\n+   rvv_vsetvli(T_BYTE, MaxVectorSize >> 3);\n+   add(t0, sp, offset);\n+   vle8_v(v, t0);\n+ }\n+\n+  void spill_copy_vmask_stack_to_stack(int src_offset, int dst_offset, int vector_length_in_bytes) {\n+    assert(vector_length_in_bytes % 4 == 0, \"unexpected vector mask reg size\");\n+    for (int i = 0; i < vector_length_in_bytes \/ 4; i++) {\n+      unspill(t0, false, src_offset + (i * 4));\n+      spill(t0, false, dst_offset + (i * 4));\n+    }\n+  }\n+\n","filename":"src\/hotspot\/cpu\/riscv\/c2_MacroAssembler_riscv.hpp","additions":32,"deletions":4,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -152,1 +152,1 @@\n-    return false;\n+    return UseRVV;\n","filename":"src\/hotspot\/cpu\/riscv\/matcher_riscv.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -833,1 +833,2 @@\n-\/\/ Class for all RVV vector registers\n+\/\/ Class for RVV vector registers\n+\/\/ Note: v0, v30 and v31 are used as mask registers.\n@@ -863,3 +864,1 @@\n-    V29, V29_H, V29_J, V29_K,\n-    V30, V30_H, V30_J, V30_K,\n-    V31, V31_H, V31_J, V31_K\n+    V29, V29_H, V29_J, V29_K\n@@ -915,0 +914,17 @@\n+\n+\/\/ Class for RVV v0 mask register\n+\/\/ https:\/\/github.com\/riscv\/riscv-v-spec\/blob\/master\/v-spec.adoc#53-vector-masking\n+\/\/ The mask value used to control execution of a masked vector\n+\/\/ instruction is always supplied by vector register v0.\n+reg_class vectmask_reg_v0 (\n+    V0\n+);\n+\n+\/\/ Class for RVV mask registers\n+\/\/ We need two more vmask registers to do the vector mask logical ops,\n+\/\/ so define v30, v31 as mask register too.\n+reg_class vectmask_reg (\n+    V0,\n+    V30,\n+    V31\n+);\n@@ -1525,1 +1541,1 @@\n-  if (src_hi != OptoReg::Bad) {\n+  if (src_hi != OptoReg::Bad && !bottom_type()->isa_vectmask()) {\n@@ -1561,0 +1577,19 @@\n+    } else if (bottom_type()->isa_vectmask() && cbuf) {\n+      C2_MacroAssembler _masm(cbuf);\n+      int vmask_size_in_bytes = Matcher::scalable_predicate_reg_slots() * 32 \/ 8;\n+      if (src_lo_rc == rc_stack && dst_lo_rc == rc_stack) {\n+        \/\/ stack to stack\n+        __ spill_copy_vmask_stack_to_stack(src_offset, dst_offset,\n+                                           vmask_size_in_bytes);\n+      } else if (src_lo_rc == rc_vector && dst_lo_rc == rc_stack) {\n+        \/\/ vmask to stack\n+        __ spill_vmask(as_VectorRegister(Matcher::_regEncode[src_lo]), ra_->reg2offset(dst_lo));\n+      } else if (src_lo_rc == rc_stack && dst_lo_rc == rc_vector) {\n+        \/\/ stack to vmask\n+        __ unspill_vmask(as_VectorRegister(Matcher::_regEncode[dst_lo]), ra_->reg2offset(src_lo));\n+      } else if (src_lo_rc == rc_vector && dst_lo_rc == rc_vector) {\n+        \/\/ vmask to vmask\n+        __ vmv1r_v(as_VectorRegister(Matcher::_regEncode[dst_lo]), as_VectorRegister(Matcher::_regEncode[src_lo]));\n+      } else {\n+        ShouldNotReachHere();\n+      }\n@@ -1645,1 +1680,1 @@\n-    if (bottom_type()->isa_vect() != NULL) {\n+    if (bottom_type()->isa_vect() && !bottom_type()->isa_vectmask()) {\n@@ -1653,0 +1688,4 @@\n+    } else if (ideal_reg() == Op_RegVectMask) {\n+      assert(Matcher::supports_scalable_vector(), \"bad register type for spill\");\n+      int vsize = Matcher::scalable_predicate_reg_slots() * 32;\n+      st->print(\"\\t# vmask spill size = %d\", vsize);\n@@ -1868,1 +1907,33 @@\n-  return false;\n+  if (!UseRVV) {\n+    return false;\n+  }\n+  switch (opcode) {\n+    case Op_AddVB:\n+    case Op_AddVS:\n+    case Op_AddVI:\n+    case Op_AddVL:\n+    case Op_AddVF:\n+    case Op_AddVD:\n+    case Op_SubVB:\n+    case Op_SubVS:\n+    case Op_SubVI:\n+    case Op_SubVL:\n+    case Op_SubVF:\n+    case Op_SubVD:\n+    case Op_MulVB:\n+    case Op_MulVS:\n+    case Op_MulVI:\n+    case Op_MulVL:\n+    case Op_MulVF:\n+    case Op_MulVD:\n+    case Op_DivVF:\n+    case Op_DivVD:\n+    case Op_VectorLoadMask:\n+    case Op_VectorMaskCmp:\n+    case Op_AndVMask:\n+    case Op_XorVMask:\n+    case Op_OrVMask:\n+      return match_rule_supported_vector(opcode, vlen, bt);\n+    default:\n+      return false;\n+  }\n@@ -1876,1 +1947,1 @@\n-  return NULL;\n+  return &_VECTMASK_REG_mask;\n@@ -1880,1 +1951,1 @@\n-  return NULL;\n+  return new TypeVectMask(elemTy, length);\n@@ -3507,0 +3578,22 @@\n+\/\/ The mask value used to control execution of a masked\n+\/\/ vector instruction is always supplied by vector register v0.\n+operand vRegMask_V0()\n+%{\n+  constraint(ALLOC_IN_RC(vectmask_reg_v0));\n+  match(RegVectMask);\n+  match(vRegMask);\n+  op_cost(0);\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+operand vRegMask()\n+%{\n+  constraint(ALLOC_IN_RC(vectmask_reg));\n+  match(RegVectMask);\n+  match(vRegMask_V0);\n+  op_cost(0);\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n","filename":"src\/hotspot\/cpu\/riscv\/riscv.ad","additions":102,"deletions":9,"binary":false,"changes":111,"status":"modified"},{"patch":"@@ -78,1 +78,0 @@\n-      case Op_VectorLoadMask:\n@@ -80,1 +79,0 @@\n-      case Op_VectorMaskCmp:\n@@ -83,1 +81,0 @@\n-      case Op_VectorStoreMask:\n@@ -126,0 +123,63 @@\n+\/\/ vector load mask\n+\n+instruct vloadmask(vRegMask dst, vReg src) %{\n+  match(Set dst (VectorLoadMask src));\n+  format %{ \"vloadmask $dst, $src\" %}\n+  ins_encode %{\n+    __ vsetvli(t0, x0, Assembler::e8);\n+    __ vmsne_vx(as_VectorRegister($dst$$reg), as_VectorRegister($src$$reg), x0);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vloadmask_masked(vRegMask dst, vReg src, vRegMask_V0 vmask) %{\n+  match(Set dst (VectorLoadMask src vmask));\n+  format %{ \"vloadmask_masked $dst, $src, $vmask\" %}\n+  ins_encode %{\n+    __ vsetvli(t0, x0, Assembler::e8);\n+    __ vmsne_vx(as_VectorRegister($dst$$reg), as_VectorRegister($src$$reg), x0, Assembler::v0_t);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector store mask\n+\n+instruct vstoremask(vReg dst, vRegMask_V0 src, immI size) %{\n+  match(Set dst (VectorStoreMask src size));\n+  format %{ \"vstoremask $dst, $src\" %}\n+  ins_encode %{\n+    __ vsetvli(t0, x0, Assembler::e8);\n+    __ vmv_v_x(as_VectorRegister($dst$$reg), x0);\n+    __ vmerge_vim(as_VectorRegister($dst$$reg), as_VectorRegister($dst$$reg), 1);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector mask compare\n+\n+instruct vmaskcmp(vRegMask dst, vReg src1, vReg src2, immI cond) %{\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));\n+  format %{ \"vmaskcmp_rvv $dst, $src1, $src2, $cond\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this);\n+    __ rvv_compare(as_VectorRegister($dst$$reg), bt, length_in_bytes, as_VectorRegister($src1$$reg),\n+                   as_VectorRegister($src2$$reg), (int)($cond$$constant));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmaskcmp_masked(vRegMask dst, vReg src1, vReg src2, immI cond, vRegMask_V0 vmask, vReg tmp) %{\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) (Binary cond vmask)));\n+  effect(TEMP tmp);\n+  format %{ \"vmaskcmp_rvv_masked $dst, $src1, $src2, $vmask, $tmp, $cond\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this);\n+    __ rvv_compare(as_VectorRegister($tmp$$reg), bt, length_in_bytes, as_VectorRegister($src1$$reg),\n+                   as_VectorRegister($src2$$reg), (int)($cond$$constant), Assembler::v0_t);\n+    __ vmv1r_v(as_VectorRegister($dst$$reg), as_VectorRegister($tmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -286,0 +346,80 @@\n+\/\/ vector add - predicated\n+\n+instruct vaddB_masked(vReg dst_src1, vReg src2, vRegMask_V0 vmask) %{\n+  match(Set dst_src1 (AddVB (Binary dst_src1 src2) vmask));\n+  ins_cost(VEC_COST);\n+  format %{ \"vadd.vv $dst_src1, $src2, $vmask\\t#@vadd_masked\" %}\n+  ins_encode %{\n+    __ rvv_vsetvli(T_BYTE, Matcher::vector_length_in_bytes(this));\n+    __ vadd_vv(as_VectorRegister($dst_src1$$reg),\n+               as_VectorRegister($dst_src1$$reg),\n+               as_VectorRegister($src2$$reg), Assembler::v0_t);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vaddS_masked(vReg dst_src1, vReg src2, vRegMask_V0 vmask) %{\n+  match(Set dst_src1 (AddVS (Binary dst_src1 src2) vmask));\n+  ins_cost(VEC_COST);\n+  format %{ \"vadd.vv $dst_src1, $src2, $vmask\\t#@vaddS_masked\" %}\n+  ins_encode %{\n+    __ rvv_vsetvli(T_SHORT, Matcher::vector_length_in_bytes(this));\n+    __ vadd_vv(as_VectorRegister($dst_src1$$reg),\n+               as_VectorRegister($dst_src1$$reg),\n+               as_VectorRegister($src2$$reg), Assembler::v0_t);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vaddI_masked(vReg dst_src1, vReg src2, vRegMask_V0 vmask) %{\n+  match(Set dst_src1 (AddVI (Binary dst_src1 src2) vmask));\n+  ins_cost(VEC_COST);\n+  format %{ \"vadd.vv $dst_src1, $src2, $vmask\\t#@vaddI_masked\" %}\n+  ins_encode %{\n+    __ rvv_vsetvli(T_INT, Matcher::vector_length_in_bytes(this));\n+    __ vadd_vv(as_VectorRegister($dst_src1$$reg),\n+               as_VectorRegister($dst_src1$$reg),\n+               as_VectorRegister($src2$$reg), Assembler::v0_t);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vaddL_masked(vReg dst_src1, vReg src2, vRegMask_V0 vmask) %{\n+  match(Set dst_src1 (AddVL (Binary dst_src1 src2) vmask));\n+  ins_cost(VEC_COST);\n+  format %{ \"vadd.vv $dst_src1, $src2, $vmask\\t#@vaddL_masked\" %}\n+  ins_encode %{\n+    __ rvv_vsetvli(T_LONG, Matcher::vector_length_in_bytes(this));\n+    __ vadd_vv(as_VectorRegister($dst_src1$$reg),\n+               as_VectorRegister($dst_src1$$reg),\n+               as_VectorRegister($src2$$reg), Assembler::v0_t);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vaddF_masked(vReg dst_src1, vReg src2, vRegMask_V0 vmask) %{\n+  match(Set dst_src1 (AddVF (Binary dst_src1 src2) vmask));\n+  ins_cost(VEC_COST);\n+  format %{ \"vfadd.vv $dst_src1, $src2, $vmask\\t#@vaddF_masked\" %}\n+  ins_encode %{\n+    __ rvv_vsetvli(T_FLOAT, Matcher::vector_length_in_bytes(this));\n+    __ vfadd_vv(as_VectorRegister($dst_src1$$reg),\n+                as_VectorRegister($dst_src1$$reg),\n+                as_VectorRegister($src2$$reg), Assembler::v0_t);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vaddD_masked(vReg dst_src1, vReg src2, vRegMask_V0 vmask) %{\n+  match(Set dst_src1 (AddVD (Binary dst_src1 src2) vmask));\n+  ins_cost(VEC_COST);\n+  format %{ \"vfadd.vv $dst_src1, $src2, $vmask\\t#@vaddD_masked\" %}\n+  ins_encode %{\n+    __ rvv_vsetvli(T_DOUBLE, Matcher::vector_length_in_bytes(this));\n+    __ vfadd_vv(as_VectorRegister($dst_src1$$reg),\n+                as_VectorRegister($dst_src1$$reg),\n+                as_VectorRegister($src2$$reg), Assembler::v0_t);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -359,0 +499,28 @@\n+\/\/ vector float div - predicated\n+\n+instruct vdivF_masked(vReg dst_src1, vReg src2, vRegMask_V0 vmask) %{\n+  match(Set dst_src1 (DivVF (Binary dst_src1 src2) vmask));\n+  ins_cost(VEC_COST);\n+  format %{ \"vfdiv.vv  $dst_src1, $src2, $vmask\\t#@vdivF_masked\" %}\n+  ins_encode %{\n+    __ rvv_vsetvli(T_FLOAT, Matcher::vector_length_in_bytes(this));\n+    __ vfdiv_vv(as_VectorRegister($dst_src1$$reg),\n+                as_VectorRegister($dst_src1$$reg),\n+                as_VectorRegister($src2$$reg), Assembler::v0_t);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vdivD_masked(vReg dst_src1, vReg src2, vRegMask_V0 vmask) %{\n+  match(Set dst_src1 (DivVD (Binary dst_src1 src2) vmask));\n+  ins_cost(VEC_COST);\n+  format %{ \"vfdiv.vv  $dst_src1, $src2, $vmask\\t#@vdivD_masked\" %}\n+  ins_encode %{\n+    __ rvv_vsetvli(T_DOUBLE, Matcher::vector_length_in_bytes(this));\n+    __ vfdiv_vv(as_VectorRegister($dst_src1$$reg),\n+                as_VectorRegister($dst_src1$$reg),\n+                as_VectorRegister($src2$$reg), Assembler::v0_t);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -759,0 +927,74 @@\n+\/\/ vector mul - predicated\n+\n+instruct vmulB_masked(vReg dst_src1, vReg src2, vRegMask_V0 vmask) %{\n+  match(Set dst_src1 (MulVB (Binary dst_src1 src2) vmask));\n+  ins_cost(VEC_COST);\n+  format %{ \"vmul.vv $dst_src1, $src2, $vmask\\t#@vmulB_masked\" %}\n+  ins_encode %{\n+    __ rvv_vsetvli(T_BYTE, Matcher::vector_length_in_bytes(this));\n+    __ vmul_vv(as_VectorRegister($dst_src1$$reg), as_VectorRegister($dst_src1$$reg),\n+               as_VectorRegister($src2$$reg), Assembler::v0_t);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmulS_masked(vReg dst_src1, vReg src2, vRegMask_V0 vmask) %{\n+  match(Set dst_src1 (MulVS (Binary dst_src1 src2) vmask));\n+  ins_cost(VEC_COST);\n+  format %{ \"vmul.vv $dst_src1, $src2, $vmask\\t#@vmulS_masked\" %}\n+  ins_encode %{\n+    __ rvv_vsetvli(T_SHORT, Matcher::vector_length_in_bytes(this));\n+    __ vmul_vv(as_VectorRegister($dst_src1$$reg), as_VectorRegister($dst_src1$$reg),\n+               as_VectorRegister($src2$$reg), Assembler::v0_t);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmulI_masked(vReg dst_src1, vReg src2, vRegMask_V0 vmask) %{\n+  match(Set dst_src1 (MulVI (Binary dst_src1 src2) vmask));\n+  ins_cost(VEC_COST);\n+  format %{ \"vmul.vv $dst_src1, $src2, $vmask\\t#@vmulI_masked\" %}\n+  ins_encode %{\n+    __ rvv_vsetvli(T_INT, Matcher::vector_length_in_bytes(this));\n+    __ vmul_vv(as_VectorRegister($dst_src1$$reg), as_VectorRegister($dst_src1$$reg),\n+               as_VectorRegister($src2$$reg), Assembler::v0_t);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmulL_masked(vReg dst_src1, vReg src2, vRegMask_V0 vmask) %{\n+  match(Set dst_src1 (MulVL (Binary dst_src1 src2) vmask));\n+  ins_cost(VEC_COST);\n+  format %{ \"vmul.vv $dst_src1, $src2, $vmask\\t#@vmulL_masked\" %}\n+  ins_encode %{\n+    __ rvv_vsetvli(T_LONG, Matcher::vector_length_in_bytes(this));\n+    __ vmul_vv(as_VectorRegister($dst_src1$$reg), as_VectorRegister($dst_src1$$reg),\n+               as_VectorRegister($src2$$reg), Assembler::v0_t);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmulF_masked(vReg dst_src1, vReg src2, vRegMask_V0 vmask) %{\n+  match(Set dst_src1 (MulVF (Binary dst_src1 src2) vmask));\n+  ins_cost(VEC_COST);\n+  format %{ \"vmul.vv $dst_src1, $src2, $vmask\\t#@vmulF_masked\" %}\n+  ins_encode %{\n+    __ rvv_vsetvli(T_FLOAT, Matcher::vector_length_in_bytes(this));\n+    __ vfmul_vv(as_VectorRegister($dst_src1$$reg), as_VectorRegister($dst_src1$$reg),\n+               as_VectorRegister($src2$$reg), Assembler::v0_t);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmulD_masked(vReg dst_src1, vReg src2, vRegMask_V0 vmask) %{\n+  match(Set dst_src1 (MulVD (Binary dst_src1 src2) vmask));\n+  ins_cost(VEC_COST);\n+  format %{ \"vmul.vv $dst_src1, $src2, $vmask\\t#@vmulD_masked\" %}\n+  ins_encode %{\n+    __ rvv_vsetvli(T_DOUBLE, Matcher::vector_length_in_bytes(this));\n+    __ vfmul_vv(as_VectorRegister($dst_src1$$reg), as_VectorRegister($dst_src1$$reg),\n+               as_VectorRegister($src2$$reg), Assembler::v0_t);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -1268,1 +1510,1 @@\n-instruct vasrB(vReg dst, vReg src, vReg shift) %{\n+instruct vasrB(vReg dst, vReg src, vReg shift, vRegMask_V0 vmask) %{\n@@ -1279,1 +1521,1 @@\n-    __ vmsgtu_vi(v0, as_VectorRegister($shift$$reg), BitsPerByte - 1);\n+    __ vmsgtu_vi(as_VectorRegister($vmask$$reg), as_VectorRegister($shift$$reg), BitsPerByte - 1);\n@@ -1283,1 +1525,1 @@\n-    __ vmnot_m(v0, v0);\n+    __ vmnot_m(as_VectorRegister($vmask$$reg), as_VectorRegister($vmask$$reg));\n@@ -1290,1 +1532,1 @@\n-instruct vasrS(vReg dst, vReg src, vReg shift) %{\n+instruct vasrS(vReg dst, vReg src, vReg shift, vRegMask_V0 vmask) %{\n@@ -1301,1 +1543,1 @@\n-    __ vmsgtu_vi(v0, as_VectorRegister($shift$$reg), BitsPerShort - 1);\n+    __ vmsgtu_vi(as_VectorRegister($vmask$$reg), as_VectorRegister($shift$$reg), BitsPerShort - 1);\n@@ -1305,1 +1547,1 @@\n-    __ vmnot_m(v0, v0);\n+    __ vmnot_m(as_VectorRegister($vmask$$reg), as_VectorRegister($vmask$$reg));\n@@ -1336,1 +1578,1 @@\n-instruct vlslB(vReg dst, vReg src, vReg shift) %{\n+instruct vlslB(vReg dst, vReg src, vReg shift, vRegMask_V0 vmask) %{\n@@ -1347,1 +1589,1 @@\n-    __ vmsgtu_vi(v0, as_VectorRegister($shift$$reg), BitsPerByte - 1);\n+    __ vmsgtu_vi(as_VectorRegister($vmask$$reg), as_VectorRegister($shift$$reg), BitsPerByte - 1);\n@@ -1351,1 +1593,1 @@\n-    __ vmnot_m(v0, v0);\n+    __ vmnot_m(as_VectorRegister($vmask$$reg), as_VectorRegister($vmask$$reg));\n@@ -1358,1 +1600,1 @@\n-instruct vlslS(vReg dst, vReg src, vReg shift) %{\n+instruct vlslS(vReg dst, vReg src, vReg shift, vRegMask_V0 vmask) %{\n@@ -1369,1 +1611,1 @@\n-    __ vmsgtu_vi(v0, as_VectorRegister($shift$$reg), BitsPerShort - 1);\n+    __ vmsgtu_vi(as_VectorRegister($vmask$$reg), as_VectorRegister($shift$$reg), BitsPerShort - 1);\n@@ -1373,1 +1615,1 @@\n-    __ vmnot_m(v0, v0);\n+    __ vmnot_m(as_VectorRegister($vmask$$reg), as_VectorRegister($vmask$$reg));\n@@ -1404,1 +1646,1 @@\n-instruct vlsrB(vReg dst, vReg src, vReg shift) %{\n+instruct vlsrB(vReg dst, vReg src, vReg shift, vRegMask_V0 vmask) %{\n@@ -1410,1 +1652,1 @@\n-            \"vmnot.m v0, v0, v0\\n\\t\"\n+            \"vmnot.m v0, v0\\n\\t\"\n@@ -1415,1 +1657,1 @@\n-    __ vmsgtu_vi(v0, as_VectorRegister($shift$$reg), BitsPerByte - 1);\n+    __ vmsgtu_vi(as_VectorRegister($vmask$$reg), as_VectorRegister($shift$$reg), BitsPerByte - 1);\n@@ -1419,1 +1661,1 @@\n-    __ vmnot_m(v0, v0);\n+    __ vmnot_m(as_VectorRegister($vmask$$reg), as_VectorRegister($vmask$$reg));\n@@ -1426,1 +1668,1 @@\n-instruct vlsrS(vReg dst, vReg src, vReg shift) %{\n+instruct vlsrS(vReg dst, vReg src, vReg shift, vRegMask_V0 vmask) %{\n@@ -1437,1 +1679,1 @@\n-    __ vmsgtu_vi(v0, as_VectorRegister($shift$$reg), BitsPerShort - 1);\n+    __ vmsgtu_vi(as_VectorRegister($vmask$$reg), as_VectorRegister($shift$$reg), BitsPerShort - 1);\n@@ -1441,1 +1683,1 @@\n-    __ vmnot_m(v0, v0);\n+    __ vmnot_m(as_VectorRegister($vmask$$reg), as_VectorRegister($vmask$$reg));\n@@ -1830,0 +2072,74 @@\n+\/\/ vector sub - predicated\n+\n+instruct vsubB_masked(vReg dst_src1, vReg src2, vRegMask_V0 vmask) %{\n+  match(Set dst_src1 (SubVB (Binary dst_src1 src2) vmask));\n+  ins_cost(VEC_COST);\n+  format %{ \"vsub.vv $dst_src1, $src2, $vmask\\t#@vsubB_masked\" %}\n+  ins_encode %{\n+    __ rvv_vsetvli(T_BYTE, Matcher::vector_length_in_bytes(this));\n+    __ vsub_vv(as_VectorRegister($dst_src1$$reg), as_VectorRegister($dst_src1$$reg),\n+               as_VectorRegister($src2$$reg), Assembler::v0_t);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vsubS_masked(vReg dst_src1, vReg src2, vRegMask_V0 vmask) %{\n+  match(Set dst_src1 (SubVS (Binary dst_src1 src2) vmask));\n+  ins_cost(VEC_COST);\n+  format %{ \"vsub.vv $dst_src1, $src2, $vmask\\t#@vsubS_masked\" %}\n+  ins_encode %{\n+    __ rvv_vsetvli(T_SHORT, Matcher::vector_length_in_bytes(this));\n+    __ vsub_vv(as_VectorRegister($dst_src1$$reg), as_VectorRegister($dst_src1$$reg),\n+               as_VectorRegister($src2$$reg), Assembler::v0_t);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vsubI_masked(vReg dst_src1, vReg src2, vRegMask_V0 vmask) %{\n+  match(Set dst_src1 (SubVI (Binary dst_src1 src2) vmask));\n+  ins_cost(VEC_COST);\n+  format %{ \"vsub.vv $dst_src1, $src2, $vmask\\t#@vsubI_masked\" %}\n+  ins_encode %{\n+    __ rvv_vsetvli(T_INT, Matcher::vector_length_in_bytes(this));\n+    __ vsub_vv(as_VectorRegister($dst_src1$$reg), as_VectorRegister($dst_src1$$reg),\n+               as_VectorRegister($src2$$reg), Assembler::v0_t);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vsubL_masked(vReg dst_src1, vReg src2, vRegMask_V0 vmask) %{\n+  match(Set dst_src1 (SubVL (Binary dst_src1 src2) vmask));\n+  ins_cost(VEC_COST);\n+  format %{ \"vsub.vv $dst_src1, $src2, $vmask\\t#@vsubL_masked\" %}\n+  ins_encode %{\n+    __ rvv_vsetvli(T_LONG, Matcher::vector_length_in_bytes(this));\n+    __ vsub_vv(as_VectorRegister($dst_src1$$reg), as_VectorRegister($dst_src1$$reg),\n+               as_VectorRegister($src2$$reg), Assembler::v0_t);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vsubF_masked(vReg dst_src1, vReg src2, vRegMask_V0 vmask) %{\n+  match(Set dst_src1 (SubVF (Binary dst_src1 src2) vmask));\n+  ins_cost(VEC_COST);\n+  format %{ \"vfsub.vv $dst_src1, $src2, $vmask\\t#@vsubF_masked\" %}\n+  ins_encode %{\n+    __ rvv_vsetvli(T_FLOAT, Matcher::vector_length_in_bytes(this));\n+    __ vfsub_vv(as_VectorRegister($dst_src1$$reg), as_VectorRegister($dst_src1$$reg),\n+                as_VectorRegister($src2$$reg), Assembler::v0_t);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vsubD_masked(vReg dst_src1, vReg src2, vRegMask_V0 vmask) %{\n+  match(Set dst_src1 (SubVD (Binary dst_src1 src2) vmask));\n+  ins_cost(VEC_COST);\n+  format %{ \"vfsub.vv $dst_src1, $src2, $vmask\\t#@vsubD_masked\" %}\n+  ins_encode %{\n+    __ rvv_vsetvli(T_DOUBLE, Matcher::vector_length_in_bytes(this));\n+    __ vfsub_vv(as_VectorRegister($dst_src1$$reg), as_VectorRegister($dst_src1$$reg),\n+                as_VectorRegister($src2$$reg), Assembler::v0_t);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -1832,1 +2148,1 @@\n-                         vReg_V2 v2, vReg_V3 v3, rFlagsReg cr)\n+                         vReg_V2 v2, vReg_V3 v3, vRegMask_V0 v0, rFlagsReg cr)\n@@ -1836,1 +2152,1 @@\n-  effect(USE_KILL str1, USE_KILL str2, USE_KILL cnt, TEMP v1, TEMP v2, TEMP v3, KILL cr);\n+  effect(USE_KILL str1, USE_KILL str2, USE_KILL cnt, TEMP v1, TEMP v2, TEMP v3, TEMP v0, KILL cr);\n@@ -1849,1 +2165,1 @@\n-                         vReg_V2 v2, vReg_V3 v3, rFlagsReg cr)\n+                         vReg_V2 v2, vReg_V3 v3, vRegMask_V0 v0, rFlagsReg cr)\n@@ -1853,1 +2169,1 @@\n-  effect(USE_KILL str1, USE_KILL str2, USE_KILL cnt, TEMP v1, TEMP v2, TEMP v3, KILL cr);\n+  effect(USE_KILL str1, USE_KILL str2, USE_KILL cnt, TEMP v1, TEMP v2, TEMP v3, TEMP v0, KILL cr);\n@@ -1865,1 +2181,1 @@\n-                        vReg_V1 v1, vReg_V2 v2, vReg_V3 v3, iRegP_R28 tmp, rFlagsReg cr)\n+                        vReg_V1 v1, vReg_V2 v2, vReg_V3 v3, vRegMask_V0 v0, iRegP_R28 tmp, rFlagsReg cr)\n@@ -1869,1 +2185,1 @@\n-  effect(KILL tmp, USE_KILL ary1, USE_KILL ary2, TEMP v1, TEMP v2, TEMP v3, KILL cr);\n+  effect(KILL tmp, USE_KILL ary1, USE_KILL ary2, TEMP v1, TEMP v2, TEMP v3, TEMP v0, KILL cr);\n@@ -1880,1 +2196,1 @@\n-                        vReg_V1 v1, vReg_V2 v2, vReg_V3 v3, iRegP_R28 tmp, rFlagsReg cr)\n+                        vReg_V1 v1, vReg_V2 v2, vReg_V3 v3, vRegMask_V0 v0, iRegP_R28 tmp, rFlagsReg cr)\n@@ -1884,1 +2200,1 @@\n-  effect(KILL tmp, USE_KILL ary1, USE_KILL ary2, TEMP v1, TEMP v2, TEMP v3, KILL cr);\n+  effect(KILL tmp, USE_KILL ary1, USE_KILL ary2, TEMP v1, TEMP v2, TEMP v3, TEMP v0, KILL cr);\n@@ -1896,1 +2212,1 @@\n-                          iRegP_R28 tmp1, iRegL_R29 tmp2)\n+                          vRegMask_V0 v0, iRegP_R28 tmp1, iRegL_R29 tmp2)\n@@ -1901,1 +2217,1 @@\n-         TEMP v1, TEMP v2, TEMP v3, TEMP v4, TEMP v5);\n+         TEMP v1, TEMP v2, TEMP v3, TEMP v4, TEMP v5, TEMP v0);\n@@ -1915,1 +2231,1 @@\n-                          iRegP_R28 tmp1, iRegL_R29 tmp2)\n+                          vRegMask_V0 v0, iRegP_R28 tmp1, iRegL_R29 tmp2)\n@@ -1920,1 +2236,1 @@\n-         TEMP v1, TEMP v2, TEMP v3, TEMP v4, TEMP v5);\n+         TEMP v1, TEMP v2, TEMP v3, TEMP v4, TEMP v5, TEMP v0);\n@@ -1934,1 +2250,1 @@\n-                           iRegP_R28 tmp1, iRegL_R29 tmp2)\n+                           vRegMask_V0 v0, iRegP_R28 tmp1, iRegL_R29 tmp2)\n@@ -1939,1 +2255,1 @@\n-         TEMP v1, TEMP v2, TEMP v3, TEMP v4, TEMP v5);\n+         TEMP v1, TEMP v2, TEMP v3, TEMP v4, TEMP v5, TEMP v0);\n@@ -1952,1 +2268,1 @@\n-                           iRegP_R28 tmp1, iRegL_R29 tmp2)\n+                           vRegMask_V0 v0, iRegP_R28 tmp1, iRegL_R29 tmp2)\n@@ -1957,1 +2273,1 @@\n-         TEMP v1, TEMP v2, TEMP v3, TEMP v4, TEMP v5);\n+         TEMP v1, TEMP v2, TEMP v3, TEMP v4, TEMP v5, TEMP v0);\n@@ -1971,1 +2287,1 @@\n-                         vReg_V1 v1, vReg_V2 v2, vReg_V3 v3, iRegLNoSp tmp)\n+                         vReg_V1 v1, vReg_V2 v2, vReg_V3 v3, vRegMask_V0 v0, iRegLNoSp tmp)\n@@ -1975,1 +2291,1 @@\n-  effect(TEMP v1, TEMP v2, TEMP v3, TEMP tmp, USE_KILL src, USE_KILL dst, USE_KILL len);\n+  effect(TEMP v1, TEMP v2, TEMP v3, TEMP v0, TEMP tmp, USE_KILL src, USE_KILL dst, USE_KILL len);\n@@ -1986,1 +2302,1 @@\n-                           vReg_V1 v1, vReg_V2 v2, vReg_V3 v3, iRegLNoSp tmp)\n+                           vReg_V1 v1, vReg_V2 v2, vReg_V3 v3, vRegMask_V0 v0, iRegLNoSp tmp)\n@@ -1991,1 +2307,1 @@\n-         TEMP v1, TEMP v2, TEMP v3, TEMP tmp);\n+         TEMP v1, TEMP v2, TEMP v3, TEMP tmp, TEMP v0);\n@@ -2003,1 +2319,1 @@\n-                          vReg_V1 v1, vReg_V2 v2, vReg_V3 v3, iRegLNoSp tmp)\n+                          vReg_V1 v1, vReg_V2 v2, vReg_V3 v3, vRegMask_V0 v0, iRegLNoSp tmp)\n@@ -2008,1 +2324,1 @@\n-         TEMP v1, TEMP v2, TEMP v3, TEMP tmp);\n+         TEMP v1, TEMP v2, TEMP v3, TEMP tmp, TEMP v0);\n@@ -2019,1 +2335,1 @@\n-                          vReg_V1 v1, vReg_V2 v2, vReg_V3 v3, iRegLNoSp tmp)\n+                          vReg_V1 v1, vReg_V2 v2, vReg_V3 v3, vRegMask_V0 v0, iRegLNoSp tmp)\n@@ -2023,1 +2339,1 @@\n-  effect(TEMP_DEF result, USE_KILL ary, USE_KILL len, TEMP v1, TEMP v2, TEMP v3, TEMP tmp);\n+  effect(TEMP_DEF result, USE_KILL ary, USE_KILL len, TEMP v1, TEMP v2, TEMP v3, TEMP tmp, TEMP v0);\n@@ -2035,1 +2351,1 @@\n-                               vReg_V1 v1, vReg_V2 v2, vReg_V3 v3)\n+                               vReg_V1 v1, vReg_V2 v2, vReg_V3 v3, vRegMask_V0 v0)\n@@ -2040,1 +2356,1 @@\n-         TEMP tmp1, TEMP tmp2, TEMP v1, TEMP v2, TEMP v3);\n+         TEMP tmp1, TEMP tmp2, TEMP v1, TEMP v2, TEMP v3, TEMP v0);\n@@ -2055,1 +2371,1 @@\n-                               vReg_V1 v1, vReg_V2 v2, vReg_V3 v3)\n+                               vReg_V1 v1, vReg_V2 v2, vReg_V3 v3, vRegMask_V0 v0)\n@@ -2060,1 +2376,1 @@\n-         TEMP tmp1, TEMP tmp2, TEMP v1, TEMP v2, TEMP v3);\n+         TEMP tmp1, TEMP tmp2, TEMP v1, TEMP v2, TEMP v3, TEMP v0);\n@@ -2075,1 +2391,1 @@\n-                             vReg_V1 vReg1, vReg_V2 vReg2, vReg_V3 vReg3)\n+                             vReg_V1 vReg1, vReg_V2 vReg2, vReg_V3 vReg3, vRegMask_V0 v0)\n@@ -2079,1 +2395,1 @@\n-  effect(USE_KILL cnt, USE_KILL base, TEMP vReg1, TEMP vReg2, TEMP vReg3);\n+  effect(USE_KILL cnt, USE_KILL base, TEMP vReg1, TEMP vReg2, TEMP vReg3, TEMP v0);\n@@ -2104,0 +2420,148 @@\n+%}\n+\n+instruct vmask_gen_I(vRegMask dst, iRegI src) %{\n+  match(Set dst (VectorMaskGen (ConvI2L src)));\n+  format %{ \"vmask_gen_I $dst, $src\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SEW sew = Assembler::elemtype_to_sew(bt);\n+    __ vsetvli(t0, $src$$Register, sew);\n+    __ vmxnor_mm(as_VectorRegister($dst$$reg), as_VectorRegister($dst$$reg), as_VectorRegister($dst$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmask_gen_L(vRegMask dst, iRegL src) %{\n+  match(Set dst (VectorMaskGen src));\n+  format %{ \"vmask_gen_L $dst, $src\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SEW sew = Assembler::elemtype_to_sew(bt);\n+    __ vsetvli(t0, $src$$Register, sew);\n+    __ vmxnor_mm(as_VectorRegister($dst$$reg), as_VectorRegister($dst$$reg), as_VectorRegister($dst$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmask_gen_imm(vRegMask dst, immL con) %{\n+  match(Set dst (VectorMaskGen con));\n+  format %{ \"vmask_gen_imm $dst, $con\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ rvv_vsetvli(bt, (uint)($con$$constant));\n+    __ vmxnor_mm(as_VectorRegister($dst$$reg), as_VectorRegister($dst$$reg), as_VectorRegister($dst$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmaskAll_immI(vRegMask dst, immI src) %{\n+  match(Set dst (MaskAll src));\n+  format %{ \"vmaskAll_immI $dst, $src\" %}\n+  ins_encode %{\n+    int con = (int)$src$$constant;\n+    if (con == 0) {\n+      __ vsetvli(t0, x0, Assembler::e8);\n+      __ vmxor_mm(as_VectorRegister($dst$$reg), as_VectorRegister($dst$$reg), as_VectorRegister($dst$$reg));\n+    } else {\n+      assert(con == -1, \"invalid constant value for mask\");\n+      __ vsetvli(t0, x0, Assembler::e8);\n+      __ vid_v(as_VectorRegister($dst$$reg));\n+      __ mv(t0, Matcher::vector_length(this));\n+      __ vmslt_vx(as_VectorRegister($dst$$reg), as_VectorRegister($dst$$reg), t0);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmaskAllI(vRegMask dst, iRegI src, vReg tmp) %{\n+  match(Set dst (MaskAll src));\n+  effect(TEMP tmp);\n+  format %{ \"vmaskAllI $dst, $src\\t# KILL $tmp\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ rvv_vsetvli(bt, Matcher::vector_length_in_bytes(this));\n+    __ vmv_v_x(as_VectorRegister($tmp$$reg), as_Register($src$$reg));\n+    __ vmsne_vx(as_VectorRegister($dst$$reg), as_VectorRegister($tmp$$reg), x0);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmaskAll_immL(vRegMask dst, immL src) %{\n+  match(Set dst (MaskAll src));\n+  format %{ \"vmaskAll_immL $dst, $src\" %}\n+  ins_encode %{\n+    long con = (long)$src$$constant;\n+    if (con == 0) {\n+      __ vsetvli(t0, x0, Assembler::e8);\n+      __ vmxor_mm(as_VectorRegister($dst$$reg), as_VectorRegister($dst$$reg), as_VectorRegister($dst$$reg));\n+    } else {\n+      assert(con == -1, \"invalid constant value for mask\");\n+      __ vsetvli(t0, x0, Assembler::e8);\n+      __ vid_v(as_VectorRegister($dst$$reg));\n+      __ mv(t0, Matcher::vector_length(this));\n+      __ vmslt_vx(as_VectorRegister($dst$$reg), as_VectorRegister($dst$$reg), t0);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmaskAllL(vRegMask dst, iRegL src, vReg tmp) %{\n+  match(Set dst (MaskAll src));\n+  effect(TEMP tmp);\n+  format %{ \"vmaskAllL $dst, $src\\t# KILL $tmp\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ rvv_vsetvli(bt, Matcher::vector_length_in_bytes(this));\n+    __ vmv_v_x(as_VectorRegister($tmp$$reg), as_Register($src$$reg));\n+    __ vmsne_vx(as_VectorRegister($dst$$reg), as_VectorRegister($tmp$$reg), x0);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ Vector mask basic OPs ------------------------\n+\n+\/\/ vector mask logical ops: and\/or\/xor\n+\n+instruct vmask_and(vRegMask dst, vRegMask src1, vRegMask src2) %{\n+  match(Set dst (AndVMask src1 src2));\n+  format %{ \"vmask_and $dst, $src1, $src2\" %}\n+  ins_encode %{\n+    __ rvv_vsetvli(T_BYTE, Matcher::vector_length_in_bytes(this));\n+    __ vmand_mm(as_VectorRegister($dst$$reg),\n+                as_VectorRegister($src1$$reg),\n+                as_VectorRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmask_or(vRegMask dst, vRegMask src1, vRegMask src2) %{\n+  match(Set dst (OrVMask src1 src2));\n+  format %{ \"vmask_or $dst, $src1, $src2\" %}\n+  ins_encode %{\n+    __ rvv_vsetvli(T_BYTE, Matcher::vector_length_in_bytes(this));\n+    __ vmor_mm(as_VectorRegister($dst$$reg),\n+               as_VectorRegister($src1$$reg),\n+               as_VectorRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmask_xor(vRegMask dst, vRegMask src1, vRegMask src2) %{\n+  match(Set dst (XorVMask src1 src2));\n+  format %{ \"vmask_xor $dst, $src1, $src2\" %}\n+  ins_encode %{\n+    __ rvv_vsetvli(T_BYTE, Matcher::vector_length_in_bytes(this));\n+    __ vmxor_mm(as_VectorRegister($dst$$reg),\n+                as_VectorRegister($src1$$reg),\n+                as_VectorRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmaskcast_same_esize_rvv(vRegMask dst_src) %{\n+  predicate(Matcher::vector_length_in_bytes(n) == Matcher::vector_length_in_bytes(n->in(1)));\n+  match(Set dst_src (VectorMaskCast dst_src));\n+  ins_cost(0);\n+  format %{ \"vmaskcast_same_esize_rvv $dst_src\\t# do nothing\" %}\n+  ins_encode(\/* empty encoding *\/);\n+  ins_pipe(pipe_class_empty);\n","filename":"src\/hotspot\/cpu\/riscv\/riscv_v.ad","additions":516,"deletions":52,"binary":false,"changes":568,"status":"modified"}]}