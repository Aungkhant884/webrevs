{"files":[{"patch":"@@ -931,0 +931,5 @@\n+\n+reg_class vmask_reg_no_v0 (\n+    V30,\n+    V31\n+);\n@@ -3600,22 +3605,0 @@\n-\/\/ The mask value used to control execution of a masked\n-\/\/ vector instruction is always supplied by vector register v0.\n-operand vRegMask_V0()\n-%{\n-  constraint(ALLOC_IN_RC(vmask_reg_v0));\n-  match(RegVectMask);\n-  match(vRegMask);\n-  op_cost(0);\n-  format %{ %}\n-  interface(REG_INTER);\n-%}\n-\n-operand vRegMask()\n-%{\n-  constraint(ALLOC_IN_RC(vmask_reg));\n-  match(RegVectMask);\n-  match(vRegMask_V0);\n-  op_cost(0);\n-  format %{ %}\n-  interface(REG_INTER);\n-%}\n-\n@@ -3672,0 +3655,33 @@\n+operand vRegMask()\n+%{\n+  constraint(ALLOC_IN_RC(vmask_reg));\n+  match(RegVectMask);\n+  match(vRegMaskNoV0);\n+  match(vRegMask_V0);\n+  op_cost(0);\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+operand vRegMaskNoV0()\n+%{\n+  constraint(ALLOC_IN_RC(vmask_reg_no_v0));\n+  match(RegVectMask);\n+  match(vRegMask);\n+  op_cost(0);\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ The mask value used to control execution of a masked\n+\/\/ vector instruction is always supplied by vector register v0.\n+operand vRegMask_V0()\n+%{\n+  constraint(ALLOC_IN_RC(vmask_reg_v0));\n+  match(RegVectMask);\n+  match(vRegMask);\n+  op_cost(0);\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n","filename":"src\/hotspot\/cpu\/riscv\/riscv.ad","additions":38,"deletions":22,"binary":false,"changes":60,"status":"modified"},{"patch":"@@ -138,3 +138,3 @@\n-instruct vloadmask_masked(vRegMask dst, vReg src, vRegMask_V0 vmask) %{\n-  match(Set dst (VectorLoadMask src vmask));\n-  format %{ \"vloadmask_masked $dst, $src, $vmask\" %}\n+instruct vloadmask_masked(vRegMask dst, vReg src, vRegMask_V0 v0) %{\n+  match(Set dst (VectorLoadMask src v0));\n+  format %{ \"vloadmask_masked $dst, $src, v0.t\" %}\n@@ -150,3 +150,3 @@\n-instruct vstoremask(vReg dst, vRegMask_V0 src, immI size) %{\n-  match(Set dst (VectorStoreMask src size));\n-  format %{ \"vstoremask $dst, $src\" %}\n+instruct vstoremask(vReg dst, vRegMask_V0 v0, immI size) %{\n+  match(Set dst (VectorStoreMask v0 size));\n+  format %{ \"vstoremask $dst, V0\" %}\n@@ -179,1 +179,3 @@\n-instruct vmaskcmp_masked(vRegMask dst, vReg src1, vReg src2, immI cond, vRegMask_V0 vmask, vReg tmp) %{\n+\/\/ Exclude mask register v0 for 'dst' as compare_integral_v clears 'dst'\n+\/\/ register on entry before using mask value held in v0.\n+instruct vmaskcmp_masked(vRegMaskNoV0 dst, vReg src1, vReg src2, immI cond, vRegMask_V0 v0) %{\n@@ -184,3 +186,2 @@\n-  match(Set dst (VectorMaskCmp (Binary src1 src2) (Binary cond vmask)));\n-  effect(TEMP tmp);\n-  format %{ \"vmaskcmp_masked $dst, $src1, $src2, $vmask, $tmp, $cond\" %}\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) (Binary cond v0)));\n+  format %{ \"vmaskcmp_masked $dst, $src1, $src2, $cond, v0.t\" %}\n@@ -190,1 +191,1 @@\n-    __ compare_integral_v(as_VectorRegister($tmp$$reg), bt, length_in_bytes, as_VectorRegister($src1$$reg),\n+    __ compare_integral_v(as_VectorRegister($dst$$reg), bt, length_in_bytes, as_VectorRegister($src1$$reg),\n@@ -192,1 +193,0 @@\n-    __ vmv1r_v(as_VectorRegister($dst$$reg), as_VectorRegister($tmp$$reg));\n@@ -216,1 +216,3 @@\n-instruct vmaskcmp_fp_masked(vRegMask dst, vReg src1, vReg src2, immI cond, vRegMask_V0 vmask, vReg tmp1, vReg tmp2, vReg tmp3) %{\n+\/\/ Exclude mask register v0 for 'dst' as compare_floating_point_v clears 'dst'\n+\/\/ register on entry before using mask value held in v0.\n+instruct vmaskcmp_fp_masked(vRegMaskNoV0 dst, vReg src1, vReg src2, immI cond, vRegMask_V0 v0, vReg tmp1, vReg tmp2) %{\n@@ -219,3 +221,3 @@\n-  match(Set dst (VectorMaskCmp (Binary src1 src2) (Binary cond vmask)));\n-  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3);\n-  format %{ \"vmaskcmp_fp_masked $dst, $src1, $src2, $vmask, $tmp1, $tmp2, $tmp3 $cond\" %}\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) (Binary cond v0)));\n+  effect(TEMP tmp1, TEMP tmp2);\n+  format %{ \"vmaskcmp_fp_masked $dst, $src1, $src2, $cond, v0.t\\t# KILL $tmp1, $tmp2\" %}\n@@ -225,1 +227,1 @@\n-    __ compare_floating_point_v(as_VectorRegister($tmp1$$reg), bt, length_in_bytes,\n+    __ compare_floating_point_v(as_VectorRegister($dst$$reg), bt, length_in_bytes,\n@@ -227,1 +229,1 @@\n-                                as_VectorRegister($tmp2$$reg), as_VectorRegister($tmp3$$reg),\n+                                as_VectorRegister($tmp1$$reg), as_VectorRegister($tmp2$$reg),\n@@ -229,1 +231,0 @@\n-    __ vmv1r_v(as_VectorRegister($dst$$reg), as_VectorRegister($tmp1$$reg));\n@@ -396,5 +397,5 @@\n-instruct vadd_masked(vReg dst_src1, vReg src2, vRegMask_V0 vmask) %{\n-  match(Set dst_src1 (AddVB (Binary dst_src1 src2) vmask));\n-  match(Set dst_src1 (AddVS (Binary dst_src1 src2) vmask));\n-  match(Set dst_src1 (AddVI (Binary dst_src1 src2) vmask));\n-  match(Set dst_src1 (AddVL (Binary dst_src1 src2) vmask));\n+instruct vadd_masked(vReg dst_src1, vReg src2, vRegMask_V0 v0) %{\n+  match(Set dst_src1 (AddVB (Binary dst_src1 src2) v0));\n+  match(Set dst_src1 (AddVS (Binary dst_src1 src2) v0));\n+  match(Set dst_src1 (AddVI (Binary dst_src1 src2) v0));\n+  match(Set dst_src1 (AddVL (Binary dst_src1 src2) v0));\n@@ -402,1 +403,1 @@\n-  format %{ \"vadd.vv $dst_src1, $src2, $vmask\\t#@vadd_masked\" %}\n+  format %{ \"vadd.vv $dst_src1, $src2, v0.t\\t#@vadd_masked\" %}\n@@ -413,3 +414,3 @@\n-instruct vadd_fp_masked(vReg dst_src1, vReg src2, vRegMask_V0 vmask) %{\n-  match(Set dst_src1 (AddVF (Binary dst_src1 src2) vmask));\n-  match(Set dst_src1 (AddVD (Binary dst_src1 src2) vmask));\n+instruct vadd_fp_masked(vReg dst_src1, vReg src2, vRegMask_V0 v0) %{\n+  match(Set dst_src1 (AddVF (Binary dst_src1 src2) v0));\n+  match(Set dst_src1 (AddVD (Binary dst_src1 src2) v0));\n@@ -417,1 +418,1 @@\n-  format %{ \"vfadd.vv $dst_src1, $src2, $vmask\\t#@vadd_fp_masked\" %}\n+  format %{ \"vfadd.vv $dst_src1, $src2, v0.t\\t#@vadd_fp_masked\" %}\n@@ -506,3 +507,3 @@\n-instruct vdiv_fp_masked(vReg dst_src1, vReg src2, vRegMask_V0 vmask) %{\n-  match(Set dst_src1 (DivVF (Binary dst_src1 src2) vmask));\n-  match(Set dst_src1 (DivVD (Binary dst_src1 src2) vmask));\n+instruct vdiv_fp_masked(vReg dst_src1, vReg src2, vRegMask_V0 v0) %{\n+  match(Set dst_src1 (DivVF (Binary dst_src1 src2) v0));\n+  match(Set dst_src1 (DivVD (Binary dst_src1 src2) v0));\n@@ -510,1 +511,1 @@\n-  format %{ \"vfdiv.vv  $dst_src1, $src2, $vmask\\t#@vdiv_fp_masked\" %}\n+  format %{ \"vfdiv.vv  $dst_src1, $src2, v0.t\\t#@vdiv_fp_masked\" %}\n@@ -923,5 +924,5 @@\n-instruct vmul_masked(vReg dst_src1, vReg src2, vRegMask_V0 vmask) %{\n-  match(Set dst_src1 (MulVB (Binary dst_src1 src2) vmask));\n-  match(Set dst_src1 (MulVS (Binary dst_src1 src2) vmask));\n-  match(Set dst_src1 (MulVI (Binary dst_src1 src2) vmask));\n-  match(Set dst_src1 (MulVL (Binary dst_src1 src2) vmask));\n+instruct vmul_masked(vReg dst_src1, vReg src2, vRegMask_V0 v0) %{\n+  match(Set dst_src1 (MulVB (Binary dst_src1 src2) v0));\n+  match(Set dst_src1 (MulVS (Binary dst_src1 src2) v0));\n+  match(Set dst_src1 (MulVI (Binary dst_src1 src2) v0));\n+  match(Set dst_src1 (MulVL (Binary dst_src1 src2) v0));\n@@ -929,1 +930,1 @@\n-  format %{ \"vmul.vv $dst_src1, $src2, $vmask\\t#@vmul_masked\" %}\n+  format %{ \"vmul.vv $dst_src1, $src2, v0.t\\t#@vmul_masked\" %}\n@@ -939,3 +940,3 @@\n-instruct vmul_fp_masked(vReg dst_src1, vReg src2, vRegMask_V0 vmask) %{\n-  match(Set dst_src1 (MulVF (Binary dst_src1 src2) vmask));\n-  match(Set dst_src1 (MulVD (Binary dst_src1 src2) vmask));\n+instruct vmul_fp_masked(vReg dst_src1, vReg src2, vRegMask_V0 v0) %{\n+  match(Set dst_src1 (MulVF (Binary dst_src1 src2) v0));\n+  match(Set dst_src1 (MulVD (Binary dst_src1 src2) v0));\n@@ -943,1 +944,1 @@\n-  format %{ \"vmul.vv $dst_src1, $src2, $vmask\\t#@vmul_fp_masked\" %}\n+  format %{ \"vmul.vv $dst_src1, $src2, v0.t\\t#@vmul_fp_masked\" %}\n@@ -1197,1 +1198,1 @@\n-  format %{ \"vreduce_maxI $dst, $src1, $src2, $tmp\" %}\n+  format %{ \"vreduce_maxI $dst, $src1, $src2\\t# KILL $tmp\" %}\n@@ -1212,1 +1213,1 @@\n-  format %{ \"vreduce_maxL $dst, $src1, $src2, $tmp\" %}\n+  format %{ \"vreduce_maxL $dst, $src1, $src2\\t# KILL $tmp\" %}\n@@ -1231,1 +1232,1 @@\n-  format %{ \"vreduce_minI $dst, $src1, $src2, $tmp\" %}\n+  format %{ \"vreduce_minI $dst, $src1, $src2\\t# KILL $tmp\" %}\n@@ -1246,1 +1247,1 @@\n-  format %{ \"vreduce_minL $dst, $src1, $src2, $tmp\" %}\n+  format %{ \"vreduce_minL $dst, $src1, $src2\\t# KILL $tmp\" %}\n@@ -1462,1 +1463,1 @@\n-instruct vasrB(vReg dst, vReg src, vReg shift, vRegMask_V0 tmp) %{\n+instruct vasrB(vReg dst, vReg src, vReg shift, vRegMask_V0 v0) %{\n@@ -1465,2 +1466,2 @@\n-  effect(TEMP_DEF dst, TEMP tmp);\n-  format %{ \"vasrB $dst, $src, $shift, $tmp\" %}\n+  effect(TEMP_DEF dst, TEMP v0);\n+  format %{ \"vasrB $dst, $src, $shift\" %}\n@@ -1470,1 +1471,1 @@\n-    __ vmsgtu_vi(as_VectorRegister($tmp$$reg), as_VectorRegister($shift$$reg), BitsPerByte - 1);\n+    __ vmsgtu_vi(as_VectorRegister($v0$$reg), as_VectorRegister($shift$$reg), BitsPerByte - 1);\n@@ -1474,1 +1475,1 @@\n-    __ vmnot_m(as_VectorRegister($tmp$$reg), as_VectorRegister($tmp$$reg));\n+    __ vmnot_m(as_VectorRegister($v0$$reg), as_VectorRegister($v0$$reg));\n@@ -1481,1 +1482,1 @@\n-instruct vasrS(vReg dst, vReg src, vReg shift, vRegMask_V0 tmp) %{\n+instruct vasrS(vReg dst, vReg src, vReg shift, vRegMask_V0 v0) %{\n@@ -1484,2 +1485,2 @@\n-  effect(TEMP_DEF dst, TEMP tmp);\n-  format %{ \"vasrS $dst, $src, $shift, $tmp\" %}\n+  effect(TEMP_DEF dst, TEMP v0);\n+  format %{ \"vasrS $dst, $src, $shift\" %}\n@@ -1489,1 +1490,1 @@\n-    __ vmsgtu_vi(as_VectorRegister($tmp$$reg), as_VectorRegister($shift$$reg), BitsPerShort - 1);\n+    __ vmsgtu_vi(as_VectorRegister($v0$$reg), as_VectorRegister($shift$$reg), BitsPerShort - 1);\n@@ -1493,1 +1494,1 @@\n-    __ vmnot_m(as_VectorRegister($tmp$$reg), as_VectorRegister($tmp$$reg));\n+    __ vmnot_m(as_VectorRegister($v0$$reg), as_VectorRegister($v0$$reg));\n@@ -1524,2 +1525,2 @@\n-instruct vasrB_masked(vReg dst_src, vReg shift, vRegMask_V0 vmask, vReg tmp) %{\n-  match(Set dst_src (RShiftVB (Binary dst_src shift) vmask));\n+instruct vasrB_masked(vReg dst_src, vReg shift, vRegMask_V0 v0, vReg tmp) %{\n+  match(Set dst_src (RShiftVB (Binary dst_src shift) v0));\n@@ -1527,2 +1528,2 @@\n-  effect(TEMP_DEF dst_src, USE vmask, TEMP tmp);\n-  format %{ \"vasrB_masked $dst_src, $dst_src, $shift, $vmask, $tmp\" %}\n+  effect(TEMP_DEF dst_src, USE v0, TEMP tmp);\n+  format %{ \"vasrB_masked $dst_src, $dst_src, $shift, v0.t\\t# KILL $tmp\" %}\n@@ -1530,1 +1531,1 @@\n-    __ vmv1r_v(as_VectorRegister($tmp$$reg), as_VectorRegister($vmask$$reg));\n+    __ vmv1r_v(as_VectorRegister($tmp$$reg), as_VectorRegister($v0$$reg));\n@@ -1532,1 +1533,1 @@\n-    __ vmsgtu_vi(as_VectorRegister($vmask$$reg), as_VectorRegister($shift$$reg), BitsPerByte - 1);\n+    __ vmsgtu_vi(as_VectorRegister($v0$$reg), as_VectorRegister($shift$$reg), BitsPerByte - 1);\n@@ -1536,1 +1537,1 @@\n-    __ vmv1r_v(as_VectorRegister($vmask$$reg), as_VectorRegister($tmp$$reg));\n+    __ vmv1r_v(as_VectorRegister($v0$$reg), as_VectorRegister($tmp$$reg));\n@@ -1543,2 +1544,2 @@\n-instruct vasrS_masked(vReg dst_src, vReg shift, vRegMask_V0 vmask, vReg tmp) %{\n-  match(Set dst_src (RShiftVS (Binary dst_src shift) vmask));\n+instruct vasrS_masked(vReg dst_src, vReg shift, vRegMask_V0 v0, vReg tmp) %{\n+  match(Set dst_src (RShiftVS (Binary dst_src shift) v0));\n@@ -1546,2 +1547,2 @@\n-  effect(TEMP_DEF dst_src, USE vmask, TEMP tmp);\n-  format %{ \"vasrS_masked $dst_src, $dst_src, $shift, $vmask, $tmp\" %}\n+  effect(TEMP_DEF dst_src, USE v0, TEMP tmp);\n+  format %{ \"vasrS_masked $dst_src, $dst_src, $shift, v0.t\\t# KILL $tmp\" %}\n@@ -1549,1 +1550,1 @@\n-    __ vmv1r_v(as_VectorRegister($tmp$$reg), as_VectorRegister($vmask$$reg));\n+    __ vmv1r_v(as_VectorRegister($tmp$$reg), as_VectorRegister($v0$$reg));\n@@ -1551,1 +1552,1 @@\n-    __ vmsgtu_vi(as_VectorRegister($vmask$$reg), as_VectorRegister($shift$$reg), BitsPerShort - 1);\n+    __ vmsgtu_vi(as_VectorRegister($v0$$reg), as_VectorRegister($shift$$reg), BitsPerShort - 1);\n@@ -1555,1 +1556,1 @@\n-    __ vmv1r_v(as_VectorRegister($vmask$$reg), as_VectorRegister($tmp$$reg));\n+    __ vmv1r_v(as_VectorRegister($v0$$reg), as_VectorRegister($tmp$$reg));\n@@ -1562,2 +1563,2 @@\n-instruct vasrI_masked(vReg dst_src, vReg shift, vRegMask_V0 vmask) %{\n-  match(Set dst_src (RShiftVI (Binary dst_src shift) vmask));\n+instruct vasrI_masked(vReg dst_src, vReg shift, vRegMask_V0 v0) %{\n+  match(Set dst_src (RShiftVI (Binary dst_src shift) v0));\n@@ -1566,1 +1567,1 @@\n-  format %{ \"vasrI_masked $dst_src, $dst_src, $shift, $vmask\" %}\n+  format %{ \"vasrI_masked $dst_src, $dst_src, $shift, v0.t\" %}\n@@ -1576,2 +1577,2 @@\n-instruct vasrL_masked(vReg dst_src, vReg shift, vRegMask_V0 vmask) %{\n-  match(Set dst_src (RShiftVL (Binary dst_src shift) vmask));\n+instruct vasrL_masked(vReg dst_src, vReg shift, vRegMask_V0 v0) %{\n+  match(Set dst_src (RShiftVL (Binary dst_src shift) v0));\n@@ -1580,1 +1581,1 @@\n-  format %{ \"vasrL_masked $dst_src, $dst_src, $shift, $vmask\" %}\n+  format %{ \"vasrL_masked $dst_src, $dst_src, $shift, v0.t\" %}\n@@ -1590,1 +1591,1 @@\n-instruct vlslB(vReg dst, vReg src, vReg shift, vRegMask_V0 tmp) %{\n+instruct vlslB(vReg dst, vReg src, vReg shift, vRegMask_V0 v0) %{\n@@ -1593,2 +1594,2 @@\n-  effect(TEMP_DEF dst, TEMP tmp);\n-  format %{ \"vlslB $dst, $src, $shift, $tmp\" %}\n+  effect(TEMP_DEF dst, TEMP v0);\n+  format %{ \"vlslB $dst, $src, $shift\" %}\n@@ -1598,1 +1599,1 @@\n-    __ vmsgtu_vi(as_VectorRegister($tmp$$reg), as_VectorRegister($shift$$reg), BitsPerByte - 1);\n+    __ vmsgtu_vi(as_VectorRegister($v0$$reg), as_VectorRegister($shift$$reg), BitsPerByte - 1);\n@@ -1602,1 +1603,1 @@\n-    __ vmnot_m(as_VectorRegister($tmp$$reg), as_VectorRegister($tmp$$reg));\n+    __ vmnot_m(as_VectorRegister($v0$$reg), as_VectorRegister($v0$$reg));\n@@ -1609,1 +1610,1 @@\n-instruct vlslS(vReg dst, vReg src, vReg shift, vRegMask_V0 tmp) %{\n+instruct vlslS(vReg dst, vReg src, vReg shift, vRegMask_V0 v0) %{\n@@ -1612,2 +1613,2 @@\n-  effect(TEMP_DEF dst, TEMP tmp);\n-  format %{ \"vlslS $dst, $src, $shift, $tmp\" %}\n+  effect(TEMP_DEF dst, TEMP v0);\n+  format %{ \"vlslS $dst, $src, $shift\" %}\n@@ -1617,1 +1618,1 @@\n-    __ vmsgtu_vi(as_VectorRegister($tmp$$reg), as_VectorRegister($shift$$reg), BitsPerShort - 1);\n+    __ vmsgtu_vi(as_VectorRegister($v0$$reg), as_VectorRegister($shift$$reg), BitsPerShort - 1);\n@@ -1621,1 +1622,1 @@\n-    __ vmnot_m(as_VectorRegister($tmp$$reg), as_VectorRegister($tmp$$reg));\n+    __ vmnot_m(as_VectorRegister($v0$$reg), as_VectorRegister($v0$$reg));\n@@ -1652,2 +1653,2 @@\n-instruct vlslB_masked(vReg dst_src, vReg shift, vRegMask_V0 vmask, vReg tmp) %{\n-  match(Set dst_src (LShiftVB (Binary dst_src shift) vmask));\n+instruct vlslB_masked(vReg dst_src, vReg shift, vRegMask_V0 v0, vReg tmp) %{\n+  match(Set dst_src (LShiftVB (Binary dst_src shift) v0));\n@@ -1655,2 +1656,2 @@\n-  effect(TEMP_DEF dst_src, USE vmask, TEMP tmp);\n-  format %{ \"vlslB_masked $dst_src, $dst_src, $shift, $vmask, $tmp\" %}\n+  effect(TEMP_DEF dst_src, USE v0, TEMP tmp);\n+  format %{ \"vlslB_masked $dst_src, $dst_src, $shift, v0.t\\t# KILL $tmp\" %}\n@@ -1658,1 +1659,1 @@\n-    __ vmv1r_v(as_VectorRegister($tmp$$reg), as_VectorRegister($vmask$$reg));\n+    __ vmv1r_v(as_VectorRegister($tmp$$reg), as_VectorRegister($v0$$reg));\n@@ -1661,2 +1662,2 @@\n-    __ vmsgtu_vi(as_VectorRegister($vmask$$reg), as_VectorRegister($shift$$reg), BitsPerByte - 1);\n-    __ vmand_mm(as_VectorRegister($vmask$$reg), as_VectorRegister($vmask$$reg),\n+    __ vmsgtu_vi(as_VectorRegister($v0$$reg), as_VectorRegister($shift$$reg), BitsPerByte - 1);\n+    __ vmand_mm(as_VectorRegister($v0$$reg), as_VectorRegister($v0$$reg),\n@@ -1667,1 +1668,1 @@\n-    __ vmv1r_v(as_VectorRegister($vmask$$reg), as_VectorRegister($tmp$$reg));\n+    __ vmv1r_v(as_VectorRegister($v0$$reg), as_VectorRegister($tmp$$reg));\n@@ -1674,2 +1675,2 @@\n-instruct vlslS_masked(vReg dst_src, vReg shift, vRegMask_V0 vmask, vReg tmp) %{\n-  match(Set dst_src (LShiftVS (Binary dst_src shift) vmask));\n+instruct vlslS_masked(vReg dst_src, vReg shift, vRegMask_V0 v0, vReg tmp) %{\n+  match(Set dst_src (LShiftVS (Binary dst_src shift) v0));\n@@ -1677,2 +1678,2 @@\n-  effect(TEMP_DEF dst_src, USE vmask, TEMP tmp);\n-  format %{ \"vlslS_masked $dst_src, $dst_src, $shift, $vmask, $tmp\" %}\n+  effect(TEMP_DEF dst_src, USE v0, TEMP tmp);\n+  format %{ \"vlslS_masked $dst_src, $dst_src, $shift, v0.t\\t# KILL $tmp\" %}\n@@ -1680,1 +1681,1 @@\n-    __ vmv1r_v(as_VectorRegister($tmp$$reg), as_VectorRegister($vmask$$reg));\n+    __ vmv1r_v(as_VectorRegister($tmp$$reg), as_VectorRegister($v0$$reg));\n@@ -1683,2 +1684,2 @@\n-    __ vmsgtu_vi(as_VectorRegister($vmask$$reg), as_VectorRegister($shift$$reg), BitsPerShort - 1);\n-    __ vmand_mm(as_VectorRegister($vmask$$reg), as_VectorRegister($vmask$$reg),\n+    __ vmsgtu_vi(as_VectorRegister($v0$$reg), as_VectorRegister($shift$$reg), BitsPerShort - 1);\n+    __ vmand_mm(as_VectorRegister($v0$$reg), as_VectorRegister($v0$$reg),\n@@ -1689,1 +1690,1 @@\n-    __ vmv1r_v(as_VectorRegister($vmask$$reg), as_VectorRegister($tmp$$reg));\n+    __ vmv1r_v(as_VectorRegister($v0$$reg), as_VectorRegister($tmp$$reg));\n@@ -1696,2 +1697,2 @@\n-instruct vlslI_masked(vReg dst_src, vReg shift, vRegMask_V0 vmask) %{\n-  match(Set dst_src (LShiftVI (Binary dst_src shift) vmask));\n+instruct vlslI_masked(vReg dst_src, vReg shift, vRegMask_V0 v0) %{\n+  match(Set dst_src (LShiftVI (Binary dst_src shift) v0));\n@@ -1700,1 +1701,1 @@\n-  format %{ \"vlslI_masked $dst_src, $dst_src, $shift, $vmask\" %}\n+  format %{ \"vlslI_masked $dst_src, $dst_src, $shift, v0.t\" %}\n@@ -1709,2 +1710,2 @@\n-instruct vlslL_masked(vReg dst_src, vReg shift, vRegMask_V0 vmask) %{\n-  match(Set dst_src (LShiftVL (Binary dst_src shift) vmask));\n+instruct vlslL_masked(vReg dst_src, vReg shift, vRegMask_V0 v0) %{\n+  match(Set dst_src (LShiftVL (Binary dst_src shift) v0));\n@@ -1713,1 +1714,1 @@\n-  format %{ \"vlslL_masked $dst_src, $dst_src, $shift, $vmask\" %}\n+  format %{ \"vlslL_masked $dst_src, $dst_src, $shift, v0.t\" %}\n@@ -1722,1 +1723,1 @@\n-instruct vlsrB(vReg dst, vReg src, vReg shift, vRegMask_V0 tmp) %{\n+instruct vlsrB(vReg dst, vReg src, vReg shift, vRegMask_V0 v0) %{\n@@ -1725,2 +1726,2 @@\n-  effect(TEMP_DEF dst, TEMP tmp);\n-  format %{ \"vlsrB $dst, $src, $shift, $tmp\" %}\n+  effect(TEMP_DEF dst, TEMP v0);\n+  format %{ \"vlsrB $dst, $src, $shift\" %}\n@@ -1730,1 +1731,1 @@\n-    __ vmsgtu_vi(as_VectorRegister($tmp$$reg), as_VectorRegister($shift$$reg), BitsPerByte - 1);\n+    __ vmsgtu_vi(as_VectorRegister($v0$$reg), as_VectorRegister($shift$$reg), BitsPerByte - 1);\n@@ -1734,1 +1735,1 @@\n-    __ vmnot_m(as_VectorRegister($tmp$$reg), as_VectorRegister($tmp$$reg));\n+    __ vmnot_m(as_VectorRegister($v0$$reg), as_VectorRegister($v0$$reg));\n@@ -1741,1 +1742,1 @@\n-instruct vlsrS(vReg dst, vReg src, vReg shift, vRegMask_V0 tmp) %{\n+instruct vlsrS(vReg dst, vReg src, vReg shift, vRegMask_V0 v0) %{\n@@ -1744,2 +1745,2 @@\n-  effect(TEMP_DEF dst, TEMP tmp);\n-  format %{ \"vlsrS $dst, $src, $shift, $tmp\" %}\n+  effect(TEMP_DEF dst, TEMP v0);\n+  format %{ \"vlsrS $dst, $src, $shift\" %}\n@@ -1749,1 +1750,1 @@\n-    __ vmsgtu_vi(as_VectorRegister($tmp$$reg), as_VectorRegister($shift$$reg), BitsPerShort - 1);\n+    __ vmsgtu_vi(as_VectorRegister($v0$$reg), as_VectorRegister($shift$$reg), BitsPerShort - 1);\n@@ -1753,1 +1754,1 @@\n-    __ vmnot_m(as_VectorRegister($tmp$$reg), as_VectorRegister($tmp$$reg));\n+    __ vmnot_m(as_VectorRegister($v0$$reg), as_VectorRegister($v0$$reg));\n@@ -1784,2 +1785,2 @@\n-instruct vlsrB_masked(vReg dst_src, vReg shift, vRegMask_V0 vmask, vReg tmp) %{\n-  match(Set dst_src (URShiftVB (Binary dst_src shift) vmask));\n+instruct vlsrB_masked(vReg dst_src, vReg shift, vRegMask_V0 v0, vReg tmp) %{\n+  match(Set dst_src (URShiftVB (Binary dst_src shift) v0));\n@@ -1787,2 +1788,2 @@\n-  effect(TEMP_DEF dst_src, USE vmask, TEMP tmp);\n-  format %{ \"vlsrB_masked $dst_src, $dst_src, $shift, $vmask, $tmp\" %}\n+  effect(TEMP_DEF dst_src, USE v0, TEMP tmp);\n+  format %{ \"vlsrB_masked $dst_src, $dst_src, $shift, v0.t\\t# KILL $tmp\" %}\n@@ -1790,1 +1791,1 @@\n-    __ vmv1r_v(as_VectorRegister($tmp$$reg), as_VectorRegister($vmask$$reg));\n+    __ vmv1r_v(as_VectorRegister($tmp$$reg), as_VectorRegister($v0$$reg));\n@@ -1793,2 +1794,2 @@\n-    __ vmsgtu_vi(as_VectorRegister($vmask$$reg), as_VectorRegister($shift$$reg), BitsPerByte - 1);\n-    __ vmand_mm(as_VectorRegister($vmask$$reg), as_VectorRegister($vmask$$reg),\n+    __ vmsgtu_vi(as_VectorRegister($v0$$reg), as_VectorRegister($shift$$reg), BitsPerByte - 1);\n+    __ vmand_mm(as_VectorRegister($v0$$reg), as_VectorRegister($v0$$reg),\n@@ -1799,1 +1800,1 @@\n-    __ vmv1r_v(as_VectorRegister($vmask$$reg), as_VectorRegister($tmp$$reg));\n+    __ vmv1r_v(as_VectorRegister($v0$$reg), as_VectorRegister($tmp$$reg));\n@@ -1806,2 +1807,2 @@\n-instruct vlsrS_masked(vReg dst_src, vReg shift, vRegMask_V0 vmask, vReg tmp) %{\n-  match(Set dst_src (URShiftVS (Binary dst_src shift) vmask));\n+instruct vlsrS_masked(vReg dst_src, vReg shift, vRegMask_V0 v0, vReg tmp) %{\n+  match(Set dst_src (URShiftVS (Binary dst_src shift) v0));\n@@ -1809,2 +1810,2 @@\n-  effect(TEMP_DEF dst_src, USE vmask, TEMP tmp);\n-  format %{ \"vlsrS_masked $dst_src, $dst_src, $shift, $vmask, $tmp\" %}\n+  effect(TEMP_DEF dst_src, USE v0, TEMP tmp);\n+  format %{ \"vlsrS_masked $dst_src, $dst_src, $shift, v0.t\\t# KILL $tmp\" %}\n@@ -1812,1 +1813,1 @@\n-    __ vmv1r_v(as_VectorRegister($tmp$$reg), as_VectorRegister($vmask$$reg));\n+    __ vmv1r_v(as_VectorRegister($tmp$$reg), as_VectorRegister($v0$$reg));\n@@ -1815,2 +1816,2 @@\n-    __ vmsgtu_vi(as_VectorRegister($vmask$$reg), as_VectorRegister($shift$$reg), BitsPerShort - 1);\n-    __ vmand_mm(as_VectorRegister($vmask$$reg), as_VectorRegister($vmask$$reg),\n+    __ vmsgtu_vi(as_VectorRegister($v0$$reg), as_VectorRegister($shift$$reg), BitsPerShort - 1);\n+    __ vmand_mm(as_VectorRegister($v0$$reg), as_VectorRegister($v0$$reg),\n@@ -1821,1 +1822,1 @@\n-    __ vmv1r_v(as_VectorRegister($vmask$$reg), as_VectorRegister($tmp$$reg));\n+    __ vmv1r_v(as_VectorRegister($v0$$reg), as_VectorRegister($tmp$$reg));\n@@ -1828,2 +1829,2 @@\n-instruct vlsrI_masked(vReg dst_src, vReg shift, vRegMask_V0 vmask) %{\n-  match(Set dst_src (URShiftVI (Binary dst_src shift) vmask));\n+instruct vlsrI_masked(vReg dst_src, vReg shift, vRegMask_V0 v0) %{\n+  match(Set dst_src (URShiftVI (Binary dst_src shift) v0));\n@@ -1832,1 +1833,1 @@\n-  format %{ \"vlsrI_masked $dst_src, $dst_src, $shift, $vmask\" %}\n+  format %{ \"vlsrI_masked $dst_src, $dst_src, $shift, v0.t\" %}\n@@ -1841,2 +1842,2 @@\n-instruct vlsrL_masked(vReg dst_src, vReg shift, vRegMask_V0 vmask) %{\n-  match(Set dst_src (URShiftVL (Binary dst_src shift) vmask));\n+instruct vlsrL_masked(vReg dst_src, vReg shift, vRegMask_V0 v0) %{\n+  match(Set dst_src (URShiftVL (Binary dst_src shift) v0));\n@@ -1845,1 +1846,1 @@\n-  format %{ \"vlsrL_masked $dst_src, $dst_src, $shift, $vmask\" %}\n+  format %{ \"vlsrL_masked $dst_src, $dst_src, $shift, v0.t\" %}\n@@ -2212,5 +2213,5 @@\n-instruct vsub_masked(vReg dst_src1, vReg src2, vRegMask_V0 vmask) %{\n-  match(Set dst_src1 (SubVB (Binary dst_src1 src2) vmask));\n-  match(Set dst_src1 (SubVS (Binary dst_src1 src2) vmask));\n-  match(Set dst_src1 (SubVI (Binary dst_src1 src2) vmask));\n-  match(Set dst_src1 (SubVL (Binary dst_src1 src2) vmask));\n+instruct vsub_masked(vReg dst_src1, vReg src2, vRegMask_V0 v0) %{\n+  match(Set dst_src1 (SubVB (Binary dst_src1 src2) v0));\n+  match(Set dst_src1 (SubVS (Binary dst_src1 src2) v0));\n+  match(Set dst_src1 (SubVI (Binary dst_src1 src2) v0));\n+  match(Set dst_src1 (SubVL (Binary dst_src1 src2) v0));\n@@ -2218,1 +2219,1 @@\n-  format %{ \"vsub.vv $dst_src1, $src2, $vmask\\t#@vsub_masked\" %}\n+  format %{ \"vsub.vv $dst_src1, $src2, v0.t\\t#@vsub_masked\" %}\n@@ -2228,3 +2229,3 @@\n-instruct vsub_fp_masked(vReg dst_src1, vReg src2, vRegMask_V0 vmask) %{\n-  match(Set dst_src1 (SubVF (Binary dst_src1 src2) vmask));\n-  match(Set dst_src1 (SubVD (Binary dst_src1 src2) vmask));\n+instruct vsub_fp_masked(vReg dst_src1, vReg src2, vRegMask_V0 v0) %{\n+  match(Set dst_src1 (SubVF (Binary dst_src1 src2) v0));\n+  match(Set dst_src1 (SubVD (Binary dst_src1 src2) v0));\n@@ -2232,1 +2233,1 @@\n-  format %{ \"vfsub.vv $dst_src1, $src2, $vmask\\t#@vsub_fp_masked\" %}\n+  format %{ \"vfsub.vv $dst_src1, $src2, v0.t\\t#@vsub_fp_masked\" %}\n@@ -2557,0 +2558,2 @@\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ rvv_vsetvli(bt, Matcher::vector_length_in_bytes(this));\n@@ -2559,1 +2562,0 @@\n-      __ vsetvli(t0, x0, Assembler::e8);\n@@ -2563,4 +2565,1 @@\n-      __ vsetvli(t0, x0, Assembler::e8);\n-      __ vid_v(as_VectorRegister($dst$$reg));\n-      __ mv(t0, Matcher::vector_length(this));\n-      __ vmslt_vx(as_VectorRegister($dst$$reg), as_VectorRegister($dst$$reg), t0);\n+      __ vmset_m(as_VectorRegister($dst$$reg));\n@@ -2572,1 +2571,1 @@\n-instruct vmaskAllI(vRegMask dst, iRegI src, vReg tmp) %{\n+instruct vmaskAllI(vRegMask dst, iRegI src) %{\n@@ -2574,2 +2573,1 @@\n-  effect(TEMP tmp);\n-  format %{ \"vmaskAllI $dst, $src\\t# KILL $tmp\" %}\n+  format %{ \"vmaskAllI $dst, $src\" %}\n@@ -2579,2 +2577,2 @@\n-    __ vmv_v_x(as_VectorRegister($tmp$$reg), as_Register($src$$reg));\n-    __ vmsne_vx(as_VectorRegister($dst$$reg), as_VectorRegister($tmp$$reg), x0);\n+    __ vmv_v_x(as_VectorRegister($dst$$reg), as_Register($src$$reg));\n+    __ vmsne_vx(as_VectorRegister($dst$$reg), as_VectorRegister($dst$$reg), x0);\n@@ -2589,0 +2587,2 @@\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ rvv_vsetvli(bt, Matcher::vector_length_in_bytes(this));\n@@ -2591,1 +2591,0 @@\n-      __ vsetvli(t0, x0, Assembler::e8);\n@@ -2595,4 +2594,1 @@\n-      __ vsetvli(t0, x0, Assembler::e8);\n-      __ vid_v(as_VectorRegister($dst$$reg));\n-      __ mv(t0, Matcher::vector_length(this));\n-      __ vmslt_vx(as_VectorRegister($dst$$reg), as_VectorRegister($dst$$reg), t0);\n+      __ vmset_m(as_VectorRegister($dst$$reg));\n@@ -2604,1 +2600,1 @@\n-instruct vmaskAllL(vRegMask dst, iRegL src, vReg tmp) %{\n+instruct vmaskAllL(vRegMask dst, iRegL src) %{\n@@ -2606,2 +2602,1 @@\n-  effect(TEMP tmp);\n-  format %{ \"vmaskAllL $dst, $src\\t# KILL $tmp\" %}\n+  format %{ \"vmaskAllL $dst, $src\" %}\n@@ -2611,2 +2606,2 @@\n-    __ vmv_v_x(as_VectorRegister($tmp$$reg), as_Register($src$$reg));\n-    __ vmsne_vx(as_VectorRegister($dst$$reg), as_VectorRegister($tmp$$reg), x0);\n+    __ vmv_v_x(as_VectorRegister($dst$$reg), as_Register($src$$reg));\n+    __ vmsne_vx(as_VectorRegister($dst$$reg), as_VectorRegister($dst$$reg), x0);\n@@ -2667,3 +2662,3 @@\n-instruct loadV_masked(vReg dst, vmemA mem, vRegMask_V0 vmask) %{\n-  match(Set dst (LoadVectorMasked mem vmask));\n-  format %{ \"loadV_masked $dst, $vmask, $mem\" %}\n+instruct loadV_masked(vReg dst, vmemA mem, vRegMask_V0 v0) %{\n+  match(Set dst (LoadVectorMasked mem v0));\n+  format %{ \"loadV_masked $dst, $mem, v0.t\" %}\n@@ -2679,3 +2674,3 @@\n-instruct storeV_masked(vReg src, vmemA mem, vRegMask_V0 vmask) %{\n-  match(Set mem (StoreVectorMasked mem (Binary src vmask)));\n-  format %{ \"storeV_masked $mem, $vmask, $src\" %}\n+instruct storeV_masked(vReg src, vmemA mem, vRegMask_V0 v0) %{\n+  match(Set mem (StoreVectorMasked mem (Binary src v0)));\n+  format %{ \"storeV_masked $mem, $src, v0.t\" %}\n@@ -2693,3 +2688,3 @@\n-instruct vblend(vReg dst, vReg src1, vReg src2, vRegMask_V0 vmask) %{\n-  match(Set dst (VectorBlend (Binary src1 src2) vmask));\n-  format %{ \"vmerge_vvm $dst, $src1, $src2\\t# vector blend\" %}\n+instruct vblend(vReg dst, vReg src1, vReg src2, vRegMask_V0 v0) %{\n+  match(Set dst (VectorBlend (Binary src1 src2) v0));\n+  format %{ \"vmerge_vvm $dst, $src1, $src2, v0\\t#@vector blend\" %}\n","filename":"src\/hotspot\/cpu\/riscv\/riscv_v.ad","additions":175,"deletions":180,"binary":false,"changes":355,"status":"modified"}]}