{"files":[{"patch":"@@ -1491,0 +1491,10 @@\n+#define INSN(NAME, op, funct3, vm, funct6)                                                         \\\n+  void NAME(VectorRegister Vd, VectorRegister Vs2, Register Rs1) {                                 \\\n+    patch_VArith(op, Vd, funct3, Rs1->raw_encoding(), Vs2, vm, funct6);                            \\\n+  }\n+\n+  \/\/ Vector Integer Merge Instructions\n+  INSN(vmerge_vxm,  0b1010111, 0b100, 0b0, 0b010111);\n+\n+#undef INSN\n+\n@@ -1545,0 +1555,11 @@\n+#define INSN(NAME, op, funct3, vm, funct6)                                    \\\n+  void NAME(VectorRegister Vd, VectorRegister Vs2, int32_t imm) {             \\\n+    guarantee(is_imm_in_range(imm, 5, 0), \"imm is invalid\");                  \\\n+    patch_VArith(op, Vd, funct3, (uint32_t)(imm & 0x1f), Vs2, vm, funct6);    \\\n+  }\n+\n+  \/\/ Vector Integer Merge Instructions\n+  INSN(vmerge_vim,  0b1010111, 0b011, 0b0, 0b010111);\n+\n+#undef INSN\n+\n@@ -1563,0 +1584,3 @@\n+  \/\/ Vector Integer Merge Instructions\n+  INSN(vmerge_vvm,  0b1010111, 0b000, 0b0, 0b010111);\n+\n","filename":"src\/hotspot\/cpu\/riscv\/assembler_riscv.hpp","additions":24,"deletions":0,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -1725,0 +1725,32 @@\n+\n+void C2_MacroAssembler::compare_v(VectorRegister vd, BasicType bt, int length_in_bytes,\n+                                  VectorRegister src1, VectorRegister src2, int cond, VectorMask vm) {\n+  rvv_vsetvli(bt, length_in_bytes);\n+  vmxor_mm(vd, vd, vd);\n+  if (bt == T_FLOAT || bt == T_DOUBLE) {\n+    switch (cond) {\n+      case BoolTest::eq: vmfeq_vv(vd, src1, src2, vm); break;\n+      case BoolTest::ne: vmfne_vv(vd, src1, src2, vm); break;\n+      case BoolTest::le: vmfle_vv(vd, src1, src2, vm); break;\n+      case BoolTest::ge: vmfge_vv(vd, src1, src2, vm); break;\n+      case BoolTest::lt: vmflt_vv(vd, src1, src2, vm); break;\n+      case BoolTest::gt: vmfgt_vv(vd, src1, src2, vm); break;\n+      default:\n+        assert(false, \"unsupported compare condition\");\n+        ShouldNotReachHere();\n+    }\n+  } else {\n+    assert(is_integral_type(bt), \"unsupported element type\");\n+    switch (cond) {\n+      case BoolTest::eq: vmseq_vv(vd, src1, src2, vm); break;\n+      case BoolTest::ne: vmsne_vv(vd, src1, src2, vm); break;\n+      case BoolTest::le: vmsle_vv(vd, src1, src2, vm); break;\n+      case BoolTest::ge: vmsge_vv(vd, src1, src2, vm); break;\n+      case BoolTest::lt: vmslt_vv(vd, src1, src2, vm); break;\n+      case BoolTest::gt: vmsgt_vv(vd, src1, src2, vm); break;\n+      default:\n+        assert(false, \"unsupported compare condition\");\n+        ShouldNotReachHere();\n+    }\n+  }\n+}\n\\ No newline at end of file\n","filename":"src\/hotspot\/cpu\/riscv\/c2_MacroAssembler_riscv.cpp","additions":32,"deletions":0,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -140,4 +140,6 @@\n-  void spill_copy_vector_stack_to_stack(int src_offset, int dst_offset, int vec_reg_size_in_bytes) {\n-    assert(vec_reg_size_in_bytes % 16 == 0, \"unexpected vector reg size\");\n-    unspill(v0, src_offset);\n-    spill(v0, dst_offset);\n+  void spill_copy_vector_stack_to_stack(int src_offset, int dst_offset, int vector_length_in_bytes) {\n+    assert(vector_length_in_bytes % 16 == 0, \"unexpected vector reg size\");\n+    for (int i = 0; i < vector_length_in_bytes \/ 8; i++) {\n+      unspill(t0, true, src_offset + (i * 8));\n+      spill(t0, true, dst_offset + (i * 8));\n+    }\n@@ -201,0 +203,32 @@\n+ void compare_v(VectorRegister dst, BasicType bt, int length_in_bytes,\n+                  VectorRegister src1, VectorRegister src2, int cond, VectorMask vm = Assembler::unmasked);\n+\n+ \/\/ In Matcher::scalable_predicate_reg_slots,\n+ \/\/ we assume each predicate register is one-eighth of the size of\n+ \/\/ scalable vector register, one mask bit per vector byte.\n+ void spill_vmask(VectorRegister v, int offset){\n+   rvv_vsetvli(T_BYTE, MaxVectorSize >> 3);\n+   add(t0, sp, offset);\n+   vse8_v(v, t0);\n+ }\n+\n+ void unspill_vmask(VectorRegister v, int offset){\n+   rvv_vsetvli(T_BYTE, MaxVectorSize >> 3);\n+   add(t0, sp, offset);\n+   vle8_v(v, t0);\n+ }\n+\n+  void spill_copy_vmask_stack_to_stack(int src_offset, int dst_offset, int vector_length_in_bytes) {\n+    assert(vector_length_in_bytes % 4 == 0, \"unexpected vector mask reg size\");\n+    for (int i = 0; i < vector_length_in_bytes \/ 4; i++) {\n+      unspill(t0, false, src_offset + (i * 4));\n+      spill(t0, false, dst_offset + (i * 4));\n+    }\n+  }\n+\n+  \/\/ Clear vector registers independent of previous vl and vtype.\n+  void clear_register_v(VectorRegister v) {\n+    vsetvli(t0, x0, Assembler::e64);\n+    vxor_vv(v, v, v);\n+  }\n+\n","filename":"src\/hotspot\/cpu\/riscv\/c2_MacroAssembler_riscv.hpp","additions":38,"deletions":4,"binary":false,"changes":42,"status":"modified"},{"patch":"@@ -1265,1 +1265,1 @@\n-  inline void vncvt_x_x_w(VectorRegister vd, VectorRegister vs, VectorMask vm) {\n+  inline void vncvt_x_x_w(VectorRegister vd, VectorRegister vs, VectorMask vm = unmasked) {\n@@ -1277,0 +1277,24 @@\n+  inline void vmsgt_vv(VectorRegister vd, VectorRegister vs2, VectorRegister vs1, VectorMask vm = unmasked) {\n+    vmslt_vv(vd, vs1, vs2, vm);\n+  }\n+\n+  inline void vmsgtu_vv(VectorRegister vd, VectorRegister vs2, VectorRegister vs1, VectorMask vm = unmasked) {\n+    vmsltu_vv(vd, vs1, vs2, vm);\n+  }\n+\n+  inline void vmsge_vv(VectorRegister vd, VectorRegister vs2, VectorRegister vs1, VectorMask vm = unmasked) {\n+    vmsle_vv(vd, vs1, vs2, vm);\n+  }\n+\n+  inline void vmsgeu_vv(VectorRegister vd, VectorRegister vs2, VectorRegister vs1, VectorMask vm = unmasked) {\n+    vmsleu_vv(vd, vs1, vs2, vm);\n+  }\n+\n+  inline void vmfgt_vv(VectorRegister vd, VectorRegister vs2, VectorRegister vs1, VectorMask vm = unmasked) {\n+    vmflt_vv(vd, vs1, vs2, vm);\n+  }\n+\n+  inline void vmfge_vv(VectorRegister vd, VectorRegister vs2, VectorRegister vs1, VectorMask vm = unmasked) {\n+    vmfle_vv(vd, vs1, vs2, vm);\n+  }\n+\n","filename":"src\/hotspot\/cpu\/riscv\/macroAssembler_riscv.hpp","additions":25,"deletions":1,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -152,1 +152,1 @@\n-    return false;\n+    return UseRVV;\n","filename":"src\/hotspot\/cpu\/riscv\/matcher_riscv.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -833,1 +833,2 @@\n-\/\/ Class for all RVV vector registers\n+\/\/ Class for RVV vector registers\n+\/\/ Note: v0, v30 and v31 are used as mask registers.\n@@ -863,3 +864,1 @@\n-    V29, V29_H, V29_J, V29_K,\n-    V30, V30_H, V30_J, V30_K,\n-    V31, V31_H, V31_J, V31_K\n+    V29, V29_H, V29_J, V29_K\n@@ -915,0 +914,17 @@\n+\n+\/\/ Class for RVV v0 mask register\n+\/\/ https:\/\/github.com\/riscv\/riscv-v-spec\/blob\/master\/v-spec.adoc#53-vector-masking\n+\/\/ The mask value used to control execution of a masked vector\n+\/\/ instruction is always supplied by vector register v0.\n+reg_class vmask_reg_v0 (\n+    V0\n+);\n+\n+\/\/ Class for RVV mask registers\n+\/\/ We need two more vmask registers to do the vector mask logical ops,\n+\/\/ so define v30, v31 as mask register too.\n+reg_class vmask_reg (\n+    V0,\n+    V30,\n+    V31\n+);\n@@ -1525,1 +1541,1 @@\n-  if (src_hi != OptoReg::Bad) {\n+  if (src_hi != OptoReg::Bad && !bottom_type()->isa_vectmask()) {\n@@ -1561,0 +1577,19 @@\n+    } else if (bottom_type()->isa_vectmask() && cbuf) {\n+      C2_MacroAssembler _masm(cbuf);\n+      int vmask_size_in_bytes = Matcher::scalable_predicate_reg_slots() * 32 \/ 8;\n+      if (src_lo_rc == rc_stack && dst_lo_rc == rc_stack) {\n+        \/\/ stack to stack\n+        __ spill_copy_vmask_stack_to_stack(src_offset, dst_offset,\n+                                           vmask_size_in_bytes);\n+      } else if (src_lo_rc == rc_vector && dst_lo_rc == rc_stack) {\n+        \/\/ vmask to stack\n+        __ spill_vmask(as_VectorRegister(Matcher::_regEncode[src_lo]), ra_->reg2offset(dst_lo));\n+      } else if (src_lo_rc == rc_stack && dst_lo_rc == rc_vector) {\n+        \/\/ stack to vmask\n+        __ unspill_vmask(as_VectorRegister(Matcher::_regEncode[dst_lo]), ra_->reg2offset(src_lo));\n+      } else if (src_lo_rc == rc_vector && dst_lo_rc == rc_vector) {\n+        \/\/ vmask to vmask\n+        __ vmv1r_v(as_VectorRegister(Matcher::_regEncode[dst_lo]), as_VectorRegister(Matcher::_regEncode[src_lo]));\n+      } else {\n+        ShouldNotReachHere();\n+      }\n@@ -1645,1 +1680,1 @@\n-    if (bottom_type()->isa_vect() != NULL) {\n+    if (bottom_type()->isa_vect() && !bottom_type()->isa_vectmask()) {\n@@ -1653,0 +1688,4 @@\n+    } else if (ideal_reg() == Op_RegVectMask) {\n+      assert(Matcher::supports_scalable_vector(), \"bad register type for spill\");\n+      int vsize = Matcher::scalable_predicate_reg_slots() * 32;\n+      st->print(\"\\t# vmask spill size = %d\", vsize);\n@@ -1868,1 +1907,50 @@\n-  return false;\n+  if (!UseRVV) {\n+    return false;\n+  }\n+  switch (opcode) {\n+    case Op_AddVB:\n+    case Op_AddVS:\n+    case Op_AddVI:\n+    case Op_AddVL:\n+    case Op_AddVF:\n+    case Op_AddVD:\n+    case Op_SubVB:\n+    case Op_SubVS:\n+    case Op_SubVI:\n+    case Op_SubVL:\n+    case Op_SubVF:\n+    case Op_SubVD:\n+    case Op_MulVB:\n+    case Op_MulVS:\n+    case Op_MulVI:\n+    case Op_MulVL:\n+    case Op_MulVF:\n+    case Op_MulVD:\n+    case Op_DivVF:\n+    case Op_DivVD:\n+    case Op_VectorLoadMask:\n+    case Op_VectorMaskCmp:\n+    case Op_AndVMask:\n+    case Op_XorVMask:\n+    case Op_OrVMask:\n+    case Op_LoadVector:\n+        opcode = Op_LoadVectorMasked;\n+    case Op_StoreVector:\n+        opcode = Op_StoreVectorMasked;\n+    case Op_RShiftVB:\n+    case Op_RShiftVS:\n+    case Op_RShiftVI:\n+    case Op_RShiftVL:\n+    case Op_LShiftVB:\n+    case Op_LShiftVS:\n+    case Op_LShiftVI:\n+    case Op_LShiftVL:\n+    case Op_URShiftVB:\n+    case Op_URShiftVS:\n+    case Op_URShiftVI:\n+    case Op_URShiftVL:\n+    case Op_VectorBlend:\n+      return match_rule_supported_vector(opcode, vlen, bt);\n+    default:\n+      return false;\n+  }\n@@ -1876,1 +1964,1 @@\n-  return NULL;\n+  return &_VMASK_REG_mask;\n@@ -1880,1 +1968,1 @@\n-  return NULL;\n+  return new TypeVectMask(elemTy, length);\n@@ -3507,0 +3595,22 @@\n+\/\/ The mask value used to control execution of a masked\n+\/\/ vector instruction is always supplied by vector register v0.\n+operand vRegMask_V0()\n+%{\n+  constraint(ALLOC_IN_RC(vmask_reg_v0));\n+  match(RegVectMask);\n+  match(vRegMask);\n+  op_cost(0);\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+operand vRegMask()\n+%{\n+  constraint(ALLOC_IN_RC(vmask_reg));\n+  match(RegVectMask);\n+  match(vRegMask_V0);\n+  op_cost(0);\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n","filename":"src\/hotspot\/cpu\/riscv\/riscv.ad","additions":119,"deletions":9,"binary":false,"changes":128,"status":"modified"},{"patch":"@@ -38,1 +38,2 @@\n-                        VectorRegister reg, BasicType bt, Register base, int length_in_bytes) {\n+                        VectorRegister reg, BasicType bt, Register base,\n+                        int length_in_bytes, Assembler::VectorMask vm = Assembler::unmasked) {\n@@ -43,1 +44,1 @@\n-      masm.vsex_v(reg, base, sew);\n+      masm.vsex_v(reg, base, sew, vm);\n@@ -45,1 +46,4 @@\n-      masm.vlex_v(reg, base, sew);\n+      if (vm == Assembler::v0_t) {\n+        masm.vxor_vv(reg, reg, reg);\n+      }\n+      masm.vlex_v(reg, base, sew, vm);\n@@ -69,1 +73,0 @@\n-      case Op_VectorBlend:\n@@ -78,1 +81,0 @@\n-      case Op_VectorLoadMask:\n@@ -80,1 +82,0 @@\n-      case Op_VectorMaskCmp:\n@@ -83,1 +84,0 @@\n-      case Op_VectorStoreMask:\n@@ -126,0 +126,63 @@\n+\/\/ vector load mask\n+\n+instruct vloadmask(vRegMask dst, vReg src) %{\n+  match(Set dst (VectorLoadMask src));\n+  format %{ \"vloadmask $dst, $src\" %}\n+  ins_encode %{\n+    __ vsetvli(t0, x0, Assembler::e8);\n+    __ vmsne_vx(as_VectorRegister($dst$$reg), as_VectorRegister($src$$reg), x0);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vloadmask_masked(vRegMask dst, vReg src, vRegMask_V0 vmask) %{\n+  match(Set dst (VectorLoadMask src vmask));\n+  format %{ \"vloadmask_masked $dst, $src, $vmask\" %}\n+  ins_encode %{\n+    __ vsetvli(t0, x0, Assembler::e8);\n+    __ vmsne_vx(as_VectorRegister($dst$$reg), as_VectorRegister($src$$reg), x0, Assembler::v0_t);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector store mask\n+\n+instruct vstoremask(vReg dst, vRegMask_V0 src, immI size) %{\n+  match(Set dst (VectorStoreMask src size));\n+  format %{ \"vstoremask $dst, $src\" %}\n+  ins_encode %{\n+    __ vsetvli(t0, x0, Assembler::e8);\n+    __ vmv_v_x(as_VectorRegister($dst$$reg), x0);\n+    __ vmerge_vim(as_VectorRegister($dst$$reg), as_VectorRegister($dst$$reg), 1);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector mask compare\n+\n+instruct vmaskcmp(vRegMask dst, vReg src1, vReg src2, immI cond) %{\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));\n+  format %{ \"vmaskcmp $dst, $src1, $src2, $cond\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this);\n+    __ compare_v(as_VectorRegister($dst$$reg), bt, length_in_bytes, as_VectorRegister($src1$$reg),\n+                   as_VectorRegister($src2$$reg), (int)($cond$$constant));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmaskcmp_masked(vRegMask dst, vReg src1, vReg src2, immI cond, vRegMask_V0 vmask, vReg tmp) %{\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) (Binary cond vmask)));\n+  effect(TEMP tmp);\n+  format %{ \"vmaskcmp_masked $dst, $src1, $src2, $vmask, $tmp, $cond\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this);\n+    __ compare_v(as_VectorRegister($tmp$$reg), bt, length_in_bytes, as_VectorRegister($src1$$reg),\n+                   as_VectorRegister($src2$$reg), (int)($cond$$constant), Assembler::v0_t);\n+    __ vmv1r_v(as_VectorRegister($dst$$reg), as_VectorRegister($tmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -286,0 +349,80 @@\n+\/\/ vector add - predicated\n+\n+instruct vaddB_masked(vReg dst_src1, vReg src2, vRegMask_V0 vmask) %{\n+  match(Set dst_src1 (AddVB (Binary dst_src1 src2) vmask));\n+  ins_cost(VEC_COST);\n+  format %{ \"vadd.vv $dst_src1, $src2, $vmask\\t#@vaddB_masked\" %}\n+  ins_encode %{\n+    __ rvv_vsetvli(T_BYTE, Matcher::vector_length_in_bytes(this));\n+    __ vadd_vv(as_VectorRegister($dst_src1$$reg),\n+               as_VectorRegister($dst_src1$$reg),\n+               as_VectorRegister($src2$$reg), Assembler::v0_t);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vaddS_masked(vReg dst_src1, vReg src2, vRegMask_V0 vmask) %{\n+  match(Set dst_src1 (AddVS (Binary dst_src1 src2) vmask));\n+  ins_cost(VEC_COST);\n+  format %{ \"vadd.vv $dst_src1, $src2, $vmask\\t#@vaddS_masked\" %}\n+  ins_encode %{\n+    __ rvv_vsetvli(T_SHORT, Matcher::vector_length_in_bytes(this));\n+    __ vadd_vv(as_VectorRegister($dst_src1$$reg),\n+               as_VectorRegister($dst_src1$$reg),\n+               as_VectorRegister($src2$$reg), Assembler::v0_t);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vaddI_masked(vReg dst_src1, vReg src2, vRegMask_V0 vmask) %{\n+  match(Set dst_src1 (AddVI (Binary dst_src1 src2) vmask));\n+  ins_cost(VEC_COST);\n+  format %{ \"vadd.vv $dst_src1, $src2, $vmask\\t#@vaddI_masked\" %}\n+  ins_encode %{\n+    __ rvv_vsetvli(T_INT, Matcher::vector_length_in_bytes(this));\n+    __ vadd_vv(as_VectorRegister($dst_src1$$reg),\n+               as_VectorRegister($dst_src1$$reg),\n+               as_VectorRegister($src2$$reg), Assembler::v0_t);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vaddL_masked(vReg dst_src1, vReg src2, vRegMask_V0 vmask) %{\n+  match(Set dst_src1 (AddVL (Binary dst_src1 src2) vmask));\n+  ins_cost(VEC_COST);\n+  format %{ \"vadd.vv $dst_src1, $src2, $vmask\\t#@vaddL_masked\" %}\n+  ins_encode %{\n+    __ rvv_vsetvli(T_LONG, Matcher::vector_length_in_bytes(this));\n+    __ vadd_vv(as_VectorRegister($dst_src1$$reg),\n+               as_VectorRegister($dst_src1$$reg),\n+               as_VectorRegister($src2$$reg), Assembler::v0_t);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vaddF_masked(vReg dst_src1, vReg src2, vRegMask_V0 vmask) %{\n+  match(Set dst_src1 (AddVF (Binary dst_src1 src2) vmask));\n+  ins_cost(VEC_COST);\n+  format %{ \"vfadd.vv $dst_src1, $src2, $vmask\\t#@vaddF_masked\" %}\n+  ins_encode %{\n+    __ rvv_vsetvli(T_FLOAT, Matcher::vector_length_in_bytes(this));\n+    __ vfadd_vv(as_VectorRegister($dst_src1$$reg),\n+                as_VectorRegister($dst_src1$$reg),\n+                as_VectorRegister($src2$$reg), Assembler::v0_t);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vaddD_masked(vReg dst_src1, vReg src2, vRegMask_V0 vmask) %{\n+  match(Set dst_src1 (AddVD (Binary dst_src1 src2) vmask));\n+  ins_cost(VEC_COST);\n+  format %{ \"vfadd.vv $dst_src1, $src2, $vmask\\t#@vaddD_masked\" %}\n+  ins_encode %{\n+    __ rvv_vsetvli(T_DOUBLE, Matcher::vector_length_in_bytes(this));\n+    __ vfadd_vv(as_VectorRegister($dst_src1$$reg),\n+                as_VectorRegister($dst_src1$$reg),\n+                as_VectorRegister($src2$$reg), Assembler::v0_t);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -359,0 +502,28 @@\n+\/\/ vector float div - predicated\n+\n+instruct vdivF_masked(vReg dst_src1, vReg src2, vRegMask_V0 vmask) %{\n+  match(Set dst_src1 (DivVF (Binary dst_src1 src2) vmask));\n+  ins_cost(VEC_COST);\n+  format %{ \"vfdiv.vv  $dst_src1, $src2, $vmask\\t#@vdivF_masked\" %}\n+  ins_encode %{\n+    __ rvv_vsetvli(T_FLOAT, Matcher::vector_length_in_bytes(this));\n+    __ vfdiv_vv(as_VectorRegister($dst_src1$$reg),\n+                as_VectorRegister($dst_src1$$reg),\n+                as_VectorRegister($src2$$reg), Assembler::v0_t);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vdivD_masked(vReg dst_src1, vReg src2, vRegMask_V0 vmask) %{\n+  match(Set dst_src1 (DivVD (Binary dst_src1 src2) vmask));\n+  ins_cost(VEC_COST);\n+  format %{ \"vfdiv.vv  $dst_src1, $src2, $vmask\\t#@vdivD_masked\" %}\n+  ins_encode %{\n+    __ rvv_vsetvli(T_DOUBLE, Matcher::vector_length_in_bytes(this));\n+    __ vfdiv_vv(as_VectorRegister($dst_src1$$reg),\n+                as_VectorRegister($dst_src1$$reg),\n+                as_VectorRegister($src2$$reg), Assembler::v0_t);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -759,0 +930,74 @@\n+\/\/ vector mul - predicated\n+\n+instruct vmulB_masked(vReg dst_src1, vReg src2, vRegMask_V0 vmask) %{\n+  match(Set dst_src1 (MulVB (Binary dst_src1 src2) vmask));\n+  ins_cost(VEC_COST);\n+  format %{ \"vmul.vv $dst_src1, $src2, $vmask\\t#@vmulB_masked\" %}\n+  ins_encode %{\n+    __ rvv_vsetvli(T_BYTE, Matcher::vector_length_in_bytes(this));\n+    __ vmul_vv(as_VectorRegister($dst_src1$$reg), as_VectorRegister($dst_src1$$reg),\n+               as_VectorRegister($src2$$reg), Assembler::v0_t);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmulS_masked(vReg dst_src1, vReg src2, vRegMask_V0 vmask) %{\n+  match(Set dst_src1 (MulVS (Binary dst_src1 src2) vmask));\n+  ins_cost(VEC_COST);\n+  format %{ \"vmul.vv $dst_src1, $src2, $vmask\\t#@vmulS_masked\" %}\n+  ins_encode %{\n+    __ rvv_vsetvli(T_SHORT, Matcher::vector_length_in_bytes(this));\n+    __ vmul_vv(as_VectorRegister($dst_src1$$reg), as_VectorRegister($dst_src1$$reg),\n+               as_VectorRegister($src2$$reg), Assembler::v0_t);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmulI_masked(vReg dst_src1, vReg src2, vRegMask_V0 vmask) %{\n+  match(Set dst_src1 (MulVI (Binary dst_src1 src2) vmask));\n+  ins_cost(VEC_COST);\n+  format %{ \"vmul.vv $dst_src1, $src2, $vmask\\t#@vmulI_masked\" %}\n+  ins_encode %{\n+    __ rvv_vsetvli(T_INT, Matcher::vector_length_in_bytes(this));\n+    __ vmul_vv(as_VectorRegister($dst_src1$$reg), as_VectorRegister($dst_src1$$reg),\n+               as_VectorRegister($src2$$reg), Assembler::v0_t);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmulL_masked(vReg dst_src1, vReg src2, vRegMask_V0 vmask) %{\n+  match(Set dst_src1 (MulVL (Binary dst_src1 src2) vmask));\n+  ins_cost(VEC_COST);\n+  format %{ \"vmul.vv $dst_src1, $src2, $vmask\\t#@vmulL_masked\" %}\n+  ins_encode %{\n+    __ rvv_vsetvli(T_LONG, Matcher::vector_length_in_bytes(this));\n+    __ vmul_vv(as_VectorRegister($dst_src1$$reg), as_VectorRegister($dst_src1$$reg),\n+               as_VectorRegister($src2$$reg), Assembler::v0_t);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmulF_masked(vReg dst_src1, vReg src2, vRegMask_V0 vmask) %{\n+  match(Set dst_src1 (MulVF (Binary dst_src1 src2) vmask));\n+  ins_cost(VEC_COST);\n+  format %{ \"vmul.vv $dst_src1, $src2, $vmask\\t#@vmulF_masked\" %}\n+  ins_encode %{\n+    __ rvv_vsetvli(T_FLOAT, Matcher::vector_length_in_bytes(this));\n+    __ vfmul_vv(as_VectorRegister($dst_src1$$reg), as_VectorRegister($dst_src1$$reg),\n+               as_VectorRegister($src2$$reg), Assembler::v0_t);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmulD_masked(vReg dst_src1, vReg src2, vRegMask_V0 vmask) %{\n+  match(Set dst_src1 (MulVD (Binary dst_src1 src2) vmask));\n+  ins_cost(VEC_COST);\n+  format %{ \"vmul.vv $dst_src1, $src2, $vmask\\t#@vmulD_masked\" %}\n+  ins_encode %{\n+    __ rvv_vsetvli(T_DOUBLE, Matcher::vector_length_in_bytes(this));\n+    __ vfmul_vv(as_VectorRegister($dst_src1$$reg), as_VectorRegister($dst_src1$$reg),\n+               as_VectorRegister($src2$$reg), Assembler::v0_t);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -1268,1 +1513,1 @@\n-instruct vasrB(vReg dst, vReg src, vReg shift) %{\n+instruct vasrBS(vReg dst, vReg src, vReg shift, vRegMask_V0 tmp) %{\n@@ -1270,21 +1515,0 @@\n-  ins_cost(VEC_COST);\n-  effect(TEMP_DEF dst);\n-  format %{ \"vmsgtu.vi v0, $shift 7\\t#@vasrB\\n\\t\"\n-            \"vsra.vi $dst, $src, 7, Assembler::v0_t\\n\\t\"\n-            \"vmnot.m v0, v0\\n\\t\"\n-            \"vsra.vv $dst, $src, $shift, Assembler::v0_t\" %}\n-  ins_encode %{\n-    __ rvv_vsetvli(T_BYTE, Matcher::vector_length_in_bytes(this));\n-    \/\/ if shift > BitsPerByte - 1, clear the low BitsPerByte - 1 bits\n-    __ vmsgtu_vi(v0, as_VectorRegister($shift$$reg), BitsPerByte - 1);\n-    __ vsra_vi(as_VectorRegister($dst$$reg), as_VectorRegister($src$$reg),\n-               BitsPerByte - 1, Assembler::v0_t);\n-    \/\/ otherwise, shift\n-    __ vmnot_m(v0, v0);\n-    __ vsra_vv(as_VectorRegister($dst$$reg), as_VectorRegister($src$$reg),\n-               as_VectorRegister($shift$$reg), Assembler::v0_t);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct vasrS(vReg dst, vReg src, vReg shift) %{\n@@ -1293,5 +1517,2 @@\n-  effect(TEMP_DEF dst);\n-  format %{ \"vmsgtu.vi v0, $shift, 15\\t#@vasrS\\n\\t\"\n-            \"vsra.vi $dst, $src, 15, Assembler::v0_t\\n\\t\"\n-            \"vmnot.m v0, v0\\n\\t\"\n-            \"vsra.vv $dst, $src, $shift, Assembler::v0_t\" %}\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"vasrBS $dst, $src, $shift $tmp\" %}\n@@ -1299,3 +1520,5 @@\n-    __ rvv_vsetvli(T_SHORT, Matcher::vector_length_in_bytes(this));\n-    \/\/ if shift > BitsPerShort - 1, clear the low BitsPerShort - 1 bits\n-    __ vmsgtu_vi(v0, as_VectorRegister($shift$$reg), BitsPerShort - 1);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int bitsPerElement = type2aelembytes(bt) * 8;\n+    __ rvv_vsetvli(bt, Matcher::vector_length_in_bytes(this));\n+    \/\/ if shift > bitsPerElement - 1, clear the low bitsPerElement - 1 bits\n+    __ vmsgtu_vi(as_VectorRegister($tmp$$reg), as_VectorRegister($shift$$reg), bitsPerElement - 1);\n@@ -1303,1 +1526,1 @@\n-               BitsPerShort - 1, Assembler::v0_t);\n+               bitsPerElement - 1, Assembler::v0_t);\n@@ -1305,1 +1528,1 @@\n-    __ vmnot_m(v0, v0);\n+    __ vmnot_m(as_VectorRegister($tmp$$reg), as_VectorRegister($tmp$$reg));\n@@ -1312,1 +1535,1 @@\n-instruct vasrI(vReg dst, vReg src, vReg shift) %{\n+instruct vasrIL(vReg dst, vReg src, vReg shift) %{\n@@ -1314,0 +1537,1 @@\n+  match(Set dst (RShiftVL src shift));\n@@ -1315,1 +1539,1 @@\n-  format %{ \"vsra.vv $dst, $src, $shift\\t#@vasrI\" %}\n+  format %{ \"vasrIL $dst, $src, $shift\" %}\n@@ -1317,1 +1541,2 @@\n-    __ rvv_vsetvli(T_INT, Matcher::vector_length_in_bytes(this));\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ rvv_vsetvli(bt, Matcher::vector_length_in_bytes(this));\n@@ -1324,2 +1549,3 @@\n-instruct vasrL(vReg dst, vReg src, vReg shift) %{\n-  match(Set dst (RShiftVL src shift));\n+instruct vasrBS_masked(vReg dst_src, vReg shift, vRegMask_V0 vmask, vReg tmp) %{\n+  match(Set dst_src (RShiftVB (Binary dst_src shift) vmask));\n+  match(Set dst_src (RShiftVS (Binary dst_src shift) vmask));\n@@ -1327,1 +1553,2 @@\n-  format %{ \"vsra.vv $dst, $src, $shift\\t#@vasrL\" %}\n+  effect(TEMP_DEF dst_src, USE vmask, TEMP tmp);\n+  format %{ \"vasrBS_masked $dst_src, $dst_src, $shift, $vmask, $tmp\" %}\n@@ -1329,3 +1556,11 @@\n-    __ rvv_vsetvli(T_LONG, Matcher::vector_length_in_bytes(this));\n-    __ vsra_vv(as_VectorRegister($dst$$reg), as_VectorRegister($src$$reg),\n-         as_VectorRegister($shift$$reg));\n+    __ vmv1r_v(as_VectorRegister($tmp$$reg), as_VectorRegister($vmask$$reg));\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int bitsPerElement = type2aelembytes(bt) * 8;\n+    __ rvv_vsetvli(bt, Matcher::vector_length_in_bytes(this));\n+    __ vmsgtu_vi(as_VectorRegister($vmask$$reg), as_VectorRegister($shift$$reg), bitsPerElement - 1);\n+    \/\/ if shift > bitsPerElement - 1, clear the low bitsPerElement - 1 bits\n+    __ vmerge_vim(as_VectorRegister($shift$$reg), as_VectorRegister($shift$$reg), bitsPerElement - 1);\n+    \/\/ otherwise, shift\n+    __ vmv1r_v(as_VectorRegister($vmask$$reg), as_VectorRegister($tmp$$reg));\n+    __ vsra_vv(as_VectorRegister($dst_src$$reg), as_VectorRegister($dst_src$$reg),\n+               as_VectorRegister($shift$$reg), Assembler::v0_t);\n@@ -1336,2 +1571,3 @@\n-instruct vlslB(vReg dst, vReg src, vReg shift) %{\n-  match(Set dst (LShiftVB src shift));\n+instruct vasrIL_masked(vReg dst_src, vReg shift, vRegMask_V0 vmask) %{\n+  match(Set dst_src (RShiftVI (Binary dst_src shift) vmask));\n+  match(Set dst_src (RShiftVL (Binary dst_src shift) vmask));\n@@ -1339,5 +1575,2 @@\n-  effect( TEMP_DEF dst);\n-  format %{ \"vmsgtu.vi v0, $shift, 7\\t#@vlslB\\n\\t\"\n-            \"vxor.vv $dst, $src, $src, Assembler::v0_t\\n\\t\"\n-            \"vmnot.m v0, v0\\n\\t\"\n-            \"vsll.vv $dst, $src, $shift, Assembler::v0_t\" %}\n+  effect(TEMP_DEF dst_src);\n+  format %{ \"vasrIL_masked $dst_src, $dst_src, $shift, $vmask\" %}\n@@ -1345,8 +1578,3 @@\n-    __ rvv_vsetvli(T_BYTE, Matcher::vector_length_in_bytes(this));\n-    \/\/ if shift > BitsPerByte - 1, clear the element\n-    __ vmsgtu_vi(v0, as_VectorRegister($shift$$reg), BitsPerByte - 1);\n-    __ vxor_vv(as_VectorRegister($dst$$reg), as_VectorRegister($src$$reg),\n-               as_VectorRegister($src$$reg), Assembler::v0_t);\n-    \/\/ otherwise, shift\n-    __ vmnot_m(v0, v0);\n-    __ vsll_vv(as_VectorRegister($dst$$reg), as_VectorRegister($src$$reg),\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ rvv_vsetvli(bt, Matcher::vector_length_in_bytes(this));\n+    __ vsra_vv(as_VectorRegister($dst_src$$reg), as_VectorRegister($dst_src$$reg),\n@@ -1358,1 +1586,2 @@\n-instruct vlslS(vReg dst, vReg src, vReg shift) %{\n+instruct vlslBS(vReg dst, vReg src, vReg shift, vRegMask_V0 tmp) %{\n+  match(Set dst (LShiftVB src shift));\n@@ -1361,5 +1590,2 @@\n-  effect(TEMP_DEF dst);\n-  format %{ \"vmsgtu.vi v0, $shift, 15\\t#@vlslS\\n\\t\"\n-            \"vxor.vv $dst, $src, $src, Assembler::v0_t\\n\\t\"\n-            \"vmnot.m v0, v0\\n\\t\"\n-            \"vsll.vv $dst, $src, $shift, Assembler::v0_t\" %}\n+  effect( TEMP_DEF dst, TEMP tmp);\n+  format %{ \"vlslBS $dst, $src, $shift $tmp\" %}\n@@ -1367,3 +1593,5 @@\n-    __ rvv_vsetvli(T_SHORT, Matcher::vector_length_in_bytes(this));\n-    \/\/ if shift > BitsPerShort - 1, clear the element\n-    __ vmsgtu_vi(v0, as_VectorRegister($shift$$reg), BitsPerShort - 1);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int bitsPerElement = type2aelembytes(bt) * 8;\n+    __ rvv_vsetvli(bt, Matcher::vector_length_in_bytes(this));\n+    \/\/ if shift > bitsPerElement - 1, clear the element\n+    __ vmsgtu_vi(as_VectorRegister($tmp$$reg), as_VectorRegister($shift$$reg), bitsPerElement - 1);\n@@ -1373,1 +1601,1 @@\n-    __ vmnot_m(v0, v0);\n+    __ vmnot_m(as_VectorRegister($tmp$$reg), as_VectorRegister($tmp$$reg));\n@@ -1380,1 +1608,1 @@\n-instruct vlslI(vReg dst, vReg src, vReg shift) %{\n+instruct vlslIL(vReg dst, vReg src, vReg shift) %{\n@@ -1382,0 +1610,1 @@\n+  match(Set dst (LShiftVL src shift));\n@@ -1383,1 +1612,1 @@\n-  format %{ \"vsll.vv $dst, $src, $shift\\t#@vlslI\" %}\n+  format %{ \"vlslIL $dst, $src, $shift\" %}\n@@ -1385,1 +1614,2 @@\n-    __ rvv_vsetvli(T_INT, Matcher::vector_length_in_bytes(this));\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ rvv_vsetvli(bt, Matcher::vector_length_in_bytes(this));\n@@ -1392,2 +1622,3 @@\n-instruct vlslL(vReg dst, vReg src, vReg shift) %{\n-  match(Set dst (LShiftVL src shift));\n+instruct vlslBS_masked(vReg dst_src, vReg shift, vRegMask_V0 vmask, vReg tmp) %{\n+  match(Set dst_src (LShiftVB (Binary dst_src shift) vmask));\n+  match(Set dst_src (LShiftVS (Binary dst_src shift) vmask));\n@@ -1395,1 +1626,2 @@\n-  format %{ \"vsll.vv $dst, $src, $shift\\t# vector (D)\" %}\n+  effect(TEMP_DEF dst_src, USE vmask, TEMP tmp);\n+  format %{ \"vlslBS_masked $dst_src, $dst_src, $shift, $vmask\" %}\n@@ -1397,3 +1629,14 @@\n-    __ rvv_vsetvli(T_LONG, Matcher::vector_length_in_bytes(this));\n-    __ vsll_vv(as_VectorRegister($dst$$reg), as_VectorRegister($src$$reg),\n-               as_VectorRegister($shift$$reg));\n+    __ vmv1r_v(as_VectorRegister($tmp$$reg), as_VectorRegister($vmask$$reg));\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int bitsPerElement = type2aelembytes(bt) * 8;\n+    __ rvv_vsetvli(bt, Matcher::vector_length_in_bytes(this));\n+    \/\/ if shift > bitsPerElement - 1, clear the element\n+    __ vmsgtu_vi(as_VectorRegister($vmask$$reg), as_VectorRegister($shift$$reg), bitsPerElement - 1);\n+    __ vmand_mm(as_VectorRegister($vmask$$reg), as_VectorRegister($vmask$$reg),\n+                as_VectorRegister($tmp$$reg));\n+    __ vxor_vv(as_VectorRegister($dst_src$$reg), as_VectorRegister($dst_src$$reg),\n+               as_VectorRegister($dst_src$$reg), Assembler::v0_t);\n+    \/\/ otherwise, shift\n+    __ vmv1r_v(as_VectorRegister($vmask$$reg), as_VectorRegister($tmp$$reg));\n+    __ vsll_vv(as_VectorRegister($dst_src$$reg), as_VectorRegister($dst_src$$reg),\n+               as_VectorRegister($shift$$reg), Assembler::v0_t);\n@@ -1404,2 +1647,3 @@\n-instruct vlsrB(vReg dst, vReg src, vReg shift) %{\n-  match(Set dst (URShiftVB src shift));\n+instruct vlslIL_masked(vReg dst_src, vReg shift, vRegMask_V0 vmask) %{\n+  match(Set dst_src (LShiftVI (Binary dst_src shift) vmask));\n+  match(Set dst_src (LShiftVL (Binary dst_src shift) vmask));\n@@ -1407,5 +1651,1 @@\n-  effect(TEMP_DEF dst);\n-  format %{ \"vmsgtu.vi v0, $shift, 7\\t#@vlsrB\\n\\t\"\n-            \"vxor.vv $dst, $src, $src, Assembler::v0_t\\n\\t\"\n-            \"vmnot.m v0, v0, v0\\n\\t\"\n-            \"vsll.vv $dst, $src, $shift, Assembler::v0_t\" %}\n+  format %{ \"vlslIL_masked $dst_src, $dst_src, $shift, $vmask\" %}\n@@ -1413,8 +1653,3 @@\n-    __ rvv_vsetvli(T_BYTE, Matcher::vector_length_in_bytes(this));\n-    \/\/ if shift > BitsPerByte - 1, clear the element\n-    __ vmsgtu_vi(v0, as_VectorRegister($shift$$reg), BitsPerByte - 1);\n-    __ vxor_vv(as_VectorRegister($dst$$reg), as_VectorRegister($src$$reg),\n-               as_VectorRegister($src$$reg), Assembler::v0_t);\n-    \/\/ otherwise, shift\n-    __ vmnot_m(v0, v0);\n-    __ vsrl_vv(as_VectorRegister($dst$$reg), as_VectorRegister($src$$reg),\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ rvv_vsetvli(bt, Matcher::vector_length_in_bytes(this));\n+    __ vsll_vv(as_VectorRegister($dst_src$$reg), as_VectorRegister($dst_src$$reg),\n@@ -1426,1 +1661,2 @@\n-instruct vlsrS(vReg dst, vReg src, vReg shift) %{\n+instruct vlsrBS(vReg dst, vReg src, vReg shift, vRegMask_V0 tmp) %{\n+  match(Set dst (URShiftVB src shift));\n@@ -1429,5 +1665,2 @@\n-  effect(TEMP_DEF dst);\n-  format %{ \"vmsgtu.vi v0, $shift, 15\\t#@vlsrS\\n\\t\"\n-            \"vxor.vv $dst, $src, $src, Assembler::v0_t\\n\\t\"\n-            \"vmnot.m v0, v0\\n\\t\"\n-            \"vsll.vv $dst, $src, $shift, Assembler::v0_t\" %}\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"vlsrBS $dst, $src, $shift $tmp\" %}\n@@ -1435,3 +1668,5 @@\n-    __ rvv_vsetvli(T_SHORT, Matcher::vector_length_in_bytes(this));\n-    \/\/ if shift > BitsPerShort - 1, clear the element\n-    __ vmsgtu_vi(v0, as_VectorRegister($shift$$reg), BitsPerShort - 1);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int bitsPerElement = type2aelembytes(bt) * 8;\n+    __ rvv_vsetvli(bt, Matcher::vector_length_in_bytes(this));\n+    \/\/ if shift > bitsPerElement - 1, clear the element\n+    __ vmsgtu_vi(as_VectorRegister($tmp$$reg), as_VectorRegister($shift$$reg), bitsPerElement - 1);\n@@ -1441,1 +1676,1 @@\n-    __ vmnot_m(v0, v0);\n+    __ vmnot_m(as_VectorRegister($tmp$$reg), as_VectorRegister($tmp$$reg));\n@@ -1448,2 +1683,1 @@\n-\n-instruct vlsrI(vReg dst, vReg src, vReg shift) %{\n+instruct vlsrIL(vReg dst, vReg src, vReg shift) %{\n@@ -1451,0 +1685,1 @@\n+  match(Set dst (URShiftVL src shift));\n@@ -1452,1 +1687,1 @@\n-  format %{ \"vsrl.vv $dst, $src, $shift\\t#@vlsrI\" %}\n+  format %{ \"vlsrIL $dst, $src, $shift\" %}\n@@ -1454,1 +1689,2 @@\n-    __ rvv_vsetvli(T_INT, Matcher::vector_length_in_bytes(this));\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ rvv_vsetvli(bt, Matcher::vector_length_in_bytes(this));\n@@ -1461,0 +1697,24 @@\n+instruct vlsrBS_masked(vReg dst_src, vReg shift, vRegMask_V0 vmask, vReg tmp) %{\n+  match(Set dst_src (URShiftVB (Binary dst_src shift) vmask));\n+  match(Set dst_src (URShiftVS (Binary dst_src shift) vmask));\n+  ins_cost(VEC_COST);\n+  effect(TEMP_DEF dst_src, USE vmask, TEMP tmp);\n+  format %{ \"vlsrBS_masked $dst_src, $dst_src, $shift, $vmask\" %}\n+  ins_encode %{\n+    __ vmv1r_v(as_VectorRegister($tmp$$reg), as_VectorRegister($vmask$$reg));\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int bitsPerElement = type2aelembytes(bt) * 8;\n+    __ rvv_vsetvli(bt, Matcher::vector_length_in_bytes(this));\n+    \/\/ if shift > bitsPerElement - 1, clear the element\n+    __ vmsgtu_vi(as_VectorRegister($vmask$$reg), as_VectorRegister($shift$$reg), bitsPerElement - 1);\n+    __ vmand_mm(as_VectorRegister($vmask$$reg), as_VectorRegister($vmask$$reg),\n+                as_VectorRegister($tmp$$reg));\n+    __ vxor_vv(as_VectorRegister($dst_src$$reg), as_VectorRegister($dst_src$$reg),\n+               as_VectorRegister($dst_src$$reg), Assembler::v0_t);\n+    \/\/ otherwise, shift\n+    __ vmv1r_v(as_VectorRegister($vmask$$reg), as_VectorRegister($tmp$$reg));\n+    __ vsrl_vv(as_VectorRegister($dst_src$$reg), as_VectorRegister($dst_src$$reg),\n+               as_VectorRegister($shift$$reg), Assembler::v0_t);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n@@ -1462,2 +1722,3 @@\n-instruct vlsrL(vReg dst, vReg src, vReg shift) %{\n-  match(Set dst (URShiftVL src shift));\n+instruct vlsrIL_masked(vReg dst_src, vReg shift, vRegMask_V0 vmask) %{\n+  match(Set dst_src (URShiftVI (Binary dst_src shift) vmask));\n+  match(Set dst_src (URShiftVL (Binary dst_src shift) vmask));\n@@ -1465,1 +1726,2 @@\n-  format %{ \"vsrl.vv $dst, $src, $shift\\t#@vlsrL\" %}\n+  effect(TEMP_DEF dst_src);\n+  format %{ \"vlsrIL_masked $dst_src, $dst_src, $shift, $vmask\" %}\n@@ -1467,3 +1729,4 @@\n-    __ rvv_vsetvli(T_LONG, Matcher::vector_length_in_bytes(this));\n-    __ vsrl_vv(as_VectorRegister($dst$$reg), as_VectorRegister($src$$reg),\n-               as_VectorRegister($shift$$reg));\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ rvv_vsetvli(bt, Matcher::vector_length_in_bytes(this));\n+    __ vsrl_vv(as_VectorRegister($dst_src$$reg), as_VectorRegister($dst_src$$reg),\n+               as_VectorRegister($shift$$reg), Assembler::v0_t);\n@@ -1830,0 +2093,74 @@\n+\/\/ vector sub - predicated\n+\n+instruct vsubB_masked(vReg dst_src1, vReg src2, vRegMask_V0 vmask) %{\n+  match(Set dst_src1 (SubVB (Binary dst_src1 src2) vmask));\n+  ins_cost(VEC_COST);\n+  format %{ \"vsub.vv $dst_src1, $src2, $vmask\\t#@vsubB_masked\" %}\n+  ins_encode %{\n+    __ rvv_vsetvli(T_BYTE, Matcher::vector_length_in_bytes(this));\n+    __ vsub_vv(as_VectorRegister($dst_src1$$reg), as_VectorRegister($dst_src1$$reg),\n+               as_VectorRegister($src2$$reg), Assembler::v0_t);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vsubS_masked(vReg dst_src1, vReg src2, vRegMask_V0 vmask) %{\n+  match(Set dst_src1 (SubVS (Binary dst_src1 src2) vmask));\n+  ins_cost(VEC_COST);\n+  format %{ \"vsub.vv $dst_src1, $src2, $vmask\\t#@vsubS_masked\" %}\n+  ins_encode %{\n+    __ rvv_vsetvli(T_SHORT, Matcher::vector_length_in_bytes(this));\n+    __ vsub_vv(as_VectorRegister($dst_src1$$reg), as_VectorRegister($dst_src1$$reg),\n+               as_VectorRegister($src2$$reg), Assembler::v0_t);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vsubI_masked(vReg dst_src1, vReg src2, vRegMask_V0 vmask) %{\n+  match(Set dst_src1 (SubVI (Binary dst_src1 src2) vmask));\n+  ins_cost(VEC_COST);\n+  format %{ \"vsub.vv $dst_src1, $src2, $vmask\\t#@vsubI_masked\" %}\n+  ins_encode %{\n+    __ rvv_vsetvli(T_INT, Matcher::vector_length_in_bytes(this));\n+    __ vsub_vv(as_VectorRegister($dst_src1$$reg), as_VectorRegister($dst_src1$$reg),\n+               as_VectorRegister($src2$$reg), Assembler::v0_t);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vsubL_masked(vReg dst_src1, vReg src2, vRegMask_V0 vmask) %{\n+  match(Set dst_src1 (SubVL (Binary dst_src1 src2) vmask));\n+  ins_cost(VEC_COST);\n+  format %{ \"vsub.vv $dst_src1, $src2, $vmask\\t#@vsubL_masked\" %}\n+  ins_encode %{\n+    __ rvv_vsetvli(T_LONG, Matcher::vector_length_in_bytes(this));\n+    __ vsub_vv(as_VectorRegister($dst_src1$$reg), as_VectorRegister($dst_src1$$reg),\n+               as_VectorRegister($src2$$reg), Assembler::v0_t);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vsubF_masked(vReg dst_src1, vReg src2, vRegMask_V0 vmask) %{\n+  match(Set dst_src1 (SubVF (Binary dst_src1 src2) vmask));\n+  ins_cost(VEC_COST);\n+  format %{ \"vfsub.vv $dst_src1, $src2, $vmask\\t#@vsubF_masked\" %}\n+  ins_encode %{\n+    __ rvv_vsetvli(T_FLOAT, Matcher::vector_length_in_bytes(this));\n+    __ vfsub_vv(as_VectorRegister($dst_src1$$reg), as_VectorRegister($dst_src1$$reg),\n+                as_VectorRegister($src2$$reg), Assembler::v0_t);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vsubD_masked(vReg dst_src1, vReg src2, vRegMask_V0 vmask) %{\n+  match(Set dst_src1 (SubVD (Binary dst_src1 src2) vmask));\n+  ins_cost(VEC_COST);\n+  format %{ \"vfsub.vv $dst_src1, $src2, $vmask\\t#@vsubD_masked\" %}\n+  ins_encode %{\n+    __ rvv_vsetvli(T_DOUBLE, Matcher::vector_length_in_bytes(this));\n+    __ vfsub_vv(as_VectorRegister($dst_src1$$reg), as_VectorRegister($dst_src1$$reg),\n+                as_VectorRegister($src2$$reg), Assembler::v0_t);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -1832,1 +2169,1 @@\n-                         vReg_V2 v2, vReg_V3 v3, rFlagsReg cr)\n+                         vReg_V2 v2, vReg_V3 v3, vRegMask_V0 v0, rFlagsReg cr)\n@@ -1836,1 +2173,1 @@\n-  effect(USE_KILL str1, USE_KILL str2, USE_KILL cnt, TEMP v1, TEMP v2, TEMP v3, KILL cr);\n+  effect(USE_KILL str1, USE_KILL str2, USE_KILL cnt, TEMP v1, TEMP v2, TEMP v3, TEMP v0, KILL cr);\n@@ -1849,1 +2186,1 @@\n-                         vReg_V2 v2, vReg_V3 v3, rFlagsReg cr)\n+                         vReg_V2 v2, vReg_V3 v3, vRegMask_V0 v0, rFlagsReg cr)\n@@ -1853,1 +2190,1 @@\n-  effect(USE_KILL str1, USE_KILL str2, USE_KILL cnt, TEMP v1, TEMP v2, TEMP v3, KILL cr);\n+  effect(USE_KILL str1, USE_KILL str2, USE_KILL cnt, TEMP v1, TEMP v2, TEMP v3, TEMP v0, KILL cr);\n@@ -1865,1 +2202,1 @@\n-                        vReg_V1 v1, vReg_V2 v2, vReg_V3 v3, iRegP_R28 tmp, rFlagsReg cr)\n+                        vReg_V1 v1, vReg_V2 v2, vReg_V3 v3, vRegMask_V0 v0, iRegP_R28 tmp, rFlagsReg cr)\n@@ -1869,1 +2206,1 @@\n-  effect(KILL tmp, USE_KILL ary1, USE_KILL ary2, TEMP v1, TEMP v2, TEMP v3, KILL cr);\n+  effect(KILL tmp, USE_KILL ary1, USE_KILL ary2, TEMP v1, TEMP v2, TEMP v3, TEMP v0, KILL cr);\n@@ -1880,1 +2217,1 @@\n-                        vReg_V1 v1, vReg_V2 v2, vReg_V3 v3, iRegP_R28 tmp, rFlagsReg cr)\n+                        vReg_V1 v1, vReg_V2 v2, vReg_V3 v3, vRegMask_V0 v0, iRegP_R28 tmp, rFlagsReg cr)\n@@ -1884,1 +2221,1 @@\n-  effect(KILL tmp, USE_KILL ary1, USE_KILL ary2, TEMP v1, TEMP v2, TEMP v3, KILL cr);\n+  effect(KILL tmp, USE_KILL ary1, USE_KILL ary2, TEMP v1, TEMP v2, TEMP v3, TEMP v0, KILL cr);\n@@ -1896,1 +2233,1 @@\n-                          iRegP_R28 tmp1, iRegL_R29 tmp2)\n+                          vRegMask_V0 v0, iRegP_R28 tmp1, iRegL_R29 tmp2)\n@@ -1901,1 +2238,1 @@\n-         TEMP v1, TEMP v2, TEMP v3, TEMP v4, TEMP v5);\n+         TEMP v1, TEMP v2, TEMP v3, TEMP v4, TEMP v5, TEMP v0);\n@@ -1915,1 +2252,1 @@\n-                          iRegP_R28 tmp1, iRegL_R29 tmp2)\n+                          vRegMask_V0 v0, iRegP_R28 tmp1, iRegL_R29 tmp2)\n@@ -1920,1 +2257,1 @@\n-         TEMP v1, TEMP v2, TEMP v3, TEMP v4, TEMP v5);\n+         TEMP v1, TEMP v2, TEMP v3, TEMP v4, TEMP v5, TEMP v0);\n@@ -1934,1 +2271,1 @@\n-                           iRegP_R28 tmp1, iRegL_R29 tmp2)\n+                           vRegMask_V0 v0, iRegP_R28 tmp1, iRegL_R29 tmp2)\n@@ -1939,1 +2276,1 @@\n-         TEMP v1, TEMP v2, TEMP v3, TEMP v4, TEMP v5);\n+         TEMP v1, TEMP v2, TEMP v3, TEMP v4, TEMP v5, TEMP v0);\n@@ -1952,1 +2289,1 @@\n-                           iRegP_R28 tmp1, iRegL_R29 tmp2)\n+                           vRegMask_V0 v0, iRegP_R28 tmp1, iRegL_R29 tmp2)\n@@ -1957,1 +2294,1 @@\n-         TEMP v1, TEMP v2, TEMP v3, TEMP v4, TEMP v5);\n+         TEMP v1, TEMP v2, TEMP v3, TEMP v4, TEMP v5, TEMP v0);\n@@ -1971,1 +2308,1 @@\n-                         vReg_V1 v1, vReg_V2 v2, vReg_V3 v3, iRegLNoSp tmp)\n+                         vReg_V1 v1, vReg_V2 v2, vReg_V3 v3, vRegMask_V0 v0, iRegLNoSp tmp)\n@@ -1975,1 +2312,1 @@\n-  effect(TEMP v1, TEMP v2, TEMP v3, TEMP tmp, USE_KILL src, USE_KILL dst, USE_KILL len);\n+  effect(TEMP v1, TEMP v2, TEMP v3, TEMP v0, TEMP tmp, USE_KILL src, USE_KILL dst, USE_KILL len);\n@@ -1986,1 +2323,1 @@\n-                           vReg_V1 v1, vReg_V2 v2, vReg_V3 v3, iRegLNoSp tmp)\n+                           vReg_V1 v1, vReg_V2 v2, vReg_V3 v3, vRegMask_V0 v0, iRegLNoSp tmp)\n@@ -1991,1 +2328,1 @@\n-         TEMP v1, TEMP v2, TEMP v3, TEMP tmp);\n+         TEMP v1, TEMP v2, TEMP v3, TEMP tmp, TEMP v0);\n@@ -2003,1 +2340,1 @@\n-                          vReg_V1 v1, vReg_V2 v2, vReg_V3 v3, iRegLNoSp tmp)\n+                          vReg_V1 v1, vReg_V2 v2, vReg_V3 v3, vRegMask_V0 v0, iRegLNoSp tmp)\n@@ -2008,1 +2345,1 @@\n-         TEMP v1, TEMP v2, TEMP v3, TEMP tmp);\n+         TEMP v1, TEMP v2, TEMP v3, TEMP tmp, TEMP v0);\n@@ -2019,1 +2356,1 @@\n-                          vReg_V1 v1, vReg_V2 v2, vReg_V3 v3, iRegLNoSp tmp)\n+                          vReg_V1 v1, vReg_V2 v2, vReg_V3 v3, vRegMask_V0 v0, iRegLNoSp tmp)\n@@ -2023,1 +2360,1 @@\n-  effect(TEMP_DEF result, USE_KILL ary, USE_KILL len, TEMP v1, TEMP v2, TEMP v3, TEMP tmp);\n+  effect(TEMP_DEF result, USE_KILL ary, USE_KILL len, TEMP v1, TEMP v2, TEMP v3, TEMP tmp, TEMP v0);\n@@ -2035,1 +2372,1 @@\n-                               vReg_V1 v1, vReg_V2 v2, vReg_V3 v3)\n+                               vReg_V1 v1, vReg_V2 v2, vReg_V3 v3, vRegMask_V0 v0)\n@@ -2040,1 +2377,1 @@\n-         TEMP tmp1, TEMP tmp2, TEMP v1, TEMP v2, TEMP v3);\n+         TEMP tmp1, TEMP tmp2, TEMP v1, TEMP v2, TEMP v3, TEMP v0);\n@@ -2055,1 +2392,1 @@\n-                               vReg_V1 v1, vReg_V2 v2, vReg_V3 v3)\n+                               vReg_V1 v1, vReg_V2 v2, vReg_V3 v3, vRegMask_V0 v0)\n@@ -2060,1 +2397,1 @@\n-         TEMP tmp1, TEMP tmp2, TEMP v1, TEMP v2, TEMP v3);\n+         TEMP tmp1, TEMP tmp2, TEMP v1, TEMP v2, TEMP v3, TEMP v0);\n@@ -2075,1 +2412,1 @@\n-                             vReg_V1 vReg1, vReg_V2 vReg2, vReg_V3 vReg3)\n+                             vReg_V1 vReg1, vReg_V2 vReg2, vReg_V3 vReg3, vRegMask_V0 v0)\n@@ -2079,1 +2416,1 @@\n-  effect(USE_KILL cnt, USE_KILL base, TEMP vReg1, TEMP vReg2, TEMP vReg3);\n+  effect(USE_KILL cnt, USE_KILL base, TEMP vReg1, TEMP vReg2, TEMP vReg3, TEMP v0);\n@@ -2104,0 +2441,187 @@\n+%}\n+\n+instruct vmask_gen_I(vRegMask dst, iRegI src) %{\n+  match(Set dst (VectorMaskGen (ConvI2L src)));\n+  format %{ \"vmask_gen_I $dst, $src\" %}\n+  ins_encode %{\n+    __ clear_register_v(as_VectorRegister($dst$$reg));\n+    __ vsetvli(t0, $src$$Register, Assembler::e8);\n+    __ vmxnor_mm(as_VectorRegister($dst$$reg), as_VectorRegister($dst$$reg), as_VectorRegister($dst$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmask_gen_L(vRegMask dst, iRegL src) %{\n+  match(Set dst (VectorMaskGen src));\n+  format %{ \"vmask_gen_L $dst, $src\" %}\n+  ins_encode %{\n+    __ clear_register_v(as_VectorRegister($dst$$reg));\n+    __ vsetvli(t0, $src$$Register, Assembler::e8);\n+    __ vmxnor_mm(as_VectorRegister($dst$$reg), as_VectorRegister($dst$$reg), as_VectorRegister($dst$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmask_gen_imm(vRegMask dst, immL con) %{\n+  match(Set dst (VectorMaskGen con));\n+  format %{ \"vmask_gen_imm $dst, $con\" %}\n+  ins_encode %{\n+    __ clear_register_v(as_VectorRegister($dst$$reg));\n+    __ rvv_vsetvli(T_BYTE, (uint)($con$$constant));\n+    __ vmxnor_mm(as_VectorRegister($dst$$reg), as_VectorRegister($dst$$reg), as_VectorRegister($dst$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmaskAll_immI(vRegMask dst, immI src) %{\n+  match(Set dst (MaskAll src));\n+  format %{ \"vmaskAll_immI $dst, $src\" %}\n+  ins_encode %{\n+    int con = (int)$src$$constant;\n+    if (con == 0) {\n+      __ vsetvli(t0, x0, Assembler::e8);\n+      __ vmxor_mm(as_VectorRegister($dst$$reg), as_VectorRegister($dst$$reg), as_VectorRegister($dst$$reg));\n+    } else {\n+      assert(con == -1, \"invalid constant value for mask\");\n+      __ vsetvli(t0, x0, Assembler::e8);\n+      __ vid_v(as_VectorRegister($dst$$reg));\n+      __ mv(t0, Matcher::vector_length(this));\n+      __ vmslt_vx(as_VectorRegister($dst$$reg), as_VectorRegister($dst$$reg), t0);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmaskAllI(vRegMask dst, iRegI src, vReg tmp) %{\n+  match(Set dst (MaskAll src));\n+  effect(TEMP tmp);\n+  format %{ \"vmaskAllI $dst, $src\\t# KILL $tmp\" %}\n+  ins_encode %{\n+    __ clear_register_v(as_VectorRegister($dst$$reg));\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ rvv_vsetvli(bt, Matcher::vector_length_in_bytes(this));\n+    __ vmv_v_x(as_VectorRegister($tmp$$reg), as_Register($src$$reg));\n+    __ vmsne_vx(as_VectorRegister($dst$$reg), as_VectorRegister($tmp$$reg), x0);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmaskAll_immL(vRegMask dst, immL src) %{\n+  match(Set dst (MaskAll src));\n+  format %{ \"vmaskAll_immL $dst, $src\" %}\n+  ins_encode %{\n+    long con = (long)$src$$constant;\n+    if (con == 0) {\n+      __ vsetvli(t0, x0, Assembler::e8);\n+      __ vmxor_mm(as_VectorRegister($dst$$reg), as_VectorRegister($dst$$reg), as_VectorRegister($dst$$reg));\n+    } else {\n+      assert(con == -1, \"invalid constant value for mask\");\n+      __ vsetvli(t0, x0, Assembler::e8);\n+      __ vid_v(as_VectorRegister($dst$$reg));\n+      __ mv(t0, Matcher::vector_length(this));\n+      __ vmslt_vx(as_VectorRegister($dst$$reg), as_VectorRegister($dst$$reg), t0);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmaskAllL(vRegMask dst, iRegL src, vReg tmp) %{\n+  match(Set dst (MaskAll src));\n+  effect(TEMP tmp);\n+  format %{ \"vmaskAllL $dst, $src\\t# KILL $tmp\" %}\n+  ins_encode %{\n+    __ clear_register_v(as_VectorRegister($dst$$reg));\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ rvv_vsetvli(bt, Matcher::vector_length_in_bytes(this));\n+    __ vmv_v_x(as_VectorRegister($tmp$$reg), as_Register($src$$reg));\n+    __ vmsne_vx(as_VectorRegister($dst$$reg), as_VectorRegister($tmp$$reg), x0);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ Vector mask basic OPs ------------------------\n+\n+\/\/ vector mask logical ops: and\/or\/xor\n+\n+instruct vmask_and(vRegMask dst, vRegMask src1, vRegMask src2) %{\n+  match(Set dst (AndVMask src1 src2));\n+  format %{ \"vmask_and $dst, $src1, $src2\" %}\n+  ins_encode %{\n+    __ rvv_vsetvli(T_BYTE, Matcher::vector_length_in_bytes(this));\n+    __ vmand_mm(as_VectorRegister($dst$$reg),\n+                as_VectorRegister($src1$$reg),\n+                as_VectorRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmask_or(vRegMask dst, vRegMask src1, vRegMask src2) %{\n+  match(Set dst (OrVMask src1 src2));\n+  format %{ \"vmask_or $dst, $src1, $src2\" %}\n+  ins_encode %{\n+    __ rvv_vsetvli(T_BYTE, Matcher::vector_length_in_bytes(this));\n+    __ vmor_mm(as_VectorRegister($dst$$reg),\n+               as_VectorRegister($src1$$reg),\n+               as_VectorRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmask_xor(vRegMask dst, vRegMask src1, vRegMask src2) %{\n+  match(Set dst (XorVMask src1 src2));\n+  format %{ \"vmask_xor $dst, $src1, $src2\" %}\n+  ins_encode %{\n+    __ rvv_vsetvli(T_BYTE, Matcher::vector_length_in_bytes(this));\n+    __ vmxor_mm(as_VectorRegister($dst$$reg),\n+                as_VectorRegister($src1$$reg),\n+                as_VectorRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmaskcast(vRegMask dst_src) %{\n+  match(Set dst_src (VectorMaskCast dst_src));\n+  ins_cost(0);\n+  format %{ \"vmaskcast $dst_src\\t# do nothing\" %}\n+  ins_encode(\/* empty encoding *\/);\n+  ins_pipe(pipe_class_empty);\n+%}\n+\n+\/\/ vector load\/store - predicated\n+\n+instruct loadV_masked(vReg dst, vmemA mem, vRegMask_V0 vmask) %{\n+  match(Set dst (LoadVectorMasked mem vmask));\n+  format %{ \"loadV_masked $dst, $vmask, $mem\" %}\n+  ins_encode %{\n+    VectorRegister dst_reg = as_VectorRegister($dst$$reg);\n+    loadStore(C2_MacroAssembler(&cbuf), false, dst_reg,\n+              Matcher::vector_element_basic_type(this), as_Register($mem$$base),\n+              Matcher::vector_length_in_bytes(this), Assembler::v0_t);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct storeV_masked(vReg src, vmemA mem, vRegMask_V0 vmask) %{\n+  match(Set mem (StoreVectorMasked mem (Binary src vmask)));\n+  format %{ \"storeV_masked $mem, $vmask, $src\" %}\n+  ins_encode %{\n+    VectorRegister src_reg = as_VectorRegister($src$$reg);\n+    loadStore(C2_MacroAssembler(&cbuf), true, src_reg,\n+              Matcher::vector_element_basic_type(this, $src), as_Register($mem$$base),\n+              Matcher::vector_length_in_bytes(this, $src), Assembler::v0_t);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ Vector blend ---------------------------------\n+\n+instruct vblend(vReg dst, vReg src1, vReg src2, vRegMask_V0 vmask) %{\n+  match(Set dst (VectorBlend (Binary src1 src2) vmask));\n+  format %{ \"vmerge_vvm $dst, $src1, $src2\\t# vector blend\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ rvv_vsetvli(bt, Matcher::vector_length_in_bytes(this));\n+    __ vmerge_vvm(as_VectorRegister($dst$$reg), as_VectorRegister($src1$$reg),\n+                  as_VectorRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n","filename":"src\/hotspot\/cpu\/riscv\/riscv_v.ad","additions":671,"deletions":147,"binary":false,"changes":818,"status":"modified"}]}