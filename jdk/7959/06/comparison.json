{"files":[{"patch":"@@ -1784,10 +1784,0 @@\n-int MachCallNativeNode::ret_addr_offset() {\n-  \/\/ This is implemented using aarch64_enc_java_to_runtime as above.\n-  CodeBlob *cb = CodeCache::find_blob(_entry_point);\n-  if (cb) {\n-    return 1 * NativeInstruction::instruction_size;\n-  } else {\n-    return 6 * NativeInstruction::instruction_size;\n-  }\n-}\n-\n@@ -16763,15 +16753,0 @@\n-instruct CallNativeDirect(method meth)\n-%{\n-  match(CallNative);\n-\n-  effect(USE meth);\n-\n-  ins_cost(CALL_COST);\n-\n-  format %{ \"CALL, native $meth\" %}\n-\n-  ins_encode( aarch64_enc_java_to_runtime(meth) );\n-\n-  ins_pipe(pipe_class_call);\n-%}\n-\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":0,"deletions":25,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -2,2 +2,2 @@\n- * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2019, Arm Limited. All rights reserved.\n+ * Copyright (c) 2020, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2019, 2021, Arm Limited. All rights reserved.\n@@ -26,0 +26,1 @@\n+#include \"code\/vmreg.inline.hpp\"\n@@ -29,0 +30,2 @@\n+#include \"oops\/oopCast.inline.hpp\"\n+#include \"opto\/matcher.hpp\"\n@@ -31,0 +34,1 @@\n+#include \"utilities\/formatBuffer.hpp\"\n@@ -42,2 +46,2 @@\n-#define INTEGER_TYPE 0\n-#define VECTOR_TYPE 1\n+static constexpr int INTEGER_TYPE = 0;\n+static constexpr int VECTOR_TYPE = 1;\n@@ -45,1 +49,1 @@\n-const ABIDescriptor ForeignGlobals::parse_abi_descriptor_impl(jobject jabi) const {\n+const ABIDescriptor ForeignGlobals::parse_abi_descriptor(jobject jabi) {\n@@ -50,1 +54,1 @@\n-  objArrayOop inputStorage = cast<objArrayOop>(abi_oop->obj_field(ABI.inputStorage_offset));\n+  objArrayOop inputStorage = jdk_internal_foreign_abi_ABIDescriptor::inputStorage(abi_oop);\n@@ -54,1 +58,1 @@\n-  objArrayOop outputStorage = cast<objArrayOop>(abi_oop->obj_field(ABI.outputStorage_offset));\n+  objArrayOop outputStorage = jdk_internal_foreign_abi_ABIDescriptor::outputStorage(abi_oop);\n@@ -58,1 +62,1 @@\n-  objArrayOop volatileStorage = cast<objArrayOop>(abi_oop->obj_field(ABI.volatileStorage_offset));\n+  objArrayOop volatileStorage = jdk_internal_foreign_abi_ABIDescriptor::volatileStorage(abi_oop);\n@@ -62,2 +66,5 @@\n-  abi._stack_alignment_bytes = abi_oop->int_field(ABI.stackAlignment_offset);\n-  abi._shadow_space_bytes = abi_oop->int_field(ABI.shadowSpace_offset);\n+  abi._stack_alignment_bytes = jdk_internal_foreign_abi_ABIDescriptor::stackAlignment(abi_oop);\n+  abi._shadow_space_bytes = jdk_internal_foreign_abi_ABIDescriptor::shadowSpace(abi_oop);\n+\n+  abi._target_addr_reg = parse_vmstorage(jdk_internal_foreign_abi_ABIDescriptor::targetAddrStorage(abi_oop))->as_Register();\n+  abi._ret_buf_addr_reg = parse_vmstorage(jdk_internal_foreign_abi_ABIDescriptor::retBufAddrStorage(abi_oop))->as_Register();\n@@ -68,11 +75,14 @@\n-const BufferLayout ForeignGlobals::parse_buffer_layout_impl(jobject jlayout) const {\n-  oop layout_oop = JNIHandles::resolve_non_null(jlayout);\n-  BufferLayout layout;\n-\n-  layout.stack_args_bytes = layout_oop->long_field(BL.stack_args_bytes_offset);\n-  layout.stack_args = layout_oop->long_field(BL.stack_args_offset);\n-  layout.arguments_next_pc = layout_oop->long_field(BL.arguments_next_pc_offset);\n-\n-  typeArrayOop input_offsets = cast<typeArrayOop>(layout_oop->obj_field(BL.input_type_offsets_offset));\n-  layout.arguments_integer = (size_t) input_offsets->long_at(INTEGER_TYPE);\n-  layout.arguments_vector = (size_t) input_offsets->long_at(VECTOR_TYPE);\n+enum class RegType {\n+  INTEGER = 0,\n+  VECTOR = 1,\n+  STACK = 3\n+};\n+\n+VMReg ForeignGlobals::vmstorage_to_vmreg(int type, int index) {\n+  switch(static_cast<RegType>(type)) {\n+    case RegType::INTEGER: return ::as_Register(index)->as_VMReg();\n+    case RegType::VECTOR: return ::as_FloatRegister(index)->as_VMReg();\n+    case RegType::STACK: return VMRegImpl::stack2reg(index LP64_ONLY(* 2));\n+  }\n+  return VMRegImpl::Bad();\n+}\n@@ -80,3 +90,12 @@\n-  typeArrayOop output_offsets = cast<typeArrayOop>(layout_oop->obj_field(BL.output_type_offsets_offset));\n-  layout.returns_integer = (size_t) output_offsets->long_at(INTEGER_TYPE);\n-  layout.returns_vector = (size_t) output_offsets->long_at(VECTOR_TYPE);\n+int RegSpiller::pd_reg_size(VMReg reg) {\n+  if (reg->is_Register()) {\n+    return 8;\n+  } else if (reg->is_FloatRegister()) {\n+    bool use_sve = Matcher::supports_scalable_vector();\n+    if (use_sve) {\n+      return Matcher::scalable_vector_reg_size(T_BYTE);\n+    }\n+    return 16;\n+  }\n+  return 0; \/\/ stack and BAD\n+}\n@@ -84,1 +103,14 @@\n-  layout.buffer_size = layout_oop->long_field(BL.size_offset);\n+void RegSpiller::pd_store_reg(MacroAssembler* masm, int offset, VMReg reg) {\n+  if (reg->is_Register()) {\n+    masm->spill(reg->as_Register(), true, offset);\n+  } else if (reg->is_FloatRegister()) {\n+    bool use_sve = Matcher::supports_scalable_vector();\n+    if (use_sve) {\n+      masm->spill_sve_vector(reg->as_FloatRegister(), offset, Matcher::scalable_vector_reg_size(T_BYTE));\n+    } else {\n+      masm->spill(reg->as_FloatRegister(), masm->Q, offset);\n+    }\n+  } else {\n+    \/\/ stack and BAD\n+  }\n+}\n@@ -86,1 +118,13 @@\n-  return layout;\n+void RegSpiller::pd_load_reg(MacroAssembler* masm, int offset, VMReg reg) {\n+  if (reg->is_Register()) {\n+    masm->unspill(reg->as_Register(), true, offset);\n+  } else if (reg->is_FloatRegister()) {\n+    bool use_sve = Matcher::supports_scalable_vector();\n+    if (use_sve) {\n+      masm->unspill_sve_vector(reg->as_FloatRegister(), offset, Matcher::scalable_vector_reg_size(T_BYTE));\n+    } else {\n+      masm->unspill(reg->as_FloatRegister(), masm->Q, offset);\n+    }\n+  } else {\n+    \/\/ stack and BAD\n+  }\n@@ -89,3 +133,35 @@\n-const CallRegs ForeignGlobals::parse_call_regs_impl(jobject jconv) const {\n-  ShouldNotCallThis();\n-  return {};\n+void ArgumentShuffle::pd_generate(MacroAssembler* masm, VMReg tmp, int in_stk_bias, int out_stk_bias) const {\n+  assert(in_stk_bias == 0 && out_stk_bias == 0, \"bias not implemented\");\n+  Register tmp_reg = tmp->as_Register();\n+  for (int i = 0; i < _moves.length(); i++) {\n+    Move move = _moves.at(i);\n+    BasicType arg_bt     = move.bt;\n+    VMRegPair from_vmreg = move.from;\n+    VMRegPair to_vmreg   = move.to;\n+\n+    masm->block_comment(err_msg(\"bt=%s\", null_safe_string(type2name(arg_bt))));\n+    switch (arg_bt) {\n+      case T_BOOLEAN:\n+      case T_BYTE:\n+      case T_SHORT:\n+      case T_CHAR:\n+      case T_INT:\n+        masm->move32_64(from_vmreg, to_vmreg, tmp_reg);\n+        break;\n+\n+      case T_FLOAT:\n+        masm->float_move(from_vmreg, to_vmreg, tmp_reg);\n+        break;\n+\n+      case T_DOUBLE:\n+        masm->double_move(from_vmreg, to_vmreg, tmp_reg);\n+        break;\n+\n+      case T_LONG :\n+        masm->long_move(from_vmreg, to_vmreg, tmp_reg);\n+        break;\n+\n+      default:\n+        fatal(\"found in upcall args: %s\", type2name(arg_bt));\n+    }\n+  }\n","filename":"src\/hotspot\/cpu\/aarch64\/foreign_globals_aarch64.cpp","additions":105,"deletions":29,"binary":false,"changes":134,"status":"modified"},{"patch":"@@ -45,0 +45,3 @@\n+  Register _target_addr_reg;\n+  Register _ret_buf_addr_reg;\n+\n@@ -49,11 +52,0 @@\n-struct BufferLayout {\n-  size_t stack_args_bytes;\n-  size_t stack_args;\n-  size_t arguments_vector;\n-  size_t arguments_integer;\n-  size_t arguments_next_pc;\n-  size_t returns_vector;\n-  size_t returns_integer;\n-  size_t buffer_size;\n-};\n-\n","filename":"src\/hotspot\/cpu\/aarch64\/foreign_globals_aarch64.hpp","additions":3,"deletions":11,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -121,0 +121,2 @@\n+    } else if (is_optimized_entry_frame()) {\n+      return fp_safe;\n@@ -223,0 +225,2 @@\n+    } else if (sender_blob->is_optimized_entry_blob()) {\n+      return false;\n@@ -372,2 +376,4 @@\n-  ShouldNotCallThis();\n-  return nullptr;\n+  assert(frame.is_optimized_entry_frame(), \"wrong frame\");\n+  \/\/ need unextended_sp here, since normal sp is wrong for interpreter callees\n+  return reinterpret_cast<OptimizedEntryBlob::FrameData*>(\n+    reinterpret_cast<char*>(frame.unextended_sp()) + in_bytes(_frame_data_offset));\n@@ -377,2 +383,4 @@\n-  ShouldNotCallThis();\n-  return false;\n+  assert(is_optimized_entry_frame(), \"must be optimzed entry frame\");\n+  OptimizedEntryBlob* blob = _cb->as_optimized_entry_blob();\n+  JavaFrameAnchor* jfa = blob->jfa_for_frame(*this);\n+  return jfa->last_Java_sp() == NULL;\n@@ -382,2 +390,19 @@\n-  ShouldNotCallThis();\n-  return {};\n+  assert(map != NULL, \"map must be set\");\n+  OptimizedEntryBlob* blob = _cb->as_optimized_entry_blob();\n+  \/\/ Java frame called from C; skip all C frames and return top C\n+  \/\/ frame of that chunk as the sender\n+  JavaFrameAnchor* jfa = blob->jfa_for_frame(*this);\n+  assert(!optimized_entry_frame_is_first(), \"must have a frame anchor to go back to\");\n+  assert(jfa->last_Java_sp() > sp(), \"must be above this frame on stack\");\n+  \/\/ Since we are walking the stack now this nested anchor is obviously walkable\n+  \/\/ even if it wasn't when it was stacked.\n+  if (!jfa->walkable()) {\n+    \/\/ Capture _last_Java_pc (if needed) and mark anchor walkable.\n+    jfa->capture_last_Java_pc();\n+  }\n+  map->clear();\n+  assert(map->include_argument_oops(), \"should be set by clear\");\n+  vmassert(jfa->last_Java_pc() != NULL, \"not walkable\");\n+  frame fr(jfa->last_Java_sp(), jfa->last_Java_fp(), jfa->last_Java_pc());\n+\n+  return fr;\n","filename":"src\/hotspot\/cpu\/aarch64\/frame_aarch64.cpp","additions":31,"deletions":6,"binary":false,"changes":37,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"compiler\/oopMap.hpp\"\n@@ -300,1 +301,1 @@\n-void MacroAssembler::safepoint_poll(Label& slow_path, bool at_return, bool acquire, bool in_nmethod) {\n+void MacroAssembler::safepoint_poll(Label& slow_path, bool at_return, bool acquire, bool in_nmethod, Register tmp) {\n@@ -302,2 +303,2 @@\n-    lea(rscratch1, Address(rthread, JavaThread::polling_word_offset()));\n-    ldar(rscratch1, rscratch1);\n+    lea(tmp, Address(rthread, JavaThread::polling_word_offset()));\n+    ldar(tmp, tmp);\n@@ -305,1 +306,1 @@\n-    ldr(rscratch1, Address(rthread, JavaThread::polling_word_offset()));\n+    ldr(tmp, Address(rthread, JavaThread::polling_word_offset()));\n@@ -310,1 +311,1 @@\n-    cmp(in_nmethod ? sp : rfp, rscratch1);\n+    cmp(in_nmethod ? sp : rfp, tmp);\n@@ -313,1 +314,11 @@\n-    tbnz(rscratch1, log2i_exact(SafepointMechanism::poll_bit()), slow_path);\n+    tbnz(tmp, log2i_exact(SafepointMechanism::poll_bit()), slow_path);\n+  }\n+}\n+\n+void MacroAssembler::rt_call(address dest, Register tmp) {\n+  CodeBlob *cb = CodeCache::find_blob(dest);\n+  if (cb) {\n+    far_call(RuntimeAddress(dest));\n+  } else {\n+    lea(tmp, RuntimeAddress(dest));\n+    blr(tmp);\n@@ -2211,1 +2222,1 @@\n-  tbz(r0, 0, not_weak);    \/\/ Test for jweak tag.\n+  tbz(value, 0, not_weak);    \/\/ Test for jweak tag.\n@@ -5339,1 +5350,1 @@\n-void MacroAssembler::verify_sve_vector_length() {\n+void MacroAssembler::verify_sve_vector_length(Register tmp) {\n@@ -5343,3 +5354,3 @@\n-  movw(rscratch1, zr);\n-  sve_inc(rscratch1, B);\n-  subsw(zr, rscratch1, VM_Version::get_initial_sve_vector_length());\n+  movw(tmp, zr);\n+  sve_inc(tmp, B);\n+  subsw(zr, tmp, VM_Version::get_initial_sve_vector_length());\n@@ -5505,0 +5516,176 @@\n+\n+\/\/ The java_calling_convention describes stack locations as ideal slots on\n+\/\/ a frame with no abi restrictions. Since we must observe abi restrictions\n+\/\/ (like the placement of the register window) the slots must be biased by\n+\/\/ the following value.\n+static int reg2offset_in(VMReg r) {\n+  \/\/ Account for saved rfp and lr\n+  \/\/ This should really be in_preserve_stack_slots\n+  return (r->reg2stack() + 4) * VMRegImpl::stack_slot_size;\n+}\n+\n+static int reg2offset_out(VMReg r) {\n+  return (r->reg2stack() + SharedRuntime::out_preserve_stack_slots()) * VMRegImpl::stack_slot_size;\n+}\n+\n+\/\/ On 64 bit we will store integer like items to the stack as\n+\/\/ 64 bits items (Aarch64 abi) even though java would only store\n+\/\/ 32bits for a parameter. On 32bit it will simply be 32 bits\n+\/\/ So this routine will do 32->32 on 32bit and 32->64 on 64bit\n+void MacroAssembler::move32_64(VMRegPair src, VMRegPair dst, Register tmp) {\n+  if (src.first()->is_stack()) {\n+    if (dst.first()->is_stack()) {\n+      \/\/ stack to stack\n+      ldr(tmp, Address(rfp, reg2offset_in(src.first())));\n+      str(tmp, Address(sp, reg2offset_out(dst.first())));\n+    } else {\n+      \/\/ stack to reg\n+      ldrsw(dst.first()->as_Register(), Address(rfp, reg2offset_in(src.first())));\n+    }\n+  } else if (dst.first()->is_stack()) {\n+    \/\/ reg to stack\n+    \/\/ Do we really have to sign extend???\n+    \/\/ __ movslq(src.first()->as_Register(), src.first()->as_Register());\n+    str(src.first()->as_Register(), Address(sp, reg2offset_out(dst.first())));\n+  } else {\n+    if (dst.first() != src.first()) {\n+      sxtw(dst.first()->as_Register(), src.first()->as_Register());\n+    }\n+  }\n+}\n+\n+\/\/ An oop arg. Must pass a handle not the oop itself\n+void MacroAssembler::object_move(\n+                        OopMap* map,\n+                        int oop_handle_offset,\n+                        int framesize_in_slots,\n+                        VMRegPair src,\n+                        VMRegPair dst,\n+                        bool is_receiver,\n+                        int* receiver_offset) {\n+\n+  \/\/ must pass a handle. First figure out the location we use as a handle\n+\n+  Register rHandle = dst.first()->is_stack() ? rscratch2 : dst.first()->as_Register();\n+\n+  \/\/ See if oop is NULL if it is we need no handle\n+\n+  if (src.first()->is_stack()) {\n+\n+    \/\/ Oop is already on the stack as an argument\n+    int offset_in_older_frame = src.first()->reg2stack() + SharedRuntime::out_preserve_stack_slots();\n+    map->set_oop(VMRegImpl::stack2reg(offset_in_older_frame + framesize_in_slots));\n+    if (is_receiver) {\n+      *receiver_offset = (offset_in_older_frame + framesize_in_slots) * VMRegImpl::stack_slot_size;\n+    }\n+\n+    ldr(rscratch1, Address(rfp, reg2offset_in(src.first())));\n+    lea(rHandle, Address(rfp, reg2offset_in(src.first())));\n+    \/\/ conditionally move a NULL\n+    cmp(rscratch1, zr);\n+    csel(rHandle, zr, rHandle, Assembler::EQ);\n+  } else {\n+\n+    \/\/ Oop is in an a register we must store it to the space we reserve\n+    \/\/ on the stack for oop_handles and pass a handle if oop is non-NULL\n+\n+    const Register rOop = src.first()->as_Register();\n+    int oop_slot;\n+    if (rOop == j_rarg0)\n+      oop_slot = 0;\n+    else if (rOop == j_rarg1)\n+      oop_slot = 1;\n+    else if (rOop == j_rarg2)\n+      oop_slot = 2;\n+    else if (rOop == j_rarg3)\n+      oop_slot = 3;\n+    else if (rOop == j_rarg4)\n+      oop_slot = 4;\n+    else if (rOop == j_rarg5)\n+      oop_slot = 5;\n+    else if (rOop == j_rarg6)\n+      oop_slot = 6;\n+    else {\n+      assert(rOop == j_rarg7, \"wrong register\");\n+      oop_slot = 7;\n+    }\n+\n+    oop_slot = oop_slot * VMRegImpl::slots_per_word + oop_handle_offset;\n+    int offset = oop_slot*VMRegImpl::stack_slot_size;\n+\n+    map->set_oop(VMRegImpl::stack2reg(oop_slot));\n+    \/\/ Store oop in handle area, may be NULL\n+    str(rOop, Address(sp, offset));\n+    if (is_receiver) {\n+      *receiver_offset = offset;\n+    }\n+\n+    cmp(rOop, zr);\n+    lea(rHandle, Address(sp, offset));\n+    \/\/ conditionally move a NULL\n+    csel(rHandle, zr, rHandle, Assembler::EQ);\n+  }\n+\n+  \/\/ If arg is on the stack then place it otherwise it is already in correct reg.\n+  if (dst.first()->is_stack()) {\n+    str(rHandle, Address(sp, reg2offset_out(dst.first())));\n+  }\n+}\n+\n+\/\/ A float arg may have to do float reg int reg conversion\n+void MacroAssembler::float_move(VMRegPair src, VMRegPair dst, Register tmp) {\n+ if (src.first()->is_stack()) {\n+    if (dst.first()->is_stack()) {\n+      ldrw(tmp, Address(rfp, reg2offset_in(src.first())));\n+      strw(tmp, Address(sp, reg2offset_out(dst.first())));\n+    } else {\n+      ldrs(dst.first()->as_FloatRegister(), Address(rfp, reg2offset_in(src.first())));\n+    }\n+  } else if (src.first() != dst.first()) {\n+    if (src.is_single_phys_reg() && dst.is_single_phys_reg())\n+      fmovs(dst.first()->as_FloatRegister(), src.first()->as_FloatRegister());\n+    else\n+      strs(src.first()->as_FloatRegister(), Address(sp, reg2offset_out(dst.first())));\n+  }\n+}\n+\n+\/\/ A long move\n+void MacroAssembler::long_move(VMRegPair src, VMRegPair dst, Register tmp) {\n+  if (src.first()->is_stack()) {\n+    if (dst.first()->is_stack()) {\n+      \/\/ stack to stack\n+      ldr(tmp, Address(rfp, reg2offset_in(src.first())));\n+      str(tmp, Address(sp, reg2offset_out(dst.first())));\n+    } else {\n+      \/\/ stack to reg\n+      ldr(dst.first()->as_Register(), Address(rfp, reg2offset_in(src.first())));\n+    }\n+  } else if (dst.first()->is_stack()) {\n+    \/\/ reg to stack\n+    \/\/ Do we really have to sign extend???\n+    \/\/ __ movslq(src.first()->as_Register(), src.first()->as_Register());\n+    str(src.first()->as_Register(), Address(sp, reg2offset_out(dst.first())));\n+  } else {\n+    if (dst.first() != src.first()) {\n+      mov(dst.first()->as_Register(), src.first()->as_Register());\n+    }\n+  }\n+}\n+\n+\n+\/\/ A double move\n+void MacroAssembler::double_move(VMRegPair src, VMRegPair dst, Register tmp) {\n+ if (src.first()->is_stack()) {\n+    if (dst.first()->is_stack()) {\n+      ldr(tmp, Address(rfp, reg2offset_in(src.first())));\n+      str(tmp, Address(sp, reg2offset_out(dst.first())));\n+    } else {\n+      ldrd(dst.first()->as_FloatRegister(), Address(rfp, reg2offset_in(src.first())));\n+    }\n+  } else if (src.first() != dst.first()) {\n+    if (src.is_single_phys_reg() && dst.is_single_phys_reg())\n+      fmovd(dst.first()->as_FloatRegister(), src.first()->as_FloatRegister());\n+    else\n+      strd(src.first()->as_FloatRegister(), Address(sp, reg2offset_out(dst.first())));\n+  }\n+}\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":198,"deletions":11,"binary":false,"changes":209,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"code\/vmreg.hpp\"\n@@ -35,0 +36,2 @@\n+class OopMap;\n+\n@@ -107,1 +110,2 @@\n-  void safepoint_poll(Label& slow_path, bool at_return, bool acquire, bool in_nmethod);\n+  void safepoint_poll(Label& slow_path, bool at_return, bool acquire, bool in_nmethod, Register tmp = rscratch1);\n+  void rt_call(address dest, Register tmp = rscratch1);\n@@ -709,0 +713,14 @@\n+  \/\/ support for argument shuffling\n+  void move32_64(VMRegPair src, VMRegPair dst, Register tmp = rscratch1);\n+  void float_move(VMRegPair src, VMRegPair dst, Register tmp = rscratch1);\n+  void long_move(VMRegPair src, VMRegPair dst, Register tmp = rscratch1);\n+  void double_move(VMRegPair src, VMRegPair dst, Register tmp = rscratch1);\n+  void object_move(\n+                   OopMap* map,\n+                   int oop_handle_offset,\n+                   int framesize_in_slots,\n+                   VMRegPair src,\n+                   VMRegPair dst,\n+                   bool is_receiver,\n+                   int* receiver_offset);\n+\n@@ -967,1 +985,1 @@\n-  void verify_sve_vector_length();\n+  void verify_sve_vector_length(Register tmp = rscratch1);\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.hpp","additions":20,"deletions":2,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -261,0 +261,15 @@\n+void MethodHandles::jump_to_native_invoker(MacroAssembler* _masm, Register nep_reg, Register temp_target) {\n+  BLOCK_COMMENT(\"jump_to_native_invoker {\");\n+  assert_different_registers(nep_reg, temp_target);\n+  assert(nep_reg != noreg, \"required register\");\n+\n+  \/\/ Load the invoker, as NEP -> .invoker\n+  __ verify_oop(nep_reg);\n+  __ access_load_at(T_ADDRESS, IN_HEAP, temp_target,\n+                    Address(nep_reg, NONZERO(jdk_internal_foreign_abi_NativeEntryPoint::invoker_offset_in_bytes())),\n+                    noreg, noreg);\n+\n+  __ br(temp_target);\n+  BLOCK_COMMENT(\"} jump_to_native_invoker\");\n+}\n+\n@@ -273,1 +288,1 @@\n-    assert(receiver_reg == (iid == vmIntrinsics::_linkToStatic ? noreg : j_rarg0), \"only valid assignment\");\n+    assert(receiver_reg == (iid == vmIntrinsics::_linkToStatic || iid == vmIntrinsics::_linkToNative ? noreg : j_rarg0), \"only valid assignment\");\n@@ -282,4 +297,1 @@\n-  if (iid == vmIntrinsics::_invokeBasic || iid == vmIntrinsics::_linkToNative) {\n-    if (iid == vmIntrinsics::_linkToNative) {\n-      assert(for_compiler_entry, \"only compiler entry is supported\");\n-    }\n+  if (iid == vmIntrinsics::_invokeBasic) {\n@@ -289,0 +301,3 @@\n+  } else if (iid == vmIntrinsics::_linkToNative) {\n+    assert(for_compiler_entry, \"only compiler entry is supported\");\n+    jump_to_native_invoker(_masm, member_reg, temp1);\n","filename":"src\/hotspot\/cpu\/aarch64\/methodHandles_aarch64.cpp","additions":20,"deletions":5,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -59,0 +59,3 @@\n+  static void jump_to_native_invoker(MacroAssembler* _masm,\n+                                     Register nep_reg, Register temp);\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/methodHandles_aarch64.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -285,14 +285,0 @@\n-\/\/ The java_calling_convention describes stack locations as ideal slots on\n-\/\/ a frame with no abi restrictions. Since we must observe abi restrictions\n-\/\/ (like the placement of the register window) the slots must be biased by\n-\/\/ the following value.\n-static int reg2offset_in(VMReg r) {\n-  \/\/ Account for saved rfp and lr\n-  \/\/ This should really be in_preserve_stack_slots\n-  return (r->reg2stack() + 4) * VMRegImpl::stack_slot_size;\n-}\n-\n-static int reg2offset_out(VMReg r) {\n-  return (r->reg2stack() + SharedRuntime::out_preserve_stack_slots()) * VMRegImpl::stack_slot_size;\n-}\n-\n@@ -929,166 +915,0 @@\n-\/\/ On 64 bit we will store integer like items to the stack as\n-\/\/ 64 bits items (Aarch64 abi) even though java would only store\n-\/\/ 32bits for a parameter. On 32bit it will simply be 32 bits\n-\/\/ So this routine will do 32->32 on 32bit and 32->64 on 64bit\n-static void move32_64(MacroAssembler* masm, VMRegPair src, VMRegPair dst) {\n-  if (src.first()->is_stack()) {\n-    if (dst.first()->is_stack()) {\n-      \/\/ stack to stack\n-      __ ldr(rscratch1, Address(rfp, reg2offset_in(src.first())));\n-      __ str(rscratch1, Address(sp, reg2offset_out(dst.first())));\n-    } else {\n-      \/\/ stack to reg\n-      __ ldrsw(dst.first()->as_Register(), Address(rfp, reg2offset_in(src.first())));\n-    }\n-  } else if (dst.first()->is_stack()) {\n-    \/\/ reg to stack\n-    \/\/ Do we really have to sign extend???\n-    \/\/ __ movslq(src.first()->as_Register(), src.first()->as_Register());\n-    __ str(src.first()->as_Register(), Address(sp, reg2offset_out(dst.first())));\n-  } else {\n-    if (dst.first() != src.first()) {\n-      __ sxtw(dst.first()->as_Register(), src.first()->as_Register());\n-    }\n-  }\n-}\n-\n-\/\/ An oop arg. Must pass a handle not the oop itself\n-static void object_move(MacroAssembler* masm,\n-                        OopMap* map,\n-                        int oop_handle_offset,\n-                        int framesize_in_slots,\n-                        VMRegPair src,\n-                        VMRegPair dst,\n-                        bool is_receiver,\n-                        int* receiver_offset) {\n-\n-  \/\/ must pass a handle. First figure out the location we use as a handle\n-\n-  Register rHandle = dst.first()->is_stack() ? rscratch2 : dst.first()->as_Register();\n-\n-  \/\/ See if oop is NULL if it is we need no handle\n-\n-  if (src.first()->is_stack()) {\n-\n-    \/\/ Oop is already on the stack as an argument\n-    int offset_in_older_frame = src.first()->reg2stack() + SharedRuntime::out_preserve_stack_slots();\n-    map->set_oop(VMRegImpl::stack2reg(offset_in_older_frame + framesize_in_slots));\n-    if (is_receiver) {\n-      *receiver_offset = (offset_in_older_frame + framesize_in_slots) * VMRegImpl::stack_slot_size;\n-    }\n-\n-    __ ldr(rscratch1, Address(rfp, reg2offset_in(src.first())));\n-    __ lea(rHandle, Address(rfp, reg2offset_in(src.first())));\n-    \/\/ conditionally move a NULL\n-    __ cmp(rscratch1, zr);\n-    __ csel(rHandle, zr, rHandle, Assembler::EQ);\n-  } else {\n-\n-    \/\/ Oop is in an a register we must store it to the space we reserve\n-    \/\/ on the stack for oop_handles and pass a handle if oop is non-NULL\n-\n-    const Register rOop = src.first()->as_Register();\n-    int oop_slot;\n-    if (rOop == j_rarg0)\n-      oop_slot = 0;\n-    else if (rOop == j_rarg1)\n-      oop_slot = 1;\n-    else if (rOop == j_rarg2)\n-      oop_slot = 2;\n-    else if (rOop == j_rarg3)\n-      oop_slot = 3;\n-    else if (rOop == j_rarg4)\n-      oop_slot = 4;\n-    else if (rOop == j_rarg5)\n-      oop_slot = 5;\n-    else if (rOop == j_rarg6)\n-      oop_slot = 6;\n-    else {\n-      assert(rOop == j_rarg7, \"wrong register\");\n-      oop_slot = 7;\n-    }\n-\n-    oop_slot = oop_slot * VMRegImpl::slots_per_word + oop_handle_offset;\n-    int offset = oop_slot*VMRegImpl::stack_slot_size;\n-\n-    map->set_oop(VMRegImpl::stack2reg(oop_slot));\n-    \/\/ Store oop in handle area, may be NULL\n-    __ str(rOop, Address(sp, offset));\n-    if (is_receiver) {\n-      *receiver_offset = offset;\n-    }\n-\n-    __ cmp(rOop, zr);\n-    __ lea(rHandle, Address(sp, offset));\n-    \/\/ conditionally move a NULL\n-    __ csel(rHandle, zr, rHandle, Assembler::EQ);\n-  }\n-\n-  \/\/ If arg is on the stack then place it otherwise it is already in correct reg.\n-  if (dst.first()->is_stack()) {\n-    __ str(rHandle, Address(sp, reg2offset_out(dst.first())));\n-  }\n-}\n-\n-\/\/ A float arg may have to do float reg int reg conversion\n-static void float_move(MacroAssembler* masm, VMRegPair src, VMRegPair dst) {\n-  assert(src.first()->is_stack() && dst.first()->is_stack() ||\n-         src.first()->is_reg() && dst.first()->is_reg(), \"Unexpected error\");\n-  if (src.first()->is_stack()) {\n-    if (dst.first()->is_stack()) {\n-      __ ldrw(rscratch1, Address(rfp, reg2offset_in(src.first())));\n-      __ strw(rscratch1, Address(sp, reg2offset_out(dst.first())));\n-    } else {\n-      ShouldNotReachHere();\n-    }\n-  } else if (src.first() != dst.first()) {\n-    if (src.is_single_phys_reg() && dst.is_single_phys_reg())\n-      __ fmovs(dst.first()->as_FloatRegister(), src.first()->as_FloatRegister());\n-    else\n-      ShouldNotReachHere();\n-  }\n-}\n-\n-\/\/ A long move\n-static void long_move(MacroAssembler* masm, VMRegPair src, VMRegPair dst) {\n-  if (src.first()->is_stack()) {\n-    if (dst.first()->is_stack()) {\n-      \/\/ stack to stack\n-      __ ldr(rscratch1, Address(rfp, reg2offset_in(src.first())));\n-      __ str(rscratch1, Address(sp, reg2offset_out(dst.first())));\n-    } else {\n-      \/\/ stack to reg\n-      __ ldr(dst.first()->as_Register(), Address(rfp, reg2offset_in(src.first())));\n-    }\n-  } else if (dst.first()->is_stack()) {\n-    \/\/ reg to stack\n-    \/\/ Do we really have to sign extend???\n-    \/\/ __ movslq(src.first()->as_Register(), src.first()->as_Register());\n-    __ str(src.first()->as_Register(), Address(sp, reg2offset_out(dst.first())));\n-  } else {\n-    if (dst.first() != src.first()) {\n-      __ mov(dst.first()->as_Register(), src.first()->as_Register());\n-    }\n-  }\n-}\n-\n-\n-\/\/ A double move\n-static void double_move(MacroAssembler* masm, VMRegPair src, VMRegPair dst) {\n-  assert(src.first()->is_stack() && dst.first()->is_stack() ||\n-         src.first()->is_reg() && dst.first()->is_reg(), \"Unexpected error\");\n-  if (src.first()->is_stack()) {\n-    if (dst.first()->is_stack()) {\n-      __ ldr(rscratch1, Address(rfp, reg2offset_in(src.first())));\n-      __ str(rscratch1, Address(sp, reg2offset_out(dst.first())));\n-    } else {\n-      ShouldNotReachHere();\n-    }\n-  } else if (src.first() != dst.first()) {\n-    if (src.is_single_phys_reg() && dst.is_single_phys_reg())\n-      __ fmovd(dst.first()->as_FloatRegister(), src.first()->as_FloatRegister());\n-    else\n-      ShouldNotReachHere();\n-  }\n-}\n-\n@@ -1160,10 +980,0 @@\n-static void rt_call(MacroAssembler* masm, address dest) {\n-  CodeBlob *cb = CodeCache::find_blob(dest);\n-  if (cb) {\n-    __ far_call(RuntimeAddress(dest));\n-  } else {\n-    __ lea(rscratch1, RuntimeAddress(dest));\n-    __ blr(rscratch1);\n-  }\n-}\n-\n@@ -1294,1 +1104,1 @@\n-  } else if (iid == vmIntrinsics::_invokeBasic || iid == vmIntrinsics::_linkToNative) {\n+  } else if (iid == vmIntrinsics::_invokeBasic) {\n@@ -1296,0 +1106,3 @@\n+  } else if (iid == vmIntrinsics::_linkToNative) {\n+    member_arg_pos = method->size_of_parameters() - 1;  \/\/ trailing NativeEntryPoint argument\n+    member_reg = r19;  \/\/ known to be free at this point\n@@ -1659,3 +1472,3 @@\n-        object_move(masm, map, oop_handle_offset, stack_slots, in_regs[i], out_regs[c_arg],\n-                    ((i == 0) && (!is_static)),\n-                    &receiver_offset);\n+        __ object_move(map, oop_handle_offset, stack_slots, in_regs[i], out_regs[c_arg],\n+                       ((i == 0) && (!is_static)),\n+                       &receiver_offset);\n@@ -1668,1 +1481,1 @@\n-        float_move(masm, in_regs[i], out_regs[c_arg]);\n+        __ float_move(in_regs[i], out_regs[c_arg]);\n@@ -1676,1 +1489,1 @@\n-        double_move(masm, in_regs[i], out_regs[c_arg]);\n+        __ double_move(in_regs[i], out_regs[c_arg]);\n@@ -1681,1 +1494,1 @@\n-        long_move(masm, in_regs[i], out_regs[c_arg]);\n+        __ long_move(in_regs[i], out_regs[c_arg]);\n@@ -1688,1 +1501,1 @@\n-        move32_64(masm, in_regs[i], out_regs[c_arg]);\n+        __ move32_64(in_regs[i], out_regs[c_arg]);\n@@ -1819,1 +1632,1 @@\n-  rt_call(masm, native_func);\n+  __ rt_call(native_func);\n@@ -2033,1 +1846,1 @@\n-    rt_call(masm, CAST_FROM_FN_PTR(address, SharedRuntime::complete_monitor_unlocking_C));\n+    __ rt_call(CAST_FROM_FN_PTR(address, SharedRuntime::complete_monitor_unlocking_C));\n@@ -2060,1 +1873,1 @@\n-  rt_call(masm, CAST_FROM_FN_PTR(address, SharedRuntime::reguard_yellow_pages));\n+  __ rt_call(CAST_FROM_FN_PTR(address, SharedRuntime::reguard_yellow_pages));\n@@ -3081,251 +2894,0 @@\n-\/\/ ---------------------------------------------------------------\n-\n-class NativeInvokerGenerator : public StubCodeGenerator {\n-  address _call_target;\n-  int _shadow_space_bytes;\n-\n-  const GrowableArray<VMReg>& _input_registers;\n-  const GrowableArray<VMReg>& _output_registers;\n-\n-  int _frame_complete;\n-  int _framesize;\n-  OopMapSet* _oop_maps;\n-public:\n-  NativeInvokerGenerator(CodeBuffer* buffer,\n-                         address call_target,\n-                         int shadow_space_bytes,\n-                         const GrowableArray<VMReg>& input_registers,\n-                         const GrowableArray<VMReg>& output_registers)\n-   : StubCodeGenerator(buffer, PrintMethodHandleStubs),\n-     _call_target(call_target),\n-     _shadow_space_bytes(shadow_space_bytes),\n-     _input_registers(input_registers),\n-     _output_registers(output_registers),\n-     _frame_complete(0),\n-     _framesize(0),\n-     _oop_maps(NULL) {\n-    assert(_output_registers.length() <= 1\n-           || (_output_registers.length() == 2 && !_output_registers.at(1)->is_valid()), \"no multi-reg returns\");\n-  }\n-\n-  void generate();\n-\n-  int spill_size_in_bytes() const {\n-    if (_output_registers.length() == 0) {\n-      return 0;\n-    }\n-    VMReg reg = _output_registers.at(0);\n-    assert(reg->is_reg(), \"must be a register\");\n-    if (reg->is_Register()) {\n-      return 8;\n-    } else if (reg->is_FloatRegister()) {\n-      bool use_sve = Matcher::supports_scalable_vector();\n-      if (use_sve) {\n-        return Matcher::scalable_vector_reg_size(T_BYTE);\n-      }\n-      return 16;\n-    } else {\n-      ShouldNotReachHere();\n-    }\n-    return 0;\n-  }\n-\n-  void spill_output_registers() {\n-    if (_output_registers.length() == 0) {\n-      return;\n-    }\n-    VMReg reg = _output_registers.at(0);\n-    assert(reg->is_reg(), \"must be a register\");\n-    MacroAssembler* masm = _masm;\n-    if (reg->is_Register()) {\n-      __ spill(reg->as_Register(), true, 0);\n-    } else if (reg->is_FloatRegister()) {\n-      bool use_sve = Matcher::supports_scalable_vector();\n-      if (use_sve) {\n-        __ spill_sve_vector(reg->as_FloatRegister(), 0, Matcher::scalable_vector_reg_size(T_BYTE));\n-      } else {\n-        __ spill(reg->as_FloatRegister(), __ Q, 0);\n-      }\n-    } else {\n-      ShouldNotReachHere();\n-    }\n-  }\n-\n-  void fill_output_registers() {\n-    if (_output_registers.length() == 0) {\n-      return;\n-    }\n-    VMReg reg = _output_registers.at(0);\n-    assert(reg->is_reg(), \"must be a register\");\n-    MacroAssembler* masm = _masm;\n-    if (reg->is_Register()) {\n-      __ unspill(reg->as_Register(), true, 0);\n-    } else if (reg->is_FloatRegister()) {\n-      bool use_sve = Matcher::supports_scalable_vector();\n-      if (use_sve) {\n-        __ unspill_sve_vector(reg->as_FloatRegister(), 0, Matcher::scalable_vector_reg_size(T_BYTE));\n-      } else {\n-        __ unspill(reg->as_FloatRegister(), __ Q, 0);\n-      }\n-    } else {\n-      ShouldNotReachHere();\n-    }\n-  }\n-\n-  int frame_complete() const {\n-    return _frame_complete;\n-  }\n-\n-  int framesize() const {\n-    return (_framesize >> (LogBytesPerWord - LogBytesPerInt));\n-  }\n-\n-  OopMapSet* oop_maps() const {\n-    return _oop_maps;\n-  }\n-\n-private:\n-#ifdef ASSERT\n-  bool target_uses_register(VMReg reg) {\n-    return _input_registers.contains(reg) || _output_registers.contains(reg);\n-  }\n-#endif\n-};\n-\n-static const int native_invoker_code_size = 1024;\n-\n-RuntimeStub* SharedRuntime::make_native_invoker(address call_target,\n-                                                int shadow_space_bytes,\n-                                                const GrowableArray<VMReg>& input_registers,\n-                                                const GrowableArray<VMReg>& output_registers) {\n-  int locs_size  = 64;\n-  CodeBuffer code(\"nep_invoker_blob\", native_invoker_code_size, locs_size);\n-  NativeInvokerGenerator g(&code, call_target, shadow_space_bytes, input_registers, output_registers);\n-  g.generate();\n-  code.log_section_sizes(\"nep_invoker_blob\");\n-\n-  RuntimeStub* stub =\n-    RuntimeStub::new_runtime_stub(\"nep_invoker_blob\",\n-                                  &code,\n-                                  g.frame_complete(),\n-                                  g.framesize(),\n-                                  g.oop_maps(), false);\n-  return stub;\n-}\n-\n-void NativeInvokerGenerator::generate() {\n-  assert(!(target_uses_register(rscratch1->as_VMReg())\n-           || target_uses_register(rscratch2->as_VMReg())\n-           || target_uses_register(rthread->as_VMReg())),\n-         \"Register conflict\");\n-\n-  enum layout {\n-    rbp_off,\n-    rbp_off2,\n-    return_off,\n-    return_off2,\n-    framesize \/\/ inclusive of return address\n-  };\n-\n-  assert(_shadow_space_bytes == 0, \"not expecting shadow space on AArch64\");\n-  _framesize = align_up(framesize + (spill_size_in_bytes() >> LogBytesPerInt), 4);\n-  assert(is_even(_framesize\/2), \"sp not 16-byte aligned\");\n-\n-  _oop_maps  = new OopMapSet();\n-  MacroAssembler* masm = _masm;\n-\n-  address start = __ pc();\n-\n-  __ enter();\n-\n-  \/\/ lr and fp are already in place\n-  __ sub(sp, rfp, ((unsigned)_framesize-4) << LogBytesPerInt); \/\/ prolog\n-\n-  _frame_complete = __ pc() - start;\n-\n-  address the_pc = __ pc();\n-  __ set_last_Java_frame(sp, rfp, the_pc, rscratch1);\n-  OopMap* map = new OopMap(_framesize, 0);\n-  _oop_maps->add_gc_map(the_pc - start, map);\n-\n-  \/\/ State transition\n-  __ mov(rscratch1, _thread_in_native);\n-  __ lea(rscratch2, Address(rthread, JavaThread::thread_state_offset()));\n-  __ stlrw(rscratch1, rscratch2);\n-\n-  rt_call(masm, _call_target);\n-\n-  __ mov(rscratch1, _thread_in_native_trans);\n-  __ strw(rscratch1, Address(rthread, JavaThread::thread_state_offset()));\n-\n-  \/\/ Force this write out before the read below\n-  __ membar(Assembler::LoadLoad | Assembler::LoadStore |\n-            Assembler::StoreLoad | Assembler::StoreStore);\n-\n-  __ verify_sve_vector_length();\n-\n-  Label L_after_safepoint_poll;\n-  Label L_safepoint_poll_slow_path;\n-\n-  __ safepoint_poll(L_safepoint_poll_slow_path, true \/* at_return *\/, true \/* acquire *\/, false \/* in_nmethod *\/);\n-\n-  __ ldrw(rscratch1, Address(rthread, JavaThread::suspend_flags_offset()));\n-  __ cbnzw(rscratch1, L_safepoint_poll_slow_path);\n-\n-  __ bind(L_after_safepoint_poll);\n-\n-  \/\/ change thread state\n-  __ mov(rscratch1, _thread_in_Java);\n-  __ lea(rscratch2, Address(rthread, JavaThread::thread_state_offset()));\n-  __ stlrw(rscratch1, rscratch2);\n-\n-  __ block_comment(\"reguard stack check\");\n-  Label L_reguard;\n-  Label L_after_reguard;\n-  __ ldrb(rscratch1, Address(rthread, JavaThread::stack_guard_state_offset()));\n-  __ cmpw(rscratch1, StackOverflow::stack_guard_yellow_reserved_disabled);\n-  __ br(Assembler::EQ, L_reguard);\n-  __ bind(L_after_reguard);\n-\n-  __ reset_last_Java_frame(true);\n-\n-  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-  __ ret(lr);\n-\n-  \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n-\n-  __ block_comment(\"{ L_safepoint_poll_slow_path\");\n-  __ bind(L_safepoint_poll_slow_path);\n-\n-  \/\/ Need to save the native result registers around any runtime calls.\n-  spill_output_registers();\n-\n-  __ mov(c_rarg0, rthread);\n-  assert(frame::arg_reg_save_area_bytes == 0, \"not expecting frame reg save area\");\n-  __ lea(rscratch1, RuntimeAddress(CAST_FROM_FN_PTR(address, JavaThread::check_special_condition_for_native_trans)));\n-  __ blr(rscratch1);\n-\n-  fill_output_registers();\n-\n-  __ b(L_after_safepoint_poll);\n-  __ block_comment(\"} L_safepoint_poll_slow_path\");\n-\n-  \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n-\n-  __ block_comment(\"{ L_reguard\");\n-  __ bind(L_reguard);\n-\n-  spill_output_registers();\n-\n-  rt_call(masm, CAST_FROM_FN_PTR(address, SharedRuntime::reguard_yellow_pages));\n-\n-  fill_output_registers();\n-\n-  __ b(L_after_reguard);\n-\n-  __ block_comment(\"} L_reguard\");\n-\n-  \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n-\n-  __ flush();\n-}\n@@ -3333,0 +2895,1 @@\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/sharedRuntime_aarch64.cpp","additions":15,"deletions":452,"binary":false,"changes":467,"status":"modified"},{"patch":"@@ -28,0 +28,4 @@\n+#include \"code\/codeCache.hpp\"\n+#include \"code\/vmreg.inline.hpp\"\n+#include \"compiler\/oopMap.hpp\"\n+#include \"logging\/logStream.hpp\"\n@@ -30,0 +34,1 @@\n+#include \"runtime\/stubCodeGenerator.hpp\"\n@@ -33,2 +38,5 @@\n-void ProgrammableInvoker::Generator::generate() {\n-  __ enter();\n+class NativeInvokerGenerator : public StubCodeGenerator {\n+  BasicType* _signature;\n+  int _num_args;\n+  BasicType _ret_bt;\n+  const ABIDescriptor& _abi;\n@@ -36,7 +44,2 @@\n-  \/\/ Name registers used in the stub code. These are all caller-save so\n-  \/\/ may be clobbered by the call to the native function. Avoid using\n-  \/\/ rscratch1 here as it's r8 which is the indirect result register in\n-  \/\/ the standard ABI.\n-  Register Rctx = r10, Rstack_size = r11;\n-  Register Rwords = r12, Rtmp = r13;\n-  Register Rsrc_ptr = r14, Rdst_ptr = r15;\n+  const GrowableArray<VMReg>& _input_registers;\n+  const GrowableArray<VMReg>& _output_registers;\n@@ -44,1 +47,1 @@\n-  assert_different_registers(Rctx, Rstack_size, rscratch1, rscratch2);\n+  bool _needs_return_buffer;\n@@ -46,2 +49,24 @@\n-  \/\/ TODO: if the callee is not using the standard C ABI then we need to\n-  \/\/       preserve more registers here.\n+  int _frame_complete;\n+  int _framesize;\n+  OopMapSet* _oop_maps;\n+public:\n+  NativeInvokerGenerator(CodeBuffer* buffer,\n+                         BasicType* signature,\n+                         int num_args,\n+                         BasicType ret_bt,\n+                         const ABIDescriptor& abi,\n+                         const GrowableArray<VMReg>& input_registers,\n+                         const GrowableArray<VMReg>& output_registers,\n+                         bool needs_return_buffer)\n+   : StubCodeGenerator(buffer, PrintMethodHandleStubs),\n+     _signature(signature),\n+     _num_args(num_args),\n+     _ret_bt(ret_bt),\n+     _abi(abi),\n+     _input_registers(input_registers),\n+     _output_registers(output_registers),\n+     _needs_return_buffer(needs_return_buffer),\n+     _frame_complete(0),\n+     _framesize(0),\n+     _oop_maps(NULL) {\n+  }\n@@ -49,1 +74,1 @@\n-  __ block_comment(\"init_and_alloc_stack\");\n+  void generate();\n@@ -51,2 +76,3 @@\n-  __ mov(Rctx, c_rarg0);\n-  __ str(Rctx, Address(__ pre(sp, -2 * wordSize)));\n+  int frame_complete() const {\n+    return _frame_complete;\n+  }\n@@ -54,1 +80,3 @@\n-  assert(_abi->_stack_alignment_bytes % 16 == 0, \"stack must be 16 byte aligned\");\n+  int framesize() const {\n+    return (_framesize >> (LogBytesPerWord - LogBytesPerInt));\n+  }\n@@ -56,5 +84,4 @@\n-  __ block_comment(\"allocate_stack\");\n-  __ ldr(Rstack_size, Address(Rctx, (int) _layout->stack_args_bytes));\n-  __ add(rscratch2, Rstack_size, _abi->_stack_alignment_bytes - 1);\n-  __ andr(rscratch2, rscratch2, -_abi->_stack_alignment_bytes);\n-  __ sub(sp, sp, rscratch2);\n+  OopMapSet* oop_maps() const {\n+    return _oop_maps;\n+  }\n+};\n@@ -62,1 +89,1 @@\n-  __ block_comment(\"load_arguments\");\n+static const int native_invoker_code_size = 1024;\n@@ -64,3 +91,12 @@\n-  __ ldr(Rsrc_ptr, Address(Rctx, (int) _layout->stack_args));\n-  __ lsr(Rwords, Rstack_size, LogBytesPerWord);\n-  __ mov(Rdst_ptr, sp);\n+RuntimeStub* ProgrammableInvoker::make_native_invoker(BasicType* signature,\n+                                                      int num_args,\n+                                                      BasicType ret_bt,\n+                                                      const ABIDescriptor& abi,\n+                                                      const GrowableArray<VMReg>& input_registers,\n+                                                      const GrowableArray<VMReg>& output_registers,\n+                                                      bool needs_return_buffer) {\n+  int locs_size  = 64;\n+  CodeBuffer code(\"nep_invoker_blob\", native_invoker_code_size, locs_size);\n+  NativeInvokerGenerator g(&code, signature, num_args, ret_bt, abi, input_registers, output_registers, needs_return_buffer);\n+  g.generate();\n+  code.log_section_sizes(\"nep_invoker_blob\");\n@@ -68,8 +104,6 @@\n-  Label Ldone, Lnext;\n-  __ bind(Lnext);\n-  __ cbz(Rwords, Ldone);\n-  __ ldr(Rtmp, __ post(Rsrc_ptr, wordSize));\n-  __ str(Rtmp, __ post(Rdst_ptr, wordSize));\n-  __ sub(Rwords, Rwords, 1);\n-  __ b(Lnext);\n-  __ bind(Ldone);\n+  RuntimeStub* stub =\n+    RuntimeStub::new_runtime_stub(\"nep_invoker_blob\",\n+                                  &code,\n+                                  g.frame_complete(),\n+                                  g.framesize(),\n+                                  g.oop_maps(), false);\n@@ -77,3 +111,6 @@\n-  for (int i = 0; i < _abi->_vector_argument_registers.length(); i++) {\n-    ssize_t offs = _layout->arguments_vector + i * float_reg_size;\n-    __ ldrq(_abi->_vector_argument_registers.at(i), Address(Rctx, offs));\n+#ifdef ASSERT\n+  LogTarget(Trace, panama) lt;\n+  if (lt.is_enabled()) {\n+    ResourceMark rm;\n+    LogStream ls(lt);\n+    stub->print_on(&ls);\n@@ -81,0 +118,4 @@\n+#endif\n+\n+  return stub;\n+}\n@@ -82,3 +123,27 @@\n-  for (int i = 0; i < _abi->_integer_argument_registers.length(); i++) {\n-    ssize_t offs = _layout->arguments_integer + i * sizeof(uintptr_t);\n-    __ ldr(_abi->_integer_argument_registers.at(i), Address(Rctx, offs));\n+void NativeInvokerGenerator::generate() {\n+  enum layout {\n+    rfp_off,\n+    rfp_off2,\n+    lr_off,\n+    lr_off2,\n+    framesize \/\/ inclusive of return address\n+    \/\/ The following are also computed dynamically:\n+    \/\/ spill area for return value\n+    \/\/ out arg area (e.g. for stack args)\n+  };\n+\n+  \/\/ we can't use rscratch1 because it is r8, and used by the ABI\n+  Register tmp1 = r9;\n+  Register tmp2 = r10;\n+\n+  Register shuffle_reg = r19;\n+  JavaCallConv in_conv;\n+  NativeCallConv out_conv(_input_registers);\n+  ArgumentShuffle arg_shuffle(_signature, _num_args, _signature, _num_args, &in_conv, &out_conv, shuffle_reg->as_VMReg());\n+\n+#ifdef ASSERT\n+  LogTarget(Trace, panama) lt;\n+  if (lt.is_enabled()) {\n+    ResourceMark rm;\n+    LogStream ls(lt);\n+    arg_shuffle.print_on(&ls);\n@@ -86,0 +151,1 @@\n+#endif\n@@ -87,1 +153,6 @@\n-  assert(_abi->_shadow_space_bytes == 0, \"shadow space not supported on AArch64\");\n+  int allocated_frame_size = 0;\n+  if (_needs_return_buffer) {\n+    allocated_frame_size += 8; \/\/ for address spill\n+  }\n+  allocated_frame_size += arg_shuffle.out_arg_stack_slots() <<LogBytesPerInt;\n+  assert(_abi._shadow_space_bytes == 0, \"not expecting shadow space on AArch64\");\n@@ -89,4 +160,5 @@\n-  \/\/ call target function\n-  __ block_comment(\"call target function\");\n-  __ ldr(rscratch2, Address(Rctx, (int) _layout->arguments_next_pc));\n-  __ blr(rscratch2);\n+  int ret_buf_addr_sp_offset = -1;\n+  if (_needs_return_buffer) {\n+     \/\/ in sync with the above\n+     ret_buf_addr_sp_offset = allocated_frame_size - 8;\n+  }\n@@ -94,1 +166,2 @@\n-  __ ldr(Rctx, Address(rfp, -2 * wordSize));   \/\/ Might have clobbered Rctx\n+  RegSpiller out_reg_spiller(_output_registers);\n+  int spill_offset = -1;\n@@ -96,1 +169,7 @@\n-  __ block_comment(\"store_registers\");\n+  if (!_needs_return_buffer) {\n+    spill_offset = 0;\n+    \/\/ spill area can be shared with the above, so we take the max of the 2\n+    allocated_frame_size = out_reg_spiller.spill_size_bytes() > allocated_frame_size\n+      ? out_reg_spiller.spill_size_bytes()\n+      : allocated_frame_size;\n+  }\n@@ -98,3 +177,29 @@\n-  for (int i = 0; i < _abi->_integer_return_registers.length(); i++) {\n-    ssize_t offs = _layout->returns_integer + i * sizeof(uintptr_t);\n-    __ str(_abi->_integer_return_registers.at(i), Address(Rctx, offs));\n+  _framesize = align_up(framesize\n+    + (allocated_frame_size >> LogBytesPerInt), 4);\n+  assert(is_even(_framesize\/2), \"sp not 16-byte aligned\");\n+\n+  _oop_maps  = new OopMapSet();\n+  address start = __ pc();\n+\n+  __ enter();\n+\n+  \/\/ lr and fp are already in place\n+  __ sub(sp, rfp, ((unsigned)_framesize-4) << LogBytesPerInt); \/\/ prolog\n+\n+  _frame_complete = __ pc() - start;\n+\n+  address the_pc = __ pc();\n+  __ set_last_Java_frame(sp, rfp, the_pc, tmp1);\n+  OopMap* map = new OopMap(_framesize, 0);\n+  _oop_maps->add_gc_map(the_pc - start, map);\n+\n+  \/\/ State transition\n+  __ mov(tmp1, _thread_in_native);\n+  __ lea(tmp2, Address(rthread, JavaThread::thread_state_offset()));\n+  __ stlrw(tmp1, tmp2);\n+\n+  __ block_comment(\"{ argument shuffle\");\n+  arg_shuffle.generate(_masm, shuffle_reg->as_VMReg(), 0, _abi._shadow_space_bytes);\n+  if (_needs_return_buffer) {\n+    assert(ret_buf_addr_sp_offset != -1, \"no return buffer addr spill\");\n+    __ str(_abi._ret_buf_addr_reg, Address(sp, ret_buf_addr_sp_offset));\n@@ -102,0 +207,4 @@\n+  __ block_comment(\"} argument shuffle\");\n+\n+  __ blr(_abi._target_addr_reg);\n+  \/\/ this call is assumed not to have killed rthread\n@@ -103,3 +212,32 @@\n-  for (int i = 0; i < _abi->_vector_return_registers.length(); i++) {\n-    ssize_t offs = _layout->returns_vector + i * float_reg_size;\n-    __ strq(_abi->_vector_return_registers.at(i), Address(Rctx, offs));\n+  if (!_needs_return_buffer) {\n+    \/\/ Unpack native results.\n+    switch (_ret_bt) {\n+      case T_BOOLEAN: __ c2bool(r0);                     break;\n+      case T_CHAR   : __ ubfx(r0, r0, 0, 16);            break;\n+      case T_BYTE   : __ sbfx(r0, r0, 0, 8);             break;\n+      case T_SHORT  : __ sbfx(r0, r0, 0, 16);            break;\n+      case T_INT    : __ sbfx(r0, r0, 0, 32);            break;\n+      case T_DOUBLE :\n+      case T_FLOAT  :\n+        \/\/ Result is in v0 we'll save as needed\n+        break;\n+      case T_VOID: break;\n+      case T_LONG: break;\n+      default       : ShouldNotReachHere();\n+    }\n+  } else {\n+    assert(ret_buf_addr_sp_offset != -1, \"no return buffer addr spill\");\n+    __ ldr(tmp1, Address(sp, ret_buf_addr_sp_offset));\n+    int offset = 0;\n+    for (int i = 0; i < _output_registers.length(); i++) {\n+      VMReg reg = _output_registers.at(i);\n+      if (reg->is_Register()) {\n+        __ str(reg->as_Register(), Address(tmp1, offset));\n+        offset += 8;\n+      } else if(reg->is_FloatRegister()) {\n+        __ strd(reg->as_FloatRegister(), Address(tmp1, offset));\n+        offset += 16;\n+      } else {\n+        ShouldNotReachHere();\n+      }\n+    }\n@@ -108,1 +246,35 @@\n-  __ leave();\n+  __ mov(tmp1, _thread_in_native_trans);\n+  __ strw(tmp1, Address(rthread, JavaThread::thread_state_offset()));\n+\n+  \/\/ Force this write out before the read below\n+  __ membar(Assembler::LoadLoad | Assembler::LoadStore |\n+            Assembler::StoreLoad | Assembler::StoreStore);\n+\n+  __ verify_sve_vector_length(tmp1);\n+\n+  Label L_after_safepoint_poll;\n+  Label L_safepoint_poll_slow_path;\n+\n+  __ safepoint_poll(L_safepoint_poll_slow_path, true \/* at_return *\/, true \/* acquire *\/, false \/* in_nmethod *\/, tmp1);\n+\n+  __ ldrw(tmp1, Address(rthread, JavaThread::suspend_flags_offset()));\n+  __ cbnzw(tmp1, L_safepoint_poll_slow_path);\n+\n+  __ bind(L_after_safepoint_poll);\n+\n+  \/\/ change thread state\n+  __ mov(tmp1, _thread_in_Java);\n+  __ lea(tmp2, Address(rthread, JavaThread::thread_state_offset()));\n+  __ stlrw(tmp1, tmp2);\n+\n+  __ block_comment(\"reguard stack check\");\n+  Label L_reguard;\n+  Label L_after_reguard;\n+  __ ldrb(tmp1, Address(rthread, JavaThread::stack_guard_state_offset()));\n+  __ cmpw(tmp1, StackOverflow::stack_guard_yellow_reserved_disabled);\n+  __ br(Assembler::EQ, L_reguard);\n+  __ bind(L_after_reguard);\n+\n+  __ reset_last_Java_frame(true);\n+\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n@@ -111,2 +283,9 @@\n-  __ flush();\n-}\n+  \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+  __ block_comment(\"{ L_safepoint_poll_slow_path\");\n+  __ bind(L_safepoint_poll_slow_path);\n+\n+  if (!_needs_return_buffer) {\n+    \/\/ Need to save the native result registers around any runtime calls.\n+    out_reg_spiller.generate_spill(_masm, spill_offset);\n+  }\n@@ -114,4 +293,4 @@\n-address ProgrammableInvoker::generate_adapter(jobject jabi, jobject jlayout) {\n-  ResourceMark rm;\n-  const ABIDescriptor abi = ForeignGlobals::parse_abi_descriptor(jabi);\n-  const BufferLayout layout = ForeignGlobals::parse_buffer_layout(jlayout);\n+  __ mov(c_rarg0, rthread);\n+  assert(frame::arg_reg_save_area_bytes == 0, \"not expecting frame reg save area\");\n+  __ lea(tmp1, RuntimeAddress(CAST_FROM_FN_PTR(address, JavaThread::check_special_condition_for_native_trans)));\n+  __ blr(tmp1);\n@@ -119,1 +298,15 @@\n-  BufferBlob* _invoke_native_blob = BufferBlob::create(\"invoke_native_blob\", native_invoker_size);\n+  if (!_needs_return_buffer) {\n+    out_reg_spiller.generate_fill(_masm, spill_offset);\n+  }\n+\n+  __ b(L_after_safepoint_poll);\n+  __ block_comment(\"} L_safepoint_poll_slow_path\");\n+\n+  \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+  __ block_comment(\"{ L_reguard\");\n+  __ bind(L_reguard);\n+\n+  if (!_needs_return_buffer) {\n+    out_reg_spiller.generate_spill(_masm, spill_offset);\n+  }\n@@ -121,4 +314,1 @@\n-  CodeBuffer code2(_invoke_native_blob);\n-  ProgrammableInvoker::Generator g2(&code2, &abi, &layout);\n-  g2.generate();\n-  code2.log_section_sizes(\"InvokeNativeBlob\");\n+  __ rt_call(CAST_FROM_FN_PTR(address, SharedRuntime::reguard_yellow_pages), tmp1);\n@@ -126,1 +316,11 @@\n-  return _invoke_native_blob->code_begin();\n+  if (!_needs_return_buffer) {\n+    out_reg_spiller.generate_fill(_masm, spill_offset);\n+  }\n+\n+  __ b(L_after_reguard);\n+\n+  __ block_comment(\"} L_reguard\");\n+\n+  \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+  __ flush();\n","filename":"src\/hotspot\/cpu\/aarch64\/universalNativeInvoker_aarch64.cpp","additions":265,"deletions":65,"binary":false,"changes":330,"status":"modified"},{"patch":"@@ -2,2 +2,2 @@\n- * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2019, Arm Limited. All rights reserved.\n+ * Copyright (c) 2020, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2019, 2021, Arm Limited. All rights reserved.\n@@ -27,0 +27,1 @@\n+#include \"logging\/logStream.hpp\"\n@@ -29,0 +30,7 @@\n+#include \"runtime\/sharedRuntime.hpp\"\n+#include \"runtime\/signature.hpp\"\n+#include \"runtime\/signature.hpp\"\n+#include \"runtime\/stubRoutines.hpp\"\n+#include \"utilities\/formatBuffer.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include \"vmreg_aarch64.inline.hpp\"\n@@ -32,9 +40,10 @@\n-\/\/ 1. Create buffer according to layout\n-\/\/ 2. Load registers & stack args into buffer\n-\/\/ 3. Call upcall helper with upcall handler instance & buffer pointer (C++ ABI)\n-\/\/ 4. Load return value from buffer into foreign ABI registers\n-\/\/ 5. Return\n-address ProgrammableUpcallHandler::generate_upcall_stub(jobject rec, jobject jabi, jobject jlayout) {\n-  ResourceMark rm;\n-  const ABIDescriptor abi = ForeignGlobals::parse_abi_descriptor(jabi);\n-  const BufferLayout layout = ForeignGlobals::parse_buffer_layout(jlayout);\n+\/\/ for callee saved regs, according to the caller's ABI\n+static int compute_reg_save_area_size(const ABIDescriptor& abi) {\n+  int size = 0;\n+  for (int i = 0; i < RegisterImpl::number_of_registers; i++) {\n+    Register reg = as_Register(i);\n+    if (reg == rfp || reg == sp) continue; \/\/ saved\/restored by prologue\/epilogue\n+    if (!abi.is_volatile_reg(reg)) {\n+      size += 8; \/\/ bytes\n+    }\n+  }\n@@ -42,1 +51,7 @@\n-  CodeBuffer buffer(\"upcall_stub\", 1024, upcall_stub_size);\n+  for (int i = 0; i < FloatRegisterImpl::number_of_registers; i++) {\n+    FloatRegister reg = as_FloatRegister(i);\n+    if (!abi.is_volatile_reg(reg)) {\n+      \/\/ Only the lower 64 bits of vector registers need to be preserved.\n+      size += 8; \/\/ bytes\n+    }\n+  }\n@@ -44,1 +59,2 @@\n-  MacroAssembler* _masm = new MacroAssembler(&buffer);\n+  return size;\n+}\n@@ -46,2 +62,4 @@\n-  \/\/ stub code\n-  __ enter();\n+static void preserve_callee_saved_registers(MacroAssembler* _masm, const ABIDescriptor& abi, int reg_save_area_offset) {\n+  \/\/ 1. iterate all registers in the architecture\n+  \/\/     - check if they are volatile or not for the given abi\n+  \/\/     - if NOT, we need to save it here\n@@ -49,2 +67,1 @@\n-  \/\/ save pointer to JNI receiver handle into constant segment\n-  Address rec_adr = InternalAddress(__ address_constant((address)rec));\n+  int offset = reg_save_area_offset;\n@@ -52,1 +69,9 @@\n-  assert(abi._stack_alignment_bytes % 16 == 0, \"stack must be 16 byte aligned\");\n+  __ block_comment(\"{ preserve_callee_saved_regs \");\n+  for (int i = 0; i < RegisterImpl::number_of_registers; i++) {\n+    Register reg = as_Register(i);\n+    if (reg == rfp || reg == sp) continue; \/\/ saved\/restored by prologue\/epilogue\n+    if (!abi.is_volatile_reg(reg)) {\n+      __ str(reg, Address(sp, offset));\n+      offset += 8;\n+    }\n+  }\n@@ -54,1 +79,10 @@\n-  __ sub(sp, sp, (int) align_up(layout.buffer_size, abi._stack_alignment_bytes));\n+  for (int i = 0; i < FloatRegisterImpl::number_of_registers; i++) {\n+    FloatRegister reg = as_FloatRegister(i);\n+    if (!abi.is_volatile_reg(reg)) {\n+      __ strd(reg, Address(sp, offset));\n+      offset += 8;\n+    }\n+  }\n+\n+  __ block_comment(\"} preserve_callee_saved_regs \");\n+}\n@@ -56,3 +90,16 @@\n-  \/\/ TODO: This stub only uses registers which are caller-save in the\n-  \/\/       standard C ABI. If this is called from a different ABI then\n-  \/\/       we need to save registers here according to abi.is_volatile_reg.\n+static void restore_callee_saved_registers(MacroAssembler* _masm, const ABIDescriptor& abi, int reg_save_area_offset) {\n+  \/\/ 1. iterate all registers in the architecture\n+  \/\/     - check if they are volatile or not for the given abi\n+  \/\/     - if NOT, we need to restore it here\n+\n+  int offset = reg_save_area_offset;\n+\n+  __ block_comment(\"{ restore_callee_saved_regs \");\n+  for (int i = 0; i < RegisterImpl::number_of_registers; i++) {\n+    Register reg = as_Register(i);\n+    if (reg == rfp || reg == sp) continue; \/\/ saved\/restored by prologue\/epilogue\n+    if (!abi.is_volatile_reg(reg)) {\n+      __ ldr(reg, Address(sp, offset));\n+      offset += 8;\n+    }\n+  }\n@@ -60,4 +107,6 @@\n-  for (int i = 0; i < abi._integer_argument_registers.length(); i++) {\n-    Register reg = abi._integer_argument_registers.at(i);\n-    ssize_t offset = layout.arguments_integer + i * sizeof(uintptr_t);\n-    __ str(reg, Address(sp, offset));\n+  for (int i = 0; i < FloatRegisterImpl::number_of_registers; i++) {\n+    FloatRegister reg = as_FloatRegister(i);\n+    if (!abi.is_volatile_reg(reg)) {\n+      __ ldrd(reg, Address(sp, offset));\n+      offset += 8;\n+    }\n@@ -66,4 +115,51 @@\n-  for (int i = 0; i < abi._vector_argument_registers.length(); i++) {\n-    FloatRegister reg = abi._vector_argument_registers.at(i);\n-    ssize_t offset = layout.arguments_vector + i * float_reg_size;\n-    __ strq(reg, Address(sp, offset));\n+  __ block_comment(\"} restore_callee_saved_regs \");\n+}\n+\n+address ProgrammableUpcallHandler::generate_optimized_upcall_stub(jobject receiver, Method* entry,\n+                                                                  BasicType* in_sig_bt, int total_in_args,\n+                                                                  BasicType* out_sig_bt, int total_out_args,\n+                                                                  BasicType ret_type,\n+                                                                  jobject jabi, jobject jconv,\n+                                                                  bool needs_return_buffer, int ret_buf_size) {\n+  ResourceMark rm;\n+  const ABIDescriptor abi = ForeignGlobals::parse_abi_descriptor(jabi);\n+  const CallRegs call_regs = ForeignGlobals::parse_call_regs(jconv);\n+  CodeBuffer buffer(\"upcall_stub_linkToNative\", \/* code_size = *\/ 2048, \/* locs_size = *\/ 1024);\n+\n+  Register shuffle_reg = r19;\n+  JavaCallConv out_conv;\n+  NativeCallConv in_conv(call_regs._arg_regs, call_regs._args_length);\n+  ArgumentShuffle arg_shuffle(in_sig_bt, total_in_args, out_sig_bt, total_out_args, &in_conv, &out_conv, shuffle_reg->as_VMReg());\n+  int stack_slots = SharedRuntime::out_preserve_stack_slots() + arg_shuffle.out_arg_stack_slots();\n+  int out_arg_area = align_up(stack_slots * VMRegImpl::stack_slot_size, StackAlignmentInBytes);\n+\n+#ifdef ASSERT\n+  LogTarget(Trace, panama) lt;\n+  if (lt.is_enabled()) {\n+    ResourceMark rm;\n+    LogStream ls(lt);\n+    arg_shuffle.print_on(&ls);\n+  }\n+#endif\n+\n+  \/\/ out_arg_area (for stack arguments) doubles as shadow space for native calls.\n+  \/\/ make sure it is big enough.\n+  if (out_arg_area < frame::arg_reg_save_area_bytes) {\n+    out_arg_area = frame::arg_reg_save_area_bytes;\n+  }\n+\n+  int reg_save_area_size = compute_reg_save_area_size(abi);\n+  RegSpiller arg_spilller(call_regs._arg_regs, call_regs._args_length);\n+  RegSpiller result_spiller(call_regs._ret_regs, call_regs._rets_length);\n+\n+  int shuffle_area_offset    = 0;\n+  int res_save_area_offset   = shuffle_area_offset    + out_arg_area;\n+  int arg_save_area_offset   = res_save_area_offset   + result_spiller.spill_size_bytes();\n+  int reg_save_area_offset   = arg_save_area_offset   + arg_spilller.spill_size_bytes();\n+  int frame_data_offset      = reg_save_area_offset   + reg_save_area_size;\n+  int frame_bottom_offset    = frame_data_offset      + sizeof(OptimizedEntryBlob::FrameData);\n+\n+  int ret_buf_offset = -1;\n+  if (needs_return_buffer) {\n+    ret_buf_offset = frame_bottom_offset;\n+    frame_bottom_offset += ret_buf_size;\n@@ -72,4 +168,2 @@\n-  \/\/ Capture prev stack pointer (stack arguments base)\n-  __ add(rscratch1, rfp, 16);   \/\/ Skip saved FP and LR\n-  Address slot = __ legitimize_address(Address(sp, layout.stack_args), wordSize, rscratch2);\n-  __ str(rscratch1, slot);\n+  int frame_size = frame_bottom_offset;\n+  frame_size = align_up(frame_size, StackAlignmentInBytes);\n@@ -77,4 +171,42 @@\n-  \/\/ Call upcall helper\n-  __ ldr(c_rarg0, rec_adr);\n-  __ mov(c_rarg1, sp);\n-  __ movptr(rscratch1, CAST_FROM_FN_PTR(uint64_t, ProgrammableUpcallHandler::attach_thread_and_do_upcall));\n+  \/\/ The space we have allocated will look like:\n+  \/\/\n+  \/\/\n+  \/\/ FP-> |                     |\n+  \/\/      |---------------------| = frame_bottom_offset = frame_size\n+  \/\/      | (optional)          |\n+  \/\/      | ret_buf             |\n+  \/\/      |---------------------| = ret_buf_offset\n+  \/\/      |                     |\n+  \/\/      | FrameData           |\n+  \/\/      |---------------------| = frame_data_offset\n+  \/\/      |                     |\n+  \/\/      | reg_save_area       |\n+  \/\/      |---------------------| = reg_save_are_offset\n+  \/\/      |                     |\n+  \/\/      | arg_save_area       |\n+  \/\/      |---------------------| = arg_save_are_offset\n+  \/\/      |                     |\n+  \/\/      | res_save_area       |\n+  \/\/      |---------------------| = res_save_are_offset\n+  \/\/      |                     |\n+  \/\/ SP-> | out_arg_area        |   needs to be at end for shadow space\n+  \/\/\n+  \/\/\n+\n+  \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+  MacroAssembler* _masm = new MacroAssembler(&buffer);\n+  address start = __ pc();\n+  __ enter(); \/\/ set up frame\n+  assert((abi._stack_alignment_bytes % 16) == 0, \"must be 16 byte aligned\");\n+  \/\/ allocate frame (frame_size is also aligned, so stack is still aligned)\n+  __ sub(sp, sp, frame_size);\n+\n+  \/\/ we have to always spill args since we need to do a call to get the thread\n+  \/\/ (and maybe attach it).\n+  arg_spilller.generate_spill(_masm, arg_save_area_offset);\n+  preserve_callee_saved_registers(_masm, abi, reg_save_area_offset);\n+\n+  __ block_comment(\"{ on_entry\");\n+  __ lea(c_rarg0, Address(sp, frame_data_offset));\n+  __ movptr(rscratch1, CAST_FROM_FN_PTR(uint64_t, ProgrammableUpcallHandler::on_entry));\n@@ -82,0 +214,3 @@\n+  __ mov(rthread, r0);\n+  __ reinit_heapbase();\n+  __ block_comment(\"} on_entry\");\n@@ -83,3 +218,5 @@\n-  for (int i = 0; i < abi._integer_return_registers.length(); i++) {\n-    ssize_t offs = layout.returns_integer + i * sizeof(uintptr_t);\n-    __ ldr(abi._integer_return_registers.at(i), Address(sp, offs));\n+  __ block_comment(\"{ argument shuffle\");\n+  arg_spilller.generate_fill(_masm, arg_save_area_offset);\n+  if (needs_return_buffer) {\n+    assert(ret_buf_offset != -1, \"no return buffer allocated\");\n+    __ lea(abi._ret_buf_addr_reg, Address(sp, ret_buf_offset));\n@@ -87,0 +224,11 @@\n+  arg_shuffle.generate(_masm, shuffle_reg->as_VMReg(), abi._shadow_space_bytes, 0);\n+  __ block_comment(\"} argument shuffle\");\n+\n+  __ block_comment(\"{ receiver \");\n+  __ movptr(shuffle_reg, (intptr_t)receiver);\n+  __ resolve_jobject(shuffle_reg, rthread, rscratch2);\n+  __ mov(j_rarg0, shuffle_reg);\n+  __ block_comment(\"} receiver \");\n+\n+  __ mov_metadata(rmethod, entry);\n+  __ str(rmethod, Address(rthread, JavaThread::callee_target_offset())); \/\/ just in case callee is deoptimized\n@@ -88,4 +236,46 @@\n-  for (int i = 0; i < abi._vector_return_registers.length(); i++) {\n-    FloatRegister reg = abi._vector_return_registers.at(i);\n-    ssize_t offs = layout.returns_vector + i * float_reg_size;\n-    __ ldrq(reg, Address(sp, offs));\n+  __ ldr(rscratch1, Address(rmethod, Method::from_compiled_offset()));\n+  __ blr(rscratch1);\n+\n+    \/\/ return value shuffle\n+  if (!needs_return_buffer) {\n+#ifdef ASSERT\n+    if (call_regs._rets_length == 1) { \/\/ 0 or 1\n+      VMReg j_expected_result_reg;\n+      switch (ret_type) {\n+        case T_BOOLEAN:\n+        case T_BYTE:\n+        case T_SHORT:\n+        case T_CHAR:\n+        case T_INT:\n+        case T_LONG:\n+        j_expected_result_reg = r0->as_VMReg();\n+        break;\n+        case T_FLOAT:\n+        case T_DOUBLE:\n+          j_expected_result_reg = v0->as_VMReg();\n+          break;\n+        default:\n+          fatal(\"unexpected return type: %s\", type2name(ret_type));\n+      }\n+      \/\/ No need to move for now, since CallArranger can pick a return type\n+      \/\/ that goes in the same reg for both CCs. But, at least assert they are the same\n+      assert(call_regs._ret_regs[0] == j_expected_result_reg,\n+      \"unexpected result register: %s != %s\", call_regs._ret_regs[0]->name(), j_expected_result_reg->name());\n+    }\n+#endif\n+  } else {\n+    assert(ret_buf_offset != -1, \"no return buffer allocated\");\n+    __ lea(rscratch1, Address(sp, ret_buf_offset));\n+    int offset = 0;\n+    for (int i = 0; i < call_regs._rets_length; i++) {\n+      VMReg reg = call_regs._ret_regs[i];\n+      if (reg->is_Register()) {\n+        __ ldr(reg->as_Register(), Address(rscratch1, offset));\n+        offset += 8;\n+      } else if (reg->is_FloatRegister()) {\n+        __ ldrd(reg->as_FloatRegister(), Address(rscratch1, offset));\n+        offset += 16; \/\/ needs to match VECTOR_REG_SIZE in AArch64Architecture (Java)\n+      } else {\n+        ShouldNotReachHere();\n+      }\n+    }\n@@ -94,0 +284,13 @@\n+  result_spiller.generate_spill(_masm, res_save_area_offset);\n+\n+  __ block_comment(\"{ on_exit\");\n+  __ lea(c_rarg0, Address(sp, frame_data_offset));\n+  \/\/ stack already aligned\n+  __ movptr(rscratch1, CAST_FROM_FN_PTR(uint64_t, ProgrammableUpcallHandler::on_exit));\n+  __ blr(rscratch1);\n+  __ block_comment(\"} on_exit\");\n+\n+  restore_callee_saved_registers(_masm, abi, reg_save_area_offset);\n+\n+  result_spiller.generate_fill(_masm, res_save_area_offset);\n+\n@@ -97,1 +300,1 @@\n-  __ flush();\n+  \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n@@ -99,1 +302,1 @@\n-  BufferBlob* blob = BufferBlob::create(\"upcall_stub\", &buffer);\n+  __ block_comment(\"{ exception handler\");\n@@ -101,2 +304,1 @@\n-  return blob->code_begin();\n-}\n+  intptr_t exception_handler_offset = __ pc() - start;\n@@ -104,4 +306,8 @@\n-address ProgrammableUpcallHandler::generate_optimized_upcall_stub(jobject mh, Method* entry, jobject jabi, jobject jconv) {\n-  ShouldNotCallThis();\n-  return nullptr;\n-}\n+  \/\/ Native caller has no idea how to handle exceptions,\n+  \/\/ so we just crash here. Up to callee to catch exceptions.\n+  __ verify_oop(r0);\n+  __ movptr(rscratch1, CAST_FROM_FN_PTR(uint64_t, ProgrammableUpcallHandler::handle_uncaught_exception));\n+  __ blr(rscratch1);\n+  __ should_not_reach_here();\n+\n+  __ block_comment(\"} exception handler\");\n@@ -109,2 +315,22 @@\n-bool ProgrammableUpcallHandler::supports_optimized_upcalls() {\n-  return false;\n+  _masm->flush();\n+\n+#ifndef PRODUCT\n+  stringStream ss;\n+  ss.print(\"optimized_upcall_stub_%s\", entry->signature()->as_C_string());\n+  const char* name = _masm->code_string(ss.as_string());\n+#else \/\/ PRODUCT\n+  const char* name = \"optimized_upcall_stub\";\n+#endif \/\/ PRODUCT\n+\n+  OptimizedEntryBlob* blob\n+    = OptimizedEntryBlob::create(name,\n+                                 &buffer,\n+                                 exception_handler_offset,\n+                                 receiver,\n+                                 in_ByteSize(frame_data_offset));\n+\n+  if (TraceOptimizedUpcallStubs) {\n+    blob->print_on(tty);\n+  }\n+\n+  return blob->code_begin();\n","filename":"src\/hotspot\/cpu\/aarch64\/universalUpcallHandler_aarch64.cpp","additions":281,"deletions":55,"binary":false,"changes":336,"status":"modified"},{"patch":"@@ -54,13 +54,0 @@\n-\n-#define INTEGER_TYPE 0\n-#define VECTOR_TYPE 1\n-#define STACK_TYPE 3\n-\n-VMReg VMRegImpl::vmStorageToVMReg(int type, int index) {\n-  switch(type) {\n-    case INTEGER_TYPE: return ::as_Register(index)->as_VMReg();\n-    case VECTOR_TYPE: return ::as_FloatRegister(index)->as_VMReg();\n-    case STACK_TYPE: return VMRegImpl::stack2reg(index LP64_ONLY(* 2));\n-  }\n-  return VMRegImpl::Bad();\n-}\n","filename":"src\/hotspot\/cpu\/aarch64\/vmreg_aarch64.cpp","additions":0,"deletions":13,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -450,5 +450,0 @@\n-\n-int MachCallNativeNode::ret_addr_offset() {\n-  Unimplemented();\n-  return -1;\n-}\n","filename":"src\/hotspot\/cpu\/arm\/arm_32.ad","additions":0,"deletions":5,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -25,0 +25,1 @@\n+#include \"code\/vmreg.hpp\"\n@@ -28,1 +29,3 @@\n-const ABIDescriptor ForeignGlobals::parse_abi_descriptor_impl(jobject jabi) const {\n+class MacroAssembler;\n+\n+const ABIDescriptor ForeignGlobals::parse_abi_descriptor(jobject jabi) {\n@@ -33,1 +36,1 @@\n-const BufferLayout ForeignGlobals::parse_buffer_layout_impl(jobject jlayout) const {\n+VMReg ForeignGlobals::vmstorage_to_vmreg(int type, int index) {\n@@ -35,1 +38,1 @@\n-  return {};\n+  return VMRegImpl::Bad();\n@@ -38,1 +41,14 @@\n-const CallRegs ForeignGlobals::parse_call_regs_impl(jobject jconv) const {\n+int RegSpiller::pd_reg_size(VMReg reg) {\n+  Unimplemented();\n+  return -1;\n+}\n+\n+void RegSpiller::pd_store_reg(MacroAssembler* masm, int offset, VMReg reg) {\n+  Unimplemented();\n+}\n+\n+void RegSpiller::pd_load_reg(MacroAssembler* masm, int offset, VMReg reg) {\n+  Unimplemented();\n+}\n+\n+void ArgumentShuffle::pd_generate(MacroAssembler* masm, VMReg tmp, int in_stk_bias, int out_stk_bias) const {\n@@ -40,1 +56,0 @@\n-  return {};\n","filename":"src\/hotspot\/cpu\/arm\/foreign_globals_arm.cpp","additions":20,"deletions":5,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -27,1 +27,0 @@\n-class BufferLayout {};\n","filename":"src\/hotspot\/cpu\/arm\/foreign_globals_arm.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -276,0 +276,6 @@\n+void MethodHandles::jump_to_native_invoker(MacroAssembler* _masm, Register nep_reg, Register temp_target) {\n+  BLOCK_COMMENT(\"jump_to_native_invoker {\");\n+  __ stop(\"Should not reach here\");\n+  BLOCK_COMMENT(\"} jump_to_native_invoker\");\n+}\n+\n@@ -305,0 +311,3 @@\n+  } else if (iid == vmIntrinsics::_linkToNative) {\n+    assert(for_compiler_entry, \"only compiler entry is supported\");\n+    jump_to_native_invoker(_masm, member_reg, temp1);\n","filename":"src\/hotspot\/cpu\/arm\/methodHandles_arm.cpp","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -52,0 +52,3 @@\n+  static void jump_to_native_invoker(MacroAssembler* _masm,\n+                                     Register nep_reg, Register temp);\n+\n","filename":"src\/hotspot\/cpu\/arm\/methodHandles_arm.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -1870,10 +1870,0 @@\n-\n-#ifdef COMPILER2\n-RuntimeStub* SharedRuntime::make_native_invoker(address call_target,\n-                                                int shadow_space_bytes,\n-                                                const GrowableArray<VMReg>& input_registers,\n-                                                const GrowableArray<VMReg>& output_registers) {\n-  Unimplemented();\n-  return nullptr;\n-}\n-#endif\n","filename":"src\/hotspot\/cpu\/arm\/sharedRuntime_arm.cpp","additions":0,"deletions":10,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -28,1 +28,7 @@\n-address ProgrammableInvoker::generate_adapter(jobject jabi, jobject jlayout) {\n+RuntimeStub* ProgrammableInvoker::make_native_invoker(BasicType* signature,\n+                                                      int num_args,\n+                                                      BasicType ret_bt,\n+                                                      const ABIDescriptor& abi,\n+                                                      const GrowableArray<VMReg>& input_registers,\n+                                                      const GrowableArray<VMReg>& output_registers,\n+                                                      bool needs_return_buffer) {\n","filename":"src\/hotspot\/cpu\/arm\/universalNativeInvoker_arm.cpp","additions":7,"deletions":1,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -28,6 +28,6 @@\n-address ProgrammableUpcallHandler::generate_upcall_stub(jobject rec, jobject jabi, jobject jlayout) {\n-  Unimplemented();\n-  return nullptr;\n-}\n-\n-address ProgrammableUpcallHandler::generate_optimized_upcall_stub(jobject mh, Method* entry, jobject jabi, jobject jconv) {\n+address ProgrammableUpcallHandler::generate_optimized_upcall_stub(jobject receiver, Method* entry,\n+                                                                  BasicType* in_sig_bt, int total_in_args,\n+                                                                  BasicType* out_sig_bt, int total_out_args,\n+                                                                  BasicType ret_type,\n+                                                                  jobject jabi, jobject jconv,\n+                                                                  bool needs_return_buffer, int ret_buf_size) {\n@@ -37,4 +37,0 @@\n-\n-bool ProgrammableUpcallHandler::supports_optimized_upcalls() {\n-  return false;\n-}\n","filename":"src\/hotspot\/cpu\/arm\/universalUpcallHandle_arm.cpp","additions":6,"deletions":10,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -52,5 +52,0 @@\n-\n-VMReg VMRegImpl::vmStorageToVMReg(int type, int index) {\n-  Unimplemented();\n-  return VMRegImpl::Bad();\n-}\n","filename":"src\/hotspot\/cpu\/arm\/vmreg_arm.cpp","additions":0,"deletions":5,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -26,0 +26,1 @@\n+#include \"code\/vmreg.hpp\"\n@@ -29,0 +30,2 @@\n+class MacroAssembler;\n+\n@@ -30,1 +33,1 @@\n-const ABIDescriptor ForeignGlobals::parse_abi_descriptor_impl(jobject jabi) const {\n+const ABIDescriptor ForeignGlobals::parse_abi_descriptor(jobject jabi) {\n@@ -35,1 +38,1 @@\n-const BufferLayout ForeignGlobals::parse_buffer_layout_impl(jobject jlayout) const {\n+VMReg ForeignGlobals::vmstorage_to_vmreg(int type, int index) {\n@@ -37,1 +40,1 @@\n-  return {};\n+  return VMRegImpl::Bad();\n@@ -40,1 +43,14 @@\n-const CallRegs ForeignGlobals::parse_call_regs_impl(jobject jconv) const {\n+int RegSpiller::pd_reg_size(VMReg reg) {\n+  Unimplemented();\n+  return -1;\n+}\n+\n+void RegSpiller::pd_store_reg(MacroAssembler* masm, int offset, VMReg reg) {\n+  Unimplemented();\n+}\n+\n+void RegSpiller::pd_load_reg(MacroAssembler* masm, int offset, VMReg reg) {\n+  Unimplemented();\n+}\n+\n+void ArgumentShuffle::pd_generate(MacroAssembler* masm, VMReg tmp, int in_stk_bias, int out_stk_bias) const {\n@@ -42,1 +58,0 @@\n-  return {};\n","filename":"src\/hotspot\/cpu\/ppc\/foreign_globals_ppc.cpp","additions":20,"deletions":5,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -28,1 +28,0 @@\n-class BufferLayout {};\n","filename":"src\/hotspot\/cpu\/ppc\/foreign_globals_ppc.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -311,0 +311,6 @@\n+void MethodHandles::jump_to_native_invoker(MacroAssembler* _masm, Register nep_reg, Register temp_target) {\n+  BLOCK_COMMENT(\"jump_to_native_invoker {\");\n+  __ stop(\"Should not reach here\");\n+  BLOCK_COMMENT(\"} jump_to_native_invoker\");\n+}\n+\n@@ -328,4 +334,1 @@\n-  if (iid == vmIntrinsics::_invokeBasic || iid == vmIntrinsics::_linkToNative) {\n-    if (iid == vmIntrinsics::_linkToNative) {\n-      assert(for_compiler_entry, \"only compiler entry is supported\");\n-    }\n+  if (iid == vmIntrinsics::_invokeBasic) {\n@@ -334,0 +337,3 @@\n+  } else if (iid == vmIntrinsics::_linkToNative) {\n+    assert(for_compiler_entry, \"only compiler entry is supported\");\n+    jump_to_native_invoker(_masm, member_reg, temp1);\n","filename":"src\/hotspot\/cpu\/ppc\/methodHandles_ppc.cpp","additions":10,"deletions":4,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -62,0 +62,3 @@\n+\n+  static void jump_to_native_invoker(MacroAssembler* _masm,\n+                                     Register nep_reg, Register temp);\n","filename":"src\/hotspot\/cpu\/ppc\/methodHandles_ppc.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -1128,5 +1128,0 @@\n-int MachCallNativeNode::ret_addr_offset() {\n-  Unimplemented();\n-  return -1;\n-}\n-\n","filename":"src\/hotspot\/cpu\/ppc\/ppc.ad","additions":0,"deletions":5,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -1575,1 +1575,1 @@\n-  } else if (iid == vmIntrinsics::_invokeBasic || iid == vmIntrinsics::_linkToNative) {\n+  } else if (iid == vmIntrinsics::_invokeBasic) {\n@@ -1577,0 +1577,3 @@\n+  } else if (iid == vmIntrinsics::_linkToNative) {\n+    member_arg_pos = method->size_of_parameters() - 1;  \/\/ trailing NativeEntryPoint argument\n+    member_reg = R19_method;  \/\/ known to be free at this point\n@@ -3273,10 +3276,0 @@\n-\n-#ifdef COMPILER2\n-RuntimeStub* SharedRuntime::make_native_invoker(address call_target,\n-                                                int shadow_space_bytes,\n-                                                const GrowableArray<VMReg>& input_registers,\n-                                                const GrowableArray<VMReg>& output_registers) {\n-  Unimplemented();\n-  return nullptr;\n-}\n-#endif\n","filename":"src\/hotspot\/cpu\/ppc\/sharedRuntime_ppc.cpp","additions":4,"deletions":11,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -29,1 +29,7 @@\n-address ProgrammableInvoker::generate_adapter(jobject jabi, jobject jlayout) {\n+RuntimeStub* ProgrammableInvoker::make_native_invoker(BasicType* signature,\n+                                                      int num_args,\n+                                                      BasicType ret_bt,\n+                                                      const ABIDescriptor& abi,\n+                                                      const GrowableArray<VMReg>& input_registers,\n+                                                      const GrowableArray<VMReg>& output_registers,\n+                                                      bool needs_return_buffer) {\n","filename":"src\/hotspot\/cpu\/ppc\/universalNativeInvoker_ppc.cpp","additions":7,"deletions":1,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -29,6 +29,6 @@\n-address ProgrammableUpcallHandler::generate_upcall_stub(jobject rec, jobject jabi, jobject jlayout) {\n-  Unimplemented();\n-  return nullptr;\n-}\n-\n-address ProgrammableUpcallHandler::generate_optimized_upcall_stub(jobject mh, Method* entry, jobject jabi, jobject jconv) {\n+address ProgrammableUpcallHandler::generate_optimized_upcall_stub(jobject receiver, Method* entry,\n+                                                                  BasicType* in_sig_bt, int total_in_args,\n+                                                                  BasicType* out_sig_bt, int total_out_args,\n+                                                                  BasicType ret_type,\n+                                                                  jobject jabi, jobject jconv,\n+                                                                  bool needs_return_buffer, int ret_buf_size) {\n@@ -38,4 +38,0 @@\n-\n-bool ProgrammableUpcallHandler::supports_optimized_upcalls() {\n-  return false;\n-}\n","filename":"src\/hotspot\/cpu\/ppc\/universalUpcallHandle_ppc.cpp","additions":6,"deletions":10,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -51,5 +51,0 @@\n-\n-VMReg VMRegImpl::vmStorageToVMReg(int type, int index) {\n-  Unimplemented();\n-  return VMRegImpl::Bad();\n-}\n","filename":"src\/hotspot\/cpu\/ppc\/vmreg_ppc.cpp","additions":0,"deletions":5,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,0 +27,1 @@\n+#include \"code\/vmreg.hpp\"\n@@ -30,3 +31,4 @@\n-\/\/ Stubbed out, implement later\n-const ABIDescriptor ForeignGlobals::parse_abi_descriptor_impl(jobject jabi) const {\n-  Unimplemented();\n+class MacroAssembler;\n+\n+const ABIDescriptor ForeignGlobals::parse_abi_descriptor(jobject jabi) {\n+  ShouldNotCallThis();\n@@ -36,1 +38,1 @@\n-const BufferLayout ForeignGlobals::parse_buffer_layout_impl(jobject jlayout) const {\n+VMReg ForeignGlobals::vmstorage_to_vmreg(int type, int index) {\n@@ -38,1 +40,1 @@\n-  return {};\n+  return VMRegImpl::Bad();\n@@ -41,3 +43,3 @@\n-const CallRegs ForeignGlobals::parse_call_regs_impl(jobject jconv) const {\n-  ShouldNotCallThis();\n-  return {};\n+int RegSpiller::pd_reg_size(VMReg reg) {\n+  Unimplemented();\n+  return -1;\n@@ -45,0 +47,13 @@\n+\n+void RegSpiller::pd_store_reg(MacroAssembler* masm, int offset, VMReg reg) {\n+  Unimplemented();\n+}\n+\n+void RegSpiller::pd_load_reg(MacroAssembler* masm, int offset, VMReg reg) {\n+  Unimplemented();\n+}\n+\n+void ArgumentShuffle::pd_generate(MacroAssembler* masm, VMReg tmp, int in_stk_bias, int out_stk_bias) const {\n+  Unimplemented();\n+}\n+\n","filename":"src\/hotspot\/cpu\/riscv\/foreign_globals_riscv.cpp","additions":24,"deletions":9,"binary":false,"changes":33,"status":"modified"},{"patch":"@@ -30,1 +30,0 @@\n-class BufferLayout {};\n","filename":"src\/hotspot\/cpu\/riscv\/foreign_globals_riscv.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -261,0 +261,5 @@\n+void MethodHandles::jump_to_native_invoker(MacroAssembler* _masm, Register nep_reg, Register temp_target) {\n+  BLOCK_COMMENT(\"jump_to_native_invoker {\");\n+  __ stop(\"Should not reach here\");\n+  BLOCK_COMMENT(\"} jump_to_native_invoker\");\n+}\n@@ -283,4 +288,1 @@\n-  if (iid == vmIntrinsics::_invokeBasic || iid == vmIntrinsics::_linkToNative) {\n-    if (iid == vmIntrinsics::_linkToNative) {\n-      assert(for_compiler_entry, \"only compiler entry is supported\");\n-    }\n+  if (iid == vmIntrinsics::_invokeBasic) {\n@@ -289,0 +291,3 @@\n+  } else if (iid == vmIntrinsics::_linkToNative) {\n+    assert(for_compiler_entry, \"only compiler entry is supported\");\n+    jump_to_native_invoker(_masm, member_reg, temp1);\n","filename":"src\/hotspot\/cpu\/riscv\/methodHandles_riscv.cpp","additions":9,"deletions":4,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -58,0 +58,3 @@\n+\n+  static void jump_to_native_invoker(MacroAssembler* _masm,\n+                                     Register nep_reg, Register temp);\n","filename":"src\/hotspot\/cpu\/riscv\/methodHandles_riscv.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -1188,5 +1188,0 @@\n-int MachCallNativeNode::ret_addr_offset() {\n-  Unimplemented();\n-  return -1;\n-}\n-\n","filename":"src\/hotspot\/cpu\/riscv\/riscv.ad","additions":0,"deletions":5,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -1070,1 +1070,1 @@\n-  } else if (iid == vmIntrinsics::_invokeBasic || iid == vmIntrinsics::_linkToNative) {\n+  } else if (iid == vmIntrinsics::_invokeBasic) {\n@@ -1072,0 +1072,3 @@\n+  } else if (iid == vmIntrinsics::_linkToNative) {\n+    member_arg_pos = method->size_of_parameters() - 1;  \/\/ trailing NativeEntryPoint argument\n+    member_reg = x9;  \/\/ known to be free at this point\n@@ -2617,8 +2620,0 @@\n-RuntimeStub* SharedRuntime::make_native_invoker(address call_target,\n-                                                int shadow_space_bytes,\n-                                                const GrowableArray<VMReg>& input_registers,\n-                                                const GrowableArray<VMReg>& output_registers) {\n-  Unimplemented();\n-  return nullptr;\n-}\n-\n","filename":"src\/hotspot\/cpu\/riscv\/sharedRuntime_riscv.cpp","additions":4,"deletions":9,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -30,1 +30,7 @@\n-address ProgrammableInvoker::generate_adapter(jobject jabi, jobject jlayout) {\n+RuntimeStub* ProgrammableInvoker::make_native_invoker(BasicType* signature,\n+                                                      int num_args,\n+                                                      BasicType ret_bt,\n+                                                      const ABIDescriptor& abi,\n+                                                      const GrowableArray<VMReg>& input_registers,\n+                                                      const GrowableArray<VMReg>& output_registers,\n+                                                      bool needs_return_buffer) {\n","filename":"src\/hotspot\/cpu\/riscv\/universalNativeInvoker_riscv.cpp","additions":7,"deletions":1,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -30,6 +30,6 @@\n-address ProgrammableUpcallHandler::generate_upcall_stub(jobject jrec, jobject jabi, jobject jlayout) {\n-  Unimplemented();\n-  return nullptr;\n-}\n-\n-address ProgrammableUpcallHandler::generate_optimized_upcall_stub(jobject mh, Method* entry, jobject jabi, jobject jconv) {\n+address ProgrammableUpcallHandler::generate_optimized_upcall_stub(jobject receiver, Method* entry,\n+                                                                  BasicType* in_sig_bt, int total_in_args,\n+                                                                  BasicType* out_sig_bt, int total_out_args,\n+                                                                  BasicType ret_type,\n+                                                                  jobject jabi, jobject jconv,\n+                                                                  bool needs_return_buffer, int ret_buf_size) {\n@@ -38,5 +38,1 @@\n-}\n-\n-bool ProgrammableUpcallHandler::supports_optimized_upcalls() {\n-  return false;\n-}\n+}\n\\ No newline at end of file\n","filename":"src\/hotspot\/cpu\/riscv\/universalUpcallHandle_riscv.cpp","additions":7,"deletions":11,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -60,5 +60,0 @@\n-\n-VMReg VMRegImpl::vmStorageToVMReg(int type, int index) {\n-  Unimplemented();\n-  return VMRegImpl::Bad();\n-}\n","filename":"src\/hotspot\/cpu\/riscv\/vmreg_riscv.cpp","additions":0,"deletions":5,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -25,0 +25,1 @@\n+#include \"code\/vmreg.hpp\"\n@@ -28,1 +29,3 @@\n-const ABIDescriptor ForeignGlobals::parse_abi_descriptor_impl(jobject jabi) const {\n+class MacroAssembler;\n+\n+const ABIDescriptor ForeignGlobals::parse_abi_descriptor(jobject jabi) {\n@@ -33,1 +36,1 @@\n-const BufferLayout ForeignGlobals::parse_buffer_layout_impl(jobject jlayout) const {\n+VMReg ForeignGlobals::vmstorage_to_vmreg(int type, int index) {\n@@ -35,1 +38,1 @@\n-  return {};\n+  return VMRegImpl::Bad();\n@@ -38,1 +41,14 @@\n-const CallRegs ForeignGlobals::parse_call_regs_impl(jobject jconv) const {\n+int RegSpiller::pd_reg_size(VMReg reg) {\n+  Unimplemented();\n+  return -1;\n+}\n+\n+void RegSpiller::pd_store_reg(MacroAssembler* masm, int offset, VMReg reg) {\n+  Unimplemented();\n+}\n+\n+void RegSpiller::pd_load_reg(MacroAssembler* masm, int offset, VMReg reg) {\n+  Unimplemented();\n+}\n+\n+void ArgumentShuffle::pd_generate(MacroAssembler* masm, VMReg tmp, int in_stk_bias, int out_stk_bias) const {\n@@ -40,1 +56,0 @@\n-  return {};\n","filename":"src\/hotspot\/cpu\/s390\/foreign_globals_s390.cpp","additions":20,"deletions":5,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -27,1 +27,0 @@\n-class BufferLayout {};\n","filename":"src\/hotspot\/cpu\/s390\/foreign_globals_s390.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -352,0 +352,6 @@\n+void MethodHandles::jump_to_native_invoker(MacroAssembler* _masm, Register nep_reg, Register temp_target) {\n+  BLOCK_COMMENT(\"jump_to_native_invoker {\");\n+  __ should_not_reach_here();\n+  BLOCK_COMMENT(\"} jump_to_native_invoker\");\n+}\n+\n@@ -365,1 +371,1 @@\n-    assert(receiver_reg == (iid == vmIntrinsics::_linkToStatic ? noreg : Z_ARG1),\n+    assert(receiver_reg == (iid == vmIntrinsics::_linkToStatic || iid == vmIntrinsics::_linkToNative ? noreg : Z_ARG1),\n@@ -378,4 +384,1 @@\n-  if (iid == vmIntrinsics::_invokeBasic || iid == vmIntrinsics::_linkToNative) {\n-    if (iid == vmIntrinsics::_linkToNative) {\n-      assert(for_compiler_entry, \"only compiler entry is supported\");\n-    }\n+  if (iid == vmIntrinsics::_invokeBasic) {\n@@ -386,0 +389,3 @@\n+  } else if (iid == vmIntrinsics::_linkToNative) {\n+    assert(for_compiler_entry, \"only compiler entry is supported\");\n+    jump_to_native_invoker(_masm, member_reg, temp1);\n","filename":"src\/hotspot\/cpu\/s390\/methodHandles_s390.cpp","additions":11,"deletions":5,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -62,0 +62,3 @@\n+\n+  static void jump_to_native_invoker(MacroAssembler* _masm,\n+                                     Register nep_reg, Register temp);\n","filename":"src\/hotspot\/cpu\/s390\/methodHandles_s390.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -642,5 +642,0 @@\n-int MachCallNativeNode::ret_addr_offset() {\n-  Unimplemented();\n-  return -1;\n-}\n-\n","filename":"src\/hotspot\/cpu\/s390\/s390.ad","additions":0,"deletions":5,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -923,0 +923,3 @@\n+  } else if (special_dispatch == vmIntrinsics::_linkToNative) {\n+    member_arg_pos = total_args_passed - 1;  \/\/ trailing NativeEntryPoint argument\n+    member_reg = Z_R9;  \/\/ known to be free at this point\n@@ -924,1 +927,1 @@\n-    guarantee(special_dispatch == vmIntrinsics::_invokeBasic || special_dispatch == vmIntrinsics::_linkToNative,\n+    guarantee(special_dispatch == vmIntrinsics::_invokeBasic,\n@@ -3281,10 +3284,0 @@\n-\n-#ifdef COMPILER2\n-RuntimeStub* SharedRuntime::make_native_invoker(address call_target,\n-                                                int shadow_space_bytes,\n-                                                const GrowableArray<VMReg>& input_registers,\n-                                                const GrowableArray<VMReg>& output_registers) {\n-  Unimplemented();\n-  return nullptr;\n-}\n-#endif\n","filename":"src\/hotspot\/cpu\/s390\/sharedRuntime_s390.cpp","additions":4,"deletions":11,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -28,1 +28,7 @@\n-address ProgrammableInvoker::generate_adapter(jobject jabi, jobject jlayout) {\n+RuntimeStub* ProgrammableInvoker::make_native_invoker(BasicType* signature,\n+                                                      int num_args,\n+                                                      BasicType ret_bt,\n+                                                      const ABIDescriptor& abi,\n+                                                      const GrowableArray<VMReg>& input_registers,\n+                                                      const GrowableArray<VMReg>& output_registers,\n+                                                      bool needs_return_buffer) {\n","filename":"src\/hotspot\/cpu\/s390\/universalNativeInvoker_s390.cpp","additions":7,"deletions":1,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -28,6 +28,6 @@\n-address ProgrammableUpcallHandler::generate_upcall_stub(jobject rec, jobject jabi, jobject jlayout) {\n-  Unimplemented();\n-  return nullptr;\n-}\n-\n-address ProgrammableUpcallHandler::generate_optimized_upcall_stub(jobject mh, Method* entry, jobject jabi, jobject jconv) {\n+address ProgrammableUpcallHandler::generate_optimized_upcall_stub(jobject receiver, Method* entry,\n+                                                                  BasicType* in_sig_bt, int total_in_args,\n+                                                                  BasicType* out_sig_bt, int total_out_args,\n+                                                                  BasicType ret_type,\n+                                                                  jobject jabi, jobject jconv,\n+                                                                  bool needs_return_buffer, int ret_buf_size) {\n@@ -37,4 +37,0 @@\n-\n-bool ProgrammableUpcallHandler::supports_optimized_upcalls() {\n-  return false;\n-}\n","filename":"src\/hotspot\/cpu\/s390\/universalUpcallHandle_s390.cpp","additions":6,"deletions":10,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -50,5 +50,0 @@\n-\n-VMReg VMRegImpl::vmStorageToVMReg(int type, int index) {\n-  Unimplemented();\n-  return VMRegImpl::Bad();\n-}\n","filename":"src\/hotspot\/cpu\/s390\/vmreg_s390.cpp","additions":0,"deletions":5,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -1,118 +0,0 @@\n-\/*\n- * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"runtime\/jniHandles.hpp\"\n-#include \"runtime\/jniHandles.inline.hpp\"\n-#include \"oops\/typeArrayOop.inline.hpp\"\n-#include \"prims\/foreign_globals.hpp\"\n-#include \"prims\/foreign_globals.inline.hpp\"\n-\n-bool ABIDescriptor::is_volatile_reg(Register reg) const {\n-    return _integer_argument_registers.contains(reg)\n-        || _integer_additional_volatile_registers.contains(reg);\n-}\n-\n-bool ABIDescriptor::is_volatile_reg(XMMRegister reg) const {\n-    return _vector_argument_registers.contains(reg)\n-        || _vector_additional_volatile_registers.contains(reg);\n-}\n-\n-#define INTEGER_TYPE 0\n-#define VECTOR_TYPE 1\n-#define X87_TYPE 2\n-\n-const ABIDescriptor ForeignGlobals::parse_abi_descriptor_impl(jobject jabi) const {\n-  oop abi_oop = JNIHandles::resolve_non_null(jabi);\n-  ABIDescriptor abi;\n-\n-  objArrayOop inputStorage = cast<objArrayOop>(abi_oop->obj_field(ABI.inputStorage_offset));\n-  loadArray(inputStorage, INTEGER_TYPE, abi._integer_argument_registers, as_Register);\n-  loadArray(inputStorage, VECTOR_TYPE, abi._vector_argument_registers, as_XMMRegister);\n-\n-  objArrayOop outputStorage = cast<objArrayOop>(abi_oop->obj_field(ABI.outputStorage_offset));\n-  loadArray(outputStorage, INTEGER_TYPE, abi._integer_return_registers, as_Register);\n-  loadArray(outputStorage, VECTOR_TYPE, abi._vector_return_registers, as_XMMRegister);\n-  objArrayOop subarray = cast<objArrayOop>(outputStorage->obj_at(X87_TYPE));\n-  abi._X87_return_registers_noof = subarray->length();\n-\n-  objArrayOop volatileStorage = cast<objArrayOop>(abi_oop->obj_field(ABI.volatileStorage_offset));\n-  loadArray(volatileStorage, INTEGER_TYPE, abi._integer_additional_volatile_registers, as_Register);\n-  loadArray(volatileStorage, VECTOR_TYPE, abi._vector_additional_volatile_registers, as_XMMRegister);\n-\n-  abi._stack_alignment_bytes = abi_oop->int_field(ABI.stackAlignment_offset);\n-  abi._shadow_space_bytes = abi_oop->int_field(ABI.shadowSpace_offset);\n-\n-  return abi;\n-}\n-\n-const BufferLayout ForeignGlobals::parse_buffer_layout_impl(jobject jlayout) const {\n-  oop layout_oop = JNIHandles::resolve_non_null(jlayout);\n-  BufferLayout layout;\n-\n-  layout.stack_args_bytes = layout_oop->long_field(BL.stack_args_bytes_offset);\n-  layout.stack_args = layout_oop->long_field(BL.stack_args_offset);\n-  layout.arguments_next_pc = layout_oop->long_field(BL.arguments_next_pc_offset);\n-\n-  typeArrayOop input_offsets = cast<typeArrayOop>(layout_oop->obj_field(BL.input_type_offsets_offset));\n-  layout.arguments_integer = (size_t) input_offsets->long_at(INTEGER_TYPE);\n-  layout.arguments_vector = (size_t) input_offsets->long_at(VECTOR_TYPE);\n-\n-  typeArrayOop output_offsets = cast<typeArrayOop>(layout_oop->obj_field(BL.output_type_offsets_offset));\n-  layout.returns_integer = (size_t) output_offsets->long_at(INTEGER_TYPE);\n-  layout.returns_vector = (size_t) output_offsets->long_at(VECTOR_TYPE);\n-  layout.returns_x87 = (size_t) output_offsets->long_at(X87_TYPE);\n-\n-  layout.buffer_size = layout_oop->long_field(BL.size_offset);\n-\n-  return layout;\n-}\n-\n-const CallRegs ForeignGlobals::parse_call_regs_impl(jobject jconv) const {\n-  oop conv_oop = JNIHandles::resolve_non_null(jconv);\n-  objArrayOop arg_regs_oop = cast<objArrayOop>(conv_oop->obj_field(CallConvOffsets.arg_regs_offset));\n-  objArrayOop ret_regs_oop = cast<objArrayOop>(conv_oop->obj_field(CallConvOffsets.ret_regs_offset));\n-\n-  CallRegs result;\n-  result._args_length = arg_regs_oop->length();\n-  result._arg_regs = NEW_RESOURCE_ARRAY(VMReg, result._args_length);\n-\n-  result._rets_length = ret_regs_oop->length();\n-  result._ret_regs = NEW_RESOURCE_ARRAY(VMReg, result._rets_length);\n-\n-  for (int i = 0; i < result._args_length; i++) {\n-    oop storage = arg_regs_oop->obj_at(i);\n-    jint index = storage->int_field(VMS.index_offset);\n-    jint type = storage->int_field(VMS.type_offset);\n-    result._arg_regs[i] = VMRegImpl::vmStorageToVMReg(type, index);\n-  }\n-\n-  for (int i = 0; i < result._rets_length; i++) {\n-    oop storage = ret_regs_oop->obj_at(i);\n-    jint index = storage->int_field(VMS.index_offset);\n-    jint type = storage->int_field(VMS.type_offset);\n-    result._ret_regs[i] = VMRegImpl::vmStorageToVMReg(type, index);\n-  }\n-\n-  return result;\n-}\n","filename":"src\/hotspot\/cpu\/x86\/foreign_globals_x86.cpp","additions":0,"deletions":118,"binary":false,"changes":118,"status":"deleted"},{"patch":"@@ -30,0 +30,2 @@\n+class outputStream;\n+\n@@ -33,5 +35,5 @@\n-    GrowableArray<Register> _integer_argument_registers;\n-    GrowableArray<Register> _integer_return_registers;\n-    GrowableArray<XMMRegister> _vector_argument_registers;\n-    GrowableArray<XMMRegister> _vector_return_registers;\n-    size_t _X87_return_registers_noof;\n+  GrowableArray<Register> _integer_argument_registers;\n+  GrowableArray<Register> _integer_return_registers;\n+  GrowableArray<XMMRegister> _vector_argument_registers;\n+  GrowableArray<XMMRegister> _vector_return_registers;\n+  size_t _X87_return_registers_noof;\n@@ -39,2 +41,2 @@\n-    GrowableArray<Register> _integer_additional_volatile_registers;\n-    GrowableArray<XMMRegister> _vector_additional_volatile_registers;\n+  GrowableArray<Register> _integer_additional_volatile_registers;\n+  GrowableArray<XMMRegister> _vector_additional_volatile_registers;\n@@ -42,2 +44,2 @@\n-    int32_t _stack_alignment_bytes;\n-    int32_t _shadow_space_bytes;\n+  int32_t _stack_alignment_bytes;\n+  int32_t _shadow_space_bytes;\n@@ -45,3 +47,2 @@\n-    bool is_volatile_reg(Register reg) const;\n-    bool is_volatile_reg(XMMRegister reg) const;\n-};\n+  Register _target_addr_reg;\n+  Register _ret_buf_addr_reg;\n@@ -49,10 +50,2 @@\n-struct BufferLayout {\n-  size_t stack_args_bytes;\n-  size_t stack_args;\n-  size_t arguments_vector;\n-  size_t arguments_integer;\n-  size_t arguments_next_pc;\n-  size_t returns_vector;\n-  size_t returns_integer;\n-  size_t returns_x87;\n-  size_t buffer_size;\n+  bool is_volatile_reg(Register reg) const;\n+  bool is_volatile_reg(XMMRegister reg) const;\n","filename":"src\/hotspot\/cpu\/x86\/foreign_globals_x86.hpp","additions":15,"deletions":22,"binary":false,"changes":37,"status":"modified"},{"patch":"@@ -0,0 +1,56 @@\n+\/*\n+ * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"code\/vmreg.hpp\"\n+#include \"prims\/foreign_globals.hpp\"\n+#include \"utilities\/debug.hpp\"\n+\n+class MacroAssembler;\n+\n+const ABIDescriptor ForeignGlobals::parse_abi_descriptor(jobject jabi) {\n+  Unimplemented();\n+  return {};\n+}\n+\n+VMReg ForeignGlobals::vmstorage_to_vmreg(int type, int index) {\n+  Unimplemented();\n+  return VMRegImpl::Bad();\n+}\n+\n+int RegSpiller::pd_reg_size(VMReg reg) {\n+  Unimplemented();\n+  return -1;\n+}\n+\n+void RegSpiller::pd_store_reg(MacroAssembler* masm, int offset, VMReg reg) {\n+  Unimplemented();\n+}\n+\n+void RegSpiller::pd_load_reg(MacroAssembler* masm, int offset, VMReg reg) {\n+  Unimplemented();\n+}\n+\n+void ArgumentShuffle::pd_generate(MacroAssembler* masm, VMReg tmp, int in_stk_bias, int out_stk_bias) const {\n+  Unimplemented();\n+}\n","filename":"src\/hotspot\/cpu\/x86\/foreign_globals_x86_32.cpp","additions":56,"deletions":0,"binary":false,"changes":56,"status":"added"},{"patch":"@@ -0,0 +1,29 @@\n+\/*\n+ * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef CPU_X86_VM_FOREIGN_GLOBALS_X86_32_HPP\n+#define CPU_X86_VM_FOREIGN_GLOBALS_X86_32_HPP\n+\n+struct ABIDescriptor {};\n+\n+#endif \/\/ CPU_X86_VM_FOREIGN_GLOBALS_X86_32_HPP\n","filename":"src\/hotspot\/cpu\/x86\/foreign_globals_x86_32.hpp","additions":29,"deletions":0,"binary":false,"changes":29,"status":"added"},{"patch":"@@ -0,0 +1,162 @@\n+\/*\n+ * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"classfile\/javaClasses.hpp\"\n+#include \"runtime\/jniHandles.inline.hpp\"\n+#include \"oops\/typeArrayOop.inline.hpp\"\n+#include \"oops\/oopCast.inline.hpp\"\n+#include \"prims\/foreign_globals.inline.hpp\"\n+#include \"runtime\/sharedRuntime.hpp\"\n+#include \"utilities\/formatBuffer.hpp\"\n+\n+bool ABIDescriptor::is_volatile_reg(Register reg) const {\n+    return _integer_argument_registers.contains(reg)\n+        || _integer_additional_volatile_registers.contains(reg);\n+}\n+\n+bool ABIDescriptor::is_volatile_reg(XMMRegister reg) const {\n+    return _vector_argument_registers.contains(reg)\n+        || _vector_additional_volatile_registers.contains(reg);\n+}\n+\n+static constexpr int INTEGER_TYPE = 0;\n+static constexpr int VECTOR_TYPE = 1;\n+static constexpr int X87_TYPE = 2;\n+\n+const ABIDescriptor ForeignGlobals::parse_abi_descriptor(jobject jabi) {\n+  oop abi_oop = JNIHandles::resolve_non_null(jabi);\n+  ABIDescriptor abi;\n+\n+  objArrayOop inputStorage = jdk_internal_foreign_abi_ABIDescriptor::inputStorage(abi_oop);\n+  loadArray(inputStorage, INTEGER_TYPE, abi._integer_argument_registers, as_Register);\n+  loadArray(inputStorage, VECTOR_TYPE, abi._vector_argument_registers, as_XMMRegister);\n+\n+  objArrayOop outputStorage = jdk_internal_foreign_abi_ABIDescriptor::outputStorage(abi_oop);\n+  loadArray(outputStorage, INTEGER_TYPE, abi._integer_return_registers, as_Register);\n+  loadArray(outputStorage, VECTOR_TYPE, abi._vector_return_registers, as_XMMRegister);\n+  objArrayOop subarray = oop_cast<objArrayOop>(outputStorage->obj_at(X87_TYPE));\n+  abi._X87_return_registers_noof = subarray->length();\n+\n+  objArrayOop volatileStorage = jdk_internal_foreign_abi_ABIDescriptor::volatileStorage(abi_oop);\n+  loadArray(volatileStorage, INTEGER_TYPE, abi._integer_additional_volatile_registers, as_Register);\n+  loadArray(volatileStorage, VECTOR_TYPE, abi._vector_additional_volatile_registers, as_XMMRegister);\n+\n+  abi._stack_alignment_bytes = jdk_internal_foreign_abi_ABIDescriptor::stackAlignment(abi_oop);\n+  abi._shadow_space_bytes = jdk_internal_foreign_abi_ABIDescriptor::shadowSpace(abi_oop);\n+\n+  abi._target_addr_reg = parse_vmstorage(jdk_internal_foreign_abi_ABIDescriptor::targetAddrStorage(abi_oop))->as_Register();\n+  abi._ret_buf_addr_reg = parse_vmstorage(jdk_internal_foreign_abi_ABIDescriptor::retBufAddrStorage(abi_oop))->as_Register();\n+\n+  return abi;\n+}\n+\n+enum class RegType {\n+  INTEGER = 0,\n+  VECTOR = 1,\n+  X87 = 2,\n+  STACK = 3\n+};\n+\n+VMReg ForeignGlobals::vmstorage_to_vmreg(int type, int index) {\n+  switch(static_cast<RegType>(type)) {\n+    case RegType::INTEGER: return ::as_Register(index)->as_VMReg();\n+    case RegType::VECTOR: return ::as_XMMRegister(index)->as_VMReg();\n+    case RegType::STACK: return VMRegImpl::stack2reg(index LP64_ONLY(* 2)); \/\/ numbering on x64 goes per 64-bits\n+    case RegType::X87: break;\n+  }\n+  return VMRegImpl::Bad();\n+}\n+\n+int RegSpiller::pd_reg_size(VMReg reg) {\n+  if (reg->is_Register()) {\n+    return 8;\n+  } else if (reg->is_XMMRegister()) {\n+    return 16;\n+  }\n+  return 0; \/\/ stack and BAD\n+}\n+\n+void RegSpiller::pd_store_reg(MacroAssembler* masm, int offset, VMReg reg) {\n+  if (reg->is_Register()) {\n+    masm->movptr(Address(rsp, offset), reg->as_Register());\n+  } else if (reg->is_XMMRegister()) {\n+    masm->movdqu(Address(rsp, offset), reg->as_XMMRegister());\n+  } else {\n+    \/\/ stack and BAD\n+  }\n+}\n+\n+void RegSpiller::pd_load_reg(MacroAssembler* masm, int offset, VMReg reg) {\n+  if (reg->is_Register()) {\n+    masm->movptr(reg->as_Register(), Address(rsp, offset));\n+  } else if (reg->is_XMMRegister()) {\n+    masm->movdqu(reg->as_XMMRegister(), Address(rsp, offset));\n+  } else {\n+    \/\/ stack and BAD\n+  }\n+}\n+\n+void ArgumentShuffle::pd_generate(MacroAssembler* masm, VMReg tmp, int in_stk_bias, int out_stk_bias) const {\n+  Register tmp_reg = tmp->as_Register();\n+  for (int i = 0; i < _moves.length(); i++) {\n+    Move move = _moves.at(i);\n+    BasicType arg_bt     = move.bt;\n+    VMRegPair from_vmreg = move.from;\n+    VMRegPair to_vmreg   = move.to;\n+\n+    masm->block_comment(err_msg(\"bt=%s\", null_safe_string(type2name(arg_bt))));\n+    switch (arg_bt) {\n+      case T_BOOLEAN:\n+      case T_BYTE:\n+      case T_SHORT:\n+      case T_CHAR:\n+      case T_INT:\n+        masm->move32_64(from_vmreg, to_vmreg, tmp_reg, in_stk_bias, out_stk_bias);\n+        break;\n+\n+      case T_FLOAT:\n+        if (to_vmreg.first()->is_Register()) { \/\/ Windows vararg call\n+          masm->movq(to_vmreg.first()->as_Register(), from_vmreg.first()->as_XMMRegister());\n+        } else {\n+          masm->float_move(from_vmreg, to_vmreg, tmp_reg, in_stk_bias, out_stk_bias);\n+        }\n+        break;\n+\n+      case T_DOUBLE:\n+        if (to_vmreg.first()->is_Register()) { \/\/ Windows vararg call\n+          masm->movq(to_vmreg.first()->as_Register(), from_vmreg.first()->as_XMMRegister());\n+        } else {\n+          masm->double_move(from_vmreg, to_vmreg, tmp_reg, in_stk_bias, out_stk_bias);\n+        }\n+        break;\n+\n+      case T_LONG:\n+        masm->long_move(from_vmreg, to_vmreg, tmp_reg, in_stk_bias, out_stk_bias);\n+        break;\n+\n+      default:\n+        fatal(\"found in upcall args: %s\", type2name(arg_bt));\n+    }\n+  }\n+}\n","filename":"src\/hotspot\/cpu\/x86\/foreign_globals_x86_64.cpp","additions":162,"deletions":0,"binary":false,"changes":162,"status":"added"},{"patch":"@@ -0,0 +1,54 @@\n+\/*\n+ * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef CPU_X86_VM_FOREIGN_GLOBALS_X86_HPP\n+#define CPU_X86_VM_FOREIGN_GLOBALS_X86_HPP\n+\n+#include \"asm\/macroAssembler.hpp\"\n+#include \"utilities\/growableArray.hpp\"\n+\n+class outputStream;\n+\n+constexpr size_t xmm_reg_size = 16; \/\/ size of XMM reg\n+\n+struct ABIDescriptor {\n+  GrowableArray<Register> _integer_argument_registers;\n+  GrowableArray<Register> _integer_return_registers;\n+  GrowableArray<XMMRegister> _vector_argument_registers;\n+  GrowableArray<XMMRegister> _vector_return_registers;\n+  size_t _X87_return_registers_noof;\n+\n+  GrowableArray<Register> _integer_additional_volatile_registers;\n+  GrowableArray<XMMRegister> _vector_additional_volatile_registers;\n+\n+  int32_t _stack_alignment_bytes;\n+  int32_t _shadow_space_bytes;\n+\n+  Register _target_addr_reg;\n+  Register _ret_buf_addr_reg;\n+\n+  bool is_volatile_reg(Register reg) const;\n+  bool is_volatile_reg(XMMRegister reg) const;\n+};\n+\n+#endif \/\/ CPU_X86_VM_FOREIGN_GLOBALS_X86_HPP\n","filename":"src\/hotspot\/cpu\/x86\/foreign_globals_x86_64.hpp","additions":54,"deletions":0,"binary":false,"changes":54,"status":"added"},{"patch":"@@ -921,1 +921,1 @@\n-void MacroAssembler::long_move(VMRegPair src, VMRegPair dst) {\n+void MacroAssembler::long_move(VMRegPair src, VMRegPair dst, Register tmp, int in_stk_bias, int out_stk_bias) {\n@@ -932,2 +932,3 @@\n-      assert(dst.is_single_reg(), \"not a stack pair\");\n-      movq(Address(rsp, reg2offset_out(dst.first())), src.first()->as_Register());\n+      assert(dst.is_single_reg(), \"not a stack pair: (%s, %s), (%s, %s)\",\n+       src.first()->name(), src.second()->name(), dst.first()->name(), dst.second()->name());\n+      movq(Address(rsp, reg2offset_out(dst.first()) + out_stk_bias), src.first()->as_Register());\n@@ -937,1 +938,1 @@\n-    movq(dst.first()->as_Register(), Address(rbp, reg2offset_out(src.first())));\n+    movq(dst.first()->as_Register(), Address(rbp, reg2offset_in(src.first()) + in_stk_bias));\n@@ -940,2 +941,2 @@\n-    movq(rax, Address(rbp, reg2offset_in(src.first())));\n-    movq(Address(rsp, reg2offset_out(dst.first())), rax);\n+    movq(tmp, Address(rbp, reg2offset_in(src.first()) + in_stk_bias));\n+    movq(Address(rsp, reg2offset_out(dst.first()) + out_stk_bias), tmp);\n@@ -946,1 +947,1 @@\n-void MacroAssembler::double_move(VMRegPair src, VMRegPair dst) {\n+void MacroAssembler::double_move(VMRegPair src, VMRegPair dst, Register tmp, int in_stk_bias, int out_stk_bias) {\n@@ -959,1 +960,1 @@\n-      movdbl(Address(rsp, reg2offset_out(dst.first())), src.first()->as_XMMRegister());\n+      movdbl(Address(rsp, reg2offset_out(dst.first()) + out_stk_bias), src.first()->as_XMMRegister());\n@@ -963,1 +964,1 @@\n-    movdbl(dst.first()->as_XMMRegister(), Address(rbp, reg2offset_out(src.first())));\n+    movdbl(dst.first()->as_XMMRegister(), Address(rbp, reg2offset_in(src.first()) + in_stk_bias));\n@@ -966,2 +967,2 @@\n-    movq(rax, Address(rbp, reg2offset_in(src.first())));\n-    movq(Address(rsp, reg2offset_out(dst.first())), rax);\n+    movq(tmp, Address(rbp, reg2offset_in(src.first()) + in_stk_bias));\n+    movq(Address(rsp, reg2offset_out(dst.first()) + out_stk_bias), tmp);\n@@ -973,1 +974,1 @@\n-void MacroAssembler::float_move(VMRegPair src, VMRegPair dst) {\n+void MacroAssembler::float_move(VMRegPair src, VMRegPair dst, Register tmp, int in_stk_bias, int out_stk_bias) {\n@@ -981,2 +982,2 @@\n-      movl(rax, Address(rbp, reg2offset_in(src.first())));\n-      movptr(Address(rsp, reg2offset_out(dst.first())), rax);\n+      movl(tmp, Address(rbp, reg2offset_in(src.first()) + in_stk_bias));\n+      movptr(Address(rsp, reg2offset_out(dst.first()) + out_stk_bias), tmp);\n@@ -986,1 +987,1 @@\n-      movflt(dst.first()->as_XMMRegister(), Address(rbp, reg2offset_in(src.first())));\n+      movflt(dst.first()->as_XMMRegister(), Address(rbp, reg2offset_in(src.first()) + in_stk_bias));\n@@ -991,1 +992,1 @@\n-    movflt(Address(rsp, reg2offset_out(dst.first())), src.first()->as_XMMRegister());\n+    movflt(Address(rsp, reg2offset_out(dst.first()) + out_stk_bias), src.first()->as_XMMRegister());\n@@ -1005,1 +1006,1 @@\n-void MacroAssembler::move32_64(VMRegPair src, VMRegPair dst) {\n+void MacroAssembler::move32_64(VMRegPair src, VMRegPair dst, Register tmp, int in_stk_bias, int out_stk_bias) {\n@@ -1009,2 +1010,2 @@\n-      movslq(rax, Address(rbp, reg2offset_in(src.first())));\n-      movq(Address(rsp, reg2offset_out(dst.first())), rax);\n+      movslq(tmp, Address(rbp, reg2offset_in(src.first()) + in_stk_bias));\n+      movq(Address(rsp, reg2offset_out(dst.first()) + out_stk_bias), tmp);\n@@ -1013,1 +1014,1 @@\n-      movslq(dst.first()->as_Register(), Address(rbp, reg2offset_in(src.first())));\n+      movslq(dst.first()->as_Register(), Address(rbp, reg2offset_in(src.first()) + in_stk_bias));\n@@ -1019,1 +1020,1 @@\n-    movq(Address(rsp, reg2offset_out(dst.first())), src.first()->as_Register());\n+    movq(Address(rsp, reg2offset_out(dst.first()) + out_stk_bias), src.first()->as_Register());\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":22,"deletions":21,"binary":false,"changes":43,"status":"modified"},{"patch":"@@ -218,4 +218,5 @@\n-  void move32_64(VMRegPair src, VMRegPair dst);\n-  void long_move(VMRegPair src, VMRegPair dst);\n-  void float_move(VMRegPair src, VMRegPair dst);\n-  void double_move(VMRegPair src, VMRegPair dst);\n+  \/\/ bias in bytes\n+  void move32_64(VMRegPair src, VMRegPair dst, Register tmp = rax, int in_stk_bias = 0, int out_stk_bias = 0);\n+  void long_move(VMRegPair src, VMRegPair dst, Register tmp = rax, int in_stk_bias = 0, int out_stk_bias = 0);\n+  void float_move(VMRegPair src, VMRegPair dst, Register tmp = rax, int in_stk_bias = 0, int out_stk_bias = 0);\n+  void double_move(VMRegPair src, VMRegPair dst, Register tmp = rax, int in_stk_bias = 0, int out_stk_bias = 0);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":5,"deletions":4,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -206,0 +206,15 @@\n+void MethodHandles::jump_to_native_invoker(MacroAssembler* _masm, Register nep_reg, Register temp_target) {\n+  BLOCK_COMMENT(\"jump_to_native_invoker {\");\n+  assert_different_registers(nep_reg, temp_target);\n+  assert(nep_reg != noreg, \"required register\");\n+\n+  \/\/ Load the invoker, as NEP -> .invoker\n+  __ verify_oop(nep_reg);\n+  __ access_load_at(T_ADDRESS, IN_HEAP, temp_target,\n+                    Address(nep_reg, NONZERO(jdk_internal_foreign_abi_NativeEntryPoint::invoker_offset_in_bytes())),\n+                    noreg, noreg);\n+\n+  __ jmp(temp_target);\n+  BLOCK_COMMENT(\"} jump_to_native_invoker\");\n+}\n+\n@@ -317,1 +332,1 @@\n-    assert(receiver_reg == (iid == vmIntrinsics::_linkToStatic ? noreg : j_rarg0), \"only valid assignment\");\n+    assert(receiver_reg == (iid == vmIntrinsics::_linkToStatic || iid == vmIntrinsics::_linkToNative ? noreg : j_rarg0), \"only valid assignment\");\n@@ -327,1 +342,1 @@\n-    assert(receiver_reg == (iid == vmIntrinsics::_linkToStatic ? noreg : rcx), \"only valid assignment\");\n+    assert(receiver_reg == (iid == vmIntrinsics::_linkToStatic || iid == vmIntrinsics::_linkToNative ? noreg : rcx), \"only valid assignment\");\n@@ -339,4 +354,1 @@\n-  if (iid == vmIntrinsics::_invokeBasic || iid == vmIntrinsics::_linkToNative) {\n-    if (iid == vmIntrinsics::_linkToNative) {\n-      assert(for_compiler_entry, \"only compiler entry is supported\");\n-    }\n+  if (iid == vmIntrinsics::_invokeBasic) {\n@@ -345,1 +357,3 @@\n-\n+  } else if (iid == vmIntrinsics::_linkToNative) {\n+    assert(for_compiler_entry, \"only compiler entry is supported\");\n+    jump_to_native_invoker(_masm, member_reg, temp1);\n","filename":"src\/hotspot\/cpu\/x86\/methodHandles_x86.cpp","additions":21,"deletions":7,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -58,0 +58,3 @@\n+  static void jump_to_native_invoker(MacroAssembler* _masm,\n+                                     Register nep_reg, Register temp);\n+\n","filename":"src\/hotspot\/cpu\/x86\/methodHandles_x86.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -2863,10 +2863,0 @@\n-\n-#ifdef COMPILER2\n-RuntimeStub* SharedRuntime::make_native_invoker(address call_target,\n-                                                int shadow_space_bytes,\n-                                                const GrowableArray<VMReg>& input_registers,\n-                                                const GrowableArray<VMReg>& output_registers) {\n-  ShouldNotCallThis();\n-  return nullptr;\n-}\n-#endif\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_32.cpp","additions":0,"deletions":10,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"compiler\/disassembler.hpp\"\n@@ -43,0 +44,1 @@\n+#include \"logging\/logStream.hpp\"\n@@ -1237,174 +1239,0 @@\n-\/\/ Different signatures may require very different orders for the move\n-\/\/ to avoid clobbering other arguments.  There's no simple way to\n-\/\/ order them safely.  Compute a safe order for issuing stores and\n-\/\/ break any cycles in those stores.  This code is fairly general but\n-\/\/ it's not necessary on the other platforms so we keep it in the\n-\/\/ platform dependent code instead of moving it into a shared file.\n-\/\/ (See bugs 7013347 & 7145024.)\n-\/\/ Note that this code is specific to LP64.\n-class ComputeMoveOrder: public StackObj {\n-  class MoveOperation: public ResourceObj {\n-    friend class ComputeMoveOrder;\n-   private:\n-    VMRegPair        _src;\n-    VMRegPair        _dst;\n-    int              _src_index;\n-    int              _dst_index;\n-    bool             _processed;\n-    MoveOperation*  _next;\n-    MoveOperation*  _prev;\n-\n-    static int get_id(VMRegPair r) {\n-      return r.first()->value();\n-    }\n-\n-   public:\n-    MoveOperation(int src_index, VMRegPair src, int dst_index, VMRegPair dst):\n-      _src(src)\n-    , _dst(dst)\n-    , _src_index(src_index)\n-    , _dst_index(dst_index)\n-    , _processed(false)\n-    , _next(NULL)\n-    , _prev(NULL) {\n-    }\n-\n-    VMRegPair src() const              { return _src; }\n-    int src_id() const                 { return get_id(src()); }\n-    int src_index() const              { return _src_index; }\n-    VMRegPair dst() const              { return _dst; }\n-    void set_dst(int i, VMRegPair dst) { _dst_index = i, _dst = dst; }\n-    int dst_index() const              { return _dst_index; }\n-    int dst_id() const                 { return get_id(dst()); }\n-    MoveOperation* next() const       { return _next; }\n-    MoveOperation* prev() const       { return _prev; }\n-    void set_processed()               { _processed = true; }\n-    bool is_processed() const          { return _processed; }\n-\n-    \/\/ insert\n-    void break_cycle(VMRegPair temp_register) {\n-      \/\/ create a new store following the last store\n-      \/\/ to move from the temp_register to the original\n-      MoveOperation* new_store = new MoveOperation(-1, temp_register, dst_index(), dst());\n-\n-      \/\/ break the cycle of links and insert new_store at the end\n-      \/\/ break the reverse link.\n-      MoveOperation* p = prev();\n-      assert(p->next() == this, \"must be\");\n-      _prev = NULL;\n-      p->_next = new_store;\n-      new_store->_prev = p;\n-\n-      \/\/ change the original store to save it's value in the temp.\n-      set_dst(-1, temp_register);\n-    }\n-\n-    void link(GrowableArray<MoveOperation*>& killer) {\n-      \/\/ link this store in front the store that it depends on\n-      MoveOperation* n = killer.at_grow(src_id(), NULL);\n-      if (n != NULL) {\n-        assert(_next == NULL && n->_prev == NULL, \"shouldn't have been set yet\");\n-        _next = n;\n-        n->_prev = this;\n-      }\n-    }\n-  };\n-\n- private:\n-  GrowableArray<MoveOperation*> edges;\n-\n- public:\n-  ComputeMoveOrder(int total_in_args, const VMRegPair* in_regs, int total_c_args, VMRegPair* out_regs,\n-                  const BasicType* in_sig_bt, GrowableArray<int>& arg_order, VMRegPair tmp_vmreg) {\n-    \/\/ Move operations where the dest is the stack can all be\n-    \/\/ scheduled first since they can't interfere with the other moves.\n-    for (int i = total_in_args - 1, c_arg = total_c_args - 1; i >= 0; i--, c_arg--) {\n-      if (in_sig_bt[i] == T_ARRAY) {\n-        c_arg--;\n-        if (out_regs[c_arg].first()->is_stack() &&\n-            out_regs[c_arg + 1].first()->is_stack()) {\n-          arg_order.push(i);\n-          arg_order.push(c_arg);\n-        } else {\n-          if (out_regs[c_arg].first()->is_stack() ||\n-              in_regs[i].first() == out_regs[c_arg].first()) {\n-            add_edge(i, in_regs[i].first(), c_arg, out_regs[c_arg + 1]);\n-          } else {\n-            add_edge(i, in_regs[i].first(), c_arg, out_regs[c_arg]);\n-          }\n-        }\n-      } else if (in_sig_bt[i] == T_VOID) {\n-        arg_order.push(i);\n-        arg_order.push(c_arg);\n-      } else {\n-        if (out_regs[c_arg].first()->is_stack() ||\n-            in_regs[i].first() == out_regs[c_arg].first()) {\n-          arg_order.push(i);\n-          arg_order.push(c_arg);\n-        } else {\n-          add_edge(i, in_regs[i].first(), c_arg, out_regs[c_arg]);\n-        }\n-      }\n-    }\n-    \/\/ Break any cycles in the register moves and emit the in the\n-    \/\/ proper order.\n-    GrowableArray<MoveOperation*>* stores = get_store_order(tmp_vmreg);\n-    for (int i = 0; i < stores->length(); i++) {\n-      arg_order.push(stores->at(i)->src_index());\n-      arg_order.push(stores->at(i)->dst_index());\n-    }\n- }\n-\n-  \/\/ Collected all the move operations\n-  void add_edge(int src_index, VMRegPair src, int dst_index, VMRegPair dst) {\n-    if (src.first() == dst.first()) return;\n-    edges.append(new MoveOperation(src_index, src, dst_index, dst));\n-  }\n-\n-  \/\/ Walk the edges breaking cycles between moves.  The result list\n-  \/\/ can be walked in order to produce the proper set of loads\n-  GrowableArray<MoveOperation*>* get_store_order(VMRegPair temp_register) {\n-    \/\/ Record which moves kill which values\n-    GrowableArray<MoveOperation*> killer;\n-    for (int i = 0; i < edges.length(); i++) {\n-      MoveOperation* s = edges.at(i);\n-      assert(killer.at_grow(s->dst_id(), NULL) == NULL, \"only one killer\");\n-      killer.at_put_grow(s->dst_id(), s, NULL);\n-    }\n-    assert(killer.at_grow(MoveOperation::get_id(temp_register), NULL) == NULL,\n-           \"make sure temp isn't in the registers that are killed\");\n-\n-    \/\/ create links between loads and stores\n-    for (int i = 0; i < edges.length(); i++) {\n-      edges.at(i)->link(killer);\n-    }\n-\n-    \/\/ at this point, all the move operations are chained together\n-    \/\/ in a doubly linked list.  Processing it backwards finds\n-    \/\/ the beginning of the chain, forwards finds the end.  If there's\n-    \/\/ a cycle it can be broken at any point,  so pick an edge and walk\n-    \/\/ backward until the list ends or we end where we started.\n-    GrowableArray<MoveOperation*>* stores = new GrowableArray<MoveOperation*>();\n-    for (int e = 0; e < edges.length(); e++) {\n-      MoveOperation* s = edges.at(e);\n-      if (!s->is_processed()) {\n-        MoveOperation* start = s;\n-        \/\/ search for the beginning of the chain or cycle\n-        while (start->prev() != NULL && start->prev() != s) {\n-          start = start->prev();\n-        }\n-        if (start->prev() == s) {\n-          start->break_cycle(temp_register);\n-        }\n-        \/\/ walk the chain forward inserting to store list\n-        while (start != NULL) {\n-          stores->append(start);\n-          start->set_processed();\n-          start = start->next();\n-        }\n-      }\n-    }\n-    return stores;\n-  }\n-};\n-\n@@ -1546,1 +1374,1 @@\n-  } else if (iid == vmIntrinsics::_invokeBasic || iid == vmIntrinsics::_linkToNative) {\n+  } else if (iid == vmIntrinsics::_invokeBasic) {\n@@ -1548,0 +1376,3 @@\n+  } else if (iid == vmIntrinsics::_linkToNative) {\n+    member_arg_pos = method->size_of_parameters() - 1;  \/\/ trailing NativeEntryPoint argument\n+    member_reg = rbx;  \/\/ known to be free at this point\n@@ -3201,257 +3032,0 @@\n-#ifdef COMPILER2\n-static const int native_invoker_code_size = MethodHandles::adapter_code_size;\n-\n-class NativeInvokerGenerator : public StubCodeGenerator {\n-  address _call_target;\n-  int _shadow_space_bytes;\n-\n-  const GrowableArray<VMReg>& _input_registers;\n-  const GrowableArray<VMReg>& _output_registers;\n-\n-  int _frame_complete;\n-  int _framesize;\n-  OopMapSet* _oop_maps;\n-public:\n-  NativeInvokerGenerator(CodeBuffer* buffer,\n-                         address call_target,\n-                         int shadow_space_bytes,\n-                         const GrowableArray<VMReg>& input_registers,\n-                         const GrowableArray<VMReg>& output_registers)\n-   : StubCodeGenerator(buffer, PrintMethodHandleStubs),\n-     _call_target(call_target),\n-     _shadow_space_bytes(shadow_space_bytes),\n-     _input_registers(input_registers),\n-     _output_registers(output_registers),\n-     _frame_complete(0),\n-     _framesize(0),\n-     _oop_maps(NULL) {\n-    assert(_output_registers.length() <= 1\n-           || (_output_registers.length() == 2 && !_output_registers.at(1)->is_valid()), \"no multi-reg returns\");\n-\n-  }\n-\n-  void generate();\n-\n-  int spill_size_in_bytes() const {\n-    if (_output_registers.length() == 0) {\n-      return 0;\n-    }\n-    VMReg reg = _output_registers.at(0);\n-    assert(reg->is_reg(), \"must be a register\");\n-    if (reg->is_Register()) {\n-      return 8;\n-    } else if (reg->is_XMMRegister()) {\n-      if (UseAVX >= 3) {\n-        return 64;\n-      } else if (UseAVX >= 1) {\n-        return 32;\n-      } else {\n-        return 16;\n-      }\n-    } else {\n-      ShouldNotReachHere();\n-    }\n-    return 0;\n-  }\n-\n-  void spill_out_registers() {\n-    if (_output_registers.length() == 0) {\n-      return;\n-    }\n-    VMReg reg = _output_registers.at(0);\n-    assert(reg->is_reg(), \"must be a register\");\n-    MacroAssembler* masm = _masm;\n-    if (reg->is_Register()) {\n-      __ movptr(Address(rsp, 0), reg->as_Register());\n-    } else if (reg->is_XMMRegister()) {\n-      if (UseAVX >= 3) {\n-        __ evmovdqul(Address(rsp, 0), reg->as_XMMRegister(), Assembler::AVX_512bit);\n-      } else if (UseAVX >= 1) {\n-        __ vmovdqu(Address(rsp, 0), reg->as_XMMRegister());\n-      } else {\n-        __ movdqu(Address(rsp, 0), reg->as_XMMRegister());\n-      }\n-    } else {\n-      ShouldNotReachHere();\n-    }\n-  }\n-\n-  void fill_out_registers() {\n-    if (_output_registers.length() == 0) {\n-      return;\n-    }\n-    VMReg reg = _output_registers.at(0);\n-    assert(reg->is_reg(), \"must be a register\");\n-    MacroAssembler* masm = _masm;\n-    if (reg->is_Register()) {\n-      __ movptr(reg->as_Register(), Address(rsp, 0));\n-    } else if (reg->is_XMMRegister()) {\n-      if (UseAVX >= 3) {\n-        __ evmovdqul(reg->as_XMMRegister(), Address(rsp, 0), Assembler::AVX_512bit);\n-      } else if (UseAVX >= 1) {\n-        __ vmovdqu(reg->as_XMMRegister(), Address(rsp, 0));\n-      } else {\n-        __ movdqu(reg->as_XMMRegister(), Address(rsp, 0));\n-      }\n-    } else {\n-      ShouldNotReachHere();\n-    }\n-  }\n-\n-  int frame_complete() const {\n-    return _frame_complete;\n-  }\n-\n-  int framesize() const {\n-    return (_framesize >> (LogBytesPerWord - LogBytesPerInt));\n-  }\n-\n-  OopMapSet* oop_maps() const {\n-    return _oop_maps;\n-  }\n-\n-private:\n-#ifdef ASSERT\n-bool target_uses_register(VMReg reg) {\n-  return _input_registers.contains(reg) || _output_registers.contains(reg);\n-}\n-#endif\n-};\n-\n-RuntimeStub* SharedRuntime::make_native_invoker(address call_target,\n-                                                int shadow_space_bytes,\n-                                                const GrowableArray<VMReg>& input_registers,\n-                                                const GrowableArray<VMReg>& output_registers) {\n-  int locs_size  = 64;\n-  CodeBuffer code(\"nep_invoker_blob\", native_invoker_code_size, locs_size);\n-  NativeInvokerGenerator g(&code, call_target, shadow_space_bytes, input_registers, output_registers);\n-  g.generate();\n-  code.log_section_sizes(\"nep_invoker_blob\");\n-\n-  RuntimeStub* stub =\n-    RuntimeStub::new_runtime_stub(\"nep_invoker_blob\",\n-                                  &code,\n-                                  g.frame_complete(),\n-                                  g.framesize(),\n-                                  g.oop_maps(), false);\n-  return stub;\n-}\n-\n-void NativeInvokerGenerator::generate() {\n-  assert(!(target_uses_register(r15_thread->as_VMReg()) || target_uses_register(rscratch1->as_VMReg())), \"Register conflict\");\n-\n-  enum layout {\n-    rbp_off,\n-    rbp_off2,\n-    return_off,\n-    return_off2,\n-    framesize \/\/ inclusive of return address\n-  };\n-\n-  _framesize = align_up(framesize + ((_shadow_space_bytes + spill_size_in_bytes()) >> LogBytesPerInt), 4);\n-  assert(is_even(_framesize\/2), \"sp not 16-byte aligned\");\n-\n-  _oop_maps  = new OopMapSet();\n-  MacroAssembler* masm = _masm;\n-\n-  address start = __ pc();\n-\n-  __ enter();\n-\n-  \/\/ return address and rbp are already in place\n-  __ subptr(rsp, (_framesize-4) << LogBytesPerInt); \/\/ prolog\n-\n-  _frame_complete = __ pc() - start;\n-\n-  address the_pc = __ pc();\n-\n-  __ set_last_Java_frame(rsp, rbp, (address)the_pc);\n-  OopMap* map = new OopMap(_framesize, 0);\n-  _oop_maps->add_gc_map(the_pc - start, map);\n-\n-  \/\/ State transition\n-  __ movl(Address(r15_thread, JavaThread::thread_state_offset()), _thread_in_native);\n-\n-  __ call(RuntimeAddress(_call_target));\n-\n-  __ restore_cpu_control_state_after_jni();\n-\n-  __ movl(Address(r15_thread, JavaThread::thread_state_offset()), _thread_in_native_trans);\n-\n-  \/\/ Force this write out before the read below\n-  __ membar(Assembler::Membar_mask_bits(\n-          Assembler::LoadLoad | Assembler::LoadStore |\n-          Assembler::StoreLoad | Assembler::StoreStore));\n-\n-  Label L_after_safepoint_poll;\n-  Label L_safepoint_poll_slow_path;\n-\n-  __ safepoint_poll(L_safepoint_poll_slow_path, r15_thread, true \/* at_return *\/, false \/* in_nmethod *\/);\n-  __ cmpl(Address(r15_thread, JavaThread::suspend_flags_offset()), 0);\n-  __ jcc(Assembler::notEqual, L_safepoint_poll_slow_path);\n-\n-  __ bind(L_after_safepoint_poll);\n-\n-  \/\/ change thread state\n-  __ movl(Address(r15_thread, JavaThread::thread_state_offset()), _thread_in_Java);\n-\n-  __ block_comment(\"reguard stack check\");\n-  Label L_reguard;\n-  Label L_after_reguard;\n-  __ cmpl(Address(r15_thread, JavaThread::stack_guard_state_offset()), StackOverflow::stack_guard_yellow_reserved_disabled);\n-  __ jcc(Assembler::equal, L_reguard);\n-  __ bind(L_after_reguard);\n-\n-  __ reset_last_Java_frame(r15_thread, true);\n-\n-  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-  __ ret(0);\n-\n-  \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n-\n-  __ block_comment(\"{ L_safepoint_poll_slow_path\");\n-  __ bind(L_safepoint_poll_slow_path);\n-  __ vzeroupper();\n-\n-  spill_out_registers();\n-\n-  __ mov(c_rarg0, r15_thread);\n-  __ mov(r12, rsp); \/\/ remember sp\n-  __ subptr(rsp, frame::arg_reg_save_area_bytes); \/\/ windows\n-  __ andptr(rsp, -16); \/\/ align stack as required by ABI\n-  __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, JavaThread::check_special_condition_for_native_trans)));\n-  __ mov(rsp, r12); \/\/ restore sp\n-  __ reinit_heapbase();\n-\n-  fill_out_registers();\n-\n-  __ jmp(L_after_safepoint_poll);\n-  __ block_comment(\"} L_safepoint_poll_slow_path\");\n-\n-  \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n-\n-  __ block_comment(\"{ L_reguard\");\n-  __ bind(L_reguard);\n-  __ vzeroupper();\n-\n-  spill_out_registers();\n-\n-  __ mov(r12, rsp); \/\/ remember sp\n-  __ subptr(rsp, frame::arg_reg_save_area_bytes); \/\/ windows\n-  __ andptr(rsp, -16); \/\/ align stack as required by ABI\n-  __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, SharedRuntime::reguard_yellow_pages)));\n-  __ mov(rsp, r12); \/\/ restore sp\n-  __ reinit_heapbase();\n-\n-  fill_out_registers();\n-\n-  __ jmp(L_after_reguard);\n-\n-  __ block_comment(\"} L_reguard\");\n-\n-  \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n-\n-  __ flush();\n-}\n-#endif \/\/ COMPILER2\n-\n@@ -3857,9 +3431,0 @@\n-void SharedRuntime::compute_move_order(const BasicType* in_sig_bt,\n-                                       int total_in_args, const VMRegPair* in_regs,\n-                                       int total_out_args, VMRegPair* out_regs,\n-                                       GrowableArray<int>& arg_order,\n-                                       VMRegPair tmp_vmreg) {\n-  ComputeMoveOrder order(total_in_args, in_regs,\n-                         total_out_args, out_regs,\n-                         in_sig_bt, arg_order, tmp_vmreg);\n-}\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_64.cpp","additions":6,"deletions":441,"binary":false,"changes":447,"status":"modified"},{"patch":"@@ -27,5 +27,7 @@\n-void ProgrammableInvoker::Generator::generate() {\n-  Unimplemented();\n-}\n-\n-address ProgrammableInvoker::generate_adapter(jobject jabi, jobject jlayout) {\n+RuntimeStub* ProgrammableInvoker::make_native_invoker(BasicType* signature,\n+                                                      int num_args,\n+                                                      BasicType ret_bt,\n+                                                      const ABIDescriptor& abi,\n+                                                      const GrowableArray<VMReg>& input_registers,\n+                                                      const GrowableArray<VMReg>& output_registers,\n+                                                      bool needs_return_buffer) {\n","filename":"src\/hotspot\/cpu\/x86\/universalNativeInvoker_x86_32.cpp","additions":7,"deletions":5,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"logging\/logStream.hpp\"\n@@ -28,0 +29,1 @@\n+#include \"prims\/foreign_globals.inline.hpp\"\n@@ -29,0 +31,3 @@\n+#include \"runtime\/sharedRuntime.hpp\"\n+#include \"runtime\/stubCodeGenerator.hpp\"\n+#include \"utilities\/formatBuffer.hpp\"\n@@ -32,2 +37,4 @@\n-void ProgrammableInvoker::Generator::generate() {\n-  __ enter();\n+class NativeInvokerGenerator : public StubCodeGenerator {\n+  BasicType* _signature;\n+  int _num_args;\n+  BasicType _ret_bt;\n@@ -35,4 +42,3 @@\n-  \/\/ Put the context pointer in ebx\/rbx - it's going to be heavily used below both before and after the call\n-  Register ctxt_reg = rbx;\n-  Register used_regs[] = { ctxt_reg, rcx, rsi, rdi };\n-  GrowableArray<Register> preserved_regs;\n+  const ABIDescriptor& _abi;\n+  const GrowableArray<VMReg>& _input_registers;\n+  const GrowableArray<VMReg>& _output_registers;\n@@ -40,5 +46,25 @@\n-  for (size_t i = 0; i < sizeof(used_regs)\/sizeof(Register); i++) {\n-    Register used_reg = used_regs[i];\n-    if (!_abi->is_volatile_reg(used_reg)) {\n-      preserved_regs.push(used_reg);\n-    }\n+  bool _needs_return_buffer;\n+\n+  int _frame_complete;\n+  int _framesize;\n+  OopMapSet* _oop_maps;\n+public:\n+  NativeInvokerGenerator(CodeBuffer* buffer,\n+                         BasicType* signature,\n+                         int num_args,\n+                         BasicType ret_bt,\n+                         const ABIDescriptor& abi,\n+                         const GrowableArray<VMReg>& input_registers,\n+                         const GrowableArray<VMReg>& output_registers,\n+                         bool needs_return_buffer)\n+   : StubCodeGenerator(buffer, PrintMethodHandleStubs),\n+     _signature(signature),\n+     _num_args(num_args),\n+     _ret_bt(ret_bt),\n+     _abi(abi),\n+     _input_registers(input_registers),\n+     _output_registers(output_registers),\n+     _needs_return_buffer(needs_return_buffer),\n+     _frame_complete(0),\n+     _framesize(0),\n+     _oop_maps(NULL) {\n@@ -47,1 +73,1 @@\n-  __ block_comment(\"init_and_alloc_stack\");\n+  void generate();\n@@ -49,2 +75,2 @@\n-  for (int i = 0; i < preserved_regs.length(); i++) {\n-    __ push(preserved_regs.at(i));\n+  int frame_complete() const {\n+    return _frame_complete;\n@@ -53,1 +79,3 @@\n-  __ movptr(ctxt_reg, c_rarg0); \/\/ FIXME c args? or java?\n+  int framesize() const {\n+    return (_framesize >> (LogBytesPerWord - LogBytesPerInt));\n+  }\n@@ -55,4 +83,4 @@\n-  __ block_comment(\"allocate_stack\");\n-  __ movptr(rcx, Address(ctxt_reg, (int) _layout->stack_args_bytes));\n-  __ subptr(rsp, rcx);\n-  __ andptr(rsp, -_abi->_stack_alignment_bytes);\n+  OopMapSet* oop_maps() const {\n+    return _oop_maps;\n+  }\n+};\n@@ -60,1 +88,1 @@\n-  \/\/ Note: rcx is used below!\n+static const int native_invoker_code_size = 1024;\n@@ -62,0 +90,12 @@\n+RuntimeStub* ProgrammableInvoker::make_native_invoker(BasicType* signature,\n+                                                      int num_args,\n+                                                      BasicType ret_bt,\n+                                                      const ABIDescriptor& abi,\n+                                                      const GrowableArray<VMReg>& input_registers,\n+                                                      const GrowableArray<VMReg>& output_registers,\n+                                                      bool needs_return_buffer) {\n+  int locs_size  = 64;\n+  CodeBuffer code(\"nep_invoker_blob\", native_invoker_code_size, locs_size);\n+  NativeInvokerGenerator g(&code, signature, num_args, ret_bt, abi, input_registers, output_registers, needs_return_buffer);\n+  g.generate();\n+  code.log_section_sizes(\"nep_invoker_blob\");\n@@ -63,1 +103,6 @@\n-  __ block_comment(\"load_arguments\");\n+  RuntimeStub* stub =\n+    RuntimeStub::new_runtime_stub(\"nep_invoker_blob\",\n+                                  &code,\n+                                  g.frame_complete(),\n+                                  g.framesize(),\n+                                  g.oop_maps(), false);\n@@ -65,4 +110,8 @@\n-  __ shrptr(rcx, LogBytesPerWord); \/\/ bytes -> words\n-  __ movptr(rsi, Address(ctxt_reg, (int) _layout->stack_args));\n-  __ movptr(rdi, rsp);\n-  __ rep_mov();\n+#ifdef ASSERT\n+  LogTarget(Trace, panama) lt;\n+  if (lt.is_enabled()) {\n+    ResourceMark rm;\n+    LogStream ls(lt);\n+    stub->print_on(&ls);\n+  }\n+#endif\n@@ -70,0 +119,15 @@\n+  return stub;\n+}\n+\n+void NativeInvokerGenerator::generate() {\n+  enum layout {\n+    rbp_off,\n+    rbp_off2,\n+    return_off,\n+    return_off2,\n+    framesize_base \/\/ inclusive of return address\n+    \/\/ The following are also computed dynamically:\n+    \/\/ shadow space\n+    \/\/ spill area\n+    \/\/ out arg area (e.g. for stack args)\n+  };\n@@ -71,5 +135,4 @@\n-  for (int i = 0; i < _abi->_vector_argument_registers.length(); i++) {\n-    \/\/ [1] -> 64 bit -> xmm\n-    \/\/ [2] -> 128 bit -> xmm\n-    \/\/ [4] -> 256 bit -> ymm\n-    \/\/ [8] -> 512 bit -> zmm\n+  Register shufffle_reg = rbx;\n+  JavaCallConv in_conv;\n+  NativeCallConv out_conv(_input_registers);\n+  ArgumentShuffle arg_shuffle(_signature, _num_args, _signature, _num_args, &in_conv, &out_conv, shufffle_reg->as_VMReg());\n@@ -77,3 +140,6 @@\n-    XMMRegister reg = _abi->_vector_argument_registers.at(i);\n-    size_t offs = _layout->arguments_vector + i * xmm_reg_size;\n-    __ movdqu(reg, Address(ctxt_reg, (int)offs));\n+#ifdef ASSERT\n+  LogTarget(Trace, panama) lt;\n+  if (lt.is_enabled()) {\n+    ResourceMark rm;\n+    LogStream ls(lt);\n+    arg_shuffle.print_on(&ls);\n@@ -81,0 +147,1 @@\n+#endif\n@@ -82,3 +149,4 @@\n-  for (int i = 0; i < _abi->_integer_argument_registers.length(); i++) {\n-    size_t offs = _layout->arguments_integer + i * sizeof(uintptr_t);\n-    __ movptr(_abi->_integer_argument_registers.at(i), Address(ctxt_reg, (int)offs));\n+  \/\/ in bytes\n+  int allocated_frame_size = 0;\n+  if (_needs_return_buffer) {\n+    allocated_frame_size += 8; \/\/ store address\n@@ -86,0 +154,2 @@\n+  allocated_frame_size += arg_shuffle.out_arg_stack_slots() << LogBytesPerInt;\n+  allocated_frame_size += _abi._shadow_space_bytes;\n@@ -87,3 +157,4 @@\n-  if (_abi->_shadow_space_bytes != 0) {\n-    __ block_comment(\"allocate shadow space for argument register spill\");\n-    __ subptr(rsp, _abi->_shadow_space_bytes);\n+  int ret_buf_addr_rsp_offset = -1;\n+  if (_needs_return_buffer) {\n+    \/\/ the above\n+    ret_buf_addr_rsp_offset = allocated_frame_size - 8;\n@@ -92,3 +163,4 @@\n-  \/\/ call target function\n-  __ block_comment(\"call target function\");\n-  __ call(Address(ctxt_reg, (int) _layout->arguments_next_pc));\n+  \/\/ when we don't use a return buffer we need to spill the return value around our slowpath calls\n+  \/\/ when we use a return buffer case this SHOULD be unused.\n+  RegSpiller out_reg_spiller(_output_registers);\n+  int spill_rsp_offset = -1;\n@@ -96,3 +168,6 @@\n-  if (_abi->_shadow_space_bytes != 0) {\n-    __ block_comment(\"pop shadow space\");\n-    __ addptr(rsp, _abi->_shadow_space_bytes);\n+  if (!_needs_return_buffer) {\n+    spill_rsp_offset = 0;\n+    \/\/ spill area can be shared with the above, so we take the max of the 2\n+    allocated_frame_size = out_reg_spiller.spill_size_bytes() > allocated_frame_size\n+      ? out_reg_spiller.spill_size_bytes()\n+      : allocated_frame_size;\n@@ -100,0 +175,9 @@\n+  allocated_frame_size = align_up(allocated_frame_size, 16);\n+  \/\/ _framesize is in 32-bit stack slots:\n+  _framesize += framesize_base + (allocated_frame_size >> LogBytesPerInt);\n+  assert(is_even(_framesize\/2), \"sp not 16-byte aligned\");\n+\n+  _oop_maps  = new OopMapSet();\n+  address start = __ pc();\n+\n+  __ enter();\n@@ -101,4 +185,61 @@\n-  __ block_comment(\"store_registers\");\n-  for (int i = 0; i < _abi->_integer_return_registers.length(); i++) {\n-    ssize_t offs = _layout->returns_integer + i * sizeof(uintptr_t);\n-    __ movptr(Address(ctxt_reg, offs), _abi->_integer_return_registers.at(i));\n+  \/\/ return address and rbp are already in place\n+  __ subptr(rsp, allocated_frame_size); \/\/ prolog\n+\n+  _frame_complete = __ pc() - start;\n+\n+  address the_pc = __ pc();\n+\n+  __ block_comment(\"{ thread java2native\");\n+  __ set_last_Java_frame(rsp, rbp, (address)the_pc);\n+  OopMap* map = new OopMap(_framesize, 0);\n+  _oop_maps->add_gc_map(the_pc - start, map);\n+\n+  \/\/ State transition\n+  __ movl(Address(r15_thread, JavaThread::thread_state_offset()), _thread_in_native);\n+  __ block_comment(\"} thread java2native\");\n+\n+  __ block_comment(\"{ argument shuffle\");\n+  arg_shuffle.generate(_masm, shufffle_reg->as_VMReg(), 0, _abi._shadow_space_bytes);\n+  if (_needs_return_buffer) {\n+    \/\/ spill our return buffer address\n+    assert(ret_buf_addr_rsp_offset != -1, \"no return buffer addr spill\");\n+    __ movptr(Address(rsp, ret_buf_addr_rsp_offset), _abi._ret_buf_addr_reg);\n+  }\n+  __ block_comment(\"} argument shuffle\");\n+\n+  __ call(_abi._target_addr_reg);\n+  \/\/ this call is assumed not to have killed r15_thread\n+\n+  if (!_needs_return_buffer) {\n+    \/\/ FIXME: this assumes we return in rax\/xmm0, which might not be the case\n+    \/\/ Unpack native results.\n+    switch (_ret_bt) {\n+      case T_BOOLEAN: __ c2bool(rax);            break;\n+      case T_CHAR   : __ movzwl(rax, rax);       break;\n+      case T_BYTE   : __ sign_extend_byte (rax); break;\n+      case T_SHORT  : __ sign_extend_short(rax); break;\n+      case T_INT    : \/* nothing to do *\/        break;\n+      case T_DOUBLE :\n+      case T_FLOAT  :\n+        \/\/ Result is in xmm0 we'll save as needed\n+        break;\n+      case T_VOID: break;\n+      case T_LONG: break;\n+      default       : ShouldNotReachHere();\n+    }\n+  } else {\n+    assert(ret_buf_addr_rsp_offset != -1, \"no return buffer addr spill\");\n+    __ movptr(rscratch1, Address(rsp, ret_buf_addr_rsp_offset));\n+    int offset = 0;\n+    for (int i = 0; i < _output_registers.length(); i++) {\n+      VMReg reg = _output_registers.at(i);\n+      if (reg->is_Register()) {\n+        __ movptr(Address(rscratch1, offset), reg->as_Register());\n+        offset += 8;\n+      } else if (reg->is_XMMRegister()) {\n+        __ movdqu(Address(rscratch1, offset), reg->as_XMMRegister());\n+        offset += 16;\n+      } else {\n+        ShouldNotReachHere();\n+      }\n+    }\n@@ -107,5 +248,2 @@\n-  for (int i = 0; i < _abi->_vector_return_registers.length(); i++) {\n-    \/\/ [1] -> 64 bit -> xmm\n-    \/\/ [2] -> 128 bit -> xmm (SSE)\n-    \/\/ [4] -> 256 bit -> ymm (AVX)\n-    \/\/ [8] -> 512 bit -> zmm (AVX-512, aka AVX3)\n+  __ block_comment(\"{ thread native2java\");\n+  __ restore_cpu_control_state_after_jni();\n@@ -113,3 +251,40 @@\n-    XMMRegister reg = _abi->_vector_return_registers.at(i);\n-    size_t offs = _layout->returns_vector + i * xmm_reg_size;\n-    __ movdqu(Address(ctxt_reg, (int)offs), reg);\n+  __ movl(Address(r15_thread, JavaThread::thread_state_offset()), _thread_in_native_trans);\n+\n+  \/\/ Force this write out before the read below\n+  __ membar(Assembler::Membar_mask_bits(\n+          Assembler::LoadLoad | Assembler::LoadStore |\n+          Assembler::StoreLoad | Assembler::StoreStore));\n+\n+  Label L_after_safepoint_poll;\n+  Label L_safepoint_poll_slow_path;\n+\n+  __ safepoint_poll(L_safepoint_poll_slow_path, r15_thread, true \/* at_return *\/, false \/* in_nmethod *\/);\n+  __ cmpl(Address(r15_thread, JavaThread::suspend_flags_offset()), 0);\n+  __ jcc(Assembler::notEqual, L_safepoint_poll_slow_path);\n+\n+  __ bind(L_after_safepoint_poll);\n+\n+  \/\/ change thread state\n+  __ movl(Address(r15_thread, JavaThread::thread_state_offset()), _thread_in_Java);\n+\n+  __ block_comment(\"reguard stack check\");\n+  Label L_reguard;\n+  Label L_after_reguard;\n+  __ cmpl(Address(r15_thread, JavaThread::stack_guard_state_offset()), StackOverflow::stack_guard_yellow_reserved_disabled);\n+  __ jcc(Assembler::equal, L_reguard);\n+  __ bind(L_after_reguard);\n+\n+  __ reset_last_Java_frame(r15_thread, true);\n+  __ block_comment(\"} thread native2java\");\n+\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n+\n+  \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+  __ block_comment(\"{ L_safepoint_poll_slow_path\");\n+  __ bind(L_safepoint_poll_slow_path);\n+  __ vzeroupper();\n+\n+  if(!_needs_return_buffer) {\n+    out_reg_spiller.generate_spill(_masm, spill_rsp_offset);\n@@ -118,3 +293,10 @@\n-  for (size_t i = 0; i < _abi->_X87_return_registers_noof; i++) {\n-    size_t offs = _layout->returns_x87 + i * (sizeof(long double));\n-    __ fstp_x(Address(ctxt_reg, (int)offs)); \/\/pop ST(0)\n+  __ mov(c_rarg0, r15_thread);\n+  __ mov(r12, rsp); \/\/ remember sp\n+  __ subptr(rsp, frame::arg_reg_save_area_bytes); \/\/ windows\n+  __ andptr(rsp, -16); \/\/ align stack as required by ABI\n+  __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, JavaThread::check_special_condition_for_native_trans)));\n+  __ mov(rsp, r12); \/\/ restore sp\n+  __ reinit_heapbase();\n+\n+  if(!_needs_return_buffer) {\n+    out_reg_spiller.generate_fill(_masm, spill_rsp_offset);\n@@ -123,3 +305,11 @@\n-  \/\/ Restore backed up preserved register\n-  for (int i = 0; i < preserved_regs.length(); i++) {\n-    __ movptr(preserved_regs.at(i), Address(rbp, -(int)(sizeof(uintptr_t) * (i + 1))));\n+  __ jmp(L_after_safepoint_poll);\n+  __ block_comment(\"} L_safepoint_poll_slow_path\");\n+\n+  \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+  __ block_comment(\"{ L_reguard\");\n+  __ bind(L_reguard);\n+  __ vzeroupper();\n+\n+  if(!_needs_return_buffer) {\n+    out_reg_spiller.generate_spill(_masm, spill_rsp_offset);\n@@ -128,2 +318,6 @@\n-  __ leave();\n-  __ ret(0);\n+  __ mov(r12, rsp); \/\/ remember sp\n+  __ subptr(rsp, frame::arg_reg_save_area_bytes); \/\/ windows\n+  __ andptr(rsp, -16); \/\/ align stack as required by ABI\n+  __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, SharedRuntime::reguard_yellow_pages)));\n+  __ mov(rsp, r12); \/\/ restore sp\n+  __ reinit_heapbase();\n@@ -131,2 +325,3 @@\n-  __ flush();\n-}\n+  if(!_needs_return_buffer) {\n+    out_reg_spiller.generate_fill(_masm, spill_rsp_offset);\n+  }\n@@ -134,4 +329,1 @@\n-address ProgrammableInvoker::generate_adapter(jobject jabi, jobject jlayout) {\n-  ResourceMark rm;\n-  const ABIDescriptor abi = ForeignGlobals::parse_abi_descriptor(jabi);\n-  const BufferLayout layout = ForeignGlobals::parse_buffer_layout(jlayout);\n+  __ jmp(L_after_reguard);\n@@ -139,1 +331,1 @@\n-  BufferBlob* _invoke_native_blob = BufferBlob::create(\"invoke_native_blob\", native_invoker_size);\n+  __ block_comment(\"} L_reguard\");\n@@ -141,4 +333,1 @@\n-  CodeBuffer code2(_invoke_native_blob);\n-  ProgrammableInvoker::Generator g2(&code2, &abi, &layout);\n-  g2.generate();\n-  code2.log_section_sizes(\"InvokeNativeBlob\");\n+  \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n@@ -146,1 +335,1 @@\n-  return _invoke_native_blob->code_begin();\n+  __ flush();\n","filename":"src\/hotspot\/cpu\/x86\/universalNativeInvoker_x86_64.cpp","additions":266,"deletions":77,"binary":false,"changes":343,"status":"modified"},{"patch":"@@ -27,6 +27,6 @@\n-address ProgrammableUpcallHandler::generate_upcall_stub(jobject rec, jobject jabi, jobject jlayout) {\n-  Unimplemented();\n-  return nullptr;\n-}\n-\n-address ProgrammableUpcallHandler::generate_optimized_upcall_stub(jobject mh, Method* entry, jobject jabi, jobject jconv) {\n+address ProgrammableUpcallHandler::generate_optimized_upcall_stub(jobject receiver, Method* entry,\n+                                                                  BasicType* in_sig_bt, int total_in_args,\n+                                                                  BasicType* out_sig_bt, int total_out_args,\n+                                                                  BasicType ret_type,\n+                                                                  jobject jabi, jobject jconv,\n+                                                                  bool needs_return_buffer, int ret_buf_size) {\n@@ -36,4 +36,0 @@\n-\n-bool ProgrammableUpcallHandler::supports_optimized_upcalls() {\n-  return false;\n-}\n","filename":"src\/hotspot\/cpu\/x86\/universalUpcallHandler_x86_32.cpp","additions":6,"deletions":10,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"prims\/foreign_globals.inline.hpp\"\n@@ -41,314 +42,0 @@\n-\/\/ 1. Create buffer according to layout\n-\/\/ 2. Load registers & stack args into buffer\n-\/\/ 3. Call upcall helper with upcall handler instance & buffer pointer (C++ ABI)\n-\/\/ 4. Load return value from buffer into foreign ABI registers\n-\/\/ 5. Return\n-address ProgrammableUpcallHandler::generate_upcall_stub(jobject rec, jobject jabi, jobject jlayout) {\n-  ResourceMark rm;\n-  const ABIDescriptor abi = ForeignGlobals::parse_abi_descriptor(jabi);\n-  const BufferLayout layout = ForeignGlobals::parse_buffer_layout(jlayout);\n-\n-  CodeBuffer buffer(\"upcall_stub\", 1024, upcall_stub_size);\n-\n-  MacroAssembler* _masm = new MacroAssembler(&buffer);\n-  int stack_alignment_C = 16; \/\/ bytes\n-  int register_size = sizeof(uintptr_t);\n-  int buffer_alignment = xmm_reg_size;\n-\n-  \/\/ stub code\n-  __ enter();\n-\n-  \/\/ save pointer to JNI receiver handle into constant segment\n-  Address rec_adr = __ as_Address(InternalAddress(__ address_constant((address)rec)));\n-\n-  __ subptr(rsp, (int) align_up(layout.buffer_size, buffer_alignment));\n-\n-  Register used[] = { c_rarg0, c_rarg1, rax, rbx, rdi, rsi, r12, r13, r14, r15 };\n-  GrowableArray<Register> preserved;\n-  \/\/ TODO need to preserve anything killed by the upcall that is non-volatile, needs XMM regs as well, probably\n-  for (size_t i = 0; i < sizeof(used)\/sizeof(Register); i++) {\n-    Register reg = used[i];\n-    if (!abi.is_volatile_reg(reg)) {\n-      preserved.push(reg);\n-    }\n-  }\n-\n-  int preserved_size = align_up(preserved.length() * register_size, stack_alignment_C); \/\/ includes register alignment\n-  int buffer_offset = preserved_size; \/\/ offset from rsp\n-\n-  __ subptr(rsp, preserved_size);\n-  for (int i = 0; i < preserved.length(); i++) {\n-    __ movptr(Address(rsp, i * register_size), preserved.at(i));\n-  }\n-\n-  for (int i = 0; i < abi._integer_argument_registers.length(); i++) {\n-    size_t offs = buffer_offset + layout.arguments_integer + i * sizeof(uintptr_t);\n-    __ movptr(Address(rsp, (int)offs), abi._integer_argument_registers.at(i));\n-  }\n-\n-  for (int i = 0; i < abi._vector_argument_registers.length(); i++) {\n-    XMMRegister reg = abi._vector_argument_registers.at(i);\n-    size_t offs = buffer_offset + layout.arguments_vector + i * xmm_reg_size;\n-    __ movdqu(Address(rsp, (int)offs), reg);\n-  }\n-\n-  \/\/ Capture prev stack pointer (stack arguments base)\n-#ifndef _WIN64\n-  __ lea(rax, Address(rbp, 16)); \/\/ skip frame+return address\n-#else\n-  __ lea(rax, Address(rbp, 16 + 32)); \/\/ also skip shadow space\n-#endif\n-  __ movptr(Address(rsp, buffer_offset + (int) layout.stack_args), rax);\n-#ifndef PRODUCT\n-  __ movptr(Address(rsp, buffer_offset + (int) layout.stack_args_bytes), -1); \/\/ unknown\n-#endif\n-\n-  \/\/ Call upcall helper\n-\n-  __ movptr(c_rarg0, rec_adr);\n-  __ lea(c_rarg1, Address(rsp, buffer_offset));\n-\n-#ifdef _WIN64\n-  __ block_comment(\"allocate shadow space for argument register spill\");\n-  __ subptr(rsp, 32);\n-#endif\n-\n-  __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, ProgrammableUpcallHandler::attach_thread_and_do_upcall)));\n-\n-#ifdef _WIN64\n-  __ block_comment(\"pop shadow space\");\n-  __ addptr(rsp, 32);\n-#endif\n-\n-  for (int i = 0; i < abi._integer_return_registers.length(); i++) {\n-    size_t offs = buffer_offset + layout.returns_integer + i * sizeof(uintptr_t);\n-    __ movptr(abi._integer_return_registers.at(i), Address(rsp, (int)offs));\n-  }\n-\n-  for (int i = 0; i < abi._vector_return_registers.length(); i++) {\n-    XMMRegister reg = abi._vector_return_registers.at(i);\n-    size_t offs = buffer_offset + layout.returns_vector + i * xmm_reg_size;\n-    __ movdqu(reg, Address(rsp, (int)offs));\n-  }\n-\n-  for (size_t i = abi._X87_return_registers_noof; i > 0 ; i--) {\n-      ssize_t offs = buffer_offset + layout.returns_x87 + (i - 1) * (sizeof(long double));\n-      __ fld_x (Address(rsp, (int)offs));\n-  }\n-\n-  \/\/ Restore preserved registers\n-  for (int i = 0; i < preserved.length(); i++) {\n-    __ movptr(preserved.at(i), Address(rsp, i * register_size));\n-  }\n-\n-  __ leave();\n-  __ ret(0);\n-\n-  _masm->flush();\n-\n-  BufferBlob* blob = BufferBlob::create(\"upcall_stub\", &buffer);\n-\n-  return blob->code_begin();\n-}\n-\n-struct ArgMove {\n-  BasicType bt;\n-  VMRegPair from;\n-  VMRegPair to;\n-\n-  bool is_identity() const {\n-      return from.first() == to.first() && from.second() == to.second();\n-  }\n-};\n-\n-static GrowableArray<ArgMove> compute_argument_shuffle(Method* entry, int& out_arg_size_bytes, const CallRegs& conv, BasicType& ret_type) {\n-  assert(entry->is_static(), \"\");\n-\n-  \/\/ Fill in the signature array, for the calling-convention call.\n-  const int total_out_args = entry->size_of_parameters();\n-  assert(total_out_args > 0, \"receiver arg \");\n-\n-  BasicType* out_sig_bt = NEW_RESOURCE_ARRAY(BasicType, total_out_args);\n-  VMRegPair* out_regs = NEW_RESOURCE_ARRAY(VMRegPair, total_out_args);\n-\n-  {\n-    int i = 0;\n-    SignatureStream ss(entry->signature());\n-    for (; !ss.at_return_type(); ss.next()) {\n-      out_sig_bt[i++] = ss.type();  \/\/ Collect remaining bits of signature\n-      if (ss.type() == T_LONG || ss.type() == T_DOUBLE)\n-        out_sig_bt[i++] = T_VOID;   \/\/ Longs & doubles take 2 Java slots\n-    }\n-    assert(i == total_out_args, \"\");\n-    ret_type = ss.type();\n-  }\n-\n-  int out_arg_slots = SharedRuntime::java_calling_convention(out_sig_bt, out_regs, total_out_args);\n-\n-  const int total_in_args = total_out_args - 1; \/\/ skip receiver\n-  BasicType* in_sig_bt  = NEW_RESOURCE_ARRAY(BasicType, total_in_args);\n-  VMRegPair* in_regs    = NEW_RESOURCE_ARRAY(VMRegPair, total_in_args);\n-\n-  for (int i = 0; i < total_in_args ; i++ ) {\n-    in_sig_bt[i] = out_sig_bt[i+1]; \/\/ skip receiver\n-  }\n-\n-  \/\/ Now figure out where the args must be stored and how much stack space they require.\n-  conv.calling_convention(in_sig_bt, in_regs, total_in_args);\n-\n-  GrowableArray<int> arg_order(2 * total_in_args);\n-\n-  VMRegPair tmp_vmreg;\n-  tmp_vmreg.set2(rbx->as_VMReg());\n-\n-  \/\/ Compute a valid move order, using tmp_vmreg to break any cycles\n-  SharedRuntime::compute_move_order(in_sig_bt,\n-                                    total_in_args, in_regs,\n-                                    total_out_args, out_regs,\n-                                    arg_order,\n-                                    tmp_vmreg);\n-\n-  GrowableArray<ArgMove> arg_order_vmreg(total_in_args); \/\/ conservative\n-\n-#ifdef ASSERT\n-  bool reg_destroyed[RegisterImpl::number_of_registers];\n-  bool freg_destroyed[XMMRegisterImpl::number_of_registers];\n-  for ( int r = 0 ; r < RegisterImpl::number_of_registers ; r++ ) {\n-    reg_destroyed[r] = false;\n-  }\n-  for ( int f = 0 ; f < XMMRegisterImpl::number_of_registers ; f++ ) {\n-    freg_destroyed[f] = false;\n-  }\n-#endif \/\/ ASSERT\n-\n-  for (int i = 0; i < arg_order.length(); i += 2) {\n-    int in_arg  = arg_order.at(i);\n-    int out_arg = arg_order.at(i + 1);\n-\n-    assert(in_arg != -1 || out_arg != -1, \"\");\n-    BasicType arg_bt = (in_arg != -1 ? in_sig_bt[in_arg] : out_sig_bt[out_arg]);\n-    switch (arg_bt) {\n-      case T_BOOLEAN:\n-      case T_BYTE:\n-      case T_SHORT:\n-      case T_CHAR:\n-      case T_INT:\n-      case T_FLOAT:\n-        break; \/\/ process\n-\n-      case T_LONG:\n-      case T_DOUBLE:\n-        assert(in_arg  == -1 || (in_arg  + 1 < total_in_args  &&  in_sig_bt[in_arg  + 1] == T_VOID), \"bad arg list: %d\", in_arg);\n-        assert(out_arg == -1 || (out_arg + 1 < total_out_args && out_sig_bt[out_arg + 1] == T_VOID), \"bad arg list: %d\", out_arg);\n-        break; \/\/ process\n-\n-      case T_VOID:\n-        continue; \/\/ skip\n-\n-      default:\n-        fatal(\"found in upcall args: %s\", type2name(arg_bt));\n-    }\n-\n-    ArgMove move;\n-    move.bt   = arg_bt;\n-    move.from = (in_arg != -1 ? in_regs[in_arg] : tmp_vmreg);\n-    move.to   = (out_arg != -1 ? out_regs[out_arg] : tmp_vmreg);\n-\n-    if(move.is_identity()) {\n-      continue; \/\/ useless move\n-    }\n-\n-#ifdef ASSERT\n-    if (in_arg != -1) {\n-      if (in_regs[in_arg].first()->is_Register()) {\n-        assert(!reg_destroyed[in_regs[in_arg].first()->as_Register()->encoding()], \"destroyed reg!\");\n-      } else if (in_regs[in_arg].first()->is_XMMRegister()) {\n-        assert(!freg_destroyed[in_regs[in_arg].first()->as_XMMRegister()->encoding()], \"destroyed reg!\");\n-      }\n-    }\n-    if (out_arg != -1) {\n-      if (out_regs[out_arg].first()->is_Register()) {\n-        reg_destroyed[out_regs[out_arg].first()->as_Register()->encoding()] = true;\n-      } else if (out_regs[out_arg].first()->is_XMMRegister()) {\n-        freg_destroyed[out_regs[out_arg].first()->as_XMMRegister()->encoding()] = true;\n-      }\n-    }\n-#endif \/* ASSERT *\/\n-\n-    arg_order_vmreg.push(move);\n-  }\n-\n-  int stack_slots = SharedRuntime::out_preserve_stack_slots() + out_arg_slots;\n-  out_arg_size_bytes = align_up(stack_slots * VMRegImpl::stack_slot_size, StackAlignmentInBytes);\n-\n-  return arg_order_vmreg;\n-}\n-\n-static const char* null_safe_string(const char* str) {\n-  return str == nullptr ? \"NULL\" : str;\n-}\n-\n-#ifdef ASSERT\n-static void print_arg_moves(const GrowableArray<ArgMove>& arg_moves, Method* entry) {\n-  LogTarget(Trace, foreign) lt;\n-  if (lt.is_enabled()) {\n-    ResourceMark rm;\n-    LogStream ls(lt);\n-    ls.print_cr(\"Argument shuffle for %s {\", entry->name_and_sig_as_C_string());\n-    for (int i = 0; i < arg_moves.length(); i++) {\n-      ArgMove arg_mv = arg_moves.at(i);\n-      BasicType arg_bt     = arg_mv.bt;\n-      VMRegPair from_vmreg = arg_mv.from;\n-      VMRegPair to_vmreg   = arg_mv.to;\n-\n-      ls.print(\"Move a %s from (\", null_safe_string(type2name(arg_bt)));\n-      from_vmreg.first()->print_on(&ls);\n-      ls.print(\",\");\n-      from_vmreg.second()->print_on(&ls);\n-      ls.print(\") to \");\n-      to_vmreg.first()->print_on(&ls);\n-      ls.print(\",\");\n-      to_vmreg.second()->print_on(&ls);\n-      ls.print_cr(\")\");\n-    }\n-    ls.print_cr(\"}\");\n-  }\n-}\n-#endif\n-\n-static void save_native_arguments(MacroAssembler* _masm, const CallRegs& conv, int arg_save_area_offset) {\n-  __ block_comment(\"{ save_native_args \");\n-  int store_offset = arg_save_area_offset;\n-  for (int i = 0; i < conv._args_length; i++) {\n-    VMReg reg = conv._arg_regs[i];\n-    if (reg->is_Register()) {\n-      __ movptr(Address(rsp, store_offset), reg->as_Register());\n-      store_offset += 8;\n-    } else if (reg->is_XMMRegister()) {\n-      \/\/ Java API doesn't support vector args\n-      __ movdqu(Address(rsp, store_offset), reg->as_XMMRegister());\n-      store_offset += 16;\n-    }\n-    \/\/ do nothing for stack\n-  }\n-  __ block_comment(\"} save_native_args \");\n-}\n-\n-static void restore_native_arguments(MacroAssembler* _masm, const CallRegs& conv, int arg_save_area_offset) {\n-  __ block_comment(\"{ restore_native_args \");\n-  int load_offset = arg_save_area_offset;\n-  for (int i = 0; i < conv._args_length; i++) {\n-    VMReg reg = conv._arg_regs[i];\n-    if (reg->is_Register()) {\n-      __ movptr(reg->as_Register(), Address(rsp, load_offset));\n-      load_offset += 8;\n-    } else if (reg->is_XMMRegister()) {\n-      \/\/ Java API doesn't support vector args\n-      __ movdqu(reg->as_XMMRegister(), Address(rsp, load_offset));\n-      load_offset += 16;\n-    }\n-    \/\/ do nothing for stack\n-  }\n-  __ block_comment(\"} restore_native_args \");\n-}\n-\n@@ -389,69 +76,0 @@\n-static int compute_arg_save_area_size(const CallRegs& conv) {\n-  int result_size = 0;\n-  for (int i = 0; i < conv._args_length; i++) {\n-    VMReg reg = conv._arg_regs[i];\n-    if (reg->is_Register()) {\n-      result_size += 8;\n-    } else if (reg->is_XMMRegister()) {\n-      \/\/ Java API doesn't support vector args\n-      result_size += 16;\n-    }\n-    \/\/ do nothing for stack\n-  }\n-  return result_size;\n-}\n-\n-static int compute_res_save_area_size(const CallRegs& conv) {\n-  int result_size = 0;\n-  for (int i = 0; i < conv._rets_length; i++) {\n-    VMReg reg = conv._ret_regs[i];\n-    if (reg->is_Register()) {\n-      result_size += 8;\n-    } else if (reg->is_XMMRegister()) {\n-      \/\/ Java API doesn't support vector args\n-      result_size += 16;\n-    } else {\n-      ShouldNotReachHere(); \/\/ unhandled type\n-    }\n-  }\n-  return result_size;\n-}\n-\n-static void save_java_result(MacroAssembler* _masm, const CallRegs& conv, int res_save_area_offset) {\n-  int offset = res_save_area_offset;\n-  __ block_comment(\"{ save java result \");\n-  for (int i = 0; i < conv._rets_length; i++) {\n-    VMReg reg = conv._ret_regs[i];\n-    if (reg->is_Register()) {\n-      __ movptr(Address(rsp, offset), reg->as_Register());\n-      offset += 8;\n-    } else if (reg->is_XMMRegister()) {\n-      \/\/ Java API doesn't support vector args\n-      __ movdqu(Address(rsp, offset), reg->as_XMMRegister());\n-      offset += 16;\n-    } else {\n-      ShouldNotReachHere(); \/\/ unhandled type\n-    }\n-  }\n-  __ block_comment(\"} save java result \");\n-}\n-\n-static void restore_java_result(MacroAssembler* _masm, const CallRegs& conv, int res_save_area_offset) {\n-  int offset = res_save_area_offset;\n-  __ block_comment(\"{ restore java result \");\n-  for (int i = 0; i < conv._rets_length; i++) {\n-    VMReg reg = conv._ret_regs[i];\n-    if (reg->is_Register()) {\n-      __ movptr(reg->as_Register(), Address(rsp, offset));\n-      offset += 8;\n-    } else if (reg->is_XMMRegister()) {\n-      \/\/ Java API doesn't support vector args\n-      __ movdqu(reg->as_XMMRegister(), Address(rsp, offset));\n-      offset += 16;\n-    } else {\n-      ShouldNotReachHere(); \/\/ unhandled type\n-    }\n-  }\n-  __ block_comment(\"} restore java result \");\n-}\n-\n@@ -549,41 +167,0 @@\n-\n-static void shuffle_arguments(MacroAssembler* _masm, const GrowableArray<ArgMove>& arg_moves) {\n-  for (int i = 0; i < arg_moves.length(); i++) {\n-    ArgMove arg_mv = arg_moves.at(i);\n-    BasicType arg_bt     = arg_mv.bt;\n-    VMRegPair from_vmreg = arg_mv.from;\n-    VMRegPair to_vmreg   = arg_mv.to;\n-\n-    assert(\n-      !((from_vmreg.first()->is_Register() && to_vmreg.first()->is_XMMRegister())\n-      || (from_vmreg.first()->is_XMMRegister() && to_vmreg.first()->is_Register())),\n-       \"move between gp and fp reg not supported\");\n-\n-    __ block_comment(err_msg(\"bt=%s\", null_safe_string(type2name(arg_bt))));\n-    switch (arg_bt) {\n-      case T_BOOLEAN:\n-      case T_BYTE:\n-      case T_SHORT:\n-      case T_CHAR:\n-      case T_INT:\n-       __ move32_64(from_vmreg, to_vmreg);\n-       break;\n-\n-      case T_FLOAT:\n-        __ float_move(from_vmreg, to_vmreg);\n-        break;\n-\n-      case T_DOUBLE:\n-        __ double_move(from_vmreg, to_vmreg);\n-        break;\n-\n-      case T_LONG :\n-        __ long_move(from_vmreg, to_vmreg);\n-        break;\n-\n-      default:\n-        fatal(\"found in upcall args: %s\", type2name(arg_bt));\n-    }\n-  }\n-}\n-\n@@ -594,2 +171,6 @@\n-address ProgrammableUpcallHandler::generate_optimized_upcall_stub(jobject receiver, Method* entry, jobject jabi, jobject jconv) {\n-  ResourceMark rm;\n+address ProgrammableUpcallHandler::generate_optimized_upcall_stub(jobject receiver, Method* entry,\n+                                                                  BasicType* in_sig_bt, int total_in_args,\n+                                                                  BasicType* out_sig_bt, int total_out_args,\n+                                                                  BasicType ret_type,\n+                                                                  jobject jabi, jobject jconv,\n+                                                                  bool needs_return_buffer, int ret_buf_size) {\n@@ -597,2 +178,1 @@\n-  const CallRegs conv = ForeignGlobals::parse_call_regs(jconv);\n-  assert(conv._rets_length <= 1, \"no multi reg returns\");\n+  const CallRegs call_regs = ForeignGlobals::parse_call_regs(jconv);\n@@ -601,2 +181,6 @@\n-  int register_size = sizeof(uintptr_t);\n-  int buffer_alignment = xmm_reg_size;\n+  Register shuffle_reg = rbx;\n+  JavaCallConv out_conv;\n+  NativeCallConv in_conv(call_regs._arg_regs, call_regs._args_length);\n+  ArgumentShuffle arg_shuffle(in_sig_bt, total_in_args, out_sig_bt, total_out_args, &in_conv, &out_conv, shuffle_reg->as_VMReg());\n+  int stack_slots = SharedRuntime::out_preserve_stack_slots() + arg_shuffle.out_arg_stack_slots();\n+  int out_arg_area = align_up(stack_slots * VMRegImpl::stack_slot_size, StackAlignmentInBytes);\n@@ -604,5 +188,8 @@\n-  int out_arg_area = -1;\n-  BasicType ret_type;\n-  GrowableArray<ArgMove> arg_moves = compute_argument_shuffle(entry, out_arg_area, conv, ret_type);\n-  assert(out_arg_area != -1, \"Should have been set\");\n-  DEBUG_ONLY(print_arg_moves(arg_moves, entry);)\n+#ifdef ASSERT\n+  LogTarget(Trace, panama) lt;\n+  if (lt.is_enabled()) {\n+    ResourceMark rm;\n+    LogStream ls(lt);\n+    arg_shuffle.print_on(&ls);\n+  }\n+#endif\n@@ -617,2 +204,2 @@\n-  int arg_save_area_size = compute_arg_save_area_size(conv);\n-  int res_save_area_size = compute_res_save_area_size(conv);\n+  RegSpiller arg_spilller(call_regs._arg_regs, call_regs._args_length);\n+  RegSpiller result_spiller(call_regs._ret_regs, call_regs._rets_length);\n@@ -622,2 +209,2 @@\n-  int arg_save_area_offset   = res_save_area_offset   + res_save_area_size;\n-  int reg_save_area_offset   = arg_save_area_offset   + arg_save_area_size;\n+  int arg_save_area_offset   = res_save_area_offset   + result_spiller.spill_size_bytes();\n+  int reg_save_area_offset   = arg_save_area_offset   + arg_spilller.spill_size_bytes();\n@@ -627,0 +214,6 @@\n+  int ret_buf_offset = -1;\n+  if (needs_return_buffer) {\n+    ret_buf_offset = frame_bottom_offset;\n+    frame_bottom_offset += ret_buf_size;\n+  }\n+\n@@ -635,0 +228,3 @@\n+  \/\/      | (optional)          |\n+  \/\/      | ret_buf             |\n+  \/\/      |---------------------| = ret_buf_offset\n@@ -666,1 +262,1 @@\n-  save_native_arguments(_masm, conv, arg_save_area_offset);\n+  arg_spilller.generate_spill(_masm, arg_save_area_offset);\n@@ -680,3 +276,6 @@\n-  \/\/ TODO merge these somehow\n-  restore_native_arguments(_masm, conv, arg_save_area_offset);\n-  shuffle_arguments(_masm, arg_moves);\n+  arg_spilller.generate_fill(_masm, arg_save_area_offset);\n+  if (needs_return_buffer) {\n+    assert(ret_buf_offset != -1, \"no return buffer allocated\");\n+    __ lea(abi._ret_buf_addr_reg, Address(rsp, ret_buf_offset));\n+  }\n+  arg_shuffle.generate(_masm, shuffle_reg->as_VMReg(), abi._shadow_space_bytes, 0);\n@@ -696,1 +295,46 @@\n-  save_java_result(_masm, conv, res_save_area_offset);\n+  \/\/ return value shuffle\n+  if (!needs_return_buffer) {\n+#ifdef ASSERT\n+    if (call_regs._rets_length == 1) { \/\/ 0 or 1\n+      VMReg j_expected_result_reg;\n+      switch (ret_type) {\n+        case T_BOOLEAN:\n+        case T_BYTE:\n+        case T_SHORT:\n+        case T_CHAR:\n+        case T_INT:\n+        case T_LONG:\n+        j_expected_result_reg = rax->as_VMReg();\n+        break;\n+        case T_FLOAT:\n+        case T_DOUBLE:\n+          j_expected_result_reg = xmm0->as_VMReg();\n+          break;\n+        default:\n+          fatal(\"unexpected return type: %s\", type2name(ret_type));\n+      }\n+      \/\/ No need to move for now, since CallArranger can pick a return type\n+      \/\/ that goes in the same reg for both CCs. But, at least assert they are the same\n+      assert(call_regs._ret_regs[0] == j_expected_result_reg,\n+      \"unexpected result register: %s != %s\", call_regs._ret_regs[0]->name(), j_expected_result_reg->name());\n+    }\n+#endif\n+  } else {\n+    assert(ret_buf_offset != -1, \"no return buffer allocated\");\n+    __ lea(rscratch1, Address(rsp, ret_buf_offset));\n+    int offset = 0;\n+    for (int i = 0; i < call_regs._rets_length; i++) {\n+      VMReg reg = call_regs._ret_regs[i];\n+      if (reg->is_Register()) {\n+        __ movptr(reg->as_Register(), Address(rscratch1, offset));\n+        offset += 8;\n+      } else if (reg->is_XMMRegister()) {\n+        __ movdqu(reg->as_XMMRegister(), Address(rscratch1, offset));\n+        offset += 16;\n+      } else {\n+        ShouldNotReachHere();\n+      }\n+    }\n+  }\n+\n+  result_spiller.generate_spill(_masm, res_save_area_offset);\n@@ -708,28 +352,1 @@\n-  restore_java_result(_masm, conv, res_save_area_offset);\n-\n-  \/\/ return value shuffle\n-#ifdef ASSERT\n-  if (conv._rets_length == 1) { \/\/ 0 or 1\n-    VMReg j_expected_result_reg;\n-    switch (ret_type) {\n-      case T_BOOLEAN:\n-      case T_BYTE:\n-      case T_SHORT:\n-      case T_CHAR:\n-      case T_INT:\n-      case T_LONG:\n-       j_expected_result_reg = rax->as_VMReg();\n-       break;\n-      case T_FLOAT:\n-      case T_DOUBLE:\n-        j_expected_result_reg = xmm0->as_VMReg();\n-        break;\n-      default:\n-        fatal(\"unexpected return type: %s\", type2name(ret_type));\n-    }\n-    \/\/ No need to move for now, since CallArranger can pick a return type\n-    \/\/ that goes in the same reg for both CCs. But, at least assert they are the same\n-    assert(conv._ret_regs[0] == j_expected_result_reg,\n-     \"unexpected result register: %s != %s\", conv._ret_regs[0]->name(), j_expected_result_reg->name());\n-  }\n-#endif\n+  result_spiller.generate_fill(_masm, res_save_area_offset);\n@@ -771,1 +388,6 @@\n-  OptimizedEntryBlob* blob = OptimizedEntryBlob::create(name, &buffer, exception_handler_offset, receiver, in_ByteSize(frame_data_offset));\n+  OptimizedEntryBlob* blob\n+    = OptimizedEntryBlob::create(name,\n+                                 &buffer,\n+                                 exception_handler_offset,\n+                                 receiver,\n+                                 in_ByteSize(frame_data_offset));\n@@ -775,1 +397,0 @@\n-    Disassembler::decode(blob, tty);\n@@ -781,4 +402,0 @@\n-\n-bool ProgrammableUpcallHandler::supports_optimized_upcalls() {\n-  return true;\n-}\n","filename":"src\/hotspot\/cpu\/x86\/universalUpcallHandler_x86_64.cpp","additions":95,"deletions":478,"binary":false,"changes":573,"status":"modified"},{"patch":"@@ -69,15 +69,0 @@\n-\n-#define INTEGER_TYPE 0\n-#define VECTOR_TYPE 1\n-#define X87_TYPE 2\n-#define STACK_TYPE 3\n-\n-\/\/TODO: Case for KRegisters\n-VMReg VMRegImpl::vmStorageToVMReg(int type, int index) {\n-  switch(type) {\n-    case INTEGER_TYPE: return ::as_Register(index)->as_VMReg();\n-    case VECTOR_TYPE: return ::as_XMMRegister(index)->as_VMReg();\n-    case STACK_TYPE: return VMRegImpl::stack2reg(index LP64_ONLY(* 2)); \/\/ numbering on x64 goes per 64-bits\n-  }\n-  return VMRegImpl::Bad();\n-}\n","filename":"src\/hotspot\/cpu\/x86\/vmreg_x86.cpp","additions":0,"deletions":15,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -316,5 +316,0 @@\n-int MachCallNativeNode::ret_addr_offset() {\n-  ShouldNotCallThis();\n-  return -1;\n-}\n-\n","filename":"src\/hotspot\/cpu\/x86\/x86_32.ad","additions":0,"deletions":5,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -507,6 +507,0 @@\n-\n-int MachCallNativeNode::ret_addr_offset() {\n-  int offset = 13; \/\/ movq r10,#addr; callq (r10)\n-  offset += clear_avx_size();\n-  return offset;\n-}\n@@ -13108,12 +13102,0 @@\n-\/\/\n-instruct CallNativeDirect(method meth)\n-%{\n-  match(CallNative);\n-  effect(USE meth);\n-\n-  ins_cost(300);\n-  format %{ \"call_native \" %}\n-  ins_encode(clear_avx, Java_To_Runtime(meth));\n-  ins_pipe(pipe_slow);\n-%}\n-\n","filename":"src\/hotspot\/cpu\/x86\/x86_64.ad","additions":0,"deletions":18,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -25,0 +25,1 @@\n+#include \"code\/vmreg.hpp\"\n@@ -28,1 +29,3 @@\n-const ABIDescriptor ForeignGlobals::parse_abi_descriptor_impl(jobject jabi) const {\n+class MacroAssembler;\n+\n+const ABIDescriptor ForeignGlobals::parse_abi_descriptor(jobject jabi) {\n@@ -33,3 +36,3 @@\n-const BufferLayout ForeignGlobals::parse_buffer_layout_impl(jobject jlayout) const {\n-  ShouldNotCallThis();\n-  return {};\n+VMReg ForeignGlobals::vmstorage_to_vmreg(int type, int index) {\n+  Unimplemented();\n+  return VMRegImpl::Bad();\n@@ -38,3 +41,15 @@\n-const CallRegs ForeignGlobals::parse_call_regs_impl(jobject jconv) const {\n-  ShouldNotCallThis();\n-  return {};\n+int RegSpiller::pd_reg_size(VMReg reg) {\n+  Unimplemented();\n+  return -1;\n+}\n+\n+void RegSpiller::pd_store_reg(MacroAssembler* masm, int offset, VMReg reg) {\n+  Unimplemented();\n+}\n+\n+void RegSpiller::pd_load_reg(MacroAssembler* masm, int offset, VMReg reg) {\n+  Unimplemented();\n+}\n+\n+void ArgumentShuffle::pd_generate(MacroAssembler* masm, VMReg tmp, int in_stk_bias, int out_stk_bias) const {\n+  Unimplemented();\n","filename":"src\/hotspot\/cpu\/zero\/foreign_globals_zero.cpp","additions":22,"deletions":7,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -27,1 +27,0 @@\n-class BufferLayout {};\n","filename":"src\/hotspot\/cpu\/zero\/foreign_globals_zero.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -27,2 +27,8 @@\n-address ProgrammableInvoker::generate_adapter(jobject jabi, jobject jlayout) {\n-  ShouldNotCallThis();\n+RuntimeStub* ProgrammableInvoker::make_native_invoker(BasicType* signature,\n+                                                      int num_args,\n+                                                      BasicType ret_bt,\n+                                                      const ABIDescriptor& abi,\n+                                                      const GrowableArray<VMReg>& input_registers,\n+                                                      const GrowableArray<VMReg>& output_registers,\n+                                                      bool needs_return_buffer) {\n+  Unimplemented();\n","filename":"src\/hotspot\/cpu\/zero\/universalNativeInvoker_zero.cpp","additions":8,"deletions":2,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -27,1 +27,6 @@\n-address ProgrammableUpcallHandler::generate_upcall_stub(jobject rec, jobject jabi, jobject jlayout) {\n+address ProgrammableUpcallHandler::generate_optimized_upcall_stub(jobject receiver, Method* entry,\n+                                                                  BasicType* in_sig_bt, int total_in_args,\n+                                                                  BasicType* out_sig_bt, int total_out_args,\n+                                                                  BasicType ret_type,\n+                                                                  jobject jabi, jobject jconv,\n+                                                                  bool needs_return_buffer, int ret_buf_size) {\n@@ -31,9 +36,0 @@\n-\n-address ProgrammableUpcallHandler::generate_optimized_upcall_stub(jobject mh, Method* entry, jobject jabi, jobject jconv) {\n-  ShouldNotCallThis();\n-  return nullptr;\n-}\n-\n-bool ProgrammableUpcallHandler::supports_optimized_upcalls() {\n-  return false;\n-}\n","filename":"src\/hotspot\/cpu\/zero\/universalUpcallHandle_zero.cpp","additions":6,"deletions":10,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -64,5 +64,0 @@\n-\n-VMReg VMRegImpl::vmStorageToVMReg(int type, int index) {\n-  ShouldNotCallThis();\n-  return VMRegImpl::Bad();\n-}\n","filename":"src\/hotspot\/cpu\/zero\/vmreg_zero.cpp","additions":0,"deletions":5,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -424,2 +424,0 @@\n-  if(_matrule->find_type(\"CallNative\",idx))       return Form::JAVA_NATIVE;\n-  idx = 0;\n@@ -1146,3 +1144,0 @@\n-  else if( is_ideal_call() == Form::JAVA_NATIVE ) {\n-    return \"MachCallNativeNode\";\n-  }\n","filename":"src\/hotspot\/share\/adlc\/formssel.cpp","additions":0,"deletions":5,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -4230,1 +4230,2 @@\n-    break; \/\/ TODO: NYI\n+    print_inlining(callee, \"Native call\", \/*success*\/ false);\n+    break;\n","filename":"src\/hotspot\/share\/c1\/c1_GraphBuilder.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -246,1 +246,0 @@\n-    bool is_opt_native = false;\n@@ -250,1 +249,1 @@\n-                             reexecute, rethrow_exception, is_method_handle_invoke, is_opt_native, return_oop,\n+                             reexecute, rethrow_exception, is_method_handle_invoke, return_oop,\n","filename":"src\/hotspot\/share\/c1\/c1_IR.hpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -51,1 +51,0 @@\n-class     ciNativeEntryPoint;\n@@ -101,1 +100,0 @@\n-friend class ciNativeEntryPoint;       \\\n","filename":"src\/hotspot\/share\/ci\/ciClassList.hpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1056,2 +1056,1 @@\n-                            RTMState  rtm_state,\n-                            const GrowableArrayView<RuntimeStub*>& native_invokers) {\n+                            RTMState  rtm_state) {\n@@ -1146,2 +1145,1 @@\n-                               compiler, task()->comp_level(),\n-                               native_invokers);\n+                               compiler, task()->comp_level());\n","filename":"src\/hotspot\/share\/ci\/ciEnv.cpp","additions":2,"deletions":4,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -397,2 +397,1 @@\n-                       RTMState                  rtm_state = NoRTM,\n-                       const GrowableArrayView<RuntimeStub*>& native_invokers = GrowableArrayView<RuntimeStub*>::EMPTY);\n+                       RTMState                  rtm_state = NoRTM);\n","filename":"src\/hotspot\/share\/ci\/ciEnv.hpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -1,88 +0,0 @@\n-\/*\n- * Copyright (c) 2020, 2021, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"ci\/ciClassList.hpp\"\n-#include \"ci\/ciNativeEntryPoint.hpp\"\n-#include \"ci\/ciUtilities.inline.hpp\"\n-#include \"ci\/ciArray.hpp\"\n-#include \"classfile\/javaClasses.hpp\"\n-#include \"oops\/oop.inline.hpp\"\n-#include \"memory\/allocation.hpp\"\n-\n-VMReg* getVMRegArray(ciArray* array) {\n-  assert(array->element_basic_type() == T_LONG, \"Unexpected type\");\n-\n-  VMReg* out = NEW_ARENA_ARRAY(CURRENT_ENV->arena(), VMReg, array->length());\n-\n-  for (int i = 0; i < array->length(); i++) {\n-    ciConstant con = array->element_value(i);\n-    VMReg reg = VMRegImpl::as_VMReg(con.as_long());\n-    out[i] = reg;\n-  }\n-\n-  return out;\n-}\n-\n-ciNativeEntryPoint::ciNativeEntryPoint(instanceHandle h_i) : ciInstance(h_i), _name(NULL) {\n-  \/\/ Copy name\n-  oop name_str = jdk_internal_invoke_NativeEntryPoint::name(get_oop());\n-  if (name_str != NULL) {\n-    char* temp_name = java_lang_String::as_quoted_ascii(name_str);\n-    size_t len = strlen(temp_name) + 1;\n-    char* name = (char*)CURRENT_ENV->arena()->Amalloc(len);\n-    strncpy(name, temp_name, len);\n-    _name = name;\n-  }\n-\n-  _arg_moves = getVMRegArray(CURRENT_ENV->get_object(jdk_internal_invoke_NativeEntryPoint::argMoves(get_oop()))->as_array());\n-  _ret_moves = getVMRegArray(CURRENT_ENV->get_object(jdk_internal_invoke_NativeEntryPoint::returnMoves(get_oop()))->as_array());\n-}\n-\n-jint ciNativeEntryPoint::shadow_space() const {\n-  VM_ENTRY_MARK;\n-  return jdk_internal_invoke_NativeEntryPoint::shadow_space(get_oop());\n-}\n-\n-VMReg* ciNativeEntryPoint::argMoves() const {\n-  return _arg_moves;\n-}\n-\n-VMReg* ciNativeEntryPoint::returnMoves() const {\n-  return _ret_moves;\n-}\n-\n-jboolean ciNativeEntryPoint::need_transition() const {\n-  VM_ENTRY_MARK;\n-  return jdk_internal_invoke_NativeEntryPoint::need_transition(get_oop());\n-}\n-\n-ciMethodType* ciNativeEntryPoint::method_type() const {\n-  VM_ENTRY_MARK;\n-  return CURRENT_ENV->get_object(jdk_internal_invoke_NativeEntryPoint::method_type(get_oop()))->as_method_type();\n-}\n-\n-const char* ciNativeEntryPoint::name() {\n-  return _name;\n-}\n","filename":"src\/hotspot\/share\/ci\/ciNativeEntryPoint.cpp","additions":0,"deletions":88,"binary":false,"changes":88,"status":"deleted"},{"patch":"@@ -1,55 +0,0 @@\n-\/*\n- * Copyright (c) 2020, 2021, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#ifndef SHARE_VM_CI_CINATIVEENTRYPOINT_HPP\n-#define SHARE_VM_CI_CINATIVEENTRYPOINT_HPP\n-\n-#include \"ci\/ciInstance.hpp\"\n-#include \"ci\/ciMethodType.hpp\"\n-\n-#include \"code\/vmreg.hpp\"\n-\n-\/\/ ciNativeEntryPoint\n-\/\/\n-\/\/ The class represents a java.lang.invoke.NativeEntryPoint object.\n-class ciNativeEntryPoint : public ciInstance {\n-private:\n-  const char* _name;\n-  VMReg* _arg_moves;\n-  VMReg* _ret_moves;\n-public:\n-  ciNativeEntryPoint(instanceHandle h_i);\n-\n-  \/\/ What kind of ciObject is this?\n-  bool is_native_entry_point() const { return true; }\n-\n-  jint           shadow_space() const;\n-  VMReg*         argMoves() const;\n-  VMReg*        returnMoves() const;\n-  jboolean       need_transition() const;\n-  ciMethodType*  method_type() const;\n-  const char*    name();\n-};\n-\n-#endif \/\/ SHARE_VM_CI_CINATIVEENTRYPOINT_HPP\n","filename":"src\/hotspot\/share\/ci\/ciNativeEntryPoint.hpp","additions":0,"deletions":55,"binary":false,"changes":55,"status":"deleted"},{"patch":"@@ -163,4 +163,0 @@\n-  ciNativeEntryPoint* as_native_entry_point() {\n-    assert(is_native_entry_point(), \"bad cast\");\n-    return (ciNativeEntryPoint*)this;\n-  }\n","filename":"src\/hotspot\/share\/ci\/ciObject.hpp","additions":0,"deletions":4,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -30,1 +30,0 @@\n-#include \"ci\/ciNativeEntryPoint.hpp\"\n@@ -347,2 +346,0 @@\n-    else if (jdk_internal_invoke_NativeEntryPoint::is_instance(o))\n-      return new (arena()) ciNativeEntryPoint(h_i);\n","filename":"src\/hotspot\/share\/ci\/ciObjectFactory.cpp","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -58,0 +58,1 @@\n+#include \"oops\/oopCast.inline.hpp\"\n@@ -4239,6 +4240,2 @@\n-int jdk_internal_invoke_NativeEntryPoint::_shadow_space_offset;\n-int jdk_internal_invoke_NativeEntryPoint::_argMoves_offset;\n-int jdk_internal_invoke_NativeEntryPoint::_returnMoves_offset;\n-int jdk_internal_invoke_NativeEntryPoint::_need_transition_offset;\n-int jdk_internal_invoke_NativeEntryPoint::_method_type_offset;\n-int jdk_internal_invoke_NativeEntryPoint::_name_offset;\n+int jdk_internal_foreign_abi_NativeEntryPoint::_method_type_offset;\n+int jdk_internal_foreign_abi_NativeEntryPoint::_invoker_offset;\n@@ -4247,4 +4244,0 @@\n-  macro(_shadow_space_offset,    k, \"shadowSpace\",    int_signature, false); \\\n-  macro(_argMoves_offset,        k, \"argMoves\",       long_array_signature, false); \\\n-  macro(_returnMoves_offset,     k, \"returnMoves\",    long_array_signature, false); \\\n-  macro(_need_transition_offset, k, \"needTransition\", bool_signature, false); \\\n@@ -4252,1 +4245,1 @@\n-  macro(_name_offset,            k, \"name\",           string_signature, false);\n+  macro(_invoker_offset,         k, \"invoker\",        long_signature, false);\n@@ -4254,1 +4247,1 @@\n-bool jdk_internal_invoke_NativeEntryPoint::is_instance(oop obj) {\n+bool jdk_internal_foreign_abi_NativeEntryPoint::is_instance(oop obj) {\n@@ -4258,1 +4251,1 @@\n-void jdk_internal_invoke_NativeEntryPoint::compute_offsets() {\n+void jdk_internal_foreign_abi_NativeEntryPoint::compute_offsets() {\n@@ -4264,1 +4257,1 @@\n-void jdk_internal_invoke_NativeEntryPoint::serialize_offsets(SerializeClosure* f) {\n+void jdk_internal_foreign_abi_NativeEntryPoint::serialize_offsets(SerializeClosure* f) {\n@@ -4269,2 +4262,2 @@\n-jint jdk_internal_invoke_NativeEntryPoint::shadow_space(oop entry) {\n-  return entry->int_field(_shadow_space_offset);\n+oop jdk_internal_foreign_abi_NativeEntryPoint::method_type(oop entry) {\n+  return entry->obj_field(_method_type_offset);\n@@ -4273,2 +4266,2 @@\n-oop jdk_internal_invoke_NativeEntryPoint::argMoves(oop entry) {\n-  return entry->obj_field(_argMoves_offset);\n+jlong jdk_internal_foreign_abi_NativeEntryPoint::invoker(oop entry) {\n+  return entry->long_field(_invoker_offset);\n@@ -4277,2 +4270,19 @@\n-oop jdk_internal_invoke_NativeEntryPoint::returnMoves(oop entry) {\n-  return entry->obj_field(_returnMoves_offset);\n+int jdk_internal_foreign_abi_ABIDescriptor::_inputStorage_offset;\n+int jdk_internal_foreign_abi_ABIDescriptor::_outputStorage_offset;\n+int jdk_internal_foreign_abi_ABIDescriptor::_volatileStorage_offset;\n+int jdk_internal_foreign_abi_ABIDescriptor::_stackAlignment_offset;\n+int jdk_internal_foreign_abi_ABIDescriptor::_shadowSpace_offset;\n+int jdk_internal_foreign_abi_ABIDescriptor::_targetAddrStorage_offset;\n+int jdk_internal_foreign_abi_ABIDescriptor::_retBufAddrStorage_offset;\n+\n+#define ABIDescriptor_FIELDS_DO(macro) \\\n+  macro(_inputStorage_offset,      k, \"inputStorage\",      jdk_internal_foreign_abi_VMStorage_array_array_signature, false); \\\n+  macro(_outputStorage_offset,     k, \"outputStorage\",     jdk_internal_foreign_abi_VMStorage_array_array_signature, false); \\\n+  macro(_volatileStorage_offset,   k, \"volatileStorage\",   jdk_internal_foreign_abi_VMStorage_array_array_signature, false); \\\n+  macro(_stackAlignment_offset,    k, \"stackAlignment\",    int_signature, false); \\\n+  macro(_shadowSpace_offset,       k, \"shadowSpace\",       int_signature, false); \\\n+  macro(_targetAddrStorage_offset, k, \"targetAddrStorage\", jdk_internal_foreign_abi_VMStorage_signature, false); \\\n+  macro(_retBufAddrStorage_offset, k, \"retBufAddrStorage\", jdk_internal_foreign_abi_VMStorage_signature, false);\n+\n+bool jdk_internal_foreign_abi_ABIDescriptor::is_instance(oop obj) {\n+  return obj != NULL && is_subclass(obj->klass());\n@@ -4281,2 +4291,3 @@\n-jboolean jdk_internal_invoke_NativeEntryPoint::need_transition(oop entry) {\n-  return entry->bool_field(_need_transition_offset);\n+void jdk_internal_foreign_abi_ABIDescriptor::compute_offsets() {\n+  InstanceKlass* k = vmClasses::ABIDescriptor_klass();\n+  ABIDescriptor_FIELDS_DO(FIELD_COMPUTE_OFFSET);\n@@ -4285,2 +4296,94 @@\n-oop jdk_internal_invoke_NativeEntryPoint::method_type(oop entry) {\n-  return entry->obj_field(_method_type_offset);\n+#if INCLUDE_CDS\n+void jdk_internal_foreign_abi_ABIDescriptor::serialize_offsets(SerializeClosure* f) {\n+  ABIDescriptor_FIELDS_DO(FIELD_SERIALIZE_OFFSET);\n+}\n+#endif\n+\n+objArrayOop jdk_internal_foreign_abi_ABIDescriptor::inputStorage(oop entry) {\n+  return oop_cast<objArrayOop>(entry->obj_field(_inputStorage_offset));\n+}\n+\n+objArrayOop jdk_internal_foreign_abi_ABIDescriptor::outputStorage(oop entry) {\n+  return oop_cast<objArrayOop>(entry->obj_field(_outputStorage_offset));\n+}\n+\n+objArrayOop jdk_internal_foreign_abi_ABIDescriptor::volatileStorage(oop entry) {\n+  return oop_cast<objArrayOop>(entry->obj_field(_volatileStorage_offset));\n+}\n+\n+jint jdk_internal_foreign_abi_ABIDescriptor::stackAlignment(oop entry) {\n+  return entry->int_field(_stackAlignment_offset);\n+}\n+\n+jint jdk_internal_foreign_abi_ABIDescriptor::shadowSpace(oop entry) {\n+  return entry->int_field(_shadowSpace_offset);\n+}\n+\n+oop jdk_internal_foreign_abi_ABIDescriptor::targetAddrStorage(oop entry) {\n+  return entry->obj_field(_targetAddrStorage_offset);\n+}\n+\n+oop jdk_internal_foreign_abi_ABIDescriptor::retBufAddrStorage(oop entry) {\n+  return entry->obj_field(_retBufAddrStorage_offset);\n+}\n+\n+int jdk_internal_foreign_abi_VMStorage::_type_offset;\n+int jdk_internal_foreign_abi_VMStorage::_index_offset;\n+int jdk_internal_foreign_abi_VMStorage::_debugName_offset;\n+\n+#define VMStorage_FIELDS_DO(macro) \\\n+  macro(_type_offset,      k, \"type\",      int_signature, false); \\\n+  macro(_index_offset,     k, \"index\",     int_signature, false); \\\n+  macro(_debugName_offset, k, \"debugName\", string_signature, false); \\\n+\n+bool jdk_internal_foreign_abi_VMStorage::is_instance(oop obj) {\n+  return obj != NULL && is_subclass(obj->klass());\n+}\n+\n+void jdk_internal_foreign_abi_VMStorage::compute_offsets() {\n+  InstanceKlass* k = vmClasses::VMStorage_klass();\n+  VMStorage_FIELDS_DO(FIELD_COMPUTE_OFFSET);\n+}\n+\n+#if INCLUDE_CDS\n+void jdk_internal_foreign_abi_VMStorage::serialize_offsets(SerializeClosure* f) {\n+  VMStorage_FIELDS_DO(FIELD_SERIALIZE_OFFSET);\n+}\n+#endif\n+\n+jint jdk_internal_foreign_abi_VMStorage::type(oop entry) {\n+  return entry->int_field(_type_offset);\n+}\n+\n+jint jdk_internal_foreign_abi_VMStorage::index(oop entry) {\n+  return entry->int_field(_index_offset);\n+}\n+\n+oop jdk_internal_foreign_abi_VMStorage::debugName(oop entry) {\n+  return entry->obj_field(_debugName_offset);\n+}\n+\n+int jdk_internal_foreign_abi_CallConv::_argRegs_offset;\n+int jdk_internal_foreign_abi_CallConv::_retRegs_offset;\n+\n+#define CallConv_FIELDS_DO(macro) \\\n+  macro(_argRegs_offset, k, \"argRegs\", jdk_internal_foreign_abi_VMStorage_array_signature, false); \\\n+  macro(_retRegs_offset, k, \"retRegs\", jdk_internal_foreign_abi_VMStorage_array_signature, false); \\\n+\n+bool jdk_internal_foreign_abi_CallConv::is_instance(oop obj) {\n+  return obj != NULL && is_subclass(obj->klass());\n+}\n+\n+void jdk_internal_foreign_abi_CallConv::compute_offsets() {\n+  InstanceKlass* k = vmClasses::CallConv_klass();\n+  CallConv_FIELDS_DO(FIELD_COMPUTE_OFFSET);\n+}\n+\n+#if INCLUDE_CDS\n+void jdk_internal_foreign_abi_CallConv::serialize_offsets(SerializeClosure* f) {\n+  CallConv_FIELDS_DO(FIELD_SERIALIZE_OFFSET);\n+}\n+#endif\n+\n+objArrayOop jdk_internal_foreign_abi_CallConv::argRegs(oop entry) {\n+  return oop_cast<objArrayOop>(entry->obj_field(_argRegs_offset));\n@@ -4289,2 +4392,2 @@\n-oop jdk_internal_invoke_NativeEntryPoint::name(oop entry) {\n-  return entry->obj_field(_name_offset);\n+objArrayOop jdk_internal_foreign_abi_CallConv::retRegs(oop entry) {\n+  return oop_cast<objArrayOop>(entry->obj_field(_retRegs_offset));\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.cpp","additions":129,"deletions":26,"binary":false,"changes":155,"status":"modified"},{"patch":"@@ -86,1 +86,4 @@\n-  f(jdk_internal_invoke_NativeEntryPoint) \\\n+  f(jdk_internal_foreign_abi_NativeEntryPoint) \\\n+  f(jdk_internal_foreign_abi_ABIDescriptor) \\\n+  f(jdk_internal_foreign_abi_VMStorage) \\\n+  f(jdk_internal_foreign_abi_CallConv) \\\n@@ -1279,1 +1282,1 @@\n-class jdk_internal_invoke_NativeEntryPoint: AllStatic {\n+class jdk_internal_foreign_abi_NativeEntryPoint: AllStatic {\n@@ -1283,4 +1286,0 @@\n-  static int _shadow_space_offset;\n-  static int _argMoves_offset;\n-  static int _returnMoves_offset;\n-  static int _need_transition_offset;\n@@ -1288,1 +1287,1 @@\n-  static int _name_offset;\n+  static int _invoker_offset;\n@@ -1296,4 +1295,0 @@\n-  static jint       shadow_space(oop entry);\n-  static oop        argMoves(oop entry);\n-  static oop        returnMoves(oop entry);\n-  static jboolean   need_transition(oop entry);\n@@ -1301,1 +1296,1 @@\n-  static oop        name(oop entry);\n+  static jlong      invoker(oop entry);\n@@ -1311,4 +1306,0 @@\n-  static int shadow_space_offset_in_bytes()    { return _shadow_space_offset;    }\n-  static int argMoves_offset_in_bytes()        { return _argMoves_offset;        }\n-  static int returnMoves_offset_in_bytes()     { return _returnMoves_offset;     }\n-  static int need_transition_offset_in_bytes() { return _need_transition_offset; }\n@@ -1316,1 +1307,85 @@\n-  static int name_offset_in_bytes()            { return _name_offset;            }\n+  static int invoker_offset_in_bytes()         { return _invoker_offset;         }\n+};\n+\n+class jdk_internal_foreign_abi_ABIDescriptor: AllStatic {\n+  friend class JavaClasses;\n+\n+ private:\n+  static int _inputStorage_offset;\n+  static int _outputStorage_offset;\n+  static int _volatileStorage_offset;\n+  static int _stackAlignment_offset;\n+  static int _shadowSpace_offset;\n+  static int _targetAddrStorage_offset;\n+  static int _retBufAddrStorage_offset;\n+\n+  static void compute_offsets();\n+\n+ public:\n+  static void serialize_offsets(SerializeClosure* f) NOT_CDS_RETURN;\n+\n+  \/\/ Accessors\n+  static objArrayOop inputStorage(oop entry);\n+  static objArrayOop outputStorage(oop entry);\n+  static objArrayOop volatileStorage(oop entry);\n+  static jint        stackAlignment(oop entry);\n+  static jint        shadowSpace(oop entry);\n+  static oop         targetAddrStorage(oop entry);\n+  static oop         retBufAddrStorage(oop entry);\n+\n+  \/\/ Testers\n+  static bool is_subclass(Klass* klass) {\n+    return vmClasses::ABIDescriptor_klass() != NULL &&\n+      klass->is_subclass_of(vmClasses::ABIDescriptor_klass());\n+  }\n+  static bool is_instance(oop obj);\n+};\n+\n+class jdk_internal_foreign_abi_VMStorage: AllStatic {\n+  friend class JavaClasses;\n+\n+ private:\n+  static int _type_offset;\n+  static int _index_offset;\n+  static int _debugName_offset;\n+\n+  static void compute_offsets();\n+\n+ public:\n+  static void serialize_offsets(SerializeClosure* f) NOT_CDS_RETURN;\n+\n+  \/\/ Accessors\n+  static jint        type(oop entry);\n+  static jint        index(oop entry);\n+  static oop         debugName(oop entry);\n+\n+  \/\/ Testers\n+  static bool is_subclass(Klass* klass) {\n+    return vmClasses::VMStorage_klass() != NULL &&\n+      klass->is_subclass_of(vmClasses::VMStorage_klass());\n+  }\n+  static bool is_instance(oop obj);\n+};\n+\n+class jdk_internal_foreign_abi_CallConv: AllStatic {\n+  friend class JavaClasses;\n+\n+ private:\n+  static int _argRegs_offset;\n+  static int _retRegs_offset;\n+\n+  static void compute_offsets();\n+\n+ public:\n+  static void serialize_offsets(SerializeClosure* f) NOT_CDS_RETURN;\n+\n+  \/\/ Accessors\n+  static objArrayOop argRegs(oop entry);\n+  static objArrayOop retRegs(oop entry);\n+\n+  \/\/ Testers\n+  static bool is_subclass(Klass* klass) {\n+    return vmClasses::CallConv_klass() != NULL &&\n+      klass->is_subclass_of(vmClasses::CallConv_klass());\n+  }\n+  static bool is_instance(oop obj);\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.hpp","additions":92,"deletions":17,"binary":false,"changes":109,"status":"modified"},{"patch":"@@ -129,1 +129,4 @@\n-  do_klass(NativeEntryPoint_klass,                      jdk_internal_invoke_NativeEntryPoint                  ) \\\n+  do_klass(NativeEntryPoint_klass,                      jdk_internal_foreign_abi_NativeEntryPoint             ) \\\n+  do_klass(ABIDescriptor_klass,                         jdk_internal_foreign_abi_ABIDescriptor                ) \\\n+  do_klass(VMStorage_klass,                             jdk_internal_foreign_abi_VMStorage                    ) \\\n+  do_klass(CallConv_klass,                              jdk_internal_foreign_abi_CallConv                     ) \\\n","filename":"src\/hotspot\/share\/classfile\/vmClassMacros.hpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -366,3 +366,8 @@\n-  \/* Foreign API Support *\/                                                                                          \\\n-  template(jdk_internal_invoke_NativeEntryPoint,                 \"jdk\/internal\/invoke\/NativeEntryPoint\")           \\\n-  template(jdk_internal_invoke_NativeEntryPoint_signature,       \"Ljdk\/internal\/invoke\/NativeEntryPoint;\")         \\\n+  \/* Foreign API Support *\/                                                                       \\\n+  template(jdk_internal_foreign_abi_NativeEntryPoint,                \"jdk\/internal\/foreign\/abi\/NativeEntryPoint\") \\\n+  template(jdk_internal_foreign_abi_ABIDescriptor,                   \"jdk\/internal\/foreign\/abi\/ABIDescriptor\") \\\n+  template(jdk_internal_foreign_abi_VMStorage,                       \"jdk\/internal\/foreign\/abi\/VMStorage\") \\\n+  template(jdk_internal_foreign_abi_VMStorage_signature,             \"Ljdk\/internal\/foreign\/abi\/VMStorage;\") \\\n+  template(jdk_internal_foreign_abi_VMStorage_array_signature,       \"[Ljdk\/internal\/foreign\/abi\/VMStorage;\") \\\n+  template(jdk_internal_foreign_abi_VMStorage_array_array_signature, \"[[Ljdk\/internal\/foreign\/abi\/VMStorage;\") \\\n+  template(jdk_internal_foreign_abi_CallConv,                        \"jdk\/internal\/foreign\/abi\/ProgrammableUpcallHandler$CallRegs\") \\\n","filename":"src\/hotspot\/share\/classfile\/vmSymbols.hpp","additions":8,"deletions":3,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -164,0 +164,12 @@\n+void RuntimeBlob::free(RuntimeBlob* blob) {\n+  assert(blob != NULL, \"caller must check for NULL\");\n+  ThreadInVMfromUnknown __tiv;  \/\/ get to VM state in case we block on CodeCache_lock\n+  blob->flush();\n+  {\n+    MutexLocker mu(CodeCache_lock, Mutex::_no_safepoint_check_flag);\n+    CodeCache::free(blob);\n+  }\n+  \/\/ Track memory usage statistic after releasing CodeCache_lock\n+  MemoryService::track_code_cache_memory_usage();\n+}\n+\n@@ -279,9 +291,1 @@\n-  assert(blob != NULL, \"caller must check for NULL\");\n-  ThreadInVMfromUnknown __tiv;  \/\/ get to VM state in case we block on CodeCache_lock\n-  blob->flush();\n-  {\n-    MutexLocker mu(CodeCache_lock, Mutex::_no_safepoint_check_flag);\n-    CodeCache::free((RuntimeBlob*)blob);\n-  }\n-  \/\/ Track memory usage statistic after releasing CodeCache_lock\n-  MemoryService::track_code_cache_memory_usage();\n+  RuntimeBlob::free(blob);\n@@ -724,1 +728,2 @@\n-OptimizedEntryBlob::OptimizedEntryBlob(const char* name, int size, CodeBuffer* cb, intptr_t exception_handler_offset,\n+OptimizedEntryBlob::OptimizedEntryBlob(const char* name, CodeBuffer* cb, int size,\n+                                       intptr_t exception_handler_offset,\n@@ -726,1 +731,2 @@\n-  BufferBlob(name, size, cb),\n+  RuntimeBlob(name, cb, sizeof(OptimizedEntryBlob), size, CodeOffsets::frame_never_safe, 0 \/* no frame size *\/,\n+              \/* oop maps = *\/ nullptr, \/* caller must gc arguments = *\/ false),\n@@ -733,1 +739,6 @@\n-OptimizedEntryBlob* OptimizedEntryBlob::create(const char* name, CodeBuffer* cb, intptr_t exception_handler_offset,\n+void* OptimizedEntryBlob::operator new(size_t s, unsigned size) throw() {\n+  return CodeCache::allocate(size, CodeBlobType::NonNMethod);\n+}\n+\n+OptimizedEntryBlob* OptimizedEntryBlob::create(const char* name, CodeBuffer* cb,\n+                                               intptr_t exception_handler_offset,\n@@ -741,1 +752,2 @@\n-    blob = new (size) OptimizedEntryBlob(name, size, cb, exception_handler_offset, receiver, frame_data_offset);\n+    blob = new (size) OptimizedEntryBlob(name, cb, size,\n+                                         exception_handler_offset, receiver, frame_data_offset);\n@@ -746,0 +758,2 @@\n+  trace_new_stub(blob, \"OptimizedEntryBlob\");\n+\n@@ -756,0 +770,24 @@\n+\n+void OptimizedEntryBlob::free(OptimizedEntryBlob* blob) {\n+  assert(blob != nullptr, \"caller must check for NULL\");\n+  JNIHandles::destroy_global(blob->receiver());\n+  RuntimeBlob::free(blob);\n+}\n+\n+void OptimizedEntryBlob::preserve_callee_argument_oops(frame fr, const RegisterMap* reg_map, OopClosure* f) {\n+  ShouldNotReachHere(); \/\/ caller should never have to gc arguments\n+}\n+\n+\/\/ Misc.\n+void OptimizedEntryBlob::verify() {\n+  \/\/ unimplemented\n+}\n+\n+void OptimizedEntryBlob::print_on(outputStream* st) const {\n+  RuntimeBlob::print_on(st);\n+  print_value_on(st);\n+}\n+\n+void OptimizedEntryBlob::print_value_on(outputStream* st) const {\n+  st->print_cr(\"OptimizedEntryBlob (\" INTPTR_FORMAT  \") used for %s\", p2i(this), name());\n+}\n","filename":"src\/hotspot\/share\/code\/codeBlob.cpp","additions":51,"deletions":13,"binary":false,"changes":64,"status":"modified"},{"patch":"@@ -64,1 +64,0 @@\n-\/\/    OptimizedEntryBlob : Used for upcalls from native code\n@@ -71,0 +70,1 @@\n+\/\/   OptimizedEntryBlob  : Used for upcalls from native code\n@@ -383,0 +383,2 @@\n+  static void free(RuntimeBlob* blob);\n+\n@@ -477,1 +479,1 @@\n-  MethodHandlesAdapterBlob(int size)                 : BufferBlob(\"MethodHandles adapters\", size) {}\n+  MethodHandlesAdapterBlob(int size): BufferBlob(\"MethodHandles adapters\", size) {}\n@@ -752,1 +754,1 @@\n-class OptimizedEntryBlob: public BufferBlob {\n+class OptimizedEntryBlob: public RuntimeBlob {\n@@ -759,1 +761,2 @@\n-  OptimizedEntryBlob(const char* name, int size, CodeBuffer* cb, intptr_t exception_handler_offset,\n+  OptimizedEntryBlob(const char* name, CodeBuffer* cb, int size,\n+                     intptr_t exception_handler_offset,\n@@ -762,0 +765,2 @@\n+  void* operator new(size_t s, unsigned size) throw();\n+\n@@ -774,2 +779,4 @@\n-                                    intptr_t exception_handler_offset, jobject receiver,\n-                                    ByteSize frame_data_offset);\n+                                    intptr_t exception_handler_offset,\n+                                    jobject receiver, ByteSize frame_data_offset);\n+\n+  static void free(OptimizedEntryBlob* blob);\n@@ -782,2 +789,0 @@\n-  void oops_do(OopClosure* f, const frame& frame);\n-\n@@ -786,0 +791,10 @@\n+\n+  \/\/ GC\/Verification support\n+  void oops_do(OopClosure* f, const frame& frame);\n+  virtual void preserve_callee_argument_oops(frame fr, const RegisterMap* reg_map, OopClosure* f) override;\n+  virtual bool is_alive() const override { return true; }\n+  virtual void verify() override;\n+\n+  \/\/ Misc.\n+  virtual void print_on(outputStream* st) const override;\n+  virtual void print_value_on(outputStream* st) const override;\n","filename":"src\/hotspot\/share\/code\/codeBlob.hpp","additions":23,"deletions":8,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -620,1 +620,0 @@\n-    nmethod* ptr = (nmethod *)cb;\n@@ -622,1 +621,1 @@\n-    if (ptr->has_dependencies()) {\n+    if (((nmethod *)cb)->has_dependencies()) {\n@@ -625,1 +624,0 @@\n-    ptr->free_native_invokers();\n","filename":"src\/hotspot\/share\/code\/codeCache.cpp","additions":1,"deletions":3,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -391,4 +391,0 @@\n-      if (ssd.is_optimized_linkToNative()) {\n-        \/\/ call was replaced\n-        return;\n-      }\n","filename":"src\/hotspot\/share\/code\/compiledMethod.cpp","additions":0,"deletions":4,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -291,1 +291,0 @@\n-                                              bool        is_optimized_linkToNative,\n@@ -310,1 +309,0 @@\n-  last_pd->set_is_optimized_linkToNative(is_optimized_linkToNative);\n","filename":"src\/hotspot\/share\/code\/debugInfoRec.cpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -109,1 +109,0 @@\n-                      bool        is_optimized_linkToNative = false,\n","filename":"src\/hotspot\/share\/code\/debugInfoRec.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -514,2 +514,1 @@\n-  int comp_level,\n-  const GrowableArrayView<RuntimeStub*>& native_invokers\n+  int comp_level\n@@ -537,1 +536,0 @@\n-      + align_up(checked_cast<int>(native_invokers.data_size_in_bytes()), oopSize)\n@@ -553,2 +551,1 @@\n-            comp_level,\n-            native_invokers\n+            comp_level\n@@ -643,2 +640,1 @@\n-    _native_invokers_offset     = _dependencies_offset;\n-    _handler_table_offset    = _native_invokers_offset;\n+    _handler_table_offset    = _dependencies_offset;\n@@ -744,2 +740,1 @@\n-  int comp_level,\n-  const GrowableArrayView<RuntimeStub*>& native_invokers\n+  int comp_level\n@@ -823,2 +818,1 @@\n-    _native_invokers_offset  = _dependencies_offset  + align_up((int)dependencies->size_in_bytes(), oopSize);\n-    _handler_table_offset    = _native_invokers_offset + align_up(checked_cast<int>(native_invokers.data_size_in_bytes()), oopSize);\n+    _handler_table_offset    = _dependencies_offset  + align_up((int)dependencies->size_in_bytes(), oopSize);\n@@ -846,4 +840,0 @@\n-    if (native_invokers.is_nonempty()) { \/\/ can not get address of zero-length array\n-      \/\/ Copy native stubs\n-      memcpy(native_invokers_begin(), native_invokers.adr_at(0), native_invokers.data_size_in_bytes());\n-    }\n@@ -1016,4 +1006,0 @@\n-    if (printmethod && native_invokers_begin() < native_invokers_end()) {\n-      print_native_invokers();\n-      tty->print_cr(\"- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \");\n-    }\n@@ -1080,6 +1066,0 @@\n-void nmethod::free_native_invokers() {\n-  for (RuntimeStub** it = native_invokers_begin(); it < native_invokers_end(); it++) {\n-    CodeCache::free(*it);\n-  }\n-}\n-\n@@ -2846,8 +2826,0 @@\n-void nmethod::print_native_invokers() {\n-  ResourceMark m;       \/\/ in case methods get printed via debugger\n-  tty->print_cr(\"Native invokers:\");\n-  for (RuntimeStub** itt = native_invokers_begin(); itt < native_invokers_end(); itt++) {\n-    (*itt)->print_on(tty);\n-  }\n-}\n-\n","filename":"src\/hotspot\/share\/code\/nmethod.cpp","additions":5,"deletions":33,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -213,1 +213,0 @@\n-  int _native_invokers_offset;\n@@ -315,2 +314,1 @@\n-          int comp_level,\n-          const GrowableArrayView<RuntimeStub*>& native_invokers\n+          int comp_level\n@@ -364,2 +362,1 @@\n-                              int comp_level,\n-                              const GrowableArrayView<RuntimeStub*>& native_invokers = GrowableArrayView<RuntimeStub*>::EMPTY\n+                              int comp_level\n@@ -415,3 +412,1 @@\n-  address dependencies_end      () const          { return           header_begin() + _native_invokers_offset ; }\n-  RuntimeStub** native_invokers_begin() const     { return (RuntimeStub**)(header_begin() + _native_invokers_offset) ; }\n-  RuntimeStub** native_invokers_end  () const     { return (RuntimeStub**)(header_begin() + _handler_table_offset); }\n+  address dependencies_end      () const          { return           header_begin() + _handler_table_offset ; }\n@@ -533,2 +528,0 @@\n-  void free_native_invokers();\n-\n@@ -674,1 +667,0 @@\n-  void print_native_invokers();\n","filename":"src\/hotspot\/share\/code\/nmethod.hpp","additions":3,"deletions":11,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -47,2 +47,1 @@\n-    PCDESC_arg_escape                = 1 << 5,\n-    PCDESC_is_optimized_linkToNative = 1 << 6\n+    PCDESC_arg_escape                = 1 << 5\n@@ -92,3 +91,0 @@\n-  bool     is_optimized_linkToNative()     const { return (_flags & PCDESC_is_optimized_linkToNative) != 0;     }\n-  void set_is_optimized_linkToNative(bool z)     { set_flag(PCDESC_is_optimized_linkToNative, z); }\n-\n","filename":"src\/hotspot\/share\/code\/pcDesc.hpp","additions":1,"deletions":5,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -42,1 +42,0 @@\n-  bool _is_optimized_linkToNative;\n@@ -49,1 +48,0 @@\n-    _is_optimized_linkToNative = pc_desc->is_optimized_linkToNative();\n@@ -58,1 +56,0 @@\n-  bool is_optimized_linkToNative() { return _is_optimized_linkToNative; }\n","filename":"src\/hotspot\/share\/code\/scopeDesc.hpp","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -144,2 +144,0 @@\n-  static VMReg vmStorageToVMReg(int type, int index);\n-\n","filename":"src\/hotspot\/share\/code\/vmreg.hpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1067,1 +1067,0 @@\n-  const bool is_opt_native         = false;\n@@ -1070,1 +1069,1 @@\n-  _debug_recorder->describe_scope(pc_offset, method, NULL, bci, reexecute, throw_exception, is_mh_invoke, is_opt_native, return_oop,\n+  _debug_recorder->describe_scope(pc_offset, method, NULL, bci, reexecute, throw_exception, is_mh_invoke, return_oop,\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCodeInstaller.cpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -2122,1 +2122,1 @@\n-                                 compiler, comp_level, GrowableArrayView<RuntimeStub*>::EMPTY,\n+                                 compiler, comp_level,\n","filename":"src\/hotspot\/share\/jvmci\/jvmciRuntime.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -138,0 +138,1 @@\n+  LOG_TAG(panama) \\\n","filename":"src\/hotspot\/share\/logging\/logTag.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -0,0 +1,50 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_OOPS_OOPCAST_INLINE_HPP\n+#define SHARE_OOPS_OOPCAST_INLINE_HPP\n+\n+#include \"oops\/oopsHierarchy.hpp\"\n+#include \"oops\/oop.inline.hpp\"\n+\n+template<typename T>\n+static bool is_oop_type(oop theOop) {\n+  static_assert(sizeof(T) == 0, \"No is_oop_type specialization found for this type\");\n+  return false;\n+}\n+template<>\n+inline bool is_oop_type<instanceOop>(oop theOop) { return theOop->is_instance(); }\n+template<>\n+inline bool is_oop_type<arrayOop>(oop theOop) { return theOop->is_array(); }\n+template<>\n+inline bool is_oop_type<objArrayOop>(oop theOop) { return theOop->is_objArray(); }\n+template<>\n+inline bool is_oop_type<typeArrayOop>(oop theOop) { return theOop->is_typeArray(); }\n+\n+template<typename R>\n+R oop_cast(oop theOop) {\n+  assert(is_oop_type<R>(theOop), \"Invalid cast\");\n+  return (R) theOop;\n+}\n+\n+#endif \/\/ SHARE_OOPS_OOPCAST_INLINE_HPP\n","filename":"src\/hotspot\/share\/oops\/oopCast.inline.hpp","additions":50,"deletions":0,"binary":false,"changes":50,"status":"added"},{"patch":"@@ -43,1 +43,0 @@\n-#include \"ci\/ciNativeEntryPoint.hpp\"\n@@ -1006,25 +1005,0 @@\n-class NativeCallGenerator : public CallGenerator {\n-private:\n-  address _call_addr;\n-  ciNativeEntryPoint* _nep;\n-public:\n-  NativeCallGenerator(ciMethod* m, address call_addr, ciNativeEntryPoint* nep)\n-   : CallGenerator(m), _call_addr(call_addr), _nep(nep) {}\n-\n-  virtual JVMState* generate(JVMState* jvms);\n-};\n-\n-JVMState* NativeCallGenerator::generate(JVMState* jvms) {\n-  GraphKit kit(jvms);\n-\n-  Node* call = kit.make_native_call(_call_addr, tf(), method()->arg_size(), _nep); \/\/ -fallback, - nep\n-  if (call == NULL) return NULL;\n-\n-  kit.C->print_inlining_update(this);\n-  if (kit.C->log() != NULL) {\n-    kit.C->log()->elem(\"l2n_intrinsification_success bci='%d' entry_point='\" INTPTR_FORMAT \"'\", jvms->bci(), p2i(_call_addr));\n-  }\n-\n-  return kit.transfer_exceptions_into_jvms();\n-}\n-\n@@ -1157,16 +1131,2 @@\n-    {\n-      Node* addr_n = kit.argument(1); \/\/ target address\n-      Node* nep_n = kit.argument(callee->arg_size() - 1); \/\/ NativeEntryPoint\n-      \/\/ This check needs to be kept in sync with the one in CallStaticJavaNode::Ideal\n-      if (addr_n->Opcode() == Op_ConL && nep_n->Opcode() == Op_ConP) {\n-        input_not_const = false;\n-        const TypeLong* addr_t = addr_n->bottom_type()->is_long();\n-        const TypeOopPtr* nep_t = nep_n->bottom_type()->is_oopptr();\n-        address addr = (address) addr_t->get_con();\n-        ciNativeEntryPoint* nep = nep_t->const_oop()->as_native_entry_point();\n-        return new NativeCallGenerator(callee, addr, nep);\n-      } else {\n-        print_inlining_failure(C, callee, jvms->depth() - 1, jvms->bci(),\n-                               \"NativeEntryPoint not constant\");\n-      }\n-    }\n+    print_inlining_failure(C, callee, jvms->depth() - 1, jvms->bci(),\n+                           \"native call\");\n@@ -1182,1 +1142,0 @@\n-\n","filename":"src\/hotspot\/share\/opto\/callGenerator.cpp","additions":2,"deletions":43,"binary":false,"changes":45,"status":"modified"},{"patch":"@@ -1086,5 +1086,1 @@\n-      if (in(TypeFunc::Parms + callee->arg_size() - 1)->Opcode() == Op_ConP \/* NEP *\/\n-          && in(TypeFunc::Parms + 1)->Opcode() == Op_ConL \/* address *\/) {\n-        phase->C->prepend_late_inline(cg);\n-        set_generator(NULL);\n-      }\n+      \/\/ never retry\n@@ -1226,65 +1222,0 @@\n-\/\/=============================================================================\n-uint CallNativeNode::size_of() const { return sizeof(*this); }\n-bool CallNativeNode::cmp( const Node &n ) const {\n-  CallNativeNode &call = (CallNativeNode&)n;\n-  return CallNode::cmp(call) && !strcmp(_name,call._name)\n-    && _arg_regs == call._arg_regs && _ret_regs == call._ret_regs;\n-}\n-Node* CallNativeNode::match(const ProjNode *proj, const Matcher *matcher) {\n-  switch (proj->_con) {\n-    case TypeFunc::Control:\n-    case TypeFunc::I_O:\n-    case TypeFunc::Memory:\n-      return new MachProjNode(this,proj->_con,RegMask::Empty,MachProjNode::unmatched_proj);\n-    case TypeFunc::ReturnAdr:\n-    case TypeFunc::FramePtr:\n-      ShouldNotReachHere();\n-    case TypeFunc::Parms: {\n-      const Type* field_at_con = tf()->range()->field_at(proj->_con);\n-      const BasicType bt = field_at_con->basic_type();\n-      OptoReg::Name optoreg = OptoReg::as_OptoReg(_ret_regs.at(proj->_con - TypeFunc::Parms));\n-      OptoRegPair regs;\n-      if (bt == T_DOUBLE || bt == T_LONG) {\n-        regs.set2(optoreg);\n-      } else {\n-        regs.set1(optoreg);\n-      }\n-      RegMask rm = RegMask(regs.first());\n-      if(OptoReg::is_valid(regs.second()))\n-        rm.Insert(regs.second());\n-      return new MachProjNode(this, proj->_con, rm, field_at_con->ideal_reg());\n-    }\n-    case TypeFunc::Parms + 1: {\n-      assert(tf()->range()->field_at(proj->_con) == Type::HALF, \"Expected HALF\");\n-      assert(_ret_regs.at(proj->_con - TypeFunc::Parms) == VMRegImpl::Bad(), \"Unexpected register for Type::HALF\");\n-      \/\/ 2nd half of doubles and longs\n-      return new MachProjNode(this, proj->_con, RegMask::Empty, (uint) OptoReg::Bad);\n-    }\n-    default:\n-      ShouldNotReachHere();\n-  }\n-  return NULL;\n-}\n-#ifndef PRODUCT\n-void CallNativeNode::print_regs(const GrowableArray<VMReg>& regs, outputStream* st) {\n-  st->print(\"{ \");\n-  for (int i = 0; i < regs.length(); i++) {\n-    regs.at(i)->print_on(st);\n-    if (i < regs.length() - 1) {\n-      st->print(\", \");\n-    }\n-  }\n-  st->print(\" } \");\n-}\n-\n-void CallNativeNode::dump_spec(outputStream *st) const {\n-  st->print(\"# \");\n-  st->print(\"%s \", _name);\n-  st->print(\"_arg_regs: \");\n-  print_regs(_arg_regs, st);\n-  st->print(\"_ret_regs: \");\n-  print_regs(_ret_regs, st);\n-  CallNode::dump_spec(st);\n-}\n-#endif\n-\n@@ -1311,34 +1242,0 @@\n-void CallNativeNode::calling_convention( BasicType* sig_bt, VMRegPair *parm_regs, uint argcnt ) const {\n-  assert((tf()->domain()->cnt() - TypeFunc::Parms) == argcnt, \"arg counts must match!\");\n-#ifdef ASSERT\n-  for (uint i = 0; i < argcnt; i++) {\n-    assert(tf()->domain()->field_at(TypeFunc::Parms + i)->basic_type() == sig_bt[i], \"types must match!\");\n-  }\n-#endif\n-  for (uint i = 0; i < argcnt; i++) {\n-    switch (sig_bt[i]) {\n-      case T_BOOLEAN:\n-      case T_CHAR:\n-      case T_BYTE:\n-      case T_SHORT:\n-      case T_INT:\n-      case T_FLOAT:\n-        parm_regs[i].set1(_arg_regs.at(i));\n-        break;\n-      case T_LONG:\n-      case T_DOUBLE:\n-        assert((i + 1) < argcnt && sig_bt[i + 1] == T_VOID, \"expecting half\");\n-        parm_regs[i].set2(_arg_regs.at(i));\n-        break;\n-      case T_VOID: \/\/ Halves of longs and doubles\n-        assert(i != 0 && (sig_bt[i - 1] == T_LONG || sig_bt[i - 1] == T_DOUBLE), \"expecting half\");\n-        assert(_arg_regs.at(i) == VMRegImpl::Bad(), \"expecting bad reg\");\n-        parm_regs[i].set_bad();\n-        break;\n-      default:\n-        ShouldNotReachHere();\n-        break;\n-    }\n-  }\n-}\n-\n","filename":"src\/hotspot\/share\/opto\/callnode.cpp","additions":1,"deletions":104,"binary":false,"changes":105,"status":"modified"},{"patch":"@@ -52,1 +52,0 @@\n-class     CallNativeNode;\n@@ -825,36 +824,0 @@\n-\/\/------------------------------CallNativeNode-----------------------------------\n-\/\/ Make a direct call into a foreign function with an arbitrary ABI\n-\/\/ safepoints\n-class CallNativeNode : public CallNode {\n-  friend class MachCallNativeNode;\n-  virtual bool cmp( const Node &n ) const;\n-  virtual uint size_of() const;\n-  static void print_regs(const GrowableArray<VMReg>& regs, outputStream* st);\n-public:\n-  GrowableArray<VMReg> _arg_regs;\n-  GrowableArray<VMReg> _ret_regs;\n-  const int _shadow_space_bytes;\n-  const bool _need_transition;\n-\n-  CallNativeNode(const TypeFunc* tf, address addr, const char* name,\n-                 const TypePtr* adr_type,\n-                 const GrowableArray<VMReg>& arg_regs,\n-                 const GrowableArray<VMReg>& ret_regs,\n-                 int shadow_space_bytes,\n-                 bool need_transition)\n-    : CallNode(tf, addr, adr_type), _arg_regs(arg_regs),\n-      _ret_regs(ret_regs), _shadow_space_bytes(shadow_space_bytes),\n-      _need_transition(need_transition)\n-  {\n-    init_class_id(Class_CallNative);\n-    _name = name;\n-  }\n-  virtual int   Opcode() const;\n-  virtual bool  guaranteed_safepoint()  { return _need_transition; }\n-  virtual Node* match(const ProjNode *proj, const Matcher *m);\n-  virtual void  calling_convention( BasicType* sig_bt, VMRegPair *parm_regs, uint argcnt ) const;\n-#ifndef PRODUCT\n-  virtual void  dump_spec(outputStream *st) const;\n-#endif\n-};\n-\n","filename":"src\/hotspot\/share\/opto\/callnode.hpp","additions":0,"deletions":37,"binary":false,"changes":37,"status":"modified"},{"patch":"@@ -64,1 +64,0 @@\n-macro(CallNative)\n","filename":"src\/hotspot\/share\/opto\/classes.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -625,1 +625,0 @@\n-                  _native_invokers(comp_arena(), 1, 0, NULL),\n@@ -899,1 +898,0 @@\n-    _native_invokers(),\n@@ -3066,1 +3064,0 @@\n-  case Op_CallNative:\n@@ -5065,4 +5062,0 @@\n-void Compile::add_native_invoker(RuntimeStub* stub) {\n-  _native_invokers.append(stub);\n-}\n-\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":0,"deletions":7,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -429,2 +429,0 @@\n-  GrowableArray<RuntimeStub*>   _native_invokers;\n-\n@@ -975,4 +973,0 @@\n-  void add_native_invoker(RuntimeStub* stub);\n-\n-  const GrowableArray<RuntimeStub*> native_invokers() const { return _native_invokers; }\n-\n","filename":"src\/hotspot\/share\/opto\/compile.hpp","additions":0,"deletions":6,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -28,1 +28,0 @@\n-#include \"ci\/ciNativeEntryPoint.hpp\"\n@@ -2565,114 +2564,0 @@\n-\/\/-----------------------------make_native_call-------------------------------\n-Node* GraphKit::make_native_call(address call_addr, const TypeFunc* call_type, uint nargs, ciNativeEntryPoint* nep) {\n-  \/\/ Select just the actual call args to pass on\n-  \/\/ [MethodHandle fallback, long addr, HALF addr, ... args , NativeEntryPoint nep]\n-  \/\/                                             |          |\n-  \/\/                                             V          V\n-  \/\/                                             [ ... args ]\n-  uint n_filtered_args = nargs - 4; \/\/ -fallback, -addr (2), -nep;\n-  ResourceMark rm;\n-  Node** argument_nodes = NEW_RESOURCE_ARRAY(Node*, n_filtered_args);\n-  const Type** arg_types = TypeTuple::fields(n_filtered_args);\n-  GrowableArray<VMReg> arg_regs(C->comp_arena(), n_filtered_args, n_filtered_args, VMRegImpl::Bad());\n-\n-  VMReg* argRegs = nep->argMoves();\n-  {\n-    for (uint vm_arg_pos = 0, java_arg_read_pos = 0;\n-        vm_arg_pos < n_filtered_args; vm_arg_pos++) {\n-      uint vm_unfiltered_arg_pos = vm_arg_pos + 3; \/\/ +3 to skip fallback handle argument and addr (2 since long)\n-      Node* node = argument(vm_unfiltered_arg_pos);\n-      const Type* type = call_type->domain()->field_at(TypeFunc::Parms + vm_unfiltered_arg_pos);\n-      VMReg reg = type == Type::HALF\n-        ? VMRegImpl::Bad()\n-        : argRegs[java_arg_read_pos++];\n-\n-      argument_nodes[vm_arg_pos] = node;\n-      arg_types[TypeFunc::Parms + vm_arg_pos] = type;\n-      arg_regs.at_put(vm_arg_pos, reg);\n-    }\n-  }\n-\n-  uint n_returns = call_type->range()->cnt() - TypeFunc::Parms;\n-  GrowableArray<VMReg> ret_regs(C->comp_arena(), n_returns, n_returns, VMRegImpl::Bad());\n-  const Type** ret_types = TypeTuple::fields(n_returns);\n-\n-  VMReg* retRegs = nep->returnMoves();\n-  {\n-    for (uint vm_ret_pos = 0, java_ret_read_pos = 0;\n-        vm_ret_pos < n_returns; vm_ret_pos++) { \/\/ 0 or 1\n-      const Type* type = call_type->range()->field_at(TypeFunc::Parms + vm_ret_pos);\n-      VMReg reg = type == Type::HALF\n-        ? VMRegImpl::Bad()\n-        : retRegs[java_ret_read_pos++];\n-\n-      ret_regs.at_put(vm_ret_pos, reg);\n-      ret_types[TypeFunc::Parms + vm_ret_pos] = type;\n-    }\n-  }\n-\n-  const TypeFunc* new_call_type = TypeFunc::make(\n-    TypeTuple::make(TypeFunc::Parms + n_filtered_args, arg_types),\n-    TypeTuple::make(TypeFunc::Parms + n_returns, ret_types)\n-  );\n-\n-  if (nep->need_transition()) {\n-    RuntimeStub* invoker = SharedRuntime::make_native_invoker(call_addr,\n-                                                              nep->shadow_space(),\n-                                                              arg_regs, ret_regs);\n-    if (invoker == NULL) {\n-      C->record_failure(\"native invoker not implemented on this platform\");\n-      return NULL;\n-    }\n-    C->add_native_invoker(invoker);\n-    call_addr = invoker->code_begin();\n-  }\n-  assert(call_addr != NULL, \"sanity\");\n-\n-  CallNativeNode* call = new CallNativeNode(new_call_type, call_addr, nep->name(), TypePtr::BOTTOM,\n-                                            arg_regs,\n-                                            ret_regs,\n-                                            nep->shadow_space(),\n-                                            nep->need_transition());\n-\n-  if (call->_need_transition) {\n-    add_safepoint_edges(call);\n-  }\n-\n-  set_predefined_input_for_runtime_call(call);\n-\n-  for (uint i = 0; i < n_filtered_args; i++) {\n-    call->init_req(i + TypeFunc::Parms, argument_nodes[i]);\n-  }\n-\n-  Node* c = gvn().transform(call);\n-  assert(c == call, \"cannot disappear\");\n-\n-  set_predefined_output_for_runtime_call(call);\n-\n-  Node* ret;\n-  if (method() == NULL || method()->return_type()->basic_type() == T_VOID) {\n-    ret = top();\n-  } else {\n-    ret =  gvn().transform(new ProjNode(call, TypeFunc::Parms));\n-    \/\/ Unpack native results if needed\n-    \/\/ Need this method type since it's unerased\n-    switch (nep->method_type()->rtype()->basic_type()) {\n-      case T_CHAR:\n-        ret = _gvn.transform(new AndINode(ret, _gvn.intcon(0xFFFF)));\n-        break;\n-      case T_BYTE:\n-        ret = sign_extend_byte(ret);\n-        break;\n-      case T_SHORT:\n-        ret = sign_extend_short(ret);\n-        break;\n-      default: \/\/ do nothing\n-        break;\n-    }\n-  }\n-\n-  push_node(method()->return_type()->basic_type(), ret);\n-\n-  return call;\n-}\n-\n","filename":"src\/hotspot\/share\/opto\/graphKit.cpp","additions":0,"deletions":115,"binary":false,"changes":115,"status":"modified"},{"patch":"@@ -803,2 +803,0 @@\n-  Node* make_native_call(address call_addr, const TypeFunc* call_type, uint nargs, ciNativeEntryPoint* nep);\n-\n","filename":"src\/hotspot\/share\/opto\/graphKit.hpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -902,6 +902,0 @@\n-    case Op_CallNative:\n-      \/\/ We use the c reg save policy here since Foreign Linker\n-      \/\/ only supports the C ABI currently.\n-      \/\/ TODO compute actual save policy based on nep->abi\n-      save_policy = _matcher._c_reg_save_policy;\n-      break;\n@@ -921,8 +915,1 @@\n-  \/\/\n-  \/\/ Also, native callees can not save oops, so we kill the SOE registers\n-  \/\/ here in case a native call has a safepoint. This doesn't work for\n-  \/\/ RBP though, which seems to be special-cased elsewhere to always be\n-  \/\/ treated as alive, so we instead manually save the location of RBP\n-  \/\/ before doing the native call (see NativeInvokerGenerator::generate).\n-  bool exclude_soe = op == Op_CallRuntime\n-    || (op == Op_CallNative && mcall->guaranteed_safepoint());\n+  bool exclude_soe = op == Op_CallRuntime;\n","filename":"src\/hotspot\/share\/opto\/lcm.cpp","additions":1,"deletions":14,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -823,17 +823,0 @@\n-uint MachCallNativeNode::size_of() const { return sizeof(*this); }\n-bool MachCallNativeNode::cmp( const Node &n ) const {\n-  MachCallNativeNode &call = (MachCallNativeNode&)n;\n-  return MachCallNode::cmp(call) && !strcmp(_name,call._name)\n-    && _arg_regs == call._arg_regs && _ret_regs == call._ret_regs;\n-}\n-#ifndef PRODUCT\n-void MachCallNativeNode::dump_spec(outputStream *st) const {\n-  st->print(\"%s \",_name);\n-  st->print(\"_arg_regs: \");\n-  CallNativeNode::print_regs(_arg_regs, st);\n-  st->print(\"_ret_regs: \");\n-  CallNativeNode::print_regs(_ret_regs, st);\n-  MachCallNode::dump_spec(st);\n-}\n-#endif\n-\/\/=============================================================================\n","filename":"src\/hotspot\/share\/opto\/machnode.cpp","additions":0,"deletions":17,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -42,1 +42,0 @@\n-class MachCallNativeNode;\n@@ -1019,19 +1018,0 @@\n-class MachCallNativeNode: public MachCallNode {\n-  virtual bool cmp( const Node &n ) const;\n-  virtual uint size_of() const;\n-  void print_regs(const GrowableArray<VMReg>& regs, outputStream* st) const;\n-public:\n-  const char *_name;\n-  GrowableArray<VMReg> _arg_regs;\n-  GrowableArray<VMReg> _ret_regs;\n-\n-  MachCallNativeNode() : MachCallNode() {\n-    init_class_id(Class_MachCallNative);\n-  }\n-\n-  virtual int ret_addr_offset();\n-#ifndef PRODUCT\n-  virtual void dump_spec(outputStream *st) const;\n-#endif\n-};\n-\n","filename":"src\/hotspot\/share\/opto\/machnode.hpp","additions":0,"deletions":20,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -1309,7 +1309,0 @@\n-    else if( mcall->is_MachCallNative() ) {\n-      MachCallNativeNode* mach_call_native = mcall->as_MachCallNative();\n-      CallNativeNode* call_native = call->as_CallNative();\n-      mach_call_native->_name = call_native->_name;\n-      mach_call_native->_arg_regs = call_native->_arg_regs;\n-      mach_call_native->_ret_regs = call_native->_ret_regs;\n-    }\n@@ -1350,2 +1343,0 @@\n-  if( call != NULL && call->is_CallNative() )\n-    out_arg_limit_per_call = OptoReg::add(out_arg_limit_per_call, call->as_CallNative()->_shadow_space_bytes);\n","filename":"src\/hotspot\/share\/opto\/matcher.cpp","additions":0,"deletions":9,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -58,1 +58,0 @@\n-class CallNativeNode;\n@@ -107,1 +106,0 @@\n-class MachCallNativeNode;\n@@ -650,1 +648,0 @@\n-          DEFINE_CLASS_ID(CallNative,       Call, 5)\n@@ -676,1 +673,0 @@\n-            DEFINE_CLASS_ID(MachCallNative,       MachCall, 2)\n@@ -848,1 +844,0 @@\n-  DEFINE_CLASS_QUERY(CallNative)\n@@ -894,1 +889,0 @@\n-  DEFINE_CLASS_QUERY(MachCallNative)\n","filename":"src\/hotspot\/share\/opto\/node.hpp","additions":0,"deletions":6,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -999,1 +999,0 @@\n-  bool is_opt_native = false;\n@@ -1018,2 +1017,0 @@\n-    } else if (mcall->is_MachCallNative()) {\n-      is_opt_native = true;\n@@ -1148,1 +1145,0 @@\n-      is_opt_native,\n@@ -3367,2 +3363,1 @@\n-                                     C->rtm_state(),\n-                                     C->native_invokers());\n+                                     C->rtm_state());\n","filename":"src\/hotspot\/share\/opto\/output.cpp","additions":1,"deletions":6,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -26,3 +26,1 @@\n-#include \"classfile\/symbolTable.hpp\"\n-#include \"classfile\/systemDictionary.hpp\"\n-#include \"classfile\/vmSymbols.hpp\"\n+#include \"classfile\/javaClasses.hpp\"\n@@ -30,2 +28,2 @@\n-#include \"runtime\/fieldDescriptor.hpp\"\n-#include \"runtime\/fieldDescriptor.inline.hpp\"\n+#include \"prims\/foreign_globals.inline.hpp\"\n+#include \"runtime\/jniHandles.inline.hpp\"\n@@ -35,7 +33,7 @@\n-static int field_offset(InstanceKlass* cls, const char* fieldname, Symbol* sigsym) {\n-  TempNewSymbol fieldnamesym = SymbolTable::new_symbol(fieldname, (int)strlen(fieldname));\n-  fieldDescriptor fd;\n-  bool success = cls->find_field(fieldnamesym, sigsym, false, &fd);\n-  assert(success, \"Field not found\");\n-  return fd.offset();\n-}\n+const CallRegs ForeignGlobals::parse_call_regs(jobject jconv) {\n+  oop conv_oop = JNIHandles::resolve_non_null(jconv);\n+  objArrayOop arg_regs_oop = jdk_internal_foreign_abi_CallConv::argRegs(conv_oop);\n+  objArrayOop ret_regs_oop = jdk_internal_foreign_abi_CallConv::retRegs(conv_oop);\n+  CallRegs result;\n+  result._args_length = arg_regs_oop->length();\n+  result._arg_regs = NEW_RESOURCE_ARRAY(VMReg, result._args_length);\n@@ -43,6 +41,10 @@\n-static InstanceKlass* find_InstanceKlass(const char* name, TRAPS) {\n-  TempNewSymbol sym = SymbolTable::new_symbol(name, (int)strlen(name));\n-  Klass* k = SystemDictionary::resolve_or_null(sym, Handle(), Handle(), THREAD);\n-  assert(k != nullptr, \"Can not find class: %s\", name);\n-  return InstanceKlass::cast(k);\n-}\n+  result._rets_length = ret_regs_oop->length();\n+  result._ret_regs = NEW_RESOURCE_ARRAY(VMReg, result._rets_length);\n+\n+  for (int i = 0; i < result._args_length; i++) {\n+    result._arg_regs[i] = parse_vmstorage(arg_regs_oop->obj_at(i));\n+  }\n+\n+  for (int i = 0; i < result._rets_length; i++) {\n+    result._ret_regs[i] = parse_vmstorage(ret_regs_oop->obj_at(i));\n+  }\n@@ -50,3 +52,1 @@\n-const ForeignGlobals& ForeignGlobals::instance() {\n-  static ForeignGlobals globals; \/\/ thread-safe lazy init-once (since C++11)\n-  return globals;\n+  return result;\n@@ -55,2 +55,4 @@\n-const ABIDescriptor ForeignGlobals::parse_abi_descriptor(jobject jabi) {\n-  return instance().parse_abi_descriptor_impl(jabi);\n+VMReg ForeignGlobals::parse_vmstorage(oop storage) {\n+  jint index = jdk_internal_foreign_abi_VMStorage::index(storage);\n+  jint type = jdk_internal_foreign_abi_VMStorage::type(storage);\n+  return vmstorage_to_vmreg(type, index);\n@@ -58,2 +60,7 @@\n-const BufferLayout ForeignGlobals::parse_buffer_layout(jobject jlayout) {\n-  return instance().parse_buffer_layout_impl(jlayout);\n+\n+int RegSpiller::compute_spill_area(const VMReg* regs, int num_regs) {\n+  int result_size = 0;\n+  for (int i = 0; i < num_regs; i++) {\n+    result_size += pd_reg_size(regs[i]);\n+  }\n+  return result_size;\n@@ -62,2 +69,12 @@\n-const CallRegs ForeignGlobals::parse_call_regs(jobject jconv) {\n-  return instance().parse_call_regs_impl(jconv);\n+void RegSpiller::generate(MacroAssembler* masm, int rsp_offset, bool spill) const {\n+  assert(rsp_offset != -1, \"rsp_offset should be set\");\n+  int offset = rsp_offset;\n+  for (int i = 0; i < _num_regs; i++) {\n+    VMReg reg = _regs[i];\n+    if (spill) {\n+      pd_store_reg(masm, offset, reg);\n+    } else {\n+      pd_load_reg(masm, offset, reg);\n+    }\n+    offset += pd_reg_size(reg);\n+  }\n@@ -66,34 +83,20 @@\n-ForeignGlobals::ForeignGlobals() {\n-  JavaThread* current_thread = JavaThread::current();\n-  ResourceMark rm(current_thread);\n-\n-  \/\/ ABIDescriptor\n-  InstanceKlass* k_ABI = find_InstanceKlass(FOREIGN_ABI \"ABIDescriptor\", current_thread);\n-  const char* strVMSArrayArray = \"[[L\" FOREIGN_ABI \"VMStorage;\";\n-  Symbol* symVMSArrayArray = SymbolTable::new_symbol(strVMSArrayArray);\n-  ABI.inputStorage_offset = field_offset(k_ABI, \"inputStorage\", symVMSArrayArray);\n-  ABI.outputStorage_offset = field_offset(k_ABI, \"outputStorage\", symVMSArrayArray);\n-  ABI.volatileStorage_offset = field_offset(k_ABI, \"volatileStorage\", symVMSArrayArray);\n-  ABI.stackAlignment_offset = field_offset(k_ABI, \"stackAlignment\", vmSymbols::int_signature());\n-  ABI.shadowSpace_offset = field_offset(k_ABI, \"shadowSpace\", vmSymbols::int_signature());\n-\n-  \/\/ VMStorage\n-  InstanceKlass* k_VMS = find_InstanceKlass(FOREIGN_ABI \"VMStorage\", current_thread);\n-  VMS.index_offset = field_offset(k_VMS, \"index\", vmSymbols::int_signature());\n-  VMS.type_offset = field_offset(k_VMS, \"type\", vmSymbols::int_signature());\n-\n-  \/\/ BufferLayout\n-  InstanceKlass* k_BL = find_InstanceKlass(FOREIGN_ABI \"BufferLayout\", current_thread);\n-  BL.size_offset = field_offset(k_BL, \"size\", vmSymbols::long_signature());\n-  BL.arguments_next_pc_offset = field_offset(k_BL, \"arguments_next_pc\", vmSymbols::long_signature());\n-  BL.stack_args_bytes_offset = field_offset(k_BL, \"stack_args_bytes\", vmSymbols::long_signature());\n-  BL.stack_args_offset = field_offset(k_BL, \"stack_args\", vmSymbols::long_signature());\n-  BL.input_type_offsets_offset = field_offset(k_BL, \"input_type_offsets\", vmSymbols::long_array_signature());\n-  BL.output_type_offsets_offset = field_offset(k_BL, \"output_type_offsets\", vmSymbols::long_array_signature());\n-\n-  \/\/ CallRegs\n-  const char* strVMSArray = \"[L\" FOREIGN_ABI \"VMStorage;\";\n-  Symbol* symVMSArray = SymbolTable::new_symbol(strVMSArray);\n-  InstanceKlass* k_CC = find_InstanceKlass(FOREIGN_ABI \"ProgrammableUpcallHandler$CallRegs\", current_thread);\n-  CallConvOffsets.arg_regs_offset = field_offset(k_CC, \"argRegs\", symVMSArray);\n-  CallConvOffsets.ret_regs_offset = field_offset(k_CC, \"retRegs\", symVMSArray);\n+void ArgumentShuffle::print_on(outputStream* os) const {\n+  os->print_cr(\"Argument shuffle {\");\n+  for (int i = 0; i < _moves.length(); i++) {\n+    Move move = _moves.at(i);\n+    BasicType arg_bt     = move.bt;\n+    VMRegPair from_vmreg = move.from;\n+    VMRegPair to_vmreg   = move.to;\n+\n+    os->print(\"Move a %s from (\", null_safe_string(type2name(arg_bt)));\n+    from_vmreg.first()->print_on(os);\n+    os->print(\",\");\n+    from_vmreg.second()->print_on(os);\n+    os->print(\") to (\");\n+    to_vmreg.first()->print_on(os);\n+    os->print(\",\");\n+    to_vmreg.second()->print_on(os);\n+    os->print_cr(\")\");\n+  }\n+  os->print_cr(\"Stack argument slots: %d\", _out_arg_stack_slots);\n+  os->print_cr(\"}\");\n@@ -102,1 +105,1 @@\n-void CallRegs::calling_convention(BasicType* sig_bt, VMRegPair *parm_regs, uint argcnt) const {\n+int NativeCallConv::calling_convention(BasicType* sig_bt, VMRegPair* out_regs, int num_args) const {\n@@ -104,1 +107,2 @@\n-  for (uint i = 0; i < argcnt; i++) {\n+  int stk_slots = 0;\n+  for (int i = 0; i < num_args; i++) {\n@@ -111,3 +115,6 @@\n-      case T_FLOAT:\n-        assert(src_pos < _args_length, \"oob\");\n-        parm_regs[i].set1(_arg_regs[src_pos++]);\n+      case T_FLOAT: {\n+        assert(src_pos < _input_regs_length, \"oob\");\n+        VMReg reg = _input_regs[src_pos++];\n+        out_regs[i].set1(reg);\n+        if (reg->is_stack())\n+          stk_slots += 2;\n@@ -115,0 +122,1 @@\n+      }\n@@ -116,4 +124,7 @@\n-      case T_DOUBLE:\n-        assert((i + 1) < argcnt && sig_bt[i + 1] == T_VOID, \"expecting half\");\n-        assert(src_pos < _args_length, \"oob\");\n-        parm_regs[i].set2(_arg_regs[src_pos++]);\n+      case T_DOUBLE: {\n+        assert((i + 1) < num_args && sig_bt[i + 1] == T_VOID, \"expecting half\");\n+        assert(src_pos < _input_regs_length, \"oob\");\n+        VMReg reg = _input_regs[src_pos++];\n+        out_regs[i].set2(reg);\n+        if (reg->is_stack())\n+          stk_slots += 2;\n@@ -121,0 +132,1 @@\n+      }\n@@ -123,1 +135,1 @@\n-        parm_regs[i].set_bad();\n+        out_regs[i].set_bad();\n@@ -130,0 +142,192 @@\n+  return stk_slots;\n+}\n+\n+\/\/ based on ComputeMoveOrder from x86_64 shared runtime code.\n+\/\/ with some changes.\n+class ForeignCMO: public StackObj {\n+  class MoveOperation: public ResourceObj {\n+    friend class ForeignCMO;\n+   private:\n+    VMRegPair        _src;\n+    VMRegPair        _dst;\n+    bool             _processed;\n+    MoveOperation*  _next;\n+    MoveOperation*  _prev;\n+    BasicType        _bt;\n+\n+    static int get_id(VMRegPair r) {\n+      return r.first()->value();\n+    }\n+\n+   public:\n+    MoveOperation(VMRegPair src, VMRegPair dst, BasicType bt):\n+      _src(src)\n+    , _dst(dst)\n+    , _processed(false)\n+    , _next(NULL)\n+    , _prev(NULL)\n+    , _bt(bt) {\n+    }\n+\n+    int src_id() const          { return get_id(_src); }\n+    int dst_id() const          { return get_id(_dst); }\n+    MoveOperation* next() const { return _next; }\n+    MoveOperation* prev() const { return _prev; }\n+    void set_processed()        { _processed = true; }\n+    bool is_processed() const   { return _processed; }\n+\n+    \/\/ insert\n+    void break_cycle(VMRegPair temp_register) {\n+      \/\/ create a new store following the last store\n+      \/\/ to move from the temp_register to the original\n+      MoveOperation* new_store = new MoveOperation(temp_register, _dst, _bt);\n+\n+      \/\/ break the cycle of links and insert new_store at the end\n+      \/\/ break the reverse link.\n+      MoveOperation* p = prev();\n+      assert(p->next() == this, \"must be\");\n+      _prev = NULL;\n+      p->_next = new_store;\n+      new_store->_prev = p;\n+\n+      \/\/ change the original store to save it's value in the temp.\n+      _dst = temp_register;\n+    }\n+\n+    void link(GrowableArray<MoveOperation*>& killer) {\n+      \/\/ link this store in front the store that it depends on\n+      MoveOperation* n = killer.at_grow(src_id(), NULL);\n+      if (n != NULL) {\n+        assert(_next == NULL && n->_prev == NULL, \"shouldn't have been set yet\");\n+        _next = n;\n+        n->_prev = this;\n+      }\n+    }\n+\n+    Move as_move() {\n+      return {_bt, _src, _dst};\n+    }\n+  };\n+\n+ private:\n+  GrowableArray<MoveOperation*> _edges;\n+  GrowableArray<Move> _moves;\n+\n+ public:\n+  ForeignCMO(int total_in_args, const VMRegPair* in_regs, int total_out_args, VMRegPair* out_regs,\n+             const BasicType* in_sig_bt, VMRegPair tmp_vmreg) : _edges(total_in_args), _moves(total_in_args) {\n+    assert(total_out_args >= total_in_args, \"can only add prefix args\");\n+    \/\/ Note that total_out_args args can be greater than total_in_args in the case of upcalls.\n+    \/\/ There will be a leading MH receiver arg in the out args in that case.\n+    \/\/\n+    \/\/ Leading args in the out args will be ignored below because we iterate from the end of\n+    \/\/ the register arrays until !(in_idx >= 0), and total_in_args is smaller.\n+    \/\/\n+    \/\/ Stub code adds a move for the receiver to j_rarg0 (and potential other prefix args) manually.\n+    for (int in_idx = total_in_args - 1, out_idx = total_out_args - 1; in_idx >= 0; in_idx--, out_idx--) {\n+      BasicType bt = in_sig_bt[in_idx];\n+      assert(bt != T_ARRAY, \"array not expected\");\n+      VMRegPair in_reg = in_regs[in_idx];\n+      VMRegPair out_reg = out_regs[out_idx];\n+\n+      if (out_reg.first()->is_stack()) {\n+        \/\/ Move operations where the dest is the stack can all be\n+        \/\/ scheduled first since they can't interfere with the other moves.\n+        \/\/ The input and output stack spaces are distinct from each other.\n+        Move move{bt, in_reg, out_reg};\n+        _moves.push(move);\n+      } else if (in_reg.first() == out_reg.first()\n+                 || bt == T_VOID) {\n+        \/\/ 1. Can skip non-stack identity moves.\n+        \/\/\n+        \/\/ 2. Upper half of long or double (T_VOID).\n+        \/\/    Don't need to do anything.\n+        continue;\n+      } else {\n+        _edges.append(new MoveOperation(in_reg, out_reg, bt));\n+      }\n+    }\n+    \/\/ Break any cycles in the register moves and emit the in the\n+    \/\/ proper order.\n+    compute_store_order(tmp_vmreg);\n+  }\n+\n+  \/\/ Walk the edges breaking cycles between moves.  The result list\n+  \/\/ can be walked in order to produce the proper set of loads\n+  void compute_store_order(VMRegPair temp_register) {\n+    \/\/ Record which moves kill which values\n+    GrowableArray<MoveOperation*> killer; \/\/ essentially a map of register id -> MoveOperation*\n+    for (int i = 0; i < _edges.length(); i++) {\n+      MoveOperation* s = _edges.at(i);\n+      assert(killer.at_grow(s->dst_id(), NULL) == NULL,\n+             \"multiple moves with the same register as destination\");\n+      killer.at_put_grow(s->dst_id(), s, NULL);\n+    }\n+    assert(killer.at_grow(MoveOperation::get_id(temp_register), NULL) == NULL,\n+           \"make sure temp isn't in the registers that are killed\");\n+\n+    \/\/ create links between loads and stores\n+    for (int i = 0; i < _edges.length(); i++) {\n+      _edges.at(i)->link(killer);\n+    }\n+\n+    \/\/ at this point, all the move operations are chained together\n+    \/\/ in one or more doubly linked lists.  Processing them backwards finds\n+    \/\/ the beginning of the chain, forwards finds the end.  If there's\n+    \/\/ a cycle it can be broken at any point,  so pick an edge and walk\n+    \/\/ backward until the list ends or we end where we started.\n+    for (int e = 0; e < _edges.length(); e++) {\n+      MoveOperation* s = _edges.at(e);\n+      if (!s->is_processed()) {\n+        MoveOperation* start = s;\n+        \/\/ search for the beginning of the chain or cycle\n+        while (start->prev() != NULL && start->prev() != s) {\n+          start = start->prev();\n+        }\n+        if (start->prev() == s) {\n+          start->break_cycle(temp_register);\n+        }\n+        \/\/ walk the chain forward inserting to store list\n+        while (start != NULL) {\n+          _moves.push(start->as_move());\n+\n+          start->set_processed();\n+          start = start->next();\n+        }\n+      }\n+    }\n+  }\n+\n+  GrowableArray<Move> moves() {\n+    return _moves;\n+  }\n+};\n+\n+ArgumentShuffle::ArgumentShuffle(\n+    BasicType* in_sig_bt,\n+    int num_in_args,\n+    BasicType* out_sig_bt,\n+    int num_out_args,\n+    const CallConvClosure* input_conv,\n+    const CallConvClosure* output_conv,\n+    VMReg shuffle_temp) {\n+\n+  VMRegPair* in_regs = NEW_RESOURCE_ARRAY(VMRegPair, num_in_args);\n+  input_conv->calling_convention(in_sig_bt, in_regs, num_in_args);\n+\n+  VMRegPair* out_regs = NEW_RESOURCE_ARRAY(VMRegPair, num_out_args);\n+  _out_arg_stack_slots = output_conv->calling_convention(out_sig_bt, out_regs, num_out_args);\n+\n+  VMRegPair tmp_vmreg;\n+  tmp_vmreg.set2(shuffle_temp);\n+\n+  \/\/ Compute a valid move order, using tmp_vmreg to break any cycles.\n+  \/\/ Note that ForeignCMO ignores the upper half of our VMRegPairs.\n+  \/\/ We are not moving Java values here, only register-sized values,\n+  \/\/ so we shouldn't have to worry about the upper half any ways.\n+  \/\/ This should work fine on 32-bit as well, since we would only be\n+  \/\/ moving 32-bit sized values (i.e. low-level MH shouldn't take any double\/long).\n+  ForeignCMO order(num_in_args, in_regs,\n+                   num_out_args, out_regs,\n+                   in_sig_bt, tmp_vmreg);\n+  _moves = order.moves();\n","filename":"src\/hotspot\/share\/prims\/foreign_globals.cpp","additions":275,"deletions":71,"binary":false,"changes":346,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"runtime\/sharedRuntime.hpp\"\n@@ -34,0 +35,5 @@\n+class CallConvClosure {\n+public:\n+  virtual int calling_convention(BasicType* sig_bt, VMRegPair* regs, int num_args) const = 0;\n+};\n+\n@@ -40,2 +46,0 @@\n-\n-  void calling_convention(BasicType* sig_bt, VMRegPair *parm_regs, uint argcnt) const;\n@@ -46,34 +50,0 @@\n-  struct {\n-    int inputStorage_offset;\n-    int outputStorage_offset;\n-    int volatileStorage_offset;\n-    int stackAlignment_offset;\n-    int shadowSpace_offset;\n-  } ABI;\n-\n-  struct {\n-    int index_offset;\n-    int type_offset;\n-  } VMS;\n-\n-  struct {\n-    int size_offset;\n-    int arguments_next_pc_offset;\n-    int stack_args_bytes_offset;\n-    int stack_args_offset;\n-    int input_type_offsets_offset;\n-    int output_type_offsets_offset;\n-  } BL;\n-\n-  struct {\n-    int arg_regs_offset;\n-    int ret_regs_offset;\n-  } CallConvOffsets;\n-\n-  ForeignGlobals();\n-\n-  static const ForeignGlobals& instance();\n-\n-  template<typename R>\n-  static R cast(oop theOop);\n-\n@@ -81,1 +51,1 @@\n-  void loadArray(objArrayOop jarray, int type_index, GrowableArray<T>& array, Func converter) const;\n+  static void loadArray(objArrayOop jarray, int type_index, GrowableArray<T>& array, Func converter);\n@@ -83,3 +53,0 @@\n-  const ABIDescriptor parse_abi_descriptor_impl(jobject jabi) const;\n-  const BufferLayout parse_buffer_layout_impl(jobject jlayout) const;\n-  const CallRegs parse_call_regs_impl(jobject jconv) const;\n@@ -88,1 +55,0 @@\n-  static const BufferLayout parse_buffer_layout(jobject jlayout);\n@@ -90,0 +56,77 @@\n+  static VMReg vmstorage_to_vmreg(int type, int index);\n+  static VMReg parse_vmstorage(oop storage);\n+};\n+\n+\n+\n+class JavaCallConv : public CallConvClosure {\n+public:\n+  int calling_convention(BasicType* sig_bt, VMRegPair* regs, int num_args) const override {\n+    return SharedRuntime::java_calling_convention(sig_bt, regs, num_args);\n+  }\n+};\n+\n+class NativeCallConv : public CallConvClosure {\n+  const VMReg* _input_regs;\n+  int _input_regs_length;\n+public:\n+  NativeCallConv(const VMReg* input_regs, int input_regs_length) :\n+    _input_regs(input_regs),\n+    _input_regs_length(input_regs_length) {\n+  }\n+  NativeCallConv(const GrowableArray<VMReg>& input_regs)\n+   : NativeCallConv(input_regs.data(), input_regs.length()) {}\n+\n+  int calling_convention(BasicType* sig_bt, VMRegPair* out_regs, int num_args) const override;\n+};\n+\n+class RegSpiller {\n+  const VMReg* _regs;\n+  int _num_regs;\n+  int _spill_size_bytes;\n+public:\n+  RegSpiller(const VMReg* regs, int num_regs) :\n+    _regs(regs), _num_regs(num_regs),\n+    _spill_size_bytes(compute_spill_area(regs, num_regs)) {\n+  }\n+  RegSpiller(const GrowableArray<VMReg>& regs) : RegSpiller(regs.data(), regs.length()) {\n+  }\n+\n+  int spill_size_bytes() const { return _spill_size_bytes; }\n+  void generate_spill(MacroAssembler* masm, int rsp_offset) const { return generate(masm, rsp_offset, true); }\n+  void generate_fill(MacroAssembler* masm, int rsp_offset) const { return generate(masm, rsp_offset, false); }\n+\n+private:\n+  static int compute_spill_area(const VMReg* regs, int num_regs);\n+  void generate(MacroAssembler* masm, int rsp_offset, bool is_spill) const;\n+\n+  static int pd_reg_size(VMReg reg);\n+  static void pd_store_reg(MacroAssembler* masm, int offset, VMReg reg);\n+  static void pd_load_reg(MacroAssembler* masm, int offset, VMReg reg);\n+};\n+\n+struct Move {\n+  BasicType bt;\n+  VMRegPair from;\n+  VMRegPair to;\n+};\n+\n+class ArgumentShuffle {\n+private:\n+  GrowableArray<Move> _moves;\n+  int _out_arg_stack_slots;\n+public:\n+  ArgumentShuffle(\n+    BasicType* in_sig_bt, int num_in_args,\n+    BasicType* out_sig_bt, int num_out_args,\n+    const CallConvClosure* input_conv, const CallConvClosure* output_conv,\n+    VMReg shuffle_temp);\n+\n+  int out_arg_stack_slots() const { return _out_arg_stack_slots; }\n+  void generate(MacroAssembler* masm, VMReg tmp, int in_stk_bias, int out_stk_bias) const {\n+    pd_generate(masm, tmp, in_stk_bias, out_stk_bias);\n+  }\n+\n+  void print_on(outputStream* os) const;\n+private:\n+  void pd_generate(MacroAssembler* masm, VMReg tmp, int in_stk_bias, int out_stk_bias) const;\n","filename":"src\/hotspot\/share\/prims\/foreign_globals.hpp","additions":84,"deletions":41,"binary":false,"changes":125,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"classfile\/javaClasses.hpp\"\n@@ -31,16 +32,1 @@\n-\n-template<typename T>\n-static bool check_type(oop theOop) {\n-  static_assert(sizeof(T) == 0, \"No check_type specialization found for this type\");\n-  return false;\n-}\n-template<>\n-inline bool check_type<objArrayOop>(oop theOop) { return theOop->is_objArray(); }\n-template<>\n-inline bool check_type<typeArrayOop>(oop theOop) { return theOop->is_typeArray(); }\n-\n-template<typename R>\n-R ForeignGlobals::cast(oop theOop) {\n-  assert(check_type<R>(theOop), \"Invalid cast\");\n-  return (R) theOop;\n-}\n+#include \"oops\/oopCast.inline.hpp\"\n@@ -49,2 +35,2 @@\n-void ForeignGlobals::loadArray(objArrayOop jarray, int type_index, GrowableArray<T>& array, Func converter) const {\n-  objArrayOop subarray = cast<objArrayOop>(jarray->obj_at(type_index));\n+void ForeignGlobals::loadArray(objArrayOop jarray, int type_index, GrowableArray<T>& array, Func converter) {\n+  objArrayOop subarray = oop_cast<objArrayOop>(jarray->obj_at(type_index));\n@@ -54,1 +40,1 @@\n-    jint index = storage->int_field(VMS.index_offset);\n+    jint index = jdk_internal_foreign_abi_VMStorage::index(storage);\n@@ -59,0 +45,4 @@\n+inline const char* null_safe_string(const char* str) {\n+  return str == nullptr ? \"NULL\" : str;\n+}\n+\n","filename":"src\/hotspot\/share\/prims\/foreign_globals.inline.hpp","additions":9,"deletions":19,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"classfile\/javaClasses.inline.hpp\"\n@@ -28,0 +29,7 @@\n+#include \"logging\/logStream.hpp\"\n+#include \"memory\/resourceArea.hpp\"\n+#include \"oops\/typeArrayOop.inline.hpp\"\n+#include \"oops\/oopCast.inline.hpp\"\n+#include \"prims\/foreign_globals.inline.hpp\"\n+#include \"prims\/universalNativeInvoker.hpp\"\n+#include \"runtime\/jniHandles.inline.hpp\"\n@@ -29,2 +37,41 @@\n-JNI_LEAF(jlong, NEP_vmStorageToVMReg(JNIEnv* env, jclass _unused, jint type, jint index))\n-  return VMRegImpl::vmStorageToVMReg(type, index)->value();\n+JNI_ENTRY(jlong, NEP_makeInvoker(JNIEnv* env, jclass _unused, jobject method_type, jobject jabi,\n+                                 jobjectArray arg_moves, jobjectArray ret_moves, jboolean needs_return_buffer))\n+  ResourceMark rm;\n+  const ABIDescriptor abi = ForeignGlobals::parse_abi_descriptor(jabi);\n+\n+  oop type = JNIHandles::resolve(method_type);\n+  objArrayOop arg_moves_oop = oop_cast<objArrayOop>(JNIHandles::resolve(arg_moves));\n+  objArrayOop ret_moves_oop = oop_cast<objArrayOop>(JNIHandles::resolve(ret_moves));\n+  int pcount = java_lang_invoke_MethodType::ptype_count(type);\n+  int pslots = java_lang_invoke_MethodType::ptype_slot_count(type);\n+  BasicType* basic_type = NEW_RESOURCE_ARRAY(BasicType, pslots);\n+\n+  GrowableArray<VMReg> input_regs(pcount);\n+  for (int i = 0, bt_idx = 0; i < pcount; i++) {\n+    oop type_oop = java_lang_invoke_MethodType::ptype(type, i);\n+    assert(java_lang_Class::is_primitive(type_oop), \"Only primitives expected\");\n+    BasicType bt = java_lang_Class::primitive_type(type_oop);\n+    basic_type[bt_idx++] = bt;\n+    input_regs.push(ForeignGlobals::parse_vmstorage(arg_moves_oop->obj_at(i)));\n+\n+    if (bt == BasicType::T_DOUBLE || bt == BasicType::T_LONG) {\n+      basic_type[bt_idx++] = T_VOID;\n+      \/\/ we only need these in the basic type\n+      \/\/ NativeCallConv ignores them, but they are needed\n+      \/\/ for JavaCallConv\n+    }\n+  }\n+\n+\n+  jint outs = ret_moves_oop->length();\n+  GrowableArray<VMReg> output_regs(outs);\n+  oop type_oop = java_lang_invoke_MethodType::rtype(type);\n+  BasicType  ret_bt = java_lang_Class::primitive_type(type_oop);\n+  for (int i = 0; i < outs; i++) {\n+    \/\/ note that we don't care about long\/double upper halfs here:\n+    \/\/ we are NOT moving Java values, we are moving register-sized values\n+    output_regs.push(ForeignGlobals::parse_vmstorage(ret_moves_oop->obj_at(i)));\n+  }\n+\n+  return (jlong) ProgrammableInvoker::make_native_invoker(\n+    basic_type, pslots, ret_bt, abi, input_regs, output_regs, needs_return_buffer)->code_begin();\n@@ -35,0 +82,3 @@\n+#define METHOD_TYPE \"Ljava\/lang\/invoke\/MethodType;\"\n+#define ABI_DESC \"Ljdk\/internal\/foreign\/abi\/ABIDescriptor;\"\n+#define VM_STORAGE_ARR \"[Ljdk\/internal\/foreign\/abi\/VMStorage;\"\n@@ -37,1 +87,1 @@\n-  {CC \"vmStorageToVMReg\", CC \"(II)J\", FN_PTR(NEP_vmStorageToVMReg)},\n+  {CC \"makeInvoker\", CC \"(\" METHOD_TYPE ABI_DESC VM_STORAGE_ARR VM_STORAGE_ARR \"Z)J\", FN_PTR(NEP_makeInvoker)},\n@@ -40,0 +90,4 @@\n+#undef METHOD_TYPE\n+#undef ABI_DESC\n+#undef VM_STORAGE_ARR\n+\n@@ -44,1 +98,1 @@\n-            \"register jdk.internal.invoke.NativeEntryPoint natives\");\n+            \"register jdk.internal.foreign.abi.NativeEntryPoint natives\");\n","filename":"src\/hotspot\/share\/prims\/nativeEntryPoint.cpp","additions":58,"deletions":4,"binary":false,"changes":62,"status":"modified"},{"patch":"@@ -206,1 +206,0 @@\n-  void JNICALL JVM_RegisterProgrammableInvokerMethods(JNIEnv *env, jclass unsafecls);\n@@ -225,2 +224,1 @@\n-  { CC\"Java_jdk_internal_foreign_abi_ProgrammableInvoker_registerNatives\",      NULL, FN_PTR(JVM_RegisterProgrammableInvokerMethods) },\n-  { CC\"Java_jdk_internal_invoke_NativeEntryPoint_registerNatives\",      NULL, FN_PTR(JVM_RegisterNativeEntryPointMethods) },\n+  { CC\"Java_jdk_internal_foreign_abi_NativeEntryPoint_registerNatives\",      NULL, FN_PTR(JVM_RegisterNativeEntryPointMethods) },\n","filename":"src\/hotspot\/share\/prims\/nativeLookup.cpp","additions":1,"deletions":3,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1,67 +0,0 @@\n-\/*\n- * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"prims\/universalNativeInvoker.hpp\"\n-#include \"runtime\/interfaceSupport.inline.hpp\"\n-\n-ProgrammableInvoker::Generator::Generator(CodeBuffer* code, const ABIDescriptor* abi, const BufferLayout* layout)\n-  : StubCodeGenerator(code),\n-    _abi(abi),\n-    _layout(layout) {}\n-\n-void ProgrammableInvoker::invoke_native(Stub stub, address buff, JavaThread* thread) {\n-  ThreadToNativeFromVM ttnfvm(thread);\n-  \/\/ We need WXExec because we are about to call a generated stub. Like in VM\n-  \/\/ entries, the thread state should be changed while we are still in WXWrite.\n-  \/\/ See JDK-8265292.\n-  MACOS_AARCH64_ONLY(ThreadWXEnable wx(WXExec, thread));\n-  stub(buff);\n-}\n-\n-JNI_ENTRY(void, PI_invokeNative(JNIEnv* env, jclass _unused, jlong adapter_stub, jlong buff))\n-  assert(thread->thread_state() == _thread_in_vm, \"thread state is: %d\", thread->thread_state());\n-  ProgrammableInvoker::Stub stub = (ProgrammableInvoker::Stub) adapter_stub;\n-  address c = (address) buff;\n-  ProgrammableInvoker::invoke_native(stub, c, thread);\n-JNI_END\n-\n-JNI_ENTRY(jlong, PI_generateAdapter(JNIEnv* env, jclass _unused, jobject abi, jobject layout))\n-  return (jlong) ProgrammableInvoker::generate_adapter(abi, layout);\n-JNI_END\n-\n-#define CC (char*)  \/*cast a literal from (const char*)*\/\n-#define FN_PTR(f) CAST_FROM_FN_PTR(void*, &f)\n-#define FOREIGN_ABI \"Ljdk\/internal\/foreign\/abi\"\n-\n-static JNINativeMethod PI_methods[] = {\n-  {CC \"invokeNative\",    CC \"(JJ)V\",                                                             FN_PTR(PI_invokeNative)   },\n-  {CC \"generateAdapter\", CC \"(\" FOREIGN_ABI \"\/ABIDescriptor;\" FOREIGN_ABI \"\/BufferLayout;\" \")J\", FN_PTR(PI_generateAdapter)}\n-};\n-\n-JNI_ENTRY(void, JVM_RegisterProgrammableInvokerMethods(JNIEnv *env, jclass PI_class))\n-  ThreadToNativeFromVM ttnfv(thread);\n-  int status = env->RegisterNatives(PI_class, PI_methods, sizeof(PI_methods)\/sizeof(JNINativeMethod));\n-  guarantee(status == JNI_OK && !env->ExceptionOccurred(),\n-            \"register jdk.internal.foreign.abi.programmable.ProgrammableInvoker natives\");\n-JNI_END\n","filename":"src\/hotspot\/share\/prims\/universalNativeInvoker.cpp","additions":0,"deletions":67,"binary":false,"changes":67,"status":"deleted"},{"patch":"@@ -27,1 +27,0 @@\n-#include \"runtime\/stubCodeGenerator.hpp\"\n@@ -30,10 +29,1 @@\n-class ProgrammableInvoker: AllStatic {\n-private:\n-  static constexpr CodeBuffer::csize_t native_invoker_size = 1024;\n-\n-  class Generator : StubCodeGenerator {\n-  private:\n-    const ABIDescriptor* _abi;\n-    const BufferLayout* _layout;\n-  public:\n-    Generator(CodeBuffer* code, const ABIDescriptor* abi, const BufferLayout* layout);\n+class RuntimeStub;\n@@ -41,2 +31,1 @@\n-    void generate();\n-  };\n+class ProgrammableInvoker: AllStatic {\n@@ -44,4 +33,7 @@\n-  using Stub = void(*)(address);\n-\n-  static void invoke_native(Stub stub, address buff, JavaThread* thread);\n-  static address generate_adapter(jobject abi, jobject layout);\n+  static RuntimeStub* make_native_invoker(BasicType*,\n+                                          int num_args,\n+                                          BasicType ret_bt,\n+                                          const ABIDescriptor& abi,\n+                                          const GrowableArray<VMReg>& input_registers,\n+                                          const GrowableArray<VMReg>& output_registers,\n+                                          bool needs_return_buffer);\n","filename":"src\/hotspot\/share\/prims\/universalNativeInvoker.hpp","additions":9,"deletions":17,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -34,0 +34,1 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -59,15 +60,0 @@\n-void ProgrammableUpcallHandler::upcall_helper(JavaThread* thread, jobject rec, address buff) {\n-  JavaThread* THREAD = thread; \/\/ For exception macros.\n-  ThreadInVMfromNative tiv(THREAD);\n-  const UpcallMethod& upcall_method = instance().upcall_method;\n-\n-  ResourceMark rm(THREAD);\n-  JavaValue result(T_VOID);\n-  JavaCallArguments args(2); \/\/ long = 2 slots\n-\n-  args.push_jobject(rec);\n-  args.push_long((jlong) buff);\n-\n-  JavaCalls::call_static(&result, upcall_method.klass, upcall_method.name, upcall_method.sig, &args, CATCH);\n-}\n-\n@@ -102,0 +88,4 @@\n+  \/\/ The call to transition_from_native below contains a safepoint check\n+  \/\/ which needs the code cache to be writable.\n+  MACOS_AARCH64_ONLY(ThreadWXEnable wx(WXWrite, thread));\n+\n@@ -119,2 +109,0 @@\n-  MACOS_AARCH64_ONLY(thread->enable_wx(WXExec));\n-\n@@ -129,2 +117,0 @@\n-  MACOS_AARCH64_ONLY(thread->enable_wx(WXWrite));\n-\n@@ -150,30 +136,0 @@\n-void ProgrammableUpcallHandler::attach_thread_and_do_upcall(jobject rec, address buff) {\n-  JavaThread* thread = maybe_attach_and_get_thread();\n-\n-  {\n-    MACOS_AARCH64_ONLY(ThreadWXEnable wx(WXWrite, thread));\n-    upcall_helper(thread, rec, buff);\n-  }\n-}\n-\n-const ProgrammableUpcallHandler& ProgrammableUpcallHandler::instance() {\n-  static ProgrammableUpcallHandler handler;\n-  return handler;\n-}\n-\n-ProgrammableUpcallHandler::ProgrammableUpcallHandler() {\n-  JavaThread* THREAD = JavaThread::current(); \/\/ For exception macros.\n-  ResourceMark rm(THREAD);\n-  Symbol* sym = SymbolTable::new_symbol(FOREIGN_ABI \"ProgrammableUpcallHandler\");\n-  Klass* k = SystemDictionary::resolve_or_null(sym, Handle(), Handle(), CATCH);\n-  k->initialize(CATCH);\n-\n-  upcall_method.klass = k;\n-  upcall_method.name = SymbolTable::new_symbol(\"invoke\");\n-  upcall_method.sig = SymbolTable::new_symbol(\"(Ljava\/lang\/invoke\/MethodHandle;J)V\");\n-\n-  assert(upcall_method.klass->lookup_method(upcall_method.name, upcall_method.sig) != nullptr,\n-    \"Could not find upcall method: %s.%s%s\", upcall_method.klass->external_name(),\n-    upcall_method.name->as_C_string(), upcall_method.sig->as_C_string());\n-}\n-\n@@ -188,7 +144,3 @@\n-JVM_ENTRY(jlong, PUH_AllocateUpcallStub(JNIEnv *env, jclass unused, jobject rec, jobject abi, jobject buffer_layout))\n-  Handle receiver(THREAD, JNIHandles::resolve(rec));\n-  jobject global_rec = JNIHandles::make_global(receiver);\n-  return (jlong) ProgrammableUpcallHandler::generate_upcall_stub(global_rec, abi, buffer_layout);\n-JNI_END\n-\n-JVM_ENTRY(jlong, PUH_AllocateOptimizedUpcallStub(JNIEnv *env, jclass unused, jobject mh, jobject abi, jobject conv))\n+JVM_ENTRY(jlong, PUH_AllocateOptimizedUpcallStub(JNIEnv *env, jclass unused, jobject mh, jobject abi, jobject conv,\n+                                                 jboolean needs_return_buffer, jlong ret_buf_size))\n+  ResourceMark rm(THREAD);\n@@ -206,2 +158,21 @@\n-  return (jlong) ProgrammableUpcallHandler::generate_optimized_upcall_stub(mh_j, entry, abi, conv);\n-JVM_END\n+  assert(entry->is_static(), \"static only\");\n+  \/\/ Fill in the signature array, for the calling-convention call.\n+  const int total_out_args = entry->size_of_parameters();\n+  assert(total_out_args > 0, \"receiver arg\");\n+\n+  BasicType* out_sig_bt = NEW_RESOURCE_ARRAY(BasicType, total_out_args);\n+  BasicType ret_type;\n+  {\n+    int i = 0;\n+    SignatureStream ss(entry->signature());\n+    for (; !ss.at_return_type(); ss.next()) {\n+      out_sig_bt[i++] = ss.type();  \/\/ Collect remaining bits of signature\n+      if (ss.type() == T_LONG || ss.type() == T_DOUBLE)\n+        out_sig_bt[i++] = T_VOID;   \/\/ Longs & doubles take 2 Java slots\n+    }\n+    assert(i == total_out_args, \"\");\n+    ret_type = ss.type();\n+  }\n+  \/\/ skip receiver\n+  BasicType* in_sig_bt = out_sig_bt + 1;\n+  int total_in_args = total_out_args - 1;\n@@ -209,2 +180,2 @@\n-JVM_ENTRY(jboolean, PUH_SupportsOptimizedUpcalls(JNIEnv *env, jclass unused))\n-  return (jboolean) ProgrammableUpcallHandler::supports_optimized_upcalls();\n+  return (jlong) ProgrammableUpcallHandler::generate_optimized_upcall_stub(\n+    mh_j, entry, in_sig_bt, total_in_args, out_sig_bt, total_out_args, ret_type, abi, conv, needs_return_buffer, checked_cast<int>(ret_buf_size));\n@@ -217,3 +188,1 @@\n-  {CC \"allocateUpcallStub\", CC \"(\" \"Ljava\/lang\/invoke\/MethodHandle;\" \"L\" FOREIGN_ABI \"ABIDescriptor;\" \"L\" FOREIGN_ABI \"BufferLayout;\" \")J\", FN_PTR(PUH_AllocateUpcallStub)},\n-  {CC \"allocateOptimizedUpcallStub\", CC \"(\" \"Ljava\/lang\/invoke\/MethodHandle;\" \"L\" FOREIGN_ABI \"ABIDescriptor;\" \"L\" FOREIGN_ABI \"ProgrammableUpcallHandler$CallRegs;\" \")J\", FN_PTR(PUH_AllocateOptimizedUpcallStub)},\n-  {CC \"supportsOptimizedUpcalls\", CC \"()Z\", FN_PTR(PUH_SupportsOptimizedUpcalls)},\n+  {CC \"allocateOptimizedUpcallStub\", CC \"(\" \"Ljava\/lang\/invoke\/MethodHandle;\" \"L\" FOREIGN_ABI \"ABIDescriptor;\" \"L\" FOREIGN_ABI \"ProgrammableUpcallHandler$CallRegs;\" \"ZJ)J\", FN_PTR(PUH_AllocateOptimizedUpcallStub)},\n","filename":"src\/hotspot\/share\/prims\/universalUpcallHandler.cpp","additions":33,"deletions":64,"binary":false,"changes":97,"status":"modified"},{"patch":"@@ -35,15 +35,0 @@\n-  static constexpr CodeBuffer::csize_t upcall_stub_size = 1024;\n-\n-  struct UpcallMethod {\n-    Klass* klass;\n-    Symbol* name;\n-    Symbol* sig;\n-  } upcall_method;\n-\n-  ProgrammableUpcallHandler();\n-\n-  static const ProgrammableUpcallHandler& instance();\n-\n-  static void upcall_helper(JavaThread* thread, jobject rec, address buff);\n-  static void attach_thread_and_do_upcall(jobject rec, address buff);\n-\n@@ -56,3 +41,6 @@\n-  static address generate_optimized_upcall_stub(jobject mh, Method* entry, jobject jabi, jobject jconv);\n-  static address generate_upcall_stub(jobject rec, jobject abi, jobject buffer_layout);\n-  static bool supports_optimized_upcalls();\n+  static address generate_optimized_upcall_stub(jobject mh, Method* entry,\n+                                                BasicType* in_sig_bt, int total_in_args,\n+                                                BasicType* out_sig_bt, int total_out_args,\n+                                                BasicType ret_type,\n+                                                jobject jabi, jobject jconv,\n+                                                bool needs_return_buffer, int ret_buf_size);\n","filename":"src\/hotspot\/share\/prims\/universalUpcallHandler.hpp","additions":6,"deletions":18,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -25,2 +25,1 @@\n-#include \"runtime\/jniHandles.inline.hpp\"\n-#include \"runtime\/interfaceSupport.inline.hpp\"\n+#include \"code\/codeBlob.hpp\"\n@@ -28,1 +27,1 @@\n-#include \"runtime\/vmOperations.hpp\"\n+#include \"runtime\/interfaceSupport.inline.hpp\"\n@@ -31,3 +30,1 @@\n-  \/\/acquire code cache lock\n-  MutexLocker mu(CodeCache_lock, Mutex::_no_safepoint_check_flag);\n-  \/\/find code blob\n+  \/\/ safe to call 'find_blob' without code cache lock, because stub is always alive\n@@ -35,1 +32,1 @@\n-  if (cb == NULL) {\n+  if (cb == nullptr) {\n@@ -38,11 +35,1 @@\n-  \/\/free global JNI handle\n-  jobject handle = NULL;\n-  if (cb->is_optimized_entry_blob()) {\n-    handle = ((OptimizedEntryBlob*)cb)->receiver();\n-  } else {\n-    jobject* handle_ptr = (jobject*)(void*)cb->content_begin();\n-    handle = *handle_ptr;\n-  }\n-  JNIHandles::destroy_global(handle);\n-  \/\/free code blob\n-  CodeCache::free(cb);\n+  OptimizedEntryBlob::free(cb->as_optimized_entry_blob());\n","filename":"src\/hotspot\/share\/prims\/upcallStubs.cpp","additions":5,"deletions":18,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -42,1 +42,0 @@\n-friend class CallNativeDirectNode;\n","filename":"src\/hotspot\/share\/runtime\/javaFrameAnchor.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -511,13 +511,0 @@\n-#ifdef COMPILER2\n-  static RuntimeStub* make_native_invoker(address call_target,\n-                                          int shadow_space_bytes,\n-                                          const GrowableArray<VMReg>& input_registers,\n-                                          const GrowableArray<VMReg>& output_registers);\n-#endif\n-\n-  static void compute_move_order(const BasicType* in_sig_bt,\n-                                 int total_in_args, const VMRegPair* in_regs,\n-                                 int total_out_args, VMRegPair* out_regs,\n-                                 GrowableArray<int>& arg_order,\n-                                 VMRegPair tmp_vmreg);\n-\n","filename":"src\/hotspot\/share\/runtime\/sharedRuntime.hpp","additions":0,"deletions":13,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -1503,1 +1503,0 @@\n-  declare_c2_type(CallNativeNode, CallNode)                               \\\n@@ -1633,1 +1632,0 @@\n-  declare_c2_type(MachCallNativeNode, MachCallNode)                       \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -144,0 +144,8 @@\n+  E* data() {\n+    return _data;\n+  }\n+\n+  const E* data() const {\n+    return _data;\n+  }\n+\n","filename":"src\/hotspot\/share\/utilities\/growableArray.hpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -30,1 +30,1 @@\n-import jdk.internal.invoke.NativeEntryPoint;\n+import jdk.internal.foreign.abi.NativeEntryPoint;\n@@ -1582,5 +1582,0 @@\n-            @Override\n-            public void ensureCustomized(MethodHandle mh) {\n-                mh.customize();\n-            }\n-\n@@ -1593,2 +1588,2 @@\n-            public MethodHandle nativeMethodHandle(NativeEntryPoint nep, MethodHandle fallback) {\n-                return NativeMethodHandle.make(nep, fallback);\n+            public MethodHandle nativeMethodHandle(NativeEntryPoint nep) {\n+                return NativeMethodHandle.make(nep);\n","filename":"src\/java.base\/share\/classes\/java\/lang\/invoke\/MethodHandleImpl.java","additions":3,"deletions":8,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -29,1 +29,1 @@\n-import jdk.internal.invoke.NativeEntryPoint;\n+import jdk.internal.foreign.abi.NativeEntryPoint;\n@@ -44,1 +44,0 @@\n-    final MethodHandle fallback;\n@@ -46,1 +45,1 @@\n-    private NativeMethodHandle(MethodType type, LambdaForm form, MethodHandle fallback, NativeEntryPoint nep) {\n+    private NativeMethodHandle(MethodType type, LambdaForm form, NativeEntryPoint nep) {\n@@ -48,1 +47,0 @@\n-        this.fallback = fallback;\n@@ -55,1 +53,1 @@\n-    public static MethodHandle make(NativeEntryPoint nep, MethodHandle fallback) {\n+    public static MethodHandle make(NativeEntryPoint nep) {\n@@ -60,2 +58,0 @@\n-        if (type != fallback.type())\n-            throw new IllegalArgumentException(\"Type of fallback must match: \" + type + \" != \" + fallback.type());\n@@ -64,1 +60,1 @@\n-        return new NativeMethodHandle(type, lform, fallback, nep);\n+        return new NativeMethodHandle(type, lform, nep);\n@@ -91,2 +87,2 @@\n-        MethodType linkerType = mtype.insertParameterTypes(0, MethodHandle.class)\n-                .appendParameterTypes(Object.class);\n+        MethodType linkerType = mtype\n+                .appendParameterTypes(Object.class); \/\/ NEP\n@@ -103,1 +99,0 @@\n-        final int GET_FALLBACK = nameCursor++;\n@@ -106,0 +101,1 @@\n+\n@@ -108,1 +104,1 @@\n-        names[GET_FALLBACK] = new LambdaForm.Name(Lazy.NF_internalFallback, names[NMH_THIS]);\n+\n@@ -110,0 +106,1 @@\n+\n@@ -111,3 +108,1 @@\n-        \/\/ Need to pass fallback here so we can call it without destroying the receiver register!!\n-        outArgs[0] = names[GET_FALLBACK];\n-        System.arraycopy(names, ARG_BASE, outArgs, 1, mtype.parameterCount());\n+        System.arraycopy(names, ARG_BASE, outArgs, 0, mtype.parameterCount());\n@@ -116,0 +111,1 @@\n+\n@@ -126,1 +122,1 @@\n-        return new NativeMethodHandle(mt, lf, fallback, nep);\n+        return new NativeMethodHandle(mt, lf, nep);\n@@ -139,5 +135,0 @@\n-    @ForceInline\n-    static MethodHandle internalFallback(Object mh) {\n-        return ((NativeMethodHandle)mh).fallback;\n-    }\n-\n@@ -152,2 +143,0 @@\n-        static final NamedFunction\n-                NF_internalFallback;\n@@ -161,2 +150,0 @@\n-                        NF_internalFallback = new NamedFunction(\n-                                THIS_CLASS.getDeclaredMethod(\"internalFallback\", Object.class))\n","filename":"src\/java.base\/share\/classes\/java\/lang\/invoke\/NativeMethodHandle.java","additions":12,"deletions":25,"binary":false,"changes":37,"status":"modified"},{"patch":"@@ -28,1 +28,1 @@\n-import jdk.internal.invoke.NativeEntryPoint;\n+import jdk.internal.foreign.abi.NativeEntryPoint;\n@@ -133,1 +133,0 @@\n-     * @param fallback the fallback handle\n@@ -136,8 +135,1 @@\n-    MethodHandle nativeMethodHandle(NativeEntryPoint nep, MethodHandle fallback);\n-\n-    \/**\n-     * Ensure given method handle is customized\n-     *\n-     * @param mh the method handle\n-     *\/\n-    void ensureCustomized(MethodHandle mh);\n+    MethodHandle nativeMethodHandle(NativeEntryPoint nep);\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/access\/JavaLangInvokeAccess.java","additions":2,"deletions":10,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -27,3 +27,6 @@\n-import jdk.internal.invoke.ABIDescriptorProxy;\n-\n-public class ABIDescriptor implements ABIDescriptorProxy {\n+\/**\n+ * Carrier class used to communicate with the VM\n+ *\n+ * It is particularly low-level since the VM will be accessing these fields directly\n+ *\/\n+public class ABIDescriptor {\n@@ -40,0 +43,3 @@\n+    final VMStorage targetAddrStorage;\n+    final VMStorage retBufAddrStorage;\n+\n@@ -41,1 +47,2 @@\n-                         VMStorage[][] volatileStorage, int stackAlignment, int shadowSpace) {\n+                         VMStorage[][] volatileStorage, int stackAlignment, int shadowSpace,\n+                         VMStorage targetAddrStorage, VMStorage retBufAddrStorage) {\n@@ -48,0 +55,6 @@\n+        this.targetAddrStorage = targetAddrStorage;\n+        this.retBufAddrStorage = retBufAddrStorage;\n+    }\n+\n+    public VMStorage targetAddrStorage() {\n+        return targetAddrStorage;\n@@ -50,3 +63,2 @@\n-    @Override\n-    public int shadowSpaceBytes() {\n-        return shadowSpace;\n+    public VMStorage retBufAddrStorage() {\n+        return retBufAddrStorage;\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/foreign\/abi\/ABIDescriptor.java","additions":19,"deletions":7,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -399,0 +399,4 @@\n+    public static ToSegment toSegment(long byteSize) {\n+        return new ToSegment(byteSize);\n+    }\n+\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/foreign\/abi\/Binding.java","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1,152 +0,0 @@\n-\/*\n- *  Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n- *  DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- *  This code is free software; you can redistribute it and\/or modify it\n- *  under the terms of the GNU General Public License version 2 only, as\n- *  published by the Free Software Foundation.  Oracle designates this\n- *  particular file as subject to the \"Classpath\" exception as provided\n- *  by Oracle in the LICENSE file that accompanied this code.\n- *\n- *  This code is distributed in the hope that it will be useful, but WITHOUT\n- *  ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- *  FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- *  version 2 for more details (a copy is included in the LICENSE file that\n- *  accompanied this code).\n- *\n- *  You should have received a copy of the GNU General Public License version\n- *  2 along with this work; if not, write to the Free Software Foundation,\n- *  Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- *  Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- *  or visit www.oracle.com if you need additional information or have any\n- *  questions.\n- *\/\n-package jdk.internal.foreign.abi;\n-\n-import java.lang.foreign.MemorySegment;\n-import java.lang.foreign.ValueLayout;\n-import jdk.internal.foreign.MemoryAddressImpl;\n-\n-import java.io.PrintStream;\n-import java.lang.invoke.VarHandle;\n-import java.util.HashMap;\n-import java.util.Map;\n-\n-class BufferLayout {\n-    static final VarHandle VH_LONG = ValueLayout.JAVA_LONG.varHandle();\n-\n-    final long size;\n-    final long arguments_next_pc;\n-    final long stack_args_bytes;\n-    final long stack_args;\n-\n-    \/\/ read by JNI\n-    final long[] input_type_offsets;\n-    final long[] output_type_offsets;\n-\n-    private final Map<VMStorage, Long> argOffsets;\n-    private final Map<VMStorage, Long> retOffsets;\n-\n-    private BufferLayout(long size, long arguments_next_pc, long stack_args_bytes, long stack_args,\n-                         long[] input_type_offsets, long[] output_type_offsets,\n-                         Map<VMStorage, Long> argOffsets, Map<VMStorage, Long> retOffsets) {\n-        this.size = size;\n-        this.arguments_next_pc = arguments_next_pc;\n-        this.stack_args_bytes = stack_args_bytes;\n-        this.stack_args = stack_args;\n-        this.input_type_offsets = input_type_offsets;\n-        this.output_type_offsets = output_type_offsets;\n-        this.argOffsets = argOffsets;\n-        this.retOffsets = retOffsets;\n-    }\n-\n-    static BufferLayout of(ABIDescriptor abi) {\n-        long offset = 0;\n-\n-        offset = SharedUtils.alignUp(offset, 8);\n-        long arguments_next_pc = offset;\n-        offset += 8;\n-\n-        offset = SharedUtils.alignUp(offset, 8);\n-        long stack_args_bytes = offset;\n-        offset += 8;\n-\n-        offset = SharedUtils.alignUp(offset, 8);\n-        long stack_args = offset;\n-        offset += 8;\n-\n-        Map<VMStorage, Long> argOffsets = new HashMap<>();\n-        long[] input_type_offsets = new long[abi.inputStorage.length];\n-        for (int i = 0; i < abi.inputStorage.length; i++) {\n-            long size = abi.arch.typeSize(i);\n-            offset = SharedUtils.alignUp(offset, size);\n-            input_type_offsets[i] = offset;\n-            for (VMStorage store : abi.inputStorage[i]) {\n-                argOffsets.put(store, offset);\n-                offset += size;\n-            }\n-        }\n-\n-        Map<VMStorage, Long> retOffsets = new HashMap<>();\n-        long[] output_type_offsets = new long[abi.outputStorage.length];\n-        for (int i = 0; i < abi.outputStorage.length; i++) {\n-            long size = abi.arch.typeSize(i);\n-            offset = SharedUtils.alignUp(offset, size);\n-            output_type_offsets[i] = offset;\n-            for (VMStorage store : abi.outputStorage[i]) {\n-                retOffsets.put(store, offset);\n-                offset += size;\n-            }\n-        }\n-\n-        return new BufferLayout(offset, arguments_next_pc, stack_args_bytes, stack_args,\n-                input_type_offsets, output_type_offsets, argOffsets, retOffsets);\n-    }\n-\n-    long argOffset(VMStorage storage) {\n-        return argOffsets.get(storage);\n-    }\n-\n-    long retOffset(VMStorage storage) {\n-        return retOffsets.get(storage);\n-    }\n-\n-    private static String getLongString(MemorySegment buffer, long offset) {\n-        return Long.toHexString((long) VH_LONG.get(buffer.asSlice(offset)));\n-    }\n-\n-    private void dumpValues(Architecture arch, MemorySegment buff, PrintStream stream,\n-                            Map<VMStorage, Long> offsets) {\n-        for (var entry : offsets.entrySet()) {\n-            VMStorage storage = entry.getKey();\n-            stream.print(storage.name());\n-            stream.print(\"={ \");\n-            MemorySegment start = buff.asSlice(entry.getValue());\n-            for (int i = 0; i < arch.typeSize(storage.type()) \/ 8; i += 8) {\n-                stream.print(getLongString(start, i));\n-                stream.print(\" \");\n-            }\n-            stream.println(\"}\");\n-        }\n-        long stack_ptr = (long) VH_LONG.get(buff.asSlice(stack_args));\n-        long stack_bytes = (long) VH_LONG.get(buff.asSlice(stack_args_bytes));\n-        MemorySegment stackArgs = MemoryAddressImpl.ofLongUnchecked(stack_ptr, stack_bytes);\n-        stream.println(\"Stack {\");\n-        for (int i = 0; i < stack_bytes \/ 8; i += 8) {\n-            stream.printf(\"    @%d: %s%n\", i, getLongString(stackArgs, i));\n-        }\n-        stream.println(\"}\");\n-    }\n-\n-    void dump(Architecture arch, MemorySegment buff, PrintStream stream) {\n-        stream.println(\"Next PC: \" + getLongString(buff, arguments_next_pc));\n-        stream.println(\"Stack args bytes: \" + getLongString(buff, stack_args_bytes));\n-        stream.println(\"Stack args ptr: \" + getLongString(buff, stack_args));\n-\n-        stream.println(\"Arguments:\");\n-        dumpValues(arch, buff, stream, argOffsets);\n-        stream.println(\"Returns:\");\n-        dumpValues(arch, buff, stream, retOffsets);\n-    }\n-}\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/foreign\/abi\/BufferLayout.java","additions":0,"deletions":152,"binary":false,"changes":152,"status":"deleted"},{"patch":"@@ -36,1 +36,3 @@\n-    private final boolean isTrivial;\n+    private final boolean needsReturnBuffer;\n+    private final long returnBufferSize;\n+    private final long allocationSize;\n@@ -42,1 +44,2 @@\n-                           boolean isTrivial, List<List<Binding>> argumentBindings, List<Binding> returnBindings) {\n+                           boolean needsReturnBuffer, long returnBufferSize, long allocationSize,\n+                           List<List<Binding>> argumentBindings, List<Binding> returnBindings) {\n@@ -45,1 +48,3 @@\n-        this.isTrivial = isTrivial;\n+        this.needsReturnBuffer = needsReturnBuffer;\n+        this.returnBufferSize = returnBufferSize;\n+        this.allocationSize = allocationSize;\n@@ -92,2 +97,10 @@\n-    public boolean isTrivial() {\n-        return isTrivial;\n+    public boolean needsReturnBuffer() {\n+        return needsReturnBuffer;\n+    }\n+\n+    public long returnBufferSize() {\n+        return returnBufferSize;\n+    }\n+\n+    public long allocationSize() {\n+        return allocationSize;\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/foreign\/abi\/CallingSequence.java","additions":18,"deletions":5,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -27,2 +27,1 @@\n-import java.lang.foreign.FunctionDescriptor;\n-import java.lang.foreign.MemoryLayout;\n+import jdk.internal.foreign.Utils;\n@@ -31,0 +30,5 @@\n+import java.lang.foreign.Addressable;\n+import java.lang.foreign.FunctionDescriptor;\n+import java.lang.foreign.MemoryLayout;\n+import java.lang.foreign.MemorySegment;\n+import java.lang.foreign.ValueLayout;\n@@ -45,1 +49,2 @@\n-    private boolean isTrivial;\n+    private final ABIDescriptor abi;\n+\n@@ -53,1 +58,2 @@\n-    public CallingSequenceBuilder(boolean forUpcall) {\n+    public CallingSequenceBuilder(ABIDescriptor abi, boolean forUpcall) {\n+        this.abi = abi;\n@@ -59,4 +65,1 @@\n-        verifyBindings(true, carrier, bindings);\n-        inputBindings.add(bindings);\n-        mt = mt.appendParameterTypes(carrier);\n-        desc = desc.appendArgumentLayouts(layout);\n+        addArgumentBinding(inputBindings.size(), carrier, layout, bindings);\n@@ -66,0 +69,7 @@\n+    private void addArgumentBinding(int index, Class<?> carrier, MemoryLayout layout, List<Binding> bindings) {\n+        verifyBindings(true, carrier, bindings);\n+        inputBindings.add(index, bindings);\n+        mt = mt.insertParameterTypes(index, carrier);\n+        desc = desc.insertArgumentLayouts(index, layout);\n+    }\n+\n@@ -75,3 +85,4 @@\n-    public CallingSequenceBuilder setTrivial(boolean isTrivial) {\n-        this.isTrivial = isTrivial;\n-        return this;\n+    private boolean needsReturnBuffer() {\n+        return outputBindings.stream()\n+            .filter(Binding.Move.class::isInstance)\n+            .count() > 1;\n@@ -81,1 +92,47 @@\n-        return new CallingSequence(mt, desc, isTrivial, inputBindings, outputBindings);\n+        boolean needsReturnBuffer = needsReturnBuffer();\n+        long returnBufferSize = needsReturnBuffer ? computeReturnBuferSize() : 0;\n+        long allocationSize = computeAllocationSize() + returnBufferSize;\n+        if (!forUpcall) {\n+            addArgumentBinding(0, Addressable.class, ValueLayout.ADDRESS, List.of(\n+                Binding.unboxAddress(Addressable.class),\n+                Binding.vmStore(abi.targetAddrStorage(), long.class)));\n+            if (needsReturnBuffer) {\n+                addArgumentBinding(0, MemorySegment.class, ValueLayout.ADDRESS, List.of(\n+                    Binding.unboxAddress(MemorySegment.class),\n+                    Binding.vmStore(abi.retBufAddrStorage(), long.class)));\n+            }\n+        } else if (needsReturnBuffer) { \/\/ forUpcall == true\n+            addArgumentBinding(0, MemorySegment.class, ValueLayout.ADDRESS, List.of(\n+                Binding.vmLoad(abi.retBufAddrStorage(), long.class),\n+                Binding.boxAddress(),\n+                Binding.toSegment(returnBufferSize)));\n+        }\n+        return new CallingSequence(mt, desc, needsReturnBuffer, returnBufferSize, allocationSize, inputBindings, outputBindings);\n+    }\n+\n+    private long computeAllocationSize() {\n+        \/\/ FIXME: > 16 bytes alignment might need extra space since the\n+        \/\/ starting address of the allocator might be un-aligned.\n+        long size = 0;\n+        for (List<Binding> bindings : inputBindings) {\n+            for (Binding b : bindings) {\n+                if (b instanceof Binding.Copy copy) {\n+                    size = Utils.alignUp(size, copy.alignment());\n+                    size += copy.size();\n+                } else if (b instanceof Binding.Allocate allocate) {\n+                    size = Utils.alignUp(size, allocate.alignment());\n+                    size += allocate.size();\n+                }\n+            }\n+        }\n+        return size;\n+    }\n+\n+    private long computeReturnBuferSize() {\n+        return outputBindings.stream()\n+                .filter(Binding.Move.class::isInstance)\n+                .map(Binding.Move.class::cast)\n+                .map(Binding.Move::storage)\n+                .map(VMStorage::type)\n+                .mapToLong(abi.arch::typeSize)\n+                .sum();\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/foreign\/abi\/CallingSequenceBuilder.java","additions":69,"deletions":12,"binary":false,"changes":81,"status":"modified"},{"patch":"@@ -0,0 +1,81 @@\n+\/*\n+ * Copyright (c) 2020, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.  Oracle designates this\n+ * particular file as subject to the \"Classpath\" exception as provided\n+ * by Oracle in the LICENSE file that accompanied this code.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package jdk.internal.foreign.abi;\n+\n+import java.lang.invoke.MethodType;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.ConcurrentHashMap;\n+\n+\/**\n+ * This class describes a 'native invoker', which is used as an appendix argument to linkToNative calls.\n+ *\/\n+public class NativeEntryPoint {\n+    static {\n+        registerNatives();\n+    }\n+\n+    private final MethodType methodType;\n+    private final long invoker; \/\/ read by VM\n+\n+    private static final Map<CacheKey, Long> INVOKER_CACHE = new ConcurrentHashMap<>();\n+    private record CacheKey(MethodType methodType, ABIDescriptor abi,\n+                            List<VMStorage> argMoves, List<VMStorage> retMoves,\n+                            boolean needsReturnBuffer) {}\n+\n+    private NativeEntryPoint(MethodType methodType, long invoker) {\n+        this.methodType = methodType;\n+        this.invoker = invoker;\n+    }\n+\n+    public static NativeEntryPoint make(ABIDescriptor abi,\n+                                        VMStorage[] argMoves, VMStorage[] returnMoves,\n+                                        MethodType methodType, boolean needsReturnBuffer) {\n+        if (returnMoves.length > 1 != needsReturnBuffer) {\n+            throw new IllegalArgumentException(\"Multiple register return, but needsReturnBuffer was false\");\n+        }\n+\n+        assert (methodType.parameterType(0) == long.class) : \"Address expected\";\n+        assert (!needsReturnBuffer || methodType.parameterType(1) == long.class) : \"return buffer address expected\";\n+\n+        CacheKey key = new CacheKey(methodType, abi, Arrays.asList(argMoves), Arrays.asList(returnMoves), needsReturnBuffer);\n+        long invoker = INVOKER_CACHE.computeIfAbsent(key, k ->\n+            makeInvoker(methodType, abi, argMoves, returnMoves, needsReturnBuffer));\n+\n+        return new NativeEntryPoint(methodType, invoker);\n+    }\n+\n+    private static native long makeInvoker(MethodType methodType, ABIDescriptor abi,\n+                                           VMStorage[] encArgMoves, VMStorage[] encRetMoves,\n+                                           boolean needsReturnBuffer);\n+\n+    public MethodType type() {\n+        return methodType;\n+    }\n+\n+    private static native void registerNatives();\n+}\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/foreign\/abi\/NativeEntryPoint.java","additions":81,"deletions":0,"binary":false,"changes":81,"status":"added"},{"patch":"@@ -28,0 +28,1 @@\n+import java.lang.foreign.MemoryLayout;\n@@ -29,1 +30,0 @@\n-import java.lang.foreign.MemorySession;\n@@ -32,0 +32,1 @@\n+\n@@ -34,2 +35,0 @@\n-import jdk.internal.invoke.NativeEntryPoint;\n-import jdk.internal.invoke.VMStorageProxy;\n@@ -42,0 +41,1 @@\n+import java.nio.ByteOrder;\n@@ -45,1 +45,0 @@\n-import java.util.concurrent.ConcurrentHashMap;\n@@ -50,1 +49,1 @@\n-import static java.lang.invoke.MethodHandles.filterArguments;\n+import static java.lang.invoke.MethodHandles.foldArguments;\n@@ -54,1 +53,0 @@\n-import static sun.security.action.GetBooleanAction.privilegedGetProperty;\n@@ -62,2 +60,0 @@\n-    private static final boolean DEBUG =\n-        privilegedGetProperty(\"jdk.internal.foreign.ProgrammableInvoker.DEBUG\");\n@@ -66,2 +62,0 @@\n-    private static final boolean USE_INTRINSICS = Boolean.parseBoolean(\n-        GetPropertyAction.privilegedGetProperty(\"jdk.internal.foreign.ProgrammableInvoker.USE_INTRINSICS\", \"true\"));\n@@ -71,3 +65,0 @@\n-    private static final VarHandle VH_LONG = ValueLayout.JAVA_LONG.varHandle();\n-\n-    private static final MethodHandle MH_INVOKE_MOVES;\n@@ -75,1 +66,0 @@\n-    private static final MethodHandle MH_ADDR_TO_LONG;\n@@ -77,2 +67,2 @@\n-\n-    private static final Map<ABIDescriptor, Long> adapterStubs = new ConcurrentHashMap<>();\n+    private static final MethodHandle MH_ALLOCATE_RETURN_BUFFER;\n+    private static final MethodHandle MH_CHECK_SYMBOL;\n@@ -85,2 +75,0 @@\n-            MH_INVOKE_MOVES = lookup.findVirtual(ProgrammableInvoker.class, \"invokeMoves\",\n-                    methodType(Object.class, long.class, Object[].class, Binding.VMStore[].class, Binding.VMLoad[].class));\n@@ -88,1 +76,1 @@\n-                    methodType(Object.class, Addressable.class, SegmentAllocator.class, Object[].class, MethodHandle.class, Map.class, Map.class));\n+                    methodType(Object.class, SegmentAllocator.class, Object[].class, InvocationData.class));\n@@ -91,1 +79,4 @@\n-            MH_ADDR_TO_LONG = lookup.findStatic(ProgrammableInvoker.class, \"unboxTargetAddress\", methodType(long.class, Addressable.class));\n+            MH_CHECK_SYMBOL = lookup.findStatic(SharedUtils.class, \"checkSymbol\",\n+                    methodType(void.class, Addressable.class));\n+            MH_ALLOCATE_RETURN_BUFFER = lookup.findStatic(ProgrammableInvoker.class, \"allocateReturnBuffer\",\n+                    methodType(MemorySegment.class, Binding.Context.class, long.class));\n@@ -98,3 +89,0 @@\n-    private final BufferLayout layout;\n-    private final long stackArgsBytes;\n-\n@@ -103,4 +91,0 @@\n-    private final long stubAddress;\n-\n-    private final long bufferCopySize;\n-\n@@ -109,3 +93,0 @@\n-        this.layout = BufferLayout.of(abi);\n-        this.stubAddress = adapterStubs.computeIfAbsent(abi, key -> generateAdapter(key, layout));\n-\n@@ -113,8 +94,0 @@\n-\n-        this.stackArgsBytes = argMoveBindingsStream(callingSequence)\n-                .map(Binding.VMStore::storage)\n-                .filter(s -> abi.arch.isStackType(s.type()))\n-                .count()\n-                * abi.arch.typeSize(abi.arch.stackType());\n-\n-        this.bufferCopySize = SharedUtils.bufferCopySize(callingSequence);\n@@ -128,5 +101,1 @@\n-        Class<?> returnType = retMoves.length == 0\n-                ? void.class\n-                : retMoves.length == 1\n-                    ? retMoves[0].type()\n-                    : Object[].class;\n+        Class<?> returnType = retMoves.length == 1 ? retMoves[0].type() : void.class;\n@@ -135,22 +104,0 @@\n-        MethodType leafTypeWithAddress = leafType.insertParameterTypes(0, long.class);\n-\n-        MethodHandle handle = insertArguments(MH_INVOKE_MOVES.bindTo(this), 2, argMoves, retMoves);\n-        MethodHandle collector = makeCollectorHandle(leafType);\n-        handle = collectArguments(handle, 1, collector);\n-        handle = handle.asType(leafTypeWithAddress);\n-\n-        boolean isSimple = !(retMoves.length > 1);\n-        boolean usesStackArgs = stackArgsBytes != 0;\n-        if (USE_INTRINSICS && isSimple && !usesStackArgs) {\n-            NativeEntryPoint nep = NativeEntryPoint.make(\n-                \"native_call\",\n-                abi,\n-                toStorageArray(argMoves),\n-                toStorageArray(retMoves),\n-                !callingSequence.isTrivial(),\n-                leafTypeWithAddress\n-            );\n-\n-            handle = JLIA.nativeMethodHandle(nep, handle);\n-        }\n-        handle = filterArguments(handle, 0, MH_ADDR_TO_LONG);\n@@ -158,1 +105,10 @@\n-        if (USE_SPEC && isSimple) {\n+        NativeEntryPoint nep = NativeEntryPoint.make(\n+            abi,\n+            toStorageArray(argMoves),\n+            toStorageArray(retMoves),\n+            leafType,\n+            callingSequence.needsReturnBuffer()\n+        );\n+        MethodHandle handle = JLIA.nativeMethodHandle(nep);\n+\n+        if (USE_SPEC) {\n@@ -164,4 +120,11 @@\n-            handle = insertArguments(MH_INVOKE_INTERP_BINDINGS.bindTo(this), 3, handle, argIndexMap, retIndexMap);\n-            MethodHandle collectorInterp = makeCollectorHandle(callingSequence.methodType());\n-            handle = collectArguments(handle, 2, collectorInterp);\n-            handle = handle.asType(handle.type().changeReturnType(callingSequence.methodType().returnType()));\n+            InvocationData invData = new InvocationData(handle, argIndexMap, retIndexMap);\n+            handle = insertArguments(MH_INVOKE_INTERP_BINDINGS.bindTo(this), 2, invData);\n+            MethodType interpType = callingSequence.methodType();\n+            if (callingSequence.needsReturnBuffer()) {\n+                \/\/ Return buffer is supplied by invokeInterpBindings\n+                assert interpType.parameterType(0) == MemorySegment.class;\n+                interpType.dropParameterTypes(0, 1);\n+            }\n+            MethodHandle collectorInterp = makeCollectorHandle(interpType);\n+            handle = collectArguments(handle, 1, collectorInterp);\n+            handle = handle.asType(handle.type().changeReturnType(interpType.returnType()));\n@@ -170,0 +133,6 @@\n+        assert handle.type().parameterType(0) == SegmentAllocator.class;\n+        assert handle.type().parameterType(1) == Addressable.class;\n+        handle = foldArguments(handle, 1, MH_CHECK_SYMBOL);\n+\n+        handle = SharedUtils.swapArguments(handle, 0, 1); \/\/ normalize parameter order\n+\n@@ -173,3 +142,2 @@\n-    private static long unboxTargetAddress(Addressable addr) {\n-        SharedUtils.checkSymbol(addr);\n-        return addr.address().toRawLongValue();\n+    private static MemorySegment allocateReturnBuffer(Binding.Context context, long size) {\n+        return context.allocator().allocate(size);\n@@ -194,0 +162,4 @@\n+        return retMoveBindingsStream(callingSequence).toArray(Binding.VMLoad[]::new);\n+    }\n+\n+    private Stream<Binding.VMLoad> retMoveBindingsStream(CallingSequence callingSequence) {\n@@ -196,2 +168,1 @@\n-                .map(Binding.VMLoad.class::cast)\n-                .toArray(Binding.VMLoad[]::new);\n+                .map(Binding.VMLoad.class::cast);\n@@ -200,2 +171,1 @@\n-\n-    private VMStorageProxy[] toStorageArray(Binding.Move[] moves) {\n+    private VMStorage[] toStorageArray(Binding.Move[] moves) {\n@@ -208,2 +178,2 @@\n-        int argInsertPos = 1;\n-        int argContextPos = 1;\n+        int argInsertPos = 0;\n+        int argContextPos = 0;\n@@ -212,1 +182,0 @@\n-\n@@ -229,0 +198,2 @@\n+            int retBufPos = -1;\n+            long retBufReadOffset = -1;\n@@ -231,0 +202,7 @@\n+            if (callingSequence.needsReturnBuffer()) {\n+                retBufPos = 0;\n+                retBufReadOffset = callingSequence.returnBufferSize();\n+                retContextPos++;\n+                retInsertPos++;\n+                returnFilter = dropArguments(returnFilter, retBufPos, MemorySegment.class);\n+            }\n@@ -235,1 +213,26 @@\n-                returnFilter = binding.specialize(returnFilter, retInsertPos, retContextPos);\n+                if (callingSequence.needsReturnBuffer() && binding.tag() == Binding.Tag.VM_LOAD) {\n+                    \/\/ spacial case this, since we need to update retBufReadOffset as well\n+                    Binding.VMLoad load = (Binding.VMLoad) binding;\n+                    ValueLayout layout = MemoryLayout.valueLayout(load.type(), ByteOrder.nativeOrder()).withBitAlignment(8);\n+                    \/\/ since we iterate the bindings in reverse, we have to compute the offset in reverse as well\n+                    retBufReadOffset -= abi.arch.typeSize(load.storage().type());\n+                    MethodHandle loadHandle = MethodHandles.insertCoordinates(MethodHandles.memorySegmentViewVarHandle(layout), 1, retBufReadOffset)\n+                            .toMethodHandle(VarHandle.AccessMode.GET);\n+\n+                    returnFilter = MethodHandles.collectArguments(returnFilter, retInsertPos, loadHandle);\n+                    assert returnFilter.type().parameterType(retInsertPos - 1) == MemorySegment.class;\n+                    assert returnFilter.type().parameterType(retInsertPos - 2) == MemorySegment.class;\n+                    returnFilter = SharedUtils.mergeArguments(returnFilter, retBufPos, retInsertPos);\n+                    \/\/ to (... MemorySegment, MemorySegment, <primitive>, ...)\n+                    \/\/ from (... MemorySegment, MemorySegment, ...)\n+                    retInsertPos -= 2; \/\/ set insert pos back to the first MS (later DUP binding will merge the 2 MS)\n+                } else {\n+                    returnFilter = binding.specialize(returnFilter, retInsertPos, retContextPos);\n+                    if (callingSequence.needsReturnBuffer() && binding.tag() == Binding.Tag.BUFFER_STORE) {\n+                        \/\/ from (... MemorySegment, ...)\n+                        \/\/ to (... MemorySegment, MemorySegment, <primitive>, ...)\n+                        retInsertPos += 2; \/\/ set insert pos to <primitive>\n+                        assert returnFilter.type().parameterType(retInsertPos - 1) == MemorySegment.class;\n+                        assert returnFilter.type().parameterType(retInsertPos - 2) == MemorySegment.class;\n+                    }\n+                }\n@@ -237,2 +240,1 @@\n-            returnFilter = MethodHandles.filterArguments(returnFilter, retContextPos, MH_WRAP_ALLOCATOR);\n-            \/\/ (SegmentAllocator, Addressable, Context, ...) -> ...\n+            \/\/ (R, Context (ret)) -> (MemorySegment?, Context (ret), MemorySegment?, Context (arg), ...)\n@@ -240,2 +242,13 @@\n-            \/\/ (Addressable, SegmentAllocator, Context, ...) -> ...\n-            specializedHandle = SharedUtils.swapArguments(specializedHandle, 0, 1); \/\/ normalize parameter order\n+            if (callingSequence.needsReturnBuffer()) {\n+                \/\/ (MemorySegment, Context (ret), Context (arg), MemorySegment,  ...) -> (MemorySegment, Context (ret), Context (arg), ...)\n+                specializedHandle = SharedUtils.mergeArguments(specializedHandle, retBufPos, retBufPos + 3);\n+\n+                \/\/ allocate the return buffer from the binding context, and then merge the 2 allocator args\n+                MethodHandle retBufAllocHandle = MethodHandles.insertArguments(MH_ALLOCATE_RETURN_BUFFER, 1, callingSequence.returnBufferSize());\n+                \/\/ (MemorySegment, Context (ret), Context (arg), ...) -> (Context (arg), Context (ret), Context (arg), ...)\n+                specializedHandle = MethodHandles.filterArguments(specializedHandle, retBufPos, retBufAllocHandle);\n+                \/\/ (Context (arg), Context (ret), Context (arg), ...) -> (Context (ret), Context (arg), ...)\n+                specializedHandle = SharedUtils.mergeArguments(specializedHandle, argContextPos + 1, retBufPos); \/\/ +1 to skip return context\n+            }\n+            \/\/ (Context (ret), Context (arg), ...) -> (SegmentAllocator, Context (arg), ...)\n+            specializedHandle = MethodHandles.filterArguments(specializedHandle, 0, MH_WRAP_ALLOCATOR);\n@@ -243,1 +256,1 @@\n-            specializedHandle = MethodHandles.dropArguments(specializedHandle, 1, SegmentAllocator.class);\n+            specializedHandle = MethodHandles.dropArguments(specializedHandle, 0, SegmentAllocator.class);\n@@ -249,1 +262,1 @@\n-        specializedHandle = SharedUtils.wrapWithAllocator(specializedHandle, argContextPos, bufferCopySize, false);\n+        specializedHandle = SharedUtils.wrapWithAllocator(specializedHandle, argContextPos, callingSequence.allocationSize(), false);\n@@ -253,36 +266,1 @@\n-    \/**\n-     * Does a native invocation by moving primitive values from the arg array into an intermediate buffer\n-     * and calling the assembly stub that forwards arguments from the buffer to the target function\n-     *\n-     * @param args an array of primitive values to be copied in to the buffer\n-     * @param argBindings Binding.Move values describing how arguments should be copied\n-     * @param returnBindings Binding.Move values describing how return values should be copied\n-     * @return null, a single primitive value, or an Object[] of primitive values\n-     *\/\n-    Object invokeMoves(long addr, Object[] args, Binding.VMStore[] argBindings, Binding.VMLoad[] returnBindings) {\n-        MemorySegment stackArgsSeg = null;\n-        try (MemorySession session = MemorySession.openConfined()) {\n-            MemorySegment argBuffer = MemorySegment.allocateNative(layout.size, 64, session);\n-            if (stackArgsBytes > 0) {\n-                stackArgsSeg = MemorySegment.allocateNative(stackArgsBytes, 8, session);\n-            }\n-\n-            VH_LONG.set(argBuffer.asSlice(layout.arguments_next_pc), addr);\n-            VH_LONG.set(argBuffer.asSlice(layout.stack_args_bytes), stackArgsBytes);\n-            VH_LONG.set(argBuffer.asSlice(layout.stack_args), stackArgsSeg == null ? 0L : stackArgsSeg.address().toRawLongValue());\n-\n-            for (int i = 0; i < argBindings.length; i++) {\n-                Binding.VMStore binding = argBindings[i];\n-                VMStorage storage = binding.storage();\n-                MemorySegment ptr = abi.arch.isStackType(storage.type())\n-                    ? stackArgsSeg.asSlice(storage.index() * abi.arch.typeSize(abi.arch.stackType()))\n-                    : argBuffer.asSlice(layout.argOffset(storage));\n-                SharedUtils.writeOverSized(ptr, binding.type(), args[i]);\n-            }\n-\n-            if (DEBUG) {\n-                System.err.println(\"Buffer state before:\");\n-                layout.dump(abi.arch, argBuffer, System.err);\n-            }\n-\n-            invokeNative(stubAddress, argBuffer.address().toRawLongValue());\n+    private record InvocationData(MethodHandle leaf, Map<VMStorage, Integer> argIndexMap, Map<VMStorage, Integer> retIndexMap) {}\n@@ -290,28 +268,3 @@\n-            if (DEBUG) {\n-                System.err.println(\"Buffer state after:\");\n-                layout.dump(abi.arch, argBuffer, System.err);\n-            }\n-\n-            if (returnBindings.length == 0) {\n-                return null;\n-            } else if (returnBindings.length == 1) {\n-                Binding.VMLoad move = returnBindings[0];\n-                VMStorage storage = move.storage();\n-                return SharedUtils.read(argBuffer.asSlice(layout.retOffset(storage)), move.type());\n-            } else { \/\/ length > 1\n-                Object[] returns = new Object[returnBindings.length];\n-                for (int i = 0; i < returnBindings.length; i++) {\n-                    Binding.VMLoad move = returnBindings[i];\n-                    VMStorage storage = move.storage();\n-                    returns[i] = SharedUtils.read(argBuffer.asSlice(layout.retOffset(storage)), move.type());\n-                }\n-                return returns;\n-            }\n-        }\n-    }\n-\n-    Object invokeInterpBindings(Addressable symbol, SegmentAllocator allocator, Object[] args, MethodHandle leaf,\n-                                Map<VMStorage, Integer> argIndexMap,\n-                                Map<VMStorage, Integer> retIndexMap) throws Throwable {\n-        Binding.Context unboxContext = bufferCopySize != 0\n-                ? Binding.Context.ofBoundedAllocator(bufferCopySize)\n+    Object invokeInterpBindings(SegmentAllocator allocator, Object[] args, InvocationData invData) throws Throwable {\n+        Binding.Context unboxContext = callingSequence.allocationSize() != 0\n+                ? Binding.Context.ofBoundedAllocator(callingSequence.allocationSize())\n@@ -320,0 +273,2 @@\n+            MemorySegment returnBuffer = null;\n+\n@@ -321,2 +276,9 @@\n-            Object[] leafArgs = new Object[leaf.type().parameterCount()];\n-            leafArgs[0] = symbol; \/\/ symbol\n+            Object[] leafArgs = new Object[invData.leaf.type().parameterCount()];\n+            if (callingSequence.needsReturnBuffer()) {\n+                \/\/ we supply the return buffer (argument array does not contain it)\n+                Object[] prefixedArgs = new Object[args.length + 1];\n+                returnBuffer = unboxContext.allocator().allocate(callingSequence.returnBufferSize());\n+                prefixedArgs[0] = returnBuffer;\n+                System.arraycopy(args, 0, prefixedArgs, 1, args.length);\n+                args = prefixedArgs;\n+            }\n@@ -327,1 +289,1 @@\n-                            leafArgs[argIndexMap.get(storage) + 1] = value; \/\/ +1 to skip symbol\n+                            leafArgs[invData.argIndexMap.get(storage)] = value;\n@@ -332,1 +294,1 @@\n-            Object o = leaf.invokeWithArguments(leafArgs);\n+            Object o = invData.leaf.invokeWithArguments(leafArgs);\n@@ -336,3 +298,4 @@\n-                return null;\n-            } else if (o instanceof Object[]) {\n-                Object[] oArr = (Object[]) o;\n+                if (!callingSequence.needsReturnBuffer()) {\n+                    return null;\n+                }\n+                MemorySegment finalReturnBuffer = returnBuffer;\n@@ -340,1 +303,9 @@\n-                        (storage, type) -> oArr[retIndexMap.get(storage)], Binding.Context.ofAllocator(allocator));\n+                        new BindingInterpreter.LoadFunc() {\n+                            int retBufReadOffset = 0;\n+                            @Override\n+                            public Object load(VMStorage storage, Class<?> type) {\n+                                Object result1 = SharedUtils.read(finalReturnBuffer.asSlice(retBufReadOffset), type);\n+                                retBufReadOffset += abi.arch.typeSize(storage.type());\n+                                return result1;\n+                            }\n+                        }, Binding.Context.ofAllocator(allocator));\n@@ -347,10 +318,0 @@\n-\n-    \/\/natives\n-\n-    static native void invokeNative(long adapterStub, long buff);\n-    static native long generateAdapter(ABIDescriptor abi, BufferLayout layout);\n-\n-    private static native void registerNatives();\n-    static {\n-        registerNatives();\n-    }\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/foreign\/abi\/ProgrammableInvoker.java","additions":130,"deletions":169,"binary":false,"changes":299,"status":"modified"},{"patch":"@@ -28,1 +28,3 @@\n-import java.lang.foreign.MemoryAddress;\n+import sun.security.action.GetPropertyAction;\n+\n+import java.lang.foreign.MemoryLayout;\n@@ -32,5 +34,0 @@\n-import jdk.internal.access.JavaLangInvokeAccess;\n-import jdk.internal.access.SharedSecrets;\n-import jdk.internal.foreign.MemoryAddressImpl;\n-import sun.security.action.GetPropertyAction;\n-\n@@ -41,0 +38,1 @@\n+import java.nio.ByteOrder;\n@@ -47,5 +45,1 @@\n-import static java.lang.invoke.MethodHandles.dropArguments;\n-import static java.lang.invoke.MethodHandles.filterReturnValue;\n-import static java.lang.invoke.MethodHandles.identity;\n-import static java.lang.invoke.MethodHandles.insertArguments;\n-import static java.lang.invoke.MethodHandles.lookup;\n+import static java.lang.invoke.MethodHandles.*;\n@@ -56,5 +50,0 @@\n-\/**\n- * This class implements upcall invocation from native code through a so called 'universal adapter'. A universal upcall adapter\n- * takes an array of storage pointers, which describes the state of the CPU at the time of the upcall. This can be used\n- * by the Java code to fetch the upcall arguments and to store the results to the desired location, as per system ABI.\n- *\/\n@@ -66,4 +55,0 @@\n-    private static final boolean USE_INTRINSICS = Boolean.parseBoolean(\n-        GetPropertyAction.privilegedGetProperty(\"jdk.internal.foreign.ProgrammableUpcallHandler.USE_INTRINSICS\", \"true\"));\n-\n-    private static final JavaLangInvokeAccess JLI = SharedSecrets.getJavaLangInvokeAccess();\n@@ -71,3 +56,0 @@\n-    private static final VarHandle VH_LONG = ValueLayout.JAVA_LONG.varHandle();\n-\n-    private static final MethodHandle MH_invokeMoves;\n@@ -79,3 +61,0 @@\n-            MH_invokeMoves = lookup.findStatic(ProgrammableUpcallHandler.class, \"invokeMoves\",\n-                    methodType(void.class, MemoryAddress.class, MethodHandle.class,\n-                               Binding.VMLoad[].class, Binding.VMStore[].class, ABIDescriptor.class, BufferLayout.class));\n@@ -83,2 +62,1 @@\n-                    methodType(Object.class, Object[].class, MethodHandle.class, Map.class, Map.class,\n-                            CallingSequence.class, long.class));\n+                    methodType(Object.class, Object[].class, InvocationData.class));\n@@ -94,7 +72,1 @@\n-        boolean isSimple = !(retMoves.length > 1);\n-\n-        Class<?> llReturn = !isSimple\n-            ? Object[].class\n-            : retMoves.length == 1\n-                ? retMoves[0].type()\n-                : void.class;\n+        Class<?> llReturn = retMoves.length == 1 ? retMoves[0].type() : void.class;\n@@ -102,1 +74,1 @@\n-        MethodType llType = MethodType.methodType(llReturn, llParams);\n+        MethodType llType = methodType(llReturn, llParams);\n@@ -105,3 +77,2 @@\n-        long bufferCopySize = SharedUtils.bufferCopySize(callingSequence);\n-        if (USE_SPEC && isSimple) {\n-            doBindings = specializedBindingHandle(target, callingSequence, llReturn, bufferCopySize);\n+        if (USE_SPEC) {\n+            doBindings = specializedBindingHandle(target, callingSequence, llReturn, abi);\n@@ -112,3 +83,7 @@\n-            target = target.asSpreader(Object[].class, callingSequence.methodType().parameterCount());\n-            doBindings = insertArguments(MH_invokeInterpBindings, 1, target, argIndices, retIndices, callingSequence,\n-                    bufferCopySize);\n+            int spreaderCount = callingSequence.methodType().parameterCount();\n+            if (callingSequence.needsReturnBuffer()) {\n+                spreaderCount--; \/\/ return buffer is dropped from the argument list\n+            }\n+            target = target.asSpreader(Object[].class, spreaderCount);\n+            InvocationData invData = new InvocationData(target, argIndices, retIndices, callingSequence, retMoves, abi);\n+            doBindings = insertArguments(MH_invokeInterpBindings, 1, invData);\n@@ -119,17 +94,7 @@\n-        long entryPoint;\n-        boolean usesStackArgs = argMoveBindingsStream(callingSequence)\n-                .map(Binding.VMLoad::storage)\n-                .anyMatch(s -> abi.arch.isStackType(s.type()));\n-        if (USE_INTRINSICS && isSimple && !usesStackArgs && supportsOptimizedUpcalls()) {\n-            checkPrimitive(doBindings.type());\n-            JLI.ensureCustomized(doBindings);\n-            VMStorage[] args = Arrays.stream(argMoves).map(Binding.Move::storage).toArray(VMStorage[]::new);\n-            VMStorage[] rets = Arrays.stream(retMoves).map(Binding.Move::storage).toArray(VMStorage[]::new);\n-            CallRegs conv = new CallRegs(args, rets);\n-            entryPoint = allocateOptimizedUpcallStub(doBindings, abi, conv);\n-        } else {\n-            BufferLayout layout = BufferLayout.of(abi);\n-            MethodHandle doBindingsErased = doBindings.asSpreader(Object[].class, doBindings.type().parameterCount());\n-            MethodHandle invokeMoves = insertArguments(MH_invokeMoves, 1, doBindingsErased, argMoves, retMoves, abi, layout);\n-            entryPoint = allocateUpcallStub(invokeMoves, abi, layout);\n-        }\n+        checkPrimitive(doBindings.type());\n+        doBindings = insertArguments(exactInvoker(doBindings.type()), 0, doBindings);\n+        VMStorage[] args = Arrays.stream(argMoves).map(Binding.Move::storage).toArray(VMStorage[]::new);\n+        VMStorage[] rets = Arrays.stream(retMoves).map(Binding.Move::storage).toArray(VMStorage[]::new);\n+        CallRegs conv = new CallRegs(args, rets);\n+        long entryPoint = allocateOptimizedUpcallStub(doBindings, abi, conv,\n+                callingSequence.needsReturnBuffer(), callingSequence.returnBufferSize());\n@@ -164,1 +129,1 @@\n-                                                         Class<?> llReturn, long bufferCopySize) {\n+                                                         Class<?> llReturn, ABIDescriptor abi) {\n@@ -169,0 +134,32 @@\n+        \/\/ we handle returns first since IMR adds an extra parameter that needs to be specialized as well\n+        if (llReturn != void.class || callingSequence.needsReturnBuffer()) {\n+            int retAllocatorPos = -1; \/\/ assumed not needed\n+            int retInsertPos;\n+            MethodHandle filter;\n+            if (callingSequence.needsReturnBuffer()) {\n+                retInsertPos = 1;\n+                filter = empty(methodType(void.class, MemorySegment.class));\n+            } else {\n+                retInsertPos = 0;\n+                filter = identity(llReturn);\n+            }\n+            long retBufWriteOffset = callingSequence.returnBufferSize();\n+            List<Binding> bindings = callingSequence.returnBindings();\n+            for (int j = bindings.size() - 1; j >= 0; j--) {\n+                Binding binding = bindings.get(j);\n+                if (callingSequence.needsReturnBuffer() && binding.tag() == Binding.Tag.VM_STORE) {\n+                    Binding.VMStore store = (Binding.VMStore) binding;\n+                    ValueLayout layout = MemoryLayout.valueLayout(store.type(), ByteOrder.nativeOrder()).withBitAlignment(8);\n+                    \/\/ since we iterate the bindings in reverse, we have to compute the offset in reverse as well\n+                    retBufWriteOffset -= abi.arch.typeSize(store.storage().type());\n+                    MethodHandle storeHandle = MethodHandles.insertCoordinates(MethodHandles.memorySegmentViewVarHandle(layout), 1, retBufWriteOffset)\n+                            .toMethodHandle(VarHandle.AccessMode.SET);\n+                    filter = collectArguments(filter, retInsertPos, storeHandle);\n+                    filter = mergeArguments(filter, retInsertPos - 1, retInsertPos);\n+                } else {\n+                    filter = binding.specialize(filter, retInsertPos, retAllocatorPos);\n+                }\n+            }\n+            specializedHandle = collectArguments(filter, retInsertPos, specializedHandle);\n+        }\n+\n@@ -188,13 +185,1 @@\n-        if (llReturn != void.class) {\n-            int retAllocatorPos = -1; \/\/ assumed not needed\n-            int retInsertPos = 0;\n-            MethodHandle filter = identity(llReturn);\n-            List<Binding> bindings = callingSequence.returnBindings();\n-            for (int j = bindings.size() - 1; j >= 0; j--) {\n-                Binding binding = bindings.get(j);\n-                filter = binding.specialize(filter, retInsertPos, retAllocatorPos);\n-            }\n-            specializedHandle = filterReturnValue(specializedHandle, filter);\n-        }\n-\n-        specializedHandle = SharedUtils.wrapWithAllocator(specializedHandle, argAllocatorPos, bufferCopySize, true);\n+        specializedHandle = SharedUtils.wrapWithAllocator(specializedHandle, argAllocatorPos, callingSequence.allocationSize(), true);\n@@ -205,8 +190,6 @@\n-    public static void invoke(MethodHandle mh, long address) throws Throwable {\n-        mh.invokeExact(MemoryAddress.ofLong(address));\n-    }\n-\n-    private static void invokeMoves(MemoryAddress buffer, MethodHandle leaf,\n-                                    Binding.VMLoad[] argBindings, Binding.VMStore[] returnBindings,\n-                                    ABIDescriptor abi, BufferLayout layout) throws Throwable {\n-        MemorySegment bufferBase = MemoryAddressImpl.ofLongUnchecked(buffer.toRawLongValue(), layout.size);\n+    private record InvocationData(MethodHandle leaf,\n+                                  Map<VMStorage, Integer> argIndexMap,\n+                                  Map<VMStorage, Integer> retIndexMap,\n+                                  CallingSequence callingSequence,\n+                                  Binding.VMStore[] retMoves,\n+                                  ABIDescriptor abi) {}\n@@ -214,48 +197,3 @@\n-        if (DEBUG) {\n-            System.err.println(\"Buffer state before:\");\n-            layout.dump(abi.arch, bufferBase, System.err);\n-        }\n-\n-        MemorySegment stackArgsBase = MemoryAddressImpl.ofLongUnchecked((long)VH_LONG.get(bufferBase.asSlice(layout.stack_args)));\n-        Object[] moves = new Object[argBindings.length];\n-        for (int i = 0; i < moves.length; i++) {\n-            Binding.VMLoad binding = argBindings[i];\n-            VMStorage storage = binding.storage();\n-            MemorySegment ptr = abi.arch.isStackType(storage.type())\n-                ? stackArgsBase.asSlice(storage.index() * abi.arch.typeSize(abi.arch.stackType()))\n-                : bufferBase.asSlice(layout.argOffset(storage));\n-            moves[i] = SharedUtils.read(ptr, binding.type());\n-        }\n-\n-        \/\/ invokeInterpBindings, and then actual target\n-        Object o = leaf.invoke(moves);\n-\n-        if (o == null) {\n-            \/\/ nop\n-        } else if (o instanceof Object[] returns) {\n-            for (int i = 0; i < returnBindings.length; i++) {\n-                Binding.VMStore binding = returnBindings[i];\n-                VMStorage storage = binding.storage();\n-                MemorySegment ptr = bufferBase.asSlice(layout.retOffset(storage));\n-                SharedUtils.writeOverSized(ptr, binding.type(), returns[i]);\n-            }\n-        } else { \/\/ single Object\n-            Binding.VMStore binding = returnBindings[0];\n-            VMStorage storage = binding.storage();\n-            MemorySegment ptr = bufferBase.asSlice(layout.retOffset(storage));\n-            SharedUtils.writeOverSized(ptr, binding.type(), o);\n-        }\n-\n-        if (DEBUG) {\n-            System.err.println(\"Buffer state after:\");\n-            layout.dump(abi.arch, bufferBase, System.err);\n-        }\n-    }\n-\n-    private static Object invokeInterpBindings(Object[] moves, MethodHandle leaf,\n-                                               Map<VMStorage, Integer> argIndexMap,\n-                                               Map<VMStorage, Integer> retIndexMap,\n-                                               CallingSequence callingSequence,\n-                                               long bufferCopySize) throws Throwable {\n-        Binding.Context allocator = bufferCopySize != 0\n-                ? Binding.Context.ofBoundedAllocator(bufferCopySize)\n+    private static Object invokeInterpBindings(Object[] lowLevelArgs, InvocationData invData) throws Throwable {\n+        Binding.Context allocator = invData.callingSequence.allocationSize() != 0\n+                ? Binding.Context.ofBoundedAllocator(invData.callingSequence.allocationSize())\n@@ -265,4 +203,13 @@\n-            Object[] args = new Object[callingSequence.methodType().parameterCount()];\n-            for (int i = 0; i < args.length; i++) {\n-                args[i] = BindingInterpreter.box(callingSequence.argumentBindings(i),\n-                        (storage, type) -> moves[argIndexMap.get(storage)], allocator);\n+            Object[] highLevelArgs = new Object[invData.callingSequence.methodType().parameterCount()];\n+            for (int i = 0; i < highLevelArgs.length; i++) {\n+                highLevelArgs[i] = BindingInterpreter.box(invData.callingSequence.argumentBindings(i),\n+                        (storage, type) -> lowLevelArgs[invData.argIndexMap.get(storage)], allocator);\n+            }\n+\n+            MemorySegment returnBuffer = null;\n+            if (invData.callingSequence.needsReturnBuffer()) {\n+                \/\/ this one is for us\n+                returnBuffer = (MemorySegment) highLevelArgs[0];\n+                Object[] newArgs = new Object[highLevelArgs.length - 1];\n+                System.arraycopy(highLevelArgs, 1, newArgs, 0, newArgs.length);\n+                highLevelArgs = newArgs;\n@@ -273,1 +220,1 @@\n-                System.err.println(Arrays.toString(args).indent(2));\n+                System.err.println(Arrays.toString(highLevelArgs).indent(2));\n@@ -277,1 +224,1 @@\n-            Object o = leaf.invoke(args);\n+            Object o = invData.leaf.invoke(highLevelArgs);\n@@ -284,4 +231,4 @@\n-            Object[] returnMoves = new Object[retIndexMap.size()];\n-            if (leaf.type().returnType() != void.class) {\n-                BindingInterpreter.unbox(o, callingSequence.returnBindings(),\n-                        (storage, type, value) -> returnMoves[retIndexMap.get(storage)] = value, null);\n+            Object[] returnValues = new Object[invData.retIndexMap.size()];\n+            if (invData.leaf.type().returnType() != void.class) {\n+                BindingInterpreter.unbox(o, invData.callingSequence.returnBindings(),\n+                        (storage, type, value) -> returnValues[invData.retIndexMap.get(storage)] = value, null);\n@@ -290,1 +237,1 @@\n-            if (returnMoves.length == 0) {\n+            if (returnValues.length == 0) {\n@@ -292,2 +239,2 @@\n-            } else if (returnMoves.length == 1) {\n-                return returnMoves[0];\n+            } else if (returnValues.length == 1) {\n+                return returnValues[0];\n@@ -295,1 +242,16 @@\n-                return returnMoves;\n+                assert invData.callingSequence.needsReturnBuffer();\n+\n+                Binding.VMStore[] retMoves = invData.callingSequence.returnBindings().stream()\n+                        .filter(Binding.VMStore.class::isInstance)\n+                        .map(Binding.VMStore.class::cast)\n+                        .toArray(Binding.VMStore[]::new);\n+\n+                assert returnValues.length == retMoves.length;\n+                int retBufWriteOffset = 0;\n+                for (int i = 0; i < retMoves.length; i++) {\n+                    Binding.VMStore store = retMoves[i];\n+                    Object value = returnValues[i];\n+                    SharedUtils.writeOverSized(returnBuffer.asSlice(retBufWriteOffset), store.type(), value);\n+                    retBufWriteOffset += invData.abi.arch.typeSize(store.storage().type());\n+                }\n+                return null;\n@@ -306,3 +268,2 @@\n-    static native long allocateOptimizedUpcallStub(MethodHandle mh, ABIDescriptor abi, CallRegs conv);\n-    static native long allocateUpcallStub(MethodHandle mh, ABIDescriptor abi, BufferLayout layout);\n-    static native boolean supportsOptimizedUpcalls();\n+    static native long allocateOptimizedUpcallStub(MethodHandle mh, ABIDescriptor abi, CallRegs conv,\n+                                                   boolean needsReturnBuffer, long returnBufferSize);\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/foreign\/abi\/ProgrammableUpcallHandler.java","additions":107,"deletions":146,"binary":false,"changes":253,"status":"modified"},{"patch":"@@ -297,21 +297,0 @@\n-    static long bufferCopySize(CallingSequence callingSequence) {\n-        \/\/ FIXME: > 16 bytes alignment might need extra space since the\n-        \/\/ starting address of the allocator might be un-aligned.\n-        long size = 0;\n-        for (int i = 0; i < callingSequence.argumentCount(); i++) {\n-            List<Binding> bindings = callingSequence.argumentBindings(i);\n-            for (Binding b : bindings) {\n-                if (b instanceof Binding.Copy) {\n-                    Binding.Copy c = (Binding.Copy) b;\n-                    size = Utils.alignUp(size, c.alignment());\n-                    size += c.size();\n-                } else if (b instanceof Binding.Allocate) {\n-                    Binding.Allocate c = (Binding.Allocate) b;\n-                    size = Utils.alignUp(size, c.alignment());\n-                    size += c.size();\n-                }\n-            }\n-        }\n-        return size;\n-    }\n-\n@@ -334,1 +313,2 @@\n-        assert destIndex > sourceIndex;\n+        if (destIndex < sourceIndex)\n+            sourceIndex--;\n@@ -372,1 +352,1 @@\n-                                          int allocatorPos, long bufferCopySize,\n+                                          int allocatorPos, long allocationSize,\n@@ -394,1 +374,1 @@\n-        \/\/ downcalls get the leading NativeSymbol\/SegmentAllocator param as well\n+        \/\/ downcalls get the leading SegmentAllocator param as well\n@@ -396,2 +376,1 @@\n-            closer = collectArguments(closer, insertPos++, reachabilityFenceHandle(Addressable.class));\n-            closer = dropArguments(closer, insertPos++, SegmentAllocator.class); \/\/ (Throwable, V?, NativeSymbol, SegmentAllocator) -> V\/void\n+            closer = dropArguments(closer, insertPos++, SegmentAllocator.class); \/\/ (Throwable, V?, SegmentAllocator, Addressable) -> V\/void\n@@ -400,1 +379,1 @@\n-        closer = collectArguments(closer, insertPos++, MH_CLOSE_CONTEXT); \/\/ (Throwable, V?, NativeSymbol?, BindingContext) -> V\/void\n+        closer = collectArguments(closer, insertPos, MH_CLOSE_CONTEXT); \/\/ (Throwable, V?, SegmentAllocator?, BindingContext) -> V\/void\n@@ -404,2 +383,2 @@\n-        if (bufferCopySize > 0) {\n-            contextFactory = MethodHandles.insertArguments(MH_MAKE_CONTEXT_BOUNDED_ALLOCATOR, 0, bufferCopySize);\n+        if (allocationSize > 0) {\n+            contextFactory = MethodHandles.insertArguments(MH_MAKE_CONTEXT_BOUNDED_ALLOCATOR, 0, allocationSize);\n@@ -559,0 +538,8 @@\n+    public static MethodHandle maybeInsertAllocator(MethodHandle handle) {\n+        if (!handle.type().returnType().equals(MemorySegment.class)) {\n+            \/\/ not returning segment, just insert a throwing allocator\n+            handle = insertArguments(handle, 1, THROWING_ALLOCATOR);\n+        }\n+        return handle;\n+    }\n+\n@@ -607,4 +594,0 @@\n-    public static boolean isTrivial(FunctionDescriptor cDesc) {\n-        return false; \/\/ FIXME: use system property?\n-    }\n-\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/foreign\/abi\/SharedUtils.java","additions":16,"deletions":33,"binary":false,"changes":49,"status":"modified"},{"patch":"@@ -27,2 +27,0 @@\n-import jdk.internal.invoke.VMStorageProxy;\n-\n@@ -31,1 +29,1 @@\n-public class VMStorage implements VMStorageProxy {\n+public class VMStorage {\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/foreign\/abi\/VMStorage.java","additions":1,"deletions":3,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -151,1 +151,2 @@\n-                                       int shadowSpace) {\n+                                       int shadowSpace,\n+                                       VMStorage targetAddrStorage, VMStorage retBufAddrStorage) {\n@@ -167,2 +168,2 @@\n-            shadowSpace\n-        );\n+            shadowSpace,\n+                targetAddrStorage, retBufAddrStorage);\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/foreign\/abi\/aarch64\/AArch64Architecture.java","additions":4,"deletions":3,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -90,1 +90,3 @@\n-        0    \/\/ No shadow space\n+        0,   \/\/ No shadow space\n+        r9,  \/\/ target addr reg\n+        r10  \/\/ return buffer addr reg\n@@ -118,1 +120,1 @@\n-        CallingSequenceBuilder csb = new CallingSequenceBuilder(forUpcall);\n+        CallingSequenceBuilder csb = new CallingSequenceBuilder(C, forUpcall);\n@@ -142,2 +144,0 @@\n-        csb.setTrivial(SharedUtils.isTrivial(cDesc));\n-\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/foreign\/abi\/aarch64\/CallArranger.java","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -67,4 +67,1 @@\n-        if (!type.returnType().equals(MemorySegment.class)) {\n-            \/\/ not returning segment, just insert a throwing allocator\n-            handle = MethodHandles.insertArguments(handle, 1, SharedUtils.THROWING_ALLOCATOR);\n-        }\n+        handle = SharedUtils.maybeInsertAllocator(handle);\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/foreign\/abi\/aarch64\/linux\/LinuxAArch64Linker.java","additions":1,"deletions":4,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -67,4 +67,1 @@\n-        if (!type.returnType().equals(MemorySegment.class)) {\n-            \/\/ not returning segment, just insert a throwing allocator\n-            handle = MethodHandles.insertArguments(handle, 1, SharedUtils.THROWING_ALLOCATOR);\n-        }\n+        handle = SharedUtils.maybeInsertAllocator(handle);\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/foreign\/abi\/aarch64\/macos\/MacOsAArch64Linker.java","additions":1,"deletions":4,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -137,1 +137,2 @@\n-                                       VMStorage[] volatileVectorRegs, int stackAlignment, int shadowSpace) {\n+                                       VMStorage[] volatileVectorRegs, int stackAlignment, int shadowSpace,\n+                                       VMStorage targetAddrStorage, VMStorage retBufAddrStorage) {\n@@ -154,2 +155,2 @@\n-            shadowSpace\n-        );\n+            shadowSpace,\n+                targetAddrStorage, retBufAddrStorage);\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/foreign\/abi\/x64\/X86_64Architecture.java","additions":4,"deletions":3,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -69,1 +69,3 @@\n-        0 \/\/no shadow space\n+        0, \/\/no shadow space\n+        r10, \/\/ target addr reg\n+        r11  \/\/ ret buf addr reg\n@@ -86,1 +88,1 @@\n-        CallingSequenceBuilder csb = new CallingSequenceBuilder(forUpcall);\n+        CallingSequenceBuilder csb = new CallingSequenceBuilder(CSysV, forUpcall);\n@@ -114,2 +116,0 @@\n-        csb.setTrivial(SharedUtils.isTrivial(cDesc));\n-\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/foreign\/abi\/x64\/sysv\/CallArranger.java","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -39,1 +39,0 @@\n-import java.lang.invoke.MethodHandles;\n@@ -77,4 +76,1 @@\n-        if (!type.returnType().equals(MemorySegment.class)) {\n-            \/\/ not returning segment, just insert a throwing allocator\n-            handle = MethodHandles.insertArguments(handle, 1, SharedUtils.THROWING_ALLOCATOR);\n-        }\n+        handle = SharedUtils.maybeInsertAllocator(handle);\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/foreign\/abi\/x64\/sysv\/SysVx64Linker.java","additions":1,"deletions":5,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -70,1 +70,3 @@\n-        32\n+        32,\n+        r10, \/\/ target addr reg\n+        r11  \/\/ ret buf addr reg\n@@ -86,1 +88,1 @@\n-            final CallingSequenceBuilder csb = new CallingSequenceBuilder(forUpcall);\n+            final CallingSequenceBuilder csb = new CallingSequenceBuilder(CWindows, forUpcall);\n@@ -118,2 +120,0 @@\n-        csb.csb.setTrivial(SharedUtils.isTrivial(cDesc));\n-\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/foreign\/abi\/x64\/windows\/CallArranger.java","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -78,4 +78,1 @@\n-        if (!type.returnType().equals(MemorySegment.class)) {\n-            \/\/ not returning segment, just insert a throwing allocator\n-            handle = MethodHandles.insertArguments(handle, 1, SharedUtils.THROWING_ALLOCATOR);\n-        }\n+        handle = SharedUtils.maybeInsertAllocator(handle);\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/foreign\/abi\/x64\/windows\/Windowsx64Linker.java","additions":1,"deletions":4,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -1,29 +0,0 @@\n-\/*\n- * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.  Oracle designates this\n- * particular file as subject to the \"Classpath\" exception as provided\n- * by Oracle in the LICENSE file that accompanied this code.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-package jdk.internal.invoke;\n-\n-public interface ABIDescriptorProxy {\n-    int shadowSpaceBytes();\n-}\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/invoke\/ABIDescriptorProxy.java","additions":0,"deletions":29,"binary":false,"changes":29,"status":"deleted"},{"patch":"@@ -1,86 +0,0 @@\n-\/*\n- * Copyright (c) 2020, 2021, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.  Oracle designates this\n- * particular file as subject to the \"Classpath\" exception as provided\n- * by Oracle in the LICENSE file that accompanied this code.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-package jdk.internal.invoke;\n-\n-import java.lang.invoke.MethodType;\n-import java.util.Objects;\n-\n-\/**\n- * This class describes a native call, including arguments\/return shuffle moves, PC entry point and\n- * various other info which are relevant when the call will be intrinsified by C2.\n- *\/\n-public class NativeEntryPoint {\n-    static {\n-        registerNatives();\n-    }\n-\n-    private final int shadowSpace;\n-\n-    \/\/ encoded as VMRegImpl*\n-    private final long[] argMoves;\n-    private final long[] returnMoves;\n-\n-    private final boolean needTransition;\n-    private final MethodType methodType; \/\/ C2 sees erased version (byte -> int), so need this explicitly\n-    private final String name;\n-\n-    private NativeEntryPoint(int shadowSpace, long[] argMoves, long[] returnMoves,\n-                     boolean needTransition, MethodType methodType, String name) {\n-        this.shadowSpace = shadowSpace;\n-        this.argMoves = Objects.requireNonNull(argMoves);\n-        this.returnMoves = Objects.requireNonNull(returnMoves);\n-        this.needTransition = needTransition;\n-        this.methodType = methodType;\n-        this.name = name;\n-    }\n-\n-    public static NativeEntryPoint make(String name, ABIDescriptorProxy abi,\n-                                        VMStorageProxy[] argMoves, VMStorageProxy[] returnMoves,\n-                                        boolean needTransition, MethodType methodType) {\n-        if (returnMoves.length > 1) {\n-            throw new IllegalArgumentException(\"Multiple register return not supported\");\n-        }\n-\n-        return new NativeEntryPoint(abi.shadowSpaceBytes(), encodeVMStorages(argMoves), encodeVMStorages(returnMoves),\n-                needTransition, methodType, name);\n-    }\n-\n-    private static long[] encodeVMStorages(VMStorageProxy[] moves) {\n-        long[] out = new long[moves.length];\n-        for (int i = 0; i < moves.length; i++) {\n-            out[i] = vmStorageToVMReg(moves[i].type(), moves[i].index());\n-        }\n-        return out;\n-    }\n-\n-    private static native long vmStorageToVMReg(int type, int index);\n-\n-    public MethodType type() {\n-        return methodType;\n-    }\n-\n-    private static native void registerNatives();\n-}\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/invoke\/NativeEntryPoint.java","additions":0,"deletions":86,"binary":false,"changes":86,"status":"deleted"},{"patch":"@@ -1,30 +0,0 @@\n-\/*\n- * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.  Oracle designates this\n- * particular file as subject to the \"Classpath\" exception as provided\n- * by Oracle in the LICENSE file that accompanied this code.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-package jdk.internal.invoke;\n-\n-public interface VMStorageProxy {\n-    int type();\n-    int index();\n-}\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/invoke\/VMStorageProxy.java","additions":0,"deletions":30,"binary":false,"changes":30,"status":"deleted"},{"patch":"@@ -35,0 +35,1 @@\n+import java.lang.foreign.Addressable;\n@@ -47,0 +48,1 @@\n+import static java.lang.foreign.ValueLayout.ADDRESS;\n@@ -64,2 +66,2 @@\n-        assertEquals(callingSequence.methodType(), mt);\n-        assertEquals(callingSequence.functionDesc(), fd);\n+        assertEquals(callingSequence.methodType(), mt.insertParameterTypes(0, Addressable.class));\n+        assertEquals(callingSequence.functionDesc(), fd.insertArgumentLayouts(0, ADDRESS));\n@@ -67,1 +69,3 @@\n-        checkArgumentBindings(callingSequence, new Binding[][]{});\n+        checkArgumentBindings(callingSequence, new Binding[][]{\n+            { unboxAddress(Addressable.class), vmStore(r9, long.class) }\n+        });\n@@ -86,2 +90,2 @@\n-        assertEquals(callingSequence.methodType(), mt);\n-        assertEquals(callingSequence.functionDesc(), fd);\n+        assertEquals(callingSequence.methodType(), mt.insertParameterTypes(0, Addressable.class));\n+        assertEquals(callingSequence.functionDesc(), fd.insertArgumentLayouts(0, ADDRESS));\n@@ -90,10 +94,11 @@\n-                { vmStore(r0, int.class) },\n-                { vmStore(r1, int.class) },\n-                { vmStore(r2, int.class) },\n-                { vmStore(r3, int.class) },\n-                { vmStore(r4, int.class) },\n-                { vmStore(r5, int.class) },\n-                { vmStore(r6, int.class) },\n-                { vmStore(r7, int.class) },\n-                { vmStore(stackStorage(0), int.class) },\n-                { vmStore(stackStorage(1), int.class) },\n+            { unboxAddress(Addressable.class), vmStore(r9, long.class) },\n+            { vmStore(r0, int.class) },\n+            { vmStore(r1, int.class) },\n+            { vmStore(r2, int.class) },\n+            { vmStore(r3, int.class) },\n+            { vmStore(r4, int.class) },\n+            { vmStore(r5, int.class) },\n+            { vmStore(r6, int.class) },\n+            { vmStore(r7, int.class) },\n+            { vmStore(stackStorage(0), int.class) },\n+            { vmStore(stackStorage(1), int.class) },\n@@ -115,2 +120,2 @@\n-        assertEquals(callingSequence.methodType(), mt);\n-        assertEquals(callingSequence.functionDesc(), fd);\n+        assertEquals(callingSequence.methodType(), mt.insertParameterTypes(0, Addressable.class));\n+        assertEquals(callingSequence.functionDesc(), fd.insertArgumentLayouts(0, ADDRESS));\n@@ -119,4 +124,5 @@\n-                { vmStore(r0, int.class) },\n-                { vmStore(r1, int.class) },\n-                { vmStore(v0, float.class) },\n-                { vmStore(v1, float.class) },\n+            { unboxAddress(Addressable.class), vmStore(r9, long.class) },\n+            { vmStore(r0, int.class) },\n+            { vmStore(r1, int.class) },\n+            { vmStore(v0, float.class) },\n+            { vmStore(v1, float.class) },\n@@ -136,2 +142,2 @@\n-        assertEquals(callingSequence.methodType(), mt);\n-        assertEquals(callingSequence.functionDesc(), fd);\n+        assertEquals(callingSequence.methodType(), mt.insertParameterTypes(0, Addressable.class));\n+        assertEquals(callingSequence.functionDesc(), fd.insertArgumentLayouts(0, ADDRESS));\n@@ -140,1 +146,2 @@\n-                expectedBindings\n+            { unboxAddress(Addressable.class), vmStore(r9, long.class) },\n+            expectedBindings\n@@ -150,31 +157,31 @@\n-                \/\/ struct s { int32_t a, b; double c; };\n-                { MemoryLayout.structLayout(C_INT, C_INT, C_DOUBLE), new Binding[] {\n-                        dup(),\n-                        \/\/ s.a & s.b\n-                        bufferLoad(0, long.class), vmStore(r0, long.class),\n-                        \/\/ s.c --> note AArch64 passes this in an *integer* register\n-                        bufferLoad(8, long.class), vmStore(r1, long.class),\n-                }},\n-                \/\/ struct s { int32_t a, b; double c; int32_t d };\n-                { struct2, new Binding[] {\n-                        copy(struct2),\n-                        unboxAddress(MemorySegment.class),\n-                        vmStore(r0, long.class)\n-                }},\n-                \/\/ struct s { int32_t a[2]; float b[2] };\n-                { MemoryLayout.structLayout(C_INT, C_INT, C_FLOAT, C_FLOAT), new Binding[] {\n-                        dup(),\n-                        \/\/ s.a[0] & s.a[1]\n-                        bufferLoad(0, long.class), vmStore(r0, long.class),\n-                        \/\/ s.b[0] & s.b[1]\n-                        bufferLoad(8, long.class), vmStore(r1, long.class),\n-                }},\n-                \/\/ struct s { float a; \/* padding *\/ double b };\n-                { MemoryLayout.structLayout(C_FLOAT, MemoryLayout.paddingLayout(32), C_DOUBLE),\n-                        new Binding[] {\n-                                dup(),\n-                                \/\/ s.a\n-                                bufferLoad(0, long.class), vmStore(r0, long.class),\n-                                \/\/ s.b\n-                                bufferLoad(8, long.class), vmStore(r1, long.class),\n-                        }},\n+            \/\/ struct s { int32_t a, b; double c; };\n+            { MemoryLayout.structLayout(C_INT, C_INT, C_DOUBLE), new Binding[] {\n+                dup(),\n+                    \/\/ s.a & s.b\n+                    bufferLoad(0, long.class), vmStore(r0, long.class),\n+                    \/\/ s.c --> note AArch64 passes this in an *integer* register\n+                    bufferLoad(8, long.class), vmStore(r1, long.class),\n+            }},\n+            \/\/ struct s { int32_t a, b; double c; int32_t d };\n+            { struct2, new Binding[] {\n+                copy(struct2),\n+                unboxAddress(MemorySegment.class),\n+                vmStore(r0, long.class)\n+            }},\n+            \/\/ struct s { int32_t a[2]; float b[2] };\n+            { MemoryLayout.structLayout(C_INT, C_INT, C_FLOAT, C_FLOAT), new Binding[] {\n+                dup(),\n+                    \/\/ s.a[0] & s.a[1]\n+                    bufferLoad(0, long.class), vmStore(r0, long.class),\n+                    \/\/ s.b[0] & s.b[1]\n+                    bufferLoad(8, long.class), vmStore(r1, long.class),\n+            }},\n+            \/\/ struct s { float a; \/* padding *\/ double b };\n+            { MemoryLayout.structLayout(C_FLOAT, MemoryLayout.paddingLayout(32), C_DOUBLE),\n+              new Binding[] {\n+                dup(),\n+                \/\/ s.a\n+                bufferLoad(0, long.class), vmStore(r0, long.class),\n+                \/\/ s.b\n+                bufferLoad(8, long.class), vmStore(r1, long.class),\n+            }},\n@@ -195,2 +202,2 @@\n-        assertEquals(callingSequence.methodType(), mt);\n-        assertEquals(callingSequence.functionDesc(), fd);\n+        assertEquals(callingSequence.methodType(), mt.insertParameterTypes(0, Addressable.class));\n+        assertEquals(callingSequence.functionDesc(), fd.insertArgumentLayouts(0, ADDRESS));\n@@ -199,11 +206,12 @@\n-                {\n-                        copy(struct1),\n-                        unboxAddress(MemorySegment.class),\n-                        vmStore(r0, long.class)\n-                },\n-                {\n-                        copy(struct2),\n-                        unboxAddress(MemorySegment.class),\n-                        vmStore(r1, long.class)\n-                },\n-                { vmStore(r2, int.class) }\n+            { unboxAddress(Addressable.class), vmStore(r9, long.class) },\n+            {\n+                copy(struct1),\n+                unboxAddress(MemorySegment.class),\n+                vmStore(r0, long.class)\n+            },\n+            {\n+                copy(struct2),\n+                unboxAddress(MemorySegment.class),\n+                vmStore(r1, long.class)\n+            },\n+            { vmStore(r2, int.class) }\n@@ -225,2 +233,2 @@\n-        assertEquals(callingSequence.methodType(), MethodType.methodType(void.class, MemoryAddress.class));\n-        assertEquals(callingSequence.functionDesc(), FunctionDescriptor.ofVoid(C_POINTER));\n+        assertEquals(callingSequence.methodType(), MethodType.methodType(void.class, Addressable.class, MemoryAddress.class));\n+        assertEquals(callingSequence.functionDesc(), FunctionDescriptor.ofVoid(ADDRESS, C_POINTER));\n@@ -229,4 +237,5 @@\n-                {\n-                        unboxAddress(),\n-                        vmStore(r8, long.class)\n-                }\n+            { unboxAddress(Addressable.class), vmStore(r9, long.class) },\n+            {\n+                unboxAddress(),\n+                vmStore(r8, long.class)\n+            }\n@@ -248,2 +257,2 @@\n-        assertEquals(callingSequence.methodType(), mt);\n-        assertEquals(callingSequence.functionDesc(), fd);\n+        assertEquals(callingSequence.methodType(), mt.insertParameterTypes(0, MemorySegment.class, Addressable.class));\n+        assertEquals(callingSequence.functionDesc(), fd.insertArgumentLayouts(0, ADDRESS, ADDRESS));\n@@ -251,1 +260,4 @@\n-        checkArgumentBindings(callingSequence, new Binding[][]{});\n+        checkArgumentBindings(callingSequence, new Binding[][]{\n+            { unboxAddress(MemorySegment.class), vmStore(r10, long.class) },\n+            { unboxAddress(Addressable.class), vmStore(r9, long.class) }\n+        });\n@@ -254,7 +266,7 @@\n-                allocate(struct),\n-                dup(),\n-                vmLoad(r0, long.class),\n-                bufferStore(0, long.class),\n-                dup(),\n-                vmLoad(r1, long.class),\n-                bufferStore(8, long.class),\n+            allocate(struct),\n+            dup(),\n+            vmLoad(r0, long.class),\n+            bufferStore(0, long.class),\n+            dup(),\n+            vmLoad(r1, long.class),\n+            bufferStore(8, long.class),\n@@ -274,2 +286,2 @@\n-        assertEquals(callingSequence.methodType(), mt);\n-        assertEquals(callingSequence.functionDesc(), fd);\n+        assertEquals(callingSequence.methodType(), mt.insertParameterTypes(0, MemorySegment.class, Addressable.class));\n+        assertEquals(callingSequence.functionDesc(), fd.insertArgumentLayouts(0, ADDRESS, ADDRESS));\n@@ -278,9 +290,11 @@\n-                { vmStore(v0, float.class) },\n-                { vmStore(r0, int.class) },\n-                {\n-                        dup(),\n-                        bufferLoad(0, float.class),\n-                        vmStore(v1, float.class),\n-                        bufferLoad(4, float.class),\n-                        vmStore(v2, float.class)\n-                }\n+            { unboxAddress(MemorySegment.class), vmStore(r10, long.class) },\n+            { unboxAddress(Addressable.class), vmStore(r9, long.class) },\n+            { vmStore(v0, float.class) },\n+            { vmStore(r0, int.class) },\n+            {\n+                dup(),\n+                bufferLoad(0, float.class),\n+                vmStore(v1, float.class),\n+                bufferLoad(4, float.class),\n+                vmStore(v2, float.class)\n+            }\n@@ -290,7 +304,7 @@\n-                allocate(hfa),\n-                dup(),\n-                vmLoad(v0, float.class),\n-                bufferStore(0, float.class),\n-                dup(),\n-                vmLoad(v1, float.class),\n-                bufferStore(4, float.class),\n+            allocate(hfa),\n+            dup(),\n+            vmLoad(v0, float.class),\n+            bufferStore(0, float.class),\n+            dup(),\n+            vmLoad(v1, float.class),\n+            bufferStore(4, float.class),\n@@ -310,2 +324,2 @@\n-        assertEquals(callingSequence.methodType(), mt);\n-        assertEquals(callingSequence.functionDesc(), fd);\n+        assertEquals(callingSequence.methodType(), mt.insertParameterTypes(0, Addressable.class));\n+        assertEquals(callingSequence.functionDesc(), fd.insertArgumentLayouts(0, ADDRESS));\n@@ -314,27 +328,28 @@\n-                {\n-                        dup(),\n-                        bufferLoad(0, float.class),\n-                        vmStore(v0, float.class),\n-                        dup(),\n-                        bufferLoad(4, float.class),\n-                        vmStore(v1, float.class),\n-                        bufferLoad(8, float.class),\n-                        vmStore(v2, float.class)\n-                },\n-                {\n-                        dup(),\n-                        bufferLoad(0, float.class),\n-                        vmStore(v3, float.class),\n-                        dup(),\n-                        bufferLoad(4, float.class),\n-                        vmStore(v4, float.class),\n-                        bufferLoad(8, float.class),\n-                        vmStore(v5, float.class)\n-                },\n-                {\n-                        dup(),\n-                        bufferLoad(0, long.class),\n-                        vmStore(stackStorage(0), long.class),\n-                        bufferLoad(8, int.class),\n-                        vmStore(stackStorage(1), int.class),\n-                }\n+            { unboxAddress(Addressable.class), vmStore(r9, long.class) },\n+            {\n+                dup(),\n+                bufferLoad(0, float.class),\n+                vmStore(v0, float.class),\n+                dup(),\n+                bufferLoad(4, float.class),\n+                vmStore(v1, float.class),\n+                bufferLoad(8, float.class),\n+                vmStore(v2, float.class)\n+            },\n+            {\n+                dup(),\n+                bufferLoad(0, float.class),\n+                vmStore(v3, float.class),\n+                dup(),\n+                bufferLoad(4, float.class),\n+                vmStore(v4, float.class),\n+                bufferLoad(8, float.class),\n+                vmStore(v5, float.class)\n+            },\n+            {\n+                dup(),\n+                bufferLoad(0, long.class),\n+                vmStore(stackStorage(0), long.class),\n+                bufferLoad(8, int.class),\n+                vmStore(stackStorage(1), int.class),\n+            }\n@@ -355,2 +370,2 @@\n-                void.class, MemorySegment.class, MemorySegment.class, int.class, int.class,\n-                int.class, int.class, int.class, int.class, MemorySegment.class, int.class);\n+            void.class, MemorySegment.class, MemorySegment.class, int.class, int.class,\n+            int.class, int.class, int.class, int.class, MemorySegment.class, int.class);\n@@ -358,1 +373,1 @@\n-                struct, struct, C_INT, C_INT, C_INT, C_INT, C_INT, C_INT, struct, C_INT);\n+            struct, struct, C_INT, C_INT, C_INT, C_INT, C_INT, C_INT, struct, C_INT);\n@@ -363,2 +378,2 @@\n-        assertEquals(callingSequence.methodType(), mt);\n-        assertEquals(callingSequence.functionDesc(), fd);\n+        assertEquals(callingSequence.methodType(), mt.insertParameterTypes(0, Addressable.class));\n+        assertEquals(callingSequence.functionDesc(), fd.insertArgumentLayouts(0, ADDRESS));\n@@ -367,10 +382,11 @@\n-                { copy(struct), unboxAddress(MemorySegment.class), vmStore(r0, long.class) },\n-                { copy(struct), unboxAddress(MemorySegment.class), vmStore(r1, long.class) },\n-                { vmStore(r2, int.class) },\n-                { vmStore(r3, int.class) },\n-                { vmStore(r4, int.class) },\n-                { vmStore(r5, int.class) },\n-                { vmStore(r6, int.class) },\n-                { vmStore(r7, int.class) },\n-                { copy(struct), unboxAddress(MemorySegment.class), vmStore(stackStorage(0), long.class) },\n-                { vmStore(stackStorage(1), int.class) },\n+            { unboxAddress(Addressable.class), vmStore(r9, long.class) },\n+            { copy(struct), unboxAddress(MemorySegment.class), vmStore(r0, long.class) },\n+            { copy(struct), unboxAddress(MemorySegment.class), vmStore(r1, long.class) },\n+            { vmStore(r2, int.class) },\n+            { vmStore(r3, int.class) },\n+            { vmStore(r4, int.class) },\n+            { vmStore(r5, int.class) },\n+            { vmStore(r6, int.class) },\n+            { vmStore(r7, int.class) },\n+            { copy(struct), unboxAddress(MemorySegment.class), vmStore(stackStorage(0), long.class) },\n+            { vmStore(stackStorage(1), int.class) },\n@@ -386,0 +402,1 @@\n+        FunctionDescriptor fdExpected = FunctionDescriptor.ofVoid(ADDRESS, C_INT, C_INT, C_FLOAT);\n@@ -390,2 +407,2 @@\n-        assertEquals(callingSequence.methodType(), mt);\n-        assertEquals(callingSequence.functionDesc(), FunctionDescriptor.ofVoid(C_INT, C_INT, C_FLOAT));\n+        assertEquals(callingSequence.methodType(), mt.insertParameterTypes(0, Addressable.class));\n+        assertEquals(callingSequence.functionDesc(), fdExpected);\n@@ -395,3 +412,4 @@\n-                { vmStore(r0, int.class) },\n-                { vmStore(r1, int.class) },\n-                { vmStore(v0, float.class) },\n+            { unboxAddress(Addressable.class), vmStore(r9, long.class) },\n+            { vmStore(r0, int.class) },\n+            { vmStore(r1, int.class) },\n+            { vmStore(v0, float.class) },\n@@ -407,0 +425,1 @@\n+        FunctionDescriptor fdExpected = FunctionDescriptor.ofVoid(ADDRESS, C_INT, C_INT, C_FLOAT);\n@@ -411,2 +430,2 @@\n-        assertEquals(callingSequence.methodType(), mt);\n-        assertEquals(callingSequence.functionDesc(), FunctionDescriptor.ofVoid(C_INT, C_INT, C_FLOAT));\n+        assertEquals(callingSequence.methodType(), mt.insertParameterTypes(0, Addressable.class));\n+        assertEquals(callingSequence.functionDesc(), fdExpected);\n@@ -416,3 +435,4 @@\n-                { vmStore(r0, int.class) },\n-                { vmStore(stackStorage(0), int.class) },\n-                { vmStore(stackStorage(1), float.class) },\n+            { unboxAddress(Addressable.class), vmStore(r9, long.class) },\n+            { vmStore(r0, int.class) },\n+            { vmStore(stackStorage(0), int.class) },\n+            { vmStore(stackStorage(1), float.class) },\n","filename":"test\/jdk\/java\/foreign\/callarranger\/TestAarch64CallArranger.java","additions":176,"deletions":156,"binary":false,"changes":332,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+import java.lang.foreign.Addressable;\n@@ -48,0 +49,1 @@\n+import static java.lang.foreign.ValueLayout.ADDRESS;\n@@ -65,2 +67,2 @@\n-        assertEquals(callingSequence.methodType(), mt.appendParameterTypes(long.class));\n-        assertEquals(callingSequence.functionDesc(), fd.appendArgumentLayouts(C_LONG));\n+        assertEquals(callingSequence.methodType(), mt.appendParameterTypes(long.class).insertParameterTypes(0, Addressable.class));\n+        assertEquals(callingSequence.functionDesc(), fd.appendArgumentLayouts(C_LONG).insertArgumentLayouts(0, ADDRESS));\n@@ -69,1 +71,2 @@\n-                { vmStore(rax, long.class) }\n+            { unboxAddress(Addressable.class), vmStore(r10, long.class) },\n+            { vmStore(rax, long.class) }\n@@ -92,2 +95,2 @@\n-        assertEquals(callingSequence.methodType(), mt.appendParameterTypes(long.class));\n-        assertEquals(callingSequence.functionDesc(), fd.appendArgumentLayouts(C_LONG));\n+        assertEquals(callingSequence.methodType(), mt.appendParameterTypes(long.class).insertParameterTypes(0, Addressable.class));\n+        assertEquals(callingSequence.functionDesc(), fd.appendArgumentLayouts(C_LONG).insertArgumentLayouts(0, ADDRESS));\n@@ -96,3 +99,4 @@\n-                { dup(), bufferLoad(0, long.class), vmStore(rdi, long.class),\n-                        bufferLoad(8, int.class), vmStore(rsi, int.class)},\n-                { vmStore(rax, long.class) },\n+            { unboxAddress(Addressable.class), vmStore(r10, long.class) },\n+            { dup(), bufferLoad(0, long.class), vmStore(rdi, long.class),\n+              bufferLoad(8, int.class), vmStore(rsi, int.class)},\n+            { vmStore(rax, long.class) },\n@@ -122,2 +126,2 @@\n-        assertEquals(callingSequence.methodType(), mt.appendParameterTypes(long.class));\n-        assertEquals(callingSequence.functionDesc(), fd.appendArgumentLayouts(C_LONG));\n+        assertEquals(callingSequence.methodType(), mt.appendParameterTypes(long.class).insertParameterTypes(0, Addressable.class));\n+        assertEquals(callingSequence.functionDesc(), fd.appendArgumentLayouts(C_LONG).insertArgumentLayouts(0, ADDRESS));\n@@ -126,3 +130,4 @@\n-                { dup(), bufferLoad(0, long.class), vmStore(rdi, long.class),\n-                        bufferLoad(8, long.class), vmStore(rsi, long.class)},\n-                { vmStore(rax, long.class) },\n+            { unboxAddress(Addressable.class), vmStore(r10, long.class) },\n+            { dup(), bufferLoad(0, long.class), vmStore(rdi, long.class),\n+                    bufferLoad(8, long.class), vmStore(rsi, long.class)},\n+            { vmStore(rax, long.class) },\n@@ -151,2 +156,2 @@\n-        assertEquals(callingSequence.methodType(), mt.appendParameterTypes(long.class));\n-        assertEquals(callingSequence.functionDesc(), fd.appendArgumentLayouts(C_LONG));\n+        assertEquals(callingSequence.methodType(), mt.appendParameterTypes(long.class).insertParameterTypes(0, Addressable.class));\n+        assertEquals(callingSequence.functionDesc(), fd.appendArgumentLayouts(C_LONG).insertArgumentLayouts(0, ADDRESS));\n@@ -155,3 +160,4 @@\n-                { dup(), bufferLoad(0, long.class), vmStore(stackStorage(0), long.class),\n-                        bufferLoad(8, long.class), vmStore(stackStorage(1), long.class)},\n-                { vmStore(rax, long.class) },\n+            { unboxAddress(Addressable.class), vmStore(r10, long.class) },\n+            { dup(), bufferLoad(0, long.class), vmStore(stackStorage(0), long.class),\n+                    bufferLoad(8, long.class), vmStore(stackStorage(1), long.class)},\n+            { vmStore(rax, long.class) },\n@@ -180,2 +186,2 @@\n-        assertEquals(callingSequence.methodType(), mt.appendParameterTypes(long.class));\n-        assertEquals(callingSequence.functionDesc(), fd.appendArgumentLayouts(C_LONG));\n+        assertEquals(callingSequence.methodType(), mt.appendParameterTypes(long.class).insertParameterTypes(0, Addressable.class));\n+        assertEquals(callingSequence.functionDesc(), fd.appendArgumentLayouts(C_LONG).insertArgumentLayouts(0, ADDRESS));\n@@ -184,3 +190,4 @@\n-                { dup(), bufferLoad(0, long.class), vmStore(stackStorage(0), long.class),\n-                        bufferLoad(8, int.class), vmStore(stackStorage(1), int.class)},\n-                { vmStore(rax, long.class) },\n+            { unboxAddress(Addressable.class), vmStore(r10, long.class) },\n+            { dup(), bufferLoad(0, long.class), vmStore(stackStorage(0), long.class),\n+                    bufferLoad(8, int.class), vmStore(stackStorage(1), int.class)},\n+            { vmStore(rax, long.class) },\n@@ -204,2 +211,2 @@\n-        assertEquals(callingSequence.methodType(), mt.appendParameterTypes(long.class));\n-        assertEquals(callingSequence.functionDesc(), fd.appendArgumentLayouts(C_LONG));\n+        assertEquals(callingSequence.methodType(), mt.appendParameterTypes(long.class).insertParameterTypes(0, Addressable.class));\n+        assertEquals(callingSequence.functionDesc(), fd.appendArgumentLayouts(C_LONG).insertArgumentLayouts(0, ADDRESS));\n@@ -208,7 +215,8 @@\n-                { vmStore(rdi, int.class) },\n-                { vmStore(rsi, int.class) },\n-                { vmStore(rdx, int.class) },\n-                { vmStore(rcx, int.class) },\n-                { vmStore(r8, int.class) },\n-                { vmStore(r9, int.class) },\n-                { vmStore(rax, long.class) },\n+            { unboxAddress(Addressable.class), vmStore(r10, long.class) },\n+            { vmStore(rdi, int.class) },\n+            { vmStore(rsi, int.class) },\n+            { vmStore(rdx, int.class) },\n+            { vmStore(rcx, int.class) },\n+            { vmStore(r8, int.class) },\n+            { vmStore(r9, int.class) },\n+            { vmStore(rax, long.class) },\n@@ -234,2 +242,2 @@\n-        assertEquals(callingSequence.methodType(), mt.appendParameterTypes(long.class));\n-        assertEquals(callingSequence.functionDesc(), fd.appendArgumentLayouts(C_LONG));\n+        assertEquals(callingSequence.methodType(), mt.appendParameterTypes(long.class).insertParameterTypes(0, Addressable.class));\n+        assertEquals(callingSequence.functionDesc(), fd.appendArgumentLayouts(C_LONG).insertArgumentLayouts(0, ADDRESS));\n@@ -238,9 +246,10 @@\n-                { vmStore(xmm0, double.class) },\n-                { vmStore(xmm1, double.class) },\n-                { vmStore(xmm2, double.class) },\n-                { vmStore(xmm3, double.class) },\n-                { vmStore(xmm4, double.class) },\n-                { vmStore(xmm5, double.class) },\n-                { vmStore(xmm6, double.class) },\n-                { vmStore(xmm7, double.class) },\n-                { vmStore(rax, long.class) },\n+            { unboxAddress(Addressable.class), vmStore(r10, long.class) },\n+            { vmStore(xmm0, double.class) },\n+            { vmStore(xmm1, double.class) },\n+            { vmStore(xmm2, double.class) },\n+            { vmStore(xmm3, double.class) },\n+            { vmStore(xmm4, double.class) },\n+            { vmStore(xmm5, double.class) },\n+            { vmStore(xmm6, double.class) },\n+            { vmStore(xmm7, double.class) },\n+            { vmStore(rax, long.class) },\n@@ -268,2 +277,2 @@\n-        assertEquals(callingSequence.methodType(), mt.appendParameterTypes(long.class));\n-        assertEquals(callingSequence.functionDesc(), fd.appendArgumentLayouts(C_LONG));\n+        assertEquals(callingSequence.methodType(), mt.appendParameterTypes(long.class).insertParameterTypes(0, Addressable.class));\n+        assertEquals(callingSequence.functionDesc(), fd.appendArgumentLayouts(C_LONG).insertArgumentLayouts(0, ADDRESS));\n@@ -272,19 +281,20 @@\n-                { vmStore(rdi, long.class) },\n-                { vmStore(rsi, long.class) },\n-                { vmStore(rdx, long.class) },\n-                { vmStore(rcx, long.class) },\n-                { vmStore(r8, long.class) },\n-                { vmStore(r9, long.class) },\n-                { vmStore(stackStorage(0), long.class) },\n-                { vmStore(stackStorage(1), long.class) },\n-                { vmStore(xmm0, float.class) },\n-                { vmStore(xmm1, float.class) },\n-                { vmStore(xmm2, float.class) },\n-                { vmStore(xmm3, float.class) },\n-                { vmStore(xmm4, float.class) },\n-                { vmStore(xmm5, float.class) },\n-                { vmStore(xmm6, float.class) },\n-                { vmStore(xmm7, float.class) },\n-                { vmStore(stackStorage(2), float.class) },\n-                { vmStore(stackStorage(3), float.class) },\n-                { vmStore(rax, long.class) },\n+            { unboxAddress(Addressable.class), vmStore(r10, long.class) },\n+            { vmStore(rdi, long.class) },\n+            { vmStore(rsi, long.class) },\n+            { vmStore(rdx, long.class) },\n+            { vmStore(rcx, long.class) },\n+            { vmStore(r8, long.class) },\n+            { vmStore(r9, long.class) },\n+            { vmStore(stackStorage(0), long.class) },\n+            { vmStore(stackStorage(1), long.class) },\n+            { vmStore(xmm0, float.class) },\n+            { vmStore(xmm1, float.class) },\n+            { vmStore(xmm2, float.class) },\n+            { vmStore(xmm3, float.class) },\n+            { vmStore(xmm4, float.class) },\n+            { vmStore(xmm5, float.class) },\n+            { vmStore(xmm6, float.class) },\n+            { vmStore(xmm7, float.class) },\n+            { vmStore(stackStorage(2), float.class) },\n+            { vmStore(stackStorage(3), float.class) },\n+            { vmStore(rax, long.class) },\n@@ -324,2 +334,2 @@\n-        assertEquals(callingSequence.methodType(), mt.appendParameterTypes(long.class));\n-        assertEquals(callingSequence.functionDesc(), fd.appendArgumentLayouts(C_LONG));\n+        assertEquals(callingSequence.methodType(), mt.appendParameterTypes(long.class).insertParameterTypes(0, Addressable.class));\n+        assertEquals(callingSequence.functionDesc(), fd.appendArgumentLayouts(C_LONG).insertArgumentLayouts(0, ADDRESS));\n@@ -328,15 +338,16 @@\n-                { vmStore(rdi, int.class) },\n-                { vmStore(rsi, int.class) },\n-                {\n-                        dup(),\n-                        bufferLoad(0, long.class), vmStore(rdx, long.class),\n-                        bufferLoad(8, double.class), vmStore(xmm0, double.class)\n-                },\n-                { vmStore(rcx, int.class) },\n-                { vmStore(r8, int.class) },\n-                { vmStore(xmm1, double.class) },\n-                { vmStore(xmm2, double.class) },\n-                { vmStore(r9, int.class) },\n-                { vmStore(stackStorage(0), int.class) },\n-                { vmStore(stackStorage(1), int.class) },\n-                { vmStore(rax, long.class) },\n+            { unboxAddress(Addressable.class), vmStore(r10, long.class) },\n+            { vmStore(rdi, int.class) },\n+            { vmStore(rsi, int.class) },\n+            {\n+                dup(),\n+                bufferLoad(0, long.class), vmStore(rdx, long.class),\n+                bufferLoad(8, double.class), vmStore(xmm0, double.class)\n+            },\n+            { vmStore(rcx, int.class) },\n+            { vmStore(r8, int.class) },\n+            { vmStore(xmm1, double.class) },\n+            { vmStore(xmm2, double.class) },\n+            { vmStore(r9, int.class) },\n+            { vmStore(stackStorage(0), int.class) },\n+            { vmStore(stackStorage(1), int.class) },\n+            { vmStore(rax, long.class) },\n@@ -366,2 +377,2 @@\n-        assertEquals(callingSequence.methodType(), mt.appendParameterTypes(long.class));\n-        assertEquals(callingSequence.functionDesc(), fd.appendArgumentLayouts(C_LONG));\n+        assertEquals(callingSequence.methodType(), mt.appendParameterTypes(long.class).insertParameterTypes(0, Addressable.class));\n+        assertEquals(callingSequence.functionDesc(), fd.appendArgumentLayouts(C_LONG).insertArgumentLayouts(0, ADDRESS));\n@@ -370,2 +381,3 @@\n-                { unboxAddress(), vmStore(rdi, long.class) },\n-                { vmStore(rax, long.class) },\n+            { unboxAddress(Addressable.class), vmStore(r10, long.class) },\n+            { unboxAddress(), vmStore(rdi, long.class) },\n+            { vmStore(rax, long.class) },\n@@ -387,2 +399,2 @@\n-        assertEquals(callingSequence.methodType(), mt.appendParameterTypes(long.class));\n-        assertEquals(callingSequence.functionDesc(), fd.appendArgumentLayouts(C_LONG));\n+        assertEquals(callingSequence.methodType(), mt.appendParameterTypes(long.class).insertParameterTypes(0, Addressable.class));\n+        assertEquals(callingSequence.functionDesc(), fd.appendArgumentLayouts(C_LONG).insertArgumentLayouts(0, ADDRESS));\n@@ -391,2 +403,3 @@\n-                expectedBindings,\n-                { vmStore(rax, long.class) },\n+            { unboxAddress(Addressable.class), vmStore(r10, long.class) },\n+            expectedBindings,\n+            { vmStore(rax, long.class) },\n@@ -404,2 +417,2 @@\n-                { MemoryLayout.structLayout(C_LONG), new Binding[]{\n-                        bufferLoad(0, long.class), vmStore(rdi, long.class)\n+            { MemoryLayout.structLayout(C_LONG), new Binding[]{\n+                    bufferLoad(0, long.class), vmStore(rdi, long.class)\n@@ -407,5 +420,5 @@\n-                },\n-                { MemoryLayout.structLayout(C_LONG, C_LONG), new Binding[]{\n-                        dup(),\n-                        bufferLoad(0, long.class), vmStore(rdi, long.class),\n-                        bufferLoad(8, long.class), vmStore(rsi, long.class)\n+            },\n+            { MemoryLayout.structLayout(C_LONG, C_LONG), new Binding[]{\n+                    dup(),\n+                    bufferLoad(0, long.class), vmStore(rdi, long.class),\n+                    bufferLoad(8, long.class), vmStore(rsi, long.class)\n@@ -413,7 +426,7 @@\n-                },\n-                { MemoryLayout.structLayout(C_LONG, C_LONG, C_LONG), new Binding[]{\n-                        dup(),\n-                        bufferLoad(0, long.class), vmStore(stackStorage(0), long.class),\n-                        dup(),\n-                        bufferLoad(8, long.class), vmStore(stackStorage(1), long.class),\n-                        bufferLoad(16, long.class), vmStore(stackStorage(2), long.class)\n+            },\n+            { MemoryLayout.structLayout(C_LONG, C_LONG, C_LONG), new Binding[]{\n+                    dup(),\n+                    bufferLoad(0, long.class), vmStore(stackStorage(0), long.class),\n+                    dup(),\n+                    bufferLoad(8, long.class), vmStore(stackStorage(1), long.class),\n+                    bufferLoad(16, long.class), vmStore(stackStorage(2), long.class)\n@@ -421,9 +434,9 @@\n-                },\n-                { MemoryLayout.structLayout(C_LONG, C_LONG, C_LONG, C_LONG), new Binding[]{\n-                        dup(),\n-                        bufferLoad(0, long.class), vmStore(stackStorage(0), long.class),\n-                        dup(),\n-                        bufferLoad(8, long.class), vmStore(stackStorage(1), long.class),\n-                        dup(),\n-                        bufferLoad(16, long.class), vmStore(stackStorage(2), long.class),\n-                        bufferLoad(24, long.class), vmStore(stackStorage(3), long.class)\n+            },\n+            { MemoryLayout.structLayout(C_LONG, C_LONG, C_LONG, C_LONG), new Binding[]{\n+                    dup(),\n+                    bufferLoad(0, long.class), vmStore(stackStorage(0), long.class),\n+                    dup(),\n+                    bufferLoad(8, long.class), vmStore(stackStorage(1), long.class),\n+                    dup(),\n+                    bufferLoad(16, long.class), vmStore(stackStorage(2), long.class),\n+                    bufferLoad(24, long.class), vmStore(stackStorage(3), long.class)\n@@ -431,1 +444,1 @@\n-                },\n+            },\n@@ -445,2 +458,2 @@\n-        assertEquals(callingSequence.methodType(), mt.appendParameterTypes(long.class));\n-        assertEquals(callingSequence.functionDesc(), fd.appendArgumentLayouts(C_LONG));\n+        assertEquals(callingSequence.methodType(), mt.appendParameterTypes(long.class).insertParameterTypes(0, MemorySegment.class, Addressable.class));\n+        assertEquals(callingSequence.functionDesc(), fd.appendArgumentLayouts(C_LONG).insertArgumentLayouts(0, ADDRESS, ADDRESS));\n@@ -449,1 +462,3 @@\n-                { vmStore(rax, long.class) }\n+            { unboxAddress(MemorySegment.class), vmStore(r11, long.class) },\n+            { unboxAddress(Addressable.class), vmStore(r10, long.class) },\n+            { vmStore(rax, long.class) }\n@@ -453,7 +468,7 @@\n-                allocate(struct),\n-                dup(),\n-                vmLoad(rax, long.class),\n-                bufferStore(0, long.class),\n-                dup(),\n-                vmLoad(rdx, long.class),\n-                bufferStore(8, long.class)\n+            allocate(struct),\n+            dup(),\n+            vmLoad(rax, long.class),\n+            bufferStore(0, long.class),\n+            dup(),\n+            vmLoad(rdx, long.class),\n+            bufferStore(8, long.class)\n@@ -475,2 +490,2 @@\n-        assertEquals(callingSequence.methodType(), MethodType.methodType(void.class, MemoryAddress.class, long.class));\n-        assertEquals(callingSequence.functionDesc(), FunctionDescriptor.ofVoid(C_POINTER, C_LONG));\n+        assertEquals(callingSequence.methodType(), MethodType.methodType(void.class, Addressable.class, MemoryAddress.class, long.class));\n+        assertEquals(callingSequence.functionDesc(), FunctionDescriptor.ofVoid(ADDRESS, C_POINTER, C_LONG));\n@@ -479,2 +494,3 @@\n-                { unboxAddress(), vmStore(rdi, long.class) },\n-                { vmStore(rax, long.class) }\n+            { unboxAddress(Addressable.class), vmStore(r10, long.class) },\n+            { unboxAddress(), vmStore(rdi, long.class) },\n+            { vmStore(rax, long.class) }\n@@ -502,1 +518,1 @@\n-                { allocate(struct), dup(), vmLoad(xmm0, float.class), bufferStore(0, float.class) },\n+            { allocate(struct), dup(), vmLoad(xmm0, float.class), bufferStore(0, float.class) },\n@@ -506,1 +522,1 @@\n-                bufferLoad(0, float.class), vmStore(xmm0, float.class)\n+            bufferLoad(0, float.class), vmStore(xmm0, float.class)\n","filename":"test\/jdk\/java\/foreign\/callarranger\/TestSysVCallArranger.java","additions":145,"deletions":129,"binary":false,"changes":274,"status":"modified"},{"patch":"@@ -40,0 +40,1 @@\n+import java.lang.foreign.Addressable;\n@@ -47,0 +48,1 @@\n+import static java.lang.foreign.ValueLayout.ADDRESS;\n@@ -63,2 +65,2 @@\n-        assertEquals(callingSequence.methodType(), mt);\n-        assertEquals(callingSequence.functionDesc(), fd);\n+        assertEquals(callingSequence.methodType(), mt.insertParameterTypes(0, Addressable.class));\n+        assertEquals(callingSequence.functionDesc(), fd.insertArgumentLayouts(0, ADDRESS));\n@@ -66,1 +68,3 @@\n-        checkArgumentBindings(callingSequence, new Binding[][]{});\n+        checkArgumentBindings(callingSequence, new Binding[][]{\n+            { unboxAddress(Addressable.class), vmStore(r10, long.class) }\n+        });\n@@ -78,2 +82,2 @@\n-        assertEquals(callingSequence.methodType(), mt);\n-        assertEquals(callingSequence.functionDesc(), fd);\n+        assertEquals(callingSequence.methodType(), mt.insertParameterTypes(0, Addressable.class));\n+        assertEquals(callingSequence.functionDesc(), fd.insertArgumentLayouts(0, ADDRESS));\n@@ -82,4 +86,5 @@\n-                { vmStore(rcx, int.class) },\n-                { vmStore(rdx, int.class) },\n-                { vmStore(r8, int.class) },\n-                { vmStore(r9, int.class) }\n+            { unboxAddress(Addressable.class), vmStore(r10, long.class) },\n+            { vmStore(rcx, int.class) },\n+            { vmStore(rdx, int.class) },\n+            { vmStore(r8, int.class) },\n+            { vmStore(r9, int.class) }\n@@ -99,2 +104,2 @@\n-        assertEquals(callingSequence.methodType(), mt);\n-        assertEquals(callingSequence.functionDesc(), fd);\n+        assertEquals(callingSequence.methodType(), mt.insertParameterTypes(0, Addressable.class));\n+        assertEquals(callingSequence.functionDesc(), fd.insertArgumentLayouts(0, ADDRESS));\n@@ -103,4 +108,5 @@\n-                { vmStore(xmm0, double.class) },\n-                { vmStore(xmm1, double.class) },\n-                { vmStore(xmm2, double.class) },\n-                { vmStore(xmm3, double.class) }\n+            { unboxAddress(Addressable.class), vmStore(r10, long.class) },\n+            { vmStore(xmm0, double.class) },\n+            { vmStore(xmm1, double.class) },\n+            { vmStore(xmm2, double.class) },\n+            { vmStore(xmm3, double.class) }\n@@ -122,2 +128,2 @@\n-        assertEquals(callingSequence.methodType(), mt);\n-        assertEquals(callingSequence.functionDesc(), fd);\n+        assertEquals(callingSequence.methodType(), mt.insertParameterTypes(0, Addressable.class));\n+        assertEquals(callingSequence.functionDesc(), fd.insertArgumentLayouts(0, ADDRESS));\n@@ -126,8 +132,9 @@\n-                { vmStore(rcx, long.class) },\n-                { vmStore(rdx, long.class) },\n-                { vmStore(xmm2, float.class) },\n-                { vmStore(xmm3, float.class) },\n-                { vmStore(stackStorage(0), long.class) },\n-                { vmStore(stackStorage(1), long.class) },\n-                { vmStore(stackStorage(2), float.class) },\n-                { vmStore(stackStorage(3), float.class) }\n+            { unboxAddress(Addressable.class), vmStore(r10, long.class) },\n+            { vmStore(rcx, long.class) },\n+            { vmStore(rdx, long.class) },\n+            { vmStore(xmm2, float.class) },\n+            { vmStore(xmm3, float.class) },\n+            { vmStore(stackStorage(0), long.class) },\n+            { vmStore(stackStorage(1), long.class) },\n+            { vmStore(stackStorage(2), float.class) },\n+            { vmStore(stackStorage(3), float.class) }\n@@ -152,2 +159,2 @@\n-        assertEquals(callingSequence.methodType(), mt);\n-        assertEquals(callingSequence.functionDesc(), fd);\n+        assertEquals(callingSequence.methodType(), mt.insertParameterTypes(0, Addressable.class));\n+        assertEquals(callingSequence.functionDesc(), fd.insertArgumentLayouts(0, ADDRESS));\n@@ -156,15 +163,16 @@\n-                { vmStore(rcx, int.class) },\n-                { vmStore(rdx, int.class) },\n-                {\n-                        copy(structLayout),\n-                        unboxAddress(MemorySegment.class),\n-                        vmStore(r8, long.class)\n-                },\n-                { vmStore(r9, int.class) },\n-                { vmStore(stackStorage(0), int.class) },\n-                { vmStore(stackStorage(1), double.class) },\n-                { vmStore(stackStorage(2), double.class) },\n-                { vmStore(stackStorage(3), double.class) },\n-                { vmStore(stackStorage(4), int.class) },\n-                { vmStore(stackStorage(5), int.class) },\n-                { vmStore(stackStorage(6), int.class) }\n+            { unboxAddress(Addressable.class), vmStore(r10, long.class) },\n+            { vmStore(rcx, int.class) },\n+            { vmStore(rdx, int.class) },\n+            {\n+                copy(structLayout),\n+                unboxAddress(MemorySegment.class),\n+                vmStore(r8, long.class)\n+            },\n+            { vmStore(r9, int.class) },\n+            { vmStore(stackStorage(0), int.class) },\n+            { vmStore(stackStorage(1), double.class) },\n+            { vmStore(stackStorage(2), double.class) },\n+            { vmStore(stackStorage(3), double.class) },\n+            { vmStore(stackStorage(4), int.class) },\n+            { vmStore(stackStorage(5), int.class) },\n+            { vmStore(stackStorage(6), int.class) }\n@@ -182,0 +190,2 @@\n+        FunctionDescriptor fdExpected = FunctionDescriptor.ofVoid(\n+                ADDRESS, C_INT, C_DOUBLE, C_INT, C_DOUBLE, C_DOUBLE);\n@@ -186,2 +196,2 @@\n-        assertEquals(callingSequence.methodType(), mt);\n-        assertEquals(callingSequence.functionDesc(), FunctionDescriptor.ofVoid(C_INT, C_DOUBLE, C_INT, C_DOUBLE, C_DOUBLE));\n+        assertEquals(callingSequence.methodType(), mt.insertParameterTypes(0, Addressable.class));\n+        assertEquals(callingSequence.functionDesc(), fdExpected);\n@@ -190,5 +200,6 @@\n-                { vmStore(rcx, int.class) },\n-                { vmStore(xmm1, double.class) },\n-                { vmStore(r8, int.class) },\n-                { dup(), vmStore(r9, double.class), vmStore(xmm3, double.class) },\n-                { vmStore(stackStorage(0), double.class) },\n+            { unboxAddress(Addressable.class), vmStore(r10, long.class) },\n+            { vmStore(rcx, int.class) },\n+            { vmStore(xmm1, double.class) },\n+            { vmStore(r8, int.class) },\n+            { dup(), vmStore(r9, double.class), vmStore(xmm3, double.class) },\n+            { vmStore(stackStorage(0), double.class) },\n@@ -219,2 +230,2 @@\n-        assertEquals(callingSequence.methodType(), mt);\n-        assertEquals(callingSequence.functionDesc(), fd);\n+        assertEquals(callingSequence.methodType(), mt.insertParameterTypes(0, Addressable.class));\n+        assertEquals(callingSequence.functionDesc(), fd.insertArgumentLayouts(0, ADDRESS));\n@@ -223,1 +234,2 @@\n-                { bufferLoad(0, long.class), vmStore(rcx, long.class) }\n+            { unboxAddress(Addressable.class), vmStore(r10, long.class) },\n+            { bufferLoad(0, long.class), vmStore(rcx, long.class) }\n@@ -248,2 +260,2 @@\n-        assertEquals(callingSequence.methodType(), mt);\n-        assertEquals(callingSequence.functionDesc(), fd);\n+        assertEquals(callingSequence.methodType(), mt.insertParameterTypes(0, Addressable.class));\n+        assertEquals(callingSequence.functionDesc(), fd.insertArgumentLayouts(0, ADDRESS));\n@@ -252,5 +264,6 @@\n-                {\n-                        copy(struct),\n-                        unboxAddress(MemorySegment.class),\n-                        vmStore(rcx, long.class)\n-                }\n+            { unboxAddress(Addressable.class), vmStore(r10, long.class) },\n+            {\n+                copy(struct),\n+                unboxAddress(MemorySegment.class),\n+                vmStore(rcx, long.class)\n+            }\n@@ -278,2 +291,2 @@\n-        assertEquals(callingSequence.methodType(), mt);\n-        assertEquals(callingSequence.functionDesc(), fd);\n+        assertEquals(callingSequence.methodType(), mt.insertParameterTypes(0, Addressable.class));\n+        assertEquals(callingSequence.functionDesc(), fd.insertArgumentLayouts(0, ADDRESS));\n@@ -282,1 +295,2 @@\n-                { unboxAddress(), vmStore(rcx, long.class) }\n+            { unboxAddress(Addressable.class), vmStore(r10, long.class) },\n+            { unboxAddress(), vmStore(rcx, long.class) }\n@@ -298,2 +312,2 @@\n-        assertEquals(callingSequence.methodType(), mt);\n-        assertEquals(callingSequence.functionDesc(), fd);\n+        assertEquals(callingSequence.methodType(), mt.insertParameterTypes(0, Addressable.class));\n+        assertEquals(callingSequence.functionDesc(), fd.insertArgumentLayouts(0, ADDRESS));\n@@ -301,1 +315,3 @@\n-        checkArgumentBindings(callingSequence, new Binding[][]{});\n+        checkArgumentBindings(callingSequence, new Binding[][]{\n+            { unboxAddress(Addressable.class), vmStore(r10, long.class) },\n+        });\n@@ -304,4 +320,4 @@\n-                new Binding[]{ allocate(struct),\n-                        dup(),\n-                        vmLoad(rax, long.class),\n-                        bufferStore(0, long.class) });\n+            new Binding[]{ allocate(struct),\n+                dup(),\n+                vmLoad(rax, long.class),\n+                bufferStore(0, long.class) });\n@@ -320,2 +336,2 @@\n-        assertEquals(callingSequence.methodType(), MethodType.methodType(void.class, MemoryAddress.class));\n-        assertEquals(callingSequence.functionDesc(), FunctionDescriptor.ofVoid(C_POINTER));\n+        assertEquals(callingSequence.methodType(), MethodType.methodType(void.class, Addressable.class, MemoryAddress.class));\n+        assertEquals(callingSequence.functionDesc(), FunctionDescriptor.ofVoid(ADDRESS, C_POINTER));\n@@ -324,1 +340,2 @@\n-                { unboxAddress(), vmStore(rcx, long.class) }\n+            { unboxAddress(Addressable.class), vmStore(r10, long.class) },\n+            { unboxAddress(), vmStore(rcx, long.class) }\n@@ -335,4 +352,4 @@\n-                MemorySegment.class, int.class, double.class, MemoryAddress.class,\n-                MemorySegment.class, int.class, double.class, MemoryAddress.class,\n-                MemorySegment.class, int.class, double.class, MemoryAddress.class,\n-                MemorySegment.class, int.class, double.class, MemoryAddress.class);\n+            MemorySegment.class, int.class, double.class, MemoryAddress.class,\n+            MemorySegment.class, int.class, double.class, MemoryAddress.class,\n+            MemorySegment.class, int.class, double.class, MemoryAddress.class,\n+            MemorySegment.class, int.class, double.class, MemoryAddress.class);\n@@ -340,4 +357,4 @@\n-                struct, C_INT, C_DOUBLE, C_POINTER,\n-                struct, C_INT, C_DOUBLE, C_POINTER,\n-                struct, C_INT, C_DOUBLE, C_POINTER,\n-                struct, C_INT, C_DOUBLE, C_POINTER);\n+            struct, C_INT, C_DOUBLE, C_POINTER,\n+            struct, C_INT, C_DOUBLE, C_POINTER,\n+            struct, C_INT, C_DOUBLE, C_POINTER,\n+            struct, C_INT, C_DOUBLE, C_POINTER);\n@@ -348,2 +365,2 @@\n-        assertEquals(callingSequence.methodType(), mt);\n-        assertEquals(callingSequence.functionDesc(), fd);\n+        assertEquals(callingSequence.methodType(), mt.insertParameterTypes(0, Addressable.class));\n+        assertEquals(callingSequence.functionDesc(), fd.insertArgumentLayouts(0, ADDRESS));\n@@ -352,16 +369,17 @@\n-                { copy(struct), unboxAddress(MemorySegment.class), vmStore(rcx, long.class) },\n-                { vmStore(rdx, int.class) },\n-                { vmStore(xmm2, double.class) },\n-                { unboxAddress(), vmStore(r9, long.class) },\n-                { copy(struct), unboxAddress(MemorySegment.class), vmStore(stackStorage(0), long.class) },\n-                { vmStore(stackStorage(1), int.class) },\n-                { vmStore(stackStorage(2), double.class) },\n-                { unboxAddress(), vmStore(stackStorage(3), long.class) },\n-                { copy(struct), unboxAddress(MemorySegment.class), vmStore(stackStorage(4), long.class) },\n-                { vmStore(stackStorage(5), int.class) },\n-                { vmStore(stackStorage(6), double.class) },\n-                { unboxAddress(), vmStore(stackStorage(7), long.class) },\n-                { copy(struct), unboxAddress(MemorySegment.class), vmStore(stackStorage(8), long.class) },\n-                { vmStore(stackStorage(9), int.class) },\n-                { vmStore(stackStorage(10), double.class) },\n-                { unboxAddress(), vmStore(stackStorage(11), long.class) },\n+            { unboxAddress(Addressable.class), vmStore(r10, long.class) },\n+            { copy(struct), unboxAddress(MemorySegment.class), vmStore(rcx, long.class) },\n+            { vmStore(rdx, int.class) },\n+            { vmStore(xmm2, double.class) },\n+            { unboxAddress(), vmStore(r9, long.class) },\n+            { copy(struct), unboxAddress(MemorySegment.class), vmStore(stackStorage(0), long.class) },\n+            { vmStore(stackStorage(1), int.class) },\n+            { vmStore(stackStorage(2), double.class) },\n+            { unboxAddress(), vmStore(stackStorage(3), long.class) },\n+            { copy(struct), unboxAddress(MemorySegment.class), vmStore(stackStorage(4), long.class) },\n+            { vmStore(stackStorage(5), int.class) },\n+            { vmStore(stackStorage(6), double.class) },\n+            { unboxAddress(), vmStore(stackStorage(7), long.class) },\n+            { copy(struct), unboxAddress(MemorySegment.class), vmStore(stackStorage(8), long.class) },\n+            { vmStore(stackStorage(9), int.class) },\n+            { vmStore(stackStorage(10), double.class) },\n+            { unboxAddress(), vmStore(stackStorage(11), long.class) },\n","filename":"test\/jdk\/java\/foreign\/callarranger\/TestWindowsCallArranger.java","additions":116,"deletions":98,"binary":false,"changes":214,"status":"modified"}]}