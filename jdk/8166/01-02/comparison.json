{"files":[{"patch":"@@ -0,0 +1,49 @@\n+\/*\n+ * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef CPU_AARCH64_CONTINUATIONENTRY_AARCH64_INLINE_HPP\n+#define CPU_AARCH64_CONTINUATIONENTRY_AARCH64_INLINE_HPP\n+\n+#include \"runtime\/continuationEntry.hpp\"\n+\n+#include \"code\/codeCache.hpp\"\n+#include \"runtime\/frame.inline.hpp\"\n+#include \"runtime\/registerMap.hpp\"\n+\n+inline frame ContinuationEntry::to_frame() const {\n+  static CodeBlob* cb = CodeCache::find_blob(entry_pc());\n+  return frame(entry_sp(), entry_sp(), entry_fp(), entry_pc(), cb);\n+}\n+\n+inline intptr_t* ContinuationEntry::entry_fp() const {\n+  return (intptr_t*)((address)this + size());\n+}\n+\n+inline void ContinuationEntry::update_register_map(RegisterMap* map) const {\n+  intptr_t** fp = (intptr_t**)(bottom_sender_sp() - frame::sender_sp_offset);\n+  frame::update_map_with_saved_link(map, fp);\n+}\n+\n+\n+#endif \/\/  CPU_AARCH64_CONTINUATIONENTRY_AARCH64_INLINE_HPP\n","filename":"src\/hotspot\/cpu\/aarch64\/continuationEntry_aarch64.inline.hpp","additions":49,"deletions":0,"binary":false,"changes":49,"status":"added"},{"patch":"@@ -25,2 +25,2 @@\n-#ifndef CPU_AARCH64_CONTINUATION_AARCH64_INLINE_HPP\n-#define CPU_AARCH64_CONTINUATION_AARCH64_INLINE_HPP\n+#ifndef CPU_AARCH64_CONTINUATIONFREEZETHAW_AARCH64_INLINE_HPP\n+#define CPU_AARCH64_CONTINUATIONFREEZETHAW_AARCH64_INLINE_HPP\n@@ -33,0 +33,12 @@\n+\n+inline void patch_callee_link(const frame& f, intptr_t* fp) {\n+  DEBUG_ONLY(intptr_t* orig = *ContinuationHelper::Frame::callee_link_address(f));\n+  *ContinuationHelper::Frame::callee_link_address(f) = fp;\n+}\n+\n+inline void patch_callee_link_relative(const frame& f, intptr_t* fp) {\n+  intptr_t* la = (intptr_t*)ContinuationHelper::Frame::callee_link_address(f);\n+  intptr_t new_value = fp - la;\n+  *la = new_value;\n+}\n+\n@@ -278,1 +290,1 @@\n-#endif \/\/ CPU_AARCH64_CONTINUATION_AARCH64_INLINE_HPP\n+#endif \/\/ CPU_AARCH64_CONTINUATIONFREEZETHAW_AARCH64_INLINE_HPP\n","filename":"src\/hotspot\/cpu\/aarch64\/continuationFreezeThaw_aarch64.inline.hpp","additions":15,"deletions":3,"binary":false,"changes":18,"previous_filename":"src\/hotspot\/cpu\/aarch64\/continuation_aarch64.inline.hpp","status":"renamed"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"runtime\/continuationEntry.inline.hpp\"\n@@ -42,11 +43,0 @@\n-static void patch_callee_link(const frame& f, intptr_t* fp) {\n-  DEBUG_ONLY(intptr_t* orig = *ContinuationHelper::Frame::callee_link_address(f));\n-  *ContinuationHelper::Frame::callee_link_address(f) = fp;\n-}\n-\n-static void patch_callee_link_relative(const frame& f, intptr_t* fp) {\n-  intptr_t* la = (intptr_t*)ContinuationHelper::Frame::callee_link_address(f);\n-  intptr_t new_value = fp - la;\n-  *la = new_value;\n-}\n-\n@@ -73,9 +63,0 @@\n-intptr_t* ContinuationEntry::entry_fp() const {\n-  return (intptr_t*)((address)this + size());\n-}\n-\n-void ContinuationEntry::update_register_map(RegisterMap* map) const {\n-  intptr_t** fp = (intptr_t**)(bottom_sender_sp() - frame::sender_sp_offset);\n-  frame::update_map_with_saved_link(map, fp);\n-}\n-\n@@ -90,5 +71,0 @@\n-frame ContinuationEntry::to_frame() const {\n-  static CodeBlob* cb = CodeCache::find_blob(entry_pc());\n-  return frame(entry_sp(), entry_sp(), entry_fp(), entry_pc(), cb);\n-}\n-\n@@ -96,1 +72,1 @@\n-void ContinuationHelper::set_anchor_to_entry_pd(JavaFrameAnchor* anchor, ContinuationEntry* entry) {\n+inline void ContinuationHelper::set_anchor_to_entry_pd(JavaFrameAnchor* anchor, ContinuationEntry* entry) {\n@@ -101,1 +77,1 @@\n-void ContinuationHelper::set_anchor_pd(JavaFrameAnchor* anchor, intptr_t* sp) {\n+inline void ContinuationHelper::set_anchor_pd(JavaFrameAnchor* anchor, intptr_t* sp) {\n","filename":"src\/hotspot\/cpu\/aarch64\/continuationHelper_aarch64.inline.hpp","additions":3,"deletions":27,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -46,0 +46,1 @@\n+#include \"runtime\/continuationEntry.inline.hpp\"\n","filename":"src\/hotspot\/cpu\/aarch64\/sharedRuntime_aarch64.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -45,0 +45,1 @@\n+#include \"runtime\/continuationEntry.inline.hpp\"\n","filename":"src\/hotspot\/cpu\/aarch64\/stubGenerator_aarch64.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -0,0 +1,47 @@\n+\/*\n+ * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef CPU_ARM_CONTINUATIONENTRY_ARM_INLINE_HPP\n+#define CPU_ARM_CONTINUATIONENTRY_ARM_INLINE_HPP\n+\n+\n+#include \"runtime\/continuationEntry.hpp\"\n+\n+\/\/ TODO: Implement\n+\n+inline frame ContinuationEntry::to_frame() const {\n+  Unimplemented();\n+  return frame();\n+}\n+\n+inline intptr_t* ContinuationEntry::entry_fp() const {\n+  Unimplemented();\n+  return nullptr;\n+}\n+\n+inline void ContinuationEntry::update_register_map(RegisterMap* map) const {\n+  Unimplemented();\n+}\n+\n+#endif \/\/ CPU_ARM_CONTINUATIONENTRY_ARM_INLINE_HPP\n","filename":"src\/hotspot\/cpu\/arm\/continuationEntry_arm.inline.hpp","additions":47,"deletions":0,"binary":false,"changes":47,"status":"added"},{"patch":"","filename":"src\/hotspot\/cpu\/arm\/continuationFreezeThaw_arm.inline.hpp","additions":0,"deletions":0,"binary":false,"changes":0,"previous_filename":"src\/hotspot\/cpu\/arm\/continuation_arm.inline.hpp","status":"renamed"},{"patch":"@@ -30,4 +30,0 @@\n-#include \"runtime\/frame.inline.hpp\"\n-#include \"runtime\/registerMap.hpp\"\n-#include \"utilities\/macros.hpp\"\n-\n@@ -36,14 +32,0 @@\n-frame ContinuationEntry::to_frame() const {\n-  Unimplemented();\n-  return frame();\n-}\n-\n-intptr_t* ContinuationEntry::entry_fp() const {\n-  Unimplemented();\n-  return nullptr;\n-}\n-\n-void ContinuationEntry::update_register_map(RegisterMap* map) const {\n-  Unimplemented();\n-}\n-\n@@ -79,1 +61,1 @@\n-void ContinuationHelper::set_anchor_to_entry_pd(JavaFrameAnchor* anchor, ContinuationEntry* cont) {\n+inline void ContinuationHelper::set_anchor_to_entry_pd(JavaFrameAnchor* anchor, ContinuationEntry* cont) {\n@@ -84,1 +66,1 @@\n-void ContinuationHelper::set_anchor_pd(JavaFrameAnchor* anchor, intptr_t* sp) {\n+inline void ContinuationHelper::set_anchor_pd(JavaFrameAnchor* anchor, intptr_t* sp) {\n","filename":"src\/hotspot\/cpu\/arm\/continuationHelper_arm.inline.hpp","additions":2,"deletions":20,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -0,0 +1,46 @@\n+\/*\n+ * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef CPU_PPC_CONTINUATIONENTRY_PPC_INLINE_HPP\n+#define CPU_PPC_CONTINUATIONENTRY_PPC_INLINE_HPP\n+\n+#include \"runtime\/continuationEntry.hpp\"\n+\n+\/\/ TODO: Implement\n+\n+inline frame ContinuationEntry::to_frame() const {\n+  Unimplemented();\n+  return frame();\n+}\n+\n+inline intptr_t* ContinuationEntry::entry_fp() const {\n+  Unimplemented();\n+  return nullptr;\n+}\n+\n+inline void ContinuationEntry::update_register_map(RegisterMap* map) const {\n+  Unimplemented();\n+}\n+\n+#endif \/\/ CPU_PPC_CONTINUATIONENTRY_PPC_INLINE_HPP\n","filename":"src\/hotspot\/cpu\/ppc\/continuationEntry_ppc.inline.hpp","additions":46,"deletions":0,"binary":false,"changes":46,"status":"added"},{"patch":"","filename":"src\/hotspot\/cpu\/ppc\/continuationFreezeThaw_ppc.inline.hpp","additions":0,"deletions":0,"binary":false,"changes":0,"previous_filename":"src\/hotspot\/cpu\/ppc\/continuation_ppc.inline.hpp","status":"renamed"},{"patch":"@@ -30,20 +30,0 @@\n-#include \"runtime\/frame.inline.hpp\"\n-#include \"runtime\/registerMap.hpp\"\n-#include \"utilities\/macros.hpp\"\n-\n-\/\/ TODO: Implement\n-\n-frame ContinuationEntry::to_frame() const {\n-  Unimplemented();\n-  return frame();\n-}\n-\n-intptr_t* ContinuationEntry::entry_fp() const {\n-  Unimplemented();\n-  return nullptr;\n-}\n-\n-void ContinuationEntry::update_register_map(RegisterMap* map) const {\n-  Unimplemented();\n-}\n-\n@@ -80,1 +60,1 @@\n-void ContinuationHelper::set_anchor_to_entry_pd(JavaFrameAnchor* anchor, ContinuationEntry* cont) {\n+inline void ContinuationHelper::set_anchor_to_entry_pd(JavaFrameAnchor* anchor, ContinuationEntry* cont) {\n@@ -85,1 +65,1 @@\n-void ContinuationHelper::set_anchor_pd(JavaFrameAnchor* anchor, intptr_t* sp) {\n+inline void ContinuationHelper::set_anchor_pd(JavaFrameAnchor* anchor, intptr_t* sp) {\n","filename":"src\/hotspot\/cpu\/ppc\/continuationHelper_ppc.inline.hpp","additions":2,"deletions":22,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -0,0 +1,46 @@\n+\/*\n+ * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef CPU_S390_CONTINUATIONENTRY_S390_INLINE_HPP\n+#define CPU_S390_CONTINUATIONENTRY_S390_INLINE_HPP\n+\n+#include \"runtime\/continuationEntry.hpp\"\n+\n+\/\/ TODO: Implement\n+\n+inline frame ContinuationEntry::to_frame() const {\n+  Unimplemented();\n+  return frame();\n+}\n+\n+inline intptr_t* ContinuationEntry::entry_fp() const {\n+  Unimplemented();\n+  return nullptr;\n+}\n+\n+inline void ContinuationEntry::update_register_map(RegisterMap* map) const {\n+  Unimplemented();\n+}\n+\n+#endif \/\/ CPU_S390_CONTINUATIONENTRY_S390_INLINE_HPP\n","filename":"src\/hotspot\/cpu\/s390\/continuationEntry_s390.inline.hpp","additions":46,"deletions":0,"binary":false,"changes":46,"status":"added"},{"patch":"","filename":"src\/hotspot\/cpu\/s390\/continuationFreezeThaw_s390.inline.hpp","additions":0,"deletions":0,"binary":false,"changes":0,"previous_filename":"src\/hotspot\/cpu\/s390\/continuation_s390.inline.hpp","status":"renamed"},{"patch":"@@ -30,4 +30,0 @@\n-#include \"runtime\/frame.inline.hpp\"\n-#include \"runtime\/registerMap.hpp\"\n-#include \"utilities\/macros.hpp\"\n-\n@@ -36,14 +32,0 @@\n-frame ContinuationEntry::to_frame() const {\n-  Unimplemented();\n-  return frame();\n-}\n-\n-intptr_t* ContinuationEntry::entry_fp() const {\n-  Unimplemented();\n-  return nullptr;\n-}\n-\n-void ContinuationEntry::update_register_map(RegisterMap* map) const {\n-  Unimplemented();\n-}\n-\n@@ -79,1 +61,1 @@\n-void ContinuationHelper::set_anchor_to_entry_pd(JavaFrameAnchor* anchor, ContinuationEntry* cont) {\n+inline void ContinuationHelper::set_anchor_to_entry_pd(JavaFrameAnchor* anchor, ContinuationEntry* cont) {\n@@ -84,1 +66,1 @@\n-void ContinuationHelper::set_anchor_pd(JavaFrameAnchor* anchor, intptr_t* sp) {\n+inline void ContinuationHelper::set_anchor_pd(JavaFrameAnchor* anchor, intptr_t* sp) {\n","filename":"src\/hotspot\/cpu\/s390\/continuationHelper_s390.inline.hpp","additions":2,"deletions":20,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -0,0 +1,48 @@\n+\/*\n+ * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef CPU_X86_CONTINUATIONENTRY_X86_INLINE_HPP\n+#define CPU_X86_CONTINUATIONENTRY_X86_INLINE_HPP\n+\n+#include \"runtime\/continuationEntry.hpp\"\n+\n+#include \"runtime\/frame.inline.hpp\"\n+#include \"runtime\/registerMap.hpp\"\n+#include \"utilities\/macros.hpp\"\n+\n+inline frame ContinuationEntry::to_frame() const {\n+  static CodeBlob* cb = CodeCache::find_blob(entry_pc());\n+  return frame(entry_sp(), entry_sp(), entry_fp(), entry_pc(), cb);\n+}\n+\n+inline intptr_t* ContinuationEntry::entry_fp() const {\n+  return (intptr_t*)((address)this + size());\n+}\n+\n+inline void ContinuationEntry::update_register_map(RegisterMap* map) const {\n+  intptr_t** fp = (intptr_t**)(bottom_sender_sp() - frame::sender_sp_offset);\n+  frame::update_map_with_saved_link(map, fp);\n+}\n+\n+#endif \/\/ CPU_X86_CONTINUATIONENTRY_X86_INLINE_HPP\n","filename":"src\/hotspot\/cpu\/x86\/continuationEntry_x86.inline.hpp","additions":48,"deletions":0,"binary":false,"changes":48,"status":"added"},{"patch":"@@ -25,2 +25,2 @@\n-#ifndef CPU_X86_CONTINUATION_X86_INLINE_HPP\n-#define CPU_X86_CONTINUATION_X86_INLINE_HPP\n+#ifndef CPU_X86_CONTINUATIONFREEZETHAW_X86_INLINE_HPP\n+#define CPU_X86_CONTINUATIONFREEZETHAW_X86_INLINE_HPP\n@@ -33,1 +33,1 @@\n-static void patch_callee_link(const frame& f, intptr_t* fp) {\n+inline void patch_callee_link(const frame& f, intptr_t* fp) {\n@@ -37,1 +37,1 @@\n-static void patch_callee_link_relative(const frame& f, intptr_t* fp) {\n+inline void patch_callee_link_relative(const frame& f, intptr_t* fp) {\n@@ -43,5 +43,0 @@\n-frame ContinuationEntry::to_frame() const {\n-  static CodeBlob* cb = CodeCache::find_blob(entry_pc());\n-  return frame(entry_sp(), entry_sp(), entry_fp(), entry_pc(), cb);\n-}\n-\n@@ -285,1 +280,1 @@\n-#endif \/\/ CPU_X86_CONTINUATION_X86_INLINE_HPP\n+#endif \/\/ CPU_X86_CONTINUATIONFREEZE_THAW_X86_INLINE_HPP\n","filename":"src\/hotspot\/cpu\/x86\/continuationFreezeThaw_x86.inline.hpp","additions":5,"deletions":10,"binary":false,"changes":15,"previous_filename":"src\/hotspot\/cpu\/x86\/continuation_x86.inline.hpp","status":"renamed"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"runtime\/continuationEntry.inline.hpp\"\n@@ -62,9 +63,0 @@\n-intptr_t* ContinuationEntry::entry_fp() const {\n-  return (intptr_t*)((address)this + size());\n-}\n-\n-void ContinuationEntry::update_register_map(RegisterMap* map) const {\n-  intptr_t** fp = (intptr_t**)(bottom_sender_sp() - frame::sender_sp_offset);\n-  frame::update_map_with_saved_link(map, fp);\n-}\n-\n@@ -79,1 +71,1 @@\n-void ContinuationHelper::set_anchor_to_entry_pd(JavaFrameAnchor* anchor, ContinuationEntry* entry) {\n+inline void ContinuationHelper::set_anchor_to_entry_pd(JavaFrameAnchor* anchor, ContinuationEntry* entry) {\n@@ -84,1 +76,1 @@\n-void ContinuationHelper::set_anchor_pd(JavaFrameAnchor* anchor, intptr_t* sp) {\n+inline void ContinuationHelper::set_anchor_pd(JavaFrameAnchor* anchor, intptr_t* sp) {\n","filename":"src\/hotspot\/cpu\/x86\/continuationHelper_x86.inline.hpp","additions":3,"deletions":11,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -49,0 +49,1 @@\n+#include \"runtime\/continuationEntry.inline.hpp\"\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_64.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -45,0 +45,1 @@\n+#include \"runtime\/continuationEntry.inline.hpp\"\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -0,0 +1,47 @@\n+\/*\n+ * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef CPU_ZERO_CONTINUATIONENTRY_ZERO_INLINE_HPP\n+#define CPU_ZERO_CONTINUATIONENTRY_ZERO_INLINE_HPP\n+\n+#include \"runtime\/continuationEntry.hpp\"\n+\n+\/\/ TODO: Implement\n+\n+inline frame ContinuationEntry::to_frame() const {\n+  Unimplemented();\n+  return frame();\n+}\n+\n+inline intptr_t* ContinuationEntry::entry_fp() const {\n+  Unimplemented();\n+  return nullptr;\n+}\n+\n+inline void ContinuationEntry::update_register_map(RegisterMap* map) const {\n+  Unimplemented();\n+}\n+\n+#endif \/\/ CPU_ZERO_CONTINUATIONENTRY_ZERO_INLINE_HPP\n+\n","filename":"src\/hotspot\/cpu\/zero\/continuationEntry_zero.inline.hpp","additions":47,"deletions":0,"binary":false,"changes":47,"status":"added"},{"patch":"","filename":"src\/hotspot\/cpu\/zero\/continuationFreezeThaw_zero.inline.hpp","additions":0,"deletions":0,"binary":false,"changes":0,"previous_filename":"src\/hotspot\/cpu\/zero\/continuation_zero.inline.hpp","status":"renamed"},{"patch":"@@ -30,20 +30,0 @@\n-#include \"runtime\/frame.inline.hpp\"\n-#include \"runtime\/registerMap.hpp\"\n-#include \"utilities\/macros.hpp\"\n-\n-\/\/ TODO: Implement\n-\n-frame ContinuationEntry::to_frame() const {\n-  Unimplemented();\n-  return frame();\n-}\n-\n-intptr_t* ContinuationEntry::entry_fp() const {\n-  Unimplemented();\n-  return nullptr;\n-}\n-\n-void ContinuationEntry::update_register_map(RegisterMap* map) const {\n-  Unimplemented();\n-}\n-\n@@ -79,1 +59,1 @@\n-void ContinuationHelper::set_anchor_to_entry_pd(JavaFrameAnchor* anchor, ContinuationEntry* cont) {\n+inline void ContinuationHelper::set_anchor_to_entry_pd(JavaFrameAnchor* anchor, ContinuationEntry* cont) {\n@@ -84,1 +64,1 @@\n-void ContinuationHelper::set_anchor_pd(JavaFrameAnchor* anchor, intptr_t* sp) {\n+inline void ContinuationHelper::set_anchor_pd(JavaFrameAnchor* anchor, intptr_t* sp) {\n","filename":"src\/hotspot\/cpu\/zero\/continuationHelper_zero.inline.hpp","additions":2,"deletions":22,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -65,0 +65,1 @@\n+#include \"runtime\/continuationEntry.inline.hpp\"\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"runtime\/continuationEntry.inline.hpp\"\n","filename":"src\/hotspot\/share\/jfr\/recorder\/stacktrace\/jfrStackTrace.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -139,0 +139,1 @@\n+  return 0;\n@@ -151,0 +152,1 @@\n+  return 0;\n","filename":"src\/hotspot\/share\/oops\/stackChunkOop.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -163,0 +163,1 @@\n+   <!ATTLIST outptr impl CDATA #IMPLIED>\n@@ -1735,1 +1736,1 @@\n-        Suspend all virtual threads except those already suspended.\n+        Suspend all virtual threads except those in the exception list.\n@@ -1863,1 +1864,1 @@\n-        Resume all virtual threads except those already resumed.\n+        Resume all virtual threads except those in the exception list.\n@@ -1914,1 +1915,1 @@\n-          <jthread\/>\n+          <jthread impl=\"noconvert\"\/>\n@@ -1918,1 +1919,1 @@\n-              <errorlink id=\"JVMTI_ERROR_INVALID_THREAD\"><\/errorlink> will be returned.\n+              <errorlink id=\"JVMTI_ERROR_UNSUPPORTED_OPERATION\"><\/errorlink> will be returned.\n@@ -1929,1 +1930,1 @@\n-        <error id=\"JVMTI_ERROR_INVALID_THREAD\">\n+        <error id=\"JVMTI_ERROR_UNSUPPORTED_OPERATION\">\n@@ -2238,1 +2239,1 @@\n-              <errorlink id=\"JVMTI_ERROR_INVALID_THREAD\"><\/errorlink> will be returned.\n+              <errorlink id=\"JVMTI_ERROR_UNSUPPORTED_OPERATION\"><\/errorlink> will be returned.\n@@ -2274,1 +2275,1 @@\n-        <error id=\"JVMTI_ERROR_INVALID_THREAD\">\n+        <error id=\"JVMTI_ERROR_UNSUPPORTED_OPERATION\">\n@@ -2443,2 +2444,1 @@\n-        thread group.  Virtual threads are not considered live threads in\n-        a thread group and not returned by this function.\n+        thread group. Virtual threads are not returned by this function.\n@@ -10975,1 +10975,1 @@\n-            <errorlink id=\"JVMTI_ERROR_INVALID_THREAD\"><\/errorlink> will be returned.\n+            <errorlink id=\"JVMTI_ERROR_UNSUPPORTED_OPERATION\"><\/errorlink> will be returned.\n@@ -11009,1 +11009,1 @@\n-        <error id=\"JVMTI_ERROR_INVALID_THREAD\">\n+        <error id=\"JVMTI_ERROR_UNSUPPORTED_OPERATION\">\n@@ -11073,1 +11073,1 @@\n-          <jthread null=\"current\"\/>\n+          <jthread null=\"current\" impl=\"noconvert\"\/>\n@@ -11077,1 +11077,1 @@\n-               <errorlink id=\"JVMTI_ERROR_INVALID_THREAD\"><\/errorlink> will be returned.\n+               <errorlink id=\"JVMTI_ERROR_UNSUPPORTED_OPERATION\"><\/errorlink> will be returned.\n@@ -11081,1 +11081,1 @@\n-          <outptr><jlong\/><\/outptr>\n+          <outptr impl=\"nonullcheck\"><jlong\/><\/outptr>\n@@ -11091,1 +11091,1 @@\n-        <error id=\"JVMTI_ERROR_INVALID_THREAD\">\n+        <error id=\"JVMTI_ERROR_UNSUPPORTED_OPERATION\">\n@@ -12063,0 +12063,3 @@\n+    <errorid id=\"JVMTI_ERROR_UNSUPPORTED_OPERATION\" num=\"73\">\n+      Functionality is unsupported in this implementation.\n+    <\/errorid>\n","filename":"src\/hotspot\/share\/prims\/jvmti.xml","additions":18,"deletions":15,"binary":false,"changes":33,"status":"modified"},{"patch":"@@ -3,1 +3,1 @@\n- Copyright (c) 2002, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ Copyright (c) 2002, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -738,1 +738,1 @@\n-  <xsl:if test=\"count(nullok)=0\">\n+  <xsl:if test=\"count(nullok)=0 and not(contains(@impl,'nonullcheck'))\">\n","filename":"src\/hotspot\/share\/prims\/jvmtiEnter.xsl","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1162,1 +1162,0 @@\n-\/\/ java_thread - protected by ThreadsListHandle and pre-checked\n@@ -1164,1 +1163,16 @@\n-JvmtiEnv::StopThread(JavaThread* java_thread, jobject exception) {\n+JvmtiEnv::StopThread(jthread thread, jobject exception) {\n+  JavaThread* current_thread = JavaThread::current();\n+  ThreadsListHandle tlh(current_thread);\n+  JavaThread* java_thread = NULL;\n+  oop thread_oop = NULL;\n+\n+  NULL_CHECK(thread, JVMTI_ERROR_INVALID_THREAD);\n+\n+  jvmtiError err = get_threadOop_and_JavaThread(tlh.list(), thread, &java_thread, &thread_oop);\n+  if (err != JVMTI_ERROR_NONE) {\n+    return err;\n+  }\n+  if (java_lang_VirtualThread::is_instance(thread_oop)) {\n+    \/\/ No support for virtual threads.\n+    return JVMTI_ERROR_UNSUPPORTED_OPERATION;\n+  }\n@@ -1534,1 +1548,1 @@\n-    return JVMTI_ERROR_INVALID_THREAD;\n+    return JVMTI_ERROR_UNSUPPORTED_OPERATION;\n@@ -3853,1 +3867,1 @@\n-      return JVMTI_ERROR_INVALID_THREAD;\n+      return JVMTI_ERROR_UNSUPPORTED_OPERATION;\n@@ -3869,1 +3883,0 @@\n-\/\/ java_thread - protected by ThreadsListHandle and pre-checked\n@@ -3872,1 +3885,16 @@\n-JvmtiEnv::GetThreadCpuTime(JavaThread* java_thread, jlong* nanos_ptr) {\n+JvmtiEnv::GetThreadCpuTime(jthread thread, jlong* nanos_ptr) {\n+  JavaThread* current_thread = JavaThread::current();\n+  ThreadsListHandle tlh(current_thread);\n+  JavaThread* java_thread = NULL;\n+  oop thread_oop = NULL;\n+\n+  jvmtiError err = get_threadOop_and_JavaThread(tlh.list(), thread, &java_thread, &thread_oop);\n+  if (err != JVMTI_ERROR_NONE) {\n+    return err;\n+  }\n+  if (java_lang_VirtualThread::is_instance(thread_oop)) {\n+    \/\/ No support for virtual threads.\n+    return JVMTI_ERROR_UNSUPPORTED_OPERATION;\n+  }\n+  NULL_CHECK(nanos_ptr, JVMTI_ERROR_NULL_POINTER);\n+\n","filename":"src\/hotspot\/share\/prims\/jvmtiEnv.cpp","additions":34,"deletions":6,"binary":false,"changes":40,"status":"modified"},{"patch":"@@ -44,0 +44,1 @@\n+#include \"runtime\/continuationEntry.inline.hpp\"\n","filename":"src\/hotspot\/share\/prims\/jvmtiEnvBase.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -382,0 +382,4 @@\n+  if (Continuations::enabled()) {\n+    \/\/ Virtual threads support. There is a performance impact of VTMT transitions enabled.\n+    java_lang_VirtualThread::set_notify_jvmti_events(true);\n+  }\n","filename":"src\/hotspot\/share\/prims\/jvmtiExport.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -133,1 +133,3 @@\n-  jc.can_support_virtual_threads = 1;\n+  if (java_lang_VirtualThread::notify_jvmti_events()) {\n+    jc.can_support_virtual_threads = 1;\n+  }\n@@ -274,4 +276,0 @@\n-  \/\/ Special case for virtual thread events.\n-  \/\/ TBD: There can be a performance impact after check for can_support_virtual_threads has been removed.\n-  java_lang_VirtualThread::set_notify_jvmti_events(true);\n-\n","filename":"src\/hotspot\/share\/prims\/jvmtiManageCapabilities.cpp","additions":3,"deletions":5,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"runtime\/continuationEntry.hpp\"\n","filename":"src\/hotspot\/share\/prims\/stackwalk.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -26,20 +26,0 @@\n-#include \"classfile\/javaClasses.inline.hpp\"\n-#include \"classfile\/vmSymbols.hpp\"\n-#include \"code\/codeCache.inline.hpp\"\n-#include \"code\/compiledMethod.inline.hpp\"\n-#include \"code\/vmreg.inline.hpp\"\n-#include \"compiler\/oopMap.inline.hpp\"\n-#include \"gc\/shared\/gc_globals.hpp\"\n-#include \"gc\/shared\/barrierSet.hpp\"\n-#include \"gc\/shared\/memAllocator.hpp\"\n-#include \"gc\/shared\/threadLocalAllocBuffer.inline.hpp\"\n-#include \"interpreter\/interpreter.hpp\"\n-#include \"jfr\/jfrEvents.hpp\"\n-#include \"logging\/log.hpp\"\n-#include \"logging\/logStream.hpp\"\n-#include \"metaprogramming\/conditional.hpp\"\n-#include \"oops\/access.inline.hpp\"\n-#include \"oops\/oopsHierarchy.hpp\"\n-#include \"oops\/objArrayOop.inline.hpp\"\n-#include \"oops\/stackChunkOop.inline.hpp\"\n-#include \"prims\/jvmtiThreadState.hpp\"\n@@ -48,0 +28,1 @@\n+#include \"runtime\/continuationEntry.inline.hpp\"\n@@ -49,1 +30,1 @@\n-#include \"runtime\/frame.inline.hpp\"\n+#include \"runtime\/continuationWrapper.inline.hpp\"\n@@ -51,3 +32,0 @@\n-#include \"runtime\/jniHandles.inline.hpp\"\n-#include \"runtime\/keepStackGCProcessed.hpp\"\n-#include \"runtime\/orderAccess.hpp\"\n@@ -55,6 +33,0 @@\n-#include \"runtime\/prefetch.inline.hpp\"\n-#include \"runtime\/smallRegisterMap.inline.hpp\"\n-#include \"runtime\/stackChunkFrameStream.inline.hpp\"\n-#include \"runtime\/stackFrameStream.inline.hpp\"\n-#include \"runtime\/stackOverflow.hpp\"\n-#include \"runtime\/stackWatermarkSet.inline.hpp\"\n@@ -63,151 +35,0 @@\n-#include \"utilities\/debug.hpp\"\n-#include \"utilities\/exceptions.hpp\"\n-#include \"utilities\/macros.hpp\"\n-\n-#define CONT_JFR false \/\/ emit low-level JFR events that count slow\/fast path for continuation peformance debugging only\n-#if CONT_JFR\n-  #define CONT_JFR_ONLY(code) code\n-#else\n-  #define CONT_JFR_ONLY(code)\n-#endif\n-\n-static const bool TEST_THAW_ONE_CHUNK_FRAME = false; \/\/ force thawing frames one-at-a-time for testing\n-\n-\/*\n- * This file contains the implementation of continuation freezing (yield) and thawing (run).\n- *\n- * This code is very latency-critical and very hot. An ordinary and well-behaved server application\n- * would likely call these operations many thousands of times per second second, on every core.\n- *\n- * Freeze might be called every time the application performs any I\/O operation, every time it\n- * acquires a j.u.c. lock, every time it takes a message from a queue, and thaw can be called\n- * multiple times in each of those cases, as it is called by the return barrier, which may be\n- * invoked on method return.\n- *\n- * The amortized budget for each of those two operations is ~100-150ns. That is why, for\n- * example, every effort is made to avoid Java-VM transitions as much as possible.\n- *\n- * On the fast path, all frames are known to be compiled, and the chunk requires no barriers\n- * and so frames simply copied, and the bottom-most one is patched.\n- * On the slow path, internal pointers in interpreted frames are de\/relativized to\/from offsets\n- * and absolute pointers, and barriers invoked.\n- *\/\n-\n-\/************************************************\n-\n-Thread-stack layout on freeze\/thaw.\n-See corresponding stack-chunk layout in instanceStackChunkKlass.hpp\n-\n-            +----------------------------+\n-            |      .                     |\n-            |      .                     |\n-            |      .                     |\n-            |   carrier frames           |\n-            |                            |\n-            |----------------------------|\n-            |                            |\n-            |    Continuation.run        |\n-            |                            |\n-            |============================|\n-            |    enterSpecial frame      |\n-            |  pc                        |\n-            |  rbp                       |\n-            |  -----                     |\n-        ^   |  int argsize               | = ContinuationEntry\n-        |   |  oopDesc* cont             |\n-        |   |  oopDesc* chunk            |\n-        |   |  ContinuationEntry* parent |\n-        |   |  ...                       |\n-        |   |============================| <------ JavaThread::_cont_entry = entry->sp()\n-        |   |  ? alignment word ?        |\n-        |   |----------------------------| <--\\\n-        |   |                            |    |\n-        |   |  ? caller stack args ?     |    |   argsize (might not be 2-word aligned) words\n-Address |   |                            |    |   Caller is still in the chunk.\n-        |   |----------------------------|    |\n-        |   |  pc (? return barrier ?)   |    |  This pc contains the return barrier when the bottom-most frame\n-        |   |  rbp                       |    |  isn't the last one in the continuation.\n-        |   |                            |    |\n-        |   |    frame                   |    |\n-        |   |                            |    |\n-            +----------------------------|     \\__ Continuation frames to be frozen\/thawed\n-            |                            |     \/\n-            |    frame                   |    |\n-            |                            |    |\n-            |----------------------------|    |\n-            |                            |    |\n-            |    frame                   |    |\n-            |                            |    |\n-            |----------------------------| <--\/\n-            |                            |\n-            |    doYield\/safepoint stub  | When preempting forcefully, we could have a safepoint stub\n-            |                            | instead of a doYield stub\n-            |============================| <- the sp passed to freeze\n-            |                            |\n-            |  Native freeze\/thaw frames |\n-            |      .                     |\n-            |      .                     |\n-            |      .                     |\n-            +----------------------------+\n-\n-************************************************\/\n-\n-\/\/ TODO: See AbstractAssembler::generate_stack_overflow_check,\n-\/\/ Compile::bang_size_in_bytes(), m->as_SafePoint()->jvms()->interpreter_frame_size()\n-\/\/ when we stack-bang, we need to update a thread field with the lowest (farthest) bang point.\n-\n-\/\/ Data invariants are defined by Continuation::debug_verify_continuation and Continuation::debug_verify_stack_chunk\n-\n-\/\/ Used to just annotatate cold\/hot branches\n-#define LIKELY(condition)   (condition)\n-#define UNLIKELY(condition) (condition)\n-\n-\/\/ debugging functions\n-#ifdef ASSERT\n-extern \"C\" bool dbg_is_safe(const void* p, intptr_t errvalue); \/\/ address p is readable and *(intptr_t*)p != errvalue\n-\n-static void verify_continuation(oop continuation) { Continuation::debug_verify_continuation(continuation); }\n-\n-static void do_deopt_after_thaw(JavaThread* thread);\n-static bool do_verify_after_thaw(JavaThread* thread, bool barriers, stackChunkOop chunk, outputStream* st);\n-static void log_frames(JavaThread* thread);\n-static void print_frame_layout(const frame& f, bool callee_complete, outputStream* st = tty);\n-\n-#else\n-static void verify_continuation(oop continuation) { }\n-#endif\n-\n-#ifndef PRODUCT\n-static jlong java_tid(JavaThread* thread);\n-#endif\n-\n-\/\/ should match Continuation.preemptStatus() in Continuation.java\n-enum freeze_result {\n-  freeze_ok = 0,\n-  freeze_ok_bottom = 1,\n-  freeze_pinned_cs = 2,\n-  freeze_pinned_native = 3,\n-  freeze_pinned_monitor = 4,\n-  freeze_exception = 5\n-};\n-\n-const char* freeze_result_names[6] = {\n-  \"freeze_ok\",\n-  \"freeze_ok_bottom\",\n-  \"freeze_pinned_cs\",\n-  \"freeze_pinned_native\",\n-  \"freeze_pinned_monitor\",\n-  \"freeze_exception\"\n-};\n-\n-static freeze_result is_pinned0(JavaThread* thread, oop cont_scope, bool safepoint);\n-template<typename ConfigT> static inline int freeze_internal(JavaThread* current, intptr_t* const sp);\n-\n-enum thaw_kind {\n-  thaw_top = 0,\n-  thaw_return_barrier = 1,\n-  thaw_exception = 2,\n-};\n-\n-static inline int prepare_thaw_internal(JavaThread* thread, bool return_barrier);\n-template<typename ConfigT> static inline intptr_t* thaw_internal(JavaThread* thread, const thaw_kind kind);\n@@ -215,0 +36,1 @@\n+\/\/ defined in continuationFreezeThaw.cpp\n@@ -217,336 +39,0 @@\n-enum class oop_kind { NARROW, WIDE };\n-template <oop_kind oops, typename BarrierSetT>\n-class Config {\n-public:\n-  typedef Config<oops, BarrierSetT> SelfT;\n-  typedef typename Conditional<oops == oop_kind::NARROW, narrowOop, oop>::type OopT;\n-\n-  static int freeze(JavaThread* thread, intptr_t* const sp) {\n-    return freeze_internal<SelfT>(thread, sp);\n-  }\n-\n-  static intptr_t* thaw(JavaThread* thread, thaw_kind kind) {\n-    return thaw_internal<SelfT>(thread, kind);\n-  }\n-};\n-\n-static oop get_continuation(JavaThread* thread) {\n-  assert(thread != nullptr, \"\");\n-  assert(thread->threadObj() != nullptr, \"\");\n-  return java_lang_Thread::continuation(thread->threadObj());\n-}\n-\n-static bool stack_overflow_check(JavaThread* thread, int size, address sp) {\n-  const int page_size = os::vm_page_size();\n-  if (size > page_size) {\n-    if (sp - size < thread->stack_overflow_state()->stack_overflow_limit()) {\n-      return false;\n-    }\n-  }\n-  return true;\n-}\n-\n-#ifdef ASSERT\n-inline void clear_anchor(JavaThread* thread) {\n-  thread->frame_anchor()->clear();\n-}\n-\n-static void set_anchor(JavaThread* thread, intptr_t* sp) {\n-  address pc = *(address*)(sp - frame::sender_sp_ret_address_offset());\n-  assert(pc != nullptr, \"\");\n-\n-  JavaFrameAnchor* anchor = thread->frame_anchor();\n-  anchor->set_last_Java_sp(sp);\n-  anchor->set_last_Java_pc(pc);\n-  ContinuationHelper::set_anchor_pd(anchor, sp);\n-\n-  assert(thread->has_last_Java_frame(), \"\");\n-  assert(thread->last_frame().cb() != nullptr, \"\");\n-}\n-#endif \/\/ ASSERT\n-\n-static void set_anchor_to_entry(JavaThread* thread, ContinuationEntry* entry) {\n-  JavaFrameAnchor* anchor = thread->frame_anchor();\n-  anchor->set_last_Java_sp(entry->entry_sp());\n-  anchor->set_last_Java_pc(entry->entry_pc());\n-  ContinuationHelper::set_anchor_to_entry_pd(anchor, entry);\n-\n-  assert(thread->has_last_Java_frame(), \"\");\n-  assert(thread->last_frame().cb() != nullptr, \"\");\n-}\n-\n-NOINLINE static void flush_stack_processing(JavaThread* thread, intptr_t* sp) {\n-  log_develop_trace(continuations)(\"flush_stack_processing\");\n-  for (StackFrameStream fst(thread, true, true); fst.current()->sp() <= sp; fst.next()) {\n-    ;\n-  }\n-}\n-\n-inline void maybe_flush_stack_processing(JavaThread* thread, intptr_t* sp) {\n-  StackWatermark* sw;\n-  uintptr_t watermark;\n-  if ((sw = StackWatermarkSet::get(thread, StackWatermarkKind::gc)) != nullptr\n-        && (watermark = sw->watermark()) != 0\n-        && watermark <= (uintptr_t)sp) {\n-    flush_stack_processing(thread, sp);\n-  }\n-}\n-\n-inline void maybe_flush_stack_processing(JavaThread* thread, const ContinuationEntry* entry) {\n-  maybe_flush_stack_processing(thread, (intptr_t*)((uintptr_t)entry->entry_sp() + ContinuationEntry::size()));\n-}\n-\n-\n-\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n-\n-\/\/ Intermediary to the jdk.internal.vm.Continuation objects and ContinuationEntry\n-\/\/ This object is created when we begin a operation for a continuation, and is destroyed when the operation completes.\n-\/\/ Contents are read from the Java object at the entry points of this module, and written at exit or calls into Java\n-\/\/ It also serves as a custom NoSafepointVerifier\n-class ContinuationWrapper : public StackObj {\n-private:\n-  JavaThread* const  _thread;   \/\/ Thread being frozen\/thawed\n-  ContinuationEntry* _entry;\n-  \/\/ These oops are managed by SafepointOp\n-  oop                _continuation;  \/\/ jdk.internal.vm.Continuation instance\n-  stackChunkOop      _tail;\n-\n-#if CONT_JFR \/\/ Profiling data for the JFR event\n-  short _e_size;\n-  short _e_num_interpreted_frames;\n-#endif\n-\n-  ContinuationWrapper(const ContinuationWrapper& cont); \/\/ no copy constructor\n-\n-private:\n-  DEBUG_ONLY(Thread* _current_thread;)\n-  friend class SafepointOp;\n-\n-  void disallow_safepoint() {\n-    #ifdef ASSERT\n-      assert(_continuation != nullptr, \"\");\n-      _current_thread = Thread::current();\n-      if (_current_thread->is_Java_thread()) {\n-        JavaThread::cast(_current_thread)->inc_no_safepoint_count();\n-      }\n-    #endif\n-  }\n-\n-  void allow_safepoint() {\n-    #ifdef ASSERT\n-      \/\/ we could have already allowed safepoints in done\n-      if (_continuation != nullptr && _current_thread->is_Java_thread()) {\n-        JavaThread::cast(_current_thread)->dec_no_safepoint_count();\n-      }\n-    #endif\n-  }\n-\n-public:\n-  void done() {\n-    allow_safepoint(); \/\/ must be done first\n-    _continuation = nullptr;\n-    _tail = (stackChunkOop)badOop;\n-  }\n-\n-  class SafepointOp : public StackObj {\n-    ContinuationWrapper& _cont;\n-    Handle _conth;\n-  public:\n-    SafepointOp(Thread* current, ContinuationWrapper& cont)\n-      : _cont(cont), _conth(current, cont._continuation) {\n-      _cont.allow_safepoint();\n-    }\n-    ~SafepointOp() { \/\/ reload oops\n-      _cont._continuation = _conth();\n-      if (_cont._tail != nullptr) {\n-        _cont._tail = jdk_internal_vm_Continuation::tail(_cont._continuation);\n-      }\n-      _cont.disallow_safepoint();\n-    }\n-  };\n-\n-public:\n-  ~ContinuationWrapper() { allow_safepoint(); }\n-\n-  ContinuationWrapper(JavaThread* thread, oop continuation);\n-  ContinuationWrapper(oop continuation);\n-  ContinuationWrapper(const RegisterMap* map);\n-\n-  JavaThread* thread() const         { return _thread; }\n-  oop continuation()                 { return _continuation; }\n-  stackChunkOop tail() const         { return _tail; }\n-  void set_tail(stackChunkOop chunk) { _tail = chunk; }\n-\n-  oop parent()                   { return jdk_internal_vm_Continuation::parent(_continuation); }\n-  bool is_preempted()            { return jdk_internal_vm_Continuation::is_preempted(_continuation); }\n-  void set_preempted(bool value) { jdk_internal_vm_Continuation::set_preempted(_continuation, value); }\n-  void read()                    { _tail  = jdk_internal_vm_Continuation::tail(_continuation); }\n-  void write() {\n-    assert(oopDesc::is_oop(_continuation), \"bad oop\");\n-    assert(oopDesc::is_oop_or_null(_tail), \"bad oop\");\n-    jdk_internal_vm_Continuation::set_tail(_continuation, _tail);\n-  }\n-\n-  NOT_PRODUCT(intptr_t hash()    { return Thread::current()->is_Java_thread() ? _continuation->identity_hash() : -1; })\n-\n-  ContinuationEntry* entry() const { return _entry; }\n-  bool is_mounted()   const { return _entry != nullptr; }\n-  intptr_t* entrySP() const { return _entry->entry_sp(); }\n-  intptr_t* entryFP() const { return _entry->entry_fp(); }\n-  address   entryPC() const { return _entry->entry_pc(); }\n-  int argsize()       const { assert(_entry->argsize() >= 0, \"\"); return _entry->argsize(); }\n-  void set_argsize(int value) { _entry->set_argsize(value); }\n-\n-  bool is_empty() const { return last_nonempty_chunk() == nullptr; }\n-  const frame last_frame();\n-\n-  stackChunkOop last_nonempty_chunk() const { return nonempty_chunk(_tail); }\n-  inline stackChunkOop nonempty_chunk(stackChunkOop chunk) const;\n-  stackChunkOop find_chunk_by_address(void* p) const;\n-\n-#if CONT_JFR\n-  inline void record_interpreted_frame() { _e_num_interpreted_frames++; }\n-  inline void record_size_copied(int size) { _e_size += size << LogBytesPerWord; }\n-  template<typename Event> void post_jfr_event(Event *e, JavaThread* jt);\n-#endif\n-\n-#ifdef ASSERT\n-  inline bool is_entry_frame(const frame& f);\n-  bool chunk_invariant(outputStream* st);\n-#endif\n-};\n-\n-ContinuationWrapper::ContinuationWrapper(JavaThread* thread, oop continuation)\n-  : _thread(thread), _entry(thread->last_continuation()), _continuation(continuation)\n-#if CONT_JFR\n-  , _e_size(0), _e_num_interpreted_frames(0)\n-#endif\n-  {\n-  assert(oopDesc::is_oop(_continuation),\n-         \"Invalid continuation object: \" INTPTR_FORMAT, p2i((void*)_continuation));\n-  assert(_continuation == _entry->cont_oop(), \"cont: \" INTPTR_FORMAT \" entry: \" INTPTR_FORMAT \" entry_sp: \"\n-         INTPTR_FORMAT, p2i((oopDesc*)_continuation), p2i((oopDesc*)_entry->cont_oop()), p2i(entrySP()));\n-  disallow_safepoint();\n-  read();\n-}\n-\n-ContinuationWrapper::ContinuationWrapper(oop continuation)\n-  : _thread(nullptr), _entry(nullptr), _continuation(continuation)\n-#if CONT_JFR\n-  , _e_size(0), _e_num_interpreted_frames(0)\n-#endif\n-  {\n-  assert(oopDesc::is_oop(_continuation),\n-         \"Invalid continuation object: \" INTPTR_FORMAT, p2i((void*)_continuation));\n-  disallow_safepoint();\n-  read();\n-}\n-\n-ContinuationWrapper::ContinuationWrapper(const RegisterMap* map)\n-  : _thread(map->thread()),\n-    _entry(Continuation::get_continuation_entry_for_continuation(_thread, map->stack_chunk()->cont())),\n-    _continuation(map->stack_chunk()->cont())\n-#if CONT_JFR\n-  , _e_size(0), _e_num_interpreted_frames(0)\n-#endif\n-  {\n-  assert(oopDesc::is_oop(_continuation),\"Invalid cont: \" INTPTR_FORMAT, p2i((void*)_continuation));\n-  assert(_entry == nullptr || _continuation == _entry->cont_oop(),\n-    \"cont: \" INTPTR_FORMAT \" entry: \" INTPTR_FORMAT \" entry_sp: \" INTPTR_FORMAT,\n-    p2i( (oopDesc*)_continuation), p2i((oopDesc*)_entry->cont_oop()), p2i(entrySP()));\n-  disallow_safepoint();\n-  read();\n-}\n-\n-const frame ContinuationWrapper::last_frame() {\n-  stackChunkOop chunk = last_nonempty_chunk();\n-  if (chunk == nullptr) {\n-    return frame();\n-  }\n-  return StackChunkFrameStream<ChunkFrames::Mixed>(chunk).to_frame();\n-}\n-\n-inline stackChunkOop ContinuationWrapper::nonempty_chunk(stackChunkOop chunk) const {\n-  while (chunk != nullptr && chunk->is_empty()) {\n-    chunk = chunk->parent();\n-  }\n-  return chunk;\n-}\n-\n-stackChunkOop ContinuationWrapper::find_chunk_by_address(void* p) const {\n-  for (stackChunkOop chunk = tail(); chunk != nullptr; chunk = chunk->parent()) {\n-    if (chunk->is_in_chunk(p)) {\n-      assert(chunk->is_usable_in_chunk(p), \"\");\n-      return chunk;\n-    }\n-  }\n-  return nullptr;\n-}\n-\n-#if CONT_JFR\n-template<typename Event> void ContinuationWrapper::post_jfr_event(Event* e, JavaThread* jt) {\n-  if (e->should_commit()) {\n-    log_develop_trace(continuations)(\"JFR event: iframes: %d size: %d\", _e_num_interpreted_frames, _e_size);\n-    e->set_carrierThread(JFR_JVM_THREAD_ID(jt));\n-    e->set_contClass(_continuation->klass());\n-    e->set_numIFrames(_e_num_interpreted_frames);\n-    e->set_size(_e_size);\n-    e->commit();\n-  }\n-}\n-#endif\n-\n-#ifdef ASSERT\n-inline bool ContinuationWrapper::is_entry_frame(const frame& f) {\n-  return f.sp() == entrySP();\n-}\n-\n-bool ContinuationWrapper::chunk_invariant(outputStream* st) {\n-  \/\/ only the topmost chunk can be empty\n-  if (_tail == nullptr) {\n-    return true;\n-  }\n-\n-  int i = 1;\n-  for (stackChunkOop chunk = _tail->parent(); chunk != nullptr; chunk = chunk->parent()) {\n-    if (chunk->is_empty()) {\n-      assert(chunk != _tail, \"\");\n-      st->print_cr(\"i: %d\", i);\n-      chunk->print_on(true, st);\n-      return false;\n-    }\n-    i++;\n-  }\n-  return true;\n-}\n-#endif \/\/ ASSERT\n-\n-\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n-\n-\/\/ Entry point to freeze. Transitions are handled manually\n-\/\/ Called from generate_cont_doYield() in stubGenerator_<cpu>.cpp through Continuation::freeze_entry();\n-template<typename ConfigT>\n-static JRT_BLOCK_ENTRY(int, freeze(JavaThread* current, intptr_t* sp))\n-  assert(sp == current->frame_anchor()->last_Java_sp(), \"\");\n-\n-  if (current->raw_cont_fastpath() > current->last_continuation()->entry_sp() || current->raw_cont_fastpath() < sp) {\n-    current->set_cont_fastpath(nullptr);\n-  }\n-\n-  return ConfigT::freeze(current, sp);\n-JRT_END\n-\n-JRT_LEAF(int, Continuation::prepare_thaw(JavaThread* thread, bool return_barrier))\n-  return prepare_thaw_internal(thread, return_barrier);\n-JRT_END\n-\n-template<typename ConfigT>\n-static JRT_LEAF(intptr_t*, thaw(JavaThread* thread, int kind))\n-  \/\/ TODO: JRT_LEAF and NoHandleMark is problematic for JFR events.\n-  \/\/ vFrameStreamCommon allocates Handles in RegisterMap for continuations.\n-  \/\/ JRT_ENTRY instead?\n-  ResetNoHandleMark rnhm;\n-\n-  return ConfigT::thaw(thread, (thaw_kind)kind);\n-JRT_END\n-\n@@ -567,3 +53,3 @@\n-JVM_ENTRY(jint, CONT_isPinned0(JNIEnv* env, jobject cont_scope)) {\n-  JavaThread* thread = JavaThread::thread_from_jni_environment(env);\n-  return is_pinned0(thread, JNIHandles::resolve(cont_scope), false);\n+#ifndef PRODUCT\n+static jlong java_tid(JavaThread* thread) {\n+  return java_lang_Thread::thread_id(thread->threadObj());\n@@ -571,1 +57,1 @@\n-JVM_END\n+#endif\n@@ -922,391 +408,0 @@\n-\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/ FREEZE \/\/\/\/\n-\n-class FreezeBase : public StackObj {\n-protected:\n-  JavaThread* const _thread;\n-  ContinuationWrapper& _cont;\n-  bool _barriers;\n-  const bool _preempt; \/\/ used only on the slow path\n-\n-  intptr_t *_bottom_address;\n-\n-  int _size; \/\/ total size of all frames plus metadata in words.\n-  int _align_size;\n-\n-  JvmtiSampledObjectAllocEventCollector* _jvmti_event_collector;\n-\n-  NOT_PRODUCT(int _frames;)\n-  DEBUG_ONLY(intptr_t* _last_write;)\n-\n-  inline FreezeBase(JavaThread* thread, ContinuationWrapper& cont, bool preempt);\n-\n-public:\n-  NOINLINE freeze_result freeze_slow();\n-\n-  void set_jvmti_event_collector(JvmtiSampledObjectAllocEventCollector* jsoaec) { _jvmti_event_collector = jsoaec; }\n-\n-protected:\n-  inline void init_rest();\n-  void throw_stack_overflow_on_humongous_chunk();\n-\n-  \/\/ fast path\n-  inline void copy_to_chunk(intptr_t* from, intptr_t* to, int size);\n-  inline void unwind_frames();\n-\n-  inline void patch_stack_pd(intptr_t* frame_sp, intptr_t* heap_sp);\n-\n-private:\n-  \/\/ slow path\n-  frame freeze_start_frame();\n-  frame freeze_start_frame_safepoint_stub(frame f);\n-  NOINLINE freeze_result freeze(frame& f, frame& caller, int callee_argsize, bool callee_interpreted, bool top);\n-  inline frame freeze_start_frame_yield_stub(frame f);\n-  template<typename FKind>\n-  inline freeze_result recurse_freeze_java_frame(const frame& f, frame& caller, int fsize, int argsize);\n-  inline void before_freeze_java_frame(const frame& f, const frame& caller, int fsize, int argsize, bool bottom);\n-  inline void after_freeze_java_frame(const frame& hf, bool bottom);\n-  freeze_result finalize_freeze(const frame& callee, frame& caller, int argsize);\n-  void patch(const frame& f, frame& hf, const frame& caller, bool bottom);\n-  NOINLINE freeze_result recurse_freeze_interpreted_frame(frame& f, frame& caller, int callee_argsize, bool callee_interpreted);\n-  freeze_result recurse_freeze_compiled_frame(frame& f, frame& caller, int callee_argsize, bool callee_interpreted);\n-  NOINLINE freeze_result recurse_freeze_stub_frame(frame& f, frame& caller);\n-  NOINLINE void finish_freeze(const frame& f, const frame& top);\n-\n-  inline bool stack_overflow();\n-\n-  static frame sender(const frame& f) { return f.is_interpreted_frame() ? sender<ContinuationHelper::InterpretedFrame>(f)\n-                                                                        : sender<ContinuationHelper::NonInterpretedUnknownFrame>(f); }\n-  template<typename FKind> static inline frame sender(const frame& f);\n-  template<typename FKind> frame new_heap_frame(frame& f, frame& caller);\n-  inline void set_top_frame_metadata_pd(const frame& hf);\n-  inline void patch_pd(frame& callee, const frame& caller);\n-  void adjust_interpreted_frame_unextended_sp(frame& f);\n-  static inline void relativize_interpreted_frame_metadata(const frame& f, const frame& hf);\n-\n-protected:\n-  virtual stackChunkOop allocate_chunk_slow(size_t stack_size) = 0;\n-};\n-\n-template <typename ConfigT>\n-class Freeze : public FreezeBase {\n-private:\n-  stackChunkOop allocate_chunk(size_t stack_size);\n-\n-public:\n-  inline Freeze(JavaThread* thread, ContinuationWrapper& cont, bool preempt)\n-    : FreezeBase(thread, cont, preempt) {}\n-\n-  inline bool is_chunk_available(intptr_t* frame_sp\n-#ifdef ASSERT\n-    , int* out_size = nullptr\n-#endif\n-  );\n-  template <bool chunk_available> freeze_result try_freeze_fast(intptr_t* sp);\n-  template <bool chunk_available> bool freeze_fast(intptr_t* frame_sp);\n-\n-protected:\n-  virtual stackChunkOop allocate_chunk_slow(size_t stack_size) override { return allocate_chunk(stack_size); }\n-};\n-\n-FreezeBase::FreezeBase(JavaThread* thread, ContinuationWrapper& cont, bool preempt) :\n-    _thread(thread), _cont(cont), _barriers(false), _preempt(preempt) {\n-  DEBUG_ONLY(_jvmti_event_collector = nullptr;)\n-\n-  assert(_thread != nullptr, \"\");\n-  assert(_thread->last_continuation()->entry_sp() == _cont.entrySP(), \"\");\n-\n-  _bottom_address = _cont.entrySP() - _cont.argsize();\n-  DEBUG_ONLY(_cont.entry()->verify_cookie();)\n-\n-  assert(!Interpreter::contains(_cont.entryPC()), \"\");\n-\n-#ifdef _LP64\n-  if (((intptr_t)_bottom_address & 0xf) != 0) {\n-    _bottom_address--;\n-  }\n-  assert(is_aligned(_bottom_address, frame::frame_alignment), \"\");\n-#endif\n-\n-  log_develop_trace(continuations)(\"bottom_address: \" INTPTR_FORMAT \" entrySP: \" INTPTR_FORMAT \" argsize: \" PTR_FORMAT,\n-                p2i(_bottom_address), p2i(_cont.entrySP()), (_cont.entrySP() - _bottom_address) << LogBytesPerWord);\n-  assert(_bottom_address != nullptr, \"\");\n-  assert(_bottom_address <= _cont.entrySP(), \"\");\n-  DEBUG_ONLY(_last_write = nullptr;)\n-}\n-\n-void FreezeBase::init_rest() { \/\/ we want to postpone some initialization after chunk handling\n-  _size = 0;\n-  _align_size = 0;\n-  NOT_PRODUCT(_frames = 0;)\n-}\n-\n-void FreezeBase::copy_to_chunk(intptr_t* from, intptr_t* to, int size) {\n-  stackChunkOop chunk = _cont.tail();\n-  chunk->copy_from_stack_to_chunk(from, to, size);\n-  CONT_JFR_ONLY(_cont.record_size_copied(size);)\n-\n-#ifdef ASSERT\n-  if (_last_write != nullptr) {\n-    assert(_last_write == to + size, \"Missed a spot: _last_write: \" INTPTR_FORMAT \" to+size: \" INTPTR_FORMAT\n-        \" stack_size: %d _last_write offset: \" PTR_FORMAT \" to+size: \" PTR_FORMAT, p2i(_last_write), p2i(to+size),\n-        chunk->stack_size(), _last_write-chunk->start_address(), to+size-chunk->start_address());\n-    _last_write = to;\n-  }\n-#endif\n-}\n-\n-\/\/ Called _after_ the last possible safepoint during the freeze operation (chunk allocation)\n-void FreezeBase::unwind_frames() {\n-  ContinuationEntry* entry = _cont.entry();\n-  maybe_flush_stack_processing(_thread, entry);\n-  set_anchor_to_entry(_thread, entry);\n-}\n-\n-template <typename ConfigT>\n-template <bool chunk_available>\n-freeze_result Freeze<ConfigT>::try_freeze_fast(intptr_t* sp) {\n-  if (freeze_fast<chunk_available>(sp)) {\n-    return freeze_ok;\n-  }\n-  if (_thread->has_pending_exception()) {\n-    return freeze_exception;\n-  }\n-\n-  EventContinuationFreezeOld e;\n-  if (e.should_commit()) {\n-    e.set_id(cast_from_oop<u8>(_cont.continuation()));\n-    e.commit();\n-  }\n-  \/\/ TODO R REMOVE when deopt change is fixed\n-  assert(!_thread->cont_fastpath() || _barriers, \"\");\n-  log_develop_trace(continuations)(\"-- RETRYING SLOW --\");\n-  return freeze_slow();\n-}\n-\n-\/\/ returns true iff there's room in the chunk for a fast, compiled-frame-only freeze\n-template <typename ConfigT>\n-bool Freeze<ConfigT>::is_chunk_available(intptr_t* frame_sp\n-#ifdef ASSERT\n-    , int* out_size\n-#endif\n-  ) {\n-  stackChunkOop chunk = _cont.tail();\n-  if (chunk == nullptr || chunk->is_gc_mode() || chunk->requires_barriers() || chunk->has_mixed_frames()) {\n-    log_develop_trace(continuations)(\"is_chunk_available %s\", chunk == nullptr ? \"no chunk\" : \"chunk requires barriers\");\n-    return false;\n-  }\n-\n-  \/\/ assert(CodeCache::find_blob(*(address*)(frame_sp - SENDER_SP_RET_ADDRESS_OFFSET)) == StubRoutines::cont_doYield_stub(), \"\"); -- fails on Windows\n-  assert(StubRoutines::cont_doYield_stub()->frame_size() == frame::metadata_words, \"\");\n-  intptr_t* const stack_top     = frame_sp + frame::metadata_words;\n-  intptr_t* const stack_bottom  = _cont.entrySP() - ContinuationHelper::frame_align_words(_cont.argsize());\n-\n-  int size = stack_bottom - stack_top; \/\/ in words\n-\n-  const int chunk_sp = chunk->sp();\n-  if (chunk_sp < chunk->stack_size()) {\n-    size -= _cont.argsize();\n-  }\n-  assert(size > 0, \"\");\n-\n-  bool available = chunk_sp - frame::metadata_words >= size;\n-  log_develop_trace(continuations)(\"is_chunk_available: %d size: %d argsize: %d top: \" INTPTR_FORMAT \" bottom: \" INTPTR_FORMAT,\n-    available, _cont.argsize(), size, p2i(stack_top), p2i(stack_bottom));\n-  DEBUG_ONLY(if (out_size != nullptr) *out_size = size;)\n-  return available;\n-}\n-\n-template <typename ConfigT>\n-template <bool chunk_available>\n-bool Freeze<ConfigT>::freeze_fast(intptr_t* frame_sp) {\n-  assert(_cont.chunk_invariant(tty), \"\");\n-  assert(!Interpreter::contains(_cont.entryPC()), \"\");\n-  assert(StubRoutines::cont_doYield_stub()->frame_size() == frame::metadata_words, \"\");\n-\n-  \/\/ properties of the continuation on the stack; all sizes are in words\n-  intptr_t* const cont_stack_top    = frame_sp + frame::metadata_words; \/\/ we add metadata_words to skip the doYield stub frame\n-  intptr_t* const cont_stack_bottom = _cont.entrySP() - ContinuationHelper::frame_align_words(_cont.argsize()); \/\/ see alignment in thaw\n-\n-  const int cont_size = cont_stack_bottom - cont_stack_top;\n-\n-  log_develop_trace(continuations)(\"freeze_fast size: %d argsize: %d top: \" INTPTR_FORMAT \" bottom: \" INTPTR_FORMAT,\n-    cont_size, _cont.argsize(), p2i(cont_stack_top), p2i(cont_stack_bottom));\n-  assert(cont_size > 0, \"\");\n-\n-#ifdef ASSERT\n-  bool empty = true;\n-  int is_chunk_available_size;\n-  bool is_chunk_available0 = is_chunk_available(frame_sp, &is_chunk_available_size);\n-  intptr_t* orig_chunk_sp = nullptr;\n-#endif\n-\n-  stackChunkOop chunk = _cont.tail();\n-  int chunk_start_sp; \/\/ the chunk's sp before the freeze, adjusted to point beyond the stack-passed arguments in the topmost frame\n-  if (chunk_available) { \/\/ LIKELY\n-    DEBUG_ONLY(orig_chunk_sp = chunk->sp_address();)\n-\n-    assert(is_chunk_available0, \"\");\n-\n-    if (chunk->sp() < chunk->stack_size()) { \/\/ we are copying into a non-empty chunk\n-      DEBUG_ONLY(empty = false;)\n-      assert(chunk->sp() < (chunk->stack_size() - chunk->argsize()), \"\");\n-      assert(*(address*)(chunk->sp_address() - frame::sender_sp_ret_address_offset()) == chunk->pc(), \"\");\n-\n-      chunk_start_sp = chunk->sp() + _cont.argsize(); \/\/ we overlap; we'll overwrite the chunk's top frame's callee arguments\n-      assert(chunk_start_sp <= chunk->stack_size(), \"sp not pointing into stack\");\n-\n-      \/\/ increase max_size by what we're freezing minus the overlap\n-      chunk->set_max_size(chunk->max_size() + cont_size - _cont.argsize());\n-\n-      intptr_t* const bottom_sp = cont_stack_bottom - _cont.argsize();\n-      assert(bottom_sp == _bottom_address, \"\");\n-      \/\/ Because the chunk isn't empty, we know there's a caller in the chunk, therefore the bottom-most frame\n-      \/\/ should have a return barrier (installed back when we thawed it).\n-      assert(*(address*)(bottom_sp-frame::sender_sp_ret_address_offset()) == StubRoutines::cont_returnBarrier(),\n-             \"should be the continuation return barrier\");\n-      \/\/ We copy the fp from the chunk back to the stack because it contains some caller data,\n-      \/\/ including, possibly, an oop that might have gone stale since we thawed.\n-      patch_stack_pd(bottom_sp, chunk->sp_address());\n-      \/\/ we don't patch the return pc at this time, so as not to make the stack unwalkable for async walks\n-    } else { \/\/ the chunk is empty\n-      chunk_start_sp = chunk->sp();\n-\n-      assert(chunk_start_sp == chunk->stack_size(), \"\");\n-\n-      chunk->set_max_size(cont_size);\n-      chunk->set_argsize(_cont.argsize());\n-    }\n-  } else { \/\/ no chunk; allocate\n-    assert(_thread->thread_state() == _thread_in_vm, \"\");\n-    assert(!is_chunk_available(frame_sp), \"\");\n-    assert(_thread->cont_fastpath(), \"\");\n-\n-    chunk = allocate_chunk(cont_size + frame::metadata_words);\n-    if (UNLIKELY(chunk == nullptr || !_thread->cont_fastpath() || _barriers)) { \/\/ OOME\/probably humongous\n-      log_develop_trace(continuations)(\"Retrying slow. Barriers: %d\", _barriers);\n-      return false;\n-    }\n-\n-    chunk->set_max_size(cont_size);\n-    chunk->set_argsize(_cont.argsize());\n-\n-    \/\/ in a fresh chunk, we freeze *with* the bottom-most frame's stack arguments.\n-    \/\/ They'll then be stored twice: in the chunk and in the parent chunk's top frame\n-    chunk_start_sp = cont_size + frame::metadata_words;\n-    assert(chunk_start_sp == chunk->stack_size(), \"\");\n-\n-    DEBUG_ONLY(orig_chunk_sp = chunk->start_address() + chunk_start_sp;)\n-  }\n-\n-  assert(chunk != nullptr, \"\");\n-  assert(!chunk->has_mixed_frames(), \"\");\n-  assert(!chunk->is_gc_mode(), \"\");\n-  assert(!chunk->has_bitmap(), \"\");\n-  assert(!chunk->requires_barriers(), \"\");\n-  assert(chunk == _cont.tail(), \"\");\n-\n-  \/\/ We unwind frames after the last safepoint so that the GC will have found the oops in the frames, but before\n-  \/\/ writing into the chunk. This is so that an asynchronous stack walk (not at a safepoint) that suspends us here\n-  \/\/ will either see no continuation on the stack, or a consistent chunk.\n-  unwind_frames();\n-\n-  log_develop_trace(continuations)(\"freeze_fast start: chunk \" INTPTR_FORMAT \" size: %d orig sp: %d argsize: %d\",\n-    p2i((oopDesc*)chunk), chunk->stack_size(), chunk_start_sp, _cont.argsize());\n-  assert(chunk_start_sp <= chunk->stack_size(), \"\");\n-  assert(chunk_start_sp >= cont_size, \"no room in the chunk\");\n-\n-  const int chunk_new_sp = chunk_start_sp - cont_size; \/\/ the chunk's new sp, after freeze\n-  assert(!is_chunk_available0 || orig_chunk_sp - (chunk->start_address() + chunk_new_sp) == is_chunk_available_size, \"\");\n-\n-  intptr_t* chunk_top = chunk->start_address() + chunk_new_sp;\n-  assert(empty || *(address*)(orig_chunk_sp - frame::sender_sp_ret_address_offset()) == chunk->pc(), \"\");\n-\n-  log_develop_trace(continuations)(\"freeze_fast start: \" INTPTR_FORMAT \" sp: %d chunk_top: \" INTPTR_FORMAT,\n-                              p2i(chunk->start_address()), chunk_new_sp, p2i(chunk_top));\n-  intptr_t* from = cont_stack_top - frame::metadata_words;\n-  intptr_t* to   = chunk_top - frame::metadata_words;\n-  copy_to_chunk(from, to, cont_size + frame::metadata_words);\n-  \/\/ Because we're not patched yet, the chunk is now in a bad state\n-\n-  \/\/ patch return pc of the bottom-most frozen frame (now in the chunk) with the actual caller's return address\n-  intptr_t* chunk_bottom_sp = chunk_top + cont_size - _cont.argsize();\n-  assert(empty || *(address*)(chunk_bottom_sp-frame::sender_sp_ret_address_offset()) == StubRoutines::cont_returnBarrier(), \"\");\n-  *(address*)(chunk_bottom_sp - frame::sender_sp_ret_address_offset()) = chunk->pc();\n-\n-  \/\/ We're always writing to a young chunk, so the GC can't see it until the next safepoint.\n-  chunk->set_sp(chunk_new_sp);\n-  \/\/ set chunk->pc to the return address of the topmost frame in the chunk\n-  chunk->set_pc(*(address*)(cont_stack_top - frame::sender_sp_ret_address_offset()));\n-\n-  _cont.write();\n-\n-  log_develop_trace(continuations)(\"FREEZE CHUNK #\" INTPTR_FORMAT \" (young)\", _cont.hash());\n-  LogTarget(Trace, continuations) lt;\n-  if (lt.develop_is_enabled()) {\n-    LogStream ls(lt);\n-    chunk->print_on(true, &ls);\n-  }\n-\n-  \/\/ Verification\n-  assert(_cont.chunk_invariant(tty), \"\");\n-  chunk->verify();\n-\n-#if CONT_JFR\n-  EventContinuationFreezeYoung e;\n-  if (e.should_commit()) {\n-    e.set_id(cast_from_oop<u8>(chunk));\n-    DEBUG_ONLY(e.set_allocate(allocated);)\n-    e.set_size(size << LogBytesPerWord);\n-    e.commit();\n-  }\n-#endif\n-\n-  return true;\n-}\n-\n-NOINLINE freeze_result FreezeBase::freeze_slow() {\n-#ifdef ASSERT\n-  ResourceMark rm;\n-#endif\n-\n-  log_develop_trace(continuations)(\"freeze_slow  #\" INTPTR_FORMAT, _cont.hash());\n-  assert(_thread->thread_state() == _thread_in_vm || _thread->thread_state() == _thread_blocked, \"\");\n-\n-  init_rest();\n-\n-  HandleMark hm(Thread::current());\n-\n-  frame f = freeze_start_frame();\n-\n-  LogTarget(Debug, continuations) lt;\n-  if (lt.develop_is_enabled()) {\n-    LogStream ls(lt);\n-    f.print_on(&ls);\n-  }\n-\n-  frame caller;\n-  freeze_result res = freeze(f, caller, 0, false, true);\n-\n-  if (res == freeze_ok) {\n-    finish_freeze(f, caller);\n-    _cont.write();\n-  }\n-\n-  return res;\n-}\n-\n-frame FreezeBase::freeze_start_frame() {\n-  frame f = _thread->last_frame();\n-  if (LIKELY(!_preempt)) {\n-    assert(StubRoutines::cont_doYield_stub()->contains(f.pc()), \"\");\n-    return freeze_start_frame_yield_stub(f);\n-  } else {\n-    return freeze_start_frame_safepoint_stub(f);\n-  }\n-}\n-\n-frame FreezeBase::freeze_start_frame_yield_stub(frame f) {\n-  assert(StubRoutines::cont_doYield_stub()->contains(f.pc()), \"must be\");\n-  f = sender<ContinuationHelper::StubFrame>(f);\n-  return f;\n-}\n@@ -1314,9 +409,1 @@\n-frame FreezeBase::freeze_start_frame_safepoint_stub(frame f) {\n-#if (defined(X86) || defined(AARCH64)) && !defined(ZERO)\n-  f.set_fp(f.real_fp()); \/\/ f.set_fp(*Frame::callee_link_address(f)); \/\/ ????\n-#else\n-  Unimplemented();\n-#endif\n-  if (!Interpreter::contains(f.pc())) {\n-    assert(ContinuationHelper::Frame::is_stub(f.cb()), \"must be\");\n-    assert(f.oop_map() != nullptr, \"must be\");\n+void continuations_init() { Continuations::init(); }\n@@ -1324,1762 +411,2 @@\n-    if (Interpreter::contains(ContinuationHelper::StubFrame::return_pc(f))) {\n-      f = sender<ContinuationHelper::StubFrame>(f); \/\/ Safepoint stub in interpreter\n-    }\n-  }\n-  return f;\n-}\n-\n-NOINLINE freeze_result FreezeBase::freeze(frame& f, frame& caller, int callee_argsize, bool callee_interpreted, bool top) {\n-  assert(f.unextended_sp() < _bottom_address, \"\"); \/\/ see recurse_freeze_java_frame\n-  assert(f.is_interpreted_frame() || ((top && _preempt) == ContinuationHelper::Frame::is_stub(f.cb())), \"\");\n-\n-  if (stack_overflow()) {\n-    return freeze_exception;\n-  }\n-\n-  if (f.is_compiled_frame()) {\n-    if (UNLIKELY(f.oop_map() == nullptr)) {\n-      \/\/ special native frame\n-      return freeze_pinned_native;\n-    }\n-    if (UNLIKELY(ContinuationHelper::CompiledFrame::is_owning_locks(_cont.thread(), SmallRegisterMap::instance, f))) {\n-      return freeze_pinned_monitor;\n-    }\n-\n-    return recurse_freeze_compiled_frame(f, caller, callee_argsize, callee_interpreted);\n-  } else if (f.is_interpreted_frame()) {\n-    assert((_preempt && top) || !f.interpreter_frame_method()->is_native(), \"\");\n-    if (ContinuationHelper::InterpretedFrame::is_owning_locks(f)) {\n-      return freeze_pinned_monitor;\n-    }\n-    if (_preempt && top && f.interpreter_frame_method()->is_native()) {\n-      \/\/ int native entry\n-      return freeze_pinned_native;\n-    }\n-\n-    return recurse_freeze_interpreted_frame(f, caller, callee_argsize, callee_interpreted);\n-  } else if (_preempt && top && ContinuationHelper::Frame::is_stub(f.cb())) {\n-    return recurse_freeze_stub_frame(f, caller);\n-  } else {\n-    return freeze_pinned_native;\n-  }\n-}\n-\n-template<typename FKind>\n-inline freeze_result FreezeBase::recurse_freeze_java_frame(const frame& f, frame& caller, int fsize, int argsize) {\n-  assert(FKind::is_instance(f), \"\");\n-\n-  assert(fsize > 0, \"\");\n-  assert(argsize >= 0, \"\");\n-  _size += fsize;\n-  NOT_PRODUCT(_frames++;)\n-\n-  if (FKind::frame_bottom(f) >= _bottom_address - 1) { \/\/ sometimes there's space after enterSpecial\n-    return finalize_freeze(f, caller, argsize); \/\/ recursion end\n-  } else {\n-    frame senderf = sender<FKind>(f);\n-    assert(FKind::interpreted || senderf.sp() == senderf.unextended_sp(), \"\");\n-    freeze_result result = freeze(senderf, caller, argsize, FKind::interpreted, false); \/\/ recursive call\n-    return result;\n-  }\n-}\n-\n-inline void FreezeBase::before_freeze_java_frame(const frame& f, const frame& caller, int fsize, int argsize, bool bottom) {\n-  LogTarget(Trace, continuations) lt;\n-  if (lt.develop_is_enabled()) {\n-    LogStream ls(lt);\n-    ls.print_cr(\"======== FREEZING FRAME interpreted: %d bottom: %d\", f.is_interpreted_frame(), bottom);\n-    ls.print_cr(\"fsize: %d argsize: %d\", fsize, argsize);\n-    f.print_on(&ls);\n-  }\n-  assert(caller.is_interpreted_frame() == Interpreter::contains(caller.pc()), \"\");\n-}\n-\n-inline void FreezeBase::after_freeze_java_frame(const frame& hf, bool bottom) {\n-  LogTarget(Trace, continuations) lt;\n-  if (lt.develop_is_enabled()) {\n-    LogStream ls(lt);\n-    DEBUG_ONLY(hf.print_value_on(&ls, nullptr);)\n-    assert(hf.is_heap_frame(), \"should be\");\n-    DEBUG_ONLY(print_frame_layout(hf, false, &ls);)\n-    if (bottom) {\n-      ls.print_cr(\"bottom h-frame:\");\n-      hf.print_on(&ls);\n-    }\n-  }\n-}\n-\n-freeze_result FreezeBase::finalize_freeze(const frame& callee, frame& caller, int argsize) {\n-  assert(callee.is_interpreted_frame()\n-    || callee.cb()->as_nmethod()->is_osr_method()\n-    || argsize == _cont.argsize(), \"argsize: %d cont.argsize: %d\", argsize, _cont.argsize());\n-  log_develop_trace(continuations)(\"bottom: \" INTPTR_FORMAT \" count %d size: %d argsize: %d\",\n-    p2i(_bottom_address), _frames, _size << LogBytesPerWord, argsize);\n-\n-  LogTarget(Trace, continuations) lt;\n-\n-#ifdef ASSERT\n-  bool empty = _cont.is_empty();\n-  log_develop_trace(continuations)(\"empty: %d\", empty);\n-#endif\n-\n-  stackChunkOop chunk = _cont.tail();\n-\n-  assert(chunk == nullptr || (chunk->max_size() == 0) == chunk->is_empty(), \"\");\n-\n-  _size += frame::metadata_words; \/\/ for top frame's metadata\n-\n-  int overlap = 0; \/\/ the args overlap the caller -- if there is one in this chunk and is of the same kind\n-  int unextended_sp = -1;\n-  if (chunk != nullptr) {\n-    unextended_sp = chunk->sp();\n-    if (!chunk->is_empty()) {\n-      bool top_interpreted = Interpreter::contains(chunk->pc());\n-      unextended_sp = chunk->sp();\n-      if (top_interpreted) {\n-        StackChunkFrameStream<ChunkFrames::Mixed> last(chunk);\n-        unextended_sp += last.unextended_sp() - last.sp(); \/\/ can be negative (-1), often with lambda forms\n-      }\n-      if (callee.is_interpreted_frame() == top_interpreted) {\n-        overlap = argsize;\n-      }\n-    }\n-  }\n-\n-  log_develop_trace(continuations)(\"finalize _size: %d overlap: %d unextended_sp: %d\", _size, overlap, unextended_sp);\n-\n-  _size -= overlap;\n-  assert(_size >= 0, \"\");\n-\n-  assert(chunk == nullptr || chunk->is_empty()\n-          || unextended_sp == chunk->to_offset(StackChunkFrameStream<ChunkFrames::Mixed>(chunk).unextended_sp()), \"\");\n-  assert(chunk != nullptr || unextended_sp < _size, \"\");\n-\n-    \/\/ _barriers can be set to true by an allocation in freeze_fast, in which case the chunk is available\n-  assert(!_barriers || (unextended_sp >= _size && chunk->is_empty()),\n-    \"unextended_sp: %d size: %d is_empty: %d\", unextended_sp, _size, chunk->is_empty());\n-\n-  DEBUG_ONLY(bool empty_chunk = true);\n-  if (unextended_sp < _size || chunk->is_gc_mode() || (!_barriers && chunk->requires_barriers())) {\n-    \/\/ ALLOCATION\n-\n-    if (lt.develop_is_enabled()) {\n-      LogStream ls(lt);\n-      if (chunk == nullptr) {\n-        ls.print_cr(\"no chunk\");\n-      } else {\n-        ls.print_cr(\"chunk barriers: %d _size: %d free size: %d\",\n-          chunk->requires_barriers(), _size, chunk->sp() - frame::metadata_words);\n-        chunk->print_on(&ls);\n-      }\n-    }\n-\n-    _size += overlap; \/\/ we're allocating a new chunk, so no overlap\n-    \/\/ overlap = 0;\n-\n-    chunk = allocate_chunk_slow(_size);\n-    if (chunk == nullptr) {\n-      return freeze_exception;\n-    }\n-\n-    int sp = chunk->stack_size() - argsize;\n-    chunk->set_sp(sp);\n-    chunk->set_argsize(argsize);\n-    assert(chunk->is_empty(), \"\");\n-  } else {\n-    log_develop_trace(continuations)(\"Reusing chunk mixed: %d empty: %d\", chunk->has_mixed_frames(), chunk->is_empty());\n-    if (chunk->is_empty()) {\n-      int sp = chunk->stack_size() - argsize;\n-      chunk->set_sp(sp);\n-      chunk->set_argsize(argsize);\n-      _size += overlap;\n-      assert(chunk->max_size() == 0, \"\");\n-    } DEBUG_ONLY(else empty_chunk = false;)\n-  }\n-  chunk->set_has_mixed_frames(true);\n-\n-  assert(chunk->requires_barriers() == _barriers, \"\");\n-  assert(!_barriers || chunk->is_empty(), \"\");\n-\n-  assert(!chunk->has_bitmap(), \"\");\n-  assert(!chunk->is_empty() || StackChunkFrameStream<ChunkFrames::Mixed>(chunk).is_done(), \"\");\n-  assert(!chunk->is_empty() || StackChunkFrameStream<ChunkFrames::Mixed>(chunk).to_frame().is_empty(), \"\");\n-\n-  \/\/ We unwind frames after the last safepoint so that the GC will have found the oops in the frames, but before\n-  \/\/ writing into the chunk. This is so that an asynchronous stack walk (not at a safepoint) that suspends us here\n-  \/\/ will either see no continuation or a consistent chunk.\n-  unwind_frames();\n-\n-  chunk->set_max_size(chunk->max_size() + _size - frame::metadata_words);\n-\n-  if (lt.develop_is_enabled()) {\n-    LogStream ls(lt);\n-    ls.print_cr(\"top chunk:\");\n-    chunk->print_on(&ls);\n-  }\n-\n-  caller = StackChunkFrameStream<ChunkFrames::Mixed>(chunk).to_frame();\n-\n-  DEBUG_ONLY(_last_write = caller.unextended_sp() + (empty_chunk ? argsize : overlap);)\n-  assert(chunk->is_in_chunk(_last_write - _size),\n-    \"last_write-size: \" INTPTR_FORMAT \" start: \" INTPTR_FORMAT, p2i(_last_write-_size), p2i(chunk->start_address()));\n-#ifdef ASSERT\n-  if (lt.develop_is_enabled()) {\n-    LogStream ls(lt);\n-    ls.print_cr(\"top hframe before (freeze):\");\n-    assert(caller.is_heap_frame(), \"should be\");\n-    caller.print_on(&ls);\n-  }\n-\n-  assert(!empty || Continuation::is_continuation_entry_frame(callee, nullptr), \"\");\n-\n-  frame entry = sender(callee);\n-\n-  assert(Continuation::is_return_barrier_entry(entry.pc()) || Continuation::is_continuation_enterSpecial(entry), \"\");\n-  assert(callee.is_interpreted_frame() || entry.sp() == entry.unextended_sp(), \"\");\n-#endif\n-\n-  return freeze_ok_bottom;\n-}\n-\n-void FreezeBase::patch(const frame& f, frame& hf, const frame& caller, bool bottom) {\n-  if (bottom) {\n-    address last_pc = caller.pc();\n-    assert((last_pc == nullptr) == _cont.tail()->is_empty(), \"\");\n-    ContinuationHelper::Frame::patch_pc(caller, last_pc);\n-  } else {\n-    assert(!caller.is_empty(), \"\");\n-  }\n-\n-  patch_pd(hf, caller);\n-\n-  if (f.is_interpreted_frame()) {\n-    assert(hf.is_heap_frame(), \"should be\");\n-    ContinuationHelper::InterpretedFrame::patch_sender_sp(hf, caller.unextended_sp());\n-  }\n-\n-#ifdef ASSERT\n-  if (hf.is_compiled_frame()) {\n-    if (f.is_deoptimized_frame()) { \/\/ TODO DEOPT: long term solution: unroll on freeze and patch pc\n-      log_develop_trace(continuations)(\"Freezing deoptimized frame\");\n-      assert(f.cb()->as_compiled_method()->is_deopt_pc(f.raw_pc()), \"\");\n-      assert(f.cb()->as_compiled_method()->is_deopt_pc(ContinuationHelper::Frame::real_pc(f)), \"\");\n-    }\n-  }\n-#endif\n-}\n-\n-#ifdef ASSERT\n-static void verify_frame_top(const frame& f, intptr_t* top) {\n-  ResourceMark rm;\n-  InterpreterOopMap mask;\n-  f.interpreted_frame_oop_map(&mask);\n-  assert(top <= ContinuationHelper::InterpretedFrame::frame_top(f, &mask),\n-         \"frame_sp: \" INTPTR_FORMAT \" Interpreted::frame_top: \" INTPTR_FORMAT,\n-           p2i(top), p2i(ContinuationHelper::InterpretedFrame::frame_top(f, &mask)));\n-}\n-#endif \/\/ ASSERT\n-\n-NOINLINE freeze_result FreezeBase::recurse_freeze_interpreted_frame(frame& f, frame& caller,\n-                                                                    int callee_argsize,\n-                                                                    bool callee_interpreted) {\n-  adjust_interpreted_frame_unextended_sp(f);\n-\n-  intptr_t* const frame_sp = ContinuationHelper::InterpretedFrame::frame_top(f, callee_argsize, callee_interpreted);\n-  const int argsize = ContinuationHelper::InterpretedFrame::stack_argsize(f);\n-  const int locals = f.interpreter_frame_method()->max_locals();\n-  assert(ContinuationHelper::InterpretedFrame::frame_bottom(f) >= f.fp() + frame::metadata_words + locals, \"\");\/\/ = on x86\n-  const int fsize = f.fp() + frame::metadata_words + locals - frame_sp;\n-\n-  DEBUG_ONLY(verify_frame_top(f, frame_sp));\n-\n-  Method* frame_method = ContinuationHelper::Frame::frame_method(f);\n-\n-  log_develop_trace(continuations)(\"recurse_freeze_interpreted_frame %s _size: %d fsize: %d argsize: %d\",\n-    frame_method->name_and_sig_as_C_string(), _size, fsize, argsize);\n-  \/\/ we'd rather not yield inside methods annotated with @JvmtiMountTransition\n-  assert(!ContinuationHelper::Frame::frame_method(f)->jvmti_mount_transition(), \"\");\n-\n-  freeze_result result = recurse_freeze_java_frame<ContinuationHelper::InterpretedFrame>(f, caller, fsize, argsize);\n-  if (UNLIKELY(result > freeze_ok_bottom)) {\n-    return result;\n-  }\n-\n-  bool bottom = result == freeze_ok_bottom;\n-\n-  DEBUG_ONLY(before_freeze_java_frame(f, caller, fsize, 0, bottom);)\n-\n-  frame hf = new_heap_frame<ContinuationHelper::InterpretedFrame>(f, caller);\n-  _align_size += frame::align_wiggle; \/\/ add alignment room for internal interpreted frame alignment om AArch64\n-\n-  intptr_t* heap_sp = ContinuationHelper::InterpretedFrame::frame_top(hf, callee_argsize, callee_interpreted);\n-  assert(ContinuationHelper::InterpretedFrame::frame_bottom(hf) == heap_sp + fsize, \"\");\n-\n-  \/\/ on AArch64 we add padding between the locals and the rest of the frame to keep the fp 16-byte-aligned\n-  copy_to_chunk(ContinuationHelper::InterpretedFrame::frame_bottom(f) - locals,\n-                ContinuationHelper::InterpretedFrame::frame_bottom(hf) - locals, locals); \/\/ copy locals\n-  copy_to_chunk(frame_sp, heap_sp, fsize - locals); \/\/ copy rest\n-  assert(!bottom || !caller.is_interpreted_frame() || (heap_sp + fsize) == (caller.unextended_sp() + argsize), \"\");\n-\n-  relativize_interpreted_frame_metadata(f, hf);\n-\n-  patch(f, hf, caller, bottom);\n-\n-  CONT_JFR_ONLY(_cont.record_interpreted_frame();)\n-  DEBUG_ONLY(after_freeze_java_frame(hf, bottom);)\n-  caller = hf;\n-\n-  \/\/ Mark frame_method's GC epoch for class redefinition on_stack calculation.\n-  frame_method->record_gc_epoch();\n-\n-  return freeze_ok;\n-}\n-\n-freeze_result FreezeBase::recurse_freeze_compiled_frame(frame& f, frame& caller, int callee_argsize, bool callee_interpreted) {\n-  intptr_t* const frame_sp = ContinuationHelper::CompiledFrame::frame_top(f, callee_argsize, callee_interpreted);\n-  const int argsize = ContinuationHelper::CompiledFrame::stack_argsize(f);\n-  const int fsize = ContinuationHelper::CompiledFrame::frame_bottom(f) + argsize - frame_sp;\n-\n-  log_develop_trace(continuations)(\"recurse_freeze_compiled_frame %s _size: %d fsize: %d argsize: %d\",\n-                             ContinuationHelper::Frame::frame_method(f) != nullptr ?\n-                             ContinuationHelper::Frame::frame_method(f)->name_and_sig_as_C_string() : \"\",\n-                             _size, fsize, argsize);\n-  \/\/ we'd rather not yield inside methods annotated with @JvmtiMountTransition\n-  assert(!ContinuationHelper::Frame::frame_method(f)->jvmti_mount_transition(), \"\");\n-\n-  freeze_result result = recurse_freeze_java_frame<ContinuationHelper::CompiledFrame>(f, caller, fsize, argsize);\n-  if (UNLIKELY(result > freeze_ok_bottom)) {\n-    return result;\n-  }\n-\n-  bool bottom = result == freeze_ok_bottom;\n-\n-  DEBUG_ONLY(before_freeze_java_frame(f, caller, fsize, argsize, bottom);)\n-\n-  frame hf = new_heap_frame<ContinuationHelper::CompiledFrame>(f, caller);\n-\n-  intptr_t* heap_sp = ContinuationHelper::CompiledFrame::frame_top(hf, callee_argsize, callee_interpreted);\n-\n-  copy_to_chunk(frame_sp, heap_sp, fsize);\n-  assert(!bottom || !caller.is_compiled_frame() || (heap_sp + fsize) == (caller.unextended_sp() + argsize), \"\");\n-\n-  if (caller.is_interpreted_frame()) {\n-    _align_size += frame::align_wiggle; \/\/ See Thaw::align\n-  }\n-\n-  patch(f, hf, caller, bottom);\n-\n-  assert(bottom || Interpreter::contains(ContinuationHelper::CompiledFrame::real_pc(caller)) == caller.is_interpreted_frame(), \"\");\n-\n-  DEBUG_ONLY(after_freeze_java_frame(hf, bottom);)\n-  caller = hf;\n-  return freeze_ok;\n-}\n-\n-NOINLINE freeze_result FreezeBase::recurse_freeze_stub_frame(frame& f, frame& caller) {\n-  intptr_t* const frame_sp = ContinuationHelper::StubFrame::frame_top(f, 0, 0);\n-  const int fsize = f.cb()->frame_size();\n-\n-  log_develop_trace(continuations)(\"recurse_freeze_stub_frame %s _size: %d fsize: %d :: \" INTPTR_FORMAT \" - \" INTPTR_FORMAT,\n-    f.cb()->name(), _size, fsize, p2i(frame_sp), p2i(frame_sp+fsize));\n-\n-  \/\/ recurse_freeze_java_frame and freeze inlined here because we need to use a full RegisterMap for lock ownership\n-  NOT_PRODUCT(_frames++;)\n-  _size += fsize;\n-\n-  RegisterMap map(_cont.thread(), true, false, false);\n-  map.set_include_argument_oops(false);\n-  ContinuationHelper::update_register_map<ContinuationHelper::StubFrame>(f, &map);\n-  f.oop_map()->update_register_map(&f, &map); \/\/ we have callee-save registers in this case\n-  frame senderf = sender<ContinuationHelper::StubFrame>(f);\n-  assert(senderf.unextended_sp() < _bottom_address - 1, \"\");\n-  assert(senderf.is_compiled_frame(), \"\");\n-\n-  if (UNLIKELY(senderf.oop_map() == nullptr)) {\n-    \/\/ native frame\n-    return freeze_pinned_native;\n-  }\n-  if (UNLIKELY(ContinuationHelper::CompiledFrame::is_owning_locks(_cont.thread(), &map, senderf))) {\n-    return freeze_pinned_monitor;\n-  }\n-\n-  freeze_result result = recurse_freeze_compiled_frame(senderf, caller, 0, 0); \/\/ This might be deoptimized\n-  if (UNLIKELY(result > freeze_ok_bottom)) {\n-    return result;\n-  }\n-  assert(result != freeze_ok_bottom, \"\");\n-  assert(!caller.is_interpreted_frame(), \"\");\n-\n-  DEBUG_ONLY(before_freeze_java_frame(f, caller, fsize, 0, false);)\n-  frame hf = new_heap_frame<ContinuationHelper::StubFrame>(f, caller);\n-  intptr_t* heap_sp = ContinuationHelper::StubFrame::frame_top(hf, 0, 0);\n-  copy_to_chunk(frame_sp, heap_sp, fsize);\n-  DEBUG_ONLY(after_freeze_java_frame(hf, false);)\n-\n-  caller = hf;\n-  return freeze_ok;\n-}\n-\n-NOINLINE void FreezeBase::finish_freeze(const frame& f, const frame& top) {\n-  stackChunkOop chunk = _cont.tail();\n-  assert(chunk->to_offset(top.sp()) <= chunk->sp(), \"\");\n-\n-  LogTarget(Trace, continuations) lt;\n-  if (lt.develop_is_enabled()) {\n-    LogStream ls(lt);\n-    assert(top.is_heap_frame(), \"should be\");\n-    top.print_on(&ls);\n-  }\n-\n-  set_top_frame_metadata_pd(top);\n-\n-  chunk->set_sp(chunk->to_offset(top.sp()));\n-  chunk->set_pc(top.pc());\n-\n-  chunk->set_max_size(chunk->max_size() + _align_size);\n-\n-  if (UNLIKELY(_barriers)) {\n-    log_develop_trace(continuations)(\"do barriers on old chunk\");\n-    _cont.tail()->do_barriers<stackChunkOopDesc::BarrierType::Store>();\n-  }\n-\n-  log_develop_trace(continuations)(\"finish_freeze: has_mixed_frames: %d\", chunk->has_mixed_frames());\n-  if (lt.develop_is_enabled()) {\n-    LogStream ls(lt);\n-    chunk->print_on(true, &ls);\n-  }\n-\n-  if (lt.develop_is_enabled()) {\n-    LogStream ls(lt);\n-    ls.print_cr(\"top hframe after (freeze):\");\n-    assert(_cont.last_frame().is_heap_frame(), \"should be\");\n-    _cont.last_frame().print_on(&ls);\n-  }\n-\n-  assert(_cont.chunk_invariant(tty), \"\");\n-}\n-\n-inline bool FreezeBase::stack_overflow() { \/\/ detect stack overflow in recursive native code\n-  JavaThread* t = !_preempt ? _thread : JavaThread::current();\n-  assert(t == JavaThread::current(), \"\");\n-  if ((address)&t < t->stack_overflow_state()->stack_overflow_limit()) {\n-    if (!_preempt) {\n-      ContinuationWrapper::SafepointOp so(t, _cont); \/\/ could also call _cont.done() instead\n-      Exceptions::_throw_msg(t, __FILE__, __LINE__, vmSymbols::java_lang_StackOverflowError(), \"Stack overflow while freezing\");\n-    }\n-    return true;\n-  }\n-  return false;\n-}\n-\n-template <typename ConfigT>\n-stackChunkOop Freeze<ConfigT>::allocate_chunk(size_t stack_size) {\n-  log_develop_trace(continuations)(\"allocate_chunk allocating new chunk\");\n-\n-  InstanceStackChunkKlass* klass = InstanceStackChunkKlass::cast(vmClasses::StackChunk_klass());\n-  size_t size_in_words = klass->instance_size(stack_size);\n-\n-  if (CollectedHeap::stack_chunk_max_size() > 0 && size_in_words >= CollectedHeap::stack_chunk_max_size()) {\n-    if (!_preempt) {\n-      throw_stack_overflow_on_humongous_chunk();\n-    }\n-    return nullptr;\n-  }\n-\n-  JavaThread* current = _preempt ? JavaThread::current() : _thread;\n-  assert(current == JavaThread::current(), \"should be current\");\n-\n-  stackChunkOop chunk;\n-  StackChunkAllocator allocator(klass, size_in_words, stack_size, current);\n-  HeapWord* start = current->tlab().allocate(size_in_words);\n-  if (start != nullptr) {\n-    chunk = stackChunkOopDesc::cast(allocator.StackChunkAllocator::initialize(start));\n-  } else {\n-    ContinuationWrapper::SafepointOp so(current, _cont);\n-    assert(_jvmti_event_collector != nullptr, \"\");\n-    _jvmti_event_collector->start(); \/\/ can safepoint\n-\n-    chunk = stackChunkOopDesc::cast(allocator.allocate()); \/\/ can safepoint\n-\n-    if (chunk == nullptr) { \/\/ OOME\n-      return nullptr;\n-    }\n-  }\n-\n-  \/\/ assert that chunk is properly initialized\n-  assert(chunk->stack_size() == (int)stack_size, \"\");\n-  assert(chunk->size() >= stack_size, \"chunk->size(): %zu size: %zu\", chunk->size(), stack_size);\n-  assert(chunk->sp() == chunk->stack_size(), \"\");\n-  assert((intptr_t)chunk->start_address() % 8 == 0, \"\");\n-  assert(chunk->max_size() == 0, \"\");\n-  assert(chunk->pc() == nullptr, \"\");\n-  assert(chunk->argsize() == 0, \"\");\n-  assert(chunk->flags() == 0, \"\");\n-  assert(chunk->is_gc_mode() == false, \"\");\n-\n-  chunk->set_mark(chunk->mark().set_age(15)); \/\/ Promote young chunks quickly\n-\n-  stackChunkOop chunk0 = _cont.tail();\n-  if (chunk0 != nullptr && chunk0->is_empty()) {\n-    chunk0 = chunk0->parent();\n-    assert(chunk0 == nullptr || !chunk0->is_empty(), \"\");\n-  }\n-  \/\/ fields are uninitialized\n-  chunk->set_parent_raw<typename ConfigT::OopT>(chunk0);\n-  chunk->set_cont_raw<typename ConfigT::OopT>(_cont.continuation());\n-\n-  assert(chunk->parent() == nullptr || chunk->parent()->is_stackChunk(), \"\");\n-\n-  if (start != nullptr) {\n-    assert(!chunk->requires_barriers(), \"Unfamiliar GC requires barriers on TLAB allocation\");\n-  } else {\n-    assert(!UseZGC || !chunk->requires_barriers(), \"Allocated ZGC object requires barriers\");\n-    _barriers = !UseZGC && chunk->requires_barriers();\n-\n-    if (_barriers) {\n-      log_develop_trace(continuations)(\"allocation requires barriers\");\n-    }\n-  }\n-\n-  _cont.set_tail(chunk);\n-  return chunk;\n-}\n-\n-void FreezeBase::throw_stack_overflow_on_humongous_chunk() {\n-  ContinuationWrapper::SafepointOp so(_thread, _cont); \/\/ could also call _cont.done() instead\n-  Exceptions::_throw_msg(_thread, __FILE__, __LINE__, vmSymbols::java_lang_StackOverflowError(), \"Humongous stack chunk\");\n-}\n-\n-#if INCLUDE_JVMTI\n-static int num_java_frames(ContinuationWrapper& cont) {\n-  ResourceMark rm; \/\/ used for scope traversal in num_java_frames(CompiledMethod*, address)\n-  int count = 0;\n-  for (stackChunkOop chunk = cont.tail(); chunk != nullptr; chunk = chunk->parent()) {\n-    count += chunk->num_java_frames();\n-  }\n-  return count;\n-}\n-\n-static void invalidate_jvmti_stack(JavaThread* thread) {\n-  if (thread->is_interp_only_mode()) {\n-    JvmtiThreadState *state = thread->jvmti_thread_state();\n-    if (state != nullptr)\n-      state->invalidate_cur_stack_depth();\n-  }\n-}\n-\n-static void jvmti_yield_cleanup(JavaThread* thread, ContinuationWrapper& cont) {\n-  if (JvmtiExport::can_post_frame_pop()) {\n-    int num_frames = num_java_frames(cont);\n-\n-    ContinuationWrapper::SafepointOp so(Thread::current(), cont);\n-    JvmtiExport::continuation_yield_cleanup(JavaThread::current(), num_frames);\n-  }\n-  invalidate_jvmti_stack(thread);\n-}\n-#endif \/\/ INCLUDE_JVMTI\n-\n-static freeze_result is_pinned(const frame& f, RegisterMap* map) {\n-  if (f.is_interpreted_frame()) {\n-    if (ContinuationHelper::InterpretedFrame::is_owning_locks(f)) {\n-      return freeze_pinned_monitor;\n-    }\n-    if (f.interpreter_frame_method()->is_native()) {\n-      return freeze_pinned_native; \/\/ interpreter native entry\n-    }\n-  } else if (f.is_compiled_frame()) {\n-    if (ContinuationHelper::CompiledFrame::is_owning_locks(map->thread(), map, f)) {\n-      return freeze_pinned_monitor;\n-    }\n-  } else {\n-    return freeze_pinned_native;\n-  }\n-  return freeze_ok;\n-}\n-\n-#ifdef ASSERT\n-static bool monitors_on_stack(JavaThread* thread) {\n-  ContinuationEntry* ce = thread->last_continuation();\n-  RegisterMap map(thread, true, false, false);\n-  map.set_include_argument_oops(false);\n-  for (frame f = thread->last_frame(); Continuation::is_frame_in_continuation(ce, f); f = f.sender(&map)) {\n-    if (is_pinned(f, &map) == freeze_pinned_monitor) {\n-      return true;\n-    }\n-  }\n-  return false;\n-}\n-\n-static bool interpreted_native_or_deoptimized_on_stack(JavaThread* thread) {\n-  ContinuationEntry* ce = thread->last_continuation();\n-  RegisterMap map(thread, false, false, false);\n-  map.set_include_argument_oops(false);\n-  for (frame f = thread->last_frame(); Continuation::is_frame_in_continuation(ce, f); f = f.sender(&map)) {\n-    if (f.is_interpreted_frame() || f.is_native_frame() || f.is_deoptimized_frame()) {\n-      return true;\n-    }\n-  }\n-  return false;\n-}\n-#endif \/\/ ASSERT\n-\n-static inline bool can_freeze_fast(JavaThread* thread) {\n-  \/\/ There are no interpreted frames if we're not called from the interpreter and we haven't ancountered an i2c adapter or called Deoptimization::unpack_frames\n-  \/\/ Calls from native frames also go through the interpreter (see JavaCalls::call_helper)\n-  assert(!thread->cont_fastpath()\n-         || (thread->cont_fastpath_thread_state() && !interpreted_native_or_deoptimized_on_stack(thread)), \"\");\n-\n-  \/\/ We also clear thread->cont_fastpath on deoptimization (notify_deopt) and when we thaw interpreted frames\n-  bool fast = thread->cont_fastpath() && UseContinuationFastPath;\n-  assert(!fast || monitors_on_stack(thread) == (thread->held_monitor_count() > 0), \"\");\n-  fast = fast && thread->held_monitor_count() == 0;\n-  return fast;\n-}\n-\n-static inline int freeze_epilog(JavaThread* thread, ContinuationWrapper& cont) {\n-  verify_continuation(cont.continuation());\n-  assert(!cont.is_empty(), \"\");\n-\n-  log_develop_debug(continuations)(\"=== End of freeze cont ### #\" INTPTR_FORMAT, cont.hash());\n-\n-  return 0;\n-}\n-\n-static int freeze_epilog(JavaThread* thread, ContinuationWrapper& cont, freeze_result res) {\n-  if (UNLIKELY(res != freeze_ok)) {\n-    verify_continuation(cont.continuation());\n-    log_develop_trace(continuations)(\"=== end of freeze (fail %d)\", res);\n-    return res;\n-  }\n-\n-  JVMTI_ONLY(jvmti_yield_cleanup(thread, cont)); \/\/ can safepoint\n-  return freeze_epilog(thread, cont);\n-}\n-\n-template<typename ConfigT>\n-static inline int freeze_internal(JavaThread* current, intptr_t* const sp) {\n-  assert(!current->has_pending_exception(), \"\");\n-\n-#ifdef ASSERT\n-  log_trace(continuations)(\"~~~~ freeze sp: \" INTPTR_FORMAT, p2i(current->last_continuation()->entry_sp()));\n-  log_frames(current);\n-#endif\n-\n-  CONT_JFR_ONLY(EventContinuationFreeze event;)\n-\n-  ContinuationEntry* entry = current->last_continuation();\n-\n-  oop oopCont = get_continuation(current);\n-  assert(oopCont == current->last_continuation()->cont_oop(), \"\");\n-  assert(ContinuationEntry::assert_entry_frame_laid_out(current), \"\");\n-\n-  verify_continuation(oopCont);\n-  ContinuationWrapper cont(current, oopCont);\n-  log_develop_debug(continuations)(\"FREEZE #\" INTPTR_FORMAT \" \" INTPTR_FORMAT, cont.hash(), p2i((oopDesc*)oopCont));\n-\n-  assert(entry->is_virtual_thread() == (entry->scope() == java_lang_VirtualThread::vthread_scope()), \"\");\n-\n-  if (entry->is_pinned()) {\n-    log_develop_debug(continuations)(\"PINNED due to critical section\");\n-    verify_continuation(cont.continuation());\n-    log_develop_trace(continuations)(\"=== end of freeze (fail %d)\", freeze_pinned_cs);\n-    return freeze_pinned_cs;\n-  }\n-\n-  Freeze<ConfigT> fr(current, cont, false);\n-\n-  bool fast = can_freeze_fast(current);\n-  if (fast && fr.is_chunk_available(sp)) {\n-    freeze_result res = fr.template try_freeze_fast<true>(sp);\n-    assert(res == freeze_ok, \"\");\n-    CONT_JFR_ONLY(cont.post_jfr_event(&event, current);)\n-    freeze_epilog(current, cont);\n-    StackWatermarkSet::after_unwind(current);\n-    return 0;\n-  }\n-\n-  log_develop_trace(continuations)(\"chunk unavailable; transitioning to VM\");\n-  assert(current == JavaThread::current(), \"must be current thread except for preempt\");\n-  JRT_BLOCK\n-    \/\/ delays a possible JvmtiSampledObjectAllocEventCollector in alloc_chunk\n-    JvmtiSampledObjectAllocEventCollector jsoaec(false);\n-    fr.set_jvmti_event_collector(&jsoaec);\n-\n-    freeze_result res = fast ? fr.template try_freeze_fast<false>(sp)\n-                             : fr.freeze_slow();\n-    CONT_JFR_ONLY(cont.post_jfr_event(&event, current);)\n-    freeze_epilog(current, cont, res);\n-    cont.done(); \/\/ allow safepoint in the transition back to Java\n-    StackWatermarkSet::after_unwind(current);\n-    return res;\n-  JRT_BLOCK_END\n-}\n-\n-static freeze_result is_pinned0(JavaThread* thread, oop cont_scope, bool safepoint) {\n-  ContinuationEntry* entry = thread->last_continuation();\n-  if (entry == nullptr) {\n-    return freeze_ok;\n-  }\n-  if (entry->is_pinned()) {\n-    return freeze_pinned_cs;\n-  }\n-\n-  RegisterMap map(thread, true, false, false);\n-  map.set_include_argument_oops(false);\n-  frame f = thread->last_frame();\n-\n-  if (!safepoint) {\n-    f = f.sender(&map); \/\/ this is the yield frame\n-  } else { \/\/ safepoint yield\n-#if (defined(X86) || defined(AARCH64)) && !defined(ZERO)\n-    f.set_fp(f.real_fp()); \/\/ Instead of this, maybe in ContinuationWrapper::set_last_frame always use the real_fp?\n-#else\n-    Unimplemented();\n-#endif\n-    if (!Interpreter::contains(f.pc())) {\n-      assert(ContinuationHelper::Frame::is_stub(f.cb()), \"must be\");\n-      assert(f.oop_map() != nullptr, \"must be\");\n-      f.oop_map()->update_register_map(&f, &map); \/\/ we have callee-save registers in this case\n-    }\n-  }\n-\n-  while (true) {\n-    freeze_result res = is_pinned(f, &map);\n-    if (res != freeze_ok) {\n-      return res;\n-    }\n-\n-    f = f.sender(&map);\n-    if (!Continuation::is_frame_in_continuation(entry, f)) {\n-      oop scope = jdk_internal_vm_Continuation::scope(entry->cont_oop());\n-      if (scope == cont_scope) {\n-        break;\n-      }\n-      entry = entry->parent();\n-      if (entry == nullptr) {\n-        break;\n-      }\n-      if (entry->is_pinned()) {\n-        return freeze_pinned_cs;\n-      }\n-    }\n-  }\n-  return freeze_ok;\n-}\n-\n-\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/ THAW \/\/\/\/\n-\n-static int thaw_size(stackChunkOop chunk) {\n-  int size = chunk->max_size();\n-  size += frame::metadata_words; \/\/ For the top pc+fp in push_return_frame or top = stack_sp - frame::metadata_words in thaw_fast\n-  size += 2*frame::align_wiggle; \/\/ in case of alignments at the top and bottom\n-  return size + 200;\n-}\n-\n-\/\/ make room on the stack for thaw\n-\/\/ returns the size in bytes, or 0 on failure\n-static inline int prepare_thaw_internal(JavaThread* thread, bool return_barrier) {\n-  log_develop_trace(continuations)(\"~~~~ prepare_thaw return_barrier: %d\", return_barrier);\n-\n-  assert(thread == JavaThread::current(), \"\");\n-\n-  ContinuationEntry* ce = thread->last_continuation();\n-  assert(ce != nullptr, \"\");\n-  oop continuation = ce->cont_oop();\n-  assert(continuation == get_continuation(thread), \"\");\n-  verify_continuation(continuation);\n-\n-  stackChunkOop chunk = jdk_internal_vm_Continuation::tail(continuation);\n-  assert(chunk != nullptr, \"\");\n-\n-  \/\/ The tail can be empty because it might still be available for another freeze.\n-  \/\/ However, here we want to thaw, so we get rid of it (it will be GCed).\n-  if (UNLIKELY(chunk->is_empty())) {\n-    chunk = chunk->parent();\n-    assert(chunk != nullptr, \"\");\n-    assert(!chunk->is_empty(), \"\");\n-    jdk_internal_vm_Continuation::set_tail(continuation, chunk);\n-  }\n-\n-  \/\/ Verification\n-  chunk->verify();\n-  assert(chunk->max_size() > 0, \"chunk invariant violated; expected to not be empty\");\n-\n-  \/\/ Only make space for the last chunk because we only thaw from the last chunk\n-  int size = thaw_size(chunk) << LogBytesPerWord;\n-\n-  const address bottom = (address)thread->last_continuation()->entry_sp();\n-  \/\/ 300 is an estimate for stack size taken for this native code, in addition to StackShadowPages\n-  \/\/ for the Java frames in the check below.\n-  if (!stack_overflow_check(thread, size + 300, bottom)) {\n-    return 0;\n-  }\n-\n-  log_develop_trace(continuations)(\"prepare_thaw bottom: \" INTPTR_FORMAT \" top: \" INTPTR_FORMAT \" size: %d\",\n-                              p2i(bottom), p2i(bottom - size), size);\n-  return size;\n-}\n-\n-class ThawBase : public StackObj {\n-protected:\n-  JavaThread* _thread;\n-  ContinuationWrapper& _cont;\n-\n-  intptr_t* _fastpath;\n-  bool _barriers;\n-  intptr_t* _top_unextended_sp;\n-  int _align_size;\n-  DEBUG_ONLY(intptr_t* _top_stack_address);\n-\n-  StackChunkFrameStream<ChunkFrames::Mixed> _stream;\n-\n-  NOT_PRODUCT(int _frames;)\n-\n-#ifdef ASSERT\n-  public:\n-    bool barriers() { return _barriers; }\n-  protected:\n-#endif\n-\n-protected:\n-  ThawBase(JavaThread* thread, ContinuationWrapper& cont) :\n-      _thread(thread), _cont(cont),\n-      _fastpath(nullptr) {\n-    DEBUG_ONLY(_top_unextended_sp = nullptr;)\n-    assert (cont.tail() != nullptr, \"no last chunk\");\n-    DEBUG_ONLY(_top_stack_address = _cont.entrySP() - thaw_size(cont.tail());)\n-  }\n-\n-  void copy_from_chunk(intptr_t* from, intptr_t* to, int size);\n-\n-  \/\/ fast path\n-  inline void prefetch_chunk_pd(void* start, int size_words);\n-  void patch_return(intptr_t* sp, bool is_last);\n-  void patch_chunk_pd(intptr_t* sp); \/\/ TODO remove\n-\n-  \/\/ slow path\n-  NOINLINE intptr_t* thaw_slow(stackChunkOop chunk, bool return_barrier);\n-\n-private:\n-  void thaw_one_frame(const frame& heap_frame, frame& caller, int num_frames, bool top);\n-  template<typename FKind> bool recurse_thaw_java_frame(frame& caller, int num_frames);\n-  void finalize_thaw(frame& entry, int argsize);\n-\n-  inline void before_thaw_java_frame(const frame& hf, const frame& caller, bool bottom, int num_frame);\n-  inline void after_thaw_java_frame(const frame& f, bool bottom);\n-  inline void patch(frame& f, const frame& caller, bool bottom);\n-  void clear_bitmap_bits(intptr_t* start, int range);\n-\n-  NOINLINE void recurse_thaw_interpreted_frame(const frame& hf, frame& caller, int num_frames);\n-  void recurse_thaw_compiled_frame(const frame& hf, frame& caller, int num_frames, bool stub_caller);\n-  void recurse_thaw_stub_frame(const frame& hf, frame& caller, int num_frames);\n-  void finish_thaw(frame& f);\n-\n-  void push_return_frame(frame& f);\n-  inline frame new_entry_frame();\n-  template<typename FKind> frame new_stack_frame(const frame& hf, frame& caller, bool bottom);\n-  inline void patch_pd(frame& f, const frame& sender);\n-  inline intptr_t* align(const frame& hf, intptr_t* frame_sp, frame& caller, bool bottom);\n-\n-  void maybe_set_fastpath(intptr_t* sp) { if (sp > _fastpath) _fastpath = sp; }\n-\n-  static inline void derelativize_interpreted_frame_metadata(const frame& hf, const frame& f);\n-  static inline void set_interpreter_frame_bottom(const frame& f, intptr_t* bottom);\n-};\n-\n-template <typename ConfigT>\n-class Thaw : public ThawBase {\n-public:\n-  Thaw(JavaThread* thread, ContinuationWrapper& cont) : ThawBase(thread, cont) {}\n-\n-  inline bool can_thaw_fast(stackChunkOop chunk) {\n-    return    !_barriers\n-           &&  _thread->cont_fastpath_thread_state()\n-           && !chunk->has_thaw_slowpath_condition()\n-           && !PreserveFramePointer;\n-  }\n-\n-  inline intptr_t* thaw(thaw_kind kind);\n-  NOINLINE intptr_t* thaw_fast(stackChunkOop chunk);\n-};\n-\n-template <typename ConfigT>\n-inline intptr_t* Thaw<ConfigT>::thaw(thaw_kind kind) {\n-  verify_continuation(_cont.continuation());\n-  assert(!jdk_internal_vm_Continuation::done(_cont.continuation()), \"\");\n-  assert(!_cont.is_empty(), \"\");\n-\n-  stackChunkOop chunk = _cont.tail();\n-  assert(chunk != nullptr, \"guaranteed by prepare_thaw\");\n-  assert(!chunk->is_empty(), \"guaranteed by prepare_thaw\");\n-\n-  _barriers = chunk->requires_barriers();\n-  return (LIKELY(can_thaw_fast(chunk))) ? thaw_fast(chunk)\n-                                        : thaw_slow(chunk, kind != thaw_top);\n-}\n-\n-template <typename ConfigT>\n-NOINLINE intptr_t* Thaw<ConfigT>::thaw_fast(stackChunkOop chunk) {\n-  assert(chunk == _cont.tail(), \"\");\n-  assert(!chunk->has_mixed_frames(), \"\");\n-  assert(!chunk->requires_barriers(), \"\");\n-  assert(!chunk->has_bitmap(), \"\");\n-  assert(!_thread->is_interp_only_mode(), \"\");\n-\n-  LogTarget(Trace, continuations) lt;\n-  if (lt.develop_is_enabled()) {\n-    LogStream ls(lt);\n-    ls.print_cr(\"thaw_fast\");\n-    chunk->print_on(true, &ls);\n-  }\n-\n-  \/\/ Below this heuristic, we thaw the whole chunk, above it we thaw just one frame.\n-  static const int threshold = 500; \/\/ words\n-\n-  int chunk_start_sp = chunk->sp();\n-  const int full_chunk_size = chunk->stack_size() - chunk_start_sp; \/\/ this initial size could be reduced if it's a partial thaw\n-  int argsize, thaw_size;\n-\n-  intptr_t* const chunk_sp = chunk->start_address() + chunk_start_sp;\n-\n-  bool partial, empty;\n-  if (LIKELY(!TEST_THAW_ONE_CHUNK_FRAME && (full_chunk_size < threshold))) {\n-    prefetch_chunk_pd(chunk->start_address(), full_chunk_size); \/\/ prefetch anticipating memcpy starting at highest address\n-\n-    partial = false;\n-\n-    argsize = chunk->argsize();\n-    empty = true;\n-\n-    chunk->set_sp(chunk->stack_size());\n-    chunk->set_argsize(0);\n-    chunk->set_max_size(0);\n-\n-    thaw_size = full_chunk_size;\n-  } else { \/\/ thaw a single frame\n-    partial = true;\n-\n-    StackChunkFrameStream<ChunkFrames::CompiledOnly> f(chunk);\n-    assert(chunk_sp == f.sp(), \"\");\n-    assert(chunk_sp == f.unextended_sp(), \"\");\n-\n-    const int frame_size = f.cb()->frame_size();\n-    argsize = f.stack_argsize();\n-\n-    f.next(SmallRegisterMap::instance, true \/* stop *\/);\n-    empty = f.is_done();\n-    assert(!empty || argsize == chunk->argsize(), \"\");\n-\n-    if (empty) {\n-      chunk->set_sp(chunk->stack_size());\n-      chunk->set_argsize(0);\n-      chunk->set_max_size(0);\n-    } else {\n-      chunk->set_sp(chunk->sp() + frame_size);\n-      chunk->set_max_size(chunk->max_size() - frame_size);\n-      \/\/ We set chunk->pc to the return pc into the next frame\n-      chunk->set_pc(f.pc());\n-      assert(f.pc() == *(address*)(chunk_sp + frame_size - frame::sender_sp_ret_address_offset()), \"unexpected pc\");\n-    }\n-    assert(empty == chunk->is_empty(), \"\");\n-    thaw_size = frame_size + argsize;\n-  }\n-\n-  \/\/ Are we thawing the last frame(s) in the continuation\n-  const bool is_last = empty && chunk->is_parent_null<typename ConfigT::OopT>();\n-\n-  log_develop_trace(continuations)(\"thaw_fast partial: %d is_last: %d empty: %d size: %d argsize: %d\",\n-                              partial, is_last, empty, thaw_size, argsize);\n-\n-  intptr_t* stack_sp = _cont.entrySP();\n-  intptr_t* bottom_sp = ContinuationHelper::frame_align_pointer(stack_sp - argsize);\n-\n-  stack_sp -= thaw_size;\n-  \/\/ possibly adds a one-word padding between entrySP and the bottom-most frame's stack args\n-  \/\/ The only possible source of misalignment is stack-passed arguments because all compiled\n-  \/\/ frames are 16-byte aligned.\n-  assert(argsize != 0 || stack_sp == ContinuationHelper::frame_align_pointer(stack_sp), \"\");\n-  stack_sp = ContinuationHelper::frame_align_pointer(stack_sp);\n-\n-  \/\/ also copy metadata words\n-  intptr_t* from = chunk_sp - frame::metadata_words;\n-  intptr_t* to   = stack_sp - frame::metadata_words;\n-  copy_from_chunk(from, to, thaw_size + frame::metadata_words);\n-  \/\/ We assert we have not overwritten the entry frame, but that we're at most\n-  \/\/ one alignment word away from it.\n-  assert(to + thaw_size + frame::metadata_words <= _cont.entrySP(), \"overwritten entry frame\");\n-  assert(_cont.entrySP() - 1 <= to + thaw_size + frame::metadata_words, \"missed entry frame\");\n-  assert(argsize != 0 || to + thaw_size + frame::metadata_words == _cont.entrySP(), \"missed entry frame\");\n-\n-  assert(!is_last || argsize == 0, \"\");\n-  _cont.set_argsize(argsize); \/\/ sets argsize in ContinuationEntry\n-  log_develop_trace(continuations)(\"setting entry argsize: %d\", _cont.argsize());\n-  assert(bottom_sp == _cont.entry()->bottom_sender_sp(), \"\");\n-\n-  \/\/ install the return barrier if not last frame, or the entry's pc if last\n-  patch_return(bottom_sp, is_last);\n-  DEBUG_ONLY(address pc = *(address*)(bottom_sp - frame::sender_sp_ret_address_offset());)\n-  assert(is_last ? CodeCache::find_blob(pc)->as_compiled_method()->method()->is_continuation_enter_intrinsic()\n-                  : pc == StubRoutines::cont_returnBarrier(), \"is_last: %d\", is_last);\n-  assert(is_last == _cont.is_empty(), \"\");\n-  assert(_cont.chunk_invariant(tty), \"\");\n-\n-#if CONT_JFR\n-  EventContinuationThawYoung e;\n-  if (e.should_commit()) {\n-    e.set_id(cast_from_oop<u8>(chunk));\n-    e.set_size(size << LogBytesPerWord);\n-    e.set_full(!partial);\n-    e.commit();\n-  }\n-#endif\n-\n-#ifdef ASSERT\n-  set_anchor(_thread, stack_sp);\n-  log_frames(_thread);\n-  if (LoomDeoptAfterThaw) {\n-    do_deopt_after_thaw(_thread);\n-  }\n-  clear_anchor(_thread);\n-#endif\n-\n-  return stack_sp;\n-}\n-\n-void ThawBase::copy_from_chunk(intptr_t* from, intptr_t* to, int size) {\n-  assert(to + size <= _cont.entrySP(), \"\");\n-  _cont.tail()->copy_from_chunk_to_stack(from, to, size);\n-  CONT_JFR_ONLY(_cont.record_size_copied(size);)\n-  assert(to >= _top_stack_address, \"overwrote past thawing space\"\n-    \" to: \" INTPTR_FORMAT \" top_address: \" INTPTR_FORMAT, p2i(to), p2i(_top_stack_address));\n-}\n-\n-void ThawBase::patch_return(intptr_t* sp, bool is_last) {\n-  log_develop_trace(continuations)(\"thaw_fast patching -- sp: \" INTPTR_FORMAT, p2i(sp));\n-\n-  address pc = !is_last ? StubRoutines::cont_returnBarrier() : _cont.entryPC();\n-  *(address*)(sp - frame::sender_sp_ret_address_offset()) = pc;\n-  \/\/ patch_chunk_pd(sp); -- TODO: If not needed - remove method; it's not used elsewhere\n-}\n-\n-NOINLINE intptr_t* ThawBase::thaw_slow(stackChunkOop chunk, bool return_barrier) {\n-  LogTarget(Trace, continuations) lt;\n-  if (lt.develop_is_enabled()) {\n-    LogStream ls(lt);\n-    ls.print_cr(\"thaw slow return_barrier: %d \" INTPTR_FORMAT, return_barrier, p2i(chunk));\n-    chunk->print_on(true, &ls);\n-  }\n-\n-  \/\/ Does this need ifdef JFR around it? Or can we remove all the conditional JFR inclusions (better)?\n-  EventContinuationThawOld e;\n-  if (e.should_commit()) {\n-    e.set_id(cast_from_oop<u8>(_cont.continuation()));\n-    e.commit();\n-  }\n-\n-  DEBUG_ONLY(_frames = 0;)\n-  _align_size = 0;\n-  int num_frames = (return_barrier ? 1 : 2);\n-  bool last_interpreted = chunk->has_mixed_frames() && Interpreter::contains(chunk->pc());\n-\n-  _stream = StackChunkFrameStream<ChunkFrames::Mixed>(chunk);\n-  _top_unextended_sp = _stream.unextended_sp();\n-\n-  frame heap_frame = _stream.to_frame();\n-  if (lt.develop_is_enabled()) {\n-    LogStream ls(lt);\n-    ls.print_cr(\"top hframe before (thaw):\");\n-    assert(heap_frame.is_heap_frame(), \"should have created a relative frame\");\n-    heap_frame.print_value_on(&ls, nullptr);\n-  }\n-\n-  frame caller;\n-  thaw_one_frame(heap_frame, caller, num_frames, true);\n-  finish_thaw(caller); \/\/ caller is now the topmost thawed frame\n-  _cont.write();\n-\n-  assert(_cont.chunk_invariant(tty), \"\");\n-\n-  JVMTI_ONLY(if (!return_barrier) invalidate_jvmti_stack(_thread));\n-\n-  _thread->set_cont_fastpath(_fastpath);\n-\n-  intptr_t* sp = caller.sp();\n-  return sp;\n-}\n-\n-void ThawBase::thaw_one_frame(const frame& heap_frame, frame& caller, int num_frames, bool top) {\n-  log_develop_debug(continuations)(\"thaw num_frames: %d\", num_frames);\n-  assert(!_cont.is_empty(), \"no more frames\");\n-  assert(num_frames > 0, \"\");\n-  assert(!heap_frame.is_empty(), \"\");\n-\n-  if (top && heap_frame.is_safepoint_blob_frame()) {\n-    assert(ContinuationHelper::Frame::is_stub(heap_frame.cb()), \"cb: %s\", heap_frame.cb()->name());\n-    recurse_thaw_stub_frame(heap_frame, caller, num_frames);\n-  } else if (!heap_frame.is_interpreted_frame()) {\n-    recurse_thaw_compiled_frame(heap_frame, caller, num_frames, false);\n-  } else {\n-    recurse_thaw_interpreted_frame(heap_frame, caller, num_frames);\n-  }\n-}\n-\n-template<typename FKind>\n-bool ThawBase::recurse_thaw_java_frame(frame& caller, int num_frames) {\n-  assert(num_frames > 0, \"\");\n-\n-  DEBUG_ONLY(_frames++;)\n-\n-  int argsize = _stream.stack_argsize();\n-\n-  _stream.next(SmallRegisterMap::instance);\n-  assert(_stream.to_frame().is_empty() == _stream.is_done(), \"\");\n-\n-  \/\/ we never leave a compiled caller of an interpreted frame as the top frame in the chunk\n-  \/\/ as it makes detecting that situation and adjusting unextended_sp tricky\n-  if (num_frames == 1 && !_stream.is_done() && FKind::interpreted && _stream.is_compiled()) {\n-    log_develop_trace(continuations)(\"thawing extra compiled frame to not leave a compiled interpreted-caller at top\");\n-    num_frames++;\n-  }\n-\n-  if (num_frames == 1 || _stream.is_done()) { \/\/ end recursion\n-    finalize_thaw(caller, FKind::interpreted ? 0 : argsize);\n-    return true; \/\/ bottom\n-  } else { \/\/ recurse\n-    thaw_one_frame(_stream.to_frame(), caller, num_frames - 1, false);\n-    return false;\n-  }\n-}\n-\n-void ThawBase::finalize_thaw(frame& entry, int argsize) {\n-  stackChunkOop chunk = _cont.tail();\n-\n-  if (!_stream.is_done()) {\n-    assert(_stream.sp() >= chunk->sp_address(), \"\");\n-    chunk->set_sp(chunk->to_offset(_stream.sp()));\n-    chunk->set_pc(_stream.pc());\n-  } else {\n-    chunk->set_argsize(0);\n-    chunk->set_sp(chunk->stack_size());\n-    chunk->set_pc(nullptr);\n-  }\n-  assert(_stream.is_done() == chunk->is_empty(), \"\");\n-\n-  int delta = _stream.unextended_sp() - _top_unextended_sp;\n-  chunk->set_max_size(chunk->max_size() - delta);\n-\n-  _cont.set_argsize(argsize);\n-  entry = new_entry_frame();\n-\n-  assert(entry.sp() == _cont.entrySP(), \"\");\n-  assert(Continuation::is_continuation_enterSpecial(entry), \"\");\n-  assert(_cont.is_entry_frame(entry), \"\");\n-}\n-\n-inline void ThawBase::before_thaw_java_frame(const frame& hf, const frame& caller, bool bottom, int num_frame) {\n-  LogTarget(Trace, continuations) lt;\n-  if (lt.develop_is_enabled()) {\n-    LogStream ls(lt);\n-    ls.print_cr(\"======== THAWING FRAME: %d\", num_frame);\n-    assert(hf.is_heap_frame(), \"should be\");\n-    hf.print_value_on(&ls, nullptr);\n-  }\n-  assert(bottom == _cont.is_entry_frame(caller), \"bottom: %d is_entry_frame: %d\", bottom, _cont.is_entry_frame(hf));\n-}\n-\n-inline void ThawBase::after_thaw_java_frame(const frame& f, bool bottom) {\n-#ifdef ASSERT\n-  LogTarget(Trace, continuations) lt;\n-  if (lt.develop_is_enabled()) {\n-    LogStream ls(lt);\n-    ls.print_cr(\"thawed frame:\");\n-    print_frame_layout(f, false, &ls); \/\/ f.print_on(&ls);\n-  }\n-#endif\n-}\n-\n-inline void ThawBase::patch(frame& f, const frame& caller, bool bottom) {\n-  assert(!bottom || caller.fp() == _cont.entryFP(), \"\");\n-  if (bottom) {\n-    ContinuationHelper::Frame::patch_pc(caller, _cont.is_empty() ? caller.raw_pc()\n-                                                                 : StubRoutines::cont_returnBarrier());\n-  }\n-\n-  patch_pd(f, caller);\n-\n-  if (f.is_interpreted_frame()) {\n-    ContinuationHelper::InterpretedFrame::patch_sender_sp(f, caller.unextended_sp());\n-  }\n-\n-  assert(!bottom || !_cont.is_empty() || Continuation::is_continuation_entry_frame(f, nullptr), \"\");\n-  assert(!bottom || (_cont.is_empty() != Continuation::is_cont_barrier_frame(f)), \"\");\n-}\n-\n-void ThawBase::clear_bitmap_bits(intptr_t* start, int range) {\n-  \/\/ we need to clear the bits that correspond to arguments as they reside in the caller frame\n-  log_develop_trace(continuations)(\"clearing bitmap for \" INTPTR_FORMAT \" - \" INTPTR_FORMAT, p2i(start), p2i(start+range));\n-  stackChunkOop chunk = _cont.tail();\n-  chunk->bitmap().clear_range(chunk->bit_index_for(start),\n-                              chunk->bit_index_for(start+range));\n-}\n-\n-NOINLINE void ThawBase::recurse_thaw_interpreted_frame(const frame& hf, frame& caller, int num_frames) {\n-  assert(hf.is_interpreted_frame(), \"\");\n-\n-  if (UNLIKELY(_barriers)) {\n-    _cont.tail()->do_barriers<stackChunkOopDesc::BarrierType::Store>(_stream, SmallRegisterMap::instance);\n-  }\n-\n-  const bool bottom = recurse_thaw_java_frame<ContinuationHelper::InterpretedFrame>(caller, num_frames);\n-\n-  DEBUG_ONLY(before_thaw_java_frame(hf, caller, bottom, num_frames);)\n-\n-  _align_size += frame::align_wiggle; \/\/ remove the added alignment room for internal interpreted frame alignment om AArch64\n-\n-  frame f = new_stack_frame<ContinuationHelper::InterpretedFrame>(hf, caller, bottom);\n-\n-  intptr_t* const frame_sp = f.sp();\n-  intptr_t* const heap_sp = hf.unextended_sp();\n-  intptr_t* const frame_bottom = ContinuationHelper::InterpretedFrame::frame_bottom(f);\n-\n-  assert(hf.is_heap_frame(), \"should be\");\n-  const int fsize = ContinuationHelper::InterpretedFrame::frame_bottom(hf) - heap_sp;\n-\n-  assert(!bottom || frame_sp + fsize >= _cont.entrySP() - 2, \"\");\n-  assert(!bottom || frame_sp + fsize <= _cont.entrySP(), \"\");\n-\n-  assert(ContinuationHelper::InterpretedFrame::frame_bottom(f) == frame_sp + fsize, \"\");\n-\n-  \/\/ on AArch64 we add padding between the locals and the rest of the frame to keep the fp 16-byte-aligned\n-  const int locals = hf.interpreter_frame_method()->max_locals();\n-  assert(hf.is_heap_frame(), \"should be\");\n-  assert(!f.is_heap_frame(), \"should not be\");\n-\n-  copy_from_chunk(ContinuationHelper::InterpretedFrame::frame_bottom(hf) - locals,\n-                  ContinuationHelper::InterpretedFrame::frame_bottom(f) - locals, locals); \/\/ copy locals\n-  copy_from_chunk(heap_sp, frame_sp, fsize - locals); \/\/ copy rest\n-\n-  set_interpreter_frame_bottom(f, frame_bottom); \/\/ the copy overwrites the metadata\n-  derelativize_interpreted_frame_metadata(hf, f);\n-  patch(f, caller, bottom);\n-\n-  assert(f.is_interpreted_frame_valid(_cont.thread()), \"invalid thawed frame\");\n-  assert(ContinuationHelper::InterpretedFrame::frame_bottom(f) <= ContinuationHelper::Frame::frame_top(caller), \"\");\n-\n-  CONT_JFR_ONLY(_cont.record_interpreted_frame();)\n-\n-  maybe_set_fastpath(f.sp());\n-\n-  if (!bottom) {\n-    \/\/ can only fix caller once this frame is thawed (due to callee saved regs)\n-    _cont.tail()->fix_thawed_frame(caller, SmallRegisterMap::instance);\n-  } else if (_cont.tail()->has_bitmap() && locals > 0) {\n-    assert(hf.is_heap_frame(), \"should be\");\n-    clear_bitmap_bits(ContinuationHelper::InterpretedFrame::frame_bottom(hf) - locals, locals);\n-  }\n-\n-  DEBUG_ONLY(after_thaw_java_frame(f, bottom);)\n-  caller = f;\n-}\n-\n-void ThawBase::recurse_thaw_compiled_frame(const frame& hf, frame& caller, int num_frames, bool stub_caller) {\n-  assert(!hf.is_interpreted_frame(), \"\");\n-  assert(_cont.is_preempted() || !stub_caller, \"stub caller not at preemption\");\n-\n-  if (!stub_caller && UNLIKELY(_barriers)) { \/\/ recurse_thaw_stub_frame already invoked our barriers with a full regmap\n-    _cont.tail()->do_barriers<stackChunkOopDesc::BarrierType::Store>(_stream, SmallRegisterMap::instance);\n-  }\n-\n-  const bool bottom = recurse_thaw_java_frame<ContinuationHelper::CompiledFrame>(caller, num_frames);\n-\n-  DEBUG_ONLY(before_thaw_java_frame(hf, caller, bottom, num_frames);)\n-\n-  assert(caller.sp() == caller.unextended_sp(), \"\");\n-\n-  if ((!bottom && caller.is_interpreted_frame()) || (bottom && Interpreter::contains(_cont.tail()->pc()))) {\n-    _align_size += frame::align_wiggle; \/\/ we add one whether or not we've aligned because we add it in freeze_interpreted_frame\n-  }\n-\n-  frame f = new_stack_frame<ContinuationHelper::CompiledFrame>(hf, caller, bottom);\n-  intptr_t* const frame_sp = f.sp();\n-  intptr_t* const heap_sp = hf.unextended_sp();\n-\n-  const int added_argsize = (bottom || caller.is_interpreted_frame()) ? hf.compiled_frame_stack_argsize() : 0;\n-  int fsize = ContinuationHelper::CompiledFrame::size(hf) + added_argsize;\n-  assert(fsize <= (int)(caller.unextended_sp() - f.unextended_sp()), \"\");\n-\n-  intptr_t* from = heap_sp - frame::metadata_words;\n-  intptr_t* to   = frame_sp - frame::metadata_words;\n-  int sz = fsize + frame::metadata_words;\n-\n-  assert(!bottom || (_cont.entrySP() - 1 <= to + sz && to + sz <= _cont.entrySP()), \"\");\n-  assert(!bottom || hf.compiled_frame_stack_argsize() != 0 || (to + sz && to + sz == _cont.entrySP()), \"\");\n-\n-  copy_from_chunk(from, to, sz); \/\/ copying good oops because we invoked barriers above\n-\n-  patch(f, caller, bottom);\n-\n-  if (f.cb()->is_nmethod()) {\n-    f.cb()->as_nmethod()->run_nmethod_entry_barrier();\n-  }\n-\n-  if (f.is_deoptimized_frame()) {\n-    maybe_set_fastpath(f.sp());\n-  } else if (_thread->is_interp_only_mode()\n-              || (_cont.is_preempted() && f.cb()->as_compiled_method()->is_marked_for_deoptimization())) {\n-    \/\/ The caller of the safepoint stub when the continuation is preempted is not at a call instruction, and so\n-    \/\/ cannot rely on nmethod patching for deopt.\n-    assert(_thread->is_interp_only_mode() || stub_caller, \"expected a stub-caller\");\n-\n-    log_develop_trace(continuations)(\"Deoptimizing thawed frame\");\n-    DEBUG_ONLY(ContinuationHelper::Frame::patch_pc(f, nullptr));\n-\n-    f.deoptimize(nullptr); \/\/ we're assuming there are no monitors; this doesn't revoke biased locks\n-    assert(f.is_deoptimized_frame(), \"\");\n-    assert(ContinuationHelper::Frame::is_deopt_return(f.raw_pc(), f), \"\");\n-    maybe_set_fastpath(f.sp());\n-  }\n-\n-  if (!bottom) {\n-    \/\/ can only fix caller once this frame is thawed (due to callee saved regs)\n-    \/\/ This happens on the stack\n-    _cont.tail()->fix_thawed_frame(caller, SmallRegisterMap::instance);\n-  } else if (_cont.tail()->has_bitmap() && added_argsize > 0) {\n-    clear_bitmap_bits(heap_sp + ContinuationHelper::CompiledFrame::size(hf), added_argsize);\n-  }\n-\n-  DEBUG_ONLY(after_thaw_java_frame(f, bottom);)\n-  caller = f;\n-}\n-\n-void ThawBase::recurse_thaw_stub_frame(const frame& hf, frame& caller, int num_frames) {\n-  DEBUG_ONLY(_frames++;)\n-\n-  {\n-    RegisterMap map(nullptr, true, false, false);\n-    map.set_include_argument_oops(false);\n-    _stream.next(&map);\n-    assert(!_stream.is_done(), \"\");\n-    if (UNLIKELY(_barriers)) { \/\/ we're now doing this on the stub's caller\n-      _cont.tail()->do_barriers<stackChunkOopDesc::BarrierType::Store>(_stream, &map);\n-    }\n-    assert(!_stream.is_done(), \"\");\n-  }\n-\n-  recurse_thaw_compiled_frame(_stream.to_frame(), caller, num_frames, true); \/\/ this could be deoptimized\n-\n-  DEBUG_ONLY(before_thaw_java_frame(hf, caller, false, num_frames);)\n-\n-  assert(ContinuationHelper::Frame::is_stub(hf.cb()), \"\");\n-  assert(caller.sp() == caller.unextended_sp(), \"\");\n-  assert(!caller.is_interpreted_frame(), \"\");\n-\n-  int fsize = ContinuationHelper::StubFrame::size(hf);\n-\n-  frame f = new_stack_frame<ContinuationHelper::StubFrame>(hf, caller, false);\n-  intptr_t* frame_sp = f.sp();\n-  intptr_t* heap_sp = hf.sp();\n-\n-  copy_from_chunk(heap_sp - frame::metadata_words, frame_sp - frame::metadata_words,\n-                  fsize + frame::metadata_words);\n-\n-  { \/\/ can only fix caller once this frame is thawed (due to callee saved regs)\n-    RegisterMap map(nullptr, true, false, false); \/\/ map.clear();\n-    map.set_include_argument_oops(false);\n-    f.oop_map()->update_register_map(&f, &map);\n-    ContinuationHelper::update_register_map_with_callee(caller, &map);\n-    _cont.tail()->fix_thawed_frame(caller, &map);\n-  }\n-\n-  DEBUG_ONLY(after_thaw_java_frame(f, false);)\n-  caller = f;\n-}\n-\n-void ThawBase::finish_thaw(frame& f) {\n-  stackChunkOop chunk = _cont.tail();\n-\n-  if (chunk->is_empty()) {\n-    \/\/ Only remove chunk from list if it can't be reused for another freeze\n-    if (_barriers) {\n-      _cont.set_tail(chunk->parent());\n-    } else {\n-      chunk->set_has_mixed_frames(false);\n-    }\n-    chunk->set_max_size(0);\n-    assert(chunk->argsize() == 0, \"\");\n-  } else {\n-    chunk->set_max_size(chunk->max_size() - _align_size);\n-  }\n-  assert(chunk->is_empty() == (chunk->max_size() == 0), \"\");\n-\n-  if ((intptr_t)f.sp() % frame::frame_alignment != 0) {\n-    assert(f.is_interpreted_frame(), \"\");\n-    f.set_sp(f.sp() - 1);\n-  }\n-  push_return_frame(f);\n-  chunk->fix_thawed_frame(f, SmallRegisterMap::instance); \/\/ can only fix caller after push_return_frame (due to callee saved regs)\n-\n-  assert(_cont.is_empty() == _cont.last_frame().is_empty(), \"\");\n-\n-  log_develop_trace(continuations)(\"thawed %d frames\", _frames);\n-\n-  LogTarget(Trace, continuations) lt;\n-  if (lt.develop_is_enabled()) {\n-    LogStream ls(lt);\n-    ls.print_cr(\"top hframe after (thaw):\");\n-    _cont.last_frame().print_value_on(&ls, nullptr);\n-  }\n-}\n-\n-void ThawBase::push_return_frame(frame& f) { \/\/ see generate_cont_thaw\n-  assert(!f.is_compiled_frame() || f.is_deoptimized_frame() == f.cb()->as_compiled_method()->is_deopt_pc(f.raw_pc()), \"\");\n-  assert(!f.is_compiled_frame() || f.is_deoptimized_frame() == (f.pc() != f.raw_pc()), \"\");\n-\n-  LogTarget(Trace, continuations) lt;\n-  if (lt.develop_is_enabled()) {\n-    LogStream ls(lt);\n-    ls.print_cr(\"push_return_frame\");\n-    f.print_value_on(&ls, nullptr);\n-  }\n-\n-  assert(f.sp() - frame::metadata_words >= _top_stack_address, \"overwrote past thawing space\"\n-    \" to: \" INTPTR_FORMAT \" top_address: \" INTPTR_FORMAT, p2i(f.sp() - frame::metadata_words), p2i(_top_stack_address));\n-  ContinuationHelper::Frame::patch_pc(f, f.raw_pc()); \/\/ in case we want to deopt the frame in a full transition, this is checked.\n-  ContinuationHelper::push_pd(f);\n-\n-  assert(ContinuationHelper::Frame::assert_frame_laid_out(f), \"\");\n-}\n-\n-\/\/ returns new top sp\n-\/\/ called after preparations (stack overflow check and making room)\n-template<typename ConfigT>\n-static inline intptr_t* thaw_internal(JavaThread* thread, const thaw_kind kind) {\n-  assert(thread == JavaThread::current(), \"Must be current thread\");\n-\n-  CONT_JFR_ONLY(EventContinuationThaw event;)\n-\n-  log_develop_trace(continuations)(\"~~~~ thaw kind: %d sp: \" INTPTR_FORMAT, kind, p2i(thread->last_continuation()->entry_sp()));\n-\n-  ContinuationEntry* entry = thread->last_continuation();\n-  assert(entry != nullptr, \"\");\n-  oop oopCont = entry->cont_oop();\n-\n-  assert(!jdk_internal_vm_Continuation::done(oopCont), \"\");\n-  assert(oopCont == get_continuation(thread), \"\");\n-  verify_continuation(oopCont);\n-\n-  assert(entry->is_virtual_thread() == (entry->scope() == java_lang_VirtualThread::vthread_scope()), \"\");\n-\n-  ContinuationWrapper cont(thread, oopCont);\n-  log_develop_debug(continuations)(\"THAW #\" INTPTR_FORMAT \" \" INTPTR_FORMAT, cont.hash(), p2i((oopDesc*)oopCont));\n-\n-#ifdef ASSERT\n-  set_anchor_to_entry(thread, cont.entry());\n-  log_frames(thread);\n-  clear_anchor(thread);\n-#endif\n-\n-  Thaw<ConfigT> thw(thread, cont);\n-  intptr_t* const sp = thw.thaw(kind);\n-  assert(is_aligned(sp, frame::frame_alignment), \"\");\n-\n-  thread->reset_held_monitor_count();\n-\n-  verify_continuation(cont.continuation());\n-\n-#ifdef ASSERT\n-  intptr_t* sp0 = sp;\n-  address pc0 = *(address*)(sp - frame::sender_sp_ret_address_offset());\n-  set_anchor(thread, sp0);\n-  log_frames(thread);\n-  if (LoomVerifyAfterThaw) {\n-    assert(do_verify_after_thaw(thread, thw.barriers(), cont.tail(), tty), \"\");\n-  }\n-  assert(ContinuationEntry::assert_entry_frame_laid_out(thread), \"\");\n-  clear_anchor(thread);\n-\n-  LogTarget(Trace, continuations) lt;\n-  if (lt.develop_is_enabled()) {\n-    LogStream ls(lt);\n-    ls.print_cr(\"Jumping to frame (thaw):\");\n-    frame(sp).print_value_on(&ls, nullptr);\n-  }\n-#endif\n-\n-  CONT_JFR_ONLY(cont.post_jfr_event(&event, thread);)\n-\n-  verify_continuation(cont.continuation());\n-  log_develop_debug(continuations)(\"=== End of thaw #\" INTPTR_FORMAT, cont.hash());\n-\n-  return sp;\n-}\n-\n-#ifdef ASSERT\n-static void do_deopt_after_thaw(JavaThread* thread) {\n-  int i = 0;\n-  StackFrameStream fst(thread, true, false);\n-  fst.register_map()->set_include_argument_oops(false);\n-  ContinuationHelper::update_register_map_with_callee(*fst.current(), fst.register_map());\n-  for (; !fst.is_done(); fst.next()) {\n-    if (fst.current()->cb()->is_compiled()) {\n-      CompiledMethod* cm = fst.current()->cb()->as_compiled_method();\n-      if (!cm->method()->is_continuation_enter_intrinsic()) {\n-        cm->make_deoptimized();\n-      }\n-    }\n-  }\n-}\n-\n-class ThawVerifyOopsClosure: public OopClosure {\n-  intptr_t* _p;\n-  outputStream* _st;\n-  bool is_good_oop(oop o) {\n-    return dbg_is_safe(o, -1) && dbg_is_safe(o->klass(), -1) && oopDesc::is_oop(o) && o->klass()->is_klass();\n-  }\n-public:\n-  ThawVerifyOopsClosure(outputStream* st) : _p(nullptr), _st(st) {}\n-  intptr_t* p() { return _p; }\n-  void reset() { _p = nullptr; }\n-\n-  virtual void do_oop(oop* p) {\n-    oop o = *p;\n-    if (o == nullptr || is_good_oop(o)) {\n-      return;\n-    }\n-    _p = (intptr_t*)p;\n-    _st->print_cr(\"*** non-oop \" PTR_FORMAT \" found at \" PTR_FORMAT, p2i(*p), p2i(p));\n-  }\n-  virtual void do_oop(narrowOop* p) {\n-    oop o = RawAccess<>::oop_load(p);\n-    if (o == nullptr || is_good_oop(o)) {\n-      return;\n-    }\n-    _p = (intptr_t*)p;\n-    _st->print_cr(\"*** (narrow) non-oop %x found at \" PTR_FORMAT, (int)(*p), p2i(p));\n-  }\n-};\n-\n-static bool do_verify_after_thaw(JavaThread* thread, bool barriers, stackChunkOop chunk, outputStream* st) {\n-  assert(thread->has_last_Java_frame(), \"\");\n-\n-  ResourceMark rm;\n-  ThawVerifyOopsClosure cl(st);\n-  CodeBlobToOopClosure cf(&cl, false);\n-\n-  StackFrameStream fst(thread, true, false);\n-  fst.register_map()->set_include_argument_oops(false);\n-  ContinuationHelper::update_register_map_with_callee(*fst.current(), fst.register_map());\n-  for (; !fst.is_done() && !Continuation::is_continuation_enterSpecial(*fst.current()); fst.next()) {\n-    if (fst.current()->cb()->is_compiled() && fst.current()->cb()->as_compiled_method()->is_marked_for_deoptimization()) {\n-      st->print_cr(\">>> do_verify_after_thaw deopt\");\n-      fst.current()->deoptimize(nullptr);\n-      fst.current()->print_on(st);\n-    }\n-\n-    fst.current()->oops_do(&cl, &cf, fst.register_map());\n-    if (cl.p() != nullptr) {\n-      frame fr = *fst.current();\n-      st->print_cr(\"Failed for frame barriers: %d %d\", barriers, chunk->requires_barriers());\n-      fr.print_on(st);\n-      if (!fr.is_interpreted_frame()) {\n-        st->print_cr(\"size: %d argsize: %d\",\n-                     ContinuationHelper::NonInterpretedUnknownFrame::size(fr),\n-                     ContinuationHelper::NonInterpretedUnknownFrame::stack_argsize(fr));\n-      }\n-      VMReg reg = fst.register_map()->find_register_spilled_here(cl.p(), fst.current()->sp());\n-      if (reg != nullptr) {\n-        st->print_cr(\"Reg %s %d\", reg->name(), reg->is_stack() ? (int)reg->reg2stack() : -99);\n-      }\n-      cl.reset();\n-      DEBUG_ONLY(thread->print_frame_layout();)\n-      if (chunk != nullptr) {\n-        chunk->print_on(true, st);\n-      }\n-      return false;\n-    }\n-  }\n-  return true;\n-}\n-\n-static void log_frames(JavaThread* thread) {\n-  const static int show_entry_callers = 3;\n-  LogTarget(Trace, continuations) lt;\n-  if (!lt.develop_is_enabled()) {\n-    return;\n-  }\n-  LogStream ls(lt);\n-\n-  ls.print_cr(\"------- frames ---------\");\n-  if (!thread->has_last_Java_frame()) {\n-    ls.print_cr(\"NO ANCHOR!\");\n-  }\n-\n-  RegisterMap map(thread, true, true, false);\n-  map.set_include_argument_oops(false);\n-\n-  if (false) {\n-    for (frame f = thread->last_frame(); !f.is_entry_frame(); f = f.sender(&map)) {\n-      f.print_on(&ls);\n-    }\n-  } else {\n-    map.set_skip_missing(true);\n-    ResetNoHandleMark rnhm;\n-    ResourceMark rm;\n-    HandleMark hm(Thread::current());\n-    FrameValues values;\n-\n-    int i = 0;\n-    int post_entry = -1;\n-    for (frame f = thread->last_frame(); !f.is_entry_frame(); f = f.sender(&map)) {\n-      f.describe(values, i++, &map);\n-      if (post_entry >= 0 || Continuation::is_continuation_enterSpecial(f))\n-        post_entry++;\n-      if (post_entry >= show_entry_callers)\n-        break;\n-    }\n-    values.print_on(thread, &ls);\n-  }\n-\n-  ls.print_cr(\"======= end frames =========\");\n-}\n-#endif \/\/ ASSERT\n-\n-#include CPU_HEADER_INLINE(continuation)\n-\n-\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n-\n-int ContinuationEntry::return_pc_offset = 0;\n-nmethod* ContinuationEntry::continuation_enter = nullptr;\n-address ContinuationEntry::return_pc = nullptr;\n-\n-void ContinuationEntry::set_enter_nmethod(nmethod* nm) {\n-  assert(return_pc_offset != 0, \"\");\n-  continuation_enter = nm;\n-  return_pc = nm->code_begin() + return_pc_offset;\n-}\n-\n-ContinuationEntry* ContinuationEntry::from_frame(const frame& f) {\n-  assert(Continuation::is_continuation_enterSpecial(f), \"\");\n-  return (ContinuationEntry*)f.unextended_sp();\n-}\n-\n-void ContinuationEntry::flush_stack_processing(JavaThread* thread) const {\n-  maybe_flush_stack_processing(thread, this);\n-}\n-\n-\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n-\n-#ifdef ASSERT\n-bool ContinuationEntry::assert_entry_frame_laid_out(JavaThread* thread) {\n-  assert(thread->has_last_Java_frame(), \"Wrong place to use this assertion\");\n-\n-  ContinuationEntry* entry =\n-    Continuation::get_continuation_entry_for_continuation(thread, get_continuation(thread));\n-  assert(entry != nullptr, \"\");\n-\n-  intptr_t* unextended_sp = entry->entry_sp();\n-  intptr_t* sp;\n-  if (entry->argsize() > 0) {\n-    sp = entry->bottom_sender_sp();\n-  } else {\n-    sp = unextended_sp;\n-    bool interpreted_bottom = false;\n-    RegisterMap map(thread, false, false, false);\n-    frame f;\n-    for (f = thread->last_frame();\n-         !f.is_first_frame() && f.sp() <= unextended_sp && !Continuation::is_continuation_enterSpecial(f);\n-         f = f.sender(&map)) {\n-      interpreted_bottom = f.is_interpreted_frame();\n-    }\n-    assert(Continuation::is_continuation_enterSpecial(f), \"\");\n-    sp = interpreted_bottom ? f.sp() : entry->bottom_sender_sp();\n-  }\n-\n-  assert(sp != nullptr, \"\");\n-  assert(sp <= entry->entry_sp(), \"\");\n-  address pc = *(address*)(sp - frame::sender_sp_ret_address_offset());\n-\n-  if (pc != StubRoutines::cont_returnBarrier()) {\n-    CodeBlob* cb = pc != nullptr ? CodeCache::find_blob(pc) : nullptr;\n-    assert(cb->as_compiled_method()->method()->is_continuation_enter_intrinsic(), \"\");\n-  }\n-\n-  return true;\n-}\n-\n-static void print_frame_layout(const frame& f, bool callee_complete, outputStream* st) {\n-  ResourceMark rm;\n-  FrameValues values;\n-  assert(f.get_cb() != nullptr, \"\");\n-  RegisterMap map(f.is_heap_frame() ?\n-                  (JavaThread*)nullptr :\n-                  JavaThread::current(), true, false, false);\n-  map.set_include_argument_oops(false);\n-  map.set_skip_missing(true);\n-  if (callee_complete) {\n-    frame::update_map_with_saved_link(&map, ContinuationHelper::Frame::callee_link_address(f));\n-  }\n-  const_cast<frame&>(f).describe(values, 0, &map);\n-  values.print_on((JavaThread*)nullptr, st);\n-}\n-#endif\n-\n-#ifndef PRODUCT\n-static jlong java_tid(JavaThread* thread) {\n-  return java_lang_Thread::thread_id(thread->threadObj());\n-}\n-#endif\n-\n-static address thaw_entry   = nullptr;\n-static address freeze_entry = nullptr;\n-\n-address Continuation::thaw_entry() {\n-  return ::thaw_entry;\n-}\n-\n-address Continuation::freeze_entry() {\n-  return ::freeze_entry;\n-}\n-\n-class ConfigResolve {\n-public:\n-  static void resolve() { resolve_compressed(); }\n-\n-  static void resolve_compressed() {\n-    UseCompressedOops ? resolve_gc<true>()\n-                      : resolve_gc<false>();\n-  }\n-\n-private:\n-  template <bool use_compressed>\n-  static void resolve_gc() {\n-    BarrierSet* bs = BarrierSet::barrier_set();\n-    assert(bs != NULL, \"freeze\/thaw invoked before BarrierSet is set\");\n-    switch (bs->kind()) {\n-#define BARRIER_SET_RESOLVE_BARRIER_CLOSURE(bs_name)                    \\\n-      case BarrierSet::bs_name: {                                       \\\n-        resolve<use_compressed, typename BarrierSet::GetType<BarrierSet::bs_name>::type>(); \\\n-      }                                                                 \\\n-        break;\n-      FOR_EACH_CONCRETE_BARRIER_SET_DO(BARRIER_SET_RESOLVE_BARRIER_CLOSURE)\n-#undef BARRIER_SET_RESOLVE_BARRIER_CLOSURE\n-\n-    default:\n-      fatal(\"BarrierSet resolving not implemented\");\n-    };\n-  }\n-\n-  template <bool use_compressed, typename BarrierSetT>\n-  static void resolve() {\n-    typedef Config<use_compressed ? oop_kind::NARROW : oop_kind::WIDE, BarrierSetT> SelectedConfigT;\n-\n-    freeze_entry = (address)freeze<SelectedConfigT>;\n-\n-    \/\/ If we wanted, we could templatize by kind and have three different thaw entries\n-    thaw_entry   = (address)thaw<SelectedConfigT>;\n-  }\n-};\n-\n-void continuations_init() { Continuations::init(); }\n-\n-void Continuations::init() {\n-  Continuation::init();\n+void Continuations::init() {\n+  Continuation::init();\n@@ -3094,4 +421,0 @@\n-void Continuation::init() {\n-  ConfigResolve::resolve();\n-}\n-\n","filename":"src\/hotspot\/share\/runtime\/continuation.cpp","additions":10,"deletions":2687,"binary":false,"changes":2697,"status":"modified"},{"patch":"@@ -28,1 +28,0 @@\n-#include \"oops\/access.hpp\"\n@@ -127,117 +126,0 @@\n-\/\/ Metadata stored in the continuation entry frame\n-class ContinuationEntry {\n-public:\n-#ifdef ASSERT\n-  int cookie;\n-  static ByteSize cookie_offset() { return byte_offset_of(ContinuationEntry, cookie); }\n-  void verify_cookie() { assert(this->cookie == 0x1234, \"\"); }\n-#endif\n-\n-public:\n-  static int return_pc_offset; \/\/ friend gen_continuation_enter\n-  static void set_enter_nmethod(nmethod* nm); \/\/ friend SharedRuntime::generate_native_wrapper\n-\n-private:\n-  static nmethod* continuation_enter;\n-  static address return_pc;\n-\n-private:\n-  ContinuationEntry* _parent;\n-  oopDesc* _cont;\n-  oopDesc* _chunk;\n-  int _flags;\n-  int _argsize;\n-  intptr_t* _parent_cont_fastpath;\n-  int _parent_held_monitor_count;\n-  uint _pin_count;\n-\n-public:\n-  static ByteSize parent_offset()   { return byte_offset_of(ContinuationEntry, _parent); }\n-  static ByteSize cont_offset()     { return byte_offset_of(ContinuationEntry, _cont); }\n-  static ByteSize chunk_offset()    { return byte_offset_of(ContinuationEntry, _chunk); }\n-  static ByteSize flags_offset()    { return byte_offset_of(ContinuationEntry, _flags); }\n-  static ByteSize argsize_offset()  { return byte_offset_of(ContinuationEntry, _argsize); }\n-  static ByteSize pin_count_offset(){ return byte_offset_of(ContinuationEntry, _pin_count); }\n-  static ByteSize parent_cont_fastpath_offset()      { return byte_offset_of(ContinuationEntry, _parent_cont_fastpath); }\n-  static ByteSize parent_held_monitor_count_offset() { return byte_offset_of(ContinuationEntry, _parent_held_monitor_count); }\n-\n-  static void setup_oopmap(OopMap* map) {\n-    map->set_oop(VMRegImpl::stack2reg(in_bytes(cont_offset())  \/ VMRegImpl::stack_slot_size));\n-    map->set_oop(VMRegImpl::stack2reg(in_bytes(chunk_offset()) \/ VMRegImpl::stack_slot_size));\n-  }\n-\n-public:\n-  static size_t size() { return align_up((int)sizeof(ContinuationEntry), 2*wordSize); }\n-\n-  ContinuationEntry* parent() const { return _parent; }\n-\n-  static address entry_pc() { return return_pc; }\n-  intptr_t* entry_sp() const { return (intptr_t*)this; }\n-  inline intptr_t* entry_fp() const;\n-\n-  int argsize() const { return _argsize; }\n-  void set_argsize(int value) { _argsize = value; }\n-\n-  bool is_pinned() { return _pin_count > 0; }\n-  bool pin() {\n-    if (_pin_count == UINT_MAX) return false;\n-    _pin_count++;\n-    return true;\n-  }\n-  bool unpin() {\n-    if (_pin_count == 0) return false;\n-    _pin_count--;\n-    return true;\n-  }\n-\n-  intptr_t* parent_cont_fastpath() const { return _parent_cont_fastpath; }\n-  void set_parent_cont_fastpath(intptr_t* x) { _parent_cont_fastpath = x; }\n-\n-  static ContinuationEntry* from_frame(const frame& f);\n-  frame to_frame() const;\n-  void update_register_map(RegisterMap* map) const;\n-  void flush_stack_processing(JavaThread* thread) const;\n-\n-  intptr_t* bottom_sender_sp() const {\n-    intptr_t* sp = entry_sp() - argsize();\n-#ifdef _LP64\n-    sp = align_down(sp, frame::frame_alignment);\n-#endif\n-    return sp;\n-  }\n-\n-  oop cont_oop() const {\n-    oop snapshot = _cont;\n-    return NativeAccess<>::oop_load(&snapshot);\n-  }\n-\n-  oop scope()     const { return Continuation::continuation_scope(cont_oop()); }\n-\n-  oop cont_raw()  const { return _cont; }\n-  oop chunk_raw() const { return _chunk; }\n-\n-  bool is_virtual_thread() const { return _flags != 0; }\n-\n-  static oop cont_oop_or_null(const ContinuationEntry* ce) {\n-    return ce == nullptr ? nullptr : ce->cont_oop();\n-  }\n-\n-#ifndef PRODUCT\n-  void describe(FrameValues& values, int frame_no) const {\n-    address usp = (address)this;\n-    values.describe(frame_no, (intptr_t*)(usp + in_bytes(ContinuationEntry::parent_offset())),    \"parent\");\n-    values.describe(frame_no, (intptr_t*)(usp + in_bytes(ContinuationEntry::cont_offset())),      \"continuation\");\n-    values.describe(frame_no, (intptr_t*)(usp + in_bytes(ContinuationEntry::flags_offset())),     \"flags\");\n-    values.describe(frame_no, (intptr_t*)(usp + in_bytes(ContinuationEntry::chunk_offset())),     \"chunk\");\n-    values.describe(frame_no, (intptr_t*)(usp + in_bytes(ContinuationEntry::argsize_offset())),   \"argsize\");\n-    values.describe(frame_no, (intptr_t*)(usp + in_bytes(ContinuationEntry::pin_count_offset())), \"pin_count\");\n-    values.describe(frame_no, (intptr_t*)(usp + in_bytes(ContinuationEntry::parent_cont_fastpath_offset())),      \"parent fastpath\");\n-    values.describe(frame_no, (intptr_t*)(usp + in_bytes(ContinuationEntry::parent_held_monitor_count_offset())), \"parent held monitor count\");\n-  }\n-#endif\n-\n-#ifdef ASSERT\n-  static bool assert_entry_frame_laid_out(JavaThread* thread);\n-#endif\n-};\n-\n","filename":"src\/hotspot\/share\/runtime\/continuation.hpp","additions":0,"deletions":118,"binary":false,"changes":118,"status":"modified"},{"patch":"@@ -0,0 +1,112 @@\n+\/*\n+ * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"code\/nmethod.hpp\"\n+#include \"runtime\/continuation.hpp\"\n+#include \"runtime\/continuationEntry.inline.hpp\"\n+#include \"runtime\/frame.inline.hpp\"\n+#include \"runtime\/stackFrameStream.inline.hpp\"\n+#include \"runtime\/stackWatermarkSet.inline.hpp\"\n+#include \"runtime\/stubRoutines.hpp\"\n+#include \"runtime\/thread.inline.hpp\"\n+\n+int ContinuationEntry::return_pc_offset = 0;\n+nmethod* ContinuationEntry::continuation_enter = nullptr;\n+address ContinuationEntry::return_pc = nullptr;\n+\n+void ContinuationEntry::set_enter_nmethod(nmethod* nm) {\n+  assert(return_pc_offset != 0, \"\");\n+  continuation_enter = nm;\n+  return_pc = nm->code_begin() + return_pc_offset;\n+}\n+\n+ContinuationEntry* ContinuationEntry::from_frame(const frame& f) {\n+  assert(Continuation::is_continuation_enterSpecial(f), \"\");\n+  return (ContinuationEntry*)f.unextended_sp();\n+}\n+\n+NOINLINE static void flush_stack_processing(JavaThread* thread, intptr_t* sp) {\n+  log_develop_trace(continuations)(\"flush_stack_processing\");\n+  for (StackFrameStream fst(thread, true, true); fst.current()->sp() <= sp; fst.next()) {\n+    ;\n+  }\n+}\n+\n+inline void maybe_flush_stack_processing(JavaThread* thread, intptr_t* sp) {\n+  StackWatermark* sw;\n+  uintptr_t watermark;\n+  if ((sw = StackWatermarkSet::get(thread, StackWatermarkKind::gc)) != nullptr\n+        && (watermark = sw->watermark()) != 0\n+        && watermark <= (uintptr_t)sp) {\n+    flush_stack_processing(thread, sp);\n+  }\n+}\n+\n+void ContinuationEntry::flush_stack_processing(JavaThread* thread) const {\n+  maybe_flush_stack_processing(thread, (intptr_t*)((uintptr_t)entry_sp() + ContinuationEntry::size()));\n+}\n+\n+void ContinuationEntry::setup_oopmap(OopMap* map) {\n+  map->set_oop(VMRegImpl::stack2reg(in_bytes(cont_offset())  \/ VMRegImpl::stack_slot_size));\n+  map->set_oop(VMRegImpl::stack2reg(in_bytes(chunk_offset()) \/ VMRegImpl::stack_slot_size));\n+}\n+\n+#ifdef ASSERT\n+bool ContinuationEntry::assert_entry_frame_laid_out(JavaThread* thread) {\n+  assert(thread->has_last_Java_frame(), \"Wrong place to use this assertion\");\n+\n+  ContinuationEntry* entry = thread->last_continuation();\n+  assert(entry != nullptr, \"\");\n+\n+  intptr_t* unextended_sp = entry->entry_sp();\n+  intptr_t* sp;\n+  if (entry->argsize() > 0) {\n+    sp = entry->bottom_sender_sp();\n+  } else {\n+    sp = unextended_sp;\n+    bool interpreted_bottom = false;\n+    RegisterMap map(thread, false, false, false);\n+    frame f;\n+    for (f = thread->last_frame();\n+         !f.is_first_frame() && f.sp() <= unextended_sp && !Continuation::is_continuation_enterSpecial(f);\n+         f = f.sender(&map)) {\n+      interpreted_bottom = f.is_interpreted_frame();\n+    }\n+    assert(Continuation::is_continuation_enterSpecial(f), \"\");\n+    sp = interpreted_bottom ? f.sp() : entry->bottom_sender_sp();\n+  }\n+\n+  assert(sp != nullptr, \"\");\n+  assert(sp <= entry->entry_sp(), \"\");\n+  address pc = *(address*)(sp - frame::sender_sp_ret_address_offset());\n+\n+  if (pc != StubRoutines::cont_returnBarrier()) {\n+    CodeBlob* cb = pc != nullptr ? CodeCache::find_blob(pc) : nullptr;\n+    assert(cb->as_compiled_method()->method()->is_continuation_enter_intrinsic(), \"\");\n+  }\n+\n+  return true;\n+}\n+#endif \/\/ ASSERT\n","filename":"src\/hotspot\/share\/runtime\/continuationEntry.cpp","additions":112,"deletions":0,"binary":false,"changes":112,"status":"added"},{"patch":"@@ -0,0 +1,149 @@\n+\/*\n+ * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_VM_RUNTIME_CONTINUATIONENTRY_HPP\n+#define SHARE_VM_RUNTIME_CONTINUATIONENTRY_HPP\n+\n+#include \"oops\/oop.hpp\"\n+#include \"oops\/oopsHierarchy.hpp\"\n+#include \"runtime\/continuation.hpp\"\n+#include \"utilities\/sizes.hpp\"\n+\n+class nmethod;\n+class RegisterMap;\n+class OopMap;\n+class JavaThread;\n+\n+\/\/ Metadata stored in the continuation entry frame\n+class ContinuationEntry {\n+public:\n+#ifdef ASSERT\n+  int cookie;\n+  static ByteSize cookie_offset() { return byte_offset_of(ContinuationEntry, cookie); }\n+  void verify_cookie() { assert(this->cookie == 0x1234, \"\"); }\n+#endif\n+\n+public:\n+  static int return_pc_offset; \/\/ friend gen_continuation_enter\n+  static void set_enter_nmethod(nmethod* nm); \/\/ friend SharedRuntime::generate_native_wrapper\n+\n+private:\n+  static nmethod* continuation_enter;\n+  static address return_pc;\n+\n+private:\n+  ContinuationEntry* _parent;\n+  oopDesc* _cont;\n+  oopDesc* _chunk;\n+  int _flags;\n+  int _argsize;\n+  intptr_t* _parent_cont_fastpath;\n+  int _parent_held_monitor_count;\n+  uint _pin_count;\n+\n+public:\n+  static ByteSize parent_offset()   { return byte_offset_of(ContinuationEntry, _parent); }\n+  static ByteSize cont_offset()     { return byte_offset_of(ContinuationEntry, _cont); }\n+  static ByteSize chunk_offset()    { return byte_offset_of(ContinuationEntry, _chunk); }\n+  static ByteSize flags_offset()    { return byte_offset_of(ContinuationEntry, _flags); }\n+  static ByteSize argsize_offset()  { return byte_offset_of(ContinuationEntry, _argsize); }\n+  static ByteSize pin_count_offset(){ return byte_offset_of(ContinuationEntry, _pin_count); }\n+  static ByteSize parent_cont_fastpath_offset()      { return byte_offset_of(ContinuationEntry, _parent_cont_fastpath); }\n+  static ByteSize parent_held_monitor_count_offset() { return byte_offset_of(ContinuationEntry, _parent_held_monitor_count); }\n+\n+  static void setup_oopmap(OopMap* map);\n+\n+public:\n+  static size_t size() { return align_up((int)sizeof(ContinuationEntry), 2*wordSize); }\n+\n+  ContinuationEntry* parent() const { return _parent; }\n+\n+  static address entry_pc() { return return_pc; }\n+  intptr_t* entry_sp() const { return (intptr_t*)this; }\n+  intptr_t* entry_fp() const;\n+\n+  int argsize() const { return _argsize; }\n+  void set_argsize(int value) { _argsize = value; }\n+\n+  bool is_pinned() { return _pin_count > 0; }\n+  bool pin() {\n+    if (_pin_count == UINT_MAX) return false;\n+    _pin_count++;\n+    return true;\n+  }\n+  bool unpin() {\n+    if (_pin_count == 0) return false;\n+    _pin_count--;\n+    return true;\n+  }\n+\n+  intptr_t* parent_cont_fastpath() const { return _parent_cont_fastpath; }\n+  void set_parent_cont_fastpath(intptr_t* x) { _parent_cont_fastpath = x; }\n+\n+  static ContinuationEntry* from_frame(const frame& f);\n+  frame to_frame() const;\n+  void update_register_map(RegisterMap* map) const;\n+  void flush_stack_processing(JavaThread* thread) const;\n+\n+  intptr_t* bottom_sender_sp() const {\n+    intptr_t* sp = entry_sp() - argsize();\n+#ifdef _LP64\n+    sp = align_down(sp, frame::frame_alignment);\n+#endif\n+    return sp;\n+  }\n+\n+  inline oop cont_oop() const;\n+\n+  oop scope()     const { return Continuation::continuation_scope(cont_oop()); }\n+\n+  oop cont_raw()  const { return _cont; }\n+  oop chunk_raw() const { return _chunk; }\n+\n+  bool is_virtual_thread() const { return _flags != 0; }\n+\n+  static oop cont_oop_or_null(const ContinuationEntry* ce) {\n+    return ce == nullptr ? nullptr : ce->cont_oop();\n+  }\n+\n+#ifndef PRODUCT\n+  void describe(FrameValues& values, int frame_no) const {\n+    address usp = (address)this;\n+    values.describe(frame_no, (intptr_t*)(usp + in_bytes(ContinuationEntry::parent_offset())),    \"parent\");\n+    values.describe(frame_no, (intptr_t*)(usp + in_bytes(ContinuationEntry::cont_offset())),      \"continuation\");\n+    values.describe(frame_no, (intptr_t*)(usp + in_bytes(ContinuationEntry::flags_offset())),     \"flags\");\n+    values.describe(frame_no, (intptr_t*)(usp + in_bytes(ContinuationEntry::chunk_offset())),     \"chunk\");\n+    values.describe(frame_no, (intptr_t*)(usp + in_bytes(ContinuationEntry::argsize_offset())),   \"argsize\");\n+    values.describe(frame_no, (intptr_t*)(usp + in_bytes(ContinuationEntry::pin_count_offset())), \"pin_count\");\n+    values.describe(frame_no, (intptr_t*)(usp + in_bytes(ContinuationEntry::parent_cont_fastpath_offset())),      \"parent fastpath\");\n+    values.describe(frame_no, (intptr_t*)(usp + in_bytes(ContinuationEntry::parent_held_monitor_count_offset())), \"parent held monitor count\");\n+  }\n+#endif\n+\n+#ifdef ASSERT\n+  static bool assert_entry_frame_laid_out(JavaThread* thread);\n+#endif\n+};\n+\n+#endif \/\/ SHARE_VM_RUNTIME_CONTINUATIONENTRY_HPP\n","filename":"src\/hotspot\/share\/runtime\/continuationEntry.hpp","additions":149,"deletions":0,"binary":false,"changes":149,"status":"added"},{"patch":"@@ -0,0 +1,40 @@\n+\/*\n+ * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_VM_RUNTIME_CONTINUATIONENTRY_INLINE_HPP\n+#define SHARE_VM_RUNTIME_CONTINUATIONENTRY_INLINE_HPP\n+\n+#include \"runtime\/continuationEntry.hpp\"\n+\n+#include \"oops\/access.hpp\"\n+\n+#include CPU_HEADER_INLINE(continuationEntry)\n+\n+inline oop ContinuationEntry::cont_oop() const {\n+  oop snapshot = _cont;\n+  return NativeAccess<>::oop_load(&snapshot);\n+}\n+\n+\n+#endif \/\/ SHARE_VM_RUNTIME_CONTINUATIONENTRY_INLINE_HPP\n","filename":"src\/hotspot\/share\/runtime\/continuationEntry.inline.hpp","additions":40,"deletions":0,"binary":false,"changes":40,"status":"added"},{"patch":"@@ -0,0 +1,2434 @@\n+\/*\n+ * Copyright (c) 2018, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"classfile\/javaClasses.inline.hpp\"\n+#include \"classfile\/vmSymbols.hpp\"\n+#include \"code\/codeCache.inline.hpp\"\n+#include \"code\/compiledMethod.inline.hpp\"\n+#include \"code\/vmreg.inline.hpp\"\n+#include \"compiler\/oopMap.inline.hpp\"\n+#include \"gc\/shared\/gc_globals.hpp\"\n+#include \"gc\/shared\/barrierSet.hpp\"\n+#include \"gc\/shared\/memAllocator.hpp\"\n+#include \"gc\/shared\/threadLocalAllocBuffer.inline.hpp\"\n+#include \"interpreter\/interpreter.hpp\"\n+#include \"jfr\/jfrEvents.hpp\"\n+#include \"logging\/log.hpp\"\n+#include \"logging\/logStream.hpp\"\n+#include \"metaprogramming\/conditional.hpp\"\n+#include \"oops\/access.inline.hpp\"\n+#include \"oops\/oopsHierarchy.hpp\"\n+#include \"oops\/objArrayOop.inline.hpp\"\n+#include \"oops\/stackChunkOop.inline.hpp\"\n+#include \"prims\/jvmtiThreadState.hpp\"\n+#include \"runtime\/arguments.hpp\"\n+#include \"runtime\/continuationEntry.inline.hpp\"\n+#include \"runtime\/continuationHelper.inline.hpp\"\n+#include \"runtime\/continuationWrapper.inline.hpp\"\n+#include \"runtime\/frame.inline.hpp\"\n+#include \"runtime\/interfaceSupport.inline.hpp\"\n+#include \"runtime\/jniHandles.inline.hpp\"\n+#include \"runtime\/keepStackGCProcessed.hpp\"\n+#include \"runtime\/orderAccess.hpp\"\n+#include \"runtime\/prefetch.inline.hpp\"\n+#include \"runtime\/smallRegisterMap.inline.hpp\"\n+#include \"runtime\/stackChunkFrameStream.inline.hpp\"\n+#include \"runtime\/stackFrameStream.inline.hpp\"\n+#include \"runtime\/stackOverflow.hpp\"\n+#include \"runtime\/stackWatermarkSet.inline.hpp\"\n+#include \"utilities\/debug.hpp\"\n+#include \"utilities\/exceptions.hpp\"\n+#include \"utilities\/macros.hpp\"\n+\n+#define CONT_JFR false \/\/ emit low-level JFR events that count slow\/fast path for continuation peformance debugging only\n+#if CONT_JFR\n+  #define CONT_JFR_ONLY(code) code\n+#else\n+  #define CONT_JFR_ONLY(code)\n+#endif\n+\n+#if CONT_JFR\n+class FreezeThawJfrInfo : public StackObj {\n+  short _e_size;\n+  short _e_num_interpreted_frames;\n+ public:\n+\n+  FreezeThawJfrInfo() : _e_size(0), _e_num_interpreted_frames(0) {}\n+  inline void record_interpreted_frame() { _e_num_interpreted_frames++; }\n+  inline void record_size_copied(int size) { _e_size += size << LogBytesPerWord; }\n+  template<typename Event> void post_jfr_event(Event *e, oop continuation, JavaThread* jt);\n+};\n+\n+template<typename Event> void FreezeThawJfrInfo::post_jfr_event(Event* e, oop continuation, JavaThread* jt) {\n+  if (e->should_commit()) {\n+    log_develop_trace(continuations)(\"JFR event: iframes: %d size: %d\", _e_num_interpreted_frames, _e_size);\n+    e->set_carrierThread(JFR_JVM_THREAD_ID(jt));\n+    e->set_contClass(continuation->klass());\n+    e->set_numIFrames(_e_num_interpreted_frames);\n+    e->set_size(_e_size);\n+    e->commit();\n+  }\n+}\n+#endif \/\/ CONT_JFR\n+\n+static const bool TEST_THAW_ONE_CHUNK_FRAME = false; \/\/ force thawing frames one-at-a-time for testing\n+\n+\/*\n+ * This file contains the implementation of continuation freezing (yield) and thawing (run).\n+ *\n+ * This code is very latency-critical and very hot. An ordinary and well-behaved server application\n+ * would likely call these operations many thousands of times per second second, on every core.\n+ *\n+ * Freeze might be called every time the application performs any I\/O operation, every time it\n+ * acquires a j.u.c. lock, every time it takes a message from a queue, and thaw can be called\n+ * multiple times in each of those cases, as it is called by the return barrier, which may be\n+ * invoked on method return.\n+ *\n+ * The amortized budget for each of those two operations is ~100-150ns. That is why, for\n+ * example, every effort is made to avoid Java-VM transitions as much as possible.\n+ *\n+ * On the fast path, all frames are known to be compiled, and the chunk requires no barriers\n+ * and so frames simply copied, and the bottom-most one is patched.\n+ * On the slow path, internal pointers in interpreted frames are de\/relativized to\/from offsets\n+ * and absolute pointers, and barriers invoked.\n+ *\/\n+\n+\/************************************************\n+\n+Thread-stack layout on freeze\/thaw.\n+See corresponding stack-chunk layout in instanceStackChunkKlass.hpp\n+\n+            +----------------------------+\n+            |      .                     |\n+            |      .                     |\n+            |      .                     |\n+            |   carrier frames           |\n+            |                            |\n+            |----------------------------|\n+            |                            |\n+            |    Continuation.run        |\n+            |                            |\n+            |============================|\n+            |    enterSpecial frame      |\n+            |  pc                        |\n+            |  rbp                       |\n+            |  -----                     |\n+        ^   |  int argsize               | = ContinuationEntry\n+        |   |  oopDesc* cont             |\n+        |   |  oopDesc* chunk            |\n+        |   |  ContinuationEntry* parent |\n+        |   |  ...                       |\n+        |   |============================| <------ JavaThread::_cont_entry = entry->sp()\n+        |   |  ? alignment word ?        |\n+        |   |----------------------------| <--\\\n+        |   |                            |    |\n+        |   |  ? caller stack args ?     |    |   argsize (might not be 2-word aligned) words\n+Address |   |                            |    |   Caller is still in the chunk.\n+        |   |----------------------------|    |\n+        |   |  pc (? return barrier ?)   |    |  This pc contains the return barrier when the bottom-most frame\n+        |   |  rbp                       |    |  isn't the last one in the continuation.\n+        |   |                            |    |\n+        |   |    frame                   |    |\n+        |   |                            |    |\n+            +----------------------------|     \\__ Continuation frames to be frozen\/thawed\n+            |                            |     \/\n+            |    frame                   |    |\n+            |                            |    |\n+            |----------------------------|    |\n+            |                            |    |\n+            |    frame                   |    |\n+            |                            |    |\n+            |----------------------------| <--\/\n+            |                            |\n+            |    doYield\/safepoint stub  | When preempting forcefully, we could have a safepoint stub\n+            |                            | instead of a doYield stub\n+            |============================| <- the sp passed to freeze\n+            |                            |\n+            |  Native freeze\/thaw frames |\n+            |      .                     |\n+            |      .                     |\n+            |      .                     |\n+            +----------------------------+\n+\n+************************************************\/\n+\n+\/\/ TODO: See AbstractAssembler::generate_stack_overflow_check,\n+\/\/ Compile::bang_size_in_bytes(), m->as_SafePoint()->jvms()->interpreter_frame_size()\n+\/\/ when we stack-bang, we need to update a thread field with the lowest (farthest) bang point.\n+\n+\/\/ Data invariants are defined by Continuation::debug_verify_continuation and Continuation::debug_verify_stack_chunk\n+\n+\/\/ Used to just annotatate cold\/hot branches\n+#define LIKELY(condition)   (condition)\n+#define UNLIKELY(condition) (condition)\n+\n+\/\/ debugging functions\n+#ifdef ASSERT\n+extern \"C\" bool dbg_is_safe(const void* p, intptr_t errvalue); \/\/ address p is readable and *(intptr_t*)p != errvalue\n+\n+static void verify_continuation(oop continuation) { Continuation::debug_verify_continuation(continuation); }\n+\n+static void do_deopt_after_thaw(JavaThread* thread);\n+static bool do_verify_after_thaw(JavaThread* thread, bool barriers, stackChunkOop chunk, outputStream* st);\n+static void log_frames(JavaThread* thread);\n+static void print_frame_layout(const frame& f, bool callee_complete, outputStream* st = tty);\n+\n+#else\n+static void verify_continuation(oop continuation) { }\n+#endif\n+\n+\/\/ should match Continuation.preemptStatus() in Continuation.java\n+enum freeze_result {\n+  freeze_ok = 0,\n+  freeze_ok_bottom = 1,\n+  freeze_pinned_cs = 2,\n+  freeze_pinned_native = 3,\n+  freeze_pinned_monitor = 4,\n+  freeze_exception = 5\n+};\n+\n+const char* freeze_result_names[6] = {\n+  \"freeze_ok\",\n+  \"freeze_ok_bottom\",\n+  \"freeze_pinned_cs\",\n+  \"freeze_pinned_native\",\n+  \"freeze_pinned_monitor\",\n+  \"freeze_exception\"\n+};\n+\n+static freeze_result is_pinned0(JavaThread* thread, oop cont_scope, bool safepoint);\n+template<typename ConfigT> static inline int freeze_internal(JavaThread* current, intptr_t* const sp);\n+\n+enum thaw_kind {\n+  thaw_top = 0,\n+  thaw_return_barrier = 1,\n+  thaw_exception = 2,\n+};\n+\n+static inline int prepare_thaw_internal(JavaThread* thread, bool return_barrier);\n+template<typename ConfigT> static inline intptr_t* thaw_internal(JavaThread* thread, const thaw_kind kind);\n+\n+\n+\/\/ Entry point to freeze. Transitions are handled manually\n+\/\/ Called from generate_cont_doYield() in stubGenerator_<cpu>.cpp through Continuation::freeze_entry();\n+template<typename ConfigT>\n+static JRT_BLOCK_ENTRY(int, freeze(JavaThread* current, intptr_t* sp))\n+  assert(sp == current->frame_anchor()->last_Java_sp(), \"\");\n+\n+  if (current->raw_cont_fastpath() > current->last_continuation()->entry_sp() || current->raw_cont_fastpath() < sp) {\n+    current->set_cont_fastpath(nullptr);\n+  }\n+\n+  return ConfigT::freeze(current, sp);\n+JRT_END\n+\n+JRT_LEAF(int, Continuation::prepare_thaw(JavaThread* thread, bool return_barrier))\n+  return prepare_thaw_internal(thread, return_barrier);\n+JRT_END\n+\n+template<typename ConfigT>\n+static JRT_LEAF(intptr_t*, thaw(JavaThread* thread, int kind))\n+  \/\/ TODO: JRT_LEAF and NoHandleMark is problematic for JFR events.\n+  \/\/ vFrameStreamCommon allocates Handles in RegisterMap for continuations.\n+  \/\/ JRT_ENTRY instead?\n+  ResetNoHandleMark rnhm;\n+\n+  return ConfigT::thaw(thread, (thaw_kind)kind);\n+JRT_END\n+\n+JVM_ENTRY(jint, CONT_isPinned0(JNIEnv* env, jobject cont_scope)) {\n+  JavaThread* thread = JavaThread::thread_from_jni_environment(env);\n+  return is_pinned0(thread, JNIHandles::resolve(cont_scope), false);\n+}\n+JVM_END\n+\n+\/\/\/\/\/\/\/\/\/\/\/\n+\n+enum class oop_kind { NARROW, WIDE };\n+template <oop_kind oops, typename BarrierSetT>\n+class Config {\n+public:\n+  typedef Config<oops, BarrierSetT> SelfT;\n+  typedef typename Conditional<oops == oop_kind::NARROW, narrowOop, oop>::type OopT;\n+\n+  static int freeze(JavaThread* thread, intptr_t* const sp) {\n+    return freeze_internal<SelfT>(thread, sp);\n+  }\n+\n+  static intptr_t* thaw(JavaThread* thread, thaw_kind kind) {\n+    return thaw_internal<SelfT>(thread, kind);\n+  }\n+};\n+\n+static bool stack_overflow_check(JavaThread* thread, int size, address sp) {\n+  const int page_size = os::vm_page_size();\n+  if (size > page_size) {\n+    if (sp - size < thread->stack_overflow_state()->stack_overflow_limit()) {\n+      return false;\n+    }\n+  }\n+  return true;\n+}\n+\n+static oop get_continuation(JavaThread* thread) {\n+  assert(thread != nullptr, \"\");\n+  assert(thread->threadObj() != nullptr, \"\");\n+  return java_lang_Thread::continuation(thread->threadObj());\n+}\n+\n+#ifdef ASSERT\n+inline void clear_anchor(JavaThread* thread) {\n+  thread->frame_anchor()->clear();\n+}\n+\n+static void set_anchor(JavaThread* thread, intptr_t* sp) {\n+  address pc = *(address*)(sp - frame::sender_sp_ret_address_offset());\n+  assert(pc != nullptr, \"\");\n+\n+  JavaFrameAnchor* anchor = thread->frame_anchor();\n+  anchor->set_last_Java_sp(sp);\n+  anchor->set_last_Java_pc(pc);\n+  ContinuationHelper::set_anchor_pd(anchor, sp);\n+\n+  assert(thread->has_last_Java_frame(), \"\");\n+  assert(thread->last_frame().cb() != nullptr, \"\");\n+}\n+#endif \/\/ ASSERT\n+\n+static void set_anchor_to_entry(JavaThread* thread, ContinuationEntry* entry) {\n+  JavaFrameAnchor* anchor = thread->frame_anchor();\n+  anchor->set_last_Java_sp(entry->entry_sp());\n+  anchor->set_last_Java_pc(entry->entry_pc());\n+  ContinuationHelper::set_anchor_to_entry_pd(anchor, entry);\n+\n+  assert(thread->has_last_Java_frame(), \"\");\n+  assert(thread->last_frame().cb() != nullptr, \"\");\n+}\n+\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/ FREEZE \/\/\/\/\n+\n+class FreezeBase : public StackObj {\n+protected:\n+  JavaThread* const _thread;\n+  ContinuationWrapper& _cont;\n+  CONT_JFR_ONLY(FreezeThawJfrInfo _jfr_info;)\n+  bool _barriers;\n+  const bool _preempt; \/\/ used only on the slow path\n+\n+  intptr_t *_bottom_address;\n+\n+  int _size; \/\/ total size of all frames plus metadata in words.\n+  int _align_size;\n+\n+  JvmtiSampledObjectAllocEventCollector* _jvmti_event_collector;\n+\n+  NOT_PRODUCT(int _frames;)\n+  DEBUG_ONLY(intptr_t* _last_write;)\n+\n+  inline FreezeBase(JavaThread* thread, ContinuationWrapper& cont, bool preempt);\n+\n+public:\n+  NOINLINE freeze_result freeze_slow();\n+\n+  CONT_JFR_ONLY(FreezeThawJfrInfo& jfr_info() { return _jfr_info; })\n+  void set_jvmti_event_collector(JvmtiSampledObjectAllocEventCollector* jsoaec) { _jvmti_event_collector = jsoaec; }\n+\n+protected:\n+  inline void init_rest();\n+  void throw_stack_overflow_on_humongous_chunk();\n+\n+  \/\/ fast path\n+  inline void copy_to_chunk(intptr_t* from, intptr_t* to, int size);\n+  inline void unwind_frames();\n+\n+  inline void patch_stack_pd(intptr_t* frame_sp, intptr_t* heap_sp);\n+\n+private:\n+  \/\/ slow path\n+  frame freeze_start_frame();\n+  frame freeze_start_frame_safepoint_stub(frame f);\n+  NOINLINE freeze_result freeze(frame& f, frame& caller, int callee_argsize, bool callee_interpreted, bool top);\n+  inline frame freeze_start_frame_yield_stub(frame f);\n+  template<typename FKind>\n+  inline freeze_result recurse_freeze_java_frame(const frame& f, frame& caller, int fsize, int argsize);\n+  inline void before_freeze_java_frame(const frame& f, const frame& caller, int fsize, int argsize, bool bottom);\n+  inline void after_freeze_java_frame(const frame& hf, bool bottom);\n+  freeze_result finalize_freeze(const frame& callee, frame& caller, int argsize);\n+  void patch(const frame& f, frame& hf, const frame& caller, bool bottom);\n+  NOINLINE freeze_result recurse_freeze_interpreted_frame(frame& f, frame& caller, int callee_argsize, bool callee_interpreted);\n+  freeze_result recurse_freeze_compiled_frame(frame& f, frame& caller, int callee_argsize, bool callee_interpreted);\n+  NOINLINE freeze_result recurse_freeze_stub_frame(frame& f, frame& caller);\n+  NOINLINE void finish_freeze(const frame& f, const frame& top);\n+\n+  inline bool stack_overflow();\n+\n+  static frame sender(const frame& f) { return f.is_interpreted_frame() ? sender<ContinuationHelper::InterpretedFrame>(f)\n+                                                                        : sender<ContinuationHelper::NonInterpretedUnknownFrame>(f); }\n+  template<typename FKind> static inline frame sender(const frame& f);\n+  template<typename FKind> frame new_heap_frame(frame& f, frame& caller);\n+  inline void set_top_frame_metadata_pd(const frame& hf);\n+  inline void patch_pd(frame& callee, const frame& caller);\n+  void adjust_interpreted_frame_unextended_sp(frame& f);\n+  static inline void relativize_interpreted_frame_metadata(const frame& f, const frame& hf);\n+\n+protected:\n+  virtual stackChunkOop allocate_chunk_slow(size_t stack_size) = 0;\n+};\n+\n+template <typename ConfigT>\n+class Freeze : public FreezeBase {\n+private:\n+  stackChunkOop allocate_chunk(size_t stack_size);\n+\n+public:\n+  inline Freeze(JavaThread* thread, ContinuationWrapper& cont, bool preempt)\n+    : FreezeBase(thread, cont, preempt) {}\n+\n+  inline bool is_chunk_available(intptr_t* frame_sp\n+#ifdef ASSERT\n+    , int* out_size = nullptr\n+#endif\n+  );\n+  template <bool chunk_available> freeze_result try_freeze_fast(intptr_t* sp);\n+  template <bool chunk_available> bool freeze_fast(intptr_t* frame_sp);\n+\n+protected:\n+  virtual stackChunkOop allocate_chunk_slow(size_t stack_size) override { return allocate_chunk(stack_size); }\n+};\n+\n+FreezeBase::FreezeBase(JavaThread* thread, ContinuationWrapper& cont, bool preempt) :\n+    _thread(thread), _cont(cont), _barriers(false), _preempt(preempt) {\n+  DEBUG_ONLY(_jvmti_event_collector = nullptr;)\n+\n+  assert(_thread != nullptr, \"\");\n+  assert(_thread->last_continuation()->entry_sp() == _cont.entrySP(), \"\");\n+\n+  _bottom_address = _cont.entrySP() - _cont.argsize();\n+  DEBUG_ONLY(_cont.entry()->verify_cookie();)\n+\n+  assert(!Interpreter::contains(_cont.entryPC()), \"\");\n+\n+#ifdef _LP64\n+  if (((intptr_t)_bottom_address & 0xf) != 0) {\n+    _bottom_address--;\n+  }\n+  assert(is_aligned(_bottom_address, frame::frame_alignment), \"\");\n+#endif\n+\n+  log_develop_trace(continuations)(\"bottom_address: \" INTPTR_FORMAT \" entrySP: \" INTPTR_FORMAT \" argsize: \" PTR_FORMAT,\n+                p2i(_bottom_address), p2i(_cont.entrySP()), (_cont.entrySP() - _bottom_address) << LogBytesPerWord);\n+  assert(_bottom_address != nullptr, \"\");\n+  assert(_bottom_address <= _cont.entrySP(), \"\");\n+  DEBUG_ONLY(_last_write = nullptr;)\n+}\n+\n+void FreezeBase::init_rest() { \/\/ we want to postpone some initialization after chunk handling\n+  _size = 0;\n+  _align_size = 0;\n+  NOT_PRODUCT(_frames = 0;)\n+}\n+\n+void FreezeBase::copy_to_chunk(intptr_t* from, intptr_t* to, int size) {\n+  stackChunkOop chunk = _cont.tail();\n+  chunk->copy_from_stack_to_chunk(from, to, size);\n+  CONT_JFR_ONLY(_jfr_info.record_size_copied(size);)\n+\n+#ifdef ASSERT\n+  if (_last_write != nullptr) {\n+    assert(_last_write == to + size, \"Missed a spot: _last_write: \" INTPTR_FORMAT \" to+size: \" INTPTR_FORMAT\n+        \" stack_size: %d _last_write offset: \" PTR_FORMAT \" to+size: \" PTR_FORMAT, p2i(_last_write), p2i(to+size),\n+        chunk->stack_size(), _last_write-chunk->start_address(), to+size-chunk->start_address());\n+    _last_write = to;\n+  }\n+#endif\n+}\n+\n+\/\/ Called _after_ the last possible safepoint during the freeze operation (chunk allocation)\n+void FreezeBase::unwind_frames() {\n+  ContinuationEntry* entry = _cont.entry();\n+  entry->flush_stack_processing(_thread);\n+  set_anchor_to_entry(_thread, entry);\n+}\n+\n+template <typename ConfigT>\n+template <bool chunk_available>\n+freeze_result Freeze<ConfigT>::try_freeze_fast(intptr_t* sp) {\n+  if (freeze_fast<chunk_available>(sp)) {\n+    return freeze_ok;\n+  }\n+  if (_thread->has_pending_exception()) {\n+    return freeze_exception;\n+  }\n+\n+  EventContinuationFreezeOld e;\n+  if (e.should_commit()) {\n+    e.set_id(cast_from_oop<u8>(_cont.continuation()));\n+    e.commit();\n+  }\n+  \/\/ TODO R REMOVE when deopt change is fixed\n+  assert(!_thread->cont_fastpath() || _barriers, \"\");\n+  log_develop_trace(continuations)(\"-- RETRYING SLOW --\");\n+  return freeze_slow();\n+}\n+\n+\/\/ returns true iff there's room in the chunk for a fast, compiled-frame-only freeze\n+template <typename ConfigT>\n+bool Freeze<ConfigT>::is_chunk_available(intptr_t* frame_sp\n+#ifdef ASSERT\n+    , int* out_size\n+#endif\n+  ) {\n+  stackChunkOop chunk = _cont.tail();\n+  if (chunk == nullptr || chunk->is_gc_mode() || chunk->requires_barriers() || chunk->has_mixed_frames()) {\n+    log_develop_trace(continuations)(\"is_chunk_available %s\", chunk == nullptr ? \"no chunk\" : \"chunk requires barriers\");\n+    return false;\n+  }\n+\n+  \/\/ assert(CodeCache::find_blob(*(address*)(frame_sp - SENDER_SP_RET_ADDRESS_OFFSET)) == StubRoutines::cont_doYield_stub(), \"\"); -- fails on Windows\n+  assert(StubRoutines::cont_doYield_stub()->frame_size() == frame::metadata_words, \"\");\n+  intptr_t* const stack_top     = frame_sp + frame::metadata_words;\n+  intptr_t* const stack_bottom  = _cont.entrySP() - ContinuationHelper::frame_align_words(_cont.argsize());\n+\n+  int size = stack_bottom - stack_top; \/\/ in words\n+\n+  const int chunk_sp = chunk->sp();\n+  if (chunk_sp < chunk->stack_size()) {\n+    size -= _cont.argsize();\n+  }\n+  assert(size > 0, \"\");\n+\n+  bool available = chunk_sp - frame::metadata_words >= size;\n+  log_develop_trace(continuations)(\"is_chunk_available: %d size: %d argsize: %d top: \" INTPTR_FORMAT \" bottom: \" INTPTR_FORMAT,\n+    available, _cont.argsize(), size, p2i(stack_top), p2i(stack_bottom));\n+  DEBUG_ONLY(if (out_size != nullptr) *out_size = size;)\n+  return available;\n+}\n+\n+template <typename ConfigT>\n+template <bool chunk_available>\n+bool Freeze<ConfigT>::freeze_fast(intptr_t* frame_sp) {\n+  assert(_cont.chunk_invariant(tty), \"\");\n+  assert(!Interpreter::contains(_cont.entryPC()), \"\");\n+  assert(StubRoutines::cont_doYield_stub()->frame_size() == frame::metadata_words, \"\");\n+\n+  \/\/ properties of the continuation on the stack; all sizes are in words\n+  intptr_t* const cont_stack_top    = frame_sp + frame::metadata_words; \/\/ we add metadata_words to skip the doYield stub frame\n+  intptr_t* const cont_stack_bottom = _cont.entrySP() - ContinuationHelper::frame_align_words(_cont.argsize()); \/\/ see alignment in thaw\n+\n+  const int cont_size = cont_stack_bottom - cont_stack_top;\n+\n+  log_develop_trace(continuations)(\"freeze_fast size: %d argsize: %d top: \" INTPTR_FORMAT \" bottom: \" INTPTR_FORMAT,\n+    cont_size, _cont.argsize(), p2i(cont_stack_top), p2i(cont_stack_bottom));\n+  assert(cont_size > 0, \"\");\n+\n+#ifdef ASSERT\n+  bool empty = true;\n+  int is_chunk_available_size;\n+  bool is_chunk_available0 = is_chunk_available(frame_sp, &is_chunk_available_size);\n+  intptr_t* orig_chunk_sp = nullptr;\n+  CONT_JFR_ONLY(bool chunk_is_allocated = false;)\n+#endif\n+\n+  stackChunkOop chunk = _cont.tail();\n+  int chunk_start_sp; \/\/ the chunk's sp before the freeze, adjusted to point beyond the stack-passed arguments in the topmost frame\n+  if (chunk_available) { \/\/ LIKELY\n+    DEBUG_ONLY(orig_chunk_sp = chunk->sp_address();)\n+\n+    assert(is_chunk_available0, \"\");\n+\n+    if (chunk->sp() < chunk->stack_size()) { \/\/ we are copying into a non-empty chunk\n+      DEBUG_ONLY(empty = false;)\n+      assert(chunk->sp() < (chunk->stack_size() - chunk->argsize()), \"\");\n+      assert(*(address*)(chunk->sp_address() - frame::sender_sp_ret_address_offset()) == chunk->pc(), \"\");\n+\n+      chunk_start_sp = chunk->sp() + _cont.argsize(); \/\/ we overlap; we'll overwrite the chunk's top frame's callee arguments\n+      assert(chunk_start_sp <= chunk->stack_size(), \"sp not pointing into stack\");\n+\n+      \/\/ increase max_size by what we're freezing minus the overlap\n+      chunk->set_max_size(chunk->max_size() + cont_size - _cont.argsize());\n+\n+      intptr_t* const bottom_sp = cont_stack_bottom - _cont.argsize();\n+      assert(bottom_sp == _bottom_address, \"\");\n+      \/\/ Because the chunk isn't empty, we know there's a caller in the chunk, therefore the bottom-most frame\n+      \/\/ should have a return barrier (installed back when we thawed it).\n+      assert(*(address*)(bottom_sp-frame::sender_sp_ret_address_offset()) == StubRoutines::cont_returnBarrier(),\n+             \"should be the continuation return barrier\");\n+      \/\/ We copy the fp from the chunk back to the stack because it contains some caller data,\n+      \/\/ including, possibly, an oop that might have gone stale since we thawed.\n+      patch_stack_pd(bottom_sp, chunk->sp_address());\n+      \/\/ we don't patch the return pc at this time, so as not to make the stack unwalkable for async walks\n+    } else { \/\/ the chunk is empty\n+      chunk_start_sp = chunk->sp();\n+\n+      assert(chunk_start_sp == chunk->stack_size(), \"\");\n+\n+      chunk->set_max_size(cont_size);\n+      chunk->set_argsize(_cont.argsize());\n+    }\n+  } else { \/\/ no chunk; allocate\n+    assert(_thread->thread_state() == _thread_in_vm, \"\");\n+    assert(!is_chunk_available(frame_sp), \"\");\n+    assert(_thread->cont_fastpath(), \"\");\n+\n+    chunk = allocate_chunk(cont_size + frame::metadata_words);\n+    if (UNLIKELY(chunk == nullptr || !_thread->cont_fastpath() || _barriers)) { \/\/ OOME\/probably humongous\n+      log_develop_trace(continuations)(\"Retrying slow. Barriers: %d\", _barriers);\n+      return false;\n+    }\n+\n+    chunk->set_max_size(cont_size);\n+    chunk->set_argsize(_cont.argsize());\n+\n+    \/\/ in a fresh chunk, we freeze *with* the bottom-most frame's stack arguments.\n+    \/\/ They'll then be stored twice: in the chunk and in the parent chunk's top frame\n+    chunk_start_sp = cont_size + frame::metadata_words;\n+    assert(chunk_start_sp == chunk->stack_size(), \"\");\n+\n+    DEBUG_ONLY(CONT_JFR_ONLY(chunk_is_allocated = true;))\n+    DEBUG_ONLY(orig_chunk_sp = chunk->start_address() + chunk_start_sp;)\n+  }\n+\n+  assert(chunk != nullptr, \"\");\n+  assert(!chunk->has_mixed_frames(), \"\");\n+  assert(!chunk->is_gc_mode(), \"\");\n+  assert(!chunk->has_bitmap(), \"\");\n+  assert(!chunk->requires_barriers(), \"\");\n+  assert(chunk == _cont.tail(), \"\");\n+\n+  \/\/ We unwind frames after the last safepoint so that the GC will have found the oops in the frames, but before\n+  \/\/ writing into the chunk. This is so that an asynchronous stack walk (not at a safepoint) that suspends us here\n+  \/\/ will either see no continuation on the stack, or a consistent chunk.\n+  unwind_frames();\n+\n+  log_develop_trace(continuations)(\"freeze_fast start: chunk \" INTPTR_FORMAT \" size: %d orig sp: %d argsize: %d\",\n+    p2i((oopDesc*)chunk), chunk->stack_size(), chunk_start_sp, _cont.argsize());\n+  assert(chunk_start_sp <= chunk->stack_size(), \"\");\n+  assert(chunk_start_sp >= cont_size, \"no room in the chunk\");\n+\n+  const int chunk_new_sp = chunk_start_sp - cont_size; \/\/ the chunk's new sp, after freeze\n+  assert(!is_chunk_available0 || orig_chunk_sp - (chunk->start_address() + chunk_new_sp) == is_chunk_available_size, \"\");\n+\n+  intptr_t* chunk_top = chunk->start_address() + chunk_new_sp;\n+  assert(empty || *(address*)(orig_chunk_sp - frame::sender_sp_ret_address_offset()) == chunk->pc(), \"\");\n+\n+  log_develop_trace(continuations)(\"freeze_fast start: \" INTPTR_FORMAT \" sp: %d chunk_top: \" INTPTR_FORMAT,\n+                              p2i(chunk->start_address()), chunk_new_sp, p2i(chunk_top));\n+  intptr_t* from = cont_stack_top - frame::metadata_words;\n+  intptr_t* to   = chunk_top - frame::metadata_words;\n+  copy_to_chunk(from, to, cont_size + frame::metadata_words);\n+  \/\/ Because we're not patched yet, the chunk is now in a bad state\n+\n+  \/\/ patch return pc of the bottom-most frozen frame (now in the chunk) with the actual caller's return address\n+  intptr_t* chunk_bottom_sp = chunk_top + cont_size - _cont.argsize();\n+  assert(empty || *(address*)(chunk_bottom_sp-frame::sender_sp_ret_address_offset()) == StubRoutines::cont_returnBarrier(), \"\");\n+  *(address*)(chunk_bottom_sp - frame::sender_sp_ret_address_offset()) = chunk->pc();\n+\n+  \/\/ We're always writing to a young chunk, so the GC can't see it until the next safepoint.\n+  chunk->set_sp(chunk_new_sp);\n+  \/\/ set chunk->pc to the return address of the topmost frame in the chunk\n+  chunk->set_pc(*(address*)(cont_stack_top - frame::sender_sp_ret_address_offset()));\n+\n+  _cont.write();\n+\n+  log_develop_trace(continuations)(\"FREEZE CHUNK #\" INTPTR_FORMAT \" (young)\", _cont.hash());\n+  LogTarget(Trace, continuations) lt;\n+  if (lt.develop_is_enabled()) {\n+    LogStream ls(lt);\n+    chunk->print_on(true, &ls);\n+  }\n+\n+  \/\/ Verification\n+  assert(_cont.chunk_invariant(tty), \"\");\n+  chunk->verify();\n+\n+#if CONT_JFR\n+  EventContinuationFreezeYoung e;\n+  if (e.should_commit()) {\n+    e.set_id(cast_from_oop<u8>(chunk));\n+    DEBUG_ONLY(e.set_allocate(chunk_is_allocated);)\n+    e.set_size(cont_size << LogBytesPerWord);\n+    e.commit();\n+  }\n+#endif\n+\n+  return true;\n+}\n+\n+NOINLINE freeze_result FreezeBase::freeze_slow() {\n+#ifdef ASSERT\n+  ResourceMark rm;\n+#endif\n+\n+  log_develop_trace(continuations)(\"freeze_slow  #\" INTPTR_FORMAT, _cont.hash());\n+  assert(_thread->thread_state() == _thread_in_vm || _thread->thread_state() == _thread_blocked, \"\");\n+\n+  init_rest();\n+\n+  HandleMark hm(Thread::current());\n+\n+  frame f = freeze_start_frame();\n+\n+  LogTarget(Debug, continuations) lt;\n+  if (lt.develop_is_enabled()) {\n+    LogStream ls(lt);\n+    f.print_on(&ls);\n+  }\n+\n+  frame caller;\n+  freeze_result res = freeze(f, caller, 0, false, true);\n+\n+  if (res == freeze_ok) {\n+    finish_freeze(f, caller);\n+    _cont.write();\n+  }\n+\n+  return res;\n+}\n+\n+frame FreezeBase::freeze_start_frame() {\n+  frame f = _thread->last_frame();\n+  if (LIKELY(!_preempt)) {\n+    assert(StubRoutines::cont_doYield_stub()->contains(f.pc()), \"\");\n+    return freeze_start_frame_yield_stub(f);\n+  } else {\n+    return freeze_start_frame_safepoint_stub(f);\n+  }\n+}\n+\n+frame FreezeBase::freeze_start_frame_yield_stub(frame f) {\n+  assert(StubRoutines::cont_doYield_stub()->contains(f.pc()), \"must be\");\n+  f = sender<ContinuationHelper::StubFrame>(f);\n+  return f;\n+}\n+\n+frame FreezeBase::freeze_start_frame_safepoint_stub(frame f) {\n+#if (defined(X86) || defined(AARCH64)) && !defined(ZERO)\n+  f.set_fp(f.real_fp()); \/\/ f.set_fp(*Frame::callee_link_address(f)); \/\/ ????\n+#else\n+  Unimplemented();\n+#endif\n+  if (!Interpreter::contains(f.pc())) {\n+    assert(ContinuationHelper::Frame::is_stub(f.cb()), \"must be\");\n+    assert(f.oop_map() != nullptr, \"must be\");\n+\n+    if (Interpreter::contains(ContinuationHelper::StubFrame::return_pc(f))) {\n+      f = sender<ContinuationHelper::StubFrame>(f); \/\/ Safepoint stub in interpreter\n+    }\n+  }\n+  return f;\n+}\n+\n+NOINLINE freeze_result FreezeBase::freeze(frame& f, frame& caller, int callee_argsize, bool callee_interpreted, bool top) {\n+  assert(f.unextended_sp() < _bottom_address, \"\"); \/\/ see recurse_freeze_java_frame\n+  assert(f.is_interpreted_frame() || ((top && _preempt) == ContinuationHelper::Frame::is_stub(f.cb())), \"\");\n+\n+  if (stack_overflow()) {\n+    return freeze_exception;\n+  }\n+\n+  if (f.is_compiled_frame()) {\n+    if (UNLIKELY(f.oop_map() == nullptr)) {\n+      \/\/ special native frame\n+      return freeze_pinned_native;\n+    }\n+    if (UNLIKELY(ContinuationHelper::CompiledFrame::is_owning_locks(_cont.thread(), SmallRegisterMap::instance, f))) {\n+      return freeze_pinned_monitor;\n+    }\n+\n+    return recurse_freeze_compiled_frame(f, caller, callee_argsize, callee_interpreted);\n+  } else if (f.is_interpreted_frame()) {\n+    assert((_preempt && top) || !f.interpreter_frame_method()->is_native(), \"\");\n+    if (ContinuationHelper::InterpretedFrame::is_owning_locks(f)) {\n+      return freeze_pinned_monitor;\n+    }\n+    if (_preempt && top && f.interpreter_frame_method()->is_native()) {\n+      \/\/ int native entry\n+      return freeze_pinned_native;\n+    }\n+\n+    return recurse_freeze_interpreted_frame(f, caller, callee_argsize, callee_interpreted);\n+  } else if (_preempt && top && ContinuationHelper::Frame::is_stub(f.cb())) {\n+    return recurse_freeze_stub_frame(f, caller);\n+  } else {\n+    return freeze_pinned_native;\n+  }\n+}\n+\n+template<typename FKind>\n+inline freeze_result FreezeBase::recurse_freeze_java_frame(const frame& f, frame& caller, int fsize, int argsize) {\n+  assert(FKind::is_instance(f), \"\");\n+\n+  assert(fsize > 0, \"\");\n+  assert(argsize >= 0, \"\");\n+  _size += fsize;\n+  NOT_PRODUCT(_frames++;)\n+\n+  if (FKind::frame_bottom(f) >= _bottom_address - 1) { \/\/ sometimes there's space after enterSpecial\n+    return finalize_freeze(f, caller, argsize); \/\/ recursion end\n+  } else {\n+    frame senderf = sender<FKind>(f);\n+    assert(FKind::interpreted || senderf.sp() == senderf.unextended_sp(), \"\");\n+    freeze_result result = freeze(senderf, caller, argsize, FKind::interpreted, false); \/\/ recursive call\n+    return result;\n+  }\n+}\n+\n+inline void FreezeBase::before_freeze_java_frame(const frame& f, const frame& caller, int fsize, int argsize, bool bottom) {\n+  LogTarget(Trace, continuations) lt;\n+  if (lt.develop_is_enabled()) {\n+    LogStream ls(lt);\n+    ls.print_cr(\"======== FREEZING FRAME interpreted: %d bottom: %d\", f.is_interpreted_frame(), bottom);\n+    ls.print_cr(\"fsize: %d argsize: %d\", fsize, argsize);\n+    f.print_on(&ls);\n+  }\n+  assert(caller.is_interpreted_frame() == Interpreter::contains(caller.pc()), \"\");\n+}\n+\n+inline void FreezeBase::after_freeze_java_frame(const frame& hf, bool bottom) {\n+  LogTarget(Trace, continuations) lt;\n+  if (lt.develop_is_enabled()) {\n+    LogStream ls(lt);\n+    DEBUG_ONLY(hf.print_value_on(&ls, nullptr);)\n+    assert(hf.is_heap_frame(), \"should be\");\n+    DEBUG_ONLY(print_frame_layout(hf, false, &ls);)\n+    if (bottom) {\n+      ls.print_cr(\"bottom h-frame:\");\n+      hf.print_on(&ls);\n+    }\n+  }\n+}\n+\n+freeze_result FreezeBase::finalize_freeze(const frame& callee, frame& caller, int argsize) {\n+  assert(callee.is_interpreted_frame()\n+    || callee.cb()->as_nmethod()->is_osr_method()\n+    || argsize == _cont.argsize(), \"argsize: %d cont.argsize: %d\", argsize, _cont.argsize());\n+  log_develop_trace(continuations)(\"bottom: \" INTPTR_FORMAT \" count %d size: %d argsize: %d\",\n+    p2i(_bottom_address), _frames, _size << LogBytesPerWord, argsize);\n+\n+  LogTarget(Trace, continuations) lt;\n+\n+#ifdef ASSERT\n+  bool empty = _cont.is_empty();\n+  log_develop_trace(continuations)(\"empty: %d\", empty);\n+#endif\n+\n+  stackChunkOop chunk = _cont.tail();\n+\n+  assert(chunk == nullptr || (chunk->max_size() == 0) == chunk->is_empty(), \"\");\n+\n+  _size += frame::metadata_words; \/\/ for top frame's metadata\n+\n+  int overlap = 0; \/\/ the args overlap the caller -- if there is one in this chunk and is of the same kind\n+  int unextended_sp = -1;\n+  if (chunk != nullptr) {\n+    unextended_sp = chunk->sp();\n+    if (!chunk->is_empty()) {\n+      bool top_interpreted = Interpreter::contains(chunk->pc());\n+      unextended_sp = chunk->sp();\n+      if (top_interpreted) {\n+        StackChunkFrameStream<ChunkFrames::Mixed> last(chunk);\n+        unextended_sp += last.unextended_sp() - last.sp(); \/\/ can be negative (-1), often with lambda forms\n+      }\n+      if (callee.is_interpreted_frame() == top_interpreted) {\n+        overlap = argsize;\n+      }\n+    }\n+  }\n+\n+  log_develop_trace(continuations)(\"finalize _size: %d overlap: %d unextended_sp: %d\", _size, overlap, unextended_sp);\n+\n+  _size -= overlap;\n+  assert(_size >= 0, \"\");\n+\n+  assert(chunk == nullptr || chunk->is_empty()\n+          || unextended_sp == chunk->to_offset(StackChunkFrameStream<ChunkFrames::Mixed>(chunk).unextended_sp()), \"\");\n+  assert(chunk != nullptr || unextended_sp < _size, \"\");\n+\n+    \/\/ _barriers can be set to true by an allocation in freeze_fast, in which case the chunk is available\n+  assert(!_barriers || (unextended_sp >= _size && chunk->is_empty()),\n+    \"unextended_sp: %d size: %d is_empty: %d\", unextended_sp, _size, chunk->is_empty());\n+\n+  DEBUG_ONLY(bool empty_chunk = true);\n+  if (unextended_sp < _size || chunk->is_gc_mode() || (!_barriers && chunk->requires_barriers())) {\n+    \/\/ ALLOCATION\n+\n+    if (lt.develop_is_enabled()) {\n+      LogStream ls(lt);\n+      if (chunk == nullptr) {\n+        ls.print_cr(\"no chunk\");\n+      } else {\n+        ls.print_cr(\"chunk barriers: %d _size: %d free size: %d\",\n+          chunk->requires_barriers(), _size, chunk->sp() - frame::metadata_words);\n+        chunk->print_on(&ls);\n+      }\n+    }\n+\n+    _size += overlap; \/\/ we're allocating a new chunk, so no overlap\n+    \/\/ overlap = 0;\n+\n+    chunk = allocate_chunk_slow(_size);\n+    if (chunk == nullptr) {\n+      return freeze_exception;\n+    }\n+\n+    int sp = chunk->stack_size() - argsize;\n+    chunk->set_sp(sp);\n+    chunk->set_argsize(argsize);\n+    assert(chunk->is_empty(), \"\");\n+  } else {\n+    log_develop_trace(continuations)(\"Reusing chunk mixed: %d empty: %d\", chunk->has_mixed_frames(), chunk->is_empty());\n+    if (chunk->is_empty()) {\n+      int sp = chunk->stack_size() - argsize;\n+      chunk->set_sp(sp);\n+      chunk->set_argsize(argsize);\n+      _size += overlap;\n+      assert(chunk->max_size() == 0, \"\");\n+    } DEBUG_ONLY(else empty_chunk = false;)\n+  }\n+  chunk->set_has_mixed_frames(true);\n+\n+  assert(chunk->requires_barriers() == _barriers, \"\");\n+  assert(!_barriers || chunk->is_empty(), \"\");\n+\n+  assert(!chunk->has_bitmap(), \"\");\n+  assert(!chunk->is_empty() || StackChunkFrameStream<ChunkFrames::Mixed>(chunk).is_done(), \"\");\n+  assert(!chunk->is_empty() || StackChunkFrameStream<ChunkFrames::Mixed>(chunk).to_frame().is_empty(), \"\");\n+\n+  \/\/ We unwind frames after the last safepoint so that the GC will have found the oops in the frames, but before\n+  \/\/ writing into the chunk. This is so that an asynchronous stack walk (not at a safepoint) that suspends us here\n+  \/\/ will either see no continuation or a consistent chunk.\n+  unwind_frames();\n+\n+  chunk->set_max_size(chunk->max_size() + _size - frame::metadata_words);\n+\n+  if (lt.develop_is_enabled()) {\n+    LogStream ls(lt);\n+    ls.print_cr(\"top chunk:\");\n+    chunk->print_on(&ls);\n+  }\n+\n+  caller = StackChunkFrameStream<ChunkFrames::Mixed>(chunk).to_frame();\n+\n+  DEBUG_ONLY(_last_write = caller.unextended_sp() + (empty_chunk ? argsize : overlap);)\n+  assert(chunk->is_in_chunk(_last_write - _size),\n+    \"last_write-size: \" INTPTR_FORMAT \" start: \" INTPTR_FORMAT, p2i(_last_write-_size), p2i(chunk->start_address()));\n+#ifdef ASSERT\n+  if (lt.develop_is_enabled()) {\n+    LogStream ls(lt);\n+    ls.print_cr(\"top hframe before (freeze):\");\n+    assert(caller.is_heap_frame(), \"should be\");\n+    caller.print_on(&ls);\n+  }\n+\n+  assert(!empty || Continuation::is_continuation_entry_frame(callee, nullptr), \"\");\n+\n+  frame entry = sender(callee);\n+\n+  assert(Continuation::is_return_barrier_entry(entry.pc()) || Continuation::is_continuation_enterSpecial(entry), \"\");\n+  assert(callee.is_interpreted_frame() || entry.sp() == entry.unextended_sp(), \"\");\n+#endif\n+\n+  return freeze_ok_bottom;\n+}\n+\n+void FreezeBase::patch(const frame& f, frame& hf, const frame& caller, bool bottom) {\n+  if (bottom) {\n+    address last_pc = caller.pc();\n+    assert((last_pc == nullptr) == _cont.tail()->is_empty(), \"\");\n+    ContinuationHelper::Frame::patch_pc(caller, last_pc);\n+  } else {\n+    assert(!caller.is_empty(), \"\");\n+  }\n+\n+  patch_pd(hf, caller);\n+\n+  if (f.is_interpreted_frame()) {\n+    assert(hf.is_heap_frame(), \"should be\");\n+    ContinuationHelper::InterpretedFrame::patch_sender_sp(hf, caller.unextended_sp());\n+  }\n+\n+#ifdef ASSERT\n+  if (hf.is_compiled_frame()) {\n+    if (f.is_deoptimized_frame()) { \/\/ TODO DEOPT: long term solution: unroll on freeze and patch pc\n+      log_develop_trace(continuations)(\"Freezing deoptimized frame\");\n+      assert(f.cb()->as_compiled_method()->is_deopt_pc(f.raw_pc()), \"\");\n+      assert(f.cb()->as_compiled_method()->is_deopt_pc(ContinuationHelper::Frame::real_pc(f)), \"\");\n+    }\n+  }\n+#endif\n+}\n+\n+#ifdef ASSERT\n+static void verify_frame_top(const frame& f, intptr_t* top) {\n+  ResourceMark rm;\n+  InterpreterOopMap mask;\n+  f.interpreted_frame_oop_map(&mask);\n+  assert(top <= ContinuationHelper::InterpretedFrame::frame_top(f, &mask),\n+         \"frame_sp: \" INTPTR_FORMAT \" Interpreted::frame_top: \" INTPTR_FORMAT,\n+           p2i(top), p2i(ContinuationHelper::InterpretedFrame::frame_top(f, &mask)));\n+}\n+#endif \/\/ ASSERT\n+\n+NOINLINE freeze_result FreezeBase::recurse_freeze_interpreted_frame(frame& f, frame& caller,\n+                                                                    int callee_argsize,\n+                                                                    bool callee_interpreted) {\n+  adjust_interpreted_frame_unextended_sp(f);\n+\n+  intptr_t* const frame_sp = ContinuationHelper::InterpretedFrame::frame_top(f, callee_argsize, callee_interpreted);\n+  const int argsize = ContinuationHelper::InterpretedFrame::stack_argsize(f);\n+  const int locals = f.interpreter_frame_method()->max_locals();\n+  assert(ContinuationHelper::InterpretedFrame::frame_bottom(f) >= f.fp() + frame::metadata_words + locals, \"\");\/\/ = on x86\n+  const int fsize = f.fp() + frame::metadata_words + locals - frame_sp;\n+\n+  DEBUG_ONLY(verify_frame_top(f, frame_sp));\n+\n+  Method* frame_method = ContinuationHelper::Frame::frame_method(f);\n+\n+  log_develop_trace(continuations)(\"recurse_freeze_interpreted_frame %s _size: %d fsize: %d argsize: %d\",\n+    frame_method->name_and_sig_as_C_string(), _size, fsize, argsize);\n+  \/\/ we'd rather not yield inside methods annotated with @JvmtiMountTransition\n+  assert(!ContinuationHelper::Frame::frame_method(f)->jvmti_mount_transition(), \"\");\n+\n+  freeze_result result = recurse_freeze_java_frame<ContinuationHelper::InterpretedFrame>(f, caller, fsize, argsize);\n+  if (UNLIKELY(result > freeze_ok_bottom)) {\n+    return result;\n+  }\n+\n+  bool bottom = result == freeze_ok_bottom;\n+\n+  DEBUG_ONLY(before_freeze_java_frame(f, caller, fsize, 0, bottom);)\n+\n+  frame hf = new_heap_frame<ContinuationHelper::InterpretedFrame>(f, caller);\n+  _align_size += frame::align_wiggle; \/\/ add alignment room for internal interpreted frame alignment om AArch64\n+\n+  intptr_t* heap_sp = ContinuationHelper::InterpretedFrame::frame_top(hf, callee_argsize, callee_interpreted);\n+  assert(ContinuationHelper::InterpretedFrame::frame_bottom(hf) == heap_sp + fsize, \"\");\n+\n+  \/\/ on AArch64 we add padding between the locals and the rest of the frame to keep the fp 16-byte-aligned\n+  copy_to_chunk(ContinuationHelper::InterpretedFrame::frame_bottom(f) - locals,\n+                ContinuationHelper::InterpretedFrame::frame_bottom(hf) - locals, locals); \/\/ copy locals\n+  copy_to_chunk(frame_sp, heap_sp, fsize - locals); \/\/ copy rest\n+  assert(!bottom || !caller.is_interpreted_frame() || (heap_sp + fsize) == (caller.unextended_sp() + argsize), \"\");\n+\n+  relativize_interpreted_frame_metadata(f, hf);\n+\n+  patch(f, hf, caller, bottom);\n+\n+  CONT_JFR_ONLY(_jfr_info.record_interpreted_frame();)\n+  DEBUG_ONLY(after_freeze_java_frame(hf, bottom);)\n+  caller = hf;\n+\n+  \/\/ Mark frame_method's GC epoch for class redefinition on_stack calculation.\n+  frame_method->record_gc_epoch();\n+\n+  return freeze_ok;\n+}\n+\n+freeze_result FreezeBase::recurse_freeze_compiled_frame(frame& f, frame& caller, int callee_argsize, bool callee_interpreted) {\n+  intptr_t* const frame_sp = ContinuationHelper::CompiledFrame::frame_top(f, callee_argsize, callee_interpreted);\n+  const int argsize = ContinuationHelper::CompiledFrame::stack_argsize(f);\n+  const int fsize = ContinuationHelper::CompiledFrame::frame_bottom(f) + argsize - frame_sp;\n+\n+  log_develop_trace(continuations)(\"recurse_freeze_compiled_frame %s _size: %d fsize: %d argsize: %d\",\n+                             ContinuationHelper::Frame::frame_method(f) != nullptr ?\n+                             ContinuationHelper::Frame::frame_method(f)->name_and_sig_as_C_string() : \"\",\n+                             _size, fsize, argsize);\n+  \/\/ we'd rather not yield inside methods annotated with @JvmtiMountTransition\n+  assert(!ContinuationHelper::Frame::frame_method(f)->jvmti_mount_transition(), \"\");\n+\n+  freeze_result result = recurse_freeze_java_frame<ContinuationHelper::CompiledFrame>(f, caller, fsize, argsize);\n+  if (UNLIKELY(result > freeze_ok_bottom)) {\n+    return result;\n+  }\n+\n+  bool bottom = result == freeze_ok_bottom;\n+\n+  DEBUG_ONLY(before_freeze_java_frame(f, caller, fsize, argsize, bottom);)\n+\n+  frame hf = new_heap_frame<ContinuationHelper::CompiledFrame>(f, caller);\n+\n+  intptr_t* heap_sp = ContinuationHelper::CompiledFrame::frame_top(hf, callee_argsize, callee_interpreted);\n+\n+  copy_to_chunk(frame_sp, heap_sp, fsize);\n+  assert(!bottom || !caller.is_compiled_frame() || (heap_sp + fsize) == (caller.unextended_sp() + argsize), \"\");\n+\n+  if (caller.is_interpreted_frame()) {\n+    _align_size += frame::align_wiggle; \/\/ See Thaw::align\n+  }\n+\n+  patch(f, hf, caller, bottom);\n+\n+  assert(bottom || Interpreter::contains(ContinuationHelper::CompiledFrame::real_pc(caller)) == caller.is_interpreted_frame(), \"\");\n+\n+  DEBUG_ONLY(after_freeze_java_frame(hf, bottom);)\n+  caller = hf;\n+  return freeze_ok;\n+}\n+\n+NOINLINE freeze_result FreezeBase::recurse_freeze_stub_frame(frame& f, frame& caller) {\n+  intptr_t* const frame_sp = ContinuationHelper::StubFrame::frame_top(f, 0, 0);\n+  const int fsize = f.cb()->frame_size();\n+\n+  log_develop_trace(continuations)(\"recurse_freeze_stub_frame %s _size: %d fsize: %d :: \" INTPTR_FORMAT \" - \" INTPTR_FORMAT,\n+    f.cb()->name(), _size, fsize, p2i(frame_sp), p2i(frame_sp+fsize));\n+\n+  \/\/ recurse_freeze_java_frame and freeze inlined here because we need to use a full RegisterMap for lock ownership\n+  NOT_PRODUCT(_frames++;)\n+  _size += fsize;\n+\n+  RegisterMap map(_cont.thread(), true, false, false);\n+  map.set_include_argument_oops(false);\n+  ContinuationHelper::update_register_map<ContinuationHelper::StubFrame>(f, &map);\n+  f.oop_map()->update_register_map(&f, &map); \/\/ we have callee-save registers in this case\n+  frame senderf = sender<ContinuationHelper::StubFrame>(f);\n+  assert(senderf.unextended_sp() < _bottom_address - 1, \"\");\n+  assert(senderf.is_compiled_frame(), \"\");\n+\n+  if (UNLIKELY(senderf.oop_map() == nullptr)) {\n+    \/\/ native frame\n+    return freeze_pinned_native;\n+  }\n+  if (UNLIKELY(ContinuationHelper::CompiledFrame::is_owning_locks(_cont.thread(), &map, senderf))) {\n+    return freeze_pinned_monitor;\n+  }\n+\n+  freeze_result result = recurse_freeze_compiled_frame(senderf, caller, 0, 0); \/\/ This might be deoptimized\n+  if (UNLIKELY(result > freeze_ok_bottom)) {\n+    return result;\n+  }\n+  assert(result != freeze_ok_bottom, \"\");\n+  assert(!caller.is_interpreted_frame(), \"\");\n+\n+  DEBUG_ONLY(before_freeze_java_frame(f, caller, fsize, 0, false);)\n+  frame hf = new_heap_frame<ContinuationHelper::StubFrame>(f, caller);\n+  intptr_t* heap_sp = ContinuationHelper::StubFrame::frame_top(hf, 0, 0);\n+  copy_to_chunk(frame_sp, heap_sp, fsize);\n+  DEBUG_ONLY(after_freeze_java_frame(hf, false);)\n+\n+  caller = hf;\n+  return freeze_ok;\n+}\n+\n+NOINLINE void FreezeBase::finish_freeze(const frame& f, const frame& top) {\n+  stackChunkOop chunk = _cont.tail();\n+  assert(chunk->to_offset(top.sp()) <= chunk->sp(), \"\");\n+\n+  LogTarget(Trace, continuations) lt;\n+  if (lt.develop_is_enabled()) {\n+    LogStream ls(lt);\n+    assert(top.is_heap_frame(), \"should be\");\n+    top.print_on(&ls);\n+  }\n+\n+  set_top_frame_metadata_pd(top);\n+\n+  chunk->set_sp(chunk->to_offset(top.sp()));\n+  chunk->set_pc(top.pc());\n+\n+  chunk->set_max_size(chunk->max_size() + _align_size);\n+\n+  if (UNLIKELY(_barriers)) {\n+    log_develop_trace(continuations)(\"do barriers on old chunk\");\n+    _cont.tail()->do_barriers<stackChunkOopDesc::BarrierType::Store>();\n+  }\n+\n+  log_develop_trace(continuations)(\"finish_freeze: has_mixed_frames: %d\", chunk->has_mixed_frames());\n+  if (lt.develop_is_enabled()) {\n+    LogStream ls(lt);\n+    chunk->print_on(true, &ls);\n+  }\n+\n+  if (lt.develop_is_enabled()) {\n+    LogStream ls(lt);\n+    ls.print_cr(\"top hframe after (freeze):\");\n+    assert(_cont.last_frame().is_heap_frame(), \"should be\");\n+    _cont.last_frame().print_on(&ls);\n+  }\n+\n+  assert(_cont.chunk_invariant(tty), \"\");\n+}\n+\n+inline bool FreezeBase::stack_overflow() { \/\/ detect stack overflow in recursive native code\n+  JavaThread* t = !_preempt ? _thread : JavaThread::current();\n+  assert(t == JavaThread::current(), \"\");\n+  if ((address)&t < t->stack_overflow_state()->stack_overflow_limit()) {\n+    if (!_preempt) {\n+      ContinuationWrapper::SafepointOp so(t, _cont); \/\/ could also call _cont.done() instead\n+      Exceptions::_throw_msg(t, __FILE__, __LINE__, vmSymbols::java_lang_StackOverflowError(), \"Stack overflow while freezing\");\n+    }\n+    return true;\n+  }\n+  return false;\n+}\n+\n+template <typename ConfigT>\n+stackChunkOop Freeze<ConfigT>::allocate_chunk(size_t stack_size) {\n+  log_develop_trace(continuations)(\"allocate_chunk allocating new chunk\");\n+\n+  InstanceStackChunkKlass* klass = InstanceStackChunkKlass::cast(vmClasses::StackChunk_klass());\n+  size_t size_in_words = klass->instance_size(stack_size);\n+\n+  if (CollectedHeap::stack_chunk_max_size() > 0 && size_in_words >= CollectedHeap::stack_chunk_max_size()) {\n+    if (!_preempt) {\n+      throw_stack_overflow_on_humongous_chunk();\n+    }\n+    return nullptr;\n+  }\n+\n+  JavaThread* current = _preempt ? JavaThread::current() : _thread;\n+  assert(current == JavaThread::current(), \"should be current\");\n+\n+  stackChunkOop chunk;\n+  StackChunkAllocator allocator(klass, size_in_words, stack_size, current);\n+  HeapWord* start = current->tlab().allocate(size_in_words);\n+  if (start != nullptr) {\n+    chunk = stackChunkOopDesc::cast(allocator.StackChunkAllocator::initialize(start));\n+  } else {\n+    ContinuationWrapper::SafepointOp so(current, _cont);\n+    assert(_jvmti_event_collector != nullptr, \"\");\n+    _jvmti_event_collector->start(); \/\/ can safepoint\n+\n+    chunk = stackChunkOopDesc::cast(allocator.allocate()); \/\/ can safepoint\n+\n+    if (chunk == nullptr) { \/\/ OOME\n+      return nullptr;\n+    }\n+  }\n+\n+  \/\/ assert that chunk is properly initialized\n+  assert(chunk->stack_size() == (int)stack_size, \"\");\n+  assert(chunk->size() >= stack_size, \"chunk->size(): %zu size: %zu\", chunk->size(), stack_size);\n+  assert(chunk->sp() == chunk->stack_size(), \"\");\n+  assert((intptr_t)chunk->start_address() % 8 == 0, \"\");\n+  assert(chunk->max_size() == 0, \"\");\n+  assert(chunk->pc() == nullptr, \"\");\n+  assert(chunk->argsize() == 0, \"\");\n+  assert(chunk->flags() == 0, \"\");\n+  assert(chunk->is_gc_mode() == false, \"\");\n+\n+  chunk->set_mark(chunk->mark().set_age(15)); \/\/ Promote young chunks quickly\n+\n+  stackChunkOop chunk0 = _cont.tail();\n+  if (chunk0 != nullptr && chunk0->is_empty()) {\n+    chunk0 = chunk0->parent();\n+    assert(chunk0 == nullptr || !chunk0->is_empty(), \"\");\n+  }\n+  \/\/ fields are uninitialized\n+  chunk->set_parent_raw<typename ConfigT::OopT>(chunk0);\n+  chunk->set_cont_raw<typename ConfigT::OopT>(_cont.continuation());\n+\n+  assert(chunk->parent() == nullptr || chunk->parent()->is_stackChunk(), \"\");\n+\n+  if (start != nullptr) {\n+    assert(!chunk->requires_barriers(), \"Unfamiliar GC requires barriers on TLAB allocation\");\n+  } else {\n+    assert(!UseZGC || !chunk->requires_barriers(), \"Allocated ZGC object requires barriers\");\n+    _barriers = !UseZGC && chunk->requires_barriers();\n+\n+    if (_barriers) {\n+      log_develop_trace(continuations)(\"allocation requires barriers\");\n+    }\n+  }\n+\n+  _cont.set_tail(chunk);\n+  return chunk;\n+}\n+\n+void FreezeBase::throw_stack_overflow_on_humongous_chunk() {\n+  ContinuationWrapper::SafepointOp so(_thread, _cont); \/\/ could also call _cont.done() instead\n+  Exceptions::_throw_msg(_thread, __FILE__, __LINE__, vmSymbols::java_lang_StackOverflowError(), \"Humongous stack chunk\");\n+}\n+\n+#if INCLUDE_JVMTI\n+static int num_java_frames(ContinuationWrapper& cont) {\n+  ResourceMark rm; \/\/ used for scope traversal in num_java_frames(CompiledMethod*, address)\n+  int count = 0;\n+  for (stackChunkOop chunk = cont.tail(); chunk != nullptr; chunk = chunk->parent()) {\n+    count += chunk->num_java_frames();\n+  }\n+  return count;\n+}\n+\n+static void invalidate_jvmti_stack(JavaThread* thread) {\n+  if (thread->is_interp_only_mode()) {\n+    JvmtiThreadState *state = thread->jvmti_thread_state();\n+    if (state != nullptr)\n+      state->invalidate_cur_stack_depth();\n+  }\n+}\n+\n+static void jvmti_yield_cleanup(JavaThread* thread, ContinuationWrapper& cont) {\n+  if (JvmtiExport::can_post_frame_pop()) {\n+    int num_frames = num_java_frames(cont);\n+\n+    ContinuationWrapper::SafepointOp so(Thread::current(), cont);\n+    JvmtiExport::continuation_yield_cleanup(JavaThread::current(), num_frames);\n+  }\n+  invalidate_jvmti_stack(thread);\n+}\n+#endif \/\/ INCLUDE_JVMTI\n+\n+static freeze_result is_pinned(const frame& f, RegisterMap* map) {\n+  if (f.is_interpreted_frame()) {\n+    if (ContinuationHelper::InterpretedFrame::is_owning_locks(f)) {\n+      return freeze_pinned_monitor;\n+    }\n+    if (f.interpreter_frame_method()->is_native()) {\n+      return freeze_pinned_native; \/\/ interpreter native entry\n+    }\n+  } else if (f.is_compiled_frame()) {\n+    if (ContinuationHelper::CompiledFrame::is_owning_locks(map->thread(), map, f)) {\n+      return freeze_pinned_monitor;\n+    }\n+  } else {\n+    return freeze_pinned_native;\n+  }\n+  return freeze_ok;\n+}\n+\n+#ifdef ASSERT\n+static bool monitors_on_stack(JavaThread* thread) {\n+  ContinuationEntry* ce = thread->last_continuation();\n+  RegisterMap map(thread, true, false, false);\n+  map.set_include_argument_oops(false);\n+  for (frame f = thread->last_frame(); Continuation::is_frame_in_continuation(ce, f); f = f.sender(&map)) {\n+    if (is_pinned(f, &map) == freeze_pinned_monitor) {\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n+static bool interpreted_native_or_deoptimized_on_stack(JavaThread* thread) {\n+  ContinuationEntry* ce = thread->last_continuation();\n+  RegisterMap map(thread, false, false, false);\n+  map.set_include_argument_oops(false);\n+  for (frame f = thread->last_frame(); Continuation::is_frame_in_continuation(ce, f); f = f.sender(&map)) {\n+    if (f.is_interpreted_frame() || f.is_native_frame() || f.is_deoptimized_frame()) {\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+#endif \/\/ ASSERT\n+\n+static inline bool can_freeze_fast(JavaThread* thread) {\n+  \/\/ There are no interpreted frames if we're not called from the interpreter and we haven't ancountered an i2c adapter or called Deoptimization::unpack_frames\n+  \/\/ Calls from native frames also go through the interpreter (see JavaCalls::call_helper)\n+  assert(!thread->cont_fastpath()\n+         || (thread->cont_fastpath_thread_state() && !interpreted_native_or_deoptimized_on_stack(thread)), \"\");\n+\n+  \/\/ We also clear thread->cont_fastpath on deoptimization (notify_deopt) and when we thaw interpreted frames\n+  bool fast = thread->cont_fastpath() && UseContinuationFastPath;\n+  assert(!fast || monitors_on_stack(thread) == (thread->held_monitor_count() > 0), \"\");\n+  fast = fast && thread->held_monitor_count() == 0;\n+  return fast;\n+}\n+\n+static inline int freeze_epilog(JavaThread* thread, ContinuationWrapper& cont) {\n+  verify_continuation(cont.continuation());\n+  assert(!cont.is_empty(), \"\");\n+\n+  log_develop_debug(continuations)(\"=== End of freeze cont ### #\" INTPTR_FORMAT, cont.hash());\n+\n+  return 0;\n+}\n+\n+static int freeze_epilog(JavaThread* thread, ContinuationWrapper& cont, freeze_result res) {\n+  if (UNLIKELY(res != freeze_ok)) {\n+    verify_continuation(cont.continuation());\n+    log_develop_trace(continuations)(\"=== end of freeze (fail %d)\", res);\n+    return res;\n+  }\n+\n+  JVMTI_ONLY(jvmti_yield_cleanup(thread, cont)); \/\/ can safepoint\n+  return freeze_epilog(thread, cont);\n+}\n+\n+template<typename ConfigT>\n+static inline int freeze_internal(JavaThread* current, intptr_t* const sp) {\n+  assert(!current->has_pending_exception(), \"\");\n+\n+#ifdef ASSERT\n+  log_trace(continuations)(\"~~~~ freeze sp: \" INTPTR_FORMAT, p2i(current->last_continuation()->entry_sp()));\n+  log_frames(current);\n+#endif\n+\n+  CONT_JFR_ONLY(EventContinuationFreeze event;)\n+\n+  ContinuationEntry* entry = current->last_continuation();\n+\n+  oop oopCont = get_continuation(current);\n+  assert(oopCont == current->last_continuation()->cont_oop(), \"\");\n+  assert(ContinuationEntry::assert_entry_frame_laid_out(current), \"\");\n+\n+  verify_continuation(oopCont);\n+  ContinuationWrapper cont(current, oopCont);\n+  log_develop_debug(continuations)(\"FREEZE #\" INTPTR_FORMAT \" \" INTPTR_FORMAT, cont.hash(), p2i((oopDesc*)oopCont));\n+\n+  assert(entry->is_virtual_thread() == (entry->scope() == java_lang_VirtualThread::vthread_scope()), \"\");\n+\n+  if (entry->is_pinned()) {\n+    log_develop_debug(continuations)(\"PINNED due to critical section\");\n+    verify_continuation(cont.continuation());\n+    log_develop_trace(continuations)(\"=== end of freeze (fail %d)\", freeze_pinned_cs);\n+    return freeze_pinned_cs;\n+  }\n+\n+  Freeze<ConfigT> fr(current, cont, false);\n+\n+  bool fast = can_freeze_fast(current);\n+  if (fast && fr.is_chunk_available(sp)) {\n+    freeze_result res = fr.template try_freeze_fast<true>(sp);\n+    assert(res == freeze_ok, \"\");\n+    CONT_JFR_ONLY(fr.jfr_info().post_jfr_event(&event, oopCont, current);)\n+    freeze_epilog(current, cont);\n+    StackWatermarkSet::after_unwind(current);\n+    return 0;\n+  }\n+\n+  log_develop_trace(continuations)(\"chunk unavailable; transitioning to VM\");\n+  assert(current == JavaThread::current(), \"must be current thread except for preempt\");\n+  JRT_BLOCK\n+    \/\/ delays a possible JvmtiSampledObjectAllocEventCollector in alloc_chunk\n+    JvmtiSampledObjectAllocEventCollector jsoaec(false);\n+    fr.set_jvmti_event_collector(&jsoaec);\n+\n+    freeze_result res = fast ? fr.template try_freeze_fast<false>(sp)\n+                             : fr.freeze_slow();\n+    CONT_JFR_ONLY(fr.jfr_info().post_jfr_event(&event, oopCont, current);)\n+    freeze_epilog(current, cont, res);\n+    cont.done(); \/\/ allow safepoint in the transition back to Java\n+    StackWatermarkSet::after_unwind(current);\n+    return res;\n+  JRT_BLOCK_END\n+}\n+\n+static freeze_result is_pinned0(JavaThread* thread, oop cont_scope, bool safepoint) {\n+  ContinuationEntry* entry = thread->last_continuation();\n+  if (entry == nullptr) {\n+    return freeze_ok;\n+  }\n+  if (entry->is_pinned()) {\n+    return freeze_pinned_cs;\n+  }\n+\n+  RegisterMap map(thread, true, false, false);\n+  map.set_include_argument_oops(false);\n+  frame f = thread->last_frame();\n+\n+  if (!safepoint) {\n+    f = f.sender(&map); \/\/ this is the yield frame\n+  } else { \/\/ safepoint yield\n+#if (defined(X86) || defined(AARCH64)) && !defined(ZERO)\n+    f.set_fp(f.real_fp()); \/\/ Instead of this, maybe in ContinuationWrapper::set_last_frame always use the real_fp?\n+#else\n+    Unimplemented();\n+#endif\n+    if (!Interpreter::contains(f.pc())) {\n+      assert(ContinuationHelper::Frame::is_stub(f.cb()), \"must be\");\n+      assert(f.oop_map() != nullptr, \"must be\");\n+      f.oop_map()->update_register_map(&f, &map); \/\/ we have callee-save registers in this case\n+    }\n+  }\n+\n+  while (true) {\n+    freeze_result res = is_pinned(f, &map);\n+    if (res != freeze_ok) {\n+      return res;\n+    }\n+\n+    f = f.sender(&map);\n+    if (!Continuation::is_frame_in_continuation(entry, f)) {\n+      oop scope = jdk_internal_vm_Continuation::scope(entry->cont_oop());\n+      if (scope == cont_scope) {\n+        break;\n+      }\n+      entry = entry->parent();\n+      if (entry == nullptr) {\n+        break;\n+      }\n+      if (entry->is_pinned()) {\n+        return freeze_pinned_cs;\n+      }\n+    }\n+  }\n+  return freeze_ok;\n+}\n+\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/ THAW \/\/\/\/\n+\n+static int thaw_size(stackChunkOop chunk) {\n+  int size = chunk->max_size();\n+  size += frame::metadata_words; \/\/ For the top pc+fp in push_return_frame or top = stack_sp - frame::metadata_words in thaw_fast\n+  size += 2*frame::align_wiggle; \/\/ in case of alignments at the top and bottom\n+  return size + 200;\n+}\n+\n+\/\/ make room on the stack for thaw\n+\/\/ returns the size in bytes, or 0 on failure\n+static inline int prepare_thaw_internal(JavaThread* thread, bool return_barrier) {\n+  log_develop_trace(continuations)(\"~~~~ prepare_thaw return_barrier: %d\", return_barrier);\n+\n+  assert(thread == JavaThread::current(), \"\");\n+\n+  ContinuationEntry* ce = thread->last_continuation();\n+  assert(ce != nullptr, \"\");\n+  oop continuation = ce->cont_oop();\n+  assert(continuation == get_continuation(thread), \"\");\n+  verify_continuation(continuation);\n+\n+  stackChunkOop chunk = jdk_internal_vm_Continuation::tail(continuation);\n+  assert(chunk != nullptr, \"\");\n+\n+  \/\/ The tail can be empty because it might still be available for another freeze.\n+  \/\/ However, here we want to thaw, so we get rid of it (it will be GCed).\n+  if (UNLIKELY(chunk->is_empty())) {\n+    chunk = chunk->parent();\n+    assert(chunk != nullptr, \"\");\n+    assert(!chunk->is_empty(), \"\");\n+    jdk_internal_vm_Continuation::set_tail(continuation, chunk);\n+  }\n+\n+  \/\/ Verification\n+  chunk->verify();\n+  assert(chunk->max_size() > 0, \"chunk invariant violated; expected to not be empty\");\n+\n+  \/\/ Only make space for the last chunk because we only thaw from the last chunk\n+  int size = thaw_size(chunk) << LogBytesPerWord;\n+\n+  const address bottom = (address)thread->last_continuation()->entry_sp();\n+  \/\/ 300 is an estimate for stack size taken for this native code, in addition to StackShadowPages\n+  \/\/ for the Java frames in the check below.\n+  if (!stack_overflow_check(thread, size + 300, bottom)) {\n+    return 0;\n+  }\n+\n+  log_develop_trace(continuations)(\"prepare_thaw bottom: \" INTPTR_FORMAT \" top: \" INTPTR_FORMAT \" size: %d\",\n+                              p2i(bottom), p2i(bottom - size), size);\n+  return size;\n+}\n+\n+class ThawBase : public StackObj {\n+protected:\n+  JavaThread* _thread;\n+  ContinuationWrapper& _cont;\n+  CONT_JFR_ONLY(FreezeThawJfrInfo _jfr_info;)\n+\n+  intptr_t* _fastpath;\n+  bool _barriers;\n+  intptr_t* _top_unextended_sp;\n+  int _align_size;\n+  DEBUG_ONLY(intptr_t* _top_stack_address);\n+\n+  StackChunkFrameStream<ChunkFrames::Mixed> _stream;\n+\n+  NOT_PRODUCT(int _frames;)\n+\n+#ifdef ASSERT\n+  public:\n+    bool barriers() { return _barriers; }\n+  protected:\n+#endif\n+\n+protected:\n+  ThawBase(JavaThread* thread, ContinuationWrapper& cont) :\n+      _thread(thread), _cont(cont),\n+      _fastpath(nullptr) {\n+    DEBUG_ONLY(_top_unextended_sp = nullptr;)\n+    assert (cont.tail() != nullptr, \"no last chunk\");\n+    DEBUG_ONLY(_top_stack_address = _cont.entrySP() - thaw_size(cont.tail());)\n+  }\n+\n+  void copy_from_chunk(intptr_t* from, intptr_t* to, int size);\n+\n+  \/\/ fast path\n+  inline void prefetch_chunk_pd(void* start, int size_words);\n+  void patch_return(intptr_t* sp, bool is_last);\n+  void patch_chunk_pd(intptr_t* sp); \/\/ TODO remove\n+\n+  \/\/ slow path\n+  NOINLINE intptr_t* thaw_slow(stackChunkOop chunk, bool return_barrier);\n+\n+private:\n+  void thaw_one_frame(const frame& heap_frame, frame& caller, int num_frames, bool top);\n+  template<typename FKind> bool recurse_thaw_java_frame(frame& caller, int num_frames);\n+  void finalize_thaw(frame& entry, int argsize);\n+\n+  inline void before_thaw_java_frame(const frame& hf, const frame& caller, bool bottom, int num_frame);\n+  inline void after_thaw_java_frame(const frame& f, bool bottom);\n+  inline void patch(frame& f, const frame& caller, bool bottom);\n+  void clear_bitmap_bits(intptr_t* start, int range);\n+\n+  NOINLINE void recurse_thaw_interpreted_frame(const frame& hf, frame& caller, int num_frames);\n+  void recurse_thaw_compiled_frame(const frame& hf, frame& caller, int num_frames, bool stub_caller);\n+  void recurse_thaw_stub_frame(const frame& hf, frame& caller, int num_frames);\n+  void finish_thaw(frame& f);\n+\n+  void push_return_frame(frame& f);\n+  inline frame new_entry_frame();\n+  template<typename FKind> frame new_stack_frame(const frame& hf, frame& caller, bool bottom);\n+  inline void patch_pd(frame& f, const frame& sender);\n+  inline intptr_t* align(const frame& hf, intptr_t* frame_sp, frame& caller, bool bottom);\n+\n+  void maybe_set_fastpath(intptr_t* sp) { if (sp > _fastpath) _fastpath = sp; }\n+\n+  static inline void derelativize_interpreted_frame_metadata(const frame& hf, const frame& f);\n+  static inline void set_interpreter_frame_bottom(const frame& f, intptr_t* bottom);\n+\n+ public:\n+  CONT_JFR_ONLY(FreezeThawJfrInfo& jfr_info() { return _jfr_info; })\n+};\n+\n+template <typename ConfigT>\n+class Thaw : public ThawBase {\n+public:\n+  Thaw(JavaThread* thread, ContinuationWrapper& cont) : ThawBase(thread, cont) {}\n+\n+  inline bool can_thaw_fast(stackChunkOop chunk) {\n+    return    !_barriers\n+           &&  _thread->cont_fastpath_thread_state()\n+           && !chunk->has_thaw_slowpath_condition()\n+           && !PreserveFramePointer;\n+  }\n+\n+  inline intptr_t* thaw(thaw_kind kind);\n+  NOINLINE intptr_t* thaw_fast(stackChunkOop chunk);\n+};\n+\n+template <typename ConfigT>\n+inline intptr_t* Thaw<ConfigT>::thaw(thaw_kind kind) {\n+  verify_continuation(_cont.continuation());\n+  assert(!jdk_internal_vm_Continuation::done(_cont.continuation()), \"\");\n+  assert(!_cont.is_empty(), \"\");\n+\n+  stackChunkOop chunk = _cont.tail();\n+  assert(chunk != nullptr, \"guaranteed by prepare_thaw\");\n+  assert(!chunk->is_empty(), \"guaranteed by prepare_thaw\");\n+\n+  _barriers = chunk->requires_barriers();\n+  return (LIKELY(can_thaw_fast(chunk))) ? thaw_fast(chunk)\n+                                        : thaw_slow(chunk, kind != thaw_top);\n+}\n+\n+template <typename ConfigT>\n+NOINLINE intptr_t* Thaw<ConfigT>::thaw_fast(stackChunkOop chunk) {\n+  assert(chunk == _cont.tail(), \"\");\n+  assert(!chunk->has_mixed_frames(), \"\");\n+  assert(!chunk->requires_barriers(), \"\");\n+  assert(!chunk->has_bitmap(), \"\");\n+  assert(!_thread->is_interp_only_mode(), \"\");\n+\n+  LogTarget(Trace, continuations) lt;\n+  if (lt.develop_is_enabled()) {\n+    LogStream ls(lt);\n+    ls.print_cr(\"thaw_fast\");\n+    chunk->print_on(true, &ls);\n+  }\n+\n+  \/\/ Below this heuristic, we thaw the whole chunk, above it we thaw just one frame.\n+  static const int threshold = 500; \/\/ words\n+\n+  int chunk_start_sp = chunk->sp();\n+  const int full_chunk_size = chunk->stack_size() - chunk_start_sp; \/\/ this initial size could be reduced if it's a partial thaw\n+  int argsize, thaw_size;\n+\n+  intptr_t* const chunk_sp = chunk->start_address() + chunk_start_sp;\n+\n+  bool partial, empty;\n+  if (LIKELY(!TEST_THAW_ONE_CHUNK_FRAME && (full_chunk_size < threshold))) {\n+    prefetch_chunk_pd(chunk->start_address(), full_chunk_size); \/\/ prefetch anticipating memcpy starting at highest address\n+\n+    partial = false;\n+\n+    argsize = chunk->argsize();\n+    empty = true;\n+\n+    chunk->set_sp(chunk->stack_size());\n+    chunk->set_argsize(0);\n+    chunk->set_max_size(0);\n+\n+    thaw_size = full_chunk_size;\n+  } else { \/\/ thaw a single frame\n+    partial = true;\n+\n+    StackChunkFrameStream<ChunkFrames::CompiledOnly> f(chunk);\n+    assert(chunk_sp == f.sp(), \"\");\n+    assert(chunk_sp == f.unextended_sp(), \"\");\n+\n+    const int frame_size = f.cb()->frame_size();\n+    argsize = f.stack_argsize();\n+\n+    f.next(SmallRegisterMap::instance, true \/* stop *\/);\n+    empty = f.is_done();\n+    assert(!empty || argsize == chunk->argsize(), \"\");\n+\n+    if (empty) {\n+      chunk->set_sp(chunk->stack_size());\n+      chunk->set_argsize(0);\n+      chunk->set_max_size(0);\n+    } else {\n+      chunk->set_sp(chunk->sp() + frame_size);\n+      chunk->set_max_size(chunk->max_size() - frame_size);\n+      \/\/ We set chunk->pc to the return pc into the next frame\n+      chunk->set_pc(f.pc());\n+      assert(f.pc() == *(address*)(chunk_sp + frame_size - frame::sender_sp_ret_address_offset()), \"unexpected pc\");\n+    }\n+    assert(empty == chunk->is_empty(), \"\");\n+    thaw_size = frame_size + argsize;\n+  }\n+\n+  \/\/ Are we thawing the last frame(s) in the continuation\n+  const bool is_last = empty && chunk->is_parent_null<typename ConfigT::OopT>();\n+\n+  log_develop_trace(continuations)(\"thaw_fast partial: %d is_last: %d empty: %d size: %d argsize: %d\",\n+                              partial, is_last, empty, thaw_size, argsize);\n+\n+  intptr_t* stack_sp = _cont.entrySP();\n+  intptr_t* bottom_sp = ContinuationHelper::frame_align_pointer(stack_sp - argsize);\n+\n+  stack_sp -= thaw_size;\n+  \/\/ possibly adds a one-word padding between entrySP and the bottom-most frame's stack args\n+  \/\/ The only possible source of misalignment is stack-passed arguments because all compiled\n+  \/\/ frames are 16-byte aligned.\n+  assert(argsize != 0 || stack_sp == ContinuationHelper::frame_align_pointer(stack_sp), \"\");\n+  stack_sp = ContinuationHelper::frame_align_pointer(stack_sp);\n+\n+  \/\/ also copy metadata words\n+  intptr_t* from = chunk_sp - frame::metadata_words;\n+  intptr_t* to   = stack_sp - frame::metadata_words;\n+  copy_from_chunk(from, to, thaw_size + frame::metadata_words);\n+  \/\/ We assert we have not overwritten the entry frame, but that we're at most\n+  \/\/ one alignment word away from it.\n+  assert(to + thaw_size + frame::metadata_words <= _cont.entrySP(), \"overwritten entry frame\");\n+  assert(_cont.entrySP() - 1 <= to + thaw_size + frame::metadata_words, \"missed entry frame\");\n+  assert(argsize != 0 || to + thaw_size + frame::metadata_words == _cont.entrySP(), \"missed entry frame\");\n+\n+  assert(!is_last || argsize == 0, \"\");\n+  _cont.set_argsize(argsize); \/\/ sets argsize in ContinuationEntry\n+  log_develop_trace(continuations)(\"setting entry argsize: %d\", _cont.argsize());\n+  assert(bottom_sp == _cont.entry()->bottom_sender_sp(), \"\");\n+\n+  \/\/ install the return barrier if not last frame, or the entry's pc if last\n+  patch_return(bottom_sp, is_last);\n+  DEBUG_ONLY(address pc = *(address*)(bottom_sp - frame::sender_sp_ret_address_offset());)\n+  assert(is_last ? CodeCache::find_blob(pc)->as_compiled_method()->method()->is_continuation_enter_intrinsic()\n+                  : pc == StubRoutines::cont_returnBarrier(), \"is_last: %d\", is_last);\n+  assert(is_last == _cont.is_empty(), \"\");\n+  assert(_cont.chunk_invariant(tty), \"\");\n+\n+#if CONT_JFR\n+  EventContinuationThawYoung e;\n+  if (e.should_commit()) {\n+    e.set_id(cast_from_oop<u8>(chunk));\n+    e.set_size(thaw_size << LogBytesPerWord);\n+    e.set_full(!partial);\n+    e.commit();\n+  }\n+#endif\n+\n+#ifdef ASSERT\n+  set_anchor(_thread, stack_sp);\n+  log_frames(_thread);\n+  if (LoomDeoptAfterThaw) {\n+    do_deopt_after_thaw(_thread);\n+  }\n+  clear_anchor(_thread);\n+#endif\n+\n+  return stack_sp;\n+}\n+\n+void ThawBase::copy_from_chunk(intptr_t* from, intptr_t* to, int size) {\n+  assert(to + size <= _cont.entrySP(), \"\");\n+  _cont.tail()->copy_from_chunk_to_stack(from, to, size);\n+  CONT_JFR_ONLY(_jfr_info.record_size_copied(size);)\n+  assert(to >= _top_stack_address, \"overwrote past thawing space\"\n+    \" to: \" INTPTR_FORMAT \" top_address: \" INTPTR_FORMAT, p2i(to), p2i(_top_stack_address));\n+}\n+\n+void ThawBase::patch_return(intptr_t* sp, bool is_last) {\n+  log_develop_trace(continuations)(\"thaw_fast patching -- sp: \" INTPTR_FORMAT, p2i(sp));\n+\n+  address pc = !is_last ? StubRoutines::cont_returnBarrier() : _cont.entryPC();\n+  *(address*)(sp - frame::sender_sp_ret_address_offset()) = pc;\n+  \/\/ patch_chunk_pd(sp); -- TODO: If not needed - remove method; it's not used elsewhere\n+}\n+\n+NOINLINE intptr_t* ThawBase::thaw_slow(stackChunkOop chunk, bool return_barrier) {\n+  LogTarget(Trace, continuations) lt;\n+  if (lt.develop_is_enabled()) {\n+    LogStream ls(lt);\n+    ls.print_cr(\"thaw slow return_barrier: %d \" INTPTR_FORMAT, return_barrier, p2i(chunk));\n+    chunk->print_on(true, &ls);\n+  }\n+\n+  \/\/ Does this need ifdef JFR around it? Or can we remove all the conditional JFR inclusions (better)?\n+  EventContinuationThawOld e;\n+  if (e.should_commit()) {\n+    e.set_id(cast_from_oop<u8>(_cont.continuation()));\n+    e.commit();\n+  }\n+\n+  DEBUG_ONLY(_frames = 0;)\n+  _align_size = 0;\n+  int num_frames = (return_barrier ? 1 : 2);\n+  bool last_interpreted = chunk->has_mixed_frames() && Interpreter::contains(chunk->pc());\n+\n+  _stream = StackChunkFrameStream<ChunkFrames::Mixed>(chunk);\n+  _top_unextended_sp = _stream.unextended_sp();\n+\n+  frame heap_frame = _stream.to_frame();\n+  if (lt.develop_is_enabled()) {\n+    LogStream ls(lt);\n+    ls.print_cr(\"top hframe before (thaw):\");\n+    assert(heap_frame.is_heap_frame(), \"should have created a relative frame\");\n+    heap_frame.print_value_on(&ls, nullptr);\n+  }\n+\n+  frame caller;\n+  thaw_one_frame(heap_frame, caller, num_frames, true);\n+  finish_thaw(caller); \/\/ caller is now the topmost thawed frame\n+  _cont.write();\n+\n+  assert(_cont.chunk_invariant(tty), \"\");\n+\n+  JVMTI_ONLY(if (!return_barrier) invalidate_jvmti_stack(_thread));\n+\n+  _thread->set_cont_fastpath(_fastpath);\n+\n+  intptr_t* sp = caller.sp();\n+  return sp;\n+}\n+\n+void ThawBase::thaw_one_frame(const frame& heap_frame, frame& caller, int num_frames, bool top) {\n+  log_develop_debug(continuations)(\"thaw num_frames: %d\", num_frames);\n+  assert(!_cont.is_empty(), \"no more frames\");\n+  assert(num_frames > 0, \"\");\n+  assert(!heap_frame.is_empty(), \"\");\n+\n+  if (top && heap_frame.is_safepoint_blob_frame()) {\n+    assert(ContinuationHelper::Frame::is_stub(heap_frame.cb()), \"cb: %s\", heap_frame.cb()->name());\n+    recurse_thaw_stub_frame(heap_frame, caller, num_frames);\n+  } else if (!heap_frame.is_interpreted_frame()) {\n+    recurse_thaw_compiled_frame(heap_frame, caller, num_frames, false);\n+  } else {\n+    recurse_thaw_interpreted_frame(heap_frame, caller, num_frames);\n+  }\n+}\n+\n+template<typename FKind>\n+bool ThawBase::recurse_thaw_java_frame(frame& caller, int num_frames) {\n+  assert(num_frames > 0, \"\");\n+\n+  DEBUG_ONLY(_frames++;)\n+\n+  int argsize = _stream.stack_argsize();\n+\n+  _stream.next(SmallRegisterMap::instance);\n+  assert(_stream.to_frame().is_empty() == _stream.is_done(), \"\");\n+\n+  \/\/ we never leave a compiled caller of an interpreted frame as the top frame in the chunk\n+  \/\/ as it makes detecting that situation and adjusting unextended_sp tricky\n+  if (num_frames == 1 && !_stream.is_done() && FKind::interpreted && _stream.is_compiled()) {\n+    log_develop_trace(continuations)(\"thawing extra compiled frame to not leave a compiled interpreted-caller at top\");\n+    num_frames++;\n+  }\n+\n+  if (num_frames == 1 || _stream.is_done()) { \/\/ end recursion\n+    finalize_thaw(caller, FKind::interpreted ? 0 : argsize);\n+    return true; \/\/ bottom\n+  } else { \/\/ recurse\n+    thaw_one_frame(_stream.to_frame(), caller, num_frames - 1, false);\n+    return false;\n+  }\n+}\n+\n+void ThawBase::finalize_thaw(frame& entry, int argsize) {\n+  stackChunkOop chunk = _cont.tail();\n+\n+  if (!_stream.is_done()) {\n+    assert(_stream.sp() >= chunk->sp_address(), \"\");\n+    chunk->set_sp(chunk->to_offset(_stream.sp()));\n+    chunk->set_pc(_stream.pc());\n+  } else {\n+    chunk->set_argsize(0);\n+    chunk->set_sp(chunk->stack_size());\n+    chunk->set_pc(nullptr);\n+  }\n+  assert(_stream.is_done() == chunk->is_empty(), \"\");\n+\n+  int delta = _stream.unextended_sp() - _top_unextended_sp;\n+  chunk->set_max_size(chunk->max_size() - delta);\n+\n+  _cont.set_argsize(argsize);\n+  entry = new_entry_frame();\n+\n+  assert(entry.sp() == _cont.entrySP(), \"\");\n+  assert(Continuation::is_continuation_enterSpecial(entry), \"\");\n+  assert(_cont.is_entry_frame(entry), \"\");\n+}\n+\n+inline void ThawBase::before_thaw_java_frame(const frame& hf, const frame& caller, bool bottom, int num_frame) {\n+  LogTarget(Trace, continuations) lt;\n+  if (lt.develop_is_enabled()) {\n+    LogStream ls(lt);\n+    ls.print_cr(\"======== THAWING FRAME: %d\", num_frame);\n+    assert(hf.is_heap_frame(), \"should be\");\n+    hf.print_value_on(&ls, nullptr);\n+  }\n+  assert(bottom == _cont.is_entry_frame(caller), \"bottom: %d is_entry_frame: %d\", bottom, _cont.is_entry_frame(hf));\n+}\n+\n+inline void ThawBase::after_thaw_java_frame(const frame& f, bool bottom) {\n+#ifdef ASSERT\n+  LogTarget(Trace, continuations) lt;\n+  if (lt.develop_is_enabled()) {\n+    LogStream ls(lt);\n+    ls.print_cr(\"thawed frame:\");\n+    print_frame_layout(f, false, &ls); \/\/ f.print_on(&ls);\n+  }\n+#endif\n+}\n+\n+inline void ThawBase::patch(frame& f, const frame& caller, bool bottom) {\n+  assert(!bottom || caller.fp() == _cont.entryFP(), \"\");\n+  if (bottom) {\n+    ContinuationHelper::Frame::patch_pc(caller, _cont.is_empty() ? caller.raw_pc()\n+                                                                 : StubRoutines::cont_returnBarrier());\n+  }\n+\n+  patch_pd(f, caller);\n+\n+  if (f.is_interpreted_frame()) {\n+    ContinuationHelper::InterpretedFrame::patch_sender_sp(f, caller.unextended_sp());\n+  }\n+\n+  assert(!bottom || !_cont.is_empty() || Continuation::is_continuation_entry_frame(f, nullptr), \"\");\n+  assert(!bottom || (_cont.is_empty() != Continuation::is_cont_barrier_frame(f)), \"\");\n+}\n+\n+void ThawBase::clear_bitmap_bits(intptr_t* start, int range) {\n+  \/\/ we need to clear the bits that correspond to arguments as they reside in the caller frame\n+  log_develop_trace(continuations)(\"clearing bitmap for \" INTPTR_FORMAT \" - \" INTPTR_FORMAT, p2i(start), p2i(start+range));\n+  stackChunkOop chunk = _cont.tail();\n+  chunk->bitmap().clear_range(chunk->bit_index_for(start),\n+                              chunk->bit_index_for(start+range));\n+}\n+\n+NOINLINE void ThawBase::recurse_thaw_interpreted_frame(const frame& hf, frame& caller, int num_frames) {\n+  assert(hf.is_interpreted_frame(), \"\");\n+\n+  if (UNLIKELY(_barriers)) {\n+    _cont.tail()->do_barriers<stackChunkOopDesc::BarrierType::Store>(_stream, SmallRegisterMap::instance);\n+  }\n+\n+  const bool bottom = recurse_thaw_java_frame<ContinuationHelper::InterpretedFrame>(caller, num_frames);\n+\n+  DEBUG_ONLY(before_thaw_java_frame(hf, caller, bottom, num_frames);)\n+\n+  _align_size += frame::align_wiggle; \/\/ remove the added alignment room for internal interpreted frame alignment om AArch64\n+\n+  frame f = new_stack_frame<ContinuationHelper::InterpretedFrame>(hf, caller, bottom);\n+\n+  intptr_t* const frame_sp = f.sp();\n+  intptr_t* const heap_sp = hf.unextended_sp();\n+  intptr_t* const frame_bottom = ContinuationHelper::InterpretedFrame::frame_bottom(f);\n+\n+  assert(hf.is_heap_frame(), \"should be\");\n+  const int fsize = ContinuationHelper::InterpretedFrame::frame_bottom(hf) - heap_sp;\n+\n+  assert(!bottom || frame_sp + fsize >= _cont.entrySP() - 2, \"\");\n+  assert(!bottom || frame_sp + fsize <= _cont.entrySP(), \"\");\n+\n+  assert(ContinuationHelper::InterpretedFrame::frame_bottom(f) == frame_sp + fsize, \"\");\n+\n+  \/\/ on AArch64 we add padding between the locals and the rest of the frame to keep the fp 16-byte-aligned\n+  const int locals = hf.interpreter_frame_method()->max_locals();\n+  assert(hf.is_heap_frame(), \"should be\");\n+  assert(!f.is_heap_frame(), \"should not be\");\n+\n+  copy_from_chunk(ContinuationHelper::InterpretedFrame::frame_bottom(hf) - locals,\n+                  ContinuationHelper::InterpretedFrame::frame_bottom(f) - locals, locals); \/\/ copy locals\n+  copy_from_chunk(heap_sp, frame_sp, fsize - locals); \/\/ copy rest\n+\n+  set_interpreter_frame_bottom(f, frame_bottom); \/\/ the copy overwrites the metadata\n+  derelativize_interpreted_frame_metadata(hf, f);\n+  patch(f, caller, bottom);\n+\n+  assert(f.is_interpreted_frame_valid(_cont.thread()), \"invalid thawed frame\");\n+  assert(ContinuationHelper::InterpretedFrame::frame_bottom(f) <= ContinuationHelper::Frame::frame_top(caller), \"\");\n+\n+  CONT_JFR_ONLY(_jfr_info.record_interpreted_frame();)\n+\n+  maybe_set_fastpath(f.sp());\n+\n+  if (!bottom) {\n+    \/\/ can only fix caller once this frame is thawed (due to callee saved regs)\n+    _cont.tail()->fix_thawed_frame(caller, SmallRegisterMap::instance);\n+  } else if (_cont.tail()->has_bitmap() && locals > 0) {\n+    assert(hf.is_heap_frame(), \"should be\");\n+    clear_bitmap_bits(ContinuationHelper::InterpretedFrame::frame_bottom(hf) - locals, locals);\n+  }\n+\n+  DEBUG_ONLY(after_thaw_java_frame(f, bottom);)\n+  caller = f;\n+}\n+\n+void ThawBase::recurse_thaw_compiled_frame(const frame& hf, frame& caller, int num_frames, bool stub_caller) {\n+  assert(!hf.is_interpreted_frame(), \"\");\n+  assert(_cont.is_preempted() || !stub_caller, \"stub caller not at preemption\");\n+\n+  if (!stub_caller && UNLIKELY(_barriers)) { \/\/ recurse_thaw_stub_frame already invoked our barriers with a full regmap\n+    _cont.tail()->do_barriers<stackChunkOopDesc::BarrierType::Store>(_stream, SmallRegisterMap::instance);\n+  }\n+\n+  const bool bottom = recurse_thaw_java_frame<ContinuationHelper::CompiledFrame>(caller, num_frames);\n+\n+  DEBUG_ONLY(before_thaw_java_frame(hf, caller, bottom, num_frames);)\n+\n+  assert(caller.sp() == caller.unextended_sp(), \"\");\n+\n+  if ((!bottom && caller.is_interpreted_frame()) || (bottom && Interpreter::contains(_cont.tail()->pc()))) {\n+    _align_size += frame::align_wiggle; \/\/ we add one whether or not we've aligned because we add it in freeze_interpreted_frame\n+  }\n+\n+  frame f = new_stack_frame<ContinuationHelper::CompiledFrame>(hf, caller, bottom);\n+  intptr_t* const frame_sp = f.sp();\n+  intptr_t* const heap_sp = hf.unextended_sp();\n+\n+  const int added_argsize = (bottom || caller.is_interpreted_frame()) ? hf.compiled_frame_stack_argsize() : 0;\n+  int fsize = ContinuationHelper::CompiledFrame::size(hf) + added_argsize;\n+  assert(fsize <= (int)(caller.unextended_sp() - f.unextended_sp()), \"\");\n+\n+  intptr_t* from = heap_sp - frame::metadata_words;\n+  intptr_t* to   = frame_sp - frame::metadata_words;\n+  int sz = fsize + frame::metadata_words;\n+\n+  assert(!bottom || (_cont.entrySP() - 1 <= to + sz && to + sz <= _cont.entrySP()), \"\");\n+  assert(!bottom || hf.compiled_frame_stack_argsize() != 0 || (to + sz && to + sz == _cont.entrySP()), \"\");\n+\n+  copy_from_chunk(from, to, sz); \/\/ copying good oops because we invoked barriers above\n+\n+  patch(f, caller, bottom);\n+\n+  if (f.cb()->is_nmethod()) {\n+    f.cb()->as_nmethod()->run_nmethod_entry_barrier();\n+  }\n+\n+  if (f.is_deoptimized_frame()) {\n+    maybe_set_fastpath(f.sp());\n+  } else if (_thread->is_interp_only_mode()\n+              || (_cont.is_preempted() && f.cb()->as_compiled_method()->is_marked_for_deoptimization())) {\n+    \/\/ The caller of the safepoint stub when the continuation is preempted is not at a call instruction, and so\n+    \/\/ cannot rely on nmethod patching for deopt.\n+    assert(_thread->is_interp_only_mode() || stub_caller, \"expected a stub-caller\");\n+\n+    log_develop_trace(continuations)(\"Deoptimizing thawed frame\");\n+    DEBUG_ONLY(ContinuationHelper::Frame::patch_pc(f, nullptr));\n+\n+    f.deoptimize(nullptr); \/\/ we're assuming there are no monitors; this doesn't revoke biased locks\n+    assert(f.is_deoptimized_frame(), \"\");\n+    assert(ContinuationHelper::Frame::is_deopt_return(f.raw_pc(), f), \"\");\n+    maybe_set_fastpath(f.sp());\n+  }\n+\n+  if (!bottom) {\n+    \/\/ can only fix caller once this frame is thawed (due to callee saved regs)\n+    \/\/ This happens on the stack\n+    _cont.tail()->fix_thawed_frame(caller, SmallRegisterMap::instance);\n+  } else if (_cont.tail()->has_bitmap() && added_argsize > 0) {\n+    clear_bitmap_bits(heap_sp + ContinuationHelper::CompiledFrame::size(hf), added_argsize);\n+  }\n+\n+  DEBUG_ONLY(after_thaw_java_frame(f, bottom);)\n+  caller = f;\n+}\n+\n+void ThawBase::recurse_thaw_stub_frame(const frame& hf, frame& caller, int num_frames) {\n+  DEBUG_ONLY(_frames++;)\n+\n+  {\n+    RegisterMap map(nullptr, true, false, false);\n+    map.set_include_argument_oops(false);\n+    _stream.next(&map);\n+    assert(!_stream.is_done(), \"\");\n+    if (UNLIKELY(_barriers)) { \/\/ we're now doing this on the stub's caller\n+      _cont.tail()->do_barriers<stackChunkOopDesc::BarrierType::Store>(_stream, &map);\n+    }\n+    assert(!_stream.is_done(), \"\");\n+  }\n+\n+  recurse_thaw_compiled_frame(_stream.to_frame(), caller, num_frames, true); \/\/ this could be deoptimized\n+\n+  DEBUG_ONLY(before_thaw_java_frame(hf, caller, false, num_frames);)\n+\n+  assert(ContinuationHelper::Frame::is_stub(hf.cb()), \"\");\n+  assert(caller.sp() == caller.unextended_sp(), \"\");\n+  assert(!caller.is_interpreted_frame(), \"\");\n+\n+  int fsize = ContinuationHelper::StubFrame::size(hf);\n+\n+  frame f = new_stack_frame<ContinuationHelper::StubFrame>(hf, caller, false);\n+  intptr_t* frame_sp = f.sp();\n+  intptr_t* heap_sp = hf.sp();\n+\n+  copy_from_chunk(heap_sp - frame::metadata_words, frame_sp - frame::metadata_words,\n+                  fsize + frame::metadata_words);\n+\n+  { \/\/ can only fix caller once this frame is thawed (due to callee saved regs)\n+    RegisterMap map(nullptr, true, false, false); \/\/ map.clear();\n+    map.set_include_argument_oops(false);\n+    f.oop_map()->update_register_map(&f, &map);\n+    ContinuationHelper::update_register_map_with_callee(caller, &map);\n+    _cont.tail()->fix_thawed_frame(caller, &map);\n+  }\n+\n+  DEBUG_ONLY(after_thaw_java_frame(f, false);)\n+  caller = f;\n+}\n+\n+void ThawBase::finish_thaw(frame& f) {\n+  stackChunkOop chunk = _cont.tail();\n+\n+  if (chunk->is_empty()) {\n+    \/\/ Only remove chunk from list if it can't be reused for another freeze\n+    if (_barriers) {\n+      _cont.set_tail(chunk->parent());\n+    } else {\n+      chunk->set_has_mixed_frames(false);\n+    }\n+    chunk->set_max_size(0);\n+    assert(chunk->argsize() == 0, \"\");\n+  } else {\n+    chunk->set_max_size(chunk->max_size() - _align_size);\n+  }\n+  assert(chunk->is_empty() == (chunk->max_size() == 0), \"\");\n+\n+  if ((intptr_t)f.sp() % frame::frame_alignment != 0) {\n+    assert(f.is_interpreted_frame(), \"\");\n+    f.set_sp(f.sp() - 1);\n+  }\n+  push_return_frame(f);\n+  chunk->fix_thawed_frame(f, SmallRegisterMap::instance); \/\/ can only fix caller after push_return_frame (due to callee saved regs)\n+\n+  assert(_cont.is_empty() == _cont.last_frame().is_empty(), \"\");\n+\n+  log_develop_trace(continuations)(\"thawed %d frames\", _frames);\n+\n+  LogTarget(Trace, continuations) lt;\n+  if (lt.develop_is_enabled()) {\n+    LogStream ls(lt);\n+    ls.print_cr(\"top hframe after (thaw):\");\n+    _cont.last_frame().print_value_on(&ls, nullptr);\n+  }\n+}\n+\n+void ThawBase::push_return_frame(frame& f) { \/\/ see generate_cont_thaw\n+  assert(!f.is_compiled_frame() || f.is_deoptimized_frame() == f.cb()->as_compiled_method()->is_deopt_pc(f.raw_pc()), \"\");\n+  assert(!f.is_compiled_frame() || f.is_deoptimized_frame() == (f.pc() != f.raw_pc()), \"\");\n+\n+  LogTarget(Trace, continuations) lt;\n+  if (lt.develop_is_enabled()) {\n+    LogStream ls(lt);\n+    ls.print_cr(\"push_return_frame\");\n+    f.print_value_on(&ls, nullptr);\n+  }\n+\n+  assert(f.sp() - frame::metadata_words >= _top_stack_address, \"overwrote past thawing space\"\n+    \" to: \" INTPTR_FORMAT \" top_address: \" INTPTR_FORMAT, p2i(f.sp() - frame::metadata_words), p2i(_top_stack_address));\n+  ContinuationHelper::Frame::patch_pc(f, f.raw_pc()); \/\/ in case we want to deopt the frame in a full transition, this is checked.\n+  ContinuationHelper::push_pd(f);\n+\n+  assert(ContinuationHelper::Frame::assert_frame_laid_out(f), \"\");\n+}\n+\n+\/\/ returns new top sp\n+\/\/ called after preparations (stack overflow check and making room)\n+template<typename ConfigT>\n+static inline intptr_t* thaw_internal(JavaThread* thread, const thaw_kind kind) {\n+  assert(thread == JavaThread::current(), \"Must be current thread\");\n+\n+  CONT_JFR_ONLY(EventContinuationThaw event;)\n+\n+  log_develop_trace(continuations)(\"~~~~ thaw kind: %d sp: \" INTPTR_FORMAT, kind, p2i(thread->last_continuation()->entry_sp()));\n+\n+  ContinuationEntry* entry = thread->last_continuation();\n+  assert(entry != nullptr, \"\");\n+  oop oopCont = entry->cont_oop();\n+\n+  assert(!jdk_internal_vm_Continuation::done(oopCont), \"\");\n+  assert(oopCont == get_continuation(thread), \"\");\n+  verify_continuation(oopCont);\n+\n+  assert(entry->is_virtual_thread() == (entry->scope() == java_lang_VirtualThread::vthread_scope()), \"\");\n+\n+  ContinuationWrapper cont(thread, oopCont);\n+  log_develop_debug(continuations)(\"THAW #\" INTPTR_FORMAT \" \" INTPTR_FORMAT, cont.hash(), p2i((oopDesc*)oopCont));\n+\n+#ifdef ASSERT\n+  set_anchor_to_entry(thread, cont.entry());\n+  log_frames(thread);\n+  clear_anchor(thread);\n+#endif\n+\n+  Thaw<ConfigT> thw(thread, cont);\n+  intptr_t* const sp = thw.thaw(kind);\n+  assert(is_aligned(sp, frame::frame_alignment), \"\");\n+\n+  thread->reset_held_monitor_count();\n+\n+  verify_continuation(cont.continuation());\n+\n+#ifdef ASSERT\n+  intptr_t* sp0 = sp;\n+  address pc0 = *(address*)(sp - frame::sender_sp_ret_address_offset());\n+  set_anchor(thread, sp0);\n+  log_frames(thread);\n+  if (LoomVerifyAfterThaw) {\n+    assert(do_verify_after_thaw(thread, thw.barriers(), cont.tail(), tty), \"\");\n+  }\n+  assert(ContinuationEntry::assert_entry_frame_laid_out(thread), \"\");\n+  clear_anchor(thread);\n+\n+  LogTarget(Trace, continuations) lt;\n+  if (lt.develop_is_enabled()) {\n+    LogStream ls(lt);\n+    ls.print_cr(\"Jumping to frame (thaw):\");\n+    frame(sp).print_value_on(&ls, nullptr);\n+  }\n+#endif\n+\n+  CONT_JFR_ONLY(thw.jfr_info().post_jfr_event(&event, cont.continuation(), thread);)\n+\n+  verify_continuation(cont.continuation());\n+  log_develop_debug(continuations)(\"=== End of thaw #\" INTPTR_FORMAT, cont.hash());\n+\n+  return sp;\n+}\n+\n+#ifdef ASSERT\n+static void do_deopt_after_thaw(JavaThread* thread) {\n+  int i = 0;\n+  StackFrameStream fst(thread, true, false);\n+  fst.register_map()->set_include_argument_oops(false);\n+  ContinuationHelper::update_register_map_with_callee(*fst.current(), fst.register_map());\n+  for (; !fst.is_done(); fst.next()) {\n+    if (fst.current()->cb()->is_compiled()) {\n+      CompiledMethod* cm = fst.current()->cb()->as_compiled_method();\n+      if (!cm->method()->is_continuation_enter_intrinsic()) {\n+        cm->make_deoptimized();\n+      }\n+    }\n+  }\n+}\n+\n+class ThawVerifyOopsClosure: public OopClosure {\n+  intptr_t* _p;\n+  outputStream* _st;\n+  bool is_good_oop(oop o) {\n+    return dbg_is_safe(o, -1) && dbg_is_safe(o->klass(), -1) && oopDesc::is_oop(o) && o->klass()->is_klass();\n+  }\n+public:\n+  ThawVerifyOopsClosure(outputStream* st) : _p(nullptr), _st(st) {}\n+  intptr_t* p() { return _p; }\n+  void reset() { _p = nullptr; }\n+\n+  virtual void do_oop(oop* p) {\n+    oop o = *p;\n+    if (o == nullptr || is_good_oop(o)) {\n+      return;\n+    }\n+    _p = (intptr_t*)p;\n+    _st->print_cr(\"*** non-oop \" PTR_FORMAT \" found at \" PTR_FORMAT, p2i(*p), p2i(p));\n+  }\n+  virtual void do_oop(narrowOop* p) {\n+    oop o = RawAccess<>::oop_load(p);\n+    if (o == nullptr || is_good_oop(o)) {\n+      return;\n+    }\n+    _p = (intptr_t*)p;\n+    _st->print_cr(\"*** (narrow) non-oop %x found at \" PTR_FORMAT, (int)(*p), p2i(p));\n+  }\n+};\n+\n+static bool do_verify_after_thaw(JavaThread* thread, bool barriers, stackChunkOop chunk, outputStream* st) {\n+  assert(thread->has_last_Java_frame(), \"\");\n+\n+  ResourceMark rm;\n+  ThawVerifyOopsClosure cl(st);\n+  CodeBlobToOopClosure cf(&cl, false);\n+\n+  StackFrameStream fst(thread, true, false);\n+  fst.register_map()->set_include_argument_oops(false);\n+  ContinuationHelper::update_register_map_with_callee(*fst.current(), fst.register_map());\n+  for (; !fst.is_done() && !Continuation::is_continuation_enterSpecial(*fst.current()); fst.next()) {\n+    if (fst.current()->cb()->is_compiled() && fst.current()->cb()->as_compiled_method()->is_marked_for_deoptimization()) {\n+      st->print_cr(\">>> do_verify_after_thaw deopt\");\n+      fst.current()->deoptimize(nullptr);\n+      fst.current()->print_on(st);\n+    }\n+\n+    fst.current()->oops_do(&cl, &cf, fst.register_map());\n+    if (cl.p() != nullptr) {\n+      frame fr = *fst.current();\n+      st->print_cr(\"Failed for frame barriers: %d %d\", barriers, chunk->requires_barriers());\n+      fr.print_on(st);\n+      if (!fr.is_interpreted_frame()) {\n+        st->print_cr(\"size: %d argsize: %d\",\n+                     ContinuationHelper::NonInterpretedUnknownFrame::size(fr),\n+                     ContinuationHelper::NonInterpretedUnknownFrame::stack_argsize(fr));\n+      }\n+      VMReg reg = fst.register_map()->find_register_spilled_here(cl.p(), fst.current()->sp());\n+      if (reg != nullptr) {\n+        st->print_cr(\"Reg %s %d\", reg->name(), reg->is_stack() ? (int)reg->reg2stack() : -99);\n+      }\n+      cl.reset();\n+      DEBUG_ONLY(thread->print_frame_layout();)\n+      if (chunk != nullptr) {\n+        chunk->print_on(true, st);\n+      }\n+      return false;\n+    }\n+  }\n+  return true;\n+}\n+\n+static void log_frames(JavaThread* thread) {\n+  const static int show_entry_callers = 3;\n+  LogTarget(Trace, continuations) lt;\n+  if (!lt.develop_is_enabled()) {\n+    return;\n+  }\n+  LogStream ls(lt);\n+\n+  ls.print_cr(\"------- frames ---------\");\n+  if (!thread->has_last_Java_frame()) {\n+    ls.print_cr(\"NO ANCHOR!\");\n+  }\n+\n+  RegisterMap map(thread, true, true, false);\n+  map.set_include_argument_oops(false);\n+\n+  if (false) {\n+    for (frame f = thread->last_frame(); !f.is_entry_frame(); f = f.sender(&map)) {\n+      f.print_on(&ls);\n+    }\n+  } else {\n+    map.set_skip_missing(true);\n+    ResetNoHandleMark rnhm;\n+    ResourceMark rm;\n+    HandleMark hm(Thread::current());\n+    FrameValues values;\n+\n+    int i = 0;\n+    int post_entry = -1;\n+    for (frame f = thread->last_frame(); !f.is_entry_frame(); f = f.sender(&map)) {\n+      f.describe(values, i++, &map);\n+      if (post_entry >= 0 || Continuation::is_continuation_enterSpecial(f))\n+        post_entry++;\n+      if (post_entry >= show_entry_callers)\n+        break;\n+    }\n+    values.print_on(thread, &ls);\n+  }\n+\n+  ls.print_cr(\"======= end frames =========\");\n+}\n+#endif \/\/ ASSERT\n+\n+#include CPU_HEADER_INLINE(continuationFreezeThaw)\n+\n+#ifdef ASSERT\n+static void print_frame_layout(const frame& f, bool callee_complete, outputStream* st) {\n+  ResourceMark rm;\n+  FrameValues values;\n+  assert(f.get_cb() != nullptr, \"\");\n+  RegisterMap map(f.is_heap_frame() ?\n+                  (JavaThread*)nullptr :\n+                  JavaThread::current(), true, false, false);\n+  map.set_include_argument_oops(false);\n+  map.set_skip_missing(true);\n+  if (callee_complete) {\n+    frame::update_map_with_saved_link(&map, ContinuationHelper::Frame::callee_link_address(f));\n+  }\n+  const_cast<frame&>(f).describe(values, 0, &map);\n+  values.print_on((JavaThread*)nullptr, st);\n+}\n+#endif\n+\n+static address thaw_entry   = nullptr;\n+static address freeze_entry = nullptr;\n+\n+address Continuation::thaw_entry() {\n+  return ::thaw_entry;\n+}\n+\n+address Continuation::freeze_entry() {\n+  return ::freeze_entry;\n+}\n+\n+class ConfigResolve {\n+public:\n+  static void resolve() { resolve_compressed(); }\n+\n+  static void resolve_compressed() {\n+    UseCompressedOops ? resolve_gc<true>()\n+                      : resolve_gc<false>();\n+  }\n+\n+private:\n+  template <bool use_compressed>\n+  static void resolve_gc() {\n+    BarrierSet* bs = BarrierSet::barrier_set();\n+    assert(bs != NULL, \"freeze\/thaw invoked before BarrierSet is set\");\n+    switch (bs->kind()) {\n+#define BARRIER_SET_RESOLVE_BARRIER_CLOSURE(bs_name)                    \\\n+      case BarrierSet::bs_name: {                                       \\\n+        resolve<use_compressed, typename BarrierSet::GetType<BarrierSet::bs_name>::type>(); \\\n+      }                                                                 \\\n+        break;\n+      FOR_EACH_CONCRETE_BARRIER_SET_DO(BARRIER_SET_RESOLVE_BARRIER_CLOSURE)\n+#undef BARRIER_SET_RESOLVE_BARRIER_CLOSURE\n+\n+    default:\n+      fatal(\"BarrierSet resolving not implemented\");\n+    };\n+  }\n+\n+  template <bool use_compressed, typename BarrierSetT>\n+  static void resolve() {\n+    typedef Config<use_compressed ? oop_kind::NARROW : oop_kind::WIDE, BarrierSetT> SelectedConfigT;\n+\n+    freeze_entry = (address)freeze<SelectedConfigT>;\n+\n+    \/\/ If we wanted, we could templatize by kind and have three different thaw entries\n+    thaw_entry   = (address)thaw<SelectedConfigT>;\n+  }\n+};\n+\n+void Continuation::init() {\n+  ConfigResolve::resolve();\n+}\n","filename":"src\/hotspot\/share\/runtime\/continuationFreezeThaw.cpp","additions":2434,"deletions":0,"binary":false,"changes":2434,"status":"added"},{"patch":"@@ -35,0 +35,1 @@\n+class ContinuationEntry;\n@@ -38,2 +39,2 @@\n-  static void set_anchor_pd(JavaFrameAnchor* anchor, intptr_t* sp);\n-  static void set_anchor_to_entry_pd(JavaFrameAnchor* anchor, ContinuationEntry* entry);\n+  static inline void set_anchor_pd(JavaFrameAnchor* anchor, intptr_t* sp);\n+  static inline void set_anchor_to_entry_pd(JavaFrameAnchor* anchor, ContinuationEntry* entry);\n@@ -42,1 +43,1 @@\n-  static void update_register_map_with_callee(const frame& f, RegisterMap* map);\n+  static inline void update_register_map_with_callee(const frame& f, RegisterMap* map);\n@@ -66,1 +67,1 @@\n-  static inline Method* frame_method(const frame& f);\n+  static Method* frame_method(const frame& f);\n@@ -70,1 +71,1 @@\n-  static address return_pc(const frame& f);\n+  static address return_pc(const frame& f) { return *return_pc_address(f); }\n@@ -96,1 +97,1 @@\n-  static inline address return_pc(const frame& f);\n+  static address return_pc(const frame& f);\n","filename":"src\/hotspot\/share\/runtime\/continuationHelper.hpp","additions":7,"deletions":6,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -51,4 +51,0 @@\n-inline address ContinuationHelper::Frame::return_pc(const frame& f) {\n-  return *return_pc_address(f);\n-}\n-\n","filename":"src\/hotspot\/share\/runtime\/continuationHelper.inline.hpp","additions":0,"deletions":4,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -0,0 +1,101 @@\n+\/*\n+ * Copyright (c) 2018, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"classfile\/javaClasses.inline.hpp\"\n+#include \"jfr\/jfrEvents.hpp\"\n+#include \"jfr\/support\/jfrThreadId.hpp\"\n+#include \"logging\/log.hpp\"\n+#include \"logging\/logStream.hpp\"\n+#include \"oops\/oopsHierarchy.hpp\"\n+#include \"oops\/stackChunkOop.inline.hpp\"\n+#include \"runtime\/continuation.hpp\"\n+#include \"runtime\/continuationEntry.inline.hpp\"\n+#include \"runtime\/continuationHelper.inline.hpp\"\n+#include \"runtime\/continuationWrapper.inline.hpp\"\n+#include \"runtime\/stackChunkFrameStream.inline.hpp\"\n+#include \"runtime\/thread.inline.hpp\"\n+\n+ContinuationWrapper::ContinuationWrapper(const RegisterMap* map)\n+  : _thread(map->thread()),\n+    _entry(Continuation::get_continuation_entry_for_continuation(_thread, map->stack_chunk()->cont())),\n+    _continuation(map->stack_chunk()->cont())\n+  {\n+  assert(oopDesc::is_oop(_continuation),\"Invalid cont: \" INTPTR_FORMAT, p2i((void*)_continuation));\n+  assert(_entry == nullptr || _continuation == _entry->cont_oop(),\n+    \"cont: \" INTPTR_FORMAT \" entry: \" INTPTR_FORMAT \" entry_sp: \" INTPTR_FORMAT,\n+    p2i( (oopDesc*)_continuation), p2i((oopDesc*)_entry->cont_oop()), p2i(entrySP()));\n+  disallow_safepoint();\n+  read();\n+}\n+\n+const frame ContinuationWrapper::last_frame() {\n+  stackChunkOop chunk = last_nonempty_chunk();\n+  if (chunk == nullptr) {\n+    return frame();\n+  }\n+  return StackChunkFrameStream<ChunkFrames::Mixed>(chunk).to_frame();\n+}\n+\n+stackChunkOop ContinuationWrapper::find_chunk_by_address(void* p) const {\n+  for (stackChunkOop chunk = tail(); chunk != nullptr; chunk = chunk->parent()) {\n+    if (chunk->is_in_chunk(p)) {\n+      assert(chunk->is_usable_in_chunk(p), \"\");\n+      return chunk;\n+    }\n+  }\n+  return nullptr;\n+}\n+\n+#ifndef PRODUCT\n+intptr_t ContinuationWrapper::hash() {\n+  return Thread::current()->is_Java_thread() ? _continuation->identity_hash() : -1;\n+}\n+#endif\n+\n+#ifdef ASSERT\n+bool ContinuationWrapper::is_entry_frame(const frame& f) {\n+  return f.sp() == entrySP();\n+}\n+\n+bool ContinuationWrapper::chunk_invariant(outputStream* st) {\n+  \/\/ only the topmost chunk can be empty\n+  if (_tail == nullptr) {\n+    return true;\n+  }\n+\n+  int i = 1;\n+  for (stackChunkOop chunk = _tail->parent(); chunk != nullptr; chunk = chunk->parent()) {\n+    if (chunk->is_empty()) {\n+      assert(chunk != _tail, \"\");\n+      st->print_cr(\"i: %d\", i);\n+      chunk->print_on(true, st);\n+      return false;\n+    }\n+    i++;\n+  }\n+  return true;\n+}\n+#endif \/\/ ASSERT\n+\n","filename":"src\/hotspot\/share\/runtime\/continuationWrapper.cpp","additions":101,"deletions":0,"binary":false,"changes":101,"status":"added"},{"patch":"@@ -0,0 +1,191 @@\n+\/*\n+ * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_VM_RUNTIME_CONTINUATIONWRAPPER_INLINE_HPP\n+#define SHARE_VM_RUNTIME_CONTINUATIONWRAPPER_INLINE_HPP\n+\n+\/\/ There is no continuationWrapper.hpp file\n+\n+#include \"classfile\/javaClasses.inline.hpp\"\n+#include \"oops\/oop.inline.hpp\"\n+#include \"memory\/allocation.hpp\"\n+#include \"oops\/oopsHierarchy.hpp\"\n+#include \"oops\/stackChunkOop.hpp\"\n+#include \"runtime\/continuationEntry.inline.hpp\"\n+#include \"runtime\/thread.hpp\"\n+\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+\/\/ Intermediary to the jdk.internal.vm.Continuation objects and ContinuationEntry\n+\/\/ This object is created when we begin a operation for a continuation, and is destroyed when the operation completes.\n+\/\/ Contents are read from the Java object at the entry points of this module, and written at exit or calls into Java\n+\/\/ It also serves as a custom NoSafepointVerifier\n+class ContinuationWrapper : public StackObj {\n+private:\n+  JavaThread* const  _thread;   \/\/ Thread being frozen\/thawed\n+  ContinuationEntry* _entry;\n+  \/\/ These oops are managed by SafepointOp\n+  oop                _continuation;  \/\/ jdk.internal.vm.Continuation instance\n+  stackChunkOop      _tail;\n+\n+  ContinuationWrapper(const ContinuationWrapper& cont); \/\/ no copy constructor\n+\n+private:\n+  DEBUG_ONLY(Thread* _current_thread;)\n+  friend class SafepointOp;\n+\n+  void disallow_safepoint() {\n+    #ifdef ASSERT\n+      assert(_continuation != nullptr, \"\");\n+      _current_thread = Thread::current();\n+      if (_current_thread->is_Java_thread()) {\n+        JavaThread::cast(_current_thread)->inc_no_safepoint_count();\n+      }\n+    #endif\n+  }\n+\n+  void allow_safepoint() {\n+    #ifdef ASSERT\n+      \/\/ we could have already allowed safepoints in done\n+      if (_continuation != nullptr && _current_thread->is_Java_thread()) {\n+        JavaThread::cast(_current_thread)->dec_no_safepoint_count();\n+      }\n+    #endif\n+  }\n+\n+public:\n+  void done() {\n+    allow_safepoint(); \/\/ must be done first\n+    _continuation = nullptr;\n+    _tail = (stackChunkOop)badOop;\n+  }\n+\n+  class SafepointOp : public StackObj {\n+    ContinuationWrapper& _cont;\n+    Handle _conth;\n+  public:\n+    SafepointOp(Thread* current, ContinuationWrapper& cont)\n+      : _cont(cont), _conth(current, cont._continuation) {\n+      _cont.allow_safepoint();\n+    }\n+    inline ~SafepointOp() { \/\/ reload oops\n+      _cont._continuation = _conth();\n+      if (_cont._tail != nullptr) {\n+        _cont._tail = jdk_internal_vm_Continuation::tail(_cont._continuation);\n+       }\n+       _cont.disallow_safepoint();\n+    }\n+  };\n+\n+public:\n+  ~ContinuationWrapper() { allow_safepoint(); }\n+\n+  ContinuationWrapper(JavaThread* thread, oop continuation);\n+  ContinuationWrapper(oop continuation);\n+  ContinuationWrapper(const RegisterMap* map);\n+\n+  JavaThread* thread() const         { return _thread; }\n+  oop continuation()                 { return _continuation; }\n+  stackChunkOop tail() const         { return _tail; }\n+  void set_tail(stackChunkOop chunk) { _tail = chunk; }\n+\n+  inline oop parent();\n+  inline bool is_preempted();\n+  inline void set_preempted(bool value);\n+  inline void read();\n+  inline void write();\n+\n+  NOT_PRODUCT(intptr_t hash();)\n+\n+  ContinuationEntry* entry() const { return _entry; }\n+  bool is_mounted()   const { return _entry != nullptr; }\n+  intptr_t* entrySP() const { return _entry->entry_sp(); }\n+  intptr_t* entryFP() const { return _entry->entry_fp(); }\n+  address   entryPC() const { return _entry->entry_pc(); }\n+  int argsize()       const { assert(_entry->argsize() >= 0, \"\"); return _entry->argsize(); }\n+  void set_argsize(int value) { _entry->set_argsize(value); }\n+\n+  bool is_empty() const { return last_nonempty_chunk() == nullptr; }\n+  const frame last_frame();\n+\n+  stackChunkOop last_nonempty_chunk() const { return nonempty_chunk(_tail); }\n+  inline stackChunkOop nonempty_chunk(stackChunkOop chunk) const;\n+  stackChunkOop find_chunk_by_address(void* p) const;\n+\n+#ifdef ASSERT\n+  bool is_entry_frame(const frame& f);\n+  bool chunk_invariant(outputStream* st);\n+#endif\n+};\n+\n+inline ContinuationWrapper::ContinuationWrapper(JavaThread* thread, oop continuation)\n+  : _thread(thread), _entry(thread->last_continuation()), _continuation(continuation)\n+  {\n+  assert(oopDesc::is_oop(_continuation),\n+         \"Invalid continuation object: \" INTPTR_FORMAT, p2i((void*)_continuation));\n+  assert(_continuation == _entry->cont_oop(), \"cont: \" INTPTR_FORMAT \" entry: \" INTPTR_FORMAT \" entry_sp: \"\n+         INTPTR_FORMAT, p2i((oopDesc*)_continuation), p2i((oopDesc*)_entry->cont_oop()), p2i(entrySP()));\n+  disallow_safepoint();\n+  read();\n+}\n+\n+inline ContinuationWrapper::ContinuationWrapper(oop continuation)\n+  : _thread(nullptr), _entry(nullptr), _continuation(continuation)\n+  {\n+  assert(oopDesc::is_oop(_continuation),\n+         \"Invalid continuation object: \" INTPTR_FORMAT, p2i((void*)_continuation));\n+  disallow_safepoint();\n+  read();\n+}\n+\n+inline oop ContinuationWrapper::parent() {\n+  return jdk_internal_vm_Continuation::parent(_continuation);\n+}\n+\n+inline bool ContinuationWrapper::is_preempted() {\n+  return jdk_internal_vm_Continuation::is_preempted(_continuation);\n+}\n+\n+inline void ContinuationWrapper::set_preempted(bool value) {\n+  jdk_internal_vm_Continuation::set_preempted(_continuation, value);\n+}\n+\n+inline void ContinuationWrapper::read() {\n+  _tail  = jdk_internal_vm_Continuation::tail(_continuation);\n+}\n+\n+inline void ContinuationWrapper::write() {\n+  assert(oopDesc::is_oop(_continuation), \"bad oop\");\n+  assert(oopDesc::is_oop_or_null(_tail), \"bad oop\");\n+  jdk_internal_vm_Continuation::set_tail(_continuation, _tail);\n+}\n+\n+inline stackChunkOop ContinuationWrapper::nonempty_chunk(stackChunkOop chunk) const {\n+  while (chunk != nullptr && chunk->is_empty()) {\n+    chunk = chunk->parent();\n+  }\n+  return chunk;\n+}\n+\n+#endif \/\/ SHARE_VM_RUNTIME_CONTINUATIONWRAPPER_INLINE_HPP\n","filename":"src\/hotspot\/share\/runtime\/continuationWrapper.inline.hpp","additions":191,"deletions":0,"binary":false,"changes":191,"status":"added"},{"patch":"@@ -60,0 +60,1 @@\n+#include \"runtime\/continuationEntry.inline.hpp\"\n","filename":"src\/hotspot\/share\/runtime\/deoptimization.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -47,0 +47,1 @@\n+#include \"runtime\/continuationEntry.inline.hpp\"\n","filename":"src\/hotspot\/share\/runtime\/frame.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -81,0 +81,2 @@\n+#include \"runtime\/continuationEntry.inline.hpp\"\n+#include \"runtime\/continuationHelper.inline.hpp\"\n@@ -2491,0 +2493,5 @@\n+oop JavaThread::get_continuation() const {\n+  assert(threadObj() != nullptr, \"must be set\");\n+  return java_lang_Thread::continuation(threadObj());\n+}\n+\n","filename":"src\/hotspot\/share\/runtime\/thread.cpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -1189,0 +1189,1 @@\n+  oop get_continuation() const;\n","filename":"src\/hotspot\/share\/runtime\/thread.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -38,0 +38,1 @@\n+#include \"runtime\/continuationEntry.inline.hpp\"\n","filename":"src\/hotspot\/share\/runtime\/thread.inline.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -56,0 +56,1 @@\n+class ContinuationEntry;\n","filename":"src\/hotspot\/share\/runtime\/vframe.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -119,1 +119,1 @@\n-                long comp = Blocker.begin();\n+                long comp = Blocker.begin(blocking);\n","filename":"src\/java.base\/linux\/classes\/sun\/nio\/ch\/EPollSelectorImpl.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -123,1 +123,1 @@\n-                long comp = Blocker.begin();\n+                long comp = Blocker.begin(blocking);\n","filename":"src\/java.base\/macosx\/classes\/sun\/nio\/ch\/KQueueSelectorImpl.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -117,2 +117,0 @@\n- *\n- *\n@@ -127,3 +125,2 @@\n- * machine from terminating. They have a fixed priority (see {@link #getPriority()\n- * getPriority}). Virtual threads are not members of a thread group (see\n- * {@link #getThreadGroup() getThreadGroup}).\n+ * machine from terminating. Virtual threads have a fixed {@linkplain #getPriority()\n+ * thread priority} that cannot be changed.\n@@ -862,1 +859,1 @@\n-         * @throws IllegalArgumentException if count is negative\n+         * @throws IllegalArgumentException if start is negative\n@@ -1921,1 +1918,2 @@\n-     * The priority of a virtual thread is always {@link Thread#NORM_PRIORITY}.\n+     *\n+     * <p> The priority of a virtual thread is always {@link Thread#NORM_PRIORITY}.\n@@ -1980,18 +1978,3 @@\n-     * <p> Virtual threads are not members of a thread group. If invoked on a\n-     * virtual thread that has not terminated, this method returns a special\n-     * thread group that behaves as follows:\n-     * <ul>\n-     *  <li> There are no {@linkplain ThreadGroup#activeCount() active} virtual\n-     *       threads in the thread group. The {@link ThreadGroup#enumerate(Thread[])\n-     *       enumerate} method does not enumerate virtual threads.\n-     *  <li> There may be active platform threads in the thread group. The thread\n-     *       group may be provided when creating a platform thread, the thread group\n-     *       may be <a href=\"Thread.html#inheritance\">inherited<\/a> when a\n-     *       virtual thread creates a platform thread, or the thread group may\n-     *       be inherited when a platform thread in the thread group creates\n-     *       another platform thread.\n-     *  <li> The {@linkplain ThreadGroup#getMaxPriority() maximum priority} of\n-     *       the thread group is {@link Thread#NORM_PRIORITY} when initially\n-     *       created. Changing the maximum priority of the thread group has no\n-     *       impact on the priority of virtual threads.\n-     * <\/ul>\n+     * <p> The thread group returned for a virtual thread is the special\n+     * <a href=\"ThreadGroup.html#virtualthreadgroup\"><em>ThreadGroup for\n+     * virtual threads<\/em><\/a>.\n@@ -2003,1 +1986,1 @@\n-            return isVirtual() ? Constants.VTHREAD_GROUP : holder.group;\n+            return isVirtual() ? virtualThreadGroup() : holder.group;\n@@ -2010,5 +1993,3 @@\n-     * Returns an estimate of the number of active threads in the current\n-     * thread's {@linkplain java.lang.ThreadGroup thread group} and its\n-     * subgroups. Virtual threads are not considered active threads in a\n-     * thread group so this method does not include virtual threads in the\n-     * estimate.\n+     * Returns an estimate of the number of {@linkplain #isAlive() live}\n+     * platform threads in the current thread's thread group and its subgroups.\n+     * Virtual threads are not included in the estimate.\n@@ -2022,3 +2003,3 @@\n-     * @return  an estimate of the number of active threads in the current\n-     *          thread's thread group and in any other thread group that\n-     *          has the current thread's thread group as an ancestor\n+     * @return  an estimate of the number of live platform threads in the\n+     *          current thread's thread group and in any other thread group\n+     *          that has the current thread's thread group as an ancestor\n@@ -2031,3 +2012,3 @@\n-     * Copies into the specified array every active thread in the current\n-     * thread's thread group and its subgroups. This method simply\n-     * invokes the {@link java.lang.ThreadGroup#enumerate(Thread[])}\n+     * Copies into the specified array every {@linkplain #isAlive() live}\n+     * platform thread in the current thread's thread group and its subgroups.\n+     * This method simply invokes the {@link java.lang.ThreadGroup#enumerate(Thread[])}\n@@ -2035,2 +2016,1 @@\n-     * not considered active threads in a thread group so this method\n-     * does not enumerate virtual threads.\n+     * not enumerated by this method.\n@@ -2041,1 +2021,1 @@\n-     * are silently ignored.<\/i>  If it is critical to obtain every active\n+     * are silently ignored.<\/i>  If it is critical to obtain every live\n@@ -2402,1 +2382,2 @@\n-     * The context {@code ClassLoader} may be set by the creator of the thread\n+     *\n+     * <p> The context {@code ClassLoader} may be set by the creator of the thread\n@@ -2405,0 +2386,4 @@\n+     * <p> The context {@code ClassLoader} cannot be set when the thread is\n+     * {@linkplain Thread.Builder#allowSetThreadLocals(boolean) not allowed} to have\n+     * its own copy of thread local variables.\n+     *\n@@ -2995,1 +2980,1 @@\n-            VTHREAD_GROUP = new ThreadGroup(root, \"VirtualThreads\", NORM_PRIORITY, false);\n+            VTHREAD_GROUP = new ThreadGroup(root, \"VirtualThreads\", MAX_PRIORITY, false);\n@@ -3013,0 +2998,7 @@\n+    \/**\n+     * Returns the special ThreadGroup for virtual threads.\n+     *\/\n+    static ThreadGroup virtualThreadGroup() {\n+        return Constants.VTHREAD_GROUP;\n+    }\n+\n","filename":"src\/java.base\/share\/classes\/java\/lang\/Thread.java","additions":34,"deletions":42,"binary":false,"changes":76,"status":"modified"},{"patch":"@@ -47,1 +47,1 @@\n- * #setMaxPriority(int)} method.\n+ * #setMaxPriority(int) setMaxPriority} method.\n@@ -57,0 +57,10 @@\n+ * <h2><a id=\"virtualthreadgroup\">The ThreadGroup for virtual threads<\/a><\/h2>\n+ * <a href=\"Thread.html#virtual-threads\">Virtual threads<\/a> are considered members\n+ * of a special thread group that is created by the Java runtime. The thread group\n+ * differs to other thread groups in that its maximum priority is fixed and cannot\n+ * be changed with the {@link #setMaxPriority(int) setMaxPriority} method.\n+ * Virtual threads are not included in the estimated thread count returned by\n+ * the {@link #activeCount() activeCount} method, are not enumerated by the {@link\n+ * #enumerate(Thread[]) enumerate} method, and are not interrupted by the {@link\n+ * #interrupt() interrupt} method.\n+ *\n@@ -259,2 +269,5 @@\n-     * Sets the maximum priority of the group. Threads in the thread\n-     * group that already have a higher priority are not affected.\n+     * Sets the maximum priority of the group. The maximum priority of the\n+     * <a href=\"ThreadGroup.html#virtualthreadgroup\"><em>ThreadGroup for virtual\n+     * threads<\/em><\/a> is not changed by this method (the new priority is ignored).\n+     * Threads in the thread group (or subgroups) that already have a higher\n+     * priority are not affected by this method.\n@@ -291,1 +304,1 @@\n-                } else {\n+                } else if (this != Thread.virtualThreadGroup()) {\n@@ -345,4 +358,4 @@\n-     * Returns an estimate of the number of active (meaning\n-     * {@linkplain Thread#isAlive() alive}) threads in this thread group\n-     * and its subgroups. Recursively iterates over all subgroups in this\n-     * thread group.\n+     * Returns an estimate of the number of {@linkplain Thread#isAlive() live}\n+     * platform threads in this thread group and its subgroups. Virtual threads\n+     * are not included in the estimate. This method recursively iterates over\n+     * all subgroups in this thread group.\n@@ -356,1 +369,1 @@\n-     * @return  an estimate of the number of active threads in this thread\n+     * @return  an estimate of the number of live threads in this thread\n@@ -373,2 +386,3 @@\n-     * Copies into the specified array every active (meaning {@linkplain\n-     * Thread#isAlive() alive}) thread in this thread group and its subgroups.\n+     * Copies into the specified array every {@linkplain Thread#isAlive() live}\n+     * platform thread in this thread group and its subgroups. Virtual threads\n+     * are not enumerated by this method.\n@@ -397,6 +411,6 @@\n-     * Copies into the specified array every active (meaning {@linkplain\n-     * Thread#isAlive() alive}) thread in this thread group. If {@code\n-     * recurse} is {@code true}, this method recursively enumerates all\n-     * subgroups of this thread group and references to every active thread\n-     * in these subgroups are also included. If the array is too short to\n-     * hold all the threads, the extra threads are silently ignored.\n+     * Copies into the specified array every {@linkplain Thread#isAlive() live}\n+     * platform thread in this thread group. Virtual threads are not enumerated\n+     * by this method. If {@code recurse} is {@code true}, this method recursively\n+     * enumerates all subgroups of this thread group and references to every live\n+     * platform thread in these subgroups are also included. If the array is too\n+     * short to hold all the threads, the extra threads are silently ignored.\n@@ -407,1 +421,1 @@\n-     * are silently ignored.<\/i>  If it is critical to obtain every active\n+     * are silently ignored.<\/i>  If it is critical to obtain every live\n@@ -555,1 +569,1 @@\n-     * Interrupts all active (meaning {@linkplain Thread#isAlive() alive}) in\n+     * Interrupts all {@linkplain Thread#isAlive() live} platform threads in\n","filename":"src\/java.base\/share\/classes\/java\/lang\/ThreadGroup.java","additions":33,"deletions":19,"binary":false,"changes":52,"status":"modified"},{"patch":"@@ -65,1 +65,1 @@\n-class VirtualThread extends Thread {\n+final class VirtualThread extends Thread {\n@@ -93,2 +93,2 @@\n-     *  PARKING -> PARKED          \/\/ yield successful, thread is parked\n-     *  PARKING -> PINNED          \/\/ yield failed, thread is pinned\n+     *  PARKING -> PARKED          \/\/ cont.yield successful, thread is parked\n+     *  PARKING -> PINNED          \/\/ cont.yield failed, thread is pinned\n","filename":"src\/java.base\/share\/classes\/java\/lang\/VirtualThread.java","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -3666,7 +3666,8 @@\n-     * For use by jdk.internal.misc.Blocker to mark the beginning of a possibly\n-     * blocking operation on a virtual thread that pins the underlying carrier\n-     * thread in this pool. This method works like managedBlock and invokes\n-     * tryCompensate to create or re-activate a spare thread to compensate for\n-     * the blocked thread. When the blocking operation is done then\n-     * endCompensatedBlock must be invoked (with the value returned by this\n-     * method) to re-adjust the parallelism.\n+     * Invoked reflectively by jdk.internal.misc.Blocker to mark the beginning\n+     * of a possibly blocking operation on a virtual thread that pins the\n+     * underlying carrier thread in this pool.\n+     *\n+     * This method works like managedBlock and invokes tryCompensate to create\n+     * or re-activate a spare thread to compensate for the blocked thread. When\n+     * the blocking operation is done then endCompensatedBlock must be invoked\n+     * with the value returned by this method to re-adjust the parallelism.\n@@ -3686,3 +3687,3 @@\n-     * For use by jdk.internal.misc.Blocker to mark the end of a blocking\n-     * operation on a virtual thread that pinned the underlying carrier\n-     * thread in this pool.\n+     * Invoked reflectively by jdk.internal.misc.Blocker to mark the end of\n+     * a blocking operation on a virtual thread that pinned the underlying\n+     * carrier thread in this pool.\n","filename":"src\/java.base\/share\/classes\/java\/util\/concurrent\/ForkJoinPool.java","additions":11,"deletions":10,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -70,0 +70,1 @@\n+     * @return the return value from the attempt to compensate\n@@ -79,1 +80,0 @@\n-\n@@ -83,0 +83,10 @@\n+    \/**\n+     * Marks the beginning of possibly blocking operation.\n+     * @param blocking true if the operation may block, otherwise false\n+     * @return the return value from the attempt to compensate when blocking is true,\n+     * another value when blocking is false\n+     *\/\n+    public static long begin(boolean blocking) {\n+        return (blocking) ? begin() : 0;\n+    }\n+\n@@ -85,0 +95,1 @@\n+     * @param compensateReturn the value returned by the begin method\n@@ -86,2 +97,2 @@\n-    public static void end(long post) {\n-        if (post > 0) {\n+    public static void end(long compensateReturn) {\n+        if (compensateReturn > 0) {\n@@ -90,1 +101,1 @@\n-            ForkJoinPools.endCompensatedBlock(ct.getPool(), post);\n+            ForkJoinPools.endCompensatedBlock(ct.getPool(), compensateReturn);\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/misc\/Blocker.java","additions":15,"deletions":4,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -112,1 +112,1 @@\n-            long comp = Blocker.begin();\n+            long comp = Blocker.begin(blocking);\n","filename":"src\/java.base\/windows\/classes\/sun\/nio\/ch\/WEPollSelectorImpl.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2186,0 +2186,1 @@\n+    \"See <a href=..\/..\/api\/java.base\/java\/lang\/ThreadGroup.html>java.lang.ThreadGroup<\/a>.\"\n@@ -2217,7 +2218,4 @@\n-        \"Returns the live threads and active thread groups directly contained \"\n-        \"in this thread group. Virtual threads are not considered live threads \"\n-        \"in a thread group and are not included. Threads and thread groups in child \"\n-        \"thread groups are not included. \"\n-        \"A thread is alive if it has been started and has not yet been stopped. \"\n-        \"See <a href=..\/..\/api\/java.base\/java\/lang\/ThreadGroup.html>java.lang.ThreadGroup<\/a> \"\n-        \"for information about active ThreadGroups.\n+        \"Returns the live platform threads and the thread groups directly \"\n+        \"contained in this thread group. Virtual threads are not included. \"\n+        \"Threads and thread groups in child thread groups are not included. \"\n+        \"A thread is alive if it has been started and has not yet terminated.\"\n","filename":"src\/java.se\/share\/data\/jdwp\/jdwp.spec","additions":5,"deletions":7,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -81,5 +81,4 @@\n-     * Returns a List containing a {@link ThreadReference} for each live thread\n-     * in this thread group. Virtual threads are not considered live threads in\n-     * a thread group and are not included. Only the live threads in this\n-     * immediate thread group (and not its subgroups) are returned. A thread is\n-     * alive if it has been started and has not yet been stopped.\n+     * Returns a List containing a {@link ThreadReference} for each live platform\n+     * thread in this thread group. Virtual threads are not included. Only the live\n+     * platform threads in this immediate thread group (and not its subgroups) are\n+     * returned. A thread is alive if it has been started and has not terminated.\n@@ -88,1 +87,1 @@\n-     * live threads from this thread group in the target VM.\n+     * live platform threads from this thread group in the target VM.\n","filename":"src\/jdk.jdi\/share\/classes\/com\/sun\/jdi\/ThreadGroupReference.java","additions":5,"deletions":6,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -24,0 +24,5 @@\n+####\n+# Bugs\n+\n+vmTestbase\/nsk\/jdb\/kill\/kill001\/kill001.java                00000000 generic-all\n+\n@@ -38,0 +43,1 @@\n+vmTestbase\/nsk\/jvmti\/GetThreadCpuTime\/thrcputime001\/TestDescription.java\n","filename":"test\/hotspot\/jtreg\/ProblemList-svc-vthread.txt","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -181,3 +181,0 @@\n-compiler\/c2\/4998314\/Test.java                                                8284109 generic-all\n-compiler\/c2\/irTests\/DivLNodeIdealizationTests.java                           8284115 generic-all\n-\n@@ -186,2 +183,0 @@\n-runtime\/cds\/appcds\/cacheObject\/ArchivedIntegerCacheTest.java                 8283555 generic-all\n-\n","filename":"test\/hotspot\/jtreg\/ProblemList.txt","additions":0,"deletions":5,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -80,1 +80,1 @@\n-    if (!is_virtual || err != JVMTI_ERROR_NO_MORE_FRAMES) { \/\/ TMP work around\n+    if (err != JVMTI_ERROR_NO_MORE_FRAMES) { \/\/ TMP work around\n@@ -83,1 +83,1 @@\n-      LOG(\"## Agent: test_get_frame_location: ignoring JVMTI_ERROR_NO_MORE_FRAMES for vt\\n\\n\");\n+      LOG(\"## Agent: test_get_frame_location: ignoring JVMTI_ERROR_NO_MORE_FRAMES in GetFrameLocation\\n\\n\");\n","filename":"test\/hotspot\/jtreg\/serviceability\/jvmti\/vthread\/SuspendResume1\/libSuspendResume1.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -37,3 +37,3 @@\n-check_jvmti_error_invalid_thread(JNIEnv* jni, const char* msg, jvmtiError err) {\n-  if (err != JVMTI_ERROR_INVALID_THREAD) {\n-    LOG(\"%s failed: expected JVMTI_ERROR_INVALID_THREAD instead of: %d\\n\", msg, err);\n+check_jvmti_error_unsupported_operation(JNIEnv* jni, const char* msg, jvmtiError err) {\n+  if (err != JVMTI_ERROR_UNSUPPORTED_OPERATION) {\n+    LOG(\"%s failed: expected JVMTI_ERROR_UNSUPPORTED_OPERATION instead of: %d\\n\", msg, err);\n@@ -63,2 +63,2 @@\n- * Execute JVMTI functions which currently don't support vthreads and check that\n- * they return error code JVMTI_ERROR_INVALID_THREAD correctly.\n+ * Execute JVMTI functions which currently don't support vthreads and check that they\n+ * return error code JVMTI_ERROR_INVALID_THREAD or JVMTI_ERROR_OPAQUE_FRAME correctly.\n@@ -92,1 +92,1 @@\n-  check_jvmti_error_invalid_thread(jni, \"StopThread\", err);\n+  check_jvmti_error_unsupported_operation(jni, \"StopThread\", err);\n@@ -104,1 +104,1 @@\n-  check_jvmti_error_invalid_thread(jni, \"GetThreadCpuTime\", err);\n+  check_jvmti_error_unsupported_operation(jni, \"GetThreadCpuTime\", err);\n@@ -110,1 +110,1 @@\n-    check_jvmti_error_invalid_thread(jni, \"GetCurrentThreadCpuTime\", err);\n+    check_jvmti_error_unsupported_operation(jni, \"GetCurrentThreadCpuTime\", err);\n@@ -114,1 +114,1 @@\n-  check_jvmti_error_invalid_thread(jni, \"RunAgentThread\", err);\n+  check_jvmti_error_unsupported_operation(jni, \"RunAgentThread\", err);\n@@ -150,1 +150,1 @@\n-  check_jvmti_error_invalid_thread(jni, \"GetCurrentThreadCpuTime\", err);\n+  check_jvmti_error_unsupported_operation(jni, \"GetCurrentThreadCpuTime\", err);\n","filename":"test\/hotspot\/jtreg\/serviceability\/jvmti\/vthread\/VThreadUnsupportedTest\/libVThreadUnsupportedTest.cpp","additions":10,"deletions":10,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -874,2 +874,0 @@\n-java\/util\/logging\/CheckZombieLockTest.java                      8283719 generic-all\n-\n@@ -878,4 +876,0 @@\n-jdk\/jfr\/event\/runtime\/TestClassLoaderStatsEvent.java            8247965 generic-all\n-jdk\/jfr\/tool\/TestPrintXML.java                                  8278262 generic-all\n-jdk\/jfr\/threading\/TestDeepVirtualStackTrace.java                8283068 macosx-aarch64\n-\n","filename":"test\/jdk\/ProblemList.txt","additions":0,"deletions":6,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2049,1 +2049,1 @@\n-     * Test Thread::getThreadGroup of virtual thread created by platform thread.\n+     * Test Thread::getThreadGroup on virtual thread created by platform thread.\n@@ -2066,1 +2066,1 @@\n-     * Test Thread::getThreadGroup of platform thread created by virtual thread.\n+     * Test Thread::getThreadGroup on platform thread created by virtual thread.\n@@ -2078,0 +2078,54 @@\n+    \/**\n+     * Test ThreadGroup returned by Thread::getThreadGroup and subgroup\n+     * created with 2-arg ThreadGroup constructor.\n+     *\/\n+    @Test\n+    public void testThreadGroup3() throws Exception {\n+        var ref = new AtomicReference<ThreadGroup>();\n+        var thread = Thread.startVirtualThread(() -> {\n+            ref.set(Thread.currentThread().getThreadGroup());\n+        });\n+        thread.join();\n+\n+        ThreadGroup vgroup = ref.get();\n+        assertTrue(vgroup.getMaxPriority() == Thread.MAX_PRIORITY);\n+\n+        ThreadGroup group = new ThreadGroup(vgroup, \"group\");\n+        assertTrue(group.getParent() == vgroup);\n+        assertTrue(group.getMaxPriority() == Thread.MAX_PRIORITY);\n+\n+        vgroup.setMaxPriority(Thread.MAX_PRIORITY - 1);\n+        assertTrue(vgroup.getMaxPriority() == Thread.MAX_PRIORITY);\n+        assertTrue(group.getMaxPriority() == Thread.MAX_PRIORITY - 1);\n+\n+        vgroup.setMaxPriority(Thread.MIN_PRIORITY);\n+        assertTrue(vgroup.getMaxPriority() == Thread.MAX_PRIORITY);\n+        assertTrue(group.getMaxPriority() == Thread.MIN_PRIORITY);\n+    }\n+\n+    \/**\n+     * Test ThreadGroup returned by Thread::getThreadGroup and subgroup\n+     * created with 1-arg ThreadGroup constructor.\n+     *\/\n+    @Test\n+    public void testThreadGroup4() throws Exception {\n+        TestHelper.runInVirtualThread(() -> {\n+            ThreadGroup vgroup = Thread.currentThread().getThreadGroup();\n+\n+            assertTrue(vgroup.getMaxPriority() == Thread.MAX_PRIORITY);\n+\n+            ThreadGroup group = new ThreadGroup(\"group\");\n+            assertTrue(group.getParent() == vgroup);\n+            assertTrue(group.getMaxPriority() == Thread.MAX_PRIORITY);\n+\n+            vgroup.setMaxPriority(Thread.MAX_PRIORITY - 1);\n+            assertTrue(vgroup.getMaxPriority() == Thread.MAX_PRIORITY);\n+            assertTrue(group.getMaxPriority() == Thread.MAX_PRIORITY - 1);\n+\n+            vgroup.setMaxPriority(Thread.MIN_PRIORITY);\n+            assertTrue(vgroup.getMaxPriority() == Thread.MAX_PRIORITY);\n+            assertTrue(group.getMaxPriority() == Thread.MIN_PRIORITY);\n+\n+        });\n+    }\n+\n","filename":"test\/jdk\/java\/lang\/Thread\/virtual\/ThreadAPI.java","additions":56,"deletions":2,"binary":false,"changes":58,"status":"modified"}]}