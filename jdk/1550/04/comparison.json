{"files":[{"patch":"@@ -497,1 +497,1 @@\n-  if (!Inline) {\n+  if (!Inline || !IncrementalInline) {\n@@ -499,0 +499,2 @@\n+    IncrementalInlineMH = false;\n+    IncrementalInlineVirtual = false;\n","filename":"src\/hotspot\/share\/compiler\/compilerDefinitions.cpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -716,0 +716,6 @@\n+  product(bool, IncrementalInlineMH, true, DIAGNOSTIC,                      \\\n+          \"do post parse inlining of method handle calls\")                  \\\n+                                                                            \\\n+  product(bool, IncrementalInlineVirtual, true, DIAGNOSTIC,                 \\\n+          \"do post parse inlining of virtual calls\")                        \\\n+                                                                            \\\n","filename":"src\/hotspot\/share\/opto\/c2_globals.hpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -124,0 +124,3 @@\n+protected:\n+  void set_call_node(CallStaticJavaNode* call) { _call_node = call; }\n+\n@@ -132,1 +135,6 @@\n-  CallStaticJavaNode* call_node() const { return _call_node; }\n+  virtual CallNode* call_node() const { return _call_node; }\n+  virtual CallGenerator* with_call_node(CallNode* call) {\n+    DirectCallGenerator* dcg = new DirectCallGenerator(method(), _separate_io_proj);\n+    dcg->set_call_node(call->as_CallStaticJava());\n+    return dcg;\n+  }\n@@ -182,0 +190,6 @@\n+  bool _separate_io_proj;\n+  CallDynamicJavaNode* _call_node;\n+\n+protected:\n+  void set_call_node(CallDynamicJavaNode* call) { _call_node = call; }\n+\n@@ -183,2 +197,2 @@\n-  VirtualCallGenerator(ciMethod* method, int vtable_index)\n-    : CallGenerator(method), _vtable_index(vtable_index)\n+  VirtualCallGenerator(ciMethod* method, int vtable_index, bool separate_io_proj)\n+    : CallGenerator(method), _vtable_index(vtable_index), _separate_io_proj(separate_io_proj), _call_node(NULL)\n@@ -191,0 +205,9 @@\n+\n+  virtual CallNode* call_node() const { return _call_node; }\n+  int vtable_index() const { return _vtable_index; }\n+\n+  virtual CallGenerator* with_call_node(CallNode* call) {\n+    VirtualCallGenerator* cg = new VirtualCallGenerator(method(), _vtable_index, _separate_io_proj);\n+    cg->set_call_node(call->as_CallDynamicJava());\n+    return cg;\n+  }\n@@ -253,0 +276,2 @@\n+  _call_node = call;  \/\/ Save the call node in case we need it later\n+\n@@ -254,2 +279,2 @@\n-  kit.set_edges_for_java_call(call);\n-  Node* ret = kit.set_results_for_java_call(call);\n+  kit.set_edges_for_java_call(call, false \/*must_throw*\/, _separate_io_proj);\n+  Node* ret = kit.set_results_for_java_call(call, _separate_io_proj);\n@@ -288,1 +313,1 @@\n-  return new VirtualCallGenerator(m, vtable_index);\n+  return new VirtualCallGenerator(m, vtable_index, false \/*separate_io_projs*\/);\n@@ -299,1 +324,3 @@\n-  virtual bool do_late_inline_check(JVMState* jvms) { return true; }\n+  virtual bool do_late_inline_check(Compile* C, JVMState* jvms) { return true; }\n+  virtual CallGenerator* inline_cg() const { return _inline_cg; }\n+  virtual bool is_pure_call() const { return _is_pure_call; }\n@@ -344,0 +371,138 @@\n+\n+  virtual CallGenerator* with_call_node(CallNode* call) {\n+    LateInlineCallGenerator* cg = new LateInlineCallGenerator(method(), _inline_cg, _is_pure_call);\n+    cg->set_call_node(call->as_CallStaticJava());\n+    return cg;\n+  }\n+};\n+\n+CallGenerator* CallGenerator::for_late_inline(ciMethod* method, CallGenerator* inline_cg) {\n+  return new LateInlineCallGenerator(method, inline_cg);\n+}\n+\n+class LateInlineMHCallGenerator : public LateInlineCallGenerator {\n+  ciMethod* _caller;\n+  bool _input_not_const;\n+\n+  virtual bool do_late_inline_check(Compile* C, JVMState* jvms);\n+\n+ public:\n+  LateInlineMHCallGenerator(ciMethod* caller, ciMethod* callee, bool input_not_const) :\n+    LateInlineCallGenerator(callee, NULL), _caller(caller), _input_not_const(input_not_const) {}\n+\n+  virtual bool is_mh_late_inline() const { return true; }\n+\n+    \/\/ Convert the CallStaticJava into an inline\n+  virtual void do_late_inline();\n+\n+  virtual JVMState* generate(JVMState* jvms) {\n+    JVMState* new_jvms = LateInlineCallGenerator::generate(jvms);\n+\n+    Compile* C = Compile::current();\n+    if (_input_not_const) {\n+      \/\/ inlining won't be possible so no need to enqueue right now.\n+      call_node()->set_generator(this);\n+    } else {\n+      C->add_late_inline(this);\n+    }\n+    return new_jvms;\n+  }\n+\n+  virtual CallGenerator* with_call_node(CallNode* call) {\n+    LateInlineMHCallGenerator* cg = new LateInlineMHCallGenerator(_caller, method(), _input_not_const);\n+    cg->set_call_node(call->as_CallStaticJava());\n+    return cg;\n+  }\n+};\n+\n+bool LateInlineMHCallGenerator::do_late_inline_check(Compile* C, JVMState* jvms) {\n+  \/\/ Even if inlining is not allowed, a virtual call can be strength-reduced to a direct call.\n+  bool allow_inline = C->inlining_incrementally();\n+  CallGenerator* cg = for_method_handle_inline(jvms, _caller, method(), allow_inline, _input_not_const);\n+  assert(!_input_not_const, \"sanity\"); \/\/ shouldn't have been scheduled for inlining in the first place\n+\n+  if (cg != NULL) {\n+    assert(!cg->is_late_inline() || cg->is_mh_late_inline() || AlwaysIncrementalInline, \"we're doing late inlining\");\n+    _inline_cg = cg;\n+    C->dec_number_of_mh_late_inlines();\n+    return true;\n+  } else {\n+    \/\/ Method handle call which has a constant appendix argument should be either inlined or replaced with a direct call\n+    \/\/ unless there's a signature mismatch between caller and callee. If the failure occurs, there's not much to be improved later,\n+    \/\/ so don't reinstall the generator to avoid pushing the generator between IGVN and incremental inlining indefinitely.\n+    return false;\n+  }\n+}\n+\n+CallGenerator* CallGenerator::for_mh_late_inline(ciMethod* caller, ciMethod* callee, bool input_not_const) {\n+  assert(IncrementalInlineMH, \"required\");\n+  Compile::current()->inc_number_of_mh_late_inlines();\n+  CallGenerator* cg = new LateInlineMHCallGenerator(caller, callee, input_not_const);\n+  return cg;\n+}\n+\n+\/\/ Allow inlining decisions to be delayed\n+class LateInlineVirtualCallGenerator : public VirtualCallGenerator {\n+ private:\n+  jlong          _unique_id;   \/\/ unique id for log compilation\n+  CallGenerator* _inline_cg;\n+  ciMethod*      _callee;\n+  bool           _is_pure_call;\n+  float          _prof_factor;\n+\n+ protected:\n+  virtual bool do_late_inline_check(Compile* C, JVMState* jvms);\n+  virtual CallGenerator* inline_cg() const { return _inline_cg; }\n+  virtual bool is_pure_call() const { return _is_pure_call; }\n+\n+ public:\n+  LateInlineVirtualCallGenerator(ciMethod* method, int vtable_index, float prof_factor)\n+  : VirtualCallGenerator(method, vtable_index, true \/*separate_io_projs*\/),\n+    _unique_id(0), _inline_cg(NULL), _callee(NULL), _is_pure_call(false), _prof_factor(prof_factor) {}\n+\n+  virtual bool is_late_inline() const { return true; }\n+\n+  virtual bool is_virtual_late_inline() const { return true; }\n+\n+  \/\/ Convert the CallDynamicJava into an inline\n+  virtual void do_late_inline();\n+\n+  virtual void set_callee_method(ciMethod* m) {\n+    assert(_callee == NULL, \"repeated inlining attempt\");\n+    _callee = m;\n+  }\n+\n+  virtual JVMState* generate(JVMState* jvms) {\n+    \/\/ Emit the CallDynamicJava and request separate projections so\n+    \/\/ that the late inlining logic can distinguish between fall\n+    \/\/ through and exceptional uses of the memory and io projections\n+    \/\/ as is done for allocations and macro expansion.\n+    JVMState* new_jvms = VirtualCallGenerator::generate(jvms);\n+    if (call_node() != NULL) {\n+      call_node()->set_generator(this);\n+    }\n+    return new_jvms;\n+  }\n+\n+  virtual void print_inlining_late(const char* msg) {\n+    CallNode* call = call_node();\n+    Compile* C = Compile::current();\n+    C->print_inlining_assert_ready();\n+    C->print_inlining(method(), call->jvms()->depth()-1, call->jvms()->bci(), msg);\n+    C->print_inlining_move_to(this);\n+    C->print_inlining_update_delayed(this);\n+  }\n+\n+  virtual void set_unique_id(jlong id) {\n+    _unique_id = id;\n+  }\n+\n+  virtual jlong unique_id() const {\n+    return _unique_id;\n+  }\n+\n+  virtual CallGenerator* with_call_node(CallNode* call) {\n+    LateInlineVirtualCallGenerator* cg = new LateInlineVirtualCallGenerator(method(), vtable_index(), _prof_factor);\n+    cg->set_call_node(call->as_CallDynamicJava());\n+    return cg;\n+  }\n@@ -346,0 +511,33 @@\n+bool LateInlineVirtualCallGenerator::do_late_inline_check(Compile* C, JVMState* jvms) {\n+  \/\/ Method handle linker case is handled in CallDynamicJavaNode::Ideal().\n+  \/\/ Unless inlining is performed, _override_symbolic_info bit will be set in DirectCallGenerator::generate().\n+\n+  \/\/ Even if inlining is not allowed, a virtual call can be strength-reduced to a direct call.\n+  bool allow_inline = C->inlining_incrementally();\n+  CallGenerator* cg = C->call_generator(_callee,\n+                                        vtable_index(),\n+                                        false \/*call_does_dispatch*\/,\n+                                        jvms,\n+                                        allow_inline,\n+                                        _prof_factor,\n+                                        NULL \/*speculative_receiver_type*\/,\n+                                        true \/*allow_intrinsics*\/);\n+\n+  if (cg != NULL) {\n+    assert(!cg->is_late_inline() || cg->is_mh_late_inline() || AlwaysIncrementalInline, \"we're doing late inlining\");\n+    _inline_cg = cg;\n+    return true;\n+  } else {\n+    \/\/ Virtual call which provably doesn't dispatch should be either inlined or replaced with a direct call.\n+    assert(false, \"no progress\");\n+    return false;\n+  }\n+}\n+\n+CallGenerator* CallGenerator::for_late_inline_virtual(ciMethod* m, int vtable_index, float prof_factor) {\n+  assert(IncrementalInlineVirtual, \"required\");\n+  assert(!m->is_static(), \"for_virtual_call mismatch\");\n+  assert(!m->is_method_handle_intrinsic(), \"should be a direct call\");\n+  return new LateInlineVirtualCallGenerator(m, vtable_index, prof_factor);\n+}\n+\n@@ -347,0 +545,15 @@\n+  CallGenerator::do_late_inline_helper();\n+}\n+\n+void LateInlineMHCallGenerator::do_late_inline() {\n+  CallGenerator::do_late_inline_helper();\n+}\n+\n+void LateInlineVirtualCallGenerator::do_late_inline() {\n+  assert(_callee != NULL, \"required\"); \/\/ set up in CallDynamicJavaNode::Ideal\n+  CallGenerator::do_late_inline_helper();\n+}\n+\n+void CallGenerator::do_late_inline_helper() {\n+  assert(is_late_inline(), \"only late inline allowed\");\n+\n@@ -348,1 +561,1 @@\n-  CallStaticJavaNode* call = call_node();\n+  CallNode* call = call_node();\n@@ -376,6 +589,6 @@\n-  if (callprojs.fallthrough_catchproj == call->in(0) ||\n-      callprojs.catchall_catchproj == call->in(0) ||\n-      callprojs.fallthrough_memproj == call->in(TypeFunc::Memory) ||\n-      callprojs.catchall_memproj == call->in(TypeFunc::Memory) ||\n-      callprojs.fallthrough_ioproj == call->in(TypeFunc::I_O) ||\n-      callprojs.catchall_ioproj == call->in(TypeFunc::I_O) ||\n+  if ((callprojs.fallthrough_catchproj == call->in(0)) ||\n+      (callprojs.catchall_catchproj    == call->in(0)) ||\n+      (callprojs.fallthrough_memproj   == call->in(TypeFunc::Memory)) ||\n+      (callprojs.catchall_memproj      == call->in(TypeFunc::Memory)) ||\n+      (callprojs.fallthrough_ioproj    == call->in(TypeFunc::I_O)) ||\n+      (callprojs.catchall_ioproj       == call->in(TypeFunc::I_O)) ||\n@@ -383,1 +596,1 @@\n-      (callprojs.exobj != NULL && call->find_edge(callprojs.exobj) != -1)) {\n+      (callprojs.exobj   != NULL && call->find_edge(callprojs.exobj) != -1)) {\n@@ -394,1 +607,1 @@\n-  if (_is_pure_call && result_not_used) {\n+  if (is_pure_call() && result_not_used) {\n@@ -437,3 +650,2 @@\n-    \/\/ This check is done here because for_method_handle_inline() method\n-    \/\/ needs jvms for inlined state.\n-    if (!do_late_inline_check(jvms)) {\n+    \/\/ JVMState is ready, so time to perform some checks and prepare for inlining attempt.\n+    if (!do_late_inline_check(C, jvms)) {\n@@ -441,0 +653,1 @@\n+      C->print_inlining_update_delayed(this);\n@@ -452,0 +665,7 @@\n+    \/\/ Virtual call involves a receiver null check which can be made implicit.\n+    if (is_virtual_late_inline()) {\n+      GraphKit kit(jvms);\n+      kit.null_check_receiver();\n+      jvms = kit.transfer_exceptions_into_jvms();\n+    }\n+\n@@ -453,1 +673,1 @@\n-    JVMState* new_jvms = _inline_cg->generate(jvms);\n+    JVMState* new_jvms = inline_cg()->generate(jvms);\n@@ -467,1 +687,4 @@\n-    C->env()->notice_inlined_method(_inline_cg->method());\n+    if (inline_cg()->is_inline()) {\n+      C->set_has_loops(C->has_loops() || inline_cg()->method()->has_loops());\n+      C->env()->notice_inlined_method(inline_cg()->method());\n+    }\n@@ -474,60 +697,0 @@\n-\n-CallGenerator* CallGenerator::for_late_inline(ciMethod* method, CallGenerator* inline_cg) {\n-  return new LateInlineCallGenerator(method, inline_cg);\n-}\n-\n-class LateInlineMHCallGenerator : public LateInlineCallGenerator {\n-  ciMethod* _caller;\n-  int _attempt;\n-  bool _input_not_const;\n-\n-  virtual bool do_late_inline_check(JVMState* jvms);\n-  virtual bool already_attempted() const { return _attempt > 0; }\n-\n- public:\n-  LateInlineMHCallGenerator(ciMethod* caller, ciMethod* callee, bool input_not_const) :\n-    LateInlineCallGenerator(callee, NULL), _caller(caller), _attempt(0), _input_not_const(input_not_const) {}\n-\n-  virtual bool is_mh_late_inline() const { return true; }\n-\n-  virtual JVMState* generate(JVMState* jvms) {\n-    JVMState* new_jvms = LateInlineCallGenerator::generate(jvms);\n-\n-    Compile* C = Compile::current();\n-    if (_input_not_const) {\n-      \/\/ inlining won't be possible so no need to enqueue right now.\n-      call_node()->set_generator(this);\n-    } else {\n-      C->add_late_inline(this);\n-    }\n-    return new_jvms;\n-  }\n-};\n-\n-bool LateInlineMHCallGenerator::do_late_inline_check(JVMState* jvms) {\n-\n-  CallGenerator* cg = for_method_handle_inline(jvms, _caller, method(), _input_not_const);\n-\n-  Compile::current()->print_inlining_update_delayed(this);\n-\n-  if (!_input_not_const) {\n-    _attempt++;\n-  }\n-\n-  if (cg != NULL && cg->is_inline()) {\n-    assert(!cg->is_late_inline(), \"we're doing late inlining\");\n-    _inline_cg = cg;\n-    Compile::current()->dec_number_of_mh_late_inlines();\n-    return true;\n-  }\n-\n-  call_node()->set_generator(this);\n-  return false;\n-}\n-\n-CallGenerator* CallGenerator::for_mh_late_inline(ciMethod* caller, ciMethod* callee, bool input_not_const) {\n-  Compile::current()->inc_number_of_mh_late_inlines();\n-  CallGenerator* cg = new LateInlineMHCallGenerator(caller, callee, input_not_const);\n-  return cg;\n-}\n-\n@@ -552,0 +715,6 @@\n+\n+  virtual CallGenerator* with_call_node(CallNode* call) {\n+    LateInlineStringCallGenerator* cg = new LateInlineStringCallGenerator(method(), _inline_cg);\n+    cg->set_call_node(call->as_CallStaticJava());\n+    return cg;\n+  }\n@@ -574,0 +743,6 @@\n+\n+  virtual CallGenerator* with_call_node(CallNode* call) {\n+    LateInlineBoxingCallGenerator* cg = new LateInlineBoxingCallGenerator(method(), _inline_cg);\n+    cg->set_call_node(call->as_CallStaticJava());\n+    return cg;\n+  }\n@@ -596,0 +771,6 @@\n+\n+  virtual CallGenerator* with_call_node(CallNode* call) {\n+    LateInlineVectorReboxingCallGenerator* cg = new LateInlineVectorReboxingCallGenerator(method(), _inline_cg);\n+    cg->set_call_node(call->as_CallStaticJava());\n+    return cg;\n+  }\n@@ -853,1 +1034,1 @@\n-CallGenerator* CallGenerator::for_method_handle_call(JVMState* jvms, ciMethod* caller, ciMethod* callee) {\n+CallGenerator* CallGenerator::for_method_handle_call(JVMState* jvms, ciMethod* caller, ciMethod* callee, bool allow_inline) {\n@@ -856,1 +1037,1 @@\n-  CallGenerator* cg = CallGenerator::for_method_handle_inline(jvms, caller, callee, input_not_const);\n+  CallGenerator* cg = CallGenerator::for_method_handle_inline(jvms, caller, callee, allow_inline, input_not_const);\n@@ -869,1 +1050,1 @@\n-  if (IncrementalInline && call_site_count > 0 &&\n+  if (IncrementalInlineMH && call_site_count > 0 &&\n@@ -903,1 +1084,1 @@\n-CallGenerator* CallGenerator::for_method_handle_inline(JVMState* jvms, ciMethod* caller, ciMethod* callee, bool& input_not_const) {\n+CallGenerator* CallGenerator::for_method_handle_inline(JVMState* jvms, ciMethod* caller, ciMethod* callee, bool allow_inline, bool& input_not_const) {\n@@ -909,0 +1090,3 @@\n+  if (StressMethodHandleLinkerInlining) {\n+    allow_inline = false;\n+  }\n@@ -929,1 +1113,1 @@\n-                                              true \/* allow_inline *\/,\n+                                              allow_inline,\n@@ -1005,1 +1189,1 @@\n-          target = C->optimize_virtual_call(caller, jvms->bci(), klass, klass,\n+          target = C->optimize_virtual_call(caller, klass, klass,\n@@ -1014,1 +1198,1 @@\n-                                              !StressMethodHandleLinkerInlining \/* allow_inline *\/,\n+                                              allow_inline,\n@@ -1029,0 +1213,1 @@\n+        input_not_const = false;\n@@ -1063,1 +1248,1 @@\n-  virtual bool      is_inlined()   const    { return true; }\n+  virtual bool      is_inline()    const    { return true; }\n","filename":"src\/hotspot\/share\/opto\/callGenerator.cpp","additions":274,"deletions":89,"binary":false,"changes":363,"status":"modified"},{"patch":"@@ -45,0 +45,6 @@\n+  void do_late_inline_helper();\n+\n+  virtual bool           do_late_inline_check(Compile* C, JVMState* jvms) { ShouldNotReachHere(); return false; }\n+  virtual CallGenerator* inline_cg()    const                             { ShouldNotReachHere(); return NULL;  }\n+  virtual bool           is_pure_call() const                             { ShouldNotReachHere(); return false; }\n+\n@@ -68,1 +74,1 @@\n-  virtual bool      is_late_inline() const      { return false; }\n+  virtual bool      is_late_inline() const         { return false; }\n@@ -70,5 +76,3 @@\n-  virtual bool      is_mh_late_inline() const   { return false; }\n-  virtual bool      is_string_late_inline() const{ return false; }\n-\n-  \/\/ for method handle calls: have we tried inlinining the call already?\n-  virtual bool      already_attempted() const   { ShouldNotReachHere(); return false; }\n+  virtual bool      is_mh_late_inline() const      { return false; }\n+  virtual bool      is_string_late_inline() const  { return false; }\n+  virtual bool      is_virtual_late_inline() const { return false; }\n@@ -79,1 +83,2 @@\n-  virtual CallStaticJavaNode* call_node() const { ShouldNotReachHere(); return NULL; }\n+  virtual CallNode* call_node() const { return NULL; }\n+  virtual CallGenerator* with_call_node(CallNode* call)  { return this; }\n@@ -84,0 +89,2 @@\n+  virtual void set_callee_method(ciMethod* callee) { ShouldNotReachHere(); }\n+\n@@ -87,0 +94,6 @@\n+  \/\/ Allocate CallGenerators only in Compile arena since some of them are referenced from CallNodes.\n+  void* operator new(size_t size) throw() {\n+    Compile* C = Compile::current();\n+    return ResourceObj::operator new(size, C->comp_arena());\n+  }\n+\n@@ -122,2 +135,2 @@\n-  static CallGenerator* for_method_handle_call(  JVMState* jvms, ciMethod* caller, ciMethod* callee);\n-  static CallGenerator* for_method_handle_inline(JVMState* jvms, ciMethod* caller, ciMethod* callee, bool& input_not_const);\n+  static CallGenerator* for_method_handle_call(  JVMState* jvms, ciMethod* caller, ciMethod* callee, bool allow_inline);\n+  static CallGenerator* for_method_handle_inline(JVMState* jvms, ciMethod* caller, ciMethod* callee, bool allow_inline, bool& input_not_const);\n@@ -137,0 +150,2 @@\n+  static CallGenerator* for_late_inline_virtual(ciMethod* m, int vtable_index, float expected_uses);\n+\n","filename":"src\/hotspot\/share\/opto\/callGenerator.hpp","additions":24,"deletions":9,"binary":false,"changes":33,"status":"modified"},{"patch":"@@ -630,1 +630,1 @@\n-  for (JVMState* p = this; p->_caller != NULL; p = p->_caller) {\n+  for (JVMState* p = this; p != NULL; p = p->_caller) {\n@@ -945,1 +945,3 @@\n-Node *CallNode::Ideal(PhaseGVN *phase, bool can_reshape) {\n+Node* CallNode::Ideal(PhaseGVN* phase, bool can_reshape) {\n+#ifdef ASSERT\n+  \/\/ Validate attached generator\n@@ -947,16 +949,3 @@\n-  if (can_reshape && cg != NULL && cg->is_mh_late_inline() && !cg->already_attempted()) {\n-    \/\/ Check whether this MH handle call becomes a candidate for inlining\n-    ciMethod* callee = cg->method();\n-    vmIntrinsics::ID iid = callee->intrinsic_id();\n-    if (iid == vmIntrinsics::_invokeBasic) {\n-      if (in(TypeFunc::Parms)->Opcode() == Op_ConP) {\n-        phase->C->prepend_late_inline(cg);\n-        set_generator(NULL);\n-      }\n-    } else {\n-      assert(callee->has_member_arg(), \"wrong type of call?\");\n-      if (in(TypeFunc::Parms + callee->arg_size() - 1)->Opcode() == Op_ConP) {\n-        phase->C->prepend_late_inline(cg);\n-        set_generator(NULL);\n-      }\n-    }\n+  if (cg != NULL) {\n+    assert(is_CallStaticJava()  && cg->is_mh_late_inline() ||\n+           is_CallDynamicJava() && cg->is_virtual_late_inline(), \"mismatch\");\n@@ -964,0 +953,1 @@\n+#endif \/\/ ASSERT\n@@ -982,1 +972,1 @@\n-void CallJavaNode::copy_call_debug_info(PhaseIterGVN* phase, SafePointNode *sfpt) {\n+void CallJavaNode::copy_call_debug_info(PhaseIterGVN* phase, SafePointNode* sfpt) {\n@@ -1037,1 +1027,1 @@\n-void CallJavaNode::dump_spec(outputStream *st) const {\n+void CallJavaNode::dump_spec(outputStream* st) const {\n@@ -1058,0 +1048,26 @@\n+Node* CallStaticJavaNode::Ideal(PhaseGVN* phase, bool can_reshape) {\n+  CallGenerator* cg = generator();\n+  if (can_reshape && cg != NULL) {\n+    assert(IncrementalInlineMH, \"required\");\n+    assert(cg->call_node() == this, \"mismatch\");\n+    assert(cg->is_mh_late_inline(), \"not virtual\");\n+\n+    \/\/ Check whether this MH handle call becomes a candidate for inlining.\n+    ciMethod* callee = cg->method();\n+    vmIntrinsics::ID iid = callee->intrinsic_id();\n+    if (iid == vmIntrinsics::_invokeBasic) {\n+      if (in(TypeFunc::Parms)->Opcode() == Op_ConP) {\n+        phase->C->prepend_late_inline(cg);\n+        set_generator(NULL);\n+      }\n+    } else {\n+      assert(callee->has_member_arg(), \"wrong type of call?\");\n+      if (in(TypeFunc::Parms + callee->arg_size() - 1)->Opcode() == Op_ConP) {\n+        phase->C->prepend_late_inline(cg);\n+        set_generator(NULL);\n+      }\n+    }\n+  }\n+  return CallNode::Ideal(phase, can_reshape);\n+}\n+\n@@ -1114,0 +1130,42 @@\n+\n+Node* CallDynamicJavaNode::Ideal(PhaseGVN* phase, bool can_reshape) {\n+  CallGenerator* cg = generator();\n+  if (can_reshape && cg != NULL) {\n+    assert(IncrementalInlineVirtual, \"required\");\n+    assert(cg->call_node() == this, \"mismatch\");\n+    assert(cg->is_virtual_late_inline(), \"not virtual\");\n+\n+    \/\/ Recover symbolic info for method resolution.\n+    ciMethod* caller = jvms()->method();\n+    ciBytecodeStream iter(caller);\n+    iter.force_bci(jvms()->bci());\n+\n+    bool             not_used1;\n+    ciSignature*     not_used2;\n+    ciMethod*        orig_callee  = iter.get_method(not_used1, &not_used2);  \/\/ callee in the bytecode\n+    ciKlass*         holder       = iter.get_declared_method_holder();\n+    if (orig_callee->is_method_handle_intrinsic()) {\n+      assert(_override_symbolic_info, \"required\");\n+      orig_callee = method();\n+      holder = method()->holder();\n+    }\n+\n+    ciInstanceKlass* klass = ciEnv::get_instance_klass_for_declared_method_holder(holder);\n+\n+    Node* receiver_node = in(TypeFunc::Parms);\n+    const TypeOopPtr* receiver_type = phase->type(receiver_node)->isa_oopptr();\n+\n+    int  not_used3;\n+    bool call_does_dispatch;\n+    ciMethod* callee = phase->C->optimize_virtual_call(caller, klass, holder, orig_callee, receiver_type, true \/*is_virtual*\/,\n+                                                       call_does_dispatch, not_used3);  \/\/ out-parameters\n+    if (!call_does_dispatch) {\n+      \/\/ Register for late inlining.\n+      cg->set_callee_method(callee);\n+      phase->C->prepend_late_inline(cg); \/\/ MH late inlining prepends to the list, so do the same\n+      set_generator(NULL);\n+    }\n+  }\n+  return CallNode::Ideal(phase, can_reshape);\n+}\n+\n","filename":"src\/hotspot\/share\/opto\/callnode.cpp","additions":78,"deletions":20,"binary":false,"changes":98,"status":"modified"},{"patch":"@@ -573,1 +573,1 @@\n-  bool may_modify_arraycopy_helper(const TypeOopPtr* dest_t, const TypeOopPtr *t_oop, PhaseTransform *phase);\n+  bool may_modify_arraycopy_helper(const TypeOopPtr* dest_t, const TypeOopPtr* t_oop, PhaseTransform* phase);\n@@ -576,5 +576,5 @@\n-  const TypeFunc *_tf;        \/\/ Function type\n-  address      _entry_point;  \/\/ Address of method being called\n-  float        _cnt;          \/\/ Estimate of number of times called\n-  CallGenerator* _generator;  \/\/ corresponding CallGenerator for some late inline calls\n-  const char *_name;           \/\/ Printable name, if _method is NULL\n+  const TypeFunc* _tf;          \/\/ Function type\n+  address         _entry_point; \/\/ Address of method being called\n+  float           _cnt;         \/\/ Estimate of number of times called\n+  CallGenerator*  _generator;   \/\/ corresponding CallGenerator for some late inline calls\n+  const char*     _name;        \/\/ Printable name, if _method is NULL\n@@ -603,1 +603,1 @@\n-  virtual const Type *bottom_type() const;\n+  virtual const Type* bottom_type() const;\n@@ -605,1 +605,1 @@\n-  virtual Node *Ideal(PhaseGVN *phase, bool can_reshape);\n+  virtual Node* Ideal(PhaseGVN* phase, bool can_reshape);\n@@ -607,1 +607,1 @@\n-  virtual bool        cmp( const Node &n ) const;\n+  virtual bool        cmp(const Node &n) const;\n@@ -609,2 +609,2 @@\n-  virtual void        calling_convention( BasicType* sig_bt, VMRegPair *parm_regs, uint argcnt ) const;\n-  virtual Node       *match( const ProjNode *proj, const Matcher *m );\n+  virtual void        calling_convention(BasicType* sig_bt, VMRegPair* parm_regs, uint argcnt) const;\n+  virtual Node*       match(const ProjNode* proj, const Matcher* m);\n@@ -626,1 +626,1 @@\n-  virtual bool        may_modify(const TypeOopPtr *t_oop, PhaseTransform *phase);\n+  virtual bool        may_modify(const TypeOopPtr* t_oop, PhaseTransform* phase);\n@@ -628,1 +628,1 @@\n-  bool                has_non_debug_use(Node *n);\n+  bool                has_non_debug_use(Node* n);\n@@ -632,1 +632,1 @@\n-  Node *result_cast();\n+  Node* result_cast();\n@@ -635,1 +635,1 @@\n-    const TypeTuple *r = tf()->range();\n+    const TypeTuple* r = tf()->range();\n@@ -649,1 +649,1 @@\n-  virtual void copy_call_debug_info(PhaseIterGVN* phase, SafePointNode *sfpt) {}\n+  virtual void copy_call_debug_info(PhaseIterGVN* phase, SafePointNode* sfpt) {}\n@@ -652,2 +652,2 @@\n-  virtual void        dump_req(outputStream *st = tty) const;\n-  virtual void        dump_spec(outputStream *st) const;\n+  virtual void        dump_req(outputStream* st = tty) const;\n+  virtual void        dump_spec(outputStream* st) const;\n@@ -739,1 +739,1 @@\n-  \/\/ Later inlining modifies the JVMState, so we need to clone it\n+  \/\/ Late inlining modifies the JVMState, so we need to clone it\n@@ -749,0 +749,2 @@\n+  virtual Node* Ideal(PhaseGVN* phase, bool can_reshape);\n+\n@@ -765,0 +767,9 @@\n+  \/\/ Late inlining modifies the JVMState, so we need to clone it\n+  \/\/ when the call node is cloned.\n+  virtual void clone_jvms(Compile* C) {\n+    if ((jvms() != NULL) && IncrementalInlineVirtual) {\n+      set_jvms(jvms()->clone_deep(C));\n+      jvms()->set_map_deep(this);\n+    }\n+  }\n+\n@@ -767,0 +778,1 @@\n+  virtual Node* Ideal(PhaseGVN* phase, bool can_reshape);\n","filename":"src\/hotspot\/share\/opto\/callnode.hpp","additions":31,"deletions":19,"binary":false,"changes":50,"status":"modified"},{"patch":"@@ -342,3 +342,6 @@\n-    CallNode* call = cg->call_node();\n-    if (shift > 0) {\n-      inlines->at_put(i-shift, cg);\n+    if (useful.member(cg->call_node())) {\n+      if (shift > 0) {\n+        inlines->at_put(i - shift, cg);\n+      }\n+    } else {\n+      shift++; \/\/ skip over the dead element\n@@ -346,2 +349,14 @@\n-    if (!useful.member(call)) {\n-      shift++;\n+  }\n+  if (shift > 0) {\n+    inlines->trunc_to(inlines->length() - shift); \/\/ remove last elements from compacted array\n+  }\n+}\n+\n+void Compile::remove_useless_late_inlines(GrowableArray<CallGenerator*>* inlines, Node* dead) {\n+  assert(dead != NULL && dead->is_Call(), \"sanity\");\n+  int found = 0;\n+  for (int i = 0; i < inlines->length(); i++) {\n+    if (inlines->at(i)->call_node() == dead) {\n+      inlines->remove_at(i);\n+      found++;\n+      NOT_DEBUG( break; ) \/\/ elements are unique, so exit early\n@@ -350,1 +365,1 @@\n-  inlines->trunc_to(inlines->length()-shift);\n+  assert(found <= 1, \"not unique\");\n@@ -357,1 +372,1 @@\n-      node_list.remove_if_existing(n);\n+      node_list.delete_at(i); \/\/ replaces i-th with last element which is known to be useful (already processed)\n@@ -362,0 +377,30 @@\n+void Compile::remove_useless_node(Node* dead) {\n+  remove_modified_node(dead);\n+\n+  \/\/ Constant node that has no out-edges and has only one in-edge from\n+  \/\/ root is usually dead. However, sometimes reshaping walk makes\n+  \/\/ it reachable by adding use edges. So, we will NOT count Con nodes\n+  \/\/ as dead to be conservative about the dead node count at any\n+  \/\/ given time.\n+  if (!dead->is_Con()) {\n+    record_dead_node(dead->_idx);\n+  }\n+  if (dead->is_macro()) {\n+    remove_macro_node(dead);\n+  }\n+  if (dead->is_expensive()) {\n+    remove_expensive_node(dead);\n+  }\n+  if (dead->for_post_loop_opts_igvn()) {\n+    remove_from_post_loop_opts_igvn(dead);\n+  }\n+  if (dead->is_Call()) {\n+    remove_useless_late_inlines(                &_late_inlines, dead);\n+    remove_useless_late_inlines(         &_string_late_inlines, dead);\n+    remove_useless_late_inlines(         &_boxing_late_inlines, dead);\n+    remove_useless_late_inlines(&_vector_reboxing_late_inlines, dead);\n+  }\n+  BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n+  bs->unregister_potential_barrier_node(dead);\n+}\n+\n@@ -397,3 +442,3 @@\n-  remove_useless_late_inlines(&_string_late_inlines, useful);\n-  remove_useless_late_inlines(&_boxing_late_inlines, useful);\n-  remove_useless_late_inlines(&_late_inlines, useful);\n+  remove_useless_late_inlines(                &_late_inlines, useful);\n+  remove_useless_late_inlines(         &_string_late_inlines, useful);\n+  remove_useless_late_inlines(         &_boxing_late_inlines, useful);\n@@ -1863,0 +1908,1 @@\n+\n@@ -1865,3 +1911,2 @@\n-  int i = 0;\n-  for (; i <_late_inlines.length() && !inlining_progress(); i++) {\n-    CallGenerator* cg = _late_inlines.at(i);\n+\n+  for (int i = 0; i < _late_inlines.length(); i++) {\n@@ -1869,6 +1914,16 @@\n-    cg->do_late_inline();\n-    if (failing())  return false;\n-  }\n-  int j = 0;\n-  for (; i < _late_inlines.length(); i++, j++) {\n-    _late_inlines.at_put(j, _late_inlines.at(i));\n+    CallGenerator* cg = _late_inlines.at(i);\n+    bool does_dispatch = cg->is_virtual_late_inline() || cg->is_mh_late_inline();\n+    if (inlining_incrementally() || does_dispatch) { \/\/ a call can be either inlined or strength-reduced to a direct call\n+      cg->do_late_inline();\n+      assert(_late_inlines.at(i) == cg, \"no insertions before current position allowed\");\n+      if (failing()) {\n+        return false;\n+      } else if (inlining_progress()) {\n+        _late_inlines_pos = i+1; \/\/ restore the position in case new elements were inserted\n+        print_method(PHASE_INCREMENTAL_INLINE_STEP, cg->call_node(), 3);\n+        break; \/\/ process one call site at a time\n+      }\n+    } else {\n+      \/\/ Ignore late inline direct calls when inlining is not allowed.\n+      \/\/ They are left in the late inline list when node budget is exhausted until the list is fully drained.\n+    }\n@@ -1876,2 +1931,5 @@\n-  _late_inlines.trunc_to(j);\n-  assert(inlining_progress() || _late_inlines.length() == 0, \"\");\n+  \/\/ Remove processed elements.\n+  _late_inlines.remove_till(_late_inlines_pos);\n+  _late_inlines_pos = 0;\n+\n+  assert(inlining_progress() || _late_inlines.length() == 0, \"no progress\");\n@@ -1899,0 +1957,1 @@\n+  print_method(PHASE_INCREMENTAL_INLINE_CLEANUP, 3);\n@@ -1922,0 +1981,12 @@\n+        bool do_print_inlining = print_inlining() || print_intrinsics();\n+        if (do_print_inlining || log() != NULL) {\n+          \/\/ Print inlining message for candidates that we couldn't inline for lack of space.\n+          for (int i = 0; i < _late_inlines.length(); i++) {\n+            CallGenerator* cg = _late_inlines.at(i);\n+            const char* msg = \"live nodes > LiveNodeCountInliningCutoff\";\n+            if (do_print_inlining) {\n+              cg->print_inlining_late(msg);\n+            }\n+            log_late_inline_failure(cg, msg);\n+          }\n+        }\n@@ -1932,1 +2003,0 @@\n-\n@@ -1940,0 +2010,4 @@\n+\n+    if (_late_inlines.length() == 0) {\n+      break; \/\/ no more progress\n+    }\n@@ -1958,0 +2032,21 @@\n+void Compile::process_late_inline_calls_no_inline(PhaseIterGVN& igvn) {\n+  \/\/ \"inlining_incrementally() == false\" is used to signal that no inlining is allowed\n+  \/\/ (see LateInlineVirtualCallGenerator::do_late_inline_check() for details).\n+  \/\/ Tracking and verification of modified nodes is disabled by setting \"_modified_nodes == NULL\"\n+  \/\/ as if \"inlining_incrementally() == true\" were set.\n+  assert(inlining_incrementally() == false, \"not allowed\");\n+  assert(_modified_nodes == NULL, \"not allowed\");\n+  assert(_late_inlines.length() > 0, \"sanity\");\n+\n+  while (_late_inlines.length() > 0) {\n+    for_igvn()->clear();\n+    initial_gvn()->replace_with(&igvn);\n+\n+    while (inline_incrementally_one()) {\n+      assert(!failing(), \"inconsistent\");\n+    }\n+    if (failing())  return;\n+\n+    inline_incrementally_cleanup(igvn);\n+  }\n+}\n@@ -2238,0 +2333,1 @@\n+\n@@ -2239,0 +2335,8 @@\n+\n+  assert(_late_inlines.length() == 0 || IncrementalInlineMH || IncrementalInlineVirtual, \"not empty\");\n+\n+  if (_late_inlines.length() > 0) {\n+    \/\/ More opportunities to optimize virtual and MH calls.\n+    \/\/ Though it's maybe too late to perform inlining, strength-reducing them to direct calls is still an option.\n+    process_late_inline_calls_no_inline(igvn);\n+  }\n@@ -2242,0 +2346,1 @@\n+\n@@ -2257,2 +2362,0 @@\n-    PhaseGVN* gvn = C->initial_gvn();\n-\n@@ -3264,3 +3367,4 @@\n-    if (OptimizeStringConcat) {\n-      ProjNode* p = n->as_Proj();\n-      if (p->_is_io_use) {\n+    if (OptimizeStringConcat || IncrementalInline) {\n+      ProjNode* proj = n->as_Proj();\n+      if (proj->_is_io_use) {\n+        assert(proj->_con == TypeFunc::I_O || proj->_con == TypeFunc::Memory, \"\");\n@@ -3270,13 +3374,4 @@\n-        \/\/ the original one.\n-        Node* proj = NULL;\n-        \/\/ Replace with just one\n-        for (SimpleDUIterator i(p->in(0)); i.has_next(); i.next()) {\n-          Node *use = i.get();\n-          if (use->is_Proj() && p != use && use->as_Proj()->_con == p->_con) {\n-            proj = use;\n-            break;\n-          }\n-        }\n-        assert(proj != NULL || p->_con == TypeFunc::I_O, \"io may be dropped at an infinite loop\");\n-        if (proj != NULL) {\n-          p->subsume_by(proj, this);\n+        \/\/ the original one. Merge them.\n+        Node* non_io_proj = proj->in(0)->as_Multi()->proj_out_or_null(proj->_con, false \/*is_io_use*\/);\n+        if (non_io_proj  != NULL) {\n+          proj->subsume_by(non_io_proj , this);\n@@ -4142,6 +4237,1 @@\n-    if (!cg->is_late_inline()) {\n-      if (print_inlining_current().cg() != NULL) {\n-        print_inlining_push();\n-      }\n-      print_inlining_commit();\n-    } else {\n+    if (cg->is_late_inline()) {\n@@ -4155,0 +4245,5 @@\n+    } else {\n+      if (print_inlining_current().cg() != NULL) {\n+        print_inlining_push();\n+      }\n+      print_inlining_commit();\n@@ -4162,1 +4257,1 @@\n-  if (print_inlining()) {\n+  if (print_inlining() || print_intrinsics()) {\n@@ -4174,1 +4269,1 @@\n-  if (print_inlining()) {\n+  if (print_inlining() || print_intrinsics()) {\n@@ -4189,16 +4284,2 @@\n-  bool do_print_inlining = print_inlining() || print_intrinsics();\n-  if (do_print_inlining || log() != NULL) {\n-    \/\/ Print inlining message for candidates that we couldn't inline\n-    \/\/ for lack of space\n-    for (int i = 0; i < _late_inlines.length(); i++) {\n-      CallGenerator* cg = _late_inlines.at(i);\n-      if (!cg->is_mh_late_inline()) {\n-        const char* msg = \"live nodes > LiveNodeCountInliningCutoff\";\n-        if (do_print_inlining) {\n-          cg->print_inlining_late(msg);\n-        }\n-        log_late_inline_failure(cg, msg);\n-      }\n-    }\n-  }\n-  if (do_print_inlining) {\n+  assert(_late_inlines.length() == 0, \"not drained yet\");\n+  if (print_inlining() || print_intrinsics()) {\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":145,"deletions":64,"binary":false,"changes":209,"status":"modified"},{"patch":"@@ -867,1 +867,1 @@\n-  ciMethod* optimize_virtual_call(ciMethod* caller, int bci, ciInstanceKlass* klass,\n+  ciMethod* optimize_virtual_call(ciMethod* caller, ciInstanceKlass* klass,\n@@ -872,1 +872,1 @@\n-  ciMethod* optimize_inlining(ciMethod* caller, int bci, ciInstanceKlass* klass,\n+  ciMethod* optimize_inlining(ciMethod* caller, ciInstanceKlass* klass,\n@@ -914,0 +914,2 @@\n+  void              remove_useless_node(Node* dead);\n+\n@@ -944,1 +946,0 @@\n-  void remove_useless_late_inlines(GrowableArray<CallGenerator*>* inlines, Unique_Node_List &useful);\n@@ -947,0 +948,3 @@\n+  void remove_useless_late_inlines(GrowableArray<CallGenerator*>* inlines, Unique_Node_List &useful);\n+  void remove_useless_late_inlines(GrowableArray<CallGenerator*>* inlines, Node* dead);\n+\n@@ -977,0 +981,2 @@\n+  void process_late_inline_calls_no_inline(PhaseIterGVN& igvn);\n+\n","filename":"src\/hotspot\/share\/opto\/compile.hpp","additions":9,"deletions":3,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -151,1 +151,1 @@\n-    CallGenerator* cg = CallGenerator::for_method_handle_call(jvms, caller, callee);\n+    CallGenerator* cg = CallGenerator::for_method_handle_call(jvms, caller, callee, allow_inline);\n@@ -277,1 +277,2 @@\n-            miss_cg = CallGenerator::for_virtual_call(callee, vtable_index);\n+            miss_cg = (IncrementalInlineVirtual ? CallGenerator::for_late_inline_virtual(callee, vtable_index, prof_factor)\n+                                                : CallGenerator::for_virtual_call(callee, vtable_index));\n@@ -344,2 +345,1 @@\n-    }\n-  }\n+    } \/\/ call_does_dispatch && bytecode == Bytecodes::_invokeinterface\n@@ -347,6 +347,7 @@\n-  \/\/ Nothing claimed the intrinsic, we go with straight-forward inlining\n-  \/\/ for already discovered intrinsic.\n-  if (allow_inline && allow_intrinsics && cg_intrinsic != NULL) {\n-    assert(cg_intrinsic->does_virtual_dispatch(), \"sanity\");\n-    return cg_intrinsic;\n-  }\n+    \/\/ Nothing claimed the intrinsic, we go with straight-forward inlining\n+    \/\/ for already discovered intrinsic.\n+    if (allow_intrinsics && cg_intrinsic != NULL) {\n+      assert(cg_intrinsic->does_virtual_dispatch(), \"sanity\");\n+      return cg_intrinsic;\n+    }\n+  } \/\/ allow_inline\n@@ -362,1 +363,5 @@\n-    return CallGenerator::for_virtual_call(callee, vtable_index);\n+    if (IncrementalInlineVirtual && allow_inline) {\n+      return CallGenerator::for_late_inline_virtual(callee, vtable_index, prof_factor); \/\/ attempt to inline through virtual call later\n+    } else {\n+      return CallGenerator::for_virtual_call(callee, vtable_index);\n+    }\n@@ -563,1 +568,1 @@\n-    callee = C->optimize_virtual_call(method(), bci(), klass, holder, orig_callee,\n+    callee = C->optimize_virtual_call(method(), klass, holder, orig_callee,\n@@ -1072,1 +1077,1 @@\n-ciMethod* Compile::optimize_virtual_call(ciMethod* caller, int bci, ciInstanceKlass* klass,\n+ciMethod* Compile::optimize_virtual_call(ciMethod* caller, ciInstanceKlass* klass,\n@@ -1082,1 +1087,1 @@\n-  ciMethod* optimized_virtual_method = optimize_inlining(caller, bci, klass, callee,\n+  ciMethod* optimized_virtual_method = optimize_inlining(caller, klass, callee,\n@@ -1097,1 +1102,1 @@\n-ciMethod* Compile::optimize_inlining(ciMethod* caller, int bci, ciInstanceKlass* klass,\n+ciMethod* Compile::optimize_inlining(ciMethod* caller, ciInstanceKlass* klass,\n@@ -1188,5 +1193,0 @@\n-      if (PrintOpto) {\n-        tty->print(\"  Calling method via exact type @%d --- \", bci);\n-        exact_method->print_name();\n-        tty->cr();\n-      }\n","filename":"src\/hotspot\/share\/opto\/doCall.cpp","additions":20,"deletions":20,"binary":false,"changes":40,"status":"modified"},{"patch":"@@ -66,0 +66,10 @@\n+ProjNode* MultiNode::proj_out_or_null(uint which_proj, bool is_io_use) const {\n+  for (DUIterator_Fast imax, i = fast_outs(imax); i < imax; i++) {\n+    ProjNode* proj = fast_out(i)->isa_Proj();\n+    if (proj != NULL && (proj->_con == which_proj) && (proj->_is_io_use == is_io_use)) {\n+      return proj;\n+    }\n+  }\n+  return NULL;\n+}\n+\n","filename":"src\/hotspot\/share\/opto\/multnode.cpp","additions":10,"deletions":0,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -51,1 +51,1 @@\n-\n+  ProjNode* proj_out_or_null(uint which_proj, bool is_io_use) const;\n","filename":"src\/hotspot\/share\/opto\/multnode.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"opto\/callGenerator.hpp\"\n@@ -557,1 +558,0 @@\n-  \/\/ cloning CallNode may need to clone JVMState\n@@ -559,0 +559,1 @@\n+    \/\/ cloning CallNode may need to clone JVMState\n@@ -560,0 +561,6 @@\n+    \/\/ CallGenerator is linked to the original node.\n+    CallGenerator* cg = n->as_Call()->generator();\n+    if (cg != NULL) {\n+      CallGenerator* cloned_cg = cg->with_call_node(n->as_Call());\n+      n->as_Call()->set_generator(cloned_cg);\n+    }\n@@ -1406,12 +1413,0 @@\n-      if (dead->is_macro()) {\n-        igvn->C->remove_macro_node(dead);\n-      }\n-      if (dead->is_expensive()) {\n-        igvn->C->remove_expensive_node(dead);\n-      }\n-      if (dead->for_post_loop_opts_igvn()) {\n-        igvn->C->remove_from_post_loop_opts_igvn(dead);\n-      }\n-      BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n-      bs->unregister_potential_barrier_node(dead);\n-      igvn->C->record_dead_node(dead->_idx);\n@@ -1440,1 +1435,1 @@\n-      igvn->C->remove_modified_node(dead);\n+      igvn->C->remove_useless_node(dead);\n","filename":"src\/hotspot\/share\/opto\/node.cpp","additions":9,"deletions":14,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -1120,4 +1120,6 @@\n-  while (modified_list->size()) {\n-    Node* n = modified_list->pop();\n-    n->dump();\n-    assert(false, \"VerifyIterativeGVN: new modified node was added\");\n+  if (modified_list != NULL) {\n+    while (modified_list->size() > 0) {\n+      Node* n = modified_list->pop();\n+      n->dump();\n+      assert(false, \"VerifyIterativeGVN: new modified node was added\");\n+    }\n@@ -1412,20 +1414,1 @@\n-      C->remove_modified_node(dead);\n-      \/\/ Constant node that has no out-edges and has only one in-edge from\n-      \/\/ root is usually dead. However, sometimes reshaping walk makes\n-      \/\/ it reachable by adding use edges. So, we will NOT count Con nodes\n-      \/\/ as dead to be conservative about the dead node count at any\n-      \/\/ given time.\n-      if (!dead->is_Con()) {\n-        C->record_dead_node(dead->_idx);\n-      }\n-      if (dead->is_macro()) {\n-        C->remove_macro_node(dead);\n-      }\n-      if (dead->is_expensive()) {\n-        C->remove_expensive_node(dead);\n-      }\n-      if (dead->for_post_loop_opts_igvn()) {\n-        C->remove_from_post_loop_opts_igvn(dead);\n-      }\n-      BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n-      bs->unregister_potential_barrier_node(dead);\n+      C->remove_useless_node(dead);\n","filename":"src\/hotspot\/share\/opto\/phaseX.cpp","additions":7,"deletions":24,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -63,0 +63,1 @@\n+  PHASE_INCREMENTAL_INLINE_CLEANUP,\n@@ -114,0 +115,1 @@\n+      case PHASE_INCREMENTAL_INLINE_CLEANUP: return \"Incremental Inline Cleanup\";\n","filename":"src\/hotspot\/share\/opto\/phasetype.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -255,0 +255,8 @@\n+  \/\/ Remove all elements up to the index (exclusive). The order is preserved.\n+  void remove_till(int idx) {\n+    for (int i = 0, j = idx; j < length(); i++, j++) {\n+      at_put(i, at(j));\n+    }\n+    trunc_to(length() - idx);\n+  }\n+\n","filename":"src\/hotspot\/share\/utilities\/growableArray.hpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"}]}