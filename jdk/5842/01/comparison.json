{"files":[{"patch":"@@ -360,0 +360,7 @@\n+    elif test \"x$OPENJDK_TARGET_CPU\" = \"xppc64le\"; then\n+      if test \"x$OPENJDK_TARGET_OS\" = \"xlinux\"; then\n+        AC_MSG_RESULT([yes])\n+      else\n+        AC_MSG_RESULT([no, $OPENJDK_TARGET_OS-$OPENJDK_TARGET_CPU])\n+        AVAILABLE=false\n+      fi\n","filename":"make\/autoconf\/jvm-features.m4","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -158,0 +158,1 @@\n+        $d\/cpu\/$(HOTSPOT_TARGET_CPU_ARCH)\/gc\/z\/z_$(HOTSPOT_TARGET_CPU_ARCH).ad \\\n","filename":"make\/hotspot\/gensrc\/GensrcAdlc.gmk","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -0,0 +1,567 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2021 SAP SE. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"asm\/register.hpp\"\n+#include \"precompiled.hpp\"\n+#include \"asm\/macroAssembler.inline.hpp\"\n+#include \"code\/codeBlob.hpp\"\n+#include \"code\/vmreg.inline.hpp\"\n+#include \"gc\/z\/zBarrier.inline.hpp\"\n+#include \"gc\/z\/zBarrierSet.hpp\"\n+#include \"gc\/z\/zBarrierSetAssembler.hpp\"\n+#include \"gc\/z\/zBarrierSetRuntime.hpp\"\n+#include \"gc\/z\/zThreadLocalData.hpp\"\n+#include \"memory\/resourceArea.hpp\"\n+#include \"register_ppc.hpp\"\n+#include \"runtime\/sharedRuntime.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include \"utilities\/macros.hpp\"\n+#ifdef COMPILER1\n+#include \"c1\/c1_LIRAssembler.hpp\"\n+#include \"c1\/c1_MacroAssembler.hpp\"\n+#include \"gc\/z\/c1\/zBarrierSetC1.hpp\"\n+#endif \/\/ COMPILER1\n+#ifdef COMPILER2\n+#include \"gc\/z\/c2\/zBarrierSetC2.hpp\"\n+#endif \/\/ COMPILER2\n+\n+#undef __\n+#define __ masm->\n+\n+void ZBarrierSetAssembler::load_at(MacroAssembler* masm, DecoratorSet decorators, BasicType type,\n+                                   Register base, RegisterOrConstant ind_or_offs, Register dst,\n+                                   Register tmp1, Register tmp2,\n+                                   MacroAssembler::PreservationLevel preservation_level, Label *L_handle_null) {\n+  __ block_comment(\"load_at (zgc) {\");\n+\n+  \/\/ Check whether a special gc barrier is required for this particular load\n+  \/\/ (e.g. whether it's a reference load or not)\n+  if (!ZBarrierSet::barrier_needed(decorators, type)) {\n+    BarrierSetAssembler::load_at(masm, decorators, type, base, ind_or_offs, dst,\n+                                 tmp1, tmp2, preservation_level, L_handle_null);\n+    return;\n+  }\n+\n+  if (ind_or_offs.is_register()) {\n+    assert_different_registers(base, ind_or_offs.as_register(), tmp1, tmp2, R0, noreg);\n+    assert_different_registers(dst, ind_or_offs.as_register(), tmp1, tmp2, R0, noreg);\n+  } else {\n+    assert_different_registers(base, tmp1, tmp2, R0, noreg);\n+    assert_different_registers(dst, tmp1, tmp2, R0, noreg);\n+  }\n+\n+  \/* ==== Load the pointer using the standard implementation for the actual heap access\n+          and the decompression of compressed pointers ==== *\/\n+  \/\/ Result of 'load_at' (standard implementation) will be written back to 'dst'.\n+  \/\/ As 'base' is required for the C-call, it must be reserved in case of a register clash.\n+  Register saved_base = base;\n+  if (base == dst) {\n+    __ mr(tmp2, base);\n+    saved_base = tmp2;\n+  }\n+\n+  BarrierSetAssembler::load_at(masm, decorators, type, base, ind_or_offs, dst,\n+                               tmp1, noreg, preservation_level, L_handle_null);\n+\n+  \/* ==== Check whether pointer is dirty ==== *\/\n+  Label skip_barrier;\n+\n+  \/\/ Load bad mask into scratch register.\n+  __ ld(tmp1, (intptr_t) ZThreadLocalData::address_bad_mask_offset(), R16_thread);\n+\n+  \/\/ The color bits of the to-be-tested pointer do not have to be equivalent to the 'bad_mask' testing bits.\n+  \/\/ A pointer is classified as dirty if any of the color bits that also match the bad mask is set.\n+  \/\/ Conversely, it follows that the logical AND of the bad mask and the pointer must be zero\n+  \/\/ if the pointer is not dirty.\n+  \/\/ Only dirty pointers must be processed by this barrier, so we can skip it in case the latter condition holds true.\n+  __ and_(tmp1, tmp1, dst);\n+  __ beq(CCR0, skip_barrier);\n+\n+  \/* ==== Invoke barrier ==== *\/\n+  int nbytes_save = 0;\n+\n+  const bool needs_frame = preservation_level >= MacroAssembler::PRESERVATION_FRAME_LR;\n+  const bool preserve_gp_registers = preservation_level >= MacroAssembler::PRESERVATION_FRAME_LR_GP_REGS;\n+  const bool preserve_fp_registers = preservation_level >= MacroAssembler::PRESERVATION_FRAME_LR_GP_FP_REGS;\n+\n+  const bool preserve_R3 = dst != R3_ARG1;\n+\n+  if (needs_frame) {\n+    if (preserve_gp_registers) {\n+      nbytes_save = (preserve_fp_registers\n+                     ? MacroAssembler::num_volatile_gp_regs + MacroAssembler::num_volatile_fp_regs\n+                     : MacroAssembler::num_volatile_gp_regs) * BytesPerWord;\n+      nbytes_save -= preserve_R3 ? 0 : BytesPerWord;\n+      __ save_volatile_gprs(R1_SP, -nbytes_save, preserve_fp_registers, preserve_R3);\n+    }\n+\n+    __ save_LR_CR(tmp1);\n+    __ push_frame_reg_args(nbytes_save, tmp1);\n+  }\n+\n+  \/\/ Setup arguments\n+  if (saved_base != R3_ARG1) {\n+    __ mr_if_needed(R3_ARG1, dst);\n+    __ add(R4_ARG2, ind_or_offs, saved_base);\n+  } else if (dst != R4_ARG2) {\n+    __ add(R4_ARG2, ind_or_offs, saved_base);\n+    __ mr(R3_ARG1, dst);\n+  } else {\n+    __ add(R0, ind_or_offs, saved_base);\n+    __ mr(R3_ARG1, dst);\n+    __ mr(R4_ARG2, R0);\n+  }\n+\n+  __ call_VM_leaf(ZBarrierSetRuntime::load_barrier_on_oop_field_preloaded_addr(decorators));\n+\n+  Register result = R3_RET;\n+  if (needs_frame) {\n+    __ pop_frame();\n+    __ restore_LR_CR(tmp1);\n+\n+    if (preserve_R3) {\n+      __ mr(R0, R3_RET);\n+      result = R0;\n+    }\n+\n+    if (preserve_gp_registers) {\n+      __ restore_volatile_gprs(R1_SP, -nbytes_save, preserve_fp_registers, preserve_R3);\n+    }\n+  }\n+  __ mr_if_needed(dst, result);\n+\n+  __ bind(skip_barrier);\n+  __ block_comment(\"} load_at (zgc)\");\n+}\n+\n+#ifdef ASSERT\n+\/\/ The Z store barrier only verifies the pointers it is operating on and is thus a sole debugging measure.\n+void ZBarrierSetAssembler::store_at(MacroAssembler* masm, DecoratorSet decorators, BasicType type,\n+                                    Register base, RegisterOrConstant ind_or_offs, Register val,\n+                                    Register tmp1, Register tmp2, Register tmp3,\n+                                    MacroAssembler::PreservationLevel preservation_level) {\n+  __ block_comment(\"store_at (zgc) {\");\n+\n+  \/\/ If the 'val' register is 'noreg', the to-be-stored value is a null pointer.\n+  if (is_reference_type(type) && val != noreg) {\n+    __ ld(tmp1, in_bytes(ZThreadLocalData::address_bad_mask_offset()), R16_thread);\n+    __ and_(tmp1, tmp1, val);\n+    __ asm_assert_eq(\"Detected dirty pointer on the heap in Z store barrier\");\n+  }\n+\n+  \/\/ Store value\n+  BarrierSetAssembler::store_at(masm, decorators, type, base, ind_or_offs, val, tmp1, tmp2, tmp3, preservation_level);\n+\n+  __ block_comment(\"} store_at (zgc)\");\n+}\n+#endif \/\/ ASSERT\n+\n+void ZBarrierSetAssembler::arraycopy_prologue(MacroAssembler *masm, DecoratorSet decorators, BasicType component_type,\n+                                              Register src, Register dst, Register count,\n+                                              Register preserve1, Register preserve2) {\n+  __ block_comment(\"arraycopy_prologue (zgc) {\");\n+\n+  \/* ==== Check whether a special gc barrier is required for this particular load ==== *\/\n+  if (!is_reference_type(component_type)) {\n+    return;\n+  }\n+\n+  Label skip_barrier;\n+\n+  \/\/ Fast path: Array is of length zero\n+  __ cmpdi(CCR0, count, 0);\n+  __ beq(CCR0, skip_barrier);\n+\n+  \/* ==== Ensure register sanity ==== *\/\n+  Register tmp_R11 = R11_scratch1;\n+\n+  assert_different_registers(src, dst, count, tmp_R11, noreg);\n+  if (preserve1 != noreg) {\n+    \/\/ Not technically required, but unlikely being intended.\n+    assert_different_registers(preserve1, preserve2);\n+  }\n+\n+  \/* ==== Invoke barrier (slowpath) ==== *\/\n+  int nbytes_save = 0;\n+\n+  {\n+    assert(!noreg->is_volatile(), \"sanity\");\n+\n+    if (preserve1->is_volatile()) {\n+      __ std(preserve1, -BytesPerWord * ++nbytes_save, R1_SP);\n+    }\n+\n+    if (preserve2->is_volatile() && preserve1 != preserve2) {\n+      __ std(preserve2, -BytesPerWord * ++nbytes_save, R1_SP);\n+    }\n+\n+    __ std(src, -BytesPerWord * ++nbytes_save, R1_SP);\n+    __ std(dst, -BytesPerWord * ++nbytes_save, R1_SP);\n+    __ std(count, -BytesPerWord * ++nbytes_save, R1_SP);\n+\n+    __ save_LR_CR(tmp_R11);\n+    __ push_frame_reg_args(nbytes_save, tmp_R11);\n+  }\n+\n+  \/\/ ZBarrierSetRuntime::load_barrier_on_oop_array_addr(src, count)\n+  if (count == R3_ARG1) {\n+    if (src == R4_ARG2) {\n+      \/\/ Arguments are provided in reverse order\n+      __ mr(tmp_R11, count);\n+      __ mr(R3_ARG1, src);\n+      __ mr(R4_ARG2, tmp_R11);\n+    } else {\n+      __ mr(R4_ARG2, count);\n+      __ mr(R3_ARG1, src);\n+    }\n+  } else {\n+    __ mr_if_needed(R3_ARG1, src);\n+    __ mr_if_needed(R4_ARG2, count);\n+  }\n+\n+  __ call_VM_leaf(ZBarrierSetRuntime::load_barrier_on_oop_array_addr());\n+\n+  __ pop_frame();\n+  __ restore_LR_CR(tmp_R11);\n+\n+  {\n+    __ ld(count, -BytesPerWord * nbytes_save--, R1_SP);\n+    __ ld(dst, -BytesPerWord * nbytes_save--, R1_SP);\n+    __ ld(src, -BytesPerWord * nbytes_save--, R1_SP);\n+\n+    if (preserve2->is_volatile() && preserve1 != preserve2) {\n+      __ ld(preserve2, -BytesPerWord * nbytes_save--, R1_SP);\n+    }\n+\n+    if (preserve1->is_volatile()) {\n+      __ ld(preserve1, -BytesPerWord * nbytes_save--, R1_SP);\n+    }\n+  }\n+\n+  __ bind(skip_barrier);\n+\n+  __ block_comment(\"} arraycopy_prologue (zgc)\");\n+}\n+\n+void ZBarrierSetAssembler::try_resolve_jobject_in_native(MacroAssembler* masm, Register dst, Register jni_env,\n+                                                         Register obj, Register tmp, Label& slowpath) {\n+  __ block_comment(\"try_resolve_jobject_in_native (zgc) {\");\n+\n+  assert_different_registers(jni_env, obj, tmp);\n+\n+  \/\/ Resolve the pointer using the standard implementation for weak tag handling and pointer verfication.\n+  BarrierSetAssembler::try_resolve_jobject_in_native(masm, dst, jni_env, obj, tmp, slowpath);\n+\n+  \/\/ Check whether pointer is dirty.\n+  __ ld(tmp,\n+        in_bytes(ZThreadLocalData::address_bad_mask_offset() - JavaThread::jni_environment_offset()),\n+        jni_env);\n+\n+  __ and_(tmp, obj, tmp);\n+  __ bne(CCR0, slowpath);\n+\n+  __ block_comment(\"} try_resolve_jobject_in_native (zgc)\");\n+}\n+\n+#undef __\n+\n+#ifdef COMPILER1\n+#define __ ce->masm()->\n+\n+\/\/ Code emitted by LIR node \"LIR_OpZLoadBarrierTest\" which in turn is emitted by ZBarrierSetC1::load_barrier.\n+\/\/ The actual compare and branch instructions are represented as stand-alone LIR nodes.\n+void ZBarrierSetAssembler::generate_c1_load_barrier_test(LIR_Assembler* ce,\n+                                                         LIR_Opr ref) const {\n+  __ block_comment(\"load_barrier_test (zgc) {\");\n+\n+  __ ld(R0, in_bytes(ZThreadLocalData::address_bad_mask_offset()), R16_thread);\n+  __ andr(R0, R0, ref->as_pointer_register());\n+  __ cmpdi(CCR5 \/* as mandated by LIR node *\/, R0, 0);\n+\n+  __ block_comment(\"} load_barrier_test (zgc)\");\n+}\n+\n+\/\/ Code emitted by code stub \"ZLoadBarrierStubC1\" which in turn is emitted by ZBarrierSetC1::load_barrier.\n+\/\/ Invokes the runtime stub which is defined just below.\n+void ZBarrierSetAssembler::generate_c1_load_barrier_stub(LIR_Assembler* ce,\n+                                                         ZLoadBarrierStubC1* stub) const {\n+  __ block_comment(\"c1_load_barrier_stub (zgc) {\");\n+\n+  __ bind(*stub->entry());\n+\n+  \/* ==== Determine relevant data registers and ensure register sanity ==== *\/\n+  Register ref = stub->ref()->as_register();\n+  Register ref_addr = noreg;\n+\n+  \/\/ Determine reference address\n+  if (stub->tmp()->is_valid()) {\n+    \/\/ 'tmp' register is given, so address might have an index or a displacement.\n+    ce->leal(stub->ref_addr(), stub->tmp());\n+    ref_addr = stub->tmp()->as_pointer_register();\n+  } else {\n+    \/\/ 'tmp' register is not given, so address must have neither an index nor a displacement.\n+    \/\/ The address' base register is thus usable as-is.\n+    assert(stub->ref_addr()->as_address_ptr()->disp() == 0, \"illegal displacement\");\n+    assert(!stub->ref_addr()->as_address_ptr()->index()->is_valid(), \"illegal index\");\n+\n+    ref_addr = stub->ref_addr()->as_address_ptr()->base()->as_pointer_register();\n+  }\n+\n+  assert_different_registers(ref, ref_addr, R0, noreg);\n+\n+  \/* ==== Invoke stub ==== *\/\n+  \/\/ Pass arguments via stack. The stack pointer will be bumped by the stub.\n+  __ std(ref, (intptr_t) -1 * BytesPerWord, R1_SP);\n+  __ std(ref_addr, (intptr_t) -2 * BytesPerWord, R1_SP);\n+\n+  __ load_const_optimized(R0, stub->runtime_stub());\n+  __ call_stub(R0);\n+\n+  \/\/ The runtime stub passes the result via the R0 register, overriding the previously-loaded stub address.\n+  __ mr_if_needed(ref, R0);\n+  __ b(*stub->continuation());\n+\n+  __ block_comment(\"} c1_load_barrier_stub (zgc)\");\n+}\n+\n+#undef __\n+#define __ sasm->\n+\n+\/\/ Code emitted by runtime code stub which in turn is emitted by ZBarrierSetC1::generate_c1_runtime_stubs.\n+void ZBarrierSetAssembler::generate_c1_load_barrier_runtime_stub(StubAssembler* sasm,\n+                                                                 DecoratorSet decorators) const {\n+  __ block_comment(\"c1_load_barrier_runtime_stub (zgc) {\");\n+\n+  const int stack_parameters = 2;\n+  const int nbytes_save = (MacroAssembler::num_volatile_regs + stack_parameters) * BytesPerWord;\n+\n+  __ save_volatile_gprs(R1_SP, -nbytes_save);\n+  __ save_LR_CR(R0);\n+\n+  \/\/ Load arguments back again from the stack.\n+  __ ld(R3_ARG1, (intptr_t) -1 * BytesPerWord, R1_SP); \/\/ ref\n+  __ ld(R4_ARG2, (intptr_t) -2 * BytesPerWord, R1_SP); \/\/ ref_addr\n+\n+  __ push_frame_reg_args(nbytes_save, R0);\n+\n+  __ call_VM_leaf(ZBarrierSetRuntime::load_barrier_on_oop_field_preloaded_addr(decorators));\n+\n+  __ verify_oop(R3_RET, \"Bad pointer after barrier invocation\");\n+  __ mr(R0, R3_RET);\n+\n+  __ pop_frame();\n+  __ restore_LR_CR(R3_RET);\n+  __ restore_volatile_gprs(R1_SP, -nbytes_save);\n+\n+  __ blr();\n+\n+  __ block_comment(\"} c1_load_barrier_runtime_stub (zgc)\");\n+}\n+\n+#undef __\n+#endif \/\/ COMPILER1\n+\n+#ifdef COMPILER2\n+\n+OptoReg::Name ZBarrierSetAssembler::refine_register(const Node* node, OptoReg::Name opto_reg) const {\n+  if (!OptoReg::is_reg(opto_reg)) {\n+    return OptoReg::Bad;\n+  }\n+\n+  VMReg vm_reg = OptoReg::as_VMReg(opto_reg);\n+  if ((vm_reg->is_Register() || vm_reg ->is_FloatRegister()) && (opto_reg & 1) != 0) {\n+    return OptoReg::Bad;\n+  }\n+\n+  return opto_reg;\n+}\n+\n+#define __ _masm->\n+\n+class ZSaveLiveRegisters {\n+\n+ private:\n+  MacroAssembler* _masm;\n+  RegMask _reg_mask;\n+  Register _result_reg;\n+\n+ public:\n+  ZSaveLiveRegisters(MacroAssembler *masm, ZLoadBarrierStubC2 *stub)\n+      : _masm(masm), _reg_mask(stub->live()), _result_reg(stub->ref()) {\n+\n+    const int total_regs_amount = iterate_over_register_mask(ACTION_SAVE);\n+\n+    __ save_LR_CR(R0);\n+    __ push_frame_reg_args(total_regs_amount * BytesPerWord, R0);\n+  }\n+\n+  ~ZSaveLiveRegisters() {\n+    __ pop_frame();\n+    __ restore_LR_CR(R0);\n+\n+    iterate_over_register_mask(ACTION_RESTORE);\n+  }\n+\n+ private:\n+  enum IterationAction : int {\n+    ACTION_SAVE = 0,\n+    ACTION_RESTORE = 1\n+  };\n+\n+  int iterate_over_register_mask(IterationAction action) {\n+    int reg_save_index = 0;\n+    RegMaskIterator live_regs_iterator(_reg_mask);\n+\n+    while(live_regs_iterator.has_next()) {\n+      const OptoReg::Name opto_reg = live_regs_iterator.next();\n+\n+      \/\/ Filter out stack slots (spilled registers, i.e., stack-allocated registers).\n+      if (!OptoReg::is_reg(opto_reg)) {\n+        continue;\n+      }\n+\n+      const VMReg vm_reg = OptoReg::as_VMReg(opto_reg);\n+      if (vm_reg->is_Register()) {\n+        Register std_reg = vm_reg->as_Register();\n+\n+        \/\/ '_result_reg' will hold the end result of the operation. Its content must thus not be preserved.\n+        if (std_reg == _result_reg) {\n+          continue;\n+        }\n+\n+        if (std_reg->encoding() >= R2->encoding() && std_reg->encoding() <= R12->encoding()) {\n+          reg_save_index++;\n+\n+          if (action == ACTION_SAVE) {\n+            _masm->std(std_reg, (intptr_t) -reg_save_index * BytesPerWord, R1_SP);\n+          } else if (action == ACTION_RESTORE) {\n+            _masm->ld(std_reg, (intptr_t) -reg_save_index * BytesPerWord, R1_SP);\n+          } else {\n+            fatal(\"Sanity\");\n+          }\n+        }\n+      } else if (vm_reg->is_FloatRegister()) {\n+        FloatRegister fp_reg = vm_reg->as_FloatRegister();\n+        if (fp_reg->encoding() >= F0->encoding() && fp_reg->encoding() <= F13->encoding()) {\n+          reg_save_index++;\n+\n+          if (action == ACTION_SAVE) {\n+            _masm->stfd(fp_reg, (intptr_t) -reg_save_index * BytesPerWord, R1_SP);\n+          } else if (action == ACTION_RESTORE) {\n+            _masm->lfd(fp_reg, (intptr_t) -reg_save_index * BytesPerWord, R1_SP);\n+          } else {\n+            fatal(\"Sanity\");\n+          }\n+        }\n+      } else if (vm_reg->is_ConditionRegister()) {\n+        \/\/ NOP. Conditions registers are covered by save_LR_CR\n+      } else {\n+        if (vm_reg->is_VectorRegister()) {\n+          fatal(\"Vector registers are unsupported. Found register %s\", vm_reg->name());\n+        } else if (vm_reg->is_SpecialRegister()) {\n+          fatal(\"Special registers are unsupported. Found register %s\", vm_reg->name());\n+        } else {\n+          fatal(\"Register type is not known\");\n+        }\n+      }\n+    }\n+\n+    return reg_save_index;\n+  }\n+};\n+\n+#undef __\n+#define __ _masm->\n+\n+class ZSetupArguments {\n+ private:\n+  MacroAssembler* const _masm;\n+  const Register        _ref;\n+  const Address         _ref_addr;\n+\n+ public:\n+  ZSetupArguments(MacroAssembler* masm, ZLoadBarrierStubC2* stub) :\n+      _masm(masm),\n+      _ref(stub->ref()),\n+      _ref_addr(stub->ref_addr()) {\n+\n+    \/\/ Desired register\/argument configuration:\n+    \/\/ _ref: R3_ARG1\n+    \/\/ _ref_addr: R4_ARG2\n+\n+    \/\/ '_ref_addr' can be unspecified. In that case, the barrier will not heal the reference.\n+    if (_ref_addr.base() == noreg) {\n+      assert_different_registers(_ref, R0, noreg);\n+\n+      __ mr_if_needed(R3_ARG1, _ref);\n+      __ li(R4_ARG2, 0);\n+    } else {\n+      assert_different_registers(_ref, _ref_addr.base(), R0, noreg);\n+      assert(!_ref_addr.index()->is_valid(), \"reference addresses must not contain an index component\");\n+\n+      if (_ref != R4_ARG2) {\n+        \/\/ Calculate address first as the address' base register might clash with R4_ARG2\n+        __ add(R4_ARG2, (intptr_t) _ref_addr.disp(), _ref_addr.base());\n+        __ mr_if_needed(R3_ARG1, _ref);\n+      } else if (_ref_addr.base() != R3_ARG1) {\n+        __ mr(R3_ARG1, _ref);\n+        __ add(R4_ARG2, (intptr_t) _ref_addr.disp(), _ref_addr.base()); \/\/ Cloberring _ref\n+      } else {\n+        \/\/ Arguments are provided in inverse order (i.e. _ref == R4_ARG2, _ref_addr == R3_ARG1)\n+        __ mr(R0, _ref);\n+        __ add(R4_ARG2, (intptr_t) _ref_addr.disp(), _ref_addr.base());\n+        __ mr(R3_ARG1, R0);\n+      }\n+    }\n+  }\n+};\n+\n+#undef __\n+#define __ masm->\n+\n+void ZBarrierSetAssembler::generate_c2_load_barrier_stub(MacroAssembler* masm, ZLoadBarrierStubC2* stub) const {\n+  __ block_comment(\"generate_c2_load_barrier_stub (zgc) {\");\n+\n+  __ bind(*stub->entry());\n+\n+  Register ref = stub->ref();\n+  Address ref_addr = stub->ref_addr();\n+\n+  assert_different_registers(ref, ref_addr.base());\n+\n+  {\n+    ZSaveLiveRegisters save_live_registers(masm, stub);\n+    ZSetupArguments setup_arguments(masm, stub);\n+\n+    __ call_VM_leaf(stub->slow_path());\n+    __ mr_if_needed(ref, R3_RET);\n+  }\n+\n+  __ b(*stub->continuation());\n+\n+  __ block_comment(\"} generate_c2_load_barrier_stub (zgc)\");\n+}\n+\n+#undef __\n+#endif \/\/ COMPILER2\n","filename":"src\/hotspot\/cpu\/ppc\/gc\/z\/zBarrierSetAssembler_ppc.cpp","additions":567,"deletions":0,"binary":false,"changes":567,"status":"added"},{"patch":"@@ -0,0 +1,86 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2021 SAP SE. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef CPU_PPC_GC_Z_ZBARRIERSETASSEMBLER_PPC_HPP\n+#define CPU_PPC_GC_Z_ZBARRIERSETASSEMBLER_PPC_HPP\n+\n+#include \"code\/vmreg.hpp\"\n+#include \"oops\/accessDecorators.hpp\"\n+#ifdef COMPILER2\n+#include \"opto\/optoreg.hpp\"\n+#endif \/\/ COMPILER2\n+\n+#ifdef COMPILER1\n+class LIR_Assembler;\n+class LIR_OprDesc;\n+typedef LIR_OprDesc* LIR_Opr;\n+class StubAssembler;\n+class ZLoadBarrierStubC1;\n+#endif \/\/ COMPILER1\n+\n+#ifdef COMPILER2\n+class Node;\n+class ZLoadBarrierStubC2;\n+#endif \/\/ COMPILER2\n+\n+class ZBarrierSetAssembler : public ZBarrierSetAssemblerBase {\n+public:\n+  virtual void load_at(MacroAssembler* masm, DecoratorSet decorators, BasicType type,\n+                       Register base, RegisterOrConstant ind_or_offs, Register dst,\n+                       Register tmp1, Register tmp2,\n+                       MacroAssembler::PreservationLevel preservation_level, Label *L_handle_null = NULL);\n+\n+#ifdef ASSERT\n+  virtual void store_at(MacroAssembler* masm, DecoratorSet decorators, BasicType type,\n+                        Register base, RegisterOrConstant ind_or_offs, Register val,\n+                        Register tmp1, Register tmp2, Register tmp3,\n+                        MacroAssembler::PreservationLevel preservation_level);\n+#endif \/\/ ASSERT\n+\n+  virtual void arraycopy_prologue(MacroAssembler* masm, DecoratorSet decorators, BasicType type,\n+                                  Register src, Register dst, Register count,\n+                                  Register preserve1, Register preserve2);\n+\n+  virtual void try_resolve_jobject_in_native(MacroAssembler* masm, Register dst, Register jni_env,\n+                                             Register obj, Register tmp, Label& slowpath);\n+\n+#ifdef COMPILER1\n+  void generate_c1_load_barrier_test(LIR_Assembler* ce,\n+                                     LIR_Opr ref) const;\n+\n+  void generate_c1_load_barrier_stub(LIR_Assembler* ce,\n+                                     ZLoadBarrierStubC1* stub) const;\n+\n+  void generate_c1_load_barrier_runtime_stub(StubAssembler* sasm,\n+                                             DecoratorSet decorators) const;\n+#endif \/\/ COMPILER1\n+\n+#ifdef COMPILER2\n+  OptoReg::Name refine_register(const Node* node, OptoReg::Name opto_reg) const;\n+\n+  void generate_c2_load_barrier_stub(MacroAssembler* masm, ZLoadBarrierStubC2* stub) const;\n+#endif \/\/ COMPILER2\n+};\n+\n+#endif \/\/ CPU_AARCH64_GC_Z_ZBARRIERSETASSEMBLER_AARCH64_HPP\n","filename":"src\/hotspot\/cpu\/ppc\/gc\/z\/zBarrierSetAssembler_ppc.hpp","additions":86,"deletions":0,"binary":false,"changes":86,"status":"added"},{"patch":"@@ -0,0 +1,203 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2021 SAP SE. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/shared\/gcLogPrecious.hpp\"\n+#include \"gc\/shared\/gc_globals.hpp\"\n+#include \"gc\/z\/zGlobals.hpp\"\n+#include \"runtime\/globals.hpp\"\n+#include \"runtime\/os.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include \"utilities\/powerOfTwo.hpp\"\n+#include <cstddef>\n+\n+#ifdef LINUX\n+#include <sys\/mman.h>\n+#endif \/\/ LINUX\n+\n+\/\/\n+\/\/ The overall memory layouts across different power platforms are similar and only differ with regards to\n+\/\/ the position of the highest addressable bit; the position of the metadata bits and the size of the actual\n+\/\/ addressable heap address space are adjusted accordingly.\n+\/\/\n+\/\/ The following memory schema shows an exemplary layout in which bit '45' is the highest addressable bit.\n+\/\/ It is assumed that this virtual memroy address space layout is predominant on the power platform.\n+\/\/\n+\/\/ Standard Address Space & Pointer Layout\n+\/\/ ---------------------------------------\n+\/\/\n+\/\/  +--------------------------------+ 0x00007FFFFFFFFFFF (127 TiB - 1)\n+\/\/  .                                .\n+\/\/  .                                .\n+\/\/  .                                .\n+\/\/  +--------------------------------+ 0x0000140000000000 (20 TiB)\n+\/\/  |         Remapped View          |\n+\/\/  +--------------------------------+ 0x0000100000000000 (16 TiB)\n+\/\/  .                                .\n+\/\/  +--------------------------------+ 0x00000c0000000000 (12 TiB)\n+\/\/  |         Marked1 View           |\n+\/\/  +--------------------------------+ 0x0000080000000000 (8  TiB)\n+\/\/  |         Marked0 View           |\n+\/\/  +--------------------------------+ 0x0000040000000000 (4  TiB)\n+\/\/  .                                .\n+\/\/  +--------------------------------+ 0x0000000000000000\n+\/\/\n+\/\/   6                  4 4  4 4\n+\/\/   3                  6 5  2 1                                             0\n+\/\/  +--------------------+----+-----------------------------------------------+\n+\/\/  |00000000 00000000 00|1111|11 11111111 11111111 11111111 11111111 11111111|\n+\/\/  +--------------------+----+-----------------------------------------------+\n+\/\/  |                    |    |\n+\/\/  |                    |    * 41-0 Object Offset (42-bits, 4TB address space)\n+\/\/  |                    |\n+\/\/  |                    * 45-42 Metadata Bits (4-bits)  0001 = Marked0      (Address view 4-8TB)\n+\/\/  |                                                    0010 = Marked1      (Address view 8-12TB)\n+\/\/  |                                                    0100 = Remapped     (Address view 16-20TB)\n+\/\/  |                                                    1000 = Finalizable  (Address view N\/A)\n+\/\/  |\n+\/\/  * 63-46 Fixed (18-bits, always zero)\n+\/\/\n+\n+\/\/ Maximum value as per spec (Power ISA v2.07): 2 ^ 60 bytes, i.e. 1 EiB (exbibyte)\n+static const unsigned int MAXIMUM_MAX_ADDRESS_BIT = 60;\n+\n+\/\/ Most modern power processors provide an address space with not more than 45 bit addressable bit,\n+\/\/ that is an address space of 32 TiB in size.\n+static const unsigned int DEFAULT_MAX_ADDRESS_BIT = 45;\n+\n+\/\/ Minimum value returned, if probing fails: 64 GiB\n+static const unsigned int MINIMUM_MAX_ADDRESS_BIT = 36;\n+\n+\/\/ Determines the highest addressable bit of the virtual address space (depends on platform)\n+\/\/ by trying to interact with memory in that address range,\n+\/\/ i.e. by syncing existing mappings (msync) or by temporarily mapping the memory area (mmap).\n+\/\/ If one of those operations succeeds, it is proven that the targeted memory area is within the virtual address space.\n+\/\/\n+\/\/ To reduce the number of required system calls to a bare minimum, the DEFAULT_MAX_ADDRESS_BIT is intentionally set\n+\/\/ lower than what the ABI would theoretically permit.\n+\/\/ Such an avoidance strategy, however, might impose unnecessary limits on processors that exceed this limit.\n+\/\/ If DEFAULT_MAX_ADDRESS_BIT is addressable, the next higher bit will be tested as well to ensure that\n+\/\/ the made assumption does not artificially restrict the memory availability.\n+static unsigned int probe_valid_max_address_bit(size_t init_bit, size_t min_bit) {\n+  assert(init_bit >= min_bit, \"Sanity\");\n+  assert(init_bit <= MAXIMUM_MAX_ADDRESS_BIT, \"Test bit is outside the assumed address space range\");\n+\n+#ifdef LINUX\n+  unsigned int max_valid_address_bit = 0;\n+  void* last_allocatable_address = nullptr;\n+\n+  const unsigned int page_size = os::vm_page_size();\n+\n+  for (size_t i = init_bit; i >= min_bit; --i) {\n+    void* base_addr = (void*) (((unsigned long) 1U) << i);\n+\n+    \/* ==== Try msync-ing already mapped memory page ==== *\/\n+    if (msync(base_addr, page_size, MS_ASYNC) == 0) {\n+      \/\/ The page of the given address was synced by the linux kernel and must thus be both, mapped and valid.\n+      max_valid_address_bit = i;\n+      break;\n+    }\n+    if (errno != ENOMEM) {\n+      \/\/ An unexpected error occurred, i.e. an error not indicating that the targeted memory page is unmapped,\n+      \/\/ but pointing out another type of issue.\n+      \/\/ Even though this should never happen, those issues may come up due to undefined behavior.\n+#ifdef ASSERT\n+      fatal(\"Received '%s' while probing the address space for the highest valid bit\", os::errno_name(errno));\n+#else \/\/ ASSERT\n+      log_warning_p(gc)(\"Received '%s' while probing the address space for the highest valid bit\", os::errno_name(errno));\n+#endif \/\/ ASSERT\n+      continue;\n+    }\n+\n+    \/* ==== Try mapping memory page on our own ==== *\/\n+    last_allocatable_address = mmap(base_addr, page_size, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS|MAP_NORESERVE, -1, 0);\n+    if (last_allocatable_address != MAP_FAILED) {\n+      munmap(last_allocatable_address, page_size);\n+    }\n+\n+    if (last_allocatable_address == base_addr) {\n+      \/\/ As the linux kernel mapped exactly the page we have requested, the address must be valid.\n+      max_valid_address_bit = i;\n+      break;\n+    }\n+\n+    log_info_p(gc, init)(\"Probe failed for bit '%zu'\", i);\n+  }\n+\n+  if (max_valid_address_bit == 0) {\n+    \/\/ Probing did not bring up any usable address bit.\n+    \/\/ As an alternative, the VM evaluates the address returned by mmap as it is expected that the reserved page\n+    \/\/ will be close to the probed address that was out-of-range.\n+    \/\/ As per mmap(2), \"the kernel [will take] [the address] as a hint about where to\n+    \/\/ place the mapping; on Linux, the mapping will be created at a nearby page boundary\".\n+    \/\/ It should thus be a \"close enough\" approximation to the real virtual memory address space limit.\n+    \/\/\n+    \/\/ This recovery strategy is only applied in production builds.\n+    \/\/ In debug builds, an assertion in 'ZPlatformAddressOffsetBits' will bail out the VM to indicate that\n+    \/\/ the assumed address space is no longer up-to-date.\n+    if (last_allocatable_address != MAP_FAILED) {\n+      const unsigned int bitpos = BitsPerSize_t - count_leading_zeros((size_t) last_allocatable_address) - 1;\n+      log_info_p(gc, init)(\"Did not find any valid addresses within the range, using address '%u' instead\", bitpos);\n+      return bitpos;\n+    }\n+\n+#ifdef ASSERT\n+    fatal(\"Available address space can not be determined\");\n+#else \/\/ ASSERT\n+    log_warning_p(gc)(\"Cannot determine available address space. Falling back to default value.\");\n+    return DEFAULT_MAX_ADDRESS_BIT;\n+#endif \/\/ ASSERT\n+  } else {\n+    if (max_valid_address_bit == init_bit) {\n+      \/\/ An usable address bit has been found immediately.\n+      \/\/ To ensure that the entire virtual address space is exploited, the next highest bit will be tested as well.\n+      log_info_p(gc, init)(\"Hit valid address '%u' on first try, retrying with next higher bit\", max_valid_address_bit);\n+      return MAX2(max_valid_address_bit, probe_valid_max_address_bit(init_bit + 1, init_bit + 1));\n+    }\n+  }\n+\n+  log_info_p(gc, init)(\"Found valid address '%u'\", max_valid_address_bit);\n+  return max_valid_address_bit;\n+#else \/\/ LINUX\n+  return DEFAULT_MAX_ADDRESS_BIT;\n+#endif \/\/ LINUX\n+}\n+\n+size_t ZPlatformAddressOffsetBits() {\n+  const static unsigned int valid_max_address_offset_bits =\n+      probe_valid_max_address_bit(DEFAULT_MAX_ADDRESS_BIT, MINIMUM_MAX_ADDRESS_BIT) + 1;\n+  assert(valid_max_address_offset_bits >= MINIMUM_MAX_ADDRESS_BIT,\n+         \"Highest addressable bit is outside the assumed address space range\");\n+\n+  const size_t max_address_offset_bits = valid_max_address_offset_bits - 3;\n+  const size_t min_address_offset_bits = max_address_offset_bits - 2;\n+  const size_t address_offset = round_up_power_of_2(MaxHeapSize * ZVirtualToPhysicalRatio);\n+  const size_t address_offset_bits = log2i_exact(address_offset);\n+\n+  return clamp(address_offset_bits, min_address_offset_bits, max_address_offset_bits);\n+}\n+\n+size_t ZPlatformAddressMetadataShift() {\n+  return ZPlatformAddressOffsetBits();\n+}\n","filename":"src\/hotspot\/cpu\/ppc\/gc\/z\/zGlobals_ppc.cpp","additions":203,"deletions":0,"binary":false,"changes":203,"status":"added"},{"patch":"@@ -0,0 +1,36 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2021 SAP SE. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef CPU_PPC_GC_Z_ZGLOBALS_PPC_HPP\n+#define CPU_PPC_GC_Z_ZGLOBALS_PPC_HPP\n+\n+#include \"globalDefinitions_ppc.hpp\"\n+const size_t ZPlatformGranuleSizeShift = 21; \/\/ 2MB\n+const size_t ZPlatformHeapViews        = 3;\n+const size_t ZPlatformCacheLineSize    = DEFAULT_CACHE_LINE_SIZE;\n+\n+size_t ZPlatformAddressOffsetBits();\n+size_t ZPlatformAddressMetadataShift();\n+\n+#endif \/\/ CPU_PPC_GC_Z_ZGLOBALS_PPC_HPP\n","filename":"src\/hotspot\/cpu\/ppc\/gc\/z\/zGlobals_ppc.hpp","additions":36,"deletions":0,"binary":false,"changes":36,"status":"added"},{"patch":"@@ -0,0 +1,298 @@\n+\/\/\n+\/\/ Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+\/\/ Copyright (c) 2021 SAP SE. All rights reserved.\n+\/\/ DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+\/\/\n+\/\/ This code is free software; you can redistribute it and\/or modify it\n+\/\/ under the terms of the GNU General Public License version 2 only, as\n+\/\/ published by the Free Software Foundation.\n+\/\/\n+\/\/ This code is distributed in the hope that it will be useful, but WITHOUT\n+\/\/ ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+\/\/ FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+\/\/ version 2 for more details (a copy is included in the LICENSE file that\n+\/\/ accompanied this code).\n+\/\/\n+\/\/ You should have received a copy of the GNU General Public License version\n+\/\/ 2 along with this work; if not, write to the Free Software Foundation,\n+\/\/ Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+\/\/\n+\/\/ Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+\/\/ or visit www.oracle.com if you need additional information or have any\n+\/\/ questions.\n+\/\/\n+\n+source_hpp %{\n+\n+#include \"gc\/shared\/gc_globals.hpp\"\n+#include \"gc\/z\/c2\/zBarrierSetC2.hpp\"\n+#include \"gc\/z\/zThreadLocalData.hpp\"\n+\n+%}\n+\n+source %{\n+\n+static void z_load_barrier(MacroAssembler& _masm, const MachNode* node, Address ref_addr, Register ref,\n+                           Register tmp, uint8_t barrier_data) {\n+  if (barrier_data == ZLoadBarrierElided) {\n+    return;\n+  }\n+\n+  ZLoadBarrierStubC2* const stub = ZLoadBarrierStubC2::create(node, ref_addr, ref, tmp, barrier_data);\n+  __ ld(tmp, in_bytes(ZThreadLocalData::address_bad_mask_offset()), R16_thread);\n+  __ and_(tmp, tmp, ref);\n+  __ bne_far(CCR0, *stub->entry(), MacroAssembler::bc_far_optimize_on_relocate);\n+  __ bind(*stub->continuation());\n+}\n+\n+static void z_load_barrier_slow_path(MacroAssembler& _masm, const MachNode* node, Address ref_addr, Register ref,\n+                                     Register tmp) {\n+  ZLoadBarrierStubC2* const stub = ZLoadBarrierStubC2::create(node, ref_addr, ref, tmp, ZLoadBarrierStrong);\n+  __ b(*stub->entry());\n+  __ bind(*stub->continuation());\n+}\n+\n+static void z_compare_and_swap(MacroAssembler& _masm, const MachNode* node,\n+                              Register res, Register mem, Register oldval, Register newval,\n+                              Register tmp_xchg, Register tmp_mask,\n+                              bool weak, bool acquire) {\n+  \/\/ z-specific load barrier requires strong CAS operations.\n+  \/\/ Weak CAS operations are thus only emitted if the barrier is elided.\n+  __ cmpxchgd(CCR0, tmp_xchg, oldval, newval, mem,\n+              MacroAssembler::MemBarNone, MacroAssembler::cmpxchgx_hint_atomic_update(), res, NULL, true,\n+              weak && node->barrier_data() == ZLoadBarrierElided);\n+\n+  if (node->barrier_data() != ZLoadBarrierElided) {\n+    Label skip_barrier;\n+\n+    __ ld(tmp_mask, in_bytes(ZThreadLocalData::address_bad_mask_offset()), R16_thread);\n+    __ and_(tmp_mask, tmp_mask, tmp_xchg);\n+    __ beq(CCR0, skip_barrier);\n+\n+    \/\/ CAS must have failed because pointer in memory is bad.\n+    z_load_barrier_slow_path(_masm, node, Address(mem), tmp_xchg, res \/* used as tmp *\/);\n+\n+    __ cmpxchgd(CCR0, tmp_xchg, oldval, newval, mem,\n+                MacroAssembler::MemBarNone, MacroAssembler::cmpxchgx_hint_atomic_update(), res, NULL, true, weak);\n+\n+    __ bind(skip_barrier);\n+  }\n+\n+  if (acquire) {\n+    if (support_IRIW_for_not_multiple_copy_atomic_cpu) {\n+      \/\/ Uses the isync instruction as an acquire barrier.\n+      \/\/ This exploits the compare and the branch in the z load barrier (load, compare and branch, isync).\n+      __ isync();\n+    } else {\n+      __ sync();\n+    }\n+  }\n+}\n+\n+static void z_compare_and_exchange(MacroAssembler& _masm, const MachNode* node,\n+                                   Register res, Register mem, Register oldval, Register newval, Register tmp,\n+                                   bool weak, bool acquire) {\n+  \/\/ z-specific load barrier requires strong CAS operations.\n+  \/\/ Weak CAS operations are thus only emitted if the barrier is elided.\n+  __ cmpxchgd(CCR0, res, oldval, newval, mem,\n+              MacroAssembler::MemBarNone, MacroAssembler::cmpxchgx_hint_atomic_update(), noreg, NULL, true,\n+              weak && node->barrier_data() == ZLoadBarrierElided);\n+\n+  if (node->barrier_data() != ZLoadBarrierElided) {\n+    Label skip_barrier;\n+    __ ld(tmp, in_bytes(ZThreadLocalData::address_bad_mask_offset()), R16_thread);\n+    __ and_(tmp, tmp, res);\n+    __ beq(CCR0, skip_barrier);\n+\n+    z_load_barrier_slow_path(_masm, node, Address(mem), res, tmp);\n+\n+    __ cmpxchgd(CCR0, res, oldval, newval, mem,\n+                MacroAssembler::MemBarNone, MacroAssembler::cmpxchgx_hint_atomic_update(), noreg, NULL, true, weak);\n+\n+    __ bind(skip_barrier);\n+  }\n+\n+  if (acquire) {\n+    if (support_IRIW_for_not_multiple_copy_atomic_cpu) {\n+      \/\/ Uses the isync instruction as an acquire barrier.\n+      \/\/ This exploits the compare and the branch in the z load barrier (load, compare and branch, isync).\n+      __ isync();\n+    } else {\n+      __ sync();\n+    }\n+  }\n+}\n+\n+%}\n+\n+instruct zLoadP(iRegPdst dst, memoryAlg4 mem, iRegPdst tmp, flagsRegCR0 cr0)\n+%{\n+  match(Set dst (LoadP mem));\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr0);\n+  ins_cost(MEMORY_REF_COST);\n+\n+  predicate((UseZGC && n->as_Load()->barrier_data() != 0)\n+            && (n->as_Load()->is_unordered() || followed_by_acquire(n)));\n+\n+  format %{ \"LD $dst, $mem\" %}\n+  ins_encode %{\n+    assert($mem$$index == 0, \"sanity\");\n+    __ ld($dst$$Register, $mem$$disp, $mem$$base$$Register);\n+    z_load_barrier(_masm, this, Address($mem$$base$$Register, $mem$$disp), $dst$$Register, $tmp$$Register, barrier_data());\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+\/\/ Load Pointer Volatile\n+instruct zLoadP_acq(iRegPdst dst, memoryAlg4 mem, iRegPdst tmp, flagsRegCR0 cr0)\n+%{\n+  match(Set dst (LoadP mem));\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr0);\n+  ins_cost(3 * MEMORY_REF_COST);\n+\n+  \/\/ Predicate on instruction order is implicitly present due to the predicate of the cheaper zLoadP operation\n+  predicate(UseZGC && n->as_Load()->barrier_data() != 0);\n+\n+  format %{ \"LD acq $dst, $mem\" %}\n+  ins_encode %{\n+    __ ld($dst$$Register, $mem$$disp, $mem$$base$$Register);\n+    z_load_barrier(_masm, this, Address($mem$$base$$Register, $mem$$disp), $dst$$Register, $tmp$$Register, barrier_data());\n+\n+    \/\/ Uses the isync instruction as an acquire barrier.\n+    \/\/ This exploits the compare and the branch in the z load barrier (load, compare and branch, isync).\n+    __ isync();\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct zCompareAndSwapP(iRegIdst res, iRegPdst mem, iRegPsrc oldval, iRegPsrc newval,\n+                          iRegPdst tmp_xchg, iRegPdst tmp_mask, flagsRegCR0 cr0) %{\n+  match(Set res (CompareAndSwapP mem (Binary oldval newval)));\n+  effect(TEMP_DEF res, TEMP tmp_xchg, TEMP tmp_mask, KILL cr0);\n+\n+  predicate((UseZGC && n->as_LoadStore()->barrier_data() == ZLoadBarrierStrong)\n+            && (((CompareAndSwapNode*)n)->order() != MemNode::acquire && ((CompareAndSwapNode*) n)->order() != MemNode::seqcst));\n+\n+  format %{ \"CMPXCHG $res, $mem, $oldval, $newval; as bool; ptr\" %}\n+  ins_encode %{\n+    z_compare_and_swap(_masm, this,\n+                       $res$$Register, $mem$$Register, $oldval$$Register, $newval$$Register,\n+                       $tmp_xchg$$Register, $tmp_mask$$Register,\n+                       false \/* weak *\/, false \/* acquire *\/);\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct zCompareAndSwapP_acq(iRegIdst res, iRegPdst mem, iRegPsrc oldval, iRegPsrc newval,\n+                              iRegPdst tmp_xchg, iRegPdst tmp_mask, flagsRegCR0 cr0) %{\n+  match(Set res (CompareAndSwapP mem (Binary oldval newval)));\n+  effect(TEMP_DEF res, TEMP tmp_xchg, TEMP tmp_mask, KILL cr0);\n+\n+  predicate((UseZGC && n->as_LoadStore()->barrier_data() == ZLoadBarrierStrong)\n+            && (((CompareAndSwapNode*)n)->order() == MemNode::acquire || ((CompareAndSwapNode*) n)->order() == MemNode::seqcst));\n+\n+  format %{ \"CMPXCHG acq $res, $mem, $oldval, $newval; as bool; ptr\" %}\n+  ins_encode %{\n+    z_compare_and_swap(_masm, this,\n+                       $res$$Register, $mem$$Register, $oldval$$Register, $newval$$Register,\n+                       $tmp_xchg$$Register, $tmp_mask$$Register,\n+                       false \/* weak *\/, true \/* acquire *\/);\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct zCompareAndSwapPWeak(iRegIdst res, iRegPdst mem, iRegPsrc oldval, iRegPsrc newval,\n+                              iRegPdst tmp_xchg, iRegPdst tmp_mask, flagsRegCR0 cr0) %{\n+  match(Set res (WeakCompareAndSwapP mem (Binary oldval newval)));\n+  effect(TEMP_DEF res, TEMP tmp_xchg, TEMP tmp_mask, KILL cr0);\n+\n+  predicate((UseZGC && n->as_LoadStore()->barrier_data() == ZLoadBarrierStrong)\n+            && ((CompareAndSwapNode*)n)->order() != MemNode::acquire && ((CompareAndSwapNode*) n)->order() != MemNode::seqcst);\n+\n+  format %{ \"weak CMPXCHG $res, $mem, $oldval, $newval; as bool; ptr\" %}\n+  ins_encode %{\n+    z_compare_and_swap(_masm, this,\n+                       $res$$Register, $mem$$Register, $oldval$$Register, $newval$$Register,\n+                       $tmp_xchg$$Register, $tmp_mask$$Register,\n+                       true \/* weak *\/, false \/* acquire *\/);\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct zCompareAndSwapPWeak_acq(iRegIdst res, iRegPdst mem, iRegPsrc oldval, iRegPsrc newval,\n+                                  iRegPdst tmp_xchg, iRegPdst tmp_mask, flagsRegCR0 cr0) %{\n+  match(Set res (WeakCompareAndSwapP mem (Binary oldval newval)));\n+  effect(TEMP_DEF res, TEMP tmp_xchg, TEMP tmp_mask, KILL cr0);\n+\n+  predicate((UseZGC && n->as_LoadStore()->barrier_data() == ZLoadBarrierStrong)\n+            && (((CompareAndSwapNode*)n)->order() == MemNode::acquire || ((CompareAndSwapNode*) n)->order() == MemNode::seqcst));\n+\n+  format %{ \"weak CMPXCHG acq $res, $mem, $oldval, $newval; as bool; ptr\" %}\n+  ins_encode %{\n+    z_compare_and_swap(_masm, this,\n+                       $res$$Register, $mem$$Register, $oldval$$Register, $newval$$Register,\n+                       $tmp_xchg$$Register, $tmp_mask$$Register,\n+                       true \/* weak *\/, true \/* acquire *\/);\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct zCompareAndExchangeP(iRegPdst res, iRegPdst mem, iRegPsrc oldval, iRegPsrc newval,\n+                              iRegPdst tmp, flagsRegCR0 cr0) %{\n+  match(Set res (CompareAndExchangeP mem (Binary oldval newval)));\n+  effect(TEMP_DEF res, TEMP tmp, KILL cr0);\n+\n+  predicate((UseZGC && n->as_LoadStore()->barrier_data() == ZLoadBarrierStrong)\n+            && (\n+              ((CompareAndSwapNode*)n)->order() != MemNode::acquire\n+              && ((CompareAndSwapNode*)n)->order() != MemNode::seqcst\n+            ));\n+\n+  format %{ \"CMPXCHG $res, $mem, $oldval, $newval; as ptr; ptr\" %}\n+  ins_encode %{\n+    z_compare_and_exchange(_masm, this,\n+                           $res$$Register, $mem$$Register, $oldval$$Register, $newval$$Register, $tmp$$Register,\n+                           false \/* weak *\/, false \/* acquire *\/);\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct zCompareAndExchangeP_acq(iRegPdst res, iRegPdst mem, iRegPsrc oldval, iRegPsrc newval,\n+                                  iRegPdst tmp, flagsRegCR0 cr0) %{\n+  match(Set res (CompareAndExchangeP mem (Binary oldval newval)));\n+  effect(TEMP_DEF res, TEMP tmp, KILL cr0);\n+\n+  predicate((UseZGC && n->as_LoadStore()->barrier_data() == ZLoadBarrierStrong)\n+            && (\n+              ((CompareAndSwapNode*)n)->order() == MemNode::acquire\n+              || ((CompareAndSwapNode*)n)->order() == MemNode::seqcst\n+            ));\n+\n+  format %{ \"CMPXCHG acq $res, $mem, $oldval, $newval; as ptr; ptr\" %}\n+  ins_encode %{\n+    z_compare_and_exchange(_masm, this,\n+                           $res$$Register, $mem$$Register, $oldval$$Register, $newval$$Register, $tmp$$Register,\n+                           false \/* weak *\/, true \/* acquire *\/);\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct zGetAndSetP(iRegPdst res, iRegPdst mem, iRegPsrc newval, iRegPdst tmp, flagsRegCR0 cr0) %{\n+  match(Set res (GetAndSetP mem newval));\n+  effect(TEMP_DEF res, TEMP tmp, KILL cr0);\n+\n+  predicate(UseZGC && n->as_LoadStore()->barrier_data() != 0);\n+\n+  format %{ \"GetAndSetP $res, $mem, $newval\" %}\n+  ins_encode %{\n+    __ getandsetd($res$$Register, $newval$$Register, $mem$$Register, MacroAssembler::cmpxchgx_hint_atomic_update());\n+    z_load_barrier(_masm, this, Address(noreg, (intptr_t) 0), $res$$Register, $tmp$$Register, barrier_data());\n+\n+    if (support_IRIW_for_not_multiple_copy_atomic_cpu) {\n+      __ isync();\n+    } else {\n+      __ sync();\n+    }\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n","filename":"src\/hotspot\/cpu\/ppc\/gc\/z\/z_ppc.ad","additions":298,"deletions":0,"binary":false,"changes":298,"status":"added"},{"patch":"@@ -5534,1 +5534,1 @@\n-  predicate(n->as_Load()->is_unordered() || followed_by_acquire(n));\n+  predicate((n->as_Load()->is_unordered() || followed_by_acquire(n)) && n->as_Load()->barrier_data() == 0);\n@@ -5548,0 +5548,2 @@\n+  predicate(n->as_Load()->barrier_data() == 0);\n+\n@@ -5559,1 +5561,1 @@\n-  predicate(_kids[0]->_leaf->as_Load()->is_unordered());\n+  predicate(_kids[0]->_leaf->as_Load()->is_unordered() && _kids[0]->_leaf->as_Load()->barrier_data() == 0);\n@@ -7481,0 +7483,1 @@\n+  predicate(n->as_LoadStore()->barrier_data() == 0);\n@@ -7645,0 +7648,1 @@\n+  predicate(n->as_LoadStore()->barrier_data() == 0);\n@@ -7867,1 +7871,1 @@\n-  predicate(((CompareAndSwapNode*)n)->order() != MemNode::acquire && ((CompareAndSwapNode*)n)->order() != MemNode::seqcst);\n+  predicate((((CompareAndSwapNode*)n)->order() != MemNode::acquire && ((CompareAndSwapNode*)n)->order() != MemNode::seqcst) && n->as_LoadStore()->barrier_data() == 0);\n@@ -7881,1 +7885,1 @@\n-  predicate(((CompareAndSwapNode*)n)->order() == MemNode::acquire || ((CompareAndSwapNode*)n)->order() == MemNode::seqcst);\n+  predicate((((CompareAndSwapNode*)n)->order() == MemNode::acquire || ((CompareAndSwapNode*)n)->order() == MemNode::seqcst) && n->as_LoadStore()->barrier_data() == 0);\n@@ -8137,1 +8141,2 @@\n-  predicate(((CompareAndSwapNode*)n)->order() != MemNode::acquire && ((CompareAndSwapNode*)n)->order() != MemNode::seqcst);\n+  predicate((((CompareAndSwapNode*)n)->order() != MemNode::acquire && ((CompareAndSwapNode*)n)->order() != MemNode::seqcst)\n+            && n->as_Load()->barrier_data() == 0);\n@@ -8151,1 +8156,2 @@\n-  predicate(((CompareAndSwapNode*)n)->order() == MemNode::acquire || ((CompareAndSwapNode*)n)->order() == MemNode::seqcst);\n+  predicate((((CompareAndSwapNode*)n)->order() == MemNode::acquire || ((CompareAndSwapNode*)n)->order() == MemNode::seqcst)\n+            && n->as_Load()->barrier_data() == 0);\n@@ -8373,0 +8379,1 @@\n+  predicate(n->as_Load()->barrier_data() == 0);\n","filename":"src\/hotspot\/cpu\/ppc\/ppc.ad","additions":13,"deletions":6,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -2,2 +2,2 @@\n- * Copyright (c) 2001, 2019, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2012, 2013 SAP SE. All rights reserved.\n+ * Copyright (c) 2001, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2012, 2021 SAP SE. All rights reserved.\n@@ -38,0 +38,15 @@\n+inline bool is_VectorRegister() {\n+  return value() >= ConcreteRegisterImpl::max_fpr &&\n+         value() < ConcreteRegisterImpl::max_vsr;\n+}\n+\n+inline bool is_ConditionRegister() {\n+  return value() >= ConcreteRegisterImpl::max_vsr &&\n+         value() < ConcreteRegisterImpl::max_cnd;\n+}\n+\n+inline bool is_SpecialRegister() {\n+  return value() >= ConcreteRegisterImpl::max_cnd &&\n+         value() < ConcreteRegisterImpl::max_spr;\n+}\n+\n","filename":"src\/hotspot\/cpu\/ppc\/vmreg_ppc.hpp","additions":17,"deletions":2,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -0,0 +1,42 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2021 SAP SE. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef OS_CPU_LINUX_PPC_GC_Z_ZSYSCALL_LINUX_PPC_HPP\n+#define OS_CPU_LINUX_PPC_GC_Z_ZSYSCALL_LINUX_PPC_HPP\n+\n+#include <sys\/syscall.h>\n+\n+\/\/\n+\/\/ Support for building on older Linux systems\n+\/\/\n+\n+\n+#ifndef SYS_memfd_create\n+#define SYS_memfd_create     360\n+#endif\n+#ifndef SYS_fallocate\n+#define SYS_fallocate        309\n+#endif\n+\n+#endif \/\/ OS_CPU_LINUX_PPC_GC_Z_ZSYSCALL_LINUX_PPC_HPP\n","filename":"src\/hotspot\/os_cpu\/linux_ppc\/gc\/z\/zSyscall_linux_ppc.hpp","additions":42,"deletions":0,"binary":false,"changes":42,"status":"added"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -30,1 +30,1 @@\n-  return Address(thread, ZThreadLocalData::address_bad_mask_offset());\n+  return Address(thread, (intptr_t) ZThreadLocalData::address_bad_mask_offset());\n@@ -34,1 +34,1 @@\n-  return Address(env, ZThreadLocalData::address_bad_mask_offset() - JavaThread::jni_environment_offset());\n+  return Address(env, (intptr_t) (ZThreadLocalData::address_bad_mask_offset() - JavaThread::jni_environment_offset()));\n","filename":"src\/hotspot\/share\/gc\/z\/zBarrierSetAssembler.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"}]}