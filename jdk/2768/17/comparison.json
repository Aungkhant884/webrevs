{"files":[{"patch":"@@ -2442,0 +2442,8 @@\n+const RegMask* Matcher::predicate_reg_mask(void) {\n+  return &_PR_REG_mask;\n+}\n+\n+const TypeVect* Matcher::predicate_reg_type(const Type* elemTy, int length) {\n+  return new TypeVectMask(elemTy, length);\n+}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n-\/\/ Copyright (c) 2008, 2020, Oracle and\/or its affiliates. All rights reserved.\n+\/\/ Copyright (c) 2008, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -996,0 +996,8 @@\n+const RegMask* Matcher::predicate_reg_mask(void) {\n+  return NULL;\n+}\n+\n+const TypeVect* Matcher::predicate_reg_type(const Type* elemTy, int length) {\n+  return NULL;\n+}\n+\n","filename":"src\/hotspot\/cpu\/arm\/arm.ad","additions":9,"deletions":1,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -2163,0 +2163,8 @@\n+const RegMask* Matcher::predicate_reg_mask(void) {\n+  return NULL;\n+}\n+\n+const TypeVect* Matcher::predicate_reg_type(const Type* elemTy, int length) {\n+  return NULL;\n+}\n+\n","filename":"src\/hotspot\/cpu\/ppc\/ppc.ad","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -1549,0 +1549,8 @@\n+const RegMask* Matcher::predicate_reg_mask(void) {\n+  return NULL;\n+}\n+\n+const TypeVect* Matcher::predicate_reg_type(const Type* elemTy, int length) {\n+  return NULL;\n+}\n+\n","filename":"src\/hotspot\/cpu\/s390\/s390.ad","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2455,0 +2455,16 @@\n+void Assembler::kmovwl(Address dst, KRegister src) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  vex_prefix(dst, 0, src->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  emit_int8((unsigned char)0x91);\n+  emit_operand((Register)src, dst);\n+}\n+\n+void Assembler::kmovwl(KRegister dst, KRegister src) {\n+  assert(VM_Version::supports_avx512bw(), \"\");\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0x90, (0xC0 | encode));\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.cpp","additions":16,"deletions":0,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -1462,0 +1462,2 @@\n+  void kmovwl(Address dst, KRegister src);\n+  void kmovwl(KRegister dst, KRegister src);\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -52,1 +52,1 @@\n-void C2_MacroAssembler::setvectmask(Register dst, Register src) {\n+void C2_MacroAssembler::setvectmask(Register dst, Register src, KRegister mask) {\n@@ -57,1 +57,1 @@\n-  Assembler::kmovdl(k1, dst);\n+  Assembler::kmovdl(mask, dst);\n@@ -61,1 +61,1 @@\n-void C2_MacroAssembler::restorevectmask() {\n+void C2_MacroAssembler::restorevectmask(KRegister mask) {\n@@ -63,1 +63,1 @@\n-  Assembler::knotwl(k1, k0);\n+  Assembler::knotwl(mask, k0);\n@@ -1896,1 +1896,1 @@\n-void C2_MacroAssembler::genmask(Register dst, Register len, Register temp) {\n+void C2_MacroAssembler::genmask(KRegister dst, Register len, Register temp) {\n@@ -1898,2 +1898,3 @@\n-  mov64(dst, -1L);\n-  bzhiq(dst, dst, len);\n+  mov64(temp, -1L);\n+  bzhiq(temp, temp, len);\n+  kmovql(dst, temp);\n@@ -2157,1 +2158,2 @@\n-void C2_MacroAssembler::vectortest(int bt, int vlen, XMMRegister src1, XMMRegister src2, XMMRegister vtmp1, XMMRegister vtmp2) {\n+void C2_MacroAssembler::vectortest(int bt, int vlen, XMMRegister src1, XMMRegister src2,\n+                                   XMMRegister vtmp1, XMMRegister vtmp2, KRegister mask) {\n@@ -2195,1 +2197,0 @@\n-        KRegister ktemp = k2; \/\/ Use a hardcoded temp due to no k register allocation.\n@@ -2197,1 +2198,1 @@\n-        evpcmpeqb(ktemp, src1, src2, Assembler::AVX_512bit);\n+        evpcmpeqb(mask, src1, src2, Assembler::AVX_512bit);\n@@ -2199,1 +2200,1 @@\n-          ktestql(ktemp, ktemp);\n+          ktestql(mask, mask);\n@@ -2202,1 +2203,1 @@\n-          kortestql(ktemp, ktemp);\n+          kortestql(mask, mask);\n@@ -2919,1 +2920,1 @@\n-                                       XMMRegister vec1, int ae) {\n+                                       XMMRegister vec1, int ae, KRegister mask) {\n@@ -3072,1 +3073,1 @@\n-        evpcmpeqb(k7, vec1, Address(str2, result, scale), Assembler::AVX_512bit); \/\/ k7 == 11..11, if operands equal, otherwise k7 has some 0\n+        evpcmpeqb(mask, vec1, Address(str2, result, scale), Assembler::AVX_512bit); \/\/ k7 == 11..11, if operands equal, otherwise k7 has some 0\n@@ -3075,1 +3076,1 @@\n-        evpcmpeqb(k7, vec1, Address(str2, result, scale2), Assembler::AVX_512bit); \/\/ k7 == 11..11, if operands equal, otherwise k7 has some 0\n+        evpcmpeqb(mask, vec1, Address(str2, result, scale2), Assembler::AVX_512bit); \/\/ k7 == 11..11, if operands equal, otherwise k7 has some 0\n@@ -3077,1 +3078,1 @@\n-      kortestql(k7, k7);\n+      kortestql(mask, mask);\n@@ -3261,1 +3262,1 @@\n-    kmovql(cnt1, k7);\n+    kmovql(cnt1, mask);\n@@ -3310,1 +3311,1 @@\n-  XMMRegister vec1, XMMRegister vec2) {\n+  XMMRegister vec1, XMMRegister vec2, KRegister mask1, KRegister mask2) {\n@@ -3342,2 +3343,2 @@\n-    evpcmpgtb(k2, vec2, Address(ary1, len, Address::times_1), Assembler::AVX_512bit);\n-    kortestql(k2, k2);\n+    evpcmpgtb(mask1, vec2, Address(ary1, len, Address::times_1), Assembler::AVX_512bit);\n+    kortestql(mask1, mask1);\n@@ -3360,1 +3361,1 @@\n-    kmovql(k3, tmp3_aliased);\n+    kmovql(mask2, tmp3_aliased);\n@@ -3385,1 +3386,1 @@\n-    evpcmpgtb(k3, vec1, Address(len, 0), Assembler::AVX_512bit);\n+    evpcmpgtb(mask2, vec1, Address(len, 0), Assembler::AVX_512bit);\n@@ -3388,2 +3389,2 @@\n-    evpcmpgtb(k2, k3, vec2, Address(ary1, 0), Assembler::AVX_512bit);\n-    ktestq(k2, k3);\n+    evpcmpgtb(mask1, mask2, vec2, Address(ary1, 0), Assembler::AVX_512bit);\n+    ktestq(mask1, mask2);\n@@ -3516,1 +3517,1 @@\n-                                      XMMRegister vec1, XMMRegister vec2, bool is_char) {\n+                                      XMMRegister vec1, XMMRegister vec2, bool is_char, KRegister mask) {\n@@ -3579,2 +3580,2 @@\n-      evpcmpeqb(k7, vec1, Address(ary2, limit, Address::times_1), Assembler::AVX_512bit);\n-      kortestql(k7, k7);\n+      evpcmpeqb(mask, vec1, Address(ary2, limit, Address::times_1), Assembler::AVX_512bit);\n+      kortestql(mask, mask);\n@@ -3597,2 +3598,2 @@\n-      evpcmpeqb(k7, vec1, Address(ary2, result, Address::times_1), Assembler::AVX_512bit);\n-      kortestql(k7, k7);\n+      evpcmpeqb(mask, vec1, Address(ary2, result, Address::times_1), Assembler::AVX_512bit);\n+      kortestql(mask, mask);\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":31,"deletions":30,"binary":false,"changes":61,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -34,2 +34,2 @@\n-  void setvectmask(Register dst, Register src);\n-  void restorevectmask();\n+  void setvectmask(Register dst, Register src, KRegister mask);\n+  void restorevectmask(KRegister mask);\n@@ -134,1 +134,1 @@\n-                  XMMRegister vtmp1 = xnoreg, XMMRegister vtmp2 = xnoreg);\n+                  XMMRegister vtmp1 = xnoreg, XMMRegister vtmp2 = xnoreg, KRegister mask = knoreg);\n@@ -149,1 +149,1 @@\n-  void genmask(Register dst, Register len, Register temp);\n+  void genmask(KRegister dst, Register len, Register temp);\n@@ -247,1 +247,1 @@\n-                      XMMRegister vec1, int ae);\n+                      XMMRegister vec1, int ae, KRegister mask = knoreg);\n@@ -253,1 +253,1 @@\n-                     XMMRegister vec1, XMMRegister vec2);\n+                     XMMRegister vec1, XMMRegister vec2, KRegister mask1 = knoreg, KRegister mask2 = knoreg);\n@@ -258,1 +258,1 @@\n-                     XMMRegister vec1, XMMRegister vec2, bool is_char);\n+                     XMMRegister vec1, XMMRegister vec2, bool is_char, KRegister mask = knoreg);\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.hpp","additions":8,"deletions":8,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -33,1 +33,1 @@\n-LP64_ONLY(extern void reg_mask_init();)\n+extern void reg_mask_init();\n@@ -64,1 +64,1 @@\n-  LP64_ONLY(reg_mask_init();)\n+  reg_mask_init();\n","filename":"src\/hotspot\/cpu\/x86\/c2_init_x86.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -152,1 +152,0 @@\n-  \/\/ Restore registers\n@@ -397,0 +396,1 @@\n+  GrowableArray<KRegister>       _opmask_registers;\n@@ -453,0 +453,5 @@\n+  void opmask_register_save(KRegister reg) {\n+    _spill_offset -= 8;\n+    __ kmovql(Address(rsp, _spill_offset), reg);\n+  }\n+\n@@ -458,0 +463,5 @@\n+  void opmask_register_restore(KRegister reg) {\n+    __ kmovql(reg, Address(rsp, _spill_offset));\n+    _spill_offset += 8;\n+  }\n+\n@@ -480,0 +490,1 @@\n+    int opmask_spill_size = 0;\n@@ -493,0 +504,7 @@\n+      } else if (vm_reg->is_KRegister()) {\n+        \/\/ All opmask registers are caller saved, thus spill the ones\n+        \/\/ which are live.\n+        if (_opmask_registers.find(vm_reg->as_KRegister()) == -1) {\n+          _opmask_registers.append(vm_reg->as_KRegister());\n+          opmask_spill_size += 8;\n+        }\n@@ -523,1 +541,1 @@\n-    _spill_offset = _spill_size = align_up(xmm_spill_size + gp_spill_size + arg_spill_size, 16);\n+    _spill_offset = _spill_size = align_up(xmm_spill_size + gp_spill_size + opmask_spill_size + arg_spill_size, 16);\n@@ -530,0 +548,1 @@\n+      _opmask_registers(),\n@@ -579,0 +598,5 @@\n+\n+    \/\/ Save opmask registers\n+    for (int i = 0; i < _opmask_registers.length(); i++) {\n+      opmask_register_save(_opmask_registers.at(i));\n+    }\n@@ -582,0 +606,5 @@\n+    \/\/ Restore opmask registers\n+    for (int i = _opmask_registers.length() - 1; i >= 0; i--) {\n+      opmask_register_restore(_opmask_registers.at(i));\n+    }\n+\n","filename":"src\/hotspot\/cpu\/x86\/gc\/z\/zBarrierSetAssembler_x86.cpp","additions":32,"deletions":3,"binary":false,"changes":35,"status":"modified"},{"patch":"@@ -2528,0 +2528,53 @@\n+void MacroAssembler::kmov(KRegister dst, Address src) {\n+  if (VM_Version::supports_avx512bw()) {\n+    kmovql(dst, src);\n+  } else {\n+    assert(VM_Version::supports_evex(), \"\");\n+    kmovwl(dst, src);\n+  }\n+}\n+\n+void MacroAssembler::kmov(Address dst, KRegister src) {\n+  if (VM_Version::supports_avx512bw()) {\n+    kmovql(dst, src);\n+  } else {\n+    assert(VM_Version::supports_evex(), \"\");\n+    kmovwl(dst, src);\n+  }\n+}\n+\n+void MacroAssembler::kmov(KRegister dst, KRegister src) {\n+  if (VM_Version::supports_avx512bw()) {\n+    kmovql(dst, src);\n+  } else {\n+    assert(VM_Version::supports_evex(), \"\");\n+    kmovwl(dst, src);\n+  }\n+}\n+\n+void MacroAssembler::kmov(Register dst, KRegister src) {\n+  if (VM_Version::supports_avx512bw()) {\n+    kmovql(dst, src);\n+  } else {\n+    assert(VM_Version::supports_evex(), \"\");\n+    kmovwl(dst, src);\n+  }\n+}\n+\n+void MacroAssembler::kmov(KRegister dst, Register src) {\n+  if (VM_Version::supports_avx512bw()) {\n+    kmovql(dst, src);\n+  } else {\n+    assert(VM_Version::supports_evex(), \"\");\n+    kmovwl(dst, src);\n+  }\n+}\n+\n+void MacroAssembler::kmovql(KRegister dst, AddressLiteral src, Register scratch_reg) {\n+  if (reachable(src)) {\n+    kmovql(dst, as_Address(src));\n+  } else {\n+    lea(scratch_reg, src);\n+    kmovql(dst, Address(scratch_reg, 0));\n+  }\n+}\n@@ -4943,1 +4996,1 @@\n-void MacroAssembler::xmm_clear_mem(Register base, Register cnt, Register rtmp, XMMRegister xtmp) {\n+void MacroAssembler::xmm_clear_mem(Register base, Register cnt, Register rtmp, XMMRegister xtmp, KRegister mask) {\n@@ -4976,1 +5029,1 @@\n-    fill64_masked_avx(3, base, 0, xtmp, k2, cnt, rtmp, true);\n+    fill64_masked_avx(3, base, 0, xtmp, mask, cnt, rtmp, true);\n@@ -4995,1 +5048,1 @@\n-    fill32_masked_avx(3, base, 0, xtmp, k2, cnt, rtmp);\n+    fill32_masked_avx(3, base, 0, xtmp, mask, cnt, rtmp);\n@@ -5009,1 +5062,1 @@\n-void MacroAssembler::clear_mem(Register base, int cnt, Register rtmp, XMMRegister xtmp) {\n+void MacroAssembler::clear_mem(Register base, int cnt, Register rtmp, XMMRegister xtmp, KRegister mask) {\n@@ -5034,2 +5087,2 @@\n-        kmovwl(k2, rtmp);\n-        evmovdqu(T_LONG, k2, Address(base, disp), xtmp, Assembler::AVX_256bit);\n+        kmovwl(mask, rtmp);\n+        evmovdqu(T_LONG, mask, Address(base, disp), xtmp, Assembler::AVX_256bit);\n@@ -5043,2 +5096,2 @@\n-          kmovwl(k2, rtmp);\n-          evmovdqu(T_LONG, k2, Address(base, disp), xtmp, Assembler::AVX_512bit);\n+          kmovwl(mask, rtmp);\n+          evmovdqu(T_LONG, mask, Address(base, disp), xtmp, Assembler::AVX_512bit);\n@@ -5053,2 +5106,2 @@\n-          kmovwl(k2, rtmp);\n-          evmovdqu(T_LONG, k2, Address(base, disp), xtmp, Assembler::AVX_512bit);\n+          kmovwl(mask, rtmp);\n+          evmovdqu(T_LONG, mask, Address(base, disp), xtmp, Assembler::AVX_512bit);\n@@ -5063,2 +5116,2 @@\n-          kmovwl(k2, rtmp);\n-          evmovdqu(T_LONG, k2, Address(base, disp), xtmp, Assembler::AVX_512bit);\n+          kmovwl(mask, rtmp);\n+          evmovdqu(T_LONG, mask, Address(base, disp), xtmp, Assembler::AVX_512bit);\n@@ -5068,2 +5121,2 @@\n-          kmovwl(k2, rtmp);\n-          evmovdqu(T_LONG, k2, Address(base, disp + 32), xtmp, Assembler::AVX_256bit);\n+          kmovwl(mask, rtmp);\n+          evmovdqu(T_LONG, mask, Address(base, disp + 32), xtmp, Assembler::AVX_256bit);\n@@ -5079,1 +5132,2 @@\n-void MacroAssembler::clear_mem(Register base, Register cnt, Register tmp, XMMRegister xtmp, bool is_large) {\n+void MacroAssembler::clear_mem(Register base, Register cnt, Register tmp, XMMRegister xtmp,\n+                               bool is_large, KRegister mask) {\n@@ -5119,1 +5173,1 @@\n-    xmm_clear_mem(base, cnt, tmp, xtmp);\n+    xmm_clear_mem(base, cnt, tmp, xtmp, mask);\n@@ -7751,1 +7805,1 @@\n-  Register tmp5, Register result) {\n+  Register tmp5, Register result, KRegister mask1, KRegister mask2) {\n@@ -7803,1 +7857,1 @@\n-    kmovdl(k3, result);\n+    kmovdl(mask2, result);\n@@ -7805,3 +7859,3 @@\n-    evmovdquw(tmp1Reg, k3, Address(src, 0), \/*merge*\/ false, Assembler::AVX_512bit);\n-    evpcmpuw(k2, k3, tmp1Reg, tmp2Reg, Assembler::le, Assembler::AVX_512bit);\n-    ktestd(k2, k3);\n+    evmovdquw(tmp1Reg, mask2, Address(src, 0), \/*merge*\/ false, Assembler::AVX_512bit);\n+    evpcmpuw(mask1, mask2, tmp1Reg, tmp2Reg, Assembler::le, Assembler::AVX_512bit);\n+    ktestd(mask1, mask2);\n@@ -7810,1 +7864,1 @@\n-    evpmovwb(Address(dst, 0), k3, tmp1Reg, Assembler::AVX_512bit);\n+    evpmovwb(Address(dst, 0), mask2, tmp1Reg, Assembler::AVX_512bit);\n@@ -7831,2 +7885,2 @@\n-    evpcmpuw(k2, tmp1Reg, tmp2Reg, Assembler::le, Assembler::AVX_512bit);\n-    kortestdl(k2, k2);\n+    evpcmpuw(mask1, tmp1Reg, tmp2Reg, Assembler::le, Assembler::AVX_512bit);\n+    kortestdl(mask1, mask1);\n@@ -7853,1 +7907,1 @@\n-    kmovdl(k3, result);\n+    kmovdl(mask2, result);\n@@ -7855,3 +7909,3 @@\n-    evmovdquw(tmp1Reg, k3, Address(src, 0), \/*merge*\/ false, Assembler::AVX_512bit);\n-    evpcmpuw(k2, k3, tmp1Reg, tmp2Reg, Assembler::le, Assembler::AVX_512bit);\n-    ktestd(k2, k3);\n+    evmovdquw(tmp1Reg, mask2, Address(src, 0), \/*merge*\/ false, Assembler::AVX_512bit);\n+    evpcmpuw(mask1, mask2, tmp1Reg, tmp2Reg, Assembler::le, Assembler::AVX_512bit);\n+    ktestd(mask1, mask2);\n@@ -7860,1 +7914,1 @@\n-    evpmovwb(Address(dst, 0), k3, tmp1Reg, Assembler::AVX_512bit);\n+    evpmovwb(Address(dst, 0), mask2, tmp1Reg, Assembler::AVX_512bit);\n@@ -7960,1 +8014,1 @@\n-  XMMRegister tmp1, Register tmp2) {\n+  XMMRegister tmp1, Register tmp2, KRegister mask) {\n@@ -8013,3 +8067,3 @@\n-    kmovdl(k2, tmp3_aliased);\n-    evpmovzxbw(tmp1, k2, Address(src, 0), Assembler::AVX_512bit);\n-    evmovdquw(Address(dst, 0), k2, tmp1, \/*merge*\/ true, Assembler::AVX_512bit);\n+    kmovdl(mask, tmp3_aliased);\n+    evpmovzxbw(tmp1, mask, Address(src, 0), Assembler::AVX_512bit);\n+    evmovdquw(Address(dst, 0), mask, tmp1, \/*merge*\/ true, Assembler::AVX_512bit);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":87,"deletions":33,"binary":false,"changes":120,"status":"modified"},{"patch":"@@ -1093,0 +1093,17 @@\n+  void kmovwl(Address dst,  KRegister src) { Assembler::kmovwl(dst, src); }\n+  void kmovwl(KRegister dst, KRegister src) { Assembler::kmovwl(dst, src); }\n+\n+  void kmovql(KRegister dst, KRegister src) { Assembler::kmovql(dst, src); }\n+  void kmovql(KRegister dst, Register src) { Assembler::kmovql(dst, src); }\n+  void kmovql(Register dst, KRegister src) { Assembler::kmovql(dst, src); }\n+  void kmovql(KRegister dst, Address src) { Assembler::kmovql(dst, src); }\n+  void kmovql(Address  dst, KRegister src) { Assembler::kmovql(dst, src); }\n+  void kmovql(KRegister dst, AddressLiteral src, Register scratch_reg = rscratch1);\n+\n+  \/\/ Safe move operation, lowers down to 16bit moves for targets supporting\n+  \/\/ AVX512F feature and 64bit moves for targets supporting AVX512BW feature.\n+  void kmov(Address  dst, KRegister src);\n+  void kmov(KRegister dst, Address src);\n+  void kmov(KRegister dst, KRegister src);\n+  void kmov(Register dst, KRegister src);\n+  void kmov(KRegister dst, Register src);\n@@ -1686,1 +1703,1 @@\n-  void clear_mem(Register base, Register cnt, Register rtmp, XMMRegister xtmp, bool is_large);\n+  void clear_mem(Register base, Register cnt, Register rtmp, XMMRegister xtmp, bool is_large, KRegister mask=knoreg);\n@@ -1689,1 +1706,1 @@\n-  void clear_mem(Register base, int cnt, Register rtmp, XMMRegister xtmp);\n+  void clear_mem(Register base, int cnt, Register rtmp, XMMRegister xtmp, KRegister mask=knoreg);\n@@ -1692,1 +1709,1 @@\n-  void xmm_clear_mem(Register base, Register cnt, Register rtmp, XMMRegister xtmp);\n+  void xmm_clear_mem(Register base, Register cnt, Register rtmp, XMMRegister xtmp, KRegister mask=knoreg);\n@@ -1805,1 +1822,2 @@\n-                           XMMRegister tmp4, Register tmp5, Register result);\n+                           XMMRegister tmp4, Register tmp5, Register result,\n+                           KRegister mask1 = knoreg, KRegister mask2 = knoreg);\n@@ -1809,1 +1827,1 @@\n-                          XMMRegister tmp1, Register tmp2);\n+                          XMMRegister tmp1, Register tmp2, KRegister mask = knoreg);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":23,"deletions":5,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -221,1 +221,3 @@\n-    max_slots_per_register = 1\n+    \/\/ opmask registers are 64bit wide on both 32 and 64 bit targets.\n+    \/\/ thus two slots are reserved per register.\n+    max_slots_per_register = 2\n@@ -259,0 +261,4 @@\n+  \/\/ x86_32.ad defines additional dummy FILL0-FILL7 registers, in order to tally\n+  \/\/ REG_COUNT (computed by ADLC based on the number of reg_defs seen in .ad files)\n+  \/\/ with ConcreteRegisterImpl::number_of_registers additional count of 8 is being\n+  \/\/ added for 32 bit jvm.\n@@ -260,1 +266,1 @@\n-      2 * FloatRegisterImpl::number_of_registers +\n+      2 * FloatRegisterImpl::number_of_registers + NOT_LP64(8) LP64_ONLY(0) +\n@@ -262,1 +268,1 @@\n-      KRegisterImpl::number_of_registers + \/\/ mask registers\n+      KRegisterImpl::number_of_registers * KRegisterImpl::max_slots_per_register + \/\/ mask registers\n","filename":"src\/hotspot\/cpu\/x86\/register_x86.hpp","additions":10,"deletions":4,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -134,0 +134,1 @@\n+  int opmask_state_bytes = KRegisterImpl::number_of_registers * 8;\n@@ -142,0 +143,1 @@\n+      additional_frame_words += opmask_state_bytes \/ wordSize;\n@@ -232,0 +234,5 @@\n+      __ subptr(rsp, opmask_state_bytes);\n+      \/\/ Save opmask registers\n+      for (int n = 0; n < KRegisterImpl::number_of_registers; n++) {\n+        __ kmov(Address(rsp, n*8), as_KRegister(n));\n+      }\n@@ -254,0 +261,1 @@\n+\n@@ -278,0 +286,2 @@\n+  int opmask_state_bytes = 0;\n+  int additional_frame_bytes = 0;\n@@ -282,1 +292,0 @@\n-  int additional_frame_bytes = 0;\n@@ -292,0 +301,2 @@\n+      opmask_state_bytes = KRegisterImpl::number_of_registers * 8;\n+      additional_frame_bytes += opmask_state_bytes;\n@@ -325,1 +336,0 @@\n-\n@@ -328,0 +338,1 @@\n+      off = opmask_state_bytes;\n@@ -329,1 +340,4 @@\n-        __ vinsertf64x4_high(as_XMMRegister(n), Address(rsp, n*32));\n+        __ vinsertf64x4_high(as_XMMRegister(n), Address(rsp, n*32+off));\n+      }\n+      for (int n = 0; n < KRegisterImpl::number_of_registers; n++) {\n+        __ kmov(as_KRegister(n), Address(rsp, n*8));\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_32.cpp","additions":17,"deletions":3,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -93,0 +93,1 @@\n+#define XSAVE_AREA_OPMASK_BEGIN 1088\n@@ -98,0 +99,1 @@\n+#define DEF_OPMASK_OFFS(regnum)    opmask ## regnum ## _off = opmask_off + (regnum)*8\/BytesPerInt,     opmask ## regnum ## H_off\n@@ -109,0 +111,4 @@\n+    opmask_off         = xmm_off + (XSAVE_AREA_OPMASK_BEGIN - XSAVE_AREA_BEGIN)\/BytesPerInt,\n+    DEF_OPMASK_OFFS(0),\n+    DEF_OPMASK_OFFS(1),\n+    \/\/ 2..7 are implied in range usage\n@@ -216,0 +222,7 @@\n+#if COMPILER2_OR_JVMCI\n+      base_addr = XSAVE_AREA_OPMASK_BEGIN;\n+      off = 0;\n+      for(int n = 0; n < KRegisterImpl::number_of_registers; n++) {\n+        __ kmov(Address(rsp, base_addr+(off++*8)), as_KRegister(n));\n+      }\n+#endif\n@@ -225,0 +238,7 @@\n+#if COMPILER2_OR_JVMCI\n+      base_addr = XSAVE_AREA_OPMASK_BEGIN;\n+      off = 0;\n+      for(int n = 0; n < KRegisterImpl::number_of_registers; n++) {\n+        __ kmov(Address(rsp, base_addr+(off++*8)), as_KRegister(n));\n+      }\n+#endif\n@@ -384,0 +404,7 @@\n+#if COMPILER2_OR_JVMCI\n+      base_addr = XSAVE_AREA_OPMASK_BEGIN;\n+      off = 0;\n+      for (int n = 0; n < KRegisterImpl::number_of_registers; n++) {\n+        __ kmov(as_KRegister(n), Address(rsp, base_addr+(off++*8)));\n+      }\n+#endif\n@@ -393,0 +420,7 @@\n+#if COMPILER2_OR_JVMCI\n+      base_addr = XSAVE_AREA_OPMASK_BEGIN;\n+      off = 0;\n+      for (int n = 0; n < KRegisterImpl::number_of_registers; n++) {\n+        __ kmov(as_KRegister(n), Address(rsp, base_addr+(off++*8)));\n+      }\n+#endif\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_64.cpp","additions":34,"deletions":0,"binary":false,"changes":34,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2006, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2006, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -75,0 +75,1 @@\n+\/\/TODO: Case for KRegisters\n","filename":"src\/hotspot\/cpu\/x86\/vmreg_x86.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2006, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2006, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -85,1 +85,1 @@\n-  return ::as_KRegister((value() - ConcreteRegisterImpl::max_xmm));\n+  return ::as_KRegister((value() - ConcreteRegisterImpl::max_xmm) >> 1);\n","filename":"src\/hotspot\/cpu\/x86\/vmreg_x86.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2006, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2006, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -46,1 +46,1 @@\n-  return VMRegImpl::as_VMReg(encoding() + ConcreteRegisterImpl::max_xmm);\n+  return VMRegImpl::as_VMReg((encoding() << 1) + ConcreteRegisterImpl::max_xmm);\n","filename":"src\/hotspot\/cpu\/x86\/vmreg_x86.inline.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -631,0 +631,23 @@\n+\/\/ AVX3 Mask Registers.\n+reg_def K1   (SOC, SOC, Op_RegI,  1, k1->as_VMReg());\n+reg_def K1_H (SOC, SOC, Op_RegI,  1, k1->as_VMReg()->next());\n+\n+reg_def K2   (SOC, SOC, Op_RegI,  2, k2->as_VMReg());\n+reg_def K2_H (SOC, SOC, Op_RegI,  2, k2->as_VMReg()->next());\n+\n+reg_def K3   (SOC, SOC, Op_RegI,  3, k3->as_VMReg());\n+reg_def K3_H (SOC, SOC, Op_RegI,  3, k3->as_VMReg()->next());\n+\n+reg_def K4   (SOC, SOC, Op_RegI,  4, k4->as_VMReg());\n+reg_def K4_H (SOC, SOC, Op_RegI,  4, k4->as_VMReg()->next());\n+\n+reg_def K5   (SOC, SOC, Op_RegI,  5, k5->as_VMReg());\n+reg_def K5_H (SOC, SOC, Op_RegI,  5, k5->as_VMReg()->next());\n+\n+reg_def K6   (SOC, SOC, Op_RegI,  6, k6->as_VMReg());\n+reg_def K6_H (SOC, SOC, Op_RegI,  6, k6->as_VMReg()->next());\n+\n+reg_def K7   (SOC, SOC, Op_RegI,  7, k7->as_VMReg());\n+reg_def K7_H (SOC, SOC, Op_RegI,  7, k7->as_VMReg()->next());\n+\n+\n@@ -667,0 +690,24 @@\n+alloc_class chunk2(K7, K7_H,\n+                   K6, K6_H,\n+                   K5, K5_H,\n+                   K4, K4_H,\n+                   K3, K3_H,\n+                   K2, K2_H,\n+                   K1, K1_H);\n+\n+reg_class  vectmask_reg(K1, K1_H,\n+                        K2, K2_H,\n+                        K3, K3_H,\n+                        K4, K4_H,\n+                        K5, K5_H,\n+                        K6, K6_H,\n+                        K7, K7_H);\n+\n+reg_class vectmask_reg_K1(K1, K1_H);\n+reg_class vectmask_reg_K2(K2, K2_H);\n+reg_class vectmask_reg_K3(K3, K3_H);\n+reg_class vectmask_reg_K4(K4, K4_H);\n+reg_class vectmask_reg_K5(K5, K5_H);\n+reg_class vectmask_reg_K6(K6, K6_H);\n+reg_class vectmask_reg_K7(K7, K7_H);\n+\n@@ -668,1 +715,2 @@\n-alloc_class chunk2(RFLAGS);\n+alloc_class chunk3(RFLAGS);\n+\n@@ -1371,0 +1419,1 @@\n+  const bool is_LP64 = LP64_ONLY(true) NOT_LP64(false);\n@@ -1527,0 +1576,1 @@\n+\n@@ -1530,1 +1580,1 @@\n-      if (UseAVX < 3 || !VM_Version::supports_bmi2()) {\n+      if (!is_LP64  || UseAVX < 3 || !VM_Version::supports_bmi2()) {\n@@ -1561,0 +1611,1 @@\n+  const bool is_LP64 = LP64_ONLY(true) NOT_LP64(false);\n@@ -1611,1 +1662,1 @@\n-      if (!VM_Version::supports_avx512bw()) {\n+      if (!is_LP64 || !VM_Version::supports_avx512bw()) {\n@@ -1834,0 +1885,8 @@\n+const RegMask* Matcher::predicate_reg_mask(void) {\n+  return &_VECTMASK_REG_mask;\n+}\n+\n+const TypeVect* Matcher::predicate_reg_type(const Type* elemTy, int length) {\n+  return new TypeVectMask(TypeInt::BOOL, length);\n+}\n+\n@@ -2555,3 +2614,7 @@\n-\n-instruct setMask(rRegI dst, rRegI src) %{\n-  predicate(Matcher::has_predicated_vectors());\n+\/\/ Existing partial implementation for post-loop multi-versioning computes\n+\/\/ the mask corresponding to tail loop in K1 opmask register. This may then be\n+\/\/ used for predicating instructions in loop body during last post-loop iteration.\n+\/\/ TODO: Remove hard-coded K1 usage while fixing existing post-loop\n+\/\/ multiversioning support.\n+instruct setMask(rRegI dst, rRegI src, kReg_K1 mask) %{\n+  predicate(PostLoopMultiversioning && Matcher::has_predicated_vectors());\n@@ -2562,1 +2625,1 @@\n-    __ setvectmask($dst$$Register, $src$$Register);\n+    __ setvectmask($dst$$Register, $src$$Register, $mask$$KRegister);\n@@ -3602,1 +3665,1 @@\n-instruct evgather(vec dst, memory mem, vec idx, rRegP tmp) %{\n+instruct evgather(vec dst, memory mem, vec idx, rRegP tmp, kReg ktmp) %{\n@@ -3605,1 +3668,1 @@\n-  effect(TEMP dst, TEMP tmp);\n+  effect(TEMP dst, TEMP tmp, TEMP ktmp);\n@@ -3615,2 +3678,1 @@\n-    KRegister ktmp = k2;\n-    __ kmovwl(k2, ExternalAddress(vector_all_bits_set()), $tmp$$Register);\n+    __ kmovwl($ktmp$$KRegister, ExternalAddress(vector_all_bits_set()), $tmp$$Register);\n@@ -3618,1 +3680,1 @@\n-    __ evgather(elem_bt, $dst$$XMMRegister, ktmp, $tmp$$Register, $idx$$XMMRegister, vlen_enc);\n+    __ evgather(elem_bt, $dst$$XMMRegister, $ktmp$$KRegister, $tmp$$Register, $idx$$XMMRegister, vlen_enc);\n@@ -3627,1 +3689,2 @@\n-instruct scatter(memory mem, vec src, vec idx, rRegP tmp) %{\n+instruct scatter(memory mem, vec src, vec idx, rRegP tmp, kReg ktmp) %{\n+  predicate(UseAVX > 2);\n@@ -3629,1 +3692,1 @@\n-  effect(TEMP tmp);\n+  effect(TEMP tmp, TEMP ktmp);\n@@ -3632,2 +3695,0 @@\n-    assert(UseAVX > 2, \"sanity\");\n-\n@@ -3640,2 +3701,1 @@\n-    KRegister ktmp = k2;\n-    __ kmovwl(k2, ExternalAddress(vector_all_bits_set()), $tmp$$Register);\n+    __ kmovwl($ktmp$$KRegister, ExternalAddress(vector_all_bits_set()), $tmp$$Register);\n@@ -3643,1 +3703,1 @@\n-    __ evscatter(elem_bt, $tmp$$Register, $idx$$XMMRegister, ktmp, $src$$XMMRegister, vlen_enc);\n+    __ evscatter(elem_bt, $tmp$$Register, $idx$$XMMRegister, $ktmp$$KRegister, $src$$XMMRegister, vlen_enc);\n@@ -5744,1 +5804,1 @@\n-instruct evminmaxFP_reg_eavx(vec dst, vec a, vec b, vec atmp, vec btmp) %{\n+instruct evminmaxFP_reg_eavx(vec dst, vec a, vec b, vec atmp, vec btmp, kReg ktmp) %{\n@@ -5749,1 +5809,1 @@\n-  effect(TEMP dst, USE a, USE b, TEMP atmp, TEMP btmp);\n+  effect(TEMP dst, USE a, USE b, TEMP atmp, TEMP btmp, TEMP ktmp);\n@@ -5758,1 +5818,0 @@\n-    KRegister ktmp = k1;\n@@ -5761,1 +5820,1 @@\n-                   ktmp, $atmp$$XMMRegister , $btmp$$XMMRegister, vlen_enc);\n+                   $ktmp$$KRegister, $atmp$$XMMRegister , $btmp$$XMMRegister, vlen_enc);\n@@ -6826,1 +6885,1 @@\n-instruct evcmpFD(vec dst, vec src1, vec src2, immI8 cond, rRegP scratch) %{\n+instruct evcmpFD(vec dst, vec src1, vec src2, immI8 cond, rRegP scratch, kReg ktmp) %{\n@@ -6830,1 +6889,1 @@\n-  effect(TEMP scratch);\n+  effect(TEMP scratch, TEMP ktmp);\n@@ -6835,1 +6894,0 @@\n-    KRegister ktmp = k2; \/\/ Use a hardcoded temp due to no k register allocation.\n@@ -6838,2 +6896,2 @@\n-      __ evcmpps(ktmp, mask, $src1$$XMMRegister, $src2$$XMMRegister, cmp, vlen_enc);\n-      __ evmovdqul($dst$$XMMRegister, ktmp, ExternalAddress(vector_all_bits_set()), false, vlen_enc, $scratch$$Register);\n+      __ evcmpps($ktmp$$KRegister, mask, $src1$$XMMRegister, $src2$$XMMRegister, cmp, vlen_enc);\n+      __ evmovdqul($dst$$XMMRegister, $ktmp$$KRegister, ExternalAddress(vector_all_bits_set()), false, vlen_enc, $scratch$$Register);\n@@ -6841,2 +6899,2 @@\n-      __ evcmppd(ktmp, mask, $src1$$XMMRegister, $src2$$XMMRegister, cmp, vlen_enc);\n-      __ evmovdquq($dst$$XMMRegister, ktmp, ExternalAddress(vector_all_bits_set()), false, vlen_enc, $scratch$$Register);\n+      __ evcmppd($ktmp$$KRegister, mask, $src1$$XMMRegister, $src2$$XMMRegister, cmp, vlen_enc);\n+      __ evmovdquq($dst$$XMMRegister, $ktmp$$KRegister, ExternalAddress(vector_all_bits_set()), false, vlen_enc, $scratch$$Register);\n@@ -6864,1 +6922,1 @@\n-instruct evcmp(vec dst, vec src1, vec src2, immI8 cond, rRegP scratch) %{\n+instruct evcmp(vec dst, vec src1, vec src2, immI8 cond, rRegP scratch, kReg ktmp) %{\n@@ -6868,1 +6926,1 @@\n-  effect(TEMP scratch);\n+  effect(TEMP scratch, TEMP ktmp);\n@@ -6875,1 +6933,0 @@\n-    KRegister ktmp = k2; \/\/ Use a hardcoded temp due to no k register allocation.\n@@ -6882,2 +6939,2 @@\n-        __ evpcmpb(ktmp, mask, $src1$$XMMRegister, $src2$$XMMRegister, cmp, vlen_enc);\n-        __ evmovdqub($dst$$XMMRegister, ktmp, ExternalAddress(vector_all_bits_set()), merge, vlen_enc, $scratch$$Register);\n+        __ evpcmpb($ktmp$$KRegister, mask, $src1$$XMMRegister, $src2$$XMMRegister, cmp, vlen_enc);\n+        __ evmovdqub($dst$$XMMRegister, $ktmp$$KRegister, ExternalAddress(vector_all_bits_set()), merge, vlen_enc, $scratch$$Register);\n@@ -6887,2 +6944,2 @@\n-        __ evpcmpw(ktmp, mask, $src1$$XMMRegister, $src2$$XMMRegister, cmp, vlen_enc);\n-        __ evmovdquw($dst$$XMMRegister, ktmp, ExternalAddress(vector_all_bits_set()), merge, vlen_enc, $scratch$$Register);\n+        __ evpcmpw($ktmp$$KRegister, mask, $src1$$XMMRegister, $src2$$XMMRegister, cmp, vlen_enc);\n+        __ evmovdquw($dst$$XMMRegister, $ktmp$$KRegister, ExternalAddress(vector_all_bits_set()), merge, vlen_enc, $scratch$$Register);\n@@ -6892,2 +6949,2 @@\n-        __ evpcmpd(ktmp, mask, $src1$$XMMRegister, $src2$$XMMRegister, cmp, vlen_enc);\n-        __ evmovdqul($dst$$XMMRegister, ktmp, ExternalAddress(vector_all_bits_set()), merge, vlen_enc, $scratch$$Register);\n+        __ evpcmpd($ktmp$$KRegister, mask, $src1$$XMMRegister, $src2$$XMMRegister, cmp, vlen_enc);\n+        __ evmovdqul($dst$$XMMRegister, $ktmp$$KRegister, ExternalAddress(vector_all_bits_set()), merge, vlen_enc, $scratch$$Register);\n@@ -6897,2 +6954,2 @@\n-        __ evpcmpq(ktmp, mask, $src1$$XMMRegister, $src2$$XMMRegister, cmp, vlen_enc);\n-        __ evmovdquq($dst$$XMMRegister, ktmp, ExternalAddress(vector_all_bits_set()), merge, vlen_enc, $scratch$$Register);\n+        __ evpcmpq($ktmp$$KRegister, mask, $src1$$XMMRegister, $src2$$XMMRegister, cmp, vlen_enc);\n+        __ evmovdquq($dst$$XMMRegister, $ktmp$$KRegister, ExternalAddress(vector_all_bits_set()), merge, vlen_enc, $scratch$$Register);\n@@ -7076,1 +7133,1 @@\n-instruct evblendvp64(vec dst, vec src1, vec src2, vec mask, rRegP scratch) %{\n+instruct evblendvp64(vec dst, vec src1, vec src2, vec mask, rRegP scratch, kReg ktmp) %{\n@@ -7080,1 +7137,1 @@\n-  effect(TEMP scratch);\n+  effect(TEMP scratch, TEMP ktmp);\n@@ -7084,3 +7141,2 @@\n-     KRegister ktmp = k2;\n-    __ evpcmp(elem_bt, ktmp, k0, $mask$$XMMRegister, ExternalAddress(vector_all_bits_set()), Assembler::eq, vlen_enc, $scratch$$Register);\n-    __ evpblend(elem_bt, $dst$$XMMRegister, ktmp, $src1$$XMMRegister, $src2$$XMMRegister, true, vlen_enc);\n+    __ evpcmp(elem_bt, $ktmp$$KRegister, k0, $mask$$XMMRegister, ExternalAddress(vector_all_bits_set()), Assembler::eq, vlen_enc, $scratch$$Register);\n+    __ evpblend(elem_bt, $dst$$XMMRegister, $ktmp$$KRegister, $src1$$XMMRegister, $src2$$XMMRegister, true, vlen_enc);\n@@ -7229,0 +7285,1 @@\n+            vector_length_in_bytes(n->in(1)) <  64 &&\n@@ -7235,1 +7292,16 @@\n-    __ vectortest(BoolTest::overflow, vlen, $src1$$XMMRegister, $src2$$XMMRegister); \n+    __ vectortest(BoolTest::overflow, vlen, $src1$$XMMRegister, $src2$$XMMRegister, xnoreg, xnoreg, knoreg);\n+    __ setb(Assembler::carrySet, $dst$$Register);\n+    __ movzbl($dst$$Register, $dst$$Register);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vptest_alltrue_evex(rRegI dst, legVec src1, legVec src2, kReg ktmp, rFlagsReg cr) %{\n+  predicate(vector_length_in_bytes(n->in(1)) == 64 &&\n+            static_cast<const VectorTestNode*>(n)->get_predicate() == BoolTest::overflow);\n+  match(Set dst (VectorTest src1 src2 ));\n+  effect(KILL cr, TEMP ktmp);\n+  format %{ \"vector_test $dst,$src1, $src2\\t! using $cr as TEMP\" %}\n+  ins_encode %{\n+    int vlen = vector_length_in_bytes(this, $src1);\n+    __ vectortest(BoolTest::overflow, vlen, $src1$$XMMRegister, $src2$$XMMRegister, xnoreg, xnoreg, $ktmp$$KRegister);\n@@ -7260,0 +7332,1 @@\n+            vector_length_in_bytes(n->in(1)) < 64  &&\n@@ -7266,1 +7339,16 @@\n-    __ vectortest(BoolTest::ne, vlen, $src1$$XMMRegister, $src2$$XMMRegister);\n+    __ vectortest(BoolTest::ne, vlen, $src1$$XMMRegister, $src2$$XMMRegister, xnoreg, xnoreg, knoreg);\n+    __ setb(Assembler::notZero, $dst$$Register);\n+    __ movzbl($dst$$Register, $dst$$Register);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vptest_anytrue_evex(rRegI dst, legVec src1, legVec src2, kReg ktmp, rFlagsReg cr) %{\n+  predicate(vector_length_in_bytes(n->in(1)) == 64 &&\n+            static_cast<const VectorTestNode*>(n)->get_predicate() == BoolTest::ne);\n+  match(Set dst (VectorTest src1 src2 ));\n+  effect(KILL cr, TEMP ktmp);\n+  format %{ \"vector_test_any_true $dst,$src1,$src2\\t! using $cr as TEMP\" %}\n+  ins_encode %{\n+    int vlen = vector_length_in_bytes(this, $src1);\n+    __ vectortest(BoolTest::ne, vlen, $src1$$XMMRegister, $src2$$XMMRegister, xnoreg, xnoreg, $ktmp$$KRegister);\n@@ -7289,0 +7377,13 @@\n+            vector_length_in_bytes(n->in(1)->in(1)) <  64 &&\n+            static_cast<const VectorTestNode*>(n->in(1))->get_predicate() == BoolTest::ne);\n+  match(Set cr (CmpI (VectorTest src1 src2) zero));\n+  format %{ \"cmp_vector_test_any_true $src1,$src2\\t!\" %}\n+  ins_encode %{\n+    int vlen = vector_length_in_bytes(this, $src1);\n+    __ vectortest(BoolTest::ne, vlen, $src1$$XMMRegister, $src2$$XMMRegister, xnoreg, xnoreg, knoreg);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct cmpvptest_anytrue_evex(rFlagsReg cr, legVec src1, legVec src2, immI_0 zero, kReg ktmp) %{\n+  predicate(vector_length_in_bytes(n->in(1)->in(1)) == 64 &&\n@@ -7291,0 +7392,1 @@\n+  effect(TEMP ktmp);\n@@ -7294,1 +7396,1 @@\n-    __ vectortest(BoolTest::ne, vlen, $src1$$XMMRegister, $src2$$XMMRegister);\n+    __ vectortest(BoolTest::ne, vlen, $src1$$XMMRegister, $src2$$XMMRegister, xnoreg, xnoreg, $ktmp$$KRegister);\n@@ -7947,2 +8049,1 @@\n-\n-instruct vmasked_load64(vec dst, memory mem, rRegL mask) %{\n+instruct vmasked_load64(vec dst, memory mem, kReg mask) %{\n@@ -7954,2 +8055,1 @@\n-    __ kmovql(k2, $mask$$Register);\n-    __ evmovdqu(elmType, k2, $dst$$XMMRegister, $mem$$Address, vector_len);\n+    __ evmovdqu(elmType, $mask$$KRegister, $dst$$XMMRegister, $mem$$Address, vector_len);\n@@ -7960,1 +8060,1 @@\n-instruct vmask_gen(rRegL dst, rRegL len, rRegL tempLen) %{\n+instruct vmask_gen(kReg dst, rRegL len, rRegL temp) %{\n@@ -7962,2 +8062,2 @@\n-  effect(TEMP_DEF dst, TEMP tempLen);\n-  format %{ \"vector_mask_gen $len \\t! vector mask generator\" %}\n+  effect(TEMP temp);\n+  format %{ \"vector_mask_gen32 $dst, $len \\t! vector mask generator\" %}\n@@ -7965,1 +8065,1 @@\n-    __ genmask($dst$$Register, $len$$Register, $tempLen$$Register);\n+    __ genmask($dst$$KRegister, $len$$Register, $temp$$Register);\n@@ -7970,1 +8070,1 @@\n-instruct vmask_gen_imm(rRegL dst, immL len) %{\n+instruct vmask_gen_imm(kReg dst, immL len, rRegL temp) %{\n@@ -7973,0 +8073,1 @@\n+  effect(TEMP temp);\n@@ -7974,1 +8075,2 @@\n-    __ mov64($dst$$Register, (0xFFFFFFFFFFFFFFFFUL >> (64 -$len$$constant)));\n+    __ mov64($temp$$Register, (0xFFFFFFFFFFFFFFFFUL >> (64 -$len$$constant)));\n+    __ kmovql($dst$$KRegister, $temp$$Register);\n@@ -7979,1 +8081,1 @@\n-instruct vmasked_store64(memory mem, vec src, rRegL mask) %{\n+instruct vmasked_store64(memory mem, vec src, kReg mask) %{\n@@ -7986,2 +8088,1 @@\n-    __ kmovql(k2, $mask$$Register);\n-    __ evmovdqu(elmType, k2, $mem$$Address, $src$$XMMRegister, vector_len);\n+    __ evmovdqu(elmType, $mask$$KRegister, $mem$$Address, $src$$XMMRegister, vector_len);\n","filename":"src\/hotspot\/cpu\/x86\/x86.ad","additions":163,"deletions":62,"binary":false,"changes":225,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n-\/\/ Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n+\/\/ Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -263,0 +263,12 @@\n+void reg_mask_init() {\n+  if (Matcher::has_predicated_vectors()) {\n+    \/\/ Post-loop multi-versioning expects mask to be present in K1 register, till the time\n+    \/\/ its fixed, RA should not be allocting K1 register, this shall prevent any accidental\n+    \/\/ curruption of value held in K1 register.\n+    if (PostLoopMultiversioning) {\n+      const_cast<RegMask*>(&_VECTMASK_REG_mask)->Remove(OptoReg::as_OptoReg(k1->as_VMReg()));\n+      const_cast<RegMask*>(&_VECTMASK_REG_mask)->Remove(OptoReg::as_OptoReg(k1->as_VMReg()->next()));\n+    }\n+  }\n+}\n+\n@@ -734,1 +746,1 @@\n-enum RC { rc_bad, rc_int, rc_float, rc_xmm, rc_stack };\n+enum RC { rc_bad, rc_int, rc_kreg, rc_float, rc_xmm, rc_stack };\n@@ -1053,1 +1065,1 @@\n-  if (bottom_type()->isa_vect() != NULL) {\n+  if (bottom_type()->isa_vect() != NULL && bottom_type()->isa_vectmask() == NULL) {\n@@ -1106,1 +1118,1 @@\n-  if( dst_first_rc == rc_int && src_first_rc == rc_stack )\n+  if( src_first_rc == rc_stack && dst_first_rc == rc_int )\n@@ -1195,1 +1207,1 @@\n-    return impl_x_helper(cbuf,do_size,false,ra_->reg2offset(dst_first),src_first, src_second, size, st);\n+    return impl_x_helper(cbuf,do_size,false,ra_->reg2offset(dst_first), src_first, src_second, size, st);\n@@ -1199,1 +1211,1 @@\n-  if( dst_first_rc == rc_xmm && src_first_rc == rc_stack ) {\n+  if( src_first_rc == rc_stack && dst_first_rc == rc_xmm ) {\n@@ -1204,1 +1216,1 @@\n-  if( dst_first_rc == rc_xmm && src_first_rc == rc_float ) {\n+  if( src_first_rc == rc_float && dst_first_rc == rc_xmm ) {\n@@ -1260,0 +1272,36 @@\n+  \/\/ AVX-512 opmask specific spilling.\n+  if (src_first_rc == rc_stack && dst_first_rc == rc_kreg) {\n+    assert((src_first & 1) == 0 && src_first + 1 == src_second, \"invalid register pair\");\n+    assert((dst_first & 1) == 0 && dst_first + 1 == dst_second, \"invalid register pair\");\n+    MacroAssembler _masm(cbuf);\n+    int offset = ra_->reg2offset(src_first);\n+    __ kmov(as_KRegister(Matcher::_regEncode[dst_first]), Address(rsp, offset));\n+    return 0;\n+  }\n+\n+  if (src_first_rc == rc_kreg && dst_first_rc == rc_stack) {\n+    assert((src_first & 1) == 0 && src_first + 1 == src_second, \"invalid register pair\");\n+    assert((dst_first & 1) == 0 && dst_first + 1 == dst_second, \"invalid register pair\");\n+    MacroAssembler _masm(cbuf);\n+    int offset = ra_->reg2offset(dst_first);\n+    __ kmov(Address(rsp, offset), as_KRegister(Matcher::_regEncode[src_first]));\n+    return 0;\n+  }\n+\n+  if (src_first_rc == rc_kreg && dst_first_rc == rc_int) {\n+    Unimplemented();\n+    return 0;\n+  }\n+\n+  if (src_first_rc == rc_int && dst_first_rc == rc_kreg) {\n+    Unimplemented();\n+    return 0;\n+  }\n+\n+  if (src_first_rc == rc_kreg && dst_first_rc == rc_kreg) {\n+    assert((src_first & 1) == 0 && src_first + 1 == src_second, \"invalid register pair\");\n+    assert((dst_first & 1) == 0 && dst_first + 1 == dst_second, \"invalid register pair\");\n+    MacroAssembler _masm(cbuf);\n+    __ kmov(as_KRegister(Matcher::_regEncode[dst_first]), as_KRegister(Matcher::_regEncode[src_first]));\n+    return 0;\n+  }\n@@ -3577,0 +3625,66 @@\n+operand kReg()\n+%{\n+  constraint(ALLOC_IN_RC(vectmask_reg));\n+  match(RegVectMask);\n+  format %{%}\n+  interface(REG_INTER);\n+%}\n+\n+operand kReg_K1()\n+%{\n+  constraint(ALLOC_IN_RC(vectmask_reg_K1));\n+  match(RegVectMask);\n+  format %{%}\n+  interface(REG_INTER);\n+%}\n+\n+operand kReg_K2()\n+%{\n+  constraint(ALLOC_IN_RC(vectmask_reg_K2));\n+  match(RegVectMask);\n+  format %{%}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Special Registers\n+operand kReg_K3()\n+%{\n+  constraint(ALLOC_IN_RC(vectmask_reg_K3));\n+  match(RegVectMask);\n+  format %{%}\n+  interface(REG_INTER);\n+%}\n+\n+operand kReg_K4()\n+%{\n+  constraint(ALLOC_IN_RC(vectmask_reg_K4));\n+  match(RegVectMask);\n+  format %{%}\n+  interface(REG_INTER);\n+%}\n+\n+operand kReg_K5()\n+%{\n+  constraint(ALLOC_IN_RC(vectmask_reg_K5));\n+  match(RegVectMask);\n+  format %{%}\n+  interface(REG_INTER);\n+%}\n+\n+operand kReg_K6()\n+%{\n+  constraint(ALLOC_IN_RC(vectmask_reg_K6));\n+  match(RegVectMask);\n+  format %{%}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Special Registers\n+operand kReg_K7()\n+%{\n+  constraint(ALLOC_IN_RC(vectmask_reg_K7));\n+  match(RegVectMask);\n+  format %{%}\n+  interface(REG_INTER);\n+%}\n+\n@@ -11414,1 +11528,3 @@\n-  predicate(!((ClearArrayNode*)n)->is_large() && (UseAVX <= 2 || !VM_Version::supports_avx512vlbw() || !n->in(2)->bottom_type()->is_int()->is_con()));\n+  predicate(!((ClearArrayNode*)n)->is_large() &&\n+              ((UseAVX <= 2 || !VM_Version::supports_avx512vlbw()) &&\n+                !n->in(2)->bottom_type()->is_int()->is_con()));\n@@ -11467,1 +11583,62 @@\n-                 $tmp$$XMMRegister, false);\n+                 $tmp$$XMMRegister, false, knoreg);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct rep_stos_evex(eCXRegI cnt, eDIRegP base, regD tmp, kReg ktmp, eAXRegI zero, Universe dummy, eFlagsReg cr) %{\n+  predicate(!((ClearArrayNode*)n)->is_large() &&\n+               UseAVX > 2 && !VM_Version::supports_avx512vlbw() &&\n+               !n->in(2)->bottom_type()->is_int()->is_con());\n+  match(Set dummy (ClearArray cnt base));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, KILL zero, KILL cr);\n+\n+  format %{ $$template\n+    $$emit$$\"XOR    EAX,EAX\\t# ClearArray:\\n\\t\"\n+    $$emit$$\"CMP    InitArrayShortSize,rcx\\n\\t\"\n+    $$emit$$\"JG     LARGE\\n\\t\"\n+    $$emit$$\"SHL    ECX, 1\\n\\t\"\n+    $$emit$$\"DEC    ECX\\n\\t\"\n+    $$emit$$\"JS     DONE\\t# Zero length\\n\\t\"\n+    $$emit$$\"MOV    EAX,(EDI,ECX,4)\\t# LOOP\\n\\t\"\n+    $$emit$$\"DEC    ECX\\n\\t\"\n+    $$emit$$\"JGE    LOOP\\n\\t\"\n+    $$emit$$\"JMP    DONE\\n\\t\"\n+    $$emit$$\"# LARGE:\\n\\t\"\n+    if (UseFastStosb) {\n+       $$emit$$\"SHL    ECX,3\\t# Convert doublewords to bytes\\n\\t\"\n+       $$emit$$\"REP STOSB\\t# store EAX into [EDI++] while ECX--\\n\\t\"\n+    } else if (UseXMMForObjInit) {\n+       $$emit$$\"MOV     RDI,RAX\\n\\t\"\n+       $$emit$$\"VPXOR    YMM0,YMM0,YMM0\\n\\t\"\n+       $$emit$$\"JMPQ    L_zero_64_bytes\\n\\t\"\n+       $$emit$$\"# L_loop:\\t# 64-byte LOOP\\n\\t\"\n+       $$emit$$\"VMOVDQU YMM0,(RAX)\\n\\t\"\n+       $$emit$$\"VMOVDQU YMM0,0x20(RAX)\\n\\t\"\n+       $$emit$$\"ADD     0x40,RAX\\n\\t\"\n+       $$emit$$\"# L_zero_64_bytes:\\n\\t\"\n+       $$emit$$\"SUB     0x8,RCX\\n\\t\"\n+       $$emit$$\"JGE     L_loop\\n\\t\"\n+       $$emit$$\"ADD     0x4,RCX\\n\\t\"\n+       $$emit$$\"JL      L_tail\\n\\t\"\n+       $$emit$$\"VMOVDQU YMM0,(RAX)\\n\\t\"\n+       $$emit$$\"ADD     0x20,RAX\\n\\t\"\n+       $$emit$$\"SUB     0x4,RCX\\n\\t\"\n+       $$emit$$\"# L_tail:\\t# Clearing tail bytes\\n\\t\"\n+       $$emit$$\"ADD     0x4,RCX\\n\\t\"\n+       $$emit$$\"JLE     L_end\\n\\t\"\n+       $$emit$$\"DEC     RCX\\n\\t\"\n+       $$emit$$\"# L_sloop:\\t# 8-byte short loop\\n\\t\"\n+       $$emit$$\"VMOVQ   XMM0,(RAX)\\n\\t\"\n+       $$emit$$\"ADD     0x8,RAX\\n\\t\"\n+       $$emit$$\"DEC     RCX\\n\\t\"\n+       $$emit$$\"JGE     L_sloop\\n\\t\"\n+       $$emit$$\"# L_end:\\n\\t\"\n+    } else {\n+       $$emit$$\"SHL    ECX,1\\t# Convert doublewords to words\\n\\t\"\n+       $$emit$$\"REP STOS\\t# store EAX into [EDI++] while ECX--\\n\\t\"\n+    }\n+    $$emit$$\"# DONE\"\n+  %}\n+  ins_encode %{\n+    __ clear_mem($base$$Register, $cnt$$Register, $zero$$Register,\n+                 $tmp$$XMMRegister, false, $ktmp$$KRegister);\n@@ -11473,1 +11650,1 @@\n-  predicate(((ClearArrayNode*)n)->is_large());\n+  predicate(UseAVX <= 2 && ((ClearArrayNode*)n)->is_large());\n@@ -11516,1 +11693,1 @@\n-                 $tmp$$XMMRegister, true);\n+                 $tmp$$XMMRegister, true, knoreg);\n@@ -11521,1 +11698,49 @@\n-instruct rep_stos_im(immI cnt, eRegP base, regD tmp, rRegI zero, Universe dummy, eFlagsReg cr)\n+instruct rep_stos_large_evex(eCXRegI cnt, eDIRegP base, regD tmp, kReg ktmp, eAXRegI zero, Universe dummy, eFlagsReg cr) %{\n+  predicate(UseAVX > 2 && ((ClearArrayNode*)n)->is_large());\n+  match(Set dummy (ClearArray cnt base));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, KILL zero, KILL cr);\n+  format %{ $$template\n+    if (UseFastStosb) {\n+       $$emit$$\"XOR    EAX,EAX\\t# ClearArray:\\n\\t\"\n+       $$emit$$\"SHL    ECX,3\\t# Convert doublewords to bytes\\n\\t\"\n+       $$emit$$\"REP STOSB\\t# store EAX into [EDI++] while ECX--\\n\\t\"\n+    } else if (UseXMMForObjInit) {\n+       $$emit$$\"MOV     RDI,RAX\\t# ClearArray:\\n\\t\"\n+       $$emit$$\"VPXOR   YMM0,YMM0,YMM0\\n\\t\"\n+       $$emit$$\"JMPQ    L_zero_64_bytes\\n\\t\"\n+       $$emit$$\"# L_loop:\\t# 64-byte LOOP\\n\\t\"\n+       $$emit$$\"VMOVDQU YMM0,(RAX)\\n\\t\"\n+       $$emit$$\"VMOVDQU YMM0,0x20(RAX)\\n\\t\"\n+       $$emit$$\"ADD     0x40,RAX\\n\\t\"\n+       $$emit$$\"# L_zero_64_bytes:\\n\\t\"\n+       $$emit$$\"SUB     0x8,RCX\\n\\t\"\n+       $$emit$$\"JGE     L_loop\\n\\t\"\n+       $$emit$$\"ADD     0x4,RCX\\n\\t\"\n+       $$emit$$\"JL      L_tail\\n\\t\"\n+       $$emit$$\"VMOVDQU YMM0,(RAX)\\n\\t\"\n+       $$emit$$\"ADD     0x20,RAX\\n\\t\"\n+       $$emit$$\"SUB     0x4,RCX\\n\\t\"\n+       $$emit$$\"# L_tail:\\t# Clearing tail bytes\\n\\t\"\n+       $$emit$$\"ADD     0x4,RCX\\n\\t\"\n+       $$emit$$\"JLE     L_end\\n\\t\"\n+       $$emit$$\"DEC     RCX\\n\\t\"\n+       $$emit$$\"# L_sloop:\\t# 8-byte short loop\\n\\t\"\n+       $$emit$$\"VMOVQ   XMM0,(RAX)\\n\\t\"\n+       $$emit$$\"ADD     0x8,RAX\\n\\t\"\n+       $$emit$$\"DEC     RCX\\n\\t\"\n+       $$emit$$\"JGE     L_sloop\\n\\t\"\n+       $$emit$$\"# L_end:\\n\\t\"\n+    } else {\n+       $$emit$$\"XOR    EAX,EAX\\t# ClearArray:\\n\\t\"\n+       $$emit$$\"SHL    ECX,1\\t# Convert doublewords to words\\n\\t\"\n+       $$emit$$\"REP STOS\\t# store EAX into [EDI++] while ECX--\\n\\t\"\n+    }\n+    $$emit$$\"# DONE\"\n+  %}\n+  ins_encode %{\n+    __ clear_mem($base$$Register, $cnt$$Register, $zero$$Register,\n+                 $tmp$$XMMRegister, true, $ktmp$$KRegister);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+instruct rep_stos_im(immI cnt, kReg ktmp, eRegP base, regD tmp, rRegI zero, Universe dummy, eFlagsReg cr)\n@@ -11525,1 +11750,1 @@\n-  effect(TEMP tmp, TEMP zero, KILL cr);\n+  effect(TEMP tmp, TEMP zero, TEMP ktmp, KILL cr);\n@@ -11528,1 +11753,1 @@\n-   __ clear_mem($base$$Register, $cnt$$constant, $zero$$Register, $tmp$$XMMRegister);\n+   __ clear_mem($base$$Register, $cnt$$constant, $zero$$Register, $tmp$$XMMRegister, $ktmp$$KRegister);\n@@ -11535,1 +11760,1 @@\n-  predicate(((StrCompNode*)n)->encoding() == StrIntrinsicNode::LL);\n+  predicate(UseAVX <= 2 && ((StrCompNode*)n)->encoding() == StrIntrinsicNode::LL);\n@@ -11543,1 +11768,16 @@\n-                      $tmp1$$XMMRegister, StrIntrinsicNode::LL);\n+                      $tmp1$$XMMRegister, StrIntrinsicNode::LL, knoreg);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct string_compareL_evex(eDIRegP str1, eCXRegI cnt1, eSIRegP str2, eDXRegI cnt2,\n+                              eAXRegI result, regD tmp1, kReg ktmp, eFlagsReg cr) %{\n+  predicate(UseAVX > 2 && ((StrCompNode*)n)->encoding() == StrIntrinsicNode::LL);\n+  match(Set result (StrComp (Binary str1 cnt1) (Binary str2 cnt2)));\n+  effect(TEMP tmp1, TEMP ktmp, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL cr);\n+\n+  format %{ \"String Compare byte[] $str1,$cnt1,$str2,$cnt2 -> $result   \/\/ KILL $tmp1\" %}\n+  ins_encode %{\n+    __ string_compare($str1$$Register, $str2$$Register,\n+                      $cnt1$$Register, $cnt2$$Register, $result$$Register,\n+                      $tmp1$$XMMRegister, StrIntrinsicNode::LL, $ktmp$$KRegister);\n@@ -11550,1 +11790,1 @@\n-  predicate(((StrCompNode*)n)->encoding() == StrIntrinsicNode::UU);\n+  predicate(UseAVX <= 2 && ((StrCompNode*)n)->encoding() == StrIntrinsicNode::UU);\n@@ -11558,1 +11798,16 @@\n-                      $tmp1$$XMMRegister, StrIntrinsicNode::UU);\n+                      $tmp1$$XMMRegister, StrIntrinsicNode::UU, knoreg);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct string_compareU_evex(eDIRegP str1, eCXRegI cnt1, eSIRegP str2, eDXRegI cnt2,\n+                              eAXRegI result, regD tmp1, kReg ktmp, eFlagsReg cr) %{\n+  predicate(UseAVX > 2 && ((StrCompNode*)n)->encoding() == StrIntrinsicNode::UU);\n+  match(Set result (StrComp (Binary str1 cnt1) (Binary str2 cnt2)));\n+  effect(TEMP tmp1, TEMP ktmp, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL cr);\n+\n+  format %{ \"String Compare char[] $str1,$cnt1,$str2,$cnt2 -> $result   \/\/ KILL $tmp1\" %}\n+  ins_encode %{\n+    __ string_compare($str1$$Register, $str2$$Register,\n+                      $cnt1$$Register, $cnt2$$Register, $result$$Register,\n+                      $tmp1$$XMMRegister, StrIntrinsicNode::UU, $ktmp$$KRegister);\n@@ -11565,1 +11820,1 @@\n-  predicate(((StrCompNode*)n)->encoding() == StrIntrinsicNode::LU);\n+  predicate(UseAVX <= 2 && ((StrCompNode*)n)->encoding() == StrIntrinsicNode::LU);\n@@ -11573,1 +11828,16 @@\n-                      $tmp1$$XMMRegister, StrIntrinsicNode::LU);\n+                      $tmp1$$XMMRegister, StrIntrinsicNode::LU, knoreg);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct string_compareLU_evex(eDIRegP str1, eCXRegI cnt1, eSIRegP str2, eDXRegI cnt2,\n+                               eAXRegI result, regD tmp1, kReg ktmp, eFlagsReg cr) %{\n+  predicate(UseAVX > 2 && ((StrCompNode*)n)->encoding() == StrIntrinsicNode::LU);\n+  match(Set result (StrComp (Binary str1 cnt1) (Binary str2 cnt2)));\n+  effect(TEMP tmp1, TEMP ktmp, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL cr);\n+\n+  format %{ \"String Compare byte[] $str1,$cnt1,$str2,$cnt2 -> $result   \/\/ KILL $tmp1\" %}\n+  ins_encode %{\n+    __ string_compare($str1$$Register, $str2$$Register,\n+                      $cnt1$$Register, $cnt2$$Register, $result$$Register,\n+                      $tmp1$$XMMRegister, StrIntrinsicNode::LU, $ktmp$$KRegister);\n@@ -11580,1 +11850,1 @@\n-  predicate(((StrCompNode*)n)->encoding() == StrIntrinsicNode::UL);\n+  predicate(UseAVX <= 2 && ((StrCompNode*)n)->encoding() == StrIntrinsicNode::UL);\n@@ -11588,1 +11858,16 @@\n-                      $tmp1$$XMMRegister, StrIntrinsicNode::UL);\n+                      $tmp1$$XMMRegister, StrIntrinsicNode::UL, knoreg);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct string_compareUL_evex(eSIRegP str1, eDXRegI cnt1, eDIRegP str2, eCXRegI cnt2,\n+                               eAXRegI result, regD tmp1, kReg ktmp, eFlagsReg cr) %{\n+  predicate(UseAVX > 2 && ((StrCompNode*)n)->encoding() == StrIntrinsicNode::UL);\n+  match(Set result (StrComp (Binary str1 cnt1) (Binary str2 cnt2)));\n+  effect(TEMP tmp1, TEMP ktmp, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL cr);\n+\n+  format %{ \"String Compare byte[] $str1,$cnt1,$str2,$cnt2 -> $result   \/\/ KILL $tmp1\" %}\n+  ins_encode %{\n+    __ string_compare($str2$$Register, $str1$$Register,\n+                      $cnt2$$Register, $cnt1$$Register, $result$$Register,\n+                      $tmp1$$XMMRegister, StrIntrinsicNode::UL, $ktmp$$KRegister);\n@@ -11596,0 +11881,1 @@\n+  predicate(UseAVX <= 2);\n@@ -11603,1 +11889,17 @@\n-                     $tmp1$$XMMRegister, $tmp2$$XMMRegister, false \/* char *\/);\n+                     $tmp1$$XMMRegister, $tmp2$$XMMRegister, false \/* char *\/, knoreg);\n+  %}\n+\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct string_equals_evex(eDIRegP str1, eSIRegP str2, eCXRegI cnt, eAXRegI result,\n+                            regD tmp1, regD tmp2, kReg ktmp, eBXRegI tmp3, eFlagsReg cr) %{\n+  predicate(UseAVX > 2);\n+  match(Set result (StrEquals (Binary str1 str2) cnt));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP ktmp, USE_KILL str1, USE_KILL str2, USE_KILL cnt, KILL tmp3, KILL cr);\n+\n+  format %{ \"String Equals $str1,$str2,$cnt -> $result    \/\/ KILL $tmp1, $tmp2, $tmp3\" %}\n+  ins_encode %{\n+    __ arrays_equals(false, $str1$$Register, $str2$$Register,\n+                     $cnt$$Register, $result$$Register, $tmp3$$Register,\n+                     $tmp1$$XMMRegister, $tmp2$$XMMRegister, false \/* char *\/, $ktmp$$KRegister);\n@@ -11609,0 +11911,1 @@\n+\n@@ -11772,1 +12075,1 @@\n-  predicate(((AryEqNode*)n)->encoding() == StrIntrinsicNode::LL);\n+  predicate(UseAVX <= 2 && ((AryEqNode*)n)->encoding() == StrIntrinsicNode::LL);\n@@ -11781,1 +12084,18 @@\n-                     $tmp1$$XMMRegister, $tmp2$$XMMRegister, false \/* char *\/);\n+                     $tmp1$$XMMRegister, $tmp2$$XMMRegister, false \/* char *\/, knoreg);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct array_equalsB_evex(eDIRegP ary1, eSIRegP ary2, eAXRegI result,\n+                       regD tmp1, regD tmp2, kReg ktmp, eCXRegI tmp3, eBXRegI tmp4, eFlagsReg cr)\n+%{\n+  predicate(UseAVX > 2 && ((AryEqNode*)n)->encoding() == StrIntrinsicNode::LL);\n+  match(Set result (AryEq ary1 ary2));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP ktmp, USE_KILL ary1, USE_KILL ary2, KILL tmp3, KILL tmp4, KILL cr);\n+  \/\/ins_cost(300);\n+\n+  format %{ \"Array Equals byte[] $ary1,$ary2 -> $result   \/\/ KILL $tmp1, $tmp2, $tmp3, $tmp4\" %}\n+  ins_encode %{\n+    __ arrays_equals(true, $ary1$$Register, $ary2$$Register,\n+                     $tmp3$$Register, $result$$Register, $tmp4$$Register,\n+                     $tmp1$$XMMRegister, $tmp2$$XMMRegister, false \/* char *\/, $ktmp$$KRegister);\n@@ -11789,1 +12109,1 @@\n-  predicate(((AryEqNode*)n)->encoding() == StrIntrinsicNode::UU);\n+  predicate(UseAVX <= 2 && ((AryEqNode*)n)->encoding() == StrIntrinsicNode::UU);\n@@ -11798,1 +12118,18 @@\n-                     $tmp1$$XMMRegister, $tmp2$$XMMRegister, true \/* char *\/);\n+                     $tmp1$$XMMRegister, $tmp2$$XMMRegister, true \/* char *\/, knoreg);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct array_equalsC_evex(eDIRegP ary1, eSIRegP ary2, eAXRegI result,\n+                            regD tmp1, regD tmp2, kReg ktmp, eCXRegI tmp3, eBXRegI tmp4, eFlagsReg cr)\n+%{\n+  predicate(UseAVX > 2 && ((AryEqNode*)n)->encoding() == StrIntrinsicNode::UU);\n+  match(Set result (AryEq ary1 ary2));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP ktmp, USE_KILL ary1, USE_KILL ary2, KILL tmp3, KILL tmp4, KILL cr);\n+  \/\/ins_cost(300);\n+\n+  format %{ \"Array Equals char[] $ary1,$ary2 -> $result   \/\/ KILL $tmp1, $tmp2, $tmp3, $tmp4\" %}\n+  ins_encode %{\n+    __ arrays_equals(true, $ary1$$Register, $ary2$$Register,\n+                     $tmp3$$Register, $result$$Register, $tmp4$$Register,\n+                     $tmp1$$XMMRegister, $tmp2$$XMMRegister, true \/* char *\/, $ktmp$$KRegister);\n@@ -11806,0 +12143,1 @@\n+  predicate(UseAVX <= 2);\n@@ -11813,1 +12151,17 @@\n-                     $tmp1$$XMMRegister, $tmp2$$XMMRegister);\n+                     $tmp1$$XMMRegister, $tmp2$$XMMRegister, knoreg, knoreg);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct has_negatives_evex(eSIRegP ary1, eCXRegI len, eAXRegI result,\n+                           regD tmp1, regD tmp2, kReg ktmp1, kReg ktmp2, eBXRegI tmp3, eFlagsReg cr)\n+%{\n+  predicate(UseAVX > 2);\n+  match(Set result (HasNegatives ary1 len));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP ktmp1, TEMP ktmp2, USE_KILL ary1, USE_KILL len, KILL tmp3, KILL cr);\n+\n+  format %{ \"has negatives byte[] $ary1,$len -> $result   \/\/ KILL $tmp1, $tmp2, $tmp3\" %}\n+  ins_encode %{\n+    __ has_negatives($ary1$$Register, $len$$Register,\n+                     $result$$Register, $tmp3$$Register,\n+                     $tmp1$$XMMRegister, $tmp2$$XMMRegister, $ktmp1$$KRegister, $ktmp2$$KRegister);\n@@ -11818,0 +12172,1 @@\n+\n@@ -11819,2 +12174,3 @@\n-instruct string_compress(eSIRegP src, eDIRegP dst, eDXRegI len, regD tmp1, regD tmp2, regD tmp3, regD tmp4,\n-                         eCXRegI tmp5, eAXRegI result, eFlagsReg cr) %{\n+instruct string_compress(eSIRegP src, eDIRegP dst, eDXRegI len, regD tmp1, regD tmp2,\n+                         regD tmp3, regD tmp4, eCXRegI tmp5, eAXRegI result, eFlagsReg cr) %{\n+  predicate(UseAVX <= 2);\n@@ -11828,1 +12184,18 @@\n-                           $tmp4$$XMMRegister, $tmp5$$Register, $result$$Register);\n+                           $tmp4$$XMMRegister, $tmp5$$Register, $result$$Register,\n+                           knoreg, knoreg);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct string_compress_evex(eSIRegP src, eDIRegP dst, eDXRegI len, regD tmp1, regD tmp2,\n+                              regD tmp3, regD tmp4, kReg ktmp1, kReg ktmp2, eCXRegI tmp5, eAXRegI result, eFlagsReg cr) %{\n+  predicate(UseAVX > 2);\n+  match(Set result (StrCompressedCopy src (Binary dst len)));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, TEMP tmp4, TEMP ktmp1, TEMP ktmp2, USE_KILL src, USE_KILL dst, USE_KILL len, KILL tmp5, KILL cr);\n+\n+  format %{ \"String Compress $src,$dst -> $result    \/\/ KILL RAX, RCX, RDX\" %}\n+  ins_encode %{\n+    __ char_array_compress($src$$Register, $dst$$Register, $len$$Register,\n+                           $tmp1$$XMMRegister, $tmp2$$XMMRegister, $tmp3$$XMMRegister,\n+                           $tmp4$$XMMRegister, $tmp5$$Register, $result$$Register,\n+                           $ktmp1$$KRegister, $ktmp2$$KRegister);\n@@ -11836,0 +12209,1 @@\n+  predicate(UseAVX <= 2);\n@@ -11842,1 +12216,15 @@\n-                          $tmp1$$XMMRegister, $tmp2$$Register);\n+                          $tmp1$$XMMRegister, $tmp2$$Register, knoreg);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct string_inflate_evex(Universe dummy, eSIRegP src, eDIRegP dst, eDXRegI len,\n+                             regD tmp1, kReg ktmp, eCXRegI tmp2, eFlagsReg cr) %{\n+  predicate(UseAVX > 2);\n+  match(Set dummy (StrInflatedCopy src (Binary dst len)));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP ktmp, USE_KILL src, USE_KILL dst, USE_KILL len, KILL cr);\n+\n+  format %{ \"String Inflate $src,$dst    \/\/ KILL $tmp1, $tmp2\" %}\n+  ins_encode %{\n+    __ byte_array_inflate($src$$Register, $dst$$Register, $len$$Register,\n+                          $tmp1$$XMMRegister, $tmp2$$Register, $ktmp$$KRegister);\n@@ -12270,2 +12658,4 @@\n-instruct jmpLoopEnd_and_restoreMask(cmpOp cop, eFlagsReg cr, label labl) %{\n-  predicate(n->has_vector_mask_set());\n+\/\/ Bounded mask operand used in following patten is needed for\n+\/\/ post-loop multiversioning.\n+instruct jmpLoopEnd_and_restoreMask(cmpOp cop, kReg_K1 ktmp, eFlagsReg cr, label labl) %{\n+  predicate(PostLoopMultiversioning && n->has_vector_mask_set());\n@@ -12273,1 +12663,1 @@\n-  effect(USE labl);\n+  effect(USE labl, TEMP ktmp);\n@@ -12282,1 +12672,1 @@\n-    __ restorevectmask();\n+    __ restorevectmask($ktmp$$KRegister);\n@@ -12288,2 +12678,4 @@\n-instruct jmpLoopEndU_and_restoreMask(cmpOpU cop, eFlagsRegU cmp, label labl) %{\n-  predicate(n->has_vector_mask_set());\n+\/\/ Bounded mask operand used in following patten is needed for\n+\/\/ post-loop multiversioning.\n+instruct jmpLoopEndU_and_restoreMask(cmpOpU cop, kReg_K1 ktmp, eFlagsRegU cmp, label labl) %{\n+  predicate(PostLoopMultiversioning && n->has_vector_mask_set());\n@@ -12291,1 +12683,1 @@\n-  effect(USE labl);\n+  effect(USE labl, TEMP ktmp);\n@@ -12300,1 +12692,1 @@\n-    __ restorevectmask();\n+    __ restorevectmask($ktmp$$KRegister);\n@@ -12305,2 +12697,4 @@\n-instruct jmpLoopEndUCF_and_restoreMask(cmpOpUCF cop, eFlagsRegUCF cmp, label labl) %{\n-  predicate(n->has_vector_mask_set());\n+\/\/ Bounded mask operand used in following patten is needed for\n+\/\/ post-loop multiversioning.\n+instruct jmpLoopEndUCF_and_restoreMask(cmpOpUCF cop, kReg_K1 ktmp, eFlagsRegUCF cmp, label labl) %{\n+  predicate(PostLoopMultiversioning && n->has_vector_mask_set());\n@@ -12308,1 +12702,1 @@\n-  effect(USE labl);\n+  effect(USE labl, TEMP ktmp);\n@@ -12317,1 +12711,1 @@\n-    __ restorevectmask();\n+    __ restorevectmask($ktmp$$KRegister);\n","filename":"src\/hotspot\/cpu\/x86\/x86_32.ad","additions":438,"deletions":44,"binary":false,"changes":482,"status":"modified"},{"patch":"@@ -427,0 +427,10 @@\n+\n+  if (Matcher::has_predicated_vectors()) {\n+    \/\/ Post-loop multi-versioning expects mask to be present in K1 register, till the time\n+    \/\/ its fixed, RA should not be allocting K1 register, this shall prevent any accidental\n+    \/\/ curruption of value held in K1 register.\n+    if (PostLoopMultiversioning) {\n+      const_cast<RegMask*>(&_VECTMASK_REG_mask)->Remove(OptoReg::as_OptoReg(k1->as_VMReg()));\n+      const_cast<RegMask*>(&_VECTMASK_REG_mask)->Remove(OptoReg::as_OptoReg(k1->as_VMReg()->next()));\n+    }\n+  }\n@@ -1017,0 +1027,1 @@\n+  rc_kreg,\n@@ -1031,0 +1042,2 @@\n+  if (r->is_KRegister()) return rc_kreg;\n+\n@@ -1144,1 +1157,1 @@\n-  if (bottom_type()->isa_vect() != NULL) {\n+  if (bottom_type()->isa_vect() != NULL && bottom_type()->isa_vectmask() == NULL) {\n@@ -1274,0 +1287,18 @@\n+#endif\n+        }\n+      }\n+      return 0;\n+    } else if (dst_first_rc == rc_kreg) {\n+      \/\/ mem -> kreg\n+      if ((src_first & 1) == 0 && src_first + 1 == src_second &&\n+          (dst_first & 1) == 0 && dst_first + 1 == dst_second) {\n+        \/\/ 64-bit\n+        int offset = ra_->reg2offset(src_first);\n+        if (cbuf) {\n+          MacroAssembler _masm(cbuf);\n+          __ kmov(as_KRegister(Matcher::_regEncode[dst_first]), Address(rsp, offset));\n+#ifndef PRODUCT\n+        } else {\n+          st->print(\"kmovq   %s, [rsp + #%d]\\t# spill\",\n+                     Matcher::regName[dst_first],\n+                     offset);\n@@ -1379,0 +1410,17 @@\n+    } else if (dst_first_rc == rc_kreg) {\n+      if ((src_first & 1) == 0 && src_first + 1 == src_second &&\n+          (dst_first & 1) == 0 && dst_first + 1 == dst_second) {\n+        \/\/ 64-bit\n+        if (cbuf) {\n+          MacroAssembler _masm(cbuf);\n+          __ kmov(as_KRegister(Matcher::_regEncode[dst_first]), as_Register(Matcher::_regEncode[src_first]));\n+  #ifndef PRODUCT\n+        } else {\n+           st->print(\"kmovq   %s, %s\\t# spill\",\n+                       Matcher::regName[dst_first],\n+                       Matcher::regName[src_first]);\n+  #endif\n+        }\n+      }\n+      Unimplemented();\n+      return 0;\n@@ -1479,0 +1527,59 @@\n+    } else if (dst_first_rc == rc_kreg) {\n+      assert(false, \"Illegal spilling\");\n+      return 0;\n+    }\n+  } else if (src_first_rc == rc_kreg) {\n+    if (dst_first_rc == rc_stack) {\n+      \/\/ mem -> kreg\n+      if ((src_first & 1) == 0 && src_first + 1 == src_second &&\n+          (dst_first & 1) == 0 && dst_first + 1 == dst_second) {\n+        \/\/ 64-bit\n+        int offset = ra_->reg2offset(dst_first);\n+        if (cbuf) {\n+          MacroAssembler _masm(cbuf);\n+          __ kmov(Address(rsp, offset), as_KRegister(Matcher::_regEncode[src_first]));\n+#ifndef PRODUCT\n+        } else {\n+          st->print(\"kmovq   [rsp + #%d] , %s\\t# spill\",\n+                     offset,\n+                     Matcher::regName[src_first]);\n+#endif\n+        }\n+      }\n+      return 0;\n+    } else if (dst_first_rc == rc_int) {\n+      if ((src_first & 1) == 0 && src_first + 1 == src_second &&\n+          (dst_first & 1) == 0 && dst_first + 1 == dst_second) {\n+        \/\/ 64-bit\n+        if (cbuf) {\n+          MacroAssembler _masm(cbuf);\n+          __ kmov(as_Register(Matcher::_regEncode[dst_first]), as_KRegister(Matcher::_regEncode[src_first]));\n+#ifndef PRODUCT\n+        } else {\n+         st->print(\"kmovq   %s, %s\\t# spill\",\n+                     Matcher::regName[dst_first],\n+                     Matcher::regName[src_first]);\n+#endif\n+        }\n+      }\n+      Unimplemented();\n+      return 0;\n+    } else if (dst_first_rc == rc_kreg) {\n+      if ((src_first & 1) == 0 && src_first + 1 == src_second &&\n+          (dst_first & 1) == 0 && dst_first + 1 == dst_second) {\n+        \/\/ 64-bit\n+        if (cbuf) {\n+          MacroAssembler _masm(cbuf);\n+          __ kmov(as_KRegister(Matcher::_regEncode[dst_first]), as_KRegister(Matcher::_regEncode[src_first]));\n+#ifndef PRODUCT\n+        } else {\n+         st->print(\"kmovq   %s, %s\\t# spill\",\n+                     Matcher::regName[dst_first],\n+                     Matcher::regName[src_first]);\n+#endif\n+        }\n+      }\n+      return 0;\n+    } else if (dst_first_rc == rc_float) {\n+      assert(false, \"Illegal spill\");\n+      return 0;\n@@ -3294,0 +3401,66 @@\n+operand kReg()\n+%{\n+  constraint(ALLOC_IN_RC(vectmask_reg));\n+  match(RegVectMask);\n+  format %{%}\n+  interface(REG_INTER);\n+%}\n+\n+operand kReg_K1()\n+%{\n+  constraint(ALLOC_IN_RC(vectmask_reg_K1));\n+  match(RegVectMask);\n+  format %{%}\n+  interface(REG_INTER);\n+%}\n+\n+operand kReg_K2()\n+%{\n+  constraint(ALLOC_IN_RC(vectmask_reg_K2));\n+  match(RegVectMask);\n+  format %{%}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Special Registers\n+operand kReg_K3()\n+%{\n+  constraint(ALLOC_IN_RC(vectmask_reg_K3));\n+  match(RegVectMask);\n+  format %{%}\n+  interface(REG_INTER);\n+%}\n+\n+operand kReg_K4()\n+%{\n+  constraint(ALLOC_IN_RC(vectmask_reg_K4));\n+  match(RegVectMask);\n+  format %{%}\n+  interface(REG_INTER);\n+%}\n+\n+operand kReg_K5()\n+%{\n+  constraint(ALLOC_IN_RC(vectmask_reg_K5));\n+  match(RegVectMask);\n+  format %{%}\n+  interface(REG_INTER);\n+%}\n+\n+operand kReg_K6()\n+%{\n+  constraint(ALLOC_IN_RC(vectmask_reg_K6));\n+  match(RegVectMask);\n+  format %{%}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Special Registers\n+operand kReg_K7()\n+%{\n+  constraint(ALLOC_IN_RC(vectmask_reg_K7));\n+  match(RegVectMask);\n+  format %{%}\n+  interface(REG_INTER);\n+%}\n+\n@@ -4704,1 +4877,0 @@\n-\n@@ -10771,3 +10943,1 @@\n-\n-\/\/ =======================================================================\n-\/\/ fast clearing of an array\n+\/\/ Fast clearing of an array\n@@ -10777,1 +10947,3 @@\n-  predicate(!((ClearArrayNode*)n)->is_large() && (UseAVX <= 2 || !VM_Version::supports_avx512vlbw() || !n->in(2)->bottom_type()->is_long()->is_con()));\n+  predicate(!((ClearArrayNode*)n)->is_large() &&\n+              ((UseAVX <= 2 || !VM_Version::supports_avx512vlbw()) &&\n+               !n->in(2)->bottom_type()->is_long()->is_con()));\n@@ -10828,1 +11000,62 @@\n-                 $tmp$$XMMRegister, false);\n+                 $tmp$$XMMRegister, false, knoreg);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct rep_stos_evex(rcx_RegL cnt, rdi_RegP base, regD tmp, kReg ktmp, rax_RegI zero,\n+                       Universe dummy, rFlagsReg cr)\n+%{\n+  predicate(!((ClearArrayNode*)n)->is_large() &&\n+               UseAVX > 2 && VM_Version::supports_avx512vlbw() &&\n+               !n->in(2)->bottom_type()->is_long()->is_con());\n+  match(Set dummy (ClearArray cnt base));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, KILL zero, KILL cr);\n+\n+  format %{ $$template\n+    $$emit$$\"xorq    rax, rax\\t# ClearArray:\\n\\t\"\n+    $$emit$$\"cmp     InitArrayShortSize,rcx\\n\\t\"\n+    $$emit$$\"jg      LARGE\\n\\t\"\n+    $$emit$$\"dec     rcx\\n\\t\"\n+    $$emit$$\"js      DONE\\t# Zero length\\n\\t\"\n+    $$emit$$\"mov     rax,(rdi,rcx,8)\\t# LOOP\\n\\t\"\n+    $$emit$$\"dec     rcx\\n\\t\"\n+    $$emit$$\"jge     LOOP\\n\\t\"\n+    $$emit$$\"jmp     DONE\\n\\t\"\n+    $$emit$$\"# LARGE:\\n\\t\"\n+    if (UseFastStosb) {\n+       $$emit$$\"shlq    rcx,3\\t# Convert doublewords to bytes\\n\\t\"\n+       $$emit$$\"rep     stosb\\t# Store rax to *rdi++ while rcx--\\n\\t\"\n+    } else if (UseXMMForObjInit) {\n+       $$emit$$\"mov     rdi,rax\\n\\t\"\n+       $$emit$$\"vpxor   ymm0,ymm0,ymm0\\n\\t\"\n+       $$emit$$\"jmpq    L_zero_64_bytes\\n\\t\"\n+       $$emit$$\"# L_loop:\\t# 64-byte LOOP\\n\\t\"\n+       $$emit$$\"vmovdqu ymm0,(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu ymm0,0x20(rax)\\n\\t\"\n+       $$emit$$\"add     0x40,rax\\n\\t\"\n+       $$emit$$\"# L_zero_64_bytes:\\n\\t\"\n+       $$emit$$\"sub     0x8,rcx\\n\\t\"\n+       $$emit$$\"jge     L_loop\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jl      L_tail\\n\\t\"\n+       $$emit$$\"vmovdqu ymm0,(rax)\\n\\t\"\n+       $$emit$$\"add     0x20,rax\\n\\t\"\n+       $$emit$$\"sub     0x4,rcx\\n\\t\"\n+       $$emit$$\"# L_tail:\\t# Clearing tail bytes\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jle     L_end\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"# L_sloop:\\t# 8-byte short loop\\n\\t\"\n+       $$emit$$\"vmovq   xmm0,(rax)\\n\\t\"\n+       $$emit$$\"add     0x8,rax\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"jge     L_sloop\\n\\t\"\n+       $$emit$$\"# L_end:\\n\\t\"\n+    } else {\n+       $$emit$$\"rep     stosq\\t# Store rax to *rdi++ while rcx--\\n\\t\"\n+    }\n+    $$emit$$\"# DONE\"\n+  %}\n+  ins_encode %{\n+    __ clear_mem($base$$Register, $cnt$$Register, $zero$$Register,\n+                 $tmp$$XMMRegister, false, $ktmp$$KRegister);\n@@ -10836,1 +11069,1 @@\n-  predicate(((ClearArrayNode*)n)->is_large());\n+  predicate(UseAVX <=2 && ((ClearArrayNode*)n)->is_large());\n@@ -10878,1 +11111,51 @@\n-                 $tmp$$XMMRegister, true);\n+                 $tmp$$XMMRegister, true, knoreg);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct rep_stos_large_evex(rcx_RegL cnt, rdi_RegP base, regD tmp, kReg ktmp, rax_RegI zero,\n+                             Universe dummy, rFlagsReg cr)\n+%{\n+  predicate(UseAVX > 2 && ((ClearArrayNode*)n)->is_large());\n+  match(Set dummy (ClearArray cnt base));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, KILL zero, KILL cr);\n+\n+  format %{ $$template\n+    if (UseFastStosb) {\n+       $$emit$$\"xorq    rax, rax\\t# ClearArray:\\n\\t\"\n+       $$emit$$\"shlq    rcx,3\\t# Convert doublewords to bytes\\n\\t\"\n+       $$emit$$\"rep     stosb\\t# Store rax to *rdi++ while rcx--\"\n+    } else if (UseXMMForObjInit) {\n+       $$emit$$\"mov     rdi,rax\\t# ClearArray:\\n\\t\"\n+       $$emit$$\"vpxor   ymm0,ymm0,ymm0\\n\\t\"\n+       $$emit$$\"jmpq    L_zero_64_bytes\\n\\t\"\n+       $$emit$$\"# L_loop:\\t# 64-byte LOOP\\n\\t\"\n+       $$emit$$\"vmovdqu ymm0,(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu ymm0,0x20(rax)\\n\\t\"\n+       $$emit$$\"add     0x40,rax\\n\\t\"\n+       $$emit$$\"# L_zero_64_bytes:\\n\\t\"\n+       $$emit$$\"sub     0x8,rcx\\n\\t\"\n+       $$emit$$\"jge     L_loop\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jl      L_tail\\n\\t\"\n+       $$emit$$\"vmovdqu ymm0,(rax)\\n\\t\"\n+       $$emit$$\"add     0x20,rax\\n\\t\"\n+       $$emit$$\"sub     0x4,rcx\\n\\t\"\n+       $$emit$$\"# L_tail:\\t# Clearing tail bytes\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jle     L_end\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"# L_sloop:\\t# 8-byte short loop\\n\\t\"\n+       $$emit$$\"vmovq   xmm0,(rax)\\n\\t\"\n+       $$emit$$\"add     0x8,rax\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"jge     L_sloop\\n\\t\"\n+       $$emit$$\"# L_end:\\n\\t\"\n+    } else {\n+       $$emit$$\"xorq    rax, rax\\t# ClearArray:\\n\\t\"\n+       $$emit$$\"rep     stosq\\t# Store rax to *rdi++ while rcx--\"\n+    }\n+  %}\n+  ins_encode %{\n+    __ clear_mem($base$$Register, $cnt$$Register, $zero$$Register,\n+                 $tmp$$XMMRegister, true, $ktmp$$KRegister);\n@@ -10883,1 +11166,1 @@\n-instruct rep_stos_im(immL cnt, rRegP base, regD tmp, rRegI zero, Universe dummy, rFlagsReg cr)\n+instruct rep_stos_im(immL cnt, rRegP base, regD tmp, rRegI zero, kReg ktmp, Universe dummy, rFlagsReg cr)\n@@ -10885,1 +11168,3 @@\n-  predicate(!((ClearArrayNode*)n)->is_large() && (UseAVX > 2 && VM_Version::supports_avx512vlbw() && n->in(2)->bottom_type()->is_long()->is_con()));\n+  predicate(!((ClearArrayNode*)n)->is_large() &&\n+              (UseAVX > 2 && VM_Version::supports_avx512vlbw() &&\n+               n->in(2)->bottom_type()->is_long()->is_con()));\n@@ -10887,1 +11172,1 @@\n-  effect(TEMP tmp, TEMP zero, KILL cr);\n+  effect(TEMP tmp, TEMP zero, TEMP ktmp, KILL cr);\n@@ -10890,1 +11175,1 @@\n-   __ clear_mem($base$$Register, $cnt$$constant, $zero$$Register, $tmp$$XMMRegister);\n+   __ clear_mem($base$$Register, $cnt$$constant, $zero$$Register, $tmp$$XMMRegister, $ktmp$$KRegister);\n@@ -10898,1 +11183,1 @@\n-  predicate(((StrCompNode*)n)->encoding() == StrIntrinsicNode::LL);\n+  predicate(UseAVX <= 2 && ((StrCompNode*)n)->encoding() == StrIntrinsicNode::LL);\n@@ -10906,1 +11191,17 @@\n-                      $tmp1$$XMMRegister, StrIntrinsicNode::LL);\n+                      $tmp1$$XMMRegister, StrIntrinsicNode::LL, knoreg);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct string_compareL_evex(rdi_RegP str1, rcx_RegI cnt1, rsi_RegP str2, rdx_RegI cnt2,\n+                              rax_RegI result, legRegD tmp1, kReg ktmp, rFlagsReg cr)\n+%{\n+  predicate(UseAVX > 2 && ((StrCompNode*)n)->encoding() == StrIntrinsicNode::LL);\n+  match(Set result (StrComp (Binary str1 cnt1) (Binary str2 cnt2)));\n+  effect(TEMP tmp1, TEMP ktmp, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL cr);\n+\n+  format %{ \"String Compare byte[] $str1,$cnt1,$str2,$cnt2 -> $result   \/\/ KILL $tmp1\" %}\n+  ins_encode %{\n+    __ string_compare($str1$$Register, $str2$$Register,\n+                      $cnt1$$Register, $cnt2$$Register, $result$$Register,\n+                      $tmp1$$XMMRegister, StrIntrinsicNode::LL, $ktmp$$KRegister);\n@@ -10914,1 +11215,1 @@\n-  predicate(((StrCompNode*)n)->encoding() == StrIntrinsicNode::UU);\n+  predicate(UseAVX <= 2 && ((StrCompNode*)n)->encoding() == StrIntrinsicNode::UU);\n@@ -10922,1 +11223,17 @@\n-                      $tmp1$$XMMRegister, StrIntrinsicNode::UU);\n+                      $tmp1$$XMMRegister, StrIntrinsicNode::UU, knoreg);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct string_compareU_evex(rdi_RegP str1, rcx_RegI cnt1, rsi_RegP str2, rdx_RegI cnt2,\n+                              rax_RegI result, legRegD tmp1, kReg ktmp, rFlagsReg cr)\n+%{\n+  predicate(UseAVX > 2 && ((StrCompNode*)n)->encoding() == StrIntrinsicNode::UU);\n+  match(Set result (StrComp (Binary str1 cnt1) (Binary str2 cnt2)));\n+  effect(TEMP tmp1, TEMP ktmp, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL cr);\n+\n+  format %{ \"String Compare char[] $str1,$cnt1,$str2,$cnt2 -> $result   \/\/ KILL $tmp1\" %}\n+  ins_encode %{\n+    __ string_compare($str1$$Register, $str2$$Register,\n+                      $cnt1$$Register, $cnt2$$Register, $result$$Register,\n+                      $tmp1$$XMMRegister, StrIntrinsicNode::UU, $ktmp$$KRegister);\n@@ -10930,1 +11247,1 @@\n-  predicate(((StrCompNode*)n)->encoding() == StrIntrinsicNode::LU);\n+  predicate(UseAVX <= 2 && ((StrCompNode*)n)->encoding() == StrIntrinsicNode::LU);\n@@ -10938,1 +11255,17 @@\n-                      $tmp1$$XMMRegister, StrIntrinsicNode::LU);\n+                      $tmp1$$XMMRegister, StrIntrinsicNode::LU, knoreg);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct string_compareLU_evex(rdi_RegP str1, rcx_RegI cnt1, rsi_RegP str2, rdx_RegI cnt2,\n+                               rax_RegI result, legRegD tmp1, kReg ktmp, rFlagsReg cr)\n+%{\n+  predicate(UseAVX > 2 && ((StrCompNode*)n)->encoding() == StrIntrinsicNode::LU);\n+  match(Set result (StrComp (Binary str1 cnt1) (Binary str2 cnt2)));\n+  effect(TEMP tmp1, TEMP ktmp, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL cr);\n+\n+  format %{ \"String Compare byte[] $str1,$cnt1,$str2,$cnt2 -> $result   \/\/ KILL $tmp1\" %}\n+  ins_encode %{\n+    __ string_compare($str1$$Register, $str2$$Register,\n+                      $cnt1$$Register, $cnt2$$Register, $result$$Register,\n+                      $tmp1$$XMMRegister, StrIntrinsicNode::LU, $ktmp$$KRegister);\n@@ -10946,1 +11279,1 @@\n-  predicate(((StrCompNode*)n)->encoding() == StrIntrinsicNode::UL);\n+  predicate(UseAVX <= 2 && ((StrCompNode*)n)->encoding() == StrIntrinsicNode::UL);\n@@ -10954,1 +11287,17 @@\n-                      $tmp1$$XMMRegister, StrIntrinsicNode::UL);\n+                      $tmp1$$XMMRegister, StrIntrinsicNode::UL, knoreg);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct string_compareUL_evex(rsi_RegP str1, rdx_RegI cnt1, rdi_RegP str2, rcx_RegI cnt2,\n+                               rax_RegI result, legRegD tmp1, kReg ktmp, rFlagsReg cr)\n+%{\n+  predicate(UseAVX > 2 && ((StrCompNode*)n)->encoding() == StrIntrinsicNode::UL);\n+  match(Set result (StrComp (Binary str1 cnt1) (Binary str2 cnt2)));\n+  effect(TEMP tmp1, TEMP ktmp, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL cr);\n+\n+  format %{ \"String Compare byte[] $str1,$cnt1,$str2,$cnt2 -> $result   \/\/ KILL $tmp1\" %}\n+  ins_encode %{\n+    __ string_compare($str2$$Register, $str1$$Register,\n+                      $cnt2$$Register, $cnt1$$Register, $result$$Register,\n+                      $tmp1$$XMMRegister, StrIntrinsicNode::UL, $ktmp$$KRegister);\n@@ -11129,0 +11478,1 @@\n+  predicate(UseAVX <= 2);\n@@ -11136,1 +11486,17 @@\n-                     $tmp1$$XMMRegister, $tmp2$$XMMRegister, false \/* char *\/);\n+                     $tmp1$$XMMRegister, $tmp2$$XMMRegister, false \/* char *\/, knoreg);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct string_equals_evex(rdi_RegP str1, rsi_RegP str2, rcx_RegI cnt, rax_RegI result,\n+                           legRegD tmp1, legRegD tmp2, kReg ktmp, rbx_RegI tmp3, rFlagsReg cr)\n+%{\n+  predicate(UseAVX > 2);\n+  match(Set result (StrEquals (Binary str1 str2) cnt));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP ktmp, USE_KILL str1, USE_KILL str2, USE_KILL cnt, KILL tmp3, KILL cr);\n+\n+  format %{ \"String Equals $str1,$str2,$cnt -> $result    \/\/ KILL $tmp1, $tmp2, $tmp3\" %}\n+  ins_encode %{\n+    __ arrays_equals(false, $str1$$Register, $str2$$Register,\n+                     $cnt$$Register, $result$$Register, $tmp3$$Register,\n+                     $tmp1$$XMMRegister, $tmp2$$XMMRegister, false \/* char *\/, $ktmp$$KRegister);\n@@ -11145,1 +11511,1 @@\n-  predicate(((AryEqNode*)n)->encoding() == StrIntrinsicNode::LL);\n+  predicate(UseAVX <= 2 && ((AryEqNode*)n)->encoding() == StrIntrinsicNode::LL);\n@@ -11153,1 +11519,17 @@\n-                     $tmp1$$XMMRegister, $tmp2$$XMMRegister, false \/* char *\/);\n+                     $tmp1$$XMMRegister, $tmp2$$XMMRegister, false \/* char *\/, knoreg);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct array_equalsB_evex(rdi_RegP ary1, rsi_RegP ary2, rax_RegI result,\n+                            legRegD tmp1, legRegD tmp2, kReg ktmp, rcx_RegI tmp3, rbx_RegI tmp4, rFlagsReg cr)\n+%{\n+  predicate(UseAVX > 2 && ((AryEqNode*)n)->encoding() == StrIntrinsicNode::LL);\n+  match(Set result (AryEq ary1 ary2));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP ktmp, USE_KILL ary1, USE_KILL ary2, KILL tmp3, KILL tmp4, KILL cr);\n+\n+  format %{ \"Array Equals byte[] $ary1,$ary2 -> $result   \/\/ KILL $tmp1, $tmp2, $tmp3, $tmp4\" %}\n+  ins_encode %{\n+    __ arrays_equals(true, $ary1$$Register, $ary2$$Register,\n+                     $tmp3$$Register, $result$$Register, $tmp4$$Register,\n+                     $tmp1$$XMMRegister, $tmp2$$XMMRegister, false \/* char *\/, $ktmp$$KRegister);\n@@ -11161,1 +11543,1 @@\n-  predicate(((AryEqNode*)n)->encoding() == StrIntrinsicNode::UU);\n+  predicate(UseAVX <= 2 && ((AryEqNode*)n)->encoding() == StrIntrinsicNode::UU);\n@@ -11169,1 +11551,17 @@\n-                     $tmp1$$XMMRegister, $tmp2$$XMMRegister, true \/* char *\/);\n+                     $tmp1$$XMMRegister, $tmp2$$XMMRegister, true \/* char *\/, knoreg);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct array_equalsC_evex(rdi_RegP ary1, rsi_RegP ary2, rax_RegI result,\n+                            legRegD tmp1, legRegD tmp2, kReg ktmp, rcx_RegI tmp3, rbx_RegI tmp4, rFlagsReg cr)\n+%{\n+  predicate(UseAVX > 2 && ((AryEqNode*)n)->encoding() == StrIntrinsicNode::UU);\n+  match(Set result (AryEq ary1 ary2));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP ktmp, USE_KILL ary1, USE_KILL ary2, KILL tmp3, KILL tmp4, KILL cr);\n+\n+  format %{ \"Array Equals char[] $ary1,$ary2 -> $result   \/\/ KILL $tmp1, $tmp2, $tmp3, $tmp4\" %}\n+  ins_encode %{\n+    __ arrays_equals(true, $ary1$$Register, $ary2$$Register,\n+                     $tmp3$$Register, $result$$Register, $tmp4$$Register,\n+                     $tmp1$$XMMRegister, $tmp2$$XMMRegister, true \/* char *\/, $ktmp$$KRegister);\n@@ -11175,1 +11573,1 @@\n-                       legRegD tmp1, legRegD tmp2, rbx_RegI tmp3, rFlagsReg cr)\n+                       legRegD tmp1, legRegD tmp2, rbx_RegI tmp3, rFlagsReg cr,)\n@@ -11177,0 +11575,1 @@\n+  predicate(UseAVX <= 2);\n@@ -11184,1 +11583,17 @@\n-                     $tmp1$$XMMRegister, $tmp2$$XMMRegister);\n+                     $tmp1$$XMMRegister, $tmp2$$XMMRegister, knoreg, knoreg);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct has_negatives_evex(rsi_RegP ary1, rcx_RegI len, rax_RegI result,\n+                            legRegD tmp1, legRegD tmp2, kReg ktmp1, kReg ktmp2, rbx_RegI tmp3, rFlagsReg cr,)\n+%{\n+  predicate(UseAVX > 2);\n+  match(Set result (HasNegatives ary1 len));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP ktmp1, TEMP ktmp2, USE_KILL ary1, USE_KILL len, KILL tmp3, KILL cr);\n+\n+  format %{ \"has negatives byte[] $ary1,$len -> $result   \/\/ KILL $tmp1, $tmp2, $tmp3\" %}\n+  ins_encode %{\n+    __ has_negatives($ary1$$Register, $len$$Register,\n+                     $result$$Register, $tmp3$$Register,\n+                     $tmp1$$XMMRegister, $tmp2$$XMMRegister, $ktmp1$$KRegister, $ktmp2$$KRegister);\n@@ -11190,2 +11605,3 @@\n-instruct string_compress(rsi_RegP src, rdi_RegP dst, rdx_RegI len, legRegD tmp1, legRegD tmp2, legRegD tmp3, legRegD tmp4,\n-                         rcx_RegI tmp5, rax_RegI result, rFlagsReg cr) %{\n+instruct string_compress(rsi_RegP src, rdi_RegP dst, rdx_RegI len, legRegD tmp1, legRegD tmp2, legRegD tmp3,\n+                         legRegD tmp4, rcx_RegI tmp5, rax_RegI result, rFlagsReg cr) %{\n+  predicate(UseAVX <= 2);\n@@ -11193,1 +11609,2 @@\n-  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, TEMP tmp4, USE_KILL src, USE_KILL dst, USE_KILL len, KILL tmp5, KILL cr);\n+  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, TEMP tmp4, USE_KILL src, USE_KILL dst,\n+         USE_KILL len, KILL tmp5, KILL cr);\n@@ -11199,1 +11616,2 @@\n-                           $tmp4$$XMMRegister, $tmp5$$Register, $result$$Register);\n+                           $tmp4$$XMMRegister, $tmp5$$Register, $result$$Register,\n+                           knoreg, knoreg);\n@@ -11204,0 +11622,16 @@\n+instruct string_compress_evex(rsi_RegP src, rdi_RegP dst, rdx_RegI len, legRegD tmp1, legRegD tmp2, legRegD tmp3,\n+                              legRegD tmp4, kReg ktmp1, kReg ktmp2, rcx_RegI tmp5, rax_RegI result, rFlagsReg cr) %{\n+  predicate(UseAVX > 2);\n+  match(Set result (StrCompressedCopy src (Binary dst len)));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, TEMP tmp4, TEMP ktmp1, TEMP ktmp2, USE_KILL src, USE_KILL dst,\n+         USE_KILL len, KILL tmp5, KILL cr);\n+\n+  format %{ \"String Compress $src,$dst -> $result    \/\/ KILL RAX, RCX, RDX\" %}\n+  ins_encode %{\n+    __ char_array_compress($src$$Register, $dst$$Register, $len$$Register,\n+                           $tmp1$$XMMRegister, $tmp2$$XMMRegister, $tmp3$$XMMRegister,\n+                           $tmp4$$XMMRegister, $tmp5$$Register, $result$$Register,\n+                           $ktmp1$$KRegister, $ktmp2$$KRegister);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n@@ -11207,0 +11641,1 @@\n+  predicate(UseAVX <= 2);\n@@ -11213,1 +11648,15 @@\n-                          $tmp1$$XMMRegister, $tmp2$$Register);\n+                          $tmp1$$XMMRegister, $tmp2$$Register, knoreg);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct string_inflate_evex(Universe dummy, rsi_RegP src, rdi_RegP dst, rdx_RegI len,\n+                             legRegD tmp1, kReg ktmp, rcx_RegI tmp2, rFlagsReg cr) %{\n+  predicate(UseAVX > 2);\n+  match(Set dummy (StrInflatedCopy src (Binary dst len)));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP ktmp, USE_KILL src, USE_KILL dst, USE_KILL len, KILL cr);\n+\n+  format %{ \"String Inflate $src,$dst    \/\/ KILL $tmp1, $tmp2\" %}\n+  ins_encode %{\n+    __ byte_array_inflate($src$$Register, $dst$$Register, $len$$Register,\n+                          $tmp1$$XMMRegister, $tmp2$$Register, $ktmp$$KRegister);\n@@ -12005,1 +12454,3 @@\n-instruct jmpLoopEnd_and_restoreMask(cmpOp cop, rFlagsReg cr, label labl)\n+\/\/ Bounded mask operand used in following patten is needed for\n+\/\/ post-loop multiversioning.\n+instruct jmpLoopEnd_and_restoreMask(cmpOp cop, kReg_K1 ktmp, rFlagsReg cr, label labl)\n@@ -12007,1 +12458,1 @@\n-  predicate(n->has_vector_mask_set());\n+  predicate(PostLoopMultiversioning && n->has_vector_mask_set());\n@@ -12009,1 +12460,1 @@\n-  effect(USE labl);\n+  effect(USE labl, TEMP ktmp);\n@@ -12018,1 +12469,1 @@\n-    __ restorevectmask();\n+    __ restorevectmask($ktmp$$KRegister);\n@@ -12024,2 +12475,4 @@\n-instruct jmpLoopEndU_and_restoreMask(cmpOpU cop, rFlagsRegU cmp, label labl) %{\n-  predicate(n->has_vector_mask_set());\n+\/\/ Bounded mask operand used in following patten is needed for\n+\/\/ post-loop multiversioning.\n+instruct jmpLoopEndU_and_restoreMask(cmpOpU cop, kReg_K1 ktmp, rFlagsRegU cmp, label labl) %{\n+  predicate(PostLoopMultiversioning && n->has_vector_mask_set());\n@@ -12027,1 +12480,1 @@\n-  effect(USE labl);\n+  effect(USE labl, TEMP ktmp);\n@@ -12036,1 +12489,1 @@\n-    __ restorevectmask();\n+    __ restorevectmask($ktmp$$KRegister);\n@@ -12041,2 +12494,4 @@\n-instruct jmpLoopEndUCF_and_restoreMask(cmpOpUCF cop, rFlagsRegUCF cmp, label labl) %{\n-  predicate(n->has_vector_mask_set());\n+\/\/ Bounded mask operand used in following patten is needed for\n+\/\/ post-loop multiversioning.\n+instruct jmpLoopEndUCF_and_restoreMask(cmpOpUCF cop, kReg_K1 ktmp, rFlagsRegUCF cmp, label labl) %{\n+  predicate(PostLoopMultiversioning && n->has_vector_mask_set());\n@@ -12044,1 +12499,1 @@\n-  effect(USE labl);\n+  effect(USE labl, TEMP ktmp);\n@@ -12053,1 +12508,1 @@\n-    __ restorevectmask();\n+    __ restorevectmask($ktmp$$KRegister);\n","filename":"src\/hotspot\/cpu\/x86\/x86_64.ad","additions":500,"deletions":45,"binary":false,"changes":545,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n-\/\/ Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n+\/\/ Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -948,2 +948,2 @@\n-  if (strncmp(idealOp, \"RegVMask\", 8) == 0) {\n-    return \"Type::BOTTOM\";\n+  if (strncmp(idealOp, \"RegVectMask\", 8) == 0) {\n+    return \"TypeVect::VECTMASK\";\n","filename":"src\/hotspot\/share\/adlc\/archDesc.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -3967,1 +3967,1 @@\n-         strcmp(opType,\"RegVMask\")==0 ||\n+         strcmp(opType,\"RegVectMask\")==0 ||\n","filename":"src\/hotspot\/share\/adlc\/formssel.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -2268,0 +2268,1 @@\n+    if (strcmp(rep_var,\"$KRegister\") == 0)     return \"as_KRegister\";\n","filename":"src\/hotspot\/share\/adlc\/output_c.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -827,1 +827,2 @@\n-        assert(n_type->isa_vect() == NULL || lrg._is_vector || ireg == Op_RegD || ireg == Op_RegL,\n+        assert(n_type->isa_vect() == NULL || lrg._is_vector ||\n+               ireg == Op_RegD || ireg == Op_RegL  || ireg == Op_RegVectMask,\n@@ -920,0 +921,4 @@\n+        case Op_RegVectMask:\n+          lrg.set_num_regs(RegMask::SlotsPerRegVectMask);\n+          lrg.set_reg_pressure(1);\n+          break;\n@@ -1039,2 +1044,2 @@\n-        assert(n->in(k)->bottom_type()->isa_vect() == NULL ||\n-               is_vect || kreg == Op_RegD || kreg == Op_RegL,\n+        assert(n->in(k)->bottom_type()->isa_vect() == NULL || is_vect ||\n+               kreg == Op_RegD || kreg == Op_RegL || kreg == Op_RegVectMask,\n","filename":"src\/hotspot\/share\/opto\/chaitin.cpp","additions":9,"deletions":4,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -166,1 +166,1 @@\n-      \/\/ Should only be a vector for now, but it could also be a RegVMask in future.\n+      \/\/ Should only be a vector for now, but it could also be a RegVectMask in future.\n","filename":"src\/hotspot\/share\/opto\/chaitin.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2443,1 +2443,1 @@\n-  if (!is_vector_bitwise_op(n)) {\n+  if (n->bottom_type()->isa_vectmask() || !is_vector_bitwise_op(n)) {\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -412,1 +412,3 @@\n-        lrg.mask().overlap(*Matcher::idealreg2regmask[Op_RegI])) {\n+        (lrg.mask().overlap(*Matcher::idealreg2regmask[Op_RegI]) ||\n+         (Matcher::has_predicated_vectors() &&\n+          lrg.mask().overlap(*Matcher::idealreg2regmask[Op_RegVectMask])))) {\n@@ -448,1 +450,3 @@\n-      if (r.overlap(*Matcher::idealreg2regmask[Op_RegI])) {\n+      if (r.overlap(*Matcher::idealreg2regmask[Op_RegI]) ||\n+           (Matcher::has_predicated_vectors() &&\n+            r.overlap(*Matcher::idealreg2regmask[Op_RegVectMask]))) {\n@@ -503,1 +507,3 @@\n-      if (rm.overlap(*Matcher::idealreg2regmask[Op_RegI])) {\n+      if (rm.overlap(*Matcher::idealreg2regmask[Op_RegI]) ||\n+           (Matcher::has_predicated_vectors() &&\n+            rm.overlap(*Matcher::idealreg2regmask[Op_RegVectMask]))) {\n","filename":"src\/hotspot\/share\/opto\/ifg.cpp","additions":10,"deletions":4,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -106,0 +106,6 @@\n+  KRegister  as_KRegister(PhaseRegAlloc *ra_, const Node *node)   const {\n+    return ::as_KRegister(reg(ra_, node));\n+  }\n+  KRegister  as_KRegister(PhaseRegAlloc *ra_, const Node *node, int idx)   const {\n+    return ::as_KRegister(reg(ra_, node, idx));\n+  }\n","filename":"src\/hotspot\/share\/opto\/machnode.hpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -236,1 +236,1 @@\n-  Node* mask_gen =  new VectorMaskGenNode(length, TypeLong::LONG, Type::get_const_basic_type(type));\n+  Node* mask_gen =  new VectorMaskGenNode(length, TypeVect::VECTMASK, Type::get_const_basic_type(type));\n","filename":"src\/hotspot\/share\/opto\/macroArrayCopy.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -98,0 +98,1 @@\n+  idealreg2spillmask  [Op_RegVectMask] = NULL;\n@@ -112,0 +113,1 @@\n+  idealreg2debugmask  [Op_RegVectMask] = NULL;\n@@ -126,0 +128,1 @@\n+  idealreg2mhdebugmask[Op_RegVectMask] = NULL;\n@@ -433,1 +436,1 @@\n-#define NOF_STACK_MASKS (3*12)\n+#define NOF_STACK_MASKS (3*13)\n@@ -490,0 +493,4 @@\n+  idealreg2spillmask  [Op_RegVectMask] = &rms[36];\n+  idealreg2debugmask  [Op_RegVectMask] = &rms[37];\n+  idealreg2mhdebugmask[Op_RegVectMask] = &rms[38];\n+\n@@ -534,0 +541,5 @@\n+  if (Matcher::has_predicated_vectors()) {\n+    *idealreg2spillmask[Op_RegVectMask] = *idealreg2regmask[Op_RegVectMask];\n+     idealreg2spillmask[Op_RegVectMask]->OR(aligned_stack_mask);\n+  }\n+\n@@ -652,0 +664,1 @@\n+  *idealreg2debugmask  [Op_RegVectMask] = *idealreg2spillmask[Op_RegVectMask];\n@@ -666,0 +679,1 @@\n+  *idealreg2mhdebugmask[Op_RegVectMask] = *idealreg2spillmask[Op_RegVectMask];\n@@ -686,0 +700,1 @@\n+  idealreg2debugmask[Op_RegVectMask]->SUBTRACT(*caller_save_mask);\n@@ -700,0 +715,1 @@\n+  idealreg2mhdebugmask[Op_RegVectMask]->SUBTRACT(*mh_caller_save_mask);\n@@ -968,0 +984,1 @@\n+  idealreg2regmask[Op_RegVectMask] = regmask_for_ideal_register(Op_RegVectMask, ret);\n@@ -2562,0 +2579,1 @@\n+    case Op_RegVectMask: return Matcher::predicate_reg_mask();\n","filename":"src\/hotspot\/share\/opto\/matcher.cpp","additions":20,"deletions":2,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -322,0 +322,2 @@\n+  static const RegMask* predicate_reg_mask(void);\n+  static const TypeVect* predicate_reg_type(const Type* elemTy, int length);\n","filename":"src\/hotspot\/share\/opto\/matcher.hpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -696,0 +696,2 @@\n+      DEFINE_CLASS_ID(Vector, Type, 7)\n+        DEFINE_CLASS_ID(VectorMaskCmp, Vector, 0)\n@@ -740,2 +742,0 @@\n-    DEFINE_CLASS_ID(Vector,   Node, 13)\n-      DEFINE_CLASS_ID(VectorMaskCmp, Vector, 0)\n","filename":"src\/hotspot\/share\/opto\/node.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -47,1 +47,1 @@\n-  \"RegVMask\",\n+  \"RegVectMask\",\n","filename":"src\/hotspot\/share\/opto\/opcodes.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -46,1 +46,1 @@\n-  macro(RegVMask)               \/\/ Vector mask\/predicate register\n+  macro(RegVectMask)            \/\/ Vector mask\/predicate register\n","filename":"src\/hotspot\/share\/opto\/opcodes.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2006, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2006, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -39,1 +39,1 @@\n-\/\/ Stack-slots (spill locations) start at the nest Chunk past the last machine\n+\/\/ Stack-slots (spill locations) start at the next Chunk past the last machine\n","filename":"src\/hotspot\/share\/opto\/optoreg.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -70,0 +70,2 @@\n+    case Op_RegVectMask:\n+      return SlotsPerRegVectMask;\n","filename":"src\/hotspot\/share\/opto\/regmask.cpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -108,0 +108,1 @@\n+         SlotsPerRegVectMask = X86_ONLY(2) NOT_X86(1)\n","filename":"src\/hotspot\/share\/opto\/regmask.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -66,0 +66,1 @@\n+  { Bad,             T_ILLEGAL,    \"vectormask:\",   false, Op_RegVectMask,       relocInfo::none          },  \/\/ VectorMask.\n@@ -73,0 +74,1 @@\n+  { Bad,             T_ILLEGAL,    \"vectormask:\",   false, Op_RegVectMask,       relocInfo::none          },  \/\/ VectorMask.\n@@ -80,0 +82,1 @@\n+  { Bad,             T_ILLEGAL,    \"vectormask:\",   false, Op_RegVectMask,       relocInfo::none          },  \/\/ VectorMask.\n@@ -661,0 +664,3 @@\n+  TypeVect::VECTMASK = (TypeVect*)(new TypeVectMask(TypeInt::BOOL, MaxVectorSize))->hashcons();\n+  mreg2type[Op_RegVectMask] = TypeVect::VECTMASK;\n+\n@@ -2379,0 +2385,1 @@\n+const TypeVect *TypeVect::VECTMASK = NULL; \/\/ predicate\/mask vector\n@@ -2406,0 +2413,9 @@\n+const TypeVect *TypeVect::makemask(const Type* elem, uint length) {\n+  if (Matcher::has_predicated_vectors()) {\n+    const TypeVect* mtype = Matcher::predicate_reg_type(elem, length);\n+    return (TypeVect*)(const_cast<TypeVect*>(mtype))->hashcons();\n+  } else {\n+    return make(elem, length);\n+  }\n+}\n+\n@@ -2420,0 +2436,7 @@\n+  case VectorMask: {\n+    const TypeVectMask* v = t->is_vectmask();\n+    assert(  base() == v->base(), \"\");\n+    assert(length() == v->length(), \"\");\n+    assert(element_basic_type() == v->element_basic_type(), \"\");\n+    return TypeVect::makemask(_elem->xmeet(v->_elem), _length);\n+  }\n@@ -2487,0 +2510,2 @@\n+  case VectorMask:\n+    st->print(\"vectormask[\"); break;\n@@ -2496,0 +2521,8 @@\n+bool TypeVectMask::eq(const Type *t) const {\n+  const TypeVectMask *v = t->is_vectmask();\n+  return (element_type() == v->element_type()) && (length() == v->length());\n+}\n+\n+const Type *TypeVectMask::xdual() const {\n+  return new TypeVectMask(element_type()->dual(), length());\n+}\n","filename":"src\/hotspot\/share\/opto\/type.cpp","additions":33,"deletions":0,"binary":false,"changes":33,"status":"modified"},{"patch":"@@ -63,0 +63,1 @@\n+class     TypeVectMask;\n@@ -92,0 +93,2 @@\n+\n+    VectorMask,                 \/\/ Vector predicate\/mask type\n@@ -302,0 +305,2 @@\n+  const TypeVectMask *is_vectmask() const;       \/\/ Predicate\/Mask Vector\n+  const TypeVectMask *isa_vectmask() const;      \/\/ Returns NULL if not a Vector Predicate\/Mask\n@@ -803,0 +808,7 @@\n+  static const TypeVect *makemask(const BasicType elem_bt, uint length) {\n+    \/\/ Use bottom primitive type.\n+    return makemask(get_const_basic_type(elem_bt), length);\n+  }\n+  static const TypeVect *makemask(const Type* elem, uint length);\n+\n+\n@@ -812,0 +824,1 @@\n+  static const TypeVect *VECTMASK;\n@@ -848,0 +861,8 @@\n+class TypeVectMask : public TypeVect {\n+public:\n+  friend class TypeVect;\n+  TypeVectMask(const Type* elem, uint length) : TypeVect(VectorMask, elem, length) {}\n+  virtual bool eq(const Type *t) const;\n+  virtual const Type *xdual() const;\n+};\n+\n@@ -1685,0 +1706,9 @@\n+inline const TypeVectMask *Type::is_vectmask() const {\n+  assert( _base == VectorMask, \"Not a Vector Mask\" );\n+  return (TypeVectMask*)this;\n+}\n+\n+inline const TypeVectMask *Type::isa_vectmask() const {\n+  return (_base == VectorMask) ? (TypeVectMask*)this : NULL;\n+}\n+\n@@ -1686,1 +1716,1 @@\n-  assert( _base >= VectorA && _base <= VectorZ, \"Not a Vector\" );\n+  assert( _base >= VectorMask && _base <= VectorZ, \"Not a Vector\" );\n@@ -1691,1 +1721,1 @@\n-  return (_base >= VectorA && _base <= VectorZ) ? (TypeVect*)this : NULL;\n+  return (_base >= VectorMask && _base <= VectorZ) ? (TypeVect*)this : NULL;\n","filename":"src\/hotspot\/share\/opto\/type.hpp","additions":32,"deletions":2,"binary":false,"changes":34,"status":"modified"},{"patch":"@@ -807,1 +807,1 @@\n-    assert(mask->bottom_type()->is_long(), \"sanity\");\n+    assert(mask->bottom_type()->is_vectmask(), \"sanity\");\n@@ -825,1 +825,1 @@\n-    assert(mask->bottom_type()->is_long(), \"sanity\");\n+    assert(mask->bottom_type()->is_vectmask(), \"sanity\");\n@@ -848,0 +848,3 @@\n+  virtual uint  ideal_reg() const {\n+    return Op_RegVectMask;\n+  }\n","filename":"src\/hotspot\/share\/opto\/vectornode.hpp","additions":5,"deletions":2,"binary":false,"changes":7,"status":"modified"}]}