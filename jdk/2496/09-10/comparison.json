{"files":[{"patch":"@@ -57,1 +57,1 @@\n-    ShenandoahHeap::atomic_update_oop(fwd, load_addr, obj);\n+    ShenandoahForwarding::update_with_forwarded(obj, load_addr, fwd);\n@@ -117,1 +117,1 @@\n-    ShenandoahHeap::atomic_update_oop(fwd, load_addr, obj);\n+    ShenandoahForwarding::update_with_forwarded(obj, load_addr, fwd);\n@@ -348,1 +348,5 @@\n-        ShenandoahHeap::atomic_update_oop(fwd, elem_ptr, o);\n+        if (EVAC) {\n+          ShenandoahForwarding::update_with_forwarded(o, elem_ptr, fwd);\n+        } else {\n+          ShenandoahForwarding::update_with_forwarded_stable(o, elem_ptr, fwd);\n+        }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahBarrierSet.inline.hpp","additions":7,"deletions":3,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -59,1 +59,5 @@\n-        ShenandoahHeap::atomic_update_oop(fwd, p, o);\n+        if (EVAC) {\n+          ShenandoahForwarding::update_with_forwarded(o, p, fwd);\n+        } else {\n+          ShenandoahForwarding::update_with_forwarded_stable(o, p, fwd);\n+        }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahBarrierSetClone.inline.hpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -164,1 +164,1 @@\n-      ShenandoahHeap::atomic_update_oop(resolved, p, o);\n+      ShenandoahForwarding::update_with_forwarded(o, p, resolved);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahClosures.inline.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -695,1 +695,1 @@\n-      ShenandoahHeap::atomic_update_oop(resolved, p, obj);\n+      ShenandoahForwarding::update_with_forwarded(obj, p, resolved);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahConcurrentGC.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -70,0 +70,10 @@\n+  \/*\n+   * Tries to atomically update the heap address to object's forwardee.\n+   * Stable versions are only safe when no evacs happen.\n+   *\/\n+  static inline void update_with_forwarded(      oop obj,       oop* addr, oop update);\n+  static inline void update_with_forwarded(      oop obj, narrowOop* addr, oop update);\n+  static inline void update_with_forwarded(narrowOop obj, narrowOop* addr, oop update);\n+  static inline void update_with_forwarded_stable(      oop obj,       oop* addr, oop update);\n+  static inline void update_with_forwarded_stable(      oop obj, narrowOop* addr, oop update);\n+  static inline void update_with_forwarded_stable(narrowOop obj, narrowOop* addr, oop update);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahForwarding.hpp","additions":10,"deletions":0,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -31,0 +31,2 @@\n+#include \"oops\/compressedOops.inline.hpp\"\n+#include \"oops\/oop.inline.hpp\"\n@@ -32,0 +34,1 @@\n+#include \"runtime\/atomic.hpp\"\n@@ -158,1 +161,2 @@\n-    return cast_to_oop(old_mark.clear_lock_bits().to_pointer());\n+    \/\/ Already forwarded, don't try to overwrite. Re-read with acquire semantics.\n+    return get_forwardee_raw(obj);\n@@ -162,1 +166,1 @@\n-  markWord prev_mark = obj->cas_set_mark(new_mark, old_mark, memory_order_acq_rel);\n+  markWord prev_mark = obj->cas_set_mark(new_mark, old_mark, memory_order_release);\n@@ -166,1 +170,2 @@\n-    return cast_to_oop(prev_mark.clear_lock_bits().to_pointer());\n+    \/\/ Lost the race. Re-read with acquire semantics.\n+    return get_forwardee_raw(obj);\n@@ -170,0 +175,93 @@\n+\/\/ Atomic updates of object with their forwardees. The reason why we need stronger-than-relaxed\n+\/\/ memory ordering has to do with coordination with GC barriers and mutator accesses.\n+\/\/\n+\/\/ In essence, stronger CAS access is required to maintain the transitive chains that mutator\n+\/\/ accesses build by themselves. To illustrate this point, consider the following example.\n+\/\/\n+\/\/ Suppose \"o\" is the object that has a field \"x\" and the reference to \"o\" is stored\n+\/\/ to field at \"addr\", which happens to be Java volatile field. Normally, the accesses to volatile\n+\/\/ field at \"addr\" would be matched with release\/acquire barriers. This changes when GC moves\n+\/\/ the object under mutator feet.\n+\/\/\n+\/\/ Thread 1 (Java)\n+\/\/         \/\/ --- previous access starts here\n+\/\/         ...\n+\/\/   T1.1: store(&o.x, 1, mo_relaxed)\n+\/\/   T1.2: store(&addr, o, mo_release) \/\/ volatile store\n+\/\/\n+\/\/         \/\/ --- new access starts here\n+\/\/         \/\/ LRB: copy and install the new copy to fwdptr\n+\/\/   T1.3: var copy = copy(o)\n+\/\/   T1.4: cas(&fwd, t, copy, mo_release) \/\/ pointer-mediated publication\n+\/\/         <access continues>\n+\/\/\n+\/\/ Thread 2 (GC updater)\n+\/\/   T2.1: var f = load(&fwd, mo_{consume|acquire}) \/\/ pointer-mediated acquisition\n+\/\/   T2.2: cas(&addr, o, f, mo_release) \/\/ this method\n+\/\/\n+\/\/ Thread 3 (Java)\n+\/\/   T3.1: var o = load(&addr, mo_acquire) \/\/ volatile read\n+\/\/   T3.2: if (o != null)\n+\/\/   T3.3:   var r = load(&o.x, mo_relaxed)\n+\/\/\n+\/\/ r is guaranteed to contain \"1\".\n+\/\/\n+\/\/ Without GC involvement, there is synchronizes-with edge from T1.2 to T3.1,\n+\/\/ which guarantees this. With GC involvement, when LRB copies the object and\n+\/\/ another thread updates the reference to it, we need to have the transitive edge\n+\/\/ from T1.4 to T2.1 (that one is guaranteed by forwarding accesses), plus the edge\n+\/\/ from T2.2 to T3.1 (which is brought by this CAS).\n+\/\/\n+\/\/ Note that we do not need to \"acquire\" in these methods, because we do not read the\n+\/\/ failure witnesses contents on any path, and \"release\" is enough.\n+\/\/\n+\/\/ Note: this derivation is valid under quite weak C++ memory model. Real hardware can\n+\/\/ provide the stronger consistency model that would obviate the need for \"release\" here.\n+\/\/ Instead of relaxing everywhere based on specific hardware knowledge, we instead\n+\/\/ provide the \"stable\" fast-path versions of these in the next section.\n+\/\/\n+\n+inline void ShenandoahForwarding::update_with_forwarded(oop obj, oop* addr, oop update) {\n+  assert(is_aligned(addr, HeapWordSize), \"Address should be aligned: \" PTR_FORMAT, p2i(addr));\n+  Atomic::cmpxchg(addr, obj, update, memory_order_release);\n+}\n+\n+inline void ShenandoahForwarding::update_with_forwarded(oop obj, narrowOop* addr, oop update) {\n+  assert(is_aligned(addr, sizeof(narrowOop)), \"Address should be aligned: \" PTR_FORMAT, p2i(addr));\n+  narrowOop o = CompressedOops::encode(obj);\n+  narrowOop u = CompressedOops::encode(update);\n+  Atomic::cmpxchg(addr, o, u, memory_order_release);\n+}\n+\n+inline void ShenandoahForwarding::update_with_forwarded(narrowOop obj, narrowOop* addr, oop update) {\n+  assert(is_aligned(addr, sizeof(narrowOop)), \"Address should be aligned: \" PTR_FORMAT, p2i(addr));\n+  narrowOop u = CompressedOops::encode(update);\n+  Atomic::cmpxchg(addr, obj, u, memory_order_release);\n+}\n+\n+\/*\n+ * Stable versions of the above.\n+ *\n+ * These do not need any special memory semantics, as these are only called when no\n+ * forwardings are being installed concurrently. This is usually happens outside of\n+ * evacuation, during the bulk heap updates.\n+ *\/\n+\n+inline void ShenandoahForwarding::update_with_forwarded_stable(oop obj, oop* addr, oop update) {\n+  assert(is_aligned(addr, HeapWordSize), \"Address should be aligned: \" PTR_FORMAT, p2i(addr));\n+  Atomic::cmpxchg(addr, obj, update, memory_order_relaxed);\n+}\n+\n+inline void ShenandoahForwarding::update_with_forwarded_stable(oop obj, narrowOop* addr, oop update) {\n+  assert(is_aligned(addr, sizeof(narrowOop)), \"Address should be aligned: \" PTR_FORMAT, p2i(addr));\n+  narrowOop o = CompressedOops::encode(obj);\n+  narrowOop u = CompressedOops::encode(update);\n+  Atomic::cmpxchg(addr, o, u, memory_order_relaxed);\n+}\n+\n+inline void ShenandoahForwarding::update_with_forwarded_stable(narrowOop obj, narrowOop* addr, oop update) {\n+  assert(is_aligned(addr, sizeof(narrowOop)), \"Address should be aligned: \" PTR_FORMAT, p2i(addr));\n+  narrowOop u = CompressedOops::encode(update);\n+  Atomic::cmpxchg(addr, obj, u, memory_order_relaxed);\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahForwarding.inline.hpp","additions":101,"deletions":3,"binary":false,"changes":104,"status":"modified"},{"patch":"@@ -638,4 +638,0 @@\n-  static inline void atomic_update_oop(oop update,       oop* addr,       oop compare);\n-  static inline void atomic_update_oop(oop update, narrowOop* addr,       oop compare);\n-  static inline void atomic_update_oop(oop update, narrowOop* addr, narrowOop compare);\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.hpp","additions":0,"deletions":4,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -135,1 +135,1 @@\n-      atomic_update_oop(fwd, p, obj);\n+      ShenandoahForwarding::update_with_forwarded_stable(obj, p, fwd);\n@@ -140,63 +140,0 @@\n-\/\/ Atomic updates of heap location. This is only expected to work with updating the same\n-\/\/ logical object with its forwardee. The reason why we need stronger-than-relaxed memory\n-\/\/ ordering has to do with coordination with GC barriers and mutator accesses.\n-\/\/\n-\/\/ In essence, stronger CAS access is required to maintain the transitive chains that mutator\n-\/\/ accesses build by themselves. To illustrate this point, consider the following example.\n-\/\/\n-\/\/ Suppose \"o\" is the object that has a field \"x\" and the reference to \"o\" is stored\n-\/\/ to field at \"addr\", which happens to be Java volatile field. Normally, the accesses to volatile\n-\/\/ field at \"addr\" would be matched with release\/acquire barriers. This changes when GC moves\n-\/\/ the object under mutator feet.\n-\/\/\n-\/\/ Thread 1 (Java)\n-\/\/         \/\/ --- previous access starts here\n-\/\/         ...\n-\/\/   T1.1: store(&o.x, 1, mo_relaxed)\n-\/\/   T1.2: store(&addr, o, mo_release) \/\/ volatile store\n-\/\/\n-\/\/         \/\/ --- new access starts here\n-\/\/         \/\/ LRB: copy and install the new copy to fwdptr\n-\/\/   T1.3: var copy = copy(o)\n-\/\/   T1.4: cas(&fwd, t, copy, mo_release) \/\/ pointer-mediated publication\n-\/\/         <access continues>\n-\/\/\n-\/\/ Thread 2 (GC updater)\n-\/\/   T2.1: var f = load(&fwd, mo_{consume|acquire}) \/\/ pointer-mediated acquisition\n-\/\/   T2.2: cas(&addr, o, f, mo_release) \/\/ this method\n-\/\/\n-\/\/ Thread 3 (Java)\n-\/\/   T3.1: var o = load(&addr, mo_acquire) \/\/ volatile read\n-\/\/   T3.2: if (o != null)\n-\/\/   T3.3:   var r = load(&o.x, mo_relaxed)\n-\/\/\n-\/\/ r is guaranteed to contain \"1\".\n-\/\/\n-\/\/ Without GC involvement, there is synchronizes-with edge from T1.2 to T3.1,\n-\/\/ which guarantees this. With GC involvement, when LRB copies the object and\n-\/\/ another thread updates the reference to it, we need to have the transitive edge\n-\/\/ from T1.4 to T2.1 (that one is guaranteed by forwarding accesses), plus the edge\n-\/\/ from T2.2 to T3.1 (which is brought by this CAS).\n-\/\/\n-\/\/ Note that we do not need to \"acquire\" in these methods, because we do not read the\n-\/\/ failure witnesses contents on any path, and \"release\" is enough.\n-\/\/\n-\n-inline void ShenandoahHeap::atomic_update_oop(oop update, oop* addr, oop compare) {\n-  assert(is_aligned(addr, HeapWordSize), \"Address should be aligned: \" PTR_FORMAT, p2i(addr));\n-  Atomic::cmpxchg(addr, compare, update, memory_order_release);\n-}\n-\n-inline void ShenandoahHeap::atomic_update_oop(oop update, narrowOop* addr, narrowOop compare) {\n-  assert(is_aligned(addr, sizeof(narrowOop)), \"Address should be aligned: \" PTR_FORMAT, p2i(addr));\n-  narrowOop u = CompressedOops::encode(update);\n-  Atomic::cmpxchg(addr, compare, u, memory_order_release);\n-}\n-\n-inline void ShenandoahHeap::atomic_update_oop(oop update, narrowOop* addr, oop compare) {\n-  assert(is_aligned(addr, sizeof(narrowOop)), \"Address should be aligned: \" PTR_FORMAT, p2i(addr));\n-  narrowOop c = CompressedOops::encode(compare);\n-  narrowOop u = CompressedOops::encode(update);\n-  Atomic::cmpxchg(addr, c, u, memory_order_release);\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.inline.hpp","additions":1,"deletions":64,"binary":false,"changes":65,"status":"modified"}]}