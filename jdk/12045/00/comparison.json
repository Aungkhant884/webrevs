{"files":[{"patch":"@@ -4873,0 +4873,17 @@\n+void Assembler::evpmovzxbd(XMMRegister dst, KRegister mask, Address src, int vector_len) {\n+  assert(VM_Version::supports_avx512vl(), \"\");\n+  assert(dst != xnoreg, \"sanity\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* rex_w *\/ false, \/* legacy_mode *\/ _legacy_mode_bw, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_HVM, \/* input_size_in_bits *\/ EVEX_NObit);\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  attributes.set_is_evex_instruction();\n+  vex_prefix(src, 0, dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8(0x31);\n+  emit_operand(dst, src, 0);\n+}\n+\n+void Assembler::evpmovzxbd(XMMRegister dst, Address src, int vector_len) {\n+  evpmovzxbd(dst, k0, src, vector_len);\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.cpp","additions":17,"deletions":0,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -1876,0 +1876,2 @@\n+  void evpmovzxbd(XMMRegister dst, KRegister mask, Address src, int vector_len);\n+  void evpmovzxbd(XMMRegister dst, Address src, int vector_len);\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n-* Copyright (c) 2021, Intel Corporation. All rights reserved.\n+* Copyright (c) 2021, 2023, Intel Corporation. All rights reserved.\n@@ -36,1 +36,1 @@\n-ATTRIBUTE_ALIGNED(32) juint ADLER32_ASCALE_TABLE[] = {\n+ATTRIBUTE_ALIGNED(64) juint ADLER32_ASCALE_TABLE[] = {\n@@ -38,1 +38,3 @@\n-    0x00000004UL, 0x00000005UL, 0x00000006UL, 0x00000007UL\n+    0x00000004UL, 0x00000005UL, 0x00000006UL, 0x00000007UL,\n+    0x00000008UL, 0x00000009UL, 0x0000000AUL, 0x0000000BUL,\n+    0x0000000CUL, 0x0000000DUL, 0x0000000EUL, 0x0000000FUL\n@@ -70,0 +72,3 @@\n+  \/\/ Choose an appropriate LIMIT for inner loop based on the granularity\n+  \/\/ of intermediate results. For int, LIMIT of 5552 will ensure intermediate\n+  \/\/ results does not overflow Integer.MAX_VALUE before modulo operations.\n@@ -75,1 +80,0 @@\n-\n@@ -104,0 +108,3 @@\n+  const XMMRegister xtmp3 = xmm8;\n+  const XMMRegister xtmp4 = xmm9;\n+  const XMMRegister xtmp5 = xmm10;\n@@ -105,1 +112,1 @@\n-  Label SLOOP1, SLOOP1A, SKIP_LOOP_1A, FINISH, LT64, DO_FINAL, FINAL_LOOP, ZERO_SIZE, END;\n+  Label SLOOP1, SPRELOOP1A_AVX2, SLOOP1A_AVX2, SLOOP1A_AVX3, SKIP_LOOP_1A, SKIP_LOOP_1A_AVX3, FINISH, LT64, DO_FINAL, FINAL_LOOP, ZERO_SIZE, END;\n@@ -109,3 +116,3 @@\n-  __ push(r12);\n-  __ push(r13);\n-  __ push(r14);\n+  __ movq(xtmp3, r12);\n+  __ movq(xtmp4, r13);\n+  __ movq(xtmp5, r14);\n@@ -115,0 +122,1 @@\n+\n@@ -124,1 +132,0 @@\n-  __ vpxor(yb, yb, yb, Assembler::AVX_256bit);\n@@ -135,11 +142,46 @@\n-  __ bind(SLOOP1A);\n-  __ vbroadcastf128(ydata, Address(data, 0), Assembler::AVX_256bit);\n-  __ addptr(data, CHUNKSIZE);\n-  __ vpshufb(ydata0, ydata, yshuf0, Assembler::AVX_256bit);\n-  __ vpaddd(ya, ya, ydata0, Assembler::AVX_256bit);\n-  __ vpaddd(yb, yb, ya, Assembler::AVX_256bit);\n-  __ vpshufb(ydata1, ydata, yshuf1, Assembler::AVX_256bit);\n-  __ vpaddd(ya, ya, ydata1, Assembler::AVX_256bit);\n-  __ vpaddd(yb, yb, ya, Assembler::AVX_256bit);\n-  __ cmpptr(data, end);\n-  __ jcc(Assembler::below, SLOOP1A);\n+  if (VM_Version::supports_avx512vl()) {\n+    __ cmpl(s, VM_Version::avx3_threshold());\n+    __ jcc(Assembler::belowEqual, SPRELOOP1A_AVX2);\n+    \/\/ AVX2 performs better for smaller inputs because of leaner post loop reduction sequence..\n+    __ cmpl(s, 128);\n+    __ jcc(Assembler::belowEqual, SPRELOOP1A_AVX2);\n+    __ vpxor(yb, yb, yb, Assembler::AVX_512bit);\n+\n+    __ bind(SLOOP1A_AVX3);\n+      __ evpmovzxbd(ydata, Address(data, 0), Assembler::AVX_512bit);\n+      __ vpaddd(ya, ya, ydata, Assembler::AVX_512bit);\n+      __ vpaddd(yb, yb, ya, Assembler::AVX_512bit);\n+      __ addptr(data, CHUNKSIZE);\n+      __ cmpptr(data, end);\n+      __ jcc(Assembler::below, SLOOP1A_AVX3);\n+\n+    __ vpslld(yb, yb, 4, Assembler::AVX_512bit); \/\/b is scaled by 16(avx512))\n+    __ vpmulld(ysa, ya, ExternalAddress((address)ADLER32_ASCALE_TABLE), Assembler::AVX_512bit, r14 \/*rscratch*\/);\n+\n+    \/\/ compute horizontal sums of ya, yb, ysa\n+    __ vextracti64x4(xtmp0, ya, 1);\n+    __ vextracti64x4(xtmp1, yb, 1);\n+    __ vextracti64x4(xtmp2, ysa, 1);\n+    __ vpaddd(xtmp0, xtmp0, ya, Assembler::AVX_256bit);\n+    __ vpaddd(xtmp1, xtmp1, yb, Assembler::AVX_256bit);\n+    __ vpaddd(xtmp2, xtmp2, ysa, Assembler::AVX_256bit);\n+    __ vextracti128(xa, xtmp0, 1);\n+    __ vextracti128(xb, xtmp1, 1);\n+    __ vextracti128(xsa, xtmp2, 1);\n+    __ vpaddd(xa, xa, xtmp0, Assembler::AVX_128bit);\n+    __ vpaddd(xb, xb, xtmp1, Assembler::AVX_128bit);\n+    __ vpaddd(xsa, xsa, xtmp2, Assembler::AVX_128bit);\n+    __ vphaddd(xa, xa, xa, Assembler::AVX_128bit);\n+    __ vphaddd(xb, xb, xb, Assembler::AVX_128bit);\n+    __ vphaddd(xsa, xsa, xsa, Assembler::AVX_128bit);\n+    __ vphaddd(xa, xa, xa, Assembler::AVX_128bit);\n+    __ vphaddd(xb, xb, xb, Assembler::AVX_128bit);\n+    __ vphaddd(xsa, xsa, xsa, Assembler::AVX_128bit);\n+\n+    __ vpsubd(xb, xb, xsa, Assembler::AVX_128bit);\n+\n+    __ addptr(end, CHUNKSIZE_M1);\n+    __ testl(s, CHUNKSIZE_M1);\n+    __ jcc(Assembler::notEqual, DO_FINAL);\n+    __ jmp(SKIP_LOOP_1A_AVX3);\n+  }\n@@ -147,4 +189,14 @@\n-  __ bind(SKIP_LOOP_1A);\n-  __ addptr(end, CHUNKSIZE_M1);\n-  __ testl(s, CHUNKSIZE_M1);\n-  __ jcc(Assembler::notEqual, DO_FINAL);\n+  __ align32();\n+  __ bind(SPRELOOP1A_AVX2);\n+  __ vpxor(yb, yb, yb, Assembler::AVX_256bit);\n+  __ bind(SLOOP1A_AVX2);\n+    __ vbroadcastf128(ydata, Address(data, 0), Assembler::AVX_256bit);\n+    __ addptr(data, CHUNKSIZE);\n+    __ vpshufb(ydata0, ydata, yshuf0, Assembler::AVX_256bit);\n+    __ vpaddd(ya, ya, ydata0, Assembler::AVX_256bit);\n+    __ vpaddd(yb, yb, ya, Assembler::AVX_256bit);\n+    __ vpshufb(ydata1, ydata, yshuf1, Assembler::AVX_256bit);\n+    __ vpaddd(ya, ya, ydata1, Assembler::AVX_256bit);\n+    __ vpaddd(yb, yb, ya, Assembler::AVX_256bit);\n+    __ cmpptr(data, end);\n+    __ jcc(Assembler::below, SLOOP1A_AVX2);\n@@ -152,2 +204,1 @@\n-  \/\/ either we're done, or we just did LIMIT\n-  __ subl(size, s);\n+  __ bind(SKIP_LOOP_1A);\n@@ -156,1 +207,1 @@\n-  __ vpslld(yb, yb, 3, Assembler::AVX_256bit); \/\/b is scaled by 8\n+  __ vpslld(yb, yb, 3, Assembler::AVX_256bit); \/\/b is scaled by 8(avx)\n@@ -173,0 +224,10 @@\n+  __ vpsubd(xb, xb, xsa, Assembler::AVX_128bit);\n+\n+  __ addptr(end, CHUNKSIZE_M1);\n+  __ testl(s, CHUNKSIZE_M1);\n+  __ jcc(Assembler::notEqual, DO_FINAL);\n+\n+  __ bind(SKIP_LOOP_1A_AVX3);\n+  \/\/ either we're done, or we just did LIMIT\n+  __ subl(size, s);\n+\n@@ -179,1 +240,0 @@\n-  __ vpsubd(xb, xb, xsa, Assembler::AVX_128bit);\n@@ -192,1 +252,0 @@\n-  __ vpxor(yb, yb, yb, Assembler::AVX_256bit);\n@@ -208,1 +267,0 @@\n-  \/\/ handle remaining 1...15 bytes\n@@ -210,18 +268,0 @@\n-  \/\/ reduce\n-  __ vpslld(yb, yb, 3, Assembler::AVX_256bit); \/\/b is scaled by 8\n-  __ vpmulld(ysa, ya, ExternalAddress((address)ADLER32_ASCALE_TABLE), Assembler::AVX_256bit, r14 \/*rscratch*\/); \/\/scaled a\n-\n-  __ vextracti128(xtmp0, ya, 1);\n-  __ vextracti128(xtmp1, yb, 1);\n-  __ vextracti128(xtmp2, ysa, 1);\n-  __ vpaddd(xa, xa, xtmp0, Assembler::AVX_128bit);\n-  __ vpaddd(xb, xb, xtmp1, Assembler::AVX_128bit);\n-  __ vpaddd(xsa, xsa, xtmp2, Assembler::AVX_128bit);\n-  __ vphaddd(xa, xa, xa, Assembler::AVX_128bit);\n-  __ vphaddd(xb, xb, xb, Assembler::AVX_128bit);\n-  __ vphaddd(xsa, xsa, xsa, Assembler::AVX_128bit);\n-  __ vphaddd(xa, xa, xa, Assembler::AVX_128bit);\n-  __ vphaddd(xb, xb, xb, Assembler::AVX_128bit);\n-  __ vphaddd(xsa, xsa, xsa, Assembler::AVX_128bit);\n-  __ vpsubd(xb, xb, xsa, Assembler::AVX_128bit);\n-\n@@ -258,3 +298,4 @@\n-  __ pop(r14);\n-  __ pop(r13);\n-  __ pop(r12);\n+\n+  __ movq(r14, xtmp5);\n+  __ movq(r13, xtmp4);\n+  __ movq(r12, xtmp3);\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64_adler.cpp","additions":92,"deletions":51,"binary":false,"changes":143,"status":"modified"},{"patch":"@@ -30,2 +30,2 @@\n-@BenchmarkMode(Mode.AverageTime)\n-@OutputTimeUnit(TimeUnit.MICROSECONDS)\n+@BenchmarkMode(Mode.Throughput)\n+@OutputTimeUnit(TimeUnit.MILLISECONDS)\n@@ -33,4 +33,0 @@\n-@Fork(value = 2)\n-@Warmup(iterations = 2, time = 30, timeUnit = TimeUnit.SECONDS)\n-@Measurement(iterations = 3, time = 60, timeUnit = TimeUnit.SECONDS)\n-\n@@ -43,1 +39,1 @@\n-    @Param({\"64\", \"128\", \"256\", \"512\", \/* \"1024\", *\/ \"2048\", \/* \"4096\", \"8192\", *\/ \"16384\", \/* \"32768\", *\/ \"65536\"})\n+    @Param({\"64\", \"128\", \"256\", \"512\", \"1024\",  \"2048\", \"5012\", \"8192\", \"16384\", \"32768\", \"65536\"})\n","filename":"test\/micro\/org\/openjdk\/bench\/java\/util\/TestAdler32.java","additions":3,"deletions":7,"binary":false,"changes":10,"status":"modified"}]}