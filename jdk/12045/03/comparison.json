{"files":[{"patch":"@@ -4873,0 +4873,17 @@\n+void Assembler::evpmovzxbd(XMMRegister dst, KRegister mask, Address src, int vector_len) {\n+  assert(VM_Version::supports_avx512vl(), \"\");\n+  assert(dst != xnoreg, \"sanity\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* rex_w *\/ false, \/* legacy_mode *\/ _legacy_mode_bw, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_HVM, \/* input_size_in_bits *\/ EVEX_NObit);\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  attributes.set_is_evex_instruction();\n+  vex_prefix(src, 0, dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8(0x31);\n+  emit_operand(dst, src, 0);\n+}\n+\n+void Assembler::evpmovzxbd(XMMRegister dst, Address src, int vector_len) {\n+  evpmovzxbd(dst, k0, src, vector_len);\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.cpp","additions":17,"deletions":0,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -1876,0 +1876,2 @@\n+  void evpmovzxbd(XMMRegister dst, KRegister mask, Address src, int vector_len);\n+  void evpmovzxbd(XMMRegister dst, Address src, int vector_len);\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n-* Copyright (c) 2021, Intel Corporation. All rights reserved.\n+* Copyright (c) 2021, 2023, Intel Corporation. All rights reserved.\n@@ -29,0 +29,1 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -36,1 +37,1 @@\n-ATTRIBUTE_ALIGNED(32) juint ADLER32_ASCALE_TABLE[] = {\n+ATTRIBUTE_ALIGNED(64) juint ADLER32_ASCALE_TABLE[] = {\n@@ -38,1 +39,3 @@\n-    0x00000004UL, 0x00000005UL, 0x00000006UL, 0x00000007UL\n+    0x00000004UL, 0x00000005UL, 0x00000006UL, 0x00000007UL,\n+    0x00000008UL, 0x00000009UL, 0x0000000AUL, 0x0000000BUL,\n+    0x0000000CUL, 0x0000000DUL, 0x0000000EUL, 0x0000000FUL\n@@ -47,2 +50,2 @@\n-    0xFFFFFF08UL, 0xFFFFFF09, 0xFFFFFF0AUL, 0xFFFFFF0BUL,\n-    0xFFFFFF0CUL, 0xFFFFFF0D, 0xFFFFFF0EUL, 0xFFFFFF0FUL\n+    0xFFFFFF08UL, 0xFFFFFF09UL, 0xFFFFFF0AUL, 0xFFFFFF0BUL,\n+    0xFFFFFF0CUL, 0xFFFFFF0DUL, 0xFFFFFF0EUL, 0xFFFFFF0FUL\n@@ -70,0 +73,3 @@\n+  \/\/ Choose an appropriate LIMIT for inner loop based on the granularity\n+  \/\/ of intermediate results. For int, LIMIT of 5552 will ensure intermediate\n+  \/\/ results does not overflow Integer.MAX_VALUE before modulo operations.\n@@ -75,1 +81,0 @@\n-\n@@ -104,0 +109,3 @@\n+  const XMMRegister xtmp3 = xmm8;\n+  const XMMRegister xtmp4 = xmm9;\n+  const XMMRegister xtmp5 = xmm10;\n@@ -105,1 +113,2 @@\n-  Label SLOOP1, SLOOP1A, SKIP_LOOP_1A, FINISH, LT64, DO_FINAL, FINAL_LOOP, ZERO_SIZE, END;\n+  Label SLOOP1, SPRELOOP1A_AVX2, SLOOP1A_AVX2, SLOOP1A_AVX3, AVX3_REDUCE, SKIP_LOOP_1A;\n+  Label SKIP_LOOP_1A_AVX3, FINISH, LT64, DO_FINAL, FINAL_LOOP, ZERO_SIZE, END;\n@@ -109,3 +118,3 @@\n-  __ push(r12);\n-  __ push(r13);\n-  __ push(r14);\n+  __ movq(xtmp3, r12);\n+  __ movq(xtmp4, r13);\n+  __ movq(xtmp5, r14);\n@@ -115,0 +124,1 @@\n+\n@@ -124,1 +134,0 @@\n-  __ vpxor(yb, yb, yb, Assembler::AVX_256bit);\n@@ -135,11 +144,77 @@\n-  __ bind(SLOOP1A);\n-  __ vbroadcastf128(ydata, Address(data, 0), Assembler::AVX_256bit);\n-  __ addptr(data, CHUNKSIZE);\n-  __ vpshufb(ydata0, ydata, yshuf0, Assembler::AVX_256bit);\n-  __ vpaddd(ya, ya, ydata0, Assembler::AVX_256bit);\n-  __ vpaddd(yb, yb, ya, Assembler::AVX_256bit);\n-  __ vpshufb(ydata1, ydata, yshuf1, Assembler::AVX_256bit);\n-  __ vpaddd(ya, ya, ydata1, Assembler::AVX_256bit);\n-  __ vpaddd(yb, yb, ya, Assembler::AVX_256bit);\n-  __ cmpptr(data, end);\n-  __ jcc(Assembler::below, SLOOP1A);\n+  if (VM_Version::supports_avx512vl()) {\n+    \/\/ AVX2 performs better for smaller inputs because of leaner post loop reduction sequence..\n+    __ cmpl(s, MAX2(128, VM_Version::avx3_threshold()));\n+    __ jcc(Assembler::belowEqual, SPRELOOP1A_AVX2);\n+\n+    __ lea(end, Address(s, data, Address::times_1, - (2*CHUNKSIZE -1)));\n+    __ vpxor(yb, yb, yb, Assembler::AVX_512bit);\n+\n+    \/\/ Some notes on vectorized main loop algorithm.\n+    \/\/ Additions are performed in slices of 16 bytes in the main loop.\n+    \/\/ input size : 64 bytes (a0 - a63).\n+    \/\/ Iteration0 : ya =  [a0 - a15]\n+    \/\/              yb =  [a0 - a15]\n+    \/\/ Iteration1 : ya =  [a0 - a15] + [a16 - a31]\n+    \/\/              yb =  2 x [a0 - a15] + [a16 - a31]\n+    \/\/ Iteration2 : ya =  [a0 - a15] + [a16 - a31] + [a32 - a47]\n+    \/\/              yb =  3 x [a0 - a15] + 2 x [a16 - a31] + [a32 - a47]\n+    \/\/ Iteration4 : ya =  [a0 - a15] + [a16 - a31] + [a32 - a47] + [a48 - a63]\n+    \/\/              yb =  4 x [a0 - a15] + 3 x [a16 - a31] + 2 x [a32 - a47] + [a48 - a63]\n+    \/\/ Before performing reduction we must scale the intermediate result appropriately.\n+    \/\/ Since addition was performed in chunks of 16 bytes, thus to match the scalar implementation\n+    \/\/ Oth lane element must be repeatedly added 16 times, 1st element 15 times and so on so forth.\n+    \/\/ Thus we first multiply yb by 16 followed by subtracting appropriately scaled ya value.\n+    \/\/ yb = 16 x yb  - [0 - 15] x ya\n+    \/\/    = 64 x [0 - 15] + 48 x [16 - 31] + 32 x [32 - 47] + 16 x [48 - 63]  -  [0 - 15] x ya\n+    \/\/    = 64 x a0 + 63 x a1 + 62 x a2 ...... + a63\n+    __ bind(SLOOP1A_AVX3);\n+      __ evpmovzxbd(ydata0, Address(data, 0), Assembler::AVX_512bit);\n+      __ evpmovzxbd(ydata1, Address(data, CHUNKSIZE), Assembler::AVX_512bit);\n+      __ vpaddd(ya, ya, ydata0, Assembler::AVX_512bit);\n+      __ vpaddd(yb, yb, ya, Assembler::AVX_512bit);\n+      __ vpaddd(ya, ya, ydata1, Assembler::AVX_512bit);\n+      __ vpaddd(yb, yb, ya, Assembler::AVX_512bit);\n+      __ addptr(data, 2*CHUNKSIZE);\n+      __ cmpptr(data, end);\n+      __ jcc(Assembler::below, SLOOP1A_AVX3);\n+\n+    __ addptr(end, CHUNKSIZE);\n+    __ cmpptr(data, end);\n+    __ jcc(Assembler::aboveEqual, AVX3_REDUCE);\n+\n+    __ evpmovzxbd(ydata0, Address(data, 0), Assembler::AVX_512bit);\n+    __ vpaddd(ya, ya, ydata0, Assembler::AVX_512bit);\n+    __ vpaddd(yb, yb, ya, Assembler::AVX_512bit);\n+    __ addptr(data, CHUNKSIZE);\n+\n+    __ bind(AVX3_REDUCE);\n+    __ vpslld(yb, yb, 4, Assembler::AVX_512bit); \/\/b is scaled by 16(avx512))\n+    __ vpmulld(ysa, ya, ExternalAddress((address)ADLER32_ASCALE_TABLE), Assembler::AVX_512bit, r14 \/*rscratch*\/);\n+\n+    \/\/ compute horizontal sums of ya, yb, ysa\n+    __ vextracti64x4(xtmp0, ya, 1);\n+    __ vextracti64x4(xtmp1, yb, 1);\n+    __ vextracti64x4(xtmp2, ysa, 1);\n+    __ vpaddd(xtmp0, xtmp0, ya, Assembler::AVX_256bit);\n+    __ vpaddd(xtmp1, xtmp1, yb, Assembler::AVX_256bit);\n+    __ vpaddd(xtmp2, xtmp2, ysa, Assembler::AVX_256bit);\n+    __ vextracti128(xa, xtmp0, 1);\n+    __ vextracti128(xb, xtmp1, 1);\n+    __ vextracti128(xsa, xtmp2, 1);\n+    __ vpaddd(xa, xa, xtmp0, Assembler::AVX_128bit);\n+    __ vpaddd(xb, xb, xtmp1, Assembler::AVX_128bit);\n+    __ vpaddd(xsa, xsa, xtmp2, Assembler::AVX_128bit);\n+    __ vphaddd(xa, xa, xa, Assembler::AVX_128bit);\n+    __ vphaddd(xb, xb, xb, Assembler::AVX_128bit);\n+    __ vphaddd(xsa, xsa, xsa, Assembler::AVX_128bit);\n+    __ vphaddd(xa, xa, xa, Assembler::AVX_128bit);\n+    __ vphaddd(xb, xb, xb, Assembler::AVX_128bit);\n+    __ vphaddd(xsa, xsa, xsa, Assembler::AVX_128bit);\n+\n+    __ vpsubd(xb, xb, xsa, Assembler::AVX_128bit);\n+\n+    __ addptr(end, CHUNKSIZE_M1);\n+    __ testl(s, CHUNKSIZE_M1);\n+    __ jcc(Assembler::notEqual, DO_FINAL);\n+    __ jmp(SKIP_LOOP_1A_AVX3);\n+  }\n@@ -147,4 +222,14 @@\n-  __ bind(SKIP_LOOP_1A);\n-  __ addptr(end, CHUNKSIZE_M1);\n-  __ testl(s, CHUNKSIZE_M1);\n-  __ jcc(Assembler::notEqual, DO_FINAL);\n+  __ align32();\n+  __ bind(SPRELOOP1A_AVX2);\n+  __ vpxor(yb, yb, yb, Assembler::AVX_256bit);\n+  __ bind(SLOOP1A_AVX2);\n+    __ vbroadcastf128(ydata, Address(data, 0), Assembler::AVX_256bit);\n+    __ addptr(data, CHUNKSIZE);\n+    __ vpshufb(ydata0, ydata, yshuf0, Assembler::AVX_256bit);\n+    __ vpaddd(ya, ya, ydata0, Assembler::AVX_256bit);\n+    __ vpaddd(yb, yb, ya, Assembler::AVX_256bit);\n+    __ vpshufb(ydata1, ydata, yshuf1, Assembler::AVX_256bit);\n+    __ vpaddd(ya, ya, ydata1, Assembler::AVX_256bit);\n+    __ vpaddd(yb, yb, ya, Assembler::AVX_256bit);\n+    __ cmpptr(data, end);\n+    __ jcc(Assembler::below, SLOOP1A_AVX2);\n@@ -152,2 +237,1 @@\n-  \/\/ either we're done, or we just did LIMIT\n-  __ subl(size, s);\n+  __ bind(SKIP_LOOP_1A);\n@@ -156,1 +240,1 @@\n-  __ vpslld(yb, yb, 3, Assembler::AVX_256bit); \/\/b is scaled by 8\n+  __ vpslld(yb, yb, 3, Assembler::AVX_256bit); \/\/b is scaled by 8(avx)\n@@ -173,0 +257,10 @@\n+  __ vpsubd(xb, xb, xsa, Assembler::AVX_128bit);\n+\n+  __ addptr(end, CHUNKSIZE_M1);\n+  __ testl(s, CHUNKSIZE_M1);\n+  __ jcc(Assembler::notEqual, DO_FINAL);\n+\n+  __ bind(SKIP_LOOP_1A_AVX3);\n+  \/\/ either we're done, or we just did LIMIT\n+  __ subl(size, s);\n+\n@@ -179,1 +273,0 @@\n-  __ vpsubd(xb, xb, xsa, Assembler::AVX_128bit);\n@@ -192,1 +285,0 @@\n-  __ vpxor(yb, yb, yb, Assembler::AVX_256bit);\n@@ -208,1 +300,0 @@\n-  \/\/ handle remaining 1...15 bytes\n@@ -210,18 +301,0 @@\n-  \/\/ reduce\n-  __ vpslld(yb, yb, 3, Assembler::AVX_256bit); \/\/b is scaled by 8\n-  __ vpmulld(ysa, ya, ExternalAddress((address)ADLER32_ASCALE_TABLE), Assembler::AVX_256bit, r14 \/*rscratch*\/); \/\/scaled a\n-\n-  __ vextracti128(xtmp0, ya, 1);\n-  __ vextracti128(xtmp1, yb, 1);\n-  __ vextracti128(xtmp2, ysa, 1);\n-  __ vpaddd(xa, xa, xtmp0, Assembler::AVX_128bit);\n-  __ vpaddd(xb, xb, xtmp1, Assembler::AVX_128bit);\n-  __ vpaddd(xsa, xsa, xtmp2, Assembler::AVX_128bit);\n-  __ vphaddd(xa, xa, xa, Assembler::AVX_128bit);\n-  __ vphaddd(xb, xb, xb, Assembler::AVX_128bit);\n-  __ vphaddd(xsa, xsa, xsa, Assembler::AVX_128bit);\n-  __ vphaddd(xa, xa, xa, Assembler::AVX_128bit);\n-  __ vphaddd(xb, xb, xb, Assembler::AVX_128bit);\n-  __ vphaddd(xsa, xsa, xsa, Assembler::AVX_128bit);\n-  __ vpsubd(xb, xb, xsa, Assembler::AVX_128bit);\n-\n@@ -258,3 +331,4 @@\n-  __ pop(r14);\n-  __ pop(r13);\n-  __ pop(r12);\n+\n+  __ movq(r14, xtmp5);\n+  __ movq(r13, xtmp4);\n+  __ movq(r12, xtmp3);\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64_adler.cpp","additions":127,"deletions":53,"binary":false,"changes":180,"status":"modified"},{"patch":"@@ -149,1 +149,1 @@\n-            return false;\n+            throw new AssertionError(\"TEST FAILED\", null);\n","filename":"test\/hotspot\/jtreg\/compiler\/intrinsics\/zip\/TestAdler32.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -30,2 +30,2 @@\n-@BenchmarkMode(Mode.AverageTime)\n-@OutputTimeUnit(TimeUnit.MICROSECONDS)\n+@BenchmarkMode(Mode.Throughput)\n+@OutputTimeUnit(TimeUnit.MILLISECONDS)\n@@ -33,4 +33,0 @@\n-@Fork(value = 2)\n-@Warmup(iterations = 2, time = 30, timeUnit = TimeUnit.SECONDS)\n-@Measurement(iterations = 3, time = 60, timeUnit = TimeUnit.SECONDS)\n-\n@@ -43,1 +39,1 @@\n-    @Param({\"64\", \"128\", \"256\", \"512\", \/* \"1024\", *\/ \"2048\", \/* \"4096\", \"8192\", *\/ \"16384\", \/* \"32768\", *\/ \"65536\"})\n+    @Param({\"64\", \"128\", \"256\", \"512\", \"1024\", \"2048\", \"5012\", \"8192\", \"16384\", \"32768\", \"65536\"})\n","filename":"test\/micro\/org\/openjdk\/bench\/java\/util\/TestAdler32.java","additions":3,"deletions":7,"binary":false,"changes":10,"status":"modified"}]}