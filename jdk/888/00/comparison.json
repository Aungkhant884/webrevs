{"files":[{"patch":"@@ -932,2 +932,0 @@\n-  set_do_count_invocations(false);\n-  set_do_method_data_update(false);\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -285,2 +285,0 @@\n-  bool                  _do_count_invocations;  \/\/ True if we generate code to count invocations\n-  bool                  _do_method_data_update; \/\/ True if we generate code to update MethodData*s\n@@ -574,4 +572,0 @@\n-  bool              do_count_invocations() const{ return _do_count_invocations; }\n-  void          set_do_count_invocations(bool z){ _do_count_invocations = z; }\n-  bool              do_method_data_update() const { return _do_method_data_update; }\n-  void          set_do_method_data_update(bool z) { _do_method_data_update = z; }\n","filename":"src\/hotspot\/share\/opto\/compile.hpp","additions":0,"deletions":6,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -646,4 +646,0 @@\n-  \/\/ Bump method data counters (We profile *before* the call is made\n-  \/\/ because exceptions don't return to the call site.)\n-  profile_call(receiver);\n-\n","filename":"src\/hotspot\/share\/opto\/doCall.cpp","additions":0,"deletions":4,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -333,2 +333,0 @@\n-  bool          _count_invocations;  \/\/ update and test invocation counter\n-  bool          _method_data_update; \/\/ update method data oop\n@@ -380,2 +378,0 @@\n-  bool          count_invocations() const  { return _count_invocations; }\n-  bool          method_data_update() const { return _method_data_update; }\n@@ -503,3 +499,0 @@\n-  \/\/ Helper function to setup for type-profile based inlining\n-  bool prepare_type_profile_inline(ciInstanceKlass* prof_klass, ciMethod* prof_method);\n-\n@@ -558,3 +551,3 @@\n-  void    jump_if_true_fork(IfNode *ifNode, int dest_bci_if_true, int prof_table_index, bool unc);\n-  void    jump_if_false_fork(IfNode *ifNode, int dest_bci_if_false, int prof_table_index, bool unc);\n-  void    jump_if_always_fork(int dest_bci_if_true, int prof_table_index, bool unc);\n+  void    jump_if_true_fork(IfNode *ifNode, int dest_bci_if_true, bool unc);\n+  void    jump_if_false_fork(IfNode *ifNode, int dest_bci_if_false, bool unc);\n+  void    jump_if_always_fork(int dest_bci_if_true, bool unc);\n@@ -570,17 +563,0 @@\n-  \/\/ helper functions for methodData style profiling\n-  void test_counter_against_threshold(Node* cnt, int limit);\n-  void increment_and_test_invocation_counter(int limit);\n-  void test_for_osr_md_counter_at(ciMethodData* md, ciProfileData* data, ByteSize offset, int limit);\n-  Node* method_data_addressing(ciMethodData* md, ciProfileData* data, ByteSize offset, Node* idx = NULL, uint stride = 0);\n-  void increment_md_counter_at(ciMethodData* md, ciProfileData* data, ByteSize offset, Node* idx = NULL, uint stride = 0);\n-  void set_md_flag_at(ciMethodData* md, ciProfileData* data, int flag_constant);\n-\n-  void profile_method_entry();\n-  void profile_taken_branch(int target_bci, bool force_update = false);\n-  void profile_not_taken_branch(bool force_update = false);\n-  void profile_call(Node* receiver);\n-  void profile_generic_call();\n-  void profile_receiver_type(Node* receiver);\n-  void profile_ret(int target_bci);\n-  void profile_null_checkcast();\n-  void profile_switch_case(int table_index);\n","filename":"src\/hotspot\/share\/opto\/parse.hpp","additions":3,"deletions":27,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -486,3 +486,0 @@\n-  _count_invocations = C->do_count_invocations();\n-  _method_data_update = C->do_method_data_update();\n-\n@@ -1231,4 +1228,0 @@\n-\n-  if (depth() == 1) {\n-    increment_and_test_invocation_counter(Tier2CompileThreshold);\n-  }\n","filename":"src\/hotspot\/share\/opto\/parse1.cpp","additions":0,"deletions":7,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -235,1 +235,1 @@\n-void Parse::jump_if_true_fork(IfNode *iff, int dest_bci_if_true, int prof_table_index, bool unc) {\n+void Parse::jump_if_true_fork(IfNode *iff, int dest_bci_if_true, bool unc) {\n@@ -248,1 +248,0 @@\n-      profile_switch_case(prof_table_index);\n@@ -258,1 +257,1 @@\n-void Parse::jump_if_false_fork(IfNode *iff, int dest_bci_if_true, int prof_table_index, bool unc) {\n+void Parse::jump_if_false_fork(IfNode *iff, int dest_bci_if_true, bool unc) {\n@@ -271,1 +270,0 @@\n-      profile_switch_case(prof_table_index);\n@@ -281,1 +279,1 @@\n-void Parse::jump_if_always_fork(int dest_bci, int prof_table_index, bool unc) {\n+void Parse::jump_if_always_fork(int dest_bci, bool unc) {\n@@ -291,1 +289,0 @@\n-    profile_switch_case(prof_table_index);\n@@ -306,4 +303,0 @@\n-\/\/ Default value for methodData switch indexing. Must be a negative value to avoid\n-\/\/ conflict with any legal switch index.\n-#define NullTableIndex -1\n-\n@@ -315,1 +308,0 @@\n-  int _table_index;             \/\/ index into method data table\n@@ -322,1 +314,0 @@\n-  int  table_index() const     { return _table_index; }\n@@ -326,1 +317,1 @@\n-  void setRange(jint lo, jint hi, int dest, int table_index, float cnt) {\n+  void setRange(jint lo, jint hi, int dest, float cnt) {\n@@ -328,1 +319,1 @@\n-    _lo = lo, _hi = hi; _dest = dest; _table_index = table_index; _cnt = cnt;\n+    _lo = lo, _hi = hi; _dest = dest; _cnt = cnt;\n@@ -331,1 +322,1 @@\n-  bool adjoinRange(jint lo, jint hi, int dest, int table_index, float cnt, bool trim_ranges) {\n+  bool adjoinRange(jint lo, jint hi, int dest, float cnt, bool trim_ranges) {\n@@ -333,1 +324,1 @@\n-    if (lo == _hi+1 && table_index == _table_index) {\n+    if (lo == _hi+1) {\n@@ -363,2 +354,2 @@\n-  void set (jint value, int dest, int table_index, float cnt) {\n-    setRange(value, value, dest, table_index, cnt);\n+  void set (jint value, int dest, float cnt) {\n+    setRange(value, value, dest, cnt);\n@@ -366,2 +357,2 @@\n-  bool adjoin(jint value, int dest, int table_index, float cnt, bool trim_ranges) {\n-    return adjoinRange(value, value, dest, table_index, cnt, trim_ranges);\n+  bool adjoin(jint value, int dest, float cnt, bool trim_ranges) {\n+    return adjoinRange(value, value, dest, cnt, trim_ranges);\n@@ -370,1 +361,1 @@\n-    return adjoinRange(other._lo, other._hi, other._dest, other._table_index, other._cnt, false);\n+    return adjoinRange(other._lo, other._hi, other._dest, other._cnt, false);\n@@ -421,1 +412,1 @@\n-      r.setRange(r.lo(), r.hi(), never_reached, r.table_index(), r.cnt());\n+      r.setRange(r.lo(), r.hi(), never_reached, r.cnt());\n@@ -450,1 +441,1 @@\n-  bool trim_ranges = !method_data_update() && !C->too_many_traps(method(), bci(), Deoptimization::Reason_unstable_if);\n+  bool trim_ranges = !C->too_many_traps(method(), bci(), Deoptimization::Reason_unstable_if);\n@@ -462,1 +453,1 @@\n-    ranges[++rp].setRange(min_jint, lo_index-1, default_dest, NullTableIndex, cnt);\n+    ranges[++rp].setRange(min_jint, lo_index-1, default_dest, cnt);\n@@ -468,1 +459,0 @@\n-    int  table_index = method_data_update() ? j : NullTableIndex;\n@@ -473,2 +463,2 @@\n-    if (rp < 0 || !ranges[rp].adjoin(match_int, dest, table_index, cnt, trim_ranges)) {\n-      ranges[++rp].set(match_int, dest, table_index, cnt);\n+    if (rp < 0 || !ranges[rp].adjoin(match_int, dest, cnt, trim_ranges)) {\n+      ranges[++rp].set(match_int, dest, cnt);\n@@ -484,2 +474,2 @@\n-    if (!ranges[rp].adjoinRange(highest+1, max_jint, default_dest, NullTableIndex, cnt, trim_ranges)) {\n-      ranges[++rp].setRange(highest+1, max_jint, default_dest, NullTableIndex, cnt);\n+    if (!ranges[rp].adjoinRange(highest+1, max_jint, default_dest, cnt, trim_ranges)) {\n+      ranges[++rp].setRange(highest+1, max_jint, default_dest, cnt);\n@@ -523,1 +513,1 @@\n-  bool trim_ranges = !method_data_update() && !C->too_many_traps(method(), bci(), Deoptimization::Reason_unstable_if);\n+  bool trim_ranges = !C->too_many_traps(method(), bci(), Deoptimization::Reason_unstable_if);\n@@ -563,1 +553,0 @@\n-    int  table_index = method_data_update() ? j : NullTableIndex;\n@@ -566,1 +555,1 @@\n-    if (match_int != next_lo && (rp < 0 || !ranges[rp].adjoinRange(next_lo, match_int-1, default_dest, NullTableIndex, c, trim_ranges))) {\n+    if (match_int != next_lo && (rp < 0 || !ranges[rp].adjoinRange(next_lo, match_int-1, default_dest, c, trim_ranges))) {\n@@ -568,1 +557,1 @@\n-      ranges[++rp].setRange(next_lo, match_int-1, default_dest, NullTableIndex, c);\n+      ranges[++rp].setRange(next_lo, match_int-1, default_dest, c);\n@@ -570,1 +559,1 @@\n-    if (rp < 0 || !ranges[rp].adjoin(match_int, dest, table_index, cnt, trim_ranges)) {\n+    if (rp < 0 || !ranges[rp].adjoin(match_int, dest, cnt, trim_ranges)) {\n@@ -572,1 +561,1 @@\n-      ranges[++rp].set(match_int, dest, table_index, cnt);\n+      ranges[++rp].set(match_int, dest, cnt);\n@@ -578,2 +567,2 @@\n-      !ranges[rp].adjoinRange(highest+1, max_jint, default_dest, NullTableIndex, default_cnt * ((float)max_jint - highest), trim_ranges)) {\n-    ranges[++rp].setRange(highest+1, max_jint, default_dest, NullTableIndex, default_cnt * ((float)max_jint - highest));\n+      !ranges[rp].adjoinRange(highest+1, max_jint, default_dest, default_cnt * ((float)max_jint - highest), trim_ranges)) {\n+    ranges[++rp].setRange(highest+1, max_jint, default_dest, default_cnt * ((float)max_jint - highest));\n@@ -738,1 +727,1 @@\n-          prev.setRange(prev.lo(), sr->hi(), prev.dest(), prev.table_index(), prev.cnt());\n+          prev.setRange(prev.lo(), sr->hi(), prev.dest(), prev.cnt());\n@@ -765,1 +754,1 @@\n-    jump_if_true_fork(iff, most_freq.dest(), most_freq.table_index(), false);\n+    jump_if_true_fork(iff, most_freq.dest(), false);\n@@ -781,3 +770,0 @@\n-  \/\/ Don't make jump table if profiling\n-  if (method_data_update())  return false;\n-\n@@ -861,1 +847,1 @@\n-    jump_if_true_fork(iff, default_dest, NullTableIndex, trim_ranges && trimmed_cnt == 0);\n+    jump_if_true_fork(iff, default_dest, trim_ranges && trimmed_cnt == 0);\n@@ -921,1 +907,1 @@\n-        jump_if_always_fork(r->dest(), r->table_index(), trim_ranges && r->cnt() == 0);\n+        jump_if_always_fork(r->dest(), trim_ranges && r->cnt() == 0);\n@@ -933,1 +919,1 @@\n-  bool trim_ranges = !method_data_update() && !C->too_many_traps(method(), bci(), Deoptimization::Reason_unstable_if);\n+  bool trim_ranges = !C->too_many_traps(method(), bci(), Deoptimization::Reason_unstable_if);\n@@ -974,1 +960,1 @@\n-      lo->setRange(min_val, lo->hi(), lo->dest(), lo->table_index(), lo->cnt());\n+      lo->setRange(min_val, lo->hi(), lo->dest(), lo->cnt());\n@@ -980,1 +966,1 @@\n-      hi->setRange(hi->lo(), max_val, hi->dest(), hi->table_index(), hi->cnt());\n+      hi->setRange(hi->lo(), max_val, hi->dest(), hi->cnt());\n@@ -995,1 +981,1 @@\n-    jump_if_always_fork(lo->dest(), lo->table_index(), trim_ranges && lo->cnt() == 0);\n+    jump_if_always_fork(lo->dest(), trim_ranges && lo->cnt() == 0);\n@@ -1033,1 +1019,1 @@\n-      jump_if_false_fork(iff_ne, mid->dest(), mid->table_index(), trim_ranges && mid->cnt() == 0);\n+      jump_if_false_fork(iff_ne, mid->dest(), trim_ranges && mid->cnt() == 0);\n@@ -1062,1 +1048,1 @@\n-        jump_if_true_fork(iff_ge, mid->dest(), mid->table_index(), trim_ranges && cnt == 0);\n+        jump_if_true_fork(iff_ge, mid->dest(), trim_ranges && cnt == 0);\n@@ -1079,1 +1065,1 @@\n-        jump_if_always_fork(lo->dest(), lo->table_index(), trim_ranges && lo->cnt() == 0);\n+        jump_if_always_fork(lo->dest(), trim_ranges && lo->cnt() == 0);\n@@ -1214,3 +1200,0 @@\n-  \/\/ Update method data\n-  profile_taken_branch(jsr_bci);\n-\n@@ -1238,1 +1221,0 @@\n-  profile_ret(target->flow()->start());\n@@ -1450,5 +1432,0 @@\n-    \/\/ We need to mark this branch as taken so that if we recompile we will\n-    \/\/ see that it is possible. In the tiered system the interpreter doesn't\n-    \/\/ do profiling and by the time we get to the lower tier from the interpreter\n-    \/\/ the path may be cold again. Make sure it doesn't look untaken\n-    profile_taken_branch(target_bci, !ProfileInterpreter);\n@@ -1488,2 +1465,0 @@\n-      \/\/ Update method data\n-      profile_taken_branch(target_bci);\n@@ -1508,2 +1483,0 @@\n-    \/\/ Update method data\n-    profile_not_taken_branch();\n@@ -1531,5 +1504,0 @@\n-    \/\/ We need to mark this branch as taken so that if we recompile we will\n-    \/\/ see that it is possible. In the tiered system the interpreter doesn't\n-    \/\/ do profiling and by the time we get to the lower tier from the interpreter\n-    \/\/ the path may be cold again. Make sure it doesn't look untaken\n-    profile_taken_branch(target_bci, !ProfileInterpreter);\n@@ -1610,2 +1578,0 @@\n-      \/\/ Update method data\n-      profile_taken_branch(target_bci);\n@@ -1629,2 +1595,0 @@\n-    \/\/ Update method data\n-    profile_not_taken_branch();\n@@ -2694,3 +2658,0 @@\n-    \/\/ Update method data\n-    profile_taken_branch(target_bci);\n-\n","filename":"src\/hotspot\/share\/opto\/parse2.cpp","additions":37,"deletions":76,"binary":false,"changes":113,"status":"modified"},{"patch":"@@ -89,3 +89,0 @@\n-    if (!stopped()) {\n-      profile_null_checkcast();\n-    }\n@@ -302,284 +299,0 @@\n-\n-\/\/=============================================================================\n-\/\/\n-\/\/ parser methods for profiling\n-\n-\n-\/\/----------------------test_counter_against_threshold ------------------------\n-void Parse::test_counter_against_threshold(Node* cnt, int limit) {\n-  \/\/ Test the counter against the limit and uncommon trap if greater.\n-\n-  \/\/ This code is largely copied from the range check code in\n-  \/\/ array_addressing()\n-\n-  \/\/ Test invocation count vs threshold\n-  Node *threshold = makecon(TypeInt::make(limit));\n-  Node *chk   = _gvn.transform( new CmpUNode( cnt, threshold) );\n-  BoolTest::mask btest = BoolTest::lt;\n-  Node *tst   = _gvn.transform( new BoolNode( chk, btest) );\n-  \/\/ Branch to failure if threshold exceeded\n-  { BuildCutout unless(this, tst, PROB_ALWAYS);\n-    uncommon_trap(Deoptimization::Reason_age,\n-                  Deoptimization::Action_maybe_recompile);\n-  }\n-}\n-\n-\/\/----------------------increment_and_test_invocation_counter-------------------\n-void Parse::increment_and_test_invocation_counter(int limit) {\n-  if (!count_invocations()) return;\n-\n-  \/\/ Get the Method* node.\n-  ciMethod* m = method();\n-  MethodCounters* counters_adr = m->ensure_method_counters();\n-  if (counters_adr == NULL) {\n-    C->record_failure(\"method counters allocation failed\");\n-    return;\n-  }\n-\n-  Node* ctrl = control();\n-  const TypePtr* adr_type = TypeRawPtr::make((address) counters_adr);\n-  Node *counters_node = makecon(adr_type);\n-  Node* adr_iic_node = basic_plus_adr(counters_node, counters_node,\n-    MethodCounters::interpreter_invocation_counter_offset_in_bytes());\n-  Node* cnt = make_load(ctrl, adr_iic_node, TypeInt::INT, T_INT, adr_type, MemNode::unordered);\n-\n-  test_counter_against_threshold(cnt, limit);\n-\n-  \/\/ Add one to the counter and store\n-  Node* incr = _gvn.transform(new AddINode(cnt, _gvn.intcon(1)));\n-  store_to_memory(ctrl, adr_iic_node, incr, T_INT, adr_type, MemNode::unordered);\n-}\n-\n-\/\/----------------------------method_data_addressing---------------------------\n-Node* Parse::method_data_addressing(ciMethodData* md, ciProfileData* data, ByteSize counter_offset, Node* idx, uint stride) {\n-  \/\/ Get offset within MethodData* of the data array\n-  ByteSize data_offset = MethodData::data_offset();\n-\n-  \/\/ Get cell offset of the ProfileData within data array\n-  int cell_offset = md->dp_to_di(data->dp());\n-\n-  \/\/ Add in counter_offset, the # of bytes into the ProfileData of counter or flag\n-  int offset = in_bytes(data_offset) + cell_offset + in_bytes(counter_offset);\n-\n-  const TypePtr* adr_type = TypeMetadataPtr::make(md);\n-  Node* mdo = makecon(adr_type);\n-  Node* ptr = basic_plus_adr(mdo, mdo, offset);\n-\n-  if (stride != 0) {\n-    Node* str = _gvn.MakeConX(stride);\n-    Node* scale = _gvn.transform( new MulXNode( idx, str ) );\n-    ptr   = _gvn.transform( new AddPNode( mdo, ptr, scale ) );\n-  }\n-\n-  return ptr;\n-}\n-\n-\/\/--------------------------increment_md_counter_at----------------------------\n-void Parse::increment_md_counter_at(ciMethodData* md, ciProfileData* data, ByteSize counter_offset, Node* idx, uint stride) {\n-  Node* adr_node = method_data_addressing(md, data, counter_offset, idx, stride);\n-\n-  const TypePtr* adr_type = _gvn.type(adr_node)->is_ptr();\n-  Node* cnt  = make_load(NULL, adr_node, TypeInt::INT, T_INT, adr_type, MemNode::unordered);\n-  Node* incr = _gvn.transform(new AddINode(cnt, _gvn.intcon(DataLayout::counter_increment)));\n-  store_to_memory(NULL, adr_node, incr, T_INT, adr_type, MemNode::unordered);\n-}\n-\n-\/\/--------------------------test_for_osr_md_counter_at-------------------------\n-void Parse::test_for_osr_md_counter_at(ciMethodData* md, ciProfileData* data, ByteSize counter_offset, int limit) {\n-  Node* adr_node = method_data_addressing(md, data, counter_offset);\n-\n-  const TypePtr* adr_type = _gvn.type(adr_node)->is_ptr();\n-  Node* cnt  = make_load(NULL, adr_node, TypeInt::INT, T_INT, adr_type, MemNode::unordered);\n-\n-  test_counter_against_threshold(cnt, limit);\n-}\n-\n-\/\/-------------------------------set_md_flag_at--------------------------------\n-void Parse::set_md_flag_at(ciMethodData* md, ciProfileData* data, int flag_constant) {\n-  Node* adr_node = method_data_addressing(md, data, DataLayout::flags_offset());\n-\n-  const TypePtr* adr_type = _gvn.type(adr_node)->is_ptr();\n-  Node* flags = make_load(NULL, adr_node, TypeInt::INT, T_INT, adr_type, MemNode::unordered);\n-  Node* incr = _gvn.transform(new OrINode(flags, _gvn.intcon(flag_constant)));\n-  store_to_memory(NULL, adr_node, incr, T_INT, adr_type, MemNode::unordered);\n-}\n-\n-\/\/----------------------------profile_taken_branch-----------------------------\n-void Parse::profile_taken_branch(int target_bci, bool force_update) {\n-  \/\/ This is a potential osr_site if we have a backedge.\n-  int cur_bci = bci();\n-  bool osr_site =\n-    (target_bci <= cur_bci) && count_invocations() && UseOnStackReplacement;\n-\n-  \/\/ If we are going to OSR, restart at the target bytecode.\n-  set_bci(target_bci);\n-\n-  \/\/ To do: factor out the the limit calculations below. These duplicate\n-  \/\/ the similar limit calculations in the interpreter.\n-\n-  if (method_data_update() || force_update) {\n-    ciMethodData* md = method()->method_data();\n-    assert(md != NULL, \"expected valid ciMethodData\");\n-    ciProfileData* data = md->bci_to_data(cur_bci);\n-    assert(data != NULL && data->is_JumpData(), \"need JumpData for taken branch\");\n-    increment_md_counter_at(md, data, JumpData::taken_offset());\n-  }\n-\n-  \/\/ In the new tiered system this is all we need to do. In the old\n-  \/\/ (c2 based) tiered sytem we must do the code below.\n-#ifndef TIERED\n-  if (method_data_update()) {\n-    ciMethodData* md = method()->method_data();\n-    if (osr_site) {\n-      ciProfileData* data = md->bci_to_data(cur_bci);\n-      assert(data != NULL && data->is_JumpData(), \"need JumpData for taken branch\");\n-      int limit = (int)((int64_t)CompileThreshold\n-                   * (OnStackReplacePercentage - InterpreterProfilePercentage) \/ 100);\n-      test_for_osr_md_counter_at(md, data, JumpData::taken_offset(), limit);\n-    }\n-  } else {\n-    \/\/ With method data update off, use the invocation counter to trigger an\n-    \/\/ OSR compilation, as done in the interpreter.\n-    if (osr_site) {\n-      int limit = (int)((int64_t)CompileThreshold * OnStackReplacePercentage \/ 100);\n-      increment_and_test_invocation_counter(limit);\n-    }\n-  }\n-#endif \/\/ TIERED\n-\n-  \/\/ Restore the original bytecode.\n-  set_bci(cur_bci);\n-}\n-\n-\/\/--------------------------profile_not_taken_branch---------------------------\n-void Parse::profile_not_taken_branch(bool force_update) {\n-\n-  if (method_data_update() || force_update) {\n-    ciMethodData* md = method()->method_data();\n-    assert(md != NULL, \"expected valid ciMethodData\");\n-    ciProfileData* data = md->bci_to_data(bci());\n-    assert(data != NULL && data->is_BranchData(), \"need BranchData for not taken branch\");\n-    increment_md_counter_at(md, data, BranchData::not_taken_offset());\n-  }\n-\n-}\n-\n-\/\/---------------------------------profile_call--------------------------------\n-void Parse::profile_call(Node* receiver) {\n-  if (!method_data_update()) return;\n-\n-  switch (bc()) {\n-  case Bytecodes::_invokevirtual:\n-  case Bytecodes::_invokeinterface:\n-    profile_receiver_type(receiver);\n-    break;\n-  case Bytecodes::_invokestatic:\n-  case Bytecodes::_invokedynamic:\n-  case Bytecodes::_invokespecial:\n-    profile_generic_call();\n-    break;\n-  default: fatal(\"unexpected call bytecode\");\n-  }\n-}\n-\n-\/\/------------------------------profile_generic_call---------------------------\n-void Parse::profile_generic_call() {\n-  assert(method_data_update(), \"must be generating profile code\");\n-\n-  ciMethodData* md = method()->method_data();\n-  assert(md != NULL, \"expected valid ciMethodData\");\n-  ciProfileData* data = md->bci_to_data(bci());\n-  assert(data != NULL && data->is_CounterData(), \"need CounterData for not taken branch\");\n-  increment_md_counter_at(md, data, CounterData::count_offset());\n-}\n-\n-\/\/-----------------------------profile_receiver_type---------------------------\n-void Parse::profile_receiver_type(Node* receiver) {\n-  assert(method_data_update(), \"must be generating profile code\");\n-\n-  ciMethodData* md = method()->method_data();\n-  assert(md != NULL, \"expected valid ciMethodData\");\n-  ciProfileData* data = md->bci_to_data(bci());\n-  assert(data != NULL && data->is_ReceiverTypeData(), \"need ReceiverTypeData here\");\n-\n-  \/\/ Skip if we aren't tracking receivers\n-  if (TypeProfileWidth < 1) {\n-    increment_md_counter_at(md, data, CounterData::count_offset());\n-    return;\n-  }\n-  ciReceiverTypeData* rdata = (ciReceiverTypeData*)data->as_ReceiverTypeData();\n-\n-  Node* method_data = method_data_addressing(md, rdata, in_ByteSize(0));\n-\n-  \/\/ Using an adr_type of TypePtr::BOTTOM to work around anti-dep problems.\n-  \/\/ A better solution might be to use TypeRawPtr::BOTTOM with RC_NARROW_MEM.\n-  make_runtime_call(RC_LEAF, OptoRuntime::profile_receiver_type_Type(),\n-                    CAST_FROM_FN_PTR(address,\n-                                     OptoRuntime::profile_receiver_type_C),\n-                    \"profile_receiver_type_C\",\n-                    TypePtr::BOTTOM,\n-                    method_data, receiver);\n-}\n-\n-\/\/---------------------------------profile_ret---------------------------------\n-void Parse::profile_ret(int target_bci) {\n-  if (!method_data_update()) return;\n-\n-  \/\/ Skip if we aren't tracking ret targets\n-  if (TypeProfileWidth < 1) return;\n-\n-  ciMethodData* md = method()->method_data();\n-  assert(md != NULL, \"expected valid ciMethodData\");\n-  ciProfileData* data = md->bci_to_data(bci());\n-  assert(data != NULL && data->is_RetData(), \"need RetData for ret\");\n-  ciRetData* ret_data = (ciRetData*)data->as_RetData();\n-\n-  \/\/ Look for the target_bci is already in the table\n-  uint row;\n-  bool table_full = true;\n-  for (row = 0; row < ret_data->row_limit(); row++) {\n-    int key = ret_data->bci(row);\n-    table_full &= (key != RetData::no_bci);\n-    if (key == target_bci) break;\n-  }\n-\n-  if (row >= ret_data->row_limit()) {\n-    \/\/ The target_bci was not found in the table.\n-    if (!table_full) {\n-      \/\/ XXX: Make slow call to update RetData\n-    }\n-    return;\n-  }\n-\n-  \/\/ the target_bci is already in the table\n-  increment_md_counter_at(md, data, RetData::bci_count_offset(row));\n-}\n-\n-\/\/--------------------------profile_null_checkcast----------------------------\n-void Parse::profile_null_checkcast() {\n-  \/\/ Set the null-seen flag, done in conjunction with the usual null check. We\n-  \/\/ never unset the flag, so this is a one-way switch.\n-  if (!method_data_update()) return;\n-\n-  ciMethodData* md = method()->method_data();\n-  assert(md != NULL, \"expected valid ciMethodData\");\n-  ciProfileData* data = md->bci_to_data(bci());\n-  assert(data != NULL && data->is_BitData(), \"need BitData for checkcast\");\n-  set_md_flag_at(md, data, BitData::null_seen_byte_constant());\n-}\n-\n-\/\/-----------------------------profile_switch_case-----------------------------\n-void Parse::profile_switch_case(int table_index) {\n-  if (!method_data_update()) return;\n-\n-  ciMethodData* md = method()->method_data();\n-  assert(md != NULL, \"expected valid ciMethodData\");\n-\n-  ciProfileData* data = md->bci_to_data(bci());\n-  assert(data != NULL && data->is_MultiBranchData(), \"need MultiBranchData for switch case\");\n-  if (table_index >= 0) {\n-    increment_md_counter_at(md, data, MultiBranchData::case_count_offset(table_index));\n-  } else {\n-    increment_md_counter_at(md, data, MultiBranchData::default_count_offset());\n-  }\n-}\n","filename":"src\/hotspot\/share\/opto\/parseHelper.cpp","additions":0,"deletions":287,"binary":false,"changes":287,"status":"modified"},{"patch":"@@ -1213,54 +1213,0 @@\n-\/\/-------------- methodData update helpers\n-\n-const TypeFunc* OptoRuntime::profile_receiver_type_Type() {\n-  \/\/ create input type (domain)\n-  const Type **fields = TypeTuple::fields(2);\n-  fields[TypeFunc::Parms+0] = TypeAryPtr::NOTNULL;    \/\/ methodData pointer\n-  fields[TypeFunc::Parms+1] = TypeInstPtr::BOTTOM;    \/\/ receiver oop\n-  const TypeTuple *domain = TypeTuple::make(TypeFunc::Parms+2, fields);\n-\n-  \/\/ create result type\n-  fields = TypeTuple::fields(1);\n-  fields[TypeFunc::Parms+0] = NULL; \/\/ void\n-  const TypeTuple *range = TypeTuple::make(TypeFunc::Parms, fields);\n-  return TypeFunc::make(domain,range);\n-}\n-\n-JRT_LEAF(void, OptoRuntime::profile_receiver_type_C(DataLayout* data, oopDesc* receiver))\n-  if (receiver == NULL) return;\n-  Klass* receiver_klass = receiver->klass();\n-\n-  intptr_t* mdp = ((intptr_t*)(data)) + DataLayout::header_size_in_cells();\n-  int empty_row = -1;           \/\/ free row, if any is encountered\n-\n-  \/\/ ReceiverTypeData* vc = new ReceiverTypeData(mdp);\n-  for (uint row = 0; row < ReceiverTypeData::row_limit(); row++) {\n-    \/\/ if (vc->receiver(row) == receiver_klass)\n-    int receiver_off = ReceiverTypeData::receiver_cell_index(row);\n-    intptr_t row_recv = *(mdp + receiver_off);\n-    if (row_recv == (intptr_t) receiver_klass) {\n-      \/\/ vc->set_receiver_count(row, vc->receiver_count(row) + DataLayout::counter_increment);\n-      int count_off = ReceiverTypeData::receiver_count_cell_index(row);\n-      *(mdp + count_off) += DataLayout::counter_increment;\n-      return;\n-    } else if (row_recv == 0) {\n-      \/\/ else if (vc->receiver(row) == NULL)\n-      empty_row = (int) row;\n-    }\n-  }\n-\n-  if (empty_row != -1) {\n-    int receiver_off = ReceiverTypeData::receiver_cell_index(empty_row);\n-    \/\/ vc->set_receiver(empty_row, receiver_klass);\n-    *(mdp + receiver_off) = (intptr_t) receiver_klass;\n-    \/\/ vc->set_receiver_count(empty_row, DataLayout::counter_increment);\n-    int count_off = ReceiverTypeData::receiver_count_cell_index(empty_row);\n-    *(mdp + count_off) = DataLayout::counter_increment;\n-  } else {\n-    \/\/ Receiver did not match any saved receiver and there is no empty row for it.\n-    \/\/ Increment total counter to indicate polymorphic case.\n-    intptr_t* count_p = (intptr_t*)(((uint8_t*)(data)) + in_bytes(CounterData::count_offset()));\n-    *count_p += DataLayout::counter_increment;\n-  }\n-JRT_END\n-\n","filename":"src\/hotspot\/share\/opto\/runtime.cpp","additions":0,"deletions":54,"binary":false,"changes":54,"status":"modified"},{"patch":"@@ -232,3 +232,0 @@\n-  \/\/ Leaf routines helping with method data update\n-  static void profile_receiver_type_C(DataLayout* data, oopDesc* receiver);\n-\n@@ -307,3 +304,0 @@\n-  \/\/ leaf methodData routine types\n-  static const TypeFunc* profile_receiver_type_Type();\n-\n","filename":"src\/hotspot\/share\/opto\/runtime.hpp","additions":0,"deletions":6,"binary":false,"changes":6,"status":"modified"}]}