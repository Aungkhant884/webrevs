{"files":[{"patch":"@@ -88,0 +88,12 @@\n+uintptr_t ZMemoryManager::peek_alloc_from_front() const {\n+  ZLocker<ZLock> locker(&_lock);\n+\n+  const ZMemory* const area = _freelist.first();\n+  if (area != NULL) {\n+    return area->start();\n+  }\n+\n+  \/\/ Out of memory\n+  return UINTPTR_MAX;\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/z\/zMemory.cpp","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -69,1 +69,1 @@\n-  ZLock          _lock;\n+  mutable ZLock  _lock;\n@@ -85,0 +85,1 @@\n+  uintptr_t peek_alloc_from_front() const;\n","filename":"src\/hotspot\/share\/gc\/z\/zMemory.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -51,0 +51,1 @@\n+static const ZStatCounter       ZCounterDefragment(\"Memory\", \"Defragment\", ZStatUnitOpsPerSecond);\n@@ -562,1 +563,11 @@\n-static bool is_alloc_satisfied(ZPageAllocation* allocation) {\n+bool ZPageAllocator::should_defragment(const ZPage* page) const {\n+  \/\/ A small page can end up at a high address (second half of the address space)\n+  \/\/ if we've split a larger page or we have a constrained address space. To help\n+  \/\/ fight address space fragmentation we remap such pages to a lower address, if\n+  \/\/ a lower address is available.\n+  return page->type() == ZPageTypeSmall &&\n+         page->start() >= _virtual.reserved() \/ 2 &&\n+         page->start() > _virtual.peek_alloc_low_address();\n+}\n+\n+bool ZPageAllocator::is_alloc_satisfied(ZPageAllocation* allocation) const {\n@@ -564,4 +575,25 @@\n-  \/\/ exactly one page, with the type and size that was requested.\n-  return allocation->pages()->size() == 1 &&\n-         allocation->pages()->first()->type() == allocation->type() &&\n-         allocation->pages()->first()->size() == allocation->size();\n+  \/\/ exactly one page, with the type and size that was requested. However,\n+  \/\/ even if the allocation is immediately satisfied we might still want to\n+  \/\/ return false here to force the page to be remapped to fight address\n+  \/\/ space fragmentation.\n+\n+  if (allocation->pages()->size() != 1) {\n+    \/\/ Not a single page\n+    return false;\n+  }\n+\n+  const ZPage* const page = allocation->pages()->first();\n+  if (page->type() != allocation->type() ||\n+      page->size() != allocation->size()) {\n+    \/\/ Wrong type or size\n+    return false;\n+  }\n+\n+  if (should_defragment(page)) {\n+    \/\/ Defragment address space\n+    ZStatInc(ZCounterDefragment);\n+    return false;\n+  }\n+\n+  \/\/ Allocation immediately satisfied\n+  return true;\n","filename":"src\/hotspot\/share\/gc\/z\/zPageAllocator.cpp","additions":37,"deletions":5,"binary":false,"changes":42,"status":"modified"},{"patch":"@@ -92,0 +92,2 @@\n+  bool should_defragment(const ZPage* page) const;\n+  bool is_alloc_satisfied(ZPageAllocation* allocation) const;\n","filename":"src\/hotspot\/share\/gc\/z\/zPageAllocator.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+    _reserved(0),\n@@ -176,0 +177,3 @@\n+  \/\/ Record reserved\n+  _reserved = reserved;\n+\n","filename":"src\/hotspot\/share\/gc\/z\/zVirtualMemory.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -51,0 +51,1 @@\n+  uintptr_t      _reserved;\n@@ -72,0 +73,3 @@\n+  size_t reserved() const;\n+  uintptr_t peek_alloc_low_address() const;\n+\n","filename":"src\/hotspot\/share\/gc\/z\/zVirtualMemory.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -60,0 +60,8 @@\n+inline size_t ZVirtualMemoryManager::reserved() const {\n+  return _reserved;\n+}\n+\n+inline uintptr_t ZVirtualMemoryManager::peek_alloc_low_address() const {\n+  return _manager.peek_alloc_from_front();\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/z\/zVirtualMemory.inline.hpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"}]}