{"files":[{"patch":"@@ -29,1 +29,0 @@\n-#include \"gc\/shared\/cardGeneration.inline.hpp\"\n@@ -31,0 +30,1 @@\n+#include \"gc\/shared\/gcLocker.hpp\"\n@@ -43,0 +43,246 @@\n+bool TenuredGeneration::grow_by(size_t bytes) {\n+  assert_correct_size_change_locking();\n+  bool result = _virtual_space.expand_by(bytes);\n+  if (result) {\n+    size_t new_word_size =\n+       heap_word_size(_virtual_space.committed_size());\n+    MemRegion mr(space()->bottom(), new_word_size);\n+    \/\/ Expand card table\n+    GenCollectedHeap::heap()->rem_set()->resize_covered_region(mr);\n+    \/\/ Expand shared block offset array\n+    _bts->resize(new_word_size);\n+\n+    \/\/ Fix for bug #4668531\n+    if (ZapUnusedHeapArea) {\n+      MemRegion mangle_region(space()->end(),\n+      (HeapWord*)_virtual_space.high());\n+      SpaceMangler::mangle_region(mangle_region);\n+    }\n+\n+    \/\/ Expand space -- also expands space's BOT\n+    \/\/ (which uses (part of) shared array above)\n+    space()->set_end((HeapWord*)_virtual_space.high());\n+\n+    \/\/ update the space and generation capacity counters\n+    update_counters();\n+\n+    size_t new_mem_size = _virtual_space.committed_size();\n+    size_t old_mem_size = new_mem_size - bytes;\n+    log_trace(gc, heap)(\"Expanding %s from \" SIZE_FORMAT \"K by \" SIZE_FORMAT \"K to \" SIZE_FORMAT \"K\",\n+                    name(), old_mem_size\/K, bytes\/K, new_mem_size\/K);\n+  }\n+  return result;\n+}\n+\n+bool TenuredGeneration::expand(size_t bytes, size_t expand_bytes) {\n+  assert_locked_or_safepoint(Heap_lock);\n+  if (bytes == 0) {\n+    return true;  \/\/ That's what grow_by(0) would return\n+  }\n+  size_t aligned_bytes  = ReservedSpace::page_align_size_up(bytes);\n+  if (aligned_bytes == 0){\n+    \/\/ The alignment caused the number of bytes to wrap.  An expand_by(0) will\n+    \/\/ return true with the implication that an expansion was done when it\n+    \/\/ was not.  A call to expand implies a best effort to expand by \"bytes\"\n+    \/\/ but not a guarantee.  Align down to give a best effort.  This is likely\n+    \/\/ the most that the generation can expand since it has some capacity to\n+    \/\/ start with.\n+    aligned_bytes = ReservedSpace::page_align_size_down(bytes);\n+  }\n+  size_t aligned_expand_bytes = ReservedSpace::page_align_size_up(expand_bytes);\n+  bool success = false;\n+  if (aligned_expand_bytes > aligned_bytes) {\n+    success = grow_by(aligned_expand_bytes);\n+  }\n+  if (!success) {\n+    success = grow_by(aligned_bytes);\n+  }\n+  if (!success) {\n+    success = grow_to_reserved();\n+  }\n+  if (success && GCLocker::is_active_and_needs_gc()) {\n+    log_trace(gc, heap)(\"Garbage collection disabled, expanded heap instead\");\n+  }\n+\n+  return success;\n+}\n+\n+bool TenuredGeneration::grow_to_reserved() {\n+  assert_correct_size_change_locking();\n+  bool success = true;\n+  const size_t remaining_bytes = _virtual_space.uncommitted_size();\n+  if (remaining_bytes > 0) {\n+    success = grow_by(remaining_bytes);\n+    DEBUG_ONLY(if (!success) log_warning(gc)(\"grow to reserved failed\");)\n+  }\n+  return success;\n+}\n+\n+void TenuredGeneration::shrink(size_t bytes) {\n+  assert_correct_size_change_locking();\n+\n+  size_t size = ReservedSpace::page_align_size_down(bytes);\n+  if (size == 0) {\n+    return;\n+  }\n+\n+  \/\/ Shrink committed space\n+  _virtual_space.shrink_by(size);\n+  \/\/ Shrink space; this also shrinks the space's BOT\n+  space()->set_end((HeapWord*) _virtual_space.high());\n+  size_t new_word_size = heap_word_size(space()->capacity());\n+  \/\/ Shrink the shared block offset array\n+  _bts->resize(new_word_size);\n+  MemRegion mr(space()->bottom(), new_word_size);\n+  \/\/ Shrink the card table\n+  GenCollectedHeap::heap()->rem_set()->resize_covered_region(mr);\n+\n+  size_t new_mem_size = _virtual_space.committed_size();\n+  size_t old_mem_size = new_mem_size + size;\n+  log_trace(gc, heap)(\"Shrinking %s from \" SIZE_FORMAT \"K to \" SIZE_FORMAT \"K\",\n+                      name(), old_mem_size\/K, new_mem_size\/K);\n+}\n+\n+\/\/ Objects in this generation may have moved, invalidate this\n+\/\/ generation's cards.\n+void TenuredGeneration::invalidate_remembered_set() {\n+  _rs->invalidate(used_region());\n+}\n+\n+void TenuredGeneration::compute_new_size_inner() {\n+  assert(_shrink_factor <= 100, \"invalid shrink factor\");\n+  size_t current_shrink_factor = _shrink_factor;\n+  if (ShrinkHeapInSteps) {\n+    \/\/ Always reset '_shrink_factor' if the heap is shrunk in steps.\n+    \/\/ If we shrink the heap in this iteration, '_shrink_factor' will\n+    \/\/ be recomputed based on the old value further down in this fuction.\n+    _shrink_factor = 0;\n+  }\n+\n+  \/\/ We don't have floating point command-line arguments\n+  \/\/ Note:  argument processing ensures that MinHeapFreeRatio < 100.\n+  const double minimum_free_percentage = MinHeapFreeRatio \/ 100.0;\n+  const double maximum_used_percentage = 1.0 - minimum_free_percentage;\n+\n+  \/\/ Compute some numbers about the state of the heap.\n+  const size_t used_after_gc = used();\n+  const size_t capacity_after_gc = capacity();\n+\n+  const double min_tmp = used_after_gc \/ maximum_used_percentage;\n+  size_t minimum_desired_capacity = (size_t)MIN2(min_tmp, double(max_uintx));\n+  \/\/ Don't shrink less than the initial generation size\n+  minimum_desired_capacity = MAX2(minimum_desired_capacity, initial_size());\n+  assert(used_after_gc <= minimum_desired_capacity, \"sanity check\");\n+\n+    const size_t free_after_gc = free();\n+    const double free_percentage = ((double)free_after_gc) \/ capacity_after_gc;\n+    log_trace(gc, heap)(\"TenuredGeneration::compute_new_size:\");\n+    log_trace(gc, heap)(\"    minimum_free_percentage: %6.2f  maximum_used_percentage: %6.2f\",\n+                  minimum_free_percentage,\n+                  maximum_used_percentage);\n+    log_trace(gc, heap)(\"     free_after_gc   : %6.1fK   used_after_gc   : %6.1fK   capacity_after_gc   : %6.1fK\",\n+                  free_after_gc \/ (double) K,\n+                  used_after_gc \/ (double) K,\n+                  capacity_after_gc \/ (double) K);\n+    log_trace(gc, heap)(\"     free_percentage: %6.2f\", free_percentage);\n+\n+  if (capacity_after_gc < minimum_desired_capacity) {\n+    \/\/ If we have less free space than we want then expand\n+    size_t expand_bytes = minimum_desired_capacity - capacity_after_gc;\n+    \/\/ Don't expand unless it's significant\n+    if (expand_bytes >= _min_heap_delta_bytes) {\n+      expand(expand_bytes, 0); \/\/ safe if expansion fails\n+    }\n+    log_trace(gc, heap)(\"    expanding:  minimum_desired_capacity: %6.1fK  expand_bytes: %6.1fK  _min_heap_delta_bytes: %6.1fK\",\n+                  minimum_desired_capacity \/ (double) K,\n+                  expand_bytes \/ (double) K,\n+                  _min_heap_delta_bytes \/ (double) K);\n+    return;\n+  }\n+\n+  \/\/ No expansion, now see if we want to shrink\n+  size_t shrink_bytes = 0;\n+  \/\/ We would never want to shrink more than this\n+  size_t max_shrink_bytes = capacity_after_gc - minimum_desired_capacity;\n+\n+  if (MaxHeapFreeRatio < 100) {\n+    const double maximum_free_percentage = MaxHeapFreeRatio \/ 100.0;\n+    const double minimum_used_percentage = 1.0 - maximum_free_percentage;\n+    const double max_tmp = used_after_gc \/ minimum_used_percentage;\n+    size_t maximum_desired_capacity = (size_t)MIN2(max_tmp, double(max_uintx));\n+    maximum_desired_capacity = MAX2(maximum_desired_capacity, initial_size());\n+    log_trace(gc, heap)(\"    maximum_free_percentage: %6.2f  minimum_used_percentage: %6.2f\",\n+                             maximum_free_percentage, minimum_used_percentage);\n+    log_trace(gc, heap)(\"    _capacity_at_prologue: %6.1fK  minimum_desired_capacity: %6.1fK  maximum_desired_capacity: %6.1fK\",\n+                             _capacity_at_prologue \/ (double) K,\n+                             minimum_desired_capacity \/ (double) K,\n+                             maximum_desired_capacity \/ (double) K);\n+    assert(minimum_desired_capacity <= maximum_desired_capacity,\n+           \"sanity check\");\n+\n+    if (capacity_after_gc > maximum_desired_capacity) {\n+      \/\/ Capacity too large, compute shrinking size\n+      shrink_bytes = capacity_after_gc - maximum_desired_capacity;\n+      if (ShrinkHeapInSteps) {\n+        \/\/ If ShrinkHeapInSteps is true (the default),\n+        \/\/ we don't want to shrink all the way back to initSize if people call\n+        \/\/ System.gc(), because some programs do that between \"phases\" and then\n+        \/\/ we'd just have to grow the heap up again for the next phase.  So we\n+        \/\/ damp the shrinking: 0% on the first call, 10% on the second call, 40%\n+        \/\/ on the third call, and 100% by the fourth call.  But if we recompute\n+        \/\/ size without shrinking, it goes back to 0%.\n+        shrink_bytes = shrink_bytes \/ 100 * current_shrink_factor;\n+        if (current_shrink_factor == 0) {\n+          _shrink_factor = 10;\n+        } else {\n+          _shrink_factor = MIN2(current_shrink_factor * 4, (size_t) 100);\n+        }\n+      }\n+      assert(shrink_bytes <= max_shrink_bytes, \"invalid shrink size\");\n+      log_trace(gc, heap)(\"    shrinking:  initSize: %.1fK  maximum_desired_capacity: %.1fK\",\n+                               initial_size() \/ (double) K, maximum_desired_capacity \/ (double) K);\n+      log_trace(gc, heap)(\"    shrink_bytes: %.1fK  current_shrink_factor: \" SIZE_FORMAT \"  new shrink factor: \" SIZE_FORMAT \"  _min_heap_delta_bytes: %.1fK\",\n+                               shrink_bytes \/ (double) K,\n+                               current_shrink_factor,\n+                               _shrink_factor,\n+                               _min_heap_delta_bytes \/ (double) K);\n+    }\n+  }\n+\n+  if (capacity_after_gc > _capacity_at_prologue) {\n+    \/\/ We might have expanded for promotions, in which case we might want to\n+    \/\/ take back that expansion if there's room after GC.  That keeps us from\n+    \/\/ stretching the heap with promotions when there's plenty of room.\n+    size_t expansion_for_promotion = capacity_after_gc - _capacity_at_prologue;\n+    expansion_for_promotion = MIN2(expansion_for_promotion, max_shrink_bytes);\n+    \/\/ We have two shrinking computations, take the largest\n+    shrink_bytes = MAX2(shrink_bytes, expansion_for_promotion);\n+    assert(shrink_bytes <= max_shrink_bytes, \"invalid shrink size\");\n+    log_trace(gc, heap)(\"    aggressive shrinking:  _capacity_at_prologue: %.1fK  capacity_after_gc: %.1fK  expansion_for_promotion: %.1fK  shrink_bytes: %.1fK\",\n+                        capacity_after_gc \/ (double) K,\n+                        _capacity_at_prologue \/ (double) K,\n+                        expansion_for_promotion \/ (double) K,\n+                        shrink_bytes \/ (double) K);\n+  }\n+  \/\/ Don't shrink unless it's significant\n+  if (shrink_bytes >= _min_heap_delta_bytes) {\n+    shrink(shrink_bytes);\n+  }\n+}\n+\n+void TenuredGeneration::space_iterate(SpaceClosure* blk,\n+                                                 bool usedOnly) {\n+  blk->do_space(space());\n+}\n+\n+void TenuredGeneration::younger_refs_iterate(OopIterateClosure* blk) {\n+  \/\/ Apply \"cl->do_oop\" to (the address of) (exactly) all the ref fields in\n+  \/\/ \"sp\" that point into the young generation.\n+  \/\/ The iteration is only over objects allocated at the start of the\n+  \/\/ iterations; objects allocated as a result of applying the closure are\n+  \/\/ not included.\n+\n+  HeapWord* gen_boundary = reserved().start();\n+  _rs->younger_refs_in_space_iterate(space(), gen_boundary, blk);\n+}\n+\n@@ -48,1 +294,3 @@\n-  CardGeneration(rs, initial_byte_size, remset)\n+  Generation(rs, initial_byte_size), _rs(remset),\n+  _min_heap_delta_bytes(), _capacity_at_prologue(),\n+  _used_at_prologue()\n@@ -50,0 +298,25 @@\n+  \/\/ If we don't shrink the heap in steps, '_shrink_factor' is always 100%.\n+  _shrink_factor = ShrinkHeapInSteps ? 0 : 100;\n+  HeapWord* start = (HeapWord*)rs.base();\n+  size_t reserved_byte_size = rs.size();\n+  assert((uintptr_t(start) & 3) == 0, \"bad alignment\");\n+  assert((reserved_byte_size & 3) == 0, \"bad alignment\");\n+  MemRegion reserved_mr(start, heap_word_size(reserved_byte_size));\n+  _bts = new BlockOffsetSharedArray(reserved_mr,\n+                                    heap_word_size(initial_byte_size));\n+  MemRegion committed_mr(start, heap_word_size(initial_byte_size));\n+  _rs->resize_covered_region(committed_mr);\n+\n+  \/\/ Verify that the start and end of this generation is the start of a card.\n+  \/\/ If this wasn't true, a single card could span more than on generation,\n+  \/\/ which would cause problems when we commit\/uncommit memory, and when we\n+  \/\/ clear and dirty cards.\n+  guarantee(_rs->is_aligned(reserved_mr.start()), \"generation must be card aligned\");\n+  if (reserved_mr.end() != GenCollectedHeap::heap()->reserved_region().end()) {\n+    \/\/ Don't check at the very end of the heap as we'll assert that we're probing off\n+    \/\/ the end if we try.\n+    guarantee(_rs->is_aligned(reserved_mr.end()), \"generation must be card aligned\");\n+  }\n+  _min_heap_delta_bytes = MinHeapDeltaBytes;\n+  _capacity_at_prologue = initial_byte_size;\n+  _used_at_prologue = 0;\n@@ -117,1 +390,1 @@\n-  CardGeneration::compute_new_size();\n+  compute_new_size_inner();\n@@ -198,4 +471,0 @@\n-bool TenuredGeneration::expand(size_t bytes, size_t expand_bytes) {\n-  return CardGeneration::expand(bytes, expand_bytes);\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/serial\/tenuredGeneration.cpp","additions":276,"deletions":7,"binary":false,"changes":283,"status":"modified"},{"patch":"@@ -29,1 +29,1 @@\n-#include \"gc\/shared\/cardGeneration.hpp\"\n+#include \"gc\/shared\/generation.hpp\"\n@@ -34,0 +34,4 @@\n+class BlockOffsetSharedArray;\n+class CardTableRS;\n+class CompactibleSpace;\n+\n@@ -35,2 +39,2 @@\n-\/\/ contained in a single contiguous space.\n-\/\/\n+\/\/ contained in a single contiguous space. This generation is covered by a card\n+\/\/ table, and uses a card-size block-offset array to implement block_start.\n@@ -39,1 +43,1 @@\n-class TenuredGeneration: public CardGeneration {\n+class TenuredGeneration: public Generation {\n@@ -45,0 +49,19 @@\n+\n+  \/\/ This is shared with other generations.\n+  CardTableRS* _rs;\n+  \/\/ This is local to this generation.\n+  BlockOffsetSharedArray* _bts;\n+\n+  \/\/ Current shrinking effect: this damps shrinking when the heap gets empty.\n+  size_t _shrink_factor;\n+\n+  size_t _min_heap_delta_bytes;   \/\/ Minimum amount to expand.\n+\n+  \/\/ Some statistics from before gc started.\n+  \/\/ These are gathered in the gc_prologue (and should_collect)\n+  \/\/ to control growing\/shrinking policy in spite of promotions.\n+  size_t _capacity_at_prologue;\n+  size_t _used_at_prologue;\n+\n+  void assert_correct_size_change_locking();\n+\n@@ -50,3 +73,0 @@\n-  \/\/ Allocation failure\n-  virtual bool expand(size_t bytes, size_t expand_bytes);\n-\n@@ -56,1 +76,7 @@\n-  void assert_correct_size_change_locking();\n+  \/\/ Attempt to expand the generation by \"bytes\".  Expand by at a\n+  \/\/ minimum \"expand_bytes\".  Return true if some amount (not\n+  \/\/ necessarily the full \"bytes\") was done.\n+  bool expand(size_t bytes, size_t expand_bytes);\n+\n+  \/\/ Shrink generation with specified size\n+  void shrink(size_t bytes);\n@@ -58,0 +84,1 @@\n+  void compute_new_size_inner();\n@@ -59,0 +86,22 @@\n+  virtual void compute_new_size();\n+\n+  virtual void invalidate_remembered_set();\n+\n+  \/\/ Grow generation with specified size (returns false if unable to grow)\n+  bool grow_by(size_t bytes);\n+  \/\/ Grow generation to reserved size.\n+  bool grow_to_reserved();\n+\n+  size_t capacity() const;\n+  size_t used() const;\n+  size_t free() const;\n+  MemRegion used_region() const;\n+\n+  void space_iterate(SpaceClosure* blk, bool usedOnly = false);\n+\n+  void younger_refs_iterate(OopIterateClosure* blk);\n+\n+  bool is_in(const void* p) const;\n+\n+  CompactibleSpace* first_compaction_space() const;\n+\n@@ -107,2 +156,0 @@\n-  virtual void compute_new_size();\n-\n","filename":"src\/hotspot\/share\/gc\/serial\/tenuredGeneration.hpp","additions":57,"deletions":10,"binary":false,"changes":67,"status":"modified"},{"patch":"@@ -32,0 +32,24 @@\n+inline size_t TenuredGeneration::capacity() const {\n+  return space()->capacity();\n+}\n+\n+inline size_t TenuredGeneration::used() const {\n+  return space()->used();\n+}\n+\n+inline size_t TenuredGeneration::free() const {\n+  return space()->free();\n+}\n+\n+inline MemRegion TenuredGeneration::used_region() const {\n+  return space()->used_region();\n+}\n+\n+inline bool TenuredGeneration::is_in(const void* p) const {\n+  return space()->is_in(p);\n+}\n+\n+inline CompactibleSpace* TenuredGeneration::first_compaction_space() const {\n+  return space();\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/serial\/tenuredGeneration.inline.hpp","additions":24,"deletions":0,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -48,1 +48,1 @@\n-  declare_type(TenuredGeneration,            CardGeneration)                  \\\n+  declare_type(TenuredGeneration,            Generation)                      \\\n","filename":"src\/hotspot\/share\/gc\/serial\/vmStructs_serial.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1,318 +0,0 @@\n-\/*\n- * Copyright (c) 2014, 2021, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#include \"precompiled.hpp\"\n-\n-#include \"gc\/shared\/blockOffsetTable.inline.hpp\"\n-#include \"gc\/shared\/cardGeneration.inline.hpp\"\n-#include \"gc\/shared\/cardTableRS.hpp\"\n-#include \"gc\/shared\/gcLocker.hpp\"\n-#include \"gc\/shared\/genCollectedHeap.hpp\"\n-#include \"gc\/shared\/genOopClosures.inline.hpp\"\n-#include \"gc\/shared\/generationSpec.hpp\"\n-#include \"gc\/shared\/space.inline.hpp\"\n-#include \"memory\/iterator.hpp\"\n-#include \"memory\/memRegion.hpp\"\n-#include \"logging\/log.hpp\"\n-#include \"runtime\/java.hpp\"\n-\n-CardGeneration::CardGeneration(ReservedSpace rs,\n-                               size_t initial_byte_size,\n-                               CardTableRS* remset) :\n-  Generation(rs, initial_byte_size), _rs(remset),\n-  _min_heap_delta_bytes(), _capacity_at_prologue(),\n-  _used_at_prologue()\n-{\n-  \/\/ If we don't shrink the heap in steps, '_shrink_factor' is always 100%.\n-  _shrink_factor = ShrinkHeapInSteps ? 0 : 100;\n-  HeapWord* start = (HeapWord*)rs.base();\n-  size_t reserved_byte_size = rs.size();\n-  assert((uintptr_t(start) & 3) == 0, \"bad alignment\");\n-  assert((reserved_byte_size & 3) == 0, \"bad alignment\");\n-  MemRegion reserved_mr(start, heap_word_size(reserved_byte_size));\n-  _bts = new BlockOffsetSharedArray(reserved_mr,\n-                                    heap_word_size(initial_byte_size));\n-  MemRegion committed_mr(start, heap_word_size(initial_byte_size));\n-  _rs->resize_covered_region(committed_mr);\n-\n-  \/\/ Verify that the start and end of this generation is the start of a card.\n-  \/\/ If this wasn't true, a single card could span more than on generation,\n-  \/\/ which would cause problems when we commit\/uncommit memory, and when we\n-  \/\/ clear and dirty cards.\n-  guarantee(_rs->is_aligned(reserved_mr.start()), \"generation must be card aligned\");\n-  if (reserved_mr.end() != GenCollectedHeap::heap()->reserved_region().end()) {\n-    \/\/ Don't check at the very end of the heap as we'll assert that we're probing off\n-    \/\/ the end if we try.\n-    guarantee(_rs->is_aligned(reserved_mr.end()), \"generation must be card aligned\");\n-  }\n-  _min_heap_delta_bytes = MinHeapDeltaBytes;\n-  _capacity_at_prologue = initial_byte_size;\n-  _used_at_prologue = 0;\n-}\n-\n-bool CardGeneration::grow_by(size_t bytes) {\n-  assert_correct_size_change_locking();\n-  bool result = _virtual_space.expand_by(bytes);\n-  if (result) {\n-    size_t new_word_size =\n-       heap_word_size(_virtual_space.committed_size());\n-    MemRegion mr(space()->bottom(), new_word_size);\n-    \/\/ Expand card table\n-    GenCollectedHeap::heap()->rem_set()->resize_covered_region(mr);\n-    \/\/ Expand shared block offset array\n-    _bts->resize(new_word_size);\n-\n-    \/\/ Fix for bug #4668531\n-    if (ZapUnusedHeapArea) {\n-      MemRegion mangle_region(space()->end(),\n-      (HeapWord*)_virtual_space.high());\n-      SpaceMangler::mangle_region(mangle_region);\n-    }\n-\n-    \/\/ Expand space -- also expands space's BOT\n-    \/\/ (which uses (part of) shared array above)\n-    space()->set_end((HeapWord*)_virtual_space.high());\n-\n-    \/\/ update the space and generation capacity counters\n-    update_counters();\n-\n-    size_t new_mem_size = _virtual_space.committed_size();\n-    size_t old_mem_size = new_mem_size - bytes;\n-    log_trace(gc, heap)(\"Expanding %s from \" SIZE_FORMAT \"K by \" SIZE_FORMAT \"K to \" SIZE_FORMAT \"K\",\n-                    name(), old_mem_size\/K, bytes\/K, new_mem_size\/K);\n-  }\n-  return result;\n-}\n-\n-bool CardGeneration::expand(size_t bytes, size_t expand_bytes) {\n-  assert_locked_or_safepoint(Heap_lock);\n-  if (bytes == 0) {\n-    return true;  \/\/ That's what grow_by(0) would return\n-  }\n-  size_t aligned_bytes  = ReservedSpace::page_align_size_up(bytes);\n-  if (aligned_bytes == 0){\n-    \/\/ The alignment caused the number of bytes to wrap.  An expand_by(0) will\n-    \/\/ return true with the implication that an expansion was done when it\n-    \/\/ was not.  A call to expand implies a best effort to expand by \"bytes\"\n-    \/\/ but not a guarantee.  Align down to give a best effort.  This is likely\n-    \/\/ the most that the generation can expand since it has some capacity to\n-    \/\/ start with.\n-    aligned_bytes = ReservedSpace::page_align_size_down(bytes);\n-  }\n-  size_t aligned_expand_bytes = ReservedSpace::page_align_size_up(expand_bytes);\n-  bool success = false;\n-  if (aligned_expand_bytes > aligned_bytes) {\n-    success = grow_by(aligned_expand_bytes);\n-  }\n-  if (!success) {\n-    success = grow_by(aligned_bytes);\n-  }\n-  if (!success) {\n-    success = grow_to_reserved();\n-  }\n-  if (success && GCLocker::is_active_and_needs_gc()) {\n-    log_trace(gc, heap)(\"Garbage collection disabled, expanded heap instead\");\n-  }\n-\n-  return success;\n-}\n-\n-bool CardGeneration::grow_to_reserved() {\n-  assert_correct_size_change_locking();\n-  bool success = true;\n-  const size_t remaining_bytes = _virtual_space.uncommitted_size();\n-  if (remaining_bytes > 0) {\n-    success = grow_by(remaining_bytes);\n-    DEBUG_ONLY(if (!success) log_warning(gc)(\"grow to reserved failed\");)\n-  }\n-  return success;\n-}\n-\n-void CardGeneration::shrink(size_t bytes) {\n-  assert_correct_size_change_locking();\n-\n-  size_t size = ReservedSpace::page_align_size_down(bytes);\n-  if (size == 0) {\n-    return;\n-  }\n-\n-  \/\/ Shrink committed space\n-  _virtual_space.shrink_by(size);\n-  \/\/ Shrink space; this also shrinks the space's BOT\n-  space()->set_end((HeapWord*) _virtual_space.high());\n-  size_t new_word_size = heap_word_size(space()->capacity());\n-  \/\/ Shrink the shared block offset array\n-  _bts->resize(new_word_size);\n-  MemRegion mr(space()->bottom(), new_word_size);\n-  \/\/ Shrink the card table\n-  GenCollectedHeap::heap()->rem_set()->resize_covered_region(mr);\n-\n-  size_t new_mem_size = _virtual_space.committed_size();\n-  size_t old_mem_size = new_mem_size + size;\n-  log_trace(gc, heap)(\"Shrinking %s from \" SIZE_FORMAT \"K to \" SIZE_FORMAT \"K\",\n-                      name(), old_mem_size\/K, new_mem_size\/K);\n-}\n-\n-\/\/ Objects in this generation may have moved, invalidate this\n-\/\/ generation's cards.\n-void CardGeneration::invalidate_remembered_set() {\n-  _rs->invalidate(used_region());\n-}\n-\n-void CardGeneration::compute_new_size() {\n-  assert(_shrink_factor <= 100, \"invalid shrink factor\");\n-  size_t current_shrink_factor = _shrink_factor;\n-  if (ShrinkHeapInSteps) {\n-    \/\/ Always reset '_shrink_factor' if the heap is shrunk in steps.\n-    \/\/ If we shrink the heap in this iteration, '_shrink_factor' will\n-    \/\/ be recomputed based on the old value further down in this fuction.\n-    _shrink_factor = 0;\n-  }\n-\n-  \/\/ We don't have floating point command-line arguments\n-  \/\/ Note:  argument processing ensures that MinHeapFreeRatio < 100.\n-  const double minimum_free_percentage = MinHeapFreeRatio \/ 100.0;\n-  const double maximum_used_percentage = 1.0 - minimum_free_percentage;\n-\n-  \/\/ Compute some numbers about the state of the heap.\n-  const size_t used_after_gc = used();\n-  const size_t capacity_after_gc = capacity();\n-\n-  const double min_tmp = used_after_gc \/ maximum_used_percentage;\n-  size_t minimum_desired_capacity = (size_t)MIN2(min_tmp, double(max_uintx));\n-  \/\/ Don't shrink less than the initial generation size\n-  minimum_desired_capacity = MAX2(minimum_desired_capacity, initial_size());\n-  assert(used_after_gc <= minimum_desired_capacity, \"sanity check\");\n-\n-    const size_t free_after_gc = free();\n-    const double free_percentage = ((double)free_after_gc) \/ capacity_after_gc;\n-    log_trace(gc, heap)(\"CardGeneration::compute_new_size:\");\n-    log_trace(gc, heap)(\"    minimum_free_percentage: %6.2f  maximum_used_percentage: %6.2f\",\n-                  minimum_free_percentage,\n-                  maximum_used_percentage);\n-    log_trace(gc, heap)(\"     free_after_gc   : %6.1fK   used_after_gc   : %6.1fK   capacity_after_gc   : %6.1fK\",\n-                  free_after_gc \/ (double) K,\n-                  used_after_gc \/ (double) K,\n-                  capacity_after_gc \/ (double) K);\n-    log_trace(gc, heap)(\"     free_percentage: %6.2f\", free_percentage);\n-\n-  if (capacity_after_gc < minimum_desired_capacity) {\n-    \/\/ If we have less free space than we want then expand\n-    size_t expand_bytes = minimum_desired_capacity - capacity_after_gc;\n-    \/\/ Don't expand unless it's significant\n-    if (expand_bytes >= _min_heap_delta_bytes) {\n-      expand(expand_bytes, 0); \/\/ safe if expansion fails\n-    }\n-    log_trace(gc, heap)(\"    expanding:  minimum_desired_capacity: %6.1fK  expand_bytes: %6.1fK  _min_heap_delta_bytes: %6.1fK\",\n-                  minimum_desired_capacity \/ (double) K,\n-                  expand_bytes \/ (double) K,\n-                  _min_heap_delta_bytes \/ (double) K);\n-    return;\n-  }\n-\n-  \/\/ No expansion, now see if we want to shrink\n-  size_t shrink_bytes = 0;\n-  \/\/ We would never want to shrink more than this\n-  size_t max_shrink_bytes = capacity_after_gc - minimum_desired_capacity;\n-\n-  if (MaxHeapFreeRatio < 100) {\n-    const double maximum_free_percentage = MaxHeapFreeRatio \/ 100.0;\n-    const double minimum_used_percentage = 1.0 - maximum_free_percentage;\n-    const double max_tmp = used_after_gc \/ minimum_used_percentage;\n-    size_t maximum_desired_capacity = (size_t)MIN2(max_tmp, double(max_uintx));\n-    maximum_desired_capacity = MAX2(maximum_desired_capacity, initial_size());\n-    log_trace(gc, heap)(\"    maximum_free_percentage: %6.2f  minimum_used_percentage: %6.2f\",\n-                             maximum_free_percentage, minimum_used_percentage);\n-    log_trace(gc, heap)(\"    _capacity_at_prologue: %6.1fK  minimum_desired_capacity: %6.1fK  maximum_desired_capacity: %6.1fK\",\n-                             _capacity_at_prologue \/ (double) K,\n-                             minimum_desired_capacity \/ (double) K,\n-                             maximum_desired_capacity \/ (double) K);\n-    assert(minimum_desired_capacity <= maximum_desired_capacity,\n-           \"sanity check\");\n-\n-    if (capacity_after_gc > maximum_desired_capacity) {\n-      \/\/ Capacity too large, compute shrinking size\n-      shrink_bytes = capacity_after_gc - maximum_desired_capacity;\n-      if (ShrinkHeapInSteps) {\n-        \/\/ If ShrinkHeapInSteps is true (the default),\n-        \/\/ we don't want to shrink all the way back to initSize if people call\n-        \/\/ System.gc(), because some programs do that between \"phases\" and then\n-        \/\/ we'd just have to grow the heap up again for the next phase.  So we\n-        \/\/ damp the shrinking: 0% on the first call, 10% on the second call, 40%\n-        \/\/ on the third call, and 100% by the fourth call.  But if we recompute\n-        \/\/ size without shrinking, it goes back to 0%.\n-        shrink_bytes = shrink_bytes \/ 100 * current_shrink_factor;\n-        if (current_shrink_factor == 0) {\n-          _shrink_factor = 10;\n-        } else {\n-          _shrink_factor = MIN2(current_shrink_factor * 4, (size_t) 100);\n-        }\n-      }\n-      assert(shrink_bytes <= max_shrink_bytes, \"invalid shrink size\");\n-      log_trace(gc, heap)(\"    shrinking:  initSize: %.1fK  maximum_desired_capacity: %.1fK\",\n-                               initial_size() \/ (double) K, maximum_desired_capacity \/ (double) K);\n-      log_trace(gc, heap)(\"    shrink_bytes: %.1fK  current_shrink_factor: \" SIZE_FORMAT \"  new shrink factor: \" SIZE_FORMAT \"  _min_heap_delta_bytes: %.1fK\",\n-                               shrink_bytes \/ (double) K,\n-                               current_shrink_factor,\n-                               _shrink_factor,\n-                               _min_heap_delta_bytes \/ (double) K);\n-    }\n-  }\n-\n-  if (capacity_after_gc > _capacity_at_prologue) {\n-    \/\/ We might have expanded for promotions, in which case we might want to\n-    \/\/ take back that expansion if there's room after GC.  That keeps us from\n-    \/\/ stretching the heap with promotions when there's plenty of room.\n-    size_t expansion_for_promotion = capacity_after_gc - _capacity_at_prologue;\n-    expansion_for_promotion = MIN2(expansion_for_promotion, max_shrink_bytes);\n-    \/\/ We have two shrinking computations, take the largest\n-    shrink_bytes = MAX2(shrink_bytes, expansion_for_promotion);\n-    assert(shrink_bytes <= max_shrink_bytes, \"invalid shrink size\");\n-    log_trace(gc, heap)(\"    aggressive shrinking:  _capacity_at_prologue: %.1fK  capacity_after_gc: %.1fK  expansion_for_promotion: %.1fK  shrink_bytes: %.1fK\",\n-                        capacity_after_gc \/ (double) K,\n-                        _capacity_at_prologue \/ (double) K,\n-                        expansion_for_promotion \/ (double) K,\n-                        shrink_bytes \/ (double) K);\n-  }\n-  \/\/ Don't shrink unless it's significant\n-  if (shrink_bytes >= _min_heap_delta_bytes) {\n-    shrink(shrink_bytes);\n-  }\n-}\n-\n-void CardGeneration::space_iterate(SpaceClosure* blk,\n-                                                 bool usedOnly) {\n-  blk->do_space(space());\n-}\n-\n-void CardGeneration::younger_refs_iterate(OopIterateClosure* blk) {\n-  \/\/ Apply \"cl->do_oop\" to (the address of) (exactly) all the ref fields in\n-  \/\/ \"sp\" that point into the young generation.\n-  \/\/ The iteration is only over objects allocated at the start of the\n-  \/\/ iterations; objects allocated as a result of applying the closure are\n-  \/\/ not included.\n-\n-  HeapWord* gen_boundary = reserved().start();\n-  _rs->younger_refs_in_space_iterate(space(), gen_boundary, blk);\n-}\n","filename":"src\/hotspot\/share\/gc\/shared\/cardGeneration.cpp","additions":0,"deletions":318,"binary":false,"changes":318,"status":"deleted"},{"patch":"@@ -1,95 +0,0 @@\n-\/*\n- * Copyright (c) 2014, 2019, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#ifndef SHARE_GC_SHARED_CARDGENERATION_HPP\n-#define SHARE_GC_SHARED_CARDGENERATION_HPP\n-\n-\/\/ Class CardGeneration is a generation that is covered by a card table,\n-\/\/ and uses a card-size block-offset array to implement block_start.\n-\n-#include \"gc\/shared\/generation.hpp\"\n-\n-class BlockOffsetSharedArray;\n-class CardTableRS;\n-class CompactibleSpace;\n-\n-class CardGeneration: public Generation {\n-  friend class VMStructs;\n- protected:\n-  \/\/ This is shared with other generations.\n-  CardTableRS* _rs;\n-  \/\/ This is local to this generation.\n-  BlockOffsetSharedArray* _bts;\n-\n-  \/\/ Current shrinking effect: this damps shrinking when the heap gets empty.\n-  size_t _shrink_factor;\n-\n-  size_t _min_heap_delta_bytes;   \/\/ Minimum amount to expand.\n-\n-  \/\/ Some statistics from before gc started.\n-  \/\/ These are gathered in the gc_prologue (and should_collect)\n-  \/\/ to control growing\/shrinking policy in spite of promotions.\n-  size_t _capacity_at_prologue;\n-  size_t _used_at_prologue;\n-\n-  CardGeneration(ReservedSpace rs, size_t initial_byte_size, CardTableRS* remset);\n-\n-  virtual void assert_correct_size_change_locking() = 0;\n-\n-  virtual CompactibleSpace* space() const = 0;\n-\n- public:\n-\n-  \/\/ Attempt to expand the generation by \"bytes\".  Expand by at a\n-  \/\/ minimum \"expand_bytes\".  Return true if some amount (not\n-  \/\/ necessarily the full \"bytes\") was done.\n-  virtual bool expand(size_t bytes, size_t expand_bytes);\n-\n-  \/\/ Shrink generation with specified size\n-  virtual void shrink(size_t bytes);\n-\n-  virtual void compute_new_size();\n-\n-  virtual void invalidate_remembered_set();\n-\n-  \/\/ Grow generation with specified size (returns false if unable to grow)\n-  bool grow_by(size_t bytes);\n-  \/\/ Grow generation to reserved size.\n-  bool grow_to_reserved();\n-\n-  size_t capacity() const;\n-  size_t used() const;\n-  size_t free() const;\n-  MemRegion used_region() const;\n-\n-  void space_iterate(SpaceClosure* blk, bool usedOnly = false);\n-\n-  void younger_refs_iterate(OopIterateClosure* blk);\n-\n-  bool is_in(const void* p) const;\n-\n-  CompactibleSpace* first_compaction_space() const;\n-};\n-\n-#endif \/\/ SHARE_GC_SHARED_CARDGENERATION_HPP\n","filename":"src\/hotspot\/share\/gc\/shared\/cardGeneration.hpp","additions":0,"deletions":95,"binary":false,"changes":95,"status":"deleted"},{"patch":"@@ -1,56 +0,0 @@\n-\/*\n- * Copyright (c) 2014, 2019, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#ifndef SHARE_GC_SHARED_CARDGENERATION_INLINE_HPP\n-#define SHARE_GC_SHARED_CARDGENERATION_INLINE_HPP\n-\n-#include \"gc\/shared\/cardGeneration.hpp\"\n-\n-#include \"gc\/shared\/space.hpp\"\n-\n-inline size_t CardGeneration::capacity() const {\n-  return space()->capacity();\n-}\n-\n-inline size_t CardGeneration::used() const {\n-  return space()->used();\n-}\n-\n-inline size_t CardGeneration::free() const {\n-  return space()->free();\n-}\n-\n-inline MemRegion CardGeneration::used_region() const {\n-  return space()->used_region();\n-}\n-\n-inline bool CardGeneration::is_in(const void* p) const {\n-  return space()->is_in(p);\n-}\n-\n-inline CompactibleSpace* CardGeneration::first_compaction_space() const {\n-  return space();\n-}\n-\n-#endif \/\/ SHARE_GC_SHARED_CARDGENERATION_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/shared\/cardGeneration.inline.hpp","additions":0,"deletions":56,"binary":false,"changes":56,"status":"deleted"},{"patch":"@@ -44,2 +44,1 @@\n-\/\/ - CardGeneration                 - abstract class adding offset array behavior\n-\/\/   - TenuredGeneration             - tenured (old object) space (markSweepCompact)\n+\/\/ - TenuredGeneration             - tenured (old object) space (markSweepCompact)\n","filename":"src\/hotspot\/share\/gc\/shared\/generation.hpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -29,1 +29,0 @@\n-#include \"gc\/shared\/cardGeneration.hpp\"\n@@ -104,5 +103,5 @@\n-  nonstatic_field(CardGeneration,              _rs,                                           CardTableRS*)                          \\\n-  nonstatic_field(CardGeneration,              _bts,                                          BlockOffsetSharedArray*)               \\\n-  nonstatic_field(CardGeneration,              _shrink_factor,                                size_t)                                \\\n-  nonstatic_field(CardGeneration,              _capacity_at_prologue,                         size_t)                                \\\n-  nonstatic_field(CardGeneration,              _used_at_prologue,                             size_t)                                \\\n+  nonstatic_field(TenuredGeneration,           _rs,                                           CardTableRS*)                          \\\n+  nonstatic_field(TenuredGeneration,           _bts,                                          BlockOffsetSharedArray*)               \\\n+  nonstatic_field(TenuredGeneration,           _shrink_factor,                                size_t)                                \\\n+  nonstatic_field(TenuredGeneration,           _capacity_at_prologue,                         size_t)                                \\\n+  nonstatic_field(TenuredGeneration,           _used_at_prologue,                             size_t)                                \\\n@@ -189,1 +188,0 @@\n-           declare_type(CardGeneration,               Generation)         \\\n","filename":"src\/hotspot\/share\/gc\/shared\/vmStructs_gc.hpp","additions":5,"deletions":7,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -42,1 +42,1 @@\n-public class TenuredGeneration extends CardGeneration {\n+public class TenuredGeneration extends Generation {\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/gc\/serial\/TenuredGeneration.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1,40 +0,0 @@\n-\/*\n- * Copyright (c) 2000, 2015, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-package sun.jvm.hotspot.gc.shared;\n-\n-import sun.jvm.hotspot.debugger.*;\n-\n-\/** Class CardGeneration is a generation that is covered by a card\n-    table, and uses a card-size block-offset array to implement\n-    block_start. *\/\n-\n-public abstract class CardGeneration extends Generation {\n-  public CardGeneration(Address addr) {\n-    super(addr);\n-  }\n-\n-  \/\/ FIXME: not sure what I need to expose from here in order to have\n-  \/\/ verification similar to that of the old RememberedSet\n-}\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/gc\/shared\/CardGeneration.java","additions":0,"deletions":40,"binary":false,"changes":40,"status":"deleted"},{"patch":"@@ -41,4 +41,1 @@\n-      <li> CardGeneration\n-        <ul>\n-        <li> TenuredGeneration\n-        <\/ul>\n+      <li> TenuredGeneration\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/gc\/shared\/Generation.java","additions":1,"deletions":4,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -60,1 +60,1 @@\n-                \"type TenuredGeneration CardGeneration\",\n+                \"type TenuredGeneration Generation\",\n","filename":"test\/hotspot\/jtreg\/serviceability\/sa\/ClhsdbVmStructsDump.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -65,1 +65,0 @@\n-                \"type CardGeneration Generation\",\n","filename":"test\/hotspot\/jtreg\/serviceability\/sa\/TestType.java","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"}]}