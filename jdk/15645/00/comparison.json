{"files":[{"patch":"@@ -40,0 +40,1 @@\n+#include \"oops\/resolvedFieldEntry.hpp\"\n@@ -315,0 +316,30 @@\n+void InterpreterMacroAssembler::load_field_entry(Register cache, Register index, int bcp_offset) {\n+  \/\/ Get index out of bytecode pointer\n+  assert_different_registers(cache, index);\n+\n+  get_index_at_bcp(index, bcp_offset, cache \/*as tmp*\/, sizeof(u2));\n+\n+  \/\/ Scale the index to be the entry index * sizeof(ResolvedFieldEntry)\n+  \/\/ sizeof(ResolvedFieldEntry) is 16 on Arm, so using shift\n+  if (is_power_of_2(sizeof(ResolvedFieldEntry))) {\n+    \/\/ load constant pool cache pointer\n+    ldr(cache, Address(FP, frame::interpreter_frame_cache_offset * wordSize));\n+    \/\/ Get address of field entries array\n+    ldr(cache, Address(cache, in_bytes(ConstantPoolCache::field_entries_offset())));\n+\n+    add(cache, cache, Array<ResolvedFieldEntry>::base_offset_in_bytes());\n+    add(cache, cache, AsmOperand(index, lsl, log2i_exact(sizeof(ResolvedFieldEntry))));\n+  }\n+  else {\n+    mov(cache, sizeof(ResolvedFieldEntry));\n+    mul(index, index, cache);\n+    \/\/ load constant pool cache pointer\n+    ldr(cache, Address(FP, frame::interpreter_frame_cache_offset * wordSize));\n+\n+    \/\/ Get address of field entries array\n+    ldr(cache, Address(cache, in_bytes(ConstantPoolCache::field_entries_offset())));\n+    add(cache, cache, Array<ResolvedFieldEntry>::base_offset_in_bytes());\n+    add(cache, cache, index);\n+  }\n+}\n+\n","filename":"src\/hotspot\/cpu\/arm\/interp_masm_arm.cpp","additions":31,"deletions":0,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -105,0 +105,1 @@\n+  void load_field_entry(Register cache, Register index, int bcp_offset = 1);\n","filename":"src\/hotspot\/cpu\/arm\/interp_masm_arm.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -42,0 +42,1 @@\n+#include \"oops\/resolvedFieldEntry.hpp\"\n@@ -230,1 +231,10 @@\n-      __ get_cache_and_index_and_bytecode_at_bcp(bc_reg, temp_reg, temp_reg, byte_no, 1, sizeof(u2));\n+      __ load_field_entry(temp_reg, bc_reg);\n+      if (byte_no == f1_byte) {\n+        __ add(temp_reg, temp_reg, in_bytes(ResolvedFieldEntry::get_code_offset()));\n+      } else {\n+        __ add(temp_reg, temp_reg, in_bytes(ResolvedFieldEntry::put_code_offset()));\n+      }\n+      \/\/ Load-acquire the bytecode to match store-release in ResolvedFieldEntry::fill_in()\n+      __ ldrb(temp_reg, temp_reg);\n+      __ membar(MacroAssembler::Membar_mask_bits(MacroAssembler::LoadLoad | MacroAssembler::LoadStore), noreg, true);\n+\n@@ -2569,0 +2579,22 @@\n+  Bytecodes::Code code = bytecode();\n+  assert(byte_no == f1_byte || byte_no == f2_byte, \"byte_no out of range\");\n+  __ get_cache_and_index_and_bytecode_at_bcp(Rcache, Rindex, Rtemp, byte_no, 1, index_size);\n+  __ cmp(Rtemp, code);  \/\/ have we resolved this bytecode?\n+  __ b(resolved, eq);\n+\n+  \/\/ resolve first time through\n+  address entry = CAST_FROM_FN_PTR(address, InterpreterRuntime::resolve_from_cache);\n+  __ mov(R1, code);\n+  __ call_VM(noreg, entry, R1);\n+  \/\/ Update registers with resolved info\n+  __ get_cache_and_index_at_bcp(Rcache, Rindex, 1, index_size);\n+  __ bind(resolved);\n+}\n+\n+void TemplateTable::resolve_cache_and_index_for_field(int byte_no,\n+                                                      Register Rcache,\n+                                                      Register Rindex) {\n+  assert_different_registers(Rcache, Rindex, Rtemp);\n+\n+  Label resolved;\n+\n@@ -2577,1 +2609,11 @@\n-  __ get_cache_and_index_and_bytecode_at_bcp(Rcache, Rindex, Rtemp, byte_no, 1, index_size);\n+  __ load_field_entry(Rcache, Rindex);\n+  if (byte_no == f1_byte) {\n+    __ add(Rtemp, Rcache, in_bytes(ResolvedFieldEntry::get_code_offset()));\n+  } else {\n+    __ add(Rtemp, Rcache, in_bytes(ResolvedFieldEntry::put_code_offset()));\n+  }\n+\n+  \/\/ Load-acquire the bytecode to match store-release in ResolvedFieldEntry::fill_in()\n+  __ ldrb(Rtemp, Rtemp);\n+  __ membar(MacroAssembler::Membar_mask_bits(MacroAssembler::LoadLoad | MacroAssembler::LoadStore), noreg, true);\n+\n@@ -2586,1 +2628,1 @@\n-  __ get_cache_and_index_at_bcp(Rcache, Rindex, 1, index_size);\n+  __ load_field_entry(Rcache, Rindex);\n@@ -2590,0 +2632,25 @@\n+void TemplateTable::load_resolved_field_entry(Register obj,\n+                                              Register cache,\n+                                              Register tos_state,\n+                                              Register offset,\n+                                              Register flags,\n+                                              bool is_static = false) {\n+  assert_different_registers(cache, tos_state, flags, offset);\n+\n+  \/\/ Field offset\n+  __ ldr(offset, Address(cache, in_bytes(ResolvedFieldEntry::field_offset_offset())));\n+\n+  \/\/ Flags\n+  __ ldrb(flags, Address(cache, in_bytes(ResolvedFieldEntry::flags_offset())));\n+\n+  \/\/ TOS state\n+  __ ldrb(tos_state, Address(cache, in_bytes(ResolvedFieldEntry::type_offset())));\n+\n+  \/\/ Klass overwrite register\n+  if (is_static) {\n+    __ ldr(obj, Address(cache, ResolvedFieldEntry::field_holder_offset()));\n+    const int mirror_offset = in_bytes(Klass::java_mirror_offset());\n+    __ ldr(obj, Address(obj, mirror_offset));\n+    __ resolve_oop_handle(obj);\n+  }\n+}\n@@ -2745,2 +2812,2 @@\n-    __ add(R2, Rcache, AsmOperand(Rindex, lsl, LogBytesPerWord));\n-    __ add(R2, R2, in_bytes(ConstantPoolCache::base_offset()));\n+    __ load_field_entry(R2, Rindex);\n+\n@@ -2759,1 +2826,1 @@\n-    __ get_cache_and_index_at_bcp(Rcache, Rindex, 1);\n+    __ load_field_entry(Rcache, Rindex);\n@@ -2776,6 +2843,7 @@\n-  const Register Roffset  = R2_tmp;\n-  const Register Robj     = R3_tmp;\n-  const Register Rcache   = R4_tmp;\n-  const Register Rflagsav = Rtmp_save0;  \/\/ R4\/R19\n-  const Register Rindex   = R5_tmp;\n-  const Register Rflags   = R5_tmp;\n+  const Register Rcache = R4_tmp;\n+  const Register Rindex = R3_tmp;\n+\n+  const Register Roffset = R2_tmp;\n+  const Register Rtos_state = R3_tmp;\n+  const Register Robj = R4_tmp; \/\/ Rcache is free at the time of loading Robj\n+  const Register Rflags = R5_tmp;\n@@ -2783,1 +2851,1 @@\n-  resolve_cache_and_index(byte_no, Rcache, Rindex, sizeof(u2));\n+  resolve_cache_and_index_for_field(byte_no, Rcache, Rindex);\n@@ -2785,3 +2853,1 @@\n-  load_field_cp_cache_entry(Rcache, Rindex, Roffset, Rflags, Robj, is_static);\n-\n-  __ mov(Rflagsav, Rflags);\n+  load_resolved_field_entry(Robj, Rcache, Rtos_state, Roffset, Rflags, is_static);\n@@ -2789,1 +2855,3 @@\n-  if (!is_static) pop_and_check_object(Robj);\n+  if (!is_static) {\n+    pop_and_check_object(Robj);\n+  }\n@@ -2794,5 +2862,0 @@\n-  \/\/ compute type\n-  __ logical_shift_right(Rflags, Rflags, ConstantPoolCacheEntry::tos_state_shift);\n-  \/\/ Make sure we don't need to mask flags after the above shift\n-  ConstantPoolCacheEntry::verify_tos_state_shift();\n-\n@@ -2821,1 +2884,1 @@\n-  __ cmp(Rflags, itos);\n+  __ cmp(Rtos_state, itos);\n@@ -2823,1 +2886,1 @@\n-    __ cmp(Rflags, atos, ne);\n+    __ cmp(Rtos_state, atos, ne);\n@@ -2828,1 +2891,1 @@\n-    __ add(PC, PC, AsmOperand(Rflags, lsl, log_max_block_size + Assembler::LogInstructionSize), ne);\n+    __ add(PC, PC, AsmOperand(Rtos_state, lsl, log_max_block_size + Assembler::LogInstructionSize), ne);\n@@ -2830,1 +2893,1 @@\n-    __ ldr(PC, Address(PC, Rflags, lsl, LogBytesPerWord), ne);\n+    __ ldr(PC, Address(PC, Rtos_state, lsl, LogBytesPerWord), ne);\n@@ -2871,1 +2934,1 @@\n-    assert(ztos == seq++, \"btos has unexpected value\");\n+    assert(ztos == seq++, \"ztos has unexpected value\");\n@@ -2995,7 +3058,7 @@\n-  \/\/ Check for volatile field\n-  Label notVolatile;\n-  __ tbz(Rflagsav, ConstantPoolCacheEntry::is_volatile_shift, notVolatile);\n-\n-  volatile_barrier(MacroAssembler::Membar_mask_bits(MacroAssembler::LoadLoad | MacroAssembler::LoadStore), Rtemp);\n-\n-  __ bind(notVolatile);\n+  {\n+    \/\/ Check for volatile field\n+    Label notVolatile;\n+    __ tbz(Rflags, ResolvedFieldEntry::is_volatile_shift, notVolatile);\n+    __ membar(MacroAssembler::Membar_mask_bits(MacroAssembler::LoadLoad | MacroAssembler::LoadStore), Rtemp);\n+    __ bind(notVolatile);\n+  }\n@@ -3032,0 +3095,2 @@\n+    __ mov(R2, Rcache);\n+\n@@ -3040,0 +3105,1 @@\n+      __ ldrb(R3, Address(Rcache, in_bytes(ResolvedFieldEntry::type_offset())));\n@@ -3041,9 +3107,2 @@\n-      __ add(Rtemp, Rcache, AsmOperand(Rindex, lsl, LogBytesPerWord));\n-      __ ldr_u32(Rtemp, Address(Rtemp, cp_base_offset + ConstantPoolCacheEntry::flags_offset()));\n-\n-      __ logical_shift_right(Rtemp, Rtemp, ConstantPoolCacheEntry::tos_state_shift);\n-      \/\/ Make sure we don't need to mask Rtemp after the above shift\n-      ConstantPoolCacheEntry::verify_tos_state_shift();\n-\n-      __ cmp(Rtemp, ltos);\n-      __ cond_cmp(Rtemp, dtos, ne);\n+      __ cmp(R3, ltos);\n+      __ cond_cmp(R3, dtos, ne);\n@@ -3057,4 +3116,0 @@\n-    \/\/ cache entry pointer\n-    __ add(R2, Rcache, AsmOperand(Rindex, lsl, LogBytesPerWord));\n-    __ add(R2, R2, in_bytes(cp_base_offset));\n-\n@@ -3069,1 +3124,1 @@\n-    __ get_cache_and_index_at_bcp(Rcache, Rindex, 1);\n+    __ load_field_entry(Rcache, Rindex);\n@@ -3079,6 +3134,7 @@\n-  const Register Roffset  = R2_tmp;\n-  const Register Robj     = R3_tmp;\n-  const Register Rcache   = R4_tmp;\n-  const Register Rflagsav = Rtmp_save0;  \/\/ R4\/R19\n-  const Register Rindex   = R5_tmp;\n-  const Register Rflags   = R5_tmp;\n+  const Register Rcache = R4_tmp;\n+  const Register Rindex = R3_tmp;\n+\n+  const Register Roffset = R2_tmp;\n+  const Register Rtos_state = R3_tmp;\n+  const Register Robj = R4_tmp; \/\/ Rcache is free at the time of loading Robj\n+  const Register Rflags = R5_tmp;\n@@ -3086,1 +3142,1 @@\n-  resolve_cache_and_index(byte_no, Rcache, Rindex, sizeof(u2));\n+  resolve_cache_and_index_for_field(byte_no, Rcache, Rindex);\n@@ -3088,1 +3144,1 @@\n-  load_field_cp_cache_entry(Rcache, Rindex, Roffset, Rflags, Robj, is_static);\n+  load_resolved_field_entry(Robj, Rcache, Rtos_state, Roffset, Rflags, is_static);\n@@ -3091,7 +3147,6 @@\n-  Label notVolatile;\n-  __ mov(Rflagsav, Rflags);\n-  __ tbz(Rflagsav, ConstantPoolCacheEntry::is_volatile_shift, notVolatile);\n-\n-  volatile_barrier(MacroAssembler::Membar_mask_bits(MacroAssembler::StoreStore | MacroAssembler::LoadStore), Rtemp);\n-\n-  __ bind(notVolatile);\n+  {\n+    Label notVolatile;\n+    __ tbz(Rflags, ResolvedFieldEntry::is_volatile_shift, notVolatile);\n+    __ membar(MacroAssembler::Membar_mask_bits(MacroAssembler::StoreStore | MacroAssembler::LoadStore), Rtemp);\n+    __ bind(notVolatile);\n+  }\n@@ -3102,5 +3157,0 @@\n-  \/\/ compute type\n-  __ logical_shift_right(Rflags, Rflags, ConstantPoolCacheEntry::tos_state_shift);\n-  \/\/ Make sure we don't need to mask flags after the above shift\n-  ConstantPoolCacheEntry::verify_tos_state_shift();\n-\n@@ -3127,1 +3177,1 @@\n-  __ cmp(Rflags, itos);\n+  __ cmp(Rtos_state, itos);\n@@ -3131,1 +3181,1 @@\n-    __ add(PC, PC, AsmOperand(Rflags, lsl, log_max_block_size + Assembler::LogInstructionSize), ne);\n+    __ add(PC, PC, AsmOperand(Rtos_state, lsl, log_max_block_size + Assembler::LogInstructionSize), ne);\n@@ -3133,1 +3183,1 @@\n-    __ ldr(PC, Address(PC, Rflags, lsl, LogBytesPerWord), ne);\n+    __ ldr(PC, Address(PC, Rtos_state, lsl, LogBytesPerWord), ne);\n@@ -3270,1 +3320,1 @@\n-    assert(atos == seq++, \"dtos has unexpected value\");\n+    assert(atos == seq++, \"atos has unexpected value\");\n@@ -3296,26 +3346,5 @@\n-  Label notVolatile2;\n-  if (is_static) {\n-    \/\/ Just check for volatile. Memory barrier for static final field\n-    \/\/ is handled by class initialization.\n-    __ tbz(Rflagsav, ConstantPoolCacheEntry::is_volatile_shift, notVolatile2);\n-    volatile_barrier(MacroAssembler::StoreLoad, Rtemp);\n-    __ bind(notVolatile2);\n-  } else {\n-    \/\/ Check for volatile field and final field\n-    Label skipMembar;\n-\n-    __ tst(Rflagsav, 1 << ConstantPoolCacheEntry::is_volatile_shift |\n-           1 << ConstantPoolCacheEntry::is_final_shift);\n-    __ b(skipMembar, eq);\n-\n-    __ tbz(Rflagsav, ConstantPoolCacheEntry::is_volatile_shift, notVolatile2);\n-\n-    \/\/ StoreLoad barrier after volatile field write\n-    volatile_barrier(MacroAssembler::StoreLoad, Rtemp);\n-    __ b(skipMembar);\n-\n-    \/\/ StoreStore barrier after final field write\n-    __ bind(notVolatile2);\n-    volatile_barrier(MacroAssembler::StoreStore, Rtemp);\n-\n-    __ bind(skipMembar);\n+  {\n+    Label notVolatile;\n+    __ tbz(Rflags, ResolvedFieldEntry::is_volatile_shift, notVolatile);\n+    __ membar(MacroAssembler::Membar_mask_bits(MacroAssembler::StoreStore | MacroAssembler::LoadStore), Rtemp);\n+    __ bind(notVolatile);\n@@ -3361,1 +3390,1 @@\n-    __ get_cache_entry_pointer_at_bcp(R2, R1, 1);\n+    __ load_field_entry(R2, R1);\n@@ -3386,5 +3415,2 @@\n-  const Register Rcache  = R2_tmp;\n-  const Register Rindex  = R3_tmp;\n-  const Register Roffset = R3_tmp;\n-  const Register Rflags  = Rtmp_save0; \/\/ R4\/R19\n-  const Register Robj    = R5_tmp;\n+  const Register Rcache = R4_tmp;\n+  const Register Rindex = R3_tmp;\n@@ -3392,2 +3418,4 @@\n-  \/\/ access constant pool cache\n-  __ get_cache_and_index_at_bcp(Rcache, Rindex, 1);\n+  const Register Roffset = R2_tmp;\n+  const Register Rtos_state = R3_tmp;\n+  const Register Robj = R4_tmp;  \/\/ Rcache is free at the time of loading Robj\n+  const Register Rflags = R5_tmp;\n@@ -3395,1 +3423,3 @@\n-  __ add(Rcache, Rcache, AsmOperand(Rindex, lsl, LogBytesPerWord));\n+  \/\/ access constant pool cache\n+  __ load_field_entry(Rcache, Rindex);\n+  load_resolved_field_entry(Robj, Rcache, Rtos_state, Roffset, Rflags);\n@@ -3398,12 +3428,7 @@\n-  __ ldr_u32(Rflags, Address(Rcache, base + ConstantPoolCacheEntry::flags_offset()));\n-\n-  \/\/ replace index with field offset from cache entry\n-  __ ldr(Roffset, Address(Rcache, base + ConstantPoolCacheEntry::f2_offset()));\n-\n-  \/\/ Check for volatile store\n-  Label notVolatile;\n-  __ tbz(Rflags, ConstantPoolCacheEntry::is_volatile_shift, notVolatile);\n-\n-  volatile_barrier(MacroAssembler::Membar_mask_bits(MacroAssembler::StoreStore | MacroAssembler::LoadStore), Rtemp);\n-\n-  __ bind(notVolatile);\n+  {\n+    \/\/ Check for volatile store\n+    Label notVolatile;\n+    __ tbz(Rflags, ResolvedFieldEntry::is_volatile_shift, notVolatile);\n+    __ membar(MacroAssembler::Membar_mask_bits(MacroAssembler::StoreStore | MacroAssembler::LoadStore), Rtemp);\n+    __ bind(notVolatile);\n+  }\n@@ -3449,15 +3474,7 @@\n-  Label notVolatile2;\n-  Label skipMembar;\n-  __ tst(Rflags, 1 << ConstantPoolCacheEntry::is_volatile_shift |\n-         1 << ConstantPoolCacheEntry::is_final_shift);\n-  __ b(skipMembar, eq);\n-\n-  __ tbz(Rflags, ConstantPoolCacheEntry::is_volatile_shift, notVolatile2);\n-\n-  \/\/ StoreLoad barrier after volatile field write\n-  volatile_barrier(MacroAssembler::StoreLoad, Rtemp);\n-  __ b(skipMembar);\n-\n-  \/\/ StoreStore barrier after final field write\n-  __ bind(notVolatile2);\n-  volatile_barrier(MacroAssembler::StoreStore, Rtemp);\n+  {\n+    \/\/ Check for volatile store\n+    Label notVolatile;\n+    __ tbz(Rflags, ResolvedFieldEntry::is_volatile_shift, notVolatile);\n+    __ membar(MacroAssembler::Membar_mask_bits(MacroAssembler::StoreStore | MacroAssembler::LoadStore), Rtemp);\n+    __ bind(notVolatile);\n+  }\n@@ -3465,1 +3482,0 @@\n-  __ bind(skipMembar);\n@@ -3479,1 +3495,1 @@\n-    __ get_cache_entry_pointer_at_bcp(R2, R1, 1);\n+    __ load_field_entry(R2, R1);\n@@ -3498,1 +3514,1 @@\n-  __ get_cache_and_index_at_bcp(Rcache, Rindex, 1);\n+  __ load_field_entry(Rcache, Rindex);\n@@ -3500,2 +3516,1 @@\n-  __ add(Rtemp, Rcache, AsmOperand(Rindex, lsl, LogBytesPerWord));\n-  __ ldr(Roffset, Address(Rtemp, ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::f2_offset()));\n+  __ ldr(Roffset, Address(Rcache, ResolvedFieldEntry::field_offset_offset()));\n@@ -3504,1 +3519,1 @@\n-  __ ldr_u32(Rflags, Address(Rtemp, ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::flags_offset()));\n+  __ ldrb(Rflags, Address(Rcache, ResolvedFieldEntry::flags_offset()));\n@@ -3507,1 +3522,1 @@\n-  __ null_check(Robj, Rtemp);\n+  __ null_check(Robj);\n@@ -3541,7 +3556,7 @@\n-  \/\/ Check for volatile load\n-  Label notVolatile;\n-  __ tbz(Rflags, ConstantPoolCacheEntry::is_volatile_shift, notVolatile);\n-\n-  volatile_barrier(MacroAssembler::Membar_mask_bits(MacroAssembler::LoadLoad | MacroAssembler::LoadStore), Rtemp);\n-\n-  __ bind(notVolatile);\n+  {\n+    \/\/ Check for volatile load\n+    Label notVolatile;\n+    __ tbz(Rflags, ResolvedFieldEntry::is_volatile_shift, notVolatile);\n+    __ membar(MacroAssembler::Membar_mask_bits(MacroAssembler::LoadLoad | MacroAssembler::LoadStore), Rtemp);\n+    __ bind(notVolatile);\n+  }\n@@ -3565,3 +3580,2 @@\n-  __ get_cache_and_index_at_bcp(Rcache, Rindex, 2);\n-  __ add(Rtemp, Rcache, AsmOperand(Rindex, lsl, LogBytesPerWord));\n-  __ ldr(Roffset, Address(Rtemp, ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::f2_offset()));\n+  __ load_field_entry(Rcache, Rindex, 2);\n+  __ ldr(Roffset, Address(Rcache, ResolvedFieldEntry::field_offset_offset()));\n@@ -3570,1 +3584,1 @@\n-  __ ldr_u32(Rflags, Address(Rtemp, ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::flags_offset()));\n+  __ ldrb(Rflags, Address(Rcache, ResolvedFieldEntry::flags_offset()));\n@@ -3593,7 +3607,7 @@\n-  \/\/ Check for volatile load\n-  Label notVolatile;\n-  __ tbz(Rflags, ConstantPoolCacheEntry::is_volatile_shift, notVolatile);\n-\n-  volatile_barrier(MacroAssembler::Membar_mask_bits(MacroAssembler::LoadLoad | MacroAssembler::LoadStore), Rtemp);\n-\n-  __ bind(notVolatile);\n+  {\n+    \/\/ Check for volatile load\n+    Label notVolatile;\n+    __ tbz(Rflags, ResolvedFieldEntry::is_volatile_shift, notVolatile);\n+    __ membar(MacroAssembler::Membar_mask_bits(MacroAssembler::LoadLoad | MacroAssembler::LoadStore), Rtemp);\n+    __ bind(notVolatile);\n+  }\n","filename":"src\/hotspot\/cpu\/arm\/templateTable_arm.cpp","additions":173,"deletions":159,"binary":false,"changes":332,"status":"modified"}]}