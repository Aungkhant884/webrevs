{"files":[{"patch":"@@ -30,0 +30,1 @@\n+#include \"utilities\/sizes.hpp\"\n","filename":"src\/hotspot\/cpu\/x86\/vm_version_ext_x86.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"memory\/universe.hpp\"\n@@ -68,0 +69,16 @@\n+#ifdef _LP64\n+\n+bool VM_Version::supports_clflush() {\n+  \/\/ clflush should always be available on x86_64\n+  \/\/ if not we are in real trouble because we rely on it\n+  \/\/ to flush the code cache.\n+  \/\/ Unfortunately, Assembler::clflush is currently called as part\n+  \/\/ of generation of the code cache flush routine. This happens\n+  \/\/ under Universe::init before the processor features are set\n+  \/\/ up. Assembler::flush calls this routine to check that clflush\n+  \/\/ is allowed. So, we give the caller a free pass if Universe init\n+  \/\/ is still in progress.\n+  assert ((!Universe::is_fully_initialized() || (_features & CPU_FLUSH) != 0), \"clflush should be available\");\n+  return true;\n+}\n+#endif\n","filename":"src\/hotspot\/cpu\/x86\/vm_version_x86.cpp","additions":17,"deletions":0,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -28,1 +28,0 @@\n-#include \"memory\/universe.hpp\"\n@@ -31,0 +30,1 @@\n+#include \"utilities\/sizes.hpp\"\n@@ -264,1 +264,3 @@\n-                           : 28;\n+                           : 10,\n+                 serialize : 1,\n+                           : 17;\n@@ -362,1 +364,2 @@\n-    decl(HV,                \"hv\",                46) \/* Hypervisor instructions *\/\n+    decl(HV,                \"hv\",                46) \/* Hypervisor instructions *\/ \\\n+    decl(SERIALIZE,         \"serialize\",         47) \/* CPU SERIALIZE *\/\n@@ -649,0 +652,2 @@\n+      if (_cpuid_info.sef_cpuid7_edx.bits.serialize != 0)\n+        result |= CPU_SERIALIZE;\n@@ -899,0 +904,1 @@\n+  static bool supports_serialize()    { return (_features & CPU_SERIALIZE) != 0; }\n@@ -1030,13 +1036,2 @@\n-  static bool supports_clflush() {\n-    \/\/ clflush should always be available on x86_64\n-    \/\/ if not we are in real trouble because we rely on it\n-    \/\/ to flush the code cache.\n-    \/\/ Unfortunately, Assembler::clflush is currently called as part\n-    \/\/ of generation of the code cache flush routine. This happens\n-    \/\/ under Universe::init before the processor features are set\n-    \/\/ up. Assembler::flush calls this routine to check that clflush\n-    \/\/ is allowed. So, we give the caller a free pass if Universe init\n-    \/\/ is still in progress.\n-    assert ((!Universe::is_fully_initialized() || (_features & CPU_FLUSH) != 0), \"clflush should be available\");\n-    return true;\n-  }\n+\n+  static bool supports_clflush(); \/\/ Can't inline due to header file conflict\n","filename":"src\/hotspot\/cpu\/x86\/vm_version_x86.hpp","additions":11,"deletions":16,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -63,2 +63,6 @@\n-  int idx = 0;\n-  __asm__ volatile (\"cpuid \" : \"+a\" (idx) : : \"ebx\", \"ecx\", \"edx\", \"memory\");\n+  if (VM_Version::supports_serialize()) {\n+    __asm__ volatile (\".byte 0x0f, 0x01, 0xe8\\n\\t\" : : :); \/\/serialize\n+  } else {\n+    int idx = 0;\n+    __asm__ volatile (\"cpuid \" : \"+a\" (idx) : : \"ebx\", \"ecx\", \"edx\", \"memory\");\n+  }\n","filename":"src\/hotspot\/os_cpu\/bsd_x86\/orderAccess_bsd_x86.hpp","additions":6,"deletions":2,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -59,1 +59,4 @@\n-  int idx = 0;\n+  if (VM_Version::supports_serialize()) {\n+    __asm__ volatile (\".byte 0x0f, 0x01, 0xe8\\n\\t\" : : :); \/\/serialize\n+  } else {\n+    int idx = 0;\n@@ -61,1 +64,1 @@\n-  __asm__ volatile (\"cpuid \" : \"+a\" (idx) : : \"ebx\", \"ecx\", \"edx\", \"memory\");\n+    __asm__ volatile (\"cpuid \" : \"+a\" (idx) : : \"ebx\", \"ecx\", \"edx\", \"memory\");\n@@ -63,3 +66,3 @@\n-  \/\/ On some x86 systems EBX is a reserved register that cannot be\n-  \/\/ clobbered, so we must protect it around the CPUID.\n-  __asm__ volatile (\"xchg %%esi, %%ebx; cpuid; xchg %%esi, %%ebx \" : \"+a\" (idx) : : \"esi\", \"ecx\", \"edx\", \"memory\");\n+    \/\/ On some x86 systems EBX is a reserved register that cannot be\n+    \/\/ clobbered, so we must protect it around the CPUID.\n+    __asm__ volatile (\"xchg %%esi, %%ebx; cpuid; xchg %%esi, %%ebx \" : \"+a\" (idx) : : \"esi\", \"ecx\", \"edx\", \"memory\");\n@@ -67,0 +70,1 @@\n+  }\n","filename":"src\/hotspot\/os_cpu\/linux_x86\/orderAccess_linux_x86.hpp","additions":9,"deletions":5,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -62,2 +62,6 @@\n-  int regs[4];\n-  __cpuid(regs, 0);\n+  if (VM_Version::supports_serialize()) {\n+    _serialize();\n+  } else {\n+    int regs[4];\n+    __cpuid(regs, 0);\n+  }\n","filename":"src\/hotspot\/os_cpu\/windows_x86\/orderAccess_windows_x86.hpp","additions":6,"deletions":2,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"runtime\/vm_version.hpp\"\n","filename":"src\/hotspot\/share\/runtime\/orderAccess.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"runtime\/globals.hpp\"\n","filename":"src\/hotspot\/share\/runtime\/vm_version.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"}]}