[{"commit":{"message":"8265263: AArch64: Combine vneg with right shift count\n\n*** Implementation\n\nIn AArch64 NEON, vector shift right is implemented by vector shift left\ninstructions (SSHL[1] and USHL[2]) with negative shift count value. In\nC2 backend, we generate a `neg` to given shift value followed by `sshl`\nor `ushl` instruction.\n\nFor vector shift right, the vector shift count has two origins:\n1) it can be duplicated from scalar variable\/immediate(case-1),\n2) it can be loaded directly from one vector(case-2).\n\nThis patch aims to optimize case-1. Specifically, we move the negate\nfrom RShiftV* rules to RShiftCntV rule. As a result, the negate can be\nhoisted outside of the loop if it's a loop invariant.\n\nIn this patch,\n1) we split vshiftcnt* rules into vslcnt* and vsrcnt* rules to handle\nshift left and shift right respectively. Compared to vslcnt* rules, the\nnegate is conducted in vsrcnt*.\n2) for each vsra* and vsrl* rules, we create one variant, i.e. vsra*_var\nand vsrl*_var. We use vsra* and vsrl* rules to handle case-1, and use\nvsra*_var and vsrl*_var rules to handle case-2. Note that\nShiftVNode::is_var_shift() can be used to distinguish case-1 from\ncase-2.\n3) we add one assertion for the vs*_imm rules as we have done on\nARM32[3].\n4) several style issues are resolved.\n\n*** Example\n\nTake function `rShiftInt()` in the newly added micro benchmark\nVectorShiftRight.java as an example.\n\n```\npublic void rShiftInt() {\n    for (int i = 0; i < SIZE; i++) {\n        intsB[i] = intsA[i] >> count;\n    }\n}\n```\n\nArithmetic shift right is conducted inside a big loop. The following\ncode snippet shows the disassembly code generated by auto-vectorization\nbefore we apply current patch. We can see that `neg` is conducted in the\nloop body.\n\n```\n0x0000ffff89057a64:   dup     v16.16b, w13              <-- dup\n0x0000ffff89057a68:   mov     w12, #0x7d00                    \/\/ #32000\n0x0000ffff89057a6c:   sub     w13, w2, w10\n0x0000ffff89057a70:   cmp     w2, w10\n0x0000ffff89057a74:   csel    w13, wzr, w13, lt\n0x0000ffff89057a78:   mov     w8, #0x7d00                     \/\/ #32000\n0x0000ffff89057a7c:   cmp     w13, w8\n0x0000ffff89057a80:   csel    w13, w12, w13, hi\n0x0000ffff89057a84:   add     w14, w13, w10\n0x0000ffff89057a88:   nop\n0x0000ffff89057a8c:   nop\n0x0000ffff89057a90:   sbfiz   x13, x10, #2, #32         <-- loop entry\n0x0000ffff89057a94:   add     x15, x17, x13\n0x0000ffff89057a98:   ldr     q17, [x15,#16]\n0x0000ffff89057a9c:   add     x13, x0, x13\n0x0000ffff89057aa0:   neg     v18.16b, v16.16b          <-- neg\n0x0000ffff89057aa4:   sshl    v17.4s, v17.4s, v18.4s    <-- shift right\n0x0000ffff89057aa8:   str     q17, [x13,#16]\n0x0000ffff89057aac:   ...\n0x0000ffff89057b1c:   add     w10, w10, #0x20\n0x0000ffff89057b20:   cmp     w10, w14\n0x0000ffff89057b24:   b.lt    0x0000ffff89057a90        <-- loop end\n```\n\nHere is the disassembly code after we apply current patch. We can see\nthat the negate is no longer conducted inside the loop, and it is\nhoisted to the outside.\n\n```\n0x0000ffff8d053a68:   neg     w14, w13                  <---- neg\n0x0000ffff8d053a6c:   dup     v16.16b, w14              <---- dup\n0x0000ffff8d053a70:   sub     w14, w2, w10\n0x0000ffff8d053a74:   cmp     w2, w10\n0x0000ffff8d053a78:   csel    w14, wzr, w14, lt\n0x0000ffff8d053a7c:   mov     w8, #0x7d00                     \/\/ #32000\n0x0000ffff8d053a80:   cmp     w14, w8\n0x0000ffff8d053a84:   csel    w14, w12, w14, hi\n0x0000ffff8d053a88:   add     w13, w14, w10\n0x0000ffff8d053a8c:   nop\n0x0000ffff8d053a90:   sbfiz   x14, x10, #2, #32         <-- loop entry\n0x0000ffff8d053a94:   add     x15, x17, x14\n0x0000ffff8d053a98:   ldr     q17, [x15,#16]\n0x0000ffff8d053a9c:   sshl    v17.4s, v17.4s, v16.4s    <-- shift right\n0x0000ffff8d053aa0:   add     x14, x0, x14\n0x0000ffff8d053aa4:   str     q17, [x14,#16]\n0x0000ffff8d053aa8:   ...\n0x0000ffff8d053afc:   add     w10, w10, #0x20\n0x0000ffff8d053b00:   cmp     w10, w13\n0x0000ffff8d053b04:   b.lt    0x0000ffff8d053a90        <-- loop end\n```\n\n*** Testing\n\nTier1~3 tests passed on Linux\/AArch64 platform.\n\n*** Performance Evaluation\n\n- Auto-vectorization\n\nOne micro benchmark, i.e. VectorShiftRight.java, is added by this patch\nin order to evaluate the optimization on vector shift right.\n\nThe following table shows the result. Column `Score-1` shows the score\nbefore we apply current patch, and column `Score-2` shows the score when\nwe apply current patch.\n\nWe witness about 30% ~ 53% improvement on microbenchmarks.\n\n```\nBenchmark                      Units    Score-1    Score-2\nVectorShiftRight.rShiftByte   ops\/ms  10601.980  13816.353\nVectorShiftRight.rShiftInt    ops\/ms   3592.831   5502.941\nVectorShiftRight.rShiftLong   ops\/ms   1584.012   2425.247\nVectorShiftRight.rShiftShort  ops\/ms   6643.414   9728.762\nVectorShiftRight.urShiftByte  ops\/ms   2066.965   2048.336 (*)\nVectorShiftRight.urShiftChar  ops\/ms   6660.805   9728.478\nVectorShiftRight.urShiftInt   ops\/ms   3592.909   5514.928\nVectorShiftRight.urShiftLong  ops\/ms   1583.995   2422.991\n\n*: Logical shift right for Byte type(urShiftByte) is not vectorized, as\ndisscussed in [4].\n```\n\n- VectorAPI\n\nFurthermore, we also evaluate the impact of this patch on VectorAPI\nbenchmarks, e.g., [5]. Details can be found in the table below. Columns\n`Score-1` and `Score-2` show the scores before and after applying\ncurrent patch.\n\n```\nBenchmark                  Units    Score-1    Score-2\nByte128Vector.LSHL        ops\/ms  10867.666  10873.993\nByte128Vector.LSHLShift   ops\/ms  10945.729  10945.741\nByte128Vector.LSHR        ops\/ms   8629.305   8629.343\nByte128Vector.LSHRShift   ops\/ms   8245.864  10303.521   <--\nByte128Vector.ASHR        ops\/ms   8619.691   8629.438\nByte128Vector.ASHRShift   ops\/ms   8245.860  10305.027   <--\nInt128Vector.LSHL         ops\/ms   3104.213   3103.702\nInt128Vector.LSHLShift    ops\/ms   3114.354   3114.371\nInt128Vector.LSHR         ops\/ms   2380.717   2380.693\nInt128Vector.LSHRShift    ops\/ms   2312.871   2992.377   <--\nInt128Vector.ASHR         ops\/ms   2380.668   2380.647\nInt128Vector.ASHRShift    ops\/ms   2312.894   2992.332   <--\nLong128Vector.LSHL        ops\/ms   1586.907   1587.591\nLong128Vector.LSHLShift   ops\/ms   1589.469   1589.540\nLong128Vector.LSHR        ops\/ms   1209.754   1209.687\nLong128Vector.LSHRShift   ops\/ms   1174.718   1527.502   <--\nLong128Vector.ASHR        ops\/ms   1209.713   1209.669\nLong128Vector.ASHRShift   ops\/ms   1174.712   1527.174   <--\nShort128Vector.LSHL       ops\/ms   5945.542   5943.770\nShort128Vector.LSHLShift  ops\/ms   5984.743   5984.640\nShort128Vector.LSHR       ops\/ms   4613.378   4613.577\nShort128Vector.LSHRShift  ops\/ms   4486.023   5746.466   <--\nShort128Vector.ASHR       ops\/ms   4613.389   4613.478\nShort128Vector.ASHRShift  ops\/ms   4486.019   5746.368   <--\n```\n\n1) For logical shift left(LSHL and LSHLShift), and shift right with\nvariable vector shift count(LSHR and ASHR) cases, we didn't find much\nchanges, which is expected.\n\n2) For shift right with scalar shift count(LSHRShift and ASHRShift)\ncase, about 25% ~ 30% improvement can be observed, and this benefit is\nintroduced by current patch.\n\n[1] https:\/\/developer.arm.com\/documentation\/ddi0596\/2020-12\/SIMD-FP-Instructions\/SSHL--Signed-Shift-Left--register--\n[2] https:\/\/developer.arm.com\/documentation\/ddi0596\/2020-12\/SIMD-FP-Instructions\/USHL--Unsigned-Shift-Left--register--\n[3] https:\/\/github.com\/openjdk\/jdk18\/pull\/41\n[4] https:\/\/github.com\/openjdk\/jdk\/pull\/1087\n[5] https:\/\/github.com\/openjdk\/panama-vector\/blob\/vectorIntrinsics\/test\/micro\/org\/openjdk\/bench\/jdk\/incubator\/vector\/operation\/Byte128Vector.java#L509"},"files":[{"filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad"},{"filename":"src\/hotspot\/cpu\/aarch64\/aarch64_neon.ad"},{"filename":"src\/hotspot\/cpu\/aarch64\/aarch64_neon_ad.m4"},{"filename":"test\/micro\/org\/openjdk\/bench\/vm\/compiler\/VectorShiftRight.java"}],"sha":"20b6762ddc7a8bd1a7686beb6590b7d764f67d64"}]