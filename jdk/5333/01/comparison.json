{"files":[{"patch":"@@ -35,0 +35,2 @@\n+#include \"gc\/g1\/g1BatchedGangTask.hpp\"\n+#include \"gc\/g1\/g1CardSetFreeMemoryTask.hpp\"\n@@ -72,1 +74,1 @@\n-#include \"gc\/g1\/g1YoungGCPostEvacuateTasks.hpp\"\n+#include \"gc\/g1\/g1YoungCollector.hpp\"\n@@ -139,6 +141,0 @@\n-Tickspan G1CollectedHeap::run_task_timed(AbstractGangTask* task) {\n-  Ticks start = Ticks::now();\n-  workers()->run_task(task);\n-  return Ticks::now() - start;\n-}\n-\n@@ -2700,68 +2696,0 @@\n-uint G1CollectedHeap::num_task_queues() const {\n-  return _task_queues->size();\n-}\n-\n-#if TASKQUEUE_STATS\n-void G1CollectedHeap::print_taskqueue_stats_hdr(outputStream* const st) {\n-  st->print_raw_cr(\"GC Task Stats\");\n-  st->print_raw(\"thr \"); TaskQueueStats::print_header(1, st); st->cr();\n-  st->print_raw(\"--- \"); TaskQueueStats::print_header(2, st); st->cr();\n-}\n-\n-void G1CollectedHeap::print_taskqueue_stats() const {\n-  if (!log_is_enabled(Trace, gc, task, stats)) {\n-    return;\n-  }\n-  Log(gc, task, stats) log;\n-  ResourceMark rm;\n-  LogStream ls(log.trace());\n-  outputStream* st = &ls;\n-\n-  print_taskqueue_stats_hdr(st);\n-\n-  TaskQueueStats totals;\n-  const uint n = num_task_queues();\n-  for (uint i = 0; i < n; ++i) {\n-    st->print(\"%3u \", i); task_queue(i)->stats.print(st); st->cr();\n-    totals += task_queue(i)->stats;\n-  }\n-  st->print_raw(\"tot \"); totals.print(st); st->cr();\n-\n-  DEBUG_ONLY(totals.verify());\n-}\n-\n-void G1CollectedHeap::reset_taskqueue_stats() {\n-  const uint n = num_task_queues();\n-  for (uint i = 0; i < n; ++i) {\n-    task_queue(i)->stats.reset();\n-  }\n-}\n-#endif \/\/ TASKQUEUE_STATS\n-\n-void G1CollectedHeap::wait_for_root_region_scanning() {\n-  double scan_wait_start = os::elapsedTime();\n-  \/\/ We have to wait until the CM threads finish scanning the\n-  \/\/ root regions as it's the only way to ensure that all the\n-  \/\/ objects on them have been correctly scanned before we start\n-  \/\/ moving them during the GC.\n-  bool waited = _cm->root_regions()->wait_until_scan_finished();\n-  double wait_time_ms = 0.0;\n-  if (waited) {\n-    double scan_wait_end = os::elapsedTime();\n-    wait_time_ms = (scan_wait_end - scan_wait_start) * 1000.0;\n-  }\n-  phase_times()->record_root_region_scan_wait_time(wait_time_ms);\n-}\n-\n-class G1PrintCollectionSetClosure : public HeapRegionClosure {\n-private:\n-  G1HRPrinter* _hr_printer;\n-public:\n-  G1PrintCollectionSetClosure(G1HRPrinter* hr_printer) : HeapRegionClosure(), _hr_printer(hr_printer) { }\n-\n-  virtual bool do_heap_region(HeapRegion* r) {\n-    _hr_printer->cset(r);\n-    return false;\n-  }\n-};\n-\n@@ -2785,18 +2713,0 @@\n-void G1CollectedHeap::calculate_collection_set(G1EvacuationInfo* evacuation_info, double target_pause_time_ms) {\n-  \/\/ Forget the current allocation region (we might even choose it to be part\n-  \/\/ of the collection set!) before finalizing the collection set.\n-  _allocator->release_mutator_alloc_regions();\n-\n-  _collection_set.finalize_initial_collection_set(target_pause_time_ms, &_survivor);\n-  evacuation_info->set_collectionset_regions(collection_set()->region_length() +\n-                                            collection_set()->optional_region_length());\n-\n-  _cm->verify_no_collection_set_oops();\n-\n-  if (_hr_printer.is_active()) {\n-    G1PrintCollectionSetClosure cl(&_hr_printer);\n-    _collection_set.iterate(&cl);\n-    _collection_set.iterate_optional(&cl);\n-  }\n-}\n-\n@@ -2877,70 +2787,0 @@\n-\/\/ GCTraceTime wrapper that constructs the message according to GC pause type and\n-\/\/ GC cause.\n-\/\/ The code relies on the fact that GCTraceTimeWrapper stores the string passed\n-\/\/ initially as a reference only, so that we can modify it as needed.\n-class G1YoungGCTraceTime {\n-  G1GCPauseType _pause_type;\n-  GCCause::Cause _pause_cause;\n-\n-  static const uint MaxYoungGCNameLength = 128;\n-  char _young_gc_name_data[MaxYoungGCNameLength];\n-\n-  GCTraceTime(Info, gc) _tt;\n-\n-  const char* update_young_gc_name() {\n-    snprintf(_young_gc_name_data,\n-             MaxYoungGCNameLength,\n-             \"Pause Young (%s) (%s)%s\",\n-             G1GCPauseTypeHelper::to_string(_pause_type),\n-             GCCause::to_string(_pause_cause),\n-             G1CollectedHeap::heap()->evacuation_failed() ? \" (Evacuation Failure)\" : \"\");\n-    return _young_gc_name_data;\n-  }\n-\n-public:\n-  G1YoungGCTraceTime(GCCause::Cause cause) :\n-    \/\/ Take snapshot of current pause type at start as it may be modified during gc.\n-    \/\/ The strings for all Concurrent Start pauses are the same, so the parameter\n-    \/\/ does not matter here.\n-    _pause_type(G1CollectedHeap::heap()->collector_state()->young_gc_pause_type(false \/* concurrent_operation_is_full_mark *\/)),\n-    _pause_cause(cause),\n-    \/\/ Fake a \"no cause\" and manually add the correct string in update_young_gc_name()\n-    \/\/ to make the string look more natural.\n-    _tt(update_young_gc_name(), NULL, GCCause::_no_gc, true) {\n-  }\n-\n-  ~G1YoungGCTraceTime() {\n-    update_young_gc_name();\n-  }\n-};\n-\n-class G1YoungGCVerifierMark : public StackObj {\n-  G1HeapVerifier::G1VerifyType _type;\n-\n-  static G1HeapVerifier::G1VerifyType young_collection_verify_type() {\n-    G1CollectorState* state = G1CollectedHeap::heap()->collector_state();\n-    if (state->in_concurrent_start_gc()) {\n-      return G1HeapVerifier::G1VerifyConcurrentStart;\n-    } else if (state->in_young_only_phase()) {\n-      return G1HeapVerifier::G1VerifyYoungNormal;\n-    } else {\n-      return G1HeapVerifier::G1VerifyMixed;\n-    }\n-  }\n-\n-public:\n-  G1YoungGCVerifierMark() : _type(young_collection_verify_type()) {\n-    G1CollectedHeap::heap()->verify_before_young_collection(_type);\n-  }\n-\n-  ~G1YoungGCVerifierMark() {\n-    G1CollectedHeap::heap()->verify_after_young_collection(_type);\n-  }\n-};\n-\n-class G1YoungGCNotifyPauseMark : public StackObj {\n-public:\n-  G1YoungGCNotifyPauseMark() { G1CollectedHeap::heap()->policy()->record_young_gc_pause_start(); }\n-  ~G1YoungGCNotifyPauseMark() { G1CollectedHeap::heap()->policy()->record_young_gc_pause_end(); }\n-};\n-\n@@ -2981,45 +2821,0 @@\n-class G1YoungGCJFRTracerMark : public G1JFRTracerMark {\n-  G1EvacuationInfo _evacuation_info;\n-\n-  G1NewTracer* tracer() const { return (G1NewTracer*)_tracer; }\n-\n-public:\n-\n-  G1EvacuationInfo* evacuation_info() { return &_evacuation_info; }\n-\n-  G1YoungGCJFRTracerMark(STWGCTimer* gc_timer_stw, G1NewTracer* gc_tracer_stw, GCCause::Cause cause) :\n-    G1JFRTracerMark(gc_timer_stw, gc_tracer_stw), _evacuation_info() { }\n-\n-  void report_pause_type(G1GCPauseType type) {\n-    tracer()->report_young_gc_pause(type);\n-  }\n-\n-  ~G1YoungGCJFRTracerMark() {\n-    G1CollectedHeap* g1h = G1CollectedHeap::heap();\n-\n-    tracer()->report_evacuation_info(&_evacuation_info);\n-    tracer()->report_tenuring_threshold(g1h->policy()->tenuring_threshold());\n-  }\n-};\n-\n-class G1PreservedMarksSet : public PreservedMarksSet {\n-public:\n-\n-  G1PreservedMarksSet(uint num_workers) : PreservedMarksSet(true \/* in_c_heap *\/) {\n-    init(num_workers);\n-  }\n-\n-  virtual ~G1PreservedMarksSet() {\n-    assert_empty();\n-    reclaim();\n-  }\n-};\n-\n-void G1CollectedHeap::set_young_collection_default_active_worker_threads(){\n-  uint active_workers = WorkerPolicy::calc_active_workers(workers()->total_workers(),\n-                                                          workers()->active_workers(),\n-                                                          Threads::number_of_non_daemon_threads());\n-  active_workers = workers()->update_active_workers(active_workers);\n-  log_info(gc,task)(\"Using %u workers of %u for evacuation\", active_workers, workers()->total_workers());\n-}\n-\n@@ -3054,0 +2849,2 @@\n+  _bytes_used_during_gc = 0;\n+\n@@ -3059,1 +2856,0 @@\n-  bool concurrent_operation_is_full_mark = false;\n@@ -3061,67 +2857,3 @@\n-  {\n-    \/\/ Do timing\/tracing\/statistics\/pre- and post-logging\/verification work not\n-    \/\/ directly related to the collection. They should not be accounted for in\n-    \/\/ collection work timing.\n-\n-    \/\/ The G1YoungGCTraceTime message depends on collector state, so must come after\n-    \/\/ determining collector state.\n-    G1YoungGCTraceTime tm(gc_cause());\n-\n-    \/\/ JFR\n-    G1YoungGCJFRTracerMark jtm(_gc_timer_stw, _gc_tracer_stw, gc_cause());\n-    \/\/ JStat\/MXBeans\n-    G1MonitoringScope ms(monitoring_support(),\n-                         false \/* full_gc *\/,\n-                         collector_state()->in_mixed_phase() \/* all_memory_pools_affected *\/);\n-    \/\/ Create the heap printer before internal pause timing to have\n-    \/\/ heap information printed as last part of detailed GC log.\n-    G1HeapPrinterMark hpm(this);\n-    \/\/ Young GC internal pause timing\n-    G1YoungGCNotifyPauseMark npm;\n-\n-    \/\/ Verification may use the gang workers, so they must be set up before.\n-    \/\/ Individual parallel phases may override this.\n-    set_young_collection_default_active_worker_threads();\n-\n-    \/\/ Wait for root region scan here to make sure that it is done before any\n-    \/\/ use of the STW work gang to maximize cpu use (i.e. all cores are available\n-    \/\/ just to do that).\n-    wait_for_root_region_scanning();\n-\n-    G1YoungGCVerifierMark vm;\n-    {\n-      \/\/ Actual collection work starts and is executed (only) in this scope.\n-\n-      \/\/ Young GC internal collection timing. The elapsed time recorded in the\n-      \/\/ policy for the collection deliberately elides verification (and some\n-      \/\/ other trivial setup above).\n-      policy()->record_young_collection_start();\n-\n-      calculate_collection_set(jtm.evacuation_info(), target_pause_time_ms);\n-\n-      G1RedirtyCardsQueueSet rdcqs(G1BarrierSet::dirty_card_queue_set().allocator());\n-      G1PreservedMarksSet preserved_marks_set(workers()->active_workers());\n-      G1ParScanThreadStateSet per_thread_states(this,\n-                                                &rdcqs,\n-                                                &preserved_marks_set,\n-                                                workers()->active_workers(),\n-                                                collection_set()->young_region_length(),\n-                                                collection_set()->optional_region_length());\n-      pre_evacuate_collection_set(jtm.evacuation_info(), &per_thread_states);\n-\n-      bool may_do_optional_evacuation = _collection_set.optional_region_length() != 0;\n-      \/\/ Actually do the work...\n-      evacuate_initial_collection_set(&per_thread_states, may_do_optional_evacuation);\n-\n-      if (may_do_optional_evacuation) {\n-        evacuate_optional_collection_set(&per_thread_states);\n-      }\n-      post_evacuate_collection_set(jtm.evacuation_info(), &per_thread_states);\n-\n-      \/\/ Refine the type of a concurrent mark operation now that we did the\n-      \/\/ evacuation, eventually aborting it.\n-      concurrent_operation_is_full_mark = policy()->concurrent_operation_is_full_mark(\"Revise IHOP\");\n-\n-      \/\/ Need to report the collection pause now since record_collection_pause_end()\n-      \/\/ modifies it to the next state.\n-      jtm.report_pause_type(collector_state()->young_gc_pause_type(concurrent_operation_is_full_mark));\n+  \/\/ Perform the collection.\n+  G1YoungCollector collector(gc_cause(), target_pause_time_ms);\n+  collector.collect();\n@@ -3129,5 +2861,0 @@\n-      policy()->record_young_collection_end(concurrent_operation_is_full_mark);\n-    }\n-    TASKQUEUE_STATS_ONLY(print_taskqueue_stats());\n-    TASKQUEUE_STATS_ONLY(reset_taskqueue_stats());\n-  }\n@@ -3143,1 +2870,1 @@\n-    start_concurrent_cycle(concurrent_operation_is_full_mark);\n+    start_concurrent_cycle(collector.concurrent_operation_is_full_mark());\n@@ -3148,22 +2875,0 @@\n-bool G1ParEvacuateFollowersClosure::offer_termination() {\n-  EventGCPhaseParallel event;\n-  G1ParScanThreadState* const pss = par_scan_state();\n-  start_term_time();\n-  const bool res = (terminator() == nullptr) ? true : terminator()->offer_termination();\n-  end_term_time();\n-  event.commit(GCId::current(), pss->worker_id(), G1GCPhaseTimes::phase_name(G1GCPhaseTimes::Termination));\n-  return res;\n-}\n-\n-void G1ParEvacuateFollowersClosure::do_void() {\n-  EventGCPhaseParallel event;\n-  G1ParScanThreadState* const pss = par_scan_state();\n-  pss->trim_queue();\n-  event.commit(GCId::current(), pss->worker_id(), G1GCPhaseTimes::phase_name(_phase));\n-  do {\n-    EventGCPhaseParallel event;\n-    pss->steal_and_trim_queue(queues());\n-    event.commit(GCId::current(), pss->worker_id(), G1GCPhaseTimes::phase_name(_phase));\n-  } while (!offer_termination());\n-}\n-\n@@ -3177,8 +2882,0 @@\n-\/\/ Weak Reference Processing support\n-\n-bool G1STWIsAliveClosure::do_object_b(oop p) {\n-  \/\/ An object is reachable if it is outside the collection set,\n-  \/\/ or is inside and copied.\n-  return !_g1h->is_in_cset(p) || p->is_forwarded();\n-}\n-\n@@ -3194,125 +2891,0 @@\n-\/\/ Non Copying Keep Alive closure\n-class G1KeepAliveClosure: public OopClosure {\n-  G1CollectedHeap*_g1h;\n-public:\n-  G1KeepAliveClosure(G1CollectedHeap* g1h) :_g1h(g1h) {}\n-  void do_oop(narrowOop* p) { guarantee(false, \"Not needed\"); }\n-  void do_oop(oop* p) {\n-    oop obj = *p;\n-    assert(obj != NULL, \"the caller should have filtered out NULL values\");\n-\n-    const G1HeapRegionAttr region_attr =_g1h->region_attr(obj);\n-    if (!region_attr.is_in_cset_or_humongous()) {\n-      return;\n-    }\n-    if (region_attr.is_in_cset()) {\n-      assert( obj->is_forwarded(), \"invariant\" );\n-      *p = obj->forwardee();\n-    } else {\n-      assert(!obj->is_forwarded(), \"invariant\" );\n-      assert(region_attr.is_humongous(),\n-             \"Only allowed G1HeapRegionAttr state is IsHumongous, but is %d\", region_attr.type());\n-     _g1h->set_humongous_is_live(obj);\n-    }\n-  }\n-};\n-\n-\/\/ Copying Keep Alive closure - can be called from both\n-\/\/ serial and parallel code as long as different worker\n-\/\/ threads utilize different G1ParScanThreadState instances\n-\/\/ and different queues.\n-\n-class G1CopyingKeepAliveClosure: public OopClosure {\n-  G1CollectedHeap*         _g1h;\n-  G1ParScanThreadState*    _par_scan_state;\n-\n-public:\n-  G1CopyingKeepAliveClosure(G1CollectedHeap* g1h,\n-                            G1ParScanThreadState* pss):\n-    _g1h(g1h),\n-    _par_scan_state(pss)\n-  {}\n-\n-  virtual void do_oop(narrowOop* p) { do_oop_work(p); }\n-  virtual void do_oop(      oop* p) { do_oop_work(p); }\n-\n-  template <class T> void do_oop_work(T* p) {\n-    oop obj = RawAccess<>::oop_load(p);\n-\n-    if (_g1h->is_in_cset_or_humongous(obj)) {\n-      \/\/ If the referent object has been forwarded (either copied\n-      \/\/ to a new location or to itself in the event of an\n-      \/\/ evacuation failure) then we need to update the reference\n-      \/\/ field and, if both reference and referent are in the G1\n-      \/\/ heap, update the RSet for the referent.\n-      \/\/\n-      \/\/ If the referent has not been forwarded then we have to keep\n-      \/\/ it alive by policy. Therefore we have copy the referent.\n-      \/\/\n-      \/\/ When the queue is drained (after each phase of reference processing)\n-      \/\/ the object and it's followers will be copied, the reference field set\n-      \/\/ to point to the new location, and the RSet updated.\n-      _par_scan_state->push_on_queue(ScannerTask(p));\n-    }\n-  }\n-};\n-\n-class G1STWRefProcProxyTask : public RefProcProxyTask {\n-  G1CollectedHeap& _g1h;\n-  G1ParScanThreadStateSet& _pss;\n-  TaskTerminator _terminator;\n-  G1ScannerTasksQueueSet& _task_queues;\n-\n-public:\n-  G1STWRefProcProxyTask(uint max_workers, G1CollectedHeap& g1h, G1ParScanThreadStateSet& pss, G1ScannerTasksQueueSet& task_queues)\n-    : RefProcProxyTask(\"G1STWRefProcProxyTask\", max_workers),\n-      _g1h(g1h),\n-      _pss(pss),\n-      _terminator(max_workers, &task_queues),\n-      _task_queues(task_queues) {}\n-\n-  void work(uint worker_id) override {\n-    assert(worker_id < _max_workers, \"sanity\");\n-    uint index = (_tm == RefProcThreadModel::Single) ? 0 : worker_id;\n-\n-    G1ParScanThreadState* pss = _pss.state_for_worker(index);\n-    pss->set_ref_discoverer(nullptr);\n-\n-    G1STWIsAliveClosure is_alive(&_g1h);\n-    G1CopyingKeepAliveClosure keep_alive(&_g1h, pss);\n-    G1ParEvacuateFollowersClosure complete_gc(&_g1h, pss, &_task_queues, _tm == RefProcThreadModel::Single ? nullptr : &_terminator, G1GCPhaseTimes::ObjCopy);\n-    _rp_task->rp_work(worker_id, &is_alive, &keep_alive, &complete_gc);\n-\n-    \/\/ We have completed copying any necessary live referent objects.\n-    assert(pss->queue_is_empty(), \"both queue and overflow should be empty\");\n-  }\n-\n-  void prepare_run_task_hook() override {\n-    _terminator.reset_for_reuse(_queue_count);\n-  }\n-};\n-\n-\/\/ End of weak reference support closures\n-\n-void G1CollectedHeap::process_discovered_references(G1ParScanThreadStateSet* per_thread_states) {\n-  Ticks start = Ticks::now();\n-\n-  ReferenceProcessor* rp = _ref_processor_stw;\n-  assert(rp->discovery_enabled(), \"should have been enabled\");\n-\n-  uint no_of_gc_workers = workers()->active_workers();\n-  rp->set_active_mt_degree(no_of_gc_workers);\n-\n-  G1STWRefProcProxyTask task(rp->max_num_queues(), *this, *per_thread_states, *_task_queues);\n-  ReferenceProcessorPhaseTimes& pt = *phase_times()->ref_phase_times();\n-  ReferenceProcessorStats stats = rp->process_discovered_references(task, pt);\n-\n-  _gc_tracer_stw->report_gc_reference_stats(stats);\n-\n-  make_pending_list_reachable();\n-\n-  rp->verify_no_references_recorded();\n-\n-  phase_times()->record_ref_proc_time((Ticks::now() - start).seconds() * MILLIUNITS);\n-}\n-\n@@ -3341,0 +2913,5 @@\n+void G1CollectedHeap::set_humongous_stats(uint num_humongous_total, uint num_humongous_candidates) {\n+  _num_humongous_objects = num_humongous_total;\n+  _num_humongous_reclaim_candidates = num_humongous_candidates;\n+}\n+\n@@ -3350,494 +2927,2 @@\n-class G1PrepareEvacuationTask : public AbstractGangTask {\n-  class G1PrepareRegionsClosure : public HeapRegionClosure {\n-    G1CollectedHeap* _g1h;\n-    G1PrepareEvacuationTask* _parent_task;\n-    uint _worker_humongous_total;\n-    uint _worker_humongous_candidates;\n-\n-    G1CardSetMemoryStats _card_set_stats;\n-\n-    void sample_card_set_size(HeapRegion* hr) {\n-      \/\/ Sample card set sizes for young gen and humongous before GC: this makes\n-      \/\/ the policy to give back memory to the OS keep the most recent amount of\n-      \/\/ memory for these regions.\n-      if (hr->is_young() || hr->is_starts_humongous()) {\n-        _card_set_stats.add(hr->rem_set()->card_set_memory_stats());\n-      }\n-    }\n-\n-    bool humongous_region_is_candidate(HeapRegion* region) const {\n-      assert(region->is_starts_humongous(), \"Must start a humongous object\");\n-\n-      oop obj = cast_to_oop(region->bottom());\n-\n-      \/\/ Dead objects cannot be eager reclaim candidates. Due to class\n-      \/\/ unloading it is unsafe to query their classes so we return early.\n-      if (_g1h->is_obj_dead(obj, region)) {\n-        return false;\n-      }\n-\n-      \/\/ If we do not have a complete remembered set for the region, then we can\n-      \/\/ not be sure that we have all references to it.\n-      if (!region->rem_set()->is_complete()) {\n-        return false;\n-      }\n-      \/\/ Candidate selection must satisfy the following constraints\n-      \/\/ while concurrent marking is in progress:\n-      \/\/\n-      \/\/ * In order to maintain SATB invariants, an object must not be\n-      \/\/ reclaimed if it was allocated before the start of marking and\n-      \/\/ has not had its references scanned.  Such an object must have\n-      \/\/ its references (including type metadata) scanned to ensure no\n-      \/\/ live objects are missed by the marking process.  Objects\n-      \/\/ allocated after the start of concurrent marking don't need to\n-      \/\/ be scanned.\n-      \/\/\n-      \/\/ * An object must not be reclaimed if it is on the concurrent\n-      \/\/ mark stack.  Objects allocated after the start of concurrent\n-      \/\/ marking are never pushed on the mark stack.\n-      \/\/\n-      \/\/ Nominating only objects allocated after the start of concurrent\n-      \/\/ marking is sufficient to meet both constraints.  This may miss\n-      \/\/ some objects that satisfy the constraints, but the marking data\n-      \/\/ structures don't support efficiently performing the needed\n-      \/\/ additional tests or scrubbing of the mark stack.\n-      \/\/\n-      \/\/ However, we presently only nominate is_typeArray() objects.\n-      \/\/ A humongous object containing references induces remembered\n-      \/\/ set entries on other regions.  In order to reclaim such an\n-      \/\/ object, those remembered sets would need to be cleaned up.\n-      \/\/\n-      \/\/ We also treat is_typeArray() objects specially, allowing them\n-      \/\/ to be reclaimed even if allocated before the start of\n-      \/\/ concurrent mark.  For this we rely on mark stack insertion to\n-      \/\/ exclude is_typeArray() objects, preventing reclaiming an object\n-      \/\/ that is in the mark stack.  We also rely on the metadata for\n-      \/\/ such objects to be built-in and so ensured to be kept live.\n-      \/\/ Frequent allocation and drop of large binary blobs is an\n-      \/\/ important use case for eager reclaim, and this special handling\n-      \/\/ may reduce needed headroom.\n-\n-      return obj->is_typeArray() &&\n-             _g1h->is_potential_eager_reclaim_candidate(region);\n-    }\n-\n-  public:\n-    G1PrepareRegionsClosure(G1CollectedHeap* g1h, G1PrepareEvacuationTask* parent_task) :\n-      _g1h(g1h),\n-      _parent_task(parent_task),\n-      _worker_humongous_total(0),\n-      _worker_humongous_candidates(0) { }\n-\n-    ~G1PrepareRegionsClosure() {\n-      _parent_task->add_humongous_candidates(_worker_humongous_candidates);\n-      _parent_task->add_humongous_total(_worker_humongous_total);\n-    }\n-\n-    virtual bool do_heap_region(HeapRegion* hr) {\n-      \/\/ First prepare the region for scanning\n-      _g1h->rem_set()->prepare_region_for_scan(hr);\n-\n-      sample_card_set_size(hr);\n-\n-      \/\/ Now check if region is a humongous candidate\n-      if (!hr->is_starts_humongous()) {\n-        _g1h->register_region_with_region_attr(hr);\n-        return false;\n-      }\n-\n-      uint index = hr->hrm_index();\n-      if (humongous_region_is_candidate(hr)) {\n-        _g1h->set_humongous_reclaim_candidate(index, true);\n-        _g1h->register_humongous_region_with_region_attr(index);\n-        _worker_humongous_candidates++;\n-        \/\/ We will later handle the remembered sets of these regions.\n-      } else {\n-        _g1h->set_humongous_reclaim_candidate(index, false);\n-        _g1h->register_region_with_region_attr(hr);\n-      }\n-      log_debug(gc, humongous)(\"Humongous region %u (object size \" SIZE_FORMAT \" @ \" PTR_FORMAT \") remset \" SIZE_FORMAT \" code roots \" SIZE_FORMAT \" marked (prev\/next) %d\/%d reclaim candidate %d type array %d\",\n-                               index,\n-                               (size_t)cast_to_oop(hr->bottom())->size() * HeapWordSize,\n-                               p2i(hr->bottom()),\n-                               hr->rem_set()->occupied(),\n-                               hr->rem_set()->strong_code_roots_list_length(),\n-                               _g1h->concurrent_mark()->prev_mark_bitmap()->is_marked(hr->bottom()),\n-                               _g1h->concurrent_mark()->next_mark_bitmap()->is_marked(hr->bottom()),\n-                               _g1h->is_humongous_reclaim_candidate(index),\n-                               cast_to_oop(hr->bottom())->is_typeArray()\n-                              );\n-      _worker_humongous_total++;\n-\n-      return false;\n-    }\n-\n-    G1CardSetMemoryStats card_set_stats() const {\n-      return _card_set_stats;\n-    }\n-  };\n-\n-  G1CollectedHeap* _g1h;\n-  HeapRegionClaimer _claimer;\n-  volatile uint _humongous_total;\n-  volatile uint _humongous_candidates;\n-\n-  G1CardSetMemoryStats _all_card_set_stats;\n-\n-public:\n-  G1PrepareEvacuationTask(G1CollectedHeap* g1h) :\n-    AbstractGangTask(\"Prepare Evacuation\"),\n-    _g1h(g1h),\n-    _claimer(_g1h->workers()->active_workers()),\n-    _humongous_total(0),\n-    _humongous_candidates(0) { }\n-\n-  void work(uint worker_id) {\n-    G1PrepareRegionsClosure cl(_g1h, this);\n-    _g1h->heap_region_par_iterate_from_worker_offset(&cl, &_claimer, worker_id);\n-\n-    MutexLocker x(ParGCRareEvent_lock, Mutex::_no_safepoint_check_flag);\n-    _all_card_set_stats.add(cl.card_set_stats());\n-  }\n-\n-  void add_humongous_candidates(uint candidates) {\n-    Atomic::add(&_humongous_candidates, candidates);\n-  }\n-\n-  void add_humongous_total(uint total) {\n-    Atomic::add(&_humongous_total, total);\n-  }\n-\n-  uint humongous_candidates() {\n-    return _humongous_candidates;\n-  }\n-\n-  uint humongous_total() {\n-    return _humongous_total;\n-  }\n-\n-  G1CardSetMemoryStats all_card_set_stats() const {\n-    return _all_card_set_stats;\n-  }\n-};\n-\n-void G1CollectedHeap::pre_evacuate_collection_set(G1EvacuationInfo* evacuation_info, G1ParScanThreadStateSet* per_thread_states) {\n-  \/\/ Please see comment in g1CollectedHeap.hpp and\n-  \/\/ G1CollectedHeap::ref_processing_init() to see how\n-  \/\/ reference processing currently works in G1.\n-  _ref_processor_stw->start_discovery(false \/* always_clear *\/);\n-\n-  _bytes_used_during_gc = 0;\n-\n-  Atomic::store(&_num_regions_failed_evacuation, 0u);\n-\n-  gc_prologue(false);\n-\n-  {\n-    Ticks start = Ticks::now();\n-    retire_tlabs();\n-    phase_times()->record_prepare_tlab_time_ms((Ticks::now() - start).seconds() * 1000.0);\n-  }\n-\n-  {\n-    \/\/ Flush dirty card queues to qset, so later phases don't need to account\n-    \/\/ for partially filled per-thread queues and such.\n-    Ticks start = Ticks::now();\n-    G1BarrierSet::dirty_card_queue_set().concatenate_logs();\n-    Tickspan dt = Ticks::now() - start;\n-    phase_times()->record_concatenate_dirty_card_logs_time_ms(dt.seconds() * MILLIUNITS);\n-  }\n-\n-  _regions_failed_evacuation.clear();\n-\n-  \/\/ Disable the hot card cache.\n-  _hot_card_cache->reset_hot_cache_claimed_index();\n-  _hot_card_cache->set_use_cache(false);\n-\n-  \/\/ Initialize the GC alloc regions.\n-  _allocator->init_gc_alloc_regions(evacuation_info);\n-\n-  {\n-    Ticks start = Ticks::now();\n-    rem_set()->prepare_for_scan_heap_roots();\n-    phase_times()->record_prepare_heap_roots_time_ms((Ticks::now() - start).seconds() * 1000.0);\n-  }\n-\n-  {\n-    G1PrepareEvacuationTask g1_prep_task(this);\n-    Tickspan task_time = run_task_timed(&g1_prep_task);\n-\n-    _young_gen_card_set_stats = g1_prep_task.all_card_set_stats();\n-\n-    phase_times()->record_register_regions(task_time.seconds() * 1000.0);\n-    _num_humongous_objects = g1_prep_task.humongous_total();\n-    _num_humongous_reclaim_candidates = g1_prep_task.humongous_candidates();\n-  }\n-\n-  assert(_verifier->check_region_attr_table(), \"Inconsistency in the region attributes table.\");\n-\n-#if COMPILER2_OR_JVMCI\n-  DerivedPointerTable::clear();\n-#endif\n-\n-  if (collector_state()->in_concurrent_start_gc()) {\n-    concurrent_mark()->pre_concurrent_start(gc_cause());\n-  }\n-\n-  \/\/ Should G1EvacuationFailureALot be in effect for this GC?\n-  evac_failure_injector()->arm_if_needed();\n-}\n-\n-class G1EvacuateRegionsBaseTask : public AbstractGangTask {\n-protected:\n-  G1CollectedHeap* _g1h;\n-  G1ParScanThreadStateSet* _per_thread_states;\n-  G1ScannerTasksQueueSet* _task_queues;\n-  TaskTerminator _terminator;\n-  uint _num_workers;\n-\n-  void evacuate_live_objects(G1ParScanThreadState* pss,\n-                             uint worker_id,\n-                             G1GCPhaseTimes::GCParPhases objcopy_phase,\n-                             G1GCPhaseTimes::GCParPhases termination_phase) {\n-    G1GCPhaseTimes* p = _g1h->phase_times();\n-\n-    Ticks start = Ticks::now();\n-    G1ParEvacuateFollowersClosure cl(_g1h, pss, _task_queues, &_terminator, objcopy_phase);\n-    cl.do_void();\n-\n-    assert(pss->queue_is_empty(), \"should be empty\");\n-\n-    Tickspan evac_time = (Ticks::now() - start);\n-    p->record_or_add_time_secs(objcopy_phase, worker_id, evac_time.seconds() - cl.term_time());\n-\n-    if (termination_phase == G1GCPhaseTimes::Termination) {\n-      p->record_time_secs(termination_phase, worker_id, cl.term_time());\n-      p->record_thread_work_item(termination_phase, worker_id, cl.term_attempts());\n-    } else {\n-      p->record_or_add_time_secs(termination_phase, worker_id, cl.term_time());\n-      p->record_or_add_thread_work_item(termination_phase, worker_id, cl.term_attempts());\n-    }\n-    assert(pss->trim_ticks().value() == 0,\n-           \"Unexpected partial trimming during evacuation value \" JLONG_FORMAT,\n-           pss->trim_ticks().value());\n-  }\n-\n-  virtual void start_work(uint worker_id) { }\n-\n-  virtual void end_work(uint worker_id) { }\n-\n-  virtual void scan_roots(G1ParScanThreadState* pss, uint worker_id) = 0;\n-\n-  virtual void evacuate_live_objects(G1ParScanThreadState* pss, uint worker_id) = 0;\n-\n-public:\n-  G1EvacuateRegionsBaseTask(const char* name,\n-                            G1ParScanThreadStateSet* per_thread_states,\n-                            G1ScannerTasksQueueSet* task_queues,\n-                            uint num_workers) :\n-    AbstractGangTask(name),\n-    _g1h(G1CollectedHeap::heap()),\n-    _per_thread_states(per_thread_states),\n-    _task_queues(task_queues),\n-    _terminator(num_workers, _task_queues),\n-    _num_workers(num_workers)\n-  { }\n-\n-  void work(uint worker_id) {\n-    start_work(worker_id);\n-\n-    {\n-      ResourceMark rm;\n-\n-      G1ParScanThreadState* pss = _per_thread_states->state_for_worker(worker_id);\n-      pss->set_ref_discoverer(_g1h->ref_processor_stw());\n-\n-      scan_roots(pss, worker_id);\n-      evacuate_live_objects(pss, worker_id);\n-    }\n-\n-    end_work(worker_id);\n-  }\n-};\n-\n-class G1EvacuateRegionsTask : public G1EvacuateRegionsBaseTask {\n-  G1RootProcessor* _root_processor;\n-  bool _has_optional_evacuation_work;\n-\n-  void scan_roots(G1ParScanThreadState* pss, uint worker_id) {\n-    _root_processor->evacuate_roots(pss, worker_id);\n-    _g1h->rem_set()->scan_heap_roots(pss, worker_id, G1GCPhaseTimes::ScanHR, G1GCPhaseTimes::ObjCopy, _has_optional_evacuation_work);\n-    _g1h->rem_set()->scan_collection_set_regions(pss, worker_id, G1GCPhaseTimes::ScanHR, G1GCPhaseTimes::CodeRoots, G1GCPhaseTimes::ObjCopy);\n-  }\n-\n-  void evacuate_live_objects(G1ParScanThreadState* pss, uint worker_id) {\n-    G1EvacuateRegionsBaseTask::evacuate_live_objects(pss, worker_id, G1GCPhaseTimes::ObjCopy, G1GCPhaseTimes::Termination);\n-  }\n-\n-  void start_work(uint worker_id) {\n-    _g1h->phase_times()->record_time_secs(G1GCPhaseTimes::GCWorkerStart, worker_id, Ticks::now().seconds());\n-  }\n-\n-  void end_work(uint worker_id) {\n-    _g1h->phase_times()->record_time_secs(G1GCPhaseTimes::GCWorkerEnd, worker_id, Ticks::now().seconds());\n-  }\n-\n-public:\n-  G1EvacuateRegionsTask(G1CollectedHeap* g1h,\n-                        G1ParScanThreadStateSet* per_thread_states,\n-                        G1ScannerTasksQueueSet* task_queues,\n-                        G1RootProcessor* root_processor,\n-                        uint num_workers,\n-                        bool has_optional_evacuation_work) :\n-    G1EvacuateRegionsBaseTask(\"G1 Evacuate Regions\", per_thread_states, task_queues, num_workers),\n-    _root_processor(root_processor),\n-    _has_optional_evacuation_work(has_optional_evacuation_work)\n-  { }\n-};\n-\n-void G1CollectedHeap::evacuate_initial_collection_set(G1ParScanThreadStateSet* per_thread_states,\n-                                                      bool has_optional_evacuation_work) {\n-  G1GCPhaseTimes* p = phase_times();\n-\n-  {\n-    Ticks start = Ticks::now();\n-    rem_set()->merge_heap_roots(true \/* initial_evacuation *\/);\n-    p->record_merge_heap_roots_time((Ticks::now() - start).seconds() * 1000.0);\n-  }\n-\n-  Tickspan task_time;\n-  const uint num_workers = workers()->active_workers();\n-\n-  Ticks start_processing = Ticks::now();\n-  {\n-    G1RootProcessor root_processor(this, num_workers);\n-    G1EvacuateRegionsTask g1_par_task(this,\n-                                      per_thread_states,\n-                                      _task_queues,\n-                                      &root_processor,\n-                                      num_workers,\n-                                      has_optional_evacuation_work);\n-    task_time = run_task_timed(&g1_par_task);\n-    \/\/ Closing the inner scope will execute the destructor for the\n-    \/\/ G1RootProcessor object. By subtracting the WorkGang task from the total\n-    \/\/ time of this scope, we get the \"NMethod List Cleanup\" time. This list is\n-    \/\/ constructed during \"STW two-phase nmethod root processing\", see more in\n-    \/\/ nmethod.hpp\n-  }\n-  Tickspan total_processing = Ticks::now() - start_processing;\n-\n-  p->record_initial_evac_time(task_time.seconds() * 1000.0);\n-  p->record_or_add_nmethod_list_cleanup_time((total_processing - task_time).seconds() * 1000.0);\n-\n-  rem_set()->complete_evac_phase(has_optional_evacuation_work);\n-}\n-\n-class G1EvacuateOptionalRegionsTask : public G1EvacuateRegionsBaseTask {\n-\n-  void scan_roots(G1ParScanThreadState* pss, uint worker_id) {\n-    _g1h->rem_set()->scan_heap_roots(pss, worker_id, G1GCPhaseTimes::OptScanHR, G1GCPhaseTimes::OptObjCopy, true \/* remember_already_scanned_cards *\/);\n-    _g1h->rem_set()->scan_collection_set_regions(pss, worker_id, G1GCPhaseTimes::OptScanHR, G1GCPhaseTimes::OptCodeRoots, G1GCPhaseTimes::OptObjCopy);\n-  }\n-\n-  void evacuate_live_objects(G1ParScanThreadState* pss, uint worker_id) {\n-    G1EvacuateRegionsBaseTask::evacuate_live_objects(pss, worker_id, G1GCPhaseTimes::OptObjCopy, G1GCPhaseTimes::OptTermination);\n-  }\n-\n-public:\n-  G1EvacuateOptionalRegionsTask(G1ParScanThreadStateSet* per_thread_states,\n-                                G1ScannerTasksQueueSet* queues,\n-                                uint num_workers) :\n-    G1EvacuateRegionsBaseTask(\"G1 Evacuate Optional Regions\", per_thread_states, queues, num_workers) {\n-  }\n-};\n-\n-void G1CollectedHeap::evacuate_next_optional_regions(G1ParScanThreadStateSet* per_thread_states) {\n-  \/\/ To access the protected constructor\/destructor\n-  class G1MarkScope : public MarkScope { };\n-\n-  Tickspan task_time;\n-\n-  Ticks start_processing = Ticks::now();\n-  {\n-    G1MarkScope code_mark_scope;\n-    G1EvacuateOptionalRegionsTask task(per_thread_states, _task_queues, workers()->active_workers());\n-    task_time = run_task_timed(&task);\n-    \/\/ See comment in evacuate_initial_collection_set() for the reason of the scope.\n-  }\n-  Tickspan total_processing = Ticks::now() - start_processing;\n-\n-  G1GCPhaseTimes* p = phase_times();\n-  p->record_or_add_nmethod_list_cleanup_time((total_processing - task_time).seconds() * 1000.0);\n-}\n-\n-void G1CollectedHeap::evacuate_optional_collection_set(G1ParScanThreadStateSet* per_thread_states) {\n-  const double collection_start_time_ms = phase_times()->cur_collection_start_sec() * 1000.0;\n-\n-  while (!evacuation_failed() && _collection_set.optional_region_length() > 0) {\n-\n-    double time_used_ms = os::elapsedTime() * 1000.0 - collection_start_time_ms;\n-    double time_left_ms = MaxGCPauseMillis - time_used_ms;\n-\n-    if (time_left_ms < 0 ||\n-        !_collection_set.finalize_optional_for_evacuation(time_left_ms * policy()->optional_evacuation_fraction())) {\n-      log_trace(gc, ergo, cset)(\"Skipping evacuation of %u optional regions, no more regions can be evacuated in %.3fms\",\n-                                _collection_set.optional_region_length(), time_left_ms);\n-      break;\n-    }\n-\n-    {\n-      Ticks start = Ticks::now();\n-      rem_set()->merge_heap_roots(false \/* initial_evacuation *\/);\n-      phase_times()->record_or_add_optional_merge_heap_roots_time((Ticks::now() - start).seconds() * 1000.0);\n-    }\n-\n-    {\n-      Ticks start = Ticks::now();\n-      evacuate_next_optional_regions(per_thread_states);\n-      phase_times()->record_or_add_optional_evac_time((Ticks::now() - start).seconds() * 1000.0);\n-    }\n-\n-    rem_set()->complete_evac_phase(true \/* has_more_than_one_evacuation_phase *\/);\n-  }\n-\n-  _collection_set.abandon_optional_collection_set(per_thread_states);\n-}\n-\n-void G1CollectedHeap::post_evacuate_collection_set(G1EvacuationInfo* evacuation_info,\n-                                                   G1ParScanThreadStateSet* per_thread_states) {\n-  G1GCPhaseTimes* p = phase_times();\n-\n-  \/\/ Process any discovered reference objects - we have\n-  \/\/ to do this _before_ we retire the GC alloc regions\n-  \/\/ as we may have to copy some 'reachable' referent\n-  \/\/ objects (and their reachable sub-graphs) that were\n-  \/\/ not copied during the pause.\n-  process_discovered_references(per_thread_states);\n-\n-  G1STWIsAliveClosure is_alive(this);\n-  G1KeepAliveClosure keep_alive(this);\n-\n-  WeakProcessor::weak_oops_do(workers(), &is_alive, &keep_alive, p->weak_phase_times());\n-\n-  _allocator->release_gc_alloc_regions(evacuation_info);\n-\n-  post_evacuate_cleanup_1(per_thread_states);\n-\n-  post_evacuate_cleanup_2(per_thread_states, evacuation_info);\n-\n-  assert_used_and_recalculate_used_equal(this);\n-\n-  rebuild_free_region_list();\n-\n-  record_obj_copy_mem_stats();\n-\n-  evacuation_info->set_collectionset_used_before(collection_set()->bytes_used_before());\n-  evacuation_info->set_bytes_used(_bytes_used_during_gc);\n-\n-  start_new_collection_set();\n-\n-  prepare_tlabs_for_mutator();\n-\n-  gc_epilogue(false);\n-\n-  expand_heap_after_young_collection();\n+void G1CollectedHeap::set_young_gen_card_set_stats(const G1CardSetMemoryStats& stats) {\n+  _young_gen_card_set_stats = stats;\n@@ -3911,19 +2996,0 @@\n-void G1CollectedHeap::post_evacuate_cleanup_1(G1ParScanThreadStateSet* per_thread_states) {\n-  Ticks start = Ticks::now();\n-  {\n-    G1PostEvacuateCollectionSetCleanupTask1 cl(per_thread_states);\n-    run_batch_task(&cl);\n-  }\n-  phase_times()->record_post_evacuate_cleanup_task_1_time((Ticks::now() - start).seconds() * 1000.0);\n-}\n-\n-void G1CollectedHeap::post_evacuate_cleanup_2(G1ParScanThreadStateSet* per_thread_states,\n-                                              G1EvacuationInfo* evacuation_info) {\n-  Ticks start = Ticks::now();\n-  {\n-    G1PostEvacuateCollectionSetCleanupTask2 cl(per_thread_states, evacuation_info);\n-    run_batch_task(&cl);\n-  }\n-  phase_times()->record_post_evacuate_cleanup_task_2_time((Ticks::now() - start).seconds() * 1000.0);\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectedHeap.cpp","additions":16,"deletions":950,"binary":false,"changes":966,"status":"modified"},{"patch":"@@ -30,1 +30,0 @@\n-#include \"gc\/g1\/g1CardSet.hpp\"\n@@ -37,1 +36,0 @@\n-#include \"gc\/g1\/g1EvacFailure.hpp\"\n@@ -39,2 +37,0 @@\n-#include \"gc\/g1\/g1EvacuationInfo.hpp\"\n-#include \"gc\/g1\/g1GCPhaseTimes.hpp\"\n@@ -48,1 +44,0 @@\n-#include \"gc\/g1\/g1RedirtyCardsQueue.hpp\"\n@@ -105,0 +100,3 @@\n+class G1GCPhaseTimes;\n+class ReferenceProcessor;\n+class G1BatchedGangTask;\n@@ -185,4 +183,0 @@\n-  void rebuild_free_region_list();\n-  \/\/ Start a new incremental collection set for the next pause.\n-  void start_new_collection_set();\n-\n@@ -193,0 +187,4 @@\n+  void rebuild_free_region_list();\n+  \/\/ Start a new incremental collection set for the next pause.\n+  void start_new_collection_set();\n+\n@@ -232,0 +230,4 @@\n+public:\n+  size_t bytes_used_during_gc() const { return _bytes_used_during_gc; }\n+\n+private:\n@@ -273,0 +275,2 @@\n+  void set_humongous_stats(uint num_humongous_total, uint num_humongous_candidates);\n+\n@@ -275,0 +279,1 @@\n+  void set_young_gen_card_set_stats(const G1CardSetMemoryStats& stats);\n@@ -542,2 +547,1 @@\n-  \/\/ Process any reference objects discovered.\n-  void process_discovered_references(G1ParScanThreadStateSet* per_thread_states);\n+  void verify_numa_regions(const char* desc);\n@@ -545,0 +549,1 @@\n+public:\n@@ -550,3 +555,0 @@\n-  void verify_numa_regions(const char* desc);\n-\n-public:\n@@ -557,3 +559,0 @@\n-  \/\/ Runs the given AbstractGangTask with the current active workers,\n-  \/\/ returning the total time taken.\n-  Tickspan run_task_timed(AbstractGangTask* task);\n@@ -761,9 +760,0 @@\n-  #if TASKQUEUE_STATS\n-  static void print_taskqueue_stats_hdr(outputStream* const st);\n-  void print_taskqueue_stats() const;\n-  void reset_taskqueue_stats();\n-  #endif \/\/ TASKQUEUE_STATS\n-\n-  \/\/ Start a concurrent cycle.\n-  void start_concurrent_cycle(bool concurrent_operation_is_full_mark);\n-\n@@ -785,2 +775,0 @@\n-  void wait_for_root_region_scanning();\n-\n@@ -799,6 +787,0 @@\n-  void set_young_collection_default_active_worker_threads();\n-\n-  void prepare_tlabs_for_mutator();\n-\n-  void retire_tlabs();\n-\n@@ -809,25 +791,0 @@\n-  void calculate_collection_set(G1EvacuationInfo* evacuation_info, double target_pause_time_ms);\n-\n-  \/\/ Actually do the work of evacuating the parts of the collection set.\n-  \/\/ The has_optional_evacuation_work flag for the initial collection set\n-  \/\/ evacuation indicates whether one or more optional evacuation steps may\n-  \/\/ follow.\n-  \/\/ If not set, G1 can avoid clearing the card tables of regions that we scan\n-  \/\/ for roots from the heap: when scanning the card table for dirty cards after\n-  \/\/ all remembered sets have been dumped onto it, for optional evacuation we\n-  \/\/ mark these cards as \"Scanned\" to know that we do not need to re-scan them\n-  \/\/ in the additional optional evacuation passes. This means that in the \"Clear\n-  \/\/ Card Table\" phase we need to clear those marks. However, if there is no\n-  \/\/ optional evacuation, g1 can immediately clean the dirty cards it encounters\n-  \/\/ as nobody else will be looking at them again, saving the clear card table\n-  \/\/ work later.\n-  \/\/ This case is very common (young only collections and most mixed gcs), so\n-  \/\/ depending on the ratio between scanned and evacuated regions (which g1 always\n-  \/\/ needs to clear), this is a big win.\n-  void evacuate_initial_collection_set(G1ParScanThreadStateSet* per_thread_states,\n-                                       bool has_optional_evacuation_work);\n-  void evacuate_optional_collection_set(G1ParScanThreadStateSet* per_thread_states);\n-private:\n-  \/\/ Evacuate the next set of optional regions.\n-  void evacuate_next_optional_regions(G1ParScanThreadStateSet* per_thread_states);\n-\n@@ -835,3 +792,8 @@\n-  void pre_evacuate_collection_set(G1EvacuationInfo* evacuation_info, G1ParScanThreadStateSet* pss);\n-  void post_evacuate_collection_set(G1EvacuationInfo* evacuation_info,\n-                                    G1ParScanThreadStateSet* pss);\n+  \/\/ Start a concurrent cycle.\n+  void start_concurrent_cycle(bool concurrent_operation_is_full_mark);\n+\n+  void wait_for_root_region_scanning();\n+\n+  void prepare_tlabs_for_mutator();\n+\n+  void retire_tlabs();\n@@ -843,0 +805,1 @@\n+private:\n@@ -851,4 +814,1 @@\n-  void post_evacuate_cleanup_1(G1ParScanThreadStateSet* per_thread_states);\n-  void post_evacuate_cleanup_2(G1ParScanThreadStateSet* per_thread_states,\n-                               G1EvacuationInfo* evacuation_info);\n-\n+public:\n@@ -878,4 +838,0 @@\n-  \/\/ Preserve the mark of \"obj\", if necessary, in preparation for its mark\n-  \/\/ word being overwritten with a self-forwarding-pointer.\n-  void preserve_mark_during_evac_failure(uint worker_id, oop obj, markWord m);\n-\n@@ -943,0 +899,1 @@\n+  G1ScannerTasksQueueSet* task_queues() const;\n@@ -945,2 +902,0 @@\n-  uint num_task_queues() const;\n-\n@@ -1017,0 +972,1 @@\n+  STWGCTimer* gc_timer_stw() const { return _gc_timer_stw; }\n@@ -1099,0 +1055,1 @@\n+  inline void reset_evacuation_failed_data();\n@@ -1285,1 +1242,1 @@\n-  const G1SurvivorRegions* survivor() const { return &_survivor; }\n+  G1SurvivorRegions* survivor() { return &_survivor; }\n@@ -1459,38 +1416,0 @@\n-class G1ParEvacuateFollowersClosure : public VoidClosure {\n-private:\n-  double _start_term;\n-  double _term_time;\n-  size_t _term_attempts;\n-\n-  void start_term_time() { _term_attempts++; _start_term = os::elapsedTime(); }\n-  void end_term_time() { _term_time += (os::elapsedTime() - _start_term); }\n-protected:\n-  G1CollectedHeap*              _g1h;\n-  G1ParScanThreadState*         _par_scan_state;\n-  G1ScannerTasksQueueSet*       _queues;\n-  TaskTerminator*               _terminator;\n-  G1GCPhaseTimes::GCParPhases   _phase;\n-\n-  G1ParScanThreadState*   par_scan_state() { return _par_scan_state; }\n-  G1ScannerTasksQueueSet* queues()         { return _queues; }\n-  TaskTerminator*         terminator()     { return _terminator; }\n-\n-public:\n-  G1ParEvacuateFollowersClosure(G1CollectedHeap* g1h,\n-                                G1ParScanThreadState* par_scan_state,\n-                                G1ScannerTasksQueueSet* queues,\n-                                TaskTerminator* terminator,\n-                                G1GCPhaseTimes::GCParPhases phase)\n-    : _start_term(0.0), _term_time(0.0), _term_attempts(0),\n-      _g1h(g1h), _par_scan_state(par_scan_state),\n-      _queues(queues), _terminator(terminator), _phase(phase) {}\n-\n-  void do_void();\n-\n-  double term_time() const { return _term_time; }\n-  size_t term_attempts() const { return _term_attempts; }\n-\n-private:\n-  inline bool offer_termination();\n-};\n-\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectedHeap.hpp","additions":30,"deletions":111,"binary":false,"changes":141,"status":"modified"},{"patch":"@@ -145,0 +145,4 @@\n+inline G1ScannerTasksQueueSet* G1CollectedHeap::task_queues() const {\n+  return _task_queues;\n+}\n+\n@@ -194,0 +198,5 @@\n+void G1CollectedHeap::reset_evacuation_failed_data() {\n+  Atomic::store(&_num_regions_failed_evacuation, 0u);\n+  _regions_failed_evacuation.clear();\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectedHeap.inline.hpp","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -0,0 +1,1107 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+\n+#include \"classfile\/classLoaderDataGraph.inline.hpp\"\n+#include \"compiler\/oopMap.hpp\"\n+#include \"gc\/g1\/g1Allocator.hpp\"\n+#include \"gc\/g1\/g1CardSetMemory.hpp\"\n+#include \"gc\/g1\/g1CollectedHeap.hpp\"\n+#include \"gc\/g1\/g1CollectorState.hpp\"\n+#include \"gc\/g1\/g1ConcurrentMark.hpp\"\n+#include \"gc\/g1\/g1GCPhaseTimes.hpp\"\n+#include \"gc\/g1\/g1YoungGCEvacFailureInjector.hpp\"\n+#include \"gc\/g1\/g1EvacuationInfo.hpp\"\n+#include \"gc\/g1\/g1HRPrinter.hpp\"\n+#include \"gc\/g1\/g1HotCardCache.hpp\"\n+#include \"gc\/g1\/g1MonitoringSupport.hpp\"\n+#include \"gc\/g1\/g1ParScanThreadState.inline.hpp\"\n+#include \"gc\/g1\/g1Policy.hpp\"\n+#include \"gc\/g1\/g1RedirtyCardsQueue.hpp\"\n+#include \"gc\/g1\/g1RemSet.hpp\"\n+#include \"gc\/g1\/g1RootProcessor.hpp\"\n+#include \"gc\/g1\/g1Trace.hpp\"\n+#include \"gc\/g1\/g1YoungCollector.hpp\"\n+#include \"gc\/g1\/g1YoungGCPostEvacuateTasks.hpp\"\n+#include \"gc\/g1\/g1_globals.hpp\"\n+#include \"gc\/shared\/concurrentGCBreakpoints.hpp\"\n+#include \"gc\/shared\/gcTraceTime.inline.hpp\"\n+#include \"gc\/shared\/gcTimer.hpp\"\n+#include \"gc\/shared\/preservedMarks.hpp\"\n+#include \"gc\/shared\/referenceProcessor.hpp\"\n+#include \"gc\/shared\/weakProcessor.inline.hpp\"\n+#include \"gc\/shared\/workerPolicy.hpp\"\n+#include \"gc\/shared\/workgroup.hpp\"\n+#include \"jfr\/jfrEvents.hpp\"\n+#include \"memory\/resourceArea.hpp\"\n+#include \"utilities\/ticks.hpp\"\n+\n+#if TASKQUEUE_STATS\n+uint G1YoungCollector::num_task_queues() const {\n+  return task_queues()->size();\n+}\n+\n+void G1YoungCollector::print_taskqueue_stats_hdr(outputStream* const st) {\n+  st->print_raw_cr(\"GC Task Stats\");\n+  st->print_raw(\"thr \"); TaskQueueStats::print_header(1, st); st->cr();\n+  st->print_raw(\"--- \"); TaskQueueStats::print_header(2, st); st->cr();\n+}\n+\n+void G1YoungCollector::print_taskqueue_stats() const {\n+  if (!log_is_enabled(Trace, gc, task, stats)) {\n+    return;\n+  }\n+  Log(gc, task, stats) log;\n+  ResourceMark rm;\n+  LogStream ls(log.trace());\n+  outputStream* st = &ls;\n+\n+  print_taskqueue_stats_hdr(st);\n+\n+  TaskQueueStats totals;\n+  const uint n = num_task_queues();\n+  for (uint i = 0; i < n; ++i) {\n+    st->print(\"%3u \", i); _g1h->task_queue(i)->stats.print(st); st->cr();\n+    totals += _g1h->task_queue(i)->stats;\n+  }\n+  st->print_raw(\"tot \"); totals.print(st); st->cr();\n+\n+  DEBUG_ONLY(totals.verify());\n+}\n+\n+void G1YoungCollector::reset_taskqueue_stats() {\n+  const uint n = num_task_queues();\n+  for (uint i = 0; i < n; ++i) {\n+    _g1h->task_queue(i)->stats.reset();\n+  }\n+}\n+#endif \/\/ TASKQUEUE_STATS\n+\n+\/\/ GCTraceTime wrapper that constructs the message according to GC pause type and\n+\/\/ GC cause.\n+\/\/ The code relies on the fact that GCTraceTimeWrapper stores the string passed\n+\/\/ initially as a reference only, so that we can modify it as needed.\n+class G1YoungGCTraceTime {\n+  G1GCPauseType _pause_type;\n+  GCCause::Cause _pause_cause;\n+\n+  static const uint MaxYoungGCNameLength = 128;\n+  char _young_gc_name_data[MaxYoungGCNameLength];\n+\n+  GCTraceTime(Info, gc) _tt;\n+\n+  const char* update_young_gc_name() {\n+    snprintf(_young_gc_name_data,\n+             MaxYoungGCNameLength,\n+             \"Pause Young (%s) (%s)%s\",\n+             G1GCPauseTypeHelper::to_string(_pause_type),\n+             GCCause::to_string(_pause_cause),\n+             G1CollectedHeap::heap()->evacuation_failed() ? \" (Evacuation Failure)\" : \"\");\n+    return _young_gc_name_data;\n+  }\n+\n+public:\n+  G1YoungGCTraceTime(GCCause::Cause cause) :\n+    \/\/ Take snapshot of current pause type at start as it may be modified during gc.\n+    \/\/ The strings for all Concurrent Start pauses are the same, so the parameter\n+    \/\/ does not matter here.\n+    _pause_type(G1CollectedHeap::heap()->collector_state()->young_gc_pause_type(false \/* concurrent_operation_is_full_mark *\/)),\n+    _pause_cause(cause),\n+    \/\/ Fake a \"no cause\" and manually add the correct string in update_young_gc_name()\n+    \/\/ to make the string look more natural.\n+    _tt(update_young_gc_name(), NULL, GCCause::_no_gc, true) {\n+  }\n+\n+  ~G1YoungGCTraceTime() {\n+    update_young_gc_name();\n+  }\n+};\n+\n+class G1YoungGCNotifyPauseMark : public StackObj {\n+public:\n+  G1YoungGCNotifyPauseMark() { G1CollectedHeap::heap()->policy()->record_young_gc_pause_start(); }\n+  ~G1YoungGCNotifyPauseMark() { G1CollectedHeap::heap()->policy()->record_young_gc_pause_end(); }\n+};\n+\n+class G1YoungGCJFRTracerMark : public G1JFRTracerMark {\n+  G1EvacuationInfo _evacuation_info;\n+\n+  G1NewTracer* tracer() const { return (G1NewTracer*)_tracer; }\n+\n+public:\n+\n+  G1EvacuationInfo* evacuation_info() { return &_evacuation_info; }\n+\n+  G1YoungGCJFRTracerMark(STWGCTimer* gc_timer_stw, G1NewTracer* gc_tracer_stw, GCCause::Cause cause) :\n+    G1JFRTracerMark(gc_timer_stw, gc_tracer_stw), _evacuation_info() { }\n+\n+  void report_pause_type(G1GCPauseType type) {\n+    tracer()->report_young_gc_pause(type);\n+  }\n+\n+  ~G1YoungGCJFRTracerMark() {\n+    G1CollectedHeap* g1h = G1CollectedHeap::heap();\n+\n+    tracer()->report_evacuation_info(&_evacuation_info);\n+    tracer()->report_tenuring_threshold(g1h->policy()->tenuring_threshold());\n+  }\n+};\n+\n+class G1YoungGCVerifierMark : public StackObj {\n+  G1HeapVerifier::G1VerifyType _type;\n+\n+  static G1HeapVerifier::G1VerifyType young_collection_verify_type() {\n+    G1CollectorState* state = G1CollectedHeap::heap()->collector_state();\n+    if (state->in_concurrent_start_gc()) {\n+      return G1HeapVerifier::G1VerifyConcurrentStart;\n+    } else if (state->in_young_only_phase()) {\n+      return G1HeapVerifier::G1VerifyYoungNormal;\n+    } else {\n+      return G1HeapVerifier::G1VerifyMixed;\n+    }\n+  }\n+\n+public:\n+  G1YoungGCVerifierMark() : _type(young_collection_verify_type()) {\n+    G1CollectedHeap::heap()->verify_before_young_collection(_type);\n+  }\n+\n+  ~G1YoungGCVerifierMark() {\n+    G1CollectedHeap::heap()->verify_after_young_collection(_type);\n+  }\n+};\n+\n+G1Allocator* G1YoungCollector::allocator() const {\n+  return _g1h->allocator();\n+}\n+\n+G1CollectionSet* G1YoungCollector::collection_set() const {\n+  return _g1h->collection_set();\n+}\n+\n+G1CollectorState* G1YoungCollector::collector_state() const {\n+  return _g1h->collector_state();\n+}\n+\n+G1ConcurrentMark* G1YoungCollector::concurrent_mark() const {\n+  return _g1h->concurrent_mark();\n+}\n+\n+STWGCTimer* G1YoungCollector::gc_timer_stw() const {\n+  return _g1h->gc_timer_stw();\n+}\n+\n+G1NewTracer* G1YoungCollector::gc_tracer_stw() const {\n+  return _g1h->gc_tracer_stw();\n+}\n+\n+G1Policy* G1YoungCollector::policy() const {\n+  return _g1h->policy();\n+}\n+\n+G1GCPhaseTimes* G1YoungCollector::phase_times() const {\n+  return _g1h->phase_times();\n+}\n+\n+G1HotCardCache* G1YoungCollector::hot_card_cache() const {\n+  return _g1h->hot_card_cache();\n+}\n+\n+G1HRPrinter* G1YoungCollector::hr_printer() const {\n+  return _g1h->hr_printer();\n+}\n+\n+G1MonitoringSupport* G1YoungCollector::monitoring_support() const {\n+  return _g1h->monitoring_support();\n+}\n+\n+G1RemSet* G1YoungCollector::rem_set() const {\n+  return _g1h->rem_set();\n+}\n+\n+G1ScannerTasksQueueSet* G1YoungCollector::task_queues() const {\n+  return _g1h->task_queues();\n+}\n+\n+G1SurvivorRegions* G1YoungCollector::survivor_regions() const {\n+  return _g1h->survivor();\n+}\n+\n+ReferenceProcessor* G1YoungCollector::ref_processor_stw() const {\n+  return _g1h->ref_processor_stw();\n+}\n+\n+WorkGang* G1YoungCollector::workers() const {\n+  return _g1h->workers();\n+}\n+\n+G1YoungGCEvacFailureInjector* G1YoungCollector::evac_failure_injector() const {\n+  return _g1h->evac_failure_injector();\n+}\n+\n+\n+void G1YoungCollector::wait_for_root_region_scanning() {\n+  Ticks start = Ticks::now();\n+  \/\/ We have to wait until the CM threads finish scanning the\n+  \/\/ root regions as it's the only way to ensure that all the\n+  \/\/ objects on them have been correctly scanned before we start\n+  \/\/ moving them during the GC.\n+  bool waited = concurrent_mark()->root_regions()->wait_until_scan_finished();\n+  Tickspan wait_time;\n+  if (waited) {\n+    wait_time = (Ticks::now() - start);\n+  }\n+  phase_times()->record_root_region_scan_wait_time(wait_time.seconds() * MILLIUNITS);\n+}\n+\n+class G1PrintCollectionSetClosure : public HeapRegionClosure {\n+private:\n+  G1HRPrinter* _hr_printer;\n+public:\n+  G1PrintCollectionSetClosure(G1HRPrinter* hr_printer) : HeapRegionClosure(), _hr_printer(hr_printer) { }\n+\n+  virtual bool do_heap_region(HeapRegion* r) {\n+    _hr_printer->cset(r);\n+    return false;\n+  }\n+};\n+\n+void G1YoungCollector::calculate_collection_set(G1EvacuationInfo* evacuation_info, double target_pause_time_ms) {\n+  \/\/ Forget the current allocation region (we might even choose it to be part\n+  \/\/ of the collection set!) before finalizing the collection set.\n+  allocator()->release_mutator_alloc_regions();\n+\n+  collection_set()->finalize_initial_collection_set(target_pause_time_ms, survivor_regions());\n+  evacuation_info->set_collectionset_regions(collection_set()->region_length() +\n+                                            collection_set()->optional_region_length());\n+\n+  concurrent_mark()->verify_no_collection_set_oops();\n+\n+  if (hr_printer()->is_active()) {\n+    G1PrintCollectionSetClosure cl(hr_printer());\n+    collection_set()->iterate(&cl);\n+    collection_set()->iterate_optional(&cl);\n+  }\n+}\n+\n+class G1PrepareEvacuationTask : public AbstractGangTask {\n+  class G1PrepareRegionsClosure : public HeapRegionClosure {\n+    G1CollectedHeap* _g1h;\n+    G1PrepareEvacuationTask* _parent_task;\n+    uint _worker_humongous_total;\n+    uint _worker_humongous_candidates;\n+\n+    G1CardSetMemoryStats _card_set_stats;\n+\n+    void sample_card_set_size(HeapRegion* hr) {\n+      \/\/ Sample card set sizes for young gen and humongous before GC: this makes\n+      \/\/ the policy to give back memory to the OS keep the most recent amount of\n+      \/\/ memory for these regions.\n+      if (hr->is_young() || hr->is_starts_humongous()) {\n+        _card_set_stats.add(hr->rem_set()->card_set_memory_stats());\n+      }\n+    }\n+\n+    bool humongous_region_is_candidate(HeapRegion* region) const {\n+      assert(region->is_starts_humongous(), \"Must start a humongous object\");\n+\n+      oop obj = cast_to_oop(region->bottom());\n+\n+      \/\/ Dead objects cannot be eager reclaim candidates. Due to class\n+      \/\/ unloading it is unsafe to query their classes so we return early.\n+      if (_g1h->is_obj_dead(obj, region)) {\n+        return false;\n+      }\n+\n+      \/\/ If we do not have a complete remembered set for the region, then we can\n+      \/\/ not be sure that we have all references to it.\n+      if (!region->rem_set()->is_complete()) {\n+        return false;\n+      }\n+      \/\/ Candidate selection must satisfy the following constraints\n+      \/\/ while concurrent marking is in progress:\n+      \/\/\n+      \/\/ * In order to maintain SATB invariants, an object must not be\n+      \/\/ reclaimed if it was allocated before the start of marking and\n+      \/\/ has not had its references scanned.  Such an object must have\n+      \/\/ its references (including type metadata) scanned to ensure no\n+      \/\/ live objects are missed by the marking process.  Objects\n+      \/\/ allocated after the start of concurrent marking don't need to\n+      \/\/ be scanned.\n+      \/\/\n+      \/\/ * An object must not be reclaimed if it is on the concurrent\n+      \/\/ mark stack.  Objects allocated after the start of concurrent\n+      \/\/ marking are never pushed on the mark stack.\n+      \/\/\n+      \/\/ Nominating only objects allocated after the start of concurrent\n+      \/\/ marking is sufficient to meet both constraints.  This may miss\n+      \/\/ some objects that satisfy the constraints, but the marking data\n+      \/\/ structures don't support efficiently performing the needed\n+      \/\/ additional tests or scrubbing of the mark stack.\n+      \/\/\n+      \/\/ However, we presently only nominate is_typeArray() objects.\n+      \/\/ A humongous object containing references induces remembered\n+      \/\/ set entries on other regions.  In order to reclaim such an\n+      \/\/ object, those remembered sets would need to be cleaned up.\n+      \/\/\n+      \/\/ We also treat is_typeArray() objects specially, allowing them\n+      \/\/ to be reclaimed even if allocated before the start of\n+      \/\/ concurrent mark.  For this we rely on mark stack insertion to\n+      \/\/ exclude is_typeArray() objects, preventing reclaiming an object\n+      \/\/ that is in the mark stack.  We also rely on the metadata for\n+      \/\/ such objects to be built-in and so ensured to be kept live.\n+      \/\/ Frequent allocation and drop of large binary blobs is an\n+      \/\/ important use case for eager reclaim, and this special handling\n+      \/\/ may reduce needed headroom.\n+\n+      return obj->is_typeArray() &&\n+             _g1h->is_potential_eager_reclaim_candidate(region);\n+    }\n+\n+  public:\n+    G1PrepareRegionsClosure(G1CollectedHeap* g1h, G1PrepareEvacuationTask* parent_task) :\n+      _g1h(g1h),\n+      _parent_task(parent_task),\n+      _worker_humongous_total(0),\n+      _worker_humongous_candidates(0) { }\n+\n+    ~G1PrepareRegionsClosure() {\n+      _parent_task->add_humongous_candidates(_worker_humongous_candidates);\n+      _parent_task->add_humongous_total(_worker_humongous_total);\n+    }\n+\n+    virtual bool do_heap_region(HeapRegion* hr) {\n+      \/\/ First prepare the region for scanning\n+      _g1h->rem_set()->prepare_region_for_scan(hr);\n+\n+      sample_card_set_size(hr);\n+\n+      \/\/ Now check if region is a humongous candidate\n+      if (!hr->is_starts_humongous()) {\n+        _g1h->register_region_with_region_attr(hr);\n+        return false;\n+      }\n+\n+      uint index = hr->hrm_index();\n+      if (humongous_region_is_candidate(hr)) {\n+        _g1h->set_humongous_reclaim_candidate(index, true);\n+        _g1h->register_humongous_region_with_region_attr(index);\n+        _worker_humongous_candidates++;\n+        \/\/ We will later handle the remembered sets of these regions.\n+      } else {\n+        _g1h->set_humongous_reclaim_candidate(index, false);\n+        _g1h->register_region_with_region_attr(hr);\n+      }\n+      log_debug(gc, humongous)(\"Humongous region %u (object size \" SIZE_FORMAT \" @ \" PTR_FORMAT \") remset \" SIZE_FORMAT \" code roots \" SIZE_FORMAT \" marked %d reclaim candidate %d type array %d\",\n+                               index,\n+                               (size_t)cast_to_oop(hr->bottom())->size() * HeapWordSize,\n+                               p2i(hr->bottom()),\n+                               hr->rem_set()->occupied(),\n+                               hr->rem_set()->strong_code_roots_list_length(),\n+                               _g1h->concurrent_mark()->next_mark_bitmap()->is_marked(hr->bottom()),\n+                               _g1h->is_humongous_reclaim_candidate(index),\n+                               cast_to_oop(hr->bottom())->is_typeArray()\n+                              );\n+      _worker_humongous_total++;\n+\n+      return false;\n+    }\n+\n+    G1CardSetMemoryStats card_set_stats() const {\n+      return _card_set_stats;\n+    }\n+  };\n+\n+  G1CollectedHeap* _g1h;\n+  HeapRegionClaimer _claimer;\n+  volatile uint _humongous_total;\n+  volatile uint _humongous_candidates;\n+\n+  G1CardSetMemoryStats _all_card_set_stats;\n+\n+public:\n+  G1PrepareEvacuationTask(G1CollectedHeap* g1h) :\n+    AbstractGangTask(\"Prepare Evacuation\"),\n+    _g1h(g1h),\n+    _claimer(_g1h->workers()->active_workers()),\n+    _humongous_total(0),\n+    _humongous_candidates(0) { }\n+\n+  void work(uint worker_id) {\n+    G1PrepareRegionsClosure cl(_g1h, this);\n+    _g1h->heap_region_par_iterate_from_worker_offset(&cl, &_claimer, worker_id);\n+\n+    MutexLocker x(ParGCRareEvent_lock, Mutex::_no_safepoint_check_flag);\n+    _all_card_set_stats.add(cl.card_set_stats());\n+  }\n+\n+  void add_humongous_candidates(uint candidates) {\n+    Atomic::add(&_humongous_candidates, candidates);\n+  }\n+\n+  void add_humongous_total(uint total) {\n+    Atomic::add(&_humongous_total, total);\n+  }\n+\n+  uint humongous_candidates() {\n+    return _humongous_candidates;\n+  }\n+\n+  uint humongous_total() {\n+    return _humongous_total;\n+  }\n+\n+  const G1CardSetMemoryStats all_card_set_stats() const {\n+    return _all_card_set_stats;\n+  }\n+};\n+\n+Tickspan G1YoungCollector::run_task_timed(AbstractGangTask* task) {\n+  Ticks start = Ticks::now();\n+  workers()->run_task(task);\n+  return Ticks::now() - start;\n+}\n+\n+void G1YoungCollector::set_young_collection_default_active_worker_threads(){\n+  uint active_workers = WorkerPolicy::calc_active_workers(workers()->total_workers(),\n+                                                          workers()->active_workers(),\n+                                                          Threads::number_of_non_daemon_threads());\n+  active_workers = workers()->update_active_workers(active_workers);\n+  log_info(gc,task)(\"Using %u workers of %u for evacuation\", active_workers, workers()->total_workers());\n+}\n+\n+void G1YoungCollector::pre_evacuate_collection_set(G1EvacuationInfo* evacuation_info, G1ParScanThreadStateSet* per_thread_states) {\n+  \/\/ Please see comment in g1CollectedHeap.hpp and\n+  \/\/ G1CollectedHeap::ref_processing_init() to see how\n+  \/\/ reference processing currently works in G1.\n+  ref_processor_stw()->start_discovery(false \/* always_clear *\/);\n+\n+  _g1h->reset_evacuation_failed_data();\n+\n+  _g1h->gc_prologue(false);\n+\n+  {\n+    Ticks start = Ticks::now();\n+    _g1h->retire_tlabs();\n+    phase_times()->record_prepare_tlab_time_ms((Ticks::now() - start).seconds() * 1000.0);\n+  }\n+\n+  {\n+    \/\/ Flush dirty card queues to qset, so later phases don't need to account\n+    \/\/ for partially filled per-thread queues and such.\n+    Ticks start = Ticks::now();\n+    G1BarrierSet::dirty_card_queue_set().concatenate_logs();\n+    Tickspan dt = Ticks::now() - start;\n+    phase_times()->record_concatenate_dirty_card_logs_time_ms(dt.seconds() * MILLIUNITS);\n+  }\n+\n+  \/\/ Disable the hot card cache.\n+  hot_card_cache()->reset_hot_cache_claimed_index();\n+  hot_card_cache()->set_use_cache(false);\n+\n+  \/\/ Initialize the GC alloc regions.\n+  allocator()->init_gc_alloc_regions(evacuation_info);\n+\n+  {\n+    Ticks start = Ticks::now();\n+    rem_set()->prepare_for_scan_heap_roots();\n+    phase_times()->record_prepare_heap_roots_time_ms((Ticks::now() - start).seconds() * 1000.0);\n+  }\n+\n+  {\n+    G1PrepareEvacuationTask g1_prep_task(_g1h);\n+    Tickspan task_time = run_task_timed(&g1_prep_task);\n+\n+    _g1h->set_young_gen_card_set_stats(g1_prep_task.all_card_set_stats());\n+    _g1h->set_humongous_stats(g1_prep_task.humongous_total(), g1_prep_task.humongous_candidates());\n+\n+    phase_times()->record_register_regions(task_time.seconds() * 1000.0);\n+  }\n+\n+  assert(_g1h->verifier()->check_region_attr_table(), \"Inconsistency in the region attributes table.\");\n+  per_thread_states->preserved_marks_set()->assert_empty();\n+\n+#if COMPILER2_OR_JVMCI\n+  DerivedPointerTable::clear();\n+#endif\n+\n+  if (collector_state()->in_concurrent_start_gc()) {\n+    concurrent_mark()->pre_concurrent_start(_gc_cause);\n+  }\n+\n+  evac_failure_injector()->arm_if_needed();\n+}\n+\n+class G1ParEvacuateFollowersClosure : public VoidClosure {\n+  double _start_term;\n+  double _term_time;\n+  size_t _term_attempts;\n+\n+  void start_term_time() { _term_attempts++; _start_term = os::elapsedTime(); }\n+  void end_term_time() { _term_time += (os::elapsedTime() - _start_term); }\n+\n+  G1CollectedHeap*              _g1h;\n+  G1ParScanThreadState*         _par_scan_state;\n+  G1ScannerTasksQueueSet*       _queues;\n+  TaskTerminator*               _terminator;\n+  G1GCPhaseTimes::GCParPhases   _phase;\n+\n+  G1ParScanThreadState*   par_scan_state() { return _par_scan_state; }\n+  G1ScannerTasksQueueSet* queues()         { return _queues; }\n+  TaskTerminator*         terminator()     { return _terminator; }\n+\n+  inline bool offer_termination() {\n+    EventGCPhaseParallel event;\n+    G1ParScanThreadState* const pss = par_scan_state();\n+    start_term_time();\n+    const bool res = (terminator() == nullptr) ? true : terminator()->offer_termination();\n+    end_term_time();\n+    event.commit(GCId::current(), pss->worker_id(), G1GCPhaseTimes::phase_name(G1GCPhaseTimes::Termination));\n+    return res;\n+  }\n+\n+public:\n+  G1ParEvacuateFollowersClosure(G1CollectedHeap* g1h,\n+                                G1ParScanThreadState* par_scan_state,\n+                                G1ScannerTasksQueueSet* queues,\n+                                TaskTerminator* terminator,\n+                                G1GCPhaseTimes::GCParPhases phase)\n+    : _start_term(0.0), _term_time(0.0), _term_attempts(0),\n+      _g1h(g1h), _par_scan_state(par_scan_state),\n+      _queues(queues), _terminator(terminator), _phase(phase) {}\n+\n+  void do_void() {\n+    EventGCPhaseParallel event;\n+    G1ParScanThreadState* const pss = par_scan_state();\n+    pss->trim_queue();\n+    event.commit(GCId::current(), pss->worker_id(), G1GCPhaseTimes::phase_name(_phase));\n+    do {\n+      EventGCPhaseParallel event;\n+      pss->steal_and_trim_queue(queues());\n+      event.commit(GCId::current(), pss->worker_id(), G1GCPhaseTimes::phase_name(_phase));\n+    } while (!offer_termination());\n+  }\n+\n+  double term_time() const { return _term_time; }\n+  size_t term_attempts() const { return _term_attempts; }\n+};\n+\n+class G1EvacuateRegionsBaseTask : public AbstractGangTask {\n+protected:\n+  G1CollectedHeap* _g1h;\n+  G1ParScanThreadStateSet* _per_thread_states;\n+  G1ScannerTasksQueueSet* _task_queues;\n+  TaskTerminator _terminator;\n+  uint _num_workers;\n+\n+  void evacuate_live_objects(G1ParScanThreadState* pss,\n+                             uint worker_id,\n+                             G1GCPhaseTimes::GCParPhases objcopy_phase,\n+                             G1GCPhaseTimes::GCParPhases termination_phase) {\n+    G1GCPhaseTimes* p = _g1h->phase_times();\n+\n+    Ticks start = Ticks::now();\n+    G1ParEvacuateFollowersClosure cl(_g1h, pss, _task_queues, &_terminator, objcopy_phase);\n+    cl.do_void();\n+\n+    assert(pss->queue_is_empty(), \"should be empty\");\n+\n+    Tickspan evac_time = (Ticks::now() - start);\n+    p->record_or_add_time_secs(objcopy_phase, worker_id, evac_time.seconds() - cl.term_time());\n+\n+    if (termination_phase == G1GCPhaseTimes::Termination) {\n+      p->record_time_secs(termination_phase, worker_id, cl.term_time());\n+      p->record_thread_work_item(termination_phase, worker_id, cl.term_attempts());\n+    } else {\n+      p->record_or_add_time_secs(termination_phase, worker_id, cl.term_time());\n+      p->record_or_add_thread_work_item(termination_phase, worker_id, cl.term_attempts());\n+    }\n+    assert(pss->trim_ticks().value() == 0,\n+           \"Unexpected partial trimming during evacuation value \" JLONG_FORMAT,\n+           pss->trim_ticks().value());\n+  }\n+\n+  virtual void start_work(uint worker_id) { }\n+\n+  virtual void end_work(uint worker_id) { }\n+\n+  virtual void scan_roots(G1ParScanThreadState* pss, uint worker_id) = 0;\n+\n+  virtual void evacuate_live_objects(G1ParScanThreadState* pss, uint worker_id) = 0;\n+\n+public:\n+  G1EvacuateRegionsBaseTask(const char* name,\n+                            G1ParScanThreadStateSet* per_thread_states,\n+                            G1ScannerTasksQueueSet* task_queues,\n+                            uint num_workers) :\n+    AbstractGangTask(name),\n+    _g1h(G1CollectedHeap::heap()),\n+    _per_thread_states(per_thread_states),\n+    _task_queues(task_queues),\n+    _terminator(num_workers, _task_queues),\n+    _num_workers(num_workers)\n+  { }\n+\n+  void work(uint worker_id) {\n+    start_work(worker_id);\n+\n+    {\n+      ResourceMark rm;\n+\n+      G1ParScanThreadState* pss = _per_thread_states->state_for_worker(worker_id);\n+      pss->set_ref_discoverer(_g1h->ref_processor_stw());\n+\n+      scan_roots(pss, worker_id);\n+      evacuate_live_objects(pss, worker_id);\n+    }\n+\n+    end_work(worker_id);\n+  }\n+};\n+\n+class G1EvacuateRegionsTask : public G1EvacuateRegionsBaseTask {\n+  G1RootProcessor* _root_processor;\n+  bool _has_optional_evacuation_work;\n+\n+  void scan_roots(G1ParScanThreadState* pss, uint worker_id) {\n+    _root_processor->evacuate_roots(pss, worker_id);\n+    _g1h->rem_set()->scan_heap_roots(pss, worker_id, G1GCPhaseTimes::ScanHR, G1GCPhaseTimes::ObjCopy, _has_optional_evacuation_work);\n+    _g1h->rem_set()->scan_collection_set_regions(pss, worker_id, G1GCPhaseTimes::ScanHR, G1GCPhaseTimes::CodeRoots, G1GCPhaseTimes::ObjCopy);\n+  }\n+\n+  void evacuate_live_objects(G1ParScanThreadState* pss, uint worker_id) {\n+    G1EvacuateRegionsBaseTask::evacuate_live_objects(pss, worker_id, G1GCPhaseTimes::ObjCopy, G1GCPhaseTimes::Termination);\n+  }\n+\n+  void start_work(uint worker_id) {\n+    _g1h->phase_times()->record_time_secs(G1GCPhaseTimes::GCWorkerStart, worker_id, Ticks::now().seconds());\n+  }\n+\n+  void end_work(uint worker_id) {\n+    _g1h->phase_times()->record_time_secs(G1GCPhaseTimes::GCWorkerEnd, worker_id, Ticks::now().seconds());\n+  }\n+\n+public:\n+  G1EvacuateRegionsTask(G1CollectedHeap* g1h,\n+                        G1ParScanThreadStateSet* per_thread_states,\n+                        G1ScannerTasksQueueSet* task_queues,\n+                        G1RootProcessor* root_processor,\n+                        uint num_workers,\n+                        bool has_optional_evacuation_work) :\n+    G1EvacuateRegionsBaseTask(\"G1 Evacuate Regions\", per_thread_states, task_queues, num_workers),\n+    _root_processor(root_processor),\n+    _has_optional_evacuation_work(has_optional_evacuation_work)\n+  { }\n+};\n+\n+void G1YoungCollector::evacuate_initial_collection_set(G1ParScanThreadStateSet* per_thread_states,\n+                                                      bool has_optional_evacuation_work) {\n+  G1GCPhaseTimes* p = phase_times();\n+\n+  {\n+    Ticks start = Ticks::now();\n+    rem_set()->merge_heap_roots(true \/* initial_evacuation *\/);\n+    p->record_merge_heap_roots_time((Ticks::now() - start).seconds() * 1000.0);\n+  }\n+\n+  Tickspan task_time;\n+  const uint num_workers = workers()->active_workers();\n+\n+  Ticks start_processing = Ticks::now();\n+  {\n+    G1RootProcessor root_processor(_g1h, num_workers);\n+    G1EvacuateRegionsTask g1_par_task(_g1h,\n+                                      per_thread_states,\n+                                      task_queues(),\n+                                      &root_processor,\n+                                      num_workers,\n+                                      has_optional_evacuation_work);\n+    task_time = run_task_timed(&g1_par_task);\n+    \/\/ Closing the inner scope will execute the destructor for the\n+    \/\/ G1RootProcessor object. By subtracting the WorkGang task from the total\n+    \/\/ time of this scope, we get the \"NMethod List Cleanup\" time. This list is\n+    \/\/ constructed during \"STW two-phase nmethod root processing\", see more in\n+    \/\/ nmethod.hpp\n+  }\n+  Tickspan total_processing = Ticks::now() - start_processing;\n+\n+  p->record_initial_evac_time(task_time.seconds() * 1000.0);\n+  p->record_or_add_nmethod_list_cleanup_time((total_processing - task_time).seconds() * 1000.0);\n+\n+  rem_set()->complete_evac_phase(has_optional_evacuation_work);\n+}\n+\n+class G1EvacuateOptionalRegionsTask : public G1EvacuateRegionsBaseTask {\n+\n+  void scan_roots(G1ParScanThreadState* pss, uint worker_id) {\n+    _g1h->rem_set()->scan_heap_roots(pss, worker_id, G1GCPhaseTimes::OptScanHR, G1GCPhaseTimes::OptObjCopy, true \/* remember_already_scanned_cards *\/);\n+    _g1h->rem_set()->scan_collection_set_regions(pss, worker_id, G1GCPhaseTimes::OptScanHR, G1GCPhaseTimes::OptCodeRoots, G1GCPhaseTimes::OptObjCopy);\n+  }\n+\n+  void evacuate_live_objects(G1ParScanThreadState* pss, uint worker_id) {\n+    G1EvacuateRegionsBaseTask::evacuate_live_objects(pss, worker_id, G1GCPhaseTimes::OptObjCopy, G1GCPhaseTimes::OptTermination);\n+  }\n+\n+public:\n+  G1EvacuateOptionalRegionsTask(G1ParScanThreadStateSet* per_thread_states,\n+                                G1ScannerTasksQueueSet* queues,\n+                                uint num_workers) :\n+    G1EvacuateRegionsBaseTask(\"G1 Evacuate Optional Regions\", per_thread_states, queues, num_workers) {\n+  }\n+};\n+\n+void G1YoungCollector::evacuate_next_optional_regions(G1ParScanThreadStateSet* per_thread_states) {\n+  \/\/ To access the protected constructor\/destructor\n+  class G1MarkScope : public MarkScope { };\n+\n+  Tickspan task_time;\n+\n+  Ticks start_processing = Ticks::now();\n+  {\n+    G1MarkScope code_mark_scope;\n+    G1EvacuateOptionalRegionsTask task(per_thread_states, task_queues(), workers()->active_workers());\n+    task_time = run_task_timed(&task);\n+    \/\/ See comment in evacuate_initial_collection_set() for the reason of the scope.\n+  }\n+  Tickspan total_processing = Ticks::now() - start_processing;\n+\n+  G1GCPhaseTimes* p = phase_times();\n+  p->record_or_add_nmethod_list_cleanup_time((total_processing - task_time).seconds() * 1000.0);\n+}\n+\n+void G1YoungCollector::evacuate_optional_collection_set(G1ParScanThreadStateSet* per_thread_states) {\n+  const double collection_start_time_ms = phase_times()->cur_collection_start_sec() * 1000.0;\n+\n+  while (!_g1h->evacuation_failed() && collection_set()->optional_region_length() > 0) {\n+\n+    double time_used_ms = os::elapsedTime() * 1000.0 - collection_start_time_ms;\n+    double time_left_ms = MaxGCPauseMillis - time_used_ms;\n+\n+    if (time_left_ms < 0 ||\n+        !collection_set()->finalize_optional_for_evacuation(time_left_ms * policy()->optional_evacuation_fraction())) {\n+      log_trace(gc, ergo, cset)(\"Skipping evacuation of %u optional regions, no more regions can be evacuated in %.3fms\",\n+                                collection_set()->optional_region_length(), time_left_ms);\n+      break;\n+    }\n+\n+    {\n+      Ticks start = Ticks::now();\n+      rem_set()->merge_heap_roots(false \/* initial_evacuation *\/);\n+      phase_times()->record_or_add_optional_merge_heap_roots_time((Ticks::now() - start).seconds() * 1000.0);\n+    }\n+\n+    {\n+      Ticks start = Ticks::now();\n+      evacuate_next_optional_regions(per_thread_states);\n+      phase_times()->record_or_add_optional_evac_time((Ticks::now() - start).seconds() * 1000.0);\n+    }\n+\n+    rem_set()->complete_evac_phase(true \/* has_more_than_one_evacuation_phase *\/);\n+  }\n+\n+  collection_set()->abandon_optional_collection_set(per_thread_states);\n+}\n+\n+\/\/ Non Copying Keep Alive closure\n+class G1KeepAliveClosure: public OopClosure {\n+  G1CollectedHeap*_g1h;\n+public:\n+  G1KeepAliveClosure(G1CollectedHeap* g1h) :_g1h(g1h) {}\n+  void do_oop(narrowOop* p) { guarantee(false, \"Not needed\"); }\n+  void do_oop(oop* p) {\n+    oop obj = *p;\n+    assert(obj != NULL, \"the caller should have filtered out NULL values\");\n+\n+    const G1HeapRegionAttr region_attr =_g1h->region_attr(obj);\n+    if (!region_attr.is_in_cset_or_humongous()) {\n+      return;\n+    }\n+    if (region_attr.is_in_cset()) {\n+      assert( obj->is_forwarded(), \"invariant\" );\n+      *p = obj->forwardee();\n+    } else {\n+      assert(!obj->is_forwarded(), \"invariant\" );\n+      assert(region_attr.is_humongous(),\n+             \"Only allowed G1HeapRegionAttr state is IsHumongous, but is %d\", region_attr.type());\n+     _g1h->set_humongous_is_live(obj);\n+    }\n+  }\n+};\n+\n+\/\/ Copying Keep Alive closure - can be called from both\n+\/\/ serial and parallel code as long as different worker\n+\/\/ threads utilize different G1ParScanThreadState instances\n+\/\/ and different queues.\n+class G1CopyingKeepAliveClosure: public OopClosure {\n+  G1CollectedHeap* _g1h;\n+  G1ParScanThreadState*    _par_scan_state;\n+\n+public:\n+  G1CopyingKeepAliveClosure(G1CollectedHeap* g1h,\n+                            G1ParScanThreadState* pss):\n+    _g1h(g1h),\n+    _par_scan_state(pss)\n+  {}\n+\n+  virtual void do_oop(narrowOop* p) { do_oop_work(p); }\n+  virtual void do_oop(      oop* p) { do_oop_work(p); }\n+\n+  template <class T> void do_oop_work(T* p) {\n+    oop obj = RawAccess<>::oop_load(p);\n+\n+    if (_g1h->is_in_cset_or_humongous(obj)) {\n+      \/\/ If the referent object has been forwarded (either copied\n+      \/\/ to a new location or to itself in the event of an\n+      \/\/ evacuation failure) then we need to update the reference\n+      \/\/ field and, if both reference and referent are in the G1\n+      \/\/ heap, update the RSet for the referent.\n+      \/\/\n+      \/\/ If the referent has not been forwarded then we have to keep\n+      \/\/ it alive by policy. Therefore we have copy the referent.\n+      \/\/\n+      \/\/ When the queue is drained (after each phase of reference processing)\n+      \/\/ the object and it's followers will be copied, the reference field set\n+      \/\/ to point to the new location, and the RSet updated.\n+      _par_scan_state->push_on_queue(ScannerTask(p));\n+    }\n+  }\n+};\n+\n+class G1STWRefProcProxyTask : public RefProcProxyTask {\n+  G1CollectedHeap& _g1h;\n+  G1ParScanThreadStateSet& _pss;\n+  TaskTerminator _terminator;\n+  G1ScannerTasksQueueSet& _task_queues;\n+\n+public:\n+  G1STWRefProcProxyTask(uint max_workers, G1CollectedHeap& g1h, G1ParScanThreadStateSet& pss, G1ScannerTasksQueueSet& task_queues)\n+    : RefProcProxyTask(\"G1STWRefProcProxyTask\", max_workers),\n+      _g1h(g1h),\n+      _pss(pss),\n+      _terminator(max_workers, &task_queues),\n+      _task_queues(task_queues) {}\n+\n+  void work(uint worker_id) override {\n+    assert(worker_id < _max_workers, \"sanity\");\n+    uint index = (_tm == RefProcThreadModel::Single) ? 0 : worker_id;\n+\n+    G1ParScanThreadState* pss = _pss.state_for_worker(index);\n+    pss->set_ref_discoverer(nullptr);\n+\n+    G1STWIsAliveClosure is_alive(&_g1h);\n+    G1CopyingKeepAliveClosure keep_alive(&_g1h, pss);\n+    G1ParEvacuateFollowersClosure complete_gc(&_g1h, pss, &_task_queues, _tm == RefProcThreadModel::Single ? nullptr : &_terminator, G1GCPhaseTimes::ObjCopy);\n+    _rp_task->rp_work(worker_id, &is_alive, &keep_alive, &complete_gc);\n+\n+    \/\/ We have completed copying any necessary live referent objects.\n+    assert(pss->queue_is_empty(), \"both queue and overflow should be empty\");\n+  }\n+\n+  void prepare_run_task_hook() override {\n+    _terminator.reset_for_reuse(_queue_count);\n+  }\n+};\n+\n+void G1YoungCollector::process_discovered_references(G1ParScanThreadStateSet* per_thread_states) {\n+  Ticks start = Ticks::now();\n+\n+  ReferenceProcessor* rp = ref_processor_stw();\n+  assert(rp->discovery_enabled(), \"should have been enabled\");\n+\n+  uint no_of_gc_workers = workers()->active_workers();\n+  rp->set_active_mt_degree(no_of_gc_workers);\n+\n+  G1STWRefProcProxyTask task(rp->max_num_queues(), *_g1h, *per_thread_states, *task_queues());\n+  ReferenceProcessorPhaseTimes& pt = *phase_times()->ref_phase_times();\n+  ReferenceProcessorStats stats = rp->process_discovered_references(task, pt);\n+\n+  gc_tracer_stw()->report_gc_reference_stats(stats);\n+\n+  _g1h->make_pending_list_reachable();\n+\n+  rp->verify_no_references_recorded();\n+\n+  phase_times()->record_ref_proc_time((Ticks::now() - start).seconds() * MILLIUNITS);\n+}\n+\n+bool G1STWIsAliveClosure::do_object_b(oop p) {\n+  \/\/ An object is reachable if it is outside the collection set,\n+  \/\/ or is inside and copied.\n+  return !_g1h->is_in_cset(p) || p->is_forwarded();\n+}\n+\n+void G1YoungCollector::post_evacuate_cleanup_1(G1ParScanThreadStateSet* per_thread_states) {\n+  Ticks start = Ticks::now();\n+  {\n+    G1PostEvacuateCollectionSetCleanupTask1 cl(per_thread_states);\n+    _g1h->run_batch_task(&cl);\n+  }\n+  phase_times()->record_post_evacuate_cleanup_task_1_time((Ticks::now() - start).seconds() * 1000.0);\n+}\n+\n+void G1YoungCollector::post_evacuate_cleanup_2(G1ParScanThreadStateSet* per_thread_states,\n+                                               G1EvacuationInfo* evacuation_info) {\n+  Ticks start = Ticks::now();\n+  {\n+    G1PostEvacuateCollectionSetCleanupTask2 cl(per_thread_states, evacuation_info);\n+    _g1h->run_batch_task(&cl);\n+  }\n+  phase_times()->record_post_evacuate_cleanup_task_2_time((Ticks::now() - start).seconds() * 1000.0);\n+}\n+\n+void G1YoungCollector::post_evacuate_collection_set(G1EvacuationInfo* evacuation_info,\n+                                                    G1ParScanThreadStateSet* per_thread_states) {\n+  G1GCPhaseTimes* p = phase_times();\n+\n+  \/\/ Process any discovered reference objects - we have\n+  \/\/ to do this _before_ we retire the GC alloc regions\n+  \/\/ as we may have to copy some 'reachable' referent\n+  \/\/ objects (and their reachable sub-graphs) that were\n+  \/\/ not copied during the pause.\n+  process_discovered_references(per_thread_states);\n+\n+  G1STWIsAliveClosure is_alive(_g1h);\n+  G1KeepAliveClosure keep_alive(_g1h);\n+\n+  WeakProcessor::weak_oops_do(workers(), &is_alive, &keep_alive, p->weak_phase_times());\n+\n+  allocator()->release_gc_alloc_regions(evacuation_info);\n+\n+  post_evacuate_cleanup_1(per_thread_states);\n+\n+  post_evacuate_cleanup_2(per_thread_states, evacuation_info);\n+\n+  assert_used_and_recalculate_used_equal(_g1h);\n+\n+  _g1h->rebuild_free_region_list();\n+\n+  _g1h->record_obj_copy_mem_stats();\n+\n+  evacuation_info->set_collectionset_used_before(collection_set()->bytes_used_before());\n+  evacuation_info->set_bytes_used(_g1h->bytes_used_during_gc());\n+\n+  _g1h->start_new_collection_set();\n+\n+  _g1h->prepare_tlabs_for_mutator();\n+\n+  _g1h->gc_epilogue(false);\n+\n+  _g1h->expand_heap_after_young_collection();\n+}\n+\n+class G1PreservedMarksSet : public PreservedMarksSet {\n+public:\n+\n+  G1PreservedMarksSet(uint num_workers) : PreservedMarksSet(true \/* in_c_heap *\/) {\n+    init(num_workers);\n+  }\n+\n+  virtual ~G1PreservedMarksSet() {\n+    assert_empty();\n+    reclaim();\n+  }\n+};\n+\n+G1YoungCollector::G1YoungCollector(GCCause::Cause gc_cause, double target_pause_time_ms) :\n+  _g1h(G1CollectedHeap::heap()),\n+  _gc_cause(gc_cause),\n+  _target_pause_time_ms(target_pause_time_ms),\n+  _concurrent_operation_is_full_mark(false)\n+{\n+}\n+\n+void G1YoungCollector::collect() {\n+  \/\/ Do timing\/tracing\/statistics\/pre- and post-logging\/verification work not\n+  \/\/ directly related to the collection. They should not be accounted for in\n+  \/\/ collection work timing.\n+\n+  \/\/ The G1YoungGCTraceTime message depends on collector state, so must come after\n+  \/\/ determining collector state.\n+  G1YoungGCTraceTime tm(_gc_cause);\n+\n+  \/\/ JFR\n+  G1YoungGCJFRTracerMark jtm(gc_timer_stw(), gc_tracer_stw(), _gc_cause);\n+  \/\/ JStat\/MXBeans\n+  G1MonitoringScope ms(monitoring_support(),\n+                       false \/* full_gc *\/,\n+                       collector_state()->in_mixed_phase() \/* all_memory_pools_affected *\/);\n+  \/\/ Create the heap printer before internal pause timing to have\n+  \/\/ heap information printed as last part of detailed GC log.\n+  G1HeapPrinterMark hpm(_g1h);\n+  \/\/ Young GC internal pause timing\n+  G1YoungGCNotifyPauseMark npm;\n+\n+  \/\/ Verification may use the gang workers, so they must be set up before.\n+  \/\/ Individual parallel phases may override this.\n+  set_young_collection_default_active_worker_threads();\n+\n+  \/\/ Wait for root region scan here to make sure that it is done before any\n+  \/\/ use of the STW work gang to maximize cpu use (i.e. all cores are available\n+  \/\/ just to do that).\n+  wait_for_root_region_scanning();\n+\n+  G1YoungGCVerifierMark vm;\n+  {\n+    \/\/ Actual collection work starts and is executed (only) in this scope.\n+\n+    \/\/ Young GC internal collection timing. The elapsed time recorded in the\n+    \/\/ policy for the collection deliberately elides verification (and some\n+    \/\/ other trivial setup above).\n+    policy()->record_young_collection_start();\n+\n+    calculate_collection_set(jtm.evacuation_info(), _target_pause_time_ms);\n+\n+    G1RedirtyCardsQueueSet rdcqs(G1BarrierSet::dirty_card_queue_set().allocator());\n+    G1PreservedMarksSet preserved_marks_set(workers()->active_workers());\n+    G1ParScanThreadStateSet per_thread_states(_g1h,\n+                                              &rdcqs,\n+                                              &preserved_marks_set,\n+                                              workers()->active_workers(),\n+                                              collection_set()->young_region_length(),\n+                                              collection_set()->optional_region_length());\n+    pre_evacuate_collection_set(jtm.evacuation_info(), &per_thread_states);\n+\n+    bool may_do_optional_evacuation = collection_set()->optional_region_length() != 0;\n+    \/\/ Actually do the work...\n+    evacuate_initial_collection_set(&per_thread_states, may_do_optional_evacuation);\n+\n+    if (may_do_optional_evacuation) {\n+      evacuate_optional_collection_set(&per_thread_states);\n+    }\n+    post_evacuate_collection_set(jtm.evacuation_info(), &per_thread_states);\n+\n+    \/\/ Refine the type of a concurrent mark operation now that we did the\n+    \/\/ evacuation, eventually aborting it.\n+    _concurrent_operation_is_full_mark = policy()->concurrent_operation_is_full_mark(\"Revise IHOP\");\n+\n+    \/\/ Need to report the collection pause now since record_collection_pause_end()\n+    \/\/ modifies it to the next state.\n+    jtm.report_pause_type(collector_state()->young_gc_pause_type(_concurrent_operation_is_full_mark));\n+\n+    policy()->record_young_collection_end(_concurrent_operation_is_full_mark);\n+  }\n+  TASKQUEUE_STATS_ONLY(print_taskqueue_stats());\n+  TASKQUEUE_STATS_ONLY(reset_taskqueue_stats());\n+}\n","filename":"src\/hotspot\/share\/gc\/g1\/g1YoungCollector.cpp","additions":1107,"deletions":0,"binary":false,"changes":1107,"status":"added"},{"patch":"@@ -0,0 +1,142 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_G1_G1YOUNGCOLLECTOR_HPP\n+#define SHARE_GC_G1_G1YOUNGCOLLECTOR_HPP\n+\n+#include \"gc\/g1\/g1YoungGCEvacFailureInjector.hpp\"\n+#include \"gc\/shared\/gcCause.hpp\"\n+#include \"gc\/shared\/taskqueue.hpp\"\n+\n+class AbstractGangTask;\n+class G1Allocator;\n+class G1BatchedGangTask;\n+class G1CardSetMemoryStats;\n+class G1CollectedHeap;\n+class G1CollectionSet;\n+class G1CollectorState;\n+class G1ConcurrentMark;\n+class G1EvacuationInfo;\n+class G1GCPhaseTimes;\n+class G1HotCardCache;\n+class G1HRPrinter;\n+class G1MonitoringSupport;\n+class G1NewTracer;\n+class G1ParScanThreadStateSet;\n+class G1Policy;\n+class G1RedirtyCardsQueueSet;\n+class G1RemSet;\n+class G1SurvivorRegions;\n+class G1YoungGCEvacFailureInjector;\n+class STWGCTimer;\n+class WorkGang;\n+\n+class outputStream;\n+\n+class G1YoungCollector {\n+\n+  G1CollectedHeap* _g1h;\n+\n+  G1Allocator* allocator() const;\n+  G1CollectionSet* collection_set() const;\n+  G1CollectorState* collector_state() const;\n+  G1ConcurrentMark* concurrent_mark() const;\n+  STWGCTimer* gc_timer_stw() const;\n+  G1NewTracer* gc_tracer_stw() const;\n+  G1HotCardCache* hot_card_cache() const;\n+  G1HRPrinter* hr_printer() const;\n+  G1MonitoringSupport* monitoring_support() const;\n+  G1GCPhaseTimes* phase_times() const;\n+  G1Policy* policy() const;\n+  G1RemSet* rem_set() const;\n+  G1ScannerTasksQueueSet* task_queues() const;\n+  G1SurvivorRegions* survivor_regions() const;\n+  ReferenceProcessor* ref_processor_stw() const;\n+  WorkGang* workers() const;\n+  G1YoungGCEvacFailureInjector* evac_failure_injector() const;\n+\n+  GCCause::Cause _gc_cause;\n+  double _target_pause_time_ms;\n+\n+  bool _concurrent_operation_is_full_mark;\n+\n+  \/\/ Runs the given AbstractGangTask with the current active workers,\n+  \/\/ returning the total time taken.\n+  Tickspan run_task_timed(AbstractGangTask* task);\n+\n+  void wait_for_root_region_scanning();\n+\n+  void calculate_collection_set(G1EvacuationInfo* evacuation_info, double target_pause_time_ms);\n+\n+  void set_young_collection_default_active_worker_threads();\n+\n+  void pre_evacuate_collection_set(G1EvacuationInfo* evacuation_info, G1ParScanThreadStateSet* pss);\n+  \/\/ Actually do the work of evacuating the parts of the collection set.\n+  \/\/ The has_optional_evacuation_work flag for the initial collection set\n+  \/\/ evacuation indicates whether one or more optional evacuation steps may\n+  \/\/ follow.\n+  \/\/ If not set, G1 can avoid clearing the card tables of regions that we scan\n+  \/\/ for roots from the heap: when scanning the card table for dirty cards after\n+  \/\/ all remembered sets have been dumped onto it, for optional evacuation we\n+  \/\/ mark these cards as \"Scanned\" to know that we do not need to re-scan them\n+  \/\/ in the additional optional evacuation passes. This means that in the \"Clear\n+  \/\/ Card Table\" phase we need to clear those marks. However, if there is no\n+  \/\/ optional evacuation, g1 can immediately clean the dirty cards it encounters\n+  \/\/ as nobody else will be looking at them again, saving the clear card table\n+  \/\/ work later.\n+  \/\/ This case is very common (young only collections and most mixed gcs), so\n+  \/\/ depending on the ratio between scanned and evacuated regions (which g1 always\n+  \/\/ needs to clear), this is a big win.\n+  void evacuate_initial_collection_set(G1ParScanThreadStateSet* per_thread_states,\n+                                       bool has_optional_evacuation_work);\n+  void evacuate_optional_collection_set(G1ParScanThreadStateSet* per_thread_states);\n+  \/\/ Evacuate the next set of optional regions.\n+  void evacuate_next_optional_regions(G1ParScanThreadStateSet* per_thread_states);\n+\n+  \/\/ Process any reference objects discovered.\n+  void process_discovered_references(G1ParScanThreadStateSet* per_thread_states);\n+  void post_evacuate_cleanup_1(G1ParScanThreadStateSet* per_thread_states);\n+  void post_evacuate_cleanup_2(G1ParScanThreadStateSet* per_thread_states,\n+                               G1EvacuationInfo* evacuation_info);\n+\n+  void post_evacuate_collection_set(G1EvacuationInfo* evacuation_info,\n+                                    G1ParScanThreadStateSet* per_thread_states);\n+\n+\n+#if TASKQUEUE_STATS\n+  uint num_task_queues() const;\n+  static void print_taskqueue_stats_hdr(outputStream* const st);\n+  void print_taskqueue_stats() const;\n+  void reset_taskqueue_stats();\n+#endif \/\/ TASKQUEUE_STATS\n+\n+public:\n+\n+  G1YoungCollector(GCCause::Cause gc_cause, double target_pause_time_ms);\n+  void collect();\n+\n+  bool concurrent_operation_is_full_mark() const { return _concurrent_operation_is_full_mark; }\n+};\n+\n+#endif \/\/ SHARE_GC_G1_G1YOUNGCOLLECTOR_HPP\n","filename":"src\/hotspot\/share\/gc\/g1\/g1YoungCollector.hpp","additions":142,"deletions":0,"binary":false,"changes":142,"status":"added"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"gc\/g1\/g1EvacuationInfo.hpp\"\n","filename":"src\/hotspot\/share\/gc\/g1\/g1YoungGCPostEvacuateTasks.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"}]}