{"files":[{"patch":"@@ -3277,4 +3277,5 @@\n-char* os::pd_reserve_memory_special(size_t bytes, size_t alignment, size_t page_size, char* addr,\n-                                    bool exec) {\n-  assert(UseLargePages, \"only for large pages\");\n-  assert(page_size == os::large_page_size(), \"Currently only support one large page size on Windows\");\n+static char* reserve_large_pages_individually(size_t size, char* req_addr, bool exec) {\n+  log_debug(pagesize)(\"Reserving large pages individually.\");\n+\n+  const DWORD prot = exec ? PAGE_EXECUTE_READWRITE : PAGE_READWRITE;\n+  const DWORD flags = MEM_RESERVE | MEM_COMMIT | MEM_LARGE_PAGES;\n@@ -3282,2 +3283,11 @@\n-  if (!is_aligned(bytes, page_size) || alignment > page_size) {\n-    return NULL; \/\/ Fallback to small pages.\n+  char * p_buf = allocate_pages_individually(size, req_addr, flags, prot, LargePagesIndividualAllocationInjectError);\n+  if (p_buf == NULL) {\n+    \/\/ give an appropriate warning message\n+    if (UseNUMAInterleaving) {\n+      warning(\"NUMA large page allocation failed, UseLargePages flag ignored\");\n+    }\n+    if (UseLargePagesIndividualAllocation) {\n+      warning(\"Individually allocated large pages failed, \"\n+              \"use -XX:-UseLargePagesIndividualAllocation to turn off\");\n+    }\n+    return NULL;\n@@ -3285,0 +3295,5 @@\n+  return p_buf;\n+}\n+\n+static char* reserve_large_pages_single_range(size_t size, char* req_addr, bool exec) {\n+  log_debug(pagesize)(\"Reserving large pages in a single large chunk.\");\n@@ -3289,0 +3304,4 @@\n+  return (char *) virtualAlloc(req_addr, size, flags, prot);\n+}\n+\n+static char* reserve_large_pages(size_t size, char* req_addr, bool exec) {\n@@ -3293,1 +3312,4 @@\n-    log_debug(pagesize)(\"Reserving large pages individually.\");\n+    return reserve_large_pages_individually(size, req_addr, exec);\n+  }\n+  return reserve_large_pages_single_range(size, req_addr, exec);\n+}\n@@ -3295,11 +3317,30 @@\n-    char * p_buf = allocate_pages_individually(bytes, addr, flags, prot, LargePagesIndividualAllocationInjectError);\n-    if (p_buf == NULL) {\n-      \/\/ give an appropriate warning message\n-      if (UseNUMAInterleaving) {\n-        warning(\"NUMA large page allocation failed, UseLargePages flag ignored\");\n-      }\n-      if (UseLargePagesIndividualAllocation) {\n-        warning(\"Individually allocated large pages failed, \"\n-                \"use -XX:-UseLargePagesIndividualAllocation to turn off\");\n-      }\n-      return NULL;\n+static char* find_aligned_address(size_t size, size_t alignment) {\n+  \/\/ Temporary reserve memory large enough to ensure we can get the requested\n+  \/\/ alignment and still fit the reservation.\n+  char* addr = (char*) virtualAlloc(NULL, size + alignment, MEM_RESERVE, PAGE_NOACCESS);\n+  \/\/ Align the address to the requested alignment.\n+  char* aligned_addr = align_up(addr, alignment);\n+  \/\/ Free the temporary reservation.\n+  virtualFree(addr, 0, MEM_RELEASE);\n+\n+  return aligned_addr;\n+}\n+\n+static char* reserve_large_pages_aligned(size_t size, size_t alignment, bool exec) {\n+  log_debug(pagesize)(\"Reserving large pages at an aligned address, alignment=\" SIZE_FORMAT \"%s\",\n+                      byte_size_in_exact_unit(alignment), exact_unit_for_byte_size(alignment));\n+\n+  \/\/ Will try to find a suitable address at most 20 times. The reason we need to try\n+  \/\/ multiple times is that between finding the aligned address and trying to commit\n+  \/\/ the large pages another thread might have reserved an overlapping region.\n+  const int attempts_limit = 20;\n+  for (int attempts = 0; attempts < attempts_limit; attempts++)  {\n+    \/\/ Find aligned address.\n+    char* aligned_address = find_aligned_address(size, alignment);\n+\n+    \/\/ Try to do the large page reservation using the aligned address.\n+    aligned_address = reserve_large_pages(size, aligned_address, exec);\n+    if (aligned_address != NULL) {\n+      \/\/ Reservation at the aligned address succeeded.\n+      guarantee(is_aligned(aligned_address, alignment), \"Must be aligned\");\n+      return aligned_address;\n@@ -3307,0 +3348,1 @@\n+  }\n@@ -3308,1 +3350,3 @@\n-    return p_buf;\n+  log_debug(pagesize)(\"Failed reserving large pages at aligned address\");\n+  return NULL;\n+}\n@@ -3310,2 +3354,6 @@\n-  } else {\n-    log_debug(pagesize)(\"Reserving large pages in a single large chunk.\");\n+char* os::pd_reserve_memory_special(size_t bytes, size_t alignment, size_t page_size, char* addr,\n+                                    bool exec) {\n+  assert(UseLargePages, \"only for large pages\");\n+  assert(page_size == os::large_page_size(), \"Currently only support one large page size on Windows\");\n+  assert(is_aligned(addr, alignment), \"Must be\");\n+  assert(is_aligned(addr, page_size), \"Must be\");\n@@ -3313,3 +3361,4 @@\n-    \/\/ normal policy just allocate it all at once\n-    DWORD flag = MEM_RESERVE | MEM_COMMIT | MEM_LARGE_PAGES;\n-    char * res = (char *)virtualAlloc(addr, bytes, flag, prot);\n+  if (!is_aligned(bytes, page_size)) {\n+    \/\/ Fallback to small pages, Windows does not support mixed mappings.\n+    return NULL;\n+  }\n@@ -3317,1 +3366,6 @@\n-    return res;\n+  \/\/ The requested alignment can be larger than the page size, for example with G1\n+  \/\/ the alignment is bound to the heap region size. So this reservation needs to\n+  \/\/ ensure that the requested alignment is met. When there is a requested address\n+  \/\/ this solves it self, since it must be properly aligned already.\n+  if (addr == NULL && alignment > page_size) {\n+    return reserve_large_pages_aligned(bytes, alignment, exec);\n@@ -3319,0 +3373,3 @@\n+\n+  \/\/ No additional requirements, just reserve the large pages.\n+  return reserve_large_pages(bytes, addr, exec);\n","filename":"src\/hotspot\/os\/windows\/os_windows.cpp","additions":82,"deletions":25,"binary":false,"changes":107,"status":"modified"}]}