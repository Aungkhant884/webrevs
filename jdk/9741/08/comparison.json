{"files":[{"patch":"@@ -191,1 +191,1 @@\n-    CodeBlob* sender_blob = CodeCache::find_blob_unsafe(sender_pc);\n+    CodeBlob* sender_blob = CodeCache::find_blob(sender_pc);\n@@ -196,5 +196,0 @@\n-    \/\/ Could be a zombie method\n-    if (sender_blob->is_zombie() || sender_blob->is_unloaded()) {\n-      return false;\n-    }\n-\n","filename":"src\/hotspot\/cpu\/aarch64\/frame_aarch64.cpp","additions":1,"deletions":6,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -168,4 +168,2 @@\n-  \/\/ when last_Java_sp is non-null but the pc fetched is junk. If we are truly\n-  \/\/ unlucky the junk value could be to a zombied method and we'll die on the\n-  \/\/ find_blob call. This is also why we can have no asserts on the validity\n-  \/\/ of the pc we find here. AsyncGetCallTrace -> pd_get_top_frame_for_signal_handler\n+  \/\/ when last_Java_sp is non-null but the pc fetched is junk.\n+  \/\/ AsyncGetCallTrace -> pd_get_top_frame_for_signal_handler\n","filename":"src\/hotspot\/cpu\/aarch64\/frame_aarch64.inline.hpp","additions":2,"deletions":4,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -163,1 +163,1 @@\n-  CodeBlob* cb = CodeCache::find_blob_unsafe(addr);   \/\/ Else we get assertion if nmethod is zombie.\n+  CodeBlob* cb = CodeCache::find_blob(addr);\n@@ -459,1 +459,1 @@\n-bool NativeInstruction::is_sigill_zombie_not_entrant() {\n+bool NativeInstruction::is_sigill_not_entrant() {\n@@ -474,1 +474,1 @@\n-\/\/ nmethod::make_not_entrant_or_zombie)\n+\/\/ nmethod::make_not_entrant)\n@@ -480,1 +480,1 @@\n-         || nativeInstruction_at(verified_entry)->is_sigill_zombie_not_entrant(),\n+         || nativeInstruction_at(verified_entry)->is_sigill_not_entrant(),\n@@ -491,2 +491,1 @@\n-    \/\/ We use an illegal instruction for marking a method as\n-    \/\/ not_entrant or zombie.\n+    \/\/ We use an illegal instruction for marking a method as not_entrant.\n","filename":"src\/hotspot\/cpu\/aarch64\/nativeInst_aarch64.cpp","additions":5,"deletions":6,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -82,1 +82,1 @@\n-  bool is_sigill_zombie_not_entrant();\n+  bool is_sigill_not_entrant();\n","filename":"src\/hotspot\/cpu\/aarch64\/nativeInst_aarch64.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -126,1 +126,1 @@\n-    CodeBlob* sender_blob = CodeCache::find_blob_unsafe(sender_pc);\n+    CodeBlob* sender_blob = CodeCache::find_blob(sender_pc);\n@@ -151,4 +151,0 @@\n-    if (sender_blob->is_zombie() || sender_blob->is_unloaded()) {\n-      return false;\n-    }\n-\n","filename":"src\/hotspot\/cpu\/arm\/frame_arm.cpp","additions":1,"deletions":5,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -293,1 +293,1 @@\n-  a[0] = zombie_illegal_instruction; \/\/ always illegal\n+  a[0] = not_entrant_illegal_instruction; \/\/ always illegal\n","filename":"src\/hotspot\/cpu\/arm\/nativeInst_arm_32.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -66,1 +66,1 @@\n-  static const int zombie_illegal_instruction = 0xe7f000f0;\n+  static const int not_entrant_illegal_instruction = 0xe7f000f0;\n","filename":"src\/hotspot\/cpu\/arm\/nativeInst_arm_32.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -122,1 +122,1 @@\n-    CodeBlob* sender_blob = CodeCache::find_blob_unsafe(sender_pc);\n+    CodeBlob* sender_blob = CodeCache::find_blob(sender_pc);\n@@ -127,5 +127,0 @@\n-    \/\/ Could be a zombie method\n-    if (sender_blob->is_zombie() || sender_blob->is_unloaded()) {\n-      return false;\n-    }\n-\n","filename":"src\/hotspot\/cpu\/ppc\/frame_ppc.cpp","additions":1,"deletions":6,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -43,1 +43,1 @@\n-\/\/ We use an illtrap for marking a method as not_entrant or zombie\n+\/\/ We use an illtrap for marking a method as not_entrant\n@@ -45,1 +45,1 @@\n-bool NativeInstruction::is_sigill_zombie_not_entrant_at(address addr) {\n+bool NativeInstruction::is_sigill_not_entrant_at(address addr) {\n@@ -47,1 +47,1 @@\n-  CodeBlob* cb = CodeCache::find_blob_unsafe(addr);\n+  CodeBlob* cb = CodeCache::find_blob(addr);\n@@ -50,1 +50,1 @@\n-  \/\/ This method is not_entrant or zombie iff the illtrap instruction is\n+  \/\/ This method is not_entrant iff the illtrap instruction is\n@@ -74,1 +74,1 @@\n-    CodeBlob* cb = CodeCache::find_blob_unsafe(addr);   \/\/ Else we get assertion if nmethod is zombie.\n+    CodeBlob* cb = CodeCache::find_blob(addr);\n@@ -199,1 +199,1 @@\n-  CodeBlob* cb = CodeCache::find_blob_unsafe(addr);\n+  CodeBlob* cb = CodeCache::find_blob(addr);\n@@ -321,1 +321,1 @@\n-    CodeBlob* cb = CodeCache::find_blob_unsafe(addr);   \/\/ find_nmethod() asserts if nmethod is zombie.\n+    CodeBlob* cb = CodeCache::find_blob(addr);\n@@ -346,1 +346,1 @@\n-    \/\/ We use an illtrap for marking a method as not_entrant or zombie.\n+    \/\/ We use an illtrap for marking a method as not_entrant.\n@@ -409,1 +409,1 @@\n-  CodeBlob* cb = nm ? nm : CodeCache::find_blob_unsafe(addr_at(0));\n+  CodeBlob* cb = nm ? nm : CodeCache::find_blob(addr_at(0));\n","filename":"src\/hotspot\/cpu\/ppc\/nativeInst_ppc.cpp","additions":9,"deletions":9,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -70,2 +70,2 @@\n-  \/\/ We use an illtrap for marking a method as not_entrant or zombie.\n-  bool is_sigill_zombie_not_entrant() {\n+  \/\/ We use an illtrap for marking a method as not_entrant.\n+  bool is_sigill_not_entrant() {\n@@ -73,1 +73,1 @@\n-    return NativeInstruction::is_sigill_zombie_not_entrant_at(addr_at(0));\n+    return NativeInstruction::is_sigill_not_entrant_at(addr_at(0));\n@@ -75,1 +75,1 @@\n-  static bool is_sigill_zombie_not_entrant_at(address addr);\n+  static bool is_sigill_not_entrant_at(address addr);\n","filename":"src\/hotspot\/cpu\/ppc\/nativeInst_ppc.hpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -178,1 +178,1 @@\n-    CodeBlob* sender_blob = CodeCache::find_blob_unsafe(sender_pc);\n+    CodeBlob* sender_blob = CodeCache::find_blob(sender_pc);\n@@ -183,5 +183,0 @@\n-    \/\/ Could be a zombie method\n-    if (sender_blob->is_zombie() || sender_blob->is_unloaded()) {\n-      return false;\n-    }\n-\n","filename":"src\/hotspot\/cpu\/riscv\/frame_riscv.cpp","additions":1,"deletions":6,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -118,4 +118,2 @@\n-  \/\/ when last_Java_sp is non-null but the pc fetched is junk. If we are truly\n-  \/\/ unlucky the junk value could be to a zombied method and we'll die on the\n-  \/\/ find_blob call. This is also why we can have no asserts on the validity\n-  \/\/ of the pc we find here. AsyncGetCallTrace -> pd_get_top_frame_for_signal_handler\n+  \/\/ when last_Java_sp is non-null but the pc fetched is junk.\n+  \/\/ AsyncGetCallTrace -> pd_get_top_frame_for_signal_handler\n","filename":"src\/hotspot\/cpu\/riscv\/frame_riscv.inline.hpp","additions":2,"deletions":4,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -127,1 +127,1 @@\n-  CodeBlob* cb = CodeCache::find_blob_unsafe(addr);   \/\/ Else we get assertion if nmethod is zombie.\n+  CodeBlob* cb = CodeCache::find_blob(addr);\n@@ -331,1 +331,1 @@\n-bool NativeInstruction::is_sigill_zombie_not_entrant() {\n+bool NativeInstruction::is_sigill_not_entrant() {\n@@ -348,1 +348,1 @@\n-\/\/ nmethod::make_not_entrant_or_zombie)\n+\/\/ nmethod::make_not_entrant)\n@@ -355,1 +355,1 @@\n-         nativeInstruction_at(verified_entry)->is_sigill_zombie_not_entrant(),\n+         nativeInstruction_at(verified_entry)->is_sigill_not_entrant(),\n@@ -374,1 +374,1 @@\n-    \/\/ not_entrant or zombie.\n+    \/\/ not_entrant.\n","filename":"src\/hotspot\/cpu\/riscv\/nativeInst_riscv.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -202,1 +202,1 @@\n-  bool is_sigill_zombie_not_entrant();\n+  bool is_sigill_not_entrant();\n","filename":"src\/hotspot\/cpu\/riscv\/nativeInst_riscv.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -125,1 +125,1 @@\n-    CodeBlob* sender_blob = CodeCache::find_blob_unsafe(sender_pc);\n+    CodeBlob* sender_blob = CodeCache::find_blob(sender_pc);\n@@ -130,5 +130,0 @@\n-    \/\/ Could be a zombie method\n-    if (sender_blob->is_zombie() || sender_blob->is_unloaded()) {\n-      return false;\n-    }\n-\n@@ -427,1 +422,1 @@\n-      blob = CodeCache::find_blob_unsafe(current_pc);\n+      blob = CodeCache::find_blob(current_pc);\n","filename":"src\/hotspot\/cpu\/s390\/frame_s390.cpp","additions":2,"deletions":7,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -4487,1 +4487,1 @@\n-    CodeBlob* cb = CodeCache::find_blob_unsafe(pc);   \/\/ Else we get assertion if nmethod is zombie.\n+    CodeBlob* cb = CodeCache::find_blob(pc);\n","filename":"src\/hotspot\/cpu\/s390\/macroAssembler_s390.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -171,2 +171,2 @@\n-\/\/ We use an illtrap for marking a method as not_entrant or zombie.\n-bool NativeInstruction::is_sigill_zombie_not_entrant() {\n+\/\/ We use an illtrap for marking a method as not_entrant.\n+bool NativeInstruction::is_sigill_not_entrant() {\n@@ -178,1 +178,1 @@\n-  CodeBlob* cb = CodeCache::find_blob_unsafe(addr_at(0));\n+  CodeBlob* cb = CodeCache::find_blob(addr_at(0));\n@@ -184,1 +184,1 @@\n-  \/\/ This method is not_entrant or zombie if the illtrap instruction\n+  \/\/ This method is not_entrant if the illtrap instruction\n","filename":"src\/hotspot\/cpu\/s390\/nativeInst_s390.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -88,2 +88,2 @@\n-  \/\/ We use an illtrap for marking a method as not_entrant or zombie.\n-  bool is_sigill_zombie_not_entrant();\n+  \/\/ We use an illtrap for marking a method as not_entrant.\n+  bool is_sigill_not_entrant();\n","filename":"src\/hotspot\/cpu\/s390\/nativeInst_s390.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -130,1 +130,1 @@\n-  CodeBlob *cb = CodeCache::find_blob_unsafe((address) _call);\n+  CodeBlob *cb = CodeCache::find_blob((address) _call);\n","filename":"src\/hotspot\/cpu\/x86\/compiledIC_x86.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -180,1 +180,1 @@\n-    CodeBlob* sender_blob = CodeCache::find_blob_unsafe(sender_pc);\n+    CodeBlob* sender_blob = CodeCache::find_blob(sender_pc);\n@@ -185,5 +185,0 @@\n-    \/\/ Could be a zombie method\n-    if (sender_blob->is_zombie() || sender_blob->is_unloaded()) {\n-      return false;\n-    }\n-\n","filename":"src\/hotspot\/cpu\/x86\/frame_x86.cpp","additions":1,"deletions":6,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -156,4 +156,2 @@\n-  \/\/ when last_Java_sp is non-null but the pc fetched is junk. If we are truly\n-  \/\/ unlucky the junk value could be to a zombied method and we'll die on the\n-  \/\/ find_blob call. This is also why we can have no asserts on the validity\n-  \/\/ of the pc we find here. AsyncGetCallTrace -> pd_get_top_frame_for_signal_handler\n+  \/\/ when last_Java_sp is non-null but the pc fetched is junk.\n+  \/\/ AsyncGetCallTrace -> pd_get_top_frame_for_signal_handler\n","filename":"src\/hotspot\/cpu\/x86\/frame_x86.inline.hpp","additions":2,"deletions":4,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -498,1 +498,1 @@\n-\/\/ MT safe inserting of a jump over an unknown instruction sequence (used by nmethod::makeZombie)\n+\/\/ MT safe inserting of a jump over an unknown instruction sequence (used by nmethod::make_not_entrant)\n","filename":"src\/hotspot\/cpu\/x86\/nativeInst_x86.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -33,1 +33,1 @@\n-\/\/ This method is called by nmethod::make_not_entrant_or_zombie to\n+\/\/ This method is called by nmethod::make_not_entrant to\n","filename":"src\/hotspot\/cpu\/zero\/nativeInst_zero.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -638,1 +638,1 @@\n-      CodeBlob* cb = CodeCache::find_blob_unsafe(pc);\n+      CodeBlob* cb = CodeCache::find_blob(pc);\n","filename":"src\/hotspot\/os\/posix\/signals_posix.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2681,1 +2681,1 @@\n-        CodeBlob* cb = CodeCache::find_blob_unsafe(pc);\n+        CodeBlob* cb = CodeCache::find_blob(pc);\n@@ -2700,1 +2700,1 @@\n-      if (nativeInstruction_at(pc)->is_sigill_zombie_not_entrant()) {\n+      if (nativeInstruction_at(pc)->is_sigill_not_entrant()) {\n@@ -2702,1 +2702,1 @@\n-          tty->print_cr(\"trap: zombie_not_entrant\");\n+          tty->print_cr(\"trap: not_entrant\");\n@@ -2731,1 +2731,1 @@\n-        CodeBlob* cb = CodeCache::find_blob_unsafe(pc);\n+        CodeBlob* cb = CodeCache::find_blob(pc);\n","filename":"src\/hotspot\/os\/windows\/os_windows.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -196,1 +196,1 @@\n-      \/\/   Safepoints, Unreachable Code, Entry points of Zombie methods,\n+      \/\/   Safepoints, Unreachable Code, Entry points of not entrant nmethods,\n@@ -205,1 +205,1 @@\n-      \/\/     - zombie methods\n+      \/\/     - not entrant nmethods\n@@ -228,1 +228,1 @@\n-      if (sig == SIGILL && nativeInstruction_at(pc)->is_sigill_zombie_not_entrant()) {\n+      if (sig == SIGILL && nativeInstruction_at(pc)->is_sigill_not_entrant()) {\n@@ -230,1 +230,1 @@\n-          tty->print_cr(\"trap: zombie_not_entrant\");\n+          tty->print_cr(\"trap: not_entrant\");\n@@ -344,1 +344,1 @@\n-        CodeBlob* cb = CodeCache::find_blob_unsafe(pc);\n+        CodeBlob* cb = CodeCache::find_blob(pc);\n","filename":"src\/hotspot\/os_cpu\/aix_ppc\/os_aix_ppc.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -249,1 +249,1 @@\n-          && nativeInstruction_at(pc)->is_sigill_zombie_not_entrant()) {\n+          && nativeInstruction_at(pc)->is_sigill_not_entrant()) {\n@@ -251,1 +251,1 @@\n-          tty->print_cr(\"trap: zombie_not_entrant\");\n+          tty->print_cr(\"trap: not_entrant\");\n@@ -268,1 +268,1 @@\n-        CodeBlob* cb = CodeCache::find_blob_unsafe(pc);\n+        CodeBlob* cb = CodeCache::find_blob(pc);\n","filename":"src\/hotspot\/os_cpu\/bsd_aarch64\/os_bsd_aarch64.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -443,1 +443,1 @@\n-        CodeBlob* cb = CodeCache::find_blob_unsafe(pc);\n+        CodeBlob* cb = CodeCache::find_blob(pc);\n","filename":"src\/hotspot\/os_cpu\/bsd_x86\/os_bsd_x86.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -210,1 +210,1 @@\n-          && nativeInstruction_at(pc)->is_sigill_zombie_not_entrant()) {\n+          && nativeInstruction_at(pc)->is_sigill_not_entrant()) {\n@@ -212,1 +212,1 @@\n-          tty->print_cr(\"trap: zombie_not_entrant (%s)\", (sig == SIGTRAP) ? \"SIGTRAP\" : \"SIGILL\");\n+          tty->print_cr(\"trap: not_entrant (%s)\", (sig == SIGTRAP) ? \"SIGTRAP\" : \"SIGILL\");\n@@ -221,1 +221,1 @@\n-        CodeBlob* cb = CodeCache::find_blob_unsafe(pc);\n+        CodeBlob* cb = CodeCache::find_blob(pc);\n","filename":"src\/hotspot\/os_cpu\/linux_aarch64\/os_linux_aarch64.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -326,1 +326,1 @@\n-        CodeBlob* cb = CodeCache::find_blob_unsafe(pc);\n+        CodeBlob* cb = CodeCache::find_blob(pc);\n@@ -334,1 +334,1 @@\n-          CodeBlob* cb = CodeCache::find_blob_unsafe(pc);\n+          CodeBlob* cb = CodeCache::find_blob(pc);\n@@ -338,2 +338,2 @@\n-      } else if (sig == SIGILL && *(int *)pc == NativeInstruction::zombie_illegal_instruction) {\n-        \/\/ Zombie\n+      } else if (sig == SIGILL && *(int *)pc == NativeInstruction::not_entrant_illegal_instruction) {\n+        \/\/ Not entrant\n","filename":"src\/hotspot\/os_cpu\/linux_arm\/os_linux_arm.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -251,1 +251,1 @@\n-      if (sig == SIGILL && nativeInstruction_at(pc)->is_sigill_zombie_not_entrant()) {\n+      if (sig == SIGILL && nativeInstruction_at(pc)->is_sigill_not_entrant()) {\n@@ -253,1 +253,1 @@\n-          tty->print_cr(\"trap: zombie_not_entrant\");\n+          tty->print_cr(\"trap: not_entrant\");\n@@ -359,1 +359,1 @@\n-        CodeBlob* cb = CodeCache::find_blob_unsafe(pc);\n+        CodeBlob* cb = CodeCache::find_blob(pc);\n","filename":"src\/hotspot\/os_cpu\/linux_ppc\/os_linux_ppc.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -211,1 +211,1 @@\n-          && nativeInstruction_at(pc)->is_sigill_zombie_not_entrant()) {\n+          && nativeInstruction_at(pc)->is_sigill_not_entrant()) {\n@@ -213,1 +213,1 @@\n-          tty->print_cr(\"trap: zombie_not_entrant (%s)\", (sig == SIGTRAP) ? \"SIGTRAP\" : \"SIGILL\");\n+          tty->print_cr(\"trap: not_entrant (%s)\", (sig == SIGTRAP) ? \"SIGTRAP\" : \"SIGILL\");\n@@ -222,1 +222,1 @@\n-        CodeBlob* cb = CodeCache::find_blob_unsafe(pc);\n+        CodeBlob* cb = CodeCache::find_blob(pc);\n","filename":"src\/hotspot\/os_cpu\/linux_riscv\/os_linux_riscv.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -245,1 +245,1 @@\n-      if (sig == SIGILL && nativeInstruction_at(pc)->is_sigill_zombie_not_entrant()) {\n+      if (sig == SIGILL && nativeInstruction_at(pc)->is_sigill_not_entrant()) {\n@@ -247,1 +247,1 @@\n-          tty->print_cr(\"trap: zombie_not_entrant (SIGILL)\");\n+          tty->print_cr(\"trap: not_entrant (SIGILL)\");\n@@ -305,1 +305,1 @@\n-        CodeBlob* cb = CodeCache::find_blob_unsafe(pc);\n+        CodeBlob* cb = CodeCache::find_blob(pc);\n","filename":"src\/hotspot\/os_cpu\/linux_s390\/os_linux_s390.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -260,1 +260,1 @@\n-        CodeBlob* cb = CodeCache::find_blob_unsafe(pc);\n+        CodeBlob* cb = CodeCache::find_blob(pc);\n","filename":"src\/hotspot\/os_cpu\/linux_x86\/os_linux_x86.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -265,3 +265,0 @@\n-  bool age_code() const {\n-    return _method->profile_aging();\n-  }\n","filename":"src\/hotspot\/share\/c1\/c1_Compilation.hpp","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -2685,4 +2685,0 @@\n-  if (compilation()->age_code()) {\n-    CodeEmitInfo* info = new CodeEmitInfo(scope()->start()->state()->copy(ValueStack::StateBefore, 0), NULL, false);\n-    decrement_age(info);\n-  }\n@@ -3255,21 +3251,0 @@\n-void LIRGenerator::decrement_age(CodeEmitInfo* info) {\n-  ciMethod* method = info->scope()->method();\n-  MethodCounters* mc_adr = method->ensure_method_counters();\n-  if (mc_adr != NULL) {\n-    LIR_Opr mc = new_pointer_register();\n-    __ move(LIR_OprFact::intptrConst(mc_adr), mc);\n-    int offset = in_bytes(MethodCounters::nmethod_age_offset());\n-    LIR_Address* counter = new LIR_Address(mc, offset, T_INT);\n-    LIR_Opr result = new_register(T_INT);\n-    __ load(counter, result);\n-    __ sub(result, LIR_OprFact::intConst(1), result);\n-    __ store(result, counter);\n-    \/\/ DeoptimizeStub will reexecute from the current state in code info.\n-    CodeStub* deopt = new DeoptimizeStub(info, Deoptimization::Reason_tenured,\n-                                         Deoptimization::Action_make_not_entrant);\n-    __ cmp(lir_cond_lessEqual, result, LIR_OprFact::intConst(0));\n-    __ branch(lir_cond_lessEqual, deopt);\n-  }\n-}\n-\n-\n","filename":"src\/hotspot\/share\/c1\/c1_LIRGenerator.cpp","additions":0,"deletions":25,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -421,1 +421,0 @@\n-  void decrement_age(CodeEmitInfo* info);\n","filename":"src\/hotspot\/share\/c1\/c1_LIRGenerator.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -45,0 +45,1 @@\n+#include \"compiler\/compilationLog.hpp\"\n@@ -1074,0 +1075,3 @@\n+    \/\/ Check if memory should be freed before allocation\n+    CodeCache::gc_on_allocation();\n+\n@@ -1165,6 +1169,0 @@\n-      \/\/ Record successful registration.\n-      \/\/ (Put nm into the task handle *before* publishing to the Java heap.)\n-      if (task() != NULL) {\n-        task()->set_code(nm);\n-      }\n-\n@@ -1211,1 +1209,1 @@\n-  }  \/\/ safepoints are allowed again\n+  }\n@@ -1213,0 +1211,1 @@\n+  NoSafepointVerifier nsv;\n@@ -1214,2 +1213,3 @@\n-    \/\/ JVMTI -- compiled method notification (must be done outside lock)\n-    nm->post_compiled_method_load_event();\n+    \/\/ Compilation succeeded, post what we know about it\n+    nm->post_compiled_method(task());\n+    task()->set_num_inlined_bytecodes(num_inlined_bytecodes());\n@@ -1220,0 +1220,2 @@\n+\n+  \/\/ safepoints are allowed again\n","filename":"src\/hotspot\/share\/ci\/ciEnv.cpp","additions":11,"deletions":9,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -399,1 +399,0 @@\n-\n","filename":"src\/hotspot\/share\/ci\/ciEnv.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -144,1 +144,0 @@\n-  _nmethod_age = h_m->nmethod_age();\n@@ -1212,9 +1211,0 @@\n-\n-\/\/ ------------------------------------------------------------------\n-\/\/ ciMethod::profile_aging\n-\/\/\n-\/\/ Should the method be compiled with an age counter?\n-bool ciMethod::profile_aging() const {\n-  return UseCodeAging && (!MethodCounters::is_nmethod_hot(nmethod_age()) &&\n-                          !MethodCounters::is_nmethod_age_unset(nmethod_age()));\n-}\n","filename":"src\/hotspot\/share\/ci\/ciMethod.cpp","additions":0,"deletions":10,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -83,1 +83,0 @@\n-  int _nmethod_age;\n@@ -194,4 +193,0 @@\n-  int nmethod_age() const                        { check_is_loaded(); return _nmethod_age; }\n-\n-  \/\/ Should the method be compiled with an age counter?\n-  bool profile_aging() const;\n","filename":"src\/hotspot\/share\/ci\/ciMethod.hpp","additions":0,"deletions":5,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -214,7 +214,0 @@\n-  \/\/ CodeCache support: really only used by the nmethods, but in order to get\n-  \/\/ asserts and certain bookkeeping to work in the CodeCache they are defined\n-  \/\/ virtual here.\n-  virtual bool is_zombie() const                 { return false; }\n-  virtual bool is_locked_by_vm() const           { return false; }\n-\n-  virtual bool is_unloaded() const               { return false; }\n@@ -223,3 +216,0 @@\n-  \/\/ GC support\n-  virtual bool is_alive() const                  = 0;\n-\n@@ -387,3 +377,0 @@\n-  \/\/ GC support\n-  virtual bool is_alive() const                  = 0;\n-\n@@ -438,1 +425,0 @@\n-  bool is_alive() const                          { return true; }\n@@ -535,1 +521,0 @@\n-  bool is_alive() const                          { return true; }\n@@ -570,2 +555,0 @@\n-  bool is_alive() const                          { return true; }\n-\n@@ -804,1 +787,0 @@\n-  virtual bool is_alive() const override { return true; }\n","filename":"src\/hotspot\/share\/code\/codeBlob.hpp","additions":0,"deletions":18,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -58,0 +58,1 @@\n+#include \"runtime\/init.hpp\"\n@@ -62,1 +63,0 @@\n-#include \"runtime\/sweeper.hpp\"\n@@ -172,3 +172,0 @@\n-int CodeCache::Sweep::_compiled_method_iterators = 0;\n-bool CodeCache::Sweep::_pending_sweep = false;\n-\n@@ -483,34 +480,0 @@\n-void CodeCache::Sweep::begin_compiled_method_iteration() {\n-  MutexLocker ml(CodeCache_lock, Mutex::_no_safepoint_check_flag);\n-  \/\/ Reach a state without concurrent sweeping\n-  while (_compiled_method_iterators < 0) {\n-    CodeCache_lock->wait_without_safepoint_check();\n-  }\n-  _compiled_method_iterators++;\n-}\n-\n-void CodeCache::Sweep::end_compiled_method_iteration() {\n-  MutexLocker ml(CodeCache_lock, Mutex::_no_safepoint_check_flag);\n-  \/\/ Let the sweeper run again, if we stalled it\n-  _compiled_method_iterators--;\n-  if (_pending_sweep) {\n-    CodeCache_lock->notify_all();\n-  }\n-}\n-\n-void CodeCache::Sweep::begin() {\n-  assert_locked_or_safepoint(CodeCache_lock);\n-  _pending_sweep = true;\n-  while (_compiled_method_iterators > 0) {\n-    CodeCache_lock->wait_without_safepoint_check();\n-  }\n-  _pending_sweep = false;\n-  _compiled_method_iterators = -1;\n-}\n-\n-void CodeCache::Sweep::end() {\n-  assert_locked_or_safepoint(CodeCache_lock);\n-  _compiled_method_iterators = 0;\n-  CodeCache_lock->notify_all();\n-}\n-\n@@ -545,2 +508,0 @@\n-  \/\/ Possibly wakes up the sweeper thread.\n-  NMethodSweeper::report_allocation();\n@@ -570,2 +531,0 @@\n-        \/\/ Note that in the sweeper, we check the reverse_free_ratio of the code heap\n-        \/\/ and force stack scanning if less than 10% of the entire code cache are free.\n@@ -689,7 +648,1 @@\n-static bool is_in_asgct() {\n-  Thread* current_thread = Thread::current_or_null_safe();\n-  return current_thread != NULL && current_thread->is_Java_thread() && JavaThread::cast(current_thread)->in_asgct();\n-}\n-\n-\/\/ This method is safe to call without holding the CodeCache_lock, as long as a dead CodeBlob is not\n-\/\/ looked up (i.e., one that has been marked for deletion). It only depends on the _segmap to contain\n+\/\/ This method is safe to call without holding the CodeCache_lock. It only depends on the _segmap to contain\n@@ -698,13 +651,0 @@\n-  CodeBlob* result = find_blob_unsafe(start);\n-  \/\/ We could potentially look up non_entrant methods\n-  bool is_zombie = result != NULL && result->is_zombie();\n-  bool is_result_safe = !is_zombie || result->is_locked_by_vm() || VMError::is_error_reported();\n-  guarantee(is_result_safe || is_in_asgct(), \"unsafe access to zombie method\");\n-  \/\/ When in ASGCT the previous gurantee will pass for a zombie method but we still don't want that code blob returned in order\n-  \/\/ to minimize the chance of accessing dead memory\n-  return is_result_safe ? result : NULL;\n-}\n-\n-\/\/ Lookup that does not fail if you lookup a zombie method (if you call this, be sure to know\n-\/\/ what you are doing)\n-CodeBlob* CodeCache::find_blob_unsafe(void* start) {\n@@ -715,1 +655,1 @@\n-      return heap->find_blob_unsafe(start);\n+      return heap->find_blob(start);\n@@ -746,1 +686,1 @@\n-  NMethodIterator iter(NMethodIterator::only_alive);\n+  NMethodIterator iter(NMethodIterator::all_blobs);\n@@ -760,0 +700,178 @@\n+\/\/ Calculate the number of GCs after which an nmethod is expected to have been\n+\/\/ used in order to not be classed as cold.\n+void CodeCache::update_cold_gc_count() {\n+  if (!MethodFlushing || !UseCodeCacheFlushing || NmethodSweepActivity == 0) {\n+    \/\/ No aging\n+    return;\n+  }\n+\n+  size_t last_used = _last_unloading_used;\n+  double last_time = _last_unloading_time;\n+\n+  double time = os::elapsedTime();\n+\n+  size_t free = unallocated_capacity();\n+  size_t max = max_capacity();\n+  size_t used = max - free;\n+  double gc_interval = time - last_time;\n+\n+  _unloading_threshold_gc_requested = false;\n+  _last_unloading_time = time;\n+  _last_unloading_used = used;\n+\n+  if (last_time == 0.0) {\n+    \/\/ The first GC doesn't have enough information to make good\n+    \/\/ decisions, so just keep everything afloat\n+    log_info(codecache)(\"Unknown code cache pressure; don't age code\");\n+    return;\n+  }\n+\n+  if (gc_interval <= 0.0 || last_used >= used) {\n+    \/\/ Dodge corner cases where there is no pressure or negative pressure\n+    \/\/ on the code cache. Just don't unload when this happens.\n+    _cold_gc_count = INT_MAX;\n+    log_info(codecache)(\"No code cache pressure; don't age code\");\n+    return;\n+  }\n+\n+  double allocation_rate = (used - last_used) \/ gc_interval;\n+\n+  _unloading_allocation_rates.add(allocation_rate);\n+  _unloading_gc_intervals.add(gc_interval);\n+\n+  size_t aggressive_sweeping_free_threshold = StartAggressiveSweepingAt \/ 100.0 * max;\n+  if (free < aggressive_sweeping_free_threshold) {\n+    \/\/ We are already in the red zone; be very aggressive to avoid disaster\n+    \/\/ But not more aggressive than 2. This ensures that an nmethod must\n+    \/\/ have been unused at least between two GCs to be considered cold still.\n+    _cold_gc_count = 2;\n+    log_info(codecache)(\"Code cache critically low; use aggressive aging\");\n+    return;\n+  }\n+\n+  \/\/ The code cache has an expected time for cold nmethods to \"time out\"\n+  \/\/ when they have not been used. The time for nmethods to time out\n+  \/\/ depends on how long we expect we can keep allocating code until\n+  \/\/ aggressive sweeping starts, based on sampled allocation rates.\n+  double average_gc_interval = _unloading_gc_intervals.avg();\n+  double average_allocation_rate = _unloading_allocation_rates.avg();\n+  double time_to_aggressive = ((double)(free - aggressive_sweeping_free_threshold)) \/ average_allocation_rate;\n+  double cold_timeout = time_to_aggressive \/ NmethodSweepActivity;\n+\n+  \/\/ Convert time to GC cycles, and crop at INT_MAX. The reason for\n+  \/\/ that is that the _cold_gc_count will be added to an epoch number\n+  \/\/ and that addition must not overflow, or we can crash the VM.\n+  \/\/ But not more aggressive than 2. This ensures that an nmethod must\n+  \/\/ have been unused at least between two GCs to be considered cold still.\n+  _cold_gc_count = MAX2(MIN2((uint64_t)(cold_timeout \/ average_gc_interval), (uint64_t)INT_MAX), (uint64_t)2);\n+\n+  double used_ratio = double(used) \/ double(max);\n+  double last_used_ratio = double(last_used) \/ double(max);\n+  log_info(codecache)(\"Allocation rate: %.3f KB\/s, time to aggressive unloading: %.3f s, cold timeout: %.3f s, cold gc count: \" UINT64_FORMAT\n+                      \", used: %.3f MB (%.3f%%), last used: %.3f MB (%.3f%%), gc interval: %.3f s\",\n+                      average_allocation_rate \/ K, time_to_aggressive, cold_timeout, _cold_gc_count,\n+                      double(used) \/ M, used_ratio * 100.0, double(last_used) \/ M, last_used_ratio * 100.0, average_gc_interval);\n+\n+}\n+\n+uint64_t CodeCache::cold_gc_count() {\n+  return _cold_gc_count;\n+}\n+\n+void CodeCache::gc_on_allocation() {\n+  if (!is_init_completed()) {\n+    \/\/ Let's not heuristically trigger GCs before the JVM is ready for GCs, no matter what\n+    return;\n+  }\n+\n+  size_t free = unallocated_capacity();\n+  size_t max = max_capacity();\n+  size_t used = max - free;\n+  double free_ratio = double(free) \/ double(max);\n+  if (free_ratio <= StartAggressiveSweepingAt \/ 100.0)  {\n+    \/\/ In case the GC is concurrent, we make sure only one thread requests the GC.\n+    if (Atomic::cmpxchg(&_unloading_threshold_gc_requested, false, true) == false) {\n+      log_info(codecache)(\"Triggering aggressive GC due to having only %.3f%% free memory\", free_ratio * 100.0);\n+      Universe::heap()->collect(GCCause::_codecache_GC_aggressive);\n+    }\n+    return;\n+  }\n+\n+  size_t last_used = _last_unloading_used;\n+  if (last_used >= used) {\n+    \/\/ No increase since last GC; no need to sweep yet\n+    return;\n+  }\n+  size_t allocated_since_last = used - last_used;\n+  double allocated_since_last_ratio = double(allocated_since_last) \/ double(max);\n+  double threshold = SweeperThreshold \/ 100.0;\n+  double used_ratio = double(used) \/ double(max);\n+  double last_used_ratio = double(last_used) \/ double(max);\n+  if (used_ratio > threshold) {\n+    \/\/ After threshold is reached, scale it by free_ratio so that more aggressive\n+    \/\/ GC is triggered as we approach code cache exhaustion\n+    threshold *= free_ratio;\n+  }\n+  \/\/ If code cache has been allocated without any GC at all, let's make sure\n+  \/\/ it is eventually invoked to avoid trouble.\n+  if (allocated_since_last_ratio > threshold) {\n+    \/\/ In case the GC is concurrent, we make sure only one thread requests the GC.\n+    if (Atomic::cmpxchg(&_unloading_threshold_gc_requested, false, true) == false) {\n+      log_info(codecache)(\"Triggering threshold (%.3f%%) GC due to allocating %.3f%% since last unloading (%.3f%% used -> %.3f%% used)\",\n+                          threshold * 100.0, allocated_since_last_ratio * 100.0, last_used_ratio * 100.0, used_ratio * 100.0);\n+      Universe::heap()->collect(GCCause::_codecache_GC_threshold);\n+    }\n+  }\n+}\n+\n+\/\/ We initialize the _gc_epoch to 2, because previous_completed_gc_marking_cycle\n+\/\/ subtracts the value by 2, and the type is unsigned. We don't want underflow.\n+\/\/\n+\/\/ Odd values mean that marking is in progress, and even values mean that no\n+\/\/ marking is currently active.\n+uint64_t CodeCache::_gc_epoch = 2;\n+\n+\/\/ How many GCs after an nmethod has not been used, do we consider it cold?\n+uint64_t CodeCache::_cold_gc_count = INT_MAX;\n+\n+double CodeCache::_last_unloading_time = 0.0;\n+size_t CodeCache::_last_unloading_used = 0;\n+volatile bool CodeCache::_unloading_threshold_gc_requested = false;\n+TruncatedSeq CodeCache::_unloading_gc_intervals(10 \/* samples *\/);\n+TruncatedSeq CodeCache::_unloading_allocation_rates(10 \/* samples *\/);\n+\n+uint64_t CodeCache::gc_epoch() {\n+  return _gc_epoch;\n+}\n+\n+bool CodeCache::is_gc_marking_cycle_active() {\n+  \/\/ Odd means that marking is active\n+  return (_gc_epoch % 2) == 1;\n+}\n+\n+uint64_t CodeCache::previous_completed_gc_marking_cycle() {\n+  if (is_gc_marking_cycle_active()) {\n+    return _gc_epoch - 2;\n+  } else {\n+    return _gc_epoch - 1;\n+  }\n+}\n+\n+void CodeCache::on_gc_marking_cycle_start() {\n+  assert(!is_gc_marking_cycle_active(), \"Previous marking cycle never ended\");\n+  ++_gc_epoch;\n+}\n+\n+void CodeCache::on_gc_marking_cycle_finish() {\n+  assert(is_gc_marking_cycle_active(), \"Marking cycle started before last one finished\");\n+  ++_gc_epoch;\n+  update_cold_gc_count();\n+}\n+\n+void CodeCache::arm_all_nmethods() {\n+  BarrierSetNMethod* bs_nm = BarrierSet::barrier_set()->barrier_set_nmethod();\n+  if (bs_nm != NULL) {\n+    bs_nm->arm_all_nmethods();\n+  }\n+}\n+\n@@ -764,1 +882,1 @@\n-  CompiledMethodIterator iter(CompiledMethodIterator::only_alive);\n+  CompiledMethodIterator iter(CompiledMethodIterator::all_blobs);\n@@ -774,2 +892,1 @@\n-      if (cb->is_alive()) {\n-        f->do_code_blob(cb);\n+      f->do_code_blob(cb);\n@@ -777,4 +894,2 @@\n-        if (cb->is_nmethod()) {\n-          Universe::heap()->verify_nmethod((nmethod*)cb);\n-        }\n-#endif \/\/ASSERT\n+      if (cb->is_nmethod()) {\n+        Universe::heap()->verify_nmethod((nmethod*)cb);\n@@ -782,0 +897,1 @@\n+#endif \/\/ASSERT\n@@ -788,1 +904,1 @@\n-  NMethodIterator iter(NMethodIterator::only_alive_and_not_unloading);\n+  NMethodIterator iter(NMethodIterator::only_not_unloading);\n@@ -791,1 +907,0 @@\n-    assert(!nm->is_unloaded(), \"Tautology\");\n@@ -843,0 +958,42 @@\n+\/\/ Register an is_unloading nmethod to be flushed after unlinking\n+void CodeCache::register_unlinked(nmethod* nm) {\n+  assert(nm->unlinked_next() == NULL, \"Only register for unloading once\");\n+  for (;;) {\n+    \/\/ Only need acquire when reading the head, when the next\n+    \/\/ pointer is walked, which it is not here.\n+    nmethod* head = Atomic::load(&_unlinked_head);\n+    nmethod* next = head != NULL ? head : nm; \/\/ Self looped means end of list\n+    nm->set_unlinked_next(next);\n+    if (Atomic::cmpxchg(&_unlinked_head, head, nm) == head) {\n+      break;\n+    }\n+  }\n+}\n+\n+\/\/ Flush all the nmethods the GC unlinked\n+void CodeCache::flush_unlinked_nmethods() {\n+  nmethod* nm = _unlinked_head;\n+  _unlinked_head = NULL;\n+  size_t freed_memory = 0;\n+  while (nm != NULL) {\n+    nmethod* next = nm->unlinked_next();\n+    freed_memory += nm->total_size();\n+    nm->flush();\n+    if (next == nm) {\n+      \/\/ Self looped means end of list\n+      break;\n+    }\n+    nm = next;\n+  }\n+\n+  \/\/ Try to start the compiler again if we freed any memory\n+  if (!CompileBroker::should_compile_new_jobs() && freed_memory != 0) {\n+    CompileBroker::set_should_compile_new_jobs(CompileBroker::run_compilation);\n+    log_info(codecache)(\"Restarting compiler\");\n+    EventJitRestart event;\n+    event.set_freedMemory(freed_memory);\n+    event.set_codeCacheMaxCapacity(CodeCache::max_capacity());\n+    event.commit();\n+  }\n+}\n+\n@@ -844,0 +1001,1 @@\n+nmethod* volatile CodeCache::_unlinked_head = NULL;\n@@ -866,0 +1024,1 @@\n+  CodeCache::flush_unlinked_nmethods();\n@@ -871,1 +1030,1 @@\n-  NMethodIterator iter(NMethodIterator::only_alive_and_not_unloading);\n+  NMethodIterator iter(NMethodIterator::only_not_unloading);\n@@ -1060,1 +1219,1 @@\n-  CompiledMethodIterator iter(CompiledMethodIterator::only_alive_and_not_unloading);\n+  CompiledMethodIterator iter(CompiledMethodIterator::only_not_unloading);\n@@ -1066,1 +1225,2 @@\n-void CodeCache::cleanup_inline_caches() {\n+\/\/ Only used by whitebox API\n+void CodeCache::cleanup_inline_caches_whitebox() {\n@@ -1068,1 +1228,1 @@\n-  NMethodIterator iter(NMethodIterator::only_alive_and_not_unloading);\n+  NMethodIterator iter(NMethodIterator::only_not_unloading);\n@@ -1070,1 +1230,1 @@\n-    iter.method()->cleanup_inline_caches(\/*clean_all=*\/true);\n+    iter.method()->cleanup_inline_caches_whitebox();\n@@ -1132,1 +1292,1 @@\n-\/\/ Remove this method when zombied or unloaded.\n+\/\/ Remove this method when flushed.\n@@ -1150,2 +1310,2 @@\n-      \/\/ Only walk alive nmethods, the dead ones will get removed by the sweeper or GC.\n-      if (cm->is_alive() && !cm->is_unloading()) {\n+      \/\/ Only walk !is_unloading nmethods, the other ones will get removed by the GC.\n+      if (!cm->is_unloading()) {\n@@ -1167,1 +1327,1 @@\n-  CompiledMethodIterator iter(CompiledMethodIterator::only_alive);\n+  CompiledMethodIterator iter(CompiledMethodIterator::all_blobs);\n@@ -1187,1 +1347,1 @@\n-  CompiledMethodIterator iter(CompiledMethodIterator::only_alive);\n+  CompiledMethodIterator iter(CompiledMethodIterator::all_blobs);\n@@ -1219,1 +1379,1 @@\n-  CompiledMethodIterator iter(CompiledMethodIterator::only_alive_and_not_unloading);\n+  CompiledMethodIterator iter(CompiledMethodIterator::only_not_unloading);\n@@ -1232,1 +1392,1 @@\n-  CompiledMethodIterator iter(CompiledMethodIterator::only_alive_and_not_unloading);\n+  CompiledMethodIterator iter(CompiledMethodIterator::only_not_unloading);\n@@ -1246,1 +1406,1 @@\n-  SweeperBlockingCompiledMethodIterator iter(SweeperBlockingCompiledMethodIterator::only_alive_and_not_unloading);\n+  RelaxedCompiledMethodIterator iter(RelaxedCompiledMethodIterator::only_not_unloading);\n@@ -1301,3 +1461,1 @@\n-      if (cb->is_alive()) {\n-        cb->verify();\n-      }\n+      cb->verify();\n@@ -1417,1 +1575,0 @@\n-  int nmethodAlive = 0;\n@@ -1419,2 +1576,0 @@\n-  int nmethodZombie = 0;\n-  int nmethodUnloaded = 0;\n@@ -1440,1 +1595,0 @@\n-          if(nm->is_alive()) { tty->print_cr(\" alive\"); }\n@@ -1442,1 +1596,0 @@\n-          if(nm->is_zombie()) { tty->print_cr(\" zombie\"); }\n@@ -1447,1 +1600,0 @@\n-        if(nm->is_alive()) { nmethodAlive++; }\n@@ -1449,2 +1601,0 @@\n-        if(nm->is_zombie()) { nmethodZombie++; }\n-        if(nm->is_unloaded()) { nmethodUnloaded++; }\n@@ -1487,1 +1637,0 @@\n-  tty->print_cr(\"\\talive: %d\",nmethodAlive);\n@@ -1489,2 +1638,0 @@\n-  tty->print_cr(\"\\tzombie: %d\",nmethodZombie);\n-  tty->print_cr(\"\\tunloaded: %d\",nmethodUnloaded);\n@@ -1498,1 +1645,1 @@\n-  tty->print_cr(\"\\nnmethod size distribution (non-zombie java)\");\n+  tty->print_cr(\"\\nnmethod size distribution\");\n@@ -1522,1 +1669,0 @@\n-  CodeBlob_sizes dead[CompLevel_full_optimization + 1];\n@@ -1535,5 +1681,1 @@\n-        if (!cb->is_alive()) {\n-          dead[level].add(cb);\n-        } else {\n-          live[level].add(cb);\n-        }\n+        live[level].add(cb);\n@@ -1571,1 +1713,0 @@\n-    dead[i].print(\"dead\");\n@@ -1598,8 +1739,6 @@\n-        if (cb->is_alive()) {\n-          number_of_blobs++;\n-          code_size += cb->code_size();\n-          ImmutableOopMapSet* set = cb->oop_maps();\n-          if (set != NULL) {\n-            number_of_oop_maps += set->count();\n-            map_size           += set->nr_of_bytes();\n-          }\n+        number_of_blobs++;\n+        code_size += cb->code_size();\n+        ImmutableOopMapSet* set = cb->oop_maps();\n+        if (set != NULL) {\n+          number_of_oop_maps += set->count();\n+          map_size           += set->nr_of_bytes();\n@@ -1662,1 +1801,1 @@\n-  CompiledMethodIterator iter(CompiledMethodIterator::only_alive_and_not_unloading);\n+  CompiledMethodIterator iter(CompiledMethodIterator::only_not_unloading);\n@@ -1701,1 +1840,1 @@\n-  AllCodeBlobsIterator iter(AllCodeBlobsIterator::only_alive_and_not_unloading);\n+  AllCodeBlobsIterator iter(AllCodeBlobsIterator::only_not_unloading);\n","filename":"src\/hotspot\/share\/code\/codeCache.cpp","additions":257,"deletions":118,"binary":false,"changes":375,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"utilities\/numberSeq.hpp\"\n@@ -98,1 +99,10 @@\n-  static uint8_t _unloading_cycle;                      \/\/ Global state for recognizing old nmethods that need to be unloaded\n+\n+  static uint8_t           _unloading_cycle;          \/\/ Global state for recognizing old nmethods that need to be unloaded\n+  static uint64_t          _gc_epoch;                 \/\/ Global state for tracking when nmethods were found to be on-stack\n+  static uint64_t          _cold_gc_count;            \/\/ Global state for determining how many GCs are needed before an nmethod is cold\n+  static size_t            _last_unloading_used;\n+  static double            _last_unloading_time;\n+  static TruncatedSeq      _unloading_gc_intervals;\n+  static TruncatedSeq      _unloading_allocation_rates;\n+  static volatile bool     _unloading_threshold_gc_requested;\n+  static nmethod* volatile _unlinked_head;\n@@ -119,15 +129,0 @@\n- public:\n-\n-  class Sweep {\n-    friend class CodeCache;\n-    template <class T, class Filter, bool is_compiled_method> friend class CodeBlobIterator;\n-  private:\n-    static int _compiled_method_iterators;\n-    static bool _pending_sweep;\n-  public:\n-    static void begin();\n-    static void end();\n-  private:\n-    static void begin_compiled_method_iteration();\n-    static void end_compiled_method_iteration();\n-  };\n@@ -171,1 +166,0 @@\n-  static CodeBlob* find_blob_unsafe(void* start);       \/\/ Same as find_blob but does not fail if looking up a zombie method\n@@ -200,0 +194,16 @@\n+  \/\/ Code cache unloading heuristics\n+  static uint64_t cold_gc_count();\n+  static void update_cold_gc_count();\n+  static void gc_on_allocation();\n+\n+  \/\/ The GC epoch and marking_cycle code below is there to support sweeping\n+  \/\/ nmethods in loom stack chunks.\n+  static uint64_t gc_epoch();\n+  static bool is_gc_marking_cycle_active();\n+  static uint64_t previous_completed_gc_marking_cycle();\n+  static void on_gc_marking_cycle_start();\n+  static void on_gc_marking_cycle_finish();\n+  static void arm_all_nmethods();\n+\n+  static void flush_unlinked_nmethods();\n+  static void register_unlinked(nmethod* nm);\n@@ -242,1 +252,1 @@\n-  static void cleanup_inline_caches();                \/\/ clean unloaded\/zombie nmethods from inline caches\n+  static void cleanup_inline_caches_whitebox();       \/\/ clean bad nmethods from inline caches\n@@ -331,1 +341,2 @@\n-template <class T, class Filter, bool is_compiled_method> class CodeBlobIterator : public StackObj {\n+\/\/ The relaxed iterators only hold the CodeCache_lock across next calls\n+template <class T, class Filter, bool is_relaxed> class CodeBlobIterator : public StackObj {\n@@ -333,1 +344,1 @@\n-  enum LivenessFilter { all_blobs, only_alive, only_alive_and_not_unloading };\n+  enum LivenessFilter { all_blobs, only_not_unloading };\n@@ -339,1 +350,0 @@\n-  bool _only_alive;\n@@ -343,13 +353,0 @@\n-    if (Filter::heaps() == NULL) {\n-      return;\n-    }\n-    _heap = Filter::heaps()->begin();\n-    _end = Filter::heaps()->end();\n-    \/\/ If set to NULL, initialized by first call to next()\n-    _code_blob = (CodeBlob*)nm;\n-    if (nm != NULL) {\n-      while(!(*_heap)->contains_blob(_code_blob)) {\n-        ++_heap;\n-      }\n-      assert((*_heap)->contains_blob(_code_blob), \"match not found\");\n-    }\n@@ -369,5 +366,0 @@\n-      \/\/ Filter is_alive as required\n-      if (_only_alive && !_code_blob->is_alive()) {\n-        continue;\n-      }\n-\n@@ -388,2 +380,1 @@\n-    : _only_alive(filter == only_alive || filter == only_alive_and_not_unloading),\n-      _only_not_unloading(filter == only_alive_and_not_unloading)\n+    : _only_not_unloading(filter == only_not_unloading)\n@@ -391,5 +382,2 @@\n-    if (is_compiled_method) {\n-      CodeCache::Sweep::begin_compiled_method_iteration();\n-      initialize_iteration(nm);\n-    } else {\n-      initialize_iteration(nm);\n+    if (Filter::heaps() == NULL) {\n+      return;\n@@ -397,5 +385,9 @@\n-  }\n-\n-  ~CodeBlobIterator() {\n-    if (is_compiled_method) {\n-      CodeCache::Sweep::end_compiled_method_iteration();\n+    _heap = Filter::heaps()->begin();\n+    _end = Filter::heaps()->end();\n+    \/\/ If set to NULL, initialized by first call to next()\n+    _code_blob = nm;\n+    if (nm != NULL) {\n+      while(!(*_heap)->contains_blob(_code_blob)) {\n+        ++_heap;\n+      }\n+      assert((*_heap)->contains_blob(_code_blob), \"match not found\");\n@@ -407,1 +399,1 @@\n-    if (is_compiled_method) {\n+    if (is_relaxed) {\n@@ -461,5 +453,4 @@\n-typedef CodeBlobIterator<CompiledMethod, CompiledMethodFilter, false \/* is_compiled_method *\/> CompiledMethodIterator;\n-typedef CodeBlobIterator<nmethod, NMethodFilter, false \/* is_compiled_method *\/> NMethodIterator;\n-typedef CodeBlobIterator<CodeBlob, AllCodeBlobsFilter, false \/* is_compiled_method *\/> AllCodeBlobsIterator;\n-\n-typedef CodeBlobIterator<CompiledMethod, CompiledMethodFilter, true \/* is_compiled_method *\/> SweeperBlockingCompiledMethodIterator;\n+typedef CodeBlobIterator<CompiledMethod, CompiledMethodFilter, false \/* is_relaxed *\/> CompiledMethodIterator;\n+typedef CodeBlobIterator<CompiledMethod, CompiledMethodFilter, true \/* is_relaxed *\/> RelaxedCompiledMethodIterator;\n+typedef CodeBlobIterator<nmethod, NMethodFilter, false \/* is_relaxed *\/> NMethodIterator;\n+typedef CodeBlobIterator<CodeBlob, AllCodeBlobsFilter, false \/* is_relaxed *\/> AllCodeBlobsIterator;\n","filename":"src\/hotspot\/share\/code\/codeCache.hpp","additions":48,"deletions":57,"binary":false,"changes":105,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"runtime\/mutexLocker.hpp\"\n@@ -31,1 +32,0 @@\n-#include \"runtime\/sweeper.hpp\"\n@@ -219,12 +219,10 @@\n-                             ,                         \"nMethod (zombie)\"\n-                             ,                              \"nMethod (unloaded)\"\n-                             ,                                   \"runtime stub\"\n-                             ,                                        \"ricochet stub\"\n-                             ,                                             \"deopt stub\"\n-                             ,                                                  \"uncommon trap stub\"\n-                             ,                                                       \"exception stub\"\n-                             ,                                                            \"safepoint stub\"\n-                             ,                                                                 \"adapter blob\"\n-                             ,                                                                      \"MH adapter blob\"\n-                             ,                                                                           \"buffer blob\"\n-                             ,                                                                                \"lastType\"\n+                             ,                         \"runtime stub\"\n+                             ,                              \"ricochet stub\"\n+                             ,                                   \"deopt stub\"\n+                             ,                                        \"uncommon trap stub\"\n+                             ,                                             \"exception stub\"\n+                             ,                                                  \"safepoint stub\"\n+                             ,                                                       \"adapter blob\"\n+                             ,                                                            \"MH adapter blob\"\n+                             ,                                                                 \"buffer blob\"\n+                             ,                                                                      \"lastType\"\n@@ -252,2 +250,0 @@\n-static unsigned int nBlocks_dead          = 0;  \/\/ counting \"zombie\" and \"unloaded\" methods only.\n-static unsigned int nBlocks_unloaded      = 0;  \/\/ counting \"unloaded\" nmethods only. This is a transient state.\n@@ -265,5 +261,0 @@\n-\/\/ nMethod temperature (hotness) indicators.\n-static int                     avgTemp    = 0;\n-static int                     maxTemp    = 0;\n-static int                     minTemp    = 0;\n-\n@@ -322,2 +313,0 @@\n-    nBlocks_dead          = CodeHeapStatArray[ix].nBlocks_dead;\n-    nBlocks_unloaded      = CodeHeapStatArray[ix].nBlocks_unloaded;\n@@ -331,3 +320,0 @@\n-    avgTemp               = CodeHeapStatArray[ix].avgTemp;\n-    maxTemp               = CodeHeapStatArray[ix].maxTemp;\n-    minTemp               = CodeHeapStatArray[ix].minTemp;\n@@ -344,2 +330,0 @@\n-    nBlocks_dead          = 0;\n-    nBlocks_unloaded      = 0;\n@@ -353,3 +337,0 @@\n-    avgTemp               = 0;\n-    maxTemp               = 0;\n-    minTemp               = 0;\n@@ -370,2 +351,0 @@\n-    CodeHeapStatArray[ix].nBlocks_dead          = nBlocks_dead;\n-    CodeHeapStatArray[ix].nBlocks_unloaded      = nBlocks_unloaded;\n@@ -379,3 +358,0 @@\n-    CodeHeapStatArray[ix].avgTemp               = avgTemp;\n-    CodeHeapStatArray[ix].maxTemp               = maxTemp;\n-    CodeHeapStatArray[ix].minTemp               = minTemp;\n@@ -662,2 +638,0 @@\n-    nBlocks_dead     = 0;\n-    nBlocks_unloaded = 0;\n@@ -695,2 +669,0 @@\n-    size_t       deadSpace     = 0;\n-    size_t       unloadedSpace = 0;\n@@ -703,1 +675,0 @@\n-    int64_t hotnessAccumulator = 0;\n@@ -705,3 +676,0 @@\n-    avgTemp       = 0;\n-    minTemp       = (int)(res_size > M ? (res_size\/M)*2 : 1);\n-    maxTemp       = -minTemp;\n@@ -761,1 +729,0 @@\n-          int temperature        = 0;\n@@ -787,2 +754,0 @@\n-                temperature = nm->hotness_counter();\n-                hotnessAccumulator += temperature;\n@@ -790,2 +755,0 @@\n-                maxTemp = (temperature > maxTemp) ? temperature : maxTemp;\n-                minTemp = (temperature < minTemp) ? temperature : minTemp;\n@@ -806,8 +769,0 @@\n-              case nMethod_unloaded:\n-                nBlocks_unloaded++;\n-                unloadedSpace  += hb_bytelen;\n-                break;\n-              case nMethod_dead:\n-                nBlocks_dead++;\n-                deadSpace      += hb_bytelen;\n-                break;\n@@ -831,1 +786,0 @@\n-              TopSizeArray[0].temperature = temperature;\n@@ -849,1 +803,0 @@\n-              TopSizeArray[used_topSizeBlocks].temperature = temperature;\n@@ -892,1 +845,0 @@\n-                      TopSizeArray[i].temperature = temperature;\n@@ -934,1 +886,0 @@\n-                          TopSizeArray[i].temperature = temperature;\n@@ -950,1 +901,0 @@\n-                          TopSizeArray[i].temperature = temperature;\n@@ -1002,12 +952,0 @@\n-            case nMethod_alive:\n-              StatArray[ix_beg].tx_count++;\n-              StatArray[ix_beg].tx_space += (unsigned short)hb_len;\n-              StatArray[ix_beg].tx_age    = StatArray[ix_beg].tx_age < compile_id ? compile_id : StatArray[ix_beg].tx_age;\n-              StatArray[ix_beg].level     = comp_lvl;\n-              StatArray[ix_beg].compiler  = cType;\n-              break;\n-            case nMethod_dead:\n-            case nMethod_unloaded:\n-              StatArray[ix_beg].dead_count++;\n-              StatArray[ix_beg].dead_space += (unsigned short)hb_len;\n-              break;\n@@ -1015,1 +953,0 @@\n-              \/\/ must be a stub, if it's not a dead or alive nMethod\n@@ -1058,21 +995,0 @@\n-            case nMethod_alive:\n-              StatArray[ix_beg].tx_count++;\n-              StatArray[ix_beg].tx_space += (unsigned short)beg_space;\n-              StatArray[ix_beg].tx_age    = StatArray[ix_beg].tx_age < compile_id ? compile_id : StatArray[ix_beg].tx_age;\n-\n-              StatArray[ix_end].tx_count++;\n-              StatArray[ix_end].tx_space += (unsigned short)end_space;\n-              StatArray[ix_end].tx_age    = StatArray[ix_end].tx_age < compile_id ? compile_id : StatArray[ix_end].tx_age;\n-\n-              StatArray[ix_beg].level     = comp_lvl;\n-              StatArray[ix_beg].compiler  = cType;\n-              StatArray[ix_end].level     = comp_lvl;\n-              StatArray[ix_end].compiler  = cType;\n-              break;\n-            case nMethod_dead:\n-            case nMethod_unloaded:\n-              StatArray[ix_beg].dead_count++;\n-              StatArray[ix_beg].dead_space += (unsigned short)beg_space;\n-              StatArray[ix_end].dead_count++;\n-              StatArray[ix_end].dead_space += (unsigned short)end_space;\n-              break;\n@@ -1080,1 +996,0 @@\n-              \/\/ must be a stub, if it's not a dead or alive nMethod\n@@ -1105,12 +1020,0 @@\n-              case nMethod_alive:\n-                StatArray[ix].tx_count++;\n-                StatArray[ix].tx_space += (unsigned short)(granule_size>>log2_seg_size);\n-                StatArray[ix].tx_age    = StatArray[ix].tx_age < compile_id ? compile_id : StatArray[ix].tx_age;\n-                StatArray[ix].level     = comp_lvl;\n-                StatArray[ix].compiler  = cType;\n-                break;\n-              case nMethod_dead:\n-              case nMethod_unloaded:\n-                StatArray[ix].dead_count++;\n-                StatArray[ix].dead_space += (unsigned short)(granule_size>>log2_seg_size);\n-                break;\n@@ -1118,1 +1021,0 @@\n-                \/\/ must be a stub, if it's not a dead or alive nMethod\n@@ -1141,2 +1043,0 @@\n-      ast->print_cr(\"  unloadedSpace  = \" SIZE_FORMAT_W(8) \"k, nBlocks_unloaded = %6d, %10.3f%% of capacity, %10.3f%% of max_capacity\", unloadedSpace\/(size_t)K, nBlocks_unloaded, (100.0*unloadedSpace)\/size, (100.0*unloadedSpace)\/res_size);\n-      ast->print_cr(\"  deadSpace      = \" SIZE_FORMAT_W(8) \"k, nBlocks_dead     = %6d, %10.3f%% of capacity, %10.3f%% of max_capacity\", deadSpace\/(size_t)K,     nBlocks_dead,     (100.0*deadSpace)\/size,     (100.0*deadSpace)\/res_size);\n@@ -1153,16 +1053,0 @@\n-      ast->cr();\n-\n-      int             reset_val = NMethodSweeper::hotness_counter_reset_val();\n-      double reverse_free_ratio = (res_size > size) ? (double)res_size\/(double)(res_size-size) : (double)res_size;\n-      printBox(ast, '-', \"Method hotness information at time of this analysis\", NULL);\n-      ast->print_cr(\"Highest possible method temperature:          %12d\", reset_val);\n-      ast->print_cr(\"Threshold for method to be considered 'cold': %12.3f\", -reset_val + reverse_free_ratio * NmethodSweepActivity);\n-      if (n_methods > 0) {\n-        avgTemp = hotnessAccumulator\/n_methods;\n-        ast->print_cr(\"min. hotness = %6d\", minTemp);\n-        ast->print_cr(\"avg. hotness = %6d\", avgTemp);\n-        ast->print_cr(\"max. hotness = %6d\", maxTemp);\n-      } else {\n-        avgTemp = 0;\n-        ast->print_cr(\"No hotness data available\");\n-      }\n@@ -1188,3 +1072,0 @@\n-        if (StatArray[ix].dead_count > granule_segs) {\n-          out->print_cr(\"dead_count[%d] = %d\", ix, StatArray[ix].dead_count);\n-        }\n@@ -1203,3 +1084,0 @@\n-        if (StatArray[ix].dead_space > granule_segs) {\n-          out->print_cr(\"dead_space[%d] = %d\", ix, StatArray[ix].dead_space);\n-        }\n@@ -1207,1 +1085,1 @@\n-        if ((size_t)(StatArray[ix].t1_count+StatArray[ix].t2_count+StatArray[ix].tx_count+StatArray[ix].stub_count+StatArray[ix].dead_count) > granule_segs) {\n+        if ((size_t)(StatArray[ix].t1_count+StatArray[ix].t2_count+StatArray[ix].tx_count+StatArray[ix].stub_count) > granule_segs) {\n@@ -1210,1 +1088,1 @@\n-        if ((size_t)(StatArray[ix].t1_space+StatArray[ix].t2_space+StatArray[ix].tx_space+StatArray[ix].stub_space+StatArray[ix].dead_space) > granule_segs) {\n+        if ((size_t)(StatArray[ix].t1_space+StatArray[ix].t2_space+StatArray[ix].tx_space+StatArray[ix].stub_space) > granule_segs) {\n@@ -1380,1 +1258,1 @@\n-    ast->print_cr(\"%18s %13s %17s %4s %9s  %5s %s\",      \"Addr(module)      \", \"offset\", \"size\", \"type\", \" type lvl\", \" temp\", \"Name\");\n+    ast->print_cr(\"%18s %13s %17s %9s  %5s %s\",      \"Addr(module)      \", \"offset\", \"size\", \"type\", \" type lvl\", \"Name\");\n@@ -1423,3 +1301,0 @@\n-          \/\/---<  method temperature  >---\n-          ast->fill_to(67);\n-          ast->print(\"%5d\", TopSizeArray[i].temperature);\n@@ -1428,3 +1303,0 @@\n-          if (TopSizeArray[i].type == nMethod_dead) {\n-            ast->print(\" zombie method \");\n-          }\n@@ -1775,1 +1647,1 @@\n-                           + StatArray[ix].stub_count + StatArray[ix].dead_count;\n+                           + StatArray[ix].stub_count;\n@@ -1862,20 +1734,0 @@\n-  {\n-    if (nBlocks_dead > 0) {\n-      printBox(ast, '-', \"Dead nMethod count only, 0x1..0xf. '*' indicates >= 16 blocks, ' ' indicates empty\", NULL);\n-\n-      granules_per_line = 128;\n-      for (unsigned int ix = 0; ix < alloc_granules; ix++) {\n-        print_line_delim(out, ast, low_bound, ix, granules_per_line);\n-        if (segment_granules && StatArray[ix].dead_count > 0) {\n-          print_blobType_single(ast, StatArray[ix].type);\n-        } else {\n-          print_count_single(ast, StatArray[ix].dead_count);\n-        }\n-      }\n-      ast->print(\"|\");\n-    } else {\n-      ast->print(\"No dead nMethods found in CodeHeap.\");\n-    }\n-    BUFFEREDSTREAM_FLUSH_LOCKED(\"\\n\\n\\n\")\n-  }\n-\n@@ -1884,1 +1736,1 @@\n-      printBox(ast, '-', \"Count by tier (combined, no dead blocks): <#t1>:<#t2>:<#s>, 0x0..0xf. '*' indicates >= 16 blocks\", NULL);\n+      printBox(ast, '-', \"Count by tier (combined): <#t1>:<#t2>:<#s>, 0x0..0xf. '*' indicates >= 16 blocks\", NULL);\n@@ -1956,1 +1808,1 @@\n-                              + StatArray[ix].stub_space + StatArray[ix].dead_space;\n+                              + StatArray[ix].stub_space;\n@@ -2043,16 +1895,0 @@\n-  {\n-    if (nBlocks_dead > 0) {\n-      printBox(ast, '-', \"Dead space consumption. ' ' indicates empty, '*' indicates full\", NULL);\n-\n-      granules_per_line = 128;\n-      for (unsigned int ix = 0; ix < alloc_granules; ix++) {\n-        print_line_delim(out, ast, low_bound, ix, granules_per_line);\n-        print_space_single(ast, StatArray[ix].dead_space);\n-      }\n-      ast->print(\"|\");\n-    } else {\n-      ast->print(\"No dead nMethods found in CodeHeap.\");\n-    }\n-    BUFFEREDSTREAM_FLUSH_LOCKED(\"\\n\\n\\n\")\n-  }\n-\n@@ -2253,1 +2089,1 @@\n-                           StatArray[ix].stub_count + StatArray[ix].dead_count;\n+                           StatArray[ix].stub_count;\n@@ -2296,1 +2132,1 @@\n-          ast->print_cr(\"%18s %13s %17s %9s  %5s %18s  %s\", \"Addr(module)      \", \"offset\", \"size\", \" type lvl\", \" temp\", \"blobType          \", \"Name\");\n+          ast->print_cr(\"%18s %13s %17s %9s  %18s  %s\", \"Addr(module)      \", \"offset\", \"size\", \" type lvl\", \"blobType          \", \"Name\");\n@@ -2313,1 +2149,0 @@\n-          int          hotness    = nm->hotness_counter();\n@@ -2321,3 +2156,0 @@\n-          \/\/---<  method temperature  >---\n-          ast->fill_to(62);\n-          ast->print(\"%5d\", hotness);\n@@ -2325,1 +2157,1 @@\n-          ast->fill_to(62+6);\n+          ast->fill_to(62);\n@@ -2327,4 +2159,1 @@\n-          ast->fill_to(82+6);\n-          if (cbType == nMethod_dead) {\n-            ast->print(\"%14s\", \" zombie method\");\n-          }\n+          ast->fill_to(82);\n@@ -2350,1 +2179,1 @@\n-          ast->fill_to(62+6);\n+          ast->fill_to(62);\n@@ -2352,1 +2181,1 @@\n-          ast->fill_to(82+6);\n+          ast->fill_to(82);\n@@ -2355,1 +2184,1 @@\n-          ast->fill_to(62+6);\n+          ast->fill_to(62);\n@@ -2537,2 +2366,0 @@\n-        if (nm->is_zombie())        return nMethod_dead;\n-        if (nm->is_unloaded())      return nMethod_unloaded;\n@@ -2540,3 +2367,2 @@\n-        if (nm->is_alive() && !(nm->is_not_entrant()))   return nMethod_notused;\n-        if (nm->is_alive())         return nMethod_alive;\n-        return nMethod_dead;\n+        if (!nm->is_not_entrant())  return nMethod_notused;\n+        return nMethod_notentrant;\n@@ -2561,1 +2387,1 @@\n-  return (nm != NULL) && (method != NULL) && nm->is_alive() && (method->signature() != NULL);\n+  return (nm != NULL) && (method != NULL) && (method->signature() != NULL);\n","filename":"src\/hotspot\/share\/code\/codeHeapState.cpp","additions":27,"deletions":201,"binary":false,"changes":228,"status":"modified"},{"patch":"@@ -55,6 +55,1 @@\n-                            \/\/ Will transition to \"zombie\" after all activations are gone.\n-    nMethod_zombie,         \/\/ No more activations exist, ready for purge (remove from code cache).\n-    nMethod_unloaded,       \/\/ No activations exist, should not be called. Transient state on the way to \"zombie\".\n-    nMethod_alive = nMethod_notentrant, \/\/ Combined state: nmethod may have activations, thus can't be purged.\n-    nMethod_dead  = nMethod_zombie,     \/\/ Combined state: nmethod does not have any activations.\n-    runtimeStub   = nMethod_unloaded + 1,\n+    runtimeStub,\n","filename":"src\/hotspot\/share\/code\/codeHeapState.hpp","additions":1,"deletions":6,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -71,1 +71,1 @@\n-  CodeBlob* cb = CodeCache::find_blob_unsafe(code);\n+  CodeBlob* cb = CodeCache::find_blob(code);\n@@ -131,1 +131,1 @@\n-    CodeBlob* cb = CodeCache::find_blob_unsafe(_call->instruction_address());\n+    CodeBlob* cb = CodeCache::find_blob(_call->instruction_address());\n@@ -320,4 +320,1 @@\n-  \/\/ Use unsafe, since an inline cache might point to a zombie method. However, the zombie\n-  \/\/ method is guaranteed to still exist, since we only remove methods after all inline caches\n-  \/\/ has been cleaned up\n-  CodeBlob* cb = CodeCache::find_blob_unsafe(ic_destination());\n+  CodeBlob* cb = CodeCache::find_blob(ic_destination());\n@@ -331,1 +328,1 @@\n-  CodeBlob* caller = CodeCache::find_blob_unsafe(instruction_address());\n+  CodeBlob* caller = CodeCache::find_blob(instruction_address());\n@@ -336,1 +333,0 @@\n-         !caller->is_alive() ||\n@@ -349,4 +345,1 @@\n-    \/\/ must use unsafe because the destination can be a zombie (and we're cleaning)\n-    \/\/ and the print_compiled_ic code wants to know if site (in the non-zombie)\n-    \/\/ is to the interpreter.\n-    CodeBlob* cb = CodeCache::find_blob_unsafe(ic_destination());\n+    CodeBlob* cb = CodeCache::find_blob(ic_destination());\n@@ -377,2 +370,0 @@\n-  \/\/ A zombie transition will always be safe, since the metadata has already been set to NULL, so\n-  \/\/ we only need to patch the destination\n@@ -463,1 +454,1 @@\n-    CodeBlob* cb = CodeCache::find_blob_unsafe(info.entry());\n+    CodeBlob* cb = CodeCache::find_blob(info.entry());\n@@ -563,1 +554,1 @@\n-  CodeBlob* cb = CodeCache::find_blob_unsafe(entry);\n+  CodeBlob* cb = CodeCache::find_blob(entry);\n","filename":"src\/hotspot\/share\/code\/compiledIC.cpp","additions":7,"deletions":16,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -109,4 +109,0 @@\n-  case zombie:\n-    return \"zombie\";\n-  case unloaded:\n-    return \"unloaded\";\n@@ -313,1 +309,1 @@\n-  \/\/ If the method is not entrant or zombie then a JMP is plastered over the\n+  \/\/ If the method is not entrant then a JMP is plastered over the\n@@ -431,5 +427,1 @@\n-  assert(SafepointSynchronize::is_at_safepoint(), \"cleaning of IC's only allowed at safepoint\");\n-  if (is_zombie()) {\n-    return;\n-  }\n-\n+  assert(SafepointSynchronize::is_at_safepoint(), \"clearing of IC's only allowed at safepoint\");\n@@ -519,2 +511,1 @@\n-  \/\/ Ok, to lookup references to zombies here\n-  CodeBlob *cb = CodeCache::find_blob_unsafe(addr);\n+  CodeBlob *cb = CodeCache::find_blob(addr);\n@@ -523,1 +514,1 @@\n-    \/\/ Clean inline caches pointing to both zombie and not_entrant methods\n+    \/\/ Clean inline caches pointing to bad nmethods\n@@ -525,35 +516,0 @@\n-      \/\/ Inline cache cleaning should only be initiated on CompiledMethods that have been\n-      \/\/ observed to be is_alive(). However, with concurrent code cache unloading, it is\n-      \/\/ possible that by now, the state has become !is_alive. This can happen in two ways:\n-      \/\/ 1) It can be racingly flipped to unloaded if the nmethod \/\/ being cleaned (from the\n-      \/\/ sweeper) is_unloading(). This is fine, because if that happens, then the inline\n-      \/\/ caches have already been cleaned under the same CompiledICLocker that we now hold during\n-      \/\/ inline cache cleaning, and we will simply walk the inline caches again, and likely not\n-      \/\/ find much of interest to clean. However, this race prevents us from asserting that the\n-      \/\/ nmethod is_alive(). The is_unloading() function is completely monotonic; once set due\n-      \/\/ to an oop dying, it remains set forever until freed. Because of that, all unloaded\n-      \/\/ nmethods are is_unloading(), but notably, an unloaded nmethod may also subsequently\n-      \/\/ become zombie (when the sweeper converts it to zombie).\n-      \/\/ 2) It can be racingly flipped to zombie if the nmethod being cleaned (by the concurrent\n-      \/\/ GC) cleans a zombie nmethod that is concurrently made zombie by the sweeper. In this\n-      \/\/ scenario, the sweeper will first transition the nmethod to zombie, and then when\n-      \/\/ unregistering from the GC, it will wait until the GC is done. The GC will then clean\n-      \/\/ the inline caches *with IC stubs*, even though no IC stubs are needed. This is fine,\n-      \/\/ as long as the IC stubs are guaranteed to be released until the next safepoint, where\n-      \/\/ IC finalization requires live IC stubs to not be associated with zombie nmethods.\n-      \/\/ This is guaranteed, because the sweeper does not have a single safepoint check until\n-      \/\/ after it completes the whole transition function; it will wake up after the GC is\n-      \/\/ done with concurrent code cache cleaning (which blocks out safepoints using the\n-      \/\/ suspendible threads set), and then call clear_ic_callsites, which will release the\n-      \/\/ associated IC stubs, before a subsequent safepoint poll can be reached. This\n-      \/\/ guarantees that the spuriously created IC stubs are released appropriately before\n-      \/\/ IC finalization in a safepoint gets to run. Therefore, this race is fine. This is also\n-      \/\/ valid in a scenario where an inline cache of a zombie nmethod gets a spurious IC stub,\n-      \/\/ and then when cleaning another inline cache, fails to request an IC stub because we\n-      \/\/ exhausted the IC stub buffer. In this scenario, the GC will request a safepoint after\n-      \/\/ yielding the suspendible therad set, effectively unblocking safepoints. Before such\n-      \/\/ a safepoint can be reached, the sweeper similarly has to wake up, clear the IC stubs,\n-      \/\/ and reach the next safepoint poll, after the whole transition function has completed.\n-      \/\/ Due to the various races that can cause an nmethod to first be is_alive() and then\n-      \/\/ racingly become !is_alive(), it is unfortunately not possible to assert the nmethod\n-      \/\/ is_alive(), !is_unloaded() or !is_zombie() here.\n@@ -621,20 +577,6 @@\n-void CompiledMethod::cleanup_inline_caches(bool clean_all) {\n-  for (;;) {\n-    ICRefillVerifier ic_refill_verifier;\n-    { CompiledICLocker ic_locker(this);\n-      if (cleanup_inline_caches_impl(false, clean_all)) {\n-        return;\n-      }\n-    }\n-    \/\/ Call this nmethod entry barrier from the sweeper.\n-    run_nmethod_entry_barrier();\n-    if (!clean_all) {\n-      MutexLocker ml(CodeCache_lock, Mutex::_no_safepoint_check_flag);\n-      CodeCache::Sweep::end();\n-    }\n-    InlineCacheBuffer::refill_ic_stubs();\n-    if (!clean_all) {\n-      MutexLocker ml(CodeCache_lock, Mutex::_no_safepoint_check_flag);\n-      CodeCache::Sweep::begin();\n-    }\n-  }\n+\/\/ Only called by whitebox test\n+void CompiledMethod::cleanup_inline_caches_whitebox() {\n+  assert_locked_or_safepoint(CodeCache_lock);\n+  CompiledICLocker ic_locker(this);\n+  guarantee(cleanup_inline_caches_impl(false \/* unloading_occurred *\/, true \/* clean_all *\/),\n+            \"Inline cache cleaning in a safepoint can't fail\");\n@@ -647,2 +589,1 @@\n-\/\/ Called to clean up after class unloading for live nmethods and from the sweeper\n-\/\/ for all methods.\n+\/\/ Called to clean up after class unloading for live nmethods\n@@ -653,2 +594,1 @@\n-  \/\/ Find all calls in an nmethod and clear the ones that point to non-entrant,\n-  \/\/ zombie and unloaded nmethods.\n+  \/\/ Find all calls in an nmethod and clear the ones that point to bad nmethods.\n","filename":"src\/hotspot\/share\/code\/compiledMethod.cpp","additions":12,"deletions":72,"binary":false,"changes":84,"status":"modified"},{"patch":"@@ -143,1 +143,0 @@\n-  friend class NMethodSweeper;\n@@ -207,5 +206,1 @@\n-         not_entrant   = 2,  \/\/ marked for deoptimization but activations may still exist,\n-                             \/\/ will be transformed to zombie when all activations are gone\n-         unloaded      = 3,  \/\/ there should be no activations, should not be called, will be\n-                             \/\/ transformed to zombie by the sweeper, when not \"locked in vm\".\n-         zombie        = 4   \/\/ no activations exist, nmethod is ready for purge\n+         not_entrant   = 2,  \/\/ marked for deoptimization but activations may still exist\n@@ -225,1 +220,0 @@\n-  virtual bool make_zombie() = 0;\n@@ -347,1 +341,0 @@\n-  virtual bool can_convert_to_zombie() = 0;\n@@ -372,2 +365,2 @@\n-  \/\/ Serial version used by sweeper and whitebox test\n-  void cleanup_inline_caches(bool clean_all);\n+  \/\/ Serial version used by whitebox test\n+  void cleanup_inline_caches_whitebox();\n","filename":"src\/hotspot\/share\/code\/compiledMethod.hpp","additions":3,"deletions":10,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -71,3 +71,1 @@\n-    \/\/ since dependencies aren't removed until an nmethod becomes a zombie,\n-    \/\/ the dependency list may contain nmethods which aren't alive.\n-    if (b->count() > 0 && nm->is_alive() && !nm->is_marked_for_deoptimization() && nm->check_dependency_on(changes)) {\n+    if (b->count() > 0 && !nm->is_marked_for_deoptimization() && nm->check_dependency_on(changes)) {\n@@ -140,34 +138,0 @@\n-\/\/\n-\/\/ Remove an nmethod dependency from the context.\n-\/\/ Decrement count of the nmethod in the dependency list and, optionally, remove\n-\/\/ the bucket completely when the count goes to 0.  This method must find\n-\/\/ a corresponding bucket otherwise there's a bug in the recording of dependencies.\n-\/\/ Can be called concurrently by parallel GC threads.\n-\/\/\n-void DependencyContext::remove_dependent_nmethod(nmethod* nm) {\n-  assert_locked_or_safepoint(CodeCache_lock);\n-  nmethodBucket* first = dependencies_not_unloading();\n-  nmethodBucket* last = NULL;\n-  for (nmethodBucket* b = first; b != NULL; b = b->next_not_unloading()) {\n-    if (nm == b->get_nmethod()) {\n-      int val = b->decrement();\n-      guarantee(val >= 0, \"Underflow: %d\", val);\n-      if (val == 0) {\n-        if (last == NULL) {\n-          \/\/ If there was not a head that was not unloading, we can set a new\n-          \/\/ head without a CAS, because we know there is no contending cleanup.\n-          set_dependencies(b->next_not_unloading());\n-        } else {\n-          \/\/ Only supports a single inserting thread (protected by CodeCache_lock)\n-          \/\/ for now. Therefore, the next pointer only competes with another cleanup\n-          \/\/ operation. That interaction does not need a CAS.\n-          last->set_next(b->next_not_unloading());\n-        }\n-        release(b);\n-      }\n-      return;\n-    }\n-    last = b;\n-  }\n-}\n-\n@@ -217,1 +181,1 @@\n-    if (b->count() > 0 && nm->is_alive() && !nm->is_marked_for_deoptimization()) {\n+    if (b->count() > 0 && !nm->is_marked_for_deoptimization()) {\n","filename":"src\/hotspot\/share\/code\/dependencyContext.cpp","additions":2,"deletions":38,"binary":false,"changes":40,"status":"modified"},{"patch":"@@ -122,1 +122,0 @@\n-  void remove_dependent_nmethod(nmethod* nm);\n","filename":"src\/hotspot\/share\/code\/dependencyContext.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"compiler\/compilationLog.hpp\"\n@@ -38,0 +39,1 @@\n+#include \"compiler\/compileTask.hpp\"\n@@ -73,1 +75,0 @@\n-#include \"runtime\/sweeper.hpp\"\n@@ -444,2 +445,0 @@\n-  _lock_count                 = 0;\n-  _stack_traversal_mark       = 0;\n@@ -447,5 +446,0 @@\n-  _unload_reported            = false;\n-\n-#ifdef ASSERT\n-  _oops_are_stale             = false;\n-#endif\n@@ -614,0 +608,1 @@\n+  _unlinked_next(NULL),\n@@ -633,1 +628,1 @@\n-    _gc_epoch                = Continuations::gc_epoch();\n+    _gc_epoch                = CodeCache::gc_epoch();\n@@ -657,1 +652,0 @@\n-    _hotness_counter         = NMethodSweeper::hotness_counter_reset_val();\n@@ -749,0 +743,1 @@\n+  _unlinked_next(NULL),\n@@ -766,2 +761,1 @@\n-    _hotness_counter         = NMethodSweeper::hotness_counter_reset_val();\n-    _gc_epoch                = Continuations::gc_epoch();\n+    _gc_epoch                = CodeCache::gc_epoch();\n@@ -940,1 +934,1 @@\n-void nmethod::maybe_print_nmethod(DirectiveSet* directive) {\n+void nmethod::maybe_print_nmethod(const DirectiveSet* directive) {\n@@ -948,2 +942,0 @@\n-  run_nmethod_entry_barrier(); \/\/ ensure all embedded OOPs are valid before printing\n-\n@@ -1123,1 +1115,0 @@\n-  assert(!is_zombie(), \"\");\n@@ -1175,2 +1166,1 @@\n-        \/\/ Ok, to lookup references to zombies here\n-        CodeBlob *cb = CodeCache::find_blob_unsafe(ic->ic_destination());\n+        CodeBlob *cb = CodeCache::find_blob(ic->ic_destination());\n@@ -1180,1 +1170,1 @@\n-          \/\/ Verify that inline caches pointing to both zombie and not_entrant methods are clean\n+          \/\/ Verify that inline caches pointing to bad nmethods are clean\n@@ -1189,1 +1179,1 @@\n-        CodeBlob *cb = CodeCache::find_blob_unsafe(csc->destination());\n+        CodeBlob *cb = CodeCache::find_blob(csc->destination());\n@@ -1193,1 +1183,1 @@\n-          \/\/ Verify that inline caches pointing to both zombie and not_entrant methods are clean\n+          \/\/ Verify that inline caches pointing to bad nmethods are clean\n@@ -1206,11 +1196,2 @@\n-\/\/ This is a private interface with the sweeper.\n-void nmethod::mark_as_seen_on_stack() {\n-  assert(is_alive(), \"Must be an alive method\");\n-  \/\/ Set the traversal mark to ensure that the sweeper does 2\n-  \/\/ cleaning passes before moving to zombie.\n-  set_stack_traversal_mark(NMethodSweeper::traversal_count());\n-}\n-\n-void nmethod::mark_as_maybe_on_continuation() {\n-  assert(is_alive(), \"Must be an alive method\");\n-  _gc_epoch = Continuations::gc_epoch();\n+void nmethod::mark_as_maybe_on_stack() {\n+  Atomic::store(&_gc_epoch, CodeCache::gc_epoch());\n@@ -1219,5 +1200,1 @@\n-bool nmethod::is_maybe_on_continuation_stack() {\n-  if (!Continuations::enabled()) {\n-    return false;\n-  }\n-\n+bool nmethod::is_maybe_on_stack() {\n@@ -1226,23 +1203,1 @@\n-  return _gc_epoch >= Continuations::previous_completed_gc_marking_cycle();\n-}\n-\n-\/\/ Tell if a non-entrant method can be converted to a zombie (i.e.,\n-\/\/ there are no activations on the stack, not in use by the VM,\n-\/\/ and not in use by the ServiceThread)\n-bool nmethod::can_convert_to_zombie() {\n-  \/\/ Note that this is called when the sweeper has observed the nmethod to be\n-  \/\/ not_entrant. However, with concurrent code cache unloading, the state\n-  \/\/ might have moved on to unloaded if it is_unloading(), due to racing\n-  \/\/ concurrent GC threads.\n-  assert(is_not_entrant() || is_unloading() ||\n-         !Thread::current()->is_Code_cache_sweeper_thread(),\n-         \"must be a non-entrant method if called from sweeper\");\n-\n-  \/\/ Since the nmethod sweeper only does partial sweep the sweeper's traversal\n-  \/\/ count can be greater than the stack traversal count before it hits the\n-  \/\/ nmethod for the second time.\n-  \/\/ If an is_unloading() nmethod is still not_entrant, then it is not safe to\n-  \/\/ convert it to zombie due to GC unloading interactions. However, if it\n-  \/\/ has become unloaded, then it is okay to convert such nmethods to zombie.\n-  return stack_traversal_mark()+1 < NMethodSweeper::traversal_count() && !is_maybe_on_continuation_stack() &&\n-         !is_locked_by_vm() && (!is_unloading() || is_unloaded());\n+  return Atomic::load(&_gc_epoch) >= CodeCache::previous_completed_gc_marking_cycle();\n@@ -1264,110 +1219,5 @@\n-#ifdef ASSERT\n-  if (new_state != unloaded) {\n-    assert_lock_strong(CompiledMethod_lock);\n-  }\n-#endif\n-  for (;;) {\n-    signed char old_state = Atomic::load(&_state);\n-    if (old_state >= new_state) {\n-      \/\/ Ensure monotonicity of transitions.\n-      return false;\n-    }\n-    if (Atomic::cmpxchg(&_state, old_state, new_state) == old_state) {\n-      return true;\n-    }\n-  }\n-}\n-\n-void nmethod::make_unloaded() {\n-  post_compiled_method_unload();\n-\n-  \/\/ This nmethod is being unloaded, make sure that dependencies\n-  \/\/ recorded in instanceKlasses get flushed.\n-  \/\/ Since this work is being done during a GC, defer deleting dependencies from the\n-  \/\/ InstanceKlass.\n-  assert(Universe::heap()->is_gc_active() ||\n-         Thread::current()->is_ConcurrentGC_thread() ||\n-         Thread::current()->is_Worker_thread(),\n-         \"should only be called during gc\");\n-  flush_dependencies(\/*delete_immediately*\/false);\n-\n-  \/\/ Break cycle between nmethod & method\n-  LogTarget(Trace, class, unload, nmethod) lt;\n-  if (lt.is_enabled()) {\n-    LogStream ls(lt);\n-    ls.print(\"making nmethod \" INTPTR_FORMAT\n-             \" unloadable, Method*(\" INTPTR_FORMAT\n-             \") \",\n-             p2i(this), p2i(_method));\n-     ls.cr();\n-  }\n-  \/\/ Unlink the osr method, so we do not look this up again\n-  if (is_osr_method()) {\n-    \/\/ Invalidate the osr nmethod only once. Note that with concurrent\n-    \/\/ code cache unloading, OSR nmethods are invalidated before they\n-    \/\/ are made unloaded. Therefore, this becomes a no-op then.\n-    if (is_in_use()) {\n-      invalidate_osr_method();\n-    }\n-#ifdef ASSERT\n-    if (method() != NULL) {\n-      \/\/ Make sure osr nmethod is invalidated, i.e. not on the list\n-      bool found = method()->method_holder()->remove_osr_nmethod(this);\n-      assert(!found, \"osr nmethod should have been invalidated\");\n-    }\n-#endif\n-  }\n-\n-  \/\/ If _method is already NULL the Method* is about to be unloaded,\n-  \/\/ so we don't have to break the cycle. Note that it is possible to\n-  \/\/ have the Method* live here, in case we unload the nmethod because\n-  \/\/ it is pointing to some oop (other than the Method*) being unloaded.\n-  if (_method != NULL) {\n-    _method->unlink_code(this);\n-  }\n-\n-  \/\/ Make the class unloaded - i.e., change state and notify sweeper\n-  assert(SafepointSynchronize::is_at_safepoint() ||\n-         Thread::current()->is_ConcurrentGC_thread() ||\n-         Thread::current()->is_Worker_thread(),\n-         \"must be at safepoint\");\n-\n-  {\n-    \/\/ Clear ICStubs and release any CompiledICHolders.\n-    CompiledICLocker ml(this);\n-    clear_ic_callsites();\n-  }\n-\n-  \/\/ Unregister must be done before the state change\n-  {\n-    MutexLocker ml(SafepointSynchronize::is_at_safepoint() ? NULL : CodeCache_lock,\n-                     Mutex::_no_safepoint_check_flag);\n-    Universe::heap()->unregister_nmethod(this);\n-  }\n-\n-  \/\/ Clear the method of this dead nmethod\n-  set_method(NULL);\n-\n-  \/\/ Log the unloading.\n-  log_state_change();\n-\n-  \/\/ The Method* is gone at this point\n-  assert(_method == NULL, \"Tautology\");\n-\n-  set_osr_link(NULL);\n-  NMethodSweeper::report_state_change(this);\n-\n-  bool transition_success = try_transition(unloaded);\n-\n-  \/\/ It is an important invariant that there exists no race between\n-  \/\/ the sweeper and GC thread competing for making the same nmethod\n-  \/\/ zombie and unloaded respectively. This is ensured by\n-  \/\/ can_convert_to_zombie() returning false for any is_unloading()\n-  \/\/ nmethod, informing the sweeper not to step on any GC toes.\n-  assert(transition_success, \"Invalid nmethod transition to unloaded\");\n-\n-#if INCLUDE_JVMCI\n-  \/\/ Clear the link between this nmethod and a HotSpotNmethod mirror\n-  JVMCINMethodData* nmethod_data = jvmci_nmethod_data();\n-  if (nmethod_data != NULL) {\n-    nmethod_data->invalidate_nmethod_mirror(this);\n+  assert_lock_strong(CompiledMethod_lock);\n+  signed char old_state = _state;\n+  if (old_state >= new_state) {\n+    \/\/ Ensure monotonicity of transitions.\n+    return false;\n@@ -1375,1 +1225,2 @@\n-#endif\n+  Atomic::store(&_state, new_state);\n+  return true;\n@@ -1390,8 +1241,2 @@\n-      if (_state == unloaded) {\n-        xtty->begin_elem(\"make_unloaded thread='\" UINTX_FORMAT \"'\",\n-                         os::current_thread_id());\n-      } else {\n-        xtty->begin_elem(\"make_not_entrant thread='\" UINTX_FORMAT \"'%s\",\n-                         os::current_thread_id(),\n-                         (_state == zombie ? \" zombie='1'\" : \"\"));\n-      }\n+      xtty->begin_elem(\"make_not_entrant thread='\" UINTX_FORMAT \"'\",\n+                       os::current_thread_id());\n@@ -1404,4 +1249,3 @@\n-  const char *state_msg = _state == zombie ? \"made zombie\" : \"made not entrant\";\n-  CompileTask::print_ul(this, state_msg);\n-  if (PrintCompilation && _state != unloaded) {\n-    print_on(tty, state_msg);\n+  CompileTask::print_ul(this, \"made not entrant\");\n+  if (PrintCompilation) {\n+    print_on(tty, \"made not entrant\");\n@@ -1417,5 +1261,4 @@\n-\/**\n- * Common functionality for both make_not_entrant and make_zombie\n- *\/\n-bool nmethod::make_not_entrant_or_zombie(int state) {\n-  assert(state == zombie || state == not_entrant, \"must be zombie or not_entrant\");\n+\/\/ Invalidate code\n+bool nmethod::make_not_entrant() {\n+  \/\/ This can be called while the system is already at a safepoint which is ok\n+  NoSafepointVerifier nsv;\n@@ -1423,1 +1266,7 @@\n-  if (Atomic::load(&_state) >= state) {\n+  if (is_unloading()) {\n+    \/\/ If the nmethod is unloading, then it is already not entrant through\n+    \/\/ the nmethod entry barriers. No need to do anything; GC will unload it.\n+    return false;\n+  }\n+\n+  if (Atomic::load(&_state) == not_entrant) {\n@@ -1431,12 +1280,0 @@\n-  \/\/ Make sure the nmethod is not flushed.\n-  nmethodLocker nml(this);\n-  \/\/ This can be called while the system is already at a safepoint which is ok\n-  NoSafepointVerifier nsv;\n-\n-  \/\/ during patching, depending on the nmethod state we must notify the GC that\n-  \/\/ code has been unloaded, unregistering it. We cannot do this right while\n-  \/\/ holding the CompiledMethod_lock because we need to use the CodeCache_lock. This\n-  \/\/ would be prone to deadlocks.\n-  \/\/ This flag is used to remember whether we need to later lock and unregister.\n-  bool nmethod_needs_unregister = false;\n-\n@@ -1447,9 +1284,1 @@\n-    \/\/ This logic is equivalent to the logic below for patching the\n-    \/\/ verified entry point of regular methods. We check that the\n-    \/\/ nmethod is in use to ensure that it is invalidated only once.\n-    if (is_osr_method() && is_in_use()) {\n-      \/\/ this effectively makes the osr nmethod not entrant\n-      invalidate_osr_method();\n-    }\n-\n-    if (Atomic::load(&_state) >= state) {\n+    if (Atomic::load(&_state) == not_entrant) {\n@@ -1461,3 +1290,8 @@\n-    \/\/ The caller can be calling the method statically or through an inline\n-    \/\/ cache call.\n-    if (!is_osr_method() && !is_not_entrant()) {\n+    if (is_osr_method()) {\n+      \/\/ This logic is equivalent to the logic below for patching the\n+      \/\/ verified entry point of regular methods.\n+      \/\/ this effectively makes the osr nmethod not entrant\n+      invalidate_osr_method();\n+    } else {\n+      \/\/ The caller can be calling the method statically or through an inline\n+      \/\/ cache call.\n@@ -1465,1 +1299,1 @@\n-                  SharedRuntime::get_handle_wrong_method_stub());\n+                                       SharedRuntime::get_handle_wrong_method_stub());\n@@ -1468,3 +1302,2 @@\n-    if (is_in_use() && update_recompile_counts()) {\n-      \/\/ It's a true state change, so mark the method as decompiled.\n-      \/\/ Do it only for transition from alive.\n+    if (update_recompile_counts()) {\n+      \/\/ Mark the method as decompiled.\n@@ -1474,14 +1307,6 @@\n-    \/\/ If the state is becoming a zombie, signal to unregister the nmethod with\n-    \/\/ the heap.\n-    \/\/ This nmethod may have already been unloaded during a full GC.\n-    if ((state == zombie) && !is_unloaded()) {\n-      nmethod_needs_unregister = true;\n-    }\n-\n-    \/\/ Must happen before state change. Otherwise we have a race condition in\n-    \/\/ nmethod::can_convert_to_zombie(). I.e., a method can immediately\n-    \/\/ transition its state from 'not_entrant' to 'zombie' without having to wait\n-    \/\/ for stack scanning.\n-    if (state == not_entrant) {\n-      mark_as_seen_on_stack();\n-      OrderAccess::storestore(); \/\/ _stack_traversal_mark and _state\n+    BarrierSetNMethod* bs_nm = BarrierSet::barrier_set()->barrier_set_nmethod();\n+    if (bs_nm == nullptr || !bs_nm->supports_entry_barrier(this)) {\n+      \/\/ If nmethod entry barriers are not supported, we won't mark\n+      \/\/ nmethods as on-stack when they become on-stack. So we\n+      \/\/ degrade to a less accurate flushing strategy, for now.\n+      mark_as_maybe_on_stack();\n@@ -1491,12 +1316,2 @@\n-    if (!try_transition(state)) {\n-      \/\/ If the transition fails, it is due to another thread making the nmethod more\n-      \/\/ dead. In particular, one thread might be making the nmethod unloaded concurrently.\n-      \/\/ If so, having patched in the jump in the verified entry unnecessarily is fine.\n-      \/\/ The nmethod is no longer possible to call by Java threads.\n-      \/\/ Incrementing the decompile count is also fine as the caller of make_not_entrant()\n-      \/\/ had a valid reason to deoptimize the nmethod.\n-      \/\/ Marking the nmethod as seen on stack also has no effect, as the nmethod is now\n-      \/\/ !is_alive(), and the seen on stack value is only used to convert not_entrant\n-      \/\/ nmethods to zombie in can_convert_to_zombie().\n-      return false;\n-    }\n+    bool success = try_transition(not_entrant);\n+    assert(success, \"Transition can't fail\");\n@@ -1528,15 +1343,2 @@\n-  \/\/ When the nmethod becomes zombie it is no longer alive so the\n-  \/\/ dependencies must be flushed.  nmethods in the not_entrant\n-  \/\/ state will be flushed later when the transition to zombie\n-  \/\/ happens or they get unloaded.\n-  if (state == zombie) {\n-    {\n-      \/\/ Flushing dependencies must be done before any possible\n-      \/\/ safepoint can sneak in, otherwise the oops used by the\n-      \/\/ dependency logic could have become stale.\n-      MutexLocker mu(CodeCache_lock, Mutex::_no_safepoint_check_flag);\n-      if (nmethod_needs_unregister) {\n-        Universe::heap()->unregister_nmethod(this);\n-      }\n-      flush_dependencies(\/*delete_immediately*\/true);\n-    }\n+  return true;\n+}\n@@ -1544,7 +1346,8 @@\n-#if INCLUDE_JVMCI\n-    \/\/ Now that the nmethod has been unregistered, it's\n-    \/\/ safe to clear the HotSpotNmethod mirror oop.\n-    if (nmethod_data != NULL) {\n-      nmethod_data->clear_nmethod_mirror(this);\n-    }\n-#endif\n+\/\/ For concurrent GCs, there must be a handshake between unlink and flush\n+void nmethod::unlink() {\n+  if (_unlinked_next != NULL) {\n+    \/\/ Already unlinked. It can be invoked twice because concurrent code cache\n+    \/\/ unloading might need to restart when inline cache cleaning fails due to\n+    \/\/ running out of ICStubs, which can only be refilled at safepoints\n+    return;\n+  }\n@@ -1552,7 +1355,1 @@\n-    \/\/ Clear ICStubs to prevent back patching stubs of zombie or flushed\n-    \/\/ nmethods during the next safepoint (see ICStub::finalize), as well\n-    \/\/ as to free up CompiledICHolder resources.\n-    {\n-      CompiledICLocker ml(this);\n-      clear_ic_callsites();\n-    }\n+  flush_dependencies();\n@@ -1560,6 +1357,6 @@\n-    \/\/ zombie only - if a JVMTI agent has enabled the CompiledMethodUnload\n-    \/\/ event and it hasn't already been reported for this nmethod then\n-    \/\/ report it now. The event may have been reported earlier if the GC\n-    \/\/ marked it for unloading). JvmtiDeferredEventQueue support means\n-    \/\/ we no longer go to a safepoint here.\n-    post_compiled_method_unload();\n+  \/\/ unlink_from_method will take the CompiledMethod_lock.\n+  \/\/ In this case we don't strictly need it when unlinking nmethods from\n+  \/\/ the Method, because it is only concurrently unlinked by\n+  \/\/ the entry barrier, which acquires the per nmethod lock.\n+  unlink_from_method();\n+  clear_ic_callsites();\n@@ -1567,10 +1364,2 @@\n-#ifdef ASSERT\n-    \/\/ It's no longer safe to access the oops section since zombie\n-    \/\/ nmethods aren't scanned for GC.\n-    _oops_are_stale = true;\n-#endif\n-     \/\/ the Method may be reclaimed by class unloading now that the\n-     \/\/ nmethod is in zombie state\n-    set_method(NULL);\n-  } else {\n-    assert(state == not_entrant, \"other cases may need to be handled differently\");\n+  if (is_osr_method()) {\n+    invalidate_osr_method();\n@@ -1579,3 +1368,5 @@\n-  if (TraceCreateZombies && state == zombie) {\n-    ResourceMark m;\n-    tty->print_cr(\"nmethod <\" INTPTR_FORMAT \"> %s code made %s\", p2i(this), this->method() ? this->method()->name_and_sig_as_C_string() : \"null\", (state == not_entrant) ? \"not entrant\" : \"zombie\");\n+#if INCLUDE_JVMCI\n+  \/\/ Clear the link between this nmethod and a HotSpotNmethod mirror\n+  JVMCINMethodData* nmethod_data = jvmci_nmethod_data();\n+  if (nmethod_data != NULL) {\n+    nmethod_data->invalidate_nmethod_mirror(this);\n@@ -1583,0 +1374,1 @@\n+#endif\n@@ -1584,2 +1376,7 @@\n-  NMethodSweeper::report_state_change(this);\n-  return true;\n+  \/\/ Post before flushing as jmethodID is being used\n+  post_compiled_method_unload();\n+\n+  \/\/ Register for flushing when it is safe. For concurrent class unloading,\n+  \/\/ that would be after the unloading handshake, and for STW class unloading\n+  \/\/ that would be when getting back to the VM thread.\n+  CodeCache::register_unlinked(this);\n@@ -1589,7 +1386,1 @@\n-  MutexLocker mu(CodeCache_lock, Mutex::_no_safepoint_check_flag);\n-  \/\/ Note that there are no valid oops in the nmethod anymore.\n-  assert(!is_osr_method() || is_unloaded() || is_zombie(),\n-         \"osr nmethod must be unloaded or zombie before flushing\");\n-  assert(is_zombie() || is_osr_method(), \"must be a zombie method\");\n-  assert (!is_locked_by_vm(), \"locked methods shouldn't be flushed\");\n-  assert_locked_or_safepoint(CodeCache_lock);\n+  MutexLocker ml(CodeCache_lock, Mutex::_no_safepoint_check_flag);\n@@ -1598,7 +1389,5 @@\n-  Events::log(JavaThread::current(), \"flushing nmethod \" INTPTR_FORMAT, p2i(this));\n-  if (PrintMethodFlushing) {\n-    tty->print_cr(\"*flushing %s nmethod %3d\/\" INTPTR_FORMAT \". Live blobs:\" UINT32_FORMAT\n-                  \"\/Free CodeCache:\" SIZE_FORMAT \"Kb\",\n-                  is_osr_method() ? \"osr\" : \"\",_compile_id, p2i(this), CodeCache::blob_count(),\n-                  CodeCache::unallocated_capacity(CodeCache::get_code_blob_type(this))\/1024);\n-  }\n+  Events::log(Thread::current(), \"flushing nmethod \" INTPTR_FORMAT, p2i(this));\n+  log_debug(codecache)(\"*flushing %s nmethod %3d\/\" INTPTR_FORMAT \". Live blobs:\" UINT32_FORMAT\n+                       \"\/Free CodeCache:\" SIZE_FORMAT \"Kb\",\n+                       is_osr_method() ? \"osr\" : \"\",_compile_id, p2i(this), CodeCache::blob_count(),\n+                       CodeCache::unallocated_capacity(CodeCache::get_code_blob_type(this))\/1024);\n@@ -1610,1 +1399,0 @@\n-  set_exception_cache(NULL);\n@@ -1617,1 +1405,1 @@\n-  Universe::heap()->flush_nmethod(this);\n+  Universe::heap()->unregister_nmethod(this);\n@@ -1640,18 +1428,3 @@\n-\/\/ longer dependent. This should only be called in two situations.\n-\/\/ First, when a nmethod transitions to a zombie all dependents need\n-\/\/ to be clear.  Since zombification happens at a safepoint there's no\n-\/\/ synchronization issues.  The second place is a little more tricky.\n-\/\/ During phase 1 of mark sweep class unloading may happen and as a\n-\/\/ result some nmethods may get unloaded.  In this case the flushing\n-\/\/ of dependencies must happen during phase 1 since after GC any\n-\/\/ dependencies in the unloaded nmethod won't be updated, so\n-\/\/ traversing the dependency information in unsafe.  In that case this\n-\/\/ function is called with a boolean argument and this function only\n-\/\/ notifies instanceKlasses that are reachable\n-\n-void nmethod::flush_dependencies(bool delete_immediately) {\n-  DEBUG_ONLY(bool called_by_gc = Universe::heap()->is_gc_active() ||\n-                                 Thread::current()->is_ConcurrentGC_thread() ||\n-                                 Thread::current()->is_Worker_thread();)\n-  assert(called_by_gc != delete_immediately,\n-  \"delete_immediately is false if and only if we are called during GC\");\n+\/\/ longer dependent.\n+\n+void nmethod::flush_dependencies() {\n@@ -1664,6 +1437,1 @@\n-        if (delete_immediately) {\n-          assert_locked_or_safepoint(CodeCache_lock);\n-          MethodHandles::remove_dependent_nmethod(call_site, this);\n-        } else {\n-          MethodHandles::clean_dependency_context(call_site);\n-        }\n+        MethodHandles::clean_dependency_context(call_site);\n@@ -1675,9 +1443,3 @@\n-        \/\/ During GC delete_immediately is false, and liveness\n-        \/\/ of dependee determines class that needs to be updated.\n-        if (delete_immediately) {\n-          assert_locked_or_safepoint(CodeCache_lock);\n-          InstanceKlass::cast(klass)->remove_dependent_nmethod(this);\n-        } else if (klass->is_loader_alive()) {\n-          \/\/ The GC may clean dependency contexts concurrently and in parallel.\n-          InstanceKlass::cast(klass)->clean_dependency_context();\n-        }\n+        \/\/ During GC liveness of dependee determines class that needs to be updated.\n+        \/\/ The GC may clean dependency contexts concurrently and in parallel.\n+        InstanceKlass::cast(klass)->clean_dependency_context();\n@@ -1689,0 +1451,17 @@\n+void nmethod::post_compiled_method(CompileTask* task) {\n+  task->mark_success();\n+  task->set_nm_content_size(content_size());\n+  task->set_nm_insts_size(insts_size());\n+  task->set_nm_total_size(total_size());\n+\n+  \/\/ JVMTI -- compiled method notification (must be done outside lock)\n+  post_compiled_method_load_event();\n+\n+  if (CompilationLog::log() != NULL) {\n+    CompilationLog::log()->log_nmethod(JavaThread::current(), this);\n+  }\n+\n+  const DirectiveSet* directive = task->directive();\n+  maybe_print_nmethod(directive);\n+}\n+\n@@ -1694,19 +1473,0 @@\n-\n-  \/\/ Don't post this nmethod load event if it is already dying\n-  \/\/ because the sweeper might already be deleting this nmethod.\n-  {\n-    MutexLocker ml(CompiledMethod_lock, Mutex::_no_safepoint_check_flag);\n-    \/\/ When the nmethod is acquired from the CodeCache iterator, it can racingly become zombie\n-    \/\/ before this code is called. Filter them out here under the CompiledMethod_lock.\n-    if (!is_alive()) {\n-      return;\n-    }\n-    \/\/ As for is_alive() nmethods, we also don't want them to racingly become zombie once we\n-    \/\/ release this lock, so we check that this is not going to be the case.\n-    if (is_not_entrant() && can_convert_to_zombie()) {\n-      return;\n-    }\n-    \/\/ Ensure the sweeper can't collect this nmethod until it become \"active\" with JvmtiThreadState::nmethods_do.\n-    mark_as_seen_on_stack();\n-  }\n-\n@@ -1747,7 +1507,1 @@\n-  if (unload_reported()) {\n-    \/\/ During unloading we transition to unloaded and then to zombie\n-    \/\/ and the unloading is reported during the first transition.\n-    return;\n-  }\n-\n-  assert(_method != NULL && !is_unloaded(), \"just checking\");\n+  assert(_method != NULL, \"just checking\");\n@@ -1757,5 +1511,1 @@\n-  \/\/ post the event. Sometime later this nmethod will be made a zombie\n-  \/\/ by the sweeper but the Method* will not be valid at that point.\n-  \/\/ The jmethodID is a weak reference to the Method* so if\n-  \/\/ it's being unloaded there's no way to look it up since the weak\n-  \/\/ ref will have been cleared.\n+  \/\/ post the event. The Method* will not be valid when this is freed.\n@@ -1765,1 +1515,0 @@\n-    assert(!unload_reported(), \"already unloaded\");\n@@ -1771,7 +1520,0 @@\n-\n-  \/\/ The JVMTI CompiledMethodUnload event can be enabled or disabled at\n-  \/\/ any time. As the nmethod is being unloaded now we mark it has\n-  \/\/ having the unload event reported - this will ensure that we don't\n-  \/\/ attempt to report the event in the unlikely scenario where the\n-  \/\/ event is enabled at the time the nmethod is made a zombie.\n-  set_unload_reported();\n@@ -1827,0 +1569,32 @@\n+\/\/ Heuristic for nuking nmethods even though their oops are live.\n+\/\/ Main purpose is to reduce code cache pressure and get rid of\n+\/\/ nmethods that don't seem to be all that relevant any longer.\n+bool nmethod::is_cold() {\n+  if (!MethodFlushing || is_native_method() || is_not_installed()) {\n+    \/\/ No heuristic unloading at all\n+    return false;\n+  }\n+\n+  if (!is_maybe_on_stack() && is_not_entrant()) {\n+    \/\/ Not entrant nmethods that are not on any stack can just\n+    \/\/ be removed\n+    return true;\n+  }\n+\n+  BarrierSetNMethod* bs_nm = BarrierSet::barrier_set()->barrier_set_nmethod();\n+  if (bs_nm == nullptr || !bs_nm->supports_entry_barrier(this)) {\n+    \/\/ On platforms that don't support nmethod entry barriers, we can't\n+    \/\/ trust the temporal aspect of the gc epochs. So we can't detect\n+    \/\/ cold nmethods on such platforms.\n+    return false;\n+  }\n+\n+  if (!UseCodeCacheFlushing) {\n+    \/\/ Bail out if we don't heuristically remove nmethods\n+    return false;\n+  }\n+\n+  \/\/ Other code can be phased out more gradually after N GCs\n+  return CodeCache::previous_completed_gc_marking_cycle() > _gc_epoch + 2 * CodeCache::cold_gc_count();\n+}\n+\n@@ -1828,1 +1602,1 @@\n-\/\/ and the result of IsUnloadingBehaviour::is_unloading() fpr that cycle.\n+\/\/ and the result of IsUnloadingBehaviour::is_unloading() for that cycle.\n@@ -1879,2 +1653,3 @@\n-  \/\/ The IsUnloadingBehaviour is responsible for checking if there are any dead\n-  \/\/ oops in the CompiledMethod, by calling oops_do on it.\n+  \/\/ The IsUnloadingBehaviour is responsible for calculating if the nmethod\n+  \/\/ should be unloaded. This can be either because there is a dead oop,\n+  \/\/ or because is_cold() heuristically determines it is time to unload.\n@@ -1882,31 +1657,1 @@\n-\n-  if (is_zombie()) {\n-    \/\/ Zombies without calculated unloading epoch are never unloading due to GC.\n-\n-    \/\/ There are no races where a previously observed is_unloading() nmethod\n-    \/\/ suddenly becomes not is_unloading() due to here being observed as zombie.\n-\n-    \/\/ With STW unloading, all is_alive() && is_unloading() nmethods are unlinked\n-    \/\/ and unloaded in the safepoint. That makes races where an nmethod is first\n-    \/\/ observed as is_alive() && is_unloading() and subsequently observed as\n-    \/\/ is_zombie() impossible.\n-\n-    \/\/ With concurrent unloading, all references to is_unloading() nmethods are\n-    \/\/ first unlinked (e.g. IC caches and dependency contexts). Then a global\n-    \/\/ handshake operation is performed with all JavaThreads before finally\n-    \/\/ unloading the nmethods. The sweeper never converts is_alive() && is_unloading()\n-    \/\/ nmethods to zombies; it waits for them to become is_unloaded(). So before\n-    \/\/ the global handshake, it is impossible for is_unloading() nmethods to\n-    \/\/ racingly become is_zombie(). And is_unloading() is calculated for all is_alive()\n-    \/\/ nmethods before taking that global handshake, meaning that it will never\n-    \/\/ be recalculated after the handshake.\n-\n-    \/\/ After that global handshake, is_unloading() nmethods are only observable\n-    \/\/ to the iterators, and they will never trigger recomputation of the cached\n-    \/\/ is_unloading_state, and hence may not suffer from such races.\n-\n-    state_is_unloading = false;\n-  } else {\n-    state_is_unloading = IsUnloadingBehaviour::current()->is_unloading(this);\n-  }\n-\n+  state_is_unloading = IsUnloadingBehaviour::is_unloading(this);\n@@ -1928,2 +1673,1 @@\n-\/\/ oops.\n-\n+\/\/ oops or is heuristically found to be not important.\n@@ -1932,3 +1676,0 @@\n-  assert(!is_zombie() && !is_unloaded(),\n-         \"should not call follow on zombie or unloaded nmethod\");\n-\n@@ -1936,1 +1677,1 @@\n-    make_unloaded();\n+    unlink();\n@@ -1948,3 +1689,0 @@\n-  \/\/ make sure the oops ready to receive visitors\n-  assert(allow_dead || is_alive(), \"should not call follow on dead nmethod: %d\", _state);\n-\n@@ -1982,2 +1720,2 @@\n-  \/\/ CodeCache sweeper support\n-  mark_as_maybe_on_continuation();\n+  \/\/ CodeCache unloading support\n+  mark_as_maybe_on_stack();\n@@ -2355,1 +2093,1 @@\n-  NMethodIterator iter(NMethodIterator::only_alive_and_not_unloading);\n+  NMethodIterator iter(NMethodIterator::only_not_unloading);\n@@ -2409,11 +2147,0 @@\n-\n-bool nmethod::is_patchable_at(address instr_addr) {\n-  assert(insts_contains(instr_addr), \"wrong nmethod used\");\n-  if (is_zombie()) {\n-    \/\/ a zombie may never be patched\n-    return false;\n-  }\n-  return true;\n-}\n-\n-\n@@ -2425,29 +2152,0 @@\n-\n-\/\/-------------------------------------------------------------------------------------------\n-\n-\n-\/\/ QQQ might we make this work from a frame??\n-nmethodLocker::nmethodLocker(address pc) {\n-  CodeBlob* cb = CodeCache::find_blob(pc);\n-  guarantee(cb != NULL && cb->is_compiled(), \"bad pc for a nmethod found\");\n-  _nm = cb->as_compiled_method();\n-  lock_nmethod(_nm);\n-}\n-\n-\/\/ Only JvmtiDeferredEvent::compiled_method_unload_event()\n-\/\/ should pass zombie_ok == true.\n-void nmethodLocker::lock_nmethod(CompiledMethod* cm, bool zombie_ok) {\n-  if (cm == NULL)  return;\n-  nmethod* nm = cm->as_nmethod();\n-  Atomic::inc(&nm->_lock_count);\n-  assert(zombie_ok || !nm->is_zombie(), \"cannot lock a zombie method: %p\", nm);\n-}\n-\n-void nmethodLocker::unlock_nmethod(CompiledMethod* cm) {\n-  if (cm == NULL)  return;\n-  nmethod* nm = cm->as_nmethod();\n-  Atomic::dec(&nm->_lock_count);\n-  assert(nm->_lock_count >= 0, \"unmatched nmethod lock\/unlock\");\n-}\n-\n-\n@@ -2489,5 +2187,1 @@\n-\n-  \/\/ Hmm. OSR methods can be deopted but not marked as zombie or not_entrant\n-  \/\/ seems odd.\n-\n-  if (is_zombie() || is_not_entrant() || is_unloaded())\n+  if (is_not_entrant())\n@@ -3554,1 +3248,1 @@\n-    CodeBlob* db = CodeCache::find_blob_unsafe(dest);\n+    CodeBlob* db = CodeCache::find_blob(dest);\n","filename":"src\/hotspot\/share\/code\/nmethod.cpp","additions":167,"deletions":473,"binary":false,"changes":640,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+class CompileTask;\n@@ -69,1 +70,0 @@\n-  friend class NMethodSweeper;\n@@ -77,7 +77,0 @@\n-  \/\/ not_entrant method removal. Each mark_sweep pass will update\n-  \/\/ this mark to current sweep invocation count if it is seen on the\n-  \/\/ stack.  An not_entrant method can be removed when there are no\n-  \/\/ more activations, i.e., when the _stack_traversal_mark is less than\n-  \/\/ current sweep traversal index.\n-  volatile int64_t _stack_traversal_mark;\n-\n@@ -206,0 +199,2 @@\n+  nmethod* _unlinked_next;\n+\n@@ -243,13 +238,0 @@\n-  \/\/ Nmethod Flushing lock. If non-zero, then the nmethod is not removed\n-  \/\/ and is not made into a zombie. However, once the nmethod is made into\n-  \/\/ a zombie, it will be locked one final time if CompiledMethodUnload\n-  \/\/ event processing needs to be done.\n-  volatile jint _lock_count;\n-\n-  \/\/ The _hotness_counter indicates the hotness of a method. The higher\n-  \/\/ the value the hotter the method. The hotness counter of a nmethod is\n-  \/\/ set to [(ReservedCodeCacheSize \/ (1024 * 1024)) * 2] each time the method\n-  \/\/ is active while stack scanning (do_stack_scanning()). The hotness\n-  \/\/ counter is decreased (by 1) while sweeping.\n-  int _hotness_counter;\n-\n@@ -276,1 +258,0 @@\n-  bool _unload_reported;\n@@ -280,7 +261,1 @@\n-  volatile signed char _state;         \/\/ {not_installed, in_use, not_entrant, zombie, unloaded}\n-\n-#ifdef ASSERT\n-  bool _oops_are_stale;  \/\/ indicates that it's no longer safe to access oops section\n-#endif\n-\n-  friend class nmethodLocker;\n+  volatile signed char _state;         \/\/ {not_installed, in_use, not_used, not_entrant}\n@@ -333,1 +308,0 @@\n-  bool make_not_entrant_or_zombie(int state);\n@@ -442,4 +416,0 @@\n-  void dec_hotness_counter()        { _hotness_counter--; }\n-  void set_hotness_counter(int val) { _hotness_counter = val; }\n-  int  hotness_counter() const      { return _hotness_counter; }\n-\n@@ -459,1 +429,0 @@\n-  bool  is_alive() const                          { return _state < unloaded; }\n@@ -461,2 +430,0 @@\n-  bool  is_zombie() const                         { return _state == zombie; }\n-  bool  is_unloaded() const                       { return _state == unloaded; }\n@@ -465,0 +432,2 @@\n+  \/\/ Heuristically deduce an nmethod isn't worth keeping around\n+  bool is_cold();\n@@ -468,0 +437,3 @@\n+  nmethod* unlinked_next() const                  { return _unlinked_next; }\n+  void set_unlinked_next(nmethod* next)           { _unlinked_next = next; }\n+\n@@ -481,4 +453,1 @@\n-  bool  make_not_entrant() {\n-    assert(!method()->is_method_handle_intrinsic(), \"Cannot make MH intrinsic not entrant\");\n-    return make_not_entrant_or_zombie(not_entrant);\n-  }\n+  bool  make_not_entrant();\n@@ -486,1 +455,0 @@\n-  bool  make_zombie()      { return make_not_entrant_or_zombie(zombie); }\n@@ -492,2 +460,0 @@\n-  void  make_unloaded();\n-\n@@ -496,1 +462,1 @@\n-  void flush_dependencies(bool delete_immediately);\n+  void flush_dependencies();\n@@ -514,1 +480,0 @@\n-    assert(!_oops_are_stale, \"oops are stale\");\n@@ -539,4 +504,0 @@\n-  \/\/ Sweeper support\n-  int64_t stack_traversal_mark()                  { return _stack_traversal_mark; }\n-  void    set_stack_traversal_mark(int64_t l)     { _stack_traversal_mark = l; }\n-\n@@ -553,3 +514,2 @@\n-  \/\/ unlink and deallocate this nmethod\n-  \/\/ Only NMethodSweeper class is expected to use this. NMethodSweeper is not\n-  \/\/ expected to use any other private methods\/data in this class.\n+  \/\/ Unlink this nmethod from the system\n+  void unlink();\n@@ -557,1 +517,1 @@\n- protected:\n+  \/\/ Deallocate this nmethod - called by the GC\n@@ -560,6 +520,0 @@\n- public:\n-  \/\/ When true is returned, it is unsafe to remove this nmethod even if\n-  \/\/ it is a zombie, since the VM or the ServiceThread might still be\n-  \/\/ using it.\n-  bool is_locked_by_vm() const                    { return _lock_count >0; }\n-\n@@ -567,4 +521,2 @@\n-  void mark_as_seen_on_stack();\n-  void mark_as_maybe_on_continuation();\n-  bool is_maybe_on_continuation_stack();\n-  bool can_convert_to_zombie();\n+  void mark_as_maybe_on_stack();\n+  bool is_maybe_on_stack();\n@@ -628,3 +580,1 @@\n-  \/\/ used by jvmti to track if the load and unload events has been reported\n-  bool  unload_reported() const                   { return _unload_reported; }\n-  void  set_unload_reported()                     { _unload_reported = true; }\n+  \/\/ used by jvmti to track if the load events has been reported\n@@ -641,0 +591,3 @@\n+  \/\/ Post successful compilation\n+  void post_compiled_method(CompileTask* task);\n+\n@@ -685,1 +638,1 @@\n-  void maybe_print_nmethod(DirectiveSet* directive);\n+  void maybe_print_nmethod(const DirectiveSet* directive);\n@@ -733,3 +686,0 @@\n-  \/\/ is it ok to patch at address?\n-  bool is_patchable_at(address instr_address);\n-\n@@ -763,46 +713,0 @@\n-\/\/ Locks an nmethod so its code will not get removed and it will not\n-\/\/ be made into a zombie, even if it is a not_entrant method. After the\n-\/\/ nmethod becomes a zombie, if CompiledMethodUnload event processing\n-\/\/ needs to be done, then lock_nmethod() is used directly to keep the\n-\/\/ generated code from being reused too early.\n-class nmethodLocker : public StackObj {\n-  CompiledMethod* _nm;\n-\n- public:\n-\n-  \/\/ note: nm can be NULL\n-  \/\/ Only JvmtiDeferredEvent::compiled_method_unload_event()\n-  \/\/ should pass zombie_ok == true.\n-  static void lock_nmethod(CompiledMethod* nm, bool zombie_ok = false);\n-  static void unlock_nmethod(CompiledMethod* nm); \/\/ (ditto)\n-\n-  nmethodLocker(address pc); \/\/ derive nm from pc\n-  nmethodLocker(nmethod *nm) { _nm = nm; lock_nmethod(_nm); }\n-  nmethodLocker(CompiledMethod *nm) {\n-    _nm = nm;\n-    lock(_nm);\n-  }\n-\n-  static void lock(CompiledMethod* method, bool zombie_ok = false) {\n-    if (method == NULL) return;\n-    lock_nmethod(method, zombie_ok);\n-  }\n-\n-  static void unlock(CompiledMethod* method) {\n-    if (method == NULL) return;\n-    unlock_nmethod(method);\n-  }\n-\n-  nmethodLocker() { _nm = NULL; }\n-  ~nmethodLocker() {\n-    unlock(_nm);\n-  }\n-\n-  CompiledMethod* code() { return _nm; }\n-  void set_code(CompiledMethod* new_nm, bool zombie_ok = false) {\n-    unlock(_nm);   \/\/ note:  This works even if _nm==new_nm.\n-    _nm = new_nm;\n-    lock(_nm, zombie_ok);\n-  }\n-};\n-\n","filename":"src\/hotspot\/share\/code\/nmethod.hpp","additions":21,"deletions":117,"binary":false,"changes":138,"status":"modified"},{"patch":"@@ -0,0 +1,75 @@\n+\/*\n+ * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"code\/nmethod.hpp\"\n+#include \"compiler\/compilationLog.hpp\"\n+#include \"compiler\/compileTask.hpp\"\n+#include \"logging\/log.hpp\"\n+#include \"memory\/resourceArea.hpp\"\n+#include \"runtime\/thread.hpp\"\n+#include \"utilities\/ostream.hpp\"\n+\n+CompilationLog* CompilationLog::_log;\n+\n+CompilationLog::CompilationLog() : StringEventLog(\"Compilation events\", \"jit\") {\n+}\n+\n+void CompilationLog::log_compile(JavaThread* thread, CompileTask* task) {\n+  StringLogMessage lm;\n+  stringStream sstr(lm.buffer(), lm.size());\n+  \/\/ msg.time_stamp().update_to(tty->time_stamp().ticks());\n+  task->print(&sstr, NULL, true, false);\n+  log(thread, \"%s\", (const char*)lm);\n+}\n+\n+void CompilationLog::log_nmethod(JavaThread* thread, nmethod* nm) {\n+  log(thread, \"nmethod %d%s \" INTPTR_FORMAT \" code [\" INTPTR_FORMAT \", \" INTPTR_FORMAT \"]\",\n+      nm->compile_id(), nm->is_osr_method() ? \"%\" : \"\",\n+      p2i(nm), p2i(nm->code_begin()), p2i(nm->code_end()));\n+}\n+\n+void CompilationLog::log_failure(JavaThread* thread, CompileTask* task, const char* reason, const char* retry_message) {\n+  StringLogMessage lm;\n+  lm.print(\"%4d   COMPILE SKIPPED: %s\", task->compile_id(), reason);\n+  if (retry_message != NULL) {\n+    lm.append(\" (%s)\", retry_message);\n+  }\n+  lm.print(\"\\n\");\n+  log(thread, \"%s\", (const char*)lm);\n+}\n+\n+void CompilationLog::log_metaspace_failure(const char* reason) {\n+  \/\/ Note: This method can be called from non-Java\/compiler threads to\n+  \/\/ log the global metaspace failure that might affect profiling.\n+  ResourceMark rm;\n+  StringLogMessage lm;\n+  lm.print(\"%4d   COMPILE PROFILING SKIPPED: %s\", -1, reason);\n+  lm.print(\"\\n\");\n+  log(Thread::current(), \"%s\", (const char*)lm);\n+}\n+\n+void CompilationLog::init() {\n+  _log = new CompilationLog();\n+}\n","filename":"src\/hotspot\/share\/compiler\/compilationLog.cpp","additions":75,"deletions":0,"binary":false,"changes":75,"status":"added"},{"patch":"@@ -0,0 +1,52 @@\n+\/*\n+ * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_COMPILER_COMPILATIONLOG_HPP\n+#define SHARE_COMPILER_COMPILATIONLOG_HPP\n+\n+#include \"utilities\/events.hpp\"\n+\n+class CompileTask;\n+class JavaThread;\n+class nmethod;\n+\n+class CompilationLog : public StringEventLog {\n+private:\n+  static CompilationLog* _log;\n+\n+  CompilationLog();\n+\n+public:\n+\n+  void log_compile(JavaThread* thread, CompileTask* task);\n+  void log_nmethod(JavaThread* thread, nmethod* nm);\n+  void log_failure(JavaThread* thread, CompileTask* task, const char* reason, const char* retry_message);\n+  void log_metaspace_failure(const char* reason);\n+\n+  static void init();\n+  static CompilationLog* log() { return _log; }\n+  using StringEventLog::log;\n+};\n+\n+#endif \/\/ SHARE_COMPILER_COMPILATIONLOG_HPP\n","filename":"src\/hotspot\/share\/compiler\/compilationLog.hpp","additions":52,"deletions":0,"binary":false,"changes":52,"status":"added"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"compiler\/compilationLog.hpp\"\n@@ -66,1 +67,0 @@\n-#include \"runtime\/sweeper.hpp\"\n@@ -197,44 +197,0 @@\n-\n-\n-class CompilationLog : public StringEventLog {\n- public:\n-  CompilationLog() : StringEventLog(\"Compilation events\", \"jit\") {\n-  }\n-\n-  void log_compile(JavaThread* thread, CompileTask* task) {\n-    StringLogMessage lm;\n-    stringStream sstr(lm.buffer(), lm.size());\n-    \/\/ msg.time_stamp().update_to(tty->time_stamp().ticks());\n-    task->print(&sstr, NULL, true, false);\n-    log(thread, \"%s\", (const char*)lm);\n-  }\n-\n-  void log_nmethod(JavaThread* thread, nmethod* nm) {\n-    log(thread, \"nmethod %d%s \" INTPTR_FORMAT \" code [\" INTPTR_FORMAT \", \" INTPTR_FORMAT \"]\",\n-        nm->compile_id(), nm->is_osr_method() ? \"%\" : \"\",\n-        p2i(nm), p2i(nm->code_begin()), p2i(nm->code_end()));\n-  }\n-\n-  void log_failure(JavaThread* thread, CompileTask* task, const char* reason, const char* retry_message) {\n-    StringLogMessage lm;\n-    lm.print(\"%4d   COMPILE SKIPPED: %s\", task->compile_id(), reason);\n-    if (retry_message != NULL) {\n-      lm.append(\" (%s)\", retry_message);\n-    }\n-    lm.print(\"\\n\");\n-    log(thread, \"%s\", (const char*)lm);\n-  }\n-\n-  void log_metaspace_failure(const char* reason) {\n-    \/\/ Note: This method can be called from non-Java\/compiler threads to\n-    \/\/ log the global metaspace failure that might affect profiling.\n-    ResourceMark rm;\n-    StringLogMessage lm;\n-    lm.print(\"%4d   COMPILE PROFILING SKIPPED: %s\", -1, reason);\n-    lm.print(\"\\n\");\n-    log(Thread::current(), \"%s\", (const char*)lm);\n-  }\n-};\n-\n-static CompilationLog* _compilation_log = NULL;\n-\n@@ -243,1 +199,1 @@\n-    _compilation_log = new CompilationLog();\n+    CompilationLog::init();\n@@ -272,1 +228,0 @@\n-  task->set_code_handle(NULL);\n@@ -455,4 +410,1 @@\n-    \/\/ no need to invoke the sweeper. As a result, the hotness of methods\n-    \/\/ remains unchanged. This behavior is desired, since we want to keep\n-    \/\/ the stable state, i.e., we do not want to evict methods from the\n-    \/\/ code cache if it is unnecessary.\n+    \/\/ no need to invoke the GC.\n@@ -702,2 +654,2 @@\n-  \/\/ Start the compiler thread(s) and the sweeper thread\n-  init_compiler_sweeper_threads();\n+  \/\/ Start the compiler thread(s)\n+  init_compiler_threads();\n@@ -831,1 +783,1 @@\n-\/\/ CompileBroker::init_compiler_sweeper_threads() iff DeoptimizeObjectsALot is enabled\n+\/\/ CompileBroker::init_compiler_threads() iff DeoptimizeObjectsALot is enabled\n@@ -894,3 +846,0 @@\n-    case sweeper_t:\n-      new_thread = new CodeCacheSweeperThread();\n-      break;\n@@ -960,4 +909,1 @@\n-void CompileBroker::init_compiler_sweeper_threads() {\n-  NMethodSweeper::set_sweep_threshold_bytes(static_cast<size_t>(SweeperThreshold * ReservedCodeCacheSize \/ 100.0));\n-  log_info(codecache, sweep)(\"Sweeper threshold: \" SIZE_FORMAT \" bytes\", NMethodSweeper::sweep_threshold_bytes());\n-\n+void CompileBroker::init_compiler_threads() {\n@@ -1035,7 +981,0 @@\n-  if (MethodFlushing) {\n-    \/\/ Initialize the sweeper thread\n-    Handle thread_oop = create_thread_oop(\"Sweeper thread\", CHECK);\n-    jobject thread_handle = JNIHandles::make_local(THREAD, thread_oop());\n-    make_thread(sweeper_t, thread_handle, NULL, NULL, THREAD);\n-  }\n-\n@@ -1759,1 +1698,0 @@\n-    assert(task->code_handle() == NULL, \"must be reset\");\n@@ -1973,2 +1911,0 @@\n-      nmethodLocker result_handle;  \/\/ (handle for the nmethod produced by this task)\n-      task->set_code_handle(&result_handle);\n@@ -2049,2 +1985,2 @@\n-  if (_compilation_log != NULL) {\n-    _compilation_log->log_metaspace_failure(message);\n+  if (CompilationLog::log() != NULL) {\n+    CompilationLog::log()->log_metaspace_failure(message);\n@@ -2126,20 +2062,10 @@\n-void CompileBroker::post_compile(CompilerThread* thread, CompileTask* task, bool success, ciEnv* ci_env,\n-                                 int compilable, const char* failure_reason) {\n-  if (success) {\n-    task->mark_success();\n-    if (ci_env != NULL) {\n-      task->set_num_inlined_bytecodes(ci_env->num_inlined_bytecodes());\n-    }\n-    if (_compilation_log != NULL) {\n-      nmethod* code = task->code();\n-      if (code != NULL) {\n-        _compilation_log->log_nmethod(thread, code);\n-      }\n-    }\n-  } else if (AbortVMOnCompilationFailure) {\n-    if (compilable == ciEnv::MethodCompilable_not_at_tier) {\n-      fatal(\"Not compilable at tier %d: %s\", task->comp_level(), failure_reason);\n-    }\n-    if (compilable == ciEnv::MethodCompilable_never) {\n-      fatal(\"Never compilable: %s\", failure_reason);\n-    }\n+void CompileBroker::handle_compile_error(CompilerThread* thread, CompileTask* task, ciEnv* ci_env,\n+                                         int compilable, const char* failure_reason) {\n+  if (!AbortVMOnCompilationFailure) {\n+    return;\n+  }\n+  if (compilable == ciEnv::MethodCompilable_not_at_tier) {\n+    fatal(\"Not compilable at tier %d: %s\", task->comp_level(), failure_reason);\n+  }\n+  if (compilable == ciEnv::MethodCompilable_never) {\n+    fatal(\"Never compilable: %s\", failure_reason);\n@@ -2158,1 +2084,1 @@\n-                                        (task->code() == NULL) ? 0 : task->code()->total_size(),\n+                                        task->nm_total_size(),\n@@ -2182,2 +2108,2 @@\n-  if (LogEvents) {\n-    _compilation_log->log_compile(thread, task);\n+  if (CompilationLog::log() != NULL) {\n+    CompilationLog::log()->log_compile(thread, task);\n@@ -2206,0 +2132,1 @@\n+    task->set_directive(directive);\n@@ -2258,1 +2185,1 @@\n-      if (task->code() == NULL) {\n+      if (!task->is_success()) {\n@@ -2262,1 +2189,3 @@\n-    post_compile(thread, task, task->code() != NULL, NULL, compilable, failure_reason);\n+    if (!task->is_success()) {\n+      handle_compile_error(thread, task, NULL, compilable, failure_reason);\n+    }\n@@ -2323,1 +2252,3 @@\n-    if (!ci_env.failing() && task->code() == NULL) {\n+    DirectivesStack::release(directive);\n+\n+    if (!ci_env.failing() && !task->is_success()) {\n@@ -2339,1 +2270,3 @@\n-    post_compile(thread, task, !ci_env.failing(), &ci_env, compilable, failure_reason);\n+    if (ci_env.failing()) {\n+      handle_compile_error(thread, task, &ci_env, compilable, failure_reason);\n+    }\n@@ -2347,2 +2280,2 @@\n-    if (_compilation_log != NULL) {\n-      _compilation_log->log_failure(thread, task, failure_reason, retry_message);\n+    if (CompilationLog::log() != NULL) {\n+      CompilationLog::log()->log_failure(thread, task, failure_reason, retry_message);\n@@ -2364,6 +2297,0 @@\n-  nmethod* nm = task->code();\n-  if (nm != NULL) {\n-    nm->maybe_print_nmethod(directive);\n-  }\n-  DirectivesStack::release(directive);\n-\n@@ -2374,2 +2301,2 @@\n-    if (task->code() != NULL) {\n-      tty->print(\"size: %d(%d) \", task->code()->total_size(), task->code()->insts_size());\n+    if (task->is_success()) {\n+      tty->print(\"size: %d(%d) \", task->nm_total_size(), task->nm_insts_size());\n@@ -2448,1 +2375,1 @@\n-        NMethodSweeper::log_sweep(\"disable_compiler\");\n+        log_info(codecache)(\"Code cache is full - disabling compilation\");\n@@ -2515,1 +2442,0 @@\n-  nmethod* code = task->code();\n@@ -2518,1 +2444,0 @@\n-  assert(code == NULL || code->is_locked_by_vm(), \"will survive the MutexLocker\");\n@@ -2537,1 +2462,1 @@\n-  } else if (code == NULL) {\n+  } else if (!task->is_success()) {\n@@ -2571,2 +2496,2 @@\n-        stats->_nmethods_size += code->total_size();\n-        stats->_nmethods_code_size += code->insts_size();\n+        stats->_nmethods_size += task->nm_total_size();\n+        stats->_nmethods_code_size += task->nm_insts_size();\n@@ -2586,2 +2511,2 @@\n-        stats->_nmethods_size += code->total_size();\n-        stats->_nmethods_code_size += code->insts_size();\n+        stats->_nmethods_size += task->nm_total_size();\n+        stats->_nmethods_code_size += task->nm_insts_size();\n@@ -2616,2 +2541,2 @@\n-    _sum_nmethod_size      += code->total_size();\n-    _sum_nmethod_code_size += code->insts_size();\n+    _sum_nmethod_size      += task->nm_total_size();\n+    _sum_nmethod_code_size += task->nm_insts_size();\n@@ -2621,2 +2546,2 @@\n-      _perf_sum_nmethod_size->inc(     code->total_size());\n-      _perf_sum_nmethod_code_size->inc(code->insts_size());\n+      _perf_sum_nmethod_size->inc(     task->nm_total_size());\n+      _perf_sum_nmethod_code_size->inc(task->nm_insts_size());\n@@ -2780,8 +2705,0 @@\n-\n-  out->cr();\n-  out->print_cr(\"CodeCache cleaning overview\");\n-  out->print_cr(\"--------------------------------------------------------\");\n-  out->cr();\n-  NMethodSweeper::print(out);\n-  out->print_cr(\"--------------------------------------------------------\");\n-  out->cr();\n","filename":"src\/hotspot\/share\/compiler\/compileBroker.cpp","additions":47,"deletions":130,"binary":false,"changes":177,"status":"modified"},{"patch":"@@ -41,1 +41,0 @@\n-class nmethodLocker;\n@@ -233,1 +232,0 @@\n-    sweeper_t,\n@@ -239,1 +237,1 @@\n-  static void init_compiler_sweeper_threads();\n+  static void init_compiler_threads();\n@@ -258,2 +256,2 @@\n-  static void post_compile(CompilerThread* thread, CompileTask* task, bool success, ciEnv* ci_env,\n-                           int compilable, const char* failure_reason);\n+  static void handle_compile_error(CompilerThread* thread, CompileTask* task, ciEnv* ci_env,\n+                                   int compilable, const char* failure_reason);\n","filename":"src\/hotspot\/share\/compiler\/compileBroker.hpp","additions":3,"deletions":5,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -68,1 +68,0 @@\n-    task->set_code(NULL);\n@@ -113,1 +112,0 @@\n-  _code_handle = NULL;\n@@ -121,0 +119,4 @@\n+  _nm_content_size = 0;\n+  _directive = NULL;\n+  _nm_insts_size = 0;\n+  _nm_total_size = 0;\n@@ -164,19 +166,0 @@\n-\/\/ ------------------------------------------------------------------\n-\/\/ CompileTask::code\/set_code\n-\/\/\n-nmethod* CompileTask::code() const {\n-  if (_code_handle == NULL)  return NULL;\n-  CodeBlob *blob = _code_handle->code();\n-  if (blob != NULL) {\n-    return blob->as_nmethod();\n-  }\n-  return NULL;\n-}\n-\n-void CompileTask::set_code(nmethod* nm) {\n-  if (_code_handle == NULL && nm == NULL)  return;\n-  guarantee(_code_handle != NULL, \"\");\n-  _code_handle->set_code(nm);\n-  if (nm == NULL)  _code_handle = NULL;  \/\/ drop the handle also\n-}\n-\n@@ -260,3 +243,0 @@\n-  \/\/ For unloaded methods the transition to zombie occurs after the\n-  \/\/ method is cleared so it's impossible to report accurate\n-  \/\/ information for that case.\n@@ -402,1 +382,0 @@\n-  nmethod* nm = code();\n@@ -404,1 +383,1 @@\n-                  _is_success, nm == NULL ? 0 : nm->content_size(),\n+                  _is_success, _nm_content_size,\n","filename":"src\/hotspot\/share\/compiler\/compileTask.cpp","additions":5,"deletions":26,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -34,0 +34,2 @@\n+class DirectiveSet;\n+\n@@ -75,9 +77,13 @@\n-  static CompileTask* _task_free_list;\n-  Monitor*     _lock;\n-  uint         _compile_id;\n-  Method*      _method;\n-  jobject      _method_holder;\n-  int          _osr_bci;\n-  bool         _is_complete;\n-  bool         _is_success;\n-  bool         _is_blocking;\n+  static CompileTask*  _task_free_list;\n+  Monitor*             _lock;\n+  uint                 _compile_id;\n+  Method*              _method;\n+  jobject              _method_holder;\n+  int                  _osr_bci;\n+  bool                 _is_complete;\n+  bool                 _is_success;\n+  bool                 _is_blocking;\n+  CodeSection::csize_t _nm_content_size;\n+  CodeSection::csize_t _nm_total_size;\n+  CodeSection::csize_t _nm_insts_size;\n+  const DirectiveSet*  _directive;\n@@ -85,1 +91,1 @@\n-  bool         _has_waiter;\n+  bool                 _has_waiter;\n@@ -87,1 +93,1 @@\n-  JVMCICompileState* _blocking_jvmci_compile_state;\n+  JVMCICompileState*   _blocking_jvmci_compile_state;\n@@ -89,5 +95,4 @@\n-  int          _comp_level;\n-  int          _num_inlined_bytecodes;\n-  nmethodLocker* _code_handle;  \/\/ holder of eventual result\n-  CompileTask* _next, *_prev;\n-  bool         _is_free;\n+  int                  _comp_level;\n+  int                  _num_inlined_bytecodes;\n+  CompileTask*         _next, *_prev;\n+  bool                 _is_free;\n@@ -95,7 +100,7 @@\n-  jlong        _time_queued;  \/\/ time when task was enqueued\n-  jlong        _time_started; \/\/ time when compilation started\n-  Method*      _hot_method;   \/\/ which method actually triggered this task\n-  jobject      _hot_method_holder;\n-  int          _hot_count;    \/\/ information about its invocation counter\n-  CompileReason _compile_reason;      \/\/ more info about the task\n-  const char*  _failure_reason;\n+  jlong                _time_queued;  \/\/ time when task was enqueued\n+  jlong                _time_started; \/\/ time when compilation started\n+  Method*              _hot_method;   \/\/ which method actually triggered this task\n+  jobject              _hot_method_holder;\n+  int                  _hot_count;    \/\/ information about its invocation counter\n+  CompileReason        _compile_reason;      \/\/ more info about the task\n+  const char*          _failure_reason;\n@@ -103,1 +108,1 @@\n-  bool         _failure_reason_on_C_heap;\n+  bool                 _failure_reason_on_C_heap;\n@@ -125,0 +130,8 @@\n+  void         set_directive(const DirectiveSet* directive) { _directive = directive; }\n+  const DirectiveSet* directive() const          { return _directive; }\n+  CodeSection::csize_t nm_content_size() { return _nm_content_size; }\n+  void         set_nm_content_size(CodeSection::csize_t size) { _nm_content_size = size; }\n+  CodeSection::csize_t nm_insts_size() { return _nm_insts_size; }\n+  void         set_nm_insts_size(CodeSection::csize_t size) { _nm_insts_size = size; }\n+  CodeSection::csize_t nm_total_size() { return _nm_total_size; }\n+  void         set_nm_total_size(CodeSection::csize_t size) { _nm_total_size = size; }\n@@ -156,5 +169,0 @@\n-  nmethodLocker* code_handle() const             { return _code_handle; }\n-  void         set_code_handle(nmethodLocker* l) { _code_handle = l; }\n-  nmethod*     code() const;                     \/\/ _code_handle->code()\n-  void         set_code(nmethod* nm);            \/\/ _code_handle->set_code(nm)\n-\n","filename":"src\/hotspot\/share\/compiler\/compileTask.hpp","additions":37,"deletions":29,"binary":false,"changes":66,"status":"modified"},{"patch":"@@ -597,13 +597,0 @@\n-  if (FLAG_IS_DEFAULT(SweeperThreshold)) {\n-    if (Continuations::enabled()) {\n-      \/\/ When continuations are enabled, the sweeper needs to trigger GC to\n-      \/\/ be able to sweep nmethods. Therefore, it's in general a good idea\n-      \/\/ to be significantly less aggressive with sweeping, in order not to\n-      \/\/ trigger excessive GC work.\n-      FLAG_SET_ERGO(SweeperThreshold, SweeperThreshold * 10.0);\n-    } else if ((SweeperThreshold * ReservedCodeCacheSize \/ 100) > (1.2 * M)) {\n-      \/\/ Cap default SweeperThreshold value to an equivalent of 1.2 Mb\n-      FLAG_SET_ERGO(SweeperThreshold, (1.2 * M * 100) \/ ReservedCodeCacheSize);\n-    }\n-  }\n-\n","filename":"src\/hotspot\/share\/compiler\/compilerDefinitions.cpp","additions":0,"deletions":13,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -30,1 +30,0 @@\n-#include \"runtime\/sweeper.hpp\"\n@@ -65,31 +64,0 @@\n-\n-\/\/ Create sweeper thread\n-CodeCacheSweeperThread::CodeCacheSweeperThread()\n-: JavaThread(&CodeCacheSweeperThread::thread_entry) {\n-  _scanned_compiled_method = NULL;\n-}\n-\n-void CodeCacheSweeperThread::thread_entry(JavaThread* thread, TRAPS) {\n-  NMethodSweeper::sweeper_loop();\n-}\n-\n-void CodeCacheSweeperThread::oops_do_no_frames(OopClosure* f, CodeBlobClosure* cf) {\n-  JavaThread::oops_do_no_frames(f, cf);\n-  if (_scanned_compiled_method != NULL && cf != NULL) {\n-    \/\/ Safepoints can occur when the sweeper is scanning an nmethod so\n-    \/\/ process it here to make sure it isn't unloaded in the middle of\n-    \/\/ a scan.\n-    cf->do_code_blob(_scanned_compiled_method);\n-  }\n-}\n-\n-void CodeCacheSweeperThread::nmethods_do(CodeBlobClosure* cf) {\n-  JavaThread::nmethods_do(cf);\n-  if (_scanned_compiled_method != NULL && cf != NULL) {\n-    \/\/ Safepoints can occur when the sweeper is scanning an nmethod so\n-    \/\/ process it here to make sure it isn't unloaded in the middle of\n-    \/\/ a scan.\n-    cf->do_code_blob(_scanned_compiled_method);\n-  }\n-}\n-\n","filename":"src\/hotspot\/share\/compiler\/compilerThread.cpp","additions":0,"deletions":32,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -119,25 +119,0 @@\n-\/\/ Dedicated thread to sweep the code cache\n-class CodeCacheSweeperThread : public JavaThread {\n-  CompiledMethod*       _scanned_compiled_method; \/\/ nmethod being scanned by the sweeper\n-\n-  static void thread_entry(JavaThread* thread, TRAPS);\n-\n- public:\n-  CodeCacheSweeperThread();\n-  \/\/ Track the nmethod currently being scanned by the sweeper\n-  void set_scanned_compiled_method(CompiledMethod* cm) {\n-    assert(_scanned_compiled_method == NULL || cm == NULL, \"should reset to NULL before writing a new value\");\n-    _scanned_compiled_method = cm;\n-  }\n-\n-  \/\/ Hide sweeper thread from external view.\n-  bool is_hidden_from_external_view() const { return true; }\n-\n-  bool is_Code_cache_sweeper_thread() const { return true; }\n-\n-  \/\/ Prevent GC from unloading _scanned_compiled_method\n-  void oops_do_no_frames(OopClosure* f, CodeBlobClosure* cf);\n-  void nmethods_do(CodeBlobClosure* cf);\n-};\n-\n-\n","filename":"src\/hotspot\/share\/compiler\/compilerThread.hpp","additions":0,"deletions":25,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -126,1 +126,0 @@\n-  virtual void flush_nmethod(nmethod* nm) {}\n","filename":"src\/hotspot\/share\/gc\/epsilon\/epsilonHeap.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -84,2 +84,2 @@\n-    \/\/ CodeCache sweeper support\n-    nm->mark_as_maybe_on_continuation();\n+    \/\/ CodeCache unloading support\n+    nm->mark_as_maybe_on_stack();\n@@ -100,2 +100,2 @@\n-  \/\/ CodeCache sweeper support\n-  nm->mark_as_maybe_on_continuation();\n+  \/\/ CodeCache unloading support\n+  nm->mark_as_maybe_on_stack();\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CodeBlobClosure.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -1891,0 +1891,1 @@\n+    case GCCause::_codecache_GC_aggressive: return true;\n@@ -3440,1 +3441,1 @@\n-  if (!Continuations::is_gc_marking_cycle_active()) {\n+  if (!CodeCache::is_gc_marking_cycle_active()) {\n@@ -3447,2 +3448,2 @@\n-    Continuations::on_gc_marking_cycle_start();\n-    Continuations::arm_all_nmethods();\n+    CodeCache::on_gc_marking_cycle_start();\n+    CodeCache::arm_all_nmethods();\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectedHeap.cpp","additions":4,"deletions":3,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -1243,3 +1243,0 @@\n-  \/\/ No nmethod flushing needed.\n-  void flush_nmethod(nmethod* nm) override {}\n-\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectedHeap.hpp","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -70,1 +70,0 @@\n-#include \"runtime\/continuation.hpp\"\n@@ -1314,2 +1313,2 @@\n-  Continuations::on_gc_marking_cycle_finish();\n-  Continuations::arm_all_nmethods();\n+  CodeCache::on_gc_marking_cycle_finish();\n+  CodeCache::arm_all_nmethods();\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ConcurrentMark.cpp","additions":2,"deletions":3,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -47,1 +47,0 @@\n-#include \"runtime\/continuation.hpp\"\n@@ -213,2 +212,2 @@\n-  Continuations::on_gc_marking_cycle_finish();\n-  Continuations::arm_all_nmethods();\n+  CodeCache::on_gc_marking_cycle_finish();\n+  CodeCache::arm_all_nmethods();\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullCollector.cpp","additions":2,"deletions":3,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -1083,0 +1083,1 @@\n+               (cause == GCCause::_codecache_GC_aggressive) ||\n","filename":"src\/hotspot\/share\/gc\/g1\/g1Policy.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -370,2 +370,8 @@\n-      if (!nm->is_alive()) {\n-        log_error(gc, verify)(\"region [\" PTR_FORMAT \",\" PTR_FORMAT \"] has dead nmethod \" PTR_FORMAT \" in its code roots\",\n+      VerifyCodeRootOopClosure oop_cl(_hr);\n+      nm->oops_do(&oop_cl);\n+      if (!oop_cl.has_oops_in_region()) {\n+        log_error(gc, verify)(\"region [\" PTR_FORMAT \",\" PTR_FORMAT \"] has nmethod \" PTR_FORMAT \" in its code roots with no pointers into region\",\n+                              p2i(_hr->bottom()), p2i(_hr->end()), p2i(nm));\n+        _failures = true;\n+      } else if (oop_cl.failures()) {\n+        log_error(gc, verify)(\"region [\" PTR_FORMAT \",\" PTR_FORMAT \"] has other failures for nmethod \" PTR_FORMAT,\n@@ -374,12 +380,0 @@\n-      } else {\n-        VerifyCodeRootOopClosure oop_cl(_hr);\n-        nm->oops_do(&oop_cl);\n-        if (!oop_cl.has_oops_in_region()) {\n-          log_error(gc, verify)(\"region [\" PTR_FORMAT \",\" PTR_FORMAT \"] has nmethod \" PTR_FORMAT \" in its code roots with no pointers into region\",\n-                                p2i(_hr->bottom()), p2i(_hr->end()), p2i(nm));\n-          _failures = true;\n-        } else if (oop_cl.failures()) {\n-          log_error(gc, verify)(\"region [\" PTR_FORMAT \",\" PTR_FORMAT \"] has other failures for nmethod \" PTR_FORMAT,\n-                                p2i(_hr->bottom()), p2i(_hr->end()), p2i(nm));\n-          _failures = true;\n-        }\n","filename":"src\/hotspot\/share\/gc\/g1\/heapRegion.cpp","additions":8,"deletions":14,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -845,4 +845,0 @@\n-void ParallelScavengeHeap::flush_nmethod(nmethod* nm) {\n-  \/\/ nothing particular\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/parallel\/parallelScavengeHeap.cpp","additions":0,"deletions":4,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -176,1 +176,0 @@\n-  virtual void flush_nmethod(nmethod* nm);\n","filename":"src\/hotspot\/share\/gc\/parallel\/parallelScavengeHeap.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -78,1 +78,0 @@\n-#include \"runtime\/continuation.hpp\"\n@@ -965,2 +964,2 @@\n-  Continuations::on_gc_marking_cycle_start();\n-  Continuations::arm_all_nmethods();\n+  CodeCache::on_gc_marking_cycle_start();\n+  CodeCache::arm_all_nmethods();\n@@ -998,2 +997,2 @@\n-  Continuations::on_gc_marking_cycle_finish();\n-  Continuations::arm_all_nmethods();\n+  CodeCache::on_gc_marking_cycle_finish();\n+  CodeCache::arm_all_nmethods();\n","filename":"src\/hotspot\/share\/gc\/parallel\/psParallelCompact.cpp","additions":4,"deletions":5,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -58,3 +58,0 @@\n-  } else if (Continuations::enabled()) {\n-    \/\/ The GC needs nmethod entry barriers to deal with continuations\n-    return new BarrierSetNMethod();\n@@ -62,2 +59,3 @@\n-    \/\/ The GC does not need nmethod entry barriers\n-    return NULL;\n+    \/\/ The GC needs nmethod entry barriers to deal with continuations\n+    \/\/ and code cache unloading\n+    return NOT_ARM32(new BarrierSetNMethod()) ARM32_ONLY(nullptr);\n@@ -80,2 +78,2 @@\n-  if (Continuations::enabled()) {\n-    BarrierSetNMethod* bs_nm = barrier_set_nmethod();\n+  BarrierSetNMethod* bs_nm = barrier_set_nmethod();\n+  if (bs_nm != nullptr) {\n","filename":"src\/hotspot\/share\/gc\/shared\/barrierSet.cpp","additions":5,"deletions":7,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -76,2 +76,2 @@\n-  \/\/ CodeCache sweeper support\n-  nm->mark_as_maybe_on_continuation();\n+  \/\/ CodeCache unloading support\n+  nm->mark_as_maybe_on_stack();\n","filename":"src\/hotspot\/share\/gc\/shared\/barrierSetNMethod.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -293,0 +293,1 @@\n+    case GCCause::_codecache_GC_aggressive:\n@@ -295,1 +296,1 @@\n-    case GCCause::_metadata_GC_threshold : {\n+    case GCCause::_metadata_GC_threshold: {\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -482,2 +482,0 @@\n-  \/\/ Callback for when nmethod is about to be deleted.\n-  virtual void flush_nmethod(nmethod* nm) = 0;\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.hpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -32,0 +32,4 @@\n+bool IsUnloadingBehaviour::is_unloading(CompiledMethod* cm) {\n+  return _current->has_dead_oop(cm) || cm->as_nmethod()->is_cold();\n+}\n+\n@@ -64,1 +68,1 @@\n-bool ClosureIsUnloadingBehaviour::is_unloading(CompiledMethod* cm) const {\n+bool ClosureIsUnloadingBehaviour::has_dead_oop(CompiledMethod* cm) const {\n","filename":"src\/hotspot\/share\/gc\/shared\/gcBehaviours.cpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -37,1 +37,2 @@\n-  virtual bool is_unloading(CompiledMethod* cm) const = 0;\n+  static bool is_unloading(CompiledMethod* cm);\n+  virtual bool has_dead_oop(CompiledMethod* cm) const = 0;\n@@ -50,1 +51,1 @@\n-  virtual bool is_unloading(CompiledMethod* cm) const;\n+  virtual bool has_dead_oop(CompiledMethod* cm) const;\n","filename":"src\/hotspot\/share\/gc\/shared\/gcBehaviours.hpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -78,0 +78,3 @@\n+    case _codecache_GC_aggressive:\n+      return \"CodeCache GC Aggressive\";\n+\n","filename":"src\/hotspot\/share\/gc\/shared\/gcCause.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -67,0 +67,1 @@\n+    _codecache_GC_aggressive,\n","filename":"src\/hotspot\/share\/gc\/shared\/gcCause.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -66,1 +66,0 @@\n-#include \"runtime\/continuation.hpp\"\n@@ -610,2 +609,2 @@\n-    Continuations::on_gc_marking_cycle_start();\n-    Continuations::arm_all_nmethods();\n+    CodeCache::on_gc_marking_cycle_start();\n+    CodeCache::arm_all_nmethods();\n@@ -620,2 +619,2 @@\n-    Continuations::on_gc_marking_cycle_finish();\n-    Continuations::arm_all_nmethods();\n+    CodeCache::on_gc_marking_cycle_finish();\n+    CodeCache::arm_all_nmethods();\n@@ -664,4 +663,0 @@\n-void GenCollectedHeap::flush_nmethod(nmethod* nm) {\n-  \/\/ Do nothing.\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/shared\/genCollectedHeap.cpp","additions":4,"deletions":9,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -214,1 +214,0 @@\n-  virtual void flush_nmethod(nmethod* nm);\n","filename":"src\/hotspot\/share\/gc\/shared\/genCollectedHeap.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -42,1 +42,1 @@\n-  CompiledMethodIterator iter(CompiledMethodIterator::only_alive);\n+  CompiledMethodIterator iter(CompiledMethodIterator::all_blobs);\n@@ -56,1 +56,1 @@\n-  CompiledMethodIterator last(CompiledMethodIterator::only_alive);\n+  CompiledMethodIterator last(CompiledMethodIterator::all_blobs);\n@@ -62,1 +62,1 @@\n-    last = CompiledMethodIterator(CompiledMethodIterator::only_alive, first);\n+    last = CompiledMethodIterator(CompiledMethodIterator::all_blobs, first);\n","filename":"src\/hotspot\/share\/gc\/shared\/parallelCleaning.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -152,2 +152,0 @@\n-    assert(cur->is_alive(), \"Must be\");\n-\n@@ -218,1 +216,1 @@\n-  NMethodIterator iter(NMethodIterator::only_alive);\n+  NMethodIterator iter(NMethodIterator::all_blobs);\n@@ -231,1 +229,1 @@\n-  NMethodIterator iter(NMethodIterator::only_alive);\n+  NMethodIterator iter(NMethodIterator::all_blobs);\n","filename":"src\/hotspot\/share\/gc\/shared\/scavengableNMethods.cpp","additions":2,"deletions":4,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -66,2 +66,2 @@\n-  \/\/ CodeCache sweeper support\n-  nm->mark_as_maybe_on_continuation();\n+  \/\/ CodeCache unloading support\n+  nm->mark_as_maybe_on_stack();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahBarrierSetNMethod.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -93,2 +93,1 @@\n-      if (cb->is_alive()) {\n-        f->do_code_blob(cb);\n+      f->do_code_blob(cb);\n@@ -96,2 +95,2 @@\n-        if (cb->is_nmethod())\n-          Universe::heap()->verify_nmethod((nmethod*)cb);\n+      if (cb->is_nmethod())\n+        Universe::heap()->verify_nmethod((nmethod*)cb);\n@@ -99,1 +98,0 @@\n-      }\n@@ -123,5 +121,0 @@\n-void ShenandoahCodeRoots::flush_nmethod(nmethod* nm) {\n-  assert(CodeCache_lock->owned_by_self(), \"Must have CodeCache_lock held\");\n-  _nmethod_table->flush_nmethod(nm);\n-}\n-\n@@ -190,16 +183,0 @@\n-   void unlink(nmethod* nm) {\n-     \/\/ Unlinking of the dependencies must happen before the\n-     \/\/ handshake separating unlink and purge.\n-     nm->flush_dependencies(false \/* delete_immediately *\/);\n-\n-     \/\/ unlink_from_method will take the CompiledMethod_lock.\n-     \/\/ In this case we don't strictly need it when unlinking nmethods from\n-     \/\/ the Method, because it is only concurrently unlinked by\n-     \/\/ the entry barrier, which acquires the per nmethod lock.\n-     nm->unlink_from_method();\n-\n-     if (nm->is_osr_method()) {\n-       \/\/ Invalidate the osr nmethod only once\n-       nm->invalidate_osr_method();\n-     }\n-   }\n@@ -222,4 +199,0 @@\n-    if (!nm->is_alive()) {\n-      return;\n-    }\n-\n@@ -228,1 +201,1 @@\n-      unlink(nm);\n+      nm->unlink();\n@@ -238,7 +211,3 @@\n-      if (Continuations::enabled()) {\n-        \/\/ Loom needs to know about visited nmethods. Arm the nmethods to get\n-        \/\/ mark_as_maybe_on_continuation() callbacks when they are used again.\n-        _bs->arm(nm, 0);\n-      } else {\n-        _bs->disarm(nm);\n-      }\n+      \/\/ Code cache unloading needs to know about on-stack nmethods. Arm the nmethods to get\n+      \/\/ mark_as_maybe_on_stack() callbacks when they are used again.\n+      _bs->arm(nm, 0);\n@@ -311,34 +280,1 @@\n-class ShenandoahNMethodPurgeClosure : public NMethodClosure {\n-public:\n-  virtual void do_nmethod(nmethod* nm) {\n-    if (nm->is_alive() && nm->is_unloading()) {\n-      nm->make_unloaded();\n-    }\n-  }\n-};\n-\n-class ShenandoahNMethodPurgeTask : public WorkerTask {\n-private:\n-  ShenandoahNMethodPurgeClosure       _cl;\n-  ShenandoahConcurrentNMethodIterator _iterator;\n-\n-public:\n-  ShenandoahNMethodPurgeTask() :\n-    WorkerTask(\"Shenandoah Purge NMethods\"),\n-    _cl(),\n-    _iterator(ShenandoahCodeRoots::table()) {\n-    MutexLocker mu(CodeCache_lock, Mutex::_no_safepoint_check_flag);\n-    _iterator.nmethods_do_begin();\n-  }\n-\n-  ~ShenandoahNMethodPurgeTask() {\n-    MutexLocker mu(CodeCache_lock, Mutex::_no_safepoint_check_flag);\n-    _iterator.nmethods_do_end();\n-  }\n-\n-  virtual void work(uint worker_id) {\n-    _iterator.nmethods_do(&_cl);\n-  }\n-};\n-\n-void ShenandoahCodeRoots::purge(WorkerThreads* workers) {\n+void ShenandoahCodeRoots::purge() {\n@@ -347,2 +283,1 @@\n-  ShenandoahNMethodPurgeTask task;\n-  workers->run_task(&task);\n+  CodeCache::flush_unlinked_nmethods();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahCodeRoots.cpp","additions":9,"deletions":74,"binary":false,"changes":83,"status":"modified"},{"patch":"@@ -99,1 +99,1 @@\n-  static void purge(WorkerThreads* workers);\n+  static void purge();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahCodeRoots.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -482,0 +482,1 @@\n+         cause == GCCause::_codecache_GC_aggressive ||\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahControlThread.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -1916,4 +1916,0 @@\n-void ShenandoahHeap::flush_nmethod(nmethod* nm) {\n-  ShenandoahCodeRoots::flush_nmethod(nm);\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":0,"deletions":4,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -505,1 +505,0 @@\n-  void flush_nmethod(nmethod* nm);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -36,1 +36,0 @@\n-#include \"runtime\/continuation.hpp\"\n@@ -50,3 +49,2 @@\n-  \/\/ Tell the sweeper that we start a marking cycle.\n-  if (!Continuations::is_gc_marking_cycle_active()) {\n-    Continuations::on_gc_marking_cycle_start();\n+  if (!CodeCache::is_gc_marking_cycle_active()) {\n+    CodeCache::on_gc_marking_cycle_start();\n@@ -57,1 +55,0 @@\n-  \/\/ Tell the sweeper that we finished a marking cycle.\n@@ -60,1 +57,1 @@\n-  Continuations::on_gc_marking_cycle_finish();\n+  CodeCache::on_gc_marking_cycle_finish();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMark.cpp","additions":3,"deletions":6,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -171,1 +171,0 @@\n-    assert(heap->cancelled_gc() || Continuations::enabled(), \"What else?\");\n@@ -303,3 +302,0 @@\n-  if (Thread::current()->is_Code_cache_sweeper_thread()) {\n-    wait_until_concurrent_iteration_done();\n-  }\n@@ -310,15 +306,0 @@\n-  ShenandoahReentrantLocker data_locker(data->lock());\n-  data->mark_unregistered();\n-}\n-\n-void ShenandoahNMethodTable::flush_nmethod(nmethod* nm) {\n-  assert(CodeCache_lock->owned_by_self(), \"Must have CodeCache_lock held\");\n-  assert(Thread::current()->is_Code_cache_sweeper_thread(), \"Must from Sweep thread\");\n-  ShenandoahNMethod* data = ShenandoahNMethod::gc_data(nm);\n-  assert(data != NULL, \"Sanity\");\n-\n-  \/\/ Can not alter the array when iteration is in progress\n-  wait_until_concurrent_iteration_done();\n-  log_flush_nmethod(nm);\n-\n-  ShenandoahLocker locker(&_lock);\n@@ -351,1 +332,0 @@\n-  assert(!iteration_in_progress(), \"Can not happen\");\n@@ -432,10 +412,0 @@\n-void ShenandoahNMethodTable::log_flush_nmethod(nmethod* nm) {\n-  LogTarget(Debug, gc, nmethod) log;\n-  if (!log.is_enabled()) {\n-    return;\n-  }\n-\n-  ResourceMark rm;\n-  log.print(\"Flush NMethod: (\" PTR_FORMAT \")\", p2i(nm));\n-}\n-\n@@ -516,5 +486,2 @@\n-      \/\/ A nmethod can become a zombie before it is unregistered.\n-      if (nmr->nm()->is_alive()) {\n-        nmr->assert_correct();\n-        f->do_code_blob(nmr->nm());\n-      }\n+      nmr->assert_correct();\n+      f->do_code_blob(nmr->nm());\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahNMethod.cpp","additions":2,"deletions":35,"binary":false,"changes":37,"status":"modified"},{"patch":"@@ -153,1 +153,0 @@\n-  void flush_nmethod(nmethod* nm);\n@@ -183,1 +182,0 @@\n-  void log_flush_nmethod(nmethod* nm);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahNMethod.hpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -79,1 +79,1 @@\n-  virtual bool is_unloading(CompiledMethod* method) const {\n+  virtual bool has_dead_oop(CompiledMethod* method) const {\n@@ -179,1 +179,1 @@\n-      ShenandoahCodeRoots::purge(heap->workers());\n+      ShenandoahCodeRoots::purge();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahUnload.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -353,1 +353,1 @@\n-  assert(ZResurrection::is_blocked() || (Continuations::enabled() && CodeCache::contains((void*)p)),\n+  assert(ZResurrection::is_blocked() || (CodeCache::contains((void*)p)),\n","filename":"src\/hotspot\/share\/gc\/z\/zBarrier.inline.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -62,2 +62,2 @@\n-  \/\/ CodeCache sweeper support\n-  nm->mark_as_maybe_on_continuation();\n+  \/\/ CodeCache unloading support\n+  nm->mark_as_maybe_on_stack();\n","filename":"src\/hotspot\/share\/gc\/z\/zBarrierSetNMethod.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -268,4 +268,0 @@\n-void ZCollectedHeap::flush_nmethod(nmethod* nm) {\n-  ZNMethod::flush_nmethod(nm);\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/z\/zCollectedHeap.cpp","additions":0,"deletions":4,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -105,1 +105,0 @@\n-  virtual void flush_nmethod(nmethod* nm);\n","filename":"src\/hotspot\/share\/gc\/z\/zCollectedHeap.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -233,1 +233,1 @@\n-  case GCCause::_codecache_GC_threshold:\n+  case GCCause::_codecache_GC_aggressive:\n@@ -244,0 +244,1 @@\n+  case GCCause::_codecache_GC_threshold:\n","filename":"src\/hotspot\/share\/gc\/z\/zDriver.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -114,1 +114,1 @@\n-  \/\/ Tell the sweeper that we start a marking cycle.\n+  \/\/ Note that we start a marking cycle.\n@@ -118,1 +118,1 @@\n-  Continuations::on_gc_marking_cycle_start();\n+  CodeCache::on_gc_marking_cycle_start();\n@@ -698,4 +698,0 @@\n-    if (!nm->is_alive()) {\n-      return;\n-    }\n-\n@@ -705,2 +701,2 @@\n-      \/\/ CodeCache sweeper support\n-      nm->mark_as_maybe_on_continuation();\n+      \/\/ CodeCache unloading support\n+      nm->mark_as_maybe_on_stack();\n@@ -829,1 +825,1 @@\n-  \/\/ Tell the sweeper that we finished a marking cycle.\n+  \/\/ Note that we finished a marking cycle.\n@@ -832,1 +828,1 @@\n-  Continuations::on_gc_marking_cycle_finish();\n+  CodeCache::on_gc_marking_cycle_finish();\n","filename":"src\/hotspot\/share\/gc\/z\/zMark.cpp","additions":6,"deletions":10,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -175,8 +175,0 @@\n-  assert(CodeCache_lock->owned_by_self(), \"Lock must be held\");\n-\n-  if (Thread::current()->is_Code_cache_sweeper_thread()) {\n-    \/\/ The sweeper must wait for any ongoing iteration to complete\n-    \/\/ before it can unregister an nmethod.\n-    ZNMethodTable::wait_until_iteration_done();\n-  }\n-\n@@ -188,1 +180,0 @@\n-}\n@@ -190,1 +181,0 @@\n-void ZNMethod::flush_nmethod(nmethod* nm) {\n@@ -219,4 +209,0 @@\n-  if (!nm->is_alive()) {\n-    return;\n-  }\n-\n@@ -298,19 +284,0 @@\n-  void unlink(nmethod* nm) {\n-    \/\/ Unlinking of the dependencies must happen before the\n-    \/\/ handshake separating unlink and purge.\n-    nm->flush_dependencies(false \/* delete_immediately *\/);\n-\n-    \/\/ unlink_from_method will take the CompiledMethod_lock.\n-    \/\/ In this case we don't strictly need it when unlinking nmethods from\n-    \/\/ the Method, because it is only concurrently unlinked by\n-    \/\/ the entry barrier, which acquires the per nmethod lock.\n-    nm->unlink_from_method();\n-\n-    if (nm->is_osr_method()) {\n-      \/\/ Invalidate the osr nmethod before the handshake. The nmethod\n-      \/\/ will be made unloaded after the handshake. Then invalidate_osr_method()\n-      \/\/ will be called again, which will be a no-op.\n-      nm->invalidate_osr_method();\n-    }\n-  }\n-\n@@ -327,4 +294,0 @@\n-    if (!nm->is_alive()) {\n-      return;\n-    }\n-\n@@ -333,1 +296,1 @@\n-      unlink(nm);\n+      nm->unlink();\n@@ -342,8 +305,1 @@\n-\n-      if (Continuations::enabled()) {\n-        \/\/ Loom needs to know about visited nmethods. Arm the nmethods to get\n-        \/\/ mark_as_maybe_on_continuation() callbacks when they are used again.\n-        ZNMethod::arm(nm, 0);\n-      } else {\n-        ZNMethod::disarm(nm);\n-      }\n+      ZNMethod::arm(nm, 0);\n@@ -410,32 +366,2 @@\n-class ZNMethodPurgeClosure : public NMethodClosure {\n-public:\n-  virtual void do_nmethod(nmethod* nm) {\n-    if (nm->is_alive() && nm->is_unloading()) {\n-      nm->make_unloaded();\n-    }\n-  }\n-};\n-\n-class ZNMethodPurgeTask : public ZTask {\n-private:\n-  ZNMethodPurgeClosure _cl;\n-\n-public:\n-  ZNMethodPurgeTask() :\n-      ZTask(\"ZNMethodPurgeTask\"),\n-      _cl() {\n-    ZNMethodTable::nmethods_do_begin();\n-  }\n-\n-  ~ZNMethodPurgeTask() {\n-    ZNMethodTable::nmethods_do_end();\n-  }\n-\n-  virtual void work() {\n-    ZNMethodTable::nmethods_do(&_cl);\n-  }\n-};\n-\n-void ZNMethod::purge(ZWorkers* workers) {\n-  ZNMethodPurgeTask task;\n-  workers->run(&task);\n+void ZNMethod::purge() {\n+  CodeCache::flush_unlinked_nmethods();\n","filename":"src\/hotspot\/share\/gc\/z\/zNMethod.cpp","additions":4,"deletions":78,"binary":false,"changes":82,"status":"modified"},{"patch":"@@ -44,1 +44,0 @@\n-  static void flush_nmethod(nmethod* nm);\n@@ -64,1 +63,1 @@\n-  static void purge(ZWorkers* workers);\n+  static void purge();\n","filename":"src\/hotspot\/share\/gc\/z\/zNMethod.hpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -78,1 +78,1 @@\n-  virtual bool is_unloading(CompiledMethod* method) const {\n+  virtual bool has_dead_oop(CompiledMethod* method) const {\n@@ -165,1 +165,1 @@\n-    ZNMethod::purge(_workers);\n+    ZNMethod::purge();\n","filename":"src\/hotspot\/share\/gc\/z\/zUnload.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -97,1 +97,1 @@\n-  \/\/ exclude compiler threads and code sweeper thread\n+  \/\/ exclude compiler threads\n","filename":"src\/hotspot\/share\/jfr\/leakprofiler\/leakProfiler.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -564,1 +564,1 @@\n-    <Field type=\"int\" contentType=\"bytes\" name=\"freedMemory\" label=\"Freed Memory\" \/>\n+    <Field type=\"ulong\" contentType=\"bytes\" name=\"freedMemory\" label=\"Freed Memory\" \/>\n@@ -605,7 +605,0 @@\n-  <Event name=\"SweepCodeCache\" category=\"Java Virtual Machine, Code Sweeper\" label=\"Sweep Code Cache\" thread=\"true\" >\n-    <Field type=\"int\" name=\"sweepId\" label=\"Sweep Identifier\" relation=\"SweepId\" \/>\n-    <Field type=\"uint\" name=\"sweptCount\" label=\"Methods Swept\" \/>\n-    <Field type=\"uint\" name=\"flushedCount\" label=\"Methods Flushed\" \/>\n-    <Field type=\"uint\" name=\"zombifiedCount\" label=\"Methods Zombified\" \/>\n-  <\/Event>\n-\n@@ -933,14 +926,0 @@\n-  <Event name=\"CodeSweeperStatistics\" category=\"Java Virtual Machine, Code Sweeper\" label=\"Code Sweeper Statistics\" thread=\"false\" period=\"everyChunk\" startTime=\"false\">\n-    <Field type=\"int\" name=\"sweepCount\" label=\"Sweeps\" \/>\n-    <Field type=\"int\" name=\"methodReclaimedCount\" label=\"Methods Reclaimed\" \/>\n-    <Field type=\"Tickspan\" name=\"totalSweepTime\" label=\"Time Spent Sweeping\" \/>\n-    <Field type=\"Tickspan\" name=\"peakFractionTime\" label=\"Peak Time Fraction Sweep\" \/>\n-    <Field type=\"Tickspan\" name=\"peakSweepTime\" label=\"Peak Time Full Sweep\" \/>\n-  <\/Event>\n-\n-  <Event name=\"CodeSweeperConfiguration\" category=\"Java Virtual Machine, Code Sweeper\" label=\"Code Sweeper Configuration\" thread=\"false\" period=\"endChunk\" startTime=\"false\">\n-    <Field type=\"boolean\" name=\"sweeperEnabled\" label=\"Code Sweeper Enabled\" \/>\n-    <Field type=\"boolean\" name=\"flushingEnabled\" label=\"Code Cache Flushing Enabled\" \/>\n-    <Field type=\"ulong\" contentType=\"bytes\" name=\"sweepThreshold\" label=\"Sweep Threshold\" \/>\n-  <\/Event>\n-\n","filename":"src\/hotspot\/share\/jfr\/metadata\/metadata.xml","additions":1,"deletions":22,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -62,1 +62,0 @@\n-#include \"runtime\/sweeper.hpp\"\n@@ -611,18 +610,0 @@\n-TRACE_REQUEST_FUNC(CodeSweeperStatistics) {\n-  EventCodeSweeperStatistics event;\n-  event.set_sweepCount(NMethodSweeper::traversal_count());\n-  event.set_methodReclaimedCount(NMethodSweeper::total_nof_methods_reclaimed());\n-  event.set_totalSweepTime(NMethodSweeper::total_time_sweeping());\n-  event.set_peakFractionTime(NMethodSweeper::peak_sweep_fraction_time());\n-  event.set_peakSweepTime(NMethodSweeper::peak_sweep_time());\n-  event.commit();\n-}\n-\n-TRACE_REQUEST_FUNC(CodeSweeperConfiguration) {\n-  EventCodeSweeperConfiguration event;\n-  event.set_sweeperEnabled(MethodFlushing);\n-  event.set_flushingEnabled(UseCodeCacheFlushing);\n-  event.set_sweepThreshold(NMethodSweeper::sweep_threshold_bytes());\n-  event.commit();\n-}\n-\n","filename":"src\/hotspot\/share\/jfr\/periodic\/jfrPeriodic.cpp","additions":0,"deletions":19,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -110,1 +110,1 @@\n-     nmethod_reclaimed, \/\/ code cache sweeper reclaimed nmethod in between its creation and being marked \"in_use\"\n+     nmethod_reclaimed,\n","filename":"src\/hotspot\/share\/jvmci\/jvmci.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -654,1 +654,0 @@\n-    nmethodLocker& nmethod_handle,\n@@ -732,0 +731,1 @@\n+    nmethod* nm = NULL; \/\/ nm is an out parameter of register_method\n@@ -734,1 +734,1 @@\n-                                        nmethod_handle,\n+                                        nm,\n@@ -756,1 +756,0 @@\n-      nmethod* nm = nmethod_handle.code()->as_nmethod_or_null();\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCodeInstaller.cpp","additions":2,"deletions":3,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -331,1 +331,0 @@\n-                                   nmethodLocker& nmethod_handle,\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCodeInstaller.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -939,1 +939,0 @@\n-  nmethodLocker nmethod_handle;\n@@ -941,0 +940,1 @@\n+\n@@ -947,1 +947,0 @@\n-      nmethod_handle,\n@@ -1000,2 +999,1 @@\n-  nmethodLocker locker;\n-  CodeBlob* cb = JVMCIENV->get_code_blob(installedCodeObject, locker);\n+  CodeBlob* cb = JVMCIENV->get_code_blob(installedCodeObject);\n@@ -1015,6 +1013,0 @@\n-  if (cb->is_nmethod()) {\n-    nmethod* nm = (nmethod*) cb;\n-    if (!nm->is_alive()) {\n-      return NULL;\n-    }\n-  }\n@@ -1046,2 +1038,1 @@\n-  nmethodLocker locker;\n-  nmethod* nm = JVMCIENV->get_nmethod(nmethod_mirror, locker);\n+  nmethod* nm = JVMCIENV->get_nmethod(nmethod_mirror);\n@@ -2517,6 +2508,5 @@\n-    nmethodLocker locker;\n-    nmethod* nm = JVMCIENV->get_nmethod(obj, locker);\n-    if (nm != NULL) {\n-      JVMCINMethodData* data = nm->jvmci_nmethod_data();\n-      if (data != NULL) {\n-        if (peerEnv->is_hotspot()) {\n+    if (peerEnv->is_hotspot()) {\n+      nmethod* nm = JVMCIENV->get_nmethod(obj);\n+      if (nm != NULL) {\n+        JVMCINMethodData* data = nm->jvmci_nmethod_data();\n+        if (data != NULL) {\n@@ -2532,0 +2522,1 @@\n+\n@@ -2541,0 +2532,1 @@\n+      nmethod* nm = JVMCIENV->get_nmethod(obj);\n@@ -2592,2 +2584,1 @@\n-  nmethodLocker locker;\n-  JVMCIENV->get_nmethod(code, locker);\n+  JVMCIENV->get_nmethod(code);\n@@ -2598,2 +2589,1 @@\n-  nmethodLocker locker;\n-  CodeBlob* cb = JVMCIENV->get_code_blob(code, locker);\n+  CodeBlob* cb = JVMCIENV->get_code_blob(code);\n@@ -2603,0 +2593,1 @@\n+  \/\/ Make a resource copy of code before the allocation causes a safepoint\n@@ -2604,0 +2595,3 @@\n+  jbyte* code_bytes = NEW_RESOURCE_ARRAY(jbyte, code_size);\n+  memcpy(code_bytes, (jbyte*) cb->code_begin(), code_size);\n+\n@@ -2605,1 +2599,1 @@\n-  JVMCIENV->copy_bytes_from((jbyte*) cb->code_begin(), result, 0, code_size);\n+  JVMCIENV->copy_bytes_from(code_bytes, result, 0, code_size);\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompilerToVM.cpp","additions":17,"deletions":23,"binary":false,"changes":40,"status":"modified"},{"patch":"@@ -1500,3 +1500,0 @@\n-    if (!nm->is_alive()) {\n-      JVMCI_THROW_MSG(InternalError, \"nmethod has been reclaimed\");\n-    }\n@@ -1521,2 +1518,1 @@\n-  nmethodLocker locker;\n-  nmethod* nm = JVMCIENV->get_nmethod(mirror, locker);\n+  nmethod* nm = JVMCIENV->get_nmethod(mirror);\n@@ -1536,5 +1532,2 @@\n-  nmethodLocker nml(nm);\n-  if (nm->is_alive()) {\n-    \/\/ Invalidating the HotSpotNmethod means we want the nmethod to be deoptimized.\n-    Deoptimization::deoptimize_all_marked(nm);\n-  }\n+  \/\/ Invalidating the HotSpotNmethod means we want the nmethod to be deoptimized.\n+  Deoptimization::deoptimize_all_marked(nm);\n@@ -1561,2 +1554,3 @@\n-CodeBlob* JVMCIEnv::get_code_blob(JVMCIObject obj, nmethodLocker& locker) {\n-  address code = (address) get_InstalledCode_address(obj);\n+\n+\/\/ Lookup an nmethod with a matching base and compile id\n+nmethod* JVMCIEnv::lookup_nmethod(address code, jlong compile_id_snapshot) {\n@@ -1566,18 +1560,0 @@\n-  if (isa_HotSpotNmethod(obj)) {\n-    nmethod* nm = NULL;\n-    {\n-      \/\/ Lookup the CodeBlob while holding the CodeCache_lock to ensure the nmethod can't be freed\n-      \/\/ by nmethod::flush while we're interrogating it.\n-      MutexLocker cm_lock(CodeCache_lock, Mutex::_no_safepoint_check_flag);\n-      CodeBlob* cb = CodeCache::find_blob_unsafe(code);\n-      if (cb == (CodeBlob*) code) {\n-        nmethod* the_nm = cb->as_nmethod_or_null();\n-        if (the_nm != NULL && the_nm->is_alive()) {\n-          \/\/ Lock the nmethod to stop any further transitions by the sweeper.  It's still possible\n-          \/\/ for this code to execute in the middle of the sweeping of the nmethod but that will be\n-          \/\/ handled below.\n-          locker.set_code(nm, true);\n-          nm = the_nm;\n-        }\n-      }\n-    }\n@@ -1585,10 +1561,5 @@\n-    if (nm != NULL) {\n-      \/\/ We found the nmethod but it could be in the process of being freed.  Check the state of the\n-      \/\/ nmethod while holding the CompiledMethod_lock.  This ensures that any transitions by other\n-      \/\/ threads have seen the is_locked_by_vm() update above.\n-      MutexLocker cm_lock(CompiledMethod_lock, Mutex::_no_safepoint_check_flag);\n-      if (!nm->is_alive()) {\n-        \/\/  It was alive when we looked it up but it's no longer alive so release it.\n-        locker.set_code(NULL);\n-        nm = NULL;\n-      }\n+  CodeBlob* cb = CodeCache::find_blob(code);\n+  if (cb == (CodeBlob*) code) {\n+    nmethod* nm = cb->as_nmethod_or_null();\n+    if (nm != NULL && (compile_id_snapshot == 0 || nm->compile_id() == compile_id_snapshot)) {\n+      return nm;\n@@ -1596,0 +1567,4 @@\n+  }\n+  return NULL;\n+}\n+\n@@ -1597,0 +1572,6 @@\n+CodeBlob* JVMCIEnv::get_code_blob(JVMCIObject obj) {\n+  address code = (address) get_InstalledCode_address(obj);\n+  if (code == NULL) {\n+    return NULL;\n+  }\n+  if (isa_HotSpotNmethod(obj)) {\n@@ -1598,14 +1579,8 @@\n-    if (compile_id_snapshot != 0L) {\n-      \/\/ Found a live nmethod with the same address, make sure it's the same nmethod\n-      if (nm == (nmethod*) code && nm->compile_id() == compile_id_snapshot && nm->is_alive()) {\n-        if (nm->is_not_entrant()) {\n-          \/\/ Zero the entry point so that the nmethod\n-          \/\/ cannot be invoked by the mirror but can\n-          \/\/ still be deoptimized.\n-          set_InstalledCode_entryPoint(obj, 0);\n-        }\n-        return nm;\n-      }\n-      \/\/ The HotSpotNmethod no longer refers to a valid nmethod so clear the state\n-      locker.set_code(NULL);\n-      nm = NULL;\n+    nmethod* nm = lookup_nmethod(code, compile_id_snapshot);\n+    if (nm != NULL && compile_id_snapshot != 0L && nm->is_not_entrant()) {\n+      \/\/ Zero the entry point so that the nmethod\n+      \/\/ cannot be invoked by the mirror but can\n+      \/\/ still be deoptimized.\n+      set_InstalledCode_entryPoint(obj, 0);\n+      \/\/ Refetch the nmethod since the previous call will be a safepoint in libjvmci\n+      nm = lookup_nmethod(code, compile_id_snapshot);\n@@ -1629,2 +1604,2 @@\n-nmethod* JVMCIEnv::get_nmethod(JVMCIObject obj, nmethodLocker& locker) {\n-  CodeBlob* cb = get_code_blob(obj, locker);\n+nmethod* JVMCIEnv::get_nmethod(JVMCIObject obj) {\n+  CodeBlob* cb = get_code_blob(obj);\n","filename":"src\/hotspot\/share\/jvmci\/jvmciEnv.cpp","additions":31,"deletions":56,"binary":false,"changes":87,"status":"modified"},{"patch":"@@ -39,1 +39,0 @@\n-class nmethodLocker;\n@@ -299,0 +298,2 @@\n+  nmethod* lookup_nmethod(address code, jlong compile_id_snapshot);\n+\n@@ -347,3 +348,2 @@\n-  \/\/ Given an instance of HotSpotInstalledCode return the corresponding CodeBlob*.  The\n-  \/\/ nmethodLocker is required to keep the CodeBlob alive in the case where it's an nmethod.\n-  CodeBlob* get_code_blob(JVMCIObject code, nmethodLocker& locker);\n+  \/\/ Given an instance of HotSpotInstalledCode return the corresponding CodeBlob*.\n+  CodeBlob* get_code_blob(JVMCIObject code);\n@@ -351,3 +351,2 @@\n-  \/\/ Given an instance of HotSpotInstalledCode return the corresponding nmethod.  The\n-  \/\/ nmethodLocker is required to keep the nmethod alive.\n-  nmethod* get_nmethod(JVMCIObject code, nmethodLocker& locker);\n+  \/\/ Given an instance of HotSpotInstalledCode return the corresponding nmethod.\n+  nmethod* get_nmethod(JVMCIObject code);\n","filename":"src\/hotspot\/share\/jvmci\/jvmciEnv.hpp","additions":6,"deletions":7,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -804,7 +804,0 @@\n-void JVMCINMethodData::clear_nmethod_mirror(nmethod* nm) {\n-  if (_nmethod_mirror_index != -1) {\n-    oop* addr = nm->oop_addr_at(_nmethod_mirror_index);\n-    *addr = NULL;\n-  }\n-}\n-\n@@ -823,1 +816,1 @@\n-    if (!nm->is_alive()) {\n+    if (nm->is_unloading()) {\n@@ -837,1 +830,1 @@\n-  if (_nmethod_mirror_index != -1 && nm->is_unloaded()) {\n+  if (_nmethod_mirror_index != -1 && nm->is_unloading()) {\n@@ -2010,1 +2003,1 @@\n-        if (compile_state->task()->code() == nullptr) {\n+        if (!compile_state->task()->is_success()) {\n@@ -2042,22 +2035,22 @@\n-                                const methodHandle& method,\n-                                nmethodLocker& code_handle,\n-                                int entry_bci,\n-                                CodeOffsets* offsets,\n-                                int orig_pc_offset,\n-                                CodeBuffer* code_buffer,\n-                                int frame_words,\n-                                OopMapSet* oop_map_set,\n-                                ExceptionHandlerTable* handler_table,\n-                                ImplicitExceptionTable* implicit_exception_table,\n-                                AbstractCompiler* compiler,\n-                                DebugInformationRecorder* debug_info,\n-                                Dependencies* dependencies,\n-                                int compile_id,\n-                                bool has_monitors,\n-                                bool has_unsafe_access,\n-                                bool has_wide_vector,\n-                                JVMCIObject compiled_code,\n-                                JVMCIObject nmethod_mirror,\n-                                FailedSpeculation** failed_speculations,\n-                                char* speculations,\n-                                int speculations_len) {\n+                                                       const methodHandle& method,\n+                                                       nmethod*& nm,\n+                                                       int entry_bci,\n+                                                       CodeOffsets* offsets,\n+                                                       int orig_pc_offset,\n+                                                       CodeBuffer* code_buffer,\n+                                                       int frame_words,\n+                                                       OopMapSet* oop_map_set,\n+                                                       ExceptionHandlerTable* handler_table,\n+                                                       ImplicitExceptionTable* implicit_exception_table,\n+                                                       AbstractCompiler* compiler,\n+                                                       DebugInformationRecorder* debug_info,\n+                                                       Dependencies* dependencies,\n+                                                       int compile_id,\n+                                                       bool has_monitors,\n+                                                       bool has_unsafe_access,\n+                                                       bool has_wide_vector,\n+                                                       JVMCIObject compiled_code,\n+                                                       JVMCIObject nmethod_mirror,\n+                                                       FailedSpeculation** failed_speculations,\n+                                                       char* speculations,\n+                                                       int speculations_len) {\n@@ -2065,1 +2058,0 @@\n-  nmethod* nm = NULL;\n@@ -2092,0 +2084,3 @@\n+    \/\/ Check if memory should be freed before allocation\n+    CodeCache::gc_on_allocation();\n+\n@@ -2156,6 +2151,0 @@\n-        \/\/ Record successful registration.\n-        \/\/ (Put nm into the task handle *before* publishing to the Java heap.)\n-        if (JVMCIENV->compile_state() != NULL) {\n-          JVMCIENV->compile_state()->task()->set_code(nm);\n-        }\n-\n@@ -2216,3 +2205,0 @@\n-    if (result == JVMCI::ok) {\n-      code_handle.set_code(nm);\n-    }\n@@ -2229,2 +2215,5 @@\n-    \/\/ JVMTI -- compiled method notification (must be done outside lock)\n-    nm->post_compiled_method_load_event();\n+    JVMCICompileState* state = JVMCIENV->compile_state();\n+    if (state != NULL) {\n+      \/\/ Compilation succeeded, post what we know about it\n+      nm->post_compiled_method(state->task());\n+    }\n","filename":"src\/hotspot\/share\/jvmci\/jvmciRuntime.cpp","additions":33,"deletions":44,"binary":false,"changes":77,"status":"modified"},{"patch":"@@ -97,3 +97,0 @@\n-\n-  \/\/ Clears the mirror in nm's oops table.\n-  void clear_nmethod_mirror(nmethod* nm);\n@@ -402,22 +399,22 @@\n-                       const methodHandle&       target,\n-                       nmethodLocker&            code_handle,\n-                       int                       entry_bci,\n-                       CodeOffsets*              offsets,\n-                       int                       orig_pc_offset,\n-                       CodeBuffer*               code_buffer,\n-                       int                       frame_words,\n-                       OopMapSet*                oop_map_set,\n-                       ExceptionHandlerTable*    handler_table,\n-                       ImplicitExceptionTable*   implicit_exception_table,\n-                       AbstractCompiler*         compiler,\n-                       DebugInformationRecorder* debug_info,\n-                       Dependencies*             dependencies,\n-                       int                       compile_id,\n-                       bool                      has_monitors,\n-                       bool                      has_unsafe_access,\n-                       bool                      has_wide_vector,\n-                       JVMCIObject               compiled_code,\n-                       JVMCIObject               nmethod_mirror,\n-                       FailedSpeculation**       failed_speculations,\n-                       char*                     speculations,\n-                       int                       speculations_len);\n+                                           const methodHandle&       target,\n+                                           nmethod*&                 nm,\n+                                           int                       entry_bci,\n+                                           CodeOffsets*              offsets,\n+                                           int                       orig_pc_offset,\n+                                           CodeBuffer*               code_buffer,\n+                                           int                       frame_words,\n+                                           OopMapSet*                oop_map_set,\n+                                           ExceptionHandlerTable*    handler_table,\n+                                           ImplicitExceptionTable*   implicit_exception_table,\n+                                           AbstractCompiler*         compiler,\n+                                           DebugInformationRecorder* debug_info,\n+                                           Dependencies*             dependencies,\n+                                           int                       compile_id,\n+                                           bool                      has_monitors,\n+                                           bool                      has_unsafe_access,\n+                                           bool                      has_wide_vector,\n+                                           JVMCIObject               compiled_code,\n+                                           JVMCIObject               nmethod_mirror,\n+                                           FailedSpeculation**       failed_speculations,\n+                                           char*                     speculations,\n+                                           int                       speculations_len);\n","filename":"src\/hotspot\/share\/jvmci\/jvmciRuntime.hpp","additions":22,"deletions":25,"binary":false,"changes":47,"status":"modified"},{"patch":"@@ -231,1 +231,0 @@\n-  nonstatic_field(MethodCounters,              _nmethod_age,                                  int)                                   \\\n","filename":"src\/hotspot\/share\/jvmci\/vmStructs_jvmci.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -486,1 +486,1 @@\n-CodeBlob* CodeHeap::find_blob_unsafe(void* start) const {\n+CodeBlob* CodeHeap::find_blob(void* start) const {\n","filename":"src\/hotspot\/share\/memory\/heap.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -179,1 +179,1 @@\n-  virtual CodeBlob* find_blob_unsafe(void* start) const;\n+  virtual CodeBlob* find_blob(void* start) const;\n","filename":"src\/hotspot\/share\/memory\/heap.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -65,2 +65,2 @@\n-      \/\/ CodeCache sweeper support\n-      nm->mark_as_maybe_on_continuation();\n+      \/\/ CodeCache unloading support\n+      nm->mark_as_maybe_on_stack();\n","filename":"src\/hotspot\/share\/memory\/iterator.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -110,1 +110,1 @@\n-  \/\/ The code cache sweeper needs to get notified about methods from stackChunkOops\n+  \/\/ The code cache unloading needs to get notified about methods from stackChunkOops\n","filename":"src\/hotspot\/share\/memory\/iterator.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2216,2 +2216,2 @@\n-bool ConstantPool::is_maybe_on_continuation_stack() const {\n-  \/\/ This method uses the similar logic as nmethod::is_maybe_on_continuation_stack()\n+bool ConstantPool::is_maybe_on_stack() const {\n+  \/\/ This method uses the similar logic as nmethod::is_maybe_on_stack()\n@@ -2224,1 +2224,1 @@\n-  return cache()->gc_epoch() >= Continuations::previous_completed_gc_marking_cycle();\n+  return cache()->gc_epoch() >= CodeCache::previous_completed_gc_marking_cycle();\n@@ -2239,1 +2239,1 @@\n-  return is_maybe_on_continuation_stack();\n+  return is_maybe_on_stack();\n","filename":"src\/hotspot\/share\/oops\/constantPool.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -209,1 +209,1 @@\n-  bool is_maybe_on_continuation_stack() const;\n+  bool is_maybe_on_stack() const;\n","filename":"src\/hotspot\/share\/oops\/constantPool.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"code\/codeCache.hpp\"\n@@ -49,1 +50,0 @@\n-#include \"runtime\/continuation.hpp\"\n@@ -700,1 +700,1 @@\n-  _gc_epoch = Continuations::gc_epoch();\n+  _gc_epoch = CodeCache::gc_epoch();\n","filename":"src\/hotspot\/share\/oops\/cpCache.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2345,4 +2345,0 @@\n-void InstanceKlass::remove_dependent_nmethod(nmethod* nm) {\n-  dependencies().remove_dependent_nmethod(nm);\n-}\n-\n","filename":"src\/hotspot\/share\/oops\/instanceKlass.cpp","additions":0,"deletions":4,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -942,1 +942,0 @@\n-  void remove_dependent_nmethod(nmethod* nm);\n","filename":"src\/hotspot\/share\/oops\/instanceKlass.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -1168,3 +1168,0 @@\n-  \/\/ If the vep() points to the zombie nmethod, the memory for the nmethod\n-  \/\/ could be flushed and the compiler and vtable stubs could still call\n-  \/\/ through it.\n","filename":"src\/hotspot\/share\/oops\/method.cpp","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -408,8 +408,0 @@\n-  int nmethod_age() const {\n-    if (method_counters() == NULL) {\n-      return INT_MAX;\n-    } else {\n-      return method_counters()->nmethod_age();\n-    }\n-  }\n-\n","filename":"src\/hotspot\/share\/oops\/method.hpp","additions":0,"deletions":8,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -33,1 +33,0 @@\n-  _nmethod_age(INT_MAX),\n@@ -42,4 +41,0 @@\n-  if (StressCodeAging) {\n-    set_nmethod_age(HotMethodDetectionLimit);\n-  }\n-\n@@ -68,1 +63,0 @@\n-  set_nmethod_age(INT_MAX);\n","filename":"src\/hotspot\/share\/oops\/methodCounters.cpp","additions":0,"deletions":6,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -42,1 +42,0 @@\n-  int               _nmethod_age;\n@@ -52,8 +51,0 @@\n-  \/\/ NMethod age is a counter for warm methods detection in the code cache sweeper.\n-  \/\/ The counter is reset by the sweeper and is decremented by some of the compiled\n-  \/\/ code. The counter values are interpreted as follows:\n-  \/\/ 1. (HotMethodDetection..INT_MAX] - initial value, no counters inserted\n-  \/\/ 2. [1..HotMethodDetectionLimit)  - the method is warm, the counter is used\n-  \/\/                                    to figure out which methods can be flushed.\n-  \/\/ 3. (INT_MIN..0]                  - method is hot and will deopt and get\n-  \/\/                                    recompiled without the counters\n@@ -125,18 +116,0 @@\n-  int nmethod_age() {\n-    return _nmethod_age;\n-  }\n-  void set_nmethod_age(int age) {\n-    _nmethod_age = age;\n-  }\n-  void reset_nmethod_age() {\n-    set_nmethod_age(HotMethodDetectionLimit);\n-  }\n-\n-  static bool is_nmethod_hot(int age)       { return age <= 0; }\n-  static bool is_nmethod_warm(int age)      { return age < HotMethodDetectionLimit; }\n-  static bool is_nmethod_age_unset(int age) { return age > HotMethodDetectionLimit; }\n-\n-  static ByteSize nmethod_age_offset() {\n-    return byte_offset_of(MethodCounters, _nmethod_age);\n-  }\n-\n","filename":"src\/hotspot\/share\/oops\/methodCounters.hpp","additions":0,"deletions":27,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -1020,1 +1020,0 @@\n-  set_age_code(has_method() && method()->profile_aging());\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -330,1 +330,0 @@\n-  bool                  _age_code;              \/\/ True if we need to profile code age (decrement the aging counter)\n@@ -620,2 +619,0 @@\n-  bool              age_code() const             { return _age_code; }\n-  void          set_age_code(bool z)             { _age_code = z; }\n","filename":"src\/hotspot\/share\/opto\/compile.hpp","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -564,2 +564,0 @@\n-  void decrement_age();\n-\n","filename":"src\/hotspot\/share\/opto\/parse.hpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -580,3 +580,0 @@\n-    if (depth() == 1 && C->age_code()) {\n-      decrement_age();\n-    }\n@@ -2177,25 +2174,0 @@\n-void Parse::decrement_age() {\n-  MethodCounters* mc = method()->ensure_method_counters();\n-  if (mc == NULL) {\n-    C->record_failure(\"Must have MCs\");\n-    return;\n-  }\n-  assert(!is_osr_parse(), \"Not doing this for OSRs\");\n-\n-  \/\/ Set starting bci for uncommon trap.\n-  set_parse_bci(0);\n-\n-  const TypePtr* adr_type = TypeRawPtr::make((address)mc);\n-  Node* mc_adr = makecon(adr_type);\n-  Node* cnt_adr = basic_plus_adr(mc_adr, mc_adr, in_bytes(MethodCounters::nmethod_age_offset()));\n-  Node* cnt = make_load(control(), cnt_adr, TypeInt::INT, T_INT, adr_type, MemNode::unordered);\n-  Node* decr = _gvn.transform(new SubINode(cnt, makecon(TypeInt::ONE)));\n-  store_to_memory(control(), cnt_adr, decr, T_INT, adr_type, MemNode::unordered);\n-  Node *chk   = _gvn.transform(new CmpINode(decr, makecon(TypeInt::ZERO)));\n-  Node* tst   = _gvn.transform(new BoolNode(chk, BoolTest::gt));\n-  { BuildCutout unless(this, tst, PROB_ALWAYS);\n-    uncommon_trap(Deoptimization::Reason_tenured,\n-                  Deoptimization::Action_make_not_entrant);\n-  }\n-}\n-\n","filename":"src\/hotspot\/share\/opto\/parse1.cpp","additions":0,"deletions":28,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -237,1 +237,1 @@\n-      NMethodIterator iter(NMethodIterator::only_alive_and_not_unloading);\n+      NMethodIterator iter(NMethodIterator::only_not_unloading);\n","filename":"src\/hotspot\/share\/prims\/jvmtiCodeBlobEvents.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2465,1 +2465,0 @@\n-  assert(!nm->is_zombie(), \"nmethod zombie in post_compiled_method_load\");\n","filename":"src\/hotspot\/share\/prims\/jvmtiExport.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -1021,2 +1021,2 @@\n-\/\/ The sweeper calls this and marks the nmethods here on the stack so that\n-\/\/ they cannot be turned into zombies while in the queue.\n+\/\/ The GC calls this and marks the nmethods here on the stack so that\n+\/\/ they cannot be unloaded while in the queue.\n@@ -1079,1 +1079,1 @@\n-  \/\/ Post events while nmethods are still in the queue and can't be unloaded or made zombie\n+  \/\/ Post events while nmethods are still in the queue and can't be unloaded.\n","filename":"src\/hotspot\/share\/prims\/jvmtiImpl.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -503,1 +503,1 @@\n-  \/\/ Sweeper support to keep nmethods from being zombied while in the queue.\n+  \/\/ GC support to keep nmethods from unloading while in the queue.\n@@ -546,1 +546,1 @@\n-  \/\/ Sweeper support to keep nmethods from being zombied while in the queue.\n+  \/\/ GC support to keep nmethods from unloading while in the queue.\n","filename":"src\/hotspot\/share\/prims\/jvmtiImpl.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1058,8 +1058,0 @@\n-void MethodHandles::remove_dependent_nmethod(oop call_site, nmethod* nm) {\n-  assert_locked_or_safepoint(CodeCache_lock);\n-\n-  oop context = java_lang_invoke_CallSite::context_no_keepalive(call_site);\n-  DependencyContext deps = java_lang_invoke_MethodHandleNatives_CallSiteContext::vmdependencies(context);\n-  deps.remove_dependent_nmethod(nm);\n-}\n-\n","filename":"src\/hotspot\/share\/prims\/methodHandles.cpp","additions":0,"deletions":8,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -83,1 +83,0 @@\n-  static void remove_dependent_nmethod(oop call_site, nmethod* nm);\n","filename":"src\/hotspot\/share\/prims\/methodHandles.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -85,1 +85,0 @@\n-#include \"runtime\/sweeper.hpp\"\n@@ -813,1 +812,1 @@\n-  return (code->is_alive() && !code->is_marked_for_deoptimization());\n+  return !code->is_marked_for_deoptimization();\n@@ -1424,5 +1423,0 @@\n-WB_ENTRY(void, WB_ForceNMethodSweep(JNIEnv* env, jobject o))\n-  \/\/ Force a code cache sweep and block until it finished\n-  NMethodSweeper::force_sweep();\n-WB_END\n-\n@@ -2663,1 +2657,0 @@\n-  {CC\"forceNMethodSweep\",  CC\"()V\",                   (void*)&WB_ForceNMethodSweep  },\n","filename":"src\/hotspot\/share\/prims\/whitebox.cpp","additions":1,"deletions":8,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -551,0 +551,1 @@\n+  { \"UseCodeAging\",                 JDK_Version::undefined(), JDK_Version::jdk(20), JDK_Version::jdk(21) },\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -423,1 +423,0 @@\n-\/\/ See NMethodSweeper::do_stack_scanning and nmethod::is_not_on_continuation_stack\n@@ -428,41 +427,0 @@\n-\/\/ We initialize the _gc_epoch to 2, because previous_completed_gc_marking_cycle\n-\/\/ subtracts the value by 2, and the type is unsigned. We don't want underflow.\n-\/\/\n-\/\/ Odd values mean that marking is in progress, and even values mean that no\n-\/\/ marking is currently active.\n-uint64_t Continuations::_gc_epoch = 2;\n-\n-uint64_t Continuations::gc_epoch() {\n-  return _gc_epoch;\n-}\n-\n-bool Continuations::is_gc_marking_cycle_active() {\n-  \/\/ Odd means that marking is active\n-  return (_gc_epoch % 2) == 1;\n-}\n-\n-uint64_t Continuations::previous_completed_gc_marking_cycle() {\n-  if (is_gc_marking_cycle_active()) {\n-    return _gc_epoch - 2;\n-  } else {\n-    return _gc_epoch - 1;\n-  }\n-}\n-\n-void Continuations::on_gc_marking_cycle_start() {\n-  assert(!is_gc_marking_cycle_active(), \"Previous marking cycle never ended\");\n-  ++_gc_epoch;\n-}\n-\n-void Continuations::on_gc_marking_cycle_finish() {\n-  assert(is_gc_marking_cycle_active(), \"Marking cycle started before last one finished\");\n-  ++_gc_epoch;\n-}\n-\n-void Continuations::arm_all_nmethods() {\n-  BarrierSetNMethod* bs_nm = BarrierSet::barrier_set()->barrier_set_nmethod();\n-  if (bs_nm != NULL) {\n-    bs_nm->arm_all_nmethods();\n-  }\n-}\n-\n","filename":"src\/hotspot\/share\/runtime\/continuation.cpp","additions":0,"deletions":42,"binary":false,"changes":42,"status":"modified"},{"patch":"@@ -40,3 +40,0 @@\n-private:\n-  static uint64_t _gc_epoch;\n-\n@@ -46,9 +43,0 @@\n-\n-  \/\/ The GC epoch and marking_cycle code below is there to support sweeping\n-  \/\/ nmethods in loom stack chunks.\n-  static uint64_t gc_epoch();\n-  static bool is_gc_marking_cycle_active();\n-  static uint64_t previous_completed_gc_marking_cycle();\n-  static void on_gc_marking_cycle_start();\n-  static void on_gc_marking_cycle_finish();\n-  static void arm_all_nmethods();\n","filename":"src\/hotspot\/share\/runtime\/continuation.hpp","additions":0,"deletions":12,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -622,1 +622,1 @@\n-  assert(CodeCache::find_blob_unsafe(frame_pcs[0]) != NULL, \"bad pc\");\n+  assert(CodeCache::find_blob(frame_pcs[0]) != NULL, \"bad pc\");\n@@ -1898,3 +1898,0 @@\n-  \/\/ Make sure the calling nmethod is not getting deoptimized and removed\n-  \/\/ before we are done with it.\n-  nmethodLocker nl(fr.pc());\n@@ -1957,1 +1954,1 @@\n-    bool create_if_missing = ProfileTraps || UseCodeAging RTM_OPT_ONLY( || UseRTMLocking );\n+    bool create_if_missing = ProfileTraps RTM_OPT_ONLY( || UseRTMLocking );\n","filename":"src\/hotspot\/share\/runtime\/deoptimization.cpp","additions":2,"deletions":5,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -230,1 +230,1 @@\n-  _cb = CodeCache::find_blob_unsafe(_pc);\n+  _cb = CodeCache::find_blob(_pc);\n@@ -235,1 +235,1 @@\n-  set_pc_preserve_deopt(newpc, CodeCache::find_blob_unsafe(newpc));\n+  set_pc_preserve_deopt(newpc, CodeCache::find_blob(newpc));\n","filename":"src\/hotspot\/share\/runtime\/frame.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -273,1 +273,1 @@\n-          \"Reclamation of zombie and not-entrant methods\")                  \\\n+          \"Reclamation of compiled methods\")                                \\\n@@ -382,1 +382,1 @@\n-          \"Create zombies (non-entrant) at exit from the runtime system\")   \\\n+          \"Create non-entrant nmethods at exit from the runtime system\")    \\\n@@ -985,3 +985,0 @@\n-  develop(bool, PrintMethodFlushing, false,                                 \\\n-          \"Print the nmethods being flushed\")                               \\\n-                                                                            \\\n@@ -991,5 +988,0 @@\n-  product(intx, HotMethodDetectionLimit, 100000, DIAGNOSTIC,                \\\n-          \"Number of compiled code invocations after which \"                \\\n-          \"the method is considered as hot by the flusher\")                 \\\n-          range(1, max_jint)                                                \\\n-                                                                            \\\n@@ -1001,6 +993,0 @@\n-  product(bool, UseCodeAging, true,                                         \\\n-          \"Insert counter to detect warm methods\")                          \\\n-                                                                            \\\n-  product(bool, StressCodeAging, false, DIAGNOSTIC,                         \\\n-          \"Start with counters compiled in\")                                \\\n-                                                                            \\\n@@ -1097,3 +1083,0 @@\n-  develop(bool, TraceCreateZombies, false,                                  \\\n-          \"trace creation of zombie nmethods\")                              \\\n-                                                                            \\\n@@ -1316,1 +1299,1 @@\n-  product(intx, NmethodSweepActivity, 10,                                   \\\n+  product(intx, NmethodSweepActivity, 4,                                    \\\n@@ -1321,6 +1304,0 @@\n-  notproduct(bool, LogSweeper, false,                                       \\\n-          \"Keep a ring buffer of sweeper activity\")                         \\\n-                                                                            \\\n-  notproduct(intx, SweeperLogEntries, 1024,                                 \\\n-          \"Number of records in the ring buffer of sweeper activity\")       \\\n-                                                                            \\\n@@ -1587,2 +1564,2 @@\n-  product(double, SweeperThreshold, 0.5,                                    \\\n-          \"Threshold controlling when code cache sweeper is invoked.\"       \\\n+  product(double, SweeperThreshold, 15.0,                                   \\\n+          \"Threshold when a code cache unloading GC is invoked.\"            \\\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":5,"deletions":28,"binary":false,"changes":33,"status":"modified"},{"patch":"@@ -71,1 +71,0 @@\n-#include \"runtime\/sweeper.hpp\"\n@@ -297,1 +296,0 @@\n-  \/\/ Does also call NMethodSweeper::print(tty)\n@@ -300,2 +298,0 @@\n-  } else if (PrintMethodFlushingStatistics) {\n-    NMethodSweeper::print(tty);\n@@ -369,1 +365,0 @@\n-  \/\/ Does also call NMethodSweeper::print(tty)\n@@ -372,2 +367,0 @@\n-  } else if (PrintMethodFlushingStatistics) {\n-    NMethodSweeper::print(tty);\n","filename":"src\/hotspot\/share\/runtime\/java.cpp","additions":0,"deletions":7,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -896,1 +896,1 @@\n-  \/\/ Sweeper operations\n+  \/\/ GC operations\n","filename":"src\/hotspot\/share\/runtime\/javaThread.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -69,1 +69,0 @@\n-Monitor* CodeSweeper_lock             = NULL;\n@@ -99,1 +98,0 @@\n-Mutex*   NMethodSweeperStats_lock     = NULL;\n@@ -324,1 +322,0 @@\n-  def(NMethodSweeperStats_lock     , PaddedMutex  , nosafepoint);\n@@ -353,1 +350,0 @@\n-  defl(CodeSweeper_lock            , PaddedMonitor, CompiledMethod_lock);\n","filename":"src\/hotspot\/share\/runtime\/mutexLocker.cpp","additions":0,"deletions":4,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -61,1 +61,0 @@\n-extern Monitor* CodeSweeper_lock;                \/\/ a lock used by the sweeper only for wait notify\n@@ -93,1 +92,0 @@\n-extern Mutex*   NMethodSweeperStats_lock;        \/\/ a lock used to serialize access to sweeper statistics\n","filename":"src\/hotspot\/share\/runtime\/mutexLocker.hpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1082,1 +1082,1 @@\n-  CodeBlob* b = CodeCache::find_blob_unsafe(addr);\n+  CodeBlob* b = CodeCache::find_blob(addr);\n","filename":"src\/hotspot\/share\/runtime\/os.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -520,1 +520,1 @@\n-    java_thread,       \/\/ Java, CodeCacheSweeper, JVMTIAgent and Service threads.\n+    java_thread,       \/\/ Java, JVMTIAgent and Service threads.\n","filename":"src\/hotspot\/share\/runtime\/os.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -65,1 +65,0 @@\n-#include \"runtime\/sweeper.hpp\"\n","filename":"src\/hotspot\/share\/runtime\/safepoint.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -233,1 +233,1 @@\n-  \/\/ If you enqueue events before the service thread runs, gc and the sweeper\n+  \/\/ If you enqueue events before the service thread runs, gc\n","filename":"src\/hotspot\/share\/runtime\/serviceThread.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1293,1 +1293,0 @@\n-  nmethodLocker nl_callee(callee);\n@@ -1389,1 +1388,1 @@\n-  assert(caller_nm->is_alive() && !caller_nm->is_unloading(), \"It should be alive\");\n+  assert(!caller_nm->is_unloading(), \"It should not be unloading\");\n@@ -2297,1 +2296,1 @@\n-    if ((method != NULL) && nm->is_alive()) {\n+    if (method != NULL) {\n@@ -3095,0 +3094,3 @@\n+  \/\/ Check if memory should be freed before allocation\n+  CodeCache::gc_on_allocation();\n+\n","filename":"src\/hotspot\/share\/runtime\/sharedRuntime.cpp","additions":5,"deletions":3,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -503,1 +503,1 @@\n-  \/\/ wrong method handling (inline cache misses, zombie methods)\n+  \/\/ wrong method handling (inline cache misses)\n","filename":"src\/hotspot\/share\/runtime\/sharedRuntime.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -209,2 +209,0 @@\n-  assert(is_interpreted() || cb()->is_alive(),\n-    \"not alive - not_entrant: %d zombie: %d unloaded: %d\", _cb->is_not_entrant(), _cb->is_zombie(), _cb->is_unloaded());\n","filename":"src\/hotspot\/share\/runtime\/stackChunkFrameStream.inline.hpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -79,1 +79,1 @@\n-  CodeBlob*   blob = CodeCache::find_blob_unsafe(cbuf->insts()->start());\n+  CodeBlob*   blob = CodeCache::find_blob(cbuf->insts()->start());\n","filename":"src\/hotspot\/share\/runtime\/stubCodeGenerator.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1,680 +0,0 @@\n-\/*\n- * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"code\/codeCache.hpp\"\n-#include \"code\/compiledIC.hpp\"\n-#include \"code\/icBuffer.hpp\"\n-#include \"code\/nmethod.hpp\"\n-#include \"compiler\/compileBroker.hpp\"\n-#include \"gc\/shared\/collectedHeap.hpp\"\n-#include \"gc\/shared\/workerThread.hpp\"\n-#include \"jfr\/jfrEvents.hpp\"\n-#include \"logging\/log.hpp\"\n-#include \"logging\/logStream.hpp\"\n-#include \"memory\/allocation.inline.hpp\"\n-#include \"memory\/resourceArea.hpp\"\n-#include \"memory\/universe.hpp\"\n-#include \"oops\/method.hpp\"\n-#include \"runtime\/interfaceSupport.inline.hpp\"\n-#include \"runtime\/handshake.hpp\"\n-#include \"runtime\/javaThread.hpp\"\n-#include \"runtime\/mutexLocker.hpp\"\n-#include \"runtime\/orderAccess.hpp\"\n-#include \"runtime\/os.hpp\"\n-#include \"runtime\/sweeper.hpp\"\n-#include \"runtime\/vmOperations.hpp\"\n-#include \"runtime\/vmThread.hpp\"\n-#include \"utilities\/events.hpp\"\n-#include \"utilities\/xmlstream.hpp\"\n-\n-#ifdef ASSERT\n-\n-#define SWEEP(nm) record_sweep(nm, __LINE__)\n-\/\/ Sweeper logging code\n-class SweeperRecord {\n- public:\n-  int64_t traversal;\n-  int compile_id;\n-  int64_t traversal_mark;\n-  int state;\n-  const char* kind;\n-  address vep;\n-  address uep;\n-  int line;\n-\n-  void print() {\n-      tty->print_cr(\"traversal = \" INT64_FORMAT \" compile_id = %d %s uep = \" PTR_FORMAT \" vep = \"\n-                    PTR_FORMAT \" state = %d traversal_mark \" INT64_FORMAT \" line = %d\",\n-                    traversal,\n-                    compile_id,\n-                    kind == NULL ? \"\" : kind,\n-                    p2i(uep),\n-                    p2i(vep),\n-                    state,\n-                    traversal_mark,\n-                    line);\n-  }\n-};\n-\n-static int _sweep_index = 0;\n-static SweeperRecord* _records = NULL;\n-\n-void NMethodSweeper::record_sweep(CompiledMethod* nm, int line) {\n-  if (_records != NULL) {\n-    _records[_sweep_index].traversal = _traversals;\n-    _records[_sweep_index].traversal_mark = nm->is_nmethod() ? ((nmethod*)nm)->stack_traversal_mark() : 0;\n-    _records[_sweep_index].compile_id = nm->compile_id();\n-    _records[_sweep_index].kind = nm->compile_kind();\n-    _records[_sweep_index].state = nm->get_state();\n-    _records[_sweep_index].vep = nm->verified_entry_point();\n-    _records[_sweep_index].uep = nm->entry_point();\n-    _records[_sweep_index].line = line;\n-    _sweep_index = (_sweep_index + 1) % SweeperLogEntries;\n-  }\n-}\n-\n-void NMethodSweeper::init_sweeper_log() {\n- if (LogSweeper && _records == NULL) {\n-   \/\/ Create the ring buffer for the logging code\n-   _records = NEW_C_HEAP_ARRAY(SweeperRecord, SweeperLogEntries, mtGC);\n-   memset(_records, 0, sizeof(SweeperRecord) * SweeperLogEntries);\n-  }\n-}\n-#else\n-#define SWEEP(nm)\n-#endif\n-\n-CompiledMethodIterator NMethodSweeper::_current(CompiledMethodIterator::all_blobs); \/\/ Current compiled method\n-int64_t  NMethodSweeper::_traversals                   = 0;    \/\/ Stack scan count, also sweep ID.\n-int64_t  NMethodSweeper::_total_nof_code_cache_sweeps  = 0;    \/\/ Total number of full sweeps of the code cache\n-int      NMethodSweeper::_seen                         = 0;    \/\/ Nof. nmethod we have currently processed in current pass of CodeCache\n-size_t   NMethodSweeper::_sweep_threshold_bytes        = 0;    \/\/ Threshold for when to sweep. Updated after ergonomics\n-\n-volatile bool NMethodSweeper::_should_sweep            = false;\/\/ Indicates if a normal sweep will be done\n-volatile bool NMethodSweeper::_force_sweep             = false;\/\/ Indicates if a forced sweep will be done\n-volatile size_t NMethodSweeper::_bytes_changed         = 0;    \/\/ Counts the total nmethod size if the nmethod changed from:\n-                                                               \/\/   1) alive       -> not_entrant\n-                                                               \/\/   2) not_entrant -> zombie\n-int    NMethodSweeper::_hotness_counter_reset_val       = 0;\n-\n-int64_t NMethodSweeper::_total_nof_methods_reclaimed    = 0;   \/\/ Accumulated nof methods flushed\n-int64_t NMethodSweeper::_total_nof_c2_methods_reclaimed = 0;   \/\/ Accumulated nof methods flushed\n-size_t NMethodSweeper::_total_flushed_size              = 0;   \/\/ Total number of bytes flushed from the code cache\n-Tickspan NMethodSweeper::_total_time_sweeping;                 \/\/ Accumulated time sweeping\n-Tickspan NMethodSweeper::_total_time_this_sweep;               \/\/ Total time this sweep\n-Tickspan NMethodSweeper::_peak_sweep_time;                     \/\/ Peak time for a full sweep\n-Tickspan NMethodSweeper::_peak_sweep_fraction_time;            \/\/ Peak time sweeping one fraction\n-\n-class MarkActivationClosure: public CodeBlobClosure {\n-public:\n-  virtual void do_code_blob(CodeBlob* cb) {\n-    nmethod* nm = cb->as_nmethod();\n-    nm->set_hotness_counter(NMethodSweeper::hotness_counter_reset_val());\n-    \/\/ If we see an activation belonging to a non_entrant nmethod, we mark it.\n-    if (nm->is_not_entrant()) {\n-      nm->mark_as_seen_on_stack();\n-    }\n-  }\n-};\n-static MarkActivationClosure mark_activation_closure;\n-\n-int NMethodSweeper::hotness_counter_reset_val() {\n-  if (_hotness_counter_reset_val == 0) {\n-    _hotness_counter_reset_val = (ReservedCodeCacheSize < M) ? 1 : (ReservedCodeCacheSize \/ M) * 2;\n-  }\n-  return _hotness_counter_reset_val;\n-}\n-bool NMethodSweeper::wait_for_stack_scanning() {\n-  return _current.end();\n-}\n-\n-class NMethodMarkingClosure : public HandshakeClosure {\n-private:\n-  CodeBlobClosure* _cl;\n-public:\n-  NMethodMarkingClosure(CodeBlobClosure* cl) : HandshakeClosure(\"NMethodMarking\"), _cl(cl) {}\n-  void do_thread(Thread* thread) {\n-    if (thread->is_Java_thread() && ! thread->is_Code_cache_sweeper_thread()) {\n-      JavaThread::cast(thread)->nmethods_do(_cl);\n-    }\n-  }\n-};\n-\n-CodeBlobClosure* NMethodSweeper::prepare_mark_active_nmethods() {\n-#ifdef ASSERT\n-  assert(Thread::current()->is_Code_cache_sweeper_thread(), \"must be executed under CodeCache_lock and in sweeper thread\");\n-  assert_lock_strong(CodeCache_lock);\n-#endif\n-\n-  \/\/ If we do not want to reclaim not-entrant or zombie methods there is no need\n-  \/\/ to scan stacks\n-  if (!MethodFlushing) {\n-    return NULL;\n-  }\n-\n-  \/\/ Check for restart\n-  assert(_current.method() == NULL, \"should only happen between sweeper cycles\");\n-  assert(wait_for_stack_scanning(), \"should only happen between sweeper cycles\");\n-\n-  _seen = 0;\n-  _current = CompiledMethodIterator(CompiledMethodIterator::all_blobs);\n-  \/\/ Initialize to first nmethod\n-  _current.next();\n-  _traversals += 1;\n-  _total_time_this_sweep = Tickspan();\n-\n-  if (PrintMethodFlushing) {\n-    tty->print_cr(\"### Sweep: stack traversal \" INT64_FORMAT, _traversals);\n-  }\n-  return &mark_activation_closure;\n-}\n-\n-\/**\n-  * This function triggers a VM operation that does stack scanning of active\n-  * methods. Stack scanning is mandatory for the sweeper to make progress.\n-  *\/\n-void NMethodSweeper::do_stack_scanning() {\n-  assert(!CodeCache_lock->owned_by_self(), \"just checking\");\n-  if (Continuations::enabled()) {\n-    \/\/ There are continuation stacks in the heap that need to be scanned.\n-    Universe::heap()->collect(GCCause::_codecache_GC_threshold);\n-  }\n-  if (wait_for_stack_scanning()) {\n-    CodeBlobClosure* code_cl;\n-    {\n-      MutexLocker ccl(CodeCache_lock, Mutex::_no_safepoint_check_flag);\n-      code_cl = prepare_mark_active_nmethods();\n-    }\n-    if (code_cl != NULL) {\n-      NMethodMarkingClosure nm_cl(code_cl);\n-      Handshake::execute(&nm_cl);\n-    }\n-  }\n-}\n-\n-void NMethodSweeper::sweeper_loop() {\n-  bool timeout;\n-  while (true) {\n-    {\n-      ThreadBlockInVM tbivm(JavaThread::current());\n-      MonitorLocker waiter(CodeSweeper_lock, Mutex::_no_safepoint_check_flag);\n-      const int64_t wait_time = 60*60*24 * 1000;\n-      timeout = waiter.wait(wait_time);\n-    }\n-    if (!timeout && (_should_sweep || _force_sweep)) {\n-      sweep();\n-    }\n-  }\n-}\n-\n-\/**\n-  * Wakes up the sweeper thread to sweep if code cache space runs low\n-  *\/\n-void NMethodSweeper::report_allocation() {\n-  if (should_start_aggressive_sweep()) {\n-    MonitorLocker waiter(CodeSweeper_lock, Mutex::_no_safepoint_check_flag);\n-    _should_sweep = true;\n-    CodeSweeper_lock->notify();\n-  }\n-}\n-\n-bool NMethodSweeper::should_start_aggressive_sweep() {\n-  \/\/ Makes sure that we do not invoke the sweeper too often during startup.\n-  double start_threshold = 100.0 \/ (double)StartAggressiveSweepingAt;\n-  double aggressive_sweep_threshold = MAX2(start_threshold, 1.1);\n-  return (CodeCache::reverse_free_ratio() >= aggressive_sweep_threshold);\n-}\n-\n-\/**\n-  * Wakes up the sweeper thread and forces a sweep. Blocks until it finished.\n-  *\/\n-void NMethodSweeper::force_sweep() {\n-  ThreadBlockInVM tbivm(JavaThread::current());\n-  MonitorLocker waiter(CodeSweeper_lock, Mutex::_no_safepoint_check_flag);\n-  \/\/ Request forced sweep\n-  _force_sweep = true;\n-  while (_force_sweep) {\n-    \/\/ Notify sweeper that we want to force a sweep and wait for completion.\n-    \/\/ In case a sweep currently takes place we timeout and try again because\n-    \/\/ we want to enforce a full sweep.\n-    CodeSweeper_lock->notify();\n-    waiter.wait(1000);\n-  }\n-}\n-\n-\/**\n- * Handle a safepoint request\n- *\/\n-void NMethodSweeper::handle_safepoint_request() {\n-  JavaThread* thread = JavaThread::current();\n-  if (SafepointMechanism::local_poll_armed(thread)) {\n-    if (PrintMethodFlushing && Verbose) {\n-      tty->print_cr(\"### Sweep at %d out of %d, yielding to safepoint\", _seen, CodeCache::nmethod_count());\n-    }\n-    MutexUnlocker mu(CodeCache_lock, Mutex::_no_safepoint_check_flag);\n-\n-    ThreadBlockInVM tbivm(thread);\n-  }\n-}\n-\n-void NMethodSweeper::sweep() {\n-  assert(_should_sweep || _force_sweep, \"must have been set\");\n-  assert(JavaThread::current()->thread_state() == _thread_in_vm, \"must run in vm mode\");\n-  Atomic::store(&_bytes_changed, static_cast<size_t>(0)); \/\/ reset regardless of sleep reason\n-  if (_should_sweep) {\n-    MutexLocker mu(CodeSweeper_lock, Mutex::_no_safepoint_check_flag);\n-    _should_sweep = false;\n-  }\n-\n-  do_stack_scanning();\n-\n-  init_sweeper_log();\n-  sweep_code_cache();\n-\n-  \/\/ We are done with sweeping the code cache once.\n-  _total_nof_code_cache_sweeps++;\n-\n-  if (_force_sweep) {\n-    \/\/ Notify requester that forced sweep finished\n-    MutexLocker mu(CodeSweeper_lock, Mutex::_no_safepoint_check_flag);\n-    _force_sweep = false;\n-    CodeSweeper_lock->notify();\n-  }\n-}\n-\n-static void post_sweep_event(EventSweepCodeCache* event,\n-                             const Ticks& start,\n-                             const Ticks& end,\n-                             s4 traversals,\n-                             int swept,\n-                             int flushed,\n-                             int zombified) {\n-  assert(event != NULL, \"invariant\");\n-  assert(event->should_commit(), \"invariant\");\n-  event->set_starttime(start);\n-  event->set_endtime(end);\n-  event->set_sweepId(traversals);\n-  event->set_sweptCount(swept);\n-  event->set_flushedCount(flushed);\n-  event->set_zombifiedCount(zombified);\n-  event->commit();\n-}\n-\n-void NMethodSweeper::sweep_code_cache() {\n-  ResourceMark rm;\n-  Ticks sweep_start_counter = Ticks::now();\n-\n-  log_debug(codecache, sweep, start)(\"CodeCache flushing\");\n-\n-  int flushed_count                = 0;\n-  int zombified_count              = 0;\n-  int flushed_c2_count     = 0;\n-\n-  if (PrintMethodFlushing && Verbose) {\n-    tty->print_cr(\"### Sweep at %d out of %d\", _seen, CodeCache::nmethod_count());\n-  }\n-\n-  int swept_count = 0;\n-  assert(!SafepointSynchronize::is_at_safepoint(), \"should not be in safepoint when we get here\");\n-  assert(!CodeCache_lock->owned_by_self(), \"just checking\");\n-\n-  int freed_memory = 0;\n-  {\n-    MutexLocker mu(CodeCache_lock, Mutex::_no_safepoint_check_flag);\n-\n-    while (!_current.end()) {\n-      CodeCache::Sweep::begin();\n-      swept_count++;\n-      \/\/ Since we will give up the CodeCache_lock, always skip ahead\n-      \/\/ to the next nmethod.  Other blobs can be deleted by other\n-      \/\/ threads but nmethods are only reclaimed by the sweeper.\n-      CompiledMethod* nm = _current.method();\n-      _current.next();\n-\n-      \/\/ Now ready to process nmethod and give up CodeCache_lock\n-      {\n-        MutexUnlocker mu(CodeCache_lock, Mutex::_no_safepoint_check_flag);\n-        \/\/ Save information before potentially flushing the nmethod\n-        \/\/ Only flushing nmethods so size only matters for them.\n-        int size = nm->is_nmethod() ? ((nmethod*)nm)->total_size() : 0;\n-        bool is_c2_method = nm->is_compiled_by_c2();\n-        bool is_osr = nm->is_osr_method();\n-        int compile_id = nm->compile_id();\n-        intptr_t address = p2i(nm);\n-        const char* state_before = nm->state();\n-        const char* state_after = \"\";\n-\n-        MethodStateChange type = process_compiled_method(nm);\n-        switch (type) {\n-          case Flushed:\n-            state_after = \"flushed\";\n-            freed_memory += size;\n-            ++flushed_count;\n-            if (is_c2_method) {\n-              ++flushed_c2_count;\n-            }\n-            break;\n-          case MadeZombie:\n-            state_after = \"made zombie\";\n-            ++zombified_count;\n-            break;\n-          case None:\n-            break;\n-          default:\n-           ShouldNotReachHere();\n-        }\n-        if (PrintMethodFlushing && Verbose && type != None) {\n-          tty->print_cr(\"### %s nmethod %3d\/\" PTR_FORMAT \" (%s) %s\", is_osr ? \"osr\" : \"\", compile_id, address, state_before, state_after);\n-        }\n-      }\n-\n-      _seen++;\n-      CodeCache::Sweep::end();\n-      handle_safepoint_request();\n-    }\n-  }\n-\n-  assert(_current.end(), \"must have scanned the whole cache\");\n-\n-  const Ticks sweep_end_counter = Ticks::now();\n-  const Tickspan sweep_time = sweep_end_counter - sweep_start_counter;\n-  {\n-    MutexLocker mu(NMethodSweeperStats_lock, Mutex::_no_safepoint_check_flag);\n-    _total_time_sweeping  += sweep_time;\n-    _total_time_this_sweep += sweep_time;\n-    _peak_sweep_fraction_time = MAX2(sweep_time, _peak_sweep_fraction_time);\n-    _total_flushed_size += freed_memory;\n-    _total_nof_methods_reclaimed += flushed_count;\n-    _total_nof_c2_methods_reclaimed += flushed_c2_count;\n-    _peak_sweep_time = MAX2(_peak_sweep_time, _total_time_this_sweep);\n-  }\n-\n-#ifdef ASSERT\n-  if(PrintMethodFlushing) {\n-    tty->print_cr(\"### sweeper:      sweep time(\" JLONG_FORMAT \"): \", sweep_time.value());\n-  }\n-#endif\n-\n-  Log(codecache, sweep) log;\n-  if (log.is_debug()) {\n-    LogStream ls(log.debug());\n-    CodeCache::print_summary(&ls, false);\n-  }\n-  log_sweep(\"finished\");\n-\n-  \/\/ Sweeper is the only case where memory is released, check here if it\n-  \/\/ is time to restart the compiler. Only checking if there is a certain\n-  \/\/ amount of free memory in the code cache might lead to re-enabling\n-  \/\/ compilation although no memory has been released. For example, there are\n-  \/\/ cases when compilation was disabled although there is 4MB (or more) free\n-  \/\/ memory in the code cache. The reason is code cache fragmentation. Therefore,\n-  \/\/ it only makes sense to re-enable compilation if we have actually freed memory.\n-  \/\/ Note that typically several kB are released for sweeping 16MB of the code\n-  \/\/ cache. As a result, 'freed_memory' > 0 to restart the compiler.\n-  if (!CompileBroker::should_compile_new_jobs() && (freed_memory > 0)) {\n-    CompileBroker::set_should_compile_new_jobs(CompileBroker::run_compilation);\n-    log.debug(\"restart compiler\");\n-    log_sweep(\"restart_compiler\");\n-    EventJitRestart event;\n-    event.set_freedMemory(freed_memory);\n-    event.set_codeCacheMaxCapacity(CodeCache::max_capacity());\n-    event.commit();\n-  }\n-\n-  EventSweepCodeCache event(UNTIMED);\n-  if (event.should_commit()) {\n-    post_sweep_event(&event, sweep_start_counter, sweep_end_counter, (s4)_traversals, swept_count, flushed_count, zombified_count);\n-  }\n-}\n-\n- \/\/ This function updates the sweeper statistics that keep track of nmethods\n- \/\/ state changes. If there is 'enough' state change, the sweeper is invoked\n- \/\/ as soon as possible. Also, we are guaranteed to invoke the sweeper if\n- \/\/ the code cache gets full.\n-void NMethodSweeper::report_state_change(nmethod* nm) {\n-  Atomic::add(&_bytes_changed, (size_t)nm->total_size());\n-  if (Atomic::load(&_bytes_changed) > _sweep_threshold_bytes) {\n-    MutexLocker mu(CodeSweeper_lock, Mutex::_no_safepoint_check_flag);\n-    _should_sweep = true;\n-    CodeSweeper_lock->notify(); \/\/ Wake up sweeper.\n-  }\n-}\n-\n-class CompiledMethodMarker: public StackObj {\n- private:\n-  CodeCacheSweeperThread* _thread;\n- public:\n-  CompiledMethodMarker(CompiledMethod* cm) {\n-    JavaThread* current = JavaThread::current();\n-    assert (current->is_Code_cache_sweeper_thread(), \"Must be\");\n-    _thread = (CodeCacheSweeperThread*)current;\n-    if (!cm->is_zombie() && !cm->is_unloading()) {\n-      \/\/ Only expose live nmethods for scanning\n-      _thread->set_scanned_compiled_method(cm);\n-    }\n-  }\n-  ~CompiledMethodMarker() {\n-    _thread->set_scanned_compiled_method(NULL);\n-  }\n-};\n-\n-NMethodSweeper::MethodStateChange NMethodSweeper::process_compiled_method(CompiledMethod* cm) {\n-  assert(cm != NULL, \"sanity\");\n-  assert(!CodeCache_lock->owned_by_self(), \"just checking\");\n-\n-  MethodStateChange result = None;\n-  \/\/ Make sure this nmethod doesn't get unloaded during the scan,\n-  \/\/ since safepoints may happen during acquired below locks.\n-  CompiledMethodMarker nmm(cm);\n-  SWEEP(cm);\n-\n-  \/\/ Skip methods that are currently referenced by the VM\n-  if (cm->is_locked_by_vm()) {\n-    \/\/ But still remember to clean-up inline caches for alive nmethods\n-    if (cm->is_alive()) {\n-      \/\/ Clean inline caches that point to zombie\/non-entrant\/unloaded nmethods\n-      cm->cleanup_inline_caches(false);\n-      SWEEP(cm);\n-    }\n-    return result;\n-  }\n-\n-  if (cm->is_zombie()) {\n-    \/\/ All inline caches that referred to this nmethod were cleaned in the\n-    \/\/ previous sweeper cycle. Now flush the nmethod from the code cache.\n-    assert(!cm->is_locked_by_vm(), \"must not flush locked Compiled Methods\");\n-    cm->flush();\n-    assert(result == None, \"sanity\");\n-    result = Flushed;\n-  } else if (cm->is_not_entrant()) {\n-    \/\/ If there are no current activations of this method on the\n-    \/\/ stack we can safely convert it to a zombie method\n-    OrderAccess::loadload(); \/\/ _stack_traversal_mark and _state\n-    if (cm->can_convert_to_zombie()) {\n-      \/\/ Code cache state change is tracked in make_zombie()\n-      cm->make_zombie();\n-      SWEEP(cm);\n-      assert(result == None, \"sanity\");\n-      result = MadeZombie;\n-      assert(cm->is_zombie(), \"nmethod must be zombie\");\n-    } else {\n-      \/\/ Still alive, clean up its inline caches\n-      cm->cleanup_inline_caches(false);\n-      SWEEP(cm);\n-    }\n-  } else if (cm->is_unloaded()) {\n-    \/\/ Code is unloaded, so there are no activations on the stack.\n-    \/\/ Convert the nmethod to zombie.\n-    \/\/ Code cache state change is tracked in make_zombie()\n-    cm->make_zombie();\n-    SWEEP(cm);\n-    assert(result == None, \"sanity\");\n-    result = MadeZombie;\n-  } else {\n-    if (cm->is_nmethod()) {\n-      possibly_flush((nmethod*)cm);\n-    }\n-    \/\/ Clean inline caches that point to zombie\/non-entrant\/unloaded nmethods\n-    cm->cleanup_inline_caches(false);\n-    SWEEP(cm);\n-  }\n-  return result;\n-}\n-\n-\n-void NMethodSweeper::possibly_flush(nmethod* nm) {\n-  if (UseCodeCacheFlushing) {\n-    if (!nm->is_locked_by_vm() && !nm->is_native_method() && !nm->is_not_installed() && !nm->is_unloading()) {\n-      bool make_not_entrant = false;\n-\n-      \/\/ Do not make native methods not-entrant\n-      nm->dec_hotness_counter();\n-      \/\/ Get the initial value of the hotness counter. This value depends on the\n-      \/\/ ReservedCodeCacheSize\n-      int reset_val = hotness_counter_reset_val();\n-      int time_since_reset = reset_val - nm->hotness_counter();\n-      double threshold = -reset_val + (CodeCache::reverse_free_ratio() * NmethodSweepActivity);\n-      \/\/ The less free space in the code cache we have - the bigger reverse_free_ratio() is.\n-      \/\/ I.e., 'threshold' increases with lower available space in the code cache and a higher\n-      \/\/ NmethodSweepActivity. If the current hotness counter - which decreases from its initial\n-      \/\/ value until it is reset by stack walking - is smaller than the computed threshold, the\n-      \/\/ corresponding nmethod is considered for removal.\n-      if ((NmethodSweepActivity > 0) && (nm->hotness_counter() < threshold) && (time_since_reset > MinPassesBeforeFlush)) {\n-        \/\/ A method is marked as not-entrant if the method is\n-        \/\/ 1) 'old enough': nm->hotness_counter() < threshold\n-        \/\/ 2) The method was in_use for a minimum amount of time: (time_since_reset > MinPassesBeforeFlush)\n-        \/\/    The second condition is necessary if we are dealing with very small code cache\n-        \/\/    sizes (e.g., <10m) and the code cache size is too small to hold all hot methods.\n-        \/\/    The second condition ensures that methods are not immediately made not-entrant\n-        \/\/    after compilation.\n-        make_not_entrant = true;\n-      }\n-\n-      \/\/ The stack-scanning low-cost detection may not see the method was used (which can happen for\n-      \/\/ flat profiles). Check the age counter for possible data.\n-      if (UseCodeAging && make_not_entrant && (nm->is_compiled_by_c2() || nm->is_compiled_by_c1())) {\n-        MethodCounters* mc = nm->method()->get_method_counters(Thread::current());\n-        if (mc != NULL) {\n-          \/\/ Snapshot the value as it's changed concurrently\n-          int age = mc->nmethod_age();\n-          if (MethodCounters::is_nmethod_hot(age)) {\n-            \/\/ The method has gone through flushing, and it became relatively hot that it deopted\n-            \/\/ before we could take a look at it. Give it more time to appear in the stack traces,\n-            \/\/ proportional to the number of deopts.\n-            MethodData* md = nm->method()->method_data();\n-            if (md != NULL && time_since_reset > (int)(MinPassesBeforeFlush * (md->tenure_traps() + 1))) {\n-              \/\/ It's been long enough, we still haven't seen it on stack.\n-              \/\/ Try to flush it, but enable counters the next time.\n-              mc->reset_nmethod_age();\n-            } else {\n-              make_not_entrant = false;\n-            }\n-          } else if (MethodCounters::is_nmethod_warm(age)) {\n-            \/\/ Method has counters enabled, and the method was used within\n-            \/\/ previous MinPassesBeforeFlush sweeps. Reset the counter. Stay in the existing\n-            \/\/ compiled state.\n-            mc->reset_nmethod_age();\n-            \/\/ delay the next check\n-            nm->set_hotness_counter(NMethodSweeper::hotness_counter_reset_val());\n-            make_not_entrant = false;\n-          } else if (MethodCounters::is_nmethod_age_unset(age)) {\n-            \/\/ No counters were used before. Set the counters to the detection\n-            \/\/ limit value. If the method is going to be used again it will be compiled\n-            \/\/ with counters that we're going to use for analysis the next time.\n-            mc->reset_nmethod_age();\n-          } else {\n-            \/\/ Method was totally idle for 10 sweeps\n-            \/\/ The counter already has the initial value, flush it and may be recompile\n-            \/\/ later with counters\n-          }\n-        }\n-      }\n-\n-      if (make_not_entrant) {\n-        nm->make_not_entrant();\n-\n-        \/\/ Code cache state change is tracked in make_not_entrant()\n-        if (PrintMethodFlushing && Verbose) {\n-          tty->print_cr(\"### Nmethod %d\/\" PTR_FORMAT \"made not-entrant: hotness counter %d\/%d threshold %f\",\n-              nm->compile_id(), p2i(nm), nm->hotness_counter(), reset_val, threshold);\n-        }\n-      }\n-    }\n-  }\n-}\n-\n-\/\/ Print out some state information about the current sweep and the\n-\/\/ state of the code cache if it's requested.\n-void NMethodSweeper::log_sweep(const char* msg, const char* format, ...) {\n-  if (PrintMethodFlushing) {\n-    ResourceMark rm;\n-    stringStream s;\n-    \/\/ Dump code cache state into a buffer before locking the tty,\n-    \/\/ because log_state() will use locks causing lock conflicts.\n-    CodeCache::log_state(&s);\n-\n-    ttyLocker ttyl;\n-    tty->print(\"### sweeper: %s \", msg);\n-    if (format != NULL) {\n-      va_list ap;\n-      va_start(ap, format);\n-      tty->vprint(format, ap);\n-      va_end(ap);\n-    }\n-    tty->print_cr(\"%s\", s.as_string());\n-  }\n-\n-  if (LogCompilation && (xtty != NULL)) {\n-    ResourceMark rm;\n-    stringStream s;\n-    \/\/ Dump code cache state into a buffer before locking the tty,\n-    \/\/ because log_state() will use locks causing lock conflicts.\n-    CodeCache::log_state(&s);\n-\n-    ttyLocker ttyl;\n-    xtty->begin_elem(\"sweeper state='%s' traversals='\" INT64_FORMAT \"' \", msg, traversal_count());\n-    if (format != NULL) {\n-      va_list ap;\n-      va_start(ap, format);\n-      xtty->vprint(format, ap);\n-      va_end(ap);\n-    }\n-    xtty->print(\"%s\", s.as_string());\n-    xtty->stamp();\n-    xtty->end_elem();\n-  }\n-}\n-\n-void NMethodSweeper::print(outputStream* out) {\n-  ttyLocker ttyl;\n-  out = (out == NULL) ? tty : out;\n-  out->print_cr(\"Code cache sweeper statistics:\");\n-  out->print_cr(\"  Total sweep time:                %1.0lf ms\", (double)_total_time_sweeping.value()\/1000000);\n-  out->print_cr(\"  Total number of full sweeps:     \" INT64_FORMAT, _total_nof_code_cache_sweeps);\n-  out->print_cr(\"  Total number of flushed methods: \" INT64_FORMAT \" (thereof \" INT64_FORMAT \" C2 methods)\",\n-                                                    _total_nof_methods_reclaimed,\n-                                                    _total_nof_c2_methods_reclaimed);\n-  out->print_cr(\"  Total size of flushed methods:   \" SIZE_FORMAT \" kB\", _total_flushed_size\/K);\n-}\n","filename":"src\/hotspot\/share\/runtime\/sweeper.cpp","additions":0,"deletions":680,"binary":false,"changes":680,"status":"deleted"},{"patch":"@@ -1,126 +0,0 @@\n-\/*\n- * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#ifndef SHARE_RUNTIME_SWEEPER_HPP\n-#define SHARE_RUNTIME_SWEEPER_HPP\n-\n-class WhiteBox;\n-\n-#include \"code\/codeCache.hpp\"\n-#include \"utilities\/ticks.hpp\"\n-\n-class CodeBlobClosure;\n-\n-\/\/ An NmethodSweeper is an incremental cleaner for:\n-\/\/    - cleanup inline caches\n-\/\/    - reclamation of nmethods\n-\/\/ Removing nmethods from the code cache includes two operations\n-\/\/  1) mark active nmethods\n-\/\/     Is done in 'do_stack_scanning()'. This function invokes a thread-local handshake\n-\/\/     that marks all nmethods that are active on a thread's stack, and resets their\n-\/\/     hotness counters. This allows the sweeper to assume that a decayed hotness counter\n-\/\/     of an nmethod implies that it is seemingly not used actively.\n-\/\/  2) sweep nmethods\n-\/\/     Is done in sweep_code_cache(). This function is the only place in the\n-\/\/     sweeper where memory is reclaimed. Note that sweep_code_cache() is not\n-\/\/     called at a safepoint. However, sweep_code_cache() stops executing if\n-\/\/     another thread requests a safepoint. Consequently, 'mark_active_nmethods()'\n-\/\/     and sweep_code_cache() cannot execute at the same time.\n-\/\/     To reclaim memory, nmethods are first marked as 'not-entrant'. Methods can\n-\/\/     be made not-entrant by (i) the sweeper, (ii) deoptimization, (iii) dependency\n-\/\/     invalidation, and (iv) being replaced by a different method version (tiered\n-\/\/     compilation). Not-entrant nmethods cannot be called by Java threads, but they\n-\/\/     can still be active on the stack. To ensure that active nmethods are not reclaimed,\n-\/\/     we have to wait until the next marking phase has completed. If a not-entrant\n-\/\/     nmethod was NOT marked as active, it can be converted to 'zombie' state. To safely\n-\/\/     remove the nmethod, all inline caches (IC) that point to the nmethod must be\n-\/\/     cleared. After that, the nmethod can be evicted from the code cache. Each nmethod's\n-\/\/     state change happens during separate sweeps. It may take at least 3 sweeps before an\n-\/\/     nmethod's space is freed.\n-\n-class NMethodSweeper : public AllStatic {\n- private:\n-  enum MethodStateChange {\n-    None,\n-    MadeZombie,\n-    Flushed\n-  };\n-  static int64_t   _traversals;                   \/\/ Stack scan count, also sweep ID.\n-  static int64_t   _total_nof_code_cache_sweeps;  \/\/ Total number of full sweeps of the code cache\n-  static CompiledMethodIterator _current;         \/\/ Current compiled method\n-  static int       _seen;                         \/\/ Nof. nmethod we have currently processed in current pass of CodeCache\n-  static size_t    _sweep_threshold_bytes;        \/\/ The threshold for when to invoke sweeps\n-\n-  static volatile bool _should_sweep;             \/\/ Indicates if a normal sweep will be done\n-  static volatile bool _force_sweep;              \/\/ Indicates if a forced sweep will be done\n-  static volatile size_t _bytes_changed;          \/\/ Counts the total nmethod size if the nmethod changed from:\n-                                                  \/\/   1) alive       -> not_entrant\n-                                                  \/\/   2) not_entrant -> zombie\n-  \/\/ Stat counters\n-  static int64_t   _total_nof_methods_reclaimed;    \/\/ Accumulated nof methods flushed\n-  static int64_t   _total_nof_c2_methods_reclaimed; \/\/ Accumulated nof C2-compiled methods flushed\n-  static size_t    _total_flushed_size;             \/\/ Total size of flushed methods\n-  static int       _hotness_counter_reset_val;\n-\n-  static Tickspan  _total_time_sweeping;          \/\/ Accumulated time sweeping\n-  static Tickspan  _total_time_this_sweep;        \/\/ Total time this sweep\n-  static Tickspan  _peak_sweep_time;              \/\/ Peak time for a full sweep\n-  static Tickspan  _peak_sweep_fraction_time;     \/\/ Peak time sweeping one fraction\n-\n-  static MethodStateChange process_compiled_method(CompiledMethod *nm);\n-\n-  static void init_sweeper_log() NOT_DEBUG_RETURN;\n-  static bool wait_for_stack_scanning();\n-  static void sweep_code_cache();\n-  static void handle_safepoint_request();\n-  static void do_stack_scanning();\n-  static void sweep();\n- public:\n-  static int64_t traversal_count()                 { return _traversals; }\n-  static size_t sweep_threshold_bytes()              { return _sweep_threshold_bytes; }\n-  static void set_sweep_threshold_bytes(size_t threshold) { _sweep_threshold_bytes = threshold; }\n-  static int64_t total_nof_methods_reclaimed()     { return _total_nof_methods_reclaimed; }\n-  static const Tickspan total_time_sweeping()      { return _total_time_sweeping; }\n-  static const Tickspan peak_sweep_time()          { return _peak_sweep_time; }\n-  static const Tickspan peak_sweep_fraction_time() { return _peak_sweep_fraction_time; }\n-  static void log_sweep(const char* msg, const char* format = NULL, ...) ATTRIBUTE_PRINTF(2, 3);\n-\n-#ifdef ASSERT\n-  \/\/ Keep track of sweeper activity in the ring buffer\n-  static void record_sweep(CompiledMethod* nm, int line);\n-#endif\n-\n-  static CodeBlobClosure* prepare_mark_active_nmethods();\n-  static void sweeper_loop();\n-  static bool should_start_aggressive_sweep();\n-  static void force_sweep();\n-  static int hotness_counter_reset_val();\n-  static void report_state_change(nmethod* nm);\n-  static void report_allocation();  \/\/ Possibly start the sweeper thread.\n-  static void possibly_flush(nmethod* nm);\n-  static void print(outputStream* out);   \/\/ Printing\/debugging\n-  static void print() { print(tty); }\n-};\n-\n-#endif \/\/ SHARE_RUNTIME_SWEEPER_HPP\n","filename":"src\/hotspot\/share\/runtime\/sweeper.hpp","additions":0,"deletions":126,"binary":false,"changes":126,"status":"deleted"},{"patch":"@@ -326,1 +326,0 @@\n-  virtual bool is_Code_cache_sweeper_thread() const  { return false; }\n","filename":"src\/hotspot\/share\/runtime\/thread.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -94,1 +94,1 @@\n-    CodeCache::cleanup_inline_caches();\n+    CodeCache::cleanup_inline_caches_whitebox();\n","filename":"src\/hotspot\/share\/runtime\/vmOperations.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -291,1 +291,0 @@\n-  nonstatic_field(MethodCounters,              _nmethod_age,                                  int)                                   \\\n@@ -665,2 +664,0 @@\n-  volatile_nonstatic_field(nmethod,            _lock_count,                                   jint)                                  \\\n-  volatile_nonstatic_field(nmethod,            _stack_traversal_mark,                         int64_t)                               \\\n@@ -1321,1 +1318,0 @@\n-        declare_type(CodeCacheSweeperThread, JavaThread)                  \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":0,"deletions":4,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -186,6 +186,0 @@\n-  \/\/ Reclamation support (really only used by the nmethods, but in order to get asserts to work\n-  \/\/ in the CodeCache they are defined virtual here)\n-  public boolean isZombie()             { return false; }\n-\n-  public boolean isLockedByVM()         { return false; }\n-\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/code\/CodeBlob.java","additions":0,"deletions":6,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -95,3 +95,0 @@\n-    if (Assert.ASSERTS_ENABLED) {\n-      Assert.that(!(result.isZombie() || result.isLockedByVM()), \"unsafe access to zombie method\");\n-    }\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/code\/CodeCache.java","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -66,10 +66,0 @@\n-  \/** NMethod Flushing lock (if non-zero, then the nmethod is not removed) *\/\n-  private static JIntField     lockCountField;\n-\n-  \/** not_entrant method removal. Each mark_sweep pass will update\n-      this mark to current sweep invocation count if it is seen on the\n-      stack.  An not_entrant method can be removed when there is no\n-      more activations, i.e., when the _stack_traversal_mark is less than\n-      current sweep traversal index. *\/\n-  private static CIntegerField stackTraversalMarkField;\n-\n@@ -105,2 +95,0 @@\n-    lockCountField              = type.getJIntField(\"_lock_count\");\n-    stackTraversalMarkField     = type.getCIntegerField(\"_stack_traversal_mark\");\n@@ -218,1 +206,0 @@\n-  \/\/ public boolean isAlive();\n@@ -220,1 +207,0 @@\n-  \/\/ public boolean isZombie();\n@@ -225,3 +211,0 @@\n-  public boolean isZombie() { return false; }\n-\n-  \/\/ public boolean isUnloaded();\n@@ -276,2 +259,0 @@\n-  public boolean isLockedByVM() { return lockCountField.getValue(addr) > 0; }\n-\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/code\/NMethod.java","additions":0,"deletions":19,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -1,41 +0,0 @@\n-\/*\n- * Copyright (c) 2000, 2014, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-package sun.jvm.hotspot.runtime;\n-\n-import java.io.*;\n-import java.util.*;\n-import sun.jvm.hotspot.debugger.*;\n-import sun.jvm.hotspot.types.*;\n-\n-public class CodeCacheSweeperThread extends JavaThread {\n-  public CodeCacheSweeperThread(Address addr) {\n-    super(addr);\n-  }\n-\n-  public boolean isJavaThread() { return false; }\n-  public boolean isHiddenFromExternalView() { return true; }\n-  public boolean isCodeCacheSweeperThread() { return true; }\n-\n-}\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/runtime\/CodeCacheSweeperThread.java","additions":0,"deletions":41,"binary":false,"changes":41,"status":"deleted"},{"patch":"@@ -127,1 +127,1 @@\n-      Only \"pure\" JavaThreads return true; CompilerThreads, the CodeCacheSweeperThread,\n+      Only \"pure\" JavaThreads return true; CompilerThreads,\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/runtime\/JavaThread.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -154,1 +154,0 @@\n-            virtualConstructor.addMapping(\"CodeCacheSweeperThread\", CodeCacheSweeperThread.class);\n@@ -198,1 +197,1 @@\n-            \" (expected type JavaThread, CompilerThread, MonitorDeflationThread, ServiceThread, JvmtiAgentThread or CodeCacheSweeperThread)\", e);\n+            \" (expected type JavaThread, CompilerThread, MonitorDeflationThread, ServiceThread or JvmtiAgentThread)\", e);\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/runtime\/Threads.java","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -543,15 +543,0 @@\n-    <event name=\"jdk.CodeSweeperConfiguration\">\n-      <setting name=\"enabled\" control=\"compiler-enabled\">true<\/setting>\n-      <setting name=\"period\">beginChunk<\/setting>\n-    <\/event>\n-\n-    <event name=\"jdk.CodeSweeperStatistics\">\n-      <setting name=\"enabled\" control=\"compiler-enabled\">true<\/setting>\n-      <setting name=\"period\">everyChunk<\/setting>\n-    <\/event>\n-\n-    <event name=\"jdk.SweepCodeCache\">\n-      <setting name=\"enabled\" control=\"compiler-enabled\">true<\/setting>\n-      <setting name=\"threshold\" control=\"compiler-sweeper-threshold\">100 ms<\/setting>\n-    <\/event>\n-\n","filename":"src\/jdk.jfr\/share\/conf\/jfr\/default.jfc","additions":0,"deletions":15,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -543,15 +543,0 @@\n-    <event name=\"jdk.CodeSweeperConfiguration\">\n-      <setting name=\"enabled\" control=\"compiler-enabled\">true<\/setting>\n-      <setting name=\"period\">beginChunk<\/setting>\n-    <\/event>\n-\n-    <event name=\"jdk.CodeSweeperStatistics\">\n-      <setting name=\"enabled\" control=\"compiler-enabled\">true<\/setting>\n-      <setting name=\"period\">everyChunk<\/setting>\n-    <\/event>\n-\n-    <event name=\"jdk.SweepCodeCache\">\n-      <setting name=\"enabled\" control=\"compiler-enabled\">true<\/setting>\n-      <setting name=\"threshold\" control=\"compiler-sweeper-threshold\">100 ms<\/setting>\n-    <\/event>\n-\n","filename":"src\/jdk.jfr\/share\/conf\/jfr\/profile.jfc","additions":0,"deletions":15,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -1,88 +0,0 @@\n-\/*\n- * Copyright (c) 2016, 2020, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"code\/dependencyContext.hpp\"\n-#include \"code\/nmethod.hpp\"\n-#include \"runtime\/mutexLocker.hpp\"\n-#include \"unittest.hpp\"\n-\n-class TestDependencyContext {\n- public:\n-  nmethod _nmethods[3];\n-\n-  nmethodBucket* volatile _dependency_context;\n-  volatile uint64_t _last_cleanup;\n-\n-  DependencyContext dependencies() {\n-    DependencyContext depContext(&_dependency_context, &_last_cleanup);\n-    return depContext;\n-  }\n-\n-  TestDependencyContext()\n-    : _dependency_context(NULL),\n-      _last_cleanup(0) {\n-    CodeCache_lock->lock_without_safepoint_check();\n-\n-    _nmethods[0].clear_unloading_state();\n-    _nmethods[1].clear_unloading_state();\n-    _nmethods[2].clear_unloading_state();\n-\n-    dependencies().add_dependent_nmethod(&_nmethods[2]);\n-    dependencies().add_dependent_nmethod(&_nmethods[1]);\n-    dependencies().add_dependent_nmethod(&_nmethods[0]);\n-  }\n-\n-  ~TestDependencyContext() {\n-    wipe();\n-    CodeCache_lock->unlock();\n-  }\n-\n-  void wipe() {\n-    DependencyContext ctx(&_dependency_context, &_last_cleanup);\n-    nmethodBucket* b = ctx.dependencies();\n-    ctx.set_dependencies(NULL);\n-    while (b != NULL) {\n-      nmethodBucket* next = b->next();\n-      delete b;\n-      b = next;\n-    }\n-  }\n-};\n-\n-static void test_remove_dependent_nmethod(int id) {\n-  TestDependencyContext c;\n-  DependencyContext depContext = c.dependencies();\n-\n-  nmethod* nm = &c._nmethods[id];\n-  depContext.remove_dependent_nmethod(nm);\n-\n-  ASSERT_FALSE(depContext.is_dependent_nmethod(nm));\n-}\n-\n-TEST_VM(code, dependency_context) {\n-  test_remove_dependent_nmethod(0);\n-  test_remove_dependent_nmethod(1);\n-  test_remove_dependent_nmethod(2);\n-}\n","filename":"test\/hotspot\/gtest\/code\/test_dependencyContext.cpp","additions":0,"deletions":88,"binary":false,"changes":88,"status":"deleted"},{"patch":"@@ -46,2 +46,1 @@\n-        String pair = \" #\\\\d+ live = \" + entry\n-                    + \" #\\\\d+ dead = \" + entry;\n+        String pair = \" #\\\\d+ live = \" + entry;\n","filename":"test\/hotspot\/jtreg\/compiler\/codecache\/CheckCodeCacheInfo.java","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -123,1 +123,1 @@\n-            \/\/ Convert some nmethods to zombie and then free them to re-enable compilation\n+            \/\/ Let the GC free nmethods and re-enable compilation\n@@ -125,2 +125,1 @@\n-            WHITE_BOX.forceNMethodSweep();\n-            WHITE_BOX.forceNMethodSweep();\n+            WHITE_BOX.fullGC();\n","filename":"test\/hotspot\/jtreg\/compiler\/codecache\/OverflowCodeCacheTest.java","additions":2,"deletions":3,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -115,1 +115,1 @@\n-    \/\/ Completely unload (i.e. make \"not-entrant\"->\"zombie\"->\"unload\/free\") a JIT-compiled\n+    \/\/ Completely unload (i.e. make \"not-entrant\"->free) a JIT-compiled\n@@ -119,3 +119,0 @@\n-        WB.forceNMethodSweep();  \/\/ Makes all \"not entrant\" nmethods \"zombie\". This requires\n-        WB.forceNMethodSweep();  \/\/ two sweeps, see 'nmethod::can_convert_to_zombie()' for why.\n-        WB.forceNMethodSweep();  \/\/ Need third sweep to actually unload\/free all \"zombie\" nmethods.\n","filename":"test\/hotspot\/jtreg\/compiler\/exceptions\/OptimizeImplicitExceptions.java","additions":1,"deletions":4,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -174,1 +174,1 @@\n-                whiteBox.forceNMethodSweep();\n+                whiteBox.fullGC();\n","filename":"test\/hotspot\/jtreg\/compiler\/jsr292\/ContinuousCallSiteTargetChange.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -61,1 +61,1 @@\n-        \/\/ check that Sweeper handels dummy blobs correctly\n+        \/\/ check that code unloading handles dummy blobs correctly\n@@ -63,2 +63,2 @@\n-                new InfiniteLoop(WHITE_BOX::forceNMethodSweep, 1L),\n-                \"ForcedSweeper\");\n+                new InfiniteLoop(WHITE_BOX::fullGC, 1L),\n+                \"ForcedGC\");\n","filename":"test\/hotspot\/jtreg\/compiler\/whitebox\/AllocationCodeBlobTest.java","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -72,1 +72,1 @@\n-        guaranteedSweep();\n+        WHITE_BOX.fullGC();\n@@ -81,1 +81,1 @@\n-        guaranteedSweep();\n+        WHITE_BOX.fullGC();\n@@ -87,1 +87,1 @@\n-        guaranteedSweep();\n+        WHITE_BOX.fullGC();\n@@ -100,7 +100,0 @@\n-    private void guaranteedSweep() {\n-        \/\/ not entrant -> ++stack_traversal_mark -> zombie -> flushed\n-        for (int i = 0; i < 5; ++i) {\n-            WHITE_BOX.fullGC();\n-            WHITE_BOX.forceNMethodSweep();\n-        }\n-    }\n","filename":"test\/hotspot\/jtreg\/compiler\/whitebox\/ForceNMethodSweepTest.java","additions":3,"deletions":10,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -81,1 +81,0 @@\n-                    \"Sweeper thread\", \"Service Thread\",\n","filename":"test\/hotspot\/jtreg\/serviceability\/sa\/ClhsdbPstack.java","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -57,1 +57,0 @@\n-                    \"Java Stack Trace for Sweeper thread\",\n","filename":"test\/hotspot\/jtreg\/serviceability\/sa\/ClhsdbWhere.java","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -65,1 +65,1 @@\n-                    \"-XX:+UseCodeAging\",\n+                    \"-XX:+UseCodeCacheFlushing\",\n@@ -100,1 +100,1 @@\n-            checkOrigin(\"UseCodeAging\", Origin.VM_CREATION);\n+            checkOrigin(\"UseCodeCacheFlushing\", Origin.VM_CREATION);\n","filename":"test\/jdk\/com\/sun\/management\/HotSpotDiagnosticMXBean\/CheckOrigin.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -42,1 +42,1 @@\n- * Test for events: vm\/code_sweeper\/sweep vm\/code_cache\/full vm\/compiler\/failure\n+ * Test for events: vm\/code_cache\/full vm\/compiler\/failure\n@@ -44,3 +44,1 @@\n- * We verify: 1. That sweptCount >= flushedCount + zombifiedCount 2. That\n- * sweepIndex increases by 1. 3. We should get at least one of each of the\n- * events listed above.\n+ * We verify that we should get at least one of each of the events listed above.\n@@ -68,1 +66,0 @@\n-    private static final String pathSweep = EventNames.SweepCodeCache;\n@@ -85,1 +82,0 @@\n-        r.enable(pathSweep);\n@@ -92,1 +88,0 @@\n-        int countEventSweep = 0;\n@@ -100,4 +95,0 @@\n-            case pathSweep:\n-                countEventSweep++;\n-                verifySingleSweepEvent(event);\n-                break;\n@@ -115,1 +106,1 @@\n-        System.out.println(String.format(\"eventCount: %d, %d, %d\", countEventSweep, countEventFull, countEventFailure));\n+        System.out.println(String.format(\"eventCount: %d, %d\", countEventFull, countEventFailure));\n@@ -134,1 +125,0 @@\n-        \/\/ to trigger the vm\/code_sweeper\/sweep event.\n@@ -198,9 +188,0 @@\n-    private static void verifySingleSweepEvent(RecordedEvent event) throws Throwable {\n-        int flushedCount = Events.assertField(event, \"flushedCount\").atLeast(0).getValue();\n-        int zombifiedCount = Events.assertField(event, \"zombifiedCount\").atLeast(0).getValue();\n-        Events.assertField(event, \"sweptCount\").atLeast(flushedCount + zombifiedCount);\n-        Events.assertField(event, \"sweepId\").atLeast(0);\n-        Asserts.assertGreaterThanOrEqual(event.getStartTime(), Instant.EPOCH, \"startTime was < 0\");\n-        Asserts.assertGreaterThanOrEqual(event.getEndTime(), event.getStartTime(), \"startTime was > endTime\");\n-    }\n-\n","filename":"test\/jdk\/jdk\/jfr\/event\/compiler\/TestCodeSweeper.java","additions":3,"deletions":22,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -1,60 +0,0 @@\n-\/*\n- * Copyright (c) 2013, 2018, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-package jdk.jfr.event.compiler;\n-\n-import java.util.List;\n-\n-import jdk.jfr.Recording;\n-import jdk.jfr.consumer.RecordedEvent;\n-import jdk.test.lib.jfr.EventNames;\n-import jdk.test.lib.jfr.Events;\n-\n-\n-\/**\n- * @test\n- * @key jfr\n- * @requires vm.hasJFR\n- * @library \/test\/lib\n- * @run main\/othervm -XX:+UseCodeCacheFlushing -XX:-SegmentedCodeCache jdk.jfr.event.compiler.TestCodeSweeperConfig\n- * @run main\/othervm -XX:+UseCodeCacheFlushing -XX:+SegmentedCodeCache jdk.jfr.event.compiler.TestCodeSweeperConfig\n- *\/\n-public class TestCodeSweeperConfig {\n-\n-    private final static String EVENT_NAME = EventNames.CodeSweeperConfiguration;\n-\n-    public static void main(String[] args) throws Exception {\n-        Recording recording = new Recording();\n-        recording.enable(EVENT_NAME);\n-        recording.start();\n-        recording.stop();\n-\n-        List<RecordedEvent> events = Events.fromRecording(recording);\n-        Events.hasEvents(events);\n-        for (RecordedEvent event : events) {\n-            System.out.println(\"Event: \" + event);\n-            Events.assertField(event, \"sweeperEnabled\").equal(true);\n-            Events.assertField(event, \"flushingEnabled\").equal(true);\n-        }\n-    }\n-}\n","filename":"test\/jdk\/jdk\/jfr\/event\/compiler\/TestCodeSweeperConfig.java","additions":0,"deletions":60,"binary":false,"changes":60,"status":"deleted"},{"patch":"@@ -1,145 +0,0 @@\n-\/*\n- * Copyright (c) 2013, 2022, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-package jdk.jfr.event.compiler;\n-\n-import java.io.File;\n-import java.lang.reflect.Method;\n-import java.net.MalformedURLException;\n-import java.net.URL;\n-import java.nio.file.Paths;\n-import java.util.List;\n-\n-import jdk.test.whitebox.WhiteBox;\n-import jdk.jfr.Recording;\n-import jdk.jfr.consumer.RecordedEvent;\n-import jdk.test.lib.classloader.FilterClassLoader;\n-import jdk.test.lib.classloader.ParentLastURLClassLoader;\n-import jdk.test.lib.jfr.EventNames;\n-import jdk.test.lib.jfr.Events;\n-import jdk.test.lib.Utils;\n-\n-\/**\n- * @test TestCodeSweeperStats\n- * @key jfr\n- * @requires vm.hasJFR\n- * @library \/test\/lib\n- * @requires vm.compMode!=\"Xint\"\n- * @build jdk.test.whitebox.WhiteBox\n- * @run driver jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox\n- * @run main\/othervm -Xbootclasspath\/a:.\n- *     -XX:+UnlockDiagnosticVMOptions -XX:+WhiteBoxAPI\n- *     -XX:CompileOnly=jdk.jfr.event.compiler.TestCodeSweeperStats::dummyMethod\n- *     -XX:+SegmentedCodeCache jdk.jfr.event.compiler.TestCodeSweeperStats\n- * @run main\/othervm -Xbootclasspath\/a:.\n- *     -XX:+UnlockDiagnosticVMOptions -XX:+WhiteBoxAPI\n- *     -XX:CompileOnly=jdk.jfr.event.compiler.TestCodeSweeperStats::dummyMethod\n- *     -XX:-SegmentedCodeCache jdk.jfr.event.compiler.TestCodeSweeperStats\n- *\/\n-public class TestCodeSweeperStats {\n-    private static final String EVENT_NAME = EventNames.CodeSweeperStatistics;\n-    private static final int WAIT_TIME = 10_000;\n-    private static final String CLASS_METHOD_TO_COMPILE = \"dummyMethod\";\n-    private static final int METHODS_TO_COMPILE = Integer.getInteger(\"compile.methods.count\", 10);\n-    private static final int COMP_LEVEL_SIMPLE = 1;\n-    private static final int COMP_LEVEL_FULL_OPTIMIZATION = 4;\n-\n-    public static void main(String[] args) throws Exception {\n-        Recording recording = new Recording();\n-        recording.enable(EVENT_NAME).with(\"period\", \"endChunk\");\n-        recording.start();\n-        compileAndSweep();\n-        recording.stop();\n-\n-        List<RecordedEvent> events = Events.fromRecording(recording);\n-        Events.hasEvents(events);\n-        for (RecordedEvent event : events) {\n-            Events.assertField(event, \"sweepCount\").atLeast(1);\n-            Events.assertField(event, \"methodReclaimedCount\").equal(METHODS_TO_COMPILE);\n-            Events.assertField(event, \"totalSweepTime\").atLeast(0L);\n-            Events.assertField(event, \"peakFractionTime\").atLeast(0L);\n-            Events.assertField(event, \"peakSweepTime\").atLeast(0L);\n-        }\n-    }\n-\n-    private static void compileAndSweep() throws InterruptedException {\n-        WhiteBox WB = WhiteBox.getWhiteBox();\n-        for (int i = 0; i < METHODS_TO_COMPILE; i++) {\n-            System.out.println(\"compile \" + i);\n-            compileMethod();\n-        }\n-\n-        WB.deoptimizeAll();\n-        System.out.println(\"All methods deoptimized\");\n-\n-        \/\/ method will be sweeped out of code cache after 5 sweep cycles\n-        for (int i = 0; i < 5; i++) {\n-            WB.fullGC();\n-            WB.forceNMethodSweep();\n-\n-        }\n-        \/\/ now wait for event(s) to be fired\n-        Thread.sleep(WAIT_TIME);\n-    }\n-\n-    public void dummyMethod() {\n-        System.out.println(\"Hello World!\");\n-    }\n-\n-    protected static void compileMethod() {\n-        ClassLoader current = TestCodeSweeperStats.class.getClassLoader();\n-        String[] cpaths = System.getProperty(\"test.classes\", \".\").split(File.pathSeparator);\n-        URL[] urls = new URL[cpaths.length];\n-        try {\n-            for (int i = 0; i < cpaths.length; i++) {\n-                urls[i] = Paths.get(cpaths[i]).toUri().toURL();\n-            }\n-        } catch (MalformedURLException e) {\n-            throw new Error(e);\n-        }\n-\n-        String currentClassName = TestCodeSweeperStats.class.getName();\n-        FilterClassLoader cl = new FilterClassLoader(new ParentLastURLClassLoader(urls, current), ClassLoader.getSystemClassLoader(), (name) -> currentClassName.equals(name));\n-        Class<?> loadedClass = null;\n-        String className = currentClassName;\n-        try {\n-            loadedClass = cl.loadClass(className);\n-        } catch (ClassNotFoundException ex) {\n-            throw new Error(\"Couldn't load class \" + className, ex);\n-        }\n-        try {\n-            Method mtd = loadedClass.getMethod(CLASS_METHOD_TO_COMPILE);\n-            WhiteBox WB = WhiteBox.getWhiteBox();\n-            WB.testSetDontInlineMethod(mtd, true);\n-            String directive = \"[{ match: \\\"\" + TestCodeSweeperStats.class.getName().replace('.', '\/')\n-                    + \".\" + CLASS_METHOD_TO_COMPILE + \"\\\", \" + \"BackgroundCompilation: false }]\";\n-            WB.addCompilerDirective(directive);\n-            if (!WB.enqueueMethodForCompilation(mtd, COMP_LEVEL_FULL_OPTIMIZATION)) {\n-                WB.enqueueMethodForCompilation(mtd, COMP_LEVEL_SIMPLE);\n-            }\n-            Utils.waitForCondition(() -> WB.isMethodCompiled(mtd));\n-        } catch (NoSuchMethodException e) {\n-            throw new Error(\"An exception while trying compile method \" + e.getMessage(), e);\n-        }\n-    }\n-}\n","filename":"test\/jdk\/jdk\/jfr\/event\/compiler\/TestCodeSweeperStats.java","additions":0,"deletions":145,"binary":false,"changes":145,"status":"deleted"},{"patch":"@@ -75,1 +75,1 @@\n-        WHITE_BOX.forceNMethodSweep();\n+        WHITE_BOX.fullGC();\n","filename":"test\/jdk\/jdk\/jfr\/event\/compiler\/TestJitRestart.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -159,3 +159,0 @@\n-    public final static String CodeSweeperStatistics = PREFIX + \"CodeSweeperStatistics\";\n-    public final static String CodeSweeperConfiguration = PREFIX + \"CodeSweeperConfiguration\";\n-    public final static String SweepCodeCache = PREFIX + \"SweepCodeCache\";\n","filename":"test\/lib\/jdk\/test\/lib\/jfr\/EventNames.java","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -404,1 +404,0 @@\n-  public native void    forceNMethodSweep();\n","filename":"test\/lib\/jdk\/test\/whitebox\/WhiteBox.java","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"}]}