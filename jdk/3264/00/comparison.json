{"files":[{"patch":"@@ -49,0 +49,1 @@\n+#include \"utilities\/population_count.hpp\"\n@@ -311,0 +312,14 @@\n+\/\/ Merge new allocation bits into _allocated_bitmask.  Only one thread at a\n+\/\/ time is ever allocating from a block, but other threads may concurrently\n+\/\/ release entries and clear bits in _allocated_bitmask.\n+\/\/ precondition: _allocated_bitmask & add == 0\n+void OopStorage::Block::atomic_add_allocated(uintx add) {\n+  \/\/ Since the current allocated bitmask has no set bits in common with add,\n+  \/\/ we can use an atomic add to implement the operation.  The assert post\n+  \/\/ facto verifies the precondition held; if there were any set bits in\n+  \/\/ common, then after the add at least one of them will be zero.\n+  uintx sum = Atomic::add(&_allocated_bitmask, add);\n+  assert((sum & add) == add, \"some already present: \" UINTX_FORMAT \":\" UINTX_FORMAT,\n+         sum, add);\n+}\n+\n@@ -312,1 +327,0 @@\n-  \/\/ Use CAS loop because release may change bitmask outside of lock.\n@@ -314,10 +328,13 @@\n-  while (true) {\n-    assert(!is_full_bitmask(allocated), \"attempt to allocate from full block\");\n-    unsigned index = count_trailing_zeros(~allocated);\n-    uintx new_value = allocated | bitmask_for_index(index);\n-    uintx fetched = Atomic::cmpxchg(&_allocated_bitmask, allocated, new_value);\n-    if (fetched == allocated) {\n-      return get_pointer(index); \/\/ CAS succeeded; return entry for index.\n-    }\n-    allocated = fetched;       \/\/ CAS failed; retry with latest value.\n-  }\n+  assert(!is_full_bitmask(allocated), \"attempt to allocate from full block\");\n+  unsigned index = count_trailing_zeros(~allocated);\n+  \/\/ Use atomic update because release may change bitmask.\n+  atomic_add_allocated(bitmask_for_index(index));\n+  return get_pointer(index);\n+}\n+\n+uintx OopStorage::Block::allocate_all() {\n+  uintx new_allocated = ~allocated_bitmask();\n+  assert(new_allocated != 0, \"attempt to allocate from full block\");\n+  \/\/ Use atomic update because release may change bitmask.\n+  atomic_add_allocated(new_allocated);\n+  return new_allocated;\n@@ -429,1 +446,1 @@\n-    log_trace(oopstorage, blocks)(\"%s: block not empty \" PTR_FORMAT, name(), p2i(block));\n+    log_block_transition(block, \"not empty\");\n@@ -438,1 +455,1 @@\n-    log_trace(oopstorage, blocks)(\"%s: block full \" PTR_FORMAT, name(), p2i(block));\n+    log_block_transition(block, \"full\");\n@@ -445,0 +462,54 @@\n+\/\/ Bulk allocation takes the first block off the _allocation_list, and marks\n+\/\/ all remaining entries in that block as allocated.  It then drops the lock\n+\/\/ and fills buffer with those newly allocated entries.  If more entries\n+\/\/ were obtained than requested, the remaining entries are released back\n+\/\/ (which is a lock-free operation).  Finally, the number actually added to\n+\/\/ the buffer is returned.  It's best to request at least as many entries as\n+\/\/ a single block can provide, to avoid the release case.  That number is\n+\/\/ available as bulk_allocate_limit.\n+size_t OopStorage::allocate(oop** ptrs, size_t size) {\n+  assert(size > 0, \"precondition\");\n+  Block* block;\n+  uintx taken;\n+  {\n+    MutexLocker ml(_allocation_mutex, Mutex::_no_safepoint_check_flag);\n+    block = block_for_allocation();\n+    if (block == NULL) return 0; \/\/ Block allocation failed.\n+    \/\/ Take exclusive use of this block for allocation.\n+    _allocation_list.unlink(*block);\n+    \/\/ Transitioning from empty to not empty.\n+    if (block->is_empty()) {\n+      log_block_transition(block, \"not empty\");\n+    }\n+    taken = block->allocate_all();\n+    \/\/ Safe to drop the lock, since we have claimed our entries.\n+    assert(!is_empty_bitmask(taken), \"invariant\");\n+  } \/\/ Drop lock, now that we've taken all available entries from block.\n+  size_t num_taken = population_count(taken);\n+  Atomic::add(&_allocation_count, num_taken);\n+  \/\/ Fill ptrs from those taken entries.\n+  size_t limit = MIN2(num_taken, size);\n+  for (size_t i = 0; i < limit; ++i) {\n+    assert(taken != 0, \"invariant\");\n+    unsigned index = count_trailing_zeros(taken);\n+    taken ^= block->bitmask_for_index(index);\n+    ptrs[i] = block->get_pointer(index);\n+  }\n+  \/\/ If more entries taken than requested, release remainder.\n+  if (taken == 0) {\n+    assert(num_taken == limit, \"invariant\");\n+  } else {\n+    assert(size == limit, \"invariant\");\n+    assert(num_taken == (limit + population_count(taken)), \"invariant\");\n+    block->release_entries(taken, this);\n+    Atomic::sub(&_allocation_count, num_taken - limit);\n+  }\n+  log_trace(oopstorage, ref)(\"%s: bulk allocate %zu, returned %zu\",\n+                             name(), limit, num_taken - limit);\n+  return limit;                 \/\/ Return number allocated.\n+}\n+\n+void OopStorage::log_block_transition(Block* block, const char* new_state) const {\n+  log_trace(oopstorage, blocks)(\"%s: block %s \" PTR_FORMAT, name(), new_state, p2i(block));\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shared\/oopStorage.cpp","additions":84,"deletions":13,"binary":false,"changes":97,"status":"modified"},{"patch":"@@ -110,1 +110,1 @@\n-  \/\/ postcondition: *result == NULL.\n+  \/\/ postcondition: result == NULL or *result == NULL.\n@@ -113,0 +113,16 @@\n+  \/\/ Maximum number of entries that can be obtained by one call to\n+  \/\/ allocate(oop**, size_t).\n+  static const size_t bulk_allocate_limit = BitsPerWord;\n+\n+  \/\/ Allocates multiple entries, returning them in the ptrs buffer.  The\n+  \/\/ number of entries that will be allocated is never more than the minimum\n+  \/\/ of size and bulk_allocate_limit, but may be less than either.  Possibly\n+  \/\/ faster than making repeated calls to allocate().  Always make maximal\n+  \/\/ requests for best efficiency.  Returns the number of entries allocated.\n+  \/\/ A result of zero indicates failure to allocate any entries.\n+  \/\/ Locks _allocation_mutex.\n+  \/\/ precondition: size > 0.\n+  \/\/ postcondition: ptrs[i] is an allocated entry for i in [0, result).\n+  \/\/ postcondition: *ptrs[i] == NULL for i in [0, result).\n+  size_t allocate(oop** ptrs, size_t size);\n+\n@@ -271,0 +287,1 @@\n+  void  log_block_transition(Block* block, const char* new_state) const;\n","filename":"src\/hotspot\/share\/gc\/shared\/oopStorage.hpp","additions":18,"deletions":1,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -151,0 +151,1 @@\n+  void atomic_add_allocated(uintx add);\n@@ -189,0 +190,1 @@\n+  uintx allocate_all();\n","filename":"src\/hotspot\/share\/gc\/shared\/oopStorage.inline.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -499,0 +499,27 @@\n+TEST_VM_F(OopStorageTest, bulk_allocation) {\n+  static const size_t max_entries = 1000;\n+  static const size_t zero = 0;\n+  oop* entries[max_entries] = {};\n+\n+  AllocationList& allocation_list = TestAccess::allocation_list(_storage);\n+\n+  EXPECT_EQ(0u, empty_block_count(_storage));\n+  size_t allocated = _storage.allocate(entries, max_entries);\n+  ASSERT_NE(allocated, zero);\n+  \/\/ ASSERT_LE would ODR-use the OopStorage constant.\n+  size_t bulk_allocate_limit = OopStorage::bulk_allocate_limit;\n+  ASSERT_LE(allocated, bulk_allocate_limit);\n+  ASSERT_LE(allocated, max_entries);\n+  for (size_t i = 0; i < allocated; ++i) {\n+    EXPECT_EQ(OopStorage::ALLOCATED_ENTRY, _storage.allocation_status(entries[i]));\n+  }\n+  for (size_t i = allocated; i < max_entries; ++i) {\n+    EXPECT_EQ(NULL, entries[i]);\n+  }\n+  _storage.release(entries, allocated);\n+  EXPECT_EQ(0u, _storage.allocation_count());\n+  for (size_t i = 0; i < allocated; ++i) {\n+    EXPECT_EQ(OopStorage::UNALLOCATED_ENTRY, _storage.allocation_status(entries[i]));\n+  }\n+}\n+\n","filename":"test\/hotspot\/gtest\/gc\/shared\/test_oopStorage.cpp","additions":27,"deletions":0,"binary":false,"changes":27,"status":"modified"}]}