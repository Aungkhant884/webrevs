{"files":[{"patch":"@@ -650,0 +650,2 @@\n+public:\n+\n@@ -653,1 +655,1 @@\n-  void emit_long(jint x) {\n+  void emit_int32(jint x) {\n@@ -659,1 +661,1 @@\n-  void emit_long(jint x) {\n+  void emit_int32(jint x) {\n@@ -664,2 +666,0 @@\n-public:\n-\n","filename":"src\/hotspot\/cpu\/aarch64\/assembler_aarch64.hpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -1,1418 +0,0 @@\n-\/*\n- * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2014, 2021, Red Hat Inc. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#ifndef CPU_AARCH64_MACROASSEMBLER_AARCH64_HPP\n-#define CPU_AARCH64_MACROASSEMBLER_AARCH64_HPP\n-\n-#include \"asm\/assembler.inline.hpp\"\n-#include \"oops\/compressedOops.hpp\"\n-#include \"runtime\/vm_version.hpp\"\n-#include \"utilities\/powerOfTwo.hpp\"\n-\n-\/\/ MacroAssembler extends Assembler by frequently used macros.\n-\/\/\n-\/\/ Instructions for which a 'better' code sequence exists depending\n-\/\/ on arguments should also go in here.\n-\n-class MacroAssembler: public Assembler {\n-  friend class LIR_Assembler;\n-\n- public:\n-  using Assembler::mov;\n-  using Assembler::movi;\n-\n- protected:\n-\n-  \/\/ Support for VM calls\n-  \/\/\n-  \/\/ This is the base routine called by the different versions of call_VM_leaf. The interpreter\n-  \/\/ may customize this version by overriding it for its purposes (e.g., to save\/restore\n-  \/\/ additional registers when doing a VM call).\n-  virtual void call_VM_leaf_base(\n-    address entry_point,               \/\/ the entry point\n-    int     number_of_arguments,        \/\/ the number of arguments to pop after the call\n-    Label *retaddr = NULL\n-  );\n-\n-  virtual void call_VM_leaf_base(\n-    address entry_point,               \/\/ the entry point\n-    int     number_of_arguments,        \/\/ the number of arguments to pop after the call\n-    Label &retaddr) {\n-    call_VM_leaf_base(entry_point, number_of_arguments, &retaddr);\n-  }\n-\n-  \/\/ This is the base routine called by the different versions of call_VM. The interpreter\n-  \/\/ may customize this version by overriding it for its purposes (e.g., to save\/restore\n-  \/\/ additional registers when doing a VM call).\n-  \/\/\n-  \/\/ If no java_thread register is specified (noreg) than rthread will be used instead. call_VM_base\n-  \/\/ returns the register which contains the thread upon return. If a thread register has been\n-  \/\/ specified, the return value will correspond to that register. If no last_java_sp is specified\n-  \/\/ (noreg) than rsp will be used instead.\n-  virtual void call_VM_base(           \/\/ returns the register containing the thread upon return\n-    Register oop_result,               \/\/ where an oop-result ends up if any; use noreg otherwise\n-    Register java_thread,              \/\/ the thread if computed before     ; use noreg otherwise\n-    Register last_java_sp,             \/\/ to set up last_Java_frame in stubs; use noreg otherwise\n-    address  entry_point,              \/\/ the entry point\n-    int      number_of_arguments,      \/\/ the number of arguments (w\/o thread) to pop after the call\n-    bool     check_exceptions          \/\/ whether to check for pending exceptions after return\n-  );\n-\n-  void call_VM_helper(Register oop_result, address entry_point, int number_of_arguments, bool check_exceptions = true);\n-\n-  enum KlassDecodeMode {\n-    KlassDecodeNone,\n-    KlassDecodeZero,\n-    KlassDecodeXor,\n-    KlassDecodeMovk\n-  };\n-\n-  KlassDecodeMode klass_decode_mode();\n-\n- private:\n-  static KlassDecodeMode _klass_decode_mode;\n-\n- public:\n-  MacroAssembler(CodeBuffer* code) : Assembler(code) {}\n-\n- \/\/ These routines should emit JVMTI PopFrame and ForceEarlyReturn handling code.\n- \/\/ The implementation is only non-empty for the InterpreterMacroAssembler,\n- \/\/ as only the interpreter handles PopFrame and ForceEarlyReturn requests.\n- virtual void check_and_handle_popframe(Register java_thread);\n- virtual void check_and_handle_earlyret(Register java_thread);\n-\n-  void safepoint_poll(Label& slow_path, bool at_return, bool acquire, bool in_nmethod);\n-\n-  \/\/ Helper functions for statistics gathering.\n-  \/\/ Unconditional atomic increment.\n-  void atomic_incw(Register counter_addr, Register tmp, Register tmp2);\n-  void atomic_incw(Address counter_addr, Register tmp1, Register tmp2, Register tmp3) {\n-    lea(tmp1, counter_addr);\n-    atomic_incw(tmp1, tmp2, tmp3);\n-  }\n-  \/\/ Load Effective Address\n-  void lea(Register r, const Address &a) {\n-    InstructionMark im(this);\n-    code_section()->relocate(inst_mark(), a.rspec());\n-    a.lea(this, r);\n-  }\n-\n-  \/* Sometimes we get misaligned loads and stores, usually from Unsafe\n-     accesses, and these can exceed the offset range. *\/\n-  Address legitimize_address(const Address &a, int size, Register scratch) {\n-    if (a.getMode() == Address::base_plus_offset) {\n-      if (! Address::offset_ok_for_immed(a.offset(), exact_log2(size))) {\n-        block_comment(\"legitimize_address {\");\n-        lea(scratch, a);\n-        block_comment(\"} legitimize_address\");\n-        return Address(scratch);\n-      }\n-    }\n-    return a;\n-  }\n-\n-  void addmw(Address a, Register incr, Register scratch) {\n-    ldrw(scratch, a);\n-    addw(scratch, scratch, incr);\n-    strw(scratch, a);\n-  }\n-\n-  \/\/ Add constant to memory word\n-  void addmw(Address a, int imm, Register scratch) {\n-    ldrw(scratch, a);\n-    if (imm > 0)\n-      addw(scratch, scratch, (unsigned)imm);\n-    else\n-      subw(scratch, scratch, (unsigned)-imm);\n-    strw(scratch, a);\n-  }\n-\n-  void bind(Label& L) {\n-    Assembler::bind(L);\n-    code()->clear_last_insn();\n-  }\n-\n-  void membar(Membar_mask_bits order_constraint);\n-\n-  using Assembler::ldr;\n-  using Assembler::str;\n-  using Assembler::ldrw;\n-  using Assembler::strw;\n-\n-  void ldr(Register Rx, const Address &adr);\n-  void ldrw(Register Rw, const Address &adr);\n-  void str(Register Rx, const Address &adr);\n-  void strw(Register Rx, const Address &adr);\n-\n-  \/\/ Frame creation and destruction shared between JITs.\n-  void build_frame(int framesize);\n-  void remove_frame(int framesize);\n-\n-  virtual void _call_Unimplemented(address call_site) {\n-    mov(rscratch2, call_site);\n-  }\n-\n-\/\/ Microsoft's MSVC team thinks that the __FUNCSIG__ is approximately (sympathy for calling conventions) equivalent to __PRETTY_FUNCTION__\n-\/\/ Also, from Clang patch: \"It is very similar to GCC's PRETTY_FUNCTION, except it prints the calling convention.\"\n-\/\/ https:\/\/reviews.llvm.org\/D3311\n-\n-#ifdef _WIN64\n-#define call_Unimplemented() _call_Unimplemented((address)__FUNCSIG__)\n-#else\n-#define call_Unimplemented() _call_Unimplemented((address)__PRETTY_FUNCTION__)\n-#endif\n-\n-  \/\/ aliases defined in AARCH64 spec\n-\n-  template<class T>\n-  inline void cmpw(Register Rd, T imm)  { subsw(zr, Rd, imm); }\n-\n-  inline void cmp(Register Rd, unsigned char imm8)  { subs(zr, Rd, imm8); }\n-  inline void cmp(Register Rd, unsigned imm) = delete;\n-\n-  inline void cmnw(Register Rd, unsigned imm) { addsw(zr, Rd, imm); }\n-  inline void cmn(Register Rd, unsigned imm) { adds(zr, Rd, imm); }\n-\n-  void cset(Register Rd, Assembler::Condition cond) {\n-    csinc(Rd, zr, zr, ~cond);\n-  }\n-  void csetw(Register Rd, Assembler::Condition cond) {\n-    csincw(Rd, zr, zr, ~cond);\n-  }\n-\n-  void cneg(Register Rd, Register Rn, Assembler::Condition cond) {\n-    csneg(Rd, Rn, Rn, ~cond);\n-  }\n-  void cnegw(Register Rd, Register Rn, Assembler::Condition cond) {\n-    csnegw(Rd, Rn, Rn, ~cond);\n-  }\n-\n-  inline void movw(Register Rd, Register Rn) {\n-    if (Rd == sp || Rn == sp) {\n-      addw(Rd, Rn, 0U);\n-    } else {\n-      orrw(Rd, zr, Rn);\n-    }\n-  }\n-  inline void mov(Register Rd, Register Rn) {\n-    assert(Rd != r31_sp && Rn != r31_sp, \"should be\");\n-    if (Rd == Rn) {\n-    } else if (Rd == sp || Rn == sp) {\n-      add(Rd, Rn, 0U);\n-    } else {\n-      orr(Rd, zr, Rn);\n-    }\n-  }\n-\n-  inline void moviw(Register Rd, unsigned imm) { orrw(Rd, zr, imm); }\n-  inline void movi(Register Rd, unsigned imm) { orr(Rd, zr, imm); }\n-\n-  inline void tstw(Register Rd, Register Rn) { andsw(zr, Rd, Rn); }\n-  inline void tst(Register Rd, Register Rn) { ands(zr, Rd, Rn); }\n-\n-  inline void tstw(Register Rd, uint64_t imm) { andsw(zr, Rd, imm); }\n-  inline void tst(Register Rd, uint64_t imm) { ands(zr, Rd, imm); }\n-\n-  inline void bfiw(Register Rd, Register Rn, unsigned lsb, unsigned width) {\n-    bfmw(Rd, Rn, ((32 - lsb) & 31), (width - 1));\n-  }\n-  inline void bfi(Register Rd, Register Rn, unsigned lsb, unsigned width) {\n-    bfm(Rd, Rn, ((64 - lsb) & 63), (width - 1));\n-  }\n-\n-  inline void bfxilw(Register Rd, Register Rn, unsigned lsb, unsigned width) {\n-    bfmw(Rd, Rn, lsb, (lsb + width - 1));\n-  }\n-  inline void bfxil(Register Rd, Register Rn, unsigned lsb, unsigned width) {\n-    bfm(Rd, Rn, lsb , (lsb + width - 1));\n-  }\n-\n-  inline void sbfizw(Register Rd, Register Rn, unsigned lsb, unsigned width) {\n-    sbfmw(Rd, Rn, ((32 - lsb) & 31), (width - 1));\n-  }\n-  inline void sbfiz(Register Rd, Register Rn, unsigned lsb, unsigned width) {\n-    sbfm(Rd, Rn, ((64 - lsb) & 63), (width - 1));\n-  }\n-\n-  inline void sbfxw(Register Rd, Register Rn, unsigned lsb, unsigned width) {\n-    sbfmw(Rd, Rn, lsb, (lsb + width - 1));\n-  }\n-  inline void sbfx(Register Rd, Register Rn, unsigned lsb, unsigned width) {\n-    sbfm(Rd, Rn, lsb , (lsb + width - 1));\n-  }\n-\n-  inline void ubfizw(Register Rd, Register Rn, unsigned lsb, unsigned width) {\n-    ubfmw(Rd, Rn, ((32 - lsb) & 31), (width - 1));\n-  }\n-  inline void ubfiz(Register Rd, Register Rn, unsigned lsb, unsigned width) {\n-    ubfm(Rd, Rn, ((64 - lsb) & 63), (width - 1));\n-  }\n-\n-  inline void ubfxw(Register Rd, Register Rn, unsigned lsb, unsigned width) {\n-    ubfmw(Rd, Rn, lsb, (lsb + width - 1));\n-  }\n-  inline void ubfx(Register Rd, Register Rn, unsigned lsb, unsigned width) {\n-    ubfm(Rd, Rn, lsb , (lsb + width - 1));\n-  }\n-\n-  inline void asrw(Register Rd, Register Rn, unsigned imm) {\n-    sbfmw(Rd, Rn, imm, 31);\n-  }\n-\n-  inline void asr(Register Rd, Register Rn, unsigned imm) {\n-    sbfm(Rd, Rn, imm, 63);\n-  }\n-\n-  inline void lslw(Register Rd, Register Rn, unsigned imm) {\n-    ubfmw(Rd, Rn, ((32 - imm) & 31), (31 - imm));\n-  }\n-\n-  inline void lsl(Register Rd, Register Rn, unsigned imm) {\n-    ubfm(Rd, Rn, ((64 - imm) & 63), (63 - imm));\n-  }\n-\n-  inline void lsrw(Register Rd, Register Rn, unsigned imm) {\n-    ubfmw(Rd, Rn, imm, 31);\n-  }\n-\n-  inline void lsr(Register Rd, Register Rn, unsigned imm) {\n-    ubfm(Rd, Rn, imm, 63);\n-  }\n-\n-  inline void rorw(Register Rd, Register Rn, unsigned imm) {\n-    extrw(Rd, Rn, Rn, imm);\n-  }\n-\n-  inline void ror(Register Rd, Register Rn, unsigned imm) {\n-    extr(Rd, Rn, Rn, imm);\n-  }\n-\n-  inline void sxtbw(Register Rd, Register Rn) {\n-    sbfmw(Rd, Rn, 0, 7);\n-  }\n-  inline void sxthw(Register Rd, Register Rn) {\n-    sbfmw(Rd, Rn, 0, 15);\n-  }\n-  inline void sxtb(Register Rd, Register Rn) {\n-    sbfm(Rd, Rn, 0, 7);\n-  }\n-  inline void sxth(Register Rd, Register Rn) {\n-    sbfm(Rd, Rn, 0, 15);\n-  }\n-  inline void sxtw(Register Rd, Register Rn) {\n-    sbfm(Rd, Rn, 0, 31);\n-  }\n-\n-  inline void uxtbw(Register Rd, Register Rn) {\n-    ubfmw(Rd, Rn, 0, 7);\n-  }\n-  inline void uxthw(Register Rd, Register Rn) {\n-    ubfmw(Rd, Rn, 0, 15);\n-  }\n-  inline void uxtb(Register Rd, Register Rn) {\n-    ubfm(Rd, Rn, 0, 7);\n-  }\n-  inline void uxth(Register Rd, Register Rn) {\n-    ubfm(Rd, Rn, 0, 15);\n-  }\n-  inline void uxtw(Register Rd, Register Rn) {\n-    ubfm(Rd, Rn, 0, 31);\n-  }\n-\n-  inline void cmnw(Register Rn, Register Rm) {\n-    addsw(zr, Rn, Rm);\n-  }\n-  inline void cmn(Register Rn, Register Rm) {\n-    adds(zr, Rn, Rm);\n-  }\n-\n-  inline void cmpw(Register Rn, Register Rm) {\n-    subsw(zr, Rn, Rm);\n-  }\n-  inline void cmp(Register Rn, Register Rm) {\n-    subs(zr, Rn, Rm);\n-  }\n-\n-  inline void negw(Register Rd, Register Rn) {\n-    subw(Rd, zr, Rn);\n-  }\n-\n-  inline void neg(Register Rd, Register Rn) {\n-    sub(Rd, zr, Rn);\n-  }\n-\n-  inline void negsw(Register Rd, Register Rn) {\n-    subsw(Rd, zr, Rn);\n-  }\n-\n-  inline void negs(Register Rd, Register Rn) {\n-    subs(Rd, zr, Rn);\n-  }\n-\n-  inline void cmnw(Register Rn, Register Rm, enum shift_kind kind, unsigned shift = 0) {\n-    addsw(zr, Rn, Rm, kind, shift);\n-  }\n-  inline void cmn(Register Rn, Register Rm, enum shift_kind kind, unsigned shift = 0) {\n-    adds(zr, Rn, Rm, kind, shift);\n-  }\n-\n-  inline void cmpw(Register Rn, Register Rm, enum shift_kind kind, unsigned shift = 0) {\n-    subsw(zr, Rn, Rm, kind, shift);\n-  }\n-  inline void cmp(Register Rn, Register Rm, enum shift_kind kind, unsigned shift = 0) {\n-    subs(zr, Rn, Rm, kind, shift);\n-  }\n-\n-  inline void negw(Register Rd, Register Rn, enum shift_kind kind, unsigned shift = 0) {\n-    subw(Rd, zr, Rn, kind, shift);\n-  }\n-\n-  inline void neg(Register Rd, Register Rn, enum shift_kind kind, unsigned shift = 0) {\n-    sub(Rd, zr, Rn, kind, shift);\n-  }\n-\n-  inline void negsw(Register Rd, Register Rn, enum shift_kind kind, unsigned shift = 0) {\n-    subsw(Rd, zr, Rn, kind, shift);\n-  }\n-\n-  inline void negs(Register Rd, Register Rn, enum shift_kind kind, unsigned shift = 0) {\n-    subs(Rd, zr, Rn, kind, shift);\n-  }\n-\n-  inline void mnegw(Register Rd, Register Rn, Register Rm) {\n-    msubw(Rd, Rn, Rm, zr);\n-  }\n-  inline void mneg(Register Rd, Register Rn, Register Rm) {\n-    msub(Rd, Rn, Rm, zr);\n-  }\n-\n-  inline void mulw(Register Rd, Register Rn, Register Rm) {\n-    maddw(Rd, Rn, Rm, zr);\n-  }\n-  inline void mul(Register Rd, Register Rn, Register Rm) {\n-    madd(Rd, Rn, Rm, zr);\n-  }\n-\n-  inline void smnegl(Register Rd, Register Rn, Register Rm) {\n-    smsubl(Rd, Rn, Rm, zr);\n-  }\n-  inline void smull(Register Rd, Register Rn, Register Rm) {\n-    smaddl(Rd, Rn, Rm, zr);\n-  }\n-\n-  inline void umnegl(Register Rd, Register Rn, Register Rm) {\n-    umsubl(Rd, Rn, Rm, zr);\n-  }\n-  inline void umull(Register Rd, Register Rn, Register Rm) {\n-    umaddl(Rd, Rn, Rm, zr);\n-  }\n-\n-#define WRAP(INSN)                                                            \\\n-  void INSN(Register Rd, Register Rn, Register Rm, Register Ra) {             \\\n-    if ((VM_Version::features() & VM_Version::CPU_A53MAC) && Ra != zr)        \\\n-      nop();                                                                  \\\n-    Assembler::INSN(Rd, Rn, Rm, Ra);                                          \\\n-  }\n-\n-  WRAP(madd) WRAP(msub) WRAP(maddw) WRAP(msubw)\n-  WRAP(smaddl) WRAP(smsubl) WRAP(umaddl) WRAP(umsubl)\n-#undef WRAP\n-\n-\n-  \/\/ macro assembly operations needed for aarch64\n-\n-  \/\/ first two private routines for loading 32 bit or 64 bit constants\n-private:\n-\n-  void mov_immediate64(Register dst, uint64_t imm64);\n-  void mov_immediate32(Register dst, uint32_t imm32);\n-\n-  int push(unsigned int bitset, Register stack);\n-  int pop(unsigned int bitset, Register stack);\n-\n-  int push_fp(unsigned int bitset, Register stack);\n-  int pop_fp(unsigned int bitset, Register stack);\n-\n-  void mov(Register dst, Address a);\n-\n-public:\n-  void push(RegSet regs, Register stack) { if (regs.bits()) push(regs.bits(), stack); }\n-  void pop(RegSet regs, Register stack) { if (regs.bits()) pop(regs.bits(), stack); }\n-\n-  void push_fp(FloatRegSet regs, Register stack) { if (regs.bits()) push_fp(regs.bits(), stack); }\n-  void pop_fp(FloatRegSet regs, Register stack) { if (regs.bits()) pop_fp(regs.bits(), stack); }\n-\n-  static RegSet call_clobbered_registers();\n-\n-  \/\/ Push and pop everything that might be clobbered by a native\n-  \/\/ runtime call except rscratch1 and rscratch2.  (They are always\n-  \/\/ scratch, so we don't have to protect them.)  Only save the lower\n-  \/\/ 64 bits of each vector register. Additonal registers can be excluded\n-  \/\/ in a passed RegSet.\n-  void push_call_clobbered_registers_except(RegSet exclude);\n-  void pop_call_clobbered_registers_except(RegSet exclude);\n-\n-  void push_call_clobbered_registers() {\n-    push_call_clobbered_registers_except(RegSet());\n-  }\n-  void pop_call_clobbered_registers() {\n-    pop_call_clobbered_registers_except(RegSet());\n-  }\n-\n-\n-  \/\/ now mov instructions for loading absolute addresses and 32 or\n-  \/\/ 64 bit integers\n-\n-  inline void mov(Register dst, address addr)             { mov_immediate64(dst, (uint64_t)addr); }\n-\n-  inline void mov(Register dst, int imm64)                { mov_immediate64(dst, (uint64_t)imm64); }\n-  inline void mov(Register dst, long imm64)               { mov_immediate64(dst, (uint64_t)imm64); }\n-  inline void mov(Register dst, long long imm64)          { mov_immediate64(dst, (uint64_t)imm64); }\n-  inline void mov(Register dst, unsigned int imm64)       { mov_immediate64(dst, (uint64_t)imm64); }\n-  inline void mov(Register dst, unsigned long imm64)      { mov_immediate64(dst, (uint64_t)imm64); }\n-  inline void mov(Register dst, unsigned long long imm64) { mov_immediate64(dst, (uint64_t)imm64); }\n-\n-  inline void movw(Register dst, uint32_t imm32)\n-  {\n-    mov_immediate32(dst, imm32);\n-  }\n-\n-  void mov(Register dst, RegisterOrConstant src) {\n-    if (src.is_register())\n-      mov(dst, src.as_register());\n-    else\n-      mov(dst, src.as_constant());\n-  }\n-\n-  void movptr(Register r, uintptr_t imm64);\n-\n-  void mov(FloatRegister Vd, SIMD_Arrangement T, uint32_t imm32);\n-\n-  void mov(FloatRegister Vd, SIMD_Arrangement T, FloatRegister Vn) {\n-    orr(Vd, T, Vn, Vn);\n-  }\n-\n-\n-public:\n-\n-  \/\/ Generalized Test Bit And Branch, including a \"far\" variety which\n-  \/\/ spans more than 32KiB.\n-  void tbr(Condition cond, Register Rt, int bitpos, Label &dest, bool isfar = false) {\n-    assert(cond == EQ || cond == NE, \"must be\");\n-\n-    if (isfar)\n-      cond = ~cond;\n-\n-    void (Assembler::* branch)(Register Rt, int bitpos, Label &L);\n-    if (cond == Assembler::EQ)\n-      branch = &Assembler::tbz;\n-    else\n-      branch = &Assembler::tbnz;\n-\n-    if (isfar) {\n-      Label L;\n-      (this->*branch)(Rt, bitpos, L);\n-      b(dest);\n-      bind(L);\n-    } else {\n-      (this->*branch)(Rt, bitpos, dest);\n-    }\n-  }\n-\n-  \/\/ macro instructions for accessing and updating floating point\n-  \/\/ status register\n-  \/\/\n-  \/\/ FPSR : op1 == 011\n-  \/\/        CRn == 0100\n-  \/\/        CRm == 0100\n-  \/\/        op2 == 001\n-\n-  inline void get_fpsr(Register reg)\n-  {\n-    mrs(0b11, 0b0100, 0b0100, 0b001, reg);\n-  }\n-\n-  inline void set_fpsr(Register reg)\n-  {\n-    msr(0b011, 0b0100, 0b0100, 0b001, reg);\n-  }\n-\n-  inline void clear_fpsr()\n-  {\n-    msr(0b011, 0b0100, 0b0100, 0b001, zr);\n-  }\n-\n-  \/\/ DCZID_EL0: op1 == 011\n-  \/\/            CRn == 0000\n-  \/\/            CRm == 0000\n-  \/\/            op2 == 111\n-  inline void get_dczid_el0(Register reg)\n-  {\n-    mrs(0b011, 0b0000, 0b0000, 0b111, reg);\n-  }\n-\n-  \/\/ CTR_EL0:   op1 == 011\n-  \/\/            CRn == 0000\n-  \/\/            CRm == 0000\n-  \/\/            op2 == 001\n-  inline void get_ctr_el0(Register reg)\n-  {\n-    mrs(0b011, 0b0000, 0b0000, 0b001, reg);\n-  }\n-\n-  \/\/ idiv variant which deals with MINLONG as dividend and -1 as divisor\n-  int corrected_idivl(Register result, Register ra, Register rb,\n-                      bool want_remainder, Register tmp = rscratch1);\n-  int corrected_idivq(Register result, Register ra, Register rb,\n-                      bool want_remainder, Register tmp = rscratch1);\n-\n-  \/\/ Support for NULL-checks\n-  \/\/\n-  \/\/ Generates code that causes a NULL OS exception if the content of reg is NULL.\n-  \/\/ If the accessed location is M[reg + offset] and the offset is known, provide the\n-  \/\/ offset. No explicit code generation is needed if the offset is within a certain\n-  \/\/ range (0 <= offset <= page_size).\n-\n-  virtual void null_check(Register reg, int offset = -1);\n-  static bool needs_explicit_null_check(intptr_t offset);\n-  static bool uses_implicit_null_check(void* address);\n-\n-  static address target_addr_for_insn(address insn_addr, unsigned insn);\n-  static address target_addr_for_insn(address insn_addr) {\n-    unsigned insn = *(unsigned*)insn_addr;\n-    return target_addr_for_insn(insn_addr, insn);\n-  }\n-\n-  \/\/ Required platform-specific helpers for Label::patch_instructions.\n-  \/\/ They _shadow_ the declarations in AbstractAssembler, which are undefined.\n-  static int pd_patch_instruction_size(address branch, address target);\n-  static void pd_patch_instruction(address branch, address target, const char* file = NULL, int line = 0) {\n-    pd_patch_instruction_size(branch, target);\n-  }\n-  static address pd_call_destination(address branch) {\n-    return target_addr_for_insn(branch);\n-  }\n-#ifndef PRODUCT\n-  static void pd_print_patched_instruction(address branch);\n-#endif\n-\n-  static int patch_oop(address insn_addr, address o);\n-  static int patch_narrow_klass(address insn_addr, narrowKlass n);\n-\n-  address emit_trampoline_stub(int insts_call_instruction_offset, address target);\n-  void emit_static_call_stub();\n-\n-  \/\/ The following 4 methods return the offset of the appropriate move instruction\n-\n-  \/\/ Support for fast byte\/short loading with zero extension (depending on particular CPU)\n-  int load_unsigned_byte(Register dst, Address src);\n-  int load_unsigned_short(Register dst, Address src);\n-\n-  \/\/ Support for fast byte\/short loading with sign extension (depending on particular CPU)\n-  int load_signed_byte(Register dst, Address src);\n-  int load_signed_short(Register dst, Address src);\n-\n-  int load_signed_byte32(Register dst, Address src);\n-  int load_signed_short32(Register dst, Address src);\n-\n-  \/\/ Support for sign-extension (hi:lo = extend_sign(lo))\n-  void extend_sign(Register hi, Register lo);\n-\n-  \/\/ Load and store values by size and signed-ness\n-  void load_sized_value(Register dst, Address src, size_t size_in_bytes, bool is_signed, Register dst2 = noreg);\n-  void store_sized_value(Address dst, Register src, size_t size_in_bytes, Register src2 = noreg);\n-\n-  \/\/ Support for inc\/dec with optimal instruction selection depending on value\n-\n-  \/\/ x86_64 aliases an unqualified register\/address increment and\n-  \/\/ decrement to call incrementq and decrementq but also supports\n-  \/\/ explicitly sized calls to incrementq\/decrementq or\n-  \/\/ incrementl\/decrementl\n-\n-  \/\/ for aarch64 the proper convention would be to use\n-  \/\/ increment\/decrement for 64 bit operatons and\n-  \/\/ incrementw\/decrementw for 32 bit operations. so when porting\n-  \/\/ x86_64 code we can leave calls to increment\/decrement as is,\n-  \/\/ replace incrementq\/decrementq with increment\/decrement and\n-  \/\/ replace incrementl\/decrementl with incrementw\/decrementw.\n-\n-  \/\/ n.b. increment\/decrement calls with an Address destination will\n-  \/\/ need to use a scratch register to load the value to be\n-  \/\/ incremented. increment\/decrement calls which add or subtract a\n-  \/\/ constant value greater than 2^12 will need to use a 2nd scratch\n-  \/\/ register to hold the constant. so, a register increment\/decrement\n-  \/\/ may trash rscratch2 and an address increment\/decrement trash\n-  \/\/ rscratch and rscratch2\n-\n-  void decrementw(Address dst, int value = 1);\n-  void decrementw(Register reg, int value = 1);\n-\n-  void decrement(Register reg, int value = 1);\n-  void decrement(Address dst, int value = 1);\n-\n-  void incrementw(Address dst, int value = 1);\n-  void incrementw(Register reg, int value = 1);\n-\n-  void increment(Register reg, int value = 1);\n-  void increment(Address dst, int value = 1);\n-\n-\n-  \/\/ Alignment\n-  void align(int modulus);\n-\n-  \/\/ Stack frame creation\/removal\n-  void enter()\n-  {\n-    stp(rfp, lr, Address(pre(sp, -2 * wordSize)));\n-    mov(rfp, sp);\n-  }\n-  void leave()\n-  {\n-    mov(sp, rfp);\n-    ldp(rfp, lr, Address(post(sp, 2 * wordSize)));\n-  }\n-\n-  \/\/ Support for getting the JavaThread pointer (i.e.; a reference to thread-local information)\n-  \/\/ The pointer will be loaded into the thread register.\n-  void get_thread(Register thread);\n-\n-\n-  \/\/ Support for VM calls\n-  \/\/\n-  \/\/ It is imperative that all calls into the VM are handled via the call_VM macros.\n-  \/\/ They make sure that the stack linkage is setup correctly. call_VM's correspond\n-  \/\/ to ENTRY\/ENTRY_X entry points while call_VM_leaf's correspond to LEAF entry points.\n-\n-\n-  void call_VM(Register oop_result,\n-               address entry_point,\n-               bool check_exceptions = true);\n-  void call_VM(Register oop_result,\n-               address entry_point,\n-               Register arg_1,\n-               bool check_exceptions = true);\n-  void call_VM(Register oop_result,\n-               address entry_point,\n-               Register arg_1, Register arg_2,\n-               bool check_exceptions = true);\n-  void call_VM(Register oop_result,\n-               address entry_point,\n-               Register arg_1, Register arg_2, Register arg_3,\n-               bool check_exceptions = true);\n-\n-  \/\/ Overloadings with last_Java_sp\n-  void call_VM(Register oop_result,\n-               Register last_java_sp,\n-               address entry_point,\n-               int number_of_arguments = 0,\n-               bool check_exceptions = true);\n-  void call_VM(Register oop_result,\n-               Register last_java_sp,\n-               address entry_point,\n-               Register arg_1, bool\n-               check_exceptions = true);\n-  void call_VM(Register oop_result,\n-               Register last_java_sp,\n-               address entry_point,\n-               Register arg_1, Register arg_2,\n-               bool check_exceptions = true);\n-  void call_VM(Register oop_result,\n-               Register last_java_sp,\n-               address entry_point,\n-               Register arg_1, Register arg_2, Register arg_3,\n-               bool check_exceptions = true);\n-\n-  void get_vm_result  (Register oop_result, Register thread);\n-  void get_vm_result_2(Register metadata_result, Register thread);\n-\n-  \/\/ These always tightly bind to MacroAssembler::call_VM_base\n-  \/\/ bypassing the virtual implementation\n-  void super_call_VM(Register oop_result, Register last_java_sp, address entry_point, int number_of_arguments = 0, bool check_exceptions = true);\n-  void super_call_VM(Register oop_result, Register last_java_sp, address entry_point, Register arg_1, bool check_exceptions = true);\n-  void super_call_VM(Register oop_result, Register last_java_sp, address entry_point, Register arg_1, Register arg_2, bool check_exceptions = true);\n-  void super_call_VM(Register oop_result, Register last_java_sp, address entry_point, Register arg_1, Register arg_2, Register arg_3, bool check_exceptions = true);\n-  void super_call_VM(Register oop_result, Register last_java_sp, address entry_point, Register arg_1, Register arg_2, Register arg_3, Register arg_4, bool check_exceptions = true);\n-\n-  void call_VM_leaf(address entry_point,\n-                    int number_of_arguments = 0);\n-  void call_VM_leaf(address entry_point,\n-                    Register arg_1);\n-  void call_VM_leaf(address entry_point,\n-                    Register arg_1, Register arg_2);\n-  void call_VM_leaf(address entry_point,\n-                    Register arg_1, Register arg_2, Register arg_3);\n-\n-  \/\/ These always tightly bind to MacroAssembler::call_VM_leaf_base\n-  \/\/ bypassing the virtual implementation\n-  void super_call_VM_leaf(address entry_point);\n-  void super_call_VM_leaf(address entry_point, Register arg_1);\n-  void super_call_VM_leaf(address entry_point, Register arg_1, Register arg_2);\n-  void super_call_VM_leaf(address entry_point, Register arg_1, Register arg_2, Register arg_3);\n-  void super_call_VM_leaf(address entry_point, Register arg_1, Register arg_2, Register arg_3, Register arg_4);\n-\n-  \/\/ last Java Frame (fills frame anchor)\n-  void set_last_Java_frame(Register last_java_sp,\n-                           Register last_java_fp,\n-                           address last_java_pc,\n-                           Register scratch);\n-\n-  void set_last_Java_frame(Register last_java_sp,\n-                           Register last_java_fp,\n-                           Label &last_java_pc,\n-                           Register scratch);\n-\n-  void set_last_Java_frame(Register last_java_sp,\n-                           Register last_java_fp,\n-                           Register last_java_pc,\n-                           Register scratch);\n-\n-  void reset_last_Java_frame(Register thread);\n-\n-  \/\/ thread in the default location (rthread)\n-  void reset_last_Java_frame(bool clear_fp);\n-\n-  \/\/ Stores\n-  void store_check(Register obj);                \/\/ store check for obj - register is destroyed afterwards\n-  void store_check(Register obj, Address dst);   \/\/ same as above, dst is exact store location (reg. is destroyed)\n-\n-  void resolve_jobject(Register value, Register thread, Register tmp);\n-\n-  \/\/ C 'boolean' to Java boolean: x == 0 ? 0 : 1\n-  void c2bool(Register x);\n-\n-  void load_method_holder_cld(Register rresult, Register rmethod);\n-  void load_method_holder(Register holder, Register method);\n-\n-  \/\/ oop manipulations\n-  void load_klass(Register dst, Register src);\n-  void store_klass(Register dst, Register src);\n-  void cmp_klass(Register oop, Register trial_klass, Register tmp);\n-\n-  void resolve_weak_handle(Register result, Register tmp);\n-  void resolve_oop_handle(Register result, Register tmp = r5);\n-  void load_mirror(Register dst, Register method, Register tmp = r5);\n-\n-  void access_load_at(BasicType type, DecoratorSet decorators, Register dst, Address src,\n-                      Register tmp1, Register tmp_thread);\n-\n-  void access_store_at(BasicType type, DecoratorSet decorators, Address dst, Register src,\n-                       Register tmp1, Register tmp_thread);\n-\n-  void load_heap_oop(Register dst, Address src, Register tmp1 = noreg,\n-                     Register thread_tmp = noreg, DecoratorSet decorators = 0);\n-\n-  void load_heap_oop_not_null(Register dst, Address src, Register tmp1 = noreg,\n-                              Register thread_tmp = noreg, DecoratorSet decorators = 0);\n-  void store_heap_oop(Address dst, Register src, Register tmp1 = noreg,\n-                      Register tmp_thread = noreg, DecoratorSet decorators = 0);\n-\n-  \/\/ currently unimplemented\n-  \/\/ Used for storing NULL. All other oop constants should be\n-  \/\/ stored using routines that take a jobject.\n-  void store_heap_oop_null(Address dst);\n-\n-  void store_klass_gap(Register dst, Register src);\n-\n-  \/\/ This dummy is to prevent a call to store_heap_oop from\n-  \/\/ converting a zero (like NULL) into a Register by giving\n-  \/\/ the compiler two choices it can't resolve\n-\n-  void store_heap_oop(Address dst, void* dummy);\n-\n-  void encode_heap_oop(Register d, Register s);\n-  void encode_heap_oop(Register r) { encode_heap_oop(r, r); }\n-  void decode_heap_oop(Register d, Register s);\n-  void decode_heap_oop(Register r) { decode_heap_oop(r, r); }\n-  void encode_heap_oop_not_null(Register r);\n-  void decode_heap_oop_not_null(Register r);\n-  void encode_heap_oop_not_null(Register dst, Register src);\n-  void decode_heap_oop_not_null(Register dst, Register src);\n-\n-  void set_narrow_oop(Register dst, jobject obj);\n-\n-  void encode_klass_not_null(Register r);\n-  void decode_klass_not_null(Register r);\n-  void encode_klass_not_null(Register dst, Register src);\n-  void decode_klass_not_null(Register dst, Register src);\n-\n-  void set_narrow_klass(Register dst, Klass* k);\n-\n-  \/\/ if heap base register is used - reinit it with the correct value\n-  void reinit_heapbase();\n-\n-  DEBUG_ONLY(void verify_heapbase(const char* msg);)\n-\n-  void push_CPU_state(bool save_vectors = false, bool use_sve = false,\n-                      int sve_vector_size_in_bytes = 0);\n-  void pop_CPU_state(bool restore_vectors = false, bool use_sve = false,\n-                      int sve_vector_size_in_bytes = 0);\n-\n-  \/\/ Round up to a power of two\n-  void round_to(Register reg, int modulus);\n-\n-  \/\/ allocation\n-  void eden_allocate(\n-    Register obj,                      \/\/ result: pointer to object after successful allocation\n-    Register var_size_in_bytes,        \/\/ object size in bytes if unknown at compile time; invalid otherwise\n-    int      con_size_in_bytes,        \/\/ object size in bytes if   known at compile time\n-    Register t1,                       \/\/ temp register\n-    Label&   slow_case                 \/\/ continuation point if fast allocation fails\n-  );\n-  void tlab_allocate(\n-    Register obj,                      \/\/ result: pointer to object after successful allocation\n-    Register var_size_in_bytes,        \/\/ object size in bytes if unknown at compile time; invalid otherwise\n-    int      con_size_in_bytes,        \/\/ object size in bytes if   known at compile time\n-    Register t1,                       \/\/ temp register\n-    Register t2,                       \/\/ temp register\n-    Label&   slow_case                 \/\/ continuation point if fast allocation fails\n-  );\n-  void verify_tlab();\n-\n-  \/\/ interface method calling\n-  void lookup_interface_method(Register recv_klass,\n-                               Register intf_klass,\n-                               RegisterOrConstant itable_index,\n-                               Register method_result,\n-                               Register scan_temp,\n-                               Label& no_such_interface,\n-                   bool return_method = true);\n-\n-  \/\/ virtual method calling\n-  \/\/ n.b. x86 allows RegisterOrConstant for vtable_index\n-  void lookup_virtual_method(Register recv_klass,\n-                             RegisterOrConstant vtable_index,\n-                             Register method_result);\n-\n-  \/\/ Test sub_klass against super_klass, with fast and slow paths.\n-\n-  \/\/ The fast path produces a tri-state answer: yes \/ no \/ maybe-slow.\n-  \/\/ One of the three labels can be NULL, meaning take the fall-through.\n-  \/\/ If super_check_offset is -1, the value is loaded up from super_klass.\n-  \/\/ No registers are killed, except temp_reg.\n-  void check_klass_subtype_fast_path(Register sub_klass,\n-                                     Register super_klass,\n-                                     Register temp_reg,\n-                                     Label* L_success,\n-                                     Label* L_failure,\n-                                     Label* L_slow_path,\n-                RegisterOrConstant super_check_offset = RegisterOrConstant(-1));\n-\n-  \/\/ The rest of the type check; must be wired to a corresponding fast path.\n-  \/\/ It does not repeat the fast path logic, so don't use it standalone.\n-  \/\/ The temp_reg and temp2_reg can be noreg, if no temps are available.\n-  \/\/ Updates the sub's secondary super cache as necessary.\n-  \/\/ If set_cond_codes, condition codes will be Z on success, NZ on failure.\n-  void check_klass_subtype_slow_path(Register sub_klass,\n-                                     Register super_klass,\n-                                     Register temp_reg,\n-                                     Register temp2_reg,\n-                                     Label* L_success,\n-                                     Label* L_failure,\n-                                     bool set_cond_codes = false);\n-\n-  \/\/ Simplified, combined version, good for typical uses.\n-  \/\/ Falls through on failure.\n-  void check_klass_subtype(Register sub_klass,\n-                           Register super_klass,\n-                           Register temp_reg,\n-                           Label& L_success);\n-\n-  void clinit_barrier(Register klass,\n-                      Register thread,\n-                      Label* L_fast_path = NULL,\n-                      Label* L_slow_path = NULL);\n-\n-  Address argument_address(RegisterOrConstant arg_slot, int extra_slot_offset = 0);\n-\n-  void verify_sve_vector_length();\n-  void reinitialize_ptrue() {\n-    if (UseSVE > 0) {\n-      sve_ptrue(ptrue, B);\n-    }\n-  }\n-  void verify_ptrue();\n-\n-  \/\/ Debugging\n-\n-  \/\/ only if +VerifyOops\n-  void verify_oop(Register reg, const char* s = \"broken oop\");\n-  void verify_oop_addr(Address addr, const char * s = \"broken oop addr\");\n-\n-\/\/ TODO: verify method and klass metadata (compare against vptr?)\n-  void _verify_method_ptr(Register reg, const char * msg, const char * file, int line) {}\n-  void _verify_klass_ptr(Register reg, const char * msg, const char * file, int line){}\n-\n-#define verify_method_ptr(reg) _verify_method_ptr(reg, \"broken method \" #reg, __FILE__, __LINE__)\n-#define verify_klass_ptr(reg) _verify_klass_ptr(reg, \"broken klass \" #reg, __FILE__, __LINE__)\n-\n-  \/\/ only if +VerifyFPU\n-  void verify_FPU(int stack_depth, const char* s = \"illegal FPU state\");\n-\n-  \/\/ prints msg, dumps registers and stops execution\n-  void stop(const char* msg);\n-\n-  static void debug64(char* msg, int64_t pc, int64_t regs[]);\n-\n-  void untested()                                { stop(\"untested\"); }\n-\n-  void unimplemented(const char* what = \"\");\n-\n-  void should_not_reach_here()                   { stop(\"should not reach here\"); }\n-\n-  \/\/ Stack overflow checking\n-  void bang_stack_with_offset(int offset) {\n-    \/\/ stack grows down, caller passes positive offset\n-    assert(offset > 0, \"must bang with negative offset\");\n-    sub(rscratch2, sp, offset);\n-    str(zr, Address(rscratch2));\n-  }\n-\n-  \/\/ Writes to stack successive pages until offset reached to check for\n-  \/\/ stack overflow + shadow pages.  Also, clobbers tmp\n-  void bang_stack_size(Register size, Register tmp);\n-\n-  \/\/ Check for reserved stack access in method being exited (for JIT)\n-  void reserved_stack_check();\n-\n-  \/\/ Arithmetics\n-\n-  void addptr(const Address &dst, int32_t src);\n-  void cmpptr(Register src1, Address src2);\n-\n-  void cmpoop(Register obj1, Register obj2);\n-\n-  \/\/ Various forms of CAS\n-\n-  void cmpxchg_obj_header(Register oldv, Register newv, Register obj, Register tmp,\n-                          Label &suceed, Label *fail);\n-  void cmpxchgptr(Register oldv, Register newv, Register addr, Register tmp,\n-                  Label &suceed, Label *fail);\n-\n-  void cmpxchgw(Register oldv, Register newv, Register addr, Register tmp,\n-                  Label &suceed, Label *fail);\n-\n-  void atomic_add(Register prev, RegisterOrConstant incr, Register addr);\n-  void atomic_addw(Register prev, RegisterOrConstant incr, Register addr);\n-  void atomic_addal(Register prev, RegisterOrConstant incr, Register addr);\n-  void atomic_addalw(Register prev, RegisterOrConstant incr, Register addr);\n-\n-  void atomic_xchg(Register prev, Register newv, Register addr);\n-  void atomic_xchgw(Register prev, Register newv, Register addr);\n-  void atomic_xchgl(Register prev, Register newv, Register addr);\n-  void atomic_xchglw(Register prev, Register newv, Register addr);\n-  void atomic_xchgal(Register prev, Register newv, Register addr);\n-  void atomic_xchgalw(Register prev, Register newv, Register addr);\n-\n-  void orptr(Address adr, RegisterOrConstant src) {\n-    ldr(rscratch1, adr);\n-    if (src.is_register())\n-      orr(rscratch1, rscratch1, src.as_register());\n-    else\n-      orr(rscratch1, rscratch1, src.as_constant());\n-    str(rscratch1, adr);\n-  }\n-\n-  \/\/ A generic CAS; success or failure is in the EQ flag.\n-  \/\/ Clobbers rscratch1\n-  void cmpxchg(Register addr, Register expected, Register new_val,\n-               enum operand_size size,\n-               bool acquire, bool release, bool weak,\n-               Register result);\n-\n-private:\n-  void compare_eq(Register rn, Register rm, enum operand_size size);\n-\n-#ifdef ASSERT\n-  \/\/ Template short-hand support to clean-up after a failed call to trampoline\n-  \/\/ call generation (see trampoline_call() below),  when a set of Labels must\n-  \/\/ be reset (before returning).\n-  template<typename Label, typename... More>\n-  void reset_labels(Label &lbl, More&... more) {\n-    lbl.reset(); reset_labels(more...);\n-  }\n-  template<typename Label>\n-  void reset_labels(Label &lbl) {\n-    lbl.reset();\n-  }\n-#endif\n-\n-public:\n-  \/\/ Calls\n-\n-  address trampoline_call(Address entry, CodeBuffer* cbuf = NULL);\n-\n-  static bool far_branches() {\n-    return ReservedCodeCacheSize > branch_range;\n-  }\n-\n-  \/\/ Jumps that can reach anywhere in the code cache.\n-  \/\/ Trashes tmp.\n-  void far_call(Address entry, CodeBuffer *cbuf = NULL, Register tmp = rscratch1);\n-  void far_jump(Address entry, CodeBuffer *cbuf = NULL, Register tmp = rscratch1);\n-\n-  static int far_branch_size() {\n-    if (far_branches()) {\n-      return 3 * 4;  \/\/ adrp, add, br\n-    } else {\n-      return 4;\n-    }\n-  }\n-\n-  \/\/ Emit the CompiledIC call idiom\n-  address ic_call(address entry, jint method_index = 0);\n-\n-public:\n-\n-  \/\/ Data\n-\n-  void mov_metadata(Register dst, Metadata* obj);\n-  Address allocate_metadata_address(Metadata* obj);\n-  Address constant_oop_address(jobject obj);\n-\n-  void movoop(Register dst, jobject obj, bool immediate = false);\n-\n-  \/\/ CRC32 code for java.util.zip.CRC32::updateBytes() instrinsic.\n-  void kernel_crc32(Register crc, Register buf, Register len,\n-        Register table0, Register table1, Register table2, Register table3,\n-        Register tmp, Register tmp2, Register tmp3);\n-  \/\/ CRC32 code for java.util.zip.CRC32C::updateBytes() instrinsic.\n-  void kernel_crc32c(Register crc, Register buf, Register len,\n-        Register table0, Register table1, Register table2, Register table3,\n-        Register tmp, Register tmp2, Register tmp3);\n-\n-  \/\/ Stack push and pop individual 64 bit registers\n-  void push(Register src);\n-  void pop(Register dst);\n-\n-  \/\/ push all registers onto the stack\n-  void pusha();\n-  void popa();\n-\n-  void repne_scan(Register addr, Register value, Register count,\n-                  Register scratch);\n-  void repne_scanw(Register addr, Register value, Register count,\n-                   Register scratch);\n-\n-  typedef void (MacroAssembler::* add_sub_imm_insn)(Register Rd, Register Rn, unsigned imm);\n-  typedef void (MacroAssembler::* add_sub_reg_insn)(Register Rd, Register Rn, Register Rm, enum shift_kind kind, unsigned shift);\n-\n-  \/\/ If a constant does not fit in an immediate field, generate some\n-  \/\/ number of MOV instructions and then perform the operation\n-  void wrap_add_sub_imm_insn(Register Rd, Register Rn, unsigned imm,\n-                             add_sub_imm_insn insn1,\n-                             add_sub_reg_insn insn2);\n-  \/\/ Seperate vsn which sets the flags\n-  void wrap_adds_subs_imm_insn(Register Rd, Register Rn, unsigned imm,\n-                             add_sub_imm_insn insn1,\n-                             add_sub_reg_insn insn2);\n-\n-#define WRAP(INSN)                                                      \\\n-  void INSN(Register Rd, Register Rn, unsigned imm) {                   \\\n-    wrap_add_sub_imm_insn(Rd, Rn, imm, &Assembler::INSN, &Assembler::INSN); \\\n-  }                                                                     \\\n-                                                                        \\\n-  void INSN(Register Rd, Register Rn, Register Rm,                      \\\n-             enum shift_kind kind, unsigned shift = 0) {                \\\n-    Assembler::INSN(Rd, Rn, Rm, kind, shift);                           \\\n-  }                                                                     \\\n-                                                                        \\\n-  void INSN(Register Rd, Register Rn, Register Rm) {                    \\\n-    Assembler::INSN(Rd, Rn, Rm);                                        \\\n-  }                                                                     \\\n-                                                                        \\\n-  void INSN(Register Rd, Register Rn, Register Rm,                      \\\n-           ext::operation option, int amount = 0) {                     \\\n-    Assembler::INSN(Rd, Rn, Rm, option, amount);                        \\\n-  }\n-\n-  WRAP(add) WRAP(addw) WRAP(sub) WRAP(subw)\n-\n-#undef WRAP\n-#define WRAP(INSN)                                                      \\\n-  void INSN(Register Rd, Register Rn, unsigned imm) {                   \\\n-    wrap_adds_subs_imm_insn(Rd, Rn, imm, &Assembler::INSN, &Assembler::INSN); \\\n-  }                                                                     \\\n-                                                                        \\\n-  void INSN(Register Rd, Register Rn, Register Rm,                      \\\n-             enum shift_kind kind, unsigned shift = 0) {                \\\n-    Assembler::INSN(Rd, Rn, Rm, kind, shift);                           \\\n-  }                                                                     \\\n-                                                                        \\\n-  void INSN(Register Rd, Register Rn, Register Rm) {                    \\\n-    Assembler::INSN(Rd, Rn, Rm);                                        \\\n-  }                                                                     \\\n-                                                                        \\\n-  void INSN(Register Rd, Register Rn, Register Rm,                      \\\n-           ext::operation option, int amount = 0) {                     \\\n-    Assembler::INSN(Rd, Rn, Rm, option, amount);                        \\\n-  }\n-\n-  WRAP(adds) WRAP(addsw) WRAP(subs) WRAP(subsw)\n-\n-  void add(Register Rd, Register Rn, RegisterOrConstant increment);\n-  void addw(Register Rd, Register Rn, RegisterOrConstant increment);\n-  void sub(Register Rd, Register Rn, RegisterOrConstant decrement);\n-  void subw(Register Rd, Register Rn, RegisterOrConstant decrement);\n-\n-  void adrp(Register reg1, const Address &dest, uint64_t &byte_offset);\n-\n-  void tableswitch(Register index, jint lowbound, jint highbound,\n-                   Label &jumptable, Label &jumptable_end, int stride = 1) {\n-    adr(rscratch1, jumptable);\n-    subsw(rscratch2, index, lowbound);\n-    subsw(zr, rscratch2, highbound - lowbound);\n-    br(Assembler::HS, jumptable_end);\n-    add(rscratch1, rscratch1, rscratch2,\n-        ext::sxtw, exact_log2(stride * Assembler::instruction_size));\n-    br(rscratch1);\n-  }\n-\n-  \/\/ Form an address from base + offset in Rd.  Rd may or may not\n-  \/\/ actually be used: you must use the Address that is returned.  It\n-  \/\/ is up to you to ensure that the shift provided matches the size\n-  \/\/ of your data.\n-  Address form_address(Register Rd, Register base, int64_t byte_offset, int shift);\n-\n-  \/\/ Return true iff an address is within the 48-bit AArch64 address\n-  \/\/ space.\n-  bool is_valid_AArch64_address(address a) {\n-    return ((uint64_t)a >> 48) == 0;\n-  }\n-\n-  \/\/ Load the base of the cardtable byte map into reg.\n-  void load_byte_map_base(Register reg);\n-\n-  \/\/ Prolog generator routines to support switch between x86 code and\n-  \/\/ generated ARM code\n-\n-  \/\/ routine to generate an x86 prolog for a stub function which\n-  \/\/ bootstraps into the generated ARM code which directly follows the\n-  \/\/ stub\n-  \/\/\n-\n-  public:\n-\n-  void ldr_constant(Register dest, const Address &const_addr) {\n-    if (NearCpool) {\n-      ldr(dest, const_addr);\n-    } else {\n-      uint64_t offset;\n-      adrp(dest, InternalAddress(const_addr.target()), offset);\n-      ldr(dest, Address(dest, offset));\n-    }\n-  }\n-\n-  address read_polling_page(Register r, relocInfo::relocType rtype);\n-  void get_polling_page(Register dest, relocInfo::relocType rtype);\n-\n-  \/\/ CRC32 code for java.util.zip.CRC32::updateBytes() instrinsic.\n-  void update_byte_crc32(Register crc, Register val, Register table);\n-  void update_word_crc32(Register crc, Register v, Register tmp,\n-        Register table0, Register table1, Register table2, Register table3,\n-        bool upper = false);\n-\n-  address has_negatives(Register ary1, Register len, Register result);\n-\n-  address arrays_equals(Register a1, Register a2, Register result, Register cnt1,\n-                        Register tmp1, Register tmp2, Register tmp3, int elem_size);\n-\n-  void string_equals(Register a1, Register a2, Register result, Register cnt1,\n-                     int elem_size);\n-\n-  void fill_words(Register base, Register cnt, Register value);\n-  void zero_words(Register base, uint64_t cnt);\n-  address zero_words(Register ptr, Register cnt);\n-  void zero_dcache_blocks(Register base, Register cnt);\n-\n-  static const int zero_words_block_size;\n-\n-  address byte_array_inflate(Register src, Register dst, Register len,\n-                             FloatRegister vtmp1, FloatRegister vtmp2,\n-                             FloatRegister vtmp3, Register tmp4);\n-\n-  void char_array_compress(Register src, Register dst, Register len,\n-                           FloatRegister tmp1Reg, FloatRegister tmp2Reg,\n-                           FloatRegister tmp3Reg, FloatRegister tmp4Reg,\n-                           Register result);\n-\n-  void encode_iso_array(Register src, Register dst,\n-                        Register len, Register result,\n-                        FloatRegister Vtmp1, FloatRegister Vtmp2,\n-                        FloatRegister Vtmp3, FloatRegister Vtmp4);\n-  void fast_log(FloatRegister vtmp0, FloatRegister vtmp1, FloatRegister vtmp2,\n-                FloatRegister vtmp3, FloatRegister vtmp4, FloatRegister vtmp5,\n-                FloatRegister tmpC1, FloatRegister tmpC2, FloatRegister tmpC3,\n-                FloatRegister tmpC4, Register tmp1, Register tmp2,\n-                Register tmp3, Register tmp4, Register tmp5);\n-  void generate_dsin_dcos(bool isCos, address npio2_hw, address two_over_pi,\n-      address pio2, address dsin_coef, address dcos_coef);\n- private:\n-  \/\/ begin trigonometric functions support block\n-  void generate__ieee754_rem_pio2(address npio2_hw, address two_over_pi, address pio2);\n-  void generate__kernel_rem_pio2(address two_over_pi, address pio2);\n-  void generate_kernel_sin(FloatRegister x, bool iyIsOne, address dsin_coef);\n-  void generate_kernel_cos(FloatRegister x, address dcos_coef);\n-  \/\/ end trigonometric functions support block\n-  void add2_with_carry(Register final_dest_hi, Register dest_hi, Register dest_lo,\n-                       Register src1, Register src2);\n-  void add2_with_carry(Register dest_hi, Register dest_lo, Register src1, Register src2) {\n-    add2_with_carry(dest_hi, dest_hi, dest_lo, src1, src2);\n-  }\n-  void multiply_64_x_64_loop(Register x, Register xstart, Register x_xstart,\n-                             Register y, Register y_idx, Register z,\n-                             Register carry, Register product,\n-                             Register idx, Register kdx);\n-  void multiply_128_x_128_loop(Register y, Register z,\n-                               Register carry, Register carry2,\n-                               Register idx, Register jdx,\n-                               Register yz_idx1, Register yz_idx2,\n-                               Register tmp, Register tmp3, Register tmp4,\n-                               Register tmp7, Register product_hi);\n-  void kernel_crc32_using_crc32(Register crc, Register buf,\n-        Register len, Register tmp0, Register tmp1, Register tmp2,\n-        Register tmp3);\n-  void kernel_crc32c_using_crc32c(Register crc, Register buf,\n-        Register len, Register tmp0, Register tmp1, Register tmp2,\n-        Register tmp3);\n-public:\n-  void multiply_to_len(Register x, Register xlen, Register y, Register ylen, Register z,\n-                       Register zlen, Register tmp1, Register tmp2, Register tmp3,\n-                       Register tmp4, Register tmp5, Register tmp6, Register tmp7);\n-  void mul_add(Register out, Register in, Register offs, Register len, Register k);\n-\n-  \/\/ Place an ISB after code may have been modified due to a safepoint.\n-  void safepoint_isb();\n-\n-private:\n-  \/\/ Return the effective address r + (r1 << ext) + offset.\n-  \/\/ Uses rscratch2.\n-  Address offsetted_address(Register r, Register r1, Address::extend ext,\n-                            int offset, int size);\n-\n-private:\n-  \/\/ Returns an address on the stack which is reachable with a ldr\/str of size\n-  \/\/ Uses rscratch2 if the address is not directly reachable\n-  Address spill_address(int size, int offset, Register tmp=rscratch2);\n-  Address sve_spill_address(int sve_reg_size_in_bytes, int offset, Register tmp=rscratch2);\n-\n-  bool merge_alignment_check(Register base, size_t size, int64_t cur_offset, int64_t prev_offset) const;\n-\n-  \/\/ Check whether two loads\/stores can be merged into ldp\/stp.\n-  bool ldst_can_merge(Register rx, const Address &adr, size_t cur_size_in_bytes, bool is_store) const;\n-\n-  \/\/ Merge current load\/store with previous load\/store into ldp\/stp.\n-  void merge_ldst(Register rx, const Address &adr, size_t cur_size_in_bytes, bool is_store);\n-\n-  \/\/ Try to merge two loads\/stores into ldp\/stp. If success, returns true else false.\n-  bool try_merge_ldst(Register rt, const Address &adr, size_t cur_size_in_bytes, bool is_store);\n-\n-public:\n-  void spill(Register Rx, bool is64, int offset) {\n-    if (is64) {\n-      str(Rx, spill_address(8, offset));\n-    } else {\n-      strw(Rx, spill_address(4, offset));\n-    }\n-  }\n-  void spill(FloatRegister Vx, SIMD_RegVariant T, int offset) {\n-    str(Vx, T, spill_address(1 << (int)T, offset));\n-  }\n-  void spill_sve_vector(FloatRegister Zx, int offset, int vector_reg_size_in_bytes) {\n-    sve_str(Zx, sve_spill_address(vector_reg_size_in_bytes, offset));\n-  }\n-  void unspill(Register Rx, bool is64, int offset) {\n-    if (is64) {\n-      ldr(Rx, spill_address(8, offset));\n-    } else {\n-      ldrw(Rx, spill_address(4, offset));\n-    }\n-  }\n-  void unspill(FloatRegister Vx, SIMD_RegVariant T, int offset) {\n-    ldr(Vx, T, spill_address(1 << (int)T, offset));\n-  }\n-  void unspill_sve_vector(FloatRegister Zx, int offset, int vector_reg_size_in_bytes) {\n-    sve_ldr(Zx, sve_spill_address(vector_reg_size_in_bytes, offset));\n-  }\n-  void spill_copy128(int src_offset, int dst_offset,\n-                     Register tmp1=rscratch1, Register tmp2=rscratch2) {\n-    if (src_offset < 512 && (src_offset & 7) == 0 &&\n-        dst_offset < 512 && (dst_offset & 7) == 0) {\n-      ldp(tmp1, tmp2, Address(sp, src_offset));\n-      stp(tmp1, tmp2, Address(sp, dst_offset));\n-    } else {\n-      unspill(tmp1, true, src_offset);\n-      spill(tmp1, true, dst_offset);\n-      unspill(tmp1, true, src_offset+8);\n-      spill(tmp1, true, dst_offset+8);\n-    }\n-  }\n-  void spill_copy_sve_vector_stack_to_stack(int src_offset, int dst_offset,\n-                                            int sve_vec_reg_size_in_bytes) {\n-    assert(sve_vec_reg_size_in_bytes % 16 == 0, \"unexpected sve vector reg size\");\n-    for (int i = 0; i < sve_vec_reg_size_in_bytes \/ 16; i++) {\n-      spill_copy128(src_offset, dst_offset);\n-      src_offset += 16;\n-      dst_offset += 16;\n-    }\n-  }\n-  void cache_wb(Address line);\n-  void cache_wbsync(bool is_pre);\n-\n-private:\n-  \/\/ Check the current thread doesn't need a cross modify fence.\n-  void verify_cross_modify_fence_not_required() PRODUCT_RETURN;\n-\n-};\n-\n-#ifdef ASSERT\n-inline bool AbstractAssembler::pd_check_instruction_mark() { return false; }\n-#endif\n-\n-\/**\n- * class SkipIfEqual:\n- *\n- * Instantiating this class will result in assembly code being output that will\n- * jump around any code emitted between the creation of the instance and it's\n- * automatic destruction at the end of a scope block, depending on the value of\n- * the flag passed to the constructor, which will be checked at run-time.\n- *\/\n-class SkipIfEqual {\n- private:\n-  MacroAssembler* _masm;\n-  Label _label;\n-\n- public:\n-   SkipIfEqual(MacroAssembler*, const bool* flag_addr, bool value);\n-   ~SkipIfEqual();\n-};\n-\n-struct tableswitch {\n-  Register _reg;\n-  int _insn_index; jint _first_key; jint _last_key;\n-  Label _after;\n-  Label _branches;\n-};\n-\n-#endif \/\/ CPU_AARCH64_MACROASSEMBLER_AARCH64_HPP\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.hpp","additions":0,"deletions":1418,"binary":false,"changes":1418,"status":"deleted"},{"patch":"@@ -0,0 +1,662 @@\n+\/*\n+ * Copyright (c) 2003, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, 2021, Red Hat Inc. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+\n+#include \"asm\/assembler.hpp\"\n+#include \"asm\/assembler.inline.hpp\"\n+#include \"macroAssembler_aarch64.hpp\"\n+#include \"memory\/resourceArea.hpp\"\n+#include \"runtime\/stubRoutines.hpp\"\n+\n+void MacroAssembler::aesecb_decrypt(Register from, Register to, Register key, Register keylen) {\n+  Label L_doLast;\n+\n+  ld1(v0, T16B, from); \/\/ get 16 bytes of input\n+\n+  ld1(v5, T16B, post(key, 16));\n+  rev32(v5, T16B, v5);\n+\n+  ld1(v1, v2, v3, v4, T16B, post(key, 64));\n+  rev32(v1, T16B, v1);\n+  rev32(v2, T16B, v2);\n+  rev32(v3, T16B, v3);\n+  rev32(v4, T16B, v4);\n+  aesd(v0, v1);\n+  aesimc(v0, v0);\n+  aesd(v0, v2);\n+  aesimc(v0, v0);\n+  aesd(v0, v3);\n+  aesimc(v0, v0);\n+  aesd(v0, v4);\n+  aesimc(v0, v0);\n+\n+  ld1(v1, v2, v3, v4, T16B, post(key, 64));\n+  rev32(v1, T16B, v1);\n+  rev32(v2, T16B, v2);\n+  rev32(v3, T16B, v3);\n+  rev32(v4, T16B, v4);\n+  aesd(v0, v1);\n+  aesimc(v0, v0);\n+  aesd(v0, v2);\n+  aesimc(v0, v0);\n+  aesd(v0, v3);\n+  aesimc(v0, v0);\n+  aesd(v0, v4);\n+  aesimc(v0, v0);\n+\n+  ld1(v1, v2, T16B, post(key, 32));\n+  rev32(v1, T16B, v1);\n+  rev32(v2, T16B, v2);\n+\n+  cmpw(keylen, 44);\n+  br(Assembler::EQ, L_doLast);\n+\n+  aesd(v0, v1);\n+  aesimc(v0, v0);\n+  aesd(v0, v2);\n+  aesimc(v0, v0);\n+\n+  ld1(v1, v2, T16B, post(key, 32));\n+  rev32(v1, T16B, v1);\n+  rev32(v2, T16B, v2);\n+\n+  cmpw(keylen, 52);\n+  br(Assembler::EQ, L_doLast);\n+\n+  aesd(v0, v1);\n+  aesimc(v0, v0);\n+  aesd(v0, v2);\n+  aesimc(v0, v0);\n+\n+  ld1(v1, v2, T16B, post(key, 32));\n+  rev32(v1, T16B, v1);\n+  rev32(v2, T16B, v2);\n+\n+  bind(L_doLast);\n+\n+  aesd(v0, v1);\n+  aesimc(v0, v0);\n+  aesd(v0, v2);\n+\n+  eor(v0, T16B, v0, v5);\n+\n+  st1(v0, T16B, to);\n+\n+  \/\/ Preserve the address of the start of the key\n+  sub(key, key, keylen, LSL, exact_log2(sizeof (jint)));\n+}\n+\n+\/\/ Load expanded key into v17..v31\n+void MacroAssembler::aesenc_loadkeys(Register key, Register keylen) {\n+  Label L_loadkeys_44, L_loadkeys_52;\n+  cmpw(keylen, 52);\n+  br(Assembler::LO, L_loadkeys_44);\n+  br(Assembler::EQ, L_loadkeys_52);\n+\n+  ld1(v17, v18,  T16B,  post(key, 32));\n+  rev32(v17,  T16B, v17);\n+  rev32(v18,  T16B, v18);\n+  bind(L_loadkeys_52);\n+  ld1(v19, v20,  T16B,  post(key, 32));\n+  rev32(v19,  T16B, v19);\n+  rev32(v20,  T16B, v20);\n+  bind(L_loadkeys_44);\n+  ld1(v21, v22, v23, v24,  T16B,  post(key, 64));\n+  rev32(v21,  T16B, v21);\n+  rev32(v22,  T16B, v22);\n+  rev32(v23,  T16B, v23);\n+  rev32(v24,  T16B, v24);\n+  ld1(v25, v26, v27, v28,  T16B,  post(key, 64));\n+  rev32(v25,  T16B, v25);\n+  rev32(v26,  T16B, v26);\n+  rev32(v27,  T16B, v27);\n+  rev32(v28,  T16B, v28);\n+  ld1(v29, v30, v31,  T16B, post(key, 48));\n+  rev32(v29,  T16B, v29);\n+  rev32(v30,  T16B, v30);\n+  rev32(v31,  T16B, v31);\n+\n+  \/\/ Preserve the address of the start of the key\n+  sub(key, key, keylen, LSL, exact_log2(sizeof (jint)));\n+}\n+\n+\/\/ NeoverseTM N1Software Optimization Guide:\n+\/\/ Adjacent AESE\/AESMC instruction pairs and adjacent AESD\/AESIMC\n+\/\/ instruction pairs will exhibit the performance characteristics\n+\/\/ described in Section 4.6.\n+void MacroAssembler::aes_round(FloatRegister input, FloatRegister subkey) {\n+  aese(input, subkey); aesmc(input, input);\n+}\n+\n+\/\/ The abstract base class of an unrolled funtion\n+\/\/ generator. Subclasses override generate(), length(), and next() to\n+\/\/ generate unrolled and interleaved functions.\n+class KernelGenerator: public MacroAssembler {\n+protected:\n+  const int _unrolls;\n+public:\n+  KernelGenerator(Assembler *as, int unrolls)\n+    : MacroAssembler(as->code()), _unrolls(unrolls) { }\n+  virtual void generate(int index) = 0;\n+  virtual int length() = 0;\n+  virtual KernelGenerator *next() = 0;\n+  int unrolls() { return _unrolls; }\n+  void unroll();\n+};\n+\n+void KernelGenerator::unroll() {\n+  KernelGenerator **generators\n+    = NEW_RESOURCE_ARRAY(KernelGenerator *, unrolls());\n+  generators[0] = this;\n+  for (int i = 1; i < unrolls(); i++) {\n+    generators[i] = generators[i-1]->next();\n+  }\n+\n+  for (int j = 0; j < length(); j++) {\n+    for (int i = 0; i < unrolls(); i++) {\n+      generators[i]->generate(j);\n+    }\n+  }\n+}\n+\n+\/\/ An unrolled and interleaved generator for the kernel AES\n+\/\/ encryption.\n+class AESKernelGenerator: public KernelGenerator {\n+  Register _from, _to;\n+  const Register _keylen;\n+  FloatRegister _data;\n+  const FloatRegister _subkeys;\n+  bool _once;\n+  Label _rounds_44, _rounds_52;\n+\n+public:\n+  AESKernelGenerator(Assembler *as, int unrolls,\n+                     Register from, Register to, Register keylen, FloatRegister data,\n+                     FloatRegister subkeys, bool once = true)\n+    : KernelGenerator(as, unrolls),\n+      _from(from), _to(to), _keylen(keylen), _data(data),\n+      _subkeys(subkeys), _once(once) {\n+  }\n+\n+  virtual void generate(int index) {\n+    switch (index) {\n+    case  0:\n+      if (_from != noreg) {\n+        ld1(_data, T16B, _from); \/\/ get 16 bytes of input\n+      }\n+      break;\n+    case  1:\n+      if (_once) {\n+        cmpw(_keylen, 52);\n+        br(Assembler::LO, _rounds_44);\n+        br(Assembler::EQ, _rounds_52);\n+      }\n+      break;\n+    case  2:  aes_round(_data, _subkeys +  0);  break;\n+    case  3:  aes_round(_data, _subkeys +  1);  break;\n+    case  4:\n+      if (_once)  bind(_rounds_52);\n+      break;\n+    case  5:  aes_round(_data, _subkeys +  2);  break;\n+    case  6:  aes_round(_data, _subkeys +  3);  break;\n+    case  7:\n+      if (_once)  bind(_rounds_44);\n+      break;\n+    case  8:  aes_round(_data, _subkeys +  4);  break;\n+    case  9:  aes_round(_data, _subkeys +  5);  break;\n+    case 10:  aes_round(_data, _subkeys +  6);  break;\n+    case 11:  aes_round(_data, _subkeys +  7);  break;\n+    case 12:  aes_round(_data, _subkeys +  8);  break;\n+    case 13:  aes_round(_data, _subkeys +  9);  break;\n+    case 14:  aes_round(_data, _subkeys + 10);  break;\n+    case 15:  aes_round(_data, _subkeys + 11);  break;\n+    case 16:  aes_round(_data, _subkeys + 12);  break;\n+    case 17:  aese(_data, _subkeys + 13);  break;\n+    case 18:  eor(_data, T16B, _data, _subkeys + 14);  break;\n+    case 19:\n+      if (_to != noreg) {\n+        st1(_data, T16B, _to);\n+      }\n+      break;\n+    default: ShouldNotReachHere();\n+    }\n+  }\n+\n+  virtual KernelGenerator *next() {\n+    return new AESKernelGenerator(this, _unrolls,\n+                                  _from, _to, _keylen,\n+                                  _data + 1, _subkeys, \/*once*\/false);\n+  }\n+\n+  virtual int length() { return 20; }\n+};\n+\n+\/\/ Uses expanded key in v17..v31\n+\/\/ Returns encrypted values in inputs.\n+\/\/ If to != noreg, store value at to; likewise from\n+\/\/ Preserves key, keylen\n+\/\/ Increments from, to\n+\/\/ Input data in v0, v1, ...\n+\/\/ unrolls controls the number of times to unroll the generated function\n+void MacroAssembler::aesecb_encrypt(Register from, Register to, Register keylen,\n+                                    FloatRegister data, int unrolls) {\n+  AESKernelGenerator(this, unrolls, from, to, keylen, data, v17) .unroll();\n+}\n+\n+\/\/ ghash_multiply and ghash_reduce are the non-unrolled versions of\n+\/\/ the GHASH function generators.\n+void MacroAssembler::ghash_multiply(FloatRegister result_lo, FloatRegister result_hi,\n+                                     FloatRegister a, FloatRegister b, FloatRegister a1_xor_a0,\n+                                     FloatRegister tmp1, FloatRegister tmp2, FloatRegister tmp3) {\n+  \/\/ Karatsuba multiplication performs a 128*128 -> 256-bit\n+  \/\/ multiplication in three 128-bit multiplications and a few\n+  \/\/ additions.\n+  \/\/\n+  \/\/ (C1:C0) = A1*B1, (D1:D0) = A0*B0, (E1:E0) = (A0+A1)(B0+B1)\n+  \/\/ (A1:A0)(B1:B0) = C1:(C0+C1+D1+E1):(D1+C0+D0+E0):D0\n+  \/\/\n+  \/\/ Inputs:\n+  \/\/\n+  \/\/ A0 in a.d[0]     (subkey)\n+  \/\/ A1 in a.d[1]\n+  \/\/ (A1+A0) in a1_xor_a0.d[0]\n+  \/\/\n+  \/\/ B0 in b.d[0]     (state)\n+  \/\/ B1 in b.d[1]\n+\n+  ext(tmp1, T16B, b, b, 0x08);\n+  pmull2(result_hi, T1Q, b, a, T2D);  \/\/ A1*B1\n+  eor(tmp1, T16B, tmp1, b);           \/\/ (B1+B0)\n+  pmull(result_lo,  T1Q, b, a, T1D);  \/\/ A0*B0\n+  pmull(tmp2, T1Q, tmp1, a1_xor_a0, T1D); \/\/ (A1+A0)(B1+B0)\n+\n+  ext(tmp1, T16B, result_lo, result_hi, 0x08);\n+  eor(tmp3, T16B, result_hi, result_lo); \/\/ A1*B1+A0*B0\n+  eor(tmp2, T16B, tmp2, tmp1);\n+  eor(tmp2, T16B, tmp2, tmp3);\n+\n+  \/\/ Register pair <result_hi:result_lo> holds the result of carry-less multiplication\n+  ins(result_hi, D, tmp2, 0, 1);\n+  ins(result_lo, D, tmp2, 1, 0);\n+}\n+\n+void MacroAssembler::ghash_reduce(FloatRegister result, FloatRegister lo, FloatRegister hi,\n+                  FloatRegister p, FloatRegister vzr, FloatRegister t1) {\n+  const FloatRegister t0 = result;\n+\n+  \/\/ The GCM field polynomial f is z^128 + p(z), where p =\n+  \/\/ z^7+z^2+z+1.\n+  \/\/\n+  \/\/    z^128 === -p(z)  (mod (z^128 + p(z)))\n+  \/\/\n+  \/\/ so, given that the product we're reducing is\n+  \/\/    a == lo + hi * z^128\n+  \/\/ substituting,\n+  \/\/      === lo - hi * p(z)  (mod (z^128 + p(z)))\n+  \/\/\n+  \/\/ we reduce by multiplying hi by p(z) and subtracting the result\n+  \/\/ from (i.e. XORing it with) lo.  Because p has no nonzero high\n+  \/\/ bits we can do this with two 64-bit multiplications, lo*p and\n+  \/\/ hi*p.\n+\n+  pmull2(t0, T1Q, hi, p, T2D);\n+  ext(t1, T16B, t0, vzr, 8);\n+  eor(hi, T16B, hi, t1);\n+  ext(t1, T16B, vzr, t0, 8);\n+  eor(lo, T16B, lo, t1);\n+  pmull(t0, T1Q, hi, p, T1D);\n+  eor(result, T16B, lo, t0);\n+}\n+\n+class GHASHMultiplyGenerator: public KernelGenerator {\n+  FloatRegister _result_lo, _result_hi, _b,\n+    _a, _vzr, _a1_xor_a0, _p,\n+    _tmp1, _tmp2, _tmp3;\n+\n+public:\n+  GHASHMultiplyGenerator(Assembler *as, int unrolls,\n+                         \/* offsetted registers *\/\n+                         FloatRegister result_lo, FloatRegister result_hi,\n+                         FloatRegister b,\n+                         \/* non-offsetted (shared) registers *\/\n+                         FloatRegister a, FloatRegister a1_xor_a0, FloatRegister p, FloatRegister vzr,\n+                         \/* offseted (temp) registers *\/\n+                         FloatRegister tmp1, FloatRegister tmp2, FloatRegister tmp3)\n+    : KernelGenerator(as, unrolls),\n+      _result_lo(result_lo), _result_hi(result_hi), _b(b),\n+      _a(a), _vzr(vzr), _a1_xor_a0(a1_xor_a0), _p(p),\n+      _tmp1(tmp1), _tmp2(tmp2), _tmp3(tmp3) { }\n+\n+  int register_stride = 7;\n+\n+  virtual void generate(int index) {\n+    \/\/ Karatsuba multiplication performs a 128*128 -> 256-bit\n+    \/\/ multiplication in three 128-bit multiplications and a few\n+    \/\/ additions.\n+    \/\/\n+    \/\/ (C1:C0) = A1*B1, (D1:D0) = A0*B0, (E1:E0) = (A0+A1)(B0+B1)\n+    \/\/ (A1:A0)(B1:B0) = C1:(C0+C1+D1+E1):(D1+C0+D0+E0):D0\n+    \/\/\n+    \/\/ Inputs:\n+    \/\/\n+    \/\/ A0 in a.d[0]     (subkey)\n+    \/\/ A1 in a.d[1]\n+    \/\/ (A1+A0) in a1_xor_a0.d[0]\n+    \/\/\n+    \/\/ B0 in b.d[0]     (state)\n+    \/\/ B1 in b.d[1]\n+\n+    switch (index) {\n+      case  0:  ext(_tmp1, T16B, _b, _b, 0x08);  break;\n+      case  1:  pmull2(_result_hi, T1Q, _b, _a, T2D);  \/\/ A1*B1\n+        break;\n+      case  2:  eor(_tmp1, T16B, _tmp1, _b);           \/\/ (B1+B0)\n+        break;\n+      case  3:  pmull(_result_lo,  T1Q, _b, _a, T1D);  \/\/ A0*B0\n+        break;\n+      case  4:  pmull(_tmp2, T1Q, _tmp1, _a1_xor_a0, T1D); \/\/ (A1+A0)(B1+B0)\n+        break;\n+\n+      case  5:  ext(_tmp1, T16B, _result_lo, _result_hi, 0x08);  break;\n+      case  6:  eor(_tmp3, T16B, _result_hi, _result_lo); \/\/ A1*B1+A0*B0\n+        break;\n+      case  7:  eor(_tmp2, T16B, _tmp2, _tmp1);  break;\n+      case  8:  eor(_tmp2, T16B, _tmp2, _tmp3);  break;\n+\n+        \/\/ Register pair <_result_hi:_result_lo> holds the _result of carry-less multiplication\n+      case  9:  ins(_result_hi, D, _tmp2, 0, 1);  break;\n+      case 10:  ins(_result_lo, D, _tmp2, 1, 0);  break;\n+      default: ShouldNotReachHere();\n+    }\n+  }\n+\n+  virtual KernelGenerator *next() {\n+    GHASHMultiplyGenerator *result = new GHASHMultiplyGenerator(*this);\n+    result->_result_lo += register_stride;\n+    result->_result_hi += register_stride;\n+    result->_b += register_stride;\n+    result->_tmp1 += register_stride;\n+    result->_tmp2 += register_stride;\n+    result->_tmp3 += register_stride;\n+    return result;\n+  }\n+\n+  virtual int length() { return 11; }\n+};\n+\n+\/\/ Reduce the 128-bit product in hi:lo by the GCM field polynomial.\n+\/\/ The FloatRegister argument called data is optional: if it is a\n+\/\/ valid register, we interleave LD1 instructions with the\n+\/\/ reduction. This is to reduce latency next time around the loop.\n+class GHASHReduceGenerator: public KernelGenerator {\n+  FloatRegister _result, _lo, _hi, _p, _vzr, _data, _t1;\n+  int _once;\n+public:\n+  GHASHReduceGenerator(Assembler *as, int unrolls,\n+                       \/* offsetted registers *\/\n+                       FloatRegister result, FloatRegister lo, FloatRegister hi,\n+                       \/* non-offsetted (shared) registers *\/\n+                       FloatRegister p, FloatRegister vzr, FloatRegister data,\n+                       \/* offseted (temp) registers *\/\n+                       FloatRegister t1)\n+    : KernelGenerator(as, unrolls),\n+      _result(result), _lo(lo), _hi(hi),\n+      _p(p), _vzr(vzr), _data(data), _t1(t1), _once(true) { }\n+\n+  int register_stride = 7;\n+\n+  virtual void generate(int index) {\n+    const FloatRegister t0 = _result;\n+\n+    switch (index) {\n+      \/\/ The GCM field polynomial f is z^128 + p(z), where p =\n+      \/\/ z^7+z^2+z+1.\n+      \/\/\n+      \/\/    z^128 === -p(z)  (mod (z^128 + p(z)))\n+      \/\/\n+      \/\/ so, given that the product we're reducing is\n+      \/\/    a == lo + hi * z^128\n+      \/\/ substituting,\n+      \/\/      === lo - hi * p(z)  (mod (z^128 + p(z)))\n+      \/\/\n+      \/\/ we reduce by multiplying hi by p(z) and subtracting the _result\n+      \/\/ from (i.e. XORing it with) lo.  Because p has no nonzero high\n+      \/\/ bits we can do this with two 64-bit multiplications, lo*p and\n+      \/\/ hi*p.\n+\n+      case  0:  pmull2(t0, T1Q, _hi, _p, T2D);  break;\n+      case  1:  ext(_t1, T16B, t0, _vzr, 8);  break;\n+      case  2:  eor(_hi, T16B, _hi, _t1);  break;\n+      case  3:  ext(_t1, T16B, _vzr, t0, 8);  break;\n+      case  4:  eor(_lo, T16B, _lo, _t1);  break;\n+      case  5:  pmull(t0, T1Q, _hi, _p, T1D);  break;\n+      case  6:  eor(_result, T16B, _lo, t0);  break;\n+      default: ShouldNotReachHere();\n+    }\n+\n+    \/\/ Sprinkle load instructions into the generated instructions\n+    if (_data->is_valid() && _once) {\n+      assert(length() >= unrolls(), \"not enough room for inteleaved loads\");\n+      if (index < unrolls()) {\n+        ld1((_data + index*register_stride), T16B, post(r2, 0x10));\n+      }\n+    }\n+  }\n+\n+  virtual KernelGenerator *next() {\n+    GHASHReduceGenerator *result = new GHASHReduceGenerator(*this);\n+    result->_result += register_stride;\n+    result->_hi += register_stride;\n+    result->_lo += register_stride;\n+    result->_t1 += register_stride;\n+    result->_once = false;\n+    return result;\n+  }\n+\n+ int length() { return 7; }\n+};\n+\n+\/\/ Perform a GHASH multiply\/reduce on a single FloatRegister.\n+void MacroAssembler::ghash_modmul(FloatRegister result,\n+                                  FloatRegister result_lo, FloatRegister result_hi, FloatRegister b,\n+                                  FloatRegister a, FloatRegister vzr, FloatRegister a1_xor_a0, FloatRegister p,\n+                                  FloatRegister t1, FloatRegister t2, FloatRegister t3) {\n+  ghash_multiply(result_lo, result_hi, a, b, a1_xor_a0, t1, t2, t3);\n+  ghash_reduce(result, result_lo, result_hi, p, vzr, t1);\n+}\n+\n+\/\/ Interleaved GHASH processing.\n+\/\/\n+\/\/ Clobbers all vector registers.\n+\/\/\n+void MacroAssembler::ghash_processBlocks_wide(address field_polynomial, Register state,\n+                                              Register subkeyH,\n+                                              Register data, Register blocks, int unrolls) {\n+  int register_stride = 7;\n+\n+  \/\/ Bafflingly, GCM uses little-endian for the byte order, but\n+  \/\/ big-endian for the bit order.  For example, the polynomial 1 is\n+  \/\/ represented as the 16-byte string 80 00 00 00 | 12 bytes of 00.\n+  \/\/\n+  \/\/ So, we must either reverse the bytes in each word and do\n+  \/\/ everything big-endian or reverse the bits in each byte and do\n+  \/\/ it little-endian.  On AArch64 it's more idiomatic to reverse\n+  \/\/ the bits in each byte (we have an instruction, RBIT, to do\n+  \/\/ that) and keep the data in little-endian bit order throught the\n+  \/\/ calculation, bit-reversing the inputs and outputs.\n+\n+  assert(unrolls * register_stride < 32, \"out of registers\");\n+\n+  FloatRegister a1_xor_a0 = v28;\n+  FloatRegister Hprime = v29;\n+  FloatRegister vzr = v30;\n+  FloatRegister p = v31;\n+  eor(vzr, T16B, vzr, vzr); \/\/ zero register\n+\n+  ldrq(p, field_polynomial);    \/\/ The field polynomial\n+\n+  ldrq(v0, Address(state));\n+  ldrq(Hprime, Address(subkeyH));\n+\n+  rev64(v0, T16B, v0);          \/\/ Bit-reverse words in state and subkeyH\n+  rbit(v0, T16B, v0);\n+  rev64(Hprime, T16B, Hprime);\n+  rbit(Hprime, T16B, Hprime);\n+\n+  \/\/ Powers of H -> Hprime\n+\n+  Label already_calculated, done;\n+  {\n+    \/\/ The first time around we'll have to calculate H**2, H**3, etc.\n+    \/\/ Look at the largest power of H in the subkeyH array to see if\n+    \/\/ it's already been calculated.\n+    ldp(rscratch1, rscratch2, Address(subkeyH, 16 * (unrolls - 1)));\n+    orr(rscratch1, rscratch1, rscratch2);\n+    cbnz(rscratch1, already_calculated);\n+\n+    orr(v6, T16B, Hprime, Hprime);  \/\/ Start with H in v6 and Hprime\n+    for (int i = 1; i < unrolls; i++) {\n+      ext(a1_xor_a0, T16B, Hprime, Hprime, 0x08); \/\/ long-swap subkeyH into a1_xor_a0\n+      eor(a1_xor_a0, T16B, a1_xor_a0, Hprime);    \/\/ xor subkeyH into subkeyL (Karatsuba: (A1+A0))\n+      ghash_modmul(\/*result*\/v6, \/*result_lo*\/v5, \/*result_hi*\/v4, \/*b*\/v6,\n+                   Hprime, vzr, a1_xor_a0, p,\n+                   \/*temps*\/v1, v3, v2);\n+      rev64(v1, T16B, v6);\n+      rbit(v1, T16B, v1);\n+      strq(v1, Address(subkeyH, 16 * i));\n+    }\n+    b(done);\n+  }\n+  {\n+    bind(already_calculated);\n+\n+    \/\/ Load the largest power of H we need into v6.\n+    ldrq(v6, Address(subkeyH, 16 * (unrolls - 1)));\n+    rev64(v6, T16B, v6);\n+    rbit(v6, T16B, v6);\n+  }\n+  bind(done);\n+\n+  orr(Hprime, T16B, v6, v6);     \/\/ Move H ** unrolls into Hprime\n+\n+  \/\/ Hprime contains (H ** 1, H ** 2, ... H ** unrolls)\n+  \/\/ v0 contains the initial state. Clear the others.\n+  for (int i = 1; i < unrolls; i++) {\n+    int ofs = register_stride * i;\n+    eor(ofs+v0, T16B, ofs+v0, ofs+v0); \/\/ zero each state register\n+  }\n+\n+  ext(a1_xor_a0, T16B, Hprime, Hprime, 0x08); \/\/ long-swap subkeyH into a1_xor_a0\n+  eor(a1_xor_a0, T16B, a1_xor_a0, Hprime);    \/\/ xor subkeyH into subkeyL (Karatsuba: (A1+A0))\n+\n+  \/\/ Load #unrolls blocks of data\n+  for (int ofs = 0; ofs < unrolls * register_stride; ofs += register_stride) {\n+    ld1(v2+ofs, T16B, post(data, 0x10));\n+  }\n+\n+  \/\/ Register assignments, replicated across 4 clones, v0 ... v23\n+  \/\/\n+  \/\/ v0: current state, result of multiply\/reduce\n+  \/\/ v1: temp\n+  \/\/ v2: one block of data (the ciphertext)\n+  \/\/ v3: temp\n+  \/\/ v4: high part of product\n+  \/\/ v5: low part ...\n+  \/\/\n+  \/\/ Not replicated:\n+  \/\/\n+  \/\/ v28: High part of H xor low part of H'\n+  \/\/ v29: H' (hash subkey)\n+  \/\/ v30: zero\n+  \/\/ v31: Reduction polynomial of the Galois field\n+\n+  \/\/ Inner loop.\n+  \/\/ Do the whole load\/add\/multiply\/reduce over all our data except\n+  \/\/ the last few rows.\n+  {\n+    Label L_ghash_loop;\n+    bind(L_ghash_loop);\n+\n+    \/\/ Prefetching doesn't help here. In fact, on Neoverse N1 it's worse.\n+    \/\/ prfm(Address(data, 128), PLDL1KEEP);\n+\n+    \/\/ Xor data into current state\n+    for (int ofs = 0; ofs < unrolls * register_stride; ofs += register_stride) {\n+      rbit((v2+ofs), T16B, (v2+ofs));\n+      eor((v2+ofs), T16B, v0+ofs, (v2+ofs));   \/\/ bit-swapped data ^ bit-swapped state\n+    }\n+\n+    \/\/ Generate fully-unrolled multiply-reduce in two stages.\n+\n+    GHASHMultiplyGenerator(this, unrolls,\n+                           \/*result_lo*\/v5, \/*result_hi*\/v4, \/*data*\/v2,\n+                           Hprime, a1_xor_a0, p, vzr,\n+                           \/*temps*\/v1, v3, \/* reuse b*\/v2) .unroll();\n+\n+    \/\/ NB: GHASHReduceGenerator also loads the next #unrolls blocks of\n+    \/\/ data into .\n+    GHASHReduceGenerator (this, unrolls,\n+                          \/*result*\/v0, \/*lo*\/v5, \/*hi*\/v4, p, vzr,\n+                          \/*data*\/v2, \/*temp*\/v3) .unroll();\n+\n+    sub(blocks, blocks, unrolls);\n+    cmp(blocks, (unsigned char)(unrolls * 2));\n+    br(GE, L_ghash_loop);\n+  }\n+\n+  \/\/ Merge the #unrolls states.  Note that the data for the next\n+  \/\/ operation has already been loaded into v4, v4+ofs, etc...\n+\n+  \/\/ First, we multiply\/reduce each clone by the appropriate power of H.\n+  for (int i = 0; i < unrolls; i++) {\n+    int ofs = register_stride * i;\n+    ldrq(Hprime, Address(subkeyH, 16 * (unrolls - i - 1)));\n+\n+    rbit(v2+ofs, T16B, v2+ofs);\n+    eor(v2+ofs, T16B, ofs+v0, v2+ofs);   \/\/ bit-swapped data ^ bit-swapped state\n+\n+    rev64(Hprime, T16B, Hprime);\n+    rbit(Hprime, T16B, Hprime);\n+    ext(a1_xor_a0, T16B, Hprime, Hprime, 0x08); \/\/ long-swap subkeyH into a1_xor_a0\n+    eor(a1_xor_a0, T16B, a1_xor_a0, Hprime);    \/\/ xor subkeyH into subkeyL (Karatsuba: (A1+A0))\n+    ghash_modmul(\/*result*\/v0+ofs, \/*result_lo*\/v5+ofs, \/*result_hi*\/v4+ofs, \/*b*\/v2+ofs,\n+                 Hprime, vzr, a1_xor_a0, p,\n+                 \/*temps*\/v1+ofs, v3+ofs, \/* reuse b*\/v2+ofs);\n+  }\n+\n+  \/\/ Then we sum the results.\n+  for (int i = 0; i < unrolls - 1; i++) {\n+    int ofs = register_stride * i;\n+    eor(v0, T16B, v0, v0 + register_stride + ofs);\n+  }\n+\n+  sub(blocks, blocks, (unsigned char)unrolls);\n+\n+  \/\/ And finally bit-reverse the state back to big endian.\n+  rev64(v0, T16B, v0);\n+  rbit(v0, T16B, v0);\n+  st1(v0, T16B, state);\n+}\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64_aes.cpp","additions":662,"deletions":0,"binary":false,"changes":662,"status":"added"},{"patch":"@@ -2563,2 +2563,0 @@\n-    Label L_doLast;\n-\n@@ -2575,69 +2573,2 @@\n-    __ ld1(v0, __ T16B, from); \/\/ get 16 bytes of input\n-\n-    __ ld1(v1, v2, v3, v4, __ T16B, __ post(key, 64));\n-    __ rev32(v1, __ T16B, v1);\n-    __ rev32(v2, __ T16B, v2);\n-    __ rev32(v3, __ T16B, v3);\n-    __ rev32(v4, __ T16B, v4);\n-    __ aese(v0, v1);\n-    __ aesmc(v0, v0);\n-    __ aese(v0, v2);\n-    __ aesmc(v0, v0);\n-    __ aese(v0, v3);\n-    __ aesmc(v0, v0);\n-    __ aese(v0, v4);\n-    __ aesmc(v0, v0);\n-\n-    __ ld1(v1, v2, v3, v4, __ T16B, __ post(key, 64));\n-    __ rev32(v1, __ T16B, v1);\n-    __ rev32(v2, __ T16B, v2);\n-    __ rev32(v3, __ T16B, v3);\n-    __ rev32(v4, __ T16B, v4);\n-    __ aese(v0, v1);\n-    __ aesmc(v0, v0);\n-    __ aese(v0, v2);\n-    __ aesmc(v0, v0);\n-    __ aese(v0, v3);\n-    __ aesmc(v0, v0);\n-    __ aese(v0, v4);\n-    __ aesmc(v0, v0);\n-\n-    __ ld1(v1, v2, __ T16B, __ post(key, 32));\n-    __ rev32(v1, __ T16B, v1);\n-    __ rev32(v2, __ T16B, v2);\n-\n-    __ cmpw(keylen, 44);\n-    __ br(Assembler::EQ, L_doLast);\n-\n-    __ aese(v0, v1);\n-    __ aesmc(v0, v0);\n-    __ aese(v0, v2);\n-    __ aesmc(v0, v0);\n-\n-    __ ld1(v1, v2, __ T16B, __ post(key, 32));\n-    __ rev32(v1, __ T16B, v1);\n-    __ rev32(v2, __ T16B, v2);\n-\n-    __ cmpw(keylen, 52);\n-    __ br(Assembler::EQ, L_doLast);\n-\n-    __ aese(v0, v1);\n-    __ aesmc(v0, v0);\n-    __ aese(v0, v2);\n-    __ aesmc(v0, v0);\n-\n-    __ ld1(v1, v2, __ T16B, __ post(key, 32));\n-    __ rev32(v1, __ T16B, v1);\n-    __ rev32(v2, __ T16B, v2);\n-\n-    __ BIND(L_doLast);\n-\n-    __ aese(v0, v1);\n-    __ aesmc(v0, v0);\n-    __ aese(v0, v2);\n-\n-    __ ld1(v1, __ T16B, key);\n-    __ rev32(v1, __ T16B, v1);\n-    __ eor(v0, __ T16B, v0, v1);\n-\n-    __ st1(v0, __ T16B, to);\n+    __ aesenc_loadkeys(key, keylen);\n+    __ aesecb_encrypt(from, to, keylen);\n@@ -2676,70 +2607,1 @@\n-    __ ld1(v0, __ T16B, from); \/\/ get 16 bytes of input\n-\n-    __ ld1(v5, __ T16B, __ post(key, 16));\n-    __ rev32(v5, __ T16B, v5);\n-\n-    __ ld1(v1, v2, v3, v4, __ T16B, __ post(key, 64));\n-    __ rev32(v1, __ T16B, v1);\n-    __ rev32(v2, __ T16B, v2);\n-    __ rev32(v3, __ T16B, v3);\n-    __ rev32(v4, __ T16B, v4);\n-    __ aesd(v0, v1);\n-    __ aesimc(v0, v0);\n-    __ aesd(v0, v2);\n-    __ aesimc(v0, v0);\n-    __ aesd(v0, v3);\n-    __ aesimc(v0, v0);\n-    __ aesd(v0, v4);\n-    __ aesimc(v0, v0);\n-\n-    __ ld1(v1, v2, v3, v4, __ T16B, __ post(key, 64));\n-    __ rev32(v1, __ T16B, v1);\n-    __ rev32(v2, __ T16B, v2);\n-    __ rev32(v3, __ T16B, v3);\n-    __ rev32(v4, __ T16B, v4);\n-    __ aesd(v0, v1);\n-    __ aesimc(v0, v0);\n-    __ aesd(v0, v2);\n-    __ aesimc(v0, v0);\n-    __ aesd(v0, v3);\n-    __ aesimc(v0, v0);\n-    __ aesd(v0, v4);\n-    __ aesimc(v0, v0);\n-\n-    __ ld1(v1, v2, __ T16B, __ post(key, 32));\n-    __ rev32(v1, __ T16B, v1);\n-    __ rev32(v2, __ T16B, v2);\n-\n-    __ cmpw(keylen, 44);\n-    __ br(Assembler::EQ, L_doLast);\n-\n-    __ aesd(v0, v1);\n-    __ aesimc(v0, v0);\n-    __ aesd(v0, v2);\n-    __ aesimc(v0, v0);\n-\n-    __ ld1(v1, v2, __ T16B, __ post(key, 32));\n-    __ rev32(v1, __ T16B, v1);\n-    __ rev32(v2, __ T16B, v2);\n-\n-    __ cmpw(keylen, 52);\n-    __ br(Assembler::EQ, L_doLast);\n-\n-    __ aesd(v0, v1);\n-    __ aesimc(v0, v0);\n-    __ aesd(v0, v2);\n-    __ aesimc(v0, v0);\n-\n-    __ ld1(v1, v2, __ T16B, __ post(key, 32));\n-    __ rev32(v1, __ T16B, v1);\n-    __ rev32(v2, __ T16B, v2);\n-\n-    __ BIND(L_doLast);\n-\n-    __ aesd(v0, v1);\n-    __ aesimc(v0, v0);\n-    __ aesd(v0, v2);\n-\n-    __ eor(v0, __ T16B, v0, v5);\n-\n-    __ st1(v0, __ T16B, to);\n+    __ aesecb_decrypt(from, to, key, keylen);\n@@ -2967,0 +2829,369 @@\n+  \/\/ CTR AES crypt.\n+  \/\/ Arguments:\n+  \/\/\n+  \/\/ Inputs:\n+  \/\/   c_rarg0   - source byte array address\n+  \/\/   c_rarg1   - destination byte array address\n+  \/\/   c_rarg2   - K (key) in little endian int array\n+  \/\/   c_rarg3   - counter vector byte array address\n+  \/\/   c_rarg4   - input length\n+  \/\/   c_rarg5   - saved encryptedCounter start\n+  \/\/   c_rarg6   - saved used length\n+  \/\/\n+  \/\/ Output:\n+  \/\/   r0       - input length\n+  \/\/\n+  address generate_counterMode_AESCrypt() {\n+    const Register in = c_rarg0;\n+    const Register out = c_rarg1;\n+    const Register key = c_rarg2;\n+    const Register counter = c_rarg3;\n+    const Register saved_len = c_rarg4, len = r10;\n+    const Register saved_encrypted_ctr = c_rarg5;\n+    const Register used_ptr = c_rarg6, used = r12;\n+\n+    const Register offset = r7;\n+    const Register keylen = r11;\n+\n+    const unsigned char block_size = 16;\n+    const int bulk_width = 4;\n+\n+\n+    \/\/ Algorithm:\n+    \/\/\n+    \/\/ int result = len;\n+    \/\/ while (len-- > 0) {\n+    \/\/     if (used >= blockSize) {\n+    \/\/         if (len >= bulk_width * blockSize()) {\n+    \/\/             CTR_large_block();\n+    \/\/             if (len == 0)\n+    \/\/                 break; \/* goto DONE; *\/\n+    \/\/         }\n+    \/\/         for (;;) {\n+    \/\/             16ByteVector v0 = counter;\n+    \/\/             embeddedCipher.encryptBlock(v0, 0, encryptedCounter, 0);\n+    \/\/             used = 0;\n+    \/\/             if (len < blockSize)\n+    \/\/                 break;  \/* goto NEXT *\/\n+    \/\/             16ByteVector v1 = load16Bytes(in, offset);\n+    \/\/             v1 = v1 ^ encryptedCounter;\n+    \/\/             store16Bytes(out, offset);\n+    \/\/             used = blockSize;\n+    \/\/             offset += blockSize;\n+    \/\/             len -= blockSize;\n+    \/\/             if (len == 0)\n+    \/\/                 goto DONE;\n+    \/\/         }\n+    \/\/     }\n+    \/\/     NEXT:\n+    \/\/     out[outOff++] = (byte)(in[inOff++] ^ encryptedCounter[used++]);\n+    \/\/ }\n+    \/\/ DONE:\n+    \/\/ return result;\n+    \/\/\n+    \/\/ CTR_large_block()\n+    \/\/    Wide bulk encryption of whole blocks.\n+\n+    __ align(CodeEntryAlignment);\n+    StubCodeMark mark(this, \"StubRoutines\", \"counterMode_AESCrypt\");\n+    const address start = __ pc();\n+    __ enter();\n+\n+    Label DONE, CTR_large_block, large_block_return;\n+    __ ldrw(used, Address(used_ptr));\n+    __ cbzw(saved_len, DONE);\n+\n+    __ mov(len, saved_len);\n+    __ mov(offset, 0);\n+\n+    \/\/ Compute #rounds for AES based on the length of the key array\n+    __ ldrw(keylen, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));\n+\n+    __ aesenc_loadkeys(key, keylen);\n+\n+    {\n+      Label L_CTR_loop, NEXT;\n+\n+      __ bind(L_CTR_loop);\n+\n+      __ cmp(used, block_size);\n+      __ br(__ LO, NEXT);\n+\n+      \/\/ Maybe we have a lot of data\n+      __ subsw(rscratch1, len, bulk_width * block_size);\n+      __ br(__ HS, CTR_large_block);\n+      __ BIND(large_block_return);\n+      __ cbzw(len, DONE);\n+\n+      \/\/ Setup the counter\n+      __ movi(v4, __ T4S, 0);\n+      __ movi(v5, __ T4S, 1);\n+      __ ins(v4, __ S, v5, 3, 3); \/\/ v4 contains { 0, 0, 0, 1 }\n+\n+      __ ld1(v0, __ T16B, counter); \/\/ Load the counter into v0\n+      __ rev32(v16, __ T16B, v0);\n+      __ addv(v16, __ T4S, v16, v4);\n+      __ rev32(v16, __ T16B, v16);\n+      __ st1(v16, __ T16B, counter); \/\/ Save the incremented counter back\n+\n+      {\n+        \/\/ We have fewer than bulk_width blocks of data left. Encrypt\n+        \/\/ them one by one until there is less than a full block\n+        \/\/ remaining, being careful to save both the encrypted counter\n+        \/\/ and the counter.\n+\n+        Label inner_loop;\n+        __ bind(inner_loop);\n+        \/\/ Counter to encrypt is in v0\n+        __ aesecb_encrypt(noreg, noreg, keylen);\n+        __ st1(v0, __ T16B, saved_encrypted_ctr);\n+\n+        \/\/ Do we have a remaining full block?\n+\n+        __ mov(used, 0);\n+        __ cmp(len, block_size);\n+        __ br(__ LO, NEXT);\n+\n+        \/\/ Yes, we have a full block\n+        __ ldrq(v1, Address(in, offset));\n+        __ eor(v1, __ T16B, v1, v0);\n+        __ strq(v1, Address(out, offset));\n+        __ mov(used, block_size);\n+        __ add(offset, offset, block_size);\n+\n+        __ subw(len, len, block_size);\n+        __ cbzw(len, DONE);\n+\n+        \/\/ Increment the counter, store it back\n+        __ orr(v0, __ T16B, v16, v16);\n+        __ rev32(v16, __ T16B, v16);\n+        __ addv(v16, __ T4S, v16, v4);\n+        __ rev32(v16, __ T16B, v16);\n+        __ st1(v16, __ T16B, counter); \/\/ Save the incremented counter back\n+\n+        __ b(inner_loop);\n+      }\n+\n+      __ BIND(NEXT);\n+\n+      \/\/ Encrypt a single byte, and loop.\n+      \/\/ We expect this to be a rare event.\n+      __ ldrb(rscratch1, Address(in, offset));\n+      __ ldrb(rscratch2, Address(saved_encrypted_ctr, used));\n+      __ eor(rscratch1, rscratch1, rscratch2);\n+      __ strb(rscratch1, Address(out, offset));\n+      __ add(offset, offset, 1);\n+      __ add(used, used, 1);\n+      __ subw(len, len,1);\n+      __ cbnzw(len, L_CTR_loop);\n+    }\n+\n+    __ bind(DONE);\n+    __ strw(used, Address(used_ptr));\n+    __ mov(r0, saved_len);\n+\n+    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+    __ ret(lr);\n+\n+    \/\/ Bulk encryption\n+\n+    __ BIND (CTR_large_block);\n+    assert(bulk_width == 4 || bulk_width == 8, \"must be\");\n+\n+    if (bulk_width == 8) {\n+      __ sub(sp, sp, 4 * 16);\n+      __ st1(v12, v13, v14, v15, __ T16B, Address(sp));\n+    }\n+    __ sub(sp, sp, 4 * 16);\n+    __ st1(v8, v9, v10, v11, __ T16B, Address(sp));\n+    RegSet saved_regs = (RegSet::of(in, out, offset)\n+                         + RegSet::of(saved_encrypted_ctr, used_ptr, len));\n+    __ push(saved_regs, sp);\n+    __ andr(len, len, -16 * bulk_width);  \/\/ 8\/4 encryptions, 16 bytes per encryption\n+    __ add(in, in, offset);\n+    __ add(out, out, offset);\n+\n+    \/\/ Keys should already be loaded into the correct registers\n+\n+    __ ld1(v0, __ T16B, counter); \/\/ v0 contains the first counter\n+    __ rev32(v16, __ T16B, v0); \/\/ v16 contains byte-reversed counter\n+\n+    \/\/ AES\/CTR loop\n+    {\n+      Label L_CTR_loop;\n+      __ BIND(L_CTR_loop);\n+\n+      \/\/ Setup the counters\n+      __ movi(v8, __ T4S, 0);\n+      __ movi(v9, __ T4S, 1);\n+      __ ins(v8, __ S, v9, 3, 3); \/\/ v8 contains { 0, 0, 0, 1 }\n+\n+      for (FloatRegister f = v0; f < v0 + bulk_width; f++) {\n+        __ rev32(f, __ T16B, v16);\n+        __ addv(v16, __ T4S, v16, v8);\n+      }\n+\n+      __ ld1(v8, v9, v10, v11, __ T16B, __ post(in, 4 * 16));\n+\n+      \/\/ Encrypt the counters\n+      __ aesecb_encrypt(noreg, noreg, keylen, v0, bulk_width);\n+\n+      if (bulk_width == 8) {\n+        __ ld1(v12, v13, v14, v15, __ T16B, __ post(in, 4 * 16));\n+      }\n+\n+      \/\/ XOR the encrypted counters with the inputs\n+      for (int i = 0; i < bulk_width; i++) {\n+        __ eor(v0 + i, __ T16B, v0 + i, v8 + i);\n+      }\n+\n+      \/\/ Write the encrypted data\n+      __ st1(v0, v1, v2, v3, __ T16B, __ post(out, 4 * 16));\n+      if (bulk_width == 8) {\n+        __ st1(v4, v5, v6, v7, __ T16B, __ post(out, 4 * 16));\n+      }\n+\n+      __ subw(len, len, 16 * bulk_width);\n+      __ cbnzw(len, L_CTR_loop);\n+    }\n+\n+    \/\/ Save the counter back where it goes\n+    __ rev32(v16, __ T16B, v16);\n+    __ st1(v16, __ T16B, counter);\n+\n+    __ pop(saved_regs, sp);\n+\n+    __ ld1(v8, v9, v10, v11, __ T16B, __ post(sp, 4 * 16));\n+    if (bulk_width == 8) {\n+      __ ld1(v12, v13, v14, v15, __ T16B, __ post(sp, 4 * 16));\n+    }\n+\n+    __ andr(rscratch1, len, -16 * bulk_width);\n+    __ sub(len, len, rscratch1);\n+    __ add(offset, offset, rscratch1);\n+    __ mov(used, 16);\n+    __ strw(used, Address(used_ptr));\n+    __ b(large_block_return);\n+\n+    return start;\n+  }\n+\n+  \/\/ Vector AES Galois Counter Mode implementation. Parameters:\n+  \/\/\n+  \/\/ in = c_rarg0\n+  \/\/ len = c_rarg1\n+  \/\/ ct = c_rarg2 - ciphertext that ghash will read (in for encrypt, out for decrypt)\n+  \/\/ out = c_rarg3\n+  \/\/ key = c_rarg4\n+  \/\/ state = c_rarg5 - GHASH.state\n+  \/\/ subkeyHtbl = c_rarg6 - powers of H\n+  \/\/ counter = c_rarg7 - pointer to 16 bytes of CTR\n+  \/\/ return - number of processed bytes\n+  address generate_galoisCounterMode_AESCrypt() {\n+    address ghash_polynomial = __ pc();\n+    __ emit_int64(0x87);  \/\/ The low-order bits of the field\n+                          \/\/ polynomial (i.e. p = z^7+z^2+z+1)\n+                          \/\/ repeated in the low and high parts of a\n+                          \/\/ 128-bit vector\n+    __ emit_int64(0x87);\n+\n+    __ align(CodeEntryAlignment);\n+     StubCodeMark mark(this, \"StubRoutines\", \"galoisCounterMode_AESCrypt\");\n+    address start = __ pc();\n+    const Register in = c_rarg0;\n+    const Register len = c_rarg1;\n+    const Register ct = c_rarg2;\n+    const Register out = c_rarg3;\n+    \/\/ and updated with the incremented counter in the end\n+\n+    const Register key = c_rarg4;\n+    const Register state = c_rarg5;\n+\n+    const Register subkeyHtbl = c_rarg6;\n+\n+    const Register counter = c_rarg7;\n+\n+    const Register keylen = r10;\n+    __ enter();\n+    \/\/ Save state before entering routine\n+    __ sub(sp, sp, 4 * 16);\n+    __ st1(v12, v13, v14, v15, __ T16B, Address(sp));\n+    __ sub(sp, sp, 4 * 16);\n+    __ st1(v8, v9, v10, v11, __ T16B, Address(sp));\n+\n+    \/\/ __ andr(len, len, -512);\n+    __ andr(len, len, -16 * 8);  \/\/ 8 encryptions, 16 bytes per encryption\n+    __ str(len, __ pre(sp, -2 * wordSize));\n+\n+    Label DONE;\n+    __ cbz(len, DONE);\n+\n+    \/\/ Compute #rounds for AES based on the length of the key array\n+    __ ldrw(keylen, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));\n+\n+    __ aesenc_loadkeys(key, keylen);\n+    __ ld1(v0, __ T16B, counter); \/\/ v0 contains the first counter\n+    __ rev32(v16, __ T16B, v0); \/\/ v16 contains byte-reversed counter\n+\n+    \/\/ AES\/CTR loop\n+    {\n+      Label L_CTR_loop;\n+      __ BIND(L_CTR_loop);\n+\n+      \/\/ Setup the counters\n+      __ movi(v8, __ T4S, 0);\n+      __ movi(v9, __ T4S, 1);\n+      __ ins(v8, __ S, v9, 3, 3); \/\/ v8 contains { 0, 0, 0, 1 }\n+      for (FloatRegister f = v0; f < v8; f++) {\n+        __ rev32(f, __ T16B, v16);\n+        __ addv(v16, __ T4S, v16, v8);\n+      }\n+\n+      __ ld1(v8, v9, v10, v11, __ T16B, __ post(in, 4 * 16));\n+\n+      \/\/ Encrypt the counters\n+      __ aesecb_encrypt(noreg, noreg, keylen, v0, 8);\n+\n+      __ ld1(v12, v13, v14, v15, __ T16B, __ post(in, 4 * 16));\n+\n+      \/\/ XOR the encrypted counters with the inputs\n+      for (int i = 0; i < 8; i++) {\n+        __ eor(v0 + i, __ T16B, v0 + i, v8 + i);\n+      }\n+      __ st1(v0, v1, v2, v3, __ T16B, __ post(out, 4 * 16));\n+      __ st1(v4, v5, v6, v7, __ T16B, __ post(out, 4 * 16));\n+\n+      __ subw(len, len, 16 * 8);\n+      __ cbnzw(len, L_CTR_loop);\n+    }\n+\n+    __ rev32(v16, __ T16B, v16);\n+    __ st1(v16, __ T16B, counter);\n+\n+    __ ldr(len, Address(sp));\n+    __ lsr(len, len, exact_log2(16));  \/\/ We want the count of blocks\n+\n+    \/\/ GHASH\/CTR loop\n+    __ ghash_processBlocks_wide(ghash_polynomial, state, subkeyHtbl, ct, len, 4);\n+\n+#ifdef ASSERT\n+    { Label L;\n+      __ cmp(len, (unsigned char)0);\n+      __ br(Assembler::EQ, L);\n+      __ stop(\"stubGenerator: abort\");\n+      __ bind(L);\n+  }\n+#endif\n+\n+  __ bind(DONE);\n+    \/\/ Return the number of bytes processed\n+    __ ldr(r0, __ post(sp, 2 * wordSize));\n+\n+    __ ld1(v8, v9, v10, v11, __ T16B, __ post(sp, 4 * 16));\n+    __ ld1(v12, v13, v14, v15, __ T16B, __ post(sp, 4 * 16));\n+\n+    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+    __ ret(lr);\n+     return start;\n+  }\n+\n@@ -4230,63 +4461,0 @@\n-  void ghash_multiply(FloatRegister result_lo, FloatRegister result_hi,\n-                      FloatRegister a, FloatRegister b, FloatRegister a1_xor_a0,\n-                      FloatRegister tmp1, FloatRegister tmp2, FloatRegister tmp3, FloatRegister tmp4) {\n-    \/\/ Karatsuba multiplication performs a 128*128 -> 256-bit\n-    \/\/ multiplication in three 128-bit multiplications and a few\n-    \/\/ additions.\n-    \/\/\n-    \/\/ (C1:C0) = A1*B1, (D1:D0) = A0*B0, (E1:E0) = (A0+A1)(B0+B1)\n-    \/\/ (A1:A0)(B1:B0) = C1:(C0+C1+D1+E1):(D1+C0+D0+E0):D0\n-    \/\/\n-    \/\/ Inputs:\n-    \/\/\n-    \/\/ A0 in a.d[0]     (subkey)\n-    \/\/ A1 in a.d[1]\n-    \/\/ (A1+A0) in a1_xor_a0.d[0]\n-    \/\/\n-    \/\/ B0 in b.d[0]     (state)\n-    \/\/ B1 in b.d[1]\n-\n-    __ ext(tmp1, __ T16B, b, b, 0x08);\n-    __ pmull2(result_hi, __ T1Q, b, a, __ T2D);  \/\/ A1*B1\n-    __ eor(tmp1, __ T16B, tmp1, b);            \/\/ (B1+B0)\n-    __ pmull(result_lo,  __ T1Q, b, a, __ T1D);  \/\/ A0*B0\n-    __ pmull(tmp2, __ T1Q, tmp1, a1_xor_a0, __ T1D); \/\/ (A1+A0)(B1+B0)\n-\n-    __ ext(tmp4, __ T16B, result_lo, result_hi, 0x08);\n-    __ eor(tmp3, __ T16B, result_hi, result_lo); \/\/ A1*B1+A0*B0\n-    __ eor(tmp2, __ T16B, tmp2, tmp4);\n-    __ eor(tmp2, __ T16B, tmp2, tmp3);\n-\n-    \/\/ Register pair <result_hi:result_lo> holds the result of carry-less multiplication\n-    __ ins(result_hi, __ D, tmp2, 0, 1);\n-    __ ins(result_lo, __ D, tmp2, 1, 0);\n-  }\n-\n-  void ghash_reduce(FloatRegister result, FloatRegister lo, FloatRegister hi,\n-                    FloatRegister p, FloatRegister z, FloatRegister t1) {\n-    const FloatRegister t0 = result;\n-\n-    \/\/ The GCM field polynomial f is z^128 + p(z), where p =\n-    \/\/ z^7+z^2+z+1.\n-    \/\/\n-    \/\/    z^128 === -p(z)  (mod (z^128 + p(z)))\n-    \/\/\n-    \/\/ so, given that the product we're reducing is\n-    \/\/    a == lo + hi * z^128\n-    \/\/ substituting,\n-    \/\/      === lo - hi * p(z)  (mod (z^128 + p(z)))\n-    \/\/\n-    \/\/ we reduce by multiplying hi by p(z) and subtracting the result\n-    \/\/ from (i.e. XORing it with) lo.  Because p has no nonzero high\n-    \/\/ bits we can do this with two 64-bit multiplications, lo*p and\n-    \/\/ hi*p.\n-\n-    __ pmull2(t0, __ T1Q, hi, p, __ T2D);\n-    __ ext(t1, __ T16B, t0, z, 8);\n-    __ eor(hi, __ T16B, hi, t1);\n-    __ ext(t1, __ T16B, z, t0, 8);\n-    __ eor(lo, __ T16B, lo, t1);\n-    __ pmull(t0, __ T1Q, hi, p, __ T1D);\n-    __ eor(result, __ T16B, lo, t0);\n-  }\n-\n@@ -5390,0 +5558,2 @@\n+    __ ldrq(v24, p);    \/\/ The field polynomial\n+\n@@ -5398,4 +5568,2 @@\n-    __ ldrq(v26, p);\n-\n-    __ ext(v16, __ T16B, v1, v1, 0x08); \/\/ long-swap subkeyH into v1\n-    __ eor(v16, __ T16B, v16, v1);      \/\/ xor subkeyH into subkeyL (Karatsuba: (A1+A0))\n+    __ ext(v4, __ T16B, v1, v1, 0x08); \/\/ long-swap subkeyH into v1\n+    __ eor(v4, __ T16B, v4, v1);       \/\/ xor subkeyH into subkeyL (Karatsuba: (A1+A0))\n@@ -5413,3 +5581,3 @@\n-      ghash_multiply(\/*result_lo*\/v5, \/*result_hi*\/v7,\n-                     \/*a*\/v1, \/*b*\/v2, \/*a1_xor_a0*\/v16,\n-                     \/*temps*\/v6, v20, v18, v21);\n+      __ ghash_multiply(\/*result_lo*\/v5, \/*result_hi*\/v7,\n+                        \/*a*\/v1, \/*b*\/v2, \/*a1_xor_a0*\/v4,\n+                        \/*temps*\/v6, v3, \/*reuse\/clobber b*\/v2);\n@@ -5417,1 +5585,1 @@\n-      ghash_reduce(v0, v5, v7, v26, vzr, v20);\n+      __ ghash_reduce(\/*result*\/v0, \/*lo*\/v5, \/*hi*\/v7, \/*p*\/v24, vzr, \/*temp*\/v3);\n@@ -5424,2 +5592,52 @@\n-    __ rev64(v1, __ T16B, v0);\n-    __ rbit(v1, __ T16B, v1);\n+    __ rev64(v0, __ T16B, v0);\n+    __ rbit(v0, __ T16B, v0);\n+\n+    __ st1(v0, __ T16B, state);\n+    __ ret(lr);\n+\n+    return start;\n+  }\n+\n+  address generate_ghash_processBlocks_wide() {\n+    address small = generate_ghash_processBlocks();\n+\n+    StubCodeMark mark(this, \"StubRoutines\", \"ghash_processBlocks\");\n+    __ align(wordSize * 2);\n+    address p = __ pc();\n+    __ emit_int64(0x87);  \/\/ The low-order bits of the field\n+                          \/\/ polynomial (i.e. p = z^7+z^2+z+1)\n+                          \/\/ repeated in the low and high parts of a\n+                          \/\/ 128-bit vector\n+    __ emit_int64(0x87);\n+\n+    __ align(CodeEntryAlignment);\n+    address start = __ pc();\n+\n+    Register state   = c_rarg0;\n+    Register subkeyH = c_rarg1;\n+    Register data    = c_rarg2;\n+    Register blocks  = c_rarg3;\n+\n+    const int unroll = 4;\n+\n+    __ cmp(blocks, (unsigned char)(unroll * 2));\n+    __ br(__ LT, small);\n+\n+    if (unroll > 1) {\n+    \/\/ Save state before entering routine\n+      __ sub(sp, sp, 4 * 16);\n+      __ st1(v12, v13, v14, v15, __ T16B, Address(sp));\n+      __ sub(sp, sp, 4 * 16);\n+      __ st1(v8, v9, v10, v11, __ T16B, Address(sp));\n+    }\n+\n+    __ ghash_processBlocks_wide(p, state, subkeyH, data, blocks, unroll);\n+\n+    if (unroll > 1) {\n+      \/\/ And restore state\n+      __ ld1(v8, v9, v10, v11, __ T16B, __ post(sp, 4 * 16));\n+      __ ld1(v12, v13, v14, v15, __ T16B, __ post(sp, 4 * 16));\n+    }\n+\n+    __ cmp(blocks, (unsigned char)0);\n+    __ br(__ GT, small);\n@@ -5427,1 +5645,0 @@\n-    __ st1(v1, __ T16B, state);\n@@ -7134,1 +7351,2 @@\n-      StubRoutines::_ghash_processBlocks = generate_ghash_processBlocks();\n+      \/\/ StubRoutines::_ghash_processBlocks = generate_ghash_processBlocks();\n+      StubRoutines::_ghash_processBlocks = generate_ghash_processBlocks_wide();\n@@ -7151,0 +7369,2 @@\n+      StubRoutines::_galoisCounterMode_AESCrypt = generate_galoisCounterMode_AESCrypt();\n+      StubRoutines::_counterMode_AESCrypt = generate_counterMode_AESCrypt();\n","filename":"src\/hotspot\/cpu\/aarch64\/stubGenerator_aarch64.cpp","additions":436,"deletions":216,"binary":false,"changes":652,"status":"modified"},{"patch":"@@ -39,1 +39,1 @@\n-  code_size2 = 28000           \/\/ simply increase if too small (assembler will crash if too small)\n+  code_size2 = 38000           \/\/ simply increase if too small (assembler will crash if too small)\n","filename":"src\/hotspot\/cpu\/aarch64\/stubRoutines_aarch64.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -240,0 +240,3 @@\n+    if (FLAG_IS_DEFAULT(UseAESCTRIntrinsics)) {\n+      FLAG_SET_DEFAULT(UseAESCTRIntrinsics, true);\n+    }\n@@ -249,0 +252,4 @@\n+    if (UseAESCTRIntrinsics) {\n+      warning(\"AES\/CTR intrinsics are not available on this CPU\");\n+      FLAG_SET_DEFAULT(UseAESCTRIntrinsics, false);\n+    }\n@@ -251,4 +258,0 @@\n-  if (UseAESCTRIntrinsics) {\n-    warning(\"AES\/CTR intrinsics are not available on this CPU\");\n-    FLAG_SET_DEFAULT(UseAESCTRIntrinsics, false);\n-  }\n","filename":"src\/hotspot\/cpu\/aarch64\/vm_version_aarch64.cpp","additions":7,"deletions":4,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -91,0 +91,4 @@\n+\n+  \/\/ All Apple-darwin Arm processors have AES.\n+  _features |= CPU_AES;\n+\n","filename":"src\/hotspot\/os_cpu\/bsd_aarch64\/vm_version_bsd_aarch64.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"}]}