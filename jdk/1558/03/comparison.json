{"files":[{"patch":"@@ -1097,1 +1097,1 @@\n-    int granularity = uabs(step);\n+    unsigned int granularity = uabs(step);\n@@ -1100,1 +1100,1 @@\n-    \/\/ <= 96 bytes do inline. Direction doesn't matter because we always\n+    \/\/ <= 80 (or 96 for SIMD) bytes do inline. Direction doesn't matter because we always\n@@ -1157,0 +1157,20 @@\n+      \/\/ Unaligned pointers can be an issue for copying.\n+      \/\/ The issue has more chances to happen when granularity of data is\n+      \/\/ less than 4(sizeof(jint)). Pointers for arrays of jint are at least\n+      \/\/ 4 byte aligned. Pointers for arrays of jlong are 8 byte aligned.\n+      \/\/ The most performance drop has been seen for the range 65-80 bytes.\n+      \/\/ For such cases using the pair of ldp\/stp instead of the third pair of\n+      \/\/ ldpq\/stpq fixes the performance issue.\n+      if (granularity < sizeof (jint)) {\n+        Label copy96;\n+        __ cmp(count, u1(80\/granularity));\n+        __ br(Assembler::HI, copy96);\n+        __ ldp(t0, t1, Address(send, -16));\n+\n+        __ stpq(v0, v1, Address(d, 0));\n+        __ stpq(v2, v3, Address(d, 32));\n+        __ stp(t0, t1, Address(dend, -16));\n+        __ b(finish);\n+\n+        __ bind(copy96);\n+      }\n@@ -1158,0 +1178,1 @@\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/stubGenerator_aarch64.cpp","additions":23,"deletions":2,"binary":false,"changes":25,"status":"modified"}]}