{"files":[{"patch":"@@ -45,0 +45,3 @@\n+extern aarch64_atomic_stub_t aarch64_atomic_cmpxchg_1_relaxed_impl;\n+extern aarch64_atomic_stub_t aarch64_atomic_cmpxchg_4_relaxed_impl;\n+extern aarch64_atomic_stub_t aarch64_atomic_cmpxchg_8_relaxed_impl;\n","filename":"src\/hotspot\/cpu\/aarch64\/atomic_aarch64.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -5577,0 +5577,1 @@\n+\n@@ -5581,1 +5582,0 @@\n-  void generate_atomic_entry_points() {\n@@ -5583,2 +5583,15 @@\n-    if (! UseLSE) {\n-      return;\n+  \/\/ class AtomicStubMark records the entry point of a stub and the\n+  \/\/ stub pointer which will point to it. The stub pointer is set to\n+  \/\/ the entry point when ~AtomicStubMark() is called, which must be\n+  \/\/ after ICache::invalidate_range. This ensures safe publication of\n+  \/\/ the generated code.\n+  class AtomicStubMark {\n+    address _entry_point;\n+    aarch64_atomic_stub_t *_stub;\n+    MacroAssembler *_masm;\n+  public:\n+    AtomicStubMark(MacroAssembler *masm, aarch64_atomic_stub_t *stub) {\n+      _masm = masm;\n+      __ align(32);\n+      _entry_point = __ pc();\n+      _stub = stub;\n@@ -5586,0 +5599,4 @@\n+    ~AtomicStubMark() {\n+      *_stub = (aarch64_atomic_stub_t)_entry_point;\n+    }\n+  };\n@@ -5587,10 +5604,56 @@\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"atomic entry points\");\n-\n-    __ align(32);\n-    aarch64_atomic_fetch_add_8_impl = (aarch64_atomic_stub_t)__ pc();\n-    {\n-      Register prev = r2, addr = c_rarg0, incr = c_rarg1;\n-      __ atomic_addal(prev, incr, addr);\n-      __ mov(r0, prev);\n-      __ ret(lr);\n+  \/\/ NB: For memory_order_conservative we need a trailing membar after\n+  \/\/ LSE atomic operations but not a leading membar.\n+  \/\/\n+  \/\/ We don't need a leading membar because a clause in the Arm ARM\n+  \/\/ says:\n+  \/\/\n+  \/\/   Barrier-ordered-before\n+  \/\/\n+  \/\/   Barrier instructions order prior Memory effects before subsequent\n+  \/\/   Memory effects generated by the same Observer. A read or a write\n+  \/\/   RW1 is Barrier-ordered-before a read or a write RW 2 from the same\n+  \/\/   Observer if and only if RW1 appears in program order before RW 2\n+  \/\/   and [ ... ] at least one of RW 1 and RW 2 is generated by an atomic\n+  \/\/   instruction with both Acquire and Release semantics.\n+  \/\/\n+  \/\/ All the atomic instructions {ldaddal, swapal, casal} have Acquire\n+  \/\/ and Release semantics, therefore we don't need a leading\n+  \/\/ barrier. However, there is no corresponding Barrier-ordered-after\n+  \/\/ relationship, therefore we need a trailing membar to prevent a\n+  \/\/ later store or load from being reordered with the store in an\n+  \/\/ atomic instruction.\n+  \/\/\n+  \/\/ This was checked by using the herd7 consistency model simulator\n+  \/\/ (http:\/\/diy.inria.fr\/) with this test case:\n+  \/\/\n+  \/\/ AArch64 LseCas\n+  \/\/ { 0:X1=x; 0:X2=y; 1:X1=x; 1:X2=y; }\n+  \/\/ P0 | P1;\n+  \/\/ LDR W4, [X2] | MOV W3, #0;\n+  \/\/ DMB LD       | MOV W4, #1;\n+  \/\/ LDR W3, [X1] | CASAL W3, W4, [X1];\n+  \/\/              | DMB ISH;\n+  \/\/              | STR W4, [X2];\n+  \/\/ exists\n+  \/\/ (0:X3=0 \/\\ 0:X4=1)\n+  \/\/\n+  \/\/ If X3 == 0 && X4 == 1, the store to y in P1 has been reordered\n+  \/\/ with the store to x in P1. Without the DMB in P1 this may happen.\n+  \/\/\n+  \/\/ At the time of writing we don't know of any AArch64 hardware that\n+  \/\/ reorders stores in this way, but the Reference Manual permits it.\n+\n+  void gen_cas_entry(Assembler::operand_size size,\n+                     atomic_memory_order order) {\n+    Register prev = r3, ptr = c_rarg0, compare_val = c_rarg1,\n+      exchange_val = c_rarg2;\n+    bool acquire, release;\n+    switch (order) {\n+      case memory_order_relaxed:\n+        acquire = false;\n+        release = false;\n+        break;\n+      default:\n+        acquire = true;\n+        release = true;\n+        break;\n@@ -5598,7 +5661,4 @@\n-    __ align(32);\n-    aarch64_atomic_fetch_add_4_impl = (aarch64_atomic_stub_t)__ pc();\n-    {\n-      Register prev = r2, addr = c_rarg0, incr = c_rarg1;\n-      __ atomic_addalw(prev, incr, addr);\n-      __ movw(r0, prev);\n-      __ ret(lr);\n+    __ mov(prev, compare_val);\n+    __ lse_cas(prev, exchange_val, ptr, size, acquire, release, \/*not_pair*\/true);\n+    if (order == memory_order_conservative) {\n+      __ membar(Assembler::StoreStore|Assembler::StoreLoad);\n@@ -5606,5 +5666,3 @@\n-    __ align(32);\n-    aarch64_atomic_xchg_4_impl = (aarch64_atomic_stub_t)__ pc();\n-    {\n-      Register prev = r2, addr = c_rarg0, newv = c_rarg1;\n-      __ atomic_xchglw(prev, newv, addr);\n+    if (size == Assembler::xword) {\n+      __ mov(r0, prev);\n+    } else {\n@@ -5612,1 +5670,0 @@\n-      __ ret(lr);\n@@ -5614,5 +5671,8 @@\n-    __ align(32);\n-    aarch64_atomic_xchg_8_impl = (aarch64_atomic_stub_t)__ pc();\n-    {\n-      Register prev = r2, addr = c_rarg0, newv = c_rarg1;\n-      __ atomic_xchgl(prev, newv, addr);\n+    __ ret(lr);\n+  }\n+\n+  void gen_ldaddal_entry(Assembler::operand_size size) {\n+    Register prev = r2, addr = c_rarg0, incr = c_rarg1;\n+    __ ldaddal(size, incr, prev, addr);\n+    __ membar(Assembler::StoreStore|Assembler::StoreLoad);\n+    if (size == Assembler::xword) {\n@@ -5620,11 +5680,1 @@\n-      __ ret(lr);\n-    }\n-    __ align(32);\n-    aarch64_atomic_cmpxchg_1_impl = (aarch64_atomic_stub_t)__ pc();\n-    {\n-      Register prev = r3, ptr = c_rarg0, compare_val = c_rarg1,\n-        exchange_val = c_rarg2;\n-      __ cmpxchg(ptr, compare_val, exchange_val,\n-                 MacroAssembler::byte,\n-                 \/*acquire*\/false, \/*release*\/false, \/*weak*\/false,\n-                 prev);\n+    } else {\n@@ -5632,1 +5682,0 @@\n-      __ ret(lr);\n@@ -5634,9 +5683,10 @@\n-    __ align(32);\n-    aarch64_atomic_cmpxchg_4_impl = (aarch64_atomic_stub_t)__ pc();\n-    {\n-      Register prev = r3, ptr = c_rarg0, compare_val = c_rarg1,\n-        exchange_val = c_rarg2;\n-      __ cmpxchg(ptr, compare_val, exchange_val,\n-                 MacroAssembler::word,\n-                 \/*acquire*\/false, \/*release*\/false, \/*weak*\/false,\n-                 prev);\n+    __ ret(lr);\n+  }\n+\n+  void gen_swpal_entry(Assembler::operand_size size) {\n+    Register prev = r2, addr = c_rarg0, incr = c_rarg1;\n+    __ swpal(size, incr, prev, addr);\n+    __ membar(Assembler::StoreStore|Assembler::StoreLoad);\n+    if (size == Assembler::xword) {\n+      __ mov(r0, prev);\n+    } else {\n@@ -5644,1 +5694,0 @@\n-      __ ret(lr);\n@@ -5646,11 +5695,6 @@\n-    __ align(32);\n-    aarch64_atomic_cmpxchg_8_impl = (aarch64_atomic_stub_t)__ pc();\n-    {\n-      Register prev = r3, ptr = c_rarg0, compare_val = c_rarg1,\n-        exchange_val = c_rarg2;\n-      __ cmpxchg(ptr, compare_val, exchange_val,\n-                 MacroAssembler::xword,\n-                 \/*acquire*\/false, \/*release*\/false, \/*weak*\/false,\n-                 prev);\n-      __ mov(r0, prev);\n-      __ ret(lr);\n+    __ ret(lr);\n+  }\n+\n+  void generate_atomic_entry_points() {\n+    if (! UseLSE) {\n+      return;\n@@ -5658,0 +5702,36 @@\n+\n+    __ align(CodeEntryAlignment);\n+    StubCodeMark mark(this, \"StubRoutines\", \"atomic entry points\");\n+    address first_entry = __ pc();\n+\n+    \/\/ All memory_order_conservative\n+    AtomicStubMark mark_fetch_add_4(_masm, &aarch64_atomic_fetch_add_4_impl);\n+    gen_ldaddal_entry(Assembler::word);\n+    AtomicStubMark mark_fetch_add_8(_masm, &aarch64_atomic_fetch_add_8_impl);\n+    gen_ldaddal_entry(Assembler::xword);\n+\n+    AtomicStubMark mark_xchg_4(_masm, &aarch64_atomic_xchg_4_impl);\n+    gen_swpal_entry(Assembler::word);\n+    AtomicStubMark mark_xchg_8_impl(_masm, &aarch64_atomic_xchg_8_impl);\n+    gen_swpal_entry(Assembler::xword);\n+\n+    \/\/ CAS, memory_order_conservative\n+    AtomicStubMark mark_cmpxchg_1(_masm, &aarch64_atomic_cmpxchg_1_impl);\n+    gen_cas_entry(MacroAssembler::byte, memory_order_conservative);\n+    AtomicStubMark mark_cmpxchg_4(_masm, &aarch64_atomic_cmpxchg_4_impl);\n+    gen_cas_entry(MacroAssembler::word, memory_order_conservative);\n+    AtomicStubMark mark_cmpxchg_8(_masm, &aarch64_atomic_cmpxchg_8_impl);\n+    gen_cas_entry(MacroAssembler::xword, memory_order_conservative);\n+\n+    \/\/ CAS, memory_order_relaxed\n+    AtomicStubMark mark_cmpxchg_1_relaxed\n+      (_masm, &aarch64_atomic_cmpxchg_1_relaxed_impl);\n+    gen_cas_entry(MacroAssembler::byte, memory_order_relaxed);\n+    AtomicStubMark mark_cmpxchg_4_relaxed\n+      (_masm, &aarch64_atomic_cmpxchg_4_relaxed_impl);\n+    gen_cas_entry(MacroAssembler::word, memory_order_relaxed);\n+    AtomicStubMark mark_cmpxchg_8_relaxed\n+      (_masm, &aarch64_atomic_cmpxchg_8_relaxed_impl);\n+    gen_cas_entry(MacroAssembler::xword, memory_order_relaxed);\n+\n+    ICache::invalidate_range(first_entry, __ pc() - first_entry);\n@@ -6775,1 +6855,0 @@\n-#if 0  \/\/ JDK-8261660: disabled for now.\n@@ -6777,1 +6856,0 @@\n-#endif\n@@ -6808,2 +6886,2 @@\n-#define DEFAULT_ATOMIC_OP(OPNAME, SIZE)                                 \\\n-  extern \"C\" uint64_t aarch64_atomic_ ## OPNAME ## _ ## SIZE ## _default_impl \\\n+#define DEFAULT_ATOMIC_OP(OPNAME, SIZE, RELAXED)                                \\\n+  extern \"C\" uint64_t aarch64_atomic_ ## OPNAME ## _ ## SIZE ## RELAXED ## _default_impl \\\n@@ -6811,10 +6889,13 @@\n-  aarch64_atomic_stub_t aarch64_atomic_ ## OPNAME ## _ ## SIZE ## _impl \\\n-    = aarch64_atomic_ ## OPNAME ## _ ## SIZE ## _default_impl;\n-\n-DEFAULT_ATOMIC_OP(fetch_add, 4)\n-DEFAULT_ATOMIC_OP(fetch_add, 8)\n-DEFAULT_ATOMIC_OP(xchg, 4)\n-DEFAULT_ATOMIC_OP(xchg, 8)\n-DEFAULT_ATOMIC_OP(cmpxchg, 1)\n-DEFAULT_ATOMIC_OP(cmpxchg, 4)\n-DEFAULT_ATOMIC_OP(cmpxchg, 8)\n+  aarch64_atomic_stub_t aarch64_atomic_ ## OPNAME ## _ ## SIZE ## RELAXED ## _impl \\\n+    = aarch64_atomic_ ## OPNAME ## _ ## SIZE ## RELAXED ## _default_impl;\n+\n+DEFAULT_ATOMIC_OP(fetch_add, 4, )\n+DEFAULT_ATOMIC_OP(fetch_add, 8, )\n+DEFAULT_ATOMIC_OP(xchg, 4, )\n+DEFAULT_ATOMIC_OP(xchg, 8, )\n+DEFAULT_ATOMIC_OP(cmpxchg, 1, )\n+DEFAULT_ATOMIC_OP(cmpxchg, 4, )\n+DEFAULT_ATOMIC_OP(cmpxchg, 8, )\n+DEFAULT_ATOMIC_OP(cmpxchg, 1, _relaxed)\n+DEFAULT_ATOMIC_OP(cmpxchg, 4, _relaxed)\n+DEFAULT_ATOMIC_OP(cmpxchg, 8, _relaxed)\n","filename":"src\/hotspot\/cpu\/aarch64\/stubGenerator_aarch64.cpp","additions":159,"deletions":78,"binary":false,"changes":237,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+        prfm    pstl1strm, [x0]\n@@ -33,0 +34,1 @@\n+        dmb     ish\n@@ -39,0 +41,1 @@\n+        prfm    pstl1strm, [x0]\n@@ -43,0 +46,1 @@\n+        dmb     ish\n@@ -49,0 +53,1 @@\n+        prfm    pstl1strm, [x0]\n@@ -52,0 +57,1 @@\n+        dmb     ish\n@@ -58,0 +64,1 @@\n+        prfm    pstl1strm, [x0]\n@@ -61,0 +68,1 @@\n+        dmb     ish\n@@ -67,0 +75,2 @@\n+        dmb     ish\n+        prfm    pstl1strm, [x0]\n@@ -74,0 +84,1 @@\n+        dmb     ish\n@@ -79,0 +90,2 @@\n+        dmb     ish\n+        prfm    pstl1strm, [x0]\n@@ -85,0 +98,1 @@\n+        dmb     ish\n@@ -90,0 +104,40 @@\n+        dmb     ish\n+        prfm    pstl1strm, [x0]\n+0:      ldxr    x3, [x0]\n+        cmp     x3, x1\n+        b.ne    1f\n+        stxr    w8, x2, [x0]\n+        cbnz    w8, 0b\n+1:      mov     x0, x3\n+        dmb     ish\n+        ret\n+\n+        .globl aarch64_atomic_cmpxchg_1_relaxed_default_impl\n+        .align 5\n+aarch64_atomic_cmpxchg_1_relaxed_default_impl:\n+        prfm    pstl1strm, [x0]\n+0:      ldxrb   w3, [x0]\n+        eor     w8, w3, w1\n+        tst     x8, #0xff\n+        b.ne    1f\n+        stxrb   w8, w2, [x0]\n+        cbnz    w8, 0b\n+1:      mov     w0, w3\n+        ret\n+\n+        .globl aarch64_atomic_cmpxchg_4_relaxed_default_impl\n+        .align 5\n+aarch64_atomic_cmpxchg_4_relaxed_default_impl:\n+        prfm    pstl1strm, [x0]\n+0:      ldxr    w3, [x0]\n+        cmp     w3, w1\n+        b.ne    1f\n+        stxr    w8, w2, [x0]\n+        cbnz    w8, 0b\n+1:      mov     w0, w3\n+        ret\n+\n+        .globl aarch64_atomic_cmpxchg_8_relaxed_default_impl\n+        .align 5\n+aarch64_atomic_cmpxchg_8_relaxed_default_impl:\n+        prfm    pstl1strm, [x0]\n","filename":"src\/hotspot\/os_cpu\/linux_aarch64\/atomic_linux_aarch64.S","additions":54,"deletions":0,"binary":false,"changes":54,"status":"modified"},{"patch":"@@ -92,1 +92,0 @@\n-    FULL_MEM_BARRIER;\n@@ -104,1 +103,0 @@\n-    FULL_MEM_BARRIER;\n@@ -115,1 +113,0 @@\n-  FULL_MEM_BARRIER;\n@@ -125,1 +122,0 @@\n-  FULL_MEM_BARRIER;\n@@ -136,11 +132,6 @@\n-  aarch64_atomic_stub_t stub = aarch64_atomic_cmpxchg_1_impl;\n-  if (order == memory_order_relaxed) {\n-    T old_value = atomic_fastcall(stub, dest,\n-                                  compare_value, exchange_value);\n-    return old_value;\n-  } else {\n-    FULL_MEM_BARRIER;\n-    T old_value = atomic_fastcall(stub, dest,\n-                                  compare_value, exchange_value);\n-    FULL_MEM_BARRIER;\n-    return old_value;\n+  aarch64_atomic_stub_t stub;\n+  switch (order) {\n+  case memory_order_relaxed:\n+    stub = aarch64_atomic_cmpxchg_1_relaxed_impl; break;\n+  default:\n+    stub = aarch64_atomic_cmpxchg_1_impl; break;\n@@ -148,0 +139,2 @@\n+\n+  return atomic_fastcall(stub, dest, compare_value, exchange_value);\n@@ -157,11 +150,6 @@\n-  aarch64_atomic_stub_t stub = aarch64_atomic_cmpxchg_4_impl;\n-  if (order == memory_order_relaxed) {\n-    T old_value = atomic_fastcall(stub, dest,\n-                                  compare_value, exchange_value);\n-    return old_value;\n-  } else {\n-    FULL_MEM_BARRIER;\n-    T old_value = atomic_fastcall(stub, dest,\n-                                  compare_value, exchange_value);\n-    FULL_MEM_BARRIER;\n-    return old_value;\n+  aarch64_atomic_stub_t stub;\n+  switch (order) {\n+  case memory_order_relaxed:\n+    stub = aarch64_atomic_cmpxchg_4_relaxed_impl; break;\n+  default:\n+    stub = aarch64_atomic_cmpxchg_4_impl; break;\n@@ -169,0 +157,2 @@\n+\n+  return atomic_fastcall(stub, dest, compare_value, exchange_value);\n@@ -178,11 +168,6 @@\n-  aarch64_atomic_stub_t stub = aarch64_atomic_cmpxchg_8_impl;\n-  if (order == memory_order_relaxed) {\n-    T old_value = atomic_fastcall(stub, dest,\n-                                  compare_value, exchange_value);\n-    return old_value;\n-  } else {\n-    FULL_MEM_BARRIER;\n-    T old_value = atomic_fastcall(stub, dest,\n-                                  compare_value, exchange_value);\n-    FULL_MEM_BARRIER;\n-    return old_value;\n+  aarch64_atomic_stub_t stub;\n+  switch (order) {\n+  case memory_order_relaxed:\n+    stub = aarch64_atomic_cmpxchg_8_relaxed_impl; break;\n+  default:\n+    stub = aarch64_atomic_cmpxchg_8_impl; break;\n@@ -190,0 +175,2 @@\n+\n+  return atomic_fastcall(stub, dest, compare_value, exchange_value);\n","filename":"src\/hotspot\/os_cpu\/linux_aarch64\/atomic_linux_aarch64.hpp","additions":24,"deletions":37,"binary":false,"changes":61,"status":"modified"}]}