{"files":[{"patch":"@@ -1775,2 +1775,7 @@\n-  ShouldNotReachHere();\n-  return -1;\n+  \/\/ This is implemented using aarch64_enc_java_to_runtime as above.\n+  CodeBlob *cb = CodeCache::find_blob(_entry_point);\n+  if (cb) {\n+    return 1 * NativeInstruction::instruction_size;\n+  } else {\n+    return 6 * NativeInstruction::instruction_size;\n+  }\n@@ -16043,0 +16048,15 @@\n+instruct CallNativeDirect(method meth)\n+%{\n+  match(CallNative);\n+\n+  effect(USE meth);\n+\n+  ins_cost(CALL_COST);\n+\n+  format %{ \"CALL, native $meth\" %}\n+\n+  ins_encode( aarch64_enc_java_to_runtime(meth) );\n+\n+  ins_pipe(pipe_class_call);\n+%}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":22,"deletions":2,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -357,0 +357,5 @@\n+\n+  if (jfa->saved_fp_address()) {\n+    update_map_with_saved_link(map, jfa->saved_fp_address());\n+  }\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/frame_aarch64.cpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -34,0 +34,3 @@\n+  \/\/ (Optional) location of saved FP register, which GCs want to inspect\n+  intptr_t** volatile _saved_fp_address;\n+\n@@ -47,0 +50,1 @@\n+    _saved_fp_address = NULL;\n@@ -65,0 +69,2 @@\n+\n+    _saved_fp_address = src->_saved_fp_address;\n@@ -75,0 +81,2 @@\n+  intptr_t** saved_fp_address(void) const        { return _saved_fp_address; }\n+\n@@ -78,0 +86,1 @@\n+  static ByteSize saved_fp_address_offset()      { return byte_offset_of(JavaFrameAnchor, _saved_fp_address); }\n","filename":"src\/hotspot\/cpu\/aarch64\/javaFrameAnchor_aarch64.hpp","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -320,0 +320,2 @@\n+\n+  str(zr, Address(rthread, JavaThread::saved_fp_address_offset()));\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -3073,0 +3073,53 @@\n+\/\/ ---------------------------------------------------------------\n+\n+class NativeInvokerGenerator : public StubCodeGenerator {\n+  address _call_target;\n+  int _shadow_space_bytes;\n+\n+  const GrowableArray<VMReg>& _input_registers;\n+  const GrowableArray<VMReg>& _output_registers;\n+public:\n+  NativeInvokerGenerator(CodeBuffer* buffer,\n+                         address call_target,\n+                         int shadow_space_bytes,\n+                         const GrowableArray<VMReg>& input_registers,\n+                         const GrowableArray<VMReg>& output_registers)\n+   : StubCodeGenerator(buffer, PrintMethodHandleStubs),\n+     _call_target(call_target),\n+     _shadow_space_bytes(shadow_space_bytes),\n+     _input_registers(input_registers),\n+     _output_registers(output_registers) {}\n+  void generate();\n+\n+  void spill_register(VMReg reg) {\n+    assert(reg->is_reg(), \"must be a register\");\n+    MacroAssembler* masm = _masm;\n+    if (reg->is_Register()) {\n+      __ push(RegSet::of(reg->as_Register()), sp);\n+    } else if (reg->is_FloatRegister()) {\n+      __ strq(reg->as_FloatRegister(), Address(__ pre(sp, 16)));\n+    } else {\n+      ShouldNotReachHere();\n+    }\n+  }\n+\n+  void fill_register(VMReg reg) {\n+    assert(reg->is_reg(), \"must be a register\");\n+    MacroAssembler* masm = _masm;\n+    if (reg->is_Register()) {\n+      __ pop(RegSet::of(reg->as_Register()), sp);\n+    } else if (reg->is_FloatRegister()) {\n+      __ ldrq(reg->as_FloatRegister(), Address(__ post(sp, 16)));\n+    } else {\n+      ShouldNotReachHere();\n+    }\n+  }\n+\n+private:\n+#ifdef ASSERT\n+  bool target_uses_register(VMReg reg) {\n+    return _input_registers.contains(reg) || _output_registers.contains(reg);\n+  }\n+#endif\n+};\n+\n@@ -3074,4 +3127,134 @@\n-                                           int shadow_space_bytes,\n-                                           const GrowableArray<VMReg>& input_registers,\n-                                           const GrowableArray<VMReg>& output_registers) {\n-  return NULL;\n+                                               int shadow_space_bytes,\n+                                               const GrowableArray<VMReg>& input_registers,\n+                                               const GrowableArray<VMReg>& output_registers) {\n+  BufferBlob* _invoke_native_blob =\n+    BufferBlob::create(\"nep_invoker_blob\", MethodHandles::adapter_code_size);\n+  if (_invoke_native_blob == NULL)\n+    return NULL; \/\/ allocation failure\n+\n+  CodeBuffer code(_invoke_native_blob);\n+  NativeInvokerGenerator g(&code, call_target, shadow_space_bytes, input_registers, output_registers);\n+  g.generate();\n+  code.log_section_sizes(\"nep_invoker_blob\");\n+\n+  return _invoke_native_blob;\n+}\n+\n+void NativeInvokerGenerator::generate() {\n+  assert(!(target_uses_register(rscratch1->as_VMReg())\n+           || target_uses_register(rscratch2->as_VMReg())\n+           || target_uses_register(rthread->as_VMReg())),\n+         \"Register conflict\");\n+\n+  MacroAssembler* masm = _masm;\n+\n+  __ set_last_Java_frame(sp, noreg, lr, rscratch1);\n+\n+  __ enter();\n+\n+  \/\/ Store a pointer to the previous R29 saved on the stack as it may\n+  \/\/ contain an oop if PreserveFramePointer is off. This value is\n+  \/\/ retrieved later by frame::sender_for_entry_frame() when the stack\n+  \/\/ is walked.\n+  __ mov(rscratch1, sp);\n+  __ str(rscratch1, Address(rthread, JavaThread::saved_fp_address_offset()));\n+\n+  \/\/ State transition\n+  __ mov(rscratch1, _thread_in_native);\n+  __ strw(rscratch1, Address(rthread, JavaThread::thread_state_offset()));\n+\n+  assert(_shadow_space_bytes == 0, \"not expecting shadow space on AArch64\");\n+\n+  rt_call(masm, _call_target);\n+\n+  assert(_output_registers.length() <= 1\n+         || (_output_registers.length() == 2 && !_output_registers.at(1)->is_valid()),\n+         \"no multi-reg returns\");\n+  bool need_spills = _output_registers.length() != 0;\n+  VMReg ret_reg = need_spills ? _output_registers.at(0) : VMRegImpl::Bad();\n+\n+  __ mov(rscratch1, _thread_in_native_trans);\n+  __ strw(rscratch1, Address(rthread, JavaThread::thread_state_offset()));\n+\n+  \/\/ Force this write out before the read below\n+  __ membar(Assembler::LoadLoad | Assembler::LoadStore |\n+            Assembler::StoreLoad | Assembler::StoreStore);\n+\n+  if (UseSVE > 0) {\n+    \/\/ Make sure that native code does not change SVE vector length.\n+    __ verify_sve_vector_length();\n+  }\n+\n+  Label L_after_safepoint_poll;\n+  Label L_safepoint_poll_slow_path;\n+\n+  __ safepoint_poll(L_safepoint_poll_slow_path, rthread, true \/* at_return *\/, false \/* in_nmethod *\/);\n+\n+  __ ldrw(rscratch1, Address(rthread, JavaThread::suspend_flags_offset()));\n+  __ cbnzw(rscratch1, L_safepoint_poll_slow_path);\n+\n+  __ bind(L_after_safepoint_poll);\n+\n+  \/\/ change thread state\n+  __ mov(rscratch1, _thread_in_Java);\n+  __ lea(rscratch2, Address(rthread, JavaThread::thread_state_offset()));\n+  __ stlrw(rscratch1, rscratch2);\n+\n+  __ block_comment(\"reguard stack check\");\n+  Label L_reguard;\n+  Label L_after_reguard;\n+  __ ldrb(rscratch1, Address(rthread, JavaThread::stack_guard_state_offset()));\n+  __ cmpw(rscratch1, StackOverflow::stack_guard_yellow_reserved_disabled);\n+  __ br(Assembler::EQ, L_reguard);\n+  __ bind(L_after_reguard);\n+\n+  __ reset_last_Java_frame(true);\n+\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(lr);\n+\n+  \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+  __ block_comment(\"{ L_safepoint_poll_slow_path\");\n+  __ bind(L_safepoint_poll_slow_path);\n+\n+  if (need_spills) {\n+    spill_register(ret_reg);\n+  }\n+\n+  __ mov(c_rarg0, rthread);\n+#ifndef PRODUCT\n+  assert(frame::arg_reg_save_area_bytes == 0, \"not expecting frame reg save area\");\n+#endif\n+  __ lea(rscratch1, RuntimeAddress(CAST_FROM_FN_PTR(address, JavaThread::check_special_condition_for_native_trans)));\n+  __ blr(rscratch1);\n+\n+  if (need_spills) {\n+    fill_register(ret_reg);\n+  }\n+\n+  __ b(L_after_safepoint_poll);\n+  __ block_comment(\"} L_safepoint_poll_slow_path\");\n+\n+  \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+  __ block_comment(\"{ L_reguard\");\n+  __ bind(L_reguard);\n+\n+  if (need_spills) {\n+    spill_register(ret_reg);\n+  }\n+\n+  rt_call(masm, CAST_FROM_FN_PTR(address, SharedRuntime::reguard_yellow_pages));\n+\n+  if (need_spills) {\n+    fill_register(ret_reg);\n+  }\n+\n+  __ b(L_after_reguard);\n+\n+  __ block_comment(\"} L_reguard\");\n+\n+  \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+  __ flush();\n","filename":"src\/hotspot\/cpu\/aarch64\/sharedRuntime_aarch64.cpp","additions":187,"deletions":4,"binary":false,"changes":191,"status":"modified"},{"patch":"@@ -42,0 +42,4 @@\n+  static ByteSize saved_fp_address_offset() {\n+    return byte_offset_of(JavaThread, _anchor) + JavaFrameAnchor::saved_fp_address_offset();\n+  }\n+\n","filename":"src\/hotspot\/os_cpu\/linux_aarch64\/thread_linux_aarch64.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -41,0 +41,4 @@\n+  static ByteSize saved_fp_address_offset() {\n+    return byte_offset_of(JavaThread, _anchor) + JavaFrameAnchor::saved_fp_address_offset();\n+  }\n+\n","filename":"src\/hotspot\/os_cpu\/windows_aarch64\/thread_windows_aarch64.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -26,1 +26,1 @@\n- * @requires os.arch==\"amd64\" | os.arch==\"x86_64\"\n+ * @requires os.arch==\"amd64\" | os.arch==\"x86_64\" | os.arch==\"aarch64\"\n","filename":"test\/jdk\/java\/foreign\/TestIntrinsics.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"}]}