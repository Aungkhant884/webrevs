{"files":[{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -75,0 +75,6 @@\n+  \/\/ The heuristics used when UseDynamicNumberOfGCThreads is\n+  \/\/ enabled defaults to using a ZAllocationSpikeTolerance of 1.\n+  if (UseDynamicNumberOfGCThreads && FLAG_IS_DEFAULT(ZAllocationSpikeTolerance)) {\n+    FLAG_SET_DEFAULT(ZAllocationSpikeTolerance, 1);\n+  }\n+\n","filename":"src\/hotspot\/share\/gc\/z\/zArguments.cpp","additions":7,"deletions":1,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -53,1 +53,0 @@\n-    _director(new ZDirector()),\n@@ -55,0 +54,1 @@\n+    _director(new ZDirector(_driver)),\n","filename":"src\/hotspot\/share\/gc\/z\/zCollectedHeap.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -47,1 +47,0 @@\n-  ZDirector*        _director;\n@@ -49,0 +48,1 @@\n+  ZDirector*        _director;\n","filename":"src\/hotspot\/share\/gc\/z\/zCollectedHeap.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -26,1 +26,0 @@\n-#include \"gc\/z\/zCollectedHeap.hpp\"\n@@ -28,0 +27,1 @@\n+#include \"gc\/z\/zDriver.hpp\"\n@@ -33,1 +33,2 @@\n-const double ZDirector::one_in_1000 = 3.290527;\n+constexpr double one_in_1000 = 3.290527;\n+constexpr double sample_interval = 1.0 \/ ZStatAllocRate::sample_hz;\n@@ -35,2 +36,2 @@\n-ZDirector::ZDirector() :\n-    _relocation_headroom(ZHeuristics::relocation_headroom()),\n+ZDirector::ZDirector(ZDriver* driver) :\n+    _driver(driver),\n@@ -42,1 +43,1 @@\n-void ZDirector::sample_allocation_rate() const {\n+static void sample_allocation_rate() {\n@@ -47,1 +48,1 @@\n-  log_debug(gc, alloc)(\"Allocation Rate: %.3fMB\/s, Avg: %.3f(+\/-%.3f)MB\/s\",\n+  log_debug(gc, alloc)(\"Allocation Rate: %.1fMB\/s, Predicted: %.1fMB\/s, Avg: %.1f(+\/-%.1f)MB\/s\",\n@@ -49,0 +50,1 @@\n+                       ZStatAllocRate::predict() \/ M,\n@@ -50,1 +52,1 @@\n-                       ZStatAllocRate::avg_sd() \/ M);\n+                       ZStatAllocRate::sd() \/ M);\n@@ -53,4 +55,5 @@\n-bool ZDirector::rule_timer() const {\n-  if (ZCollectionInterval <= 0) {\n-    \/\/ Rule disabled\n-    return false;\n+static ZDriverRequest rule_allocation_stall() {\n+  \/\/ Perform GC if we've observed at least one allocation stall since\n+  \/\/ the last GC started.\n+  if (!ZHeap::heap()->has_alloc_stalled()) {\n+    return GCCause::_no_gc;\n@@ -59,6 +62,1 @@\n-  \/\/ Perform GC if timer has expired.\n-  const double time_since_last_gc = ZStatCycle::time_since_last();\n-  const double time_until_gc = ZCollectionInterval - time_since_last_gc;\n-\n-  log_debug(gc, director)(\"Rule: Timer, Interval: %.3fs, TimeUntilGC: %.3fs\",\n-                          ZCollectionInterval, time_until_gc);\n+  log_debug(gc, director)(\"Rule: Allocation Stall Observed\");\n@@ -66,1 +64,1 @@\n-  return time_until_gc <= 0;\n+  return GCCause::_z_allocation_stall;\n@@ -69,1 +67,1 @@\n-bool ZDirector::rule_warmup() const {\n+static ZDriverRequest rule_warmup() {\n@@ -72,1 +70,1 @@\n-    return false;\n+    return GCCause::_no_gc;\n@@ -86,1 +84,5 @@\n-  return used >= used_threshold;\n+  if (used < used_threshold) {\n+    return GCCause::_no_gc;\n+  }\n+\n+  return GCCause::_z_warmup;\n@@ -89,2 +91,2 @@\n-bool ZDirector::rule_allocation_rate() const {\n-  if (!ZStatCycle::is_normalized_duration_trustable()) {\n+static ZDriverRequest rule_timer() {\n+  if (ZCollectionInterval <= 0) {\n@@ -92,1 +94,148 @@\n-    return false;\n+    return GCCause::_no_gc;\n+  }\n+\n+  \/\/ Perform GC if timer has expired.\n+  const double time_since_last_gc = ZStatCycle::time_since_last();\n+  const double time_until_gc = ZCollectionInterval - time_since_last_gc;\n+\n+  log_debug(gc, director)(\"Rule: Timer, Interval: %.3fs, TimeUntilGC: %.3fs\",\n+                          ZCollectionInterval, time_until_gc);\n+\n+  if (time_until_gc > 0) {\n+    return GCCause::_no_gc;\n+  }\n+\n+  return GCCause::_z_timer;\n+}\n+\n+static double estimated_gc_workers(double serial_gc_time, double parallelizable_gc_time, double time_until_deadline) {\n+  const double parallelizable_time_until_deadline = MAX2(time_until_deadline - serial_gc_time, 0.001);\n+  return parallelizable_gc_time \/ parallelizable_time_until_deadline;\n+}\n+\n+static uint discrete_gc_workers(double gc_workers) {\n+  return clamp<uint>(ceil(gc_workers), 1, ConcGCThreads);\n+}\n+\n+static double select_gc_workers(double serial_gc_time, double parallelizable_gc_time, double alloc_rate_sd_percent, double time_until_oom) {\n+  \/\/ Use all workers until we're warm\n+  if (!ZStatCycle::is_warm()) {\n+    const double not_warm_gc_workers = ConcGCThreads;\n+    log_debug(gc, director)(\"Select GC Workers (Not Warm), GCWorkers: %.3f\", not_warm_gc_workers);\n+    return not_warm_gc_workers;\n+  }\n+\n+  \/\/ Calculate number of GC workers needed to avoid a long GC cycle and to avoid OOM.\n+  const double avoid_long_gc_workers = estimated_gc_workers(serial_gc_time, parallelizable_gc_time, 10 \/* seconds *\/);\n+  const double avoid_oom_gc_workers = estimated_gc_workers(serial_gc_time, parallelizable_gc_time, time_until_oom);\n+\n+  const double gc_workers = MAX2(avoid_long_gc_workers, avoid_oom_gc_workers);\n+  const uint actual_gc_workers = discrete_gc_workers(gc_workers);\n+  const uint last_gc_workers = ZStatCycle::last_active_workers();\n+\n+  \/\/ More than 15% division from the average is considered unsteady\n+  if (alloc_rate_sd_percent >= 0.15) {\n+    const double half_gc_workers = ConcGCThreads \/ 2.0;\n+    const double unsteady_gc_workers = MAX3<double>(gc_workers, last_gc_workers, half_gc_workers);\n+    log_debug(gc, director)(\"Select GC Workers (Unsteady), \"\n+                            \"AvoidLongGCWorkers: %.3f, AvoidOOMGCWorkers: %.3f, LastGCWorkers: %.3f, HalfGCWorkers: %.3f, GCWorkers: %.3f\",\n+                            avoid_long_gc_workers, avoid_oom_gc_workers, (double)last_gc_workers, half_gc_workers, unsteady_gc_workers);\n+    return unsteady_gc_workers;\n+  }\n+\n+  if (actual_gc_workers < last_gc_workers) {\n+    \/\/ Before decreasing number of GC workers compared to the previous GC cycle, check if the\n+    \/\/ next GC cycle will need to increase it again. If so, use the same number of GC workers\n+    \/\/ that will be needed in the next cycle.\n+    const double gc_duration_delta = (parallelizable_gc_time \/ actual_gc_workers) - (parallelizable_gc_time \/ last_gc_workers);\n+    const double additional_time_for_allocations = ZStatCycle::time_since_last() - gc_duration_delta - sample_interval;\n+    const double next_time_until_oom = time_until_oom + additional_time_for_allocations;\n+    const double next_avoid_oom_gc_workers = estimated_gc_workers(serial_gc_time, parallelizable_gc_time, next_time_until_oom);\n+\n+    \/\/ Add 0.5 to increase friction and avoid lowering too eagerly\n+    const double next_gc_workers = next_avoid_oom_gc_workers + 0.5;\n+    const double try_lowering_gc_workers = clamp<double>(next_gc_workers, actual_gc_workers, last_gc_workers);\n+\n+    log_debug(gc, director)(\"Select GC Workers (Try Lowering), \"\n+                           \"AvoidLongGCWorkers: %.3f, AvoidOOMGCWorkers: %.3f, NextAvoidOOMGCWorkers: %.3f, LastGCWorkers: %.3f, GCWorkers: %.3f\",\n+                            avoid_long_gc_workers, avoid_oom_gc_workers, next_avoid_oom_gc_workers, (double)last_gc_workers, try_lowering_gc_workers);\n+    return try_lowering_gc_workers;\n+  }\n+\n+  log_debug(gc, director)(\"Select GC Workers (Normal), \"\n+                         \"AvoidLongGCWorkers: %.3f, AvoidOOMGCWorkers: %.3f, LastGCWorkers: %.3f, GCWorkers: %.3f\",\n+                         avoid_long_gc_workers, avoid_oom_gc_workers, (double)last_gc_workers, gc_workers);\n+  return gc_workers;\n+}\n+\n+ZDriverRequest rule_allocation_rate_dynamic() {\n+  if (!ZStatCycle::is_time_trustable()) {\n+    \/\/ Rule disabled\n+    return GCCause::_no_gc;\n+  }\n+\n+  \/\/ Calculate amount of free memory available. Note that we take the\n+  \/\/ relocation headroom into account to avoid in-place relocation.\n+  const size_t soft_max_capacity = ZHeap::heap()->soft_max_capacity();\n+  const size_t used = ZHeap::heap()->used();\n+  const size_t free_including_headroom = soft_max_capacity - MIN2(soft_max_capacity, used);\n+  const size_t free = free_including_headroom - MIN2(free_including_headroom, ZHeuristics::relocation_headroom());\n+\n+  \/\/ Calculate time until OOM given the max allocation rate and the amount\n+  \/\/ of free memory. The allocation rate is a moving average and we multiply\n+  \/\/ that with an allocation spike tolerance factor to guard against unforeseen\n+  \/\/ phase changes in the allocate rate. We then add ~3.3 sigma to account for\n+  \/\/ the allocation rate variance, which means the probability is 1 in 1000\n+  \/\/ that a sample is outside of the confidence interval.\n+  const double alloc_rate_predict = ZStatAllocRate::predict();\n+  const double alloc_rate_avg = ZStatAllocRate::avg();\n+  const double alloc_rate_sd = ZStatAllocRate::sd();\n+  const double alloc_rate_sd_percent = alloc_rate_sd \/ (alloc_rate_avg + 1.0);\n+  const double alloc_rate = (MAX2(alloc_rate_predict, alloc_rate_avg) * ZAllocationSpikeTolerance) + (alloc_rate_sd * one_in_1000) + 1.0;\n+  const double time_until_oom = (free \/ alloc_rate) \/ (1.0 + alloc_rate_sd_percent);\n+\n+  \/\/ Calculate max serial\/parallel times of a GC cycle. The times are\n+  \/\/ moving averages, we add ~3.3 sigma to account for the variance.\n+  const double serial_gc_time = ZStatCycle::serial_time().davg() + (ZStatCycle::serial_time().dsd() * one_in_1000);\n+  const double parallelizable_gc_time = ZStatCycle::parallelizable_time().davg() + (ZStatCycle::parallelizable_time().dsd() * one_in_1000);\n+\n+  \/\/ Calculate number of GC workers needed to avoid OOM.\n+  const double gc_workers = select_gc_workers(serial_gc_time, parallelizable_gc_time, alloc_rate_sd_percent, time_until_oom);\n+\n+  \/\/ Convert to a discrete number of GC workers within limits.\n+  const uint actual_gc_workers = discrete_gc_workers(gc_workers);\n+\n+  \/\/ Calculate GC duration given number of GC workers needed.\n+  const double actual_gc_duration = serial_gc_time + (parallelizable_gc_time \/ actual_gc_workers);\n+  const uint last_gc_workers = ZStatCycle::last_active_workers();\n+\n+  \/\/ Calculate time until GC given the time until OOM and GC duration.\n+  \/\/ We also subtract the sample interval, so that we don't overshoot the\n+  \/\/ target time and end up starting the GC too late in the next interval.\n+  const double more_safety_for_fewer_workers = (ConcGCThreads - actual_gc_workers) * sample_interval;\n+  const double time_until_gc = time_until_oom - actual_gc_duration - sample_interval - more_safety_for_fewer_workers;\n+\n+  log_debug(gc, director)(\"Rule: Allocation Rate (Dynamic GC Workers), \"\n+                          \"MaxAllocRate: %.1fMB\/s (+\/-%.1f%%), Free: \" SIZE_FORMAT \"MB, GCCPUTime: %.3f, \"\n+                          \"GCDuration: %.3fs, TimeUntilOOM: %.3fs, TimeUntilGC: %.3fs, GCWorkers: %u -> %u\",\n+                          alloc_rate \/ M,\n+                          alloc_rate_sd_percent * 100,\n+                          free \/ M,\n+                          serial_gc_time + parallelizable_gc_time,\n+                          serial_gc_time + (parallelizable_gc_time \/ actual_gc_workers),\n+                          time_until_oom,\n+                          time_until_gc,\n+                          last_gc_workers,\n+                          actual_gc_workers);\n+\n+  if (actual_gc_workers <= last_gc_workers && time_until_gc > 0) {\n+    return ZDriverRequest(GCCause::_no_gc, actual_gc_workers);\n+  }\n+\n+  return ZDriverRequest(GCCause::_z_allocation_rate, actual_gc_workers);\n+}\n+\n+static ZDriverRequest rule_allocation_rate_static() {\n+  if (!ZStatCycle::is_time_trustable()) {\n+    \/\/ Rule disabled\n+    return GCCause::_no_gc;\n@@ -106,1 +255,1 @@\n-  const size_t free = free_including_headroom - MIN2(free_including_headroom, _relocation_headroom);\n+  const size_t free = free_including_headroom - MIN2(free_including_headroom, ZHeuristics::relocation_headroom());\n@@ -114,1 +263,1 @@\n-  const double max_alloc_rate = (ZStatAllocRate::avg() * ZAllocationSpikeTolerance) + (ZStatAllocRate::avg_sd() * one_in_1000);\n+  const double max_alloc_rate = (ZStatAllocRate::avg() * ZAllocationSpikeTolerance) + (ZStatAllocRate::sd() * one_in_1000);\n@@ -117,4 +266,7 @@\n-  \/\/ Calculate max duration of a GC cycle. The duration of GC is a moving\n-  \/\/ average, we add ~3.3 sigma to account for the GC duration variance.\n-  const AbsSeq& duration_of_gc = ZStatCycle::normalized_duration();\n-  const double max_duration_of_gc = duration_of_gc.davg() + (duration_of_gc.dsd() * one_in_1000);\n+  \/\/ Calculate max serial\/parallel times of a GC cycle. The times are\n+  \/\/ moving averages, we add ~3.3 sigma to account for the variance.\n+  const double serial_gc_time = ZStatCycle::serial_time().davg() + (ZStatCycle::serial_time().dsd() * one_in_1000);\n+  const double parallelizable_gc_time = ZStatCycle::parallelizable_time().davg() + (ZStatCycle::parallelizable_time().dsd() * one_in_1000);\n+\n+  \/\/ Calculate GC duration given number of GC workers needed.\n+  const double gc_duration = serial_gc_time + (parallelizable_gc_time \/ ConcGCThreads);\n@@ -125,2 +277,11 @@\n-  const double sample_interval = 1.0 \/ ZStatAllocRate::sample_hz;\n-  const double time_until_gc = time_until_oom - max_duration_of_gc - sample_interval;\n+  const double time_until_gc = time_until_oom - gc_duration - sample_interval;\n+\n+  log_debug(gc, director)(\"Rule: Allocation Rate (Static GC Workers), MaxAllocRate: %.1fMB\/s, Free: \" SIZE_FORMAT \"MB, GCDuration: %.3fs, TimeUntilGC: %.3fs\",\n+                          max_alloc_rate \/ M, free \/ M, gc_duration, time_until_gc);\n+\n+  if (time_until_gc > 0) {\n+    return GCCause::_no_gc;\n+  }\n+\n+  return GCCause::_z_allocation_rate;\n+}\n@@ -128,2 +289,29 @@\n-  log_debug(gc, director)(\"Rule: Allocation Rate, MaxAllocRate: %.3fMB\/s, Free: \" SIZE_FORMAT \"MB, MaxDurationOfGC: %.3fs, TimeUntilGC: %.3fs\",\n-                          max_alloc_rate \/ M, free \/ M, max_duration_of_gc, time_until_gc);\n+static ZDriverRequest rule_allocation_rate() {\n+  if (UseDynamicNumberOfGCThreads) {\n+    return rule_allocation_rate_dynamic();\n+  } else {\n+    return rule_allocation_rate_static();\n+  }\n+}\n+\n+static ZDriverRequest rule_high_usage() {\n+  \/\/ Perform GC if the amount of free memory is 5% or less. This is a preventive\n+  \/\/ meassure in the case where the application has a very low allocation rate,\n+  \/\/ such that the allocation rate rule doesn't trigger, but the amount of free\n+  \/\/ memory is still slowly but surely heading towards zero. In this situation,\n+  \/\/ we start a GC cycle to avoid a potential allocation stall later.\n+\n+  \/\/ Calculate amount of free memory available. Note that we take the\n+  \/\/ relocation headroom into account to avoid in-place relocation.\n+  const size_t soft_max_capacity = ZHeap::heap()->soft_max_capacity();\n+  const size_t used = ZHeap::heap()->used();\n+  const size_t free_including_headroom = soft_max_capacity - MIN2(soft_max_capacity, used);\n+  const size_t free = free_including_headroom - MIN2(free_including_headroom, ZHeuristics::relocation_headroom());\n+  const double free_percent = percent_of(free, soft_max_capacity);\n+\n+  log_debug(gc, director)(\"Rule: High Usage, Free: \" SIZE_FORMAT \"MB(%.1f%%)\",\n+                          free \/ M, free_percent);\n+\n+  if (free_percent > 5.0) {\n+    return GCCause::_no_gc;\n+  }\n@@ -131,1 +319,1 @@\n-  return time_until_gc <= 0;\n+  return GCCause::_z_high_usage;\n@@ -134,1 +322,1 @@\n-bool ZDirector::rule_proactive() const {\n+static ZDriverRequest rule_proactive() {\n@@ -137,1 +325,1 @@\n-    return false;\n+    return GCCause::_no_gc;\n@@ -160,1 +348,1 @@\n-    return false;\n+    return GCCause::_no_gc;\n@@ -165,3 +353,4 @@\n-  const AbsSeq& duration_of_gc = ZStatCycle::normalized_duration();\n-  const double max_duration_of_gc = duration_of_gc.davg() + (duration_of_gc.dsd() * one_in_1000);\n-  const double acceptable_gc_interval = max_duration_of_gc * ((assumed_throughput_drop_during_gc \/ acceptable_throughput_drop) - 1.0);\n+  const double serial_gc_time = ZStatCycle::serial_time().davg() + (ZStatCycle::serial_time().dsd() * one_in_1000);\n+  const double parallelizable_gc_time = ZStatCycle::parallelizable_time().davg() + (ZStatCycle::parallelizable_time().dsd() * one_in_1000);\n+  const double gc_duration = serial_gc_time + (parallelizable_gc_time \/ ConcGCThreads);\n+  const double acceptable_gc_interval = gc_duration * ((assumed_throughput_drop_during_gc \/ acceptable_throughput_drop) - 1.0);\n@@ -173,28 +362,2 @@\n-  return time_until_gc <= 0;\n-}\n-\n-bool ZDirector::rule_high_usage() const {\n-  \/\/ Perform GC if the amount of free memory is 5% or less. This is a preventive\n-  \/\/ meassure in the case where the application has a very low allocation rate,\n-  \/\/ such that the allocation rate rule doesn't trigger, but the amount of free\n-  \/\/ memory is still slowly but surely heading towards zero. In this situation,\n-  \/\/ we start a GC cycle to avoid a potential allocation stall later.\n-\n-  \/\/ Calculate amount of free memory available. Note that we take the\n-  \/\/ relocation headroom into account to avoid in-place relocation.\n-  const size_t soft_max_capacity = ZHeap::heap()->soft_max_capacity();\n-  const size_t used = ZHeap::heap()->used();\n-  const size_t free_including_headroom = soft_max_capacity - MIN2(soft_max_capacity, used);\n-  const size_t free = free_including_headroom - MIN2(free_including_headroom, _relocation_headroom);\n-  const double free_percent = percent_of(free, soft_max_capacity);\n-\n-  log_debug(gc, director)(\"Rule: High Usage, Free: \" SIZE_FORMAT \"MB(%.1f%%)\",\n-                          free \/ M, free_percent);\n-\n-  return free_percent <= 5.0;\n-}\n-\n-GCCause::Cause ZDirector::make_gc_decision() const {\n-  \/\/ Rule 0: Timer\n-  if (rule_timer()) {\n-    return GCCause::_z_timer;\n+  if (time_until_gc > 0) {\n+    return GCCause::_no_gc;\n@@ -203,14 +366,2 @@\n-  \/\/ Rule 1: Warmup\n-  if (rule_warmup()) {\n-    return GCCause::_z_warmup;\n-  }\n-\n-  \/\/ Rule 2: Allocation rate\n-  if (rule_allocation_rate()) {\n-    return GCCause::_z_allocation_rate;\n-  }\n-\n-  \/\/ Rule 3: Proactive\n-  if (rule_proactive()) {\n-    return GCCause::_z_proactive;\n-  }\n+  return GCCause::_z_proactive;\n+}\n@@ -218,3 +369,18 @@\n-  \/\/ Rule 4: High usage\n-  if (rule_high_usage()) {\n-    return GCCause::_z_high_usage;\n+static ZDriverRequest make_gc_decision() {\n+  \/\/ List of rules\n+  using ZDirectorRule = ZDriverRequest (*)();\n+  const ZDirectorRule rules[] = {\n+    rule_allocation_stall,\n+    rule_warmup,\n+    rule_timer,\n+    rule_allocation_rate,\n+    rule_high_usage,\n+    rule_proactive,\n+  };\n+\n+  \/\/ Execute rules\n+  for (size_t i = 0; i < ARRAY_SIZE(rules); i++) {\n+    const ZDriverRequest request = rules[i]();\n+    if (request.cause() != GCCause::_no_gc) {\n+      return request;\n+    }\n@@ -223,1 +389,0 @@\n-  \/\/ No GC\n@@ -231,3 +396,5 @@\n-    const GCCause::Cause cause = make_gc_decision();\n-    if (cause != GCCause::_no_gc) {\n-      ZCollectedHeap::heap()->collect(cause);\n+    if (!_driver->is_busy()) {\n+      const ZDriverRequest request = make_gc_decision();\n+      if (request.cause() != GCCause::_no_gc) {\n+        _driver->collect(request);\n+      }\n","filename":"src\/hotspot\/share\/gc\/z\/zDirector.cpp","additions":257,"deletions":90,"binary":false,"changes":347,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,1 +28,0 @@\n-#include \"gc\/shared\/gcCause.hpp\"\n@@ -31,0 +30,2 @@\n+class ZDriver;\n+\n@@ -33,13 +34,2 @@\n-  static const double one_in_1000;\n-\n-  const size_t _relocation_headroom;\n-  ZMetronome   _metronome;\n-\n-  void sample_allocation_rate() const;\n-\n-  bool rule_timer() const;\n-  bool rule_warmup() const;\n-  bool rule_allocation_rate() const;\n-  bool rule_proactive() const;\n-  bool rule_high_usage() const;\n-  GCCause::Cause make_gc_decision() const;\n+  ZDriver* const _driver;\n+  ZMetronome     _metronome;\n@@ -52,1 +42,1 @@\n-  ZDirector();\n+  ZDirector(ZDriver* driver);\n","filename":"src\/hotspot\/share\/gc\/z\/zDirector.hpp","additions":6,"deletions":16,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -57,0 +57,22 @@\n+ZDriverRequest::ZDriverRequest() :\n+    ZDriverRequest(GCCause::_no_gc) {}\n+\n+ZDriverRequest::ZDriverRequest(GCCause::Cause cause) :\n+    ZDriverRequest(cause, ConcGCThreads) {}\n+\n+ZDriverRequest::ZDriverRequest(GCCause::Cause cause, uint nworkers) :\n+    _cause(cause),\n+    _nworkers(nworkers) {}\n+\n+bool ZDriverRequest::operator==(const ZDriverRequest& other) const {\n+  return _cause == other._cause;\n+}\n+\n+GCCause::Cause ZDriverRequest::cause() const {\n+  return _cause;\n+}\n+\n+uint ZDriverRequest::nworkers() const {\n+  return _nworkers;\n+}\n+\n@@ -121,41 +143,0 @@\n-static bool should_clear_soft_references() {\n-  \/\/ Clear if one or more allocations have stalled\n-  const bool stalled = ZHeap::heap()->is_alloc_stalled();\n-  if (stalled) {\n-    \/\/ Clear\n-    return true;\n-  }\n-\n-  \/\/ Clear if implied by the GC cause\n-  const GCCause::Cause cause = ZCollectedHeap::heap()->gc_cause();\n-  if (cause == GCCause::_wb_full_gc ||\n-      cause == GCCause::_metadata_GC_clear_soft_refs) {\n-    \/\/ Clear\n-    return true;\n-  }\n-\n-  \/\/ Don't clear\n-  return false;\n-}\n-\n-static bool should_boost_worker_threads() {\n-  \/\/ Boost worker threads if one or more allocations have stalled\n-  const bool stalled = ZHeap::heap()->is_alloc_stalled();\n-  if (stalled) {\n-    \/\/ Boost\n-    return true;\n-  }\n-\n-  \/\/ Boost worker threads if implied by the GC cause\n-  const GCCause::Cause cause = ZCollectedHeap::heap()->gc_cause();\n-  if (cause == GCCause::_wb_full_gc ||\n-      cause == GCCause::_java_lang_system_gc ||\n-      cause == GCCause::_metadata_GC_clear_soft_refs) {\n-    \/\/ Boost\n-    return true;\n-  }\n-\n-  \/\/ Don't boost\n-  return false;\n-}\n-\n@@ -176,8 +157,0 @@\n-    \/\/ Set up soft reference policy\n-    const bool clear = should_clear_soft_references();\n-    ZHeap::heap()->set_soft_reference_policy(clear);\n-\n-    \/\/ Set up boost mode\n-    const bool boost = should_boost_worker_threads();\n-    ZHeap::heap()->set_boost_worker_threads(boost);\n-\n@@ -244,2 +217,6 @@\n-void ZDriver::collect(GCCause::Cause cause) {\n-  switch (cause) {\n+bool ZDriver::is_busy() const {\n+  return _gc_cycle_port.is_busy();\n+}\n+\n+void ZDriver::collect(const ZDriverRequest& request) {\n+  switch (request.cause()) {\n@@ -256,1 +233,1 @@\n-    _gc_cycle_port.send_sync(cause);\n+    _gc_cycle_port.send_sync(request);\n@@ -267,1 +244,1 @@\n-    _gc_cycle_port.send_async(cause);\n+    _gc_cycle_port.send_async(request);\n@@ -277,1 +254,1 @@\n-    _gc_cycle_port.send_async(cause);\n+    _gc_cycle_port.send_async(request);\n@@ -282,1 +259,1 @@\n-    fatal(\"Unsupported GC cause (%s)\", GCCause::to_string(cause));\n+    fatal(\"Unsupported GC cause (%s)\", GCCause::to_string(request.cause()));\n@@ -372,0 +349,44 @@\n+static bool should_clear_soft_references(const ZDriverRequest& request) {\n+  \/\/ Clear soft references if implied by the GC cause\n+  if (request.cause() == GCCause::_wb_full_gc ||\n+      request.cause() == GCCause::_metadata_GC_clear_soft_refs ||\n+      request.cause() == GCCause::_z_allocation_stall) {\n+    \/\/ Clear\n+    return true;\n+  }\n+\n+  \/\/ Don't clear\n+  return false;\n+}\n+\n+static uint select_active_worker_threads_dynamic(const ZDriverRequest& request) {\n+  \/\/ Use requested number of worker threads\n+  return request.nworkers();\n+}\n+\n+static uint select_active_worker_threads_static(const ZDriverRequest& request) {\n+  const GCCause::Cause cause = request.cause();\n+  const uint nworkers = request.nworkers();\n+\n+  \/\/ Boost number of worker threads if implied by the GC cause\n+  if (cause == GCCause::_wb_full_gc ||\n+      cause == GCCause::_java_lang_system_gc ||\n+      cause == GCCause::_metadata_GC_clear_soft_refs ||\n+      cause == GCCause::_z_allocation_stall) {\n+    \/\/ Boost\n+    const uint boosted_nworkers = MAX2(nworkers, ParallelGCThreads);\n+    return boosted_nworkers;\n+  }\n+\n+  \/\/ Use requested number of worker threads\n+  return nworkers;\n+}\n+\n+static uint select_active_worker_threads(const ZDriverRequest& request) {\n+  if (UseDynamicNumberOfGCThreads) {\n+    return select_active_worker_threads_dynamic(request);\n+  } else {\n+    return select_active_worker_threads_static(request);\n+  }\n+}\n+\n@@ -381,1 +402,1 @@\n-  ZDriverGCScope(GCCause::Cause cause) :\n+  ZDriverGCScope(const ZDriverRequest& request) :\n@@ -383,2 +404,2 @@\n-      _gc_cause(cause),\n-      _gc_cause_setter(ZCollectedHeap::heap(), cause),\n+      _gc_cause(request.cause()),\n+      _gc_cause_setter(ZCollectedHeap::heap(), _gc_cause),\n@@ -389,0 +410,8 @@\n+\n+    \/\/ Set up soft reference policy\n+    const bool clear = should_clear_soft_references(request);\n+    ZHeap::heap()->set_soft_reference_policy(clear);\n+\n+    \/\/ Select number of worker threads to use\n+    const uint nworkers = select_active_worker_threads(request);\n+    ZHeap::heap()->set_active_workers(nworkers);\n@@ -392,4 +421,0 @@\n-    \/\/ Calculate boost factor\n-    const double boost_factor = (double)ZHeap::heap()->nconcurrent_worker_threads() \/\n-                                (double)ZHeap::heap()->nconcurrent_no_boost_worker_threads();\n-\n@@ -397,1 +422,1 @@\n-    ZStatCycle::at_end(_gc_cause, boost_factor);\n+    ZStatCycle::at_end(_gc_cause, ZHeap::heap()->active_workers());\n@@ -420,2 +445,2 @@\n-void ZDriver::gc(GCCause::Cause cause) {\n-  ZDriverGCScope scope(cause);\n+void ZDriver::gc(const ZDriverRequest& request) {\n+  ZDriverGCScope scope(request);\n@@ -461,2 +486,2 @@\n-    const GCCause::Cause cause = _gc_cycle_port.receive();\n-    if (cause == GCCause::_no_gc) {\n+    const ZDriverRequest request = _gc_cycle_port.receive();\n+    if (request.cause() == GCCause::_no_gc) {\n@@ -469,1 +494,1 @@\n-    gc(cause);\n+    gc(request);\n","filename":"src\/hotspot\/share\/gc\/z\/zDriver.cpp","additions":94,"deletions":69,"binary":false,"changes":163,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -33,0 +33,16 @@\n+class ZDriverRequest {\n+private:\n+  GCCause::Cause _cause;\n+  uint           _nworkers;\n+\n+public:\n+  ZDriverRequest();\n+  ZDriverRequest(GCCause::Cause cause);\n+  ZDriverRequest(GCCause::Cause cause, uint nworkers);\n+\n+  bool operator==(const ZDriverRequest& other) const;\n+\n+  GCCause::Cause cause() const;\n+  uint nworkers() const;\n+};\n+\n@@ -35,1 +51,1 @@\n-  ZMessagePort<GCCause::Cause> _gc_cycle_port;\n+  ZMessagePort<ZDriverRequest> _gc_cycle_port;\n@@ -54,1 +70,1 @@\n-  void gc(GCCause::Cause cause);\n+  void gc(const ZDriverRequest& request);\n@@ -63,1 +79,3 @@\n-  void collect(GCCause::Cause cause);\n+  bool is_busy() const;\n+\n+  void collect(const ZDriverRequest& request);\n","filename":"src\/hotspot\/share\/gc\/z\/zDriver.hpp","additions":22,"deletions":4,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -25,0 +25,1 @@\n+#include \"gc\/shared\/gc_globals.hpp\"\n@@ -42,1 +43,1 @@\n-#include \"gc\/z\/zWorkers.inline.hpp\"\n+#include \"gc\/z\/zWorkers.hpp\"\n@@ -151,2 +152,2 @@\n-uint ZHeap::nconcurrent_worker_threads() const {\n-  return _workers.nconcurrent();\n+uint ZHeap::active_workers() const {\n+  return _workers.active_workers();\n@@ -155,6 +156,2 @@\n-uint ZHeap::nconcurrent_no_boost_worker_threads() const {\n-  return _workers.nconcurrent_no_boost();\n-}\n-\n-void ZHeap::set_boost_worker_threads(bool boost) {\n-  _workers.set_boost(boost);\n+void ZHeap::set_active_workers(uint nworkers) {\n+  _workers.set_active_workers(nworkers);\n","filename":"src\/hotspot\/share\/gc\/z\/zHeap.cpp","additions":6,"deletions":9,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -96,3 +96,2 @@\n-  uint nconcurrent_worker_threads() const;\n-  uint nconcurrent_no_boost_worker_threads() const;\n-  void set_boost_worker_threads(bool boost);\n+  uint active_workers() const;\n+  void set_active_workers(uint nworkers);\n@@ -119,1 +118,1 @@\n-  bool is_alloc_stalled() const;\n+  bool has_alloc_stalled() const;\n","filename":"src\/hotspot\/share\/gc\/z\/zHeap.hpp","additions":3,"deletions":4,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -121,2 +121,2 @@\n-inline bool ZHeap::is_alloc_stalled() const {\n-  return _page_allocator.is_alloc_stalled();\n+inline bool ZHeap::has_alloc_stalled() const {\n+  return _page_allocator.has_alloc_stalled();\n","filename":"src\/hotspot\/share\/gc\/z\/zHeap.inline.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -59,1 +59,2 @@\n-  return (MAX2(ParallelGCThreads, ConcGCThreads) * ZPageSizeSmall) + ZPageSizeMedium;\n+  const uint nworkers = UseDynamicNumberOfGCThreads ? ConcGCThreads : MAX2(ConcGCThreads, ParallelGCThreads);\n+  return (nworkers * ZPageSizeSmall) + ZPageSizeMedium;\n@@ -96,7 +97,7 @@\n-  \/\/ Use 12.5% of the CPUs, rounded up. The number of concurrent threads we\n-  \/\/ would like to use heavily depends on the type of workload we are running.\n-  \/\/ Using too many threads will have a negative impact on the application\n-  \/\/ throughput, while using too few threads will prolong the GC-cycle and\n-  \/\/ we then risk being out-run by the application. Using 12.5% of the active\n-  \/\/ processors appears to be a fairly good balance.\n-  return nworkers(12.5);\n+  \/\/ The number of concurrent threads we would like to use heavily depends\n+  \/\/ on the type of workload we are running. Using too many threads will have\n+  \/\/ a negative impact on the application throughput, while using too few\n+  \/\/ threads will prolong the GC-cycle and we then risk being out-run by the\n+  \/\/ application. When in dynamic mode, use up to 25% of the active processors.\n+  \/\/  When in non-dynamic mode, use 12.5% of the active processors.\n+  return nworkers(UseDynamicNumberOfGCThreads ? 25.0 : 12.5);\n","filename":"src\/hotspot\/share\/gc\/z\/zHeuristics.cpp","additions":9,"deletions":8,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"gc\/shared\/gc_globals.hpp\"\n@@ -48,1 +49,1 @@\n-#include \"gc\/z\/zWorkers.inline.hpp\"\n+#include \"gc\/z\/zWorkers.hpp\"\n@@ -114,1 +115,1 @@\n-  _nworkers = _workers->nconcurrent();\n+  _nworkers = _workers->active_workers();\n@@ -138,1 +139,1 @@\n-  assert(_nworkers == _workers->nconcurrent(), \"Invalid number of workers\");\n+  assert(_nworkers == _workers->active_workers(), \"Invalid number of workers\");\n@@ -720,1 +721,1 @@\n-    _workers->run_concurrent(&task);\n+    _workers->run(&task);\n@@ -724,1 +725,1 @@\n-  _workers->run_concurrent(&task);\n+  _workers->run(&task);\n@@ -733,1 +734,1 @@\n-  _workers->run_concurrent(&task);\n+  _workers->run(&task);\n","filename":"src\/hotspot\/share\/gc\/z\/zMark.cpp","additions":7,"deletions":6,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -38,5 +38,5 @@\n-  Monitor        _monitor;\n-  bool           _has_message;\n-  T              _message;\n-  uint64_t       _seqnum;\n-  ZList<Request> _queue;\n+  mutable Monitor _monitor;\n+  bool            _has_message;\n+  T               _message;\n+  uint64_t        _seqnum;\n+  ZList<Request>  _queue;\n@@ -47,2 +47,4 @@\n-  void send_sync(T message);\n-  void send_async(T message);\n+  bool is_busy() const;\n+\n+  void send_sync(const T& message);\n+  void send_async(const T& message);\n","filename":"src\/hotspot\/share\/gc\/z\/zMessagePort.hpp","additions":10,"deletions":8,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -78,1 +78,7 @@\n-inline void ZMessagePort<T>::send_sync(T message) {\n+inline bool ZMessagePort<T>::is_busy() const {\n+  MonitorLocker ml(&_monitor, Monitor::_no_safepoint_check_flag);\n+  return _has_message;\n+}\n+\n+template <typename T>\n+inline void ZMessagePort<T>::send_sync(const T& message) {\n@@ -105,1 +111,1 @@\n-inline void ZMessagePort<T>::send_async(T message) {\n+inline void ZMessagePort<T>::send_async(const T& message) {\n","filename":"src\/hotspot\/share\/gc\/z\/zMessagePort.inline.hpp","additions":9,"deletions":3,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -379,1 +379,1 @@\n-      workers->run_concurrent(&task);\n+      workers->run(&task);\n@@ -424,1 +424,1 @@\n-  workers->run_concurrent(&task);\n+  workers->run(&task);\n","filename":"src\/hotspot\/share\/gc\/z\/zNMethod.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -148,0 +148,1 @@\n+    _nstalled(0),\n@@ -227,1 +228,1 @@\n-    workers->run_parallel(&task);\n+    workers->run_all(&task);\n@@ -286,0 +287,1 @@\n+  _nstalled = 0;\n@@ -451,0 +453,3 @@\n+  \/\/ Increment stalled counter\n+  Atomic::inc(&_nstalled);\n+\n@@ -651,1 +656,1 @@\n-  if (!flags.worker_relocation()) {\n+  if (!flags.worker_relocation() && is_init_completed()) {\n@@ -807,3 +812,2 @@\n-bool ZPageAllocator::is_alloc_stalled() const {\n-  assert(SafepointSynchronize::is_at_safepoint(), \"Should be at safepoint\");\n-  return !_stalled.is_empty();\n+bool ZPageAllocator::has_alloc_stalled() const {\n+  return Atomic::load(&_nstalled) != 0;\n","filename":"src\/hotspot\/share\/gc\/z\/zPageAllocator.cpp","additions":10,"deletions":6,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -63,0 +63,1 @@\n+  volatile uint64_t          _nstalled;\n@@ -130,1 +131,1 @@\n-  bool is_alloc_stalled() const;\n+  bool has_alloc_stalled() const;\n","filename":"src\/hotspot\/share\/gc\/z\/zPageAllocator.hpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -428,1 +428,1 @@\n-  _workers->run_concurrent(&task);\n+  _workers->run(&task);\n","filename":"src\/hotspot\/share\/gc\/z\/zReferenceProcessor.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -418,1 +418,1 @@\n-  _workers->run_concurrent(&task);\n+  _workers->run(&task);\n","filename":"src\/hotspot\/share\/gc\/z\/zRelocate.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -118,1 +118,1 @@\n-  _workers->run_concurrent(&task);\n+  _workers->run(&task);\n","filename":"src\/hotspot\/share\/gc\/z\/zRelocationSet.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -62,1 +62,1 @@\n-             nworkers(),\n+             ParallelGCThreads,\n@@ -66,1 +66,1 @@\n-  log_info_p(gc, init)(\"Runtime Workers: %u parallel\", nworkers());\n+  log_info_p(gc, init)(\"Runtime Workers: %u\", _workers.total_workers());\n@@ -70,2 +70,2 @@\n-  _workers.update_active_workers(nworkers());\n-  if (_workers.active_workers() != nworkers()) {\n+  _workers.update_active_workers(_workers.total_workers());\n+  if (_workers.active_workers() != _workers.total_workers()) {\n@@ -77,1 +77,1 @@\n-  ZRuntimeWorkersInitializeTask task(nworkers());\n+  ZRuntimeWorkersInitializeTask task(_workers.total_workers());\n@@ -81,4 +81,0 @@\n-uint ZRuntimeWorkers::nworkers() const {\n-  return ParallelGCThreads;\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/z\/zRuntimeWorkers.cpp","additions":5,"deletions":9,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -35,2 +35,0 @@\n-  uint nworkers() const;\n-\n","filename":"src\/hotspot\/share\/gc\/z\/zRuntimeWorkers.hpp","additions":1,"deletions":3,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -833,2 +833,2 @@\n-TruncatedSeq                ZStatAllocRate::_rate(ZStatAllocRate::sample_window_sec * ZStatAllocRate::sample_hz);\n-TruncatedSeq                ZStatAllocRate::_rate_avg(ZStatAllocRate::sample_window_sec * ZStatAllocRate::sample_hz);\n+TruncatedSeq                ZStatAllocRate::_samples(ZStatAllocRate::sample_hz);\n+TruncatedSeq                ZStatAllocRate::_rate(ZStatAllocRate::sample_hz);\n@@ -842,1 +842,1 @@\n-  const uint64_t bytes_per_second = bytes_per_sample._counter * sample_hz;\n+  _samples.add(bytes_per_sample._counter);\n@@ -844,0 +844,1 @@\n+  const uint64_t bytes_per_second = _samples.sum();\n@@ -845,1 +846,0 @@\n-  _rate_avg.add(_rate.avg());\n@@ -850,0 +850,4 @@\n+double ZStatAllocRate::predict() {\n+  return _rate.predict_next();\n+}\n+\n@@ -854,2 +858,2 @@\n-double ZStatAllocRate::avg_sd() {\n-  return _rate_avg.sd();\n+double ZStatAllocRate::sd() {\n+  return _rate.sd();\n@@ -1061,1 +1065,3 @@\n-NumberSeq ZStatCycle::_normalized_duration(0.7 \/* alpha *\/);\n+NumberSeq ZStatCycle::_serial_time(0.7 \/* alpha *\/);\n+NumberSeq ZStatCycle::_parallelizable_time(0.7 \/* alpha *\/);\n+uint      ZStatCycle::_last_active_workers = 0;\n@@ -1067,1 +1073,1 @@\n-void ZStatCycle::at_end(GCCause::Cause cause, double boost_factor) {\n+void ZStatCycle::at_end(GCCause::Cause cause, uint active_workers) {\n@@ -1074,3 +1080,3 @@\n-  \/\/ Calculate normalized cycle duration. The measured duration is\n-  \/\/ normalized using the boost factor to avoid artificial deflation\n-  \/\/ of the duration when boost mode is enabled.\n+  _last_active_workers = active_workers;\n+\n+  \/\/ Calculate serial and parallelizable GC cycle times\n@@ -1078,2 +1084,5 @@\n-  const double normalized_duration = duration * boost_factor;\n-  _normalized_duration.add(normalized_duration);\n+  const double workers_duration = ZStatWorkers::get_and_reset_duration();\n+  const double serial_time = duration - workers_duration;\n+  const double parallelizable_time = workers_duration * active_workers;\n+  _serial_time.add(serial_time);\n+  _parallelizable_time.add(parallelizable_time);\n@@ -1090,3 +1099,3 @@\n-bool ZStatCycle::is_normalized_duration_trustable() {\n-  \/\/ The normalized duration is considered trustable if we have\n-  \/\/ completed at least one warmup cycle\n+bool ZStatCycle::is_time_trustable() {\n+  \/\/ The times are considered trustable if we\n+  \/\/ have completed at least one warmup cycle.\n@@ -1096,2 +1105,10 @@\n-const AbsSeq& ZStatCycle::normalized_duration() {\n-  return _normalized_duration;\n+const AbsSeq& ZStatCycle::serial_time() {\n+  return _serial_time;\n+}\n+\n+const AbsSeq& ZStatCycle::parallelizable_time() {\n+  return _parallelizable_time;\n+}\n+\n+uint ZStatCycle::last_active_workers() {\n+  return _last_active_workers;\n@@ -1110,0 +1127,23 @@\n+\n+\/\/\n+\/\/ Stat workers\n+\/\/\n+Ticks ZStatWorkers::_start_of_last;\n+Tickspan ZStatWorkers::_accumulated_duration;\n+\n+void ZStatWorkers::at_start() {\n+  _start_of_last = Ticks::now();\n+}\n+\n+void ZStatWorkers::at_end() {\n+  const Ticks now = Ticks::now();\n+  const Tickspan duration = now - _start_of_last;\n+  _accumulated_duration += duration;\n+}\n+\n+double ZStatWorkers::get_and_reset_duration() {\n+  const double duration = _accumulated_duration.seconds();\n+  const Ticks now = Ticks::now();\n+  _accumulated_duration = now - now;\n+  return duration;\n+}\n","filename":"src\/hotspot\/share\/gc\/z\/zStat.cpp","additions":58,"deletions":18,"binary":false,"changes":76,"status":"modified"},{"patch":"@@ -334,2 +334,2 @@\n-  static TruncatedSeq                _rate;     \/\/ B\/s\n-  static TruncatedSeq                _rate_avg; \/\/ B\/s\n+  static TruncatedSeq                _samples;\n+  static TruncatedSeq                _rate;\n@@ -338,2 +338,1 @@\n-  static const uint64_t sample_window_sec = 1; \/\/ seconds\n-  static const uint64_t sample_hz         = 10;\n+  static const uint64_t sample_hz = 10;\n@@ -344,0 +343,1 @@\n+  static double predict();\n@@ -345,1 +345,1 @@\n-  static double avg_sd();\n+  static double sd();\n@@ -377,1 +377,3 @@\n-  static NumberSeq _normalized_duration;\n+  static NumberSeq _serial_time;\n+  static NumberSeq _parallelizable_time;\n+  static uint      _last_active_workers;\n@@ -381,1 +383,1 @@\n-  static void at_end(GCCause::Cause cause, double boost_factor);\n+  static void at_end(GCCause::Cause cause, uint active_workers);\n@@ -386,2 +388,5 @@\n-  static bool is_normalized_duration_trustable();\n-  static const AbsSeq& normalized_duration();\n+  static bool is_time_trustable();\n+  static const AbsSeq& serial_time();\n+  static const AbsSeq& parallelizable_time();\n+\n+  static uint last_active_workers();\n@@ -392,0 +397,15 @@\n+\/\/\n+\/\/ Stat workers\n+\/\/\n+class ZStatWorkers : public AllStatic {\n+private:\n+  static Ticks    _start_of_last;\n+  static Tickspan _accumulated_duration;\n+\n+public:\n+  static void at_start();\n+  static void at_end();\n+\n+  static double get_and_reset_duration();\n+};\n+\n","filename":"src\/hotspot\/share\/gc\/z\/zStat.hpp","additions":29,"deletions":9,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -109,1 +109,1 @@\n-  return MAX2(ParallelGCThreads, ConcGCThreads);\n+  return UseDynamicNumberOfGCThreads ? ConcGCThreads : MAX2(ConcGCThreads, ParallelGCThreads);\n","filename":"src\/hotspot\/share\/gc\/z\/zValue.inline.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -78,1 +78,1 @@\n-  _workers->run_concurrent(&task);\n+  _workers->run(&task);\n","filename":"src\/hotspot\/share\/gc\/z\/zWeakRootsProcessor.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -25,0 +25,1 @@\n+#include \"gc\/shared\/gc_globals.hpp\"\n@@ -27,0 +28,1 @@\n+#include \"gc\/z\/zStat.hpp\"\n@@ -29,1 +31,1 @@\n-#include \"gc\/z\/zWorkers.inline.hpp\"\n+#include \"gc\/z\/zWorkers.hpp\"\n@@ -32,1 +34,1 @@\n-class ZWorkersInitializeTask : public ZTask {\n+class ZWorkersInitializeTask : public AbstractGangTask {\n@@ -40,1 +42,1 @@\n-      ZTask(\"ZWorkersInitializeTask\"),\n+      AbstractGangTask(\"ZWorkersInitializeTask\"),\n@@ -45,1 +47,1 @@\n-  virtual void work() {\n+  virtual void work(uint worker_id) {\n@@ -63,1 +65,0 @@\n-    _boost(false),\n@@ -65,1 +66,1 @@\n-             nworkers(),\n+             UseDynamicNumberOfGCThreads ? ConcGCThreads : MAX2(ConcGCThreads, ParallelGCThreads),\n@@ -69,1 +70,5 @@\n-  log_info_p(gc, init)(\"Workers: %u parallel, %u concurrent\", nparallel(), nconcurrent());\n+  if (UseDynamicNumberOfGCThreads) {\n+    log_info_p(gc, init)(\"GC Workers: %u (dynamic)\", _workers.total_workers());\n+  } else {\n+    log_info_p(gc, init)(\"GC Workers: %u\/%u (static)\", ConcGCThreads, _workers.total_workers());\n+  }\n@@ -73,2 +78,2 @@\n-  _workers.update_active_workers(nworkers());\n-  if (_workers.active_workers() != nworkers()) {\n+  _workers.update_active_workers(_workers.total_workers());\n+  if (_workers.active_workers() != _workers.total_workers()) {\n@@ -79,2 +84,2 @@\n-  ZWorkersInitializeTask task(nworkers());\n-  run(&task, nworkers());\n+  ZWorkersInitializeTask task(_workers.total_workers());\n+  _workers.run_task(&task);\n@@ -83,6 +88,2 @@\n-void ZWorkers::set_boost(bool boost) {\n-  if (boost) {\n-    log_debug(gc)(\"Boosting workers\");\n-  }\n-\n-  _boost = boost;\n+uint ZWorkers::active_workers() const {\n+  return _workers.active_workers();\n@@ -91,2 +92,2 @@\n-void ZWorkers::run(ZTask* task, uint nworkers) {\n-  log_debug(gc, task)(\"Executing Task: %s, Active Workers: %u\", task->name(), nworkers);\n+void ZWorkers::set_active_workers(uint nworkers) {\n+  log_info(gc, task)(\"Using %u workers\", nworkers);\n@@ -94,1 +95,0 @@\n-  _workers.run_task(task->gang_task());\n@@ -97,2 +97,5 @@\n-void ZWorkers::run_parallel(ZTask* task) {\n-  run(task, nparallel());\n+void ZWorkers::run(ZTask* task) {\n+  log_debug(gc, task)(\"Executing Task: %s, Active Workers: %u\", task->name(), active_workers());\n+  ZStatWorkers::at_start();\n+  _workers.run_task(task->gang_task());\n+  ZStatWorkers::at_end();\n@@ -101,2 +104,11 @@\n-void ZWorkers::run_concurrent(ZTask* task) {\n-  run(task, nconcurrent());\n+void ZWorkers::run_all(ZTask* task) {\n+  \/\/ Save number of active workers\n+  const uint prev_active_workers = _workers.active_workers();\n+\n+  \/\/ Execute task using all workers\n+  _workers.update_active_workers(_workers.total_workers());\n+  log_debug(gc, task)(\"Executing Task: %s, Active Workers: %u\", task->name(), active_workers());\n+  _workers.run_task(task->gang_task());\n+\n+  \/\/ Restore number of active workers\n+  _workers.update_active_workers(prev_active_workers);\n","filename":"src\/hotspot\/share\/gc\/z\/zWorkers.cpp","additions":37,"deletions":25,"binary":false,"changes":62,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -34,1 +34,0 @@\n-  bool     _boost;\n@@ -37,2 +36,0 @@\n-  void run(ZTask* task, uint nworkers);\n-\n@@ -42,7 +39,2 @@\n-  uint nparallel() const;\n-  uint nparallel_no_boost() const;\n-  uint nconcurrent() const;\n-  uint nconcurrent_no_boost() const;\n-  uint nworkers() const;\n-\n-  void set_boost(bool boost);\n+  uint active_workers() const;\n+  void set_active_workers(uint nworkers);\n@@ -50,2 +42,2 @@\n-  void run_parallel(ZTask* task);\n-  void run_concurrent(ZTask* task);\n+  void run(ZTask* task);\n+  void run_all(ZTask* task);\n","filename":"src\/hotspot\/share\/gc\/z\/zWorkers.hpp","additions":5,"deletions":13,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -1,52 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_Z_ZWORKERS_INLINE_HPP\n-#define SHARE_GC_Z_ZWORKERS_INLINE_HPP\n-\n-#include \"gc\/z\/zWorkers.hpp\"\n-\n-#include \"gc\/shared\/gc_globals.hpp\"\n-#include \"utilities\/globalDefinitions.hpp\"\n-\n-inline uint ZWorkers::nparallel() const {\n-  return _boost ? nworkers() : nparallel_no_boost();\n-}\n-\n-inline uint ZWorkers::nparallel_no_boost() const {\n-  return ParallelGCThreads;\n-}\n-\n-inline uint ZWorkers::nconcurrent() const {\n-  return _boost ? nworkers() : nconcurrent_no_boost();\n-}\n-\n-inline uint ZWorkers::nconcurrent_no_boost() const {\n-  return ConcGCThreads;\n-}\n-\n-inline uint ZWorkers::nworkers() const {\n-  return MAX2(ParallelGCThreads, ConcGCThreads);\n-}\n-\n-#endif \/\/ SHARE_GC_Z_ZWORKERS_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/z\/zWorkers.inline.hpp","additions":0,"deletions":52,"binary":false,"changes":52,"status":"deleted"}]}