{"files":[{"patch":"@@ -2423,4 +2423,0 @@\n-const bool Matcher::has_predicated_vectors(void) {\n-  return UseSVE > 0;\n-}\n-\n@@ -2435,8 +2431,0 @@\n-bool Matcher::supports_vector_variable_shifts(void) {\n-  return true;\n-}\n-\n-bool Matcher::supports_vector_variable_rotates(void) {\n-  return false; \/\/ not supported\n-}\n-\n@@ -2457,11 +2445,0 @@\n-const bool Matcher::isSimpleConstant64(jlong value) {\n-  \/\/ Will one (StoreL ConL) be cheaper than two (StoreI ConI)?.\n-  \/\/ Probably always true, even if a temp register is required.\n-  return true;\n-}\n-\n-\/\/ true just means we have fast l2f conversion\n-const bool Matcher::convL2FSupported(void) {\n-  return true;\n-}\n-\n@@ -2503,4 +2480,0 @@\n-const bool Matcher::supports_scalable_vector() {\n-  return UseSVE > 0;\n-}\n-\n@@ -2528,29 +2501,0 @@\n-\/\/ aarch64 supports misaligned vectors store\/load.\n-const bool Matcher::misaligned_vectors_ok() {\n-  return true;\n-}\n-\n-\/\/ false => size gets scaled to BytesPerLong, ok.\n-const bool Matcher::init_array_count_is_in_bytes = false;\n-\n-\/\/ Use conditional move (CMOVL)\n-const int Matcher::long_cmove_cost() {\n-  \/\/ long cmoves are no more expensive than int cmoves\n-  return 0;\n-}\n-\n-const int Matcher::float_cmove_cost() {\n-  \/\/ float cmoves are no more expensive than int cmoves\n-  return 0;\n-}\n-\n-\/\/ Does the CPU require late expand (see block.cpp for description of late expand)?\n-const bool Matcher::require_postalloc_expand = false;\n-\n-\/\/ Do we need to mask the count passed to shift instructions or does\n-\/\/ the cpu only look at the lower 5\/6 bits anyway?\n-const bool Matcher::need_masked_shift_count = false;\n-\n-\/\/ No support for generic vector operands.\n-const bool Matcher::supports_generic_vector_operands  = false;\n-\n@@ -2572,56 +2516,0 @@\n-\/\/ This affects two different things:\n-\/\/  - how Decode nodes are matched\n-\/\/  - how ImplicitNullCheck opportunities are recognized\n-\/\/ If true, the matcher will try to remove all Decodes and match them\n-\/\/ (as operands) into nodes. NullChecks are not prepared to deal with\n-\/\/ Decodes by final_graph_reshaping().\n-\/\/ If false, final_graph_reshaping() forces the decode behind the Cmp\n-\/\/ for a NullCheck. The matcher matches the Decode node into a register.\n-\/\/ Implicit_null_check optimization moves the Decode along with the\n-\/\/ memory operation back up before the NullCheck.\n-bool Matcher::narrow_oop_use_complex_address() {\n-  return CompressedOops::shift() == 0;\n-}\n-\n-bool Matcher::narrow_klass_use_complex_address() {\n-\/\/ TODO\n-\/\/ decide whether we need to set this to true\n-  return false;\n-}\n-\n-bool Matcher::const_oop_prefer_decode() {\n-  \/\/ Prefer ConN+DecodeN over ConP in simple compressed oops mode.\n-  return CompressedOops::base() == NULL;\n-}\n-\n-bool Matcher::const_klass_prefer_decode() {\n-  \/\/ Prefer ConNKlass+DecodeNKlass over ConP in simple compressed klass mode.\n-  return CompressedKlassPointers::base() == NULL;\n-}\n-\n-\/\/ Is it better to copy float constants, or load them directly from\n-\/\/ memory?  Intel can load a float constant from a direct address,\n-\/\/ requiring no extra registers.  Most RISCs will have to materialize\n-\/\/ an address into a register first, so they would do better to copy\n-\/\/ the constant from stack.\n-const bool Matcher::rematerialize_float_constants = false;\n-\n-\/\/ If CPU can load and store mis-aligned doubles directly then no\n-\/\/ fixup is needed.  Else we split the double into 2 integer pieces\n-\/\/ and move it piece-by-piece.  Only happens when passing doubles into\n-\/\/ C code as the Java calling convention forces doubles to be aligned.\n-const bool Matcher::misaligned_doubles_ok = true;\n-\n-\/\/ Advertise here if the CPU requires explicit rounding operations to implement strictfp mode.\n-const bool Matcher::strict_fp_requires_explicit_rounding = false;\n-\n-\/\/ Are floats converted to double when stored to stack during\n-\/\/ deoptimization?\n-bool Matcher::float_in_double() { return false; }\n-\n-\/\/ Do ints take an entire long register or just half?\n-\/\/ The relevant question is how the int is callee-saved:\n-\/\/ the whole long is written but de-opt'ing will have to extract\n-\/\/ the relevant 32 bits.\n-const bool Matcher::int_in_long = true;\n-\n@@ -2704,2 +2592,0 @@\n-const bool Matcher::convi2l_type_required = false;\n-\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":0,"deletions":114,"binary":false,"changes":114,"status":"modified"},{"patch":"@@ -0,0 +1,152 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef CPU_AARCH64_MATCHER_AARCH64_HPP\n+#define CPU_AARCH64_MATCHER_AARCH64_HPP\n+\n+  \/\/ Defined within class Matcher\n+\n+  \/\/ false => size gets scaled to BytesPerLong, ok.\n+  static const bool init_array_count_is_in_bytes = false;\n+\n+  \/\/ Whether this platform implements the scalable vector feature\n+  static const bool implements_scalable_vector = true;\n+\n+  static const bool supports_scalable_vector() {\n+    return UseSVE > 0;\n+  }\n+\n+  \/\/ aarch64 supports misaligned vectors store\/load.\n+  static constexpr bool misaligned_vectors_ok() {\n+    return true;\n+  }\n+\n+  \/\/ Whether code generation need accurate ConvI2L types.\n+  static const bool convi2l_type_required = false;\n+\n+  \/\/ Does the CPU require late expand (see block.cpp for description of late expand)?\n+  static const bool require_postalloc_expand = false;\n+\n+  \/\/ Do we need to mask the count passed to shift instructions or does\n+  \/\/ the cpu only look at the lower 5\/6 bits anyway?\n+  static const bool need_masked_shift_count = false;\n+\n+  \/\/ No support for generic vector operands.\n+  static const bool supports_generic_vector_operands = false;\n+\n+  static constexpr bool isSimpleConstant64(jlong value) {\n+    \/\/ Will one (StoreL ConL) be cheaper than two (StoreI ConI)?.\n+    \/\/ Probably always true, even if a temp register is required.\n+    return true;\n+  }\n+\n+  \/\/ Use conditional move (CMOVL)\n+  static constexpr int long_cmove_cost() {\n+    \/\/ long cmoves are no more expensive than int cmoves\n+    return 0;\n+  }\n+\n+  static constexpr int float_cmove_cost() {\n+    \/\/ float cmoves are no more expensive than int cmoves\n+    return 0;\n+  }\n+\n+  \/\/ This affects two different things:\n+  \/\/  - how Decode nodes are matched\n+  \/\/  - how ImplicitNullCheck opportunities are recognized\n+  \/\/ If true, the matcher will try to remove all Decodes and match them\n+  \/\/ (as operands) into nodes. NullChecks are not prepared to deal with\n+  \/\/ Decodes by final_graph_reshaping().\n+  \/\/ If false, final_graph_reshaping() forces the decode behind the Cmp\n+  \/\/ for a NullCheck. The matcher matches the Decode node into a register.\n+  \/\/ Implicit_null_check optimization moves the Decode along with the\n+  \/\/ memory operation back up before the NullCheck.\n+  static bool narrow_oop_use_complex_address() {\n+    return CompressedOops::shift() == 0;\n+  }\n+\n+  static bool narrow_klass_use_complex_address() {\n+    \/\/ TODO\n+    \/\/ decide whether we need to set this to true\n+    return false;\n+  }\n+\n+  static bool const_oop_prefer_decode() {\n+    \/\/ Prefer ConN+DecodeN over ConP in simple compressed oops mode.\n+    return CompressedOops::base() == NULL;\n+  }\n+\n+  static bool const_klass_prefer_decode() {\n+    \/\/ Prefer ConNKlass+DecodeNKlass over ConP in simple compressed klass mode.\n+    return CompressedKlassPointers::base() == NULL;\n+  }\n+\n+  \/\/ Is it better to copy float constants, or load them directly from\n+  \/\/ memory?  Intel can load a float constant from a direct address,\n+  \/\/ requiring no extra registers.  Most RISCs will have to materialize\n+  \/\/ an address into a register first, so they would do better to copy\n+  \/\/ the constant from stack.\n+  static const bool rematerialize_float_constants = false;\n+\n+  \/\/ If CPU can load and store mis-aligned doubles directly then no\n+  \/\/ fixup is needed.  Else we split the double into 2 integer pieces\n+  \/\/ and move it piece-by-piece.  Only happens when passing doubles into\n+  \/\/ C code as the Java calling convention forces doubles to be aligned.\n+  static const bool misaligned_doubles_ok = true;\n+\n+  \/\/ Advertise here if the CPU requires explicit rounding operations to implement strictfp mode.\n+  static const bool strict_fp_requires_explicit_rounding = false;\n+\n+  \/\/ Are floats converted to double when stored to stack during\n+  \/\/ deoptimization?\n+  static constexpr bool float_in_double() { return false; }\n+\n+  \/\/ Do ints take an entire long register or just half?\n+  \/\/ The relevant question is how the int is callee-saved:\n+  \/\/ the whole long is written but de-opt'ing will have to extract\n+  \/\/ the relevant 32 bits.\n+  static const bool int_in_long = true;\n+\n+  \/\/ Does the CPU supports vector variable shift instructions?\n+  static constexpr bool supports_vector_variable_shifts(void) {\n+    return true;\n+  }\n+\n+  \/\/ Does the CPU supports vector variable rotate instructions?\n+  static constexpr bool supports_vector_variable_rotates(void) {\n+    return false;\n+  }\n+\n+  \/\/ Some microarchitectures have mask registers used on vectors\n+  static const bool has_predicated_vectors(void) {\n+    return UseSVE > 0;\n+  }\n+\n+  \/\/ true means we have fast l2f convers\n+  \/\/ false means that conversion is done by runtime call\n+  static constexpr bool convL2FSupported(void) {\n+      return true;\n+  }\n+\n+#endif \/\/ CPU_AARCH64_MATCHER_AARCH64_HPP\n","filename":"src\/hotspot\/cpu\/aarch64\/matcher_aarch64.hpp","additions":152,"deletions":0,"binary":false,"changes":152,"status":"added"},{"patch":"@@ -986,4 +986,0 @@\n-const bool Matcher::has_predicated_vectors(void) {\n-  return false;\n-}\n-\n@@ -998,8 +994,0 @@\n-bool Matcher::supports_vector_variable_shifts(void) {\n-  return VM_Version::has_simd();\n-}\n-\n-bool Matcher::supports_vector_variable_rotates(void) {\n-  return false; \/\/ not supported\n-}\n-\n@@ -1015,4 +1003,0 @@\n-const bool Matcher::supports_scalable_vector() {\n-  return false;\n-}\n-\n@@ -1045,9 +1029,0 @@\n-\/\/ ARM doesn't support misaligned vectors store\/load.\n-const bool Matcher::misaligned_vectors_ok() {\n-  return false;\n-}\n-\n-const bool Matcher::convL2FSupported(void) {\n-  return false;\n-}\n-\n@@ -1068,27 +1043,0 @@\n-const bool Matcher::isSimpleConstant64(jlong value) {\n-  \/\/ Will one (StoreL ConL) be cheaper than two (StoreI ConI)?.\n-  return false;\n-}\n-\n-\/\/ No scaling for the parameter the ClearArray node.\n-const bool Matcher::init_array_count_is_in_bytes = true;\n-\n-\/\/ Needs 2 CMOV's for longs.\n-const int Matcher::long_cmove_cost() { return 2; }\n-\n-\/\/ CMOVF\/CMOVD are expensive on ARM.\n-const int Matcher::float_cmove_cost() { return ConditionalMoveLimit; }\n-\n-\/\/ Does the CPU require late expand (see block.cpp for description of late expand)?\n-const bool Matcher::require_postalloc_expand = false;\n-\n-\/\/ Do we need to mask the count passed to shift instructions or does\n-\/\/ the cpu only look at the lower 5\/6 bits anyway?\n-\/\/ FIXME: does this handle vector shifts as well?\n-const bool Matcher::need_masked_shift_count = true;\n-\n-const bool Matcher::convi2l_type_required = true;\n-\n-\/\/ No support for generic vector operands.\n-const bool Matcher::supports_generic_vector_operands  = false;\n-\n@@ -1126,54 +1074,0 @@\n-bool Matcher::narrow_oop_use_complex_address() {\n-  NOT_LP64(ShouldNotCallThis());\n-  assert(UseCompressedOops, \"only for compressed oops code\");\n-  return false;\n-}\n-\n-bool Matcher::narrow_klass_use_complex_address() {\n-  NOT_LP64(ShouldNotCallThis());\n-  assert(UseCompressedClassPointers, \"only for compressed klass code\");\n-  return false;\n-}\n-\n-bool Matcher::const_oop_prefer_decode() {\n-  NOT_LP64(ShouldNotCallThis());\n-  return true;\n-}\n-\n-bool Matcher::const_klass_prefer_decode() {\n-  NOT_LP64(ShouldNotCallThis());\n-  return true;\n-}\n-\n-\/\/ Is it better to copy float constants, or load them directly from memory?\n-\/\/ Intel can load a float constant from a direct address, requiring no\n-\/\/ extra registers.  Most RISCs will have to materialize an address into a\n-\/\/ register first, so they would do better to copy the constant from stack.\n-const bool Matcher::rematerialize_float_constants = false;\n-\n-\/\/ If CPU can load and store mis-aligned doubles directly then no fixup is\n-\/\/ needed.  Else we split the double into 2 integer pieces and move it\n-\/\/ piece-by-piece.  Only happens when passing doubles into C code as the\n-\/\/ Java calling convention forces doubles to be aligned.\n-const bool Matcher::misaligned_doubles_ok = false;\n-\n-\/\/ Advertise here if the CPU requires explicit rounding operations to implement strictfp mode.\n-const bool Matcher::strict_fp_requires_explicit_rounding = false;\n-\n-\/\/ Are floats converted to double when stored to stack during deoptimization?\n-\/\/ ARM does not handle callee-save floats.\n-bool Matcher::float_in_double() {\n-  return false;\n-}\n-\n-\/\/ Do ints take an entire long register or just half?\n-\/\/ Note that we if-def off of _LP64.\n-\/\/ The relevant question is how the int is callee-saved.  In _LP64\n-\/\/ the whole long is written but de-opt'ing will have to extract\n-\/\/ the relevant 32 bits, in not-_LP64 only the low 32 bits is written.\n-#ifdef _LP64\n-const bool Matcher::int_in_long = true;\n-#else\n-const bool Matcher::int_in_long = false;\n-#endif\n-\n","filename":"src\/hotspot\/cpu\/arm\/arm.ad","additions":0,"deletions":106,"binary":false,"changes":106,"status":"modified"},{"patch":"@@ -0,0 +1,145 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef CPU_ARM_MATCHER_ARM_HPP\n+#define CPU_ARM_MATCHER_ARM_HPP\n+\n+  \/\/ Defined within class Matcher\n+\n+  \/\/ No scaling for the parameter the ClearArray node.\n+  static const bool init_array_count_is_in_bytes = true;\n+\n+  \/\/ Whether this platform implements the scalable vector feature\n+  static const bool implements_scalable_vector = false;\n+\n+  static constexpr bool supports_scalable_vector() {\n+    return false;\n+  }\n+\n+  \/\/ ARM doesn't support misaligned vectors store\/load.\n+  static constexpr bool misaligned_vectors_ok() {\n+    return false;\n+  }\n+\n+  \/\/ Whether code generation need accurate ConvI2L types.\n+  static const bool convi2l_type_required = true;\n+\n+  \/\/ Do we need to mask the count passed to shift instructions or does\n+  \/\/ the cpu only look at the lower 5\/6 bits anyway?\n+  \/\/ FIXME: does this handle vector shifts as well?\n+  static const bool need_masked_shift_count = true;\n+\n+  \/\/ Does the CPU require late expand (see block.cpp for description of late expand)?\n+  static const bool require_postalloc_expand = false;\n+\n+  \/\/ No support for generic vector operands.\n+  static const bool supports_generic_vector_operands = false;\n+\n+  static constexpr bool isSimpleConstant64(jlong value) {\n+    \/\/ Will one (StoreL ConL) be cheaper than two (StoreI ConI)?.\n+    return false;\n+  }\n+\n+  \/\/ Needs 2 CMOV's for longs.\n+  static constexpr int long_cmove_cost() { return 2; }\n+\n+  \/\/ CMOVF\/CMOVD are expensive on ARM.\n+  static int float_cmove_cost() { return ConditionalMoveLimit; }\n+\n+  static bool narrow_oop_use_complex_address() {\n+    NOT_LP64(ShouldNotCallThis());\n+    assert(UseCompressedOops, \"only for compressed oops code\");\n+    return false;\n+  }\n+\n+  static bool narrow_klass_use_complex_address() {\n+    NOT_LP64(ShouldNotCallThis());\n+    assert(UseCompressedClassPointers, \"only for compressed klass code\");\n+    return false;\n+  }\n+\n+  static bool const_oop_prefer_decode() {\n+    NOT_LP64(ShouldNotCallThis());\n+    return true;\n+  }\n+\n+  static bool const_klass_prefer_decode() {\n+    NOT_LP64(ShouldNotCallThis());\n+    return true;\n+  }\n+\n+  \/\/ Is it better to copy float constants, or load them directly from memory?\n+  \/\/ Intel can load a float constant from a direct address, requiring no\n+  \/\/ extra registers.  Most RISCs will have to materialize an address into a\n+  \/\/ register first, so they would do better to copy the constant from stack.\n+  static const bool rematerialize_float_constants = false;\n+\n+  \/\/ If CPU can load and store mis-aligned doubles directly then no fixup is\n+  \/\/ needed.  Else we split the double into 2 integer pieces and move it\n+  \/\/ piece-by-piece.  Only happens when passing doubles into C code as the\n+  \/\/ Java calling convention forces doubles to be aligned.\n+  static const bool misaligned_doubles_ok = false;\n+\n+  \/\/ Advertise here if the CPU requires explicit rounding operations to implement strictfp mode.\n+  static const bool strict_fp_requires_explicit_rounding = false;\n+\n+  \/\/ Are floats converted to double when stored to stack during deoptimization?\n+  \/\/ ARM does not handle callee-save floats.\n+  static constexpr bool float_in_double() {\n+    return false;\n+  }\n+\n+  \/\/ Do ints take an entire long register or just half?\n+  \/\/ Note that we if-def off of _LP64.\n+  \/\/ The relevant question is how the int is callee-saved.  In _LP64\n+  \/\/ the whole long is written but de-opt'ing will have to extract\n+  \/\/ the relevant 32 bits, in not-_LP64 only the low 32 bits is written.\n+#ifdef _LP64\n+  static const bool int_in_long = true;\n+#else\n+  static const bool int_in_long = false;\n+#endif\n+\n+  \/\/ Does the CPU supports vector variable shift instructions?\n+  static bool supports_vector_variable_shifts(void) {\n+    return VM_Version::has_simd();\n+  }\n+\n+  \/\/ Does the CPU supports vector variable rotate instructions?\n+  static constexpr bool supports_vector_variable_rotates(void) {\n+    return false; \/\/ not supported\n+  }\n+\n+  \/\/ Some microarchitectures have mask registers used on vectors\n+  static constexpr bool has_predicated_vectors(void) {\n+    return false;\n+  }\n+\n+  \/\/ true means we have fast l2f convers\n+  \/\/ false means that conversion is done by runtime call\n+  static constexpr bool convL2FSupported(void) {\n+      return true;\n+  }\n+\n+#endif \/\/ CPU_ARM_MATCHER_ARM_HPP\n","filename":"src\/hotspot\/cpu\/arm\/matcher_arm.hpp","additions":145,"deletions":0,"binary":false,"changes":145,"status":"added"},{"patch":"@@ -0,0 +1,155 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef CPU_PPC_MATCHER_PPC_HPP\n+#define CPU_PPC_MATCHER_PPC_HPP\n+\n+  \/\/ Defined within class Matcher\n+\n+  \/\/ false => size gets scaled to BytesPerLong, ok.\n+  static const bool init_array_count_is_in_bytes = false;\n+\n+  \/\/ Whether this platform implements the scalable vector feature\n+  static const bool implements_scalable_vector = false;\n+\n+  static constexpr bool supports_scalable_vector() {\n+    return false;\n+  }\n+\n+  \/\/ PPC implementation uses VSX load\/store instructions (if\n+  \/\/ SuperwordUseVSX) which support 4 byte but not arbitrary alignment\n+  static const bool misaligned_vectors_ok() {\n+    return false;\n+  }\n+\n+  \/\/ Whether code generation need accurate ConvI2L types.\n+  static const bool convi2l_type_required = true;\n+\n+  \/\/ Do we need to mask the count passed to shift instructions or does\n+  \/\/ the cpu only look at the lower 5\/6 bits anyway?\n+  \/\/ PowerPC requires masked shift counts.\n+  static const bool need_masked_shift_count = true;\n+\n+  \/\/ Power6 requires postalloc expand (see block.cpp for description of postalloc expand).\n+  static const bool require_postalloc_expand = true;\n+\n+  \/\/ No support for generic vector operands.\n+  static const bool supports_generic_vector_operands = false;\n+\n+  static constexpr bool isSimpleConstant64(jlong value) {\n+    \/\/ Probably always true, even if a temp register is required.\n+    return true;\n+  }\n+\n+  \/\/ Use conditional move (CMOVL) on Power7.\n+  static constexpr int long_cmove_cost() { return 0; } \/\/ this only makes long cmoves more expensive than int cmoves\n+\n+  \/\/ Suppress CMOVF. Conditional move available (sort of) on PPC64 only from P7 onwards. Not exploited yet.\n+  \/\/ fsel doesn't accept a condition register as input, so this would be slightly different.\n+  static int float_cmove_cost() { return ConditionalMoveLimit; }\n+\n+  \/\/ This affects two different things:\n+  \/\/  - how Decode nodes are matched\n+  \/\/  - how ImplicitNullCheck opportunities are recognized\n+  \/\/ If true, the matcher will try to remove all Decodes and match them\n+  \/\/ (as operands) into nodes. NullChecks are not prepared to deal with\n+  \/\/ Decodes by final_graph_reshaping().\n+  \/\/ If false, final_graph_reshaping() forces the decode behind the Cmp\n+  \/\/ for a NullCheck. The matcher matches the Decode node into a register.\n+  \/\/ Implicit_null_check optimization moves the Decode along with the\n+  \/\/ memory operation back up before the NullCheck.\n+  static constexpr bool narrow_oop_use_complex_address() {\n+    \/\/ TODO: PPC port if (MatchDecodeNodes) return true;\n+    return false;\n+  }\n+\n+  static bool narrow_klass_use_complex_address() {\n+    NOT_LP64(ShouldNotCallThis());\n+    assert(UseCompressedClassPointers, \"only for compressed klass code\");\n+    \/\/ TODO: PPC port if (MatchDecodeNodes) return true;\n+    return false;\n+  }\n+\n+  static bool const_oop_prefer_decode() {\n+    \/\/ Prefer ConN+DecodeN over ConP in simple compressed oops mode.\n+    return CompressedOops::base() == NULL;\n+  }\n+\n+  static bool const_klass_prefer_decode() {\n+    \/\/ Prefer ConNKlass+DecodeNKlass over ConP in simple compressed klass mode.\n+    return CompressedKlassPointers::base() == NULL;\n+  }\n+\n+  \/\/ Is it better to copy float constants, or load them directly from memory?\n+  \/\/ Intel can load a float constant from a direct address, requiring no\n+  \/\/ extra registers. Most RISCs will have to materialize an address into a\n+  \/\/ register first, so they would do better to copy the constant from stack.\n+  static const bool rematerialize_float_constants = false;\n+\n+  \/\/ If CPU can load and store mis-aligned doubles directly then no fixup is\n+  \/\/ needed. Else we split the double into 2 integer pieces and move it\n+  \/\/ piece-by-piece. Only happens when passing doubles into C code as the\n+  \/\/ Java calling convention forces doubles to be aligned.\n+  static const bool misaligned_doubles_ok = true;\n+\n+  \/\/ Advertise here if the CPU requires explicit rounding operations to implement strictfp mode.\n+  static const bool strict_fp_requires_explicit_rounding = false;\n+\n+  \/\/ Do floats take an entire double register or just half?\n+  \/\/\n+  \/\/ A float occupies a ppc64 double register. For the allocator, a\n+  \/\/ ppc64 double register appears as a pair of float registers.\n+  static constexpr bool float_in_double() { return true; }\n+\n+  \/\/ Do ints take an entire long register or just half?\n+  \/\/ The relevant question is how the int is callee-saved:\n+  \/\/ the whole long is written but de-opt'ing will have to extract\n+  \/\/ the relevant 32 bits.\n+  static const bool int_in_long = true;\n+\n+  \/\/ Does the CPU supports vector variable shift instructions?\n+  static constexpr bool supports_vector_variable_shifts(void) {\n+    return false;\n+  }\n+\n+  \/\/ Does the CPU supports vector variable rotate instructions?\n+  static constexpr bool supports_vector_variable_rotates(void) {\n+    return false;\n+  }\n+\n+  \/\/ Some microarchitectures have mask registers used on vectors\n+  static constexpr bool has_predicated_vectors(void) {\n+    return false;\n+  }\n+\n+  \/\/ true means we have fast l2f convers\n+  \/\/ false means that conversion is done by runtime call\n+  static const bool convL2FSupported(void) {\n+    \/\/ fcfids can do the conversion (>= Power7).\n+    \/\/ fcfid + frsp showed rounding problem when result should be 0x3f800001.\n+    return VM_Version::has_fcfids();\n+  }\n+\n+\n+#endif \/\/ CPU_PPC_MATCHER_PPC_HPP\n","filename":"src\/hotspot\/cpu\/ppc\/matcher_ppc.hpp","additions":155,"deletions":0,"binary":false,"changes":155,"status":"added"},{"patch":"@@ -2178,4 +2178,0 @@\n-const bool Matcher::has_predicated_vectors(void) {\n-  return false;\n-}\n-\n@@ -2190,8 +2186,0 @@\n-bool Matcher::supports_vector_variable_shifts(void) {\n-  return false; \/\/ not supported\n-}\n-\n-bool Matcher::supports_vector_variable_rotates(void) {\n-  return false; \/\/ not supported\n-}\n-\n@@ -2202,6 +2190,0 @@\n-const bool Matcher::convL2FSupported(void) {\n-  \/\/ fcfids can do the conversion (>= Power7).\n-  \/\/ fcfid + frsp showed rounding problem when result should be 0x3f800001.\n-  return VM_Version::has_fcfids(); \/\/ False means that conversion is done by runtime call.\n-}\n-\n@@ -2240,4 +2222,0 @@\n-const bool Matcher::supports_scalable_vector() {\n-  return false;\n-}\n-\n@@ -2248,6 +2226,0 @@\n-\/\/ PPC implementation uses VSX load\/store instructions (if\n-\/\/ SuperwordUseVSX) which support 4 byte but not arbitrary alignment\n-const bool Matcher::misaligned_vectors_ok() {\n-  return false;\n-}\n-\n@@ -2276,4 +2248,0 @@\n-const bool Matcher::isSimpleConstant64(jlong value) {\n-  \/\/ Probably always true, even if a temp register is required.\n-  return true;\n-}\n@@ -2292,21 +2260,0 @@\n-\/\/ false => size gets scaled to BytesPerLong, ok.\n-const bool Matcher::init_array_count_is_in_bytes = false;\n-\n-\/\/ Use conditional move (CMOVL) on Power7.\n-const int Matcher::long_cmove_cost() { return 0; } \/\/ this only makes long cmoves more expensive than int cmoves\n-\n-\/\/ Suppress CMOVF. Conditional move available (sort of) on PPC64 only from P7 onwards. Not exploited yet.\n-\/\/ fsel doesn't accept a condition register as input, so this would be slightly different.\n-const int Matcher::float_cmove_cost() { return ConditionalMoveLimit; }\n-\n-\/\/ Power6 requires postalloc expand (see block.cpp for description of postalloc expand).\n-const bool Matcher::require_postalloc_expand = true;\n-\n-\/\/ Do we need to mask the count passed to shift instructions or does\n-\/\/ the cpu only look at the lower 5\/6 bits anyway?\n-\/\/ PowerPC requires masked shift counts.\n-const bool Matcher::need_masked_shift_count = true;\n-\n-\/\/ No support for generic vector operands.\n-const bool Matcher::supports_generic_vector_operands  = false;\n-\n@@ -2328,59 +2275,0 @@\n-\/\/ This affects two different things:\n-\/\/  - how Decode nodes are matched\n-\/\/  - how ImplicitNullCheck opportunities are recognized\n-\/\/ If true, the matcher will try to remove all Decodes and match them\n-\/\/ (as operands) into nodes. NullChecks are not prepared to deal with\n-\/\/ Decodes by final_graph_reshaping().\n-\/\/ If false, final_graph_reshaping() forces the decode behind the Cmp\n-\/\/ for a NullCheck. The matcher matches the Decode node into a register.\n-\/\/ Implicit_null_check optimization moves the Decode along with the\n-\/\/ memory operation back up before the NullCheck.\n-bool Matcher::narrow_oop_use_complex_address() {\n-  \/\/ TODO: PPC port if (MatchDecodeNodes) return true;\n-  return false;\n-}\n-\n-bool Matcher::narrow_klass_use_complex_address() {\n-  NOT_LP64(ShouldNotCallThis());\n-  assert(UseCompressedClassPointers, \"only for compressed klass code\");\n-  \/\/ TODO: PPC port if (MatchDecodeNodes) return true;\n-  return false;\n-}\n-\n-bool Matcher::const_oop_prefer_decode() {\n-  \/\/ Prefer ConN+DecodeN over ConP in simple compressed oops mode.\n-  return CompressedOops::base() == NULL;\n-}\n-\n-bool Matcher::const_klass_prefer_decode() {\n-  \/\/ Prefer ConNKlass+DecodeNKlass over ConP in simple compressed klass mode.\n-  return CompressedKlassPointers::base() == NULL;\n-}\n-\n-\/\/ Is it better to copy float constants, or load them directly from memory?\n-\/\/ Intel can load a float constant from a direct address, requiring no\n-\/\/ extra registers. Most RISCs will have to materialize an address into a\n-\/\/ register first, so they would do better to copy the constant from stack.\n-const bool Matcher::rematerialize_float_constants = false;\n-\n-\/\/ If CPU can load and store mis-aligned doubles directly then no fixup is\n-\/\/ needed. Else we split the double into 2 integer pieces and move it\n-\/\/ piece-by-piece. Only happens when passing doubles into C code as the\n-\/\/ Java calling convention forces doubles to be aligned.\n-const bool Matcher::misaligned_doubles_ok = true;\n-\n-\/\/ Advertise here if the CPU requires explicit rounding operations to implement strictfp mode.\n-const bool Matcher::strict_fp_requires_explicit_rounding = false;\n-\n-\/\/ Do floats take an entire double register or just half?\n-\/\/\n-\/\/ A float occupies a ppc64 double register. For the allocator, a\n-\/\/ ppc64 double register appears as a pair of float registers.\n-bool Matcher::float_in_double() { return true; }\n-\n-\/\/ Do ints take an entire long register or just half?\n-\/\/ The relevant question is how the int is callee-saved:\n-\/\/ the whole long is written but de-opt'ing will have to extract\n-\/\/ the relevant 32 bits.\n-const bool Matcher::int_in_long = true;\n-\n@@ -2500,2 +2388,0 @@\n-const bool Matcher::convi2l_type_required = true;\n-\n","filename":"src\/hotspot\/cpu\/ppc\/ppc.ad","additions":0,"deletions":114,"binary":false,"changes":114,"status":"modified"},{"patch":"@@ -0,0 +1,142 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef CPU_S390_MATCHER_S390_HPP\n+#define CPU_S390_MATCHER_S390_HPP\n+\n+  \/\/ Defined within class Matcher\n+\n+\n+  \/\/ Should correspond to setting above\n+  static const bool init_array_count_is_in_bytes = false;\n+\n+  \/\/ Whether this platform implements the scalable vector feature\n+  static const bool implements_scalable_vector = false;\n+\n+  static constexpr const bool supports_scalable_vector() {\n+    return false;\n+  }\n+\n+  \/\/ z\/Architecture does support misaligned store\/load at minimal extra cost.\n+  static constexpr bool misaligned_vectors_ok() {\n+    return true;\n+  }\n+\n+  \/\/ Whether code generation need accurate ConvI2L types.\n+  static const bool convi2l_type_required = true;\n+\n+  \/\/ Do the processor's shift instructions only use the low 5\/6 bits\n+  \/\/ of the count for 32\/64 bit ints? If not we need to do the masking\n+  \/\/ ourselves.\n+  static const bool need_masked_shift_count = false;\n+\n+  \/\/ Does the CPU require late expand (see block.cpp for description of late expand)?\n+  static const bool require_postalloc_expand = false;\n+\n+  \/\/ No support for generic vector operands.\n+  static const bool supports_generic_vector_operands = false;\n+\n+  static constexpr bool isSimpleConstant64(jlong value) {\n+    \/\/ Probably always true, even if a temp register is required.\n+    return true;\n+  }\n+\n+  \/\/ Suppress CMOVL. Conditional move available on z\/Architecture only from z196 onwards. Not exploited yet.\n+  static const int long_cmove_cost() { return ConditionalMoveLimit; }\n+\n+  \/\/ Suppress CMOVF. Conditional move available on z\/Architecture only from z196 onwards. Not exploited yet.\n+  static const int float_cmove_cost() { return ConditionalMoveLimit; }\n+\n+  \/\/ Set this as clone_shift_expressions.\n+  static bool narrow_oop_use_complex_address() {\n+    if (CompressedOops::base() == NULL && CompressedOops::shift() == 0) return true;\n+    return false;\n+  }\n+\n+  static bool narrow_klass_use_complex_address() {\n+    NOT_LP64(ShouldNotCallThis());\n+    assert(UseCompressedClassPointers, \"only for compressed klass code\");\n+    \/\/ TODO HS25: z port if (MatchDecodeNodes) return true;\n+    return false;\n+  }\n+\n+  static bool const_oop_prefer_decode() {\n+    \/\/ Prefer ConN+DecodeN over ConP in simple compressed oops mode.\n+    return CompressedOops::base() == NULL;\n+  }\n+\n+  static bool const_klass_prefer_decode() {\n+    \/\/ Prefer ConNKlass+DecodeNKlass over ConP in simple compressed klass mode.\n+    return CompressedKlassPointers::base() == NULL;\n+  }\n+\n+  \/\/ Is it better to copy float constants, or load them directly from memory?\n+  \/\/ Most RISCs will have to materialize an address into a\n+  \/\/ register first, so they would do better to copy the constant from stack.\n+  static const bool rematerialize_float_constants = false;\n+\n+  \/\/ If CPU can load and store mis-aligned doubles directly then no fixup is\n+  \/\/ needed. Else we split the double into 2 integer pieces and move it\n+  \/\/ piece-by-piece. Only happens when passing doubles into C code as the\n+  \/\/ Java calling convention forces doubles to be aligned.\n+  static const bool misaligned_doubles_ok = true;\n+\n+  \/\/ Advertise here if the CPU requires explicit rounding operations to implement strictfp mode.\n+  static const bool strict_fp_requires_explicit_rounding = false;\n+\n+  \/\/ Do floats take an entire double register or just half?\n+  \/\/\n+  \/\/ A float in resides in a zarch double register. When storing it by\n+  \/\/ z_std, it cannot be restored in C-code by reloading it as a double\n+  \/\/ and casting it into a float afterwards.\n+  static constexpr bool float_in_double() { return false; }\n+\n+  \/\/ Do ints take an entire long register or just half?\n+  \/\/ The relevant question is how the int is callee-saved:\n+  \/\/ the whole long is written but de-opt'ing will have to extract\n+  \/\/ the relevant 32 bits.\n+  static const bool int_in_long = true;\n+\n+  \/\/ Does the CPU supports vector variable shift instructions?\n+  static constexpr bool supports_vector_variable_shifts(void) {\n+    return false;\n+  }\n+\n+  \/\/ Does the CPU supports vector variable rotate instructions?\n+  static constexpr bool supports_vector_variable_rotates(void) {\n+    return false;\n+  }\n+\n+  \/\/ Some microarchitectures have mask registers used on vectors\n+  static constexpr bool has_predicated_vectors(void) {\n+    return false;\n+  }\n+\n+  \/\/ true means we have fast l2f convers\n+  \/\/ false means that conversion is done by runtime call\n+  static constexpr bool convL2FSupported(void) {\n+      return true;\n+  }\n+\n+#endif \/\/ CPU_S390_MATCHER_S390_HPP\n","filename":"src\/hotspot\/cpu\/s390\/matcher_s390.hpp","additions":142,"deletions":0,"binary":false,"changes":142,"status":"added"},{"patch":"@@ -1539,4 +1539,0 @@\n-const bool Matcher::has_predicated_vectors(void) {\n-  return false;\n-}\n-\n@@ -1551,8 +1547,0 @@\n-bool Matcher::supports_vector_variable_shifts(void) {\n-  return false; \/\/ not supported\n-}\n-\n-bool Matcher::supports_vector_variable_rotates(void) {\n-  return false; \/\/ not supported\n-}\n-\n@@ -1563,4 +1551,0 @@\n-const bool Matcher::convL2FSupported(void) {\n-  return true; \/\/ False means that conversion is done by runtime call.\n-}\n-\n@@ -1591,4 +1575,0 @@\n-const bool Matcher::supports_scalable_vector() {\n-  return false;\n-}\n-\n@@ -1599,5 +1579,0 @@\n-\/\/ z\/Architecture does support misaligned store\/load at minimal extra cost.\n-const bool Matcher::misaligned_vectors_ok() {\n-  return true;\n-}\n-\n@@ -1622,26 +1597,0 @@\n-const bool Matcher::isSimpleConstant64(jlong value) {\n-  \/\/ Probably always true, even if a temp register is required.\n-  return true;\n-}\n-\n-\/\/ Should correspond to setting above\n-const bool Matcher::init_array_count_is_in_bytes = false;\n-\n-\/\/ Suppress CMOVL. Conditional move available on z\/Architecture only from z196 onwards. Not exploited yet.\n-const int Matcher::long_cmove_cost() { return ConditionalMoveLimit; }\n-\n-\/\/ Suppress CMOVF. Conditional move available on z\/Architecture only from z196 onwards. Not exploited yet.\n-const int Matcher::float_cmove_cost() { return ConditionalMoveLimit; }\n-\n-\/\/ Does the CPU require postalloc expand (see block.cpp for description of postalloc expand)?\n-const bool Matcher::require_postalloc_expand = false;\n-\n-\/\/ Do we need to mask the count passed to shift instructions or does\n-\/\/ the cpu only look at the lower 5\/6 bits anyway?\n-\/\/ 32bit shifts mask in emitter, 64bit shifts need no mask.\n-\/\/ Constant shift counts are handled in Ideal phase.\n-const bool Matcher::need_masked_shift_count = false;\n-\n-\/\/ No support for generic vector operands.\n-const bool Matcher::supports_generic_vector_operands  = false;\n-\n@@ -1663,50 +1612,0 @@\n-\/\/ Set this as clone_shift_expressions.\n-bool Matcher::narrow_oop_use_complex_address() {\n-  if (CompressedOops::base() == NULL && CompressedOops::shift() == 0) return true;\n-  return false;\n-}\n-\n-bool Matcher::narrow_klass_use_complex_address() {\n-  NOT_LP64(ShouldNotCallThis());\n-  assert(UseCompressedClassPointers, \"only for compressed klass code\");\n-  \/\/ TODO HS25: z port if (MatchDecodeNodes) return true;\n-  return false;\n-}\n-\n-bool Matcher::const_oop_prefer_decode() {\n-  \/\/ Prefer ConN+DecodeN over ConP in simple compressed oops mode.\n-  return CompressedOops::base() == NULL;\n-}\n-\n-bool Matcher::const_klass_prefer_decode() {\n-  \/\/ Prefer ConNKlass+DecodeNKlass over ConP in simple compressed klass mode.\n-  return CompressedKlassPointers::base() == NULL;\n-}\n-\n-\/\/ Is it better to copy float constants, or load them directly from memory?\n-\/\/ Most RISCs will have to materialize an address into a\n-\/\/ register first, so they would do better to copy the constant from stack.\n-const bool Matcher::rematerialize_float_constants = false;\n-\n-\/\/ If CPU can load and store mis-aligned doubles directly then no fixup is\n-\/\/ needed. Else we split the double into 2 integer pieces and move it\n-\/\/ piece-by-piece. Only happens when passing doubles into C code as the\n-\/\/ Java calling convention forces doubles to be aligned.\n-const bool Matcher::misaligned_doubles_ok = true;\n-\n-\/\/ Advertise here if the CPU requires explicit rounding operations to implement strictfp mode.\n-const bool Matcher::strict_fp_requires_explicit_rounding = false;\n-\n-\/\/ Do floats take an entire double register or just half?\n-\/\/\n-\/\/ A float in resides in a zarch double register. When storing it by\n-\/\/ z_std, it cannot be restored in C-code by reloading it as a double\n-\/\/ and casting it into a float afterwards.\n-bool Matcher::float_in_double() { return false; }\n-\n-\/\/ Do ints take an entire long register or just half?\n-\/\/ The relevant question is how the int is callee-saved:\n-\/\/ the whole long is written but de-opt'ing will have to extract\n-\/\/ the relevant 32 bits.\n-const bool Matcher::int_in_long = true;\n-\n@@ -1788,2 +1687,0 @@\n-const bool Matcher::convi2l_type_required = true;\n-\n","filename":"src\/hotspot\/cpu\/s390\/s390.ad","additions":0,"deletions":103,"binary":false,"changes":103,"status":"modified"},{"patch":"@@ -0,0 +1,176 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef CPU_X86_MATCHER_X86_HPP\n+#define CPU_X86_MATCHER_X86_HPP\n+\n+  \/\/ Defined within class Matcher\n+\n+  \/\/ The ecx parameter to rep stosq for the ClearArray node is in words.\n+  static const bool init_array_count_is_in_bytes = false;\n+\n+  \/\/ Whether this platform implements the scalable vector feature\n+  static const bool implements_scalable_vector = false;\n+\n+  static constexpr bool supports_scalable_vector() {\n+    return false;\n+  }\n+\n+  \/\/ x86 supports misaligned vectors store\/load.\n+  static constexpr bool misaligned_vectors_ok() {\n+    return true;\n+  }\n+\n+  \/\/ Whether code generation need accurate ConvI2L types.\n+  static const bool convi2l_type_required = true;\n+\n+  \/\/ Do the processor's shift instructions only use the low 5\/6 bits\n+  \/\/ of the count for 32\/64 bit ints? If not we need to do the masking\n+  \/\/ ourselves.\n+  static const bool need_masked_shift_count = false;\n+\n+  \/\/ Does the CPU require late expand (see block.cpp for description of late expand)?\n+  static const bool require_postalloc_expand = false;\n+\n+  \/\/ x86 supports generic vector operands: vec and legVec.\n+  static const bool supports_generic_vector_operands = true;\n+\n+  static constexpr bool isSimpleConstant64(jlong value) {\n+    \/\/ Will one (StoreL ConL) be cheaper than two (StoreI ConI)?.\n+    \/\/return value == (int) value;  \/\/ Cf. storeImmL and immL32.\n+\n+    \/\/ Probably always true, even if a temp register is required.\n+#ifdef _LP64\n+    return true;\n+#else\n+    return false;\n+#endif\n+  }\n+\n+#ifdef _LP64\n+  \/\/ No additional cost for CMOVL.\n+  static constexpr int long_cmove_cost() { return 0; }\n+#else\n+  \/\/ Needs 2 CMOV's for longs.\n+  static constexpr int long_cmove_cost() { return 1; }\n+#endif\n+\n+#ifdef _LP64\n+  \/\/ No CMOVF\/CMOVD with SSE2\n+  static int float_cmove_cost() { return ConditionalMoveLimit; }\n+#else\n+  \/\/ No CMOVF\/CMOVD with SSE\/SSE2\n+  static int float_cmove_cost() { return (UseSSE>=1) ? ConditionalMoveLimit : 0; }\n+#endif\n+\n+  static bool narrow_oop_use_complex_address() {\n+    NOT_LP64(ShouldNotCallThis();)\n+    assert(UseCompressedOops, \"only for compressed oops code\");\n+    return (LogMinObjAlignmentInBytes <= 3);\n+  }\n+\n+  static bool narrow_klass_use_complex_address() {\n+    NOT_LP64(ShouldNotCallThis();)\n+    assert(UseCompressedClassPointers, \"only for compressed klass code\");\n+    return (LogKlassAlignmentInBytes <= 3);\n+  }\n+\n+  \/\/ Prefer ConN+DecodeN over ConP.\n+  static const bool const_oop_prefer_decode() {\n+    NOT_LP64(ShouldNotCallThis();)\n+    \/\/ Prefer ConN+DecodeN over ConP.\n+    return true;\n+  }\n+\n+  \/\/ Prefer ConP over ConNKlass+DecodeNKlass.\n+  static const bool const_klass_prefer_decode() {\n+    NOT_LP64(ShouldNotCallThis();)\n+    return false;\n+  }\n+\n+  \/\/ Is it better to copy float constants, or load them directly from memory?\n+  \/\/ Intel can load a float constant from a direct address, requiring no\n+  \/\/ extra registers.  Most RISCs will have to materialize an address into a\n+  \/\/ register first, so they would do better to copy the constant from stack.\n+  static const bool rematerialize_float_constants = true;\n+\n+  \/\/ If CPU can load and store mis-aligned doubles directly then no fixup is\n+  \/\/ needed.  Else we split the double into 2 integer pieces and move it\n+  \/\/ piece-by-piece.  Only happens when passing doubles into C code as the\n+  \/\/ Java calling convention forces doubles to be aligned.\n+  static const bool misaligned_doubles_ok = true;\n+\n+  \/\/ Advertise here if the CPU requires explicit rounding operations to implement strictfp mode.\n+#ifdef _LP64\n+  static const bool strict_fp_requires_explicit_rounding = false;\n+#else\n+  static const bool strict_fp_requires_explicit_rounding = true;\n+#endif\n+\n+  \/\/ Are floats converted to double when stored to stack during deoptimization?\n+  \/\/ On x64 it is stored without convertion so we can use normal access.\n+  \/\/ On x32 it is stored with convertion only when FPU is used for floats.\n+#ifdef _LP64\n+  static constexpr bool float_in_double() {\n+    return false;\n+  }\n+#else\n+  static bool float_in_double() {\n+    return (UseSSE == 0);\n+  }\n+#endif\n+\n+  \/\/ Do ints take an entire long register or just half?\n+#ifdef _LP64\n+  static const bool int_in_long = true;\n+#else\n+  static const bool int_in_long = false;\n+#endif\n+\n+  \/\/ Does the CPU supports vector variable shift instructions?\n+  static bool supports_vector_variable_shifts(void) {\n+    return (UseAVX >= 2);\n+  }\n+\n+  \/\/ Does the CPU supports vector variable rotate instructions?\n+  static constexpr bool supports_vector_variable_rotates(void) {\n+    return true;\n+  }\n+\n+  \/\/ Some microarchitectures have mask registers used on vectors\n+  static const bool has_predicated_vectors(void) {\n+    bool ret_value = false;\n+    if (UseAVX > 2) {\n+      ret_value = VM_Version::supports_avx512vl();\n+    }\n+    return ret_value;\n+  }\n+\n+  \/\/ true means we have fast l2f convers\n+  \/\/ false means that conversion is done by runtime call\n+  static constexpr bool convL2FSupported(void) {\n+      return true;\n+  }\n+\n+#endif \/\/ CPU_X86_MATCHER_X86_HPP\n","filename":"src\/hotspot\/cpu\/x86\/matcher_x86.hpp","additions":176,"deletions":0,"binary":false,"changes":176,"status":"added"},{"patch":"@@ -1824,3 +1824,0 @@\n-\/\/ x86 supports generic vector operands: vec and legVec.\n-const bool Matcher::supports_generic_vector_operands = true;\n-\n@@ -1878,17 +1875,0 @@\n-bool Matcher::supports_vector_variable_shifts(void) {\n-  return (UseAVX >= 2);\n-}\n-\n-bool Matcher::supports_vector_variable_rotates(void) {\n-  return true;\n-}\n-\n-const bool Matcher::has_predicated_vectors(void) {\n-  bool ret_value = false;\n-  if (UseAVX > 2) {\n-    ret_value = VM_Version::supports_avx512vl();\n-  }\n-\n-  return ret_value;\n-}\n-\n@@ -1969,4 +1949,0 @@\n-const bool Matcher::supports_scalable_vector() {\n-  return false;\n-}\n-\n@@ -1991,8 +1967,0 @@\n-\/\/ x86 supports misaligned vectors store\/load.\n-const bool Matcher::misaligned_vectors_ok() {\n-  return true;\n-}\n-\n-\n-const bool Matcher::convi2l_type_required = true;\n-\n","filename":"src\/hotspot\/cpu\/x86\/x86.ad","additions":0,"deletions":32,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -1401,5 +1401,0 @@\n-\/\/ This is UltraSparc specific, true just means we have fast l2f conversion\n-const bool Matcher::convL2FSupported(void) {\n-  return true;\n-}\n-\n@@ -1423,63 +1418,0 @@\n-const bool Matcher::isSimpleConstant64(jlong value) {\n-  \/\/ Will one (StoreL ConL) be cheaper than two (StoreI ConI)?.\n-  return false;\n-}\n-\n-\/\/ The ecx parameter to rep stos for the ClearArray node is in dwords.\n-const bool Matcher::init_array_count_is_in_bytes = false;\n-\n-\/\/ Needs 2 CMOV's for longs.\n-const int Matcher::long_cmove_cost() { return 1; }\n-\n-\/\/ No CMOVF\/CMOVD with SSE\/SSE2\n-const int Matcher::float_cmove_cost() { return (UseSSE>=1) ? ConditionalMoveLimit : 0; }\n-\n-\/\/ Does the CPU require late expand (see block.cpp for description of late expand)?\n-const bool Matcher::require_postalloc_expand = false;\n-\n-\/\/ Do we need to mask the count passed to shift instructions or does\n-\/\/ the cpu only look at the lower 5\/6 bits anyway?\n-const bool Matcher::need_masked_shift_count = false;\n-\n-bool Matcher::narrow_oop_use_complex_address() {\n-  ShouldNotCallThis();\n-  return true;\n-}\n-\n-bool Matcher::narrow_klass_use_complex_address() {\n-  ShouldNotCallThis();\n-  return true;\n-}\n-\n-bool Matcher::const_oop_prefer_decode() {\n-  ShouldNotCallThis();\n-  return true;\n-}\n-\n-bool Matcher::const_klass_prefer_decode() {\n-  ShouldNotCallThis();\n-  return true;\n-}\n-\n-\/\/ Is it better to copy float constants, or load them directly from memory?\n-\/\/ Intel can load a float constant from a direct address, requiring no\n-\/\/ extra registers.  Most RISCs will have to materialize an address into a\n-\/\/ register first, so they would do better to copy the constant from stack.\n-const bool Matcher::rematerialize_float_constants = true;\n-\n-\/\/ If CPU can load and store mis-aligned doubles directly then no fixup is\n-\/\/ needed.  Else we split the double into 2 integer pieces and move it\n-\/\/ piece-by-piece.  Only happens when passing doubles into C code as the\n-\/\/ Java calling convention forces doubles to be aligned.\n-const bool Matcher::misaligned_doubles_ok = true;\n-\n-\/\/ Advertise here if the CPU requires explicit rounding operations to implement strictfp mode.\n-const bool Matcher::strict_fp_requires_explicit_rounding = true;\n-\n-\/\/ Are floats conerted to double when stored to stack during deoptimization?\n-\/\/ On x32 it is stored with convertion only when FPU is used for floats.\n-bool Matcher::float_in_double() { return (UseSSE == 0); }\n-\n-\/\/ Do ints take an entire long register or just half?\n-const bool Matcher::int_in_long = false;\n-\n","filename":"src\/hotspot\/cpu\/x86\/x86_32.ad","additions":0,"deletions":68,"binary":false,"changes":68,"status":"modified"},{"patch":"@@ -1695,5 +1695,0 @@\n-\/\/ This is UltraSparc specific, true just means we have fast l2f conversion\n-const bool Matcher::convL2FSupported(void) {\n-  return true;\n-}\n-\n@@ -1717,67 +1712,0 @@\n-const bool Matcher::isSimpleConstant64(jlong value) {\n-  \/\/ Will one (StoreL ConL) be cheaper than two (StoreI ConI)?.\n-  \/\/return value == (int) value;  \/\/ Cf. storeImmL and immL32.\n-\n-  \/\/ Probably always true, even if a temp register is required.\n-  return true;\n-}\n-\n-\/\/ The ecx parameter to rep stosq for the ClearArray node is in words.\n-const bool Matcher::init_array_count_is_in_bytes = false;\n-\n-\/\/ No additional cost for CMOVL.\n-const int Matcher::long_cmove_cost() { return 0; }\n-\n-\/\/ No CMOVF\/CMOVD with SSE2\n-const int Matcher::float_cmove_cost() { return ConditionalMoveLimit; }\n-\n-\/\/ Does the CPU require late expand (see block.cpp for description of late expand)?\n-const bool Matcher::require_postalloc_expand = false;\n-\n-\/\/ Do we need to mask the count passed to shift instructions or does\n-\/\/ the cpu only look at the lower 5\/6 bits anyway?\n-const bool Matcher::need_masked_shift_count = false;\n-\n-bool Matcher::narrow_oop_use_complex_address() {\n-  assert(UseCompressedOops, \"only for compressed oops code\");\n-  return (LogMinObjAlignmentInBytes <= 3);\n-}\n-\n-bool Matcher::narrow_klass_use_complex_address() {\n-  assert(UseCompressedClassPointers, \"only for compressed klass code\");\n-  return (LogKlassAlignmentInBytes <= 3);\n-}\n-\n-bool Matcher::const_oop_prefer_decode() {\n-  \/\/ Prefer ConN+DecodeN over ConP.\n-  return true;\n-}\n-\n-bool Matcher::const_klass_prefer_decode() {\n-  \/\/ Prefer ConP over ConNKlass+DecodeNKlass.\n-  return false;\n-}\n-\n-\/\/ Is it better to copy float constants, or load them directly from\n-\/\/ memory?  Intel can load a float constant from a direct address,\n-\/\/ requiring no extra registers.  Most RISCs will have to materialize\n-\/\/ an address into a register first, so they would do better to copy\n-\/\/ the constant from stack.\n-const bool Matcher::rematerialize_float_constants = true; \/\/ XXX\n-\n-\/\/ If CPU can load and store mis-aligned doubles directly then no\n-\/\/ fixup is needed.  Else we split the double into 2 integer pieces\n-\/\/ and move it piece-by-piece.  Only happens when passing doubles into\n-\/\/ C code as the Java calling convention forces doubles to be aligned.\n-const bool Matcher::misaligned_doubles_ok = true;\n-\n-\/\/ Advertise here if the CPU requires explicit rounding operations to implement strictfp mode.\n-const bool Matcher::strict_fp_requires_explicit_rounding = false;\n-\n-\/\/ Are floats conerted to double when stored to stack during deoptimization?\n-\/\/ On x64 it is stored without convertion so we can use normal access.\n-bool Matcher::float_in_double() { return false; }\n-\n-\/\/ Do ints take an entire long register or just half?\n-const bool Matcher::int_in_long = true;\n-\n","filename":"src\/hotspot\/cpu\/x86\/x86_64.ad","additions":0,"deletions":72,"binary":false,"changes":72,"status":"modified"},{"patch":"@@ -816,1 +816,1 @@\n-          if (ireg == Op_VecA) {\n+          if (Matcher::implements_scalable_vector && ireg == Op_VecA) {\n","filename":"src\/hotspot\/share\/opto\/chaitin.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -170,1 +170,1 @@\n-    return _is_scalable;\n+    return Matcher::implements_scalable_vector && _is_scalable;\n","filename":"src\/hotspot\/share\/opto\/chaitin.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"oops\/compressedOops.hpp\"\n@@ -33,0 +34,1 @@\n+#include \"runtime\/vm_version.hpp\"\n@@ -46,0 +48,3 @@\n+  \/\/ Machine-dependent definitions\n+#include CPU_HEADER(matcher)\n+\n@@ -323,2 +328,0 @@\n-  \/\/ Some microarchitectures have mask registers used on vectors\n-  static const bool has_predicated_vectors(void);\n@@ -331,4 +334,0 @@\n-  \/\/ Used to determine if we have fast l2f conversion\n-  \/\/ USII has it, USIII doesn't\n-  static const bool convL2FSupported(void);\n-\n@@ -346,1 +345,0 @@\n-  static const bool supports_scalable_vector();\n@@ -353,14 +351,0 @@\n-  \/\/ Does the CPU supports vector variable shift instructions?\n-  static bool supports_vector_variable_shifts(void);\n-\n-  \/\/ Does the CPU supports vector vairable rotate instructions?\n-  static bool supports_vector_variable_rotates(void);\n-\n-  \/\/ CPU supports misaligned vectors store\/load.\n-  static const bool misaligned_vectors_ok();\n-\n-  \/\/ Used to determine a \"low complexity\" 64-bit constant.  (Zero is simple.)\n-  \/\/ The standard of comparison is one (StoreL ConL) vs. two (StoreI ConI).\n-  \/\/ Depends on the details of 64-bit constant generation on the CPU.\n-  static const bool isSimpleConstant64(jlong con);\n-\n@@ -437,9 +421,0 @@\n-  \/\/ Optional scaling for the parameter to the ClearArray\/CopyArray node.\n-  static const bool init_array_count_is_in_bytes;\n-\n-  \/\/ Some hardware needs 2 CMOV's for longs.\n-  static const int long_cmove_cost();\n-\n-  \/\/ Some hardware have expensive CMOV for float and double.\n-  static const int float_cmove_cost();\n-\n@@ -458,6 +433,0 @@\n-  static bool narrow_oop_use_complex_address();\n-  static bool narrow_klass_use_complex_address();\n-\n-  static bool const_oop_prefer_decode();\n-  static bool const_klass_prefer_decode();\n-\n@@ -490,21 +459,0 @@\n-  \/\/ Is it better to copy float constants, or load them directly from memory?\n-  \/\/ Intel can load a float constant from a direct address, requiring no\n-  \/\/ extra registers.  Most RISCs will have to materialize an address into a\n-  \/\/ register first, so they may as well materialize the constant immediately.\n-  static const bool rematerialize_float_constants;\n-\n-  \/\/ If CPU can load and store mis-aligned doubles directly then no fixup is\n-  \/\/ needed.  Else we split the double into 2 integer pieces and move it\n-  \/\/ piece-by-piece.  Only happens when passing doubles into C code or when\n-  \/\/ calling i2c adapters as the Java calling convention forces doubles to be\n-  \/\/ aligned.\n-  static const bool misaligned_doubles_ok;\n-\n-  \/\/ Does the CPU require postalloc expand (see block.cpp for description of\n-  \/\/ postalloc expand)?\n-  static const bool require_postalloc_expand;\n-\n-  \/\/ Does the platform support generic vector operands?\n-  \/\/ Requires cleanup after selection phase.\n-  static const bool supports_generic_vector_operands;\n-\n@@ -529,16 +477,0 @@\n-  \/\/ Advertise here if the CPU requires explicit rounding operations to implement strictfp mode.\n-  static const bool strict_fp_requires_explicit_rounding;\n-\n-  \/\/ Are floats conerted to double when stored to stack during deoptimization?\n-  static bool float_in_double();\n-  \/\/ Do ints take an entire long register or just half?\n-  static const bool int_in_long;\n-\n-  \/\/ Do the processor's shift instructions only use the low 5\/6 bits\n-  \/\/ of the count for 32\/64 bit ints? If not we need to do the masking\n-  \/\/ ourselves.\n-  static const bool need_masked_shift_count;\n-\n-  \/\/ Whether code generation need accurate ConvI2L types.\n-  static const bool convi2l_type_required;\n-\n","filename":"src\/hotspot\/share\/opto\/matcher.hpp","additions":5,"deletions":73,"binary":false,"changes":78,"status":"modified"}]}