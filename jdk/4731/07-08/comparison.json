{"files":[{"patch":"@@ -79,0 +79,1 @@\n+#include \"services\/finalizerService.hpp\"\n@@ -88,3 +89,0 @@\n-#if INCLUDE_MANAGEMENT\n-#include \"services\/finalizerTable.hpp\"\n-#endif\n@@ -1606,1 +1604,1 @@\n-      MANAGEMENT_ONLY(FinalizerTable::purge_unloaded();)\n+      MANAGEMENT_ONLY(FinalizerService::purge_unloaded();)\n","filename":"src\/hotspot\/share\/classfile\/systemDictionary.cpp","additions":2,"deletions":4,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -44,1 +44,1 @@\n-#include \"services\/finalizerTable.hpp\"\n+#include \"services\/finalizerService.hpp\"\n@@ -359,4 +359,1 @@\n-    assert(referent->is_instance(), \"invariant\");\n-    const InstanceKlass* const ik = InstanceKlass::cast(referent->klass());\n-    assert(ik->has_finalizer(), \"invariant\");\n-    FinalizerTable::on_enqueue(ik);\n+    FinalizerService::on_enqueue(referent);\n","filename":"src\/hotspot\/share\/gc\/shared\/referenceProcessor.cpp","additions":2,"deletions":5,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -38,1 +38,1 @@\n-#include \"services\/finalizerTable.hpp\"\n+#include \"services\/finalizerService.hpp\"\n@@ -108,1 +108,1 @@\n-  send_event(FinalizerTable::lookup(ik, thread), ik, JfrTicks::now(), thread);\n+  send_event(FinalizerService::lookup(ik, thread), ik, JfrTicks::now(), thread);\n@@ -132,1 +132,1 @@\n-  FinalizerTable::do_entries(&fec, thread);\n+  FinalizerService::do_entries(&fec, thread);\n","filename":"src\/hotspot\/share\/jfr\/periodic\/jfrFinalizerEvent.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -74,1 +74,1 @@\n-  LOG_TAG(finalizertable) \\\n+  LOG_TAG(finalizer) \\\n","filename":"src\/hotspot\/share\/logging\/logTag.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -86,0 +86,1 @@\n+#include \"services\/finalizerService.hpp\"\n@@ -98,4 +99,0 @@\n-#if INCLUDE_MANAGEMENT\n-#include \"services\/finalizerTable.hpp\"\n-#endif\n-\n@@ -1413,1 +1410,1 @@\n-  MANAGEMENT_ONLY(FinalizerTable::on_register(h_i, THREAD);)\n+  MANAGEMENT_ONLY(FinalizerService::on_register(h_i(), THREAD);)\n","filename":"src\/hotspot\/share\/oops\/instanceKlass.cpp","additions":2,"deletions":5,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -69,1 +69,1 @@\n-#include \"services\/finalizerTable.hpp\"\n+#include \"services\/finalizerService.hpp\"\n@@ -596,1 +596,1 @@\n-      if (FinalizerTable::needs_rehashing()) {\n+      if (FinalizerService::needs_rehashing()) {\n@@ -598,1 +598,1 @@\n-        FinalizerTable::rehash_table();\n+        FinalizerService::rehash();\n","filename":"src\/hotspot\/share\/runtime\/safepoint.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -50,1 +50,1 @@\n-#include \"services\/finalizerTable.hpp\"\n+#include \"services\/finalizerService.hpp\"\n@@ -119,1 +119,1 @@\n-    bool finalizertable_work = false;\n+    bool finalizerservice_work = false;\n@@ -150,1 +150,1 @@\n-              (finalizertable_work = FinalizerTable::has_work()) |\n+              (finalizerservice_work = FinalizerService::has_work()) |\n@@ -178,2 +178,2 @@\n-    if (finalizertable_work) {\n-      FinalizerTable::do_concurrent_work(jt);\n+    if (finalizerservice_work) {\n+      FinalizerService::do_concurrent_work(jt);\n","filename":"src\/hotspot\/share\/runtime\/serviceThread.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -0,0 +1,502 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#if INCLUDE_MANAGEMENT\n+#include \"classfile\/classLoaderDataGraph.inline.hpp\"\n+#include \"logging\/log.hpp\"\n+#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/handles.hpp\"\n+#include \"runtime\/interfaceSupport.inline.hpp\"\n+#include \"runtime\/javaCalls.hpp\"\n+#include \"runtime\/mutexLocker.hpp\"\n+#include \"runtime\/synchronizer.hpp\"\n+#include \"runtime\/thread.inline.hpp\"\n+#include \"runtime\/vm_version.hpp\"\n+#include \"services\/finalizerService.hpp\"\n+#include \"utilities\/concurrentHashTableTasks.inline.hpp\"\n+#include \"utilities\/debug.hpp\"\n+\n+FinalizerEntry::FinalizerEntry(const InstanceKlass* ik) :\n+    _ik(ik),\n+    _registered(0),\n+    _enqueued(0),\n+    _finalized(0) {}\n+\n+static inline void atomic_inc(uint64_t* volatile dest) {\n+  assert(VM_Version::supports_cx8(), \"invariant\");\n+  uint64_t compare;\n+  uint64_t exchange;\n+  do {\n+    compare = *dest;\n+    exchange = compare + 1;\n+  } while (Atomic::cmpxchg(dest, compare, exchange) != compare);\n+}\n+\n+const InstanceKlass* FinalizerEntry::klass() const {\n+  return _ik;\n+}\n+\n+uint64_t FinalizerEntry::registered() const {\n+  return Atomic::load(&_registered);\n+}\n+\n+uint64_t FinalizerEntry::enqueued() const {\n+  return Atomic::load(&_enqueued);\n+}\n+\n+uint64_t FinalizerEntry::finalized() const {\n+  return Atomic::load(&_finalized);\n+}\n+\n+void FinalizerEntry::on_register() {\n+  atomic_inc(&_registered);\n+}\n+\n+void FinalizerEntry::on_enqueue() {\n+  atomic_inc(&_enqueued);\n+}\n+\n+void FinalizerEntry::on_complete() {\n+  atomic_inc(&_finalized);\n+}\n+\n+static constexpr const size_t DEFAULT_TABLE_SIZE = 2048;\n+\/\/ 2^24 is max size, like StringTable.\n+static constexpr const size_t MAX_SIZE = 24;\n+\/\/ If a chain gets to 50, something might be wrong\n+static constexpr const size_t REHASH_LEN = 50;\n+static constexpr const double PREF_AVG_LIST_LEN = 8.0;\n+\n+static size_t _table_size = 0;\n+static volatile uint64_t _entries = 0;\n+static volatile uint64_t _count = 0;\n+static volatile bool _has_work = 0;\n+static volatile bool _needs_rehashing = false;\n+static volatile bool _has_items_to_clean = false;\n+\n+static inline void reset_has_items_to_clean() {\n+  Atomic::store(&_has_items_to_clean, false);\n+}\n+\n+static inline void set_has_items_to_clean() {\n+  Atomic::store(&_has_items_to_clean, true);\n+}\n+\n+static inline bool has_items_to_clean() {\n+  return Atomic::load(&_has_items_to_clean);\n+}\n+\n+static inline void added() {\n+  Atomic::inc(&_count);\n+}\n+\n+static inline void removed() {\n+  Atomic::dec(&_count);\n+}\n+\n+static inline uintx hash_function(const InstanceKlass* ik) {\n+  assert(ik != nullptr, \"invariant\");\n+  return primitive_hash(ik);\n+}\n+\n+static inline uintx hash_function(const FinalizerEntry* fe) {\n+  return hash_function(fe->klass());\n+}\n+\n+class FinalizerEntryLookup : StackObj {\n+ private:\n+  const InstanceKlass* const _ik;\n+ public:\n+  FinalizerEntryLookup(const InstanceKlass* ik) : _ik(ik) {}\n+  uintx get_hash() const { return hash_function(_ik); }\n+  bool equals(FinalizerEntry** value, bool* is_dead) {\n+    assert(value != nullptr, \"invariant\");\n+    assert(*value != nullptr, \"invariant\");\n+    return (*value)->klass() == _ik;\n+  }\n+};\n+\n+class FinalizerTableConfig : public AllStatic {\n+ public:\n+  typedef FinalizerEntry* Value;  \/\/ value of the Node in the hashtable\n+\n+  static uintx get_hash(Value const& value, bool* is_dead) {\n+    return hash_function(value);\n+  }\n+  \/\/ We use default allocation\/deallocation but counted\n+  static void* allocate_node(void* context, size_t size, Value const& value) {\n+    added();\n+    return AllocateHeap(size, mtClass);\n+  }\n+  static void free_node(void* context, void* memory, Value const& value) {\n+    \/\/ We get here because some threads lost a race to insert a newly created FinalizerEntry\n+    FreeHeap(memory);\n+    removed();\n+  }\n+};\n+\n+typedef ConcurrentHashTable<FinalizerTableConfig, mtClass> FinalizerHashtable;\n+static FinalizerHashtable* _table = nullptr;\n+\n+static size_t ceil_log2(size_t value) {\n+  size_t ret;\n+  for (ret = 1; ((size_t)1 << ret) < value; ++ret);\n+  return ret;\n+}\n+\n+static double table_load_factor() {\n+  return (double)_count \/ _table_size;\n+}\n+\n+static inline size_t table_size() {\n+  return ((size_t)1) << _table->get_size_log2(Thread::current());\n+}\n+\n+static inline bool table_needs_rehashing() {\n+  return _needs_rehashing;\n+}\n+\n+static inline void update_table_needs_rehash(bool rehash) {\n+  if (rehash) {\n+    _needs_rehashing = true;\n+  }\n+}\n+\n+class FinalizerEntryLookupResult {\n+ private:\n+  FinalizerEntry* _result;\n+ public:\n+  FinalizerEntryLookupResult() : _result(nullptr) {}\n+  void operator()(FinalizerEntry* node) {\n+    assert(node != nullptr, \"invariant\");\n+    _result = node;\n+  }\n+  FinalizerEntry* result() const { return _result; }\n+};\n+\n+class FinalizerEntryLookupGet {\n+ private:\n+  FinalizerEntry* _result;\n+ public:\n+  FinalizerEntryLookupGet() : _result(nullptr) {}\n+  void operator()(FinalizerEntry** node) {\n+    assert(node != nullptr, \"invariant\");\n+    _result = *node;\n+  }\n+  FinalizerEntry* result() const { return _result; }\n+};\n+\n+static void trigger_table_cleanup() {\n+  MutexLocker ml(Service_lock, Mutex::_no_safepoint_check_flag);\n+  _has_work = true;\n+  Service_lock->notify_all();\n+}\n+\n+static void check_table_concurrent_work() {\n+  if (_has_work) {\n+    return;\n+  }\n+  \/\/ We should clean\/resize if we have\n+  \/\/ more items than preferred load factor or\n+  \/\/ more dead items than water mark.\n+  if (has_items_to_clean() || (table_load_factor() > PREF_AVG_LIST_LEN)) {\n+    trigger_table_cleanup();\n+  }\n+}\n+\n+static FinalizerEntry* add_to_table_if_needed(const InstanceKlass* ik, Thread* thread) {\n+  FinalizerEntryLookup lookup(ik);\n+  bool clean_hint = false;\n+  bool rehash_warning = false;\n+  FinalizerEntry* entry = nullptr;\n+  do {\n+    \/\/ We have looked up the entry once, proceed with insertion.\n+    entry = new FinalizerEntry(ik);\n+    if (_table->insert(thread, lookup, entry, &rehash_warning, &clean_hint)) {\n+      break;\n+    }\n+    \/\/ In case another thread did a concurrent add, return value already in the table.\n+    \/\/ This could fail if the entry got deleted concurrently, so loop back until success.\n+    FinalizerEntryLookupGet felg;\n+    if (_table->get(thread, lookup, felg, &rehash_warning)) {\n+      entry = felg.result();\n+      break;\n+    }\n+  } while (true);\n+  update_table_needs_rehash(rehash_warning);\n+  if (clean_hint) {\n+    set_has_items_to_clean();\n+    check_table_concurrent_work();\n+  }\n+  assert(entry != nullptr, \"invariant\");\n+  return entry;\n+}\n+\n+\/\/ Concurrent work\n+static void grow_table(JavaThread* jt) {\n+  FinalizerHashtable::GrowTask gt(_table);\n+  if (!gt.prepare(jt)) {\n+    return;\n+  }\n+  while (gt.do_task(jt)) {\n+    gt.pause(jt);\n+    {\n+      ThreadBlockInVM tbivm(jt);\n+    }\n+    gt.cont(jt);\n+  }\n+  gt.done(jt);\n+  _table_size = table_size();\n+}\n+\n+struct FinalizerEntryDelete : StackObj {\n+  size_t _deleted;\n+  FinalizerEntryDelete() : _deleted(0) {}\n+  void operator()(FinalizerEntry** value) {\n+    assert(value != nullptr, \"invariant\");\n+    assert(*value != nullptr, \"invariant\");\n+    _deleted++;\n+  }\n+};\n+\n+struct FinalizerEntryDeleteCheck : StackObj {\n+  size_t _processed;\n+  FinalizerEntryDeleteCheck() : _processed(0) {}\n+  bool operator()(FinalizerEntry** value) {\n+    assert(value != nullptr, \"invariant\");\n+    assert(*value != nullptr, \"invariant\");\n+    _processed++;\n+    return true;\n+  }\n+};\n+\n+static void clean_table_entries(JavaThread* jt) {\n+  FinalizerHashtable::BulkDeleteTask bdt(_table);\n+  if (!bdt.prepare(jt)) {\n+    return;\n+  }\n+  FinalizerEntryDeleteCheck fedc;\n+  FinalizerEntryDelete fed;\n+  while (bdt.do_task(jt, fedc, fed)) {\n+    bdt.pause(jt);\n+    {\n+      ThreadBlockInVM tbivm(jt);\n+    }\n+    bdt.cont(jt);\n+  }\n+  reset_has_items_to_clean();\n+  bdt.done(jt);\n+}\n+\n+static void do_table_concurrent_work(JavaThread* jt) {\n+  \/\/ We prefer growing, since that also removes dead items\n+  if (table_load_factor() > PREF_AVG_LIST_LEN && !_table->is_max_size_reached()) {\n+    grow_table(jt);\n+  } else {\n+    clean_table_entries(jt);\n+  }\n+  _has_work = false;\n+}\n+\n+\/\/ Rehash\n+static bool do_table_rehash() {\n+  if (!_table->is_safepoint_safe()) {\n+    return false;\n+  }\n+  Thread* const thread = Thread::current();\n+  \/\/ We use current size\n+  const size_t new_size = _table->get_size_log2(thread);\n+  FinalizerHashtable* const new_table = new FinalizerHashtable(new_size, MAX_SIZE, REHASH_LEN);\n+  if (!_table->try_move_nodes_to(thread, new_table)) {\n+    delete new_table;\n+    return false;\n+  }\n+  \/\/ free old table\n+  delete _table;\n+  _table = new_table;\n+  return true;\n+}\n+\n+bool FinalizerService::needs_rehashing() {\n+  return _needs_rehashing;\n+}\n+\n+void FinalizerService::rehash() {\n+  static bool rehashed = false;\n+  log_debug(finalizer)(\"Table imbalanced, rehashing called.\");\n+  \/\/ Grow instead of rehash.\n+  if (table_load_factor() > PREF_AVG_LIST_LEN && !_table->is_max_size_reached()) {\n+    log_debug(finalizer)(\"Choosing growing over rehashing.\");\n+    trigger_table_cleanup();\n+    _needs_rehashing = false;\n+    return;\n+  }\n+  \/\/ Already rehashed.\n+  if (rehashed) {\n+    log_warning(finalizer)(\"Rehashing already done, still long lists.\");\n+    trigger_table_cleanup();\n+    _needs_rehashing = false;\n+    return;\n+  }\n+  if (do_table_rehash()) {\n+    rehashed = true;\n+  } else {\n+    log_debug(finalizer)(\"Resizes in progress rehashing skipped.\");\n+  }\n+  _needs_rehashing = false;\n+}\n+\n+bool FinalizerService::has_work() {\n+  return _has_work;\n+}\n+\n+void FinalizerService::do_concurrent_work(JavaThread* service_thread) {\n+  assert(service_thread != nullptr, \"invariant\");\n+  if (_has_work) {\n+    do_table_concurrent_work(service_thread);\n+  }\n+}\n+\n+void FinalizerService::init() {\n+  assert(_table == nullptr, \"invariant\");\n+  const size_t start_size_log_2 = ceil_log2(DEFAULT_TABLE_SIZE);\n+  _table_size = ((size_t)1) << start_size_log_2;\n+  _table = new FinalizerHashtable(start_size_log_2, MAX_SIZE, REHASH_LEN);\n+}\n+\n+static FinalizerEntry* lookup_entry(const InstanceKlass* ik, Thread* thread) {\n+  FinalizerEntryLookup lookup(ik);\n+  FinalizerEntryLookupGet felg;\n+  bool rehash_warning;\n+  _table->get(thread, lookup, felg, &rehash_warning);\n+  return felg.result();\n+}\n+\n+const FinalizerEntry* FinalizerService::lookup(const InstanceKlass* ik, Thread* thread) {\n+  assert(ik != nullptr, \"invariant\");\n+  assert(thread != nullptr, \"invariant\");\n+  assert(ik->has_finalizer(), \"invariant\");\n+  return lookup_entry(ik, thread);\n+}\n+\n+\/\/ Add if not exist.\n+static FinalizerEntry* get_entry(const InstanceKlass* ik, Thread* thread) {\n+  assert(ik != nullptr, \"invariant\");\n+  assert(ik->has_finalizer(), \"invariant\");\n+  FinalizerEntry* const entry = lookup_entry(ik, thread);\n+  return entry != nullptr ? entry : add_to_table_if_needed(ik, thread);\n+}\n+\n+static FinalizerEntry* get_entry(oop finalizee, Thread* thread) {\n+  assert(finalizee != nullptr, \"invariant\");\n+  assert(finalizee->is_instance(), \"invariant\");\n+  return get_entry(InstanceKlass::cast(finalizee->klass()), thread);\n+}\n+\n+static void log_registered(oop finalizee, Thread* thread) {\n+  ResourceMark rm(thread);\n+  const intptr_t identity_hash = ObjectSynchronizer::FastHashCode(thread, finalizee);\n+  log_info(finalizer)(\"Registered object (\" INTPTR_FORMAT \") of class %s as finalizable\", identity_hash, finalizee->klass()->external_name());\n+}\n+\n+void FinalizerService::on_register(oop finalizee, Thread* thread) {\n+  FinalizerEntry* const fe = get_entry(finalizee, thread);\n+  assert(fe != nullptr, \"invariant\");\n+  fe->on_register();\n+  if (log_is_enabled(Info, finalizer)) {\n+    log_registered(finalizee, thread);\n+  }\n+}\n+\n+\/\/ Can't use FastHashCode for object identification here.\n+static void log_enqueued(oop finalizee, Thread* thread) {\n+  ResourceMark rm(thread);\n+  log_debug(finalizer)(\"Enqueued an object of class %s for finalization\", finalizee->klass()->external_name());\n+}\n+\n+void FinalizerService::on_enqueue(oop finalizee) {\n+  Thread* const thread = Thread::current();\n+  FinalizerEntry* const fe = get_entry(finalizee, thread);\n+  assert(fe != nullptr, \"invariant\");\n+  fe->on_enqueue();\n+  if (log_is_enabled(Debug, finalizer)) {\n+    log_enqueued(finalizee, thread);\n+  }\n+}\n+\n+static void log_completed(oop finalizee, Thread* thread) {\n+  ResourceMark rm(thread);\n+  const intptr_t identity_hash = ObjectSynchronizer::FastHashCode(thread, finalizee);\n+  log_info(finalizer)(\"Finalization complete for object (\" INTPTR_FORMAT \") of class %s\", identity_hash, finalizee->klass()->external_name());\n+}\n+\n+void FinalizerService::on_complete(oop finalizee, JavaThread* finalizer_thread) {\n+  FinalizerEntry* const fe = get_entry(finalizee, finalizer_thread);\n+  assert(fe != nullptr, \"invariant\");\n+  fe->on_complete();\n+  if (log_is_enabled(Info, finalizer)) {\n+    log_completed(finalizee, finalizer_thread);\n+  }\n+}\n+\n+class FinalizerScan : public StackObj {\n+ private:\n+  FinalizerEntryClosure* _closure;\n+ public:\n+  FinalizerScan(FinalizerEntryClosure* closure) : _closure(closure) {}\n+  bool operator()(FinalizerEntry** fe) {\n+    return _closure->do_entry(*fe);\n+  }\n+};\n+\n+void FinalizerService::do_entries(FinalizerEntryClosure* closure, Thread* thread) {\n+  assert(closure != nullptr, \"invariant\");\n+  FinalizerScan scan(closure);\n+  _table->do_scan(thread, scan);\n+}\n+\n+static bool remove_entry(const InstanceKlass* ik) {\n+  assert(ik != nullptr, \"invariant\");\n+  FinalizerEntryLookup lookup(ik);\n+  return _table->remove(Thread::current(), lookup);\n+}\n+\n+static void on_unloading(Klass* klass) {\n+  assert(klass != nullptr, \"invariant\");\n+  if (!klass->is_instance_klass()) {\n+    return;\n+  }\n+  const InstanceKlass* const ik = InstanceKlass::cast(klass);\n+  if (ik->has_finalizer()) {\n+    remove_entry(ik);\n+  }\n+}\n+\n+void FinalizerService::purge_unloaded() {\n+  assert_locked_or_safepoint(ClassLoaderDataGraph_lock);\n+  ClassLoaderDataGraph::classes_unloading_do(&on_unloading);\n+}\n+\n+#endif \/\/ INCLUDE_MANAGEMENT\n","filename":"src\/hotspot\/share\/services\/finalizerService.cpp","additions":502,"deletions":0,"binary":false,"changes":502,"status":"added"},{"patch":"@@ -0,0 +1,74 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_SERVICES_FINALIZERSERVICE_HPP\n+#define SHARE_SERVICES_FINALIZERSERVICE_HPP\n+\n+#include \"memory\/allocation.hpp\"\n+#include \"oops\/oopsHierarchy.hpp\"\n+\n+class InstanceKlass;\n+class JavaThread;\n+class Thread;\n+\n+class FinalizerEntry : public CHeapObj<mtClass> {\n+ private:\n+  const InstanceKlass* const _ik;\n+  uint64_t _registered;\n+  uint64_t _enqueued;\n+  uint64_t _finalized;\n+ public:\n+  FinalizerEntry(const InstanceKlass* ik);\n+  const InstanceKlass* klass() const NOT_MANAGEMENT_RETURN_(nullptr);\n+  uint64_t registered() const NOT_MANAGEMENT_RETURN_(0L);\n+  uint64_t enqueued() const NOT_MANAGEMENT_RETURN_(0L);\n+  uint64_t finalized() const NOT_MANAGEMENT_RETURN_(0L);\n+  void on_register() NOT_MANAGEMENT_RETURN;\n+  void on_enqueue() NOT_MANAGEMENT_RETURN;\n+  void on_complete() NOT_MANAGEMENT_RETURN;\n+};\n+\n+class FinalizerEntryClosure : public StackObj {\n+ public:\n+  virtual bool do_entry(const FinalizerEntry* fe) = 0;\n+};\n+\n+class FinalizerService : AllStatic {\n+  friend class ServiceThread;\n+ private:\n+  static bool has_work() NOT_MANAGEMENT_RETURN_(false);\n+  static void do_concurrent_work(JavaThread* service_thread) NOT_MANAGEMENT_RETURN;;\n+ public:\n+  static void init() NOT_MANAGEMENT_RETURN;\n+  static void rehash() NOT_MANAGEMENT_RETURN;\n+  static bool needs_rehashing() NOT_MANAGEMENT_RETURN_(false);\n+  static void purge_unloaded() NOT_MANAGEMENT_RETURN;\n+  static void on_register(oop finalizee, Thread* thread) NOT_MANAGEMENT_RETURN;\n+  static void on_enqueue(oop finalizee) NOT_MANAGEMENT_RETURN;\n+  static void on_complete(oop finalizee, JavaThread* finalizer_thread) NOT_MANAGEMENT_RETURN;\n+  static void do_entries(FinalizerEntryClosure* closure, Thread* thread) NOT_MANAGEMENT_RETURN;\n+  static const FinalizerEntry* lookup(const InstanceKlass* ik, Thread* thread) NOT_MANAGEMENT_RETURN_(nullptr);\n+};\n+\n+#endif \/\/ SHARE_SERVICES_FINALIZERSERVICE_HPP\n","filename":"src\/hotspot\/share\/services\/finalizerService.hpp","additions":74,"deletions":0,"binary":false,"changes":74,"status":"added"},{"patch":"@@ -1,469 +0,0 @@\n-\/*\n- * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#if INCLUDE_MANAGEMENT\n-#include \"classfile\/classLoaderDataGraph.inline.hpp\"\n-#include \"runtime\/atomic.hpp\"\n-#include \"runtime\/handles.hpp\"\n-#include \"runtime\/interfaceSupport.inline.hpp\"\n-#include \"runtime\/javaCalls.hpp\"\n-#include \"runtime\/mutexLocker.hpp\"\n-#include \"runtime\/thread.inline.hpp\"\n-#include \"runtime\/vm_version.hpp\"\n-#include \"services\/finalizerTable.hpp\"\n-#include \"utilities\/concurrentHashTableTasks.inline.hpp\"\n-#include \"utilities\/debug.hpp\"\n-\n-static inline void atomic_inc(uint64_t* volatile dest) {\n-  assert(VM_Version::supports_cx8(), \"invariant\");\n-  uint64_t compare;\n-  uint64_t exchange;\n-  do {\n-    compare = *dest;\n-    exchange = compare + 1;\n-  } while (Atomic::cmpxchg(dest, compare, exchange) != compare);\n-}\n-\n-const InstanceKlass* FinalizerEntry::klass() const {\n-  return _ik;\n-}\n-\n-uint64_t FinalizerEntry::registered() const {\n-  return Atomic::load(&_registered);\n-}\n-\n-uint64_t FinalizerEntry::enqueued() const {\n-  return Atomic::load(&_enqueued);\n-}\n-\n-uint64_t FinalizerEntry::finalized() const {\n-  return Atomic::load(&_finalized);\n-}\n-\n-void FinalizerEntry::on_register() {\n-  atomic_inc(&_registered);\n-}\n-\n-void FinalizerEntry::on_enqueue() {\n-  atomic_inc(&_enqueued);\n-}\n-\n-void FinalizerEntry::on_complete() {\n-  atomic_inc(&_finalized);\n-}\n-\n-static constexpr const size_t DEFAULT_TABLE_SIZE = 2048;\n-\/\/ 2^24 is max size, like StringTable.\n-static constexpr const size_t MAX_SIZE = 24;\n-\/\/ If a chain gets to 50, something might be wrong\n-static constexpr const size_t REHASH_LEN = 50;\n-static constexpr const double PREF_AVG_LIST_LEN = 8.0;\n-\n-static size_t _table_size = 0;\n-static volatile uint64_t _entries = 0;\n-static volatile uint64_t _count = 0;\n-static volatile bool _has_work = 0;\n-static volatile bool _needs_rehashing = false;\n-static volatile bool _has_items_to_clean = false;\n-\n-static inline void reset_has_items_to_clean() {\n-  Atomic::store(&_has_items_to_clean, false);\n-}\n-\n-static inline void set_has_items_to_clean() {\n-  Atomic::store(&_has_items_to_clean, true);\n-}\n-\n-static inline bool has_items_to_clean() {\n-  return Atomic::load(&_has_items_to_clean);\n-}\n-\n-static inline void added() {\n-  Atomic::inc(&_count);\n-}\n-\n-static inline void removed() {\n-  Atomic::dec(&_count);\n-}\n-\n-static inline uintx hash_function(const InstanceKlass* ik) {\n-  assert(ik != nullptr, \"invariant\");\n-  return primitive_hash(ik);\n-}\n-\n-static inline uintx hash_function(const FinalizerEntry* fe) {\n-  return hash_function(fe->klass());\n-}\n-\n-class FinalizerEntryLookup : StackObj {\n- private:\n-  const InstanceKlass* const _ik;\n- public:\n-  FinalizerEntryLookup(const InstanceKlass* ik) : _ik(ik) {}\n-  uintx get_hash() const { return hash_function(_ik); }\n-  bool equals(FinalizerEntry** value, bool* is_dead) {\n-    assert(value != nullptr, \"invariant\");\n-    assert(*value != nullptr, \"invariant\");\n-    return (*value)->klass() == _ik;\n-  }\n-};\n-\n-class FinalizerTableConfig : public AllStatic {\n- public:\n-  typedef FinalizerEntry* Value;  \/\/ value of the Node in the hashtable\n-\n-  static uintx get_hash(Value const& value, bool* is_dead) {\n-    return hash_function(value);\n-  }\n-  \/\/ We use default allocation\/deallocation but counted\n-  static void* allocate_node(void* context, size_t size, Value const& value) {\n-    added();\n-    return AllocateHeap(size, mtClass);\n-  }\n-  static void free_node(void* context, void* memory, Value const& value) {\n-    \/\/ We get here because some threads lost a race to insert a newly created FinalizerEntry\n-    FreeHeap(memory);\n-    removed();\n-  }\n-};\n-\n-typedef ConcurrentHashTable<FinalizerTableConfig, mtClass> FinalizerHashtable;\n-static FinalizerHashtable* _table = nullptr;\n-\n-static size_t ceil_log2(size_t value) {\n-  size_t ret;\n-  for (ret = 1; ((size_t)1 << ret) < value; ++ret);\n-  return ret;\n-}\n-\n-static double table_load_factor() {\n-  return (double)_count \/ _table_size;\n-}\n-\n-static inline size_t table_size() {\n-  return ((size_t)1) << _table->get_size_log2(Thread::current());\n-}\n-\n-static inline bool table_needs_rehashing() {\n-  return _needs_rehashing;\n-}\n-\n-static inline void update_table_needs_rehash(bool rehash) {\n-  if (rehash) {\n-    _needs_rehashing = true;\n-  }\n-}\n-\n-class FinalizerEntryLookupResult {\n- private:\n-  FinalizerEntry* _result;\n- public:\n-  FinalizerEntryLookupResult() : _result(nullptr) {}\n-  void operator()(FinalizerEntry* node) {\n-    assert(node != nullptr, \"invariant\");\n-    _result = node;\n-  }\n-  FinalizerEntry* result() const { return _result; }\n-};\n-\n-class FinalizerEntryLookupGet {\n- private:\n-  FinalizerEntry* _result;\n- public:\n-  FinalizerEntryLookupGet() : _result(nullptr) {}\n-  void operator()(FinalizerEntry** node) {\n-    assert(node != nullptr, \"invariant\");\n-    _result = *node;\n-  }\n-  FinalizerEntry* result() const { return _result; }\n-};\n-\n-static void trigger_table_cleanup() {\n-  MutexLocker ml(Service_lock, Mutex::_no_safepoint_check_flag);\n-  _has_work = true;\n-  Service_lock->notify_all();\n-}\n-\n-static void check_table_concurrent_work() {\n-  if (_has_work) {\n-    return;\n-  }\n-  \/\/ We should clean\/resize if we have\n-  \/\/ more items than preferred load factor or\n-  \/\/ more dead items than water mark.\n-  if (has_items_to_clean() || (table_load_factor() > PREF_AVG_LIST_LEN)) {\n-    trigger_table_cleanup();\n-  }\n-}\n-\n-static FinalizerEntry* add_to_table_if_needed(const InstanceKlass* ik, Thread* thread) {\n-  FinalizerEntryLookup lookup(ik);\n-  bool clean_hint = false;\n-  bool rehash_warning = false;\n-  FinalizerEntry* entry = nullptr;\n-  do {\n-    \/\/ We have looked up the entry once, proceed with insertion.\n-    entry = new FinalizerEntry(ik);\n-    if (_table->insert(thread, lookup, entry, &rehash_warning, &clean_hint)) {\n-      break;\n-    }\n-    \/\/ In case another thread did a concurrent add, return value already in the table.\n-    \/\/ This could fail if the entry got deleted concurrently, so loop back until success.\n-    FinalizerEntryLookupGet felg;\n-    if (_table->get(thread, lookup, felg, &rehash_warning)) {\n-      entry = felg.result();\n-      break;\n-    }\n-  } while (true);\n-  update_table_needs_rehash(rehash_warning);\n-  if (clean_hint) {\n-    set_has_items_to_clean();\n-    check_table_concurrent_work();\n-  }\n-  assert(entry != nullptr, \"invariant\");\n-  return entry;\n-}\n-\n-\/\/ Concurrent work\n-static void grow_table(JavaThread* jt) {\n-  FinalizerHashtable::GrowTask gt(_table);\n-  if (!gt.prepare(jt)) {\n-    return;\n-  }\n-  while (gt.do_task(jt)) {\n-    gt.pause(jt);\n-    {\n-      ThreadBlockInVM tbivm(jt);\n-    }\n-    gt.cont(jt);\n-  }\n-  gt.done(jt);\n-  _table_size = table_size();\n-}\n-\n-struct FinalizerEntryDelete : StackObj {\n-  size_t _deleted;\n-  FinalizerEntryDelete() : _deleted(0) {}\n-  void operator()(FinalizerEntry** value) {\n-    assert(value != nullptr, \"invariant\");\n-    assert(*value != nullptr, \"invariant\");\n-    _deleted++;\n-  }\n-};\n-\n-struct FinalizerEntryDeleteCheck : StackObj {\n-  size_t _processed;\n-  FinalizerEntryDeleteCheck() : _processed(0) {}\n-  bool operator()(FinalizerEntry** value) {\n-    assert(value != nullptr, \"invariant\");\n-    assert(*value != nullptr, \"invariant\");\n-    _processed++;\n-    return true;\n-  }\n-};\n-\n-static void clean_table_entries(JavaThread* jt) {\n-  FinalizerHashtable::BulkDeleteTask bdt(_table);\n-  if (!bdt.prepare(jt)) {\n-    return;\n-  }\n-  FinalizerEntryDeleteCheck fedc;\n-  FinalizerEntryDelete fed;\n-  while (bdt.do_task(jt, fedc, fed)) {\n-    bdt.pause(jt);\n-    {\n-      ThreadBlockInVM tbivm(jt);\n-    }\n-    bdt.cont(jt);\n-  }\n-  reset_has_items_to_clean();\n-  bdt.done(jt);\n-}\n-\n-static void do_table_concurrent_work(JavaThread* jt) {\n-  \/\/ We prefer growing, since that also removes dead items\n-  if (table_load_factor() > PREF_AVG_LIST_LEN && !_table->is_max_size_reached()) {\n-    grow_table(jt);\n-  } else {\n-    clean_table_entries(jt);\n-  }\n-  _has_work = false;\n-}\n-\n-\/\/ Rehash\n-static bool do_table_rehash() {\n-  if (!_table->is_safepoint_safe()) {\n-    return false;\n-  }\n-  Thread* const thread = Thread::current();\n-  \/\/ We use current size\n-  const size_t new_size = _table->get_size_log2(thread);\n-  FinalizerHashtable* const new_table = new FinalizerHashtable(new_size, MAX_SIZE, REHASH_LEN);\n-  if (!_table->try_move_nodes_to(thread, new_table)) {\n-    delete new_table;\n-    return false;\n-  }\n-  \/\/ free old table\n-  delete _table;\n-  _table = new_table;\n-  return true;\n-}\n-\n-bool FinalizerTable::needs_rehashing() {\n-  return _needs_rehashing;\n-}\n-\n-void FinalizerTable::rehash_table() {\n-  static bool rehashed = false;\n-  log_debug(finalizertable)(\"Table imbalanced, rehashing called.\");\n-  \/\/ Grow instead of rehash.\n-  if (table_load_factor() > PREF_AVG_LIST_LEN && !_table->is_max_size_reached()) {\n-    log_debug(finalizertable)(\"Choosing growing over rehashing.\");\n-    trigger_table_cleanup();\n-    _needs_rehashing = false;\n-    return;\n-  }\n-  \/\/ Already rehashed.\n-  if (rehashed) {\n-    log_warning(finalizertable)(\"Rehashing already done, still long lists.\");\n-    trigger_table_cleanup();\n-    _needs_rehashing = false;\n-    return;\n-  }\n-  if (do_table_rehash()) {\n-    rehashed = true;\n-  } else {\n-    log_info(finalizertable)(\"Resizes in progress rehashing skipped.\");\n-  }\n-  _needs_rehashing = false;\n-}\n-\n-bool FinalizerTable::has_work() {\n-  return _has_work;\n-}\n-\n-void FinalizerTable::do_concurrent_work(JavaThread* service_thread) {\n-  assert(service_thread != nullptr, \"invariant\");\n-  if (_has_work) {\n-    do_table_concurrent_work(service_thread);\n-  }\n-}\n-\n-bool FinalizerTable::create_table() {\n-  assert(_table == nullptr, \"invariant\");\n-  const size_t start_size_log_2 = ceil_log2(DEFAULT_TABLE_SIZE);\n-  _table_size = ((size_t)1) << start_size_log_2;\n-  _table = new FinalizerHashtable(start_size_log_2, MAX_SIZE, REHASH_LEN);\n-  return _table != nullptr;\n-}\n-\n-static FinalizerEntry* lookup_entry(const InstanceKlass* ik, Thread* thread) {\n-  FinalizerEntryLookup lookup(ik);\n-  FinalizerEntryLookupGet felg;\n-  bool rehash_warning;\n-  _table->get(thread, lookup, felg, &rehash_warning);\n-  return felg.result();\n-}\n-\n-static FinalizerEntry* get_entry(const InstanceKlass* ik, Thread* thread) {\n-  assert(ik != nullptr, \"invariant\");\n-  FinalizerEntry* const entry = lookup_entry(ik, thread);\n-  return entry != nullptr ? entry : add_to_table_if_needed(ik, thread);\n-}\n-\n-const FinalizerEntry* FinalizerTable::lookup(const InstanceKlass* ik, Thread* thread) {\n-  assert(ik != nullptr, \"invariant\");\n-  assert(thread != nullptr, \"invariant\");\n-  assert(ik->has_finalizer(), \"invariant\");\n-  return lookup_entry(ik, thread);\n-}\n-\n-void FinalizerTable::on_register(const instanceHandle& h_i, Thread* thread) {\n-  assert(h_i.not_null(), \"invariant\");\n-  const InstanceKlass* const ik = InstanceKlass::cast(h_i->klass());\n-  assert(ik != nullptr, \"invariant\");\n-  assert(ik->has_finalizer(), \"invariant\");\n-  FinalizerEntry* const fe = get_entry(ik, thread);\n-  assert(fe != nullptr, \"invariant\");\n-  fe->on_register();\n-}\n-\n-void FinalizerTable::on_enqueue(const InstanceKlass* ik) {\n-  assert(ik != nullptr, \"invariant\");\n-  assert(ik->has_finalizer(), \"invariant\");\n-  FinalizerEntry* const fe = get_entry(ik, Thread::current());\n-  assert(fe != nullptr, \"invariant\");\n-  fe->on_enqueue();\n-}\n-\n-void FinalizerTable::on_complete(const instanceHandle& h_i, JavaThread* finalizerThread) {\n-  assert(h_i.not_null(), \"invariant\");\n-  const InstanceKlass* const ik = InstanceKlass::cast(h_i->klass());\n-  assert(ik != nullptr, \"invariant\");\n-  assert(ik->has_finalizer(), \"invariant\");\n-  FinalizerEntry* const fe = get_entry(ik, finalizerThread);\n-  assert(fe != nullptr, \"invariant\");\n-  fe->on_complete();\n-}\n-\n-class FinalizerScan : public StackObj {\n- private:\n-  FinalizerEntryClosure* _closure;\n- public:\n-  FinalizerScan(FinalizerEntryClosure* closure) : _closure(closure) {}\n-  bool operator()(FinalizerEntry** fe) {\n-    return _closure->do_entry(*fe);\n-  }\n-};\n-\n-void FinalizerTable::do_entries(FinalizerEntryClosure* closure, Thread* thread) {\n-  assert(closure != nullptr, \"invariant\");\n-  FinalizerScan scan(closure);\n-  _table->do_scan(thread, scan);\n-}\n-\n-static bool remove_entry(const InstanceKlass* ik) {\n-  assert(ik != nullptr, \"invariant\");\n-  FinalizerEntryLookup lookup(ik);\n-  return _table->remove(Thread::current(), lookup);\n-}\n-\n-static void on_unloading(Klass* klass) {\n-  assert(klass != nullptr, \"invariant\");\n-  if (!klass->is_instance_klass()) {\n-    return;\n-  }\n-  const InstanceKlass* const ik = InstanceKlass::cast(klass);\n-  if (ik->has_finalizer()) {\n-    remove_entry(ik);\n-  }\n-}\n-\n-void FinalizerTable::purge_unloaded() {\n-  assert_locked_or_safepoint(ClassLoaderDataGraph_lock);\n-  ClassLoaderDataGraph::classes_unloading_do(&on_unloading);\n-}\n-\n-#endif \/\/ INCLUDE_MANAGEMENT\n","filename":"src\/hotspot\/share\/services\/finalizerTable.cpp","additions":0,"deletions":469,"binary":false,"changes":469,"status":"deleted"},{"patch":"@@ -1,74 +0,0 @@\n-\/*\n- * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#ifndef SHARE_SERVICES_FINALIZERTABLE_HPP\n-#define SHARE_SERVICES_FINALIZERTABLE_HPP\n-\n-#include \"memory\/allocation.hpp\"\n-\n-class instanceHandle;\n-class InstanceKlass;\n-class JavaThread;\n-class Thread;\n-\n-class FinalizerEntry : public CHeapObj<mtClass> {\n- private:\n-  const InstanceKlass* const _ik;\n-  uint64_t _registered;\n-  uint64_t _enqueued;\n-  uint64_t _finalized;\n- public:\n-  FinalizerEntry(const InstanceKlass* ik) : _ik(ik), _registered(0), _enqueued(0), _finalized(0) {}\n-  const InstanceKlass* klass() const NOT_MANAGEMENT_RETURN_(nullptr);\n-  uint64_t registered() const NOT_MANAGEMENT_RETURN_(0L);\n-  uint64_t enqueued() const NOT_MANAGEMENT_RETURN_(0L);\n-  uint64_t finalized() const NOT_MANAGEMENT_RETURN_(0L);\n-  void on_register() NOT_MANAGEMENT_RETURN;\n-  void on_enqueue() NOT_MANAGEMENT_RETURN;\n-  void on_complete() NOT_MANAGEMENT_RETURN;\n-};\n-\n-class FinalizerEntryClosure : public StackObj {\n- public:\n-  virtual bool do_entry(const FinalizerEntry* fe) = 0;\n-};\n-\n-class FinalizerTable : AllStatic {\n-  friend class ServiceThread;\n- private:\n-  static bool has_work() NOT_MANAGEMENT_RETURN_(false);\n-  static void do_concurrent_work(JavaThread* service_thread) NOT_MANAGEMENT_RETURN;;\n- public:\n-  static bool create_table() NOT_MANAGEMENT_RETURN_(false);\n-  static void rehash_table() NOT_MANAGEMENT_RETURN;\n-  static bool needs_rehashing() NOT_MANAGEMENT_RETURN_(false);\n-  static void purge_unloaded() NOT_MANAGEMENT_RETURN;\n-  static void on_register(const instanceHandle& i, Thread* thread) NOT_MANAGEMENT_RETURN;\n-  static void on_enqueue(const InstanceKlass* ik) NOT_MANAGEMENT_RETURN;\n-  static void on_complete(const instanceHandle& i, JavaThread* finalizerThread) NOT_MANAGEMENT_RETURN;\n-  static void do_entries(FinalizerEntryClosure* closure, Thread* thread) NOT_MANAGEMENT_RETURN;\n-  static const FinalizerEntry* lookup(const InstanceKlass* ik, Thread* thread) NOT_MANAGEMENT_RETURN_(nullptr);\n-};\n-\n-#endif \/\/ SHARE_SERVICES_FINALIZERTABLE_HPP\n","filename":"src\/hotspot\/share\/services\/finalizerTable.hpp","additions":0,"deletions":74,"binary":false,"changes":74,"status":"deleted"},{"patch":"@@ -58,1 +58,1 @@\n-#include \"services\/finalizerTable.hpp\"\n+#include \"services\/finalizerService.hpp\"\n@@ -98,1 +98,1 @@\n-  FinalizerTable::create_table();\n+  FinalizerService::init();\n@@ -2087,3 +2087,1 @@\n-  HandleMark hm(THREAD);\n-  instanceHandle h_i(THREAD, instanceOop(JNIHandles::resolve_non_null(finalizee)));\n-  FinalizerTable::on_complete(h_i, THREAD);\n+  FinalizerService::on_complete(JNIHandles::resolve_non_null(finalizee), THREAD);\n","filename":"src\/hotspot\/share\/services\/management.cpp","additions":3,"deletions":5,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -41,1 +41,1 @@\n- * @run main\/othervm -Xlog:class+unload -Xlog:gc -Xmx16m jdk.jfr.event.runtime.TestFinalizerEvent\n+ * @run main\/othervm -Xlog:class+unload,finalizer -Xmx16m jdk.jfr.event.runtime.TestFinalizerEvent\n","filename":"test\/jdk\/jdk\/jfr\/event\/runtime\/TestFinalizerEvent.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"}]}