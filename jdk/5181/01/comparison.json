{"files":[{"patch":"@@ -117,40 +117,9 @@\n-    if (obj->is_forwarded() && obj->forwardee() == obj) {\n-      \/\/ The object failed to move.\n-\n-      zap_dead_objects(_last_forwarded_object_end, obj_addr);\n-      \/\/ We consider all objects that we find self-forwarded to be\n-      \/\/ live. What we'll do is that we'll update the prev marking\n-      \/\/ info so that they are all under PTAMS and explicitly marked.\n-      if (!_cm->is_marked_in_prev_bitmap(obj)) {\n-        _cm->mark_in_prev_bitmap(obj);\n-      }\n-      if (_during_concurrent_start) {\n-        \/\/ For the next marking info we'll only mark the\n-        \/\/ self-forwarded objects explicitly if we are during\n-        \/\/ concurrent start (since, normally, we only mark objects pointed\n-        \/\/ to by roots if we succeed in copying them). By marking all\n-        \/\/ self-forwarded objects we ensure that we mark any that are\n-        \/\/ still pointed to be roots. During concurrent marking, and\n-        \/\/ after concurrent start, we don't need to mark any objects\n-        \/\/ explicitly and all objects in the CSet are considered\n-        \/\/ (implicitly) live. So, we won't mark them explicitly and\n-        \/\/ we'll leave them over NTAMS.\n-        _cm->mark_in_next_bitmap(_worker_id, _hr, obj);\n-      }\n-      size_t obj_size = obj->size();\n-\n-      _marked_bytes += (obj_size * HeapWordSize);\n-      PreservedMarks::init_forwarded_mark(obj);\n-\n-      \/\/ During evacuation failure we do not record inter-region\n-      \/\/ references referencing regions that need a remembered set\n-      \/\/ update originating from young regions (including eden) that\n-      \/\/ failed evacuation. Make up for that omission now by rescanning\n-      \/\/ these failed objects.\n-      if (_is_young) {\n-        obj->oop_iterate(_log_buffer_cl);\n-      }\n-\n-      HeapWord* obj_end = obj_addr + obj_size;\n-      _last_forwarded_object_end = obj_end;\n-      _hr->cross_threshold(obj_addr, obj_end);\n+    \/\/ The object failed to move.\n+    assert(obj->is_forwarded() && obj->forwardee() == obj, \"sanity\");\n+\n+    zap_dead_objects(_last_forwarded_object_end, obj_addr);\n+    \/\/ We consider all objects that we find self-forwarded to be\n+    \/\/ live. What we'll do is that we'll update the prev marking\n+    \/\/ info so that they are all under PTAMS and explicitly marked.\n+    if (!_cm->is_marked_in_prev_bitmap(obj)) {\n+      _cm->mark_in_prev_bitmap(obj);\n@@ -158,0 +127,30 @@\n+    if (_during_concurrent_start) {\n+      \/\/ For the next marking info we'll only mark the\n+      \/\/ self-forwarded objects explicitly if we are during\n+      \/\/ concurrent start (since, normally, we only mark objects pointed\n+      \/\/ to by roots if we succeed in copying them). By marking all\n+      \/\/ self-forwarded objects we ensure that we mark any that are\n+      \/\/ still pointed to be roots. During concurrent marking, and\n+      \/\/ after concurrent start, we don't need to mark any objects\n+      \/\/ explicitly and all objects in the CSet are considered\n+      \/\/ (implicitly) live. So, we won't mark them explicitly and\n+      \/\/ we'll leave them over NTAMS.\n+      _cm->mark_in_next_bitmap(_worker_id, _hr, obj);\n+    }\n+    size_t obj_size = obj->size();\n+\n+    _marked_bytes += (obj_size * HeapWordSize);\n+    PreservedMarks::init_forwarded_mark(obj);\n+\n+    \/\/ During evacuation failure we do not record inter-region\n+    \/\/ references referencing regions that need a remembered set\n+    \/\/ update originating from young regions (including eden) that\n+    \/\/ failed evacuation. Make up for that omission now by rescanning\n+    \/\/ these failed objects.\n+    if (_is_young) {\n+      obj->oop_iterate(_log_buffer_cl);\n+    }\n+\n+    HeapWord* obj_end = obj_addr + obj_size;\n+    _last_forwarded_object_end = obj_end;\n+    _hr->cross_threshold(obj_addr, obj_end);\n@@ -226,1 +225,2 @@\n-    hr->object_iterate(&rspc);\n+    \/\/ Iterates evac failure objs which are recorded during evacuation.\n+    hr->iterate_evac_failure_objs(&rspc);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1EvacFailure.cpp","additions":41,"deletions":41,"binary":false,"changes":82,"status":"modified"},{"patch":"@@ -0,0 +1,108 @@\n+\/*\n+ * Copyright (c) 2021, Huawei and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"g1EvacuationFailureObjsInHR.hpp\"\n+#include \"gc\/g1\/g1CollectedHeap.hpp\"\n+#include \"gc\/g1\/heapRegion.hpp\"\n+#include \"gc\/g1\/heapRegion.inline.hpp\"\n+#include \"utilities\/quickSort.hpp\"\n+\n+\n+\n+\/\/ === G1EvacuationFailureObjsInHR ===\n+\n+void G1EvacuationFailureObjsInHR::visit(Elem elem) {\n+  uint32_t offset = elem;\n+  _offset_array[_objs_num++] = offset;\n+}\n+\n+void G1EvacuationFailureObjsInHR::visit(Array<NODE_LENGTH, Elem>::NODE_XXX* node, uint32_t limit) {\n+  ::memcpy(&_offset_array[_objs_num], node->_oop_offsets, limit * sizeof(Elem));\n+  _objs_num += limit;\n+}\n+\n+void G1EvacuationFailureObjsInHR::compact() {\n+  assert(_offset_array == NULL, \"must be\");\n+  _offset_array = NEW_C_HEAP_ARRAY(Elem, _nodes_array.objs_num(), mtGC);\n+  \/\/ _nodes_array.iterate_elements(this);\n+  _nodes_array.iterate_nodes(this);\n+  uint expected = _nodes_array.objs_num();\n+  assert(_objs_num == expected, \"must be %u, %u\", _objs_num, expected);\n+  _nodes_array.reset();\n+}\n+\n+static int32_t order_oop(G1EvacuationFailureObjsInHR::Elem a,\n+                         G1EvacuationFailureObjsInHR::Elem b) {\n+  \/\/ assert(a != b, \"must be\");\n+  int r = a-b;\n+  return r;\n+}\n+\n+void G1EvacuationFailureObjsInHR::sort() {\n+  QuickSort::sort(_offset_array, _objs_num, order_oop, true);\n+}\n+\n+void G1EvacuationFailureObjsInHR::clear_array() {\n+  FREE_C_HEAP_ARRAY(oop, _offset_array);\n+  _offset_array = NULL;\n+  _objs_num = 0;\n+}\n+\n+void G1EvacuationFailureObjsInHR::iterate_internal(ObjectClosure* closure) {\n+  Elem prev = 0;\n+  for (uint i = 0; i < _objs_num; i++) {\n+    assert(prev < _offset_array[i], \"must be\");\n+    closure->do_object(cast_from_offset(prev = _offset_array[i]));\n+  }\n+  clear_array();\n+}\n+\n+G1EvacuationFailureObjsInHR::G1EvacuationFailureObjsInHR(uint region_idx, HeapWord* bottom) :\n+  offset_mask((1l << HeapRegion::LogOfHRGrainBytes) - 1),\n+  _region_idx(region_idx),\n+  _bottom(bottom),\n+  _nodes_array(HeapRegion::GrainWords \/ NODE_LENGTH + 1),\n+  _offset_array(NULL),\n+  _objs_num(0) {\n+}\n+\n+G1EvacuationFailureObjsInHR::~G1EvacuationFailureObjsInHR() {\n+  clear_array();\n+}\n+\n+void G1EvacuationFailureObjsInHR::record(oop obj) {\n+  assert(obj != NULL, \"must be\");\n+  assert(G1CollectedHeap::heap()->heap_region_containing(obj)->hrm_index() == _region_idx, \"must be\");\n+  Elem offset = cast_from_oop_addr(obj);\n+  assert(obj == cast_from_offset(offset), \"must be\");\n+  assert(offset < (1<<25), \"must be\");\n+  _nodes_array.add(offset);\n+}\n+\n+void G1EvacuationFailureObjsInHR::iterate(ObjectClosure* closure) {\n+  compact();\n+  sort();\n+  iterate_internal(closure);\n+}\n","filename":"src\/hotspot\/share\/gc\/g1\/g1EvacuationFailureObjsInHR.cpp","additions":108,"deletions":0,"binary":false,"changes":108,"status":"added"},{"patch":"@@ -0,0 +1,229 @@\n+\/*\n+ * Copyright (c) 2021, Huawei and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_G1_G1EVACUATIONFAILUREOBJSINHR_HPP\n+#define SHARE_GC_G1_G1EVACUATIONFAILUREOBJSINHR_HPP\n+\n+#include \"memory\/iterator.hpp\"\n+#include \"oops\/oop.hpp\"\n+\n+\/\/ This class\n+\/\/   1. records the objects per region which have failed to evacuate.\n+\/\/   2. speeds up removing self forwarded ptrs in post evacuation phase.\n+\/\/\n+class G1EvacuationFailureObjsInHR {\n+  template<uint32_t LEN, typename Elem>\n+  class Array;\n+\n+  template<uint32_t LEN, typename Elem>\n+  class Node : public CHeapObj<mtGC>{\n+    friend G1EvacuationFailureObjsInHR;\n+    friend Array<LEN, Elem>;\n+\n+  private:\n+    static const uint32_t LENGTH = LEN;\n+    static const size_t SIZE = LENGTH * sizeof(Elem);\n+    Elem* _oop_offsets;\n+\n+  public:\n+    Node() {\n+      _oop_offsets = (Elem*)AllocateHeap(SIZE, mtGC);\n+    }\n+    Elem& operator[] (size_t idx) {\n+      return _oop_offsets[idx];\n+    }\n+    static Node<LEN, Elem>* create_node() {\n+      return new Node<LEN, Elem>();\n+    }\n+    static void free_node(Node<LEN, Elem>* node) {\n+      FreeHeap(node->_oop_offsets);\n+      delete(node);\n+    }\n+  };\n+\n+  template<uint32_t NODE_SIZE, typename Elem>\n+  class Array : public CHeapObj<mtGC> {\n+  public:\n+    typedef Node<NODE_SIZE, Elem> NODE_XXX;\n+\n+  private:\n+    static const uint64_t TMP = 1l << 32;\n+    static const uint64_t LOW_MASK = TMP - 1;\n+    static const uint64_t HIGH_MASK = LOW_MASK << 32;\n+    const uint32_t _max_nodes_length;\n+\n+    volatile uint64_t _cur_pos;\n+    NODE_XXX* volatile * _nodes;\n+    volatile uint _elements_num;\n+\n+  private:\n+    uint64_t low(uint64_t n) {\n+      return (n & LOW_MASK);\n+    }\n+    uint64_t high(uint64_t n) {\n+      return (n & HIGH_MASK);\n+    }\n+    uint32_t elem_index(uint64_t n) {\n+      assert(low(n) < NODE_XXX::LENGTH, \"must be\");\n+      return low(n);\n+    }\n+    uint32_t node_index(uint64_t n) {\n+      return high(n) >> 32;\n+    }\n+\n+    uint64_t next(uint64_t n) {\n+      uint64_t lo = low(n);\n+      uint64_t hi = high(n);\n+      assert((lo < NODE_XXX::LENGTH) && (NODE_XXX::LENGTH <= LOW_MASK), \"must be\");\n+      assert(hi < HIGH_MASK, \"must be\");\n+      if ((lo+1) == NODE_XXX::LENGTH) {\n+        lo = 0;\n+        hi += (1l << 32);\n+      } else {\n+        lo++;\n+      }\n+      assert(hi <= HIGH_MASK, \"must be\");\n+      return hi | lo;\n+    }\n+\n+  public:\n+    Array(uint32_t max_nodes_length) : _max_nodes_length(max_nodes_length) {\n+      _nodes = (NODE_XXX**)AllocateHeap(_max_nodes_length * sizeof(NODE_XXX*), mtGC);\n+      for (uint32_t i = 0; i < _max_nodes_length; i++) {\n+        Atomic::store(&_nodes[i], (NODE_XXX *)NULL);\n+      }\n+\n+      Atomic::store(&_elements_num, 0u);\n+      Atomic::store(&_cur_pos, (uint64_t)0);\n+    }\n+\n+    ~Array() {\n+      reset();\n+      FreeHeap((NODE_XXX**)_nodes);\n+    }\n+\n+    uint objs_num() {\n+      return Atomic::load(&_elements_num);\n+    }\n+\n+    void add(Elem elem) {\n+      while (true) {\n+        uint64_t pos = Atomic::load(&_cur_pos);\n+        uint64_t next_pos = next(pos);\n+        uint64_t res = Atomic::cmpxchg(&_cur_pos, pos, next_pos);\n+        if (res == pos) {\n+          uint32_t hi = node_index(pos);\n+          uint32_t lo = elem_index(pos);\n+          if (lo == 0) {\n+            Atomic::store(&_nodes[hi], NODE_XXX::create_node());\n+          }\n+          NODE_XXX* node = NULL;\n+          while ((node = Atomic::load(&_nodes[hi])) == NULL);\n+\n+          node->operator[](lo) = elem;\n+          Atomic::inc(&_elements_num);\n+          break;\n+        }\n+      }\n+    }\n+\n+    template<typename VISITOR>\n+    void iterate_elements(VISITOR v) {\n+      int64_t pos = Atomic::load(&_cur_pos);\n+      DEBUG_ONLY(uint total = 0);\n+      uint32_t hi = node_index(pos);\n+      uint32_t lo = elem_index(pos);\n+      for (uint32_t i = 0; i <= hi; i++) {\n+        uint32_t limit = (i == hi) ? lo : NODE_XXX::LENGTH;\n+        NODE_XXX* node = Atomic::load(&_nodes[i]);\n+        for (uint32_t j = 0; j < limit; j++) {\n+          v->visit(node->operator[](j));\n+          DEBUG_ONLY(total++);\n+        }\n+      }\n+      assert(total == Atomic::load(&_elements_num), \"must be\");\n+    }\n+\n+    template<typename VISITOR>\n+    void iterate_nodes(VISITOR v) {\n+      int64_t pos = Atomic::load(&_cur_pos);\n+      uint32_t hi = node_index(pos);\n+      uint32_t lo = elem_index(pos);\n+      for (uint32_t i = 0; i <= hi; i++) {\n+        NODE_XXX* node = Atomic::load(&_nodes[i]);\n+        uint32_t limit = (i == hi) ? lo : NODE_XXX::LENGTH;\n+        v->visit(node, limit);\n+      }\n+    }\n+\n+    void reset() {\n+      int64_t pos = Atomic::load(&_cur_pos);\n+      for (uint32_t hi = 0; hi <= node_index(pos); hi++) {\n+        NODE_XXX::free_node(_nodes[hi]);\n+        Atomic::store(&_nodes[hi], (NODE_XXX *)NULL);\n+      }\n+      Atomic::store(&_elements_num, 0u);\n+      Atomic::store(&_cur_pos, (uint64_t)0);\n+    }\n+  };\n+\n+public:\n+  typedef uint32_t Elem;\n+\n+private:\n+  static const uint32_t NODE_LENGTH = 256;\n+  const uint64_t offset_mask;\n+  const uint _region_idx;\n+  const HeapWord* _bottom;\n+  Array<NODE_LENGTH, Elem> _nodes_array;\n+  Elem* _offset_array;\n+  uint _objs_num;\n+\n+private:\n+  oop cast_from_offset(Elem offset) {\n+    return oop(_bottom + offset);\n+  }\n+  Elem cast_from_oop_addr(oop obj) {\n+    const HeapWord* o = cast_from_oop<const HeapWord*>(obj);\n+    size_t offset = pointer_delta(o, _bottom);\n+    assert(offset_mask >= offset, \"must be\");\n+    return offset & offset_mask;\n+  }\n+  void visit(Elem);\n+  void visit(Array<NODE_LENGTH, Elem>::NODE_XXX* node, uint32_t limit);\n+  void compact();\n+  void sort();\n+  void clear_array();\n+  void iterate_internal(ObjectClosure* closure);\n+\n+public:\n+  G1EvacuationFailureObjsInHR(uint region_idx, HeapWord* bottom);\n+  ~G1EvacuationFailureObjsInHR();\n+\n+  void record(oop obj);\n+  void iterate(ObjectClosure* closure);\n+};\n+\n+\n+#endif \/\/SHARE_GC_G1_G1EVACUATIONFAILUREOBJSINHR_HPP\n","filename":"src\/hotspot\/share\/gc\/g1\/g1EvacuationFailureObjsInHR.hpp","additions":229,"deletions":0,"binary":false,"changes":229,"status":"added"},{"patch":"@@ -605,0 +605,3 @@\n+    \/\/ Records evac failure objs, this will help speed up iteration\n+    \/\/ of these objs later in *remove self forward* phase of post evacuation.\n+    r->record_evac_failure_obj(old);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ParScanThreadState.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -111,0 +111,8 @@\n+void HeapRegion::record_evac_failure_obj(oop obj) {\n+  _evac_failure_objs.record(obj);\n+}\n+\n+void HeapRegion::iterate_evac_failure_objs(ObjectClosure* closure) {\n+  _evac_failure_objs.iterate(closure);\n+}\n+\n@@ -251,1 +259,2 @@\n-  _node_index(G1NUMA::UnknownNodeIndex)\n+  _node_index(G1NUMA::UnknownNodeIndex),\n+  _evac_failure_objs(hrm_index, _bottom)\n","filename":"src\/hotspot\/share\/gc\/g1\/heapRegion.cpp","additions":10,"deletions":1,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"gc\/g1\/g1EvacuationFailureObjsInHR.hpp\"\n@@ -261,0 +262,2 @@\n+  G1EvacuationFailureObjsInHR _evac_failure_objs;\n+\n@@ -557,0 +560,5 @@\n+  \/\/ Records evac failure objs during evaucation, this will help speed up iteration\n+  \/\/ of these objs later in *remove self forward* phase of post evacuation.\n+  void record_evac_failure_obj(oop obj);\n+  \/\/ Iterates evac failure objs which are recorded during evcauation.\n+  void iterate_evac_failure_objs(ObjectClosure* closure);\n","filename":"src\/hotspot\/share\/gc\/g1\/heapRegion.hpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"}]}