{"files":[{"patch":"@@ -28,0 +28,1 @@\n+#include \"gc\/g1\/g1SegmentedArray.inline.hpp\"\n@@ -49,0 +50,5 @@\n+template <class Elem>\n+G1CardSetAllocator<Elem>::~G1CardSetAllocator() {\n+  drop_all();\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CardSetMemory.cpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -116,3 +116,1 @@\n-  ~G1CardSetAllocator() {\n-    drop_all();\n-  }\n+  ~G1CardSetAllocator();\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CardSetMemory.hpp","additions":1,"deletions":3,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -71,0 +71,1 @@\n+    assert(_last_forwarded_object_end <= obj_addr, \"should iterate in ascending address order\");\n@@ -73,2 +74,2 @@\n-    if (obj->is_forwarded() && obj->forwardee() == obj) {\n-      \/\/ The object failed to move.\n+    \/\/ The object failed to move.\n+    assert(obj->is_forwarded() && obj->forwardee() == obj, \"sanity\");\n@@ -76,21 +77,21 @@\n-      zap_dead_objects(_last_forwarded_object_end, obj_addr);\n-      \/\/ We consider all objects that we find self-forwarded to be\n-      \/\/ live. What we'll do is that we'll update the prev marking\n-      \/\/ info so that they are all under PTAMS and explicitly marked.\n-      if (!_cm->is_marked_in_prev_bitmap(obj)) {\n-        _cm->mark_in_prev_bitmap(obj);\n-      }\n-      if (_during_concurrent_start) {\n-        \/\/ For the next marking info we'll only mark the\n-        \/\/ self-forwarded objects explicitly if we are during\n-        \/\/ concurrent start (since, normally, we only mark objects pointed\n-        \/\/ to by roots if we succeed in copying them). By marking all\n-        \/\/ self-forwarded objects we ensure that we mark any that are\n-        \/\/ still pointed to be roots. During concurrent marking, and\n-        \/\/ after concurrent start, we don't need to mark any objects\n-        \/\/ explicitly and all objects in the CSet are considered\n-        \/\/ (implicitly) live. So, we won't mark them explicitly and\n-        \/\/ we'll leave them over NTAMS.\n-        _cm->mark_in_next_bitmap(_worker_id, _hr, obj);\n-      }\n-      size_t obj_size = obj->size();\n+    zap_dead_objects(_last_forwarded_object_end, obj_addr);\n+    \/\/ We consider all objects that we find self-forwarded to be\n+    \/\/ live. What we'll do is that we'll update the prev marking\n+    \/\/ info so that they are all under PTAMS and explicitly marked.\n+    if (!_cm->is_marked_in_prev_bitmap(obj)) {\n+      _cm->mark_in_prev_bitmap(obj);\n+    }\n+    if (_during_concurrent_start) {\n+      \/\/ For the next marking info we'll only mark the\n+      \/\/ self-forwarded objects explicitly if we are during\n+      \/\/ concurrent start (since, normally, we only mark objects pointed\n+      \/\/ to by roots if we succeed in copying them). By marking all\n+      \/\/ self-forwarded objects we ensure that we mark any that are\n+      \/\/ still pointed to be roots. During concurrent marking, and\n+      \/\/ after concurrent start, we don't need to mark any objects\n+      \/\/ explicitly and all objects in the CSet are considered\n+      \/\/ (implicitly) live. So, we won't mark them explicitly and\n+      \/\/ we'll leave them over NTAMS.\n+      _cm->mark_in_next_bitmap(_worker_id, _hr, obj);\n+    }\n+    size_t obj_size = obj->size();\n@@ -98,2 +99,2 @@\n-      _marked_bytes += (obj_size * HeapWordSize);\n-      PreservedMarks::init_forwarded_mark(obj);\n+    _marked_bytes += (obj_size * HeapWordSize);\n+    PreservedMarks::init_forwarded_mark(obj);\n@@ -101,4 +102,3 @@\n-      HeapWord* obj_end = obj_addr + obj_size;\n-      _last_forwarded_object_end = obj_end;\n-      _hr->alloc_block_in_bot(obj_addr, obj_end);\n-    }\n+    HeapWord* obj_end = obj_addr + obj_size;\n+    _last_forwarded_object_end = obj_end;\n+    _hr->alloc_block_in_bot(obj_addr, obj_end);\n@@ -167,1 +167,2 @@\n-    hr->object_iterate(&rspc);\n+    \/\/ Iterates evac failure objs which are recorded during evacuation.\n+    hr->iterate_evac_failure_objs(&rspc);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1EvacFailure.cpp","additions":31,"deletions":30,"binary":false,"changes":61,"status":"modified"},{"patch":"@@ -0,0 +1,149 @@\n+\/*\n+ * Copyright (c) 2021, Huawei and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/g1\/g1EvacFailureObjectsSet.hpp\"\n+#include \"gc\/g1\/g1CollectedHeap.hpp\"\n+#include \"gc\/g1\/g1SegmentedArray.inline.hpp\"\n+#include \"gc\/g1\/heapRegion.hpp\"\n+#include \"gc\/g1\/heapRegion.inline.hpp\"\n+#include \"utilities\/quickSort.hpp\"\n+\n+\n+const G1SegmentedArrayAllocOptions G1EvacFailureObjectsSet::_alloc_options =\n+  G1SegmentedArrayAllocOptions((uint)sizeof(OffsetInRegion), BufferLength, UINT_MAX, Alignment);\n+\n+G1SegmentedArrayBufferList<mtGC> G1EvacFailureObjectsSet::_free_buffer_list;\n+\n+#ifdef ASSERT\n+void G1EvacFailureObjectsSet::assert_is_valid_offset(size_t offset) const {\n+  const uint max_offset = 1u << (HeapRegion::LogOfHRGrainBytes - LogHeapWordSize);\n+  assert(offset < max_offset, \"must be, but is \" SIZE_FORMAT, offset);\n+}\n+#endif\n+\n+oop G1EvacFailureObjectsSet::from_offset(OffsetInRegion offset) const {\n+  assert_is_valid_offset(offset);\n+  return cast_to_oop(_bottom + offset);\n+}\n+\n+G1EvacFailureObjectsSet::OffsetInRegion G1EvacFailureObjectsSet::cast_to_offset(oop obj) const {\n+  const HeapWord* o = cast_from_oop<const HeapWord*>(obj);\n+  size_t offset = pointer_delta(o, _bottom);\n+  assert_is_valid_offset(offset);\n+  assert(obj == from_offset(static_cast<OffsetInRegion>(offset)), \"must be\");\n+  return static_cast<OffsetInRegion>(offset);\n+}\n+\n+G1EvacFailureObjectsSet::G1EvacFailureObjectsSet(uint region_idx, HeapWord* bottom) :\n+  DEBUG_ONLY(_region_idx(region_idx) COMMA)\n+  _bottom(bottom),\n+  _offsets(\"\", &_alloc_options, &_free_buffer_list)  {\n+  assert(HeapRegion::LogOfHRGrainBytes < 32, \"must be\");\n+}\n+\n+void G1EvacFailureObjectsSet::record(oop obj) {\n+  assert(obj != NULL, \"must be\");\n+  assert(_region_idx == G1CollectedHeap::heap()->heap_region_containing(obj)->hrm_index(), \"must be\");\n+  OffsetInRegion* e = _offsets.allocate();\n+  *e = cast_to_offset(obj);\n+}\n+\n+\/\/ Helper class to join, sort and iterate over the previously collected segmented\n+\/\/ array of objects that failed evacuation.\n+class G1EvacFailureObjectsIterator {\n+  typedef G1EvacFailureObjectsSet::OffsetInRegion OffsetInRegion;\n+  friend class G1SegmentedArray<OffsetInRegion, mtGC>;\n+  friend class G1SegmentedArrayBuffer<mtGC>;\n+\n+  G1EvacFailureObjectsSet* _collector;\n+  const G1SegmentedArray<OffsetInRegion, mtGC>* _segments;\n+  OffsetInRegion* _offset_array;\n+  uint _array_length;\n+\n+  static int order_oop(OffsetInRegion a, OffsetInRegion b) {\n+    return static_cast<int>(a-b);\n+  }\n+\n+  void join_and_sort() {\n+    uint num = _segments->num_allocated_nodes();\n+    _offset_array = NEW_C_HEAP_ARRAY(OffsetInRegion, num, mtGC);\n+\n+    _segments->iterate_nodes(*this);\n+    assert(_array_length == num, \"must be %u, %u\", _array_length, num);\n+\n+    QuickSort::sort(_offset_array, _array_length, order_oop, true);\n+  }\n+\n+  void iterate_internal(ObjectClosure* closure) {\n+    for (uint i = 0; i < _array_length; i++) {\n+      _collector->assert_is_valid_offset(_offset_array[i]);\n+      oop cur = _collector->from_offset(_offset_array[i]);\n+      closure->do_object(cur);\n+    }\n+\n+    FREE_C_HEAP_ARRAY(OffsetInRegion, _offset_array);\n+  }\n+\n+  \/\/ Callback of G1SegmentedArray::iterate_nodes\n+  void visit_buffer(G1SegmentedArrayBuffer<mtGC>* node, uint length) {\n+    node->copy_to(&_offset_array[_array_length]);\n+    _array_length += length;\n+\n+    \/\/ Verify elements in the node\n+    DEBUG_ONLY(node->iterate_elems(*this));\n+  }\n+\n+#ifdef ASSERT\n+  \/\/ Callback of G1SegmentedArrayBuffer::iterate_elems\n+  \/\/ Verify a single element in a segment node\n+  void visit_elem(void* elem) {\n+    uint* ptr = (uint*)elem;\n+    _collector->assert_is_valid_offset(*ptr);\n+  }\n+#endif\n+\n+public:\n+  G1EvacFailureObjectsIterator(G1EvacFailureObjectsSet* collector) :\n+    _collector(collector),\n+    _segments(&_collector->_offsets),\n+    _offset_array(nullptr),\n+    _array_length(0) { }\n+\n+  ~G1EvacFailureObjectsIterator() { }\n+\n+  void iterate(ObjectClosure* closure) {\n+    join_and_sort();\n+    iterate_internal(closure);\n+  }\n+};\n+\n+void G1EvacFailureObjectsSet::iterate(ObjectClosure* closure) {\n+  assert_at_safepoint();\n+\n+  G1EvacFailureObjectsIterator iterator(this);\n+  iterator.iterate(closure);\n+\n+  _offsets.drop_all();\n+}\n","filename":"src\/hotspot\/share\/gc\/g1\/g1EvacFailureObjectsSet.cpp","additions":149,"deletions":0,"binary":false,"changes":149,"status":"added"},{"patch":"@@ -0,0 +1,82 @@\n+\/*\n+ * Copyright (c) 2021, Huawei and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_G1_G1EVACUATIONFAILUREOBJSINHR_HPP\n+#define SHARE_GC_G1_G1EVACUATIONFAILUREOBJSINHR_HPP\n+\n+#include \"gc\/g1\/g1SegmentedArray.hpp\"\n+#include \"memory\/iterator.hpp\"\n+#include \"oops\/oop.hpp\"\n+\n+class G1EvacFailureObjectsIterator;\n+\n+\/\/ This class collects addresses of objects that failed evacuation in a specific\n+\/\/ heap region.\n+\/\/ Provides sorted iteration of these elements for processing during the remove\n+\/\/ self forwards phase.\n+class G1EvacFailureObjectsSet {\n+  friend class G1EvacFailureObjectsIterator;\n+\n+public:\n+  \/\/ Storage type of an object that failed evacuation within a region. Given\n+  \/\/ heap region size and possible object locations within a region, it is\n+  \/\/ sufficient to use an uint here to save some space instead of full pointers.\n+  typedef uint OffsetInRegion;\n+\n+private:\n+  static const uint BufferLength = 256;\n+  static const uint Alignment = 4;\n+\n+  static const G1SegmentedArrayAllocOptions _alloc_options;\n+\n+  \/\/ This free list is shared among evacuation failure process in all regions.\n+  static G1SegmentedArrayBufferList<mtGC> _free_buffer_list;\n+\n+  DEBUG_ONLY(const uint _region_idx;)\n+\n+  \/\/ Region bottom\n+  const HeapWord* _bottom;\n+\n+  \/\/ Offsets within region containing objects that failed evacuation.\n+  G1SegmentedArray<OffsetInRegion, mtGC> _offsets;\n+\n+  void assert_is_valid_offset(size_t offset) const NOT_DEBUG_RETURN;\n+  \/\/ Converts between an offset within a region and an oop address.\n+  oop from_offset(OffsetInRegion offset) const;\n+  OffsetInRegion cast_to_offset(oop obj) const;\n+\n+public:\n+  G1EvacFailureObjectsSet(uint region_idx, HeapWord* bottom);\n+  ~G1EvacFailureObjectsSet() { }\n+\n+  \/\/ Record an object that failed evacuation.\n+  void record(oop obj);\n+\n+  \/\/ Apply the given ObjectClosure to all objects that failed evacuation. Objects\n+  \/\/ are passed in increasing address order.\n+  void iterate(ObjectClosure* closure);\n+};\n+\n+\n+#endif \/\/SHARE_GC_G1_G1EVACUATIONFAILUREOBJSINHR_HPP\n","filename":"src\/hotspot\/share\/gc\/g1\/g1EvacFailureObjectsSet.hpp","additions":82,"deletions":0,"binary":false,"changes":82,"status":"added"},{"patch":"@@ -610,0 +610,3 @@\n+    \/\/ Records evac failure objs, this will help speed up iteration\n+    \/\/ of these objs later in *remove self forward* phase of post evacuation.\n+    r->record_evac_failure_obj(old);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ParScanThreadState.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -76,0 +76,14 @@\n+  uint length() const {\n+    \/\/ _next_allocate might grow larger than _num_elems in multi-thread environments\n+    \/\/ due to races.\n+    return MIN2(_next_allocate, _num_elems);\n+  }\n+\n+  \/\/ Copies the (valid) contents of this buffer into the destination.\n+  void copy_to(void* dest) const {\n+    ::memcpy(dest, _buffer, length() * _elem_size);\n+  }\n+\n+  template<typename Visitor>\n+  void iterate_elems(Visitor& v) const;\n+\n@@ -193,0 +207,2 @@\n+  DEBUG_ONLY(uint calculate_length() const;)\n+\n@@ -197,1 +213,5 @@\n-  uint num_allocated_nodes() const { return Atomic::load(&_num_allocated_nodes); }\n+  uint num_allocated_nodes() const {\n+    uint allocated = Atomic::load(&_num_allocated_nodes);\n+    assert(calculate_length() == allocated, \"Must be\");\n+    return allocated;\n+  }\n@@ -204,3 +224,1 @@\n-  ~G1SegmentedArray() {\n-    drop_all();\n-  }\n+  ~G1SegmentedArray();\n@@ -215,0 +233,3 @@\n+\n+  template<typename Visitor>\n+  void iterate_nodes(Visitor& v) const;\n","filename":"src\/hotspot\/share\/gc\/g1\/g1SegmentedArray.hpp","additions":25,"deletions":4,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -58,0 +58,9 @@\n+template<MEMFLAGS flag>\n+template<typename Visitor>\n+void G1SegmentedArrayBuffer<flag>::iterate_elems(Visitor& v) const {\n+  for (uint i = 0; i < length(); i++) {\n+    void* ptr = _buffer + i * _elem_size;\n+    v.visit_elem(ptr);\n+  }\n+}\n+\n@@ -171,0 +180,5 @@\n+template <class Elem, MEMFLAGS flag>\n+G1SegmentedArray<Elem, flag>::~G1SegmentedArray() {\n+  drop_all();\n+}\n+\n@@ -236,0 +250,36 @@\n+#ifdef ASSERT\n+template <MEMFLAGS flag>\n+class LengthVisitor {\n+  uint _total;\n+public:\n+  LengthVisitor() : _total(0) {}\n+  void visit_buffer(G1SegmentedArrayBuffer<flag>* node, uint limit) {\n+    _total += limit;\n+  }\n+  uint length() const {\n+    return _total;\n+  }\n+};\n+\n+template <class Elem, MEMFLAGS flag>\n+uint G1SegmentedArray<Elem, flag>::calculate_length() const {\n+  LengthVisitor<flag> v;\n+  iterate_nodes(v);\n+  return v.length();\n+}\n+#endif\n+\n+template <class Elem, MEMFLAGS flag>\n+template <typename Visitor>\n+void G1SegmentedArray<Elem, flag>::iterate_nodes(Visitor& v) const {\n+  G1SegmentedArrayBuffer<flag>* cur = Atomic::load_acquire(&_first);\n+\n+  assert((cur != nullptr) == (_last != nullptr),\n+         \"If there is at least one element, there must be a last one\");\n+\n+  while (cur != nullptr) {\n+    v.visit_buffer(cur, cur->length());\n+    cur = cur->next();\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1SegmentedArray.inline.hpp","additions":50,"deletions":0,"binary":false,"changes":50,"status":"modified"},{"patch":"@@ -111,0 +111,8 @@\n+void HeapRegion::record_evac_failure_obj(oop obj) {\n+  _evac_failure_objs.record(obj);\n+}\n+\n+void HeapRegion::iterate_evac_failure_objs(ObjectClosure* closure) {\n+  _evac_failure_objs.iterate(closure);\n+}\n+\n@@ -251,1 +259,2 @@\n-  _node_index(G1NUMA::UnknownNodeIndex)\n+  _node_index(G1NUMA::UnknownNodeIndex),\n+  _evac_failure_objs(hrm_index, _bottom)\n","filename":"src\/hotspot\/share\/gc\/g1\/heapRegion.cpp","additions":10,"deletions":1,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"gc\/g1\/g1EvacFailureObjectsSet.hpp\"\n@@ -261,0 +262,2 @@\n+  G1EvacFailureObjectsSet _evac_failure_objs;\n+\n@@ -557,0 +560,5 @@\n+  \/\/ Records evac failure objs during evaucation, this will help speed up iteration\n+  \/\/ of these objs later in *remove self forward* phase of post evacuation.\n+  void record_evac_failure_obj(oop obj);\n+  \/\/ Iterates evac failure objs which are recorded during evcauation.\n+  void iterate_evac_failure_objs(ObjectClosure* closure);\n","filename":"src\/hotspot\/share\/gc\/g1\/heapRegion.hpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"gc\/g1\/g1SegmentedArray.inline.hpp\"\n","filename":"src\/hotspot\/share\/gc\/g1\/heapRegion.inline.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -25,0 +25,3 @@\n+\n+#include \"gc\/g1\/g1BlockOffsetTable.inline.hpp\"\n+#include \"gc\/g1\/g1CardSet.inline.hpp\"\n@@ -26,0 +29,6 @@\n+#include \"gc\/g1\/g1RegionToSpaceMapper.hpp\"\n+#include \"gc\/g1\/heapRegion.inline.hpp\"\n+#include \"gc\/g1\/heapRegionSet.hpp\"\n+#include \"memory\/allocation.hpp\"\n+#include \"memory\/memRegion.hpp\"\n+#include \"memory\/virtualspace.hpp\"\n","filename":"test\/hotspot\/gtest\/gc\/g1\/test_freeRegionList.cpp","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"}]}