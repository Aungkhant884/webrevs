{"files":[{"patch":"@@ -147,0 +147,10 @@\n+uint G1CollectedHeap::get_chunks_per_region() {\n+  uint log_region_size = HeapRegion::LogOfHRGrainBytes;\n+  \/\/ Limit the expected input values to current known possible values of the\n+  \/\/ (log) region size. Adjust as necessary after testing if changing the permissible\n+  \/\/ values for region size.\n+  assert(log_region_size >= 20 && log_region_size <= 29,\n+         \"expected value in [20,29], but got %u\", log_region_size);\n+  return 1u << (log_region_size \/ 2 - 4);\n+}\n+\n@@ -3293,4 +3303,3 @@\n-void G1CollectedHeap::mark_evac_failure_object(const oop obj) const {\n-  \/\/ All objects failing evacuation are live. What we'll do is\n-  \/\/ that we'll update the marking info so that they are\n-  \/\/ all below TAMS and explicitly marked.\n+void G1CollectedHeap::mark_evac_failure_object(uint worker_id, const oop obj, size_t obj_size) const {\n+  assert(!_cm->is_marked_in_bitmap(obj), \"must be\");\n+\n@@ -3298,0 +3307,3 @@\n+  if (collector_state()->in_concurrent_start_gc()) {\n+    _cm->add_to_liveness(worker_id, obj, obj_size);\n+  }\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectedHeap.cpp","additions":16,"deletions":4,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -526,0 +526,8 @@\n+  \/\/ Return \"optimal\" number of chunks per region we want to use for claiming areas\n+  \/\/ within a region to claim.\n+  \/\/ The returned value is a trade-off between granularity of work distribution and\n+  \/\/ memory usage and maintenance costs of that table.\n+  \/\/ Testing showed that 64 for 1M\/2M region, 128 for 4M\/8M regions, 256 for 16\/32M regions,\n+  \/\/ and so on seems to be such a good trade-off.\n+  static uint get_chunks_per_region();\n+\n@@ -1219,0 +1227,1 @@\n+  inline static bool is_obj_filler(const oop obj);\n@@ -1232,2 +1241,2 @@\n-  \/\/ Mark the live object that failed evacuation in the prev bitmap.\n-  void mark_evac_failure_object(oop obj) const;\n+  \/\/ Mark the live object that failed evacuation in the bitmap.\n+  void mark_evac_failure_object(uint worker_id, oop obj, size_t obj_size) const;\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectedHeap.hpp","additions":11,"deletions":2,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -216,0 +216,5 @@\n+inline bool G1CollectedHeap::is_obj_filler(const oop obj) {\n+  Klass* k = obj->klass();\n+  return k == Universe::fillerArrayKlassObj() || k == vmClasses::FillerObject_klass();\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectedHeap.inline.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -606,1 +606,2 @@\n-        \/\/ No need to clear bitmaps for empty regions.\n+        \/\/ No need to clear bitmaps for empty regions (which includes regions we\n+        \/\/ did not mark through).\n@@ -655,1 +656,1 @@\n-      r->note_end_of_clearing();\n+      r->reset_top_at_mark_start();\n@@ -1890,1 +1891,0 @@\n-  hr->note_end_of_clearing();\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ConcurrentMark.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -582,1 +582,1 @@\n-  inline void raw_mark_in_bitmap(oop p);\n+  inline void raw_mark_in_bitmap(oop obj);\n@@ -585,1 +585,1 @@\n-  \/\/ bitmap. This should only be used clean the bitmap during a\n+  \/\/ bitmap. This should only be used to clean the bitmap during a\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ConcurrentMark.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -283,2 +283,2 @@\n-inline void G1ConcurrentMark::raw_mark_in_bitmap(oop p) {\n-  _mark_bitmap.par_mark(p);\n+inline void G1ConcurrentMark::raw_mark_in_bitmap(oop obj) {\n+  _mark_bitmap.par_mark(obj);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ConcurrentMark.inline.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -32,1 +32,0 @@\n-#include \"gc\/g1\/g1HeapVerifier.hpp\"\n@@ -34,1 +33,1 @@\n-#include \"gc\/g1\/heapRegion.hpp\"\n+#include \"gc\/g1\/heapRegion.inline.hpp\"\n@@ -39,0 +38,2 @@\n+#include \"runtime\/prefetch.hpp\"\n+#include \"utilities\/bitMap.inline.hpp\"\n@@ -40,6 +41,6 @@\n-class RemoveSelfForwardPtrObjClosure {\n-  G1CollectedHeap* _g1h;\n-  G1ConcurrentMark* _cm;\n-  HeapRegion* _hr;\n-  size_t _marked_words;\n-  bool _during_concurrent_start;\n+\n+class PhaseTimesStat {\n+  static constexpr G1GCPhaseTimes::GCParPhases phase_name =\n+    G1GCPhaseTimes::RemoveSelfForwards;\n+\n+  G1GCPhaseTimes* _phase_times;\n@@ -47,1 +48,1 @@\n-  HeapWord* _last_forwarded_object_end;\n+  Ticks _start;\n@@ -50,8 +51,2 @@\n-  RemoveSelfForwardPtrObjClosure(HeapRegion* hr,\n-                                 bool during_concurrent_start,\n-                                 uint worker_id) :\n-    _g1h(G1CollectedHeap::heap()),\n-    _cm(_g1h->concurrent_mark()),\n-    _hr(hr),\n-    _marked_words(0),\n-    _during_concurrent_start(during_concurrent_start),\n+  PhaseTimesStat(G1GCPhaseTimes* phase_times, uint worker_id) :\n+    _phase_times(phase_times),\n@@ -59,31 +54,1 @@\n-    _last_forwarded_object_end(hr->bottom()) { }\n-\n-  size_t marked_bytes() { return _marked_words * HeapWordSize; }\n-\n-  \/\/ Handle the marked objects in the region. These are self-forwarded objects\n-  \/\/ that need to be kept live. We need to update the remembered sets of these\n-  \/\/ objects. Further update the BOT and marks.\n-  \/\/ We can coalesce and overwrite the remaining heap contents with dummy objects\n-  \/\/ as they have either been dead or evacuated (which are unreferenced now, i.e.\n-  \/\/ dead too) already.\n-  size_t apply(oop obj) {\n-    HeapWord* obj_addr = cast_from_oop<HeapWord*>(obj);\n-    size_t obj_size = obj->size();\n-    assert(_last_forwarded_object_end <= obj_addr, \"should iterate in ascending address order\");\n-    assert(_hr->is_in(obj_addr), \"sanity\");\n-\n-    \/\/ The object failed to move.\n-    assert(obj->is_forwarded() && obj->forwardee() == obj, \"sanity\");\n-\n-    zap_dead_objects(_last_forwarded_object_end, obj_addr);\n-\n-    assert(_cm->is_marked_in_bitmap(obj), \"should be correctly marked\");\n-    if (_during_concurrent_start) {\n-      \/\/ If the evacuation failure occurs during concurrent start we should do\n-      \/\/ any additional necessary per-object actions.\n-      _cm->add_to_liveness(_worker_id, obj, obj_size);\n-    }\n-\n-    _marked_words += obj_size;\n-    \/\/ Reset the markWord\n-    obj->init_mark();\n+    _start(Ticks::now()) { }\n@@ -91,4 +56,4 @@\n-    HeapWord* obj_end = obj_addr + obj_size;\n-    _last_forwarded_object_end = obj_end;\n-    _hr->update_bot_for_block(obj_addr, obj_end);\n-    return obj_size;\n+  ~PhaseTimesStat() {\n+    _phase_times->record_or_add_time_secs(phase_name,\n+                                          _worker_id,\n+                                          (Ticks::now() - _start).seconds());\n@@ -97,6 +62,6 @@\n-  \/\/ Fill the memory area from start to end with filler objects, and update the BOT\n-  \/\/ accordingly.\n-  void zap_dead_objects(HeapWord* start, HeapWord* end) {\n-    if (start == end) {\n-      return;\n-    }\n+  void register_empty_chunk() {\n+    _phase_times->record_or_add_thread_work_item(phase_name,\n+                                                 _worker_id,\n+                                                 1,\n+                                                 G1GCPhaseTimes::RemoveSelfForwardEmptyChunksNum);\n+  }\n@@ -104,1 +69,5 @@\n-    _hr->fill_range_with_dead_objects(start, end);\n+  void register_nonempty_chunk() {\n+    _phase_times->record_or_add_thread_work_item(phase_name,\n+                                                 _worker_id,\n+                                                 1,\n+                                                 G1GCPhaseTimes::RemoveSelfForwardChunksNum);\n@@ -107,2 +76,11 @@\n-  void zap_remainder() {\n-    zap_dead_objects(_last_forwarded_object_end, _hr->top());\n+  void register_objects_count_and_size(size_t num_marked_obj, size_t marked_words) {\n+    _phase_times->record_or_add_thread_work_item(phase_name,\n+                                                 _worker_id,\n+                                                 num_marked_obj,\n+                                                 G1GCPhaseTimes::RemoveSelfForwardObjectsNum);\n+\n+    size_t marked_bytes = marked_words * HeapWordSize;\n+    _phase_times->record_or_add_thread_work_item(phase_name,\n+                                                 _worker_id,\n+                                                 marked_bytes,\n+                                                 G1GCPhaseTimes::RemoveSelfForwardObjectsBytes);\n@@ -112,3 +90,8 @@\n-class RemoveSelfForwardPtrHRClosure: public HeapRegionClosure {\n-  G1CollectedHeap* _g1h;\n-  uint _worker_id;\n+\/\/ Fill the memory area from start to end with filler objects, and update the BOT\n+\/\/ accordingly. Since we clear and use the bitmap for marking objects that failed\n+\/\/ evacuation, there is no other work to be done there.\n+static size_t zap_dead_objects(HeapRegion* hr, HeapWord* start, HeapWord* end) {\n+  assert(start <= end, \"precondition\");\n+  if (start == end) {\n+    return 0;\n+  }\n@@ -116,1 +99,3 @@\n-  G1EvacFailureRegions* _evac_failure_regions;\n+  hr->fill_range_with_dead_objects(start, end);\n+  return pointer_delta(end, start);\n+}\n@@ -118,1 +103,5 @@\n-  G1GCPhaseTimes* _phase_times;\n+static void update_garbage_words_in_hr(HeapRegion* hr, size_t garbage_words) {\n+  if (garbage_words != 0) {\n+    hr->note_self_forward_chunk_done(garbage_words * HeapWordSize);\n+  }\n+}\n@@ -120,7 +109,19 @@\n-public:\n-  RemoveSelfForwardPtrHRClosure(uint worker_id,\n-                                G1EvacFailureRegions* evac_failure_regions) :\n-    _g1h(G1CollectedHeap::heap()),\n-    _worker_id(worker_id),\n-    _evac_failure_regions(evac_failure_regions),\n-    _phase_times(G1CollectedHeap::heap()->phase_times()) {\n+static void prefetch_obj(HeapWord* obj_addr) {\n+  Prefetch::write(obj_addr, PrefetchScanIntervalInBytes);\n+}\n+\n+void G1RemoveSelfForwardsTask::process_chunk(uint worker_id,\n+                                             uint chunk_idx) {\n+  PhaseTimesStat stat(_g1h->phase_times(), worker_id);\n+\n+  G1CMBitMap* bitmap = _cm->mark_bitmap();\n+  const uint region_idx = _evac_failure_regions->get_region_idx(chunk_idx \/ _num_chunks_per_region);\n+  HeapRegion* hr = _g1h->region_at(region_idx);\n+\n+  HeapWord* hr_bottom = hr->bottom();\n+  HeapWord* hr_top = hr->top();\n+  HeapWord* chunk_start = hr_bottom + (chunk_idx % _num_chunks_per_region) * _chunk_size;\n+\n+  assert(chunk_start < hr->end(), \"inv\");\n+  if (chunk_start >= hr_top) {\n+    return;\n@@ -129,24 +130,4 @@\n-  size_t remove_self_forward_ptr_by_walking_hr(HeapRegion* hr,\n-                                               bool during_concurrent_start) {\n-    RemoveSelfForwardPtrObjClosure rspc(hr,\n-                                        during_concurrent_start,\n-                                        _worker_id);\n-\n-    \/\/ All objects that failed evacuation has been marked in the bitmap.\n-    \/\/ Use the bitmap to apply the above closure to all failing objects.\n-    G1CMBitMap* bitmap = _g1h->concurrent_mark()->mark_bitmap();\n-    hr->apply_to_marked_objects(bitmap, &rspc);\n-    \/\/ Need to zap the remainder area of the processed region.\n-    rspc.zap_remainder();\n-    \/\/ Now clear all the marks to be ready for a new marking cyle.\n-    if (!during_concurrent_start) {\n-      assert(hr->top_at_mark_start() == hr->bottom(), \"TAMS must be bottom to make all objects look live\");\n-      _g1h->clear_bitmap_for_region(hr);\n-    } else {\n-      assert(hr->top_at_mark_start() == hr->top(), \"TAMS must be top for bitmap to have any value\");\n-      \/\/ Keep the bits.\n-    }\n-    \/\/ We never evacuate Old (non-humongous, non-archive) regions during scrubbing\n-    \/\/ (only afterwards); other regions (young, humongous, archive) never need\n-    \/\/ scrubbing, so the following must hold.\n-    assert(hr->parsable_bottom() == hr->bottom(), \"PB must be bottom to make the whole area parsable\");\n+  HeapWord* chunk_end = MIN2(chunk_start + _chunk_size, hr_top);\n+  HeapWord* first_marked_addr = bitmap->get_next_marked_addr(chunk_start, hr_top);\n+\n+  size_t garbage_words = 0;\n@@ -154,1 +135,3 @@\n-    return rspc.marked_bytes();\n+  if (chunk_start == hr_bottom) {\n+    \/\/ This is the bottom-most chunk in this region; zap [bottom, first_marked_addr).\n+    garbage_words += zap_dead_objects(hr, hr_bottom, first_marked_addr);\n@@ -157,4 +140,5 @@\n-  bool do_heap_region(HeapRegion *hr) {\n-    assert(!hr->is_pinned(), \"Unexpected pinned region at index %u\", hr->hrm_index());\n-    assert(hr->in_collection_set(), \"bad CS\");\n-    assert(_evac_failure_regions->contains(hr->hrm_index()), \"precondition\");\n+  if (first_marked_addr >= chunk_end) {\n+    stat.register_empty_chunk();\n+    update_garbage_words_in_hr(hr, garbage_words);\n+    return;\n+  }\n@@ -162,1 +146,1 @@\n-    hr->clear_index_in_opt_cset();\n+  stat.register_nonempty_chunk();\n@@ -164,1 +148,2 @@\n-    bool during_concurrent_start = _g1h->collector_state()->in_concurrent_start_gc();\n+  size_t num_marked_objs = 0;\n+  size_t marked_words = 0;\n@@ -166,1 +151,7 @@\n-    hr->note_self_forwarding_removal_start(during_concurrent_start);\n+  HeapWord* obj_addr = first_marked_addr;\n+  assert(chunk_start <= obj_addr && obj_addr < chunk_end,\n+         \"object \" PTR_FORMAT \" must be within chunk [\" PTR_FORMAT \", \" PTR_FORMAT \"[\",\n+         p2i(obj_addr), p2i(chunk_start), p2i(chunk_end));\n+  do {\n+    assert(bitmap->is_marked(obj_addr), \"inv\");\n+    prefetch_obj(obj_addr);\n@@ -168,4 +159,3 @@\n-    _phase_times->record_or_add_thread_work_item(G1GCPhaseTimes::RestoreRetainedRegions,\n-                                                   _worker_id,\n-                                                   1,\n-                                                   G1GCPhaseTimes::RestoreRetainedRegionsNum);\n+    oop obj = cast_to_oop(obj_addr);\n+    const size_t obj_size = obj->size();\n+    HeapWord* const obj_end_addr = obj_addr + obj_size;\n@@ -173,1 +163,5 @@\n-    size_t live_bytes = remove_self_forward_ptr_by_walking_hr(hr, during_concurrent_start);\n+    {\n+      \/\/ Process marked object.\n+      assert(obj->is_forwarded() && obj->forwardee() == obj, \"must be self-forwarded\");\n+      obj->init_mark();\n+      hr->update_bot_for_block(obj_addr, obj_end_addr);\n@@ -175,2 +169,4 @@\n-    hr->rem_set()->clean_code_roots(hr);\n-    hr->rem_set()->clear_locked(true);\n+      \/\/ Statistics\n+      num_marked_objs++;\n+      marked_words += obj_size;\n+    }\n@@ -178,1 +174,7 @@\n-    hr->note_self_forwarding_removal_end(live_bytes);\n+    assert(obj_end_addr <= hr_top, \"inv\");\n+    \/\/ Use hr_top as the limit so that we zap dead ranges up to the next\n+    \/\/ marked obj or hr_top.\n+    HeapWord* next_marked_obj_addr = bitmap->get_next_marked_addr(obj_end_addr, hr_top);\n+    garbage_words += zap_dead_objects(hr, obj_end_addr, next_marked_obj_addr);\n+    obj_addr = next_marked_obj_addr;\n+  } while (obj_addr < chunk_end);\n@@ -180,3 +182,1 @@\n-    return false;\n-  }\n-};\n+  assert(marked_words > 0 && num_marked_objs > 0, \"inv\");\n@@ -184,1 +184,6 @@\n-G1ParRemoveSelfForwardPtrsTask::G1ParRemoveSelfForwardPtrsTask(G1EvacFailureRegions* evac_failure_regions) :\n+  stat.register_objects_count_and_size(num_marked_objs, marked_words);\n+\n+  update_garbage_words_in_hr(hr, garbage_words);\n+}\n+\n+G1RemoveSelfForwardsTask::G1RemoveSelfForwardsTask(G1EvacFailureRegions* evac_failure_regions) :\n@@ -187,2 +192,6 @@\n-  _hrclaimer(_g1h->workers()->active_workers()),\n-  _evac_failure_regions(evac_failure_regions) { }\n+  _cm(_g1h->concurrent_mark()),\n+  _evac_failure_regions(evac_failure_regions),\n+  _chunk_bitmap(mtGC) {\n+\n+  _num_evac_fail_regions = _evac_failure_regions->num_regions_failed_evacuation();\n+  _num_chunks_per_region = G1CollectedHeap::get_chunks_per_region();\n@@ -190,2 +199,1 @@\n-void G1ParRemoveSelfForwardPtrsTask::work(uint worker_id) {\n-  RemoveSelfForwardPtrHRClosure rsfp_cl(worker_id, _evac_failure_regions);\n+  _chunk_size = static_cast<uint>(HeapRegion::GrainWords \/ _num_chunks_per_region);\n@@ -193,2 +201,4 @@\n-  \/\/ Iterate through all regions that failed evacuation during the entire collection.\n-  _evac_failure_regions->par_iterate(&rsfp_cl, &_hrclaimer, worker_id);\n+  log_debug(gc, ergo)(\"Initializing removing self forwards with %u chunks per region\",\n+                      _num_chunks_per_region);\n+\n+  _chunk_bitmap.resize(_num_chunks_per_region * _num_evac_fail_regions);\n@@ -197,2 +207,11 @@\n-uint G1ParRemoveSelfForwardPtrsTask::num_failed_regions() const {\n-  return _evac_failure_regions->num_regions_failed_evacuation();\n+void G1RemoveSelfForwardsTask::work(uint worker_id) {\n+  const uint total_workers = G1CollectedHeap::heap()->workers()->active_workers();\n+  const uint total_chunks = _num_chunks_per_region * _num_evac_fail_regions;\n+  const uint start_chunk_idx = worker_id * total_chunks \/ total_workers;\n+\n+  for (uint i = 0; i < total_chunks; i++) {\n+    const uint chunk_idx = (start_chunk_idx + i) % total_chunks;\n+    if (claim_chunk(chunk_idx)) {\n+      process_chunk(worker_id, chunk_idx);\n+    }\n+  }\n","filename":"src\/hotspot\/share\/gc\/g1\/g1EvacFailure.cpp","additions":144,"deletions":125,"binary":false,"changes":269,"status":"modified"},{"patch":"@@ -28,2 +28,0 @@\n-#include \"gc\/g1\/g1OopClosures.hpp\"\n-#include \"gc\/g1\/heapRegionManager.hpp\"\n@@ -32,0 +30,1 @@\n+#include \"utilities\/bitMap.hpp\"\n@@ -34,0 +33,1 @@\n+class G1ConcurrentMark;\n@@ -36,4 +36,3 @@\n-\/\/ Task to fixup self-forwarding pointers\n-\/\/ installed as a result of an evacuation failure.\n-class G1ParRemoveSelfForwardPtrsTask: public WorkerTask {\n-protected:\n+\/\/ Task to fixup self-forwarding pointers within the objects installed as a result\n+\/\/ of an evacuation failure.\n+class G1RemoveSelfForwardsTask : public WorkerTask {\n@@ -41,1 +40,1 @@\n-  HeapRegionClaimer _hrclaimer;\n+  G1ConcurrentMark* _cm;\n@@ -44,1 +43,11 @@\n-  uint volatile _num_failed_regions;\n+  CHeapBitMap _chunk_bitmap;\n+\n+  uint _num_chunks_per_region;\n+  uint _num_evac_fail_regions;\n+  size_t _chunk_size;\n+\n+  bool claim_chunk(uint chunk_idx) {\n+    return _chunk_bitmap.par_set_bit(chunk_idx);\n+  }\n+\n+  void process_chunk(uint worker_id, uint chunk_idx);\n@@ -47,1 +56,1 @@\n-  G1ParRemoveSelfForwardPtrsTask(G1EvacFailureRegions* evac_failure_regions);\n+  explicit G1RemoveSelfForwardsTask(G1EvacFailureRegions* evac_failure_regions);\n@@ -50,2 +59,0 @@\n-\n-  uint num_failed_regions() const;\n","filename":"src\/hotspot\/share\/gc\/g1\/g1EvacFailure.hpp","additions":18,"deletions":11,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2021, Huawei Technologies Co., Ltd. All rights reserved.\n+ * Copyright (c) 2021, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n@@ -27,1 +27,2 @@\n-#include \"gc\/g1\/g1CollectedHeap.hpp\"\n+#include \"gc\/g1\/g1BatchedTask.hpp\"\n+#include \"gc\/g1\/g1CollectedHeap.inline.hpp\"\n@@ -37,2 +38,1 @@\n-  _evac_failure_regions_cur_length(0),\n-  _max_regions(0) { }\n+  _evac_failure_regions_cur_length(0) { }\n@@ -46,3 +46,2 @@\n-  _max_regions = max_regions;\n-  _regions_failed_evacuation.resize(_max_regions);\n-  _evac_failure_regions = NEW_C_HEAP_ARRAY(uint, _max_regions, mtGC);\n+  _regions_failed_evacuation.resize(max_regions);\n+  _evac_failure_regions = NEW_C_HEAP_ARRAY(uint, max_regions, mtGC);\n@@ -53,0 +52,1 @@\n+\n@@ -55,1 +55,4 @@\n-  _max_regions = 0; \/\/ To have any record() attempt fail in the future.\n+}\n+\n+bool G1EvacFailureRegions::contains(uint region_idx) const {\n+  return _regions_failed_evacuation.par_at(region_idx, memory_order_relaxed);\n@@ -59,1 +62,1 @@\n-                                       HeapRegionClaimer* _hrclaimer,\n+                                       HeapRegionClaimer* hrclaimer,\n@@ -62,1 +65,1 @@\n-                                                     _hrclaimer,\n+                                                     hrclaimer,\n@@ -67,5 +70,0 @@\n-\n-bool G1EvacFailureRegions::contains(uint region_idx) const {\n-  assert(region_idx < _max_regions, \"must be\");\n-  return _regions_failed_evacuation.par_at(region_idx, memory_order_relaxed);\n-}\n","filename":"src\/hotspot\/share\/gc\/g1\/g1EvacFailureRegions.cpp","additions":13,"deletions":15,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2021, Huawei Technologies Co., Ltd. All rights reserved.\n+ * Copyright (c) 2021, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n@@ -31,0 +31,2 @@\n+class G1AbstractSubTask;\n+class G1HeapRegionChunkClosure;\n@@ -44,2 +46,0 @@\n-  \/\/ Maximum of regions number.\n-  uint _max_regions;\n@@ -51,0 +51,5 @@\n+  uint get_region_idx(uint idx) const {\n+    assert(idx < _evac_failure_regions_cur_length, \"precondition\");\n+    return _evac_failure_regions[idx];\n+  }\n+\n@@ -58,1 +63,1 @@\n-                   HeapRegionClaimer* _hrclaimer,\n+                   HeapRegionClaimer* hrclaimer,\n@@ -61,0 +66,3 @@\n+  \/\/ Return a G1AbstractSubTask which does necessary preparation for evacuation failure regions\n+  G1AbstractSubTask* create_prepare_regions_task();\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1EvacFailureRegions.hpp","additions":12,"deletions":4,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2021, Huawei Technologies Co., Ltd. All rights reserved.\n+ * Copyright (c) 2021, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n@@ -30,1 +30,0 @@\n-#include \"utilities\/bitMap.inline.hpp\"\n@@ -33,1 +32,0 @@\n-  assert(region_idx < _max_regions, \"must be\");\n@@ -39,0 +37,5 @@\n+\n+    G1CollectedHeap* g1h = G1CollectedHeap::heap();\n+    HeapRegion* hr = g1h->region_at(region_idx);\n+    G1CollectorState* state = g1h->collector_state();\n+    hr->note_evacuation_failure(state->in_concurrent_start_gc());\n","filename":"src\/hotspot\/share\/gc\/g1\/g1EvacFailureRegions.inline.hpp","additions":6,"deletions":3,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -106,0 +106,1 @@\n+  _gc_par_phases[RemoveSelfForwards] = new WorkerDataArray<double>(\"RemoveSelfForwards\", \"Remove Self Forwards (ms):\", max_gc_threads);\n@@ -115,0 +116,1 @@\n+  _gc_par_phases[ClearRetainedRegionBitmaps] = new WorkerDataArray<double>(\"ClearRetainedRegionsBitmap\", \"Clear Retained Region Bitmaps (ms):\", max_gc_threads);\n@@ -137,0 +139,5 @@\n+  _gc_par_phases[RemoveSelfForwards]->create_thread_work_items(\"Forward Chunks:\", RemoveSelfForwardChunksNum);\n+  _gc_par_phases[RemoveSelfForwards]->create_thread_work_items(\"Empty Forward Chunks:\", RemoveSelfForwardEmptyChunksNum);\n+  _gc_par_phases[RemoveSelfForwards]->create_thread_work_items(\"Forward Objects:\", RemoveSelfForwardObjectsNum);\n+  _gc_par_phases[RemoveSelfForwards]->create_thread_work_items(\"Forward Bytes:\", RemoveSelfForwardObjectsBytes);\n+\n@@ -486,0 +493,1 @@\n+    debug_phase(_gc_par_phases[RemoveSelfForwards], 2);\n@@ -493,0 +501,1 @@\n+    debug_phase(_gc_par_phases[ClearRetainedRegionBitmaps], 1);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1GCPhaseTimes.cpp","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -81,0 +81,1 @@\n+    RemoveSelfForwards,\n@@ -90,0 +91,1 @@\n+    ClearRetainedRegionBitmaps,\n@@ -152,0 +154,7 @@\n+  enum RemoveSelfForwardsWorkItems {\n+    RemoveSelfForwardChunksNum,\n+    RemoveSelfForwardEmptyChunksNum,\n+    RemoveSelfForwardObjectsNum,\n+    RemoveSelfForwardObjectsBytes,\n+  };\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1GCPhaseTimes.hpp","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -626,5 +626,0 @@\n-    \/\/ Objects failing evacuation will turn into old objects since the regions\n-    \/\/ are relabeled as such. We mark the failing objects in the marking bitmap\n-    \/\/ and later use it to handle all failed objects.\n-    _g1h->mark_evac_failure_object(old);\n-\n@@ -635,0 +630,4 @@\n+    \/\/ Mark the failing object in the marking bitmap and later use the bitmap to handle\n+    \/\/ evacuation failure recovery.\n+    _g1h->mark_evac_failure_object(_worker_id, old, word_sz);\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ParScanThreadState.cpp","additions":4,"deletions":5,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -106,15 +106,0 @@\n-  \/\/ Return \"optimal\" number of chunks per region we want to use for claiming areas\n-  \/\/ within a region to claim. Dependent on the region size as proxy for the heap\n-  \/\/ size, we limit the total number of chunks to limit memory usage and maintenance\n-  \/\/ effort of that table vs. granularity of distributing scanning work.\n-  \/\/ Testing showed that 64 for 1M\/2M region, 128 for 4M\/8M regions, 256 for 16\/32M regions,\n-  \/\/ and so on seems to be such a good trade-off.\n-  static uint get_chunks_per_region(uint log_region_size) {\n-    \/\/ Limit the expected input values to current known possible values of the\n-    \/\/ (log) region size. Adjust as necessary after testing if changing the permissible\n-    \/\/ values for region size.\n-    assert(log_region_size >= 20 && log_region_size <= 29,\n-           \"expected value in [20,29], but got %u\", log_region_size);\n-    return 1u << (log_region_size \/ 2 - 4);\n-  }\n-\n@@ -287,1 +272,1 @@\n-    _scan_chunks_per_region(get_chunks_per_region(HeapRegion::LogOfHRGrainBytes)),\n+    _scan_chunks_per_region(G1CollectedHeap::get_chunks_per_region()),\n@@ -1271,1 +1256,1 @@\n-             \"Bitmap should have no mark for region %u\", hr->hrm_index());\n+             \"Bitmap should have no mark for region %u (%s)\", hr->hrm_index(), hr->get_short_type_str());\n@@ -1300,0 +1285,1 @@\n+        hr->reset_top_at_mark_start();\n","filename":"src\/hotspot\/share\/gc\/g1\/g1RemSet.cpp","additions":3,"deletions":17,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -100,2 +100,2 @@\n-class G1PostEvacuateCollectionSetCleanupTask1::RemoveSelfForwardPtrsTask : public G1AbstractSubTask {\n-  G1ParRemoveSelfForwardPtrsTask _task;\n+class G1PostEvacuateCollectionSetCleanupTask1::RestoreRetainedRegionsTask : public G1AbstractSubTask {\n+  G1RemoveSelfForwardsTask _task;\n@@ -105,1 +105,1 @@\n-  RemoveSelfForwardPtrsTask(G1EvacFailureRegions* evac_failure_regions) :\n+  RestoreRetainedRegionsTask(G1EvacFailureRegions* evac_failure_regions) :\n@@ -108,1 +108,2 @@\n-    _evac_failure_regions(evac_failure_regions) { }\n+    _evac_failure_regions(evac_failure_regions) {\n+  }\n@@ -112,1 +113,3 @@\n-    return _evac_failure_regions->num_regions_failed_evacuation();\n+\n+    double workers_per_region = (double)G1CollectedHeap::get_chunks_per_region() \/ G1RestoreRetainedRegionChunksPerWorker;\n+    return workers_per_region * _evac_failure_regions->num_regions_failed_evacuation();\n@@ -131,0 +134,1 @@\n+  add_parallel_task(G1CollectedHeap::heap()->rem_set()->create_cleanup_after_scan_heap_roots_task());\n@@ -132,1 +136,1 @@\n-    add_parallel_task(new RemoveSelfForwardPtrsTask(evac_failure_regions));\n+    add_parallel_task(new RestoreRetainedRegionsTask(evac_failure_regions));\n@@ -134,1 +138,0 @@\n-  add_parallel_task(G1CollectedHeap::heap()->rem_set()->create_cleanup_after_scan_heap_roots_task());\n@@ -320,1 +323,0 @@\n- private:\n@@ -336,1 +338,1 @@\n- public:\n+public:\n@@ -357,0 +359,39 @@\n+class G1PostEvacuateCollectionSetCleanupTask2::ClearRetainedRegionBitmaps : public G1AbstractSubTask {\n+  G1EvacFailureRegions* _evac_failure_regions;\n+  HeapRegionClaimer _claimer;\n+\n+  class ClearRetainedRegionBitmapsClosure : public HeapRegionClosure {\n+  public:\n+\n+    bool do_heap_region(HeapRegion* r) override {\n+      assert(r->bottom() == r->top_at_mark_start(),\n+             \"TAMS should have been reset for region %u\", r->hrm_index());\n+      G1CollectedHeap::heap()->clear_bitmap_for_region(r);\n+      return false;\n+    }\n+  };\n+\n+public:\n+\n+  ClearRetainedRegionBitmaps(G1EvacFailureRegions* evac_failure_regions) :\n+    G1AbstractSubTask(G1GCPhaseTimes::ClearRetainedRegionBitmaps),\n+    _evac_failure_regions(evac_failure_regions),\n+    _claimer(0) {\n+    assert(!G1CollectedHeap::heap()->collector_state()->in_concurrent_start_gc(),\n+           \"Should not clear bitmaps of retained regions during concurrent start\");\n+  }\n+\n+  void set_max_workers(uint max_workers) override {\n+    _claimer.set_n_workers(max_workers);\n+  }\n+\n+  double worker_cost() const override {\n+    return _evac_failure_regions->num_regions_failed_evacuation();\n+  }\n+\n+  void do_work(uint worker_id) override {\n+    ClearRetainedRegionBitmapsClosure cl;\n+    _evac_failure_regions->par_iterate(&cl, &_claimer, worker_id);\n+  }\n+};\n+\n@@ -532,0 +573,9 @@\n+    G1GCPhaseTimes* p = _g1h->phase_times();\n+    assert(!r->is_pinned(), \"Unexpected pinned region at index %u\", r->hrm_index());\n+    assert(r->in_collection_set(), \"bad CS\");\n+\n+    p->record_or_add_thread_work_item(G1GCPhaseTimes::RestoreRetainedRegions,\n+                                      _worker_id,\n+                                      1,\n+                                      G1GCPhaseTimes::RestoreRetainedRegionsNum);\n+\n@@ -679,0 +729,4 @@\n+    \/\/ Keep marks on bitmaps in retained regions during concurrent start - they will all be old.\n+    if (!G1CollectedHeap::heap()->collector_state()->in_concurrent_start_gc()) {\n+      add_parallel_task(new ClearRetainedRegionBitmaps(evac_failure_regions));\n+    }\n@@ -682,2 +736,2 @@\n-                                                   per_thread_states->surviving_young_words(),\n-                                                   evac_failure_regions));\n+                                              per_thread_states->surviving_young_words(),\n+                                              evac_failure_regions));\n","filename":"src\/hotspot\/share\/gc\/g1\/g1YoungGCPostEvacuateTasks.cpp","additions":65,"deletions":11,"binary":false,"changes":76,"status":"modified"},{"patch":"@@ -42,1 +42,0 @@\n-\/\/ - Remove Self Forwards (on evacuation failure)\n@@ -44,0 +43,1 @@\n+\/\/ - Restore retained regions (on evacuation failure)\n@@ -48,1 +48,1 @@\n-  class RemoveSelfForwardPtrsTask;\n+  class RestoreRetainedRegionsTask;\n@@ -60,0 +60,1 @@\n+\/\/ - Clear Retained Region Bitmaps (on evacuation failure)\n@@ -71,0 +72,1 @@\n+  class ClearRetainedRegionBitmaps;\n","filename":"src\/hotspot\/share\/gc\/g1\/g1YoungGCPostEvacuateTasks.hpp","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -375,0 +375,5 @@\n+  product(uint, G1RestoreRetainedRegionChunksPerWorker, 16, DIAGNOSTIC,     \\\n+          \"The number of chunks assigned per worker thread for \"            \\\n+          \"retained region restore purposes.\")                              \\\n+          range(1, 256)                                                     \\\n+                                                                            \\\n","filename":"src\/hotspot\/share\/gc\/g1\/g1_globals.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -106,1 +106,5 @@\n-  set_old();\n+  clear_index_in_opt_cset();\n+  move_to_old();\n+\n+  _rem_set->clean_code_roots(this);\n+  _rem_set->clear_locked(true \/* only_cardset *\/);\n@@ -271,3 +275,5 @@\n-void HeapRegion::note_self_forwarding_removal_start(bool during_concurrent_start) {\n-  \/\/ We always scrub the region to make sure the entire region is\n-  \/\/ parsable after the self-forwarding point removal.\n+void HeapRegion::note_evacuation_failure(bool during_concurrent_start) {\n+  \/\/ PB must be bottom - we only evacuate old gen regions after scrubbing, and\n+  \/\/ young gen regions never have their PB set to anything other than bottom.\n+  assert(parsable_bottom_acquire() == bottom(), \"must be\");\n+\n@@ -288,4 +294,2 @@\n-void HeapRegion::note_self_forwarding_removal_end(size_t marked_bytes) {\n-  assert(marked_bytes <= used(),\n-         \"marked: \" SIZE_FORMAT \" used: \" SIZE_FORMAT, marked_bytes, used());\n-  _garbage_bytes = used() - marked_bytes;\n+void HeapRegion::note_self_forward_chunk_done(size_t garbage_bytes) {\n+  Atomic::add(&_garbage_bytes, garbage_bytes, memory_order_relaxed);\n@@ -295,1 +299,0 @@\n-\n@@ -444,1 +447,1 @@\n-               p2i(top_at_mark_start()), p2i(parsable_bottom_acquire()), rem_set()->get_state_str());\n+            p2i(top_at_mark_start()), p2i(parsable_bottom_acquire()), rem_set()->get_state_str());\n","filename":"src\/hotspot\/share\/gc\/g1\/heapRegion.cpp","additions":13,"deletions":10,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -146,2 +146,0 @@\n-  static bool obj_is_filler(oop obj);\n-\n@@ -149,0 +147,1 @@\n+\n@@ -247,5 +246,1 @@\n-  void init_top_at_mark_start() {\n-    set_top_at_mark_start(bottom());\n-    _parsable_bottom = bottom();\n-    _garbage_bytes = 0;\n-  }\n+  inline void init_top_at_mark_start();\n@@ -379,1 +374,1 @@\n-  inline void note_end_of_clearing();\n+  inline void reset_top_at_mark_start();\n@@ -508,3 +503,3 @@\n-  \/\/ Notify the region that we are about to start processing\n-  \/\/ self-forwarded objects during evac failure handling.\n-  void note_self_forwarding_removal_start(bool during_concurrent_start);\n+  \/\/ Notify the region that an evacuation failure occurred for an object within this\n+  \/\/ region.\n+  void note_evacuation_failure(bool during_concurrent_start);\n@@ -512,3 +507,3 @@\n-  \/\/ Notify the region that we have finished processing self-forwarded\n-  \/\/ objects during evac failure handling.\n-  void note_self_forwarding_removal_end(size_t marked_bytes);\n+  \/\/ Notify the region that we have partially finished processing self-forwarded\n+  \/\/ objects during evacuation failure handling.\n+  void note_self_forward_chunk_done(size_t garbage_bytes);\n","filename":"src\/hotspot\/share\/gc\/g1\/heapRegion.hpp","additions":9,"deletions":14,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -142,5 +142,0 @@\n-inline bool HeapRegion::obj_is_filler(const oop obj) {\n-  Klass* k = obj->klass();\n-  return k == Universe::fillerArrayKlassObj() || k == vmClasses::FillerObject_klass();\n-}\n-\n@@ -162,1 +157,1 @@\n-  return obj_is_filler(obj);\n+  return G1CollectedHeap::is_obj_filler(obj);\n@@ -204,1 +199,1 @@\n-  set_top_at_mark_start(bottom());\n+  reset_top_at_mark_start();\n@@ -325,1 +320,7 @@\n-inline void HeapRegion::note_end_of_clearing() {\n+inline void HeapRegion::init_top_at_mark_start() {\n+  reset_top_at_mark_start();\n+  _parsable_bottom = bottom();\n+  _garbage_bytes = 0;\n+}\n+\n+inline void HeapRegion::reset_top_at_mark_start() {\n","filename":"src\/hotspot\/share\/gc\/g1\/heapRegion.inline.hpp","additions":9,"deletions":8,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -128,1 +128,1 @@\n-            \/\/ The following two phases only occur on evacuation failure.\n+            \/\/ The following phases only occur on evacuation failure.\n@@ -130,0 +130,1 @@\n+            \"RemoveSelfForwards\",\n@@ -131,1 +132,2 @@\n-\n+            \"ClearRetainedRegionsBitmap\",\n+            \/\/ Generally optional phases.\n","filename":"test\/jdk\/jdk\/jfr\/event\/gc\/collection\/TestG1ParallelPhases.java","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"}]}