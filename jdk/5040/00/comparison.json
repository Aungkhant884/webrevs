{"files":[{"patch":"@@ -47,1 +47,1 @@\n-\/\/ MT-safe pool of chunks to reduce malloc\/free thrashing\n+\/\/ MT-safe pool of same-sized chunks to reduce malloc\/free thrashing\n@@ -49,1 +49,1 @@\n-class ChunkPool: public CHeapObj<mtInternal> {\n+class ChunkPool {\n@@ -52,2 +52,1 @@\n-  size_t       _num_used;     \/\/ number of chunks currently checked out\n-  const size_t _size;         \/\/ size of each chunk (must be uniform)\n+  const size_t _size;         \/\/ (inner payload) size of the chunks this pool serves\n@@ -56,4 +55,2 @@\n-  static ChunkPool* _large_pool;\n-  static ChunkPool* _medium_pool;\n-  static ChunkPool* _small_pool;\n-  static ChunkPool* _tiny_pool;\n+  static const int _num_pools = 4;\n+  static ChunkPool _pools[_num_pools];\n@@ -61,2 +58,6 @@\n-  \/\/ return first element or null\n-  void* get_first() {\n+ public:\n+  ChunkPool(size_t size) : _first(NULL), _num_chunks(0), _size(size) {}\n+\n+  \/\/ Remove a chunk from the pool and return it; NULL if pool is empty.\n+  Chunk* remove_chunk() {\n+    ThreadCritical tc;\n@@ -71,21 +72,0 @@\n- public:\n-  \/\/ All chunks in a ChunkPool has the same size\n-   ChunkPool(size_t size) : _size(size) { _first = NULL; _num_chunks = _num_used = 0; }\n-\n-  \/\/ Allocate a new chunk from the pool (might expand the pool)\n-  NOINLINE void* allocate(size_t bytes, AllocFailType alloc_failmode) {\n-    assert(bytes == _size, \"bad size\");\n-    void* p = NULL;\n-    \/\/ No VM lock can be taken inside ThreadCritical lock, so os::malloc\n-    \/\/ should be done outside ThreadCritical lock due to NMT\n-    { ThreadCritical tc;\n-      _num_used++;\n-      p = get_first();\n-    }\n-    if (p == NULL) p = os::malloc(bytes, mtChunk, CURRENT_PC);\n-    if (p == NULL && alloc_failmode == AllocFailStrategy::EXIT_OOM) {\n-      vm_exit_out_of_memory(bytes, OOM_MALLOC_ERROR, \"ChunkPool::allocate\");\n-    }\n-    return p;\n-  }\n-\n@@ -93,2 +73,2 @@\n-  void free(Chunk* chunk) {\n-    assert(chunk->length() + Chunk::aligned_overhead_size() == _size, \"bad size\");\n+  void return_chunk(Chunk* chunk) {\n+    assert(chunk->length() == _size, \"wrong pool for this chunk\");\n@@ -96,3 +76,0 @@\n-    _num_used--;\n-\n-    \/\/ Add chunk to list\n@@ -105,1 +82,2 @@\n-  void free_all_but(size_t n) {\n+  void prune() {\n+    static const int blocksToKeep = 5;\n@@ -111,1 +89,1 @@\n-      if (_num_chunks > n) {\n+      if (_num_chunks > blocksToKeep) {\n@@ -114,1 +92,3 @@\n-        for (size_t i = 0; i < (n - 1) && cur != NULL; i++) cur = cur->next();\n+        for (size_t i = 0; i < (blocksToKeep - 1) && cur != NULL; i++) {\n+          cur = cur->next();\n+        }\n@@ -134,13 +114,0 @@\n-  \/\/ Accessors to preallocated pool's\n-  static ChunkPool* large_pool()  { assert(_large_pool  != NULL, \"must be initialized\"); return _large_pool;  }\n-  static ChunkPool* medium_pool() { assert(_medium_pool != NULL, \"must be initialized\"); return _medium_pool; }\n-  static ChunkPool* small_pool()  { assert(_small_pool  != NULL, \"must be initialized\"); return _small_pool;  }\n-  static ChunkPool* tiny_pool()   { assert(_tiny_pool   != NULL, \"must be initialized\"); return _tiny_pool;   }\n-\n-  static void initialize() {\n-    _large_pool  = new ChunkPool(Chunk::size        + Chunk::aligned_overhead_size());\n-    _medium_pool = new ChunkPool(Chunk::medium_size + Chunk::aligned_overhead_size());\n-    _small_pool  = new ChunkPool(Chunk::init_size   + Chunk::aligned_overhead_size());\n-    _tiny_pool   = new ChunkPool(Chunk::tiny_size   + Chunk::aligned_overhead_size());\n-  }\n-\n@@ -148,5 +115,3 @@\n-    enum { BlocksToKeep = 5 };\n-     _tiny_pool->free_all_but(BlocksToKeep);\n-     _small_pool->free_all_but(BlocksToKeep);\n-     _medium_pool->free_all_but(BlocksToKeep);\n-     _large_pool->free_all_but(BlocksToKeep);\n+    for (int i = 0; i < _num_pools; i++) {\n+      _pools[i].prune();\n+    }\n@@ -154,1 +119,0 @@\n-};\n@@ -156,4 +120,9 @@\n-ChunkPool* ChunkPool::_large_pool  = NULL;\n-ChunkPool* ChunkPool::_medium_pool = NULL;\n-ChunkPool* ChunkPool::_small_pool  = NULL;\n-ChunkPool* ChunkPool::_tiny_pool   = NULL;\n+  \/\/ Given a (inner payload) size, return the pool responsible for it, or NULL if the size is non-standard\n+  static ChunkPool* get_pool_for_size(size_t size) {\n+    for (int i = 0; i < _num_pools; i++) {\n+      if (_pools[i]._size == size) {\n+        return _pools + i;\n+      }\n+    }\n+    return NULL;\n+  }\n@@ -161,3 +130,1 @@\n-void chunkpool_init() {\n-  ChunkPool::initialize();\n-}\n+};\n@@ -165,0 +132,1 @@\n+ChunkPool ChunkPool::_pools[] = { Chunk::size, Chunk::medium_size, Chunk::init_size, Chunk::tiny_size };\n@@ -184,1 +152,0 @@\n-\n@@ -206,0 +173,10 @@\n+  \/\/ Try to reuse a cached chunk from the pool\n+  ChunkPool* pool = ChunkPool::get_pool_for_size(length);\n+  if (pool != NULL) {\n+    Chunk* c = pool->remove_chunk();\n+    if (c != NULL) {\n+      assert(c->length() == length, \"wrong length?\");\n+      return c;\n+    }\n+  }\n+  \/\/ Either the pool was empty, or this is a non-standard length. Allocate a new Chunk from C-heap.\n@@ -207,14 +184,3 @@\n-  switch (length) {\n-   case Chunk::size:        return ChunkPool::large_pool()->allocate(bytes, alloc_failmode);\n-   case Chunk::medium_size: return ChunkPool::medium_pool()->allocate(bytes, alloc_failmode);\n-   case Chunk::init_size:   return ChunkPool::small_pool()->allocate(bytes, alloc_failmode);\n-   case Chunk::tiny_size:   return ChunkPool::tiny_pool()->allocate(bytes, alloc_failmode);\n-   default: {\n-     void* p = os::malloc(bytes, mtChunk, CALLER_PC);\n-     if (p == NULL && alloc_failmode == AllocFailStrategy::EXIT_OOM) {\n-       vm_exit_out_of_memory(bytes, OOM_MALLOC_ERROR, \"Chunk::new\");\n-     }\n-     \/\/ We rely on arena alignment <= malloc alignment.\n-     assert(is_aligned(p, ARENA_AMALLOC_ALIGNMENT), \"Chunk start address misaligned.\");\n-     return p;\n-   }\n+  void* p = os::malloc(bytes, mtChunk, CALLER_PC);\n+  if (p == NULL && alloc_failmode == AllocFailStrategy::EXIT_OOM) {\n+    vm_exit_out_of_memory(bytes, OOM_MALLOC_ERROR, \"Chunk::new\");\n@@ -222,0 +188,3 @@\n+  \/\/ We rely on arena alignment <= malloc alignment.\n+  assert(is_aligned(p, ARENA_AMALLOC_ALIGNMENT), \"Chunk start address misaligned.\");\n+  return p;\n@@ -225,0 +194,1 @@\n+  \/\/ If this is a standard-sized chunk, return it to its pool; otherwise free it.\n@@ -226,8 +196,6 @@\n-  switch (c->length()) {\n-   case Chunk::size:        ChunkPool::large_pool()->free(c); break;\n-   case Chunk::medium_size: ChunkPool::medium_pool()->free(c); break;\n-   case Chunk::init_size:   ChunkPool::small_pool()->free(c); break;\n-   case Chunk::tiny_size:   ChunkPool::tiny_pool()->free(c); break;\n-   default:\n-     ThreadCritical tc;  \/\/ Free chunks under TC lock so that NMT adjustment is stable.\n-     os::free(c);\n+  ChunkPool* pool = ChunkPool::get_pool_for_size(c->length());\n+  if (pool != NULL) {\n+    pool->return_chunk(c);\n+  } else {\n+    ThreadCritical tc;  \/\/ Free chunks under TC lock so that NMT adjustment is stable.\n+    os::free(c);\n","filename":"src\/hotspot\/share\/memory\/arena.cpp","additions":56,"deletions":88,"binary":false,"changes":144,"status":"modified"},{"patch":"@@ -59,1 +59,0 @@\n-void chunkpool_init();\n@@ -107,1 +106,0 @@\n-  chunkpool_init();\n","filename":"src\/hotspot\/share\/runtime\/init.cpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -369,0 +369,31 @@\n+\n+static size_t random_arena_chunk_size() {\n+  \/\/ Return with a 50% rate a standard size, otherwise some random size\n+  if (os::random() % 10 < 5) {\n+    static const size_t standard_sizes[4] = {\n+        Chunk::tiny_size, Chunk::init_size, Chunk::size, Chunk::medium_size\n+    };\n+    return standard_sizes[os::random() % 4];\n+  }\n+  return ARENA_ALIGN(os::random() % 1024);\n+}\n+\n+TEST_VM(Arena, different_chunk_sizes) {\n+  \/\/ Test the creation\/pooling of chunks; since ChunkPool is hidden, the\n+  \/\/  only way to test this is to create\/destroy arenas with different init sizes,\n+  \/\/  which determines the initial chunk size.\n+  \/\/ Note that since the chunk pools are global and get cleaned out periodically,\n+  \/\/  there is no safe way to actually test their occupancy here.\n+  for (int i = 0; i < 1000; i ++) {\n+    \/\/ Unfortunately, Arenas cannot be newed,\n+    \/\/ so we are left with awkwardly placing a few on the stack.\n+    Arena ar0(mtTest, random_arena_chunk_size());\n+    Arena ar1(mtTest, random_arena_chunk_size());\n+    Arena ar2(mtTest, random_arena_chunk_size());\n+    Arena ar3(mtTest, random_arena_chunk_size());\n+    Arena ar4(mtTest, random_arena_chunk_size());\n+    Arena ar5(mtTest, random_arena_chunk_size());\n+    Arena ar6(mtTest, random_arena_chunk_size());\n+    Arena ar7(mtTest, random_arena_chunk_size());\n+  }\n+}\n","filename":"test\/hotspot\/gtest\/memory\/test_arena.cpp","additions":31,"deletions":0,"binary":false,"changes":31,"status":"modified"}]}