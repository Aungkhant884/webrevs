{"files":[{"patch":"@@ -30,2 +30,0 @@\n-#include \"gc\/shenandoah\/shenandoahHeap.inline.hpp\"\n-#include \"gc\/shenandoah\/shenandoahHeapRegion.inline.hpp\"\n@@ -36,0 +34,21 @@\n+\/\/ These constants are used to adjust the margin of error for the moving\n+\/\/ average of the allocation rate and cycle time. The units are standard\n+\/\/ deviations.\n+const double ShenandoahAdaptiveHeuristics::FULL_PENALTY_SD = 0.2;\n+const double ShenandoahAdaptiveHeuristics::DEGENERATE_PENALTY_SD = 0.1;\n+\n+\/\/ These are used to decide if we want to make any adjustments at all\n+\/\/ at the end of a successful concurrent cycle.\n+const double ShenandoahAdaptiveHeuristics::LOWEST_EXPECTED_AVAILABLE_AT_END = -0.5;\n+const double ShenandoahAdaptiveHeuristics::HIGHEST_EXPECTED_AVAILABLE_AT_END = 0.5;\n+\n+\/\/ These values are the confidence interval expressed as standard deviations.\n+\/\/ At the minimum confidence level, there is a 25% chance that the true value of\n+\/\/ the estimate (average cycle time or allocation rate) is not more than\n+\/\/ MINIMUM_CONFIDENCE standard deviations away from our estimate. Similarly, the\n+\/\/ MAXIMUM_CONFIDENCE interval here means there is a one in a thousand chance\n+\/\/ that the true value of our estimate is outside the interval. These are used\n+\/\/ as bounds on the adjustments applied at the outcome of a GC cycle.\n+const double ShenandoahAdaptiveHeuristics::MINIMUM_CONFIDENCE = 0.319; \/\/ 25%\n+const double ShenandoahAdaptiveHeuristics::MAXIMUM_CONFIDENCE = 3.291; \/\/ 99.9%\n+\n@@ -37,1 +56,4 @@\n-  ShenandoahHeuristics() {}\n+  ShenandoahHeuristics(),\n+  _margin_of_error_sd(ShenandoahAdaptiveInitialConfidence),\n+  _spike_threshold_sd(ShenandoahAdaptiveInitialSpikeThreshold),\n+  _last_trigger(OTHER) { }\n@@ -101,0 +123,59 @@\n+  _allocation_rate.allocation_counter_reset();\n+}\n+\n+void ShenandoahAdaptiveHeuristics::record_success_concurrent() {\n+  ShenandoahHeuristics::record_success_concurrent();\n+\n+  size_t available = ShenandoahHeap::heap()->free_set()->available();\n+\n+  _available.add(available);\n+  double z_score = 0.0;\n+  if (_available.sd() > 0) {\n+    z_score = (available - _available.avg()) \/ _available.sd();\n+  }\n+\n+  log_debug(gc, ergo)(\"Available: \" SIZE_FORMAT \" %sB, z-score=%.3f. Average available: %.1f %sB +\/- %.1f %sB.\",\n+                      byte_size_in_proper_unit(available), proper_unit_for_byte_size(available),\n+                      z_score,\n+                      byte_size_in_proper_unit(_available.avg()), proper_unit_for_byte_size(_available.avg()),\n+                      byte_size_in_proper_unit(_available.sd()), proper_unit_for_byte_size(_available.sd()));\n+\n+  \/\/ In the case when a concurrent GC cycle completes successfully but with an\n+  \/\/ unusually small amount of available memory we will adjust our trigger\n+  \/\/ parameters so that they are more likely to initiate a new cycle.\n+  \/\/ Conversely, when a GC cycle results in an above average amount of available\n+  \/\/ memory, we will adjust the trigger parameters to be less likely to initiate\n+  \/\/ a GC cycle.\n+  \/\/\n+  \/\/ The z-score we've computed is in no way statistically related to the\n+  \/\/ trigger parameters, but it has the nice property that worse z-scores for\n+  \/\/ available memory indicate making larger adjustments to the trigger\n+  \/\/ parameters. It also results in fewer adjustments as the application\n+  \/\/ stabilizes.\n+  \/\/\n+  \/\/ In order to avoid making endless and likely unnecessary adjustments to the\n+  \/\/ trigger parameters, the change in available memory (with respect to the\n+  \/\/ average) at the end of a cycle must be beyond these threshold values.\n+  if (z_score < LOWEST_EXPECTED_AVAILABLE_AT_END ||\n+      z_score > HIGHEST_EXPECTED_AVAILABLE_AT_END) {\n+    \/\/ The sign is flipped because a negative z-score indicates that the\n+    \/\/ available memory at the end of the cycle is below average. Positive\n+    \/\/ adjustments make the triggers more sensitive (i.e., more likely to fire).\n+    \/\/ The z-score also gives us a measure of just how far below normal. This\n+    \/\/ property allows us to adjust the trigger parameters proportionally.\n+    \/\/\n+    \/\/ The `100` here is used to attenuate the size of our adjustments. This\n+    \/\/ number was chosen empirically. It also means the adjustments at the end of\n+    \/\/ a concurrent cycle are an order of magnitude smaller than the adjustments\n+    \/\/ made for a degenerated or full GC cycle (which themselves were also\n+    \/\/ chosen empirically).\n+    adjust_last_trigger_parameters(z_score \/ -100);\n+  }\n+}\n+\n+void ShenandoahAdaptiveHeuristics::record_success_degenerated() {\n+  ShenandoahHeuristics::record_success_degenerated();\n+  \/\/ Adjust both trigger's parameters in the case of a degenerated GC because\n+  \/\/ either of them should have triggered earlier to avoid this case.\n+  adjust_margin_of_error(DEGENERATE_PENALTY_SD);\n+  adjust_spike_threshold(DEGENERATE_PENALTY_SD);\n@@ -103,1 +184,13 @@\n-bool ShenandoahAdaptiveHeuristics::should_start_gc() const {\n+void ShenandoahAdaptiveHeuristics::record_success_full() {\n+  ShenandoahHeuristics::record_success_full();\n+  \/\/ Adjust both trigger's parameters in the case of a full GC because\n+  \/\/ either of them should have triggered earlier to avoid this case.\n+  adjust_margin_of_error(FULL_PENALTY_SD);\n+  adjust_spike_threshold(FULL_PENALTY_SD);\n+}\n+\n+static double saturate(double value, double min, double max) {\n+  return MAX2(MIN2(value, max), min);\n+}\n+\n+bool ShenandoahAdaptiveHeuristics::should_start_gc() {\n@@ -108,0 +201,1 @@\n+  size_t allocated = heap->bytes_allocated_since_gc_start();\n@@ -113,2 +207,4 @@\n-  \/\/ Check if we are falling below the worst limit, time to trigger the GC, regardless of\n-  \/\/ anything else.\n+  \/\/ Track allocation rate even if we decide to start a cycle for other reasons.\n+  double rate = _allocation_rate.sample(allocated);\n+  _last_trigger = OTHER;\n+\n@@ -123,1 +219,0 @@\n-  \/\/ Check if are need to learn a bit about the application\n@@ -139,1 +234,0 @@\n-\n@@ -148,5 +242,8 @@\n-  \/\/ TODO: Allocation rate is way too averaged to be useful during state changes\n-\n-  double average_gc = _gc_time_history->avg();\n-  double time_since_last = time_since_last_gc();\n-  double allocation_rate = heap->bytes_allocated_since_gc_start() \/ time_since_last;\n+  double avg_cycle_time = _gc_time_history->davg() + (_margin_of_error_sd * _gc_time_history->dsd());\n+  double avg_alloc_rate = _allocation_rate.upper_bound(_margin_of_error_sd);\n+  if (avg_cycle_time > allocation_headroom \/ avg_alloc_rate) {\n+    log_info(gc)(\"Trigger: Average GC time (%.2f ms) is above the time for average allocation rate (%.0f %sB\/s) to deplete free headroom (\" SIZE_FORMAT \"%s) (margin of error = %.2f)\",\n+                 avg_cycle_time * 1000,\n+                 byte_size_in_proper_unit(avg_alloc_rate), proper_unit_for_byte_size(avg_alloc_rate),\n+                 byte_size_in_proper_unit(allocation_headroom), proper_unit_for_byte_size(allocation_headroom),\n+                 _margin_of_error_sd);\n@@ -154,5 +251,0 @@\n-  if (average_gc > allocation_headroom \/ allocation_rate) {\n-    log_info(gc)(\"Trigger: Average GC time (%.2f ms) is above the time for allocation rate (%.0f %sB\/s) to deplete free headroom (\" SIZE_FORMAT \"%s)\",\n-                 average_gc * 1000,\n-                 byte_size_in_proper_unit(allocation_rate),     proper_unit_for_byte_size(allocation_rate),\n-                 byte_size_in_proper_unit(allocation_headroom), proper_unit_for_byte_size(allocation_headroom));\n@@ -160,4 +252,6 @@\n-                 byte_size_in_proper_unit(available),           proper_unit_for_byte_size(available),\n-                 byte_size_in_proper_unit(spike_headroom),      proper_unit_for_byte_size(spike_headroom),\n-                 byte_size_in_proper_unit(penalties),           proper_unit_for_byte_size(penalties),\n-                 byte_size_in_proper_unit(allocation_headroom), proper_unit_for_byte_size(allocation_headroom));\n+                       byte_size_in_proper_unit(available),           proper_unit_for_byte_size(available),\n+                       byte_size_in_proper_unit(spike_headroom),      proper_unit_for_byte_size(spike_headroom),\n+                       byte_size_in_proper_unit(penalties),           proper_unit_for_byte_size(penalties),\n+                       byte_size_in_proper_unit(allocation_headroom), proper_unit_for_byte_size(allocation_headroom));\n+\n+    _last_trigger = RATE;\n@@ -167,0 +261,13 @@\n+  if (rate > 0.0) {\n+    bool is_spiking = _allocation_rate.is_spiking(rate, _spike_threshold_sd);\n+    if (is_spiking && avg_cycle_time > allocation_headroom \/ rate) {\n+      log_info(gc)(\"Trigger: Average GC time (%.2f ms) is above the time for instantaneous allocation rate (%.0f %sB\/s) to deplete free headroom (\" SIZE_FORMAT \"%s) (spike threshold = %.2f)\",\n+                   avg_cycle_time * 1000,\n+                   byte_size_in_proper_unit(rate), proper_unit_for_byte_size(rate),\n+                   byte_size_in_proper_unit(allocation_headroom), proper_unit_for_byte_size(allocation_headroom),\n+                   _spike_threshold_sd);\n+      _last_trigger = SPIKE;\n+      return true;\n+    }\n+  }\n+\n@@ -169,0 +276,88 @@\n+\n+void ShenandoahAdaptiveHeuristics::adjust_last_trigger_parameters(double amount) {\n+  switch (_last_trigger) {\n+    case RATE:\n+      adjust_margin_of_error(amount);\n+      break;\n+    case SPIKE:\n+      adjust_spike_threshold(amount);\n+      break;\n+    case OTHER:\n+      \/\/ nothing to adjust here.\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+  }\n+}\n+\n+void ShenandoahAdaptiveHeuristics::adjust_margin_of_error(double amount) {\n+  _margin_of_error_sd = saturate(_margin_of_error_sd + amount, MINIMUM_CONFIDENCE, MAXIMUM_CONFIDENCE);\n+  log_debug(gc, ergo)(\"Margin of error now %.2f\", _margin_of_error_sd);\n+}\n+\n+void ShenandoahAdaptiveHeuristics::adjust_spike_threshold(double amount) {\n+  _spike_threshold_sd = saturate(_spike_threshold_sd - amount, MINIMUM_CONFIDENCE, MAXIMUM_CONFIDENCE);\n+  log_debug(gc, ergo)(\"Spike threshold now: %.2f\", _spike_threshold_sd);\n+}\n+\n+ShenandoahAllocationRate::ShenandoahAllocationRate() :\n+  _last_sample_time(os::elapsedTime()),\n+  _last_sample_value(0),\n+  _interval_sec(1.0 \/ ShenandoahAdaptiveSampleFrequencyHz),\n+  _rate(int(ShenandoahAdaptiveSampleSizeSeconds * ShenandoahAdaptiveSampleFrequencyHz), ShenandoahAdaptiveDecayFactor),\n+  _rate_avg(int(ShenandoahAdaptiveSampleSizeSeconds * ShenandoahAdaptiveSampleFrequencyHz), ShenandoahAdaptiveDecayFactor) {\n+}\n+\n+double ShenandoahAllocationRate::sample(size_t allocated) {\n+  double now = os::elapsedTime();\n+  double rate = 0.0;\n+  if (now - _last_sample_time > _interval_sec) {\n+    if (allocated >= _last_sample_value) {\n+      rate = instantaneous_rate(now, allocated);\n+      _rate.add(rate);\n+      _rate_avg.add(_rate.avg());\n+    }\n+\n+    _last_sample_time = now;\n+    _last_sample_value = allocated;\n+  }\n+  return rate;\n+}\n+\n+double ShenandoahAllocationRate::upper_bound(double sds) const {\n+  \/\/ Here we are using the standard deviation of the computed running\n+  \/\/ average, rather than the standard deviation of the samples that went\n+  \/\/ into the moving average. This is a much more stable value and is tied\n+  \/\/ to the actual statistic in use (moving average over samples of averages).\n+  return _rate.davg() + (sds * _rate_avg.dsd());\n+}\n+\n+void ShenandoahAllocationRate::allocation_counter_reset() {\n+  _last_sample_time = os::elapsedTime();\n+  _last_sample_value = 0;\n+}\n+\n+bool ShenandoahAllocationRate::is_spiking(double rate, double threshold) const {\n+  double sd = _rate.sd();\n+  if (sd > 0) {\n+    \/\/ There is a small chance that that rate has already been sampled, but it\n+    \/\/ seems not to matter in practice.\n+    double z_score = (rate - _rate.avg()) \/ sd;\n+    if (z_score > threshold) {\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n+double ShenandoahAllocationRate::instantaneous_rate(size_t allocated) const {\n+  return instantaneous_rate(os::elapsedTime(), allocated);\n+}\n+\n+double ShenandoahAllocationRate::instantaneous_rate(double time, size_t allocated) const {\n+  size_t last_value = _last_sample_value;\n+  double last_time = _last_sample_time;\n+  size_t allocation_delta = (allocated > last_value) ? (allocated - last_value) : 0;\n+  double time_delta_sec = time - last_time;\n+  return (time_delta_sec > 0)  ? (allocation_delta \/ time_delta_sec) : 0;\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahAdaptiveHeuristics.cpp","additions":217,"deletions":22,"binary":false,"changes":239,"status":"modified"},{"patch":"@@ -32,0 +32,22 @@\n+class ShenandoahAllocationRate : public CHeapObj<mtGC> {\n+ public:\n+  explicit ShenandoahAllocationRate();\n+  void allocation_counter_reset();\n+\n+  double sample(size_t allocated);\n+\n+  double instantaneous_rate(size_t allocated) const;\n+  double upper_bound(double sds) const;\n+  bool is_spiking(double rate, double threshold) const;\n+\n+ private:\n+\n+  double instantaneous_rate(double time, size_t allocated) const;\n+\n+  double _last_sample_time;\n+  size_t _last_sample_value;\n+  double _interval_sec;\n+  TruncatedSeq _rate;\n+  TruncatedSeq _rate_avg;\n+};\n+\n@@ -43,0 +65,3 @@\n+  void record_success_concurrent();\n+  void record_success_degenerated();\n+  void record_success_full();\n@@ -44,1 +69,1 @@\n-  virtual bool should_start_gc() const;\n+  virtual bool should_start_gc();\n@@ -49,0 +74,55 @@\n+\n+ private:\n+  \/\/ These are used to adjust the margin of error and the spike threshold\n+  \/\/ in response to GC cycle outcomes. These values are shared, but the\n+  \/\/ margin of error and spike threshold trend in opposite directions.\n+  const static double FULL_PENALTY_SD;\n+  const static double DEGENERATE_PENALTY_SD;\n+\n+  const static double MINIMUM_CONFIDENCE;\n+  const static double MAXIMUM_CONFIDENCE;\n+\n+  const static double LOWEST_EXPECTED_AVAILABLE_AT_END;\n+  const static double HIGHEST_EXPECTED_AVAILABLE_AT_END;\n+\n+  friend class ShenandoahAllocationRate;\n+\n+  \/\/ Used to record the last trigger that signaled to start a GC.\n+  \/\/ This itself is used to decide whether or not to adjust the margin of\n+  \/\/ error for the average cycle time and allocation rate or the allocation\n+  \/\/ spike detection threshold.\n+  enum Trigger {\n+    SPIKE, RATE, OTHER\n+  };\n+\n+  void adjust_last_trigger_parameters(double amount);\n+  void adjust_margin_of_error(double amount);\n+  void adjust_spike_threshold(double amount);\n+\n+  ShenandoahAllocationRate _allocation_rate;\n+\n+  \/\/ The margin of error expressed in standard deviations to add to our\n+  \/\/ average cycle time and allocation rate. As this value increases we\n+  \/\/ tend to over estimate the rate at which mutators will deplete the\n+  \/\/ heap. In other words, erring on the side of caution will trigger more\n+  \/\/ concurrent GCs.\n+  double _margin_of_error_sd;\n+\n+  \/\/ The allocation spike threshold is expressed in standard deviations.\n+  \/\/ If the standard deviation of the most recent sample of the allocation\n+  \/\/ rate exceeds this threshold, a GC cycle is started. As this value\n+  \/\/ decreases the sensitivity to allocation spikes increases. In other\n+  \/\/ words, lowering the spike threshold will tend to increase the number\n+  \/\/ of concurrent GCs.\n+  double _spike_threshold_sd;\n+\n+  \/\/ Remember which trigger is responsible for the last GC cycle. When the\n+  \/\/ outcome of the cycle is evaluated we will adjust the parameters for the\n+  \/\/ corresponding triggers. Note that successful outcomes will raise\n+  \/\/ the spike threshold and lower the margin of error.\n+  Trigger _last_trigger;\n+\n+  \/\/ Keep track of the available memory at the end of a GC cycle. This\n+  \/\/ establishes what is 'normal' for the application and is used as a\n+  \/\/ source of feedback to adjust trigger parameters.\n+  TruncatedSeq _available;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahAdaptiveHeuristics.hpp","additions":81,"deletions":1,"binary":false,"changes":82,"status":"modified"},{"patch":"@@ -59,1 +59,1 @@\n-bool ShenandoahAggressiveHeuristics::should_start_gc() const {\n+bool ShenandoahAggressiveHeuristics::should_start_gc() {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahAggressiveHeuristics.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -38,1 +38,1 @@\n-  virtual bool should_start_gc() const;\n+  virtual bool should_start_gc();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahAggressiveHeuristics.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -47,1 +47,1 @@\n-bool ShenandoahCompactHeuristics::should_start_gc() const {\n+bool ShenandoahCompactHeuristics::should_start_gc() {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahCompactHeuristics.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -34,1 +34,1 @@\n-  virtual bool should_start_gc() const;\n+  virtual bool should_start_gc();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahCompactHeuristics.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -53,1 +53,1 @@\n-  _gc_time_history(new TruncatedSeq(5)),\n+  _gc_time_history(new TruncatedSeq(10, ShenandoahAdaptiveDecayFactor)),\n@@ -185,1 +185,1 @@\n-bool ShenandoahHeuristics::should_start_gc() const {\n+bool ShenandoahHeuristics::should_start_gc() {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahHeuristics.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -107,1 +107,1 @@\n-  virtual bool should_start_gc() const;\n+  virtual bool should_start_gc();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahHeuristics.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -34,1 +34,1 @@\n-bool ShenandoahPassiveHeuristics::should_start_gc() const {\n+bool ShenandoahPassiveHeuristics::should_start_gc() {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahPassiveHeuristics.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -32,1 +32,1 @@\n-  virtual bool should_start_gc() const;\n+  virtual bool should_start_gc();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahPassiveHeuristics.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -42,1 +42,1 @@\n-bool ShenandoahStaticHeuristics::should_start_gc() const {\n+bool ShenandoahStaticHeuristics::should_start_gc() {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahStaticHeuristics.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -36,1 +36,1 @@\n-  virtual bool should_start_gc() const;\n+  virtual bool should_start_gc();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahStaticHeuristics.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -130,0 +130,28 @@\n+  product(uintx, ShenandoahAdaptiveSampleFrequencyHz, 10, EXPERIMENTAL,     \\\n+          \"The number of times per second to update the allocation rate \"   \\\n+          \"moving average.\")                                                \\\n+                                                                            \\\n+  product(uintx, ShenandoahAdaptiveSampleSizeSeconds, 10, EXPERIMENTAL,     \\\n+          \"The size of the moving window over which the average \"           \\\n+          \"allocation rate is maintained. The total number of samples \"     \\\n+          \"is the product of this number and the sample frequency.\")        \\\n+                                                                            \\\n+  product(double, ShenandoahAdaptiveInitialConfidence, 1.8, EXPERIMENTAL,   \\\n+          \"The number of standard deviations used to determine an initial \" \\\n+          \"margin of error for the average cycle time and average \"         \\\n+          \"allocation rate. Increasing this value will cause the \"          \\\n+          \"heuristic to initiate more concurrent cycles.\" )                 \\\n+                                                                            \\\n+  product(double, ShenandoahAdaptiveInitialSpikeThreshold, 1.8, EXPERIMENTAL, \\\n+          \"If the most recently sampled allocation rate is more than \"      \\\n+          \"this many standard deviations away from the moving average, \"    \\\n+          \"then a cycle is initiated. This value controls how sensitive \"   \\\n+          \"the heuristic is to allocation spikes. Decreasing this number \"  \\\n+          \"increases the sensitivity. \")                                    \\\n+                                                                            \\\n+  product(double, ShenandoahAdaptiveDecayFactor, 0.5, EXPERIMENTAL,         \\\n+          \"The decay factor (alpha) used for values in the weighted \"       \\\n+          \"moving average of cycle time and allocation rate. \"              \\\n+          \"Larger values give more weight to recent values.\")               \\\n+          range(0,1.0)                                                      \\\n+                                                                            \\\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoah_globals.hpp","additions":28,"deletions":0,"binary":false,"changes":28,"status":"modified"}]}