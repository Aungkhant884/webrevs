{"files":[{"patch":"@@ -542,0 +542,78 @@\n+void ParallelScavengeHeap::object_iterate_parallel(ObjectClosure* cl,\n+                                                   uint worker_id,\n+                                                   HeapBlockClaimer* claimer) {\n+  uint block_index;\n+  \/\/ Iterate until all blocks are claimed\n+  while (claimer->claim_and_get_block(&block_index)) {\n+    if (block_index == HeapBlockClaimer::eden_index) {\n+      young_gen()->eden_space()->object_iterate(cl);\n+    } else if (block_index == HeapBlockClaimer::survivor_index) {\n+      young_gen()->from_space()->object_iterate(cl);\n+      young_gen()->to_space()->object_iterate(cl);\n+    } else {\n+      uint index = block_index - HeapBlockClaimer::num_inseparable_spaces;\n+      old_gen()->block_iterate(cl, index);\n+    }\n+  }\n+}\n+\n+HeapBlockClaimer::HeapBlockClaimer(uint n_workers) :\n+    _n_workers(n_workers), _n_blocks(0), _claims(NULL) {\n+  assert(n_workers > 0, \"Need at least one worker.\");\n+  size_t old_gen_used = ParallelScavengeHeap::heap()->old_gen()->used_in_bytes();\n+  size_t block_size = ParallelScavengeHeap::heap()->old_gen()->iterate_block_size();\n+  uint n_blocks_in_old = old_gen_used \/ block_size + 1;\n+  _n_blocks = n_blocks_in_old + num_inseparable_spaces;\n+  _unclaimed_index = 0;\n+  uint* new_claims = NEW_C_HEAP_ARRAY(uint, _n_blocks, mtGC);\n+  memset(new_claims, Unclaimed, sizeof(*_claims) * _n_blocks);\n+  _claims = new_claims;\n+}\n+\n+HeapBlockClaimer::~HeapBlockClaimer() {\n+   FREE_C_HEAP_ARRAY(uint, _claims);\n+}\n+\n+bool HeapBlockClaimer::is_block_claimed(uint block_index) const {\n+  assert(block_index < _n_blocks, \"Invalid index.\");\n+  return _claims[block_index] == Claimed;\n+}\n+\n+bool HeapBlockClaimer::claim_and_get_block(uint* block_index) {\n+  assert(block_index != NULL, \"Invalid index pointer\");\n+  uint next_index = Atomic::load(&_unclaimed_index);\n+  while (true) {\n+    if (next_index >= _n_blocks) {\n+      return false;\n+    }\n+    uint old_val = Atomic::cmpxchg(&_claims[next_index], Unclaimed, Claimed);\n+    if (old_val == Unclaimed) {\n+      *block_index = next_index;\n+      Atomic::inc(&_unclaimed_index);\n+      return true;\n+    }\n+    next_index = Atomic::load(&_unclaimed_index);\n+  }\n+}\n+\n+class PSScavengeParallelObjectIterator : public ParallelObjectIterator {\n+private:\n+  uint _thread_num;\n+  ParallelScavengeHeap*  _heap;\n+  HeapBlockClaimer      _claimer;\n+\n+public:\n+  PSScavengeParallelObjectIterator(uint thread_num) :\n+      _thread_num(thread_num),\n+      _heap(ParallelScavengeHeap::heap()),\n+      _claimer(thread_num == 0 ? ParallelScavengeHeap::heap()->workers().active_workers() : thread_num) {}\n+\n+  virtual void object_iterate(ObjectClosure* cl, uint worker_id) {\n+    _heap->object_iterate_parallel(cl, worker_id, &_claimer);\n+  }\n+};\n+\n+ParallelObjectIterator* ParallelScavengeHeap::parallel_object_iterator(uint thread_num) {\n+  return new PSScavengeParallelObjectIterator(thread_num);\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/parallel\/parallelScavengeHeap.cpp","additions":78,"deletions":0,"binary":false,"changes":78,"status":"modified"},{"patch":"@@ -48,0 +48,1 @@\n+class HeapBlockClaimer;\n@@ -56,0 +57,1 @@\n+  friend class HeapBlockClaimer;\n@@ -212,0 +214,2 @@\n+  void object_iterate_parallel(ObjectClosure* cl, uint worker_id, HeapBlockClaimer* claimer);\n+  virtual ParallelObjectIterator* parallel_object_iterator(uint thread_num);\n@@ -291,0 +295,36 @@\n+\/\/ The HeapBlockClaimer is used during parallel iteration over heap,\n+\/\/ allowing workers to claim heap blocks, gaining exclusive rights to these blocks.\n+\/\/ The eden, survivor spaces are treated as single blocks as it is hard to divide\n+\/\/ these spaces.\n+\/\/ The old spaces are divided into serveral fixed-size blocks.\n+class HeapBlockClaimer : public StackObj {\n+  uint           _n_workers;\n+  uint           _n_blocks;\n+  uint           _unclaimed_index;\n+  volatile uint* _claims;\n+\n+  static const uint Unclaimed = 0;\n+  static const uint Claimed   = 1;\n+\n+  public:\n+  HeapBlockClaimer(uint n_workers);\n+  ~HeapBlockClaimer();\n+\n+  inline uint n_blocks() const {\n+    return _n_blocks;\n+  }\n+\n+  \/\/ Return a start offset given a worker id.\n+  uint offset_for_worker(uint worker_id) const;\n+\n+  \/\/ Check if block has been claimed with this Claimer.\n+  bool is_block_claimed(uint block_index) const;\n+\n+  \/\/ Claim the block and get the block index.\n+  bool claim_and_get_block(uint* block_index);\n+\n+  static const uint eden_index = 0;\n+  static const uint survivor_index = 1;\n+  static const uint num_inseparable_spaces = 2;\n+};\n+\n","filename":"src\/hotspot\/share\/gc\/parallel\/parallelScavengeHeap.hpp","additions":40,"deletions":0,"binary":false,"changes":40,"status":"modified"},{"patch":"@@ -44,1 +44,2 @@\n-  _max_gen_size(max_size)\n+  _max_gen_size(max_size),\n+  _iterate_block_size(1024 * 1024) \/\/ 1M (HeapWord)\n@@ -174,0 +175,43 @@\n+\/*\n+ * Divide space into blocks, processes block begins at\n+ * bottom + block_index  * _iterate_block_size.\n+ * NOTE:\n+ * - The initial block start address may not be a valid\n+ * object address, _start_array is used to correct it.\n+ *\n+ * - The end address is not necessary to be object address.\n+ *\n+ * - If there is an object that crosses blocks, it is\n+ * processed by the worker that owns the block within\n+ * which the object starts.\n+ *\n+ *\/\n+void PSOldGen::block_iterate(ObjectClosure* cl, uint block_index) {\n+  MutableSpace *space = object_space();\n+  HeapWord* bottom = space->bottom();\n+  HeapWord* top = space->top();\n+  HeapWord* begin = bottom + block_index * _iterate_block_size;\n+\n+  assert((_iterate_block_size % (ObjectStartArray::block_size)) == 0,\n+         \"BLOCK SIZE not a multiple of start_array block\");\n+\n+  \/\/ iterate objects in block.\n+  HeapWord* end = MIN2(top, begin + _iterate_block_size);\n+  \/\/ There can be no object between begin and end.\n+  if (start_array()->object_starts_in_range(begin, end)) {\n+    \/\/ There are objects in the range. Find the object of begin address.\n+    \/\/ Note that object_start() can return the last object in previous block,\n+    \/\/ and the object is processed by other worker. Here only focus objects that\n+    \/\/ fall into the current block.\n+    HeapWord* start = start_array()->object_start(begin);\n+    if (start < begin) {\n+      start += oop(start)->size();\n+    }\n+    assert(begin <= start && start < end,\n+           \"object %p must in the range of [%p, %p)\\n\", start, begin, end);\n+    for (HeapWord* p = start; p < end; p += oop(p)->size()) {\n+      cl->do_object(oop(p));\n+    }\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/parallel\/psOldGen.cpp","additions":45,"deletions":1,"binary":false,"changes":46,"status":"modified"},{"patch":"@@ -55,0 +55,3 @@\n+  \/\/ Block size for parallel iteration\n+  const size_t _iterate_block_size;\n+\n@@ -126,0 +129,1 @@\n+  size_t iterate_block_size() const { return _iterate_block_size; }\n@@ -165,0 +169,3 @@\n+  \/\/ Iterate block with given block_index\n+  void block_iterate(ObjectClosure* cl, uint block_index);\n+\n","filename":"src\/hotspot\/share\/gc\/parallel\/psOldGen.hpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"}]}