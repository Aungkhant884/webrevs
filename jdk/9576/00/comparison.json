{"files":[{"patch":"@@ -160,1 +160,1 @@\n-    eden_allocate(obj, var_size_in_bytes, con_size_in_bytes, t1, slow_case);\n+    b(slow_case);\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_MacroAssembler_aarch64.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -659,50 +659,0 @@\n-        \/\/ If TLAB is disabled, see if there is support for inlining contiguous\n-        \/\/ allocations.\n-        \/\/ Otherwise, just go to the slow path.\n-        if ((id == fast_new_instance_id || id == fast_new_instance_init_check_id) &&\n-            !UseTLAB && Universe::heap()->supports_inline_contig_alloc()) {\n-          Label slow_path;\n-          Register obj_size = r19;\n-          Register t1       = r10;\n-          Register t2       = r11;\n-          assert_different_registers(klass, obj, obj_size, t1, t2);\n-\n-          __ stp(r19, zr, Address(__ pre(sp, -2 * wordSize)));\n-\n-          if (id == fast_new_instance_init_check_id) {\n-            \/\/ make sure the klass is initialized\n-            __ ldrb(rscratch1, Address(klass, InstanceKlass::init_state_offset()));\n-            __ cmpw(rscratch1, InstanceKlass::fully_initialized);\n-            __ br(Assembler::NE, slow_path);\n-          }\n-\n-#ifdef ASSERT\n-          \/\/ assert object can be fast path allocated\n-          {\n-            Label ok, not_ok;\n-            __ ldrw(obj_size, Address(klass, Klass::layout_helper_offset()));\n-            __ cmp(obj_size, (u1)0);\n-            __ br(Assembler::LE, not_ok);  \/\/ make sure it's an instance (LH > 0)\n-            __ tstw(obj_size, Klass::_lh_instance_slow_path_bit);\n-            __ br(Assembler::EQ, ok);\n-            __ bind(not_ok);\n-            __ stop(\"assert(can be fast path allocated)\");\n-            __ should_not_reach_here();\n-            __ bind(ok);\n-          }\n-#endif \/\/ ASSERT\n-\n-          \/\/ get the instance size (size is positive so movl is fine for 64bit)\n-          __ ldrw(obj_size, Address(klass, Klass::layout_helper_offset()));\n-\n-          __ eden_allocate(obj, obj_size, 0, t1, slow_path);\n-\n-          __ initialize_object(obj, klass, obj_size, 0, t1, t2, \/* is_tlab_allocated *\/ false);\n-          __ verify_oop(obj);\n-          __ ldp(r19, zr, Address(__ post(sp, 2 * wordSize)));\n-          __ ret(lr);\n-\n-          __ bind(slow_path);\n-          __ ldp(r19, zr, Address(__ post(sp, 2 * wordSize)));\n-        }\n-\n@@ -774,45 +724,0 @@\n-        \/\/ If TLAB is disabled, see if there is support for inlining contiguous\n-        \/\/ allocations.\n-        \/\/ Otherwise, just go to the slow path.\n-        if (!UseTLAB && Universe::heap()->supports_inline_contig_alloc()) {\n-          Register arr_size = r5;\n-          Register t1       = r10;\n-          Register t2       = r11;\n-          Label slow_path;\n-          assert_different_registers(length, klass, obj, arr_size, t1, t2);\n-\n-          \/\/ check that array length is small enough for fast path.\n-          __ mov(rscratch1, C1_MacroAssembler::max_array_allocation_length);\n-          __ cmpw(length, rscratch1);\n-          __ br(Assembler::HI, slow_path);\n-\n-          \/\/ get the allocation size: round_up(hdr + length << (layout_helper & 0x1F))\n-          \/\/ since size is positive ldrw does right thing on 64bit\n-          __ ldrw(t1, Address(klass, Klass::layout_helper_offset()));\n-          \/\/ since size is positive movw does right thing on 64bit\n-          __ movw(arr_size, length);\n-          __ lslvw(arr_size, length, t1);\n-          __ ubfx(t1, t1, Klass::_lh_header_size_shift,\n-                  exact_log2(Klass::_lh_header_size_mask + 1));\n-          __ add(arr_size, arr_size, t1);\n-          __ add(arr_size, arr_size, MinObjAlignmentInBytesMask); \/\/ align up\n-          __ andr(arr_size, arr_size, ~MinObjAlignmentInBytesMask);\n-\n-          __ eden_allocate(obj, arr_size, 0, t1, slow_path);  \/\/ preserves arr_size\n-\n-          __ initialize_header(obj, klass, length, t1, t2);\n-          __ ldrb(t1, Address(klass, in_bytes(Klass::layout_helper_offset()) + (Klass::_lh_header_size_shift \/ BitsPerByte)));\n-          assert(Klass::_lh_header_size_shift % BitsPerByte == 0, \"bytewise\");\n-          assert(Klass::_lh_header_size_mask <= 0xFF, \"bytewise\");\n-          __ andr(t1, t1, Klass::_lh_header_size_mask);\n-          __ sub(arr_size, arr_size, t1);  \/\/ body length\n-          __ add(t1, t1, obj);       \/\/ body start\n-          __ initialize_body(t1, arr_size, 0, t1, t2);\n-          __ membar(Assembler::StoreStore);\n-          __ verify_oop(obj);\n-\n-          __ ret(lr);\n-\n-          __ bind(slow_path);\n-        }\n-\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_Runtime1_aarch64.cpp","additions":0,"deletions":95,"binary":false,"changes":95,"status":"modified"},{"patch":"@@ -163,57 +163,0 @@\n-\/\/ Defines obj, preserves var_size_in_bytes\n-void BarrierSetAssembler::eden_allocate(MacroAssembler* masm, Register obj,\n-                                        Register var_size_in_bytes,\n-                                        int con_size_in_bytes,\n-                                        Register t1,\n-                                        Label& slow_case) {\n-  assert_different_registers(obj, var_size_in_bytes, t1);\n-  if (!Universe::heap()->supports_inline_contig_alloc()) {\n-    __ b(slow_case);\n-  } else {\n-    Register end = t1;\n-    Register heap_end = rscratch2;\n-    Label retry;\n-    __ bind(retry);\n-    {\n-      uint64_t offset;\n-      __ adrp(rscratch1, ExternalAddress((address) Universe::heap()->end_addr()), offset);\n-      __ ldr(heap_end, Address(rscratch1, offset));\n-    }\n-\n-    ExternalAddress heap_top((address) Universe::heap()->top_addr());\n-\n-    \/\/ Get the current top of the heap\n-    {\n-      uint64_t offset;\n-      __ adrp(rscratch1, heap_top, offset);\n-      \/\/ Use add() here after ARDP, rather than lea().\n-      \/\/ lea() does not generate anything if its offset is zero.\n-      \/\/ However, relocs expect to find either an ADD or a load\/store\n-      \/\/ insn after an ADRP.  add() always generates an ADD insn, even\n-      \/\/ for add(Rn, Rn, 0).\n-      __ add(rscratch1, rscratch1, offset);\n-      __ ldaxr(obj, rscratch1);\n-    }\n-\n-    \/\/ Adjust it my the size of our new object\n-    if (var_size_in_bytes == noreg) {\n-      __ lea(end, Address(obj, con_size_in_bytes));\n-    } else {\n-      __ lea(end, Address(obj, var_size_in_bytes));\n-    }\n-\n-    \/\/ if end < obj then we wrapped around high memory\n-    __ cmp(end, obj);\n-    __ br(Assembler::LO, slow_case);\n-\n-    __ cmp(end, heap_end);\n-    __ br(Assembler::HI, slow_case);\n-\n-    \/\/ If heap_top hasn't been changed by some other thread, update it.\n-    __ stlxr(rscratch2, end, rscratch1);\n-    __ cbnzw(rscratch2, retry);\n-\n-    incr_allocated_bytes(masm, var_size_in_bytes, con_size_in_bytes, t1);\n-  }\n-}\n-\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/shared\/barrierSetAssembler_aarch64.cpp","additions":0,"deletions":57,"binary":false,"changes":57,"status":"modified"},{"patch":"@@ -62,7 +62,0 @@\n-  void eden_allocate(MacroAssembler* masm,\n-    Register obj,                      \/\/ result: pointer to object after successful allocation\n-    Register var_size_in_bytes,        \/\/ object size in bytes if unknown at compile time; invalid otherwise\n-    int      con_size_in_bytes,        \/\/ object size in bytes if   known at compile time\n-    Register t1,                       \/\/ temp register\n-    Label&   slow_case                 \/\/ continuation point if fast allocation fails\n-  );\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/shared\/barrierSetAssembler_aarch64.hpp","additions":0,"deletions":7,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -4267,10 +4267,0 @@\n-\/\/ Defines obj, preserves var_size_in_bytes\n-void MacroAssembler::eden_allocate(Register obj,\n-                                   Register var_size_in_bytes,\n-                                   int con_size_in_bytes,\n-                                   Register t1,\n-                                   Label& slow_case) {\n-  BarrierSetAssembler *bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-  bs->eden_allocate(this, obj, var_size_in_bytes, con_size_in_bytes, t1, slow_case);\n-}\n-\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":0,"deletions":10,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -912,7 +912,0 @@\n-  void eden_allocate(\n-    Register obj,                      \/\/ result: pointer to object after successful allocation\n-    Register var_size_in_bytes,        \/\/ object size in bytes if unknown at compile time; invalid otherwise\n-    int      con_size_in_bytes,        \/\/ object size in bytes if   known at compile time\n-    Register t1,                       \/\/ temp register\n-    Label&   slow_case                 \/\/ continuation point if fast allocation fails\n-  );\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.hpp","additions":0,"deletions":7,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -3471,1 +3471,0 @@\n-  Label initialize_object; \/\/ including clearing the fields\n@@ -3504,5 +3503,0 @@\n-  \/\/  Else If inline contiguous allocations are enabled:\n-  \/\/    Try to allocate in eden.\n-  \/\/    If fails due to heap end, go to slow path.\n-  \/\/\n-  \/\/  If TLAB is enabled OR inline contiguous is enabled:\n@@ -3513,2 +3507,0 @@\n-  const bool allow_shared_alloc =\n-    Universe::heap()->supports_inline_contig_alloc();\n@@ -3522,3 +3514,0 @@\n-    } else {\n-      \/\/ initialize both the header and fields\n-      __ b(initialize_object);\n@@ -3526,8 +3515,0 @@\n-  } else {\n-    \/\/ Allocation in the shared Eden, if allowed.\n-    \/\/\n-    \/\/ r3: instance size in bytes\n-    if (allow_shared_alloc) {\n-      __ eden_allocate(r0, r3, 0, r10, slow_case);\n-    }\n-  }\n@@ -3535,3 +3516,0 @@\n-  \/\/ If UseTLAB or allow_shared_alloc are true, the object is created above and\n-  \/\/ there is an initialize need. Otherwise, skip and go to the slow path.\n-  if (UseTLAB || allow_shared_alloc) {\n@@ -3540,1 +3518,0 @@\n-    __ bind(initialize_object);\n","filename":"src\/hotspot\/cpu\/aarch64\/templateTable_aarch64.cpp","additions":0,"deletions":23,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -84,1 +84,1 @@\n-    eden_allocate(obj, obj_end, tmp1, tmp2, size_expression, slow_case);\n+    b(slow_case);\n","filename":"src\/hotspot\/cpu\/arm\/c1_MacroAssembler_arm.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -422,34 +422,0 @@\n-        \/\/ If TLAB is disabled, see if there is support for inlining contiguous\n-        \/\/ allocations.\n-        \/\/ Otherwise, just go to the slow path.\n-        if (!UseTLAB && Universe::heap()->supports_inline_contig_alloc() && id != new_instance_id) {\n-          Label slow_case, slow_case_no_pop;\n-\n-          \/\/ Make sure the class is fully initialized\n-          if (id == fast_new_instance_init_check_id) {\n-            __ ldrb(result, Address(klass, InstanceKlass::init_state_offset()));\n-            __ cmp(result, InstanceKlass::fully_initialized);\n-            __ b(slow_case_no_pop, ne);\n-          }\n-\n-          \/\/ Free some temporary registers\n-          const Register obj_size = R4;\n-          const Register tmp1     = R5;\n-          const Register tmp2     = LR;\n-          const Register obj_end  = Rtemp;\n-\n-          __ raw_push(R4, R5, LR);\n-\n-          __ ldr_u32(obj_size, Address(klass, Klass::layout_helper_offset()));\n-          __ eden_allocate(result, obj_end, tmp1, tmp2, obj_size, slow_case);        \/\/ initializes result and obj_end\n-          __ initialize_object(result, obj_end, klass, noreg \/* len *\/, tmp1, tmp2,\n-                               instanceOopDesc::header_size() * HeapWordSize, -1,\n-                               \/* is_tlab_allocated *\/ false);\n-          __ raw_pop_and_ret(R4, R5);\n-\n-          __ bind(slow_case);\n-          __ raw_pop(R4, R5, LR);\n-\n-          __ bind(slow_case_no_pop);\n-        }\n-\n@@ -492,41 +458,0 @@\n-        \/\/ If TLAB is disabled, see if there is support for inlining contiguous\n-        \/\/ allocations.\n-        \/\/ Otherwise, just go to the slow path.\n-        if (!UseTLAB && Universe::heap()->supports_inline_contig_alloc()) {\n-          Label slow_case, slow_case_no_pop;\n-\n-          __ cmp_32(length, C1_MacroAssembler::max_array_allocation_length);\n-          __ b(slow_case_no_pop, hs);\n-\n-          \/\/ Free some temporary registers\n-          const Register arr_size = R4;\n-          const Register tmp1     = R5;\n-          const Register tmp2     = LR;\n-          const Register tmp3     = Rtemp;\n-          const Register obj_end  = tmp3;\n-\n-          __ raw_push(R4, R5, LR);\n-\n-          \/\/ Get the allocation size: round_up((length << (layout_helper & 0xff)) + header_size)\n-          __ ldr_u32(tmp1, Address(klass, Klass::layout_helper_offset()));\n-          __ mov(arr_size, MinObjAlignmentInBytesMask);\n-          __ and_32(tmp2, tmp1, (unsigned int)(Klass::_lh_header_size_mask << Klass::_lh_header_size_shift));\n-\n-          __ add(arr_size, arr_size, AsmOperand(length, lsl, tmp1));\n-\n-          __ add(arr_size, arr_size, AsmOperand(tmp2, lsr, Klass::_lh_header_size_shift));\n-          __ align_reg(arr_size, arr_size, MinObjAlignmentInBytes);\n-\n-          \/\/ eden_allocate destroys tmp2, so reload header_size after allocation\n-          \/\/ eden_allocate initializes result and obj_end\n-          __ eden_allocate(result, obj_end, tmp1, tmp2, arr_size, slow_case);\n-          __ ldrb(tmp2, Address(klass, in_bytes(Klass::layout_helper_offset()) +\n-                                       Klass::_lh_header_size_shift \/ BitsPerByte));\n-          __ initialize_object(result, obj_end, klass, length, tmp1, tmp2, tmp2, -1, \/* is_tlab_allocated *\/ false);\n-          __ raw_pop_and_ret(R4, R5);\n-\n-          __ bind(slow_case);\n-          __ raw_pop(R4, R5, LR);\n-          __ bind(slow_case_no_pop);\n-        }\n-\n","filename":"src\/hotspot\/cpu\/arm\/c1_Runtime1_arm.cpp","additions":0,"deletions":75,"binary":false,"changes":75,"status":"modified"},{"patch":"@@ -146,44 +146,0 @@\n-void BarrierSetAssembler::eden_allocate(MacroAssembler* masm, Register obj, Register obj_end, Register tmp1, Register tmp2,\n-                                 RegisterOrConstant size_expression, Label& slow_case) {\n-  if (!Universe::heap()->supports_inline_contig_alloc()) {\n-    __ b(slow_case);\n-    return;\n-  }\n-\n-  CollectedHeap* ch = Universe::heap();\n-\n-  const Register top_addr = tmp1;\n-  const Register heap_end = tmp2;\n-\n-  if (size_expression.is_register()) {\n-    assert_different_registers(obj, obj_end, top_addr, heap_end, size_expression.as_register());\n-  } else {\n-    assert_different_registers(obj, obj_end, top_addr, heap_end);\n-  }\n-\n-  bool load_const = VM_Version::supports_movw();\n-  if (load_const) {\n-    __ mov_address(top_addr, (address)Universe::heap()->top_addr());\n-  } else {\n-    __ ldr(top_addr, Address(Rthread, JavaThread::heap_top_addr_offset()));\n-  }\n-  \/\/ Calculate new heap_top by adding the size of the object\n-  Label retry;\n-  __ bind(retry);\n-  __ ldr(obj, Address(top_addr));\n-  __ ldr(heap_end, Address(top_addr, (intptr_t)ch->end_addr() - (intptr_t)ch->top_addr()));\n-  __ add_rc(obj_end, obj, size_expression);\n-  \/\/ Check if obj_end wrapped around, i.e., obj_end < obj. If yes, jump to the slow case.\n-  __ cmp(obj_end, obj);\n-  __ b(slow_case, lo);\n-  \/\/ Update heap_top if allocation succeeded\n-  __ cmp(obj_end, heap_end);\n-  __ b(slow_case, hi);\n-\n-  __ atomic_cas_bool(obj, obj_end, top_addr, 0, heap_end\/*scratched*\/);\n-  __ b(retry, ne);\n-\n-  incr_allocated_bytes(masm, size_expression, tmp1);\n-}\n-\n-\/\/ Puts address of allocated object into register `obj` and end of allocated object into register `obj_end`.\n","filename":"src\/hotspot\/cpu\/arm\/gc\/shared\/barrierSetAssembler_arm.cpp","additions":0,"deletions":44,"binary":false,"changes":44,"status":"modified"},{"patch":"@@ -50,9 +50,0 @@\n-  virtual void eden_allocate(MacroAssembler* masm,\n-    Register           obj,              \/\/ result: pointer to object after successful allocation\n-    Register           obj_end,          \/\/ result: pointer to end of object after successful allocation\n-    Register           tmp1,             \/\/ temp register\n-    Register           tmp2,             \/\/ temp register\n-    RegisterOrConstant size_expression,  \/\/ size of object\n-    Label&             slow_case         \/\/ continuation point if fast allocation fails\n-  );\n-\n","filename":"src\/hotspot\/cpu\/arm\/gc\/shared\/barrierSetAssembler_arm.hpp","additions":0,"deletions":9,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -952,7 +952,0 @@\n-\/\/ Puts address of allocated object into register `obj` and end of allocated object into register `obj_end`.\n-void MacroAssembler::eden_allocate(Register obj, Register obj_end, Register tmp1, Register tmp2,\n-                                 RegisterOrConstant size_expression, Label& slow_case) {\n-  BarrierSetAssembler *bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-  bs->eden_allocate(this, obj, obj_end, tmp1, tmp2, size_expression, slow_case);\n-}\n-\n","filename":"src\/hotspot\/cpu\/arm\/macroAssembler_arm.cpp","additions":0,"deletions":7,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -343,2 +343,0 @@\n-  void eden_allocate(Register obj, Register obj_end, Register tmp1, Register tmp2,\n-                     RegisterOrConstant size_expression, Label& slow_case);\n","filename":"src\/hotspot\/cpu\/arm\/macroAssembler_arm.hpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -3851,7 +3851,0 @@\n-  Label initialize_object;  \/\/ including clearing the fields\n-\n-  const bool allow_shared_alloc =\n-    Universe::heap()->supports_inline_contig_alloc();\n-\n-  \/\/ Literals\n-  InlinedAddress Lheap_top_addr(allow_shared_alloc ? (address)Universe::heap()->top_addr() : NULL);\n@@ -3895,5 +3888,0 @@\n-  \/\/  Else If inline contiguous allocations are enabled:\n-  \/\/    Try to allocate in eden.\n-  \/\/    If fails due to heap end, go to slow path.\n-  \/\/\n-  \/\/  If TLAB is enabled OR inline contiguous is enabled:\n@@ -3913,3 +3901,0 @@\n-    } else {\n-      \/\/ initialize both the header and fields\n-      __ b(initialize_object);\n@@ -3917,11 +3902,0 @@\n-  } else {\n-    \/\/ Allocation in the shared Eden, if allowed.\n-    if (allow_shared_alloc) {\n-      const Register Rheap_top_addr = R2_tmp;\n-      const Register Rheap_top = R5_tmp;\n-      const Register Rheap_end = Rtemp;\n-      assert_different_registers(Robj, Rklass, Rsize, Rheap_top_addr, Rheap_top, Rheap_end, LR);\n-\n-      __ eden_allocate(Robj, Rheap_top, Rheap_top_addr, Rheap_end, Rsize, slow_case);\n-    }\n-  }\n@@ -3929,1 +3903,0 @@\n-  if (UseTLAB || allow_shared_alloc) {\n@@ -3938,1 +3911,0 @@\n-    __ bind(initialize_object);\n@@ -4000,4 +3972,0 @@\n-  if (allow_shared_alloc) {\n-    __ bind_literal(Lheap_top_addr);\n-  }\n-\n","filename":"src\/hotspot\/cpu\/arm\/templateTable_arm.cpp","additions":0,"deletions":32,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -203,5 +203,1 @@\n-    eden_allocate(obj, var_size_in_bytes, con_size_in_bytes, t1, t2, slow_case);\n-    RegisterOrConstant size_in_bytes = var_size_in_bytes->is_valid()\n-                                       ? RegisterOrConstant(var_size_in_bytes)\n-                                       : RegisterOrConstant(con_size_in_bytes);\n-    incr_allocated_bytes(size_in_bytes, t1, t2);\n+    b(slow_case);\n@@ -360,5 +356,1 @@\n-  if (UseTLAB) {\n-    tlab_allocate(obj, arr_size, 0, t2, slow_case);\n-  } else {\n-    eden_allocate(obj, arr_size, 0, t2, t3, slow_case);\n-  }\n+  try_allocate(obj, arr_size, 0, t2, t3, slow_case);\n","filename":"src\/hotspot\/cpu\/ppc\/c1_MacroAssembler_ppc.cpp","additions":2,"deletions":10,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -2079,12 +2079,0 @@\n-\/\/ allocation (for C1)\n-void MacroAssembler::eden_allocate(\n-  Register obj,                      \/\/ result: pointer to object after successful allocation\n-  Register var_size_in_bytes,        \/\/ object size in bytes if unknown at compile time; invalid otherwise\n-  int      con_size_in_bytes,        \/\/ object size in bytes if   known at compile time\n-  Register t1,                       \/\/ temp register\n-  Register t2,                       \/\/ temp register\n-  Label&   slow_case                 \/\/ continuation point if fast allocation fails\n-) {\n-  b(slow_case);\n-}\n-\n","filename":"src\/hotspot\/cpu\/ppc\/macroAssembler_ppc.cpp","additions":0,"deletions":12,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -603,8 +603,0 @@\n-  void eden_allocate(\n-    Register obj,                      \/\/ result: pointer to object after successful allocation\n-    Register var_size_in_bytes,        \/\/ object size in bytes if unknown at compile time; invalid otherwise\n-    int      con_size_in_bytes,        \/\/ object size in bytes if   known at compile time\n-    Register t1,                       \/\/ temp register\n-    Register t2,                       \/\/ temp register\n-    Label&   slow_case                 \/\/ continuation point if fast allocation fails\n-  );\n","filename":"src\/hotspot\/cpu\/ppc\/macroAssembler_ppc.hpp","additions":0,"deletions":8,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -144,1 +144,1 @@\n-    eden_allocate(obj, var_size_in_bytes, con_size_in_bytes, tmp1, slow_case, \/* is_far *\/ true);\n+    j(slow_case);\n","filename":"src\/hotspot\/cpu\/riscv\/c1_MacroAssembler_riscv.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -671,60 +671,0 @@\n-        \/\/ If TLAB is disabled, see if there is support for inlining contiguous\n-        \/\/ allocations.\n-        \/\/ Otherwise, just go to the slow path.\n-        if ((id == fast_new_instance_id || id == fast_new_instance_init_check_id) &&\n-            !UseTLAB && Universe::heap()->supports_inline_contig_alloc()) {\n-          Label slow_path;\n-          Register obj_size   = x12;\n-          Register tmp1       = x9;\n-          Register tmp2       = x14;\n-          assert_different_registers(klass, obj, obj_size, tmp1, tmp2);\n-\n-          const int sp_offset = 2;\n-          const int x9_offset = 1;\n-          const int zr_offset = 0;\n-          __ addi(sp, sp, -(sp_offset * wordSize));\n-          __ sd(x9, Address(sp, x9_offset * wordSize));\n-          __ sd(zr, Address(sp, zr_offset * wordSize));\n-\n-          if (id == fast_new_instance_init_check_id) {\n-            \/\/ make sure the klass is initialized\n-            __ lbu(t0, Address(klass, InstanceKlass::init_state_offset()));\n-            __ mv(t1, InstanceKlass::fully_initialized);\n-            __ bne(t0, t1, slow_path);\n-          }\n-\n-#ifdef ASSERT\n-          \/\/ assert object can be fast path allocated\n-          {\n-            Label ok, not_ok;\n-            __ lw(obj_size, Address(klass, Klass::layout_helper_offset()));\n-            \/\/ make sure it's an instance. For instances, layout helper is a positive number.\n-            \/\/ For arrays, layout helper is a negative number\n-            __ blez(obj_size, not_ok);\n-            __ andi(t0, obj_size, Klass::_lh_instance_slow_path_bit);\n-            __ beqz(t0, ok);\n-            __ bind(not_ok);\n-            __ stop(\"assert(can be fast path allocated)\");\n-            __ should_not_reach_here();\n-            __ bind(ok);\n-          }\n-#endif \/\/ ASSERT\n-\n-          \/\/ get the instance size\n-          __ lwu(obj_size, Address(klass, Klass::layout_helper_offset()));\n-\n-          __ eden_allocate(obj, obj_size, 0, tmp1, slow_path);\n-\n-          __ initialize_object(obj, klass, obj_size, 0, tmp1, tmp2, \/* is_tlab_allocated *\/ false);\n-          __ verify_oop(obj);\n-          __ ld(x9, Address(sp, x9_offset * wordSize));\n-          __ ld(zr, Address(sp, zr_offset * wordSize));\n-          __ addi(sp, sp, sp_offset * wordSize);\n-          __ ret();\n-\n-          __ bind(slow_path);\n-          __ ld(x9, Address(sp, x9_offset * wordSize));\n-          __ ld(zr, Address(sp, zr_offset * wordSize));\n-          __ addi(sp, sp, sp_offset * wordSize);\n-        }\n-\n@@ -801,46 +741,0 @@\n-        \/\/ If TLAB is disabled, see if there is support for inlining contiguous\n-        \/\/ allocations.\n-        \/\/ Otherwise, just go to the slow path.\n-        if (!UseTLAB && Universe::heap()->supports_inline_contig_alloc()) {\n-          Register arr_size   = x14;\n-          Register tmp1       = x12;\n-          Register tmp2       = x15;\n-          Label slow_path;\n-          assert_different_registers(length, klass, obj, arr_size, tmp1, tmp2);\n-\n-          \/\/ check that array length is small enough for fast path.\n-          __ mv(t0, C1_MacroAssembler::max_array_allocation_length);\n-          __ bgtu(length, t0, slow_path);\n-\n-          \/\/ get the allocation size: round_up(hdr + length << (layout_helper & 0x1F))\n-          __ lwu(tmp1, Address(klass, Klass::layout_helper_offset()));\n-          __ andi(t0, tmp1, 0x1f);\n-          __ sll(arr_size, length, t0);\n-          int lh_header_size_width = exact_log2(Klass::_lh_header_size_mask + 1);\n-          int lh_header_size_msb = Klass::_lh_header_size_shift + lh_header_size_width;\n-          __ slli(tmp1, tmp1, XLEN - lh_header_size_msb);\n-          __ srli(tmp1, tmp1, XLEN - lh_header_size_width);\n-          __ add(arr_size, arr_size, tmp1);\n-          __ addi(arr_size, arr_size, MinObjAlignmentInBytesMask); \/\/ align up\n-          __ andi(arr_size, arr_size, ~(uint)MinObjAlignmentInBytesMask);\n-\n-          __ eden_allocate(obj, arr_size, 0, tmp1, slow_path); \/\/ preserves arr_size\n-\n-          __ initialize_header(obj, klass, length, tmp1, tmp2);\n-          __ lbu(tmp1, Address(klass,\n-                               in_bytes(Klass::layout_helper_offset()) +\n-                               (Klass::_lh_header_size_shift \/ BitsPerByte)));\n-          assert(Klass::_lh_header_size_shift % BitsPerByte == 0, \"bytewise\");\n-          assert(Klass::_lh_header_size_mask <= 0xFF, \"bytewise\");\n-          __ andi(tmp1, tmp1, Klass::_lh_header_size_mask);\n-          __ sub(arr_size, arr_size, tmp1); \/\/ body length\n-          __ add(tmp1, tmp1, obj);       \/\/ body start\n-          __ initialize_body(tmp1, arr_size, 0, tmp2);\n-          __ membar(MacroAssembler::StoreStore);\n-          __ verify_oop(obj);\n-\n-          __ ret();\n-\n-          __ bind(slow_path);\n-        }\n-\n","filename":"src\/hotspot\/cpu\/riscv\/c1_Runtime1_riscv.cpp","additions":0,"deletions":106,"binary":false,"changes":106,"status":"modified"},{"patch":"@@ -165,53 +165,0 @@\n-\/\/ Defines obj, preserves var_size_in_bytes\n-void BarrierSetAssembler::eden_allocate(MacroAssembler* masm, Register obj,\n-                                        Register var_size_in_bytes,\n-                                        int con_size_in_bytes,\n-                                        Register tmp1,\n-                                        Label& slow_case,\n-                                        bool is_far) {\n-  assert_cond(masm != NULL);\n-  assert_different_registers(obj, var_size_in_bytes, tmp1);\n-  if (!Universe::heap()->supports_inline_contig_alloc()) {\n-    __ j(slow_case);\n-  } else {\n-    Register end = tmp1;\n-    Label retry;\n-    __ bind(retry);\n-\n-    \/\/ Get the current end of the heap\n-    ExternalAddress address_end((address) Universe::heap()->end_addr());\n-    {\n-      int32_t offset;\n-      __ la_patchable(t1, address_end, offset);\n-      __ ld(t1, Address(t1, offset));\n-    }\n-\n-    \/\/ Get the current top of the heap\n-    ExternalAddress address_top((address) Universe::heap()->top_addr());\n-    {\n-      int32_t offset;\n-      __ la_patchable(t0, address_top, offset);\n-      __ addi(t0, t0, offset);\n-      __ lr_d(obj, t0, Assembler::aqrl);\n-    }\n-\n-    \/\/ Adjust it my the size of our new object\n-    if (var_size_in_bytes == noreg) {\n-      __ la(end, Address(obj, con_size_in_bytes));\n-    } else {\n-      __ add(end, obj, var_size_in_bytes);\n-    }\n-\n-    \/\/ if end < obj then we wrapped around high memory\n-    __ bltu(end, obj, slow_case, is_far);\n-\n-    __ bgtu(end, t1, slow_case, is_far);\n-\n-    \/\/ If heap_top hasn't been changed by some other thread, update it.\n-    __ sc_d(t1, end, t0, Assembler::rl);\n-    __ bnez(t1, retry);\n-\n-    incr_allocated_bytes(masm, var_size_in_bytes, con_size_in_bytes, tmp1);\n-  }\n-}\n-\n","filename":"src\/hotspot\/cpu\/riscv\/gc\/shared\/barrierSetAssembler_riscv.cpp","additions":0,"deletions":53,"binary":false,"changes":53,"status":"modified"},{"patch":"@@ -64,8 +64,0 @@\n-  void eden_allocate(MacroAssembler* masm,\n-    Register obj,                      \/\/ result: pointer to object after successful allocation\n-    Register var_size_in_bytes,        \/\/ object size in bytes if unknown at compile time; invalid otherwise\n-    int      con_size_in_bytes,        \/\/ object size in bytes if   known at compile time\n-    Register tmp1,                     \/\/ temp register\n-    Label&   slow_case,                \/\/ continuation point if fast allocation fails\n-    bool is_far = false\n-  );\n","filename":"src\/hotspot\/cpu\/riscv\/gc\/shared\/barrierSetAssembler_riscv.hpp","additions":0,"deletions":8,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2690,12 +2690,0 @@\n-\/\/ Defines obj, preserves var_size_in_bytes\n-void MacroAssembler::eden_allocate(Register obj,\n-                                   Register var_size_in_bytes,\n-                                   int con_size_in_bytes,\n-                                   Register tmp,\n-                                   Label& slow_case,\n-                                   bool is_far) {\n-  BarrierSetAssembler *bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-  bs->eden_allocate(this, obj, var_size_in_bytes, con_size_in_bytes, tmp, slow_case, is_far);\n-}\n-\n-\n","filename":"src\/hotspot\/cpu\/riscv\/macroAssembler_riscv.cpp","additions":0,"deletions":12,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -275,9 +275,0 @@\n-  void eden_allocate(\n-    Register obj,                   \/\/ result: pointer to object after successful allocation\n-    Register var_size_in_bytes,     \/\/ object size in bytes if unknown at compile time; invalid otherwise\n-    int      con_size_in_bytes,     \/\/ object size in bytes if   known at compile time\n-    Register tmp,                   \/\/ temp register\n-    Label&   slow_case,             \/\/ continuation point if fast allocation fails\n-    bool is_far = false\n-  );\n-\n","filename":"src\/hotspot\/cpu\/riscv\/macroAssembler_riscv.hpp","additions":0,"deletions":9,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -3477,1 +3477,0 @@\n-  Label initialize_object; \/\/ including clearing the fields\n@@ -3511,5 +3510,0 @@\n-  \/\/  Else If inline contiguous allocations are enabled:\n-  \/\/    Try to allocate in eden.\n-  \/\/    If fails due to heap end, go to slow path\n-  \/\/\n-  \/\/  If TLAB is enabled OR inline contiguous is enabled:\n@@ -3519,1 +3513,0 @@\n-  const bool allow_shared_alloc = Universe::heap()->supports_inline_contig_alloc();\n@@ -3527,3 +3520,0 @@\n-    } else {\n-      \/\/ initialize both the header and fields\n-      __ j(initialize_object);\n@@ -3531,8 +3521,0 @@\n-  } else {\n-    \/\/ Allocation in the shared Eden, if allowed.\n-    \/\/\n-    \/\/ x13: instance size in bytes\n-    if (allow_shared_alloc) {\n-      __ eden_allocate(x10, x13, 0, x28, slow_case);\n-    }\n-  }\n@@ -3540,3 +3522,0 @@\n-  \/\/ If USETLAB or allow_shared_alloc are true, the object is created above and\n-  \/\/ there is an initialized need. Otherwise, skip and go to the slow path.\n-  if (UseTLAB || allow_shared_alloc) {\n@@ -3545,1 +3524,0 @@\n-    __ bind(initialize_object);\n","filename":"src\/hotspot\/cpu\/riscv\/templateTable_riscv.cpp","additions":0,"deletions":22,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -141,1 +141,1 @@\n-    eden_allocate(noreg, obj, var_size_in_bytes, con_size_in_bytes, t1, slow_case);\n+    jmp(slow_case);\n","filename":"src\/hotspot\/cpu\/x86\/c1_MacroAssembler_x86.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1033,55 +1033,0 @@\n-        \/\/ If TLAB is disabled, see if there is support for inlining contiguous\n-        \/\/ allocations.\n-        \/\/ Otherwise, just go to the slow path.\n-        if ((id == fast_new_instance_id || id == fast_new_instance_init_check_id) && !UseTLAB\n-            && Universe::heap()->supports_inline_contig_alloc()) {\n-          Label slow_path;\n-          Register obj_size = rcx;\n-          Register t1       = rbx;\n-          Register t2       = rsi;\n-          assert_different_registers(klass, obj, obj_size, t1, t2);\n-\n-          __ push(rdi);\n-          __ push(rbx);\n-\n-          if (id == fast_new_instance_init_check_id) {\n-            \/\/ make sure the klass is initialized\n-            __ cmpb(Address(klass, InstanceKlass::init_state_offset()), InstanceKlass::fully_initialized);\n-            __ jcc(Assembler::notEqual, slow_path);\n-          }\n-\n-#ifdef ASSERT\n-          \/\/ assert object can be fast path allocated\n-          {\n-            Label ok, not_ok;\n-            __ movl(obj_size, Address(klass, Klass::layout_helper_offset()));\n-            __ cmpl(obj_size, 0);  \/\/ make sure it's an instance (LH > 0)\n-            __ jcc(Assembler::lessEqual, not_ok);\n-            __ testl(obj_size, Klass::_lh_instance_slow_path_bit);\n-            __ jcc(Assembler::zero, ok);\n-            __ bind(not_ok);\n-            __ stop(\"assert(can be fast path allocated)\");\n-            __ should_not_reach_here();\n-            __ bind(ok);\n-          }\n-#endif \/\/ ASSERT\n-\n-          const Register thread = NOT_LP64(rdi) LP64_ONLY(r15_thread);\n-          NOT_LP64(__ get_thread(thread));\n-\n-          \/\/ get the instance size (size is positive so movl is fine for 64bit)\n-          __ movl(obj_size, Address(klass, Klass::layout_helper_offset()));\n-\n-          __ eden_allocate(thread, obj, obj_size, 0, t1, slow_path);\n-\n-          __ initialize_object(obj, klass, obj_size, 0, t1, t2, \/* is_tlab_allocated *\/ false);\n-          __ verify_oop(obj);\n-          __ pop(rbx);\n-          __ pop(rdi);\n-          __ ret(0);\n-\n-          __ bind(slow_path);\n-          __ pop(rbx);\n-          __ pop(rdi);\n-        }\n-\n@@ -1152,41 +1097,0 @@\n-        \/\/ If TLAB is disabled, see if there is support for inlining contiguous\n-        \/\/ allocations.\n-        \/\/ Otherwise, just go to the slow path.\n-        if (!UseTLAB && Universe::heap()->supports_inline_contig_alloc()) {\n-          Register arr_size = rsi;\n-          Register t1       = rcx;  \/\/ must be rcx for use as shift count\n-          Register t2       = rdi;\n-          Label slow_path;\n-\n-          \/\/ get the allocation size: round_up(hdr + length << (layout_helper & 0x1F))\n-          \/\/ since size is positive movl does right thing on 64bit\n-          __ movl(t1, Address(klass, Klass::layout_helper_offset()));\n-          \/\/ since size is positive movl does right thing on 64bit\n-          __ movl(arr_size, length);\n-          assert(t1 == rcx, \"fixed register usage\");\n-          __ shlptr(arr_size \/* by t1=rcx, mod 32 *\/);\n-          __ shrptr(t1, Klass::_lh_header_size_shift);\n-          __ andptr(t1, Klass::_lh_header_size_mask);\n-          __ addptr(arr_size, t1);\n-          __ addptr(arr_size, MinObjAlignmentInBytesMask); \/\/ align up\n-          __ andptr(arr_size, ~MinObjAlignmentInBytesMask);\n-\n-          \/\/ Using t2 for non 64-bit.\n-          const Register thread = NOT_LP64(t2) LP64_ONLY(r15_thread);\n-          NOT_LP64(__ get_thread(thread));\n-          __ eden_allocate(thread, obj, arr_size, 0, t1, slow_path);  \/\/ preserves arr_size\n-\n-          __ initialize_header(obj, klass, length, t1, t2);\n-          __ movb(t1, Address(klass, in_bytes(Klass::layout_helper_offset()) + (Klass::_lh_header_size_shift \/ BitsPerByte)));\n-          assert(Klass::_lh_header_size_shift % BitsPerByte == 0, \"bytewise\");\n-          assert(Klass::_lh_header_size_mask <= 0xFF, \"bytewise\");\n-          __ andptr(t1, Klass::_lh_header_size_mask);\n-          __ subptr(arr_size, t1);  \/\/ body length\n-          __ addptr(t1, obj);       \/\/ body start\n-          __ initialize_body(t1, arr_size, 0, t2);\n-          __ verify_oop(obj);\n-          __ ret(0);\n-\n-          __ bind(slow_path);\n-        }\n-\n","filename":"src\/hotspot\/cpu\/x86\/c1_Runtime1_x86.cpp","additions":0,"deletions":96,"binary":false,"changes":96,"status":"modified"},{"patch":"@@ -245,36 +245,0 @@\n-\/\/ Defines obj, preserves var_size_in_bytes\n-void BarrierSetAssembler::eden_allocate(MacroAssembler* masm,\n-                                        Register thread, Register obj,\n-                                        Register var_size_in_bytes,\n-                                        int con_size_in_bytes,\n-                                        Register t1,\n-                                        Label& slow_case) {\n-  assert(obj == rax, \"obj must be in rax, for cmpxchg\");\n-  assert_different_registers(obj, var_size_in_bytes, t1);\n-  if (!Universe::heap()->supports_inline_contig_alloc()) {\n-    __ jmp(slow_case);\n-  } else {\n-    Register end = t1;\n-    Label retry;\n-    __ bind(retry);\n-    ExternalAddress heap_top((address) Universe::heap()->top_addr());\n-    __ movptr(obj, heap_top);\n-    if (var_size_in_bytes == noreg) {\n-      __ lea(end, Address(obj, con_size_in_bytes));\n-    } else {\n-      __ lea(end, Address(obj, var_size_in_bytes, Address::times_1));\n-    }\n-    \/\/ if end < obj then we wrapped around => object too long => slow case\n-    __ cmpptr(end, obj);\n-    __ jcc(Assembler::below, slow_case);\n-    __ cmpptr(end, ExternalAddress((address) Universe::heap()->end_addr()));\n-    __ jcc(Assembler::above, slow_case);\n-    \/\/ Compare obj with the top addr, and if still equal, store the new top addr in\n-    \/\/ end at the address of the top addr pointer. Sets ZF if was equal, and clears\n-    \/\/ it otherwise. Use lock prefix for atomicity on MPs.\n-    __ locked_cmpxchgptr(end, heap_top);\n-    __ jcc(Assembler::notEqual, retry);\n-    incr_allocated_bytes(masm, thread, var_size_in_bytes, con_size_in_bytes, thread->is_valid() ? noreg : t1);\n-  }\n-}\n-\n","filename":"src\/hotspot\/cpu\/x86\/gc\/shared\/barrierSetAssembler_x86.cpp","additions":0,"deletions":36,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -62,6 +62,0 @@\n-  virtual void eden_allocate(MacroAssembler* masm,\n-                             Register thread, Register obj,\n-                             Register var_size_in_bytes,\n-                             int con_size_in_bytes,\n-                             Register t1,\n-                             Label& slow_case);\n","filename":"src\/hotspot\/cpu\/x86\/gc\/shared\/barrierSetAssembler_x86.hpp","additions":0,"deletions":6,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -3923,10 +3923,0 @@\n-\/\/ Defines obj, preserves var_size_in_bytes\n-void MacroAssembler::eden_allocate(Register thread, Register obj,\n-                                   Register var_size_in_bytes,\n-                                   int con_size_in_bytes,\n-                                   Register t1,\n-                                   Label& slow_case) {\n-  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-  bs->eden_allocate(this, thread, obj, var_size_in_bytes, con_size_in_bytes, t1, slow_case);\n-}\n-\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":0,"deletions":10,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -568,8 +568,0 @@\n-  void eden_allocate(\n-    Register thread,                   \/\/ Current thread\n-    Register obj,                      \/\/ result: pointer to object after successful allocation\n-    Register var_size_in_bytes,        \/\/ object size in bytes if unknown at compile time; invalid otherwise\n-    int      con_size_in_bytes,        \/\/ object size in bytes if   known at compile time\n-    Register t1,                       \/\/ temp register\n-    Label&   slow_case                 \/\/ continuation point if fast allocation fails\n-  );\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":0,"deletions":8,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -3925,1 +3925,0 @@\n-  Label initialize_object;  \/\/ including clearing the fields\n@@ -3955,5 +3954,0 @@\n-  \/\/  Else If inline contiguous allocations are enabled:\n-  \/\/    Try to allocate in eden.\n-  \/\/    If fails due to heap end, go to slow path.\n-  \/\/\n-  \/\/  If TLAB is enabled OR inline contiguous is enabled:\n@@ -3965,3 +3959,0 @@\n-  const bool allow_shared_alloc =\n-    Universe::heap()->supports_inline_contig_alloc();\n-\n@@ -3969,5 +3960,0 @@\n-#ifndef _LP64\n-  if (UseTLAB || allow_shared_alloc) {\n-    __ get_thread(thread);\n-  }\n-#endif \/\/ _LP64\n@@ -3976,0 +3962,1 @@\n+    NOT_LP64(__ get_thread(thread);)\n@@ -3980,3 +3967,0 @@\n-    } else {\n-      \/\/ initialize both the header and fields\n-      __ jmp(initialize_object);\n@@ -3984,6 +3968,0 @@\n-  } else {\n-    \/\/ Allocation in the shared Eden, if allowed.\n-    \/\/\n-    \/\/ rdx: instance size in bytes\n-    __ eden_allocate(thread, rax, rdx, 0, rbx, slow_case);\n-  }\n@@ -3991,3 +3969,0 @@\n-  \/\/ If UseTLAB or allow_shared_alloc are true, the object is created above and\n-  \/\/ there is an initialize need. Otherwise, skip and go to the slow path.\n-  if (UseTLAB || allow_shared_alloc) {\n@@ -3996,1 +3971,0 @@\n-    __ bind(initialize_object);\n","filename":"src\/hotspot\/cpu\/x86\/templateTable_x86.cpp","additions":1,"deletions":27,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -46,9 +46,0 @@\n-  const bool allow_shared_alloc =\n-    Universe::heap()->supports_inline_contig_alloc();\n-\n-  if (allow_shared_alloc) {\n-    _heap_top_addr = (address) Universe::heap()->top_addr();\n-  } else {\n-    _heap_top_addr = NULL;\n-  }\n-\n","filename":"src\/hotspot\/os_cpu\/linux_arm\/javaThread_linux_arm.cpp","additions":0,"deletions":9,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -31,1 +31,0 @@\n-  address _heap_top_addr;\n@@ -47,1 +46,0 @@\n-  static ByteSize heap_top_addr_offset()         { return byte_offset_of(JavaThread, _heap_top_addr); }\n","filename":"src\/hotspot\/os_cpu\/linux_arm\/javaThread_linux_arm.hpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -221,5 +221,0 @@\n-  bool supports_inline_contig_alloc() const { return !UseNUMA; }\n-\n-  HeapWord* volatile* top_addr() const { return !UseNUMA ? young_gen()->top_addr() : (HeapWord* volatile*)-1; }\n-  HeapWord** end_addr() const { return !UseNUMA ? young_gen()->end_addr() : (HeapWord**)-1; }\n-\n","filename":"src\/hotspot\/share\/gc\/parallel\/parallelScavengeHeap.hpp","additions":0,"deletions":5,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -136,3 +136,0 @@\n-  HeapWord* volatile* top_addr() const   { return eden_space()->top_addr(); }\n-  HeapWord** end_addr() const   { return eden_space()->end_addr(); }\n-\n","filename":"src\/hotspot\/share\/gc\/parallel\/psYoungGen.hpp","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -461,3 +461,0 @@\n-HeapWord* volatile* DefNewGeneration::top_addr() const { return eden()->top_addr(); }\n-HeapWord** DefNewGeneration::end_addr() const { return eden()->end_addr(); }\n-\n","filename":"src\/hotspot\/share\/gc\/serial\/defNewGeneration.cpp","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -226,4 +226,0 @@\n-  bool supports_inline_contig_alloc() const { return true; }\n-  HeapWord* volatile* top_addr() const;\n-  HeapWord** end_addr() const;\n-\n","filename":"src\/hotspot\/share\/gc\/serial\/defNewGeneration.hpp","additions":0,"deletions":4,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -702,0 +702,1 @@\n+  assert(UseTLAB, \"Only for TLAB enabled allocations\");\n@@ -703,2 +704,3 @@\n-  Node* eden_top_adr;\n-  Node* eden_end_adr;\n+  Node* thread = macro->transform_later(new ThreadLocalNode());\n+  Node* tlab_top_adr = macro->basic_plus_adr(macro->top()\/*not oop*\/, thread, in_bytes(JavaThread::tlab_top_offset()));\n+  Node* tlab_end_adr = macro->basic_plus_adr(macro->top()\/*not oop*\/, thread, in_bytes(JavaThread::tlab_end_offset()));\n@@ -706,3 +708,1 @@\n-  macro->set_eden_pointers(eden_top_adr, eden_end_adr);\n-\n-  \/\/ Load Eden::end.  Loop invariant and hoisted.\n+  \/\/ Load TLAB end.\n@@ -710,2 +710,2 @@\n-  \/\/ Note: We set the control input on \"eden_end\" and \"old_eden_top\" when using\n-  \/\/       a TLAB to work around a bug where these values were being moved across\n+  \/\/ Note: We set the control input on \"tlab_end\" and \"old_tlab_top\" to work around\n+  \/\/       a bug where these values were being moved across\n@@ -718,19 +718,13 @@\n-  Node *eden_end = macro->make_load(toobig_false, mem, eden_end_adr, 0, TypeRawPtr::BOTTOM, T_ADDRESS);\n-\n-  \/\/ We need a Region for the loop-back contended case.\n-  enum { fall_in_path = 1, contended_loopback_path = 2 };\n-  Node *contended_region;\n-  Node *contended_phi_rawmem;\n-  if (UseTLAB) {\n-    contended_region = toobig_false;\n-    contended_phi_rawmem = mem;\n-  } else {\n-    contended_region = new RegionNode(3);\n-    contended_phi_rawmem = new PhiNode(contended_region, Type::MEMORY, TypeRawPtr::BOTTOM);\n-    \/\/ Now handle the passing-too-big test.  We fall into the contended\n-    \/\/ loop-back merge point.\n-    contended_region    ->init_req(fall_in_path, toobig_false);\n-    contended_phi_rawmem->init_req(fall_in_path, mem);\n-    macro->transform_later(contended_region);\n-    macro->transform_later(contended_phi_rawmem);\n-  }\n+  Node *tlab_end = macro->make_load(toobig_false, mem, tlab_end_adr, 0, TypeRawPtr::BOTTOM, T_ADDRESS);\n+\n+  \/\/ Load the TLAB top.\n+  Node *old_tlab_top = new LoadPNode(toobig_false, mem, tlab_top_adr, TypeRawPtr::BOTTOM, TypeRawPtr::BOTTOM, MemNode::unordered);\n+  macro->transform_later(old_tlab_top);\n+\n+  \/\/ Add to heap top to get a new TLAB top\n+  Node* new_tlab_top = new AddPNode(macro->top(), old_tlab_top, size_in_bytes);\n+  macro->transform_later(new_tlab_top);\n+\n+  \/\/ Check against TLAB end\n+  Node* tlab_full = new CmpPNode(new_tlab_top, tlab_end);\n+  macro->transform_later(tlab_full);\n@@ -738,14 +732,1 @@\n-  \/\/ Load(-locked) the heap top.\n-  \/\/ See note above concerning the control input when using a TLAB\n-  Node *old_eden_top = UseTLAB\n-    ? new LoadPNode      (toobig_false, contended_phi_rawmem, eden_top_adr, TypeRawPtr::BOTTOM, TypeRawPtr::BOTTOM, MemNode::unordered)\n-    : new LoadPLockedNode(contended_region, contended_phi_rawmem, eden_top_adr, MemNode::acquire);\n-\n-  macro->transform_later(old_eden_top);\n-  \/\/ Add to heap top to get a new heap top\n-  Node *new_eden_top = new AddPNode(macro->top(), old_eden_top, size_in_bytes);\n-  macro->transform_later(new_eden_top);\n-  \/\/ Check for needing a GC; compare against heap end\n-  Node *needgc_cmp = new CmpPNode(new_eden_top, eden_end);\n-  macro->transform_later(needgc_cmp);\n-  Node *needgc_bol = new BoolNode(needgc_cmp, BoolTest::ge);\n+  Node* needgc_bol = new BoolNode(tlab_full, BoolTest::ge);\n@@ -753,1 +734,1 @@\n-  IfNode *needgc_iff = new IfNode(contended_region, needgc_bol, PROB_UNLIKELY_MAG(4), COUNT_UNKNOWN);\n+  IfNode* needgc_iff = new IfNode(toobig_false, needgc_bol, PROB_UNLIKELY_MAG(4), COUNT_UNKNOWN);\n@@ -761,1 +742,1 @@\n-  \/\/ No need for a GC.  Setup for the Store-Conditional\n+  \/\/ No need for a GC.\n@@ -765,58 +746,12 @@\n-  i_o = macro->prefetch_allocation(i_o, needgc_false, contended_phi_rawmem,\n-                                   old_eden_top, new_eden_top, prefetch_lines);\n-\n-  Node* fast_oop = old_eden_top;\n-\n-  \/\/ Store (-conditional) the modified eden top back down.\n-  \/\/ StorePConditional produces flags for a test PLUS a modified raw\n-  \/\/ memory state.\n-  if (UseTLAB) {\n-    Node* store_eden_top =\n-      new StorePNode(needgc_false, contended_phi_rawmem, eden_top_adr,\n-                     TypeRawPtr::BOTTOM, new_eden_top, MemNode::unordered);\n-    macro->transform_later(store_eden_top);\n-    fast_oop_ctrl = needgc_false; \/\/ No contention, so this is the fast path\n-    fast_oop_rawmem = store_eden_top;\n-  } else {\n-    Node* store_eden_top =\n-      new StorePConditionalNode(needgc_false, contended_phi_rawmem, eden_top_adr,\n-                                new_eden_top, fast_oop\/*old_eden_top*\/);\n-    macro->transform_later(store_eden_top);\n-    Node *contention_check = new BoolNode(store_eden_top, BoolTest::ne);\n-    macro->transform_later(contention_check);\n-    store_eden_top = new SCMemProjNode(store_eden_top);\n-    macro->transform_later(store_eden_top);\n-\n-    \/\/ If not using TLABs, check to see if there was contention.\n-    IfNode *contention_iff = new IfNode (needgc_false, contention_check, PROB_MIN, COUNT_UNKNOWN);\n-    macro->transform_later(contention_iff);\n-    Node *contention_true = new IfTrueNode(contention_iff);\n-    macro->transform_later(contention_true);\n-    \/\/ If contention, loopback and try again.\n-    contended_region->init_req(contended_loopback_path, contention_true);\n-    contended_phi_rawmem->init_req(contended_loopback_path, store_eden_top);\n-\n-    \/\/ Fast-path succeeded with no contention!\n-    Node *contention_false = new IfFalseNode(contention_iff);\n-    macro->transform_later(contention_false);\n-    fast_oop_ctrl = contention_false;\n-\n-    \/\/ Bump total allocated bytes for this thread\n-    Node* thread = new ThreadLocalNode();\n-    macro->transform_later(thread);\n-    Node* alloc_bytes_adr = macro->basic_plus_adr(macro->top()\/*not oop*\/, thread,\n-                                                  in_bytes(JavaThread::allocated_bytes_offset()));\n-    Node* alloc_bytes = macro->make_load(fast_oop_ctrl, store_eden_top, alloc_bytes_adr,\n-                                         0, TypeLong::LONG, T_LONG);\n-#ifdef _LP64\n-    Node* alloc_size = size_in_bytes;\n-#else\n-    Node* alloc_size = new ConvI2LNode(size_in_bytes);\n-    macro->transform_later(alloc_size);\n-#endif\n-    Node* new_alloc_bytes = new AddLNode(alloc_bytes, alloc_size);\n-    macro->transform_later(new_alloc_bytes);\n-    fast_oop_rawmem = macro->make_store(fast_oop_ctrl, store_eden_top, alloc_bytes_adr,\n-                                        0, new_alloc_bytes, T_LONG);\n-  }\n-  return fast_oop;\n+  \/\/ Fast path:\n+  i_o = macro->prefetch_allocation(i_o, needgc_false, mem,\n+                                   old_tlab_top, new_tlab_top, prefetch_lines);\n+\n+  \/\/ Store the modified TLAB top back down.\n+  Node* store_tlab_top = new StorePNode(needgc_false, mem, tlab_top_adr,\n+                   TypeRawPtr::BOTTOM, new_tlab_top, MemNode::unordered);\n+  macro->transform_later(store_tlab_top);\n+\n+  fast_oop_ctrl = needgc_false;\n+  fast_oop_rawmem = store_tlab_top;\n+  return old_tlab_top;\n","filename":"src\/hotspot\/share\/gc\/shared\/c2\/barrierSetC2.cpp","additions":35,"deletions":100,"binary":false,"changes":135,"status":"modified"},{"patch":"@@ -323,21 +323,0 @@\n-  \/\/ Some heaps may offer a contiguous region for shared non-blocking\n-  \/\/ allocation, via inlined code (by exporting the address of the top and\n-  \/\/ end fields defining the extent of the contiguous allocation region.)\n-\n-  \/\/ This function returns \"true\" iff the heap supports this kind of\n-  \/\/ allocation.  (Default is \"no\".)\n-  virtual bool supports_inline_contig_alloc() const {\n-    return false;\n-  }\n-  \/\/ These functions return the addresses of the fields that define the\n-  \/\/ boundaries of the contiguous allocation area.  (These fields should be\n-  \/\/ physically near to one another.)\n-  virtual HeapWord* volatile* top_addr() const {\n-    guarantee(false, \"inline contiguous allocation not supported\");\n-    return NULL;\n-  }\n-  virtual HeapWord** end_addr() const {\n-    guarantee(false, \"inline contiguous allocation not supported\");\n-    return NULL;\n-  }\n-\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.hpp","additions":0,"deletions":21,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -306,2 +306,0 @@\n-    assert(young->supports_inline_contig_alloc(),\n-      \"Otherwise, must do alloc within heap lock\");\n@@ -820,12 +818,0 @@\n-bool GenCollectedHeap::supports_inline_contig_alloc() const {\n-  return _young_gen->supports_inline_contig_alloc();\n-}\n-\n-HeapWord* volatile* GenCollectedHeap::top_addr() const {\n-  return _young_gen->top_addr();\n-}\n-\n-HeapWord** GenCollectedHeap::end_addr() const {\n-  return _young_gen->end_addr();\n-}\n-\n@@ -1192,2 +1178,0 @@\n-  size_t actual_gap = pointer_delta((HeapWord*) (max_uintx-3), *(end_addr()));\n-  guarantee(!CompilerConfig::is_c2_or_jvmci_compiler_enabled() || actual_gap > (size_t)FastAllocateSizeLimit, \"inline allocation wraps\");\n","filename":"src\/hotspot\/share\/gc\/shared\/genCollectedHeap.cpp","additions":0,"deletions":16,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -184,6 +184,0 @@\n-  \/\/ We may support a shared contiguous allocation area, if the youngest\n-  \/\/ generation does.\n-  bool supports_inline_contig_alloc() const;\n-  HeapWord* volatile* top_addr() const;\n-  HeapWord** end_addr() const;\n-\n","filename":"src\/hotspot\/share\/gc\/shared\/genCollectedHeap.hpp","additions":0,"deletions":6,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -228,18 +228,0 @@\n-  \/\/ Some generation may offer a region for shared, contiguous allocation,\n-  \/\/ via inlined code (by exporting the address of the top and end fields\n-  \/\/ defining the extent of the contiguous allocation region.)\n-\n-  \/\/ This function returns \"true\" iff the heap supports this kind of\n-  \/\/ allocation.  (More precisely, this means the style of allocation that\n-  \/\/ increments *top_addr()\" with a CAS.) (Default is \"no\".)\n-  \/\/ A generation that supports this allocation style must use lock-free\n-  \/\/ allocation for *all* allocation, since there are times when lock free\n-  \/\/ allocation will be concurrent with plain \"allocate\" calls.\n-  virtual bool supports_inline_contig_alloc() const { return false; }\n-\n-  \/\/ These functions return the addresses of the fields that define the\n-  \/\/ boundaries of the contiguous allocation area.  (These fields should be\n-  \/\/ physically near to one another.)\n-  virtual HeapWord* volatile* top_addr() const { return NULL; }\n-  virtual HeapWord** end_addr() const { return NULL; }\n-\n","filename":"src\/hotspot\/share\/gc\/shared\/generation.hpp","additions":0,"deletions":18,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -122,3 +122,3 @@\n-  _supports_inline_contig_alloc = Universe::heap()->supports_inline_contig_alloc();\n-  _heap_end_addr = _supports_inline_contig_alloc ? Universe::heap()->end_addr() : (HeapWord**) -1;\n-  _heap_top_addr = _supports_inline_contig_alloc ? Universe::heap()->top_addr() : (HeapWord* volatile*) -1;\n+  _supports_inline_contig_alloc = false;\n+  _heap_end_addr = (HeapWord**) -1;\n+  _heap_top_addr = (HeapWord* volatile*) -1;\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompilerToVMInit.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -1120,17 +1120,0 @@\n-\/\/---------------------------set_eden_pointers-------------------------\n-void PhaseMacroExpand::set_eden_pointers(Node* &eden_top_adr, Node* &eden_end_adr) {\n-  if (UseTLAB) {                \/\/ Private allocation: load from TLS\n-    Node* thread = transform_later(new ThreadLocalNode());\n-    int tlab_top_offset = in_bytes(JavaThread::tlab_top_offset());\n-    int tlab_end_offset = in_bytes(JavaThread::tlab_end_offset());\n-    eden_top_adr = basic_plus_adr(top()\/*not oop*\/, thread, tlab_top_offset);\n-    eden_end_adr = basic_plus_adr(top()\/*not oop*\/, thread, tlab_end_offset);\n-  } else {                      \/\/ Shared allocation: load from globals\n-    CollectedHeap* ch = Universe::heap();\n-    address top_adr = (address)ch->top_addr();\n-    address end_adr = (address)ch->end_addr();\n-    eden_top_adr = makecon(TypeRawPtr::make(top_adr));\n-    eden_end_adr = basic_plus_adr(eden_top_adr, end_adr - top_adr);\n-  }\n-}\n-\n@@ -1247,1 +1230,1 @@\n-  if (!UseTLAB && !Universe::heap()->supports_inline_contig_alloc()) {\n+  if (!UseTLAB) {\n","filename":"src\/hotspot\/share\/opto\/macro.cpp","additions":1,"deletions":18,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -61,1 +61,0 @@\n-  void set_eden_pointers(Node* &eden_top_adr, Node* &eden_end_adr);\n","filename":"src\/hotspot\/share\/opto\/macro.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"}]}