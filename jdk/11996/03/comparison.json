{"files":[{"patch":"@@ -231,1 +231,1 @@\n-    guarantee (chk == -1 || chk == 0, \"Field too big for insn\");\n+    guarantee (chk == -1 || chk == 0, \"Field too big for insn at \" INTPTR_FORMAT, p2i(a));\n","filename":"src\/hotspot\/cpu\/aarch64\/assembler_aarch64.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -38,0 +38,1 @@\n+#include \"utilities\/formatBuffer.hpp\"\n@@ -39,0 +40,3 @@\n+#if INCLUDE_JVMCI\n+#include \"jvmci\/jvmciRuntime.hpp\"\n+#endif\n@@ -45,1 +49,1 @@\n-\/\/ This is the offset of the entry barrier from where the frame is completed.\n+\/\/ This is the offset of the entry barrier relative to where the frame is completed.\n@@ -65,2 +69,10 @@\n-class NativeNMethodBarrier: public NativeInstruction {\n-  address instruction_address() const { return addr_at(0); }\n+class NativeNMethodBarrier {\n+  address  _instruction_address;\n+  int*     _guard_addr;\n+  nmethod* _nm;\n+\n+  address instruction_address() const { return _instruction_address; }\n+\n+  int *guard_addr() {\n+    return _guard_addr;\n+  }\n@@ -73,9 +85,22 @@\n-  int *guard_addr(nmethod* nm) {\n-    if (nm->is_compiled_by_c2()) {\n-      \/\/ With c2 compiled code, the guard is out-of-line in a stub\n-      \/\/ We find it using the RelocIterator.\n-      RelocIterator iter(nm);\n-      while (iter.next()) {\n-        if (iter.type() == relocInfo::entry_guard_type) {\n-          entry_guard_Relocation* const reloc = iter.entry_guard_reloc();\n-          return reinterpret_cast<int*>(reloc->addr());\n+public:\n+  NativeNMethodBarrier(nmethod* nm): _nm(nm) {\n+#if INCLUDE_JVMCI\n+    if (nm->is_compiled_by_jvmci()) {\n+      _instruction_address = nm->code_begin() + nm->frame_complete_offset();\n+      _guard_addr = reinterpret_cast<int*>(nm->consts_begin() + nm->jvmci_nmethod_data()->nmethod_entry_patch_offset());\n+    } else\n+#endif\n+      {\n+        _instruction_address = nm->code_begin() + nm->frame_complete_offset() + entry_barrier_offset(nm);\n+        if (nm->is_compiled_by_c2()) {\n+          \/\/ With c2 compiled code, the guard is out-of-line in a stub\n+          \/\/ We find it using the RelocIterator.\n+          RelocIterator iter(nm);\n+          while (iter.next()) {\n+            if (iter.type() == relocInfo::entry_guard_type) {\n+              entry_guard_Relocation* const reloc = iter.entry_guard_reloc();\n+              _guard_addr = reinterpret_cast<int*>(reloc->addr());\n+              return;\n+            }\n+          }\n+          ShouldNotReachHere();\n@@ -83,0 +108,1 @@\n+        _guard_addr =  reinterpret_cast<int*>(instruction_address() + local_guard_offset(nm));\n@@ -84,3 +110,0 @@\n-      ShouldNotReachHere();\n-    }\n-    return reinterpret_cast<int*>(instruction_address() + local_guard_offset(nm));\n@@ -89,3 +112,2 @@\n-public:\n-  int get_value(nmethod* nm) {\n-    return Atomic::load_acquire(guard_addr(nm));\n+  int get_value() {\n+    return Atomic::load_acquire(guard_addr());\n@@ -94,2 +116,2 @@\n-  void set_value(nmethod* nm, int value) {\n-    Atomic::release_store(guard_addr(nm), value);\n+  void set_value(int value) {\n+    Atomic::release_store(guard_addr(), value);\n@@ -98,1 +120,5 @@\n-  void verify() const;\n+  bool check_barrier(FormatBuffer<>& msg) const;\n+  void verify() const {\n+    FormatBuffer<> msg(\"%s\", \"\");\n+    assert(check_barrier(msg), \"%s\", msg.buffer());\n+  }\n@@ -110,1 +136,1 @@\n-void NativeNMethodBarrier::verify() const {\n+bool NativeNMethodBarrier::check_barrier(FormatBuffer<>& msg) const {\n@@ -114,2 +140,2 @@\n-    tty->print_cr(\"Addr: \" INTPTR_FORMAT \" Code: 0x%x\", (intptr_t)addr, inst);\n-    fatal(\"not an ldr (literal) instruction.\");\n+    msg.print(\"Addr: \" INTPTR_FORMAT \" Code: 0x%x not an ldr\", p2i(addr), inst);\n+    return false;\n@@ -117,0 +143,1 @@\n+  return true;\n@@ -159,7 +186,0 @@\n-static NativeNMethodBarrier* native_nmethod_barrier(nmethod* nm) {\n-  address barrier_address = nm->code_begin() + nm->frame_complete_offset() + entry_barrier_offset(nm);\n-  NativeNMethodBarrier* barrier = reinterpret_cast<NativeNMethodBarrier*>(barrier_address);\n-  debug_only(barrier->verify());\n-  return barrier;\n-}\n-\n@@ -182,2 +202,2 @@\n-  NativeNMethodBarrier* barrier = native_nmethod_barrier(nm);\n-  barrier->set_value(nm, value);\n+  NativeNMethodBarrier barrier(nm);\n+  barrier.set_value(value);\n@@ -191,2 +211,8 @@\n-  NativeNMethodBarrier* barrier = native_nmethod_barrier(nm);\n-  return barrier->get_value(nm);\n+  NativeNMethodBarrier barrier(nm);\n+  return barrier.get_value();\n+}\n+\n+#if INCLUDE_JVMCI\n+bool BarrierSetNMethod::verify_barrier(nmethod* nm, FormatBuffer<>& msg) {\n+  NativeNMethodBarrier barrier(nm);\n+  return barrier.check_barrier(msg);\n@@ -194,0 +220,1 @@\n+#endif\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/shared\/barrierSetNMethod_aarch64.cpp","additions":62,"deletions":35,"binary":false,"changes":97,"status":"modified"},{"patch":"@@ -61,2 +61,0 @@\n-#define COMPRESSED_CLASS_POINTERS_DEPENDS_ON_COMPRESSED_OOPS false\n-\n","filename":"src\/hotspot\/cpu\/aarch64\/globalDefinitions_aarch64.hpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -93,0 +93,4 @@\n+    if (_nmethod_entry_patch_offset == pc_offset) {\n+      \/\/ Remember the offset into constants\n+      _nmethod_entry_patch_offset = data_offset;\n+    }\n@@ -125,0 +129,1 @@\n+  NativeCall* call = NULL;\n@@ -127,1 +132,1 @@\n-      break;\n+      return;\n@@ -131,1 +136,1 @@\n-      NativeCall* call = nativeCall_at(_instructions->start() + pc_offset);\n+      call = nativeCall_at(_instructions->start() + pc_offset);\n@@ -138,1 +143,1 @@\n-      NativeCall* call = nativeCall_at(_instructions->start() + pc_offset);\n+      call = nativeCall_at(_instructions->start() + pc_offset);\n@@ -145,1 +150,1 @@\n-      NativeCall* call = nativeCall_at(_instructions->start() + pc_offset);\n+      call = nativeCall_at(_instructions->start() + pc_offset);\n@@ -154,0 +159,9 @@\n+  if (Continuations::enabled()) {\n+    \/\/ Check for proper post_call_nop\n+    NativePostCallNop* nop = nativePostCallNop_at(call->next_instruction_address());\n+    if (nop == NULL) {\n+      JVMCI_ERROR(\"missing post call nop at offset %d\", pc_offset);\n+    } else {\n+      _instructions->relocate(call->next_instruction_address(), relocInfo::post_call_nop_type);\n+    }\n+  }\n","filename":"src\/hotspot\/cpu\/aarch64\/jvmciCodeInstaller_aarch64.cpp","additions":18,"deletions":4,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -62,2 +62,0 @@\n-#define COMPRESSED_CLASS_POINTERS_DEPENDS_ON_COMPRESSED_OOPS false\n-\n","filename":"src\/hotspot\/cpu\/ppc\/globalDefinitions_ppc.hpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -39,0 +39,3 @@\n+#if INCLUDE_JVMCI\n+#include \"jvmci\/jvmciRuntime.hpp\"\n+#endif\n@@ -60,2 +63,10 @@\n-class NativeNMethodBarrier: public NativeInstruction {\n-  address instruction_address() const { return addr_at(0); }\n+class NativeNMethodBarrier {\n+  address  _instruction_address;\n+  int*     _guard_addr;\n+  nmethod* _nm;\n+\n+  address instruction_address() const { return _instruction_address; }\n+\n+  int *guard_addr() {\n+    return _guard_addr;\n+  }\n@@ -68,9 +79,23 @@\n-  int *guard_addr(nmethod* nm) {\n-    if (nm->is_compiled_by_c2()) {\n-      \/\/ With c2 compiled code, the guard is out-of-line in a stub\n-      \/\/ We find it using the RelocIterator.\n-      RelocIterator iter(nm);\n-      while (iter.next()) {\n-        if (iter.type() == relocInfo::entry_guard_type) {\n-          entry_guard_Relocation* const reloc = iter.entry_guard_reloc();\n-          return reinterpret_cast<int*>(reloc->addr());\n+public:\n+  NativeNMethodBarrier(nmethod* nm): _nm(nm) {\n+    address barrier_address;\n+#if INCLUDE_JVMCI\n+    if (nm->is_compiled_by_jvmci()) {\n+      _instruction_address = nm->code_begin() + nm->frame_complete_offset();\n+      _guard_addr = reinterpret_cast<int*>(nm->consts_begin() + nm->jvmci_nmethod_data()->nmethod_entry_patch_offset());\n+    } else\n+#endif\n+      {\n+        _instruction_address = nm->code_begin() + nm->frame_complete_offset() + entry_barrier_offset(nm);\n+        if (nm->is_compiled_by_c2()) {\n+          \/\/ With c2 compiled code, the guard is out-of-line in a stub\n+          \/\/ We find it using the RelocIterator.\n+          RelocIterator iter(nm);\n+          while (iter.next()) {\n+            if (iter.type() == relocInfo::entry_guard_type) {\n+              entry_guard_Relocation* const reloc = iter.entry_guard_reloc();\n+              _guard_addr = reinterpret_cast<int*>(reloc->addr());\n+              return;\n+            }\n+          }\n+          ShouldNotReachHere();\n@@ -78,0 +103,1 @@\n+        _guard_addr = reinterpret_cast<int*>(instruction_address() + local_guard_offset(nm));\n@@ -79,3 +105,0 @@\n-      ShouldNotReachHere();\n-    }\n-    return reinterpret_cast<int*>(instruction_address() + local_guard_offset(nm));\n@@ -84,3 +107,2 @@\n-public:\n-  int get_value(nmethod* nm) {\n-    return Atomic::load_acquire(guard_addr(nm));\n+  int get_value() {\n+    return Atomic::load_acquire(guard_addr());\n@@ -89,2 +111,2 @@\n-  void set_value(nmethod* nm, int value) {\n-    Atomic::release_store(guard_addr(nm), value);\n+  void set_value(int value) {\n+    Atomic::release_store(guard_addr(), value);\n@@ -93,1 +115,5 @@\n-  void verify() const;\n+  bool check_barrier(FormatBuffer<>& msg) const;\n+  void verify() const {\n+    FormatBuffer<> msg(\"%s\", \"\");\n+    assert(check_barrier(msg), \"%s\", msg.buffer());\n+  }\n@@ -115,1 +141,1 @@\n-void NativeNMethodBarrier::verify() const {\n+bool NativeNMethodBarrier::check_barrier(FormatBuffer<>& msg) const {\n@@ -120,2 +146,2 @@\n-      tty->print_cr(\"Addr: \" INTPTR_FORMAT \" Code: 0x%x\", addr, inst);\n-      fatal(\"not an %s instruction.\", barrierInsn[i].name);\n+      msg.print(\"Addr: \" INTPTR_FORMAT \" Code: 0x%x not an %s instruction\", addr, inst, barrierInsn[i].name);\n+      return false;\n@@ -125,0 +151,1 @@\n+  return true;\n@@ -167,7 +194,0 @@\n-static NativeNMethodBarrier* native_nmethod_barrier(nmethod* nm) {\n-  address barrier_address = nm->code_begin() + nm->frame_complete_offset() + entry_barrier_offset(nm);\n-  NativeNMethodBarrier* barrier = reinterpret_cast<NativeNMethodBarrier*>(barrier_address);\n-  debug_only(barrier->verify());\n-  return barrier;\n-}\n-\n@@ -190,2 +210,2 @@\n-  NativeNMethodBarrier* barrier = native_nmethod_barrier(nm);\n-  barrier->set_value(nm, value);\n+  NativeNMethodBarrier barrier(nm);\n+  barrier.set_value(value);\n@@ -199,2 +219,8 @@\n-  NativeNMethodBarrier* barrier = native_nmethod_barrier(nm);\n-  return barrier->get_value(nm);\n+  NativeNMethodBarrier barrier(nm);\n+  return barrier.get_value();\n+}\n+\n+#if INCLUDE_JVMCI\n+bool BarrierSetNMethod::verify_barrier(nmethod* nm, FormatBuffer<>& msg) {\n+  NativeNMethodBarrier barrier(nm);\n+  return barrier.check_barrier(msg);\n@@ -202,0 +228,1 @@\n+#endif\n\\ No newline at end of file\n","filename":"src\/hotspot\/cpu\/riscv\/gc\/shared\/barrierSetNMethod_riscv.cpp","additions":61,"deletions":34,"binary":false,"changes":95,"status":"modified"},{"patch":"@@ -49,2 +49,0 @@\n-#define COMPRESSED_CLASS_POINTERS_DEPENDS_ON_COMPRESSED_OOPS false\n-\n","filename":"src\/hotspot\/cpu\/s390\/globalDefinitions_s390.hpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"utilities\/formatBuffer.hpp\"\n@@ -36,0 +37,3 @@\n+#if INCLUDE_JVMCI\n+#include \"jvmci\/jvmciRuntime.hpp\"\n+#endif\n@@ -61,1 +65,5 @@\n-  void verify() const;\n+  bool check_barrier(FormatBuffer<>& msg) const;\n+  void verify() const {\n+    FormatBuffer<> msg(\"%s\", \"\");\n+    assert(check_barrier(msg), \"%s\", msg.buffer());\n+  }\n@@ -65,1 +73,2 @@\n-void NativeNMethodCmpBarrier::verify() const {\n+bool NativeNMethodCmpBarrier::check_barrier(FormatBuffer<>& msg) const {\n+  \/\/ Only require 4 byte alignment\n@@ -67,1 +76,2 @@\n-    fatal(\"Not properly aligned\");\n+    msg.print(\"Addr: \" INTPTR_FORMAT \" not properly aligned\", p2i(instruction_address()));\n+    return false;\n@@ -72,3 +82,2 @@\n-    tty->print_cr(\"Addr: \" INTPTR_FORMAT \" Prefix: 0x%x\", p2i(instruction_address()),\n-        prefix);\n-    fatal(\"not a cmp barrier\");\n+    msg.print(\"Addr: \" INTPTR_FORMAT \" Code: 0x%x expected 0x%x\", p2i(instruction_address()), prefix, instruction_rex_prefix);\n+    return false;\n@@ -79,3 +88,2 @@\n-    tty->print_cr(\"Addr: \" INTPTR_FORMAT \" Code: 0x%x\", p2i(instruction_address()),\n-        inst);\n-    fatal(\"not a cmp barrier\");\n+    msg.print(\"Addr: \" INTPTR_FORMAT \" Code: 0x%x expected 0x%x\", p2i(instruction_address()), inst, instruction_code);\n+    return false;\n@@ -86,3 +94,2 @@\n-    tty->print_cr(\"Addr: \" INTPTR_FORMAT \" mod\/rm: 0x%x\", p2i(instruction_address()),\n-        modrm);\n-    fatal(\"not a cmp barrier\");\n+    msg.print(\"Addr: \" INTPTR_FORMAT \" Code: 0x%x expected mod\/rm 0x%x\", p2i(instruction_address()), modrm, instruction_modrm);\n+    return false;\n@@ -90,0 +97,1 @@\n+  return true;\n@@ -92,1 +100,1 @@\n-void NativeNMethodCmpBarrier::verify() const {\n+bool NativeNMethodCmpBarrier::check_barrier(FormatBuffer<>& msg) const {\n@@ -94,1 +102,2 @@\n-    fatal(\"Not properly aligned\");\n+    msg.print(\"Addr: \" INTPTR_FORMAT \" not properly aligned\", p2i(instruction_address()));\n+    return false;\n@@ -99,1 +108,1 @@\n-    tty->print_cr(\"Addr: \" INTPTR_FORMAT \" Code: 0x%x\", p2i(instruction_address()),\n+    msg.print(\"Addr: \" INTPTR_FORMAT \" Code: 0x%x\", p2i(instruction_address()),\n@@ -101,1 +110,1 @@\n-    fatal(\"not a cmp barrier\");\n+    return false;\n@@ -106,1 +115,1 @@\n-    tty->print_cr(\"Addr: \" INTPTR_FORMAT \" mod\/rm: 0x%x\", p2i(instruction_address()),\n+    msg.print(\"Addr: \" INTPTR_FORMAT \" mod\/rm: 0x%x\", p2i(instruction_address()),\n@@ -108,1 +117,1 @@\n-    fatal(\"not a cmp barrier\");\n+    return false;\n@@ -110,0 +119,1 @@\n+  return true;\n@@ -173,1 +183,10 @@\n-  address barrier_address = nm->code_begin() + nm->frame_complete_offset() + entry_barrier_offset(nm);\n+  address barrier_address;\n+#if INCLUDE_JVMCI\n+  if (nm->is_compiled_by_jvmci()) {\n+    barrier_address = nm->code_begin() + nm->jvmci_nmethod_data()->nmethod_entry_patch_offset();\n+  } else\n+#endif\n+    {\n+      barrier_address = nm->code_begin() + nm->frame_complete_offset() + entry_barrier_offset(nm);\n+    }\n+\n@@ -175,1 +194,1 @@\n-  debug_only(barrier->verify());\n+  barrier->verify();\n@@ -196,0 +215,8 @@\n+\n+\n+#if INCLUDE_JVMCI\n+bool BarrierSetNMethod::verify_barrier(nmethod* nm, FormatBuffer<>& msg) {\n+  NativeNMethodCmpBarrier* barrier = native_nmethod_barrier(nm);\n+  return barrier->check_barrier(msg);\n+}\n+#endif\n","filename":"src\/hotspot\/cpu\/x86\/gc\/shared\/barrierSetNMethod_x86.cpp","additions":47,"deletions":20,"binary":false,"changes":67,"status":"modified"},{"patch":"@@ -74,6 +74,0 @@\n-#if INCLUDE_JVMCI\n-#define COMPRESSED_CLASS_POINTERS_DEPENDS_ON_COMPRESSED_OOPS EnableJVMCI\n-#else\n-#define COMPRESSED_CLASS_POINTERS_DEPENDS_ON_COMPRESSED_OOPS false\n-#endif\n-\n","filename":"src\/hotspot\/cpu\/x86\/globalDefinitions_x86.hpp","additions":0,"deletions":6,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -188,0 +188,9 @@\n+  if (Continuations::enabled()) {\n+    \/\/ Check for proper post_call_nop\n+    NativePostCallNop* nop = nativePostCallNop_at(call->next_instruction_address());\n+    if (nop == NULL) {\n+      JVMCI_ERROR(\"missing post call nop at offset %d\", pc_offset);\n+    } else {\n+      _instructions->relocate(call->next_instruction_address(), relocInfo::post_call_nop_type);\n+    }\n+  }\n","filename":"src\/hotspot\/cpu\/x86\/jvmciCodeInstaller_x86.cpp","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -45,2 +45,0 @@\n-#define COMPRESSED_CLASS_POINTERS_DEPENDS_ON_COMPRESSED_OOPS false\n-\n","filename":"src\/hotspot\/cpu\/zero\/globalDefinitions_zero.hpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -553,3 +553,1 @@\n-  int nmethod_mirror_index,\n-  const char* nmethod_mirror_name,\n-  FailedSpeculation** failed_speculations\n+  JVMCINMethodData* jvmci_data\n@@ -564,1 +562,1 @@\n-  int jvmci_data_size = !compiler->is_jvmci() ? 0 : JVMCINMethodData::compute_size(nmethod_mirror_name);\n+  int jvmci_data_size = compiler->is_jvmci() ? jvmci_data->size() : 0;\n@@ -591,1 +589,1 @@\n-            jvmci_data_size\n+            jvmci_data\n@@ -596,6 +594,0 @@\n-#if INCLUDE_JVMCI\n-      if (compiler->is_jvmci()) {\n-        \/\/ Initialize the JVMCINMethodData object inlined into nm\n-        nm->jvmci_nmethod_data()->initialize(nmethod_mirror_index, nmethod_mirror_name, failed_speculations);\n-      }\n-#endif\n@@ -789,1 +781,1 @@\n-  int jvmci_data_size\n+  JVMCINMethodData* jvmci_data\n@@ -869,0 +861,1 @@\n+    int jvmci_data_size = compiler->is_jvmci() ? jvmci_data->size() : 0;\n@@ -888,0 +881,7 @@\n+#if INCLUDE_JVMCI\n+    if (compiler->is_jvmci()) {\n+      \/\/ Initialize the JVMCINMethodData object inlined into nm\n+      jvmci_nmethod_data()->copy(jvmci_data);\n+    }\n+#endif\n+\n","filename":"src\/hotspot\/share\/code\/nmethod.cpp","additions":12,"deletions":12,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -295,3 +295,3 @@\n-          , char* speculations,\n-          int speculations_len,\n-          int jvmci_data_size\n+          , char* speculations = nullptr,\n+          int speculations_len = 0,\n+          JVMCINMethodData* jvmci_data = nullptr\n@@ -348,3 +348,1 @@\n-                              int nmethod_mirror_index = -1,\n-                              const char* nmethod_mirror_name = nullptr,\n-                              FailedSpeculation** failed_speculations = nullptr\n+                              JVMCINMethodData* jvmci_data = nullptr\n","filename":"src\/hotspot\/share\/code\/nmethod.hpp","additions":4,"deletions":6,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -42,0 +42,3 @@\n+#if INCLUDE_JVMCI\n+#include \"jvmci\/jvmciRuntime.hpp\"\n+#endif\n@@ -65,2 +68,7 @@\n-  if (!nm->is_native_method() && !nm->is_compiled_by_c2() && !nm->is_compiled_by_c1()) {\n-    return false;\n+  if (nm->is_native_method() || nm->is_compiled_by_c2() || nm->is_compiled_by_c1()) {\n+    return true;\n+  }\n+\n+#if INCLUDE_JVMCI\n+  if (nm->is_compiled_by_jvmci() && nm->jvmci_nmethod_data()->has_entry_barrier()) {\n+    return true;\n@@ -68,0 +76,1 @@\n+#endif\n@@ -69,1 +78,1 @@\n-  return true;\n+  return false;\n","filename":"src\/hotspot\/share\/gc\/shared\/barrierSetNMethod.cpp","additions":12,"deletions":3,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -31,0 +31,3 @@\n+#if INCLUDE_JVMCI\n+#include \"utilities\/formatBuffer.hpp\"\n+#endif\n@@ -58,0 +61,4 @@\n+\n+#if INCLUDE_JVMCI\n+  bool verify_barrier(nmethod* nm, FormatBuffer<>& msg);\n+#endif\n","filename":"src\/hotspot\/share\/gc\/shared\/barrierSetNMethod.hpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"gc\/shared\/barrierSetNMethod.hpp\"\n@@ -382,1 +383,1 @@\n-    oopDesc::verify(obj);\n+    guarantee(oopDesc::is_oop_or_null(obj), \"invalid oop: \" INTPTR_FORMAT, p2i((oopDesc*) obj));\n@@ -730,0 +731,5 @@\n+    if (UseZGC && _nmethod_entry_patch_offset == -1) {\n+      \/\/ ZGC requires the use of entry barriers for correctness\n+      JVMCI_THROW_MSG_(IllegalArgumentException, \"InstalledCode entry barrier is missing\", JVMCI::ok);\n+    }\n+\n@@ -754,1 +760,2 @@\n-                                        speculations_len);\n+                                        speculations_len,\n+                                        _nmethod_entry_patch_offset);\n@@ -763,0 +770,11 @@\n+\n+      if (nm != nullptr) {\n+        if (_nmethod_entry_patch_offset != -1) {\n+          FormatBuffer<> msg(\"%s\", \"\");\n+          BarrierSetNMethod* bs_nm = BarrierSet::barrier_set()->barrier_set_nmethod();\n+\n+          if (!bs_nm->verify_barrier(nm, msg)) {\n+            JVMCI_THROW_MSG_(IllegalArgumentException, err_msg(\"nmethod entry barrier is malformed: %s\", msg.buffer()), JVMCI::ok);\n+          }\n+        }\n+      }\n@@ -807,0 +825,1 @@\n+  _has_monitors = false;\n@@ -808,0 +827,1 @@\n+  _nmethod_entry_patch_offset = -1;\n@@ -1257,0 +1277,3 @@\n+    case ENTRY_BARRIER_PATCH:\n+      _nmethod_entry_patch_offset = pc_offset;\n+      break;\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCodeInstaller.cpp","additions":25,"deletions":2,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -155,0 +155,1 @@\n+    ENTRY_BARRIER_PATCH,\n@@ -274,0 +275,1 @@\n+  int           _nmethod_entry_patch_offset;\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCodeInstaller.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -416,0 +416,11 @@\n+    if (klass == nullptr) {\n+      return nullptr;\n+    }\n+    if (base_object.is_non_null()) {\n+      \/\/ Reads from real objects are expected to be strongly reachable\n+      guarantee(klass->is_loader_alive(), \"klass must be alive\");\n+    } else if (!klass->is_loader_alive()) {\n+      \/\/ Reads from other memory like the HotSpotMethodData might be concurrently unloading so\n+      \/\/ return null in that case.\n+      return nullptr;\n+    }\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompilerToVM.cpp","additions":11,"deletions":0,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"gc\/shared\/barrierSetAssembler.hpp\"\n@@ -51,0 +52,20 @@\n+    static address SharedRuntime_polling_page_return_handler;\n+\n+    static address nmethod_entry_barrier;\n+    static int thread_disarmed_guard_value_offset;\n+    static int thread_address_bad_mask_offset;\n+#ifdef AARCH64\n+    static int BarrierSetAssembler_nmethod_patching_type;\n+    static address BarrierSetAssembler_patching_epoch_addr;\n+#endif\n+\n+    static address ZBarrierSetRuntime_load_barrier_on_oop_field_preloaded;\n+    static address ZBarrierSetRuntime_load_barrier_on_weak_oop_field_preloaded;\n+    static address ZBarrierSetRuntime_load_barrier_on_phantom_oop_field_preloaded;\n+    static address ZBarrierSetRuntime_weak_load_barrier_on_oop_field_preloaded;\n+    static address ZBarrierSetRuntime_weak_load_barrier_on_weak_oop_field_preloaded;\n+    static address ZBarrierSetRuntime_weak_load_barrier_on_phantom_oop_field_preloaded;\n+    static address ZBarrierSetRuntime_load_barrier_on_oop_array;\n+    static address ZBarrierSetRuntime_clone;\n+\n+    static bool continuations_enabled;\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompilerToVM.hpp","additions":21,"deletions":0,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"gc\/shared\/barrierSetAssembler.hpp\"\n@@ -33,0 +34,3 @@\n+#include \"gc\/shared\/barrierSetNMethod.hpp\"\n+#include \"gc\/z\/zThreadLocalData.hpp\"\n+#include \"gc\/z\/zBarrierSetRuntime.hpp\"\n@@ -55,0 +59,21 @@\n+address CompilerToVM::Data::SharedRuntime_polling_page_return_handler;\n+\n+address CompilerToVM::Data::nmethod_entry_barrier;\n+int CompilerToVM::Data::thread_disarmed_guard_value_offset;\n+int CompilerToVM::Data::thread_address_bad_mask_offset;\n+\n+address CompilerToVM::Data::ZBarrierSetRuntime_load_barrier_on_oop_field_preloaded;\n+address CompilerToVM::Data::ZBarrierSetRuntime_load_barrier_on_weak_oop_field_preloaded;\n+address CompilerToVM::Data::ZBarrierSetRuntime_load_barrier_on_phantom_oop_field_preloaded;\n+address CompilerToVM::Data::ZBarrierSetRuntime_weak_load_barrier_on_oop_field_preloaded;\n+address CompilerToVM::Data::ZBarrierSetRuntime_weak_load_barrier_on_weak_oop_field_preloaded;\n+address CompilerToVM::Data::ZBarrierSetRuntime_weak_load_barrier_on_phantom_oop_field_preloaded;\n+address CompilerToVM::Data::ZBarrierSetRuntime_load_barrier_on_oop_array;\n+address CompilerToVM::Data::ZBarrierSetRuntime_clone;\n+\n+bool CompilerToVM::Data::continuations_enabled;\n+\n+#ifdef AARCH64\n+int CompilerToVM::Data::BarrierSetAssembler_nmethod_patching_type;\n+address CompilerToVM::Data::BarrierSetAssembler_patching_epoch_addr;\n+#endif\n@@ -111,0 +136,25 @@\n+  SharedRuntime_polling_page_return_handler = SharedRuntime::polling_page_return_handler_blob()->entry_point();\n+\n+  BarrierSetNMethod* bs_nm = BarrierSet::barrier_set()->barrier_set_nmethod();\n+  if (bs_nm != nullptr) {\n+    thread_disarmed_guard_value_offset = in_bytes(bs_nm->thread_disarmed_guard_value_offset());\n+    AMD64_ONLY(nmethod_entry_barrier = StubRoutines::x86::method_entry_barrier());\n+    AARCH64_ONLY(nmethod_entry_barrier = StubRoutines::aarch64::method_entry_barrier());\n+    BarrierSetAssembler* bs_asm = BarrierSet::barrier_set()->barrier_set_assembler();\n+    AARCH64_ONLY(BarrierSetAssembler_nmethod_patching_type = (int) bs_asm->nmethod_patching_type());\n+    AARCH64_ONLY(BarrierSetAssembler_patching_epoch_addr = bs_asm->patching_epoch_addr());\n+  }\n+\n+  if (UseZGC) {\n+    thread_address_bad_mask_offset = in_bytes(ZThreadLocalData::address_bad_mask_offset());\n+    ZBarrierSetRuntime_load_barrier_on_oop_field_preloaded =                     ZBarrierSetRuntime::load_barrier_on_oop_field_preloaded_addr();\n+    ZBarrierSetRuntime_load_barrier_on_weak_oop_field_preloaded =                ZBarrierSetRuntime::load_barrier_on_weak_oop_field_preloaded_addr();\n+    ZBarrierSetRuntime_load_barrier_on_phantom_oop_field_preloaded =             ZBarrierSetRuntime::load_barrier_on_phantom_oop_field_preloaded_addr();\n+    ZBarrierSetRuntime_weak_load_barrier_on_oop_field_preloaded =                ZBarrierSetRuntime::weak_load_barrier_on_oop_field_preloaded_addr();\n+    ZBarrierSetRuntime_weak_load_barrier_on_weak_oop_field_preloaded =           ZBarrierSetRuntime::weak_load_barrier_on_weak_oop_field_preloaded_addr();\n+    ZBarrierSetRuntime_weak_load_barrier_on_phantom_oop_field_preloaded =        ZBarrierSetRuntime::weak_load_barrier_on_phantom_oop_field_preloaded_addr();\n+    ZBarrierSetRuntime_load_barrier_on_oop_array =                               ZBarrierSetRuntime::load_barrier_on_oop_array_addr();\n+    ZBarrierSetRuntime_clone =                                                   ZBarrierSetRuntime::clone_addr();\n+  }\n+\n+  continuations_enabled = Continuations::enabled();\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompilerToVMInit.cpp","additions":50,"deletions":0,"binary":false,"changes":50,"status":"modified"},{"patch":"@@ -1481,1 +1481,1 @@\n-  oop obj = *((oopDesc**) oopHandle);\n+  oop obj = NativeAccess<>::oop_load(reinterpret_cast<oop*>(oopHandle));\n@@ -1483,1 +1483,1 @@\n-    oopDesc::verify(obj);\n+    guarantee(oopDesc::is_oop_or_null(obj), \"invalid oop: \" INTPTR_FORMAT, p2i((oopDesc*) obj));\n","filename":"src\/hotspot\/share\/jvmci\/jvmciEnv.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -261,0 +261,6 @@\n+\n+  \/\/ The frame we rethrow the exception to might not have been processed by the GC yet.\n+  \/\/ The stack watermark barrier takes care of detecting that and ensuring the frame\n+  \/\/ has updated oops.\n+  StackWatermarkSet::after_unwind(current);\n+\n@@ -756,4 +762,4 @@\n-void JVMCINMethodData::initialize(\n-  int nmethod_mirror_index,\n-  const char* name,\n-  FailedSpeculation** failed_speculations)\n+void JVMCINMethodData::initialize(int nmethod_mirror_index,\n+                                  int nmethod_entry_patch_offset,\n+                                  const char* nmethod_mirror_name,\n+                                  FailedSpeculation** failed_speculations)\n@@ -763,1 +769,2 @@\n-  if (name != NULL) {\n+  _nmethod_entry_patch_offset = nmethod_entry_patch_offset;\n+  if (nmethod_mirror_name != NULL) {\n@@ -765,2 +772,2 @@\n-    char* dest = (char*) this->name();\n-    strcpy(dest, name);\n+    char* dest = (char*) name();\n+    strcpy(dest, nmethod_mirror_name);\n@@ -772,0 +779,4 @@\n+void JVMCINMethodData::copy(JVMCINMethodData* data) {\n+  initialize(data->_nmethod_mirror_index, data->_nmethod_entry_patch_offset, data->name(), data->_failed_speculations);\n+}\n+\n@@ -855,1 +866,1 @@\n-  return (jlong) ptr;\n+  return reinterpret_cast<jlong>(ptr);\n@@ -936,1 +947,2 @@\n-    int to_release = next - num_alive;\n+    if (next != num_alive) {\n+      int to_release = next - num_alive;\n@@ -938,2 +950,2 @@\n-    \/\/ `next` is now the index of the first null handle\n-    \/\/ Example: to_release: 2\n+      \/\/ `next` is now the index of the first null handle\n+      \/\/ Example: to_release: 2\n@@ -941,2 +953,2 @@\n-    \/\/ Bulk release the handles with a null referent\n-    object_handles()->release(_oop_handles.adr_at(num_alive), to_release);\n+      \/\/ Bulk release the handles with a null referent\n+      object_handles()->release(_oop_handles.adr_at(num_alive), to_release);\n@@ -944,4 +956,4 @@\n-    \/\/ Truncate oop handles to only those with a non-null referent\n-    JVMCI_event_1(\"compacted oop handles in JVMCI runtime %d from %d to %d\", _id, _oop_handles.length(), num_alive);\n-    _oop_handles.trunc_to(num_alive);\n-    \/\/ Example: HHH\n+      \/\/ Truncate oop handles to only those with a non-null referent\n+      JVMCI_event_1(\"compacted oop handles in JVMCI runtime %d from %d to %d\", _id, _oop_handles.length(), num_alive);\n+      _oop_handles.trunc_to(num_alive);\n+      \/\/ Example: HHH\n@@ -949,1 +961,2 @@\n-    return to_release;\n+      return to_release;\n+    }\n@@ -2089,1 +2102,2 @@\n-                                                       int speculations_len) {\n+                                                       int speculations_len,\n+                                                       int nmethod_entry_patch_offset) {\n@@ -2157,0 +2171,4 @@\n+      JVMCINMethodData* data = JVMCINMethodData::create(nmethod_mirror_index,\n+                                                        nmethod_entry_patch_offset,\n+                                                        nmethod_mirror_name,\n+                                                        failed_speculations);\n@@ -2166,2 +2184,1 @@\n-                                 speculations, speculations_len,\n-                                 nmethod_mirror_index, nmethod_mirror_name, failed_speculations);\n+                                 speculations, speculations_len, data);\n","filename":"src\/hotspot\/share\/jvmci\/jvmciRuntime.cpp","additions":38,"deletions":21,"binary":false,"changes":59,"status":"modified"},{"patch":"@@ -43,4 +43,4 @@\n-\/\/ Encapsulates the JVMCI metadata for an nmethod.\n-\/\/ JVMCINMethodData objects are inlined into nmethods\n-\/\/ at nmethod::_jvmci_data_offset.\n-class JVMCINMethodData {\n+\/\/ Encapsulates the JVMCI metadata for an nmethod.  JVMCINMethodData objects are normally inlined\n+\/\/ into nmethods at nmethod::_jvmci_data_offset but during construction of the nmethod they are\n+\/\/ resource allocated so they can be passed into the nmethod constructor.\n+class JVMCINMethodData : public ResourceObj {\n@@ -48,3 +48,0 @@\n-  \/\/ Index for the HotSpotNmethod mirror in the nmethod's oops table.\n-  \/\/ This is -1 if there is no mirror in the oops table.\n-  int _nmethod_mirror_index;\n@@ -56,0 +53,10 @@\n+  \/\/ Index for the HotSpotNmethod mirror in the nmethod's oops table.\n+  \/\/ This is -1 if there is no mirror in the oops table.\n+  int _nmethod_mirror_index;\n+\n+  \/\/ This is the offset of the patchable part of the nmethod entry barrier sequence.  The meaning is\n+  \/\/ somewhat platform dependent as the way patching is done varies by architecture.  Older JVMCI\n+  \/\/ based compilers didn't emit the entry barrier so having a positive value for this offset\n+  \/\/ confirms that the installed code supports the entry barrier.\n+  int _nmethod_entry_patch_offset;\n+\n@@ -68,0 +75,12 @@\n+  \/\/ Allocate a temporary data object for use during installation\n+  void initialize(int nmethod_mirror_index,\n+                   int nmethod_entry_patch_offset,\n+                   const char* nmethod_mirror_name,\n+                   FailedSpeculation** failed_speculations);\n+\n+  void* operator new(size_t size, const char* nmethod_mirror_name) {\n+    assert(size == sizeof(JVMCINMethodData), \"must agree\");\n+    size_t total_size = compute_size(nmethod_mirror_name);\n+    return (address)resource_allocate_bytes(total_size);\n+  }\n+\n@@ -69,0 +88,12 @@\n+  static JVMCINMethodData* create(int nmethod_mirror_index,\n+                                  int nmethod_entry_patch_offset,\n+                                  const char* nmethod_mirror_name,\n+                                  FailedSpeculation** failed_speculations) {\n+    JVMCINMethodData* result = new (nmethod_mirror_name) JVMCINMethodData();\n+    result->initialize(nmethod_mirror_index,\n+                       nmethod_entry_patch_offset,\n+                       nmethod_mirror_name,\n+                       failed_speculations);\n+    return result;\n+  }\n+\n@@ -78,3 +109,6 @@\n-  void initialize(int nmethod_mirror_index,\n-             const char* name,\n-             FailedSpeculation** failed_speculations);\n+  int size() {\n+    return compute_size(name());\n+  }\n+\n+  \/\/ Copy the contents of this object into data which is normally the storage allocated in the nmethod.\n+  void copy(JVMCINMethodData* data);\n@@ -97,0 +131,9 @@\n+\n+  bool has_entry_barrier() {\n+    return _nmethod_entry_patch_offset != -1;\n+  }\n+\n+  int nmethod_entry_patch_offset() {\n+    guarantee(_nmethod_entry_patch_offset != -1, \"missing entry barrier\");\n+    return _nmethod_entry_patch_offset;\n+  }\n@@ -417,1 +460,2 @@\n-                                           int                       speculations_len);\n+                                           int                       speculations_len,\n+                                           int                       nmethod_entry_patch_offset);\n","filename":"src\/hotspot\/share\/jvmci\/jvmciRuntime.hpp","additions":55,"deletions":11,"binary":false,"changes":66,"status":"modified"},{"patch":"@@ -214,1 +214,1 @@\n-  return UseSerialGC || UseParallelGC || UseG1GC;\n+  return UseSerialGC || UseParallelGC || UseG1GC || UseZGC;\n","filename":"src\/hotspot\/share\/jvmci\/jvmci_globals.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -59,0 +59,18 @@\n+  static_field(CompilerToVM::Data,             SharedRuntime_polling_page_return_handler,                                            \\\n+                                                                                       address)                                      \\\n+                                                                                                                                     \\\n+  static_field(CompilerToVM::Data,             nmethod_entry_barrier, address)                                                       \\\n+  static_field(CompilerToVM::Data,             thread_disarmed_guard_value_offset, int)                                              \\\n+  static_field(CompilerToVM::Data,             thread_address_bad_mask_offset, int)                                                  \\\n+  AARCH64_ONLY(static_field(CompilerToVM::Data, BarrierSetAssembler_nmethod_patching_type, int))                                     \\\n+                                                                                                                                     \\\n+  static_field(CompilerToVM::Data,             ZBarrierSetRuntime_load_barrier_on_oop_field_preloaded, address)                      \\\n+  static_field(CompilerToVM::Data,             ZBarrierSetRuntime_load_barrier_on_weak_oop_field_preloaded, address)                 \\\n+  static_field(CompilerToVM::Data,             ZBarrierSetRuntime_load_barrier_on_phantom_oop_field_preloaded, address)              \\\n+  static_field(CompilerToVM::Data,             ZBarrierSetRuntime_weak_load_barrier_on_oop_field_preloaded, address)                 \\\n+  static_field(CompilerToVM::Data,             ZBarrierSetRuntime_weak_load_barrier_on_weak_oop_field_preloaded, address)            \\\n+  static_field(CompilerToVM::Data,             ZBarrierSetRuntime_weak_load_barrier_on_phantom_oop_field_preloaded, address)         \\\n+  static_field(CompilerToVM::Data,             ZBarrierSetRuntime_load_barrier_on_oop_array, address)                                \\\n+  static_field(CompilerToVM::Data,             ZBarrierSetRuntime_clone, address)                                                    \\\n+                                                                                                                                     \\\n+  static_field(CompilerToVM::Data,             continuations_enabled, bool)                                                          \\\n@@ -186,0 +204,1 @@\n+  nonstatic_field(JavaThread,                  _saved_exception_pc,                           address)                               \\\n@@ -474,0 +493,1 @@\n+  declare_constant(CodeInstaller::ENTRY_BARRIER_PATCH)                    \\\n@@ -700,0 +720,4 @@\n+  AARCH64_ONLY(declare_constant(NMethodPatchingType::stw_instruction_and_data_patch))  \\\n+  AARCH64_ONLY(declare_constant(NMethodPatchingType::conc_instruction_and_data_patch)) \\\n+  AARCH64_ONLY(declare_constant(NMethodPatchingType::conc_data_patch))                 \\\n+                                                                          \\\n","filename":"src\/hotspot\/share\/jvmci\/vmStructs_jvmci.cpp","additions":24,"deletions":0,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -1504,3 +1504,0 @@\n-      if (COMPRESSED_CLASS_POINTERS_DEPENDS_ON_COMPRESSED_OOPS) {\n-        FLAG_SET_DEFAULT(UseCompressedClassPointers, false);\n-      }\n@@ -1524,16 +1521,9 @@\n-  if (COMPRESSED_CLASS_POINTERS_DEPENDS_ON_COMPRESSED_OOPS && !UseCompressedOops) {\n-    if (UseCompressedClassPointers) {\n-      warning(\"UseCompressedClassPointers requires UseCompressedOops\");\n-    }\n-    FLAG_SET_DEFAULT(UseCompressedClassPointers, false);\n-  } else {\n-    \/\/ Turn on UseCompressedClassPointers too\n-    if (FLAG_IS_DEFAULT(UseCompressedClassPointers)) {\n-      FLAG_SET_ERGO(UseCompressedClassPointers, true);\n-    }\n-    \/\/ Check the CompressedClassSpaceSize to make sure we use compressed klass ptrs.\n-    if (UseCompressedClassPointers) {\n-      if (CompressedClassSpaceSize > KlassEncodingMetaspaceMax) {\n-        warning(\"CompressedClassSpaceSize is too large for UseCompressedClassPointers\");\n-        FLAG_SET_DEFAULT(UseCompressedClassPointers, false);\n-      }\n+  \/\/ Turn on UseCompressedClassPointers too\n+  if (FLAG_IS_DEFAULT(UseCompressedClassPointers)) {\n+    FLAG_SET_ERGO(UseCompressedClassPointers, true);\n+  }\n+  \/\/ Check the CompressedClassSpaceSize to make sure we use compressed klass ptrs.\n+  if (UseCompressedClassPointers) {\n+    if (CompressedClassSpaceSize > KlassEncodingMetaspaceMax) {\n+      warning(\"CompressedClassSpaceSize is too large for UseCompressedClassPointers\");\n+      FLAG_SET_DEFAULT(UseCompressedClassPointers, false);\n@@ -1704,3 +1694,0 @@\n-          if (COMPRESSED_CLASS_POINTERS_DEPENDS_ON_COMPRESSED_OOPS) {\n-            FLAG_SET_ERGO(UseCompressedClassPointers, false);\n-          }\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":9,"deletions":22,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -321,1 +321,1 @@\n-                        finishedDataPatches, false, frameSize, deoptRescue, method, 0, id, 0L, false);\n+                        finishedDataPatches, false, frameSize, deoptRescue, method, -1, id, 0L, false);\n","filename":"test\/hotspot\/jtreg\/compiler\/jvmci\/jdk.vm.ci.code.test\/src\/jdk\/vm\/ci\/code\/test\/TestAssembler.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"}]}