{"files":[{"patch":"@@ -178,0 +178,14 @@\n+static volatile uint32_t _patching_epoch = 0;\n+\n+address BarrierSetAssembler::patching_epoch_addr() {\n+  return (address)&_patching_epoch;\n+}\n+\n+void BarrierSetAssembler::increment_patching_epoch() {\n+  Atomic::inc(&_patching_epoch);\n+}\n+\n+void BarrierSetAssembler::clear_patching_epoch() {\n+  _patching_epoch = 0;\n+}\n+\n@@ -209,0 +223,21 @@\n+  } else if (patching_type == NMethodPatchingType::conc_instruction_and_data_patch) {\n+    \/\/ If we patch code we need both a code patching and a loadload\n+    \/\/ fence. It's not super cheap, so we use a global epoch mechanism\n+    \/\/ to hide them in a slow path.\n+    \/\/ The high level idea of the global epoch mechanism is to detect\n+    \/\/ when any thread has performed the required fencing, after the\n+    \/\/ last nmethod was disarmed. This implies that the required\n+    \/\/ fencing has been performed for all preceding nmethod disarms\n+    \/\/ as well. Therefore, we do not need any further fencing.\n+    __ lea(rscratch2, ExternalAddress((address)&_patching_epoch));\n+    \/\/ Embed an artificial data dependency to order the guard load\n+    \/\/ before the epoch load.\n+    __ orr(rscratch2, rscratch2, rscratch1, Assembler::LSR, 32);\n+    \/\/ Read the global epoch value.\n+    __ ldrw(rscratch2, rscratch2);\n+    \/\/ Combine the guard value (low order) with the epoch value (high order).\n+    __ orr(rscratch1, rscratch1, rscratch2, Assembler::LSL, 32);\n+    \/\/ Compare the global values with the thread-local values.\n+    Address thread_disarmed_and_epoch_addr(rthread, in_bytes(bs_nm->thread_disarmed_guard_value_offset()));\n+    __ ldr(rscratch2, thread_disarmed_and_epoch_addr);\n+    __ cmp(rscratch1, rscratch2);\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/shared\/barrierSetAssembler_aarch64.cpp","additions":35,"deletions":0,"binary":false,"changes":35,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+  conc_instruction_and_data_patch,\n@@ -76,1 +77,2 @@\n-    return patching_type == NMethodPatchingType::stw_instruction_and_data_patch;\n+    return patching_type == NMethodPatchingType::conc_instruction_and_data_patch ||\n+            patching_type == NMethodPatchingType::stw_instruction_and_data_patch;\n@@ -78,0 +80,4 @@\n+\n+  static address patching_epoch_addr();\n+  static void clear_patching_epoch();\n+  static void increment_patching_epoch();\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/shared\/barrierSetAssembler_aarch64.hpp","additions":7,"deletions":1,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -60,0 +60,2 @@\n+  case NMethodPatchingType::conc_instruction_and_data_patch:\n+    return -4 * (10 + slow_path_size(nm));\n@@ -189,0 +191,11 @@\n+  if (value == disarmed_guard_value()) {\n+    \/\/ The patching epoch is incremented before the nmethod is disarmed. Disarming\n+    \/\/ is performed with a release store. In the nmethod entry barrier, the values\n+    \/\/ are read in the opposite order, such that the load of the nmethod guard\n+    \/\/ acquires the patching epoch. This way, the guard is guaranteed to block\n+    \/\/ entries to the nmethod, until it has safely published the requirement for\n+    \/\/ further fencing by mutators, before they are allowed to enter.\n+    BarrierSetAssembler* bs_asm = BarrierSet::barrier_set()->barrier_set_assembler();\n+    bs_asm->increment_patching_epoch();\n+  }\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/shared\/barrierSetNMethod_aarch64.cpp","additions":13,"deletions":0,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -5331,0 +5331,14 @@\n+    BarrierSetAssembler* bs_asm = BarrierSet::barrier_set()->barrier_set_assembler();\n+\n+    if (bs_asm->nmethod_patching_type() == NMethodPatchingType::conc_instruction_and_data_patch) {\n+      BarrierSetNMethod* bs_nm = BarrierSet::barrier_set()->barrier_set_nmethod();\n+      \/\/ We can get here despite the nmethod being good, if we have not\n+      \/\/ yet applied our cross modification fence (or data fence).\n+      Address thread_epoch_addr(rthread, in_bytes(bs_nm->thread_disarmed_guard_value_offset()) + 4);\n+      __ lea(rscratch2, ExternalAddress(bs_asm->patching_epoch_addr()));\n+      __ ldrw(rscratch2, rscratch2);\n+      __ strw(rscratch2, thread_epoch_addr);\n+      __ isb();\n+      __ membar(__ LoadLoad);\n+    }\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/stubGenerator_aarch64.cpp","additions":14,"deletions":0,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -37,0 +37,1 @@\n+  conc_instruction_and_data_patch,\n@@ -78,1 +79,2 @@\n-    return patching_type == NMethodPatchingType::stw_instruction_and_data_patch;\n+    return patching_type == NMethodPatchingType::conc_instruction_and_data_patch ||\n+            patching_type == NMethodPatchingType::stw_instruction_and_data_patch;\n","filename":"src\/hotspot\/cpu\/riscv\/gc\/shared\/barrierSetAssembler_riscv.hpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2460,0 +2460,11 @@\n+    BarrierSetAssembler* bs_asm = BarrierSet::barrier_set()->barrier_set_assembler();\n+\n+    if (bs_asm->nmethod_patching_type() == NMethodPatchingType::conc_instruction_and_data_patch) {\n+      BarrierSetNMethod* bs_nm = BarrierSet::barrier_set()->barrier_set_nmethod();\n+      Address thread_epoch_addr(xthread, in_bytes(bs_nm->thread_disarmed_guard_value_offset()) + 4);\n+      __ la(t1, ExternalAddress(bs_asm->patching_epoch_addr()));\n+      __ lwu(t1, t1);\n+      __ sw(t1, thread_epoch_addr);\n+      __ membar(__ LoadLoad);\n+    }\n+\n","filename":"src\/hotspot\/cpu\/riscv\/stubGenerator_riscv.cpp","additions":11,"deletions":0,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -159,1 +159,1 @@\n-#if defined(RISCV64) && !defined(ZERO)\n+#if (defined(AARCH64) || defined(RISCV64)) && !defined(ZERO)\n","filename":"src\/hotspot\/share\/gc\/shared\/barrierSetNMethod.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -59,0 +59,1 @@\n+    static address BarrierSetAssembler_patch_epoch_addr;\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompilerToVM.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -144,0 +144,1 @@\n+    AARCH64_ONLY(BarrierSetAssembler_patching_epoch_addr = bs_asm->patching_epoch_addr());\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompilerToVMInit.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"}]}