{"files":[{"patch":"@@ -1923,1 +1923,18 @@\n-    bs->nmethod_entry_barrier(&_masm);\n+    if (BarrierSet::barrier_set()->barrier_set_nmethod() != NULL) {\n+      \/\/ Dummy labels for just measuring the code size\n+      Label dummy_slow_path;\n+      Label dummy_continuation;\n+      Label dummy_guard;\n+      Label* slow_path = &dummy_slow_path;\n+      Label* continuation = &dummy_continuation;\n+      Label* guard = &dummy_guard;\n+      if (!Compile::current()->output()->in_scratch_emit_size()) {\n+        \/\/ Use real labels from actual stub when not emitting code for the purpose of measuring its size\n+        C2EntryBarrierStub* stub = Compile::current()->output()->entry_barrier_table()->add_entry_barrier();\n+        slow_path = &stub->slow_path();\n+        continuation = &stub->continuation();\n+        guard = &stub->guard();\n+      }\n+      \/\/ In the C2 code, we move the non-hot part of nmethod entry barriers out-of-line to a stub.\n+      bs->nmethod_entry_barrier(&_masm, slow_path, continuation, guard);\n+    }\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":18,"deletions":1,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -301,1 +301,1 @@\n-  bs->nmethod_entry_barrier(this);\n+  bs->nmethod_entry_barrier(this, NULL \/* slow_path *\/, NULL \/* continuation *\/, NULL \/* guard *\/);\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_MacroAssembler_aarch64.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"opto\/output.hpp\"\n@@ -46,0 +47,15 @@\n+void C2_MacroAssembler::emit_entry_barrier_stub(C2EntryBarrierStub* stub) {\n+  bind(stub->slow_path());\n+  movptr(rscratch1, (uintptr_t) StubRoutines::aarch64::method_entry_barrier());\n+  blr(rscratch1);\n+  b(stub->continuation());\n+\n+  bind(stub->guard());\n+  relocate(entry_guard_Relocation::spec());\n+  emit_int32(0);   \/\/ nmethod guard value\n+}\n+\n+int C2_MacroAssembler::entry_barrier_stub_size() {\n+  return 4 * 6;\n+}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/c2_MacroAssembler_aarch64.cpp","additions":16,"deletions":0,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -31,2 +31,2 @@\n-  void emit_entry_barrier_stub(C2EntryBarrierStub* stub) {}\n-  static int entry_barrier_stub_size() { return 0; }\n+  void emit_entry_barrier_stub(C2EntryBarrierStub* stub);\n+  static int entry_barrier_stub_size();\n","filename":"src\/hotspot\/cpu\/aarch64\/c2_MacroAssembler_aarch64.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -249,1 +249,1 @@\n-void BarrierSetAssembler::nmethod_entry_barrier(MacroAssembler* masm) {\n+void BarrierSetAssembler::nmethod_entry_barrier(MacroAssembler* masm, Label* slow_path, Label* continuation, Label* guard) {\n@@ -256,1 +256,3 @@\n-  Label skip_barrier, guard;\n+  Label local_guard;\n+  Label skip_barrier;\n+  NMethodPatchingType patching_type = nmethod_patching_type();\n@@ -258,1 +260,7 @@\n-  __ ldrw(rscratch1, guard);\n+  if (slow_path == NULL) {\n+    guard = &local_guard;\n+  }\n+\n+  \/\/ If the slow path is out of line in a stub, we flip the condition\n+  Assembler::Condition condition = slow_path == NULL ? Assembler::EQ : Assembler::NE;\n+  Label& barrier_target = slow_path == NULL ? skip_barrier : *slow_path;\n@@ -260,1 +268,13 @@\n-  if (nmethod_code_patching()) {\n+  __ ldrw(rscratch1, *guard);\n+\n+  if (patching_type == NMethodPatchingType::stw_instruction_and_data_patch) {\n+    \/\/ With STW patching, no data or instructions are updated concurrently,\n+    \/\/ which means there isn't really any need for any fencing for neither\n+    \/\/ data nor instruction modifications happening concurrently. The\n+    \/\/ instruction patching is handled with isb fences on the way back\n+    \/\/ from the safepoint to Java. So here we can do a plain conditional\n+    \/\/ branch with no fencing.\n+    Address thread_disarmed_addr(rthread, in_bytes(bs_nm->thread_disarmed_offset()));\n+    __ ldrw(rscratch2, thread_disarmed_addr);\n+    __ cmp(rscratch1, rscratch2);\n+  } else if (patching_type == NMethodPatchingType::conc_instruction_and_data_patch) {\n@@ -281,1 +301,0 @@\n-    __ br(Assembler::EQ, skip_barrier);\n@@ -283,0 +302,1 @@\n+    assert(patching_type == NMethodPatchingType::conc_data_patch, \"must be\");\n@@ -289,1 +309,0 @@\n-    __ br(Assembler::EQ, skip_barrier);\n@@ -291,0 +310,1 @@\n+  __ br(condition, barrier_target);\n@@ -292,3 +312,4 @@\n-  __ movptr(rscratch1, (uintptr_t) StubRoutines::aarch64::method_entry_barrier());\n-  __ blr(rscratch1);\n-  __ b(skip_barrier);\n+  if (slow_path == NULL) {\n+    __ movptr(rscratch1, (uintptr_t) StubRoutines::aarch64::method_entry_barrier());\n+    __ blr(rscratch1);\n+    __ b(skip_barrier);\n@@ -296,1 +317,1 @@\n-  __ bind(guard);\n+    __ bind(local_guard);\n@@ -298,1 +319,4 @@\n-  __ emit_int32(0);   \/\/ nmethod guard value. Skipped over in common case.\n+    __ emit_int32(0);   \/\/ nmethod guard value. Skipped over in common case.\n+  } else {\n+    __ bind(*continuation);\n+  }\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/shared\/barrierSetAssembler_aarch64.cpp","additions":35,"deletions":11,"binary":false,"changes":46,"status":"modified"},{"patch":"@@ -34,0 +34,6 @@\n+enum class NMethodPatchingType {\n+  stw_instruction_and_data_patch,\n+  conc_instruction_and_data_patch,\n+  conc_data_patch\n+};\n+\n@@ -71,1 +77,1 @@\n-  virtual bool nmethod_code_patching() { return true; }\n+  virtual NMethodPatchingType nmethod_patching_type() { return NMethodPatchingType::stw_instruction_and_data_patch; }\n@@ -73,1 +79,1 @@\n-  virtual void nmethod_entry_barrier(MacroAssembler* masm);\n+  virtual void nmethod_entry_barrier(MacroAssembler* masm, Label* slow_path, Label* continuation, Label* guard);\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/shared\/barrierSetAssembler_aarch64.hpp","additions":8,"deletions":2,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -40,0 +40,25 @@\n+static int slow_path_size(nmethod* nm) {\n+  \/\/ The slow path code is out of line with C2\n+  return nm->is_compiled_by_c2() ? 0 : 6;\n+}\n+\n+\/\/ This is the offset of the entry barrier from where the frame is completed.\n+\/\/ If any code changes between the end of the verified entry where the entry\n+\/\/ barrier resides, and the completion of the frame, then\n+\/\/ NativeNMethodCmpBarrier::verify() will immediately complain when it does\n+\/\/ not find the expected native instruction at this offset, which needs updating.\n+\/\/ Note that this offset is invariant of PreserveFramePointer.\n+static int entry_barrier_offset(nmethod* nm) {\n+  BarrierSetAssembler* bs_asm = BarrierSet::barrier_set()->barrier_set_assembler();\n+  switch (bs_asm->nmethod_patching_type()) {\n+  case NMethodPatchingType::stw_instruction_and_data_patch:\n+    return -4 * (4 + slow_path_size(nm));\n+  case NMethodPatchingType::conc_instruction_and_data_patch:\n+    return -4 * (10 + slow_path_size(nm));\n+  case NMethodPatchingType::conc_data_patch:\n+    return -4 * (5 + slow_path_size(nm));\n+  }\n+  ShouldNotReachHere();\n+  return 0;\n+}\n+\n@@ -43,7 +68,3 @@\n-  int guard_offset() {\n-    BarrierSetAssembler* bs_asm = BarrierSet::barrier_set()->barrier_set_assembler();\n-    if (bs_asm->nmethod_code_patching()) {\n-      return 4 * 15;\n-    } else {\n-      return 4 * 10;\n-    }\n+  int local_guard_offset(nmethod* nm) {\n+    \/\/ It's the last instruction\n+    return (-entry_barrier_offset(nm)) - 4;\n@@ -52,2 +73,14 @@\n-  int *guard_addr() {\n-    return reinterpret_cast<int*>(instruction_address() + guard_offset());\n+  int *guard_addr(nmethod* nm) {\n+    if (nm->is_compiled_by_c2()) {\n+      \/\/ With c2 compiled code, the guard is out-of-line in a stub\n+      \/\/ We find it using the RelocIterator.\n+      RelocIterator iter(nm);\n+      while (iter.next()) {\n+        if (iter.type() == relocInfo::entry_guard_type) {\n+          entry_guard_Relocation* const reloc = iter.entry_guard_reloc();\n+          return reinterpret_cast<int*>(reloc->addr());\n+        }\n+      }\n+      ShouldNotReachHere();\n+    }\n+    return reinterpret_cast<int*>(instruction_address() + local_guard_offset(nm));\n@@ -57,2 +90,2 @@\n-  int get_value() {\n-    return Atomic::load_acquire(guard_addr());\n+  int get_value(nmethod* nm) {\n+    return Atomic::load_acquire(guard_addr(nm));\n@@ -61,2 +94,2 @@\n-  void set_value(int value) {\n-    Atomic::release_store(guard_addr(), value);\n+  void set_value(nmethod* nm, int value) {\n+    Atomic::release_store(guard_addr(nm), value);\n@@ -123,16 +156,0 @@\n-\/\/ This is the offset of the entry barrier from where the frame is completed.\n-\/\/ If any code changes between the end of the verified entry where the entry\n-\/\/ barrier resides, and the completion of the frame, then\n-\/\/ NativeNMethodCmpBarrier::verify() will immediately complain when it does\n-\/\/ not find the expected native instruction at this offset, which needs updating.\n-\/\/ Note that this offset is invariant of PreserveFramePointer.\n-\n-static int entry_barrier_offset() {\n-  BarrierSetAssembler* bs_asm = BarrierSet::barrier_set()->barrier_set_assembler();\n-  if (bs_asm->nmethod_code_patching()) {\n-    return -4 * 16;\n-  } else {\n-    return -4 * 11;\n-  }\n-}\n-\n@@ -140,1 +157,1 @@\n-  address barrier_address = nm->code_begin() + nm->frame_complete_offset() + entry_barrier_offset();\n+  address barrier_address = nm->code_begin() + nm->frame_complete_offset() + entry_barrier_offset(nm);\n@@ -163,1 +180,1 @@\n-  barrier->set_value(disarmed_value());\n+  barrier->set_value(nm, disarmed_value());\n@@ -183,1 +200,1 @@\n-  barrier->set_value(arm_value);\n+  barrier->set_value(nm, arm_value);\n@@ -192,1 +209,1 @@\n-  return barrier->get_value() != disarmed_value();\n+  return barrier->get_value(nm) != disarmed_value();\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/shared\/barrierSetNMethod_aarch64.cpp","additions":50,"deletions":33,"binary":false,"changes":83,"status":"modified"},{"patch":"@@ -65,1 +65,1 @@\n-  virtual bool nmethod_code_patching() { return false; }\n+  virtual NMethodPatchingType nmethod_patching_type() { return NMethodPatchingType::conc_data_patch; }\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/shenandoah\/shenandoahBarrierSetAssembler_aarch64.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -79,1 +79,1 @@\n-  virtual bool nmethod_code_patching() { return false; }\n+  virtual NMethodPatchingType nmethod_patching_type() { return NMethodPatchingType::conc_data_patch; }\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/z\/zBarrierSetAssembler_aarch64.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -4478,1 +4478,1 @@\n-  if ((bs->barrier_set_nmethod() != NULL && !bs->barrier_set_assembler()->nmethod_code_patching()) || !immediate) {\n+  if ((bs->barrier_set_nmethod() != NULL && bs->barrier_set_assembler()->nmethod_patching_type() == NMethodPatchingType::conc_data_patch) || !immediate) {\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1427,1 +1427,1 @@\n-  bs->nmethod_entry_barrier(masm);\n+  bs->nmethod_entry_barrier(masm, NULL \/* slow_path *\/, NULL \/* continuation *\/, NULL \/* guard *\/);\n","filename":"src\/hotspot\/cpu\/aarch64\/sharedRuntime_aarch64.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -5148,1 +5148,1 @@\n-    address generate_method_entry_barrier() {\n+  address generate_method_entry_barrier() {\n@@ -5158,1 +5158,1 @@\n-    if (bs_asm->nmethod_code_patching()) {\n+    if (bs_asm->nmethod_patching_type() == NMethodPatchingType::conc_instruction_and_data_patch) {\n@@ -5161,1 +5161,1 @@\n-      \/\/ yet applied our cross modification fence.\n+      \/\/ yet applied our cross modification fence (or data fence).\n","filename":"src\/hotspot\/cpu\/aarch64\/stubGenerator_aarch64.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -272,0 +272,1 @@\n+    entry_guard_type        = 17, \/\/ A tag for an nmethod entry barrier guard value\n@@ -312,0 +313,1 @@\n+    visitor(entry_guard) \\\n@@ -886,0 +888,13 @@\n+class entry_guard_Relocation : public Relocation {\n+  friend class RelocIterator;\n+\n+public:\n+  entry_guard_Relocation() : Relocation(relocInfo::entry_guard_type) { }\n+\n+  static RelocationHolder spec() {\n+    RelocationHolder rh = newHolder();\n+    new(rh) entry_guard_Relocation();\n+    return rh;\n+  }\n+};\n+\n","filename":"src\/hotspot\/share\/code\/relocInfo.hpp","additions":15,"deletions":0,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -121,0 +121,1 @@\n+  Label _guard; \/\/ Used on AArch64\n@@ -125,1 +126,2 @@\n-    _continuation() {}\n+    _continuation(),\n+    _guard() {}\n@@ -129,0 +131,2 @@\n+  Label& guard() { return _guard; }\n+\n","filename":"src\/hotspot\/share\/opto\/output.hpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"}]}