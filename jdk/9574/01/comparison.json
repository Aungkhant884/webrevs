{"files":[{"patch":"@@ -1923,1 +1923,18 @@\n-    bs->nmethod_entry_barrier(&_masm);\n+    if (BarrierSet::barrier_set()->barrier_set_nmethod() != NULL) {\n+      \/\/ Dummy labels for just measuring the code size\n+      Label dummy_slow_path;\n+      Label dummy_continuation;\n+      Label dummy_guard;\n+      Label* slow_path = &dummy_slow_path;\n+      Label* continuation = &dummy_continuation;\n+      Label* guard = &dummy_guard;\n+      if (!Compile::current()->output()->in_scratch_emit_size()) {\n+        \/\/ Use real labels from actual stub when not emitting code for the purpose of measuring its size\n+        C2EntryBarrierStub* stub = Compile::current()->output()->entry_barrier_table()->add_entry_barrier();\n+        slow_path = &stub->slow_path();\n+        continuation = &stub->continuation();\n+        guard = &stub->guard();\n+      }\n+      \/\/ In the C2 code, we move the non-hot part of nmethod entry barriers out-of-line to a stub.\n+      bs->nmethod_entry_barrier(&_masm, slow_path, continuation, guard);\n+    }\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":18,"deletions":1,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -301,1 +301,1 @@\n-  bs->nmethod_entry_barrier(this);\n+  bs->nmethod_entry_barrier(this, NULL \/* slow_path *\/, NULL \/* continuation *\/, NULL \/* guard *\/);\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_MacroAssembler_aarch64.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"opto\/output.hpp\"\n@@ -46,0 +47,15 @@\n+void C2_MacroAssembler::emit_entry_barrier_stub(C2EntryBarrierStub* stub) {\n+  bind(stub->slow_path());\n+  movptr(rscratch1, (uintptr_t) StubRoutines::aarch64::method_entry_barrier());\n+  blr(rscratch1);\n+  b(stub->continuation());\n+\n+  bind(stub->guard());\n+  relocate(entry_guard_Relocation::spec());\n+  emit_int32(0);   \/\/ nmethod guard value\n+}\n+\n+int C2_MacroAssembler::entry_barrier_stub_size() {\n+  return 4 * 6;\n+}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/c2_MacroAssembler_aarch64.cpp","additions":16,"deletions":0,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -31,0 +31,2 @@\n+  void emit_entry_barrier_stub(C2EntryBarrierStub* stub);\n+  static int entry_barrier_stub_size();\n","filename":"src\/hotspot\/cpu\/aarch64\/c2_MacroAssembler_aarch64.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -249,1 +249,1 @@\n-void BarrierSetAssembler::nmethod_entry_barrier(MacroAssembler* masm) {\n+void BarrierSetAssembler::nmethod_entry_barrier(MacroAssembler* masm, Label* slow_path, Label* continuation, Label* guard) {\n@@ -256,1 +256,3 @@\n-  Label skip_barrier, guard;\n+  Label local_guard;\n+  Label skip_barrier;\n+  NMethodPatchingType patching_type = nmethod_patching_type();\n@@ -258,1 +260,7 @@\n-  __ ldrw(rscratch1, guard);\n+  if (slow_path == NULL) {\n+    guard = &local_guard;\n+  }\n+\n+  \/\/ If the slow path is out of line in a stub, we flip the condition\n+  Assembler::Condition condition = slow_path == NULL ? Assembler::EQ : Assembler::NE;\n+  Label& barrier_target = slow_path == NULL ? skip_barrier : *slow_path;\n@@ -260,1 +268,13 @@\n-  if (nmethod_code_patching()) {\n+  __ ldrw(rscratch1, *guard);\n+\n+  if (patching_type == NMethodPatchingType::stw_instruction_and_data_patch) {\n+    \/\/ With STW patching, no data or instructions are updated concurrently,\n+    \/\/ which means there isn't really any need for any fencing for neither\n+    \/\/ data nor instruction modifications happening concurrently. The\n+    \/\/ instruction patching is handled with isb fences on the way back\n+    \/\/ from the safepoint to Java. So here we can do a plain conditional\n+    \/\/ branch with no fencing.\n+    Address thread_disarmed_addr(rthread, in_bytes(bs_nm->thread_disarmed_offset()));\n+    __ ldrw(rscratch2, thread_disarmed_addr);\n+    __ cmp(rscratch1, rscratch2);\n+  } else if (patching_type == NMethodPatchingType::conc_instruction_and_data_patch) {\n@@ -281,1 +301,0 @@\n-    __ br(Assembler::EQ, skip_barrier);\n@@ -283,0 +302,1 @@\n+    assert(patching_type == NMethodPatchingType::conc_data_patch, \"must be\");\n@@ -289,1 +309,0 @@\n-    __ br(Assembler::EQ, skip_barrier);\n@@ -291,0 +310,1 @@\n+  __ br(condition, barrier_target);\n@@ -292,3 +312,4 @@\n-  __ movptr(rscratch1, (uintptr_t) StubRoutines::aarch64::method_entry_barrier());\n-  __ blr(rscratch1);\n-  __ b(skip_barrier);\n+  if (slow_path == NULL) {\n+    __ movptr(rscratch1, (uintptr_t) StubRoutines::aarch64::method_entry_barrier());\n+    __ blr(rscratch1);\n+    __ b(skip_barrier);\n@@ -296,1 +317,1 @@\n-  __ bind(guard);\n+    __ bind(local_guard);\n@@ -298,1 +319,4 @@\n-  __ emit_int32(0);   \/\/ nmethod guard value. Skipped over in common case.\n+    __ emit_int32(0);   \/\/ nmethod guard value. Skipped over in common case.\n+  } else {\n+    __ bind(*continuation);\n+  }\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/shared\/barrierSetAssembler_aarch64.cpp","additions":35,"deletions":11,"binary":false,"changes":46,"status":"modified"},{"patch":"@@ -34,0 +34,6 @@\n+enum class NMethodPatchingType {\n+  stw_instruction_and_data_patch,\n+  conc_instruction_and_data_patch,\n+  conc_data_patch\n+};\n+\n@@ -71,1 +77,1 @@\n-  virtual bool nmethod_code_patching() { return true; }\n+  virtual NMethodPatchingType nmethod_patching_type() { return NMethodPatchingType::stw_instruction_and_data_patch; }\n@@ -73,1 +79,1 @@\n-  virtual void nmethod_entry_barrier(MacroAssembler* masm);\n+  virtual void nmethod_entry_barrier(MacroAssembler* masm, Label* slow_path, Label* continuation, Label* guard);\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/shared\/barrierSetAssembler_aarch64.hpp","additions":8,"deletions":2,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -40,0 +40,25 @@\n+static int slow_path_size(nmethod* nm) {\n+  \/\/ The slow path code is out of line with C2\n+  return nm->is_compiled_by_c2() ? 0 : 6;\n+}\n+\n+\/\/ This is the offset of the entry barrier from where the frame is completed.\n+\/\/ If any code changes between the end of the verified entry where the entry\n+\/\/ barrier resides, and the completion of the frame, then\n+\/\/ NativeNMethodCmpBarrier::verify() will immediately complain when it does\n+\/\/ not find the expected native instruction at this offset, which needs updating.\n+\/\/ Note that this offset is invariant of PreserveFramePointer.\n+static int entry_barrier_offset(nmethod* nm) {\n+  BarrierSetAssembler* bs_asm = BarrierSet::barrier_set()->barrier_set_assembler();\n+  switch (bs_asm->nmethod_patching_type()) {\n+  case NMethodPatchingType::stw_instruction_and_data_patch:\n+    return -4 * (4 + slow_path_size(nm));\n+  case NMethodPatchingType::conc_instruction_and_data_patch:\n+    return -4 * (10 + slow_path_size(nm));\n+  case NMethodPatchingType::conc_data_patch:\n+    return -4 * (5 + slow_path_size(nm));\n+  }\n+  ShouldNotReachHere();\n+  return 0;\n+}\n+\n@@ -43,7 +68,3 @@\n-  int guard_offset() {\n-    BarrierSetAssembler* bs_asm = BarrierSet::barrier_set()->barrier_set_assembler();\n-    if (bs_asm->nmethod_code_patching()) {\n-      return 4 * 15;\n-    } else {\n-      return 4 * 10;\n-    }\n+  int local_guard_offset(nmethod* nm) {\n+    \/\/ It's the last instruction\n+    return (-entry_barrier_offset(nm)) - 4;\n@@ -52,2 +73,14 @@\n-  int *guard_addr() {\n-    return reinterpret_cast<int*>(instruction_address() + guard_offset());\n+  int *guard_addr(nmethod* nm) {\n+    if (nm->is_compiled_by_c2()) {\n+      \/\/ With c2 compiled code, the guard is out-of-line in a stub\n+      \/\/ We find it using the RelocIterator.\n+      RelocIterator iter(nm);\n+      while (iter.next()) {\n+        if (iter.type() == relocInfo::entry_guard_type) {\n+          entry_guard_Relocation* const reloc = iter.entry_guard_reloc();\n+          return reinterpret_cast<int*>(reloc->addr());\n+        }\n+      }\n+      ShouldNotReachHere();\n+    }\n+    return reinterpret_cast<int*>(instruction_address() + local_guard_offset(nm));\n@@ -57,2 +90,2 @@\n-  int get_value() {\n-    return Atomic::load_acquire(guard_addr());\n+  int get_value(nmethod* nm) {\n+    return Atomic::load_acquire(guard_addr(nm));\n@@ -61,2 +94,2 @@\n-  void set_value(int value) {\n-    Atomic::release_store(guard_addr(), value);\n+  void set_value(nmethod* nm, int value) {\n+    Atomic::release_store(guard_addr(nm), value);\n@@ -123,16 +156,0 @@\n-\/\/ This is the offset of the entry barrier from where the frame is completed.\n-\/\/ If any code changes between the end of the verified entry where the entry\n-\/\/ barrier resides, and the completion of the frame, then\n-\/\/ NativeNMethodCmpBarrier::verify() will immediately complain when it does\n-\/\/ not find the expected native instruction at this offset, which needs updating.\n-\/\/ Note that this offset is invariant of PreserveFramePointer.\n-\n-static int entry_barrier_offset() {\n-  BarrierSetAssembler* bs_asm = BarrierSet::barrier_set()->barrier_set_assembler();\n-  if (bs_asm->nmethod_code_patching()) {\n-    return -4 * 16;\n-  } else {\n-    return -4 * 11;\n-  }\n-}\n-\n@@ -140,1 +157,1 @@\n-  address barrier_address = nm->code_begin() + nm->frame_complete_offset() + entry_barrier_offset();\n+  address barrier_address = nm->code_begin() + nm->frame_complete_offset() + entry_barrier_offset(nm);\n@@ -163,1 +180,1 @@\n-  barrier->set_value(disarmed_value());\n+  barrier->set_value(nm, disarmed_value());\n@@ -183,1 +200,1 @@\n-  barrier->set_value(arm_value);\n+  barrier->set_value(nm, arm_value);\n@@ -192,1 +209,1 @@\n-  return barrier->get_value() != disarmed_value();\n+  return barrier->get_value(nm) != disarmed_value();\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/shared\/barrierSetNMethod_aarch64.cpp","additions":50,"deletions":33,"binary":false,"changes":83,"status":"modified"},{"patch":"@@ -65,1 +65,1 @@\n-  virtual bool nmethod_code_patching() { return false; }\n+  virtual NMethodPatchingType nmethod_patching_type() { return NMethodPatchingType::conc_data_patch; }\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/shenandoah\/shenandoahBarrierSetAssembler_aarch64.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -79,1 +79,1 @@\n-  virtual bool nmethod_code_patching() { return false; }\n+  virtual NMethodPatchingType nmethod_patching_type() { return NMethodPatchingType::conc_data_patch; }\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/z\/zBarrierSetAssembler_aarch64.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -4224,1 +4224,1 @@\n-  if ((bs->barrier_set_nmethod() != NULL && !bs->barrier_set_assembler()->nmethod_code_patching()) || !immediate) {\n+  if ((bs->barrier_set_nmethod() != NULL && bs->barrier_set_assembler()->nmethod_patching_type() == NMethodPatchingType::conc_data_patch) || !immediate) {\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1427,1 +1427,1 @@\n-  bs->nmethod_entry_barrier(masm);\n+  bs->nmethod_entry_barrier(masm, NULL \/* slow_path *\/, NULL \/* continuation *\/, NULL \/* guard *\/);\n","filename":"src\/hotspot\/cpu\/aarch64\/sharedRuntime_aarch64.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -5148,1 +5148,1 @@\n-    address generate_method_entry_barrier() {\n+  address generate_method_entry_barrier() {\n@@ -5158,1 +5158,1 @@\n-    if (bs_asm->nmethod_code_patching()) {\n+    if (bs_asm->nmethod_patching_type() == NMethodPatchingType::conc_instruction_and_data_patch) {\n@@ -5161,1 +5161,1 @@\n-      \/\/ yet applied our cross modification fence.\n+      \/\/ yet applied our cross modification fence (or data fence).\n","filename":"src\/hotspot\/cpu\/aarch64\/stubGenerator_aarch64.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -31,0 +31,3 @@\n+  void emit_entry_barrier_stub(C2EntryBarrierStub* stub) {}\n+  static int entry_barrier_stub_size() { return 0; }\n+\n","filename":"src\/hotspot\/cpu\/arm\/c2_MacroAssembler_arm.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -31,0 +31,3 @@\n+  void emit_entry_barrier_stub(C2EntryBarrierStub* stub) {}\n+  static int entry_barrier_stub_size() { return 0; }\n+\n","filename":"src\/hotspot\/cpu\/ppc\/c2_MacroAssembler_ppc.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -39,0 +39,2 @@\n+  void emit_entry_barrier_stub(C2EntryBarrierStub* stub) {}\n+  static int entry_barrier_stub_size() { return 0; }\n","filename":"src\/hotspot\/cpu\/riscv\/c2_MacroAssembler_riscv.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -32,0 +32,3 @@\n+  void emit_entry_barrier_stub(C2EntryBarrierStub* stub) {}\n+  static int entry_barrier_stub_size() { return 0; }\n+\n","filename":"src\/hotspot\/cpu\/s390\/c2_MacroAssembler_s390.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -328,1 +328,2 @@\n-  bs->nmethod_entry_barrier(this);\n+  \/\/ C1 code is not hot enough to micro optimize the nmethod entry barrier with an out-of-line stub\n+  bs->nmethod_entry_barrier(this, NULL \/* slow_path *\/, NULL \/* continuation *\/);\n","filename":"src\/hotspot\/cpu\/x86\/c1_MacroAssembler_x86.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"opto\/output.hpp\"\n@@ -131,1 +132,19 @@\n-    bs->nmethod_entry_barrier(this);\n+ #ifdef _LP64\n+    if (BarrierSet::barrier_set()->barrier_set_nmethod() != NULL) {\n+      \/\/ We put the non-hot code of the nmethod entry barrier out-of-line in a stub.\n+      Label dummy_slow_path;\n+      Label dummy_continuation;\n+      Label* slow_path = &dummy_slow_path;\n+      Label* continuation = &dummy_continuation;\n+      if (!Compile::current()->output()->in_scratch_emit_size()) {\n+        \/\/ Use real labels from actual stub when not emitting code for the purpose of measuring its size\n+        C2EntryBarrierStub* stub = Compile::current()->output()->entry_barrier_table()->add_entry_barrier();\n+        slow_path = &stub->slow_path();\n+        continuation = &stub->continuation();\n+      }\n+      bs->nmethod_entry_barrier(this, slow_path, continuation);\n+    }\n+#else\n+    \/\/ Don't bother with out-of-line nmethod entry barrier stub for x86_32.\n+    bs->nmethod_entry_barrier(this, NULL \/* slow_path *\/, NULL \/* continuation *\/);\n+#endif\n@@ -135,0 +154,10 @@\n+void C2_MacroAssembler::emit_entry_barrier_stub(C2EntryBarrierStub* stub) {\n+  bind(stub->slow_path());\n+  call(RuntimeAddress(StubRoutines::x86::method_entry_barrier()));\n+  jmp(stub->continuation(), false \/* maybe_short *\/);\n+}\n+\n+int C2_MacroAssembler::entry_barrier_stub_size() {\n+  return 10;\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":30,"deletions":1,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -34,0 +34,3 @@\n+  void emit_entry_barrier_stub(C2EntryBarrierStub* stub);\n+  static int entry_barrier_stub_size();\n+\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -312,1 +312,1 @@\n-void BarrierSetAssembler::nmethod_entry_barrier(MacroAssembler* masm) {\n+void BarrierSetAssembler::nmethod_entry_barrier(MacroAssembler* masm, Label* slow_path, Label* continuation) {\n@@ -317,1 +317,0 @@\n-  Label continuation;\n@@ -320,1 +319,5 @@\n-  __ align(8);\n+  \/\/ The immediate is the last 4 bytes, so if we align the start of the cmp\n+  \/\/ instruction to 4 bytes, we know that the second half of it is also 4\n+  \/\/ byte aligned, which means that the immediate will not cross a cache line\n+  __ align(4);\n+  uintptr_t before_cmp = (uintptr_t)__ pc();\n@@ -322,3 +325,12 @@\n-  __ jcc(Assembler::equal, continuation);\n-  __ call(RuntimeAddress(StubRoutines::x86::method_entry_barrier()));\n-  __ bind(continuation);\n+  uintptr_t after_cmp = (uintptr_t)__ pc();\n+  guarantee(after_cmp - before_cmp == 8, \"Wrong assumed instruction length\");\n+\n+  if (slow_path != NULL) {\n+    __ jcc(Assembler::notEqual, *slow_path);\n+    __ bind(*continuation);\n+  } else {\n+    Label done;\n+    __ jccb(Assembler::equal, done);\n+    __ call(RuntimeAddress(StubRoutines::x86::method_entry_barrier()));\n+    __ bind(done);\n+  }\n@@ -327,1 +339,1 @@\n-void BarrierSetAssembler::nmethod_entry_barrier(MacroAssembler* masm) {\n+void BarrierSetAssembler::nmethod_entry_barrier(MacroAssembler* masm, Label*, Label*) {\n","filename":"src\/hotspot\/cpu\/x86\/gc\/shared\/barrierSetAssembler_x86.cpp","additions":19,"deletions":7,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -71,1 +71,1 @@\n-  virtual void nmethod_entry_barrier(MacroAssembler* masm);\n+  virtual void nmethod_entry_barrier(MacroAssembler* masm, Label* slow_path, Label* continuation);\n","filename":"src\/hotspot\/cpu\/x86\/gc\/shared\/barrierSetAssembler_x86.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"utilities\/macros.hpp\"\n@@ -65,1 +66,1 @@\n-  if (((uintptr_t) instruction_address()) & 0x7) {\n+  if (((uintptr_t) instruction_address()) & 0x3) {\n@@ -159,1 +160,11 @@\n-static const int entry_barrier_offset = LP64_ONLY(-19) NOT_LP64(-18);\n+static const int entry_barrier_offset(nmethod* nm) {\n+#ifdef _LP64\n+  if (nm->is_compiled_by_c2()) {\n+    return -14;\n+  } else {\n+    return -15;\n+  }\n+#else\n+  return -18;\n+#endif\n+}\n@@ -162,1 +173,1 @@\n-  address barrier_address = nm->code_begin() + nm->frame_complete_offset() + entry_barrier_offset;\n+  address barrier_address = nm->code_begin() + nm->frame_complete_offset() + entry_barrier_offset(nm);\n","filename":"src\/hotspot\/cpu\/x86\/gc\/shared\/barrierSetNMethod_x86.cpp","additions":14,"deletions":3,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -1521,1 +1521,1 @@\n-  bs->nmethod_entry_barrier(masm);\n+  bs->nmethod_entry_barrier(masm, NULL \/* slow_path *\/, NULL \/* continuation *\/);\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_32.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1747,1 +1747,2 @@\n-  bs->nmethod_entry_barrier(masm);\n+  \/\/ native wrapper is not hot enough to micro optimize the nmethod entry barrier with an out-of-line stub\n+  bs->nmethod_entry_barrier(masm, NULL \/* slow_path *\/, NULL \/* continuation *\/);\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_64.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -272,0 +272,1 @@\n+    entry_guard_type        = 17, \/\/ A tag for an nmethod entry barrier guard value\n@@ -312,0 +313,1 @@\n+    visitor(entry_guard) \\\n@@ -886,0 +888,13 @@\n+class entry_guard_Relocation : public Relocation {\n+  friend class RelocIterator;\n+\n+public:\n+  entry_guard_Relocation() : Relocation(relocInfo::entry_guard_type) { }\n+\n+  static RelocationHolder spec() {\n+    RelocationHolder rh = newHolder();\n+    new(rh) entry_guard_Relocation();\n+    return rh;\n+  }\n+};\n+\n","filename":"src\/hotspot\/share\/code\/relocInfo.hpp","additions":15,"deletions":0,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -32,0 +32,2 @@\n+class C2EntryBarrierStub;\n+\n","filename":"src\/hotspot\/share\/opto\/c2_MacroAssembler.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -42,0 +42,1 @@\n+#include \"opto\/c2_MacroAssembler.hpp\"\n@@ -287,0 +288,37 @@\n+\/\/ Nmethod entry barrier stubs\n+C2EntryBarrierStub* C2EntryBarrierStubTable::add_entry_barrier() {\n+  assert(_stub == NULL, \"There can only be one entry barrier stub\");\n+  _stub = new (Compile::current()->comp_arena()) C2EntryBarrierStub();\n+  return _stub;\n+}\n+\n+void C2EntryBarrierStubTable::emit(CodeBuffer& cb) {\n+  if (_stub == NULL) {\n+    \/\/ No stub - nothing to do\n+    return;\n+  }\n+\n+  C2_MacroAssembler masm(&cb);\n+  \/\/ Make sure there is enough space in the code buffer\n+  if (cb.insts()->maybe_expand_to_ensure_remaining(PhaseOutput::MAX_inst_size) && cb.blob() == NULL) {\n+    ciEnv::current()->record_failure(\"CodeCache is full\");\n+    return;\n+  }\n+\n+  intptr_t before = masm.offset();\n+  masm.emit_entry_barrier_stub(_stub);\n+  intptr_t after = masm.offset();\n+  int actual_size = (int)(after - before);\n+  int expected_size = masm.entry_barrier_stub_size();\n+  assert(actual_size == expected_size, \"Estimated size is wrong, expected %d, was %d\", expected_size, actual_size);\n+}\n+\n+int C2EntryBarrierStubTable::estimate_stub_size() const {\n+  if (BarrierSet::barrier_set()->barrier_set_nmethod() == NULL) {\n+    \/\/ No nmethod entry barrier?\n+    return 0;\n+  }\n+\n+  return C2_MacroAssembler::entry_barrier_stub_size();\n+}\n+\n@@ -293,0 +331,2 @@\n+    _safepoint_poll_table(),\n+    _entry_barrier_table(),\n@@ -1305,0 +1345,1 @@\n+  stub_req += entry_barrier_table()->estimate_stub_size();\n@@ -1815,0 +1856,4 @@\n+  \/\/ Fill in stubs for calling the runtime from nmethod entries.\n+  entry_barrier_table()->emit(*cb);\n+  if (C->failing())  return;\n+\n","filename":"src\/hotspot\/share\/opto\/output.cpp","additions":45,"deletions":0,"binary":false,"changes":45,"status":"modified"},{"patch":"@@ -43,0 +43,1 @@\n+class C2_MacroAssembler;\n@@ -116,0 +117,27 @@\n+\/\/ We move non-hot code of the nmethod entry barrier to an out-of-line stub\n+class C2EntryBarrierStub: public ResourceObj {\n+  Label _slow_path;\n+  Label _continuation;\n+  Label _guard; \/\/ Used on AArch64\n+\n+public:\n+  C2EntryBarrierStub() :\n+    _slow_path(),\n+    _continuation(),\n+    _guard() {}\n+\n+  Label& slow_path() { return _slow_path; }\n+  Label& continuation() { return _continuation; }\n+  Label& guard() { return _guard; }\n+};\n+\n+class C2EntryBarrierStubTable {\n+  C2EntryBarrierStub* _stub;\n+\n+public:\n+  C2EntryBarrierStubTable() : _stub(NULL) {}\n+  C2EntryBarrierStub* add_entry_barrier();\n+  int estimate_stub_size() const;\n+  void emit(CodeBuffer& cb);\n+};\n+\n@@ -125,0 +153,1 @@\n+  C2EntryBarrierStubTable _entry_barrier_table;  \/\/ Table for entry barrier stubs\n@@ -175,0 +204,3 @@\n+  \/\/ Entry barrier table\n+  C2EntryBarrierStubTable* entry_barrier_table() { return &_entry_barrier_table; }\n+\n","filename":"src\/hotspot\/share\/opto\/output.hpp","additions":32,"deletions":0,"binary":false,"changes":32,"status":"modified"}]}