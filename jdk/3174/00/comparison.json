{"files":[{"patch":"@@ -1275,1 +1275,1 @@\n-  AllocateArrayNode* alloc = tightly_coupled_allocation(dst, NULL);\n+  AllocateArrayNode* alloc = tightly_coupled_allocation(dst);\n@@ -1392,1 +1392,2 @@\n-    AllocateArrayNode* alloc = tightly_coupled_allocation(newcopy, NULL);\n+    AllocateArrayNode* alloc = tightly_coupled_allocation(newcopy);\n+    guarantee(alloc != NULL, \"created above\");\n@@ -1410,19 +1411,7 @@\n-    if (alloc != NULL) {\n-      if (alloc->maybe_set_complete(&_gvn)) {\n-        \/\/ \"You break it, you buy it.\"\n-        InitializeNode* init = alloc->initialization();\n-        assert(init->is_complete(), \"we just did this\");\n-        init->set_complete_with_arraycopy();\n-        assert(newcopy->is_CheckCastPP(), \"sanity\");\n-        assert(newcopy->in(0)->in(0) == init, \"dest pinned\");\n-      }\n-      \/\/ Do not let stores that initialize this object be reordered with\n-      \/\/ a subsequent store that would make this object accessible by\n-      \/\/ other threads.\n-      \/\/ Record what AllocateNode this StoreStore protects so that\n-      \/\/ escape analysis can go from the MemBarStoreStoreNode to the\n-      \/\/ AllocateNode and eliminate the MemBarStoreStoreNode if possible\n-      \/\/ based on the escape status of the AllocateNode.\n-      insert_mem_bar(Op_MemBarStoreStore, alloc->proj_out_or_null(AllocateNode::RawAddress));\n-    } else {\n-      insert_mem_bar(Op_MemBarCPUOrder);\n+    if (alloc->maybe_set_complete(&_gvn)) {\n+      \/\/ \"You break it, you buy it.\"\n+      InitializeNode* init = alloc->initialization();\n+      assert(init->is_complete(), \"we just did this\");\n+      init->set_complete_with_arraycopy();\n+      assert(newcopy->is_CheckCastPP(), \"sanity\");\n+      assert(newcopy->in(0)->in(0) == init, \"dest pinned\");\n@@ -1430,0 +1419,8 @@\n+    \/\/ Do not let stores that initialize this object be reordered with\n+    \/\/ a subsequent store that would make this object accessible by\n+    \/\/ other threads.\n+    \/\/ Record what AllocateNode this StoreStore protects so that\n+    \/\/ escape analysis can go from the MemBarStoreStoreNode to the\n+    \/\/ AllocateNode and eliminate the MemBarStoreStoreNode if possible\n+    \/\/ based on the escape status of the AllocateNode.\n+    insert_mem_bar(Op_MemBarStoreStore, alloc->proj_out_or_null(AllocateNode::RawAddress));\n@@ -1457,1 +1454,1 @@\n-  AllocateArrayNode* alloc = tightly_coupled_allocation(dst, NULL);\n+  AllocateArrayNode* alloc = tightly_coupled_allocation(dst);\n@@ -4398,1 +4395,1 @@\n-  AllocateArrayNode* alloc = tightly_coupled_allocation(dest, NULL);\n+  AllocateArrayNode* alloc = tightly_coupled_allocation(dest);\n@@ -4434,1 +4431,1 @@\n-    alloc = tightly_coupled_allocation(dest, NULL);\n+    alloc = tightly_coupled_allocation(dest);\n@@ -4647,2 +4644,1 @@\n-LibraryCallKit::tightly_coupled_allocation(Node* ptr,\n-                                           RegionNode* slow_region) {\n+LibraryCallKit::tightly_coupled_allocation(Node* ptr) {\n@@ -4686,4 +4682,0 @@\n-      if (slow_region != NULL && slow_region->find_edge(not_ctl) >= 1) {\n-        ctl = iff->in(0);       \/\/ This test feeds the known slow_region.\n-        continue;\n-      }\n","filename":"src\/hotspot\/share\/opto\/library_call.cpp","additions":22,"deletions":30,"binary":false,"changes":52,"status":"modified"},{"patch":"@@ -254,2 +254,1 @@\n-  AllocateArrayNode* tightly_coupled_allocation(Node* ptr,\n-                                                RegionNode* slow_region);\n+  AllocateArrayNode* tightly_coupled_allocation(Node* ptr);\n","filename":"src\/hotspot\/share\/opto\/library_call.hpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"}]}