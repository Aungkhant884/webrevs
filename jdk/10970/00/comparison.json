{"files":[{"patch":"@@ -42,26 +42,22 @@\n-bool ArchiveHeapLoader::_closed_regions_mapped = false;\n-bool ArchiveHeapLoader::_open_regions_mapped = false;\n-bool ArchiveHeapLoader::_is_loaded = false;\n-address ArchiveHeapLoader::_narrow_oop_base;\n-int     ArchiveHeapLoader::_narrow_oop_shift;\n-\n-\/\/ Support for loaded heap.\n-uintptr_t ArchiveHeapLoader::_loaded_heap_bottom = 0;\n-uintptr_t ArchiveHeapLoader::_loaded_heap_top = 0;\n-uintptr_t ArchiveHeapLoader::_dumptime_base_0 = UINTPTR_MAX;\n-uintptr_t ArchiveHeapLoader::_dumptime_base_1 = UINTPTR_MAX;\n-uintptr_t ArchiveHeapLoader::_dumptime_base_2 = UINTPTR_MAX;\n-uintptr_t ArchiveHeapLoader::_dumptime_base_3 = UINTPTR_MAX;\n-uintptr_t ArchiveHeapLoader::_dumptime_top    = 0;\n-intx ArchiveHeapLoader::_runtime_offset_0 = 0;\n-intx ArchiveHeapLoader::_runtime_offset_1 = 0;\n-intx ArchiveHeapLoader::_runtime_offset_2 = 0;\n-intx ArchiveHeapLoader::_runtime_offset_3 = 0;\n-bool ArchiveHeapLoader::_loading_failed = false;\n-\n-\/\/ Support for mapped heap (!UseCompressedOops only)\n-ptrdiff_t ArchiveHeapLoader::_runtime_delta = 0;\n-\n-void ArchiveHeapLoader::init_narrow_oop_decoding(address base, int shift) {\n-  _narrow_oop_base = base;\n-  _narrow_oop_shift = shift;\n+bool ArchiveHeapLoader::_heap_pointers_need_patching = false;\n+ArchiveHeapRegions ArchiveHeapLoader::_closed_heap_regions;\n+ArchiveHeapRegions ArchiveHeapLoader::_open_heap_regions;\n+ArchiveOopDecoder* ArchiveHeapLoader::_oop_decoder = NULL;\n+\n+void ArchiveHeapLoader::init_archive_heap_regions(FileMapInfo* map_info, int first_region_idx, int last_region_idx, ArchiveHeapRegions* heap_regions) {\n+  heap_regions->init(last_region_idx - first_region_idx + 1);\n+  int count = 0;\n+\n+  for (int i = first_region_idx; i <= last_region_idx; i++) {\n+    FileMapRegion* si = map_info->space_at(i);\n+    si->assert_is_heap_region();\n+    size_t size = si->used();\n+    if (size > 0) {\n+      HeapWord* start = (HeapWord*)map_info->start_address_at_dumptime(si);\n+      heap_regions->set_dumptime_region(count, MemRegion(start, size \/ HeapWordSize));\n+      heap_regions->set_region_index(count, i);\n+      count += 1;\n+    }\n+  }\n+  heap_regions->set_num_regions(count);\n+  return;\n@@ -70,6 +66,12 @@\n-void ArchiveHeapLoader::fixup_regions() {\n-  FileMapInfo* mapinfo = FileMapInfo::current_info();\n-  if (is_mapped()) {\n-    mapinfo->fixup_mapped_heap_regions();\n-  } else if (_loading_failed) {\n-    fill_failed_loaded_heap();\n+void ArchiveHeapLoader::cleanup_regions(FileMapInfo* map_info, ArchiveHeapRegions* heap_regions) {\n+  if (heap_regions->is_mapped()) {\n+    \/\/ unmap the regions ...\n+    for (int i = 0; i < heap_regions->num_regions(); i++) {\n+      int region_idx = heap_regions->region_index(i);\n+      assert(region_idx >= MetaspaceShared::first_archive_heap_region\n+             && region_idx <= MetaspaceShared::last_archive_heap_region,\n+             \"invalid index\");\n+      map_info->unmap_region(region_idx);\n+    }\n+    \/\/ .. and now change state to HEAP_RESERVED.\n+    heap_regions->set_state(ArchiveHeapRegions::HEAP_RESERVED);\n@@ -77,4 +79,7 @@\n-  if (is_fully_available()) {\n-    if (!MetaspaceShared::use_full_module_graph()) {\n-      \/\/ Need to remove all the archived java.lang.Module objects from HeapShared::roots().\n-      ClassLoaderDataShared::clear_archived_oops();\n+  if (heap_regions->is_runtime_space_reserved()) {\n+    if (dealloc_heap_regions(heap_regions)) {\n+      heap_regions->set_state(ArchiveHeapRegions::MAPPING_FAILED_DEALLOCATED);\n+    } else {\n+      \/\/ if we fail to dealloc, the regions will be filled up with dummy objects\n+      \/\/ later in ArchiveHeapLoader::fixup_regions to make them parseable\n+      heap_regions->set_state(ArchiveHeapRegions::MAPPING_FAILED);\n@@ -85,25 +90,4 @@\n-\/\/ ------------------ Support for Region MAPPING -----------------------------------------\n-\n-\/\/ Patch all the embedded oop pointers inside an archived heap region,\n-\/\/ to be consistent with the runtime oop encoding.\n-class PatchCompressedEmbeddedPointers: public BitMapClosure {\n-  narrowOop* _start;\n-\n- public:\n-  PatchCompressedEmbeddedPointers(narrowOop* start) : _start(start) {}\n-\n-  bool do_bit(size_t offset) {\n-    narrowOop* p = _start + offset;\n-    narrowOop v = *p;\n-    assert(!CompressedOops::is_null(v), \"null oops should have been filtered out at dump time\");\n-    oop o = ArchiveHeapLoader::decode_from_archive(v);\n-    RawAccess<IS_NOT_NULL>::oop_store(p, o);\n-    return true;\n-  }\n-};\n-\n-class PatchUncompressedEmbeddedPointers: public BitMapClosure {\n-  oop* _start;\n-\n- public:\n-  PatchUncompressedEmbeddedPointers(oop* start) : _start(start) {}\n+void ArchiveHeapLoader::cleanup(FileMapInfo* map_info) {\n+  cleanup_regions(map_info, &_closed_heap_regions);\n+  cleanup_regions(map_info, &_open_heap_regions);\n+}\n@@ -111,6 +95,6 @@\n-  bool do_bit(size_t offset) {\n-    oop* p = _start + offset;\n-    intptr_t dumptime_oop = (intptr_t)((void*)*p);\n-    assert(dumptime_oop != 0, \"null oops should have been filtered out at dump time\");\n-    intptr_t runtime_oop = dumptime_oop + ArchiveHeapLoader::runtime_delta();\n-    RawAccess<IS_NOT_NULL>::oop_store(p, cast_to_oop(runtime_oop));\n+bool ArchiveHeapLoader::get_heap_range_for_archive_regions(ArchiveHeapRegions* heap_regions, bool is_open) {\n+  if (Universe::heap()->alloc_archive_regions(heap_regions->dumptime_regions(),\n+                                           heap_regions->num_regions(),\n+                                           heap_regions->runtime_regions(),\n+                                           is_open)) {\n+    heap_regions->set_state(ArchiveHeapRegions::HEAP_RESERVED);\n@@ -119,21 +103,1 @@\n-};\n-\n-\/\/ Patch all the non-null pointers that are embedded in the archived heap objects\n-\/\/ in this (mapped) region\n-void ArchiveHeapLoader::patch_embedded_pointers(MemRegion region, address oopmap,\n-                                                size_t oopmap_size_in_bits) {\n-  BitMapView bm((BitMap::bm_word_t*)oopmap, oopmap_size_in_bits);\n-\n-#ifndef PRODUCT\n-  ResourceMark rm;\n-  ResourceBitMap checkBm = HeapShared::calculate_oopmap(region);\n-  assert(bm.is_same(checkBm), \"sanity\");\n-#endif\n-\n-  if (UseCompressedOops) {\n-    PatchCompressedEmbeddedPointers patcher((narrowOop*)region.start());\n-    bm.iterate(&patcher);\n-  } else {\n-    PatchUncompressedEmbeddedPointers patcher((oop*)region.start());\n-    bm.iterate(&patcher);\n-  }\n+  return false;\n@@ -142,23 +106,4 @@\n-\/\/ ------------------ Support for Region LOADING -----------------------------------------\n-\n-\/\/ The CDS archive remembers each heap object by its address at dump time, but\n-\/\/ the heap object may be loaded at a different address at run time. This structure is used\n-\/\/ to translate the dump time addresses for all objects in FileMapInfo::space_at(region_index)\n-\/\/ to their runtime addresses.\n-struct LoadedArchiveHeapRegion {\n-  int       _region_index;   \/\/ index for FileMapInfo::space_at(index)\n-  size_t    _region_size;    \/\/ number of bytes in this region\n-  uintptr_t _dumptime_base;  \/\/ The dump-time (decoded) address of the first object in this region\n-  intx      _runtime_offset; \/\/ If an object's dump time address P is within in this region, its\n-                             \/\/ runtime address is P + _runtime_offset\n-\n-  static int comparator(const void* a, const void* b) {\n-    LoadedArchiveHeapRegion* reg_a = (LoadedArchiveHeapRegion*)a;\n-    LoadedArchiveHeapRegion* reg_b = (LoadedArchiveHeapRegion*)b;\n-    if (reg_a->_dumptime_base < reg_b->_dumptime_base) {\n-      return -1;\n-    } else if (reg_a->_dumptime_base == reg_b->_dumptime_base) {\n-      return 0;\n-    } else {\n-      return 1;\n-    }\n+bool ArchiveHeapLoader::is_pointer_patching_needed(FileMapInfo* map_info) {\n+  if (!_closed_heap_regions.is_mapped()) {\n+    assert(!_open_heap_regions.is_mapped(), \"open heap regions must not be mapped when closed heap regions are not mapped\");\n+    return false;\n@@ -166,3 +111,3 @@\n-\n-  uintptr_t top() {\n-    return _dumptime_base + _region_size;\n+  if (_closed_heap_regions.is_relocated()) {\n+    log_info(cds)(\"CDS heap data needs to be relocated.\");\n+    return true;\n@@ -170,18 +115,4 @@\n-};\n-\n-void ArchiveHeapLoader::init_loaded_heap_relocation(LoadedArchiveHeapRegion* loaded_regions,\n-                                                    int num_loaded_regions) {\n-  _dumptime_base_0 = loaded_regions[0]._dumptime_base;\n-  _dumptime_base_1 = loaded_regions[1]._dumptime_base;\n-  _dumptime_base_2 = loaded_regions[2]._dumptime_base;\n-  _dumptime_base_3 = loaded_regions[3]._dumptime_base;\n-  _dumptime_top = loaded_regions[num_loaded_regions-1].top();\n-\n-  _runtime_offset_0 = loaded_regions[0]._runtime_offset;\n-  _runtime_offset_1 = loaded_regions[1]._runtime_offset;\n-  _runtime_offset_2 = loaded_regions[2]._runtime_offset;\n-  _runtime_offset_3 = loaded_regions[3]._runtime_offset;\n-\n-  assert(2 <= num_loaded_regions && num_loaded_regions <= 4, \"must be\");\n-  if (num_loaded_regions < 4) {\n-    _dumptime_base_3 = UINTPTR_MAX;\n+  assert(_open_heap_regions.is_mapped(), \"open heap regions must be mapped\");\n+  if (_open_heap_regions.is_relocated()) {\n+    log_info(cds)(\"CDS heap data needs to be relocated.\");\n+    return true;\n@@ -189,2 +120,5 @@\n-  if (num_loaded_regions < 3) {\n-    _dumptime_base_2 = UINTPTR_MAX;\n+  if (map_info->narrow_oop_mode() != CompressedOops::mode() ||\n+      map_info->narrow_oop_base() != CompressedOops::base() ||\n+      map_info->narrow_oop_shift() != CompressedOops::shift()) {\n+    log_info(cds)(\"CDS heap data needs to be relocated because the archive was created with an incompatible oop encoding mode.\");\n+    return true;\n@@ -192,0 +126,1 @@\n+  return false;\n@@ -194,2 +129,11 @@\n-bool ArchiveHeapLoader::can_load() {\n-  return Universe::heap()->can_load_archived_objects();\n+void ArchiveHeapLoader::log_mapped_regions(ArchiveHeapRegions* heap_regions, bool is_open) {\n+  if (is_open) {\n+    log_info(cds)(\"open heap regions:\");\n+  } else {\n+    log_info(cds)(\"closed heap regions:\");\n+  }\n+  for (int i = 0; i < heap_regions->num_regions(); i++) {\n+    log_info(cds)(\"dumptime region: [\" PTR_FORMAT \" - \" PTR_FORMAT \"] mapped to [\" PTR_FORMAT \" - \" PTR_FORMAT \"]\",\n+                   p2i(heap_regions->dumptime_region(i).start()), p2i(heap_regions->dumptime_region(i).end()),\n+                   p2i(heap_regions->runtime_region(i).start()), p2i(heap_regions->runtime_region(i).end()));\n+  }\n@@ -198,29 +142,7 @@\n-template <int NUM_LOADED_REGIONS>\n-class PatchLoadedRegionPointers: public BitMapClosure {\n-  narrowOop* _start;\n-  intx _offset_0;\n-  intx _offset_1;\n-  intx _offset_2;\n-  intx _offset_3;\n-  uintptr_t _base_0;\n-  uintptr_t _base_1;\n-  uintptr_t _base_2;\n-  uintptr_t _base_3;\n-  uintptr_t _top;\n-\n-  static_assert(MetaspaceShared::max_num_heap_regions == 4, \"can't handle more than 4 regions\");\n-  static_assert(NUM_LOADED_REGIONS >= 2, \"we have at least 2 loaded regions\");\n-  static_assert(NUM_LOADED_REGIONS <= 4, \"we have at most 4 loaded regions\");\n-\n- public:\n-  PatchLoadedRegionPointers(narrowOop* start, LoadedArchiveHeapRegion* loaded_regions)\n-    : _start(start),\n-      _offset_0(loaded_regions[0]._runtime_offset),\n-      _offset_1(loaded_regions[1]._runtime_offset),\n-      _offset_2(loaded_regions[2]._runtime_offset),\n-      _offset_3(loaded_regions[3]._runtime_offset),\n-      _base_0(loaded_regions[0]._dumptime_base),\n-      _base_1(loaded_regions[1]._dumptime_base),\n-      _base_2(loaded_regions[2]._dumptime_base),\n-      _base_3(loaded_regions[3]._dumptime_base) {\n-    _top = loaded_regions[NUM_LOADED_REGIONS-1].top();\n+void ArchiveHeapLoader::map_heap_regions(FileMapInfo* map_info) {\n+  init_archive_heap_regions(map_info, MetaspaceShared::first_closed_heap_region,\n+                            MetaspaceShared::last_closed_heap_region, &_closed_heap_regions);\n+  if (!get_heap_range_for_archive_regions(&_closed_heap_regions, false)) {\n+    log_info(cds)(\"Failed to find free regions in the heap for closed heap archive space\");\n+    cleanup(map_info);\n+    return;\n@@ -229,21 +151,6 @@\n-  bool do_bit(size_t offset) {\n-    narrowOop* p = _start + offset;\n-    narrowOop v = *p;\n-    assert(!CompressedOops::is_null(v), \"null oops should have been filtered out at dump time\");\n-    uintptr_t o = cast_from_oop<uintptr_t>(ArchiveHeapLoader::decode_from_archive(v));\n-    assert(_base_0 <= o && o < _top, \"must be\");\n-\n-\n-    \/\/ We usually have only 2 regions for the default archive. Use template to avoid unnecessary comparisons.\n-    if (NUM_LOADED_REGIONS > 3 && o >= _base_3) {\n-      o += _offset_3;\n-    } else if (NUM_LOADED_REGIONS > 2 && o >= _base_2) {\n-      o += _offset_2;\n-    } else if (o >= _base_1) {\n-      o += _offset_1;\n-    } else {\n-      o += _offset_0;\n-    }\n-    ArchiveHeapLoader::assert_in_loaded_heap(o);\n-    RawAccess<IS_NOT_NULL>::oop_store(p, cast_to_oop(o));\n-    return true;\n+  init_archive_heap_regions(map_info, MetaspaceShared::first_open_heap_region,\n+                            MetaspaceShared::last_open_heap_region, &_open_heap_regions);\n+  if (!get_heap_range_for_archive_regions(&_open_heap_regions, true)) {\n+    log_info(cds)(\"Failed to find free regions for in the heap for open heap archive space\");\n+    cleanup(map_info);\n+    return;\n@@ -251,1 +158,0 @@\n-};\n@@ -253,16 +159,5 @@\n-int ArchiveHeapLoader::init_loaded_regions(FileMapInfo* mapinfo, LoadedArchiveHeapRegion* loaded_regions,\n-                                           MemRegion& archive_space) {\n-  size_t total_bytes = 0;\n-  int num_loaded_regions = 0;\n-  for (int i = MetaspaceShared::first_archive_heap_region;\n-       i <= MetaspaceShared::last_archive_heap_region; i++) {\n-    FileMapRegion* r = mapinfo->space_at(i);\n-    r->assert_is_heap_region();\n-    if (r->used() > 0) {\n-      assert(is_aligned(r->used(), HeapWordSize), \"must be\");\n-      total_bytes += r->used();\n-      LoadedArchiveHeapRegion* ri = &loaded_regions[num_loaded_regions++];\n-      ri->_region_index = i;\n-      ri->_region_size = r->used();\n-      ri->_dumptime_base = (uintptr_t)mapinfo->start_address_as_decoded_from_archive(r);\n-    }\n+  char* bitmap_base = map_info->map_bitmap_region();\n+  if (bitmap_base == NULL) {\n+    log_info(cds)(\"CDS heap cannot be used because bitmap region cannot be mapped\");\n+    cleanup(map_info);\n+    return;\n@@ -271,5 +166,7 @@\n-  assert(is_aligned(total_bytes, HeapWordSize), \"must be\");\n-  size_t word_size = total_bytes \/ HeapWordSize;\n-  HeapWord* buffer = Universe::heap()->allocate_loaded_archive_space(word_size);\n-  if (buffer == nullptr) {\n-    return 0;\n+  \/\/ Map the heap regions\n+  \/\/ closed regions: GC does not write into these regions.\n+  \/\/ open regions: GC can write into these regions.\n+  if (!map_heap_regions(map_info, &_closed_heap_regions) ||\n+      !map_heap_regions(map_info, &_open_heap_regions)) {\n+    cleanup(map_info);\n+    return;\n@@ -278,6 +175,1 @@\n-  archive_space = MemRegion(buffer, word_size);\n-  _loaded_heap_bottom = (uintptr_t)archive_space.start();\n-  _loaded_heap_top    = _loaded_heap_bottom + total_bytes;\n-\n-  return num_loaded_regions;\n-}\n+  _heap_pointers_need_patching = is_pointer_patching_needed(map_info);\n@@ -285,13 +177,5 @@\n-void ArchiveHeapLoader::sort_loaded_regions(LoadedArchiveHeapRegion* loaded_regions, int num_loaded_regions,\n-                                            uintptr_t buffer) {\n-  \/\/ Find the relocation offset of the pointers in each region\n-  qsort(loaded_regions, num_loaded_regions, sizeof(LoadedArchiveHeapRegion),\n-        LoadedArchiveHeapRegion::comparator);\n-\n-  uintptr_t p = buffer;\n-  for (int i = 0; i < num_loaded_regions; i++) {\n-    \/\/ This region will be loaded at p, so all objects inside this\n-    \/\/ region will be shifted by ri->offset\n-    LoadedArchiveHeapRegion* ri = &loaded_regions[i];\n-    ri->_runtime_offset = p - ri->_dumptime_base;\n-    p += ri->_region_size;\n+  if (_closed_heap_regions.is_mapped()) {\n+    log_mapped_regions(&_closed_heap_regions, false);\n+  }\n+  if (_open_heap_regions.is_mapped()) {\n+    log_mapped_regions(&_open_heap_regions, true);\n@@ -299,1 +183,0 @@\n-  assert(p == _loaded_heap_top, \"must be\");\n@@ -302,17 +185,14 @@\n-bool ArchiveHeapLoader::load_regions(FileMapInfo* mapinfo, LoadedArchiveHeapRegion* loaded_regions,\n-                                     int num_loaded_regions, uintptr_t buffer) {\n-  uintptr_t bitmap_base = (uintptr_t)mapinfo->map_bitmap_region();\n-  if (bitmap_base == 0) {\n-    _loading_failed = true;\n-    return false; \/\/ OOM or CRC error\n-  }\n-  uintptr_t load_address = buffer;\n-  for (int i = 0; i < num_loaded_regions; i++) {\n-    LoadedArchiveHeapRegion* ri = &loaded_regions[i];\n-    FileMapRegion* r = mapinfo->space_at(ri->_region_index);\n-\n-    if (!mapinfo->read_region(ri->_region_index, (char*)load_address, r->used(), \/* do_commit = *\/ false)) {\n-      \/\/ There's no easy way to free the buffer, so we will fill it with zero later\n-      \/\/ in fill_failed_loaded_heap(), and it will eventually be GC'ed.\n-      log_warning(cds)(\"Loading of heap region %d has failed. Archived objects are disabled\", i);\n-      _loading_failed = true;\n+bool ArchiveHeapLoader::map_heap_regions(FileMapInfo* map_info, ArchiveHeapRegions* heap_regions) {\n+  MemRegion* regions = heap_regions->runtime_regions();\n+  int num_regions = heap_regions->num_regions();\n+\n+  assert(heap_regions->is_runtime_space_reserved(), \"heap space for the archive heap regions must be reserved\");\n+  for (int i = 0; i < num_regions; i++) {\n+    FileMapRegion* si = map_info->space_at(heap_regions->region_index(i));\n+    char* addr = (char*)regions[i].start();\n+    char* base = map_info->map_region_at_address(si, addr, regions[i].byte_size());\n+    if (base == NULL || base != addr) {\n+      log_info(cds)(\"UseSharedSpaces: Unable to map at required address in java heap. \"\n+                    INTPTR_FORMAT \", size = \" SIZE_FORMAT \" bytes\",\n+                    p2i(addr), regions[i].byte_size());\n+      cleanup_regions(map_info, heap_regions);\n@@ -321,4 +201,0 @@\n-    log_info(cds)(\"Loaded heap    region #%d at base \" INTPTR_FORMAT \" top \" INTPTR_FORMAT\n-                  \" size \" SIZE_FORMAT_W(6) \" delta \" INTX_FORMAT,\n-                  ri->_region_index, load_address, load_address + ri->_region_size,\n-                  ri->_region_size, ri->_runtime_offset);\n@@ -326,2 +202,2 @@\n-    uintptr_t oopmap = bitmap_base + r->oopmap_offset();\n-    BitMapView bm((BitMap::bm_word_t*)oopmap, r->oopmap_size_in_bits());\n+    si->set_mapped_base(base);\n+    heap_regions->set_state(ArchiveHeapRegions::MAPPED);\n@@ -329,10 +205,4 @@\n-    if (num_loaded_regions == 4) {\n-      PatchLoadedRegionPointers<4> patcher((narrowOop*)load_address, loaded_regions);\n-      bm.iterate(&patcher);\n-    } else if (num_loaded_regions == 3) {\n-      PatchLoadedRegionPointers<3> patcher((narrowOop*)load_address, loaded_regions);\n-      bm.iterate(&patcher);\n-    } else {\n-      assert(num_loaded_regions == 2, \"must be\");\n-      PatchLoadedRegionPointers<2> patcher((narrowOop*)load_address, loaded_regions);\n-      bm.iterate(&patcher);\n+    if (VerifySharedSpaces && !map_info->region_crc_check(addr, regions[i].byte_size(), si->crc())) {\n+      log_info(cds)(\"UseSharedSpaces: mapped heap regions are corrupt\");\n+      cleanup_regions(map_info, heap_regions);\n+      return false;\n@@ -340,3 +210,0 @@\n-\n-    r->set_mapped_base((char*)load_address);\n-    load_address += r->used();\n@@ -348,5 +215,4 @@\n-bool ArchiveHeapLoader::load_heap_regions(FileMapInfo* mapinfo) {\n-  init_narrow_oop_decoding(mapinfo->narrow_oop_base(), mapinfo->narrow_oop_shift());\n-\n-  LoadedArchiveHeapRegion loaded_regions[MetaspaceShared::max_num_heap_regions];\n-  memset(loaded_regions, 0, sizeof(loaded_regions));\n+\/\/ dealloc the archive regions from java heap\n+bool ArchiveHeapLoader::dealloc_heap_regions(ArchiveHeapRegions* heap_regions) {\n+  return Universe::heap()->dealloc_archive_regions(heap_regions->runtime_regions(), heap_regions->num_regions());\n+}\n@@ -354,9 +220,9 @@\n-  MemRegion archive_space;\n-  int num_loaded_regions = init_loaded_regions(mapinfo, loaded_regions, archive_space);\n-  if (num_loaded_regions <= 0) {\n-    return false;\n-  }\n-  sort_loaded_regions(loaded_regions, num_loaded_regions, (uintptr_t)archive_space.start());\n-  if (!load_regions(mapinfo, loaded_regions, num_loaded_regions, (uintptr_t)archive_space.start())) {\n-    assert(_loading_failed, \"must be\");\n-    return false;\n+ArchiveOopDecoder* ArchiveHeapLoader::get_oop_decoder(FileMapInfo* map_info) {\n+  if (closed_regions_mapped() && !_oop_decoder) {\n+    assert(open_regions_mapped(), \"open heap regions must be mapped\");\n+    if (UseCompressedOops) {\n+      _oop_decoder = new ArchiveNarrowOopDecoder(&_closed_heap_regions, &_open_heap_regions,\n+                                                 map_info->narrow_oop_base(), map_info->narrow_oop_shift());\n+    } else {\n+      _oop_decoder = new ArchiveWideOopDecoder(&_closed_heap_regions, &_open_heap_regions);\n+    }\n@@ -364,5 +230,1 @@\n-\n-  init_loaded_heap_relocation(loaded_regions, num_loaded_regions);\n-  _is_loaded = true;\n-\n-  return true;\n+  return _oop_decoder;\n@@ -371,2 +233,6 @@\n-class VerifyLoadedHeapEmbeddedPointers: public BasicOopIterateClosure {\n-  ResourceHashtable<uintptr_t, bool>* _table;\n+\/\/ Patch all the embedded oop pointers inside an archived heap region,\n+\/\/ to be consistent with the runtime oop encoding.\n+template <typename T>\n+class PatchEmbeddedPointers: public BitMapClosure {\n+  T* _start;\n+  ArchiveOopDecoder* _oop_decoder;\n@@ -375,15 +241,12 @@\n-  VerifyLoadedHeapEmbeddedPointers(ResourceHashtable<uintptr_t, bool>* table) : _table(table) {}\n-\n-  virtual void do_oop(narrowOop* p) {\n-    \/\/ This should be called before the loaded regions are modified, so all the embedded pointers\n-    \/\/ must be NULL, or must point to a valid object in the loaded regions.\n-    narrowOop v = *p;\n-    if (!CompressedOops::is_null(v)) {\n-      oop o = CompressedOops::decode_not_null(v);\n-      uintptr_t u = cast_from_oop<uintptr_t>(o);\n-      ArchiveHeapLoader::assert_in_loaded_heap(u);\n-      guarantee(_table->contains(u), \"must point to beginning of object in loaded archived regions\");\n-    }\n-  }\n-  virtual void do_oop(oop* p) {\n-    ShouldNotReachHere();\n+  PatchEmbeddedPointers(T* start, ArchiveOopDecoder* oop_decoder) :\n+    _start(start),\n+    _oop_decoder(oop_decoder)\n+  {}\n+\n+  bool do_bit(size_t offset) {\n+    T* p = _start + offset;\n+    uintptr_t dumptime_oop = (uintptr_t)((void *)*p);\n+    oop runtime_oop = _oop_decoder->decode(dumptime_oop);\n+    assert(runtime_oop != NULL, \"null oops should have been filtered out at dump time\");\n+    RawAccess<IS_NOT_NULL>::oop_store((T *)p, runtime_oop);\n+    return true;\n@@ -393,7 +256,16 @@\n-void ArchiveHeapLoader::finish_initialization() {\n-  if (is_loaded()) {\n-    \/\/ These operations are needed only when the heap is loaded (not mapped).\n-    finish_loaded_heap();\n-    if (VerifyArchivedFields > 0) {\n-      verify_loaded_heap();\n-    }\n+void ArchiveHeapLoader::patch_embedded_pointers(FileMapInfo* map_info, MemRegion region, address oopmap, size_t oopmap_size_in_bits) {\n+  BitMapView bm((BitMap::bm_word_t*)oopmap, oopmap_size_in_bits);\n+  ArchiveOopDecoder* oop_decoder = get_oop_decoder(map_info);\n+\n+#ifndef PRODUCT\n+  ResourceMark rm;\n+  ResourceBitMap checkBm = HeapShared::calculate_oopmap(region);\n+  assert(bm.is_same(checkBm), \"sanity\");\n+#endif\n+\n+  if (UseCompressedOops) {\n+    PatchEmbeddedPointers<narrowOop> patcher((narrowOop*)region.start(), oop_decoder);\n+    bm.iterate(&patcher);\n+  } else {\n+    PatchEmbeddedPointers<oop> patcher((oop*)region.start(), oop_decoder);\n+    bm.iterate(&patcher);\n@@ -401,1 +273,0 @@\n-  patch_native_pointers();\n@@ -404,3 +275,3 @@\n-void ArchiveHeapLoader::finish_loaded_heap() {\n-  HeapWord* bottom = (HeapWord*)_loaded_heap_bottom;\n-  HeapWord* top    = (HeapWord*)_loaded_heap_top;\n+void ArchiveHeapLoader::patch_heap_embedded_pointers(FileMapInfo* map_info, ArchiveHeapRegions* heap_regions) {\n+  char* bitmap_base = map_info->map_bitmap_region();\n+  assert(bitmap_base != NULL, \"must have already been mapped\");\n@@ -408,2 +279,6 @@\n-  MemRegion archive_space = MemRegion(bottom, top);\n-  Universe::heap()->complete_loaded_archive_space(archive_space);\n+  for (int i = 0; i < heap_regions->num_regions(); i++) {\n+    FileMapRegion* si = map_info->space_at(heap_regions->region_index(i));\n+    patch_embedded_pointers(map_info, heap_regions->runtime_region(i),\n+      (address)(bitmap_base) + si->oopmap_offset(),\n+      si->oopmap_size_in_bits());\n+  }\n@@ -412,13 +287,3 @@\n-void ArchiveHeapLoader::verify_loaded_heap() {\n-  log_info(cds, heap)(\"Verify all oops and pointers in loaded heap\");\n-\n-  ResourceMark rm;\n-  ResourceHashtable<uintptr_t, bool> table;\n-  VerifyLoadedHeapEmbeddedPointers verifier(&table);\n-  HeapWord* bottom = (HeapWord*)_loaded_heap_bottom;\n-  HeapWord* top    = (HeapWord*)_loaded_heap_top;\n-\n-  for (HeapWord* p = bottom; p < top; ) {\n-    oop o = cast_to_oop(p);\n-    table.put(cast_from_oop<uintptr_t>(o), true);\n-    p += o->size();\n+void ArchiveHeapLoader::patch_heap_embedded_pointers(FileMapInfo* map_info) {\n+  if (!_heap_pointers_need_patching) {\n+    return;\n@@ -427,4 +292,19 @@\n-  for (HeapWord* p = bottom; p < top; ) {\n-    oop o = cast_to_oop(p);\n-    o->oop_iterate(&verifier);\n-    p += o->size();\n+  log_info(cds)(\"patching heap embedded pointers\");\n+\n+  assert(_closed_heap_regions.is_mapped(), \"closed heap regions must have been successfully mapped\");\n+  assert(_open_heap_regions.is_mapped(), \"open regions must have been successfully mapped\");\n+  patch_heap_embedded_pointers(map_info, &_closed_heap_regions);\n+  patch_heap_embedded_pointers(map_info, &_open_heap_regions);\n+}\n+\n+bool ArchiveHeapLoader::is_archived_object(oop object) {\n+  if (_closed_heap_regions.is_mapped()) {\n+    if (_closed_heap_regions.is_in_runtime_region(cast_from_oop<uintptr_t>(object))) {\n+      return true;\n+    }\n+    assert(_open_heap_regions.is_mapped(), \"open heap regions must be mapped\");\n+    if (_open_heap_regions.is_in_runtime_region(cast_from_oop<uintptr_t>(object))) {\n+      return true;\n+    }\n+  } else {\n+    assert(!_open_heap_regions.is_mapped(), \"open heap regions should not be mapped when closed heap regions are not mapped\");\n@@ -432,0 +312,1 @@\n+  return false;\n@@ -434,7 +315,6 @@\n-void ArchiveHeapLoader::fill_failed_loaded_heap() {\n-  assert(_loading_failed, \"must be\");\n-  if (_loaded_heap_bottom != 0) {\n-    assert(_loaded_heap_top != 0, \"must be\");\n-    HeapWord* bottom = (HeapWord*)_loaded_heap_bottom;\n-    HeapWord* top = (HeapWord*)_loaded_heap_top;\n-    Universe::heap()->fill_with_objects(bottom, top - bottom);\n+void ArchiveHeapLoader::complete_heap_regions_mapping() {\n+  if (closed_regions_mapped()) {\n+    Universe::heap()->complete_archive_regions_alloc(_closed_heap_regions.runtime_regions(), _closed_heap_regions.num_regions());\n+  }\n+  if (open_regions_mapped()) {\n+    Universe::heap()->complete_archive_regions_alloc(_open_heap_regions.runtime_regions(), _open_heap_regions.num_regions());\n@@ -462,4 +342,0 @@\n-  if (MetaspaceShared::relocation_delta() == 0) {\n-    return;\n-  }\n-\n@@ -477,0 +353,26 @@\n+\n+void ArchiveHeapLoader::finish_initialization() {\n+  complete_heap_regions_mapping();\n+  patch_native_pointers();\n+}\n+\n+void ArchiveHeapLoader::fill_failed_mapped_regions() {\n+  if (_closed_heap_regions.is_mapping_failed()) {\n+    Universe::heap()->fill_heap_regions(_closed_heap_regions.runtime_regions(), _closed_heap_regions.num_regions());\n+  }\n+  if (_open_heap_regions.is_mapping_failed()) {\n+    Universe::heap()->fill_heap_regions(_open_heap_regions.runtime_regions(), _open_heap_regions.num_regions());\n+  }\n+}\n+\n+void ArchiveHeapLoader::fixup_regions() {\n+  if (can_use()) {\n+    fill_failed_mapped_regions();\n+  }\n+  if (is_archived_heap_available()) {\n+    if (!MetaspaceShared::use_full_module_graph()) {\n+      \/\/ Need to remove all the archived java.lang.Module objects from HeapShared::roots().\n+      ClassLoaderDataShared::clear_archived_oops();\n+    }\n+  }\n+}\n","filename":"src\/hotspot\/share\/cds\/archiveHeapLoader.cpp","additions":243,"deletions":341,"binary":false,"changes":584,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"cds\/archiveUtils.hpp\"\n@@ -40,17 +41,21 @@\n-public:\n-  \/\/ At runtime, heap regions in the CDS archive can be used in two different ways,\n-  \/\/ depending on the GC type:\n-  \/\/ - Mapped: (G1 only) the regions are directly mapped into the Java heap\n-  \/\/ - Loaded: At VM start-up, the objects in the heap regions are copied into the\n-  \/\/           Java heap. This is easier to implement than mapping but\n-  \/\/           slightly less efficient, as the embedded pointers need to be relocated.\n-  static bool can_use() { return can_map() || can_load(); }\n-\n-  \/\/ Can this VM map archived heap regions? Currently only G1+compressed{oops,cp}\n-  static bool can_map() {\n-    CDS_JAVA_HEAP_ONLY(return (UseG1GC && UseCompressedClassPointers);)\n-    NOT_CDS_JAVA_HEAP(return false;)\n-  }\n-  static bool is_mapped() {\n-    return closed_regions_mapped() && open_regions_mapped();\n-  }\n+#if INCLUDE_CDS_JAVA_HEAP\n+private:\n+  static ArchiveOopDecoder* _oop_decoder;\n+  static ArchiveHeapRegions _closed_heap_regions;\n+  static ArchiveHeapRegions _open_heap_regions;\n+\n+  static bool _heap_pointers_need_patching;\n+\n+  static void init_archive_heap_regions(FileMapInfo* map_info, int first_region_idx, int last_region_idx, ArchiveHeapRegions* heap_regions);\n+  static void cleanup_regions(FileMapInfo* map_info, ArchiveHeapRegions* heap_regions);\n+  static void cleanup(FileMapInfo* map_info);\n+  static bool get_heap_range_for_archive_regions(ArchiveHeapRegions* heap_regions, bool is_open);\n+  static bool is_pointer_patching_needed(FileMapInfo* map_info);\n+  static void log_mapped_regions(ArchiveHeapRegions* heap_regions, bool is_open);\n+  static bool map_heap_regions(FileMapInfo* map_info, ArchiveHeapRegions* heap_regions);\n+  static bool dealloc_heap_regions(ArchiveHeapRegions* heap_regions);\n+  static void patch_embedded_pointers(FileMapInfo* map_info, MemRegion region, address oopmap, size_t oopmap_size_in_bits);\n+  static void patch_heap_embedded_pointers(FileMapInfo* map_info, ArchiveHeapRegions* heap_regions);\n+  static void fill_failed_mapped_regions();\n+  static void patch_native_pointers();\n+#endif \/* INCLUDE_CDS_JAVA_HEAP *\/\n@@ -58,5 +63,4 @@\n-  \/\/ Can this VM load the objects from archived heap regions into the heap at start-up?\n-  static bool can_load()  NOT_CDS_JAVA_HEAP_RETURN_(false);\n-  static void finish_initialization() NOT_CDS_JAVA_HEAP_RETURN;\n-  static bool is_loaded() {\n-    CDS_JAVA_HEAP_ONLY(return _is_loaded;)\n+public:\n+  \/\/ Can this VM map archived heap regions?\n+  static bool can_use() {\n+    CDS_JAVA_HEAP_ONLY(return ((UseG1GC || UseEpsilonGC || UseParallelGC || UseSerialGC) && UseCompressedClassPointers);)\n@@ -65,1 +69,0 @@\n-\n@@ -67,4 +70,1 @@\n-    return is_loaded() || closed_regions_mapped();\n-  }\n-  static bool are_archived_mirrors_available() {\n-    return is_fully_available();\n+    return closed_regions_mapped();\n@@ -72,2 +72,2 @@\n-  static bool is_fully_available() {\n-    return is_loaded() || is_mapped();\n+  static bool is_archived_heap_available() {\n+    return closed_regions_mapped() && open_regions_mapped();\n@@ -75,5 +75,2 @@\n-\n-  static ptrdiff_t runtime_delta() {\n-    assert(!UseCompressedOops, \"must be\");\n-    CDS_JAVA_HEAP_ONLY(return _runtime_delta;)\n-    NOT_CDS_JAVA_HEAP_RETURN_(0L);\n+  static bool are_archived_mirrors_available() {\n+    return is_archived_heap_available();\n@@ -82,4 +79,0 @@\n-  static void set_closed_regions_mapped() {\n-    CDS_JAVA_HEAP_ONLY(_closed_regions_mapped = true;)\n-    NOT_CDS_JAVA_HEAP_RETURN;\n-  }\n@@ -87,1 +80,1 @@\n-    CDS_JAVA_HEAP_ONLY(return _closed_regions_mapped;)\n+    CDS_JAVA_HEAP_ONLY(return _closed_heap_regions.is_mapped();)\n@@ -90,4 +83,0 @@\n-  static void set_open_regions_mapped() {\n-    CDS_JAVA_HEAP_ONLY(_open_regions_mapped = true;)\n-    NOT_CDS_JAVA_HEAP_RETURN;\n-  }\n@@ -95,1 +84,1 @@\n-    CDS_JAVA_HEAP_ONLY(return _open_regions_mapped;)\n+    CDS_JAVA_HEAP_ONLY(return _open_heap_regions.is_mapped();)\n@@ -99,11 +88,6 @@\n-  \/\/ NarrowOops stored in the CDS archive may use a different encoding scheme\n-  \/\/ than CompressedOops::{base,shift} -- see FileMapInfo::map_heap_regions_impl.\n-  \/\/ To decode them, do not use CompressedOops::decode_not_null. Use this\n-  \/\/ function instead.\n-  inline static oop decode_from_archive(narrowOop v) NOT_CDS_JAVA_HEAP_RETURN_(NULL);\n-\n-  static void init_narrow_oop_decoding(address base, int shift) NOT_CDS_JAVA_HEAP_RETURN;\n-\n-  static void patch_embedded_pointers(MemRegion region, address oopmap,\n-                                      size_t oopmap_in_bits) NOT_CDS_JAVA_HEAP_RETURN;\n-\n+  static void map_heap_regions(FileMapInfo* map_info) NOT_CDS_JAVA_HEAP_RETURN;\n+  static void complete_heap_regions_mapping() NOT_CDS_JAVA_HEAP_RETURN;\n+  static ArchiveOopDecoder* get_oop_decoder(FileMapInfo* map_info) NOT_CDS_JAVA_HEAP_RETURN_(NULL);\n+  static void patch_heap_embedded_pointers(FileMapInfo* map_info) NOT_CDS_JAVA_HEAP_RETURN;\n+  static bool is_archived_object(oop object) NOT_CDS_JAVA_HEAP_RETURN_(false);\n+  static void finish_initialization() NOT_CDS_JAVA_HEAP_RETURN;\n@@ -111,60 +95,0 @@\n-\n-#if INCLUDE_CDS_JAVA_HEAP\n-private:\n-  static bool _closed_regions_mapped;\n-  static bool _open_regions_mapped;\n-  static bool _is_loaded;\n-\n-  \/\/ Support for loaded archived heap. These are cached values from\n-  \/\/ LoadedArchiveHeapRegion's.\n-  static uintptr_t _dumptime_base_0;\n-  static uintptr_t _dumptime_base_1;\n-  static uintptr_t _dumptime_base_2;\n-  static uintptr_t _dumptime_base_3;\n-  static uintptr_t _dumptime_top;\n-  static intx _runtime_offset_0;\n-  static intx _runtime_offset_1;\n-  static intx _runtime_offset_2;\n-  static intx _runtime_offset_3;\n-\n-  static uintptr_t _loaded_heap_bottom;\n-  static uintptr_t _loaded_heap_top;\n-  static bool _loading_failed;\n-\n-  \/\/ UseCompressedOops only: Used by decode_from_archive\n-  static address _narrow_oop_base;\n-  static int     _narrow_oop_shift;\n-\n-  \/\/ !UseCompressedOops only: used to relocate pointers to the archived objects\n-  static ptrdiff_t _runtime_delta;\n-\n-  static int init_loaded_regions(FileMapInfo* mapinfo, LoadedArchiveHeapRegion* loaded_regions,\n-                                 MemRegion& archive_space);\n-  static void sort_loaded_regions(LoadedArchiveHeapRegion* loaded_regions, int num_loaded_regions,\n-                                  uintptr_t buffer);\n-  static bool load_regions(FileMapInfo* mapinfo, LoadedArchiveHeapRegion* loaded_regions,\n-                           int num_loaded_regions, uintptr_t buffer);\n-  static void init_loaded_heap_relocation(LoadedArchiveHeapRegion* reloc_info,\n-                                          int num_loaded_regions);\n-  static void patch_native_pointers();\n-  static void finish_loaded_heap();\n-  static void verify_loaded_heap();\n-  static void fill_failed_loaded_heap();\n-\n-  static bool is_in_loaded_heap(uintptr_t o) {\n-    return (_loaded_heap_bottom <= o && o < _loaded_heap_top);\n-  }\n-\n-public:\n-\n-  static bool load_heap_regions(FileMapInfo* mapinfo);\n-  static void assert_in_loaded_heap(uintptr_t o) {\n-    assert(is_in_loaded_heap(o), \"must be\");\n-  }\n-\n-  static void set_runtime_delta(ptrdiff_t delta) {\n-    assert(!UseCompressedOops, \"must be\");\n-    _runtime_delta = delta;\n-  }\n-#endif \/\/ INCLUDE_CDS_JAVA_HEAP\n-\n","filename":"src\/hotspot\/share\/cds\/archiveHeapLoader.hpp","additions":39,"deletions":115,"binary":false,"changes":154,"status":"modified"},{"patch":"@@ -35,21 +35,0 @@\n-inline oop ArchiveHeapLoader::decode_from_archive(narrowOop v) {\n-  assert(!CompressedOops::is_null(v), \"narrow oop value can never be zero\");\n-  uintptr_t p = ((uintptr_t)_narrow_oop_base) + ((uintptr_t)v << _narrow_oop_shift);\n-  if (p >= _dumptime_base_0) {\n-    assert(p < _dumptime_top, \"must be\");\n-    if (p >= _dumptime_base_3) {\n-      p += _runtime_offset_3;\n-    } else if (p >= _dumptime_base_2) {\n-      p += _runtime_offset_2;\n-    } else if (p >= _dumptime_base_1) {\n-      p += _runtime_offset_1;\n-    } else {\n-      p += _runtime_offset_0;\n-    }\n-  }\n-\n-  oop result = cast_to_oop((uintptr_t)p);\n-  assert(is_object_aligned(result), \"address not aligned: \" INTPTR_FORMAT, p2i((void*) result));\n-  return result;\n-}\n-\n","filename":"src\/hotspot\/share\/cds\/archiveHeapLoader.inline.hpp","additions":0,"deletions":21,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -150,0 +150,20 @@\n+void ArchiveHeapRegions::init(int max_region_count) {\n+  _dumptime_regions = MemRegion::create_array(max_region_count, mtInternal);\n+  _runtime_regions = MemRegion::create_array(max_region_count, mtInternal);\n+  _region_idx = (int *)os::malloc(sizeof(int) * max_region_count, mtInternal);\n+  _num_regions = max_region_count;\n+  _state = ArchiveHeapRegions::UNINITIALIZED;\n+}\n+\n+ArchiveHeapRegions::~ArchiveHeapRegions() {\n+  if (_dumptime_regions) {\n+    MemRegion::destroy_array(_dumptime_regions, _num_regions);\n+  }\n+  if (_runtime_regions) {\n+    MemRegion::destroy_array(_runtime_regions, _num_regions);\n+  }\n+  if (_region_idx) {\n+    os::free(_region_idx);\n+  }\n+}\n+\n@@ -290,0 +310,30 @@\n+oop ArchiveNarrowOopDecoder::decode(uintptr_t ptr) {\n+  narrowOop o = CompressedOops::narrow_oop_cast(ptr);\n+  if (CompressedOops::is_null(o)) {\n+    return NULL;\n+  }\n+  uintptr_t p = (uintptr_t)_narrow_oop_base + (ptr << _narrow_oop_shift);\n+  uintptr_t result = _closed_regions->dumptime_to_runtime(p);\n+  if (!result) {\n+    \/\/ if ptr is not found in closed heap region, it should be present in open heap region\n+    assert(_open_regions->is_mapped(), \"open heap regions must be mapped\");\n+    result = _open_regions->dumptime_to_runtime(p);\n+  }\n+  assert(result != 0, \"decoded oop cannot be null\");\n+  return cast_to_oop(result);\n+}\n+\n+oop ArchiveWideOopDecoder::decode(uintptr_t ptr) {\n+  if (ptr == 0) {\n+    return NULL;\n+  }\n+  uintptr_t result = _closed_regions->dumptime_to_runtime(ptr);\n+  if (!result) {\n+    \/\/ if ptr is not found in closed heap region, it should be present in open heap region\n+    assert(_open_regions->is_mapped(), \"open heap regions must be mapped\");\n+    result = _open_regions->dumptime_to_runtime(ptr);\n+  }\n+  assert(result != 0, \"decoded oop cannot be null\");\n+  return cast_to_oop(result);\n+}\n+\n@@ -317,9 +367,3 @@\n-  if (UseCompressedOops) {\n-    narrowOop o = CompressedOops::narrow_oop_cast(nextPtr());\n-    if (CompressedOops::is_null(o) || !ArchiveHeapLoader::is_fully_available()) {\n-      *p = NULL;\n-    } else {\n-      assert(ArchiveHeapLoader::can_use(), \"sanity\");\n-      assert(ArchiveHeapLoader::is_fully_available(), \"must be\");\n-      *p = ArchiveHeapLoader::decode_from_archive(o);\n-    }\n+  uintptr_t ptr = nextPtr();\n+  if (_oop_decoder) {\n+    *p = _oop_decoder->decode(ptr);\n@@ -327,7 +371,1 @@\n-    intptr_t dumptime_oop = nextPtr();\n-    if (dumptime_oop == 0 || !ArchiveHeapLoader::is_fully_available()) {\n-      *p = NULL;\n-    } else {\n-      intptr_t runtime_oop = dumptime_oop + ArchiveHeapLoader::runtime_delta();\n-      *p = cast_to_oop(runtime_oop);\n-    }\n+    *p = NULL;\n","filename":"src\/hotspot\/share\/cds\/archiveUtils.cpp","additions":54,"deletions":16,"binary":false,"changes":70,"status":"modified"},{"patch":"@@ -82,0 +82,79 @@\n+class ArchiveHeapRegions : public CHeapObj<mtInternal> {\n+public:\n+  enum State {\n+    UNINITIALIZED,\n+    HEAP_RESERVED,\n+    MAPPED,\n+    MAPPING_FAILED,\n+    MAPPING_FAILED_DEALLOCATED,\n+  };\n+\n+private:\n+  MemRegion *_dumptime_regions;\n+  MemRegion *_runtime_regions;\n+  int *_region_idx;\n+  int _num_regions;\n+  State _state;\n+\n+  int dumptime_region_index_for(intptr_t ptr) {\n+    for (int i = 0; i < num_regions(); i++) {\n+      if (dumptime_region(i).contains((const void *)ptr)) {\n+        return i;\n+      }\n+    }\n+    return -1;\n+  }\n+\n+public:\n+  ArchiveHeapRegions() {}\n+  ~ArchiveHeapRegions();\n+\n+  void init(int max_region_count);\n+  void set_state(State state) { _state = state; }\n+  bool is_runtime_space_reserved() { return _state == HEAP_RESERVED; }\n+  bool is_mapped() { return _state == MAPPED; }\n+  bool is_mapping_failed() { return _state == MAPPING_FAILED; }\n+  void set_dumptime_region(int index, MemRegion region) { _dumptime_regions[index] = region; }\n+  void set_runtime_region(int index, MemRegion region) { _runtime_regions[index] = region; }\n+  void set_region_index(int index, int region_index) { _region_idx[index] = region_index; }\n+  void set_num_regions(int count) { _num_regions = count; }\n+\n+  MemRegion* dumptime_regions() { return _dumptime_regions; }\n+  MemRegion* runtime_regions() { return _runtime_regions; }\n+  MemRegion dumptime_region(int idx) { return _dumptime_regions[idx]; }\n+  MemRegion runtime_region(int idx) { return _runtime_regions[idx]; }\n+  int region_index(int memory_region_index) { return _region_idx[memory_region_index]; }\n+  int num_regions() { return _num_regions; }\n+\n+  uintptr_t dumptime_to_runtime(uintptr_t ptr) {\n+    int idx = dumptime_region_index_for(ptr);\n+    if (idx != -1) {\n+      return ptr + delta_for(idx);\n+    }\n+    return 0;\n+  }\n+\n+  ptrdiff_t delta_for(int region_idx) {\n+    return (uintptr_t)runtime_region(region_idx).start() - (uintptr_t)dumptime_region(region_idx).start();\n+  }\n+\n+  bool is_relocated() {\n+    for (int i = 0; i < num_regions(); i++) {\n+      if (runtime_region(i).start() != dumptime_region(i).start()) {\n+        return true;\n+        break;\n+      }\n+    }\n+    return false;\n+  }\n+\n+  bool is_in_runtime_region(uintptr_t ptr) {\n+    for(int i = 0; i < num_regions(); i++) {\n+      if (runtime_region(i).contains((const void *)ptr)) {\n+        return true;\n+      }\n+    }\n+    return false;\n+  }\n+};\n+\n@@ -208,0 +287,38 @@\n+class ArchiveOopDecoder : public CHeapObj<mtInternal> {\n+public:\n+  virtual oop decode(uintptr_t ptr) = 0;\n+};\n+\n+class ArchiveNarrowOopDecoder : public ArchiveOopDecoder {\n+private:\n+  ArchiveHeapRegions* _closed_regions;\n+  ArchiveHeapRegions* _open_regions;\n+  address _narrow_oop_base;\n+  int _narrow_oop_shift;\n+\n+public:\n+  ArchiveNarrowOopDecoder(ArchiveHeapRegions* closed_regions, ArchiveHeapRegions* open_regions,\n+                    address narrow_oop_base, int narrow_oop_shift):\n+    _closed_regions(closed_regions),\n+    _open_regions(open_regions),\n+    _narrow_oop_base(narrow_oop_base),\n+    _narrow_oop_shift(narrow_oop_shift)\n+  {}\n+\n+  oop decode(uintptr_t ptr);\n+};\n+\n+class ArchiveWideOopDecoder : public ArchiveOopDecoder {\n+private:\n+  ArchiveHeapRegions* _closed_regions;\n+  ArchiveHeapRegions* _open_regions;\n+\n+public:\n+  ArchiveWideOopDecoder(ArchiveHeapRegions* closed_regions, ArchiveHeapRegions* open_regions) :\n+    _closed_regions(closed_regions),\n+    _open_regions(open_regions)\n+  {}\n+\n+  oop decode(uintptr_t ptr);\n+};\n+\n@@ -214,0 +331,1 @@\n+  ArchiveOopDecoder* _oop_decoder;\n@@ -220,1 +338,4 @@\n-  ReadClosure(intptr_t** ptr_array) { _ptr_array = ptr_array; }\n+  ReadClosure(intptr_t** ptr_array, ArchiveOopDecoder* oop_decoder = NULL) :\n+    _ptr_array(ptr_array),\n+    _oop_decoder(oop_decoder)\n+  {}\n","filename":"src\/hotspot\/share\/cds\/archiveUtils.hpp","additions":122,"deletions":1,"binary":false,"changes":123,"status":"modified"},{"patch":"@@ -2063,15 +2063,0 @@\n-address FileMapInfo::decode_start_address(FileMapRegion* spc, bool with_current_oop_encoding_mode) {\n-  size_t offset = spc->mapping_offset();\n-  narrowOop n = CompressedOops::narrow_oop_cast(offset);\n-  if (with_current_oop_encoding_mode) {\n-    return cast_from_oop<address>(CompressedOops::decode_raw_not_null(n));\n-  } else {\n-    return cast_from_oop<address>(ArchiveHeapLoader::decode_from_archive(n));\n-  }\n-}\n-\n-static MemRegion *closed_heap_regions = NULL;\n-static MemRegion *open_heap_regions = NULL;\n-static int num_closed_heap_regions = 0;\n-static int num_open_heap_regions = 0;\n-\n@@ -2083,28 +2068,0 @@\n-\/\/ Returns the address range of the archived heap regions computed using the\n-\/\/ current oop encoding mode. This range may be different than the one seen at\n-\/\/ dump time due to encoding mode differences. The result is used in determining\n-\/\/ if\/how these regions should be relocated at run time.\n-MemRegion FileMapInfo::get_heap_regions_range_with_current_oop_encoding_mode() {\n-  address start = (address) max_uintx;\n-  address end   = NULL;\n-\n-  for (int i = MetaspaceShared::first_closed_heap_region;\n-           i <= MetaspaceShared::last_valid_region;\n-           i++) {\n-    FileMapRegion* si = space_at(i);\n-    size_t size = si->used();\n-    if (size > 0) {\n-      address s = start_address_as_decoded_with_current_oop_encoding_mode(si);\n-      address e = s + size;\n-      if (start > s) {\n-        start = s;\n-      }\n-      if (end < e) {\n-        end = e;\n-      }\n-    }\n-  }\n-  assert(end != NULL, \"must have at least one used heap region\");\n-  return MemRegion((HeapWord*)start, (HeapWord*)end);\n-}\n-\n@@ -2112,2 +2069,0 @@\n-  bool success = false;\n-\n@@ -2115,4 +2070,2 @@\n-    if (ArchiveHeapLoader::can_map()) {\n-      success = map_heap_regions();\n-    } else if (ArchiveHeapLoader::can_load()) {\n-      success = ArchiveHeapLoader::load_heap_regions(this);\n+    if (ArchiveHeapLoader::can_use()) {\n+      ArchiveHeapLoader::map_heap_regions(this);\n@@ -2120,1 +2073,1 @@\n-      log_info(cds)(\"Cannot use CDS heap data. UseEpsilonGC, UseG1GC, UseSerialGC or UseParallelGC are required.\");\n+      log_info(cds)(\"Cannot use CDS heap data.\");\n@@ -2124,1 +2077,1 @@\n-  if (!success) {\n+  if (!ArchiveHeapLoader::is_archived_heap_available()) {\n@@ -2161,0 +2114,2 @@\n+  \/* heap range is not available for all GC policies, skip displaying it *\/\n+#if 0\n@@ -2166,0 +2121,1 @@\n+#endif\n@@ -2174,0 +2130,1 @@\n+#endif \/\/ INCLUDE_CDS_JAVA_HEAP\n@@ -2175,308 +2132,5 @@\n-\/\/ The address where the bottom of this shared heap region should be mapped\n-\/\/ at runtime\n-address FileMapInfo::heap_region_runtime_start_address(FileMapRegion* spc) {\n-  assert(UseSharedSpaces, \"runtime only\");\n-  spc->assert_is_heap_region();\n-  if (UseCompressedOops) {\n-    return start_address_as_decoded_from_archive(spc);\n-  } else {\n-    assert(is_aligned(spc->mapping_offset(), sizeof(HeapWord)), \"must be\");\n-    return header()->heap_begin() + spc->mapping_offset() + ArchiveHeapLoader::runtime_delta();\n-  }\n-}\n-\n-void FileMapInfo::set_shared_heap_runtime_delta(ptrdiff_t delta) {\n-  if (UseCompressedOops) {\n-    ArchiveHeapLoader::init_narrow_oop_decoding(narrow_oop_base() + delta, narrow_oop_shift());\n-  } else {\n-    ArchiveHeapLoader::set_runtime_delta(delta);\n-  }\n-}\n-\n-\/\/\n-\/\/ Map the closed and open archive heap objects to the runtime java heap.\n-\/\/\n-\/\/ The shared objects are mapped at (or close to ) the java heap top in\n-\/\/ closed archive regions. The mapped objects contain no out-going\n-\/\/ references to any other java heap regions. GC does not write into the\n-\/\/ mapped closed archive heap region.\n-\/\/\n-\/\/ The open archive heap objects are mapped below the shared objects in\n-\/\/ the runtime java heap. The mapped open archive heap data only contains\n-\/\/ references to the shared objects and open archive objects initially.\n-\/\/ During runtime execution, out-going references to any other java heap\n-\/\/ regions may be added. GC may mark and update references in the mapped\n-\/\/ open archive objects.\n-void FileMapInfo::map_heap_regions_impl() {\n-  if (narrow_oop_mode() != CompressedOops::mode() ||\n-      narrow_oop_base() != CompressedOops::base() ||\n-      narrow_oop_shift() != CompressedOops::shift()) {\n-    log_info(cds)(\"CDS heap data needs to be relocated because the archive was created with an incompatible oop encoding mode.\");\n-    _heap_pointers_need_patching = true;\n-  } else {\n-    if (UseCompressedOops) {\n-      MemRegion range = get_heap_regions_range_with_current_oop_encoding_mode();\n-      if (!CompressedOops::is_in(range)) {\n-        log_info(cds)(\"CDS heap data needs to be relocated because\");\n-        log_info(cds)(\"the desired range \" PTR_FORMAT \" - \"  PTR_FORMAT, p2i(range.start()), p2i(range.end()));\n-        log_info(cds)(\"is outside of the heap \" PTR_FORMAT \" - \"  PTR_FORMAT, p2i(CompressedOops::begin()), p2i(CompressedOops::end()));\n-        _heap_pointers_need_patching = true;\n-      } else if (header()->heap_end() != CompressedOops::end()) {\n-        log_info(cds)(\"CDS heap data needs to be relocated to the end of the runtime heap to reduce fragmentation\");\n-        _heap_pointers_need_patching = true;\n-      }\n-    } else {\n-      MemRegion range((HeapWord*)header()->heap_begin(), (HeapWord*)header()->heap_end());\n-      if (!G1CollectedHeap::heap()->reserved().contains(range)) {\n-        log_info(cds)(\"CDS heap data needs to be relocated because\");\n-        log_info(cds)(\"the desired range \" PTR_FORMAT \" - \"  PTR_FORMAT, p2i(range.start()), p2i(range.end()));\n-        log_info(cds)(\"is outside of the heap \" PTR_FORMAT \" - \"  PTR_FORMAT,\n-            p2i((address)G1CollectedHeap::heap()->reserved().start()), p2i((address)G1CollectedHeap::heap()->reserved().end()));\n-        _heap_pointers_need_patching = true;\n-      } else if (header()->heap_end() != (address)G1CollectedHeap::heap()->reserved().end()) {\n-        log_info(cds)(\"CDS heap data needs to be relocated to the end of the runtime heap to reduce fragmentation\");\n-        _heap_pointers_need_patching = true;\n-      }\n-    }\n-  }\n-\n-  ptrdiff_t delta = 0;\n-  if (_heap_pointers_need_patching) {\n-    \/\/   dumptime heap end  ------------v\n-    \/\/   [      |archived heap regions| ]         run time heap end -----v\n-    \/\/                                       [   |archived heap regions| ]\n-    \/\/          ^\n-    \/\/          D                                ^\n-    \/\/                                           R\n-    \/\/                                  |<-----delta-------------------->|\n-    \/\/\n-    \/\/ At dump time, the archived heap regions were near the top of the heap.\n-    \/\/ At run time, if the heap ends at a different address, we need to\n-    \/\/ move them near to top of the run time heap. This can be done by\n-    \/\/ the simple math of adding the delta as shown above.\n-    \/\/\n-    \/\/ Also: D = bottom of a heap region at dump time\n-    \/\/       R = bottom of a heap region at run time\n-    \/\/\n-    \/\/ FileMapRegion* spc = ...;\n-    \/\/   address D = header()->heap_begin() + spc->mapping_offset();\n-    \/\/   address R = D + delta;\n-    address dumptime_heap_end = header()->heap_end();\n-    address runtime_heap_end = UseCompressedOops ? CompressedOops::end() :\n-                                                   (address)G1CollectedHeap::heap()->reserved().end();\n-    delta = runtime_heap_end - dumptime_heap_end;\n-  }\n-\n-  log_info(cds)(\"CDS heap data relocation delta = \" INTX_FORMAT \" bytes\", delta);\n-\n-  set_shared_heap_runtime_delta(delta);\n-\n-  FileMapRegion* si = space_at(MetaspaceShared::first_closed_heap_region);\n-  address relocated_closed_heap_region_bottom = heap_region_runtime_start_address(si);\n-\n-  if (!is_aligned(relocated_closed_heap_region_bottom, HeapRegion::GrainBytes)) {\n-    \/\/ Align the bottom of the closed archive heap regions at G1 region boundary.\n-    \/\/ This will avoid the situation where the highest open region and the lowest\n-    \/\/ closed region sharing the same G1 region. Otherwise we will fail to map the\n-    \/\/ open regions.\n-    size_t align = size_t(relocated_closed_heap_region_bottom) % HeapRegion::GrainBytes;\n-    delta -= align;\n-    log_info(cds)(\"CDS heap data needs to be relocated lower by a further \" SIZE_FORMAT\n-                  \" bytes to \" INTX_FORMAT \" to be aligned with HeapRegion::GrainBytes\",\n-                  align, delta);\n-    set_shared_heap_runtime_delta(delta);\n-    relocated_closed_heap_region_bottom = heap_region_runtime_start_address(si);\n-    _heap_pointers_need_patching = true;\n-  }\n-  assert(is_aligned(relocated_closed_heap_region_bottom, HeapRegion::GrainBytes),\n-         \"must be\");\n-\n-  if (_heap_pointers_need_patching) {\n-    char* bitmap_base = map_bitmap_region();\n-    if (bitmap_base == NULL) {\n-      log_info(cds)(\"CDS heap cannot be used because bitmap region cannot be mapped\");\n-      _heap_pointers_need_patching = false;\n-      return;\n-    }\n-  }\n-\n-  \/\/ Map the closed heap regions: GC does not write into these regions.\n-  if (map_heap_regions(MetaspaceShared::first_closed_heap_region,\n-                       MetaspaceShared::max_num_closed_heap_regions,\n-                       \/*is_open_archive=*\/ false,\n-                       &closed_heap_regions, &num_closed_heap_regions)) {\n-    ArchiveHeapLoader::set_closed_regions_mapped();\n-\n-    \/\/ Now, map the open heap regions: GC can write into these regions.\n-    if (map_heap_regions(MetaspaceShared::first_open_heap_region,\n-                         MetaspaceShared::max_num_open_heap_regions,\n-                         \/*is_open_archive=*\/ true,\n-                         &open_heap_regions, &num_open_heap_regions)) {\n-      ArchiveHeapLoader::set_open_regions_mapped();\n-    }\n-  }\n-}\n-\n-bool FileMapInfo::map_heap_regions() {\n-  map_heap_regions_impl();\n-\n-  if (!ArchiveHeapLoader::closed_regions_mapped()) {\n-    assert(closed_heap_regions == NULL &&\n-           num_closed_heap_regions == 0, \"sanity\");\n-  }\n-\n-  if (!ArchiveHeapLoader::open_regions_mapped()) {\n-    assert(open_heap_regions == NULL && num_open_heap_regions == 0, \"sanity\");\n-    return false;\n-  } else {\n-    return true;\n-  }\n-}\n-\n-bool FileMapInfo::map_heap_regions(int first, int max,  bool is_open_archive,\n-                                   MemRegion** regions_ret, int* num_regions_ret) {\n-  MemRegion* regions = MemRegion::create_array(max, mtInternal);\n-\n-  struct Cleanup {\n-    MemRegion* _regions;\n-    uint _length;\n-    bool _aborted;\n-    Cleanup(MemRegion* regions, uint length) : _regions(regions), _length(length), _aborted(true) { }\n-    ~Cleanup() { if (_aborted) { MemRegion::destroy_array(_regions, _length); } }\n-  } cleanup(regions, max);\n-\n-  FileMapRegion* si;\n-  int num_regions = 0;\n-\n-  for (int i = first;\n-           i < first + max; i++) {\n-    si = space_at(i);\n-    size_t size = si->used();\n-    if (size > 0) {\n-      HeapWord* start = (HeapWord*)heap_region_runtime_start_address(si);\n-      regions[num_regions] = MemRegion(start, size \/ HeapWordSize);\n-      num_regions ++;\n-      log_info(cds)(\"Trying to map heap data: region[%d] at \" INTPTR_FORMAT \", size = \" SIZE_FORMAT_W(8) \" bytes\",\n-                    i, p2i(start), size);\n-    }\n-  }\n-\n-  if (num_regions == 0) {\n-    return false; \/\/ no archived java heap data\n-  }\n-\n-  \/\/ Check that regions are within the java heap\n-  if (!G1CollectedHeap::heap()->check_archive_addresses(regions, num_regions)) {\n-    log_info(cds)(\"UseSharedSpaces: Unable to allocate region, range is not within java heap.\");\n-    return false;\n-  }\n-\n-  \/\/ allocate from java heap\n-  if (!G1CollectedHeap::heap()->alloc_archive_regions(\n-             regions, num_regions, is_open_archive)) {\n-    log_info(cds)(\"UseSharedSpaces: Unable to allocate region, java heap range is already in use.\");\n-    return false;\n-  }\n-\n-  \/\/ Map the archived heap data. No need to call MemTracker::record_virtual_memory_type()\n-  \/\/ for mapped regions as they are part of the reserved java heap, which is\n-  \/\/ already recorded.\n-  for (int i = 0; i < num_regions; i++) {\n-    si = space_at(first + i);\n-    char* addr = (char*)regions[i].start();\n-    char* base = os::map_memory(_fd, _full_path, si->file_offset(),\n-                                addr, regions[i].byte_size(), si->read_only(),\n-                                si->allow_exec());\n-    if (base == NULL || base != addr) {\n-      \/\/ dealloc the regions from java heap\n-      dealloc_heap_regions(regions, num_regions);\n-      log_info(cds)(\"UseSharedSpaces: Unable to map at required address in java heap. \"\n-                    INTPTR_FORMAT \", size = \" SIZE_FORMAT \" bytes\",\n-                    p2i(addr), regions[i].byte_size());\n-      return false;\n-    }\n-\n-    if (VerifySharedSpaces && !region_crc_check(addr, regions[i].byte_size(), si->crc())) {\n-      \/\/ dealloc the regions from java heap\n-      dealloc_heap_regions(regions, num_regions);\n-      log_info(cds)(\"UseSharedSpaces: mapped heap regions are corrupt\");\n-      return false;\n-    }\n-\n-    si->set_mapped_base(base);\n-  }\n-\n-  cleanup._aborted = false;\n-  \/\/ the shared heap data is mapped successfully\n-  *regions_ret = regions;\n-  *num_regions_ret = num_regions;\n-  return true;\n-}\n-\n-void FileMapInfo::patch_heap_embedded_pointers() {\n-  if (!_heap_pointers_need_patching) {\n-    return;\n-  }\n-\n-  log_info(cds)(\"patching heap embedded pointers\");\n-  patch_heap_embedded_pointers(closed_heap_regions,\n-                               num_closed_heap_regions,\n-                               MetaspaceShared::first_closed_heap_region);\n-\n-  patch_heap_embedded_pointers(open_heap_regions,\n-                               num_open_heap_regions,\n-                               MetaspaceShared::first_open_heap_region);\n-}\n-\n-void FileMapInfo::patch_heap_embedded_pointers(MemRegion* regions, int num_regions,\n-                                               int first_region_idx) {\n-  char* bitmap_base = map_bitmap_region();\n-  assert(bitmap_base != NULL, \"must have already been mapped\");\n-  for (int i=0; i<num_regions; i++) {\n-    FileMapRegion* si = space_at(i + first_region_idx);\n-    ArchiveHeapLoader::patch_embedded_pointers(\n-      regions[i],\n-      (address)(space_at(MetaspaceShared::bm)->mapped_base()) + si->oopmap_offset(),\n-      si->oopmap_size_in_bits());\n-  }\n-}\n-\n-\/\/ This internally allocates objects using vmClasses::Object_klass(), so it\n-\/\/ must be called after the Object_klass is loaded\n-void FileMapInfo::fixup_mapped_heap_regions() {\n-  assert(vmClasses::Object_klass_loaded(), \"must be\");\n-  \/\/ If any closed regions were found, call the fill routine to make them parseable.\n-  \/\/ Note that closed_heap_regions may be non-NULL even if no regions were found.\n-  if (num_closed_heap_regions != 0) {\n-    assert(closed_heap_regions != NULL,\n-           \"Null closed_heap_regions array with non-zero count\");\n-    G1CollectedHeap::heap()->fill_archive_regions(closed_heap_regions,\n-                                                  num_closed_heap_regions);\n-    \/\/ G1 marking uses the BOT for object chunking during marking in\n-    \/\/ G1CMObjArrayProcessor::process_slice(); for this reason we need to\n-    \/\/ initialize the BOT for closed archive regions too.\n-    G1CollectedHeap::heap()->populate_archive_regions_bot_part(closed_heap_regions,\n-                                                               num_closed_heap_regions);\n-  }\n-\n-  \/\/ do the same for mapped open archive heap regions\n-  if (num_open_heap_regions != 0) {\n-    assert(open_heap_regions != NULL, \"NULL open_heap_regions array with non-zero count\");\n-    G1CollectedHeap::heap()->fill_archive_regions(open_heap_regions,\n-                                                  num_open_heap_regions);\n-\n-    \/\/ Populate the open archive regions' G1BlockOffsetTableParts. That ensures\n-    \/\/ fast G1BlockOffsetTablePart::block_start operations for any given address\n-    \/\/ within the open archive regions when trying to find start of an object\n-    \/\/ (e.g. during card table scanning).\n-    G1CollectedHeap::heap()->populate_archive_regions_bot_part(open_heap_regions,\n-                                                               num_open_heap_regions);\n-  }\n-}\n-\n-\/\/ dealloc the archive regions from java heap\n-void FileMapInfo::dealloc_heap_regions(MemRegion* regions, int num) {\n-  if (num > 0) {\n-    assert(regions != NULL, \"Null archive regions array with non-zero count\");\n-    G1CollectedHeap::heap()->dealloc_archive_regions(regions, num);\n-  }\n+char* FileMapInfo::map_region_at_address(FileMapRegion* si, char* requested_addr, size_t byte_size) const {\n+  char* base = os::map_memory(_fd, _full_path, si->file_offset(),\n+                              requested_addr, byte_size, si->read_only(),\n+                              si->allow_exec());\n+  return base;\n@@ -2484,1 +2138,0 @@\n-#endif \/\/ INCLUDE_CDS_JAVA_HEAP\n@@ -2605,2 +2258,7 @@\n-    return si->used() > 0 ?\n-          (char*)start_address_as_decoded_with_current_oop_encoding_mode(si) : NULL;\n+    if (si->used() > 0) {\n+      size_t offset = si->mapping_offset();\n+      narrowOop n = CompressedOops::narrow_oop_cast(offset);\n+      return cast_from_oop<char *>(CompressedOops::decode_raw_not_null(n));\n+    } else {\n+      return NULL;\n+    }\n@@ -2729,34 +2387,0 @@\n-\/\/ Check if a given address is within one of the shared regions\n-bool FileMapInfo::is_in_shared_region(const void* p, int idx) {\n-  assert(idx == MetaspaceShared::ro ||\n-         idx == MetaspaceShared::rw, \"invalid region index\");\n-  char* base = region_addr(idx);\n-  if (p >= base && p < base + space_at(idx)->used()) {\n-    return true;\n-  }\n-  return false;\n-}\n-\n-\/\/ Unmap mapped regions of shared space.\n-void FileMapInfo::stop_sharing_and_unmap(const char* msg) {\n-  MetaspaceShared::set_shared_metaspace_range(NULL, NULL, NULL);\n-\n-  FileMapInfo *map_info = FileMapInfo::current_info();\n-  if (map_info) {\n-    map_info->fail_continue(\"%s\", msg);\n-    for (int i = 0; i < MetaspaceShared::num_non_heap_regions; i++) {\n-      if (!HeapShared::is_heap_region(i)) {\n-        map_info->unmap_region(i);\n-      }\n-    }\n-    \/\/ Dealloc the archive heap regions only without unmapping. The regions are part\n-    \/\/ of the java heap. Unmapping of the heap regions are managed by GC.\n-    map_info->dealloc_heap_regions(open_heap_regions,\n-                                   num_open_heap_regions);\n-    map_info->dealloc_heap_regions(closed_heap_regions,\n-                                   num_closed_heap_regions);\n-  } else if (DumpSharedSpaces) {\n-    fail_stop(\"%s\", msg);\n-  }\n-}\n-\n","filename":"src\/hotspot\/share\/cds\/filemap.cpp","additions":20,"deletions":396,"binary":false,"changes":416,"status":"modified"},{"patch":"@@ -31,1 +31,1 @@\n-#include \"oops\/compressedOops.hpp\"\n+#include \"oops\/compressedOops.inline.hpp\"\n@@ -463,4 +463,0 @@\n-  void  fixup_mapped_heap_regions() NOT_CDS_JAVA_HEAP_RETURN;\n-  void  patch_heap_embedded_pointers() NOT_CDS_JAVA_HEAP_RETURN;\n-  void  patch_heap_embedded_pointers(MemRegion* regions, int num_regions,\n-                                     int first_region_idx) NOT_CDS_JAVA_HEAP_RETURN;\n@@ -468,1 +464,0 @@\n-  MemRegion get_heap_regions_range_with_current_oop_encoding_mode() NOT_CDS_JAVA_HEAP_RETURN_(MemRegion());\n@@ -488,4 +483,0 @@\n-  bool is_in_shared_region(const void* p, int idx) NOT_CDS_RETURN_(false);\n-\n-  \/\/ Stop CDS sharing and unmap CDS regions.\n-  static void stop_sharing_and_unmap(const char* msg);\n@@ -557,0 +548,13 @@\n+  char* map_region_at_address(FileMapRegion* si, char* requested_addr, size_t byte_size) const;\n+\n+  address start_address_at_dumptime(FileMapRegion* spc) {\n+    if (UseCompressedOops) {\n+      size_t offset = spc->mapping_offset();\n+      narrowOop n = CompressedOops::narrow_oop_cast(offset);\n+      uintptr_t p = ((uintptr_t)narrow_oop_base()) + ((uintptr_t)n << narrow_oop_shift());\n+      return (address)p;\n+    } else {\n+      assert(is_aligned(spc->mapping_offset(), sizeof(HeapWord)), \"must be\");\n+      return header()->heap_begin() + spc->mapping_offset();\n+    }\n+  }\n@@ -574,4 +578,0 @@\n-  bool  map_heap_regions(int first, int max, bool is_open_archive,\n-                         MemRegion** regions_ret, int* num_regions_ret) NOT_CDS_JAVA_HEAP_RETURN_(false);\n-  bool  region_crc_check(char* buf, size_t size, int expected_crc) NOT_CDS_RETURN_(false);\n-  void  dealloc_heap_regions(MemRegion* regions, int num) NOT_CDS_JAVA_HEAP_RETURN;\n@@ -579,5 +579,0 @@\n-  bool  load_heap_regions() NOT_CDS_JAVA_HEAP_RETURN_(false);\n-  bool  map_heap_regions() NOT_CDS_JAVA_HEAP_RETURN_(false);\n-  address heap_region_runtime_start_address(FileMapRegion* spc) NOT_CDS_JAVA_HEAP_RETURN_(NULL);\n-  void set_shared_heap_runtime_delta(ptrdiff_t delta) NOT_CDS_JAVA_HEAP_RETURN;\n-  void  map_heap_regions_impl() NOT_CDS_JAVA_HEAP_RETURN;\n@@ -589,6 +584,0 @@\n-  address decode_start_address(FileMapRegion* spc, bool with_current_oop_encoding_mode);\n-\n-  \/\/ The starting address of spc, as calculated with CompressedOop::decode_non_null()\n-  address start_address_as_decoded_with_current_oop_encoding_mode(FileMapRegion* spc) {\n-    return decode_start_address(spc, true);\n-  }\n@@ -596,4 +585,1 @@\n-  \/\/ The starting address of spc, as calculated with HeapShared::decode_from_archive()\n-  address start_address_as_decoded_from_archive(FileMapRegion* spc) {\n-    return decode_start_address(spc, false);\n-  }\n+  bool region_crc_check(char* buf, size_t size, int expected_crc) NOT_CDS_RETURN_(false);\n","filename":"src\/hotspot\/share\/cds\/filemap.hpp","additions":15,"deletions":29,"binary":false,"changes":44,"status":"modified"},{"patch":"@@ -275,1 +275,1 @@\n-  if (ArchiveHeapLoader::is_fully_available()) {\n+  if (ArchiveHeapLoader::is_archived_heap_available()) {\n@@ -447,1 +447,1 @@\n-  if (!ArchiveHeapLoader::is_fully_available()) {\n+  if (!ArchiveHeapLoader::is_archived_heap_available()) {\n@@ -844,1 +844,1 @@\n-      assert(ArchiveHeapLoader::is_fully_available(), \"must be\");\n+      assert(ArchiveHeapLoader::is_archived_heap_available(), \"must be\");\n@@ -899,1 +899,1 @@\n-  if (!ArchiveHeapLoader::is_fully_available()) {\n+  if (!ArchiveHeapLoader::is_archived_heap_available()) {\n@@ -932,1 +932,1 @@\n-  if (!ArchiveHeapLoader::is_fully_available()) {\n+  if (!ArchiveHeapLoader::is_archived_heap_available()) {\n","filename":"src\/hotspot\/share\/cds\/heapShared.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -248,7 +248,0 @@\n-  \/\/ UseCompressedOops only: Used by decode_from_archive\n-  static address _narrow_oop_base;\n-  static int     _narrow_oop_shift;\n-\n-  \/\/ !UseCompressedOops only: used to relocate pointers to the archived objects\n-  static ptrdiff_t _runtime_delta;\n-\n","filename":"src\/hotspot\/share\/cds\/heapShared.hpp","additions":0,"deletions":7,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -958,5 +958,0 @@\n-\/\/ Return true if given address is in the misc data region\n-bool MetaspaceShared::is_in_shared_region(const void* p, int idx) {\n-  return UseSharedSpaces && FileMapInfo::current_info()->is_in_shared_region(p, idx);\n-}\n-\n@@ -1491,1 +1486,1 @@\n-  ReadClosure rc(&array);\n+  ReadClosure rc(&array, ArchiveHeapLoader::get_oop_decoder(static_mapinfo));\n@@ -1499,1 +1494,1 @@\n-  static_mapinfo->patch_heap_embedded_pointers();\n+  ArchiveHeapLoader::patch_heap_embedded_pointers(static_mapinfo);\n","filename":"src\/hotspot\/share\/cds\/metaspaceShared.cpp","additions":2,"deletions":7,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -117,3 +117,0 @@\n-  \/\/ Return true if given address is in the shared region corresponding to the idx\n-  static bool is_in_shared_region(const void* p, int idx) NOT_CDS_RETURN_(false);\n-\n","filename":"src\/hotspot\/share\/cds\/metaspaceShared.hpp","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -1268,1 +1268,1 @@\n-  if (ArchiveHeapLoader::is_mapped()) {\n+  if (ArchiveHeapLoader::is_archived_heap_available()) {\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -75,11 +75,6 @@\n-  if (UseCompressedOops) {\n-    assert(sizeof(narrowOop) == sizeof(offset), \"must be\");\n-    narrowOop v = CompressedOops::narrow_oop_cast(offset);\n-    return ArchiveHeapLoader::decode_from_archive(v);\n-  } else {\n-    intptr_t dumptime_oop = (uintptr_t)offset;\n-    assert(dumptime_oop != 0, \"null strings cannot be interned\");\n-    intptr_t runtime_oop = dumptime_oop +\n-                           (intptr_t)FileMapInfo::current_info()->header()->heap_begin() +\n-                           (intptr_t)ArchiveHeapLoader::runtime_delta();\n-    return (oop)cast_to_oop(runtime_oop);\n+  FileMapInfo* current_info = FileMapInfo::current_info();\n+  ArchiveOopDecoder* oop_decoder = ArchiveHeapLoader::get_oop_decoder(current_info);\n+  assert(oop_decoder != NULL, \"oop decoder cannot be null for shared strings\");\n+  uintptr_t ptr = (uintptr_t)offset;\n+  if (!UseCompressedOops) {\n+    ptr += (uintptr_t)current_info->header()->heap_begin();\n@@ -87,0 +82,1 @@\n+  return oop_decoder->decode(ptr);\n@@ -860,1 +856,0 @@\n-  assert(ArchiveHeapLoader::is_loaded(), \"must be\");\n","filename":"src\/hotspot\/share\/classfile\/stringTable.cpp","additions":7,"deletions":12,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -104,1 +104,1 @@\n-HeapWord* EpsilonHeap::allocate_work(size_t size, bool verbose) {\n+HeapWord* EpsilonHeap::allocate_work(size_t size, bool verbose, size_t alignment) {\n@@ -106,0 +106,4 @@\n+  if (alignment > 0) {\n+    assert(is_object_aligned(alignment), \"Requested alignment of \" SIZE_FORMAT\n+          \" bytes is not a multiple of object alignment\", alignment);\n+  }\n@@ -110,1 +114,6 @@\n-    res = _space->par_allocate(size);\n+    if (alignment > 0) {\n+      res = _space->par_allocate_aligned(size, alignment);\n+      res = align_up(res, alignment);\n+    } else {\n+      res = _space->par_allocate(size);\n+    }\n@@ -147,0 +156,3 @@\n+  assert(alignment == 0 || is_aligned(res, alignment), \"Allocated space \" PTR_FORMAT\n+           \" does not have requested alignment of \" SIZE_FORMAT \" bytes\", p2i(res), alignment);\n+\n@@ -263,1 +275,11 @@\n-HeapWord* EpsilonHeap::allocate_loaded_archive_space(size_t size) {\n+bool EpsilonHeap::alloc_archive_regions(MemRegion* dumptime_regions, int num_regions, MemRegion* runtime_regions, bool is_open) {\n+  size_t total_size = 0;\n+  size_t alignment = os::vm_page_size();\n+\n+  for (int i = 0; i < num_regions; i++) {\n+    size_t region_size = dumptime_regions[i].byte_size();\n+    assert(is_aligned(region_size, alignment), \"region size (\" SIZE_FORMAT \" bytes) \"\n+           \"is not aligned to OS default page size\", region_size);\n+    total_size += region_size;\n+  }\n+\n@@ -265,1 +287,18 @@\n-  return allocate_work(size, \/* verbose = *\/false);\n+  HeapWord* result = allocate_work(total_size \/ HeapWordSize, false, alignment);\n+  if (result == NULL) {\n+    return false;\n+  }\n+\n+  for (int i = 0; i < num_regions; i++) {\n+    size_t word_size = dumptime_regions[i].word_size();\n+    MemRegion* curr_range = &runtime_regions[i];\n+    if (i == 0) {\n+      curr_range->set_start(result);\n+    } else {\n+      \/\/ next range should be aligned to page size\n+      curr_range->set_start(runtime_regions[i-1].end());\n+    }\n+    assert(is_aligned(curr_range->start(), alignment), \"region does not start at OS default page size\");\n+    curr_range->set_end(curr_range->start() + word_size);\n+  }\n+  return true;\n","filename":"src\/hotspot\/share\/gc\/epsilon\/epsilonHeap.cpp","additions":43,"deletions":4,"binary":false,"changes":47,"status":"modified"},{"patch":"@@ -93,1 +93,1 @@\n-  HeapWord* allocate_work(size_t size, bool verbose = true);\n+  HeapWord* allocate_work(size_t size, bool verbose = true, size_t alignment = 0);\n@@ -136,2 +136,1 @@\n-  virtual bool can_load_archived_objects() const { return UseCompressedOops; }\n-  virtual HeapWord* allocate_loaded_archive_space(size_t size);\n+  virtual bool alloc_archive_regions(MemRegion* dumptime_regions, int num_regions, MemRegion* runtime_regions, bool is_open);\n","filename":"src\/hotspot\/share\/gc\/epsilon\/epsilonHeap.hpp","additions":2,"deletions":3,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -541,18 +541,1 @@\n-bool G1CollectedHeap::check_archive_addresses(MemRegion* ranges, size_t count) {\n-  assert(ranges != NULL, \"MemRegion array NULL\");\n-  assert(count != 0, \"No MemRegions provided\");\n-  MemRegion reserved = _hrm.reserved();\n-  for (size_t i = 0; i < count; i++) {\n-    if (!reserved.contains(ranges[i].start()) || !reserved.contains(ranges[i].last())) {\n-      return false;\n-    }\n-  }\n-  return true;\n-}\n-\n-bool G1CollectedHeap::alloc_archive_regions(MemRegion* ranges,\n-                                            size_t count,\n-                                            bool open) {\n-  assert(!is_init_completed(), \"Expect to be called at JVM init time\");\n-  assert(ranges != NULL, \"MemRegion array NULL\");\n-  assert(count != 0, \"No MemRegions provided\");\n+bool G1CollectedHeap::alloc_archive_regions(MemRegion* dumptime_regions, int num_regions, MemRegion* runtime_regions, bool is_open) {\n@@ -560,5 +543,0 @@\n-\n-  MemRegion reserved = _hrm.reserved();\n-  HeapWord* prev_last_addr = NULL;\n-  HeapRegion* prev_last_region = NULL;\n-\n@@ -569,17 +547,2 @@\n-  \/\/ For each specified MemRegion range, allocate the corresponding G1\n-  \/\/ regions and mark them as archive regions. We expect the ranges\n-  \/\/ in ascending starting address order, without overlap.\n-  for (size_t i = 0; i < count; i++) {\n-    MemRegion curr_range = ranges[i];\n-    HeapWord* start_address = curr_range.start();\n-    size_t word_size = curr_range.word_size();\n-    HeapWord* last_address = curr_range.last();\n-    size_t commits = 0;\n-\n-    guarantee(reserved.contains(start_address) && reserved.contains(last_address),\n-              \"MemRegion outside of heap [\" PTR_FORMAT \", \" PTR_FORMAT \"]\",\n-              p2i(start_address), p2i(last_address));\n-    guarantee(start_address > prev_last_addr,\n-              \"Ranges not in ascending order: \" PTR_FORMAT \" <= \" PTR_FORMAT ,\n-              p2i(start_address), p2i(prev_last_addr));\n-    prev_last_addr = last_address;\n+  size_t total_size = 0;\n+  size_t alignment = os::vm_page_size();\n@@ -587,11 +550,25 @@\n-    \/\/ Check for ranges that start in the same G1 region in which the previous\n-    \/\/ range ended, and adjust the start address so we don't try to allocate\n-    \/\/ the same region again. If the current range is entirely within that\n-    \/\/ region, skip it, just adjusting the recorded top.\n-    HeapRegion* start_region = _hrm.addr_to_region(start_address);\n-    if ((prev_last_region != NULL) && (start_region == prev_last_region)) {\n-      start_address = start_region->end();\n-      if (start_address > last_address) {\n-        increase_used(word_size * HeapWordSize);\n-        start_region->set_top(last_address + 1);\n-        continue;\n+  for (int i = 0; i < num_regions; i++) {\n+    size_t region_size = dumptime_regions[i].byte_size();\n+    assert(is_aligned(region_size, alignment), \"region size (\" SIZE_FORMAT \" bytes) \"\n+           \"is not aligned to OS default page size\", region_size);\n+    total_size += region_size;\n+  }\n+  size_t allocated = 0;\n+  HeapRegion* hr = NULL;\n+  HeapWord* mem_end = NULL;\n+  HeapWord* mem_begin = NULL;\n+\n+  \/\/ start allocating heap regions from the heap end\n+  do {\n+    hr = alloc_highest_free_region();\n+    if (hr == NULL) {\n+      int shrink_count = 0;\n+      \/\/ deallocate previously allocated regions\n+      while (mem_begin != mem_end) {\n+        HeapRegion* curr_region = _hrm.addr_to_region(mem_begin);\n+        _hrm.shrink_at(curr_region->hrm_index(), 1);\n+        mem_begin += HeapRegion::GrainWords;\n+        shrink_count += 1;\n+      }\n+      if (shrink_count > 0) {\n+        uncommit_regions(shrink_count);\n@@ -599,8 +576,0 @@\n-      start_region->set_top(start_address);\n-      curr_range = MemRegion(start_address, last_address + 1);\n-      start_region = _hrm.addr_to_region(start_address);\n-    }\n-\n-    \/\/ Perform the actual region allocation, exiting if it fails.\n-    \/\/ Then note how much new space we have allocated.\n-    if (!_hrm.allocate_containing_regions(curr_range, &commits, workers())) {\n@@ -609,4 +578,14 @@\n-    increase_used(word_size * HeapWordSize);\n-    if (commits != 0) {\n-      log_debug(gc, ergo, heap)(\"Attempt heap expansion (allocate archive regions). Total size: \" SIZE_FORMAT \"B\",\n-                                HeapRegion::GrainWords * HeapWordSize * commits);\n+    mem_begin = hr->bottom();\n+    if (allocated == 0) { \/\/ first allocated region\n+      mem_end = hr->end();\n+    }\n+    \/\/ Mark each region as archive\n+    if (is_open) {\n+      hr->set_open_archive();\n+    } else {\n+      hr->set_closed_archive();\n+    }\n+    allocated += HeapRegion::GrainBytes;\n+  } while (allocated < total_size);\n+\n+  mem_begin = hr->bottom();\n@@ -614,0 +593,9 @@\n+  HeapRegion* prev_range_last_region = NULL;\n+  for (int i = 0; i < num_regions; i++) {\n+    size_t word_size = dumptime_regions[i].word_size();\n+    MemRegion* curr_range = &runtime_regions[i];\n+    if (i == 0) {\n+      curr_range->set_start(mem_begin);\n+    } else {\n+      \/\/ next range should be aligned to page size\n+      curr_range->set_start(align_up(runtime_regions[i-1].end(), alignment));\n@@ -615,0 +603,2 @@\n+    assert(is_aligned(curr_range->start(), alignment), \"region does not start at OS default page size\");\n+    curr_range->set_end(curr_range->start() + word_size);\n@@ -616,3 +606,3 @@\n-    \/\/ Mark each G1 region touched by the range as archive, add it to\n-    \/\/ the old set, and set top.\n-    HeapRegion* curr_region = _hrm.addr_to_region(start_address);\n+    \/\/ Add each G1 region touched by the range to the old set, and set the top.\n+    HeapRegion* curr_region = _hrm.addr_to_region(curr_range->start());\n+    HeapWord* last_address = curr_range->last();\n@@ -620,1 +610,0 @@\n-    prev_last_region = last_region;\n@@ -623,6 +612,3 @@\n-      assert(curr_region->is_empty() && !curr_region->is_pinned(),\n-             \"Region already in use (index %u)\", curr_region->hrm_index());\n-      if (open) {\n-        curr_region->set_open_archive();\n-      } else {\n-        curr_region->set_closed_archive();\n+      if (curr_region != prev_range_last_region) {\n+        _hr_printer.alloc(curr_region);\n+        _archive_set.add(curr_region);\n@@ -630,4 +616,0 @@\n-      _hr_printer.alloc(curr_region);\n-      _archive_set.add(curr_region);\n-      HeapWord* top;\n-      HeapRegion* next_region;\n@@ -635,2 +617,2 @@\n-        top = curr_region->end();\n-        next_region = _hrm.next_region_in_heap(curr_region);\n+        curr_region->set_top(curr_region->end());\n+        curr_region = _hrm.next_region_in_heap(curr_region);\n@@ -638,2 +620,2 @@\n-        top = last_address + 1;\n-        next_region = NULL;\n+        curr_region->set_top(last_address + 1);\n+        curr_region = NULL;\n@@ -641,2 +623,0 @@\n-      curr_region->set_top(top);\n-      curr_region = next_region;\n@@ -644,0 +624,1 @@\n+    prev_range_last_region = last_region;\n@@ -645,0 +626,2 @@\n+\n+  increase_used(total_size);\n@@ -648,2 +631,1 @@\n-void G1CollectedHeap::fill_archive_regions(MemRegion* ranges, size_t count) {\n-  assert(!is_init_completed(), \"Expect to be called at JVM init time\");\n+bool G1CollectedHeap::check_archive_addresses(MemRegion* ranges, size_t count) {\n@@ -653,8 +635,0 @@\n-  HeapWord *prev_last_addr = NULL;\n-  HeapRegion* prev_last_region = NULL;\n-\n-  \/\/ For each MemRegion, create filler objects, if needed, in the G1 regions\n-  \/\/ that contain the address range. The address range actually within the\n-  \/\/ MemRegion will not be modified. That is assumed to have been initialized\n-  \/\/ elsewhere, probably via an mmap of archived heap data.\n-  MutexLocker x(Heap_lock);\n@@ -662,44 +636,2 @@\n-    HeapWord* start_address = ranges[i].start();\n-    HeapWord* last_address = ranges[i].last();\n-\n-    assert(reserved.contains(start_address) && reserved.contains(last_address),\n-           \"MemRegion outside of heap [\" PTR_FORMAT \", \" PTR_FORMAT \"]\",\n-           p2i(start_address), p2i(last_address));\n-    assert(start_address > prev_last_addr,\n-           \"Ranges not in ascending order: \" PTR_FORMAT \" <= \" PTR_FORMAT ,\n-           p2i(start_address), p2i(prev_last_addr));\n-\n-    HeapRegion* start_region = _hrm.addr_to_region(start_address);\n-    HeapRegion* last_region = _hrm.addr_to_region(last_address);\n-    HeapWord* bottom_address = start_region->bottom();\n-\n-    \/\/ Check for a range beginning in the same region in which the\n-    \/\/ previous one ended.\n-    if (start_region == prev_last_region) {\n-      bottom_address = prev_last_addr + 1;\n-    }\n-\n-    \/\/ Verify that the regions were all marked as archive regions by\n-    \/\/ alloc_archive_regions.\n-    HeapRegion* curr_region = start_region;\n-    while (curr_region != NULL) {\n-      guarantee(curr_region->is_archive(),\n-                \"Expected archive region at index %u\", curr_region->hrm_index());\n-      if (curr_region != last_region) {\n-        curr_region = _hrm.next_region_in_heap(curr_region);\n-      } else {\n-        curr_region = NULL;\n-      }\n-    }\n-\n-    prev_last_addr = last_address;\n-    prev_last_region = last_region;\n-\n-    \/\/ Fill the memory below the allocated range with dummy object(s),\n-    \/\/ if the region bottom does not match the range start, or if the previous\n-    \/\/ range ended within the same G1 region, and there is a gap.\n-    assert(start_address >= bottom_address, \"bottom address should not be greater than start address\");\n-    if (start_address > bottom_address) {\n-      size_t fill_size = pointer_delta(start_address, bottom_address);\n-      G1CollectedHeap::fill_with_objects(bottom_address, fill_size);\n-      increase_used(fill_size * HeapWordSize);\n+    if (!reserved.contains(ranges[i].start()) || !reserved.contains(ranges[i].last())) {\n+      return false;\n@@ -708,0 +640,1 @@\n+  return true;\n@@ -735,0 +668,4 @@\n+void G1CollectedHeap::complete_archive_regions_alloc(MemRegion* regions, int num_regions) {\n+  populate_archive_regions_bot_part(regions, num_regions);\n+}\n+\n@@ -756,1 +693,1 @@\n-void G1CollectedHeap::dealloc_archive_regions(MemRegion* ranges, size_t count) {\n+bool G1CollectedHeap::dealloc_archive_regions_impl(MemRegion* ranges, int num_regions) {\n@@ -759,1 +696,1 @@\n-  assert(count != 0, \"No MemRegions provided\");\n+  assert(num_regions != 0, \"No MemRegions provided\");\n@@ -769,1 +706,1 @@\n-  for (size_t i = 0; i < count; i++) {\n+  for (int i = 0; i < num_regions; i++) {\n@@ -826,0 +763,1 @@\n+  return true;\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectedHeap.cpp","additions":78,"deletions":140,"binary":false,"changes":218,"status":"modified"},{"patch":"@@ -513,0 +513,8 @@\n+  \/\/ Populate the G1BlockOffsetTablePart for archived regions with the given\n+  \/\/ memory ranges.\n+  void populate_archive_regions_bot_part(MemRegion* range, size_t count);\n+\n+  bool dealloc_archive_regions_impl(MemRegion* range, int num_regions) override;\n+\n+  virtual bool heap_region_dealloc_supported() override { return true; }\n+\n@@ -704,0 +712,8 @@\n+  virtual bool can_archive_regions_be_collected() override { return false; }\n+\n+  \/\/ Commit the appropriate G1 regions containing the specified MemRegions\n+  \/\/ and mark them as 'archive' regions. The regions in the array must be\n+  \/\/ non-overlapping and in order of ascending address.\n+  virtual bool alloc_archive_regions(MemRegion* dumptime_regions, int num_regions, MemRegion* runtime_regions, bool is_open) override;\n+  virtual void complete_archive_regions_alloc(MemRegion* regions, int num_regions) override;\n+\n@@ -711,20 +727,0 @@\n-  \/\/ Commit the appropriate G1 regions containing the specified MemRegions\n-  \/\/ and mark them as 'archive' regions. The regions in the array must be\n-  \/\/ non-overlapping and in order of ascending address.\n-  bool alloc_archive_regions(MemRegion* range, size_t count, bool open);\n-\n-  \/\/ Insert any required filler objects in the G1 regions around the specified\n-  \/\/ ranges to make the regions parseable. This must be called after\n-  \/\/ alloc_archive_regions, and after class loading has occurred.\n-  void fill_archive_regions(MemRegion* range, size_t count);\n-\n-  \/\/ Populate the G1BlockOffsetTablePart for archived regions with the given\n-  \/\/ memory ranges.\n-  void populate_archive_regions_bot_part(MemRegion* range, size_t count);\n-\n-  \/\/ For each of the specified MemRegions, uncommit the containing G1 regions\n-  \/\/ which had been allocated by alloc_archive_regions. This should be called\n-  \/\/ rather than fill_archive_regions at JVM init time if the archive file\n-  \/\/ mapping failed, with the same non-overlapping and sorted MemRegion array.\n-  void dealloc_archive_regions(MemRegion* range, size_t count);\n-\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectedHeap.hpp","additions":16,"deletions":20,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -846,0 +846,7 @@\n+HeapWord* MutableNUMASpace::cas_allocate_aligned(size_t size, size_t alignment) {\n+  \/\/ MutableNUMASpace is never used for allocating archived regions,\n+  \/\/ and this API is used only for allocating archived regions.\n+  ShouldNotReachHere();\n+  return NULL;\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/parallel\/mutableNUMASpace.cpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -231,0 +231,1 @@\n+  virtual HeapWord* cas_allocate_aligned(size_t word_size, size_t alignment);\n","filename":"src\/hotspot\/share\/gc\/parallel\/mutableNUMASpace.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -211,0 +211,29 @@\n+HeapWord* MutableSpace::cas_allocate_aligned(size_t size, size_t alignment) {\n+  do {\n+    \/\/ Read top before end, else the range check may pass when it shouldn't.\n+    \/\/ If end is read first, other threads may advance end and top such that\n+    \/\/ current top > old end and current top + size > current end.  Then\n+    \/\/ pointer_delta underflows, allowing installation of top > current end.\n+    HeapWord* curr_top = Atomic::load_acquire(top_addr());\n+    HeapWord* obj = align_up(curr_top, alignment);\n+    if (pointer_delta(end(), obj) >= size) {\n+      HeapWord* new_top = obj + size;\n+      HeapWord* result = Atomic::cmpxchg(top_addr(), curr_top, new_top);\n+      \/\/ result can be one of two:\n+      \/\/  the old top value: the exchange succeeded\n+      \/\/  otherwise: the new value of the top is returned.\n+      if (result != curr_top) {\n+        continue; \/\/ another thread beat us to the allocation, try again\n+      }\n+      assert(is_object_aligned(obj) && is_object_aligned(new_top),\n+             \"checking alignment\");\n+      if (curr_top != obj) {\n+        CollectedHeap::fill_with_object(curr_top, obj);\n+      }\n+      return curr_top;\n+    } else {\n+      return NULL;\n+    }\n+  } while (true);\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/parallel\/mutableSpace.cpp","additions":29,"deletions":0,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -143,0 +143,1 @@\n+  virtual HeapWord* cas_allocate_aligned(size_t size, size_t alignment);\n","filename":"src\/hotspot\/share\/gc\/parallel\/mutableSpace.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -805,2 +805,30 @@\n-HeapWord* ParallelScavengeHeap::allocate_loaded_archive_space(size_t size) {\n-  return _old_gen->allocate(size);\n+bool ParallelScavengeHeap::alloc_archive_regions(MemRegion* dumptime_regions, int num_regions, MemRegion* runtime_regions, bool is_open) {\n+  size_t total_size = 0;\n+  size_t alignment = os::vm_page_size();\n+\n+  for (int i = 0; i < num_regions; i++) {\n+    size_t region_size = dumptime_regions[i].byte_size();\n+    assert(is_aligned(region_size, alignment), \"region size (\" SIZE_FORMAT \" bytes) \"\n+           \"is not aligned to OS default page size\", region_size);\n+    total_size += region_size;\n+  }\n+\n+  \/\/ Cannot use verbose=true because Metaspace is not initialized\n+  HeapWord* result = _old_gen->allocate_aligned(total_size \/ HeapWordSize, alignment);\n+  if (result == NULL) {\n+    return false;\n+  }\n+\n+  for (int i = 0; i < num_regions; i++) {\n+    size_t word_size = dumptime_regions[i].word_size();\n+    MemRegion* curr_range = &runtime_regions[i];\n+    if (i == 0) {\n+      curr_range->set_start(result);\n+    } else {\n+      \/\/ next range should be aligned to page size\n+      curr_range->set_start(runtime_regions[i-1].end());\n+    }\n+    assert(is_aligned(curr_range->start(), alignment), \"region does not start at OS default page size\");\n+    curr_range->set_end(curr_range->start() + word_size);\n+  }\n+  return true;\n@@ -809,4 +837,4 @@\n-void ParallelScavengeHeap::complete_loaded_archive_space(MemRegion archive_space) {\n-  assert(_old_gen->object_space()->used_region().contains(archive_space),\n-         \"Archive space not contained in old gen\");\n-  _old_gen->complete_loaded_archive_space(archive_space);\n+void ParallelScavengeHeap::complete_archive_regions_alloc(MemRegion* regions, int num_regions) {\n+  for (int i = 0; i < num_regions; i++) {\n+    _old_gen->complete_archive_region_alloc(regions[i]);\n+  }\n","filename":"src\/hotspot\/share\/gc\/parallel\/parallelScavengeHeap.cpp","additions":34,"deletions":6,"binary":false,"changes":40,"status":"modified"},{"patch":"@@ -273,3 +273,2 @@\n-  bool can_load_archived_objects() const { return UseCompressedOops; }\n-  HeapWord* allocate_loaded_archive_space(size_t size);\n-  void complete_loaded_archive_space(MemRegion archive_space);\n+  virtual bool alloc_archive_regions(MemRegion* dumptime_regions, int num_regions, MemRegion* runtime_regions, bool is_open);\n+  virtual void complete_archive_regions_alloc(MemRegion* regions, int num_regions);\n","filename":"src\/hotspot\/share\/gc\/parallel\/parallelScavengeHeap.hpp","additions":2,"deletions":3,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -286,1 +286,1 @@\n-void PSOldGen::complete_loaded_archive_space(MemRegion archive_space) {\n+void PSOldGen::complete_archive_region_alloc(MemRegion archive_space) {\n","filename":"src\/hotspot\/share\/gc\/parallel\/psOldGen.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -81,0 +81,10 @@\n+  HeapWord* cas_allocate_aligned_noexpand(size_t word_size, size_t alignment) {\n+    assert_locked_or_safepoint(Heap_lock);\n+    HeapWord* res = object_space()->cas_allocate_aligned(word_size, alignment);\n+    if (res != NULL) {\n+      DEBUG_ONLY(assert_block_in_covered_region(MemRegion(res, word_size)));\n+      _start_array.allocate_block(res);\n+    }\n+    return align_up(res, alignment);\n+  }\n+\n@@ -137,1 +147,1 @@\n-  void complete_loaded_archive_space(MemRegion archive_space);\n+  void complete_archive_region_alloc(MemRegion archive_space);\n@@ -151,0 +161,9 @@\n+  HeapWord* allocate_aligned(size_t word_size, size_t alignment) {\n+    HeapWord* res;\n+    do {\n+      res = cas_allocate_aligned_noexpand(word_size, alignment);\n+      \/\/ Retry failed allocation if expand succeeds.\n+    } while ((res == nullptr) && expand_for_allocate(word_size));\n+    return res;\n+  }\n+\n","filename":"src\/hotspot\/share\/gc\/parallel\/psOldGen.hpp","additions":20,"deletions":1,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -117,3 +117,29 @@\n-HeapWord* SerialHeap::allocate_loaded_archive_space(size_t word_size) {\n-  MutexLocker ml(Heap_lock);\n-  return old_gen()->allocate(word_size, false \/* is_tlab *\/);\n+bool SerialHeap::alloc_archive_regions(MemRegion* dumptime_regions, int num_regions, MemRegion* runtime_regions, bool is_open) {\n+  size_t total_size = 0;\n+  size_t alignment = os::vm_page_size();\n+\n+  for (int i = 0; i < num_regions; i++) {\n+    size_t region_size = dumptime_regions[i].byte_size();\n+    assert(is_aligned(region_size, alignment), \"region size (\" SIZE_FORMAT \" bytes) \"\n+           \"is not aligned to OS default page size\", region_size);\n+    total_size += region_size;\n+  }\n+\n+  HeapWord* result = old_gen()->par_allocate_aligned(total_size \/ HeapWordSize, alignment, false);\n+  if (result == NULL) {\n+    return false;\n+  }\n+\n+  for (int i = 0; i < num_regions; i++) {\n+    size_t word_size = dumptime_regions[i].word_size();\n+    MemRegion* curr_range = &runtime_regions[i];\n+    if (i == 0) {\n+      curr_range->set_start(result);\n+    } else {\n+      \/\/ next range should be aligned to page size\n+      curr_range->set_start(runtime_regions[i-1].end());\n+    }\n+    assert(is_aligned(curr_range->start(), alignment), \"region does not start at OS default page size\");\n+    curr_range->set_end(curr_range->start() + word_size);\n+  }\n+  return true;\n@@ -122,3 +148,5 @@\n-void SerialHeap::complete_loaded_archive_space(MemRegion archive_space) {\n-  assert(old_gen()->used_region().contains(archive_space), \"Archive space not contained in old gen\");\n-  old_gen()->complete_loaded_archive_space(archive_space);\n+void SerialHeap::complete_archive_regions_alloc(MemRegion* regions, int num_regions) {\n+  for (int i = 0; i < num_regions; i++) {\n+    assert(old_gen()->used_region().contains(regions[i]), \"Archive space not contained in old gen\");\n+    old_gen()->complete_archive_region_alloc(regions[i]);\n+  }\n","filename":"src\/hotspot\/share\/gc\/serial\/serialHeap.cpp","additions":34,"deletions":6,"binary":false,"changes":40,"status":"modified"},{"patch":"@@ -108,3 +108,2 @@\n-  bool can_load_archived_objects() const { return UseCompressedOops; }\n-  HeapWord* allocate_loaded_archive_space(size_t size);\n-  void complete_loaded_archive_space(MemRegion archive_space);\n+  virtual bool alloc_archive_regions(MemRegion* dumptime_regions, int num_regions, MemRegion* runtime_regions, bool is_open);\n+  virtual void complete_archive_regions_alloc(MemRegion* regions, int num_regions);\n","filename":"src\/hotspot\/share\/gc\/serial\/serialHeap.hpp","additions":2,"deletions":3,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -487,1 +487,1 @@\n-void TenuredGeneration::complete_loaded_archive_space(MemRegion archive_space) {\n+void TenuredGeneration::complete_archive_region_alloc(MemRegion archive_space) {\n@@ -490,1 +490,0 @@\n-  space->initialize_threshold();\n","filename":"src\/hotspot\/share\/gc\/serial\/tenuredGeneration.cpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -126,1 +126,1 @@\n-  void complete_loaded_archive_space(MemRegion archive_space);\n+  void complete_archive_region_alloc(MemRegion archive_space);\n@@ -130,0 +130,1 @@\n+  virtual inline HeapWord* par_allocate_aligned(size_t word_size, size_t alignment, bool is_tlab);\n","filename":"src\/hotspot\/share\/gc\/serial\/tenuredGeneration.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -68,0 +68,7 @@\n+HeapWord* TenuredGeneration::par_allocate_aligned(size_t word_size,\n+                                                     size_t alignment,\n+                                                     bool is_tlab) {\n+  assert(!is_tlab, \"TenuredGeneration does not support TLAB allocation\");\n+  return _the_space->par_allocate_aligned(word_size, alignment);\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/serial\/tenuredGeneration.inline.hpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -26,0 +26,1 @@\n+#include \"cds\/archiveHeapLoader.hpp\"\n@@ -652,1 +653,1 @@\n-  return false;\n+  return ArchiveHeapLoader::is_archived_object(object);\n@@ -666,0 +667,13 @@\n+\n+bool CollectedHeap::dealloc_archive_regions(MemRegion* range, int num_regions) {\n+  if (heap_region_dealloc_supported()) {\n+    return dealloc_archive_regions_impl(range, num_regions);\n+  }\n+  return false;\n+}\n+\n+void CollectedHeap::fill_heap_regions(MemRegion* range, int num_regions) {\n+  for (int i = 0; i < num_regions; i++) {\n+    fill_with_objects(range[i].start(), range[i].word_size());\n+  }\n+}\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.cpp","additions":15,"deletions":1,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -422,0 +422,10 @@\n+  \/\/ For each of the specified MemRegions, deallocate the memory which had\n+  \/\/ been allocated by alloc_archive_regions.\n+  virtual bool dealloc_archive_regions_impl(MemRegion* range, int num_regions) {\n+    return false;\n+  }\n+\n+  virtual bool heap_region_dealloc_supported() {\n+    return false;\n+  }\n+\n@@ -515,5 +525,6 @@\n-  \/\/ Support for loading objects from CDS archive into the heap\n-  \/\/ (usually as a snapshot of the old generation).\n-  virtual bool can_load_archived_objects() const { return false; }\n-  virtual HeapWord* allocate_loaded_archive_space(size_t size) { return NULL; }\n-  virtual void complete_loaded_archive_space(MemRegion archive_space) { }\n+  \/\/ Support for mapping archive regions into the heap\n+  virtual bool alloc_archive_regions(MemRegion* dumptime_regions, int num_regions, MemRegion* runtime_regions, bool is_open) { return false; }\n+  virtual void complete_archive_regions_alloc(MemRegion* regions, int num_regions) { return; }\n+  bool dealloc_archive_regions(MemRegion* range, int num_regions);\n+  virtual void fill_heap_regions(MemRegion* range, int num_regions);\n+  virtual bool can_archive_regions_be_collected() { return true; }\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.hpp","additions":16,"deletions":5,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -695,36 +695,0 @@\n-\/\/ This version requires locking.\n-inline HeapWord* ContiguousSpace::allocate_impl(size_t size) {\n-  assert(Heap_lock->owned_by_self() ||\n-         (SafepointSynchronize::is_at_safepoint() && Thread::current()->is_VM_thread()),\n-         \"not locked\");\n-  HeapWord* obj = top();\n-  if (pointer_delta(end(), obj) >= size) {\n-    HeapWord* new_top = obj + size;\n-    set_top(new_top);\n-    assert(is_aligned(obj) && is_aligned(new_top), \"checking alignment\");\n-    return obj;\n-  } else {\n-    return NULL;\n-  }\n-}\n-\n-\/\/ This version is lock-free.\n-inline HeapWord* ContiguousSpace::par_allocate_impl(size_t size) {\n-  do {\n-    HeapWord* obj = top();\n-    if (pointer_delta(end(), obj) >= size) {\n-      HeapWord* new_top = obj + size;\n-      HeapWord* result = Atomic::cmpxchg(top_addr(), obj, new_top);\n-      \/\/ result can be one of two:\n-      \/\/  the old top value: the exchange succeeded\n-      \/\/  otherwise: the new value of the top is returned.\n-      if (result == obj) {\n-        assert(is_aligned(obj) && is_aligned(new_top), \"checking alignment\");\n-        return obj;\n-      }\n-    } else {\n-      return NULL;\n-    }\n-  } while (true);\n-}\n-\n@@ -741,0 +705,6 @@\n+\/\/ Lock-free with alignment constraint.\n+HeapWord* ContiguousSpace::par_allocate_aligned(size_t size, size_t alignment) {\n+  HeapWord* res = par_allocate_aligned_impl(size, alignment);\n+  return align_up(res, alignment);\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shared\/space.cpp","additions":6,"deletions":36,"binary":false,"changes":42,"status":"modified"},{"patch":"@@ -217,0 +217,4 @@\n+  \/\/ Allocation (return NULL if full).  Enforces mutual exclusion internally.\n+  \/\/ Returned address is aligned to \"alignment\" bytes.\n+  virtual HeapWord* par_allocate_aligned(size_t word_size, size_t alignment) = 0;\n+\n@@ -436,0 +440,1 @@\n+  inline HeapWord* par_allocate_aligned_impl(size_t word_size, size_t alignment);\n@@ -487,0 +492,1 @@\n+  virtual HeapWord* par_allocate_aligned(size_t word_size, size_t alignment);\n@@ -631,0 +637,1 @@\n+  virtual inline HeapWord* par_allocate_aligned(size_t word_size, size_t alignment);\n","filename":"src\/hotspot\/share\/gc\/shared\/space.hpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -46,0 +46,60 @@\n+\/\/ This version requires locking.\n+inline HeapWord* ContiguousSpace::allocate_impl(size_t size) {\n+  assert(Heap_lock->owned_by_self() ||\n+         (SafepointSynchronize::is_at_safepoint() && Thread::current()->is_VM_thread()),\n+         \"not locked\");\n+  HeapWord* obj = top();\n+  if (pointer_delta(end(), obj) >= size) {\n+    HeapWord* new_top = obj + size;\n+    set_top(new_top);\n+    assert(is_aligned(obj) && is_aligned(new_top), \"checking alignment\");\n+    return obj;\n+  } else {\n+    return NULL;\n+  }\n+}\n+\n+\/\/ This version is lock-free.\n+inline HeapWord* ContiguousSpace::par_allocate_impl(size_t size) {\n+  do {\n+    HeapWord* obj = top();\n+    if (pointer_delta(end(), obj) >= size) {\n+      HeapWord* new_top = obj + size;\n+      HeapWord* result = Atomic::cmpxchg(top_addr(), obj, new_top);\n+      \/\/ result can be one of two:\n+      \/\/  the old top value: the exchange succeeded\n+      \/\/  otherwise: the new value of the top is returned.\n+      if (result == obj) {\n+        assert(is_aligned(obj) && is_aligned(new_top), \"checking alignment\");\n+        return obj;\n+      }\n+    } else {\n+      return NULL;\n+    }\n+  } while (true);\n+}\n+\n+\/\/ This version is lock-free.\n+inline HeapWord* ContiguousSpace::par_allocate_aligned_impl(size_t size, size_t alignment) {\n+  do {\n+    HeapWord* curr_top = top();\n+    HeapWord* obj = align_up(curr_top, alignment);\n+    if (pointer_delta(end(), obj) >= size) {\n+      HeapWord* new_top = obj + size;\n+      HeapWord* result = Atomic::cmpxchg(top_addr(), curr_top, new_top);\n+      \/\/ result can be one of two:\n+      \/\/  the old top value: the exchange succeeded\n+      \/\/  otherwise: the new value of the top is returned.\n+      if (result == curr_top) {\n+        assert(is_aligned(obj) && is_aligned(new_top), \"checking alignment\");\n+        if (curr_top != obj) {\n+          CollectedHeap::fill_with_object(curr_top, obj);\n+        }\n+        return curr_top;\n+      }\n+    } else {\n+      return NULL;\n+    }\n+  } while (true);\n+}\n+\n@@ -75,0 +135,10 @@\n+inline HeapWord* OffsetTableContigSpace::par_allocate_aligned(size_t size, size_t alignment) {\n+  MutexLocker x(&_par_alloc_lock);\n+  HeapWord* res = ContiguousSpace::par_allocate_aligned(size, alignment);\n+  if (res != NULL) {\n+    size_t actual_size = size + align_up(res, alignment) - res;\n+    _offsets.alloc_block(res, actual_size);\n+  }\n+  return res;\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shared\/space.inline.hpp","additions":70,"deletions":0,"binary":false,"changes":70,"status":"modified"},{"patch":"@@ -807,1 +807,1 @@\n-    if (ArchiveHeapLoader::is_loaded()) {\n+    if (Universe::heap()->can_archive_regions_be_collected()) {\n","filename":"src\/hotspot\/share\/memory\/universe.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -335,1 +335,1 @@\n-    if (ArchiveHeapLoader::is_fully_available() &&\n+    if (ArchiveHeapLoader::is_archived_heap_available() &&\n","filename":"src\/hotspot\/share\/oops\/constantPool.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -229,1 +229,2 @@\n-  assert(!Universe::heap()->is_archived_object(forwardee) && !Universe::heap()->is_archived_object(this),\n+  assert(Universe::heap()->can_archive_regions_be_collected() ||\n+        (!Universe::heap()->is_archived_object(forwardee) && !Universe::heap()->is_archived_object(this)),\n","filename":"src\/hotspot\/share\/oops\/oop.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -144,1 +144,1 @@\n-        static final Pattern SECTION_START_PATT = Pattern.compile(\"^([a-f0-9]+)-([a-f0-9]+) [\\\\-rwpsx]{4}.*\");\n+        static final Pattern SECTION_START_PATT = Pattern.compile(\"^([a-f0-9]+)-([a-f0-9]+) [\\\\-rwpsx]{4} [a-f0-9]+ [a-f0-9]{2}:[a-f0-9]{2} [a-f0-9]+[\\\\s]+(.*)\");\n@@ -149,0 +149,1 @@\n+        String file;\n@@ -162,1 +163,1 @@\n-                RangeWithPageSize range = new RangeWithPageSize(start, end, ps, vmFlags);\n+                RangeWithPageSize range = new RangeWithPageSize(start, end, file, ps, vmFlags);\n@@ -176,0 +177,1 @@\n+                file = matSectionStart.group(3);\n@@ -292,0 +294,3 @@\n+                    } else if (pageSizeFromTrace > pageSizeFromSmaps && !range.getBackingFile().isEmpty()) {\n+                        \/\/ For file mapping, the page size can mismatch\n+                        debug(\"Success: \" + pageSizeFromTrace + \" > \" + pageSizeFromSmaps + \" and is a file mapping\");\n@@ -328,0 +333,1 @@\n+    private String backingFile;\n@@ -332,1 +338,1 @@\n-    public RangeWithPageSize(String start, String end, String pageSize, String vmFlags) {\n+    public RangeWithPageSize(String start, String end, String file, String pageSize, String vmFlags) {\n@@ -337,0 +343,1 @@\n+        this.backingFile = file;\n@@ -353,0 +360,4 @@\n+    public String getBackingFile() {\n+        return backingFile;\n+    }\n+\n","filename":"test\/hotspot\/jtreg\/runtime\/os\/TestTracePageSizes.java","additions":14,"deletions":3,"binary":false,"changes":17,"status":"modified"}]}