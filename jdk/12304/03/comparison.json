{"files":[{"patch":"@@ -27,0 +27,1 @@\n+#include \"cds\/archiveHeapWriter.hpp\"\n@@ -833,3 +834,1 @@\n-\/\/ Update a Java object to point its Klass* to the address whene\n-\/\/ the class would be mapped at runtime.\n-void ArchiveBuilder::relocate_klass_ptr_of_oop(oop o) {\n+narrowKlass ArchiveBuilder::get_requested_narrow_klass(Klass* k) {\n@@ -837,1 +836,1 @@\n-  Klass* k = get_buffered_klass(o->klass());\n+  k = get_buffered_klass(k);\n@@ -839,2 +838,1 @@\n-  narrowKlass nk = CompressedKlassPointers::encode_not_null(requested_k, _requested_static_archive_bottom);\n-  o->set_narrow_klass(nk);\n+  return CompressedKlassPointers::encode_not_null(requested_k, _requested_static_archive_bottom);\n@@ -1065,2 +1063,1 @@\n-        oop archived_oop = cast_to_oop(start);\n-        oop original_oop = HeapShared::get_original_object(archived_oop);\n+        oop original_oop = ArchiveHeapWriter::buffered_addr_to_source_obj(start);\n@@ -1072,1 +1069,1 @@\n-        } else if (archived_oop == HeapShared::roots()) {\n+        } else if (start == ArchiveHeapWriter::buffered_heap_roots_addr()) {\n@@ -1077,1 +1074,1 @@\n-          byte_size = objArrayOopDesc::object_size(HeapShared::roots()->length()) * BytesPerWord;\n+          byte_size = ArchiveHeapWriter::heap_roots_word_size() * BytesPerWord;\n@@ -1094,1 +1091,1 @@\n-    return HeapShared::to_requested_address(p);\n+    return ArchiveHeapWriter::buffered_addr_to_requested_addr(p);\n","filename":"src\/hotspot\/share\/cds\/archiveBuilder.cpp","additions":8,"deletions":11,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -446,1 +446,1 @@\n-  void relocate_klass_ptr_of_oop(oop o);\n+  narrowKlass get_requested_narrow_klass(Klass* k);\n","filename":"src\/hotspot\/share\/cds\/archiveBuilder.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -0,0 +1,657 @@\n+\/*\n+ * Copyright (c) 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"cds\/archiveHeapWriter.hpp\"\n+#include \"cds\/filemap.hpp\"\n+#include \"cds\/heapShared.hpp\"\n+#include \"gc\/shared\/collectedHeap.hpp\"\n+#include \"memory\/iterator.inline.hpp\"\n+#include \"memory\/oopFactory.hpp\"\n+#include \"memory\/universe.hpp\"\n+#include \"oops\/compressedOops.hpp\"\n+#include \"oops\/oop.inline.hpp\"\n+#include \"oops\/objArrayOop.inline.hpp\"\n+#include \"oops\/oopHandle.inline.hpp\"\n+#include \"oops\/typeArrayKlass.hpp\"\n+#include \"oops\/typeArrayOop.hpp\"\n+#include \"runtime\/java.hpp\"\n+#include \"runtime\/mutexLocker.hpp\"\n+#include \"utilities\/bitMap.inline.hpp\"\n+\n+#if INCLUDE_G1GC\n+#include \"gc\/g1\/g1CollectedHeap.hpp\"\n+#include \"gc\/g1\/heapRegion.hpp\"\n+#endif\n+\n+#if INCLUDE_CDS_JAVA_HEAP\n+\n+\n+GrowableArrayCHeap<u1, mtClassShared>* ArchiveHeapWriter::_buffer;\n+\n+\/\/ The following are offsets from buffer_bottom()\n+size_t ArchiveHeapWriter::_buffer_top;\n+size_t ArchiveHeapWriter::_open_bottom;\n+size_t ArchiveHeapWriter::_open_top;\n+size_t ArchiveHeapWriter::_closed_bottom;\n+size_t ArchiveHeapWriter::_closed_top;\n+size_t ArchiveHeapWriter::_heap_roots_bottom;\n+\n+size_t ArchiveHeapWriter::_heap_roots_word_size;\n+\n+address ArchiveHeapWriter::_requested_open_region_bottom;\n+address ArchiveHeapWriter::_requested_open_region_top;\n+address ArchiveHeapWriter::_requested_closed_region_bottom;\n+address ArchiveHeapWriter::_requested_closed_region_top;\n+\n+ResourceBitMap* ArchiveHeapWriter::_closed_oopmap;\n+ResourceBitMap* ArchiveHeapWriter::_open_oopmap;\n+\n+GrowableArrayCHeap<ArchiveHeapWriter::NativePointerInfo, mtClassShared>* ArchiveHeapWriter::_native_pointers;\n+GrowableArrayCHeap<oop, mtClassShared>* ArchiveHeapWriter::_source_objs;\n+\n+ArchiveHeapWriter::BufferOffsetToSourceObjectTable*\n+  ArchiveHeapWriter::_buffer_offset_to_source_obj_table = nullptr;\n+\n+void ArchiveHeapWriter::init() {\n+  if (HeapShared::can_write()) {\n+    Universe::heap()->collect(GCCause::_java_lang_system_gc);\n+\n+    _buffer_offset_to_source_obj_table = new BufferOffsetToSourceObjectTable();\n+\n+    _requested_open_region_bottom = nullptr;\n+    _requested_open_region_top = nullptr;\n+    _requested_closed_region_bottom = nullptr;\n+    _requested_closed_region_top = nullptr;\n+\n+    _native_pointers = new GrowableArrayCHeap<NativePointerInfo, mtClassShared>(2048);\n+    _source_objs = new GrowableArrayCHeap<oop, mtClassShared>(10000);\n+\n+    guarantee(UseG1GC, \"implementation limitation\");\n+    guarantee(MIN_GC_REGION_ALIGNMENT <= \/*G1*\/HeapRegion::min_region_size_in_words() * HeapWordSize, \"must be\");\n+  }\n+}\n+\n+void ArchiveHeapWriter::add_source_obj(oop src_obj) {\n+  _source_objs->append(src_obj);\n+}\n+\n+\/\/ For the time being, always support two regions (to be strictly compatible with existing G1\n+\/\/ mapping code. We might eventually use a single region (JDK-8298048).\n+void ArchiveHeapWriter::write(GrowableArrayCHeap<oop, mtClassShared>* roots,\n+                              GrowableArray<MemRegion>* closed_regions, GrowableArray<MemRegion>* open_regions,\n+                              GrowableArray<ArchiveHeapBitmapInfo>* closed_bitmaps,\n+                              GrowableArray<ArchiveHeapBitmapInfo>* open_bitmaps) {\n+  assert(HeapShared::can_write(), \"sanity\");\n+  allocate_buffer();\n+  copy_source_objs_to_buffer(roots);\n+  set_requested_address_for_regions(closed_regions, open_regions);\n+  relocate_embedded_oops(roots, closed_bitmaps, open_bitmaps);\n+}\n+\n+bool ArchiveHeapWriter::is_too_large_to_archive(oop o) {\n+  return is_too_large_to_archive(o->size());\n+}\n+\n+bool ArchiveHeapWriter::is_string_too_large_to_archive(oop string) {\n+  typeArrayOop value = java_lang_String::value_no_keepalive(string);\n+  return is_too_large_to_archive(value);\n+}\n+\n+bool ArchiveHeapWriter::is_too_large_to_archive(size_t size) {\n+  assert(size > 0, \"no zero-size object\");\n+  assert(size * HeapWordSize > size, \"no overflow\");\n+  static_assert(MIN_GC_REGION_ALIGNMENT > 0, \"must be positive\");\n+\n+  size_t byte_size = size * HeapWordSize;\n+  if (byte_size > size_t(MIN_GC_REGION_ALIGNMENT)) {\n+    return true;\n+  } else {\n+    return false;\n+  }\n+}\n+\n+\/\/ Various lookup functions between source_obj, buffered_obj and requested_obj\n+bool ArchiveHeapWriter::is_in_requested_regions(oop o) {\n+  assert(_requested_open_region_bottom != nullptr, \"do not call before this is initialized\");\n+  assert(_requested_closed_region_bottom != nullptr, \"do not call before this is initialized\");\n+\n+  address a = cast_from_oop<address>(o);\n+  return (_requested_open_region_bottom <= a && a < _requested_open_region_top) ||\n+         (_requested_closed_region_bottom <= a && a < _requested_closed_region_top);\n+}\n+\n+oop ArchiveHeapWriter::requested_obj_from_buffer_offset(size_t offset) {\n+  oop req_obj = cast_to_oop(_requested_open_region_bottom + offset);\n+  assert(is_in_requested_regions(req_obj), \"must be\");\n+  return req_obj;\n+}\n+\n+oop ArchiveHeapWriter::source_obj_to_requested_obj(oop src_obj) {\n+  assert(DumpSharedSpaces, \"dump-time only\");\n+  HeapShared::CachedOopInfo* p = HeapShared::archived_object_cache()->get(src_obj);\n+  if (p != nullptr) {\n+    return requested_obj_from_buffer_offset(p->buffer_offset());\n+  } else {\n+    return nullptr;\n+  }\n+}\n+\n+oop ArchiveHeapWriter::buffered_addr_to_source_obj(address buffered_addr) {\n+  oop* p = _buffer_offset_to_source_obj_table->get(buffered_address_to_offset(buffered_addr));\n+  if (p != nullptr) {\n+    return *p;\n+  } else {\n+    return nullptr;\n+  }\n+}\n+\n+address ArchiveHeapWriter::buffered_addr_to_requested_addr(address buffered_addr) {\n+  return _requested_open_region_bottom + buffered_address_to_offset(buffered_addr);\n+}\n+\n+oop ArchiveHeapWriter::heap_roots_requested_address() {\n+  return requested_obj_from_buffer_offset(_heap_roots_bottom);\n+}\n+\n+address ArchiveHeapWriter::heap_region_requested_bottom(int heap_region_idx) {\n+  assert(_buffer != nullptr, \"must be initialized\");\n+  switch (heap_region_idx) {\n+  case MetaspaceShared::first_closed_heap_region:\n+    return _requested_closed_region_bottom;\n+  case MetaspaceShared::first_open_heap_region:\n+    return _requested_open_region_bottom;\n+  default:\n+    ShouldNotReachHere();\n+    return nullptr;\n+  }\n+}\n+\n+void ArchiveHeapWriter::allocate_buffer() {\n+  int initial_buffer_size = 100000;\n+  _buffer = new GrowableArrayCHeap<u1, mtClassShared>(initial_buffer_size);\n+  _open_bottom = _buffer_top = 0;\n+  ensure_buffer_space(1); \/\/ so that buffer_bottom() works\n+}\n+\n+void ArchiveHeapWriter::ensure_buffer_space(size_t min_bytes) {\n+  \/\/ We usually have very small heaps. If we get a huge one it's probably caused by a bug.\n+  guarantee(min_bytes <= max_jint, \"we dont support archiving more than 2G of objects\");\n+  _buffer->at_grow(to_array_index(min_bytes));\n+}\n+\n+void ArchiveHeapWriter::copy_roots_to_buffer(GrowableArrayCHeap<oop, mtClassShared>* roots) {\n+  Klass* k = Universe::objectArrayKlassObj(); \/\/ already relocated to point to archived klass\n+  int length = roots != nullptr ? roots->length() : 0;\n+  _heap_roots_word_size = objArrayOopDesc::object_size(length);\n+  size_t byte_size = _heap_roots_word_size * HeapWordSize;\n+  if (byte_size >= MIN_GC_REGION_ALIGNMENT) {\n+    log_error(cds, heap)(\"roots array is too large. Please reduce the number of classes\");\n+    vm_exit(1);\n+  }\n+\n+  maybe_fill_gc_region_gap(byte_size);\n+\n+  size_t new_top = _buffer_top + byte_size;\n+  ensure_buffer_space(new_top);\n+\n+  HeapWord* mem = offset_to_buffered_address<HeapWord*>(_buffer_top);\n+  memset(mem, 0, byte_size);\n+  {\n+    \/\/ This is copied from MemAllocator::finish\n+    oopDesc::set_mark(mem, markWord::prototype());\n+    oopDesc::release_set_klass(mem, k);\n+  }\n+  {\n+    \/\/ This is copied from ObjArrayAllocator::initialize\n+    arrayOopDesc::set_length(mem, length);\n+  }\n+\n+  objArrayOop arrayOop = objArrayOop(cast_to_oop(mem));\n+  for (int i = 0; i < length; i++) {\n+    \/\/ Do not use arrayOop->obj_at_put(i, o) as arrayOop is outside of the real heap!\n+    oop o = roots->at(i);\n+    if (UseCompressedOops) {\n+      * arrayOop->obj_at_addr<narrowOop>(i) = CompressedOops::encode(o);\n+    } else {\n+      * arrayOop->obj_at_addr<oop>(i) = o;\n+    }\n+  }\n+  log_info(cds)(\"archived obj roots[%d] = \" SIZE_FORMAT \" bytes, klass = %p, obj = %p\", length, byte_size, k, mem);\n+\n+  _heap_roots_bottom = _buffer_top;\n+  _buffer_top = new_top;\n+}\n+\n+void ArchiveHeapWriter::copy_source_objs_to_buffer(GrowableArrayCHeap<oop, mtClassShared>* roots) {\n+  copy_source_objs_to_buffer_by_region(\/*copy_open_region=*\/true);\n+  copy_roots_to_buffer(roots);\n+  _open_top = _buffer_top;\n+\n+  \/\/ Align the closed region to the next G1 region\n+  _buffer_top = _closed_bottom = align_up(_buffer_top, HeapRegion::GrainBytes);\n+  copy_source_objs_to_buffer_by_region(\/*copy_open_region=*\/false);\n+  _closed_top = _buffer_top;\n+\n+  log_info(cds, heap)(\"Size of open region   = \" SIZE_FORMAT \" bytes\", _open_top   - _open_bottom);\n+  log_info(cds, heap)(\"Size of closed region = \" SIZE_FORMAT \" bytes\", _closed_top - _closed_bottom);\n+}\n+\n+void ArchiveHeapWriter::copy_source_objs_to_buffer_by_region(bool copy_open_region) {\n+  for (int i = 0; i < _source_objs->length(); i++) {\n+    oop src_obj = _source_objs->at(i);\n+    HeapShared::CachedOopInfo* info = HeapShared::archived_object_cache()->get(src_obj);\n+    assert(info != nullptr, \"must be\");\n+    if (info->in_open_region() == copy_open_region) {\n+      \/\/ For region-based collectors such as G1, we need to make sure that we don't have\n+      \/\/ an object that can possible span across two regions.\n+      size_t buffer_offset = copy_one_source_obj_to_buffer(src_obj);\n+      info->set_buffer_offset(buffer_offset);\n+\n+      _buffer_offset_to_source_obj_table->put(buffer_offset, src_obj);\n+    }\n+  }\n+}\n+\n+size_t ArchiveHeapWriter::filler_array_byte_size(int length) {\n+  size_t byte_size = objArrayOopDesc::object_size(length) * HeapWordSize;\n+  return byte_size;\n+}\n+\n+int ArchiveHeapWriter::filler_array_length(size_t fill_bytes) {\n+  assert(is_object_aligned(fill_bytes), \"must be\");\n+  size_t elemSize = (UseCompressedOops ? sizeof(narrowOop) : sizeof(oop));\n+\n+  int initial_length = to_array_length(fill_bytes \/ elemSize);\n+  for (int length = initial_length; length >= 0; length --) {\n+    size_t array_byte_size = filler_array_byte_size(length);\n+    if (array_byte_size == fill_bytes) {\n+      return length;\n+    }\n+  }\n+\n+  ShouldNotReachHere();\n+  return -1;\n+}\n+\n+void ArchiveHeapWriter::init_filler_array_at_buffer_top(int array_length, size_t fill_bytes) {\n+  assert(UseCompressedClassPointers, \"Archived heap only supported for compressed klasses\");\n+  Klass* oak = Universe::objectArrayKlassObj(); \/\/ already relocated to point to archived klass\n+  HeapWord* mem = offset_to_buffered_address<HeapWord*>(_buffer_top);\n+  memset(mem, 0, fill_bytes);\n+  oopDesc::set_mark(mem, markWord::prototype());\n+  narrowKlass nk = ArchiveBuilder::current()->get_requested_narrow_klass(oak);\n+  cast_to_oop(mem)->set_narrow_klass(nk);\n+  arrayOopDesc::set_length(mem, array_length);\n+}\n+\n+void ArchiveHeapWriter::maybe_fill_gc_region_gap(size_t required_byte_size) {\n+  \/\/ We fill only with arrays (so we don't need to use a single HeapWord filler if the\n+  \/\/ leftover space is smaller than a zero-sized array object). Therefore, we need to\n+  \/\/ make sure there's enough space of min_filler_byte_size in the current region after\n+  \/\/ required_byte_size has been allocated. If not, fill the remainder of the current\n+  \/\/ region.\n+  size_t min_filler_byte_size = filler_array_byte_size(0);\n+  size_t new_top = _buffer_top + required_byte_size + min_filler_byte_size;\n+\n+  const size_t cur_min_region_bottom = align_down(_buffer_top, MIN_GC_REGION_ALIGNMENT);\n+  const size_t next_min_region_bottom = align_down(new_top, MIN_GC_REGION_ALIGNMENT);\n+\n+  if (cur_min_region_bottom != next_min_region_bottom) {\n+    \/\/ Make sure that no objects span across MIN_GC_REGION_ALIGNMENT. This way\n+    \/\/ we can map the region in any region-based collector.\n+    assert(next_min_region_bottom > cur_min_region_bottom, \"must be\");\n+    assert(next_min_region_bottom - cur_min_region_bottom == MIN_GC_REGION_ALIGNMENT,\n+           \"no buffered object can be larger than %d bytes\",  MIN_GC_REGION_ALIGNMENT);\n+\n+    const size_t filler_end = next_min_region_bottom;\n+    const size_t fill_bytes = filler_end - _buffer_top;\n+    assert(fill_bytes > 0, \"must be\");\n+    ensure_buffer_space(filler_end);\n+\n+    int array_length = filler_array_length(fill_bytes);\n+    log_info(cds, heap)(\"Inserting filler obj array of %d elements (\" SIZE_FORMAT \" bytes total) @ buffer offset \" SIZE_FORMAT,\n+                        array_length, fill_bytes, _buffer_top);\n+    init_filler_array_at_buffer_top(array_length, fill_bytes);\n+\n+    _buffer_top = filler_end;\n+  }\n+}\n+\n+size_t ArchiveHeapWriter::copy_one_source_obj_to_buffer(oop src_obj) {\n+  assert(!is_too_large_to_archive(src_obj), \"already checked\");\n+  size_t byte_size = src_obj->size() * HeapWordSize;\n+  assert(byte_size > 0, \"no zero-size objects\");\n+\n+  maybe_fill_gc_region_gap(byte_size);\n+\n+  size_t new_top = _buffer_top + byte_size;\n+  assert(new_top > _buffer_top, \"no wrap around\");\n+\n+  size_t cur_min_region_bottom = align_down(_buffer_top, MIN_GC_REGION_ALIGNMENT);\n+  size_t next_min_region_bottom = align_down(new_top, MIN_GC_REGION_ALIGNMENT);\n+  assert(cur_min_region_bottom == next_min_region_bottom, \"no object should cross minimal GC region boundaries\");\n+\n+  ensure_buffer_space(new_top);\n+\n+  address from = cast_from_oop<address>(src_obj);\n+  address to = offset_to_buffered_address<address>(_buffer_top);\n+  assert(is_object_aligned(_buffer_top), \"sanity\");\n+  assert(is_object_aligned(byte_size), \"sanity\");\n+  memcpy(to, from, byte_size);\n+\n+  size_t buffered_obj_offset = _buffer_top;\n+  _buffer_top = new_top;\n+\n+  return buffered_obj_offset;\n+}\n+\n+void ArchiveHeapWriter::set_requested_address_for_regions(GrowableArray<MemRegion>* closed_regions,\n+                                                          GrowableArray<MemRegion>* open_regions) {\n+  assert(closed_regions->length() == 0, \"must be\");\n+  assert(open_regions->length() == 0, \"must be\");\n+\n+  assert(UseG1GC, \"must be\");\n+  address heap_end = (address)G1CollectedHeap::heap()->reserved().end();\n+  log_info(cds, heap)(\"Heap end = %p\", heap_end);\n+\n+  size_t closed_region_byte_size = _closed_top - _closed_bottom;\n+  size_t open_region_byte_size = _open_top - _open_bottom;\n+  assert(closed_region_byte_size > 0, \"must archived at least one object for closed region!\");\n+  assert(open_region_byte_size > 0, \"must archived at least one object for open region!\");\n+\n+  \/\/ The following two asserts are ensured by copy_source_objs_to_buffer_by_region().\n+  assert(is_aligned(_closed_bottom, HeapRegion::GrainBytes), \"sanity\");\n+  assert(is_aligned(_open_bottom, HeapRegion::GrainBytes), \"sanity\");\n+\n+  _requested_closed_region_bottom = align_down(heap_end - closed_region_byte_size, HeapRegion::GrainBytes);\n+  _requested_open_region_bottom = _requested_closed_region_bottom - (_closed_bottom - _open_bottom);\n+\n+  assert(is_aligned(_requested_closed_region_bottom, HeapRegion::GrainBytes), \"sanity\");\n+  assert(is_aligned(_requested_open_region_bottom, HeapRegion::GrainBytes), \"sanity\");\n+\n+  _requested_open_region_top = _requested_open_region_bottom + (_open_top - _open_bottom);\n+  _requested_closed_region_top = _requested_closed_region_bottom + (_closed_top - _closed_bottom);\n+\n+  assert(_requested_open_region_top <= _requested_closed_region_bottom, \"no overlap\");\n+\n+  closed_regions->append(MemRegion(offset_to_buffered_address<HeapWord*>(_closed_bottom),\n+                                   offset_to_buffered_address<HeapWord*>(_closed_top)));\n+  open_regions->append(  MemRegion(offset_to_buffered_address<HeapWord*>(_open_bottom),\n+                                   offset_to_buffered_address<HeapWord*>(_open_top)));\n+}\n+\n+\/\/ Oop relocation\n+\n+template <typename T> T* ArchiveHeapWriter::requested_addr_to_buffered_addr(T* p) {\n+  assert(is_in_requested_regions(cast_to_oop(p)), \"must be\");\n+\n+  address addr = address(p);\n+  assert(addr >= _requested_open_region_bottom, \"must be\");\n+  size_t offset = addr - _requested_open_region_bottom;\n+  return offset_to_buffered_address<T*>(offset);\n+}\n+\n+template <typename T> oop ArchiveHeapWriter::load_source_oop_from_buffer(T* buffered_addr) {\n+  oop o = load_oop_from_buffer(buffered_addr);\n+  assert(!in_buffer(cast_from_oop<address>(o)), \"must point to source oop\");\n+  return o;\n+}\n+\n+template <typename T> void ArchiveHeapWriter::store_requested_oop_in_buffer(T* buffered_addr,\n+                                                                            oop request_oop) {\n+  assert(is_in_requested_regions(request_oop), \"must be\");\n+  store_oop_in_buffer(buffered_addr, request_oop);\n+}\n+\n+void ArchiveHeapWriter::store_oop_in_buffer(oop* buffered_addr, oop requested_obj) {\n+  \/\/ Make heap content deterministic. See comments inside HeapShared::to_requested_address.\n+  *buffered_addr = HeapShared::to_requested_address(requested_obj);\n+}\n+\n+void ArchiveHeapWriter::store_oop_in_buffer(narrowOop* buffered_addr, oop requested_obj) {\n+  \/\/ Note: HeapShared::to_requested_address() is not necessary because\n+  \/\/ the heap always starts at a deterministic address with UseCompressedOops==true.\n+  narrowOop val = CompressedOops::encode_not_null(requested_obj);\n+  *buffered_addr = val;\n+}\n+\n+oop ArchiveHeapWriter::load_oop_from_buffer(oop* buffered_addr) {\n+  return *buffered_addr;\n+}\n+\n+oop ArchiveHeapWriter::load_oop_from_buffer(narrowOop* buffered_addr) {\n+  return CompressedOops::decode(*buffered_addr);\n+}\n+\n+template <typename T> void ArchiveHeapWriter::relocate_field_in_buffer(T* field_addr_in_buffer) {\n+  oop source_referent = load_source_oop_from_buffer<T>(field_addr_in_buffer);\n+  if (!CompressedOops::is_null(source_referent)) {\n+    oop request_referent = source_obj_to_requested_obj(source_referent);\n+    store_requested_oop_in_buffer<T>(field_addr_in_buffer, request_referent);\n+    mark_oop_pointer<T>(field_addr_in_buffer);\n+  }\n+}\n+\n+template <typename T> void ArchiveHeapWriter::mark_oop_pointer(T* buffered_addr) {\n+  T* request_p = (T*)(buffered_addr_to_requested_addr((address)buffered_addr));\n+  ResourceBitMap* oopmap;\n+  address requested_region_bottom;\n+\n+  if (request_p >= (T*)_requested_closed_region_bottom) {\n+    assert(request_p < (T*)_requested_closed_region_top, \"sanity\");\n+    oopmap = _closed_oopmap;\n+    requested_region_bottom = _requested_closed_region_bottom;\n+  } else {\n+    assert(request_p >= (T*)_requested_open_region_bottom, \"sanity\");\n+    assert(request_p <  (T*)_requested_open_region_top, \"sanity\");\n+    oopmap = _open_oopmap;\n+    requested_region_bottom = _requested_open_region_bottom;\n+  }\n+\n+  \/\/ Mark the pointer in the oopmap\n+  T* region_bottom = (T*)requested_region_bottom;\n+  assert(request_p >= region_bottom, \"must be\");\n+  BitMap::idx_t idx = request_p - region_bottom;\n+  assert(idx < oopmap->size(), \"overflow\");\n+  oopmap->set_bit(idx);\n+}\n+\n+void ArchiveHeapWriter::update_header_for_requested_obj(oop requested_obj, oop src_obj,  Klass* src_klass) {\n+  assert(UseCompressedClassPointers, \"Archived heap only supported for compressed klasses\");\n+  narrowKlass nk = ArchiveBuilder::current()->get_requested_narrow_klass(src_klass);\n+  address buffered_addr = requested_addr_to_buffered_addr(cast_from_oop<address>(requested_obj));\n+\n+  oop fake_oop = cast_to_oop(buffered_addr);\n+  fake_oop->set_narrow_klass(nk);\n+\n+  \/\/ We need to retain the identity_hash, because it may have been used by some hashtables\n+  \/\/ in the shared heap. This also has the side effect of pre-initializing the\n+  \/\/ identity_hash for all shared objects, so they are less likely to be written\n+  \/\/ into during run time, increasing the potential of memory sharing.\n+  if (src_obj != nullptr) {\n+    int src_hash = src_obj->identity_hash();\n+    fake_oop->set_mark(markWord::prototype().copy_set_hash(src_hash));\n+    assert(fake_oop->mark().is_unlocked(), \"sanity\");\n+\n+    DEBUG_ONLY(int archived_hash = fake_oop->identity_hash());\n+    assert(src_hash == archived_hash, \"Different hash codes: original %x, archived %x\", src_hash, archived_hash);\n+  }\n+}\n+\n+\/\/ Relocate an element in the buffered copy of HeapShared::roots()\n+template <typename T> void ArchiveHeapWriter::relocate_root_at(oop requested_roots, int index) {\n+  size_t offset = (size_t)((objArrayOop)requested_roots)->obj_at_offset<T>(index);\n+  relocate_field_in_buffer<T>((T*)(buffered_heap_roots_addr() + offset));\n+}\n+\n+class ArchiveHeapWriter::EmbeddedOopRelocator: public BasicOopIterateClosure {\n+  oop _src_obj;\n+  address _buffered_obj;\n+\n+public:\n+  EmbeddedOopRelocator(oop src_obj, address buffered_obj) :\n+    _src_obj(src_obj), _buffered_obj(buffered_obj) {}\n+\n+  void do_oop(narrowOop *p) { EmbeddedOopRelocator::do_oop_work(p); }\n+  void do_oop(      oop *p) { EmbeddedOopRelocator::do_oop_work(p); }\n+\n+private:\n+  template <class T> void do_oop_work(T *p) {\n+    size_t field_offset = pointer_delta(p, _src_obj, sizeof(char));\n+    ArchiveHeapWriter::relocate_field_in_buffer<T>((T*)(_buffered_obj + field_offset));\n+  }\n+};\n+\n+\/\/ Update all oop fields embedded in the buffered objects\n+void ArchiveHeapWriter::relocate_embedded_oops(GrowableArrayCHeap<oop, mtClassShared>* roots,\n+                                               GrowableArray<ArchiveHeapBitmapInfo>* closed_bitmaps,\n+                                               GrowableArray<ArchiveHeapBitmapInfo>* open_bitmaps) {\n+  size_t oopmap_unit = (UseCompressedOops ? sizeof(narrowOop) : sizeof(oop));\n+  size_t closed_region_byte_size = _closed_top - _closed_bottom;\n+  size_t open_region_byte_size   = _open_top   - _open_bottom;\n+  ResourceBitMap closed_oopmap(closed_region_byte_size \/ oopmap_unit);\n+  ResourceBitMap open_oopmap  (open_region_byte_size   \/ oopmap_unit);\n+\n+  _closed_oopmap = &closed_oopmap;\n+  _open_oopmap = &open_oopmap;\n+\n+  auto iterator = [&] (oop src_obj, HeapShared::CachedOopInfo& info) {\n+    oop requested_obj = requested_obj_from_buffer_offset(info.buffer_offset());\n+    update_header_for_requested_obj(requested_obj, src_obj, src_obj->klass());\n+\n+    address buffered_obj = offset_to_buffered_address<address>(info.buffer_offset());\n+    EmbeddedOopRelocator relocator(src_obj, buffered_obj);\n+\n+    src_obj->oop_iterate(&relocator);\n+  };\n+  HeapShared::archived_object_cache()->iterate_all(iterator);\n+\n+  \/\/ Relocate HeapShared::roots(), which is created in copy_roots_to_buffer() and\n+  \/\/ doesn't have a corresponding src_obj, so we can't use EmbeddedOopRelocator on it.\n+  oop requested_roots = requested_obj_from_buffer_offset(_heap_roots_bottom);\n+  update_header_for_requested_obj(requested_roots, nullptr, Universe::objectArrayKlassObj());\n+  int length = roots != nullptr ? roots->length() : 0;\n+  for (int i = 0; i < length; i++) {\n+    if (UseCompressedOops) {\n+      relocate_root_at<narrowOop>(requested_roots, i);\n+    } else {\n+      relocate_root_at<oop>(requested_roots, i);\n+    }\n+  }\n+\n+  closed_bitmaps->append(make_bitmap_info(&closed_oopmap, \/*is_open=*\/false, \/*is_oopmap=*\/true));\n+  open_bitmaps  ->append(make_bitmap_info(&open_oopmap,   \/*is_open=*\/false, \/*is_oopmap=*\/true));\n+\n+  closed_bitmaps->append(compute_ptrmap(\/*is_open=*\/false));\n+  open_bitmaps  ->append(compute_ptrmap(\/*is_open=*\/true));\n+\n+  _closed_oopmap = nullptr;\n+  _open_oopmap = nullptr;\n+}\n+\n+ArchiveHeapBitmapInfo ArchiveHeapWriter::make_bitmap_info(ResourceBitMap* bitmap, bool is_open,  bool is_oopmap) {\n+  size_t size_in_bits = bitmap->size();\n+  size_t size_in_bytes;\n+  uintptr_t* buffer;\n+\n+  if (size_in_bits > 0) {\n+    size_in_bytes = bitmap->size_in_bytes();\n+    buffer = (uintptr_t*)NEW_C_HEAP_ARRAY(char, size_in_bytes, mtInternal);\n+    bitmap->write_to(buffer, size_in_bytes);\n+  } else {\n+    size_in_bytes = 0;\n+    buffer = nullptr;\n+  }\n+\n+  log_info(cds, heap)(\"%s @ \" INTPTR_FORMAT \" (\" SIZE_FORMAT_W(6) \" bytes) for %s heap region\",\n+                      is_oopmap ? \"Oopmap\" : \"Ptrmap\",\n+                      p2i(buffer), size_in_bytes,\n+                      is_open? \"open\" : \"closed\");\n+\n+  ArchiveHeapBitmapInfo info;\n+  info._map = (address)buffer;\n+  info._size_in_bits = size_in_bits;\n+  info._size_in_bytes = size_in_bytes;\n+\n+  return info;\n+}\n+\n+void ArchiveHeapWriter::mark_native_pointer(oop src_obj, int field_offset) {\n+  Metadata* ptr = src_obj->metadata_field_acquire(field_offset);\n+  if (ptr != nullptr) {\n+    NativePointerInfo info;\n+    info._src_obj = src_obj;\n+    info._field_offset = field_offset;\n+    _native_pointers->append(info);\n+  }\n+}\n+\n+ArchiveHeapBitmapInfo ArchiveHeapWriter::compute_ptrmap(bool is_open) {\n+  int num_non_null_ptrs = 0;\n+  Metadata** bottom = (Metadata**) (is_open ? _requested_open_region_bottom: _requested_closed_region_bottom);\n+  Metadata** top = (Metadata**) (is_open ? _requested_open_region_top: _requested_closed_region_top); \/\/ exclusive\n+  ResourceBitMap ptrmap(top - bottom);\n+\n+  for (int i = 0; i < _native_pointers->length(); i++) {\n+    NativePointerInfo info = _native_pointers->at(i);\n+    oop src_obj = info._src_obj;\n+    int field_offset = info._field_offset;\n+    HeapShared::CachedOopInfo* p = HeapShared::archived_object_cache()->get(src_obj);\n+    if (p->in_open_region() == is_open) {\n+      \/\/ requested_field_addr = the address of this field in the requested space\n+      oop requested_obj = requested_obj_from_buffer_offset(p->buffer_offset());\n+      Metadata** requested_field_addr = (Metadata**)(cast_from_oop<address>(requested_obj) + field_offset);\n+      assert(bottom <= requested_field_addr && requested_field_addr < top, \"range check\");\n+\n+      \/\/ Mark this field in the bitmap\n+      BitMap::idx_t idx = requested_field_addr - bottom;\n+      ptrmap.set_bit(idx);\n+      num_non_null_ptrs ++;\n+\n+      \/\/ Set the native pointer to the requested address of the metadata (at runtime, the metadata will have\n+      \/\/ this address if the RO\/RW regions are mapped at the default location).\n+\n+      Metadata** buffered_field_addr = requested_addr_to_buffered_addr(requested_field_addr);\n+      Metadata* native_ptr = *buffered_field_addr;\n+      assert(native_ptr != nullptr, \"sanity\");\n+\n+      address buffered_native_ptr = ArchiveBuilder::current()->get_buffered_addr((address)native_ptr);\n+      address requested_native_ptr = ArchiveBuilder::current()->to_requested(buffered_native_ptr);\n+      *buffered_field_addr = (Metadata*)requested_native_ptr;\n+    }\n+  }\n+\n+  log_info(cds, heap)(\"compute_ptrmap: marked %d non-null native pointers for %s heap region\",\n+                      num_non_null_ptrs, is_open ? \"open\" : \"closed\");\n+\n+  if (num_non_null_ptrs == 0) {\n+    ResourceBitMap empty;\n+    return make_bitmap_info(&empty, is_open, \/*is_oopmap=*\/ false);\n+  } else {\n+    return make_bitmap_info(&ptrmap, is_open, \/*is_oopmap=*\/ false);\n+  }\n+}\n+\n+#endif \/\/ INCLUDE_CDS_JAVA_HEAP\n","filename":"src\/hotspot\/share\/cds\/archiveHeapWriter.cpp","additions":657,"deletions":0,"binary":false,"changes":657,"status":"added"},{"patch":"@@ -0,0 +1,202 @@\n+\/*\n+ * Copyright (c) 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_CDS_ARCHIVEHEAPWRITER_HPP\n+#define SHARE_CDS_ARCHIVEHEAPWRITER_HPP\n+\n+#include \"cds\/heapShared.hpp\"\n+#include \"memory\/allocation.hpp\"\n+#include \"memory\/allStatic.hpp\"\n+#include \"oops\/oopHandle.hpp\"\n+#include \"utilities\/bitMap.hpp\"\n+#include \"utilities\/exceptions.hpp\"\n+#include \"utilities\/growableArray.hpp\"\n+#include \"utilities\/macros.hpp\"\n+#include \"utilities\/resourceHash.hpp\"\n+\n+#if INCLUDE_CDS_JAVA_HEAP\n+\n+struct ArchiveHeapBitmapInfo;\n+class MemRegion;\n+\n+class ArchiveHeapWriter : AllStatic {\n+  class EmbeddedOopRelocator;\n+  struct NativePointerInfo {\n+    oop _src_obj;\n+    int _field_offset;\n+  };\n+\n+  \/\/ The minimum region size of all collectors that are supported by CDS in\n+  \/\/ ArchiveHeapLoader::can_map() mode. Currently only G1 is supported. G1's region size\n+  \/\/ depends on -Xmx, but can never be smaller than 1 * M.\n+  \/\/ (TODO: Perhaps change to 256K to be compatible with Shenandoah)\n+  static constexpr int MIN_GC_REGION_ALIGNMENT = 1 * M;\n+\n+  \/\/ \"source\" vs \"buffered\" vs \"requested\"\n+  \/\/\n+  \/\/ [1] HeapShared::archive_objects() identifies all of the oops that need to be stored\n+  \/\/     into the CDS archive. These are entered into HeapShared::archived_object_cache().\n+  \/\/     These are called \"source objects\"\n+  \/\/\n+  \/\/ [2] ArchiveHeapWriter::write() copies all source objects into ArchiveHeapWriter::_buffer,\n+  \/\/     which is a GrowableArray that sites outside of the valid heap range. Therefore\n+  \/\/     we avoid using the addresses of these copies as oops. They are usually\n+  \/\/     called \"buffered_addr\" in the code (of the type \"address\").\n+  \/\/\n+  \/\/ [3] Each archived object has a \"requested address\" -- at run time, if the object\n+  \/\/     can be mapped at this address, we can avoid relocation.\n+  \/\/\n+  \/\/ Note: the design and convention is the same as for the archiving of Metaspace objects.\n+  \/\/ See archiveBuilder.hpp.\n+\n+  static GrowableArrayCHeap<u1, mtClassShared>* _buffer;\n+\n+  \/\/ The exclusive top of the last object that has been copied into this->_buffer.\n+  static size_t _buffer_top;\n+\n+  \/\/ The bounds of the open region inside this->_buffer.\n+  static size_t _open_bottom;  \/\/ inclusive\n+  static size_t _open_top;     \/\/ exclusive\n+\n+  \/\/ The bounds of the closed region inside this->_buffer.\n+  static size_t _closed_bottom;  \/\/ inclusive\n+  static size_t _closed_top;     \/\/ exclusive\n+\n+  \/\/ The bottom of the copy of Heap::roots() inside this->_buffer.\n+  static size_t _heap_roots_bottom;\n+  static size_t _heap_roots_word_size;\n+\n+  static address _requested_open_region_bottom;\n+  static address _requested_open_region_top;\n+  static address _requested_closed_region_bottom;\n+  static address _requested_closed_region_top;\n+\n+  static ResourceBitMap* _closed_oopmap;\n+  static ResourceBitMap* _open_oopmap;\n+\n+  static ArchiveHeapBitmapInfo _closed_oopmap_info;\n+  static ArchiveHeapBitmapInfo _open_oopmap_info;\n+\n+  static GrowableArrayCHeap<NativePointerInfo, mtClassShared>* _native_pointers;\n+  static GrowableArrayCHeap<oop, mtClassShared>* _source_objs;\n+\n+  typedef ResourceHashtable<size_t, oop,\n+      36137, \/\/ prime number\n+      AnyObj::C_HEAP,\n+      mtClassShared> BufferOffsetToSourceObjectTable;\n+  static BufferOffsetToSourceObjectTable* _buffer_offset_to_source_obj_table;\n+\n+  static void allocate_buffer();\n+  static void ensure_buffer_space(size_t min_bytes);\n+\n+  \/\/ Both Java bytearray and GrowableArraty use int indices and lengths. Do a safe typecast with range check\n+  static int to_array_index(size_t i) {\n+    assert(i <= (size_t)max_jint, \"must be\");\n+    return (size_t)i;\n+  }\n+  static int to_array_length(size_t n) {\n+    return to_array_index(n);\n+  }\n+\n+  template <typename T> static T offset_to_buffered_address(size_t offset) {\n+    return (T)(_buffer->adr_at(to_array_index(offset)));\n+  }\n+\n+  static address buffer_bottom() {\n+    return offset_to_buffered_address<address>(0);\n+  }\n+\n+  static address buffer_top() {\n+    return buffer_bottom() + _buffer_top;\n+  }\n+\n+  static bool in_buffer(address buffered_addr) {\n+    return (buffer_bottom() <= buffered_addr) && (buffered_addr < buffer_top());\n+  }\n+\n+  static size_t buffered_address_to_offset(address buffered_addr) {\n+    assert(in_buffer(buffered_addr), \"sanity\");\n+    return buffered_addr - buffer_bottom();\n+  }\n+\n+  static void copy_roots_to_buffer(GrowableArrayCHeap<oop, mtClassShared>* roots);\n+  static void copy_source_objs_to_buffer(GrowableArrayCHeap<oop, mtClassShared>* roots);\n+  static void copy_source_objs_to_buffer_by_region(bool copy_open_region);\n+  static size_t copy_one_source_obj_to_buffer(oop src_obj);\n+\n+  static void maybe_fill_gc_region_gap(size_t required_byte_size);\n+  static size_t filler_array_byte_size(int length);\n+  static int filler_array_length(size_t fill_bytes);\n+  static void init_filler_array_at_buffer_top(int array_length, size_t fill_bytes);\n+\n+  static void set_requested_address_for_regions(GrowableArray<MemRegion>* closed_regions,\n+                                                GrowableArray<MemRegion>* open_regions);\n+  static void relocate_embedded_oops(GrowableArrayCHeap<oop, mtClassShared>* roots,\n+                                     GrowableArray<ArchiveHeapBitmapInfo>* closed_bitmaps,\n+                                     GrowableArray<ArchiveHeapBitmapInfo>* open_bitmaps);\n+  static ArchiveHeapBitmapInfo compute_ptrmap(bool is_open);\n+  static ArchiveHeapBitmapInfo make_bitmap_info(ResourceBitMap* bitmap, bool is_open,  bool is_oopmap);\n+  static bool is_in_requested_regions(oop o);\n+  static oop requested_obj_from_buffer_offset(size_t offset);\n+\n+  static oop load_oop_from_buffer(oop* buffered_addr);\n+  static oop load_oop_from_buffer(narrowOop* buffered_addr);\n+  static void store_oop_in_buffer(oop* buffered_addr, oop requested_obj);\n+  static void store_oop_in_buffer(narrowOop* buffered_addr, oop requested_obj);\n+\n+  template <typename T> static oop load_source_oop_from_buffer(T* buffered_addr);\n+  template <typename T> static void store_requested_oop_in_buffer(T* buffered_addr, oop request_oop);\n+\n+  template <typename T> static T* requested_addr_to_buffered_addr(T* p);\n+  template <typename T> static void relocate_field_in_buffer(T* field_addr_in_buffer);\n+  template <typename T> static void mark_oop_pointer(T* buffered_addr);\n+  template <typename T> static void relocate_root_at(oop requested_roots, int index);\n+\n+  static void update_header_for_requested_obj(oop requested_obj, oop src_obj, Klass* src_klass);\n+public:\n+  static void init() NOT_CDS_JAVA_HEAP_RETURN;\n+  static void add_source_obj(oop src_obj);\n+  static bool is_too_large_to_archive(size_t size);\n+  static bool is_too_large_to_archive(oop obj);\n+  static bool is_string_too_large_to_archive(oop string);\n+  static void write(GrowableArrayCHeap<oop, mtClassShared>*,\n+                    GrowableArray<MemRegion>* closed_regions, GrowableArray<MemRegion>* open_regions,\n+                    GrowableArray<ArchiveHeapBitmapInfo>* closed_bitmaps,\n+                    GrowableArray<ArchiveHeapBitmapInfo>* open_bitmaps);\n+  static address heap_region_requested_bottom(int heap_region_idx);\n+  static oop heap_roots_requested_address();\n+  static address buffered_heap_roots_addr() {\n+    return offset_to_buffered_address<address>(_heap_roots_bottom);\n+  }\n+  static size_t heap_roots_word_size() {\n+    return _heap_roots_word_size;\n+  }\n+\n+  static void mark_native_pointer(oop src_obj, int offset);\n+  static oop source_obj_to_requested_obj(oop src_obj);\n+  static oop buffered_addr_to_source_obj(address buffered_addr);\n+  static address buffered_addr_to_requested_addr(address buffered_addr);\n+};\n+#endif \/\/ INCLUDE_CDS_JAVA_HEAP\n+#endif \/\/ SHARE_CDS_ARCHIVEHEAPWRITER_HPP\n","filename":"src\/hotspot\/share\/cds\/archiveHeapWriter.hpp","additions":202,"deletions":0,"binary":false,"changes":202,"status":"added"},{"patch":"@@ -279,2 +279,2 @@\n-  if (info->_referrer != nullptr) {\n-    HeapShared::CachedOopInfo* ref = HeapShared::archived_object_cache()->get(info->_referrer);\n+  if (info->orig_referrer() != nullptr) {\n+    HeapShared::CachedOopInfo* ref = HeapShared::archived_object_cache()->get(info->orig_referrer());\n@@ -282,1 +282,1 @@\n-    level = trace_to_root(st, info->_referrer, orig_obj, ref) + 1;\n+    level = trace_to_root(st, info->orig_referrer(), orig_obj, ref) + 1;\n","filename":"src\/hotspot\/share\/cds\/cdsHeapVerifier.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"cds\/archiveHeapWriter.hpp\"\n@@ -1635,0 +1636,2 @@\n+    assert(HeapShared::can_write(), \"sanity\");\n+#if INCLUDE_CDS_JAVA_HEAP\n@@ -1636,1 +1639,1 @@\n-    requested_base = base;\n+    requested_base = (char*)ArchiveHeapWriter::heap_region_requested_bottom(region);\n@@ -1638,1 +1641,1 @@\n-      mapping_offset = (size_t)((address)base - CompressedOops::base());\n+      mapping_offset = (size_t)((address)requested_base - CompressedOops::base());\n@@ -1645,0 +1648,1 @@\n+#endif \/\/ INCLUDE_CDS_JAVA_HEAP\n","filename":"src\/hotspot\/share\/cds\/filemap.cpp","additions":6,"deletions":2,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"cds\/archiveHeapWriter.hpp\"\n@@ -64,1 +65,0 @@\n-#include \"gc\/g1\/heapRegion.hpp\"\n@@ -85,0 +85,1 @@\n+bool HeapShared::_copying_open_region_objects = false;\n@@ -86,1 +87,0 @@\n-GrowableArrayCHeap<Metadata**, mtClassShared>* HeapShared::_native_pointers = nullptr;\n@@ -146,8 +146,0 @@\n-#ifdef ASSERT\n-bool HeapShared::is_archived_object_during_dumptime(oop p) {\n-  assert(HeapShared::can_write(), \"must be\");\n-  assert(DumpSharedSpaces, \"this function is only used with -Xshare:dump\");\n-  return Universe::heap()->is_archived_object(p);\n-}\n-#endif\n-\n@@ -223,2 +215,2 @@\n-HeapShared::OriginalObjectTable* HeapShared::_original_object_table = nullptr;\n-oop HeapShared::find_archived_heap_object(oop obj) {\n+\n+bool HeapShared::has_been_archived(oop obj) {\n@@ -226,7 +218,1 @@\n-  ArchivedObjectCache* cache = archived_object_cache();\n-  CachedOopInfo* p = cache->get(obj);\n-  if (p != nullptr) {\n-    return p->_obj;\n-  } else {\n-    return nullptr;\n-  }\n+  return archived_object_cache()->get(obj) != nullptr;\n@@ -266,12 +252,5 @@\n-  if (DumpSharedSpaces) {\n-    assert(Thread::current() == (Thread*)VMThread::vm_thread(), \"should be in vm thread\");\n-    assert(_pending_roots != nullptr, \"sanity\");\n-    return _pending_roots->at(index);\n-  } else {\n-    assert(UseSharedSpaces, \"must be\");\n-    assert(!_roots.is_empty(), \"must have loaded shared heap\");\n-    oop result = roots()->obj_at(index);\n-    if (clear) {\n-      clear_root(index);\n-    }\n-    return result;\n+  assert(!DumpSharedSpaces && UseSharedSpaces, \"runtime only\");\n+  assert(!_roots.is_empty(), \"must have loaded shared heap\");\n+  oop result = roots()->obj_at(index);\n+  if (clear) {\n+    clear_root(index);\n@@ -279,0 +258,1 @@\n+  return result;\n@@ -293,10 +273,1 @@\n-bool HeapShared::is_too_large_to_archive(oop o) {\n-  \/\/ TODO: To make the CDS heap mappable for all collectors, this function should\n-  \/\/ reject objects that may be too large for *any* collector.\n-  assert(UseG1GC, \"implementation limitation\");\n-  size_t sz = align_up(o->size() * HeapWordSize, ObjectAlignmentInBytes);\n-  size_t max = \/*G1*\/HeapRegion::min_region_size_in_words() * HeapWordSize;\n-  return (sz > max);\n-}\n-\n-oop HeapShared::archive_object(oop obj) {\n+bool HeapShared::archive_object(oop obj) {\n@@ -306,5 +277,2 @@\n-\n-  oop ao = find_archived_heap_object(obj);\n-  if (ao != nullptr) {\n-    \/\/ already archived\n-    return ao;\n+  if (has_been_archived(obj)) {\n+    return true;\n@@ -313,2 +281,1 @@\n-  int len = obj->size();\n-  if (G1CollectedHeap::heap()->is_archive_alloc_too_large(len)) {\n+  if (ArchiveHeapWriter::is_too_large_to_archive(obj->size())) {\n@@ -316,3 +283,9 @@\n-                         p2i(obj), (size_t)obj->size());\n-    return nullptr;\n-  }\n+                         p2i(obj), obj->size());\n+    return false;\n+  } else {\n+    count_allocation(obj->size());\n+    ArchiveHeapWriter::add_source_obj(obj);\n+\n+    CachedOopInfo info = make_cached_oop_info();\n+    archived_object_cache()->put(obj, info);\n+    mark_native_pointers(obj);\n@@ -320,24 +293,0 @@\n-  oop archived_oop = cast_to_oop(G1CollectedHeap::heap()->archive_mem_allocate(len));\n-  if (archived_oop != nullptr) {\n-    count_allocation(len);\n-    Copy::aligned_disjoint_words(cast_from_oop<HeapWord*>(obj), cast_from_oop<HeapWord*>(archived_oop), len);\n-    \/\/ Reinitialize markword to remove age\/marking\/locking\/etc.\n-    \/\/\n-    \/\/ We need to retain the identity_hash, because it may have been used by some hashtables\n-    \/\/ in the shared heap. This also has the side effect of pre-initializing the\n-    \/\/ identity_hash for all shared objects, so they are less likely to be written\n-    \/\/ into during run time, increasing the potential of memory sharing.\n-    int hash_original = obj->identity_hash();\n-    archived_oop->set_mark(markWord::prototype().copy_set_hash(hash_original));\n-    assert(archived_oop->mark().is_unlocked(), \"sanity\");\n-\n-    DEBUG_ONLY(int hash_archived = archived_oop->identity_hash());\n-    assert(hash_original == hash_archived, \"Different hash codes: original %x, archived %x\", hash_original, hash_archived);\n-\n-    ArchivedObjectCache* cache = archived_object_cache();\n-    CachedOopInfo info = make_cached_oop_info(archived_oop);\n-    cache->put(obj, info);\n-    if (_original_object_table != nullptr) {\n-      _original_object_table->put(archived_oop, obj);\n-    }\n-    mark_native_pointers(obj, archived_oop);\n@@ -346,2 +295,2 @@\n-      log_debug(cds, heap)(\"Archived heap object \" PTR_FORMAT \" ==> \" PTR_FORMAT \" : %s\",\n-                           p2i(obj), p2i(archived_oop), obj->klass()->external_name());\n+      log_debug(cds, heap)(\"Archived heap object \" PTR_FORMAT \" : %s\",\n+                           p2i(obj), obj->klass()->external_name());\n@@ -349,7 +298,15 @@\n-  } else {\n-    log_error(cds, heap)(\n-      \"Cannot allocate space for object \" PTR_FORMAT \" in archived heap region\",\n-      p2i(obj));\n-    log_error(cds)(\"Out of memory. Please run with a larger Java heap, current MaxHeapSize = \"\n-        SIZE_FORMAT \"M\", MaxHeapSize\/M);\n-    os::_exit(-1);\n+\n+    if (java_lang_Module::is_instance(obj)) {\n+      if (Modules::check_module_oop(obj)) {\n+        Modules::update_oops_in_archived_module(obj, append_root(obj));\n+      }\n+      java_lang_Module::set_module_entry(obj, nullptr);\n+    } else if (java_lang_ClassLoader::is_instance(obj)) {\n+      \/\/ class_data will be restored explicitly at run time.\n+      guarantee(obj == SystemDictionary::java_platform_loader() ||\n+                obj == SystemDictionary::java_system_loader() ||\n+                java_lang_ClassLoader::loader_data(obj) == nullptr, \"must be\");\n+      java_lang_ClassLoader::release_set_loader_data(obj, nullptr);\n+    }\n+\n+    return true;\n@@ -357,1 +314,0 @@\n-  return archived_oop;\n@@ -427,2 +383,2 @@\n-      oop archived_m = archive_reachable_objects_from(1, _default_subgraph_info, m, \/*is_closed_archive=*\/ false);\n-      assert(archived_m != nullptr, \"sanity\");\n+      bool success = archive_reachable_objects_from(1, _default_subgraph_info, m, \/*is_closed_archive=*\/ false);\n+      assert(success, \"sanity\");\n@@ -431,2 +387,2 @@\n-        \"Archived %s mirror object from \" PTR_FORMAT \" ==> \" PTR_FORMAT,\n-        type2name(bt), p2i(m), p2i(archived_m));\n+        \"Archived %s mirror object from \" PTR_FORMAT,\n+        type2name(bt), p2i(m));\n@@ -434,1 +390,1 @@\n-      Universe::set_archived_basic_type_mirror_index(bt, append_root(archived_m));\n+      Universe::set_archived_basic_type_mirror_index(bt, append_root(m));\n@@ -445,3 +401,3 @@\n-      oop archived_m = archive_reachable_objects_from(1, _default_subgraph_info, m, \/*is_closed_archive=*\/ false);\n-      guarantee(archived_m != nullptr, \"scratch mirrors should not point to any unachivable objects\");\n-      buffered_k->set_archived_java_mirror(append_root(archived_m));\n+      bool success = archive_reachable_objects_from(1, _default_subgraph_info, m, \/*is_closed_archive=*\/ false);\n+      guarantee(success, \"scratch mirrors should not point to any unachivable objects\");\n+      buffered_k->set_archived_java_mirror(append_root(m));\n@@ -450,2 +406,2 @@\n-        \"Archived %s mirror object from \" PTR_FORMAT \" ==> \" PTR_FORMAT,\n-        buffered_k->external_name(), p2i(m), p2i(archived_m));\n+        \"Archived %s mirror object from \" PTR_FORMAT,\n+        buffered_k->external_name(), p2i(m));\n@@ -457,5 +413,5 @@\n-        if (rr != nullptr && !is_too_large_to_archive(rr)) {\n-          oop archived_obj = HeapShared::archive_reachable_objects_from(1, _default_subgraph_info, rr,\n-                                                                        \/*is_closed_archive=*\/false);\n-          assert(archived_obj != nullptr,  \"already checked not too large to archive\");\n-          int root_index = append_root(archived_obj);\n+        if (rr != nullptr && !ArchiveHeapWriter::is_too_large_to_archive(rr)) {\n+          bool success = HeapShared::archive_reachable_objects_from(1, _default_subgraph_info, rr,\n+                                                                    \/*is_closed_archive=*\/false);\n+          assert(success, \"must be\");\n+          int root_index = append_root(rr);\n@@ -471,1 +427,1 @@\n-void HeapShared::mark_native_pointers(oop orig_obj, oop archived_obj) {\n+void HeapShared::mark_native_pointers(oop orig_obj) {\n@@ -473,21 +429,2 @@\n-    mark_one_native_pointer(archived_obj, java_lang_Class::klass_offset());\n-    mark_one_native_pointer(archived_obj, java_lang_Class::array_klass_offset());\n-  }\n-}\n-\n-void HeapShared::mark_one_native_pointer(oop archived_obj, int offset) {\n-  Metadata* ptr = archived_obj->metadata_field_acquire(offset);\n-  if (ptr != nullptr) {\n-    \/\/ Set the native pointer to the requested address (at runtime, if the metadata\n-    \/\/ is mapped at the default location, it will be at this address).\n-    address buffer_addr = ArchiveBuilder::current()->get_buffered_addr((address)ptr);\n-    address requested_addr = ArchiveBuilder::current()->to_requested(buffer_addr);\n-    archived_obj->metadata_field_put(offset, (Metadata*)requested_addr);\n-\n-    \/\/ Remember this pointer. At runtime, if the metadata is mapped at a non-default\n-    \/\/ location, the pointer needs to be patched (see ArchiveHeapLoader::patch_native_pointers()).\n-    _native_pointers->append(archived_obj->field_addr<Metadata*>(offset));\n-\n-    log_debug(cds, heap, mirror)(\n-        \"Marked metadata field at %d: \" PTR_FORMAT \" ==> \" PTR_FORMAT,\n-         offset, p2i(ptr), p2i(requested_addr));\n+    ArchiveHeapWriter::mark_native_pointer(orig_obj, java_lang_Class::klass_offset());\n+    ArchiveHeapWriter::mark_native_pointer(orig_obj, java_lang_Class::array_klass_offset());\n@@ -547,3 +484,3 @@\n-        oop archived_oop_field = archive_reachable_objects_from(level, subgraph_info, oop_field, is_closed_archive);\n-        int root_index = append_root(archived_oop_field);\n-        log_info(cds, heap)(\"Archived enum obj @%d %s::%s (\" INTPTR_FORMAT \" -> \" INTPTR_FORMAT \")\",\n+        bool success = archive_reachable_objects_from(level, subgraph_info, oop_field, is_closed_archive);\n+        int root_index = append_root(oop_field);\n+        log_info(cds, heap)(\"Archived enum obj @%d %s::%s (\" INTPTR_FORMAT \")\",\n@@ -551,1 +488,1 @@\n-                            p2i((oopDesc*)oop_field), p2i((oopDesc*)archived_oop_field));\n+                            p2i((oopDesc*)oop_field));\n@@ -585,19 +522,0 @@\n-void HeapShared::run_full_gc_in_vm_thread() {\n-  if (HeapShared::can_write()) {\n-    \/\/ Avoid fragmentation while archiving heap objects.\n-    \/\/ We do this inside a safepoint, so that no further allocation can happen after GC\n-    \/\/ has finished.\n-    if (GCLocker::is_active()) {\n-      \/\/ Just checking for safety ...\n-      \/\/ This should not happen during -Xshare:dump. If you see this, probably the Java core lib\n-      \/\/ has been modified such that JNI code is executed in some clean up threads after\n-      \/\/ we have finished class loading.\n-      log_warning(cds)(\"GC locker is held, unable to start extra compacting GC. This may produce suboptimal results.\");\n-    } else {\n-      log_info(cds)(\"Run GC ...\");\n-      Universe::heap()->collect_as_vm_thread(GCCause::_archive_time_gc);\n-      log_info(cds)(\"Run GC done\");\n-    }\n-  }\n-}\n-\n@@ -605,4 +523,3 @@\n-                                 GrowableArray<MemRegion>* open_regions) {\n-\n-  G1HeapVerifier::verify_ready_for_archiving();\n-\n+                                 GrowableArray<MemRegion>* open_regions,\n+                                 GrowableArray<ArchiveHeapBitmapInfo>* closed_bitmaps,\n+                                 GrowableArray<ArchiveHeapBitmapInfo>* open_bitmaps) {\n@@ -615,1 +532,1 @@\n-    create_archived_object_cache(log_is_enabled(Info, cds, map));\n+    create_archived_object_cache();\n@@ -623,1 +540,3 @@\n-    copy_closed_objects(closed_regions);\n+    copy_closed_objects();\n+\n+    _copying_open_region_objects = true;\n@@ -626,1 +545,1 @@\n-    copy_open_objects(open_regions);\n+    copy_open_objects();\n@@ -632,1 +551,1 @@\n-  G1HeapVerifier::verify_archive_regions();\n+  ArchiveHeapWriter::write(_pending_roots, closed_regions, open_regions, closed_bitmaps, open_bitmaps);\n@@ -641,5 +560,4 @@\n-    typeArrayOop value = java_lang_String::value_no_keepalive(s);\n-    if (!HeapShared::is_too_large_to_archive(value)) {\n-      oop archived_s = archive_reachable_objects_from(1, _default_subgraph_info,\n-                                                      s, \/*is_closed_archive=*\/true);\n-      assert(archived_s != nullptr, \"already checked not too large to archive\");\n+    if (!ArchiveHeapWriter::is_string_too_large_to_archive(s)) {\n+      bool success = archive_reachable_objects_from(1, _default_subgraph_info,\n+                                                    s, \/*is_closed_archive=*\/true);\n+      assert(success, \"must be\");\n@@ -648,1 +566,1 @@\n-      java_lang_String::set_deduplication_forbidden(archived_s);\n+      java_lang_String::set_deduplication_forbidden(s);\n@@ -656,1 +574,1 @@\n-void HeapShared::copy_closed_objects(GrowableArray<MemRegion>* closed_regions) {\n+void HeapShared::copy_closed_objects() {\n@@ -659,2 +577,0 @@\n-  G1CollectedHeap::heap()->begin_archive_alloc_range();\n-\n@@ -667,3 +583,0 @@\n-\n-  G1CollectedHeap::heap()->end_archive_alloc_range(closed_regions,\n-                                                   os::vm_allocation_granularity());\n@@ -672,1 +585,1 @@\n-void HeapShared::copy_open_objects(GrowableArray<MemRegion>* open_regions) {\n+void HeapShared::copy_open_objects() {\n@@ -675,2 +588,0 @@\n-  G1CollectedHeap::heap()->begin_archive_alloc_range(true \/* open *\/);\n-\n@@ -688,37 +599,0 @@\n-\n-  copy_roots();\n-\n-  G1CollectedHeap::heap()->end_archive_alloc_range(open_regions,\n-                                                   os::vm_allocation_granularity());\n-}\n-\n-\/\/ Copy _pending_archive_roots into an objArray\n-void HeapShared::copy_roots() {\n-  \/\/ HeapShared::roots() points into an ObjArray in the open archive region. A portion of the\n-  \/\/ objects in this array are discovered during HeapShared::archive_objects(). For example,\n-  \/\/ in HeapShared::archive_reachable_objects_from() ->  HeapShared::check_enum_obj().\n-  \/\/ However, HeapShared::archive_objects() happens inside a safepoint, so we can't\n-  \/\/ allocate a \"regular\" ObjArray and pass the result to HeapShared::archive_object().\n-  \/\/ Instead, we have to roll our own alloc\/copy routine here.\n-  int length = _pending_roots != nullptr ? _pending_roots->length() : 0;\n-  size_t size = objArrayOopDesc::object_size(length);\n-  Klass* k = Universe::objectArrayKlassObj(); \/\/ already relocated to point to archived klass\n-  HeapWord* mem = G1CollectedHeap::heap()->archive_mem_allocate(size);\n-\n-  memset(mem, 0, size * BytesPerWord);\n-  {\n-    \/\/ This is copied from MemAllocator::finish\n-    oopDesc::set_mark(mem, markWord::prototype());\n-    oopDesc::release_set_klass(mem, k);\n-  }\n-  {\n-    \/\/ This is copied from ObjArrayAllocator::initialize\n-    arrayOopDesc::set_length(mem, length);\n-  }\n-\n-  _roots = OopHandle(Universe::vm_global(), cast_to_oop(mem));\n-  for (int i = 0; i < length; i++) {\n-    roots()->obj_at_put(i, _pending_roots->at(i));\n-  }\n-  log_info(cds)(\"archived obj roots[%d] = \" SIZE_FORMAT \" words, klass = %p, obj = %p\", length, size, k, mem);\n-  count_allocation(roots()->size());\n@@ -988,1 +862,3 @@\n-    roots_oop = roots();\n+    if (HeapShared::can_write()) {\n+      roots_oop = ArchiveHeapWriter::heap_roots_requested_address();\n+    }\n@@ -1226,2 +1102,1 @@\n-  oop _orig_referencing_obj;\n-  oop _archived_referencing_obj;\n+  oop _referencing_obj;\n@@ -1238,1 +1113,1 @@\n-                           oop orig, oop archived) :\n+                           oop orig) :\n@@ -1242,1 +1117,1 @@\n-    _orig_referencing_obj(orig), _archived_referencing_obj(archived) {\n+    _referencing_obj(orig) {\n@@ -1256,5 +1131,1 @@\n-      assert(!HeapShared::is_archived_object_during_dumptime(obj),\n-             \"original objects must not point to archived objects\");\n-\n-      size_t field_delta = pointer_delta(p, _orig_referencing_obj, sizeof(char));\n-      T* new_p = (T*)(cast_from_oop<address>(_archived_referencing_obj) + field_delta);\n+      size_t field_delta = pointer_delta(p, _referencing_obj, sizeof(char));\n@@ -1265,1 +1136,1 @@\n-                             _orig_referencing_obj->klass()->external_name(), field_delta,\n+                             _referencing_obj->klass()->external_name(), field_delta,\n@@ -1274,1 +1145,1 @@\n-      oop archived = HeapShared::archive_reachable_objects_from(\n+      bool success = HeapShared::archive_reachable_objects_from(\n@@ -1276,9 +1147,1 @@\n-      assert(archived != nullptr, \"VM should have exited with unarchivable objects for _level > 1\");\n-      assert(HeapShared::is_archived_object_during_dumptime(archived), \"must be\");\n-\n-      if (!_record_klasses_only) {\n-        \/\/ Update the reference in the archived copy of the referencing object.\n-        log_debug(cds, heap)(\"(%d) updating oop @[\" PTR_FORMAT \"] \" PTR_FORMAT \" ==> \" PTR_FORMAT,\n-                             _level, p2i(new_p), p2i(obj), p2i(archived));\n-        RawAccess<IS_NOT_NULL>::oop_store(new_p, archived);\n-      }\n+      assert(success, \"VM should have exited with unarchivable objects for _level > 1\");\n@@ -1290,1 +1153,1 @@\n-  oop orig_referencing_obj()                  { return _orig_referencing_obj; }\n+  oop referencing_obj()                       { return _referencing_obj;      }\n@@ -1296,2 +1159,1 @@\n-HeapShared::CachedOopInfo HeapShared::make_cached_oop_info(oop orig_obj) {\n-  CachedOopInfo info;\n+HeapShared::CachedOopInfo HeapShared::make_cached_oop_info() {\n@@ -1299,6 +1161,2 @@\n-\n-  info._subgraph_info = (walker == nullptr) ? nullptr : walker->subgraph_info();\n-  info._referrer = (walker == nullptr) ? nullptr : walker->orig_referencing_obj();\n-  info._obj = orig_obj;\n-\n-  return info;\n+  oop referrer = (walker == nullptr) ? nullptr : walker->referencing_obj();\n+  return CachedOopInfo(referrer, _copying_open_region_objects);\n@@ -1327,4 +1185,4 @@\n-oop HeapShared::archive_reachable_objects_from(int level,\n-                                               KlassSubGraphInfo* subgraph_info,\n-                                               oop orig_obj,\n-                                               bool is_closed_archive) {\n+bool HeapShared::archive_reachable_objects_from(int level,\n+                                                KlassSubGraphInfo* subgraph_info,\n+                                                oop orig_obj,\n+                                                bool is_closed_archive) {\n@@ -1332,1 +1190,0 @@\n-  assert(!is_archived_object_during_dumptime(orig_obj), \"sanity\");\n@@ -1353,7 +1210,0 @@\n-  oop archived_obj = find_archived_heap_object(orig_obj);\n-  if (java_lang_String::is_instance(orig_obj) && archived_obj != nullptr) {\n-    \/\/ To save time, don't walk strings that are already archived. They just contain\n-    \/\/ pointers to a type array, whose klass doesn't need to be recorded.\n-    return archived_obj;\n-  }\n-\n@@ -1362,1 +1212,1 @@\n-    return archived_obj;\n+    return true;\n@@ -1367,2 +1217,3 @@\n-  bool record_klasses_only = (archived_obj != nullptr);\n-  if (archived_obj == nullptr) {\n+  bool already_archived = has_been_archived(orig_obj);\n+  bool record_klasses_only = already_archived;\n+  if (!already_archived) {\n@@ -1370,2 +1221,1 @@\n-    archived_obj = archive_object(orig_obj);\n-    if (archived_obj == nullptr) {\n+    if (!archive_object(orig_obj)) {\n@@ -1381,1 +1231,1 @@\n-        return nullptr;\n+        return false;\n@@ -1389,13 +1239,0 @@\n-\n-    if (java_lang_Module::is_instance(orig_obj)) {\n-      if (Modules::check_module_oop(orig_obj)) {\n-        Modules::update_oops_in_archived_module(orig_obj, append_root(archived_obj));\n-      }\n-      java_lang_Module::set_module_entry(archived_obj, nullptr);\n-    } else if (java_lang_ClassLoader::is_instance(orig_obj)) {\n-      \/\/ class_data will be restored explicitly at run time.\n-      guarantee(orig_obj == SystemDictionary::java_platform_loader() ||\n-                orig_obj == SystemDictionary::java_system_loader() ||\n-                java_lang_ClassLoader::loader_data(orig_obj) == nullptr, \"must be\");\n-      java_lang_ClassLoader::release_set_loader_data(archived_obj, nullptr);\n-    }\n@@ -1404,1 +1241,0 @@\n-  assert(archived_obj != nullptr, \"must be\");\n@@ -1409,1 +1245,1 @@\n-                                  subgraph_info, orig_obj, archived_obj);\n+                                  subgraph_info, orig_obj);\n@@ -1416,1 +1252,1 @@\n-  return archived_obj;\n+  return true;\n@@ -1475,1 +1311,1 @@\n-    oop af = archive_reachable_objects_from(1, subgraph_info, f, is_closed_archive);\n+    bool success = archive_reachable_objects_from(1, subgraph_info, f, is_closed_archive);\n@@ -1477,1 +1313,1 @@\n-    if (af == nullptr) {\n+    if (!success) {\n@@ -1484,2 +1320,2 @@\n-      subgraph_info->add_subgraph_entry_field(field_offset, af, is_closed_archive);\n-      log_info(cds, heap)(\"Archived field %s::%s => \" PTR_FORMAT, klass_name, field_name, p2i(af));\n+      subgraph_info->add_subgraph_entry_field(field_offset, f, is_closed_archive);\n+      log_info(cds, heap)(\"Archived field %s::%s => \" PTR_FORMAT, klass_name, field_name, p2i(f));\n@@ -1496,3 +1332,0 @@\n- private:\n-  bool _is_archived;\n-\n@@ -1500,2 +1333,0 @@\n-  VerifySharedOopClosure(bool is_archived) : _is_archived(is_archived) {}\n-\n@@ -1509,1 +1340,1 @@\n-      HeapShared::verify_reachable_objects_from(obj, _is_archived);\n+      HeapShared::verify_reachable_objects_from(obj);\n@@ -1526,2 +1357,1 @@\n-  oop archived_obj = find_archived_heap_object(orig_obj);\n-  if (archived_obj == nullptr) {\n+  if (!has_been_archived(orig_obj)) {\n@@ -1535,1 +1365,1 @@\n-  verify_reachable_objects_from(orig_obj, false);\n+  verify_reachable_objects_from(orig_obj);\n@@ -1537,8 +1367,0 @@\n-\n-  \/\/ Note: we could also verify that all objects reachable from the archived\n-  \/\/ copy of orig_obj can only point to archived objects, with:\n-  \/\/      init_seen_objects_table();\n-  \/\/      verify_reachable_objects_from(archived_obj, true);\n-  \/\/      init_seen_objects_table();\n-  \/\/ but that's already done in G1HeapVerifier::verify_archive_regions so we\n-  \/\/ won't do it here.\n@@ -1547,1 +1369,1 @@\n-void HeapShared::verify_reachable_objects_from(oop obj, bool is_archived) {\n+void HeapShared::verify_reachable_objects_from(oop obj) {\n@@ -1551,10 +1373,2 @@\n-\n-    if (is_archived) {\n-      assert(is_archived_object_during_dumptime(obj), \"must be\");\n-      assert(find_archived_heap_object(obj) == nullptr, \"must be\");\n-    } else {\n-      assert(!is_archived_object_during_dumptime(obj), \"must be\");\n-      assert(find_archived_heap_object(obj) != nullptr, \"must be\");\n-    }\n-\n-    VerifySharedOopClosure walker(is_archived);\n+    assert(has_been_archived(obj), \"must be\");\n+    VerifySharedOopClosure walker;\n@@ -1814,1 +1628,0 @@\n-    _native_pointers = new GrowableArrayCHeap<Metadata**, mtClassShared>(2048);\n@@ -1880,0 +1693,1 @@\n+  assert(!ArchiveHeapWriter::is_string_too_large_to_archive(string), \"must be\");\n@@ -1884,0 +1698,1 @@\n+#ifndef PRODUCT\n@@ -1915,4 +1730,0 @@\n-      if (DumpSharedSpaces) {\n-        \/\/ Make heap content deterministic.\n-        *p = HeapShared::to_requested_address(*p);\n-      }\n@@ -1926,1 +1737,1 @@\n-\n+#endif\n@@ -1955,0 +1766,1 @@\n+#ifndef PRODUCT\n@@ -1962,1 +1774,0 @@\n-  ArchiveBuilder* builder = DumpSharedSpaces ? ArchiveBuilder::current() : nullptr;\n@@ -1969,3 +1780,0 @@\n-    if (DumpSharedSpaces) {\n-      builder->relocate_klass_ptr_of_oop(o);\n-    }\n@@ -1980,28 +1788,1 @@\n-\n-ResourceBitMap HeapShared::calculate_ptrmap(MemRegion region) {\n-  size_t num_bits = region.byte_size() \/ sizeof(Metadata*);\n-  ResourceBitMap oopmap(num_bits);\n-\n-  Metadata** start = (Metadata**)region.start();\n-  Metadata** end   = (Metadata**)region.end();\n-\n-  int num_non_null_ptrs = 0;\n-  int len = _native_pointers->length();\n-  for (int i = 0; i < len; i++) {\n-    Metadata** p = _native_pointers->at(i);\n-    if (start <= p && p < end) {\n-      assert(*p != nullptr, \"must be non-null\");\n-      num_non_null_ptrs ++;\n-      size_t idx = p - start;\n-      oopmap.set_bit(idx);\n-    }\n-  }\n-\n-  log_info(cds, heap)(\"calculate_ptrmap: marked %d non-null native pointers out of \"\n-                      SIZE_FORMAT \" possible locations\", num_non_null_ptrs, num_bits);\n-  if (num_non_null_ptrs > 0) {\n-    return oopmap;\n-  } else {\n-    return ResourceBitMap(0);\n-  }\n-}\n+#endif \/\/ !PRODUCT\n","filename":"src\/hotspot\/share\/cds\/heapShared.cpp","additions":120,"deletions":339,"binary":false,"changes":459,"status":"modified"},{"patch":"@@ -168,0 +168,1 @@\n+  static bool _copying_open_region_objects;\n@@ -169,1 +170,0 @@\n-  static GrowableArrayCHeap<Metadata**, mtClassShared>* _native_pointers;\n@@ -186,5 +186,15 @@\n-  struct CachedOopInfo {\n-    KlassSubGraphInfo* _subgraph_info;\n-    oop _referrer;\n-    oop _obj;\n-    CachedOopInfo() :_subgraph_info(), _referrer(), _obj() {}\n+  class CachedOopInfo {\n+    \/\/ See \"TEMP notes: What are these?\" in archiveHeapWriter.hpp\n+    oop _orig_referrer;\n+\n+    \/\/ The location of this object inside ArchiveHeapWriter::_buffer\n+    size_t _buffer_offset;\n+    bool _in_open_region;\n+  public:\n+    CachedOopInfo(oop orig_referrer, bool in_open_region)\n+      : _orig_referrer(orig_referrer),\n+        _buffer_offset(0), _in_open_region(in_open_region) {}\n+    oop orig_referrer()             const { return _orig_referrer;   }\n+    bool in_open_region()           const { return _in_open_region;  }\n+    void set_buffer_offset(size_t offset) { _buffer_offset = offset; }\n+    size_t buffer_offset()          const { return _buffer_offset;   }\n@@ -206,7 +216,0 @@\n-  typedef ResourceHashtable<oop, oop,\n-      36137, \/\/ prime number\n-      AnyObj::C_HEAP,\n-      mtClassShared,\n-      HeapShared::oop_hash> OriginalObjectTable;\n-  static OriginalObjectTable* _original_object_table;\n-\n@@ -240,1 +243,1 @@\n-  static CachedOopInfo make_cached_oop_info(oop orig_obj);\n+  static CachedOopInfo make_cached_oop_info();\n@@ -254,1 +257,1 @@\n-  static void verify_reachable_objects_from(oop obj, bool is_archived) PRODUCT_RETURN;\n+  static void verify_reachable_objects_from(oop obj) PRODUCT_RETURN;\n@@ -319,1 +322,1 @@\n-  static oop archive_object(oop obj);\n+  static bool archive_object(oop obj);\n@@ -341,2 +344,3 @@\n-  static void mark_native_pointers(oop orig_obj, oop archived_obj);\n-  static void mark_one_native_pointer(oop archived_obj, int offset);\n+  static void mark_native_pointers(oop orig_obj);\n+  static bool has_been_archived(oop orig_obj);\n+  static void archive_java_mirrors();\n@@ -345,1 +349,1 @@\n-  static void create_archived_object_cache(bool create_orig_table) {\n+  static void create_archived_object_cache() {\n@@ -348,6 +352,0 @@\n-    if (create_orig_table) {\n-      _original_object_table =\n-        new (mtClass)OriginalObjectTable();\n-    } else {\n-      _original_object_table = nullptr;\n-    }\n@@ -358,4 +356,0 @@\n-    if (_original_object_table != nullptr) {\n-      delete _original_object_table;\n-      _original_object_table = nullptr;\n-    }\n@@ -366,14 +360,0 @@\n-  static oop get_original_object(oop archived_object) {\n-    assert(_original_object_table != nullptr, \"sanity\");\n-    oop* r = _original_object_table->get(archived_object);\n-    if (r == nullptr) {\n-      return nullptr;\n-    } else {\n-      return *r;\n-    }\n-  }\n-\n-  static bool is_too_large_to_archive(oop o);\n-  static oop find_archived_heap_object(oop obj);\n-\n-  static void archive_java_mirrors();\n@@ -382,3 +362,5 @@\n-                              GrowableArray<MemRegion>* open_regions);\n-  static void copy_closed_objects(GrowableArray<MemRegion>* closed_regions);\n-  static void copy_open_objects(GrowableArray<MemRegion>* open_regions);\n+                              GrowableArray<MemRegion>* open_regions,\n+                              GrowableArray<ArchiveHeapBitmapInfo>* closed_bitmaps,\n+                              GrowableArray<ArchiveHeapBitmapInfo>* open_bitmaps);\n+  static void copy_closed_objects();\n+  static void copy_open_objects();\n@@ -386,4 +368,4 @@\n-  static oop archive_reachable_objects_from(int level,\n-                                            KlassSubGraphInfo* subgraph_info,\n-                                            oop orig_obj,\n-                                            bool is_closed_archive);\n+  static bool archive_reachable_objects_from(int level,\n+                                             KlassSubGraphInfo* subgraph_info,\n+                                             oop orig_obj,\n+                                             bool is_closed_archive);\n@@ -392,1 +374,0 @@\n-  static ResourceBitMap calculate_ptrmap(MemRegion region); \/\/ marks all the native pointers\n@@ -429,2 +410,0 @@\n-  static void run_full_gc_in_vm_thread() NOT_CDS_JAVA_HEAP_RETURN;\n-\n@@ -437,2 +416,0 @@\n-  static bool is_archived_object_during_dumptime(oop p) NOT_CDS_JAVA_HEAP_RETURN_(false);\n-\n","filename":"src\/hotspot\/share\/cds\/heapShared.hpp","additions":32,"deletions":55,"binary":false,"changes":87,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"cds\/archiveHeapWriter.hpp\"\n@@ -84,3 +85,0 @@\n-#if INCLUDE_G1GC\n-#include \"gc\/g1\/g1CollectedHeap.inline.hpp\"\n-#endif\n@@ -333,11 +331,5 @@\n-#if INCLUDE_G1GC\n-        if (UseG1GC) {\n-          typeArrayOop body = java_lang_String::value(str);\n-          const HeapRegion* hr = G1CollectedHeap::heap()->heap_region_containing(body);\n-          if (hr->is_humongous()) {\n-            \/\/ Don't keep it alive, so it will be GC'ed before we dump the strings, in order\n-            \/\/ to maximize free heap space and minimize fragmentation.\n-            log_warning(cds, heap)(\"[line %d] extra interned string ignored; size too large: %d\",\n-                                reader.last_line_no(), utf8_length);\n-            continue;\n-          }\n+#if INCLUDE_CDS_JAVA_HEAP\n+        if (ArchiveHeapWriter::is_string_too_large_to_archive(str)) {\n+          log_warning(cds, heap)(\"[line %d] extra interned string ignored; size too large: %d\",\n+                                 reader.last_line_no(), utf8_length);\n+          continue;\n@@ -345,1 +337,0 @@\n-#endif\n@@ -349,0 +340,1 @@\n+#endif\n@@ -438,1 +430,1 @@\n-class VM_PopulateDumpSharedSpace : public VM_GC_Operation {\n+class VM_PopulateDumpSharedSpace : public VM_Operation {\n@@ -447,5 +439,0 @@\n-  void dump_heap_bitmaps() NOT_CDS_JAVA_HEAP_RETURN;\n-  void dump_heap_bitmaps(GrowableArray<MemRegion>* regions,\n-                         GrowableArray<ArchiveHeapBitmapInfo>* bitmaps);\n-  void dump_one_heap_bitmap(MemRegion region, GrowableArray<ArchiveHeapBitmapInfo>* bitmaps,\n-                            ResourceBitMap bitmap, bool is_oopmap);\n@@ -460,2 +447,1 @@\n-  VM_PopulateDumpSharedSpace() :\n-    VM_GC_Operation(0 \/* total collections, ignored *\/, GCCause::_archive_time_gc),\n+  VM_PopulateDumpSharedSpace() : VM_Operation(),\n@@ -510,3 +496,0 @@\n-  \/\/ Write the bitmaps for patching the archive heap regions\n-  dump_heap_bitmaps();\n-\n@@ -517,2 +500,0 @@\n-  HeapShared::run_full_gc_in_vm_thread();\n-\n@@ -822,3 +803,4 @@\n-    if (use_full_module_graph()) {\n-      HeapShared::reset_archived_object_states(CHECK);\n-    }\n+  ArchiveHeapWriter::init();\n+  if (use_full_module_graph()) {\n+    HeapShared::reset_archived_object_states(CHECK);\n+  }\n@@ -897,1 +879,4 @@\n-  HeapShared::archive_objects(_closed_heap_regions, _open_heap_regions);\n+  _closed_heap_bitmaps = new GrowableArray<ArchiveHeapBitmapInfo>(2);\n+  _open_heap_bitmaps = new GrowableArray<ArchiveHeapBitmapInfo>(2);\n+  HeapShared::archive_objects(_closed_heap_regions, _open_heap_regions,\n+                              _closed_heap_bitmaps, _open_heap_bitmaps);\n@@ -901,50 +886,0 @@\n-\n-void VM_PopulateDumpSharedSpace::dump_heap_bitmaps() {\n-  if (HeapShared::can_write()) {\n-    _closed_heap_bitmaps = new GrowableArray<ArchiveHeapBitmapInfo>(2);\n-    dump_heap_bitmaps(_closed_heap_regions, _closed_heap_bitmaps);\n-\n-    _open_heap_bitmaps = new GrowableArray<ArchiveHeapBitmapInfo>(2);\n-    dump_heap_bitmaps(_open_heap_regions, _open_heap_bitmaps);\n-  }\n-}\n-\n-void VM_PopulateDumpSharedSpace::dump_heap_bitmaps(GrowableArray<MemRegion>* regions,\n-                                                   GrowableArray<ArchiveHeapBitmapInfo>* bitmaps) {\n-  for (int i = 0; i < regions->length(); i++) {\n-    MemRegion region = regions->at(i);\n-    ResourceBitMap oopmap = HeapShared::calculate_oopmap(region);\n-    ResourceBitMap ptrmap = HeapShared::calculate_ptrmap(region);\n-    dump_one_heap_bitmap(region, bitmaps, oopmap, true);\n-    dump_one_heap_bitmap(region, bitmaps, ptrmap, false);\n-  }\n-}\n-\n-void VM_PopulateDumpSharedSpace::dump_one_heap_bitmap(MemRegion region,\n-                                                      GrowableArray<ArchiveHeapBitmapInfo>* bitmaps,\n-                                                      ResourceBitMap bitmap, bool is_oopmap) {\n-  size_t size_in_bits = bitmap.size();\n-  size_t size_in_bytes;\n-  uintptr_t* buffer;\n-\n-  if (size_in_bits > 0) {\n-    size_in_bytes = bitmap.size_in_bytes();\n-    buffer = (uintptr_t*)NEW_C_HEAP_ARRAY(char, size_in_bytes, mtInternal);\n-    bitmap.write_to(buffer, size_in_bytes);\n-  } else {\n-    size_in_bytes = 0;\n-    buffer = nullptr;\n-  }\n-\n-  log_info(cds, heap)(\"%s = \" INTPTR_FORMAT \" (\" SIZE_FORMAT_W(6) \" bytes) for heap region \"\n-                      INTPTR_FORMAT \" (\" SIZE_FORMAT_W(8) \" bytes)\",\n-                      is_oopmap ? \"Oopmap\" : \"Ptrmap\",\n-                      p2i(buffer), size_in_bytes,\n-                      p2i(region.start()), region.byte_size());\n-\n-  ArchiveHeapBitmapInfo info;\n-  info._map = (address)buffer;\n-  info._size_in_bits = size_in_bits;\n-  info._size_in_bytes = size_in_bytes;\n-  bitmaps->append(info);\n-}\n","filename":"src\/hotspot\/share\/cds\/metaspaceShared.cpp","additions":17,"deletions":82,"binary":false,"changes":99,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"cds\/archiveHeapWriter.hpp\"\n@@ -773,8 +774,8 @@\n-    oop new_s = HeapShared::find_archived_heap_object(s);\n-    if (new_s != nullptr) { \/\/ could be null if the string is too big\n-      unsigned int hash = java_lang_String::hash_code(s);\n-      if (UseCompressedOops) {\n-        _writer->add(hash, CompressedOops::narrow_oop_value(new_s));\n-      } else {\n-        _writer->add(hash, compute_delta(new_s));\n-      }\n+    assert(!ArchiveHeapWriter::is_string_too_large_to_archive(s), \"must be\");\n+    oop req_s = ArchiveHeapWriter::source_obj_to_requested_obj(s);\n+    assert(req_s != nullptr, \"must have been archived\");\n+    unsigned int hash = java_lang_String::hash_code(s);\n+    if (UseCompressedOops) {\n+      _writer->add(hash, CompressedOops::narrow_oop_value(req_s));\n+    } else {\n+      _writer->add(hash, compute_delta(req_s));\n","filename":"src\/hotspot\/share\/classfile\/stringTable.cpp","additions":9,"deletions":8,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2014, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -487,150 +487,0 @@\n-\n-G1ArchiveAllocator* G1ArchiveAllocator::create_allocator(G1CollectedHeap* g1h, bool open) {\n-  return new G1ArchiveAllocator(g1h, open);\n-}\n-\n-bool G1ArchiveAllocator::alloc_new_region() {\n-  \/\/ Allocate the highest free region in the reserved heap,\n-  \/\/ and add it to our list of allocated regions. It is marked\n-  \/\/ archive and added to the old set.\n-  HeapRegion* hr = _g1h->alloc_highest_free_region();\n-  if (hr == NULL) {\n-    return false;\n-  }\n-  assert(hr->is_empty(), \"expected empty region (index %u)\", hr->hrm_index());\n-  if (_open) {\n-    hr->set_open_archive();\n-  } else {\n-    hr->set_closed_archive();\n-  }\n-  _g1h->policy()->remset_tracker()->update_at_allocate(hr);\n-  _g1h->archive_set_add(hr);\n-  _g1h->hr_printer()->alloc(hr);\n-  _allocated_regions.append(hr);\n-  _allocation_region = hr;\n-\n-  \/\/ Set up _bottom and _max to begin allocating in the lowest\n-  \/\/ min_region_size'd chunk of the allocated G1 region.\n-  _bottom = hr->bottom();\n-  _max = _bottom + HeapRegion::min_region_size_in_words();\n-\n-  \/\/ Since we've modified the old set, call update_sizes.\n-  _g1h->monitoring_support()->update_sizes();\n-  return true;\n-}\n-\n-HeapWord* G1ArchiveAllocator::archive_mem_allocate(size_t word_size) {\n-  assert(word_size != 0, \"size must not be zero\");\n-  if (_allocation_region == NULL) {\n-    if (!alloc_new_region()) {\n-      return NULL;\n-    }\n-  }\n-  HeapWord* old_top = _allocation_region->top();\n-  assert(_bottom >= _allocation_region->bottom(),\n-         \"inconsistent allocation state: \" PTR_FORMAT \" < \" PTR_FORMAT,\n-         p2i(_bottom), p2i(_allocation_region->bottom()));\n-  assert(_max <= _allocation_region->end(),\n-         \"inconsistent allocation state: \" PTR_FORMAT \" > \" PTR_FORMAT,\n-         p2i(_max), p2i(_allocation_region->end()));\n-  assert(_bottom <= old_top && old_top <= _max,\n-         \"inconsistent allocation state: expected \"\n-         PTR_FORMAT \" <= \" PTR_FORMAT \" <= \" PTR_FORMAT,\n-         p2i(_bottom), p2i(old_top), p2i(_max));\n-\n-  \/\/ Try to allocate word_size in the current allocation chunk. Two cases\n-  \/\/ require special treatment:\n-  \/\/ 1. no enough space for word_size\n-  \/\/ 2. after allocating word_size, there's non-zero space left, but too small for the minimal filler\n-  \/\/ In both cases, we retire the current chunk and move on to the next one.\n-  size_t free_words = pointer_delta(_max, old_top);\n-  if (free_words < word_size ||\n-      ((free_words - word_size != 0) && (free_words - word_size < CollectedHeap::min_fill_size()))) {\n-    \/\/ Retiring the current chunk\n-    if (old_top != _max) {\n-      \/\/ Non-zero space; need to insert the filler\n-      size_t fill_size = free_words;\n-      CollectedHeap::fill_with_object(old_top, fill_size);\n-    }\n-    \/\/ Set the current chunk as \"full\"\n-    _allocation_region->set_top(_max);\n-\n-    \/\/ Check if we've just used up the last min_region_size'd chunk\n-    \/\/ in the current region, and if so, allocate a new one.\n-    if (_max != _allocation_region->end()) {\n-      \/\/ Shift to the next chunk\n-      old_top = _bottom = _max;\n-      _max = _bottom + HeapRegion::min_region_size_in_words();\n-    } else {\n-      if (!alloc_new_region()) {\n-        return NULL;\n-      }\n-      old_top = _allocation_region->bottom();\n-    }\n-  }\n-  assert(pointer_delta(_max, old_top) >= word_size, \"enough space left\");\n-  _allocation_region->set_top(old_top + word_size);\n-\n-  return old_top;\n-}\n-\n-void G1ArchiveAllocator::complete_archive(GrowableArray<MemRegion>* ranges,\n-                                          size_t end_alignment_in_bytes) {\n-  assert((end_alignment_in_bytes >> LogHeapWordSize) < HeapRegion::min_region_size_in_words(),\n-         \"alignment \" SIZE_FORMAT \" too large\", end_alignment_in_bytes);\n-  assert(is_aligned(end_alignment_in_bytes, HeapWordSize),\n-         \"alignment \" SIZE_FORMAT \" is not HeapWord (%u) aligned\", end_alignment_in_bytes, HeapWordSize);\n-\n-  \/\/ If we've allocated nothing, simply return.\n-  if (_allocation_region == NULL) {\n-    return;\n-  }\n-\n-  \/\/ If an end alignment was requested, insert filler objects.\n-  if (end_alignment_in_bytes != 0) {\n-    HeapWord* currtop = _allocation_region->top();\n-    HeapWord* newtop = align_up(currtop, end_alignment_in_bytes);\n-    size_t fill_size = pointer_delta(newtop, currtop);\n-    if (fill_size != 0) {\n-      if (fill_size < CollectedHeap::min_fill_size()) {\n-        \/\/ If the required fill is smaller than we can represent,\n-        \/\/ bump up to the next aligned address. We know we won't exceed the current\n-        \/\/ region boundary because the max supported alignment is smaller than the min\n-        \/\/ region size, and because the allocation code never leaves space smaller than\n-        \/\/ the min_fill_size at the top of the current allocation region.\n-        newtop = align_up(currtop + CollectedHeap::min_fill_size(),\n-                          end_alignment_in_bytes);\n-        fill_size = pointer_delta(newtop, currtop);\n-      }\n-      HeapWord* fill = archive_mem_allocate(fill_size);\n-      CollectedHeap::fill_with_objects(fill, fill_size);\n-    }\n-  }\n-\n-  \/\/ Loop through the allocated regions, and create MemRegions summarizing\n-  \/\/ the allocated address range, combining contiguous ranges. Add the\n-  \/\/ MemRegions to the GrowableArray provided by the caller.\n-  int index = _allocated_regions.length() - 1;\n-  assert(_allocated_regions.at(index) == _allocation_region,\n-         \"expected region %u at end of array, found %u\",\n-         _allocation_region->hrm_index(), _allocated_regions.at(index)->hrm_index());\n-  HeapWord* base_address = _allocation_region->bottom();\n-  HeapWord* top = base_address;\n-\n-  while (index >= 0) {\n-    HeapRegion* next = _allocated_regions.at(index);\n-    HeapWord* new_base = next->bottom();\n-    HeapWord* new_top = next->top();\n-    if (new_base != top) {\n-      ranges->append(MemRegion(base_address, pointer_delta(top, base_address)));\n-      base_address = new_base;\n-    }\n-    top = new_top;\n-    index = index - 1;\n-  }\n-\n-  assert(top != base_address, \"zero-sized range, address \" PTR_FORMAT, p2i(base_address));\n-  ranges->append(MemRegion(base_address, pointer_delta(top, base_address)));\n-  _allocated_regions.clear();\n-  _allocation_region = NULL;\n-};\n","filename":"src\/hotspot\/share\/gc\/g1\/g1Allocator.cpp","additions":1,"deletions":151,"binary":false,"changes":152,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2014, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -228,56 +228,0 @@\n-\/\/ G1ArchiveAllocator is used to allocate memory in archive\n-\/\/ regions. Such regions are not scavenged nor compacted by GC.\n-\/\/ There are two types of archive regions, which are\n-\/\/ differ in the kind of references allowed for the contained objects:\n-\/\/\n-\/\/ - 'Closed' archive region contain no references outside of other\n-\/\/   closed archive regions. The region is immutable by GC. GC does\n-\/\/   not mark object header in 'closed' archive region.\n-\/\/ - An 'open' archive region allow references to any other regions,\n-\/\/   including closed archive, open archive and other java heap regions.\n-\/\/   GC can adjust pointers and mark object header in 'open' archive region.\n-class G1ArchiveAllocator : public CHeapObj<mtGC> {\n-protected:\n-  bool _open; \/\/ Indicate if the region is 'open' archive.\n-  G1CollectedHeap* _g1h;\n-\n-  \/\/ The current allocation region\n-  HeapRegion* _allocation_region;\n-\n-  \/\/ Regions allocated for the current archive range.\n-  GrowableArrayCHeap<HeapRegion*, mtGC> _allocated_regions;\n-\n-  \/\/ Current allocation window within the current region.\n-  HeapWord* _bottom;\n-  HeapWord* _top;\n-  HeapWord* _max;\n-\n-  \/\/ Allocate a new region for this archive allocator.\n-  \/\/ Allocation is from the top of the reserved heap downward.\n-  bool alloc_new_region();\n-\n-public:\n-  G1ArchiveAllocator(G1CollectedHeap* g1h, bool open) :\n-    _open(open),\n-    _g1h(g1h),\n-    _allocation_region(NULL),\n-    _allocated_regions(2),\n-    _bottom(NULL),\n-    _top(NULL),\n-    _max(NULL) { }\n-\n-  virtual ~G1ArchiveAllocator() {\n-    assert(_allocation_region == NULL, \"_allocation_region not NULL\");\n-  }\n-\n-  static G1ArchiveAllocator* create_allocator(G1CollectedHeap* g1h, bool open);\n-\n-  \/\/ Allocate memory for an individual object.\n-  HeapWord* archive_mem_allocate(size_t word_size);\n-\n-  \/\/ Return the memory ranges used in the current archive, after\n-  \/\/ aligning to the requested alignment.\n-  void complete_archive(GrowableArray<MemRegion>* ranges,\n-                        size_t end_alignment_in_bytes);\n-};\n-\n","filename":"src\/hotspot\/share\/gc\/g1\/g1Allocator.hpp","additions":1,"deletions":57,"binary":false,"changes":58,"status":"modified"},{"patch":"@@ -493,34 +493,0 @@\n-void G1CollectedHeap::begin_archive_alloc_range(bool open) {\n-  assert_at_safepoint_on_vm_thread();\n-  assert(_archive_allocator == nullptr, \"should not be initialized\");\n-  _archive_allocator = G1ArchiveAllocator::create_allocator(this, open);\n-}\n-\n-bool G1CollectedHeap::is_archive_alloc_too_large(size_t word_size) {\n-  \/\/ Allocations in archive regions cannot be of a size that would be considered\n-  \/\/ humongous even for a minimum-sized region, because G1 region sizes\/boundaries\n-  \/\/ may be different at archive-restore time.\n-  return word_size >= humongous_threshold_for(HeapRegion::min_region_size_in_words());\n-}\n-\n-HeapWord* G1CollectedHeap::archive_mem_allocate(size_t word_size) {\n-  assert_at_safepoint_on_vm_thread();\n-  assert(_archive_allocator != nullptr, \"_archive_allocator not initialized\");\n-  if (is_archive_alloc_too_large(word_size)) {\n-    return nullptr;\n-  }\n-  return _archive_allocator->archive_mem_allocate(word_size);\n-}\n-\n-void G1CollectedHeap::end_archive_alloc_range(GrowableArray<MemRegion>* ranges,\n-                                              size_t end_alignment_in_bytes) {\n-  assert_at_safepoint_on_vm_thread();\n-  assert(_archive_allocator != nullptr, \"_archive_allocator not initialized\");\n-\n-  \/\/ Call complete_archive to do the real work, filling in the MemRegion\n-  \/\/ array with the archive regions.\n-  _archive_allocator->complete_archive(ranges, end_alignment_in_bytes);\n-  delete _archive_allocator;\n-  _archive_allocator = nullptr;\n-}\n-\n@@ -1426,1 +1392,0 @@\n-  _archive_allocator(nullptr),\n@@ -1829,1 +1794,0 @@\n-  assert(_archive_allocator == nullptr, \"must be, should not contribute to used\");\n@@ -3110,1 +3074,0 @@\n-    assert(_archive_allocator == nullptr, \"must be, should not contribute to used\");\n@@ -3309,2 +3272,0 @@\n-\n-    assert(_archive_allocator == nullptr, \"must be, should not contribute to used\");\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectedHeap.cpp","additions":0,"deletions":39,"binary":false,"changes":39,"status":"modified"},{"patch":"@@ -69,1 +69,0 @@\n-class G1ArchiveAllocator;\n@@ -221,3 +220,0 @@\n-  \/\/ Class that handles archive allocation ranges.\n-  G1ArchiveAllocator* _archive_allocator;\n-\n@@ -685,20 +681,0 @@\n-  \/\/ Facility for allocating in 'archive' regions in high heap memory and\n-  \/\/ recording the allocated ranges. These should all be called from the\n-  \/\/ VM thread at safepoints, without the heap lock held. They can be used\n-  \/\/ to create and archive a set of heap regions which can be mapped at the\n-  \/\/ same fixed addresses in a subsequent JVM invocation.\n-  void begin_archive_alloc_range(bool open = false);\n-\n-  \/\/ Check if the requested size would be too large for an archive allocation.\n-  bool is_archive_alloc_too_large(size_t word_size);\n-\n-  \/\/ Allocate memory of the requested size from the archive region. This will\n-  \/\/ return NULL if the size is too large or if no memory is available. It\n-  \/\/ does not trigger a garbage collection.\n-  HeapWord* archive_mem_allocate(size_t word_size);\n-\n-  \/\/ Optionally aligns the end address and returns the allocated ranges in\n-  \/\/ an array of MemRegions in order of ascending addresses.\n-  void end_archive_alloc_range(GrowableArray<MemRegion>* ranges,\n-                               size_t end_alignment_in_bytes = 0);\n-\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectedHeap.hpp","additions":0,"deletions":24,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -313,22 +313,0 @@\n-\/\/ We want all used regions to be moved to the bottom-end of the heap, so we have\n-\/\/ a contiguous range of free regions at the top end of the heap. This way, we can\n-\/\/ avoid fragmentation while allocating the archive regions.\n-\/\/\n-\/\/ Before calling this, a full GC should have been executed with a single worker thread,\n-\/\/ so that no old regions would be moved to the middle of the heap.\n-void G1HeapVerifier::verify_ready_for_archiving() {\n-  VerifyReadyForArchivingRegionClosure cl;\n-  G1CollectedHeap::heap()->heap_region_iterate(&cl);\n-  if (cl.has_holes()) {\n-    log_warning(gc, verify)(\"All free regions should be at the top end of the heap, but\"\n-                            \" we found holes. This is probably caused by (unmovable) humongous\"\n-                            \" allocations or active GCLocker, and may lead to fragmentation while\"\n-                            \" writing archive heap memory regions.\");\n-  }\n-  if (cl.has_humongous()) {\n-    log_warning(gc, verify)(\"(Unmovable) humongous regions have been found and\"\n-                            \" may lead to fragmentation while\"\n-                            \" writing archive heap memory regions.\");\n-  }\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/g1\/g1HeapVerifier.cpp","additions":1,"deletions":23,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -84,1 +84,0 @@\n-  static void verify_ready_for_archiving();\n","filename":"src\/hotspot\/share\/gc\/g1\/g1HeapVerifier.hpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -301,1 +301,0 @@\n-    case GCCause::_archive_time_gc:\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -63,3 +63,0 @@\n-    case _archive_time_gc:\n-      return \"Full GC for -Xshare:dump\";\n-\n","filename":"src\/hotspot\/share\/gc\/shared\/gcCause.cpp","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -56,1 +56,0 @@\n-    _archive_time_gc,\n","filename":"src\/hotspot\/share\/gc\/shared\/gcCause.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -26,0 +26,1 @@\n+#include \"cds\/archiveHeapWriter.hpp\"\n@@ -297,2 +298,1 @@\n-          typeArrayOop value = java_lang_String::value_no_keepalive(obj);\n-          if (!HeapShared::is_too_large_to_archive(value)) {\n+          if (!ArchiveHeapWriter::is_string_too_large_to_archive(obj)) {\n@@ -314,1 +314,2 @@\n-      if (java_lang_String::is_instance(p)) {\n+      if (java_lang_String::is_instance(p) &&\n+          !ArchiveHeapWriter::is_string_too_large_to_archive(p)) {\n","filename":"src\/hotspot\/share\/oops\/constantPool.cpp","additions":4,"deletions":3,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -38,0 +38,1 @@\n+  friend class ArchiveHeapWriter;\n","filename":"src\/hotspot\/share\/oops\/objArrayOop.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n-# Copyright (c) 2013, 2022, Oracle and\/or its affiliates. All rights reserved.\n+# Copyright (c) 2013, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -421,1 +421,0 @@\n- -runtime\/cds\/appcds\/javaldr\/HumongousDuringDump.java \\\n","filename":"test\/hotspot\/jtreg\/TEST.groups","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -152,2 +152,1 @@\n-            \"Cannot archive the sub-graph referenced from [Ljava.lang.Integer; object\",\n-            \"humongous regions have been found and may lead to fragmentation\");\n+            \"Cannot archive the sub-graph referenced from [Ljava.lang.Integer; object\");\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/appcds\/cacheObject\/ArchivedIntegerCacheTest.java","additions":2,"deletions":3,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -1,86 +0,0 @@\n-\/*\n- * Copyright (c) 2018, 2021, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-\/*\n- * @test\n- * @summary Test how CDS dumping handles the existence of humongous G1 regions.\n- * @library \/test\/lib \/test\/hotspot\/jtreg\/runtime\/cds\/appcds \/test\/hotspot\/jtreg\/runtime\/cds\/appcds\/test-classes\n- * @requires vm.cds.write.archived.java.heap\n- * @requires vm.jvmti\n- * @run driver\/timeout=240 HumongousDuringDump\n- *\/\n-\n-import jdk.test.lib.cds.CDSOptions;\n-import jdk.test.lib.process.OutputAnalyzer;\n-import jdk.test.lib.process.ProcessTools;\n-import jdk.test.lib.helpers.ClassFileInstaller;\n-\n-public class HumongousDuringDump {\n-    public static String appClasses[] = {\n-        Hello.class.getName(),\n-    };\n-    public static String agentClasses[] = {\n-        HumongousDuringDumpTransformer.class.getName(),\n-    };\n-\n-    public static void main(String[] args) throws Throwable {\n-        String agentJar =\n-            ClassFileInstaller.writeJar(\"HumongousDuringDumpTransformer.jar\",\n-                                        ClassFileInstaller.Manifest.fromSourceFile(\"HumongousDuringDumpTransformer.mf\"),\n-                                        agentClasses);\n-\n-        String appJar =\n-            ClassFileInstaller.writeJar(\"HumongousDuringDumpApp.jar\", appClasses);\n-\n-        String gcLog = Boolean.getBoolean(\"test.cds.verbose.gc\") ?\n-            \"-Xlog:gc*=info,gc+region=trace,gc+alloc+region=debug\" : \"-showversion\";\n-\n-        String extraArg = \"-javaagent:\" + agentJar;\n-        String extraOption = \"-XX:+AllowArchivingWithJavaAgent\";\n-\n-        OutputAnalyzer out =\n-          TestCommon.testDump(appJar, TestCommon.list(Hello.class.getName()),\n-                              \"-XX:+UnlockDiagnosticVMOptions\", extraOption,\n-                              \"-Xlog:gc+region+cds\",\n-                              \"-Xlog:gc+region=trace\",\n-                              extraArg, \"-Xmx64m\", gcLog);\n-        out.shouldContain(\"(Unmovable) humongous regions have been found and may lead to fragmentation\");\n-        out.shouldContain(\"All free regions should be at the top end of the heap, but we found holes.\");\n-        out.shouldMatch(\"gc,region,cds. HeapRegion .* HUM. hole\");\n-        String pattern = \"gc,region,cds. HeapRegion .*hole\";\n-        out.shouldMatch(pattern);\n-        out.shouldNotMatch(pattern + \".*unexpected\");\n-\n-        TestCommon.run(\n-                \"-cp\", appJar,\n-                \"-verbose\",\n-                \"-Xmx64m\",\n-                \"-Xlog:cds=info\",\n-                \"-XX:+UnlockDiagnosticVMOptions\", extraOption,\n-                gcLog,\n-                Hello.class.getName())\n-              .assertNormalExit();\n-    }\n-}\n-\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/appcds\/javaldr\/HumongousDuringDump.java","additions":0,"deletions":86,"binary":false,"changes":86,"status":"deleted"},{"patch":"@@ -1,112 +0,0 @@\n-\/*\n- * Copyright (c) 2018, 2019, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-import java.lang.instrument.ClassFileTransformer;\n-import java.lang.instrument.Instrumentation;\n-import java.lang.instrument.IllegalClassFormatException;\n-import java.security.ProtectionDomain;\n-\n-\/\/ This test is sensitive to -Xmx. It must be run with -xmx64m.\n-\/\/ Running with a different -Xmx requires changing the parameters and careful re-testing.\n-public class HumongousDuringDumpTransformer implements ClassFileTransformer {\n-    public byte[] transform(ClassLoader loader, String name, Class<?> classBeingRedefined,\n-                            ProtectionDomain pd, byte[] buffer) throws IllegalClassFormatException {\n-        if (name.equals(\"Hello\")) {\n-            try {\n-                makeHumongousRegions();\n-            } catch (Throwable t) {\n-                array = null;\n-                humon = null;\n-                System.out.println(\"Unexpected error: \" + t);\n-                t.printStackTrace();\n-            }\n-        }\n-        array = null;\n-        return null;\n-    }\n-\n-    private static Instrumentation savedInstrumentation;\n-\n-    public static void premain(String agentArguments, Instrumentation instrumentation) {\n-        long xmx = Runtime.getRuntime().maxMemory();\n-        if (xmx < 60 * 1024 * 1024 || xmx > 80 * 1024 * 1024) {\n-            System.out.println(\"Running with incorrect heap size: \" + xmx);\n-            System.exit(1);\n-        }\n-\n-        System.out.println(\"ClassFileTransformer.premain() is called\");\n-        instrumentation.addTransformer(new HumongousDuringDumpTransformer(), \/*canRetransform=*\/true);\n-        savedInstrumentation = instrumentation;\n-    }\n-\n-    public static Instrumentation getInstrumentation() {\n-        return savedInstrumentation;\n-    }\n-\n-    public static void agentmain(String args, Instrumentation inst) throws Exception {\n-        premain(args, inst);\n-    }\n-\n-    Object[] array;\n-\n-    static final int DUMMY_SIZE = 4096 - 16 - 8;\n-    static final int HUMON_SIZE = 4 * 1024 * 1024 - 16 - 8;\n-    static final int SKIP = 13;\n-\n-    byte humon[] = null;\n-    boolean first = true;\n-\n-    public synchronized void makeHumongousRegions() {\n-        if (!first) {\n-            return;\n-        }\n-        System.out.println(\"===============================================================================\");\n-        first = false;\n-\n-        int total = 0;\n-        array = new Object[100000];\n-        System.out.println(array);\n-\n-        \/\/ (1) Allocate about 8MB of old objects.\n-        for (int n=0, i=0; total < 8 * 1024 * 1024; n++) {\n-            \/\/ Make enough allocations to cause a GC (for 64MB heap) to create\n-            \/\/ old regions.\n-            \/\/\n-            \/\/ But don't completely fill up the heap. That would cause OOM and\n-            \/\/ may not be handled gracefully inside class transformation!\n-            Object x = new byte[DUMMY_SIZE];\n-            if ((n  % SKIP) == 0) {\n-                array[i++] = x;\n-                total += DUMMY_SIZE;\n-            }\n-        }\n-\n-        System.gc();\n-\n-        \/\/ (2) Now allocate a humongous array. It will sit above the 8MB of old regions.\n-        humon = new byte[HUMON_SIZE];\n-        array = null;\n-        System.gc();\n-    }\n-}\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/appcds\/javaldr\/HumongousDuringDumpTransformer.java","additions":0,"deletions":112,"binary":false,"changes":112,"status":"deleted"},{"patch":"@@ -1,5 +0,0 @@\n-Manifest-Version: 1.0\n-Premain-Class: HumongousDuringDumpTransformer\n-Agent-Class: HumongousDuringDumpTransformer\n-Can-Retransform-Classes: true\n-Can-Redefine-Classes: true\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/appcds\/javaldr\/HumongousDuringDumpTransformer.mf","additions":0,"deletions":5,"binary":false,"changes":5,"status":"deleted"}]}