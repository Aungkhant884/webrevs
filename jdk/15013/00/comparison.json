{"files":[{"patch":"@@ -632,1 +632,1 @@\n-    if (u->is_Phi() && u->bottom_type() == Type::MEMORY) {\n+    if (u->is_memory_phi()) {\n@@ -2680,1 +2680,1 @@\n-          if (uu->is_Phi() && uu->bottom_type() == Type::MEMORY) {\n+          if (uu->is_memory_phi()) {\n@@ -5723,0 +5723,20 @@\n+\/\/ Find pre loop end from main loop. Returns nullptr if none.\n+CountedLoopEndNode* CountedLoopNode::find_pre_loop_end() {\n+  assert(is_main_loop(), \"Can only find pre-loop from main-loop\");\n+  \/\/ The loop cannot be optimized if the graph shape at the loop entry is\n+  \/\/ inappropriate.\n+  if (is_canonical_loop_entry() == nullptr) {\n+    return nullptr;\n+  }\n+  Node* p_f = skip_predicates()->in(0)->in(0);\n+  if (!p_f->is_IfFalse() || !p_f->in(0)->is_CountedLoopEnd()) {\n+    return nullptr;\n+  }\n+  CountedLoopEndNode* pre_end = p_f->in(0)->as_CountedLoopEnd();\n+  CountedLoopNode* loop_node = pre_end->loopnode();\n+  if (loop_node == nullptr || !loop_node->is_pre_loop()) {\n+    return nullptr;\n+  }\n+  return pre_end;\n+}\n+\n","filename":"src\/hotspot\/share\/opto\/loopnode.cpp","additions":22,"deletions":2,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -336,0 +336,1 @@\n+  CountedLoopEndNode* find_pre_loop_end();\n","filename":"src\/hotspot\/share\/opto\/loopnode.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -1216,0 +1216,3 @@\n+  \/\/ Whether this is a memory phi node\n+  bool is_memory_phi() const { return is_Phi() && bottom_type() == Type::MEMORY; }\n+\n","filename":"src\/hotspot\/share\/opto\/node.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -25,1 +25,0 @@\n-#include \"compiler\/compileLog.hpp\"\n@@ -30,1 +29,0 @@\n-#include \"opto\/callnode.hpp\"\n@@ -33,1 +31,0 @@\n-#include \"opto\/divnode.hpp\"\n@@ -36,1 +33,0 @@\n-#include \"opto\/mulnode.hpp\"\n@@ -39,1 +35,0 @@\n-#include \"opto\/rootnode.hpp\"\n@@ -156,1 +151,1 @@\n-    CountedLoopEndNode* pre_end = find_pre_loop_end(cl);\n+    CountedLoopEndNode* pre_end = cl->find_pre_loop_end();\n@@ -219,1 +214,1 @@\n-    if (n->is_Phi() && (n->bottom_type() == Type::MEMORY)) {\n+    if (n->is_memory_phi()) {\n@@ -267,1 +262,1 @@\n-          SWPointer p1(current, this, &nstack, true);\n+          VPointer p1(current, phase(), lpt(), &nstack, true);\n@@ -628,1 +623,1 @@\n-    SWPointer align_to_ref_p(mem_ref, this, nullptr, false);\n+    VPointer align_to_ref_p(mem_ref, phase(), lpt(), nullptr, false);\n@@ -634,1 +629,1 @@\n-        SWPointer p2(s, this, nullptr, false);\n+        VPointer p2(s, phase(), lpt(), nullptr, false);\n@@ -756,1 +751,1 @@\n-bool SuperWord::mem_ref_has_no_alignment_violation(MemNode* mem_ref, int iv_adjustment, SWPointer &align_to_ref_p,\n+bool SuperWord::mem_ref_has_no_alignment_violation(MemNode* mem_ref, int iv_adjustment, VPointer& align_to_ref_p,\n@@ -787,1 +782,1 @@\n-  SWPointer p2(best_align_to_mem_ref, this, nullptr, false);\n+  VPointer p2(best_align_to_mem_ref, phase(), lpt(), nullptr, false);\n@@ -806,1 +801,1 @@\n-    SWPointer p1(s1, this, nullptr, false);\n+    VPointer p1(s1, phase(), lpt(), nullptr, false);\n@@ -816,1 +811,1 @@\n-        SWPointer p2(s2, this, nullptr, false);\n+        VPointer p2(s2, phase(), lpt(), nullptr, false);\n@@ -837,1 +832,1 @@\n-      SWPointer p(s, this, nullptr, false);\n+      VPointer p(s, phase(), lpt(), nullptr, false);\n@@ -860,1 +855,1 @@\n-        SWPointer p(s, this, nullptr, false);\n+        VPointer p(s, phase(), lpt(), nullptr, false);\n@@ -924,1 +919,1 @@\n-bool SuperWord::ref_is_alignable(SWPointer& p) {\n+bool SuperWord::ref_is_alignable(VPointer& p) {\n@@ -1022,1 +1017,1 @@\n-  SWPointer align_to_ref_p(mem_ref, this, nullptr, false);\n+  VPointer align_to_ref_p(mem_ref, phase(), lpt(), nullptr, false);\n@@ -1066,1 +1061,1 @@\n-    if (n->is_Mem() || (n->is_Phi() && n->bottom_type() == Type::MEMORY)) {\n+    if (n->is_Mem() || n->is_memory_phi()) {\n@@ -1103,1 +1098,1 @@\n-      SWPointer p1(s1->as_Mem(), this, nullptr, false);\n+      VPointer p1(s1->as_Mem(), phase(), lpt(), nullptr, false);\n@@ -1109,1 +1104,1 @@\n-        SWPointer p2(s2->as_Mem(), this, nullptr, false);\n+        VPointer p2(s2->as_Mem(), phase(), lpt(), nullptr, false);\n@@ -1118,1 +1113,1 @@\n-        if (!SWPointer::not_equal(cmp)) {\n+        if (!VPointer::not_equal(cmp)) {\n@@ -1174,1 +1169,1 @@\n-        } else if (out->is_Phi() && out->bottom_type() == Type::MEMORY && !in_bb(out)) {\n+        } else if (out->is_memory_phi() && !in_bb(out)) {\n@@ -1260,2 +1255,2 @@\n-  SWPointer p1(s1->as_Mem(), this, nullptr, false);\n-  SWPointer p2(s2->as_Mem(), this, nullptr, false);\n+  VPointer p1(s1->as_Mem(), phase(), lpt(), nullptr, false);\n+  VPointer p2(s2->as_Mem(), phase(), lpt(), nullptr, false);\n@@ -2534,1 +2529,1 @@\n-  NOT_PRODUCT(if(is_trace_loop_reverse()) {tty->print_cr(\"SWPointer::output: print loop before create_reserve_version_of_loop\"); print_loop(true);})\n+  NOT_PRODUCT(if(is_trace_loop_reverse()) {tty->print_cr(\"VPointer::output: print loop before create_reserve_version_of_loop\"); print_loop(true);})\n@@ -2538,1 +2533,1 @@\n-  NOT_PRODUCT(if(is_trace_loop_reverse()) {tty->print_cr(\"SWPointer::output: print loop after create_reserve_version_of_loop\"); print_loop(true);})\n+  NOT_PRODUCT(if(is_trace_loop_reverse()) {tty->print_cr(\"VPointer::output: print loop after create_reserve_version_of_loop\"); print_loop(true);})\n@@ -2541,1 +2536,1 @@\n-    NOT_PRODUCT(if(is_trace_loop_reverse() || TraceLoopOpts) {tty->print_cr(\"SWPointer::output: loop was not reserved correctly, exiting SuperWord\");})\n+    NOT_PRODUCT(if(is_trace_loop_reverse() || TraceLoopOpts) {tty->print_cr(\"VPointer::output: loop was not reserved correctly, exiting SuperWord\");})\n@@ -2556,1 +2551,1 @@\n-      NOT_PRODUCT(if(is_trace_cmov()) {tty->print_cr(\"SWPointer::output: %d executed first, %d executed last in pack\", first->_idx, n->_idx); print_pack(p);})\n+      NOT_PRODUCT(if(is_trace_cmov()) {tty->print_cr(\"VPointer::output: %d executed first, %d executed last in pack\", first->_idx, n->_idx); print_pack(p);})\n@@ -2561,1 +2556,1 @@\n-        SWPointer p1(n->as_Mem(), this, nullptr, false);\n+        VPointer p1(n->as_Mem(), phase(), lpt(), nullptr, false);\n@@ -2567,1 +2562,1 @@\n-          SWPointer p2(mem->as_Mem(), this, nullptr, false);\n+          VPointer p2(mem->as_Mem(), phase(), lpt(), nullptr, false);\n@@ -2569,1 +2564,1 @@\n-          if (SWPointer::not_equal(cmp) || !SWPointer::comparable(cmp)) {\n+          if (VPointer::not_equal(cmp) || !VPointer::comparable(cmp)) {\n@@ -2584,1 +2579,1 @@\n-            NOT_PRODUCT(if(is_trace_loop_reverse() || TraceLoopOpts) {tty->print_cr(\"SWPointer::output: val should not be null, exiting SuperWord\");})\n+            NOT_PRODUCT(if(is_trace_loop_reverse() || TraceLoopOpts) {tty->print_cr(\"VPointer::output: val should not be null, exiting SuperWord\");})\n@@ -2718,1 +2713,1 @@\n-              NOT_PRODUCT(if(is_trace_loop_reverse() || TraceLoopOpts) {tty->print_cr(\"SWPointer::output: in1 should not be null, exiting SuperWord\");})\n+              NOT_PRODUCT(if(is_trace_loop_reverse() || TraceLoopOpts) {tty->print_cr(\"VPointer::output: in1 should not be null, exiting SuperWord\");})\n@@ -2728,1 +2723,1 @@\n-            NOT_PRODUCT(if(is_trace_loop_reverse() || TraceLoopOpts) {tty->print_cr(\"SWPointer::output: in2 should not be null, exiting SuperWord\");})\n+            NOT_PRODUCT(if(is_trace_loop_reverse() || TraceLoopOpts) {tty->print_cr(\"VPointer::output: in2 should not be null, exiting SuperWord\");})\n@@ -2802,1 +2797,1 @@\n-          NOT_PRODUCT(if(is_trace_loop_reverse() || TraceLoopOpts) {tty->print_cr(\"SWPointer::output: Unhandled scalar opcode (%s), ShouldNotReachHere, exiting SuperWord\", NodeClassNames[opc]);})\n+          NOT_PRODUCT(if(is_trace_loop_reverse() || TraceLoopOpts) {tty->print_cr(\"VPointer::output: Unhandled scalar opcode (%s), ShouldNotReachHere, exiting SuperWord\", NodeClassNames[opc]);})\n@@ -2812,1 +2807,1 @@\n-          NOT_PRODUCT(if(is_trace_loop_reverse() || TraceLoopOpts) {tty->print_cr(\"SWPointer::output: got null node, cannot proceed, exiting SuperWord\");})\n+          NOT_PRODUCT(if(is_trace_loop_reverse() || TraceLoopOpts) {tty->print_cr(\"VPointer::output: got null node, cannot proceed, exiting SuperWord\");})\n@@ -3172,1 +3167,1 @@\n-    if (in_bb(n) && (n->is_Phi() && n->bottom_type() == Type::MEMORY)) {\n+    if (in_bb(n) && n->is_memory_phi()) {\n@@ -3483,2 +3478,1 @@\n-  NOT_PRODUCT(SWPointer::Tracer::Depth ddd(0);)\n-  SWPointer p(s, this, nullptr, false);\n+  VPointer p(s, phase(), lpt(), nullptr, false);\n@@ -3486,1 +3480,1 @@\n-    NOT_PRODUCT(if(is_trace_alignment()) tty->print_cr(\"SWPointer::memory_alignment: SWPointer p invalid, return bottom_align\");)\n+    NOT_PRODUCT(if(is_trace_alignment()) tty->print_cr(\"VPointer::memory_alignment: VPointer p invalid, return bottom_align\");)\n@@ -3491,1 +3485,1 @@\n-    NOT_PRODUCT(if(is_trace_alignment()) tty->print_cr(\"SWPointer::memory_alignment: vector_width_in_bytes < 2, return bottom_align\");)\n+    NOT_PRODUCT(if(is_trace_alignment()) tty->print_cr(\"VPointer::memory_alignment: vector_width_in_bytes < 2, return bottom_align\");)\n@@ -3500,1 +3494,1 @@\n-    tty->print_cr(\"SWPointer::memory_alignment: off_rem = %d, off_mod = %d (offset = %d)\", off_rem, off_mod, offset);\n+    tty->print_cr(\"VPointer::memory_alignment: off_rem = %d, off_mod = %d (offset = %d)\", off_rem, off_mod, offset);\n@@ -3633,1 +3627,1 @@\n-  SWPointer align_to_ref_p(align_to_ref, this, nullptr, false);\n+  VPointer align_to_ref_p(align_to_ref, phase(), lpt(), nullptr, false);\n@@ -3775,18 +3769,0 @@\n-\/\/----------------------------get_pre_loop_end---------------------------\n-\/\/ Find pre loop end from main loop.  Returns null if none.\n-CountedLoopEndNode* SuperWord::find_pre_loop_end(CountedLoopNode* cl) const {\n-  \/\/ The loop cannot be optimized if the graph shape at\n-  \/\/ the loop entry is inappropriate.\n-  if (cl->is_canonical_loop_entry() == nullptr) {\n-    return nullptr;\n-  }\n-\n-  Node* p_f = cl->skip_predicates()->in(0)->in(0);\n-  if (!p_f->is_IfFalse()) return nullptr;\n-  if (!p_f->in(0)->is_CountedLoopEnd()) return nullptr;\n-  CountedLoopEndNode* pre_end = p_f->in(0)->as_CountedLoopEnd();\n-  CountedLoopNode* loop_node = pre_end->loopnode();\n-  if (loop_node == nullptr || !loop_node->is_pre_loop()) return nullptr;\n-  return pre_end;\n-}\n-\n@@ -3855,667 +3831,0 @@\n-\/\/==============================SWPointer===========================\n-#ifndef PRODUCT\n-int SWPointer::Tracer::_depth = 0;\n-#endif\n-\/\/----------------------------SWPointer------------------------\n-SWPointer::SWPointer(MemNode* mem, SuperWord* slp, Node_Stack *nstack, bool analyze_only) :\n-  _mem(mem), _slp(slp), _base(nullptr), _adr(nullptr),\n-  _scale(0), _offset(0), _invar(nullptr),\n-#ifdef ASSERT\n-  _debug_invar(nullptr), _debug_negate_invar(false), _debug_invar_scale(nullptr),\n-#endif\n-  _nstack(nstack), _analyze_only(analyze_only),\n-  _stack_idx(0)\n-#ifndef PRODUCT\n-  , _tracer(slp)\n-#endif\n-{\n-  NOT_PRODUCT(_tracer.ctor_1(mem);)\n-\n-  Node* adr = mem->in(MemNode::Address);\n-  if (!adr->is_AddP()) {\n-    assert(!valid(), \"too complex\");\n-    return;\n-  }\n-  \/\/ Match AddP(base, AddP(ptr, k*iv [+ invariant]), constant)\n-  Node* base = adr->in(AddPNode::Base);\n-  \/\/ The base address should be loop invariant\n-  if (is_loop_member(base)) {\n-    assert(!valid(), \"base address is loop variant\");\n-    return;\n-  }\n-  \/\/ unsafe references require misaligned vector access support\n-  if (base->is_top() && !Matcher::misaligned_vectors_ok()) {\n-    assert(!valid(), \"unsafe access\");\n-    return;\n-  }\n-\n-  NOT_PRODUCT(if(_slp->is_trace_alignment()) _tracer.store_depth();)\n-  NOT_PRODUCT(_tracer.ctor_2(adr);)\n-\n-  int i;\n-  for (i = 0; ; i++) {\n-    NOT_PRODUCT(_tracer.ctor_3(adr, i);)\n-\n-    if (!scaled_iv_plus_offset(adr->in(AddPNode::Offset))) {\n-      assert(!valid(), \"too complex\");\n-      return;\n-    }\n-    adr = adr->in(AddPNode::Address);\n-    NOT_PRODUCT(_tracer.ctor_4(adr, i);)\n-\n-    if (base == adr || !adr->is_AddP()) {\n-      NOT_PRODUCT(_tracer.ctor_5(adr, base, i);)\n-      break; \/\/ stop looking at addp's\n-    }\n-  }\n-  if (is_loop_member(adr)) {\n-    assert(!valid(), \"adr is loop variant\");\n-    return;\n-  }\n-\n-  if (!base->is_top() && adr != base) {\n-    assert(!valid(), \"adr and base differ\");\n-    return;\n-  }\n-\n-  NOT_PRODUCT(if(_slp->is_trace_alignment()) _tracer.restore_depth();)\n-  NOT_PRODUCT(_tracer.ctor_6(mem);)\n-\n-  _base = base;\n-  _adr  = adr;\n-  assert(valid(), \"Usable\");\n-}\n-\n-\/\/ Following is used to create a temporary object during\n-\/\/ the pattern match of an address expression.\n-SWPointer::SWPointer(SWPointer* p) :\n-  _mem(p->_mem), _slp(p->_slp), _base(nullptr), _adr(nullptr),\n-  _scale(0), _offset(0), _invar(nullptr),\n-#ifdef ASSERT\n-  _debug_invar(nullptr), _debug_negate_invar(false), _debug_invar_scale(nullptr),\n-#endif\n-  _nstack(p->_nstack), _analyze_only(p->_analyze_only),\n-  _stack_idx(p->_stack_idx)\n-  #ifndef PRODUCT\n-  , _tracer(p->_slp)\n-  #endif\n-{}\n-\n-bool SWPointer::is_loop_member(Node* n) const {\n-  Node* n_c = phase()->get_ctrl(n);\n-  return lpt()->is_member(phase()->get_loop(n_c));\n-}\n-\n-bool SWPointer::invariant(Node* n) const {\n-  NOT_PRODUCT(Tracer::Depth dd;)\n-  Node* n_c = phase()->get_ctrl(n);\n-  NOT_PRODUCT(_tracer.invariant_1(n, n_c);)\n-  bool is_not_member = !is_loop_member(n);\n-  if (is_not_member && _slp->lp()->is_main_loop()) {\n-    \/\/ Check that n_c dominates the pre loop head node. If it does not, then we cannot use n as invariant for the pre loop\n-    \/\/ CountedLoopEndNode check because n_c is either part of the pre loop or between the pre and the main loop (illegal\n-    \/\/ invariant: Happens, for example, when n_c is a CastII node that prevents data nodes to flow above the main loop).\n-    return phase()->is_dominator(n_c, _slp->pre_loop_head());\n-  }\n-  return is_not_member;\n-}\n-\n-\/\/------------------------scaled_iv_plus_offset--------------------\n-\/\/ Match: k*iv + offset\n-\/\/ where: k is a constant that maybe zero, and\n-\/\/        offset is (k2 [+\/- invariant]) where k2 maybe zero and invariant is optional\n-bool SWPointer::scaled_iv_plus_offset(Node* n) {\n-  NOT_PRODUCT(Tracer::Depth ddd;)\n-  NOT_PRODUCT(_tracer.scaled_iv_plus_offset_1(n);)\n-\n-  if (scaled_iv(n)) {\n-    NOT_PRODUCT(_tracer.scaled_iv_plus_offset_2(n);)\n-    return true;\n-  }\n-\n-  if (offset_plus_k(n)) {\n-    NOT_PRODUCT(_tracer.scaled_iv_plus_offset_3(n);)\n-    return true;\n-  }\n-\n-  int opc = n->Opcode();\n-  if (opc == Op_AddI) {\n-    if (offset_plus_k(n->in(2)) && scaled_iv_plus_offset(n->in(1))) {\n-      NOT_PRODUCT(_tracer.scaled_iv_plus_offset_4(n);)\n-      return true;\n-    }\n-    if (offset_plus_k(n->in(1)) && scaled_iv_plus_offset(n->in(2))) {\n-      NOT_PRODUCT(_tracer.scaled_iv_plus_offset_5(n);)\n-      return true;\n-    }\n-  } else if (opc == Op_SubI || opc == Op_SubL) {\n-    if (offset_plus_k(n->in(2), true) && scaled_iv_plus_offset(n->in(1))) {\n-      NOT_PRODUCT(_tracer.scaled_iv_plus_offset_6(n);)\n-      return true;\n-    }\n-    if (offset_plus_k(n->in(1)) && scaled_iv_plus_offset(n->in(2))) {\n-      _scale *= -1;\n-      NOT_PRODUCT(_tracer.scaled_iv_plus_offset_7(n);)\n-      return true;\n-    }\n-  }\n-\n-  NOT_PRODUCT(_tracer.scaled_iv_plus_offset_8(n);)\n-  return false;\n-}\n-\n-\/\/----------------------------scaled_iv------------------------\n-\/\/ Match: k*iv where k is a constant that's not zero\n-bool SWPointer::scaled_iv(Node* n) {\n-  NOT_PRODUCT(Tracer::Depth ddd;)\n-  NOT_PRODUCT(_tracer.scaled_iv_1(n);)\n-\n-  if (_scale != 0) { \/\/ already found a scale\n-    NOT_PRODUCT(_tracer.scaled_iv_2(n, _scale);)\n-    return false;\n-  }\n-\n-  if (n == iv()) {\n-    _scale = 1;\n-    NOT_PRODUCT(_tracer.scaled_iv_3(n, _scale);)\n-    return true;\n-  }\n-  if (_analyze_only && (is_loop_member(n))) {\n-    _nstack->push(n, _stack_idx++);\n-  }\n-\n-  int opc = n->Opcode();\n-  if (opc == Op_MulI) {\n-    if (n->in(1) == iv() && n->in(2)->is_Con()) {\n-      _scale = n->in(2)->get_int();\n-      NOT_PRODUCT(_tracer.scaled_iv_4(n, _scale);)\n-      return true;\n-    } else if (n->in(2) == iv() && n->in(1)->is_Con()) {\n-      _scale = n->in(1)->get_int();\n-      NOT_PRODUCT(_tracer.scaled_iv_5(n, _scale);)\n-      return true;\n-    }\n-  } else if (opc == Op_LShiftI) {\n-    if (n->in(1) == iv() && n->in(2)->is_Con()) {\n-      _scale = 1 << n->in(2)->get_int();\n-      NOT_PRODUCT(_tracer.scaled_iv_6(n, _scale);)\n-      return true;\n-    }\n-  } else if (opc == Op_ConvI2L || opc == Op_CastII) {\n-    if (scaled_iv_plus_offset(n->in(1))) {\n-      NOT_PRODUCT(_tracer.scaled_iv_7(n);)\n-      return true;\n-    }\n-  } else if (opc == Op_LShiftL && n->in(2)->is_Con()) {\n-    if (!has_iv()) {\n-      \/\/ Need to preserve the current _offset value, so\n-      \/\/ create a temporary object for this expression subtree.\n-      \/\/ Hacky, so should re-engineer the address pattern match.\n-      NOT_PRODUCT(Tracer::Depth dddd;)\n-      SWPointer tmp(this);\n-      NOT_PRODUCT(_tracer.scaled_iv_8(n, &tmp);)\n-\n-      if (tmp.scaled_iv_plus_offset(n->in(1))) {\n-        int scale = n->in(2)->get_int();\n-        _scale   = tmp._scale  << scale;\n-        _offset += tmp._offset << scale;\n-        if (tmp._invar != nullptr) {\n-          BasicType bt = tmp._invar->bottom_type()->basic_type();\n-          assert(bt == T_INT || bt == T_LONG, \"\");\n-          maybe_add_to_invar(register_if_new(LShiftNode::make(tmp._invar, n->in(2), bt)), false);\n-#ifdef ASSERT\n-          _debug_invar_scale = n->in(2);\n-#endif\n-        }\n-        NOT_PRODUCT(_tracer.scaled_iv_9(n, _scale, _offset, _invar);)\n-        return true;\n-      }\n-    }\n-  }\n-  NOT_PRODUCT(_tracer.scaled_iv_10(n);)\n-  return false;\n-}\n-\n-\/\/----------------------------offset_plus_k------------------------\n-\/\/ Match: offset is (k [+\/- invariant])\n-\/\/ where k maybe zero and invariant is optional, but not both.\n-bool SWPointer::offset_plus_k(Node* n, bool negate) {\n-  NOT_PRODUCT(Tracer::Depth ddd;)\n-  NOT_PRODUCT(_tracer.offset_plus_k_1(n);)\n-\n-  int opc = n->Opcode();\n-  if (opc == Op_ConI) {\n-    _offset += negate ? -(n->get_int()) : n->get_int();\n-    NOT_PRODUCT(_tracer.offset_plus_k_2(n, _offset);)\n-    return true;\n-  } else if (opc == Op_ConL) {\n-    \/\/ Okay if value fits into an int\n-    const TypeLong* t = n->find_long_type();\n-    if (t->higher_equal(TypeLong::INT)) {\n-      jlong loff = n->get_long();\n-      jint  off  = (jint)loff;\n-      _offset += negate ? -off : loff;\n-      NOT_PRODUCT(_tracer.offset_plus_k_3(n, _offset);)\n-      return true;\n-    }\n-    NOT_PRODUCT(_tracer.offset_plus_k_4(n);)\n-    return false;\n-  }\n-  assert((_debug_invar == nullptr) == (_invar == nullptr), \"\");\n-\n-  if (_analyze_only && is_loop_member(n)) {\n-    _nstack->push(n, _stack_idx++);\n-  }\n-  if (opc == Op_AddI) {\n-    if (n->in(2)->is_Con() && invariant(n->in(1))) {\n-      maybe_add_to_invar(n->in(1), negate);\n-      _offset += negate ? -(n->in(2)->get_int()) : n->in(2)->get_int();\n-      NOT_PRODUCT(_tracer.offset_plus_k_6(n, _invar, negate, _offset);)\n-      return true;\n-    } else if (n->in(1)->is_Con() && invariant(n->in(2))) {\n-      _offset += negate ? -(n->in(1)->get_int()) : n->in(1)->get_int();\n-      maybe_add_to_invar(n->in(2), negate);\n-      NOT_PRODUCT(_tracer.offset_plus_k_7(n, _invar, negate, _offset);)\n-      return true;\n-    }\n-  }\n-  if (opc == Op_SubI) {\n-    if (n->in(2)->is_Con() && invariant(n->in(1))) {\n-      maybe_add_to_invar(n->in(1), negate);\n-      _offset += !negate ? -(n->in(2)->get_int()) : n->in(2)->get_int();\n-      NOT_PRODUCT(_tracer.offset_plus_k_8(n, _invar, negate, _offset);)\n-      return true;\n-    } else if (n->in(1)->is_Con() && invariant(n->in(2))) {\n-      _offset += negate ? -(n->in(1)->get_int()) : n->in(1)->get_int();\n-      maybe_add_to_invar(n->in(2), !negate);\n-      NOT_PRODUCT(_tracer.offset_plus_k_9(n, _invar, !negate, _offset);)\n-      return true;\n-    }\n-  }\n-\n-  if (!is_loop_member(n)) {\n-    \/\/ 'n' is loop invariant. Skip ConvI2L and CastII nodes before checking if 'n' is dominating the pre loop.\n-    if (opc == Op_ConvI2L) {\n-      n = n->in(1);\n-    }\n-    if (n->Opcode() == Op_CastII) {\n-      \/\/ Skip CastII nodes\n-      assert(!is_loop_member(n), \"sanity\");\n-      n = n->in(1);\n-    }\n-    \/\/ Check if 'n' can really be used as invariant (not in main loop and dominating the pre loop).\n-    if (invariant(n)) {\n-      maybe_add_to_invar(n, negate);\n-      NOT_PRODUCT(_tracer.offset_plus_k_10(n, _invar, negate, _offset);)\n-      return true;\n-    }\n-  }\n-\n-  NOT_PRODUCT(_tracer.offset_plus_k_11(n);)\n-  return false;\n-}\n-\n-Node* SWPointer::maybe_negate_invar(bool negate, Node* invar) {\n-#ifdef ASSERT\n-  _debug_negate_invar = negate;\n-#endif\n-  if (negate) {\n-    BasicType bt = invar->bottom_type()->basic_type();\n-    assert(bt == T_INT || bt == T_LONG, \"\");\n-    PhaseIterGVN& igvn = phase()->igvn();\n-    Node* zero = igvn.zerocon(bt);\n-    phase()->set_ctrl(zero, phase()->C->root());\n-    Node* sub = SubNode::make(zero, invar, bt);\n-    invar = register_if_new(sub);\n-  }\n-  return invar;\n-}\n-\n-Node* SWPointer::register_if_new(Node* n) const {\n-  PhaseIterGVN& igvn = phase()->igvn();\n-  Node* prev = igvn.hash_find_insert(n);\n-  if (prev != nullptr) {\n-    n->destruct(&igvn);\n-    n = prev;\n-  } else {\n-    Node* c = phase()->get_early_ctrl(n);\n-    phase()->register_new_node(n, c);\n-  }\n-  return n;\n-}\n-\n-void SWPointer::maybe_add_to_invar(Node* new_invar, bool negate) {\n-  new_invar = maybe_negate_invar(negate, new_invar);\n-  if (_invar == nullptr) {\n-    _invar = new_invar;\n-#ifdef ASSERT\n-    _debug_invar = new_invar;\n-#endif\n-    return;\n-  }\n-#ifdef ASSERT\n-  _debug_invar = NodeSentinel;\n-#endif\n-  BasicType new_invar_bt = new_invar->bottom_type()->basic_type();\n-  assert(new_invar_bt == T_INT || new_invar_bt == T_LONG, \"\");\n-  BasicType invar_bt = _invar->bottom_type()->basic_type();\n-  assert(invar_bt == T_INT || invar_bt == T_LONG, \"\");\n-\n-  BasicType bt = (new_invar_bt == T_LONG || invar_bt == T_LONG) ? T_LONG : T_INT;\n-  Node* current_invar = _invar;\n-  if (invar_bt != bt) {\n-    assert(bt == T_LONG && invar_bt == T_INT, \"\");\n-    assert(new_invar_bt == bt, \"\");\n-    current_invar = register_if_new(new ConvI2LNode(current_invar));\n-  } else if (new_invar_bt != bt) {\n-    assert(bt == T_LONG && new_invar_bt == T_INT, \"\");\n-    assert(invar_bt == bt, \"\");\n-    new_invar = register_if_new(new ConvI2LNode(new_invar));\n-  }\n-  Node* add = AddNode::make(current_invar, new_invar, bt);\n-  _invar = register_if_new(add);\n-}\n-\n-\/\/----------------------------print------------------------\n-void SWPointer::print() {\n-#ifndef PRODUCT\n-  tty->print(\"base: [%d]  adr: [%d]  scale: %d  offset: %d\",\n-             _base != nullptr ? _base->_idx : 0,\n-             _adr  != nullptr ? _adr->_idx  : 0,\n-             _scale, _offset);\n-  if (_invar != nullptr) {\n-    tty->print(\"  invar: [%d]\", _invar->_idx);\n-  }\n-  tty->cr();\n-#endif\n-}\n-\n-\/\/----------------------------tracing------------------------\n-#ifndef PRODUCT\n-void SWPointer::Tracer::print_depth() const {\n-  for (int ii = 0; ii < _depth; ++ii) {\n-    tty->print(\"  \");\n-  }\n-}\n-\n-void SWPointer::Tracer::ctor_1 (Node* mem) {\n-  if(_slp->is_trace_alignment()) {\n-    print_depth(); tty->print(\" %d SWPointer::SWPointer: start alignment analysis\", mem->_idx); mem->dump();\n-  }\n-}\n-\n-void SWPointer::Tracer::ctor_2(Node* adr) {\n-  if(_slp->is_trace_alignment()) {\n-    \/\/store_depth();\n-    inc_depth();\n-    print_depth(); tty->print(\" %d (adr) SWPointer::SWPointer: \", adr->_idx); adr->dump();\n-    inc_depth();\n-    print_depth(); tty->print(\" %d (base) SWPointer::SWPointer: \", adr->in(AddPNode::Base)->_idx); adr->in(AddPNode::Base)->dump();\n-  }\n-}\n-\n-void SWPointer::Tracer::ctor_3(Node* adr, int i) {\n-  if(_slp->is_trace_alignment()) {\n-    inc_depth();\n-    Node* offset = adr->in(AddPNode::Offset);\n-    print_depth(); tty->print(\" %d (offset) SWPointer::SWPointer: i = %d: \", offset->_idx, i); offset->dump();\n-  }\n-}\n-\n-void SWPointer::Tracer::ctor_4(Node* adr, int i) {\n-  if(_slp->is_trace_alignment()) {\n-    inc_depth();\n-    print_depth(); tty->print(\" %d (adr) SWPointer::SWPointer: i = %d: \", adr->_idx, i); adr->dump();\n-  }\n-}\n-\n-void SWPointer::Tracer::ctor_5(Node* adr, Node* base, int i) {\n-  if(_slp->is_trace_alignment()) {\n-    inc_depth();\n-    if (base == adr) {\n-      print_depth(); tty->print_cr(\"  \\\\ %d (adr) == %d (base) SWPointer::SWPointer: breaking analysis at i = %d\", adr->_idx, base->_idx, i);\n-    } else if (!adr->is_AddP()) {\n-      print_depth(); tty->print_cr(\"  \\\\ %d (adr) is NOT Addp SWPointer::SWPointer: breaking analysis at i = %d\", adr->_idx, i);\n-    }\n-  }\n-}\n-\n-void SWPointer::Tracer::ctor_6(Node* mem) {\n-  if(_slp->is_trace_alignment()) {\n-    \/\/restore_depth();\n-    print_depth(); tty->print_cr(\" %d (adr) SWPointer::SWPointer: stop analysis\", mem->_idx);\n-  }\n-}\n-\n-void SWPointer::Tracer::invariant_1(Node *n, Node *n_c) const {\n-  if (_slp->do_vector_loop() && _slp->is_debug() && _slp->_lpt->is_member(_slp->_phase->get_loop(n_c)) != (int)_slp->in_bb(n)) {\n-    int is_member =  _slp->_lpt->is_member(_slp->_phase->get_loop(n_c));\n-    int in_bb     =  _slp->in_bb(n);\n-    print_depth(); tty->print(\"  \\\\ \");  tty->print_cr(\" %d SWPointer::invariant  conditions differ: n_c %d\", n->_idx, n_c->_idx);\n-    print_depth(); tty->print(\"  \\\\ \");  tty->print_cr(\"is_member %d, in_bb %d\", is_member, in_bb);\n-    print_depth(); tty->print(\"  \\\\ \");  n->dump();\n-    print_depth(); tty->print(\"  \\\\ \");  n_c->dump();\n-  }\n-}\n-\n-void SWPointer::Tracer::scaled_iv_plus_offset_1(Node* n) {\n-  if(_slp->is_trace_alignment()) {\n-    print_depth(); tty->print(\" %d SWPointer::scaled_iv_plus_offset testing node: \", n->_idx);\n-    n->dump();\n-  }\n-}\n-\n-void SWPointer::Tracer::scaled_iv_plus_offset_2(Node* n) {\n-  if(_slp->is_trace_alignment()) {\n-    print_depth(); tty->print_cr(\" %d SWPointer::scaled_iv_plus_offset: PASSED\", n->_idx);\n-  }\n-}\n-\n-void SWPointer::Tracer::scaled_iv_plus_offset_3(Node* n) {\n-  if(_slp->is_trace_alignment()) {\n-    print_depth(); tty->print_cr(\" %d SWPointer::scaled_iv_plus_offset: PASSED\", n->_idx);\n-  }\n-}\n-\n-void SWPointer::Tracer::scaled_iv_plus_offset_4(Node* n) {\n-  if(_slp->is_trace_alignment()) {\n-    print_depth(); tty->print_cr(\" %d SWPointer::scaled_iv_plus_offset: Op_AddI PASSED\", n->_idx);\n-    print_depth(); tty->print(\"  \\\\ %d SWPointer::scaled_iv_plus_offset: in(1) is scaled_iv: \", n->in(1)->_idx); n->in(1)->dump();\n-    print_depth(); tty->print(\"  \\\\ %d SWPointer::scaled_iv_plus_offset: in(2) is offset_plus_k: \", n->in(2)->_idx); n->in(2)->dump();\n-  }\n-}\n-\n-void SWPointer::Tracer::scaled_iv_plus_offset_5(Node* n) {\n-  if(_slp->is_trace_alignment()) {\n-    print_depth(); tty->print_cr(\" %d SWPointer::scaled_iv_plus_offset: Op_AddI PASSED\", n->_idx);\n-    print_depth(); tty->print(\"  \\\\ %d SWPointer::scaled_iv_plus_offset: in(2) is scaled_iv: \", n->in(2)->_idx); n->in(2)->dump();\n-    print_depth(); tty->print(\"  \\\\ %d SWPointer::scaled_iv_plus_offset: in(1) is offset_plus_k: \", n->in(1)->_idx); n->in(1)->dump();\n-  }\n-}\n-\n-void SWPointer::Tracer::scaled_iv_plus_offset_6(Node* n) {\n-  if(_slp->is_trace_alignment()) {\n-    print_depth(); tty->print_cr(\" %d SWPointer::scaled_iv_plus_offset: Op_%s PASSED\", n->_idx, n->Name());\n-    print_depth(); tty->print(\"  \\\\  %d SWPointer::scaled_iv_plus_offset: in(1) is scaled_iv: \", n->in(1)->_idx); n->in(1)->dump();\n-    print_depth(); tty->print(\"  \\\\ %d SWPointer::scaled_iv_plus_offset: in(2) is offset_plus_k: \", n->in(2)->_idx); n->in(2)->dump();\n-  }\n-}\n-\n-void SWPointer::Tracer::scaled_iv_plus_offset_7(Node* n) {\n-  if(_slp->is_trace_alignment()) {\n-    print_depth(); tty->print_cr(\" %d SWPointer::scaled_iv_plus_offset: Op_%s PASSED\", n->_idx, n->Name());\n-    print_depth(); tty->print(\"  \\\\ %d SWPointer::scaled_iv_plus_offset: in(2) is scaled_iv: \", n->in(2)->_idx); n->in(2)->dump();\n-    print_depth(); tty->print(\"  \\\\ %d SWPointer::scaled_iv_plus_offset: in(1) is offset_plus_k: \", n->in(1)->_idx); n->in(1)->dump();\n-  }\n-}\n-\n-void SWPointer::Tracer::scaled_iv_plus_offset_8(Node* n) {\n-  if(_slp->is_trace_alignment()) {\n-    print_depth(); tty->print_cr(\" %d SWPointer::scaled_iv_plus_offset: FAILED\", n->_idx);\n-  }\n-}\n-\n-void SWPointer::Tracer::scaled_iv_1(Node* n) {\n-  if(_slp->is_trace_alignment()) {\n-    print_depth(); tty->print(\" %d SWPointer::scaled_iv: testing node: \", n->_idx); n->dump();\n-  }\n-}\n-\n-void SWPointer::Tracer::scaled_iv_2(Node* n, int scale) {\n-  if(_slp->is_trace_alignment()) {\n-    print_depth(); tty->print_cr(\" %d SWPointer::scaled_iv: FAILED since another _scale has been detected before\", n->_idx);\n-    print_depth(); tty->print_cr(\"  \\\\ SWPointer::scaled_iv: _scale (%d) != 0\", scale);\n-  }\n-}\n-\n-void SWPointer::Tracer::scaled_iv_3(Node* n, int scale) {\n-  if(_slp->is_trace_alignment()) {\n-    print_depth(); tty->print_cr(\" %d SWPointer::scaled_iv: is iv, setting _scale = %d\", n->_idx, scale);\n-  }\n-}\n-\n-void SWPointer::Tracer::scaled_iv_4(Node* n, int scale) {\n-  if(_slp->is_trace_alignment()) {\n-    print_depth(); tty->print_cr(\" %d SWPointer::scaled_iv: Op_MulI PASSED, setting _scale = %d\", n->_idx, scale);\n-    print_depth(); tty->print(\"  \\\\ %d SWPointer::scaled_iv: in(1) is iv: \", n->in(1)->_idx); n->in(1)->dump();\n-    print_depth(); tty->print(\"  \\\\ %d SWPointer::scaled_iv: in(2) is Con: \", n->in(2)->_idx); n->in(2)->dump();\n-  }\n-}\n-\n-void SWPointer::Tracer::scaled_iv_5(Node* n, int scale) {\n-  if(_slp->is_trace_alignment()) {\n-    print_depth(); tty->print_cr(\" %d SWPointer::scaled_iv: Op_MulI PASSED, setting _scale = %d\", n->_idx, scale);\n-    print_depth(); tty->print(\"  \\\\ %d SWPointer::scaled_iv: in(2) is iv: \", n->in(2)->_idx); n->in(2)->dump();\n-    print_depth(); tty->print(\"  \\\\ %d SWPointer::scaled_iv: in(1) is Con: \", n->in(1)->_idx); n->in(1)->dump();\n-  }\n-}\n-\n-void SWPointer::Tracer::scaled_iv_6(Node* n, int scale) {\n-  if(_slp->is_trace_alignment()) {\n-    print_depth(); tty->print_cr(\" %d SWPointer::scaled_iv: Op_LShiftI PASSED, setting _scale = %d\", n->_idx, scale);\n-    print_depth(); tty->print(\"  \\\\ %d SWPointer::scaled_iv: in(1) is iv: \", n->in(1)->_idx); n->in(1)->dump();\n-    print_depth(); tty->print(\"  \\\\ %d SWPointer::scaled_iv: in(2) is Con: \", n->in(2)->_idx); n->in(2)->dump();\n-  }\n-}\n-\n-void SWPointer::Tracer::scaled_iv_7(Node* n) {\n-  if(_slp->is_trace_alignment()) {\n-    print_depth(); tty->print_cr(\" %d SWPointer::scaled_iv: Op_ConvI2L PASSED\", n->_idx);\n-    print_depth(); tty->print_cr(\"  \\\\ SWPointer::scaled_iv: in(1) %d is scaled_iv_plus_offset: \", n->in(1)->_idx);\n-    inc_depth(); inc_depth();\n-    print_depth(); n->in(1)->dump();\n-    dec_depth(); dec_depth();\n-  }\n-}\n-\n-void SWPointer::Tracer::scaled_iv_8(Node* n, SWPointer* tmp) {\n-  if(_slp->is_trace_alignment()) {\n-    print_depth(); tty->print(\" %d SWPointer::scaled_iv: Op_LShiftL, creating tmp SWPointer: \", n->_idx); tmp->print();\n-  }\n-}\n-\n-void SWPointer::Tracer::scaled_iv_9(Node* n, int scale, int offset, Node* invar) {\n-  if(_slp->is_trace_alignment()) {\n-    print_depth(); tty->print_cr(\" %d SWPointer::scaled_iv: Op_LShiftL PASSED, setting _scale = %d, _offset = %d\", n->_idx, scale, offset);\n-    print_depth(); tty->print_cr(\"  \\\\ SWPointer::scaled_iv: in(1) [%d] is scaled_iv_plus_offset, in(2) [%d] used to scale: _scale = %d, _offset = %d\",\n-    n->in(1)->_idx, n->in(2)->_idx, scale, offset);\n-    if (invar != nullptr) {\n-      print_depth(); tty->print_cr(\"  \\\\ SWPointer::scaled_iv: scaled invariant: [%d]\", invar->_idx);\n-    }\n-    inc_depth(); inc_depth();\n-    print_depth(); n->in(1)->dump();\n-    print_depth(); n->in(2)->dump();\n-    if (invar != nullptr) {\n-      print_depth(); invar->dump();\n-    }\n-    dec_depth(); dec_depth();\n-  }\n-}\n-\n-void SWPointer::Tracer::scaled_iv_10(Node* n) {\n-  if(_slp->is_trace_alignment()) {\n-    print_depth(); tty->print_cr(\" %d SWPointer::scaled_iv: FAILED\", n->_idx);\n-  }\n-}\n-\n-void SWPointer::Tracer::offset_plus_k_1(Node* n) {\n-  if(_slp->is_trace_alignment()) {\n-    print_depth(); tty->print(\" %d SWPointer::offset_plus_k: testing node: \", n->_idx); n->dump();\n-  }\n-}\n-\n-void SWPointer::Tracer::offset_plus_k_2(Node* n, int _offset) {\n-  if(_slp->is_trace_alignment()) {\n-    print_depth(); tty->print_cr(\" %d SWPointer::offset_plus_k: Op_ConI PASSED, setting _offset = %d\", n->_idx, _offset);\n-  }\n-}\n-\n-void SWPointer::Tracer::offset_plus_k_3(Node* n, int _offset) {\n-  if(_slp->is_trace_alignment()) {\n-    print_depth(); tty->print_cr(\" %d SWPointer::offset_plus_k: Op_ConL PASSED, setting _offset = %d\", n->_idx, _offset);\n-  }\n-}\n-\n-void SWPointer::Tracer::offset_plus_k_4(Node* n) {\n-  if(_slp->is_trace_alignment()) {\n-    print_depth(); tty->print_cr(\" %d SWPointer::offset_plus_k: FAILED\", n->_idx);\n-    print_depth(); tty->print_cr(\"  \\\\ \" JLONG_FORMAT \" SWPointer::offset_plus_k: Op_ConL FAILED, k is too big\", n->get_long());\n-  }\n-}\n-\n-void SWPointer::Tracer::offset_plus_k_5(Node* n, Node* _invar) {\n-  if(_slp->is_trace_alignment()) {\n-    print_depth(); tty->print_cr(\" %d SWPointer::offset_plus_k: FAILED since another invariant has been detected before\", n->_idx);\n-    print_depth(); tty->print(\"  \\\\ %d SWPointer::offset_plus_k: _invar is not null: \", _invar->_idx); _invar->dump();\n-  }\n-}\n-\n-void SWPointer::Tracer::offset_plus_k_6(Node* n, Node* _invar, bool _negate_invar, int _offset) {\n-  if(_slp->is_trace_alignment()) {\n-    print_depth(); tty->print_cr(\" %d SWPointer::offset_plus_k: Op_AddI PASSED, setting _debug_negate_invar = %d, _invar = %d, _offset = %d\",\n-    n->_idx, _negate_invar, _invar->_idx, _offset);\n-    print_depth(); tty->print(\"  \\\\ %d SWPointer::offset_plus_k: in(2) is Con: \", n->in(2)->_idx); n->in(2)->dump();\n-    print_depth(); tty->print(\"  \\\\ %d SWPointer::offset_plus_k: in(1) is invariant: \", _invar->_idx); _invar->dump();\n-  }\n-}\n-\n-void SWPointer::Tracer::offset_plus_k_7(Node* n, Node* _invar, bool _negate_invar, int _offset) {\n-  if(_slp->is_trace_alignment()) {\n-    print_depth(); tty->print_cr(\" %d SWPointer::offset_plus_k: Op_AddI PASSED, setting _debug_negate_invar = %d, _invar = %d, _offset = %d\",\n-    n->_idx, _negate_invar, _invar->_idx, _offset);\n-    print_depth(); tty->print(\"  \\\\ %d SWPointer::offset_plus_k: in(1) is Con: \", n->in(1)->_idx); n->in(1)->dump();\n-    print_depth(); tty->print(\"  \\\\ %d SWPointer::offset_plus_k: in(2) is invariant: \", _invar->_idx); _invar->dump();\n-  }\n-}\n-\n-void SWPointer::Tracer::offset_plus_k_8(Node* n, Node* _invar, bool _negate_invar, int _offset) {\n-  if(_slp->is_trace_alignment()) {\n-    print_depth(); tty->print_cr(\" %d SWPointer::offset_plus_k: Op_SubI is PASSED, setting _debug_negate_invar = %d, _invar = %d, _offset = %d\",\n-    n->_idx, _negate_invar, _invar->_idx, _offset);\n-    print_depth(); tty->print(\"  \\\\ %d SWPointer::offset_plus_k: in(2) is Con: \", n->in(2)->_idx); n->in(2)->dump();\n-    print_depth(); tty->print(\"  \\\\ %d SWPointer::offset_plus_k: in(1) is invariant: \", _invar->_idx); _invar->dump();\n-  }\n-}\n-\n-void SWPointer::Tracer::offset_plus_k_9(Node* n, Node* _invar, bool _negate_invar, int _offset) {\n-  if(_slp->is_trace_alignment()) {\n-    print_depth(); tty->print_cr(\" %d SWPointer::offset_plus_k: Op_SubI PASSED, setting _debug_negate_invar = %d, _invar = %d, _offset = %d\", n->_idx, _negate_invar, _invar->_idx, _offset);\n-    print_depth(); tty->print(\"  \\\\ %d SWPointer::offset_plus_k: in(1) is Con: \", n->in(1)->_idx); n->in(1)->dump();\n-    print_depth(); tty->print(\"  \\\\ %d SWPointer::offset_plus_k: in(2) is invariant: \", _invar->_idx); _invar->dump();\n-  }\n-}\n-\n-void SWPointer::Tracer::offset_plus_k_10(Node* n, Node* _invar, bool _negate_invar, int _offset) {\n-  if(_slp->is_trace_alignment()) {\n-    print_depth(); tty->print_cr(\" %d SWPointer::offset_plus_k: PASSED, setting _debug_negate_invar = %d, _invar = %d, _offset = %d\", n->_idx, _negate_invar, _invar->_idx, _offset);\n-    print_depth(); tty->print_cr(\"  \\\\ %d SWPointer::offset_plus_k: is invariant\", n->_idx);\n-  }\n-}\n-\n-void SWPointer::Tracer::offset_plus_k_11(Node* n) {\n-  if(_slp->is_trace_alignment()) {\n-    print_depth(); tty->print_cr(\" %d SWPointer::offset_plus_k: FAILED\", n->_idx);\n-  }\n-}\n-\n-#endif\n@@ -4641,1 +3950,1 @@\n-  } else if (_n->is_Mem() || (_n->is_Phi() && _n->bottom_type() == Type::MEMORY)) {\n+  } else if (_n->is_Mem() || _n->is_memory_phi()) {\n@@ -4674,1 +3983,0 @@\n-\n","filename":"src\/hotspot\/share\/opto\/superword.cpp","additions":39,"deletions":731,"binary":false,"changes":770,"status":"modified"},{"patch":"@@ -27,4 +27,1 @@\n-#include \"opto\/loopnode.hpp\"\n-#include \"opto\/node.hpp\"\n-#include \"opto\/phaseX.hpp\"\n-#include \"opto\/vectornode.hpp\"\n+#include \"opto\/vectorization.hpp\"\n@@ -33,1 +30,0 @@\n-#include \"libadt\/dict.hpp\"\n@@ -63,1 +59,1 @@\n-class SWPointer;\n+class VPointer;\n@@ -232,39 +228,0 @@\n-\/\/ -----------------------VectorElementSizeStats-----------------------\n-\/\/ Vector lane size statistics for loop vectorization with vector masks\n-class VectorElementSizeStats {\n- private:\n-  static const int NO_SIZE = -1;\n-  static const int MIXED_SIZE = -2;\n-  int* _stats;\n-\n- public:\n-  VectorElementSizeStats(Arena* a) : _stats(NEW_ARENA_ARRAY(a, int, 4)) {\n-    memset(_stats, 0, sizeof(int) * 4);\n-  }\n-\n-  void record_size(int size) {\n-    assert(1 <= size && size <= 8 && is_power_of_2(size), \"Illegal size\");\n-    _stats[exact_log2(size)]++;\n-  }\n-\n-  int smallest_size() {\n-    for (int i = 0; i <= 3; i++) {\n-      if (_stats[i] > 0) return (1 << i);\n-    }\n-    return NO_SIZE;\n-  }\n-\n-  int largest_size() {\n-    for (int i = 3; i >= 0; i--) {\n-      if (_stats[i] > 0) return (1 << i);\n-    }\n-    return NO_SIZE;\n-  }\n-\n-  int unique_size() {\n-    int small = smallest_size();\n-    int large = largest_size();\n-    return (small == large) ? small : MIXED_SIZE;\n-  }\n-};\n-\n@@ -274,1 +231,1 @@\n- friend class SWPointer;\n+ friend class VPointer;\n@@ -313,1 +270,1 @@\n-  \/\/ Accessors for SWPointer\n+  \/\/ Accessors for VPointer\n@@ -377,1 +334,1 @@\n-    Node* found_pre_end = find_pre_loop_end(_lp);\n+    Node* found_pre_end = _lp->find_pre_loop_end();\n@@ -517,1 +474,1 @@\n-  bool mem_ref_has_no_alignment_violation(MemNode* mem_ref, int iv_adjustment, SWPointer &align_to_ref_p,\n+  bool mem_ref_has_no_alignment_violation(MemNode* mem_ref, int iv_adjustment, VPointer& align_to_ref_p,\n@@ -525,1 +482,1 @@\n-  bool ref_is_alignable(SWPointer& p);\n+  bool ref_is_alignable(VPointer& p);\n@@ -617,2 +574,0 @@\n-  \/\/ Find pre loop end from main loop.  Returns null if none.\n-  CountedLoopEndNode* find_pre_loop_end(CountedLoopNode *cl) const;\n@@ -632,172 +587,0 @@\n-\n-\n-\/\/------------------------------SWPointer---------------------------\n-\/\/ Information about an address for dependence checking and vector alignment\n-class SWPointer : public ArenaObj {\n- protected:\n-  MemNode*   _mem;           \/\/ My memory reference node\n-  SuperWord* _slp;           \/\/ SuperWord class\n-\n-  Node* _base;               \/\/ null if unsafe nonheap reference\n-  Node* _adr;                \/\/ address pointer\n-  int   _scale;              \/\/ multiplier for iv (in bytes), 0 if no loop iv\n-  int   _offset;             \/\/ constant offset (in bytes)\n-\n-  Node* _invar;              \/\/ invariant offset (in bytes), null if none\n-#ifdef ASSERT\n-  Node* _debug_invar;\n-  bool  _debug_negate_invar;       \/\/ if true then use: (0 - _invar)\n-  Node* _debug_invar_scale;        \/\/ multiplier for invariant\n-#endif\n-\n-  Node_Stack* _nstack;       \/\/ stack used to record a swpointer trace of variants\n-  bool        _analyze_only; \/\/ Used in loop unrolling only for swpointer trace\n-  uint        _stack_idx;    \/\/ Used in loop unrolling only for swpointer trace\n-\n-  PhaseIdealLoop* phase() const { return _slp->phase(); }\n-  IdealLoopTree*  lpt() const   { return _slp->lpt(); }\n-  PhiNode*        iv() const    { return _slp->iv();  } \/\/ Induction var\n-\n-  bool is_loop_member(Node* n) const;\n-  bool invariant(Node* n) const;\n-\n-  \/\/ Match: k*iv + offset\n-  bool scaled_iv_plus_offset(Node* n);\n-  \/\/ Match: k*iv where k is a constant that's not zero\n-  bool scaled_iv(Node* n);\n-  \/\/ Match: offset is (k [+\/- invariant])\n-  bool offset_plus_k(Node* n, bool negate = false);\n-\n- public:\n-  enum CMP {\n-    Less          = 1,\n-    Greater       = 2,\n-    Equal         = 4,\n-    NotEqual      = (Less | Greater),\n-    NotComparable = (Less | Greater | Equal)\n-  };\n-\n-  SWPointer(MemNode* mem, SuperWord* slp, Node_Stack *nstack, bool analyze_only);\n-  \/\/ Following is used to create a temporary object during\n-  \/\/ the pattern match of an address expression.\n-  SWPointer(SWPointer* p);\n-\n-  bool valid()  { return _adr != nullptr; }\n-  bool has_iv() { return _scale != 0; }\n-\n-  Node* base()             { return _base; }\n-  Node* adr()              { return _adr; }\n-  MemNode* mem()           { return _mem; }\n-  int   scale_in_bytes()   { return _scale; }\n-  Node* invar()            { return _invar; }\n-  int   offset_in_bytes()  { return _offset; }\n-  int   memory_size()      { return _mem->memory_size(); }\n-  Node_Stack* node_stack() { return _nstack; }\n-\n-  \/\/ Comparable?\n-  bool invar_equals(SWPointer& q) {\n-    assert(_debug_invar == NodeSentinel || q._debug_invar == NodeSentinel ||\n-           (_invar == q._invar) == (_debug_invar == q._debug_invar &&\n-                                    _debug_invar_scale == q._debug_invar_scale &&\n-                                    _debug_negate_invar == q._debug_negate_invar), \"\");\n-    return _invar == q._invar;\n-  }\n-\n-  int cmp(SWPointer& q) {\n-    if (valid() && q.valid() &&\n-        (_adr == q._adr || (_base == _adr && q._base == q._adr)) &&\n-        _scale == q._scale   && invar_equals(q)) {\n-      bool overlap = q._offset <   _offset +   memory_size() &&\n-                       _offset < q._offset + q.memory_size();\n-      return overlap ? Equal : (_offset < q._offset ? Less : Greater);\n-    } else {\n-      return NotComparable;\n-    }\n-  }\n-\n-  bool not_equal(SWPointer& q)    { return not_equal(cmp(q)); }\n-  bool equal(SWPointer& q)        { return equal(cmp(q)); }\n-  bool comparable(SWPointer& q)   { return comparable(cmp(q)); }\n-  static bool not_equal(int cmp)  { return cmp <= NotEqual; }\n-  static bool equal(int cmp)      { return cmp == Equal; }\n-  static bool comparable(int cmp) { return cmp < NotComparable; }\n-\n-  void print();\n-\n-#ifndef PRODUCT\n-  class Tracer {\n-    friend class SuperWord;\n-    friend class SWPointer;\n-    SuperWord*   _slp;\n-    static int   _depth;\n-    int _depth_save;\n-    void print_depth() const;\n-    int  depth() const    { return _depth; }\n-    void set_depth(int d) { _depth = d; }\n-    void inc_depth()      { _depth++;}\n-    void dec_depth()      { if (_depth > 0) _depth--;}\n-    void store_depth()    {_depth_save = _depth;}\n-    void restore_depth()  {_depth = _depth_save;}\n-\n-    class Depth {\n-      friend class Tracer;\n-      friend class SWPointer;\n-      friend class SuperWord;\n-      Depth()  { ++_depth; }\n-      Depth(int x)  { _depth = 0; }\n-      ~Depth() { if (_depth > 0) --_depth;}\n-    };\n-    Tracer (SuperWord* slp) : _slp(slp) {}\n-\n-    \/\/ tracing functions\n-    void ctor_1(Node* mem);\n-    void ctor_2(Node* adr);\n-    void ctor_3(Node* adr, int i);\n-    void ctor_4(Node* adr, int i);\n-    void ctor_5(Node* adr, Node* base,  int i);\n-    void ctor_6(Node* mem);\n-\n-    void invariant_1(Node *n, Node *n_c) const;\n-\n-    void scaled_iv_plus_offset_1(Node* n);\n-    void scaled_iv_plus_offset_2(Node* n);\n-    void scaled_iv_plus_offset_3(Node* n);\n-    void scaled_iv_plus_offset_4(Node* n);\n-    void scaled_iv_plus_offset_5(Node* n);\n-    void scaled_iv_plus_offset_6(Node* n);\n-    void scaled_iv_plus_offset_7(Node* n);\n-    void scaled_iv_plus_offset_8(Node* n);\n-\n-    void scaled_iv_1(Node* n);\n-    void scaled_iv_2(Node* n, int scale);\n-    void scaled_iv_3(Node* n, int scale);\n-    void scaled_iv_4(Node* n, int scale);\n-    void scaled_iv_5(Node* n, int scale);\n-    void scaled_iv_6(Node* n, int scale);\n-    void scaled_iv_7(Node* n);\n-    void scaled_iv_8(Node* n, SWPointer* tmp);\n-    void scaled_iv_9(Node* n, int _scale, int _offset, Node* _invar);\n-    void scaled_iv_10(Node* n);\n-\n-    void offset_plus_k_1(Node* n);\n-    void offset_plus_k_2(Node* n, int _offset);\n-    void offset_plus_k_3(Node* n, int _offset);\n-    void offset_plus_k_4(Node* n);\n-    void offset_plus_k_5(Node* n, Node* _invar);\n-    void offset_plus_k_6(Node* n, Node* _invar, bool _negate_invar, int _offset);\n-    void offset_plus_k_7(Node* n, Node* _invar, bool _negate_invar, int _offset);\n-    void offset_plus_k_8(Node* n, Node* _invar, bool _negate_invar, int _offset);\n-    void offset_plus_k_9(Node* n, Node* _invar, bool _negate_invar, int _offset);\n-    void offset_plus_k_10(Node* n, Node* _invar, bool _negate_invar, int _offset);\n-    void offset_plus_k_11(Node* n);\n-\n-  } _tracer;\/\/TRacer;\n-#endif\n-\n-  Node* maybe_negate_invar(bool negate, Node* invar);\n-\n-  void maybe_add_to_invar(Node* new_invar, bool negate);\n-\n-  Node* register_if_new(Node* n) const;\n-};\n-\n","filename":"src\/hotspot\/share\/opto\/superword.hpp","additions":7,"deletions":224,"binary":false,"changes":231,"status":"modified"},{"patch":"@@ -0,0 +1,698 @@\n+\/*\n+ * Copyright (c) 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2023, Arm Limited. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"opto\/addnode.hpp\"\n+#include \"opto\/connode.hpp\"\n+#include \"opto\/convertnode.hpp\"\n+#include \"opto\/matcher.hpp\"\n+#include \"opto\/mulnode.hpp\"\n+#include \"opto\/rootnode.hpp\"\n+#include \"opto\/vectorization.hpp\"\n+\n+\/\/ ==================================VPointer==================================\n+\n+#ifndef PRODUCT\n+int VPointer::Tracer::_depth = 0;\n+#endif\n+\n+VPointer::VPointer(MemNode* mem, PhaseIdealLoop* phase, IdealLoopTree* lpt,\n+                   Node_Stack* nstack, bool analyze_only) :\n+  _mem(mem), _phase(phase), _lpt(lpt),\n+  _iv(lpt->_head->as_CountedLoop()->phi()->as_Phi()),\n+  _base(nullptr), _adr(nullptr), _scale(0), _offset(0), _invar(nullptr),\n+#ifdef ASSERT\n+  _debug_invar(nullptr), _debug_negate_invar(false), _debug_invar_scale(nullptr),\n+#endif\n+  _nstack(nstack), _analyze_only(analyze_only), _stack_idx(0)\n+#ifndef PRODUCT\n+  , _tracer((phase->C->directive()->VectorizeDebugOption & 2) > 0)\n+#endif\n+{\n+  NOT_PRODUCT(_tracer.ctor_1(mem);)\n+\n+  Node* adr = mem->in(MemNode::Address);\n+  if (!adr->is_AddP()) {\n+    assert(!valid(), \"too complex\");\n+    return;\n+  }\n+  \/\/ Match AddP(base, AddP(ptr, k*iv [+ invariant]), constant)\n+  Node* base = adr->in(AddPNode::Base);\n+  \/\/ The base address should be loop invariant\n+  if (is_loop_member(base)) {\n+    assert(!valid(), \"base address is loop variant\");\n+    return;\n+  }\n+  \/\/ unsafe references require misaligned vector access support\n+  if (base->is_top() && !Matcher::misaligned_vectors_ok()) {\n+    assert(!valid(), \"unsafe access\");\n+    return;\n+  }\n+\n+  NOT_PRODUCT(if(_tracer._is_trace_alignment) _tracer.store_depth();)\n+  NOT_PRODUCT(_tracer.ctor_2(adr);)\n+\n+  int i;\n+  for (i = 0; ; i++) {\n+    NOT_PRODUCT(_tracer.ctor_3(adr, i);)\n+\n+    if (!scaled_iv_plus_offset(adr->in(AddPNode::Offset))) {\n+      assert(!valid(), \"too complex\");\n+      return;\n+    }\n+    adr = adr->in(AddPNode::Address);\n+    NOT_PRODUCT(_tracer.ctor_4(adr, i);)\n+\n+    if (base == adr || !adr->is_AddP()) {\n+      NOT_PRODUCT(_tracer.ctor_5(adr, base, i);)\n+      break; \/\/ stop looking at addp's\n+    }\n+  }\n+  if (is_loop_member(adr)) {\n+    assert(!valid(), \"adr is loop variant\");\n+    return;\n+  }\n+\n+  if (!base->is_top() && adr != base) {\n+    assert(!valid(), \"adr and base differ\");\n+    return;\n+  }\n+\n+  NOT_PRODUCT(if(_tracer._is_trace_alignment) _tracer.restore_depth();)\n+  NOT_PRODUCT(_tracer.ctor_6(mem);)\n+\n+  _base = base;\n+  _adr  = adr;\n+  assert(valid(), \"Usable\");\n+}\n+\n+\/\/ Following is used to create a temporary object during\n+\/\/ the pattern match of an address expression.\n+VPointer::VPointer(VPointer* p) :\n+  _mem(p->_mem), _phase(p->_phase), _lpt(p->_lpt), _iv(p->_iv),\n+  _base(nullptr), _adr(nullptr), _scale(0), _offset(0), _invar(nullptr),\n+#ifdef ASSERT\n+  _debug_invar(nullptr), _debug_negate_invar(false), _debug_invar_scale(nullptr),\n+#endif\n+  _nstack(p->_nstack), _analyze_only(p->_analyze_only), _stack_idx(p->_stack_idx)\n+#ifndef PRODUCT\n+  , _tracer(p->_tracer._is_trace_alignment)\n+#endif\n+{}\n+\n+bool VPointer::is_loop_member(Node* n) const {\n+  Node* n_c = phase()->get_ctrl(n);\n+  return lpt()->is_member(phase()->get_loop(n_c));\n+}\n+\n+bool VPointer::invariant(Node* n) const {\n+  NOT_PRODUCT(Tracer::Depth dd;)\n+  \/\/ TODO: Add more trace output for invariant check after later refactoring\n+  bool is_not_member = !is_loop_member(n);\n+  if (is_not_member) {\n+    CountedLoopNode* cl = lpt()->_head->as_CountedLoop();\n+    if (cl->is_main_loop()) {\n+      \/\/ Check that n_c dominates the pre loop head node. If it does not, then\n+      \/\/ we cannot use n as invariant for the pre loop CountedLoopEndNode check\n+      \/\/ because n_c is either part of the pre loop or between the pre and the\n+      \/\/ main loop (Illegal invariant happens when n_c is a CastII node that\n+      \/\/ prevents data nodes to flow above the main loop).\n+      CountedLoopEndNode* pre_loop_end = cl->find_pre_loop_end();\n+      if (pre_loop_end != nullptr) {\n+        Node* n_c = phase()->get_ctrl(n);\n+        return phase()->is_dominator(n_c, pre_loop_end->loopnode());\n+      }\n+    }\n+  }\n+  return is_not_member;\n+}\n+\n+\/\/------------------------scaled_iv_plus_offset--------------------\n+\/\/ Match: k*iv + offset\n+\/\/ where: k is a constant that maybe zero, and\n+\/\/        offset is (k2 [+\/- invariant]) where k2 maybe zero and invariant is optional\n+bool VPointer::scaled_iv_plus_offset(Node* n) {\n+  NOT_PRODUCT(Tracer::Depth ddd;)\n+  NOT_PRODUCT(_tracer.scaled_iv_plus_offset_1(n);)\n+\n+  if (scaled_iv(n)) {\n+    NOT_PRODUCT(_tracer.scaled_iv_plus_offset_2(n);)\n+    return true;\n+  }\n+\n+  if (offset_plus_k(n)) {\n+    NOT_PRODUCT(_tracer.scaled_iv_plus_offset_3(n);)\n+    return true;\n+  }\n+\n+  int opc = n->Opcode();\n+  if (opc == Op_AddI) {\n+    if (offset_plus_k(n->in(2)) && scaled_iv_plus_offset(n->in(1))) {\n+      NOT_PRODUCT(_tracer.scaled_iv_plus_offset_4(n);)\n+      return true;\n+    }\n+    if (offset_plus_k(n->in(1)) && scaled_iv_plus_offset(n->in(2))) {\n+      NOT_PRODUCT(_tracer.scaled_iv_plus_offset_5(n);)\n+      return true;\n+    }\n+  } else if (opc == Op_SubI || opc == Op_SubL) {\n+    if (offset_plus_k(n->in(2), true) && scaled_iv_plus_offset(n->in(1))) {\n+      NOT_PRODUCT(_tracer.scaled_iv_plus_offset_6(n);)\n+      return true;\n+    }\n+    if (offset_plus_k(n->in(1)) && scaled_iv_plus_offset(n->in(2))) {\n+      _scale *= -1;\n+      NOT_PRODUCT(_tracer.scaled_iv_plus_offset_7(n);)\n+      return true;\n+    }\n+  }\n+\n+  NOT_PRODUCT(_tracer.scaled_iv_plus_offset_8(n);)\n+  return false;\n+}\n+\n+\/\/----------------------------scaled_iv------------------------\n+\/\/ Match: k*iv where k is a constant that's not zero\n+bool VPointer::scaled_iv(Node* n) {\n+  NOT_PRODUCT(Tracer::Depth ddd;)\n+  NOT_PRODUCT(_tracer.scaled_iv_1(n);)\n+\n+  if (_scale != 0) { \/\/ already found a scale\n+    NOT_PRODUCT(_tracer.scaled_iv_2(n, _scale);)\n+    return false;\n+  }\n+\n+  if (n == iv()) {\n+    _scale = 1;\n+    NOT_PRODUCT(_tracer.scaled_iv_3(n, _scale);)\n+    return true;\n+  }\n+  if (_analyze_only && (is_loop_member(n))) {\n+    _nstack->push(n, _stack_idx++);\n+  }\n+\n+  int opc = n->Opcode();\n+  if (opc == Op_MulI) {\n+    if (n->in(1) == iv() && n->in(2)->is_Con()) {\n+      _scale = n->in(2)->get_int();\n+      NOT_PRODUCT(_tracer.scaled_iv_4(n, _scale);)\n+      return true;\n+    } else if (n->in(2) == iv() && n->in(1)->is_Con()) {\n+      _scale = n->in(1)->get_int();\n+      NOT_PRODUCT(_tracer.scaled_iv_5(n, _scale);)\n+      return true;\n+    }\n+  } else if (opc == Op_LShiftI) {\n+    if (n->in(1) == iv() && n->in(2)->is_Con()) {\n+      _scale = 1 << n->in(2)->get_int();\n+      NOT_PRODUCT(_tracer.scaled_iv_6(n, _scale);)\n+      return true;\n+    }\n+  } else if (opc == Op_ConvI2L || opc == Op_CastII) {\n+    if (scaled_iv_plus_offset(n->in(1))) {\n+      NOT_PRODUCT(_tracer.scaled_iv_7(n);)\n+      return true;\n+    }\n+  } else if (opc == Op_LShiftL && n->in(2)->is_Con()) {\n+    if (!has_iv()) {\n+      \/\/ Need to preserve the current _offset value, so\n+      \/\/ create a temporary object for this expression subtree.\n+      \/\/ Hacky, so should re-engineer the address pattern match.\n+      NOT_PRODUCT(Tracer::Depth dddd;)\n+      VPointer tmp(this);\n+      NOT_PRODUCT(_tracer.scaled_iv_8(n, &tmp);)\n+\n+      if (tmp.scaled_iv_plus_offset(n->in(1))) {\n+        int scale = n->in(2)->get_int();\n+        _scale   = tmp._scale  << scale;\n+        _offset += tmp._offset << scale;\n+        if (tmp._invar != nullptr) {\n+          BasicType bt = tmp._invar->bottom_type()->basic_type();\n+          assert(bt == T_INT || bt == T_LONG, \"\");\n+          maybe_add_to_invar(register_if_new(LShiftNode::make(tmp._invar, n->in(2), bt)), false);\n+#ifdef ASSERT\n+          _debug_invar_scale = n->in(2);\n+#endif\n+        }\n+        NOT_PRODUCT(_tracer.scaled_iv_9(n, _scale, _offset, _invar);)\n+        return true;\n+      }\n+    }\n+  }\n+  NOT_PRODUCT(_tracer.scaled_iv_10(n);)\n+  return false;\n+}\n+\n+\/\/----------------------------offset_plus_k------------------------\n+\/\/ Match: offset is (k [+\/- invariant])\n+\/\/ where k maybe zero and invariant is optional, but not both.\n+bool VPointer::offset_plus_k(Node* n, bool negate) {\n+  NOT_PRODUCT(Tracer::Depth ddd;)\n+  NOT_PRODUCT(_tracer.offset_plus_k_1(n);)\n+\n+  int opc = n->Opcode();\n+  if (opc == Op_ConI) {\n+    _offset += negate ? -(n->get_int()) : n->get_int();\n+    NOT_PRODUCT(_tracer.offset_plus_k_2(n, _offset);)\n+    return true;\n+  } else if (opc == Op_ConL) {\n+    \/\/ Okay if value fits into an int\n+    const TypeLong* t = n->find_long_type();\n+    if (t->higher_equal(TypeLong::INT)) {\n+      jlong loff = n->get_long();\n+      jint  off  = (jint)loff;\n+      _offset += negate ? -off : loff;\n+      NOT_PRODUCT(_tracer.offset_plus_k_3(n, _offset);)\n+      return true;\n+    }\n+    NOT_PRODUCT(_tracer.offset_plus_k_4(n);)\n+    return false;\n+  }\n+  assert((_debug_invar == nullptr) == (_invar == nullptr), \"\");\n+\n+  if (_analyze_only && is_loop_member(n)) {\n+    _nstack->push(n, _stack_idx++);\n+  }\n+  if (opc == Op_AddI) {\n+    if (n->in(2)->is_Con() && invariant(n->in(1))) {\n+      maybe_add_to_invar(n->in(1), negate);\n+      _offset += negate ? -(n->in(2)->get_int()) : n->in(2)->get_int();\n+      NOT_PRODUCT(_tracer.offset_plus_k_6(n, _invar, negate, _offset);)\n+      return true;\n+    } else if (n->in(1)->is_Con() && invariant(n->in(2))) {\n+      _offset += negate ? -(n->in(1)->get_int()) : n->in(1)->get_int();\n+      maybe_add_to_invar(n->in(2), negate);\n+      NOT_PRODUCT(_tracer.offset_plus_k_7(n, _invar, negate, _offset);)\n+      return true;\n+    }\n+  }\n+  if (opc == Op_SubI) {\n+    if (n->in(2)->is_Con() && invariant(n->in(1))) {\n+      maybe_add_to_invar(n->in(1), negate);\n+      _offset += !negate ? -(n->in(2)->get_int()) : n->in(2)->get_int();\n+      NOT_PRODUCT(_tracer.offset_plus_k_8(n, _invar, negate, _offset);)\n+      return true;\n+    } else if (n->in(1)->is_Con() && invariant(n->in(2))) {\n+      _offset += negate ? -(n->in(1)->get_int()) : n->in(1)->get_int();\n+      maybe_add_to_invar(n->in(2), !negate);\n+      NOT_PRODUCT(_tracer.offset_plus_k_9(n, _invar, !negate, _offset);)\n+      return true;\n+    }\n+  }\n+\n+  if (!is_loop_member(n)) {\n+    \/\/ 'n' is loop invariant. Skip ConvI2L and CastII nodes before checking if 'n' is dominating the pre loop.\n+    if (opc == Op_ConvI2L) {\n+      n = n->in(1);\n+    }\n+    if (n->Opcode() == Op_CastII) {\n+      \/\/ Skip CastII nodes\n+      assert(!is_loop_member(n), \"sanity\");\n+      n = n->in(1);\n+    }\n+    \/\/ Check if 'n' can really be used as invariant (not in main loop and dominating the pre loop).\n+    if (invariant(n)) {\n+      maybe_add_to_invar(n, negate);\n+      NOT_PRODUCT(_tracer.offset_plus_k_10(n, _invar, negate, _offset);)\n+      return true;\n+    }\n+  }\n+\n+  NOT_PRODUCT(_tracer.offset_plus_k_11(n);)\n+  return false;\n+}\n+\n+Node* VPointer::maybe_negate_invar(bool negate, Node* invar) {\n+#ifdef ASSERT\n+  _debug_negate_invar = negate;\n+#endif\n+  if (negate) {\n+    BasicType bt = invar->bottom_type()->basic_type();\n+    assert(bt == T_INT || bt == T_LONG, \"\");\n+    PhaseIterGVN& igvn = phase()->igvn();\n+    Node* zero = igvn.zerocon(bt);\n+    phase()->set_ctrl(zero, phase()->C->root());\n+    Node* sub = SubNode::make(zero, invar, bt);\n+    invar = register_if_new(sub);\n+  }\n+  return invar;\n+}\n+\n+Node* VPointer::register_if_new(Node* n) const {\n+  PhaseIterGVN& igvn = phase()->igvn();\n+  Node* prev = igvn.hash_find_insert(n);\n+  if (prev != nullptr) {\n+    n->destruct(&igvn);\n+    n = prev;\n+  } else {\n+    Node* c = phase()->get_early_ctrl(n);\n+    phase()->register_new_node(n, c);\n+  }\n+  return n;\n+}\n+\n+void VPointer::maybe_add_to_invar(Node* new_invar, bool negate) {\n+  new_invar = maybe_negate_invar(negate, new_invar);\n+  if (_invar == nullptr) {\n+    _invar = new_invar;\n+#ifdef ASSERT\n+    _debug_invar = new_invar;\n+#endif\n+    return;\n+  }\n+#ifdef ASSERT\n+  _debug_invar = NodeSentinel;\n+#endif\n+  BasicType new_invar_bt = new_invar->bottom_type()->basic_type();\n+  assert(new_invar_bt == T_INT || new_invar_bt == T_LONG, \"\");\n+  BasicType invar_bt = _invar->bottom_type()->basic_type();\n+  assert(invar_bt == T_INT || invar_bt == T_LONG, \"\");\n+\n+  BasicType bt = (new_invar_bt == T_LONG || invar_bt == T_LONG) ? T_LONG : T_INT;\n+  Node* current_invar = _invar;\n+  if (invar_bt != bt) {\n+    assert(bt == T_LONG && invar_bt == T_INT, \"\");\n+    assert(new_invar_bt == bt, \"\");\n+    current_invar = register_if_new(new ConvI2LNode(current_invar));\n+  } else if (new_invar_bt != bt) {\n+    assert(bt == T_LONG && new_invar_bt == T_INT, \"\");\n+    assert(invar_bt == bt, \"\");\n+    new_invar = register_if_new(new ConvI2LNode(new_invar));\n+  }\n+  Node* add = AddNode::make(current_invar, new_invar, bt);\n+  _invar = register_if_new(add);\n+}\n+\n+\/\/----------------------------print------------------------\n+void VPointer::print() {\n+#ifndef PRODUCT\n+  tty->print(\"base: [%d]  adr: [%d]  scale: %d  offset: %d\",\n+             _base != nullptr ? _base->_idx : 0,\n+             _adr  != nullptr ? _adr->_idx  : 0,\n+             _scale, _offset);\n+  if (_invar != nullptr) {\n+    tty->print(\"  invar: [%d]\", _invar->_idx);\n+  }\n+  tty->cr();\n+#endif\n+}\n+\n+\/\/----------------------------tracing------------------------\n+#ifndef PRODUCT\n+void VPointer::Tracer::print_depth() const {\n+  for (int ii = 0; ii < _depth; ++ii) {\n+    tty->print(\"  \");\n+  }\n+}\n+\n+void VPointer::Tracer::ctor_1(Node* mem) {\n+  if (_is_trace_alignment) {\n+    print_depth(); tty->print(\" %d VPointer::VPointer: start alignment analysis\", mem->_idx); mem->dump();\n+  }\n+}\n+\n+void VPointer::Tracer::ctor_2(Node* adr) {\n+  if (_is_trace_alignment) {\n+    \/\/store_depth();\n+    inc_depth();\n+    print_depth(); tty->print(\" %d (adr) VPointer::VPointer: \", adr->_idx); adr->dump();\n+    inc_depth();\n+    print_depth(); tty->print(\" %d (base) VPointer::VPointer: \", adr->in(AddPNode::Base)->_idx); adr->in(AddPNode::Base)->dump();\n+  }\n+}\n+\n+void VPointer::Tracer::ctor_3(Node* adr, int i) {\n+  if (_is_trace_alignment) {\n+    inc_depth();\n+    Node* offset = adr->in(AddPNode::Offset);\n+    print_depth(); tty->print(\" %d (offset) VPointer::VPointer: i = %d: \", offset->_idx, i); offset->dump();\n+  }\n+}\n+\n+void VPointer::Tracer::ctor_4(Node* adr, int i) {\n+  if (_is_trace_alignment) {\n+    inc_depth();\n+    print_depth(); tty->print(\" %d (adr) VPointer::VPointer: i = %d: \", adr->_idx, i); adr->dump();\n+  }\n+}\n+\n+void VPointer::Tracer::ctor_5(Node* adr, Node* base, int i) {\n+  if (_is_trace_alignment) {\n+    inc_depth();\n+    if (base == adr) {\n+      print_depth(); tty->print_cr(\"  \\\\ %d (adr) == %d (base) VPointer::VPointer: breaking analysis at i = %d\", adr->_idx, base->_idx, i);\n+    } else if (!adr->is_AddP()) {\n+      print_depth(); tty->print_cr(\"  \\\\ %d (adr) is NOT Addp VPointer::VPointer: breaking analysis at i = %d\", adr->_idx, i);\n+    }\n+  }\n+}\n+\n+void VPointer::Tracer::ctor_6(Node* mem) {\n+  if (_is_trace_alignment) {\n+    \/\/restore_depth();\n+    print_depth(); tty->print_cr(\" %d (adr) VPointer::VPointer: stop analysis\", mem->_idx);\n+  }\n+}\n+\n+void VPointer::Tracer::scaled_iv_plus_offset_1(Node* n) {\n+  if (_is_trace_alignment) {\n+    print_depth(); tty->print(\" %d VPointer::scaled_iv_plus_offset testing node: \", n->_idx);\n+    n->dump();\n+  }\n+}\n+\n+void VPointer::Tracer::scaled_iv_plus_offset_2(Node* n) {\n+  if (_is_trace_alignment) {\n+    print_depth(); tty->print_cr(\" %d VPointer::scaled_iv_plus_offset: PASSED\", n->_idx);\n+  }\n+}\n+\n+void VPointer::Tracer::scaled_iv_plus_offset_3(Node* n) {\n+  if (_is_trace_alignment) {\n+    print_depth(); tty->print_cr(\" %d VPointer::scaled_iv_plus_offset: PASSED\", n->_idx);\n+  }\n+}\n+\n+void VPointer::Tracer::scaled_iv_plus_offset_4(Node* n) {\n+  if (_is_trace_alignment) {\n+    print_depth(); tty->print_cr(\" %d VPointer::scaled_iv_plus_offset: Op_AddI PASSED\", n->_idx);\n+    print_depth(); tty->print(\"  \\\\ %d VPointer::scaled_iv_plus_offset: in(1) is scaled_iv: \", n->in(1)->_idx); n->in(1)->dump();\n+    print_depth(); tty->print(\"  \\\\ %d VPointer::scaled_iv_plus_offset: in(2) is offset_plus_k: \", n->in(2)->_idx); n->in(2)->dump();\n+  }\n+}\n+\n+void VPointer::Tracer::scaled_iv_plus_offset_5(Node* n) {\n+  if (_is_trace_alignment) {\n+    print_depth(); tty->print_cr(\" %d VPointer::scaled_iv_plus_offset: Op_AddI PASSED\", n->_idx);\n+    print_depth(); tty->print(\"  \\\\ %d VPointer::scaled_iv_plus_offset: in(2) is scaled_iv: \", n->in(2)->_idx); n->in(2)->dump();\n+    print_depth(); tty->print(\"  \\\\ %d VPointer::scaled_iv_plus_offset: in(1) is offset_plus_k: \", n->in(1)->_idx); n->in(1)->dump();\n+  }\n+}\n+\n+void VPointer::Tracer::scaled_iv_plus_offset_6(Node* n) {\n+  if (_is_trace_alignment) {\n+    print_depth(); tty->print_cr(\" %d VPointer::scaled_iv_plus_offset: Op_%s PASSED\", n->_idx, n->Name());\n+    print_depth(); tty->print(\"  \\\\  %d VPointer::scaled_iv_plus_offset: in(1) is scaled_iv: \", n->in(1)->_idx); n->in(1)->dump();\n+    print_depth(); tty->print(\"  \\\\ %d VPointer::scaled_iv_plus_offset: in(2) is offset_plus_k: \", n->in(2)->_idx); n->in(2)->dump();\n+  }\n+}\n+\n+void VPointer::Tracer::scaled_iv_plus_offset_7(Node* n) {\n+  if (_is_trace_alignment) {\n+    print_depth(); tty->print_cr(\" %d VPointer::scaled_iv_plus_offset: Op_%s PASSED\", n->_idx, n->Name());\n+    print_depth(); tty->print(\"  \\\\ %d VPointer::scaled_iv_plus_offset: in(2) is scaled_iv: \", n->in(2)->_idx); n->in(2)->dump();\n+    print_depth(); tty->print(\"  \\\\ %d VPointer::scaled_iv_plus_offset: in(1) is offset_plus_k: \", n->in(1)->_idx); n->in(1)->dump();\n+  }\n+}\n+\n+void VPointer::Tracer::scaled_iv_plus_offset_8(Node* n) {\n+  if (_is_trace_alignment) {\n+    print_depth(); tty->print_cr(\" %d VPointer::scaled_iv_plus_offset: FAILED\", n->_idx);\n+  }\n+}\n+\n+void VPointer::Tracer::scaled_iv_1(Node* n) {\n+  if (_is_trace_alignment) {\n+    print_depth(); tty->print(\" %d VPointer::scaled_iv: testing node: \", n->_idx); n->dump();\n+  }\n+}\n+\n+void VPointer::Tracer::scaled_iv_2(Node* n, int scale) {\n+  if (_is_trace_alignment) {\n+    print_depth(); tty->print_cr(\" %d VPointer::scaled_iv: FAILED since another _scale has been detected before\", n->_idx);\n+    print_depth(); tty->print_cr(\"  \\\\ VPointer::scaled_iv: _scale (%d) != 0\", scale);\n+  }\n+}\n+\n+void VPointer::Tracer::scaled_iv_3(Node* n, int scale) {\n+  if (_is_trace_alignment) {\n+    print_depth(); tty->print_cr(\" %d VPointer::scaled_iv: is iv, setting _scale = %d\", n->_idx, scale);\n+  }\n+}\n+\n+void VPointer::Tracer::scaled_iv_4(Node* n, int scale) {\n+  if (_is_trace_alignment) {\n+    print_depth(); tty->print_cr(\" %d VPointer::scaled_iv: Op_MulI PASSED, setting _scale = %d\", n->_idx, scale);\n+    print_depth(); tty->print(\"  \\\\ %d VPointer::scaled_iv: in(1) is iv: \", n->in(1)->_idx); n->in(1)->dump();\n+    print_depth(); tty->print(\"  \\\\ %d VPointer::scaled_iv: in(2) is Con: \", n->in(2)->_idx); n->in(2)->dump();\n+  }\n+}\n+\n+void VPointer::Tracer::scaled_iv_5(Node* n, int scale) {\n+  if (_is_trace_alignment) {\n+    print_depth(); tty->print_cr(\" %d VPointer::scaled_iv: Op_MulI PASSED, setting _scale = %d\", n->_idx, scale);\n+    print_depth(); tty->print(\"  \\\\ %d VPointer::scaled_iv: in(2) is iv: \", n->in(2)->_idx); n->in(2)->dump();\n+    print_depth(); tty->print(\"  \\\\ %d VPointer::scaled_iv: in(1) is Con: \", n->in(1)->_idx); n->in(1)->dump();\n+  }\n+}\n+\n+void VPointer::Tracer::scaled_iv_6(Node* n, int scale) {\n+  if (_is_trace_alignment) {\n+    print_depth(); tty->print_cr(\" %d VPointer::scaled_iv: Op_LShiftI PASSED, setting _scale = %d\", n->_idx, scale);\n+    print_depth(); tty->print(\"  \\\\ %d VPointer::scaled_iv: in(1) is iv: \", n->in(1)->_idx); n->in(1)->dump();\n+    print_depth(); tty->print(\"  \\\\ %d VPointer::scaled_iv: in(2) is Con: \", n->in(2)->_idx); n->in(2)->dump();\n+  }\n+}\n+\n+void VPointer::Tracer::scaled_iv_7(Node* n) {\n+  if (_is_trace_alignment) {\n+    print_depth(); tty->print_cr(\" %d VPointer::scaled_iv: Op_ConvI2L PASSED\", n->_idx);\n+    print_depth(); tty->print_cr(\"  \\\\ VPointer::scaled_iv: in(1) %d is scaled_iv_plus_offset: \", n->in(1)->_idx);\n+    inc_depth(); inc_depth();\n+    print_depth(); n->in(1)->dump();\n+    dec_depth(); dec_depth();\n+  }\n+}\n+\n+void VPointer::Tracer::scaled_iv_8(Node* n, VPointer* tmp) {\n+  if (_is_trace_alignment) {\n+    print_depth(); tty->print(\" %d VPointer::scaled_iv: Op_LShiftL, creating tmp VPointer: \", n->_idx); tmp->print();\n+  }\n+}\n+\n+void VPointer::Tracer::scaled_iv_9(Node* n, int scale, int offset, Node* invar) {\n+  if (_is_trace_alignment) {\n+    print_depth(); tty->print_cr(\" %d VPointer::scaled_iv: Op_LShiftL PASSED, setting _scale = %d, _offset = %d\", n->_idx, scale, offset);\n+    print_depth(); tty->print_cr(\"  \\\\ VPointer::scaled_iv: in(1) [%d] is scaled_iv_plus_offset, in(2) [%d] used to scale: _scale = %d, _offset = %d\",\n+    n->in(1)->_idx, n->in(2)->_idx, scale, offset);\n+    if (invar != nullptr) {\n+      print_depth(); tty->print_cr(\"  \\\\ VPointer::scaled_iv: scaled invariant: [%d]\", invar->_idx);\n+    }\n+    inc_depth(); inc_depth();\n+    print_depth(); n->in(1)->dump();\n+    print_depth(); n->in(2)->dump();\n+    if (invar != nullptr) {\n+      print_depth(); invar->dump();\n+    }\n+    dec_depth(); dec_depth();\n+  }\n+}\n+\n+void VPointer::Tracer::scaled_iv_10(Node* n) {\n+  if (_is_trace_alignment) {\n+    print_depth(); tty->print_cr(\" %d VPointer::scaled_iv: FAILED\", n->_idx);\n+  }\n+}\n+\n+void VPointer::Tracer::offset_plus_k_1(Node* n) {\n+  if (_is_trace_alignment) {\n+    print_depth(); tty->print(\" %d VPointer::offset_plus_k: testing node: \", n->_idx); n->dump();\n+  }\n+}\n+\n+void VPointer::Tracer::offset_plus_k_2(Node* n, int _offset) {\n+  if (_is_trace_alignment) {\n+    print_depth(); tty->print_cr(\" %d VPointer::offset_plus_k: Op_ConI PASSED, setting _offset = %d\", n->_idx, _offset);\n+  }\n+}\n+\n+void VPointer::Tracer::offset_plus_k_3(Node* n, int _offset) {\n+  if (_is_trace_alignment) {\n+    print_depth(); tty->print_cr(\" %d VPointer::offset_plus_k: Op_ConL PASSED, setting _offset = %d\", n->_idx, _offset);\n+  }\n+}\n+\n+void VPointer::Tracer::offset_plus_k_4(Node* n) {\n+  if (_is_trace_alignment) {\n+    print_depth(); tty->print_cr(\" %d VPointer::offset_plus_k: FAILED\", n->_idx);\n+    print_depth(); tty->print_cr(\"  \\\\ \" JLONG_FORMAT \" VPointer::offset_plus_k: Op_ConL FAILED, k is too big\", n->get_long());\n+  }\n+}\n+\n+void VPointer::Tracer::offset_plus_k_5(Node* n, Node* _invar) {\n+  if (_is_trace_alignment) {\n+    print_depth(); tty->print_cr(\" %d VPointer::offset_plus_k: FAILED since another invariant has been detected before\", n->_idx);\n+    print_depth(); tty->print(\"  \\\\ %d VPointer::offset_plus_k: _invar is not null: \", _invar->_idx); _invar->dump();\n+  }\n+}\n+\n+void VPointer::Tracer::offset_plus_k_6(Node* n, Node* _invar, bool _negate_invar, int _offset) {\n+  if (_is_trace_alignment) {\n+    print_depth(); tty->print_cr(\" %d VPointer::offset_plus_k: Op_AddI PASSED, setting _debug_negate_invar = %d, _invar = %d, _offset = %d\",\n+    n->_idx, _negate_invar, _invar->_idx, _offset);\n+    print_depth(); tty->print(\"  \\\\ %d VPointer::offset_plus_k: in(2) is Con: \", n->in(2)->_idx); n->in(2)->dump();\n+    print_depth(); tty->print(\"  \\\\ %d VPointer::offset_plus_k: in(1) is invariant: \", _invar->_idx); _invar->dump();\n+  }\n+}\n+\n+void VPointer::Tracer::offset_plus_k_7(Node* n, Node* _invar, bool _negate_invar, int _offset) {\n+  if (_is_trace_alignment) {\n+    print_depth(); tty->print_cr(\" %d VPointer::offset_plus_k: Op_AddI PASSED, setting _debug_negate_invar = %d, _invar = %d, _offset = %d\",\n+    n->_idx, _negate_invar, _invar->_idx, _offset);\n+    print_depth(); tty->print(\"  \\\\ %d VPointer::offset_plus_k: in(1) is Con: \", n->in(1)->_idx); n->in(1)->dump();\n+    print_depth(); tty->print(\"  \\\\ %d VPointer::offset_plus_k: in(2) is invariant: \", _invar->_idx); _invar->dump();\n+  }\n+}\n+\n+void VPointer::Tracer::offset_plus_k_8(Node* n, Node* _invar, bool _negate_invar, int _offset) {\n+  if (_is_trace_alignment) {\n+    print_depth(); tty->print_cr(\" %d VPointer::offset_plus_k: Op_SubI is PASSED, setting _debug_negate_invar = %d, _invar = %d, _offset = %d\",\n+    n->_idx, _negate_invar, _invar->_idx, _offset);\n+    print_depth(); tty->print(\"  \\\\ %d VPointer::offset_plus_k: in(2) is Con: \", n->in(2)->_idx); n->in(2)->dump();\n+    print_depth(); tty->print(\"  \\\\ %d VPointer::offset_plus_k: in(1) is invariant: \", _invar->_idx); _invar->dump();\n+  }\n+}\n+\n+void VPointer::Tracer::offset_plus_k_9(Node* n, Node* _invar, bool _negate_invar, int _offset) {\n+  if (_is_trace_alignment) {\n+    print_depth(); tty->print_cr(\" %d VPointer::offset_plus_k: Op_SubI PASSED, setting _debug_negate_invar = %d, _invar = %d, _offset = %d\", n->_idx, _negate_invar, _invar->_idx, _offset);\n+    print_depth(); tty->print(\"  \\\\ %d VPointer::offset_plus_k: in(1) is Con: \", n->in(1)->_idx); n->in(1)->dump();\n+    print_depth(); tty->print(\"  \\\\ %d VPointer::offset_plus_k: in(2) is invariant: \", _invar->_idx); _invar->dump();\n+  }\n+}\n+\n+void VPointer::Tracer::offset_plus_k_10(Node* n, Node* _invar, bool _negate_invar, int _offset) {\n+  if (_is_trace_alignment) {\n+    print_depth(); tty->print_cr(\" %d VPointer::offset_plus_k: PASSED, setting _debug_negate_invar = %d, _invar = %d, _offset = %d\", n->_idx, _negate_invar, _invar->_idx, _offset);\n+    print_depth(); tty->print_cr(\"  \\\\ %d VPointer::offset_plus_k: is invariant\", n->_idx);\n+  }\n+}\n+\n+void VPointer::Tracer::offset_plus_k_11(Node* n) {\n+  if (_is_trace_alignment) {\n+    print_depth(); tty->print_cr(\" %d VPointer::offset_plus_k: FAILED\", n->_idx);\n+  }\n+}\n+\n+#endif\n","filename":"src\/hotspot\/share\/opto\/vectorization.cpp","additions":698,"deletions":0,"binary":false,"changes":698,"status":"added"},{"patch":"@@ -0,0 +1,244 @@\n+\/*\n+ * Copyright (c) 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2023, Arm Limited. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_OPTO_VECTORIZATION_HPP\n+#define SHARE_OPTO_VECTORIZATION_HPP\n+\n+#include \"opto\/node.hpp\"\n+#include \"opto\/loopnode.hpp\"\n+\n+\/\/ ----------------------------------VPointer----------------------------------\n+\/\/ Information about an address for dependence checking and vector alignment\n+class VPointer : public ArenaObj {\n+ protected:\n+  MemNode*        _mem;      \/\/ My memory reference node\n+  PhaseIdealLoop* _phase;    \/\/ PhaseIdealLoop handle\n+  IdealLoopTree*  _lpt;      \/\/ Current IdealLoopTree\n+  PhiNode*        _iv;       \/\/ The loop induction variable\n+\n+  Node* _base;               \/\/ null if unsafe nonheap reference\n+  Node* _adr;                \/\/ address pointer\n+  int   _scale;              \/\/ multiplier for iv (in bytes), 0 if no loop iv\n+  int   _offset;             \/\/ constant offset (in bytes)\n+\n+  Node* _invar;              \/\/ invariant offset (in bytes), null if none\n+#ifdef ASSERT\n+  Node* _debug_invar;\n+  bool  _debug_negate_invar; \/\/ if true then use: (0 - _invar)\n+  Node* _debug_invar_scale;  \/\/ multiplier for invariant\n+#endif\n+\n+  Node_Stack* _nstack;       \/\/ stack used to record a vpointer trace of variants\n+  bool        _analyze_only; \/\/ Used in loop unrolling only for vpointer trace\n+  uint        _stack_idx;    \/\/ Used in loop unrolling only for vpointer trace\n+\n+  PhaseIdealLoop* phase() const { return _phase; }\n+  IdealLoopTree*  lpt() const   { return _lpt; }\n+  PhiNode*        iv() const    { return _iv; }\n+\n+  bool is_loop_member(Node* n) const;\n+  bool invariant(Node* n) const;\n+\n+  \/\/ Match: k*iv + offset\n+  bool scaled_iv_plus_offset(Node* n);\n+  \/\/ Match: k*iv where k is a constant that's not zero\n+  bool scaled_iv(Node* n);\n+  \/\/ Match: offset is (k [+\/- invariant])\n+  bool offset_plus_k(Node* n, bool negate = false);\n+\n+ public:\n+  enum CMP {\n+    Less          = 1,\n+    Greater       = 2,\n+    Equal         = 4,\n+    NotEqual      = (Less | Greater),\n+    NotComparable = (Less | Greater | Equal)\n+  };\n+\n+  VPointer(MemNode* mem, PhaseIdealLoop* phase, IdealLoopTree* lpt,\n+            Node_Stack* nstack, bool analyze_only);\n+  \/\/ Following is used to create a temporary object during\n+  \/\/ the pattern match of an address expression.\n+  VPointer(VPointer* p);\n+\n+  bool valid()  { return _adr != nullptr; }\n+  bool has_iv() { return _scale != 0; }\n+\n+  Node* base()             { return _base; }\n+  Node* adr()              { return _adr; }\n+  MemNode* mem()           { return _mem; }\n+  int   scale_in_bytes()   { return _scale; }\n+  Node* invar()            { return _invar; }\n+  int   offset_in_bytes()  { return _offset; }\n+  int   memory_size()      { return _mem->memory_size(); }\n+  Node_Stack* node_stack() { return _nstack; }\n+\n+  \/\/ Comparable?\n+  bool invar_equals(VPointer& q) {\n+    assert(_debug_invar == NodeSentinel || q._debug_invar == NodeSentinel ||\n+           (_invar == q._invar) == (_debug_invar == q._debug_invar &&\n+                                    _debug_invar_scale == q._debug_invar_scale &&\n+                                    _debug_negate_invar == q._debug_negate_invar), \"\");\n+    return _invar == q._invar;\n+  }\n+\n+  int cmp(VPointer& q) {\n+    if (valid() && q.valid() &&\n+        (_adr == q._adr || (_base == _adr && q._base == q._adr)) &&\n+        _scale == q._scale   && invar_equals(q)) {\n+      bool overlap = q._offset <   _offset +   memory_size() &&\n+                       _offset < q._offset + q.memory_size();\n+      return overlap ? Equal : (_offset < q._offset ? Less : Greater);\n+    } else {\n+      return NotComparable;\n+    }\n+  }\n+\n+  bool not_equal(VPointer& q)     { return not_equal(cmp(q)); }\n+  bool equal(VPointer& q)         { return equal(cmp(q)); }\n+  bool comparable(VPointer& q)    { return comparable(cmp(q)); }\n+  static bool not_equal(int cmp)  { return cmp <= NotEqual; }\n+  static bool equal(int cmp)      { return cmp == Equal; }\n+  static bool comparable(int cmp) { return cmp < NotComparable; }\n+\n+  void print();\n+\n+#ifndef PRODUCT\n+  class Tracer {\n+    friend class VPointer;\n+    bool _is_trace_alignment;\n+    static int _depth;\n+    int _depth_save;\n+    void print_depth() const;\n+    int  depth() const    { return _depth; }\n+    void set_depth(int d) { _depth = d; }\n+    void inc_depth()      { _depth++; }\n+    void dec_depth()      { if (_depth > 0) _depth--; }\n+    void store_depth()    { _depth_save = _depth; }\n+    void restore_depth()  { _depth = _depth_save; }\n+\n+    class Depth {\n+      friend class VPointer;\n+      Depth()      { ++_depth; }\n+      Depth(int x) { _depth = 0; }\n+      ~Depth()     { if (_depth > 0) --_depth; }\n+    };\n+    Tracer(bool is_trace_alignment) : _is_trace_alignment(is_trace_alignment) {}\n+\n+    \/\/ tracing functions\n+    void ctor_1(Node* mem);\n+    void ctor_2(Node* adr);\n+    void ctor_3(Node* adr, int i);\n+    void ctor_4(Node* adr, int i);\n+    void ctor_5(Node* adr, Node* base,  int i);\n+    void ctor_6(Node* mem);\n+\n+    void scaled_iv_plus_offset_1(Node* n);\n+    void scaled_iv_plus_offset_2(Node* n);\n+    void scaled_iv_plus_offset_3(Node* n);\n+    void scaled_iv_plus_offset_4(Node* n);\n+    void scaled_iv_plus_offset_5(Node* n);\n+    void scaled_iv_plus_offset_6(Node* n);\n+    void scaled_iv_plus_offset_7(Node* n);\n+    void scaled_iv_plus_offset_8(Node* n);\n+\n+    void scaled_iv_1(Node* n);\n+    void scaled_iv_2(Node* n, int scale);\n+    void scaled_iv_3(Node* n, int scale);\n+    void scaled_iv_4(Node* n, int scale);\n+    void scaled_iv_5(Node* n, int scale);\n+    void scaled_iv_6(Node* n, int scale);\n+    void scaled_iv_7(Node* n);\n+    void scaled_iv_8(Node* n, VPointer* tmp);\n+    void scaled_iv_9(Node* n, int _scale, int _offset, Node* _invar);\n+    void scaled_iv_10(Node* n);\n+\n+    void offset_plus_k_1(Node* n);\n+    void offset_plus_k_2(Node* n, int _offset);\n+    void offset_plus_k_3(Node* n, int _offset);\n+    void offset_plus_k_4(Node* n);\n+    void offset_plus_k_5(Node* n, Node* _invar);\n+    void offset_plus_k_6(Node* n, Node* _invar, bool _negate_invar, int _offset);\n+    void offset_plus_k_7(Node* n, Node* _invar, bool _negate_invar, int _offset);\n+    void offset_plus_k_8(Node* n, Node* _invar, bool _negate_invar, int _offset);\n+    void offset_plus_k_9(Node* n, Node* _invar, bool _negate_invar, int _offset);\n+    void offset_plus_k_10(Node* n, Node* _invar, bool _negate_invar, int _offset);\n+    void offset_plus_k_11(Node* n);\n+  } _tracer; \/\/ Tracer\n+#endif\n+\n+  Node* maybe_negate_invar(bool negate, Node* invar);\n+\n+  void maybe_add_to_invar(Node* new_invar, bool negate);\n+\n+  Node* register_if_new(Node* n) const;\n+};\n+\n+\/\/ ---------------------------VectorElementSizeStats---------------------------\n+\/\/ Vector element size statistics for loop vectorization with vector masks\n+class VectorElementSizeStats {\n+ private:\n+  static const int NO_SIZE = -1;\n+  static const int MIXED_SIZE = -2;\n+  int* _stats;\n+\n+ public:\n+  VectorElementSizeStats(Arena* a) : _stats(NEW_ARENA_ARRAY(a, int, 4)) {\n+    clear();\n+  }\n+\n+  void clear() { memset(_stats, 0, sizeof(int) * 4); }\n+\n+  void record_size(int size) {\n+    assert(1 <= size && size <= 8 && is_power_of_2(size), \"Illegal size\");\n+    _stats[exact_log2(size)]++;\n+  }\n+\n+  int count_size(int size) {\n+    assert(1 <= size && size <= 8 && is_power_of_2(size), \"Illegal size\");\n+    return _stats[exact_log2(size)];\n+  }\n+\n+  int smallest_size() {\n+    for (int i = 0; i <= 3; i++) {\n+      if (_stats[i] > 0) return (1 << i);\n+    }\n+    return NO_SIZE;\n+  }\n+\n+  int largest_size() {\n+    for (int i = 3; i >= 0; i--) {\n+      if (_stats[i] > 0) return (1 << i);\n+    }\n+    return NO_SIZE;\n+  }\n+\n+  int unique_size() {\n+    int small = smallest_size();\n+    int large = largest_size();\n+    return (small == large) ? small : MIXED_SIZE;\n+  }\n+};\n+\n+#endif \/\/ SHARE_OPTO_VECTORIZATION_HPP\n","filename":"src\/hotspot\/share\/opto\/vectorization.hpp","additions":244,"deletions":0,"binary":false,"changes":244,"status":"added"}]}