{"files":[{"patch":"@@ -145,2 +145,1 @@\n-        $d\/cpu\/$(HOTSPOT_TARGET_CPU_ARCH)\/$(HOTSPOT_TARGET_CPU_ARCH)_neon.ad \\\n-        $d\/cpu\/$(HOTSPOT_TARGET_CPU_ARCH)\/$(HOTSPOT_TARGET_CPU_ARCH)_sve.ad \\\n+        $d\/cpu\/$(HOTSPOT_TARGET_CPU_ARCH)\/$(HOTSPOT_TARGET_CPU_ARCH)_vector.ad \\\n","filename":"make\/hotspot\/gensrc\/GensrcAdlc.gmk","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -182,4 +182,0 @@\n-  reg_def V0_L ( SOC, SOC, Op_RegF, 0, v0->as_VMReg()->next(4) );\n-  reg_def V0_M ( SOC, SOC, Op_RegF, 0, v0->as_VMReg()->next(5) );\n-  reg_def V0_N ( SOC, SOC, Op_RegF, 0, v0->as_VMReg()->next(6) );\n-  reg_def V0_O ( SOC, SOC, Op_RegF, 0, v0->as_VMReg()->next(7) );\n@@ -191,4 +187,0 @@\n-  reg_def V1_L ( SOC, SOC, Op_RegF, 1, v1->as_VMReg()->next(4) );\n-  reg_def V1_M ( SOC, SOC, Op_RegF, 1, v1->as_VMReg()->next(5) );\n-  reg_def V1_N ( SOC, SOC, Op_RegF, 1, v1->as_VMReg()->next(6) );\n-  reg_def V1_O ( SOC, SOC, Op_RegF, 1, v1->as_VMReg()->next(7) );\n@@ -200,4 +192,0 @@\n-  reg_def V2_L ( SOC, SOC, Op_RegF, 2, v2->as_VMReg()->next(4) );\n-  reg_def V2_M ( SOC, SOC, Op_RegF, 2, v2->as_VMReg()->next(5) );\n-  reg_def V2_N ( SOC, SOC, Op_RegF, 2, v2->as_VMReg()->next(6) );\n-  reg_def V2_O ( SOC, SOC, Op_RegF, 2, v2->as_VMReg()->next(7) );\n@@ -209,4 +197,0 @@\n-  reg_def V3_L ( SOC, SOC, Op_RegF, 3, v3->as_VMReg()->next(4) );\n-  reg_def V3_M ( SOC, SOC, Op_RegF, 3, v3->as_VMReg()->next(5) );\n-  reg_def V3_N ( SOC, SOC, Op_RegF, 3, v3->as_VMReg()->next(6) );\n-  reg_def V3_O ( SOC, SOC, Op_RegF, 3, v3->as_VMReg()->next(7) );\n@@ -218,4 +202,0 @@\n-  reg_def V4_L ( SOC, SOC, Op_RegF, 4, v4->as_VMReg()->next(4) );\n-  reg_def V4_M ( SOC, SOC, Op_RegF, 4, v4->as_VMReg()->next(5) );\n-  reg_def V4_N ( SOC, SOC, Op_RegF, 4, v4->as_VMReg()->next(6) );\n-  reg_def V4_O ( SOC, SOC, Op_RegF, 4, v4->as_VMReg()->next(7) );\n@@ -227,4 +207,0 @@\n-  reg_def V5_L ( SOC, SOC, Op_RegF, 5, v5->as_VMReg()->next(4) );\n-  reg_def V5_M ( SOC, SOC, Op_RegF, 5, v5->as_VMReg()->next(5) );\n-  reg_def V5_N ( SOC, SOC, Op_RegF, 5, v5->as_VMReg()->next(6) );\n-  reg_def V5_O ( SOC, SOC, Op_RegF, 5, v5->as_VMReg()->next(7) );\n@@ -236,4 +212,0 @@\n-  reg_def V6_L ( SOC, SOC, Op_RegF, 6, v6->as_VMReg()->next(4) );\n-  reg_def V6_M ( SOC, SOC, Op_RegF, 6, v6->as_VMReg()->next(5) );\n-  reg_def V6_N ( SOC, SOC, Op_RegF, 6, v6->as_VMReg()->next(6) );\n-  reg_def V6_O ( SOC, SOC, Op_RegF, 6, v6->as_VMReg()->next(7) );\n@@ -245,4 +217,0 @@\n-  reg_def V7_L ( SOC, SOC, Op_RegF, 7, v7->as_VMReg()->next(4) );\n-  reg_def V7_M ( SOC, SOC, Op_RegF, 7, v7->as_VMReg()->next(5) );\n-  reg_def V7_N ( SOC, SOC, Op_RegF, 7, v7->as_VMReg()->next(6) );\n-  reg_def V7_O ( SOC, SOC, Op_RegF, 7, v7->as_VMReg()->next(7) );\n@@ -254,4 +222,0 @@\n-  reg_def V8_L ( SOC, SOC, Op_RegF, 8, v8->as_VMReg()->next(4) );\n-  reg_def V8_M ( SOC, SOC, Op_RegF, 8, v8->as_VMReg()->next(5) );\n-  reg_def V8_N ( SOC, SOC, Op_RegF, 8, v8->as_VMReg()->next(6) );\n-  reg_def V8_O ( SOC, SOC, Op_RegF, 8, v8->as_VMReg()->next(7) );\n@@ -263,4 +227,0 @@\n-  reg_def V9_L ( SOC, SOC, Op_RegF, 9, v9->as_VMReg()->next(4) );\n-  reg_def V9_M ( SOC, SOC, Op_RegF, 9, v9->as_VMReg()->next(5) );\n-  reg_def V9_N ( SOC, SOC, Op_RegF, 9, v9->as_VMReg()->next(6) );\n-  reg_def V9_O ( SOC, SOC, Op_RegF, 9, v9->as_VMReg()->next(7) );\n@@ -272,4 +232,0 @@\n-  reg_def V10_L ( SOC, SOC, Op_RegF, 10, v10->as_VMReg()->next(4) );\n-  reg_def V10_M ( SOC, SOC, Op_RegF, 10, v10->as_VMReg()->next(5) );\n-  reg_def V10_N ( SOC, SOC, Op_RegF, 10, v10->as_VMReg()->next(6) );\n-  reg_def V10_O ( SOC, SOC, Op_RegF, 10, v10->as_VMReg()->next(7) );\n@@ -281,4 +237,0 @@\n-  reg_def V11_L ( SOC, SOC, Op_RegF, 11, v11->as_VMReg()->next(4) );\n-  reg_def V11_M ( SOC, SOC, Op_RegF, 11, v11->as_VMReg()->next(5) );\n-  reg_def V11_N ( SOC, SOC, Op_RegF, 11, v11->as_VMReg()->next(6) );\n-  reg_def V11_O ( SOC, SOC, Op_RegF, 11, v11->as_VMReg()->next(7) );\n@@ -290,4 +242,0 @@\n-  reg_def V12_L ( SOC, SOC, Op_RegF, 12, v12->as_VMReg()->next(4) );\n-  reg_def V12_M ( SOC, SOC, Op_RegF, 12, v12->as_VMReg()->next(5) );\n-  reg_def V12_N ( SOC, SOC, Op_RegF, 12, v12->as_VMReg()->next(6) );\n-  reg_def V12_O ( SOC, SOC, Op_RegF, 12, v12->as_VMReg()->next(7) );\n@@ -299,4 +247,0 @@\n-  reg_def V13_L ( SOC, SOC, Op_RegF, 13, v13->as_VMReg()->next(4) );\n-  reg_def V13_M ( SOC, SOC, Op_RegF, 13, v13->as_VMReg()->next(5) );\n-  reg_def V13_N ( SOC, SOC, Op_RegF, 13, v13->as_VMReg()->next(6) );\n-  reg_def V13_O ( SOC, SOC, Op_RegF, 13, v13->as_VMReg()->next(7) );\n@@ -308,4 +252,0 @@\n-  reg_def V14_L ( SOC, SOC, Op_RegF, 14, v14->as_VMReg()->next(4) );\n-  reg_def V14_M ( SOC, SOC, Op_RegF, 14, v14->as_VMReg()->next(5) );\n-  reg_def V14_N ( SOC, SOC, Op_RegF, 14, v14->as_VMReg()->next(6) );\n-  reg_def V14_O ( SOC, SOC, Op_RegF, 14, v14->as_VMReg()->next(7) );\n@@ -317,4 +257,0 @@\n-  reg_def V15_L ( SOC, SOC, Op_RegF, 15, v15->as_VMReg()->next(4) );\n-  reg_def V15_M ( SOC, SOC, Op_RegF, 15, v15->as_VMReg()->next(5) );\n-  reg_def V15_N ( SOC, SOC, Op_RegF, 15, v15->as_VMReg()->next(6) );\n-  reg_def V15_O ( SOC, SOC, Op_RegF, 15, v15->as_VMReg()->next(7) );\n@@ -326,4 +262,0 @@\n-  reg_def V16_L ( SOC, SOC, Op_RegF, 16, v16->as_VMReg()->next(4) );\n-  reg_def V16_M ( SOC, SOC, Op_RegF, 16, v16->as_VMReg()->next(5) );\n-  reg_def V16_N ( SOC, SOC, Op_RegF, 16, v16->as_VMReg()->next(6) );\n-  reg_def V16_O ( SOC, SOC, Op_RegF, 16, v16->as_VMReg()->next(7) );\n@@ -335,4 +267,0 @@\n-  reg_def V17_L ( SOC, SOC, Op_RegF, 17, v17->as_VMReg()->next(4) );\n-  reg_def V17_M ( SOC, SOC, Op_RegF, 17, v17->as_VMReg()->next(5) );\n-  reg_def V17_N ( SOC, SOC, Op_RegF, 17, v17->as_VMReg()->next(6) );\n-  reg_def V17_O ( SOC, SOC, Op_RegF, 17, v17->as_VMReg()->next(7) );\n@@ -344,4 +272,0 @@\n-  reg_def V18_L ( SOC, SOC, Op_RegF, 18, v18->as_VMReg()->next(4) );\n-  reg_def V18_M ( SOC, SOC, Op_RegF, 18, v18->as_VMReg()->next(5) );\n-  reg_def V18_N ( SOC, SOC, Op_RegF, 18, v18->as_VMReg()->next(6) );\n-  reg_def V18_O ( SOC, SOC, Op_RegF, 18, v18->as_VMReg()->next(7) );\n@@ -353,4 +277,0 @@\n-  reg_def V19_L ( SOC, SOC, Op_RegF, 19, v19->as_VMReg()->next(4) );\n-  reg_def V19_M ( SOC, SOC, Op_RegF, 19, v19->as_VMReg()->next(5) );\n-  reg_def V19_N ( SOC, SOC, Op_RegF, 19, v19->as_VMReg()->next(6) );\n-  reg_def V19_O ( SOC, SOC, Op_RegF, 19, v19->as_VMReg()->next(7) );\n@@ -362,4 +282,0 @@\n-  reg_def V20_L ( SOC, SOC, Op_RegF, 20, v20->as_VMReg()->next(4) );\n-  reg_def V20_M ( SOC, SOC, Op_RegF, 20, v20->as_VMReg()->next(5) );\n-  reg_def V20_N ( SOC, SOC, Op_RegF, 20, v20->as_VMReg()->next(6) );\n-  reg_def V20_O ( SOC, SOC, Op_RegF, 20, v20->as_VMReg()->next(7) );\n@@ -371,4 +287,0 @@\n-  reg_def V21_L ( SOC, SOC, Op_RegF, 21, v21->as_VMReg()->next(4) );\n-  reg_def V21_M ( SOC, SOC, Op_RegF, 21, v21->as_VMReg()->next(5) );\n-  reg_def V21_N ( SOC, SOC, Op_RegF, 21, v21->as_VMReg()->next(6) );\n-  reg_def V21_O ( SOC, SOC, Op_RegF, 21, v21->as_VMReg()->next(7) );\n@@ -380,4 +292,0 @@\n-  reg_def V22_L ( SOC, SOC, Op_RegF, 22, v22->as_VMReg()->next(4) );\n-  reg_def V22_M ( SOC, SOC, Op_RegF, 22, v22->as_VMReg()->next(5) );\n-  reg_def V22_N ( SOC, SOC, Op_RegF, 22, v22->as_VMReg()->next(6) );\n-  reg_def V22_O ( SOC, SOC, Op_RegF, 22, v22->as_VMReg()->next(7) );\n@@ -389,4 +297,0 @@\n-  reg_def V23_L ( SOC, SOC, Op_RegF, 23, v23->as_VMReg()->next(4) );\n-  reg_def V23_M ( SOC, SOC, Op_RegF, 23, v23->as_VMReg()->next(5) );\n-  reg_def V23_N ( SOC, SOC, Op_RegF, 23, v23->as_VMReg()->next(6) );\n-  reg_def V23_O ( SOC, SOC, Op_RegF, 23, v23->as_VMReg()->next(7) );\n@@ -398,4 +302,0 @@\n-  reg_def V24_L ( SOC, SOC, Op_RegF, 24, v24->as_VMReg()->next(4) );\n-  reg_def V24_M ( SOC, SOC, Op_RegF, 24, v24->as_VMReg()->next(5) );\n-  reg_def V24_N ( SOC, SOC, Op_RegF, 24, v24->as_VMReg()->next(6) );\n-  reg_def V24_O ( SOC, SOC, Op_RegF, 24, v24->as_VMReg()->next(7) );\n@@ -407,4 +307,0 @@\n-  reg_def V25_L ( SOC, SOC, Op_RegF, 25, v25->as_VMReg()->next(4) );\n-  reg_def V25_M ( SOC, SOC, Op_RegF, 25, v25->as_VMReg()->next(5) );\n-  reg_def V25_N ( SOC, SOC, Op_RegF, 25, v25->as_VMReg()->next(6) );\n-  reg_def V25_O ( SOC, SOC, Op_RegF, 25, v25->as_VMReg()->next(7) );\n@@ -416,4 +312,0 @@\n-  reg_def V26_L ( SOC, SOC, Op_RegF, 26, v26->as_VMReg()->next(4) );\n-  reg_def V26_M ( SOC, SOC, Op_RegF, 26, v26->as_VMReg()->next(5) );\n-  reg_def V26_N ( SOC, SOC, Op_RegF, 26, v26->as_VMReg()->next(6) );\n-  reg_def V26_O ( SOC, SOC, Op_RegF, 26, v26->as_VMReg()->next(7) );\n@@ -425,4 +317,0 @@\n-  reg_def V27_L ( SOC, SOC, Op_RegF, 27, v27->as_VMReg()->next(4) );\n-  reg_def V27_M ( SOC, SOC, Op_RegF, 27, v27->as_VMReg()->next(5) );\n-  reg_def V27_N ( SOC, SOC, Op_RegF, 27, v27->as_VMReg()->next(6) );\n-  reg_def V27_O ( SOC, SOC, Op_RegF, 27, v27->as_VMReg()->next(7) );\n@@ -434,4 +322,0 @@\n-  reg_def V28_L ( SOC, SOC, Op_RegF, 28, v28->as_VMReg()->next(4) );\n-  reg_def V28_M ( SOC, SOC, Op_RegF, 28, v28->as_VMReg()->next(5) );\n-  reg_def V28_N ( SOC, SOC, Op_RegF, 28, v28->as_VMReg()->next(6) );\n-  reg_def V28_O ( SOC, SOC, Op_RegF, 28, v28->as_VMReg()->next(7) );\n@@ -443,4 +327,0 @@\n-  reg_def V29_L ( SOC, SOC, Op_RegF, 29, v29->as_VMReg()->next(4) );\n-  reg_def V29_M ( SOC, SOC, Op_RegF, 29, v29->as_VMReg()->next(5) );\n-  reg_def V29_N ( SOC, SOC, Op_RegF, 29, v29->as_VMReg()->next(6) );\n-  reg_def V29_O ( SOC, SOC, Op_RegF, 29, v29->as_VMReg()->next(7) );\n@@ -452,4 +332,0 @@\n-  reg_def V30_L ( SOC, SOC, Op_RegF, 30, v30->as_VMReg()->next(4) );\n-  reg_def V30_M ( SOC, SOC, Op_RegF, 30, v30->as_VMReg()->next(5) );\n-  reg_def V30_N ( SOC, SOC, Op_RegF, 30, v30->as_VMReg()->next(6) );\n-  reg_def V30_O ( SOC, SOC, Op_RegF, 30, v30->as_VMReg()->next(7) );\n@@ -461,5 +337,0 @@\n-  reg_def V31_L ( SOC, SOC, Op_RegF, 31, v31->as_VMReg()->next(4) );\n-  reg_def V31_M ( SOC, SOC, Op_RegF, 31, v31->as_VMReg()->next(5) );\n-  reg_def V31_N ( SOC, SOC, Op_RegF, 31, v31->as_VMReg()->next(6) );\n-  reg_def V31_O ( SOC, SOC, Op_RegF, 31, v31->as_VMReg()->next(7) );\n-\n@@ -553,16 +424,16 @@\n-    V16, V16_H, V16_J, V16_K, V16_L, V16_M, V16_N, V16_O,\n-    V17, V17_H, V17_J, V17_K, V17_L, V17_M, V17_N, V17_O,\n-    V18, V18_H, V18_J, V18_K, V18_L, V18_M, V18_N, V18_O,\n-    V19, V19_H, V19_J, V19_K, V19_L, V19_M, V19_N, V19_O,\n-    V20, V20_H, V20_J, V20_K, V20_L, V20_M, V20_N, V20_O,\n-    V21, V21_H, V21_J, V21_K, V21_L, V21_M, V21_N, V21_O,\n-    V22, V22_H, V22_J, V22_K, V22_L, V22_M, V22_N, V22_O,\n-    V23, V23_H, V23_J, V23_K, V23_L, V23_M, V23_N, V23_O,\n-    V24, V24_H, V24_J, V24_K, V24_L, V24_M, V24_N, V24_O,\n-    V25, V25_H, V25_J, V25_K, V25_L, V25_M, V25_N, V25_O,\n-    V26, V26_H, V26_J, V26_K, V26_L, V26_M, V26_N, V26_O,\n-    V27, V27_H, V27_J, V27_K, V27_L, V27_M, V27_N, V27_O,\n-    V28, V28_H, V28_J, V28_K, V28_L, V28_M, V28_N, V28_O,\n-    V29, V29_H, V29_J, V29_K, V29_L, V29_M, V29_N, V29_O,\n-    V30, V30_H, V30_J, V30_K, V30_L, V30_M, V30_N, V30_O,\n-    V31, V31_H, V31_J, V31_K, V31_L, V31_M, V31_N, V31_O,\n+    V16, V16_H, V16_J, V16_K,\n+    V17, V17_H, V17_J, V17_K,\n+    V18, V18_H, V18_J, V18_K,\n+    V19, V19_H, V19_J, V19_K,\n+    V20, V20_H, V20_J, V20_K,\n+    V21, V21_H, V21_J, V21_K,\n+    V22, V22_H, V22_J, V22_K,\n+    V23, V23_H, V23_J, V23_K,\n+    V24, V24_H, V24_J, V24_K,\n+    V25, V25_H, V25_J, V25_K,\n+    V26, V26_H, V26_J, V26_K,\n+    V27, V27_H, V27_J, V27_K,\n+    V28, V28_H, V28_J, V28_K,\n+    V29, V29_H, V29_J, V29_K,\n+    V30, V30_H, V30_J, V30_K,\n+    V31, V31_H, V31_J, V31_K,\n@@ -571,8 +442,8 @@\n-    V0, V0_H, V0_J, V0_K, V0_L, V0_M, V0_N, V0_O,\n-    V1, V1_H, V1_J, V1_K, V1_L, V1_M, V1_N, V1_O,\n-    V2, V2_H, V2_J, V2_K, V2_L, V2_M, V2_N, V2_O,\n-    V3, V3_H, V3_J, V3_K, V3_L, V3_M, V3_N, V3_O,\n-    V4, V4_H, V4_J, V4_K, V4_L, V4_M, V4_N, V4_O,\n-    V5, V5_H, V5_J, V5_K, V5_L, V5_M, V5_N, V5_O,\n-    V6, V6_H, V6_J, V6_K, V6_L, V6_M, V6_N, V6_O,\n-    V7, V7_H, V7_J, V7_K, V7_L, V7_M, V7_N, V7_O,\n+    V0, V0_H, V0_J, V0_K,\n+    V1, V1_H, V1_J, V1_K,\n+    V2, V2_H, V2_J, V2_K,\n+    V3, V3_H, V3_J, V3_K,\n+    V4, V4_H, V4_J, V4_K,\n+    V5, V5_H, V5_J, V5_K,\n+    V6, V6_H, V6_J, V6_K,\n+    V7, V7_H, V7_J, V7_K,\n@@ -581,8 +452,8 @@\n-    V8, V8_H, V8_J, V8_K, V8_L, V8_M, V8_N, V8_O,\n-    V9, V9_H, V9_J, V9_K, V9_L, V9_M, V9_N, V9_O,\n-    V10, V10_H, V10_J, V10_K, V10_L, V10_M, V10_N, V10_O,\n-    V11, V11_H, V11_J, V11_K, V11_L, V11_M, V11_N, V11_O,\n-    V12, V12_H, V12_J, V12_K, V12_L, V12_M, V12_N, V12_O,\n-    V13, V13_H, V13_J, V13_K, V13_L, V13_M, V13_N, V13_O,\n-    V14, V14_H, V14_J, V14_K, V14_L, V14_M, V14_N, V14_O,\n-    V15, V15_H, V15_J, V15_K, V15_L, V15_M, V15_N, V15_O,\n+    V8, V8_H, V8_J, V8_K,\n+    V9, V9_H, V9_J, V9_K,\n+    V10, V10_H, V10_J, V10_K,\n+    V11, V11_H, V11_J, V11_K,\n+    V12, V12_H, V12_J, V12_K,\n+    V13, V13_H, V13_J, V13_K,\n+    V14, V14_H, V14_J, V14_K,\n+    V15, V15_H, V15_J, V15_K,\n@@ -903,32 +774,32 @@\n-    V0, V0_H, V0_J, V0_K, V0_L, V0_M, V0_N, V0_O,\n-    V1, V1_H, V1_J, V1_K, V1_L, V1_M, V1_N, V1_O,\n-    V2, V2_H, V2_J, V2_K, V2_L, V2_M, V2_N, V2_O,\n-    V3, V3_H, V3_J, V3_K, V3_L, V3_M, V3_N, V3_O,\n-    V4, V4_H, V4_J, V4_K, V4_L, V4_M, V4_N, V4_O,\n-    V5, V5_H, V5_J, V5_K, V5_L, V5_M, V5_N, V5_O,\n-    V6, V6_H, V6_J, V6_K, V6_L, V6_M, V6_N, V6_O,\n-    V7, V7_H, V7_J, V7_K, V7_L, V7_M, V7_N, V7_O,\n-    V8, V8_H, V8_J, V8_K, V8_L, V8_M, V8_N, V8_O,\n-    V9, V9_H, V9_J, V9_K, V9_L, V9_M, V9_N, V9_O,\n-    V10, V10_H, V10_J, V10_K, V10_L, V10_M, V10_N, V10_O,\n-    V11, V11_H, V11_J, V11_K, V11_L, V11_M, V11_N, V11_O,\n-    V12, V12_H, V12_J, V12_K, V12_L, V12_M, V12_N, V12_O,\n-    V13, V13_H, V13_J, V13_K, V13_L, V13_M, V13_N, V13_O,\n-    V14, V14_H, V14_J, V14_K, V14_L, V14_M, V14_N, V14_O,\n-    V15, V15_H, V15_J, V15_K, V15_L, V15_M, V15_N, V15_O,\n-    V16, V16_H, V16_J, V16_K, V16_L, V16_M, V16_N, V16_O,\n-    V17, V17_H, V17_J, V17_K, V17_L, V17_M, V17_N, V17_O,\n-    V18, V18_H, V18_J, V18_K, V18_L, V18_M, V18_N, V18_O,\n-    V19, V19_H, V19_J, V19_K, V19_L, V19_M, V19_N, V19_O,\n-    V20, V20_H, V20_J, V20_K, V20_L, V20_M, V20_N, V20_O,\n-    V21, V21_H, V21_J, V21_K, V21_L, V21_M, V21_N, V21_O,\n-    V22, V22_H, V22_J, V22_K, V22_L, V22_M, V22_N, V22_O,\n-    V23, V23_H, V23_J, V23_K, V23_L, V23_M, V23_N, V23_O,\n-    V24, V24_H, V24_J, V24_K, V24_L, V24_M, V24_N, V24_O,\n-    V25, V25_H, V25_J, V25_K, V25_L, V25_M, V25_N, V25_O,\n-    V26, V26_H, V26_J, V26_K, V26_L, V26_M, V26_N, V26_O,\n-    V27, V27_H, V27_J, V27_K, V27_L, V27_M, V27_N, V27_O,\n-    V28, V28_H, V28_J, V28_K, V28_L, V28_M, V28_N, V28_O,\n-    V29, V29_H, V29_J, V29_K, V29_L, V29_M, V29_N, V29_O,\n-    V30, V30_H, V30_J, V30_K, V30_L, V30_M, V30_N, V30_O,\n-    V31, V31_H, V31_J, V31_K, V31_L, V31_M, V31_N, V31_O,\n+    V0, V0_H, V0_J, V0_K,\n+    V1, V1_H, V1_J, V1_K,\n+    V2, V2_H, V2_J, V2_K,\n+    V3, V3_H, V3_J, V3_K,\n+    V4, V4_H, V4_J, V4_K,\n+    V5, V5_H, V5_J, V5_K,\n+    V6, V6_H, V6_J, V6_K,\n+    V7, V7_H, V7_J, V7_K,\n+    V8, V8_H, V8_J, V8_K,\n+    V9, V9_H, V9_J, V9_K,\n+    V10, V10_H, V10_J, V10_K,\n+    V11, V11_H, V11_J, V11_K,\n+    V12, V12_H, V12_J, V12_K,\n+    V13, V13_H, V13_J, V13_K,\n+    V14, V14_H, V14_J, V14_K,\n+    V15, V15_H, V15_J, V15_K,\n+    V16, V16_H, V16_J, V16_K,\n+    V17, V17_H, V17_J, V17_K,\n+    V18, V18_H, V18_J, V18_K,\n+    V19, V19_H, V19_J, V19_K,\n+    V20, V20_H, V20_J, V20_K,\n+    V21, V21_H, V21_J, V21_K,\n+    V22, V22_H, V22_J, V22_K,\n+    V23, V23_H, V23_J, V23_K,\n+    V24, V24_H, V24_J, V24_K,\n+    V25, V25_H, V25_J, V25_K,\n+    V26, V26_H, V26_J, V26_K,\n+    V27, V27_H, V27_J, V27_K,\n+    V28, V28_H, V28_J, V28_K,\n+    V29, V29_H, V29_J, V29_K,\n+    V30, V30_H, V30_J, V30_K,\n+    V31, V31_H, V31_J, V31_K,\n@@ -1317,3 +1188,0 @@\n-  \/\/ Assert that the given node is not a variable shift.\n-  bool assert_not_var_shift(const Node* n);\n-\n@@ -1734,6 +1602,0 @@\n-\/\/ Assert that the given node is not a variable shift.\n-bool assert_not_var_shift(const Node* n) {\n-  assert(!n->as_ShiftV()->is_var_shift(), \"illegal variable shift\");\n-  return true;\n-}\n-\n@@ -2418,12 +2280,0 @@\n-    case Op_LoadVectorMasked:\n-    case Op_StoreVectorMasked:\n-    case Op_LoadVectorGatherMasked:\n-    case Op_StoreVectorScatterMasked:\n-    case Op_MaskAll:\n-    case Op_AndVMask:\n-    case Op_OrVMask:\n-    case Op_XorVMask:\n-      if (UseSVE == 0) {\n-        ret_value = false;\n-      }\n-      break;\n@@ -2435,70 +2285,0 @@\n-const bool Matcher::match_rule_supported_superword(int opcode, int vlen, BasicType bt) {\n-  if (UseSVE == 0) {\n-    \/\/ ConvD2I and ConvL2F are not profitable to be vectorized on NEON, because no direct\n-    \/\/ NEON instructions support them. But the match rule support for them is profitable for\n-    \/\/ Vector API intrinsics.\n-    if ((opcode == Op_VectorCastD2X && bt == T_INT) ||\n-        (opcode == Op_VectorCastL2X && bt == T_FLOAT)) {\n-      return false;\n-    }\n-  }\n-  return match_rule_supported_vector(opcode, vlen, bt);\n-}\n-\n-\/\/ Identify extra cases that we might want to provide match rules for vector nodes and\n-\/\/ other intrinsics guarded with vector length (vlen) and element type (bt).\n-const bool Matcher::match_rule_supported_vector(int opcode, int vlen, BasicType bt) {\n-  if (!match_rule_supported(opcode)) {\n-    return false;\n-  }\n-  int bit_size = vlen * type2aelembytes(bt) * 8;\n-  if (UseSVE == 0 && bit_size > 128) {\n-    return false;\n-  }\n-  if (UseSVE > 0) {\n-    return op_sve_supported(opcode, vlen, bt);\n-  } else { \/\/ NEON\n-    \/\/ Special cases\n-    switch (opcode) {\n-    case Op_VectorMaskCmp:\n-      if (vlen < 2 || bit_size < 64) {\n-        return false;\n-      }\n-      break;\n-    case Op_MulAddVS2VI:\n-      if (bit_size < 128) {\n-        return false;\n-      }\n-      break;\n-    case Op_MulVL:\n-    case Op_PopulateIndex:\n-      return false;\n-    case Op_VectorLoadShuffle:\n-    case Op_VectorRearrange:\n-      if (vlen < 4) {\n-        return false;\n-      }\n-      break;\n-    case Op_LoadVectorGather:\n-    case Op_StoreVectorScatter:\n-    case Op_CompressV:\n-    case Op_CompressM:\n-    case Op_ExpandV:\n-    case Op_VectorLongToMask:\n-      return false;\n-    default:\n-      break;\n-    }\n-  }\n-  return vector_size_supported(bt, vlen);\n-}\n-\n-const bool Matcher::match_rule_supported_vector_masked(int opcode, int vlen, BasicType bt) {\n-  \/\/ Only SVE supports masked operations.\n-  if (UseSVE == 0) {\n-    return false;\n-  }\n-  return match_rule_supported(opcode) &&\n-         masked_op_sve_supported(opcode, vlen, bt);\n-}\n-\n@@ -2571,1 +2351,1 @@\n-  if (UseSVE > 0 && 2 <= len && len <= 256) {\n+  if (UseSVE > 0 && 16 < len && len <= 256) {\n@@ -2585,2 +2365,8 @@\n-MachOper* Matcher::pd_specialize_generic_vector_operand(MachOper* original_opnd, uint ideal_reg, bool is_temp) {\n-  ShouldNotReachHere(); \/\/ generic vector operands not supported\n+MachOper* Matcher::pd_specialize_generic_vector_operand(MachOper* generic_opnd, uint ideal_reg, bool is_temp) {\n+  assert(Matcher::is_generic_vector(generic_opnd), \"not generic\");\n+  switch (ideal_reg) {\n+    case Op_VecA: return new vecAOper();\n+    case Op_VecD: return new vecDOper();\n+    case Op_VecX: return new vecXOper();\n+  }\n+  ShouldNotReachHere();\n@@ -2591,1 +2377,0 @@\n-  ShouldNotReachHere();  \/\/ generic vector operands not supported\n@@ -2596,2 +2381,1 @@\n-  ShouldNotReachHere();  \/\/ generic vector operands not supported\n-  return false;\n+  return opnd->opcode() == VREG;\n@@ -3177,1 +2961,1 @@\n-  enc_class aarch64_enc_ldrvH(vecD dst, memory mem) %{\n+  enc_class aarch64_enc_ldrvH(vReg dst, memory mem) %{\n@@ -3183,1 +2967,1 @@\n-  enc_class aarch64_enc_ldrvS(vecD dst, memory mem) %{\n+  enc_class aarch64_enc_ldrvS(vReg dst, memory mem) %{\n@@ -3189,1 +2973,1 @@\n-  enc_class aarch64_enc_ldrvD(vecD dst, memory mem) %{\n+  enc_class aarch64_enc_ldrvD(vReg dst, memory mem) %{\n@@ -3195,1 +2979,1 @@\n-  enc_class aarch64_enc_ldrvQ(vecX dst, memory mem) %{\n+  enc_class aarch64_enc_ldrvQ(vReg dst, memory mem) %{\n@@ -3201,1 +2985,1 @@\n-  enc_class aarch64_enc_strvH(vecD src, memory mem) %{\n+  enc_class aarch64_enc_strvH(vReg src, memory mem) %{\n@@ -3207,1 +2991,1 @@\n-  enc_class aarch64_enc_strvS(vecD src, memory mem) %{\n+  enc_class aarch64_enc_strvS(vReg src, memory mem) %{\n@@ -3213,1 +2997,1 @@\n-  enc_class aarch64_enc_strvD(vecD src, memory mem) %{\n+  enc_class aarch64_enc_strvD(vReg src, memory mem) %{\n@@ -3219,1 +3003,1 @@\n-  enc_class aarch64_enc_strvQ(vecX src, memory mem) %{\n+  enc_class aarch64_enc_strvQ(vReg src, memory mem) %{\n@@ -4327,40 +4111,0 @@\n-operand immI_31()\n-%{\n-  predicate(n->get_int() == 31);\n-  match(ConI);\n-\n-  op_cost(0);\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n-operand immI_2()\n-%{\n-  predicate(n->get_int() == 2);\n-  match(ConI);\n-\n-  op_cost(0);\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n-operand immI_4()\n-%{\n-  predicate(n->get_int() == 4);\n-  match(ConI);\n-\n-  op_cost(0);\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n-operand immI_8()\n-%{\n-  predicate(n->get_int() == 8);\n-  match(ConI);\n-\n-  op_cost(0);\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n@@ -5405,2 +5149,1 @@\n-\/\/ all vector operands, including NEON and SVE,\n-\/\/ but currently only used for SVE VecA.\n+\/\/ all vector operands, including NEON and SVE.\n@@ -5408,0 +5151,12 @@\n+%{\n+  constraint(ALLOC_IN_RC(dynamic));\n+  match(VecA);\n+  match(VecD);\n+  match(VecX);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+operand vecA()\n@@ -5411,0 +5166,1 @@\n+\n@@ -6715,284 +6471,0 @@\n-pipe_class vmul64(vecD dst, vecD src1, vecD src2)\n-%{\n-  single_instruction;\n-  dst    : S5(write);\n-  src1   : S1(read);\n-  src2   : S1(read);\n-  INS01  : ISS;\n-  NEON_FP : S5;\n-%}\n-\n-pipe_class vmul128(vecX dst, vecX src1, vecX src2)\n-%{\n-  single_instruction;\n-  dst    : S5(write);\n-  src1   : S1(read);\n-  src2   : S1(read);\n-  INS0   : ISS;\n-  NEON_FP : S5;\n-%}\n-\n-pipe_class vmla64(vecD dst, vecD src1, vecD src2)\n-%{\n-  single_instruction;\n-  dst    : S5(write);\n-  src1   : S1(read);\n-  src2   : S1(read);\n-  dst    : S1(read);\n-  INS01  : ISS;\n-  NEON_FP : S5;\n-%}\n-\n-pipe_class vmla128(vecX dst, vecX src1, vecX src2)\n-%{\n-  single_instruction;\n-  dst    : S5(write);\n-  src1   : S1(read);\n-  src2   : S1(read);\n-  dst    : S1(read);\n-  INS0   : ISS;\n-  NEON_FP : S5;\n-%}\n-\n-pipe_class vdop64(vecD dst, vecD src1, vecD src2)\n-%{\n-  single_instruction;\n-  dst    : S4(write);\n-  src1   : S2(read);\n-  src2   : S2(read);\n-  INS01  : ISS;\n-  NEON_FP : S4;\n-%}\n-\n-pipe_class vdop128(vecX dst, vecX src1, vecX src2)\n-%{\n-  single_instruction;\n-  dst    : S4(write);\n-  src1   : S2(read);\n-  src2   : S2(read);\n-  INS0   : ISS;\n-  NEON_FP : S4;\n-%}\n-\n-pipe_class vlogical64(vecD dst, vecD src1, vecD src2)\n-%{\n-  single_instruction;\n-  dst    : S3(write);\n-  src1   : S2(read);\n-  src2   : S2(read);\n-  INS01  : ISS;\n-  NEON_FP : S3;\n-%}\n-\n-pipe_class vlogical128(vecX dst, vecX src1, vecX src2)\n-%{\n-  single_instruction;\n-  dst    : S3(write);\n-  src1   : S2(read);\n-  src2   : S2(read);\n-  INS0   : ISS;\n-  NEON_FP : S3;\n-%}\n-\n-pipe_class vshift64(vecD dst, vecD src, vecX shift)\n-%{\n-  single_instruction;\n-  dst    : S3(write);\n-  src    : S1(read);\n-  shift  : S1(read);\n-  INS01  : ISS;\n-  NEON_FP : S3;\n-%}\n-\n-pipe_class vshift128(vecX dst, vecX src, vecX shift)\n-%{\n-  single_instruction;\n-  dst    : S3(write);\n-  src    : S1(read);\n-  shift  : S1(read);\n-  INS0   : ISS;\n-  NEON_FP : S3;\n-%}\n-\n-pipe_class vshift64_imm(vecD dst, vecD src, immI shift)\n-%{\n-  single_instruction;\n-  dst    : S3(write);\n-  src    : S1(read);\n-  INS01  : ISS;\n-  NEON_FP : S3;\n-%}\n-\n-pipe_class vshift128_imm(vecX dst, vecX src, immI shift)\n-%{\n-  single_instruction;\n-  dst    : S3(write);\n-  src    : S1(read);\n-  INS0   : ISS;\n-  NEON_FP : S3;\n-%}\n-\n-pipe_class vdop_fp64(vecD dst, vecD src1, vecD src2)\n-%{\n-  single_instruction;\n-  dst    : S5(write);\n-  src1   : S1(read);\n-  src2   : S1(read);\n-  INS01  : ISS;\n-  NEON_FP : S5;\n-%}\n-\n-pipe_class vdop_fp128(vecX dst, vecX src1, vecX src2)\n-%{\n-  single_instruction;\n-  dst    : S5(write);\n-  src1   : S1(read);\n-  src2   : S1(read);\n-  INS0   : ISS;\n-  NEON_FP : S5;\n-%}\n-\n-pipe_class vmuldiv_fp64(vecD dst, vecD src1, vecD src2)\n-%{\n-  single_instruction;\n-  dst    : S5(write);\n-  src1   : S1(read);\n-  src2   : S1(read);\n-  INS0   : ISS;\n-  NEON_FP : S5;\n-%}\n-\n-pipe_class vmuldiv_fp128(vecX dst, vecX src1, vecX src2)\n-%{\n-  single_instruction;\n-  dst    : S5(write);\n-  src1   : S1(read);\n-  src2   : S1(read);\n-  INS0   : ISS;\n-  NEON_FP : S5;\n-%}\n-\n-pipe_class vsqrt_fp128(vecX dst, vecX src)\n-%{\n-  single_instruction;\n-  dst    : S5(write);\n-  src    : S1(read);\n-  INS0   : ISS;\n-  NEON_FP : S5;\n-%}\n-\n-pipe_class vunop_fp64(vecD dst, vecD src)\n-%{\n-  single_instruction;\n-  dst    : S5(write);\n-  src    : S1(read);\n-  INS01  : ISS;\n-  NEON_FP : S5;\n-%}\n-\n-pipe_class vunop_fp128(vecX dst, vecX src)\n-%{\n-  single_instruction;\n-  dst    : S5(write);\n-  src    : S1(read);\n-  INS0   : ISS;\n-  NEON_FP : S5;\n-%}\n-\n-pipe_class vdup_reg_reg64(vecD dst, iRegI src)\n-%{\n-  single_instruction;\n-  dst    : S3(write);\n-  src    : S1(read);\n-  INS01  : ISS;\n-  NEON_FP : S3;\n-%}\n-\n-pipe_class vdup_reg_reg128(vecX dst, iRegI src)\n-%{\n-  single_instruction;\n-  dst    : S3(write);\n-  src    : S1(read);\n-  INS01  : ISS;\n-  NEON_FP : S3;\n-%}\n-\n-pipe_class vdup_reg_freg64(vecD dst, vRegF src)\n-%{\n-  single_instruction;\n-  dst    : S3(write);\n-  src    : S1(read);\n-  INS01  : ISS;\n-  NEON_FP : S3;\n-%}\n-\n-pipe_class vdup_reg_freg128(vecX dst, vRegF src)\n-%{\n-  single_instruction;\n-  dst    : S3(write);\n-  src    : S1(read);\n-  INS01  : ISS;\n-  NEON_FP : S3;\n-%}\n-\n-pipe_class vdup_reg_dreg128(vecX dst, vRegD src)\n-%{\n-  single_instruction;\n-  dst    : S3(write);\n-  src    : S1(read);\n-  INS01  : ISS;\n-  NEON_FP : S3;\n-%}\n-\n-pipe_class vmovi_reg_imm64(vecD dst)\n-%{\n-  single_instruction;\n-  dst    : S3(write);\n-  INS01  : ISS;\n-  NEON_FP : S3;\n-%}\n-\n-pipe_class vmovi_reg_imm128(vecX dst)\n-%{\n-  single_instruction;\n-  dst    : S3(write);\n-  INS0   : ISS;\n-  NEON_FP : S3;\n-%}\n-\n-pipe_class vload_reg_mem64(vecD dst, vmem8 mem)\n-%{\n-  single_instruction;\n-  dst    : S5(write);\n-  mem    : ISS(read);\n-  INS01  : ISS;\n-  NEON_FP : S3;\n-%}\n-\n-pipe_class vload_reg_mem128(vecX dst, vmem16 mem)\n-%{\n-  single_instruction;\n-  dst    : S5(write);\n-  mem    : ISS(read);\n-  INS01  : ISS;\n-  NEON_FP : S3;\n-%}\n-\n-pipe_class vstore_reg_mem64(vecD src, vmem8 mem)\n-%{\n-  single_instruction;\n-  mem    : ISS(read);\n-  src    : S2(read);\n-  INS01  : ISS;\n-  NEON_FP : S3;\n-%}\n-\n-pipe_class vstore_reg_mem128(vecD src, vmem16 mem)\n-%{\n-  single_instruction;\n-  mem    : ISS(read);\n-  src    : S2(read);\n-  INS01  : ISS;\n-  NEON_FP : S3;\n-%}\n-\n@@ -9131,22 +8603,0 @@\n-instruct castVVD(vecD dst)\n-%{\n-  match(Set dst (CastVV dst));\n-\n-  size(0);\n-  format %{ \"# castVV of $dst\" %}\n-  ins_encode(\/* empty encoding *\/);\n-  ins_cost(0);\n-  ins_pipe(pipe_class_empty);\n-%}\n-\n-instruct castVVX(vecX dst)\n-%{\n-  match(Set dst (CastVV dst));\n-\n-  size(0);\n-  format %{ \"# castVV of $dst\" %}\n-  ins_encode(\/* empty encoding *\/);\n-  ins_cost(0);\n-  ins_pipe(pipe_class_empty);\n-%}\n-\n@@ -16999,0 +16449,2 @@\n+\/\/ Intrisics for String.compareTo()\n+\n@@ -17074,0 +16526,96 @@\n+\/\/ Note that Z registers alias the corresponding NEON registers, we declare the vector operands of\n+\/\/ these string_compare variants as NEON register type for convenience so that the prototype of\n+\/\/ string_compare can be shared with all variants.\n+\n+instruct string_compareLL_sve(iRegP_R1 str1, iRegI_R2 cnt1, iRegP_R3 str2, iRegI_R4 cnt2,\n+                              iRegI_R0 result, iRegP_R10 tmp1, iRegL_R11 tmp2,\n+                              vRegD_V0 vtmp1, vRegD_V1 vtmp2, pRegGov_P0 pgtmp1,\n+                              pRegGov_P1 pgtmp2, rFlagsReg cr)\n+%{\n+  predicate((UseSVE > 0) && (((StrCompNode*)n)->encoding() == StrIntrinsicNode::LL));\n+  match(Set result (StrComp (Binary str1 cnt1) (Binary str2 cnt2)));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP vtmp1, TEMP vtmp2, TEMP pgtmp1, TEMP pgtmp2,\n+         USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL cr);\n+\n+  format %{ \"String Compare $str1,$cnt1,$str2,$cnt2 -> $result   # USE sve\" %}\n+  ins_encode %{\n+    \/\/ Count is in 8-bit bytes; non-Compact chars are 16 bits.\n+    __ string_compare($str1$$Register, $str2$$Register,\n+                      $cnt1$$Register, $cnt2$$Register, $result$$Register,\n+                      $tmp1$$Register, $tmp2$$Register,\n+                      $vtmp1$$FloatRegister, $vtmp2$$FloatRegister, fnoreg,\n+                      as_PRegister($pgtmp1$$reg), as_PRegister($pgtmp2$$reg),\n+                      StrIntrinsicNode::LL);\n+  %}\n+  ins_pipe(pipe_class_memory);\n+%}\n+\n+instruct string_compareLU_sve(iRegP_R1 str1, iRegI_R2 cnt1, iRegP_R3 str2, iRegI_R4 cnt2,\n+                              iRegI_R0 result, iRegP_R10 tmp1, iRegL_R11 tmp2,\n+                              vRegD_V0 vtmp1, vRegD_V1 vtmp2, pRegGov_P0 pgtmp1,\n+                              pRegGov_P1 pgtmp2, rFlagsReg cr)\n+%{\n+  predicate((UseSVE > 0) && (((StrCompNode*)n)->encoding() == StrIntrinsicNode::LU));\n+  match(Set result (StrComp (Binary str1 cnt1) (Binary str2 cnt2)));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP vtmp1, TEMP vtmp2, TEMP pgtmp1, TEMP pgtmp2,\n+         USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL cr);\n+\n+  format %{ \"String Compare $str1,$cnt1,$str2,$cnt2 -> $result   # USE sve\" %}\n+  ins_encode %{\n+    \/\/ Count is in 8-bit bytes; non-Compact chars are 16 bits.\n+    __ string_compare($str1$$Register, $str2$$Register,\n+                      $cnt1$$Register, $cnt2$$Register, $result$$Register,\n+                      $tmp1$$Register, $tmp2$$Register,\n+                      $vtmp1$$FloatRegister, $vtmp2$$FloatRegister, fnoreg,\n+                      as_PRegister($pgtmp1$$reg), as_PRegister($pgtmp2$$reg),\n+                      StrIntrinsicNode::LU);\n+  %}\n+  ins_pipe(pipe_class_memory);\n+%}\n+\n+instruct string_compareUL_sve(iRegP_R1 str1, iRegI_R2 cnt1, iRegP_R3 str2, iRegI_R4 cnt2,\n+                              iRegI_R0 result, iRegP_R10 tmp1, iRegL_R11 tmp2,\n+                              vRegD_V0 vtmp1, vRegD_V1 vtmp2, pRegGov_P0 pgtmp1,\n+                              pRegGov_P1 pgtmp2, rFlagsReg cr)\n+%{\n+  predicate((UseSVE > 0) && (((StrCompNode*)n)->encoding() == StrIntrinsicNode::UL));\n+  match(Set result (StrComp (Binary str1 cnt1) (Binary str2 cnt2)));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP vtmp1, TEMP vtmp2, TEMP pgtmp1, TEMP pgtmp2,\n+         USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL cr);\n+\n+  format %{ \"String Compare $str1,$cnt1,$str2,$cnt2 -> $result   # USE sve\" %}\n+  ins_encode %{\n+    \/\/ Count is in 8-bit bytes; non-Compact chars are 16 bits.\n+    __ string_compare($str1$$Register, $str2$$Register,\n+                      $cnt1$$Register, $cnt2$$Register, $result$$Register,\n+                      $tmp1$$Register, $tmp2$$Register,\n+                      $vtmp1$$FloatRegister, $vtmp2$$FloatRegister, fnoreg,\n+                      as_PRegister($pgtmp1$$reg), as_PRegister($pgtmp2$$reg),\n+                      StrIntrinsicNode::UL);\n+  %}\n+  ins_pipe(pipe_class_memory);\n+%}\n+\n+instruct string_compareUU_sve(iRegP_R1 str1, iRegI_R2 cnt1, iRegP_R3 str2, iRegI_R4 cnt2,\n+                              iRegI_R0 result, iRegP_R10 tmp1, iRegL_R11 tmp2,\n+                              vRegD_V0 vtmp1, vRegD_V1 vtmp2, pRegGov_P0 pgtmp1,\n+                              pRegGov_P1 pgtmp2, rFlagsReg cr)\n+%{\n+  predicate((UseSVE > 0) && (((StrCompNode*)n)->encoding() == StrIntrinsicNode::UU));\n+  match(Set result (StrComp (Binary str1 cnt1) (Binary str2 cnt2)));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP vtmp1, TEMP vtmp2, TEMP pgtmp1, TEMP pgtmp2,\n+         USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL cr);\n+\n+  format %{ \"String Compare $str1,$cnt1,$str2,$cnt2 -> $result   # USE sve\" %}\n+  ins_encode %{\n+    \/\/ Count is in 8-bit bytes; non-Compact chars are 16 bits.\n+    __ string_compare($str1$$Register, $str2$$Register,\n+                      $cnt1$$Register, $cnt2$$Register, $result$$Register,\n+                      $tmp1$$Register, $tmp2$$Register,\n+                      $vtmp1$$FloatRegister, $vtmp2$$FloatRegister, fnoreg,\n+                      as_PRegister($pgtmp1$$reg), as_PRegister($pgtmp2$$reg),\n+                      StrIntrinsicNode::UU);\n+  %}\n+  ins_pipe(pipe_class_memory);\n+%}\n+\n@@ -17238,0 +16786,32 @@\n+instruct stringL_indexof_char_sve(iRegP_R1 str1, iRegI_R2 cnt1, iRegI_R3 ch,\n+                                  iRegI_R0 result, vecA ztmp1, vecA ztmp2,\n+                                  pRegGov pgtmp, pReg ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 && ((StrIndexOfCharNode*)n)->encoding() == StrIntrinsicNode::L);\n+  match(Set result (StrIndexOfChar (Binary str1 cnt1) ch));\n+  effect(TEMP ztmp1, TEMP ztmp2, TEMP pgtmp, TEMP ptmp, KILL cr);\n+  format %{ \"StringLatin1 IndexOf char[] $str1,$cnt1,$ch -> $result # use sve\" %}\n+  ins_encode %{\n+    __ string_indexof_char_sve($str1$$Register, $cnt1$$Register, $ch$$Register,\n+                               $result$$Register, $ztmp1$$FloatRegister,\n+                               $ztmp2$$FloatRegister, $pgtmp$$PRegister,\n+                               $ptmp$$PRegister, true \/* isL *\/);\n+  %}\n+  ins_pipe(pipe_class_memory);\n+%}\n+\n+instruct stringU_indexof_char_sve(iRegP_R1 str1, iRegI_R2 cnt1, iRegI_R3 ch,\n+                                  iRegI_R0 result, vecA ztmp1, vecA ztmp2,\n+                                  pRegGov pgtmp, pReg ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 && ((StrIndexOfCharNode*)n)->encoding() == StrIntrinsicNode::U);\n+  match(Set result (StrIndexOfChar (Binary str1 cnt1) ch));\n+  effect(TEMP ztmp1, TEMP ztmp2, TEMP pgtmp, TEMP ptmp, KILL cr);\n+  format %{ \"StringUTF16 IndexOf char[] $str1,$cnt1,$ch -> $result # use sve\" %}\n+  ins_encode %{\n+    __ string_indexof_char_sve($str1$$Register, $cnt1$$Register, $ch$$Register,\n+                               $result$$Register, $ztmp1$$FloatRegister,\n+                               $ztmp2$$FloatRegister, $pgtmp$$PRegister,\n+                               $ptmp$$PRegister, false \/* isL *\/);\n+  %}\n+  ins_pipe(pipe_class_memory);\n+%}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":226,"deletions":646,"binary":false,"changes":872,"status":"modified"},{"patch":"@@ -0,0 +1,5964 @@\n+\/\/\n+\/\/ Copyright (c) 2020, 2022, Oracle and\/or its affiliates. All rights reserved.\n+\/\/ Copyright (c) 2020, 2022, Arm Limited. All rights reserved.\n+\/\/ DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+\/\/\n+\/\/ This code is free software; you can redistribute it and\/or modify it\n+\/\/ under the terms of the GNU General Public License version 2 only, as\n+\/\/ published by the Free Software Foundation.\n+\/\/\n+\/\/ This code is distributed in the hope that it will be useful, but WITHOUT\n+\/\/ ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+\/\/ FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+\/\/ version 2 for more details (a copy is included in the LICENSE file that\n+\/\/ accompanied this code).\n+\/\/\n+\/\/ You should have received a copy of the GNU General Public License version\n+\/\/ 2 along with this work; if not, write to the Free Software Foundation,\n+\/\/ Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+\/\/\n+\/\/ Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+\/\/ or visit www.oracle.com if you need additional information or have any\n+\/\/ questions.\n+\/\/\n+\/\/\n+\n+\/\/ AArch64 VECTOR Architecture Description File\n+\n+\n+\/\/ 4 bit signed offset -- for predicated load\/store\n+\n+operand vmemA_immIOffset4() %{\n+  \/\/ (esize \/ msize) = 1\n+  predicate(Address::offset_ok_for_sve_immed(n->get_int(), 4,\n+            Matcher::scalable_vector_reg_size(T_BYTE)));\n+  match(ConI);\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+operand vmemA_immLOffset4() %{\n+  \/\/ (esize \/ msize) = 1\n+  predicate(Address::offset_ok_for_sve_immed(n->get_long(), 4,\n+            Matcher::scalable_vector_reg_size(T_BYTE)));\n+  match(ConL);\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+operand vmemA_indOffI4(iRegP reg, vmemA_immIOffset4 off) %{\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  match(AddP reg off);\n+  op_cost(0);\n+  format %{ \"[$reg, $off]\" %}\n+  interface(MEMORY_INTER) %{\n+    base($reg);\n+    index(0xffffffff);\n+    scale(0x0);\n+    disp($off);\n+  %}\n+%}\n+\n+operand vmemA_indOffL4(iRegP reg, vmemA_immLOffset4 off) %{\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  match(AddP reg off);\n+  op_cost(0);\n+  format %{ \"[$reg, $off]\" %}\n+  interface(MEMORY_INTER) %{\n+    base($reg);\n+    index(0xffffffff);\n+    scale(0x0);\n+    disp($off);\n+  %}\n+%}\n+\n+\/\/ The indOff of vmemA is valid only when the vector element (load to\/store from)\n+\/\/ size equals to memory element (load from\/store to) size.\n+opclass vmemA(indirect, vmemA_indOffI4, vmemA_indOffL4);\n+\n+source_hpp %{\n+  \/\/ Assert that the given node is not a variable shift.\n+  bool assert_not_var_shift(const Node* n);\n+\n+  Assembler::SIMD_Arrangement get_arrangement(const Node* n);\n+%}\n+\n+source %{\n+\n+  typedef void (C2_MacroAssembler::* sve_mem_insn_predicate)(FloatRegister Rt, Assembler::SIMD_RegVariant T,\n+                                                             PRegister Pg, const Address &adr);\n+\n+  \/\/ Predicated load\/store, with optional ptrue to all elements of given predicate register.\n+  static void loadStoreA_predicated(C2_MacroAssembler masm, bool is_store, FloatRegister reg,\n+                                    PRegister pg, BasicType mem_elem_bt, BasicType vector_elem_bt,\n+                                    int opcode, Register base, int index, int size, int disp) {\n+    sve_mem_insn_predicate insn;\n+    int mesize = type2aelembytes(mem_elem_bt);\n+    if (index == -1) {\n+      assert(size == 0, \"unsupported address mode: scale size = %d\", size);\n+      switch(mesize) {\n+      case 1:\n+        insn = is_store ? &C2_MacroAssembler::sve_st1b : &C2_MacroAssembler::sve_ld1b;\n+        break;\n+      case 2:\n+        insn = is_store ? &C2_MacroAssembler::sve_st1h : &C2_MacroAssembler::sve_ld1h;\n+        break;\n+      case 4:\n+        insn = is_store ? &C2_MacroAssembler::sve_st1w : &C2_MacroAssembler::sve_ld1w;\n+        break;\n+      case 8:\n+        insn = is_store ? &C2_MacroAssembler::sve_st1d : &C2_MacroAssembler::sve_ld1d;\n+        break;\n+      default:\n+        assert(false, \"unsupported\");\n+        ShouldNotReachHere();\n+      }\n+      int imm4 = disp \/ mesize \/ Matcher::scalable_vector_reg_size(vector_elem_bt);\n+      (masm.*insn)(reg, Assembler::elemType_to_regVariant(vector_elem_bt), pg, Address(base, imm4));\n+    } else {\n+      assert(false, \"unimplemented\");\n+      ShouldNotReachHere();\n+    }\n+  }\n+\n+  const bool Matcher::match_rule_supported_superword(int opcode, int vlen, BasicType bt) {\n+    if (UseSVE == 0) {\n+      \/\/ ConvD2I and ConvL2F are not profitable to be vectorized on NEON, because no direct\n+      \/\/ NEON instructions support them. But the match rule support for them is profitable for\n+      \/\/ Vector API intrinsics.\n+      if ((opcode == Op_VectorCastD2X && bt == T_INT) ||\n+          (opcode == Op_VectorCastL2X && bt == T_FLOAT)) {\n+        return false;\n+      }\n+    }\n+    return match_rule_supported_vector(opcode, vlen, bt);\n+  }\n+\n+  \/\/ Identify extra cases that we might want to provide match rules for vector nodes and\n+  \/\/ other intrinsics guarded with vector length (vlen) and element type (bt).\n+  const bool Matcher::match_rule_supported_vector(int opcode, int vlen, BasicType bt) {\n+    if (!match_rule_supported(opcode)) {\n+      return false;\n+    }\n+\n+    int length_in_bytes = vlen * type2aelembytes(bt);\n+    if (UseSVE == 0 && length_in_bytes > 16) {\n+      return false;\n+    }\n+\n+    switch (opcode) {\n+      case Op_MulVL:\n+      case Op_AndVMask:\n+      case Op_OrVMask:\n+      case Op_XorVMask:\n+      case Op_MaskAll:\n+      case Op_VectorMaskGen:\n+      case Op_LoadVectorMasked:\n+      case Op_StoreVectorMasked:\n+      case Op_LoadVectorGather:\n+      case Op_StoreVectorScatter:\n+      case Op_LoadVectorGatherMasked:\n+      case Op_StoreVectorScatterMasked:\n+      case Op_PopulateIndex:\n+      case Op_CompressM:\n+      case Op_CompressV:\n+        if (UseSVE == 0) {\n+          return false;\n+        }\n+        break;\n+      case Op_MulAddVS2VI:\n+        return length_in_bytes == 16;\n+      case Op_MulReductionVD:\n+      case Op_MulReductionVF:\n+      case Op_MulReductionVI:\n+      case Op_MulReductionVL:\n+        \/\/ No multiply reduction instructions, and we emit scalar\n+        \/\/ instructions for 64\/128-bit vectors.\n+        return vlen >= 2 && (length_in_bytes == 8 || length_in_bytes == 16);\n+      case Op_VectorMaskCmp:\n+        if (length_in_bytes < 8) {\n+          return false;\n+        }\n+        break;\n+      case Op_VectorLoadShuffle:\n+      case Op_VectorRearrange:\n+        if (vlen < 4) {\n+          return false;\n+        }\n+        break;\n+      case Op_ExpandV:\n+        if (UseSVE < 2 || is_subword_type(bt)) {\n+          return false;\n+        }\n+        break;\n+      case Op_VectorMaskToLong:\n+        if (UseSVE > 0 && vlen > 64) {\n+          return false;\n+        }\n+        break;\n+      case Op_VectorLongToMask:\n+        if (UseSVE < 2 || vlen > 64 || !VM_Version::supports_svebitperm()) {\n+          return false;\n+        }\n+        break;\n+      default:\n+        break;\n+    }\n+    return vector_size_supported(bt, vlen);\n+  }\n+\n+  const bool Matcher::match_rule_supported_vector_masked(int opcode, int vlen, BasicType bt) {\n+    \/\/ Only SVE supports masked operations.\n+    if (UseSVE == 0 || !match_rule_supported_vector(opcode, vlen, bt)) {\n+      return false;\n+    }\n+\n+    \/\/ If an opcode does not support the masked version,\n+    \/\/ unpredicated node with VectorBlend node will be used instead.\n+    switch (opcode) {\n+      case Op_VectorRearrange:\n+      case Op_MulReductionVD:\n+      case Op_MulReductionVF:\n+      case Op_MulReductionVI:\n+      case Op_MulReductionVL:\n+        return false;\n+      default:\n+        return true;\n+    }\n+  }\n+\n+  \/\/ Assert that the given node is not a variable shift.\n+  bool assert_not_var_shift(const Node* n) {\n+    assert(!n->as_ShiftV()->is_var_shift(), \"illegal variable shift\");\n+    return true;\n+  }\n+\n+  Assembler::SIMD_Arrangement get_arrangement(const Node* n) {\n+    BasicType bt = Matcher::vector_element_basic_type(n);\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(n);\n+    return Assembler::esize2arrangement((uint)type2aelembytes(bt),\n+                                        \/* isQ *\/ length_in_bytes == 16);\n+  }\n+%}\n+\n+\n+\/\/ All VECTOR instructions\n+\n+\/\/ ------------------------------ Vector load\/store ----------------------------\n+\n+\/\/ Load Vector (16 bits)\n+instruct loadV2(vReg dst, vmem2 mem) %{\n+  predicate(n->as_LoadVector()->memory_size() == 2);\n+  match(Set dst (LoadVector mem));\n+  format %{ \"loadV2 $dst, $mem\\t# vector (16 bits)\" %}\n+  ins_encode( aarch64_enc_ldrvH(dst, mem) );\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Store Vector (16 bits)\n+instruct storeV2(vReg src, vmem2 mem) %{\n+  predicate(n->as_StoreVector()->memory_size() == 2);\n+  match(Set mem (StoreVector mem src));\n+  format %{ \"storeV2 $mem, $src\\t# vector (16 bits)\" %}\n+  ins_encode( aarch64_enc_strvH(src, mem) );\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Load Vector (32 bits)\n+instruct loadV4(vReg dst, vmem4 mem) %{\n+  predicate(n->as_LoadVector()->memory_size() == 4);\n+  match(Set dst (LoadVector mem));\n+  format %{ \"loadV4 $dst, $mem\\t# vector (32 bits)\" %}\n+  ins_encode( aarch64_enc_ldrvS(dst, mem) );\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Store Vector (32 bits)\n+instruct storeV4(vReg src, vmem4 mem) %{\n+  predicate(n->as_StoreVector()->memory_size() == 4);\n+  match(Set mem (StoreVector mem src));\n+  format %{ \"storeV4 $mem, $src\\t# vector (32 bits)\" %}\n+  ins_encode( aarch64_enc_strvS(src, mem) );\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Load Vector (64 bits)\n+instruct loadV8(vReg dst, vmem8 mem) %{\n+  predicate(n->as_LoadVector()->memory_size() == 8);\n+  match(Set dst (LoadVector mem));\n+  format %{ \"loadV8 $dst, $mem\\t# vector (64 bits)\" %}\n+  ins_encode( aarch64_enc_ldrvD(dst, mem) );\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Store Vector (64 bits)\n+instruct storeV8(vReg src, vmem8 mem) %{\n+  predicate(n->as_StoreVector()->memory_size() == 8);\n+  match(Set mem (StoreVector mem src));\n+  format %{ \"storeV8 $mem, $src\\t# vector (64 bits)\" %}\n+  ins_encode( aarch64_enc_strvD(src, mem) );\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Load Vector (128 bits)\n+instruct loadV16(vReg dst, vmem16 mem) %{\n+  predicate(n->as_LoadVector()->memory_size() == 16);\n+  match(Set dst (LoadVector mem));\n+  format %{ \"loadV16 $dst, $mem\\t# vector (128 bits)\" %}\n+  ins_encode( aarch64_enc_ldrvQ(dst, mem) );\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Store Vector (128 bits)\n+instruct storeV16(vReg src, vmem16 mem) %{\n+  predicate(n->as_StoreVector()->memory_size() == 16);\n+  match(Set mem (StoreVector mem src));\n+  format %{ \"storeV16 $mem, $src\\t# vector (128 bits)\" %}\n+  ins_encode( aarch64_enc_strvQ(src, mem) );\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Load Vector (> 128 bits, full sized)\n+instruct loadV(vReg dst, vmemA mem) %{\n+  predicate(n->as_LoadVector()->memory_size() > 16 &&\n+            n->as_LoadVector()->memory_size() == MaxVectorSize);\n+  match(Set dst (LoadVector mem));\n+  format %{ \"loadV $dst, $mem\\t# vector (sve)\" %}\n+  ins_encode %{\n+    assert(UseSVE > 0, \"must be sve\");\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    loadStoreA_predicated(C2_MacroAssembler(&cbuf), \/* is_store *\/ false,\n+                          $dst$$FloatRegister, ptrue, bt, bt, $mem->opcode(),\n+                          as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Store Vector (> 128 bits, full sized)\n+instruct storeV(vReg src, vmemA mem) %{\n+  predicate(n->as_StoreVector()->memory_size() > 16 &&\n+            n->as_StoreVector()->memory_size() == MaxVectorSize);\n+  match(Set mem (StoreVector mem src));\n+  format %{ \"storeV $mem, $src\\t# vector (sve)\" %}\n+  ins_encode %{\n+    assert(UseSVE > 0, \"must be sve\");\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n+    loadStoreA_predicated(C2_MacroAssembler(&cbuf), \/* is_store *\/ true,\n+                          $src$$FloatRegister, ptrue, bt, bt, $mem->opcode(),\n+                          as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Predicated vector load\/store, based on the vector length of the node.\n+\/\/ Only load\/store values in the range of the memory_size. This is needed\n+\/\/ when the memory_size is lower than the hardware supported max vector size.\n+\/\/ And this might happen for Vector API mask vector load\/store.\n+\n+\/\/ Load Vector (> 128 bits, partial)\n+instruct loadV_partial(vReg dst, vmemA mem, pRegGov pgtmp) %{\n+  predicate(n->as_LoadVector()->memory_size() > 16 &&\n+            n->as_LoadVector()->memory_size() < MaxVectorSize);\n+  match(Set dst (LoadVector mem));\n+  effect(TEMP pgtmp);\n+  format %{ \"loadV_partial $dst, $mem\\t# vector (sve). KILL $pgtmp\" %}\n+  ins_encode %{\n+    assert(UseSVE > 0, \"must be sve\");\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ sve_ptrue_lanecnt($pgtmp$$PRegister, __ elemType_to_regVariant(bt),\n+                         Matcher::vector_length(this));\n+    loadStoreA_predicated(C2_MacroAssembler(&cbuf), \/* is_store *\/ false, $dst$$FloatRegister,\n+                          $pgtmp$$PRegister, bt, bt, $mem->opcode(),\n+                          as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Store Vector (> 128 bits, partial)\n+instruct storeV_partial(vReg src, vmemA mem, pRegGov pgtmp) %{\n+  predicate(n->as_StoreVector()->memory_size() > 16 &&\n+            n->as_StoreVector()->memory_size() < MaxVectorSize);\n+  match(Set mem (StoreVector mem src));\n+  effect(TEMP pgtmp);\n+  format %{ \"storeV_partial $mem, $src\\t# vector (sve). KILL $pgtmp\" %}\n+  ins_encode %{\n+    assert(UseSVE > 0, \"must be sve\");\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n+    __ sve_ptrue_lanecnt($pgtmp$$PRegister, __ elemType_to_regVariant(bt),\n+                         Matcher::vector_length(this, $src));\n+    loadStoreA_predicated(C2_MacroAssembler(&cbuf), \/* is_store *\/ true, $src$$FloatRegister,\n+                          $pgtmp$$PRegister, bt, bt, $mem->opcode(),\n+                          as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector load\/store - predicated\n+\n+instruct loadV_masked(vReg dst, vmemA mem, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst (LoadVectorMasked mem pg));\n+  format %{ \"loadV_masked $dst, $pg, $mem\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    loadStoreA_predicated(C2_MacroAssembler(&cbuf), \/* is_store *\/ false, $dst$$FloatRegister,\n+                          $pg$$PRegister, bt, bt, $mem->opcode(),\n+                          as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct storeV_masked(vReg src, vmemA mem, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set mem (StoreVectorMasked mem (Binary src pg)));\n+  format %{ \"storeV_masked $mem, $pg, $src\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n+    loadStoreA_predicated(C2_MacroAssembler(&cbuf), \/* is_store *\/ true, $src$$FloatRegister,\n+                          $pg$$PRegister, bt, bt, $mem->opcode(),\n+                          as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector load const\n+\n+instruct vloadconB(vReg dst, immI0 src) %{\n+  predicate(Matcher::vector_element_basic_type(n) == T_BYTE);\n+  match(Set dst (VectorLoadConst src));\n+  format %{ \"vloadconB $dst, $src\\t# load\/generate iota indices\" %}\n+  ins_encode %{\n+    if (UseSVE == 0) {\n+      uint length_in_bytes = Matcher::vector_length_in_bytes(this);\n+      assert(length_in_bytes <= 16, \"must be\");\n+      __ lea(rscratch1, ExternalAddress(StubRoutines::aarch64::vector_iota_indices()));\n+      if (length_in_bytes == 16) {\n+        __ ldrq($dst$$FloatRegister, rscratch1);\n+      } else {\n+        __ ldrd($dst$$FloatRegister, rscratch1);\n+      }\n+    } else {\n+      __ sve_index($dst$$FloatRegister, __ B, 0, 1);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ Vector add -----------------------------------\n+\n+\/\/ vector add\n+\n+instruct vadd(vReg dst, vReg src1, vReg src2) %{\n+  match(Set dst (AddVB src1 src2));\n+  match(Set dst (AddVS src1 src2));\n+  match(Set dst (AddVI src1 src2));\n+  match(Set dst (AddVL src1 src2));\n+  match(Set dst (AddVF src1 src2));\n+  match(Set dst (AddVD src1 src2));\n+  format %{ \"vadd $dst, $src1, $src2\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this);\n+    if (length_in_bytes <= 16) {\n+      if (is_floating_point_type(bt)) {\n+        __ fadd($dst$$FloatRegister, get_arrangement(this),\n+                $src1$$FloatRegister, $src2$$FloatRegister);\n+      } else {\n+        assert(is_integral_type(bt), \"unsupported type\");\n+        __ addv($dst$$FloatRegister, get_arrangement(this),\n+                $src1$$FloatRegister, $src2$$FloatRegister);\n+      }\n+    } else {\n+      assert(UseSVE > 0, \"must be sve\");\n+      if (is_floating_point_type(bt)) {\n+        __ sve_fadd($dst$$FloatRegister, __ elemType_to_regVariant(bt),\n+                    $src1$$FloatRegister, $src2$$FloatRegister);\n+      } else {\n+        assert(is_integral_type(bt), \"unsupported type\");\n+        __ sve_add($dst$$FloatRegister, __ elemType_to_regVariant(bt),\n+                   $src1$$FloatRegister, $src2$$FloatRegister);\n+      }\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector add - predicated\n+\n+instruct vadd_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (AddVB (Binary dst_src1 src2) pg));\n+  match(Set dst_src1 (AddVS (Binary dst_src1 src2) pg));\n+  match(Set dst_src1 (AddVI (Binary dst_src1 src2) pg));\n+  match(Set dst_src1 (AddVL (Binary dst_src1 src2) pg));\n+  match(Set dst_src1 (AddVF (Binary dst_src1 src2) pg));\n+  match(Set dst_src1 (AddVD (Binary dst_src1 src2) pg));\n+  format %{ \"vadd_masked $dst_src1, $pg, $dst_src1, $src2\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    if (is_floating_point_type(bt)) {\n+      __ sve_fadd($dst_src1$$FloatRegister, __ elemType_to_regVariant(bt),\n+                  $pg$$PRegister, $src2$$FloatRegister);\n+    } else {\n+      assert(is_integral_type(bt), \"unsupported type\");\n+      __ sve_add($dst_src1$$FloatRegister, __ elemType_to_regVariant(bt),\n+                 $pg$$PRegister, $src2$$FloatRegister);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector add reg imm (unpredicated)\n+\n+instruct vaddImmB(vReg dst_src, immBAddSubV con) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (AddVB dst_src (ReplicateB con)));\n+  format %{ \"vaddImmB $dst_src, $dst_src, $con\" %}\n+  ins_encode %{\n+    int val = (int)$con$$constant;\n+    if (val > 0) {\n+      __ sve_add($dst_src$$FloatRegister, __ B, val);\n+    } else {\n+      __ sve_sub($dst_src$$FloatRegister, __ B, -val);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vaddImmS(vReg dst_src, immIAddSubV con) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (AddVS dst_src (ReplicateS con)));\n+  format %{ \"vaddImmS $dst_src, $dst_src, $con\" %}\n+  ins_encode %{\n+    int val = (int)$con$$constant;\n+    if (val > 0) {\n+      __ sve_add($dst_src$$FloatRegister, __ H, val);\n+    } else {\n+      __ sve_sub($dst_src$$FloatRegister, __ H, -val);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vaddImmI(vReg dst_src, immIAddSubV con) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (AddVI dst_src (ReplicateI con)));\n+  format %{ \"vaddImmI $dst_src, $dst_src, $con\" %}\n+  ins_encode %{\n+    int val = (int)$con$$constant;\n+    if (val > 0) {\n+      __ sve_add($dst_src$$FloatRegister, __ S, val);\n+    } else {\n+      __ sve_sub($dst_src$$FloatRegister, __ S, -val);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vaddImmL(vReg dst_src, immLAddSubV con) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (AddVL dst_src (ReplicateL con)));\n+  format %{ \"vaddImmL $dst_src, $dst_src, $con\" %}\n+  ins_encode %{\n+    int val = (int)$con$$constant;\n+    if (val > 0) {\n+      __ sve_add($dst_src$$FloatRegister, __ D, val);\n+    } else {\n+      __ sve_sub($dst_src$$FloatRegister, __ D, -val);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ Vector sub -----------------------------------\n+\n+\/\/ vector sub\n+\n+instruct vsub(vReg dst, vReg src1, vReg src2) %{\n+  match(Set dst (SubVB src1 src2));\n+  match(Set dst (SubVS src1 src2));\n+  match(Set dst (SubVI src1 src2));\n+  match(Set dst (SubVL src1 src2));\n+  match(Set dst (SubVF src1 src2));\n+  match(Set dst (SubVD src1 src2));\n+  format %{ \"vsub $dst, $src1, $src2\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this);\n+    if (length_in_bytes <= 16) {\n+      if (is_floating_point_type(bt)) {\n+        __ fsub($dst$$FloatRegister, get_arrangement(this),\n+                $src1$$FloatRegister, $src2$$FloatRegister);\n+      } else {\n+        assert(is_integral_type(bt), \"unsupported type\");\n+        __ subv($dst$$FloatRegister, get_arrangement(this),\n+                $src1$$FloatRegister, $src2$$FloatRegister);\n+      }\n+    } else {\n+      assert(UseSVE > 0, \"must be sve\");\n+      if (is_floating_point_type(bt)) {\n+        __ sve_fsub($dst$$FloatRegister, __ elemType_to_regVariant(bt),\n+                    $src1$$FloatRegister, $src2$$FloatRegister);\n+      } else {\n+        assert(is_integral_type(bt), \"unsupported type\");\n+        __ sve_sub($dst$$FloatRegister, __ elemType_to_regVariant(bt),\n+                   $src1$$FloatRegister, $src2$$FloatRegister);\n+      }\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector sub - predicated\n+\n+instruct vsub_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (SubVB (Binary dst_src1 src2) pg));\n+  match(Set dst_src1 (SubVS (Binary dst_src1 src2) pg));\n+  match(Set dst_src1 (SubVI (Binary dst_src1 src2) pg));\n+  match(Set dst_src1 (SubVL (Binary dst_src1 src2) pg));\n+  match(Set dst_src1 (SubVF (Binary dst_src1 src2) pg));\n+  match(Set dst_src1 (SubVD (Binary dst_src1 src2) pg));\n+  format %{ \"vsub_masked $dst_src1, $pg, $dst_src1, $src2\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    if (is_floating_point_type(bt)) {\n+      __ sve_fsub($dst_src1$$FloatRegister, __ elemType_to_regVariant(bt),\n+                  $pg$$PRegister, $src2$$FloatRegister);\n+    } else {\n+      assert(is_integral_type(bt), \"unsupported type\");\n+      __ sve_sub($dst_src1$$FloatRegister, __ elemType_to_regVariant(bt),\n+                 $pg$$PRegister, $src2$$FloatRegister);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ Vector mul -----------------------------------\n+\n+\/\/ vector mul - BYTE, CHAR, SHORT, INT\n+\n+instruct vmulI_le128b(vReg dst, vReg src1, vReg src2) %{\n+  predicate(Matcher::vector_length_in_bytes(n) <= 16);\n+  match(Set dst (MulVB src1 src2));\n+  match(Set dst (MulVS src1 src2));\n+  match(Set dst (MulVI src1 src2));\n+  format %{ \"vmulI_le128b $dst, $src1, $src2\\t# vector <= 128 bits\" %}\n+  ins_encode %{\n+    __ mulv($dst$$FloatRegister, get_arrangement(this),\n+            $src1$$FloatRegister, $src2$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmulI_gt128b(vReg dst_src1, vReg src2) %{\n+  predicate(Matcher::vector_length_in_bytes(n) > 16);\n+  match(Set dst_src1 (MulVB dst_src1 src2));\n+  match(Set dst_src1 (MulVS dst_src1 src2));\n+  match(Set dst_src1 (MulVI dst_src1 src2));\n+  format %{ \"vmulI_gt128b $dst_src1, $dst_src1, $src2\\t# vector > 128 bits\" %}\n+  ins_encode %{\n+    assert(UseSVE > 0, \"must be sve\");\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ sve_mul($dst_src1$$FloatRegister, __ elemType_to_regVariant(bt),\n+               ptrue, $src2$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector mul - LONG\n+\n+instruct vmulL_neon(vReg dst, vReg src1, vReg src2) %{\n+  predicate(UseSVE == 0);\n+  match(Set dst (MulVL src1 src2));\n+  format %{ \"vmulL_neon $dst, $src1, $src2\\t# 2L\" %}\n+  ins_encode %{\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this);\n+    assert(length_in_bytes == 16, \"must be\");\n+    __ umov(rscratch1, $src1$$FloatRegister, __ D, 0);\n+    __ umov(rscratch2, $src2$$FloatRegister, __ D, 0);\n+    __ mul(rscratch2, rscratch2, rscratch1);\n+    __ mov($dst$$FloatRegister, __ D, 0, rscratch2);\n+    __ umov(rscratch1, $src1$$FloatRegister, __ D, 1);\n+    __ umov(rscratch2, $src2$$FloatRegister, __ D, 1);\n+    __ mul(rscratch2, rscratch2, rscratch1);\n+    __ mov($dst$$FloatRegister, __ D, 1, rscratch2);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmulL_sve(vReg dst_src1, vReg src2) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (MulVL dst_src1 src2));\n+  format %{ \"vmulL_sve $dst_src1, $dst_src1, $src2\" %}\n+  ins_encode %{\n+    assert(UseSVE > 0, \"must be sve\");\n+    __ sve_mul($dst_src1$$FloatRegister, __ D, ptrue, $src2$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector mul - floating-point\n+\n+instruct vmul_fp(vReg dst, vReg src1, vReg src2) %{\n+  match(Set dst (MulVF src1 src2));\n+  match(Set dst (MulVD src1 src2));\n+  format %{ \"vmul_fp $dst, $src1, $src2\" %}\n+  ins_encode %{\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this);\n+    if (length_in_bytes <= 16) {\n+      __ fmul($dst$$FloatRegister, get_arrangement(this),\n+              $src1$$FloatRegister, $src2$$FloatRegister);\n+    } else {\n+      assert(UseSVE > 0, \"must be sve\");\n+      BasicType bt = Matcher::vector_element_basic_type(this);\n+      __ sve_fmul($dst$$FloatRegister, __ elemType_to_regVariant(bt),\n+                  $src1$$FloatRegister, $src2$$FloatRegister);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector mul - predicated\n+\n+instruct vmul_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (MulVB (Binary dst_src1 src2) pg));\n+  match(Set dst_src1 (MulVS (Binary dst_src1 src2) pg));\n+  match(Set dst_src1 (MulVI (Binary dst_src1 src2) pg));\n+  match(Set dst_src1 (MulVL (Binary dst_src1 src2) pg));\n+  match(Set dst_src1 (MulVF (Binary dst_src1 src2) pg));\n+  match(Set dst_src1 (MulVD (Binary dst_src1 src2) pg));\n+  format %{ \"vmul_masked $dst_src1, $pg, $dst_src1, $src2\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    if (is_floating_point_type(bt)) {\n+      __ sve_fmul($dst_src1$$FloatRegister, __ elemType_to_regVariant(bt),\n+                  $pg$$PRegister, $src2$$FloatRegister);\n+    } else {\n+      assert(is_integral_type(bt), \"unsupported type\");\n+      __ sve_mul($dst_src1$$FloatRegister, __ elemType_to_regVariant(bt),\n+                 $pg$$PRegister, $src2$$FloatRegister);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ Vector float div -----------------------------\n+\n+\/\/ vector float div\n+\n+instruct vfdiv_le128b(vReg dst, vReg src1, vReg src2) %{\n+  predicate(Matcher::vector_length_in_bytes(n) <= 16);\n+  match(Set dst (DivVF src1 src2));\n+  match(Set dst (DivVD src1 src2));\n+  format %{ \"vfdiv_le128b $dst, $src1, $src2\\t# vector <= 128 bits\" %}\n+  ins_encode %{\n+    __ fdiv($dst$$FloatRegister, get_arrangement(this),\n+            $src1$$FloatRegister, $src2$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vfdiv_gt128b(vReg dst_src1, vReg src2) %{\n+  predicate(Matcher::vector_length_in_bytes(n) > 16);\n+  match(Set dst_src1 (DivVF dst_src1 src2));\n+  match(Set dst_src1 (DivVD dst_src1 src2));\n+  format %{ \"vfdiv_gt128b $dst_src1, $dst_src1, $src2\\t# vector > 128 bits\" %}\n+  ins_encode %{\n+    assert(UseSVE > 0, \"must be sve\");\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ sve_fdiv($dst_src1$$FloatRegister, __ elemType_to_regVariant(bt),\n+                ptrue, $src2$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector float div - predicated\n+\n+instruct vfdiv_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (DivVF (Binary dst_src1 src2) pg));\n+  match(Set dst_src1 (DivVD (Binary dst_src1 src2) pg));\n+  format %{ \"vfdiv_masked $dst_src1, $pg, $dst_src1, $src2\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ sve_fdiv($dst_src1$$FloatRegister, __ elemType_to_regVariant(bt),\n+                $pg$$PRegister, $src2$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ Vector and -----------------------------------\n+\n+\/\/ vector and\n+\n+instruct vand(vReg dst, vReg src1, vReg src2) %{\n+  match(Set dst (AndV src1 src2));\n+  format %{ \"vand $dst, $src1, $src2\" %}\n+  ins_encode %{\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this);\n+    if(length_in_bytes <= 16) {\n+      __ andr($dst$$FloatRegister, length_in_bytes == 16 ? __ T16B : __ T8B,\n+              $src1$$FloatRegister, $src2$$FloatRegister);\n+    } else {\n+      assert(UseSVE > 0, \"must be sve\");\n+      __ sve_and($dst$$FloatRegister, $src1$$FloatRegister, $src2$$FloatRegister);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector and - predicated\n+\n+instruct vand_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (AndV (Binary dst_src1 src2) pg));\n+  format %{ \"vand_masked $dst_src1, $pg, $dst_src1, $src2\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ sve_and($dst_src1$$FloatRegister, __ elemType_to_regVariant(bt),\n+               $pg$$PRegister, $src2$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector and reg imm (unpredicated)\n+\n+instruct vandImmB(vReg dst_src, immBLog con) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (AndV dst_src (ReplicateB con)));\n+  format %{ \"vandImmB $dst_src, $dst_src, $con\" %}\n+  ins_encode %{\n+    __ sve_and($dst_src$$FloatRegister, __ B, (uint64_t)($con$$constant));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vandImmS(vReg dst_src, immSLog con) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (AndV dst_src (ReplicateS con)));\n+  format %{ \"vandImmS $dst_src, $dst_src, $con\" %}\n+  ins_encode %{\n+    __ sve_and($dst_src$$FloatRegister, __ H, (uint64_t)($con$$constant));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vandImmI(vReg dst_src, immILog con) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (AndV dst_src (ReplicateI con)));\n+  format %{ \"vandImmI $dst_src, $dst_src, $con\" %}\n+  ins_encode %{\n+    __ sve_and($dst_src$$FloatRegister, __ S, (uint64_t)($con$$constant));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vandImmL(vReg dst_src, immLLog con) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (AndV dst_src (ReplicateL con)));\n+  format %{ \"vandImmL $dst_src, $dst_src, $con\" %}\n+  ins_encode %{\n+    __ sve_and($dst_src$$FloatRegister, __ D, (uint64_t)($con$$constant));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ Vector or ------------------------------------\n+\n+\/\/ vector or\n+\n+instruct vor(vReg dst, vReg src1, vReg src2) %{\n+  match(Set dst (OrV src1 src2));\n+  format %{ \"vor $dst, $src1, $src2\" %}\n+  ins_encode %{\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this);\n+    if(length_in_bytes <= 16) {\n+      __ orr($dst$$FloatRegister, length_in_bytes == 16 ? __ T16B : __ T8B,\n+             $src1$$FloatRegister, $src2$$FloatRegister);\n+    } else {\n+      assert(UseSVE > 0, \"must be sve\");\n+      __ sve_orr($dst$$FloatRegister, $src1$$FloatRegister, $src2$$FloatRegister);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector or - predicated\n+\n+instruct vor_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (OrV (Binary dst_src1 src2) pg));\n+  format %{ \"vor_masked $dst_src1, $pg, $dst_src1, $src2\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ sve_orr($dst_src1$$FloatRegister, __ elemType_to_regVariant(bt),\n+               $pg$$PRegister, $src2$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector or reg imm (unpredicated)\n+\n+instruct vorImmB(vReg dst_src, immBLog con) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (OrV dst_src (ReplicateB con)));\n+  format %{ \"vorImmB $dst_src, $dst_src, $con\" %}\n+  ins_encode %{\n+    __ sve_orr($dst_src$$FloatRegister, __ B, (uint64_t)($con$$constant));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vorImmS(vReg dst_src, immSLog con) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (OrV dst_src (ReplicateS con)));\n+  format %{ \"vorImmS $dst_src, $dst_src, $con\" %}\n+  ins_encode %{\n+    __ sve_orr($dst_src$$FloatRegister, __ H, (uint64_t)($con$$constant));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vorImmI(vReg dst_src, immILog con) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (OrV dst_src (ReplicateI con)));\n+  format %{ \"vorImmI $dst_src, $dst_src, $con\" %}\n+  ins_encode %{\n+    __ sve_orr($dst_src$$FloatRegister, __ S, (uint64_t)($con$$constant));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vorImmL(vReg dst_src, immLLog con) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (OrV dst_src (ReplicateL con)));\n+  format %{ \"vorImmL $dst_src, $dst_src, $con\" %}\n+  ins_encode %{\n+    __ sve_orr($dst_src$$FloatRegister, __ D, (uint64_t)($con$$constant));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ Vector xor -----------------------------------\n+\n+\/\/ vector xor\n+\n+instruct vxor(vReg dst, vReg src1, vReg src2) %{\n+  match(Set dst (XorV src1 src2));\n+  format %{ \"vxor $dst, $src1, $src2\" %}\n+  ins_encode %{\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this);\n+    if(length_in_bytes <= 16) {\n+      __ eor($dst$$FloatRegister, length_in_bytes == 16 ? __ T16B : __ T8B,\n+             $src1$$FloatRegister, $src2$$FloatRegister);\n+    } else {\n+      assert(UseSVE > 0, \"must be sve\");\n+      __ sve_eor($dst$$FloatRegister, $src1$$FloatRegister, $src2$$FloatRegister);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector xor - predicated\n+\n+instruct vxor_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (XorV (Binary dst_src1 src2) pg));\n+  format %{ \"vxor_masked $dst_src1, $pg, $dst_src1, $src2\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ sve_eor($dst_src1$$FloatRegister, __ elemType_to_regVariant(bt),\n+               $pg$$PRegister, $src2$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector xor reg imm (unpredicated)\n+\n+instruct vxorImmB(vReg dst_src, immBLog con) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (XorV dst_src (ReplicateB con)));\n+  format %{ \"vxorImmB $dst_src, $dst_src, $con\" %}\n+  ins_encode %{\n+    __ sve_eor($dst_src$$FloatRegister, __ B, (uint64_t)($con$$constant));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vxorImmS(vReg dst_src, immSLog con) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (XorV dst_src (ReplicateS con)));\n+  format %{ \"vxorImmS $dst_src, $dst_src, $con\" %}\n+  ins_encode %{\n+    __ sve_eor($dst_src$$FloatRegister, __ H, (uint64_t)($con$$constant));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vxorImmI(vReg dst_src, immILog con) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (XorV dst_src (ReplicateI con)));\n+  format %{ \"vxorImmI $dst_src, $dst_src, $con\" %}\n+  ins_encode %{\n+    __ sve_eor($dst_src$$FloatRegister, __ S, (uint64_t)($con$$constant));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vxorImmL(vReg dst_src, immLLog con) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (XorV dst_src (ReplicateL con)));\n+  format %{ \"vxorImmL $dst_src, $dst_src, $con\" %}\n+  ins_encode %{\n+    __ sve_eor($dst_src$$FloatRegister, __ D, (uint64_t)($con$$constant));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ Vector not -----------------------------------\n+\n+\/\/ vector not\n+\n+instruct vnotI(vReg dst, vReg src, immI_M1 m1) %{\n+  match(Set dst (XorV src (ReplicateB m1)));\n+  match(Set dst (XorV src (ReplicateS m1)));\n+  match(Set dst (XorV src (ReplicateI m1)));\n+  format %{ \"vnotI $dst, $src\" %}\n+  ins_encode %{\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this);\n+    if (length_in_bytes <= 16) {\n+      __ notr($dst$$FloatRegister, length_in_bytes == 16 ? __ T16B : __ T8B,\n+              $src$$FloatRegister);\n+    } else {\n+      assert(UseSVE > 0, \"must be sve\");\n+      __ sve_not($dst$$FloatRegister, __ D, ptrue, $src$$FloatRegister);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vnotL(vReg dst, vReg src, immL_M1 m1) %{\n+  match(Set dst (XorV src (ReplicateL m1)));\n+  format %{ \"vnotL $dst, $src\" %}\n+  ins_encode %{\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this);\n+    if (length_in_bytes == 16) {\n+      __ notr($dst$$FloatRegister, __ T16B, $src$$FloatRegister);\n+    } else {\n+      assert(UseSVE > 0 && length_in_bytes > 16, \"must be\");\n+      __ sve_not($dst$$FloatRegister, __ D, ptrue, $src$$FloatRegister);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector not - predicated\n+\n+instruct vnotI_masked(vReg dst_src, immI_M1 m1, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (XorV (Binary dst_src (ReplicateB m1)) pg));\n+  match(Set dst_src (XorV (Binary dst_src (ReplicateS m1)) pg));\n+  match(Set dst_src (XorV (Binary dst_src (ReplicateI m1)) pg));\n+  format %{ \"vnotI_masked $dst_src, $pg, $dst_src\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ sve_not($dst_src$$FloatRegister, __ elemType_to_regVariant(bt),\n+               $pg$$PRegister, $dst_src$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vnotL_masked(vReg dst_src, immL_M1 m1, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (XorV (Binary dst_src (ReplicateL m1)) pg));\n+  format %{ \"vnotL_masked $dst_src, $pg, $dst_src\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ sve_not($dst_src$$FloatRegister, __ elemType_to_regVariant(bt),\n+               $pg$$PRegister, $dst_src$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ Vector and_not -------------------------------\n+\n+\/\/ vector and_not\n+\n+instruct vand_notI(vReg dst, vReg src1, vReg src2, immI_M1 m1) %{\n+  match(Set dst (AndV src1 (XorV src2 (ReplicateB m1))));\n+  match(Set dst (AndV src1 (XorV src2 (ReplicateS m1))));\n+  match(Set dst (AndV src1 (XorV src2 (ReplicateI m1))));\n+  format %{ \"vand_notI $dst, $src1, $src2\" %}\n+  ins_encode %{\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this);\n+    if (length_in_bytes <= 16) {\n+      __ bic($dst$$FloatRegister, length_in_bytes == 16 ? __ T16B : __ T8B,\n+             $src1$$FloatRegister, $src2$$FloatRegister);\n+    } else {\n+      assert(UseSVE > 0, \"must be sve\");\n+      __ sve_bic($dst$$FloatRegister, $src1$$FloatRegister, $src2$$FloatRegister);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vand_notL(vReg dst, vReg src1, vReg src2, immL_M1 m1) %{\n+  match(Set dst (AndV src1 (XorV src2 (ReplicateL m1))));\n+  format %{ \"vand_notL $dst, $src1, $src2\" %}\n+  ins_encode %{\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this);\n+    if (length_in_bytes == 16) {\n+      __ bic($dst$$FloatRegister, __ T16B, $src1$$FloatRegister, $src2$$FloatRegister);\n+    } else {\n+      assert(UseSVE > 0 && length_in_bytes > 16, \"must be\");\n+      __ sve_bic($dst$$FloatRegister, $src1$$FloatRegister, $src2$$FloatRegister);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector and_not - predicated\n+\n+instruct vand_notI_masked(vReg dst_src1, vReg src2, immI_M1 m1, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (AndV (Binary dst_src1 (XorV src2 (ReplicateB m1))) pg));\n+  match(Set dst_src1 (AndV (Binary dst_src1 (XorV src2 (ReplicateS m1))) pg));\n+  match(Set dst_src1 (AndV (Binary dst_src1 (XorV src2 (ReplicateI m1))) pg));\n+  format %{ \"vand_notI_masked $dst_src1, $pg, $dst_src1, $src2\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ sve_bic($dst_src1$$FloatRegister, __ elemType_to_regVariant(bt),\n+               $pg$$PRegister, $src2$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vand_notL_masked(vReg dst_src1, vReg src2, immL_M1 m1, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (AndV (Binary dst_src1 (XorV src2 (ReplicateL m1))) pg));\n+  format %{ \"vand_notL_masked $dst_src1, $pg, $dst_src1, $src2\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ sve_bic($dst_src1$$FloatRegister, __ elemType_to_regVariant(bt),\n+               $pg$$PRegister, $src2$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ Vector abs -----------------------------------\n+\n+\/\/ vector abs\n+\n+instruct vabs(vReg dst, vReg src) %{\n+  predicate(!n->as_Vector()->is_predicated_vector());\n+  match(Set dst (AbsVB src));\n+  match(Set dst (AbsVS src));\n+  match(Set dst (AbsVI src));\n+  match(Set dst (AbsVL src));\n+  match(Set dst (AbsVF src));\n+  match(Set dst (AbsVD src));\n+  format %{ \"vabs $dst, $src\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this);\n+    if (length_in_bytes <= 16) {\n+      if (is_floating_point_type(bt)) {\n+        __ fabs($dst$$FloatRegister, get_arrangement(this), $src$$FloatRegister);\n+      } else {\n+        assert(is_integral_type(bt), \"unsupported type\");\n+        __ absr($dst$$FloatRegister, get_arrangement(this), $src$$FloatRegister);\n+      }\n+    } else {\n+      assert(UseSVE > 0, \"must be sve\");\n+      if (is_floating_point_type(bt)) {\n+        __ sve_fabs($dst$$FloatRegister, __ elemType_to_regVariant(bt),\n+                    ptrue, $src$$FloatRegister);\n+      } else {\n+        assert(is_integral_type(bt), \"unsupported type\");\n+        __ sve_abs($dst$$FloatRegister, __ elemType_to_regVariant(bt),\n+                   ptrue, $src$$FloatRegister);\n+      }\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector abs - predicated\n+\n+instruct vabs_masked(vReg dst_src, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (AbsVB dst_src pg));\n+  match(Set dst_src (AbsVS dst_src pg));\n+  match(Set dst_src (AbsVI dst_src pg));\n+  match(Set dst_src (AbsVL dst_src pg));\n+  match(Set dst_src (AbsVF dst_src pg));\n+  match(Set dst_src (AbsVD dst_src pg));\n+  format %{ \"vabs_masked $dst_src, $pg, $dst_src\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    if (is_floating_point_type(bt)) {\n+      __ sve_fabs($dst_src$$FloatRegister, __ elemType_to_regVariant(bt),\n+                  $pg$$PRegister, $dst_src$$FloatRegister);\n+    } else {\n+      assert(is_integral_type(bt), \"unsupported type\");\n+      __ sve_abs($dst_src$$FloatRegister, __ elemType_to_regVariant(bt),\n+                 $pg$$PRegister, $dst_src$$FloatRegister);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ Vector fabd ----------------------------------\n+\n+\/\/ vector fabs diff\n+\n+instruct vfabd(vReg dst, vReg src1, vReg src2) %{\n+  predicate(Matcher::vector_length_in_bytes(n) <= 16);\n+  match(Set dst (AbsVF (SubVF src1 src2)));\n+  match(Set dst (AbsVD (SubVD src1 src2)));\n+  format %{ \"vfabd $dst, $src1, $src2\\t# vector <= 128 bits\" %}\n+  ins_encode %{\n+    __ fabd($dst$$FloatRegister, get_arrangement(this),\n+            $src1$$FloatRegister, $src2$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ Vector neg -----------------------------------\n+\n+\/\/ vector neg\n+\n+instruct vneg(vReg dst, vReg src) %{\n+  predicate(!n->as_Vector()->is_predicated_vector());\n+  match(Set dst (NegVI src));\n+  match(Set dst (NegVL src));\n+  match(Set dst (NegVF src));\n+  match(Set dst (NegVD src));\n+  format %{ \"vneg $dst, $src\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this);\n+    if (length_in_bytes <= 16) {\n+      if (is_floating_point_type(bt)) {\n+        __ fneg($dst$$FloatRegister, get_arrangement(this), $src$$FloatRegister);\n+      } else {\n+        assert(is_integral_type(bt), \"unsupported type\");\n+        __ negr($dst$$FloatRegister, get_arrangement(this), $src$$FloatRegister);\n+      }\n+    } else {\n+      assert(UseSVE > 0, \"must be sve\");\n+      if (is_floating_point_type(bt)) {\n+        __ sve_fneg($dst$$FloatRegister, __ elemType_to_regVariant(bt),\n+                    ptrue, $src$$FloatRegister);\n+      } else {\n+        assert(is_integral_type(bt), \"unsupported type\");\n+        __ sve_neg($dst$$FloatRegister, __ elemType_to_regVariant(bt),\n+                   ptrue, $src$$FloatRegister);\n+      }\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector neg - predicated\n+\n+instruct vneg_masked(vReg dst_src, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (NegVI dst_src pg));\n+  match(Set dst_src (NegVL dst_src pg));\n+  match(Set dst_src (NegVF dst_src pg));\n+  match(Set dst_src (NegVD dst_src pg));\n+  format %{ \"vneg_masked $dst_src, $pg, $dst_src\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    if (is_floating_point_type(bt)) {\n+      __ sve_fneg($dst_src$$FloatRegister, __ elemType_to_regVariant(bt),\n+                  $pg$$PRegister, $dst_src$$FloatRegister);\n+    } else {\n+      assert(is_integral_type(bt), \"unsupported type\");\n+      __ sve_neg($dst_src$$FloatRegister, __ elemType_to_regVariant(bt),\n+                 $pg$$PRegister, $dst_src$$FloatRegister);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ Vector sqrt ----------------------------------\n+\n+\/\/ vector sqrt\n+\n+instruct vsqrt(vReg dst, vReg src) %{\n+  predicate(!n->as_Vector()->is_predicated_vector());\n+  match(Set dst (SqrtVF src));\n+  match(Set dst (SqrtVD src));\n+  format %{ \"vsqrt $dst, $src\" %}\n+  ins_encode %{\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this);\n+    if (length_in_bytes <= 16) {\n+      __ fsqrt($dst$$FloatRegister, get_arrangement(this), $src$$FloatRegister);\n+    } else {\n+      assert(UseSVE > 0, \"must be sve\");\n+      BasicType bt = Matcher::vector_element_basic_type(this);\n+      __ sve_fsqrt($dst$$FloatRegister, __ elemType_to_regVariant(bt),\n+                   ptrue, $src$$FloatRegister);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector sqrt - predicated\n+\n+instruct vsqrt_masked(vReg dst_src, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (SqrtVF dst_src pg));\n+  match(Set dst_src (SqrtVD dst_src pg));\n+  format %{ \"vsqrt_masked $dst_src, $pg, $dst_src\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ sve_fsqrt($dst_src$$FloatRegister, __ elemType_to_regVariant(bt),\n+                 $pg$$PRegister, $dst_src$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ Vector min -----------------------------------\n+\n+\/\/ vector min - LONG\n+\n+instruct vminL_neon(vReg dst, vReg src1, vReg src2) %{\n+  predicate(UseSVE == 0 && Matcher::vector_element_basic_type(n) == T_LONG);\n+  match(Set dst (MinV src1 src2));\n+  effect(TEMP_DEF dst);\n+  format %{ \"vminL_neon $dst, $src1, $src2\\t# 2L\" %}\n+  ins_encode %{\n+    __ cmgt($dst$$FloatRegister, __ T2D, $src1$$FloatRegister, $src2$$FloatRegister);\n+    __ bsl($dst$$FloatRegister, __ T16B, $src2$$FloatRegister, $src1$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vminL_sve(vReg dst_src1, vReg src2) %{\n+  predicate(UseSVE > 0 && Matcher::vector_element_basic_type(n) == T_LONG);\n+  match(Set dst_src1 (MinV dst_src1 src2));\n+  format %{ \"vminL_sve $dst_src1, $dst_src1, $src2\" %}\n+  ins_encode %{\n+    __ sve_smin($dst_src1$$FloatRegister, __ D, ptrue, $src2$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector min - B\/S\/I\/F\/D\n+\n+instruct vmin_le128b(vReg dst, vReg src1, vReg src2) %{\n+  predicate(Matcher::vector_element_basic_type(n) != T_LONG &&\n+            Matcher::vector_length_in_bytes(n) <= 16);\n+  match(Set dst (MinV src1 src2));\n+  format %{ \"vmin_le128b $dst, $src1, $src2\\t# vector <= 128 bits. B\/S\/I\/F\/D\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    if (is_floating_point_type(bt)) {\n+      __ fmin($dst$$FloatRegister, get_arrangement(this),\n+              $src1$$FloatRegister, $src2$$FloatRegister);\n+    } else {\n+      assert(is_integral_type(bt) && bt != T_LONG, \"unsupported type\");\n+      __ minv($dst$$FloatRegister, get_arrangement(this),\n+              $src1$$FloatRegister, $src2$$FloatRegister);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmin_gt128b(vReg dst_src1, vReg src2) %{\n+  predicate(Matcher::vector_element_basic_type(n) != T_LONG &&\n+            Matcher::vector_length_in_bytes(n) > 16);\n+  match(Set dst_src1 (MinV dst_src1 src2));\n+  format %{ \"vmin_gt128b $dst_src1, $dst_src1, $src2\\t# vector > 128 bits. B\/S\/I\/F\/D\" %}\n+  ins_encode %{\n+    assert(UseSVE > 0, \"must be sve\");\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    if (is_floating_point_type(bt)) {\n+      __ sve_fmin($dst_src1$$FloatRegister, __ elemType_to_regVariant(bt),\n+                  ptrue, $src2$$FloatRegister);\n+    } else {\n+      assert(is_integral_type(bt) && bt != T_LONG, \"unsupported type\");\n+      __ sve_smin($dst_src1$$FloatRegister, __ elemType_to_regVariant(bt),\n+                  ptrue, $src2$$FloatRegister);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector min - predicated\n+\n+instruct vmin_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (MinV (Binary dst_src1 src2) pg));\n+  format %{ \"vmin_masked $dst_src1, $pg, $dst_src1, $src2\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    if (is_floating_point_type(bt)) {\n+      __ sve_fmin($dst_src1$$FloatRegister, __ elemType_to_regVariant(bt),\n+                  $pg$$PRegister, $src2$$FloatRegister);\n+    } else {\n+      assert(is_integral_type(bt), \"unsupported type\");\n+      __ sve_smin($dst_src1$$FloatRegister, __ elemType_to_regVariant(bt),\n+                  $pg$$PRegister, $src2$$FloatRegister);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ Vector max -----------------------------------\n+\n+\/\/ vector max - LONG\n+\n+instruct vmaxL_neon(vReg dst, vReg src1, vReg src2) %{\n+  predicate(UseSVE == 0 && Matcher::vector_element_basic_type(n) == T_LONG);\n+  match(Set dst (MaxV src1 src2));\n+  effect(TEMP_DEF dst);\n+  format %{ \"vmaxL_neon $dst, $src1, $src2\\t# 2L\" %}\n+  ins_encode %{\n+    __ cmgt($dst$$FloatRegister, __ T2D, $src1$$FloatRegister, $src2$$FloatRegister);\n+    __ bsl($dst$$FloatRegister, __ T16B, $src1$$FloatRegister, $src2$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmaxL_sve(vReg dst_src1, vReg src2) %{\n+  predicate(UseSVE > 0 && Matcher::vector_element_basic_type(n) == T_LONG);\n+  match(Set dst_src1 (MaxV dst_src1 src2));\n+  format %{ \"vmaxL_sve $dst_src1, $dst_src1, $src2\" %}\n+  ins_encode %{\n+    __ sve_smax($dst_src1$$FloatRegister, __ D, ptrue, $src2$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector max - B\/S\/I\/F\/D\n+\n+instruct vmax_le128b(vReg dst, vReg src1, vReg src2) %{\n+  predicate(Matcher::vector_element_basic_type(n) != T_LONG &&\n+            Matcher::vector_length_in_bytes(n) <= 16);\n+  match(Set dst (MaxV src1 src2));\n+  format %{ \"vmax_le128b $dst, $src1, $src2\\t# vector <= 128 bits. B\/S\/I\/F\/D\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    if (is_floating_point_type(bt)) {\n+      __ fmax($dst$$FloatRegister, get_arrangement(this),\n+              $src1$$FloatRegister, $src2$$FloatRegister);\n+    } else {\n+      assert(is_integral_type(bt) && bt != T_LONG, \"unsupported type\");\n+      __ maxv($dst$$FloatRegister, get_arrangement(this),\n+              $src1$$FloatRegister, $src2$$FloatRegister);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmax_gt128b(vReg dst_src1, vReg src2) %{\n+  predicate(Matcher::vector_element_basic_type(n) != T_LONG &&\n+            Matcher::vector_length_in_bytes(n) > 16);\n+  match(Set dst_src1 (MaxV dst_src1 src2));\n+  format %{ \"vmax_gt128b $dst_src1, $dst_src1, $src2\\t# vector > 128 bits. B\/S\/I\/F\/D\" %}\n+  ins_encode %{\n+    assert(UseSVE > 0, \"must be sve\");\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    if (is_floating_point_type(bt)) {\n+      __ sve_fmax($dst_src1$$FloatRegister, __ elemType_to_regVariant(bt),\n+                  ptrue, $src2$$FloatRegister);\n+    } else {\n+      assert(is_integral_type(bt) && bt != T_LONG, \"unsupported type\");\n+      __ sve_smax($dst_src1$$FloatRegister, __ elemType_to_regVariant(bt),\n+                  ptrue, $src2$$FloatRegister);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector max - predicated\n+\n+instruct vmax_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (MaxV (Binary dst_src1 src2) pg));\n+  format %{ \"vmax_masked $dst_src1, $pg, $dst_src1, $src2\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    if (is_floating_point_type(bt)) {\n+      __ sve_fmax($dst_src1$$FloatRegister, __ elemType_to_regVariant(bt),\n+                  $pg$$PRegister, $src2$$FloatRegister);\n+    } else {\n+      assert(is_integral_type(bt), \"unsupported type\");\n+      __ sve_smax($dst_src1$$FloatRegister, __ elemType_to_regVariant(bt),\n+                  $pg$$PRegister, $src2$$FloatRegister);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ MLA RELATED ----------------------------------\n+\n+\/\/ vector mla\n+\/\/ dst_src1 = dst_src1 + src2 * src3\n+\n+instruct vmla(vReg dst_src1, vReg src2, vReg src3) %{\n+  match(Set dst_src1 (AddVB dst_src1 (MulVB src2 src3)));\n+  match(Set dst_src1 (AddVS dst_src1 (MulVS src2 src3)));\n+  match(Set dst_src1 (AddVI dst_src1 (MulVI src2 src3)));\n+  match(Set dst_src1 (AddVL dst_src1 (MulVL src2 src3)));\n+  format %{ \"vmla $dst_src1, src2, src3\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this);\n+    if (length_in_bytes <= 16 && bt != T_LONG) {\n+      \/\/ mlav cannot accept T2D arrangement in neon\n+      __ mlav($dst_src1$$FloatRegister, get_arrangement(this),\n+              $src2$$FloatRegister, $src3$$FloatRegister);\n+    } else {\n+      assert(UseSVE > 0, \"must be sve\");\n+      __ sve_mla($dst_src1$$FloatRegister, __ elemType_to_regVariant(bt),\n+                 ptrue, $src2$$FloatRegister, $src3$$FloatRegister);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmla_masked(vReg dst_src1, vReg src2, vReg src3, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (AddVB (Binary dst_src1 (MulVB src2 src3)) pg));\n+  match(Set dst_src1 (AddVS (Binary dst_src1 (MulVS src2 src3)) pg));\n+  match(Set dst_src1 (AddVI (Binary dst_src1 (MulVI src2 src3)) pg));\n+  match(Set dst_src1 (AddVL (Binary dst_src1 (MulVL src2 src3)) pg));\n+  format %{ \"vmla_masked $dst_src1, $pg, src2, src3\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ sve_mla($dst_src1$$FloatRegister, __ elemType_to_regVariant(bt),\n+               $pg$$PRegister, $src2$$FloatRegister, $src3$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector fmla\n+\/\/ dst_src1 = dst_src1 + src2 * src3\n+\n+instruct vfmla(vReg dst_src1, vReg src2, vReg src3) %{\n+  predicate(UseFMA);\n+  match(Set dst_src1 (FmaVF dst_src1 (Binary src2 src3)));\n+  match(Set dst_src1 (FmaVD dst_src1 (Binary src2 src3)));\n+  format %{ \"vfmla $dst_src1, $src2, $src3\" %}\n+  ins_encode %{\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this);\n+    if (length_in_bytes <= 16) {\n+      __ fmla($dst_src1$$FloatRegister, get_arrangement(this),\n+              $src2$$FloatRegister, $src3$$FloatRegister);\n+    } else {\n+      assert(UseSVE > 0, \"must be sve\");\n+      BasicType bt = Matcher::vector_element_basic_type(this);\n+      __ sve_fmla($dst_src1$$FloatRegister, __ elemType_to_regVariant(bt),\n+                  ptrue, $src2$$FloatRegister, $src3$$FloatRegister);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector fmad - predicated\n+\/\/ dst_src1 = dst_src1 * src2 + src3\n+\n+instruct vfmad_masked(vReg dst_src1, vReg src2, vReg src3, pRegGov pg) %{\n+  predicate(UseFMA && UseSVE > 0);\n+  match(Set dst_src1 (FmaVF (Binary dst_src1 src2) (Binary src3 pg)));\n+  match(Set dst_src1 (FmaVD (Binary dst_src1 src2) (Binary src3 pg)));\n+  format %{ \"vfmad_masked $dst_src1, $pg, $src2, $src3\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ sve_fmad($dst_src1$$FloatRegister, __ elemType_to_regVariant(bt),\n+                $pg$$PRegister, $src2$$FloatRegister, $src3$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector mls\n+\/\/ dst_src1 = dst_src1 - src2 * src3\n+\n+instruct vmls(vReg dst_src1, vReg src2, vReg src3) %{\n+  match(Set dst_src1 (SubVB dst_src1 (MulVB src2 src3)));\n+  match(Set dst_src1 (SubVS dst_src1 (MulVS src2 src3)));\n+  match(Set dst_src1 (SubVI dst_src1 (MulVI src2 src3)));\n+  match(Set dst_src1 (SubVL dst_src1 (MulVL src2 src3)));\n+  format %{ \"vmls $dst_src1, src2, src3\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this);\n+    if (length_in_bytes <= 16 && bt != T_LONG) {\n+      \/\/ mlsv cannot accept T2D arrangement in neon\n+      __ mlsv($dst_src1$$FloatRegister, get_arrangement(this),\n+              $src2$$FloatRegister, $src3$$FloatRegister);\n+    } else {\n+      assert(UseSVE > 0, \"must be sve\");\n+      __ sve_mls($dst_src1$$FloatRegister, __ elemType_to_regVariant(bt),\n+                 ptrue, $src2$$FloatRegister, $src3$$FloatRegister);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmls_masked(vReg dst_src1, vReg src2, vReg src3, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (SubVB (Binary dst_src1 (MulVB src2 src3)) pg));\n+  match(Set dst_src1 (SubVS (Binary dst_src1 (MulVS src2 src3)) pg));\n+  match(Set dst_src1 (SubVI (Binary dst_src1 (MulVI src2 src3)) pg));\n+  match(Set dst_src1 (SubVL (Binary dst_src1 (MulVL src2 src3)) pg));\n+  format %{ \"vmls_masked $dst_src1, $pg, src2, src3\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ sve_mls($dst_src1$$FloatRegister, __ elemType_to_regVariant(bt),\n+               $pg$$PRegister, $src2$$FloatRegister, $src3$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector fmls\n+\n+\/\/ dst_src1 = dst_src1 + -src2 * src3\n+\/\/ The NegVF\/NegVD must not be predicated.\n+instruct vfmls1(vReg dst_src1, vReg src2, vReg src3) %{\n+  predicate(UseFMA && !n->in(2)->in(1)->as_Vector()->is_predicated_vector());\n+  match(Set dst_src1 (FmaVF dst_src1 (Binary (NegVF src2) src3)));\n+  match(Set dst_src1 (FmaVD dst_src1 (Binary (NegVD src2) src3)));\n+  format %{ \"vfmls1 $dst_src1, $src2, $src3\" %}\n+  ins_encode %{\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this);\n+    if (length_in_bytes <= 16) {\n+      __ fmls($dst_src1$$FloatRegister, get_arrangement(this),\n+              $src2$$FloatRegister, $src3$$FloatRegister);\n+    } else {\n+      assert(UseSVE > 0, \"must be sve\");\n+      BasicType bt = Matcher::vector_element_basic_type(this);\n+      __ sve_fmls($dst_src1$$FloatRegister, __ elemType_to_regVariant(bt),\n+                  ptrue, $src2$$FloatRegister, $src3$$FloatRegister);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ dst_src1 = dst_src1 + src2 * -src3\n+\/\/ The NegVF\/NegVD must not be predicated.\n+instruct vfmls2(vReg dst_src1, vReg src2, vReg src3) %{\n+  predicate(UseFMA && !n->in(2)->in(2)->as_Vector()->is_predicated_vector());\n+  match(Set dst_src1 (FmaVF dst_src1 (Binary src2 (NegVF src3))));\n+  match(Set dst_src1 (FmaVD dst_src1 (Binary src2 (NegVD src3))));\n+  format %{ \"vfmls2 $dst_src1, $src2, $src3\" %}\n+  ins_encode %{\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this);\n+    if (length_in_bytes <= 16) {\n+      __ fmls($dst_src1$$FloatRegister, get_arrangement(this),\n+              $src2$$FloatRegister, $src3$$FloatRegister);\n+    } else {\n+      assert(UseSVE > 0, \"must be sve\");\n+      BasicType bt = Matcher::vector_element_basic_type(this);\n+      __ sve_fmls($dst_src1$$FloatRegister, __ elemType_to_regVariant(bt),\n+                  ptrue, $src2$$FloatRegister, $src3$$FloatRegister);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector fmsb - predicated\n+\n+\/\/ dst_src1 = dst_src1 * -src2 + src3\n+\/\/ The NegVF\/NegVD must not be predicated.\n+instruct vfmsb_masked(vReg dst_src1, vReg src2, vReg src3, pRegGov pg) %{\n+  predicate(UseFMA && UseSVE > 0 &&\n+            !n->in(1)->in(2)->as_Vector()->is_predicated_vector());\n+  match(Set dst_src1 (FmaVF (Binary dst_src1 (NegVF src2)) (Binary src3 pg)));\n+  match(Set dst_src1 (FmaVD (Binary dst_src1 (NegVD src2)) (Binary src3 pg)));\n+  format %{ \"vfmsb_masked $dst_src1, $pg, $src2, $src3\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ sve_fmsb($dst_src1$$FloatRegister, __ elemType_to_regVariant(bt),\n+                $pg$$PRegister, $src2$$FloatRegister, $src3$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector fnmla (sve)\n+\n+\/\/ dst_src1 = -dst_src1 + -src2 * src3\n+\/\/ The NegVF\/NegVD must not be predicated.\n+instruct vfnmla1(vReg dst_src1, vReg src2, vReg src3) %{\n+  predicate(UseFMA && UseSVE > 0 &&\n+            !n->in(1)->as_Vector()->is_predicated_vector() &&\n+            !n->in(2)->in(1)->as_Vector()->is_predicated_vector());\n+  match(Set dst_src1 (FmaVF (NegVF dst_src1) (Binary (NegVF src2) src3)));\n+  match(Set dst_src1 (FmaVD (NegVD dst_src1) (Binary (NegVD src2) src3)));\n+  format %{ \"vfnmla1 $dst_src1, $src2, $src3\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ sve_fnmla($dst_src1$$FloatRegister, __ elemType_to_regVariant(bt),\n+                 ptrue, $src2$$FloatRegister, $src3$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ dst_src1 = -dst_src1 + src2 * -src3\n+\/\/ The NegVF\/NegVD must not be predicated.\n+instruct vfnmla2(vReg dst_src1, vReg src2, vReg src3) %{\n+  predicate(UseFMA && UseSVE > 0 &&\n+            !n->in(1)->as_Vector()->is_predicated_vector() &&\n+            !n->in(2)->in(2)->as_Vector()->is_predicated_vector());\n+  match(Set dst_src1 (FmaVF (NegVF dst_src1) (Binary src2 (NegVF src3))));\n+  match(Set dst_src1 (FmaVD (NegVD dst_src1) (Binary src2 (NegVD src3))));\n+  format %{ \"vfnmla2 $dst_src1, $src2, $src3\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ sve_fnmla($dst_src1$$FloatRegister, __ elemType_to_regVariant(bt),\n+                 ptrue, $src2$$FloatRegister, $src3$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector fnmad - predicated\n+\n+\/\/ dst_src1 = -src3 + dst_src1 * -src2\n+\/\/ The NegVF\/NegVD must not be predicated.\n+instruct vfnmad_masked(vReg dst_src1, vReg src2, vReg src3, pRegGov pg) %{\n+  predicate(UseFMA && UseSVE > 0 &&\n+            !n->in(1)->in(2)->as_Vector()->is_predicated_vector() &&\n+            !n->in(2)->in(1)->as_Vector()->is_predicated_vector());\n+  match(Set dst_src1 (FmaVF (Binary dst_src1 (NegVF src2)) (Binary (NegVF src3) pg)));\n+  match(Set dst_src1 (FmaVD (Binary dst_src1 (NegVD src2)) (Binary (NegVD src3) pg)));\n+  format %{ \"vfnmad_masked $dst_src1, $pg, $src2, $src3\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ sve_fnmad($dst_src1$$FloatRegister, __ elemType_to_regVariant(bt),\n+                 $pg$$PRegister, $src2$$FloatRegister, $src3$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector fnmls (sve)\n+\n+\/\/ dst_src1 = -dst_src1 + src2 * src3\n+\/\/ The NegVF\/NegVD must not be predicated.\n+instruct vfnmls(vReg dst_src1, vReg src2, vReg src3) %{\n+  predicate(UseFMA && UseSVE > 0 &&\n+            !n->in(1)->as_Vector()->is_predicated_vector());\n+  match(Set dst_src1 (FmaVF (NegVF dst_src1) (Binary src2 src3)));\n+  match(Set dst_src1 (FmaVD (NegVD dst_src1) (Binary src2 src3)));\n+  format %{ \"vfnmls $dst_src1, $src2, $src3\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ sve_fnmls($dst_src1$$FloatRegister, __ elemType_to_regVariant(bt),\n+                 ptrue, $src2$$FloatRegister, $src3$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector fnmsb - predicated\n+\n+\/\/ dst_src1 = -src3 + dst_src1 * src2\n+\/\/ The NegVF\/NegVD must not be predicated.\n+instruct vfnmsb_masked(vReg dst_src1, vReg src2, vReg src3, pRegGov pg) %{\n+  predicate(UseFMA && UseSVE > 0 &&\n+            !n->in(2)->in(1)->as_Vector()->is_predicated_vector());\n+  match(Set dst_src1 (FmaVF (Binary dst_src1 src2) (Binary (NegVF src3) pg)));\n+  match(Set dst_src1 (FmaVD (Binary dst_src1 src2) (Binary (NegVD src3) pg)));\n+  format %{ \"vfnmsb_masked $dst_src1, $pg, $src2, $src3\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ sve_fnmsb($dst_src1$$FloatRegister, __ elemType_to_regVariant(bt),\n+                 $pg$$PRegister, $src2$$FloatRegister, $src3$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ MulAddVS2VI\n+\/\/ Vector Multiply-Add Shorts into Integer\n+\n+instruct vmuladdS2I(vReg dst, vReg src1, vReg src2, vReg tmp) %{\n+  predicate(Matcher::vector_length_in_bytes(n) == 16 &&\n+            Matcher::vector_element_basic_type(n->in(1)) == T_SHORT);\n+  match(Set dst (MulAddVS2VI src1 src2));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"vmuladdS2I $dst, $src1, $src2\\t# KILL $tmp\" %}\n+  ins_encode %{\n+    __ smullv($tmp$$FloatRegister, __ T4H, $src1$$FloatRegister, $src2$$FloatRegister);\n+    __ smullv($dst$$FloatRegister, __ T8H, $src1$$FloatRegister, $src2$$FloatRegister);\n+    __ addpv($dst$$FloatRegister, __ T4S, $tmp$$FloatRegister, $dst$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ Vector shift ---------------------------------\n+\n+\/\/ Vector right shift in AArch64 ASIMD\n+\/\/\n+\/\/ Right shifts with vector shift count on AArch64 ASIMD are implemented\n+\/\/ as left shift by negative shift count.\n+\/\/ There are two cases for vector shift count.\n+\/\/\n+\/\/ Case 1: The vector shift count is from replication.\n+\/\/        |            |\n+\/\/    LoadVector  RShiftCntV\n+\/\/        |       \/\n+\/\/     RShiftVI\n+\/\/\n+\/\/ Case 2: The vector shift count is from loading.\n+\/\/ This case isn't supported by middle-end now. But it's supported by\n+\/\/ panama\/vectorIntrinsics(JEP 338: Vector API).\n+\/\/        |            |\n+\/\/    LoadVector  LoadVector\n+\/\/        |       \/\n+\/\/     RShiftVI\n+\/\/\n+\/\/ The negate is conducted in RShiftCntV rule for case 1, whereas it's done in\n+\/\/ RShiftV* rules for case 2. Because there exists an optimization opportunity\n+\/\/ for case 1, that is, multiple neg instructions in inner loop can be hoisted\n+\/\/ to outer loop and merged into one neg instruction.\n+\/\/\n+\/\/ Note that ShiftVNode::is_var_shift() indicates whether the vector shift\n+\/\/ count is a variable vector(case 2) or not(a vector generated by RShiftCntV,\n+\/\/ i.e. case 1).\n+\/\/\n+\/\/ Note that we may generate low-cost NEON instructions on 128-bit SVE machines\n+\/\/ for some nodes, e.g., LoadVector and left shifts. However, considering the\n+\/\/ overhead of negation, we conservatively generate SVE shift instructions for\n+\/\/ right shifts on 128-bit SVE machines.\n+\n+\/\/ vector shift count\n+\n+instruct vshiftcntL(vReg dst, iRegIorL2I cnt) %{\n+  match(Set dst (LShiftCntV cnt));\n+  format %{ \"vshiftcntL $dst, $cnt\" %}\n+  ins_encode %{\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this);\n+    if (length_in_bytes <= 16) {\n+      __ dup($dst$$FloatRegister, get_arrangement(this), $cnt$$Register);\n+    } else {\n+      assert(UseSVE > 0, \"must be sve\");\n+      BasicType bt = Matcher::vector_element_basic_type(this);\n+      __ sve_dup($dst$$FloatRegister, __ elemType_to_regVariant(bt), $cnt$$Register);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vshiftcntR(vReg dst, iRegIorL2I cnt) %{\n+  match(Set dst (RShiftCntV cnt));\n+  format %{ \"vshiftcntR $dst, $cnt\" %}\n+  ins_encode %{\n+    if (UseSVE == 0) {\n+      uint length_in_bytes = Matcher::vector_length_in_bytes(this);\n+      assert(length_in_bytes <= 16, \"must be\");\n+      __ negw(rscratch1, $cnt$$Register);\n+      __ dup($dst$$FloatRegister, get_arrangement(this), rscratch1);\n+    } else {\n+      BasicType bt = Matcher::vector_element_basic_type(this);\n+      __ sve_dup($dst$$FloatRegister, __ elemType_to_regVariant(bt), $cnt$$Register);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector shift left\n+\n+instruct vlsl_le128b(vReg dst, vReg src, vReg shift) %{\n+  predicate(Matcher::vector_length_in_bytes(n) <= 16);\n+  match(Set dst (LShiftVB src shift));\n+  match(Set dst (LShiftVS src shift));\n+  match(Set dst (LShiftVI src shift));\n+  match(Set dst (LShiftVL src shift));\n+  format %{ \"vlsl_le128b $dst, $src, $shift\\t# vector <= 128 bits\" %}\n+  ins_encode %{\n+    __ sshl($dst$$FloatRegister, get_arrangement(this),\n+            $src$$FloatRegister, $shift$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vlsl_gt128b(vReg dst_src, vReg shift) %{\n+  predicate(Matcher::vector_length_in_bytes(n) > 16);\n+  match(Set dst_src (LShiftVB dst_src shift));\n+  match(Set dst_src (LShiftVS dst_src shift));\n+  match(Set dst_src (LShiftVI dst_src shift));\n+  match(Set dst_src (LShiftVL dst_src shift));\n+  format %{ \"vlsl_gt128b $dst_src, $dst_src, $shift\\t# vector > 128 bits\" %}\n+  ins_encode %{\n+    assert(UseSVE > 0, \"must be sve\");\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ sve_lsl($dst_src$$FloatRegister, __ elemType_to_regVariant(bt),\n+               ptrue, $shift$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector shift right (arithmetic)\n+\n+instruct vasr_neon(vReg dst, vReg src, vReg shift) %{\n+  predicate(UseSVE == 0 && Matcher::vector_length_in_bytes(n) <= 16 &&\n+            !n->as_ShiftV()->is_var_shift());\n+  match(Set dst (RShiftVB src shift));\n+  match(Set dst (RShiftVS src shift));\n+  match(Set dst (RShiftVI src shift));\n+  match(Set dst (RShiftVL src shift));\n+  format %{ \"vasr_neon $dst, $src, $shift\\t# not variable shift\" %}\n+  ins_encode %{\n+    __ sshl($dst$$FloatRegister, get_arrangement(this),\n+            $src$$FloatRegister, $shift$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vasr_neon_var(vReg dst, vReg src, vReg shift) %{\n+  predicate(UseSVE == 0 && Matcher::vector_length_in_bytes(n) <= 16 &&\n+            n->as_ShiftV()->is_var_shift());\n+  match(Set dst (RShiftVB src shift));\n+  match(Set dst (RShiftVS src shift));\n+  match(Set dst (RShiftVI src shift));\n+  match(Set dst (RShiftVL src shift));\n+  effect(TEMP_DEF dst);\n+  format %{ \"vasr_neon_var $dst, $src, $shift\\t# variable shift\" %}\n+  ins_encode %{\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this);\n+    __ negr($dst$$FloatRegister, length_in_bytes == 16 ? __ T16B : __ T8B,\n+            $shift$$FloatRegister);\n+    __ sshl($dst$$FloatRegister, get_arrangement(this),\n+            $src$$FloatRegister, $dst$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vasr_sve(vReg dst_src, vReg shift) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (RShiftVB dst_src shift));\n+  match(Set dst_src (RShiftVS dst_src shift));\n+  match(Set dst_src (RShiftVI dst_src shift));\n+  match(Set dst_src (RShiftVL dst_src shift));\n+  format %{ \"vasr_sve $dst_src, $dst_src, $shift\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ sve_asr($dst_src$$FloatRegister, __ elemType_to_regVariant(bt),\n+               ptrue, $shift$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector shift right (logical)\n+\n+instruct vlsr_neon(vReg dst, vReg src, vReg shift) %{\n+  predicate(UseSVE == 0 && Matcher::vector_length_in_bytes(n) <= 16 &&\n+            !n->as_ShiftV()->is_var_shift());\n+  match(Set dst (URShiftVB src shift));\n+  match(Set dst (URShiftVS src shift));\n+  match(Set dst (URShiftVI src shift));\n+  match(Set dst (URShiftVL src shift));\n+  format %{ \"vlsr_neon $dst, $src, $shift\\t# not variable shift\" %}\n+  ins_encode %{\n+    __ ushl($dst$$FloatRegister, get_arrangement(this),\n+            $src$$FloatRegister, $shift$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vlsr_neon_var(vReg dst, vReg src, vReg shift) %{\n+  predicate(UseSVE == 0 && Matcher::vector_length_in_bytes(n) <= 16 &&\n+            n->as_ShiftV()->is_var_shift());\n+  match(Set dst (URShiftVB src shift));\n+  match(Set dst (URShiftVS src shift));\n+  match(Set dst (URShiftVI src shift));\n+  match(Set dst (URShiftVL src shift));\n+  effect(TEMP_DEF dst);\n+  format %{ \"vlsr_neon_var $dst, $src, $shift\\t# variable shift\" %}\n+  ins_encode %{\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this);\n+    __ negr($dst$$FloatRegister, length_in_bytes == 16 ? __ T16B : __ T8B,\n+            $shift$$FloatRegister);\n+    __ ushl($dst$$FloatRegister, get_arrangement(this),\n+            $src$$FloatRegister, $dst$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vlsr_sve(vReg dst_src, vReg shift) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (URShiftVB dst_src shift));\n+  match(Set dst_src (URShiftVS dst_src shift));\n+  match(Set dst_src (URShiftVI dst_src shift));\n+  match(Set dst_src (URShiftVL dst_src shift));\n+  format %{ \"vlsr_sve $dst_src, $dst_src, $shift\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ sve_lsr($dst_src$$FloatRegister, __ elemType_to_regVariant(bt),\n+               ptrue, $shift$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector shift with imm\n+\n+instruct vlsl_imm(vReg dst, vReg src, immI shift) %{\n+  predicate(assert_not_var_shift(n));\n+  match(Set dst (LShiftVB src (LShiftCntV shift)));\n+  match(Set dst (LShiftVS src (LShiftCntV shift)));\n+  match(Set dst (LShiftVI src (LShiftCntV shift)));\n+  match(Set dst (LShiftVL src (LShiftCntV shift)));\n+  format %{ \"vlsl_imm $dst, $src, $shift\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this);\n+    int con = (int)$shift$$constant;\n+    if (is_subword_type(bt)) {\n+      \/\/ Optimize for B\/S\n+      int esize_in_bits = type2aelembytes(bt) * BitsPerByte;\n+      if (con >= esize_in_bits) {\n+        if (length_in_bytes <= 16) {\n+          __ eor($dst$$FloatRegister, length_in_bytes == 16 ? __ T16B : __ T8B,\n+                 $src$$FloatRegister, $src$$FloatRegister);\n+        } else {\n+          assert(UseSVE > 0, \"must be sve\");\n+          __ sve_eor($dst$$FloatRegister, $src$$FloatRegister, $src$$FloatRegister);\n+        }\n+        return;\n+      }\n+    }\n+    if (length_in_bytes <= 16) {\n+      __ shl($dst$$FloatRegister, get_arrangement(this), $src$$FloatRegister, con);\n+    } else {\n+      assert(UseSVE > 0, \"must be sve\");\n+      __ sve_lsl($dst$$FloatRegister, __ elemType_to_regVariant(bt), $src$$FloatRegister, con);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vasr_imm(vReg dst, vReg src, immI shift) %{\n+  predicate(assert_not_var_shift(n));\n+  match(Set dst (RShiftVB src (RShiftCntV shift)));\n+  match(Set dst (RShiftVS src (RShiftCntV shift)));\n+  match(Set dst (RShiftVI src (RShiftCntV shift)));\n+  match(Set dst (RShiftVL src (RShiftCntV shift)));\n+  format %{ \"vasr_imm $dst, $src, $shift\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this);\n+    int con = (int)$shift$$constant;\n+    if (con == 0) {\n+      if ($dst$$FloatRegister != $src$$FloatRegister) {\n+        if (length_in_bytes <= 16) {\n+          __ orr($dst$$FloatRegister, length_in_bytes == 16 ? __ T16B : __ T8B,\n+                 $src$$FloatRegister, $src$$FloatRegister);\n+        } else {\n+          assert(UseSVE > 0, \"must be sve\");\n+          __ sve_orr($dst$$FloatRegister, $src$$FloatRegister, $src$$FloatRegister);\n+        }\n+      }\n+      return;\n+    }\n+    if (is_subword_type(bt)) {\n+      \/\/ Refine con for B\/S\n+      int esize_in_bits = type2aelembytes(bt) * BitsPerByte;\n+      if (con >= esize_in_bits) con = esize_in_bits - 1;\n+    }\n+    if (length_in_bytes <= 16) {\n+      __ sshr($dst$$FloatRegister, get_arrangement(this), $src$$FloatRegister, con);\n+    } else {\n+      assert(UseSVE > 0, \"must be sve\");\n+      __ sve_asr($dst$$FloatRegister, __ elemType_to_regVariant(bt), $src$$FloatRegister, con);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vlsr_imm(vReg dst, vReg src, immI shift) %{\n+  predicate(assert_not_var_shift(n));\n+  match(Set dst (URShiftVB src (RShiftCntV shift)));\n+  match(Set dst (URShiftVS src (RShiftCntV shift)));\n+  match(Set dst (URShiftVI src (RShiftCntV shift)));\n+  match(Set dst (URShiftVL src (RShiftCntV shift)));\n+  format %{ \"vlsr_imm $dst, $src, $shift\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this);\n+    int con = (int)$shift$$constant;\n+    if (con == 0) {\n+      if ($dst$$FloatRegister != $src$$FloatRegister) {\n+        if (length_in_bytes <= 16) {\n+          __ orr($dst$$FloatRegister, length_in_bytes == 16 ? __ T16B : __ T8B,\n+                 $src$$FloatRegister, $src$$FloatRegister);\n+        } else {\n+          assert(UseSVE > 0, \"must be sve\");\n+          __ sve_orr($dst$$FloatRegister, $src$$FloatRegister, $src$$FloatRegister);\n+        }\n+      }\n+      return;\n+    }\n+    if (is_subword_type(bt)) {\n+      \/\/ Optimize for B\/S\n+      int esize_in_bits = type2aelembytes(bt) * BitsPerByte;\n+      if (con >= esize_in_bits) {\n+        if (length_in_bytes <= 16) {\n+          __ eor($dst$$FloatRegister, length_in_bytes == 16 ? __ T16B : __ T8B,\n+                 $src$$FloatRegister, $src$$FloatRegister);\n+        } else {\n+          assert(UseSVE > 0, \"must be sve\");\n+          __ sve_eor($dst$$FloatRegister, $src$$FloatRegister, $src$$FloatRegister);\n+        }\n+        return;\n+      }\n+    }\n+    if (length_in_bytes <= 16) {\n+      __ ushr($dst$$FloatRegister, get_arrangement(this), $src$$FloatRegister, con);\n+    } else {\n+      assert(UseSVE > 0, \"must be sve\");\n+      __ sve_lsr($dst$$FloatRegister, __ elemType_to_regVariant(bt), $src$$FloatRegister, con);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ shift right add with imm (vector length <= 128 bits only)\n+\n+instruct vasra_imm(vReg dst, vReg src, immI shift) %{\n+  predicate(Matcher::vector_length_in_bytes(n) <= 16);\n+  match(Set dst (AddVB dst (RShiftVB src (RShiftCntV shift))));\n+  match(Set dst (AddVS dst (RShiftVS src (RShiftCntV shift))));\n+  match(Set dst (AddVI dst (RShiftVI src (RShiftCntV shift))));\n+  match(Set dst (AddVL dst (RShiftVL src (RShiftCntV shift))));\n+  format %{ \"vasra_imm $dst, $src, $shift\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int con = (int)$shift$$constant;\n+    if (is_subword_type(bt)) {\n+      \/\/ Refine con for B\/S\n+      int esize_in_bits = type2aelembytes(bt) * BitsPerByte;\n+      if (con >= esize_in_bits) con = esize_in_bits - 1;\n+    }\n+    __ ssra($dst$$FloatRegister, get_arrangement(this), $src$$FloatRegister, con);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vlsra_imm(vReg dst, vReg src, immI shift) %{\n+  predicate(Matcher::vector_length_in_bytes(n) <= 16);\n+  match(Set dst (AddVB dst (URShiftVB src (RShiftCntV shift))));\n+  match(Set dst (AddVS dst (URShiftVS src (RShiftCntV shift))));\n+  match(Set dst (AddVI dst (URShiftVI src (RShiftCntV shift))));\n+  match(Set dst (AddVL dst (URShiftVL src (RShiftCntV shift))));\n+  format %{ \"vlsra_imm $dst, $src, $shift\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int con = (int)$shift$$constant;\n+    if (is_subword_type(bt)) { \/\/ for B\/H\n+      int esize_in_bits = type2aelembytes(bt) * BitsPerByte;\n+      if (con < esize_in_bits) {\n+        __ usra($dst$$FloatRegister, get_arrangement(this), $src$$FloatRegister, con);\n+      }\n+    } else { \/\/ for S\/D\n+      assert(type2aelembytes(bt) == 4 || type2aelembytes(bt) == 8, \"unsupported type\");\n+      __ usra($dst$$FloatRegister, get_arrangement(this), $src$$FloatRegister, con);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector shift - predicated\n+\n+instruct vlsl_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (LShiftVB (Binary dst_src1 src2) pg));\n+  match(Set dst_src1 (LShiftVS (Binary dst_src1 src2) pg));\n+  match(Set dst_src1 (LShiftVI (Binary dst_src1 src2) pg));\n+  match(Set dst_src1 (LShiftVL (Binary dst_src1 src2) pg));\n+  format %{ \"vlsl_masked $dst_src1, $pg, $dst_src1, $src2\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ sve_lsl($dst_src1$$FloatRegister, __ elemType_to_regVariant(bt),\n+               $pg$$PRegister, $src2$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vasr_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (RShiftVB (Binary dst_src1 src2) pg));\n+  match(Set dst_src1 (RShiftVS (Binary dst_src1 src2) pg));\n+  match(Set dst_src1 (RShiftVI (Binary dst_src1 src2) pg));\n+  match(Set dst_src1 (RShiftVL (Binary dst_src1 src2) pg));\n+  format %{ \"vasr_masked $dst_src1, $pg, $dst_src1, $src2\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ sve_asr($dst_src1$$FloatRegister, __ elemType_to_regVariant(bt),\n+               $pg$$PRegister, $src2$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vlsr_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (URShiftVB (Binary dst_src1 src2) pg));\n+  match(Set dst_src1 (URShiftVS (Binary dst_src1 src2) pg));\n+  match(Set dst_src1 (URShiftVI (Binary dst_src1 src2) pg));\n+  match(Set dst_src1 (URShiftVL (Binary dst_src1 src2) pg));\n+  format %{ \"vlsr_masked $dst_src1, $pg, $dst_src1, $src2\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ sve_lsr($dst_src1$$FloatRegister, __ elemType_to_regVariant(bt),\n+               $pg$$PRegister, $src2$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector shift with imm - predicated\n+\n+instruct vlsl_imm_masked(vReg dst_src, immI shift, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (LShiftVB (Binary dst_src (LShiftCntV shift)) pg));\n+  match(Set dst_src (LShiftVS (Binary dst_src (LShiftCntV shift)) pg));\n+  match(Set dst_src (LShiftVI (Binary dst_src (LShiftCntV shift)) pg));\n+  match(Set dst_src (LShiftVL (Binary dst_src (LShiftCntV shift)) pg));\n+  format %{ \"vlsl_imm_masked $dst_src, $pg, $dst_src, $shift\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int esize_in_bits = type2aelembytes(bt) * BitsPerByte;\n+    int con = (int)$shift$$constant;\n+    assert(con >= 0 && con < esize_in_bits, \"invalid shift immediate\");\n+    __ sve_lsl($dst_src$$FloatRegister, __ elemType_to_regVariant(bt),\n+               $pg$$PRegister, con);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vasr_imm_masked(vReg dst_src, immI shift, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (RShiftVB (Binary dst_src (RShiftCntV shift)) pg));\n+  match(Set dst_src (RShiftVS (Binary dst_src (RShiftCntV shift)) pg));\n+  match(Set dst_src (RShiftVI (Binary dst_src (RShiftCntV shift)) pg));\n+  match(Set dst_src (RShiftVL (Binary dst_src (RShiftCntV shift)) pg));\n+  format %{ \"vasr_imm_masked $dst_src, $pg, $dst_src, $shift\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int esize_in_bits = type2aelembytes(bt) * BitsPerByte;\n+    int con = (int)$shift$$constant;\n+    assert(con > 0 && con < esize_in_bits, \"invalid shift immediate\");\n+    __ sve_asr($dst_src$$FloatRegister, __ elemType_to_regVariant(bt),\n+               $pg$$PRegister, con);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vlsr_imm_masked(vReg dst_src, immI shift, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (URShiftVB (Binary dst_src (RShiftCntV shift)) pg));\n+  match(Set dst_src (URShiftVS (Binary dst_src (RShiftCntV shift)) pg));\n+  match(Set dst_src (URShiftVI (Binary dst_src (RShiftCntV shift)) pg));\n+  match(Set dst_src (URShiftVL (Binary dst_src (RShiftCntV shift)) pg));\n+  format %{ \"vlsr_imm_masked $dst_src, $pg, $dst_src, $shift\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int esize_in_bits = type2aelembytes(bt) * BitsPerByte;\n+    int con = (int)$shift$$constant;\n+    assert(con > 0 && con < esize_in_bits, \"invalid shift immediate\");\n+    __ sve_lsr($dst_src$$FloatRegister, __ elemType_to_regVariant(bt),\n+               $pg$$PRegister, con);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ Vector reduction add -------------------------\n+\n+\/\/ reduction addI\n+\n+instruct reduce_addI_le128b(iRegINoSp dst, iRegIorL2I isrc, vReg vsrc, vReg tmp) %{\n+  predicate(Matcher::vector_length_in_bytes(n->in(2)) <= 16);\n+  match(Set dst (AddReductionVI isrc vsrc));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"reduce_addI_le128b $dst, $isrc, $vsrc\\t# vector <= 128 bits. KILL $tmp\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this, $vsrc);\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this, $vsrc);\n+    __ neon_reduce_add_integral($dst$$Register, bt,\n+                                $isrc$$Register, $vsrc$$FloatRegister,\n+                                length_in_bytes, $tmp$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_addI_gt128b(iRegINoSp dst, iRegIorL2I isrc, vReg vsrc, vRegD tmp) %{\n+  predicate(Matcher::vector_length_in_bytes(n->in(2)) > 16 &&\n+            Matcher::vector_length_in_bytes(n->in(2)) == MaxVectorSize);\n+  match(Set dst (AddReductionVI isrc vsrc));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"reduce_addI_gt128b $dst, $isrc, $vsrc\\t# vector > 128 bits. KILL $tmp\" %}\n+  ins_encode %{\n+    assert(UseSVE > 0, \"must be sve\");\n+    BasicType bt = Matcher::vector_element_basic_type(this, $vsrc);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $isrc$$Register, $vsrc$$FloatRegister,\n+                           ptrue, $tmp$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_addI_partial(iRegINoSp dst, iRegIorL2I isrc, vReg vsrc,\n+                             vRegD tmp, pRegGov pgtmp) %{\n+  predicate(Matcher::vector_length_in_bytes(n->in(2)) > 16 &&\n+            Matcher::vector_length_in_bytes(n->in(2)) < MaxVectorSize);\n+  match(Set dst (AddReductionVI isrc vsrc));\n+  effect(TEMP_DEF dst, TEMP tmp, TEMP pgtmp);\n+  format %{ \"reduce_addI_partial $dst, $isrc, $vsrc\\t# KILL $tmp, $pgtmp\" %}\n+  ins_encode %{\n+    assert(UseSVE > 0, \"must be sve\");\n+    BasicType bt = Matcher::vector_element_basic_type(this, $vsrc);\n+    __ sve_ptrue_lanecnt($pgtmp$$PRegister, __ elemType_to_regVariant(bt),\n+                          Matcher::vector_length(this, $vsrc));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $isrc$$Register, $vsrc$$FloatRegister,\n+                           $pgtmp$$PRegister, $tmp$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ reduction addL\n+\n+instruct reduce_addL_128b(iRegLNoSp dst, iRegL isrc, vReg vsrc, vReg tmp) %{\n+  predicate(Matcher::vector_length_in_bytes(n->in(2)) == 16);\n+  match(Set dst (AddReductionVL isrc vsrc));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"reduce_addL_128b $dst, $isrc, $vsrc\\t# 2L. KILL $tmp\" %}\n+  ins_encode %{\n+    __ neon_reduce_add_integral($dst$$Register, T_LONG,\n+                                $isrc$$Register, $vsrc$$FloatRegister,\n+                                \/* vector_length_in_bytes *\/ 16, $tmp$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_addL_gt128b(iRegLNoSp dst, iRegL isrc, vReg vsrc, vRegD tmp) %{\n+  predicate(Matcher::vector_length_in_bytes(n->in(2)) > 16 &&\n+            Matcher::vector_length_in_bytes(n->in(2)) == MaxVectorSize);\n+  match(Set dst (AddReductionVL isrc vsrc));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"reduce_addL_gt128b $dst, $isrc, $vsrc\\t# vector > 128 bits. KILL tmp\" %}\n+  ins_encode %{\n+    assert(UseSVE > 0, \"must be sve\");\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $isrc$$Register, $vsrc$$FloatRegister,\n+                           ptrue, $tmp$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_addL_partial(iRegLNoSp dst, iRegL isrc, vReg vsrc, vRegD tmp, pRegGov pgtmp) %{\n+  predicate(Matcher::vector_length_in_bytes(n->in(2)) > 16 &&\n+            Matcher::vector_length_in_bytes(n->in(2)) < MaxVectorSize);\n+  match(Set dst (AddReductionVL isrc vsrc));\n+  effect(TEMP_DEF dst, TEMP tmp, TEMP pgtmp);\n+  format %{ \"reduce_addL_partial $dst, $isrc, $vsrc\\t# KILL $tmp, $pgtmp\" %}\n+  ins_encode %{\n+    assert(UseSVE > 0, \"must be sve\");\n+    __ sve_ptrue_lanecnt($pgtmp$$PRegister, __ D, Matcher::vector_length(this, $vsrc));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $isrc$$Register, $vsrc$$FloatRegister,\n+                           $pgtmp$$PRegister, $tmp$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ reduction addF\n+\n+instruct reduce_addF_le128b(vRegF dst, vRegF fsrc, vReg vsrc, vReg tmp) %{\n+  predicate(Matcher::vector_length_in_bytes(n->in(2)) == 8 ||\n+            (Matcher::vector_length_in_bytes(n->in(2)) == 16 && UseSVE == 0));\n+  match(Set dst (AddReductionVF fsrc vsrc));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"reduce_addF_le128b $dst, $fsrc, $vsrc\\t# vector <= 128 bits. KILL $tmp\" %}\n+  ins_encode %{\n+    __ fadds($dst$$FloatRegister, $fsrc$$FloatRegister, $vsrc$$FloatRegister);\n+    __ ins($tmp$$FloatRegister, __ S, $vsrc$$FloatRegister, 0, 1);\n+    __ fadds($dst$$FloatRegister, $dst$$FloatRegister, $tmp$$FloatRegister);\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this, $vsrc);\n+    if (length_in_bytes == 16) {\n+      __ ins($tmp$$FloatRegister, __ S, $vsrc$$FloatRegister, 0, 2);\n+      __ fadds($dst$$FloatRegister, $dst$$FloatRegister, $tmp$$FloatRegister);\n+      __ ins($tmp$$FloatRegister, __ S, $vsrc$$FloatRegister, 0, 3);\n+      __ fadds($dst$$FloatRegister, $dst$$FloatRegister, $tmp$$FloatRegister);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_addF_ge128b(vRegF dst_src1, vReg src2) %{\n+  predicate(UseSVE > 0 &&\n+            Matcher::vector_length_in_bytes(n->in(2)) >= 16 &&\n+            Matcher::vector_length_in_bytes(n->in(2)) == MaxVectorSize);\n+  match(Set dst_src1 (AddReductionVF dst_src1 src2));\n+  format %{ \"reduce_addF_ge128b $dst_src1, $dst_src1, $src2\\t# vector >= 128 bits\" %}\n+  ins_encode %{\n+    __ sve_fadda($dst_src1$$FloatRegister, __ S, ptrue, $src2$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_addF_partial(vRegF dst_src1, vReg src2, pRegGov pgtmp) %{\n+  predicate(UseSVE > 0 &&\n+            Matcher::vector_length_in_bytes(n->in(2)) >= 16 &&\n+            Matcher::vector_length_in_bytes(n->in(2)) < MaxVectorSize);\n+  match(Set dst_src1 (AddReductionVF dst_src1 src2));\n+  effect(TEMP pgtmp);\n+  format %{ \"reduce_addF_partial $dst_src1, $dst_src1, $src2\\t# KILL $pgtmp\" %}\n+  ins_encode %{\n+    __ sve_ptrue_lanecnt($pgtmp$$PRegister, __ S, Matcher::vector_length(this, $src2));\n+    __ sve_fadda($dst_src1$$FloatRegister, __ S, $pgtmp$$PRegister, $src2$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ reduction addD\n+\n+instruct reduce_addD_128b(vRegD dst, vRegD dsrc, vReg vsrc, vReg tmp) %{\n+  predicate(Matcher::vector_length_in_bytes(n->in(2)) == 16);\n+  match(Set dst (AddReductionVD dsrc vsrc));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"reduce_addD_128b $dst, $dsrc, $vsrc\\t# 2D. KILL $tmp\" %}\n+  ins_encode %{\n+    __ faddd($dst$$FloatRegister, $dsrc$$FloatRegister, $vsrc$$FloatRegister);\n+    __ ins($tmp$$FloatRegister, __ D, $vsrc$$FloatRegister, 0, 1);\n+    __ faddd($dst$$FloatRegister, $dst$$FloatRegister, $tmp$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_addD_gt128b(vRegD dst_src1, vReg src2) %{\n+  predicate(Matcher::vector_length_in_bytes(n->in(2)) > 16 &&\n+            Matcher::vector_length_in_bytes(n->in(2)) == MaxVectorSize);\n+  match(Set dst_src1 (AddReductionVD dst_src1 src2));\n+  format %{ \"reduce_addD_gt128b $dst_src1, $dst_src1, $src2\\t# vector > 128 bits\" %}\n+  ins_encode %{\n+    assert(UseSVE > 0, \"must be sve\");\n+    __ sve_fadda($dst_src1$$FloatRegister, __ D, ptrue, $src2$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_addD_partial(vRegD dst_src1, vReg src2, pRegGov pgtmp) %{\n+  predicate(Matcher::vector_length_in_bytes(n->in(2)) > 16 &&\n+            Matcher::vector_length_in_bytes(n->in(2)) < MaxVectorSize);\n+  match(Set dst_src1 (AddReductionVD dst_src1 src2));\n+  effect(TEMP pgtmp);\n+  format %{ \"reduce_addD_partial $dst_src1, $dst_src1, $src2\\t# KILL $pgtmp\" %}\n+  ins_encode %{\n+    __ sve_ptrue_lanecnt($pgtmp$$PRegister, __ D, Matcher::vector_length(this, $src2));\n+    __ sve_fadda($dst_src1$$FloatRegister, __ D, $pgtmp$$PRegister, $src2$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ reduction add - predicated\n+\n+instruct reduce_addI_masked(iRegINoSp dst, iRegIorL2I isrc, vReg vsrc, pRegGov pg, vRegD tmp) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst (AddReductionVI (Binary isrc vsrc) pg));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"reduce_addI_masked $dst, $isrc, $pg, $vsrc\\t# KILL $tmp\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this, $vsrc);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $isrc$$Register, $vsrc$$FloatRegister,\n+                           $pg$$PRegister, $tmp$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_addL_masked(iRegLNoSp dst, iRegL isrc, vReg vsrc, pRegGov pg, vRegD tmp) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst (AddReductionVL (Binary isrc vsrc) pg));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"reduce_addL_masked $dst, $isrc, $pg, $vsrc\\t# KILL $tmp\" %}\n+  ins_encode %{\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $isrc$$Register, $vsrc$$FloatRegister,\n+                           $pg$$PRegister, $tmp$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_addF_masked(vRegF dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (AddReductionVF (Binary dst_src1 src2) pg));\n+  format %{ \"reduce_addF_masked $dst_src1, $pg, $dst_src1, $src2\" %}\n+  ins_encode %{\n+    __ sve_fadda($dst_src1$$FloatRegister, __ S,\n+                 $pg$$PRegister, $src2$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_addD_masked(vRegD dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (AddReductionVD (Binary dst_src1 src2) pg));\n+  format %{ \"reduce_addD_masked $dst_src1, $pg, $dst_src1, $src2\" %}\n+  ins_encode %{\n+    __ sve_fadda($dst_src1$$FloatRegister, __ D,\n+                 $pg$$PRegister, $src2$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ Vector reduction mul -------------------------\n+\n+instruct reduce_mulI(iRegINoSp dst, iRegIorL2I isrc, vReg vsrc,\n+                     vReg tmp1, vReg tmp2) %{\n+  predicate(Matcher::vector_length_in_bytes(n->in(2)) == 8 ||\n+            Matcher::vector_length_in_bytes(n->in(2)) == 16);\n+  match(Set dst (MulReductionVI isrc vsrc));\n+  effect(TEMP_DEF dst, TEMP tmp1, TEMP tmp2);\n+  format %{ \"reduce_mulI $dst, $isrc, $vsrc\\t# vector (64\/128 bits). KILL $tmp1, $tmp2\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this, $vsrc);\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this, $vsrc);\n+    __ neon_reduce_mul_integral($dst$$Register, bt, $isrc$$Register,\n+                                $vsrc$$FloatRegister, length_in_bytes,\n+                                $tmp1$$FloatRegister, $tmp2$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_mulL(iRegLNoSp dst, iRegL isrc, vReg vsrc) %{\n+  predicate(Matcher::vector_length_in_bytes(n->in(2)) == 16);\n+  match(Set dst (MulReductionVL isrc vsrc));\n+  effect(TEMP_DEF dst);\n+  format %{ \"reduce_mulL $dst, $isrc, $vsrc\\t# 2L\" %}\n+  ins_encode %{\n+    __ neon_reduce_mul_integral($dst$$Register, T_LONG, $isrc$$Register,\n+                                $vsrc$$FloatRegister, 16, fnoreg, fnoreg);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_mulF(vRegF dst, vRegF fsrc, vReg vsrc, vReg tmp) %{\n+  predicate(Matcher::vector_length_in_bytes(n->in(2)) <= 16);\n+  match(Set dst (MulReductionVF fsrc vsrc));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"reduce_mulF $dst, $fsrc, $vsrc\\t# 2F\/4F. KILL $tmp\" %}\n+  ins_encode %{\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this, $vsrc);\n+    __ neon_reduce_mul_fp($dst$$FloatRegister, T_FLOAT, $fsrc$$FloatRegister,\n+                          $vsrc$$FloatRegister, length_in_bytes, $tmp$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_mulD(vRegD dst, vRegD dsrc, vReg vsrc, vReg tmp) %{\n+  predicate(Matcher::vector_length_in_bytes(n->in(2)) == 16);\n+  match(Set dst (MulReductionVD dsrc vsrc));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"reduce_mulD $dst, $dsrc, $vsrc\\t# 2D. KILL $tmp\" %}\n+  ins_encode %{\n+    __ neon_reduce_mul_fp($dst$$FloatRegister, T_DOUBLE, $dsrc$$FloatRegister,\n+                          $vsrc$$FloatRegister, 16, $tmp$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ Vector reduction and -------------------------\n+\n+\/\/ reduction andI\n+\n+instruct reduce_andI_neon(iRegINoSp dst, iRegIorL2I isrc, vReg vsrc) %{\n+  predicate(UseSVE == 0 &&\n+            Matcher::vector_element_basic_type(n->in(2)) != T_LONG);\n+  match(Set dst (AndReductionV isrc vsrc));\n+  effect(TEMP_DEF dst);\n+  format %{ \"reduce_andI_neon $dst, $isrc, $vsrc\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this, $vsrc);\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this, $vsrc);\n+    __ neon_reduce_logical(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $isrc$$Register, $vsrc$$FloatRegister, length_in_bytes);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_andI_sve(iRegINoSp dst, iRegIorL2I isrc, vReg vsrc, vRegD tmp) %{\n+  predicate(UseSVE > 0 &&\n+            Matcher::vector_element_basic_type(n->in(2)) != T_LONG &&\n+            Matcher::vector_length_in_bytes(n->in(2)) == MaxVectorSize);\n+  match(Set dst (AndReductionV isrc vsrc));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"reduce_andI_sve $dst, $isrc, $vsrc\\t# KILL $tmp\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this, $vsrc);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $isrc$$Register, $vsrc$$FloatRegister,\n+                           ptrue, $tmp$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_andI_partial(iRegINoSp dst, iRegIorL2I isrc, vReg vsrc,\n+                             vRegD tmp, pRegGov pgtmp) %{\n+  predicate(UseSVE > 0 &&\n+            Matcher::vector_element_basic_type(n->in(2)) != T_LONG &&\n+            Matcher::vector_length_in_bytes(n->in(2)) < MaxVectorSize);\n+  match(Set dst (AndReductionV isrc vsrc));\n+  effect(TEMP_DEF dst, TEMP tmp, TEMP pgtmp);\n+  format %{ \"reduce_andI_partial $dst, $isrc, $vsrc\\t# KILL $tmp, $pgtmp\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this, $vsrc);\n+    __ sve_ptrue_lanecnt($pgtmp$$PRegister, __ elemType_to_regVariant(bt),\n+                         Matcher::vector_length(this, $vsrc));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $isrc$$Register, $vsrc$$FloatRegister,\n+                           $pgtmp$$PRegister, $tmp$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ reduction andL\n+\n+instruct reduce_andL_neon(iRegLNoSp dst, iRegL isrc, vReg vsrc) %{\n+  predicate(UseSVE == 0 &&\n+            Matcher::vector_element_basic_type(n->in(2)) == T_LONG);\n+  match(Set dst (AndReductionV isrc vsrc));\n+  effect(TEMP_DEF dst);\n+  format %{ \"reduce_andL_neon $dst, $isrc, $vsrc\\t# 2L\" %}\n+  ins_encode %{\n+    __ neon_reduce_logical(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $isrc$$Register, $vsrc$$FloatRegister,\n+                           \/* vector_length_in_bytes *\/ 16);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_andL_sve(iRegLNoSp dst, iRegL isrc, vReg vsrc, vRegD tmp) %{\n+  predicate(UseSVE > 0 &&\n+            Matcher::vector_element_basic_type(n->in(2)) == T_LONG &&\n+            Matcher::vector_length_in_bytes(n->in(2)) == MaxVectorSize);\n+  match(Set dst (AndReductionV isrc vsrc));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"reduce_andL_sve $dst, $isrc, $vsrc\\t# KILL $tmp\" %}\n+  ins_encode %{\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $isrc$$Register, $vsrc$$FloatRegister,\n+                           ptrue, $tmp$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_andL_partial(iRegLNoSp dst, iRegL isrc, vReg vsrc,\n+                             vRegD tmp, pRegGov pgtmp) %{\n+  predicate(UseSVE > 0 &&\n+            Matcher::vector_element_basic_type(n->in(2)) == T_LONG &&\n+            Matcher::vector_length_in_bytes(n->in(2)) < MaxVectorSize);\n+  match(Set dst (AndReductionV isrc vsrc));\n+  effect(TEMP_DEF dst, TEMP tmp, TEMP pgtmp);\n+  format %{ \"reduce_andL_partial $dst, $isrc, $vsrc\\t# KILL $tmp, $pgtmp\" %}\n+  ins_encode %{\n+    __ sve_ptrue_lanecnt($pgtmp$$PRegister, __ D, Matcher::vector_length(this, $vsrc));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $isrc$$Register, $vsrc$$FloatRegister,\n+                           $pgtmp$$PRegister, $tmp$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ reduction and - predicated\n+\n+instruct reduce_andI_masked(iRegINoSp dst, iRegIorL2I isrc, vReg vsrc, pRegGov pg, vRegD tmp) %{\n+  predicate(UseSVE > 0 && Matcher::vector_element_basic_type(n->in(1)->in(2)) != T_LONG);\n+  match(Set dst (AndReductionV (Binary isrc vsrc) pg));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"reduce_andI_masked $dst, $isrc, $pg, $vsrc\\t# KILL $tmp\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this, $vsrc);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $isrc$$Register, $vsrc$$FloatRegister,\n+                           $pg$$PRegister, $tmp$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_andL_masked(iRegLNoSp dst, iRegL isrc, vReg vsrc, pRegGov pg, vRegD tmp) %{\n+  predicate(UseSVE > 0 && Matcher::vector_element_basic_type(n->in(1)->in(2)) == T_LONG);\n+  match(Set dst (AndReductionV (Binary isrc vsrc) pg));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"reduce_andL_masked $dst, $isrc, $pg, $vsrc\\t# KILL $tmp\" %}\n+  ins_encode %{\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $isrc$$Register, $vsrc$$FloatRegister,\n+                           $pg$$PRegister, $tmp$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ Vector reduction or --------------------------\n+\n+\/\/ reduction orI\n+\n+instruct reduce_orI_neon(iRegINoSp dst, iRegIorL2I isrc, vReg vsrc) %{\n+  predicate(UseSVE == 0 &&\n+            Matcher::vector_element_basic_type(n->in(2)) != T_LONG);\n+  match(Set dst (OrReductionV isrc vsrc));\n+  effect(TEMP_DEF dst);\n+  format %{ \"reduce_orI_neon $dst, $isrc, $vsrc\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this, $vsrc);\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this, $vsrc);\n+    __ neon_reduce_logical(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $isrc$$Register, $vsrc$$FloatRegister, length_in_bytes);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_orI_sve(iRegINoSp dst, iRegIorL2I isrc, vReg vsrc, vRegD tmp) %{\n+  predicate(UseSVE > 0 &&\n+            Matcher::vector_element_basic_type(n->in(2)) != T_LONG &&\n+            Matcher::vector_length_in_bytes(n->in(2)) == MaxVectorSize);\n+  match(Set dst (OrReductionV isrc vsrc));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"reduce_orI_sve $dst, $isrc, $vsrc\\t# KILL $tmp\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this, $vsrc);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $isrc$$Register, $vsrc$$FloatRegister,\n+                           ptrue, $tmp$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_orI_partial(iRegINoSp dst, iRegIorL2I isrc, vReg vsrc,\n+                            vRegD tmp, pRegGov pgtmp) %{\n+  predicate(UseSVE > 0 &&\n+            Matcher::vector_element_basic_type(n->in(2)) != T_LONG &&\n+            Matcher::vector_length_in_bytes(n->in(2)) < MaxVectorSize);\n+  match(Set dst (OrReductionV isrc vsrc));\n+  effect(TEMP_DEF dst, TEMP tmp, TEMP pgtmp);\n+  format %{ \"reduce_orI_partial $dst, $isrc, $vsrc\\t# KILL $tmp, $pgtmp\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this, $vsrc);\n+    __ sve_ptrue_lanecnt($pgtmp$$PRegister, __ elemType_to_regVariant(bt),\n+                         Matcher::vector_length(this, $vsrc));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $isrc$$Register, $vsrc$$FloatRegister,\n+                           $pgtmp$$PRegister, $tmp$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ reduction orL\n+\n+instruct reduce_orL_neon(iRegLNoSp dst, iRegL isrc, vReg vsrc) %{\n+  predicate(UseSVE == 0 &&\n+            Matcher::vector_element_basic_type(n->in(2)) == T_LONG);\n+  match(Set dst (OrReductionV isrc vsrc));\n+  effect(TEMP_DEF dst);\n+  format %{ \"reduce_orL_neon $dst, $isrc, $vsrc\\t# 2L\" %}\n+  ins_encode %{\n+    __ neon_reduce_logical(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $isrc$$Register, $vsrc$$FloatRegister,\n+                           \/* vector_length_in_bytes *\/ 16);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_orL_sve(iRegLNoSp dst, iRegL isrc, vReg vsrc, vRegD tmp) %{\n+  predicate(UseSVE > 0 &&\n+            Matcher::vector_element_basic_type(n->in(2)) == T_LONG &&\n+            Matcher::vector_length_in_bytes(n->in(2)) == MaxVectorSize);\n+  match(Set dst (OrReductionV isrc vsrc));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"reduce_orL_sve $dst, $isrc, $vsrc\\t# KILL $tmp\" %}\n+  ins_encode %{\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $isrc$$Register, $vsrc$$FloatRegister,\n+                           ptrue, $tmp$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_orL_partial(iRegLNoSp dst, iRegL isrc, vReg vsrc,\n+                            vRegD tmp, pRegGov pgtmp) %{\n+  predicate(UseSVE > 0 &&\n+            Matcher::vector_element_basic_type(n->in(2)) == T_LONG &&\n+            Matcher::vector_length_in_bytes(n->in(2)) < MaxVectorSize);\n+  match(Set dst (OrReductionV isrc vsrc));\n+  effect(TEMP_DEF dst, TEMP tmp, TEMP pgtmp);\n+  format %{ \"reduce_orL_partial $dst, $isrc, $vsrc\\t# KILL $tmp, $pgtmp\" %}\n+  ins_encode %{\n+    __ sve_ptrue_lanecnt($pgtmp$$PRegister, __ D, Matcher::vector_length(this, $vsrc));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $isrc$$Register, $vsrc$$FloatRegister,\n+                           $pgtmp$$PRegister, $tmp$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ reduction or - predicated\n+\n+instruct reduce_orI_masked(iRegINoSp dst, iRegIorL2I isrc, vReg vsrc, pRegGov pg, vRegD tmp) %{\n+  predicate(UseSVE > 0 && Matcher::vector_element_basic_type(n->in(1)->in(2)) != T_LONG);\n+  match(Set dst (OrReductionV (Binary isrc vsrc) pg));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"reduce_orI_masked $dst, $isrc, $pg, $vsrc\\t# KILL $tmp\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this, $vsrc);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $isrc$$Register, $vsrc$$FloatRegister,\n+                           $pg$$PRegister, $tmp$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_orL_masked(iRegLNoSp dst, iRegL isrc, vReg vsrc, pRegGov pg, vRegD tmp) %{\n+  predicate(UseSVE > 0 && Matcher::vector_element_basic_type(n->in(1)->in(2)) == T_LONG);\n+  match(Set dst (OrReductionV (Binary isrc vsrc) pg));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"reduce_orL_masked $dst, $isrc, $pg, $vsrc\\t# KILL $tmp\" %}\n+  ins_encode %{\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $isrc$$Register, $vsrc$$FloatRegister,\n+                           $pg$$PRegister, $tmp$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ Vector reduction xor -------------------------\n+\n+\/\/ reduction xorI\n+\n+instruct reduce_xorI_neon(iRegINoSp dst, iRegIorL2I isrc, vReg vsrc) %{\n+  predicate(UseSVE == 0 &&\n+            Matcher::vector_element_basic_type(n->in(2)) != T_LONG);\n+  match(Set dst (XorReductionV isrc vsrc));\n+  effect(TEMP_DEF dst);\n+  format %{ \"reduce_xorI_neon $dst, $isrc, $vsrc\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this, $vsrc);\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this, $vsrc);\n+    __ neon_reduce_logical(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $isrc$$Register, $vsrc$$FloatRegister, length_in_bytes);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_xorI_sve(iRegINoSp dst, iRegIorL2I isrc, vReg vsrc, vRegD tmp) %{\n+  predicate(UseSVE > 0 &&\n+            Matcher::vector_element_basic_type(n->in(2)) != T_LONG &&\n+            Matcher::vector_length_in_bytes(n->in(2)) == MaxVectorSize);\n+  match(Set dst (XorReductionV isrc vsrc));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"reduce_xorI_sve $dst, $isrc, $vsrc\\t# KILL $tmp\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this, $vsrc);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $isrc$$Register, $vsrc$$FloatRegister,\n+                           ptrue, $tmp$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_xorI_partial(iRegINoSp dst, iRegIorL2I isrc, vReg vsrc,\n+                             vRegD tmp, pRegGov pgtmp) %{\n+  predicate(UseSVE > 0 &&\n+            Matcher::vector_element_basic_type(n->in(2)) != T_LONG &&\n+            Matcher::vector_length_in_bytes(n->in(2)) < MaxVectorSize);\n+  match(Set dst (XorReductionV isrc vsrc));\n+  effect(TEMP_DEF dst, TEMP tmp, TEMP pgtmp);\n+  format %{ \"reduce_xorI_partial $dst, $isrc, $vsrc\\t# KILL $tmp, $pgtmp\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this, $vsrc);\n+    __ sve_ptrue_lanecnt($pgtmp$$PRegister, __ elemType_to_regVariant(bt),\n+                         Matcher::vector_length(this, $vsrc));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $isrc$$Register, $vsrc$$FloatRegister,\n+                           $pgtmp$$PRegister, $tmp$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ reduction xorL\n+\n+instruct reduce_xorL_neon(iRegLNoSp dst, iRegL isrc, vReg vsrc) %{\n+  predicate(UseSVE == 0 &&\n+            Matcher::vector_element_basic_type(n->in(2)) == T_LONG);\n+  match(Set dst (XorReductionV isrc vsrc));\n+  effect(TEMP_DEF dst);\n+  format %{ \"reduce_xorL_neon $dst, $isrc, $vsrc\\t# 2L\" %}\n+  ins_encode %{\n+    __ neon_reduce_logical(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $isrc$$Register, $vsrc$$FloatRegister,\n+                           \/* vector_length_in_bytes *\/ 16);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_xorL_sve(iRegLNoSp dst, iRegL isrc, vReg vsrc, vRegD tmp) %{\n+  predicate(UseSVE > 0 &&\n+            Matcher::vector_element_basic_type(n->in(2)) == T_LONG &&\n+            Matcher::vector_length_in_bytes(n->in(2)) == MaxVectorSize);\n+  match(Set dst (XorReductionV isrc vsrc));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"reduce_xorL_sve $dst, $isrc, $vsrc\\t# KILL $tmp\" %}\n+  ins_encode %{\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $isrc$$Register, $vsrc$$FloatRegister,\n+                           ptrue, $tmp$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_xorL_partial(iRegLNoSp dst, iRegL isrc, vReg vsrc,\n+                             vRegD tmp, pRegGov pgtmp) %{\n+  predicate(UseSVE > 0 &&\n+            Matcher::vector_element_basic_type(n->in(2)) == T_LONG &&\n+            Matcher::vector_length_in_bytes(n->in(2)) < MaxVectorSize);\n+  match(Set dst (XorReductionV isrc vsrc));\n+  effect(TEMP_DEF dst, TEMP tmp, TEMP pgtmp);\n+  format %{ \"reduce_xorL_partial $dst, $isrc, $vsrc\\t# KILL $tmp, $pgtmp\" %}\n+  ins_encode %{\n+    __ sve_ptrue_lanecnt($pgtmp$$PRegister, __ D, Matcher::vector_length(this, $vsrc));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $isrc$$Register, $vsrc$$FloatRegister,\n+                           $pgtmp$$PRegister, $tmp$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ reduction xor - predicated\n+\n+instruct reduce_xorI_masked(iRegINoSp dst, iRegIorL2I isrc, vReg vsrc, pRegGov pg, vRegD tmp) %{\n+  predicate(UseSVE > 0 && Matcher::vector_element_basic_type(n->in(1)->in(2)) != T_LONG);\n+  match(Set dst (XorReductionV (Binary isrc vsrc) pg));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"reduce_xorI_masked $dst, $isrc, $pg, $vsrc\\t# KILL $tmp\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this, $vsrc);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $isrc$$Register, $vsrc$$FloatRegister,\n+                           $pg$$PRegister, $tmp$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_xorL_masked(iRegLNoSp dst, iRegL isrc, vReg vsrc, pRegGov pg, vRegD tmp) %{\n+  predicate(UseSVE > 0 && Matcher::vector_element_basic_type(n->in(1)->in(2)) == T_LONG);\n+  match(Set dst (XorReductionV (Binary isrc vsrc) pg));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"reduce_xorL_masked $dst, $isrc, $pg, $vsrc\\t# KILL $tmp\" %}\n+  ins_encode %{\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $isrc$$Register, $vsrc$$FloatRegister,\n+                           $pg$$PRegister, $tmp$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ Vector reduction max -------------------------\n+\n+\/\/ reduction maxI\n+\n+instruct reduce_maxI_le128b(iRegINoSp dst, iRegIorL2I isrc, vReg vsrc,\n+                            vReg tmp, rFlagsReg cr) %{\n+  predicate(Matcher::vector_length_in_bytes(n->in(2)) <= 16 &&\n+            (Matcher::vector_element_basic_type(n->in(2)) == T_BYTE ||\n+             Matcher::vector_element_basic_type(n->in(2)) == T_SHORT ||\n+             Matcher::vector_element_basic_type(n->in(2)) == T_INT));\n+  match(Set dst (MaxReductionV isrc vsrc));\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n+  format %{ \"reduce_maxI_le128b $dst, $isrc, $vsrc\\t# vector <= 128 bits. KILL $tmp, cr\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this, $vsrc);\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this, $vsrc);\n+    __ neon_reduce_minmax_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                                   $isrc$$Register, $vsrc$$FloatRegister,\n+                                   length_in_bytes, $tmp$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_maxI_gt128b(iRegINoSp dst, iRegIorL2I isrc, vReg vsrc,\n+                            vRegD tmp, rFlagsReg cr) %{\n+  predicate(Matcher::vector_length_in_bytes(n->in(2)) > 16 &&\n+            Matcher::vector_length_in_bytes(n->in(2)) == MaxVectorSize &&\n+            (Matcher::vector_element_basic_type(n->in(2)) == T_BYTE ||\n+             Matcher::vector_element_basic_type(n->in(2)) == T_SHORT ||\n+             Matcher::vector_element_basic_type(n->in(2)) == T_INT));\n+  match(Set dst (MaxReductionV isrc vsrc));\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n+  format %{ \"reduce_maxI_gt128b $dst, $isrc, $vsrc\\t# vector > 128 bits. KILL $tmp, cr\" %}\n+  ins_encode %{\n+    assert(UseSVE > 0, \"must be sve\");\n+    BasicType bt = Matcher::vector_element_basic_type(this, $vsrc);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $isrc$$Register, $vsrc$$FloatRegister,\n+                           ptrue, $tmp$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_maxI_partial(iRegINoSp dst, iRegIorL2I isrc, vReg vsrc, vRegD tmp,\n+                             pRegGov pgtmp, rFlagsReg cr) %{\n+  predicate(Matcher::vector_length_in_bytes(n->in(2)) > 16 &&\n+            Matcher::vector_length_in_bytes(n->in(2)) < MaxVectorSize &&\n+            (Matcher::vector_element_basic_type(n->in(2)) == T_BYTE ||\n+             Matcher::vector_element_basic_type(n->in(2)) == T_SHORT ||\n+             Matcher::vector_element_basic_type(n->in(2)) == T_INT));\n+  match(Set dst (MaxReductionV isrc vsrc));\n+  effect(TEMP_DEF dst, TEMP tmp, TEMP pgtmp, KILL cr);\n+  format %{ \"reduce_maxI_partial $dst, $isrc, $vsrc\\t# KILL $tmp, $pgtmp, cr\" %}\n+  ins_encode %{\n+    assert(UseSVE > 0, \"must be sve\");\n+    BasicType bt = Matcher::vector_element_basic_type(this, $vsrc);\n+    __ sve_ptrue_lanecnt($pgtmp$$PRegister, __ elemType_to_regVariant(bt),\n+                         Matcher::vector_length(this, $vsrc));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $isrc$$Register, $vsrc$$FloatRegister,\n+                           $pgtmp$$PRegister, $tmp$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ reduction maxL\n+\n+instruct reduce_maxL_neon(iRegLNoSp dst, iRegL isrc, vReg vsrc, rFlagsReg cr) %{\n+  predicate(UseSVE == 0 &&\n+            Matcher::vector_element_basic_type(n->in(2)) == T_LONG);\n+  match(Set dst (MaxReductionV isrc vsrc));\n+  effect(TEMP_DEF dst, KILL cr);\n+  format %{ \"reduce_maxL_neon $dst, $isrc, $vsrc\\t# 2L. KILL cr\" %}\n+  ins_encode %{\n+    __ neon_reduce_minmax_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                                   $isrc$$Register, $vsrc$$FloatRegister,\n+                                   \/* vector_length_in_bytes *\/ 16, fnoreg);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_maxL_sve(iRegLNoSp dst, iRegL isrc, vReg vsrc,\n+                         vRegD tmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            Matcher::vector_length_in_bytes(n->in(2)) == MaxVectorSize &&\n+            Matcher::vector_element_basic_type(n->in(2)) == T_LONG);\n+  match(Set dst (MaxReductionV isrc vsrc));\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n+  format %{ \"reduce_maxL_sve $dst, $isrc, $vsrc\\t# KILL $tmp, cr\" %}\n+  ins_encode %{\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $isrc$$Register, $vsrc$$FloatRegister,\n+                           ptrue, $tmp$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_maxL_partial(iRegLNoSp dst, iRegL isrc, vReg vsrc, vRegD tmp,\n+                             pRegGov pgtmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            Matcher::vector_length_in_bytes(n->in(2)) < MaxVectorSize &&\n+            Matcher::vector_element_basic_type(n->in(2)) == T_LONG);\n+  match(Set dst (MaxReductionV isrc vsrc));\n+  effect(TEMP_DEF dst, TEMP tmp, TEMP pgtmp, KILL cr);\n+  format %{ \"reduce_maxL_partial $dst, $isrc, $vsrc\\t# KILL $tmp, $pgtmp, cr\" %}\n+  ins_encode %{\n+    __ sve_ptrue_lanecnt($pgtmp$$PRegister, __ D, Matcher::vector_length(this, $vsrc));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $isrc$$Register, $vsrc$$FloatRegister,\n+                           $pgtmp$$PRegister, $tmp$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ reduction maxF\n+\n+instruct reduce_maxF(vRegF dst, vRegF fsrc, vReg vsrc) %{\n+  predicate(Matcher::vector_element_basic_type(n->in(2)) == T_FLOAT &&\n+            (Matcher::vector_length_in_bytes(n->in(2)) <= 16 ||\n+             Matcher::vector_length_in_bytes(n->in(2)) == MaxVectorSize));\n+  match(Set dst (MaxReductionV fsrc vsrc));\n+  effect(TEMP_DEF dst);\n+  format %{ \"reduce_maxF $dst, $fsrc, $vsrc\" %}\n+  ins_encode %{\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this, $vsrc);\n+    if (length_in_bytes == 8) {\n+      __ fmaxp($dst$$FloatRegister, $vsrc$$FloatRegister, __ S);\n+    } else if (length_in_bytes == 16) {\n+      __ fmaxv($dst$$FloatRegister, __ T4S, $vsrc$$FloatRegister);\n+    } else {\n+      assert(UseSVE > 0 && length_in_bytes > 16, \"must be\");\n+      __ sve_fmaxv($dst$$FloatRegister, __ S, ptrue, $vsrc$$FloatRegister);\n+    }\n+    __ fmaxs($dst$$FloatRegister, $dst$$FloatRegister, $fsrc$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_maxF_partial(vRegF dst, vRegF fsrc, vReg vsrc, pRegGov pgtmp) %{\n+  predicate(Matcher::vector_element_basic_type(n->in(2)) == T_FLOAT &&\n+            Matcher::vector_length_in_bytes(n->in(2)) > 16 &&\n+            Matcher::vector_length_in_bytes(n->in(2)) < MaxVectorSize);\n+  match(Set dst (MaxReductionV fsrc vsrc));\n+  effect(TEMP_DEF dst, TEMP pgtmp);\n+  format %{ \"reduce_maxF_partial $dst, $fsrc, $vsrc\\t# KILL $pgtmp\" %}\n+  ins_encode %{\n+    assert(UseSVE > 0, \"must be sve\");\n+    __ sve_ptrue_lanecnt($pgtmp$$PRegister, __ S, Matcher::vector_length(this, $vsrc));\n+    __ sve_fmaxv($dst$$FloatRegister, __ S, $pgtmp$$PRegister, $vsrc$$FloatRegister);\n+    __ fmaxs($dst$$FloatRegister, $dst$$FloatRegister, $fsrc$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ reduction maxD\n+\n+instruct reduce_maxD(vRegD dst, vRegD dsrc, vReg vsrc) %{\n+  predicate(Matcher::vector_element_basic_type(n->in(2)) == T_DOUBLE &&\n+            (Matcher::vector_length_in_bytes(n->in(2)) == 16 ||\n+             Matcher::vector_length_in_bytes(n->in(2)) == MaxVectorSize));\n+  match(Set dst (MaxReductionV dsrc vsrc));\n+  effect(TEMP_DEF dst);\n+  format %{ \"reduce_maxD $dst, $dsrc, $vsrc\" %}\n+  ins_encode %{\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this, $vsrc);\n+    if (length_in_bytes == 16) {\n+      __ fmaxp($dst$$FloatRegister, $vsrc$$FloatRegister, __ D);\n+    } else {\n+      assert(UseSVE > 0 && length_in_bytes > 16, \"must be\");\n+      __ sve_fmaxv($dst$$FloatRegister, __ D, ptrue, $vsrc$$FloatRegister);\n+    }\n+    __ fmaxd($dst$$FloatRegister, $dst$$FloatRegister, $dsrc$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_maxD_partial(vRegD dst, vRegD dsrc, vReg vsrc, pRegGov pgtmp) %{\n+  predicate(Matcher::vector_element_basic_type(n->in(2)) == T_DOUBLE &&\n+            Matcher::vector_length_in_bytes(n->in(2)) > 16 &&\n+            Matcher::vector_length_in_bytes(n->in(2)) < MaxVectorSize);\n+  match(Set dst (MaxReductionV dsrc vsrc));\n+  effect(TEMP_DEF dst, TEMP pgtmp);\n+  format %{ \"reduce_maxD_partial $dst, $dsrc, $vsrc\\t# KILL $pgtmp\" %}\n+  ins_encode %{\n+      assert(UseSVE > 0, \"must be sve\");\n+    __ sve_ptrue_lanecnt($pgtmp$$PRegister, __ D, Matcher::vector_length(this, $vsrc));\n+    __ sve_fmaxv($dst$$FloatRegister, __ D, $pgtmp$$PRegister, $vsrc$$FloatRegister);\n+    __ fmaxd($dst$$FloatRegister, $dst$$FloatRegister, $dsrc$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ reduction max - predicated\n+\n+instruct reduce_maxI_masked(iRegINoSp dst, iRegIorL2I isrc, vReg vsrc, pRegGov pg,\n+                            vRegD tmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            (Matcher::vector_element_basic_type(n->in(1)->in(2)) == T_BYTE ||\n+             Matcher::vector_element_basic_type(n->in(1)->in(2)) == T_SHORT ||\n+             Matcher::vector_element_basic_type(n->in(1)->in(2)) == T_INT));\n+  match(Set dst (MaxReductionV (Binary isrc vsrc) pg));\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n+  format %{ \"reduce_maxI_masked $dst, $isrc, $pg, $vsrc\\t# KILL $tmp, cr\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this, $vsrc);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $isrc$$Register, $vsrc$$FloatRegister,\n+                           $pg$$PRegister, $tmp$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_maxL_masked(iRegLNoSp dst, iRegL isrc, vReg vsrc, pRegGov pg,\n+                            vRegD tmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 && Matcher::vector_element_basic_type(n->in(1)->in(2)) == T_LONG);\n+  match(Set dst (MaxReductionV (Binary isrc vsrc) pg));\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n+  format %{ \"reduce_maxL_masked $dst, $isrc, $pg, $vsrc\\t# KILL $tmp, cr\" %}\n+  ins_encode %{\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $isrc$$Register, $vsrc$$FloatRegister,\n+                           $pg$$PRegister, $tmp$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_maxF_masked(vRegF dst, vRegF fsrc, vReg vsrc, pRegGov pg) %{\n+  predicate(UseSVE > 0 && Matcher::vector_element_basic_type(n->in(1)->in(2)) == T_FLOAT);\n+  match(Set dst (MaxReductionV (Binary fsrc vsrc) pg));\n+  effect(TEMP_DEF dst);\n+  format %{ \"reduce_maxF_masked $dst, $fsrc, $pg, $vsrc\" %}\n+  ins_encode %{\n+    __ sve_fmaxv($dst$$FloatRegister, __ S, $pg$$PRegister, $vsrc$$FloatRegister);\n+    __ fmaxs($dst$$FloatRegister, $dst$$FloatRegister, $fsrc$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_maxD_masked(vRegD dst, vRegD dsrc, vReg vsrc, pRegGov pg) %{\n+  predicate(UseSVE > 0 && Matcher::vector_element_basic_type(n->in(1)->in(2)) == T_DOUBLE);\n+  match(Set dst (MaxReductionV (Binary dsrc vsrc) pg));\n+  effect(TEMP_DEF dst);\n+  format %{ \"reduce_maxD_masked $dst, $dsrc, $pg, $vsrc\" %}\n+  ins_encode %{\n+    __ sve_fmaxv($dst$$FloatRegister, __ D, $pg$$PRegister, $vsrc$$FloatRegister);\n+    __ fmaxd($dst$$FloatRegister, $dst$$FloatRegister, $dsrc$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ Vector reduction min -------------------------\n+\n+\/\/ reduction minI\n+\n+instruct reduce_minI_le128b(iRegINoSp dst, iRegIorL2I isrc, vReg vsrc,\n+                            vReg tmp, rFlagsReg cr) %{\n+  predicate(Matcher::vector_length_in_bytes(n->in(2)) <= 16 &&\n+            (Matcher::vector_element_basic_type(n->in(2)) == T_BYTE ||\n+             Matcher::vector_element_basic_type(n->in(2)) == T_SHORT ||\n+             Matcher::vector_element_basic_type(n->in(2)) == T_INT));\n+  match(Set dst (MinReductionV isrc vsrc));\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n+  format %{ \"reduce_minI_le128b $dst, $isrc, $vsrc\\t# vector <= 128 bits. KILL $tmp, cr\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this, $vsrc);\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this, $vsrc);\n+    __ neon_reduce_minmax_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                                   $isrc$$Register, $vsrc$$FloatRegister,\n+                                   length_in_bytes, $tmp$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_minI_gt128b(iRegINoSp dst, iRegIorL2I isrc, vReg vsrc,\n+                            vRegD tmp, rFlagsReg cr) %{\n+  predicate(Matcher::vector_length_in_bytes(n->in(2)) > 16 &&\n+            Matcher::vector_length_in_bytes(n->in(2)) == MaxVectorSize &&\n+            (Matcher::vector_element_basic_type(n->in(2)) == T_BYTE ||\n+             Matcher::vector_element_basic_type(n->in(2)) == T_SHORT ||\n+             Matcher::vector_element_basic_type(n->in(2)) == T_INT));\n+  match(Set dst (MinReductionV isrc vsrc));\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n+  format %{ \"reduce_minI_gt128b $dst, $isrc, $vsrc\\t# vector > 128 bits. KILL $tmp, cr\" %}\n+  ins_encode %{\n+    assert(UseSVE > 0, \"must be sve\");\n+    BasicType bt = Matcher::vector_element_basic_type(this, $vsrc);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $isrc$$Register, $vsrc$$FloatRegister,\n+                           ptrue, $tmp$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_minI_partial(iRegINoSp dst, iRegIorL2I isrc, vReg vsrc, vRegD tmp,\n+                             pRegGov pgtmp, rFlagsReg cr) %{\n+  predicate(Matcher::vector_length_in_bytes(n->in(2)) > 16 &&\n+            Matcher::vector_length_in_bytes(n->in(2)) < MaxVectorSize &&\n+            (Matcher::vector_element_basic_type(n->in(2)) == T_BYTE ||\n+             Matcher::vector_element_basic_type(n->in(2)) == T_SHORT ||\n+             Matcher::vector_element_basic_type(n->in(2)) == T_INT));\n+  match(Set dst (MinReductionV isrc vsrc));\n+  effect(TEMP_DEF dst, TEMP tmp, TEMP pgtmp, KILL cr);\n+  format %{ \"reduce_minI_partial $dst, $isrc, $vsrc\\t# KILL $tmp, $pgtmp, cr\" %}\n+  ins_encode %{\n+    assert(UseSVE > 0, \"must be sve\");\n+    BasicType bt = Matcher::vector_element_basic_type(this, $vsrc);\n+    __ sve_ptrue_lanecnt($pgtmp$$PRegister, __ elemType_to_regVariant(bt),\n+                         Matcher::vector_length(this, $vsrc));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $isrc$$Register, $vsrc$$FloatRegister,\n+                           $pgtmp$$PRegister, $tmp$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ reduction minL\n+\n+instruct reduce_minL_neon(iRegLNoSp dst, iRegL isrc, vReg vsrc, rFlagsReg cr) %{\n+  predicate(UseSVE == 0 &&\n+            Matcher::vector_element_basic_type(n->in(2)) == T_LONG);\n+  match(Set dst (MinReductionV isrc vsrc));\n+  effect(TEMP_DEF dst, KILL cr);\n+  format %{ \"reduce_minL_neon $dst, $isrc, $vsrc\\t# 2L. KILL cr\" %}\n+  ins_encode %{\n+    __ neon_reduce_minmax_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                                   $isrc$$Register, $vsrc$$FloatRegister,\n+                                   \/* vector_length_in_bytes *\/ 16, fnoreg);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_minL_sve(iRegLNoSp dst, iRegL isrc, vReg vsrc,\n+                         vRegD tmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            Matcher::vector_length_in_bytes(n->in(2)) == MaxVectorSize &&\n+            Matcher::vector_element_basic_type(n->in(2)) == T_LONG);\n+  match(Set dst (MinReductionV isrc vsrc));\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n+  format %{ \"reduce_minL_sve $dst, $isrc, $vsrc\\t# KILL $tmp, cr\" %}\n+  ins_encode %{\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $isrc$$Register, $vsrc$$FloatRegister,\n+                           ptrue, $tmp$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_minL_partial(iRegLNoSp dst, iRegL isrc, vReg vsrc, vRegD tmp,\n+                             pRegGov pgtmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            Matcher::vector_length_in_bytes(n->in(2)) < MaxVectorSize &&\n+            Matcher::vector_element_basic_type(n->in(2)) == T_LONG);\n+  match(Set dst (MinReductionV isrc vsrc));\n+  effect(TEMP_DEF dst, TEMP tmp, TEMP pgtmp, KILL cr);\n+  format %{ \"reduce_minL_partial $dst, $isrc, $vsrc\\t# KILL $tmp, $pgtmp, cr\" %}\n+  ins_encode %{\n+    __ sve_ptrue_lanecnt($pgtmp$$PRegister, __ D, Matcher::vector_length(this, $vsrc));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $isrc$$Register, $vsrc$$FloatRegister,\n+                           $pgtmp$$PRegister, $tmp$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ reduction minF\n+\n+instruct reduce_minF(vRegF dst, vRegF fsrc, vReg vsrc) %{\n+  predicate(Matcher::vector_element_basic_type(n->in(2)) == T_FLOAT &&\n+            (Matcher::vector_length_in_bytes(n->in(2)) <= 16 ||\n+             Matcher::vector_length_in_bytes(n->in(2)) == MaxVectorSize));\n+  match(Set dst (MinReductionV fsrc vsrc));\n+  effect(TEMP_DEF dst);\n+  format %{ \"reduce_minF $dst, $fsrc, $vsrc\" %}\n+  ins_encode %{\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this, $vsrc);\n+    if (length_in_bytes == 8) {\n+      __ fminp($dst$$FloatRegister, $vsrc$$FloatRegister, __ S);\n+    } else if (length_in_bytes == 16) {\n+      __ fminv($dst$$FloatRegister, __ T4S, $vsrc$$FloatRegister);\n+    } else {\n+      assert(UseSVE > 0 && length_in_bytes > 16, \"must be\");\n+      __ sve_fminv($dst$$FloatRegister, __ S, ptrue, $vsrc$$FloatRegister);\n+    }\n+    __ fmins($dst$$FloatRegister, $dst$$FloatRegister, $fsrc$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_minF_partial(vRegF dst, vRegF fsrc, vReg vsrc, pRegGov pgtmp) %{\n+  predicate(Matcher::vector_element_basic_type(n->in(2)) == T_FLOAT &&\n+            Matcher::vector_length_in_bytes(n->in(2)) > 16 &&\n+            Matcher::vector_length_in_bytes(n->in(2)) < MaxVectorSize);\n+  match(Set dst (MinReductionV fsrc vsrc));\n+  effect(TEMP_DEF dst, TEMP pgtmp);\n+  format %{ \"reduce_minF_partial $dst, $fsrc, $vsrc\\t# KILL $pgtmp\" %}\n+  ins_encode %{\n+    assert(UseSVE > 0, \"must be sve\");\n+    __ sve_ptrue_lanecnt($pgtmp$$PRegister, __ S, Matcher::vector_length(this, $vsrc));\n+    __ sve_fminv($dst$$FloatRegister, __ S, $pgtmp$$PRegister, $vsrc$$FloatRegister);\n+    __ fmins($dst$$FloatRegister, $dst$$FloatRegister, $fsrc$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ reduction minD\n+\n+instruct reduce_minD(vRegD dst, vRegD dsrc, vReg vsrc) %{\n+  predicate(Matcher::vector_element_basic_type(n->in(2)) == T_DOUBLE &&\n+            (Matcher::vector_length_in_bytes(n->in(2)) == 16 ||\n+             Matcher::vector_length_in_bytes(n->in(2)) == MaxVectorSize));\n+  match(Set dst (MinReductionV dsrc vsrc));\n+  effect(TEMP_DEF dst);\n+  format %{ \"reduce_minD $dst, $dsrc, $vsrc\" %}\n+  ins_encode %{\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this, $vsrc);\n+    if (length_in_bytes == 16) {\n+      __ fminp($dst$$FloatRegister, $vsrc$$FloatRegister, __ D);\n+    } else {\n+      assert(UseSVE > 0 && length_in_bytes > 16, \"must be\");\n+      __ sve_fminv($dst$$FloatRegister, __ D, ptrue, $vsrc$$FloatRegister);\n+    }\n+    __ fmind($dst$$FloatRegister, $dst$$FloatRegister, $dsrc$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_minD_partial(vRegD dst, vRegD dsrc, vReg vsrc, pRegGov pgtmp) %{\n+  predicate(Matcher::vector_element_basic_type(n->in(2)) == T_DOUBLE &&\n+            Matcher::vector_length_in_bytes(n->in(2)) > 16 &&\n+            Matcher::vector_length_in_bytes(n->in(2)) < MaxVectorSize);\n+  match(Set dst (MinReductionV dsrc vsrc));\n+  effect(TEMP_DEF dst, TEMP pgtmp);\n+  format %{ \"reduce_minD_partial $dst, $dsrc, $vsrc\\t# KILL $pgtmp\" %}\n+  ins_encode %{\n+    assert(UseSVE > 0, \"must be sve\");\n+    __ sve_ptrue_lanecnt($pgtmp$$PRegister, __ D, Matcher::vector_length(this, $vsrc));\n+    __ sve_fminv($dst$$FloatRegister, __ D, $pgtmp$$PRegister, $vsrc$$FloatRegister);\n+    __ fmind($dst$$FloatRegister, $dst$$FloatRegister, $dsrc$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ reduction min - predicated\n+\n+instruct reduce_minI_masked(iRegINoSp dst, iRegIorL2I isrc, vReg vsrc, pRegGov pg,\n+                            vRegD tmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            (Matcher::vector_element_basic_type(n->in(1)->in(2)) == T_BYTE ||\n+             Matcher::vector_element_basic_type(n->in(1)->in(2)) == T_SHORT ||\n+             Matcher::vector_element_basic_type(n->in(1)->in(2)) == T_INT));\n+  match(Set dst (MinReductionV (Binary isrc vsrc) pg));\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n+  format %{ \"reduce_minI_masked $dst, $isrc, $pg, $vsrc\\t# KILL $tmp, cr\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this, $vsrc);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $isrc$$Register, $vsrc$$FloatRegister,\n+                           $pg$$PRegister, $tmp$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_minL_masked(iRegLNoSp dst, iRegL isrc, vReg vsrc, pRegGov pg,\n+                            vRegD tmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 && Matcher::vector_element_basic_type(n->in(1)->in(2)) == T_LONG);\n+  match(Set dst (MinReductionV (Binary isrc vsrc) pg));\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n+  format %{ \"reduce_minL_masked $dst, $isrc, $pg, $vsrc\\t# KILL $tmp, cr\" %}\n+  ins_encode %{\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $isrc$$Register, $vsrc$$FloatRegister,\n+                           $pg$$PRegister, $tmp$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_minF_masked(vRegF dst, vRegF fsrc, vReg vsrc, pRegGov pg) %{\n+  predicate(UseSVE > 0 && Matcher::vector_element_basic_type(n->in(1)->in(2)) == T_FLOAT);\n+  match(Set dst (MinReductionV (Binary fsrc vsrc) pg));\n+  format %{ \"reduce_minF_masked $dst, $fsrc, $pg, $vsrc\" %}\n+  ins_encode %{\n+    __ sve_fminv($dst$$FloatRegister, __ S, $pg$$PRegister, $vsrc$$FloatRegister);\n+    __ fmins($dst$$FloatRegister, $dst$$FloatRegister, $fsrc$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_minD_masked(vRegD dst, vRegD dsrc, vReg vsrc, pRegGov pg) %{\n+  predicate(UseSVE > 0 && Matcher::vector_element_basic_type(n->in(1)->in(2)) == T_DOUBLE);\n+  match(Set dst (MinReductionV (Binary dsrc vsrc) pg));\n+  effect(TEMP_DEF dst);\n+  format %{ \"reduce_minD_masked $dst, $dsrc, $pg, $vsrc\" %}\n+  ins_encode %{\n+    __ sve_fminv($dst$$FloatRegister, __ D, $pg$$PRegister, $vsrc$$FloatRegister);\n+    __ fmind($dst$$FloatRegister, $dst$$FloatRegister, $dsrc$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ Vector reinterpret ---------------------------\n+\n+instruct reinterpret_same_size(vReg dst_src) %{\n+  predicate(Matcher::vector_length_in_bytes(n) == Matcher::vector_length_in_bytes(n->in(1)));\n+  match(Set dst_src (VectorReinterpret dst_src));\n+  ins_cost(0);\n+  format %{ \"reinterpret_same_size $dst_src\\t# do nothing\" %}\n+  ins_encode(\/* empty encoding *\/);\n+  ins_pipe(pipe_class_empty);\n+%}\n+\n+instruct reinterpret_resize_le128b(vReg dst, vReg src) %{\n+  predicate(Matcher::vector_length_in_bytes(n) != Matcher::vector_length_in_bytes(n->in(1)) &&\n+            Matcher::vector_length_in_bytes(n) <= 16 &&\n+            Matcher::vector_length_in_bytes(n->in(1)) <= 16);\n+  match(Set dst (VectorReinterpret src));\n+  format %{ \"reinterpret_resize_le128b $dst, $src\\t# vector <= 128 bits.\" %}\n+  ins_encode %{\n+    uint length_in_bytes_src = Matcher::vector_length_in_bytes(this, $src);\n+    uint length_in_bytes_dst = Matcher::vector_length_in_bytes(this);\n+    \/\/ The higher bits in \"dst\" register must be cleared to zero.\n+    if ((length_in_bytes_src == 4 && length_in_bytes_dst == 8) ||\n+        (length_in_bytes_src == 8 && length_in_bytes_dst == 4)) {\n+      \/\/ Reinterpret between 32 bits and 64 bits\n+      __ dup($dst$$FloatRegister, __ S, $src$$FloatRegister);\n+    } else if ((length_in_bytes_src == 4 && length_in_bytes_dst == 16) ||\n+               (length_in_bytes_src == 16 && length_in_bytes_dst == 4)) {\n+      \/\/ Reinterpret between 32 bits and 128 bits\n+      __ dup($dst$$FloatRegister, __ S, $src$$FloatRegister);\n+    } else if ((length_in_bytes_src == 8 && length_in_bytes_dst == 16) ||\n+               (length_in_bytes_src == 16 && length_in_bytes_dst == 8)) {\n+      \/\/ Reinterpret between 64 bits and 128 bits\n+      __ orr($dst$$FloatRegister, __ T8B, $src$$FloatRegister, $src$$FloatRegister);\n+    } else {\n+      assert(false, \"invalid vector length\");\n+      ShouldNotReachHere();\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reinterpret_resize_gt128b(vReg dst, vReg src, pReg ptmp) %{\n+  predicate(Matcher::vector_length_in_bytes(n) != Matcher::vector_length_in_bytes(n->in(1)) &&\n+            (Matcher::vector_length_in_bytes(n) > 16 ||\n+             Matcher::vector_length_in_bytes(n->in(1)) > 16));\n+  match(Set dst (VectorReinterpret src));\n+  effect(TEMP_DEF dst, TEMP ptmp);\n+  format %{ \"reinterpret_resize_gt128b $dst, $src\\t# vector > 128 bits. KILL $ptmp\" %}\n+  ins_encode %{\n+    assert(UseSVE > 0, \"must be sve\");\n+    uint length_in_bytes_src = Matcher::vector_length_in_bytes(this, $src);\n+    uint length_in_bytes_dst = Matcher::vector_length_in_bytes(this);\n+    uint length_in_bytes_resize = length_in_bytes_src < length_in_bytes_dst ?\n+                                  length_in_bytes_src : length_in_bytes_dst;\n+    assert(length_in_bytes_src <= MaxVectorSize && length_in_bytes_dst <= MaxVectorSize,\n+           \"invalid vector length\");\n+    __ sve_ptrue_lanecnt($ptmp$$PRegister, __ B, length_in_bytes_resize);\n+    __ sve_dup($dst$$FloatRegister, __ B, 0);\n+    __ sve_sel($dst$$FloatRegister, __ B, $ptmp$$PRegister,\n+               $src$$FloatRegister, $dst$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ Vector cast ----------------------------------\n+\n+\/\/ VectorCastB2X\n+\n+instruct vcvtBtoX(vReg dst, vReg src) %{\n+  match(Set dst (VectorCastB2X src));\n+  format %{ \"vcvtBtoX $dst, $src\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this);\n+    if ((length_in_bytes == 8 || length_in_bytes == 16) && bt == T_SHORT) {\n+      \/\/ 4B\/8B to 4S\/8S\n+      __ sxtl($dst$$FloatRegister, __ T8H, $src$$FloatRegister, __ T8B);\n+    } else if (length_in_bytes == 16 && type2aelembytes(bt) == 4) {\n+      \/\/ 4B to 4I\/4F\n+      __ sxtl($dst$$FloatRegister, __ T8H, $src$$FloatRegister, __ T8B);\n+      __ sxtl($dst$$FloatRegister, __ T4S, $dst$$FloatRegister, __ T4H);\n+      if (bt == T_FLOAT) {\n+        __ scvtfv(__ T4S, $dst$$FloatRegister, $dst$$FloatRegister);\n+      }\n+    } else {\n+      assert(UseSVE > 0, \"must be sve\");\n+      Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+      __ sve_vector_extend($dst$$FloatRegister, size, $src$$FloatRegister, __ B);\n+      if (is_floating_point_type(bt)) {\n+        __ sve_scvtf($dst$$FloatRegister, size, ptrue, $dst$$FloatRegister, size);\n+      }\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ VectorCastS2X\n+\n+instruct vcvtStoB(vReg dst, vReg src, vReg tmp) %{\n+  predicate(Matcher::vector_element_basic_type(n) == T_BYTE);\n+  match(Set dst (VectorCastS2X src));\n+  effect(TEMP tmp);\n+  format %{ \"vcvtStoB $dst, $src\\t# KILL $tmp\" %}\n+  ins_encode %{\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this);\n+    if (length_in_bytes == 4 || length_in_bytes == 8) {\n+      \/\/ 4S\/8S to 4B\/8B\n+      __ xtn($dst$$FloatRegister, __ T8B, $src$$FloatRegister, __ T8H);\n+    } else {\n+      assert(UseSVE > 0, \"must be sve\");\n+      __ sve_vector_narrow($dst$$FloatRegister, __ B,\n+                           $src$$FloatRegister, __ H, $tmp$$FloatRegister);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vcvtStoX_extend(vReg dst, vReg src) %{\n+  predicate(type2aelembytes(Matcher::vector_element_basic_type(n)) >= 4);\n+  match(Set dst (VectorCastS2X src));\n+  format %{ \"vcvtStoX_extend $dst, $src\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this);\n+    if (length_in_bytes == 16 && type2aelembytes(bt) == 4) {\n+      \/\/ 4S to 4I\/4F\n+      __ sxtl($dst$$FloatRegister, __ T4S, $src$$FloatRegister, __ T4H);\n+      if (bt == T_FLOAT) {\n+        __ scvtfv(__ T4S, $dst$$FloatRegister, $dst$$FloatRegister);\n+      }\n+    } else {\n+      assert(UseSVE > 0, \"must be sve\");\n+      Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+      __ sve_vector_extend($dst$$FloatRegister, size, $src$$FloatRegister, __ H);\n+      if (is_floating_point_type(bt)) {\n+        __ sve_scvtf($dst$$FloatRegister, size, ptrue, $dst$$FloatRegister, size);\n+      }\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ VectorCastI2X\n+\n+instruct vcvtItoX_narrow(vReg dst, vReg src, vReg tmp) %{\n+  predicate(Matcher::vector_element_basic_type(n) == T_BYTE ||\n+            Matcher::vector_element_basic_type(n) == T_SHORT);\n+  match(Set dst (VectorCastI2X src));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"vcvtItoX_narrow $dst, $src\\t# KILL $tmp\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this);\n+    if (length_in_bytes == 4 && bt == T_BYTE) {\n+      \/\/ 4I to 4B\n+      __ xtn($dst$$FloatRegister, __ T4H, $src$$FloatRegister, __ T4S);\n+      __ xtn($dst$$FloatRegister, __ T8B, $dst$$FloatRegister, __ T8H);\n+    } else if (length_in_bytes == 8 && bt == T_SHORT) {\n+      \/\/ 4I to 4S\n+      __ xtn($dst$$FloatRegister, __ T4H, $src$$FloatRegister, __ T4S);\n+    } else {\n+      assert(UseSVE > 0, \"must be sve\");\n+      __ sve_vector_narrow($dst$$FloatRegister, __ elemType_to_regVariant(bt),\n+                           $src$$FloatRegister, __ S, $tmp$$FloatRegister);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vcvtItoX(vReg dst, vReg src) %{\n+  predicate(type2aelembytes(Matcher::vector_element_basic_type(n)) >= 4);\n+  match(Set dst (VectorCastI2X src));\n+  format %{ \"vcvtItoX $dst, $src\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    uint length_in_bytes =  Matcher::vector_length_in_bytes(this);\n+    if (bt == T_LONG) {\n+      if (length_in_bytes == 16) {\n+        \/\/ 2I to 2L\n+        __ sxtl($dst$$FloatRegister, __ T2D, $src$$FloatRegister, __ T2S);\n+      } else {\n+        assert(UseSVE > 0, \"must be sve\");\n+        __ sve_vector_extend($dst$$FloatRegister, __ D, $src$$FloatRegister, __ S);\n+      }\n+    } else if (bt == T_FLOAT) {\n+      if (length_in_bytes <= 16) {\n+        \/\/ 2I\/4I to 2F\/4F\n+        __ scvtfv(get_arrangement(this), $dst$$FloatRegister, $src$$FloatRegister);\n+      } else {\n+        assert(UseSVE > 0, \"must be sve\");\n+        __ sve_scvtf($dst$$FloatRegister, __ S, ptrue, $src$$FloatRegister, __ S);\n+      }\n+    } else {\n+      assert(bt == T_DOUBLE, \"unsupported type\");\n+      if (length_in_bytes == 16) {\n+        \/\/ 2I to 2D\n+        __ sxtl($dst$$FloatRegister, __ T2D, $src$$FloatRegister, __ T2S);\n+        __ scvtfv(__ T2D, $dst$$FloatRegister, $dst$$FloatRegister);\n+      } else {\n+        assert(UseSVE > 0, \"must be sve\");\n+        __ sve_sunpklo($dst$$FloatRegister, __ D, $src$$FloatRegister);\n+        __ sve_scvtf($dst$$FloatRegister, __ D, ptrue, $dst$$FloatRegister, __ D);\n+      }\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ VectorCastL2X\n+\n+instruct vcvt2Lto2I(vReg dst, vReg src) %{\n+  predicate(Matcher::vector_element_basic_type(n) == T_INT &&\n+            Matcher::vector_length_in_bytes(n) == 8);\n+  match(Set dst (VectorCastL2X src));\n+  format %{ \"vcvt2Lto2I $dst, $src\" %}\n+  ins_encode %{\n+    \/\/ 2L to 2I\n+    __ xtn($dst$$FloatRegister, __ T2S, $src$$FloatRegister, __ T2D);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vcvtLtoI_sve(vReg dst, vReg src, vReg tmp) %{\n+  predicate((Matcher::vector_element_basic_type(n) == T_INT &&\n+             Matcher::vector_length_in_bytes(n) > 8) ||\n+            Matcher::vector_element_basic_type(n) == T_BYTE ||\n+            Matcher::vector_element_basic_type(n) == T_SHORT);\n+  match(Set dst (VectorCastL2X src));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"vcvtLtoI_sve $dst, $src\\t# KILL $tmp\" %}\n+  ins_encode %{\n+    assert(UseSVE > 0, \"must be sve\");\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ sve_vector_narrow($dst$$FloatRegister, __ elemType_to_regVariant(bt),\n+                         $src$$FloatRegister, __ D, $tmp$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vcvtLtoF_neon(vReg dst, vReg src, vRegF tmp) %{\n+  predicate(UseSVE == 0 && Matcher::vector_element_basic_type(n) == T_FLOAT);\n+  match(Set dst (VectorCastL2X src));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"vcvtLtoF_neon $dst, $src\\t# 2L to 2F. KILL $tmp\" %}\n+  ins_encode %{\n+    \/\/ 2L to 2F\n+    __ umov(rscratch1, $src$$FloatRegister, __ D, 0);\n+    __ scvtfs($dst$$FloatRegister, rscratch1);\n+    __ umov(rscratch1, $src$$FloatRegister, __ D, 1);\n+    __ scvtfs($tmp$$FloatRegister, rscratch1);\n+    __ ins($dst$$FloatRegister, __ S, $tmp$$FloatRegister, 1, 0);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vcvtLtoF_sve(vReg dst, vReg src, vReg tmp) %{\n+  predicate(UseSVE > 0 && Matcher::vector_element_basic_type(n) == T_FLOAT);\n+  match(Set dst (VectorCastL2X src));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"vcvtLtoF_sve $dst, $src\\t# KILL $tmp\" %}\n+  ins_encode %{\n+    __ sve_scvtf($dst$$FloatRegister, __ S, ptrue, $src$$FloatRegister, __ D);\n+    __ sve_vector_narrow($dst$$FloatRegister, __ S,\n+                         $dst$$FloatRegister, __ D, $tmp$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vcvtLtoD(vReg dst, vReg src) %{\n+  predicate(Matcher::vector_element_basic_type(n) == T_DOUBLE);\n+  match(Set dst (VectorCastL2X src));\n+  format %{ \"vcvtLtoD $dst, $src\" %}\n+  ins_encode %{\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this);\n+    if (length_in_bytes == 16) {\n+      \/\/ 2L to 2D\n+      __ scvtfv(__ T2D, $dst$$FloatRegister, $src$$FloatRegister);\n+    } else {\n+      assert(UseSVE > 0 && length_in_bytes > 16, \"must be\");\n+      __ sve_scvtf($dst$$FloatRegister, __ D, ptrue, $src$$FloatRegister, __ D);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ VectorCastF2X\n+\n+instruct vcvt4Fto4X_narrow(vReg dst, vReg src) %{\n+  predicate(Matcher::vector_length(n) == 4 &&\n+            (Matcher::vector_element_basic_type(n) == T_BYTE ||\n+             Matcher::vector_element_basic_type(n) == T_SHORT));\n+  match(Set dst (VectorCastF2X src));\n+  effect(TEMP_DEF dst);\n+  format %{ \"vcvt4Fto4X_narrow $dst, $src\\t# 4F to 4B\/4S\" %}\n+  ins_encode %{\n+    \/\/ 4F to 4B\/4S\n+    __ fcvtzs($dst$$FloatRegister, __ T4S, $src$$FloatRegister);\n+    __ xtn($dst$$FloatRegister, __ T4H, $dst$$FloatRegister, __ T4S);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    if (bt == T_BYTE) {\n+      __ xtn($dst$$FloatRegister, __ T8B, $dst$$FloatRegister, __ T8H);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vcvtFtoX_narrow_sve(vReg dst, vReg src, vReg tmp) %{\n+  predicate(Matcher::vector_length(n) != 4 &&\n+            (Matcher::vector_element_basic_type(n) == T_BYTE ||\n+             Matcher::vector_element_basic_type(n) == T_SHORT));\n+  match(Set dst (VectorCastF2X src));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"vcvtFtoX_narrow_sve $dst, $src\\t# KILL $tmp\" %}\n+  ins_encode %{\n+    assert(UseSVE > 0, \"must be sve\");\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ sve_fcvtzs($dst$$FloatRegister, __ S, ptrue, $src$$FloatRegister, __ S);\n+    __ sve_vector_narrow($dst$$FloatRegister, __ elemType_to_regVariant(bt),\n+                         $dst$$FloatRegister, __ S, $tmp$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vcvtFtoX(vReg dst, vReg src) %{\n+  predicate(type2aelembytes(Matcher::vector_element_basic_type(n)) >= 4);\n+  match(Set dst (VectorCastF2X src));\n+  format %{ \"vcvtFtoX $dst, $src\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    uint length_in_bytes =  Matcher::vector_length_in_bytes(this);\n+    if (bt == T_INT) {\n+      if (length_in_bytes <= 16) {\n+        \/\/ 2F\/4F to 2I\/4I\n+        __ fcvtzs($dst$$FloatRegister, get_arrangement(this), $src$$FloatRegister);\n+      } else {\n+        assert(UseSVE > 0, \"must be sve\");\n+        __ sve_fcvtzs($dst$$FloatRegister, __ S, ptrue, $src$$FloatRegister, __ S);\n+      }\n+    } else if (bt == T_LONG) {\n+      if (length_in_bytes == 16) {\n+        \/\/ 2F to 2L\n+        __ fcvtl($dst$$FloatRegister, __ T2D, $src$$FloatRegister, __ T2S);\n+        __ fcvtzs($dst$$FloatRegister, __ T2D, $dst$$FloatRegister);\n+      } else {\n+        assert(UseSVE > 0 && length_in_bytes > 16, \"must be\");\n+        __ sve_sunpklo($dst$$FloatRegister, __ D, $src$$FloatRegister);\n+        __ sve_fcvtzs($dst$$FloatRegister, __ D, ptrue, $dst$$FloatRegister, __ S);\n+      }\n+    } else {\n+      assert(bt == T_DOUBLE, \"unsupported type\");\n+      if (length_in_bytes == 16) {\n+        \/\/ 2F to 2D\n+        __ fcvtl($dst$$FloatRegister, __ T2D, $src$$FloatRegister, __ T2S);\n+      } else {\n+        assert(UseSVE > 0 && length_in_bytes > 16, \"must be\");\n+        __ sve_vector_extend($dst$$FloatRegister, __ D, $src$$FloatRegister, __ S);\n+        __ sve_fcvt($dst$$FloatRegister, __ D, ptrue, $dst$$FloatRegister, __ S);\n+      }\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ VectorCastD2X\n+\n+instruct vcvtDtoI_neon(vReg dst, vReg src) %{\n+  predicate(UseSVE == 0 && Matcher::vector_element_basic_type(n) == T_INT);\n+  match(Set dst (VectorCastD2X src));\n+  effect(TEMP_DEF dst);\n+  format %{ \"vcvtDtoI_neon $dst, $src\\t# 2D to 2I\" %}\n+  ins_encode %{\n+    \/\/ 2D to 2I\n+    __ ins($dst$$FloatRegister, __ D, $src$$FloatRegister, 0, 1);\n+    \/\/ We can't use fcvtzs(vector, integer) instruction here because we need\n+    \/\/ saturation arithmetic. See JDK-8276151.\n+    __ fcvtzdw(rscratch1, $src$$FloatRegister);\n+    __ fcvtzdw(rscratch2, $dst$$FloatRegister);\n+    __ fmovs($dst$$FloatRegister, rscratch1);\n+    __ mov($dst$$FloatRegister, __ S, 1, rscratch2);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vcvtDtoI_sve(vReg dst, vReg src, vReg tmp) %{\n+  predicate(UseSVE > 0 &&\n+            (Matcher::vector_element_basic_type(n) == T_BYTE ||\n+             Matcher::vector_element_basic_type(n) == T_SHORT ||\n+             Matcher::vector_element_basic_type(n) == T_INT));\n+  match(Set dst (VectorCastD2X src));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"vcvtDtoI_sve $dst, $src\\t# KILL $tmp\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ sve_fcvtzs($dst$$FloatRegister, __ S, ptrue, $src$$FloatRegister, __ D);\n+    __ sve_vector_narrow($dst$$FloatRegister, __ elemType_to_regVariant(bt),\n+                         $dst$$FloatRegister, __ D, $tmp$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vcvtDtoL(vReg dst, vReg src) %{\n+  predicate(Matcher::vector_element_basic_type(n) == T_LONG);\n+  match(Set dst (VectorCastD2X src));\n+  format %{ \"vcvtDtoL $dst, $src\" %}\n+  ins_encode %{\n+    uint length_in_bytes =  Matcher::vector_length_in_bytes(this);\n+    if (length_in_bytes == 16) {\n+      \/\/ 2D to 2L\n+      __ fcvtzs($dst$$FloatRegister, __ T2D, $src$$FloatRegister);\n+    } else {\n+      assert(UseSVE > 0 && length_in_bytes > 16, \"must be\");\n+      __ sve_fcvtzs($dst$$FloatRegister, __ D, ptrue, $src$$FloatRegister, __ D);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vcvtDtoF_64b(vReg dst, vReg src) %{\n+  predicate(Matcher::vector_element_basic_type(n) == T_FLOAT &&\n+            Matcher::vector_length_in_bytes(n) == 8);\n+  match(Set dst (VectorCastD2X src));\n+  format %{ \"vcvtDtoF_64b $dst, $src\\t# 2D to 2F\" %}\n+  ins_encode %{\n+    \/\/ 2D to 2F\n+    __ fcvtn($dst$$FloatRegister, __ T2S, $src$$FloatRegister, __ T2D);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vcvtDtoF_gt64b(vReg dst, vReg src, vReg tmp) %{\n+  predicate(Matcher::vector_element_basic_type(n) == T_FLOAT &&\n+            Matcher::vector_length_in_bytes(n) > 8);\n+  match(Set dst (VectorCastD2X src));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"vcvtDtoF_gt64b $dst, $src\\t# vector > 64 bits. KILL $tmp\" %}\n+  ins_encode %{\n+    __ sve_fcvt($dst$$FloatRegister, __ S, ptrue, $src$$FloatRegister, __ D);\n+    __ sve_vector_narrow($dst$$FloatRegister, __ S,\n+                         $dst$$FloatRegister, __ D, $tmp$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ Replicate ------------------------------------\n+\n+\/\/ replicate from reg\n+\n+instruct replicateI(vReg dst, iRegIorL2I src) %{\n+  match(Set dst (ReplicateB src));\n+  match(Set dst (ReplicateS src));\n+  match(Set dst (ReplicateI src));\n+  format %{ \"replicateI $dst, $src\" %}\n+  ins_encode %{\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this);\n+    if (length_in_bytes <= 16) {\n+      __ dup($dst$$FloatRegister, get_arrangement(this), $src$$Register);\n+    } else {\n+      assert(UseSVE > 0, \"must be sve\");\n+      BasicType bt = Matcher::vector_element_basic_type(this);\n+      __ sve_dup($dst$$FloatRegister, __ elemType_to_regVariant(bt), $src$$Register);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct replicateL(vReg dst, iRegL src) %{\n+  match(Set dst (ReplicateL src));\n+  format %{ \"replicateL $dst, $src\" %}\n+  ins_encode %{\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this);\n+    if (length_in_bytes == 16) {\n+      __ dup($dst$$FloatRegister, __ T2D, $src$$Register);\n+    } else {\n+      assert(UseSVE > 0 && length_in_bytes > 16, \"must be\");\n+      __ sve_dup($dst$$FloatRegister, __ D, $src$$Register);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct replicateF(vReg dst, vRegF src) %{\n+  match(Set dst (ReplicateF src));\n+  format %{ \"replicateF $dst, $src\" %}\n+  ins_encode %{\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this);\n+    if (length_in_bytes <= 16) {\n+      __ dup($dst$$FloatRegister, length_in_bytes == 16 ? __ T4S: __ T2S,\n+             $src$$FloatRegister);\n+    } else {\n+      assert(UseSVE > 0, \"must be sve\");\n+      __ sve_cpy($dst$$FloatRegister, __ S, ptrue, $src$$FloatRegister);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct replicateD(vReg dst, vRegD src) %{\n+  match(Set dst (ReplicateD src));\n+  format %{ \"replicateD $dst, $src\" %}\n+  ins_encode %{\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this);\n+    if (length_in_bytes == 16) {\n+      __ dup($dst$$FloatRegister, __ T2D, $src$$FloatRegister);\n+    } else {\n+      assert(UseSVE > 0 && length_in_bytes > 16, \"must be\");\n+      __ sve_cpy($dst$$FloatRegister, __ D, ptrue, $src$$FloatRegister);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ replicate from imm\n+\n+instruct replicateI_imm_le128b(vReg dst, immI con) %{\n+  predicate(Matcher::vector_length_in_bytes(n) <= 16);\n+  match(Set dst (ReplicateB con));\n+  match(Set dst (ReplicateS con));\n+  match(Set dst (ReplicateI con));\n+  format %{ \"replicateI_imm_le128b $dst, $con\\t# vector <= 128 bits\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int imm = (int)$con$$constant;\n+    if (type2aelembytes(bt) == 1) {\n+      \/\/ Refine imm for B\n+      imm = imm & 0xff;\n+    } else if (type2aelembytes(bt) == 2) {\n+      \/\/ Refine imm for S\n+      imm = imm & 0xffff;\n+    }\n+    __ mov($dst$$FloatRegister, get_arrangement(this), imm);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct replicateB_imm8_gt128b(vReg dst, immI8 con) %{\n+  predicate(Matcher::vector_length_in_bytes(n) > 16);\n+  match(Set dst (ReplicateB con));\n+  format %{ \"replicateB_imm8_gt128b $dst, $con\\t# vector > 128 bits\" %}\n+  ins_encode %{\n+    assert(UseSVE > 0, \"must be sve\");\n+    __ sve_dup($dst$$FloatRegister, __ B, (int)($con$$constant));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct replicateS_imm8_gt128b(vReg dst, immI8_shift8 con) %{\n+  predicate(Matcher::vector_length_in_bytes(n) > 16);\n+  match(Set dst (ReplicateS con));\n+  format %{ \"replicateS_imm8_gt128b $dst, $con\\t# vector > 128 bits\" %}\n+  ins_encode %{\n+    assert(UseSVE > 0, \"must be sve\");\n+    __ sve_dup($dst$$FloatRegister, __ H, (int)($con$$constant));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct replicateI_imm8_gt128b(vReg dst, immI8_shift8 con) %{\n+  predicate(Matcher::vector_length_in_bytes(n) > 16);\n+  match(Set dst (ReplicateI con));\n+  format %{ \"replicateI_imm8_gt128b $dst, $con\\t# vector > 128 bits\" %}\n+  ins_encode %{\n+    assert(UseSVE > 0, \"must be sve\");\n+    __ sve_dup($dst$$FloatRegister, __ S, (int)($con$$constant));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct replicateL_imm_128b(vReg dst, immL con) %{\n+  predicate(Matcher::vector_length_in_bytes(n) == 16);\n+  match(Set dst (ReplicateL con));\n+  format %{ \"replicateL_imm_128b $dst, $con\\t# vector > 128 bits\" %}\n+  ins_encode %{\n+    __ mov($dst$$FloatRegister, __ T2D, (uint64_t)($con$$constant));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct replicateL_imm8_gt128b(vReg dst, immL8_shift8 con) %{\n+  predicate(Matcher::vector_length_in_bytes(n) > 16);\n+  match(Set dst (ReplicateL con));\n+  format %{ \"replicateL_imm8_gt128b $dst, $con\\t# vector > 128 bits\" %}\n+  ins_encode %{\n+    assert(UseSVE > 0, \"must be sve\");\n+    __ sve_dup($dst$$FloatRegister, __ D, (int)($con$$constant));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ Vector insert --------------------------------\n+\n+\/\/ BYTE, SHORT, INT\n+\n+instruct insertI_le128b(vReg dst, vReg src, iRegIorL2I val, immI idx) %{\n+  predicate(Matcher::vector_length_in_bytes(n) <= 16 &&\n+            (Matcher::vector_element_basic_type(n) == T_BYTE ||\n+             Matcher::vector_element_basic_type(n) == T_SHORT ||\n+             Matcher::vector_element_basic_type(n) == T_INT));\n+  match(Set dst (VectorInsert (Binary src val) idx));\n+  format %{ \"insertI_le128b $dst, $src, $val, $idx\\t# vector <= 128 bits\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this);\n+    if ($dst$$FloatRegister != $src$$FloatRegister) {\n+      __ orr($dst$$FloatRegister, length_in_bytes == 16 ? __ T16B : __ T8B,\n+             $src$$FloatRegister, $src$$FloatRegister);\n+    }\n+    __ mov($dst$$FloatRegister, __ elemType_to_regVariant(bt),\n+           (int)($idx$$constant), $val$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct insertI_index_lt32(vReg dst, vReg src, iRegIorL2I val, immI idx,\n+                            vReg tmp, pRegGov pgtmp, rFlagsReg cr) %{\n+  predicate(n->in(2)->get_int() < 32 &&\n+            Matcher::vector_length_in_bytes(n) > 16 &&\n+            (Matcher::vector_element_basic_type(n) == T_BYTE ||\n+             Matcher::vector_element_basic_type(n) == T_SHORT ||\n+             Matcher::vector_element_basic_type(n) == T_INT));\n+  match(Set dst (VectorInsert (Binary src val) idx));\n+  effect(TEMP tmp, TEMP pgtmp, KILL cr);\n+  format %{ \"insertI_index_lt32 $dst, $src, $val, $idx\\t# vector > 128 bits, index < 31. KILL $tmp, $pgtmp, cr\" %}\n+  ins_encode %{\n+    assert(UseSVE > 0, \"must be sve\");\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    __ sve_index($tmp$$FloatRegister, size, -16, 1);\n+    __ sve_cmp(Assembler::EQ, $pgtmp$$PRegister, size, ptrue,\n+               $tmp$$FloatRegister, (int)($idx$$constant) - 16);\n+    if ($dst$$FloatRegister != $src$$FloatRegister) {\n+      __ sve_orr($dst$$FloatRegister, $src$$FloatRegister, $src$$FloatRegister);\n+    }\n+    __ sve_cpy($dst$$FloatRegister, size, $pgtmp$$PRegister, $val$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct insertI_index_ge32(vReg dst, vReg src, iRegIorL2I val, immI idx, vReg tmp1,\n+                            vReg tmp2, pRegGov pgtmp, rFlagsReg cr) %{\n+  predicate(n->in(2)->get_int() >= 32 &&\n+            (Matcher::vector_element_basic_type(n) == T_BYTE ||\n+             Matcher::vector_element_basic_type(n) == T_SHORT ||\n+             Matcher::vector_element_basic_type(n) == T_INT));\n+  match(Set dst (VectorInsert (Binary src val) idx));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP pgtmp, KILL cr);\n+  format %{ \"insertI_index_ge32 $dst, $src, $val, $idx\\t# index >= 32. KILL $tmp1, $tmp2, $pgtmp, cr\" %}\n+  ins_encode %{\n+    assert(UseSVE > 0, \"must be sve\");\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    __ sve_index($tmp1$$FloatRegister, size, 0, 1);\n+    __ sve_dup($tmp2$$FloatRegister, size, (int)($idx$$constant));\n+    __ sve_cmp(Assembler::EQ, $pgtmp$$PRegister, size, ptrue,\n+               $tmp1$$FloatRegister, $tmp2$$FloatRegister);\n+    if ($dst$$FloatRegister != $src$$FloatRegister) {\n+      __ sve_orr($dst$$FloatRegister, $src$$FloatRegister, $src$$FloatRegister);\n+    }\n+    __ sve_cpy($dst$$FloatRegister, size, $pgtmp$$PRegister, $val$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ LONG\n+\n+instruct insertL_128b(vReg dst, vReg src, iRegL val, immI idx) %{\n+  predicate(Matcher::vector_length_in_bytes(n) == 16 &&\n+            Matcher::vector_element_basic_type(n) == T_LONG);\n+  match(Set dst (VectorInsert (Binary src val) idx));\n+  format %{ \"insertL_128b $dst, $src, $val, $idx\\t# 2L\" %}\n+  ins_encode %{\n+    if ($dst$$FloatRegister != $src$$FloatRegister) {\n+      __ orr($dst$$FloatRegister, __ T16B, $src$$FloatRegister, $src$$FloatRegister);\n+    }\n+    __ mov($dst$$FloatRegister, __ D, (int)($idx$$constant), $val$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct insertL_gt128b(vReg dst, vReg src, iRegL val, immI idx,\n+                        vReg tmp, pRegGov pgtmp, rFlagsReg cr) %{\n+  predicate(Matcher::vector_length_in_bytes(n) > 16 &&\n+            Matcher::vector_element_basic_type(n) == T_LONG);\n+  match(Set dst (VectorInsert (Binary src val) idx));\n+  effect(TEMP tmp, TEMP pgtmp, KILL cr);\n+  format %{ \"insertL_gt128b $dst, $src, $val, $idx\\t# vector > 128 bits. KILL $tmp, $pgtmp, cr\" %}\n+  ins_encode %{\n+    assert(UseSVE > 0, \"must be sve\");\n+    __ sve_index($tmp$$FloatRegister, __ D, -16, 1);\n+    __ sve_cmp(Assembler::EQ, $pgtmp$$PRegister, __ D, ptrue,\n+               $tmp$$FloatRegister, (int)($idx$$constant) - 16);\n+    if ($dst$$FloatRegister != $src$$FloatRegister) {\n+      __ sve_orr($dst$$FloatRegister, $src$$FloatRegister, $src$$FloatRegister);\n+    }\n+    __ sve_cpy($dst$$FloatRegister, __ D, $pgtmp$$PRegister, $val$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ FLOAT\n+\n+instruct insertF_le128b(vReg dst, vReg src, vRegF val, immI idx) %{\n+  predicate(Matcher::vector_length_in_bytes(n) <= 16 &&\n+            Matcher::vector_element_basic_type(n) == T_FLOAT);\n+  match(Set dst (VectorInsert (Binary src val) idx));\n+  effect(TEMP_DEF dst);\n+  format %{ \"insertF_le128b $dst, $src, $val, $idx\\t# vector <= 128 bits\" %}\n+  ins_encode %{\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this);\n+    if ($dst$$FloatRegister != $src$$FloatRegister) {\n+      __ orr($dst$$FloatRegister, length_in_bytes == 16 ? __ T16B : __ T8B,\n+             $src$$FloatRegister, $src$$FloatRegister);\n+    }\n+    __ ins($dst$$FloatRegister, __ S, $val$$FloatRegister, (int)($idx$$constant), 0);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct insertF_index_lt32(vReg dst, vReg src, vRegF val, immI idx,\n+                            pRegGov pgtmp, rFlagsReg cr) %{\n+  predicate(n->in(2)->get_int() < 32 &&\n+            Matcher::vector_length_in_bytes(n) > 16 &&\n+            Matcher::vector_element_basic_type(n) == T_FLOAT);\n+  match(Set dst (VectorInsert (Binary src val) idx));\n+  effect(TEMP_DEF dst, TEMP pgtmp, KILL cr);\n+  format %{ \"insertF_index_lt32 $dst, $src, $val, $idx\\t# vector > 128 bits, index < 32. KILL $pgtmp, cr\" %}\n+  ins_encode %{\n+    assert(UseSVE > 0, \"must be sve\");\n+    __ sve_index($dst$$FloatRegister, __ S, -16, 1);\n+    __ sve_cmp(Assembler::EQ, $pgtmp$$PRegister, __ S, ptrue,\n+               $dst$$FloatRegister, (int)($idx$$constant) - 16);\n+    __ sve_orr($dst$$FloatRegister, $src$$FloatRegister, $src$$FloatRegister);\n+    __ sve_cpy($dst$$FloatRegister, __ S, $pgtmp$$PRegister, $val$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct insertF_index_ge32(vReg dst, vReg src, vRegF val, immI idx, vReg tmp,\n+                            pRegGov pgtmp, rFlagsReg cr) %{\n+  predicate(n->in(2)->get_int() >= 32 &&\n+            Matcher::vector_element_basic_type(n) == T_FLOAT);\n+  match(Set dst (VectorInsert (Binary src val) idx));\n+  effect(TEMP_DEF dst, TEMP tmp, TEMP pgtmp, KILL cr);\n+  format %{ \"insertF_index_ge32 $dst, $src, $val, $idx\\t# index >= 32. KILL $tmp, $pgtmp, cr\" %}\n+  ins_encode %{\n+    assert(UseSVE > 0, \"must be sve\");\n+    __ sve_index($tmp$$FloatRegister, __ S, 0, 1);\n+    __ sve_dup($dst$$FloatRegister, __ S, (int)($idx$$constant));\n+    __ sve_cmp(Assembler::EQ, $pgtmp$$PRegister, __ S, ptrue,\n+               $tmp$$FloatRegister, $dst$$FloatRegister);\n+    __ sve_orr($dst$$FloatRegister, $src$$FloatRegister, $src$$FloatRegister);\n+    __ sve_cpy($dst$$FloatRegister, __ S, $pgtmp$$PRegister, $val$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ DOUBLE\n+\n+instruct insertD_128b(vReg dst, vReg src, vRegD val, immI idx) %{\n+  predicate(Matcher::vector_length_in_bytes(n) == 16 &&\n+            Matcher::vector_element_basic_type(n) == T_DOUBLE);\n+  match(Set dst (VectorInsert (Binary src val) idx));\n+  effect(TEMP_DEF dst);\n+  format %{ \"insertD_128b $dst, $src, $val, $idx\\t# 2D\" %}\n+  ins_encode %{\n+    if ($dst$$FloatRegister != $src$$FloatRegister) {\n+      __ orr($dst$$FloatRegister, __ T16B, $src$$FloatRegister, $src$$FloatRegister);\n+    }\n+    __ ins($dst$$FloatRegister, __ D, $val$$FloatRegister, (int)($idx$$constant), 0);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct insertD_gt128b(vReg dst, vReg src, vRegD val, immI idx,\n+                        pRegGov pgtmp, rFlagsReg cr) %{\n+  predicate(Matcher::vector_length_in_bytes(n) > 16 &&\n+            Matcher::vector_element_basic_type(n) == T_DOUBLE);\n+  match(Set dst (VectorInsert (Binary src val) idx));\n+  effect(TEMP_DEF dst, TEMP pgtmp, KILL cr);\n+  format %{ \"insertD_gt128b $dst, $src, $val, $idx\\t# vector > 128 bits. KILL $pgtmp, cr\" %}\n+  ins_encode %{\n+    assert(UseSVE > 0, \"must be sve\");\n+    __ sve_index($dst$$FloatRegister, __ D, -16, 1);\n+    __ sve_cmp(Assembler::EQ, $pgtmp$$PRegister, __ D, ptrue,\n+               $dst$$FloatRegister, (int)($idx$$constant) - 16);\n+    __ sve_orr($dst$$FloatRegister, $src$$FloatRegister, $src$$FloatRegister);\n+    __ sve_cpy($dst$$FloatRegister, __ D, $pgtmp$$PRegister, $val$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ Extract --------------------------------------\n+\n+\/\/ BYTE, SHORT, INT\n+\n+instruct extractI_small_index(iRegINoSp dst, vReg src, immI idx) %{\n+  predicate((Matcher::vector_element_basic_type(n->in(1)) == T_BYTE &&\n+             n->in(2)->get_int() < 16) ||\n+            (Matcher::vector_element_basic_type(n->in(1)) == T_SHORT &&\n+             n->in(2)->get_int() < 8) ||\n+            (Matcher::vector_element_basic_type(n->in(1)) == T_INT &&\n+             n->in(2)->get_int() < 4));\n+  match(Set dst (ExtractB src idx));\n+  match(Set dst (ExtractS src idx));\n+  match(Set dst (ExtractI src idx));\n+  format %{ \"extractI_small_index $dst, $src, $idx\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n+    if (bt == T_INT) {\n+      __ umov($dst$$Register, $src$$FloatRegister, __ S, (int)($idx$$constant));\n+    } else {\n+      __ smov($dst$$Register, $src$$FloatRegister, __ elemType_to_regVariant(bt),\n+              (int)($idx$$constant));\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct extractI_large_index(iRegINoSp dst, vReg src, immI idx, vReg tmp) %{\n+  predicate((Matcher::vector_element_basic_type(n->in(1)) == T_BYTE &&\n+             n->in(2)->get_int() >= 16) ||\n+            (Matcher::vector_element_basic_type(n->in(1)) == T_SHORT &&\n+             n->in(2)->get_int() >= 8) ||\n+            (Matcher::vector_element_basic_type(n->in(1)) == T_INT &&\n+             n->in(2)->get_int() >= 4));\n+  match(Set dst (ExtractB src idx));\n+  match(Set dst (ExtractS src idx));\n+  match(Set dst (ExtractI src idx));\n+  effect(TEMP tmp);\n+  format %{ \"extractI_large_index $dst, $src, $idx\\t# KILL $tmp\" %}\n+  ins_encode %{\n+    assert(UseSVE > 0, \"must be sve\");\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n+    __ sve_extract_integral($dst$$Register, bt, $src$$FloatRegister,\n+                            (int)($idx$$constant), $tmp$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ LONG\n+\n+instruct extractL_index_lt2(iRegLNoSp dst, vReg src, immI idx) %{\n+  predicate(n->in(2)->get_int() < 2);\n+  match(Set dst (ExtractL src idx));\n+  format %{ \"extractL_index_lt2 $dst, $src, $idx\\t# index < 2\" %}\n+  ins_encode %{\n+    __ umov($dst$$Register, $src$$FloatRegister, __ D, (int)($idx$$constant));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct extractL_index_ge2(iRegLNoSp dst, vReg src, immI idx, vReg tmp) %{\n+  predicate(n->in(2)->get_int() >= 2);\n+  match(Set dst (ExtractL src idx));\n+  effect(TEMP tmp);\n+  format %{ \"extractL_index_ge2 $dst, $src, $idx\\t# index >=2. KILL $tmp\" %}\n+  ins_encode %{\n+    assert(UseSVE > 0, \"must be sve\");\n+    __ sve_extract_integral($dst$$Register, T_LONG, $src$$FloatRegister,\n+                            (int)($idx$$constant), $tmp$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ FLOAT\n+\n+instruct extractF(vRegF dst, vReg src, immI idx) %{\n+  match(Set dst (ExtractF src idx));\n+  effect(TEMP_DEF dst);\n+  format %{ \"extractF $dst, $src, $idx\" %}\n+  ins_encode %{\n+    int index = (int)$idx$$constant;\n+    if (index == 0) {\n+      __ fmovs($dst$$FloatRegister, $src$$FloatRegister);\n+    } else if (index < 4) {\n+      __ ins($dst$$FloatRegister, __ S, $src$$FloatRegister, 0, index);\n+    } else {\n+      assert(UseSVE > 0, \"must be sve\");\n+      __ sve_orr($dst$$FloatRegister, $src$$FloatRegister, $src$$FloatRegister);\n+      __ sve_ext($dst$$FloatRegister, $dst$$FloatRegister, index << 2);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ DOUBLE\n+\n+instruct extractD(vRegD dst, vReg src, immI idx) %{\n+  match(Set dst (ExtractD src idx));\n+  effect(TEMP_DEF dst);\n+  format %{ \"extractD $dst, $src, $idx\" %}\n+  ins_encode %{\n+    int index = (int)$idx$$constant;\n+    if (index == 0) {\n+      __ fmovd($dst$$FloatRegister, $src$$FloatRegister);\n+    } else if (index == 1) {\n+      __ ins($dst$$FloatRegister, __ D, $src$$FloatRegister, 0, index);\n+    } else {\n+      assert(UseSVE > 0, \"must be sve\");\n+      __ sve_orr($dst$$FloatRegister, $src$$FloatRegister, $src$$FloatRegister);\n+      __ sve_ext($dst$$FloatRegister, $dst$$FloatRegister, index << 3);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ Vector mask loat\/store -----------------------\n+\n+\/\/ vector load mask\n+\n+instruct vloadmask_neon(vReg dst, vReg src) %{\n+  predicate(UseSVE == 0 &&\n+            (Matcher::vector_length_in_bytes(n) == 8 ||\n+             Matcher::vector_length_in_bytes(n) == 16));\n+  match(Set dst (VectorLoadMask src ));\n+  format %{ \"vloadmask_neon $dst, $src\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    if (bt == T_BYTE) {\n+      uint length_in_bytes = Matcher::vector_length_in_bytes(this);\n+      __ negr($dst$$FloatRegister, length_in_bytes == 16 ? __ T16B : __ T8B,\n+              $src$$FloatRegister);\n+    } else {\n+      __ uxtl($dst$$FloatRegister, __ T8H, $src$$FloatRegister, __ T8B);\n+      if (type2aelembytes(bt) >= 4) {\n+        __ uxtl($dst$$FloatRegister, __ T4S, $dst$$FloatRegister, __ T4H);\n+      }\n+      if (type2aelembytes(bt) == 8) {\n+        __ uxtl($dst$$FloatRegister, __ T2D, $dst$$FloatRegister, __ T2S);\n+      }\n+      __ negr($dst$$FloatRegister, get_arrangement(this), $dst$$FloatRegister);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ The following two rules set predicate registers. They can guarantee the high\n+\/\/ bits of dst are cleared with zero when the vector length is less than the full\n+\/\/ size of hardware vector register width.\n+\n+instruct vloadmaskB_sve(pRegGov dst, vReg src, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 && Matcher::vector_element_basic_type(n) == T_BYTE);\n+  match(Set dst (VectorLoadMask src));\n+  effect(KILL cr);\n+  format %{ \"vloadmaskB_sve $dst, $src\\t# KILL cr\" %}\n+  ins_encode %{\n+    __ sve_cmp(Assembler::NE, $dst$$PRegister, __ B,\n+               ptrue, $src$$FloatRegister, 0);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vloadmask_extend_sve(pRegGov dst, vReg src, vReg tmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 && Matcher::vector_element_basic_type(n) != T_BYTE);\n+  match(Set dst (VectorLoadMask src));\n+  effect(TEMP tmp, KILL cr);\n+  format %{ \"vloadmask_extend_sve $dst, $src\\t# KILL $tmp, cr\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    __ sve_vector_extend($tmp$$FloatRegister, size, $src$$FloatRegister, __ B);\n+    __ sve_cmp(Assembler::NE, $dst$$PRegister, size, ptrue, $tmp$$FloatRegister, 0);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector store mask - neon\n+\n+instruct vstoremaskB_neon(vReg dst, vReg src, immI_1 size) %{\n+  predicate(UseSVE == 0);\n+  match(Set dst (VectorStoreMask src size));\n+  format %{ \"vstoremaskB_neon $dst, $src\" %}\n+  ins_encode %{\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this);\n+    assert(length_in_bytes == 8 || length_in_bytes == 16, \"must be\");\n+    __ negr($dst$$FloatRegister, length_in_bytes == 16 ? __ T16B : __ T8B,\n+            $src$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct vstoremask_narrow_neon(vReg dst, vReg src, immI_gt_1 size) %{\n+  predicate(UseSVE == 0);\n+  match(Set dst (VectorStoreMask src size));\n+  format %{ \"vstoremask_narrow_neon $dst, $src\" %}\n+  ins_encode %{\n+    int esize = (int)$size$$constant;\n+    if (esize == 2) {\n+      __ xtn($dst$$FloatRegister, __ T8B, $src$$FloatRegister, __ T8H);\n+    } else if (esize == 4) {\n+      __ xtn($dst$$FloatRegister, __ T4H, $src$$FloatRegister, __ T4S);\n+      __ xtn($dst$$FloatRegister, __ T8B, $dst$$FloatRegister, __ T8H);\n+    } else {\n+      assert(esize == 8, \"must be\");\n+      __ xtn($dst$$FloatRegister, __ T2S, $src$$FloatRegister, __ T2D);\n+      __ xtn($dst$$FloatRegister, __ T4H, $dst$$FloatRegister, __ T4S);\n+      __ xtn($dst$$FloatRegister, __ T8B, $dst$$FloatRegister, __ T8H);\n+    }\n+    __ negr($dst$$FloatRegister, __ T8B, $dst$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector store mask - sve\n+\n+instruct vstoremaskB_sve(vReg dst, pRegGov src, immI_1 size) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst (VectorStoreMask src size));\n+  format %{ \"vstoremaskB_sve $dst, $src\" %}\n+  ins_encode %{\n+    __ sve_cpy($dst$$FloatRegister, __ B, $src$$PRegister, 1, false);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vstoremask_narrow_sve(vReg dst, pRegGov src, immI_gt_1 size, vReg tmp) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst (VectorStoreMask src size));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"vstoremask_narrow_sve $dst, $src\\t# KILL $tmp\" %}\n+  ins_encode %{\n+    Assembler::SIMD_RegVariant size = __ elemBytes_to_regVariant((int)$size$$constant);\n+    __ sve_cpy($dst$$FloatRegister, size, $src$$PRegister, 1, false);\n+    __ sve_vector_narrow($dst$$FloatRegister, __ B,\n+                         $dst$$FloatRegister, size, $tmp$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Combine LoadVector+VectorLoadMask when the vector element type is not T_BYTE\n+\n+instruct vloadmask_loadV(pRegGov dst, indirect mem, vReg tmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            Matcher::vector_length_in_bytes(n) == MaxVectorSize &&\n+            type2aelembytes(Matcher::vector_element_basic_type(n)) > 1);\n+  match(Set dst (VectorLoadMask (LoadVector mem)));\n+  effect(TEMP tmp, KILL cr);\n+  format %{ \"vloadmask_loadV $dst, $mem\\t# KILL $tmp, cr\" %}\n+  ins_encode %{\n+    \/\/ Load mask values which are boolean type, and extend them to the\n+    \/\/ expected vector element type. Convert the vector to predicate.\n+    BasicType to_vect_bt = Matcher::vector_element_basic_type(this);\n+    loadStoreA_predicated(C2_MacroAssembler(&cbuf), false, $tmp$$FloatRegister,\n+                          ptrue, T_BOOLEAN, to_vect_bt, $mem->opcode(),\n+                          as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+    __ sve_cmp(Assembler::NE, $dst$$PRegister, __ elemType_to_regVariant(to_vect_bt),\n+               ptrue, $tmp$$FloatRegister, 0);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vloadmask_loadV_partial(pRegGov dst, indirect mem, vReg tmp,\n+                                 pRegGov pgtmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            Matcher::vector_length_in_bytes(n) > 16 &&\n+            Matcher::vector_length_in_bytes(n) < MaxVectorSize &&\n+            type2aelembytes(Matcher::vector_element_basic_type(n)) > 1);\n+  match(Set dst (VectorLoadMask (LoadVector mem)));\n+  effect(TEMP tmp, TEMP pgtmp, KILL cr);\n+  format %{ \"vloadmask_loadV_partial $dst, $mem\\t# KILL $tmp, $pgtmp, cr\" %}\n+  ins_encode %{\n+    \/\/ Load valid mask values which are boolean type, and extend them to the\n+    \/\/ expected vector element type. Convert the vector to predicate.\n+    BasicType to_vect_bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(to_vect_bt);\n+    __ sve_ptrue_lanecnt($pgtmp$$PRegister, size, Matcher::vector_length(this));\n+    loadStoreA_predicated(C2_MacroAssembler(&cbuf), false, $tmp$$FloatRegister,\n+                          $pgtmp$$PRegister, T_BOOLEAN, to_vect_bt, $mem->opcode(),\n+                          as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+    __ sve_cmp(Assembler::NE, $dst$$PRegister, size, ptrue, $tmp$$FloatRegister, 0);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Combine VectorStoreMask+StoreVector when the vector element type is not T_BYTE\n+\n+instruct storeV_vstoremask(indirect mem, pRegGov src, immI_gt_1 esize, vReg tmp) %{\n+  predicate(UseSVE > 0 &&\n+            Matcher::vector_length_in_bytes(n->as_StoreVector()->in(MemNode::ValueIn)->in(1)) == MaxVectorSize);\n+  match(Set mem (StoreVector mem (VectorStoreMask src esize)));\n+  effect(TEMP tmp);\n+  format %{ \"storeV_vstoremask $mem, $src\\t# KILL $tmp\" %}\n+  ins_encode %{\n+    BasicType from_vect_bt = Matcher::vector_element_basic_type(this, $src);\n+    assert(type2aelembytes(from_vect_bt) == (int)$esize$$constant, \"unsupported type\");\n+    Assembler::SIMD_RegVariant size = __ elemBytes_to_regVariant($esize$$constant);\n+    __ sve_cpy($tmp$$FloatRegister, size, $src$$PRegister, 1, false);\n+    loadStoreA_predicated(C2_MacroAssembler(&cbuf), true, $tmp$$FloatRegister,\n+                          ptrue, T_BOOLEAN, from_vect_bt, $mem->opcode(),\n+                          as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct storeV_vstoremask_partial(indirect mem, pRegGov src, immI_gt_1 esize,\n+                                   vReg tmp, pRegGov pgtmp) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_StoreVector()->memory_size() > 16 &&\n+            type2aelembytes(n->as_StoreVector()->vect_type()->element_basic_type()) > 1 &&\n+            Matcher::vector_length_in_bytes(n->as_StoreVector()->in(MemNode::ValueIn)->in(1)) < MaxVectorSize);\n+  match(Set mem (StoreVector mem (VectorStoreMask src esize)));\n+  effect(TEMP tmp, TEMP pgtmp);\n+  format %{ \"storeV_vstoremask_partial $mem, $src\\t# KILL $tmp, $pgtmp\" %}\n+  ins_encode %{\n+    \/\/ Convert the valid src predicate to vector, and store the vector\n+    \/\/ elements as boolean values.\n+    BasicType from_vect_bt = Matcher::vector_element_basic_type(this, $src);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(from_vect_bt);\n+    __ sve_cpy($tmp$$FloatRegister, size, $src$$PRegister, 1, false);\n+    __ sve_ptrue_lanecnt($pgtmp$$PRegister, size, Matcher::vector_length(this, $src));\n+    loadStoreA_predicated(C2_MacroAssembler(&cbuf), true, $tmp$$FloatRegister,\n+                          $pgtmp$$PRegister, T_BOOLEAN, from_vect_bt, $mem->opcode(),\n+                          as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ Vector mask basic OPs ------------------------\n+\n+\/\/ vector mask logical ops: and\/or\/xor\/and_not\n+\n+instruct vmask_and(pRegGov pd, pRegGov pn, pRegGov pm) %{\n+  predicate(UseSVE > 0);\n+  match(Set pd (AndVMask pn pm));\n+  format %{ \"vmask_and $pd, $pn, $pm\" %}\n+  ins_encode %{\n+    __ sve_and($pd$$PRegister, ptrue, $pn$$PRegister, $pm$$PRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmask_or(pRegGov pd, pRegGov pn, pRegGov pm) %{\n+  predicate(UseSVE > 0);\n+  match(Set pd (OrVMask pn pm));\n+  format %{ \"vmask_or $pd, $pn, $pm\" %}\n+  ins_encode %{\n+    __ sve_orr($pd$$PRegister, ptrue, $pn$$PRegister, $pm$$PRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmask_xor(pRegGov pd, pRegGov pn, pRegGov pm) %{\n+  predicate(UseSVE > 0);\n+  match(Set pd (XorVMask pn pm));\n+  format %{ \"vmask_xor $pd, $pn, $pm\" %}\n+  ins_encode %{\n+    __ sve_eor($pd$$PRegister, ptrue, $pn$$PRegister, $pm$$PRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmask_and_notI(pRegGov pd, pRegGov pn, pRegGov pm, immI_M1 m1) %{\n+  predicate(UseSVE > 0);\n+  match(Set pd (AndVMask pn (XorVMask pm (MaskAll m1))));\n+  format %{ \"vmask_and_notI $pd, $pn, $pm\" %}\n+  ins_encode %{\n+    __ sve_bic($pd$$PRegister, ptrue, $pn$$PRegister, $pm$$PRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmask_and_notL(pRegGov pd, pRegGov pn, pRegGov pm, immL_M1 m1) %{\n+  predicate(UseSVE > 0);\n+  match(Set pd (AndVMask pn (XorVMask pm (MaskAll m1))));\n+  format %{ \"vmask_and_notL $pd, $pn, $pm\" %}\n+  ins_encode %{\n+    __ sve_bic($pd$$PRegister, ptrue, $pn$$PRegister, $pm$$PRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector mask compare\n+\n+instruct vmaskcmp_neon(vReg dst, vReg src1, vReg src2, immI cond) %{\n+  predicate(UseSVE == 0 &&\n+            (Matcher::vector_length_in_bytes(n) == 8 ||\n+             Matcher::vector_length_in_bytes(n) == 16));\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));\n+  format %{ \"vmaskcmp_neon $dst, $src1, $src2, $cond\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this);\n+    __ neon_compare($dst$$FloatRegister, bt, $src1$$FloatRegister,\n+                    $src2$$FloatRegister, (int)($cond$$constant),\n+                    \/* isQ *\/ length_in_bytes == 16);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ The following two rules set predicate registers. They can guarantee the high\n+\/\/ bits of dst are cleared with zero when the vector length is less than the full\n+\/\/ size of hardware vector register width.\n+\n+instruct vmaskcmp_sve(pRegGov dst, vReg src1, vReg src2, immI cond, rFlagsReg cr) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));\n+  effect(KILL cr);\n+  format %{ \"vmaskcmp_sve $dst, $src1, $src2, $cond\\t# KILL cr\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this);\n+    if (length_in_bytes == MaxVectorSize) {\n+      __ sve_compare($dst$$PRegister, bt, ptrue, $src1$$FloatRegister,\n+                     $src2$$FloatRegister, (int)($cond$$constant));\n+    } else {\n+      __ sve_ptrue_lanecnt($dst$$PRegister, __ elemType_to_regVariant(bt),\n+                           Matcher::vector_length(this));\n+      __ sve_compare($dst$$PRegister, bt, $dst$$PRegister, $src1$$FloatRegister,\n+                     $src2$$FloatRegister, (int)($cond$$constant));\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmaskcmp_masked(pRegGov dst, vReg src1, vReg src2, immI cond,\n+                         pRegGov pg, rFlagsReg cr) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) (Binary cond pg)));\n+  effect(KILL cr);\n+  format %{ \"vmaskcmp_masked $dst, $pg, $src1, $src2, $cond\\t# KILL cr\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ sve_compare($dst$$PRegister, bt, $pg$$PRegister, $src1$$FloatRegister,\n+                   $src2$$FloatRegister, (int)($cond$$constant));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector mask cast\n+\n+instruct vmaskcast_same_esize_neon(vReg dst_src) %{\n+  predicate(UseSVE == 0 &&\n+            Matcher::vector_length_in_bytes(n) == Matcher::vector_length_in_bytes(n->in(1)) &&\n+            (Matcher::vector_length_in_bytes(n) == 8 || Matcher::vector_length_in_bytes(n) == 16));\n+  match(Set dst_src (VectorMaskCast dst_src));\n+  ins_cost(0);\n+  format %{ \"vmaskcast_same_esize_neon $dst_src\\t# do nothing\" %}\n+  ins_encode(\/* empty encoding *\/);\n+  ins_pipe(pipe_class_empty);\n+%}\n+\n+instruct vmaskcast_same_esize_sve(pRegGov dst_src) %{\n+  predicate(UseSVE > 0 &&\n+            Matcher::vector_length_in_bytes(n) == Matcher::vector_length_in_bytes(n->in(1)));\n+  match(Set dst_src (VectorMaskCast dst_src));\n+  ins_cost(0);\n+  format %{ \"vmaskcast_same_esize_sve $dst_src\\t# do nothing\" %}\n+  ins_encode(\/* empty encoding *\/);\n+  ins_pipe(pipe_class_empty);\n+%}\n+\n+instruct vmaskcast_extend(pRegGov dst, pReg src) %{\n+  predicate(UseSVE > 0 &&\n+            Matcher::vector_length_in_bytes(n) > Matcher::vector_length_in_bytes(n->in(1)));\n+  match(Set dst (VectorMaskCast src));\n+  format %{ \"vmaskcast_extend $dst, $src\" %}\n+  ins_encode %{\n+    uint length_in_bytes_dst = Matcher::vector_length_in_bytes(this);\n+    uint length_in_bytes_src = Matcher::vector_length_in_bytes(this, $src);\n+    assert(length_in_bytes_dst == 2 * length_in_bytes_src ||\n+           length_in_bytes_dst == 4 * length_in_bytes_src ||\n+           length_in_bytes_dst == 8 * length_in_bytes_src, \"invalid vector length\");\n+    __ sve_vmaskcast_extend($dst$$PRegister, $src$$PRegister,\n+                            length_in_bytes_dst, length_in_bytes_src);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmaskcast_narrow(pRegGov dst, pReg src) %{\n+  predicate(UseSVE > 0 &&\n+            Matcher::vector_length_in_bytes(n) < Matcher::vector_length_in_bytes(n->in(1)));\n+  match(Set dst (VectorMaskCast src));\n+  format %{ \"vmaskcast_narrow $dst, $src\" %}\n+  ins_encode %{\n+    uint length_in_bytes_dst = Matcher::vector_length_in_bytes(this);\n+    uint length_in_bytes_src = Matcher::vector_length_in_bytes(this, $src);\n+    assert(length_in_bytes_dst * 2 == length_in_bytes_src ||\n+           length_in_bytes_dst * 4 == length_in_bytes_src ||\n+           length_in_bytes_dst * 8 == length_in_bytes_src, \"invalid vector length\");\n+    __ sve_vmaskcast_narrow($dst$$PRegister, $src$$PRegister,\n+                            length_in_bytes_dst, length_in_bytes_src);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector mask reinterpret\n+\n+instruct vmask_reinterpret_same_esize(pRegGov dst_src) %{\n+  predicate(UseSVE > 0 &&\n+            Matcher::vector_length(n) == Matcher::vector_length(n->in(1)) &&\n+            Matcher::vector_length_in_bytes(n) == Matcher::vector_length_in_bytes(n->in(1)));\n+  match(Set dst_src (VectorReinterpret dst_src));\n+  ins_cost(0);\n+  format %{ \"vmask_reinterpret_same_esize $dst_src\\t# do nothing\" %}\n+  ins_encode(\/* empty encoding *\/);\n+  ins_pipe(pipe_class_empty);\n+%}\n+\n+instruct vmask_reinterpret_diff_esize(pRegGov dst, pRegGov src, vReg tmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            Matcher::vector_length(n) != Matcher::vector_length(n->in(1)) &&\n+            Matcher::vector_length_in_bytes(n) == Matcher::vector_length_in_bytes(n->in(1)));\n+  match(Set dst (VectorReinterpret src));\n+  effect(TEMP tmp, KILL cr);\n+  format %{ \"vmask_reinterpret_diff_esize $dst, $src\\t# KILL $tmp, cr\" %}\n+  ins_encode %{\n+    BasicType from_bt = Matcher::vector_element_basic_type(this, $src);\n+    Assembler::SIMD_RegVariant from_size = __ elemType_to_regVariant(from_bt);\n+    BasicType to_bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant to_size = __ elemType_to_regVariant(to_bt);\n+    __ sve_cpy($tmp$$FloatRegister, from_size, $src$$PRegister, -1, false);\n+    __ sve_cmp(Assembler::EQ, $dst$$PRegister, to_size, ptrue, $tmp$$FloatRegister, -1);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ Vector mask reductions -----------------------\n+\n+\/\/ true count\n+\n+instruct vmask_truecount_neon(iRegINoSp dst, vReg src, vReg tmp) %{\n+  predicate(UseSVE == 0);\n+  match(Set dst (VectorMaskTrueCount src));\n+  effect(TEMP tmp);\n+  format %{ \"vmask_truecount_neon $dst, $src\\t# KILL $tmp\" %}\n+  ins_encode %{\n+    \/\/ Input \"src\" is a vector of boolean represented as bytes with\n+    \/\/ 0x00\/0x01 as element values.\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n+    assert(bt == T_BOOLEAN, \"unsupported type\");\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this, $src);\n+    __ addv($tmp$$FloatRegister, length_in_bytes == 16 ? __ T16B : __ T8B,\n+            $src$$FloatRegister);\n+    __ umov($dst$$Register, $tmp$$FloatRegister, __ B, 0);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmask_truecount_sve(iRegINoSp dst, pReg src) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst (VectorMaskTrueCount src));\n+  format %{ \"vmask_truecount_sve $dst, $src\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n+    __ sve_cntp($dst$$Register, __ elemType_to_regVariant(bt),\n+                ptrue, $src$$PRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ first true\n+\n+instruct vmask_firsttrue_lt8e(iRegINoSp dst, vReg src, rFlagsReg cr) %{\n+  predicate(UseSVE == 0 &&\n+            Matcher::vector_length(n->in(1)) < 8);\n+  match(Set dst (VectorMaskFirstTrue src));\n+  effect(KILL cr);\n+  format %{ \"vmask_firsttrue_lt8e $dst, $src\\t# vector < 8 elements (neon). KILL cr\" %}\n+  ins_encode %{\n+    \/\/ Returns the index of the first active lane of the\n+    \/\/ vector mask, or VLENGTH if no lane is active.\n+    \/\/\n+    \/\/ Input \"src\" is a vector of boolean represented as\n+    \/\/ bytes with 0x00\/0x01 as element values.\n+    \/\/\n+    \/\/ Computed by reversing the bits and counting the leading\n+    \/\/ zero bytes.\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n+    assert(bt == T_BOOLEAN, \"unsupported type\");\n+    __ fmovd($dst$$Register, $src$$FloatRegister);\n+    __ rbit($dst$$Register, $dst$$Register);\n+    __ clz($dst$$Register, $dst$$Register);\n+    __ lsrw($dst$$Register, $dst$$Register, 3);\n+    __ movw(rscratch1, Matcher::vector_length(this, $src));\n+    __ cmpw($dst$$Register, rscratch1);\n+    __ cselw($dst$$Register, rscratch1, $dst$$Register, Assembler::GE);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmask_firsttrue_8or16e(iRegINoSp dst, vReg src) %{\n+  predicate(UseSVE == 0 &&\n+            (Matcher::vector_length(n->in(1)) == 8 || Matcher::vector_length(n->in(1)) == 16));\n+  match(Set dst (VectorMaskFirstTrue src));\n+  format %{ \"vmask_firsttrue_8or16e $dst, $src\\t# vector 8B\/16B (neon)\" %}\n+  ins_encode %{\n+    \/\/ Returns the index of the first active lane of the\n+    \/\/ vector mask, or VLENGTH if no lane is active.\n+    \/\/\n+    \/\/ Input \"src\" is a vector of boolean represented as\n+    \/\/ bytes with 0x00\/0x01 as element values.\n+    \/\/\n+    \/\/ Computed by reversing the bits and counting the leading\n+    \/\/ zero bytes.\n+\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n+    assert(bt == T_BOOLEAN, \"unsupported type\");\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this, $src);\n+    if (length_in_bytes == 8) {\n+      __ fmovd($dst$$Register, $src$$FloatRegister);\n+      __ rbit($dst$$Register, $dst$$Register);\n+      __ clz($dst$$Register, $dst$$Register);\n+      __ lsrw($dst$$Register, $dst$$Register, 3);\n+    } else {\n+      assert(length_in_bytes == 16, \"must be\");\n+      Label FIRST_TRUE_INDEX;\n+\n+      \/\/ Try to compute the result from lower 64 bits.\n+      __ fmovd($dst$$Register, $src$$FloatRegister);\n+      __ movw(rscratch1, zr);\n+      __ cbnz($dst$$Register, FIRST_TRUE_INDEX);\n+\n+      \/\/ Compute the result from the higher 64 bits.\n+      __ fmovhid($dst$$Register, $src$$FloatRegister);\n+      __ movw(rscratch1, 8);\n+\n+      \/\/ Reverse the bits and count the leading zero bytes.\n+      __ bind(FIRST_TRUE_INDEX);\n+      __ rbit($dst$$Register, $dst$$Register);\n+      __ clz($dst$$Register, $dst$$Register);\n+      __ addw($dst$$Register, rscratch1, $dst$$Register, Assembler::LSR, 3);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Return the index of the first mask lane that is set, or vector length if none of\n+\/\/ them are set.\n+\n+instruct vmask_firsttrue_sve(iRegINoSp dst, pReg src, pReg ptmp) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst (VectorMaskFirstTrue src));\n+  effect(TEMP ptmp);\n+  format %{ \"vmask_firsttrue_sve $dst, $src\\t# KILL $ptmp\" %}\n+  ins_encode %{\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this, $src);\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    \/\/ When the input predicate is all-false, the result should be the vector length\n+    \/\/ instead of max vector register size.\n+    if (length_in_bytes == MaxVectorSize) {\n+      __ sve_brkb($ptmp$$PRegister, ptrue, $src$$PRegister, false);\n+    } else {\n+      __ sve_ptrue_lanecnt($ptmp$$PRegister, size, Matcher::vector_length(this, $src));\n+      __ sve_brkb($ptmp$$PRegister, $ptmp$$PRegister, $src$$PRegister, false);\n+    }\n+    __ sve_cntp($dst$$Register, size, ptrue, $ptmp$$PRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ last true\n+\n+instruct vmask_lasttrue_neon(iRegINoSp dst, vReg src) %{\n+  predicate(UseSVE == 0);\n+  match(Set dst (VectorMaskLastTrue src));\n+  format %{ \"vmask_lasttrue_neon $dst, $src\" %}\n+  ins_encode %{\n+    \/\/ Returns the index of the last active lane of the\n+    \/\/ vector mask, or -1 if no lane is active.\n+    \/\/\n+    \/\/ Input \"src\" is a vector of boolean represented as\n+    \/\/ bytes with 0x00\/0x01 as element values.\n+\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n+    assert(bt == T_BOOLEAN, \"unsupported type\");\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this, $src);\n+    if (length_in_bytes <= 8) {\n+      \/\/ Computed by counting the leading zero bytes and\n+      \/\/ subtracting it by 7 (VLENGTH - 1).\n+      __ fmovd($dst$$Register, $src$$FloatRegister);\n+      __ clz($dst$$Register, $dst$$Register);\n+      __ movw(rscratch1, 7);\n+      __ subw($dst$$Register, rscratch1, $dst$$Register, Assembler::LSR, 3);\n+    } else {\n+      assert(length_in_bytes == 16, \"must be\");\n+      Label LAST_TRUE_INDEX;\n+\n+      \/\/ Try to compute the result from higher 64 bits.\n+      __ fmovhid($dst$$Register, $src$$FloatRegister);\n+      __ movw(rscratch1, 16 - 1);\n+      __ cbnz($dst$$Register, LAST_TRUE_INDEX);\n+\n+      \/\/ Compute the result from the lower 64 bits.\n+      __ fmovd($dst$$Register, $src$$FloatRegister);\n+      __ movw(rscratch1, 8 - 1);\n+\n+      \/\/ Count the leading zero bytes and subtract it by 15 (VLENGTH - 1).\n+      __ bind(LAST_TRUE_INDEX);\n+      __ clz($dst$$Register, $dst$$Register);\n+      __ subw($dst$$Register, rscratch1, $dst$$Register, Assembler::LSR, 3);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmask_lasttrue_sve(iRegINoSp dst, pReg src, pReg ptmp) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst (VectorMaskLastTrue src));\n+  effect(TEMP ptmp);\n+  format %{ \"vmask_lasttrue_sve $dst, $src\\t# KILL $ptmp\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n+    __ sve_vmask_lasttrue($dst$$Register, bt, $src$$PRegister, $ptmp$$PRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ tolong\n+\n+instruct vmask_tolong_neon(iRegLNoSp dst, vReg src) %{\n+  predicate(UseSVE == 0);\n+  match(Set dst (VectorMaskToLong src));\n+  format %{ \"vmask_tolong_neon $dst, $src\" %}\n+  ins_encode %{\n+    \/\/ Input \"src\" is a vector of boolean represented as\n+    \/\/ bytes with 0x00\/0x01 as element values.\n+\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this, $src);\n+    if (length_in_bytes <= 8) {\n+      __ fmovd($dst$$Register, $src$$FloatRegister);\n+      __ bytemask_compress($dst$$Register);\n+    } else {\n+      assert(length_in_bytes == 16, \"must be\");\n+      __ umov($dst$$Register, $src$$FloatRegister, __ D, 0);\n+      __ umov(rscratch1, $src$$FloatRegister, __ D, 1);\n+      __ bytemask_compress($dst$$Register);\n+      __ bytemask_compress(rscratch1);\n+      __ orr($dst$$Register, $dst$$Register, rscratch1, Assembler::LSL, 8);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmask_tolong_sve(iRegLNoSp dst, pReg src, vReg tmp1, vReg tmp2) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst (VectorMaskToLong src));\n+  effect(TEMP tmp1, TEMP tmp2);\n+  format %{ \"vmask_tolong_sve $dst, $src\\t# KILL $tmp1, $tmp2\" %}\n+  ins_encode %{\n+    __ sve_vmask_tolong($dst$$Register, $src$$PRegister,\n+                        Matcher::vector_element_basic_type(this, $src),\n+                        Matcher::vector_length(this, $src),\n+                        $tmp1$$FloatRegister, $tmp2$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ fromlong\n+\n+instruct vmask_fromlong(pRegGov dst, iRegL src, vReg tmp1, vReg tmp2) %{\n+  match(Set dst (VectorLongToMask src));\n+  effect(TEMP tmp1, TEMP tmp2);\n+  format %{ \"vmask_fromlong $dst, $src\\t# vector (sve2). KILL $tmp1, $tmp2\" %}\n+  ins_encode %{\n+    __ sve_vmask_fromlong($dst$$PRegister, $src$$Register,\n+                          Matcher::vector_element_basic_type(this),\n+                          Matcher::vector_length(this),\n+                          $tmp1$$FloatRegister, $tmp2$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ Vector mask generation -----------------------\n+\n+\/\/ The rules below set predicate registers. They can guarantee the high bits of dst\n+\/\/ are cleared with zero when the vector length is less than the full size of\n+\/\/ hardware vector register width.\n+\n+\/\/ maskAll (full or partial predicate size)\n+\n+instruct vmaskAll_immI(pRegGov dst, immI src) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst (MaskAll src));\n+  format %{ \"vmaskAll_immI $dst, $src\" %}\n+  ins_encode %{\n+    int con = (int)$src$$constant;\n+    if (con == 0) {\n+      __ sve_pfalse($dst$$PRegister);\n+    } else {\n+      assert(con == -1, \"invalid constant value for mask\");\n+      BasicType bt = Matcher::vector_element_basic_type(this);\n+      __ sve_ptrue_lanecnt($dst$$PRegister, __ elemType_to_regVariant(bt),\n+                           Matcher::vector_length(this));\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmaskAllI(pRegGov dst, iRegIorL2I src, vReg tmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst (MaskAll src));\n+  effect(TEMP tmp, KILL cr);\n+  format %{ \"vmaskAllI $dst, $src\\t# KILL $tmp, cr\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this);\n+    __ sve_dup($tmp$$FloatRegister, size, $src$$Register);\n+    if (length_in_bytes < MaxVectorSize) {\n+      __ sve_ptrue_lanecnt($dst$$PRegister, size, Matcher::vector_length(this));\n+      __ sve_cmp(Assembler::NE, $dst$$PRegister, size,\n+                 $dst$$PRegister, $tmp$$FloatRegister, 0);\n+    } else {\n+      __ sve_cmp(Assembler::NE, $dst$$PRegister, size, ptrue, $tmp$$FloatRegister, 0);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmaskAll_immL(pRegGov dst, immL src) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst (MaskAll src));\n+  format %{ \"vmaskAll_immL $dst, $src\" %}\n+  ins_encode %{\n+    long con = (long)$src$$constant;\n+    if (con == 0) {\n+      __ sve_pfalse($dst$$PRegister);\n+    } else {\n+      assert(con == -1, \"invalid constant value for mask\");\n+      BasicType bt = Matcher::vector_element_basic_type(this);\n+      __ sve_ptrue_lanecnt($dst$$PRegister, __ elemType_to_regVariant(bt),\n+                           Matcher::vector_length(this));\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmaskAllL(pRegGov dst, iRegL src, vReg tmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst (MaskAll src));\n+  effect(TEMP tmp, KILL cr);\n+  format %{ \"vmaskAllL $dst, $src\\t# KILL $tmp, cr\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this);\n+    __ sve_dup($tmp$$FloatRegister, size, $src$$Register);\n+    if (length_in_bytes < MaxVectorSize) {\n+      __ sve_ptrue_lanecnt($dst$$PRegister, size, Matcher::vector_length(this));\n+      __ sve_cmp(Assembler::NE, $dst$$PRegister, size,\n+                 $dst$$PRegister, $tmp$$FloatRegister, 0);\n+    } else {\n+      __ sve_cmp(Assembler::NE, $dst$$PRegister, size, ptrue, $tmp$$FloatRegister, 0);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vetcor mask generation\n+\n+instruct vmask_gen(pRegGov pg, iRegL len, rFlagsReg cr) %{\n+  predicate(UseSVE > 0);\n+  match(Set pg (VectorMaskGen len));\n+  effect(KILL cr);\n+  format %{ \"vmask_gen $pg, $len\\t# KILL cr\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ sve_whilelo($pg$$PRegister, __ elemType_to_regVariant(bt), zr, $len$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ Popcount vector ------------------------------\n+\n+\/\/ vector popcount - INT\n+\n+instruct vpopcountI(vReg dst, vReg src) %{\n+  predicate(!n->as_Vector()->is_predicated_vector());\n+  match(Set dst (PopCountVI src));\n+  format %{ \"vpopcountI $dst, $src\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this);\n+    if (bt == T_BYTE && length_in_bytes <= 16) {\n+      __ cnt($dst$$FloatRegister, length_in_bytes == 16 ? __ T16B : __ T8B,\n+             $src$$FloatRegister);\n+    } else if (UseSVE == 0) {\n+      assert(bt == T_SHORT || bt == T_INT, \"unsupported\");\n+      assert(length_in_bytes == 8 || length_in_bytes == 16, \"unsupported\");\n+      __ cnt($dst$$FloatRegister, length_in_bytes == 16 ? __ T16B : __ T8B,\n+             $src$$FloatRegister);\n+      __ uaddlp($dst$$FloatRegister, length_in_bytes == 16 ? __ T16B : __ T8B,\n+                $dst$$FloatRegister);\n+      if (bt == T_INT) {\n+        __ uaddlp($dst$$FloatRegister, length_in_bytes == 16 ? __ T8H : __ T4H,\n+                  $dst$$FloatRegister);\n+      }\n+    } else {\n+      assert(UseSVE > 0, \"must be sve\");\n+      __ sve_cnt($dst$$FloatRegister, __ elemType_to_regVariant(bt),\n+                 ptrue, $src$$FloatRegister);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector popcount - LONG\n+\n+instruct vpopcountL(vReg dst, vReg src) %{\n+  predicate(Matcher::vector_element_basic_type(n) == T_LONG &&\n+            !n->as_Vector()->is_predicated_vector());\n+  match(Set dst (PopCountVL src));\n+  format %{ \"vpopcountL $dst, $src\" %}\n+  ins_encode %{\n+    if (UseSVE == 0) {\n+      __ cnt($dst$$FloatRegister, __ T16B, $src$$FloatRegister);\n+      __ uaddlp($dst$$FloatRegister, __ T16B, $dst$$FloatRegister);\n+      __ uaddlp($dst$$FloatRegister, __ T8H, $dst$$FloatRegister);\n+      __ uaddlp($dst$$FloatRegister, __ T4S, $dst$$FloatRegister);\n+    } else {\n+      __ sve_cnt($dst$$FloatRegister, __ D, ptrue, $src$$FloatRegister);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ If the PopCountVL is generated by auto-vectorization, the dst basic\n+\/\/ type is T_INT. And once we have unified the type definition for\n+\/\/ Vector API and auto-vectorization, this rule can be merged with\n+\/\/ \"vpopcountL\" rule.\n+\n+instruct vpopcountL_I(vReg dst, vReg src, vReg tmp) %{\n+  predicate(Matcher::vector_element_basic_type(n) == T_INT &&\n+            !n->as_Vector()->is_predicated_vector());\n+  match(Set dst (PopCountVL src));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"vpopcountL_I $dst, $src\\t# KILL $tmp\" %}\n+  ins_encode %{\n+    if (UseSVE == 0) {\n+      __ cnt($dst$$FloatRegister, __ T16B, $src$$FloatRegister);\n+      __ uaddlp($dst$$FloatRegister, __ T16B, $dst$$FloatRegister);\n+      __ uaddlp($dst$$FloatRegister, __ T8H, $dst$$FloatRegister);\n+      __ uaddlp($dst$$FloatRegister, __ T4S, $dst$$FloatRegister);\n+      __ xtn($dst$$FloatRegister, __ T2S, $dst$$FloatRegister, __ T2D);\n+    } else {\n+      __ sve_cnt($dst$$FloatRegister, __ D, ptrue, $src$$FloatRegister);\n+      __ sve_vector_narrow($dst$$FloatRegister, __ S,\n+                           $dst$$FloatRegister, __ D, $tmp$$FloatRegister);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector popcount - predicated\n+\n+instruct vpopcountI_masked(vReg dst_src, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (PopCountVI dst_src pg));\n+  format %{ \"vpopcountI_masked $dst_src, $pg, $dst_src\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ sve_cnt($dst_src$$FloatRegister, __ elemType_to_regVariant(bt),\n+               $pg$$PRegister, $dst_src$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vpopcountL_masked(vReg dst_src, pRegGov pg) %{\n+  predicate(UseSVE > 0 && Matcher::vector_element_basic_type(n) == T_LONG);\n+  match(Set dst_src (PopCountVL dst_src pg));\n+  format %{ \"vpopcountL_masked $dst_src, $pg, $dst_src\" %}\n+  ins_encode %{\n+    __ sve_cnt($dst_src$$FloatRegister, __ D,\n+               $pg$$PRegister, $dst_src$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ Vector blend ---------------------------------\n+\n+instruct vblend_neon(vReg dst, vReg src1, vReg src2) %{\n+  predicate(UseSVE == 0);\n+  match(Set dst (VectorBlend (Binary src1 src2) dst));\n+  format %{ \"vblend_neon $dst, $src1, $src2\" %}\n+  ins_encode %{\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this);\n+    assert(length_in_bytes == 8 || length_in_bytes == 16, \"must be\");\n+    __ bsl($dst$$FloatRegister, length_in_bytes == 16 ? __ T16B : __ T8B,\n+           $src2$$FloatRegister, $src1$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vblend_sve(vReg dst, vReg src1, vReg src2, pReg pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst (VectorBlend (Binary src1 src2) pg));\n+  format %{ \"vblend_sve $dst, $pg, $src1, $src2\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ sve_sel($dst$$FloatRegister, __ elemType_to_regVariant(bt),\n+               $pg$$PRegister, $src2$$FloatRegister, $src1$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ Vector round ---------------------------------\n+\n+\/\/ vector Math.round\n+\n+instruct vround_le128b(vReg dst, vReg src, vReg tmp1, vReg tmp2,\n+                       vReg tmp3, rFlagsReg cr) %{\n+  predicate(Matcher::vector_length_in_bytes(n) <= 16);\n+  match(Set dst (RoundVF src));\n+  match(Set dst (RoundVD src));\n+  effect(TEMP_DEF dst, TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  format %{ \"vround_le128b $dst, $src\\t# vector <= 128 bits. KILL $tmp1, $tmp2, $tmp3, cr\" %}\n+  ins_encode %{\n+    __ vector_round_neon($dst$$FloatRegister, $src$$FloatRegister,\n+                         $tmp1$$FloatRegister, $tmp2$$FloatRegister,\n+                         $tmp3$$FloatRegister, get_arrangement(this));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vround_gt128b(vReg dst, vReg src, vReg tmp1, vReg tmp2,\n+                       pRegGov pgtmp, rFlagsReg cr) %{\n+  predicate(Matcher::vector_length_in_bytes(n) > 16);\n+  match(Set dst (RoundVF src));\n+  match(Set dst (RoundVD src));\n+  effect(TEMP_DEF dst, TEMP tmp1, TEMP tmp2, TEMP pgtmp, KILL cr);\n+  format %{ \"vround_gt128b $dst, $src\\t# vector > 128 bits. KILL $tmp1, $tmp2, $pgtmp, cr\" %}\n+  ins_encode %{\n+    assert(UseSVE > 0, \"must be sve\");\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ vector_round_sve($dst$$FloatRegister, $src$$FloatRegister,\n+                        $tmp1$$FloatRegister, $tmp2$$FloatRegister,\n+                        $pgtmp$$PRegister, __ elemType_to_regVariant(bt));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ RoundDouble ----------------------------------\n+\n+\/\/ vector Math.rint, floor, ceil\n+\n+instruct vroundD(vReg dst, vReg src, immI rmode) %{\n+  predicate(Matcher::vector_element_basic_type(n) == T_DOUBLE);\n+  match(Set dst (RoundDoubleModeV src rmode));\n+  format %{ \"vroundD $dst, $src, $rmode\" %}\n+  ins_encode %{\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this);\n+    if (length_in_bytes == 16) {\n+      switch ($rmode$$constant) {\n+        case RoundDoubleModeNode::rmode_rint:\n+          __ frintn($dst$$FloatRegister, __ T2D, $src$$FloatRegister);\n+          break;\n+        case RoundDoubleModeNode::rmode_floor:\n+          __ frintm($dst$$FloatRegister, __ T2D, $src$$FloatRegister);\n+          break;\n+        case RoundDoubleModeNode::rmode_ceil:\n+          __ frintp($dst$$FloatRegister, __ T2D, $src$$FloatRegister);\n+          break;\n+        default:\n+          assert(false, \"unsupported\");\n+          ShouldNotReachHere();\n+      }\n+    } else {\n+      assert(UseSVE > 0 && length_in_bytes > 16, \"must be\");\n+      switch ($rmode$$constant) {\n+        case RoundDoubleModeNode::rmode_rint:\n+          __ sve_frintn($dst$$FloatRegister, __ D, ptrue, $src$$FloatRegister);\n+          break;\n+        case RoundDoubleModeNode::rmode_floor:\n+          __ sve_frintm($dst$$FloatRegister, __ D, ptrue, $src$$FloatRegister);\n+          break;\n+        case RoundDoubleModeNode::rmode_ceil:\n+          __ sve_frintp($dst$$FloatRegister, __ D, ptrue, $src$$FloatRegister);\n+          break;\n+        default:\n+          assert(false, \"unsupported\");\n+          ShouldNotReachHere();\n+      }\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ VectorTest -----------------------------------\n+\n+\/\/ anytrue\n+\n+instruct vtest_anytrue_neon(iRegINoSp dst, vReg src1, vReg src2, vReg tmp, rFlagsReg cr) %{\n+  predicate(UseSVE == 0 &&\n+            static_cast<const VectorTestNode*>(n)->get_predicate() == BoolTest::ne);\n+  match(Set dst (VectorTest src1 src2 ));\n+  effect(TEMP tmp, KILL cr);\n+  format %{ \"vtest_anytrue_neon $dst, $src1\\t# KILL $tmp, cr\" %}\n+  ins_encode %{\n+    \/\/ No need to use src2.\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this, $src1);\n+    assert(length_in_bytes == 8 || length_in_bytes == 16, \"must be\");\n+    __ addv($tmp$$FloatRegister, length_in_bytes == 16 ? __ T16B : __ T8B, $src1$$FloatRegister);\n+    __ umov($dst$$Register, $tmp$$FloatRegister, __ B, 0);\n+    __ cmpw($dst$$Register, zr);\n+    __ csetw($dst$$Register, Assembler::NE);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vtest_anytrue_sve(iRegINoSp dst, pRegGov src1, pRegGov src2, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            static_cast<const VectorTestNode*>(n)->get_predicate() == BoolTest::ne);\n+  match(Set dst (VectorTest src1 src2));\n+  effect(KILL cr);\n+  format %{ \"vtest_anytrue_sve $dst, $src1\\t# KILL cr\" %}\n+  ins_encode %{\n+    \/\/ \"src2\" is not used for sve.\n+    __ sve_ptest(ptrue, $src1$$PRegister);\n+    __ csetw($dst$$Register, Assembler::NE);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ alltrue\n+\n+instruct vtest_alltrue_neon(iRegINoSp dst, vReg src1, vReg src2, vReg tmp, rFlagsReg cr) %{\n+  predicate(UseSVE == 0 &&\n+            static_cast<const VectorTestNode*>(n)->get_predicate() == BoolTest::overflow);\n+  match(Set dst (VectorTest src1 src2));\n+  effect(TEMP tmp, KILL cr);\n+  format %{ \"vtest_alltrue_neon $dst, $src1\\t# KILL $tmp, cr\" %}\n+  ins_encode %{\n+    \/\/ No need to use src2.\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this, $src1);\n+    assert(length_in_bytes == 8 || length_in_bytes == 16, \"must be\");\n+    __ uminv($tmp$$FloatRegister, length_in_bytes == 16 ? __ T16B : __ T8B, $src1$$FloatRegister);\n+    __ umov($dst$$Register, $tmp$$FloatRegister, __ B, 0);\n+    __ cmpw($dst$$Register, 0xff);\n+    __ csetw($dst$$Register, Assembler::EQ);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vtest_alltrue_sve(iRegINoSp dst, pRegGov src1, pRegGov src2, pReg ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            static_cast<const VectorTestNode*>(n)->get_predicate() == BoolTest::overflow);\n+  match(Set dst (VectorTest src1 src2));\n+  effect(TEMP ptmp, KILL cr);\n+  format %{ \"vtest_alltrue_sve $dst, $src1, $src2\\t# KILL $ptmp, cr\" %}\n+  ins_encode %{\n+    __ sve_eors($ptmp$$PRegister, ptrue, $src1$$PRegister, $src2$$PRegister);\n+    __ csetw($dst$$Register, Assembler::EQ);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ Vector shuffle -------------------------------\n+\n+instruct loadshuffle(vReg dst, vReg src) %{\n+  match(Set dst (VectorLoadShuffle src));\n+  format %{ \"loadshuffle $dst, $src\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this);\n+    if (bt == T_BYTE) {\n+      if ($dst$$FloatRegister != $src$$FloatRegister) {\n+        if (length_in_bytes <= 16) {\n+          __ orr($dst$$FloatRegister, length_in_bytes == 16 ? __ T16B : __ T8B,\n+                 $src$$FloatRegister, $src$$FloatRegister);\n+        } else {\n+          assert(UseSVE > 0, \"must be sve\");\n+          __ sve_orr($dst$$FloatRegister, $src$$FloatRegister, $src$$FloatRegister);\n+        }\n+      }\n+    } else if (bt == T_SHORT && length_in_bytes <= 16) {\n+      __ uxtl($dst$$FloatRegister, __ T8H, $src$$FloatRegister, __ T8B);\n+    } else if (type2aelembytes(bt) == 4 && length_in_bytes == 16) {\n+      __ uxtl($dst$$FloatRegister, __ T8H, $src$$FloatRegister, __ T8B);\n+      __ uxtl($dst$$FloatRegister, __ T4S, $dst$$FloatRegister, __ T4H);\n+    } else {\n+      assert(UseSVE > 0, \"must be sve\");\n+      __ sve_vector_extend($dst$$FloatRegister,  __ elemType_to_regVariant(bt),\n+                           $src$$FloatRegister, __ B);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ Vector rearrange -----------------------------\n+\n+\/\/ Here is an example that rearranges a NEON vector with 4 ints:\n+\/\/ Rearrange V1 int[a0, a1, a2, a3] to V2 int[a2, a3, a0, a1]\n+\/\/   1. Get the indices of V1 and store them as Vi byte[0, 1, 2, 3].\n+\/\/   2. Convert Vi byte[0, 1, 2, 3] to the indices of V2 and also store them as Vi byte[2, 3, 0, 1].\n+\/\/   3. Unsigned extend Long Vi from byte[2, 3, 0, 1] to int[2, 3, 0, 1].\n+\/\/   4. Multiply Vi int[2, 3, 0, 1] with constant int[0x04040404, 0x04040404, 0x04040404, 0x04040404]\n+\/\/      and get tbl base Vm int[0x08080808, 0x0c0c0c0c, 0x00000000, 0x04040404].\n+\/\/   5. Add Vm with constant int[0x03020100, 0x03020100, 0x03020100, 0x03020100]\n+\/\/      and get tbl index Vm int[0x0b0a0908, 0x0f0e0d0c, 0x03020100, 0x07060504]\n+\/\/   6. Use Vm as index register, and use V1 as table register.\n+\/\/      Then get V2 as the result by tbl NEON instructions.\n+\/\/ Notes:\n+\/\/   Step 1 matches VectorLoadConst.\n+\/\/   Step 3 matches VectorLoadShuffle.\n+\/\/   Step 4, 5, 6 match VectorRearrange.\n+\/\/   For VectorRearrange short\/int, the reason why such complex calculation is\n+\/\/   required is because NEON tbl supports bytes table only, so for short\/int, we\n+\/\/   need to lookup 2\/4 bytes as a group. For VectorRearrange long, we use bsl\n+\/\/   to implement rearrange.\n+\n+instruct rearrange_HS_neon(vReg dst, vReg src, vReg shuffle, vReg tmp1, vReg tmp2) %{\n+  predicate(UseSVE == 0 &&\n+            (Matcher::vector_element_basic_type(n) == T_SHORT ||\n+             (type2aelembytes(Matcher::vector_element_basic_type(n)) == 4 &&\n+              Matcher::vector_length_in_bytes(n) == 16)));\n+  match(Set dst (VectorRearrange src shuffle));\n+  effect(TEMP_DEF dst, TEMP tmp1, TEMP tmp2);\n+  format %{ \"rearrange_HS_neon $dst, $src, $shuffle\\t# vector (4S\/8S\/4I\/4F). KILL $tmp1, $tmp2\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    if (bt == T_SHORT) {\n+      uint length_in_bytes = Matcher::vector_length_in_bytes(this);\n+      assert(length_in_bytes == 8 || length_in_bytes == 16, \"must be\");\n+      Assembler::SIMD_Arrangement size1 = length_in_bytes == 16 ? __ T16B : __ T8B;\n+      Assembler::SIMD_Arrangement size2 = length_in_bytes == 16 ? __ T8H : __ T4H;\n+      __ mov($tmp1$$FloatRegister, size1, 0x02);\n+      __ mov($tmp2$$FloatRegister, size2, 0x0100);\n+      __ mulv($dst$$FloatRegister, size2, $shuffle$$FloatRegister, $tmp1$$FloatRegister);\n+      __ addv($dst$$FloatRegister, size1, $dst$$FloatRegister, $tmp2$$FloatRegister);\n+      __ tbl($dst$$FloatRegister, size1, $src$$FloatRegister, 1, $dst$$FloatRegister);\n+    } else {\n+      assert(bt == T_INT || bt == T_FLOAT, \"unsupported type\");\n+      __ mov($tmp1$$FloatRegister, __ T16B, 0x04);\n+      __ mov($tmp2$$FloatRegister, __ T4S, 0x03020100);\n+      __ mulv($dst$$FloatRegister, __ T4S, $shuffle$$FloatRegister, $tmp1$$FloatRegister);\n+      __ addv($dst$$FloatRegister, __ T16B, $dst$$FloatRegister, $tmp2$$FloatRegister);\n+      __ tbl($dst$$FloatRegister, __ T16B, $src$$FloatRegister, 1, $dst$$FloatRegister);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct rearrange(vReg dst, vReg src, vReg shuffle) %{\n+  predicate(Matcher::vector_element_basic_type(n) == T_BYTE || UseSVE > 0);\n+  match(Set dst (VectorRearrange src shuffle));\n+  format %{ \"rearrange $dst, $src, $shuffle\" %}\n+  ins_encode %{\n+    BasicType bt_dst = Matcher::vector_element_basic_type(this);\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this);\n+    if (bt_dst == T_BYTE && length_in_bytes <= 16) {\n+      __ tbl($dst$$FloatRegister, length_in_bytes == 16 ? __ T16B : __ T8B,\n+             $src$$FloatRegister, 1, $shuffle$$FloatRegister);\n+    } else {\n+      assert(UseSVE > 0, \"must be sve\");\n+      BasicType bt_src = Matcher::vector_element_basic_type(this, $src);\n+      __ sve_tbl($dst$$FloatRegister, __ elemType_to_regVariant(bt_src),\n+                 $src$$FloatRegister, $shuffle$$FloatRegister);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ Vector Load Gather ---------------------------\n+\n+instruct gather_loadS(vReg dst, indirect mem, vReg idx) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_LoadVectorGather()->memory_size() == MaxVectorSize &&\n+            type2aelembytes(Matcher::vector_element_basic_type(n)) == 4);\n+  match(Set dst (LoadVectorGather mem idx));\n+  format %{ \"gather_loadS $dst, $mem, $idx\\t# vector (sve)\" %}\n+  ins_encode %{\n+    __ sve_ld1w_gather($dst$$FloatRegister, ptrue,\n+                       as_Register($mem$$base), $idx$$FloatRegister);\n+ %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct gather_loadD(vReg dst, indirect mem, vReg idx, vReg tmp) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_LoadVectorGather()->memory_size() == MaxVectorSize &&\n+            type2aelembytes(Matcher::vector_element_basic_type(n)) == 8);\n+  match(Set dst (LoadVectorGather mem idx));\n+  effect(TEMP tmp);\n+  format %{ \"gather_loadD $dst, $mem, $idx\\t# vector (sve). KILL $tmp\" %}\n+  ins_encode %{\n+    __ sve_uunpklo($tmp$$FloatRegister, __ D, $idx$$FloatRegister);\n+    __ sve_ld1d_gather($dst$$FloatRegister, ptrue, as_Register($mem$$base),\n+                       $tmp$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct gather_loadS_partial(vReg dst, indirect mem, vReg idx, pRegGov pgtmp) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_LoadVectorGather()->memory_size() < MaxVectorSize &&\n+            type2aelembytes(Matcher::vector_element_basic_type(n)) == 4);\n+  match(Set dst (LoadVectorGather mem idx));\n+  effect(TEMP pgtmp);\n+  format %{ \"gather_loadS_partial $dst, $mem, $idx\\t# KILL $pgtmp\" %}\n+  ins_encode %{\n+    __ sve_ptrue_lanecnt($pgtmp$$PRegister, __ S, Matcher::vector_length(this));\n+    __ sve_ld1w_gather($dst$$FloatRegister, $pgtmp$$PRegister,\n+                       as_Register($mem$$base), $idx$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct gather_loadD_partial(vReg dst, indirect mem, vReg idx, vReg tmp, pRegGov pgtmp) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_LoadVectorGather()->memory_size() < MaxVectorSize &&\n+            type2aelembytes(Matcher::vector_element_basic_type(n)) == 8);\n+  match(Set dst (LoadVectorGather mem idx));\n+  effect(TEMP tmp, TEMP pgtmp);\n+  format %{ \"gather_loadD_partial $dst, $mem, $idx\\t# KILL $tmp, $pgtmp\" %}\n+  ins_encode %{\n+    __ sve_ptrue_lanecnt($pgtmp$$PRegister, __ D, Matcher::vector_length(this));\n+    __ sve_uunpklo($tmp$$FloatRegister, __ D, $idx$$FloatRegister);\n+    __ sve_ld1d_gather($dst$$FloatRegister, $pgtmp$$PRegister,\n+                       as_Register($mem$$base), $tmp$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct gather_loadS_masked(vReg dst, indirect mem, vReg idx, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            type2aelembytes(Matcher::vector_element_basic_type(n)) == 4);\n+  match(Set dst (LoadVectorGatherMasked mem (Binary idx pg)));\n+  format %{ \"gather_loadS_masked $dst, $pg, $mem, $idx\" %}\n+  ins_encode %{\n+    __ sve_ld1w_gather($dst$$FloatRegister, $pg$$PRegister,\n+                       as_Register($mem$$base), $idx$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct gather_loadD_masked(vReg dst, indirect mem, vReg idx, pRegGov pg, vReg tmp) %{\n+  predicate(UseSVE > 0 &&\n+            type2aelembytes(Matcher::vector_element_basic_type(n)) == 8);\n+  match(Set dst (LoadVectorGatherMasked mem (Binary idx pg)));\n+  effect(TEMP tmp);\n+  format %{ \"gather_loadD_masked $dst, $pg, $mem, $idx\\t# KILL $tmp\" %}\n+  ins_encode %{\n+    __ sve_uunpklo($tmp$$FloatRegister, __ D, $idx$$FloatRegister);\n+    __ sve_ld1d_gather($dst$$FloatRegister, $pg$$PRegister,\n+                       as_Register($mem$$base), $tmp$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ Vector Store Scatter -------------------------\n+\n+instruct scatter_storeS(indirect mem, vReg src, vReg idx) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_StoreVectorScatter()->memory_size() == MaxVectorSize &&\n+            type2aelembytes(Matcher::vector_element_basic_type(n->in(3)->in(1))) == 4);\n+  match(Set mem (StoreVectorScatter mem (Binary src idx)));\n+  format %{ \"scatter_storeS $mem, $idx, $src\\t# vector (sve)\" %}\n+  ins_encode %{\n+    __ sve_st1w_scatter($src$$FloatRegister, ptrue,\n+                        as_Register($mem$$base), $idx$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct scatter_storeD(indirect mem, vReg src, vReg idx, vReg tmp) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_StoreVectorScatter()->memory_size() == MaxVectorSize &&\n+            type2aelembytes(Matcher::vector_element_basic_type(n->in(3)->in(1))) == 8);\n+  match(Set mem (StoreVectorScatter mem (Binary src idx)));\n+  effect(TEMP tmp);\n+  format %{ \"scatter_storeD $mem, $idx, $src\\t# vector (sve). KILL $tmp\" %}\n+  ins_encode %{\n+    __ sve_uunpklo($tmp$$FloatRegister, __ D, $idx$$FloatRegister);\n+    __ sve_st1d_scatter($src$$FloatRegister, ptrue,\n+                        as_Register($mem$$base), $tmp$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct scatter_storeS_partial(indirect mem, vReg src, vReg idx, pRegGov pgtmp) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_StoreVectorScatter()->memory_size() < MaxVectorSize &&\n+            type2aelembytes(Matcher::vector_element_basic_type(n->in(3)->in(1))) == 4);\n+  match(Set mem (StoreVectorScatter mem (Binary src idx)));\n+  effect(TEMP pgtmp);\n+  format %{ \"scatter_storeS_partial $mem, $idx, $src\\t# KILL $pgtmp\" %}\n+  ins_encode %{\n+    __ sve_ptrue_lanecnt($pgtmp$$PRegister, __ S, Matcher::vector_length(this, $src));\n+    __ sve_st1w_scatter($src$$FloatRegister, $pgtmp$$PRegister,\n+                        as_Register($mem$$base), $idx$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct scatter_storeD_partial(indirect mem, vReg src, vReg idx, vReg tmp, pRegGov pgtmp) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_StoreVectorScatter()->memory_size() < MaxVectorSize &&\n+            type2aelembytes(Matcher::vector_element_basic_type(n->in(3)->in(1))) == 8);\n+  match(Set mem (StoreVectorScatter mem (Binary src idx)));\n+  effect(TEMP tmp, TEMP pgtmp);\n+  format %{ \"scatter_storeD_partial $mem, $idx, $src\\t# KILL $tmp, $pgtmp\" %}\n+  ins_encode %{\n+    __ sve_ptrue_lanecnt($pgtmp$$PRegister, __ D, Matcher::vector_length(this, $src));\n+    __ sve_uunpklo($tmp$$FloatRegister, __ D, $idx$$FloatRegister);\n+    __ sve_st1d_scatter($src$$FloatRegister, $pgtmp$$PRegister,\n+                        as_Register($mem$$base), $tmp$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct scatter_storeS_masked(indirect mem, vReg src, vReg idx, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            type2aelembytes(Matcher::vector_element_basic_type(n->in(3)->in(1))) == 4);\n+  match(Set mem (StoreVectorScatterMasked mem (Binary src (Binary idx pg))));\n+  format %{ \"scatter_storeS_masked $mem, $pg, $idx, $src\" %}\n+  ins_encode %{\n+    __ sve_st1w_scatter($src$$FloatRegister, $pg$$PRegister,\n+                        as_Register($mem$$base), $idx$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct scatter_storeD_masked(indirect mem, vReg src, vReg idx, pRegGov pg, vReg tmp) %{\n+  predicate(UseSVE > 0 &&\n+            type2aelembytes(Matcher::vector_element_basic_type(n->in(3)->in(1))) == 8);\n+  match(Set mem (StoreVectorScatterMasked mem (Binary src (Binary idx pg))));\n+  effect(TEMP tmp);\n+  format %{ \"scatter_storeD_masked $mem, $pg, $idx, $src\\t# KILL $tmp\" %}\n+  ins_encode %{\n+    __ sve_uunpklo($tmp$$FloatRegister, __ D, $idx$$FloatRegister);\n+    __ sve_st1d_scatter($src$$FloatRegister, $pg$$PRegister,\n+                        as_Register($mem$$base), $tmp$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ CountLeadingZerosV ---------------------------\n+\n+instruct vcountLeadingZeros(vReg dst, vReg src) %{\n+  predicate(!n->as_Vector()->is_predicated_vector());\n+  match(Set dst (CountLeadingZerosV src));\n+  format %{ \"vcountLeadingZeros $dst, $src\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this);\n+    if (UseSVE == 0 && bt == T_LONG) {\n+      __ umov(rscratch1, $src$$FloatRegister, __ D, 0);\n+      __ clz(rscratch1, rscratch1);\n+      __ mov($dst$$FloatRegister, __ D, 0, rscratch1);\n+      __ umov(rscratch1, $src$$FloatRegister, __ D, 1);\n+      __ clz(rscratch1, rscratch1);\n+      __ mov($dst$$FloatRegister, __ D, 1, rscratch1);\n+    } else if (bt != T_LONG && length_in_bytes <= 16) {\n+      __ clz($dst$$FloatRegister, get_arrangement(this), $src$$FloatRegister);\n+    } else {\n+      assert(UseSVE > 0, \"must be sve\");\n+      __ sve_clz($dst$$FloatRegister, __ elemType_to_regVariant(bt),\n+                 ptrue, $src$$FloatRegister);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ The dst and src should use the same register to make sure the\n+\/\/ inactive lanes in dst save the same elements as src.\n+instruct vcountLeadingZeros_masked(vReg dst_src, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (CountLeadingZerosV dst_src pg));\n+  format %{ \"vcountLeadingZeros_masked $dst_src, $pg, $dst_src\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ sve_clz($dst_src$$FloatRegister, __ elemType_to_regVariant(bt),\n+               $pg$$PRegister, $dst_src$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ CountTrailingZerosV --------------------------\n+\n+instruct vcountTrailingZeros(vReg dst, vReg src) %{\n+  predicate(!n->as_Vector()->is_predicated_vector());\n+  match(Set dst (CountTrailingZerosV src));\n+  format %{ \"vcountTrailingZeros $dst, $src\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this);\n+    if (bt == T_BYTE && length_in_bytes <= 16) {\n+      __ rbit($dst$$FloatRegister, length_in_bytes == 16 ? __ T16B : __ T8B,\n+              $src$$FloatRegister);\n+      __ clz($dst$$FloatRegister, get_arrangement(this), $dst$$FloatRegister);\n+    } else if (UseSVE == 0) {\n+      assert(bt == T_SHORT || bt == T_INT || bt == T_LONG, \"unsupported type\");\n+      assert(length_in_bytes == 8 || length_in_bytes == 16, \"unsupported\");\n+      __ neon_reverse_bits($dst$$FloatRegister, $src$$FloatRegister,\n+                           bt, \/* isQ *\/ length_in_bytes == 16);\n+      if (bt != T_LONG) {\n+        __ clz($dst$$FloatRegister, get_arrangement(this), $dst$$FloatRegister);\n+      } else {\n+        __ umov(rscratch1, $dst$$FloatRegister, __ D, 0);\n+        __ clz(rscratch1, rscratch1);\n+        __ mov($dst$$FloatRegister, __ D, 0, rscratch1);\n+        __ umov(rscratch1, $dst$$FloatRegister, __ D, 1);\n+        __ clz(rscratch1, rscratch1);\n+        __ mov($dst$$FloatRegister, __ D, 1, rscratch1);\n+      }\n+    } else {\n+      assert(UseSVE > 0, \"must be sve\");\n+      Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+      __ sve_rbit($dst$$FloatRegister, size, ptrue, $src$$FloatRegister);\n+      __ sve_clz($dst$$FloatRegister, size, ptrue, $dst$$FloatRegister);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ The dst and src should use the same register to make sure the\n+\/\/ inactive lanes in dst save the same elements as src.\n+instruct vcountTrailingZeros_masked(vReg dst_src, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (CountTrailingZerosV dst_src pg));\n+  format %{ \"vcountTrailingZeros_masked $dst_src, $pg, $dst_src\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    __ sve_rbit($dst_src$$FloatRegister, size,\n+                $pg$$PRegister, $dst_src$$FloatRegister);\n+    __ sve_clz($dst_src$$FloatRegister, size,\n+               $pg$$PRegister, $dst_src$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ ReverseV -------------------------------------\n+\n+instruct vreverse(vReg dst, vReg src) %{\n+  predicate(!n->as_Vector()->is_predicated_vector());\n+  match(Set dst (ReverseV src));\n+  format %{ \"vreverse $dst, $src\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this);\n+    if (bt == T_BYTE && length_in_bytes <= 16) {\n+      __ rbit($dst$$FloatRegister, length_in_bytes == 16 ? __ T16B : __ T8B,\n+              $src$$FloatRegister);\n+    } else if (UseSVE == 0) {\n+      assert(bt == T_SHORT || bt == T_INT || bt == T_LONG, \"unsupported type\");\n+      assert(length_in_bytes == 8 || length_in_bytes == 16, \"unsupported\");\n+      __ neon_reverse_bits($dst$$FloatRegister, $src$$FloatRegister,\n+                           bt, \/* isQ *\/ length_in_bytes == 16);\n+    } else {\n+      assert(UseSVE > 0, \"must be sve\");\n+      __ sve_rbit($dst$$FloatRegister, __ elemType_to_regVariant(bt),\n+                  ptrue, $src$$FloatRegister);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ The dst and src should use the same register to make sure the\n+\/\/ inactive lanes in dst save the same elements as src.\n+instruct vreverse_masked(vReg dst_src, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (ReverseV dst_src pg));\n+  format %{ \"vreverse_masked $dst_src, $pg, $dst_src\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ sve_rbit($dst_src$$FloatRegister, __ elemType_to_regVariant(bt),\n+                $pg$$PRegister, $dst_src$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ ReverseBytesV --------------------------------\n+\n+instruct vreverseBytes(vReg dst, vReg src) %{\n+  predicate(!n->as_Vector()->is_predicated_vector());\n+  match(Set dst (ReverseBytesV src));\n+  format %{ \"vreverseBytes $dst, $src\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this);\n+    if (length_in_bytes <= 16) {\n+      assert(length_in_bytes == 8 || length_in_bytes == 16, \"unsupported\");\n+      __ neon_reverse_bytes($dst$$FloatRegister, $src$$FloatRegister,\n+                            bt, \/* isQ *\/ length_in_bytes == 16);\n+    } else {\n+      assert(UseSVE > 0, \"must be sve\");\n+      if (bt == T_BYTE) {\n+        if ($dst$$FloatRegister != $src$$FloatRegister) {\n+          __ sve_orr($dst$$FloatRegister, $src$$FloatRegister, $src$$FloatRegister);\n+        }\n+      } else {\n+        __ sve_revb($dst$$FloatRegister, __ elemType_to_regVariant(bt),\n+                    ptrue, $src$$FloatRegister);\n+      }\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ The dst and src should use the same register to make sure the\n+\/\/ inactive lanes in dst save the same elements as src.\n+instruct vreverseBytes_masked(vReg dst_src, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (ReverseBytesV dst_src pg));\n+  format %{ \"vreverseBytes_masked $dst_src, $pg, $dst_src\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    if (bt == T_BYTE) {\n+      \/\/ do nothing\n+    } else {\n+      __ sve_revb($dst_src$$FloatRegister, __ elemType_to_regVariant(bt),\n+                  $pg$$PRegister, $dst_src$$FloatRegister);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ Populate Index to a Vector -------------------\n+\n+instruct populateindex(vReg dst, iRegIorL2I src1, immI src2) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst (PopulateIndex src1 src2));\n+  format %{ \"populateindex $dst, $src1, $src2\\t # populate index (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ sve_index($dst$$FloatRegister, __ elemType_to_regVariant(bt),\n+                 $src1$$Register, (int)($src2$$constant));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ Compress\/Expand Operations -------------------\n+\n+instruct mcompress(pReg dst, pReg pg, rFlagsReg cr) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst (CompressM pg));\n+  effect(KILL cr);\n+  format %{ \"mcompress $dst, $pg\\t# KILL cr\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    __ sve_cntp(rscratch1, size, ptrue, $pg$$PRegister);\n+    __ sve_whilelo(as_PRegister($dst$$reg), size, zr, rscratch1);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vcompress(vReg dst, vReg src, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            !is_subword_type(Matcher::vector_element_basic_type(n)));\n+  match(Set dst (CompressV src pg));\n+  format %{ \"vcompress $dst, $src, $pg\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ sve_compact($dst$$FloatRegister, __ elemType_to_regVariant(bt),\n+                   $src$$FloatRegister, $pg$$PRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vcompressB(vReg dst, vReg src, pReg pg, vReg tmp1, vReg tmp2,\n+                    vReg tmp3, vReg tmp4, pReg ptmp, pRegGov pgtmp) %{\n+  predicate(UseSVE > 0 && Matcher::vector_element_basic_type(n) == T_BYTE);\n+  effect(TEMP_DEF dst, TEMP tmp1, TEMP tmp2, TEMP tmp3, TEMP tmp4, TEMP ptmp, TEMP pgtmp);\n+  match(Set dst (CompressV src pg));\n+  format %{ \"vcompressB $dst, $src, $pg\\t# KILL $tmp1, $tmp2, $tmp3, tmp4, $ptmp, $pgtmp\" %}\n+  ins_encode %{\n+    __ sve_compress_byte($dst$$FloatRegister, $src$$FloatRegister, $pg$$PRegister,\n+                         $tmp1$$FloatRegister,$tmp2$$FloatRegister,\n+                         $tmp3$$FloatRegister,$tmp4$$FloatRegister,\n+                         $ptmp$$PRegister, $pgtmp$$PRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vcompressS(vReg dst, vReg src, pReg pg,\n+                    vReg tmp1, vReg tmp2, pRegGov pgtmp) %{\n+  predicate(UseSVE > 0 && Matcher::vector_element_basic_type(n) == T_SHORT);\n+  effect(TEMP_DEF dst, TEMP tmp1, TEMP tmp2, TEMP pgtmp);\n+  match(Set dst (CompressV src pg));\n+  format %{ \"vcompressS $dst, $src, $pg\\t# KILL $tmp1, $tmp2, $pgtmp\" %}\n+  ins_encode %{\n+    __ sve_compress_short($dst$$FloatRegister, $src$$FloatRegister, $pg$$PRegister,\n+                          $tmp1$$FloatRegister,$tmp2$$FloatRegister, $pgtmp$$PRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vexpand(vReg dst, vReg src, pRegGov pg) %{\n+  match(Set dst (ExpandV src pg));\n+  effect(TEMP_DEF dst);\n+  format %{ \"vexpand $dst, $pg, $src\" %}\n+  ins_encode %{\n+    \/\/ Example input:   src   = 1 2 3 4 5 6 7 8\n+    \/\/                  pg    = 1 0 0 1 1 0 1 1\n+    \/\/ Expected result: dst   = 4 0 0 5 6 0 7 8\n+\n+    \/\/ The basic idea is to use TBL which can shuffle the elements in the given\n+    \/\/ vector flexibly. HISTCNT + SUB is used to generate the second source input\n+    \/\/ for TBL whose value is used to select the indexed element from src vector.\n+\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    assert(UseSVE == 2 && !is_subword_type(bt), \"unsupported\");\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    \/\/ dst = 0 0 0 0 0 0 0 0\n+    __ sve_dup($dst$$FloatRegister, size, 0);\n+    \/\/ dst = 5 0 0 4 3 0 2 1\n+    __ sve_histcnt($dst$$FloatRegister, size, $pg$$PRegister,\n+                   $dst$$FloatRegister, $dst$$FloatRegister);\n+    \/\/ dst = 4 -1 -1 3 2 -1 1 0\n+    __ sve_sub($dst$$FloatRegister, size, 1);\n+    \/\/ dst = 4 0 0 5 6 0 7 8\n+    __ sve_tbl($dst$$FloatRegister, size, $src$$FloatRegister, $dst$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64_vector.ad","additions":5964,"deletions":0,"binary":false,"changes":5964,"status":"added"},{"patch":"@@ -989,1 +989,1 @@\n-      sve_extract_integral(rscratch1, D, vtmp1, idx, \/* is_signed *\/ false, vtmp2);\n+      sve_extract_integral(rscratch1, T_LONG, vtmp1, idx, vtmp2);\n@@ -1093,0 +1093,1 @@\n+\/\/ Clobbers: rflags\n@@ -1242,0 +1243,269 @@\n+\/\/ Vector reduction add for integral type with ASIMD instructions.\n+void C2_MacroAssembler::neon_reduce_add_integral(Register dst, BasicType bt,\n+                                                 Register isrc, FloatRegister vsrc,\n+                                                 unsigned vector_length_in_bytes,\n+                                                 FloatRegister vtmp) {\n+  assert(vector_length_in_bytes == 8 || vector_length_in_bytes == 16, \"unsupported\");\n+  assert_different_registers(dst, isrc);\n+  bool isQ = vector_length_in_bytes == 16;\n+\n+  BLOCK_COMMENT(\"neon_reduce_add_integral {\");\n+    switch(bt) {\n+      case T_BYTE:\n+        addv(vtmp, isQ ? T16B : T8B, vsrc);\n+        smov(dst, vtmp, B, 0);\n+        addw(dst, dst, isrc, ext::sxtb);\n+        break;\n+      case T_SHORT:\n+        addv(vtmp, isQ ? T8H : T4H, vsrc);\n+        smov(dst, vtmp, H, 0);\n+        addw(dst, dst, isrc, ext::sxth);\n+        break;\n+      case T_INT:\n+        isQ ? addv(vtmp, T4S, vsrc) : addpv(vtmp, T2S, vsrc, vsrc);\n+        umov(dst, vtmp, S, 0);\n+        addw(dst, dst, isrc);\n+        break;\n+      case T_LONG:\n+        assert(isQ, \"unsupported\");\n+        addpd(vtmp, vsrc);\n+        umov(dst, vtmp, D, 0);\n+        add(dst, dst, isrc);\n+        break;\n+      default:\n+        assert(false, \"unsupported\");\n+        ShouldNotReachHere();\n+    }\n+  BLOCK_COMMENT(\"} neon_reduce_add_integral\");\n+}\n+\n+\/\/ Vector reduction multiply for integral type with ASIMD instructions.\n+\/\/ Note: temporary registers vtmp1 and vtmp2 are not used in some cases.\n+\/\/ Clobbers: rscratch1\n+void C2_MacroAssembler::neon_reduce_mul_integral(Register dst, BasicType bt,\n+                                                 Register isrc, FloatRegister vsrc,\n+                                                 unsigned vector_length_in_bytes,\n+                                                 FloatRegister vtmp1, FloatRegister vtmp2) {\n+  assert(vector_length_in_bytes == 8 || vector_length_in_bytes == 16, \"unsupported\");\n+  bool isQ = vector_length_in_bytes == 16;\n+\n+  BLOCK_COMMENT(\"neon_reduce_mul_integral {\");\n+    switch(bt) {\n+      case T_BYTE:\n+        if (isQ) {\n+          \/\/ Multiply the lower half and higher half of vector iteratively.\n+          \/\/ vtmp1 = vsrc[8:15]\n+          ins(vtmp1, D, vsrc, 0, 1);\n+          \/\/ vtmp1[n] = vsrc[n] * vsrc[n + 8], where n=[0, 7]\n+          mulv(vtmp1, T8B, vtmp1, vsrc);\n+          \/\/ vtmp2 = vtmp1[4:7]\n+          ins(vtmp2, S, vtmp1, 0, 1);\n+          \/\/ vtmp1[n] = vtmp1[n] * vtmp1[n + 4], where n=[0, 3]\n+          mulv(vtmp1, T8B, vtmp2, vtmp1);\n+        } else {\n+          ins(vtmp1, S, vsrc, 0, 1);\n+          mulv(vtmp1, T8B, vtmp1, vsrc);\n+        }\n+        \/\/ vtmp2 = vtmp1[2:3]\n+        ins(vtmp2, H, vtmp1, 0, 1);\n+        \/\/ vtmp2[n] = vtmp1[n] * vtmp1[n + 2], where n=[0, 1]\n+        mulv(vtmp2, T8B, vtmp2, vtmp1);\n+        \/\/ dst = vtmp2[0] * isrc * vtmp2[1]\n+        umov(rscratch1, vtmp2, B, 0);\n+        mulw(dst, rscratch1, isrc);\n+        sxtb(dst, dst);\n+        umov(rscratch1, vtmp2, B, 1);\n+        mulw(dst, rscratch1, dst);\n+        sxtb(dst, dst);\n+        break;\n+      case T_SHORT:\n+        if (isQ) {\n+          ins(vtmp2, D, vsrc, 0, 1);\n+          mulv(vtmp2, T4H, vtmp2, vsrc);\n+          ins(vtmp1, S, vtmp2, 0, 1);\n+          mulv(vtmp1, T4H, vtmp1, vtmp2);\n+        } else {\n+          ins(vtmp1, S, vsrc, 0, 1);\n+          mulv(vtmp1, T4H, vtmp1, vsrc);\n+        }\n+        umov(rscratch1, vtmp1, H, 0);\n+        mulw(dst, rscratch1, isrc);\n+        sxth(dst, dst);\n+        umov(rscratch1, vtmp1, H, 1);\n+        mulw(dst, rscratch1, dst);\n+        sxth(dst, dst);\n+        break;\n+      case T_INT:\n+        if (isQ) {\n+          ins(vtmp1, D, vsrc, 0, 1);\n+          mulv(vtmp1, T2S, vtmp1, vsrc);\n+        } else {\n+          vtmp1 = vsrc;\n+        }\n+        umov(rscratch1, vtmp1, S, 0);\n+        mul(dst, rscratch1, isrc);\n+        umov(rscratch1, vtmp1, S, 1);\n+        mul(dst, rscratch1, dst);\n+        break;\n+      case T_LONG:\n+        umov(rscratch1, vsrc, D, 0);\n+        mul(dst, isrc, rscratch1);\n+        umov(rscratch1, vsrc, D, 1);\n+        mul(dst, dst, rscratch1);\n+        break;\n+      default:\n+        assert(false, \"unsupported\");\n+        ShouldNotReachHere();\n+    }\n+  BLOCK_COMMENT(\"} neon_reduce_mul_integral\");\n+}\n+\n+\/\/ Vector reduction multiply for floating-point type with ASIMD instructions.\n+void C2_MacroAssembler::neon_reduce_mul_fp(FloatRegister dst, BasicType bt,\n+                                           FloatRegister fsrc, FloatRegister vsrc,\n+                                           unsigned vector_length_in_bytes,\n+                                           FloatRegister vtmp) {\n+  assert(vector_length_in_bytes == 8 || vector_length_in_bytes == 16, \"unsupported\");\n+  bool isQ = vector_length_in_bytes == 16;\n+\n+  BLOCK_COMMENT(\"neon_reduce_mul_fp {\");\n+    switch(bt) {\n+      case T_FLOAT:\n+        fmuls(dst, fsrc, vsrc);\n+        ins(vtmp, S, vsrc, 0, 1);\n+        fmuls(dst, dst, vtmp);\n+        if (isQ) {\n+          ins(vtmp, S, vsrc, 0, 2);\n+          fmuls(dst, dst, vtmp);\n+          ins(vtmp, S, vsrc, 0, 3);\n+          fmuls(dst, dst, vtmp);\n+         }\n+        break;\n+      case T_DOUBLE:\n+        assert(isQ, \"unsupported\");\n+        fmuld(dst, fsrc, vsrc);\n+        ins(vtmp, D, vsrc, 0, 1);\n+        fmuld(dst, dst, vtmp);\n+        break;\n+      default:\n+        assert(false, \"unsupported\");\n+        ShouldNotReachHere();\n+    }\n+  BLOCK_COMMENT(\"} neon_reduce_mul_fp\");\n+}\n+\n+\/\/ Helper to select logical instruction\n+void C2_MacroAssembler::neon_reduce_logical_helper(int opc, bool is64, Register Rd,\n+                                                   Register Rn, Register Rm,\n+                                                   enum shift_kind kind, unsigned shift) {\n+  switch(opc) {\n+    case Op_AndReductionV:\n+      is64 ? andr(Rd, Rn, Rm, kind, shift) : andw(Rd, Rn, Rm, kind, shift);\n+      break;\n+    case Op_OrReductionV:\n+      is64 ? orr(Rd, Rn, Rm, kind, shift) : orrw(Rd, Rn, Rm, kind, shift);\n+      break;\n+    case Op_XorReductionV:\n+      is64 ? eor(Rd, Rn, Rm, kind, shift) : eorw(Rd, Rn, Rm, kind, shift);\n+      break;\n+    default:\n+      assert(false, \"unsupported\");\n+      ShouldNotReachHere();\n+  }\n+}\n+\n+\/\/ Vector reduction logical operations And, Or, Xor\n+\/\/ Clobbers: rscratch1\n+void C2_MacroAssembler::neon_reduce_logical(int opc, Register dst, BasicType bt,\n+                                            Register isrc, FloatRegister vsrc,\n+                                            unsigned vector_length_in_bytes) {\n+  assert(opc == Op_AndReductionV || opc == Op_OrReductionV || opc == Op_XorReductionV,\n+         \"unsupported\");\n+  assert(vector_length_in_bytes == 8 || vector_length_in_bytes == 16, \"unsupported\");\n+  assert_different_registers(dst, isrc);\n+  bool isQ = vector_length_in_bytes == 16;\n+\n+  BLOCK_COMMENT(\"neon_reduce_logical {\");\n+    umov(rscratch1, vsrc, isQ ? D : S, 0);\n+    umov(dst, vsrc, isQ ? D : S, 1);\n+    neon_reduce_logical_helper(opc, \/* is64 *\/ true, dst, dst, rscratch1);\n+    switch(bt) {\n+      case T_BYTE:\n+        if (isQ) {\n+          neon_reduce_logical_helper(opc, \/* is64 *\/ true, dst, dst, dst, Assembler::LSR, 32);\n+        }\n+        neon_reduce_logical_helper(opc, \/* is64 *\/ false, dst, dst, dst, Assembler::LSR, 16);\n+        neon_reduce_logical_helper(opc, \/* is64 *\/ false, dst, dst, dst, Assembler::LSR, 8);\n+        neon_reduce_logical_helper(opc, \/* is64 *\/ false, dst, isrc, dst);\n+        sxtb(dst, dst);\n+        break;\n+      case T_SHORT:\n+        if (isQ) {\n+          neon_reduce_logical_helper(opc, \/* is64 *\/ true, dst, dst, dst, Assembler::LSR, 32);\n+        }\n+        neon_reduce_logical_helper(opc, \/* is64 *\/ false, dst, dst, dst, Assembler::LSR, 16);\n+        neon_reduce_logical_helper(opc, \/* is64 *\/ false, dst, isrc, dst);\n+        sxth(dst, dst);\n+        break;\n+      case T_INT:\n+        if (isQ) {\n+          neon_reduce_logical_helper(opc, \/* is64 *\/ true, dst, dst, dst, Assembler::LSR, 32);\n+        }\n+        neon_reduce_logical_helper(opc, \/* is64 *\/ false, dst, isrc, dst);\n+        break;\n+      case T_LONG:\n+        assert(isQ, \"unsupported\");\n+        neon_reduce_logical_helper(opc, \/* is64 *\/ true, dst, isrc, dst);\n+        break;\n+      default:\n+        assert(false, \"unsupported\");\n+        ShouldNotReachHere();\n+    }\n+  BLOCK_COMMENT(\"} neon_reduce_logical\");\n+}\n+\n+\/\/ Vector reduction min\/max for integral type with ASIMD instructions.\n+\/\/ Note: vtmp is not used and expected to be fnoreg for T_LONG case.\n+\/\/ Clobbers: rscratch1, rflags\n+void C2_MacroAssembler::neon_reduce_minmax_integral(int opc, Register dst, BasicType bt,\n+                                                    Register isrc, FloatRegister vsrc,\n+                                                    unsigned vector_length_in_bytes,\n+                                                    FloatRegister vtmp) {\n+  assert(opc == Op_MinReductionV || opc == Op_MaxReductionV, \"unsupported\");\n+  assert(vector_length_in_bytes == 8 || vector_length_in_bytes == 16, \"unsupported\");\n+  assert(bt == T_BYTE || bt == T_SHORT || bt == T_INT || bt == T_LONG, \"unsupported\");\n+  assert_different_registers(dst, isrc);\n+  bool isQ = vector_length_in_bytes == 16;\n+  bool is_min = opc == Op_MinReductionV;\n+\n+  BLOCK_COMMENT(\"neon_reduce_minmax_integral {\");\n+    if (bt == T_LONG) {\n+      assert(vtmp == fnoreg, \"should be\");\n+      assert(isQ, \"should be\");\n+      umov(rscratch1, vsrc, D, 0);\n+      cmp(isrc, rscratch1);\n+      csel(dst, isrc, rscratch1, is_min ? LT : GT);\n+      umov(rscratch1, vsrc, D, 1);\n+      cmp(dst, rscratch1);\n+      csel(dst, dst, rscratch1, is_min ? LT : GT);\n+    } else {\n+      SIMD_Arrangement size = esize2arrangement((unsigned)type2aelembytes(bt), isQ);\n+      if (size == T2S) {\n+        is_min ? sminp(vtmp, size, vsrc, vsrc) : smaxp(vtmp, size, vsrc, vsrc);\n+      } else {\n+        is_min ? sminv(vtmp, size, vsrc) : smaxv(vtmp, size, vsrc);\n+      }\n+      if (bt == T_INT) {\n+        umov(dst, vtmp, S, 0);\n+      } else {\n+        smov(dst, vtmp, elemType_to_regVariant(bt), 0);\n+      }\n+      cmpw(dst, isrc);\n+      cselw(dst, dst, isrc, is_min ? LT : GT);\n+    }\n+  BLOCK_COMMENT(\"} neon_reduce_minmax_integral\");\n+}\n+\n+\/\/ Vector reduction for integral type with SVE instruction.\n+\/\/ Supported operations are Add, And, Or, Xor, Max, Min.\n+\/\/ rflags would be clobbered if opc is Op_MaxReductionV or Op_MinReductionV.\n@@ -1252,1 +1522,0 @@\n-      smov(dst, tmp, size, 0);\n@@ -1254,0 +1523,1 @@\n+        smov(dst, tmp, size, 0);\n@@ -1256,0 +1526,1 @@\n+        smov(dst, tmp, size, 0);\n@@ -1258,0 +1529,1 @@\n+        umov(dst, tmp, size, 0);\n@@ -1270,1 +1542,1 @@\n-      if (bt == T_LONG) {\n+      if (bt == T_INT || bt == T_LONG) {\n@@ -1272,1 +1544,0 @@\n-        andr(dst, dst, src1);\n@@ -1275,0 +1546,4 @@\n+      }\n+      if (bt == T_LONG) {\n+        andr(dst, dst, src1);\n+      } else {\n@@ -1281,1 +1556,1 @@\n-      if (bt == T_LONG) {\n+      if (bt == T_INT || bt == T_LONG) {\n@@ -1283,1 +1558,0 @@\n-        orr(dst, dst, src1);\n@@ -1286,0 +1560,4 @@\n+      }\n+      if (bt == T_LONG) {\n+        orr(dst, dst, src1);\n+      } else {\n@@ -1292,1 +1570,1 @@\n-      if (bt == T_LONG) {\n+      if (bt == T_INT || bt == T_LONG) {\n@@ -1294,1 +1572,0 @@\n-        eor(dst, dst, src1);\n@@ -1297,0 +1574,4 @@\n+      }\n+      if (bt == T_LONG) {\n+        eor(dst, dst, src1);\n+      } else {\n@@ -1303,1 +1584,1 @@\n-      if (bt == T_LONG) {\n+      if (bt == T_INT || bt == T_LONG) {\n@@ -1305,0 +1586,4 @@\n+      } else {\n+        smov(dst, tmp, size, 0);\n+      }\n+      if (bt == T_LONG) {\n@@ -1308,1 +1593,0 @@\n-        smov(dst, tmp, size, 0);\n@@ -1316,1 +1600,1 @@\n-      if (bt == T_LONG) {\n+      if (bt == T_INT || bt == T_LONG) {\n@@ -1318,0 +1602,4 @@\n+      } else {\n+        smov(dst, tmp, size, 0);\n+      }\n+      if (bt == T_LONG) {\n@@ -1321,1 +1609,0 @@\n-        smov(dst, tmp, size, 0);\n@@ -1527,4 +1814,4 @@\n-void C2_MacroAssembler::sve_extract_integral(Register dst, SIMD_RegVariant size, FloatRegister src, int idx,\n-                                             bool is_signed, FloatRegister vtmp) {\n-  assert(UseSVE > 0 && size != Q, \"unsupported\");\n-  assert(!(is_signed && size == D), \"signed extract (D) not supported.\");\n+void C2_MacroAssembler::sve_extract_integral(Register dst, BasicType bt, FloatRegister src,\n+                                             int idx, FloatRegister vtmp) {\n+  assert(bt == T_BYTE || bt == T_SHORT || bt == T_INT || bt == T_LONG, \"unsupported element type\");\n+  Assembler::SIMD_RegVariant size = elemType_to_regVariant(bt);\n@@ -1532,1 +1819,5 @@\n-    is_signed ? smov(dst, src, size, idx) : umov(dst, src, size, idx);\n+    if (bt == T_INT || bt == T_LONG) {\n+      umov(dst, src, size, idx);\n+    } else {\n+      smov(dst, src, size, idx);\n+    }\n@@ -1536,1 +1827,5 @@\n-    is_signed ? smov(dst, vtmp, size, 0) : umov(dst, vtmp, size, 0);\n+    if (bt == T_INT || bt == T_LONG) {\n+      umov(dst, vtmp, size, 0);\n+    } else {\n+      smov(dst, vtmp, size, 0);\n+    }\n@@ -1542,0 +1837,1 @@\n+\/\/ Clobbers: rscratch1, rflags\n@@ -1543,1 +1839,1 @@\n-                                       FloatRegister tmp2, FloatRegister tmp3, SIMD_Arrangement T) {\n+                                          FloatRegister tmp2, FloatRegister tmp3, SIMD_Arrangement T) {\n@@ -1574,0 +1870,1 @@\n+\/\/ Clobbers: rscratch1, rflags\n@@ -1575,1 +1872,2 @@\n-                                      FloatRegister tmp2, PRegister ptmp, SIMD_RegVariant T) {\n+                                         FloatRegister tmp2, PRegister pgtmp, SIMD_RegVariant T) {\n+  assert(pgtmp->is_governing(), \"This register has to be a governing predicate register\");\n@@ -1586,1 +1884,1 @@\n-      assert(T == S || T == D, \"invalid arrangement\");\n+      assert(T == S || T == D, \"invalid register variant\");\n@@ -1596,1 +1894,1 @@\n-  sve_cmp(HS, ptmp, T, ptrue, tmp2, tmp1);\n+  sve_cmp(HS, pgtmp, T, ptrue, tmp2, tmp1);\n@@ -1599,3 +1897,3 @@\n-    sve_cpy(tmp1, T, ptmp, 0.5);\n-    sve_fadd(tmp1, T, ptmp, src);\n-    sve_frintm(dst, T, ptmp, tmp1);\n+    sve_cpy(tmp1, T, pgtmp, 0.5);\n+    sve_fadd(tmp1, T, pgtmp, src);\n+    sve_frintm(dst, T, pgtmp, tmp1);\n","filename":"src\/hotspot\/cpu\/aarch64\/c2_MacroAssembler_aarch64.cpp","additions":323,"deletions":25,"binary":false,"changes":348,"status":"modified"},{"patch":"@@ -30,0 +30,5 @@\n+ private:\n+\n+  void neon_reduce_logical_helper(int opc, bool sf, Register Rd, Register Rn, Register Rm,\n+                                  enum shift_kind kind = Assembler::LSL, unsigned shift = 0);\n+\n@@ -93,0 +98,21 @@\n+  \/\/ Vector reduction\n+  void neon_reduce_add_integral(Register dst, BasicType bt,\n+                                Register isrc, FloatRegister vsrc,\n+                                unsigned vector_length_in_bytes, FloatRegister vtmp);\n+\n+  void neon_reduce_mul_integral(Register dst, BasicType bt,\n+                                Register isrc, FloatRegister vsrc,\n+                                unsigned vector_length_in_bytes,\n+                                FloatRegister vtmp1, FloatRegister vtmp2);\n+\n+  void neon_reduce_mul_fp(FloatRegister dst, BasicType bt,\n+                          FloatRegister fsrc, FloatRegister vsrc,\n+                          unsigned vector_length_in_bytes, FloatRegister vtmp);\n+\n+  void neon_reduce_logical(int opc, Register dst, BasicType bt, Register isrc,\n+                           FloatRegister vsrc, unsigned vector_length_in_bytes);\n+\n+  void neon_reduce_minmax_integral(int opc, Register dst, BasicType bt,\n+                                   Register isrc, FloatRegister vsrc,\n+                                   unsigned vector_length_in_bytes, FloatRegister vtmp);\n+\n@@ -102,2 +128,2 @@\n-  void sve_extract_integral(Register dst, SIMD_RegVariant size, FloatRegister src, int idx,\n-                            bool is_signed, FloatRegister vtmp);\n+  void sve_extract_integral(Register dst, BasicType bt, FloatRegister src,\n+                            int idx, FloatRegister vtmp);\n@@ -110,1 +136,1 @@\n-                        FloatRegister tmp2, PRegister ptmp,\n+                        FloatRegister tmp2, PRegister pgtmp,\n","filename":"src\/hotspot\/cpu\/aarch64\/c2_MacroAssembler_aarch64.hpp","additions":29,"deletions":3,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -55,2 +55,2 @@\n-  \/\/ No support for generic vector operands.\n-  static const bool supports_generic_vector_operands = false;\n+  \/\/ aarch64 supports generic vector operands: vReg.\n+  static const bool supports_generic_vector_operands = true;\n","filename":"src\/hotspot\/cpu\/aarch64\/matcher_aarch64.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -140,1 +140,1 @@\n-    max_slots_per_register = 8,\n+    max_slots_per_register = 4,\n","filename":"src\/hotspot\/cpu\/aarch64\/register_aarch64.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -2279,0 +2279,3 @@\n+#endif\n+#if defined(AARCH64)\n+    if (strcmp(rep_var,\"$PRegister\") == 0)  return \"as_PRegister\";\n","filename":"src\/hotspot\/share\/adlc\/output_c.cpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -138,0 +138,8 @@\n+#if defined(AARCH64)\n+  PRegister as_PRegister(PhaseRegAlloc* ra_, const Node* node) const {\n+    return ::as_PRegister(reg(ra_, node));\n+  }\n+  PRegister as_PRegister(PhaseRegAlloc* ra_, const Node* node, int idx) const {\n+    return ::as_PRegister(reg(ra_, node, idx));\n+  }\n+#endif\n","filename":"src\/hotspot\/share\/opto\/machnode.hpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -102,1 +102,1 @@\n-         SlotsPerVecA = RISCV_ONLY(4) NOT_RISCV(8),\n+         SlotsPerVecA = 4,\n","filename":"src\/hotspot\/share\/opto\/regmask.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -227,1 +227,1 @@\n-    @IR(counts = { \"sve_mla\", \">= 1\" })\n+    @IR(counts = { \"vmla_masked\", \">= 1\" })\n@@ -240,1 +240,1 @@\n-    @IR(counts = { \"sve_mls\", \">= 1\" })\n+    @IR(counts = { \"vmls_masked\", \">= 1\" })\n@@ -253,1 +253,1 @@\n-    @IR(counts = { \"sve_mla\", \">= 1\" })\n+    @IR(counts = { \"vmla_masked\", \">= 1\" })\n@@ -266,1 +266,1 @@\n-    @IR(counts = { \"sve_mls\", \">= 1\" })\n+    @IR(counts = { \"vmls_masked\", \">= 1\" })\n@@ -279,1 +279,1 @@\n-    @IR(counts = { \"sve_mla\", \">= 1\" })\n+    @IR(counts = { \"vmla_masked\", \">= 1\" })\n@@ -292,1 +292,1 @@\n-    @IR(counts = { \"sve_mls\", \">= 1\" })\n+    @IR(counts = { \"vmls_masked\", \">= 1\" })\n@@ -305,1 +305,1 @@\n-    @IR(counts = { \"sve_mla\", \">= 1\" })\n+    @IR(counts = { \"vmla_masked\", \">= 1\" })\n@@ -318,1 +318,1 @@\n-    @IR(counts = { \"sve_mls\", \">= 1\" })\n+    @IR(counts = { \"vmls_masked\", \">= 1\" })\n@@ -331,1 +331,1 @@\n-    @IR(counts = { \"sve_fmsb\", \">= 1\" })\n+    @IR(counts = { \"vfmsb_masked\", \">= 1\" })\n@@ -344,1 +344,1 @@\n-    @IR(counts = { \"sve_fnmad\", \">= 1\" })\n+    @IR(counts = { \"vfnmad_masked\", \">= 1\" })\n@@ -357,1 +357,1 @@\n-    @IR(counts = { \"sve_fnmsb\", \">= 1\" })\n+    @IR(counts = { \"vfnmsb_masked\", \">= 1\" })\n@@ -370,1 +370,1 @@\n-    @IR(counts = { \"sve_fmsb\", \">= 1\" })\n+    @IR(counts = { \"vfmsb_masked\", \">= 1\" })\n@@ -383,1 +383,1 @@\n-    @IR(counts = { \"sve_fnmad\", \">= 1\" })\n+    @IR(counts = { \"vfnmad_masked\", \">= 1\" })\n@@ -396,1 +396,1 @@\n-    @IR(counts = { \"sve_fnmsb\", \">= 1\" })\n+    @IR(counts = { \"vfnmsb_masked\", \">= 1\" })\n","filename":"test\/hotspot\/jtreg\/compiler\/vectorapi\/VectorFusedMultiplyAddSubTest.java","additions":14,"deletions":14,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -80,1 +80,1 @@\n-    @IR(counts = { \"sve_not\", \">= 1\" })\n+    @IR(counts = { \"vnotI_masked\", \">= 1\" })\n@@ -98,1 +98,1 @@\n-    @IR(counts = { \"sve_not\", \">= 1\" })\n+    @IR(counts = { \"vnotL_masked\", \">= 1\" })\n","filename":"test\/hotspot\/jtreg\/compiler\/vectorapi\/VectorMaskedNotTest.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"}]}