{"files":[{"patch":"@@ -5441,2 +5441,2 @@\n-    Register sp    = c_rarg1;  \/\/ source start offset\n-    Register sl    = c_rarg2;  \/\/ source end offset\n+    Register soff  = c_rarg1;  \/\/ source start offset\n+    Register send  = c_rarg2;  \/\/ source end offset\n@@ -5444,1 +5444,1 @@\n-    Register dp    = c_rarg4;  \/\/ position for writing to dest array\n+    Register doff  = c_rarg4;  \/\/ position for writing to dest array\n@@ -5452,28 +5452,3 @@\n-    #define BASE64_ENCODE_SIMD_ROUND(in0, in1, in2, out0, out1, out2, out3, SZ) \\\n-      __ ld3(in0,  in1, in2, __ T##SZ##B, __ post(src, 3 * SZ));                \\\n-                                                                                \\\n-      __ ushr(v20, __ T##SZ##B, in0, 2);                                        \\\n-                                                                                \\\n-      __ ushr(v21, __ T##SZ##B, in1, 2);                                        \\\n-      __ shl(in0,  __ T##SZ##B, in0, 6);                                        \\\n-      __ orr(v21,  __ T##SZ##B, v21, in0);                                      \\\n-      __ ushr(v21, __ T##SZ##B, v21, 2);                                        \\\n-                                                                                \\\n-      __ ushr(v22, __ T##SZ##B, in2, 4);                                        \\\n-      __ shl(in1,  __ T##SZ##B, in1, 4);                                        \\\n-      __ orr(v22,  __ T##SZ##B, in1, v22);                                      \\\n-      __ ushr(v22, __ T##SZ##B, v22, 2);                                        \\\n-                                                                                \\\n-      __ shl(v23,  __ T##SZ##B, in2, 2);                                        \\\n-      __ ushr(v23, __ T##SZ##B, v23, 2);                                        \\\n-                                                                                \\\n-      __ tbl(out0, __ T##SZ##B, v0,  4, v20);                                   \\\n-      __ tbl(out1, __ T##SZ##B, v0,  4, v21);                                   \\\n-      __ tbl(out2, __ T##SZ##B, v0,  4, v22);                                   \\\n-      __ tbl(out3, __ T##SZ##B, v0,  4, v23);                                   \\\n-                                                                                \\\n-      __ st4(out0, out1, out2, out3, __ T##SZ##B, __ post(dst, 4 * SZ));        \\\n-\n-    __ add(src, src, sp);\n-    __ add(dst, dst, dp);\n-    __ sub(length, sl, sp);\n+    __ add(src, src, soff);\n+    __ add(dst, dst, doff);\n+    __ sub(length, send, soff);\n@@ -5497,1 +5472,21 @@\n-    BASE64_ENCODE_SIMD_ROUND(v4, v5, v6, v16, v17, v18, v19, 16);\n+    \/\/ load data\n+    __ ld3(v4,  v5, v6, __ T16B, __ post(src, 48));\n+    \/\/ calculate indices\n+    __ ushr(v20, __ T16B, v4,  2);\n+    __ ushr(v21, __ T16B, v5,  2);\n+    __ shl(v4,   __ T16B, v4,  6);\n+    __ orr(v21,  __ T16B, v21, v4);\n+    __ ushr(v21, __ T16B, v21, 2);\n+    __ ushr(v22, __ T16B, v6,  4);\n+    __ shl(v5,   __ T16B, v5,  4);\n+    __ orr(v22,  __ T16B, v5,  v22);\n+    __ ushr(v22, __ T16B, v22, 2);\n+    __ shl(v23,  __ T16B, v6,  2);\n+    __ ushr(v23, __ T16B, v23, 2);\n+    \/\/ lookup codec\n+    __ tbl(v16,  __ T16B, v0,  4, v20);\n+    __ tbl(v17,  __ T16B, v0,  4, v21);\n+    __ tbl(v18,  __ T16B, v0,  4, v22);\n+    __ tbl(v19,  __ T16B, v0,  4, v23);\n+    \/\/ store result\n+    __ st4(v16, v17, v18, v19, __ T16B, __ post(dst, 64));\n@@ -5504,1 +5499,21 @@\n-    BASE64_ENCODE_SIMD_ROUND(v4, v5, v6, v16, v17, v18, v19, 8);\n+    \/\/ load data\n+    __ ld3(v4,  v5, v6, __ T8B, __ post(src, 24));\n+    \/\/ calculate indices\n+    __ ushr(v20, __ T8B, v4,  2);\n+    __ ushr(v21, __ T8B, v5,  2);\n+    __ shl(v4,   __ T8B, v4,  6);\n+    __ orr(v21,  __ T8B, v21, v4);\n+    __ ushr(v21, __ T8B, v21, 2);\n+    __ ushr(v22, __ T8B, v6,  4);\n+    __ shl(v5,   __ T8B, v5,  4);\n+    __ orr(v22,  __ T8B, v5,  v22);\n+    __ ushr(v22, __ T8B, v22, 2);\n+    __ shl(v23,  __ T8B, v6,  2);\n+    __ ushr(v23, __ T8B, v23, 2);\n+    \/\/ lookup codec\n+    __ tbl(v16,  __ T8B, v0,  4, v20);\n+    __ tbl(v17,  __ T8B, v0,  4, v21);\n+    __ tbl(v18,  __ T8B, v0,  4, v22);\n+    __ tbl(v19,  __ T8B, v0,  4, v23);\n+    \/\/ store result\n+    __ st4(v16, v17, v18, v19, __ T8B, __ post(dst, 32));\n@@ -5506,1 +5521,0 @@\n-\n","filename":"src\/hotspot\/cpu\/aarch64\/stubGenerator_aarch64.cpp","additions":48,"deletions":34,"binary":false,"changes":82,"status":"modified"}]}