{"files":[{"patch":"@@ -2169,0 +2169,9 @@\n+size_t os::vm_min_address() {\n+  \/\/ On AIX, we need to make sure we don't block the sbrk. However, this is\n+  \/\/ done at actual reservation time, where we honor a \"no-mmap\" area following\n+  \/\/ the break. See MaxExpectedDataSegmentSize. So we can return a very low\n+  \/\/ address here.\n+  assert(is_aligned(_vm_min_address_default, os::vm_allocation_granularity()), \"Sanity\");\n+  return _vm_min_address_default;\n+}\n+\n","filename":"src\/hotspot\/os\/aix\/os_aix.cpp","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -1825,0 +1825,11 @@\n+size_t os::vm_min_address() {\n+#ifdef __APPLE__\n+  \/\/ On MacOS, the lowest 4G are denied to the application (see \"PAGEZERO\" resp.\n+  \/\/ -pagezero_size linker option).\n+  return 4 * G;\n+#else\n+  assert(is_aligned(_vm_min_address_default, os::vm_allocation_granularity()), \"Sanity\");\n+  return _vm_min_address_default;\n+#endif\n+}\n+\n","filename":"src\/hotspot\/os\/bsd\/os_bsd.cpp","additions":11,"deletions":0,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -73,0 +73,1 @@\n+#include \"utilities\/debug.hpp\"\n@@ -4247,0 +4248,17 @@\n+size_t os::vm_min_address() {\n+  \/\/ Determined by sysctl vm.mmap_min_addr. It exists as a safety zone to prevent\n+  \/\/ NULL pointer dereferences.\n+  \/\/ Most distros set this value to 64 KB. It *can* be zero, but rarely is. Here,\n+  \/\/ we impose a minimum value if vm.mmap_min_addr is too low, for increased protection.\n+  static size_t value = 0;\n+  if (value == 0) {\n+    assert(is_aligned(_vm_min_address_default, os::vm_allocation_granularity()), \"Sanity\");\n+    FILE* f = fopen(\"\/proc\/sys\/vm\/mmap_min_addr\", \"r\");\n+    if (fscanf(f, \"%zu\", &value) != 1) {\n+      value = _vm_min_address_default;\n+    }\n+    value = MAX2(_vm_min_address_default, value);\n+  }\n+  return value;\n+}\n+\n","filename":"src\/hotspot\/os\/linux\/os_linux.cpp","additions":18,"deletions":0,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -3438,0 +3438,5 @@\n+size_t os::vm_min_address() {\n+  assert(is_aligned(_vm_min_address_default, os::vm_allocation_granularity()), \"Sanity\");\n+  return _vm_min_address_default;\n+}\n+\n","filename":"src\/hotspot\/os\/windows\/os_windows.cpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -1331,2 +1331,6 @@\n-      \/\/ Reserve at any address, but leave it up to the platform to choose a good one.\n-      total_space_rs = Metaspace::reserve_address_space_for_compressed_classes(total_range_size);\n+      \/\/ We did not manage to reserve at the preferred address, or were instructed to relocate. In that\n+      \/\/ case we reserve whereever possible, but the start address needs to be encodable as narrow Klass\n+      \/\/ encoding base since the archived heap objects contain nKlass IDs precalculated toward the start\n+      \/\/ of the shared Metaspace. That prevents us from using zero-based encoding and therefore we won't\n+      \/\/ try allocating in low-address regions.\n+      total_space_rs = Metaspace::reserve_address_space_for_compressed_classes(total_range_size, false \/* try_in_low_address_ranges *\/);\n","filename":"src\/hotspot\/share\/cds\/metaspaceShared.cpp","additions":6,"deletions":2,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -62,0 +62,1 @@\n+#include \"virtualspace.hpp\"\n@@ -584,45 +585,18 @@\n-\/\/ Reserve a range of memory at an address suitable for en\/decoding narrow\n-\/\/ Klass pointers (see: CompressedClassPointers::is_valid_base()).\n-\/\/ The returned address shall both be suitable as a compressed class pointers\n-\/\/  base, and aligned to Metaspace::reserve_alignment (which is equal to or a\n-\/\/  multiple of allocation granularity).\n-\/\/ On error, returns an unreserved space.\n-ReservedSpace Metaspace::reserve_address_space_for_compressed_classes(size_t size) {\n-\n-#if defined(AARCH64) || defined(PPC64)\n-  const size_t alignment = Metaspace::reserve_alignment();\n-\n-  \/\/ AArch64: Try to align metaspace class space so that we can decode a\n-  \/\/ compressed klass with a single MOVK instruction. We can do this iff the\n-  \/\/ compressed class base is a multiple of 4G.\n-  \/\/ Additionally, above 32G, ensure the lower LogKlassAlignmentInBytes bits\n-  \/\/ of the upper 32-bits of the address are zero so we can handle a shift\n-  \/\/ when decoding.\n-\n-  \/\/ PPC64: smaller heaps up to 2g will be mapped just below 4g. Then the\n-  \/\/ attempt to place the compressed class space just after the heap fails on\n-  \/\/ Linux 4.1.42 and higher because the launcher is loaded at 4g\n-  \/\/ (ELF_ET_DYN_BASE). In that case we reach here and search the address space\n-  \/\/ below 32g to get a zerobased CCS. For simplicity we reuse the search\n-  \/\/ strategy for AARCH64.\n-\n-  static const struct {\n-    address from;\n-    address to;\n-    size_t increment;\n-  } search_ranges[] = {\n-    {  (address)(4*G),   (address)(32*G),   4*G, },\n-    {  (address)(32*G),  (address)(1024*G), (4 << LogKlassAlignmentInBytes) * G },\n-    {  nullptr, nullptr, 0 }\n-  };\n-\n-  \/\/ Calculate a list of all possible values for the starting address for the\n-  \/\/ compressed class space.\n-  ResourceMark rm;\n-  GrowableArray<address> list(36);\n-  for (int i = 0; search_ranges[i].from != nullptr; i ++) {\n-    address a = search_ranges[i].from;\n-    assert(CompressedKlassPointers::is_valid_base(a), \"Sanity\");\n-    while (a < search_ranges[i].to) {\n-      list.append(a);\n-      a +=  search_ranges[i].increment;\n+\/\/ Reserve a range of memory that is to contain narrow Klass IDs. If \"try_in_low_address_ranges\"\n+\/\/ is true, we will attempt to reserve memory suitable for zero-based encoding.\n+ReservedSpace Metaspace::reserve_address_space_for_compressed_classes(size_t size, bool try_in_low_address_ranges) {\n+\n+  char* result = nullptr;\n+  const bool randomize = RandomizeClassSpaceLocation;\n+\n+  \/\/ First try to reserve in low address ranges.\n+  if (try_in_low_address_ranges) {\n+    constexpr uintptr_t unscaled_max = ((uintptr_t)UINT_MAX + 1);\n+    log_debug(metaspace, map)(\"Trying below \" SIZE_FORMAT_X \" for unscaled narrow Klass encoding\", unscaled_max);\n+    result = os::attempt_reserve_memory_between(nullptr, (char*)unscaled_max,\n+                                                size, Metaspace::reserve_alignment(), randomize);\n+    if (result == nullptr) {\n+      constexpr uintptr_t zerobased_max = unscaled_max << LogKlassAlignmentInBytes;\n+      log_debug(metaspace, map)(\"Trying below \" SIZE_FORMAT_X \" for zero-based narrow Klass encoding\", zerobased_max);\n+      result = os::attempt_reserve_memory_between((char*)unscaled_max, (char*)zerobased_max,\n+                                                  size, Metaspace::reserve_alignment(), randomize);\n@@ -630,0 +604,26 @@\n+  } \/\/ end: low-address reservation\n+\n+#if defined(AARCH64) || defined(PPC64) || defined(S390)\n+  if (result == nullptr) {\n+    \/\/ Failing zero-based allocation, or in strict_base mode, try to come up with\n+    \/\/ an optimized start address that is amenable to JITs that use 16-bit moves to\n+    \/\/ load the encoding base as a short immediate.\n+    \/\/ Therefore we try here for an address that when right-shifted by\n+    \/\/ LogKlassAlignmentInBytes has only 1s in the third 16-bit quadrant.\n+    \/\/\n+    \/\/ Example: for shift=3, the address space searched would be\n+    \/\/ [0x0080_0000_0000 - 0xFFF8_0000_0000].\n+\n+    \/\/ Number of least significant bits that should be zero\n+    constexpr int lo_zero_bits = 32 + LogKlassAlignmentInBytes;\n+    \/\/ Number of most significant bits that should be zero\n+    constexpr int hi_zero_bits = 16;\n+\n+    constexpr size_t alignment = nth_bit(lo_zero_bits);\n+    assert(alignment >= Metaspace::reserve_alignment(), \"Sanity\");\n+    constexpr uint64_t min = alignment;\n+    constexpr uint64_t max = nth_bit(64 - hi_zero_bits);\n+\n+    log_debug(metaspace, map)(\"Trying between \" UINT64_FORMAT_X \" and \" UINT64_FORMAT_X\n+                              \" with \" SIZE_FORMAT_X \" alignment\", min, max, alignment);\n+    result = os::attempt_reserve_memory_between((char*)min, (char*)max, size, alignment, randomize);\n@@ -631,0 +631,1 @@\n+#endif \/\/ defined(AARCH64) || defined(PPC64) || defined(S390)\n@@ -632,10 +633,4 @@\n-  int len = list.length();\n-  int r = 0;\n-  if (!DumpSharedSpaces) {\n-    \/\/ Starting from a random position in the list. If the address cannot be reserved\n-    \/\/ (the OS already assigned it for something else), go to the next position, wrapping\n-    \/\/ around if necessary, until we exhaust all the items.\n-    os::init_random((int)os::javaTimeNanos());\n-    r = os::random();\n-    log_info(metaspace)(\"Randomizing compressed class space: start from %d out of %d locations\",\n-                        r % len, len);\n+  if (result == nullptr) {\n+    \/\/ Fallback: reserve anywhere and hope the resulting block is usable.\n+    log_debug(metaspace, map)(\"Trying anywhere...\");\n+    result = os::reserve_memory_aligned(size, Metaspace::reserve_alignment(), false);\n@@ -643,8 +638,9 @@\n-  for (int i = 0; i < len; i++) {\n-    address a = list.at((i + r) % len);\n-    ReservedSpace rs(size, Metaspace::reserve_alignment(),\n-                     os::vm_page_size(), (char*)a);\n-    if (rs.is_reserved()) {\n-      assert(a == (address)rs.base(), \"Sanity\");\n-      return rs;\n-    }\n+\n+  \/\/ Wrap resulting range in ReservedSpace\n+  ReservedSpace rs;\n+  if (result != nullptr) {\n+    assert(is_aligned(result, Metaspace::reserve_alignment()), \"Alignment too small for metaspace\");\n+    rs = ReservedSpace::space_for_range(result, size, Metaspace::reserve_alignment(),\n+                                                      os::vm_page_size(), false, false);\n+  } else {\n+    rs = ReservedSpace();\n@@ -652,10 +648,1 @@\n-#endif \/\/ defined(AARCH64) || defined(PPC64)\n-\n-#ifdef AARCH64\n-  \/\/ Note: on AARCH64, if the code above does not find any good placement, we\n-  \/\/ have no recourse. We return an empty space and the VM will exit.\n-  return ReservedSpace();\n-#else\n-  \/\/ Default implementation: Just reserve anywhere.\n-  return ReservedSpace(size, Metaspace::reserve_alignment(), os::vm_page_size(), (char*)nullptr);\n-#endif \/\/ AARCH64\n+  return rs;\n@@ -663,1 +650,0 @@\n-\n@@ -784,1 +770,0 @@\n-    address base = nullptr;\n@@ -791,1 +776,1 @@\n-      base = (address)CompressedClassSpaceBaseAddress;\n+      const address base = (address)CompressedClassSpaceBaseAddress;\n@@ -809,18 +794,0 @@\n-    if (!rs.is_reserved()) {\n-      \/\/ If UseCompressedOops=1 and the java heap has been placed in coops-friendly\n-      \/\/  territory, i.e. its base is under 32G, then we attempt to place ccs\n-      \/\/  right above the java heap.\n-      \/\/ Otherwise the lower 32G are still free. We try to place ccs at the lowest\n-      \/\/ allowed mapping address.\n-      base = (UseCompressedOops && (uint64_t)CompressedOops::base() < OopEncodingHeapMax) ?\n-              CompressedOops::end() : (address)HeapBaseMinAddress;\n-      base = align_up(base, Metaspace::reserve_alignment());\n-\n-      if (base != nullptr) {\n-        if (CompressedKlassPointers::is_valid_base(base)) {\n-          rs = ReservedSpace(size, Metaspace::reserve_alignment(),\n-                             os::vm_page_size(), (char*)base);\n-        }\n-      }\n-    }\n-\n@@ -829,1 +796,1 @@\n-      rs = Metaspace::reserve_address_space_for_compressed_classes(size);\n+      rs = Metaspace::reserve_address_space_for_compressed_classes(size, true);\n","filename":"src\/hotspot\/share\/memory\/metaspace.cpp","additions":62,"deletions":95,"binary":false,"changes":157,"status":"modified"},{"patch":"@@ -77,7 +77,3 @@\n-  \/\/ Reserve a range of memory at an address suitable for en\/decoding narrow\n-  \/\/ Klass pointers (see: CompressedClassPointers::is_valid_base()).\n-  \/\/ The returned address shall both be suitable as a compressed class pointers\n-  \/\/  base, and aligned to Metaspace::reserve_alignment (which is equal to or a\n-  \/\/  multiple of allocation granularity).\n-  \/\/ On error, returns an unreserved space.\n-  static ReservedSpace reserve_address_space_for_compressed_classes(size_t size);\n+  \/\/ Reserve a range of memory that is to contain narrow Klass IDs. If \"try_in_low_address_ranges\"\n+  \/\/ is true, we will attempt to reserve memory suitable for zero-based encoding.\n+  static ReservedSpace reserve_address_space_for_compressed_classes(size_t size, bool try_in_low_address_ranges);\n","filename":"src\/hotspot\/share\/memory\/metaspace.hpp","additions":3,"deletions":7,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -358,0 +358,11 @@\n+\/\/ Put a ReservedSpace over an existing range\n+ReservedSpace ReservedSpace::space_for_range(char* base, size_t size, size_t alignment,\n+                                             size_t page_size, bool special, bool executable) {\n+  assert(is_aligned(base, os::vm_allocation_granularity()), \"Unaligned base\");\n+  assert(is_aligned(size, os::vm_page_size()), \"Unaligned size\");\n+  assert(os::page_sizes().contains(page_size), \"Invalid pagesize\");\n+  ReservedSpace space;\n+  space.initialize_members(base, size, alignment, page_size, special, executable);\n+  return space;\n+}\n+\n@@ -549,2 +560,0 @@\n-    \/\/ But leave room for the compressed class pointers, which is allocated above\n-    \/\/ the heap.\n@@ -552,8 +561,0 @@\n-    const size_t class_space = align_up(CompressedClassSpaceSize, alignment);\n-    \/\/ For small heaps, save some space for compressed class pointer\n-    \/\/ space so it can be decoded with no base.\n-    if (UseCompressedClassPointers && !UseSharedSpaces && !DumpSharedSpaces &&\n-        OopEncodingHeapMax <= KlassEncodingMetaspaceMax &&\n-        (uint64_t)(aligned_heap_base_min_address + size + class_space) <= KlassEncodingMetaspaceMax) {\n-      zerobased_max = (char *)OopEncodingHeapMax - class_space;\n-    }\n","filename":"src\/hotspot\/share\/memory\/virtualspace.cpp","additions":11,"deletions":10,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -110,0 +110,4 @@\n+\n+  \/\/ Put a ReservedSpace over an existing range\n+  static ReservedSpace space_for_range(char* base, size_t size, size_t alignment,\n+                                       size_t page_size, bool special, bool executable);\n","filename":"src\/hotspot\/share\/memory\/virtualspace.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -65,1 +65,0 @@\n-  assert(is_valid_base(addr), \"Address must be a valid encoding base\");\n@@ -93,0 +92,2 @@\n+\n+  assert(is_valid_base(_base), \"Address must be a valid encoding base\");\n","filename":"src\/hotspot\/share\/oops\/compressedKlass.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -1422,0 +1422,3 @@\n+  develop(bool, RandomizeClassSpaceLocation, true,                          \\\n+          \"Randomize location of class space.\")                             \\\n+                                                                            \\\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -77,0 +77,1 @@\n+#include \"utilities\/fastrand.hpp\"\n@@ -1814,0 +1815,1 @@\n+    log_debug(os)(\"Reserved memory at \" INTPTR_FORMAT \" for \" SIZE_FORMAT \" bytes.\", p2i(addr), bytes);\n@@ -1818,0 +1820,188 @@\n+\n+  return result;\n+}\n+\n+#ifdef ASSERT\n+static void print_points(const char* s, unsigned* points, unsigned num) {\n+  stringStream ss;\n+  for (unsigned i = 0; i < num; i ++) {\n+    ss.print(\"%u \", points[i]);\n+  }\n+  log_trace(os, map)(\"%s, %u Points: %s\", s, num, ss.base());\n+}\n+#endif\n+\n+\/\/ Helper for os::attempt_reserve_memory_between\n+\/\/ Given an array of things, shuffle them (Fisher-Yates)\n+template <typename T>\n+static void shuffle_fisher_yates(T* arr, unsigned num, FastRandom& frand) {\n+  for (unsigned i = num - 1; i >= 1; i--) {\n+    unsigned j = frand.next() % i;\n+    swap(arr[i], arr[j]);\n+  }\n+}\n+\n+\/\/ Helper for os::attempt_reserve_memory_between\n+\/\/ Given an array of things, do a hemisphere split such that the resulting\n+\/\/ order is: [first, last, first + 1, last - 1, ...]\n+template <typename T>\n+static void hemi_split(T* arr, unsigned num) {\n+  T* tmp = (T*)::alloca(sizeof(T) * num);\n+  for (unsigned i = 0; i < num; i++) {\n+    tmp[i] = arr[i];\n+  }\n+  for (unsigned i = 0; i < num; i++) {\n+    arr[i] = is_even(i) ? tmp[i \/ 2] : tmp[num - (i \/ 2) - 1];\n+  }\n+}\n+\n+\/\/ Given an address range [min, max), attempts to reserve memory within this area, with the given alignment.\n+\/\/ If randomize is true, the location will be randomized.\n+char* os::attempt_reserve_memory_between(char* min, char* max, size_t bytes, size_t alignment, bool randomize) {\n+\n+  \/\/ Please keep the following constants in sync with the companion gtests:\n+\n+  \/\/ Number of mmap attemts we will undertake.\n+  constexpr unsigned max_attempts = 32;\n+\n+  \/\/ In randomization mode: We require a minimum number of possible attach points for\n+  \/\/ randomness. Below that we refuse to reserve anything.\n+  constexpr unsigned min_random_value_range = 16;\n+\n+  \/\/ In randomization mode: If the possible value range is below this threshold, we\n+  \/\/ use a total shuffle without regard for address space fragmentation, otherwise\n+  \/\/ we attempt to minimize fragmentation.\n+  constexpr unsigned total_shuffle_threshold = 1024;\n+\n+#define ARGSFMT \" range [\" PTR_FORMAT \"-\" PTR_FORMAT \"), size \" SIZE_FORMAT_X \", alignment \" SIZE_FORMAT_X \", randomize: %d\"\n+#define ARGSFMTARGS p2i(min), p2i(max), bytes, alignment, randomize\n+\n+  log_trace(os, map) (\"reserve_between (\" ARGSFMT \")\", ARGSFMTARGS);\n+\n+  assert(is_power_of_2(alignment), \"alignment invalid (\" ARGSFMT \")\", ARGSFMTARGS);\n+  assert(alignment < SIZE_MAX \/ 2, \"alignment too large (\" ARGSFMT \")\", ARGSFMTARGS);\n+  assert(is_aligned(bytes, os::vm_page_size()), \"size not page aligned (\" ARGSFMT \")\", ARGSFMTARGS);\n+  assert(max >= min, \"invalid range (\" ARGSFMT \")\", ARGSFMTARGS);\n+\n+  char* const absolute_max = (char*)(NOT_LP64(G * 3) LP64_ONLY(G * 128 * 1024));\n+  char* const absolute_min = (char*) os::vm_min_address();\n+\n+  const size_t alignment_adjusted = MAX2(alignment, os::vm_allocation_granularity());\n+\n+  \/\/ Calculate first and last possible attach points:\n+  char* const lo_att = align_up(MAX2(absolute_min, min), alignment_adjusted);\n+  if (lo_att == nullptr) {\n+    return nullptr; \/\/ overflow\n+  }\n+\n+  char* const hi_att = align_down(MIN2(max, absolute_max) - bytes, alignment_adjusted);\n+  if (hi_att > max) {\n+    return nullptr; \/\/ overflow\n+  }\n+\n+  \/\/ no possible attach points\n+  if (hi_att < lo_att) {\n+    return nullptr;\n+  }\n+\n+  char* result = nullptr;\n+\n+  const size_t num_attach_points = (size_t)((hi_att - lo_att) \/ alignment_adjusted) + 1;\n+  assert(num_attach_points > 0, \"Sanity\");\n+\n+  \/\/ If this fires, the input range is too large for the given alignment (we work\n+  \/\/ with int below to keep things simple). Since alignment is bound to page size,\n+  \/\/ and the lowest page size is 4K, this gives us a minimum of 4K*4G=8TB address\n+  \/\/ range.\n+  assert(num_attach_points <= UINT_MAX,\n+         \"Too many possible attach points - range too large or alignment too small (\" ARGSFMT \")\", ARGSFMTARGS);\n+\n+  const unsigned num_attempts = MIN2((unsigned)num_attach_points, max_attempts);\n+  unsigned points[max_attempts];\n+\n+  if (randomize) {\n+    FastRandom frand;\n+\n+    if (num_attach_points < min_random_value_range) {\n+      return nullptr;\n+    }\n+\n+    \/\/ We pre-calc the attach points:\n+    \/\/ 1 We divide the attach range into equidistant sections and calculate an attach\n+    \/\/   point within each section.\n+    \/\/ 2 We wiggle those attach points around within their section (depends on attach\n+    \/\/   point granularity)\n+    \/\/ 3 Should that not be enough to get effective randomization, shuffle all\n+    \/\/   attach points\n+    \/\/ 4 Otherwise, re-order them to get an optimized probing sequence.\n+    const unsigned stepsize = (unsigned)num_attach_points \/ num_attempts;\n+    const unsigned half = num_attempts \/ 2;\n+\n+    \/\/ 1+2: pre-calc points\n+    for (unsigned i = 0; i < num_attempts; i++) {\n+      const unsigned deviation = stepsize > 1 ? (frand.next() % stepsize) : 0;\n+      points[i] = (i * stepsize) + deviation;\n+    }\n+\n+    if (num_attach_points < total_shuffle_threshold) {\n+      \/\/ 3:\n+      \/\/ The numeber of possible attach points is too low for the \"wiggle\" from\n+      \/\/ point 2 to be enough to provide randomization. In that case, shuffle\n+      \/\/ all attach points at the cost of possible fragmentation (e.g. if we\n+      \/\/ end up mapping into the middle of the range).\n+      shuffle_fisher_yates(points, num_attempts, frand);\n+    } else {\n+      \/\/ 4\n+      \/\/ We have a large enough number of attach points to satisfy the randomness\n+      \/\/ goal without. In that case, we optimize probing by sorting the attach\n+      \/\/ points: We attempt outermost points first, then work ourselves up to\n+      \/\/ the middle. That reduces address space fragmentation. We also alternate\n+      \/\/ hemispheres, which increases the chance of successfull mappings if the\n+      \/\/ previous mapping had been blocked by large maps.\n+      hemi_split(points, num_attempts);\n+    }\n+  } \/\/ end: randomized\n+  else\n+  {\n+    \/\/ Non-randomized. We just attempt to reserve by probing sequentially. We\n+    \/\/ alternate between hemispheres, working ourselves up to the middle.\n+    const int stepsize = (unsigned)num_attach_points \/ num_attempts;\n+    for (unsigned i = 0; i < num_attempts; i++) {\n+      points[i] = (i * stepsize);\n+    }\n+    hemi_split(points, num_attempts);\n+  }\n+\n+#ifdef ASSERT\n+  \/\/ Print + check all pre-calculated attach points\n+  print_points(\"before reserve\", points, num_attempts);\n+  for (unsigned i = 0; i < num_attempts; i++) {\n+    assert(points[i] < num_attach_points, \"Candidate attach point %d out of range (%u, num_attach_points: %zu) \" ARGSFMT,\n+           i, points[i], num_attach_points, ARGSFMTARGS);\n+  }\n+#endif\n+\n+  \/\/ Now reserve\n+  for (unsigned i = 0; result == nullptr && i < num_attempts; i++) {\n+    const unsigned candidate_offset = points[i];\n+    char* const candidate = lo_att + candidate_offset * alignment_adjusted;\n+    assert(candidate <= hi_att, \"Invalid offset %u (\" ARGSFMT \")\", candidate_offset, ARGSFMTARGS);\n+    result = os::pd_attempt_reserve_memory_at(candidate, bytes, false);\n+    if (!result) {\n+      log_trace(os, map)(\"Failed to attach at \" PTR_FORMAT, p2i(candidate));\n+    }\n+  }\n+\n+  \/\/ Sanity checks, logging, NMT stuff:\n+  if (result != nullptr) {\n+#define ERRFMT \"result: \" PTR_FORMAT \" \" ARGSFMT\n+#define ERRFMTARGS p2i(result), ARGSFMTARGS\n+    assert(result >= min, \"OOB min (\" ERRFMT \")\", ERRFMTARGS);\n+    assert((result + bytes) <= max, \"OOB max (\" ERRFMT \")\", ERRFMTARGS);\n+    assert(result >= (char*)os::vm_min_address(), \"OOB vm.map min (\" ERRFMT \")\", ERRFMTARGS);\n+    assert((result + bytes) <= absolute_max, \"OOB vm.map max (\" ERRFMT \")\", ERRFMTARGS);\n+    assert(is_aligned(result, alignment), \"alignment invalid (\" ERRFMT \")\", ERRFMTARGS);\n+    log_trace(os, map)(ERRFMT, ERRFMTARGS);\n+    log_debug(os, map)(\"successfully attached at \" PTR_FORMAT, p2i(result));\n+    MemTracker::record_virtual_memory_reserve((address)result, bytes, CALLER_PC);\n+  }\n@@ -1819,0 +2009,4 @@\n+#undef ARGSFMT\n+#undef ERRFMT\n+#undef ARGSFMTARGS\n+#undef ERRFMTARGS\n","filename":"src\/hotspot\/share\/runtime\/os.cpp","additions":194,"deletions":0,"binary":false,"changes":194,"status":"modified"},{"patch":"@@ -193,0 +193,5 @@\n+  \/\/ The default value for os::vm_min_address() unless the platform knows better. This value\n+  \/\/ is chosen to give us reasonable protection against NULL pointer dereferences while being\n+  \/\/ low enough to leave most of the valuable low-4gb address space open.\n+  static constexpr size_t _vm_min_address_default = 16 * M;\n+\n@@ -423,0 +428,3 @@\n+  \/\/ Returns the lowest address the process is allowed to map against.\n+  static size_t vm_min_address();\n+\n@@ -435,0 +443,4 @@\n+  \/\/ Given an address range [min, max), attempts to reserve memory within this area, with the given alignment.\n+  \/\/ If randomize is true, the location will be randomized.\n+  static char* attempt_reserve_memory_between(char* min, char* max, size_t bytes, size_t alignment, bool randomize);\n+\n","filename":"src\/hotspot\/share\/runtime\/os.hpp","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -0,0 +1,47 @@\n+\/*\n+ * Copyright (c) 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2023, Red Hat, Inc. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_UTILITIES_FASTRAND_HPP\n+#define SHARE_UTILITIES_FASTRAND_HPP\n+\n+#include \"runtime\/os.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+\/\/ Simple utility class to generate random numbers for use in a single-threaded\n+\/\/ context. Since os::random() needs to update the global seed, this is faster\n+\/\/ when used on within a single thread.\n+\/\/ Seed initialization happens, similar to os::init_random(), via os::javaTimeNanos());\n+\n+class FastRandom {\n+  unsigned _seed;\n+  public:\n+  FastRandom () : _seed((unsigned) os::javaTimeNanos()) {}\n+  unsigned next() {\n+    _seed = os::next_random(_seed);\n+    return _seed;\n+  }\n+};\n+\n+#endif \/\/ SHARE_UTILITIES_FASTRAND_HPP\n","filename":"src\/hotspot\/share\/utilities\/fastrand.hpp","additions":47,"deletions":0,"binary":false,"changes":47,"status":"added"},{"patch":"@@ -950,0 +950,11 @@\n+\n+TEST_VM(os, vm_min_address) {\n+  size_t s = os::vm_min_address();\n+  ASSERT_GE(s, M);\n+  \/\/ Test upper limit. On Linux, its adjustable, so we just test for absurd values to prevent errors\n+  \/\/ with high vm.mmap_min_addr settings.\n+#if defined(_LP64)\n+  ASSERT_LE(s, NOT_LINUX(G * 4) LINUX_ONLY(G * 1024));\n+#endif\n+}\n+\n","filename":"test\/hotspot\/gtest\/runtime\/test_os.cpp","additions":11,"deletions":0,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -0,0 +1,346 @@\n+\/*\n+ * Copyright (c) 2023, Red Hat, Inc. All rights reserved.\n+ * Copyright (c) 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"logging\/log.hpp\"\n+#include \"memory\/resourceArea.hpp\"\n+#include \"runtime\/os.hpp\"\n+#include \"utilities\/align.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include \"utilities\/macros.hpp\"\n+#include \"utilities\/resourceHash.hpp\"\n+\n+#define LOG_PLEASE\n+#include \"testutils.hpp\"\n+#include \"unittest.hpp\"\n+\n+\/\/ On AIX, these tests make no sense as long as JDK-8315321 remains unfixed since the attach\n+\/\/ addresses are not predictable.\n+#ifndef AIX\n+\n+\/\/ Must be the same as in os::attempt_reserve_memory_between()\n+struct ARMB_constants {\n+  static constexpr uintptr_t absolute_max = NOT_LP64(G * 3) LP64_ONLY(G * 128 * 1024);\n+  static constexpr unsigned max_attempts = 32;\n+  static constexpr unsigned min_random_value_range = 16;\n+  static constexpr unsigned total_shuffle_threshold = 1024;\n+};\n+\n+\/\/ Testing os::attempt_reserve_memory_between()\n+\n+static void release_if_needed(char* p, size_t s) {\n+  if (p != nullptr) {\n+    os::release_memory(p, s);\n+  }\n+}\n+\n+#define ERRINFO \"addr: \" << ((void*)addr) << \" min: \" << ((void*)min) << \" max: \" << ((void*)max) \\\n+                 << \" bytes: \" << bytes << \" alignment: \" << alignment << \" randomized: \" << randomized\n+\n+static char* call_attempt_reserve_memory_between(char* min, char* max, size_t bytes, size_t alignment, bool randomized) {\n+  char* const  addr = os::attempt_reserve_memory_between(min, max, bytes, alignment, randomized);\n+  if (addr != nullptr) {\n+    EXPECT_TRUE(is_aligned(addr, alignment)) << ERRINFO;\n+    EXPECT_TRUE(is_aligned(addr, os::vm_allocation_granularity())) << ERRINFO;\n+    EXPECT_LE(addr, max - bytes) << ERRINFO;\n+    EXPECT_LE(addr, (char*)ARMB_constants::absolute_max - bytes) << ERRINFO;\n+    EXPECT_GE(addr, min) << ERRINFO;\n+    EXPECT_GE(addr, (char*)os::vm_min_address()) << ERRINFO;\n+  }\n+  return addr;\n+}\n+\n+class Expect {\n+  const bool _expect_success;\n+  const bool _expect_failure;\n+  const char* const _expected_result; \/\/ if _expect_success\n+public:\n+  Expect(bool expect_success, bool expect_failure, char* expected_result)\n+    : _expect_success(expect_success), _expect_failure(expect_failure), _expected_result(expected_result)\n+  {\n+    assert(!expect_success || !expect_failure, \"make up your mind\");\n+  }\n+  bool check_reality(char* result) const {\n+    if (_expect_failure) {\n+      return result == nullptr;\n+    }\n+    if (_expect_success) {\n+      return (_expected_result == nullptr) ? result != nullptr : result == _expected_result;\n+    }\n+    return true;\n+  }\n+  static Expect failure()           { return Expect(false, true, nullptr); }\n+  static Expect success_any()       { return Expect(true, false, nullptr); }\n+  static Expect success(char* addr) { return Expect(true, false, addr); }\n+  static Expect dontcare()          { return Expect(false, false, nullptr); }\n+};\n+\n+static void test_attempt_reserve_memory_between(char* min, char* max, size_t bytes, size_t alignment, bool randomized,\n+                                                Expect expectation, int line = -1) {\n+  char* const addr = call_attempt_reserve_memory_between(min, max, bytes, alignment, randomized);\n+  EXPECT_TRUE(expectation.check_reality(addr)) << ERRINFO << \" L\" << line;\n+  release_if_needed(addr, bytes);\n+}\n+#undef ERRINFO\n+\n+\/\/ Helper for attempt_reserve_memory_between tests to\n+\/\/ reserve an area with a hole in the middle\n+struct SpaceWithHole {\n+  char* _base;\n+  const size_t _len;\n+  const size_t _hole_offset;\n+  const size_t _hole_size;\n+\n+\n+  static constexpr size_t _p1_offset = 0;\n+  const size_t _p1_size;\n+  const size_t _p2_offset;\n+  const size_t _p2_size;\n+\n+  char* _p1;\n+  char* _p2;\n+\n+  size_t p1size() const { return hole_offset(); }\n+  size_t p2size() const { return _len - hole_size() - hole_offset(); }\n+\n+public:\n+\n+  char* base() const          { return _base; }\n+  char* end() const           { return _base + _len; }\n+  char* hole() const          { return _base + hole_offset(); }\n+  char* hole_end() const      { return hole() + hole_size(); }\n+\n+  size_t hole_size() const    { return _hole_size; }\n+  size_t hole_offset() const  { return _hole_offset; }\n+\n+  SpaceWithHole(size_t total_size, size_t hole_offset, size_t hole_size) :\n+    _base(nullptr), _len(total_size), _hole_offset(hole_offset), _hole_size(hole_size),\n+    _p1_size(hole_offset), _p2_offset(hole_offset + hole_size), _p2_size(total_size - hole_offset - hole_size),\n+    _p1(nullptr), _p2(nullptr)\n+  {\n+    assert(_p1_size > 0 && _p2_size > 0, \"Cannot have holes at the border\");\n+  }\n+\n+  bool reserve() {\n+    \/\/ We cannot create a hole by punching, since NMT cannot cope with releases\n+    \/\/ crossing reservation boundaries. Therefore we first reserve the total,\n+    \/\/ release it again, reserve the parts.\n+    for (int i = 56; _base == nullptr && i > 32; i--) {\n+      \/\/ We reserve at weird outlier addresses, in order to minimize the chance of concurrent mmaps grabbing\n+      \/\/ the hole.\n+      const uintptr_t candidate = nth_bit(i);\n+      if ((candidate + _len) <= ARMB_constants::absolute_max) {\n+        _base = os::attempt_reserve_memory_at((char*)candidate, _len);\n+      }\n+    }\n+    if (_base == nullptr) {\n+      return false;\n+    }\n+    \/\/ Release total mapping, remap the individual non-holy parts\n+    os::release_memory(_base, _len);\n+    _p1 = os::attempt_reserve_memory_at(_base + _p1_offset, _p1_size);\n+    _p2 = os::attempt_reserve_memory_at(_base + _p2_offset, _p2_size);\n+    if (_p1 == nullptr || _p2 == nullptr) {\n+      return false;\n+    }\n+    LOG_HERE(\"SpaceWithHole: [\" PTR_FORMAT \" ... [\" PTR_FORMAT \" ... \" PTR_FORMAT \") ... \" PTR_FORMAT \")\",\n+             p2i(base()), p2i(hole()), p2i(hole_end()), p2i(end()));\n+    return true;\n+  }\n+\n+  ~SpaceWithHole() {\n+    release_if_needed(_p1, _p1_size);\n+    release_if_needed(_p2, _p2_size);\n+  }\n+};\n+\n+\/\/ Test that, when reserving in a range randomly, we get random results\n+static void test_attempt_reserve_memory_between_random_distribution(unsigned num_possible_attach_points) {\n+\n+  const size_t ag = os::vm_allocation_granularity();\n+\n+  \/\/ Create a space that is mostly a hole bordered by two small stripes of reserved memory, with\n+  \/\/ as many attach points as we need.\n+  SpaceWithHole space((2 + num_possible_attach_points) * ag, ag, num_possible_attach_points * ag);\n+  if (!space.reserve()) {\n+    tty->print_cr(\"Failed to reserve holed space, skipping.\");\n+    return;\n+  }\n+\n+  const size_t bytes = ag;\n+  const size_t alignment = ag;\n+\n+  \/\/ Below this threshold the API should never return memory since the randomness is too weak.\n+  const bool expect_failure = (num_possible_attach_points < ARMB_constants::min_random_value_range);\n+\n+  \/\/ Below this threshold we expect values to be completely random, otherwise they randomized but still ordered.\n+  const bool total_shuffled = (num_possible_attach_points < ARMB_constants::total_shuffle_threshold);\n+\n+  \/\/ Allocate n times within that hole (with subsequent deletions) and remember unique addresses returned.\n+  constexpr unsigned num_tries_per_attach_point = 100;\n+  ResourceMark rm;\n+  ResourceHashtable<char*, unsigned> ht;\n+  const unsigned num_tries = expect_failure ? 3 : (num_possible_attach_points * num_tries_per_attach_point);\n+  unsigned num_uniq = 0; \/\/ Number of uniq addresses returned\n+\n+  \/\/ In \"total shuffle\" mode, all possible attach points are randomized; outside that mode, the API\n+  \/\/ attempts to limit fragmentation by favouring the ends of the ranges.\n+  const unsigned expected_variance =\n+    total_shuffled ? num_possible_attach_points : (num_possible_attach_points \/ ARMB_constants::max_attempts);\n+\n+  \/\/ Its not easy to find a good threshold for automated tests to test randomness\n+  \/\/ that rules out intermittent errors. We apply a generous fudge factor.\n+  constexpr double fudge_factor = 0.25f;\n+  const unsigned expected_variance_with_fudge = MAX2(2u, (unsigned)((double)expected_variance * fudge_factor));\n+\n+#define ERRINFO \" num_possible_attach_points: \" << num_possible_attach_points << \" total_shuffle? \" << total_shuffled \\\n+                << \" expected variance: \" << expected_variance << \" with fudge: \" << expected_variance_with_fudge \\\n+                << \" alignment: \" << alignment << \" bytes: \" << bytes;\n+\n+  for (unsigned i = 0; i < num_tries &&\n+       num_uniq < expected_variance_with_fudge; \/\/ Stop early if we confirmed enough variance.\n+       i ++) {\n+    char* p = call_attempt_reserve_memory_between(space.base(), space.end(), bytes, alignment, true);\n+    if (p != nullptr) {\n+      ASSERT_GE(p, space.hole()) << ERRINFO;\n+      ASSERT_LE(p + bytes, space.hole_end()) << ERRINFO;\n+      release_if_needed(p, bytes);\n+      bool created = false;\n+      unsigned* num = ht.put_if_absent(p, 0, &created);\n+      (*num) ++;\n+      num_uniq = (unsigned)ht.number_of_entries();\n+    }\n+  }\n+\n+  ASSERT_LE(num_uniq, num_possible_attach_points) << num_uniq << ERRINFO;\n+\n+  if (!expect_failure) {\n+    ASSERT_GE(num_uniq, expected_variance_with_fudge) << ERRINFO;\n+  }\n+#undef ERRINFO\n+}\n+\n+#define RANDOMIZED_RANGE_TEST(num) \\\n+  TEST_VM(os, attempt_reserve_memory_between_random_distribution_ ## num ## _attach_points) { \\\n+    test_attempt_reserve_memory_between_random_distribution(num); \\\n+}\n+\n+RANDOMIZED_RANGE_TEST(2)\n+RANDOMIZED_RANGE_TEST(15)\n+RANDOMIZED_RANGE_TEST(16)\n+RANDOMIZED_RANGE_TEST(712)\n+RANDOMIZED_RANGE_TEST(12000)\n+\n+\/\/ Test that, given a smallish range - not many attach points - with a hole, we attach within that hole.\n+TEST_VM(os, attempt_reserve_memory_randomization_threshold) {\n+\n+  constexpr int threshold = ARMB_constants::min_random_value_range;\n+  const size_t ps = os::vm_page_size();\n+  const size_t ag = os::vm_allocation_granularity();\n+\n+  SpaceWithHole space(ag * (threshold + 2), ag, ag * threshold);\n+  if (!space.reserve()) {\n+    tty->print_cr(\"Failed to reserve holed space, skipping.\");\n+    return;\n+  }\n+\n+  \/\/ Test with a range that only allows for (threshold - 1) reservations\n+  test_attempt_reserve_memory_between(space.hole(), space.hole_end() - ag, ps, ag, true, Expect::failure());\n+\n+  \/\/ Test with a range just above the threshold. Should succeed.\n+  test_attempt_reserve_memory_between(space.hole(), space.hole_end(), ps, ag, true, Expect::success_any());\n+}\n+\n+\/\/ Test all possible combos\n+TEST_VM(os, attempt_reserve_memory_between_combos) {\n+  const size_t large_end = NOT_LP64(G) LP64_ONLY(64 * G);\n+  for (size_t range_size = os::vm_allocation_granularity(); range_size <= large_end; range_size *= 2) {\n+    for (size_t start_offset = 0; start_offset <= large_end; start_offset += (large_end \/ 2)) {\n+      char* const min = (char*)(uintptr_t)start_offset;\n+      char* const max = min + range_size;\n+      for (size_t bytes = os::vm_page_size(); bytes < large_end; bytes *= 2) {\n+        for (size_t alignment = os::vm_allocation_granularity(); alignment < large_end; alignment *= 2) {\n+          test_attempt_reserve_memory_between(min, max, bytes, alignment, true, Expect::dontcare(), __LINE__);\n+          test_attempt_reserve_memory_between(min, max, bytes, alignment, false, Expect::dontcare(), __LINE__);\n+        }\n+      }\n+    }\n+  }\n+}\n+\n+TEST_VM(os, attempt_reserve_memory_randomization_cornercases) {\n+  const size_t ps = os::vm_page_size();\n+  const size_t ag = os::vm_allocation_granularity();\n+  constexpr size_t quarter_address_space = NOT_LP64(nth_bit(30)) LP64_ONLY(nth_bit(62));\n+\n+  \/\/ Zero-sized range\n+  test_attempt_reserve_memory_between(nullptr, nullptr, ps, ag, false, Expect::failure());\n+  test_attempt_reserve_memory_between((char*)(3 * G), (char*)(3 * G), ps, ag, false, Expect::dontcare(), __LINE__);\n+  test_attempt_reserve_memory_between((char*)SIZE_MAX, (char*)SIZE_MAX, ps, ag, false, Expect::failure(), __LINE__);\n+\n+  test_attempt_reserve_memory_between(nullptr, nullptr, ps, ag, true, Expect::failure());\n+  test_attempt_reserve_memory_between((char*)(3 * G), (char*)(3 * G), ps, ag, true, Expect::dontcare(), __LINE__);\n+  test_attempt_reserve_memory_between((char*)(3 * G), (char*)(3 * G), ps, ag, true, Expect::dontcare(), __LINE__);\n+  test_attempt_reserve_memory_between((char*)SIZE_MAX, (char*)SIZE_MAX, ps, ag, true, Expect::failure(), __LINE__);\n+\n+  \/\/ Full size\n+  \/\/ Note: paradoxically, success is not guaranteed here, since a significant portion of the attach points\n+  \/\/ could be located in un-allocatable territory.\n+  test_attempt_reserve_memory_between(nullptr, (char*)SIZE_MAX, ps, quarter_address_space \/ 8, false, Expect::dontcare(), __LINE__);\n+  test_attempt_reserve_memory_between(nullptr, (char*)SIZE_MAX, ps, quarter_address_space \/ 8, true, Expect::dontcare(), __LINE__);\n+\n+  \/\/ Very small range at start\n+  test_attempt_reserve_memory_between(nullptr, (char*)ag, ps, ag, false, Expect::dontcare(), __LINE__);\n+  test_attempt_reserve_memory_between(nullptr, (char*)ag, ps, ag, true, Expect::dontcare(), __LINE__);\n+\n+  \/\/ Very small range at end\n+  test_attempt_reserve_memory_between((char*)(SIZE_MAX - (ag * 2)), (char*)(SIZE_MAX), ps, ag, false, Expect::dontcare(), __LINE__);\n+  test_attempt_reserve_memory_between((char*)(SIZE_MAX - (ag * 2)), (char*)(SIZE_MAX), ps, ag, true, Expect::dontcare(), __LINE__);\n+\n+  \/\/ At start, high alignment, check if we run into neg. overflow problems\n+  test_attempt_reserve_memory_between(nullptr, (char*)G, ps, G, false, Expect::dontcare(), __LINE__);\n+  test_attempt_reserve_memory_between(nullptr, (char*)G, ps, G, true, Expect::dontcare(), __LINE__);\n+\n+  \/\/ At start, very high alignment, check if we run into neg. overflow problems\n+  test_attempt_reserve_memory_between((char*)quarter_address_space, (char*)SIZE_MAX, ps, quarter_address_space, false, Expect::dontcare(), __LINE__);\n+  test_attempt_reserve_memory_between((char*)quarter_address_space, (char*)SIZE_MAX, ps, quarter_address_space, true, Expect::dontcare(), __LINE__);\n+}\n+\n+\/\/ Test that, regardless where the hole is in the [min, max) range, if we probe nonrandomly, we will fill that hole\n+\/\/ as long as the range size is smaller than the number of probe attempts\n+TEST_VM(os, attempt_reserve_memory_between_small_range_fill_hole) {\n+  const size_t ps = os::vm_page_size();\n+  const size_t ag = os::vm_allocation_granularity();\n+  constexpr int num = ARMB_constants::max_attempts;\n+  for (int i = 0; i < num; i ++) {\n+    SpaceWithHole space(ag * (num + 2), ag * (i + 1), ag);\n+    if (!space.reserve()) {\n+      tty->print_cr(\"Failed to reserve holed space, skipping.\");\n+    } else {\n+      test_attempt_reserve_memory_between(space.base() + ag, space.end() - ag, space.hole_size(), space.hole_size(), false, Expect::success(space.hole()), __LINE__);\n+    }\n+  }\n+}\n+\n+#endif \/\/ AIX\n","filename":"test\/hotspot\/gtest\/runtime\/test_os_reserve_between.cpp","additions":346,"deletions":0,"binary":false,"changes":346,"status":"added"},{"patch":"@@ -71,0 +71,3 @@\n+\/\/ handy for error analysis\n+#define PING { printf(\"%s:%d\\n\", __FILE__, __LINE__); fflush(stdout); }\n+\n","filename":"test\/hotspot\/gtest\/testutils.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"}]}