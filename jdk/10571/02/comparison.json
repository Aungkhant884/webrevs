{"files":[{"patch":"@@ -7222,1 +7222,1 @@\n-void Assembler::vpmullq(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {\n+void Assembler::evpmullq(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {\n@@ -7257,1 +7257,1 @@\n-void Assembler::vpmullq(XMMRegister dst, XMMRegister nds, Address src, int vector_len) {\n+void Assembler::evpmullq(XMMRegister dst, XMMRegister nds, Address src, int vector_len) {\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2522,1 +2522,1 @@\n-  void vpmullq(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len);\n+  void evpmullq(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len);\n@@ -2526,1 +2526,1 @@\n-  void vpmullq(XMMRegister dst, XMMRegister nds, Address src, int vector_len);\n+  void evpmullq(XMMRegister dst, XMMRegister nds, Address src, int vector_len);\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1760,1 +1760,1 @@\n-                            vpmullq(dst, dst, src, vector_len); break;\n+                            evpmullq(dst, dst, src, vector_len); break;\n@@ -1808,1 +1808,1 @@\n-    case Op_MulReductionVL: vpmullq(dst, src1, src2, vector_len); break;\n+    case Op_MulReductionVL: evpmullq(dst, src1, src2, vector_len); break;\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -194,0 +194,4 @@\n+      case Op_MulVB:\n+        return 7;\n+      case Op_MulVL:\n+        return VM_Version::supports_avx512vldq() ? 0 : 6;\n@@ -195,1 +199,1 @@\n-      case Op_VectorCastD2X: {\n+      case Op_VectorCastD2X:\n@@ -197,1 +201,0 @@\n-      }\n@@ -213,1 +216,1 @@\n-      case Op_RoundVD: {\n+      case Op_RoundVD:\n@@ -215,1 +218,0 @@\n-      }\n","filename":"src\/hotspot\/cpu\/x86\/matcher_x86.hpp","additions":6,"deletions":4,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -1737,1 +1737,0 @@\n-    case Op_MulVL:\n@@ -5643,3 +5642,2 @@\n-instruct mulB_reg(vec dst, vec src1, vec src2, vec tmp) %{\n-  predicate(Matcher::vector_length(n) == 4 ||\n-            Matcher::vector_length(n) == 8);\n+instruct vmul8B(vec dst, vec src1, vec src2, vec xtmp) %{\n+  predicate(Matcher::vector_length_in_bytes(n) <= 8);\n@@ -5647,2 +5645,2 @@\n-  effect(TEMP dst, TEMP tmp);\n-  format %{\"vector_mulB $dst,$src1,$src2\" %}\n+  effect(TEMP dst, TEMP xtmp);\n+  format %{ \"mulVB   $dst, $src1, $src2\\t! using $xtmp as TEMP\" %}\n@@ -5651,5 +5649,5 @@\n-    __ pmovsxbw($tmp$$XMMRegister, $src1$$XMMRegister);\n-    __ pmovsxbw($dst$$XMMRegister, $src2$$XMMRegister);\n-    __ pmullw($tmp$$XMMRegister, $dst$$XMMRegister);\n-    __ movdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), noreg);\n-    __ pand($dst$$XMMRegister, $tmp$$XMMRegister);\n+    __ pmovsxbw($dst$$XMMRegister, $src1$$XMMRegister);\n+    __ pmovsxbw($xtmp$$XMMRegister, $src2$$XMMRegister);\n+    __ pmullw($dst$$XMMRegister, $xtmp$$XMMRegister);\n+    __ psllw($dst$$XMMRegister, 8);\n+    __ psrlw($dst$$XMMRegister, 8);\n@@ -5661,2 +5659,2 @@\n-instruct mul16B_reg(vec dst, vec src1, vec src2, vec tmp1, vec tmp2) %{\n-  predicate(Matcher::vector_length(n) == 16 && UseAVX <= 1);\n+instruct vmulB(vec dst, vec src1, vec src2, vec xtmp) %{\n+  predicate(UseAVX == 0 && Matcher::vector_length_in_bytes(n) > 8);\n@@ -5664,2 +5662,2 @@\n-  effect(TEMP dst, TEMP tmp1, TEMP tmp2);\n-  format %{\"vector_mulB $dst,$src1,$src2\" %}\n+  effect(TEMP dst, TEMP xtmp);\n+  format %{ \"mulVB   $dst, $src1, $src2\\t! using $xtmp as TEMP\" %}\n@@ -5668,36 +5666,20 @@\n-    __ pmovsxbw($tmp1$$XMMRegister, $src1$$XMMRegister);\n-    __ pmovsxbw($tmp2$$XMMRegister, $src2$$XMMRegister);\n-    __ pmullw($tmp1$$XMMRegister, $tmp2$$XMMRegister);\n-    __ pshufd($tmp2$$XMMRegister, $src1$$XMMRegister, 0xEE);\n-    __ pshufd($dst$$XMMRegister, $src2$$XMMRegister, 0xEE);\n-    __ pmovsxbw($tmp2$$XMMRegister, $tmp2$$XMMRegister);\n-    __ pmovsxbw($dst$$XMMRegister, $dst$$XMMRegister);\n-    __ pmullw($tmp2$$XMMRegister, $dst$$XMMRegister);\n-    __ movdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), noreg);\n-    __ pand($tmp2$$XMMRegister, $dst$$XMMRegister);\n-    __ pand($dst$$XMMRegister, $tmp1$$XMMRegister);\n-    __ packuswb($dst$$XMMRegister, $tmp2$$XMMRegister);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct vmul16B_reg_avx(vec dst, vec src1, vec src2, vec tmp) %{\n-  predicate(Matcher::vector_length(n) == 16 && UseAVX > 1);\n-  match(Set dst (MulVB src1 src2));\n-  effect(TEMP dst, TEMP tmp);\n-  format %{\"vector_mulB $dst,$src1,$src2\" %}\n-  ins_encode %{\n-  int vlen_enc = Assembler::AVX_256bit;\n-    __ vpmovsxbw($tmp$$XMMRegister, $src1$$XMMRegister, vlen_enc);\n-    __ vpmovsxbw($dst$$XMMRegister, $src2$$XMMRegister, vlen_enc);\n-    __ vpmullw($tmp$$XMMRegister, $tmp$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n-    __ vmovdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), noreg);\n-    __ vpand($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister, vlen_enc);\n-    __ vextracti128_high($tmp$$XMMRegister, $dst$$XMMRegister);\n-    __ vpackuswb($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister, 0);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct vmul32B_reg_avx(vec dst, vec src1, vec src2, vec tmp1, vec tmp2) %{\n-  predicate(Matcher::vector_length(n) == 32);\n+    \/\/ Odd-index elements\n+    __ movdqu($dst$$XMMRegister, $src1$$XMMRegister);\n+    __ psrlw($dst$$XMMRegister, 8);\n+    __ movdqu($xtmp$$XMMRegister, $src2$$XMMRegister);\n+    __ psrlw($xtmp$$XMMRegister, 8);\n+    __ pmullw($dst$$XMMRegister, $xtmp$$XMMRegister);\n+    __ psllw($dst$$XMMRegister, 8);\n+    \/\/ Even-index elements\n+    __ movdqu($xtmp$$XMMRegister, $src1$$XMMRegister);\n+    __ pmullw($xtmp$$XMMRegister, $src2$$XMMRegister);\n+    __ psllw($xtmp$$XMMRegister, 8);\n+    __ psrlw($xtmp$$XMMRegister, 8);\n+    \/\/ Combine\n+    __ por($dst$$XMMRegister, $xtmp$$XMMRegister);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vmulB_reg(vec dst, vec src1, vec src2, vec xtmp1, vec xtmp2) %{\n+  predicate(UseAVX > 0 && Matcher::vector_length_in_bytes(n) > 8);\n@@ -5705,2 +5687,2 @@\n-  effect(TEMP dst, TEMP tmp1, TEMP tmp2);\n-  format %{\"vector_mulB $dst,$src1,$src2\" %}\n+  effect(TEMP xtmp1, TEMP xtmp2);\n+  format %{ \"vmulVB  $dst, $src1, $src2\\t! using $xtmp1, $xtmp2 as TEMP\" %}\n@@ -5708,43 +5690,12 @@\n-    assert(UseAVX > 1, \"required\");\n-    int vlen_enc = Assembler::AVX_256bit;\n-    __ vextracti128_high($tmp1$$XMMRegister, $src1$$XMMRegister);\n-    __ vextracti128_high($dst$$XMMRegister, $src2$$XMMRegister);\n-    __ vpmovsxbw($tmp1$$XMMRegister, $tmp1$$XMMRegister, vlen_enc);\n-    __ vpmovsxbw($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n-    __ vpmullw($tmp1$$XMMRegister, $tmp1$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n-    __ vpmovsxbw($tmp2$$XMMRegister, $src1$$XMMRegister, vlen_enc);\n-    __ vpmovsxbw($dst$$XMMRegister, $src2$$XMMRegister, vlen_enc);\n-    __ vpmullw($tmp2$$XMMRegister, $tmp2$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n-    __ vmovdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), noreg);\n-    __ vpbroadcastd($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n-    __ vpand($tmp1$$XMMRegister, $tmp1$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n-    __ vpand($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister, vlen_enc);\n-    __ vpackuswb($dst$$XMMRegister, $dst$$XMMRegister, $tmp1$$XMMRegister, vlen_enc);\n-    __ vpermq($dst$$XMMRegister, $dst$$XMMRegister, 0xD8, vlen_enc);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct vmul64B_reg_avx(vec dst, vec src1, vec src2, vec tmp1, vec tmp2) %{\n-  predicate(Matcher::vector_length(n) == 64);\n-  match(Set dst (MulVB src1 src2));\n-  effect(TEMP dst, TEMP tmp1, TEMP tmp2);\n-  format %{\"vector_mulB $dst,$src1,$src2\\n\\t\" %}\n-  ins_encode %{\n-    assert(UseAVX > 2, \"required\");\n-    int vlen_enc = Assembler::AVX_512bit;\n-    __ vextracti64x4_high($tmp1$$XMMRegister, $src1$$XMMRegister);\n-    __ vextracti64x4_high($dst$$XMMRegister, $src2$$XMMRegister);\n-    __ vpmovsxbw($tmp1$$XMMRegister, $tmp1$$XMMRegister, vlen_enc);\n-    __ vpmovsxbw($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n-    __ vpmullw($tmp1$$XMMRegister, $tmp1$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n-    __ vpmovsxbw($tmp2$$XMMRegister, $src1$$XMMRegister, vlen_enc);\n-    __ vpmovsxbw($dst$$XMMRegister, $src2$$XMMRegister, vlen_enc);\n-    __ vpmullw($tmp2$$XMMRegister, $tmp2$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n-    __ vmovdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), noreg);\n-    __ vpbroadcastd($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n-    __ vpand($tmp1$$XMMRegister, $tmp1$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n-    __ vpand($tmp2$$XMMRegister, $tmp2$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n-    __ vpackuswb($dst$$XMMRegister, $tmp1$$XMMRegister, $tmp2$$XMMRegister, vlen_enc);\n-    __ evmovdquq($tmp2$$XMMRegister, ExternalAddress(vector_byte_perm_mask()), vlen_enc, noreg);\n-    __ vpermq($dst$$XMMRegister, $tmp2$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n+    int vlen_enc = vector_length_encoding(this);\n+    \/\/ Odd-index elements\n+    __ vpsrlw($xtmp2$$XMMRegister, $src1$$XMMRegister, 8, vlen_enc);\n+    __ vpsrlw($xtmp1$$XMMRegister, $src2$$XMMRegister, 8, vlen_enc);\n+    __ vpmullw($xtmp2$$XMMRegister, $xtmp1$$XMMRegister, $xtmp2$$XMMRegister, vlen_enc);\n+    __ vpsllw($xtmp2$$XMMRegister, $xtmp2$$XMMRegister, 8, vlen_enc);\n+    \/\/ Even-index elements\n+    __ vpmullw($xtmp1$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vlen_enc);\n+    __ vpsllw($xtmp1$$XMMRegister, $xtmp1$$XMMRegister, 8, vlen_enc);\n+    __ vpsrlw($xtmp1$$XMMRegister, $xtmp1$$XMMRegister, 8, vlen_enc);\n+    \/\/ Combine\n+    __ vpor($dst$$XMMRegister, $xtmp1$$XMMRegister, $xtmp2$$XMMRegister, vlen_enc);\n@@ -5759,1 +5710,1 @@\n-  format %{ \"pmullw $dst,$src\\t! mul packedS\" %}\n+  format %{ \"pmullw  $dst,$src\\t! mul packedS\" %}\n@@ -5825,2 +5776,4 @@\n-instruct vmulL_reg(vec dst, vec src1, vec src2) %{\n-  predicate(VM_Version::supports_avx512dq());\n+instruct evmulL_reg(vec dst, vec src1, vec src2) %{\n+  predicate((Matcher::vector_length_in_bytes(n) == 64 &&\n+             VM_Version::supports_avx512dq()) ||\n+            VM_Version::supports_avx512vldq());\n@@ -5828,1 +5781,1 @@\n-  format %{ \"vpmullq $dst,$src1,$src2\\t! mul packedL\" %}\n+  format %{ \"evpmullq $dst,$src1,$src2\\t! mul packedL\" %}\n@@ -5832,1 +5785,1 @@\n-    __ vpmullq($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vlen_enc);\n+    __ evpmullq($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vlen_enc);\n@@ -5837,3 +5790,5 @@\n-instruct vmulL_mem(vec dst, vec src, memory mem) %{\n-  predicate(VM_Version::supports_avx512dq() &&\n-              (Matcher::vector_length_in_bytes(n->in(1)) > 8));\n+instruct evmulL_mem(vec dst, vec src, memory mem) %{\n+  predicate((Matcher::vector_length_in_bytes(n) == 64 &&\n+             VM_Version::supports_avx512dq()) ||\n+            (Matcher::vector_length_in_bytes(n) > 8 &&\n+             VM_Version::supports_avx512vldq()));\n@@ -5841,1 +5796,1 @@\n-  format %{ \"vpmullq $dst,$src,$mem\\t! mul packedL\" %}\n+  format %{ \"evpmullq $dst,$src,$mem\\t! mul packedL\" %}\n@@ -5845,1 +5800,1 @@\n-    __ vpmullq($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vlen_enc);\n+    __ evpmullq($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vlen_enc);\n@@ -5850,12 +5805,5 @@\n-instruct mul2L_reg(vec dst, vec src2, legVec tmp) %{\n-  predicate(Matcher::vector_length(n) == 2 && !VM_Version::supports_avx512dq());\n-  match(Set dst (MulVL dst src2));\n-  effect(TEMP dst, TEMP tmp);\n-  format %{ \"pshufd $tmp,$src2, 177\\n\\t\"\n-            \"pmulld $tmp,$dst\\n\\t\"\n-            \"phaddd $tmp,$tmp\\n\\t\"\n-            \"pmovzxdq $tmp,$tmp\\n\\t\"\n-            \"psllq $tmp, 32\\n\\t\"\n-            \"pmuludq $dst,$src2\\n\\t\"\n-            \"paddq $dst,$tmp\\n\\t! mul packed2L\" %}\n-\n+instruct vmulL(vec dst, vec src1, vec src2, vec xtmp) %{\n+  predicate(UseAVX == 0);\n+  match(Set dst (MulVL src1 src2));\n+  effect(TEMP dst, TEMP xtmp);\n+  format %{ \"mulVL   $dst, $src1, $src2\\t! using $xtmp as TEMP\" %}\n@@ -5864,8 +5812,10 @@\n-    int vlen_enc = Assembler::AVX_128bit;\n-    __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 177);\n-    __ pmulld($tmp$$XMMRegister, $dst$$XMMRegister);\n-    __ phaddd($tmp$$XMMRegister, $tmp$$XMMRegister);\n-    __ pmovzxdq($tmp$$XMMRegister, $tmp$$XMMRegister);\n-    __ psllq($tmp$$XMMRegister, 32);\n-    __ pmuludq($dst$$XMMRegister, $src2$$XMMRegister);\n-    __ paddq($dst$$XMMRegister, $tmp$$XMMRegister);\n+    \/\/ Get the lo-hi products, only the lower 32 bits is in concerns\n+    __ pshufd($xtmp$$XMMRegister, $src2$$XMMRegister, 0xB1);\n+    __ pmulld($xtmp$$XMMRegister, $src1$$XMMRegister);\n+    __ pshufd($dst$$XMMRegister, $xtmp$$XMMRegister, 0xB1);\n+    __ paddd($dst$$XMMRegister, $xtmp$$XMMRegister);\n+    __ psllq($dst$$XMMRegister, 32);\n+    \/\/ Get the lo-lo products\n+    __ movdqu($xtmp$$XMMRegister, $src1$$XMMRegister);\n+    __ pmuludq($xtmp$$XMMRegister, $src2$$XMMRegister);\n+    __ paddq($dst$$XMMRegister, $xtmp$$XMMRegister);\n@@ -5876,2 +5826,6 @@\n-instruct vmul4L_reg_avx(vec dst, vec src1, vec src2, legVec tmp, legVec tmp1) %{\n-  predicate(Matcher::vector_length(n) == 4 && !VM_Version::supports_avx512dq());\n+instruct vmulL_reg(vec dst, vec src1, vec src2, vec xtmp1, vec xtmp2) %{\n+  predicate(UseAVX > 0 &&\n+            ((Matcher::vector_length_in_bytes(n) == 64 &&\n+              !VM_Version::supports_avx512dq()) ||\n+             (Matcher::vector_length_in_bytes(n) < 64 &&\n+              !VM_Version::supports_avx512vldq())));\n@@ -5879,8 +5833,2 @@\n-  effect(TEMP tmp1, TEMP tmp);\n-  format %{ \"vpshufd $tmp,$src2\\n\\t\"\n-            \"vpmulld $tmp,$src1,$tmp\\n\\t\"\n-            \"vphaddd $tmp,$tmp,$tmp\\n\\t\"\n-            \"vpmovzxdq $tmp,$tmp\\n\\t\"\n-            \"vpsllq $tmp,$tmp\\n\\t\"\n-            \"vpmuludq $tmp1,$src1,$src2\\n\\t\"\n-            \"vpaddq $dst,$tmp,$tmp1\\t! mul packed4L\" %}\n+  effect(TEMP xtmp1, TEMP xtmp2);\n+  format %{ \"vmulVL  $dst, $src1, $src2\\t! using $xtmp1, $xtmp2 as TEMP\" %}\n@@ -5888,9 +5836,10 @@\n-    int vlen_enc = Assembler::AVX_256bit;\n-    __ vpshufd($tmp$$XMMRegister, $src2$$XMMRegister, 177, vlen_enc);\n-    __ vpmulld($tmp$$XMMRegister, $src1$$XMMRegister, $tmp$$XMMRegister, vlen_enc);\n-    __ vextracti128_high($tmp1$$XMMRegister, $tmp$$XMMRegister);\n-    __ vphaddd($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp1$$XMMRegister, vlen_enc);\n-    __ vpmovzxdq($tmp$$XMMRegister, $tmp$$XMMRegister, vlen_enc);\n-    __ vpsllq($tmp$$XMMRegister, $tmp$$XMMRegister, 32, vlen_enc);\n-    __ vpmuludq($tmp1$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vlen_enc);\n-    __ vpaddq($dst$$XMMRegister, $tmp$$XMMRegister, $tmp1$$XMMRegister, vlen_enc);\n+    int vlen_enc = vector_length_encoding(this);\n+    \/\/ Get the lo-hi products, only the lower 32 bits is in concerns\n+    __ vpshufd($xtmp1$$XMMRegister, $src2$$XMMRegister, 0xB1, vlen_enc);\n+    __ vpmulld($xtmp1$$XMMRegister, $src1$$XMMRegister, $xtmp1$$XMMRegister, vlen_enc);\n+    __ vpshufd($xtmp2$$XMMRegister, $xtmp1$$XMMRegister, 0xB1, vlen_enc);\n+    __ vpaddd($xtmp2$$XMMRegister, $xtmp2$$XMMRegister, $xtmp1$$XMMRegister, vlen_enc);\n+    __ vpsllq($xtmp2$$XMMRegister, $xtmp2$$XMMRegister, 32, vlen_enc);\n+    \/\/ Get the lo-lo products\n+    __ vpmuludq($xtmp1$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vlen_enc);\n+    __ vpaddq($dst$$XMMRegister, $xtmp1$$XMMRegister, $xtmp2$$XMMRegister, vlen_enc);\n","filename":"src\/hotspot\/cpu\/x86\/x86.ad","additions":94,"deletions":145,"binary":false,"changes":239,"status":"modified"}]}