{"files":[{"patch":"@@ -7187,1 +7187,1 @@\n-void Assembler::vpmullq(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {\n+void Assembler::evpmullq(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {\n@@ -7222,1 +7222,1 @@\n-void Assembler::vpmullq(XMMRegister dst, XMMRegister nds, Address src, int vector_len) {\n+void Assembler::evpmullq(XMMRegister dst, XMMRegister nds, Address src, int vector_len) {\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2514,1 +2514,1 @@\n-  void vpmullq(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len);\n+  void evpmullq(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len);\n@@ -2518,1 +2518,1 @@\n-  void vpmullq(XMMRegister dst, XMMRegister nds, Address src, int vector_len);\n+  void evpmullq(XMMRegister dst, XMMRegister nds, Address src, int vector_len);\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1760,1 +1760,1 @@\n-                            vpmullq(dst, dst, src, vector_len); break;\n+                            evpmullq(dst, dst, src, vector_len); break;\n@@ -1808,1 +1808,1 @@\n-    case Op_MulReductionVL: vpmullq(dst, src1, src2, vector_len); break;\n+    case Op_MulReductionVL: evpmullq(dst, src1, src2, vector_len); break;\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -191,0 +191,4 @@\n+      case Op_MulVB:\n+        return 7;\n+      case Op_MulVL:\n+        return VM_Version::supports_avx512vldq() ? 0 : 6;\n","filename":"src\/hotspot\/cpu\/x86\/matcher_x86.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1728,1 +1728,0 @@\n-    case Op_MulVL:\n@@ -5612,3 +5611,2 @@\n-instruct mulB_reg(vec dst, vec src1, vec src2, vec tmp) %{\n-  predicate(Matcher::vector_length(n) == 4 ||\n-            Matcher::vector_length(n) == 8);\n+instruct vmul8B(vec dst, vec src1, vec src2, vec xtmp) %{\n+  predicate(Matcher::vector_length_in_bytes(n) <= 8);\n@@ -5616,2 +5614,2 @@\n-  effect(TEMP dst, TEMP tmp);\n-  format %{\"vector_mulB $dst,$src1,$src2\" %}\n+  effect(TEMP dst, TEMP xtmp);\n+  format %{ \"mulVB   $dst, $src1, $src2\\t! using $xtmp as TEMP\" %}\n@@ -5620,5 +5618,5 @@\n-    __ pmovsxbw($tmp$$XMMRegister, $src1$$XMMRegister);\n-    __ pmovsxbw($dst$$XMMRegister, $src2$$XMMRegister);\n-    __ pmullw($tmp$$XMMRegister, $dst$$XMMRegister);\n-    __ movdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), noreg);\n-    __ pand($dst$$XMMRegister, $tmp$$XMMRegister);\n+    __ pmovsxbw($dst$$XMMRegister, $src1$$XMMRegister);\n+    __ pmovsxbw($xtmp$$XMMRegister, $src2$$XMMRegister);\n+    __ pmullw($dst$$XMMRegister, $xtmp$$XMMRegister);\n+    __ psllw($dst$$XMMRegister, 8);\n+    __ psrlw($dst$$XMMRegister, 8);\n@@ -5630,2 +5628,2 @@\n-instruct mul16B_reg(vec dst, vec src1, vec src2, vec tmp1, vec tmp2) %{\n-  predicate(Matcher::vector_length(n) == 16 && UseAVX <= 1);\n+instruct vmulB(vec dst, vec src1, vec src2, vec xtmp) %{\n+  predicate(UseAVX == 0 && Matcher::vector_length_in_bytes(n) > 8);\n@@ -5633,2 +5631,2 @@\n-  effect(TEMP dst, TEMP tmp1, TEMP tmp2);\n-  format %{\"vector_mulB $dst,$src1,$src2\" %}\n+  effect(TEMP dst, TEMP xtmp);\n+  format %{ \"mulVB   $dst, $src1, $src2\\t! using $xtmp as TEMP\" %}\n@@ -5637,36 +5635,20 @@\n-    __ pmovsxbw($tmp1$$XMMRegister, $src1$$XMMRegister);\n-    __ pmovsxbw($tmp2$$XMMRegister, $src2$$XMMRegister);\n-    __ pmullw($tmp1$$XMMRegister, $tmp2$$XMMRegister);\n-    __ pshufd($tmp2$$XMMRegister, $src1$$XMMRegister, 0xEE);\n-    __ pshufd($dst$$XMMRegister, $src2$$XMMRegister, 0xEE);\n-    __ pmovsxbw($tmp2$$XMMRegister, $tmp2$$XMMRegister);\n-    __ pmovsxbw($dst$$XMMRegister, $dst$$XMMRegister);\n-    __ pmullw($tmp2$$XMMRegister, $dst$$XMMRegister);\n-    __ movdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), noreg);\n-    __ pand($tmp2$$XMMRegister, $dst$$XMMRegister);\n-    __ pand($dst$$XMMRegister, $tmp1$$XMMRegister);\n-    __ packuswb($dst$$XMMRegister, $tmp2$$XMMRegister);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct vmul16B_reg_avx(vec dst, vec src1, vec src2, vec tmp) %{\n-  predicate(Matcher::vector_length(n) == 16 && UseAVX > 1);\n-  match(Set dst (MulVB src1 src2));\n-  effect(TEMP dst, TEMP tmp);\n-  format %{\"vector_mulB $dst,$src1,$src2\" %}\n-  ins_encode %{\n-  int vlen_enc = Assembler::AVX_256bit;\n-    __ vpmovsxbw($tmp$$XMMRegister, $src1$$XMMRegister, vlen_enc);\n-    __ vpmovsxbw($dst$$XMMRegister, $src2$$XMMRegister, vlen_enc);\n-    __ vpmullw($tmp$$XMMRegister, $tmp$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n-    __ vmovdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), noreg);\n-    __ vpand($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister, vlen_enc);\n-    __ vextracti128_high($tmp$$XMMRegister, $dst$$XMMRegister);\n-    __ vpackuswb($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister, 0);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct vmul32B_reg_avx(vec dst, vec src1, vec src2, vec tmp1, vec tmp2) %{\n-  predicate(Matcher::vector_length(n) == 32);\n+    \/\/ Odd-index elements\n+    __ movdqu($dst$$XMMRegister, $src1$$XMMRegister);\n+    __ psrlw($dst$$XMMRegister, 8);\n+    __ movdqu($xtmp$$XMMRegister, $src2$$XMMRegister);\n+    __ psrlw($xtmp$$XMMRegister, 8);\n+    __ pmullw($dst$$XMMRegister, $xtmp$$XMMRegister);\n+    __ psllw($dst$$XMMRegister, 8);\n+    \/\/ Even-index elements\n+    __ movdqu($xtmp$$XMMRegister, $src1$$XMMRegister);\n+    __ pmullw($xtmp$$XMMRegister, $src2$$XMMRegister);\n+    __ psllw($xtmp$$XMMRegister, 8);\n+    __ psrlw($xtmp$$XMMRegister, 8);\n+    \/\/ Combine\n+    __ por($dst$$XMMRegister, $xtmp$$XMMRegister);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vmulB_reg(vec dst, vec src1, vec src2, vec xtmp1, vec xtmp2) %{\n+  predicate(UseAVX > 0 && Matcher::vector_length_in_bytes(n) > 8);\n@@ -5674,2 +5656,2 @@\n-  effect(TEMP dst, TEMP tmp1, TEMP tmp2);\n-  format %{\"vector_mulB $dst,$src1,$src2\" %}\n+  effect(TEMP xtmp1, TEMP xtmp2);\n+  format %{ \"vmulVB  $dst, $src1, $src2\\t! using $xtmp1, $xtmp2 as TEMP\" %}\n@@ -5677,43 +5659,12 @@\n-    assert(UseAVX > 1, \"required\");\n-    int vlen_enc = Assembler::AVX_256bit;\n-    __ vextracti128_high($tmp1$$XMMRegister, $src1$$XMMRegister);\n-    __ vextracti128_high($dst$$XMMRegister, $src2$$XMMRegister);\n-    __ vpmovsxbw($tmp1$$XMMRegister, $tmp1$$XMMRegister, vlen_enc);\n-    __ vpmovsxbw($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n-    __ vpmullw($tmp1$$XMMRegister, $tmp1$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n-    __ vpmovsxbw($tmp2$$XMMRegister, $src1$$XMMRegister, vlen_enc);\n-    __ vpmovsxbw($dst$$XMMRegister, $src2$$XMMRegister, vlen_enc);\n-    __ vpmullw($tmp2$$XMMRegister, $tmp2$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n-    __ vmovdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), noreg);\n-    __ vpbroadcastd($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n-    __ vpand($tmp1$$XMMRegister, $tmp1$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n-    __ vpand($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister, vlen_enc);\n-    __ vpackuswb($dst$$XMMRegister, $dst$$XMMRegister, $tmp1$$XMMRegister, vlen_enc);\n-    __ vpermq($dst$$XMMRegister, $dst$$XMMRegister, 0xD8, vlen_enc);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct vmul64B_reg_avx(vec dst, vec src1, vec src2, vec tmp1, vec tmp2) %{\n-  predicate(Matcher::vector_length(n) == 64);\n-  match(Set dst (MulVB src1 src2));\n-  effect(TEMP dst, TEMP tmp1, TEMP tmp2);\n-  format %{\"vector_mulB $dst,$src1,$src2\\n\\t\" %}\n-  ins_encode %{\n-    assert(UseAVX > 2, \"required\");\n-    int vlen_enc = Assembler::AVX_512bit;\n-    __ vextracti64x4_high($tmp1$$XMMRegister, $src1$$XMMRegister);\n-    __ vextracti64x4_high($dst$$XMMRegister, $src2$$XMMRegister);\n-    __ vpmovsxbw($tmp1$$XMMRegister, $tmp1$$XMMRegister, vlen_enc);\n-    __ vpmovsxbw($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n-    __ vpmullw($tmp1$$XMMRegister, $tmp1$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n-    __ vpmovsxbw($tmp2$$XMMRegister, $src1$$XMMRegister, vlen_enc);\n-    __ vpmovsxbw($dst$$XMMRegister, $src2$$XMMRegister, vlen_enc);\n-    __ vpmullw($tmp2$$XMMRegister, $tmp2$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n-    __ vmovdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), noreg);\n-    __ vpbroadcastd($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n-    __ vpand($tmp1$$XMMRegister, $tmp1$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n-    __ vpand($tmp2$$XMMRegister, $tmp2$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n-    __ vpackuswb($dst$$XMMRegister, $tmp1$$XMMRegister, $tmp2$$XMMRegister, vlen_enc);\n-    __ evmovdquq($tmp2$$XMMRegister, ExternalAddress(vector_byte_perm_mask()), vlen_enc, noreg);\n-    __ vpermq($dst$$XMMRegister, $tmp2$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n+    int vlen_enc = vector_length_encoding(this);\n+    \/\/ Odd-index elements\n+    __ vpsrlw($xtmp2$$XMMRegister, $src1$$XMMRegister, 8, vlen_enc);\n+    __ vpsrlw($xtmp1$$XMMRegister, $src2$$XMMRegister, 8, vlen_enc);\n+    __ vpmullw($xtmp2$$XMMRegister, $xtmp1$$XMMRegister, $xtmp2$$XMMRegister, vlen_enc);\n+    __ vpsllw($xtmp2$$XMMRegister, $xtmp2$$XMMRegister, 8, vlen_enc);\n+    \/\/ Even-index elements\n+    __ vpmullw($xtmp1$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vlen_enc);\n+    __ vpsllw($xtmp1$$XMMRegister, $xtmp1$$XMMRegister, 8, vlen_enc);\n+    __ vpsrlw($xtmp1$$XMMRegister, $xtmp1$$XMMRegister, 8, vlen_enc);\n+    \/\/ Combine\n+    __ vpor($dst$$XMMRegister, $xtmp1$$XMMRegister, $xtmp2$$XMMRegister, vlen_enc);\n@@ -5728,1 +5679,1 @@\n-  format %{ \"pmullw $dst,$src\\t! mul packedS\" %}\n+  format %{ \"pmullw  $dst,$src\\t! mul packedS\" %}\n@@ -5794,2 +5745,4 @@\n-instruct vmulL_reg(vec dst, vec src1, vec src2) %{\n-  predicate(VM_Version::supports_avx512dq());\n+instruct evmulL_reg(vec dst, vec src1, vec src2) %{\n+  predicate(VM_Version::supports_avx512dq() &&\n+            (Matcher::vector_length_in_bytes(n) == 64 ||\n+             VM_Version::supports_avx512vl()));\n@@ -5797,1 +5750,1 @@\n-  format %{ \"vpmullq $dst,$src1,$src2\\t! mul packedL\" %}\n+  format %{ \"evpmullq $dst,$src1,$src2\\t! mul packedL\" %}\n@@ -5801,1 +5754,1 @@\n-    __ vpmullq($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vlen_enc);\n+    __ evpmullq($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vlen_enc);\n@@ -5806,1 +5759,1 @@\n-instruct vmulL_mem(vec dst, vec src, memory mem) %{\n+instruct evmulL_mem(vec dst, vec src, memory mem) %{\n@@ -5808,1 +5761,3 @@\n-              (Matcher::vector_length_in_bytes(n->in(1)) > 8));\n+            (Matcher::vector_length_in_bytes(n) == 64 ||\n+             (Matcher::vector_length_in_bytes(n) > 8 &&\n+              VM_Version::supports_avx512vl())));\n@@ -5810,1 +5765,1 @@\n-  format %{ \"vpmullq $dst,$src,$mem\\t! mul packedL\" %}\n+  format %{ \"evpmullq $dst,$src,$mem\\t! mul packedL\" %}\n@@ -5814,1 +5769,1 @@\n-    __ vpmullq($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vlen_enc);\n+    __ evpmullq($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vlen_enc);\n@@ -5819,12 +5774,5 @@\n-instruct mul2L_reg(vec dst, vec src2, legVec tmp) %{\n-  predicate(Matcher::vector_length(n) == 2 && !VM_Version::supports_avx512dq());\n-  match(Set dst (MulVL dst src2));\n-  effect(TEMP dst, TEMP tmp);\n-  format %{ \"pshufd $tmp,$src2, 177\\n\\t\"\n-            \"pmulld $tmp,$dst\\n\\t\"\n-            \"phaddd $tmp,$tmp\\n\\t\"\n-            \"pmovzxdq $tmp,$tmp\\n\\t\"\n-            \"psllq $tmp, 32\\n\\t\"\n-            \"pmuludq $dst,$src2\\n\\t\"\n-            \"paddq $dst,$tmp\\n\\t! mul packed2L\" %}\n-\n+instruct vmulL(vec dst, vec src1, vec src2, vec xtmp) %{\n+  predicate(UseAVX == 0);\n+  match(Set dst (MulVL src1 src2));\n+  effect(TEMP dst, TEMP xtmp);\n+  format %{ \"mulVL   $dst, $src1, $src2\\t! using $xtmp as TEMP\" %}\n@@ -5833,8 +5781,10 @@\n-    int vlen_enc = Assembler::AVX_128bit;\n-    __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 177);\n-    __ pmulld($tmp$$XMMRegister, $dst$$XMMRegister);\n-    __ phaddd($tmp$$XMMRegister, $tmp$$XMMRegister);\n-    __ pmovzxdq($tmp$$XMMRegister, $tmp$$XMMRegister);\n-    __ psllq($tmp$$XMMRegister, 32);\n-    __ pmuludq($dst$$XMMRegister, $src2$$XMMRegister);\n-    __ paddq($dst$$XMMRegister, $tmp$$XMMRegister);\n+    \/\/ Get the lo-hi products, only the lower 32 bits is in concerns\n+    __ pshufd($xtmp$$XMMRegister, $src2$$XMMRegister, 0xB1);\n+    __ pmulld($xtmp$$XMMRegister, $src1$$XMMRegister);\n+    __ pshufd($dst$$XMMRegister, $xtmp$$XMMRegister, 0xB1);\n+    __ paddd($dst$$XMMRegister, $xtmp$$XMMRegister);\n+    __ psllq($dst$$XMMRegister, 32);\n+    \/\/ Get the lo-lo products\n+    __ movdqu($xtmp$$XMMRegister, $src1$$XMMRegister);\n+    __ pmuludq($xtmp$$XMMRegister, $src2$$XMMRegister);\n+    __ paddq($dst$$XMMRegister, $xtmp$$XMMRegister);\n@@ -5845,2 +5795,2 @@\n-instruct vmul4L_reg_avx(vec dst, vec src1, vec src2, legVec tmp, legVec tmp1) %{\n-  predicate(Matcher::vector_length(n) == 4 && !VM_Version::supports_avx512dq());\n+instruct vmulL_reg(vec dst, vec src1, vec src2, vec xtmp1, vec xtmp2) %{\n+  predicate(UseAVX > 0 && !VM_Version::supports_avx512dq());\n@@ -5848,8 +5798,2 @@\n-  effect(TEMP tmp1, TEMP tmp);\n-  format %{ \"vpshufd $tmp,$src2\\n\\t\"\n-            \"vpmulld $tmp,$src1,$tmp\\n\\t\"\n-            \"vphaddd $tmp,$tmp,$tmp\\n\\t\"\n-            \"vpmovzxdq $tmp,$tmp\\n\\t\"\n-            \"vpsllq $tmp,$tmp\\n\\t\"\n-            \"vpmuludq $tmp1,$src1,$src2\\n\\t\"\n-            \"vpaddq $dst,$tmp,$tmp1\\t! mul packed4L\" %}\n+  effect(TEMP xtmp1, TEMP xtmp2);\n+  format %{ \"vmulVL  $dst, $src1, $src2\\t! using $xtmp1, $xtmp2 as TEMP\" %}\n@@ -5857,9 +5801,10 @@\n-    int vlen_enc = Assembler::AVX_256bit;\n-    __ vpshufd($tmp$$XMMRegister, $src2$$XMMRegister, 177, vlen_enc);\n-    __ vpmulld($tmp$$XMMRegister, $src1$$XMMRegister, $tmp$$XMMRegister, vlen_enc);\n-    __ vextracti128_high($tmp1$$XMMRegister, $tmp$$XMMRegister);\n-    __ vphaddd($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp1$$XMMRegister, vlen_enc);\n-    __ vpmovzxdq($tmp$$XMMRegister, $tmp$$XMMRegister, vlen_enc);\n-    __ vpsllq($tmp$$XMMRegister, $tmp$$XMMRegister, 32, vlen_enc);\n-    __ vpmuludq($tmp1$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vlen_enc);\n-    __ vpaddq($dst$$XMMRegister, $tmp$$XMMRegister, $tmp1$$XMMRegister, vlen_enc);\n+    int vlen_enc = vector_length_encoding(this);\n+    \/\/ Get the lo-hi products, only the lower 32 bits is in concerns\n+    __ vpshufd($xtmp1$$XMMRegister, $src2$$XMMRegister, 0xB1, vlen_enc);\n+    __ vpmulld($xtmp1$$XMMRegister, $src1$$XMMRegister, $xtmp1$$XMMRegister, vlen_enc);\n+    __ vpshufd($xtmp2$$XMMRegister, $xtmp1$$XMMRegister, 0xB1, vlen_enc);\n+    __ vpaddd($xtmp2$$XMMRegister, $xtmp2$$XMMRegister, $xtmp1$$XMMRegister, vlen_enc);\n+    __ vpsllq($xtmp2$$XMMRegister, $xtmp2$$XMMRegister, 32, vlen_enc);\n+    \/\/ Get the lo-lo products\n+    __ vpmuludq($xtmp1$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vlen_enc);\n+    __ vpaddq($dst$$XMMRegister, $xtmp1$$XMMRegister, $xtmp2$$XMMRegister, vlen_enc);\n","filename":"src\/hotspot\/cpu\/x86\/x86.ad","additions":89,"deletions":144,"binary":false,"changes":233,"status":"modified"}]}