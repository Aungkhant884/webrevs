{"files":[{"patch":"@@ -84,0 +84,3 @@\n+\n+  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  bs->nmethod_entry_barrier(this, R20);\n","filename":"src\/hotspot\/cpu\/ppc\/c1_MacroAssembler_ppc.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -26,0 +26,1 @@\n+#include \"nativeInst_ppc.hpp\"\n@@ -29,0 +30,1 @@\n+#include \"gc\/shared\/barrierSetNMethod.hpp\"\n@@ -32,0 +34,2 @@\n+#include \"runtime\/sharedRuntime.hpp\"\n+#include \"runtime\/stubRoutines.hpp\"\n@@ -128,0 +132,66 @@\n+\n+void BarrierSetAssembler::nmethod_entry_barrier(MacroAssembler* masm, Register tmp) {\n+  BarrierSetNMethod* bs_nm = BarrierSet::barrier_set()->barrier_set_nmethod();\n+  if (bs_nm == nullptr) {\n+    return;\n+  }\n+\n+  assert_different_registers(tmp, R0);\n+\n+  \/\/ Load stub address using toc (fixed instruction size, unlike load_const_optimized)\n+  __ calculate_address_from_global_toc(tmp, StubRoutines::ppc::nmethod_entry_barrier(),\n+                                       true, true, false); \/\/ 2 instructions\n+  __ mtctr(tmp);\n+\n+  \/\/ This is a compound instruction. Patching support is provided by NativeMovRegMem.\n+  \/\/ Actual patching is done in (platform-specific part of) BarrierSetNMethod.\n+  __ load_const32(tmp, 0 \/* Value is patched *\/); \/\/ 2 instructions\n+\n+  __ lwz(R0, in_bytes(bs_nm->thread_disarmed_offset()), R16_thread);\n+  __ cmpw(CCR0, R0, tmp);\n+\n+  __ bnectrl(CCR0);\n+\n+  \/\/ Oops may have been changed; exploiting isync semantics (used as acquire) to make those updates observable.\n+  __ isync();\n+}\n+\n+void BarrierSetAssembler::c2i_entry_barrier(MacroAssembler *masm, Register tmp1, Register tmp2, Register tmp3) {\n+  BarrierSetNMethod* bs_nm = BarrierSet::barrier_set()->barrier_set_nmethod();\n+  if (bs_nm == nullptr) {\n+    return;\n+  }\n+\n+  assert_different_registers(tmp1, tmp2, tmp3);\n+\n+  Register tmp1_class_loader_data = tmp1;\n+\n+  Label bad_call, skip_barrier;\n+\n+  \/\/ Fast path: If no method is given, the call is definitely bad.\n+  __ cmpdi(CCR0, R19_method, 0);\n+  __ beq(CCR0, bad_call);\n+\n+  \/\/ Load class loader data to determine whether the method's holder is concurrently unloading.\n+  __ load_method_holder(tmp1, R19_method);\n+  __ ld(tmp1_class_loader_data, in_bytes(InstanceKlass::class_loader_data_offset()), tmp1);\n+\n+  \/\/ Fast path: If class loader is strong, the holder cannot be unloaded.\n+  __ ld(tmp2, in_bytes(ClassLoaderData::keep_alive_offset()), tmp1_class_loader_data);\n+  __ cmpdi(CCR0, tmp2, 0);\n+  __ bne(CCR0, skip_barrier);\n+\n+  \/\/ Class loader is weak. Determine whether the holder still alive.\n+  __ ld(tmp2, in_bytes(ClassLoaderData::holder_offset()), tmp1_class_loader_data);\n+  __ resolve_weak_handle(tmp2, tmp1, tmp3, MacroAssembler::PreservationLevel::PRESERVATION_FRAME_LR_GP_FP_REGS);\n+  __ cmpdi(CCR0, tmp2, 0);\n+  __ bne(CCR0, skip_barrier);\n+\n+  __ bind(bad_call);\n+\n+  __ load_const_optimized(tmp1, SharedRuntime::get_handle_wrong_method_stub(), tmp2);\n+  __ mtctr(tmp1);\n+  __ bctr();\n+\n+  __ bind(skip_barrier);\n+}\n","filename":"src\/hotspot\/cpu\/ppc\/gc\/shared\/barrierSetAssembler_ppc.cpp","additions":70,"deletions":0,"binary":false,"changes":70,"status":"modified"},{"patch":"@@ -60,0 +60,3 @@\n+\n+  virtual void nmethod_entry_barrier(MacroAssembler* masm, Register tmp);\n+  virtual void c2i_entry_barrier(MacroAssembler* masm, Register tmp1, Register tmp2, Register tmp3);\n","filename":"src\/hotspot\/cpu\/ppc\/gc\/shared\/barrierSetAssembler_ppc.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -26,0 +26,3 @@\n+#include \"code\/codeBlob.hpp\"\n+#include \"code\/nmethod.hpp\"\n+#include \"code\/nativeInst.hpp\"\n@@ -29,0 +32,78 @@\n+class NativeNMethodBarrier: public NativeInstruction {\n+\n+  address get_barrier_start_address() const {\n+    return NativeInstruction::addr_at(0);\n+  }\n+\n+  NativeMovRegMem* get_patchable_instruction_handle() const {\n+    \/\/ Endianness is handled by NativeMovRegMem\n+    return reinterpret_cast<NativeMovRegMem*>(get_barrier_start_address() + 3 * 4);\n+  }\n+\n+public:\n+  unsigned short get_guard_value() const {\n+    \/\/ Retrieve the guard value (naming of 'offset' function is misleading).\n+    return get_patchable_instruction_handle()->offset();\n+  }\n+\n+  void release_set_guard_value(unsigned short value) {\n+    \/\/ Patching is not atomic.\n+    \/\/ Stale observations of the \"armed\" state is okay as invoking the barrier stub in that case has no\n+    \/\/ unwanted side effects. Disarming is thus a non-critical operation.\n+    \/\/ The visibility of the \"armed\" state must be ensured by safepoint\/handshake.\n+\n+    OrderAccess::release(); \/\/ Release modified oops\n+\n+    \/\/ Set the guard value (naming of 'offset' function is misleading).\n+    get_patchable_instruction_handle()->set_offset(value);\n+  }\n+\n+  void verify() const {\n+    \/\/ Although it's possible to just validate the to-be-patched instruction,\n+    \/\/ all instructions are validated to ensure that the barrier is hit properly - especially since\n+    \/\/ the pattern used in load_const32 is a quite common one.\n+\n+    uint* current_instruction = reinterpret_cast<uint*>(get_barrier_start_address());\n+\n+    \/\/ calculate_address_from_global_toc (compound instruction)\n+    verify_op_code_manually(current_instruction, MacroAssembler::is_addis(*current_instruction));\n+    verify_op_code_manually(current_instruction, MacroAssembler::is_addi(*current_instruction));\n+\n+    verify_op_code_manually(current_instruction, MacroAssembler::is_mtctr(*current_instruction));\n+\n+    get_patchable_instruction_handle()->verify();\n+    current_instruction += 2;\n+\n+    verify_op_code(current_instruction, Assembler::LWZ_OPCODE);\n+\n+    \/\/ cmpw (mnemonic)\n+    verify_op_code(current_instruction, Assembler::CMP_OPCODE);\n+\n+    \/\/ bnectrl (mnemonic) (weak check; not checking the exact type)\n+    verify_op_code(current_instruction, Assembler::BCCTR_OPCODE);\n+\n+    verify_op_code(current_instruction, Assembler::ISYNC_OPCODE);\n+  }\n+\n+private:\n+  static void verify_op_code_manually(uint*& current_instruction, bool result) {\n+    assert(result, \"illegal instruction sequence for nmethod entry barrier\");\n+    current_instruction++;\n+  }\n+  static void verify_op_code(uint*& current_instruction, uint expected,\n+                             unsigned int mask = 63u << Assembler::OPCODE_SHIFT) {\n+    \/\/ Masking both, current instruction and opcode, as some opcodes in Assembler contain additional information\n+    \/\/ to uniquely identify simplified mnemonics.\n+    \/\/ As long as the caller doesn't provide a custom mask, that additional information is discarded.\n+    verify_op_code_manually(current_instruction, (*current_instruction & mask) == (expected & mask));\n+  }\n+};\n+\n+static NativeNMethodBarrier* get_nmethod_barrier(nmethod* nm) {\n+  address barrier_address = nm->code_begin() + nm->frame_complete_offset() + (-9 * 4);\n+\n+  auto barrier = reinterpret_cast<NativeNMethodBarrier*>(barrier_address);\n+  debug_only(barrier->verify());\n+  return barrier;\n+}\n+\n@@ -30,1 +111,3 @@\n-  ShouldNotReachHere();\n+  \/\/ Nothing to do.\n+  \/\/ Unlike other platforms, the frame resolution is done in the nmethod entry barrier stub.\n+  \/\/ This way, writing frame information on the stack can be avoided.\n@@ -34,1 +117,6 @@\n-  ShouldNotReachHere();\n+  if (!supports_entry_barrier(nm)) {\n+    return;\n+  }\n+\n+  NativeNMethodBarrier* barrier = get_nmethod_barrier(nm);\n+  barrier->release_set_guard_value(disarmed_value());\n@@ -38,2 +126,6 @@\n-  ShouldNotReachHere();\n-  return false;\n+  if (!supports_entry_barrier(nm)) {\n+    return false;\n+  }\n+\n+  NativeNMethodBarrier* barrier = get_nmethod_barrier(nm);\n+  return barrier->get_guard_value() != disarmed_value();\n","filename":"src\/hotspot\/cpu\/ppc\/gc\/shared\/barrierSetNMethod_ppc.cpp","additions":96,"deletions":4,"binary":false,"changes":100,"status":"modified"},{"patch":"@@ -3382,1 +3382,1 @@\n-  \/\/ Layout: See StubRoutines::generate_crc_constants.\n+  \/\/ Layout: See StubRoutines::ppc::generate_crc_constants.\n","filename":"src\/hotspot\/cpu\/ppc\/macroAssembler_ppc.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1379,0 +1379,4 @@\n+\n+  if (C->stub_function() == NULL) {\n+    st->print(\"nmethod entry barrier\\n\\t\");\n+  }\n@@ -1532,0 +1536,5 @@\n+  if (C->stub_function() == NULL) {\n+    BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+    bs->nmethod_entry_barrier(&_masm, push_frame_temp);\n+  }\n+\n","filename":"src\/hotspot\/cpu\/ppc\/ppc.ad","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -1304,0 +1304,3 @@\n+  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  bs->c2i_entry_barrier(masm, \/* tmp register*\/ ic_klass, \/* tmp register*\/ receiver_klass, \/* tmp register*\/ code);\n+\n@@ -1306,1 +1309,2 @@\n-  return AdapterHandlerLibrary::new_entry(fingerprint, i2c_entry, c2i_entry, c2i_unverified_entry, c2i_no_clinit_check_entry);\n+  return AdapterHandlerLibrary::new_entry(fingerprint, i2c_entry, c2i_entry, c2i_unverified_entry,\n+                                          c2i_no_clinit_check_entry);\n@@ -1976,0 +1980,4 @@\n+\n+  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  bs->nmethod_entry_barrier(masm, r_temp_1);\n+\n","filename":"src\/hotspot\/cpu\/ppc\/sharedRuntime_ppc.cpp","additions":9,"deletions":1,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"gc\/shared\/barrierSetNMethod.hpp\"\n@@ -3551,0 +3552,47 @@\n+  address generate_nmethod_entry_barrier() {\n+    __ align(CodeEntryAlignment);\n+    StubCodeMark mark(this, \"StubRoutines\", \"nmethod_entry_barrier\");\n+\n+    address stub_address = __ pc();\n+\n+    int nbytes_save = MacroAssembler::num_volatile_regs * BytesPerWord;\n+    __ save_volatile_gprs(R1_SP, -nbytes_save, true);\n+\n+    \/\/ Link register points to instruction in prologue of the guarded nmethod.\n+    \/\/ As the stub requires one layer of indirection (argument is of type address* and not address),\n+    \/\/ passing the link register's value directly doesn't work.\n+    \/\/ Since we have to save the link register on the stack anyway, we calculate the corresponding stack address\n+    \/\/ and pass that one instead.\n+    __ add(R3_ARG1, _abi0(lr), R1_SP);\n+\n+    __ save_LR_CR(R0);\n+    __ push_frame_reg_args(nbytes_save, R0);\n+\n+    __ call_VM_leaf(CAST_FROM_FN_PTR(address, BarrierSetNMethod::nmethod_stub_entry_barrier));\n+    __ mr(R0, R3_RET);\n+\n+    __ pop_frame();\n+    __ restore_LR_CR(R3_RET \/* used as tmp register *\/);\n+    __ restore_volatile_gprs(R1_SP, -nbytes_save, true);\n+\n+    __ cmpdi(CCR0, R0, 0);\n+\n+    \/\/ Return to prologue if no deoptimization is required (bnelr)\n+    __ bclr(Assembler::bcondCRbiIs1_bhintIsTaken, Assembler::bi0(CCR0, Assembler::equal), Assembler::bhintIsTaken);\n+\n+    \/\/ Deoptimization required.\n+    \/\/ For actually handling the deoptimization, the 'wrong method stub' is invoked.\n+    __ load_const_optimized(R0, SharedRuntime::get_handle_wrong_method_stub());\n+    __ mtctr(R0);\n+\n+    \/\/ Pop the frame built in the prologue.\n+    __ pop_frame();\n+\n+    \/\/ Restore link register. Points to an instruction in the previous Java frame; effectively resuming\n+    \/\/ its execution after this method's deoptimization.  This method's prologue is aborted.\n+    __ restore_LR_CR(R0);\n+\n+    __ bctr();\n+    return stub_address;\n+  }\n+\n@@ -4465,1 +4513,1 @@\n-      StubRoutines::_crc_table_adr = StubRoutines::generate_crc_constants(REVERSE_CRC32_POLY);\n+      StubRoutines::_crc_table_adr = StubRoutines::ppc::generate_crc_constants(REVERSE_CRC32_POLY);\n@@ -4471,1 +4519,1 @@\n-      StubRoutines::_crc32c_table_addr = StubRoutines::generate_crc_constants(REVERSE_CRC32C_POLY);\n+      StubRoutines::_crc32c_table_addr = StubRoutines::ppc::generate_crc_constants(REVERSE_CRC32C_POLY);\n@@ -4497,0 +4545,6 @@\n+    \/\/ nmethod entry barriers for concurrent class unloading\n+    BarrierSetNMethod* bs_nm = BarrierSet::barrier_set()->barrier_set_nmethod();\n+    if (bs_nm != NULL) {\n+      StubRoutines::ppc::_nmethod_entry_barrier            = generate_nmethod_entry_barrier();\n+    }\n+\n","filename":"src\/hotspot\/cpu\/ppc\/stubGenerator_ppc.cpp","additions":56,"deletions":2,"binary":false,"changes":58,"status":"modified"},{"patch":"@@ -49,1 +49,11 @@\n-static address generate_crc_constants(juint reverse_poly);\n+class ppc {\n+  friend class StubGenerator;\n+\n+ private:\n+  static address _nmethod_entry_barrier;\n+\n+ public:\n+  static address nmethod_entry_barrier();\n+\n+  static address generate_crc_constants(juint reverse_poly);\n+};\n","filename":"src\/hotspot\/cpu\/ppc\/stubRoutines_ppc.hpp","additions":11,"deletions":1,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -83,1 +83,1 @@\n-address StubRoutines::generate_crc_constants(juint reverse_poly) {\n+address StubRoutines::ppc::generate_crc_constants(juint reverse_poly) {\n@@ -213,0 +213,5 @@\n+\n+address StubRoutines::ppc::_nmethod_entry_barrier = nullptr;\n+address StubRoutines::ppc::nmethod_entry_barrier() {\n+  return _nmethod_entry_barrier;\n+}\n","filename":"src\/hotspot\/cpu\/ppc\/stubRoutines_ppc_64.cpp","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"}]}