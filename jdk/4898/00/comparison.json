{"files":[{"patch":"@@ -137,0 +137,9 @@\n+    \/\/ JDK-8270308: Amalloc guarantees 64-bit alignment and we need to ensure that in case the preceding\n+    \/\/ allocation was AmallocWords. Note though that padding plays havoc with arenas holding packed arrays,\n+    \/\/ like HandleAreas. Those areas should never mix Amalloc.. calls with differing alignment.\n+#ifndef LP64 \/\/ Since this is a hot path, and on 64-bit Amalloc and AmallocWords are identical, restrict this alignment to 32-bit.\n+    if (x > 0) {\n+      _hwm = ARENA_ALIGN(_hwm);\n+      _hwm = MIN2(_hwm, _max); \/\/ _max is not guaranteed to be 64 bit aligned.\n+    }\n+#endif \/\/ !LP64\n","filename":"src\/hotspot\/share\/memory\/arena.hpp","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -0,0 +1,390 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2021 SAP SE. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"memory\/allocation.hpp\"\n+#include \"memory\/arena.hpp\"\n+#include \"runtime\/os.hpp\"\n+#include \"unittest.hpp\"\n+\n+#define ASSERT_NOT_NULL(p) ASSERT_NE(p, (char*)NULL)\n+#define ASSERT_NULL(p) ASSERT_EQ(p, (char*)NULL)\n+\n+#define ASSERT_CONTAINS(ar, p) ASSERT_TRUE(ar.contains(p))\n+\n+\/\/ Note:\n+\/\/ - Amalloc returns 64bit aligned pointer (also on 32-bit)\n+\/\/ - AmallocWords returns word-aligned pointer\n+#define ASSERT_ALIGN(p, n) ASSERT_TRUE(is_aligned(p, n))\n+#define ASSERT_ALIGN_64(p) ASSERT_ALIGN(p, 8)\n+\n+\/\/ Check a return address from Amalloc, expected to be non-null\n+#define ASSERT_AMALLOC(ar, p) \\\n+  ASSERT_NOT_NULL(p); \\\n+  ASSERT_CONTAINS(ar, p); \\\n+  ASSERT_ALIGN_64(p);\n+\n+\/\/ #define LOG(s) tty->print_cr s;\n+#define LOG(s)\n+\n+\/\/ Given a memory range, check that the whole range is filled with the expected byte.\n+\/\/ If not, print the surrounding bytes as hex and return false.\n+static bool check_range(const void* p, size_t s, uint8_t expected) {\n+  if (p == NULL || s == 0) {\n+    return true;\n+  }\n+\n+  const char* first_wrong = NULL;\n+  char* p2 = (char*)p;\n+  const char* const end = p2 + s;\n+  while (p2 < end) {\n+    if (*p2 != (char)expected) {\n+      first_wrong = p2;\n+      break;\n+    }\n+    p2 ++;\n+  }\n+\n+  if (first_wrong != NULL) {\n+    tty->print_cr(\"wrong pattern around \" PTR_FORMAT, p2i(first_wrong));\n+    os::print_hex_dump(tty, (address)(align_down(p2, 0x10) - 0x10),\n+                            (address)(align_up(end, 0x10) + 0x10), 1);\n+  }\n+\n+  return first_wrong == NULL;\n+}\n+\n+\/\/ Fill range with a byte mark.\n+static void mark_range_with(void* p, size_t s, uint8_t mark) {\n+  assert(mark != badResourceValue, \"choose a different mark please\");\n+  if (p != NULL && s > 0) {\n+    ::memset(p, mark, s);\n+  }\n+}\n+#define ASSERT_RANGE_IS_MARKED_WITH(p, size, mark)  ASSERT_TRUE(check_range(p, size, mark))\n+\n+\/\/ Fill range with a fixed byte mark.\n+static void mark_range(void* p, size_t s) {\n+  mark_range_with(p, s, '#');\n+}\n+#define ASSERT_RANGE_IS_MARKED(p, size)             ASSERT_TRUE(check_range(p, size, '#'))\n+\n+\/\/ Test behavior for Amalloc(0):\n+TEST_VM(Arena, alloc_size_0) {\n+  \/\/ Amalloc(0) returns a (non-unique) non-NULL pointer.\n+  Arena ar(mtTest);\n+  void* p = ar.Amalloc(0);\n+  \/\/ The returned pointer should be not null, aligned, but not (!) contained in the arena\n+  \/\/ since it has size 0 and points at hwm, thus beyond the arena content. Should we ever\n+  \/\/ change that behavior (size 0 -> size 1, like we do in os::malloc) arena.contains will\n+  \/\/ work as expected for 0 sized allocations too. Note that UseMallocOnly behaves differently,\n+  \/\/ but there, arena.contains() is broken anyway for pointers other than the start of a block.\n+  ASSERT_NOT_NULL(p);\n+  ASSERT_ALIGN_64(p);\n+  if (!UseMallocOnly) {\n+    ASSERT_FALSE(ar.contains(p));\n+  }\n+\n+  \/\/ Allocate again. The new allocations should have the same position as the 0-sized\n+  \/\/ first one.\n+  if (!UseMallocOnly) {\n+    void* p2 = ar.Amalloc(1);\n+    ASSERT_AMALLOC(ar, p2);\n+    ASSERT_EQ(p2, p);\n+  }\n+}\n+\n+\/\/ Test behavior for Arealloc(p, 0)\n+TEST_VM(Arena, realloc_size_0) {\n+  \/\/ Arealloc(p, 0) behaves like Afree(p). It should release the memory\n+  \/\/ and, if top position, roll back the hwm.\n+  Arena ar(mtTest);\n+  void* p1 = ar.Amalloc(0x10);\n+  ASSERT_AMALLOC(ar, p1);\n+  void* p2 = ar.Arealloc(p1, 0x10, 0);\n+  ASSERT_NULL(p2);\n+\n+  \/\/ a subsequent allocation should get the same pointer\n+  if (!UseMallocOnly) {\n+    void* p3 = ar.Amalloc(0x20);\n+    ASSERT_EQ(p3, p1);\n+  }\n+}\n+\n+\/\/ Realloc equal sizes is a noop\n+TEST_VM(Arena, realloc_same_size) {\n+  Arena ar(mtTest);\n+  void* p1 = ar.Amalloc(0x200);\n+  ASSERT_AMALLOC(ar, p1);\n+  mark_range(p1, 0x200);\n+\n+  void* p2 = ar.Arealloc(p1, 0x200, 0x200);\n+\n+  if (!UseMallocOnly) {\n+    ASSERT_EQ(p2, p1);\n+  }\n+  ASSERT_RANGE_IS_MARKED(p2, 0x200);\n+}\n+\n+\/\/ Test behavior for Afree(NULL) and Arealloc(NULL, x)\n+TEST_VM(Arena, free_null) {\n+  Arena ar(mtTest);\n+  ar.Afree(NULL, 10); \/\/ should just be ignored\n+}\n+\n+TEST_VM(Arena, realloc_null) {\n+  Arena ar(mtTest);\n+  void* p = ar.Arealloc(NULL, 0, 20); \/\/ equivalent to Amalloc(20)\n+  ASSERT_AMALLOC(ar, p);\n+}\n+\n+\/\/ Check Arena.Afree in a non-top position.\n+\/\/ The free'd allocation should be zapped (debug only),\n+\/\/ surrounding blocks should be unaffected.\n+TEST_VM(Arena, free_nontop) {\n+  Arena ar(mtTest);\n+\n+  void* p_before = ar.Amalloc(0x10);\n+  ASSERT_AMALLOC(ar, p_before);\n+  mark_range(p_before, 0x10);\n+\n+  void* p = ar.Amalloc(0x10);\n+  ASSERT_AMALLOC(ar, p);\n+  mark_range_with(p, 0x10, 'Z');\n+\n+  void* p_after = ar.Amalloc(0x10);\n+  ASSERT_AMALLOC(ar, p_after);\n+  mark_range(p_after, 0x10);\n+\n+  ASSERT_RANGE_IS_MARKED(p_before, 0x10);\n+  ASSERT_RANGE_IS_MARKED_WITH(p, 0x10, 'Z');\n+  ASSERT_RANGE_IS_MARKED(p_after, 0x10);\n+\n+  ar.Afree(p, 0x10);\n+\n+  ASSERT_RANGE_IS_MARKED(p_before, 0x10);\n+  DEBUG_ONLY(ASSERT_RANGE_IS_MARKED_WITH(p, 0x10, badResourceValue);)\n+  ASSERT_RANGE_IS_MARKED(p_after, 0x10);\n+}\n+\n+\/\/ Check Arena.Afree in a top position.\n+\/\/ The free'd allocation (non-top) should be zapped (debug only),\n+\/\/ the hwm should have been rolled back.\n+TEST_VM(Arena, free_top) {\n+  Arena ar(mtTest);\n+\n+  void* p = ar.Amalloc(0x10);\n+  ASSERT_AMALLOC(ar, p);\n+  mark_range_with(p, 0x10, 'Z');\n+\n+  ar.Afree(p, 0x10);\n+  DEBUG_ONLY(ASSERT_RANGE_IS_MARKED_WITH(p, 0x10, badResourceValue);)\n+\n+  \/\/ a subsequent allocation should get the same pointer\n+  if (!UseMallocOnly) {\n+    void* p2 = ar.Amalloc(0x20);\n+    ASSERT_EQ(p2, p);\n+  }\n+}\n+\n+\/\/ In-place shrinking.\n+TEST_VM(Arena, realloc_top_shrink) {\n+  if (!UseMallocOnly) {\n+    Arena ar(mtTest);\n+\n+    void* p1 = ar.Amalloc(0x200);\n+    ASSERT_AMALLOC(ar, p1);\n+    mark_range(p1, 0x200);\n+\n+    void* p2 = ar.Arealloc(p1, 0x200, 0x100);\n+    ASSERT_EQ(p1, p2);\n+    ASSERT_RANGE_IS_MARKED(p2, 0x100); \/\/ realloc should preserve old content\n+\n+    \/\/ A subsequent allocation should be placed right after the end of the first, shrunk, allocation\n+    void* p3 = ar.Amalloc(1);\n+    ASSERT_EQ(p3, ((char*)p1) + 0x100);\n+  }\n+}\n+\n+\/\/ not-in-place shrinking.\n+TEST_VM(Arena, realloc_nontop_shrink) {\n+  Arena ar(mtTest);\n+\n+  void* p1 = ar.Amalloc(200);\n+  ASSERT_AMALLOC(ar, p1);\n+  mark_range(p1, 200);\n+\n+  void* p_other = ar.Amalloc(20); \/\/ new top, p1 not top anymore\n+\n+  void* p2 = ar.Arealloc(p1, 200, 100);\n+  if (!UseMallocOnly) {\n+    ASSERT_EQ(p1, p2); \/\/ should still shrink in place\n+  }\n+  ASSERT_RANGE_IS_MARKED(p2, 100); \/\/ realloc should preserve old content\n+}\n+\n+\/\/ in-place growing.\n+TEST_VM(Arena, realloc_top_grow) {\n+  Arena ar(mtTest); \/\/ initial chunk size large enough to ensure below allocation grows in-place.\n+\n+  void* p1 = ar.Amalloc(0x10);\n+  ASSERT_AMALLOC(ar, p1);\n+  mark_range(p1, 0x10);\n+\n+  void* p2 = ar.Arealloc(p1, 0x10, 0x20);\n+  if (!UseMallocOnly) {\n+    ASSERT_EQ(p1, p2);\n+  }\n+  ASSERT_RANGE_IS_MARKED(p2, 0x10); \/\/ realloc should preserve old content\n+}\n+\n+\/\/ not-in-place growing.\n+TEST_VM(Arena, realloc_nontop_grow) {\n+  Arena ar(mtTest);\n+\n+  void* p1 = ar.Amalloc(10);\n+  ASSERT_AMALLOC(ar, p1);\n+  mark_range(p1, 10);\n+\n+  void* p_other = ar.Amalloc(20); \/\/ new top, p1 not top anymore\n+\n+  void* p2 = ar.Arealloc(p1, 10, 20);\n+  ASSERT_AMALLOC(ar, p2);\n+  ASSERT_RANGE_IS_MARKED(p2, 10); \/\/ realloc should preserve old content\n+}\n+\n+\/\/ -------- random alloc test -------------\n+\n+static uint8_t canary(int i) {\n+  return (uint8_t)('A' + i % 26);\n+}\n+\n+TEST_VM(Arena, random_allocs) {\n+\n+  \/\/ Randomly allocate with random sizes and differing alignments;\n+  \/\/  check alignment and check for overwriters.\n+  \/\/ We do this a large number of times, to give chunk handling a\n+  \/\/  good workout too.\n+\n+  const int num_allocs = 250 * 1000;\n+  const int avg_alloc_size = 64;\n+\n+  void** ptrs = NEW_C_HEAP_ARRAY(void*, num_allocs, mtTest);\n+  size_t* sizes = NEW_C_HEAP_ARRAY(size_t, num_allocs, mtTest);\n+  size_t* alignments = NEW_C_HEAP_ARRAY(size_t, num_allocs, mtTest);\n+\n+  Arena ar(mtTest);\n+\n+  \/\/ Allocate\n+  for (int i = 0; i < num_allocs; i ++) {\n+    size_t size = os::random() % (avg_alloc_size * 2); \/\/ Note: 0 is possible and should work\n+    size_t alignment = 0;\n+    void* p = NULL;\n+    if (os::random() % 2) { \/\/ randomly switch between Amalloc and AmallocWords\n+      p = ar.Amalloc(size);\n+      alignment = BytesPerLong;\n+    } else {\n+      \/\/ Inconsistency: AmallocWords wants its input size word aligned, whereas Amalloc takes\n+      \/\/  care of alignment itself. We may want to clean this up, but for now just go with it.\n+      size = align_up(size, BytesPerWord);\n+      p = ar.AmallocWords(size);\n+      alignment = BytesPerWord;\n+    }\n+    LOG((\"[%d]: \" PTR_FORMAT \", size \" SIZE_FORMAT \", aligned \" SIZE_FORMAT,\n+         i, p2i(p), size, alignment));\n+    ASSERT_NOT_NULL(p);\n+    if (size > 0) {\n+      ASSERT_ALIGN(p, alignment);\n+      ASSERT_CONTAINS(ar, p);\n+    }\n+    mark_range_with(p, size, canary(i));\n+    ptrs[i] = p; sizes[i] = size; alignments[i] = alignment;\n+  }\n+\n+  \/\/ Check pattern in allocations for overwriters.\n+  for (int i = 0; i < num_allocs; i ++) {\n+    ASSERT_RANGE_IS_MARKED_WITH(ptrs[i], sizes[i], canary(i));\n+  }\n+\n+  \/\/ realloc all of them randomly\n+  for (int i = 0; i < num_allocs; i ++) {\n+    size_t new_size = os::random() % (avg_alloc_size * 2);  \/\/ Note: 0 is possible and should work\n+    void* p2 = ar.Arealloc(ptrs[i], sizes[i], new_size);\n+    LOG((\"[%d]: realloc \" PTR_FORMAT \", size \" SIZE_FORMAT \", aligned \" SIZE_FORMAT,\n+         i, p2i(p2), new_size, alignments[i]));\n+    if (new_size > 0) {\n+      ASSERT_NOT_NULL(p2);\n+      ASSERT_TRUE(ar.contains(p2));\n+      \/\/ Arealloc only guarantees the original alignment, nothing bigger (if the block was resized in-place,\n+      \/\/ it keeps the original alignment)\n+      ASSERT_ALIGN(p2, alignments[i]);\n+      \/\/ old content should have been preserved\n+      ASSERT_RANGE_IS_MARKED_WITH(p2, MIN2(sizes[i], new_size), canary(i));\n+      \/\/ mark new range\n+      mark_range_with(p2, new_size, canary(i));\n+    } else {\n+      ASSERT_NULL(p2);\n+    }\n+    ptrs[i] = p2; sizes[i] = new_size;\n+  }\n+\n+  \/\/ Check test pattern again\n+  \/\/  Note that we don't check the gap pattern anymore since if allocations had been shrunk in place\n+  \/\/  this now gets difficult.\n+  for (int i = 0; i < num_allocs; i ++) {\n+    ASSERT_RANGE_IS_MARKED_WITH(ptrs[i], sizes[i], canary(i));\n+  }\n+\n+  \/\/ Randomly free a bunch of allocations.\n+  for (int i = 0; i < num_allocs; i ++) {\n+    if (os::random() % 10 == 0) {\n+      ar.Afree(ptrs[i], sizes[i]);\n+      \/\/ In debug builds the free should have filled the space with badResourceValue\n+      DEBUG_ONLY(ASSERT_RANGE_IS_MARKED_WITH(ptrs[i], sizes[i], badResourceValue));\n+      ptrs[i] = NULL;\n+    }\n+  }\n+\n+  \/\/ Check test pattern again\n+  for (int i = 0; i < num_allocs; i ++) {\n+    ASSERT_RANGE_IS_MARKED_WITH(ptrs[i], sizes[i], canary(i));\n+  }\n+\n+  FREE_C_HEAP_ARRAY(char*, ptrs);\n+  FREE_C_HEAP_ARRAY(size_t, sizes);\n+  FREE_C_HEAP_ARRAY(size_t, alignments);\n+\n+}\n+\n+TEST(Arena, mixed_alignment_allocation) {\n+  \/\/ Test that mixed alignment allocations work and provide allocations with the correct\n+  \/\/ alignment\n+  Arena ar(mtTest);\n+  void* p1 = ar.AmallocWords(BytesPerWord);\n+  void* p2 = ar.Amalloc(BytesPerLong);\n+  ASSERT_NOT_NULL(p1);\n+  ASSERT_TRUE(is_aligned(p1, BytesPerWord));\n+  ASSERT_NOT_NULL(p2);\n+  ASSERT_TRUE(is_aligned(p2, BytesPerLong));\n+}\n","filename":"test\/hotspot\/gtest\/memory\/test_arena.cpp","additions":390,"deletions":0,"binary":false,"changes":390,"status":"added"},{"patch":"@@ -101,1 +101,2 @@\n-  gtest\/LargePageGtests.java\n+  gtest\/LargePageGtests.java \\\n+  gtest\/ArenaGtests.java\n","filename":"test\/hotspot\/jtreg\/TEST.groups","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -0,0 +1,41 @@\n+\/*\n+ * Copyright (c) 2020, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020 SAP SE. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+\/*\n+ * This runs the parts of the gtest which test hotspot arena coding with UseMallocOnly.\n+ * (No point in testing the standard configuration since that gets tested as part of\n+ *  the normal tests)\n+ * We only execute them in debug, since without asserts they are a bit useless.\n+ *\/\n+\n+\/* @test\n+ * @summary Run arena tests with UseMallocOnly\n+ * @requires vm.debug\n+ * @library \/test\/lib\n+ * @modules java.base\/jdk.internal.misc\n+ *          java.xml\n+ * @requires vm.flagless\n+ * @run main\/native GTestWrapper --gtest_filter=Arena* -XX:+UseMallocOnly\n+ *\/\n","filename":"test\/hotspot\/jtreg\/gtest\/ArenaGtests.java","additions":41,"deletions":0,"binary":false,"changes":41,"status":"added"}]}