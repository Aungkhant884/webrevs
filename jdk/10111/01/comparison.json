{"files":[{"patch":"@@ -27,2 +27,0 @@\n-#include \"asm\/macroAssembler.inline.hpp\"\n-#include \"ci\/ciUtilities.hpp\"\n@@ -34,1 +32,0 @@\n-#include \"interpreter\/interpreter.hpp\"\n@@ -36,3 +33,0 @@\n-#include \"nativeInst_x86.hpp\"\n-#include \"oops\/instanceOop.hpp\"\n-#include \"oops\/method.hpp\"\n@@ -40,1 +34,0 @@\n-#include \"oops\/oop.inline.hpp\"\n@@ -42,1 +35,0 @@\n-#include \"prims\/methodHandles.hpp\"\n@@ -44,4 +36,0 @@\n-#include \"runtime\/continuation.hpp\"\n-#include \"runtime\/continuationEntry.inline.hpp\"\n-#include \"runtime\/frame.inline.hpp\"\n-#include \"runtime\/handles.inline.hpp\"\n@@ -50,1 +38,0 @@\n-#include \"runtime\/stubCodeGenerator.hpp\"\n@@ -52,0 +39,1 @@\n+#include \"stubGenerator_x86_64.hpp\"\n@@ -71,1 +59,0 @@\n-#define a__ ((Assembler*)_masm)->\n@@ -77,1 +64,1 @@\n-#endif\n+#endif \/\/ PRODUCT\n@@ -80,6 +67,0 @@\n-const int MXCSR_MASK = 0xFFC0;  \/\/ Mask out any pending exceptions\n-\n-\/\/ Stub Code definitions\n-\n-class StubGenerator: public StubCodeGenerator {\n- private:\n@@ -90,3 +71,0 @@\n-  void inc_counter_np(int& counter, Register rscratch) {\n-    __ incrementl(ExternalAddress((address)&counter), rscratch);\n-  }\n@@ -94,75 +72,2 @@\n-  BLOCK_COMMENT(\"inc_counter \" #counter); \\\n-  inc_counter_np(counter, rscratch);\n-#endif\n-\n-  \/\/ Call stubs are used to call Java from C\n-  \/\/\n-  \/\/ Linux Arguments:\n-  \/\/    c_rarg0:   call wrapper address                   address\n-  \/\/    c_rarg1:   result                                 address\n-  \/\/    c_rarg2:   result type                            BasicType\n-  \/\/    c_rarg3:   method                                 Method*\n-  \/\/    c_rarg4:   (interpreter) entry point              address\n-  \/\/    c_rarg5:   parameters                             intptr_t*\n-  \/\/    16(rbp): parameter size (in words)              int\n-  \/\/    24(rbp): thread                                 Thread*\n-  \/\/\n-  \/\/     [ return_from_Java     ] <--- rsp\n-  \/\/     [ argument word n      ]\n-  \/\/      ...\n-  \/\/ -12 [ argument word 1      ]\n-  \/\/ -11 [ saved r15            ] <--- rsp_after_call\n-  \/\/ -10 [ saved r14            ]\n-  \/\/  -9 [ saved r13            ]\n-  \/\/  -8 [ saved r12            ]\n-  \/\/  -7 [ saved rbx            ]\n-  \/\/  -6 [ call wrapper         ]\n-  \/\/  -5 [ result               ]\n-  \/\/  -4 [ result type          ]\n-  \/\/  -3 [ method               ]\n-  \/\/  -2 [ entry point          ]\n-  \/\/  -1 [ parameters           ]\n-  \/\/   0 [ saved rbp            ] <--- rbp\n-  \/\/   1 [ return address       ]\n-  \/\/   2 [ parameter size       ]\n-  \/\/   3 [ thread               ]\n-  \/\/\n-  \/\/ Windows Arguments:\n-  \/\/    c_rarg0:   call wrapper address                   address\n-  \/\/    c_rarg1:   result                                 address\n-  \/\/    c_rarg2:   result type                            BasicType\n-  \/\/    c_rarg3:   method                                 Method*\n-  \/\/    48(rbp): (interpreter) entry point              address\n-  \/\/    56(rbp): parameters                             intptr_t*\n-  \/\/    64(rbp): parameter size (in words)              int\n-  \/\/    72(rbp): thread                                 Thread*\n-  \/\/\n-  \/\/     [ return_from_Java     ] <--- rsp\n-  \/\/     [ argument word n      ]\n-  \/\/      ...\n-  \/\/ -60 [ argument word 1      ]\n-  \/\/ -59 [ saved xmm31          ] <--- rsp after_call\n-  \/\/     [ saved xmm16-xmm30    ] (EVEX enabled, else the space is blank)\n-  \/\/ -27 [ saved xmm15          ]\n-  \/\/     [ saved xmm7-xmm14     ]\n-  \/\/  -9 [ saved xmm6           ] (each xmm register takes 2 slots)\n-  \/\/  -7 [ saved r15            ]\n-  \/\/  -6 [ saved r14            ]\n-  \/\/  -5 [ saved r13            ]\n-  \/\/  -4 [ saved r12            ]\n-  \/\/  -3 [ saved rdi            ]\n-  \/\/  -2 [ saved rsi            ]\n-  \/\/  -1 [ saved rbx            ]\n-  \/\/   0 [ saved rbp            ] <--- rbp\n-  \/\/   1 [ return address       ]\n-  \/\/   2 [ call wrapper         ]\n-  \/\/   3 [ result               ]\n-  \/\/   4 [ result type          ]\n-  \/\/   5 [ method               ]\n-  \/\/   6 [ entry point          ]\n-  \/\/   7 [ parameters           ]\n-  \/\/   8 [ parameter size       ]\n-  \/\/   9 [ thread               ]\n-  \/\/\n-  \/\/    Windows reserves the callers stack space for arguments 1-4.\n-  \/\/    We spill c_rarg0-c_rarg3 to this space.\n+BLOCK_COMMENT(\"inc_counter \" #counter); \\\n+inc_counter_np(_masm, counter, rscratch);\n@@ -170,44 +75,3 @@\n-  \/\/ Call stub stack layout word offsets from rbp\n-  enum call_stub_layout {\n-#ifdef _WIN64\n-    xmm_save_first     = 6,  \/\/ save from xmm6\n-    xmm_save_last      = 31, \/\/ to xmm31\n-    xmm_save_base      = -9,\n-    rsp_after_call_off = xmm_save_base - 2 * (xmm_save_last - xmm_save_first), \/\/ -27\n-    r15_off            = -7,\n-    r14_off            = -6,\n-    r13_off            = -5,\n-    r12_off            = -4,\n-    rdi_off            = -3,\n-    rsi_off            = -2,\n-    rbx_off            = -1,\n-    rbp_off            =  0,\n-    retaddr_off        =  1,\n-    call_wrapper_off   =  2,\n-    result_off         =  3,\n-    result_type_off    =  4,\n-    method_off         =  5,\n-    entry_point_off    =  6,\n-    parameters_off     =  7,\n-    parameter_size_off =  8,\n-    thread_off         =  9\n-#else\n-    rsp_after_call_off = -12,\n-    mxcsr_off          = rsp_after_call_off,\n-    r15_off            = -11,\n-    r14_off            = -10,\n-    r13_off            = -9,\n-    r12_off            = -8,\n-    rbx_off            = -7,\n-    call_wrapper_off   = -6,\n-    result_off         = -5,\n-    result_type_off    = -4,\n-    method_off         = -3,\n-    entry_point_off    = -2,\n-    parameters_off     = -1,\n-    rbp_off            =  0,\n-    retaddr_off        =  1,\n-    parameter_size_off =  2,\n-    thread_off         =  3\n-#endif\n-  };\n+static void inc_counter_np(MacroAssembler* _masm, int& counter, Register rscratch) {\n+  __ incrementl(ExternalAddress((address)&counter), rscratch);\n+}\n@@ -215,4 +79,10 @@\n-#ifdef _WIN64\n-  Address xmm_save(int reg) {\n-    assert(reg >= xmm_save_first && reg <= xmm_save_last, \"XMM register number out of range\");\n-    return Address(rbp, (xmm_save_base - (reg - xmm_save_first) * 2) * wordSize);\n+static int& get_profile_ctr(int shift) {\n+  if (shift == 0) {\n+    return SharedRuntime::_jbyte_array_copy_ctr;\n+  } else if (shift == 1) {\n+    return SharedRuntime::_jshort_array_copy_ctr;\n+  } else if (shift == 2) {\n+    return SharedRuntime::_jint_array_copy_ctr;\n+  } else {\n+    assert(shift == 3, \"\");\n+    return SharedRuntime::_jlong_array_copy_ctr;\n@@ -220,1 +90,73 @@\n-#endif\n+}\n+#endif \/\/ !PRODUCT\n+\n+\/\/\n+\/\/ Linux Arguments:\n+\/\/    c_rarg0:   call wrapper address                   address\n+\/\/    c_rarg1:   result                                 address\n+\/\/    c_rarg2:   result type                            BasicType\n+\/\/    c_rarg3:   method                                 Method*\n+\/\/    c_rarg4:   (interpreter) entry point              address\n+\/\/    c_rarg5:   parameters                             intptr_t*\n+\/\/    16(rbp): parameter size (in words)              int\n+\/\/    24(rbp): thread                                 Thread*\n+\/\/\n+\/\/     [ return_from_Java     ] <--- rsp\n+\/\/     [ argument word n      ]\n+\/\/      ...\n+\/\/ -12 [ argument word 1      ]\n+\/\/ -11 [ saved r15            ] <--- rsp_after_call\n+\/\/ -10 [ saved r14            ]\n+\/\/  -9 [ saved r13            ]\n+\/\/  -8 [ saved r12            ]\n+\/\/  -7 [ saved rbx            ]\n+\/\/  -6 [ call wrapper         ]\n+\/\/  -5 [ result               ]\n+\/\/  -4 [ result type          ]\n+\/\/  -3 [ method               ]\n+\/\/  -2 [ entry point          ]\n+\/\/  -1 [ parameters           ]\n+\/\/   0 [ saved rbp            ] <--- rbp\n+\/\/   1 [ return address       ]\n+\/\/   2 [ parameter size       ]\n+\/\/   3 [ thread               ]\n+\/\/\n+\/\/ Windows Arguments:\n+\/\/    c_rarg0:   call wrapper address                   address\n+\/\/    c_rarg1:   result                                 address\n+\/\/    c_rarg2:   result type                            BasicType\n+\/\/    c_rarg3:   method                                 Method*\n+\/\/    48(rbp): (interpreter) entry point              address\n+\/\/    56(rbp): parameters                             intptr_t*\n+\/\/    64(rbp): parameter size (in words)              int\n+\/\/    72(rbp): thread                                 Thread*\n+\/\/\n+\/\/     [ return_from_Java     ] <--- rsp\n+\/\/     [ argument word n      ]\n+\/\/      ...\n+\/\/ -60 [ argument word 1      ]\n+\/\/ -59 [ saved xmm31          ] <--- rsp after_call\n+\/\/     [ saved xmm16-xmm30    ] (EVEX enabled, else the space is blank)\n+\/\/ -27 [ saved xmm15          ]\n+\/\/     [ saved xmm7-xmm14     ]\n+\/\/  -9 [ saved xmm6           ] (each xmm register takes 2 slots)\n+\/\/  -7 [ saved r15            ]\n+\/\/  -6 [ saved r14            ]\n+\/\/  -5 [ saved r13            ]\n+\/\/  -4 [ saved r12            ]\n+\/\/  -3 [ saved rdi            ]\n+\/\/  -2 [ saved rsi            ]\n+\/\/  -1 [ saved rbx            ]\n+\/\/   0 [ saved rbp            ] <--- rbp\n+\/\/   1 [ return address       ]\n+\/\/   2 [ call wrapper         ]\n+\/\/   3 [ result               ]\n+\/\/   4 [ result type          ]\n+\/\/   5 [ method               ]\n+\/\/   6 [ entry point          ]\n+\/\/   7 [ parameters           ]\n+\/\/   8 [ parameter size       ]\n+\/\/   9 [ thread               ]\n+\/\/\n+\/\/    Windows reserves the callers stack space for arguments 1-4.\n+\/\/    We spill c_rarg0-c_rarg3 to this space.\n@@ -222,32 +164,85 @@\n-  address generate_call_stub(address& return_address) {\n-    assert((int)frame::entry_frame_after_call_words == -(int)rsp_after_call_off + 1 &&\n-           (int)frame::entry_frame_call_wrapper_offset == (int)call_wrapper_off,\n-           \"adjust this code\");\n-    StubCodeMark mark(this, \"StubRoutines\", \"call_stub\");\n-    address start = __ pc();\n-\n-    \/\/ same as in generate_catch_exception()!\n-    const Address rsp_after_call(rbp, rsp_after_call_off * wordSize);\n-\n-    const Address call_wrapper  (rbp, call_wrapper_off   * wordSize);\n-    const Address result        (rbp, result_off         * wordSize);\n-    const Address result_type   (rbp, result_type_off    * wordSize);\n-    const Address method        (rbp, method_off         * wordSize);\n-    const Address entry_point   (rbp, entry_point_off    * wordSize);\n-    const Address parameters    (rbp, parameters_off     * wordSize);\n-    const Address parameter_size(rbp, parameter_size_off * wordSize);\n-\n-    \/\/ same as in generate_catch_exception()!\n-    const Address thread        (rbp, thread_off         * wordSize);\n-\n-    const Address r15_save(rbp, r15_off * wordSize);\n-    const Address r14_save(rbp, r14_off * wordSize);\n-    const Address r13_save(rbp, r13_off * wordSize);\n-    const Address r12_save(rbp, r12_off * wordSize);\n-    const Address rbx_save(rbp, rbx_off * wordSize);\n-\n-    \/\/ stub code\n-    __ enter();\n-    __ subptr(rsp, -rsp_after_call_off * wordSize);\n-\n-    \/\/ save register parameters\n+\/\/ Call stub stack layout word offsets from rbp\n+#ifdef _WIN64\n+enum call_stub_layout {\n+  xmm_save_first     = 6,  \/\/ save from xmm6\n+  xmm_save_last      = 31, \/\/ to xmm31\n+  xmm_save_base      = -9,\n+  rsp_after_call_off = xmm_save_base - 2 * (xmm_save_last - xmm_save_first), \/\/ -27\n+  r15_off            = -7,\n+  r14_off            = -6,\n+  r13_off            = -5,\n+  r12_off            = -4,\n+  rdi_off            = -3,\n+  rsi_off            = -2,\n+  rbx_off            = -1,\n+  rbp_off            =  0,\n+  retaddr_off        =  1,\n+  call_wrapper_off   =  2,\n+  result_off         =  3,\n+  result_type_off    =  4,\n+  method_off         =  5,\n+  entry_point_off    =  6,\n+  parameters_off     =  7,\n+  parameter_size_off =  8,\n+  thread_off         =  9\n+};\n+\n+static Address xmm_save(int reg) {\n+  assert(reg >= xmm_save_first && reg <= xmm_save_last, \"XMM register number out of range\");\n+  return Address(rbp, (xmm_save_base - (reg - xmm_save_first) * 2) * wordSize);\n+}\n+#else \/\/ !_WIN64\n+enum call_stub_layout {\n+  rsp_after_call_off = -12,\n+  mxcsr_off          = rsp_after_call_off,\n+  r15_off            = -11,\n+  r14_off            = -10,\n+  r13_off            = -9,\n+  r12_off            = -8,\n+  rbx_off            = -7,\n+  call_wrapper_off   = -6,\n+  result_off         = -5,\n+  result_type_off    = -4,\n+  method_off         = -3,\n+  entry_point_off    = -2,\n+  parameters_off     = -1,\n+  rbp_off            =  0,\n+  retaddr_off        =  1,\n+  parameter_size_off =  2,\n+  thread_off         =  3\n+};\n+#endif \/\/ _WIN64\n+\n+address StubGenerator::generate_call_stub(address& return_address) {\n+\n+  assert((int)frame::entry_frame_after_call_words == -(int)rsp_after_call_off + 1 &&\n+         (int)frame::entry_frame_call_wrapper_offset == (int)call_wrapper_off,\n+         \"adjust this code\");\n+  StubCodeMark mark(this, \"StubRoutines\", \"call_stub\");\n+  address start = __ pc();\n+\n+  \/\/ same as in generate_catch_exception()!\n+  const Address rsp_after_call(rbp, rsp_after_call_off * wordSize);\n+\n+  const Address call_wrapper  (rbp, call_wrapper_off   * wordSize);\n+  const Address result        (rbp, result_off         * wordSize);\n+  const Address result_type   (rbp, result_type_off    * wordSize);\n+  const Address method        (rbp, method_off         * wordSize);\n+  const Address entry_point   (rbp, entry_point_off    * wordSize);\n+  const Address parameters    (rbp, parameters_off     * wordSize);\n+  const Address parameter_size(rbp, parameter_size_off * wordSize);\n+\n+  \/\/ same as in generate_catch_exception()!\n+  const Address thread        (rbp, thread_off         * wordSize);\n+\n+  const Address r15_save(rbp, r15_off * wordSize);\n+  const Address r14_save(rbp, r14_off * wordSize);\n+  const Address r13_save(rbp, r13_off * wordSize);\n+  const Address r12_save(rbp, r12_off * wordSize);\n+  const Address rbx_save(rbp, rbx_off * wordSize);\n+\n+  \/\/ stub code\n+  __ enter();\n+  __ subptr(rsp, -rsp_after_call_off * wordSize);\n+\n+  \/\/ save register parameters\n@@ -255,2 +250,2 @@\n-    __ movptr(parameters,   c_rarg5); \/\/ parameters\n-    __ movptr(entry_point,  c_rarg4); \/\/ entry_point\n+  __ movptr(parameters,   c_rarg5); \/\/ parameters\n+  __ movptr(entry_point,  c_rarg4); \/\/ entry_point\n@@ -259,4 +254,4 @@\n-    __ movptr(method,       c_rarg3); \/\/ method\n-    __ movl(result_type,  c_rarg2);   \/\/ result type\n-    __ movptr(result,       c_rarg1); \/\/ result\n-    __ movptr(call_wrapper, c_rarg0); \/\/ call wrapper\n+  __ movptr(method,       c_rarg3); \/\/ method\n+  __ movl(result_type,  c_rarg2);   \/\/ result type\n+  __ movptr(result,       c_rarg1); \/\/ result\n+  __ movptr(call_wrapper, c_rarg0); \/\/ call wrapper\n@@ -264,6 +259,6 @@\n-    \/\/ save regs belonging to calling function\n-    __ movptr(rbx_save, rbx);\n-    __ movptr(r12_save, r12);\n-    __ movptr(r13_save, r13);\n-    __ movptr(r14_save, r14);\n-    __ movptr(r15_save, r15);\n+  \/\/ save regs belonging to calling function\n+  __ movptr(rbx_save, rbx);\n+  __ movptr(r12_save, r12);\n+  __ movptr(r13_save, r13);\n+  __ movptr(r14_save, r14);\n+  __ movptr(r15_save, r15);\n@@ -272,3 +267,7 @@\n-    int last_reg = 15;\n-    if (UseAVX > 2) {\n-      last_reg = 31;\n+  int last_reg = 15;\n+  if (UseAVX > 2) {\n+    last_reg = 31;\n+  }\n+  if (VM_Version::supports_evex()) {\n+    for (int i = xmm_save_first; i <= last_reg; i++) {\n+      __ vextractf32x4(xmm_save(i), as_XMMRegister(i), 0);\n@@ -276,8 +275,3 @@\n-    if (VM_Version::supports_evex()) {\n-      for (int i = xmm_save_first; i <= last_reg; i++) {\n-        __ vextractf32x4(xmm_save(i), as_XMMRegister(i), 0);\n-      }\n-    } else {\n-      for (int i = xmm_save_first; i <= last_reg; i++) {\n-        __ movdqu(xmm_save(i), as_XMMRegister(i));\n-      }\n+  } else {\n+    for (int i = xmm_save_first; i <= last_reg; i++) {\n+      __ movdqu(xmm_save(i), as_XMMRegister(i));\n@@ -285,0 +279,1 @@\n+  }\n@@ -286,2 +281,2 @@\n-    const Address rdi_save(rbp, rdi_off * wordSize);\n-    const Address rsi_save(rbp, rsi_off * wordSize);\n+  const Address rdi_save(rbp, rdi_off * wordSize);\n+  const Address rsi_save(rbp, rsi_off * wordSize);\n@@ -289,2 +284,2 @@\n-    __ movptr(rsi_save, rsi);\n-    __ movptr(rdi_save, rdi);\n+  __ movptr(rsi_save, rsi);\n+  __ movptr(rdi_save, rdi);\n@@ -292,12 +287,12 @@\n-    const Address mxcsr_save(rbp, mxcsr_off * wordSize);\n-    {\n-      Label skip_ldmx;\n-      __ stmxcsr(mxcsr_save);\n-      __ movl(rax, mxcsr_save);\n-      __ andl(rax, MXCSR_MASK);    \/\/ Only check control and mask bits\n-      ExternalAddress mxcsr_std(StubRoutines::x86::addr_mxcsr_std());\n-      __ cmp32(rax, mxcsr_std, rscratch1);\n-      __ jcc(Assembler::equal, skip_ldmx);\n-      __ ldmxcsr(mxcsr_std, rscratch1);\n-      __ bind(skip_ldmx);\n-    }\n+  const Address mxcsr_save(rbp, mxcsr_off * wordSize);\n+  {\n+    Label skip_ldmx;\n+    __ stmxcsr(mxcsr_save);\n+    __ movl(rax, mxcsr_save);\n+    __ andl(rax, 0xFFC0); \/\/ Mask out any pending exceptions (only check control and mask bits)\n+    ExternalAddress mxcsr_std(StubRoutines::x86::addr_mxcsr_std());\n+    __ cmp32(rax, mxcsr_std, rscratch1);\n+    __ jcc(Assembler::equal, skip_ldmx);\n+    __ ldmxcsr(mxcsr_std, rscratch1);\n+    __ bind(skip_ldmx);\n+  }\n@@ -306,3 +301,3 @@\n-    \/\/ Load up thread register\n-    __ movptr(r15_thread, thread);\n-    __ reinit_heapbase();\n+  \/\/ Load up thread register\n+  __ movptr(r15_thread, thread);\n+  __ reinit_heapbase();\n@@ -311,8 +306,8 @@\n-    \/\/ make sure we have no pending exceptions\n-    {\n-      Label L;\n-      __ cmpptr(Address(r15_thread, Thread::pending_exception_offset()), NULL_WORD);\n-      __ jcc(Assembler::equal, L);\n-      __ stop(\"StubRoutines::call_stub: entered with pending exception\");\n-      __ bind(L);\n-    }\n+  \/\/ make sure we have no pending exceptions\n+  {\n+    Label L;\n+    __ cmpptr(Address(r15_thread, Thread::pending_exception_offset()), NULL_WORD);\n+    __ jcc(Assembler::equal, L);\n+    __ stop(\"StubRoutines::call_stub: entered with pending exception\");\n+    __ bind(L);\n+  }\n@@ -321,49 +316,49 @@\n-    \/\/ pass parameters if any\n-    BLOCK_COMMENT(\"pass parameters if any\");\n-    Label parameters_done;\n-    __ movl(c_rarg3, parameter_size);\n-    __ testl(c_rarg3, c_rarg3);\n-    __ jcc(Assembler::zero, parameters_done);\n-\n-    Label loop;\n-    __ movptr(c_rarg2, parameters);       \/\/ parameter pointer\n-    __ movl(c_rarg1, c_rarg3);            \/\/ parameter counter is in c_rarg1\n-    __ BIND(loop);\n-    __ movptr(rax, Address(c_rarg2, 0));\/\/ get parameter\n-    __ addptr(c_rarg2, wordSize);       \/\/ advance to next parameter\n-    __ decrementl(c_rarg1);             \/\/ decrement counter\n-    __ push(rax);                       \/\/ pass parameter\n-    __ jcc(Assembler::notZero, loop);\n-\n-    \/\/ call Java function\n-    __ BIND(parameters_done);\n-    __ movptr(rbx, method);             \/\/ get Method*\n-    __ movptr(c_rarg1, entry_point);    \/\/ get entry_point\n-    __ mov(r13, rsp);                   \/\/ set sender sp\n-    BLOCK_COMMENT(\"call Java function\");\n-    __ call(c_rarg1);\n-\n-    BLOCK_COMMENT(\"call_stub_return_address:\");\n-    return_address = __ pc();\n-\n-    \/\/ store result depending on type (everything that is not\n-    \/\/ T_OBJECT, T_LONG, T_FLOAT or T_DOUBLE is treated as T_INT)\n-    __ movptr(c_rarg0, result);\n-    Label is_long, is_float, is_double, exit;\n-    __ movl(c_rarg1, result_type);\n-    __ cmpl(c_rarg1, T_OBJECT);\n-    __ jcc(Assembler::equal, is_long);\n-    __ cmpl(c_rarg1, T_LONG);\n-    __ jcc(Assembler::equal, is_long);\n-    __ cmpl(c_rarg1, T_FLOAT);\n-    __ jcc(Assembler::equal, is_float);\n-    __ cmpl(c_rarg1, T_DOUBLE);\n-    __ jcc(Assembler::equal, is_double);\n-\n-    \/\/ handle T_INT case\n-    __ movl(Address(c_rarg0, 0), rax);\n-\n-    __ BIND(exit);\n-\n-    \/\/ pop parameters\n-    __ lea(rsp, rsp_after_call);\n+  \/\/ pass parameters if any\n+  BLOCK_COMMENT(\"pass parameters if any\");\n+  Label parameters_done;\n+  __ movl(c_rarg3, parameter_size);\n+  __ testl(c_rarg3, c_rarg3);\n+  __ jcc(Assembler::zero, parameters_done);\n+\n+  Label loop;\n+  __ movptr(c_rarg2, parameters);       \/\/ parameter pointer\n+  __ movl(c_rarg1, c_rarg3);            \/\/ parameter counter is in c_rarg1\n+  __ BIND(loop);\n+  __ movptr(rax, Address(c_rarg2, 0));\/\/ get parameter\n+  __ addptr(c_rarg2, wordSize);       \/\/ advance to next parameter\n+  __ decrementl(c_rarg1);             \/\/ decrement counter\n+  __ push(rax);                       \/\/ pass parameter\n+  __ jcc(Assembler::notZero, loop);\n+\n+  \/\/ call Java function\n+  __ BIND(parameters_done);\n+  __ movptr(rbx, method);             \/\/ get Method*\n+  __ movptr(c_rarg1, entry_point);    \/\/ get entry_point\n+  __ mov(r13, rsp);                   \/\/ set sender sp\n+  BLOCK_COMMENT(\"call Java function\");\n+  __ call(c_rarg1);\n+\n+  BLOCK_COMMENT(\"call_stub_return_address:\");\n+  return_address = __ pc();\n+\n+  \/\/ store result depending on type (everything that is not\n+  \/\/ T_OBJECT, T_LONG, T_FLOAT or T_DOUBLE is treated as T_INT)\n+  __ movptr(c_rarg0, result);\n+  Label is_long, is_float, is_double, exit;\n+  __ movl(c_rarg1, result_type);\n+  __ cmpl(c_rarg1, T_OBJECT);\n+  __ jcc(Assembler::equal, is_long);\n+  __ cmpl(c_rarg1, T_LONG);\n+  __ jcc(Assembler::equal, is_long);\n+  __ cmpl(c_rarg1, T_FLOAT);\n+  __ jcc(Assembler::equal, is_float);\n+  __ cmpl(c_rarg1, T_DOUBLE);\n+  __ jcc(Assembler::equal, is_double);\n+\n+  \/\/ handle T_INT case\n+  __ movl(Address(c_rarg0, 0), rax);\n+\n+  __ BIND(exit);\n+\n+  \/\/ pop parameters\n+  __ lea(rsp, rsp_after_call);\n@@ -372,17 +367,17 @@\n-    \/\/ verify that threads correspond\n-    {\n-     Label L1, L2, L3;\n-      __ cmpptr(r15_thread, thread);\n-      __ jcc(Assembler::equal, L1);\n-      __ stop(\"StubRoutines::call_stub: r15_thread is corrupted\");\n-      __ bind(L1);\n-      __ get_thread(rbx);\n-      __ cmpptr(r15_thread, thread);\n-      __ jcc(Assembler::equal, L2);\n-      __ stop(\"StubRoutines::call_stub: r15_thread is modified by call\");\n-      __ bind(L2);\n-      __ cmpptr(r15_thread, rbx);\n-      __ jcc(Assembler::equal, L3);\n-      __ stop(\"StubRoutines::call_stub: threads must correspond\");\n-      __ bind(L3);\n-    }\n+  \/\/ verify that threads correspond\n+  {\n+   Label L1, L2, L3;\n+    __ cmpptr(r15_thread, thread);\n+    __ jcc(Assembler::equal, L1);\n+    __ stop(\"StubRoutines::call_stub: r15_thread is corrupted\");\n+    __ bind(L1);\n+    __ get_thread(rbx);\n+    __ cmpptr(r15_thread, thread);\n+    __ jcc(Assembler::equal, L2);\n+    __ stop(\"StubRoutines::call_stub: r15_thread is modified by call\");\n+    __ bind(L2);\n+    __ cmpptr(r15_thread, rbx);\n+    __ jcc(Assembler::equal, L3);\n+    __ stop(\"StubRoutines::call_stub: threads must correspond\");\n+    __ bind(L3);\n+  }\n@@ -391,1 +386,1 @@\n-    __ pop_cont_fastpath();\n+  __ pop_cont_fastpath();\n@@ -393,1 +388,1 @@\n-    \/\/ restore regs belonging to calling function\n+  \/\/ restore regs belonging to calling function\n@@ -395,9 +390,8 @@\n-    \/\/ emit the restores for xmm regs\n-    if (VM_Version::supports_evex()) {\n-      for (int i = xmm_save_first; i <= last_reg; i++) {\n-        __ vinsertf32x4(as_XMMRegister(i), as_XMMRegister(i), xmm_save(i), 0);\n-      }\n-    } else {\n-      for (int i = xmm_save_first; i <= last_reg; i++) {\n-        __ movdqu(as_XMMRegister(i), xmm_save(i));\n-      }\n+  \/\/ emit the restores for xmm regs\n+  if (VM_Version::supports_evex()) {\n+    for (int i = xmm_save_first; i <= last_reg; i++) {\n+      __ vinsertf32x4(as_XMMRegister(i), as_XMMRegister(i), xmm_save(i), 0);\n+    }\n+  } else {\n+    for (int i = xmm_save_first; i <= last_reg; i++) {\n+      __ movdqu(as_XMMRegister(i), xmm_save(i));\n@@ -405,0 +399,1 @@\n+  }\n@@ -406,5 +401,5 @@\n-    __ movptr(r15, r15_save);\n-    __ movptr(r14, r14_save);\n-    __ movptr(r13, r13_save);\n-    __ movptr(r12, r12_save);\n-    __ movptr(rbx, rbx_save);\n+  __ movptr(r15, r15_save);\n+  __ movptr(r14, r14_save);\n+  __ movptr(r13, r13_save);\n+  __ movptr(r12, r12_save);\n+  __ movptr(rbx, rbx_save);\n@@ -413,2 +408,2 @@\n-    __ movptr(rdi, rdi_save);\n-    __ movptr(rsi, rsi_save);\n+  __ movptr(rdi, rdi_save);\n+  __ movptr(rsi, rsi_save);\n@@ -416,1 +411,1 @@\n-    __ ldmxcsr(mxcsr_save);\n+  __ ldmxcsr(mxcsr_save);\n@@ -419,2 +414,2 @@\n-    \/\/ restore rsp\n-    __ addptr(rsp, -rsp_after_call_off * wordSize);\n+  \/\/ restore rsp\n+  __ addptr(rsp, -rsp_after_call_off * wordSize);\n@@ -422,4 +417,4 @@\n-    \/\/ return\n-    __ vzeroupper();\n-    __ pop(rbp);\n-    __ ret(0);\n+  \/\/ return\n+  __ vzeroupper();\n+  __ pop(rbp);\n+  __ ret(0);\n@@ -427,4 +422,4 @@\n-    \/\/ handle return types different from T_INT\n-    __ BIND(is_long);\n-    __ movq(Address(c_rarg0, 0), rax);\n-    __ jmp(exit);\n+  \/\/ handle return types different from T_INT\n+  __ BIND(is_long);\n+  __ movq(Address(c_rarg0, 0), rax);\n+  __ jmp(exit);\n@@ -432,3 +427,3 @@\n-    __ BIND(is_float);\n-    __ movflt(Address(c_rarg0, 0), xmm0);\n-    __ jmp(exit);\n+  __ BIND(is_float);\n+  __ movflt(Address(c_rarg0, 0), xmm0);\n+  __ jmp(exit);\n@@ -436,3 +431,3 @@\n-    __ BIND(is_double);\n-    __ movdbl(Address(c_rarg0, 0), xmm0);\n-    __ jmp(exit);\n+  __ BIND(is_double);\n+  __ movdbl(Address(c_rarg0, 0), xmm0);\n+  __ jmp(exit);\n@@ -440,2 +435,2 @@\n-    return start;\n-  }\n+  return start;\n+}\n@@ -443,11 +438,11 @@\n-  \/\/ Return point for a Java call if there's an exception thrown in\n-  \/\/ Java code.  The exception is caught and transformed into a\n-  \/\/ pending exception stored in JavaThread that can be tested from\n-  \/\/ within the VM.\n-  \/\/\n-  \/\/ Note: Usually the parameters are removed by the callee. In case\n-  \/\/ of an exception crossing an activation frame boundary, that is\n-  \/\/ not the case if the callee is compiled code => need to setup the\n-  \/\/ rsp.\n-  \/\/\n-  \/\/ rax: exception oop\n+\/\/ Return point for a Java call if there's an exception thrown in\n+\/\/ Java code.  The exception is caught and transformed into a\n+\/\/ pending exception stored in JavaThread that can be tested from\n+\/\/ within the VM.\n+\/\/\n+\/\/ Note: Usually the parameters are removed by the callee. In case\n+\/\/ of an exception crossing an activation frame boundary, that is\n+\/\/ not the case if the callee is compiled code => need to setup the\n+\/\/ rsp.\n+\/\/\n+\/\/ rax: exception oop\n@@ -455,3 +450,3 @@\n-  address generate_catch_exception() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"catch_exception\");\n-    address start = __ pc();\n+address StubGenerator::generate_catch_exception() {\n+  StubCodeMark mark(this, \"StubRoutines\", \"catch_exception\");\n+  address start = __ pc();\n@@ -459,3 +454,3 @@\n-    \/\/ same as in generate_call_stub():\n-    const Address rsp_after_call(rbp, rsp_after_call_off * wordSize);\n-    const Address thread        (rbp, thread_off         * wordSize);\n+  \/\/ same as in generate_call_stub():\n+  const Address rsp_after_call(rbp, rsp_after_call_off * wordSize);\n+  const Address thread        (rbp, thread_off         * wordSize);\n@@ -464,17 +459,17 @@\n-    \/\/ verify that threads correspond\n-    {\n-      Label L1, L2, L3;\n-      __ cmpptr(r15_thread, thread);\n-      __ jcc(Assembler::equal, L1);\n-      __ stop(\"StubRoutines::catch_exception: r15_thread is corrupted\");\n-      __ bind(L1);\n-      __ get_thread(rbx);\n-      __ cmpptr(r15_thread, thread);\n-      __ jcc(Assembler::equal, L2);\n-      __ stop(\"StubRoutines::catch_exception: r15_thread is modified by call\");\n-      __ bind(L2);\n-      __ cmpptr(r15_thread, rbx);\n-      __ jcc(Assembler::equal, L3);\n-      __ stop(\"StubRoutines::catch_exception: threads must correspond\");\n-      __ bind(L3);\n-    }\n+  \/\/ verify that threads correspond\n+  {\n+    Label L1, L2, L3;\n+    __ cmpptr(r15_thread, thread);\n+    __ jcc(Assembler::equal, L1);\n+    __ stop(\"StubRoutines::catch_exception: r15_thread is corrupted\");\n+    __ bind(L1);\n+    __ get_thread(rbx);\n+    __ cmpptr(r15_thread, thread);\n+    __ jcc(Assembler::equal, L2);\n+    __ stop(\"StubRoutines::catch_exception: r15_thread is modified by call\");\n+    __ bind(L2);\n+    __ cmpptr(r15_thread, rbx);\n+    __ jcc(Assembler::equal, L3);\n+    __ stop(\"StubRoutines::catch_exception: threads must correspond\");\n+    __ bind(L3);\n+  }\n@@ -483,2 +478,2 @@\n-    \/\/ set pending exception\n-    __ verify_oop(rax);\n+  \/\/ set pending exception\n+  __ verify_oop(rax);\n@@ -486,4 +481,4 @@\n-    __ movptr(Address(r15_thread, Thread::pending_exception_offset()), rax);\n-    __ lea(rscratch1, ExternalAddress((address)__FILE__));\n-    __ movptr(Address(r15_thread, Thread::exception_file_offset()), rscratch1);\n-    __ movl(Address(r15_thread, Thread::exception_line_offset()), (int)  __LINE__);\n+  __ movptr(Address(r15_thread, Thread::pending_exception_offset()), rax);\n+  __ lea(rscratch1, ExternalAddress((address)__FILE__));\n+  __ movptr(Address(r15_thread, Thread::exception_file_offset()), rscratch1);\n+  __ movl(Address(r15_thread, Thread::exception_line_offset()), (int)  __LINE__);\n@@ -491,4 +486,4 @@\n-    \/\/ complete return to VM\n-    assert(StubRoutines::_call_stub_return_address != NULL,\n-           \"_call_stub_return_address must have been generated before\");\n-    __ jump(RuntimeAddress(StubRoutines::_call_stub_return_address));\n+  \/\/ complete return to VM\n+  assert(StubRoutines::_call_stub_return_address != NULL,\n+         \"_call_stub_return_address must have been generated before\");\n+  __ jump(RuntimeAddress(StubRoutines::_call_stub_return_address));\n@@ -496,2 +491,2 @@\n-    return start;\n-  }\n+  return start;\n+}\n@@ -499,10 +494,10 @@\n-  \/\/ Continuation point for runtime calls returning with a pending\n-  \/\/ exception.  The pending exception check happened in the runtime\n-  \/\/ or native call stub.  The pending exception in Thread is\n-  \/\/ converted into a Java-level exception.\n-  \/\/\n-  \/\/ Contract with Java-level exception handlers:\n-  \/\/ rax: exception\n-  \/\/ rdx: throwing pc\n-  \/\/\n-  \/\/ NOTE: At entry of this stub, exception-pc must be on stack !!\n+\/\/ Continuation point for runtime calls returning with a pending\n+\/\/ exception.  The pending exception check happened in the runtime\n+\/\/ or native call stub.  The pending exception in Thread is\n+\/\/ converted into a Java-level exception.\n+\/\/\n+\/\/ Contract with Java-level exception handlers:\n+\/\/ rax: exception\n+\/\/ rdx: throwing pc\n+\/\/\n+\/\/ NOTE: At entry of this stub, exception-pc must be on stack !!\n@@ -510,3 +505,3 @@\n-  address generate_forward_exception() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"forward exception\");\n-    address start = __ pc();\n+address StubGenerator::generate_forward_exception() {\n+  StubCodeMark mark(this, \"StubRoutines\", \"forward exception\");\n+  address start = __ pc();\n@@ -514,8 +509,8 @@\n-    \/\/ Upon entry, the sp points to the return address returning into\n-    \/\/ Java (interpreted or compiled) code; i.e., the return address\n-    \/\/ becomes the throwing pc.\n-    \/\/\n-    \/\/ Arguments pushed before the runtime call are still on the stack\n-    \/\/ but the exception handler will reset the stack pointer ->\n-    \/\/ ignore them.  A potential result in registers can be ignored as\n-    \/\/ well.\n+  \/\/ Upon entry, the sp points to the return address returning into\n+  \/\/ Java (interpreted or compiled) code; i.e., the return address\n+  \/\/ becomes the throwing pc.\n+  \/\/\n+  \/\/ Arguments pushed before the runtime call are still on the stack\n+  \/\/ but the exception handler will reset the stack pointer ->\n+  \/\/ ignore them.  A potential result in registers can be ignored as\n+  \/\/ well.\n@@ -524,8 +519,8 @@\n-    \/\/ make sure this code is only executed if there is a pending exception\n-    {\n-      Label L;\n-      __ cmpptr(Address(r15_thread, Thread::pending_exception_offset()), NULL_WORD);\n-      __ jcc(Assembler::notEqual, L);\n-      __ stop(\"StubRoutines::forward exception: no pending exception (1)\");\n-      __ bind(L);\n-    }\n+  \/\/ make sure this code is only executed if there is a pending exception\n+  {\n+    Label L;\n+    __ cmpptr(Address(r15_thread, Thread::pending_exception_offset()), NULL_WORD);\n+    __ jcc(Assembler::notEqual, L);\n+    __ stop(\"StubRoutines::forward exception: no pending exception (1)\");\n+    __ bind(L);\n+  }\n@@ -534,7 +529,7 @@\n-    \/\/ compute exception handler into rbx\n-    __ movptr(c_rarg0, Address(rsp, 0));\n-    BLOCK_COMMENT(\"call exception_handler_for_return_address\");\n-    __ call_VM_leaf(CAST_FROM_FN_PTR(address,\n-                         SharedRuntime::exception_handler_for_return_address),\n-                    r15_thread, c_rarg0);\n-    __ mov(rbx, rax);\n+  \/\/ compute exception handler into rbx\n+  __ movptr(c_rarg0, Address(rsp, 0));\n+  BLOCK_COMMENT(\"call exception_handler_for_return_address\");\n+  __ call_VM_leaf(CAST_FROM_FN_PTR(address,\n+                       SharedRuntime::exception_handler_for_return_address),\n+                  r15_thread, c_rarg0);\n+  __ mov(rbx, rax);\n@@ -542,4 +537,4 @@\n-    \/\/ setup rax & rdx, remove return address & clear pending exception\n-    __ pop(rdx);\n-    __ movptr(rax, Address(r15_thread, Thread::pending_exception_offset()));\n-    __ movptr(Address(r15_thread, Thread::pending_exception_offset()), NULL_WORD);\n+  \/\/ setup rax & rdx, remove return address & clear pending exception\n+  __ pop(rdx);\n+  __ movptr(rax, Address(r15_thread, Thread::pending_exception_offset()));\n+  __ movptr(Address(r15_thread, Thread::pending_exception_offset()), NULL_WORD);\n@@ -548,8 +543,8 @@\n-    \/\/ make sure exception is set\n-    {\n-      Label L;\n-      __ testptr(rax, rax);\n-      __ jcc(Assembler::notEqual, L);\n-      __ stop(\"StubRoutines::forward exception: no pending exception (2)\");\n-      __ bind(L);\n-    }\n+  \/\/ make sure exception is set\n+  {\n+    Label L;\n+    __ testptr(rax, rax);\n+    __ jcc(Assembler::notEqual, L);\n+    __ stop(\"StubRoutines::forward exception: no pending exception (2)\");\n+    __ bind(L);\n+  }\n@@ -558,9 +553,6 @@\n-    \/\/ continue at exception handler (return address removed)\n-    \/\/ rax: exception\n-    \/\/ rbx: exception handler\n-    \/\/ rdx: throwing pc\n-    __ verify_oop(rax);\n-    __ jmp(rbx);\n-\n-    return start;\n-  }\n+  \/\/ continue at exception handler (return address removed)\n+  \/\/ rax: exception\n+  \/\/ rbx: exception handler\n+  \/\/ rdx: throwing pc\n+  __ verify_oop(rax);\n+  __ jmp(rbx);\n@@ -568,10 +560,2 @@\n-  \/\/ Support for intptr_t OrderAccess::fence()\n-  \/\/\n-  \/\/ Arguments :\n-  \/\/\n-  \/\/ Result:\n-  address generate_orderaccess_fence() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"orderaccess_fence\");\n-    address start = __ pc();\n-    __ membar(Assembler::StoreLoad);\n-    __ ret(0);\n+  return start;\n+}\n@@ -579,2 +563,8 @@\n-    return start;\n-  }\n+\/\/ Support for intptr_t OrderAccess::fence()\n+\/\/\n+\/\/ Arguments :\n+\/\/\n+\/\/ Result:\n+address StubGenerator::generate_orderaccess_fence() {\n+  StubCodeMark mark(this, \"StubRoutines\", \"orderaccess_fence\");\n+  address start = __ pc();\n@@ -582,0 +572,2 @@\n+  __ membar(Assembler::StoreLoad);\n+  __ ret(0);\n@@ -583,11 +575,2 @@\n-  \/\/ Support for intptr_t get_previous_sp()\n-  \/\/\n-  \/\/ This routine is used to find the previous stack pointer for the\n-  \/\/ caller.\n-  address generate_get_previous_sp() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"get_previous_sp\");\n-    address start = __ pc();\n-\n-    __ movptr(rax, rsp);\n-    __ addptr(rax, 8); \/\/ return address is at the top of the stack.\n-    __ ret(0);\n+  return start;\n+}\n@@ -595,2 +578,0 @@\n-    return start;\n-  }\n@@ -598,32 +579,7 @@\n-  \/\/----------------------------------------------------------------------------------------------------\n-  \/\/ Support for void verify_mxcsr()\n-  \/\/\n-  \/\/ This routine is used with -Xcheck:jni to verify that native\n-  \/\/ JNI code does not return to Java code without restoring the\n-  \/\/ MXCSR register to our expected state.\n-\n-  address generate_verify_mxcsr() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"verify_mxcsr\");\n-    address start = __ pc();\n-\n-    const Address mxcsr_save(rsp, 0);\n-\n-    if (CheckJNICalls) {\n-      Label ok_ret;\n-      ExternalAddress mxcsr_std(StubRoutines::x86::addr_mxcsr_std());\n-      __ push(rax);\n-      __ subptr(rsp, wordSize);      \/\/ allocate a temp location\n-      __ stmxcsr(mxcsr_save);\n-      __ movl(rax, mxcsr_save);\n-      __ andl(rax, MXCSR_MASK);    \/\/ Only check control and mask bits\n-      __ cmp32(rax, mxcsr_std, rscratch1);\n-      __ jcc(Assembler::equal, ok_ret);\n-\n-      __ warn(\"MXCSR changed by native JNI code, use -XX:+RestoreMXCSROnJNICall\");\n-\n-      __ ldmxcsr(mxcsr_std, rscratch1);\n-\n-      __ bind(ok_ret);\n-      __ addptr(rsp, wordSize);\n-      __ pop(rax);\n-    }\n+\/\/ Support for intptr_t get_previous_sp()\n+\/\/\n+\/\/ This routine is used to find the previous stack pointer for the\n+\/\/ caller.\n+address StubGenerator::generate_get_previous_sp() {\n+  StubCodeMark mark(this, \"StubRoutines\", \"get_previous_sp\");\n+  address start = __ pc();\n@@ -631,1 +587,3 @@\n-    __ ret(0);\n+  __ movptr(rax, rsp);\n+  __ addptr(rax, 8); \/\/ return address is at the top of the stack.\n+  __ ret(0);\n@@ -633,2 +591,2 @@\n-    return start;\n-  }\n+  return start;\n+}\n@@ -636,3 +594,6 @@\n-  address generate_f2i_fixup() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"f2i_fixup\");\n-    Address inout(rsp, 5 * wordSize); \/\/ return address + 4 saves\n+\/\/----------------------------------------------------------------------------------------------------\n+\/\/ Support for void verify_mxcsr()\n+\/\/\n+\/\/ This routine is used with -Xcheck:jni to verify that native\n+\/\/ JNI code does not return to Java code without restoring the\n+\/\/ MXCSR register to our expected state.\n@@ -640,1 +601,3 @@\n-    address start = __ pc();\n+address StubGenerator::generate_verify_mxcsr() {\n+  StubCodeMark mark(this, \"StubRoutines\", \"verify_mxcsr\");\n+  address start = __ pc();\n@@ -642,1 +605,1 @@\n-    Label L;\n+  const Address mxcsr_save(rsp, 0);\n@@ -644,0 +607,3 @@\n+  if (CheckJNICalls) {\n+    Label ok_ret;\n+    ExternalAddress mxcsr_std(StubRoutines::x86::addr_mxcsr_std());\n@@ -645,18 +611,6 @@\n-    __ push(c_rarg3);\n-    __ push(c_rarg2);\n-    __ push(c_rarg1);\n-\n-    __ movl(rax, 0x7f800000);\n-    __ xorl(c_rarg3, c_rarg3);\n-    __ movl(c_rarg2, inout);\n-    __ movl(c_rarg1, c_rarg2);\n-    __ andl(c_rarg1, 0x7fffffff);\n-    __ cmpl(rax, c_rarg1); \/\/ NaN? -> 0\n-    __ jcc(Assembler::negative, L);\n-    __ testl(c_rarg2, c_rarg2); \/\/ signed ? min_jint : max_jint\n-    __ movl(c_rarg3, 0x80000000);\n-    __ movl(rax, 0x7fffffff);\n-    __ cmovl(Assembler::positive, c_rarg3, rax);\n-\n-    __ bind(L);\n-    __ movptr(inout, c_rarg3);\n+    __ subptr(rsp, wordSize);      \/\/ allocate a temp location\n+    __ stmxcsr(mxcsr_save);\n+    __ movl(rax, mxcsr_save);\n+    __ andl(rax, 0xFFC0); \/\/ Mask out any pending exceptions (only check control and mask bits)\n+    __ cmp32(rax, mxcsr_std, rscratch1);\n+    __ jcc(Assembler::equal, ok_ret);\n@@ -664,4 +618,1 @@\n-    __ pop(c_rarg1);\n-    __ pop(c_rarg2);\n-    __ pop(c_rarg3);\n-    __ pop(rax);\n+    __ warn(\"MXCSR changed by native JNI code, use -XX:+RestoreMXCSROnJNICall\");\n@@ -669,1 +620,1 @@\n-    __ ret(0);\n+    __ ldmxcsr(mxcsr_std, rscratch1);\n@@ -671,1 +622,3 @@\n-    return start;\n+    __ bind(ok_ret);\n+    __ addptr(rsp, wordSize);\n+    __ pop(rax);\n@@ -674,4 +627,1 @@\n-  address generate_f2l_fixup() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"f2l_fixup\");\n-    Address inout(rsp, 5 * wordSize); \/\/ return address + 4 saves\n-    address start = __ pc();\n+  __ ret(0);\n@@ -679,1 +629,2 @@\n-    Label L;\n+  return start;\n+}\n@@ -681,16 +632,3 @@\n-    __ push(rax);\n-    __ push(c_rarg3);\n-    __ push(c_rarg2);\n-    __ push(c_rarg1);\n-\n-    __ movl(rax, 0x7f800000);\n-    __ xorl(c_rarg3, c_rarg3);\n-    __ movl(c_rarg2, inout);\n-    __ movl(c_rarg1, c_rarg2);\n-    __ andl(c_rarg1, 0x7fffffff);\n-    __ cmpl(rax, c_rarg1); \/\/ NaN? -> 0\n-    __ jcc(Assembler::negative, L);\n-    __ testl(c_rarg2, c_rarg2); \/\/ signed ? min_jlong : max_jlong\n-    __ mov64(c_rarg3, 0x8000000000000000);\n-    __ mov64(rax, 0x7fffffffffffffff);\n-    __ cmov(Assembler::positive, c_rarg3, rax);\n+address StubGenerator::generate_f2i_fixup() {\n+  StubCodeMark mark(this, \"StubRoutines\", \"f2i_fixup\");\n+  Address inout(rsp, 5 * wordSize); \/\/ return address + 4 saves\n@@ -698,2 +636,1 @@\n-    __ bind(L);\n-    __ movptr(inout, c_rarg3);\n+  address start = __ pc();\n@@ -701,4 +638,1 @@\n-    __ pop(c_rarg1);\n-    __ pop(c_rarg2);\n-    __ pop(c_rarg3);\n-    __ pop(rax);\n+  Label L;\n@@ -706,1 +640,4 @@\n-    __ ret(0);\n+  __ push(rax);\n+  __ push(c_rarg3);\n+  __ push(c_rarg2);\n+  __ push(c_rarg1);\n@@ -708,2 +645,11 @@\n-    return start;\n-  }\n+  __ movl(rax, 0x7f800000);\n+  __ xorl(c_rarg3, c_rarg3);\n+  __ movl(c_rarg2, inout);\n+  __ movl(c_rarg1, c_rarg2);\n+  __ andl(c_rarg1, 0x7fffffff);\n+  __ cmpl(rax, c_rarg1); \/\/ NaN? -> 0\n+  __ jcc(Assembler::negative, L);\n+  __ testl(c_rarg2, c_rarg2); \/\/ signed ? min_jint : max_jint\n+  __ movl(c_rarg3, 0x80000000);\n+  __ movl(rax, 0x7fffffff);\n+  __ cmovl(Assembler::positive, c_rarg3, rax);\n@@ -711,3 +657,2 @@\n-  address generate_d2i_fixup() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"d2i_fixup\");\n-    Address inout(rsp, 6 * wordSize); \/\/ return address + 5 saves\n+  __ bind(L);\n+  __ movptr(inout, c_rarg3);\n@@ -715,1 +660,4 @@\n-    address start = __ pc();\n+  __ pop(c_rarg1);\n+  __ pop(c_rarg2);\n+  __ pop(c_rarg3);\n+  __ pop(rax);\n@@ -717,1 +665,1 @@\n-    Label L;\n+  __ ret(0);\n@@ -719,24 +667,2 @@\n-    __ push(rax);\n-    __ push(c_rarg3);\n-    __ push(c_rarg2);\n-    __ push(c_rarg1);\n-    __ push(c_rarg0);\n-\n-    __ movl(rax, 0x7ff00000);\n-    __ movq(c_rarg2, inout);\n-    __ movl(c_rarg3, c_rarg2);\n-    __ mov(c_rarg1, c_rarg2);\n-    __ mov(c_rarg0, c_rarg2);\n-    __ negl(c_rarg3);\n-    __ shrptr(c_rarg1, 0x20);\n-    __ orl(c_rarg3, c_rarg2);\n-    __ andl(c_rarg1, 0x7fffffff);\n-    __ xorl(c_rarg2, c_rarg2);\n-    __ shrl(c_rarg3, 0x1f);\n-    __ orl(c_rarg1, c_rarg3);\n-    __ cmpl(rax, c_rarg1);\n-    __ jcc(Assembler::negative, L); \/\/ NaN -> 0\n-    __ testptr(c_rarg0, c_rarg0); \/\/ signed ? min_jint : max_jint\n-    __ movl(c_rarg2, 0x80000000);\n-    __ movl(rax, 0x7fffffff);\n-    __ cmov(Assembler::positive, c_rarg2, rax);\n+  return start;\n+}\n@@ -744,2 +670,4 @@\n-    __ bind(L);\n-    __ movptr(inout, c_rarg2);\n+address StubGenerator::generate_f2l_fixup() {\n+  StubCodeMark mark(this, \"StubRoutines\", \"f2l_fixup\");\n+  Address inout(rsp, 5 * wordSize); \/\/ return address + 4 saves\n+  address start = __ pc();\n@@ -747,5 +675,1 @@\n-    __ pop(c_rarg0);\n-    __ pop(c_rarg1);\n-    __ pop(c_rarg2);\n-    __ pop(c_rarg3);\n-    __ pop(rax);\n+  Label L;\n@@ -753,1 +677,4 @@\n-    __ ret(0);\n+  __ push(rax);\n+  __ push(c_rarg3);\n+  __ push(c_rarg2);\n+  __ push(c_rarg1);\n@@ -755,2 +682,11 @@\n-    return start;\n-  }\n+  __ movl(rax, 0x7f800000);\n+  __ xorl(c_rarg3, c_rarg3);\n+  __ movl(c_rarg2, inout);\n+  __ movl(c_rarg1, c_rarg2);\n+  __ andl(c_rarg1, 0x7fffffff);\n+  __ cmpl(rax, c_rarg1); \/\/ NaN? -> 0\n+  __ jcc(Assembler::negative, L);\n+  __ testl(c_rarg2, c_rarg2); \/\/ signed ? min_jlong : max_jlong\n+  __ mov64(c_rarg3, 0x8000000000000000);\n+  __ mov64(rax, 0x7fffffffffffffff);\n+  __ cmov(Assembler::positive, c_rarg3, rax);\n@@ -758,3 +694,2 @@\n-  address generate_d2l_fixup() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"d2l_fixup\");\n-    Address inout(rsp, 6 * wordSize); \/\/ return address + 5 saves\n+  __ bind(L);\n+  __ movptr(inout, c_rarg3);\n@@ -762,1 +697,4 @@\n-    address start = __ pc();\n+  __ pop(c_rarg1);\n+  __ pop(c_rarg2);\n+  __ pop(c_rarg3);\n+  __ pop(rax);\n@@ -764,1 +702,1 @@\n-    Label L;\n+  __ ret(0);\n@@ -766,24 +704,2 @@\n-    __ push(rax);\n-    __ push(c_rarg3);\n-    __ push(c_rarg2);\n-    __ push(c_rarg1);\n-    __ push(c_rarg0);\n-\n-    __ movl(rax, 0x7ff00000);\n-    __ movq(c_rarg2, inout);\n-    __ movl(c_rarg3, c_rarg2);\n-    __ mov(c_rarg1, c_rarg2);\n-    __ mov(c_rarg0, c_rarg2);\n-    __ negl(c_rarg3);\n-    __ shrptr(c_rarg1, 0x20);\n-    __ orl(c_rarg3, c_rarg2);\n-    __ andl(c_rarg1, 0x7fffffff);\n-    __ xorl(c_rarg2, c_rarg2);\n-    __ shrl(c_rarg3, 0x1f);\n-    __ orl(c_rarg1, c_rarg3);\n-    __ cmpl(rax, c_rarg1);\n-    __ jcc(Assembler::negative, L); \/\/ NaN -> 0\n-    __ testq(c_rarg0, c_rarg0); \/\/ signed ? min_jlong : max_jlong\n-    __ mov64(c_rarg2, 0x8000000000000000);\n-    __ mov64(rax, 0x7fffffffffffffff);\n-    __ cmovq(Assembler::positive, c_rarg2, rax);\n+  return start;\n+}\n@@ -791,2 +707,43 @@\n-    __ bind(L);\n-    __ movq(inout, c_rarg2);\n+address StubGenerator::generate_d2i_fixup() {\n+  StubCodeMark mark(this, \"StubRoutines\", \"d2i_fixup\");\n+  Address inout(rsp, 6 * wordSize); \/\/ return address + 5 saves\n+\n+  address start = __ pc();\n+\n+  Label L;\n+\n+  __ push(rax);\n+  __ push(c_rarg3);\n+  __ push(c_rarg2);\n+  __ push(c_rarg1);\n+  __ push(c_rarg0);\n+\n+  __ movl(rax, 0x7ff00000);\n+  __ movq(c_rarg2, inout);\n+  __ movl(c_rarg3, c_rarg2);\n+  __ mov(c_rarg1, c_rarg2);\n+  __ mov(c_rarg0, c_rarg2);\n+  __ negl(c_rarg3);\n+  __ shrptr(c_rarg1, 0x20);\n+  __ orl(c_rarg3, c_rarg2);\n+  __ andl(c_rarg1, 0x7fffffff);\n+  __ xorl(c_rarg2, c_rarg2);\n+  __ shrl(c_rarg3, 0x1f);\n+  __ orl(c_rarg1, c_rarg3);\n+  __ cmpl(rax, c_rarg1);\n+  __ jcc(Assembler::negative, L); \/\/ NaN -> 0\n+  __ testptr(c_rarg0, c_rarg0); \/\/ signed ? min_jint : max_jint\n+  __ movl(c_rarg2, 0x80000000);\n+  __ movl(rax, 0x7fffffff);\n+  __ cmov(Assembler::positive, c_rarg2, rax);\n+\n+  __ bind(L);\n+  __ movptr(inout, c_rarg2);\n+\n+  __ pop(c_rarg0);\n+  __ pop(c_rarg1);\n+  __ pop(c_rarg2);\n+  __ pop(c_rarg3);\n+  __ pop(rax);\n+\n+  __ ret(0);\n@@ -794,5 +751,2 @@\n-    __ pop(c_rarg0);\n-    __ pop(c_rarg1);\n-    __ pop(c_rarg2);\n-    __ pop(c_rarg3);\n-    __ pop(rax);\n+  return start;\n+}\n@@ -800,1 +754,43 @@\n-    __ ret(0);\n+address StubGenerator::generate_d2l_fixup() {\n+  StubCodeMark mark(this, \"StubRoutines\", \"d2l_fixup\");\n+  Address inout(rsp, 6 * wordSize); \/\/ return address + 5 saves\n+\n+  address start = __ pc();\n+\n+  Label L;\n+\n+  __ push(rax);\n+  __ push(c_rarg3);\n+  __ push(c_rarg2);\n+  __ push(c_rarg1);\n+  __ push(c_rarg0);\n+\n+  __ movl(rax, 0x7ff00000);\n+  __ movq(c_rarg2, inout);\n+  __ movl(c_rarg3, c_rarg2);\n+  __ mov(c_rarg1, c_rarg2);\n+  __ mov(c_rarg0, c_rarg2);\n+  __ negl(c_rarg3);\n+  __ shrptr(c_rarg1, 0x20);\n+  __ orl(c_rarg3, c_rarg2);\n+  __ andl(c_rarg1, 0x7fffffff);\n+  __ xorl(c_rarg2, c_rarg2);\n+  __ shrl(c_rarg3, 0x1f);\n+  __ orl(c_rarg1, c_rarg3);\n+  __ cmpl(rax, c_rarg1);\n+  __ jcc(Assembler::negative, L); \/\/ NaN -> 0\n+  __ testq(c_rarg0, c_rarg0); \/\/ signed ? min_jlong : max_jlong\n+  __ mov64(c_rarg2, 0x8000000000000000);\n+  __ mov64(rax, 0x7fffffffffffffff);\n+  __ cmovq(Assembler::positive, c_rarg2, rax);\n+\n+  __ bind(L);\n+  __ movq(inout, c_rarg2);\n+\n+  __ pop(c_rarg0);\n+  __ pop(c_rarg1);\n+  __ pop(c_rarg2);\n+  __ pop(c_rarg3);\n+  __ pop(rax);\n+\n+  __ ret(0);\n@@ -802,2 +798,2 @@\n-    return start;\n-  }\n+  return start;\n+}\n@@ -805,208 +801,4 @@\n-  address generate_count_leading_zeros_lut(const char *stub_name) {\n-    __ align64();\n-    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n-    address start = __ pc();\n-    __ emit_data64(0x0101010102020304, relocInfo::none);\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0101010102020304, relocInfo::none);\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0101010102020304, relocInfo::none);\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0101010102020304, relocInfo::none);\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    return start;\n-  }\n-\n-  address generate_popcount_avx_lut(const char *stub_name) {\n-    __ align64();\n-    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n-    address start = __ pc();\n-    __ emit_data64(0x0302020102010100, relocInfo::none);\n-    __ emit_data64(0x0403030203020201, relocInfo::none);\n-    __ emit_data64(0x0302020102010100, relocInfo::none);\n-    __ emit_data64(0x0403030203020201, relocInfo::none);\n-    __ emit_data64(0x0302020102010100, relocInfo::none);\n-    __ emit_data64(0x0403030203020201, relocInfo::none);\n-    __ emit_data64(0x0302020102010100, relocInfo::none);\n-    __ emit_data64(0x0403030203020201, relocInfo::none);\n-    return start;\n-  }\n-\n-  address generate_iota_indices(const char *stub_name) {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n-    address start = __ pc();\n-    __ emit_data64(0x0706050403020100, relocInfo::none);\n-    __ emit_data64(0x0F0E0D0C0B0A0908, relocInfo::none);\n-    __ emit_data64(0x1716151413121110, relocInfo::none);\n-    __ emit_data64(0x1F1E1D1C1B1A1918, relocInfo::none);\n-    __ emit_data64(0x2726252423222120, relocInfo::none);\n-    __ emit_data64(0x2F2E2D2C2B2A2928, relocInfo::none);\n-    __ emit_data64(0x3736353433323130, relocInfo::none);\n-    __ emit_data64(0x3F3E3D3C3B3A3938, relocInfo::none);\n-    return start;\n-  }\n-\n-  address generate_vector_reverse_bit_lut(const char *stub_name) {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n-    address start = __ pc();\n-    __ emit_data64(0x0E060A020C040800, relocInfo::none);\n-    __ emit_data64(0x0F070B030D050901, relocInfo::none);\n-    __ emit_data64(0x0E060A020C040800, relocInfo::none);\n-    __ emit_data64(0x0F070B030D050901, relocInfo::none);\n-    __ emit_data64(0x0E060A020C040800, relocInfo::none);\n-    __ emit_data64(0x0F070B030D050901, relocInfo::none);\n-    __ emit_data64(0x0E060A020C040800, relocInfo::none);\n-    __ emit_data64(0x0F070B030D050901, relocInfo::none);\n-    return start;\n-  }\n-\n-  address generate_vector_reverse_byte_perm_mask_long(const char *stub_name) {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n-    address start = __ pc();\n-    __ emit_data64(0x0001020304050607, relocInfo::none);\n-    __ emit_data64(0x08090A0B0C0D0E0F, relocInfo::none);\n-    __ emit_data64(0x0001020304050607, relocInfo::none);\n-    __ emit_data64(0x08090A0B0C0D0E0F, relocInfo::none);\n-    __ emit_data64(0x0001020304050607, relocInfo::none);\n-    __ emit_data64(0x08090A0B0C0D0E0F, relocInfo::none);\n-    __ emit_data64(0x0001020304050607, relocInfo::none);\n-    __ emit_data64(0x08090A0B0C0D0E0F, relocInfo::none);\n-    return start;\n-  }\n-\n-  address generate_vector_reverse_byte_perm_mask_int(const char *stub_name) {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n-    address start = __ pc();\n-    __ emit_data64(0x0405060700010203, relocInfo::none);\n-    __ emit_data64(0x0C0D0E0F08090A0B, relocInfo::none);\n-    __ emit_data64(0x0405060700010203, relocInfo::none);\n-    __ emit_data64(0x0C0D0E0F08090A0B, relocInfo::none);\n-    __ emit_data64(0x0405060700010203, relocInfo::none);\n-    __ emit_data64(0x0C0D0E0F08090A0B, relocInfo::none);\n-    __ emit_data64(0x0405060700010203, relocInfo::none);\n-    __ emit_data64(0x0C0D0E0F08090A0B, relocInfo::none);\n-    return start;\n-  }\n-\n-  address generate_vector_reverse_byte_perm_mask_short(const char *stub_name) {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n-    address start = __ pc();\n-    __ emit_data64(0x0607040502030001, relocInfo::none);\n-    __ emit_data64(0x0E0F0C0D0A0B0809, relocInfo::none);\n-    __ emit_data64(0x0607040502030001, relocInfo::none);\n-    __ emit_data64(0x0E0F0C0D0A0B0809, relocInfo::none);\n-    __ emit_data64(0x0607040502030001, relocInfo::none);\n-    __ emit_data64(0x0E0F0C0D0A0B0809, relocInfo::none);\n-    __ emit_data64(0x0607040502030001, relocInfo::none);\n-    __ emit_data64(0x0E0F0C0D0A0B0809, relocInfo::none);\n-    return start;\n-  }\n-\n-  address generate_vector_byte_shuffle_mask(const char *stub_name) {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n-    address start = __ pc();\n-    __ emit_data64(0x7070707070707070, relocInfo::none);\n-    __ emit_data64(0x7070707070707070, relocInfo::none);\n-    __ emit_data64(0xF0F0F0F0F0F0F0F0, relocInfo::none);\n-    __ emit_data64(0xF0F0F0F0F0F0F0F0, relocInfo::none);\n-    return start;\n-  }\n-\n-  address generate_fp_mask(const char *stub_name, int64_t mask) {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n-    address start = __ pc();\n-\n-    __ emit_data64( mask, relocInfo::none );\n-    __ emit_data64( mask, relocInfo::none );\n-\n-    return start;\n-  }\n-\n-  address generate_vector_mask(const char *stub_name, int64_t mask) {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n-    address start = __ pc();\n-\n-    __ emit_data64(mask, relocInfo::none);\n-    __ emit_data64(mask, relocInfo::none);\n-    __ emit_data64(mask, relocInfo::none);\n-    __ emit_data64(mask, relocInfo::none);\n-    __ emit_data64(mask, relocInfo::none);\n-    __ emit_data64(mask, relocInfo::none);\n-    __ emit_data64(mask, relocInfo::none);\n-    __ emit_data64(mask, relocInfo::none);\n-\n-    return start;\n-  }\n-\n-  address generate_vector_byte_perm_mask(const char *stub_name) {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n-    address start = __ pc();\n-\n-    __ emit_data64(0x0000000000000001, relocInfo::none);\n-    __ emit_data64(0x0000000000000003, relocInfo::none);\n-    __ emit_data64(0x0000000000000005, relocInfo::none);\n-    __ emit_data64(0x0000000000000007, relocInfo::none);\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000002, relocInfo::none);\n-    __ emit_data64(0x0000000000000004, relocInfo::none);\n-    __ emit_data64(0x0000000000000006, relocInfo::none);\n-\n-    return start;\n-  }\n-\n-  address generate_vector_fp_mask(const char *stub_name, int64_t mask) {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n-    address start = __ pc();\n-\n-    __ emit_data64(mask, relocInfo::none);\n-    __ emit_data64(mask, relocInfo::none);\n-    __ emit_data64(mask, relocInfo::none);\n-    __ emit_data64(mask, relocInfo::none);\n-    __ emit_data64(mask, relocInfo::none);\n-    __ emit_data64(mask, relocInfo::none);\n-    __ emit_data64(mask, relocInfo::none);\n-    __ emit_data64(mask, relocInfo::none);\n-\n-    return start;\n-  }\n-\n-  address generate_vector_custom_i32(const char *stub_name, Assembler::AvxVectorLen len,\n-                                     int32_t val0, int32_t val1, int32_t val2, int32_t val3,\n-                                     int32_t val4 = 0, int32_t val5 = 0, int32_t val6 = 0, int32_t val7 = 0,\n-                                     int32_t val8 = 0, int32_t val9 = 0, int32_t val10 = 0, int32_t val11 = 0,\n-                                     int32_t val12 = 0, int32_t val13 = 0, int32_t val14 = 0, int32_t val15 = 0) {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n-    address start = __ pc();\n-\n-    assert(len != Assembler::AVX_NoVec, \"vector len must be specified\");\n-    __ emit_data(val0, relocInfo::none, 0);\n-    __ emit_data(val1, relocInfo::none, 0);\n-    __ emit_data(val2, relocInfo::none, 0);\n-    __ emit_data(val3, relocInfo::none, 0);\n-    if (len >= Assembler::AVX_256bit) {\n-      __ emit_data(val4, relocInfo::none, 0);\n-      __ emit_data(val5, relocInfo::none, 0);\n-      __ emit_data(val6, relocInfo::none, 0);\n-      __ emit_data(val7, relocInfo::none, 0);\n-      if (len >= Assembler::AVX_512bit) {\n-        __ emit_data(val8, relocInfo::none, 0);\n-        __ emit_data(val9, relocInfo::none, 0);\n-        __ emit_data(val10, relocInfo::none, 0);\n-        __ emit_data(val11, relocInfo::none, 0);\n-        __ emit_data(val12, relocInfo::none, 0);\n-        __ emit_data(val13, relocInfo::none, 0);\n-        __ emit_data(val14, relocInfo::none, 0);\n-        __ emit_data(val15, relocInfo::none, 0);\n-      }\n-    }\n+address StubGenerator::generate_count_leading_zeros_lut(const char *stub_name) {\n+  __ align64();\n+  StubCodeMark mark(this, \"StubRoutines\", stub_name);\n+  address start = __ pc();\n@@ -1014,2 +806,8 @@\n-    return start;\n-  }\n+  __ emit_data64(0x0101010102020304, relocInfo::none);\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+  __ emit_data64(0x0101010102020304, relocInfo::none);\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+  __ emit_data64(0x0101010102020304, relocInfo::none);\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+  __ emit_data64(0x0101010102020304, relocInfo::none);\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n@@ -1017,48 +815,2 @@\n-  \/\/ Non-destructive plausibility checks for oops\n-  \/\/\n-  \/\/ Arguments:\n-  \/\/    all args on stack!\n-  \/\/\n-  \/\/ Stack after saving c_rarg3:\n-  \/\/    [tos + 0]: saved c_rarg3\n-  \/\/    [tos + 1]: saved c_rarg2\n-  \/\/    [tos + 2]: saved r12 (several TemplateTable methods use it)\n-  \/\/    [tos + 3]: saved flags\n-  \/\/    [tos + 4]: return address\n-  \/\/  * [tos + 5]: error message (char*)\n-  \/\/  * [tos + 6]: object to verify (oop)\n-  \/\/  * [tos + 7]: saved rax - saved by caller and bashed\n-  \/\/  * [tos + 8]: saved r10 (rscratch1) - saved by caller\n-  \/\/  * = popped on exit\n-  address generate_verify_oop() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"verify_oop\");\n-    address start = __ pc();\n-\n-    Label exit, error;\n-\n-    __ pushf();\n-    __ incrementl(ExternalAddress((address) StubRoutines::verify_oop_count_addr()), rscratch1);\n-\n-    __ push(r12);\n-\n-    \/\/ save c_rarg2 and c_rarg3\n-    __ push(c_rarg2);\n-    __ push(c_rarg3);\n-\n-    enum {\n-           \/\/ After previous pushes.\n-           oop_to_verify = 6 * wordSize,\n-           saved_rax     = 7 * wordSize,\n-           saved_r10     = 8 * wordSize,\n-\n-           \/\/ Before the call to MacroAssembler::debug(), see below.\n-           return_addr   = 16 * wordSize,\n-           error_msg     = 17 * wordSize\n-    };\n-\n-    \/\/ get object\n-    __ movptr(rax, Address(rsp, oop_to_verify));\n-\n-    \/\/ make sure object is 'reasonable'\n-    __ testptr(rax, rax);\n-    __ jcc(Assembler::zero, exit); \/\/ if obj is NULL it is OK\n+  return start;\n+}\n@@ -1066,7 +818,4 @@\n-#if INCLUDE_ZGC\n-    if (UseZGC) {\n-      \/\/ Check if metadata bits indicate a bad oop\n-      __ testptr(rax, Address(r15_thread, ZThreadLocalData::address_bad_mask_offset()));\n-      __ jcc(Assembler::notZero, error);\n-    }\n-#endif\n+address StubGenerator::generate_popcount_avx_lut(const char *stub_name) {\n+  __ align64();\n+  StubCodeMark mark(this, \"StubRoutines\", stub_name);\n+  address start = __ pc();\n@@ -1074,7 +823,8 @@\n-    \/\/ Check if the oop is in the right area of memory\n-    __ movptr(c_rarg2, rax);\n-    __ movptr(c_rarg3, (intptr_t) Universe::verify_oop_mask());\n-    __ andptr(c_rarg2, c_rarg3);\n-    __ movptr(c_rarg3, (intptr_t) Universe::verify_oop_bits());\n-    __ cmpptr(c_rarg2, c_rarg3);\n-    __ jcc(Assembler::notZero, error);\n+  __ emit_data64(0x0302020102010100, relocInfo::none);\n+  __ emit_data64(0x0403030203020201, relocInfo::none);\n+  __ emit_data64(0x0302020102010100, relocInfo::none);\n+  __ emit_data64(0x0403030203020201, relocInfo::none);\n+  __ emit_data64(0x0302020102010100, relocInfo::none);\n+  __ emit_data64(0x0403030203020201, relocInfo::none);\n+  __ emit_data64(0x0302020102010100, relocInfo::none);\n+  __ emit_data64(0x0403030203020201, relocInfo::none);\n@@ -1082,50 +832,2 @@\n-    \/\/ make sure klass is 'reasonable', which is not zero.\n-    __ load_klass(rax, rax, rscratch1);  \/\/ get klass\n-    __ testptr(rax, rax);\n-    __ jcc(Assembler::zero, error); \/\/ if klass is NULL it is broken\n-\n-    \/\/ return if everything seems ok\n-    __ bind(exit);\n-    __ movptr(rax, Address(rsp, saved_rax));     \/\/ get saved rax back\n-    __ movptr(rscratch1, Address(rsp, saved_r10)); \/\/ get saved r10 back\n-    __ pop(c_rarg3);                             \/\/ restore c_rarg3\n-    __ pop(c_rarg2);                             \/\/ restore c_rarg2\n-    __ pop(r12);                                 \/\/ restore r12\n-    __ popf();                                   \/\/ restore flags\n-    __ ret(4 * wordSize);                        \/\/ pop caller saved stuff\n-\n-    \/\/ handle errors\n-    __ bind(error);\n-    __ movptr(rax, Address(rsp, saved_rax));     \/\/ get saved rax back\n-    __ movptr(rscratch1, Address(rsp, saved_r10)); \/\/ get saved r10 back\n-    __ pop(c_rarg3);                             \/\/ get saved c_rarg3 back\n-    __ pop(c_rarg2);                             \/\/ get saved c_rarg2 back\n-    __ pop(r12);                                 \/\/ get saved r12 back\n-    __ popf();                                   \/\/ get saved flags off stack --\n-                                                 \/\/ will be ignored\n-\n-    __ pusha();                                  \/\/ push registers\n-                                                 \/\/ (rip is already\n-                                                 \/\/ already pushed)\n-    \/\/ debug(char* msg, int64_t pc, int64_t regs[])\n-    \/\/ We've popped the registers we'd saved (c_rarg3, c_rarg2 and flags), and\n-    \/\/ pushed all the registers, so now the stack looks like:\n-    \/\/     [tos +  0] 16 saved registers\n-    \/\/     [tos + 16] return address\n-    \/\/   * [tos + 17] error message (char*)\n-    \/\/   * [tos + 18] object to verify (oop)\n-    \/\/   * [tos + 19] saved rax - saved by caller and bashed\n-    \/\/   * [tos + 20] saved r10 (rscratch1) - saved by caller\n-    \/\/   * = popped on exit\n-\n-    __ movptr(c_rarg0, Address(rsp, error_msg));    \/\/ pass address of error message\n-    __ movptr(c_rarg1, Address(rsp, return_addr));  \/\/ pass return address\n-    __ movq(c_rarg2, rsp);                          \/\/ pass address of regs on stack\n-    __ mov(r12, rsp);                               \/\/ remember rsp\n-    __ subptr(rsp, frame::arg_reg_save_area_bytes); \/\/ windows\n-    __ andptr(rsp, -16);                            \/\/ align stack as required by ABI\n-    BLOCK_COMMENT(\"call MacroAssembler::debug\");\n-    __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, MacroAssembler::debug64)));\n-    __ hlt();\n-    return start;\n-  }\n+  return start;\n+}\n@@ -1133,19 +835,4 @@\n-  \/\/\n-  \/\/ Verify that a register contains clean 32-bits positive value\n-  \/\/ (high 32-bits are 0) so it could be used in 64-bits shifts.\n-  \/\/\n-  \/\/  Input:\n-  \/\/    Rint  -  32-bits value\n-  \/\/    Rtmp  -  scratch\n-  \/\/\n-  void assert_clean_int(Register Rint, Register Rtmp) {\n-#ifdef ASSERT\n-    Label L;\n-    assert_different_registers(Rtmp, Rint);\n-    __ movslq(Rtmp, Rint);\n-    __ cmpq(Rtmp, Rint);\n-    __ jcc(Assembler::equal, L);\n-    __ stop(\"high 32-bits of int value are not 0\");\n-    __ bind(L);\n-#endif\n-  }\n+address StubGenerator::generate_iota_indices(const char *stub_name) {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", stub_name);\n+  address start = __ pc();\n@@ -1153,36 +840,8 @@\n-  \/\/  Generate overlap test for array copy stubs\n-  \/\/\n-  \/\/  Input:\n-  \/\/     c_rarg0 - from\n-  \/\/     c_rarg1 - to\n-  \/\/     c_rarg2 - element count\n-  \/\/\n-  \/\/  Output:\n-  \/\/     rax   - &from[element count - 1]\n-  \/\/\n-  void array_overlap_test(address no_overlap_target, Address::ScaleFactor sf) {\n-    assert(no_overlap_target != NULL, \"must be generated\");\n-    array_overlap_test(no_overlap_target, NULL, sf);\n-  }\n-  void array_overlap_test(Label& L_no_overlap, Address::ScaleFactor sf) {\n-    array_overlap_test(NULL, &L_no_overlap, sf);\n-  }\n-  void array_overlap_test(address no_overlap_target, Label* NOLp, Address::ScaleFactor sf) {\n-    const Register from     = c_rarg0;\n-    const Register to       = c_rarg1;\n-    const Register count    = c_rarg2;\n-    const Register end_from = rax;\n-\n-    __ cmpptr(to, from);\n-    __ lea(end_from, Address(from, count, sf, 0));\n-    if (NOLp == NULL) {\n-      ExternalAddress no_overlap(no_overlap_target);\n-      __ jump_cc(Assembler::belowEqual, no_overlap);\n-      __ cmpptr(to, end_from);\n-      __ jump_cc(Assembler::aboveEqual, no_overlap);\n-    } else {\n-      __ jcc(Assembler::belowEqual, (*NOLp));\n-      __ cmpptr(to, end_from);\n-      __ jcc(Assembler::aboveEqual, (*NOLp));\n-    }\n-  }\n+  __ emit_data64(0x0706050403020100, relocInfo::none);\n+  __ emit_data64(0x0F0E0D0C0B0A0908, relocInfo::none);\n+  __ emit_data64(0x1716151413121110, relocInfo::none);\n+  __ emit_data64(0x1F1E1D1C1B1A1918, relocInfo::none);\n+  __ emit_data64(0x2726252423222120, relocInfo::none);\n+  __ emit_data64(0x2F2E2D2C2B2A2928, relocInfo::none);\n+  __ emit_data64(0x3736353433323130, relocInfo::none);\n+  __ emit_data64(0x3F3E3D3C3B3A3938, relocInfo::none);\n@@ -1190,12 +849,2 @@\n-  \/\/ Shuffle first three arg regs on Windows into Linux\/Solaris locations.\n-  \/\/\n-  \/\/ Outputs:\n-  \/\/    rdi - rcx\n-  \/\/    rsi - rdx\n-  \/\/    rdx - r8\n-  \/\/    rcx - r9\n-  \/\/\n-  \/\/ Registers r9 and r10 are used to save rdi and rsi on Windows, which latter\n-  \/\/ are non-volatile.  r9 and r10 should not be used by the caller.\n-  \/\/\n-  DEBUG_ONLY(bool regs_in_thread;)\n+  return start;\n+}\n@@ -1203,22 +852,4 @@\n-  void setup_arg_regs(int nargs = 3) {\n-    const Register saved_rdi = r9;\n-    const Register saved_rsi = r10;\n-    assert(nargs == 3 || nargs == 4, \"else fix\");\n-#ifdef _WIN64\n-    assert(c_rarg0 == rcx && c_rarg1 == rdx && c_rarg2 == r8 && c_rarg3 == r9,\n-           \"unexpected argument registers\");\n-    if (nargs >= 4)\n-      __ mov(rax, r9);  \/\/ r9 is also saved_rdi\n-    __ movptr(saved_rdi, rdi);\n-    __ movptr(saved_rsi, rsi);\n-    __ mov(rdi, rcx); \/\/ c_rarg0\n-    __ mov(rsi, rdx); \/\/ c_rarg1\n-    __ mov(rdx, r8);  \/\/ c_rarg2\n-    if (nargs >= 4)\n-      __ mov(rcx, rax); \/\/ c_rarg3 (via rax)\n-#else\n-    assert(c_rarg0 == rdi && c_rarg1 == rsi && c_rarg2 == rdx && c_rarg3 == rcx,\n-           \"unexpected argument registers\");\n-#endif\n-    DEBUG_ONLY(regs_in_thread = false;)\n-  }\n+address StubGenerator::generate_vector_reverse_bit_lut(const char *stub_name) {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", stub_name);\n+  address start = __ pc();\n@@ -1226,9 +857,8 @@\n-  void restore_arg_regs() {\n-    assert(!regs_in_thread, \"wrong call to restore_arg_regs\");\n-    const Register saved_rdi = r9;\n-    const Register saved_rsi = r10;\n-#ifdef _WIN64\n-    __ movptr(rdi, saved_rdi);\n-    __ movptr(rsi, saved_rsi);\n-#endif\n-  }\n+  __ emit_data64(0x0E060A020C040800, relocInfo::none);\n+  __ emit_data64(0x0F070B030D050901, relocInfo::none);\n+  __ emit_data64(0x0E060A020C040800, relocInfo::none);\n+  __ emit_data64(0x0F070B030D050901, relocInfo::none);\n+  __ emit_data64(0x0E060A020C040800, relocInfo::none);\n+  __ emit_data64(0x0F070B030D050901, relocInfo::none);\n+  __ emit_data64(0x0E060A020C040800, relocInfo::none);\n+  __ emit_data64(0x0F070B030D050901, relocInfo::none);\n@@ -1236,21 +866,2 @@\n-  \/\/ This is used in places where r10 is a scratch register, and can\n-  \/\/ be adapted if r9 is needed also.\n-  void setup_arg_regs_using_thread() {\n-    const Register saved_r15 = r9;\n-#ifdef _WIN64\n-    __ mov(saved_r15, r15);  \/\/ r15 is callee saved and needs to be restored\n-    __ get_thread(r15_thread);\n-    assert(c_rarg0 == rcx && c_rarg1 == rdx && c_rarg2 == r8 && c_rarg3 == r9,\n-           \"unexpected argument registers\");\n-    __ movptr(Address(r15_thread, in_bytes(JavaThread::windows_saved_rdi_offset())), rdi);\n-    __ movptr(Address(r15_thread, in_bytes(JavaThread::windows_saved_rsi_offset())), rsi);\n-\n-    __ mov(rdi, rcx); \/\/ c_rarg0\n-    __ mov(rsi, rdx); \/\/ c_rarg1\n-    __ mov(rdx, r8);  \/\/ c_rarg2\n-#else\n-    assert(c_rarg0 == rdi && c_rarg1 == rsi && c_rarg2 == rdx && c_rarg3 == rcx,\n-           \"unexpected argument registers\");\n-#endif\n-    DEBUG_ONLY(regs_in_thread = true;)\n-  }\n+  return start;\n+}\n@@ -1258,10 +869,4 @@\n-  void restore_arg_regs_using_thread() {\n-    assert(regs_in_thread, \"wrong call to restore_arg_regs\");\n-    const Register saved_r15 = r9;\n-#ifdef _WIN64\n-    __ get_thread(r15_thread);\n-    __ movptr(rsi, Address(r15_thread, in_bytes(JavaThread::windows_saved_rsi_offset())));\n-    __ movptr(rdi, Address(r15_thread, in_bytes(JavaThread::windows_saved_rdi_offset())));\n-    __ mov(r15, saved_r15);  \/\/ r15 is callee saved and needs to be restored\n-#endif\n-  }\n+address StubGenerator::generate_vector_reverse_byte_perm_mask_long(const char *stub_name) {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", stub_name);\n+  address start = __ pc();\n@@ -1269,34 +874,8 @@\n-  \/\/ Copy big chunks forward\n-  \/\/\n-  \/\/ Inputs:\n-  \/\/   end_from     - source arrays end address\n-  \/\/   end_to       - destination array end address\n-  \/\/   qword_count  - 64-bits element count, negative\n-  \/\/   to           - scratch\n-  \/\/   L_copy_bytes - entry label\n-  \/\/   L_copy_8_bytes  - exit  label\n-  \/\/\n-  void copy_bytes_forward(Register end_from, Register end_to,\n-                             Register qword_count, Register to,\n-                             Label& L_copy_bytes, Label& L_copy_8_bytes) {\n-    DEBUG_ONLY(__ stop(\"enter at entry label, not here\"));\n-    Label L_loop;\n-    __ align(OptoLoopAlignment);\n-    if (UseUnalignedLoadStores) {\n-      Label L_end;\n-      __ BIND(L_loop);\n-      if (UseAVX >= 2) {\n-        __ vmovdqu(xmm0, Address(end_from, qword_count, Address::times_8, -56));\n-        __ vmovdqu(Address(end_to, qword_count, Address::times_8, -56), xmm0);\n-        __ vmovdqu(xmm1, Address(end_from, qword_count, Address::times_8, -24));\n-        __ vmovdqu(Address(end_to, qword_count, Address::times_8, -24), xmm1);\n-      } else {\n-        __ movdqu(xmm0, Address(end_from, qword_count, Address::times_8, -56));\n-        __ movdqu(Address(end_to, qword_count, Address::times_8, -56), xmm0);\n-        __ movdqu(xmm1, Address(end_from, qword_count, Address::times_8, -40));\n-        __ movdqu(Address(end_to, qword_count, Address::times_8, -40), xmm1);\n-        __ movdqu(xmm2, Address(end_from, qword_count, Address::times_8, -24));\n-        __ movdqu(Address(end_to, qword_count, Address::times_8, -24), xmm2);\n-        __ movdqu(xmm3, Address(end_from, qword_count, Address::times_8, - 8));\n-        __ movdqu(Address(end_to, qword_count, Address::times_8, - 8), xmm3);\n-      }\n+  __ emit_data64(0x0001020304050607, relocInfo::none);\n+  __ emit_data64(0x08090A0B0C0D0E0F, relocInfo::none);\n+  __ emit_data64(0x0001020304050607, relocInfo::none);\n+  __ emit_data64(0x08090A0B0C0D0E0F, relocInfo::none);\n+  __ emit_data64(0x0001020304050607, relocInfo::none);\n+  __ emit_data64(0x08090A0B0C0D0E0F, relocInfo::none);\n+  __ emit_data64(0x0001020304050607, relocInfo::none);\n+  __ emit_data64(0x08090A0B0C0D0E0F, relocInfo::none);\n@@ -1304,36 +883,2 @@\n-      __ BIND(L_copy_bytes);\n-      __ addptr(qword_count, 8);\n-      __ jcc(Assembler::lessEqual, L_loop);\n-      __ subptr(qword_count, 4);  \/\/ sub(8) and add(4)\n-      __ jccb(Assembler::greater, L_end);\n-      \/\/ Copy trailing 32 bytes\n-      if (UseAVX >= 2) {\n-        __ vmovdqu(xmm0, Address(end_from, qword_count, Address::times_8, -24));\n-        __ vmovdqu(Address(end_to, qword_count, Address::times_8, -24), xmm0);\n-      } else {\n-        __ movdqu(xmm0, Address(end_from, qword_count, Address::times_8, -24));\n-        __ movdqu(Address(end_to, qword_count, Address::times_8, -24), xmm0);\n-        __ movdqu(xmm1, Address(end_from, qword_count, Address::times_8, - 8));\n-        __ movdqu(Address(end_to, qword_count, Address::times_8, - 8), xmm1);\n-      }\n-      __ addptr(qword_count, 4);\n-      __ BIND(L_end);\n-    } else {\n-      \/\/ Copy 32-bytes per iteration\n-      __ BIND(L_loop);\n-      __ movq(to, Address(end_from, qword_count, Address::times_8, -24));\n-      __ movq(Address(end_to, qword_count, Address::times_8, -24), to);\n-      __ movq(to, Address(end_from, qword_count, Address::times_8, -16));\n-      __ movq(Address(end_to, qword_count, Address::times_8, -16), to);\n-      __ movq(to, Address(end_from, qword_count, Address::times_8, - 8));\n-      __ movq(Address(end_to, qword_count, Address::times_8, - 8), to);\n-      __ movq(to, Address(end_from, qword_count, Address::times_8, - 0));\n-      __ movq(Address(end_to, qword_count, Address::times_8, - 0), to);\n-\n-      __ BIND(L_copy_bytes);\n-      __ addptr(qword_count, 4);\n-      __ jcc(Assembler::lessEqual, L_loop);\n-    }\n-    __ subptr(qword_count, 4);\n-    __ jcc(Assembler::less, L_copy_8_bytes); \/\/ Copy trailing qwords\n-  }\n+  return start;\n+}\n@@ -1341,34 +886,4 @@\n-  \/\/ Copy big chunks backward\n-  \/\/\n-  \/\/ Inputs:\n-  \/\/   from         - source arrays address\n-  \/\/   dest         - destination array address\n-  \/\/   qword_count  - 64-bits element count\n-  \/\/   to           - scratch\n-  \/\/   L_copy_bytes - entry label\n-  \/\/   L_copy_8_bytes  - exit  label\n-  \/\/\n-  void copy_bytes_backward(Register from, Register dest,\n-                              Register qword_count, Register to,\n-                              Label& L_copy_bytes, Label& L_copy_8_bytes) {\n-    DEBUG_ONLY(__ stop(\"enter at entry label, not here\"));\n-    Label L_loop;\n-    __ align(OptoLoopAlignment);\n-    if (UseUnalignedLoadStores) {\n-      Label L_end;\n-      __ BIND(L_loop);\n-      if (UseAVX >= 2) {\n-        __ vmovdqu(xmm0, Address(from, qword_count, Address::times_8, 32));\n-        __ vmovdqu(Address(dest, qword_count, Address::times_8, 32), xmm0);\n-        __ vmovdqu(xmm1, Address(from, qword_count, Address::times_8,  0));\n-        __ vmovdqu(Address(dest, qword_count, Address::times_8,  0), xmm1);\n-      } else {\n-        __ movdqu(xmm0, Address(from, qword_count, Address::times_8, 48));\n-        __ movdqu(Address(dest, qword_count, Address::times_8, 48), xmm0);\n-        __ movdqu(xmm1, Address(from, qword_count, Address::times_8, 32));\n-        __ movdqu(Address(dest, qword_count, Address::times_8, 32), xmm1);\n-        __ movdqu(xmm2, Address(from, qword_count, Address::times_8, 16));\n-        __ movdqu(Address(dest, qword_count, Address::times_8, 16), xmm2);\n-        __ movdqu(xmm3, Address(from, qword_count, Address::times_8,  0));\n-        __ movdqu(Address(dest, qword_count, Address::times_8,  0), xmm3);\n-      }\n+address StubGenerator::generate_vector_reverse_byte_perm_mask_int(const char *stub_name) {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", stub_name);\n+  address start = __ pc();\n@@ -1376,3 +891,8 @@\n-      __ BIND(L_copy_bytes);\n-      __ subptr(qword_count, 8);\n-      __ jcc(Assembler::greaterEqual, L_loop);\n+  __ emit_data64(0x0405060700010203, relocInfo::none);\n+  __ emit_data64(0x0C0D0E0F08090A0B, relocInfo::none);\n+  __ emit_data64(0x0405060700010203, relocInfo::none);\n+  __ emit_data64(0x0C0D0E0F08090A0B, relocInfo::none);\n+  __ emit_data64(0x0405060700010203, relocInfo::none);\n+  __ emit_data64(0x0C0D0E0F08090A0B, relocInfo::none);\n+  __ emit_data64(0x0405060700010203, relocInfo::none);\n+  __ emit_data64(0x0C0D0E0F08090A0B, relocInfo::none);\n@@ -1380,46 +900,2 @@\n-      __ addptr(qword_count, 4);  \/\/ add(8) and sub(4)\n-      __ jccb(Assembler::less, L_end);\n-      \/\/ Copy trailing 32 bytes\n-      if (UseAVX >= 2) {\n-        __ vmovdqu(xmm0, Address(from, qword_count, Address::times_8, 0));\n-        __ vmovdqu(Address(dest, qword_count, Address::times_8, 0), xmm0);\n-      } else {\n-        __ movdqu(xmm0, Address(from, qword_count, Address::times_8, 16));\n-        __ movdqu(Address(dest, qword_count, Address::times_8, 16), xmm0);\n-        __ movdqu(xmm1, Address(from, qword_count, Address::times_8,  0));\n-        __ movdqu(Address(dest, qword_count, Address::times_8,  0), xmm1);\n-      }\n-      __ subptr(qword_count, 4);\n-      __ BIND(L_end);\n-    } else {\n-      \/\/ Copy 32-bytes per iteration\n-      __ BIND(L_loop);\n-      __ movq(to, Address(from, qword_count, Address::times_8, 24));\n-      __ movq(Address(dest, qword_count, Address::times_8, 24), to);\n-      __ movq(to, Address(from, qword_count, Address::times_8, 16));\n-      __ movq(Address(dest, qword_count, Address::times_8, 16), to);\n-      __ movq(to, Address(from, qword_count, Address::times_8,  8));\n-      __ movq(Address(dest, qword_count, Address::times_8,  8), to);\n-      __ movq(to, Address(from, qword_count, Address::times_8,  0));\n-      __ movq(Address(dest, qword_count, Address::times_8,  0), to);\n-\n-      __ BIND(L_copy_bytes);\n-      __ subptr(qword_count, 4);\n-      __ jcc(Assembler::greaterEqual, L_loop);\n-    }\n-    __ addptr(qword_count, 4);\n-    __ jcc(Assembler::greater, L_copy_8_bytes); \/\/ Copy trailing qwords\n-  }\n-\n-#ifndef PRODUCT\n-    int& get_profile_ctr(int shift) {\n-      if ( 0 == shift)\n-        return SharedRuntime::_jbyte_array_copy_ctr;\n-      else if(1 == shift)\n-        return SharedRuntime::_jshort_array_copy_ctr;\n-      else if(2 == shift)\n-        return SharedRuntime::_jint_array_copy_ctr;\n-      else\n-        return SharedRuntime::_jlong_array_copy_ctr;\n-    }\n-#endif\n+  return start;\n+}\n@@ -1427,9 +903,4 @@\n-  void setup_argument_regs(BasicType type) {\n-    if (type == T_BYTE || type == T_SHORT) {\n-      setup_arg_regs(); \/\/ from => rdi, to => rsi, count => rdx\n-                        \/\/ r9 and r10 may be used to save non-volatile registers\n-    } else {\n-      setup_arg_regs_using_thread(); \/\/ from => rdi, to => rsi, count => rdx\n-                                     \/\/ r9 is used to save r15_thread\n-    }\n-  }\n+address StubGenerator::generate_vector_reverse_byte_perm_mask_short(const char *stub_name) {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", stub_name);\n+  address start = __ pc();\n@@ -1437,7 +908,8 @@\n-  void restore_argument_regs(BasicType type) {\n-    if (type == T_BYTE || type == T_SHORT) {\n-      restore_arg_regs();\n-    } else {\n-      restore_arg_regs_using_thread();\n-    }\n-  }\n+  __ emit_data64(0x0607040502030001, relocInfo::none);\n+  __ emit_data64(0x0E0F0C0D0A0B0809, relocInfo::none);\n+  __ emit_data64(0x0607040502030001, relocInfo::none);\n+  __ emit_data64(0x0E0F0C0D0A0B0809, relocInfo::none);\n+  __ emit_data64(0x0607040502030001, relocInfo::none);\n+  __ emit_data64(0x0E0F0C0D0A0B0809, relocInfo::none);\n+  __ emit_data64(0x0607040502030001, relocInfo::none);\n+  __ emit_data64(0x0E0F0C0D0A0B0809, relocInfo::none);\n@@ -1445,23 +917,2 @@\n-#if COMPILER2_OR_JVMCI\n-  \/\/ Note: Following rules apply to AVX3 optimized arraycopy stubs:-\n-  \/\/ - If target supports AVX3 features (BW+VL+F) then implementation uses 32 byte vectors (YMMs)\n-  \/\/   for both special cases (various small block sizes) and aligned copy loop. This is the\n-  \/\/   default configuration.\n-  \/\/ - If copy length is above AVX3Threshold, then implementation use 64 byte vectors (ZMMs)\n-  \/\/   for main copy loop (and subsequent tail) since bulk of the cycles will be consumed in it.\n-  \/\/ - If user forces MaxVectorSize=32 then above 4096 bytes its seen that REP MOVs shows a\n-  \/\/   better performance for disjoint copies. For conjoint\/backward copy vector based\n-  \/\/   copy performs better.\n-  \/\/ - If user sets AVX3Threshold=0, then special cases for small blocks sizes operate over\n-  \/\/   64 byte vector registers (ZMMs).\n-\n-  \/\/ Inputs:\n-  \/\/   c_rarg0   - source array address\n-  \/\/   c_rarg1   - destination array address\n-  \/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n-  \/\/\n-  \/\/\n-  \/\/ Side Effects:\n-  \/\/   disjoint_copy_avx3_masked is set to the no-overlap entry point\n-  \/\/   used by generate_conjoint_[byte\/int\/short\/long]_copy().\n-  \/\/\n+  return start;\n+}\n@@ -1469,27 +920,4 @@\n-  address generate_disjoint_copy_avx3_masked(address* entry, const char *name, int shift,\n-                                             bool aligned, bool is_oop, bool dest_uninitialized) {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ pc();\n-    int avx3threshold = VM_Version::avx3_threshold();\n-    bool use64byteVector = (MaxVectorSize > 32) && (avx3threshold == 0);\n-    Label L_main_loop, L_main_loop_64bytes, L_tail, L_tail64, L_exit, L_entry;\n-    Label L_repmovs, L_main_pre_loop, L_main_pre_loop_64bytes, L_pre_main_post_64;\n-    const Register from        = rdi;  \/\/ source array address\n-    const Register to          = rsi;  \/\/ destination array address\n-    const Register count       = rdx;  \/\/ elements count\n-    const Register temp1       = r8;\n-    const Register temp2       = r11;\n-    const Register temp3       = rax;\n-    const Register temp4       = rcx;\n-    \/\/ End pointers are inclusive, and if count is not zero they point\n-    \/\/ to the last unit copied:  end_to[0] := end_from[0]\n-\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    assert_clean_int(c_rarg2, rax);    \/\/ Make sure 'count' is clean int.\n-\n-    if (entry != NULL) {\n-      *entry = __ pc();\n-       \/\/ caller can pass a 64-bit byte count here (from Unsafe.copyMemory)\n-      BLOCK_COMMENT(\"Entry:\");\n-    }\n+address StubGenerator::generate_vector_byte_shuffle_mask(const char *stub_name) {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", stub_name);\n+  address start = __ pc();\n@@ -1497,2 +925,4 @@\n-    BasicType type_vec[] = { T_BYTE,  T_SHORT,  T_INT,   T_LONG};\n-    BasicType type = is_oop ? T_OBJECT : type_vec[shift];\n+  __ emit_data64(0x7070707070707070, relocInfo::none);\n+  __ emit_data64(0x7070707070707070, relocInfo::none);\n+  __ emit_data64(0xF0F0F0F0F0F0F0F0, relocInfo::none);\n+  __ emit_data64(0xF0F0F0F0F0F0F0F0, relocInfo::none);\n@@ -1500,1 +930,2 @@\n-    setup_argument_regs(type);\n+  return start;\n+}\n@@ -1502,9 +933,4 @@\n-    DecoratorSet decorators = IN_HEAP | IS_ARRAY | ARRAYCOPY_DISJOINT;\n-    if (dest_uninitialized) {\n-      decorators |= IS_DEST_UNINITIALIZED;\n-    }\n-    if (aligned) {\n-      decorators |= ARRAYCOPY_ALIGNED;\n-    }\n-    BarrierSetAssembler *bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-    bs->arraycopy_prologue(_masm, decorators, type, from, to, count);\n+address StubGenerator::generate_fp_mask(const char *stub_name, int64_t mask) {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", stub_name);\n+  address start = __ pc();\n@@ -1512,4 +938,2 @@\n-    {\n-      \/\/ Type(shift)           byte(0), short(1), int(2),   long(3)\n-      int loop_size[]        = { 192,     96,       48,      24};\n-      int threshold[]        = { 4096,    2048,     1024,    512};\n+  __ emit_data64( mask, relocInfo::none );\n+  __ emit_data64( mask, relocInfo::none );\n@@ -1517,3 +941,2 @@\n-      \/\/ UnsafeCopyMemory page error: continue after ucm\n-      UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n-      \/\/ 'from', 'to' and 'count' are now valid\n+  return start;\n+}\n@@ -1521,4 +944,4 @@\n-      \/\/ temp1 holds remaining count and temp4 holds running count used to compute\n-      \/\/ next address offset for start of to\/from addresses (temp4 * scale).\n-      __ mov64(temp4, 0);\n-      __ movq(temp1, count);\n+address StubGenerator::generate_vector_mask(const char *stub_name, int64_t mask) {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", stub_name);\n+  address start = __ pc();\n@@ -1526,4 +949,8 @@\n-      \/\/ Zero length check.\n-      __ BIND(L_tail);\n-      __ cmpq(temp1, 0);\n-      __ jcc(Assembler::lessEqual, L_exit);\n+  __ emit_data64(mask, relocInfo::none);\n+  __ emit_data64(mask, relocInfo::none);\n+  __ emit_data64(mask, relocInfo::none);\n+  __ emit_data64(mask, relocInfo::none);\n+  __ emit_data64(mask, relocInfo::none);\n+  __ emit_data64(mask, relocInfo::none);\n+  __ emit_data64(mask, relocInfo::none);\n+  __ emit_data64(mask, relocInfo::none);\n@@ -1531,3 +958,2 @@\n-      \/\/ Special cases using 32 byte [masked] vector copy operations.\n-      __ arraycopy_avx3_special_cases(xmm1, k2, from, to, temp1, shift,\n-                                      temp4, temp3, use64byteVector, L_entry, L_exit);\n+  return start;\n+}\n@@ -1535,14 +961,4 @@\n-      \/\/ PRE-MAIN-POST loop for aligned copy.\n-      __ BIND(L_entry);\n-\n-      if (avx3threshold != 0) {\n-        __ cmpq(count, threshold[shift]);\n-        if (MaxVectorSize == 64) {\n-          \/\/ Copy using 64 byte vectors.\n-          __ jcc(Assembler::greaterEqual, L_pre_main_post_64);\n-        } else {\n-          assert(MaxVectorSize < 64, \"vector size should be < 64 bytes\");\n-          \/\/ REP MOVS offer a faster copy path.\n-          __ jcc(Assembler::greaterEqual, L_repmovs);\n-        }\n-      }\n+address StubGenerator::generate_vector_byte_perm_mask(const char *stub_name) {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", stub_name);\n+  address start = __ pc();\n@@ -1550,5 +966,8 @@\n-      if ((MaxVectorSize < 64)  || (avx3threshold != 0)) {\n-        \/\/ Partial copy to make dst address 32 byte aligned.\n-        __ movq(temp2, to);\n-        __ andq(temp2, 31);\n-        __ jcc(Assembler::equal, L_main_pre_loop);\n+  __ emit_data64(0x0000000000000001, relocInfo::none);\n+  __ emit_data64(0x0000000000000003, relocInfo::none);\n+  __ emit_data64(0x0000000000000005, relocInfo::none);\n+  __ emit_data64(0x0000000000000007, relocInfo::none);\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+  __ emit_data64(0x0000000000000002, relocInfo::none);\n+  __ emit_data64(0x0000000000000004, relocInfo::none);\n+  __ emit_data64(0x0000000000000006, relocInfo::none);\n@@ -1556,10 +975,2 @@\n-        __ negptr(temp2);\n-        __ addq(temp2, 32);\n-        if (shift) {\n-          __ shrq(temp2, shift);\n-        }\n-        __ movq(temp3, temp2);\n-        __ copy32_masked_avx(to, from, xmm1, k2, temp3, temp4, temp1, shift);\n-        __ movq(temp4, temp2);\n-        __ movq(temp1, count);\n-        __ subq(temp1, temp2);\n+  return start;\n+}\n@@ -1567,2 +978,4 @@\n-        __ cmpq(temp1, loop_size[shift]);\n-        __ jcc(Assembler::less, L_tail);\n+address StubGenerator::generate_vector_fp_mask(const char *stub_name, int64_t mask) {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", stub_name);\n+  address start = __ pc();\n@@ -1570,2 +983,8 @@\n-        __ BIND(L_main_pre_loop);\n-        __ subq(temp1, loop_size[shift]);\n+  __ emit_data64(mask, relocInfo::none);\n+  __ emit_data64(mask, relocInfo::none);\n+  __ emit_data64(mask, relocInfo::none);\n+  __ emit_data64(mask, relocInfo::none);\n+  __ emit_data64(mask, relocInfo::none);\n+  __ emit_data64(mask, relocInfo::none);\n+  __ emit_data64(mask, relocInfo::none);\n+  __ emit_data64(mask, relocInfo::none);\n@@ -1573,9 +992,2 @@\n-        \/\/ Main loop with aligned copy block size of 192 bytes at 32 byte granularity.\n-        __ align32();\n-        __ BIND(L_main_loop);\n-           __ copy64_avx(to, from, temp4, xmm1, false, shift, 0);\n-           __ copy64_avx(to, from, temp4, xmm1, false, shift, 64);\n-           __ copy64_avx(to, from, temp4, xmm1, false, shift, 128);\n-           __ addptr(temp4, loop_size[shift]);\n-           __ subq(temp1, loop_size[shift]);\n-           __ jcc(Assembler::greater, L_main_loop);\n+  return start;\n+}\n@@ -1583,1 +995,32 @@\n-        __ addq(temp1, loop_size[shift]);\n+address StubGenerator::generate_vector_custom_i32(const char *stub_name, Assembler::AvxVectorLen len,\n+                                   int32_t val0, int32_t val1, int32_t val2, int32_t val3,\n+                                   int32_t val4, int32_t val5, int32_t val6, int32_t val7,\n+                                   int32_t val8, int32_t val9, int32_t val10, int32_t val11,\n+                                   int32_t val12, int32_t val13, int32_t val14, int32_t val15) {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", stub_name);\n+  address start = __ pc();\n+\n+  assert(len != Assembler::AVX_NoVec, \"vector len must be specified\");\n+  __ emit_data(val0, relocInfo::none, 0);\n+  __ emit_data(val1, relocInfo::none, 0);\n+  __ emit_data(val2, relocInfo::none, 0);\n+  __ emit_data(val3, relocInfo::none, 0);\n+  if (len >= Assembler::AVX_256bit) {\n+    __ emit_data(val4, relocInfo::none, 0);\n+    __ emit_data(val5, relocInfo::none, 0);\n+    __ emit_data(val6, relocInfo::none, 0);\n+    __ emit_data(val7, relocInfo::none, 0);\n+    if (len >= Assembler::AVX_512bit) {\n+      __ emit_data(val8, relocInfo::none, 0);\n+      __ emit_data(val9, relocInfo::none, 0);\n+      __ emit_data(val10, relocInfo::none, 0);\n+      __ emit_data(val11, relocInfo::none, 0);\n+      __ emit_data(val12, relocInfo::none, 0);\n+      __ emit_data(val13, relocInfo::none, 0);\n+      __ emit_data(val14, relocInfo::none, 0);\n+      __ emit_data(val15, relocInfo::none, 0);\n+    }\n+  }\n+  return start;\n+}\n@@ -1585,2 +1028,41 @@\n-        \/\/ Tail loop.\n-        __ jmp(L_tail);\n+\/\/ Non-destructive plausibility checks for oops\n+\/\/\n+\/\/ Arguments:\n+\/\/    all args on stack!\n+\/\/\n+\/\/ Stack after saving c_rarg3:\n+\/\/    [tos + 0]: saved c_rarg3\n+\/\/    [tos + 1]: saved c_rarg2\n+\/\/    [tos + 2]: saved r12 (several TemplateTable methods use it)\n+\/\/    [tos + 3]: saved flags\n+\/\/    [tos + 4]: return address\n+\/\/  * [tos + 5]: error message (char*)\n+\/\/  * [tos + 6]: object to verify (oop)\n+\/\/  * [tos + 7]: saved rax - saved by caller and bashed\n+\/\/  * [tos + 8]: saved r10 (rscratch1) - saved by caller\n+\/\/  * = popped on exit\n+address StubGenerator::generate_verify_oop() {\n+  StubCodeMark mark(this, \"StubRoutines\", \"verify_oop\");\n+  address start = __ pc();\n+\n+  Label exit, error;\n+\n+  __ pushf();\n+  __ incrementl(ExternalAddress((address) StubRoutines::verify_oop_count_addr()), rscratch1);\n+\n+  __ push(r12);\n+\n+  \/\/ save c_rarg2 and c_rarg3\n+  __ push(c_rarg2);\n+  __ push(c_rarg3);\n+\n+  enum {\n+    \/\/ After previous pushes.\n+    oop_to_verify = 6 * wordSize,\n+    saved_rax     = 7 * wordSize,\n+    saved_r10     = 8 * wordSize,\n+\n+    \/\/ Before the call to MacroAssembler::debug(), see below.\n+    return_addr   = 16 * wordSize,\n+    error_msg     = 17 * wordSize\n+  };\n@@ -1588,26 +1070,2 @@\n-        __ BIND(L_repmovs);\n-          __ movq(temp2, temp1);\n-          \/\/ Swap to(RSI) and from(RDI) addresses to comply with REP MOVs semantics.\n-          __ movq(temp3, to);\n-          __ movq(to,  from);\n-          __ movq(from, temp3);\n-          \/\/ Save to\/from for restoration post rep_mov.\n-          __ movq(temp1, to);\n-          __ movq(temp3, from);\n-          if(shift < 3) {\n-            __ shrq(temp2, 3-shift);     \/\/ quad word count\n-          }\n-          __ movq(temp4 , temp2);        \/\/ move quad ward count into temp4(RCX).\n-          __ rep_mov();\n-          __ shlq(temp2, 3);             \/\/ convert quad words into byte count.\n-          if(shift) {\n-            __ shrq(temp2, shift);       \/\/ type specific count.\n-          }\n-          \/\/ Restore original addresses in to\/from.\n-          __ movq(to, temp3);\n-          __ movq(from, temp1);\n-          __ movq(temp4, temp2);\n-          __ movq(temp1, count);\n-          __ subq(temp1, temp2);         \/\/ tailing part (less than a quad ward size).\n-          __ jmp(L_tail);\n-      }\n+  \/\/ get object\n+  __ movptr(rax, Address(rsp, oop_to_verify));\n@@ -1615,48 +1073,121 @@\n-      if (MaxVectorSize > 32) {\n-        __ BIND(L_pre_main_post_64);\n-        \/\/ Partial copy to make dst address 64 byte aligned.\n-        __ movq(temp2, to);\n-        __ andq(temp2, 63);\n-        __ jcc(Assembler::equal, L_main_pre_loop_64bytes);\n-\n-        __ negptr(temp2);\n-        __ addq(temp2, 64);\n-        if (shift) {\n-          __ shrq(temp2, shift);\n-        }\n-        __ movq(temp3, temp2);\n-        __ copy64_masked_avx(to, from, xmm1, k2, temp3, temp4, temp1, shift, 0 , true);\n-        __ movq(temp4, temp2);\n-        __ movq(temp1, count);\n-        __ subq(temp1, temp2);\n-\n-        __ cmpq(temp1, loop_size[shift]);\n-        __ jcc(Assembler::less, L_tail64);\n-\n-        __ BIND(L_main_pre_loop_64bytes);\n-        __ subq(temp1, loop_size[shift]);\n-\n-        \/\/ Main loop with aligned copy block size of 192 bytes at\n-        \/\/ 64 byte copy granularity.\n-        __ align32();\n-        __ BIND(L_main_loop_64bytes);\n-           __ copy64_avx(to, from, temp4, xmm1, false, shift, 0 , true);\n-           __ copy64_avx(to, from, temp4, xmm1, false, shift, 64, true);\n-           __ copy64_avx(to, from, temp4, xmm1, false, shift, 128, true);\n-           __ addptr(temp4, loop_size[shift]);\n-           __ subq(temp1, loop_size[shift]);\n-           __ jcc(Assembler::greater, L_main_loop_64bytes);\n-\n-        __ addq(temp1, loop_size[shift]);\n-        \/\/ Zero length check.\n-        __ jcc(Assembler::lessEqual, L_exit);\n-\n-        __ BIND(L_tail64);\n-\n-        \/\/ Tail handling using 64 byte [masked] vector copy operations.\n-        use64byteVector = true;\n-        __ arraycopy_avx3_special_cases(xmm1, k2, from, to, temp1, shift,\n-                                        temp4, temp3, use64byteVector, L_entry, L_exit);\n-      }\n-      __ BIND(L_exit);\n-    }\n+  \/\/ make sure object is 'reasonable'\n+  __ testptr(rax, rax);\n+  __ jcc(Assembler::zero, exit); \/\/ if obj is NULL it is OK\n+\n+#if INCLUDE_ZGC\n+  if (UseZGC) {\n+    \/\/ Check if metadata bits indicate a bad oop\n+    __ testptr(rax, Address(r15_thread, ZThreadLocalData::address_bad_mask_offset()));\n+    __ jcc(Assembler::notZero, error);\n+  }\n+#endif\n+\n+  \/\/ Check if the oop is in the right area of memory\n+  __ movptr(c_rarg2, rax);\n+  __ movptr(c_rarg3, (intptr_t) Universe::verify_oop_mask());\n+  __ andptr(c_rarg2, c_rarg3);\n+  __ movptr(c_rarg3, (intptr_t) Universe::verify_oop_bits());\n+  __ cmpptr(c_rarg2, c_rarg3);\n+  __ jcc(Assembler::notZero, error);\n+\n+  \/\/ make sure klass is 'reasonable', which is not zero.\n+  __ load_klass(rax, rax, rscratch1);  \/\/ get klass\n+  __ testptr(rax, rax);\n+  __ jcc(Assembler::zero, error); \/\/ if klass is NULL it is broken\n+\n+  \/\/ return if everything seems ok\n+  __ bind(exit);\n+  __ movptr(rax, Address(rsp, saved_rax));     \/\/ get saved rax back\n+  __ movptr(rscratch1, Address(rsp, saved_r10)); \/\/ get saved r10 back\n+  __ pop(c_rarg3);                             \/\/ restore c_rarg3\n+  __ pop(c_rarg2);                             \/\/ restore c_rarg2\n+  __ pop(r12);                                 \/\/ restore r12\n+  __ popf();                                   \/\/ restore flags\n+  __ ret(4 * wordSize);                        \/\/ pop caller saved stuff\n+\n+  \/\/ handle errors\n+  __ bind(error);\n+  __ movptr(rax, Address(rsp, saved_rax));     \/\/ get saved rax back\n+  __ movptr(rscratch1, Address(rsp, saved_r10)); \/\/ get saved r10 back\n+  __ pop(c_rarg3);                             \/\/ get saved c_rarg3 back\n+  __ pop(c_rarg2);                             \/\/ get saved c_rarg2 back\n+  __ pop(r12);                                 \/\/ get saved r12 back\n+  __ popf();                                   \/\/ get saved flags off stack --\n+                                               \/\/ will be ignored\n+\n+  __ pusha();                                  \/\/ push registers\n+                                               \/\/ (rip is already\n+                                               \/\/ already pushed)\n+  \/\/ debug(char* msg, int64_t pc, int64_t regs[])\n+  \/\/ We've popped the registers we'd saved (c_rarg3, c_rarg2 and flags), and\n+  \/\/ pushed all the registers, so now the stack looks like:\n+  \/\/     [tos +  0] 16 saved registers\n+  \/\/     [tos + 16] return address\n+  \/\/   * [tos + 17] error message (char*)\n+  \/\/   * [tos + 18] object to verify (oop)\n+  \/\/   * [tos + 19] saved rax - saved by caller and bashed\n+  \/\/   * [tos + 20] saved r10 (rscratch1) - saved by caller\n+  \/\/   * = popped on exit\n+\n+  __ movptr(c_rarg0, Address(rsp, error_msg));    \/\/ pass address of error message\n+  __ movptr(c_rarg1, Address(rsp, return_addr));  \/\/ pass return address\n+  __ movq(c_rarg2, rsp);                          \/\/ pass address of regs on stack\n+  __ mov(r12, rsp);                               \/\/ remember rsp\n+  __ subptr(rsp, frame::arg_reg_save_area_bytes); \/\/ windows\n+  __ andptr(rsp, -16);                            \/\/ align stack as required by ABI\n+  BLOCK_COMMENT(\"call MacroAssembler::debug\");\n+  __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, MacroAssembler::debug64)));\n+  __ hlt();\n+\n+  return start;\n+}\n+\n+\/\/\n+\/\/ Verify that a register contains clean 32-bits positive value\n+\/\/ (high 32-bits are 0) so it could be used in 64-bits shifts.\n+\/\/\n+\/\/  Input:\n+\/\/    Rint  -  32-bits value\n+\/\/    Rtmp  -  scratch\n+\/\/\n+void StubGenerator::assert_clean_int(Register Rint, Register Rtmp) {\n+#ifdef ASSERT\n+  Label L;\n+  assert_different_registers(Rtmp, Rint);\n+  __ movslq(Rtmp, Rint);\n+  __ cmpq(Rtmp, Rint);\n+  __ jcc(Assembler::equal, L);\n+  __ stop(\"high 32-bits of int value are not 0\");\n+  __ bind(L);\n+#endif\n+}\n+\n+\/\/  Generate overlap test for array copy stubs\n+\/\/\n+\/\/  Input:\n+\/\/     c_rarg0 - from\n+\/\/     c_rarg1 - to\n+\/\/     c_rarg2 - element count\n+\/\/\n+\/\/  Output:\n+\/\/     rax   - &from[element count - 1]\n+\/\/\n+void StubGenerator::array_overlap_test(address no_overlap_target, Label* NOLp, Address::ScaleFactor sf) {\n+  const Register from     = c_rarg0;\n+  const Register to       = c_rarg1;\n+  const Register count    = c_rarg2;\n+  const Register end_from = rax;\n+\n+  __ cmpptr(to, from);\n+  __ lea(end_from, Address(from, count, sf, 0));\n+  if (NOLp == NULL) {\n+    ExternalAddress no_overlap(no_overlap_target);\n+    __ jump_cc(Assembler::belowEqual, no_overlap);\n+    __ cmpptr(to, end_from);\n+    __ jump_cc(Assembler::aboveEqual, no_overlap);\n+  } else {\n+    __ jcc(Assembler::belowEqual, (*NOLp));\n+    __ cmpptr(to, end_from);\n+    __ jcc(Assembler::aboveEqual, (*NOLp));\n+  }\n+}\n@@ -1664,5 +1195,126 @@\n-    address ucme_exit_pc = __ pc();\n-    \/\/ When called from generic_arraycopy r11 contains specific values\n-    \/\/ used during arraycopy epilogue, re-initializing r11.\n-    if (is_oop) {\n-      __ movq(r11, shift == 3 ? count : to);\n+\/\/ Shuffle first three arg regs on Windows into Linux\/Solaris locations.\n+\/\/\n+\/\/ Outputs:\n+\/\/    rdi - rcx\n+\/\/    rsi - rdx\n+\/\/    rdx - r8\n+\/\/    rcx - r9\n+\/\/\n+\/\/ Registers r9 and r10 are used to save rdi and rsi on Windows, which latter\n+\/\/ are non-volatile.  r9 and r10 should not be used by the caller.\n+\/\/\n+void StubGenerator::setup_arg_regs(int nargs) {\n+  const Register saved_rdi = r9;\n+  const Register saved_rsi = r10;\n+  assert(nargs == 3 || nargs == 4, \"else fix\");\n+#ifdef _WIN64\n+  assert(c_rarg0 == rcx && c_rarg1 == rdx && c_rarg2 == r8 && c_rarg3 == r9,\n+         \"unexpected argument registers\");\n+  if (nargs >= 4)\n+    __ mov(rax, r9);  \/\/ r9 is also saved_rdi\n+  __ movptr(saved_rdi, rdi);\n+  __ movptr(saved_rsi, rsi);\n+  __ mov(rdi, rcx); \/\/ c_rarg0\n+  __ mov(rsi, rdx); \/\/ c_rarg1\n+  __ mov(rdx, r8);  \/\/ c_rarg2\n+  if (nargs >= 4)\n+    __ mov(rcx, rax); \/\/ c_rarg3 (via rax)\n+#else\n+  assert(c_rarg0 == rdi && c_rarg1 == rsi && c_rarg2 == rdx && c_rarg3 == rcx,\n+         \"unexpected argument registers\");\n+#endif\n+  DEBUG_ONLY(_regs_in_thread = false;)\n+}\n+\n+void StubGenerator::restore_arg_regs() {\n+  assert(!_regs_in_thread, \"wrong call to restore_arg_regs\");\n+  const Register saved_rdi = r9;\n+  const Register saved_rsi = r10;\n+#ifdef _WIN64\n+  __ movptr(rdi, saved_rdi);\n+  __ movptr(rsi, saved_rsi);\n+#endif\n+}\n+\n+\/\/ This is used in places where r10 is a scratch register, and can\n+\/\/ be adapted if r9 is needed also.\n+void StubGenerator::setup_arg_regs_using_thread() {\n+  const Register saved_r15 = r9;\n+#ifdef _WIN64\n+  __ mov(saved_r15, r15);  \/\/ r15 is callee saved and needs to be restored\n+  __ get_thread(r15_thread);\n+  assert(c_rarg0 == rcx && c_rarg1 == rdx && c_rarg2 == r8 && c_rarg3 == r9,\n+         \"unexpected argument registers\");\n+  __ movptr(Address(r15_thread, in_bytes(JavaThread::windows_saved_rdi_offset())), rdi);\n+  __ movptr(Address(r15_thread, in_bytes(JavaThread::windows_saved_rsi_offset())), rsi);\n+\n+  __ mov(rdi, rcx); \/\/ c_rarg0\n+  __ mov(rsi, rdx); \/\/ c_rarg1\n+  __ mov(rdx, r8);  \/\/ c_rarg2\n+#else\n+  assert(c_rarg0 == rdi && c_rarg1 == rsi && c_rarg2 == rdx && c_rarg3 == rcx,\n+         \"unexpected argument registers\");\n+#endif\n+  DEBUG_ONLY(_regs_in_thread = true;)\n+}\n+\n+void StubGenerator::restore_arg_regs_using_thread() {\n+  assert(_regs_in_thread, \"wrong call to restore_arg_regs\");\n+  const Register saved_r15 = r9;\n+#ifdef _WIN64\n+  __ get_thread(r15_thread);\n+  __ movptr(rsi, Address(r15_thread, in_bytes(JavaThread::windows_saved_rsi_offset())));\n+  __ movptr(rdi, Address(r15_thread, in_bytes(JavaThread::windows_saved_rdi_offset())));\n+  __ mov(r15, saved_r15);  \/\/ r15 is callee saved and needs to be restored\n+#endif\n+}\n+\n+\/\/ Copy big chunks forward\n+\/\/\n+\/\/ Inputs:\n+\/\/   end_from     - source arrays end address\n+\/\/   end_to       - destination array end address\n+\/\/   qword_count  - 64-bits element count, negative\n+\/\/   to           - scratch\n+\/\/   L_copy_bytes - entry label\n+\/\/   L_copy_8_bytes  - exit  label\n+\/\/\n+void StubGenerator::copy_bytes_forward(Register end_from, Register end_to,\n+                                       Register qword_count, Register to,\n+                                       Label& L_copy_bytes, Label& L_copy_8_bytes) {\n+  DEBUG_ONLY(__ stop(\"enter at entry label, not here\"));\n+  Label L_loop;\n+  __ align(OptoLoopAlignment);\n+  if (UseUnalignedLoadStores) {\n+    Label L_end;\n+    __ BIND(L_loop);\n+    if (UseAVX >= 2) {\n+      __ vmovdqu(xmm0, Address(end_from, qword_count, Address::times_8, -56));\n+      __ vmovdqu(Address(end_to, qword_count, Address::times_8, -56), xmm0);\n+      __ vmovdqu(xmm1, Address(end_from, qword_count, Address::times_8, -24));\n+      __ vmovdqu(Address(end_to, qword_count, Address::times_8, -24), xmm1);\n+    } else {\n+      __ movdqu(xmm0, Address(end_from, qword_count, Address::times_8, -56));\n+      __ movdqu(Address(end_to, qword_count, Address::times_8, -56), xmm0);\n+      __ movdqu(xmm1, Address(end_from, qword_count, Address::times_8, -40));\n+      __ movdqu(Address(end_to, qword_count, Address::times_8, -40), xmm1);\n+      __ movdqu(xmm2, Address(end_from, qword_count, Address::times_8, -24));\n+      __ movdqu(Address(end_to, qword_count, Address::times_8, -24), xmm2);\n+      __ movdqu(xmm3, Address(end_from, qword_count, Address::times_8, - 8));\n+      __ movdqu(Address(end_to, qword_count, Address::times_8, - 8), xmm3);\n+    }\n+\n+    __ BIND(L_copy_bytes);\n+    __ addptr(qword_count, 8);\n+    __ jcc(Assembler::lessEqual, L_loop);\n+    __ subptr(qword_count, 4);  \/\/ sub(8) and add(4)\n+    __ jccb(Assembler::greater, L_end);\n+    \/\/ Copy trailing 32 bytes\n+    if (UseAVX >= 2) {\n+      __ vmovdqu(xmm0, Address(end_from, qword_count, Address::times_8, -24));\n+      __ vmovdqu(Address(end_to, qword_count, Address::times_8, -24), xmm0);\n+    } else {\n+      __ movdqu(xmm0, Address(end_from, qword_count, Address::times_8, -24));\n+      __ movdqu(Address(end_to, qword_count, Address::times_8, -24), xmm0);\n+      __ movdqu(xmm1, Address(end_from, qword_count, Address::times_8, - 8));\n+      __ movdqu(Address(end_to, qword_count, Address::times_8, - 8), xmm1);\n@@ -1670,8 +1322,17 @@\n-    bs->arraycopy_epilogue(_masm, decorators, type, from, to, count);\n-    restore_argument_regs(type);\n-    INC_COUNTER_NP(get_profile_ctr(shift), rscratch1); \/\/ Update counter after rscratch1 is free\n-    __ xorptr(rax, rax); \/\/ return 0\n-    __ vzeroupper();\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-    return start;\n+    __ addptr(qword_count, 4);\n+    __ BIND(L_end);\n+  } else {\n+    \/\/ Copy 32-bytes per iteration\n+    __ BIND(L_loop);\n+    __ movq(to, Address(end_from, qword_count, Address::times_8, -24));\n+    __ movq(Address(end_to, qword_count, Address::times_8, -24), to);\n+    __ movq(to, Address(end_from, qword_count, Address::times_8, -16));\n+    __ movq(Address(end_to, qword_count, Address::times_8, -16), to);\n+    __ movq(to, Address(end_from, qword_count, Address::times_8, - 8));\n+    __ movq(Address(end_to, qword_count, Address::times_8, - 8), to);\n+    __ movq(to, Address(end_from, qword_count, Address::times_8, - 0));\n+    __ movq(Address(end_to, qword_count, Address::times_8, - 0), to);\n+\n+    __ BIND(L_copy_bytes);\n+    __ addptr(qword_count, 4);\n+    __ jcc(Assembler::lessEqual, L_loop);\n@@ -1679,0 +1340,3 @@\n+  __ subptr(qword_count, 4);\n+  __ jcc(Assembler::less, L_copy_8_bytes); \/\/ Copy trailing qwords\n+}\n@@ -1680,35 +1344,50 @@\n-  \/\/ Inputs:\n-  \/\/   c_rarg0   - source array address\n-  \/\/   c_rarg1   - destination array address\n-  \/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n-  \/\/\n-  \/\/\n-  address generate_conjoint_copy_avx3_masked(address* entry, const char *name, int shift,\n-                                             address nooverlap_target, bool aligned, bool is_oop,\n-                                             bool dest_uninitialized) {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ pc();\n-\n-    int avx3threshold = VM_Version::avx3_threshold();\n-    bool use64byteVector = (MaxVectorSize > 32) && (avx3threshold == 0);\n-\n-    Label L_main_pre_loop, L_main_pre_loop_64bytes, L_pre_main_post_64;\n-    Label L_main_loop, L_main_loop_64bytes, L_tail, L_tail64, L_exit, L_entry;\n-    const Register from        = rdi;  \/\/ source array address\n-    const Register to          = rsi;  \/\/ destination array address\n-    const Register count       = rdx;  \/\/ elements count\n-    const Register temp1       = r8;\n-    const Register temp2       = rcx;\n-    const Register temp3       = r11;\n-    const Register temp4       = rax;\n-    \/\/ End pointers are inclusive, and if count is not zero they point\n-    \/\/ to the last unit copied:  end_to[0] := end_from[0]\n-\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    assert_clean_int(c_rarg2, rax);    \/\/ Make sure 'count' is clean int.\n-\n-    if (entry != NULL) {\n-      *entry = __ pc();\n-       \/\/ caller can pass a 64-bit byte count here (from Unsafe.copyMemory)\n-      BLOCK_COMMENT(\"Entry:\");\n+\/\/ Copy big chunks backward\n+\/\/\n+\/\/ Inputs:\n+\/\/   from         - source arrays address\n+\/\/   dest         - destination array address\n+\/\/   qword_count  - 64-bits element count\n+\/\/   to           - scratch\n+\/\/   L_copy_bytes - entry label\n+\/\/   L_copy_8_bytes  - exit  label\n+\/\/\n+void StubGenerator::copy_bytes_backward(Register from, Register dest,\n+                                        Register qword_count, Register to,\n+                                        Label& L_copy_bytes, Label& L_copy_8_bytes) {\n+  DEBUG_ONLY(__ stop(\"enter at entry label, not here\"));\n+  Label L_loop;\n+  __ align(OptoLoopAlignment);\n+  if (UseUnalignedLoadStores) {\n+    Label L_end;\n+    __ BIND(L_loop);\n+    if (UseAVX >= 2) {\n+      __ vmovdqu(xmm0, Address(from, qword_count, Address::times_8, 32));\n+      __ vmovdqu(Address(dest, qword_count, Address::times_8, 32), xmm0);\n+      __ vmovdqu(xmm1, Address(from, qword_count, Address::times_8,  0));\n+      __ vmovdqu(Address(dest, qword_count, Address::times_8,  0), xmm1);\n+    } else {\n+      __ movdqu(xmm0, Address(from, qword_count, Address::times_8, 48));\n+      __ movdqu(Address(dest, qword_count, Address::times_8, 48), xmm0);\n+      __ movdqu(xmm1, Address(from, qword_count, Address::times_8, 32));\n+      __ movdqu(Address(dest, qword_count, Address::times_8, 32), xmm1);\n+      __ movdqu(xmm2, Address(from, qword_count, Address::times_8, 16));\n+      __ movdqu(Address(dest, qword_count, Address::times_8, 16), xmm2);\n+      __ movdqu(xmm3, Address(from, qword_count, Address::times_8,  0));\n+      __ movdqu(Address(dest, qword_count, Address::times_8,  0), xmm3);\n+    }\n+\n+    __ BIND(L_copy_bytes);\n+    __ subptr(qword_count, 8);\n+    __ jcc(Assembler::greaterEqual, L_loop);\n+\n+    __ addptr(qword_count, 4);  \/\/ add(8) and sub(4)\n+    __ jccb(Assembler::less, L_end);\n+    \/\/ Copy trailing 32 bytes\n+    if (UseAVX >= 2) {\n+      __ vmovdqu(xmm0, Address(from, qword_count, Address::times_8, 0));\n+      __ vmovdqu(Address(dest, qword_count, Address::times_8, 0), xmm0);\n+    } else {\n+      __ movdqu(xmm0, Address(from, qword_count, Address::times_8, 16));\n+      __ movdqu(Address(dest, qword_count, Address::times_8, 16), xmm0);\n+      __ movdqu(xmm1, Address(from, qword_count, Address::times_8,  0));\n+      __ movdqu(Address(dest, qword_count, Address::times_8,  0), xmm1);\n@@ -1716,0 +1395,21 @@\n+    __ subptr(qword_count, 4);\n+    __ BIND(L_end);\n+  } else {\n+    \/\/ Copy 32-bytes per iteration\n+    __ BIND(L_loop);\n+    __ movq(to, Address(from, qword_count, Address::times_8, 24));\n+    __ movq(Address(dest, qword_count, Address::times_8, 24), to);\n+    __ movq(to, Address(from, qword_count, Address::times_8, 16));\n+    __ movq(Address(dest, qword_count, Address::times_8, 16), to);\n+    __ movq(to, Address(from, qword_count, Address::times_8,  8));\n+    __ movq(Address(dest, qword_count, Address::times_8,  8), to);\n+    __ movq(to, Address(from, qword_count, Address::times_8,  0));\n+    __ movq(Address(dest, qword_count, Address::times_8,  0), to);\n+\n+    __ BIND(L_copy_bytes);\n+    __ subptr(qword_count, 4);\n+    __ jcc(Assembler::greaterEqual, L_loop);\n+  }\n+  __ addptr(qword_count, 4);\n+  __ jcc(Assembler::greater, L_copy_8_bytes); \/\/ Copy trailing qwords\n+}\n@@ -1717,1 +1417,9 @@\n-    array_overlap_test(nooverlap_target, (Address::ScaleFactor)(shift));\n+void StubGenerator::setup_argument_regs(BasicType type) {\n+  if (type == T_BYTE || type == T_SHORT) {\n+    setup_arg_regs(); \/\/ from => rdi, to => rsi, count => rdx\n+                      \/\/ r9 and r10 may be used to save non-volatile registers\n+  } else {\n+    setup_arg_regs_using_thread(); \/\/ from => rdi, to => rsi, count => rdx\n+                                   \/\/ r9 is used to save r15_thread\n+  }\n+}\n@@ -1719,2 +1427,7 @@\n-    BasicType type_vec[] = { T_BYTE,  T_SHORT,  T_INT,   T_LONG};\n-    BasicType type = is_oop ? T_OBJECT : type_vec[shift];\n+void StubGenerator::restore_argument_regs(BasicType type) {\n+  if (type == T_BYTE || type == T_SHORT) {\n+    restore_arg_regs();\n+  } else {\n+    restore_arg_regs_using_thread();\n+  }\n+}\n@@ -1722,1 +1435,90 @@\n-    setup_argument_regs(type);\n+#if COMPILER2_OR_JVMCI\n+\/\/ Note: Following rules apply to AVX3 optimized arraycopy stubs:-\n+\/\/ - If target supports AVX3 features (BW+VL+F) then implementation uses 32 byte vectors (YMMs)\n+\/\/   for both special cases (various small block sizes) and aligned copy loop. This is the\n+\/\/   default configuration.\n+\/\/ - If copy length is above AVX3Threshold, then implementation use 64 byte vectors (ZMMs)\n+\/\/   for main copy loop (and subsequent tail) since bulk of the cycles will be consumed in it.\n+\/\/ - If user forces MaxVectorSize=32 then above 4096 bytes its seen that REP MOVs shows a\n+\/\/   better performance for disjoint copies. For conjoint\/backward copy vector based\n+\/\/   copy performs better.\n+\/\/ - If user sets AVX3Threshold=0, then special cases for small blocks sizes operate over\n+\/\/   64 byte vector registers (ZMMs).\n+\n+\/\/ Inputs:\n+\/\/   c_rarg0   - source array address\n+\/\/   c_rarg1   - destination array address\n+\/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n+\/\/\n+\/\/\n+\/\/ Side Effects:\n+\/\/   disjoint_copy_avx3_masked is set to the no-overlap entry point\n+\/\/   used by generate_conjoint_[byte\/int\/short\/long]_copy().\n+\/\/\n+address StubGenerator::generate_disjoint_copy_avx3_masked(address* entry, const char *name,\n+                                                          int shift, bool aligned, bool is_oop,\n+                                                          bool dest_uninitialized) {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", name);\n+  address start = __ pc();\n+\n+  int avx3threshold = VM_Version::avx3_threshold();\n+  bool use64byteVector = (MaxVectorSize > 32) && (avx3threshold == 0);\n+  Label L_main_loop, L_main_loop_64bytes, L_tail, L_tail64, L_exit, L_entry;\n+  Label L_repmovs, L_main_pre_loop, L_main_pre_loop_64bytes, L_pre_main_post_64;\n+  const Register from        = rdi;  \/\/ source array address\n+  const Register to          = rsi;  \/\/ destination array address\n+  const Register count       = rdx;  \/\/ elements count\n+  const Register temp1       = r8;\n+  const Register temp2       = r11;\n+  const Register temp3       = rax;\n+  const Register temp4       = rcx;\n+  \/\/ End pointers are inclusive, and if count is not zero they point\n+  \/\/ to the last unit copied:  end_to[0] := end_from[0]\n+\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  assert_clean_int(c_rarg2, rax);    \/\/ Make sure 'count' is clean int.\n+\n+  if (entry != NULL) {\n+    *entry = __ pc();\n+     \/\/ caller can pass a 64-bit byte count here (from Unsafe.copyMemory)\n+    BLOCK_COMMENT(\"Entry:\");\n+  }\n+\n+  BasicType type_vec[] = { T_BYTE,  T_SHORT,  T_INT,   T_LONG};\n+  BasicType type = is_oop ? T_OBJECT : type_vec[shift];\n+\n+  setup_argument_regs(type);\n+\n+  DecoratorSet decorators = IN_HEAP | IS_ARRAY | ARRAYCOPY_DISJOINT;\n+  if (dest_uninitialized) {\n+    decorators |= IS_DEST_UNINITIALIZED;\n+  }\n+  if (aligned) {\n+    decorators |= ARRAYCOPY_ALIGNED;\n+  }\n+  BarrierSetAssembler *bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  bs->arraycopy_prologue(_masm, decorators, type, from, to, count);\n+\n+  {\n+    \/\/ Type(shift)           byte(0), short(1), int(2),   long(3)\n+    int loop_size[]        = { 192,     96,       48,      24};\n+    int threshold[]        = { 4096,    2048,     1024,    512};\n+\n+    \/\/ UnsafeCopyMemory page error: continue after ucm\n+    UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n+    \/\/ 'from', 'to' and 'count' are now valid\n+\n+    \/\/ temp1 holds remaining count and temp4 holds running count used to compute\n+    \/\/ next address offset for start of to\/from addresses (temp4 * scale).\n+    __ mov64(temp4, 0);\n+    __ movq(temp1, count);\n+\n+    \/\/ Zero length check.\n+    __ BIND(L_tail);\n+    __ cmpq(temp1, 0);\n+    __ jcc(Assembler::lessEqual, L_exit);\n+\n+    \/\/ Special cases using 32 byte [masked] vector copy operations.\n+    __ arraycopy_avx3_special_cases(xmm1, k2, from, to, temp1, shift,\n+                                    temp4, temp3, use64byteVector, L_entry, L_exit);\n@@ -1724,3 +1526,13 @@\n-    DecoratorSet decorators = IN_HEAP | IS_ARRAY;\n-    if (dest_uninitialized) {\n-      decorators |= IS_DEST_UNINITIALIZED;\n+    \/\/ PRE-MAIN-POST loop for aligned copy.\n+    __ BIND(L_entry);\n+\n+    if (avx3threshold != 0) {\n+      __ cmpq(count, threshold[shift]);\n+      if (MaxVectorSize == 64) {\n+        \/\/ Copy using 64 byte vectors.\n+        __ jcc(Assembler::greaterEqual, L_pre_main_post_64);\n+      } else {\n+        assert(MaxVectorSize < 64, \"vector size should be < 64 bytes\");\n+        \/\/ REP MOVS offer a faster copy path.\n+        __ jcc(Assembler::greaterEqual, L_repmovs);\n+      }\n@@ -1728,2 +1540,64 @@\n-    if (aligned) {\n-      decorators |= ARRAYCOPY_ALIGNED;\n+\n+    if ((MaxVectorSize < 64)  || (avx3threshold != 0)) {\n+      \/\/ Partial copy to make dst address 32 byte aligned.\n+      __ movq(temp2, to);\n+      __ andq(temp2, 31);\n+      __ jcc(Assembler::equal, L_main_pre_loop);\n+\n+      __ negptr(temp2);\n+      __ addq(temp2, 32);\n+      if (shift) {\n+        __ shrq(temp2, shift);\n+      }\n+      __ movq(temp3, temp2);\n+      __ copy32_masked_avx(to, from, xmm1, k2, temp3, temp4, temp1, shift);\n+      __ movq(temp4, temp2);\n+      __ movq(temp1, count);\n+      __ subq(temp1, temp2);\n+\n+      __ cmpq(temp1, loop_size[shift]);\n+      __ jcc(Assembler::less, L_tail);\n+\n+      __ BIND(L_main_pre_loop);\n+      __ subq(temp1, loop_size[shift]);\n+\n+      \/\/ Main loop with aligned copy block size of 192 bytes at 32 byte granularity.\n+      __ align32();\n+      __ BIND(L_main_loop);\n+         __ copy64_avx(to, from, temp4, xmm1, false, shift, 0);\n+         __ copy64_avx(to, from, temp4, xmm1, false, shift, 64);\n+         __ copy64_avx(to, from, temp4, xmm1, false, shift, 128);\n+         __ addptr(temp4, loop_size[shift]);\n+         __ subq(temp1, loop_size[shift]);\n+         __ jcc(Assembler::greater, L_main_loop);\n+\n+      __ addq(temp1, loop_size[shift]);\n+\n+      \/\/ Tail loop.\n+      __ jmp(L_tail);\n+\n+      __ BIND(L_repmovs);\n+        __ movq(temp2, temp1);\n+        \/\/ Swap to(RSI) and from(RDI) addresses to comply with REP MOVs semantics.\n+        __ movq(temp3, to);\n+        __ movq(to,  from);\n+        __ movq(from, temp3);\n+        \/\/ Save to\/from for restoration post rep_mov.\n+        __ movq(temp1, to);\n+        __ movq(temp3, from);\n+        if(shift < 3) {\n+          __ shrq(temp2, 3-shift);     \/\/ quad word count\n+        }\n+        __ movq(temp4 , temp2);        \/\/ move quad ward count into temp4(RCX).\n+        __ rep_mov();\n+        __ shlq(temp2, 3);             \/\/ convert quad words into byte count.\n+        if(shift) {\n+          __ shrq(temp2, shift);       \/\/ type specific count.\n+        }\n+        \/\/ Restore original addresses in to\/from.\n+        __ movq(to, temp3);\n+        __ movq(from, temp1);\n+        __ movq(temp4, temp2);\n+        __ movq(temp1, count);\n+        __ subq(temp1, temp2);         \/\/ tailing part (less than a quad ward size).\n+        __ jmp(L_tail);\n@@ -1731,12 +1605,16 @@\n-    BarrierSetAssembler *bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-    bs->arraycopy_prologue(_masm, decorators, type, from, to, count);\n-    {\n-      \/\/ Type(shift)       byte(0), short(1), int(2),   long(3)\n-      int loop_size[]   = { 192,     96,       48,      24};\n-      int threshold[]   = { 4096,    2048,     1024,    512};\n-\n-      \/\/ UnsafeCopyMemory page error: continue after ucm\n-      UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n-      \/\/ 'from', 'to' and 'count' are now valid\n-\n-      \/\/ temp1 holds remaining count.\n+\n+    if (MaxVectorSize > 32) {\n+      __ BIND(L_pre_main_post_64);\n+      \/\/ Partial copy to make dst address 64 byte aligned.\n+      __ movq(temp2, to);\n+      __ andq(temp2, 63);\n+      __ jcc(Assembler::equal, L_main_pre_loop_64bytes);\n+\n+      __ negptr(temp2);\n+      __ addq(temp2, 64);\n+      if (shift) {\n+        __ shrq(temp2, shift);\n+      }\n+      __ movq(temp3, temp2);\n+      __ copy64_masked_avx(to, from, xmm1, k2, temp3, temp4, temp1, shift, 0 , true);\n+      __ movq(temp4, temp2);\n@@ -1744,0 +1622,7 @@\n+      __ subq(temp1, temp2);\n+\n+      __ cmpq(temp1, loop_size[shift]);\n+      __ jcc(Assembler::less, L_tail64);\n+\n+      __ BIND(L_main_pre_loop_64bytes);\n+      __ subq(temp1, loop_size[shift]);\n@@ -1745,0 +1630,12 @@\n+      \/\/ Main loop with aligned copy block size of 192 bytes at\n+      \/\/ 64 byte copy granularity.\n+      __ align32();\n+      __ BIND(L_main_loop_64bytes);\n+         __ copy64_avx(to, from, temp4, xmm1, false, shift, 0 , true);\n+         __ copy64_avx(to, from, temp4, xmm1, false, shift, 64, true);\n+         __ copy64_avx(to, from, temp4, xmm1, false, shift, 128, true);\n+         __ addptr(temp4, loop_size[shift]);\n+         __ subq(temp1, loop_size[shift]);\n+         __ jcc(Assembler::greater, L_main_loop_64bytes);\n+\n+      __ addq(temp1, loop_size[shift]);\n@@ -1746,2 +1643,0 @@\n-      __ BIND(L_tail);\n-      __ cmpq(temp1, 0);\n@@ -1750,5 +1645,1 @@\n-      __ mov64(temp2, 0);\n-      __ movq(temp3, temp1);\n-      \/\/ Special cases using 32 byte [masked] vector copy operations.\n-      __ arraycopy_avx3_special_cases_conjoint(xmm1, k2, from, to, temp2, temp3, temp1, shift,\n-                                               temp4, use64byteVector, L_entry, L_exit);\n+      __ BIND(L_tail64);\n@@ -1756,2 +1647,7 @@\n-      \/\/ PRE-MAIN-POST loop for aligned copy.\n-      __ BIND(L_entry);\n+      \/\/ Tail handling using 64 byte [masked] vector copy operations.\n+      use64byteVector = true;\n+      __ arraycopy_avx3_special_cases(xmm1, k2, from, to, temp1, shift,\n+                                      temp4, temp3, use64byteVector, L_entry, L_exit);\n+    }\n+    __ BIND(L_exit);\n+  }\n@@ -1759,4 +1655,13 @@\n-      if ((MaxVectorSize > 32) && (avx3threshold != 0)) {\n-        __ cmpq(temp1, threshold[shift]);\n-        __ jcc(Assembler::greaterEqual, L_pre_main_post_64);\n-      }\n+  address ucme_exit_pc = __ pc();\n+  \/\/ When called from generic_arraycopy r11 contains specific values\n+  \/\/ used during arraycopy epilogue, re-initializing r11.\n+  if (is_oop) {\n+    __ movq(r11, shift == 3 ? count : to);\n+  }\n+  bs->arraycopy_epilogue(_masm, decorators, type, from, to, count);\n+  restore_argument_regs(type);\n+  INC_COUNTER_NP(get_profile_ctr(shift), rscratch1); \/\/ Update counter after rscratch1 is free\n+  __ xorptr(rax, rax); \/\/ return 0\n+  __ vzeroupper();\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n@@ -1764,5 +1669,2 @@\n-      if ((MaxVectorSize < 64)  || (avx3threshold != 0)) {\n-        \/\/ Partial copy to make dst address 32 byte aligned.\n-        __ leaq(temp2, Address(to, temp1, (Address::ScaleFactor)(shift), 0));\n-        __ andq(temp2, 31);\n-        __ jcc(Assembler::equal, L_main_pre_loop);\n+  return start;\n+}\n@@ -1770,5 +1672,35 @@\n-        if (shift) {\n-          __ shrq(temp2, shift);\n-        }\n-        __ subq(temp1, temp2);\n-        __ copy32_masked_avx(to, from, xmm1, k2, temp2, temp1, temp3, shift);\n+\/\/ Inputs:\n+\/\/   c_rarg0   - source array address\n+\/\/   c_rarg1   - destination array address\n+\/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n+\/\/\n+address StubGenerator::generate_conjoint_copy_avx3_masked(address* entry, const char *name, int shift,\n+                                                          address nooverlap_target, bool aligned,\n+                                                          bool is_oop, bool dest_uninitialized) {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", name);\n+  address start = __ pc();\n+\n+  int avx3threshold = VM_Version::avx3_threshold();\n+  bool use64byteVector = (MaxVectorSize > 32) && (avx3threshold == 0);\n+\n+  Label L_main_pre_loop, L_main_pre_loop_64bytes, L_pre_main_post_64;\n+  Label L_main_loop, L_main_loop_64bytes, L_tail, L_tail64, L_exit, L_entry;\n+  const Register from        = rdi;  \/\/ source array address\n+  const Register to          = rsi;  \/\/ destination array address\n+  const Register count       = rdx;  \/\/ elements count\n+  const Register temp1       = r8;\n+  const Register temp2       = rcx;\n+  const Register temp3       = r11;\n+  const Register temp4       = rax;\n+  \/\/ End pointers are inclusive, and if count is not zero they point\n+  \/\/ to the last unit copied:  end_to[0] := end_from[0]\n+\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  assert_clean_int(c_rarg2, rax);    \/\/ Make sure 'count' is clean int.\n+\n+  if (entry != NULL) {\n+    *entry = __ pc();\n+     \/\/ caller can pass a 64-bit byte count here (from Unsafe.copyMemory)\n+    BLOCK_COMMENT(\"Entry:\");\n+  }\n@@ -1776,2 +1708,1 @@\n-        __ cmpq(temp1, loop_size[shift]);\n-        __ jcc(Assembler::less, L_tail);\n+  array_overlap_test(nooverlap_target, (Address::ScaleFactor)(shift));\n@@ -1779,1 +1710,2 @@\n-        __ BIND(L_main_pre_loop);\n+  BasicType type_vec[] = { T_BYTE,  T_SHORT,  T_INT,   T_LONG};\n+  BasicType type = is_oop ? T_OBJECT : type_vec[shift];\n@@ -1781,9 +1713,1 @@\n-        \/\/ Main loop with aligned copy block size of 192 bytes at 32 byte granularity.\n-        __ align32();\n-        __ BIND(L_main_loop);\n-           __ copy64_avx(to, from, temp1, xmm1, true, shift, -64);\n-           __ copy64_avx(to, from, temp1, xmm1, true, shift, -128);\n-           __ copy64_avx(to, from, temp1, xmm1, true, shift, -192);\n-           __ subptr(temp1, loop_size[shift]);\n-           __ cmpq(temp1, loop_size[shift]);\n-           __ jcc(Assembler::greater, L_main_loop);\n+  setup_argument_regs(type);\n@@ -1791,2 +1715,48 @@\n-        \/\/ Tail loop.\n-        __ jmp(L_tail);\n+  DecoratorSet decorators = IN_HEAP | IS_ARRAY;\n+  if (dest_uninitialized) {\n+    decorators |= IS_DEST_UNINITIALIZED;\n+  }\n+  if (aligned) {\n+    decorators |= ARRAYCOPY_ALIGNED;\n+  }\n+  BarrierSetAssembler *bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  bs->arraycopy_prologue(_masm, decorators, type, from, to, count);\n+  {\n+    \/\/ Type(shift)       byte(0), short(1), int(2),   long(3)\n+    int loop_size[]   = { 192,     96,       48,      24};\n+    int threshold[]   = { 4096,    2048,     1024,    512};\n+\n+    \/\/ UnsafeCopyMemory page error: continue after ucm\n+    UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n+    \/\/ 'from', 'to' and 'count' are now valid\n+\n+    \/\/ temp1 holds remaining count.\n+    __ movq(temp1, count);\n+\n+    \/\/ Zero length check.\n+    __ BIND(L_tail);\n+    __ cmpq(temp1, 0);\n+    __ jcc(Assembler::lessEqual, L_exit);\n+\n+    __ mov64(temp2, 0);\n+    __ movq(temp3, temp1);\n+    \/\/ Special cases using 32 byte [masked] vector copy operations.\n+    __ arraycopy_avx3_special_cases_conjoint(xmm1, k2, from, to, temp2, temp3, temp1, shift,\n+                                             temp4, use64byteVector, L_entry, L_exit);\n+\n+    \/\/ PRE-MAIN-POST loop for aligned copy.\n+    __ BIND(L_entry);\n+\n+    if ((MaxVectorSize > 32) && (avx3threshold != 0)) {\n+      __ cmpq(temp1, threshold[shift]);\n+      __ jcc(Assembler::greaterEqual, L_pre_main_post_64);\n+    }\n+\n+    if ((MaxVectorSize < 64)  || (avx3threshold != 0)) {\n+      \/\/ Partial copy to make dst address 32 byte aligned.\n+      __ leaq(temp2, Address(to, temp1, (Address::ScaleFactor)(shift), 0));\n+      __ andq(temp2, 31);\n+      __ jcc(Assembler::equal, L_main_pre_loop);\n+\n+      if (shift) {\n+        __ shrq(temp2, shift);\n@@ -1794,0 +1764,2 @@\n+      __ subq(temp1, temp2);\n+      __ copy32_masked_avx(to, from, xmm1, k2, temp2, temp1, temp3, shift);\n@@ -1795,6 +1767,2 @@\n-      if (MaxVectorSize > 32) {\n-        __ BIND(L_pre_main_post_64);\n-        \/\/ Partial copy to make dst address 64 byte aligned.\n-        __ leaq(temp2, Address(to, temp1, (Address::ScaleFactor)(shift), 0));\n-        __ andq(temp2, 63);\n-        __ jcc(Assembler::equal, L_main_pre_loop_64bytes);\n+      __ cmpq(temp1, loop_size[shift]);\n+      __ jcc(Assembler::less, L_tail);\n@@ -1802,34 +1770,25 @@\n-        if (shift) {\n-          __ shrq(temp2, shift);\n-        }\n-        __ subq(temp1, temp2);\n-        __ copy64_masked_avx(to, from, xmm1, k2, temp2, temp1, temp3, shift, 0 , true);\n-\n-        __ cmpq(temp1, loop_size[shift]);\n-        __ jcc(Assembler::less, L_tail64);\n-\n-        __ BIND(L_main_pre_loop_64bytes);\n-\n-        \/\/ Main loop with aligned copy block size of 192 bytes at\n-        \/\/ 64 byte copy granularity.\n-        __ align32();\n-        __ BIND(L_main_loop_64bytes);\n-           __ copy64_avx(to, from, temp1, xmm1, true, shift, -64 , true);\n-           __ copy64_avx(to, from, temp1, xmm1, true, shift, -128, true);\n-           __ copy64_avx(to, from, temp1, xmm1, true, shift, -192, true);\n-           __ subq(temp1, loop_size[shift]);\n-           __ cmpq(temp1, loop_size[shift]);\n-           __ jcc(Assembler::greater, L_main_loop_64bytes);\n-\n-        \/\/ Zero length check.\n-        __ cmpq(temp1, 0);\n-        __ jcc(Assembler::lessEqual, L_exit);\n-\n-        __ BIND(L_tail64);\n-\n-        \/\/ Tail handling using 64 byte [masked] vector copy operations.\n-        use64byteVector = true;\n-        __ mov64(temp2, 0);\n-        __ movq(temp3, temp1);\n-        __ arraycopy_avx3_special_cases_conjoint(xmm1, k2, from, to, temp2, temp3, temp1, shift,\n-                                                 temp4, use64byteVector, L_entry, L_exit);\n+      __ BIND(L_main_pre_loop);\n+\n+      \/\/ Main loop with aligned copy block size of 192 bytes at 32 byte granularity.\n+      __ align32();\n+      __ BIND(L_main_loop);\n+         __ copy64_avx(to, from, temp1, xmm1, true, shift, -64);\n+         __ copy64_avx(to, from, temp1, xmm1, true, shift, -128);\n+         __ copy64_avx(to, from, temp1, xmm1, true, shift, -192);\n+         __ subptr(temp1, loop_size[shift]);\n+         __ cmpq(temp1, loop_size[shift]);\n+         __ jcc(Assembler::greater, L_main_loop);\n+\n+      \/\/ Tail loop.\n+      __ jmp(L_tail);\n+    }\n+\n+    if (MaxVectorSize > 32) {\n+      __ BIND(L_pre_main_post_64);\n+      \/\/ Partial copy to make dst address 64 byte aligned.\n+      __ leaq(temp2, Address(to, temp1, (Address::ScaleFactor)(shift), 0));\n+      __ andq(temp2, 63);\n+      __ jcc(Assembler::equal, L_main_pre_loop_64bytes);\n+\n+      if (shift) {\n+        __ shrq(temp2, shift);\n@@ -1837,7 +1796,31 @@\n-      __ BIND(L_exit);\n-    }\n-    address ucme_exit_pc = __ pc();\n-    \/\/ When called from generic_arraycopy r11 contains specific values\n-    \/\/ used during arraycopy epilogue, re-initializing r11.\n-    if(is_oop) {\n-      __ movq(r11, count);\n+      __ subq(temp1, temp2);\n+      __ copy64_masked_avx(to, from, xmm1, k2, temp2, temp1, temp3, shift, 0 , true);\n+\n+      __ cmpq(temp1, loop_size[shift]);\n+      __ jcc(Assembler::less, L_tail64);\n+\n+      __ BIND(L_main_pre_loop_64bytes);\n+\n+      \/\/ Main loop with aligned copy block size of 192 bytes at\n+      \/\/ 64 byte copy granularity.\n+      __ align32();\n+      __ BIND(L_main_loop_64bytes);\n+         __ copy64_avx(to, from, temp1, xmm1, true, shift, -64 , true);\n+         __ copy64_avx(to, from, temp1, xmm1, true, shift, -128, true);\n+         __ copy64_avx(to, from, temp1, xmm1, true, shift, -192, true);\n+         __ subq(temp1, loop_size[shift]);\n+         __ cmpq(temp1, loop_size[shift]);\n+         __ jcc(Assembler::greater, L_main_loop_64bytes);\n+\n+      \/\/ Zero length check.\n+      __ cmpq(temp1, 0);\n+      __ jcc(Assembler::lessEqual, L_exit);\n+\n+      __ BIND(L_tail64);\n+\n+      \/\/ Tail handling using 64 byte [masked] vector copy operations.\n+      use64byteVector = true;\n+      __ mov64(temp2, 0);\n+      __ movq(temp3, temp1);\n+      __ arraycopy_avx3_special_cases_conjoint(xmm1, k2, from, to, temp2, temp3, temp1, shift,\n+                                               temp4, use64byteVector, L_entry, L_exit);\n@@ -1845,8 +1828,1 @@\n-    bs->arraycopy_epilogue(_masm, decorators, type, from, to, count);\n-    restore_argument_regs(type);\n-    INC_COUNTER_NP(get_profile_ctr(shift), rscratch1); \/\/ Update counter after rscratch1 is free\n-    __ xorptr(rax, rax); \/\/ return 0\n-    __ vzeroupper();\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-    return start;\n+    __ BIND(L_exit);\n@@ -1854,0 +1830,16 @@\n+  address ucme_exit_pc = __ pc();\n+  \/\/ When called from generic_arraycopy r11 contains specific values\n+  \/\/ used during arraycopy epilogue, re-initializing r11.\n+  if(is_oop) {\n+    __ movq(r11, count);\n+  }\n+  bs->arraycopy_epilogue(_masm, decorators, type, from, to, count);\n+  restore_argument_regs(type);\n+  INC_COUNTER_NP(get_profile_ctr(shift), rscratch1); \/\/ Update counter after rscratch1 is free\n+  __ xorptr(rax, rax); \/\/ return 0\n+  __ vzeroupper();\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n+\n+  return start;\n+}\n@@ -1857,20 +1849,20 @@\n-  \/\/ Arguments:\n-  \/\/   aligned - true => Input and output aligned on a HeapWord == 8-byte boundary\n-  \/\/             ignored\n-  \/\/   name    - stub name string\n-  \/\/\n-  \/\/ Inputs:\n-  \/\/   c_rarg0   - source array address\n-  \/\/   c_rarg1   - destination array address\n-  \/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n-  \/\/\n-  \/\/ If 'from' and\/or 'to' are aligned on 4-, 2-, or 1-byte boundaries,\n-  \/\/ we let the hardware handle it.  The one to eight bytes within words,\n-  \/\/ dwords or qwords that span cache line boundaries will still be loaded\n-  \/\/ and stored atomically.\n-  \/\/\n-  \/\/ Side Effects:\n-  \/\/   disjoint_byte_copy_entry is set to the no-overlap entry point\n-  \/\/   used by generate_conjoint_byte_copy().\n-  \/\/\n-  address generate_disjoint_byte_copy(bool aligned, address* entry, const char *name) {\n+\/\/ Arguments:\n+\/\/   aligned - true => Input and output aligned on a HeapWord == 8-byte boundary\n+\/\/             ignored\n+\/\/   name    - stub name string\n+\/\/\n+\/\/ Inputs:\n+\/\/   c_rarg0   - source array address\n+\/\/   c_rarg1   - destination array address\n+\/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n+\/\/\n+\/\/ If 'from' and\/or 'to' are aligned on 4-, 2-, or 1-byte boundaries,\n+\/\/ we let the hardware handle it.  The one to eight bytes within words,\n+\/\/ dwords or qwords that span cache line boundaries will still be loaded\n+\/\/ and stored atomically.\n+\/\/\n+\/\/ Side Effects:\n+\/\/   disjoint_byte_copy_entry is set to the no-overlap entry point\n+\/\/   used by generate_conjoint_byte_copy().\n+\/\/\n+address StubGenerator::generate_disjoint_byte_copy(bool aligned, address* entry, const char *name) {\n@@ -1878,4 +1870,4 @@\n-    if (VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2() && MaxVectorSize  >= 32) {\n-       return generate_disjoint_copy_avx3_masked(entry, \"jbyte_disjoint_arraycopy_avx3\", 0,\n-                                                 aligned, false, false);\n-    }\n+  if (VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2() && MaxVectorSize  >= 32) {\n+     return generate_disjoint_copy_avx3_masked(entry, \"jbyte_disjoint_arraycopy_avx3\", 0,\n+                                               aligned, false, false);\n+  }\n@@ -1883,34 +1875,24 @@\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ pc();\n-\n-    Label L_copy_bytes, L_copy_8_bytes, L_copy_4_bytes, L_copy_2_bytes;\n-    Label L_copy_byte, L_exit;\n-    const Register from        = rdi;  \/\/ source array address\n-    const Register to          = rsi;  \/\/ destination array address\n-    const Register count       = rdx;  \/\/ elements count\n-    const Register byte_count  = rcx;\n-    const Register qword_count = count;\n-    const Register end_from    = from; \/\/ source array end address\n-    const Register end_to      = to;   \/\/ destination array end address\n-    \/\/ End pointers are inclusive, and if count is not zero they point\n-    \/\/ to the last unit copied:  end_to[0] := end_from[0]\n-\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    assert_clean_int(c_rarg2, rax);    \/\/ Make sure 'count' is clean int.\n-\n-    if (entry != NULL) {\n-      *entry = __ pc();\n-       \/\/ caller can pass a 64-bit byte count here (from Unsafe.copyMemory)\n-      BLOCK_COMMENT(\"Entry:\");\n-    }\n-\n-    setup_arg_regs(); \/\/ from => rdi, to => rsi, count => rdx\n-                      \/\/ r9 and r10 may be used to save non-volatile registers\n-\n-    {\n-      \/\/ UnsafeCopyMemory page error: continue after ucm\n-      UnsafeCopyMemoryMark ucmm(this, !aligned, true);\n-      \/\/ 'from', 'to' and 'count' are now valid\n-      __ movptr(byte_count, count);\n-      __ shrptr(count, 3); \/\/ count => qword_count\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", name);\n+  address start = __ pc();\n+\n+  Label L_copy_bytes, L_copy_8_bytes, L_copy_4_bytes, L_copy_2_bytes;\n+  Label L_copy_byte, L_exit;\n+  const Register from        = rdi;  \/\/ source array address\n+  const Register to          = rsi;  \/\/ destination array address\n+  const Register count       = rdx;  \/\/ elements count\n+  const Register byte_count  = rcx;\n+  const Register qword_count = count;\n+  const Register end_from    = from; \/\/ source array end address\n+  const Register end_to      = to;   \/\/ destination array end address\n+  \/\/ End pointers are inclusive, and if count is not zero they point\n+  \/\/ to the last unit copied:  end_to[0] := end_from[0]\n+\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  assert_clean_int(c_rarg2, rax);    \/\/ Make sure 'count' is clean int.\n+\n+  if (entry != NULL) {\n+    *entry = __ pc();\n+     \/\/ caller can pass a 64-bit byte count here (from Unsafe.copyMemory)\n+    BLOCK_COMMENT(\"Entry:\");\n+  }\n@@ -1918,5 +1900,2 @@\n-      \/\/ Copy from low to high addresses.  Use 'to' as scratch.\n-      __ lea(end_from, Address(from, qword_count, Address::times_8, -8));\n-      __ lea(end_to,   Address(to,   qword_count, Address::times_8, -8));\n-      __ negptr(qword_count); \/\/ make the count negative\n-      __ jmp(L_copy_bytes);\n+  setup_arg_regs(); \/\/ from => rdi, to => rsi, count => rdx\n+                    \/\/ r9 and r10 may be used to save non-volatile registers\n@@ -1924,1 +1903,14 @@\n-      \/\/ Copy trailing qwords\n+  {\n+    \/\/ UnsafeCopyMemory page error: continue after ucm\n+    UnsafeCopyMemoryMark ucmm(this, !aligned, true);\n+    \/\/ 'from', 'to' and 'count' are now valid\n+    __ movptr(byte_count, count);\n+    __ shrptr(count, 3); \/\/ count => qword_count\n+\n+    \/\/ Copy from low to high addresses.  Use 'to' as scratch.\n+    __ lea(end_from, Address(from, qword_count, Address::times_8, -8));\n+    __ lea(end_to,   Address(to,   qword_count, Address::times_8, -8));\n+    __ negptr(qword_count); \/\/ make the count negative\n+    __ jmp(L_copy_bytes);\n+\n+    \/\/ Copy trailing qwords\n@@ -1926,4 +1918,4 @@\n-      __ movq(rax, Address(end_from, qword_count, Address::times_8, 8));\n-      __ movq(Address(end_to, qword_count, Address::times_8, 8), rax);\n-      __ increment(qword_count);\n-      __ jcc(Assembler::notZero, L_copy_8_bytes);\n+    __ movq(rax, Address(end_from, qword_count, Address::times_8, 8));\n+    __ movq(Address(end_to, qword_count, Address::times_8, 8), rax);\n+    __ increment(qword_count);\n+    __ jcc(Assembler::notZero, L_copy_8_bytes);\n@@ -1931,1 +1923,1 @@\n-      \/\/ Check for and copy trailing dword\n+    \/\/ Check for and copy trailing dword\n@@ -1933,4 +1925,4 @@\n-      __ testl(byte_count, 4);\n-      __ jccb(Assembler::zero, L_copy_2_bytes);\n-      __ movl(rax, Address(end_from, 8));\n-      __ movl(Address(end_to, 8), rax);\n+    __ testl(byte_count, 4);\n+    __ jccb(Assembler::zero, L_copy_2_bytes);\n+    __ movl(rax, Address(end_from, 8));\n+    __ movl(Address(end_to, 8), rax);\n@@ -1938,2 +1930,2 @@\n-      __ addptr(end_from, 4);\n-      __ addptr(end_to, 4);\n+    __ addptr(end_from, 4);\n+    __ addptr(end_to, 4);\n@@ -1941,1 +1933,1 @@\n-      \/\/ Check for and copy trailing word\n+    \/\/ Check for and copy trailing word\n@@ -1943,4 +1935,4 @@\n-      __ testl(byte_count, 2);\n-      __ jccb(Assembler::zero, L_copy_byte);\n-      __ movw(rax, Address(end_from, 8));\n-      __ movw(Address(end_to, 8), rax);\n+    __ testl(byte_count, 2);\n+    __ jccb(Assembler::zero, L_copy_byte);\n+    __ movw(rax, Address(end_from, 8));\n+    __ movw(Address(end_to, 8), rax);\n@@ -1948,2 +1940,2 @@\n-      __ addptr(end_from, 2);\n-      __ addptr(end_to, 2);\n+    __ addptr(end_from, 2);\n+    __ addptr(end_to, 2);\n@@ -1951,1 +1943,1 @@\n-      \/\/ Check for and copy trailing byte\n+    \/\/ Check for and copy trailing byte\n@@ -1953,5 +1945,5 @@\n-      __ testl(byte_count, 1);\n-      __ jccb(Assembler::zero, L_exit);\n-      __ movb(rax, Address(end_from, 8));\n-      __ movb(Address(end_to, 8), rax);\n-    }\n+    __ testl(byte_count, 1);\n+    __ jccb(Assembler::zero, L_exit);\n+    __ movb(rax, Address(end_from, 8));\n+    __ movb(Address(end_to, 8), rax);\n+  }\n@@ -1959,7 +1951,7 @@\n-    address ucme_exit_pc = __ pc();\n-    restore_arg_regs();\n-    INC_COUNTER_NP(SharedRuntime::_jbyte_array_copy_ctr, rscratch1); \/\/ Update counter after rscratch1 is free\n-    __ xorptr(rax, rax); \/\/ return 0\n-    __ vzeroupper();\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n+  address ucme_exit_pc = __ pc();\n+  restore_arg_regs();\n+  INC_COUNTER_NP(SharedRuntime::_jbyte_array_copy_ctr, rscratch1); \/\/ Update counter after rscratch1 is free\n+  __ xorptr(rax, rax); \/\/ return 0\n+  __ vzeroupper();\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n@@ -1967,7 +1959,5 @@\n-    {\n-      UnsafeCopyMemoryMark ucmm(this, !aligned, false, ucme_exit_pc);\n-      \/\/ Copy in multi-bytes chunks\n-      copy_bytes_forward(end_from, end_to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n-      __ jmp(L_copy_4_bytes);\n-    }\n-    return start;\n+  {\n+    UnsafeCopyMemoryMark ucmm(this, !aligned, false, ucme_exit_pc);\n+    \/\/ Copy in multi-bytes chunks\n+    copy_bytes_forward(end_from, end_to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n+    __ jmp(L_copy_4_bytes);\n@@ -1975,0 +1965,2 @@\n+  return start;\n+}\n@@ -1976,17 +1968,17 @@\n-  \/\/ Arguments:\n-  \/\/   aligned - true => Input and output aligned on a HeapWord == 8-byte boundary\n-  \/\/             ignored\n-  \/\/   name    - stub name string\n-  \/\/\n-  \/\/ Inputs:\n-  \/\/   c_rarg0   - source array address\n-  \/\/   c_rarg1   - destination array address\n-  \/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n-  \/\/\n-  \/\/ If 'from' and\/or 'to' are aligned on 4-, 2-, or 1-byte boundaries,\n-  \/\/ we let the hardware handle it.  The one to eight bytes within words,\n-  \/\/ dwords or qwords that span cache line boundaries will still be loaded\n-  \/\/ and stored atomically.\n-  \/\/\n-  address generate_conjoint_byte_copy(bool aligned, address nooverlap_target,\n-                                      address* entry, const char *name) {\n+\/\/ Arguments:\n+\/\/   aligned - true => Input and output aligned on a HeapWord == 8-byte boundary\n+\/\/             ignored\n+\/\/   name    - stub name string\n+\/\/\n+\/\/ Inputs:\n+\/\/   c_rarg0   - source array address\n+\/\/   c_rarg1   - destination array address\n+\/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n+\/\/\n+\/\/ If 'from' and\/or 'to' are aligned on 4-, 2-, or 1-byte boundaries,\n+\/\/ we let the hardware handle it.  The one to eight bytes within words,\n+\/\/ dwords or qwords that span cache line boundaries will still be loaded\n+\/\/ and stored atomically.\n+\/\/\n+address StubGenerator::generate_conjoint_byte_copy(bool aligned, address nooverlap_target,\n+                                                   address* entry, const char *name) {\n@@ -1994,4 +1986,4 @@\n-    if (VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2() && MaxVectorSize  >= 32) {\n-       return generate_conjoint_copy_avx3_masked(entry, \"jbyte_conjoint_arraycopy_avx3\", 0,\n-                                                 nooverlap_target, aligned, false, false);\n-    }\n+  if (VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2() && MaxVectorSize  >= 32) {\n+     return generate_conjoint_copy_avx3_masked(entry, \"jbyte_conjoint_arraycopy_avx3\", 0,\n+                                               nooverlap_target, aligned, false, false);\n+  }\n@@ -1999,32 +1991,19 @@\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ pc();\n-\n-    Label L_copy_bytes, L_copy_8_bytes, L_copy_4_bytes, L_copy_2_bytes;\n-    const Register from        = rdi;  \/\/ source array address\n-    const Register to          = rsi;  \/\/ destination array address\n-    const Register count       = rdx;  \/\/ elements count\n-    const Register byte_count  = rcx;\n-    const Register qword_count = count;\n-\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    assert_clean_int(c_rarg2, rax);    \/\/ Make sure 'count' is clean int.\n-\n-    if (entry != NULL) {\n-      *entry = __ pc();\n-      \/\/ caller can pass a 64-bit byte count here (from Unsafe.copyMemory)\n-      BLOCK_COMMENT(\"Entry:\");\n-    }\n-\n-    array_overlap_test(nooverlap_target, Address::times_1);\n-    setup_arg_regs(); \/\/ from => rdi, to => rsi, count => rdx\n-                      \/\/ r9 and r10 may be used to save non-volatile registers\n-\n-    {\n-      \/\/ UnsafeCopyMemory page error: continue after ucm\n-      UnsafeCopyMemoryMark ucmm(this, !aligned, true);\n-      \/\/ 'from', 'to' and 'count' are now valid\n-      __ movptr(byte_count, count);\n-      __ shrptr(count, 3);   \/\/ count => qword_count\n-\n-      \/\/ Copy from high to low addresses.\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", name);\n+  address start = __ pc();\n+\n+  Label L_copy_bytes, L_copy_8_bytes, L_copy_4_bytes, L_copy_2_bytes;\n+  const Register from        = rdi;  \/\/ source array address\n+  const Register to          = rsi;  \/\/ destination array address\n+  const Register count       = rdx;  \/\/ elements count\n+  const Register byte_count  = rcx;\n+  const Register qword_count = count;\n+\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  assert_clean_int(c_rarg2, rax);    \/\/ Make sure 'count' is clean int.\n+\n+  if (entry != NULL) {\n+    *entry = __ pc();\n+    \/\/ caller can pass a 64-bit byte count here (from Unsafe.copyMemory)\n+    BLOCK_COMMENT(\"Entry:\");\n+  }\n@@ -2032,6 +2011,3 @@\n-      \/\/ Check for and copy trailing byte\n-      __ testl(byte_count, 1);\n-      __ jcc(Assembler::zero, L_copy_2_bytes);\n-      __ movb(rax, Address(from, byte_count, Address::times_1, -1));\n-      __ movb(Address(to, byte_count, Address::times_1, -1), rax);\n-      __ decrement(byte_count); \/\/ Adjust for possible trailing word\n+  array_overlap_test(nooverlap_target, Address::times_1);\n+  setup_arg_regs(); \/\/ from => rdi, to => rsi, count => rdx\n+                    \/\/ r9 and r10 may be used to save non-volatile registers\n@@ -2039,1 +2015,17 @@\n-      \/\/ Check for and copy trailing word\n+  {\n+    \/\/ UnsafeCopyMemory page error: continue after ucm\n+    UnsafeCopyMemoryMark ucmm(this, !aligned, true);\n+    \/\/ 'from', 'to' and 'count' are now valid\n+    __ movptr(byte_count, count);\n+    __ shrptr(count, 3);   \/\/ count => qword_count\n+\n+    \/\/ Copy from high to low addresses.\n+\n+    \/\/ Check for and copy trailing byte\n+    __ testl(byte_count, 1);\n+    __ jcc(Assembler::zero, L_copy_2_bytes);\n+    __ movb(rax, Address(from, byte_count, Address::times_1, -1));\n+    __ movb(Address(to, byte_count, Address::times_1, -1), rax);\n+    __ decrement(byte_count); \/\/ Adjust for possible trailing word\n+\n+    \/\/ Check for and copy trailing word\n@@ -2041,4 +2033,4 @@\n-      __ testl(byte_count, 2);\n-      __ jcc(Assembler::zero, L_copy_4_bytes);\n-      __ movw(rax, Address(from, byte_count, Address::times_1, -2));\n-      __ movw(Address(to, byte_count, Address::times_1, -2), rax);\n+    __ testl(byte_count, 2);\n+    __ jcc(Assembler::zero, L_copy_4_bytes);\n+    __ movw(rax, Address(from, byte_count, Address::times_1, -2));\n+    __ movw(Address(to, byte_count, Address::times_1, -2), rax);\n@@ -2046,1 +2038,1 @@\n-      \/\/ Check for and copy trailing dword\n+    \/\/ Check for and copy trailing dword\n@@ -2048,5 +2040,5 @@\n-      __ testl(byte_count, 4);\n-      __ jcc(Assembler::zero, L_copy_bytes);\n-      __ movl(rax, Address(from, qword_count, Address::times_8));\n-      __ movl(Address(to, qword_count, Address::times_8), rax);\n-      __ jmp(L_copy_bytes);\n+    __ testl(byte_count, 4);\n+    __ jcc(Assembler::zero, L_copy_bytes);\n+    __ movl(rax, Address(from, qword_count, Address::times_8));\n+    __ movl(Address(to, qword_count, Address::times_8), rax);\n+    __ jmp(L_copy_bytes);\n@@ -2054,1 +2046,1 @@\n-      \/\/ Copy trailing qwords\n+    \/\/ Copy trailing qwords\n@@ -2056,24 +2048,11 @@\n-      __ movq(rax, Address(from, qword_count, Address::times_8, -8));\n-      __ movq(Address(to, qword_count, Address::times_8, -8), rax);\n-      __ decrement(qword_count);\n-      __ jcc(Assembler::notZero, L_copy_8_bytes);\n-    }\n-    restore_arg_regs();\n-    INC_COUNTER_NP(SharedRuntime::_jbyte_array_copy_ctr, rscratch1); \/\/ Update counter after rscratch1 is free\n-    __ xorptr(rax, rax); \/\/ return 0\n-    __ vzeroupper();\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-\n-    {\n-      \/\/ UnsafeCopyMemory page error: continue after ucm\n-      UnsafeCopyMemoryMark ucmm(this, !aligned, true);\n-      \/\/ Copy in multi-bytes chunks\n-      copy_bytes_backward(from, to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n-    }\n-    restore_arg_regs();\n-    INC_COUNTER_NP(SharedRuntime::_jbyte_array_copy_ctr, rscratch1); \/\/ Update counter after rscratch1 is free\n-    __ xorptr(rax, rax); \/\/ return 0\n-    __ vzeroupper();\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n+    __ movq(rax, Address(from, qword_count, Address::times_8, -8));\n+    __ movq(Address(to, qword_count, Address::times_8, -8), rax);\n+    __ decrement(qword_count);\n+    __ jcc(Assembler::notZero, L_copy_8_bytes);\n+  }\n+  restore_arg_regs();\n+  INC_COUNTER_NP(SharedRuntime::_jbyte_array_copy_ctr, rscratch1); \/\/ Update counter after rscratch1 is free\n+  __ xorptr(rax, rax); \/\/ return 0\n+  __ vzeroupper();\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n@@ -2081,1 +2060,5 @@\n-    return start;\n+  {\n+    \/\/ UnsafeCopyMemory page error: continue after ucm\n+    UnsafeCopyMemoryMark ucmm(this, !aligned, true);\n+    \/\/ Copy in multi-bytes chunks\n+    copy_bytes_backward(from, to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n@@ -2083,0 +2066,6 @@\n+  restore_arg_regs();\n+  INC_COUNTER_NP(SharedRuntime::_jbyte_array_copy_ctr, rscratch1); \/\/ Update counter after rscratch1 is free\n+  __ xorptr(rax, rax); \/\/ return 0\n+  __ vzeroupper();\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n@@ -2084,20 +2073,23 @@\n-  \/\/ Arguments:\n-  \/\/   aligned - true => Input and output aligned on a HeapWord == 8-byte boundary\n-  \/\/             ignored\n-  \/\/   name    - stub name string\n-  \/\/\n-  \/\/ Inputs:\n-  \/\/   c_rarg0   - source array address\n-  \/\/   c_rarg1   - destination array address\n-  \/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n-  \/\/\n-  \/\/ If 'from' and\/or 'to' are aligned on 4- or 2-byte boundaries, we\n-  \/\/ let the hardware handle it.  The two or four words within dwords\n-  \/\/ or qwords that span cache line boundaries will still be loaded\n-  \/\/ and stored atomically.\n-  \/\/\n-  \/\/ Side Effects:\n-  \/\/   disjoint_short_copy_entry is set to the no-overlap entry point\n-  \/\/   used by generate_conjoint_short_copy().\n-  \/\/\n-  address generate_disjoint_short_copy(bool aligned, address *entry, const char *name) {\n+  return start;\n+}\n+\n+\/\/ Arguments:\n+\/\/   aligned - true => Input and output aligned on a HeapWord == 8-byte boundary\n+\/\/             ignored\n+\/\/   name    - stub name string\n+\/\/\n+\/\/ Inputs:\n+\/\/   c_rarg0   - source array address\n+\/\/   c_rarg1   - destination array address\n+\/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n+\/\/\n+\/\/ If 'from' and\/or 'to' are aligned on 4- or 2-byte boundaries, we\n+\/\/ let the hardware handle it.  The two or four words within dwords\n+\/\/ or qwords that span cache line boundaries will still be loaded\n+\/\/ and stored atomically.\n+\/\/\n+\/\/ Side Effects:\n+\/\/   disjoint_short_copy_entry is set to the no-overlap entry point\n+\/\/   used by generate_conjoint_short_copy().\n+\/\/\n+address StubGenerator::generate_disjoint_short_copy(bool aligned, address *entry, const char *name) {\n@@ -2105,4 +2097,4 @@\n-    if (VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2() && MaxVectorSize  >= 32) {\n-       return generate_disjoint_copy_avx3_masked(entry, \"jshort_disjoint_arraycopy_avx3\", 1,\n-                                                 aligned, false, false);\n-    }\n+  if (VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2() && MaxVectorSize  >= 32) {\n+     return generate_disjoint_copy_avx3_masked(entry, \"jshort_disjoint_arraycopy_avx3\", 1,\n+                                               aligned, false, false);\n+  }\n@@ -2111,33 +2103,23 @@\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ pc();\n-\n-    Label L_copy_bytes, L_copy_8_bytes, L_copy_4_bytes,L_copy_2_bytes,L_exit;\n-    const Register from        = rdi;  \/\/ source array address\n-    const Register to          = rsi;  \/\/ destination array address\n-    const Register count       = rdx;  \/\/ elements count\n-    const Register word_count  = rcx;\n-    const Register qword_count = count;\n-    const Register end_from    = from; \/\/ source array end address\n-    const Register end_to      = to;   \/\/ destination array end address\n-    \/\/ End pointers are inclusive, and if count is not zero they point\n-    \/\/ to the last unit copied:  end_to[0] := end_from[0]\n-\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    assert_clean_int(c_rarg2, rax);    \/\/ Make sure 'count' is clean int.\n-\n-    if (entry != NULL) {\n-      *entry = __ pc();\n-      \/\/ caller can pass a 64-bit byte count here (from Unsafe.copyMemory)\n-      BLOCK_COMMENT(\"Entry:\");\n-    }\n-\n-    setup_arg_regs(); \/\/ from => rdi, to => rsi, count => rdx\n-                      \/\/ r9 and r10 may be used to save non-volatile registers\n-\n-    {\n-      \/\/ UnsafeCopyMemory page error: continue after ucm\n-      UnsafeCopyMemoryMark ucmm(this, !aligned, true);\n-      \/\/ 'from', 'to' and 'count' are now valid\n-      __ movptr(word_count, count);\n-      __ shrptr(count, 2); \/\/ count => qword_count\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", name);\n+  address start = __ pc();\n+\n+  Label L_copy_bytes, L_copy_8_bytes, L_copy_4_bytes,L_copy_2_bytes,L_exit;\n+  const Register from        = rdi;  \/\/ source array address\n+  const Register to          = rsi;  \/\/ destination array address\n+  const Register count       = rdx;  \/\/ elements count\n+  const Register word_count  = rcx;\n+  const Register qword_count = count;\n+  const Register end_from    = from; \/\/ source array end address\n+  const Register end_to      = to;   \/\/ destination array end address\n+  \/\/ End pointers are inclusive, and if count is not zero they point\n+  \/\/ to the last unit copied:  end_to[0] := end_from[0]\n+\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  assert_clean_int(c_rarg2, rax);    \/\/ Make sure 'count' is clean int.\n+\n+  if (entry != NULL) {\n+    *entry = __ pc();\n+    \/\/ caller can pass a 64-bit byte count here (from Unsafe.copyMemory)\n+    BLOCK_COMMENT(\"Entry:\");\n+  }\n@@ -2145,5 +2127,2 @@\n-      \/\/ Copy from low to high addresses.  Use 'to' as scratch.\n-      __ lea(end_from, Address(from, qword_count, Address::times_8, -8));\n-      __ lea(end_to,   Address(to,   qword_count, Address::times_8, -8));\n-      __ negptr(qword_count);\n-      __ jmp(L_copy_bytes);\n+  setup_arg_regs(); \/\/ from => rdi, to => rsi, count => rdx\n+                    \/\/ r9 and r10 may be used to save non-volatile registers\n@@ -2151,1 +2130,14 @@\n-      \/\/ Copy trailing qwords\n+  {\n+    \/\/ UnsafeCopyMemory page error: continue after ucm\n+    UnsafeCopyMemoryMark ucmm(this, !aligned, true);\n+    \/\/ 'from', 'to' and 'count' are now valid\n+    __ movptr(word_count, count);\n+    __ shrptr(count, 2); \/\/ count => qword_count\n+\n+    \/\/ Copy from low to high addresses.  Use 'to' as scratch.\n+    __ lea(end_from, Address(from, qword_count, Address::times_8, -8));\n+    __ lea(end_to,   Address(to,   qword_count, Address::times_8, -8));\n+    __ negptr(qword_count);\n+    __ jmp(L_copy_bytes);\n+\n+    \/\/ Copy trailing qwords\n@@ -2153,4 +2145,4 @@\n-      __ movq(rax, Address(end_from, qword_count, Address::times_8, 8));\n-      __ movq(Address(end_to, qword_count, Address::times_8, 8), rax);\n-      __ increment(qword_count);\n-      __ jcc(Assembler::notZero, L_copy_8_bytes);\n+    __ movq(rax, Address(end_from, qword_count, Address::times_8, 8));\n+    __ movq(Address(end_to, qword_count, Address::times_8, 8), rax);\n+    __ increment(qword_count);\n+    __ jcc(Assembler::notZero, L_copy_8_bytes);\n@@ -2158,2 +2150,2 @@\n-      \/\/ Original 'dest' is trashed, so we can't use it as a\n-      \/\/ base register for a possible trailing word copy\n+    \/\/ Original 'dest' is trashed, so we can't use it as a\n+    \/\/ base register for a possible trailing word copy\n@@ -2161,1 +2153,1 @@\n-      \/\/ Check for and copy trailing dword\n+    \/\/ Check for and copy trailing dword\n@@ -2163,4 +2155,4 @@\n-      __ testl(word_count, 2);\n-      __ jccb(Assembler::zero, L_copy_2_bytes);\n-      __ movl(rax, Address(end_from, 8));\n-      __ movl(Address(end_to, 8), rax);\n+    __ testl(word_count, 2);\n+    __ jccb(Assembler::zero, L_copy_2_bytes);\n+    __ movl(rax, Address(end_from, 8));\n+    __ movl(Address(end_to, 8), rax);\n@@ -2168,2 +2160,2 @@\n-      __ addptr(end_from, 4);\n-      __ addptr(end_to, 4);\n+    __ addptr(end_from, 4);\n+    __ addptr(end_to, 4);\n@@ -2171,1 +2163,1 @@\n-      \/\/ Check for and copy trailing word\n+    \/\/ Check for and copy trailing word\n@@ -2173,5 +2165,5 @@\n-      __ testl(word_count, 1);\n-      __ jccb(Assembler::zero, L_exit);\n-      __ movw(rax, Address(end_from, 8));\n-      __ movw(Address(end_to, 8), rax);\n-    }\n+    __ testl(word_count, 1);\n+    __ jccb(Assembler::zero, L_exit);\n+    __ movw(rax, Address(end_from, 8));\n+    __ movw(Address(end_to, 8), rax);\n+  }\n@@ -2179,7 +2171,7 @@\n-    address ucme_exit_pc = __ pc();\n-    restore_arg_regs();\n-    INC_COUNTER_NP(SharedRuntime::_jshort_array_copy_ctr, rscratch1); \/\/ Update counter after rscratch1 is free\n-    __ xorptr(rax, rax); \/\/ return 0\n-    __ vzeroupper();\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n+  address ucme_exit_pc = __ pc();\n+  restore_arg_regs();\n+  INC_COUNTER_NP(SharedRuntime::_jshort_array_copy_ctr, rscratch1); \/\/ Update counter after rscratch1 is free\n+  __ xorptr(rax, rax); \/\/ return 0\n+  __ vzeroupper();\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n@@ -2187,8 +2179,5 @@\n-    {\n-      UnsafeCopyMemoryMark ucmm(this, !aligned, false, ucme_exit_pc);\n-      \/\/ Copy in multi-bytes chunks\n-      copy_bytes_forward(end_from, end_to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n-      __ jmp(L_copy_4_bytes);\n-    }\n-\n-    return start;\n+  {\n+    UnsafeCopyMemoryMark ucmm(this, !aligned, false, ucme_exit_pc);\n+    \/\/ Copy in multi-bytes chunks\n+    copy_bytes_forward(end_from, end_to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n+    __ jmp(L_copy_4_bytes);\n@@ -2197,4 +2186,2 @@\n-  address generate_fill(BasicType t, bool aligned, const char *name) {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ pc();\n+  return start;\n+}\n@@ -2202,1 +2189,4 @@\n-    BLOCK_COMMENT(\"Entry:\");\n+address StubGenerator::generate_fill(BasicType t, bool aligned, const char *name) {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", name);\n+  address start = __ pc();\n@@ -2204,4 +2194,1 @@\n-    const Register to       = c_rarg0;  \/\/ destination array address\n-    const Register value    = c_rarg1;  \/\/ value\n-    const Register count    = c_rarg2;  \/\/ elements count\n-    __ mov(r11, count);\n+  BLOCK_COMMENT(\"Entry:\");\n@@ -2209,1 +2196,4 @@\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  const Register to       = c_rarg0;  \/\/ destination array address\n+  const Register value    = c_rarg1;  \/\/ value\n+  const Register count    = c_rarg2;  \/\/ elements count\n+  __ mov(r11, count);\n@@ -2211,1 +2201,1 @@\n-    __ generate_fill(t, aligned, to, value, r11, rax, xmm0);\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n@@ -2213,5 +2203,1 @@\n-    __ vzeroupper();\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-    return start;\n-  }\n+  __ generate_fill(t, aligned, to, value, r11, rax, xmm0);\n@@ -2219,17 +2205,24 @@\n-  \/\/ Arguments:\n-  \/\/   aligned - true => Input and output aligned on a HeapWord == 8-byte boundary\n-  \/\/             ignored\n-  \/\/   name    - stub name string\n-  \/\/\n-  \/\/ Inputs:\n-  \/\/   c_rarg0   - source array address\n-  \/\/   c_rarg1   - destination array address\n-  \/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n-  \/\/\n-  \/\/ If 'from' and\/or 'to' are aligned on 4- or 2-byte boundaries, we\n-  \/\/ let the hardware handle it.  The two or four words within dwords\n-  \/\/ or qwords that span cache line boundaries will still be loaded\n-  \/\/ and stored atomically.\n-  \/\/\n-  address generate_conjoint_short_copy(bool aligned, address nooverlap_target,\n-                                       address *entry, const char *name) {\n+  __ vzeroupper();\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n+\n+  return start;\n+}\n+\n+\/\/ Arguments:\n+\/\/   aligned - true => Input and output aligned on a HeapWord == 8-byte boundary\n+\/\/             ignored\n+\/\/   name    - stub name string\n+\/\/\n+\/\/ Inputs:\n+\/\/   c_rarg0   - source array address\n+\/\/   c_rarg1   - destination array address\n+\/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n+\/\/\n+\/\/ If 'from' and\/or 'to' are aligned on 4- or 2-byte boundaries, we\n+\/\/ let the hardware handle it.  The two or four words within dwords\n+\/\/ or qwords that span cache line boundaries will still be loaded\n+\/\/ and stored atomically.\n+\/\/\n+address StubGenerator::generate_conjoint_short_copy(bool aligned, address nooverlap_target,\n+                                                    address *entry, const char *name) {\n@@ -2237,4 +2230,4 @@\n-    if (VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2() && MaxVectorSize  >= 32) {\n-       return generate_conjoint_copy_avx3_masked(entry, \"jshort_conjoint_arraycopy_avx3\", 1,\n-                                                 nooverlap_target, aligned, false, false);\n-    }\n+  if (VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2() && MaxVectorSize  >= 32) {\n+     return generate_conjoint_copy_avx3_masked(entry, \"jshort_conjoint_arraycopy_avx3\", 1,\n+                                               nooverlap_target, aligned, false, false);\n+  }\n@@ -2242,19 +2235,19 @@\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ pc();\n-\n-    Label L_copy_bytes, L_copy_8_bytes, L_copy_4_bytes;\n-    const Register from        = rdi;  \/\/ source array address\n-    const Register to          = rsi;  \/\/ destination array address\n-    const Register count       = rdx;  \/\/ elements count\n-    const Register word_count  = rcx;\n-    const Register qword_count = count;\n-\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    assert_clean_int(c_rarg2, rax);    \/\/ Make sure 'count' is clean int.\n-\n-    if (entry != NULL) {\n-      *entry = __ pc();\n-      \/\/ caller can pass a 64-bit byte count here (from Unsafe.copyMemory)\n-      BLOCK_COMMENT(\"Entry:\");\n-    }\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", name);\n+  address start = __ pc();\n+\n+  Label L_copy_bytes, L_copy_8_bytes, L_copy_4_bytes;\n+  const Register from        = rdi;  \/\/ source array address\n+  const Register to          = rsi;  \/\/ destination array address\n+  const Register count       = rdx;  \/\/ elements count\n+  const Register word_count  = rcx;\n+  const Register qword_count = count;\n+\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  assert_clean_int(c_rarg2, rax);    \/\/ Make sure 'count' is clean int.\n+\n+  if (entry != NULL) {\n+    *entry = __ pc();\n+    \/\/ caller can pass a 64-bit byte count here (from Unsafe.copyMemory)\n+    BLOCK_COMMENT(\"Entry:\");\n+  }\n@@ -2262,3 +2255,3 @@\n-    array_overlap_test(nooverlap_target, Address::times_2);\n-    setup_arg_regs(); \/\/ from => rdi, to => rsi, count => rdx\n-                      \/\/ r9 and r10 may be used to save non-volatile registers\n+  array_overlap_test(nooverlap_target, Address::times_2);\n+  setup_arg_regs(); \/\/ from => rdi, to => rsi, count => rdx\n+                    \/\/ r9 and r10 may be used to save non-volatile registers\n@@ -2266,6 +2259,6 @@\n-    {\n-      \/\/ UnsafeCopyMemory page error: continue after ucm\n-      UnsafeCopyMemoryMark ucmm(this, !aligned, true);\n-      \/\/ 'from', 'to' and 'count' are now valid\n-      __ movptr(word_count, count);\n-      __ shrptr(count, 2); \/\/ count => qword_count\n+  {\n+    \/\/ UnsafeCopyMemory page error: continue after ucm\n+    UnsafeCopyMemoryMark ucmm(this, !aligned, true);\n+    \/\/ 'from', 'to' and 'count' are now valid\n+    __ movptr(word_count, count);\n+    __ shrptr(count, 2); \/\/ count => qword_count\n@@ -2273,1 +2266,1 @@\n-      \/\/ Copy from high to low addresses.  Use 'to' as scratch.\n+    \/\/ Copy from high to low addresses.  Use 'to' as scratch.\n@@ -2275,5 +2268,5 @@\n-      \/\/ Check for and copy trailing word\n-      __ testl(word_count, 1);\n-      __ jccb(Assembler::zero, L_copy_4_bytes);\n-      __ movw(rax, Address(from, word_count, Address::times_2, -2));\n-      __ movw(Address(to, word_count, Address::times_2, -2), rax);\n+    \/\/ Check for and copy trailing word\n+    __ testl(word_count, 1);\n+    __ jccb(Assembler::zero, L_copy_4_bytes);\n+    __ movw(rax, Address(from, word_count, Address::times_2, -2));\n+    __ movw(Address(to, word_count, Address::times_2, -2), rax);\n@@ -2281,1 +2274,1 @@\n-     \/\/ Check for and copy trailing dword\n+   \/\/ Check for and copy trailing dword\n@@ -2283,5 +2276,5 @@\n-      __ testl(word_count, 2);\n-      __ jcc(Assembler::zero, L_copy_bytes);\n-      __ movl(rax, Address(from, qword_count, Address::times_8));\n-      __ movl(Address(to, qword_count, Address::times_8), rax);\n-      __ jmp(L_copy_bytes);\n+    __ testl(word_count, 2);\n+    __ jcc(Assembler::zero, L_copy_bytes);\n+    __ movl(rax, Address(from, qword_count, Address::times_8));\n+    __ movl(Address(to, qword_count, Address::times_8), rax);\n+    __ jmp(L_copy_bytes);\n@@ -2289,1 +2282,1 @@\n-      \/\/ Copy trailing qwords\n+    \/\/ Copy trailing qwords\n@@ -2291,24 +2284,11 @@\n-      __ movq(rax, Address(from, qword_count, Address::times_8, -8));\n-      __ movq(Address(to, qword_count, Address::times_8, -8), rax);\n-      __ decrement(qword_count);\n-      __ jcc(Assembler::notZero, L_copy_8_bytes);\n-    }\n-    restore_arg_regs();\n-    INC_COUNTER_NP(SharedRuntime::_jshort_array_copy_ctr, rscratch1); \/\/ Update counter after rscratch1 is free\n-    __ xorptr(rax, rax); \/\/ return 0\n-    __ vzeroupper();\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-\n-    {\n-      \/\/ UnsafeCopyMemory page error: continue after ucm\n-      UnsafeCopyMemoryMark ucmm(this, !aligned, true);\n-      \/\/ Copy in multi-bytes chunks\n-      copy_bytes_backward(from, to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n-    }\n-    restore_arg_regs();\n-    INC_COUNTER_NP(SharedRuntime::_jshort_array_copy_ctr, rscratch1); \/\/ Update counter after rscratch1 is free\n-    __ xorptr(rax, rax); \/\/ return 0\n-    __ vzeroupper();\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n+    __ movq(rax, Address(from, qword_count, Address::times_8, -8));\n+    __ movq(Address(to, qword_count, Address::times_8, -8), rax);\n+    __ decrement(qword_count);\n+    __ jcc(Assembler::notZero, L_copy_8_bytes);\n+  }\n+  restore_arg_regs();\n+  INC_COUNTER_NP(SharedRuntime::_jshort_array_copy_ctr, rscratch1); \/\/ Update counter after rscratch1 is free\n+  __ xorptr(rax, rax); \/\/ return 0\n+  __ vzeroupper();\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n@@ -2316,1 +2296,5 @@\n-    return start;\n+  {\n+    \/\/ UnsafeCopyMemory page error: continue after ucm\n+    UnsafeCopyMemoryMark ucmm(this, !aligned, true);\n+    \/\/ Copy in multi-bytes chunks\n+    copy_bytes_backward(from, to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n@@ -2318,0 +2302,6 @@\n+  restore_arg_regs();\n+  INC_COUNTER_NP(SharedRuntime::_jshort_array_copy_ctr, rscratch1); \/\/ Update counter after rscratch1 is free\n+  __ xorptr(rax, rax); \/\/ return 0\n+  __ vzeroupper();\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n@@ -2319,21 +2309,24 @@\n-  \/\/ Arguments:\n-  \/\/   aligned - true => Input and output aligned on a HeapWord == 8-byte boundary\n-  \/\/             ignored\n-  \/\/   is_oop  - true => oop array, so generate store check code\n-  \/\/   name    - stub name string\n-  \/\/\n-  \/\/ Inputs:\n-  \/\/   c_rarg0   - source array address\n-  \/\/   c_rarg1   - destination array address\n-  \/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n-  \/\/\n-  \/\/ If 'from' and\/or 'to' are aligned on 4-byte boundaries, we let\n-  \/\/ the hardware handle it.  The two dwords within qwords that span\n-  \/\/ cache line boundaries will still be loaded and stored atomically.\n-  \/\/\n-  \/\/ Side Effects:\n-  \/\/   disjoint_int_copy_entry is set to the no-overlap entry point\n-  \/\/   used by generate_conjoint_int_oop_copy().\n-  \/\/\n-  address generate_disjoint_int_oop_copy(bool aligned, bool is_oop, address* entry,\n-                                         const char *name, bool dest_uninitialized = false) {\n+  return start;\n+}\n+\n+\/\/ Arguments:\n+\/\/   aligned - true => Input and output aligned on a HeapWord == 8-byte boundary\n+\/\/             ignored\n+\/\/   is_oop  - true => oop array, so generate store check code\n+\/\/   name    - stub name string\n+\/\/\n+\/\/ Inputs:\n+\/\/   c_rarg0   - source array address\n+\/\/   c_rarg1   - destination array address\n+\/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n+\/\/\n+\/\/ If 'from' and\/or 'to' are aligned on 4-byte boundaries, we let\n+\/\/ the hardware handle it.  The two dwords within qwords that span\n+\/\/ cache line boundaries will still be loaded and stored atomically.\n+\/\/\n+\/\/ Side Effects:\n+\/\/   disjoint_int_copy_entry is set to the no-overlap entry point\n+\/\/   used by generate_conjoint_int_oop_copy().\n+\/\/\n+address StubGenerator::generate_disjoint_int_oop_copy(bool aligned, bool is_oop, address* entry,\n+                                                      const char *name, bool dest_uninitialized) {\n@@ -2341,4 +2334,4 @@\n-    if (VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2() && MaxVectorSize  >= 32) {\n-       return generate_disjoint_copy_avx3_masked(entry, \"jint_disjoint_arraycopy_avx3\", 2,\n-                                                 aligned, is_oop, dest_uninitialized);\n-    }\n+  if (VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2() && MaxVectorSize  >= 32) {\n+     return generate_disjoint_copy_avx3_masked(entry, \"jint_disjoint_arraycopy_avx3\", 2,\n+                                               aligned, is_oop, dest_uninitialized);\n+  }\n@@ -2347,34 +2340,23 @@\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ pc();\n-\n-    Label L_copy_bytes, L_copy_8_bytes, L_copy_4_bytes, L_exit;\n-    const Register from        = rdi;  \/\/ source array address\n-    const Register to          = rsi;  \/\/ destination array address\n-    const Register count       = rdx;  \/\/ elements count\n-    const Register dword_count = rcx;\n-    const Register qword_count = count;\n-    const Register end_from    = from; \/\/ source array end address\n-    const Register end_to      = to;   \/\/ destination array end address\n-    \/\/ End pointers are inclusive, and if count is not zero they point\n-    \/\/ to the last unit copied:  end_to[0] := end_from[0]\n-\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    assert_clean_int(c_rarg2, rax);    \/\/ Make sure 'count' is clean int.\n-\n-    if (entry != NULL) {\n-      *entry = __ pc();\n-      \/\/ caller can pass a 64-bit byte count here (from Unsafe.copyMemory)\n-      BLOCK_COMMENT(\"Entry:\");\n-    }\n-\n-    setup_arg_regs_using_thread(); \/\/ from => rdi, to => rsi, count => rdx\n-                                   \/\/ r9 is used to save r15_thread\n-\n-    DecoratorSet decorators = IN_HEAP | IS_ARRAY | ARRAYCOPY_DISJOINT;\n-    if (dest_uninitialized) {\n-      decorators |= IS_DEST_UNINITIALIZED;\n-    }\n-    if (aligned) {\n-      decorators |= ARRAYCOPY_ALIGNED;\n-    }\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", name);\n+  address start = __ pc();\n+\n+  Label L_copy_bytes, L_copy_8_bytes, L_copy_4_bytes, L_exit;\n+  const Register from        = rdi;  \/\/ source array address\n+  const Register to          = rsi;  \/\/ destination array address\n+  const Register count       = rdx;  \/\/ elements count\n+  const Register dword_count = rcx;\n+  const Register qword_count = count;\n+  const Register end_from    = from; \/\/ source array end address\n+  const Register end_to      = to;   \/\/ destination array end address\n+  \/\/ End pointers are inclusive, and if count is not zero they point\n+  \/\/ to the last unit copied:  end_to[0] := end_from[0]\n+\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  assert_clean_int(c_rarg2, rax);    \/\/ Make sure 'count' is clean int.\n+\n+  if (entry != NULL) {\n+    *entry = __ pc();\n+    \/\/ caller can pass a 64-bit byte count here (from Unsafe.copyMemory)\n+    BLOCK_COMMENT(\"Entry:\");\n+  }\n@@ -2382,3 +2364,2 @@\n-    BasicType type = is_oop ? T_OBJECT : T_INT;\n-    BarrierSetAssembler *bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-    bs->arraycopy_prologue(_masm, decorators, type, from, to, count);\n+  setup_arg_regs_using_thread(); \/\/ from => rdi, to => rsi, count => rdx\n+                                 \/\/ r9 is used to save r15_thread\n@@ -2386,6 +2367,7 @@\n-    {\n-      \/\/ UnsafeCopyMemory page error: continue after ucm\n-      UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n-      \/\/ 'from', 'to' and 'count' are now valid\n-      __ movptr(dword_count, count);\n-      __ shrptr(count, 1); \/\/ count => qword_count\n+  DecoratorSet decorators = IN_HEAP | IS_ARRAY | ARRAYCOPY_DISJOINT;\n+  if (dest_uninitialized) {\n+    decorators |= IS_DEST_UNINITIALIZED;\n+  }\n+  if (aligned) {\n+    decorators |= ARRAYCOPY_ALIGNED;\n+  }\n@@ -2393,5 +2375,3 @@\n-      \/\/ Copy from low to high addresses.  Use 'to' as scratch.\n-      __ lea(end_from, Address(from, qword_count, Address::times_8, -8));\n-      __ lea(end_to,   Address(to,   qword_count, Address::times_8, -8));\n-      __ negptr(qword_count);\n-      __ jmp(L_copy_bytes);\n+  BasicType type = is_oop ? T_OBJECT : T_INT;\n+  BarrierSetAssembler *bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  bs->arraycopy_prologue(_masm, decorators, type, from, to, count);\n@@ -2399,1 +2379,14 @@\n-      \/\/ Copy trailing qwords\n+  {\n+    \/\/ UnsafeCopyMemory page error: continue after ucm\n+    UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n+    \/\/ 'from', 'to' and 'count' are now valid\n+    __ movptr(dword_count, count);\n+    __ shrptr(count, 1); \/\/ count => qword_count\n+\n+    \/\/ Copy from low to high addresses.  Use 'to' as scratch.\n+    __ lea(end_from, Address(from, qword_count, Address::times_8, -8));\n+    __ lea(end_to,   Address(to,   qword_count, Address::times_8, -8));\n+    __ negptr(qword_count);\n+    __ jmp(L_copy_bytes);\n+\n+    \/\/ Copy trailing qwords\n@@ -2401,4 +2394,4 @@\n-      __ movq(rax, Address(end_from, qword_count, Address::times_8, 8));\n-      __ movq(Address(end_to, qword_count, Address::times_8, 8), rax);\n-      __ increment(qword_count);\n-      __ jcc(Assembler::notZero, L_copy_8_bytes);\n+    __ movq(rax, Address(end_from, qword_count, Address::times_8, 8));\n+    __ movq(Address(end_to, qword_count, Address::times_8, 8), rax);\n+    __ increment(qword_count);\n+    __ jcc(Assembler::notZero, L_copy_8_bytes);\n@@ -2406,1 +2399,1 @@\n-      \/\/ Check for and copy trailing dword\n+    \/\/ Check for and copy trailing dword\n@@ -2408,5 +2401,5 @@\n-      __ testl(dword_count, 1); \/\/ Only byte test since the value is 0 or 1\n-      __ jccb(Assembler::zero, L_exit);\n-      __ movl(rax, Address(end_from, 8));\n-      __ movl(Address(end_to, 8), rax);\n-    }\n+    __ testl(dword_count, 1); \/\/ Only byte test since the value is 0 or 1\n+    __ jccb(Assembler::zero, L_exit);\n+    __ movl(rax, Address(end_from, 8));\n+    __ movl(Address(end_to, 8), rax);\n+  }\n@@ -2414,8 +2407,8 @@\n-    address ucme_exit_pc = __ pc();\n-    bs->arraycopy_epilogue(_masm, decorators, type, from, to, dword_count);\n-    restore_arg_regs_using_thread();\n-    INC_COUNTER_NP(SharedRuntime::_jint_array_copy_ctr, rscratch1); \/\/ Update counter after rscratch1 is free\n-    __ vzeroupper();\n-    __ xorptr(rax, rax); \/\/ return 0\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n+  address ucme_exit_pc = __ pc();\n+  bs->arraycopy_epilogue(_masm, decorators, type, from, to, dword_count);\n+  restore_arg_regs_using_thread();\n+  INC_COUNTER_NP(SharedRuntime::_jint_array_copy_ctr, rscratch1); \/\/ Update counter after rscratch1 is free\n+  __ vzeroupper();\n+  __ xorptr(rax, rax); \/\/ return 0\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n@@ -2423,6 +2416,9 @@\n-    {\n-      UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, false, ucme_exit_pc);\n-      \/\/ Copy in multi-bytes chunks\n-      copy_bytes_forward(end_from, end_to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n-      __ jmp(L_copy_4_bytes);\n-    }\n+  {\n+    UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, false, ucme_exit_pc);\n+    \/\/ Copy in multi-bytes chunks\n+    copy_bytes_forward(end_from, end_to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n+    __ jmp(L_copy_4_bytes);\n+  }\n+\n+  return start;\n+}\n@@ -2430,1 +2426,42 @@\n-    return start;\n+\/\/ Arguments:\n+\/\/   aligned - true => Input and output aligned on a HeapWord == 8-byte boundary\n+\/\/             ignored\n+\/\/   is_oop  - true => oop array, so generate store check code\n+\/\/   name    - stub name string\n+\/\/\n+\/\/ Inputs:\n+\/\/   c_rarg0   - source array address\n+\/\/   c_rarg1   - destination array address\n+\/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n+\/\/\n+\/\/ If 'from' and\/or 'to' are aligned on 4-byte boundaries, we let\n+\/\/ the hardware handle it.  The two dwords within qwords that span\n+\/\/ cache line boundaries will still be loaded and stored atomically.\n+\/\/\n+address StubGenerator::generate_conjoint_int_oop_copy(bool aligned, bool is_oop, address nooverlap_target,\n+                                                      address *entry, const char *name,\n+                                                      bool dest_uninitialized) {\n+#if COMPILER2_OR_JVMCI\n+  if (VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2() && MaxVectorSize  >= 32) {\n+     return generate_conjoint_copy_avx3_masked(entry, \"jint_conjoint_arraycopy_avx3\", 2,\n+                                               nooverlap_target, aligned, is_oop, dest_uninitialized);\n+  }\n+#endif\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", name);\n+  address start = __ pc();\n+\n+  Label L_copy_bytes, L_copy_8_bytes, L_exit;\n+  const Register from        = rdi;  \/\/ source array address\n+  const Register to          = rsi;  \/\/ destination array address\n+  const Register count       = rdx;  \/\/ elements count\n+  const Register dword_count = rcx;\n+  const Register qword_count = count;\n+\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  assert_clean_int(c_rarg2, rax);    \/\/ Make sure 'count' is clean int.\n+\n+  if (entry != NULL) {\n+    *entry = __ pc();\n+     \/\/ caller can pass a 64-bit byte count here (from Unsafe.copyMemory)\n+    BLOCK_COMMENT(\"Entry:\");\n@@ -2433,18 +2470,87 @@\n-  \/\/ Arguments:\n-  \/\/   aligned - true => Input and output aligned on a HeapWord == 8-byte boundary\n-  \/\/             ignored\n-  \/\/   is_oop  - true => oop array, so generate store check code\n-  \/\/   name    - stub name string\n-  \/\/\n-  \/\/ Inputs:\n-  \/\/   c_rarg0   - source array address\n-  \/\/   c_rarg1   - destination array address\n-  \/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n-  \/\/\n-  \/\/ If 'from' and\/or 'to' are aligned on 4-byte boundaries, we let\n-  \/\/ the hardware handle it.  The two dwords within qwords that span\n-  \/\/ cache line boundaries will still be loaded and stored atomically.\n-  \/\/\n-  address generate_conjoint_int_oop_copy(bool aligned, bool is_oop, address nooverlap_target,\n-                                         address *entry, const char *name,\n-                                         bool dest_uninitialized = false) {\n+  array_overlap_test(nooverlap_target, Address::times_4);\n+  setup_arg_regs_using_thread(); \/\/ from => rdi, to => rsi, count => rdx\n+                                 \/\/ r9 is used to save r15_thread\n+\n+  DecoratorSet decorators = IN_HEAP | IS_ARRAY;\n+  if (dest_uninitialized) {\n+    decorators |= IS_DEST_UNINITIALIZED;\n+  }\n+  if (aligned) {\n+    decorators |= ARRAYCOPY_ALIGNED;\n+  }\n+\n+  BasicType type = is_oop ? T_OBJECT : T_INT;\n+  BarrierSetAssembler *bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  \/\/ no registers are destroyed by this call\n+  bs->arraycopy_prologue(_masm, decorators, type, from, to, count);\n+\n+  assert_clean_int(count, rax); \/\/ Make sure 'count' is clean int.\n+  {\n+    \/\/ UnsafeCopyMemory page error: continue after ucm\n+    UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n+    \/\/ 'from', 'to' and 'count' are now valid\n+    __ movptr(dword_count, count);\n+    __ shrptr(count, 1); \/\/ count => qword_count\n+\n+    \/\/ Copy from high to low addresses.  Use 'to' as scratch.\n+\n+    \/\/ Check for and copy trailing dword\n+    __ testl(dword_count, 1);\n+    __ jcc(Assembler::zero, L_copy_bytes);\n+    __ movl(rax, Address(from, dword_count, Address::times_4, -4));\n+    __ movl(Address(to, dword_count, Address::times_4, -4), rax);\n+    __ jmp(L_copy_bytes);\n+\n+    \/\/ Copy trailing qwords\n+    __ BIND(L_copy_8_bytes);\n+    __ movq(rax, Address(from, qword_count, Address::times_8, -8));\n+    __ movq(Address(to, qword_count, Address::times_8, -8), rax);\n+    __ decrement(qword_count);\n+    __ jcc(Assembler::notZero, L_copy_8_bytes);\n+  }\n+  if (is_oop) {\n+    __ jmp(L_exit);\n+  }\n+  restore_arg_regs_using_thread();\n+  INC_COUNTER_NP(SharedRuntime::_jint_array_copy_ctr, rscratch1); \/\/ Update counter after rscratch1 is free\n+  __ xorptr(rax, rax); \/\/ return 0\n+  __ vzeroupper();\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n+\n+  {\n+    \/\/ UnsafeCopyMemory page error: continue after ucm\n+    UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n+    \/\/ Copy in multi-bytes chunks\n+    copy_bytes_backward(from, to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n+  }\n+\n+  __ BIND(L_exit);\n+  bs->arraycopy_epilogue(_masm, decorators, type, from, to, dword_count);\n+  restore_arg_regs_using_thread();\n+  INC_COUNTER_NP(SharedRuntime::_jint_array_copy_ctr, rscratch1); \/\/ Update counter after rscratch1 is free\n+  __ xorptr(rax, rax); \/\/ return 0\n+  __ vzeroupper();\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n+\n+  return start;\n+}\n+\n+\/\/ Arguments:\n+\/\/   aligned - true => Input and output aligned on a HeapWord boundary == 8 bytes\n+\/\/             ignored\n+\/\/   is_oop  - true => oop array, so generate store check code\n+\/\/   name    - stub name string\n+\/\/\n+\/\/ Inputs:\n+\/\/   c_rarg0   - source array address\n+\/\/   c_rarg1   - destination array address\n+\/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n+\/\/\n+ \/\/ Side Effects:\n+\/\/   disjoint_oop_copy_entry or disjoint_long_copy_entry is set to the\n+\/\/   no-overlap entry point used by generate_conjoint_long_oop_copy().\n+\/\/\n+address StubGenerator::generate_disjoint_long_oop_copy(bool aligned, bool is_oop, address *entry,\n+                                                       const char *name, bool dest_uninitialized) {\n@@ -2452,4 +2558,4 @@\n-    if (VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2() && MaxVectorSize  >= 32) {\n-       return generate_conjoint_copy_avx3_masked(entry, \"jint_conjoint_arraycopy_avx3\", 2,\n-                                                 nooverlap_target, aligned, is_oop, dest_uninitialized);\n-    }\n+  if (VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2() && MaxVectorSize  >= 32) {\n+     return generate_disjoint_copy_avx3_masked(entry, \"jlong_disjoint_arraycopy_avx3\", 3,\n+                                               aligned, is_oop, dest_uninitialized);\n+  }\n@@ -2457,19 +2563,23 @@\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ pc();\n-\n-    Label L_copy_bytes, L_copy_8_bytes, L_exit;\n-    const Register from        = rdi;  \/\/ source array address\n-    const Register to          = rsi;  \/\/ destination array address\n-    const Register count       = rdx;  \/\/ elements count\n-    const Register dword_count = rcx;\n-    const Register qword_count = count;\n-\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    assert_clean_int(c_rarg2, rax);    \/\/ Make sure 'count' is clean int.\n-\n-    if (entry != NULL) {\n-      *entry = __ pc();\n-       \/\/ caller can pass a 64-bit byte count here (from Unsafe.copyMemory)\n-      BLOCK_COMMENT(\"Entry:\");\n-    }\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", name);\n+  address start = __ pc();\n+\n+  Label L_copy_bytes, L_copy_8_bytes, L_exit;\n+  const Register from        = rdi;  \/\/ source array address\n+  const Register to          = rsi;  \/\/ destination array address\n+  const Register qword_count = rdx;  \/\/ elements count\n+  const Register end_from    = from; \/\/ source array end address\n+  const Register end_to      = rcx;  \/\/ destination array end address\n+  const Register saved_count = r11;\n+  \/\/ End pointers are inclusive, and if count is not zero they point\n+  \/\/ to the last unit copied:  end_to[0] := end_from[0]\n+\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  \/\/ Save no-overlap entry point for generate_conjoint_long_oop_copy()\n+  assert_clean_int(c_rarg2, rax);    \/\/ Make sure 'count' is clean int.\n+\n+  if (entry != NULL) {\n+    *entry = __ pc();\n+    \/\/ caller can pass a 64-bit byte count here (from Unsafe.copyMemory)\n+    BLOCK_COMMENT(\"Entry:\");\n+  }\n@@ -2477,2 +2587,1 @@\n-    array_overlap_test(nooverlap_target, Address::times_4);\n-    setup_arg_regs_using_thread(); \/\/ from => rdi, to => rsi, count => rdx\n+  setup_arg_regs_using_thread(); \/\/ from => rdi, to => rsi, count => rdx\n@@ -2480,0 +2589,1 @@\n+  \/\/ 'from', 'to' and 'qword_count' are now valid\n@@ -2481,7 +2591,7 @@\n-    DecoratorSet decorators = IN_HEAP | IS_ARRAY;\n-    if (dest_uninitialized) {\n-      decorators |= IS_DEST_UNINITIALIZED;\n-    }\n-    if (aligned) {\n-      decorators |= ARRAYCOPY_ALIGNED;\n-    }\n+  DecoratorSet decorators = IN_HEAP | IS_ARRAY | ARRAYCOPY_DISJOINT;\n+  if (dest_uninitialized) {\n+    decorators |= IS_DEST_UNINITIALIZED;\n+  }\n+  if (aligned) {\n+    decorators |= ARRAYCOPY_ALIGNED;\n+  }\n@@ -2489,23 +2599,14 @@\n-    BasicType type = is_oop ? T_OBJECT : T_INT;\n-    BarrierSetAssembler *bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-    \/\/ no registers are destroyed by this call\n-    bs->arraycopy_prologue(_masm, decorators, type, from, to, count);\n-\n-    assert_clean_int(count, rax); \/\/ Make sure 'count' is clean int.\n-    {\n-      \/\/ UnsafeCopyMemory page error: continue after ucm\n-      UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n-      \/\/ 'from', 'to' and 'count' are now valid\n-      __ movptr(dword_count, count);\n-      __ shrptr(count, 1); \/\/ count => qword_count\n-\n-      \/\/ Copy from high to low addresses.  Use 'to' as scratch.\n-\n-      \/\/ Check for and copy trailing dword\n-      __ testl(dword_count, 1);\n-      __ jcc(Assembler::zero, L_copy_bytes);\n-      __ movl(rax, Address(from, dword_count, Address::times_4, -4));\n-      __ movl(Address(to, dword_count, Address::times_4, -4), rax);\n-      __ jmp(L_copy_bytes);\n-\n-      \/\/ Copy trailing qwords\n+  BasicType type = is_oop ? T_OBJECT : T_LONG;\n+  BarrierSetAssembler *bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  bs->arraycopy_prologue(_masm, decorators, type, from, to, qword_count);\n+  {\n+    \/\/ UnsafeCopyMemory page error: continue after ucm\n+    UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n+\n+    \/\/ Copy from low to high addresses.  Use 'to' as scratch.\n+    __ lea(end_from, Address(from, qword_count, Address::times_8, -8));\n+    __ lea(end_to,   Address(to,   qword_count, Address::times_8, -8));\n+    __ negptr(qword_count);\n+    __ jmp(L_copy_bytes);\n+\n+    \/\/ Copy trailing qwords\n@@ -2513,8 +2614,8 @@\n-      __ movq(rax, Address(from, qword_count, Address::times_8, -8));\n-      __ movq(Address(to, qword_count, Address::times_8, -8), rax);\n-      __ decrement(qword_count);\n-      __ jcc(Assembler::notZero, L_copy_8_bytes);\n-    }\n-    if (is_oop) {\n-      __ jmp(L_exit);\n-    }\n+    __ movq(rax, Address(end_from, qword_count, Address::times_8, 8));\n+    __ movq(Address(end_to, qword_count, Address::times_8, 8), rax);\n+    __ increment(qword_count);\n+    __ jcc(Assembler::notZero, L_copy_8_bytes);\n+  }\n+  if (is_oop) {\n+    __ jmp(L_exit);\n+  } else {\n@@ -2522,1 +2623,1 @@\n-    INC_COUNTER_NP(SharedRuntime::_jint_array_copy_ctr, rscratch1); \/\/ Update counter after rscratch1 is free\n+    INC_COUNTER_NP(SharedRuntime::_jlong_array_copy_ctr, rscratch1); \/\/ Update counter after rscratch1 is free\n@@ -2527,0 +2628,1 @@\n+  }\n@@ -2528,6 +2630,6 @@\n-    {\n-      \/\/ UnsafeCopyMemory page error: continue after ucm\n-      UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n-      \/\/ Copy in multi-bytes chunks\n-      copy_bytes_backward(from, to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n-    }\n+  {\n+    \/\/ UnsafeCopyMemory page error: continue after ucm\n+    UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n+    \/\/ Copy in multi-bytes chunks\n+    copy_bytes_forward(end_from, end_to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n+  }\n@@ -2536,7 +2638,9 @@\n-    bs->arraycopy_epilogue(_masm, decorators, type, from, to, dword_count);\n-    restore_arg_regs_using_thread();\n-    INC_COUNTER_NP(SharedRuntime::_jint_array_copy_ctr, rscratch1); \/\/ Update counter after rscratch1 is free\n-    __ xorptr(rax, rax); \/\/ return 0\n-    __ vzeroupper();\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n+  bs->arraycopy_epilogue(_masm, decorators, type, from, to, qword_count);\n+  restore_arg_regs_using_thread();\n+  INC_COUNTER_NP(is_oop ? SharedRuntime::_oop_array_copy_ctr :\n+                          SharedRuntime::_jlong_array_copy_ctr,\n+                 rscratch1); \/\/ Update counter after rscratch1 is free\n+  __ vzeroupper();\n+  __ xorptr(rax, rax); \/\/ return 0\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n@@ -2544,2 +2648,2 @@\n-    return start;\n-  }\n+  return start;\n+}\n@@ -2547,17 +2651,14 @@\n-  \/\/ Arguments:\n-  \/\/   aligned - true => Input and output aligned on a HeapWord boundary == 8 bytes\n-  \/\/             ignored\n-  \/\/   is_oop  - true => oop array, so generate store check code\n-  \/\/   name    - stub name string\n-  \/\/\n-  \/\/ Inputs:\n-  \/\/   c_rarg0   - source array address\n-  \/\/   c_rarg1   - destination array address\n-  \/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n-  \/\/\n- \/\/ Side Effects:\n-  \/\/   disjoint_oop_copy_entry or disjoint_long_copy_entry is set to the\n-  \/\/   no-overlap entry point used by generate_conjoint_long_oop_copy().\n-  \/\/\n-  address generate_disjoint_long_oop_copy(bool aligned, bool is_oop, address *entry,\n-                                          const char *name, bool dest_uninitialized = false) {\n+\/\/ Arguments:\n+\/\/   aligned - true => Input and output aligned on a HeapWord boundary == 8 bytes\n+\/\/             ignored\n+\/\/   is_oop  - true => oop array, so generate store check code\n+\/\/   name    - stub name string\n+\/\/\n+\/\/ Inputs:\n+\/\/   c_rarg0   - source array address\n+\/\/   c_rarg1   - destination array address\n+\/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n+\/\/\n+address StubGenerator::generate_conjoint_long_oop_copy(bool aligned, bool is_oop, address nooverlap_target,\n+                                                       address *entry, const char *name,\n+                                                       bool dest_uninitialized) {\n@@ -2565,4 +2666,4 @@\n-    if (VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2() && MaxVectorSize  >= 32) {\n-       return generate_disjoint_copy_avx3_masked(entry, \"jlong_disjoint_arraycopy_avx3\", 3,\n-                                                 aligned, is_oop, dest_uninitialized);\n-    }\n+  if (VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2() && MaxVectorSize  >= 32) {\n+     return generate_conjoint_copy_avx3_masked(entry, \"jlong_conjoint_arraycopy_avx3\", 3,\n+                                               nooverlap_target, aligned, is_oop, dest_uninitialized);\n+  }\n@@ -2570,35 +2671,18 @@\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ pc();\n-\n-    Label L_copy_bytes, L_copy_8_bytes, L_exit;\n-    const Register from        = rdi;  \/\/ source array address\n-    const Register to          = rsi;  \/\/ destination array address\n-    const Register qword_count = rdx;  \/\/ elements count\n-    const Register end_from    = from; \/\/ source array end address\n-    const Register end_to      = rcx;  \/\/ destination array end address\n-    const Register saved_count = r11;\n-    \/\/ End pointers are inclusive, and if count is not zero they point\n-    \/\/ to the last unit copied:  end_to[0] := end_from[0]\n-\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    \/\/ Save no-overlap entry point for generate_conjoint_long_oop_copy()\n-    assert_clean_int(c_rarg2, rax);    \/\/ Make sure 'count' is clean int.\n-\n-    if (entry != NULL) {\n-      *entry = __ pc();\n-      \/\/ caller can pass a 64-bit byte count here (from Unsafe.copyMemory)\n-      BLOCK_COMMENT(\"Entry:\");\n-    }\n-\n-    setup_arg_regs_using_thread(); \/\/ from => rdi, to => rsi, count => rdx\n-                                     \/\/ r9 is used to save r15_thread\n-    \/\/ 'from', 'to' and 'qword_count' are now valid\n-\n-    DecoratorSet decorators = IN_HEAP | IS_ARRAY | ARRAYCOPY_DISJOINT;\n-    if (dest_uninitialized) {\n-      decorators |= IS_DEST_UNINITIALIZED;\n-    }\n-    if (aligned) {\n-      decorators |= ARRAYCOPY_ALIGNED;\n-    }\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", name);\n+  address start = __ pc();\n+\n+  Label L_copy_bytes, L_copy_8_bytes, L_exit;\n+  const Register from        = rdi;  \/\/ source array address\n+  const Register to          = rsi;  \/\/ destination array address\n+  const Register qword_count = rdx;  \/\/ elements count\n+  const Register saved_count = rcx;\n+\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  assert_clean_int(c_rarg2, rax);    \/\/ Make sure 'count' is clean int.\n+\n+  if (entry != NULL) {\n+    *entry = __ pc();\n+    \/\/ caller can pass a 64-bit byte count here (from Unsafe.copyMemory)\n+    BLOCK_COMMENT(\"Entry:\");\n+  }\n@@ -2606,6 +2690,4 @@\n-    BasicType type = is_oop ? T_OBJECT : T_LONG;\n-    BarrierSetAssembler *bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-    bs->arraycopy_prologue(_masm, decorators, type, from, to, qword_count);\n-    {\n-      \/\/ UnsafeCopyMemory page error: continue after ucm\n-      UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n+  array_overlap_test(nooverlap_target, Address::times_8);\n+  setup_arg_regs_using_thread(); \/\/ from => rdi, to => rsi, count => rdx\n+                                 \/\/ r9 is used to save r15_thread\n+  \/\/ 'from', 'to' and 'qword_count' are now valid\n@@ -2613,5 +2695,7 @@\n-      \/\/ Copy from low to high addresses.  Use 'to' as scratch.\n-      __ lea(end_from, Address(from, qword_count, Address::times_8, -8));\n-      __ lea(end_to,   Address(to,   qword_count, Address::times_8, -8));\n-      __ negptr(qword_count);\n-      __ jmp(L_copy_bytes);\n+  DecoratorSet decorators = IN_HEAP | IS_ARRAY;\n+  if (dest_uninitialized) {\n+    decorators |= IS_DEST_UNINITIALIZED;\n+  }\n+  if (aligned) {\n+    decorators |= ARRAYCOPY_ALIGNED;\n+  }\n@@ -2619,17 +2703,6 @@\n-      \/\/ Copy trailing qwords\n-    __ BIND(L_copy_8_bytes);\n-      __ movq(rax, Address(end_from, qword_count, Address::times_8, 8));\n-      __ movq(Address(end_to, qword_count, Address::times_8, 8), rax);\n-      __ increment(qword_count);\n-      __ jcc(Assembler::notZero, L_copy_8_bytes);\n-    }\n-    if (is_oop) {\n-      __ jmp(L_exit);\n-    } else {\n-      restore_arg_regs_using_thread();\n-      INC_COUNTER_NP(SharedRuntime::_jlong_array_copy_ctr, rscratch1); \/\/ Update counter after rscratch1 is free\n-      __ xorptr(rax, rax); \/\/ return 0\n-      __ vzeroupper();\n-      __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-      __ ret(0);\n-    }\n+  BasicType type = is_oop ? T_OBJECT : T_LONG;\n+  BarrierSetAssembler *bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  bs->arraycopy_prologue(_masm, decorators, type, from, to, qword_count);\n+  {\n+    \/\/ UnsafeCopyMemory page error: continue after ucm\n+    UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n@@ -2637,6 +2710,1 @@\n-    {\n-      \/\/ UnsafeCopyMemory page error: continue after ucm\n-      UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n-      \/\/ Copy in multi-bytes chunks\n-      copy_bytes_forward(end_from, end_to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n-    }\n+    __ jmp(L_copy_bytes);\n@@ -2644,2 +2712,10 @@\n-    __ BIND(L_exit);\n-    bs->arraycopy_epilogue(_masm, decorators, type, from, to, qword_count);\n+    \/\/ Copy trailing qwords\n+    __ BIND(L_copy_8_bytes);\n+    __ movq(rax, Address(from, qword_count, Address::times_8, -8));\n+    __ movq(Address(to, qword_count, Address::times_8, -8), rax);\n+    __ decrement(qword_count);\n+    __ jcc(Assembler::notZero, L_copy_8_bytes);\n+  }\n+  if (is_oop) {\n+    __ jmp(L_exit);\n+  } else {\n@@ -2647,4 +2723,1 @@\n-    INC_COUNTER_NP(is_oop ? SharedRuntime::_oop_array_copy_ctr :\n-                            SharedRuntime::_jlong_array_copy_ctr,\n-                   rscratch1); \/\/ Update counter after rscratch1 is free\n-    __ vzeroupper();\n+    INC_COUNTER_NP(SharedRuntime::_jlong_array_copy_ctr, rscratch1); \/\/ Update counter after rscratch1 is free\n@@ -2652,0 +2725,1 @@\n+    __ vzeroupper();\n@@ -2654,0 +2728,4 @@\n+  }\n+  {\n+    \/\/ UnsafeCopyMemory page error: continue after ucm\n+    UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n@@ -2655,1 +2733,2 @@\n-    return start;\n+    \/\/ Copy in multi-bytes chunks\n+    copy_bytes_backward(from, to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n@@ -2657,0 +2736,10 @@\n+  __ BIND(L_exit);\n+  bs->arraycopy_epilogue(_masm, decorators, type, from, to, qword_count);\n+  restore_arg_regs_using_thread();\n+  INC_COUNTER_NP(is_oop ? SharedRuntime::_oop_array_copy_ctr :\n+                          SharedRuntime::_jlong_array_copy_ctr,\n+                 rscratch1); \/\/ Update counter after rscratch1 is free\n+  __ vzeroupper();\n+  __ xorptr(rax, rax); \/\/ return 0\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n@@ -2658,38 +2747,2 @@\n-  \/\/ Arguments:\n-  \/\/   aligned - true => Input and output aligned on a HeapWord boundary == 8 bytes\n-  \/\/             ignored\n-  \/\/   is_oop  - true => oop array, so generate store check code\n-  \/\/   name    - stub name string\n-  \/\/\n-  \/\/ Inputs:\n-  \/\/   c_rarg0   - source array address\n-  \/\/   c_rarg1   - destination array address\n-  \/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n-  \/\/\n-  address generate_conjoint_long_oop_copy(bool aligned, bool is_oop,\n-                                          address nooverlap_target, address *entry,\n-                                          const char *name, bool dest_uninitialized = false) {\n-#if COMPILER2_OR_JVMCI\n-    if (VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2() && MaxVectorSize  >= 32) {\n-       return generate_conjoint_copy_avx3_masked(entry, \"jlong_conjoint_arraycopy_avx3\", 3,\n-                                                 nooverlap_target, aligned, is_oop, dest_uninitialized);\n-    }\n-#endif\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ pc();\n-\n-    Label L_copy_bytes, L_copy_8_bytes, L_exit;\n-    const Register from        = rdi;  \/\/ source array address\n-    const Register to          = rsi;  \/\/ destination array address\n-    const Register qword_count = rdx;  \/\/ elements count\n-    const Register saved_count = rcx;\n-\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    assert_clean_int(c_rarg2, rax);    \/\/ Make sure 'count' is clean int.\n-\n-    if (entry != NULL) {\n-      *entry = __ pc();\n-      \/\/ caller can pass a 64-bit byte count here (from Unsafe.copyMemory)\n-      BLOCK_COMMENT(\"Entry:\");\n-    }\n+  return start;\n+}\n@@ -2697,4 +2750,0 @@\n-    array_overlap_test(nooverlap_target, Address::times_8);\n-    setup_arg_regs_using_thread(); \/\/ from => rdi, to => rsi, count => rdx\n-                                   \/\/ r9 is used to save r15_thread\n-    \/\/ 'from', 'to' and 'qword_count' are now valid\n@@ -2702,7 +2751,7 @@\n-    DecoratorSet decorators = IN_HEAP | IS_ARRAY;\n-    if (dest_uninitialized) {\n-      decorators |= IS_DEST_UNINITIALIZED;\n-    }\n-    if (aligned) {\n-      decorators |= ARRAYCOPY_ALIGNED;\n-    }\n+\/\/ Helper for generating a dynamic type check.\n+\/\/ Smashes no registers.\n+void StubGenerator::generate_type_check(Register sub_klass,\n+                                        Register super_check_offset,\n+                                        Register super_klass,\n+                                        Label& L_success) {\n+  assert_different_registers(sub_klass, super_check_offset, super_klass);\n@@ -2710,6 +2759,1 @@\n-    BasicType type = is_oop ? T_OBJECT : T_LONG;\n-    BarrierSetAssembler *bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-    bs->arraycopy_prologue(_masm, decorators, type, from, to, qword_count);\n-    {\n-      \/\/ UnsafeCopyMemory page error: continue after ucm\n-      UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n+  BLOCK_COMMENT(\"type_check:\");\n@@ -2717,1 +2761,1 @@\n-      __ jmp(L_copy_bytes);\n+  Label L_miss;\n@@ -2719,20 +2763,3 @@\n-      \/\/ Copy trailing qwords\n-    __ BIND(L_copy_8_bytes);\n-      __ movq(rax, Address(from, qword_count, Address::times_8, -8));\n-      __ movq(Address(to, qword_count, Address::times_8, -8), rax);\n-      __ decrement(qword_count);\n-      __ jcc(Assembler::notZero, L_copy_8_bytes);\n-    }\n-    if (is_oop) {\n-      __ jmp(L_exit);\n-    } else {\n-      restore_arg_regs_using_thread();\n-      INC_COUNTER_NP(SharedRuntime::_jlong_array_copy_ctr, rscratch1); \/\/ Update counter after rscratch1 is free\n-      __ xorptr(rax, rax); \/\/ return 0\n-      __ vzeroupper();\n-      __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-      __ ret(0);\n-    }\n-    {\n-      \/\/ UnsafeCopyMemory page error: continue after ucm\n-      UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n+  __ check_klass_subtype_fast_path(sub_klass, super_klass, noreg,        &L_success, &L_miss, NULL,\n+                                   super_check_offset);\n+  __ check_klass_subtype_slow_path(sub_klass, super_klass, noreg, noreg, &L_success, NULL);\n@@ -2740,13 +2767,3 @@\n-      \/\/ Copy in multi-bytes chunks\n-      copy_bytes_backward(from, to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n-    }\n-    __ BIND(L_exit);\n-    bs->arraycopy_epilogue(_masm, decorators, type, from, to, qword_count);\n-    restore_arg_regs_using_thread();\n-    INC_COUNTER_NP(is_oop ? SharedRuntime::_oop_array_copy_ctr :\n-                            SharedRuntime::_jlong_array_copy_ctr,\n-                   rscratch1); \/\/ Update counter after rscratch1 is free\n-    __ vzeroupper();\n-    __ xorptr(rax, rax); \/\/ return 0\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n+  \/\/ Fall through on failure!\n+  __ BIND(L_miss);\n+}\n@@ -2754,2 +2771,17 @@\n-    return start;\n-  }\n+\/\/  Generate checkcasting array copy stub\n+\/\/\n+\/\/  Input:\n+\/\/    c_rarg0   - source array address\n+\/\/    c_rarg1   - destination array address\n+\/\/    c_rarg2   - element count, treated as ssize_t, can be zero\n+\/\/    c_rarg3   - size_t ckoff (super_check_offset)\n+\/\/ not Win64\n+\/\/    c_rarg4   - oop ckval (super_klass)\n+\/\/ Win64\n+\/\/    rsp+40    - oop ckval (super_klass)\n+\/\/\n+\/\/  Output:\n+\/\/    rax ==  0  -  success\n+\/\/    rax == -1^K - failure, where K is partial transfer count\n+\/\/\n+address StubGenerator::generate_checkcast_copy(const char *name, address *entry, bool dest_uninitialized) {\n@@ -2757,0 +2789,1 @@\n+  Label L_load_element, L_store_element, L_do_card_marks, L_done;\n@@ -2758,7 +2791,6 @@\n-  \/\/ Helper for generating a dynamic type check.\n-  \/\/ Smashes no registers.\n-  void generate_type_check(Register sub_klass,\n-                           Register super_check_offset,\n-                           Register super_klass,\n-                           Label& L_success) {\n-    assert_different_registers(sub_klass, super_check_offset, super_klass);\n+  \/\/ Input registers (after setup_arg_regs)\n+  const Register from        = rdi;   \/\/ source array address\n+  const Register to          = rsi;   \/\/ destination array address\n+  const Register length      = rdx;   \/\/ elements count\n+  const Register ckoff       = rcx;   \/\/ super_check_offset\n+  const Register ckval       = r8;    \/\/ super_klass\n@@ -2766,1 +2798,7 @@\n-    BLOCK_COMMENT(\"type_check:\");\n+  \/\/ Registers used as temps (r13, r14 are save-on-entry)\n+  const Register end_from    = from;  \/\/ source array end address\n+  const Register end_to      = r13;   \/\/ destination array end address\n+  const Register count       = rdx;   \/\/ -(count_remaining)\n+  const Register r14_length  = r14;   \/\/ saved copy of length\n+  \/\/ End pointers are inclusive, and if length is not zero they point\n+  \/\/ to the last unit copied:  end_to[0] := end_from[0]\n@@ -2768,1 +2806,2 @@\n-    Label L_miss;\n+  const Register rax_oop    = rax;    \/\/ actual oop copied\n+  const Register r11_klass  = r11;    \/\/ oop._klass\n@@ -2770,3 +2809,6 @@\n-    __ check_klass_subtype_fast_path(sub_klass, super_klass, noreg,        &L_success, &L_miss, NULL,\n-                                     super_check_offset);\n-    __ check_klass_subtype_slow_path(sub_klass, super_klass, noreg, noreg, &L_success, NULL);\n+  \/\/---------------------------------------------------------------\n+  \/\/ Assembler stub will be used for this call to arraycopy\n+  \/\/ if the two arrays are subtypes of Object[] but the\n+  \/\/ destination array type is not equal to or a supertype\n+  \/\/ of the source type.  Each element must be separately\n+  \/\/ checked.\n@@ -2774,3 +2816,3 @@\n-    \/\/ Fall through on failure!\n-    __ BIND(L_miss);\n-  }\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", name);\n+  address start = __ pc();\n@@ -2778,52 +2820,1 @@\n-  \/\/\n-  \/\/  Generate checkcasting array copy stub\n-  \/\/\n-  \/\/  Input:\n-  \/\/    c_rarg0   - source array address\n-  \/\/    c_rarg1   - destination array address\n-  \/\/    c_rarg2   - element count, treated as ssize_t, can be zero\n-  \/\/    c_rarg3   - size_t ckoff (super_check_offset)\n-  \/\/ not Win64\n-  \/\/    c_rarg4   - oop ckval (super_klass)\n-  \/\/ Win64\n-  \/\/    rsp+40    - oop ckval (super_klass)\n-  \/\/\n-  \/\/  Output:\n-  \/\/    rax ==  0  -  success\n-  \/\/    rax == -1^K - failure, where K is partial transfer count\n-  \/\/\n-  address generate_checkcast_copy(const char *name, address *entry,\n-                                  bool dest_uninitialized = false) {\n-\n-    Label L_load_element, L_store_element, L_do_card_marks, L_done;\n-\n-    \/\/ Input registers (after setup_arg_regs)\n-    const Register from        = rdi;   \/\/ source array address\n-    const Register to          = rsi;   \/\/ destination array address\n-    const Register length      = rdx;   \/\/ elements count\n-    const Register ckoff       = rcx;   \/\/ super_check_offset\n-    const Register ckval       = r8;    \/\/ super_klass\n-\n-    \/\/ Registers used as temps (r13, r14 are save-on-entry)\n-    const Register end_from    = from;  \/\/ source array end address\n-    const Register end_to      = r13;   \/\/ destination array end address\n-    const Register count       = rdx;   \/\/ -(count_remaining)\n-    const Register r14_length  = r14;   \/\/ saved copy of length\n-    \/\/ End pointers are inclusive, and if length is not zero they point\n-    \/\/ to the last unit copied:  end_to[0] := end_from[0]\n-\n-    const Register rax_oop    = rax;    \/\/ actual oop copied\n-    const Register r11_klass  = r11;    \/\/ oop._klass\n-\n-    \/\/---------------------------------------------------------------\n-    \/\/ Assembler stub will be used for this call to arraycopy\n-    \/\/ if the two arrays are subtypes of Object[] but the\n-    \/\/ destination array type is not equal to or a supertype\n-    \/\/ of the source type.  Each element must be separately\n-    \/\/ checked.\n-\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ pc();\n-\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n@@ -2832,7 +2823,7 @@\n-    \/\/ caller guarantees that the arrays really are different\n-    \/\/ otherwise, we would have to make conjoint checks\n-    { Label L;\n-      array_overlap_test(L, TIMES_OOP);\n-      __ stop(\"checkcast_copy within a single array\");\n-      __ bind(L);\n-    }\n+  \/\/ caller guarantees that the arrays really are different\n+  \/\/ otherwise, we would have to make conjoint checks\n+  { Label L;\n+    array_overlap_test(L, TIMES_OOP);\n+    __ stop(\"checkcast_copy within a single array\");\n+    __ bind(L);\n+  }\n@@ -2841,3 +2832,3 @@\n-    setup_arg_regs(4); \/\/ from => rdi, to => rsi, length => rdx\n-                       \/\/ ckoff => rcx, ckval => r8\n-                       \/\/ r9 and r10 may be used to save non-volatile registers\n+  setup_arg_regs(4); \/\/ from => rdi, to => rsi, length => rdx\n+                     \/\/ ckoff => rcx, ckval => r8\n+                     \/\/ r9 and r10 may be used to save non-volatile registers\n@@ -2845,2 +2836,2 @@\n-    \/\/ last argument (#4) is on stack on Win64\n-    __ movptr(ckval, Address(rsp, 6 * wordSize));\n+  \/\/ last argument (#4) is on stack on Win64\n+  __ movptr(ckval, Address(rsp, 6 * wordSize));\n@@ -2849,5 +2840,5 @@\n-    \/\/ Caller of this entry point must set up the argument registers.\n-    if (entry != NULL) {\n-      *entry = __ pc();\n-      BLOCK_COMMENT(\"Entry:\");\n-    }\n+  \/\/ Caller of this entry point must set up the argument registers.\n+  if (entry != NULL) {\n+    *entry = __ pc();\n+    BLOCK_COMMENT(\"Entry:\");\n+  }\n@@ -2855,11 +2846,11 @@\n-    \/\/ allocate spill slots for r13, r14\n-    enum {\n-      saved_r13_offset,\n-      saved_r14_offset,\n-      saved_r10_offset,\n-      saved_rbp_offset\n-    };\n-    __ subptr(rsp, saved_rbp_offset * wordSize);\n-    __ movptr(Address(rsp, saved_r13_offset * wordSize), r13);\n-    __ movptr(Address(rsp, saved_r14_offset * wordSize), r14);\n-    __ movptr(Address(rsp, saved_r10_offset * wordSize), r10);\n+  \/\/ allocate spill slots for r13, r14\n+  enum {\n+    saved_r13_offset,\n+    saved_r14_offset,\n+    saved_r10_offset,\n+    saved_rbp_offset\n+  };\n+  __ subptr(rsp, saved_rbp_offset * wordSize);\n+  __ movptr(Address(rsp, saved_r13_offset * wordSize), r13);\n+  __ movptr(Address(rsp, saved_r14_offset * wordSize), r14);\n+  __ movptr(Address(rsp, saved_r10_offset * wordSize), r10);\n@@ -2868,6 +2859,6 @@\n-      Label L2;\n-      __ get_thread(r14);\n-      __ cmpptr(r15_thread, r14);\n-      __ jcc(Assembler::equal, L2);\n-      __ stop(\"StubRoutines::call_stub: r15_thread is modified by call\");\n-      __ bind(L2);\n+    Label L2;\n+    __ get_thread(r14);\n+    __ cmpptr(r15_thread, r14);\n+    __ jcc(Assembler::equal, L2);\n+    __ stop(\"StubRoutines::call_stub: r15_thread is modified by call\");\n+    __ bind(L2);\n@@ -2876,3 +2867,3 @@\n-    \/\/ check that int operands are properly extended to size_t\n-    assert_clean_int(length, rax);\n-    assert_clean_int(ckoff, rax);\n+  \/\/ check that int operands are properly extended to size_t\n+  assert_clean_int(length, rax);\n+  assert_clean_int(ckoff, rax);\n@@ -2881,10 +2872,10 @@\n-    BLOCK_COMMENT(\"assert consistent ckoff\/ckval\");\n-    \/\/ The ckoff and ckval must be mutually consistent,\n-    \/\/ even though caller generates both.\n-    { Label L;\n-      int sco_offset = in_bytes(Klass::super_check_offset_offset());\n-      __ cmpl(ckoff, Address(ckval, sco_offset));\n-      __ jcc(Assembler::equal, L);\n-      __ stop(\"super_check_offset inconsistent\");\n-      __ bind(L);\n-    }\n+  BLOCK_COMMENT(\"assert consistent ckoff\/ckval\");\n+  \/\/ The ckoff and ckval must be mutually consistent,\n+  \/\/ even though caller generates both.\n+  { Label L;\n+    int sco_offset = in_bytes(Klass::super_check_offset_offset());\n+    __ cmpl(ckoff, Address(ckval, sco_offset));\n+    __ jcc(Assembler::equal, L);\n+    __ stop(\"super_check_offset inconsistent\");\n+    __ bind(L);\n+  }\n@@ -2893,78 +2884,6 @@\n-    \/\/ Loop-invariant addresses.  They are exclusive end pointers.\n-    Address end_from_addr(from, length, TIMES_OOP, 0);\n-    Address   end_to_addr(to,   length, TIMES_OOP, 0);\n-    \/\/ Loop-variant addresses.  They assume post-incremented count < 0.\n-    Address from_element_addr(end_from, count, TIMES_OOP, 0);\n-    Address   to_element_addr(end_to,   count, TIMES_OOP, 0);\n-\n-    DecoratorSet decorators = IN_HEAP | IS_ARRAY | ARRAYCOPY_CHECKCAST | ARRAYCOPY_DISJOINT;\n-    if (dest_uninitialized) {\n-      decorators |= IS_DEST_UNINITIALIZED;\n-    }\n-\n-    BasicType type = T_OBJECT;\n-    BarrierSetAssembler *bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-    bs->arraycopy_prologue(_masm, decorators, type, from, to, count);\n-\n-    \/\/ Copy from low to high addresses, indexed from the end of each array.\n-    __ lea(end_from, end_from_addr);\n-    __ lea(end_to,   end_to_addr);\n-    __ movptr(r14_length, length);        \/\/ save a copy of the length\n-    assert(length == count, \"\");          \/\/ else fix next line:\n-    __ negptr(count);                     \/\/ negate and test the length\n-    __ jcc(Assembler::notZero, L_load_element);\n-\n-    \/\/ Empty array:  Nothing to do.\n-    __ xorptr(rax, rax);                  \/\/ return 0 on (trivial) success\n-    __ jmp(L_done);\n-\n-    \/\/ ======== begin loop ========\n-    \/\/ (Loop is rotated; its entry is L_load_element.)\n-    \/\/ Loop control:\n-    \/\/   for (count = -count; count != 0; count++)\n-    \/\/ Base pointers src, dst are biased by 8*(count-1),to last element.\n-    __ align(OptoLoopAlignment);\n-\n-    __ BIND(L_store_element);\n-    __ store_heap_oop(to_element_addr, rax_oop, noreg, noreg, noreg, AS_RAW);  \/\/ store the oop\n-    __ increment(count);               \/\/ increment the count toward zero\n-    __ jcc(Assembler::zero, L_do_card_marks);\n-\n-    \/\/ ======== loop entry is here ========\n-    __ BIND(L_load_element);\n-    __ load_heap_oop(rax_oop, from_element_addr, noreg, noreg, AS_RAW); \/\/ load the oop\n-    __ testptr(rax_oop, rax_oop);\n-    __ jcc(Assembler::zero, L_store_element);\n-\n-    __ load_klass(r11_klass, rax_oop, rscratch1);\/\/ query the object klass\n-    generate_type_check(r11_klass, ckoff, ckval, L_store_element);\n-    \/\/ ======== end loop ========\n-\n-    \/\/ It was a real error; we must depend on the caller to finish the job.\n-    \/\/ Register rdx = -1 * number of *remaining* oops, r14 = *total* oops.\n-    \/\/ Emit GC store barriers for the oops we have copied (r14 + rdx),\n-    \/\/ and report their number to the caller.\n-    assert_different_registers(rax, r14_length, count, to, end_to, rcx, rscratch1);\n-    Label L_post_barrier;\n-    __ addptr(r14_length, count);     \/\/ K = (original - remaining) oops\n-    __ movptr(rax, r14_length);       \/\/ save the value\n-    __ notptr(rax);                   \/\/ report (-1^K) to caller (does not affect flags)\n-    __ jccb(Assembler::notZero, L_post_barrier);\n-    __ jmp(L_done); \/\/ K == 0, nothing was copied, skip post barrier\n-\n-    \/\/ Come here on success only.\n-    __ BIND(L_do_card_marks);\n-    __ xorptr(rax, rax);              \/\/ return 0 on success\n-\n-    __ BIND(L_post_barrier);\n-    bs->arraycopy_epilogue(_masm, decorators, type, from, to, r14_length);\n-\n-    \/\/ Common exit point (success or failure).\n-    __ BIND(L_done);\n-    __ movptr(r13, Address(rsp, saved_r13_offset * wordSize));\n-    __ movptr(r14, Address(rsp, saved_r14_offset * wordSize));\n-    __ movptr(r10, Address(rsp, saved_r10_offset * wordSize));\n-    restore_arg_regs();\n-    INC_COUNTER_NP(SharedRuntime::_checkcast_array_copy_ctr, rscratch1); \/\/ Update counter after rscratch1 is free\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n+  \/\/ Loop-invariant addresses.  They are exclusive end pointers.\n+  Address end_from_addr(from, length, TIMES_OOP, 0);\n+  Address   end_to_addr(to,   length, TIMES_OOP, 0);\n+  \/\/ Loop-variant addresses.  They assume post-incremented count < 0.\n+  Address from_element_addr(end_from, count, TIMES_OOP, 0);\n+  Address   to_element_addr(end_to,   count, TIMES_OOP, 0);\n@@ -2972,1 +2891,3 @@\n-    return start;\n+  DecoratorSet decorators = IN_HEAP | IS_ARRAY | ARRAYCOPY_CHECKCAST | ARRAYCOPY_DISJOINT;\n+  if (dest_uninitialized) {\n+    decorators |= IS_DEST_UNINITIALIZED;\n@@ -2975,18 +2896,66 @@\n-  \/\/\n-  \/\/  Generate 'unsafe' array copy stub\n-  \/\/  Though just as safe as the other stubs, it takes an unscaled\n-  \/\/  size_t argument instead of an element count.\n-  \/\/\n-  \/\/  Input:\n-  \/\/    c_rarg0   - source array address\n-  \/\/    c_rarg1   - destination array address\n-  \/\/    c_rarg2   - byte count, treated as ssize_t, can be zero\n-  \/\/\n-  \/\/ Examines the alignment of the operands and dispatches\n-  \/\/ to a long, int, short, or byte copy loop.\n-  \/\/\n-  address generate_unsafe_copy(const char *name,\n-                               address byte_copy_entry, address short_copy_entry,\n-                               address int_copy_entry, address long_copy_entry) {\n-\n-    Label L_long_aligned, L_int_aligned, L_short_aligned;\n+  BasicType type = T_OBJECT;\n+  BarrierSetAssembler *bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  bs->arraycopy_prologue(_masm, decorators, type, from, to, count);\n+\n+  \/\/ Copy from low to high addresses, indexed from the end of each array.\n+  __ lea(end_from, end_from_addr);\n+  __ lea(end_to,   end_to_addr);\n+  __ movptr(r14_length, length);        \/\/ save a copy of the length\n+  assert(length == count, \"\");          \/\/ else fix next line:\n+  __ negptr(count);                     \/\/ negate and test the length\n+  __ jcc(Assembler::notZero, L_load_element);\n+\n+  \/\/ Empty array:  Nothing to do.\n+  __ xorptr(rax, rax);                  \/\/ return 0 on (trivial) success\n+  __ jmp(L_done);\n+\n+  \/\/ ======== begin loop ========\n+  \/\/ (Loop is rotated; its entry is L_load_element.)\n+  \/\/ Loop control:\n+  \/\/   for (count = -count; count != 0; count++)\n+  \/\/ Base pointers src, dst are biased by 8*(count-1),to last element.\n+  __ align(OptoLoopAlignment);\n+\n+  __ BIND(L_store_element);\n+  __ store_heap_oop(to_element_addr, rax_oop, noreg, noreg, noreg, AS_RAW);  \/\/ store the oop\n+  __ increment(count);               \/\/ increment the count toward zero\n+  __ jcc(Assembler::zero, L_do_card_marks);\n+\n+  \/\/ ======== loop entry is here ========\n+  __ BIND(L_load_element);\n+  __ load_heap_oop(rax_oop, from_element_addr, noreg, noreg, AS_RAW); \/\/ load the oop\n+  __ testptr(rax_oop, rax_oop);\n+  __ jcc(Assembler::zero, L_store_element);\n+\n+  __ load_klass(r11_klass, rax_oop, rscratch1);\/\/ query the object klass\n+  generate_type_check(r11_klass, ckoff, ckval, L_store_element);\n+  \/\/ ======== end loop ========\n+\n+  \/\/ It was a real error; we must depend on the caller to finish the job.\n+  \/\/ Register rdx = -1 * number of *remaining* oops, r14 = *total* oops.\n+  \/\/ Emit GC store barriers for the oops we have copied (r14 + rdx),\n+  \/\/ and report their number to the caller.\n+  assert_different_registers(rax, r14_length, count, to, end_to, rcx, rscratch1);\n+  Label L_post_barrier;\n+  __ addptr(r14_length, count);     \/\/ K = (original - remaining) oops\n+  __ movptr(rax, r14_length);       \/\/ save the value\n+  __ notptr(rax);                   \/\/ report (-1^K) to caller (does not affect flags)\n+  __ jccb(Assembler::notZero, L_post_barrier);\n+  __ jmp(L_done); \/\/ K == 0, nothing was copied, skip post barrier\n+\n+  \/\/ Come here on success only.\n+  __ BIND(L_do_card_marks);\n+  __ xorptr(rax, rax);              \/\/ return 0 on success\n+\n+  __ BIND(L_post_barrier);\n+  bs->arraycopy_epilogue(_masm, decorators, type, from, to, r14_length);\n+\n+  \/\/ Common exit point (success or failure).\n+  __ BIND(L_done);\n+  __ movptr(r13, Address(rsp, saved_r13_offset * wordSize));\n+  __ movptr(r14, Address(rsp, saved_r14_offset * wordSize));\n+  __ movptr(r10, Address(rsp, saved_r10_offset * wordSize));\n+  restore_arg_regs();\n+  INC_COUNTER_NP(SharedRuntime::_checkcast_array_copy_ctr, rscratch1); \/\/ Update counter after rscratch1 is free\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n@@ -2994,7 +2963,2 @@\n-    \/\/ Input registers (before setup_arg_regs)\n-    const Register from        = c_rarg0;  \/\/ source array address\n-    const Register to          = c_rarg1;  \/\/ destination array address\n-    const Register size        = c_rarg2;  \/\/ byte count (size_t)\n-\n-    \/\/ Register used as a temp\n-    const Register bits        = rax;      \/\/ test copy of low bits\n+  return start;\n+}\n@@ -3002,3 +2966,15 @@\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ pc();\n+\/\/  Generate 'unsafe' array copy stub\n+\/\/  Though just as safe as the other stubs, it takes an unscaled\n+\/\/  size_t argument instead of an element count.\n+\/\/\n+\/\/  Input:\n+\/\/    c_rarg0   - source array address\n+\/\/    c_rarg1   - destination array address\n+\/\/    c_rarg2   - byte count, treated as ssize_t, can be zero\n+\/\/\n+\/\/ Examines the alignment of the operands and dispatches\n+\/\/ to a long, int, short, or byte copy loop.\n+\/\/\n+address StubGenerator::generate_unsafe_copy(const char *name,\n+                                            address byte_copy_entry, address short_copy_entry,\n+                                            address int_copy_entry, address long_copy_entry) {\n@@ -3006,1 +2982,1 @@\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  Label L_long_aligned, L_int_aligned, L_short_aligned;\n@@ -3008,2 +2984,4 @@\n-    \/\/ bump this on entry, not on exit:\n-    INC_COUNTER_NP(SharedRuntime::_unsafe_array_copy_ctr, rscratch1);\n+  \/\/ Input registers (before setup_arg_regs)\n+  const Register from        = c_rarg0;  \/\/ source array address\n+  const Register to          = c_rarg1;  \/\/ destination array address\n+  const Register size        = c_rarg2;  \/\/ byte count (size_t)\n@@ -3011,3 +2989,2 @@\n-    __ mov(bits, from);\n-    __ orptr(bits, to);\n-    __ orptr(bits, size);\n+  \/\/ Register used as a temp\n+  const Register bits        = rax;      \/\/ test copy of low bits\n@@ -3015,2 +2992,3 @@\n-    __ testb(bits, BytesPerLong-1);\n-    __ jccb(Assembler::zero, L_long_aligned);\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", name);\n+  address start = __ pc();\n@@ -3018,2 +2996,1 @@\n-    __ testb(bits, BytesPerInt-1);\n-    __ jccb(Assembler::zero, L_int_aligned);\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n@@ -3021,2 +2998,2 @@\n-    __ testb(bits, BytesPerShort-1);\n-    __ jump_cc(Assembler::notZero, RuntimeAddress(byte_copy_entry));\n+  \/\/ bump this on entry, not on exit:\n+  INC_COUNTER_NP(SharedRuntime::_unsafe_array_copy_ctr, rscratch1);\n@@ -3024,3 +3001,3 @@\n-    __ BIND(L_short_aligned);\n-    __ shrptr(size, LogBytesPerShort); \/\/ size => short_count\n-    __ jump(RuntimeAddress(short_copy_entry));\n+  __ mov(bits, from);\n+  __ orptr(bits, to);\n+  __ orptr(bits, size);\n@@ -3028,3 +3005,2 @@\n-    __ BIND(L_int_aligned);\n-    __ shrptr(size, LogBytesPerInt); \/\/ size => int_count\n-    __ jump(RuntimeAddress(int_copy_entry));\n+  __ testb(bits, BytesPerLong-1);\n+  __ jccb(Assembler::zero, L_long_aligned);\n@@ -3032,3 +3008,2 @@\n-    __ BIND(L_long_aligned);\n-    __ shrptr(size, LogBytesPerLong); \/\/ size => qword_count\n-    __ jump(RuntimeAddress(long_copy_entry));\n+  __ testb(bits, BytesPerInt-1);\n+  __ jccb(Assembler::zero, L_int_aligned);\n@@ -3036,2 +3011,2 @@\n-    return start;\n-  }\n+  __ testb(bits, BytesPerShort-1);\n+  __ jump_cc(Assembler::notZero, RuntimeAddress(byte_copy_entry));\n@@ -3039,11 +3014,3 @@\n-  \/\/ Perform range checks on the proposed arraycopy.\n-  \/\/ Kills temp, but nothing else.\n-  \/\/ Also, clean the sign bits of src_pos and dst_pos.\n-  void arraycopy_range_checks(Register src,     \/\/ source array oop (c_rarg0)\n-                              Register src_pos, \/\/ source position (c_rarg1)\n-                              Register dst,     \/\/ destination array oo (c_rarg2)\n-                              Register dst_pos, \/\/ destination position (c_rarg3)\n-                              Register length,\n-                              Register temp,\n-                              Label& L_failed) {\n-    BLOCK_COMMENT(\"arraycopy_range_checks:\");\n+  __ BIND(L_short_aligned);\n+  __ shrptr(size, LogBytesPerShort); \/\/ size => short_count\n+  __ jump(RuntimeAddress(short_copy_entry));\n@@ -3051,5 +3018,3 @@\n-    \/\/  if (src_pos + length > arrayOop(src)->length())  FAIL;\n-    __ movl(temp, length);\n-    __ addl(temp, src_pos);             \/\/ src_pos + length\n-    __ cmpl(temp, Address(src, arrayOopDesc::length_offset_in_bytes()));\n-    __ jcc(Assembler::above, L_failed);\n+  __ BIND(L_int_aligned);\n+  __ shrptr(size, LogBytesPerInt); \/\/ size => int_count\n+  __ jump(RuntimeAddress(int_copy_entry));\n@@ -3057,5 +3022,3 @@\n-    \/\/  if (dst_pos + length > arrayOop(dst)->length())  FAIL;\n-    __ movl(temp, length);\n-    __ addl(temp, dst_pos);             \/\/ dst_pos + length\n-    __ cmpl(temp, Address(dst, arrayOopDesc::length_offset_in_bytes()));\n-    __ jcc(Assembler::above, L_failed);\n+  __ BIND(L_long_aligned);\n+  __ shrptr(size, LogBytesPerLong); \/\/ size => qword_count\n+  __ jump(RuntimeAddress(long_copy_entry));\n@@ -3063,4 +3026,2 @@\n-    \/\/ Have to clean up high 32-bits of 'src_pos' and 'dst_pos'.\n-    \/\/ Move with sign extension can be used since they are positive.\n-    __ movslq(src_pos, src_pos);\n-    __ movslq(dst_pos, dst_pos);\n+  return start;\n+}\n@@ -3068,2 +3029,31 @@\n-    BLOCK_COMMENT(\"arraycopy_range_checks done\");\n-  }\n+\/\/ Perform range checks on the proposed arraycopy.\n+\/\/ Kills temp, but nothing else.\n+\/\/ Also, clean the sign bits of src_pos and dst_pos.\n+void StubGenerator::arraycopy_range_checks(Register src,     \/\/ source array oop (c_rarg0)\n+                                           Register src_pos, \/\/ source position (c_rarg1)\n+                                           Register dst,     \/\/ destination array oo (c_rarg2)\n+                                           Register dst_pos, \/\/ destination position (c_rarg3)\n+                                           Register length,\n+                                           Register temp,\n+                                           Label& L_failed) {\n+  BLOCK_COMMENT(\"arraycopy_range_checks:\");\n+\n+  \/\/  if (src_pos + length > arrayOop(src)->length())  FAIL;\n+  __ movl(temp, length);\n+  __ addl(temp, src_pos);             \/\/ src_pos + length\n+  __ cmpl(temp, Address(src, arrayOopDesc::length_offset_in_bytes()));\n+  __ jcc(Assembler::above, L_failed);\n+\n+  \/\/  if (dst_pos + length > arrayOop(dst)->length())  FAIL;\n+  __ movl(temp, length);\n+  __ addl(temp, dst_pos);             \/\/ dst_pos + length\n+  __ cmpl(temp, Address(dst, arrayOopDesc::length_offset_in_bytes()));\n+  __ jcc(Assembler::above, L_failed);\n+\n+  \/\/ Have to clean up high 32-bits of 'src_pos' and 'dst_pos'.\n+  \/\/ Move with sign extension can be used since they are positive.\n+  __ movslq(src_pos, src_pos);\n+  __ movslq(dst_pos, dst_pos);\n+\n+  BLOCK_COMMENT(\"arraycopy_range_checks done\");\n+}\n@@ -3071,30 +3061,29 @@\n-  \/\/\n-  \/\/  Generate generic array copy stubs\n-  \/\/\n-  \/\/  Input:\n-  \/\/    c_rarg0    -  src oop\n-  \/\/    c_rarg1    -  src_pos (32-bits)\n-  \/\/    c_rarg2    -  dst oop\n-  \/\/    c_rarg3    -  dst_pos (32-bits)\n-  \/\/ not Win64\n-  \/\/    c_rarg4    -  element count (32-bits)\n-  \/\/ Win64\n-  \/\/    rsp+40     -  element count (32-bits)\n-  \/\/\n-  \/\/  Output:\n-  \/\/    rax ==  0  -  success\n-  \/\/    rax == -1^K - failure, where K is partial transfer count\n-  \/\/\n-  address generate_generic_copy(const char *name,\n-                                address byte_copy_entry, address short_copy_entry,\n-                                address int_copy_entry, address oop_copy_entry,\n-                                address long_copy_entry, address checkcast_copy_entry) {\n-\n-    Label L_failed, L_failed_0, L_objArray;\n-    Label L_copy_shorts, L_copy_ints, L_copy_longs;\n-\n-    \/\/ Input registers\n-    const Register src        = c_rarg0;  \/\/ source array oop\n-    const Register src_pos    = c_rarg1;  \/\/ source position\n-    const Register dst        = c_rarg2;  \/\/ destination array oop\n-    const Register dst_pos    = c_rarg3;  \/\/ destination position\n+\/\/  Generate generic array copy stubs\n+\/\/\n+\/\/  Input:\n+\/\/    c_rarg0    -  src oop\n+\/\/    c_rarg1    -  src_pos (32-bits)\n+\/\/    c_rarg2    -  dst oop\n+\/\/    c_rarg3    -  dst_pos (32-bits)\n+\/\/ not Win64\n+\/\/    c_rarg4    -  element count (32-bits)\n+\/\/ Win64\n+\/\/    rsp+40     -  element count (32-bits)\n+\/\/\n+\/\/  Output:\n+\/\/    rax ==  0  -  success\n+\/\/    rax == -1^K - failure, where K is partial transfer count\n+\/\/\n+address StubGenerator::generate_generic_copy(const char *name,\n+                                             address byte_copy_entry, address short_copy_entry,\n+                                             address int_copy_entry, address oop_copy_entry,\n+                                             address long_copy_entry, address checkcast_copy_entry) {\n+\n+  Label L_failed, L_failed_0, L_objArray;\n+  Label L_copy_shorts, L_copy_ints, L_copy_longs;\n+\n+  \/\/ Input registers\n+  const Register src        = c_rarg0;  \/\/ source array oop\n+  const Register src_pos    = c_rarg1;  \/\/ source position\n+  const Register dst        = c_rarg2;  \/\/ destination array oop\n+  const Register dst_pos    = c_rarg3;  \/\/ destination position\n@@ -3102,2 +3091,2 @@\n-    const Register length     = c_rarg4;\n-    const Register rklass_tmp = r9;  \/\/ load_klass\n+  const Register length     = c_rarg4;\n+  const Register rklass_tmp = r9;  \/\/ load_klass\n@@ -3105,2 +3094,2 @@\n-    const Address  length(rsp, 7 * wordSize);  \/\/ elements count is on stack on Win64\n-    const Register rklass_tmp = rdi;  \/\/ load_klass\n+  const Address  length(rsp, 7 * wordSize);  \/\/ elements count is on stack on Win64\n+  const Register rklass_tmp = rdi;  \/\/ load_klass\n@@ -3109,7 +3098,7 @@\n-    { int modulus = CodeEntryAlignment;\n-      int target  = modulus - 5; \/\/ 5 = sizeof jmp(L_failed)\n-      int advance = target - (__ offset() % modulus);\n-      if (advance < 0)  advance += modulus;\n-      if (advance > 0)  __ nop(advance);\n-    }\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n+  { int modulus = CodeEntryAlignment;\n+    int target  = modulus - 5; \/\/ 5 = sizeof jmp(L_failed)\n+    int advance = target - (__ offset() % modulus);\n+    if (advance < 0)  advance += modulus;\n+    if (advance > 0)  __ nop(advance);\n+  }\n+  StubCodeMark mark(this, \"StubRoutines\", name);\n@@ -3117,4 +3106,4 @@\n-    \/\/ Short-hop target to L_failed.  Makes for denser prologue code.\n-    __ BIND(L_failed_0);\n-    __ jmp(L_failed);\n-    assert(__ offset() % CodeEntryAlignment == 0, \"no further alignment needed\");\n+  \/\/ Short-hop target to L_failed.  Makes for denser prologue code.\n+  __ BIND(L_failed_0);\n+  __ jmp(L_failed);\n+  assert(__ offset() % CodeEntryAlignment == 0, \"no further alignment needed\");\n@@ -3122,2 +3111,2 @@\n-    __ align(CodeEntryAlignment);\n-    address start = __ pc();\n+  __ align(CodeEntryAlignment);\n+  address start = __ pc();\n@@ -3125,1 +3114,1 @@\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n@@ -3128,1 +3117,1 @@\n-    __ push(rklass_tmp); \/\/ rdi is callee-save on Windows\n+  __ push(rklass_tmp); \/\/ rdi is callee-save on Windows\n@@ -3131,2 +3120,2 @@\n-    \/\/ bump this on entry, not on exit:\n-    INC_COUNTER_NP(SharedRuntime::_generic_array_copy_ctr, rscratch1);\n+  \/\/ bump this on entry, not on exit:\n+  INC_COUNTER_NP(SharedRuntime::_generic_array_copy_ctr, rscratch1);\n@@ -3134,13 +3123,13 @@\n-    \/\/-----------------------------------------------------------------------\n-    \/\/ Assembler stub will be used for this call to arraycopy\n-    \/\/ if the following conditions are met:\n-    \/\/\n-    \/\/ (1) src and dst must not be null.\n-    \/\/ (2) src_pos must not be negative.\n-    \/\/ (3) dst_pos must not be negative.\n-    \/\/ (4) length  must not be negative.\n-    \/\/ (5) src klass and dst klass should be the same and not NULL.\n-    \/\/ (6) src and dst should be arrays.\n-    \/\/ (7) src_pos + length must not exceed length of src.\n-    \/\/ (8) dst_pos + length must not exceed length of dst.\n-    \/\/\n+  \/\/-----------------------------------------------------------------------\n+  \/\/ Assembler stub will be used for this call to arraycopy\n+  \/\/ if the following conditions are met:\n+  \/\/\n+  \/\/ (1) src and dst must not be null.\n+  \/\/ (2) src_pos must not be negative.\n+  \/\/ (3) dst_pos must not be negative.\n+  \/\/ (4) length  must not be negative.\n+  \/\/ (5) src klass and dst klass should be the same and not NULL.\n+  \/\/ (6) src and dst should be arrays.\n+  \/\/ (7) src_pos + length must not exceed length of src.\n+  \/\/ (8) dst_pos + length must not exceed length of dst.\n+  \/\/\n@@ -3148,36 +3137,36 @@\n-    \/\/  if (src == NULL) return -1;\n-    __ testptr(src, src);         \/\/ src oop\n-    size_t j1off = __ offset();\n-    __ jccb(Assembler::zero, L_failed_0);\n-\n-    \/\/  if (src_pos < 0) return -1;\n-    __ testl(src_pos, src_pos); \/\/ src_pos (32-bits)\n-    __ jccb(Assembler::negative, L_failed_0);\n-\n-    \/\/  if (dst == NULL) return -1;\n-    __ testptr(dst, dst);         \/\/ dst oop\n-    __ jccb(Assembler::zero, L_failed_0);\n-\n-    \/\/  if (dst_pos < 0) return -1;\n-    __ testl(dst_pos, dst_pos); \/\/ dst_pos (32-bits)\n-    size_t j4off = __ offset();\n-    __ jccb(Assembler::negative, L_failed_0);\n-\n-    \/\/ The first four tests are very dense code,\n-    \/\/ but not quite dense enough to put four\n-    \/\/ jumps in a 16-byte instruction fetch buffer.\n-    \/\/ That's good, because some branch predicters\n-    \/\/ do not like jumps so close together.\n-    \/\/ Make sure of this.\n-    guarantee(((j1off ^ j4off) & ~15) != 0, \"I$ line of 1st & 4th jumps\");\n-\n-    \/\/ registers used as temp\n-    const Register r11_length    = r11; \/\/ elements count to copy\n-    const Register r10_src_klass = r10; \/\/ array klass\n-\n-    \/\/  if (length < 0) return -1;\n-    __ movl(r11_length, length);        \/\/ length (elements count, 32-bits value)\n-    __ testl(r11_length, r11_length);\n-    __ jccb(Assembler::negative, L_failed_0);\n-\n-    __ load_klass(r10_src_klass, src, rklass_tmp);\n+  \/\/  if (src == NULL) return -1;\n+  __ testptr(src, src);         \/\/ src oop\n+  size_t j1off = __ offset();\n+  __ jccb(Assembler::zero, L_failed_0);\n+\n+  \/\/  if (src_pos < 0) return -1;\n+  __ testl(src_pos, src_pos); \/\/ src_pos (32-bits)\n+  __ jccb(Assembler::negative, L_failed_0);\n+\n+  \/\/  if (dst == NULL) return -1;\n+  __ testptr(dst, dst);         \/\/ dst oop\n+  __ jccb(Assembler::zero, L_failed_0);\n+\n+  \/\/  if (dst_pos < 0) return -1;\n+  __ testl(dst_pos, dst_pos); \/\/ dst_pos (32-bits)\n+  size_t j4off = __ offset();\n+  __ jccb(Assembler::negative, L_failed_0);\n+\n+  \/\/ The first four tests are very dense code,\n+  \/\/ but not quite dense enough to put four\n+  \/\/ jumps in a 16-byte instruction fetch buffer.\n+  \/\/ That's good, because some branch predicters\n+  \/\/ do not like jumps so close together.\n+  \/\/ Make sure of this.\n+  guarantee(((j1off ^ j4off) & ~15) != 0, \"I$ line of 1st & 4th jumps\");\n+\n+  \/\/ registers used as temp\n+  const Register r11_length    = r11; \/\/ elements count to copy\n+  const Register r10_src_klass = r10; \/\/ array klass\n+\n+  \/\/  if (length < 0) return -1;\n+  __ movl(r11_length, length);        \/\/ length (elements count, 32-bits value)\n+  __ testl(r11_length, r11_length);\n+  __ jccb(Assembler::negative, L_failed_0);\n+\n+  __ load_klass(r10_src_klass, src, rklass_tmp);\n@@ -3185,14 +3174,14 @@\n-    \/\/  assert(src->klass() != NULL);\n-    {\n-      BLOCK_COMMENT(\"assert klasses not null {\");\n-      Label L1, L2;\n-      __ testptr(r10_src_klass, r10_src_klass);\n-      __ jcc(Assembler::notZero, L2);   \/\/ it is broken if klass is NULL\n-      __ bind(L1);\n-      __ stop(\"broken null klass\");\n-      __ bind(L2);\n-      __ load_klass(rax, dst, rklass_tmp);\n-      __ cmpq(rax, 0);\n-      __ jcc(Assembler::equal, L1);     \/\/ this would be broken also\n-      BLOCK_COMMENT(\"} assert klasses not null done\");\n-    }\n+  \/\/  assert(src->klass() != NULL);\n+  {\n+    BLOCK_COMMENT(\"assert klasses not null {\");\n+    Label L1, L2;\n+    __ testptr(r10_src_klass, r10_src_klass);\n+    __ jcc(Assembler::notZero, L2);   \/\/ it is broken if klass is NULL\n+    __ bind(L1);\n+    __ stop(\"broken null klass\");\n+    __ bind(L2);\n+    __ load_klass(rax, dst, rklass_tmp);\n+    __ cmpq(rax, 0);\n+    __ jcc(Assembler::equal, L1);     \/\/ this would be broken also\n+    BLOCK_COMMENT(\"} assert klasses not null done\");\n+  }\n@@ -3201,7 +3190,7 @@\n-    \/\/ Load layout helper (32-bits)\n-    \/\/\n-    \/\/  |array_tag|     | header_size | element_type |     |log2_element_size|\n-    \/\/ 32        30    24            16              8     2                 0\n-    \/\/\n-    \/\/   array_tag: typeArray = 0x3, objArray = 0x2, non-array = 0x0\n-    \/\/\n+  \/\/ Load layout helper (32-bits)\n+  \/\/\n+  \/\/  |array_tag|     | header_size | element_type |     |log2_element_size|\n+  \/\/ 32        30    24            16              8     2                 0\n+  \/\/\n+  \/\/   array_tag: typeArray = 0x3, objArray = 0x2, non-array = 0x0\n+  \/\/\n@@ -3209,1 +3198,1 @@\n-    const int lh_offset = in_bytes(Klass::layout_helper_offset());\n+  const int lh_offset = in_bytes(Klass::layout_helper_offset());\n@@ -3211,4 +3200,4 @@\n-    \/\/ Handle objArrays completely differently...\n-    const jint objArray_lh = Klass::array_layout_helper(T_OBJECT);\n-    __ cmpl(Address(r10_src_klass, lh_offset), objArray_lh);\n-    __ jcc(Assembler::equal, L_objArray);\n+  \/\/ Handle objArrays completely differently...\n+  const jint objArray_lh = Klass::array_layout_helper(T_OBJECT);\n+  __ cmpl(Address(r10_src_klass, lh_offset), objArray_lh);\n+  __ jcc(Assembler::equal, L_objArray);\n@@ -3216,4 +3205,4 @@\n-    \/\/  if (src->klass() != dst->klass()) return -1;\n-    __ load_klass(rax, dst, rklass_tmp);\n-    __ cmpq(r10_src_klass, rax);\n-    __ jcc(Assembler::notEqual, L_failed);\n+  \/\/  if (src->klass() != dst->klass()) return -1;\n+  __ load_klass(rax, dst, rklass_tmp);\n+  __ cmpq(r10_src_klass, rax);\n+  __ jcc(Assembler::notEqual, L_failed);\n@@ -3221,2 +3210,2 @@\n-    const Register rax_lh = rax;  \/\/ layout helper\n-    __ movl(rax_lh, Address(r10_src_klass, lh_offset));\n+  const Register rax_lh = rax;  \/\/ layout helper\n+  __ movl(rax_lh, Address(r10_src_klass, lh_offset));\n@@ -3224,3 +3213,3 @@\n-    \/\/  if (!src->is_Array()) return -1;\n-    __ cmpl(rax_lh, Klass::_lh_neutral_value);\n-    __ jcc(Assembler::greaterEqual, L_failed);\n+  \/\/  if (!src->is_Array()) return -1;\n+  __ cmpl(rax_lh, Klass::_lh_neutral_value);\n+  __ jcc(Assembler::greaterEqual, L_failed);\n@@ -3228,1 +3217,1 @@\n-    \/\/ At this point, it is known to be a typeArray (array_tag 0x3).\n+  \/\/ At this point, it is known to be a typeArray (array_tag 0x3).\n@@ -3230,9 +3219,9 @@\n-    {\n-      BLOCK_COMMENT(\"assert primitive array {\");\n-      Label L;\n-      __ cmpl(rax_lh, (Klass::_lh_array_tag_type_value << Klass::_lh_array_tag_shift));\n-      __ jcc(Assembler::greaterEqual, L);\n-      __ stop(\"must be a primitive array\");\n-      __ bind(L);\n-      BLOCK_COMMENT(\"} assert primitive array done\");\n-    }\n+  {\n+    BLOCK_COMMENT(\"assert primitive array {\");\n+    Label L;\n+    __ cmpl(rax_lh, (Klass::_lh_array_tag_type_value << Klass::_lh_array_tag_shift));\n+    __ jcc(Assembler::greaterEqual, L);\n+    __ stop(\"must be a primitive array\");\n+    __ bind(L);\n+    BLOCK_COMMENT(\"} assert primitive array done\");\n+  }\n@@ -3241,2 +3230,2 @@\n-    arraycopy_range_checks(src, src_pos, dst, dst_pos, r11_length,\n-                           r10, L_failed);\n+  arraycopy_range_checks(src, src_pos, dst, dst_pos, r11_length,\n+                         r10, L_failed);\n@@ -3244,5 +3233,5 @@\n-    \/\/ TypeArrayKlass\n-    \/\/\n-    \/\/ src_addr = (src + array_header_in_bytes()) + (src_pos << log2elemsize);\n-    \/\/ dst_addr = (dst + array_header_in_bytes()) + (dst_pos << log2elemsize);\n-    \/\/\n+  \/\/ TypeArrayKlass\n+  \/\/\n+  \/\/ src_addr = (src + array_header_in_bytes()) + (src_pos << log2elemsize);\n+  \/\/ dst_addr = (dst + array_header_in_bytes()) + (dst_pos << log2elemsize);\n+  \/\/\n@@ -3250,2 +3239,2 @@\n-    const Register r10_offset = r10;    \/\/ array offset\n-    const Register rax_elsize = rax_lh; \/\/ element size\n+  const Register r10_offset = r10;    \/\/ array offset\n+  const Register rax_elsize = rax_lh; \/\/ element size\n@@ -3253,7 +3242,7 @@\n-    __ movl(r10_offset, rax_lh);\n-    __ shrl(r10_offset, Klass::_lh_header_size_shift);\n-    __ andptr(r10_offset, Klass::_lh_header_size_mask);   \/\/ array_offset\n-    __ addptr(src, r10_offset);           \/\/ src array offset\n-    __ addptr(dst, r10_offset);           \/\/ dst array offset\n-    BLOCK_COMMENT(\"choose copy loop based on element size\");\n-    __ andl(rax_lh, Klass::_lh_log2_element_size_mask); \/\/ rax_lh -> rax_elsize\n+  __ movl(r10_offset, rax_lh);\n+  __ shrl(r10_offset, Klass::_lh_header_size_shift);\n+  __ andptr(r10_offset, Klass::_lh_header_size_mask);   \/\/ array_offset\n+  __ addptr(src, r10_offset);           \/\/ src array offset\n+  __ addptr(dst, r10_offset);           \/\/ dst array offset\n+  BLOCK_COMMENT(\"choose copy loop based on element size\");\n+  __ andl(rax_lh, Klass::_lh_log2_element_size_mask); \/\/ rax_lh -> rax_elsize\n@@ -3262,1 +3251,1 @@\n-    __ pop(rklass_tmp); \/\/ Restore callee-save rdi\n+  __ pop(rklass_tmp); \/\/ Restore callee-save rdi\n@@ -3265,32 +3254,32 @@\n-    \/\/ next registers should be set before the jump to corresponding stub\n-    const Register from     = c_rarg0;  \/\/ source array address\n-    const Register to       = c_rarg1;  \/\/ destination array address\n-    const Register count    = c_rarg2;  \/\/ elements count\n-\n-    \/\/ 'from', 'to', 'count' registers should be set in such order\n-    \/\/ since they are the same as 'src', 'src_pos', 'dst'.\n-\n-    __ cmpl(rax_elsize, 0);\n-    __ jccb(Assembler::notEqual, L_copy_shorts);\n-    __ lea(from, Address(src, src_pos, Address::times_1, 0));\/\/ src_addr\n-    __ lea(to,   Address(dst, dst_pos, Address::times_1, 0));\/\/ dst_addr\n-    __ movl2ptr(count, r11_length); \/\/ length\n-    __ jump(RuntimeAddress(byte_copy_entry));\n-\n-  __ BIND(L_copy_shorts);\n-    __ cmpl(rax_elsize, LogBytesPerShort);\n-    __ jccb(Assembler::notEqual, L_copy_ints);\n-    __ lea(from, Address(src, src_pos, Address::times_2, 0));\/\/ src_addr\n-    __ lea(to,   Address(dst, dst_pos, Address::times_2, 0));\/\/ dst_addr\n-    __ movl2ptr(count, r11_length); \/\/ length\n-    __ jump(RuntimeAddress(short_copy_entry));\n-\n-  __ BIND(L_copy_ints);\n-    __ cmpl(rax_elsize, LogBytesPerInt);\n-    __ jccb(Assembler::notEqual, L_copy_longs);\n-    __ lea(from, Address(src, src_pos, Address::times_4, 0));\/\/ src_addr\n-    __ lea(to,   Address(dst, dst_pos, Address::times_4, 0));\/\/ dst_addr\n-    __ movl2ptr(count, r11_length); \/\/ length\n-    __ jump(RuntimeAddress(int_copy_entry));\n-\n-  __ BIND(L_copy_longs);\n+  \/\/ next registers should be set before the jump to corresponding stub\n+  const Register from     = c_rarg0;  \/\/ source array address\n+  const Register to       = c_rarg1;  \/\/ destination array address\n+  const Register count    = c_rarg2;  \/\/ elements count\n+\n+  \/\/ 'from', 'to', 'count' registers should be set in such order\n+  \/\/ since they are the same as 'src', 'src_pos', 'dst'.\n+\n+  __ cmpl(rax_elsize, 0);\n+  __ jccb(Assembler::notEqual, L_copy_shorts);\n+  __ lea(from, Address(src, src_pos, Address::times_1, 0));\/\/ src_addr\n+  __ lea(to,   Address(dst, dst_pos, Address::times_1, 0));\/\/ dst_addr\n+  __ movl2ptr(count, r11_length); \/\/ length\n+  __ jump(RuntimeAddress(byte_copy_entry));\n+\n+__ BIND(L_copy_shorts);\n+  __ cmpl(rax_elsize, LogBytesPerShort);\n+  __ jccb(Assembler::notEqual, L_copy_ints);\n+  __ lea(from, Address(src, src_pos, Address::times_2, 0));\/\/ src_addr\n+  __ lea(to,   Address(dst, dst_pos, Address::times_2, 0));\/\/ dst_addr\n+  __ movl2ptr(count, r11_length); \/\/ length\n+  __ jump(RuntimeAddress(short_copy_entry));\n+\n+__ BIND(L_copy_ints);\n+  __ cmpl(rax_elsize, LogBytesPerInt);\n+  __ jccb(Assembler::notEqual, L_copy_longs);\n+  __ lea(from, Address(src, src_pos, Address::times_4, 0));\/\/ src_addr\n+  __ lea(to,   Address(dst, dst_pos, Address::times_4, 0));\/\/ dst_addr\n+  __ movl2ptr(count, r11_length); \/\/ length\n+  __ jump(RuntimeAddress(int_copy_entry));\n+\n+__ BIND(L_copy_longs);\n@@ -3298,9 +3287,9 @@\n-    {\n-      BLOCK_COMMENT(\"assert long copy {\");\n-      Label L;\n-      __ cmpl(rax_elsize, LogBytesPerLong);\n-      __ jcc(Assembler::equal, L);\n-      __ stop(\"must be long copy, but elsize is wrong\");\n-      __ bind(L);\n-      BLOCK_COMMENT(\"} assert long copy done\");\n-    }\n+  {\n+    BLOCK_COMMENT(\"assert long copy {\");\n+    Label L;\n+    __ cmpl(rax_elsize, LogBytesPerLong);\n+    __ jcc(Assembler::equal, L);\n+    __ stop(\"must be long copy, but elsize is wrong\");\n+    __ bind(L);\n+    BLOCK_COMMENT(\"} assert long copy done\");\n+  }\n@@ -3308,8 +3297,29 @@\n-    __ lea(from, Address(src, src_pos, Address::times_8, 0));\/\/ src_addr\n-    __ lea(to,   Address(dst, dst_pos, Address::times_8, 0));\/\/ dst_addr\n-    __ movl2ptr(count, r11_length); \/\/ length\n-    __ jump(RuntimeAddress(long_copy_entry));\n-\n-    \/\/ ObjArrayKlass\n-  __ BIND(L_objArray);\n-    \/\/ live at this point:  r10_src_klass, r11_length, src[_pos], dst[_pos]\n+  __ lea(from, Address(src, src_pos, Address::times_8, 0));\/\/ src_addr\n+  __ lea(to,   Address(dst, dst_pos, Address::times_8, 0));\/\/ dst_addr\n+  __ movl2ptr(count, r11_length); \/\/ length\n+  __ jump(RuntimeAddress(long_copy_entry));\n+\n+  \/\/ ObjArrayKlass\n+__ BIND(L_objArray);\n+  \/\/ live at this point:  r10_src_klass, r11_length, src[_pos], dst[_pos]\n+\n+  Label L_plain_copy, L_checkcast_copy;\n+  \/\/  test array classes for subtyping\n+  __ load_klass(rax, dst, rklass_tmp);\n+  __ cmpq(r10_src_klass, rax); \/\/ usual case is exact equality\n+  __ jcc(Assembler::notEqual, L_checkcast_copy);\n+\n+  \/\/ Identically typed arrays can be copied without element-wise checks.\n+  arraycopy_range_checks(src, src_pos, dst, dst_pos, r11_length,\n+                         r10, L_failed);\n+\n+  __ lea(from, Address(src, src_pos, TIMES_OOP,\n+               arrayOopDesc::base_offset_in_bytes(T_OBJECT))); \/\/ src_addr\n+  __ lea(to,   Address(dst, dst_pos, TIMES_OOP,\n+               arrayOopDesc::base_offset_in_bytes(T_OBJECT))); \/\/ dst_addr\n+  __ movl2ptr(count, r11_length); \/\/ length\n+__ BIND(L_plain_copy);\n+#ifdef _WIN64\n+  __ pop(rklass_tmp); \/\/ Restore callee-save rdi\n+#endif\n+  __ jump(RuntimeAddress(oop_copy_entry));\n@@ -3317,5 +3327,6 @@\n-    Label L_plain_copy, L_checkcast_copy;\n-    \/\/  test array classes for subtyping\n-    __ load_klass(rax, dst, rklass_tmp);\n-    __ cmpq(r10_src_klass, rax); \/\/ usual case is exact equality\n-    __ jcc(Assembler::notEqual, L_checkcast_copy);\n+__ BIND(L_checkcast_copy);\n+  \/\/ live at this point:  r10_src_klass, r11_length, rax (dst_klass)\n+  {\n+    \/\/ Before looking at dst.length, make sure dst is also an objArray.\n+    __ cmpl(Address(rax, lh_offset), objArray_lh);\n+    __ jcc(Assembler::notEqual, L_failed);\n@@ -3323,1 +3334,1 @@\n-    \/\/ Identically typed arrays can be copied without element-wise checks.\n+    \/\/ It is safe to examine both src.length and dst.length.\n@@ -3325,1 +3336,4 @@\n-                           r10, L_failed);\n+                           rax, L_failed);\n+\n+    const Register r11_dst_klass = r11;\n+    __ load_klass(r11_dst_klass, dst, rklass_tmp); \/\/ reload\n@@ -3327,0 +3341,1 @@\n+    \/\/ Marshal the base address arguments now, freeing registers.\n@@ -3328,1 +3343,1 @@\n-                 arrayOopDesc::base_offset_in_bytes(T_OBJECT))); \/\/ src_addr\n+                 arrayOopDesc::base_offset_in_bytes(T_OBJECT)));\n@@ -3330,44 +3345,18 @@\n-                 arrayOopDesc::base_offset_in_bytes(T_OBJECT))); \/\/ dst_addr\n-    __ movl2ptr(count, r11_length); \/\/ length\n-  __ BIND(L_plain_copy);\n-#ifdef _WIN64\n-    __ pop(rklass_tmp); \/\/ Restore callee-save rdi\n-#endif\n-    __ jump(RuntimeAddress(oop_copy_entry));\n-\n-  __ BIND(L_checkcast_copy);\n-    \/\/ live at this point:  r10_src_klass, r11_length, rax (dst_klass)\n-    {\n-      \/\/ Before looking at dst.length, make sure dst is also an objArray.\n-      __ cmpl(Address(rax, lh_offset), objArray_lh);\n-      __ jcc(Assembler::notEqual, L_failed);\n-\n-      \/\/ It is safe to examine both src.length and dst.length.\n-      arraycopy_range_checks(src, src_pos, dst, dst_pos, r11_length,\n-                             rax, L_failed);\n-\n-      const Register r11_dst_klass = r11;\n-      __ load_klass(r11_dst_klass, dst, rklass_tmp); \/\/ reload\n-\n-      \/\/ Marshal the base address arguments now, freeing registers.\n-      __ lea(from, Address(src, src_pos, TIMES_OOP,\n-                   arrayOopDesc::base_offset_in_bytes(T_OBJECT)));\n-      __ lea(to,   Address(dst, dst_pos, TIMES_OOP,\n-                   arrayOopDesc::base_offset_in_bytes(T_OBJECT)));\n-      __ movl(count, length);           \/\/ length (reloaded)\n-      Register sco_temp = c_rarg3;      \/\/ this register is free now\n-      assert_different_registers(from, to, count, sco_temp,\n-                                 r11_dst_klass, r10_src_klass);\n-      assert_clean_int(count, sco_temp);\n-\n-      \/\/ Generate the type check.\n-      const int sco_offset = in_bytes(Klass::super_check_offset_offset());\n-      __ movl(sco_temp, Address(r11_dst_klass, sco_offset));\n-      assert_clean_int(sco_temp, rax);\n-      generate_type_check(r10_src_klass, sco_temp, r11_dst_klass, L_plain_copy);\n-\n-      \/\/ Fetch destination element klass from the ObjArrayKlass header.\n-      int ek_offset = in_bytes(ObjArrayKlass::element_klass_offset());\n-      __ movptr(r11_dst_klass, Address(r11_dst_klass, ek_offset));\n-      __ movl(  sco_temp,      Address(r11_dst_klass, sco_offset));\n-      assert_clean_int(sco_temp, rax);\n+                 arrayOopDesc::base_offset_in_bytes(T_OBJECT)));\n+    __ movl(count, length);           \/\/ length (reloaded)\n+    Register sco_temp = c_rarg3;      \/\/ this register is free now\n+    assert_different_registers(from, to, count, sco_temp,\n+                               r11_dst_klass, r10_src_klass);\n+    assert_clean_int(count, sco_temp);\n+\n+    \/\/ Generate the type check.\n+    const int sco_offset = in_bytes(Klass::super_check_offset_offset());\n+    __ movl(sco_temp, Address(r11_dst_klass, sco_offset));\n+    assert_clean_int(sco_temp, rax);\n+    generate_type_check(r10_src_klass, sco_temp, r11_dst_klass, L_plain_copy);\n+\n+    \/\/ Fetch destination element klass from the ObjArrayKlass header.\n+    int ek_offset = in_bytes(ObjArrayKlass::element_klass_offset());\n+    __ movptr(r11_dst_klass, Address(r11_dst_klass, ek_offset));\n+    __ movl(  sco_temp,      Address(r11_dst_klass, sco_offset));\n+    assert_clean_int(sco_temp, rax);\n@@ -3376,1 +3365,1 @@\n-      __ pop(rklass_tmp); \/\/ Restore callee-save rdi\n+    __ pop(rklass_tmp); \/\/ Restore callee-save rdi\n@@ -3379,7 +3368,7 @@\n-      \/\/ the checkcast_copy loop needs two extra arguments:\n-      assert(c_rarg3 == sco_temp, \"#3 already in place\");\n-      \/\/ Set up arguments for checkcast_copy_entry.\n-      setup_arg_regs(4);\n-      __ movptr(r8, r11_dst_klass);  \/\/ dst.klass.element_klass, r8 is c_rarg4 on Linux\/Solaris\n-      __ jump(RuntimeAddress(checkcast_copy_entry));\n-    }\n+    \/\/ the checkcast_copy loop needs two extra arguments:\n+    assert(c_rarg3 == sco_temp, \"#3 already in place\");\n+    \/\/ Set up arguments for checkcast_copy_entry.\n+    setup_arg_regs(4);\n+    __ movptr(r8, r11_dst_klass);  \/\/ dst.klass.element_klass, r8 is c_rarg4 on Linux\/Solaris\n+    __ jump(RuntimeAddress(checkcast_copy_entry));\n+  }\n@@ -3387,1 +3376,1 @@\n-  __ BIND(L_failed);\n+__ BIND(L_failed);\n@@ -3389,1 +3378,1 @@\n-    __ pop(rklass_tmp); \/\/ Restore callee-save rdi\n+  __ pop(rklass_tmp); \/\/ Restore callee-save rdi\n@@ -3391,4 +3380,4 @@\n-    __ xorptr(rax, rax);\n-    __ notptr(rax); \/\/ return -1\n-    __ leave();   \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n+  __ xorptr(rax, rax);\n+  __ notptr(rax); \/\/ return -1\n+  __ leave();   \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n@@ -3396,2 +3385,2 @@\n-    return start;\n-  }\n+  return start;\n+}\n@@ -3399,2 +3388,2 @@\n-  address generate_data_cache_writeback() {\n-    const Register src        = c_rarg0;  \/\/ source address\n+address StubGenerator::generate_data_cache_writeback() {\n+  const Register src        = c_rarg0;  \/\/ source address\n@@ -3402,1 +3391,1 @@\n-    __ align(CodeEntryAlignment);\n+  __ align(CodeEntryAlignment);\n@@ -3404,1 +3393,1 @@\n-    StubCodeMark mark(this, \"StubRoutines\", \"_data_cache_writeback\");\n+  StubCodeMark mark(this, \"StubRoutines\", \"_data_cache_writeback\");\n@@ -3406,5 +3395,1 @@\n-    address start = __ pc();\n-    __ enter();\n-    __ cache_wb(Address(src, 0));\n-    __ leave();\n-    __ ret(0);\n+  address start = __ pc();\n@@ -3412,2 +3397,4 @@\n-    return start;\n-  }\n+  __ enter();\n+  __ cache_wb(Address(src, 0));\n+  __ leave();\n+  __ ret(0);\n@@ -3415,2 +3402,2 @@\n-  address generate_data_cache_writeback_sync() {\n-    const Register is_pre    = c_rarg0;  \/\/ pre or post sync\n+  return start;\n+}\n@@ -3418,1 +3405,2 @@\n-    __ align(CodeEntryAlignment);\n+address StubGenerator::generate_data_cache_writeback_sync() {\n+  const Register is_pre    = c_rarg0;  \/\/ pre or post sync\n@@ -3420,1 +3408,1 @@\n-    StubCodeMark mark(this, \"StubRoutines\", \"_data_cache_writeback_sync\");\n+  __ align(CodeEntryAlignment);\n@@ -3422,2 +3410,1 @@\n-    \/\/ pre wbsync is a no-op\n-    \/\/ post wbsync translates to an sfence\n+  StubCodeMark mark(this, \"StubRoutines\", \"_data_cache_writeback_sync\");\n@@ -3425,9 +3412,2 @@\n-    Label skip;\n-    address start = __ pc();\n-    __ enter();\n-    __ cmpl(is_pre, 0);\n-    __ jcc(Assembler::notEqual, skip);\n-    __ cache_wbsync(false);\n-    __ bind(skip);\n-    __ leave();\n-    __ ret(0);\n+  \/\/ pre wbsync is a no-op\n+  \/\/ post wbsync translates to an sfence\n@@ -3435,56 +3415,2 @@\n-    return start;\n-  }\n-\n-  void generate_arraycopy_stubs() {\n-    address entry;\n-    address entry_jbyte_arraycopy;\n-    address entry_jshort_arraycopy;\n-    address entry_jint_arraycopy;\n-    address entry_oop_arraycopy;\n-    address entry_jlong_arraycopy;\n-    address entry_checkcast_arraycopy;\n-\n-    StubRoutines::_jbyte_disjoint_arraycopy  = generate_disjoint_byte_copy(false, &entry,\n-                                                                           \"jbyte_disjoint_arraycopy\");\n-    StubRoutines::_jbyte_arraycopy           = generate_conjoint_byte_copy(false, entry, &entry_jbyte_arraycopy,\n-                                                                           \"jbyte_arraycopy\");\n-\n-    StubRoutines::_jshort_disjoint_arraycopy = generate_disjoint_short_copy(false, &entry,\n-                                                                            \"jshort_disjoint_arraycopy\");\n-    StubRoutines::_jshort_arraycopy          = generate_conjoint_short_copy(false, entry, &entry_jshort_arraycopy,\n-                                                                            \"jshort_arraycopy\");\n-\n-    StubRoutines::_jint_disjoint_arraycopy   = generate_disjoint_int_oop_copy(false, false, &entry,\n-                                                                              \"jint_disjoint_arraycopy\");\n-    StubRoutines::_jint_arraycopy            = generate_conjoint_int_oop_copy(false, false, entry,\n-                                                                              &entry_jint_arraycopy, \"jint_arraycopy\");\n-\n-    StubRoutines::_jlong_disjoint_arraycopy  = generate_disjoint_long_oop_copy(false, false, &entry,\n-                                                                               \"jlong_disjoint_arraycopy\");\n-    StubRoutines::_jlong_arraycopy           = generate_conjoint_long_oop_copy(false, false, entry,\n-                                                                               &entry_jlong_arraycopy, \"jlong_arraycopy\");\n-\n-\n-    if (UseCompressedOops) {\n-      StubRoutines::_oop_disjoint_arraycopy  = generate_disjoint_int_oop_copy(false, true, &entry,\n-                                                                              \"oop_disjoint_arraycopy\");\n-      StubRoutines::_oop_arraycopy           = generate_conjoint_int_oop_copy(false, true, entry,\n-                                                                              &entry_oop_arraycopy, \"oop_arraycopy\");\n-      StubRoutines::_oop_disjoint_arraycopy_uninit  = generate_disjoint_int_oop_copy(false, true, &entry,\n-                                                                                     \"oop_disjoint_arraycopy_uninit\",\n-                                                                                     \/*dest_uninitialized*\/true);\n-      StubRoutines::_oop_arraycopy_uninit           = generate_conjoint_int_oop_copy(false, true, entry,\n-                                                                                     NULL, \"oop_arraycopy_uninit\",\n-                                                                                     \/*dest_uninitialized*\/true);\n-    } else {\n-      StubRoutines::_oop_disjoint_arraycopy  = generate_disjoint_long_oop_copy(false, true, &entry,\n-                                                                               \"oop_disjoint_arraycopy\");\n-      StubRoutines::_oop_arraycopy           = generate_conjoint_long_oop_copy(false, true, entry,\n-                                                                               &entry_oop_arraycopy, \"oop_arraycopy\");\n-      StubRoutines::_oop_disjoint_arraycopy_uninit  = generate_disjoint_long_oop_copy(false, true, &entry,\n-                                                                                      \"oop_disjoint_arraycopy_uninit\",\n-                                                                                      \/*dest_uninitialized*\/true);\n-      StubRoutines::_oop_arraycopy_uninit           = generate_conjoint_long_oop_copy(false, true, entry,\n-                                                                                      NULL, \"oop_arraycopy_uninit\",\n-                                                                                      \/*dest_uninitialized*\/true);\n-    }\n+  Label skip;\n+  address start = __ pc();\n@@ -3492,65 +3418,7 @@\n-    StubRoutines::_checkcast_arraycopy        = generate_checkcast_copy(\"checkcast_arraycopy\", &entry_checkcast_arraycopy);\n-    StubRoutines::_checkcast_arraycopy_uninit = generate_checkcast_copy(\"checkcast_arraycopy_uninit\", NULL,\n-                                                                        \/*dest_uninitialized*\/true);\n-\n-    StubRoutines::_unsafe_arraycopy    = generate_unsafe_copy(\"unsafe_arraycopy\",\n-                                                              entry_jbyte_arraycopy,\n-                                                              entry_jshort_arraycopy,\n-                                                              entry_jint_arraycopy,\n-                                                              entry_jlong_arraycopy);\n-    StubRoutines::_generic_arraycopy   = generate_generic_copy(\"generic_arraycopy\",\n-                                                               entry_jbyte_arraycopy,\n-                                                               entry_jshort_arraycopy,\n-                                                               entry_jint_arraycopy,\n-                                                               entry_oop_arraycopy,\n-                                                               entry_jlong_arraycopy,\n-                                                               entry_checkcast_arraycopy);\n-\n-    StubRoutines::_jbyte_fill = generate_fill(T_BYTE, false, \"jbyte_fill\");\n-    StubRoutines::_jshort_fill = generate_fill(T_SHORT, false, \"jshort_fill\");\n-    StubRoutines::_jint_fill = generate_fill(T_INT, false, \"jint_fill\");\n-    StubRoutines::_arrayof_jbyte_fill = generate_fill(T_BYTE, true, \"arrayof_jbyte_fill\");\n-    StubRoutines::_arrayof_jshort_fill = generate_fill(T_SHORT, true, \"arrayof_jshort_fill\");\n-    StubRoutines::_arrayof_jint_fill = generate_fill(T_INT, true, \"arrayof_jint_fill\");\n-\n-    \/\/ We don't generate specialized code for HeapWord-aligned source\n-    \/\/ arrays, so just use the code we've already generated\n-    StubRoutines::_arrayof_jbyte_disjoint_arraycopy  = StubRoutines::_jbyte_disjoint_arraycopy;\n-    StubRoutines::_arrayof_jbyte_arraycopy           = StubRoutines::_jbyte_arraycopy;\n-\n-    StubRoutines::_arrayof_jshort_disjoint_arraycopy = StubRoutines::_jshort_disjoint_arraycopy;\n-    StubRoutines::_arrayof_jshort_arraycopy          = StubRoutines::_jshort_arraycopy;\n-\n-    StubRoutines::_arrayof_jint_disjoint_arraycopy   = StubRoutines::_jint_disjoint_arraycopy;\n-    StubRoutines::_arrayof_jint_arraycopy            = StubRoutines::_jint_arraycopy;\n-\n-    StubRoutines::_arrayof_jlong_disjoint_arraycopy  = StubRoutines::_jlong_disjoint_arraycopy;\n-    StubRoutines::_arrayof_jlong_arraycopy           = StubRoutines::_jlong_arraycopy;\n-\n-    StubRoutines::_arrayof_oop_disjoint_arraycopy    = StubRoutines::_oop_disjoint_arraycopy;\n-    StubRoutines::_arrayof_oop_arraycopy             = StubRoutines::_oop_arraycopy;\n-\n-    StubRoutines::_arrayof_oop_disjoint_arraycopy_uninit    = StubRoutines::_oop_disjoint_arraycopy_uninit;\n-    StubRoutines::_arrayof_oop_arraycopy_uninit             = StubRoutines::_oop_arraycopy_uninit;\n-  }\n-\n-  \/\/ AES intrinsic stubs\n-  enum {AESBlockSize = 16};\n-\n-  address generate_key_shuffle_mask() {\n-    __ align(16);\n-    StubCodeMark mark(this, \"StubRoutines\", \"key_shuffle_mask\");\n-    address start = __ pc();\n-    __ emit_data64( 0x0405060700010203, relocInfo::none );\n-    __ emit_data64( 0x0c0d0e0f08090a0b, relocInfo::none );\n-    return start;\n-  }\n-\n-  address generate_counter_shuffle_mask() {\n-    __ align(16);\n-    StubCodeMark mark(this, \"StubRoutines\", \"counter_shuffle_mask\");\n-    address start = __ pc();\n-    __ emit_data64(0x08090a0b0c0d0e0f, relocInfo::none);\n-    __ emit_data64(0x0001020304050607, relocInfo::none);\n-    return start;\n-  }\n+  __ enter();\n+  __ cmpl(is_pre, 0);\n+  __ jcc(Assembler::notEqual, skip);\n+  __ cache_wbsync(false);\n+  __ bind(skip);\n+  __ leave();\n+  __ ret(0);\n@@ -3558,10 +3426,2 @@\n-  \/\/ Utility routine for loading a 128-bit key word in little endian format\n-  \/\/ can optionally specify that the shuffle mask is already in an xmmregister\n-  void load_key(XMMRegister xmmdst, Register key, int offset, XMMRegister xmm_shuf_mask = xnoreg) {\n-    __ movdqu(xmmdst, Address(key, offset));\n-    if (xmm_shuf_mask != xnoreg) {\n-      __ pshufb(xmmdst, xmm_shuf_mask);\n-    } else {\n-      __ pshufb(xmmdst, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()));\n-    }\n-  }\n+  return start;\n+}\n@@ -3569,10 +3429,50 @@\n-  \/\/ Utility routine for increase 128bit counter (iv in CTR mode)\n-  void inc_counter(Register reg, XMMRegister xmmdst, int inc_delta, Label& next_block) {\n-    __ pextrq(reg, xmmdst, 0x0);\n-    __ addq(reg, inc_delta);\n-    __ pinsrq(xmmdst, reg, 0x0);\n-    __ jcc(Assembler::carryClear, next_block); \/\/ jump if no carry\n-    __ pextrq(reg, xmmdst, 0x01); \/\/ Carry\n-    __ addq(reg, 0x01);\n-    __ pinsrq(xmmdst, reg, 0x01); \/\/Carry end\n-    __ BIND(next_block);          \/\/ next instruction\n+void StubGenerator::generate_arraycopy_stubs() {\n+  address entry;\n+  address entry_jbyte_arraycopy;\n+  address entry_jshort_arraycopy;\n+  address entry_jint_arraycopy;\n+  address entry_oop_arraycopy;\n+  address entry_jlong_arraycopy;\n+  address entry_checkcast_arraycopy;\n+\n+  StubRoutines::_jbyte_disjoint_arraycopy  = generate_disjoint_byte_copy(false, &entry,\n+                                                                         \"jbyte_disjoint_arraycopy\");\n+  StubRoutines::_jbyte_arraycopy           = generate_conjoint_byte_copy(false, entry, &entry_jbyte_arraycopy,\n+                                                                         \"jbyte_arraycopy\");\n+\n+  StubRoutines::_jshort_disjoint_arraycopy = generate_disjoint_short_copy(false, &entry,\n+                                                                          \"jshort_disjoint_arraycopy\");\n+  StubRoutines::_jshort_arraycopy          = generate_conjoint_short_copy(false, entry, &entry_jshort_arraycopy,\n+                                                                          \"jshort_arraycopy\");\n+\n+  StubRoutines::_jint_disjoint_arraycopy   = generate_disjoint_int_oop_copy(false, false, &entry,\n+                                                                            \"jint_disjoint_arraycopy\");\n+  StubRoutines::_jint_arraycopy            = generate_conjoint_int_oop_copy(false, false, entry,\n+                                                                            &entry_jint_arraycopy, \"jint_arraycopy\");\n+\n+  StubRoutines::_jlong_disjoint_arraycopy  = generate_disjoint_long_oop_copy(false, false, &entry,\n+                                                                             \"jlong_disjoint_arraycopy\");\n+  StubRoutines::_jlong_arraycopy           = generate_conjoint_long_oop_copy(false, false, entry,\n+                                                                             &entry_jlong_arraycopy, \"jlong_arraycopy\");\n+  if (UseCompressedOops) {\n+    StubRoutines::_oop_disjoint_arraycopy  = generate_disjoint_int_oop_copy(false, true, &entry,\n+                                                                            \"oop_disjoint_arraycopy\");\n+    StubRoutines::_oop_arraycopy           = generate_conjoint_int_oop_copy(false, true, entry,\n+                                                                            &entry_oop_arraycopy, \"oop_arraycopy\");\n+    StubRoutines::_oop_disjoint_arraycopy_uninit  = generate_disjoint_int_oop_copy(false, true, &entry,\n+                                                                                   \"oop_disjoint_arraycopy_uninit\",\n+                                                                                   \/*dest_uninitialized*\/true);\n+    StubRoutines::_oop_arraycopy_uninit           = generate_conjoint_int_oop_copy(false, true, entry,\n+                                                                                   NULL, \"oop_arraycopy_uninit\",\n+                                                                                   \/*dest_uninitialized*\/true);\n+  } else {\n+    StubRoutines::_oop_disjoint_arraycopy  = generate_disjoint_long_oop_copy(false, true, &entry,\n+                                                                             \"oop_disjoint_arraycopy\");\n+    StubRoutines::_oop_arraycopy           = generate_conjoint_long_oop_copy(false, true, entry,\n+                                                                             &entry_oop_arraycopy, \"oop_arraycopy\");\n+    StubRoutines::_oop_disjoint_arraycopy_uninit  = generate_disjoint_long_oop_copy(false, true, &entry,\n+                                                                                    \"oop_disjoint_arraycopy_uninit\",\n+                                                                                    \/*dest_uninitialized*\/true);\n+    StubRoutines::_oop_arraycopy_uninit           = generate_conjoint_long_oop_copy(false, true, entry,\n+                                                                                    NULL, \"oop_arraycopy_uninit\",\n+                                                                                    \/*dest_uninitialized*\/true);\n@@ -3581,40 +3481,44 @@\n-  \/\/ Arguments:\n-  \/\/\n-  \/\/ Inputs:\n-  \/\/   c_rarg0   - source byte array address\n-  \/\/   c_rarg1   - destination byte array address\n-  \/\/   c_rarg2   - K (key) in little endian int array\n-  \/\/\n-  address generate_aescrypt_encryptBlock() {\n-    assert(UseAES, \"need AES instructions and misaligned SSE support\");\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"aescrypt_encryptBlock\");\n-    Label L_doLast;\n-    address start = __ pc();\n-\n-    const Register from        = c_rarg0;  \/\/ source array address\n-    const Register to          = c_rarg1;  \/\/ destination array address\n-    const Register key         = c_rarg2;  \/\/ key array address\n-    const Register keylen      = rax;\n-\n-    const XMMRegister xmm_result = xmm0;\n-    const XMMRegister xmm_key_shuf_mask = xmm1;\n-    \/\/ On win64 xmm6-xmm15 must be preserved so don't use them.\n-    const XMMRegister xmm_temp1  = xmm2;\n-    const XMMRegister xmm_temp2  = xmm3;\n-    const XMMRegister xmm_temp3  = xmm4;\n-    const XMMRegister xmm_temp4  = xmm5;\n-\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-\n-    \/\/ keylen could be only {11, 13, 15} * 4 = {44, 52, 60}\n-    __ movl(keylen, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));\n-\n-    __ movdqu(xmm_key_shuf_mask, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()));\n-    __ movdqu(xmm_result, Address(from, 0));  \/\/ get 16 bytes of input\n-\n-    \/\/ For encryption, the java expanded key ordering is just what we need\n-    \/\/ we don't know if the key is aligned, hence not using load-execute form\n-\n-    load_key(xmm_temp1, key, 0x00, xmm_key_shuf_mask);\n-    __ pxor(xmm_result, xmm_temp1);\n+  StubRoutines::_checkcast_arraycopy        = generate_checkcast_copy(\"checkcast_arraycopy\", &entry_checkcast_arraycopy);\n+  StubRoutines::_checkcast_arraycopy_uninit = generate_checkcast_copy(\"checkcast_arraycopy_uninit\", NULL,\n+                                                                      \/*dest_uninitialized*\/true);\n+\n+  StubRoutines::_unsafe_arraycopy    = generate_unsafe_copy(\"unsafe_arraycopy\",\n+                                                            entry_jbyte_arraycopy,\n+                                                            entry_jshort_arraycopy,\n+                                                            entry_jint_arraycopy,\n+                                                            entry_jlong_arraycopy);\n+  StubRoutines::_generic_arraycopy   = generate_generic_copy(\"generic_arraycopy\",\n+                                                             entry_jbyte_arraycopy,\n+                                                             entry_jshort_arraycopy,\n+                                                             entry_jint_arraycopy,\n+                                                             entry_oop_arraycopy,\n+                                                             entry_jlong_arraycopy,\n+                                                             entry_checkcast_arraycopy);\n+\n+  StubRoutines::_jbyte_fill = generate_fill(T_BYTE, false, \"jbyte_fill\");\n+  StubRoutines::_jshort_fill = generate_fill(T_SHORT, false, \"jshort_fill\");\n+  StubRoutines::_jint_fill = generate_fill(T_INT, false, \"jint_fill\");\n+  StubRoutines::_arrayof_jbyte_fill = generate_fill(T_BYTE, true, \"arrayof_jbyte_fill\");\n+  StubRoutines::_arrayof_jshort_fill = generate_fill(T_SHORT, true, \"arrayof_jshort_fill\");\n+  StubRoutines::_arrayof_jint_fill = generate_fill(T_INT, true, \"arrayof_jint_fill\");\n+\n+  \/\/ We don't generate specialized code for HeapWord-aligned source\n+  \/\/ arrays, so just use the code we've already generated\n+  StubRoutines::_arrayof_jbyte_disjoint_arraycopy  = StubRoutines::_jbyte_disjoint_arraycopy;\n+  StubRoutines::_arrayof_jbyte_arraycopy           = StubRoutines::_jbyte_arraycopy;\n+\n+  StubRoutines::_arrayof_jshort_disjoint_arraycopy = StubRoutines::_jshort_disjoint_arraycopy;\n+  StubRoutines::_arrayof_jshort_arraycopy          = StubRoutines::_jshort_arraycopy;\n+\n+  StubRoutines::_arrayof_jint_disjoint_arraycopy   = StubRoutines::_jint_disjoint_arraycopy;\n+  StubRoutines::_arrayof_jint_arraycopy            = StubRoutines::_jint_arraycopy;\n+\n+  StubRoutines::_arrayof_jlong_disjoint_arraycopy  = StubRoutines::_jlong_disjoint_arraycopy;\n+  StubRoutines::_arrayof_jlong_arraycopy           = StubRoutines::_jlong_arraycopy;\n+\n+  StubRoutines::_arrayof_oop_disjoint_arraycopy    = StubRoutines::_oop_disjoint_arraycopy;\n+  StubRoutines::_arrayof_oop_arraycopy             = StubRoutines::_oop_arraycopy;\n+\n+  StubRoutines::_arrayof_oop_disjoint_arraycopy_uninit    = StubRoutines::_oop_disjoint_arraycopy_uninit;\n+  StubRoutines::_arrayof_oop_arraycopy_uninit             = StubRoutines::_oop_arraycopy_uninit;\n+}\n@@ -3622,4 +3526,1 @@\n-    load_key(xmm_temp1, key, 0x10, xmm_key_shuf_mask);\n-    load_key(xmm_temp2, key, 0x20, xmm_key_shuf_mask);\n-    load_key(xmm_temp3, key, 0x30, xmm_key_shuf_mask);\n-    load_key(xmm_temp4, key, 0x40, xmm_key_shuf_mask);\n+\/\/ AES intrinsic stubs\n@@ -3627,4 +3528,4 @@\n-    __ aesenc(xmm_result, xmm_temp1);\n-    __ aesenc(xmm_result, xmm_temp2);\n-    __ aesenc(xmm_result, xmm_temp3);\n-    __ aesenc(xmm_result, xmm_temp4);\n+address StubGenerator::generate_key_shuffle_mask() {\n+  __ align(16);\n+  StubCodeMark mark(this, \"StubRoutines\", \"key_shuffle_mask\");\n+  address start = __ pc();\n@@ -3632,4 +3533,2 @@\n-    load_key(xmm_temp1, key, 0x50, xmm_key_shuf_mask);\n-    load_key(xmm_temp2, key, 0x60, xmm_key_shuf_mask);\n-    load_key(xmm_temp3, key, 0x70, xmm_key_shuf_mask);\n-    load_key(xmm_temp4, key, 0x80, xmm_key_shuf_mask);\n+  __ emit_data64( 0x0405060700010203, relocInfo::none );\n+  __ emit_data64( 0x0c0d0e0f08090a0b, relocInfo::none );\n@@ -3637,4 +3536,2 @@\n-    __ aesenc(xmm_result, xmm_temp1);\n-    __ aesenc(xmm_result, xmm_temp2);\n-    __ aesenc(xmm_result, xmm_temp3);\n-    __ aesenc(xmm_result, xmm_temp4);\n+  return start;\n+}\n@@ -3642,2 +3539,4 @@\n-    load_key(xmm_temp1, key, 0x90, xmm_key_shuf_mask);\n-    load_key(xmm_temp2, key, 0xa0, xmm_key_shuf_mask);\n+address StubGenerator::generate_counter_shuffle_mask() {\n+  __ align(16);\n+  StubCodeMark mark(this, \"StubRoutines\", \"counter_shuffle_mask\");\n+  address start = __ pc();\n@@ -3645,2 +3544,2 @@\n-    __ cmpl(keylen, 44);\n-    __ jccb(Assembler::equal, L_doLast);\n+  __ emit_data64(0x08090a0b0c0d0e0f, relocInfo::none);\n+  __ emit_data64(0x0001020304050607, relocInfo::none);\n@@ -3648,2 +3547,2 @@\n-    __ aesenc(xmm_result, xmm_temp1);\n-    __ aesenc(xmm_result, xmm_temp2);\n+  return start;\n+}\n@@ -3651,2 +3550,10 @@\n-    load_key(xmm_temp1, key, 0xb0, xmm_key_shuf_mask);\n-    load_key(xmm_temp2, key, 0xc0, xmm_key_shuf_mask);\n+\/\/ Utility routine for loading a 128-bit key word in little endian format\n+\/\/ can optionally specify that the shuffle mask is already in an xmmregister\n+void StubGenerator::load_key(XMMRegister xmmdst, Register key, int offset, XMMRegister xmm_shuf_mask) {\n+  __ movdqu(xmmdst, Address(key, offset));\n+  if (xmm_shuf_mask != xnoreg) {\n+    __ pshufb(xmmdst, xmm_shuf_mask);\n+  } else {\n+    __ pshufb(xmmdst, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()));\n+  }\n+}\n@@ -3654,2 +3561,11 @@\n-    __ cmpl(keylen, 52);\n-    __ jccb(Assembler::equal, L_doLast);\n+\/\/ Utility routine for increase 128bit counter (iv in CTR mode)\n+void StubGenerator::inc_counter(Register reg, XMMRegister xmmdst, int inc_delta, Label& next_block) {\n+  __ pextrq(reg, xmmdst, 0x0);\n+  __ addq(reg, inc_delta);\n+  __ pinsrq(xmmdst, reg, 0x0);\n+  __ jcc(Assembler::carryClear, next_block); \/\/ jump if no carry\n+  __ pextrq(reg, xmmdst, 0x01); \/\/ Carry\n+  __ addq(reg, 0x01);\n+  __ pinsrq(xmmdst, reg, 0x01); \/\/Carry end\n+  __ BIND(next_block);          \/\/ next instruction\n+}\n@@ -3657,2 +3573,13 @@\n-    __ aesenc(xmm_result, xmm_temp1);\n-    __ aesenc(xmm_result, xmm_temp2);\n+\/\/ Arguments:\n+\/\/\n+\/\/ Inputs:\n+\/\/   c_rarg0   - source byte array address\n+\/\/   c_rarg1   - destination byte array address\n+\/\/   c_rarg2   - K (key) in little endian int array\n+\/\/\n+address StubGenerator::generate_aescrypt_encryptBlock() {\n+  assert(UseAES, \"need AES instructions and misaligned SSE support\");\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"aescrypt_encryptBlock\");\n+  Label L_doLast;\n+  address start = __ pc();\n@@ -3660,2 +3587,4 @@\n-    load_key(xmm_temp1, key, 0xd0, xmm_key_shuf_mask);\n-    load_key(xmm_temp2, key, 0xe0, xmm_key_shuf_mask);\n+  const Register from        = c_rarg0;  \/\/ source array address\n+  const Register to          = c_rarg1;  \/\/ destination array address\n+  const Register key         = c_rarg2;  \/\/ key array address\n+  const Register keylen      = rax;\n@@ -3663,7 +3592,7 @@\n-    __ BIND(L_doLast);\n-    __ aesenc(xmm_result, xmm_temp1);\n-    __ aesenclast(xmm_result, xmm_temp2);\n-    __ movdqu(Address(to, 0), xmm_result);        \/\/ store the result\n-    __ xorptr(rax, rax); \/\/ return 0\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n+  const XMMRegister xmm_result = xmm0;\n+  const XMMRegister xmm_key_shuf_mask = xmm1;\n+  \/\/ On win64 xmm6-xmm15 must be preserved so don't use them.\n+  const XMMRegister xmm_temp1  = xmm2;\n+  const XMMRegister xmm_temp2  = xmm3;\n+  const XMMRegister xmm_temp3  = xmm4;\n+  const XMMRegister xmm_temp4  = xmm5;\n@@ -3671,2 +3600,1 @@\n-    return start;\n-  }\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n@@ -3674,0 +3602,2 @@\n+  \/\/ keylen could be only {11, 13, 15} * 4 = {44, 52, 60}\n+  __ movl(keylen, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));\n@@ -3675,18 +3605,2 @@\n-  \/\/ Arguments:\n-  \/\/\n-  \/\/ Inputs:\n-  \/\/   c_rarg0   - source byte array address\n-  \/\/   c_rarg1   - destination byte array address\n-  \/\/   c_rarg2   - K (key) in little endian int array\n-  \/\/\n-  address generate_aescrypt_decryptBlock() {\n-    assert(UseAES, \"need AES instructions and misaligned SSE support\");\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"aescrypt_decryptBlock\");\n-    Label L_doLast;\n-    address start = __ pc();\n-\n-    const Register from        = c_rarg0;  \/\/ source array address\n-    const Register to          = c_rarg1;  \/\/ destination array address\n-    const Register key         = c_rarg2;  \/\/ key array address\n-    const Register keylen      = rax;\n+  __ movdqu(xmm_key_shuf_mask, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()));\n+  __ movdqu(xmm_result, Address(from, 0));  \/\/ get 16 bytes of input\n@@ -3694,7 +3608,2 @@\n-    const XMMRegister xmm_result = xmm0;\n-    const XMMRegister xmm_key_shuf_mask = xmm1;\n-    \/\/ On win64 xmm6-xmm15 must be preserved so don't use them.\n-    const XMMRegister xmm_temp1  = xmm2;\n-    const XMMRegister xmm_temp2  = xmm3;\n-    const XMMRegister xmm_temp3  = xmm4;\n-    const XMMRegister xmm_temp4  = xmm5;\n+  \/\/ For encryption, the java expanded key ordering is just what we need\n+  \/\/ we don't know if the key is aligned, hence not using load-execute form\n@@ -3702,1 +3611,2 @@\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  load_key(xmm_temp1, key, 0x00, xmm_key_shuf_mask);\n+  __ pxor(xmm_result, xmm_temp1);\n@@ -3704,2 +3614,4 @@\n-    \/\/ keylen could be only {11, 13, 15} * 4 = {44, 52, 60}\n-    __ movl(keylen, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));\n+  load_key(xmm_temp1, key, 0x10, xmm_key_shuf_mask);\n+  load_key(xmm_temp2, key, 0x20, xmm_key_shuf_mask);\n+  load_key(xmm_temp3, key, 0x30, xmm_key_shuf_mask);\n+  load_key(xmm_temp4, key, 0x40, xmm_key_shuf_mask);\n@@ -3707,2 +3619,4 @@\n-    __ movdqu(xmm_key_shuf_mask, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()));\n-    __ movdqu(xmm_result, Address(from, 0));\n+  __ aesenc(xmm_result, xmm_temp1);\n+  __ aesenc(xmm_result, xmm_temp2);\n+  __ aesenc(xmm_result, xmm_temp3);\n+  __ aesenc(xmm_result, xmm_temp4);\n@@ -3710,7 +3624,4 @@\n-    \/\/ for decryption java expanded key ordering is rotated one position from what we want\n-    \/\/ so we start from 0x10 here and hit 0x00 last\n-    \/\/ we don't know if the key is aligned, hence not using load-execute form\n-    load_key(xmm_temp1, key, 0x10, xmm_key_shuf_mask);\n-    load_key(xmm_temp2, key, 0x20, xmm_key_shuf_mask);\n-    load_key(xmm_temp3, key, 0x30, xmm_key_shuf_mask);\n-    load_key(xmm_temp4, key, 0x40, xmm_key_shuf_mask);\n+  load_key(xmm_temp1, key, 0x50, xmm_key_shuf_mask);\n+  load_key(xmm_temp2, key, 0x60, xmm_key_shuf_mask);\n+  load_key(xmm_temp3, key, 0x70, xmm_key_shuf_mask);\n+  load_key(xmm_temp4, key, 0x80, xmm_key_shuf_mask);\n@@ -3718,4 +3629,4 @@\n-    __ pxor  (xmm_result, xmm_temp1);\n-    __ aesdec(xmm_result, xmm_temp2);\n-    __ aesdec(xmm_result, xmm_temp3);\n-    __ aesdec(xmm_result, xmm_temp4);\n+  __ aesenc(xmm_result, xmm_temp1);\n+  __ aesenc(xmm_result, xmm_temp2);\n+  __ aesenc(xmm_result, xmm_temp3);\n+  __ aesenc(xmm_result, xmm_temp4);\n@@ -3723,4 +3634,2 @@\n-    load_key(xmm_temp1, key, 0x50, xmm_key_shuf_mask);\n-    load_key(xmm_temp2, key, 0x60, xmm_key_shuf_mask);\n-    load_key(xmm_temp3, key, 0x70, xmm_key_shuf_mask);\n-    load_key(xmm_temp4, key, 0x80, xmm_key_shuf_mask);\n+  load_key(xmm_temp1, key, 0x90, xmm_key_shuf_mask);\n+  load_key(xmm_temp2, key, 0xa0, xmm_key_shuf_mask);\n@@ -3728,4 +3637,2 @@\n-    __ aesdec(xmm_result, xmm_temp1);\n-    __ aesdec(xmm_result, xmm_temp2);\n-    __ aesdec(xmm_result, xmm_temp3);\n-    __ aesdec(xmm_result, xmm_temp4);\n+  __ cmpl(keylen, 44);\n+  __ jccb(Assembler::equal, L_doLast);\n@@ -3733,3 +3640,2 @@\n-    load_key(xmm_temp1, key, 0x90, xmm_key_shuf_mask);\n-    load_key(xmm_temp2, key, 0xa0, xmm_key_shuf_mask);\n-    load_key(xmm_temp3, key, 0x00, xmm_key_shuf_mask);\n+  __ aesenc(xmm_result, xmm_temp1);\n+  __ aesenc(xmm_result, xmm_temp2);\n@@ -3737,2 +3643,2 @@\n-    __ cmpl(keylen, 44);\n-    __ jccb(Assembler::equal, L_doLast);\n+  load_key(xmm_temp1, key, 0xb0, xmm_key_shuf_mask);\n+  load_key(xmm_temp2, key, 0xc0, xmm_key_shuf_mask);\n@@ -3740,2 +3646,2 @@\n-    __ aesdec(xmm_result, xmm_temp1);\n-    __ aesdec(xmm_result, xmm_temp2);\n+  __ cmpl(keylen, 52);\n+  __ jccb(Assembler::equal, L_doLast);\n@@ -3743,2 +3649,2 @@\n-    load_key(xmm_temp1, key, 0xb0, xmm_key_shuf_mask);\n-    load_key(xmm_temp2, key, 0xc0, xmm_key_shuf_mask);\n+  __ aesenc(xmm_result, xmm_temp1);\n+  __ aesenc(xmm_result, xmm_temp2);\n@@ -3746,2 +3652,2 @@\n-    __ cmpl(keylen, 52);\n-    __ jccb(Assembler::equal, L_doLast);\n+  load_key(xmm_temp1, key, 0xd0, xmm_key_shuf_mask);\n+  load_key(xmm_temp2, key, 0xe0, xmm_key_shuf_mask);\n@@ -3749,2 +3655,7 @@\n-    __ aesdec(xmm_result, xmm_temp1);\n-    __ aesdec(xmm_result, xmm_temp2);\n+  __ BIND(L_doLast);\n+  __ aesenc(xmm_result, xmm_temp1);\n+  __ aesenclast(xmm_result, xmm_temp2);\n+  __ movdqu(Address(to, 0), xmm_result);        \/\/ store the result\n+  __ xorptr(rax, rax); \/\/ return 0\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n@@ -3752,2 +3663,2 @@\n-    load_key(xmm_temp1, key, 0xd0, xmm_key_shuf_mask);\n-    load_key(xmm_temp2, key, 0xe0, xmm_key_shuf_mask);\n+  return start;\n+}\n@@ -3755,3 +3666,0 @@\n-    __ BIND(L_doLast);\n-    __ aesdec(xmm_result, xmm_temp1);\n-    __ aesdec(xmm_result, xmm_temp2);\n@@ -3759,6 +3667,90 @@\n-    \/\/ for decryption the aesdeclast operation is always on key+0x00\n-    __ aesdeclast(xmm_result, xmm_temp3);\n-    __ movdqu(Address(to, 0), xmm_result);  \/\/ store the result\n-    __ xorptr(rax, rax); \/\/ return 0\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n+\/\/ Arguments:\n+\/\/\n+\/\/ Inputs:\n+\/\/   c_rarg0   - source byte array address\n+\/\/   c_rarg1   - destination byte array address\n+\/\/   c_rarg2   - K (key) in little endian int array\n+\/\/\n+address StubGenerator::generate_aescrypt_decryptBlock() {\n+  assert(UseAES, \"need AES instructions and misaligned SSE support\");\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"aescrypt_decryptBlock\");\n+  Label L_doLast;\n+  address start = __ pc();\n+\n+  const Register from        = c_rarg0;  \/\/ source array address\n+  const Register to          = c_rarg1;  \/\/ destination array address\n+  const Register key         = c_rarg2;  \/\/ key array address\n+  const Register keylen      = rax;\n+\n+  const XMMRegister xmm_result = xmm0;\n+  const XMMRegister xmm_key_shuf_mask = xmm1;\n+  \/\/ On win64 xmm6-xmm15 must be preserved so don't use them.\n+  const XMMRegister xmm_temp1  = xmm2;\n+  const XMMRegister xmm_temp2  = xmm3;\n+  const XMMRegister xmm_temp3  = xmm4;\n+  const XMMRegister xmm_temp4  = xmm5;\n+\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+\n+  \/\/ keylen could be only {11, 13, 15} * 4 = {44, 52, 60}\n+  __ movl(keylen, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));\n+\n+  __ movdqu(xmm_key_shuf_mask, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()));\n+  __ movdqu(xmm_result, Address(from, 0));\n+\n+  \/\/ for decryption java expanded key ordering is rotated one position from what we want\n+  \/\/ so we start from 0x10 here and hit 0x00 last\n+  \/\/ we don't know if the key is aligned, hence not using load-execute form\n+  load_key(xmm_temp1, key, 0x10, xmm_key_shuf_mask);\n+  load_key(xmm_temp2, key, 0x20, xmm_key_shuf_mask);\n+  load_key(xmm_temp3, key, 0x30, xmm_key_shuf_mask);\n+  load_key(xmm_temp4, key, 0x40, xmm_key_shuf_mask);\n+\n+  __ pxor  (xmm_result, xmm_temp1);\n+  __ aesdec(xmm_result, xmm_temp2);\n+  __ aesdec(xmm_result, xmm_temp3);\n+  __ aesdec(xmm_result, xmm_temp4);\n+\n+  load_key(xmm_temp1, key, 0x50, xmm_key_shuf_mask);\n+  load_key(xmm_temp2, key, 0x60, xmm_key_shuf_mask);\n+  load_key(xmm_temp3, key, 0x70, xmm_key_shuf_mask);\n+  load_key(xmm_temp4, key, 0x80, xmm_key_shuf_mask);\n+\n+  __ aesdec(xmm_result, xmm_temp1);\n+  __ aesdec(xmm_result, xmm_temp2);\n+  __ aesdec(xmm_result, xmm_temp3);\n+  __ aesdec(xmm_result, xmm_temp4);\n+\n+  load_key(xmm_temp1, key, 0x90, xmm_key_shuf_mask);\n+  load_key(xmm_temp2, key, 0xa0, xmm_key_shuf_mask);\n+  load_key(xmm_temp3, key, 0x00, xmm_key_shuf_mask);\n+\n+  __ cmpl(keylen, 44);\n+  __ jccb(Assembler::equal, L_doLast);\n+\n+  __ aesdec(xmm_result, xmm_temp1);\n+  __ aesdec(xmm_result, xmm_temp2);\n+\n+  load_key(xmm_temp1, key, 0xb0, xmm_key_shuf_mask);\n+  load_key(xmm_temp2, key, 0xc0, xmm_key_shuf_mask);\n+\n+  __ cmpl(keylen, 52);\n+  __ jccb(Assembler::equal, L_doLast);\n+\n+  __ aesdec(xmm_result, xmm_temp1);\n+  __ aesdec(xmm_result, xmm_temp2);\n+\n+  load_key(xmm_temp1, key, 0xd0, xmm_key_shuf_mask);\n+  load_key(xmm_temp2, key, 0xe0, xmm_key_shuf_mask);\n+\n+  __ BIND(L_doLast);\n+  __ aesdec(xmm_result, xmm_temp1);\n+  __ aesdec(xmm_result, xmm_temp2);\n+\n+  \/\/ for decryption the aesdeclast operation is always on key+0x00\n+  __ aesdeclast(xmm_result, xmm_temp3);\n+  __ movdqu(Address(to, 0), xmm_result);  \/\/ store the result\n+  __ xorptr(rax, rax); \/\/ return 0\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n@@ -3766,2 +3758,2 @@\n-    return start;\n-  }\n+  return start;\n+}\n@@ -3770,24 +3762,24 @@\n-  \/\/ Arguments:\n-  \/\/\n-  \/\/ Inputs:\n-  \/\/   c_rarg0   - source byte array address\n-  \/\/   c_rarg1   - destination byte array address\n-  \/\/   c_rarg2   - K (key) in little endian int array\n-  \/\/   c_rarg3   - r vector byte array address\n-  \/\/   c_rarg4   - input length\n-  \/\/\n-  \/\/ Output:\n-  \/\/   rax       - input length\n-  \/\/\n-  address generate_cipherBlockChaining_encryptAESCrypt() {\n-    assert(UseAES, \"need AES instructions and misaligned SSE support\");\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"cipherBlockChaining_encryptAESCrypt\");\n-    address start = __ pc();\n-\n-    Label L_exit, L_key_192_256, L_key_256, L_loopTop_128, L_loopTop_192, L_loopTop_256;\n-    const Register from        = c_rarg0;  \/\/ source array address\n-    const Register to          = c_rarg1;  \/\/ destination array address\n-    const Register key         = c_rarg2;  \/\/ key array address\n-    const Register rvec        = c_rarg3;  \/\/ r byte array initialized from initvector array address\n-                                           \/\/ and left with the results of the last encryption block\n+\/\/ Arguments:\n+\/\/\n+\/\/ Inputs:\n+\/\/   c_rarg0   - source byte array address\n+\/\/   c_rarg1   - destination byte array address\n+\/\/   c_rarg2   - K (key) in little endian int array\n+\/\/   c_rarg3   - r vector byte array address\n+\/\/   c_rarg4   - input length\n+\/\/\n+\/\/ Output:\n+\/\/   rax       - input length\n+\/\/\n+address StubGenerator::generate_cipherBlockChaining_encryptAESCrypt() {\n+  assert(UseAES, \"need AES instructions and misaligned SSE support\");\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"cipherBlockChaining_encryptAESCrypt\");\n+  address start = __ pc();\n+\n+  Label L_exit, L_key_192_256, L_key_256, L_loopTop_128, L_loopTop_192, L_loopTop_256;\n+  const Register from        = c_rarg0;  \/\/ source array address\n+  const Register to          = c_rarg1;  \/\/ destination array address\n+  const Register key         = c_rarg2;  \/\/ key array address\n+  const Register rvec        = c_rarg3;  \/\/ r byte array initialized from initvector array address\n+                                         \/\/ and left with the results of the last encryption block\n@@ -3795,1 +3787,1 @@\n-    const Register len_reg     = c_rarg4;  \/\/ src len (must be multiple of blocksize 16)\n+  const Register len_reg     = c_rarg4;  \/\/ src len (must be multiple of blocksize 16)\n@@ -3797,2 +3789,2 @@\n-    const Address  len_mem(rbp, 6 * wordSize);  \/\/ length is on stack on Win64\n-    const Register len_reg     = r11;      \/\/ pick the volatile windows register\n+  const Address  len_mem(rbp, 6 * wordSize);  \/\/ length is on stack on Win64\n+  const Register len_reg     = r11;      \/\/ pick the volatile windows register\n@@ -3800,15 +3792,15 @@\n-    const Register pos         = rax;\n-\n-    \/\/ xmm register assignments for the loops below\n-    const XMMRegister xmm_result = xmm0;\n-    const XMMRegister xmm_temp   = xmm1;\n-    \/\/ keys 0-10 preloaded into xmm2-xmm12\n-    const int XMM_REG_NUM_KEY_FIRST = 2;\n-    const int XMM_REG_NUM_KEY_LAST  = 15;\n-    const XMMRegister xmm_key0   = as_XMMRegister(XMM_REG_NUM_KEY_FIRST);\n-    const XMMRegister xmm_key10  = as_XMMRegister(XMM_REG_NUM_KEY_FIRST+10);\n-    const XMMRegister xmm_key11  = as_XMMRegister(XMM_REG_NUM_KEY_FIRST+11);\n-    const XMMRegister xmm_key12  = as_XMMRegister(XMM_REG_NUM_KEY_FIRST+12);\n-    const XMMRegister xmm_key13  = as_XMMRegister(XMM_REG_NUM_KEY_FIRST+13);\n-\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  const Register pos         = rax;\n+\n+  \/\/ xmm register assignments for the loops below\n+  const XMMRegister xmm_result = xmm0;\n+  const XMMRegister xmm_temp   = xmm1;\n+  \/\/ keys 0-10 preloaded into xmm2-xmm12\n+  const int XMM_REG_NUM_KEY_FIRST = 2;\n+  const int XMM_REG_NUM_KEY_LAST  = 15;\n+  const XMMRegister xmm_key0   = as_XMMRegister(XMM_REG_NUM_KEY_FIRST);\n+  const XMMRegister xmm_key10  = as_XMMRegister(XMM_REG_NUM_KEY_FIRST+10);\n+  const XMMRegister xmm_key11  = as_XMMRegister(XMM_REG_NUM_KEY_FIRST+11);\n+  const XMMRegister xmm_key12  = as_XMMRegister(XMM_REG_NUM_KEY_FIRST+12);\n+  const XMMRegister xmm_key13  = as_XMMRegister(XMM_REG_NUM_KEY_FIRST+13);\n+\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n@@ -3817,2 +3809,2 @@\n-    \/\/ on win64, fill len_reg from stack position\n-    __ movl(len_reg, len_mem);\n+  \/\/ on win64, fill len_reg from stack position\n+  __ movl(len_reg, len_mem);\n@@ -3820,1 +3812,1 @@\n-    __ push(len_reg); \/\/ Save\n+  __ push(len_reg); \/\/ Save\n@@ -3823,31 +3815,31 @@\n-    const XMMRegister xmm_key_shuf_mask = xmm_temp;  \/\/ used temporarily to swap key bytes up front\n-    __ movdqu(xmm_key_shuf_mask, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()));\n-    \/\/ load up xmm regs xmm2 thru xmm12 with key 0x00 - 0xa0\n-    for (int rnum = XMM_REG_NUM_KEY_FIRST, offset = 0x00; rnum <= XMM_REG_NUM_KEY_FIRST+10; rnum++) {\n-      load_key(as_XMMRegister(rnum), key, offset, xmm_key_shuf_mask);\n-      offset += 0x10;\n-    }\n-    __ movdqu(xmm_result, Address(rvec, 0x00));   \/\/ initialize xmm_result with r vec\n-\n-    \/\/ now split to different paths depending on the keylen (len in ints of AESCrypt.KLE array (52=192, or 60=256))\n-    __ movl(rax, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));\n-    __ cmpl(rax, 44);\n-    __ jcc(Assembler::notEqual, L_key_192_256);\n-\n-    \/\/ 128 bit code follows here\n-    __ movptr(pos, 0);\n-    __ align(OptoLoopAlignment);\n-\n-    __ BIND(L_loopTop_128);\n-    __ movdqu(xmm_temp, Address(from, pos, Address::times_1, 0));   \/\/ get next 16 bytes of input\n-    __ pxor  (xmm_result, xmm_temp);               \/\/ xor with the current r vector\n-    __ pxor  (xmm_result, xmm_key0);               \/\/ do the aes rounds\n-    for (int rnum = XMM_REG_NUM_KEY_FIRST + 1; rnum <= XMM_REG_NUM_KEY_FIRST + 9; rnum++) {\n-      __ aesenc(xmm_result, as_XMMRegister(rnum));\n-    }\n-    __ aesenclast(xmm_result, xmm_key10);\n-    __ movdqu(Address(to, pos, Address::times_1, 0), xmm_result);     \/\/ store into the next 16 bytes of output\n-    \/\/ no need to store r to memory until we exit\n-    __ addptr(pos, AESBlockSize);\n-    __ subptr(len_reg, AESBlockSize);\n-    __ jcc(Assembler::notEqual, L_loopTop_128);\n+  const XMMRegister xmm_key_shuf_mask = xmm_temp;  \/\/ used temporarily to swap key bytes up front\n+  __ movdqu(xmm_key_shuf_mask, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()));\n+  \/\/ load up xmm regs xmm2 thru xmm12 with key 0x00 - 0xa0\n+  for (int rnum = XMM_REG_NUM_KEY_FIRST, offset = 0x00; rnum <= XMM_REG_NUM_KEY_FIRST+10; rnum++) {\n+    load_key(as_XMMRegister(rnum), key, offset, xmm_key_shuf_mask);\n+    offset += 0x10;\n+  }\n+  __ movdqu(xmm_result, Address(rvec, 0x00));   \/\/ initialize xmm_result with r vec\n+\n+  \/\/ now split to different paths depending on the keylen (len in ints of AESCrypt.KLE array (52=192, or 60=256))\n+  __ movl(rax, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));\n+  __ cmpl(rax, 44);\n+  __ jcc(Assembler::notEqual, L_key_192_256);\n+\n+  \/\/ 128 bit code follows here\n+  __ movptr(pos, 0);\n+  __ align(OptoLoopAlignment);\n+\n+  __ BIND(L_loopTop_128);\n+  __ movdqu(xmm_temp, Address(from, pos, Address::times_1, 0));   \/\/ get next 16 bytes of input\n+  __ pxor  (xmm_result, xmm_temp);               \/\/ xor with the current r vector\n+  __ pxor  (xmm_result, xmm_key0);               \/\/ do the aes rounds\n+  for (int rnum = XMM_REG_NUM_KEY_FIRST + 1; rnum <= XMM_REG_NUM_KEY_FIRST + 9; rnum++) {\n+    __ aesenc(xmm_result, as_XMMRegister(rnum));\n+  }\n+  __ aesenclast(xmm_result, xmm_key10);\n+  __ movdqu(Address(to, pos, Address::times_1, 0), xmm_result);     \/\/ store into the next 16 bytes of output\n+  \/\/ no need to store r to memory until we exit\n+  __ addptr(pos, AESBlockSize);\n+  __ subptr(len_reg, AESBlockSize);\n+  __ jcc(Assembler::notEqual, L_loopTop_128);\n@@ -3855,2 +3847,2 @@\n-    __ BIND(L_exit);\n-    __ movdqu(Address(rvec, 0), xmm_result);     \/\/ final value of r stored in rvec of CipherBlockChaining object\n+  __ BIND(L_exit);\n+  __ movdqu(Address(rvec, 0), xmm_result);     \/\/ final value of r stored in rvec of CipherBlockChaining object\n@@ -3859,56 +3851,24 @@\n-    __ movl(rax, len_mem);\n-#else\n-    __ pop(rax); \/\/ return length\n-#endif\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-\n-    __ BIND(L_key_192_256);\n-    \/\/ here rax = len in ints of AESCrypt.KLE array (52=192, or 60=256)\n-    load_key(xmm_key11, key, 0xb0, xmm_key_shuf_mask);\n-    load_key(xmm_key12, key, 0xc0, xmm_key_shuf_mask);\n-    __ cmpl(rax, 52);\n-    __ jcc(Assembler::notEqual, L_key_256);\n-\n-    \/\/ 192-bit code follows here (could be changed to use more xmm registers)\n-    __ movptr(pos, 0);\n-    __ align(OptoLoopAlignment);\n-\n-    __ BIND(L_loopTop_192);\n-    __ movdqu(xmm_temp, Address(from, pos, Address::times_1, 0));   \/\/ get next 16 bytes of input\n-    __ pxor  (xmm_result, xmm_temp);               \/\/ xor with the current r vector\n-    __ pxor  (xmm_result, xmm_key0);               \/\/ do the aes rounds\n-    for (int rnum = XMM_REG_NUM_KEY_FIRST + 1; rnum  <= XMM_REG_NUM_KEY_FIRST + 11; rnum++) {\n-      __ aesenc(xmm_result, as_XMMRegister(rnum));\n-    }\n-    __ aesenclast(xmm_result, xmm_key12);\n-    __ movdqu(Address(to, pos, Address::times_1, 0), xmm_result);     \/\/ store into the next 16 bytes of output\n-    \/\/ no need to store r to memory until we exit\n-    __ addptr(pos, AESBlockSize);\n-    __ subptr(len_reg, AESBlockSize);\n-    __ jcc(Assembler::notEqual, L_loopTop_192);\n-    __ jmp(L_exit);\n-\n-    __ BIND(L_key_256);\n-    \/\/ 256-bit code follows here (could be changed to use more xmm registers)\n-    load_key(xmm_key13, key, 0xd0, xmm_key_shuf_mask);\n-    __ movptr(pos, 0);\n-    __ align(OptoLoopAlignment);\n-\n-    __ BIND(L_loopTop_256);\n-    __ movdqu(xmm_temp, Address(from, pos, Address::times_1, 0));   \/\/ get next 16 bytes of input\n-    __ pxor  (xmm_result, xmm_temp);               \/\/ xor with the current r vector\n-    __ pxor  (xmm_result, xmm_key0);               \/\/ do the aes rounds\n-    for (int rnum = XMM_REG_NUM_KEY_FIRST + 1; rnum  <= XMM_REG_NUM_KEY_FIRST + 13; rnum++) {\n-      __ aesenc(xmm_result, as_XMMRegister(rnum));\n-    }\n-    load_key(xmm_temp, key, 0xe0);\n-    __ aesenclast(xmm_result, xmm_temp);\n-    __ movdqu(Address(to, pos, Address::times_1, 0), xmm_result);     \/\/ store into the next 16 bytes of output\n-    \/\/ no need to store r to memory until we exit\n-    __ addptr(pos, AESBlockSize);\n-    __ subptr(len_reg, AESBlockSize);\n-    __ jcc(Assembler::notEqual, L_loopTop_256);\n-    __ jmp(L_exit);\n-\n-    return start;\n+  __ movl(rax, len_mem);\n+#else\n+  __ pop(rax); \/\/ return length\n+#endif\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n+\n+  __ BIND(L_key_192_256);\n+  \/\/ here rax = len in ints of AESCrypt.KLE array (52=192, or 60=256)\n+  load_key(xmm_key11, key, 0xb0, xmm_key_shuf_mask);\n+  load_key(xmm_key12, key, 0xc0, xmm_key_shuf_mask);\n+  __ cmpl(rax, 52);\n+  __ jcc(Assembler::notEqual, L_key_256);\n+\n+  \/\/ 192-bit code follows here (could be changed to use more xmm registers)\n+  __ movptr(pos, 0);\n+  __ align(OptoLoopAlignment);\n+\n+  __ BIND(L_loopTop_192);\n+  __ movdqu(xmm_temp, Address(from, pos, Address::times_1, 0));   \/\/ get next 16 bytes of input\n+  __ pxor  (xmm_result, xmm_temp);               \/\/ xor with the current r vector\n+  __ pxor  (xmm_result, xmm_key0);               \/\/ do the aes rounds\n+  for (int rnum = XMM_REG_NUM_KEY_FIRST + 1; rnum  <= XMM_REG_NUM_KEY_FIRST + 11; rnum++) {\n+    __ aesenc(xmm_result, as_XMMRegister(rnum));\n@@ -3916,0 +3876,29 @@\n+  __ aesenclast(xmm_result, xmm_key12);\n+  __ movdqu(Address(to, pos, Address::times_1, 0), xmm_result);     \/\/ store into the next 16 bytes of output\n+  \/\/ no need to store r to memory until we exit\n+  __ addptr(pos, AESBlockSize);\n+  __ subptr(len_reg, AESBlockSize);\n+  __ jcc(Assembler::notEqual, L_loopTop_192);\n+  __ jmp(L_exit);\n+\n+  __ BIND(L_key_256);\n+  \/\/ 256-bit code follows here (could be changed to use more xmm registers)\n+  load_key(xmm_key13, key, 0xd0, xmm_key_shuf_mask);\n+  __ movptr(pos, 0);\n+  __ align(OptoLoopAlignment);\n+\n+  __ BIND(L_loopTop_256);\n+  __ movdqu(xmm_temp, Address(from, pos, Address::times_1, 0));   \/\/ get next 16 bytes of input\n+  __ pxor  (xmm_result, xmm_temp);               \/\/ xor with the current r vector\n+  __ pxor  (xmm_result, xmm_key0);               \/\/ do the aes rounds\n+  for (int rnum = XMM_REG_NUM_KEY_FIRST + 1; rnum  <= XMM_REG_NUM_KEY_FIRST + 13; rnum++) {\n+    __ aesenc(xmm_result, as_XMMRegister(rnum));\n+  }\n+  load_key(xmm_temp, key, 0xe0);\n+  __ aesenclast(xmm_result, xmm_temp);\n+  __ movdqu(Address(to, pos, Address::times_1, 0), xmm_result);     \/\/ store into the next 16 bytes of output\n+  \/\/ no need to store r to memory until we exit\n+  __ addptr(pos, AESBlockSize);\n+  __ subptr(len_reg, AESBlockSize);\n+  __ jcc(Assembler::notEqual, L_loopTop_256);\n+  __ jmp(L_exit);\n@@ -3917,26 +3906,29 @@\n-  \/\/ This is a version of CBC\/AES Decrypt which does 4 blocks in a loop at a time\n-  \/\/ to hide instruction latency\n-  \/\/\n-  \/\/ Arguments:\n-  \/\/\n-  \/\/ Inputs:\n-  \/\/   c_rarg0   - source byte array address\n-  \/\/   c_rarg1   - destination byte array address\n-  \/\/   c_rarg2   - K (key) in little endian int array\n-  \/\/   c_rarg3   - r vector byte array address\n-  \/\/   c_rarg4   - input length\n-  \/\/\n-  \/\/ Output:\n-  \/\/   rax       - input length\n-  \/\/\n-  address generate_cipherBlockChaining_decryptAESCrypt_Parallel() {\n-    assert(UseAES, \"need AES instructions and misaligned SSE support\");\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"cipherBlockChaining_decryptAESCrypt\");\n-    address start = __ pc();\n-\n-    const Register from        = c_rarg0;  \/\/ source array address\n-    const Register to          = c_rarg1;  \/\/ destination array address\n-    const Register key         = c_rarg2;  \/\/ key array address\n-    const Register rvec        = c_rarg3;  \/\/ r byte array initialized from initvector array address\n-                                           \/\/ and left with the results of the last encryption block\n+  return start;\n+}\n+\n+\/\/ This is a version of CBC\/AES Decrypt which does 4 blocks in a loop at a time\n+\/\/ to hide instruction latency\n+\/\/\n+\/\/ Arguments:\n+\/\/\n+\/\/ Inputs:\n+\/\/   c_rarg0   - source byte array address\n+\/\/   c_rarg1   - destination byte array address\n+\/\/   c_rarg2   - K (key) in little endian int array\n+\/\/   c_rarg3   - r vector byte array address\n+\/\/   c_rarg4   - input length\n+\/\/\n+\/\/ Output:\n+\/\/   rax       - input length\n+\/\/\n+address StubGenerator::generate_cipherBlockChaining_decryptAESCrypt_Parallel() {\n+  assert(UseAES, \"need AES instructions and misaligned SSE support\");\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"cipherBlockChaining_decryptAESCrypt\");\n+  address start = __ pc();\n+\n+  const Register from        = c_rarg0;  \/\/ source array address\n+  const Register to          = c_rarg1;  \/\/ destination array address\n+  const Register key         = c_rarg2;  \/\/ key array address\n+  const Register rvec        = c_rarg3;  \/\/ r byte array initialized from initvector array address\n+                                         \/\/ and left with the results of the last encryption block\n@@ -3944,1 +3936,1 @@\n-    const Register len_reg     = c_rarg4;  \/\/ src len (must be multiple of blocksize 16)\n+  const Register len_reg     = c_rarg4;  \/\/ src len (must be multiple of blocksize 16)\n@@ -3946,2 +3938,2 @@\n-    const Address  len_mem(rbp, 6 * wordSize);  \/\/ length is on stack on Win64\n-    const Register len_reg     = r11;      \/\/ pick the volatile windows register\n+  const Address  len_mem(rbp, 6 * wordSize);  \/\/ length is on stack on Win64\n+  const Register len_reg     = r11;      \/\/ pick the volatile windows register\n@@ -3949,1 +3941,1 @@\n-    const Register pos         = rax;\n+  const Register pos         = rax;\n@@ -3951,2 +3943,2 @@\n-    const int PARALLEL_FACTOR = 4;\n-    const int ROUNDS[3] = { 10, 12, 14 }; \/\/ aes rounds for key128, key192, key256\n+  const int PARALLEL_FACTOR = 4;\n+  const int ROUNDS[3] = { 10, 12, 14 }; \/\/ aes rounds for key128, key192, key256\n@@ -3954,6 +3946,6 @@\n-    Label L_exit;\n-    Label L_singleBlock_loopTopHead[3]; \/\/ 128, 192, 256\n-    Label L_singleBlock_loopTopHead2[3]; \/\/ 128, 192, 256\n-    Label L_singleBlock_loopTop[3]; \/\/ 128, 192, 256\n-    Label L_multiBlock_loopTopHead[3]; \/\/ 128, 192, 256\n-    Label L_multiBlock_loopTop[3]; \/\/ 128, 192, 256\n+  Label L_exit;\n+  Label L_singleBlock_loopTopHead[3]; \/\/ 128, 192, 256\n+  Label L_singleBlock_loopTopHead2[3]; \/\/ 128, 192, 256\n+  Label L_singleBlock_loopTop[3]; \/\/ 128, 192, 256\n+  Label L_multiBlock_loopTopHead[3]; \/\/ 128, 192, 256\n+  Label L_multiBlock_loopTop[3]; \/\/ 128, 192, 256\n@@ -3961,5 +3953,5 @@\n-    \/\/ keys 0-10 preloaded into xmm5-xmm15\n-    const int XMM_REG_NUM_KEY_FIRST = 5;\n-    const int XMM_REG_NUM_KEY_LAST  = 15;\n-    const XMMRegister xmm_key_first = as_XMMRegister(XMM_REG_NUM_KEY_FIRST);\n-    const XMMRegister xmm_key_last  = as_XMMRegister(XMM_REG_NUM_KEY_LAST);\n+  \/\/ keys 0-10 preloaded into xmm5-xmm15\n+  const int XMM_REG_NUM_KEY_FIRST = 5;\n+  const int XMM_REG_NUM_KEY_LAST  = 15;\n+  const XMMRegister xmm_key_first = as_XMMRegister(XMM_REG_NUM_KEY_FIRST);\n+  const XMMRegister xmm_key_last  = as_XMMRegister(XMM_REG_NUM_KEY_LAST);\n@@ -3967,1 +3959,1 @@\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n@@ -3970,2 +3962,2 @@\n-    \/\/ on win64, fill len_reg from stack position\n-    __ movl(len_reg, len_mem);\n+  \/\/ on win64, fill len_reg from stack position\n+  __ movl(len_reg, len_mem);\n@@ -3973,1 +3965,1 @@\n-    __ push(len_reg); \/\/ Save\n+  __ push(len_reg); \/\/ Save\n@@ -3975,11 +3967,11 @@\n-    __ push(rbx);\n-    \/\/ the java expanded key ordering is rotated one position from what we want\n-    \/\/ so we start from 0x10 here and hit 0x00 last\n-    const XMMRegister xmm_key_shuf_mask = xmm1;  \/\/ used temporarily to swap key bytes up front\n-    __ movdqu(xmm_key_shuf_mask, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()));\n-    \/\/ load up xmm regs 5 thru 15 with key 0x10 - 0xa0 - 0x00\n-    for (int rnum = XMM_REG_NUM_KEY_FIRST, offset = 0x10; rnum < XMM_REG_NUM_KEY_LAST; rnum++) {\n-      load_key(as_XMMRegister(rnum), key, offset, xmm_key_shuf_mask);\n-      offset += 0x10;\n-    }\n-    load_key(xmm_key_last, key, 0x00, xmm_key_shuf_mask);\n+  __ push(rbx);\n+  \/\/ the java expanded key ordering is rotated one position from what we want\n+  \/\/ so we start from 0x10 here and hit 0x00 last\n+  const XMMRegister xmm_key_shuf_mask = xmm1;  \/\/ used temporarily to swap key bytes up front\n+  __ movdqu(xmm_key_shuf_mask, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()));\n+  \/\/ load up xmm regs 5 thru 15 with key 0x10 - 0xa0 - 0x00\n+  for (int rnum = XMM_REG_NUM_KEY_FIRST, offset = 0x10; rnum < XMM_REG_NUM_KEY_LAST; rnum++) {\n+    load_key(as_XMMRegister(rnum), key, offset, xmm_key_shuf_mask);\n+    offset += 0x10;\n+  }\n+  load_key(xmm_key_last, key, 0x00, xmm_key_shuf_mask);\n@@ -3987,1 +3979,1 @@\n-    const XMMRegister xmm_prev_block_cipher = xmm1;  \/\/ holds cipher of previous block\n+  const XMMRegister xmm_prev_block_cipher = xmm1;  \/\/ holds cipher of previous block\n@@ -3989,5 +3981,5 @@\n-    \/\/ registers holding the four results in the parallelized loop\n-    const XMMRegister xmm_result0 = xmm0;\n-    const XMMRegister xmm_result1 = xmm2;\n-    const XMMRegister xmm_result2 = xmm3;\n-    const XMMRegister xmm_result3 = xmm4;\n+  \/\/ registers holding the four results in the parallelized loop\n+  const XMMRegister xmm_result0 = xmm0;\n+  const XMMRegister xmm_result1 = xmm2;\n+  const XMMRegister xmm_result2 = xmm3;\n+  const XMMRegister xmm_result3 = xmm4;\n@@ -3995,1 +3987,1 @@\n-    __ movdqu(xmm_prev_block_cipher, Address(rvec, 0x00));   \/\/ initialize with initial rvec\n+  __ movdqu(xmm_prev_block_cipher, Address(rvec, 0x00));   \/\/ initialize with initial rvec\n@@ -3997,1 +3989,1 @@\n-    __ xorptr(pos, pos);\n+  __ xorptr(pos, pos);\n@@ -3999,6 +3991,6 @@\n-    \/\/ now split to different paths depending on the keylen (len in ints of AESCrypt.KLE array (52=192, or 60=256))\n-    __ movl(rbx, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));\n-    __ cmpl(rbx, 52);\n-    __ jcc(Assembler::equal, L_multiBlock_loopTopHead[1]);\n-    __ cmpl(rbx, 60);\n-    __ jcc(Assembler::equal, L_multiBlock_loopTopHead[2]);\n+  \/\/ now split to different paths depending on the keylen (len in ints of AESCrypt.KLE array (52=192, or 60=256))\n+  __ movl(rbx, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));\n+  __ cmpl(rbx, 52);\n+  __ jcc(Assembler::equal, L_multiBlock_loopTopHead[1]);\n+  __ cmpl(rbx, 60);\n+  __ jcc(Assembler::equal, L_multiBlock_loopTopHead[2]);\n@@ -4007,32 +3999,8 @@\n-  __ opc(xmm_result0, src_reg);         \\\n-  __ opc(xmm_result1, src_reg);         \\\n-  __ opc(xmm_result2, src_reg);         \\\n-  __ opc(xmm_result3, src_reg);         \\\n-\n-    for (int k = 0; k < 3; ++k) {\n-      __ BIND(L_multiBlock_loopTopHead[k]);\n-      if (k != 0) {\n-        __ cmpptr(len_reg, PARALLEL_FACTOR * AESBlockSize); \/\/ see if at least 4 blocks left\n-        __ jcc(Assembler::less, L_singleBlock_loopTopHead2[k]);\n-      }\n-      if (k == 1) {\n-        __ subptr(rsp, 6 * wordSize);\n-        __ movdqu(Address(rsp, 0), xmm15); \/\/save last_key from xmm15\n-        load_key(xmm15, key, 0xb0); \/\/ 0xb0; 192-bit key goes up to 0xc0\n-        __ movdqu(Address(rsp, 2 * wordSize), xmm15);\n-        load_key(xmm1, key, 0xc0);  \/\/ 0xc0;\n-        __ movdqu(Address(rsp, 4 * wordSize), xmm1);\n-      } else if (k == 2) {\n-        __ subptr(rsp, 10 * wordSize);\n-        __ movdqu(Address(rsp, 0), xmm15); \/\/save last_key from xmm15\n-        load_key(xmm15, key, 0xd0); \/\/ 0xd0; 256-bit key goes up to 0xe0\n-        __ movdqu(Address(rsp, 6 * wordSize), xmm15);\n-        load_key(xmm1, key, 0xe0);  \/\/ 0xe0;\n-        __ movdqu(Address(rsp, 8 * wordSize), xmm1);\n-        load_key(xmm15, key, 0xb0); \/\/ 0xb0;\n-        __ movdqu(Address(rsp, 2 * wordSize), xmm15);\n-        load_key(xmm1, key, 0xc0);  \/\/ 0xc0;\n-        __ movdqu(Address(rsp, 4 * wordSize), xmm1);\n-      }\n-      __ align(OptoLoopAlignment);\n-      __ BIND(L_multiBlock_loopTop[k]);\n+__ opc(xmm_result0, src_reg);         \\\n+__ opc(xmm_result1, src_reg);         \\\n+__ opc(xmm_result2, src_reg);         \\\n+__ opc(xmm_result3, src_reg);         \\\n+\n+  for (int k = 0; k < 3; ++k) {\n+    __ BIND(L_multiBlock_loopTopHead[k]);\n+    if (k != 0) {\n@@ -4040,11 +4008,25 @@\n-      __ jcc(Assembler::less, L_singleBlock_loopTopHead[k]);\n-\n-      if  (k != 0) {\n-        __ movdqu(xmm15, Address(rsp, 2 * wordSize));\n-        __ movdqu(xmm1, Address(rsp, 4 * wordSize));\n-      }\n-\n-      __ movdqu(xmm_result0, Address(from, pos, Address::times_1, 0 * AESBlockSize)); \/\/ get next 4 blocks into xmmresult registers\n-      __ movdqu(xmm_result1, Address(from, pos, Address::times_1, 1 * AESBlockSize));\n-      __ movdqu(xmm_result2, Address(from, pos, Address::times_1, 2 * AESBlockSize));\n-      __ movdqu(xmm_result3, Address(from, pos, Address::times_1, 3 * AESBlockSize));\n+      __ jcc(Assembler::less, L_singleBlock_loopTopHead2[k]);\n+    }\n+    if (k == 1) {\n+      __ subptr(rsp, 6 * wordSize);\n+      __ movdqu(Address(rsp, 0), xmm15); \/\/save last_key from xmm15\n+      load_key(xmm15, key, 0xb0); \/\/ 0xb0; 192-bit key goes up to 0xc0\n+      __ movdqu(Address(rsp, 2 * wordSize), xmm15);\n+      load_key(xmm1, key, 0xc0);  \/\/ 0xc0;\n+      __ movdqu(Address(rsp, 4 * wordSize), xmm1);\n+    } else if (k == 2) {\n+      __ subptr(rsp, 10 * wordSize);\n+      __ movdqu(Address(rsp, 0), xmm15); \/\/save last_key from xmm15\n+      load_key(xmm15, key, 0xd0); \/\/ 0xd0; 256-bit key goes up to 0xe0\n+      __ movdqu(Address(rsp, 6 * wordSize), xmm15);\n+      load_key(xmm1, key, 0xe0);  \/\/ 0xe0;\n+      __ movdqu(Address(rsp, 8 * wordSize), xmm1);\n+      load_key(xmm15, key, 0xb0); \/\/ 0xb0;\n+      __ movdqu(Address(rsp, 2 * wordSize), xmm15);\n+      load_key(xmm1, key, 0xc0);  \/\/ 0xc0;\n+      __ movdqu(Address(rsp, 4 * wordSize), xmm1);\n+    }\n+    __ align(OptoLoopAlignment);\n+    __ BIND(L_multiBlock_loopTop[k]);\n+    __ cmpptr(len_reg, PARALLEL_FACTOR * AESBlockSize); \/\/ see if at least 4 blocks left\n+    __ jcc(Assembler::less, L_singleBlock_loopTopHead[k]);\n@@ -4052,27 +4034,4 @@\n-      DoFour(pxor, xmm_key_first);\n-      if (k == 0) {\n-        for (int rnum = 1; rnum < ROUNDS[k]; rnum++) {\n-          DoFour(aesdec, as_XMMRegister(rnum + XMM_REG_NUM_KEY_FIRST));\n-        }\n-        DoFour(aesdeclast, xmm_key_last);\n-      } else if (k == 1) {\n-        for (int rnum = 1; rnum <= ROUNDS[k]-2; rnum++) {\n-          DoFour(aesdec, as_XMMRegister(rnum + XMM_REG_NUM_KEY_FIRST));\n-        }\n-        __ movdqu(xmm_key_last, Address(rsp, 0)); \/\/ xmm15 needs to be loaded again.\n-        DoFour(aesdec, xmm1);  \/\/ key : 0xc0\n-        __ movdqu(xmm_prev_block_cipher, Address(rvec, 0x00));  \/\/ xmm1 needs to be loaded again\n-        DoFour(aesdeclast, xmm_key_last);\n-      } else if (k == 2) {\n-        for (int rnum = 1; rnum <= ROUNDS[k] - 4; rnum++) {\n-          DoFour(aesdec, as_XMMRegister(rnum + XMM_REG_NUM_KEY_FIRST));\n-        }\n-        DoFour(aesdec, xmm1);  \/\/ key : 0xc0\n-        __ movdqu(xmm15, Address(rsp, 6 * wordSize));\n-        __ movdqu(xmm1, Address(rsp, 8 * wordSize));\n-        DoFour(aesdec, xmm15);  \/\/ key : 0xd0\n-        __ movdqu(xmm_key_last, Address(rsp, 0)); \/\/ xmm15 needs to be loaded again.\n-        DoFour(aesdec, xmm1);  \/\/ key : 0xe0\n-        __ movdqu(xmm_prev_block_cipher, Address(rvec, 0x00));  \/\/ xmm1 needs to be loaded again\n-        DoFour(aesdeclast, xmm_key_last);\n-      }\n+    if  (k != 0) {\n+      __ movdqu(xmm15, Address(rsp, 2 * wordSize));\n+      __ movdqu(xmm1, Address(rsp, 4 * wordSize));\n+    }\n@@ -4080,12 +4039,4 @@\n-      \/\/ for each result, xor with the r vector of previous cipher block\n-      __ pxor(xmm_result0, xmm_prev_block_cipher);\n-      __ movdqu(xmm_prev_block_cipher, Address(from, pos, Address::times_1, 0 * AESBlockSize));\n-      __ pxor(xmm_result1, xmm_prev_block_cipher);\n-      __ movdqu(xmm_prev_block_cipher, Address(from, pos, Address::times_1, 1 * AESBlockSize));\n-      __ pxor(xmm_result2, xmm_prev_block_cipher);\n-      __ movdqu(xmm_prev_block_cipher, Address(from, pos, Address::times_1, 2 * AESBlockSize));\n-      __ pxor(xmm_result3, xmm_prev_block_cipher);\n-      __ movdqu(xmm_prev_block_cipher, Address(from, pos, Address::times_1, 3 * AESBlockSize));   \/\/ this will carry over to next set of blocks\n-      if (k != 0) {\n-        __ movdqu(Address(rvec, 0x00), xmm_prev_block_cipher);\n-      }\n+    __ movdqu(xmm_result0, Address(from, pos, Address::times_1, 0 * AESBlockSize)); \/\/ get next 4 blocks into xmmresult registers\n+    __ movdqu(xmm_result1, Address(from, pos, Address::times_1, 1 * AESBlockSize));\n+    __ movdqu(xmm_result2, Address(from, pos, Address::times_1, 2 * AESBlockSize));\n+    __ movdqu(xmm_result3, Address(from, pos, Address::times_1, 3 * AESBlockSize));\n@@ -4093,32 +4044,4 @@\n-      __ movdqu(Address(to, pos, Address::times_1, 0 * AESBlockSize), xmm_result0);     \/\/ store 4 results into the next 64 bytes of output\n-      __ movdqu(Address(to, pos, Address::times_1, 1 * AESBlockSize), xmm_result1);\n-      __ movdqu(Address(to, pos, Address::times_1, 2 * AESBlockSize), xmm_result2);\n-      __ movdqu(Address(to, pos, Address::times_1, 3 * AESBlockSize), xmm_result3);\n-\n-      __ addptr(pos, PARALLEL_FACTOR * AESBlockSize);\n-      __ subptr(len_reg, PARALLEL_FACTOR * AESBlockSize);\n-      __ jmp(L_multiBlock_loopTop[k]);\n-\n-      \/\/ registers used in the non-parallelized loops\n-      \/\/ xmm register assignments for the loops below\n-      const XMMRegister xmm_result = xmm0;\n-      const XMMRegister xmm_prev_block_cipher_save = xmm2;\n-      const XMMRegister xmm_key11 = xmm3;\n-      const XMMRegister xmm_key12 = xmm4;\n-      const XMMRegister key_tmp = xmm4;\n-\n-      __ BIND(L_singleBlock_loopTopHead[k]);\n-      if (k == 1) {\n-        __ addptr(rsp, 6 * wordSize);\n-      } else if (k == 2) {\n-        __ addptr(rsp, 10 * wordSize);\n-      }\n-      __ cmpptr(len_reg, 0); \/\/ any blocks left??\n-      __ jcc(Assembler::equal, L_exit);\n-      __ BIND(L_singleBlock_loopTopHead2[k]);\n-      if (k == 1) {\n-        load_key(xmm_key11, key, 0xb0); \/\/ 0xb0; 192-bit key goes up to 0xc0\n-        load_key(xmm_key12, key, 0xc0); \/\/ 0xc0; 192-bit key goes up to 0xc0\n-      }\n-      if (k == 2) {\n-        load_key(xmm_key11, key, 0xb0); \/\/ 0xb0; 256-bit key goes up to 0xe0\n+    DoFour(pxor, xmm_key_first);\n+    if (k == 0) {\n+      for (int rnum = 1; rnum < ROUNDS[k]; rnum++) {\n+        DoFour(aesdec, as_XMMRegister(rnum + XMM_REG_NUM_KEY_FIRST));\n@@ -4126,7 +4049,4 @@\n-      __ align(OptoLoopAlignment);\n-      __ BIND(L_singleBlock_loopTop[k]);\n-      __ movdqu(xmm_result, Address(from, pos, Address::times_1, 0)); \/\/ get next 16 bytes of cipher input\n-      __ movdqa(xmm_prev_block_cipher_save, xmm_result); \/\/ save for next r vector\n-      __ pxor(xmm_result, xmm_key_first); \/\/ do the aes dec rounds\n-      for (int rnum = 1; rnum <= 9 ; rnum++) {\n-          __ aesdec(xmm_result, as_XMMRegister(rnum + XMM_REG_NUM_KEY_FIRST));\n+      DoFour(aesdeclast, xmm_key_last);\n+    } else if (k == 1) {\n+      for (int rnum = 1; rnum <= ROUNDS[k]-2; rnum++) {\n+        DoFour(aesdec, as_XMMRegister(rnum + XMM_REG_NUM_KEY_FIRST));\n@@ -4134,3 +4054,7 @@\n-      if (k == 1) {\n-        __ aesdec(xmm_result, xmm_key11);\n-        __ aesdec(xmm_result, xmm_key12);\n+      __ movdqu(xmm_key_last, Address(rsp, 0)); \/\/ xmm15 needs to be loaded again.\n+      DoFour(aesdec, xmm1);  \/\/ key : 0xc0\n+      __ movdqu(xmm_prev_block_cipher, Address(rvec, 0x00));  \/\/ xmm1 needs to be loaded again\n+      DoFour(aesdeclast, xmm_key_last);\n+    } else if (k == 2) {\n+      for (int rnum = 1; rnum <= ROUNDS[k] - 4; rnum++) {\n+        DoFour(aesdec, as_XMMRegister(rnum + XMM_REG_NUM_KEY_FIRST));\n@@ -4138,22 +4062,90 @@\n-      if (k == 2) {\n-        __ aesdec(xmm_result, xmm_key11);\n-        load_key(key_tmp, key, 0xc0);\n-        __ aesdec(xmm_result, key_tmp);\n-        load_key(key_tmp, key, 0xd0);\n-        __ aesdec(xmm_result, key_tmp);\n-        load_key(key_tmp, key, 0xe0);\n-        __ aesdec(xmm_result, key_tmp);\n-      }\n-\n-      __ aesdeclast(xmm_result, xmm_key_last); \/\/ xmm15 always came from key+0\n-      __ pxor(xmm_result, xmm_prev_block_cipher); \/\/ xor with the current r vector\n-      __ movdqu(Address(to, pos, Address::times_1, 0), xmm_result); \/\/ store into the next 16 bytes of output\n-      \/\/ no need to store r to memory until we exit\n-      __ movdqa(xmm_prev_block_cipher, xmm_prev_block_cipher_save); \/\/ set up next r vector with cipher input from this block\n-      __ addptr(pos, AESBlockSize);\n-      __ subptr(len_reg, AESBlockSize);\n-      __ jcc(Assembler::notEqual, L_singleBlock_loopTop[k]);\n-      if (k != 2) {\n-        __ jmp(L_exit);\n-      }\n-    } \/\/for 128\/192\/256\n+      DoFour(aesdec, xmm1);  \/\/ key : 0xc0\n+      __ movdqu(xmm15, Address(rsp, 6 * wordSize));\n+      __ movdqu(xmm1, Address(rsp, 8 * wordSize));\n+      DoFour(aesdec, xmm15);  \/\/ key : 0xd0\n+      __ movdqu(xmm_key_last, Address(rsp, 0)); \/\/ xmm15 needs to be loaded again.\n+      DoFour(aesdec, xmm1);  \/\/ key : 0xe0\n+      __ movdqu(xmm_prev_block_cipher, Address(rvec, 0x00));  \/\/ xmm1 needs to be loaded again\n+      DoFour(aesdeclast, xmm_key_last);\n+    }\n+\n+    \/\/ for each result, xor with the r vector of previous cipher block\n+    __ pxor(xmm_result0, xmm_prev_block_cipher);\n+    __ movdqu(xmm_prev_block_cipher, Address(from, pos, Address::times_1, 0 * AESBlockSize));\n+    __ pxor(xmm_result1, xmm_prev_block_cipher);\n+    __ movdqu(xmm_prev_block_cipher, Address(from, pos, Address::times_1, 1 * AESBlockSize));\n+    __ pxor(xmm_result2, xmm_prev_block_cipher);\n+    __ movdqu(xmm_prev_block_cipher, Address(from, pos, Address::times_1, 2 * AESBlockSize));\n+    __ pxor(xmm_result3, xmm_prev_block_cipher);\n+    __ movdqu(xmm_prev_block_cipher, Address(from, pos, Address::times_1, 3 * AESBlockSize));   \/\/ this will carry over to next set of blocks\n+    if (k != 0) {\n+      __ movdqu(Address(rvec, 0x00), xmm_prev_block_cipher);\n+    }\n+\n+    __ movdqu(Address(to, pos, Address::times_1, 0 * AESBlockSize), xmm_result0);     \/\/ store 4 results into the next 64 bytes of output\n+    __ movdqu(Address(to, pos, Address::times_1, 1 * AESBlockSize), xmm_result1);\n+    __ movdqu(Address(to, pos, Address::times_1, 2 * AESBlockSize), xmm_result2);\n+    __ movdqu(Address(to, pos, Address::times_1, 3 * AESBlockSize), xmm_result3);\n+\n+    __ addptr(pos, PARALLEL_FACTOR * AESBlockSize);\n+    __ subptr(len_reg, PARALLEL_FACTOR * AESBlockSize);\n+    __ jmp(L_multiBlock_loopTop[k]);\n+\n+    \/\/ registers used in the non-parallelized loops\n+    \/\/ xmm register assignments for the loops below\n+    const XMMRegister xmm_result = xmm0;\n+    const XMMRegister xmm_prev_block_cipher_save = xmm2;\n+    const XMMRegister xmm_key11 = xmm3;\n+    const XMMRegister xmm_key12 = xmm4;\n+    const XMMRegister key_tmp = xmm4;\n+\n+    __ BIND(L_singleBlock_loopTopHead[k]);\n+    if (k == 1) {\n+      __ addptr(rsp, 6 * wordSize);\n+    } else if (k == 2) {\n+      __ addptr(rsp, 10 * wordSize);\n+    }\n+    __ cmpptr(len_reg, 0); \/\/ any blocks left??\n+    __ jcc(Assembler::equal, L_exit);\n+    __ BIND(L_singleBlock_loopTopHead2[k]);\n+    if (k == 1) {\n+      load_key(xmm_key11, key, 0xb0); \/\/ 0xb0; 192-bit key goes up to 0xc0\n+      load_key(xmm_key12, key, 0xc0); \/\/ 0xc0; 192-bit key goes up to 0xc0\n+    }\n+    if (k == 2) {\n+      load_key(xmm_key11, key, 0xb0); \/\/ 0xb0; 256-bit key goes up to 0xe0\n+    }\n+    __ align(OptoLoopAlignment);\n+    __ BIND(L_singleBlock_loopTop[k]);\n+    __ movdqu(xmm_result, Address(from, pos, Address::times_1, 0)); \/\/ get next 16 bytes of cipher input\n+    __ movdqa(xmm_prev_block_cipher_save, xmm_result); \/\/ save for next r vector\n+    __ pxor(xmm_result, xmm_key_first); \/\/ do the aes dec rounds\n+    for (int rnum = 1; rnum <= 9 ; rnum++) {\n+        __ aesdec(xmm_result, as_XMMRegister(rnum + XMM_REG_NUM_KEY_FIRST));\n+    }\n+    if (k == 1) {\n+      __ aesdec(xmm_result, xmm_key11);\n+      __ aesdec(xmm_result, xmm_key12);\n+    }\n+    if (k == 2) {\n+      __ aesdec(xmm_result, xmm_key11);\n+      load_key(key_tmp, key, 0xc0);\n+      __ aesdec(xmm_result, key_tmp);\n+      load_key(key_tmp, key, 0xd0);\n+      __ aesdec(xmm_result, key_tmp);\n+      load_key(key_tmp, key, 0xe0);\n+      __ aesdec(xmm_result, key_tmp);\n+    }\n+\n+    __ aesdeclast(xmm_result, xmm_key_last); \/\/ xmm15 always came from key+0\n+    __ pxor(xmm_result, xmm_prev_block_cipher); \/\/ xor with the current r vector\n+    __ movdqu(Address(to, pos, Address::times_1, 0), xmm_result); \/\/ store into the next 16 bytes of output\n+    \/\/ no need to store r to memory until we exit\n+    __ movdqa(xmm_prev_block_cipher, xmm_prev_block_cipher_save); \/\/ set up next r vector with cipher input from this block\n+    __ addptr(pos, AESBlockSize);\n+    __ subptr(len_reg, AESBlockSize);\n+    __ jcc(Assembler::notEqual, L_singleBlock_loopTop[k]);\n+    if (k != 2) {\n+      __ jmp(L_exit);\n+    }\n+  } \/\/for 128\/192\/256\n@@ -4161,3 +4153,3 @@\n-    __ BIND(L_exit);\n-    __ movdqu(Address(rvec, 0), xmm_prev_block_cipher);     \/\/ final value of r stored in rvec of CipherBlockChaining object\n-    __ pop(rbx);\n+  __ BIND(L_exit);\n+  __ movdqu(Address(rvec, 0), xmm_prev_block_cipher);     \/\/ final value of r stored in rvec of CipherBlockChaining object\n+  __ pop(rbx);\n@@ -4165,1 +4157,1 @@\n-    __ movl(rax, len_mem);\n+  __ movl(rax, len_mem);\n@@ -4167,1 +4159,1 @@\n-    __ pop(rax); \/\/ return length\n+  __ pop(rax); \/\/ return length\n@@ -4169,19 +4161,22 @@\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-    return start;\n-}\n-\n-  address generate_electronicCodeBook_encryptAESCrypt() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"electronicCodeBook_encryptAESCrypt\");\n-    address start = __ pc();\n-    const Register from = c_rarg0;  \/\/ source array address\n-    const Register to = c_rarg1;  \/\/ destination array address\n-    const Register key = c_rarg2;  \/\/ key array address\n-    const Register len = c_rarg3;  \/\/ src len (must be multiple of blocksize 16)\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ aesecb_encrypt(from, to, key, len);\n-    __ vzeroupper();\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-    return start;\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n+\n+  return start;\n+}\n+\n+address StubGenerator::generate_electronicCodeBook_encryptAESCrypt() {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"electronicCodeBook_encryptAESCrypt\");\n+  address start = __ pc();\n+\n+  const Register from = c_rarg0;  \/\/ source array address\n+  const Register to = c_rarg1;  \/\/ destination array address\n+  const Register key = c_rarg2;  \/\/ key array address\n+  const Register len = c_rarg3;  \/\/ src len (must be multiple of blocksize 16)\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ aesecb_encrypt(from, to, key, len);\n+  __ vzeroupper();\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n+\n+  return start;\n@@ -4190,42 +4185,14 @@\n-  address generate_electronicCodeBook_decryptAESCrypt() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"electronicCodeBook_decryptAESCrypt\");\n-    address start = __ pc();\n-    const Register from = c_rarg0;  \/\/ source array address\n-    const Register to = c_rarg1;  \/\/ destination array address\n-    const Register key = c_rarg2;  \/\/ key array address\n-    const Register len = c_rarg3;  \/\/ src len (must be multiple of blocksize 16)\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ aesecb_decrypt(from, to, key, len);\n-    __ vzeroupper();\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-    return start;\n-  }\n-\n-  \/\/ ofs and limit are use for multi-block byte array.\n-  \/\/ int com.sun.security.provider.MD5.implCompress(byte[] b, int ofs)\n-  address generate_md5_implCompress(bool multi_block, const char *name) {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ pc();\n-\n-    const Register buf_param = r15;\n-    const Address state_param(rsp, 0 * wordSize);\n-    const Address ofs_param  (rsp, 1 * wordSize    );\n-    const Address limit_param(rsp, 1 * wordSize + 4);\n-\n-    __ enter();\n-    __ push(rbx);\n-    __ push(rdi);\n-    __ push(rsi);\n-    __ push(r15);\n-    __ subptr(rsp, 2 * wordSize);\n-\n-    __ movptr(buf_param, c_rarg0);\n-    __ movptr(state_param, c_rarg1);\n-    if (multi_block) {\n-      __ movl(ofs_param, c_rarg2);\n-      __ movl(limit_param, c_rarg3);\n-    }\n-    __ fast_md5(buf_param, state_param, ofs_param, limit_param, multi_block);\n+address StubGenerator::generate_electronicCodeBook_decryptAESCrypt() {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"electronicCodeBook_decryptAESCrypt\");\n+  address start = __ pc();\n+\n+  const Register from = c_rarg0;  \/\/ source array address\n+  const Register to = c_rarg1;  \/\/ destination array address\n+  const Register key = c_rarg2;  \/\/ key array address\n+  const Register len = c_rarg3;  \/\/ src len (must be multiple of blocksize 16)\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ aesecb_decrypt(from, to, key, len);\n+  __ vzeroupper();\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n@@ -4233,9 +4200,2 @@\n-    __ addptr(rsp, 2 * wordSize);\n-    __ pop(r15);\n-    __ pop(rsi);\n-    __ pop(rdi);\n-    __ pop(rbx);\n-    __ leave();\n-    __ ret(0);\n-    return start;\n-  }\n+  return start;\n+}\n@@ -4243,7 +4203,24 @@\n-  address generate_upper_word_mask() {\n-    __ align64();\n-    StubCodeMark mark(this, \"StubRoutines\", \"upper_word_mask\");\n-    address start = __ pc();\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0xFFFFFFFF00000000, relocInfo::none);\n-    return start;\n+\/\/ ofs and limit are use for multi-block byte array.\n+\/\/ int com.sun.security.provider.MD5.implCompress(byte[] b, int ofs)\n+address StubGenerator::generate_md5_implCompress(bool multi_block, const char *name) {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", name);\n+  address start = __ pc();\n+\n+  const Register buf_param = r15;\n+  const Address state_param(rsp, 0 * wordSize);\n+  const Address ofs_param  (rsp, 1 * wordSize    );\n+  const Address limit_param(rsp, 1 * wordSize + 4);\n+\n+  __ enter();\n+  __ push(rbx);\n+  __ push(rdi);\n+  __ push(rsi);\n+  __ push(r15);\n+  __ subptr(rsp, 2 * wordSize);\n+\n+  __ movptr(buf_param, c_rarg0);\n+  __ movptr(state_param, c_rarg1);\n+  if (multi_block) {\n+    __ movl(ofs_param, c_rarg2);\n+    __ movl(limit_param, c_rarg3);\n@@ -4251,0 +4228,1 @@\n+  __ fast_md5(buf_param, state_param, ofs_param, limit_param, multi_block);\n@@ -4252,8 +4230,18 @@\n-  address generate_shuffle_byte_flip_mask() {\n-    __ align64();\n-    StubCodeMark mark(this, \"StubRoutines\", \"shuffle_byte_flip_mask\");\n-    address start = __ pc();\n-    __ emit_data64(0x08090a0b0c0d0e0f, relocInfo::none);\n-    __ emit_data64(0x0001020304050607, relocInfo::none);\n-    return start;\n-  }\n+  __ addptr(rsp, 2 * wordSize);\n+  __ pop(r15);\n+  __ pop(rsi);\n+  __ pop(rdi);\n+  __ pop(rbx);\n+  __ leave();\n+  __ ret(0);\n+\n+  return start;\n+}\n+\n+address StubGenerator::generate_upper_word_mask() {\n+  __ align64();\n+  StubCodeMark mark(this, \"StubRoutines\", \"upper_word_mask\");\n+  address start = __ pc();\n+\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+  __ emit_data64(0xFFFFFFFF00000000, relocInfo::none);\n@@ -4261,6 +4249,2 @@\n-  \/\/ ofs and limit are use for multi-block byte array.\n-  \/\/ int com.sun.security.provider.DigestBase.implCompressMultiBlock(byte[] b, int ofs, int limit)\n-  address generate_sha1_implCompress(bool multi_block, const char *name) {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ pc();\n+  return start;\n+}\n@@ -4268,4 +4252,4 @@\n-    Register buf = c_rarg0;\n-    Register state = c_rarg1;\n-    Register ofs = c_rarg2;\n-    Register limit = c_rarg3;\n+address StubGenerator::generate_shuffle_byte_flip_mask() {\n+  __ align64();\n+  StubCodeMark mark(this, \"StubRoutines\", \"shuffle_byte_flip_mask\");\n+  address start = __ pc();\n@@ -4273,4 +4257,2 @@\n-    const XMMRegister abcd = xmm0;\n-    const XMMRegister e0 = xmm1;\n-    const XMMRegister e1 = xmm2;\n-    const XMMRegister msg0 = xmm3;\n+  __ emit_data64(0x08090a0b0c0d0e0f, relocInfo::none);\n+  __ emit_data64(0x0001020304050607, relocInfo::none);\n@@ -4278,4 +4260,2 @@\n-    const XMMRegister msg1 = xmm4;\n-    const XMMRegister msg2 = xmm5;\n-    const XMMRegister msg3 = xmm6;\n-    const XMMRegister shuf_mask = xmm7;\n+  return start;\n+}\n@@ -4283,1 +4263,6 @@\n-    __ enter();\n+\/\/ ofs and limit are use for multi-block byte array.\n+\/\/ int com.sun.security.provider.DigestBase.implCompressMultiBlock(byte[] b, int ofs, int limit)\n+address StubGenerator::generate_sha1_implCompress(bool multi_block, const char *name) {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", name);\n+  address start = __ pc();\n@@ -4285,1 +4270,4 @@\n-    __ subptr(rsp, 4 * wordSize);\n+  Register buf = c_rarg0;\n+  Register state = c_rarg1;\n+  Register ofs = c_rarg2;\n+  Register limit = c_rarg3;\n@@ -4287,2 +4275,4 @@\n-    __ fast_sha1(abcd, e0, e1, msg0, msg1, msg2, msg3, shuf_mask,\n-      buf, state, ofs, limit, rsp, multi_block);\n+  const XMMRegister abcd = xmm0;\n+  const XMMRegister e0 = xmm1;\n+  const XMMRegister e1 = xmm2;\n+  const XMMRegister msg0 = xmm3;\n@@ -4290,1 +4280,4 @@\n-    __ addptr(rsp, 4 * wordSize);\n+  const XMMRegister msg1 = xmm4;\n+  const XMMRegister msg2 = xmm5;\n+  const XMMRegister msg3 = xmm6;\n+  const XMMRegister shuf_mask = xmm7;\n@@ -4292,4 +4285,1 @@\n-    __ leave();\n-    __ ret(0);\n-    return start;\n-  }\n+  __ enter();\n@@ -4297,6 +4287,1 @@\n-  address generate_pshuffle_byte_flip_mask() {\n-    __ align64();\n-    StubCodeMark mark(this, \"StubRoutines\", \"pshuffle_byte_flip_mask\");\n-    address start = __ pc();\n-    __ emit_data64(0x0405060700010203, relocInfo::none);\n-    __ emit_data64(0x0c0d0e0f08090a0b, relocInfo::none);\n+  __ subptr(rsp, 4 * wordSize);\n@@ -4304,14 +4289,10 @@\n-    if (VM_Version::supports_avx2()) {\n-      __ emit_data64(0x0405060700010203, relocInfo::none); \/\/ second copy\n-      __ emit_data64(0x0c0d0e0f08090a0b, relocInfo::none);\n-      \/\/ _SHUF_00BA\n-      __ emit_data64(0x0b0a090803020100, relocInfo::none);\n-      __ emit_data64(0xFFFFFFFFFFFFFFFF, relocInfo::none);\n-      __ emit_data64(0x0b0a090803020100, relocInfo::none);\n-      __ emit_data64(0xFFFFFFFFFFFFFFFF, relocInfo::none);\n-      \/\/ _SHUF_DC00\n-      __ emit_data64(0xFFFFFFFFFFFFFFFF, relocInfo::none);\n-      __ emit_data64(0x0b0a090803020100, relocInfo::none);\n-      __ emit_data64(0xFFFFFFFFFFFFFFFF, relocInfo::none);\n-      __ emit_data64(0x0b0a090803020100, relocInfo::none);\n-    }\n+  __ fast_sha1(abcd, e0, e1, msg0, msg1, msg2, msg3, shuf_mask,\n+    buf, state, ofs, limit, rsp, multi_block);\n+\n+  __ addptr(rsp, 4 * wordSize);\n+\n+  __ leave();\n+  __ ret(0);\n+\n+  return start;\n+}\n@@ -4319,1 +4300,21 @@\n-    return start;\n+address StubGenerator::generate_pshuffle_byte_flip_mask() {\n+  __ align64();\n+  StubCodeMark mark(this, \"StubRoutines\", \"pshuffle_byte_flip_mask\");\n+  address start = __ pc();\n+\n+  __ emit_data64(0x0405060700010203, relocInfo::none);\n+  __ emit_data64(0x0c0d0e0f08090a0b, relocInfo::none);\n+\n+  if (VM_Version::supports_avx2()) {\n+    __ emit_data64(0x0405060700010203, relocInfo::none); \/\/ second copy\n+    __ emit_data64(0x0c0d0e0f08090a0b, relocInfo::none);\n+    \/\/ _SHUF_00BA\n+    __ emit_data64(0x0b0a090803020100, relocInfo::none);\n+    __ emit_data64(0xFFFFFFFFFFFFFFFF, relocInfo::none);\n+    __ emit_data64(0x0b0a090803020100, relocInfo::none);\n+    __ emit_data64(0xFFFFFFFFFFFFFFFF, relocInfo::none);\n+    \/\/ _SHUF_DC00\n+    __ emit_data64(0xFFFFFFFFFFFFFFFF, relocInfo::none);\n+    __ emit_data64(0x0b0a090803020100, relocInfo::none);\n+    __ emit_data64(0xFFFFFFFFFFFFFFFF, relocInfo::none);\n+    __ emit_data64(0x0b0a090803020100, relocInfo::none);\n@@ -4322,15 +4323,8 @@\n-  \/\/Mask for byte-swapping a couple of qwords in an XMM register using (v)pshufb.\n-  address generate_pshuffle_byte_flip_mask_sha512() {\n-    __ align32();\n-    StubCodeMark mark(this, \"StubRoutines\", \"pshuffle_byte_flip_mask_sha512\");\n-    address start = __ pc();\n-    if (VM_Version::supports_avx2()) {\n-      __ emit_data64(0x0001020304050607, relocInfo::none); \/\/ PSHUFFLE_BYTE_FLIP_MASK\n-      __ emit_data64(0x08090a0b0c0d0e0f, relocInfo::none);\n-      __ emit_data64(0x1011121314151617, relocInfo::none);\n-      __ emit_data64(0x18191a1b1c1d1e1f, relocInfo::none);\n-      __ emit_data64(0x0000000000000000, relocInfo::none); \/\/MASK_YMM_LO\n-      __ emit_data64(0x0000000000000000, relocInfo::none);\n-      __ emit_data64(0xFFFFFFFFFFFFFFFF, relocInfo::none);\n-      __ emit_data64(0xFFFFFFFFFFFFFFFF, relocInfo::none);\n-    }\n+  return start;\n+}\n+\n+\/\/Mask for byte-swapping a couple of qwords in an XMM register using (v)pshufb.\n+address StubGenerator::generate_pshuffle_byte_flip_mask_sha512() {\n+  __ align32();\n+  StubCodeMark mark(this, \"StubRoutines\", \"pshuffle_byte_flip_mask_sha512\");\n+  address start = __ pc();\n@@ -4338,1 +4332,9 @@\n-    return start;\n+  if (VM_Version::supports_avx2()) {\n+    __ emit_data64(0x0001020304050607, relocInfo::none); \/\/ PSHUFFLE_BYTE_FLIP_MASK\n+    __ emit_data64(0x08090a0b0c0d0e0f, relocInfo::none);\n+    __ emit_data64(0x1011121314151617, relocInfo::none);\n+    __ emit_data64(0x18191a1b1c1d1e1f, relocInfo::none);\n+    __ emit_data64(0x0000000000000000, relocInfo::none); \/\/MASK_YMM_LO\n+    __ emit_data64(0x0000000000000000, relocInfo::none);\n+    __ emit_data64(0xFFFFFFFFFFFFFFFF, relocInfo::none);\n+    __ emit_data64(0xFFFFFFFFFFFFFFFF, relocInfo::none);\n@@ -4341,0 +4343,3 @@\n+  return start;\n+}\n+\n@@ -4343,39 +4348,33 @@\n-  address generate_sha256_implCompress(bool multi_block, const char *name) {\n-    assert(VM_Version::supports_sha() || VM_Version::supports_avx2(), \"\");\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ pc();\n-\n-    Register buf = c_rarg0;\n-    Register state = c_rarg1;\n-    Register ofs = c_rarg2;\n-    Register limit = c_rarg3;\n-\n-    const XMMRegister msg = xmm0;\n-    const XMMRegister state0 = xmm1;\n-    const XMMRegister state1 = xmm2;\n-    const XMMRegister msgtmp0 = xmm3;\n-\n-    const XMMRegister msgtmp1 = xmm4;\n-    const XMMRegister msgtmp2 = xmm5;\n-    const XMMRegister msgtmp3 = xmm6;\n-    const XMMRegister msgtmp4 = xmm7;\n-\n-    const XMMRegister shuf_mask = xmm8;\n-\n-    __ enter();\n-\n-    __ subptr(rsp, 4 * wordSize);\n-\n-    if (VM_Version::supports_sha()) {\n-      __ fast_sha256(msg, state0, state1, msgtmp0, msgtmp1, msgtmp2, msgtmp3, msgtmp4,\n-        buf, state, ofs, limit, rsp, multi_block, shuf_mask);\n-    } else if (VM_Version::supports_avx2()) {\n-      __ sha256_AVX2(msg, state0, state1, msgtmp0, msgtmp1, msgtmp2, msgtmp3, msgtmp4,\n-        buf, state, ofs, limit, rsp, multi_block, shuf_mask);\n-    }\n-    __ addptr(rsp, 4 * wordSize);\n-    __ vzeroupper();\n-    __ leave();\n-    __ ret(0);\n-    return start;\n+address StubGenerator::generate_sha256_implCompress(bool multi_block, const char *name) {\n+  assert(VM_Version::supports_sha() || VM_Version::supports_avx2(), \"\");\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", name);\n+  address start = __ pc();\n+\n+  Register buf = c_rarg0;\n+  Register state = c_rarg1;\n+  Register ofs = c_rarg2;\n+  Register limit = c_rarg3;\n+\n+  const XMMRegister msg = xmm0;\n+  const XMMRegister state0 = xmm1;\n+  const XMMRegister state1 = xmm2;\n+  const XMMRegister msgtmp0 = xmm3;\n+\n+  const XMMRegister msgtmp1 = xmm4;\n+  const XMMRegister msgtmp2 = xmm5;\n+  const XMMRegister msgtmp3 = xmm6;\n+  const XMMRegister msgtmp4 = xmm7;\n+\n+  const XMMRegister shuf_mask = xmm8;\n+\n+  __ enter();\n+\n+  __ subptr(rsp, 4 * wordSize);\n+\n+  if (VM_Version::supports_sha()) {\n+    __ fast_sha256(msg, state0, state1, msgtmp0, msgtmp1, msgtmp2, msgtmp3, msgtmp4,\n+      buf, state, ofs, limit, rsp, multi_block, shuf_mask);\n+  } else if (VM_Version::supports_avx2()) {\n+    __ sha256_AVX2(msg, state0, state1, msgtmp0, msgtmp1, msgtmp2, msgtmp3, msgtmp4,\n+      buf, state, ofs, limit, rsp, multi_block, shuf_mask);\n@@ -4383,0 +4382,4 @@\n+  __ addptr(rsp, 4 * wordSize);\n+  __ vzeroupper();\n+  __ leave();\n+  __ ret(0);\n@@ -4384,6 +4387,2 @@\n-  address generate_sha512_implCompress(bool multi_block, const char *name) {\n-    assert(VM_Version::supports_avx2(), \"\");\n-    assert(VM_Version::supports_bmi2(), \"\");\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ pc();\n+  return start;\n+}\n@@ -4391,4 +4390,6 @@\n-    Register buf = c_rarg0;\n-    Register state = c_rarg1;\n-    Register ofs = c_rarg2;\n-    Register limit = c_rarg3;\n+address StubGenerator::generate_sha512_implCompress(bool multi_block, const char *name) {\n+  assert(VM_Version::supports_avx2(), \"\");\n+  assert(VM_Version::supports_bmi2(), \"\");\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", name);\n+  address start = __ pc();\n@@ -4396,8 +4397,4 @@\n-    const XMMRegister msg = xmm0;\n-    const XMMRegister state0 = xmm1;\n-    const XMMRegister state1 = xmm2;\n-    const XMMRegister msgtmp0 = xmm3;\n-    const XMMRegister msgtmp1 = xmm4;\n-    const XMMRegister msgtmp2 = xmm5;\n-    const XMMRegister msgtmp3 = xmm6;\n-    const XMMRegister msgtmp4 = xmm7;\n+  Register buf = c_rarg0;\n+  Register state = c_rarg1;\n+  Register ofs = c_rarg2;\n+  Register limit = c_rarg3;\n@@ -4405,1 +4402,8 @@\n-    const XMMRegister shuf_mask = xmm8;\n+  const XMMRegister msg = xmm0;\n+  const XMMRegister state0 = xmm1;\n+  const XMMRegister state1 = xmm2;\n+  const XMMRegister msgtmp0 = xmm3;\n+  const XMMRegister msgtmp1 = xmm4;\n+  const XMMRegister msgtmp2 = xmm5;\n+  const XMMRegister msgtmp3 = xmm6;\n+  const XMMRegister msgtmp4 = xmm7;\n@@ -4407,1 +4411,1 @@\n-    __ enter();\n+  const XMMRegister shuf_mask = xmm8;\n@@ -4409,2 +4413,1 @@\n-    __ sha512_AVX2(msg, state0, state1, msgtmp0, msgtmp1, msgtmp2, msgtmp3, msgtmp4,\n-    buf, state, ofs, limit, rsp, multi_block, shuf_mask);\n+  __ enter();\n@@ -4412,45 +4415,55 @@\n-    __ vzeroupper();\n-    __ leave();\n-    __ ret(0);\n-    return start;\n-  }\n-\n-  address ghash_polynomial512_addr() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"_ghash_poly512_addr\");\n-    address start = __ pc();\n-    __ emit_data64(0x00000001C2000000, relocInfo::none); \/\/ POLY for reduction\n-    __ emit_data64(0xC200000000000000, relocInfo::none);\n-    __ emit_data64(0x00000001C2000000, relocInfo::none);\n-    __ emit_data64(0xC200000000000000, relocInfo::none);\n-    __ emit_data64(0x00000001C2000000, relocInfo::none);\n-    __ emit_data64(0xC200000000000000, relocInfo::none);\n-    __ emit_data64(0x00000001C2000000, relocInfo::none);\n-    __ emit_data64(0xC200000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000001, relocInfo::none); \/\/ POLY\n-    __ emit_data64(0xC200000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000001, relocInfo::none); \/\/ TWOONE\n-    __ emit_data64(0x0000000100000000, relocInfo::none);\n-    return start;\n-}\n-\n-  \/\/ Vector AES Galois Counter Mode implementation. Parameters:\n-  \/\/ Windows regs            |  Linux regs\n-  \/\/ in = c_rarg0 (rcx)      |  c_rarg0 (rsi)\n-  \/\/ len = c_rarg1 (rdx)     |  c_rarg1 (rdi)\n-  \/\/ ct = c_rarg2 (r8)       |  c_rarg2 (rdx)\n-  \/\/ out = c_rarg3 (r9)      |  c_rarg3 (rcx)\n-  \/\/ key = r10               |  c_rarg4 (r8)\n-  \/\/ state = r13             |  c_rarg5 (r9)\n-  \/\/ subkeyHtbl = r14        |  r11\n-  \/\/ counter = rsi           |  r12\n-  \/\/ return - number of processed bytes\n-  address generate_galoisCounterMode_AESCrypt() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"galoisCounterMode_AESCrypt\");\n-    address start = __ pc();\n-    const Register in = c_rarg0;\n-    const Register len = c_rarg1;\n-    const Register ct = c_rarg2;\n-    const Register out = c_rarg3;\n-    \/\/ and updated with the incremented counter in the end\n+  __ sha512_AVX2(msg, state0, state1, msgtmp0, msgtmp1, msgtmp2, msgtmp3, msgtmp4,\n+  buf, state, ofs, limit, rsp, multi_block, shuf_mask);\n+\n+  __ vzeroupper();\n+  __ leave();\n+  __ ret(0);\n+\n+  return start;\n+}\n+\n+address StubGenerator::ghash_polynomial512_addr() {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"_ghash_poly512_addr\");\n+  address start = __ pc();\n+\n+  __ emit_data64(0x00000001C2000000, relocInfo::none); \/\/ POLY for reduction\n+  __ emit_data64(0xC200000000000000, relocInfo::none);\n+  __ emit_data64(0x00000001C2000000, relocInfo::none);\n+  __ emit_data64(0xC200000000000000, relocInfo::none);\n+  __ emit_data64(0x00000001C2000000, relocInfo::none);\n+  __ emit_data64(0xC200000000000000, relocInfo::none);\n+  __ emit_data64(0x00000001C2000000, relocInfo::none);\n+  __ emit_data64(0xC200000000000000, relocInfo::none);\n+  __ emit_data64(0x0000000000000001, relocInfo::none); \/\/ POLY\n+  __ emit_data64(0xC200000000000000, relocInfo::none);\n+  __ emit_data64(0x0000000000000001, relocInfo::none); \/\/ TWOONE\n+  __ emit_data64(0x0000000100000000, relocInfo::none);\n+\n+  return start;\n+}\n+\n+\/\/ Vector AES Galois Counter Mode implementation.\n+\/\/\n+\/\/ Inputs:           Windows    |   Linux\n+\/\/   in         = rcx (c_rarg0) | rsi (c_rarg0)\n+\/\/   len        = rdx (c_rarg1) | rdi (c_rarg1)\n+\/\/   ct         = r8  (c_rarg2) | rdx (c_rarg2)\n+\/\/   out        = r9  (c_rarg3) | rcx (c_rarg3)\n+\/\/   key        = r10           | r8  (c_rarg4)\n+\/\/   state      = r13           | r9  (c_rarg5)\n+\/\/   subkeyHtbl = r14           | r11\n+\/\/   counter    = rsi           | r12\n+\/\/\n+\/\/ Output:\n+\/\/   rax - number of processed bytes\n+address StubGenerator::generate_galoisCounterMode_AESCrypt() {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"galoisCounterMode_AESCrypt\");\n+  address start = __ pc();\n+\n+  const Register in = c_rarg0;\n+  const Register len = c_rarg1;\n+  const Register ct = c_rarg2;\n+  const Register out = c_rarg3;\n+  \/\/ and updated with the incremented counter in the end\n@@ -4458,7 +4471,7 @@\n-    const Register key = c_rarg4;\n-    const Register state = c_rarg5;\n-    const Address subkeyH_mem(rbp, 2 * wordSize);\n-    const Register subkeyHtbl = r11;\n-    const Register avx512_subkeyHtbl = r13;\n-    const Address counter_mem(rbp, 3 * wordSize);\n-    const Register counter = r12;\n+  const Register key = c_rarg4;\n+  const Register state = c_rarg5;\n+  const Address subkeyH_mem(rbp, 2 * wordSize);\n+  const Register subkeyHtbl = r11;\n+  const Register avx512_subkeyHtbl = r13;\n+  const Address counter_mem(rbp, 3 * wordSize);\n+  const Register counter = r12;\n@@ -4466,9 +4479,9 @@\n-    const Address key_mem(rbp, 6 * wordSize);\n-    const Register key = r10;\n-    const Address state_mem(rbp, 7 * wordSize);\n-    const Register state = r13;\n-    const Address subkeyH_mem(rbp, 8 * wordSize);\n-    const Register subkeyHtbl = r14;\n-    const Register avx512_subkeyHtbl = r12;\n-    const Address counter_mem(rbp, 9 * wordSize);\n-    const Register counter = rsi;\n+  const Address key_mem(rbp, 6 * wordSize);\n+  const Register key = r10;\n+  const Address state_mem(rbp, 7 * wordSize);\n+  const Register state = r13;\n+  const Address subkeyH_mem(rbp, 8 * wordSize);\n+  const Register subkeyHtbl = r14;\n+  const Register avx512_subkeyHtbl = r12;\n+  const Address counter_mem(rbp, 9 * wordSize);\n+  const Register counter = rsi;\n@@ -4476,7 +4489,7 @@\n-    __ enter();\n-   \/\/ Save state before entering routine\n-    __ push(r12);\n-    __ push(r13);\n-    __ push(r14);\n-    __ push(r15);\n-    __ push(rbx);\n+  __ enter();\n+ \/\/ Save state before entering routine\n+  __ push(r12);\n+  __ push(r13);\n+  __ push(r14);\n+  __ push(r15);\n+  __ push(rbx);\n@@ -4484,4 +4497,4 @@\n-    \/\/ on win64, fill len_reg from stack position\n-    __ push(rsi);\n-    __ movptr(key, key_mem);\n-    __ movptr(state, state_mem);\n+  \/\/ on win64, fill len_reg from stack position\n+  __ push(rsi);\n+  __ movptr(key, key_mem);\n+  __ movptr(state, state_mem);\n@@ -4489,2 +4502,2 @@\n-    __ movptr(subkeyHtbl, subkeyH_mem);\n-    __ movptr(counter, counter_mem);\n+  __ movptr(subkeyHtbl, subkeyH_mem);\n+  __ movptr(counter, counter_mem);\n@@ -4492,2 +4505,2 @@\n-    __ push(rbp);\n-    __ movq(rbp, rsp);\n+  __ push(rbp);\n+  __ movq(rbp, rsp);\n@@ -4495,3 +4508,3 @@\n-    __ andq(rsp, -64);\n-    __ subptr(rsp, 96 * longSize); \/\/ Create space on the stack for htbl entries\n-    __ movptr(avx512_subkeyHtbl, rsp);\n+  __ andq(rsp, -64);\n+  __ subptr(rsp, 96 * longSize); \/\/ Create space on the stack for htbl entries\n+  __ movptr(avx512_subkeyHtbl, rsp);\n@@ -4499,2 +4512,2 @@\n-    __ aesgcm_encrypt(in, len, ct, out, key, state, subkeyHtbl, avx512_subkeyHtbl, counter);\n-    __ vzeroupper();\n+  __ aesgcm_encrypt(in, len, ct, out, key, state, subkeyHtbl, avx512_subkeyHtbl, counter);\n+  __ vzeroupper();\n@@ -4502,2 +4515,2 @@\n-    __ movq(rsp, rbp);\n-    __ pop(rbp);\n+  __ movq(rsp, rbp);\n+  __ pop(rbp);\n@@ -4505,1 +4518,1 @@\n-    \/\/ Restore state before leaving routine\n+  \/\/ Restore state before leaving routine\n@@ -4507,1 +4520,1 @@\n-    __ pop(rsi);\n+  __ pop(rsi);\n@@ -4509,5 +4522,5 @@\n-    __ pop(rbx);\n-    __ pop(r15);\n-    __ pop(r14);\n-    __ pop(r13);\n-    __ pop(r12);\n+  __ pop(rbx);\n+  __ pop(r15);\n+  __ pop(r14);\n+  __ pop(r13);\n+  __ pop(r12);\n@@ -4515,4 +4528,2 @@\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-     return start;\n-  }\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n@@ -4520,55 +4531,60 @@\n-  \/\/ This mask is used for incrementing counter value(linc0, linc4, etc.)\n-  address counter_mask_addr() {\n-    __ align64();\n-    StubCodeMark mark(this, \"StubRoutines\", \"counter_mask_addr\");\n-    address start = __ pc();\n-    __ emit_data64(0x08090a0b0c0d0e0f, relocInfo::none);\/\/lbswapmask\n-    __ emit_data64(0x0001020304050607, relocInfo::none);\n-    __ emit_data64(0x08090a0b0c0d0e0f, relocInfo::none);\n-    __ emit_data64(0x0001020304050607, relocInfo::none);\n-    __ emit_data64(0x08090a0b0c0d0e0f, relocInfo::none);\n-    __ emit_data64(0x0001020304050607, relocInfo::none);\n-    __ emit_data64(0x08090a0b0c0d0e0f, relocInfo::none);\n-    __ emit_data64(0x0001020304050607, relocInfo::none);\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\/\/linc0 = counter_mask_addr+64\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000001, relocInfo::none);\/\/counter_mask_addr() + 80\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000002, relocInfo::none);\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000003, relocInfo::none);\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000004, relocInfo::none);\/\/linc4 = counter_mask_addr() + 128\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000004, relocInfo::none);\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000004, relocInfo::none);\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000004, relocInfo::none);\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000008, relocInfo::none);\/\/linc8 = counter_mask_addr() + 192\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000008, relocInfo::none);\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000008, relocInfo::none);\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000008, relocInfo::none);\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000020, relocInfo::none);\/\/linc32 = counter_mask_addr() + 256\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000020, relocInfo::none);\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000020, relocInfo::none);\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000020, relocInfo::none);\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000010, relocInfo::none);\/\/linc16 = counter_mask_addr() + 320\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000010, relocInfo::none);\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000010, relocInfo::none);\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000010, relocInfo::none);\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    return start;\n-  }\n+  return start;\n+}\n+\n+\/\/ This mask is used for incrementing counter value(linc0, linc4, etc.)\n+address StubGenerator::counter_mask_addr() {\n+  __ align64();\n+  StubCodeMark mark(this, \"StubRoutines\", \"counter_mask_addr\");\n+  address start = __ pc();\n+\n+  __ emit_data64(0x08090a0b0c0d0e0f, relocInfo::none);\/\/lbswapmask\n+  __ emit_data64(0x0001020304050607, relocInfo::none);\n+  __ emit_data64(0x08090a0b0c0d0e0f, relocInfo::none);\n+  __ emit_data64(0x0001020304050607, relocInfo::none);\n+  __ emit_data64(0x08090a0b0c0d0e0f, relocInfo::none);\n+  __ emit_data64(0x0001020304050607, relocInfo::none);\n+  __ emit_data64(0x08090a0b0c0d0e0f, relocInfo::none);\n+  __ emit_data64(0x0001020304050607, relocInfo::none);\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\/\/linc0 = counter_mask_addr+64\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+  __ emit_data64(0x0000000000000001, relocInfo::none);\/\/counter_mask_addr() + 80\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+  __ emit_data64(0x0000000000000002, relocInfo::none);\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+  __ emit_data64(0x0000000000000003, relocInfo::none);\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+  __ emit_data64(0x0000000000000004, relocInfo::none);\/\/linc4 = counter_mask_addr() + 128\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+  __ emit_data64(0x0000000000000004, relocInfo::none);\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+  __ emit_data64(0x0000000000000004, relocInfo::none);\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+  __ emit_data64(0x0000000000000004, relocInfo::none);\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+  __ emit_data64(0x0000000000000008, relocInfo::none);\/\/linc8 = counter_mask_addr() + 192\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+  __ emit_data64(0x0000000000000008, relocInfo::none);\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+  __ emit_data64(0x0000000000000008, relocInfo::none);\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+  __ emit_data64(0x0000000000000008, relocInfo::none);\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+  __ emit_data64(0x0000000000000020, relocInfo::none);\/\/linc32 = counter_mask_addr() + 256\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+  __ emit_data64(0x0000000000000020, relocInfo::none);\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+  __ emit_data64(0x0000000000000020, relocInfo::none);\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+  __ emit_data64(0x0000000000000020, relocInfo::none);\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+  __ emit_data64(0x0000000000000010, relocInfo::none);\/\/linc16 = counter_mask_addr() + 320\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+  __ emit_data64(0x0000000000000010, relocInfo::none);\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+  __ emit_data64(0x0000000000000010, relocInfo::none);\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+  __ emit_data64(0x0000000000000010, relocInfo::none);\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+\n+  return start;\n+}\n@@ -4577,9 +4593,10 @@\n-  address generate_counterMode_VectorAESCrypt()  {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"counterMode_AESCrypt\");\n-    address start = __ pc();\n-    const Register from = c_rarg0; \/\/ source array address\n-    const Register to = c_rarg1; \/\/ destination array address\n-    const Register key = c_rarg2; \/\/ key array address r8\n-    const Register counter = c_rarg3; \/\/ counter byte array initialized from counter array address\n-    \/\/ and updated with the incremented counter in the end\n+address StubGenerator::generate_counterMode_VectorAESCrypt()  {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"counterMode_AESCrypt\");\n+  address start = __ pc();\n+\n+  const Register from = c_rarg0; \/\/ source array address\n+  const Register to = c_rarg1; \/\/ destination array address\n+  const Register key = c_rarg2; \/\/ key array address r8\n+  const Register counter = c_rarg3; \/\/ counter byte array initialized from counter array address\n+  \/\/ and updated with the incremented counter in the end\n@@ -4587,5 +4604,5 @@\n-    const Register len_reg = c_rarg4;\n-    const Register saved_encCounter_start = c_rarg5;\n-    const Register used_addr = r10;\n-    const Address  used_mem(rbp, 2 * wordSize);\n-    const Register used = r11;\n+  const Register len_reg = c_rarg4;\n+  const Register saved_encCounter_start = c_rarg5;\n+  const Register used_addr = r10;\n+  const Address  used_mem(rbp, 2 * wordSize);\n+  const Register used = r11;\n@@ -4593,7 +4610,7 @@\n-    const Address len_mem(rbp, 6 * wordSize); \/\/ length is on stack on Win64\n-    const Address saved_encCounter_mem(rbp, 7 * wordSize); \/\/ saved encrypted counter is on stack on Win64\n-    const Address used_mem(rbp, 8 * wordSize); \/\/ used length is on stack on Win64\n-    const Register len_reg = r10; \/\/ pick the first volatile windows register\n-    const Register saved_encCounter_start = r11;\n-    const Register used_addr = r13;\n-    const Register used = r14;\n+  const Address len_mem(rbp, 6 * wordSize); \/\/ length is on stack on Win64\n+  const Address saved_encCounter_mem(rbp, 7 * wordSize); \/\/ saved encrypted counter is on stack on Win64\n+  const Address used_mem(rbp, 8 * wordSize); \/\/ used length is on stack on Win64\n+  const Register len_reg = r10; \/\/ pick the first volatile windows register\n+  const Register saved_encCounter_start = r11;\n+  const Register used_addr = r13;\n+  const Register used = r14;\n@@ -4601,6 +4618,6 @@\n-    __ enter();\n-   \/\/ Save state before entering routine\n-    __ push(r12);\n-    __ push(r13);\n-    __ push(r14);\n-    __ push(r15);\n+  __ enter();\n+ \/\/ Save state before entering routine\n+  __ push(r12);\n+  __ push(r13);\n+  __ push(r14);\n+  __ push(r15);\n@@ -4608,5 +4625,5 @@\n-    \/\/ on win64, fill len_reg from stack position\n-    __ movl(len_reg, len_mem);\n-    __ movptr(saved_encCounter_start, saved_encCounter_mem);\n-    __ movptr(used_addr, used_mem);\n-    __ movl(used, Address(used_addr, 0));\n+  \/\/ on win64, fill len_reg from stack position\n+  __ movl(len_reg, len_mem);\n+  __ movptr(saved_encCounter_start, saved_encCounter_mem);\n+  __ movptr(used_addr, used_mem);\n+  __ movl(used, Address(used_addr, 0));\n@@ -4614,3 +4631,3 @@\n-    __ push(len_reg); \/\/ Save\n-    __ movptr(used_addr, used_mem);\n-    __ movl(used, Address(used_addr, 0));\n+  __ push(len_reg); \/\/ Save\n+  __ movptr(used_addr, used_mem);\n+  __ movl(used, Address(used_addr, 0));\n@@ -4618,5 +4635,5 @@\n-    __ push(rbx);\n-    __ aesctr_encrypt(from, to, key, counter, len_reg, used, used_addr, saved_encCounter_start);\n-    __ vzeroupper();\n-    \/\/ Restore state before leaving routine\n-    __ pop(rbx);\n+  __ push(rbx);\n+  __ aesctr_encrypt(from, to, key, counter, len_reg, used, used_addr, saved_encCounter_start);\n+  __ vzeroupper();\n+  \/\/ Restore state before leaving routine\n+  __ pop(rbx);\n@@ -4624,1 +4641,1 @@\n-    __ movl(rax, len_mem); \/\/ return length\n+  __ movl(rax, len_mem); \/\/ return length\n@@ -4626,1 +4643,1 @@\n-    __ pop(rax); \/\/ return length\n+  __ pop(rax); \/\/ return length\n@@ -4628,4 +4645,4 @@\n-    __ pop(r15);\n-    __ pop(r14);\n-    __ pop(r13);\n-    __ pop(r12);\n+  __ pop(r15);\n+  __ pop(r14);\n+  __ pop(r13);\n+  __ pop(r12);\n@@ -4633,4 +4650,2 @@\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-    return start;\n-  }\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n@@ -4638,32 +4653,36 @@\n-  \/\/ This is a version of CTR\/AES crypt which does 6 blocks in a loop at a time\n-  \/\/ to hide instruction latency\n-  \/\/\n-  \/\/ Arguments:\n-  \/\/\n-  \/\/ Inputs:\n-  \/\/   c_rarg0   - source byte array address\n-  \/\/   c_rarg1   - destination byte array address\n-  \/\/   c_rarg2   - K (key) in little endian int array\n-  \/\/   c_rarg3   - counter vector byte array address\n-  \/\/   Linux\n-  \/\/     c_rarg4   -          input length\n-  \/\/     c_rarg5   -          saved encryptedCounter start\n-  \/\/     rbp + 6 * wordSize - saved used length\n-  \/\/   Windows\n-  \/\/     rbp + 6 * wordSize - input length\n-  \/\/     rbp + 7 * wordSize - saved encryptedCounter start\n-  \/\/     rbp + 8 * wordSize - saved used length\n-  \/\/\n-  \/\/ Output:\n-  \/\/   rax       - input length\n-  \/\/\n-  address generate_counterMode_AESCrypt_Parallel() {\n-    assert(UseAES, \"need AES instructions and misaligned SSE support\");\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"counterMode_AESCrypt\");\n-    address start = __ pc();\n-    const Register from = c_rarg0; \/\/ source array address\n-    const Register to = c_rarg1; \/\/ destination array address\n-    const Register key = c_rarg2; \/\/ key array address\n-    const Register counter = c_rarg3; \/\/ counter byte array initialized from counter array address\n-                                      \/\/ and updated with the incremented counter in the end\n+  return start;\n+}\n+\n+\/\/ This is a version of CTR\/AES crypt which does 6 blocks in a loop at a time\n+\/\/ to hide instruction latency\n+\/\/\n+\/\/ Arguments:\n+\/\/\n+\/\/ Inputs:\n+\/\/   c_rarg0   - source byte array address\n+\/\/   c_rarg1   - destination byte array address\n+\/\/   c_rarg2   - K (key) in little endian int array\n+\/\/   c_rarg3   - counter vector byte array address\n+\/\/   Linux\n+\/\/     c_rarg4   -          input length\n+\/\/     c_rarg5   -          saved encryptedCounter start\n+\/\/     rbp + 6 * wordSize - saved used length\n+\/\/   Windows\n+\/\/     rbp + 6 * wordSize - input length\n+\/\/     rbp + 7 * wordSize - saved encryptedCounter start\n+\/\/     rbp + 8 * wordSize - saved used length\n+\/\/\n+\/\/ Output:\n+\/\/   rax       - input length\n+\/\/\n+address StubGenerator::generate_counterMode_AESCrypt_Parallel() {\n+  assert(UseAES, \"need AES instructions and misaligned SSE support\");\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"counterMode_AESCrypt\");\n+  address start = __ pc();\n+\n+  const Register from = c_rarg0; \/\/ source array address\n+  const Register to = c_rarg1; \/\/ destination array address\n+  const Register key = c_rarg2; \/\/ key array address\n+  const Register counter = c_rarg3; \/\/ counter byte array initialized from counter array address\n+                                    \/\/ and updated with the incremented counter in the end\n@@ -4671,5 +4690,5 @@\n-    const Register len_reg = c_rarg4;\n-    const Register saved_encCounter_start = c_rarg5;\n-    const Register used_addr = r10;\n-    const Address  used_mem(rbp, 2 * wordSize);\n-    const Register used = r11;\n+  const Register len_reg = c_rarg4;\n+  const Register saved_encCounter_start = c_rarg5;\n+  const Register used_addr = r10;\n+  const Address  used_mem(rbp, 2 * wordSize);\n+  const Register used = r11;\n@@ -4677,7 +4696,7 @@\n-    const Address len_mem(rbp, 6 * wordSize); \/\/ length is on stack on Win64\n-    const Address saved_encCounter_mem(rbp, 7 * wordSize); \/\/ length is on stack on Win64\n-    const Address used_mem(rbp, 8 * wordSize); \/\/ length is on stack on Win64\n-    const Register len_reg = r10; \/\/ pick the first volatile windows register\n-    const Register saved_encCounter_start = r11;\n-    const Register used_addr = r13;\n-    const Register used = r14;\n+  const Address len_mem(rbp, 6 * wordSize); \/\/ length is on stack on Win64\n+  const Address saved_encCounter_mem(rbp, 7 * wordSize); \/\/ length is on stack on Win64\n+  const Address used_mem(rbp, 8 * wordSize); \/\/ length is on stack on Win64\n+  const Register len_reg = r10; \/\/ pick the first volatile windows register\n+  const Register saved_encCounter_start = r11;\n+  const Register used_addr = r13;\n+  const Register used = r14;\n@@ -4685,38 +4704,38 @@\n-    const Register pos = rax;\n-\n-    const int PARALLEL_FACTOR = 6;\n-    const XMMRegister xmm_counter_shuf_mask = xmm0;\n-    const XMMRegister xmm_key_shuf_mask = xmm1; \/\/ used temporarily to swap key bytes up front\n-    const XMMRegister xmm_curr_counter = xmm2;\n-\n-    const XMMRegister xmm_key_tmp0 = xmm3;\n-    const XMMRegister xmm_key_tmp1 = xmm4;\n-\n-    \/\/ registers holding the four results in the parallelized loop\n-    const XMMRegister xmm_result0 = xmm5;\n-    const XMMRegister xmm_result1 = xmm6;\n-    const XMMRegister xmm_result2 = xmm7;\n-    const XMMRegister xmm_result3 = xmm8;\n-    const XMMRegister xmm_result4 = xmm9;\n-    const XMMRegister xmm_result5 = xmm10;\n-\n-    const XMMRegister xmm_from0 = xmm11;\n-    const XMMRegister xmm_from1 = xmm12;\n-    const XMMRegister xmm_from2 = xmm13;\n-    const XMMRegister xmm_from3 = xmm14; \/\/the last one is xmm14. we have to preserve it on WIN64.\n-    const XMMRegister xmm_from4 = xmm3; \/\/reuse xmm3~4. Because xmm_key_tmp0~1 are useless when loading input text\n-    const XMMRegister xmm_from5 = xmm4;\n-\n-    \/\/for key_128, key_192, key_256\n-    const int rounds[3] = {10, 12, 14};\n-    Label L_exit_preLoop, L_preLoop_start;\n-    Label L_multiBlock_loopTop[3];\n-    Label L_singleBlockLoopTop[3];\n-    Label L__incCounter[3][6]; \/\/for 6 blocks\n-    Label L__incCounter_single[3]; \/\/for single block, key128, key192, key256\n-    Label L_processTail_insr[3], L_processTail_4_insr[3], L_processTail_2_insr[3], L_processTail_1_insr[3], L_processTail_exit_insr[3];\n-    Label L_processTail_4_extr[3], L_processTail_2_extr[3], L_processTail_1_extr[3], L_processTail_exit_extr[3];\n-\n-    Label L_exit;\n-\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  const Register pos = rax;\n+\n+  const int PARALLEL_FACTOR = 6;\n+  const XMMRegister xmm_counter_shuf_mask = xmm0;\n+  const XMMRegister xmm_key_shuf_mask = xmm1; \/\/ used temporarily to swap key bytes up front\n+  const XMMRegister xmm_curr_counter = xmm2;\n+\n+  const XMMRegister xmm_key_tmp0 = xmm3;\n+  const XMMRegister xmm_key_tmp1 = xmm4;\n+\n+  \/\/ registers holding the four results in the parallelized loop\n+  const XMMRegister xmm_result0 = xmm5;\n+  const XMMRegister xmm_result1 = xmm6;\n+  const XMMRegister xmm_result2 = xmm7;\n+  const XMMRegister xmm_result3 = xmm8;\n+  const XMMRegister xmm_result4 = xmm9;\n+  const XMMRegister xmm_result5 = xmm10;\n+\n+  const XMMRegister xmm_from0 = xmm11;\n+  const XMMRegister xmm_from1 = xmm12;\n+  const XMMRegister xmm_from2 = xmm13;\n+  const XMMRegister xmm_from3 = xmm14; \/\/the last one is xmm14. we have to preserve it on WIN64.\n+  const XMMRegister xmm_from4 = xmm3; \/\/reuse xmm3~4. Because xmm_key_tmp0~1 are useless when loading input text\n+  const XMMRegister xmm_from5 = xmm4;\n+\n+  \/\/for key_128, key_192, key_256\n+  const int rounds[3] = {10, 12, 14};\n+  Label L_exit_preLoop, L_preLoop_start;\n+  Label L_multiBlock_loopTop[3];\n+  Label L_singleBlockLoopTop[3];\n+  Label L__incCounter[3][6]; \/\/for 6 blocks\n+  Label L__incCounter_single[3]; \/\/for single block, key128, key192, key256\n+  Label L_processTail_insr[3], L_processTail_4_insr[3], L_processTail_2_insr[3], L_processTail_1_insr[3], L_processTail_exit_insr[3];\n+  Label L_processTail_4_extr[3], L_processTail_2_extr[3], L_processTail_1_extr[3], L_processTail_exit_extr[3];\n+\n+  Label L_exit;\n+\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n@@ -4725,14 +4744,14 @@\n-    \/\/ allocate spill slots for r13, r14\n-    enum {\n-        saved_r13_offset,\n-        saved_r14_offset\n-    };\n-    __ subptr(rsp, 2 * wordSize);\n-    __ movptr(Address(rsp, saved_r13_offset * wordSize), r13);\n-    __ movptr(Address(rsp, saved_r14_offset * wordSize), r14);\n-\n-    \/\/ on win64, fill len_reg from stack position\n-    __ movl(len_reg, len_mem);\n-    __ movptr(saved_encCounter_start, saved_encCounter_mem);\n-    __ movptr(used_addr, used_mem);\n-    __ movl(used, Address(used_addr, 0));\n+  \/\/ allocate spill slots for r13, r14\n+  enum {\n+      saved_r13_offset,\n+      saved_r14_offset\n+  };\n+  __ subptr(rsp, 2 * wordSize);\n+  __ movptr(Address(rsp, saved_r13_offset * wordSize), r13);\n+  __ movptr(Address(rsp, saved_r14_offset * wordSize), r14);\n+\n+  \/\/ on win64, fill len_reg from stack position\n+  __ movl(len_reg, len_mem);\n+  __ movptr(saved_encCounter_start, saved_encCounter_mem);\n+  __ movptr(used_addr, used_mem);\n+  __ movl(used, Address(used_addr, 0));\n@@ -4740,3 +4759,3 @@\n-    __ push(len_reg); \/\/ Save\n-    __ movptr(used_addr, used_mem);\n-    __ movl(used, Address(used_addr, 0));\n+  __ push(len_reg); \/\/ Save\n+  __ movptr(used_addr, used_mem);\n+  __ movl(used, Address(used_addr, 0));\n@@ -4745,31 +4764,31 @@\n-    __ push(rbx); \/\/ Save RBX\n-    __ movdqu(xmm_curr_counter, Address(counter, 0x00)); \/\/ initialize counter with initial counter\n-    __ movdqu(xmm_counter_shuf_mask, ExternalAddress(StubRoutines::x86::counter_shuffle_mask_addr()), pos); \/\/ pos as scratch\n-    __ pshufb(xmm_curr_counter, xmm_counter_shuf_mask); \/\/counter is shuffled\n-    __ movptr(pos, 0);\n-\n-    \/\/ Use the partially used encrpyted counter from last invocation\n-    __ BIND(L_preLoop_start);\n-    __ cmpptr(used, 16);\n-    __ jcc(Assembler::aboveEqual, L_exit_preLoop);\n-      __ cmpptr(len_reg, 0);\n-      __ jcc(Assembler::lessEqual, L_exit_preLoop);\n-      __ movb(rbx, Address(saved_encCounter_start, used));\n-      __ xorb(rbx, Address(from, pos));\n-      __ movb(Address(to, pos), rbx);\n-      __ addptr(pos, 1);\n-      __ addptr(used, 1);\n-      __ subptr(len_reg, 1);\n-\n-    __ jmp(L_preLoop_start);\n-\n-    __ BIND(L_exit_preLoop);\n-    __ movl(Address(used_addr, 0), used);\n-\n-    \/\/ key length could be only {11, 13, 15} * 4 = {44, 52, 60}\n-    __ movdqu(xmm_key_shuf_mask, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()), rbx); \/\/ rbx as scratch\n-    __ movl(rbx, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));\n-    __ cmpl(rbx, 52);\n-    __ jcc(Assembler::equal, L_multiBlock_loopTop[1]);\n-    __ cmpl(rbx, 60);\n-    __ jcc(Assembler::equal, L_multiBlock_loopTop[2]);\n+  __ push(rbx); \/\/ Save RBX\n+  __ movdqu(xmm_curr_counter, Address(counter, 0x00)); \/\/ initialize counter with initial counter\n+  __ movdqu(xmm_counter_shuf_mask, ExternalAddress(StubRoutines::x86::counter_shuffle_mask_addr()), pos); \/\/ pos as scratch\n+  __ pshufb(xmm_curr_counter, xmm_counter_shuf_mask); \/\/counter is shuffled\n+  __ movptr(pos, 0);\n+\n+  \/\/ Use the partially used encrpyted counter from last invocation\n+  __ BIND(L_preLoop_start);\n+  __ cmpptr(used, 16);\n+  __ jcc(Assembler::aboveEqual, L_exit_preLoop);\n+    __ cmpptr(len_reg, 0);\n+    __ jcc(Assembler::lessEqual, L_exit_preLoop);\n+    __ movb(rbx, Address(saved_encCounter_start, used));\n+    __ xorb(rbx, Address(from, pos));\n+    __ movb(Address(to, pos), rbx);\n+    __ addptr(pos, 1);\n+    __ addptr(used, 1);\n+    __ subptr(len_reg, 1);\n+\n+  __ jmp(L_preLoop_start);\n+\n+  __ BIND(L_exit_preLoop);\n+  __ movl(Address(used_addr, 0), used);\n+\n+  \/\/ key length could be only {11, 13, 15} * 4 = {44, 52, 60}\n+  __ movdqu(xmm_key_shuf_mask, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()), rbx); \/\/ rbx as scratch\n+  __ movl(rbx, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));\n+  __ cmpl(rbx, 52);\n+  __ jcc(Assembler::equal, L_multiBlock_loopTop[1]);\n+  __ cmpl(rbx, 60);\n+  __ jcc(Assembler::equal, L_multiBlock_loopTop[2]);\n@@ -4778,41 +4797,39 @@\n-    __ opc(xmm_result0, src_reg);              \\\n-    __ opc(xmm_result1, src_reg);              \\\n-    __ opc(xmm_result2, src_reg);              \\\n-    __ opc(xmm_result3, src_reg);              \\\n-    __ opc(xmm_result4, src_reg);              \\\n-    __ opc(xmm_result5, src_reg);\n-\n-    \/\/ k == 0 :  generate code for key_128\n-    \/\/ k == 1 :  generate code for key_192\n-    \/\/ k == 2 :  generate code for key_256\n-    for (int k = 0; k < 3; ++k) {\n-      \/\/multi blocks starts here\n-      __ align(OptoLoopAlignment);\n-      __ BIND(L_multiBlock_loopTop[k]);\n-      __ cmpptr(len_reg, PARALLEL_FACTOR * AESBlockSize); \/\/ see if at least PARALLEL_FACTOR blocks left\n-      __ jcc(Assembler::less, L_singleBlockLoopTop[k]);\n-      load_key(xmm_key_tmp0, key, 0x00, xmm_key_shuf_mask);\n-\n-      \/\/load, then increase counters\n-      CTR_DoSix(movdqa, xmm_curr_counter);\n-      inc_counter(rbx, xmm_result1, 0x01, L__incCounter[k][0]);\n-      inc_counter(rbx, xmm_result2, 0x02, L__incCounter[k][1]);\n-      inc_counter(rbx, xmm_result3, 0x03, L__incCounter[k][2]);\n-      inc_counter(rbx, xmm_result4, 0x04, L__incCounter[k][3]);\n-      inc_counter(rbx, xmm_result5,  0x05, L__incCounter[k][4]);\n-      inc_counter(rbx, xmm_curr_counter, 0x06, L__incCounter[k][5]);\n-      CTR_DoSix(pshufb, xmm_counter_shuf_mask); \/\/ after increased, shuffled counters back for PXOR\n-      CTR_DoSix(pxor, xmm_key_tmp0);   \/\/PXOR with Round 0 key\n-\n-      \/\/load two ROUND_KEYs at a time\n-      for (int i = 1; i < rounds[k]; ) {\n-        load_key(xmm_key_tmp1, key, (0x10 * i), xmm_key_shuf_mask);\n-        load_key(xmm_key_tmp0, key, (0x10 * (i+1)), xmm_key_shuf_mask);\n-        CTR_DoSix(aesenc, xmm_key_tmp1);\n-        i++;\n-        if (i != rounds[k]) {\n-          CTR_DoSix(aesenc, xmm_key_tmp0);\n-        } else {\n-          CTR_DoSix(aesenclast, xmm_key_tmp0);\n-        }\n-        i++;\n+  __ opc(xmm_result0, src_reg);              \\\n+  __ opc(xmm_result1, src_reg);              \\\n+  __ opc(xmm_result2, src_reg);              \\\n+  __ opc(xmm_result3, src_reg);              \\\n+  __ opc(xmm_result4, src_reg);              \\\n+  __ opc(xmm_result5, src_reg);\n+\n+  \/\/ k == 0 :  generate code for key_128\n+  \/\/ k == 1 :  generate code for key_192\n+  \/\/ k == 2 :  generate code for key_256\n+  for (int k = 0; k < 3; ++k) {\n+    \/\/multi blocks starts here\n+    __ align(OptoLoopAlignment);\n+    __ BIND(L_multiBlock_loopTop[k]);\n+    __ cmpptr(len_reg, PARALLEL_FACTOR * AESBlockSize); \/\/ see if at least PARALLEL_FACTOR blocks left\n+    __ jcc(Assembler::less, L_singleBlockLoopTop[k]);\n+    load_key(xmm_key_tmp0, key, 0x00, xmm_key_shuf_mask);\n+\n+    \/\/load, then increase counters\n+    CTR_DoSix(movdqa, xmm_curr_counter);\n+    inc_counter(rbx, xmm_result1, 0x01, L__incCounter[k][0]);\n+    inc_counter(rbx, xmm_result2, 0x02, L__incCounter[k][1]);\n+    inc_counter(rbx, xmm_result3, 0x03, L__incCounter[k][2]);\n+    inc_counter(rbx, xmm_result4, 0x04, L__incCounter[k][3]);\n+    inc_counter(rbx, xmm_result5,  0x05, L__incCounter[k][4]);\n+    inc_counter(rbx, xmm_curr_counter, 0x06, L__incCounter[k][5]);\n+    CTR_DoSix(pshufb, xmm_counter_shuf_mask); \/\/ after increased, shuffled counters back for PXOR\n+    CTR_DoSix(pxor, xmm_key_tmp0);   \/\/PXOR with Round 0 key\n+\n+    \/\/load two ROUND_KEYs at a time\n+    for (int i = 1; i < rounds[k]; ) {\n+      load_key(xmm_key_tmp1, key, (0x10 * i), xmm_key_shuf_mask);\n+      load_key(xmm_key_tmp0, key, (0x10 * (i+1)), xmm_key_shuf_mask);\n+      CTR_DoSix(aesenc, xmm_key_tmp1);\n+      i++;\n+      if (i != rounds[k]) {\n+        CTR_DoSix(aesenc, xmm_key_tmp0);\n+      } else {\n+        CTR_DoSix(aesenclast, xmm_key_tmp0);\n@@ -4820,2 +4837,48 @@\n-\n-      \/\/ get next PARALLEL_FACTOR blocks into xmm_result registers\n+      i++;\n+    }\n+\n+    \/\/ get next PARALLEL_FACTOR blocks into xmm_result registers\n+    __ movdqu(xmm_from0, Address(from, pos, Address::times_1, 0 * AESBlockSize));\n+    __ movdqu(xmm_from1, Address(from, pos, Address::times_1, 1 * AESBlockSize));\n+    __ movdqu(xmm_from2, Address(from, pos, Address::times_1, 2 * AESBlockSize));\n+    __ movdqu(xmm_from3, Address(from, pos, Address::times_1, 3 * AESBlockSize));\n+    __ movdqu(xmm_from4, Address(from, pos, Address::times_1, 4 * AESBlockSize));\n+    __ movdqu(xmm_from5, Address(from, pos, Address::times_1, 5 * AESBlockSize));\n+\n+    __ pxor(xmm_result0, xmm_from0);\n+    __ pxor(xmm_result1, xmm_from1);\n+    __ pxor(xmm_result2, xmm_from2);\n+    __ pxor(xmm_result3, xmm_from3);\n+    __ pxor(xmm_result4, xmm_from4);\n+    __ pxor(xmm_result5, xmm_from5);\n+\n+    \/\/ store 6 results into the next 64 bytes of output\n+    __ movdqu(Address(to, pos, Address::times_1, 0 * AESBlockSize), xmm_result0);\n+    __ movdqu(Address(to, pos, Address::times_1, 1 * AESBlockSize), xmm_result1);\n+    __ movdqu(Address(to, pos, Address::times_1, 2 * AESBlockSize), xmm_result2);\n+    __ movdqu(Address(to, pos, Address::times_1, 3 * AESBlockSize), xmm_result3);\n+    __ movdqu(Address(to, pos, Address::times_1, 4 * AESBlockSize), xmm_result4);\n+    __ movdqu(Address(to, pos, Address::times_1, 5 * AESBlockSize), xmm_result5);\n+\n+    __ addptr(pos, PARALLEL_FACTOR * AESBlockSize); \/\/ increase the length of crypt text\n+    __ subptr(len_reg, PARALLEL_FACTOR * AESBlockSize); \/\/ decrease the remaining length\n+    __ jmp(L_multiBlock_loopTop[k]);\n+\n+    \/\/ singleBlock starts here\n+    __ align(OptoLoopAlignment);\n+    __ BIND(L_singleBlockLoopTop[k]);\n+    __ cmpptr(len_reg, 0);\n+    __ jcc(Assembler::lessEqual, L_exit);\n+    load_key(xmm_key_tmp0, key, 0x00, xmm_key_shuf_mask);\n+    __ movdqa(xmm_result0, xmm_curr_counter);\n+    inc_counter(rbx, xmm_curr_counter, 0x01, L__incCounter_single[k]);\n+    __ pshufb(xmm_result0, xmm_counter_shuf_mask);\n+    __ pxor(xmm_result0, xmm_key_tmp0);\n+    for (int i = 1; i < rounds[k]; i++) {\n+      load_key(xmm_key_tmp0, key, (0x10 * i), xmm_key_shuf_mask);\n+      __ aesenc(xmm_result0, xmm_key_tmp0);\n+    }\n+    load_key(xmm_key_tmp0, key, (rounds[k] * 0x10), xmm_key_shuf_mask);\n+    __ aesenclast(xmm_result0, xmm_key_tmp0);\n+    __ cmpptr(len_reg, AESBlockSize);\n+    __ jcc(Assembler::less, L_processTail_insr[k]);\n@@ -4823,6 +4886,0 @@\n-      __ movdqu(xmm_from1, Address(from, pos, Address::times_1, 1 * AESBlockSize));\n-      __ movdqu(xmm_from2, Address(from, pos, Address::times_1, 2 * AESBlockSize));\n-      __ movdqu(xmm_from3, Address(from, pos, Address::times_1, 3 * AESBlockSize));\n-      __ movdqu(xmm_from4, Address(from, pos, Address::times_1, 4 * AESBlockSize));\n-      __ movdqu(xmm_from5, Address(from, pos, Address::times_1, 5 * AESBlockSize));\n-\n@@ -4830,7 +4887,0 @@\n-      __ pxor(xmm_result1, xmm_from1);\n-      __ pxor(xmm_result2, xmm_from2);\n-      __ pxor(xmm_result3, xmm_from3);\n-      __ pxor(xmm_result4, xmm_from4);\n-      __ pxor(xmm_result5, xmm_from5);\n-\n-      \/\/ store 6 results into the next 64 bytes of output\n@@ -4838,90 +4888,58 @@\n-      __ movdqu(Address(to, pos, Address::times_1, 1 * AESBlockSize), xmm_result1);\n-      __ movdqu(Address(to, pos, Address::times_1, 2 * AESBlockSize), xmm_result2);\n-      __ movdqu(Address(to, pos, Address::times_1, 3 * AESBlockSize), xmm_result3);\n-      __ movdqu(Address(to, pos, Address::times_1, 4 * AESBlockSize), xmm_result4);\n-      __ movdqu(Address(to, pos, Address::times_1, 5 * AESBlockSize), xmm_result5);\n-\n-      __ addptr(pos, PARALLEL_FACTOR * AESBlockSize); \/\/ increase the length of crypt text\n-      __ subptr(len_reg, PARALLEL_FACTOR * AESBlockSize); \/\/ decrease the remaining length\n-      __ jmp(L_multiBlock_loopTop[k]);\n-\n-      \/\/ singleBlock starts here\n-      __ align(OptoLoopAlignment);\n-      __ BIND(L_singleBlockLoopTop[k]);\n-      __ cmpptr(len_reg, 0);\n-      __ jcc(Assembler::lessEqual, L_exit);\n-      load_key(xmm_key_tmp0, key, 0x00, xmm_key_shuf_mask);\n-      __ movdqa(xmm_result0, xmm_curr_counter);\n-      inc_counter(rbx, xmm_curr_counter, 0x01, L__incCounter_single[k]);\n-      __ pshufb(xmm_result0, xmm_counter_shuf_mask);\n-      __ pxor(xmm_result0, xmm_key_tmp0);\n-      for (int i = 1; i < rounds[k]; i++) {\n-        load_key(xmm_key_tmp0, key, (0x10 * i), xmm_key_shuf_mask);\n-        __ aesenc(xmm_result0, xmm_key_tmp0);\n-      }\n-      load_key(xmm_key_tmp0, key, (rounds[k] * 0x10), xmm_key_shuf_mask);\n-      __ aesenclast(xmm_result0, xmm_key_tmp0);\n-      __ cmpptr(len_reg, AESBlockSize);\n-      __ jcc(Assembler::less, L_processTail_insr[k]);\n-        __ movdqu(xmm_from0, Address(from, pos, Address::times_1, 0 * AESBlockSize));\n-        __ pxor(xmm_result0, xmm_from0);\n-        __ movdqu(Address(to, pos, Address::times_1, 0 * AESBlockSize), xmm_result0);\n-        __ addptr(pos, AESBlockSize);\n-        __ subptr(len_reg, AESBlockSize);\n-        __ jmp(L_singleBlockLoopTop[k]);\n-      __ BIND(L_processTail_insr[k]);                               \/\/ Process the tail part of the input array\n-        __ addptr(pos, len_reg);                                    \/\/ 1. Insert bytes from src array into xmm_from0 register\n-        __ testptr(len_reg, 8);\n-        __ jcc(Assembler::zero, L_processTail_4_insr[k]);\n-          __ subptr(pos,8);\n-          __ pinsrq(xmm_from0, Address(from, pos), 0);\n-        __ BIND(L_processTail_4_insr[k]);\n-        __ testptr(len_reg, 4);\n-        __ jcc(Assembler::zero, L_processTail_2_insr[k]);\n-          __ subptr(pos,4);\n-          __ pslldq(xmm_from0, 4);\n-          __ pinsrd(xmm_from0, Address(from, pos), 0);\n-        __ BIND(L_processTail_2_insr[k]);\n-        __ testptr(len_reg, 2);\n-        __ jcc(Assembler::zero, L_processTail_1_insr[k]);\n-          __ subptr(pos, 2);\n-          __ pslldq(xmm_from0, 2);\n-          __ pinsrw(xmm_from0, Address(from, pos), 0);\n-        __ BIND(L_processTail_1_insr[k]);\n-        __ testptr(len_reg, 1);\n-        __ jcc(Assembler::zero, L_processTail_exit_insr[k]);\n-          __ subptr(pos, 1);\n-          __ pslldq(xmm_from0, 1);\n-          __ pinsrb(xmm_from0, Address(from, pos), 0);\n-        __ BIND(L_processTail_exit_insr[k]);\n-\n-        __ movdqu(Address(saved_encCounter_start, 0), xmm_result0);  \/\/ 2. Perform pxor of the encrypted counter and plaintext Bytes.\n-        __ pxor(xmm_result0, xmm_from0);                             \/\/    Also the encrypted counter is saved for next invocation.\n-\n-        __ testptr(len_reg, 8);\n-        __ jcc(Assembler::zero, L_processTail_4_extr[k]);            \/\/ 3. Extract bytes from xmm_result0 into the dest. array\n-          __ pextrq(Address(to, pos), xmm_result0, 0);\n-          __ psrldq(xmm_result0, 8);\n-          __ addptr(pos, 8);\n-        __ BIND(L_processTail_4_extr[k]);\n-        __ testptr(len_reg, 4);\n-        __ jcc(Assembler::zero, L_processTail_2_extr[k]);\n-          __ pextrd(Address(to, pos), xmm_result0, 0);\n-          __ psrldq(xmm_result0, 4);\n-          __ addptr(pos, 4);\n-        __ BIND(L_processTail_2_extr[k]);\n-        __ testptr(len_reg, 2);\n-        __ jcc(Assembler::zero, L_processTail_1_extr[k]);\n-          __ pextrw(Address(to, pos), xmm_result0, 0);\n-          __ psrldq(xmm_result0, 2);\n-          __ addptr(pos, 2);\n-        __ BIND(L_processTail_1_extr[k]);\n-        __ testptr(len_reg, 1);\n-        __ jcc(Assembler::zero, L_processTail_exit_extr[k]);\n-          __ pextrb(Address(to, pos), xmm_result0, 0);\n-\n-        __ BIND(L_processTail_exit_extr[k]);\n-        __ movl(Address(used_addr, 0), len_reg);\n-        __ jmp(L_exit);\n-\n-    }\n+      __ addptr(pos, AESBlockSize);\n+      __ subptr(len_reg, AESBlockSize);\n+      __ jmp(L_singleBlockLoopTop[k]);\n+    __ BIND(L_processTail_insr[k]);                               \/\/ Process the tail part of the input array\n+      __ addptr(pos, len_reg);                                    \/\/ 1. Insert bytes from src array into xmm_from0 register\n+      __ testptr(len_reg, 8);\n+      __ jcc(Assembler::zero, L_processTail_4_insr[k]);\n+        __ subptr(pos,8);\n+        __ pinsrq(xmm_from0, Address(from, pos), 0);\n+      __ BIND(L_processTail_4_insr[k]);\n+      __ testptr(len_reg, 4);\n+      __ jcc(Assembler::zero, L_processTail_2_insr[k]);\n+        __ subptr(pos,4);\n+        __ pslldq(xmm_from0, 4);\n+        __ pinsrd(xmm_from0, Address(from, pos), 0);\n+      __ BIND(L_processTail_2_insr[k]);\n+      __ testptr(len_reg, 2);\n+      __ jcc(Assembler::zero, L_processTail_1_insr[k]);\n+        __ subptr(pos, 2);\n+        __ pslldq(xmm_from0, 2);\n+        __ pinsrw(xmm_from0, Address(from, pos), 0);\n+      __ BIND(L_processTail_1_insr[k]);\n+      __ testptr(len_reg, 1);\n+      __ jcc(Assembler::zero, L_processTail_exit_insr[k]);\n+        __ subptr(pos, 1);\n+        __ pslldq(xmm_from0, 1);\n+        __ pinsrb(xmm_from0, Address(from, pos), 0);\n+      __ BIND(L_processTail_exit_insr[k]);\n+\n+      __ movdqu(Address(saved_encCounter_start, 0), xmm_result0);  \/\/ 2. Perform pxor of the encrypted counter and plaintext Bytes.\n+      __ pxor(xmm_result0, xmm_from0);                             \/\/    Also the encrypted counter is saved for next invocation.\n+\n+      __ testptr(len_reg, 8);\n+      __ jcc(Assembler::zero, L_processTail_4_extr[k]);            \/\/ 3. Extract bytes from xmm_result0 into the dest. array\n+        __ pextrq(Address(to, pos), xmm_result0, 0);\n+        __ psrldq(xmm_result0, 8);\n+        __ addptr(pos, 8);\n+      __ BIND(L_processTail_4_extr[k]);\n+      __ testptr(len_reg, 4);\n+      __ jcc(Assembler::zero, L_processTail_2_extr[k]);\n+        __ pextrd(Address(to, pos), xmm_result0, 0);\n+        __ psrldq(xmm_result0, 4);\n+        __ addptr(pos, 4);\n+      __ BIND(L_processTail_2_extr[k]);\n+      __ testptr(len_reg, 2);\n+      __ jcc(Assembler::zero, L_processTail_1_extr[k]);\n+        __ pextrw(Address(to, pos), xmm_result0, 0);\n+        __ psrldq(xmm_result0, 2);\n+        __ addptr(pos, 2);\n+      __ BIND(L_processTail_1_extr[k]);\n+      __ testptr(len_reg, 1);\n+      __ jcc(Assembler::zero, L_processTail_exit_extr[k]);\n+        __ pextrb(Address(to, pos), xmm_result0, 0);\n+\n+      __ BIND(L_processTail_exit_extr[k]);\n+      __ movl(Address(used_addr, 0), len_reg);\n+      __ jmp(L_exit);\n+  }\n@@ -4929,4 +4947,4 @@\n-    __ BIND(L_exit);\n-    __ pshufb(xmm_curr_counter, xmm_counter_shuf_mask); \/\/counter is shuffled back.\n-    __ movdqu(Address(counter, 0), xmm_curr_counter); \/\/save counter back\n-    __ pop(rbx); \/\/ pop the saved RBX.\n+  __ BIND(L_exit);\n+  __ pshufb(xmm_curr_counter, xmm_counter_shuf_mask); \/\/counter is shuffled back.\n+  __ movdqu(Address(counter, 0), xmm_curr_counter); \/\/save counter back\n+  __ pop(rbx); \/\/ pop the saved RBX.\n@@ -4934,4 +4952,4 @@\n-    __ movl(rax, len_mem);\n-    __ movptr(r13, Address(rsp, saved_r13_offset * wordSize));\n-    __ movptr(r14, Address(rsp, saved_r14_offset * wordSize));\n-    __ addptr(rsp, 2 * wordSize);\n+  __ movl(rax, len_mem);\n+  __ movptr(r13, Address(rsp, saved_r13_offset * wordSize));\n+  __ movptr(r14, Address(rsp, saved_r14_offset * wordSize));\n+  __ addptr(rsp, 2 * wordSize);\n@@ -4939,1 +4957,1 @@\n-    __ pop(rax); \/\/ return 'len'\n+  __ pop(rax); \/\/ return 'len'\n@@ -4941,4 +4959,2 @@\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-    return start;\n-  }\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n@@ -4946,1 +4962,4 @@\n-void roundDec(XMMRegister xmm_reg) {\n+  return start;\n+}\n+\n+void StubGenerator::roundDec(XMMRegister xmm_reg) {\n@@ -4957,1 +4976,1 @@\n-void roundDeclast(XMMRegister xmm_reg) {\n+void StubGenerator::roundDeclast(XMMRegister xmm_reg) {\n@@ -4968,1 +4987,1 @@\n-void ev_load_key(XMMRegister xmmdst, Register key, int offset, XMMRegister xmm_shuf_mask = xnoreg) {\n+void StubGenerator::ev_load_key(XMMRegister xmmdst, Register key, int offset, XMMRegister xmm_shuf_mask) {\n@@ -4976,0 +4995,332 @@\n+}\n+\n+address StubGenerator::generate_cipherBlockChaining_decryptVectorAESCrypt() {\n+  assert(VM_Version::supports_avx512_vaes(), \"need AES instructions and misaligned SSE support\");\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"cipherBlockChaining_decryptAESCrypt\");\n+  address start = __ pc();\n+\n+  const Register from = c_rarg0;  \/\/ source array address\n+  const Register to = c_rarg1;  \/\/ destination array address\n+  const Register key = c_rarg2;  \/\/ key array address\n+  const Register rvec = c_rarg3;  \/\/ r byte array initialized from initvector array address\n+  \/\/ and left with the results of the last encryption block\n+#ifndef _WIN64\n+  const Register len_reg = c_rarg4;  \/\/ src len (must be multiple of blocksize 16)\n+#else\n+  const Address  len_mem(rbp, 6 * wordSize);  \/\/ length is on stack on Win64\n+  const Register len_reg = r11;      \/\/ pick the volatile windows register\n+#endif\n+\n+  Label Loop, Loop1, L_128, L_256, L_192, KEY_192, KEY_256, Loop2, Lcbc_dec_rem_loop,\n+        Lcbc_dec_rem_last, Lcbc_dec_ret, Lcbc_dec_rem, Lcbc_exit;\n+\n+  __ enter();\n+\n+#ifdef _WIN64\n+\/\/ on win64, fill len_reg from stack position\n+  __ movl(len_reg, len_mem);\n+#else\n+  __ push(len_reg); \/\/ Save\n+#endif\n+  __ push(rbx);\n+  __ vzeroupper();\n+\n+  \/\/ Temporary variable declaration for swapping key bytes\n+  const XMMRegister xmm_key_shuf_mask = xmm1;\n+  __ movdqu(xmm_key_shuf_mask, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()));\n+\n+  \/\/ Calculate number of rounds from key size: 44 for 10-rounds, 52 for 12-rounds, 60 for 14-rounds\n+  const Register rounds = rbx;\n+  __ movl(rounds, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));\n+\n+  const XMMRegister IV = xmm0;\n+  \/\/ Load IV and broadcast value to 512-bits\n+  __ evbroadcasti64x2(IV, Address(rvec, 0), Assembler::AVX_512bit);\n+\n+  \/\/ Temporary variables for storing round keys\n+  const XMMRegister RK0 = xmm30;\n+  const XMMRegister RK1 = xmm9;\n+  const XMMRegister RK2 = xmm18;\n+  const XMMRegister RK3 = xmm19;\n+  const XMMRegister RK4 = xmm20;\n+  const XMMRegister RK5 = xmm21;\n+  const XMMRegister RK6 = xmm22;\n+  const XMMRegister RK7 = xmm23;\n+  const XMMRegister RK8 = xmm24;\n+  const XMMRegister RK9 = xmm25;\n+  const XMMRegister RK10 = xmm26;\n+\n+  \/\/ Load and shuffle key\n+  \/\/ the java expanded key ordering is rotated one position from what we want\n+  \/\/ so we start from 1*16 here and hit 0*16 last\n+  ev_load_key(RK1, key, 1 * 16, xmm_key_shuf_mask);\n+  ev_load_key(RK2, key, 2 * 16, xmm_key_shuf_mask);\n+  ev_load_key(RK3, key, 3 * 16, xmm_key_shuf_mask);\n+  ev_load_key(RK4, key, 4 * 16, xmm_key_shuf_mask);\n+  ev_load_key(RK5, key, 5 * 16, xmm_key_shuf_mask);\n+  ev_load_key(RK6, key, 6 * 16, xmm_key_shuf_mask);\n+  ev_load_key(RK7, key, 7 * 16, xmm_key_shuf_mask);\n+  ev_load_key(RK8, key, 8 * 16, xmm_key_shuf_mask);\n+  ev_load_key(RK9, key, 9 * 16, xmm_key_shuf_mask);\n+  ev_load_key(RK10, key, 10 * 16, xmm_key_shuf_mask);\n+  ev_load_key(RK0, key, 0*16, xmm_key_shuf_mask);\n+\n+  \/\/ Variables for storing source cipher text\n+  const XMMRegister S0 = xmm10;\n+  const XMMRegister S1 = xmm11;\n+  const XMMRegister S2 = xmm12;\n+  const XMMRegister S3 = xmm13;\n+  const XMMRegister S4 = xmm14;\n+  const XMMRegister S5 = xmm15;\n+  const XMMRegister S6 = xmm16;\n+  const XMMRegister S7 = xmm17;\n+\n+  \/\/ Variables for storing decrypted text\n+  const XMMRegister B0 = xmm1;\n+  const XMMRegister B1 = xmm2;\n+  const XMMRegister B2 = xmm3;\n+  const XMMRegister B3 = xmm4;\n+  const XMMRegister B4 = xmm5;\n+  const XMMRegister B5 = xmm6;\n+  const XMMRegister B6 = xmm7;\n+  const XMMRegister B7 = xmm8;\n+\n+  __ cmpl(rounds, 44);\n+  __ jcc(Assembler::greater, KEY_192);\n+  __ jmp(Loop);\n+\n+  __ BIND(KEY_192);\n+  const XMMRegister RK11 = xmm27;\n+  const XMMRegister RK12 = xmm28;\n+  ev_load_key(RK11, key, 11*16, xmm_key_shuf_mask);\n+  ev_load_key(RK12, key, 12*16, xmm_key_shuf_mask);\n+\n+  __ cmpl(rounds, 52);\n+  __ jcc(Assembler::greater, KEY_256);\n+  __ jmp(Loop);\n+\n+  __ BIND(KEY_256);\n+  const XMMRegister RK13 = xmm29;\n+  const XMMRegister RK14 = xmm31;\n+  ev_load_key(RK13, key, 13*16, xmm_key_shuf_mask);\n+  ev_load_key(RK14, key, 14*16, xmm_key_shuf_mask);\n+\n+  __ BIND(Loop);\n+  __ cmpl(len_reg, 512);\n+  __ jcc(Assembler::below, Lcbc_dec_rem);\n+  __ BIND(Loop1);\n+  __ subl(len_reg, 512);\n+  __ evmovdquq(S0, Address(from, 0 * 64), Assembler::AVX_512bit);\n+  __ evmovdquq(S1, Address(from, 1 * 64), Assembler::AVX_512bit);\n+  __ evmovdquq(S2, Address(from, 2 * 64), Assembler::AVX_512bit);\n+  __ evmovdquq(S3, Address(from, 3 * 64), Assembler::AVX_512bit);\n+  __ evmovdquq(S4, Address(from, 4 * 64), Assembler::AVX_512bit);\n+  __ evmovdquq(S5, Address(from, 5 * 64), Assembler::AVX_512bit);\n+  __ evmovdquq(S6, Address(from, 6 * 64), Assembler::AVX_512bit);\n+  __ evmovdquq(S7, Address(from, 7 * 64), Assembler::AVX_512bit);\n+  __ leaq(from, Address(from, 8 * 64));\n+\n+  __ evpxorq(B0, S0, RK1, Assembler::AVX_512bit);\n+  __ evpxorq(B1, S1, RK1, Assembler::AVX_512bit);\n+  __ evpxorq(B2, S2, RK1, Assembler::AVX_512bit);\n+  __ evpxorq(B3, S3, RK1, Assembler::AVX_512bit);\n+  __ evpxorq(B4, S4, RK1, Assembler::AVX_512bit);\n+  __ evpxorq(B5, S5, RK1, Assembler::AVX_512bit);\n+  __ evpxorq(B6, S6, RK1, Assembler::AVX_512bit);\n+  __ evpxorq(B7, S7, RK1, Assembler::AVX_512bit);\n+\n+  __ evalignq(IV, S0, IV, 0x06);\n+  __ evalignq(S0, S1, S0, 0x06);\n+  __ evalignq(S1, S2, S1, 0x06);\n+  __ evalignq(S2, S3, S2, 0x06);\n+  __ evalignq(S3, S4, S3, 0x06);\n+  __ evalignq(S4, S5, S4, 0x06);\n+  __ evalignq(S5, S6, S5, 0x06);\n+  __ evalignq(S6, S7, S6, 0x06);\n+\n+  roundDec(RK2);\n+  roundDec(RK3);\n+  roundDec(RK4);\n+  roundDec(RK5);\n+  roundDec(RK6);\n+  roundDec(RK7);\n+  roundDec(RK8);\n+  roundDec(RK9);\n+  roundDec(RK10);\n+\n+  __ cmpl(rounds, 44);\n+  __ jcc(Assembler::belowEqual, L_128);\n+  roundDec(RK11);\n+  roundDec(RK12);\n+\n+  __ cmpl(rounds, 52);\n+  __ jcc(Assembler::belowEqual, L_192);\n+  roundDec(RK13);\n+  roundDec(RK14);\n+\n+  __ BIND(L_256);\n+  roundDeclast(RK0);\n+  __ jmp(Loop2);\n+\n+  __ BIND(L_128);\n+  roundDeclast(RK0);\n+  __ jmp(Loop2);\n+\n+  __ BIND(L_192);\n+  roundDeclast(RK0);\n+\n+  __ BIND(Loop2);\n+  __ evpxorq(B0, B0, IV, Assembler::AVX_512bit);\n+  __ evpxorq(B1, B1, S0, Assembler::AVX_512bit);\n+  __ evpxorq(B2, B2, S1, Assembler::AVX_512bit);\n+  __ evpxorq(B3, B3, S2, Assembler::AVX_512bit);\n+  __ evpxorq(B4, B4, S3, Assembler::AVX_512bit);\n+  __ evpxorq(B5, B5, S4, Assembler::AVX_512bit);\n+  __ evpxorq(B6, B6, S5, Assembler::AVX_512bit);\n+  __ evpxorq(B7, B7, S6, Assembler::AVX_512bit);\n+  __ evmovdquq(IV, S7, Assembler::AVX_512bit);\n+\n+  __ evmovdquq(Address(to, 0 * 64), B0, Assembler::AVX_512bit);\n+  __ evmovdquq(Address(to, 1 * 64), B1, Assembler::AVX_512bit);\n+  __ evmovdquq(Address(to, 2 * 64), B2, Assembler::AVX_512bit);\n+  __ evmovdquq(Address(to, 3 * 64), B3, Assembler::AVX_512bit);\n+  __ evmovdquq(Address(to, 4 * 64), B4, Assembler::AVX_512bit);\n+  __ evmovdquq(Address(to, 5 * 64), B5, Assembler::AVX_512bit);\n+  __ evmovdquq(Address(to, 6 * 64), B6, Assembler::AVX_512bit);\n+  __ evmovdquq(Address(to, 7 * 64), B7, Assembler::AVX_512bit);\n+  __ leaq(to, Address(to, 8 * 64));\n+  __ jmp(Loop);\n+\n+  __ BIND(Lcbc_dec_rem);\n+  __ evshufi64x2(IV, IV, IV, 0x03, Assembler::AVX_512bit);\n+\n+  __ BIND(Lcbc_dec_rem_loop);\n+  __ subl(len_reg, 16);\n+  __ jcc(Assembler::carrySet, Lcbc_dec_ret);\n+\n+  __ movdqu(S0, Address(from, 0));\n+  __ evpxorq(B0, S0, RK1, Assembler::AVX_512bit);\n+  __ vaesdec(B0, B0, RK2, Assembler::AVX_512bit);\n+  __ vaesdec(B0, B0, RK3, Assembler::AVX_512bit);\n+  __ vaesdec(B0, B0, RK4, Assembler::AVX_512bit);\n+  __ vaesdec(B0, B0, RK5, Assembler::AVX_512bit);\n+  __ vaesdec(B0, B0, RK6, Assembler::AVX_512bit);\n+  __ vaesdec(B0, B0, RK7, Assembler::AVX_512bit);\n+  __ vaesdec(B0, B0, RK8, Assembler::AVX_512bit);\n+  __ vaesdec(B0, B0, RK9, Assembler::AVX_512bit);\n+  __ vaesdec(B0, B0, RK10, Assembler::AVX_512bit);\n+  __ cmpl(rounds, 44);\n+  __ jcc(Assembler::belowEqual, Lcbc_dec_rem_last);\n+\n+  __ vaesdec(B0, B0, RK11, Assembler::AVX_512bit);\n+  __ vaesdec(B0, B0, RK12, Assembler::AVX_512bit);\n+  __ cmpl(rounds, 52);\n+  __ jcc(Assembler::belowEqual, Lcbc_dec_rem_last);\n+\n+  __ vaesdec(B0, B0, RK13, Assembler::AVX_512bit);\n+  __ vaesdec(B0, B0, RK14, Assembler::AVX_512bit);\n+\n+  __ BIND(Lcbc_dec_rem_last);\n+  __ vaesdeclast(B0, B0, RK0, Assembler::AVX_512bit);\n+\n+  __ evpxorq(B0, B0, IV, Assembler::AVX_512bit);\n+  __ evmovdquq(IV, S0, Assembler::AVX_512bit);\n+  __ movdqu(Address(to, 0), B0);\n+  __ leaq(from, Address(from, 16));\n+  __ leaq(to, Address(to, 16));\n+  __ jmp(Lcbc_dec_rem_loop);\n+\n+  __ BIND(Lcbc_dec_ret);\n+  __ movdqu(Address(rvec, 0), IV);\n+\n+  \/\/ Zero out the round keys\n+  __ evpxorq(RK0, RK0, RK0, Assembler::AVX_512bit);\n+  __ evpxorq(RK1, RK1, RK1, Assembler::AVX_512bit);\n+  __ evpxorq(RK2, RK2, RK2, Assembler::AVX_512bit);\n+  __ evpxorq(RK3, RK3, RK3, Assembler::AVX_512bit);\n+  __ evpxorq(RK4, RK4, RK4, Assembler::AVX_512bit);\n+  __ evpxorq(RK5, RK5, RK5, Assembler::AVX_512bit);\n+  __ evpxorq(RK6, RK6, RK6, Assembler::AVX_512bit);\n+  __ evpxorq(RK7, RK7, RK7, Assembler::AVX_512bit);\n+  __ evpxorq(RK8, RK8, RK8, Assembler::AVX_512bit);\n+  __ evpxorq(RK9, RK9, RK9, Assembler::AVX_512bit);\n+  __ evpxorq(RK10, RK10, RK10, Assembler::AVX_512bit);\n+  __ cmpl(rounds, 44);\n+  __ jcc(Assembler::belowEqual, Lcbc_exit);\n+  __ evpxorq(RK11, RK11, RK11, Assembler::AVX_512bit);\n+  __ evpxorq(RK12, RK12, RK12, Assembler::AVX_512bit);\n+  __ cmpl(rounds, 52);\n+  __ jcc(Assembler::belowEqual, Lcbc_exit);\n+  __ evpxorq(RK13, RK13, RK13, Assembler::AVX_512bit);\n+  __ evpxorq(RK14, RK14, RK14, Assembler::AVX_512bit);\n+\n+  __ BIND(Lcbc_exit);\n+  __ vzeroupper();\n+  __ pop(rbx);\n+#ifdef _WIN64\n+  __ movl(rax, len_mem);\n+#else\n+  __ pop(rax); \/\/ return length\n+#endif\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n+\n+  return start;\n+}\n+\n+\/\/ Polynomial x^128+x^127+x^126+x^121+1\n+address StubGenerator::ghash_polynomial_addr() {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"_ghash_poly_addr\");\n+  address start = __ pc();\n+\n+  __ emit_data64(0x0000000000000001, relocInfo::none);\n+  __ emit_data64(0xc200000000000000, relocInfo::none);\n+\n+  return start;\n+}\n+\n+address StubGenerator::ghash_shufflemask_addr() {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"_ghash_shuffmask_addr\");\n+  address start = __ pc();\n+\n+  __ emit_data64(0x0f0f0f0f0f0f0f0f, relocInfo::none);\n+  __ emit_data64(0x0f0f0f0f0f0f0f0f, relocInfo::none);\n+\n+  return start;\n+}\n+\n+\/\/ Ghash single and multi block operations using AVX instructions\n+address StubGenerator::generate_avx_ghash_processBlocks() {\n+  __ align(CodeEntryAlignment);\n+\n+  StubCodeMark mark(this, \"StubRoutines\", \"ghash_processBlocks\");\n+  address start = __ pc();\n+\n+  \/\/ arguments\n+  const Register state = c_rarg0;\n+  const Register htbl = c_rarg1;\n+  const Register data = c_rarg2;\n+  const Register blocks = c_rarg3;\n+  __ enter();\n+ \/\/ Save state before entering routine\n+  __ avx_ghash(state, htbl, data, blocks);\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n+\n+  return start;\n+}\n+\n+\/\/ byte swap x86 long\n+address StubGenerator::generate_ghash_long_swap_mask() {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"ghash_long_swap_mask\");\n+  address start = __ pc();\n+\n+  __ emit_data64(0x0f0e0d0c0b0a0908, relocInfo::none );\n+  __ emit_data64(0x0706050403020100, relocInfo::none );\n+\n+return start;\n+}\n@@ -4977,0 +5328,10 @@\n+\/\/ byte swap x86 byte array\n+address StubGenerator::generate_ghash_byte_swap_mask() {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"ghash_byte_swap_mask\");\n+  address start = __ pc();\n+\n+  __ emit_data64(0x08090a0b0c0d0e0f, relocInfo::none );\n+  __ emit_data64(0x0001020304050607, relocInfo::none );\n+\n+return start;\n@@ -4979,5 +5340,6 @@\n-address generate_cipherBlockChaining_decryptVectorAESCrypt() {\n-    assert(VM_Version::supports_avx512_vaes(), \"need AES instructions and misaligned SSE support\");\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"cipherBlockChaining_decryptAESCrypt\");\n-    address start = __ pc();\n+\/* Single and multi-block ghash operations *\/\n+address StubGenerator::generate_ghash_processBlocks() {\n+  __ align(CodeEntryAlignment);\n+  Label L_ghash_loop, L_exit;\n+  StubCodeMark mark(this, \"StubRoutines\", \"ghash_processBlocks\");\n+  address start = __ pc();\n@@ -4985,11 +5347,4 @@\n-    const Register from = c_rarg0;  \/\/ source array address\n-    const Register to = c_rarg1;  \/\/ destination array address\n-    const Register key = c_rarg2;  \/\/ key array address\n-    const Register rvec = c_rarg3;  \/\/ r byte array initialized from initvector array address\n-    \/\/ and left with the results of the last encryption block\n-#ifndef _WIN64\n-    const Register len_reg = c_rarg4;  \/\/ src len (must be multiple of blocksize 16)\n-#else\n-    const Address  len_mem(rbp, 6 * wordSize);  \/\/ length is on stack on Win64\n-    const Register len_reg = r11;      \/\/ pick the volatile windows register\n-#endif\n+  const Register state        = c_rarg0;\n+  const Register subkeyH      = c_rarg1;\n+  const Register data         = c_rarg2;\n+  const Register blocks       = c_rarg3;\n@@ -4997,2 +5352,11 @@\n-    Label Loop, Loop1, L_128, L_256, L_192, KEY_192, KEY_256, Loop2, Lcbc_dec_rem_loop,\n-          Lcbc_dec_rem_last, Lcbc_dec_ret, Lcbc_dec_rem, Lcbc_exit;\n+  const XMMRegister xmm_temp0 = xmm0;\n+  const XMMRegister xmm_temp1 = xmm1;\n+  const XMMRegister xmm_temp2 = xmm2;\n+  const XMMRegister xmm_temp3 = xmm3;\n+  const XMMRegister xmm_temp4 = xmm4;\n+  const XMMRegister xmm_temp5 = xmm5;\n+  const XMMRegister xmm_temp6 = xmm6;\n+  const XMMRegister xmm_temp7 = xmm7;\n+  const XMMRegister xmm_temp8 = xmm8;\n+  const XMMRegister xmm_temp9 = xmm9;\n+  const XMMRegister xmm_temp10 = xmm10;\n@@ -5000,1 +5364,1 @@\n-    __ enter();\n+  __ enter();\n@@ -5002,8 +5366,1 @@\n-#ifdef _WIN64\n-  \/\/ on win64, fill len_reg from stack position\n-    __ movl(len_reg, len_mem);\n-#else\n-    __ push(len_reg); \/\/ Save\n-#endif\n-    __ push(rbx);\n-    __ vzeroupper();\n+  __ movdqu(xmm_temp10, ExternalAddress(StubRoutines::x86::ghash_long_swap_mask_addr()));\n@@ -5011,241 +5368,2 @@\n-    \/\/ Temporary variable declaration for swapping key bytes\n-    const XMMRegister xmm_key_shuf_mask = xmm1;\n-    __ movdqu(xmm_key_shuf_mask, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()));\n-\n-    \/\/ Calculate number of rounds from key size: 44 for 10-rounds, 52 for 12-rounds, 60 for 14-rounds\n-    const Register rounds = rbx;\n-    __ movl(rounds, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));\n-\n-    const XMMRegister IV = xmm0;\n-    \/\/ Load IV and broadcast value to 512-bits\n-    __ evbroadcasti64x2(IV, Address(rvec, 0), Assembler::AVX_512bit);\n-\n-    \/\/ Temporary variables for storing round keys\n-    const XMMRegister RK0 = xmm30;\n-    const XMMRegister RK1 = xmm9;\n-    const XMMRegister RK2 = xmm18;\n-    const XMMRegister RK3 = xmm19;\n-    const XMMRegister RK4 = xmm20;\n-    const XMMRegister RK5 = xmm21;\n-    const XMMRegister RK6 = xmm22;\n-    const XMMRegister RK7 = xmm23;\n-    const XMMRegister RK8 = xmm24;\n-    const XMMRegister RK9 = xmm25;\n-    const XMMRegister RK10 = xmm26;\n-\n-     \/\/ Load and shuffle key\n-    \/\/ the java expanded key ordering is rotated one position from what we want\n-    \/\/ so we start from 1*16 here and hit 0*16 last\n-    ev_load_key(RK1, key, 1 * 16, xmm_key_shuf_mask);\n-    ev_load_key(RK2, key, 2 * 16, xmm_key_shuf_mask);\n-    ev_load_key(RK3, key, 3 * 16, xmm_key_shuf_mask);\n-    ev_load_key(RK4, key, 4 * 16, xmm_key_shuf_mask);\n-    ev_load_key(RK5, key, 5 * 16, xmm_key_shuf_mask);\n-    ev_load_key(RK6, key, 6 * 16, xmm_key_shuf_mask);\n-    ev_load_key(RK7, key, 7 * 16, xmm_key_shuf_mask);\n-    ev_load_key(RK8, key, 8 * 16, xmm_key_shuf_mask);\n-    ev_load_key(RK9, key, 9 * 16, xmm_key_shuf_mask);\n-    ev_load_key(RK10, key, 10 * 16, xmm_key_shuf_mask);\n-    ev_load_key(RK0, key, 0*16, xmm_key_shuf_mask);\n-\n-    \/\/ Variables for storing source cipher text\n-    const XMMRegister S0 = xmm10;\n-    const XMMRegister S1 = xmm11;\n-    const XMMRegister S2 = xmm12;\n-    const XMMRegister S3 = xmm13;\n-    const XMMRegister S4 = xmm14;\n-    const XMMRegister S5 = xmm15;\n-    const XMMRegister S6 = xmm16;\n-    const XMMRegister S7 = xmm17;\n-\n-    \/\/ Variables for storing decrypted text\n-    const XMMRegister B0 = xmm1;\n-    const XMMRegister B1 = xmm2;\n-    const XMMRegister B2 = xmm3;\n-    const XMMRegister B3 = xmm4;\n-    const XMMRegister B4 = xmm5;\n-    const XMMRegister B5 = xmm6;\n-    const XMMRegister B6 = xmm7;\n-    const XMMRegister B7 = xmm8;\n-\n-    __ cmpl(rounds, 44);\n-    __ jcc(Assembler::greater, KEY_192);\n-    __ jmp(Loop);\n-\n-    __ BIND(KEY_192);\n-    const XMMRegister RK11 = xmm27;\n-    const XMMRegister RK12 = xmm28;\n-    ev_load_key(RK11, key, 11*16, xmm_key_shuf_mask);\n-    ev_load_key(RK12, key, 12*16, xmm_key_shuf_mask);\n-\n-    __ cmpl(rounds, 52);\n-    __ jcc(Assembler::greater, KEY_256);\n-    __ jmp(Loop);\n-\n-    __ BIND(KEY_256);\n-    const XMMRegister RK13 = xmm29;\n-    const XMMRegister RK14 = xmm31;\n-    ev_load_key(RK13, key, 13*16, xmm_key_shuf_mask);\n-    ev_load_key(RK14, key, 14*16, xmm_key_shuf_mask);\n-\n-    __ BIND(Loop);\n-    __ cmpl(len_reg, 512);\n-    __ jcc(Assembler::below, Lcbc_dec_rem);\n-    __ BIND(Loop1);\n-    __ subl(len_reg, 512);\n-    __ evmovdquq(S0, Address(from, 0 * 64), Assembler::AVX_512bit);\n-    __ evmovdquq(S1, Address(from, 1 * 64), Assembler::AVX_512bit);\n-    __ evmovdquq(S2, Address(from, 2 * 64), Assembler::AVX_512bit);\n-    __ evmovdquq(S3, Address(from, 3 * 64), Assembler::AVX_512bit);\n-    __ evmovdquq(S4, Address(from, 4 * 64), Assembler::AVX_512bit);\n-    __ evmovdquq(S5, Address(from, 5 * 64), Assembler::AVX_512bit);\n-    __ evmovdquq(S6, Address(from, 6 * 64), Assembler::AVX_512bit);\n-    __ evmovdquq(S7, Address(from, 7 * 64), Assembler::AVX_512bit);\n-    __ leaq(from, Address(from, 8 * 64));\n-\n-    __ evpxorq(B0, S0, RK1, Assembler::AVX_512bit);\n-    __ evpxorq(B1, S1, RK1, Assembler::AVX_512bit);\n-    __ evpxorq(B2, S2, RK1, Assembler::AVX_512bit);\n-    __ evpxorq(B3, S3, RK1, Assembler::AVX_512bit);\n-    __ evpxorq(B4, S4, RK1, Assembler::AVX_512bit);\n-    __ evpxorq(B5, S5, RK1, Assembler::AVX_512bit);\n-    __ evpxorq(B6, S6, RK1, Assembler::AVX_512bit);\n-    __ evpxorq(B7, S7, RK1, Assembler::AVX_512bit);\n-\n-    __ evalignq(IV, S0, IV, 0x06);\n-    __ evalignq(S0, S1, S0, 0x06);\n-    __ evalignq(S1, S2, S1, 0x06);\n-    __ evalignq(S2, S3, S2, 0x06);\n-    __ evalignq(S3, S4, S3, 0x06);\n-    __ evalignq(S4, S5, S4, 0x06);\n-    __ evalignq(S5, S6, S5, 0x06);\n-    __ evalignq(S6, S7, S6, 0x06);\n-\n-    roundDec(RK2);\n-    roundDec(RK3);\n-    roundDec(RK4);\n-    roundDec(RK5);\n-    roundDec(RK6);\n-    roundDec(RK7);\n-    roundDec(RK8);\n-    roundDec(RK9);\n-    roundDec(RK10);\n-\n-    __ cmpl(rounds, 44);\n-    __ jcc(Assembler::belowEqual, L_128);\n-    roundDec(RK11);\n-    roundDec(RK12);\n-\n-    __ cmpl(rounds, 52);\n-    __ jcc(Assembler::belowEqual, L_192);\n-    roundDec(RK13);\n-    roundDec(RK14);\n-\n-    __ BIND(L_256);\n-    roundDeclast(RK0);\n-    __ jmp(Loop2);\n-\n-    __ BIND(L_128);\n-    roundDeclast(RK0);\n-    __ jmp(Loop2);\n-\n-    __ BIND(L_192);\n-    roundDeclast(RK0);\n-\n-    __ BIND(Loop2);\n-    __ evpxorq(B0, B0, IV, Assembler::AVX_512bit);\n-    __ evpxorq(B1, B1, S0, Assembler::AVX_512bit);\n-    __ evpxorq(B2, B2, S1, Assembler::AVX_512bit);\n-    __ evpxorq(B3, B3, S2, Assembler::AVX_512bit);\n-    __ evpxorq(B4, B4, S3, Assembler::AVX_512bit);\n-    __ evpxorq(B5, B5, S4, Assembler::AVX_512bit);\n-    __ evpxorq(B6, B6, S5, Assembler::AVX_512bit);\n-    __ evpxorq(B7, B7, S6, Assembler::AVX_512bit);\n-    __ evmovdquq(IV, S7, Assembler::AVX_512bit);\n-\n-    __ evmovdquq(Address(to, 0 * 64), B0, Assembler::AVX_512bit);\n-    __ evmovdquq(Address(to, 1 * 64), B1, Assembler::AVX_512bit);\n-    __ evmovdquq(Address(to, 2 * 64), B2, Assembler::AVX_512bit);\n-    __ evmovdquq(Address(to, 3 * 64), B3, Assembler::AVX_512bit);\n-    __ evmovdquq(Address(to, 4 * 64), B4, Assembler::AVX_512bit);\n-    __ evmovdquq(Address(to, 5 * 64), B5, Assembler::AVX_512bit);\n-    __ evmovdquq(Address(to, 6 * 64), B6, Assembler::AVX_512bit);\n-    __ evmovdquq(Address(to, 7 * 64), B7, Assembler::AVX_512bit);\n-    __ leaq(to, Address(to, 8 * 64));\n-    __ jmp(Loop);\n-\n-    __ BIND(Lcbc_dec_rem);\n-    __ evshufi64x2(IV, IV, IV, 0x03, Assembler::AVX_512bit);\n-\n-    __ BIND(Lcbc_dec_rem_loop);\n-    __ subl(len_reg, 16);\n-    __ jcc(Assembler::carrySet, Lcbc_dec_ret);\n-\n-    __ movdqu(S0, Address(from, 0));\n-    __ evpxorq(B0, S0, RK1, Assembler::AVX_512bit);\n-    __ vaesdec(B0, B0, RK2, Assembler::AVX_512bit);\n-    __ vaesdec(B0, B0, RK3, Assembler::AVX_512bit);\n-    __ vaesdec(B0, B0, RK4, Assembler::AVX_512bit);\n-    __ vaesdec(B0, B0, RK5, Assembler::AVX_512bit);\n-    __ vaesdec(B0, B0, RK6, Assembler::AVX_512bit);\n-    __ vaesdec(B0, B0, RK7, Assembler::AVX_512bit);\n-    __ vaesdec(B0, B0, RK8, Assembler::AVX_512bit);\n-    __ vaesdec(B0, B0, RK9, Assembler::AVX_512bit);\n-    __ vaesdec(B0, B0, RK10, Assembler::AVX_512bit);\n-    __ cmpl(rounds, 44);\n-    __ jcc(Assembler::belowEqual, Lcbc_dec_rem_last);\n-\n-    __ vaesdec(B0, B0, RK11, Assembler::AVX_512bit);\n-    __ vaesdec(B0, B0, RK12, Assembler::AVX_512bit);\n-    __ cmpl(rounds, 52);\n-    __ jcc(Assembler::belowEqual, Lcbc_dec_rem_last);\n-\n-    __ vaesdec(B0, B0, RK13, Assembler::AVX_512bit);\n-    __ vaesdec(B0, B0, RK14, Assembler::AVX_512bit);\n-\n-    __ BIND(Lcbc_dec_rem_last);\n-    __ vaesdeclast(B0, B0, RK0, Assembler::AVX_512bit);\n-\n-    __ evpxorq(B0, B0, IV, Assembler::AVX_512bit);\n-    __ evmovdquq(IV, S0, Assembler::AVX_512bit);\n-    __ movdqu(Address(to, 0), B0);\n-    __ leaq(from, Address(from, 16));\n-    __ leaq(to, Address(to, 16));\n-    __ jmp(Lcbc_dec_rem_loop);\n-\n-    __ BIND(Lcbc_dec_ret);\n-    __ movdqu(Address(rvec, 0), IV);\n-\n-    \/\/ Zero out the round keys\n-    __ evpxorq(RK0, RK0, RK0, Assembler::AVX_512bit);\n-    __ evpxorq(RK1, RK1, RK1, Assembler::AVX_512bit);\n-    __ evpxorq(RK2, RK2, RK2, Assembler::AVX_512bit);\n-    __ evpxorq(RK3, RK3, RK3, Assembler::AVX_512bit);\n-    __ evpxorq(RK4, RK4, RK4, Assembler::AVX_512bit);\n-    __ evpxorq(RK5, RK5, RK5, Assembler::AVX_512bit);\n-    __ evpxorq(RK6, RK6, RK6, Assembler::AVX_512bit);\n-    __ evpxorq(RK7, RK7, RK7, Assembler::AVX_512bit);\n-    __ evpxorq(RK8, RK8, RK8, Assembler::AVX_512bit);\n-    __ evpxorq(RK9, RK9, RK9, Assembler::AVX_512bit);\n-    __ evpxorq(RK10, RK10, RK10, Assembler::AVX_512bit);\n-    __ cmpl(rounds, 44);\n-    __ jcc(Assembler::belowEqual, Lcbc_exit);\n-    __ evpxorq(RK11, RK11, RK11, Assembler::AVX_512bit);\n-    __ evpxorq(RK12, RK12, RK12, Assembler::AVX_512bit);\n-    __ cmpl(rounds, 52);\n-    __ jcc(Assembler::belowEqual, Lcbc_exit);\n-    __ evpxorq(RK13, RK13, RK13, Assembler::AVX_512bit);\n-    __ evpxorq(RK14, RK14, RK14, Assembler::AVX_512bit);\n-\n-    __ BIND(Lcbc_exit);\n-    __ vzeroupper();\n-    __ pop(rbx);\n-#ifdef _WIN64\n-    __ movl(rax, len_mem);\n-#else\n-    __ pop(rax); \/\/ return length\n-#endif\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-    return start;\n-}\n+  __ movdqu(xmm_temp0, Address(state, 0));\n+  __ pshufb(xmm_temp0, xmm_temp10);\n@@ -5253,8 +5371,97 @@\n-\/\/ Polynomial x^128+x^127+x^126+x^121+1\n-address ghash_polynomial_addr() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"_ghash_poly_addr\");\n-    address start = __ pc();\n-    __ emit_data64(0x0000000000000001, relocInfo::none);\n-    __ emit_data64(0xc200000000000000, relocInfo::none);\n-    return start;\n+\n+  __ BIND(L_ghash_loop);\n+  __ movdqu(xmm_temp2, Address(data, 0));\n+  __ pshufb(xmm_temp2, ExternalAddress(StubRoutines::x86::ghash_byte_swap_mask_addr()));\n+\n+  __ movdqu(xmm_temp1, Address(subkeyH, 0));\n+  __ pshufb(xmm_temp1, xmm_temp10);\n+\n+  __ pxor(xmm_temp0, xmm_temp2);\n+\n+  \/\/\n+  \/\/ Multiply with the hash key\n+  \/\/\n+  __ movdqu(xmm_temp3, xmm_temp0);\n+  __ pclmulqdq(xmm_temp3, xmm_temp1, 0);      \/\/ xmm3 holds a0*b0\n+  __ movdqu(xmm_temp4, xmm_temp0);\n+  __ pclmulqdq(xmm_temp4, xmm_temp1, 16);     \/\/ xmm4 holds a0*b1\n+\n+  __ movdqu(xmm_temp5, xmm_temp0);\n+  __ pclmulqdq(xmm_temp5, xmm_temp1, 1);      \/\/ xmm5 holds a1*b0\n+  __ movdqu(xmm_temp6, xmm_temp0);\n+  __ pclmulqdq(xmm_temp6, xmm_temp1, 17);     \/\/ xmm6 holds a1*b1\n+\n+  __ pxor(xmm_temp4, xmm_temp5);      \/\/ xmm4 holds a0*b1 + a1*b0\n+\n+  __ movdqu(xmm_temp5, xmm_temp4);    \/\/ move the contents of xmm4 to xmm5\n+  __ psrldq(xmm_temp4, 8);    \/\/ shift by xmm4 64 bits to the right\n+  __ pslldq(xmm_temp5, 8);    \/\/ shift by xmm5 64 bits to the left\n+  __ pxor(xmm_temp3, xmm_temp5);\n+  __ pxor(xmm_temp6, xmm_temp4);      \/\/ Register pair <xmm6:xmm3> holds the result\n+                                      \/\/ of the carry-less multiplication of\n+                                      \/\/ xmm0 by xmm1.\n+\n+  \/\/ We shift the result of the multiplication by one bit position\n+  \/\/ to the left to cope for the fact that the bits are reversed.\n+  __ movdqu(xmm_temp7, xmm_temp3);\n+  __ movdqu(xmm_temp8, xmm_temp6);\n+  __ pslld(xmm_temp3, 1);\n+  __ pslld(xmm_temp6, 1);\n+  __ psrld(xmm_temp7, 31);\n+  __ psrld(xmm_temp8, 31);\n+  __ movdqu(xmm_temp9, xmm_temp7);\n+  __ pslldq(xmm_temp8, 4);\n+  __ pslldq(xmm_temp7, 4);\n+  __ psrldq(xmm_temp9, 12);\n+  __ por(xmm_temp3, xmm_temp7);\n+  __ por(xmm_temp6, xmm_temp8);\n+  __ por(xmm_temp6, xmm_temp9);\n+\n+  \/\/\n+  \/\/ First phase of the reduction\n+  \/\/\n+  \/\/ Move xmm3 into xmm7, xmm8, xmm9 in order to perform the shifts\n+  \/\/ independently.\n+  __ movdqu(xmm_temp7, xmm_temp3);\n+  __ movdqu(xmm_temp8, xmm_temp3);\n+  __ movdqu(xmm_temp9, xmm_temp3);\n+  __ pslld(xmm_temp7, 31);    \/\/ packed right shift shifting << 31\n+  __ pslld(xmm_temp8, 30);    \/\/ packed right shift shifting << 30\n+  __ pslld(xmm_temp9, 25);    \/\/ packed right shift shifting << 25\n+  __ pxor(xmm_temp7, xmm_temp8);      \/\/ xor the shifted versions\n+  __ pxor(xmm_temp7, xmm_temp9);\n+  __ movdqu(xmm_temp8, xmm_temp7);\n+  __ pslldq(xmm_temp7, 12);\n+  __ psrldq(xmm_temp8, 4);\n+  __ pxor(xmm_temp3, xmm_temp7);      \/\/ first phase of the reduction complete\n+\n+  \/\/\n+  \/\/ Second phase of the reduction\n+  \/\/\n+  \/\/ Make 3 copies of xmm3 in xmm2, xmm4, xmm5 for doing these\n+  \/\/ shift operations.\n+  __ movdqu(xmm_temp2, xmm_temp3);\n+  __ movdqu(xmm_temp4, xmm_temp3);\n+  __ movdqu(xmm_temp5, xmm_temp3);\n+  __ psrld(xmm_temp2, 1);     \/\/ packed left shifting >> 1\n+  __ psrld(xmm_temp4, 2);     \/\/ packed left shifting >> 2\n+  __ psrld(xmm_temp5, 7);     \/\/ packed left shifting >> 7\n+  __ pxor(xmm_temp2, xmm_temp4);      \/\/ xor the shifted versions\n+  __ pxor(xmm_temp2, xmm_temp5);\n+  __ pxor(xmm_temp2, xmm_temp8);\n+  __ pxor(xmm_temp3, xmm_temp2);\n+  __ pxor(xmm_temp6, xmm_temp3);      \/\/ the result is in xmm6\n+\n+  __ decrement(blocks);\n+  __ jcc(Assembler::zero, L_exit);\n+  __ movdqu(xmm_temp0, xmm_temp6);\n+  __ addptr(data, 16);\n+  __ jmp(L_ghash_loop);\n+\n+  __ BIND(L_exit);\n+  __ pshufb(xmm_temp6, xmm_temp10);          \/\/ Byte swap 16-byte result\n+  __ movdqu(Address(state, 0), xmm_temp6);   \/\/ store the result\n+  __ leave();\n+  __ ret(0);\n+\n+  return start;\n@@ -5263,7 +5470,17 @@\n-address ghash_shufflemask_addr() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"_ghash_shuffmask_addr\");\n-    address start = __ pc();\n-    __ emit_data64(0x0f0f0f0f0f0f0f0f, relocInfo::none);\n-    __ emit_data64(0x0f0f0f0f0f0f0f0f, relocInfo::none);\n-    return start;\n+address StubGenerator::base64_shuffle_addr() {\n+  __ align64();\n+  StubCodeMark mark(this, \"StubRoutines\", \"shuffle_base64\");\n+  address start = __ pc();\n+\n+  assert(((unsigned long long)start & 0x3f) == 0,\n+         \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n+  __ emit_data64(0x0405030401020001, relocInfo::none);\n+  __ emit_data64(0x0a0b090a07080607, relocInfo::none);\n+  __ emit_data64(0x10110f100d0e0c0d, relocInfo::none);\n+  __ emit_data64(0x1617151613141213, relocInfo::none);\n+  __ emit_data64(0x1c1d1b1c191a1819, relocInfo::none);\n+  __ emit_data64(0x222321221f201e1f, relocInfo::none);\n+  __ emit_data64(0x2829272825262425, relocInfo::none);\n+  __ emit_data64(0x2e2f2d2e2b2c2a2b, relocInfo::none);\n+\n+  return start;\n@@ -5272,18 +5489,11 @@\n-\/\/ Ghash single and multi block operations using AVX instructions\n-address generate_avx_ghash_processBlocks() {\n-    __ align(CodeEntryAlignment);\n-\n-    StubCodeMark mark(this, \"StubRoutines\", \"ghash_processBlocks\");\n-    address start = __ pc();\n-\n-    \/\/ arguments\n-    const Register state = c_rarg0;\n-    const Register htbl = c_rarg1;\n-    const Register data = c_rarg2;\n-    const Register blocks = c_rarg3;\n-    __ enter();\n-   \/\/ Save state before entering routine\n-    __ avx_ghash(state, htbl, data, blocks);\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-    return start;\n+address StubGenerator::base64_avx2_shuffle_addr() {\n+  __ align32();\n+  StubCodeMark mark(this, \"StubRoutines\", \"avx2_shuffle_base64\");\n+  address start = __ pc();\n+\n+  __ emit_data64(0x0809070805060405, relocInfo::none);\n+  __ emit_data64(0x0e0f0d0e0b0c0a0b, relocInfo::none);\n+  __ emit_data64(0x0405030401020001, relocInfo::none);\n+  __ emit_data64(0x0a0b090a07080607, relocInfo::none);\n+\n+  return start;\n@@ -5292,7 +5502,10 @@\n-  \/\/ byte swap x86 long\n-  address generate_ghash_long_swap_mask() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"ghash_long_swap_mask\");\n-    address start = __ pc();\n-    __ emit_data64(0x0f0e0d0c0b0a0908, relocInfo::none );\n-    __ emit_data64(0x0706050403020100, relocInfo::none );\n+address StubGenerator::base64_avx2_input_mask_addr() {\n+  __ align32();\n+  StubCodeMark mark(this, \"StubRoutines\", \"avx2_input_mask_base64\");\n+  address start = __ pc();\n+\n+  __ emit_data64(0x8000000000000000, relocInfo::none);\n+  __ emit_data64(0x8000000080000000, relocInfo::none);\n+  __ emit_data64(0x8000000080000000, relocInfo::none);\n+  __ emit_data64(0x8000000080000000, relocInfo::none);\n+\n@@ -5300,1 +5513,17 @@\n-  }\n+}\n+\n+address StubGenerator::base64_avx2_lut_addr() {\n+  __ align32();\n+  StubCodeMark mark(this, \"StubRoutines\", \"avx2_lut_base64\");\n+  address start = __ pc();\n+\n+  __ emit_data64(0xfcfcfcfcfcfc4741, relocInfo::none);\n+  __ emit_data64(0x0000f0edfcfcfcfc, relocInfo::none);\n+  __ emit_data64(0xfcfcfcfcfcfc4741, relocInfo::none);\n+  __ emit_data64(0x0000f0edfcfcfcfc, relocInfo::none);\n+\n+  \/\/ URL LUT\n+  __ emit_data64(0xfcfcfcfcfcfc4741, relocInfo::none);\n+  __ emit_data64(0x000020effcfcfcfc, relocInfo::none);\n+  __ emit_data64(0xfcfcfcfcfcfc4741, relocInfo::none);\n+  __ emit_data64(0x000020effcfcfcfc, relocInfo::none);\n@@ -5302,7 +5531,0 @@\n-  \/\/ byte swap x86 byte array\n-  address generate_ghash_byte_swap_mask() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"ghash_byte_swap_mask\");\n-    address start = __ pc();\n-    __ emit_data64(0x08090a0b0c0d0e0f, relocInfo::none );\n-    __ emit_data64(0x0001020304050607, relocInfo::none );\n@@ -5310,1 +5532,1 @@\n-  }\n+}\n@@ -5312,6 +5534,24 @@\n-  \/* Single and multi-block ghash operations *\/\n-  address generate_ghash_processBlocks() {\n-    __ align(CodeEntryAlignment);\n-    Label L_ghash_loop, L_exit;\n-    StubCodeMark mark(this, \"StubRoutines\", \"ghash_processBlocks\");\n-    address start = __ pc();\n+address StubGenerator::base64_encoding_table_addr() {\n+  __ align64();\n+  StubCodeMark mark(this, \"StubRoutines\", \"encoding_table_base64\");\n+  address start = __ pc();\n+\n+  assert(((unsigned long long)start & 0x3f) == 0, \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n+  __ emit_data64(0x4847464544434241, relocInfo::none);\n+  __ emit_data64(0x504f4e4d4c4b4a49, relocInfo::none);\n+  __ emit_data64(0x5857565554535251, relocInfo::none);\n+  __ emit_data64(0x6665646362615a59, relocInfo::none);\n+  __ emit_data64(0x6e6d6c6b6a696867, relocInfo::none);\n+  __ emit_data64(0x767574737271706f, relocInfo::none);\n+  __ emit_data64(0x333231307a797877, relocInfo::none);\n+  __ emit_data64(0x2f2b393837363534, relocInfo::none);\n+\n+  \/\/ URL table\n+  __ emit_data64(0x4847464544434241, relocInfo::none);\n+  __ emit_data64(0x504f4e4d4c4b4a49, relocInfo::none);\n+  __ emit_data64(0x5857565554535251, relocInfo::none);\n+  __ emit_data64(0x6665646362615a59, relocInfo::none);\n+  __ emit_data64(0x6e6d6c6b6a696867, relocInfo::none);\n+  __ emit_data64(0x767574737271706f, relocInfo::none);\n+  __ emit_data64(0x333231307a797877, relocInfo::none);\n+  __ emit_data64(0x5f2d393837363534, relocInfo::none);\n@@ -5319,4 +5559,26 @@\n-    const Register state        = c_rarg0;\n-    const Register subkeyH      = c_rarg1;\n-    const Register data         = c_rarg2;\n-    const Register blocks       = c_rarg3;\n+  return start;\n+}\n+\n+\/\/ Code for generating Base64 encoding.\n+\/\/ Intrinsic function prototype in Base64.java:\n+\/\/ private void encodeBlock(byte[] src, int sp, int sl, byte[] dst, int dp,\n+\/\/ boolean isURL) {\n+address StubGenerator::generate_base64_encodeBlock()\n+{\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"implEncode\");\n+  address start = __ pc();\n+\n+  __ enter();\n+\n+  \/\/ Save callee-saved registers before using them\n+  __ push(r12);\n+  __ push(r13);\n+  __ push(r14);\n+  __ push(r15);\n+\n+  \/\/ arguments\n+  const Register source = c_rarg0;       \/\/ Source Array\n+  const Register start_offset = c_rarg1; \/\/ start offset\n+  const Register end_offset = c_rarg2;   \/\/ end offset\n+  const Register dest = c_rarg3;   \/\/ destination array\n@@ -5324,11 +5586,11 @@\n-    const XMMRegister xmm_temp0 = xmm0;\n-    const XMMRegister xmm_temp1 = xmm1;\n-    const XMMRegister xmm_temp2 = xmm2;\n-    const XMMRegister xmm_temp3 = xmm3;\n-    const XMMRegister xmm_temp4 = xmm4;\n-    const XMMRegister xmm_temp5 = xmm5;\n-    const XMMRegister xmm_temp6 = xmm6;\n-    const XMMRegister xmm_temp7 = xmm7;\n-    const XMMRegister xmm_temp8 = xmm8;\n-    const XMMRegister xmm_temp9 = xmm9;\n-    const XMMRegister xmm_temp10 = xmm10;\n+#ifndef _WIN64\n+  const Register dp = c_rarg4;    \/\/ Position for writing to dest array\n+  const Register isURL = c_rarg5; \/\/ Base64 or URL character set\n+#else\n+  const Address dp_mem(rbp, 6 * wordSize); \/\/ length is on stack on Win64\n+  const Address isURL_mem(rbp, 7 * wordSize);\n+  const Register isURL = r10; \/\/ pick the volatile windows register\n+  const Register dp = r12;\n+  __ movl(dp, dp_mem);\n+  __ movl(isURL, isURL_mem);\n+#endif\n@@ -5336,1 +5598,26 @@\n-    __ enter();\n+  const Register length = r14;\n+  const Register encode_table = r13;\n+  Label L_process3, L_exit, L_processdata, L_vbmiLoop, L_not512, L_32byteLoop;\n+\n+  \/\/ calculate length from offsets\n+  __ movl(length, end_offset);\n+  __ subl(length, start_offset);\n+  __ cmpl(length, 0);\n+  __ jcc(Assembler::lessEqual, L_exit);\n+\n+  \/\/ Code for 512-bit VBMI encoding.  Encodes 48 input bytes into 64\n+  \/\/ output bytes. We read 64 input bytes and ignore the last 16, so be\n+  \/\/ sure not to read past the end of the input buffer.\n+  if (VM_Version::supports_avx512_vbmi()) {\n+    __ cmpl(length, 64); \/\/ Do not overrun input buffer.\n+    __ jcc(Assembler::below, L_not512);\n+\n+    __ shll(isURL, 6); \/\/ index into decode table based on isURL\n+    __ lea(encode_table, ExternalAddress(StubRoutines::x86::base64_encoding_table_addr()));\n+    __ addptr(encode_table, isURL);\n+    __ shrl(isURL, 6); \/\/ restore isURL\n+\n+    __ mov64(rax, 0x3036242a1016040aull); \/\/ Shifts\n+    __ evmovdquq(xmm3, ExternalAddress(StubRoutines::x86::base64_shuffle_addr()), Assembler::AVX_512bit, r15);\n+    __ evmovdquq(xmm2, Address(encode_table, 0), Assembler::AVX_512bit);\n+    __ evpbroadcastq(xmm1, rax, Assembler::AVX_512bit);\n@@ -5338,1 +5625,2 @@\n-    __ movdqu(xmm_temp10, ExternalAddress(StubRoutines::x86::ghash_long_swap_mask_addr()));\n+    __ align32();\n+    __ BIND(L_vbmiLoop);\n@@ -5340,2 +5628,2 @@\n-    __ movdqu(xmm_temp0, Address(state, 0));\n-    __ pshufb(xmm_temp0, xmm_temp10);\n+    __ vpermb(xmm0, xmm3, Address(source, start_offset), Assembler::AVX_512bit);\n+    __ subl(length, 48);\n@@ -5343,0 +5631,4 @@\n+    \/\/ Put the input bytes into the proper lanes for writing, then\n+    \/\/ encode them.\n+    __ evpmultishiftqb(xmm0, xmm1, xmm0, Assembler::AVX_512bit);\n+    __ vpermb(xmm0, xmm0, xmm2, Assembler::AVX_512bit);\n@@ -5344,3 +5636,2 @@\n-    __ BIND(L_ghash_loop);\n-    __ movdqu(xmm_temp2, Address(data, 0));\n-    __ pshufb(xmm_temp2, ExternalAddress(StubRoutines::x86::ghash_byte_swap_mask_addr()));\n+    \/\/ Write to destination\n+    __ evmovdquq(Address(dest, dp), xmm0, Assembler::AVX_512bit);\n@@ -5348,2 +5639,4 @@\n-    __ movdqu(xmm_temp1, Address(subkeyH, 0));\n-    __ pshufb(xmm_temp1, xmm_temp10);\n+    __ addptr(dest, 64);\n+    __ addptr(source, 48);\n+    __ cmpl(length, 64);\n+    __ jcc(Assembler::aboveEqual, L_vbmiLoop);\n@@ -5351,1 +5644,2 @@\n-    __ pxor(xmm_temp0, xmm_temp2);\n+    __ vzeroupper();\n+  }\n@@ -5353,0 +5647,34 @@\n+  __ BIND(L_not512);\n+  if (VM_Version::supports_avx2()\n+      && VM_Version::supports_avx512vlbw()) {\n+    \/*\n+    ** This AVX2 encoder is based off the paper at:\n+    **      https:\/\/dl.acm.org\/doi\/10.1145\/3132709\n+    **\n+    ** We use AVX2 SIMD instructions to encode 24 bytes into 32\n+    ** output bytes.\n+    **\n+    *\/\n+    \/\/ Lengths under 32 bytes are done with scalar routine\n+    __ cmpl(length, 31);\n+    __ jcc(Assembler::belowEqual, L_process3);\n+\n+    \/\/ Set up supporting constant table data\n+    __ vmovdqu(xmm9, ExternalAddress(StubRoutines::x86::base64_avx2_shuffle_addr()), rax);\n+    \/\/ 6-bit mask for 2nd and 4th (and multiples) 6-bit values\n+    __ movl(rax, 0x0fc0fc00);\n+    __ vmovdqu(xmm1, ExternalAddress(StubRoutines::x86::base64_avx2_input_mask_addr()), rax);\n+    __ evpbroadcastd(xmm8, rax, Assembler::AVX_256bit);\n+\n+    \/\/ Multiplication constant for \"shifting\" right by 6 and 10\n+    \/\/ bits\n+    __ movl(rax, 0x04000040);\n+\n+    __ subl(length, 24);\n+    __ evpbroadcastd(xmm7, rax, Assembler::AVX_256bit);\n+\n+    \/\/ For the first load, we mask off reading of the first 4\n+    \/\/ bytes into the register. This is so we can get 4 3-byte\n+    \/\/ chunks into each lane of the register, avoiding having to\n+    \/\/ handle end conditions.  We then shuffle these bytes into a\n+    \/\/ specific order so that manipulation is easier.\n@@ -5354,1 +5682,1 @@\n-    \/\/ Multiply with the hash key\n+    \/\/ The initial read loads the XMM register like this:\n@@ -5356,36 +5684,5 @@\n-    __ movdqu(xmm_temp3, xmm_temp0);\n-    __ pclmulqdq(xmm_temp3, xmm_temp1, 0);      \/\/ xmm3 holds a0*b0\n-    __ movdqu(xmm_temp4, xmm_temp0);\n-    __ pclmulqdq(xmm_temp4, xmm_temp1, 16);     \/\/ xmm4 holds a0*b1\n-\n-    __ movdqu(xmm_temp5, xmm_temp0);\n-    __ pclmulqdq(xmm_temp5, xmm_temp1, 1);      \/\/ xmm5 holds a1*b0\n-    __ movdqu(xmm_temp6, xmm_temp0);\n-    __ pclmulqdq(xmm_temp6, xmm_temp1, 17);     \/\/ xmm6 holds a1*b1\n-\n-    __ pxor(xmm_temp4, xmm_temp5);      \/\/ xmm4 holds a0*b1 + a1*b0\n-\n-    __ movdqu(xmm_temp5, xmm_temp4);    \/\/ move the contents of xmm4 to xmm5\n-    __ psrldq(xmm_temp4, 8);    \/\/ shift by xmm4 64 bits to the right\n-    __ pslldq(xmm_temp5, 8);    \/\/ shift by xmm5 64 bits to the left\n-    __ pxor(xmm_temp3, xmm_temp5);\n-    __ pxor(xmm_temp6, xmm_temp4);      \/\/ Register pair <xmm6:xmm3> holds the result\n-                                        \/\/ of the carry-less multiplication of\n-                                        \/\/ xmm0 by xmm1.\n-\n-    \/\/ We shift the result of the multiplication by one bit position\n-    \/\/ to the left to cope for the fact that the bits are reversed.\n-    __ movdqu(xmm_temp7, xmm_temp3);\n-    __ movdqu(xmm_temp8, xmm_temp6);\n-    __ pslld(xmm_temp3, 1);\n-    __ pslld(xmm_temp6, 1);\n-    __ psrld(xmm_temp7, 31);\n-    __ psrld(xmm_temp8, 31);\n-    __ movdqu(xmm_temp9, xmm_temp7);\n-    __ pslldq(xmm_temp8, 4);\n-    __ pslldq(xmm_temp7, 4);\n-    __ psrldq(xmm_temp9, 12);\n-    __ por(xmm_temp3, xmm_temp7);\n-    __ por(xmm_temp6, xmm_temp8);\n-    __ por(xmm_temp6, xmm_temp9);\n-\n+    \/\/ Lower 128-bit lane:\n+    \/\/ +----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n+    \/\/ | XX | XX | XX | XX | A0 | A1 | A2 | B0 | B1 | B2 | C0 | C1\n+    \/\/ | C2 | D0 | D1 | D2 |\n+    \/\/ +----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n@@ -5393,1 +5690,5 @@\n-    \/\/ First phase of the reduction\n+    \/\/ Upper 128-bit lane:\n+    \/\/ +----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n+    \/\/ | E0 | E1 | E2 | F0 | F1 | F2 | G0 | G1 | G2 | H0 | H1 | H2\n+    \/\/ | XX | XX | XX | XX |\n+    \/\/ +----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n@@ -5395,15 +5696,3 @@\n-    \/\/ Move xmm3 into xmm7, xmm8, xmm9 in order to perform the shifts\n-    \/\/ independently.\n-    __ movdqu(xmm_temp7, xmm_temp3);\n-    __ movdqu(xmm_temp8, xmm_temp3);\n-    __ movdqu(xmm_temp9, xmm_temp3);\n-    __ pslld(xmm_temp7, 31);    \/\/ packed right shift shifting << 31\n-    __ pslld(xmm_temp8, 30);    \/\/ packed right shift shifting << 30\n-    __ pslld(xmm_temp9, 25);    \/\/ packed right shift shifting << 25\n-    __ pxor(xmm_temp7, xmm_temp8);      \/\/ xor the shifted versions\n-    __ pxor(xmm_temp7, xmm_temp9);\n-    __ movdqu(xmm_temp8, xmm_temp7);\n-    __ pslldq(xmm_temp7, 12);\n-    __ psrldq(xmm_temp8, 4);\n-    __ pxor(xmm_temp3, xmm_temp7);      \/\/ first phase of the reduction complete\n-\n+    \/\/ Where A0 is the first input byte, B0 is the fourth, etc.\n+    \/\/ The alphabetical significance denotes the 3 bytes to be\n+    \/\/ consumed and encoded into 4 bytes.\n@@ -5411,1 +5700,5 @@\n-    \/\/ Second phase of the reduction\n+    \/\/ We then shuffle the register so each 32-bit word contains\n+    \/\/ the sequence:\n+    \/\/    A1 A0 A2 A1, B1, B0, B2, B1, etc.\n+    \/\/ Each of these byte sequences are then manipulated into 4\n+    \/\/ 6-bit values ready for encoding.\n@@ -5413,19 +5706,105 @@\n-    \/\/ Make 3 copies of xmm3 in xmm2, xmm4, xmm5 for doing these\n-    \/\/ shift operations.\n-    __ movdqu(xmm_temp2, xmm_temp3);\n-    __ movdqu(xmm_temp4, xmm_temp3);\n-    __ movdqu(xmm_temp5, xmm_temp3);\n-    __ psrld(xmm_temp2, 1);     \/\/ packed left shifting >> 1\n-    __ psrld(xmm_temp4, 2);     \/\/ packed left shifting >> 2\n-    __ psrld(xmm_temp5, 7);     \/\/ packed left shifting >> 7\n-    __ pxor(xmm_temp2, xmm_temp4);      \/\/ xor the shifted versions\n-    __ pxor(xmm_temp2, xmm_temp5);\n-    __ pxor(xmm_temp2, xmm_temp8);\n-    __ pxor(xmm_temp3, xmm_temp2);\n-    __ pxor(xmm_temp6, xmm_temp3);      \/\/ the result is in xmm6\n-\n-    __ decrement(blocks);\n-    __ jcc(Assembler::zero, L_exit);\n-    __ movdqu(xmm_temp0, xmm_temp6);\n-    __ addptr(data, 16);\n-    __ jmp(L_ghash_loop);\n+    \/\/ If we focus on one set of 3-byte chunks, changing the\n+    \/\/ nomenclature such that A0 => a, A1 => b, and A2 => c, we\n+    \/\/ shuffle such that each 24-bit chunk contains:\n+    \/\/\n+    \/\/ b7 b6 b5 b4 b3 b2 b1 b0 | a7 a6 a5 a4 a3 a2 a1 a0 | c7 c6\n+    \/\/ c5 c4 c3 c2 c1 c0 | b7 b6 b5 b4 b3 b2 b1 b0\n+    \/\/ Explain this step.\n+    \/\/ b3 b2 b1 b0 c5 c4 c3 c2 | c1 c0 d5 d4 d3 d2 d1 d0 | a5 a4\n+    \/\/ a3 a2 a1 a0 b5 b4 | b3 b2 b1 b0 c5 c4 c3 c2\n+    \/\/\n+    \/\/ W first and off all but bits 4-9 and 16-21 (c5..c0 and\n+    \/\/ a5..a0) and shift them using a vector multiplication\n+    \/\/ operation (vpmulhuw) which effectively shifts c right by 6\n+    \/\/ bits and a right by 10 bits.  We similarly mask bits 10-15\n+    \/\/ (d5..d0) and 22-27 (b5..b0) and shift them left by 8 and 4\n+    \/\/ bits respectively.  This is done using vpmullw.  We end up\n+    \/\/ with 4 6-bit values, thus splitting the 3 input bytes,\n+    \/\/ ready for encoding:\n+    \/\/    0 0 d5..d0 0 0 c5..c0 0 0 b5..b0 0 0 a5..a0\n+    \/\/\n+    \/\/ For translation, we recognize that there are 5 distinct\n+    \/\/ ranges of legal Base64 characters as below:\n+    \/\/\n+    \/\/   +-------------+-------------+------------+\n+    \/\/   | 6-bit value | ASCII range |   offset   |\n+    \/\/   +-------------+-------------+------------+\n+    \/\/   |    0..25    |    A..Z     |     65     |\n+    \/\/   |   26..51    |    a..z     |     71     |\n+    \/\/   |   52..61    |    0..9     |     -4     |\n+    \/\/   |     62      |   + or -    | -19 or -17 |\n+    \/\/   |     63      |   \/ or _    | -16 or 32  |\n+    \/\/   +-------------+-------------+------------+\n+    \/\/\n+    \/\/ We note that vpshufb does a parallel lookup in a\n+    \/\/ destination register using the lower 4 bits of bytes from a\n+    \/\/ source register.  If we use a saturated subtraction and\n+    \/\/ subtract 51 from each 6-bit value, bytes from [0,51]\n+    \/\/ saturate to 0, and [52,63] map to a range of [1,12].  We\n+    \/\/ distinguish the [0,25] and [26,51] ranges by assigning a\n+    \/\/ value of 13 for all 6-bit values less than 26.  We end up\n+    \/\/ with:\n+    \/\/\n+    \/\/   +-------------+-------------+------------+\n+    \/\/   | 6-bit value |   Reduced   |   offset   |\n+    \/\/   +-------------+-------------+------------+\n+    \/\/   |    0..25    |     13      |     65     |\n+    \/\/   |   26..51    |      0      |     71     |\n+    \/\/   |   52..61    |    0..9     |     -4     |\n+    \/\/   |     62      |     11      | -19 or -17 |\n+    \/\/   |     63      |     12      | -16 or 32  |\n+    \/\/   +-------------+-------------+------------+\n+    \/\/\n+    \/\/ We then use a final vpshufb to add the appropriate offset,\n+    \/\/ translating the bytes.\n+    \/\/\n+    \/\/ Load input bytes - only 28 bytes.  Mask the first load to\n+    \/\/ not load into the full register.\n+    __ vpmaskmovd(xmm1, xmm1, Address(source, start_offset, Address::times_1, -4), Assembler::AVX_256bit);\n+\n+    \/\/ Move 3-byte chunks of input (12 bytes) into 16 bytes,\n+    \/\/ ordering by:\n+    \/\/   1, 0, 2, 1; 4, 3, 5, 4; etc.  This groups 6-bit chunks\n+    \/\/   for easy masking\n+    __ vpshufb(xmm1, xmm1, xmm9, Assembler::AVX_256bit);\n+\n+    __ addl(start_offset, 24);\n+\n+    \/\/ Load masking register for first and third (and multiples)\n+    \/\/ 6-bit values.\n+    __ movl(rax, 0x003f03f0);\n+    __ evpbroadcastd(xmm6, rax, Assembler::AVX_256bit);\n+    \/\/ Multiplication constant for \"shifting\" left by 4 and 8 bits\n+    __ movl(rax, 0x01000010);\n+    __ evpbroadcastd(xmm5, rax, Assembler::AVX_256bit);\n+\n+    \/\/ Isolate 6-bit chunks of interest\n+    __ vpand(xmm0, xmm8, xmm1, Assembler::AVX_256bit);\n+\n+    \/\/ Load constants for encoding\n+    __ movl(rax, 0x19191919);\n+    __ evpbroadcastd(xmm3, rax, Assembler::AVX_256bit);\n+    __ movl(rax, 0x33333333);\n+    __ evpbroadcastd(xmm4, rax, Assembler::AVX_256bit);\n+\n+    \/\/ Shift output bytes 0 and 2 into proper lanes\n+    __ vpmulhuw(xmm2, xmm0, xmm7, Assembler::AVX_256bit);\n+\n+    \/\/ Mask and shift output bytes 1 and 3 into proper lanes and\n+    \/\/ combine\n+    __ vpand(xmm0, xmm6, xmm1, Assembler::AVX_256bit);\n+    __ vpmullw(xmm0, xmm5, xmm0, Assembler::AVX_256bit);\n+    __ vpor(xmm0, xmm0, xmm2, Assembler::AVX_256bit);\n+\n+    \/\/ Find out which are 0..25.  This indicates which input\n+    \/\/ values fall in the range of 'A'-'Z', which require an\n+    \/\/ additional offset (see comments above)\n+    __ vpcmpgtb(xmm2, xmm0, xmm3, Assembler::AVX_256bit);\n+    __ vpsubusb(xmm1, xmm0, xmm4, Assembler::AVX_256bit);\n+    __ vpsubb(xmm1, xmm1, xmm2, Assembler::AVX_256bit);\n+\n+    \/\/ Load the proper lookup table\n+    __ lea(r11, ExternalAddress(StubRoutines::x86::base64_avx2_lut_addr()));\n+    __ movl(r15, isURL);\n+    __ shll(r15, 5);\n+    __ vmovdqu(xmm2, Address(r11, r15));\n@@ -5433,7 +5812,5 @@\n-    __ BIND(L_exit);\n-    __ pshufb(xmm_temp6, xmm_temp10);          \/\/ Byte swap 16-byte result\n-    __ movdqu(Address(state, 0), xmm_temp6);   \/\/ store the result\n-    __ leave();\n-    __ ret(0);\n-    return start;\n-  }\n+    \/\/ Shuffle the offsets based on the range calculation done\n+    \/\/ above. This allows us to add the correct offset to the\n+    \/\/ 6-bit value corresponding to the range documented above.\n+    __ vpshufb(xmm1, xmm2, xmm1, Assembler::AVX_256bit);\n+    __ vpaddb(xmm0, xmm1, xmm0, Assembler::AVX_256bit);\n@@ -5441,29 +5818,6 @@\n-  address base64_shuffle_addr()\n-  {\n-    __ align64();\n-    StubCodeMark mark(this, \"StubRoutines\", \"shuffle_base64\");\n-    address start = __ pc();\n-    assert(((unsigned long long)start & 0x3f) == 0,\n-           \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n-    __ emit_data64(0x0405030401020001, relocInfo::none);\n-    __ emit_data64(0x0a0b090a07080607, relocInfo::none);\n-    __ emit_data64(0x10110f100d0e0c0d, relocInfo::none);\n-    __ emit_data64(0x1617151613141213, relocInfo::none);\n-    __ emit_data64(0x1c1d1b1c191a1819, relocInfo::none);\n-    __ emit_data64(0x222321221f201e1f, relocInfo::none);\n-    __ emit_data64(0x2829272825262425, relocInfo::none);\n-    __ emit_data64(0x2e2f2d2e2b2c2a2b, relocInfo::none);\n-    return start;\n-  }\n-\n-  address base64_avx2_shuffle_addr()\n-  {\n-    __ align32();\n-    StubCodeMark mark(this, \"StubRoutines\", \"avx2_shuffle_base64\");\n-    address start = __ pc();\n-    __ emit_data64(0x0809070805060405, relocInfo::none);\n-    __ emit_data64(0x0e0f0d0e0b0c0a0b, relocInfo::none);\n-    __ emit_data64(0x0405030401020001, relocInfo::none);\n-    __ emit_data64(0x0a0b090a07080607, relocInfo::none);\n-    return start;\n-  }\n+    \/\/ Store the encoded bytes\n+    __ vmovdqu(Address(dest, dp), xmm0);\n+    __ addl(dp, 32);\n+\n+    __ cmpl(length, 31);\n+    __ jcc(Assembler::belowEqual, L_process3);\n@@ -5471,2 +5825,0 @@\n-  address base64_avx2_input_mask_addr()\n-  {\n@@ -5474,7 +5826,36 @@\n-    StubCodeMark mark(this, \"StubRoutines\", \"avx2_input_mask_base64\");\n-    address start = __ pc();\n-    __ emit_data64(0x8000000000000000, relocInfo::none);\n-    __ emit_data64(0x8000000080000000, relocInfo::none);\n-    __ emit_data64(0x8000000080000000, relocInfo::none);\n-    __ emit_data64(0x8000000080000000, relocInfo::none);\n-    return start;\n+    __ BIND(L_32byteLoop);\n+\n+    \/\/ Get next 32 bytes\n+    __ vmovdqu(xmm1, Address(source, start_offset, Address::times_1, -4));\n+\n+    __ subl(length, 24);\n+    __ addl(start_offset, 24);\n+\n+    \/\/ This logic is identical to the above, with only constant\n+    \/\/ register loads removed.  Shuffle the input, mask off 6-bit\n+    \/\/ chunks, shift them into place, then add the offset to\n+    \/\/ encode.\n+    __ vpshufb(xmm1, xmm1, xmm9, Assembler::AVX_256bit);\n+\n+    __ vpand(xmm0, xmm8, xmm1, Assembler::AVX_256bit);\n+    __ vpmulhuw(xmm10, xmm0, xmm7, Assembler::AVX_256bit);\n+    __ vpand(xmm0, xmm6, xmm1, Assembler::AVX_256bit);\n+    __ vpmullw(xmm0, xmm5, xmm0, Assembler::AVX_256bit);\n+    __ vpor(xmm0, xmm0, xmm10, Assembler::AVX_256bit);\n+    __ vpcmpgtb(xmm10, xmm0, xmm3, Assembler::AVX_256bit);\n+    __ vpsubusb(xmm1, xmm0, xmm4, Assembler::AVX_256bit);\n+    __ vpsubb(xmm1, xmm1, xmm10, Assembler::AVX_256bit);\n+    __ vpshufb(xmm1, xmm2, xmm1, Assembler::AVX_256bit);\n+    __ vpaddb(xmm0, xmm1, xmm0, Assembler::AVX_256bit);\n+\n+    \/\/ Store the encoded bytes\n+    __ vmovdqu(Address(dest, dp), xmm0);\n+    __ addl(dp, 32);\n+\n+    __ cmpl(length, 31);\n+    __ jcc(Assembler::above, L_32byteLoop);\n+\n+    __ BIND(L_process3);\n+    __ vzeroupper();\n+  } else {\n+    __ BIND(L_process3);\n@@ -5483,67 +5864,2 @@\n-  address base64_avx2_lut_addr()\n-  {\n-    __ align32();\n-    StubCodeMark mark(this, \"StubRoutines\", \"avx2_lut_base64\");\n-    address start = __ pc();\n-    __ emit_data64(0xfcfcfcfcfcfc4741, relocInfo::none);\n-    __ emit_data64(0x0000f0edfcfcfcfc, relocInfo::none);\n-    __ emit_data64(0xfcfcfcfcfcfc4741, relocInfo::none);\n-    __ emit_data64(0x0000f0edfcfcfcfc, relocInfo::none);\n-\n-    \/\/ URL LUT\n-    __ emit_data64(0xfcfcfcfcfcfc4741, relocInfo::none);\n-    __ emit_data64(0x000020effcfcfcfc, relocInfo::none);\n-    __ emit_data64(0xfcfcfcfcfcfc4741, relocInfo::none);\n-    __ emit_data64(0x000020effcfcfcfc, relocInfo::none);\n-    return start;\n-  }\n-\n-  address base64_encoding_table_addr()\n-  {\n-    __ align64();\n-    StubCodeMark mark(this, \"StubRoutines\", \"encoding_table_base64\");\n-    address start = __ pc();\n-    assert(((unsigned long long)start & 0x3f) == 0, \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n-    __ emit_data64(0x4847464544434241, relocInfo::none);\n-    __ emit_data64(0x504f4e4d4c4b4a49, relocInfo::none);\n-    __ emit_data64(0x5857565554535251, relocInfo::none);\n-    __ emit_data64(0x6665646362615a59, relocInfo::none);\n-    __ emit_data64(0x6e6d6c6b6a696867, relocInfo::none);\n-    __ emit_data64(0x767574737271706f, relocInfo::none);\n-    __ emit_data64(0x333231307a797877, relocInfo::none);\n-    __ emit_data64(0x2f2b393837363534, relocInfo::none);\n-\n-    \/\/ URL table\n-    __ emit_data64(0x4847464544434241, relocInfo::none);\n-    __ emit_data64(0x504f4e4d4c4b4a49, relocInfo::none);\n-    __ emit_data64(0x5857565554535251, relocInfo::none);\n-    __ emit_data64(0x6665646362615a59, relocInfo::none);\n-    __ emit_data64(0x6e6d6c6b6a696867, relocInfo::none);\n-    __ emit_data64(0x767574737271706f, relocInfo::none);\n-    __ emit_data64(0x333231307a797877, relocInfo::none);\n-    __ emit_data64(0x5f2d393837363534, relocInfo::none);\n-    return start;\n-  }\n-\n-  \/\/ Code for generating Base64 encoding.\n-  \/\/ Intrinsic function prototype in Base64.java:\n-  \/\/ private void encodeBlock(byte[] src, int sp, int sl, byte[] dst, int dp,\n-  \/\/ boolean isURL) {\n-  address generate_base64_encodeBlock()\n-  {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"implEncode\");\n-    address start = __ pc();\n-    __ enter();\n-\n-    \/\/ Save callee-saved registers before using them\n-    __ push(r12);\n-    __ push(r13);\n-    __ push(r14);\n-    __ push(r15);\n-\n-    \/\/ arguments\n-    const Register source = c_rarg0;       \/\/ Source Array\n-    const Register start_offset = c_rarg1; \/\/ start offset\n-    const Register end_offset = c_rarg2;   \/\/ end offset\n-    const Register dest = c_rarg3;   \/\/ destination array\n+  __ cmpl(length, 3);\n+  __ jcc(Assembler::below, L_exit);\n@@ -5551,11 +5867,5 @@\n-#ifndef _WIN64\n-    const Register dp = c_rarg4;    \/\/ Position for writing to dest array\n-    const Register isURL = c_rarg5; \/\/ Base64 or URL character set\n-#else\n-    const Address dp_mem(rbp, 6 * wordSize); \/\/ length is on stack on Win64\n-    const Address isURL_mem(rbp, 7 * wordSize);\n-    const Register isURL = r10; \/\/ pick the volatile windows register\n-    const Register dp = r12;\n-    __ movl(dp, dp_mem);\n-    __ movl(isURL, isURL_mem);\n-#endif\n+  \/\/ Load the encoding table based on isURL\n+  __ lea(r11, ExternalAddress(StubRoutines::x86::base64_encoding_table_addr()));\n+  __ movl(r15, isURL);\n+  __ shll(r15, 6);\n+  __ addptr(r11, r15);\n@@ -5563,3 +5873,1 @@\n-    const Register length = r14;\n-    const Register encode_table = r13;\n-    Label L_process3, L_exit, L_processdata, L_vbmiLoop, L_not512, L_32byteLoop;\n+  __ BIND(L_processdata);\n@@ -5567,5 +5875,4 @@\n-    \/\/ calculate length from offsets\n-    __ movl(length, end_offset);\n-    __ subl(length, start_offset);\n-    __ cmpl(length, 0);\n-    __ jcc(Assembler::lessEqual, L_exit);\n+  \/\/ Load 3 bytes\n+  __ load_unsigned_byte(r15, Address(source, start_offset));\n+  __ load_unsigned_byte(r10, Address(source, start_offset, Address::times_1, 1));\n+  __ load_unsigned_byte(r13, Address(source, start_offset, Address::times_1, 2));\n@@ -5573,6 +5880,4 @@\n-    \/\/ Code for 512-bit VBMI encoding.  Encodes 48 input bytes into 64\n-    \/\/ output bytes. We read 64 input bytes and ignore the last 16, so be\n-    \/\/ sure not to read past the end of the input buffer.\n-    if (VM_Version::supports_avx512_vbmi()) {\n-      __ cmpl(length, 64); \/\/ Do not overrun input buffer.\n-      __ jcc(Assembler::below, L_not512);\n+  \/\/ Build a 32-bit word with bytes 1, 2, 0, 1\n+  __ movl(rax, r10);\n+  __ shll(r10, 24);\n+  __ orl(rax, r10);\n@@ -5580,4 +5885,1 @@\n-      __ shll(isURL, 6); \/\/ index into decode table based on isURL\n-      __ lea(encode_table, ExternalAddress(StubRoutines::x86::base64_encoding_table_addr()));\n-      __ addptr(encode_table, isURL);\n-      __ shrl(isURL, 6); \/\/ restore isURL\n+  __ subl(length, 3);\n@@ -5585,4 +5887,3 @@\n-      __ mov64(rax, 0x3036242a1016040aull); \/\/ Shifts\n-      __ evmovdquq(xmm3, ExternalAddress(StubRoutines::x86::base64_shuffle_addr()), Assembler::AVX_512bit, r15);\n-      __ evmovdquq(xmm2, Address(encode_table, 0), Assembler::AVX_512bit);\n-      __ evpbroadcastq(xmm1, rax, Assembler::AVX_512bit);\n+  __ shll(r15, 8);\n+  __ shll(r13, 16);\n+  __ orl(rax, r15);\n@@ -5590,2 +5891,1 @@\n-      __ align32();\n-      __ BIND(L_vbmiLoop);\n+  __ addl(start_offset, 3);\n@@ -5593,2 +5893,6 @@\n-      __ vpermb(xmm0, xmm3, Address(source, start_offset), Assembler::AVX_512bit);\n-      __ subl(length, 48);\n+  __ orl(rax, r13);\n+  \/\/ At this point, rax contains | byte1 | byte2 | byte0 | byte1\n+  \/\/ r13 has byte2 << 16 - need low-order 6 bits to translate.\n+  \/\/ This translated byte is the fourth output byte.\n+  __ shrl(r13, 16);\n+  __ andl(r13, 0x3f);\n@@ -5596,4 +5900,3 @@\n-      \/\/ Put the input bytes into the proper lanes for writing, then\n-      \/\/ encode them.\n-      __ evpmultishiftqb(xmm0, xmm1, xmm0, Assembler::AVX_512bit);\n-      __ vpermb(xmm0, xmm0, xmm2, Assembler::AVX_512bit);\n+  \/\/ The high-order 6 bits of r15 (byte0) is translated.\n+  \/\/ The translated byte is the first output byte.\n+  __ shrl(r15, 10);\n@@ -5601,2 +5904,2 @@\n-      \/\/ Write to destination\n-      __ evmovdquq(Address(dest, dp), xmm0, Assembler::AVX_512bit);\n+  __ load_unsigned_byte(r13, Address(r11, r13));\n+  __ load_unsigned_byte(r15, Address(r11, r15));\n@@ -5604,4 +5907,1 @@\n-      __ addptr(dest, 64);\n-      __ addptr(source, 48);\n-      __ cmpl(length, 64);\n-      __ jcc(Assembler::aboveEqual, L_vbmiLoop);\n+  __ movb(Address(dest, dp, Address::times_1, 3), r13);\n@@ -5609,2 +5909,5 @@\n-      __ vzeroupper();\n-    }\n+  \/\/ Extract high-order 4 bits of byte1 and low-order 2 bits of byte0.\n+  \/\/ This translated byte is the second output byte.\n+  __ shrl(rax, 4);\n+  __ movl(r10, rax);\n+  __ andl(rax, 0x3f);\n@@ -5612,177 +5915,1 @@\n-    __ BIND(L_not512);\n-    if (VM_Version::supports_avx2()\n-        && VM_Version::supports_avx512vlbw()) {\n-      \/*\n-      ** This AVX2 encoder is based off the paper at:\n-      **      https:\/\/dl.acm.org\/doi\/10.1145\/3132709\n-      **\n-      ** We use AVX2 SIMD instructions to encode 24 bytes into 32\n-      ** output bytes.\n-      **\n-      *\/\n-      \/\/ Lengths under 32 bytes are done with scalar routine\n-      __ cmpl(length, 31);\n-      __ jcc(Assembler::belowEqual, L_process3);\n-\n-      \/\/ Set up supporting constant table data\n-      __ vmovdqu(xmm9, ExternalAddress(StubRoutines::x86::base64_avx2_shuffle_addr()), rax);\n-      \/\/ 6-bit mask for 2nd and 4th (and multiples) 6-bit values\n-      __ movl(rax, 0x0fc0fc00);\n-      __ vmovdqu(xmm1, ExternalAddress(StubRoutines::x86::base64_avx2_input_mask_addr()), rax);\n-      __ evpbroadcastd(xmm8, rax, Assembler::AVX_256bit);\n-\n-      \/\/ Multiplication constant for \"shifting\" right by 6 and 10\n-      \/\/ bits\n-      __ movl(rax, 0x04000040);\n-\n-      __ subl(length, 24);\n-      __ evpbroadcastd(xmm7, rax, Assembler::AVX_256bit);\n-\n-      \/\/ For the first load, we mask off reading of the first 4\n-      \/\/ bytes into the register. This is so we can get 4 3-byte\n-      \/\/ chunks into each lane of the register, avoiding having to\n-      \/\/ handle end conditions.  We then shuffle these bytes into a\n-      \/\/ specific order so that manipulation is easier.\n-      \/\/\n-      \/\/ The initial read loads the XMM register like this:\n-      \/\/\n-      \/\/ Lower 128-bit lane:\n-      \/\/ +----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n-      \/\/ | XX | XX | XX | XX | A0 | A1 | A2 | B0 | B1 | B2 | C0 | C1\n-      \/\/ | C2 | D0 | D1 | D2 |\n-      \/\/ +----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n-      \/\/\n-      \/\/ Upper 128-bit lane:\n-      \/\/ +----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n-      \/\/ | E0 | E1 | E2 | F0 | F1 | F2 | G0 | G1 | G2 | H0 | H1 | H2\n-      \/\/ | XX | XX | XX | XX |\n-      \/\/ +----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n-      \/\/\n-      \/\/ Where A0 is the first input byte, B0 is the fourth, etc.\n-      \/\/ The alphabetical significance denotes the 3 bytes to be\n-      \/\/ consumed and encoded into 4 bytes.\n-      \/\/\n-      \/\/ We then shuffle the register so each 32-bit word contains\n-      \/\/ the sequence:\n-      \/\/    A1 A0 A2 A1, B1, B0, B2, B1, etc.\n-      \/\/ Each of these byte sequences are then manipulated into 4\n-      \/\/ 6-bit values ready for encoding.\n-      \/\/\n-      \/\/ If we focus on one set of 3-byte chunks, changing the\n-      \/\/ nomenclature such that A0 => a, A1 => b, and A2 => c, we\n-      \/\/ shuffle such that each 24-bit chunk contains:\n-      \/\/\n-      \/\/ b7 b6 b5 b4 b3 b2 b1 b0 | a7 a6 a5 a4 a3 a2 a1 a0 | c7 c6\n-      \/\/ c5 c4 c3 c2 c1 c0 | b7 b6 b5 b4 b3 b2 b1 b0\n-      \/\/ Explain this step.\n-      \/\/ b3 b2 b1 b0 c5 c4 c3 c2 | c1 c0 d5 d4 d3 d2 d1 d0 | a5 a4\n-      \/\/ a3 a2 a1 a0 b5 b4 | b3 b2 b1 b0 c5 c4 c3 c2\n-      \/\/\n-      \/\/ W first and off all but bits 4-9 and 16-21 (c5..c0 and\n-      \/\/ a5..a0) and shift them using a vector multiplication\n-      \/\/ operation (vpmulhuw) which effectively shifts c right by 6\n-      \/\/ bits and a right by 10 bits.  We similarly mask bits 10-15\n-      \/\/ (d5..d0) and 22-27 (b5..b0) and shift them left by 8 and 4\n-      \/\/ bits respectively.  This is done using vpmullw.  We end up\n-      \/\/ with 4 6-bit values, thus splitting the 3 input bytes,\n-      \/\/ ready for encoding:\n-      \/\/    0 0 d5..d0 0 0 c5..c0 0 0 b5..b0 0 0 a5..a0\n-      \/\/\n-      \/\/ For translation, we recognize that there are 5 distinct\n-      \/\/ ranges of legal Base64 characters as below:\n-      \/\/\n-      \/\/   +-------------+-------------+------------+\n-      \/\/   | 6-bit value | ASCII range |   offset   |\n-      \/\/   +-------------+-------------+------------+\n-      \/\/   |    0..25    |    A..Z     |     65     |\n-      \/\/   |   26..51    |    a..z     |     71     |\n-      \/\/   |   52..61    |    0..9     |     -4     |\n-      \/\/   |     62      |   + or -    | -19 or -17 |\n-      \/\/   |     63      |   \/ or _    | -16 or 32  |\n-      \/\/   +-------------+-------------+------------+\n-      \/\/\n-      \/\/ We note that vpshufb does a parallel lookup in a\n-      \/\/ destination register using the lower 4 bits of bytes from a\n-      \/\/ source register.  If we use a saturated subtraction and\n-      \/\/ subtract 51 from each 6-bit value, bytes from [0,51]\n-      \/\/ saturate to 0, and [52,63] map to a range of [1,12].  We\n-      \/\/ distinguish the [0,25] and [26,51] ranges by assigning a\n-      \/\/ value of 13 for all 6-bit values less than 26.  We end up\n-      \/\/ with:\n-      \/\/\n-      \/\/   +-------------+-------------+------------+\n-      \/\/   | 6-bit value |   Reduced   |   offset   |\n-      \/\/   +-------------+-------------+------------+\n-      \/\/   |    0..25    |     13      |     65     |\n-      \/\/   |   26..51    |      0      |     71     |\n-      \/\/   |   52..61    |    0..9     |     -4     |\n-      \/\/   |     62      |     11      | -19 or -17 |\n-      \/\/   |     63      |     12      | -16 or 32  |\n-      \/\/   +-------------+-------------+------------+\n-      \/\/\n-      \/\/ We then use a final vpshufb to add the appropriate offset,\n-      \/\/ translating the bytes.\n-      \/\/\n-      \/\/ Load input bytes - only 28 bytes.  Mask the first load to\n-      \/\/ not load into the full register.\n-      __ vpmaskmovd(xmm1, xmm1, Address(source, start_offset, Address::times_1, -4), Assembler::AVX_256bit);\n-\n-      \/\/ Move 3-byte chunks of input (12 bytes) into 16 bytes,\n-      \/\/ ordering by:\n-      \/\/   1, 0, 2, 1; 4, 3, 5, 4; etc.  This groups 6-bit chunks\n-      \/\/   for easy masking\n-      __ vpshufb(xmm1, xmm1, xmm9, Assembler::AVX_256bit);\n-\n-      __ addl(start_offset, 24);\n-\n-      \/\/ Load masking register for first and third (and multiples)\n-      \/\/ 6-bit values.\n-      __ movl(rax, 0x003f03f0);\n-      __ evpbroadcastd(xmm6, rax, Assembler::AVX_256bit);\n-      \/\/ Multiplication constant for \"shifting\" left by 4 and 8 bits\n-      __ movl(rax, 0x01000010);\n-      __ evpbroadcastd(xmm5, rax, Assembler::AVX_256bit);\n-\n-      \/\/ Isolate 6-bit chunks of interest\n-      __ vpand(xmm0, xmm8, xmm1, Assembler::AVX_256bit);\n-\n-      \/\/ Load constants for encoding\n-      __ movl(rax, 0x19191919);\n-      __ evpbroadcastd(xmm3, rax, Assembler::AVX_256bit);\n-      __ movl(rax, 0x33333333);\n-      __ evpbroadcastd(xmm4, rax, Assembler::AVX_256bit);\n-\n-      \/\/ Shift output bytes 0 and 2 into proper lanes\n-      __ vpmulhuw(xmm2, xmm0, xmm7, Assembler::AVX_256bit);\n-\n-      \/\/ Mask and shift output bytes 1 and 3 into proper lanes and\n-      \/\/ combine\n-      __ vpand(xmm0, xmm6, xmm1, Assembler::AVX_256bit);\n-      __ vpmullw(xmm0, xmm5, xmm0, Assembler::AVX_256bit);\n-      __ vpor(xmm0, xmm0, xmm2, Assembler::AVX_256bit);\n-\n-      \/\/ Find out which are 0..25.  This indicates which input\n-      \/\/ values fall in the range of 'A'-'Z', which require an\n-      \/\/ additional offset (see comments above)\n-      __ vpcmpgtb(xmm2, xmm0, xmm3, Assembler::AVX_256bit);\n-      __ vpsubusb(xmm1, xmm0, xmm4, Assembler::AVX_256bit);\n-      __ vpsubb(xmm1, xmm1, xmm2, Assembler::AVX_256bit);\n-\n-      \/\/ Load the proper lookup table\n-      __ lea(r11, ExternalAddress(StubRoutines::x86::base64_avx2_lut_addr()));\n-      __ movl(r15, isURL);\n-      __ shll(r15, 5);\n-      __ vmovdqu(xmm2, Address(r11, r15));\n-\n-      \/\/ Shuffle the offsets based on the range calculation done\n-      \/\/ above. This allows us to add the correct offset to the\n-      \/\/ 6-bit value corresponding to the range documented above.\n-      __ vpshufb(xmm1, xmm2, xmm1, Assembler::AVX_256bit);\n-      __ vpaddb(xmm0, xmm1, xmm0, Assembler::AVX_256bit);\n-\n-      \/\/ Store the encoded bytes\n-      __ vmovdqu(Address(dest, dp), xmm0);\n-      __ addl(dp, 32);\n-\n-      __ cmpl(length, 31);\n-      __ jcc(Assembler::belowEqual, L_process3);\n+  __ movb(Address(dest, dp, Address::times_1, 0), r15);\n@@ -5790,38 +5917,1 @@\n-      __ align32();\n-      __ BIND(L_32byteLoop);\n-\n-      \/\/ Get next 32 bytes\n-      __ vmovdqu(xmm1, Address(source, start_offset, Address::times_1, -4));\n-\n-      __ subl(length, 24);\n-      __ addl(start_offset, 24);\n-\n-      \/\/ This logic is identical to the above, with only constant\n-      \/\/ register loads removed.  Shuffle the input, mask off 6-bit\n-      \/\/ chunks, shift them into place, then add the offset to\n-      \/\/ encode.\n-      __ vpshufb(xmm1, xmm1, xmm9, Assembler::AVX_256bit);\n-\n-      __ vpand(xmm0, xmm8, xmm1, Assembler::AVX_256bit);\n-      __ vpmulhuw(xmm10, xmm0, xmm7, Assembler::AVX_256bit);\n-      __ vpand(xmm0, xmm6, xmm1, Assembler::AVX_256bit);\n-      __ vpmullw(xmm0, xmm5, xmm0, Assembler::AVX_256bit);\n-      __ vpor(xmm0, xmm0, xmm10, Assembler::AVX_256bit);\n-      __ vpcmpgtb(xmm10, xmm0, xmm3, Assembler::AVX_256bit);\n-      __ vpsubusb(xmm1, xmm0, xmm4, Assembler::AVX_256bit);\n-      __ vpsubb(xmm1, xmm1, xmm10, Assembler::AVX_256bit);\n-      __ vpshufb(xmm1, xmm2, xmm1, Assembler::AVX_256bit);\n-      __ vpaddb(xmm0, xmm1, xmm0, Assembler::AVX_256bit);\n-\n-      \/\/ Store the encoded bytes\n-      __ vmovdqu(Address(dest, dp), xmm0);\n-      __ addl(dp, 32);\n-\n-      __ cmpl(length, 31);\n-      __ jcc(Assembler::above, L_32byteLoop);\n-\n-      __ BIND(L_process3);\n-      __ vzeroupper();\n-    } else {\n-      __ BIND(L_process3);\n-    }\n+  __ load_unsigned_byte(rax, Address(r11, rax));\n@@ -5829,2 +5919,4 @@\n-    __ cmpl(length, 3);\n-    __ jcc(Assembler::below, L_exit);\n+  \/\/ Extract low-order 2 bits of byte1 and high-order 4 bits of byte2.\n+  \/\/ This translated byte is the third output byte.\n+  __ shrl(r10, 18);\n+  __ andl(r10, 0x3f);\n@@ -5832,5 +5924,1 @@\n-    \/\/ Load the encoding table based on isURL\n-    __ lea(r11, ExternalAddress(StubRoutines::x86::base64_encoding_table_addr()));\n-    __ movl(r15, isURL);\n-    __ shll(r15, 6);\n-    __ addptr(r11, r15);\n+  __ load_unsigned_byte(r10, Address(r11, r10));\n@@ -5838,1 +5926,2 @@\n-    __ BIND(L_processdata);\n+  __ movb(Address(dest, dp, Address::times_1, 1), rax);\n+  __ movb(Address(dest, dp, Address::times_1, 2), r10);\n@@ -5840,4 +5929,3 @@\n-    \/\/ Load 3 bytes\n-    __ load_unsigned_byte(r15, Address(source, start_offset));\n-    __ load_unsigned_byte(r10, Address(source, start_offset, Address::times_1, 1));\n-    __ load_unsigned_byte(r13, Address(source, start_offset, Address::times_1, 2));\n+  __ addl(dp, 4);\n+  __ cmpl(length, 3);\n+  __ jcc(Assembler::aboveEqual, L_processdata);\n@@ -5845,4 +5933,7 @@\n-    \/\/ Build a 32-bit word with bytes 1, 2, 0, 1\n-    __ movl(rax, r10);\n-    __ shll(r10, 24);\n-    __ orl(rax, r10);\n+  __ BIND(L_exit);\n+  __ pop(r15);\n+  __ pop(r14);\n+  __ pop(r13);\n+  __ pop(r12);\n+  __ leave();\n+  __ ret(0);\n@@ -5850,1 +5941,2 @@\n-    __ subl(length, 3);\n+  return start;\n+}\n@@ -5852,3 +5944,16 @@\n-    __ shll(r15, 8);\n-    __ shll(r13, 16);\n-    __ orl(rax, r15);\n+\/\/ base64 AVX512vbmi tables\n+address StubGenerator::base64_vbmi_lookup_lo_addr() {\n+  __ align64();\n+  StubCodeMark mark(this, \"StubRoutines\", \"lookup_lo_base64\");\n+  address start = __ pc();\n+\n+  assert(((unsigned long long)start & 0x3f) == 0,\n+         \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n+  __ emit_data64(0x8080808080808080, relocInfo::none);\n+  __ emit_data64(0x8080808080808080, relocInfo::none);\n+  __ emit_data64(0x8080808080808080, relocInfo::none);\n+  __ emit_data64(0x8080808080808080, relocInfo::none);\n+  __ emit_data64(0x8080808080808080, relocInfo::none);\n+  __ emit_data64(0x3f8080803e808080, relocInfo::none);\n+  __ emit_data64(0x3b3a393837363534, relocInfo::none);\n+  __ emit_data64(0x8080808080803d3c, relocInfo::none);\n@@ -5856,1 +5961,2 @@\n-    __ addl(start_offset, 3);\n+  return start;\n+}\n@@ -5858,6 +5964,15 @@\n-    __ orl(rax, r13);\n-    \/\/ At this point, rax contains | byte1 | byte2 | byte0 | byte1\n-    \/\/ r13 has byte2 << 16 - need low-order 6 bits to translate.\n-    \/\/ This translated byte is the fourth output byte.\n-    __ shrl(r13, 16);\n-    __ andl(r13, 0x3f);\n+address StubGenerator::base64_vbmi_lookup_hi_addr() {\n+  __ align64();\n+  StubCodeMark mark(this, \"StubRoutines\", \"lookup_hi_base64\");\n+  address start = __ pc();\n+\n+  assert(((unsigned long long)start & 0x3f) == 0,\n+         \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n+  __ emit_data64(0x0605040302010080, relocInfo::none);\n+  __ emit_data64(0x0e0d0c0b0a090807, relocInfo::none);\n+  __ emit_data64(0x161514131211100f, relocInfo::none);\n+  __ emit_data64(0x8080808080191817, relocInfo::none);\n+  __ emit_data64(0x201f1e1d1c1b1a80, relocInfo::none);\n+  __ emit_data64(0x2827262524232221, relocInfo::none);\n+  __ emit_data64(0x302f2e2d2c2b2a29, relocInfo::none);\n+  __ emit_data64(0x8080808080333231, relocInfo::none);\n@@ -5865,3 +5980,17 @@\n-    \/\/ The high-order 6 bits of r15 (byte0) is translated.\n-    \/\/ The translated byte is the first output byte.\n-    __ shrl(r15, 10);\n+  return start;\n+}\n+address StubGenerator::base64_vbmi_lookup_lo_url_addr() {\n+  __ align64();\n+  StubCodeMark mark(this, \"StubRoutines\", \"lookup_lo_base64url\");\n+  address start = __ pc();\n+\n+  assert(((unsigned long long)start & 0x3f) == 0,\n+         \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n+  __ emit_data64(0x8080808080808080, relocInfo::none);\n+  __ emit_data64(0x8080808080808080, relocInfo::none);\n+  __ emit_data64(0x8080808080808080, relocInfo::none);\n+  __ emit_data64(0x8080808080808080, relocInfo::none);\n+  __ emit_data64(0x8080808080808080, relocInfo::none);\n+  __ emit_data64(0x80803e8080808080, relocInfo::none);\n+  __ emit_data64(0x3b3a393837363534, relocInfo::none);\n+  __ emit_data64(0x8080808080803d3c, relocInfo::none);\n@@ -5869,2 +5998,2 @@\n-    __ load_unsigned_byte(r13, Address(r11, r13));\n-    __ load_unsigned_byte(r15, Address(r11, r15));\n+  return start;\n+}\n@@ -5872,1 +6001,15 @@\n-    __ movb(Address(dest, dp, Address::times_1, 3), r13);\n+address StubGenerator::base64_vbmi_lookup_hi_url_addr() {\n+  __ align64();\n+  StubCodeMark mark(this, \"StubRoutines\", \"lookup_hi_base64url\");\n+  address start = __ pc();\n+\n+  assert(((unsigned long long)start & 0x3f) == 0,\n+         \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n+  __ emit_data64(0x0605040302010080, relocInfo::none);\n+  __ emit_data64(0x0e0d0c0b0a090807, relocInfo::none);\n+  __ emit_data64(0x161514131211100f, relocInfo::none);\n+  __ emit_data64(0x3f80808080191817, relocInfo::none);\n+  __ emit_data64(0x201f1e1d1c1b1a80, relocInfo::none);\n+  __ emit_data64(0x2827262524232221, relocInfo::none);\n+  __ emit_data64(0x302f2e2d2c2b2a29, relocInfo::none);\n+  __ emit_data64(0x8080808080333231, relocInfo::none);\n@@ -5874,5 +6017,2 @@\n-    \/\/ Extract high-order 4 bits of byte1 and low-order 2 bits of byte0.\n-    \/\/ This translated byte is the second output byte.\n-    __ shrl(rax, 4);\n-    __ movl(r10, rax);\n-    __ andl(rax, 0x3f);\n+  return start;\n+}\n@@ -5880,1 +6020,15 @@\n-    __ movb(Address(dest, dp, Address::times_1, 0), r15);\n+address StubGenerator::base64_vbmi_pack_vec_addr() {\n+  __ align64();\n+  StubCodeMark mark(this, \"StubRoutines\", \"pack_vec_base64\");\n+  address start = __ pc();\n+\n+  assert(((unsigned long long)start & 0x3f) == 0,\n+         \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n+  __ emit_data64(0x090a040506000102, relocInfo::none);\n+  __ emit_data64(0x161011120c0d0e08, relocInfo::none);\n+  __ emit_data64(0x1c1d1e18191a1415, relocInfo::none);\n+  __ emit_data64(0x292a242526202122, relocInfo::none);\n+  __ emit_data64(0x363031322c2d2e28, relocInfo::none);\n+  __ emit_data64(0x3c3d3e38393a3435, relocInfo::none);\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n@@ -5882,1 +6036,2 @@\n-    __ load_unsigned_byte(rax, Address(r11, rax));\n+  return start;\n+}\n@@ -5884,4 +6039,15 @@\n-    \/\/ Extract low-order 2 bits of byte1 and high-order 4 bits of byte2.\n-    \/\/ This translated byte is the third output byte.\n-    __ shrl(r10, 18);\n-    __ andl(r10, 0x3f);\n+address StubGenerator::base64_vbmi_join_0_1_addr() {\n+  __ align64();\n+  StubCodeMark mark(this, \"StubRoutines\", \"join_0_1_base64\");\n+  address start = __ pc();\n+\n+  assert(((unsigned long long)start & 0x3f) == 0,\n+         \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n+  __ emit_data64(0x090a040506000102, relocInfo::none);\n+  __ emit_data64(0x161011120c0d0e08, relocInfo::none);\n+  __ emit_data64(0x1c1d1e18191a1415, relocInfo::none);\n+  __ emit_data64(0x292a242526202122, relocInfo::none);\n+  __ emit_data64(0x363031322c2d2e28, relocInfo::none);\n+  __ emit_data64(0x3c3d3e38393a3435, relocInfo::none);\n+  __ emit_data64(0x494a444546404142, relocInfo::none);\n+  __ emit_data64(0x565051524c4d4e48, relocInfo::none);\n@@ -5889,1 +6055,2 @@\n-    __ load_unsigned_byte(r10, Address(r11, r10));\n+  return start;\n+}\n@@ -5891,2 +6058,15 @@\n-    __ movb(Address(dest, dp, Address::times_1, 1), rax);\n-    __ movb(Address(dest, dp, Address::times_1, 2), r10);\n+address StubGenerator::base64_vbmi_join_1_2_addr() {\n+  __ align64();\n+  StubCodeMark mark(this, \"StubRoutines\", \"join_1_2_base64\");\n+  address start = __ pc();\n+\n+  assert(((unsigned long long)start & 0x3f) == 0,\n+         \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n+  __ emit_data64(0x1c1d1e18191a1415, relocInfo::none);\n+  __ emit_data64(0x292a242526202122, relocInfo::none);\n+  __ emit_data64(0x363031322c2d2e28, relocInfo::none);\n+  __ emit_data64(0x3c3d3e38393a3435, relocInfo::none);\n+  __ emit_data64(0x494a444546404142, relocInfo::none);\n+  __ emit_data64(0x565051524c4d4e48, relocInfo::none);\n+  __ emit_data64(0x5c5d5e58595a5455, relocInfo::none);\n+  __ emit_data64(0x696a646566606162, relocInfo::none);\n@@ -5894,3 +6074,2 @@\n-    __ addl(dp, 4);\n-    __ cmpl(length, 3);\n-    __ jcc(Assembler::aboveEqual, L_processdata);\n+  return start;\n+}\n@@ -5898,217 +6077,92 @@\n-    __ BIND(L_exit);\n-    __ pop(r15);\n-    __ pop(r14);\n-    __ pop(r13);\n-    __ pop(r12);\n-    __ leave();\n-    __ ret(0);\n-    return start;\n-  }\n-\n-  \/\/ base64 AVX512vbmi tables\n-  address base64_vbmi_lookup_lo_addr() {\n-    __ align64();\n-    StubCodeMark mark(this, \"StubRoutines\", \"lookup_lo_base64\");\n-    address start = __ pc();\n-    assert(((unsigned long long)start & 0x3f) == 0,\n-           \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n-    __ emit_data64(0x8080808080808080, relocInfo::none);\n-    __ emit_data64(0x8080808080808080, relocInfo::none);\n-    __ emit_data64(0x8080808080808080, relocInfo::none);\n-    __ emit_data64(0x8080808080808080, relocInfo::none);\n-    __ emit_data64(0x8080808080808080, relocInfo::none);\n-    __ emit_data64(0x3f8080803e808080, relocInfo::none);\n-    __ emit_data64(0x3b3a393837363534, relocInfo::none);\n-    __ emit_data64(0x8080808080803d3c, relocInfo::none);\n-    return start;\n-  }\n-\n-  address base64_vbmi_lookup_hi_addr() {\n-    __ align64();\n-    StubCodeMark mark(this, \"StubRoutines\", \"lookup_hi_base64\");\n-    address start = __ pc();\n-    assert(((unsigned long long)start & 0x3f) == 0,\n-           \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n-    __ emit_data64(0x0605040302010080, relocInfo::none);\n-    __ emit_data64(0x0e0d0c0b0a090807, relocInfo::none);\n-    __ emit_data64(0x161514131211100f, relocInfo::none);\n-    __ emit_data64(0x8080808080191817, relocInfo::none);\n-    __ emit_data64(0x201f1e1d1c1b1a80, relocInfo::none);\n-    __ emit_data64(0x2827262524232221, relocInfo::none);\n-    __ emit_data64(0x302f2e2d2c2b2a29, relocInfo::none);\n-    __ emit_data64(0x8080808080333231, relocInfo::none);\n-    return start;\n-  }\n-  address base64_vbmi_lookup_lo_url_addr() {\n-    __ align64();\n-    StubCodeMark mark(this, \"StubRoutines\", \"lookup_lo_base64url\");\n-    address start = __ pc();\n-    assert(((unsigned long long)start & 0x3f) == 0,\n-           \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n-    __ emit_data64(0x8080808080808080, relocInfo::none);\n-    __ emit_data64(0x8080808080808080, relocInfo::none);\n-    __ emit_data64(0x8080808080808080, relocInfo::none);\n-    __ emit_data64(0x8080808080808080, relocInfo::none);\n-    __ emit_data64(0x8080808080808080, relocInfo::none);\n-    __ emit_data64(0x80803e8080808080, relocInfo::none);\n-    __ emit_data64(0x3b3a393837363534, relocInfo::none);\n-    __ emit_data64(0x8080808080803d3c, relocInfo::none);\n-    return start;\n-  }\n-\n-  address base64_vbmi_lookup_hi_url_addr() {\n-    __ align64();\n-    StubCodeMark mark(this, \"StubRoutines\", \"lookup_hi_base64url\");\n-    address start = __ pc();\n-    assert(((unsigned long long)start & 0x3f) == 0,\n-           \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n-    __ emit_data64(0x0605040302010080, relocInfo::none);\n-    __ emit_data64(0x0e0d0c0b0a090807, relocInfo::none);\n-    __ emit_data64(0x161514131211100f, relocInfo::none);\n-    __ emit_data64(0x3f80808080191817, relocInfo::none);\n-    __ emit_data64(0x201f1e1d1c1b1a80, relocInfo::none);\n-    __ emit_data64(0x2827262524232221, relocInfo::none);\n-    __ emit_data64(0x302f2e2d2c2b2a29, relocInfo::none);\n-    __ emit_data64(0x8080808080333231, relocInfo::none);\n-    return start;\n-  }\n-\n-  address base64_vbmi_pack_vec_addr() {\n-    __ align64();\n-    StubCodeMark mark(this, \"StubRoutines\", \"pack_vec_base64\");\n-    address start = __ pc();\n-    assert(((unsigned long long)start & 0x3f) == 0,\n-           \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n-    __ emit_data64(0x090a040506000102, relocInfo::none);\n-    __ emit_data64(0x161011120c0d0e08, relocInfo::none);\n-    __ emit_data64(0x1c1d1e18191a1415, relocInfo::none);\n-    __ emit_data64(0x292a242526202122, relocInfo::none);\n-    __ emit_data64(0x363031322c2d2e28, relocInfo::none);\n-    __ emit_data64(0x3c3d3e38393a3435, relocInfo::none);\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    return start;\n-  }\n-\n-  address base64_vbmi_join_0_1_addr() {\n-    __ align64();\n-    StubCodeMark mark(this, \"StubRoutines\", \"join_0_1_base64\");\n-    address start = __ pc();\n-    assert(((unsigned long long)start & 0x3f) == 0,\n-           \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n-    __ emit_data64(0x090a040506000102, relocInfo::none);\n-    __ emit_data64(0x161011120c0d0e08, relocInfo::none);\n-    __ emit_data64(0x1c1d1e18191a1415, relocInfo::none);\n-    __ emit_data64(0x292a242526202122, relocInfo::none);\n-    __ emit_data64(0x363031322c2d2e28, relocInfo::none);\n-    __ emit_data64(0x3c3d3e38393a3435, relocInfo::none);\n-    __ emit_data64(0x494a444546404142, relocInfo::none);\n-    __ emit_data64(0x565051524c4d4e48, relocInfo::none);\n-    return start;\n-  }\n-\n-  address base64_vbmi_join_1_2_addr() {\n-    __ align64();\n-    StubCodeMark mark(this, \"StubRoutines\", \"join_1_2_base64\");\n-    address start = __ pc();\n-    assert(((unsigned long long)start & 0x3f) == 0,\n-           \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n-    __ emit_data64(0x1c1d1e18191a1415, relocInfo::none);\n-    __ emit_data64(0x292a242526202122, relocInfo::none);\n-    __ emit_data64(0x363031322c2d2e28, relocInfo::none);\n-    __ emit_data64(0x3c3d3e38393a3435, relocInfo::none);\n-    __ emit_data64(0x494a444546404142, relocInfo::none);\n-    __ emit_data64(0x565051524c4d4e48, relocInfo::none);\n-    __ emit_data64(0x5c5d5e58595a5455, relocInfo::none);\n-    __ emit_data64(0x696a646566606162, relocInfo::none);\n-    return start;\n-  }\n-\n-  address base64_vbmi_join_2_3_addr() {\n-    __ align64();\n-    StubCodeMark mark(this, \"StubRoutines\", \"join_2_3_base64\");\n-    address start = __ pc();\n-    assert(((unsigned long long)start & 0x3f) == 0,\n-           \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n-    __ emit_data64(0x363031322c2d2e28, relocInfo::none);\n-    __ emit_data64(0x3c3d3e38393a3435, relocInfo::none);\n-    __ emit_data64(0x494a444546404142, relocInfo::none);\n-    __ emit_data64(0x565051524c4d4e48, relocInfo::none);\n-    __ emit_data64(0x5c5d5e58595a5455, relocInfo::none);\n-    __ emit_data64(0x696a646566606162, relocInfo::none);\n-    __ emit_data64(0x767071726c6d6e68, relocInfo::none);\n-    __ emit_data64(0x7c7d7e78797a7475, relocInfo::none);\n-    return start;\n-  }\n-\n-  address base64_decoding_table_addr() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"decoding_table_base64\");\n-    address start = __ pc();\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0x3fffffff3effffff, relocInfo::none);\n-    __ emit_data64(0x3b3a393837363534, relocInfo::none);\n-    __ emit_data64(0xffffffffffff3d3c, relocInfo::none);\n-    __ emit_data64(0x06050403020100ff, relocInfo::none);\n-    __ emit_data64(0x0e0d0c0b0a090807, relocInfo::none);\n-    __ emit_data64(0x161514131211100f, relocInfo::none);\n-    __ emit_data64(0xffffffffff191817, relocInfo::none);\n-    __ emit_data64(0x201f1e1d1c1b1aff, relocInfo::none);\n-    __ emit_data64(0x2827262524232221, relocInfo::none);\n-    __ emit_data64(0x302f2e2d2c2b2a29, relocInfo::none);\n-    __ emit_data64(0xffffffffff333231, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-\n-    \/\/ URL table\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffff3effffffffff, relocInfo::none);\n-    __ emit_data64(0x3b3a393837363534, relocInfo::none);\n-    __ emit_data64(0xffffffffffff3d3c, relocInfo::none);\n-    __ emit_data64(0x06050403020100ff, relocInfo::none);\n-    __ emit_data64(0x0e0d0c0b0a090807, relocInfo::none);\n-    __ emit_data64(0x161514131211100f, relocInfo::none);\n-    __ emit_data64(0x3fffffffff191817, relocInfo::none);\n-    __ emit_data64(0x201f1e1d1c1b1aff, relocInfo::none);\n-    __ emit_data64(0x2827262524232221, relocInfo::none);\n-    __ emit_data64(0x302f2e2d2c2b2a29, relocInfo::none);\n-    __ emit_data64(0xffffffffff333231, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    return start;\n-  }\n+address StubGenerator::base64_vbmi_join_2_3_addr() {\n+  __ align64();\n+  StubCodeMark mark(this, \"StubRoutines\", \"join_2_3_base64\");\n+  address start = __ pc();\n+\n+  assert(((unsigned long long)start & 0x3f) == 0,\n+         \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n+  __ emit_data64(0x363031322c2d2e28, relocInfo::none);\n+  __ emit_data64(0x3c3d3e38393a3435, relocInfo::none);\n+  __ emit_data64(0x494a444546404142, relocInfo::none);\n+  __ emit_data64(0x565051524c4d4e48, relocInfo::none);\n+  __ emit_data64(0x5c5d5e58595a5455, relocInfo::none);\n+  __ emit_data64(0x696a646566606162, relocInfo::none);\n+  __ emit_data64(0x767071726c6d6e68, relocInfo::none);\n+  __ emit_data64(0x7c7d7e78797a7475, relocInfo::none);\n+\n+  return start;\n+}\n+\n+address StubGenerator::base64_decoding_table_addr() {\n+  StubCodeMark mark(this, \"StubRoutines\", \"decoding_table_base64\");\n+  address start = __ pc();\n+\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0x3fffffff3effffff, relocInfo::none);\n+  __ emit_data64(0x3b3a393837363534, relocInfo::none);\n+  __ emit_data64(0xffffffffffff3d3c, relocInfo::none);\n+  __ emit_data64(0x06050403020100ff, relocInfo::none);\n+  __ emit_data64(0x0e0d0c0b0a090807, relocInfo::none);\n+  __ emit_data64(0x161514131211100f, relocInfo::none);\n+  __ emit_data64(0xffffffffff191817, relocInfo::none);\n+  __ emit_data64(0x201f1e1d1c1b1aff, relocInfo::none);\n+  __ emit_data64(0x2827262524232221, relocInfo::none);\n+  __ emit_data64(0x302f2e2d2c2b2a29, relocInfo::none);\n+  __ emit_data64(0xffffffffff333231, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+\n+  \/\/ URL table\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffff3effffffffff, relocInfo::none);\n+  __ emit_data64(0x3b3a393837363534, relocInfo::none);\n+  __ emit_data64(0xffffffffffff3d3c, relocInfo::none);\n+  __ emit_data64(0x06050403020100ff, relocInfo::none);\n+  __ emit_data64(0x0e0d0c0b0a090807, relocInfo::none);\n+  __ emit_data64(0x161514131211100f, relocInfo::none);\n+  __ emit_data64(0x3fffffffff191817, relocInfo::none);\n+  __ emit_data64(0x201f1e1d1c1b1aff, relocInfo::none);\n+  __ emit_data64(0x2827262524232221, relocInfo::none);\n+  __ emit_data64(0x302f2e2d2c2b2a29, relocInfo::none);\n+  __ emit_data64(0xffffffffff333231, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+\n+  return start;\n+}\n@@ -6123,19 +6177,20 @@\n-  address generate_base64_decodeBlock() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"implDecode\");\n-    address start = __ pc();\n-    __ enter();\n-\n-    \/\/ Save callee-saved registers before using them\n-    __ push(r12);\n-    __ push(r13);\n-    __ push(r14);\n-    __ push(r15);\n-    __ push(rbx);\n-\n-    \/\/ arguments\n-    const Register source = c_rarg0; \/\/ Source Array\n-    const Register start_offset = c_rarg1; \/\/ start offset\n-    const Register end_offset = c_rarg2; \/\/ end offset\n-    const Register dest = c_rarg3; \/\/ destination array\n-    const Register isMIME = rbx;\n+address StubGenerator::generate_base64_decodeBlock() {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"implDecode\");\n+  address start = __ pc();\n+\n+  __ enter();\n+\n+  \/\/ Save callee-saved registers before using them\n+  __ push(r12);\n+  __ push(r13);\n+  __ push(r14);\n+  __ push(r15);\n+  __ push(rbx);\n+\n+  \/\/ arguments\n+  const Register source = c_rarg0; \/\/ Source Array\n+  const Register start_offset = c_rarg1; \/\/ start offset\n+  const Register end_offset = c_rarg2; \/\/ end offset\n+  const Register dest = c_rarg3; \/\/ destination array\n+  const Register isMIME = rbx;\n@@ -6144,3 +6199,3 @@\n-    const Register dp = c_rarg4;  \/\/ Position for writing to dest array\n-    const Register isURL = c_rarg5;\/\/ Base64 or URL character set\n-    __ movl(isMIME, Address(rbp, 2 * wordSize));\n+  const Register dp = c_rarg4;  \/\/ Position for writing to dest array\n+  const Register isURL = c_rarg5;\/\/ Base64 or URL character set\n+  __ movl(isMIME, Address(rbp, 2 * wordSize));\n@@ -6148,7 +6203,7 @@\n-    const Address  dp_mem(rbp, 6 * wordSize);  \/\/ length is on stack on Win64\n-    const Address isURL_mem(rbp, 7 * wordSize);\n-    const Register isURL = r10;      \/\/ pick the volatile windows register\n-    const Register dp = r12;\n-    __ movl(dp, dp_mem);\n-    __ movl(isURL, isURL_mem);\n-    __ movl(isMIME, Address(rbp, 8 * wordSize));\n+  const Address  dp_mem(rbp, 6 * wordSize);  \/\/ length is on stack on Win64\n+  const Address isURL_mem(rbp, 7 * wordSize);\n+  const Register isURL = r10;      \/\/ pick the volatile windows register\n+  const Register dp = r12;\n+  __ movl(dp, dp_mem);\n+  __ movl(isURL, isURL_mem);\n+  __ movl(isMIME, Address(rbp, 8 * wordSize));\n@@ -6157,79 +6212,79 @@\n-    const XMMRegister lookup_lo = xmm5;\n-    const XMMRegister lookup_hi = xmm6;\n-    const XMMRegister errorvec = xmm7;\n-    const XMMRegister pack16_op = xmm9;\n-    const XMMRegister pack32_op = xmm8;\n-    const XMMRegister input0 = xmm3;\n-    const XMMRegister input1 = xmm20;\n-    const XMMRegister input2 = xmm21;\n-    const XMMRegister input3 = xmm19;\n-    const XMMRegister join01 = xmm12;\n-    const XMMRegister join12 = xmm11;\n-    const XMMRegister join23 = xmm10;\n-    const XMMRegister translated0 = xmm2;\n-    const XMMRegister translated1 = xmm1;\n-    const XMMRegister translated2 = xmm0;\n-    const XMMRegister translated3 = xmm4;\n-\n-    const XMMRegister merged0 = xmm2;\n-    const XMMRegister merged1 = xmm1;\n-    const XMMRegister merged2 = xmm0;\n-    const XMMRegister merged3 = xmm4;\n-    const XMMRegister merge_ab_bc0 = xmm2;\n-    const XMMRegister merge_ab_bc1 = xmm1;\n-    const XMMRegister merge_ab_bc2 = xmm0;\n-    const XMMRegister merge_ab_bc3 = xmm4;\n-\n-    const XMMRegister pack24bits = xmm4;\n-\n-    const Register length = r14;\n-    const Register output_size = r13;\n-    const Register output_mask = r15;\n-    const KRegister input_mask = k1;\n-\n-    const XMMRegister input_initial_valid_b64 = xmm0;\n-    const XMMRegister tmp = xmm10;\n-    const XMMRegister mask = xmm0;\n-    const XMMRegister invalid_b64 = xmm1;\n-\n-    Label L_process256, L_process64, L_process64Loop, L_exit, L_processdata, L_loadURL;\n-    Label L_continue, L_finalBit, L_padding, L_donePadding, L_bruteForce;\n-    Label L_forceLoop, L_bottomLoop, L_checkMIME, L_exit_no_vzero;\n-\n-    \/\/ calculate length from offsets\n-    __ movl(length, end_offset);\n-    __ subl(length, start_offset);\n-    __ push(dest);          \/\/ Save for return value calc\n-\n-    \/\/ If AVX512 VBMI not supported, just compile non-AVX code\n-    if(VM_Version::supports_avx512_vbmi() &&\n-       VM_Version::supports_avx512bw()) {\n-      __ cmpl(length, 128);     \/\/ 128-bytes is break-even for AVX-512\n-      __ jcc(Assembler::lessEqual, L_bruteForce);\n-\n-      __ cmpl(isMIME, 0);\n-      __ jcc(Assembler::notEqual, L_bruteForce);\n-\n-      \/\/ Load lookup tables based on isURL\n-      __ cmpl(isURL, 0);\n-      __ jcc(Assembler::notZero, L_loadURL);\n-\n-      __ evmovdquq(lookup_lo, ExternalAddress(StubRoutines::x86::base64_vbmi_lookup_lo_addr()), Assembler::AVX_512bit, r13);\n-      __ evmovdquq(lookup_hi, ExternalAddress(StubRoutines::x86::base64_vbmi_lookup_hi_addr()), Assembler::AVX_512bit, r13);\n-\n-      __ BIND(L_continue);\n-\n-      __ movl(r15, 0x01400140);\n-      __ evpbroadcastd(pack16_op, r15, Assembler::AVX_512bit);\n-\n-      __ movl(r15, 0x00011000);\n-      __ evpbroadcastd(pack32_op, r15, Assembler::AVX_512bit);\n-\n-      __ cmpl(length, 0xff);\n-      __ jcc(Assembler::lessEqual, L_process64);\n-\n-      \/\/ load masks required for decoding data\n-      __ BIND(L_processdata);\n-      __ evmovdquq(join01, ExternalAddress(StubRoutines::x86::base64_vbmi_join_0_1_addr()), Assembler::AVX_512bit,r13);\n-      __ evmovdquq(join12, ExternalAddress(StubRoutines::x86::base64_vbmi_join_1_2_addr()), Assembler::AVX_512bit, r13);\n-      __ evmovdquq(join23, ExternalAddress(StubRoutines::x86::base64_vbmi_join_2_3_addr()), Assembler::AVX_512bit, r13);\n+  const XMMRegister lookup_lo = xmm5;\n+  const XMMRegister lookup_hi = xmm6;\n+  const XMMRegister errorvec = xmm7;\n+  const XMMRegister pack16_op = xmm9;\n+  const XMMRegister pack32_op = xmm8;\n+  const XMMRegister input0 = xmm3;\n+  const XMMRegister input1 = xmm20;\n+  const XMMRegister input2 = xmm21;\n+  const XMMRegister input3 = xmm19;\n+  const XMMRegister join01 = xmm12;\n+  const XMMRegister join12 = xmm11;\n+  const XMMRegister join23 = xmm10;\n+  const XMMRegister translated0 = xmm2;\n+  const XMMRegister translated1 = xmm1;\n+  const XMMRegister translated2 = xmm0;\n+  const XMMRegister translated3 = xmm4;\n+\n+  const XMMRegister merged0 = xmm2;\n+  const XMMRegister merged1 = xmm1;\n+  const XMMRegister merged2 = xmm0;\n+  const XMMRegister merged3 = xmm4;\n+  const XMMRegister merge_ab_bc0 = xmm2;\n+  const XMMRegister merge_ab_bc1 = xmm1;\n+  const XMMRegister merge_ab_bc2 = xmm0;\n+  const XMMRegister merge_ab_bc3 = xmm4;\n+\n+  const XMMRegister pack24bits = xmm4;\n+\n+  const Register length = r14;\n+  const Register output_size = r13;\n+  const Register output_mask = r15;\n+  const KRegister input_mask = k1;\n+\n+  const XMMRegister input_initial_valid_b64 = xmm0;\n+  const XMMRegister tmp = xmm10;\n+  const XMMRegister mask = xmm0;\n+  const XMMRegister invalid_b64 = xmm1;\n+\n+  Label L_process256, L_process64, L_process64Loop, L_exit, L_processdata, L_loadURL;\n+  Label L_continue, L_finalBit, L_padding, L_donePadding, L_bruteForce;\n+  Label L_forceLoop, L_bottomLoop, L_checkMIME, L_exit_no_vzero;\n+\n+  \/\/ calculate length from offsets\n+  __ movl(length, end_offset);\n+  __ subl(length, start_offset);\n+  __ push(dest);          \/\/ Save for return value calc\n+\n+  \/\/ If AVX512 VBMI not supported, just compile non-AVX code\n+  if(VM_Version::supports_avx512_vbmi() &&\n+     VM_Version::supports_avx512bw()) {\n+    __ cmpl(length, 128);     \/\/ 128-bytes is break-even for AVX-512\n+    __ jcc(Assembler::lessEqual, L_bruteForce);\n+\n+    __ cmpl(isMIME, 0);\n+    __ jcc(Assembler::notEqual, L_bruteForce);\n+\n+    \/\/ Load lookup tables based on isURL\n+    __ cmpl(isURL, 0);\n+    __ jcc(Assembler::notZero, L_loadURL);\n+\n+    __ evmovdquq(lookup_lo, ExternalAddress(StubRoutines::x86::base64_vbmi_lookup_lo_addr()), Assembler::AVX_512bit, r13);\n+    __ evmovdquq(lookup_hi, ExternalAddress(StubRoutines::x86::base64_vbmi_lookup_hi_addr()), Assembler::AVX_512bit, r13);\n+\n+    __ BIND(L_continue);\n+\n+    __ movl(r15, 0x01400140);\n+    __ evpbroadcastd(pack16_op, r15, Assembler::AVX_512bit);\n+\n+    __ movl(r15, 0x00011000);\n+    __ evpbroadcastd(pack32_op, r15, Assembler::AVX_512bit);\n+\n+    __ cmpl(length, 0xff);\n+    __ jcc(Assembler::lessEqual, L_process64);\n+\n+    \/\/ load masks required for decoding data\n+    __ BIND(L_processdata);\n+    __ evmovdquq(join01, ExternalAddress(StubRoutines::x86::base64_vbmi_join_0_1_addr()), Assembler::AVX_512bit,r13);\n+    __ evmovdquq(join12, ExternalAddress(StubRoutines::x86::base64_vbmi_join_1_2_addr()), Assembler::AVX_512bit, r13);\n+    __ evmovdquq(join23, ExternalAddress(StubRoutines::x86::base64_vbmi_join_2_3_addr()), Assembler::AVX_512bit, r13);\n@@ -6237,84 +6292,74 @@\n-      __ align32();\n-      __ BIND(L_process256);\n-      \/\/ Grab input data\n-      __ evmovdquq(input0, Address(source, start_offset, Address::times_1, 0x00), Assembler::AVX_512bit);\n-      __ evmovdquq(input1, Address(source, start_offset, Address::times_1, 0x40), Assembler::AVX_512bit);\n-      __ evmovdquq(input2, Address(source, start_offset, Address::times_1, 0x80), Assembler::AVX_512bit);\n-      __ evmovdquq(input3, Address(source, start_offset, Address::times_1, 0xc0), Assembler::AVX_512bit);\n-\n-      \/\/ Copy the low part of the lookup table into the destination of the permutation\n-      __ evmovdquq(translated0, lookup_lo, Assembler::AVX_512bit);\n-      __ evmovdquq(translated1, lookup_lo, Assembler::AVX_512bit);\n-      __ evmovdquq(translated2, lookup_lo, Assembler::AVX_512bit);\n-      __ evmovdquq(translated3, lookup_lo, Assembler::AVX_512bit);\n-\n-      \/\/ Translate the base64 input into \"decoded\" bytes\n-      __ evpermt2b(translated0, input0, lookup_hi, Assembler::AVX_512bit);\n-      __ evpermt2b(translated1, input1, lookup_hi, Assembler::AVX_512bit);\n-      __ evpermt2b(translated2, input2, lookup_hi, Assembler::AVX_512bit);\n-      __ evpermt2b(translated3, input3, lookup_hi, Assembler::AVX_512bit);\n-\n-      \/\/ OR all of the translations together to check for errors (high-order bit of byte set)\n-      __ vpternlogd(input0, 0xfe, input1, input2, Assembler::AVX_512bit);\n-\n-      __ vpternlogd(input3, 0xfe, translated0, translated1, Assembler::AVX_512bit);\n-      __ vpternlogd(input0, 0xfe, translated2, translated3, Assembler::AVX_512bit);\n-      __ vpor(errorvec, input3, input0, Assembler::AVX_512bit);\n-\n-      \/\/ Check if there was an error - if so, try 64-byte chunks\n-      __ evpmovb2m(k3, errorvec, Assembler::AVX_512bit);\n-      __ kortestql(k3, k3);\n-      __ jcc(Assembler::notZero, L_process64);\n-\n-      \/\/ The merging and shuffling happens here\n-      \/\/ We multiply each byte pair [00dddddd | 00cccccc | 00bbbbbb | 00aaaaaa]\n-      \/\/ Multiply [00cccccc] by 2^6 added to [00dddddd] to get [0000cccc | ccdddddd]\n-      \/\/ The pack16_op is a vector of 0x01400140, so multiply D by 1 and C by 0x40\n-      __ vpmaddubsw(merge_ab_bc0, translated0, pack16_op, Assembler::AVX_512bit);\n-      __ vpmaddubsw(merge_ab_bc1, translated1, pack16_op, Assembler::AVX_512bit);\n-      __ vpmaddubsw(merge_ab_bc2, translated2, pack16_op, Assembler::AVX_512bit);\n-      __ vpmaddubsw(merge_ab_bc3, translated3, pack16_op, Assembler::AVX_512bit);\n-\n-      \/\/ Now do the same with packed 16-bit values.\n-      \/\/ We start with [0000cccc | ccdddddd | 0000aaaa | aabbbbbb]\n-      \/\/ pack32_op is 0x00011000 (2^12, 1), so this multiplies [0000aaaa | aabbbbbb] by 2^12\n-      \/\/ and adds [0000cccc | ccdddddd] to yield [00000000 | aaaaaabb | bbbbcccc | ccdddddd]\n-      __ vpmaddwd(merged0, merge_ab_bc0, pack32_op, Assembler::AVX_512bit);\n-      __ vpmaddwd(merged1, merge_ab_bc1, pack32_op, Assembler::AVX_512bit);\n-      __ vpmaddwd(merged2, merge_ab_bc2, pack32_op, Assembler::AVX_512bit);\n-      __ vpmaddwd(merged3, merge_ab_bc3, pack32_op, Assembler::AVX_512bit);\n-\n-      \/\/ The join vectors specify which byte from which vector goes into the outputs\n-      \/\/ One of every 4 bytes in the extended vector is zero, so we pack them into their\n-      \/\/ final positions in the register for storing (256 bytes in, 192 bytes out)\n-      __ evpermt2b(merged0, join01, merged1, Assembler::AVX_512bit);\n-      __ evpermt2b(merged1, join12, merged2, Assembler::AVX_512bit);\n-      __ evpermt2b(merged2, join23, merged3, Assembler::AVX_512bit);\n-\n-      \/\/ Store result\n-      __ evmovdquq(Address(dest, dp, Address::times_1, 0x00), merged0, Assembler::AVX_512bit);\n-      __ evmovdquq(Address(dest, dp, Address::times_1, 0x40), merged1, Assembler::AVX_512bit);\n-      __ evmovdquq(Address(dest, dp, Address::times_1, 0x80), merged2, Assembler::AVX_512bit);\n-\n-      __ addptr(source, 0x100);\n-      __ addptr(dest, 0xc0);\n-      __ subl(length, 0x100);\n-      __ cmpl(length, 64 * 4);\n-      __ jcc(Assembler::greaterEqual, L_process256);\n-\n-      \/\/ At this point, we've decoded 64 * 4 * n bytes.\n-      \/\/ The remaining length will be <= 64 * 4 - 1.\n-      \/\/ UNLESS there was an error decoding the first 256-byte chunk.  In this\n-      \/\/ case, the length will be arbitrarily long.\n-      \/\/\n-      \/\/ Note that this will be the path for MIME-encoded strings.\n-\n-      __ BIND(L_process64);\n-\n-      __ evmovdquq(pack24bits, ExternalAddress(StubRoutines::x86::base64_vbmi_pack_vec_addr()), Assembler::AVX_512bit, r13);\n-\n-      __ cmpl(length, 63);\n-      __ jcc(Assembler::lessEqual, L_finalBit);\n-\n-      __ mov64(rax, 0x0000ffffffffffff);\n-      __ kmovql(k2, rax);\n+    __ align32();\n+    __ BIND(L_process256);\n+    \/\/ Grab input data\n+    __ evmovdquq(input0, Address(source, start_offset, Address::times_1, 0x00), Assembler::AVX_512bit);\n+    __ evmovdquq(input1, Address(source, start_offset, Address::times_1, 0x40), Assembler::AVX_512bit);\n+    __ evmovdquq(input2, Address(source, start_offset, Address::times_1, 0x80), Assembler::AVX_512bit);\n+    __ evmovdquq(input3, Address(source, start_offset, Address::times_1, 0xc0), Assembler::AVX_512bit);\n+\n+    \/\/ Copy the low part of the lookup table into the destination of the permutation\n+    __ evmovdquq(translated0, lookup_lo, Assembler::AVX_512bit);\n+    __ evmovdquq(translated1, lookup_lo, Assembler::AVX_512bit);\n+    __ evmovdquq(translated2, lookup_lo, Assembler::AVX_512bit);\n+    __ evmovdquq(translated3, lookup_lo, Assembler::AVX_512bit);\n+\n+    \/\/ Translate the base64 input into \"decoded\" bytes\n+    __ evpermt2b(translated0, input0, lookup_hi, Assembler::AVX_512bit);\n+    __ evpermt2b(translated1, input1, lookup_hi, Assembler::AVX_512bit);\n+    __ evpermt2b(translated2, input2, lookup_hi, Assembler::AVX_512bit);\n+    __ evpermt2b(translated3, input3, lookup_hi, Assembler::AVX_512bit);\n+\n+    \/\/ OR all of the translations together to check for errors (high-order bit of byte set)\n+    __ vpternlogd(input0, 0xfe, input1, input2, Assembler::AVX_512bit);\n+\n+    __ vpternlogd(input3, 0xfe, translated0, translated1, Assembler::AVX_512bit);\n+    __ vpternlogd(input0, 0xfe, translated2, translated3, Assembler::AVX_512bit);\n+    __ vpor(errorvec, input3, input0, Assembler::AVX_512bit);\n+\n+    \/\/ Check if there was an error - if so, try 64-byte chunks\n+    __ evpmovb2m(k3, errorvec, Assembler::AVX_512bit);\n+    __ kortestql(k3, k3);\n+    __ jcc(Assembler::notZero, L_process64);\n+\n+    \/\/ The merging and shuffling happens here\n+    \/\/ We multiply each byte pair [00dddddd | 00cccccc | 00bbbbbb | 00aaaaaa]\n+    \/\/ Multiply [00cccccc] by 2^6 added to [00dddddd] to get [0000cccc | ccdddddd]\n+    \/\/ The pack16_op is a vector of 0x01400140, so multiply D by 1 and C by 0x40\n+    __ vpmaddubsw(merge_ab_bc0, translated0, pack16_op, Assembler::AVX_512bit);\n+    __ vpmaddubsw(merge_ab_bc1, translated1, pack16_op, Assembler::AVX_512bit);\n+    __ vpmaddubsw(merge_ab_bc2, translated2, pack16_op, Assembler::AVX_512bit);\n+    __ vpmaddubsw(merge_ab_bc3, translated3, pack16_op, Assembler::AVX_512bit);\n+\n+    \/\/ Now do the same with packed 16-bit values.\n+    \/\/ We start with [0000cccc | ccdddddd | 0000aaaa | aabbbbbb]\n+    \/\/ pack32_op is 0x00011000 (2^12, 1), so this multiplies [0000aaaa | aabbbbbb] by 2^12\n+    \/\/ and adds [0000cccc | ccdddddd] to yield [00000000 | aaaaaabb | bbbbcccc | ccdddddd]\n+    __ vpmaddwd(merged0, merge_ab_bc0, pack32_op, Assembler::AVX_512bit);\n+    __ vpmaddwd(merged1, merge_ab_bc1, pack32_op, Assembler::AVX_512bit);\n+    __ vpmaddwd(merged2, merge_ab_bc2, pack32_op, Assembler::AVX_512bit);\n+    __ vpmaddwd(merged3, merge_ab_bc3, pack32_op, Assembler::AVX_512bit);\n+\n+    \/\/ The join vectors specify which byte from which vector goes into the outputs\n+    \/\/ One of every 4 bytes in the extended vector is zero, so we pack them into their\n+    \/\/ final positions in the register for storing (256 bytes in, 192 bytes out)\n+    __ evpermt2b(merged0, join01, merged1, Assembler::AVX_512bit);\n+    __ evpermt2b(merged1, join12, merged2, Assembler::AVX_512bit);\n+    __ evpermt2b(merged2, join23, merged3, Assembler::AVX_512bit);\n+\n+    \/\/ Store result\n+    __ evmovdquq(Address(dest, dp, Address::times_1, 0x00), merged0, Assembler::AVX_512bit);\n+    __ evmovdquq(Address(dest, dp, Address::times_1, 0x40), merged1, Assembler::AVX_512bit);\n+    __ evmovdquq(Address(dest, dp, Address::times_1, 0x80), merged2, Assembler::AVX_512bit);\n+\n+    __ addptr(source, 0x100);\n+    __ addptr(dest, 0xc0);\n+    __ subl(length, 0x100);\n+    __ cmpl(length, 64 * 4);\n+    __ jcc(Assembler::greaterEqual, L_process256);\n+\n+    \/\/ At this point, we've decoded 64 * 4 * n bytes.\n+    \/\/ The remaining length will be <= 64 * 4 - 1.\n+    \/\/ UNLESS there was an error decoding the first 256-byte chunk.  In this\n+    \/\/ case, the length will be arbitrarily long.\n+    \/\/\n+    \/\/ Note that this will be the path for MIME-encoded strings.\n@@ -6322,2 +6367,1 @@\n-      __ align32();\n-      __ BIND(L_process64Loop);\n+    __ BIND(L_process64);\n@@ -6325,1 +6369,1 @@\n-      \/\/ Handle first 64-byte block\n+    __ evmovdquq(pack24bits, ExternalAddress(StubRoutines::x86::base64_vbmi_pack_vec_addr()), Assembler::AVX_512bit, r13);\n@@ -6327,3 +6371,2 @@\n-      __ evmovdquq(input0, Address(source, start_offset), Assembler::AVX_512bit);\n-      __ evmovdquq(translated0, lookup_lo, Assembler::AVX_512bit);\n-      __ evpermt2b(translated0, input0, lookup_hi, Assembler::AVX_512bit);\n+    __ cmpl(length, 63);\n+    __ jcc(Assembler::lessEqual, L_finalBit);\n@@ -6331,1 +6374,2 @@\n-      __ vpor(errorvec, translated0, input0, Assembler::AVX_512bit);\n+    __ mov64(rax, 0x0000ffffffffffff);\n+    __ kmovql(k2, rax);\n@@ -6333,4 +6377,2 @@\n-      \/\/ Check for error and bomb out before updating dest\n-      __ evpmovb2m(k3, errorvec, Assembler::AVX_512bit);\n-      __ kortestql(k3, k3);\n-      __ jcc(Assembler::notZero, L_exit);\n+    __ align32();\n+    __ BIND(L_process64Loop);\n@@ -6338,4 +6380,1 @@\n-      \/\/ Pack output register, selecting correct byte ordering\n-      __ vpmaddubsw(merge_ab_bc0, translated0, pack16_op, Assembler::AVX_512bit);\n-      __ vpmaddwd(merged0, merge_ab_bc0, pack32_op, Assembler::AVX_512bit);\n-      __ vpermb(merged0, pack24bits, merged0, Assembler::AVX_512bit);\n+    \/\/ Handle first 64-byte block\n@@ -6343,1 +6382,3 @@\n-      __ evmovdqub(Address(dest, dp), k2, merged0, true, Assembler::AVX_512bit);\n+    __ evmovdquq(input0, Address(source, start_offset), Assembler::AVX_512bit);\n+    __ evmovdquq(translated0, lookup_lo, Assembler::AVX_512bit);\n+    __ evpermt2b(translated0, input0, lookup_hi, Assembler::AVX_512bit);\n@@ -6345,3 +6386,1 @@\n-      __ subl(length, 64);\n-      __ addptr(source, 64);\n-      __ addptr(dest, 48);\n+    __ vpor(errorvec, translated0, input0, Assembler::AVX_512bit);\n@@ -6349,2 +6388,4 @@\n-      __ cmpl(length, 64);\n-      __ jcc(Assembler::greaterEqual, L_process64Loop);\n+    \/\/ Check for error and bomb out before updating dest\n+    __ evpmovb2m(k3, errorvec, Assembler::AVX_512bit);\n+    __ kortestql(k3, k3);\n+    __ jcc(Assembler::notZero, L_exit);\n@@ -6352,2 +6393,4 @@\n-      __ cmpl(length, 0);\n-      __ jcc(Assembler::lessEqual, L_exit);\n+    \/\/ Pack output register, selecting correct byte ordering\n+    __ vpmaddubsw(merge_ab_bc0, translated0, pack16_op, Assembler::AVX_512bit);\n+    __ vpmaddwd(merged0, merge_ab_bc0, pack32_op, Assembler::AVX_512bit);\n+    __ vpermb(merged0, pack24bits, merged0, Assembler::AVX_512bit);\n@@ -6355,102 +6398,1 @@\n-      __ BIND(L_finalBit);\n-      \/\/ Now have 1 to 63 bytes left to decode\n-\n-      \/\/ I was going to let Java take care of the final fragment\n-      \/\/ however it will repeatedly call this routine for every 4 bytes\n-      \/\/ of input data, so handle the rest here.\n-      __ movq(rax, -1);\n-      __ bzhiq(rax, rax, length);    \/\/ Input mask in rax\n-\n-      __ movl(output_size, length);\n-      __ shrl(output_size, 2);   \/\/ Find (len \/ 4) * 3 (output length)\n-      __ lea(output_size, Address(output_size, output_size, Address::times_2, 0));\n-      \/\/ output_size in r13\n-\n-      \/\/ Strip pad characters, if any, and adjust length and mask\n-      __ cmpb(Address(source, length, Address::times_1, -1), '=');\n-      __ jcc(Assembler::equal, L_padding);\n-\n-      __ BIND(L_donePadding);\n-\n-      \/\/ Output size is (64 - output_size), output mask is (all 1s >> output_size).\n-      __ kmovql(input_mask, rax);\n-      __ movq(output_mask, -1);\n-      __ bzhiq(output_mask, output_mask, output_size);\n-\n-      \/\/ Load initial input with all valid base64 characters.  Will be used\n-      \/\/ in merging source bytes to avoid masking when determining if an error occurred.\n-      __ movl(rax, 0x61616161);\n-      __ evpbroadcastd(input_initial_valid_b64, rax, Assembler::AVX_512bit);\n-\n-      \/\/ A register containing all invalid base64 decoded values\n-      __ movl(rax, 0x80808080);\n-      __ evpbroadcastd(invalid_b64, rax, Assembler::AVX_512bit);\n-\n-      \/\/ input_mask is in k1\n-      \/\/ output_size is in r13\n-      \/\/ output_mask is in r15\n-      \/\/ zmm0 - free\n-      \/\/ zmm1 - 0x00011000\n-      \/\/ zmm2 - 0x01400140\n-      \/\/ zmm3 - errorvec\n-      \/\/ zmm4 - pack vector\n-      \/\/ zmm5 - lookup_lo\n-      \/\/ zmm6 - lookup_hi\n-      \/\/ zmm7 - errorvec\n-      \/\/ zmm8 - 0x61616161\n-      \/\/ zmm9 - 0x80808080\n-\n-      \/\/ Load only the bytes from source, merging into our \"fully-valid\" register\n-      __ evmovdqub(input_initial_valid_b64, input_mask, Address(source, start_offset, Address::times_1, 0x0), true, Assembler::AVX_512bit);\n-\n-      \/\/ Decode all bytes within our merged input\n-      __ evmovdquq(tmp, lookup_lo, Assembler::AVX_512bit);\n-      __ evpermt2b(tmp, input_initial_valid_b64, lookup_hi, Assembler::AVX_512bit);\n-      __ vporq(mask, tmp, input_initial_valid_b64, Assembler::AVX_512bit);\n-\n-      \/\/ Check for error.  Compare (decoded | initial) to all invalid.\n-      \/\/ If any bytes have their high-order bit set, then we have an error.\n-      __ evptestmb(k2, mask, invalid_b64, Assembler::AVX_512bit);\n-      __ kortestql(k2, k2);\n-\n-      \/\/ If we have an error, use the brute force loop to decode what we can (4-byte chunks).\n-      __ jcc(Assembler::notZero, L_bruteForce);\n-\n-      \/\/ Shuffle output bytes\n-      __ vpmaddubsw(tmp, tmp, pack16_op, Assembler::AVX_512bit);\n-      __ vpmaddwd(tmp, tmp, pack32_op, Assembler::AVX_512bit);\n-\n-      __ vpermb(tmp, pack24bits, tmp, Assembler::AVX_512bit);\n-      __ kmovql(k1, output_mask);\n-      __ evmovdqub(Address(dest, dp), k1, tmp, true, Assembler::AVX_512bit);\n-\n-      __ addptr(dest, output_size);\n-\n-      __ BIND(L_exit);\n-      __ vzeroupper();\n-      __ pop(rax);             \/\/ Get original dest value\n-      __ subptr(dest, rax);      \/\/ Number of bytes converted\n-      __ movptr(rax, dest);\n-      __ pop(rbx);\n-      __ pop(r15);\n-      __ pop(r14);\n-      __ pop(r13);\n-      __ pop(r12);\n-      __ leave();\n-      __ ret(0);\n-\n-      __ BIND(L_loadURL);\n-      __ evmovdquq(lookup_lo, ExternalAddress(StubRoutines::x86::base64_vbmi_lookup_lo_url_addr()), Assembler::AVX_512bit, r13);\n-      __ evmovdquq(lookup_hi, ExternalAddress(StubRoutines::x86::base64_vbmi_lookup_hi_url_addr()), Assembler::AVX_512bit, r13);\n-      __ jmp(L_continue);\n-\n-      __ BIND(L_padding);\n-      __ decrementq(output_size, 1);\n-      __ shrq(rax, 1);\n-\n-      __ cmpb(Address(source, length, Address::times_1, -2), '=');\n-      __ jcc(Assembler::notEqual, L_donePadding);\n-\n-      __ decrementq(output_size, 1);\n-      __ shrq(rax, 1);\n-      __ jmp(L_donePadding);\n+    __ evmovdqub(Address(dest, dp), k2, merged0, true, Assembler::AVX_512bit);\n@@ -6458,37 +6400,3 @@\n-      __ align32();\n-      __ BIND(L_bruteForce);\n-    }   \/\/ End of if(avx512_vbmi)\n-\n-    \/\/ Use non-AVX code to decode 4-byte chunks into 3 bytes of output\n-\n-    \/\/ Register state (Linux):\n-    \/\/ r12-15 - saved on stack\n-    \/\/ rdi - src\n-    \/\/ rsi - sp\n-    \/\/ rdx - sl\n-    \/\/ rcx - dst\n-    \/\/ r8 - dp\n-    \/\/ r9 - isURL\n-\n-    \/\/ Register state (Windows):\n-    \/\/ r12-15 - saved on stack\n-    \/\/ rcx - src\n-    \/\/ rdx - sp\n-    \/\/ r8 - sl\n-    \/\/ r9 - dst\n-    \/\/ r12 - dp\n-    \/\/ r10 - isURL\n-\n-    \/\/ Registers (common):\n-    \/\/ length (r14) - bytes in src\n-\n-    const Register decode_table = r11;\n-    const Register out_byte_count = rbx;\n-    const Register byte1 = r13;\n-    const Register byte2 = r15;\n-    const Register byte3 = WINDOWS_ONLY(r8) NOT_WINDOWS(rdx);\n-    const Register byte4 = WINDOWS_ONLY(r10) NOT_WINDOWS(r9);\n-\n-    __ shrl(length, 2);    \/\/ Multiple of 4 bytes only - length is # 4-byte chunks\n-    __ cmpl(length, 0);\n-    __ jcc(Assembler::lessEqual, L_exit_no_vzero);\n+    __ subl(length, 64);\n+    __ addptr(source, 64);\n+    __ addptr(dest, 48);\n@@ -6496,3 +6404,2 @@\n-    __ shll(isURL, 8);    \/\/ index into decode table based on isURL\n-    __ lea(decode_table, ExternalAddress(StubRoutines::x86::base64_decoding_table_addr()));\n-    __ addptr(decode_table, isURL);\n+    __ cmpl(length, 64);\n+    __ jcc(Assembler::greaterEqual, L_process64Loop);\n@@ -6500,1 +6407,2 @@\n-    __ jmp(L_bottomLoop);\n+    __ cmpl(length, 0);\n+    __ jcc(Assembler::lessEqual, L_exit);\n@@ -6502,38 +6410,76 @@\n-    __ align32();\n-    __ BIND(L_forceLoop);\n-    __ shll(byte1, 18);\n-    __ shll(byte2, 12);\n-    __ shll(byte3, 6);\n-    __ orl(byte1, byte2);\n-    __ orl(byte1, byte3);\n-    __ orl(byte1, byte4);\n-\n-    __ addptr(source, 4);\n-\n-    __ movb(Address(dest, dp, Address::times_1, 2), byte1);\n-    __ shrl(byte1, 8);\n-    __ movb(Address(dest, dp, Address::times_1, 1), byte1);\n-    __ shrl(byte1, 8);\n-    __ movb(Address(dest, dp, Address::times_1, 0), byte1);\n-\n-    __ addptr(dest, 3);\n-    __ decrementl(length, 1);\n-    __ jcc(Assembler::zero, L_exit_no_vzero);\n-\n-    __ BIND(L_bottomLoop);\n-    __ load_unsigned_byte(byte1, Address(source, start_offset, Address::times_1, 0x00));\n-    __ load_unsigned_byte(byte2, Address(source, start_offset, Address::times_1, 0x01));\n-    __ load_signed_byte(byte1, Address(decode_table, byte1));\n-    __ load_signed_byte(byte2, Address(decode_table, byte2));\n-    __ load_unsigned_byte(byte3, Address(source, start_offset, Address::times_1, 0x02));\n-    __ load_unsigned_byte(byte4, Address(source, start_offset, Address::times_1, 0x03));\n-    __ load_signed_byte(byte3, Address(decode_table, byte3));\n-    __ load_signed_byte(byte4, Address(decode_table, byte4));\n-\n-    __ mov(rax, byte1);\n-    __ orl(rax, byte2);\n-    __ orl(rax, byte3);\n-    __ orl(rax, byte4);\n-    __ jcc(Assembler::positive, L_forceLoop);\n-\n-    __ BIND(L_exit_no_vzero);\n+    __ BIND(L_finalBit);\n+    \/\/ Now have 1 to 63 bytes left to decode\n+\n+    \/\/ I was going to let Java take care of the final fragment\n+    \/\/ however it will repeatedly call this routine for every 4 bytes\n+    \/\/ of input data, so handle the rest here.\n+    __ movq(rax, -1);\n+    __ bzhiq(rax, rax, length);    \/\/ Input mask in rax\n+\n+    __ movl(output_size, length);\n+    __ shrl(output_size, 2);   \/\/ Find (len \/ 4) * 3 (output length)\n+    __ lea(output_size, Address(output_size, output_size, Address::times_2, 0));\n+    \/\/ output_size in r13\n+\n+    \/\/ Strip pad characters, if any, and adjust length and mask\n+    __ cmpb(Address(source, length, Address::times_1, -1), '=');\n+    __ jcc(Assembler::equal, L_padding);\n+\n+    __ BIND(L_donePadding);\n+\n+    \/\/ Output size is (64 - output_size), output mask is (all 1s >> output_size).\n+    __ kmovql(input_mask, rax);\n+    __ movq(output_mask, -1);\n+    __ bzhiq(output_mask, output_mask, output_size);\n+\n+    \/\/ Load initial input with all valid base64 characters.  Will be used\n+    \/\/ in merging source bytes to avoid masking when determining if an error occurred.\n+    __ movl(rax, 0x61616161);\n+    __ evpbroadcastd(input_initial_valid_b64, rax, Assembler::AVX_512bit);\n+\n+    \/\/ A register containing all invalid base64 decoded values\n+    __ movl(rax, 0x80808080);\n+    __ evpbroadcastd(invalid_b64, rax, Assembler::AVX_512bit);\n+\n+    \/\/ input_mask is in k1\n+    \/\/ output_size is in r13\n+    \/\/ output_mask is in r15\n+    \/\/ zmm0 - free\n+    \/\/ zmm1 - 0x00011000\n+    \/\/ zmm2 - 0x01400140\n+    \/\/ zmm3 - errorvec\n+    \/\/ zmm4 - pack vector\n+    \/\/ zmm5 - lookup_lo\n+    \/\/ zmm6 - lookup_hi\n+    \/\/ zmm7 - errorvec\n+    \/\/ zmm8 - 0x61616161\n+    \/\/ zmm9 - 0x80808080\n+\n+    \/\/ Load only the bytes from source, merging into our \"fully-valid\" register\n+    __ evmovdqub(input_initial_valid_b64, input_mask, Address(source, start_offset, Address::times_1, 0x0), true, Assembler::AVX_512bit);\n+\n+    \/\/ Decode all bytes within our merged input\n+    __ evmovdquq(tmp, lookup_lo, Assembler::AVX_512bit);\n+    __ evpermt2b(tmp, input_initial_valid_b64, lookup_hi, Assembler::AVX_512bit);\n+    __ vporq(mask, tmp, input_initial_valid_b64, Assembler::AVX_512bit);\n+\n+    \/\/ Check for error.  Compare (decoded | initial) to all invalid.\n+    \/\/ If any bytes have their high-order bit set, then we have an error.\n+    __ evptestmb(k2, mask, invalid_b64, Assembler::AVX_512bit);\n+    __ kortestql(k2, k2);\n+\n+    \/\/ If we have an error, use the brute force loop to decode what we can (4-byte chunks).\n+    __ jcc(Assembler::notZero, L_bruteForce);\n+\n+    \/\/ Shuffle output bytes\n+    __ vpmaddubsw(tmp, tmp, pack16_op, Assembler::AVX_512bit);\n+    __ vpmaddwd(tmp, tmp, pack32_op, Assembler::AVX_512bit);\n+\n+    __ vpermb(tmp, pack24bits, tmp, Assembler::AVX_512bit);\n+    __ kmovql(k1, output_mask);\n+    __ evmovdqub(Address(dest, dp), k1, tmp, true, Assembler::AVX_512bit);\n+\n+    __ addptr(dest, output_size);\n+\n+    __ BIND(L_exit);\n+    __ vzeroupper();\n@@ -6551,32 +6497,4 @@\n-    return start;\n-  }\n-\n-\n-  \/**\n-   *  Arguments:\n-   *\n-   * Inputs:\n-   *   c_rarg0   - int crc\n-   *   c_rarg1   - byte* buf\n-   *   c_rarg2   - int length\n-   *\n-   * Output:\n-   *       rax   - int crc result\n-   *\/\n-  address generate_updateBytesCRC32() {\n-    assert(UseCRC32Intrinsics, \"need AVX and CLMUL instructions\");\n-\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"updateBytesCRC32\");\n-\n-    address start = __ pc();\n-    \/\/ Win64: rcx, rdx, r8, r9 (c_rarg0, c_rarg1, ...)\n-    \/\/ Unix:  rdi, rsi, rdx, rcx, r8, r9 (c_rarg0, c_rarg1, ...)\n-    \/\/ rscratch1: r10\n-    const Register crc   = c_rarg0;  \/\/ crc\n-    const Register buf   = c_rarg1;  \/\/ source java byte array address\n-    const Register len   = c_rarg2;  \/\/ length\n-    const Register table = c_rarg3;  \/\/ crc_table address (reuse register)\n-    const Register tmp1   = r11;\n-    const Register tmp2   = r10;\n-    assert_different_registers(crc, buf, len, table, tmp1, tmp2, rax);\n+    __ BIND(L_loadURL);\n+    __ evmovdquq(lookup_lo, ExternalAddress(StubRoutines::x86::base64_vbmi_lookup_lo_url_addr()), Assembler::AVX_512bit, r13);\n+    __ evmovdquq(lookup_hi, ExternalAddress(StubRoutines::x86::base64_vbmi_lookup_hi_url_addr()), Assembler::AVX_512bit, r13);\n+    __ jmp(L_continue);\n@@ -6584,16 +6502,3 @@\n-    BLOCK_COMMENT(\"Entry:\");\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-\n-    if (VM_Version::supports_sse4_1() && VM_Version::supports_avx512_vpclmulqdq() &&\n-        VM_Version::supports_avx512bw() &&\n-        VM_Version::supports_avx512vl()) {\n-        \/\/ The constants used in the CRC32 algorithm requires the 1's compliment of the initial crc value.\n-        \/\/ However, the constant table for CRC32-C assumes the original crc value.  Account for this\n-        \/\/ difference before calling and after returning.\n-      __ lea(table, ExternalAddress(StubRoutines::x86::crc_table_avx512_addr()));\n-      __ notl(crc);\n-      __ kernel_crc32_avx512(crc, buf, len, table, tmp1, tmp2);\n-      __ notl(crc);\n-    } else {\n-      __ kernel_crc32(crc, buf, len, table, tmp1);\n-    }\n+    __ BIND(L_padding);\n+    __ decrementq(output_size, 1);\n+    __ shrq(rax, 1);\n@@ -6601,4 +6506,99 @@\n-    __ movl(rax, crc);\n-    __ vzeroupper();\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n+    __ cmpb(Address(source, length, Address::times_1, -2), '=');\n+    __ jcc(Assembler::notEqual, L_donePadding);\n+\n+    __ decrementq(output_size, 1);\n+    __ shrq(rax, 1);\n+    __ jmp(L_donePadding);\n+\n+    __ align32();\n+    __ BIND(L_bruteForce);\n+  }   \/\/ End of if(avx512_vbmi)\n+\n+  \/\/ Use non-AVX code to decode 4-byte chunks into 3 bytes of output\n+\n+  \/\/ Register state (Linux):\n+  \/\/ r12-15 - saved on stack\n+  \/\/ rdi - src\n+  \/\/ rsi - sp\n+  \/\/ rdx - sl\n+  \/\/ rcx - dst\n+  \/\/ r8 - dp\n+  \/\/ r9 - isURL\n+\n+  \/\/ Register state (Windows):\n+  \/\/ r12-15 - saved on stack\n+  \/\/ rcx - src\n+  \/\/ rdx - sp\n+  \/\/ r8 - sl\n+  \/\/ r9 - dst\n+  \/\/ r12 - dp\n+  \/\/ r10 - isURL\n+\n+  \/\/ Registers (common):\n+  \/\/ length (r14) - bytes in src\n+\n+  const Register decode_table = r11;\n+  const Register out_byte_count = rbx;\n+  const Register byte1 = r13;\n+  const Register byte2 = r15;\n+  const Register byte3 = WIN64_ONLY(r8) NOT_WIN64(rdx);\n+  const Register byte4 = WIN64_ONLY(r10) NOT_WIN64(r9);\n+\n+  __ shrl(length, 2);    \/\/ Multiple of 4 bytes only - length is # 4-byte chunks\n+  __ cmpl(length, 0);\n+  __ jcc(Assembler::lessEqual, L_exit_no_vzero);\n+\n+  __ shll(isURL, 8);    \/\/ index into decode table based on isURL\n+  __ lea(decode_table, ExternalAddress(StubRoutines::x86::base64_decoding_table_addr()));\n+  __ addptr(decode_table, isURL);\n+\n+  __ jmp(L_bottomLoop);\n+\n+  __ align32();\n+  __ BIND(L_forceLoop);\n+  __ shll(byte1, 18);\n+  __ shll(byte2, 12);\n+  __ shll(byte3, 6);\n+  __ orl(byte1, byte2);\n+  __ orl(byte1, byte3);\n+  __ orl(byte1, byte4);\n+\n+  __ addptr(source, 4);\n+\n+  __ movb(Address(dest, dp, Address::times_1, 2), byte1);\n+  __ shrl(byte1, 8);\n+  __ movb(Address(dest, dp, Address::times_1, 1), byte1);\n+  __ shrl(byte1, 8);\n+  __ movb(Address(dest, dp, Address::times_1, 0), byte1);\n+\n+  __ addptr(dest, 3);\n+  __ decrementl(length, 1);\n+  __ jcc(Assembler::zero, L_exit_no_vzero);\n+\n+  __ BIND(L_bottomLoop);\n+  __ load_unsigned_byte(byte1, Address(source, start_offset, Address::times_1, 0x00));\n+  __ load_unsigned_byte(byte2, Address(source, start_offset, Address::times_1, 0x01));\n+  __ load_signed_byte(byte1, Address(decode_table, byte1));\n+  __ load_signed_byte(byte2, Address(decode_table, byte2));\n+  __ load_unsigned_byte(byte3, Address(source, start_offset, Address::times_1, 0x02));\n+  __ load_unsigned_byte(byte4, Address(source, start_offset, Address::times_1, 0x03));\n+  __ load_signed_byte(byte3, Address(decode_table, byte3));\n+  __ load_signed_byte(byte4, Address(decode_table, byte4));\n+\n+  __ mov(rax, byte1);\n+  __ orl(rax, byte2);\n+  __ orl(rax, byte3);\n+  __ orl(rax, byte4);\n+  __ jcc(Assembler::positive, L_forceLoop);\n+\n+  __ BIND(L_exit_no_vzero);\n+  __ pop(rax);             \/\/ Get original dest value\n+  __ subptr(dest, rax);      \/\/ Number of bytes converted\n+  __ movptr(rax, dest);\n+  __ pop(rbx);\n+  __ pop(r15);\n+  __ pop(r14);\n+  __ pop(r13);\n+  __ pop(r12);\n+  __ leave();\n+  __ ret(0);\n@@ -6606,31 +6606,88 @@\n-    return start;\n-  }\n-\n-  \/**\n-  *  Arguments:\n-  *\n-  * Inputs:\n-  *   c_rarg0   - int crc\n-  *   c_rarg1   - byte* buf\n-  *   c_rarg2   - long length\n-  *   c_rarg3   - table_start - optional (present only when doing a library_call,\n-  *              not used by x86 algorithm)\n-  *\n-  * Output:\n-  *       rax   - int crc result\n-  *\/\n-  address generate_updateBytesCRC32C(bool is_pclmulqdq_supported) {\n-      assert(UseCRC32CIntrinsics, \"need SSE4_2\");\n-      __ align(CodeEntryAlignment);\n-      StubCodeMark mark(this, \"StubRoutines\", \"updateBytesCRC32C\");\n-      address start = __ pc();\n-      \/\/reg.arg        int#0        int#1        int#2        int#3        int#4        int#5        float regs\n-      \/\/Windows        RCX          RDX          R8           R9           none         none         XMM0..XMM3\n-      \/\/Lin \/ Sol      RDI          RSI          RDX          RCX          R8           R9           XMM0..XMM7\n-      const Register crc = c_rarg0;  \/\/ crc\n-      const Register buf = c_rarg1;  \/\/ source java byte array address\n-      const Register len = c_rarg2;  \/\/ length\n-      const Register a = rax;\n-      const Register j = r9;\n-      const Register k = r10;\n-      const Register l = r11;\n+  return start;\n+}\n+\n+\n+\/**\n+ *  Arguments:\n+ *\n+ * Inputs:\n+ *   c_rarg0   - int crc\n+ *   c_rarg1   - byte* buf\n+ *   c_rarg2   - int length\n+ *\n+ * Output:\n+ *       rax   - int crc result\n+ *\/\n+address StubGenerator::generate_updateBytesCRC32() {\n+  assert(UseCRC32Intrinsics, \"need AVX and CLMUL instructions\");\n+\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"updateBytesCRC32\");\n+\n+  address start = __ pc();\n+\n+  \/\/ Win64: rcx, rdx, r8, r9 (c_rarg0, c_rarg1, ...)\n+  \/\/ Unix:  rdi, rsi, rdx, rcx, r8, r9 (c_rarg0, c_rarg1, ...)\n+  \/\/ rscratch1: r10\n+  const Register crc   = c_rarg0;  \/\/ crc\n+  const Register buf   = c_rarg1;  \/\/ source java byte array address\n+  const Register len   = c_rarg2;  \/\/ length\n+  const Register table = c_rarg3;  \/\/ crc_table address (reuse register)\n+  const Register tmp1   = r11;\n+  const Register tmp2   = r10;\n+  assert_different_registers(crc, buf, len, table, tmp1, tmp2, rax);\n+\n+  BLOCK_COMMENT(\"Entry:\");\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+\n+  if (VM_Version::supports_sse4_1() && VM_Version::supports_avx512_vpclmulqdq() &&\n+      VM_Version::supports_avx512bw() &&\n+      VM_Version::supports_avx512vl()) {\n+      \/\/ The constants used in the CRC32 algorithm requires the 1's compliment of the initial crc value.\n+      \/\/ However, the constant table for CRC32-C assumes the original crc value.  Account for this\n+      \/\/ difference before calling and after returning.\n+    __ lea(table, ExternalAddress(StubRoutines::x86::crc_table_avx512_addr()));\n+    __ notl(crc);\n+    __ kernel_crc32_avx512(crc, buf, len, table, tmp1, tmp2);\n+    __ notl(crc);\n+  } else {\n+    __ kernel_crc32(crc, buf, len, table, tmp1);\n+  }\n+\n+  __ movl(rax, crc);\n+  __ vzeroupper();\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n+\n+  return start;\n+}\n+\n+\/**\n+*  Arguments:\n+*\n+* Inputs:\n+*   c_rarg0   - int crc\n+*   c_rarg1   - byte* buf\n+*   c_rarg2   - long length\n+*   c_rarg3   - table_start - optional (present only when doing a library_call,\n+*              not used by x86 algorithm)\n+*\n+* Output:\n+*       rax   - int crc result\n+*\/\n+address StubGenerator::generate_updateBytesCRC32C(bool is_pclmulqdq_supported) {\n+  assert(UseCRC32CIntrinsics, \"need SSE4_2\");\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"updateBytesCRC32C\");\n+  address start = __ pc();\n+\n+  \/\/reg.arg        int#0        int#1        int#2        int#3        int#4        int#5        float regs\n+  \/\/Windows        RCX          RDX          R8           R9           none         none         XMM0..XMM3\n+  \/\/Lin \/ Sol      RDI          RSI          RDX          RCX          R8           R9           XMM0..XMM7\n+  const Register crc = c_rarg0;  \/\/ crc\n+  const Register buf = c_rarg1;  \/\/ source java byte array address\n+  const Register len = c_rarg2;  \/\/ length\n+  const Register a = rax;\n+  const Register j = r9;\n+  const Register k = r10;\n+  const Register l = r11;\n@@ -6638,2 +6695,2 @@\n-      const Register y = rdi;\n-      const Register z = rsi;\n+  const Register y = rdi;\n+  const Register z = rsi;\n@@ -6641,2 +6698,2 @@\n-      const Register y = rcx;\n-      const Register z = r8;\n+  const Register y = rcx;\n+  const Register z = r8;\n@@ -6644,10 +6701,10 @@\n-      assert_different_registers(crc, buf, len, a, j, k, l, y, z);\n-\n-      BLOCK_COMMENT(\"Entry:\");\n-      __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-      if (VM_Version::supports_sse4_1() && VM_Version::supports_avx512_vpclmulqdq() &&\n-          VM_Version::supports_avx512bw() &&\n-          VM_Version::supports_avx512vl()) {\n-        __ lea(j, ExternalAddress(StubRoutines::x86::crc32c_table_avx512_addr()));\n-        __ kernel_crc32_avx512(crc, buf, len, j, l, k);\n-      } else {\n+  assert_different_registers(crc, buf, len, a, j, k, l, y, z);\n+\n+  BLOCK_COMMENT(\"Entry:\");\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  if (VM_Version::supports_sse4_1() && VM_Version::supports_avx512_vpclmulqdq() &&\n+      VM_Version::supports_avx512bw() &&\n+      VM_Version::supports_avx512vl()) {\n+    __ lea(j, ExternalAddress(StubRoutines::x86::crc32c_table_avx512_addr()));\n+    __ kernel_crc32_avx512(crc, buf, len, j, l, k);\n+  } else {\n@@ -6655,2 +6712,2 @@\n-        __ push(y);\n-        __ push(z);\n+    __ push(y);\n+    __ push(z);\n@@ -6658,5 +6715,5 @@\n-        __ crc32c_ipl_alg2_alt2(crc, buf, len,\n-                                a, j, k,\n-                                l, y, z,\n-                                c_farg0, c_farg1, c_farg2,\n-                                is_pclmulqdq_supported);\n+    __ crc32c_ipl_alg2_alt2(crc, buf, len,\n+                            a, j, k,\n+                            l, y, z,\n+                            c_farg0, c_farg1, c_farg2,\n+                            is_pclmulqdq_supported);\n@@ -6664,2 +6721,2 @@\n-        __ pop(z);\n-        __ pop(y);\n+    __ pop(z);\n+    __ pop(y);\n@@ -6667,85 +6724,5 @@\n-      }\n-      __ movl(rax, crc);\n-      __ vzeroupper();\n-      __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-      __ ret(0);\n-\n-      return start;\n-  }\n-\n-\n-  \/***\n-   *  Arguments:\n-   *\n-   *  Inputs:\n-   *   c_rarg0   - int   adler\n-   *   c_rarg1   - byte* buff\n-   *   c_rarg2   - int   len\n-   *\n-   * Output:\n-   *   rax   - int adler result\n-   *\/\n-\n-  address generate_updateBytesAdler32() {\n-      assert(UseAdler32Intrinsics, \"need AVX2\");\n-\n-      __ align(CodeEntryAlignment);\n-      StubCodeMark mark(this, \"StubRoutines\", \"updateBytesAdler32\");\n-\n-      address start = __ pc();\n-\n-      const Register data = r9;\n-      const Register size = r10;\n-\n-      const XMMRegister yshuf0 = xmm6;\n-      const XMMRegister yshuf1 = xmm7;\n-      assert_different_registers(c_rarg0, c_rarg1, c_rarg2, data, size);\n-\n-      BLOCK_COMMENT(\"Entry:\");\n-      __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-\n-      __ vmovdqu(yshuf0, ExternalAddress((address) StubRoutines::x86::_adler32_shuf0_table), r9);\n-      __ vmovdqu(yshuf1, ExternalAddress((address) StubRoutines::x86::_adler32_shuf1_table), r9);\n-      __ movptr(data, c_rarg1); \/\/data\n-      __ movl(size, c_rarg2); \/\/length\n-      __ updateBytesAdler32(c_rarg0, data, size, yshuf0, yshuf1, ExternalAddress((address) StubRoutines::x86::_adler32_ascale_table));\n-      __ leave();\n-      __ ret(0);\n-      return start;\n-  }\n-\n-  \/**\n-   *  Arguments:\n-   *\n-   *  Input:\n-   *    c_rarg0   - x address\n-   *    c_rarg1   - x length\n-   *    c_rarg2   - y address\n-   *    c_rarg3   - y length\n-   * not Win64\n-   *    c_rarg4   - z address\n-   *    c_rarg5   - z length\n-   * Win64\n-   *    rsp+40    - z address\n-   *    rsp+48    - z length\n-   *\/\n-  address generate_multiplyToLen() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"multiplyToLen\");\n-\n-    address start = __ pc();\n-    \/\/ Win64: rcx, rdx, r8, r9 (c_rarg0, c_rarg1, ...)\n-    \/\/ Unix:  rdi, rsi, rdx, rcx, r8, r9 (c_rarg0, c_rarg1, ...)\n-    const Register x     = rdi;\n-    const Register xlen  = rax;\n-    const Register y     = rsi;\n-    const Register ylen  = rcx;\n-    const Register z     = r8;\n-    const Register zlen  = r11;\n-\n-    \/\/ Next registers will be saved on stack in multiply_to_len().\n-    const Register tmp1  = r12;\n-    const Register tmp2  = r13;\n-    const Register tmp3  = r14;\n-    const Register tmp4  = r15;\n-    const Register tmp5  = rbx;\n+  }\n+  __ movl(rax, crc);\n+  __ vzeroupper();\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n@@ -6753,2 +6730,82 @@\n-    BLOCK_COMMENT(\"Entry:\");\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  return start;\n+}\n+\n+\n+\/***\n+ *  Arguments:\n+ *\n+ *  Inputs:\n+ *   c_rarg0   - int   adler\n+ *   c_rarg1   - byte* buff\n+ *   c_rarg2   - int   len\n+ *\n+ * Output:\n+ *   rax   - int adler result\n+ *\/\n+\n+address StubGenerator::generate_updateBytesAdler32() {\n+  assert(UseAdler32Intrinsics, \"need AVX2\");\n+\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"updateBytesAdler32\");\n+  address start = __ pc();\n+\n+  const Register data = r9;\n+  const Register size = r10;\n+\n+  const XMMRegister yshuf0 = xmm6;\n+  const XMMRegister yshuf1 = xmm7;\n+  assert_different_registers(c_rarg0, c_rarg1, c_rarg2, data, size);\n+\n+  BLOCK_COMMENT(\"Entry:\");\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+\n+  __ vmovdqu(yshuf0, ExternalAddress((address) StubRoutines::x86::_adler32_shuf0_table), r9);\n+  __ vmovdqu(yshuf1, ExternalAddress((address) StubRoutines::x86::_adler32_shuf1_table), r9);\n+  __ movptr(data, c_rarg1); \/\/data\n+  __ movl(size, c_rarg2); \/\/length\n+  __ updateBytesAdler32(c_rarg0, data, size, yshuf0, yshuf1, ExternalAddress((address) StubRoutines::x86::_adler32_ascale_table));\n+  __ leave();\n+  __ ret(0);\n+\n+  return start;\n+}\n+\n+\/**\n+ *  Arguments:\n+ *\n+ *  Input:\n+ *    c_rarg0   - x address\n+ *    c_rarg1   - x length\n+ *    c_rarg2   - y address\n+ *    c_rarg3   - y length\n+ * not Win64\n+ *    c_rarg4   - z address\n+ *    c_rarg5   - z length\n+ * Win64\n+ *    rsp+40    - z address\n+ *    rsp+48    - z length\n+ *\/\n+address StubGenerator::generate_multiplyToLen() {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"multiplyToLen\");\n+  address start = __ pc();\n+\n+  \/\/ Win64: rcx, rdx, r8, r9 (c_rarg0, c_rarg1, ...)\n+  \/\/ Unix:  rdi, rsi, rdx, rcx, r8, r9 (c_rarg0, c_rarg1, ...)\n+  const Register x     = rdi;\n+  const Register xlen  = rax;\n+  const Register y     = rsi;\n+  const Register ylen  = rcx;\n+  const Register z     = r8;\n+  const Register zlen  = r11;\n+\n+  \/\/ Next registers will be saved on stack in multiply_to_len().\n+  const Register tmp1  = r12;\n+  const Register tmp2  = r13;\n+  const Register tmp3  = r14;\n+  const Register tmp4  = r15;\n+  const Register tmp5  = rbx;\n+\n+  BLOCK_COMMENT(\"Entry:\");\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n@@ -6757,1 +6814,1 @@\n-    __ movptr(zlen, r9); \/\/ Save r9 in r11 - zlen\n+  __ movptr(zlen, r9); \/\/ Save r9 in r11 - zlen\n@@ -6759,3 +6816,3 @@\n-    setup_arg_regs(4); \/\/ x => rdi, xlen => rsi, y => rdx\n-                       \/\/ ylen => rcx, z => r8, zlen => r11\n-                       \/\/ r9 and r10 may be used to save non-volatile registers\n+  setup_arg_regs(4); \/\/ x => rdi, xlen => rsi, y => rdx\n+                     \/\/ ylen => rcx, z => r8, zlen => r11\n+                     \/\/ r9 and r10 may be used to save non-volatile registers\n@@ -6763,3 +6820,3 @@\n-    \/\/ last 2 arguments (#4, #5) are on stack on Win64\n-    __ movptr(z, Address(rsp, 6 * wordSize));\n-    __ movptr(zlen, Address(rsp, 7 * wordSize));\n+  \/\/ last 2 arguments (#4, #5) are on stack on Win64\n+  __ movptr(z, Address(rsp, 6 * wordSize));\n+  __ movptr(zlen, Address(rsp, 7 * wordSize));\n@@ -6768,3 +6825,3 @@\n-    __ movptr(xlen, rsi);\n-    __ movptr(y,    rdx);\n-    __ multiply_to_len(x, xlen, y, ylen, z, zlen, tmp1, tmp2, tmp3, tmp4, tmp5);\n+  __ movptr(xlen, rsi);\n+  __ movptr(y,    rdx);\n+  __ multiply_to_len(x, xlen, y, ylen, z, zlen, tmp1, tmp2, tmp3, tmp4, tmp5);\n@@ -6772,1 +6829,1 @@\n-    restore_arg_regs();\n+  restore_arg_regs();\n@@ -6774,2 +6831,2 @@\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n@@ -6777,19 +6834,2 @@\n-    return start;\n-  }\n-\n-  \/**\n-  *  Arguments:\n-  *\n-  *  Input:\n-  *    c_rarg0   - obja     address\n-  *    c_rarg1   - objb     address\n-  *    c_rarg3   - length   length\n-  *    c_rarg4   - scale    log2_array_indxscale\n-  *\n-  *  Output:\n-  *        rax   - int >= mismatched index, < 0 bitwise complement of tail\n-  *\/\n-  address generate_vectorizedMismatch() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"vectorizedMismatch\");\n-    address start = __ pc();\n+  return start;\n+}\n@@ -6797,2 +6837,19 @@\n-    BLOCK_COMMENT(\"Entry:\");\n-    __ enter();\n+\/**\n+*  Arguments:\n+*\n+*  Input:\n+*    c_rarg0   - obja     address\n+*    c_rarg1   - objb     address\n+*    c_rarg3   - length   length\n+*    c_rarg4   - scale    log2_array_indxscale\n+*\n+*  Output:\n+*        rax   - int >= mismatched index, < 0 bitwise complement of tail\n+*\/\n+address StubGenerator::generate_vectorizedMismatch() {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"vectorizedMismatch\");\n+  address start = __ pc();\n+\n+  BLOCK_COMMENT(\"Entry:\");\n+  __ enter();\n@@ -6801,8 +6858,8 @@\n-    const Register scale = c_rarg0;  \/\/rcx, will exchange with r9\n-    const Register objb = c_rarg1;   \/\/rdx\n-    const Register length = c_rarg2; \/\/r8\n-    const Register obja = c_rarg3;   \/\/r9\n-    __ xchgq(obja, scale);  \/\/now obja and scale contains the correct contents\n-\n-    const Register tmp1 = r10;\n-    const Register tmp2 = r11;\n+  const Register scale = c_rarg0;  \/\/rcx, will exchange with r9\n+  const Register objb = c_rarg1;   \/\/rdx\n+  const Register length = c_rarg2; \/\/r8\n+  const Register obja = c_rarg3;   \/\/r9\n+  __ xchgq(obja, scale);  \/\/now obja and scale contains the correct contents\n+\n+  const Register tmp1 = r10;\n+  const Register tmp2 = r11;\n@@ -6811,6 +6868,6 @@\n-    const Register obja = c_rarg0;   \/\/U:rdi\n-    const Register objb = c_rarg1;   \/\/U:rsi\n-    const Register length = c_rarg2; \/\/U:rdx\n-    const Register scale = c_rarg3;  \/\/U:rcx\n-    const Register tmp1 = r8;\n-    const Register tmp2 = r9;\n+  const Register obja = c_rarg0;   \/\/U:rdi\n+  const Register objb = c_rarg1;   \/\/U:rsi\n+  const Register length = c_rarg2; \/\/U:rdx\n+  const Register scale = c_rarg3;  \/\/U:rcx\n+  const Register tmp1 = r8;\n+  const Register tmp2 = r9;\n@@ -6818,4 +6875,4 @@\n-    const Register result = rax; \/\/return value\n-    const XMMRegister vec0 = xmm0;\n-    const XMMRegister vec1 = xmm1;\n-    const XMMRegister vec2 = xmm2;\n+  const Register result = rax; \/\/return value\n+  const XMMRegister vec0 = xmm0;\n+  const XMMRegister vec1 = xmm1;\n+  const XMMRegister vec2 = xmm2;\n@@ -6823,1 +6880,1 @@\n-    __ vectorized_mismatch(obja, objb, length, scale, result, tmp1, tmp2, vec0, vec1, vec2);\n+  __ vectorized_mismatch(obja, objb, length, scale, result, tmp1, tmp2, vec0, vec1, vec2);\n@@ -6825,3 +6882,3 @@\n-    __ vzeroupper();\n-    __ leave();\n-    __ ret(0);\n+  __ vzeroupper();\n+  __ leave();\n+  __ ret(0);\n@@ -6829,2 +6886,2 @@\n-    return start;\n-  }\n+  return start;\n+}\n@@ -6833,27 +6890,10 @@\n-   *  Arguments:\n-   *\n-  \/\/  Input:\n-  \/\/    c_rarg0   - x address\n-  \/\/    c_rarg1   - x length\n-  \/\/    c_rarg2   - z address\n-  \/\/    c_rarg3   - z length\n-   *\n-   *\/\n-  address generate_squareToLen() {\n-\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"squareToLen\");\n-\n-    address start = __ pc();\n-    \/\/ Win64: rcx, rdx, r8, r9 (c_rarg0, c_rarg1, ...)\n-    \/\/ Unix:  rdi, rsi, rdx, rcx (c_rarg0, c_rarg1, ...)\n-    const Register x      = rdi;\n-    const Register len    = rsi;\n-    const Register z      = r8;\n-    const Register zlen   = rcx;\n-\n-   const Register tmp1      = r12;\n-   const Register tmp2      = r13;\n-   const Register tmp3      = r14;\n-   const Register tmp4      = r15;\n-   const Register tmp5      = rbx;\n+ *  Arguments:\n+ *\n+\/\/  Input:\n+\/\/    c_rarg0   - x address\n+\/\/    c_rarg1   - x length\n+\/\/    c_rarg2   - z address\n+\/\/    c_rarg3   - z length\n+ *\n+ *\/\n+address StubGenerator::generate_squareToLen() {\n@@ -6861,2 +6901,3 @@\n-    BLOCK_COMMENT(\"Entry:\");\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"squareToLen\");\n+  address start = __ pc();\n@@ -6864,5 +6905,6 @@\n-    setup_arg_regs(4); \/\/ x => rdi, len => rsi, z => rdx\n-                       \/\/ zlen => rcx\n-                       \/\/ r9 and r10 may be used to save non-volatile registers\n-    __ movptr(r8, rdx);\n-    __ square_to_len(x, len, z, zlen, tmp1, tmp2, tmp3, tmp4, tmp5, rdx, rax);\n+  \/\/ Win64: rcx, rdx, r8, r9 (c_rarg0, c_rarg1, ...)\n+  \/\/ Unix:  rdi, rsi, rdx, rcx (c_rarg0, c_rarg1, ...)\n+  const Register x      = rdi;\n+  const Register len    = rsi;\n+  const Register z      = r8;\n+  const Register zlen   = rcx;\n@@ -6870,1 +6912,5 @@\n-    restore_arg_regs();\n+ const Register tmp1      = r12;\n+ const Register tmp2      = r13;\n+ const Register tmp3      = r14;\n+ const Register tmp4      = r15;\n+ const Register tmp5      = rbx;\n@@ -6872,2 +6918,2 @@\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n+  BLOCK_COMMENT(\"Entry:\");\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n@@ -6875,2 +6921,10 @@\n-    return start;\n-  }\n+  setup_arg_regs(4); \/\/ x => rdi, len => rsi, z => rdx\n+                     \/\/ zlen => rcx\n+                     \/\/ r9 and r10 may be used to save non-volatile registers\n+  __ movptr(r8, rdx);\n+  __ square_to_len(x, len, z, zlen, tmp1, tmp2, tmp3, tmp4, tmp5, rdx, rax);\n+\n+  restore_arg_regs();\n+\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n@@ -6878,3 +6932,2 @@\n-  address generate_method_entry_barrier() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"nmethod_entry_barrier\");\n+  return start;\n+}\n@@ -6882,1 +6935,4 @@\n-    Label deoptimize_label;\n+address StubGenerator::generate_method_entry_barrier() {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"nmethod_entry_barrier\");\n+  address start = __ pc();\n@@ -6884,1 +6940,1 @@\n-    address start = __ pc();\n+  Label deoptimize_label;\n@@ -6886,1 +6942,1 @@\n-    __ push(-1); \/\/ cookie, this is used for writing the new rsp when deoptimizing\n+  __ push(-1); \/\/ cookie, this is used for writing the new rsp when deoptimizing\n@@ -6888,43 +6944,2 @@\n-    BLOCK_COMMENT(\"Entry:\");\n-    __ enter(); \/\/ save rbp\n-\n-    \/\/ save c_rarg0, because we want to use that value.\n-    \/\/ We could do without it but then we depend on the number of slots used by pusha\n-    __ push(c_rarg0);\n-\n-    __ lea(c_rarg0, Address(rsp, wordSize * 3)); \/\/ 1 for cookie, 1 for rbp, 1 for c_rarg0 - this should be the return address\n-\n-    __ pusha();\n-\n-    \/\/ The method may have floats as arguments, and we must spill them before calling\n-    \/\/ the VM runtime.\n-    assert(Argument::n_float_register_parameters_j == 8, \"Assumption\");\n-    const int xmm_size = wordSize * 2;\n-    const int xmm_spill_size = xmm_size * Argument::n_float_register_parameters_j;\n-    __ subptr(rsp, xmm_spill_size);\n-    __ movdqu(Address(rsp, xmm_size * 7), xmm7);\n-    __ movdqu(Address(rsp, xmm_size * 6), xmm6);\n-    __ movdqu(Address(rsp, xmm_size * 5), xmm5);\n-    __ movdqu(Address(rsp, xmm_size * 4), xmm4);\n-    __ movdqu(Address(rsp, xmm_size * 3), xmm3);\n-    __ movdqu(Address(rsp, xmm_size * 2), xmm2);\n-    __ movdqu(Address(rsp, xmm_size * 1), xmm1);\n-    __ movdqu(Address(rsp, xmm_size * 0), xmm0);\n-\n-    __ call_VM_leaf(CAST_FROM_FN_PTR(address, static_cast<int (*)(address*)>(BarrierSetNMethod::nmethod_stub_entry_barrier)), 1);\n-\n-    __ movdqu(xmm0, Address(rsp, xmm_size * 0));\n-    __ movdqu(xmm1, Address(rsp, xmm_size * 1));\n-    __ movdqu(xmm2, Address(rsp, xmm_size * 2));\n-    __ movdqu(xmm3, Address(rsp, xmm_size * 3));\n-    __ movdqu(xmm4, Address(rsp, xmm_size * 4));\n-    __ movdqu(xmm5, Address(rsp, xmm_size * 5));\n-    __ movdqu(xmm6, Address(rsp, xmm_size * 6));\n-    __ movdqu(xmm7, Address(rsp, xmm_size * 7));\n-    __ addptr(rsp, xmm_spill_size);\n-\n-    __ cmpl(rax, 1); \/\/ 1 means deoptimize\n-    __ jcc(Assembler::equal, deoptimize_label);\n-\n-    __ popa();\n-    __ pop(c_rarg0);\n+  BLOCK_COMMENT(\"Entry:\");\n+  __ enter(); \/\/ save rbp\n@@ -6932,1 +6947,3 @@\n-    __ leave();\n+  \/\/ save c_rarg0, because we want to use that value.\n+  \/\/ We could do without it but then we depend on the number of slots used by pusha\n+  __ push(c_rarg0);\n@@ -6934,2 +6951,1 @@\n-    __ addptr(rsp, 1 * wordSize); \/\/ cookie\n-    __ ret(0);\n+  __ lea(c_rarg0, Address(rsp, wordSize * 3)); \/\/ 1 for cookie, 1 for rbp, 1 for c_rarg0 - this should be the return address\n@@ -6937,0 +6953,1 @@\n+  __ pusha();\n@@ -6938,1 +6955,14 @@\n-    __ BIND(deoptimize_label);\n+  \/\/ The method may have floats as arguments, and we must spill them before calling\n+  \/\/ the VM runtime.\n+  assert(Argument::n_float_register_parameters_j == 8, \"Assumption\");\n+  const int xmm_size = wordSize * 2;\n+  const int xmm_spill_size = xmm_size * Argument::n_float_register_parameters_j;\n+  __ subptr(rsp, xmm_spill_size);\n+  __ movdqu(Address(rsp, xmm_size * 7), xmm7);\n+  __ movdqu(Address(rsp, xmm_size * 6), xmm6);\n+  __ movdqu(Address(rsp, xmm_size * 5), xmm5);\n+  __ movdqu(Address(rsp, xmm_size * 4), xmm4);\n+  __ movdqu(Address(rsp, xmm_size * 3), xmm3);\n+  __ movdqu(Address(rsp, xmm_size * 2), xmm2);\n+  __ movdqu(Address(rsp, xmm_size * 1), xmm1);\n+  __ movdqu(Address(rsp, xmm_size * 0), xmm0);\n@@ -6940,2 +6970,1 @@\n-    __ popa();\n-    __ pop(c_rarg0);\n+  __ call_VM_leaf(CAST_FROM_FN_PTR(address, static_cast<int (*)(address*)>(BarrierSetNMethod::nmethod_stub_entry_barrier)), 1);\n@@ -6943,1 +6972,9 @@\n-    __ leave();\n+  __ movdqu(xmm0, Address(rsp, xmm_size * 0));\n+  __ movdqu(xmm1, Address(rsp, xmm_size * 1));\n+  __ movdqu(xmm2, Address(rsp, xmm_size * 2));\n+  __ movdqu(xmm3, Address(rsp, xmm_size * 3));\n+  __ movdqu(xmm4, Address(rsp, xmm_size * 4));\n+  __ movdqu(xmm5, Address(rsp, xmm_size * 5));\n+  __ movdqu(xmm6, Address(rsp, xmm_size * 6));\n+  __ movdqu(xmm7, Address(rsp, xmm_size * 7));\n+  __ addptr(rsp, xmm_spill_size);\n@@ -6945,42 +6982,2 @@\n-    \/\/ this can be taken out, but is good for verification purposes. getting a SIGSEGV\n-    \/\/ here while still having a correct stack is valuable\n-    __ testptr(rsp, Address(rsp, 0));\n-\n-    __ movptr(rsp, Address(rsp, 0)); \/\/ new rsp was written in the barrier\n-    __ jmp(Address(rsp, -1 * wordSize)); \/\/ jmp target should be callers verified_entry_point\n-\n-    return start;\n-  }\n-\n-   \/**\n-   *  Arguments:\n-   *\n-   *  Input:\n-   *    c_rarg0   - out address\n-   *    c_rarg1   - in address\n-   *    c_rarg2   - offset\n-   *    c_rarg3   - len\n-   * not Win64\n-   *    c_rarg4   - k\n-   * Win64\n-   *    rsp+40    - k\n-   *\/\n-  address generate_mulAdd() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"mulAdd\");\n-\n-    address start = __ pc();\n-    \/\/ Win64: rcx, rdx, r8, r9 (c_rarg0, c_rarg1, ...)\n-    \/\/ Unix:  rdi, rsi, rdx, rcx, r8, r9 (c_rarg0, c_rarg1, ...)\n-    const Register out     = rdi;\n-    const Register in      = rsi;\n-    const Register offset  = r11;\n-    const Register len     = rcx;\n-    const Register k       = r8;\n-\n-    \/\/ Next registers will be saved on stack in mul_add().\n-    const Register tmp1  = r12;\n-    const Register tmp2  = r13;\n-    const Register tmp3  = r14;\n-    const Register tmp4  = r15;\n-    const Register tmp5  = rbx;\n+  __ cmpl(rax, 1); \/\/ 1 means deoptimize\n+  __ jcc(Assembler::equal, deoptimize_label);\n@@ -6988,2 +6985,2 @@\n-    BLOCK_COMMENT(\"Entry:\");\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ popa();\n+  __ pop(c_rarg0);\n@@ -6991,9 +6988,1 @@\n-    setup_arg_regs(4); \/\/ out => rdi, in => rsi, offset => rdx\n-                       \/\/ len => rcx, k => r8\n-                       \/\/ r9 and r10 may be used to save non-volatile registers\n-#ifdef _WIN64\n-    \/\/ last argument is on stack on Win64\n-    __ movl(k, Address(rsp, 6 * wordSize));\n-#endif\n-    __ movptr(r11, rdx);  \/\/ move offset in rdx to offset(r11)\n-    __ mul_add(out, in, offset, len, k, tmp1, tmp2, tmp3, tmp4, tmp5, rdx, rax);\n+  __ leave();\n@@ -7001,1 +6990,2 @@\n-    restore_arg_regs();\n+  __ addptr(rsp, 1 * wordSize); \/\/ cookie\n+  __ ret(0);\n@@ -7003,2 +6993,0 @@\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n@@ -7006,2 +6994,1 @@\n-    return start;\n-  }\n+  __ BIND(deoptimize_label);\n@@ -7009,3 +6996,2 @@\n-  address generate_bigIntegerRightShift() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"bigIntegerRightShiftWorker\");\n+  __ popa();\n+  __ pop(c_rarg0);\n@@ -7013,8 +6999,1 @@\n-    address start = __ pc();\n-    Label Shift512Loop, ShiftTwo, ShiftTwoLoop, ShiftOne, Exit;\n-    \/\/ For Unix, the arguments are as follows: rdi, rsi, rdx, rcx, r8.\n-    const Register newArr = rdi;\n-    const Register oldArr = rsi;\n-    const Register newIdx = rdx;\n-    const Register shiftCount = rcx;  \/\/ It was intentional to have shiftCount in rcx since it is used implicitly for shift.\n-    const Register totalNumIter = r8;\n+  __ leave();\n@@ -7022,8 +7001,3 @@\n-    \/\/ For windows, we use r9 and r10 as temps to save rdi and rsi. Thus we cannot allocate them for our temps.\n-    \/\/ For everything else, we prefer using r9 and r10 since we do not have to save them before use.\n-    const Register tmp1 = r11;                    \/\/ Caller save.\n-    const Register tmp2 = rax;                    \/\/ Caller save.\n-    const Register tmp3 = WINDOWS_ONLY(r12) NOT_WINDOWS(r9);   \/\/ Windows: Callee save. Linux: Caller save.\n-    const Register tmp4 = WINDOWS_ONLY(r13) NOT_WINDOWS(r10);  \/\/ Windows: Callee save. Linux: Caller save.\n-    const Register tmp5 = r14;                    \/\/ Callee save.\n-    const Register tmp6 = r15;\n+  \/\/ this can be taken out, but is good for verification purposes. getting a SIGSEGV\n+  \/\/ here while still having a correct stack is valuable\n+  __ testptr(rsp, Address(rsp, 0));\n@@ -7031,3 +7005,2 @@\n-    const XMMRegister x0 = xmm0;\n-    const XMMRegister x1 = xmm1;\n-    const XMMRegister x2 = xmm2;\n+  __ movptr(rsp, Address(rsp, 0)); \/\/ new rsp was written in the barrier\n+  __ jmp(Address(rsp, -1 * wordSize)); \/\/ jmp target should be callers verified_entry_point\n@@ -7035,2 +7008,2 @@\n-    BLOCK_COMMENT(\"Entry:\");\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  return start;\n+}\n@@ -7038,7 +7011,42 @@\n-#ifdef _WINDOWS\n-    setup_arg_regs(4);\n-    \/\/ For windows, since last argument is on stack, we need to move it to the appropriate register.\n-    __ movl(totalNumIter, Address(rsp, 6 * wordSize));\n-    \/\/ Save callee save registers.\n-    __ push(tmp3);\n-    __ push(tmp4);\n+ \/**\n+ *  Arguments:\n+ *\n+ *  Input:\n+ *    c_rarg0   - out address\n+ *    c_rarg1   - in address\n+ *    c_rarg2   - offset\n+ *    c_rarg3   - len\n+ * not Win64\n+ *    c_rarg4   - k\n+ * Win64\n+ *    rsp+40    - k\n+ *\/\n+address StubGenerator::generate_mulAdd() {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"mulAdd\");\n+  address start = __ pc();\n+\n+  \/\/ Win64: rcx, rdx, r8, r9 (c_rarg0, c_rarg1, ...)\n+  \/\/ Unix:  rdi, rsi, rdx, rcx, r8, r9 (c_rarg0, c_rarg1, ...)\n+  const Register out     = rdi;\n+  const Register in      = rsi;\n+  const Register offset  = r11;\n+  const Register len     = rcx;\n+  const Register k       = r8;\n+\n+  \/\/ Next registers will be saved on stack in mul_add().\n+  const Register tmp1  = r12;\n+  const Register tmp2  = r13;\n+  const Register tmp3  = r14;\n+  const Register tmp4  = r15;\n+  const Register tmp5  = rbx;\n+\n+  BLOCK_COMMENT(\"Entry:\");\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+\n+  setup_arg_regs(4); \/\/ out => rdi, in => rsi, offset => rdx\n+                     \/\/ len => rcx, k => r8\n+                     \/\/ r9 and r10 may be used to save non-volatile registers\n+#ifdef _WIN64\n+  \/\/ last argument is on stack on Win64\n+  __ movl(k, Address(rsp, 6 * wordSize));\n@@ -7046,22 +7054,2 @@\n-    __ push(tmp5);\n-\n-    \/\/ Rename temps used throughout the code.\n-    const Register idx = tmp1;\n-    const Register nIdx = tmp2;\n-\n-    __ xorl(idx, idx);\n-\n-    \/\/ Start right shift from end of the array.\n-    \/\/ For example, if #iteration = 4 and newIdx = 1\n-    \/\/ then dest[4] = src[4] >> shiftCount  | src[3] <<< (shiftCount - 32)\n-    \/\/ if #iteration = 4 and newIdx = 0\n-    \/\/ then dest[3] = src[4] >> shiftCount  | src[3] <<< (shiftCount - 32)\n-    __ movl(idx, totalNumIter);\n-    __ movl(nIdx, idx);\n-    __ addl(nIdx, newIdx);\n-\n-    \/\/ If vectorization is enabled, check if the number of iterations is at least 64\n-    \/\/ If not, then go to ShifTwo processing 2 iterations\n-    if (VM_Version::supports_avx512_vbmi2()) {\n-      __ cmpptr(totalNumIter, (AVX3Threshold\/64));\n-      __ jcc(Assembler::less, ShiftTwo);\n+  __ movptr(r11, rdx);  \/\/ move offset in rdx to offset(r11)\n+  __ mul_add(out, in, offset, len, k, tmp1, tmp2, tmp3, tmp4, tmp5, rdx, rax);\n@@ -7069,98 +7057,7 @@\n-      if (AVX3Threshold < 16 * 64) {\n-        __ cmpl(totalNumIter, 16);\n-        __ jcc(Assembler::less, ShiftTwo);\n-      }\n-      __ evpbroadcastd(x0, shiftCount, Assembler::AVX_512bit);\n-      __ subl(idx, 16);\n-      __ subl(nIdx, 16);\n-      __ BIND(Shift512Loop);\n-      __ evmovdqul(x2, Address(oldArr, idx, Address::times_4, 4), Assembler::AVX_512bit);\n-      __ evmovdqul(x1, Address(oldArr, idx, Address::times_4), Assembler::AVX_512bit);\n-      __ vpshrdvd(x2, x1, x0, Assembler::AVX_512bit);\n-      __ evmovdqul(Address(newArr, nIdx, Address::times_4), x2, Assembler::AVX_512bit);\n-      __ subl(nIdx, 16);\n-      __ subl(idx, 16);\n-      __ jcc(Assembler::greaterEqual, Shift512Loop);\n-      __ addl(idx, 16);\n-      __ addl(nIdx, 16);\n-    }\n-    __ BIND(ShiftTwo);\n-    __ cmpl(idx, 2);\n-    __ jcc(Assembler::less, ShiftOne);\n-    __ subl(idx, 2);\n-    __ subl(nIdx, 2);\n-    __ BIND(ShiftTwoLoop);\n-    __ movl(tmp5, Address(oldArr, idx, Address::times_4, 8));\n-    __ movl(tmp4, Address(oldArr, idx, Address::times_4, 4));\n-    __ movl(tmp3, Address(oldArr, idx, Address::times_4));\n-    __ shrdl(tmp5, tmp4);\n-    __ shrdl(tmp4, tmp3);\n-    __ movl(Address(newArr, nIdx, Address::times_4, 4), tmp5);\n-    __ movl(Address(newArr, nIdx, Address::times_4), tmp4);\n-    __ subl(nIdx, 2);\n-    __ subl(idx, 2);\n-    __ jcc(Assembler::greaterEqual, ShiftTwoLoop);\n-    __ addl(idx, 2);\n-    __ addl(nIdx, 2);\n-\n-    \/\/ Do the last iteration\n-    __ BIND(ShiftOne);\n-    __ cmpl(idx, 1);\n-    __ jcc(Assembler::less, Exit);\n-    __ subl(idx, 1);\n-    __ subl(nIdx, 1);\n-    __ movl(tmp4, Address(oldArr, idx, Address::times_4, 4));\n-    __ movl(tmp3, Address(oldArr, idx, Address::times_4));\n-    __ shrdl(tmp4, tmp3);\n-    __ movl(Address(newArr, nIdx, Address::times_4), tmp4);\n-    __ BIND(Exit);\n-    __ vzeroupper();\n-    \/\/ Restore callee save registers.\n-    __ pop(tmp5);\n-#ifdef _WINDOWS\n-    __ pop(tmp4);\n-    __ pop(tmp3);\n-    restore_arg_regs();\n-#endif\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-    return start;\n-  }\n-\n-   \/**\n-   *  Arguments:\n-   *\n-   *  Input:\n-   *    c_rarg0   - newArr address\n-   *    c_rarg1   - oldArr address\n-   *    c_rarg2   - newIdx\n-   *    c_rarg3   - shiftCount\n-   * not Win64\n-   *    c_rarg4   - numIter\n-   * Win64\n-   *    rsp40    - numIter\n-   *\/\n-  address generate_bigIntegerLeftShift() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this,  \"StubRoutines\", \"bigIntegerLeftShiftWorker\");\n-    address start = __ pc();\n-    Label Shift512Loop, ShiftTwo, ShiftTwoLoop, ShiftOne, Exit;\n-    \/\/ For Unix, the arguments are as follows: rdi, rsi, rdx, rcx, r8.\n-    const Register newArr = rdi;\n-    const Register oldArr = rsi;\n-    const Register newIdx = rdx;\n-    const Register shiftCount = rcx;  \/\/ It was intentional to have shiftCount in rcx since it is used implicitly for shift.\n-    const Register totalNumIter = r8;\n-    \/\/ For windows, we use r9 and r10 as temps to save rdi and rsi. Thus we cannot allocate them for our temps.\n-    \/\/ For everything else, we prefer using r9 and r10 since we do not have to save them before use.\n-    const Register tmp1 = r11;                    \/\/ Caller save.\n-    const Register tmp2 = rax;                    \/\/ Caller save.\n-    const Register tmp3 = WINDOWS_ONLY(r12) NOT_WINDOWS(r9);   \/\/ Windows: Callee save. Linux: Caller save.\n-    const Register tmp4 = WINDOWS_ONLY(r13) NOT_WINDOWS(r10);  \/\/ Windows: Callee save. Linux: Caller save.\n-    const Register tmp5 = r14;                    \/\/ Callee save.\n-\n-    const XMMRegister x0 = xmm0;\n-    const XMMRegister x1 = xmm1;\n-    const XMMRegister x2 = xmm2;\n-    BLOCK_COMMENT(\"Entry:\");\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  restore_arg_regs();\n+\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n+\n+  return start;\n+}\n@@ -7168,7 +7065,36 @@\n-#ifdef _WINDOWS\n-    setup_arg_regs(4);\n-    \/\/ For windows, since last argument is on stack, we need to move it to the appropriate register.\n-    __ movl(totalNumIter, Address(rsp, 6 * wordSize));\n-    \/\/ Save callee save registers.\n-    __ push(tmp3);\n-    __ push(tmp4);\n+address StubGenerator::generate_bigIntegerRightShift() {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"bigIntegerRightShiftWorker\");\n+  address start = __ pc();\n+\n+  Label Shift512Loop, ShiftTwo, ShiftTwoLoop, ShiftOne, Exit;\n+  \/\/ For Unix, the arguments are as follows: rdi, rsi, rdx, rcx, r8.\n+  const Register newArr = rdi;\n+  const Register oldArr = rsi;\n+  const Register newIdx = rdx;\n+  const Register shiftCount = rcx;  \/\/ It was intentional to have shiftCount in rcx since it is used implicitly for shift.\n+  const Register totalNumIter = r8;\n+\n+  \/\/ For windows, we use r9 and r10 as temps to save rdi and rsi. Thus we cannot allocate them for our temps.\n+  \/\/ For everything else, we prefer using r9 and r10 since we do not have to save them before use.\n+  const Register tmp1 = r11;                    \/\/ Caller save.\n+  const Register tmp2 = rax;                    \/\/ Caller save.\n+  const Register tmp3 = WIN64_ONLY(r12) NOT_WIN64(r9);   \/\/ Windows: Callee save. Linux: Caller save.\n+  const Register tmp4 = WIN64_ONLY(r13) NOT_WIN64(r10);  \/\/ Windows: Callee save. Linux: Caller save.\n+  const Register tmp5 = r14;                    \/\/ Callee save.\n+  const Register tmp6 = r15;\n+\n+  const XMMRegister x0 = xmm0;\n+  const XMMRegister x1 = xmm1;\n+  const XMMRegister x2 = xmm2;\n+\n+  BLOCK_COMMENT(\"Entry:\");\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+\n+#ifdef _WIN64\n+  setup_arg_regs(4);\n+  \/\/ For windows, since last argument is on stack, we need to move it to the appropriate register.\n+  __ movl(totalNumIter, Address(rsp, 6 * wordSize));\n+  \/\/ Save callee save registers.\n+  __ push(tmp3);\n+  __ push(tmp4);\n@@ -7176,16 +7102,25 @@\n-    __ push(tmp5);\n-\n-    \/\/ Rename temps used throughout the code\n-    const Register idx = tmp1;\n-    const Register numIterTmp = tmp2;\n-\n-    \/\/ Start idx from zero.\n-    __ xorl(idx, idx);\n-    \/\/ Compute interior pointer for new array. We do this so that we can use same index for both old and new arrays.\n-    __ lea(newArr, Address(newArr, newIdx, Address::times_4));\n-    __ movl(numIterTmp, totalNumIter);\n-\n-    \/\/ If vectorization is enabled, check if the number of iterations is at least 64\n-    \/\/ If not, then go to ShiftTwo shifting two numbers at a time\n-    if (VM_Version::supports_avx512_vbmi2()) {\n-      __ cmpl(totalNumIter, (AVX3Threshold\/64));\n+  __ push(tmp5);\n+\n+  \/\/ Rename temps used throughout the code.\n+  const Register idx = tmp1;\n+  const Register nIdx = tmp2;\n+\n+  __ xorl(idx, idx);\n+\n+  \/\/ Start right shift from end of the array.\n+  \/\/ For example, if #iteration = 4 and newIdx = 1\n+  \/\/ then dest[4] = src[4] >> shiftCount  | src[3] <<< (shiftCount - 32)\n+  \/\/ if #iteration = 4 and newIdx = 0\n+  \/\/ then dest[3] = src[4] >> shiftCount  | src[3] <<< (shiftCount - 32)\n+  __ movl(idx, totalNumIter);\n+  __ movl(nIdx, idx);\n+  __ addl(nIdx, newIdx);\n+\n+  \/\/ If vectorization is enabled, check if the number of iterations is at least 64\n+  \/\/ If not, then go to ShifTwo processing 2 iterations\n+  if (VM_Version::supports_avx512_vbmi2()) {\n+    __ cmpptr(totalNumIter, (AVX3Threshold\/64));\n+    __ jcc(Assembler::less, ShiftTwo);\n+\n+    if (AVX3Threshold < 16 * 64) {\n+      __ cmpl(totalNumIter, 16);\n@@ -7193,16 +7128,0 @@\n-\n-      if (AVX3Threshold < 16 * 64) {\n-        __ cmpl(totalNumIter, 16);\n-        __ jcc(Assembler::less, ShiftTwo);\n-      }\n-      __ evpbroadcastd(x0, shiftCount, Assembler::AVX_512bit);\n-      __ subl(numIterTmp, 16);\n-      __ BIND(Shift512Loop);\n-      __ evmovdqul(x1, Address(oldArr, idx, Address::times_4), Assembler::AVX_512bit);\n-      __ evmovdqul(x2, Address(oldArr, idx, Address::times_4, 0x4), Assembler::AVX_512bit);\n-      __ vpshldvd(x1, x2, x0, Assembler::AVX_512bit);\n-      __ evmovdqul(Address(newArr, idx, Address::times_4), x1, Assembler::AVX_512bit);\n-      __ addl(idx, 16);\n-      __ subl(numIterTmp, 16);\n-      __ jcc(Assembler::greaterEqual, Shift512Loop);\n-      __ addl(numIterTmp, 16);\n@@ -7210,40 +7129,13 @@\n-    __ BIND(ShiftTwo);\n-    __ cmpl(totalNumIter, 1);\n-    __ jcc(Assembler::less, Exit);\n-    __ movl(tmp3, Address(oldArr, idx, Address::times_4));\n-    __ subl(numIterTmp, 2);\n-    __ jcc(Assembler::less, ShiftOne);\n-\n-    __ BIND(ShiftTwoLoop);\n-    __ movl(tmp4, Address(oldArr, idx, Address::times_4, 0x4));\n-    __ movl(tmp5, Address(oldArr, idx, Address::times_4, 0x8));\n-    __ shldl(tmp3, tmp4);\n-    __ shldl(tmp4, tmp5);\n-    __ movl(Address(newArr, idx, Address::times_4), tmp3);\n-    __ movl(Address(newArr, idx, Address::times_4, 0x4), tmp4);\n-    __ movl(tmp3, tmp5);\n-    __ addl(idx, 2);\n-    __ subl(numIterTmp, 2);\n-    __ jcc(Assembler::greaterEqual, ShiftTwoLoop);\n-\n-    \/\/ Do the last iteration\n-    __ BIND(ShiftOne);\n-    __ addl(numIterTmp, 2);\n-    __ cmpl(numIterTmp, 1);\n-    __ jcc(Assembler::less, Exit);\n-    __ movl(tmp4, Address(oldArr, idx, Address::times_4, 0x4));\n-    __ shldl(tmp3, tmp4);\n-    __ movl(Address(newArr, idx, Address::times_4), tmp3);\n-\n-    __ BIND(Exit);\n-    __ vzeroupper();\n-    \/\/ Restore callee save registers.\n-    __ pop(tmp5);\n-#ifdef _WINDOWS\n-    __ pop(tmp4);\n-    __ pop(tmp3);\n-    restore_arg_regs();\n-#endif\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-    return start;\n+    __ evpbroadcastd(x0, shiftCount, Assembler::AVX_512bit);\n+    __ subl(idx, 16);\n+    __ subl(nIdx, 16);\n+    __ BIND(Shift512Loop);\n+    __ evmovdqul(x2, Address(oldArr, idx, Address::times_4, 4), Assembler::AVX_512bit);\n+    __ evmovdqul(x1, Address(oldArr, idx, Address::times_4), Assembler::AVX_512bit);\n+    __ vpshrdvd(x2, x1, x0, Assembler::AVX_512bit);\n+    __ evmovdqul(Address(newArr, nIdx, Address::times_4), x2, Assembler::AVX_512bit);\n+    __ subl(nIdx, 16);\n+    __ subl(idx, 16);\n+    __ jcc(Assembler::greaterEqual, Shift512Loop);\n+    __ addl(idx, 16);\n+    __ addl(nIdx, 16);\n@@ -7251,0 +7143,40 @@\n+  __ BIND(ShiftTwo);\n+  __ cmpl(idx, 2);\n+  __ jcc(Assembler::less, ShiftOne);\n+  __ subl(idx, 2);\n+  __ subl(nIdx, 2);\n+  __ BIND(ShiftTwoLoop);\n+  __ movl(tmp5, Address(oldArr, idx, Address::times_4, 8));\n+  __ movl(tmp4, Address(oldArr, idx, Address::times_4, 4));\n+  __ movl(tmp3, Address(oldArr, idx, Address::times_4));\n+  __ shrdl(tmp5, tmp4);\n+  __ shrdl(tmp4, tmp3);\n+  __ movl(Address(newArr, nIdx, Address::times_4, 4), tmp5);\n+  __ movl(Address(newArr, nIdx, Address::times_4), tmp4);\n+  __ subl(nIdx, 2);\n+  __ subl(idx, 2);\n+  __ jcc(Assembler::greaterEqual, ShiftTwoLoop);\n+  __ addl(idx, 2);\n+  __ addl(nIdx, 2);\n+\n+  \/\/ Do the last iteration\n+  __ BIND(ShiftOne);\n+  __ cmpl(idx, 1);\n+  __ jcc(Assembler::less, Exit);\n+  __ subl(idx, 1);\n+  __ subl(nIdx, 1);\n+  __ movl(tmp4, Address(oldArr, idx, Address::times_4, 4));\n+  __ movl(tmp3, Address(oldArr, idx, Address::times_4));\n+  __ shrdl(tmp4, tmp3);\n+  __ movl(Address(newArr, nIdx, Address::times_4), tmp4);\n+  __ BIND(Exit);\n+  __ vzeroupper();\n+  \/\/ Restore callee save registers.\n+  __ pop(tmp5);\n+#ifdef _WIN64\n+  __ pop(tmp4);\n+  __ pop(tmp3);\n+  restore_arg_regs();\n+#endif\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n@@ -7252,2 +7184,2 @@\n-  address generate_libmExp() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"libmExp\");\n+  return start;\n+}\n@@ -7255,1 +7187,38 @@\n-    address start = __ pc();\n+ \/**\n+ *  Arguments:\n+ *\n+ *  Input:\n+ *    c_rarg0   - newArr address\n+ *    c_rarg1   - oldArr address\n+ *    c_rarg2   - newIdx\n+ *    c_rarg3   - shiftCount\n+ * not Win64\n+ *    c_rarg4   - numIter\n+ * Win64\n+ *    rsp40    - numIter\n+ *\/\n+address StubGenerator::generate_bigIntegerLeftShift() {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this,  \"StubRoutines\", \"bigIntegerLeftShiftWorker\");\n+  address start = __ pc();\n+\n+  Label Shift512Loop, ShiftTwo, ShiftTwoLoop, ShiftOne, Exit;\n+  \/\/ For Unix, the arguments are as follows: rdi, rsi, rdx, rcx, r8.\n+  const Register newArr = rdi;\n+  const Register oldArr = rsi;\n+  const Register newIdx = rdx;\n+  const Register shiftCount = rcx;  \/\/ It was intentional to have shiftCount in rcx since it is used implicitly for shift.\n+  const Register totalNumIter = r8;\n+  \/\/ For windows, we use r9 and r10 as temps to save rdi and rsi. Thus we cannot allocate them for our temps.\n+  \/\/ For everything else, we prefer using r9 and r10 since we do not have to save them before use.\n+  const Register tmp1 = r11;                    \/\/ Caller save.\n+  const Register tmp2 = rax;                    \/\/ Caller save.\n+  const Register tmp3 = WIN64_ONLY(r12) NOT_WIN64(r9);   \/\/ Windows: Callee save. Linux: Caller save.\n+  const Register tmp4 = WIN64_ONLY(r13) NOT_WIN64(r10);  \/\/ Windows: Callee save. Linux: Caller save.\n+  const Register tmp5 = r14;                    \/\/ Callee save.\n+\n+  const XMMRegister x0 = xmm0;\n+  const XMMRegister x1 = xmm1;\n+  const XMMRegister x2 = xmm2;\n+  BLOCK_COMMENT(\"Entry:\");\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n@@ -7257,2 +7226,81 @@\n-    BLOCK_COMMENT(\"Entry:\");\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+#ifdef _WIN64\n+  setup_arg_regs(4);\n+  \/\/ For windows, since last argument is on stack, we need to move it to the appropriate register.\n+  __ movl(totalNumIter, Address(rsp, 6 * wordSize));\n+  \/\/ Save callee save registers.\n+  __ push(tmp3);\n+  __ push(tmp4);\n+#endif\n+  __ push(tmp5);\n+\n+  \/\/ Rename temps used throughout the code\n+  const Register idx = tmp1;\n+  const Register numIterTmp = tmp2;\n+\n+  \/\/ Start idx from zero.\n+  __ xorl(idx, idx);\n+  \/\/ Compute interior pointer for new array. We do this so that we can use same index for both old and new arrays.\n+  __ lea(newArr, Address(newArr, newIdx, Address::times_4));\n+  __ movl(numIterTmp, totalNumIter);\n+\n+  \/\/ If vectorization is enabled, check if the number of iterations is at least 64\n+  \/\/ If not, then go to ShiftTwo shifting two numbers at a time\n+  if (VM_Version::supports_avx512_vbmi2()) {\n+    __ cmpl(totalNumIter, (AVX3Threshold\/64));\n+    __ jcc(Assembler::less, ShiftTwo);\n+\n+    if (AVX3Threshold < 16 * 64) {\n+      __ cmpl(totalNumIter, 16);\n+      __ jcc(Assembler::less, ShiftTwo);\n+    }\n+    __ evpbroadcastd(x0, shiftCount, Assembler::AVX_512bit);\n+    __ subl(numIterTmp, 16);\n+    __ BIND(Shift512Loop);\n+    __ evmovdqul(x1, Address(oldArr, idx, Address::times_4), Assembler::AVX_512bit);\n+    __ evmovdqul(x2, Address(oldArr, idx, Address::times_4, 0x4), Assembler::AVX_512bit);\n+    __ vpshldvd(x1, x2, x0, Assembler::AVX_512bit);\n+    __ evmovdqul(Address(newArr, idx, Address::times_4), x1, Assembler::AVX_512bit);\n+    __ addl(idx, 16);\n+    __ subl(numIterTmp, 16);\n+    __ jcc(Assembler::greaterEqual, Shift512Loop);\n+    __ addl(numIterTmp, 16);\n+  }\n+  __ BIND(ShiftTwo);\n+  __ cmpl(totalNumIter, 1);\n+  __ jcc(Assembler::less, Exit);\n+  __ movl(tmp3, Address(oldArr, idx, Address::times_4));\n+  __ subl(numIterTmp, 2);\n+  __ jcc(Assembler::less, ShiftOne);\n+\n+  __ BIND(ShiftTwoLoop);\n+  __ movl(tmp4, Address(oldArr, idx, Address::times_4, 0x4));\n+  __ movl(tmp5, Address(oldArr, idx, Address::times_4, 0x8));\n+  __ shldl(tmp3, tmp4);\n+  __ shldl(tmp4, tmp5);\n+  __ movl(Address(newArr, idx, Address::times_4), tmp3);\n+  __ movl(Address(newArr, idx, Address::times_4, 0x4), tmp4);\n+  __ movl(tmp3, tmp5);\n+  __ addl(idx, 2);\n+  __ subl(numIterTmp, 2);\n+  __ jcc(Assembler::greaterEqual, ShiftTwoLoop);\n+\n+  \/\/ Do the last iteration\n+  __ BIND(ShiftOne);\n+  __ addl(numIterTmp, 2);\n+  __ cmpl(numIterTmp, 1);\n+  __ jcc(Assembler::less, Exit);\n+  __ movl(tmp4, Address(oldArr, idx, Address::times_4, 0x4));\n+  __ shldl(tmp3, tmp4);\n+  __ movl(Address(newArr, idx, Address::times_4), tmp3);\n+\n+  __ BIND(Exit);\n+  __ vzeroupper();\n+  \/\/ Restore callee save registers.\n+  __ pop(tmp5);\n+#ifdef _WIN64\n+  __ pop(tmp4);\n+  __ pop(tmp3);\n+  restore_arg_regs();\n+#endif\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n@@ -7260,2 +7308,2 @@\n-    __ fast_exp(xmm0, xmm1, xmm2, xmm3, xmm4, xmm5, xmm6, xmm7,\n-                rax, rcx, rdx, r11);\n+  return start;\n+}\n@@ -7263,2 +7311,3 @@\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n+address StubGenerator::generate_libmExp() {\n+  StubCodeMark mark(this, \"StubRoutines\", \"libmExp\");\n+  address start = __ pc();\n@@ -7266,1 +7315,2 @@\n-    return start;\n+  BLOCK_COMMENT(\"Entry:\");\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n@@ -7268,1 +7318,2 @@\n-  }\n+  __ fast_exp(xmm0, xmm1, xmm2, xmm3, xmm4, xmm5, xmm6, xmm7,\n+              rax, rcx, rdx, r11);\n@@ -7270,2 +7321,2 @@\n-  address generate_libmLog() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"libmLog\");\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n@@ -7273,1 +7324,2 @@\n-    address start = __ pc();\n+  return start;\n+}\n@@ -7275,2 +7327,3 @@\n-    BLOCK_COMMENT(\"Entry:\");\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+address StubGenerator::generate_libmLog() {\n+  StubCodeMark mark(this, \"StubRoutines\", \"libmLog\");\n+  address start = __ pc();\n@@ -7278,2 +7331,2 @@\n-    __ fast_log(xmm0, xmm1, xmm2, xmm3, xmm4, xmm5, xmm6, xmm7,\n-                rax, rcx, rdx, r11, r8);\n+  BLOCK_COMMENT(\"Entry:\");\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n@@ -7281,2 +7334,2 @@\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n+  __ fast_log(xmm0, xmm1, xmm2, xmm3, xmm4, xmm5, xmm6, xmm7,\n+              rax, rcx, rdx, r11, r8);\n@@ -7284,1 +7337,2 @@\n-    return start;\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n@@ -7286,1 +7340,2 @@\n-  }\n+  return start;\n+}\n@@ -7288,2 +7343,3 @@\n-  address generate_libmLog10() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"libmLog10\");\n+address StubGenerator::generate_libmLog10() {\n+  StubCodeMark mark(this, \"StubRoutines\", \"libmLog10\");\n+  address start = __ pc();\n@@ -7291,1 +7347,2 @@\n-    address start = __ pc();\n+  BLOCK_COMMENT(\"Entry:\");\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n@@ -7293,2 +7350,2 @@\n-    BLOCK_COMMENT(\"Entry:\");\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ fast_log10(xmm0, xmm1, xmm2, xmm3, xmm4, xmm5, xmm6, xmm7,\n+                rax, rcx, rdx, r11, r8);\n@@ -7296,2 +7353,2 @@\n-    __ fast_log10(xmm0, xmm1, xmm2, xmm3, xmm4, xmm5, xmm6, xmm7,\n-                  rax, rcx, rdx, r11, r8);\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n@@ -7299,2 +7356,2 @@\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n+  return start;\n+}\n@@ -7302,1 +7359,3 @@\n-    return start;\n+address StubGenerator::generate_libmPow() {\n+  StubCodeMark mark(this, \"StubRoutines\", \"libmPow\");\n+  address start = __ pc();\n@@ -7304,1 +7363,2 @@\n-  }\n+  BLOCK_COMMENT(\"Entry:\");\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n@@ -7306,2 +7366,2 @@\n-  address generate_libmPow() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"libmPow\");\n+  __ fast_pow(xmm0, xmm1, xmm2, xmm3, xmm4, xmm5, xmm6, xmm7,\n+              rax, rcx, rdx, r8, r9, r10, r11);\n@@ -7309,1 +7369,2 @@\n-    address start = __ pc();\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n@@ -7311,2 +7372,2 @@\n-    BLOCK_COMMENT(\"Entry:\");\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  return start;\n+}\n@@ -7314,2 +7375,3 @@\n-    __ fast_pow(xmm0, xmm1, xmm2, xmm3, xmm4, xmm5, xmm6, xmm7,\n-                rax, rcx, rdx, r8, r9, r10, r11);\n+address StubGenerator::generate_libmSin() {\n+  StubCodeMark mark(this, \"StubRoutines\", \"libmSin\");\n+  address start = __ pc();\n@@ -7317,2 +7379,2 @@\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n+  BLOCK_COMMENT(\"Entry:\");\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n@@ -7320,1 +7382,10 @@\n-    return start;\n+#ifdef _WIN64\n+  __ push(rsi);\n+  __ push(rdi);\n+#endif\n+  __ fast_sin(xmm0, xmm1, xmm2, xmm3, xmm4, xmm5, xmm6, xmm7,\n+              rax, rbx, rcx, rdx, r8);\n+#ifdef _WIN64\n+  __ pop(rdi);\n+  __ pop(rsi);\n+#endif\n@@ -7322,1 +7393,2 @@\n-  }\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n@@ -7324,2 +7396,2 @@\n-  address generate_libmSin() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"libmSin\");\n+  return start;\n+}\n@@ -7327,1 +7399,3 @@\n-    address start = __ pc();\n+address StubGenerator::generate_libmCos() {\n+  StubCodeMark mark(this, \"StubRoutines\", \"libmCos\");\n+  address start = __ pc();\n@@ -7329,2 +7403,2 @@\n-    BLOCK_COMMENT(\"Entry:\");\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  BLOCK_COMMENT(\"Entry:\");\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n@@ -7333,2 +7407,2 @@\n-    __ push(rsi);\n-    __ push(rdi);\n+  __ push(rsi);\n+  __ push(rdi);\n@@ -7336,2 +7410,3 @@\n-    __ fast_sin(xmm0, xmm1, xmm2, xmm3, xmm4, xmm5, xmm6, xmm7,\n-                rax, rbx, rcx, rdx, r8);\n+  __ fast_cos(xmm0, xmm1, xmm2, xmm3, xmm4, xmm5, xmm6, xmm7,\n+              rax, rcx, rdx, r8, r9, r10, r11, rbx);\n+\n@@ -7339,2 +7414,2 @@\n-    __ pop(rdi);\n-    __ pop(rsi);\n+  __ pop(rdi);\n+  __ pop(rsi);\n@@ -7343,6 +7418,2 @@\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-\n-    return start;\n-\n-  }\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n@@ -7350,2 +7421,2 @@\n-  address generate_libmCos() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"libmCos\");\n+  return start;\n+}\n@@ -7353,1 +7424,3 @@\n-    address start = __ pc();\n+address StubGenerator::generate_libmTan() {\n+  StubCodeMark mark(this, \"StubRoutines\", \"libmTan\");\n+  address start = __ pc();\n@@ -7355,2 +7428,2 @@\n-    BLOCK_COMMENT(\"Entry:\");\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  BLOCK_COMMENT(\"Entry:\");\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n@@ -7359,2 +7432,2 @@\n-    __ push(rsi);\n-    __ push(rdi);\n+  __ push(rsi);\n+  __ push(rdi);\n@@ -7362,2 +7435,2 @@\n-    __ fast_cos(xmm0, xmm1, xmm2, xmm3, xmm4, xmm5, xmm6, xmm7,\n-                rax, rcx, rdx, r8, r9, r10, r11, rbx);\n+  __ fast_tan(xmm0, xmm1, xmm2, xmm3, xmm4, xmm5, xmm6, xmm7,\n+              rax, rcx, rdx, r8, r9, r10, r11, rbx);\n@@ -7366,2 +7439,2 @@\n-    __ pop(rdi);\n-    __ pop(rsi);\n+  __ pop(rdi);\n+  __ pop(rsi);\n@@ -7370,2 +7443,2 @@\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n@@ -7373,1 +7446,2 @@\n-    return start;\n+  return start;\n+}\n@@ -7375,1 +7449,2 @@\n-  }\n+RuntimeStub* StubGenerator::generate_cont_doYield() {\n+  if (!Continuations::enabled()) return nullptr;\n@@ -7377,2 +7452,7 @@\n-  address generate_libmTan() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"libmTan\");\n+  enum layout {\n+    rbp_off,\n+    rbpH_off,\n+    return_off,\n+    return_off2,\n+    framesize \/\/ inclusive of return address\n+  };\n@@ -7380,1 +7460,48 @@\n-    address start = __ pc();\n+  CodeBuffer code(\"cont_doYield\", 512, 64);\n+  MacroAssembler* _masm = new MacroAssembler(&code);\n+  address start = __ pc();\n+\n+  __ enter();\n+  address the_pc = __ pc();\n+\n+  int frame_complete = the_pc - start;\n+\n+  \/\/ This nop must be exactly at the PC we push into the frame info.\n+  \/\/ We use this nop for fast CodeBlob lookup, associate the OopMap\n+  \/\/ with it right away.\n+  __ post_call_nop();\n+  OopMapSet* oop_maps = new OopMapSet();\n+  OopMap* map = new OopMap(framesize, 1);\n+  oop_maps->add_gc_map(frame_complete, map);\n+\n+  __ set_last_Java_frame(rsp, rbp, the_pc, rscratch1);\n+  __ movptr(c_rarg0, r15_thread);\n+  __ movptr(c_rarg1, rsp);\n+  __ call_VM_leaf(Continuation::freeze_entry(), 2);\n+  __ reset_last_Java_frame(true);\n+\n+  Label L_pinned;\n+\n+  __ testptr(rax, rax);\n+  __ jcc(Assembler::notZero, L_pinned);\n+\n+  __ movptr(rsp, Address(r15_thread, JavaThread::cont_entry_offset()));\n+  __ continuation_enter_cleanup();\n+  __ pop(rbp);\n+  __ ret(0);\n+\n+  __ bind(L_pinned);\n+\n+  \/\/ Pinned, return to caller\n+  __ leave();\n+  __ ret(0);\n+\n+  RuntimeStub* stub =\n+    RuntimeStub::new_runtime_stub(code.name(),\n+                                  &code,\n+                                  frame_complete,\n+                                  (framesize >> (LogBytesPerWord - LogBytesPerInt)),\n+                                  oop_maps,\n+                                  false);\n+  return stub;\n+}\n@@ -7382,2 +7509,2 @@\n-    BLOCK_COMMENT(\"Entry:\");\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+address StubGenerator::generate_cont_thaw(const char* label, Continuation::thaw_kind kind) {\n+  if (!Continuations::enabled()) return nullptr;\n@@ -7385,6 +7512,2 @@\n-#ifdef _WIN64\n-    __ push(rsi);\n-    __ push(rdi);\n-#endif\n-    __ fast_tan(xmm0, xmm1, xmm2, xmm3, xmm4, xmm5, xmm6, xmm7,\n-                rax, rcx, rdx, r8, r9, r10, r11, rbx);\n+  bool return_barrier = Continuation::is_thaw_return_barrier(kind);\n+  bool return_barrier_exception = Continuation::is_thaw_return_barrier_exception(kind);\n@@ -7392,4 +7515,2 @@\n-#ifdef _WIN64\n-    __ pop(rdi);\n-    __ pop(rsi);\n-#endif\n+  StubCodeMark mark(this, \"StubRoutines\", label);\n+  address start = __ pc();\n@@ -7397,2 +7518,30 @@\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n+  \/\/ TODO: Handle Valhalla return types. May require generating different return barriers.\n+\n+  if (!return_barrier) {\n+    \/\/ Pop return address. If we don't do this, we get a drift,\n+    \/\/ where the bottom-most frozen frame continuously grows.\n+    __ pop(c_rarg3);\n+  } else {\n+    __ movptr(rsp, Address(r15_thread, JavaThread::cont_entry_offset()));\n+  }\n+\n+#ifdef ASSERT\n+  {\n+    Label L_good_sp;\n+    __ cmpptr(rsp, Address(r15_thread, JavaThread::cont_entry_offset()));\n+    __ jcc(Assembler::equal, L_good_sp);\n+    __ stop(\"Incorrect rsp at thaw entry\");\n+    __ BIND(L_good_sp);\n+  }\n+#endif \/\/ ASSERT\n+\n+  if (return_barrier) {\n+    \/\/ Preserve possible return value from a method returning to the return barrier.\n+    __ push(rax);\n+    __ push_d(xmm0);\n+  }\n+\n+  __ movptr(c_rarg0, r15_thread);\n+  __ movptr(c_rarg1, (return_barrier ? 1 : 0));\n+  __ call_VM_leaf(CAST_FROM_FN_PTR(address, Continuation::prepare_thaw), 2);\n+  __ movptr(rbx, rax);\n@@ -7400,1 +7549,6 @@\n-    return start;\n+  if (return_barrier) {\n+    \/\/ Restore return value from a method returning to the return barrier.\n+    \/\/ No safepoint in the call to thaw, so even an oop return value should be OK.\n+    __ pop_d(xmm0);\n+    __ pop(rax);\n+  }\n@@ -7402,0 +7556,7 @@\n+#ifdef ASSERT\n+  {\n+    Label L_good_sp;\n+    __ cmpptr(rsp, Address(r15_thread, JavaThread::cont_entry_offset()));\n+    __ jcc(Assembler::equal, L_good_sp);\n+    __ stop(\"Incorrect rsp after prepare thaw\");\n+    __ BIND(L_good_sp);\n@@ -7403,0 +7564,1 @@\n+#endif \/\/ ASSERT\n@@ -7404,2 +7566,6 @@\n-  RuntimeStub* generate_cont_doYield() {\n-    if (!Continuations::enabled()) return nullptr;\n+  \/\/ rbx contains the size of the frames to thaw, 0 if overflow or no more frames\n+  Label L_thaw_success;\n+  __ testptr(rbx, rbx);\n+  __ jccb(Assembler::notZero, L_thaw_success);\n+  __ jump(ExternalAddress(StubRoutines::throw_StackOverflowError_entry()));\n+  __ bind(L_thaw_success);\n@@ -7407,7 +7573,3 @@\n-    enum layout {\n-      rbp_off,\n-      rbpH_off,\n-      return_off,\n-      return_off2,\n-      framesize \/\/ inclusive of return address\n-    };\n+  \/\/ Make room for the thawed frames and align the stack.\n+  __ subptr(rsp, rbx);\n+  __ andptr(rsp, -StackAlignmentInBytes);\n@@ -7415,2 +7577,5 @@\n-    CodeBuffer code(\"cont_doYield\", 512, 64);\n-    MacroAssembler* _masm = new MacroAssembler(&code);\n+  if (return_barrier) {\n+    \/\/ Preserve possible return value from a method returning to the return barrier. (Again.)\n+    __ push(rax);\n+    __ push_d(xmm0);\n+  }\n@@ -7418,3 +7583,5 @@\n-    address start = __ pc();\n-    __ enter();\n-    address the_pc = __ pc();\n+  \/\/ If we want, we can templatize thaw by kind, and have three different entries.\n+  __ movptr(c_rarg0, r15_thread);\n+  __ movptr(c_rarg1, kind);\n+  __ call_VM_leaf(Continuation::thaw_entry(), 2);\n+  __ movptr(rbx, rax);\n@@ -7422,1 +7589,9 @@\n-    int frame_complete = the_pc - start;\n+  if (return_barrier) {\n+    \/\/ Restore return value from a method returning to the return barrier. (Again.)\n+    \/\/ No safepoint in the call to thaw, so even an oop return value should be OK.\n+    __ pop_d(xmm0);\n+    __ pop(rax);\n+  } else {\n+    \/\/ Return 0 (success) from doYield.\n+    __ xorptr(rax, rax);\n+  }\n@@ -7424,7 +7599,4 @@\n-    \/\/ This nop must be exactly at the PC we push into the frame info.\n-    \/\/ We use this nop for fast CodeBlob lookup, associate the OopMap\n-    \/\/ with it right away.\n-    __ post_call_nop();\n-    OopMapSet* oop_maps = new OopMapSet();\n-    OopMap* map = new OopMap(framesize, 1);\n-    oop_maps->add_gc_map(frame_complete, map);\n+  \/\/ After thawing, rbx is the SP of the yielding frame.\n+  \/\/ Move there, and then to saved RBP slot.\n+  __ movptr(rsp, rbx);\n+  __ subptr(rsp, 2*wordSize);\n@@ -7432,1 +7604,1 @@\n-    __ set_last_Java_frame(rsp, rbp, the_pc, rscratch1);\n+  if (return_barrier_exception) {\n@@ -7434,3 +7606,1 @@\n-    __ movptr(c_rarg1, rsp);\n-    __ call_VM_leaf(Continuation::freeze_entry(), 2);\n-    __ reset_last_Java_frame(true);\n+    __ movptr(c_rarg1, Address(rsp, wordSize)); \/\/ return address\n@@ -7438,1 +7608,2 @@\n-    Label L_pinned;\n+    \/\/ rax still holds the original exception oop, save it before the call\n+    __ push(rax);\n@@ -7440,2 +7611,2 @@\n-    __ testptr(rax, rax);\n-    __ jcc(Assembler::notZero, L_pinned);\n+    __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::exception_handler_for_return_address), 2);\n+    __ movptr(rbx, rax);\n@@ -7443,2 +7614,11 @@\n-    __ movptr(rsp, Address(r15_thread, JavaThread::cont_entry_offset()));\n-    __ continuation_enter_cleanup();\n+    \/\/ Continue at exception handler:\n+    \/\/   rax: exception oop\n+    \/\/   rbx: exception handler\n+    \/\/   rdx: exception pc\n+    __ pop(rax);\n+    __ verify_oop(rax);\n+    __ pop(rbp); \/\/ pop out RBP here too\n+    __ pop(rdx);\n+    __ jmp(rbx);\n+  } else {\n+    \/\/ We are \"returning\" into the topmost thawed frame; see Thaw::push_return_frame\n@@ -7447,0 +7627,1 @@\n+  }\n@@ -7448,1 +7629,2 @@\n-    __ bind(L_pinned);\n+  return start;\n+}\n@@ -7450,3 +7632,3 @@\n-    \/\/ Pinned, return to caller\n-    __ leave();\n-    __ ret(0);\n+address StubGenerator::generate_cont_thaw() {\n+  return generate_cont_thaw(\"Cont thaw\", Continuation::thaw_top);\n+}\n@@ -7454,9 +7636,1 @@\n-    RuntimeStub* stub =\n-      RuntimeStub::new_runtime_stub(code.name(),\n-                                    &code,\n-                                    frame_complete,\n-                                    (framesize >> (LogBytesPerWord - LogBytesPerInt)),\n-                                    oop_maps,\n-                                    false);\n-    return stub;\n-  }\n+\/\/ TODO: will probably need multiple return barriers depending on return type\n@@ -7464,2 +7638,3 @@\n-  address generate_cont_thaw(const char* label, Continuation::thaw_kind kind) {\n-    if (!Continuations::enabled()) return nullptr;\n+address StubGenerator::generate_cont_returnBarrier() {\n+  return generate_cont_thaw(\"Cont thaw return barrier\", Continuation::thaw_return_barrier);\n+}\n@@ -7467,2 +7642,3 @@\n-    bool return_barrier = Continuation::is_thaw_return_barrier(kind);\n-    bool return_barrier_exception = Continuation::is_thaw_return_barrier_exception(kind);\n+address StubGenerator::generate_cont_returnBarrier_exception() {\n+  return generate_cont_thaw(\"Cont thaw return barrier exception\", Continuation::thaw_return_barrier_exception);\n+}\n@@ -7470,2 +7646,1 @@\n-    StubCodeMark mark(this, \"StubRoutines\", label);\n-    address start = __ pc();\n+#if INCLUDE_JFR\n@@ -7473,1 +7648,11 @@\n-    \/\/ TODO: Handle Valhalla return types. May require generating different return barriers.\n+\/\/ For c2: c_rarg0 is junk, call to runtime to write a checkpoint.\n+\/\/ It returns a jobject handle to the event writer.\n+\/\/ The handle is dereferenced and the return value is the event writer oop.\n+RuntimeStub* StubGenerator::generate_jfr_write_checkpoint() {\n+  enum layout {\n+    rbp_off,\n+    rbpH_off,\n+    return_off,\n+    return_off2,\n+    framesize \/\/ inclusive of return address\n+  };\n@@ -7475,7 +7660,3 @@\n-    if (!return_barrier) {\n-      \/\/ Pop return address. If we don't do this, we get a drift,\n-      \/\/ where the bottom-most frozen frame continuously grows.\n-      __ pop(c_rarg3);\n-    } else {\n-      __ movptr(rsp, Address(r15_thread, JavaThread::cont_entry_offset()));\n-    }\n+  CodeBuffer code(\"jfr_write_checkpoint\", 512, 64);\n+  MacroAssembler* _masm = new MacroAssembler(&code);\n+  address start = __ pc();\n@@ -7483,9 +7664,2 @@\n-#ifdef ASSERT\n-    {\n-      Label L_good_sp;\n-      __ cmpptr(rsp, Address(r15_thread, JavaThread::cont_entry_offset()));\n-      __ jcc(Assembler::equal, L_good_sp);\n-      __ stop(\"Incorrect rsp at thaw entry\");\n-      __ BIND(L_good_sp);\n-    }\n-#endif\n+  __ enter();\n+  address the_pc = __ pc();\n@@ -7493,5 +7667,1 @@\n-    if (return_barrier) {\n-      \/\/ Preserve possible return value from a method returning to the return barrier.\n-      __ push(rax);\n-      __ push_d(xmm0);\n-    }\n+  int frame_complete = the_pc - start;\n@@ -7499,4 +7669,4 @@\n-    __ movptr(c_rarg0, r15_thread);\n-    __ movptr(c_rarg1, (return_barrier ? 1 : 0));\n-    __ call_VM_leaf(CAST_FROM_FN_PTR(address, Continuation::prepare_thaw), 2);\n-    __ movptr(rbx, rax);\n+  __ set_last_Java_frame(rsp, rbp, the_pc, rscratch1);\n+  __ movptr(c_rarg0, r15_thread);\n+  __ call_VM_leaf(CAST_FROM_FN_PTR(address, JfrIntrinsicSupport::write_checkpoint), 1);\n+  __ reset_last_Java_frame(true);\n@@ -7504,6 +7674,4 @@\n-    if (return_barrier) {\n-      \/\/ Restore return value from a method returning to the return barrier.\n-      \/\/ No safepoint in the call to thaw, so even an oop return value should be OK.\n-      __ pop_d(xmm0);\n-      __ pop(rax);\n-    }\n+  \/\/ rax is jobject handle result, unpack and process it through a barrier.\n+  Label L_null_jobject;\n+  __ testptr(rax, rax);\n+  __ jcc(Assembler::zero, L_null_jobject);\n@@ -7511,9 +7679,2 @@\n-#ifdef ASSERT\n-    {\n-      Label L_good_sp;\n-      __ cmpptr(rsp, Address(r15_thread, JavaThread::cont_entry_offset()));\n-      __ jcc(Assembler::equal, L_good_sp);\n-      __ stop(\"Incorrect rsp after prepare thaw\");\n-      __ BIND(L_good_sp);\n-    }\n-#endif\n+  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  bs->load_at(_masm, ACCESS_READ | IN_NATIVE, T_OBJECT, rax, Address(rax, 0), c_rarg0, r15_thread);\n@@ -7521,16 +7682,1 @@\n-    \/\/ rbx contains the size of the frames to thaw, 0 if overflow or no more frames\n-    Label L_thaw_success;\n-    __ testptr(rbx, rbx);\n-    __ jccb(Assembler::notZero, L_thaw_success);\n-    __ jump(ExternalAddress(StubRoutines::throw_StackOverflowError_entry()));\n-    __ bind(L_thaw_success);\n-\n-    \/\/ Make room for the thawed frames and align the stack.\n-    __ subptr(rsp, rbx);\n-    __ andptr(rsp, -StackAlignmentInBytes);\n-\n-    if (return_barrier) {\n-      \/\/ Preserve possible return value from a method returning to the return barrier. (Again.)\n-      __ push(rax);\n-      __ push_d(xmm0);\n-    }\n+  __ bind(L_null_jobject);\n@@ -7538,5 +7684,2 @@\n-    \/\/ If we want, we can templatize thaw by kind, and have three different entries.\n-    __ movptr(c_rarg0, r15_thread);\n-    __ movptr(c_rarg1, kind);\n-    __ call_VM_leaf(Continuation::thaw_entry(), 2);\n-    __ movptr(rbx, rax);\n+  __ leave();\n+  __ ret(0);\n@@ -7544,9 +7687,3 @@\n-    if (return_barrier) {\n-      \/\/ Restore return value from a method returning to the return barrier. (Again.)\n-      \/\/ No safepoint in the call to thaw, so even an oop return value should be OK.\n-      __ pop_d(xmm0);\n-      __ pop(rax);\n-    } else {\n-      \/\/ Return 0 (success) from doYield.\n-      __ xorptr(rax, rax);\n-    }\n+  OopMapSet* oop_maps = new OopMapSet();\n+  OopMap* map = new OopMap(framesize, 1);\n+  oop_maps->add_gc_map(frame_complete, map);\n@@ -7554,29 +7691,9 @@\n-    \/\/ After thawing, rbx is the SP of the yielding frame.\n-    \/\/ Move there, and then to saved RBP slot.\n-    __ movptr(rsp, rbx);\n-    __ subptr(rsp, 2*wordSize);\n-\n-    if (return_barrier_exception) {\n-      __ movptr(c_rarg0, r15_thread);\n-      __ movptr(c_rarg1, Address(rsp, wordSize)); \/\/ return address\n-\n-      \/\/ rax still holds the original exception oop, save it before the call\n-      __ push(rax);\n-\n-      __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::exception_handler_for_return_address), 2);\n-      __ movptr(rbx, rax);\n-\n-      \/\/ Continue at exception handler:\n-      \/\/   rax: exception oop\n-      \/\/   rbx: exception handler\n-      \/\/   rdx: exception pc\n-      __ pop(rax);\n-      __ verify_oop(rax);\n-      __ pop(rbp); \/\/ pop out RBP here too\n-      __ pop(rdx);\n-      __ jmp(rbx);\n-    } else {\n-      \/\/ We are \"returning\" into the topmost thawed frame; see Thaw::push_return_frame\n-      __ pop(rbp);\n-      __ ret(0);\n-    }\n+  RuntimeStub* stub =\n+    RuntimeStub::new_runtime_stub(code.name(),\n+                                  &code,\n+                                  frame_complete,\n+                                  (framesize >> (LogBytesPerWord - LogBytesPerInt)),\n+                                  oop_maps,\n+                                  false);\n+  return stub;\n+}\n@@ -7584,2 +7701,1 @@\n-    return start;\n-  }\n+#endif \/\/ INCLUDE_JFR\n@@ -7587,3 +7703,30 @@\n-  address generate_cont_thaw() {\n-    return generate_cont_thaw(\"Cont thaw\", Continuation::thaw_top);\n-  }\n+\/\/ Continuation point for throwing of implicit exceptions that are\n+\/\/ not handled in the current activation. Fabricates an exception\n+\/\/ oop and initiates normal exception dispatching in this\n+\/\/ frame. Since we need to preserve callee-saved values (currently\n+\/\/ only for C2, but done for C1 as well) we need a callee-saved oop\n+\/\/ map and therefore have to make these stubs into RuntimeStubs\n+\/\/ rather than BufferBlobs.  If the compiler needs all registers to\n+\/\/ be preserved between the fault point and the exception handler\n+\/\/ then it must assume responsibility for that in\n+\/\/ AbstractCompiler::continuation_for_implicit_null_exception or\n+\/\/ continuation_for_implicit_division_by_zero_exception. All other\n+\/\/ implicit exceptions (e.g., NullPointerException or\n+\/\/ AbstractMethodError on entry) are either at call sites or\n+\/\/ otherwise assume that stack unwinding will be initiated, so\n+\/\/ caller saved registers were assumed volatile in the compiler.\n+address StubGenerator::generate_throw_exception(const char* name,\n+                                                address runtime_entry,\n+                                                Register arg1,\n+                                                Register arg2) {\n+  \/\/ Information about frame layout at time of blocking runtime call.\n+  \/\/ Note that we only have to preserve callee-saved registers since\n+  \/\/ the compilers are responsible for supplying a continuation point\n+  \/\/ if they expect all registers to be preserved.\n+  enum layout {\n+    rbp_off = frame::arg_reg_save_area_bytes\/BytesPerInt,\n+    rbp_off2,\n+    return_off,\n+    return_off2,\n+    framesize \/\/ inclusive of return address\n+  };\n@@ -7591,1 +7734,2 @@\n-  \/\/ TODO: will probably need multiple return barriers depending on return type\n+  int insts_size = 512;\n+  int locs_size  = 64;\n@@ -7593,3 +7737,3 @@\n-  address generate_cont_returnBarrier() {\n-    return generate_cont_thaw(\"Cont thaw return barrier\", Continuation::thaw_return_barrier);\n-  }\n+  CodeBuffer code(name, insts_size, locs_size);\n+  OopMapSet* oop_maps  = new OopMapSet();\n+  MacroAssembler* _masm = new MacroAssembler(&code);\n@@ -7597,3 +7741,1 @@\n-  address generate_cont_returnBarrier_exception() {\n-    return generate_cont_thaw(\"Cont thaw return barrier exception\", Continuation::thaw_return_barrier_exception);\n-  }\n+  address start = __ pc();\n@@ -7601,1 +7743,4 @@\n-#if INCLUDE_JFR\n+  \/\/ This is an inlined and slightly modified version of call_VM\n+  \/\/ which has the ability to fetch the return PC out of\n+  \/\/ thread-local storage and also sets up last_Java_sp slightly\n+  \/\/ differently than the real call_VM\n@@ -7603,11 +7748,1 @@\n-  \/\/ For c2: c_rarg0 is junk, call to runtime to write a checkpoint.\n-  \/\/ It returns a jobject handle to the event writer.\n-  \/\/ The handle is dereferenced and the return value is the event writer oop.\n-  RuntimeStub* generate_jfr_write_checkpoint() {\n-    enum layout {\n-      rbp_off,\n-      rbpH_off,\n-      return_off,\n-      return_off2,\n-      framesize \/\/ inclusive of return address\n-    };\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n@@ -7615,2 +7750,1 @@\n-    CodeBuffer code(\"jfr_write_checkpoint\", 512, 64);\n-    MacroAssembler* _masm = new MacroAssembler(&code);\n+  assert(is_even(framesize\/2), \"sp not 16-byte aligned\");\n@@ -7618,3 +7752,2 @@\n-    address start = __ pc();\n-    __ enter();\n-    address the_pc = __ pc();\n+  \/\/ return address and rbp are already in place\n+  __ subptr(rsp, (framesize-4) << LogBytesPerInt); \/\/ prolog\n@@ -7622,1 +7755,1 @@\n-    int frame_complete = the_pc - start;\n+  int frame_complete = __ pc() - start;\n@@ -7624,4 +7757,4 @@\n-    __ set_last_Java_frame(rsp, rbp, the_pc, rscratch1);\n-    __ movptr(c_rarg0, r15_thread);\n-    __ call_VM_leaf(CAST_FROM_FN_PTR(address, JfrIntrinsicSupport::write_checkpoint), 1);\n-    __ reset_last_Java_frame(true);\n+  \/\/ Set up last_Java_sp and last_Java_fp\n+  address the_pc = __ pc();\n+  __ set_last_Java_frame(rsp, rbp, the_pc, rscratch1);\n+  __ andptr(rsp, -(StackAlignmentInBytes));    \/\/ Align stack\n@@ -7629,4 +7762,11 @@\n-    \/\/ rax is jobject handle result, unpack and process it through a barrier.\n-    Label L_null_jobject;\n-    __ testptr(rax, rax);\n-    __ jcc(Assembler::zero, L_null_jobject);\n+  \/\/ Call runtime\n+  if (arg1 != noreg) {\n+    assert(arg2 != c_rarg1, \"clobbered\");\n+    __ movptr(c_rarg1, arg1);\n+  }\n+  if (arg2 != noreg) {\n+    __ movptr(c_rarg2, arg2);\n+  }\n+  __ movptr(c_rarg0, r15_thread);\n+  BLOCK_COMMENT(\"call runtime_entry\");\n+  __ call(RuntimeAddress(runtime_entry));\n@@ -7634,2 +7774,2 @@\n-    BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-    bs->load_at(_masm, ACCESS_READ | IN_NATIVE, T_OBJECT, rax, Address(rax, 0), c_rarg0, r15_thread);\n+  \/\/ Generate oop map\n+  OopMap* map = new OopMap(framesize, 0);\n@@ -7637,1 +7777,1 @@\n-    __ bind(L_null_jobject);\n+  oop_maps->add_gc_map(the_pc - start, map);\n@@ -7639,2 +7779,1 @@\n-    __ leave();\n-    __ ret(0);\n+  __ reset_last_Java_frame(true);\n@@ -7642,3 +7781,1 @@\n-    OopMapSet* oop_maps = new OopMapSet();\n-    OopMap* map = new OopMap(framesize, 1);\n-    oop_maps->add_gc_map(frame_complete, map);\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n@@ -7646,9 +7783,9 @@\n-    RuntimeStub* stub =\n-      RuntimeStub::new_runtime_stub(code.name(),\n-                                    &code,\n-                                    frame_complete,\n-                                    (framesize >> (LogBytesPerWord - LogBytesPerInt)),\n-                                    oop_maps,\n-                                    false);\n-    return stub;\n-  }\n+  \/\/ check for pending exceptions\n+#ifdef ASSERT\n+  Label L;\n+  __ cmpptr(Address(r15_thread, Thread::pending_exception_offset()), NULL_WORD);\n+  __ jcc(Assembler::notEqual, L);\n+  __ should_not_reach_here();\n+  __ bind(L);\n+#endif \/\/ ASSERT\n+  __ jump(RuntimeAddress(StubRoutines::forward_exception_entry()));\n@@ -7656,1 +7793,0 @@\n-#endif \/\/ INCLUDE_JFR\n@@ -7658,73 +7794,9 @@\n-#undef __\n-#define __ masm->\n-\n-  \/\/ Continuation point for throwing of implicit exceptions that are\n-  \/\/ not handled in the current activation. Fabricates an exception\n-  \/\/ oop and initiates normal exception dispatching in this\n-  \/\/ frame. Since we need to preserve callee-saved values (currently\n-  \/\/ only for C2, but done for C1 as well) we need a callee-saved oop\n-  \/\/ map and therefore have to make these stubs into RuntimeStubs\n-  \/\/ rather than BufferBlobs.  If the compiler needs all registers to\n-  \/\/ be preserved between the fault point and the exception handler\n-  \/\/ then it must assume responsibility for that in\n-  \/\/ AbstractCompiler::continuation_for_implicit_null_exception or\n-  \/\/ continuation_for_implicit_division_by_zero_exception. All other\n-  \/\/ implicit exceptions (e.g., NullPointerException or\n-  \/\/ AbstractMethodError on entry) are either at call sites or\n-  \/\/ otherwise assume that stack unwinding will be initiated, so\n-  \/\/ caller saved registers were assumed volatile in the compiler.\n-  address generate_throw_exception(const char* name,\n-                                   address runtime_entry,\n-                                   Register arg1 = noreg,\n-                                   Register arg2 = noreg) {\n-    \/\/ Information about frame layout at time of blocking runtime call.\n-    \/\/ Note that we only have to preserve callee-saved registers since\n-    \/\/ the compilers are responsible for supplying a continuation point\n-    \/\/ if they expect all registers to be preserved.\n-    enum layout {\n-      rbp_off = frame::arg_reg_save_area_bytes\/BytesPerInt,\n-      rbp_off2,\n-      return_off,\n-      return_off2,\n-      framesize \/\/ inclusive of return address\n-    };\n-\n-    int insts_size = 512;\n-    int locs_size  = 64;\n-\n-    CodeBuffer code(name, insts_size, locs_size);\n-    OopMapSet* oop_maps  = new OopMapSet();\n-    MacroAssembler* masm = new MacroAssembler(&code);\n-\n-    address start = __ pc();\n-\n-    \/\/ This is an inlined and slightly modified version of call_VM\n-    \/\/ which has the ability to fetch the return PC out of\n-    \/\/ thread-local storage and also sets up last_Java_sp slightly\n-    \/\/ differently than the real call_VM\n-\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-\n-    assert(is_even(framesize\/2), \"sp not 16-byte aligned\");\n-\n-    \/\/ return address and rbp are already in place\n-    __ subptr(rsp, (framesize-4) << LogBytesPerInt); \/\/ prolog\n-\n-    int frame_complete = __ pc() - start;\n-\n-    \/\/ Set up last_Java_sp and last_Java_fp\n-    address the_pc = __ pc();\n-    __ set_last_Java_frame(rsp, rbp, the_pc, rscratch1);\n-    __ andptr(rsp, -(StackAlignmentInBytes));    \/\/ Align stack\n-\n-    \/\/ Call runtime\n-    if (arg1 != noreg) {\n-      assert(arg2 != c_rarg1, \"clobbered\");\n-      __ movptr(c_rarg1, arg1);\n-    }\n-    if (arg2 != noreg) {\n-      __ movptr(c_rarg2, arg2);\n-    }\n-    __ movptr(c_rarg0, r15_thread);\n-    BLOCK_COMMENT(\"call runtime_entry\");\n-    __ call(RuntimeAddress(runtime_entry));\n+  \/\/ codeBlob framesize is in words (not VMRegImpl::slot_size)\n+  RuntimeStub* stub =\n+    RuntimeStub::new_runtime_stub(name,\n+                                  &code,\n+                                  frame_complete,\n+                                  (framesize >> (LogBytesPerWord - LogBytesPerInt)),\n+                                  oop_maps, false);\n+  return stub->entry_point();\n+}\n@@ -7732,2 +7804,4 @@\n-    \/\/ Generate oop map\n-    OopMap* map = new OopMap(framesize, 0);\n+void StubGenerator::create_control_words() {\n+  \/\/ Round to nearest, 64-bit mode, exceptions masked\n+  StubRoutines::x86::_mxcsr_std = 0x1F80;\n+}\n@@ -7735,1 +7809,55 @@\n-    oop_maps->add_gc_map(the_pc - start, map);\n+\/\/ Initialization\n+void StubGenerator::generate_initial() {\n+  \/\/ Generates all stubs and initializes the entry points\n+\n+  \/\/ This platform-specific settings are needed by generate_call_stub()\n+  create_control_words();\n+\n+  \/\/ entry points that exist in all platforms Note: This is code\n+  \/\/ that could be shared among different platforms - however the\n+  \/\/ benefit seems to be smaller than the disadvantage of having a\n+  \/\/ much more complicated generator structure. See also comment in\n+  \/\/ stubRoutines.hpp.\n+\n+  StubRoutines::_forward_exception_entry = generate_forward_exception();\n+\n+  StubRoutines::_call_stub_entry =\n+    generate_call_stub(StubRoutines::_call_stub_return_address);\n+\n+  \/\/ is referenced by megamorphic call\n+  StubRoutines::_catch_exception_entry = generate_catch_exception();\n+\n+  \/\/ atomic calls\n+  StubRoutines::_fence_entry                = generate_orderaccess_fence();\n+\n+  \/\/ platform dependent\n+  StubRoutines::x86::_get_previous_sp_entry = generate_get_previous_sp();\n+\n+  StubRoutines::x86::_verify_mxcsr_entry    = generate_verify_mxcsr();\n+\n+  StubRoutines::x86::_f2i_fixup             = generate_f2i_fixup();\n+  StubRoutines::x86::_f2l_fixup             = generate_f2l_fixup();\n+  StubRoutines::x86::_d2i_fixup             = generate_d2i_fixup();\n+  StubRoutines::x86::_d2l_fixup             = generate_d2l_fixup();\n+\n+  StubRoutines::x86::_float_sign_mask       = generate_fp_mask(\"float_sign_mask\",  0x7FFFFFFF7FFFFFFF);\n+  StubRoutines::x86::_float_sign_flip       = generate_fp_mask(\"float_sign_flip\",  0x8000000080000000);\n+  StubRoutines::x86::_double_sign_mask      = generate_fp_mask(\"double_sign_mask\", 0x7FFFFFFFFFFFFFFF);\n+  StubRoutines::x86::_double_sign_flip      = generate_fp_mask(\"double_sign_flip\", 0x8000000000000000);\n+\n+  \/\/ Build this early so it's available for the interpreter.\n+  StubRoutines::_throw_StackOverflowError_entry =\n+    generate_throw_exception(\"StackOverflowError throw_exception\",\n+                             CAST_FROM_FN_PTR(address,\n+                                              SharedRuntime::\n+                                              throw_StackOverflowError));\n+  StubRoutines::_throw_delayed_StackOverflowError_entry =\n+    generate_throw_exception(\"delayed StackOverflowError throw_exception\",\n+                             CAST_FROM_FN_PTR(address,\n+                                              SharedRuntime::\n+                                              throw_delayed_StackOverflowError));\n+  if (UseCRC32Intrinsics) {\n+    \/\/ set table address before stub generation which use it\n+    StubRoutines::_crc_table_adr = (address)StubRoutines::x86::_crc_table;\n+    StubRoutines::_updateBytesCRC32 = generate_updateBytesCRC32();\n+  }\n@@ -7737,1 +7865,6 @@\n-    __ reset_last_Java_frame(true);\n+  if (UseCRC32CIntrinsics) {\n+    bool supports_clmul = VM_Version::supports_clmul();\n+    StubRoutines::x86::generate_CRC32C_table(supports_clmul);\n+    StubRoutines::_crc32c_table_addr = (address)StubRoutines::x86::_crc32c_table;\n+    StubRoutines::_updateBytesCRC32C = generate_updateBytesCRC32C(supports_clmul);\n+  }\n@@ -7739,1 +7872,3 @@\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  if (UseAdler32Intrinsics) {\n+     StubRoutines::_updateBytesAdler32 = generate_updateBytesAdler32();\n+  }\n@@ -7741,80 +7876,3 @@\n-    \/\/ check for pending exceptions\n-#ifdef ASSERT\n-    Label L;\n-    __ cmpptr(Address(r15_thread, Thread::pending_exception_offset()), NULL_WORD);\n-    __ jcc(Assembler::notEqual, L);\n-    __ should_not_reach_here();\n-    __ bind(L);\n-#endif \/\/ ASSERT\n-    __ jump(RuntimeAddress(StubRoutines::forward_exception_entry()));\n-\n-\n-    \/\/ codeBlob framesize is in words (not VMRegImpl::slot_size)\n-    RuntimeStub* stub =\n-      RuntimeStub::new_runtime_stub(name,\n-                                    &code,\n-                                    frame_complete,\n-                                    (framesize >> (LogBytesPerWord - LogBytesPerInt)),\n-                                    oop_maps, false);\n-    return stub->entry_point();\n-  }\n-\n-  void create_control_words() {\n-    \/\/ Round to nearest, 64-bit mode, exceptions masked\n-    StubRoutines::x86::_mxcsr_std = 0x1F80;\n-  }\n-\n-  \/\/ Initialization\n-  void generate_initial() {\n-    \/\/ Generates all stubs and initializes the entry points\n-\n-    \/\/ This platform-specific settings are needed by generate_call_stub()\n-    create_control_words();\n-\n-    \/\/ entry points that exist in all platforms Note: This is code\n-    \/\/ that could be shared among different platforms - however the\n-    \/\/ benefit seems to be smaller than the disadvantage of having a\n-    \/\/ much more complicated generator structure. See also comment in\n-    \/\/ stubRoutines.hpp.\n-\n-    StubRoutines::_forward_exception_entry = generate_forward_exception();\n-\n-    StubRoutines::_call_stub_entry =\n-      generate_call_stub(StubRoutines::_call_stub_return_address);\n-\n-    \/\/ is referenced by megamorphic call\n-    StubRoutines::_catch_exception_entry = generate_catch_exception();\n-\n-    \/\/ atomic calls\n-    StubRoutines::_fence_entry                = generate_orderaccess_fence();\n-\n-    \/\/ platform dependent\n-    StubRoutines::x86::_get_previous_sp_entry = generate_get_previous_sp();\n-\n-    StubRoutines::x86::_verify_mxcsr_entry    = generate_verify_mxcsr();\n-\n-    StubRoutines::x86::_f2i_fixup             = generate_f2i_fixup();\n-    StubRoutines::x86::_f2l_fixup             = generate_f2l_fixup();\n-    StubRoutines::x86::_d2i_fixup             = generate_d2i_fixup();\n-    StubRoutines::x86::_d2l_fixup             = generate_d2l_fixup();\n-\n-    StubRoutines::x86::_float_sign_mask       = generate_fp_mask(\"float_sign_mask\",  0x7FFFFFFF7FFFFFFF);\n-    StubRoutines::x86::_float_sign_flip       = generate_fp_mask(\"float_sign_flip\",  0x8000000080000000);\n-    StubRoutines::x86::_double_sign_mask      = generate_fp_mask(\"double_sign_mask\", 0x7FFFFFFFFFFFFFFF);\n-    StubRoutines::x86::_double_sign_flip      = generate_fp_mask(\"double_sign_flip\", 0x8000000000000000);\n-\n-    \/\/ Build this early so it's available for the interpreter.\n-    StubRoutines::_throw_StackOverflowError_entry =\n-      generate_throw_exception(\"StackOverflowError throw_exception\",\n-                               CAST_FROM_FN_PTR(address,\n-                                                SharedRuntime::\n-                                                throw_StackOverflowError));\n-    StubRoutines::_throw_delayed_StackOverflowError_entry =\n-      generate_throw_exception(\"delayed StackOverflowError throw_exception\",\n-                               CAST_FROM_FN_PTR(address,\n-                                                SharedRuntime::\n-                                                throw_delayed_StackOverflowError));\n-    if (UseCRC32Intrinsics) {\n-      \/\/ set table address before stub generation which use it\n-      StubRoutines::_crc_table_adr = (address)StubRoutines::x86::_crc_table;\n-      StubRoutines::_updateBytesCRC32 = generate_updateBytesCRC32();\n+  if (UseLibmIntrinsic && InlineIntrinsics) {\n+    if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dexp)) {\n+      StubRoutines::_dexp = generate_libmExp();\n@@ -7822,6 +7880,2 @@\n-\n-    if (UseCRC32CIntrinsics) {\n-      bool supports_clmul = VM_Version::supports_clmul();\n-      StubRoutines::x86::generate_CRC32C_table(supports_clmul);\n-      StubRoutines::_crc32c_table_addr = (address)StubRoutines::x86::_crc32c_table;\n-      StubRoutines::_updateBytesCRC32C = generate_updateBytesCRC32C(supports_clmul);\n+    if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dlog)) {\n+      StubRoutines::_dlog = generate_libmLog();\n@@ -7829,3 +7883,2 @@\n-\n-    if (UseAdler32Intrinsics) {\n-       StubRoutines::_updateBytesAdler32 = generate_updateBytesAdler32();\n+    if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dlog10)) {\n+      StubRoutines::_dlog10 = generate_libmLog10();\n@@ -7833,23 +7886,11 @@\n-\n-    if (UseLibmIntrinsic && InlineIntrinsics) {\n-      if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dexp)) {\n-        StubRoutines::_dexp = generate_libmExp();\n-      }\n-      if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dlog)) {\n-        StubRoutines::_dlog = generate_libmLog();\n-      }\n-      if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dlog10)) {\n-        StubRoutines::_dlog10 = generate_libmLog10();\n-      }\n-      if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dpow)) {\n-        StubRoutines::_dpow = generate_libmPow();\n-      }\n-      if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dsin)) {\n-        StubRoutines::_dsin = generate_libmSin();\n-      }\n-      if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dcos)) {\n-        StubRoutines::_dcos = generate_libmCos();\n-      }\n-      if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dtan)) {\n-        StubRoutines::_dtan = generate_libmTan();\n-      }\n+    if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dpow)) {\n+      StubRoutines::_dpow = generate_libmPow();\n+    }\n+    if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dsin)) {\n+      StubRoutines::_dsin = generate_libmSin();\n+    }\n+    if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dcos)) {\n+      StubRoutines::_dcos = generate_libmCos();\n+    }\n+    if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dtan)) {\n+      StubRoutines::_dtan = generate_libmTan();\n@@ -7858,0 +7899,1 @@\n+}\n@@ -7859,68 +7901,73 @@\n-  void generate_phase1() {\n-    \/\/ Continuation stubs:\n-    StubRoutines::_cont_thaw          = generate_cont_thaw();\n-    StubRoutines::_cont_returnBarrier = generate_cont_returnBarrier();\n-    StubRoutines::_cont_returnBarrierExc = generate_cont_returnBarrier_exception();\n-    StubRoutines::_cont_doYield_stub = generate_cont_doYield();\n-    StubRoutines::_cont_doYield      = StubRoutines::_cont_doYield_stub == nullptr ? nullptr\n-                                        : StubRoutines::_cont_doYield_stub->entry_point();\n-\n-    JFR_ONLY(StubRoutines::_jfr_write_checkpoint_stub = generate_jfr_write_checkpoint();)\n-    JFR_ONLY(StubRoutines::_jfr_write_checkpoint = StubRoutines::_jfr_write_checkpoint_stub->entry_point();)\n-  }\n-\n-  void generate_all() {\n-    \/\/ Generates all stubs and initializes the entry points\n-\n-    \/\/ These entry points require SharedInfo::stack0 to be set up in\n-    \/\/ non-core builds and need to be relocatable, so they each\n-    \/\/ fabricate a RuntimeStub internally.\n-    StubRoutines::_throw_AbstractMethodError_entry =\n-      generate_throw_exception(\"AbstractMethodError throw_exception\",\n-                               CAST_FROM_FN_PTR(address,\n-                                                SharedRuntime::\n-                                                throw_AbstractMethodError));\n-\n-    StubRoutines::_throw_IncompatibleClassChangeError_entry =\n-      generate_throw_exception(\"IncompatibleClassChangeError throw_exception\",\n-                               CAST_FROM_FN_PTR(address,\n-                                                SharedRuntime::\n-                                                throw_IncompatibleClassChangeError));\n-\n-    StubRoutines::_throw_NullPointerException_at_call_entry =\n-      generate_throw_exception(\"NullPointerException at call throw_exception\",\n-                               CAST_FROM_FN_PTR(address,\n-                                                SharedRuntime::\n-                                                throw_NullPointerException_at_call));\n-\n-    \/\/ entry points that are platform specific\n-    StubRoutines::x86::_vector_float_sign_mask = generate_vector_mask(\"vector_float_sign_mask\", 0x7FFFFFFF7FFFFFFF);\n-    StubRoutines::x86::_vector_float_sign_flip = generate_vector_mask(\"vector_float_sign_flip\", 0x8000000080000000);\n-    StubRoutines::x86::_vector_double_sign_mask = generate_vector_mask(\"vector_double_sign_mask\", 0x7FFFFFFFFFFFFFFF);\n-    StubRoutines::x86::_vector_double_sign_flip = generate_vector_mask(\"vector_double_sign_flip\", 0x8000000000000000);\n-    StubRoutines::x86::_vector_all_bits_set = generate_vector_mask(\"vector_all_bits_set\", 0xFFFFFFFFFFFFFFFF);\n-    StubRoutines::x86::_vector_int_mask_cmp_bits = generate_vector_mask(\"vector_int_mask_cmp_bits\", 0x0000000100000001);\n-    StubRoutines::x86::_vector_short_to_byte_mask = generate_vector_mask(\"vector_short_to_byte_mask\", 0x00ff00ff00ff00ff);\n-    StubRoutines::x86::_vector_byte_perm_mask = generate_vector_byte_perm_mask(\"vector_byte_perm_mask\");\n-    StubRoutines::x86::_vector_int_to_byte_mask = generate_vector_mask(\"vector_int_to_byte_mask\", 0x000000ff000000ff);\n-    StubRoutines::x86::_vector_int_to_short_mask = generate_vector_mask(\"vector_int_to_short_mask\", 0x0000ffff0000ffff);\n-    StubRoutines::x86::_vector_32_bit_mask = generate_vector_custom_i32(\"vector_32_bit_mask\", Assembler::AVX_512bit,\n-                                                                        0xFFFFFFFF, 0, 0, 0);\n-    StubRoutines::x86::_vector_64_bit_mask = generate_vector_custom_i32(\"vector_64_bit_mask\", Assembler::AVX_512bit,\n-                                                                        0xFFFFFFFF, 0xFFFFFFFF, 0, 0);\n-    StubRoutines::x86::_vector_int_shuffle_mask = generate_vector_mask(\"vector_int_shuffle_mask\", 0x0302010003020100);\n-    StubRoutines::x86::_vector_byte_shuffle_mask = generate_vector_byte_shuffle_mask(\"vector_byte_shuffle_mask\");\n-    StubRoutines::x86::_vector_short_shuffle_mask = generate_vector_mask(\"vector_short_shuffle_mask\", 0x0100010001000100);\n-    StubRoutines::x86::_vector_long_shuffle_mask = generate_vector_mask(\"vector_long_shuffle_mask\", 0x0000000100000000);\n-    StubRoutines::x86::_vector_long_sign_mask = generate_vector_mask(\"vector_long_sign_mask\", 0x8000000000000000);\n-    StubRoutines::x86::_vector_iota_indices = generate_iota_indices(\"iota_indices\");\n-    StubRoutines::x86::_vector_count_leading_zeros_lut = generate_count_leading_zeros_lut(\"count_leading_zeros_lut\");\n-    StubRoutines::x86::_vector_reverse_bit_lut = generate_vector_reverse_bit_lut(\"reverse_bit_lut\");\n-    StubRoutines::x86::_vector_reverse_byte_perm_mask_long = generate_vector_reverse_byte_perm_mask_long(\"perm_mask_long\");\n-    StubRoutines::x86::_vector_reverse_byte_perm_mask_int = generate_vector_reverse_byte_perm_mask_int(\"perm_mask_int\");\n-    StubRoutines::x86::_vector_reverse_byte_perm_mask_short = generate_vector_reverse_byte_perm_mask_short(\"perm_mask_short\");\n-\n-    if (VM_Version::supports_avx2() && !VM_Version::supports_avx512_vpopcntdq()) {\n-      \/\/ lut implementation influenced by counting 1s algorithm from section 5-1 of Hackers' Delight.\n-      StubRoutines::x86::_vector_popcount_lut = generate_popcount_avx_lut(\"popcount_lut\");\n-    }\n+void StubGenerator::generate_phase1() {\n+  \/\/ Continuation stubs:\n+  StubRoutines::_cont_thaw          = generate_cont_thaw();\n+  StubRoutines::_cont_returnBarrier = generate_cont_returnBarrier();\n+  StubRoutines::_cont_returnBarrierExc = generate_cont_returnBarrier_exception();\n+  StubRoutines::_cont_doYield_stub = generate_cont_doYield();\n+  StubRoutines::_cont_doYield      = StubRoutines::_cont_doYield_stub == nullptr ? nullptr\n+                                      : StubRoutines::_cont_doYield_stub->entry_point();\n+\n+  JFR_ONLY(StubRoutines::_jfr_write_checkpoint_stub = generate_jfr_write_checkpoint();)\n+  JFR_ONLY(StubRoutines::_jfr_write_checkpoint = StubRoutines::_jfr_write_checkpoint_stub->entry_point();)\n+}\n+\n+void StubGenerator::generate_all() {\n+  \/\/ Generates all stubs and initializes the entry points\n+\n+  \/\/ These entry points require SharedInfo::stack0 to be set up in\n+  \/\/ non-core builds and need to be relocatable, so they each\n+  \/\/ fabricate a RuntimeStub internally.\n+  StubRoutines::_throw_AbstractMethodError_entry =\n+    generate_throw_exception(\"AbstractMethodError throw_exception\",\n+                             CAST_FROM_FN_PTR(address,\n+                                              SharedRuntime::\n+                                              throw_AbstractMethodError));\n+\n+  StubRoutines::_throw_IncompatibleClassChangeError_entry =\n+    generate_throw_exception(\"IncompatibleClassChangeError throw_exception\",\n+                             CAST_FROM_FN_PTR(address,\n+                                              SharedRuntime::\n+                                              throw_IncompatibleClassChangeError));\n+\n+  StubRoutines::_throw_NullPointerException_at_call_entry =\n+    generate_throw_exception(\"NullPointerException at call throw_exception\",\n+                             CAST_FROM_FN_PTR(address,\n+                                              SharedRuntime::\n+                                              throw_NullPointerException_at_call));\n+\n+  \/\/ entry points that are platform specific\n+  StubRoutines::x86::_vector_float_sign_mask = generate_vector_mask(\"vector_float_sign_mask\", 0x7FFFFFFF7FFFFFFF);\n+  StubRoutines::x86::_vector_float_sign_flip = generate_vector_mask(\"vector_float_sign_flip\", 0x8000000080000000);\n+  StubRoutines::x86::_vector_double_sign_mask = generate_vector_mask(\"vector_double_sign_mask\", 0x7FFFFFFFFFFFFFFF);\n+  StubRoutines::x86::_vector_double_sign_flip = generate_vector_mask(\"vector_double_sign_flip\", 0x8000000000000000);\n+  StubRoutines::x86::_vector_all_bits_set = generate_vector_mask(\"vector_all_bits_set\", 0xFFFFFFFFFFFFFFFF);\n+  StubRoutines::x86::_vector_int_mask_cmp_bits = generate_vector_mask(\"vector_int_mask_cmp_bits\", 0x0000000100000001);\n+  StubRoutines::x86::_vector_short_to_byte_mask = generate_vector_mask(\"vector_short_to_byte_mask\", 0x00ff00ff00ff00ff);\n+  StubRoutines::x86::_vector_byte_perm_mask = generate_vector_byte_perm_mask(\"vector_byte_perm_mask\");\n+  StubRoutines::x86::_vector_int_to_byte_mask = generate_vector_mask(\"vector_int_to_byte_mask\", 0x000000ff000000ff);\n+  StubRoutines::x86::_vector_int_to_short_mask = generate_vector_mask(\"vector_int_to_short_mask\", 0x0000ffff0000ffff);\n+  StubRoutines::x86::_vector_32_bit_mask = generate_vector_custom_i32(\"vector_32_bit_mask\", Assembler::AVX_512bit,\n+                                                                      0xFFFFFFFF, 0, 0, 0);\n+  StubRoutines::x86::_vector_64_bit_mask = generate_vector_custom_i32(\"vector_64_bit_mask\", Assembler::AVX_512bit,\n+                                                                      0xFFFFFFFF, 0xFFFFFFFF, 0, 0);\n+  StubRoutines::x86::_vector_int_shuffle_mask = generate_vector_mask(\"vector_int_shuffle_mask\", 0x0302010003020100);\n+  StubRoutines::x86::_vector_byte_shuffle_mask = generate_vector_byte_shuffle_mask(\"vector_byte_shuffle_mask\");\n+  StubRoutines::x86::_vector_short_shuffle_mask = generate_vector_mask(\"vector_short_shuffle_mask\", 0x0100010001000100);\n+  StubRoutines::x86::_vector_long_shuffle_mask = generate_vector_mask(\"vector_long_shuffle_mask\", 0x0000000100000000);\n+  StubRoutines::x86::_vector_long_sign_mask = generate_vector_mask(\"vector_long_sign_mask\", 0x8000000000000000);\n+  StubRoutines::x86::_vector_iota_indices = generate_iota_indices(\"iota_indices\");\n+  StubRoutines::x86::_vector_count_leading_zeros_lut = generate_count_leading_zeros_lut(\"count_leading_zeros_lut\");\n+  StubRoutines::x86::_vector_reverse_bit_lut = generate_vector_reverse_bit_lut(\"reverse_bit_lut\");\n+  StubRoutines::x86::_vector_reverse_byte_perm_mask_long = generate_vector_reverse_byte_perm_mask_long(\"perm_mask_long\");\n+  StubRoutines::x86::_vector_reverse_byte_perm_mask_int = generate_vector_reverse_byte_perm_mask_int(\"perm_mask_int\");\n+  StubRoutines::x86::_vector_reverse_byte_perm_mask_short = generate_vector_reverse_byte_perm_mask_short(\"perm_mask_short\");\n+\n+  if (VM_Version::supports_avx2() && !VM_Version::supports_avx512_vpopcntdq()) {\n+    \/\/ lut implementation influenced by counting 1s algorithm from section 5-1 of Hackers' Delight.\n+    StubRoutines::x86::_vector_popcount_lut = generate_popcount_avx_lut(\"popcount_lut\");\n+  }\n+\n+  \/\/ support for verify_oop (must happen after universe_init)\n+  if (VerifyOops) {\n+    StubRoutines::_verify_oop_subroutine_entry = generate_verify_oop();\n+  }\n@@ -7928,3 +7975,23 @@\n-    \/\/ support for verify_oop (must happen after universe_init)\n-    if (VerifyOops) {\n-      StubRoutines::_verify_oop_subroutine_entry = generate_verify_oop();\n+  \/\/ data cache line writeback\n+  StubRoutines::_data_cache_writeback = generate_data_cache_writeback();\n+  StubRoutines::_data_cache_writeback_sync = generate_data_cache_writeback_sync();\n+\n+  \/\/ arraycopy stubs used by compilers\n+  generate_arraycopy_stubs();\n+\n+  \/\/ don't bother generating these AES intrinsic stubs unless global flag is set\n+  if (UseAESIntrinsics) {\n+    StubRoutines::x86::_key_shuffle_mask_addr = generate_key_shuffle_mask();  \/\/ needed by the others\n+    StubRoutines::_aescrypt_encryptBlock = generate_aescrypt_encryptBlock();\n+    StubRoutines::_aescrypt_decryptBlock = generate_aescrypt_decryptBlock();\n+    StubRoutines::_cipherBlockChaining_encryptAESCrypt = generate_cipherBlockChaining_encryptAESCrypt();\n+    if (VM_Version::supports_avx512_vaes() &&  VM_Version::supports_avx512vl() && VM_Version::supports_avx512dq() ) {\n+      StubRoutines::_cipherBlockChaining_decryptAESCrypt = generate_cipherBlockChaining_decryptVectorAESCrypt();\n+      StubRoutines::_electronicCodeBook_encryptAESCrypt = generate_electronicCodeBook_encryptAESCrypt();\n+      StubRoutines::_electronicCodeBook_decryptAESCrypt = generate_electronicCodeBook_decryptAESCrypt();\n+      StubRoutines::x86::_counter_mask_addr = counter_mask_addr();\n+      StubRoutines::x86::_ghash_poly512_addr = ghash_polynomial512_addr();\n+      StubRoutines::x86::_ghash_long_swap_mask_addr = generate_ghash_long_swap_mask();\n+      StubRoutines::_galoisCounterMode_AESCrypt = generate_galoisCounterMode_AESCrypt();\n+    } else {\n+      StubRoutines::_cipherBlockChaining_decryptAESCrypt = generate_cipherBlockChaining_decryptAESCrypt_Parallel();\n@@ -7932,0 +7999,1 @@\n+  }\n@@ -7933,17 +8001,3 @@\n-    \/\/ data cache line writeback\n-    StubRoutines::_data_cache_writeback = generate_data_cache_writeback();\n-    StubRoutines::_data_cache_writeback_sync = generate_data_cache_writeback_sync();\n-\n-    \/\/ arraycopy stubs used by compilers\n-    generate_arraycopy_stubs();\n-\n-    \/\/ don't bother generating these AES intrinsic stubs unless global flag is set\n-    if (UseAESIntrinsics) {\n-      StubRoutines::x86::_key_shuffle_mask_addr = generate_key_shuffle_mask();  \/\/ needed by the others\n-      StubRoutines::_aescrypt_encryptBlock = generate_aescrypt_encryptBlock();\n-      StubRoutines::_aescrypt_decryptBlock = generate_aescrypt_decryptBlock();\n-      StubRoutines::_cipherBlockChaining_encryptAESCrypt = generate_cipherBlockChaining_encryptAESCrypt();\n-      if (VM_Version::supports_avx512_vaes() &&  VM_Version::supports_avx512vl() && VM_Version::supports_avx512dq() ) {\n-        StubRoutines::_cipherBlockChaining_decryptAESCrypt = generate_cipherBlockChaining_decryptVectorAESCrypt();\n-        StubRoutines::_electronicCodeBook_encryptAESCrypt = generate_electronicCodeBook_encryptAESCrypt();\n-        StubRoutines::_electronicCodeBook_decryptAESCrypt = generate_electronicCodeBook_decryptAESCrypt();\n+  if (UseAESCTRIntrinsics) {\n+    if (VM_Version::supports_avx512_vaes() && VM_Version::supports_avx512bw() && VM_Version::supports_avx512vl()) {\n+      if (StubRoutines::x86::_counter_mask_addr == NULL) {\n@@ -7951,5 +8005,0 @@\n-        StubRoutines::x86::_ghash_poly512_addr = ghash_polynomial512_addr();\n-        StubRoutines::x86::_ghash_long_swap_mask_addr = generate_ghash_long_swap_mask();\n-        StubRoutines::_galoisCounterMode_AESCrypt = generate_galoisCounterMode_AESCrypt();\n-      } else {\n-        StubRoutines::_cipherBlockChaining_decryptAESCrypt = generate_cipherBlockChaining_decryptAESCrypt_Parallel();\n@@ -7957,0 +8006,4 @@\n+      StubRoutines::_counterMode_AESCrypt = generate_counterMode_VectorAESCrypt();\n+    } else {\n+      StubRoutines::x86::_counter_shuffle_mask_addr = generate_counter_shuffle_mask();\n+      StubRoutines::_counterMode_AESCrypt = generate_counterMode_AESCrypt_Parallel();\n@@ -7958,0 +8011,1 @@\n+  }\n@@ -7959,11 +8013,29 @@\n-    if (UseAESCTRIntrinsics) {\n-      if (VM_Version::supports_avx512_vaes() && VM_Version::supports_avx512bw() && VM_Version::supports_avx512vl()) {\n-        if (StubRoutines::x86::_counter_mask_addr == NULL) {\n-          StubRoutines::x86::_counter_mask_addr = counter_mask_addr();\n-        }\n-        StubRoutines::_counterMode_AESCrypt = generate_counterMode_VectorAESCrypt();\n-      } else {\n-        StubRoutines::x86::_counter_shuffle_mask_addr = generate_counter_shuffle_mask();\n-        StubRoutines::_counterMode_AESCrypt = generate_counterMode_AESCrypt_Parallel();\n-      }\n-    }\n+  if (UseMD5Intrinsics) {\n+    StubRoutines::_md5_implCompress = generate_md5_implCompress(false, \"md5_implCompress\");\n+    StubRoutines::_md5_implCompressMB = generate_md5_implCompress(true, \"md5_implCompressMB\");\n+  }\n+  if (UseSHA1Intrinsics) {\n+    StubRoutines::x86::_upper_word_mask_addr = generate_upper_word_mask();\n+    StubRoutines::x86::_shuffle_byte_flip_mask_addr = generate_shuffle_byte_flip_mask();\n+    StubRoutines::_sha1_implCompress = generate_sha1_implCompress(false, \"sha1_implCompress\");\n+    StubRoutines::_sha1_implCompressMB = generate_sha1_implCompress(true, \"sha1_implCompressMB\");\n+  }\n+  if (UseSHA256Intrinsics) {\n+    StubRoutines::x86::_k256_adr = (address)StubRoutines::x86::_k256;\n+    char* dst = (char*)StubRoutines::x86::_k256_W;\n+    char* src = (char*)StubRoutines::x86::_k256;\n+    for (int ii = 0; ii < 16; ++ii) {\n+      memcpy(dst + 32 * ii,      src + 16 * ii, 16);\n+      memcpy(dst + 32 * ii + 16, src + 16 * ii, 16);\n+    }\n+    StubRoutines::x86::_k256_W_adr = (address)StubRoutines::x86::_k256_W;\n+    StubRoutines::x86::_pshuffle_byte_flip_mask_addr = generate_pshuffle_byte_flip_mask();\n+    StubRoutines::_sha256_implCompress = generate_sha256_implCompress(false, \"sha256_implCompress\");\n+    StubRoutines::_sha256_implCompressMB = generate_sha256_implCompress(true, \"sha256_implCompressMB\");\n+  }\n+  if (UseSHA512Intrinsics) {\n+    StubRoutines::x86::_k512_W_addr = (address)StubRoutines::x86::_k512_W;\n+    StubRoutines::x86::_pshuffle_byte_flip_mask_addr_sha512 = generate_pshuffle_byte_flip_mask_sha512();\n+    StubRoutines::_sha512_implCompress = generate_sha512_implCompress(false, \"sha512_implCompress\");\n+    StubRoutines::_sha512_implCompressMB = generate_sha512_implCompress(true, \"sha512_implCompressMB\");\n+  }\n@@ -7971,22 +8043,4 @@\n-    if (UseMD5Intrinsics) {\n-      StubRoutines::_md5_implCompress = generate_md5_implCompress(false, \"md5_implCompress\");\n-      StubRoutines::_md5_implCompressMB = generate_md5_implCompress(true, \"md5_implCompressMB\");\n-    }\n-    if (UseSHA1Intrinsics) {\n-      StubRoutines::x86::_upper_word_mask_addr = generate_upper_word_mask();\n-      StubRoutines::x86::_shuffle_byte_flip_mask_addr = generate_shuffle_byte_flip_mask();\n-      StubRoutines::_sha1_implCompress = generate_sha1_implCompress(false, \"sha1_implCompress\");\n-      StubRoutines::_sha1_implCompressMB = generate_sha1_implCompress(true, \"sha1_implCompressMB\");\n-    }\n-    if (UseSHA256Intrinsics) {\n-      StubRoutines::x86::_k256_adr = (address)StubRoutines::x86::_k256;\n-      char* dst = (char*)StubRoutines::x86::_k256_W;\n-      char* src = (char*)StubRoutines::x86::_k256;\n-      for (int ii = 0; ii < 16; ++ii) {\n-        memcpy(dst + 32 * ii,      src + 16 * ii, 16);\n-        memcpy(dst + 32 * ii + 16, src + 16 * ii, 16);\n-      }\n-      StubRoutines::x86::_k256_W_adr = (address)StubRoutines::x86::_k256_W;\n-      StubRoutines::x86::_pshuffle_byte_flip_mask_addr = generate_pshuffle_byte_flip_mask();\n-      StubRoutines::_sha256_implCompress = generate_sha256_implCompress(false, \"sha256_implCompress\");\n-      StubRoutines::_sha256_implCompressMB = generate_sha256_implCompress(true, \"sha256_implCompressMB\");\n+  \/\/ Generate GHASH intrinsics code\n+  if (UseGHASHIntrinsics) {\n+    if (StubRoutines::x86::_ghash_long_swap_mask_addr == NULL) {\n+      StubRoutines::x86::_ghash_long_swap_mask_addr = generate_ghash_long_swap_mask();\n@@ -7994,20 +8048,7 @@\n-    if (UseSHA512Intrinsics) {\n-      StubRoutines::x86::_k512_W_addr = (address)StubRoutines::x86::_k512_W;\n-      StubRoutines::x86::_pshuffle_byte_flip_mask_addr_sha512 = generate_pshuffle_byte_flip_mask_sha512();\n-      StubRoutines::_sha512_implCompress = generate_sha512_implCompress(false, \"sha512_implCompress\");\n-      StubRoutines::_sha512_implCompressMB = generate_sha512_implCompress(true, \"sha512_implCompressMB\");\n-    }\n-\n-    \/\/ Generate GHASH intrinsics code\n-    if (UseGHASHIntrinsics) {\n-      if (StubRoutines::x86::_ghash_long_swap_mask_addr == NULL) {\n-        StubRoutines::x86::_ghash_long_swap_mask_addr = generate_ghash_long_swap_mask();\n-      }\n-    StubRoutines::x86::_ghash_byte_swap_mask_addr = generate_ghash_byte_swap_mask();\n-      if (VM_Version::supports_avx()) {\n-        StubRoutines::x86::_ghash_shuffmask_addr = ghash_shufflemask_addr();\n-        StubRoutines::x86::_ghash_poly_addr = ghash_polynomial_addr();\n-        StubRoutines::_ghash_processBlocks = generate_avx_ghash_processBlocks();\n-      } else {\n-        StubRoutines::_ghash_processBlocks = generate_ghash_processBlocks();\n-      }\n+  StubRoutines::x86::_ghash_byte_swap_mask_addr = generate_ghash_byte_swap_mask();\n+    if (VM_Version::supports_avx()) {\n+      StubRoutines::x86::_ghash_shuffmask_addr = ghash_shufflemask_addr();\n+      StubRoutines::x86::_ghash_poly_addr = ghash_polynomial_addr();\n+      StubRoutines::_ghash_processBlocks = generate_avx_ghash_processBlocks();\n+    } else {\n+      StubRoutines::_ghash_processBlocks = generate_ghash_processBlocks();\n@@ -8015,0 +8056,1 @@\n+  }\n@@ -8017,23 +8059,7 @@\n-    if (UseBASE64Intrinsics) {\n-      if(VM_Version::supports_avx2() &&\n-         VM_Version::supports_avx512bw() &&\n-         VM_Version::supports_avx512vl()) {\n-        StubRoutines::x86::_avx2_shuffle_base64 = base64_avx2_shuffle_addr();\n-        StubRoutines::x86::_avx2_input_mask_base64 = base64_avx2_input_mask_addr();\n-        StubRoutines::x86::_avx2_lut_base64 = base64_avx2_lut_addr();\n-      }\n-      StubRoutines::x86::_encoding_table_base64 = base64_encoding_table_addr();\n-      if (VM_Version::supports_avx512_vbmi()) {\n-        StubRoutines::x86::_shuffle_base64 = base64_shuffle_addr();\n-        StubRoutines::x86::_lookup_lo_base64 = base64_vbmi_lookup_lo_addr();\n-        StubRoutines::x86::_lookup_hi_base64 = base64_vbmi_lookup_hi_addr();\n-        StubRoutines::x86::_lookup_lo_base64url = base64_vbmi_lookup_lo_url_addr();\n-        StubRoutines::x86::_lookup_hi_base64url = base64_vbmi_lookup_hi_url_addr();\n-        StubRoutines::x86::_pack_vec_base64 = base64_vbmi_pack_vec_addr();\n-        StubRoutines::x86::_join_0_1_base64 = base64_vbmi_join_0_1_addr();\n-        StubRoutines::x86::_join_1_2_base64 = base64_vbmi_join_1_2_addr();\n-        StubRoutines::x86::_join_2_3_base64 = base64_vbmi_join_2_3_addr();\n-      }\n-      StubRoutines::x86::_decoding_table_base64 = base64_decoding_table_addr();\n-      StubRoutines::_base64_encodeBlock = generate_base64_encodeBlock();\n-      StubRoutines::_base64_decodeBlock = generate_base64_decodeBlock();\n+  if (UseBASE64Intrinsics) {\n+    if(VM_Version::supports_avx2() &&\n+       VM_Version::supports_avx512bw() &&\n+       VM_Version::supports_avx512vl()) {\n+      StubRoutines::x86::_avx2_shuffle_base64 = base64_avx2_shuffle_addr();\n+      StubRoutines::x86::_avx2_input_mask_base64 = base64_avx2_input_mask_addr();\n+      StubRoutines::x86::_avx2_lut_base64 = base64_avx2_lut_addr();\n@@ -8041,0 +8067,16 @@\n+    StubRoutines::x86::_encoding_table_base64 = base64_encoding_table_addr();\n+    if (VM_Version::supports_avx512_vbmi()) {\n+      StubRoutines::x86::_shuffle_base64 = base64_shuffle_addr();\n+      StubRoutines::x86::_lookup_lo_base64 = base64_vbmi_lookup_lo_addr();\n+      StubRoutines::x86::_lookup_hi_base64 = base64_vbmi_lookup_hi_addr();\n+      StubRoutines::x86::_lookup_lo_base64url = base64_vbmi_lookup_lo_url_addr();\n+      StubRoutines::x86::_lookup_hi_base64url = base64_vbmi_lookup_hi_url_addr();\n+      StubRoutines::x86::_pack_vec_base64 = base64_vbmi_pack_vec_addr();\n+      StubRoutines::x86::_join_0_1_base64 = base64_vbmi_join_0_1_addr();\n+      StubRoutines::x86::_join_1_2_base64 = base64_vbmi_join_1_2_addr();\n+      StubRoutines::x86::_join_2_3_base64 = base64_vbmi_join_2_3_addr();\n+    }\n+    StubRoutines::x86::_decoding_table_base64 = base64_decoding_table_addr();\n+    StubRoutines::_base64_encodeBlock = generate_base64_encodeBlock();\n+    StubRoutines::_base64_decodeBlock = generate_base64_decodeBlock();\n+  }\n@@ -8042,4 +8084,4 @@\n-    BarrierSetNMethod* bs_nm = BarrierSet::barrier_set()->barrier_set_nmethod();\n-    if (bs_nm != NULL) {\n-      StubRoutines::x86::_method_entry_barrier = generate_method_entry_barrier();\n-    }\n+  BarrierSetNMethod* bs_nm = BarrierSet::barrier_set()->barrier_set_nmethod();\n+  if (bs_nm != NULL) {\n+    StubRoutines::x86::_method_entry_barrier = generate_method_entry_barrier();\n+  }\n@@ -8047,21 +8089,21 @@\n-    if (UseMultiplyToLenIntrinsic) {\n-      StubRoutines::_multiplyToLen = generate_multiplyToLen();\n-    }\n-    if (UseSquareToLenIntrinsic) {\n-      StubRoutines::_squareToLen = generate_squareToLen();\n-    }\n-    if (UseMulAddIntrinsic) {\n-      StubRoutines::_mulAdd = generate_mulAdd();\n-    }\n-    if (VM_Version::supports_avx512_vbmi2()) {\n-      StubRoutines::_bigIntegerRightShiftWorker = generate_bigIntegerRightShift();\n-      StubRoutines::_bigIntegerLeftShiftWorker = generate_bigIntegerLeftShift();\n-    }\n-    if (UseMontgomeryMultiplyIntrinsic) {\n-      StubRoutines::_montgomeryMultiply\n-        = CAST_FROM_FN_PTR(address, SharedRuntime::montgomery_multiply);\n-    }\n-    if (UseMontgomerySquareIntrinsic) {\n-      StubRoutines::_montgomerySquare\n-        = CAST_FROM_FN_PTR(address, SharedRuntime::montgomery_square);\n-    }\n+  if (UseMultiplyToLenIntrinsic) {\n+    StubRoutines::_multiplyToLen = generate_multiplyToLen();\n+  }\n+  if (UseSquareToLenIntrinsic) {\n+    StubRoutines::_squareToLen = generate_squareToLen();\n+  }\n+  if (UseMulAddIntrinsic) {\n+    StubRoutines::_mulAdd = generate_mulAdd();\n+  }\n+  if (VM_Version::supports_avx512_vbmi2()) {\n+    StubRoutines::_bigIntegerRightShiftWorker = generate_bigIntegerRightShift();\n+    StubRoutines::_bigIntegerLeftShiftWorker = generate_bigIntegerLeftShift();\n+  }\n+  if (UseMontgomeryMultiplyIntrinsic) {\n+    StubRoutines::_montgomeryMultiply\n+      = CAST_FROM_FN_PTR(address, SharedRuntime::montgomery_multiply);\n+  }\n+  if (UseMontgomerySquareIntrinsic) {\n+    StubRoutines::_montgomerySquare\n+      = CAST_FROM_FN_PTR(address, SharedRuntime::montgomery_square);\n+  }\n@@ -8069,39 +8111,25 @@\n-    \/\/ Get svml stub routine addresses\n-    void *libjsvml = NULL;\n-    char ebuf[1024];\n-    char dll_name[JVM_MAXPATHLEN];\n-    if (os::dll_locate_lib(dll_name, sizeof(dll_name), Arguments::get_dll_dir(), \"jsvml\")) {\n-      libjsvml = os::dll_load(dll_name, ebuf, sizeof ebuf);\n-    }\n-    if (libjsvml != NULL) {\n-      \/\/ SVML method naming convention\n-      \/\/   All the methods are named as __jsvml_op<T><N>_ha_<VV>\n-      \/\/   Where:\n-      \/\/      ha stands for high accuracy\n-      \/\/      <T> is optional to indicate float\/double\n-      \/\/              Set to f for vector float operation\n-      \/\/              Omitted for vector double operation\n-      \/\/      <N> is the number of elements in the vector\n-      \/\/              1, 2, 4, 8, 16\n-      \/\/              e.g. 128 bit float vector has 4 float elements\n-      \/\/      <VV> indicates the avx\/sse level:\n-      \/\/              z0 is AVX512, l9 is AVX2, e9 is AVX1 and ex is for SSE2\n-      \/\/      e.g. __jsvml_expf16_ha_z0 is the method for computing 16 element vector float exp using AVX 512 insns\n-      \/\/           __jsvml_exp8_ha_z0 is the method for computing 8 element vector double exp using AVX 512 insns\n-\n-      log_info(library)(\"Loaded library %s, handle \" INTPTR_FORMAT, JNI_LIB_PREFIX \"jsvml\" JNI_LIB_SUFFIX, p2i(libjsvml));\n-      if (UseAVX > 2) {\n-        for (int op = 0; op < VectorSupport::NUM_SVML_OP; op++) {\n-          int vop = VectorSupport::VECTOR_OP_SVML_START + op;\n-          if ((!VM_Version::supports_avx512dq()) &&\n-              (vop == VectorSupport::VECTOR_OP_LOG || vop == VectorSupport::VECTOR_OP_LOG10 || vop == VectorSupport::VECTOR_OP_POW)) {\n-            continue;\n-          }\n-          snprintf(ebuf, sizeof(ebuf), \"__jsvml_%sf16_ha_z0\", VectorSupport::svmlname[op]);\n-          StubRoutines::_vector_f_math[VectorSupport::VEC_SIZE_512][op] = (address)os::dll_lookup(libjsvml, ebuf);\n-\n-          snprintf(ebuf, sizeof(ebuf), \"__jsvml_%s8_ha_z0\", VectorSupport::svmlname[op]);\n-          StubRoutines::_vector_d_math[VectorSupport::VEC_SIZE_512][op] = (address)os::dll_lookup(libjsvml, ebuf);\n-        }\n-      }\n-      const char* avx_sse_str = (UseAVX >= 2) ? \"l9\" : ((UseAVX == 1) ? \"e9\" : \"ex\");\n+  \/\/ Get svml stub routine addresses\n+  void *libjsvml = NULL;\n+  char ebuf[1024];\n+  char dll_name[JVM_MAXPATHLEN];\n+  if (os::dll_locate_lib(dll_name, sizeof(dll_name), Arguments::get_dll_dir(), \"jsvml\")) {\n+    libjsvml = os::dll_load(dll_name, ebuf, sizeof ebuf);\n+  }\n+  if (libjsvml != NULL) {\n+    \/\/ SVML method naming convention\n+    \/\/   All the methods are named as __jsvml_op<T><N>_ha_<VV>\n+    \/\/   Where:\n+    \/\/      ha stands for high accuracy\n+    \/\/      <T> is optional to indicate float\/double\n+    \/\/              Set to f for vector float operation\n+    \/\/              Omitted for vector double operation\n+    \/\/      <N> is the number of elements in the vector\n+    \/\/              1, 2, 4, 8, 16\n+    \/\/              e.g. 128 bit float vector has 4 float elements\n+    \/\/      <VV> indicates the avx\/sse level:\n+    \/\/              z0 is AVX512, l9 is AVX2, e9 is AVX1 and ex is for SSE2\n+    \/\/      e.g. __jsvml_expf16_ha_z0 is the method for computing 16 element vector float exp using AVX 512 insns\n+    \/\/           __jsvml_exp8_ha_z0 is the method for computing 8 element vector double exp using AVX 512 insns\n+\n+    log_info(library)(\"Loaded library %s, handle \" INTPTR_FORMAT, JNI_LIB_PREFIX \"jsvml\" JNI_LIB_SUFFIX, p2i(libjsvml));\n+    if (UseAVX > 2) {\n@@ -8110,1 +8138,2 @@\n-        if (vop == VectorSupport::VECTOR_OP_POW) {\n+        if ((!VM_Version::supports_avx512dq()) &&\n+            (vop == VectorSupport::VECTOR_OP_LOG || vop == VectorSupport::VECTOR_OP_LOG10 || vop == VectorSupport::VECTOR_OP_POW)) {\n@@ -8113,2 +8142,2 @@\n-        snprintf(ebuf, sizeof(ebuf), \"__jsvml_%sf4_ha_%s\", VectorSupport::svmlname[op], avx_sse_str);\n-        StubRoutines::_vector_f_math[VectorSupport::VEC_SIZE_64][op] = (address)os::dll_lookup(libjsvml, ebuf);\n+        snprintf(ebuf, sizeof(ebuf), \"__jsvml_%sf16_ha_z0\", VectorSupport::svmlname[op]);\n+        StubRoutines::_vector_f_math[VectorSupport::VEC_SIZE_512][op] = (address)os::dll_lookup(libjsvml, ebuf);\n@@ -8116,2 +8145,12 @@\n-        snprintf(ebuf, sizeof(ebuf), \"__jsvml_%sf4_ha_%s\", VectorSupport::svmlname[op], avx_sse_str);\n-        StubRoutines::_vector_f_math[VectorSupport::VEC_SIZE_128][op] = (address)os::dll_lookup(libjsvml, ebuf);\n+        snprintf(ebuf, sizeof(ebuf), \"__jsvml_%s8_ha_z0\", VectorSupport::svmlname[op]);\n+        StubRoutines::_vector_d_math[VectorSupport::VEC_SIZE_512][op] = (address)os::dll_lookup(libjsvml, ebuf);\n+      }\n+    }\n+    const char* avx_sse_str = (UseAVX >= 2) ? \"l9\" : ((UseAVX == 1) ? \"e9\" : \"ex\");\n+    for (int op = 0; op < VectorSupport::NUM_SVML_OP; op++) {\n+      int vop = VectorSupport::VECTOR_OP_SVML_START + op;\n+      if (vop == VectorSupport::VECTOR_OP_POW) {\n+        continue;\n+      }\n+      snprintf(ebuf, sizeof(ebuf), \"__jsvml_%sf4_ha_%s\", VectorSupport::svmlname[op], avx_sse_str);\n+      StubRoutines::_vector_f_math[VectorSupport::VEC_SIZE_64][op] = (address)os::dll_lookup(libjsvml, ebuf);\n@@ -8119,2 +8158,2 @@\n-        snprintf(ebuf, sizeof(ebuf), \"__jsvml_%sf8_ha_%s\", VectorSupport::svmlname[op], avx_sse_str);\n-        StubRoutines::_vector_f_math[VectorSupport::VEC_SIZE_256][op] = (address)os::dll_lookup(libjsvml, ebuf);\n+      snprintf(ebuf, sizeof(ebuf), \"__jsvml_%sf4_ha_%s\", VectorSupport::svmlname[op], avx_sse_str);\n+      StubRoutines::_vector_f_math[VectorSupport::VEC_SIZE_128][op] = (address)os::dll_lookup(libjsvml, ebuf);\n@@ -8122,2 +8161,2 @@\n-        snprintf(ebuf, sizeof(ebuf), \"__jsvml_%s1_ha_%s\", VectorSupport::svmlname[op], avx_sse_str);\n-        StubRoutines::_vector_d_math[VectorSupport::VEC_SIZE_64][op] = (address)os::dll_lookup(libjsvml, ebuf);\n+      snprintf(ebuf, sizeof(ebuf), \"__jsvml_%sf8_ha_%s\", VectorSupport::svmlname[op], avx_sse_str);\n+      StubRoutines::_vector_f_math[VectorSupport::VEC_SIZE_256][op] = (address)os::dll_lookup(libjsvml, ebuf);\n@@ -8125,2 +8164,2 @@\n-        snprintf(ebuf, sizeof(ebuf), \"__jsvml_%s2_ha_%s\", VectorSupport::svmlname[op], avx_sse_str);\n-        StubRoutines::_vector_d_math[VectorSupport::VEC_SIZE_128][op] = (address)os::dll_lookup(libjsvml, ebuf);\n+      snprintf(ebuf, sizeof(ebuf), \"__jsvml_%s1_ha_%s\", VectorSupport::svmlname[op], avx_sse_str);\n+      StubRoutines::_vector_d_math[VectorSupport::VEC_SIZE_64][op] = (address)os::dll_lookup(libjsvml, ebuf);\n@@ -8128,5 +8167,2 @@\n-        snprintf(ebuf, sizeof(ebuf), \"__jsvml_%s4_ha_%s\", VectorSupport::svmlname[op], avx_sse_str);\n-        StubRoutines::_vector_d_math[VectorSupport::VEC_SIZE_256][op] = (address)os::dll_lookup(libjsvml, ebuf);\n-      }\n-    }\n-#endif \/\/ COMPILER2\n+      snprintf(ebuf, sizeof(ebuf), \"__jsvml_%s2_ha_%s\", VectorSupport::svmlname[op], avx_sse_str);\n+      StubRoutines::_vector_d_math[VectorSupport::VEC_SIZE_128][op] = (address)os::dll_lookup(libjsvml, ebuf);\n@@ -8134,2 +8170,2 @@\n-    if (UseVectorizedMismatchIntrinsic) {\n-      StubRoutines::_vectorizedMismatch = generate_vectorizedMismatch();\n+      snprintf(ebuf, sizeof(ebuf), \"__jsvml_%s4_ha_%s\", VectorSupport::svmlname[op], avx_sse_str);\n+      StubRoutines::_vector_d_math[VectorSupport::VEC_SIZE_256][op] = (address)os::dll_lookup(libjsvml, ebuf);\n@@ -8138,0 +8174,1 @@\n+#endif \/\/ COMPILER2\n@@ -8139,9 +8176,2 @@\n- public:\n-  StubGenerator(CodeBuffer* code, int phase) : StubCodeGenerator(code) {\n-    if (phase == 0) {\n-      generate_initial();\n-    } else if (phase == 1) {\n-      generate_phase1(); \/\/ stubs that must be available for the interpreter\n-    } else {\n-      generate_all();\n-    }\n+  if (UseVectorizedMismatchIntrinsic) {\n+    StubRoutines::_vectorizedMismatch = generate_vectorizedMismatch();\n@@ -8149,1 +8179,1 @@\n-}; \/\/ end class declaration\n+}\n@@ -8151,1 +8181,0 @@\n-#define UCM_TABLE_MAX_ENTRIES 16\n@@ -8154,1 +8183,1 @@\n-    UnsafeCopyMemory::create_table(UCM_TABLE_MAX_ENTRIES);\n+    UnsafeCopyMemory::create_table(16);\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.cpp","additions":7200,"deletions":7171,"binary":false,"changes":14371,"status":"modified"},{"patch":"@@ -0,0 +1,455 @@\n+\/*\n+ * Copyright (c) 2003, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef CPU_X86_STUBGENERATOR_X86_64_HPP\n+#define CPU_X86_STUBGENERATOR_X86_64_HPP\n+\n+#include \"runtime\/continuation.hpp\"\n+#include \"runtime\/stubCodeGenerator.hpp\"\n+\n+\/\/ Stub Code definitions\n+\n+class StubGenerator: public StubCodeGenerator {\n+ private:\n+\n+  \/\/ Call stubs are used to call Java from C.\n+  address generate_call_stub(address& return_address);\n+\n+  \/\/ Return point for a Java call if there's an exception thrown in\n+  \/\/ Java code.  The exception is caught and transformed into a\n+  \/\/ pending exception stored in JavaThread that can be tested from\n+  \/\/ within the VM.\n+  \/\/\n+  \/\/ Note: Usually the parameters are removed by the callee. In case\n+  \/\/ of an exception crossing an activation frame boundary, that is\n+  \/\/ not the case if the callee is compiled code => need to setup the\n+  \/\/ rsp.\n+  \/\/\n+  \/\/ rax: exception oop\n+\n+  address generate_catch_exception();\n+\n+  \/\/ Continuation point for runtime calls returning with a pending\n+  \/\/ exception.  The pending exception check happened in the runtime\n+  \/\/ or native call stub.  The pending exception in Thread is\n+  \/\/ converted into a Java-level exception.\n+  \/\/\n+  \/\/ Contract with Java-level exception handlers:\n+  \/\/ rax: exception\n+  \/\/ rdx: throwing pc\n+  \/\/\n+  \/\/ NOTE: At entry of this stub, exception-pc must be on stack !!\n+\n+  address generate_forward_exception();\n+\n+  \/\/ Support for intptr_t OrderAccess::fence()\n+  address generate_orderaccess_fence();\n+\n+  \/\/ Support for intptr_t get_previous_sp()\n+  \/\/\n+  \/\/ This routine is used to find the previous stack pointer for the\n+  \/\/ caller.\n+  address generate_get_previous_sp();\n+\n+  \/\/----------------------------------------------------------------------------------------------------\n+  \/\/ Support for void verify_mxcsr()\n+  \/\/\n+  \/\/ This routine is used with -Xcheck:jni to verify that native\n+  \/\/ JNI code does not return to Java code without restoring the\n+  \/\/ MXCSR register to our expected state.\n+\n+  address generate_verify_mxcsr();\n+\n+  address generate_f2i_fixup();\n+  address generate_f2l_fixup();\n+  address generate_d2i_fixup();\n+  address generate_d2l_fixup();\n+\n+  address generate_count_leading_zeros_lut(const char *stub_name);\n+  address generate_popcount_avx_lut(const char *stub_name);\n+  address generate_iota_indices(const char *stub_name);\n+  address generate_vector_reverse_bit_lut(const char *stub_name);\n+\n+  address generate_vector_reverse_byte_perm_mask_long(const char *stub_name);\n+  address generate_vector_reverse_byte_perm_mask_int(const char *stub_name);\n+  address generate_vector_reverse_byte_perm_mask_short(const char *stub_name);\n+  address generate_vector_byte_shuffle_mask(const char *stub_name);\n+\n+  address generate_fp_mask(const char *stub_name, int64_t mask);\n+\n+  address generate_vector_mask(const char *stub_name, int64_t mask);\n+\n+  address generate_vector_byte_perm_mask(const char *stub_name);\n+\n+  address generate_vector_fp_mask(const char *stub_name, int64_t mask);\n+\n+  address generate_vector_custom_i32(const char *stub_name, Assembler::AvxVectorLen len,\n+                                     int32_t val0, int32_t val1, int32_t val2, int32_t val3,\n+                                     int32_t val4 = 0, int32_t val5 = 0, int32_t val6 = 0, int32_t val7 = 0,\n+                                     int32_t val8 = 0, int32_t val9 = 0, int32_t val10 = 0, int32_t val11 = 0,\n+                                     int32_t val12 = 0, int32_t val13 = 0, int32_t val14 = 0, int32_t val15 = 0);\n+\n+  \/\/ Non-destructive plausibility checks for oops\n+  address generate_verify_oop();\n+\n+  \/\/ Verify that a register contains clean 32-bits positive value\n+  \/\/ (high 32-bits are 0) so it could be used in 64-bits shifts.\n+  void assert_clean_int(Register Rint, Register Rtmp);\n+\n+  \/\/  Generate overlap test for array copy stubs\n+  void array_overlap_test(address no_overlap_target, Label* NOLp, Address::ScaleFactor sf);\n+\n+  void array_overlap_test(address no_overlap_target, Address::ScaleFactor sf) {\n+    assert(no_overlap_target != NULL, \"must be generated\");\n+    array_overlap_test(no_overlap_target, NULL, sf);\n+  }\n+  void array_overlap_test(Label& L_no_overlap, Address::ScaleFactor sf) {\n+    array_overlap_test(NULL, &L_no_overlap, sf);\n+  }\n+\n+\n+  \/\/ Shuffle first three arg regs on Windows into Linux\/Solaris locations.\n+  void setup_arg_regs(int nargs = 3);\n+  void restore_arg_regs();\n+\n+#ifdef ASSERT\n+  bool _regs_in_thread;\n+#endif\n+\n+  \/\/ This is used in places where r10 is a scratch register, and can\n+  \/\/ be adapted if r9 is needed also.\n+  void setup_arg_regs_using_thread();\n+\n+  void restore_arg_regs_using_thread();\n+\n+  \/\/ Copy big chunks forward\n+  void copy_bytes_forward(Register end_from, Register end_to,\n+                          Register qword_count, Register to,\n+                          Label& L_copy_bytes, Label& L_copy_8_bytes);\n+\n+  \/\/ Copy big chunks backward\n+  void copy_bytes_backward(Register from, Register dest,\n+                           Register qword_count, Register to,\n+                           Label& L_copy_bytes, Label& L_copy_8_bytes);\n+\n+  void setup_argument_regs(BasicType type);\n+\n+  void restore_argument_regs(BasicType type);\n+\n+#if COMPILER2_OR_JVMCI\n+  \/\/ Following rules apply to AVX3 optimized arraycopy stubs:\n+  \/\/ - If target supports AVX3 features (BW+VL+F) then implementation uses 32 byte vectors (YMMs)\n+  \/\/   for both special cases (various small block sizes) and aligned copy loop. This is the\n+  \/\/   default configuration.\n+  \/\/ - If copy length is above AVX3Threshold, then implementation use 64 byte vectors (ZMMs)\n+  \/\/   for main copy loop (and subsequent tail) since bulk of the cycles will be consumed in it.\n+  \/\/ - If user forces MaxVectorSize=32 then above 4096 bytes its seen that REP MOVs shows a\n+  \/\/   better performance for disjoint copies. For conjoint\/backward copy vector based\n+  \/\/   copy performs better.\n+  \/\/ - If user sets AVX3Threshold=0, then special cases for small blocks sizes operate over\n+  \/\/   64 byte vector registers (ZMMs).\n+\n+  address generate_disjoint_copy_avx3_masked(address* entry, const char *name, int shift,\n+                                             bool aligned, bool is_oop, bool dest_uninitialized);\n+\n+  address generate_conjoint_copy_avx3_masked(address* entry, const char *name, int shift,\n+                                             address nooverlap_target, bool aligned, bool is_oop,\n+                                             bool dest_uninitialized);\n+\n+#endif \/\/ COMPILER2_OR_JVMCI\n+\n+  address generate_disjoint_byte_copy(bool aligned, address* entry, const char *name);\n+\n+  address generate_conjoint_byte_copy(bool aligned, address nooverlap_target,\n+                                      address* entry, const char *name);\n+\n+  address generate_disjoint_short_copy(bool aligned, address *entry, const char *name);\n+\n+  address generate_fill(BasicType t, bool aligned, const char *name);\n+\n+  address generate_conjoint_short_copy(bool aligned, address nooverlap_target,\n+                                       address *entry, const char *name);\n+  address generate_disjoint_int_oop_copy(bool aligned, bool is_oop, address* entry,\n+                                         const char *name, bool dest_uninitialized = false);\n+  address generate_conjoint_int_oop_copy(bool aligned, bool is_oop, address nooverlap_target,\n+                                         address *entry, const char *name,\n+                                         bool dest_uninitialized = false);\n+  address generate_disjoint_long_oop_copy(bool aligned, bool is_oop, address *entry,\n+                                          const char *name, bool dest_uninitialized = false);\n+  address generate_conjoint_long_oop_copy(bool aligned, bool is_oop,\n+                                          address nooverlap_target, address *entry,\n+                                          const char *name, bool dest_uninitialized = false);\n+\n+  \/\/ Helper for generating a dynamic type check.\n+  \/\/ Smashes no registers.\n+  void generate_type_check(Register sub_klass,\n+                           Register super_check_offset,\n+                           Register super_klass,\n+                           Label& L_success);\n+\n+  \/\/ Generate checkcasting array copy stub\n+  address generate_checkcast_copy(const char *name, address *entry,\n+                                  bool dest_uninitialized = false);\n+\n+  \/\/ Generate 'unsafe' array copy stub\n+  \/\/ Though just as safe as the other stubs, it takes an unscaled\n+  \/\/ size_t argument instead of an element count.\n+  \/\/\n+  \/\/ Examines the alignment of the operands and dispatches\n+  \/\/ to a long, int, short, or byte copy loop.\n+  address generate_unsafe_copy(const char *name,\n+                               address byte_copy_entry, address short_copy_entry,\n+                               address int_copy_entry, address long_copy_entry);\n+\n+  \/\/ Perform range checks on the proposed arraycopy.\n+  \/\/ Kills temp, but nothing else.\n+  \/\/ Also, clean the sign bits of src_pos and dst_pos.\n+  void arraycopy_range_checks(Register src,     \/\/ source array oop (c_rarg0)\n+                              Register src_pos, \/\/ source position (c_rarg1)\n+                              Register dst,     \/\/ destination array oo (c_rarg2)\n+                              Register dst_pos, \/\/ destination position (c_rarg3)\n+                              Register length,\n+                              Register temp,\n+                              Label& L_failed);\n+\n+  \/\/ Generate generic array copy stubs\n+  address generate_generic_copy(const char *name,\n+                                address byte_copy_entry, address short_copy_entry,\n+                                address int_copy_entry, address oop_copy_entry,\n+                                address long_copy_entry, address checkcast_copy_entry);\n+\n+  address generate_data_cache_writeback();\n+\n+  address generate_data_cache_writeback_sync();\n+\n+  void generate_arraycopy_stubs();\n+\n+  \/\/ AES intrinsic stubs\n+\n+  enum {\n+    AESBlockSize = 16\n+  };\n+\n+  address generate_key_shuffle_mask();\n+\n+  address generate_counter_shuffle_mask();\n+\n+  \/\/ Utility routine for loading a 128-bit key word in little endian format\n+  \/\/ can optionally specify that the shuffle mask is already in an xmmregister\n+  void load_key(XMMRegister xmmdst, Register key, int offset, XMMRegister xmm_shuf_mask = xnoreg);\n+\n+  \/\/ Utility routine for increase 128bit counter (iv in CTR mode)\n+  void inc_counter(Register reg, XMMRegister xmmdst, int inc_delta, Label& next_block);\n+\n+  address generate_aescrypt_encryptBlock();\n+\n+  address generate_aescrypt_decryptBlock();\n+\n+  address generate_cipherBlockChaining_encryptAESCrypt();\n+\n+  \/\/ A version of CBC\/AES Decrypt which does 4 blocks in a loop at a time\n+  \/\/ to hide instruction latency\n+  address generate_cipherBlockChaining_decryptAESCrypt_Parallel();\n+\n+  address generate_electronicCodeBook_encryptAESCrypt();\n+\n+  address generate_electronicCodeBook_decryptAESCrypt();\n+\n+  \/\/ ofs and limit are use for multi-block byte array.\n+  \/\/ int com.sun.security.provider.MD5.implCompress(byte[] b, int ofs)\n+  address generate_md5_implCompress(bool multi_block, const char *name);\n+\n+  address generate_upper_word_mask();\n+\n+  address generate_shuffle_byte_flip_mask();\n+\n+  \/\/ ofs and limit are use for multi-block byte array.\n+  \/\/ int com.sun.security.provider.DigestBase.implCompressMultiBlock(byte[] b, int ofs, int limit)\n+  address generate_sha1_implCompress(bool multi_block, const char *name);\n+\n+  address generate_pshuffle_byte_flip_mask();\n+\n+  \/\/ Mask for byte-swapping a couple of qwords in an XMM register using (v)pshufb.\n+  address generate_pshuffle_byte_flip_mask_sha512();\n+\n+  \/\/ ofs and limit are use for multi-block byte array.\n+  \/\/ int com.sun.security.provider.DigestBase.implCompressMultiBlock(byte[] b, int ofs, int limit)\n+  address generate_sha256_implCompress(bool multi_block, const char *name);\n+  address generate_sha512_implCompress(bool multi_block, const char *name);\n+\n+  address ghash_polynomial512_addr();\n+\n+  \/\/ Vector AES Galois Counter Mode implementation\n+  address generate_galoisCounterMode_AESCrypt();\n+\n+  \/\/ This mask is used for incrementing counter value(linc0, linc4, etc.)\n+  address counter_mask_addr();\n+\n+ \/\/ Vector AES Counter implementation\n+  address generate_counterMode_VectorAESCrypt();\n+\n+  \/\/ This is a version of CTR\/AES crypt which does 6 blocks in a loop at a time\n+  \/\/ to hide instruction latency\n+  address generate_counterMode_AESCrypt_Parallel();\n+\n+  void roundDec(XMMRegister xmm_reg);\n+\n+  void roundDeclast(XMMRegister xmm_reg);\n+\n+  void ev_load_key(XMMRegister xmmdst, Register key, int offset, XMMRegister xmm_shuf_mask = xnoreg);\n+\n+  address generate_cipherBlockChaining_decryptVectorAESCrypt();\n+\n+  \/\/ Polynomial x^128+x^127+x^126+x^121+1\n+  address ghash_polynomial_addr();\n+\n+  address ghash_shufflemask_addr();\n+\n+  \/\/ Ghash single and multi block operations using AVX instructions\n+  address generate_avx_ghash_processBlocks();\n+\n+  \/\/ byte swap x86 long\n+  address generate_ghash_long_swap_mask();\n+\n+  \/\/ byte swap x86 byte array\n+  address generate_ghash_byte_swap_mask();\n+\n+  \/\/ Single and multi-block ghash operations\n+  address generate_ghash_processBlocks();\n+\n+  address base64_shuffle_addr();\n+  address base64_avx2_shuffle_addr();\n+  address base64_avx2_input_mask_addr();\n+  address base64_avx2_lut_addr();\n+  address base64_encoding_table_addr();\n+\n+  \/\/ Code for generating Base64 encoding.\n+  \/\/ Intrinsic function prototype in Base64.java:\n+  \/\/ private void encodeBlock(byte[] src, int sp, int sl, byte[] dst, int dp, boolean isURL)\n+  address generate_base64_encodeBlock();\n+\n+  \/\/ base64 AVX512vbmi tables\n+  address base64_vbmi_lookup_lo_addr();\n+  address base64_vbmi_lookup_hi_addr();\n+  address base64_vbmi_lookup_lo_url_addr();\n+  address base64_vbmi_lookup_hi_url_addr();\n+  address base64_vbmi_pack_vec_addr();\n+  address base64_vbmi_join_0_1_addr();\n+  address base64_vbmi_join_1_2_addr();\n+  address base64_vbmi_join_2_3_addr();\n+  address base64_decoding_table_addr();\n+\n+  \/\/ Code for generating Base64 decoding.\n+  \/\/\n+  \/\/ Based on the article (and associated code) from https:\/\/arxiv.org\/abs\/1910.05109.\n+  \/\/\n+  \/\/ Intrinsic function prototype in Base64.java:\n+  \/\/ private void decodeBlock(byte[] src, int sp, int sl, byte[] dst, int dp, boolean isURL, isMIME);\n+  address generate_base64_decodeBlock();\n+\n+  address generate_updateBytesCRC32();\n+  address generate_updateBytesCRC32C(bool is_pclmulqdq_supported);\n+\n+  address generate_updateBytesAdler32();\n+\n+  address generate_multiplyToLen();\n+\n+  address generate_vectorizedMismatch();\n+\n+  address generate_squareToLen();\n+\n+  address generate_method_entry_barrier();\n+\n+  address generate_mulAdd();\n+\n+  address generate_bigIntegerRightShift();\n+  address generate_bigIntegerLeftShift();\n+\n+  address generate_libmExp();\n+  address generate_libmLog();\n+  address generate_libmLog10();\n+  address generate_libmPow();\n+  address generate_libmSin();\n+  address generate_libmCos();\n+  address generate_libmTan();\n+\n+  RuntimeStub* generate_cont_doYield();\n+\n+  address generate_cont_thaw(const char* label, Continuation::thaw_kind kind);\n+  address generate_cont_thaw();\n+\n+  \/\/ TODO: will probably need multiple return barriers depending on return type\n+  address generate_cont_returnBarrier();\n+  address generate_cont_returnBarrier_exception();\n+\n+#if INCLUDE_JFR\n+\n+  \/\/ For c2: c_rarg0 is junk, call to runtime to write a checkpoint.\n+  \/\/ It returns a jobject handle to the event writer.\n+  \/\/ The handle is dereferenced and the return value is the event writer oop.\n+  RuntimeStub* generate_jfr_write_checkpoint();\n+\n+#endif \/\/ INCLUDE_JFR\n+\n+  \/\/ Continuation point for throwing of implicit exceptions that are\n+  \/\/ not handled in the current activation. Fabricates an exception\n+  \/\/ oop and initiates normal exception dispatching in this\n+  \/\/ frame. Since we need to preserve callee-saved values (currently\n+  \/\/ only for C2, but done for C1 as well) we need a callee-saved oop\n+  \/\/ map and therefore have to make these stubs into RuntimeStubs\n+  \/\/ rather than BufferBlobs.  If the compiler needs all registers to\n+  \/\/ be preserved between the fault point and the exception handler\n+  \/\/ then it must assume responsibility for that in\n+  \/\/ AbstractCompiler::continuation_for_implicit_null_exception or\n+  \/\/ continuation_for_implicit_division_by_zero_exception. All other\n+  \/\/ implicit exceptions (e.g., NullPointerException or\n+  \/\/ AbstractMethodError on entry) are either at call sites or\n+  \/\/ otherwise assume that stack unwinding will be initiated, so\n+  \/\/ caller saved registers were assumed volatile in the compiler.\n+  address generate_throw_exception(const char* name,\n+                                   address runtime_entry,\n+                                   Register arg1 = noreg,\n+                                   Register arg2 = noreg);\n+\n+  void create_control_words();\n+\n+  \/\/ Initialization\n+  void generate_initial();\n+  void generate_phase1();\n+  void generate_all();\n+\n+ public:\n+  StubGenerator(CodeBuffer* code, int phase) : StubCodeGenerator(code) {\n+    DEBUG_ONLY( _regs_in_thread = false; )\n+    if (phase == 0) {\n+      generate_initial();\n+    } else if (phase == 1) {\n+      generate_phase1(); \/\/ stubs that must be available for the interpreter\n+    } else {\n+      generate_all();\n+    }\n+  }\n+};\n+\n+#endif \/\/ CPU_X86_STUBGENERATOR_X86_64_HPP\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.hpp","additions":455,"deletions":0,"binary":false,"changes":455,"status":"added"}]}