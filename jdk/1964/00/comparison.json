{"files":[{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2013, 2019, Red Hat, Inc. All rights reserved.\n+ * Copyright (c) 2013, 2021, Red Hat, Inc. All rights reserved.\n@@ -28,0 +28,1 @@\n+#include \"gc\/shenandoah\/shenandoahGC.hpp\"\n@@ -44,1 +45,1 @@\n-  Copy::zero_to_bytes(_degen_points, sizeof(size_t) * ShenandoahHeap::_DEGENERATED_LIMIT);\n+  Copy::zero_to_bytes(_degen_points, sizeof(size_t) * ShenandoahGC::_DEGENERATED_LIMIT);\n@@ -70,2 +71,2 @@\n-void ShenandoahCollectorPolicy::record_alloc_failure_to_degenerated(ShenandoahHeap::ShenandoahDegenPoint point) {\n-  assert(point < ShenandoahHeap::_DEGENERATED_LIMIT, \"sanity\");\n+void ShenandoahCollectorPolicy::record_alloc_failure_to_degenerated(ShenandoahGC::ShenandoahDegenPoint point) {\n+  assert(point < ShenandoahGC::_DEGENERATED_LIMIT, \"sanity\");\n@@ -122,1 +123,1 @@\n-  for (int c = 0; c < ShenandoahHeap::_DEGENERATED_LIMIT; c++) {\n+  for (int c = 0; c < ShenandoahGC::_DEGENERATED_LIMIT; c++) {\n@@ -124,1 +125,1 @@\n-      const char* desc = ShenandoahHeap::degen_point_to_string((ShenandoahHeap::ShenandoahDegenPoint)c);\n+      const char* desc = ShenandoahGC::degen_point_to_string((ShenandoahGC::ShenandoahDegenPoint)c);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahCollectorPolicy.cpp","additions":7,"deletions":6,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2013, 2019, Red Hat, Inc. All rights reserved.\n+ * Copyright (c) 2013, 2021, Red Hat, Inc. All rights reserved.\n@@ -29,1 +29,2 @@\n-#include \"gc\/shenandoah\/shenandoahHeap.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGC.hpp\"\n+#include \"gc\/shenandoah\/shenandoahSharedVariables.hpp\"\n@@ -50,1 +51,1 @@\n-  size_t _degen_points[ShenandoahHeap::_DEGENERATED_LIMIT];\n+  size_t _degen_points[ShenandoahGC::_DEGENERATED_LIMIT];\n@@ -68,1 +69,1 @@\n-  void record_alloc_failure_to_degenerated(ShenandoahHeap::ShenandoahDegenPoint point);\n+  void record_alloc_failure_to_degenerated(ShenandoahGC::ShenandoahDegenPoint point);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahCollectorPolicy.hpp","additions":5,"deletions":4,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -0,0 +1,955 @@\n+\/*\n+ * Copyright (c) 2021, Red Hat, Inc. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+\n+#include \"gc\/shared\/barrierSetNMethod.hpp\"\n+#include \"gc\/shared\/collectorCounters.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahHeuristics.hpp\"\n+#include \"gc\/shenandoah\/shenandoahCollectorPolicy.hpp\"\n+#include \"gc\/shenandoah\/shenandoahConcurrentGC.hpp\"\n+#include \"gc\/shenandoah\/shenandoahFreeSet.hpp\"\n+#include \"gc\/shenandoah\/shenandoahLock.hpp\"\n+#include \"gc\/shenandoah\/shenandoahMark.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahMonitoringSupport.hpp\"\n+#include \"gc\/shenandoah\/shenandoahPhaseTimings.hpp\"\n+#include \"gc\/shenandoah\/shenandoahReferenceProcessor.hpp\"\n+#include \"gc\/shenandoah\/shenandoahRootProcessor.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahVMOperations.hpp\"\n+#include \"gc\/shenandoah\/shenandoahUtils.hpp\"\n+#include \"gc\/shenandoah\/shenandoahVerifier.hpp\"\n+#include \"gc\/shenandoah\/shenandoahWorkGroup.hpp\"\n+#include \"gc\/shenandoah\/shenandoahWorkerPolicy.hpp\"\n+#include \"prims\/jvmtiTagMap.hpp\"\n+#include \"runtime\/vmThread.hpp\"\n+#include \"utilities\/events.hpp\"\n+\n+ShenandoahConcurrentGC::ShenandoahConcurrentGC() :\n+  _mark(),\n+  _degen_point(ShenandoahDegenPoint::_degenerated_unset) {\n+}\n+\n+ShenandoahGC::ShenandoahDegenPoint ShenandoahConcurrentGC::degen_point() const {\n+  return _degen_point;\n+}\n+\n+void ShenandoahConcurrentGC::cancel() {\n+  ShenandoahConcurrentMark::cancel();\n+}\n+\n+bool ShenandoahConcurrentGC::collect(GCCause::Cause cause) {\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+\n+  \/\/ Reset for upcoming marking\n+  entry_reset();\n+\n+  \/\/ Start initial mark under STW\n+  vmop_entry_init_mark();\n+\n+    \/\/ Concurrent mark roots\n+  entry_mark_roots();\n+  if (check_cancellation_and_abort(ShenandoahDegenPoint::_degenerated_outside_cycle)) return false;\n+\n+  \/\/ Continue concurrent mark\n+  entry_mark();\n+  if (check_cancellation_and_abort(ShenandoahDegenPoint::_degenerated_mark)) return false;\n+\n+  \/\/ Complete marking under STW, and start evacuation\n+  vmop_entry_final_mark();\n+\n+  \/\/ Process weak roots that might still point to regions that would be broken by cleanup\n+  if (heap->is_concurrent_weak_root_in_progress()) {\n+    entry_weak_refs();\n+    entry_weak_roots();\n+  }\n+\n+  \/\/ Final mark might have reclaimed some immediate garbage, kick cleanup to reclaim\n+  \/\/ the space. This would be the last action if there is nothing to evacuate.\n+  entry_cleanup_early();\n+\n+  {\n+    ShenandoahHeapLocker locker(heap->lock());\n+    heap->free_set()->log_status();\n+  }\n+\n+  \/\/ Perform concurrent class unloading\n+  if (heap->is_concurrent_weak_root_in_progress() &&\n+      ShenandoahConcurrentRoots::should_do_concurrent_class_unloading()) {\n+    entry_class_unloading();\n+  }\n+\n+  \/\/ Processing strong roots\n+  \/\/ This may be skipped if there is nothing to update\/evacuate.\n+  \/\/ If so, strong_root_in_progress would be unset.\n+  if (heap->is_concurrent_strong_root_in_progress()) {\n+    entry_strong_roots();\n+  }\n+\n+  \/\/ Continue the cycle with evacuation and optional update-refs.\n+  \/\/ This may be skipped if there is nothing to evacuate.\n+  \/\/ If so, evac_in_progress would be unset by collection set preparation code.\n+  if (heap->is_evacuation_in_progress()) {\n+    \/\/ Concurrently evacuate\n+    entry_evacuate();\n+    if (check_cancellation_and_abort(ShenandoahDegenPoint::_degenerated_evac)) return false;\n+\n+    \/\/ Perform update-refs phase.\n+    vmop_entry_init_updaterefs();\n+    entry_updaterefs();\n+    if (check_cancellation_and_abort(ShenandoahDegenPoint::_degenerated_updaterefs)) return false;\n+\n+    \/\/ Concurrent update thread roots\n+    entry_update_thread_roots();\n+    if (check_cancellation_and_abort(ShenandoahDegenPoint::_degenerated_updaterefs)) return false;\n+\n+    vmop_entry_final_updaterefs();\n+\n+    \/\/ Update references freed up collection set, kick the cleanup to reclaim the space.\n+    entry_cleanup_complete();\n+  } else {\n+    \/\/ Concurrent weak\/strong root flags are unset concurrently. We depend on updateref GC safepoints\n+    \/\/ to ensure the changes are visible to all mutators before gc cycle is completed.\n+    \/\/ In case of no evacuation, updateref GC safepoints are skipped. Therefore, we will need\n+    \/\/ to perform thread handshake to ensure their consistences.\n+    entry_rendezvous_roots();\n+  }\n+\n+  return true;\n+}\n+\n+void ShenandoahConcurrentGC::vmop_entry_init_mark() {\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+  TraceCollectorStats tcs(heap->monitoring_support()->stw_collection_counters());\n+  ShenandoahTimingsTracker timing(ShenandoahPhaseTimings::init_mark_gross);\n+\n+  heap->try_inject_alloc_failure();\n+  VM_ShenandoahInitMark op(this);\n+  VMThread::execute(&op); \/\/ jump to entry_init_mark() under safepoint\n+}\n+\n+void ShenandoahConcurrentGC::vmop_entry_final_mark() {\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+  TraceCollectorStats tcs(heap->monitoring_support()->stw_collection_counters());\n+  ShenandoahTimingsTracker timing(ShenandoahPhaseTimings::final_mark_gross);\n+\n+  heap->try_inject_alloc_failure();\n+  VM_ShenandoahFinalMarkStartEvac op(this);\n+  VMThread::execute(&op); \/\/ jump to entry_final_mark under safepoint\n+}\n+\n+void ShenandoahConcurrentGC::vmop_entry_init_updaterefs() {\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+  TraceCollectorStats tcs(heap->monitoring_support()->stw_collection_counters());\n+  ShenandoahTimingsTracker timing(ShenandoahPhaseTimings::init_update_refs_gross);\n+\n+  heap->try_inject_alloc_failure();\n+  VM_ShenandoahInitUpdateRefs op(this);\n+  VMThread::execute(&op);\n+}\n+\n+void ShenandoahConcurrentGC::vmop_entry_final_updaterefs() {\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+  TraceCollectorStats tcs(heap->monitoring_support()->stw_collection_counters());\n+  ShenandoahTimingsTracker timing(ShenandoahPhaseTimings::final_update_refs_gross);\n+\n+  heap->try_inject_alloc_failure();\n+  VM_ShenandoahFinalUpdateRefs op(this);\n+  VMThread::execute(&op);\n+}\n+\n+void ShenandoahConcurrentGC::entry_init_mark() {\n+  const char* msg = init_mark_event_message();\n+  ShenandoahPausePhase gc_phase(msg, ShenandoahPhaseTimings::init_mark);\n+  EventMark em(\"%s\", msg);\n+\n+  ShenandoahWorkerScope scope(ShenandoahHeap::heap()->workers(),\n+                              ShenandoahWorkerPolicy::calc_workers_for_init_marking(),\n+                              \"init marking\");\n+\n+  op_init_mark();\n+}\n+\n+void ShenandoahConcurrentGC::entry_final_mark() {\n+  const char* msg = final_mark_event_message();\n+  ShenandoahPausePhase gc_phase(msg, ShenandoahPhaseTimings::final_mark);\n+  EventMark em(\"%s\", msg);\n+\n+  ShenandoahWorkerScope scope(ShenandoahHeap::heap()->workers(),\n+                              ShenandoahWorkerPolicy::calc_workers_for_final_marking(),\n+                              \"final marking\");\n+\n+  op_final_mark();\n+}\n+\n+void ShenandoahConcurrentGC::entry_init_updaterefs() {\n+  static const char* msg = \"Pause Init Update Refs\";\n+  ShenandoahPausePhase gc_phase(msg, ShenandoahPhaseTimings::init_update_refs);\n+  EventMark em(\"%s\", msg);\n+\n+  \/\/ No workers used in this phase, no setup required\n+  op_init_updaterefs();\n+}\n+\n+void ShenandoahConcurrentGC::entry_final_updaterefs() {\n+  static const char* msg = \"Pause Final Update Refs\";\n+  ShenandoahPausePhase gc_phase(msg, ShenandoahPhaseTimings::final_update_refs);\n+  EventMark em(\"%s\", msg);\n+\n+  ShenandoahWorkerScope scope(ShenandoahHeap::heap()->workers(),\n+                              ShenandoahWorkerPolicy::calc_workers_for_final_update_ref(),\n+                              \"final reference update\");\n+\n+  op_final_updaterefs();\n+}\n+\n+void ShenandoahConcurrentGC::entry_reset() {\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+  TraceCollectorStats tcs(heap->monitoring_support()->concurrent_collection_counters());\n+  static const char* msg = \"Concurrent reset\";\n+  ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_reset);\n+  EventMark em(\"%s\", msg);\n+\n+  ShenandoahWorkerScope scope(heap->workers(),\n+                              ShenandoahWorkerPolicy::calc_workers_for_conc_reset(),\n+                              \"concurrent reset\");\n+\n+  heap->try_inject_alloc_failure();\n+  op_reset();\n+}\n+\n+void ShenandoahConcurrentGC::entry_mark_roots() {\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+  TraceCollectorStats tcs(heap->monitoring_support()->concurrent_collection_counters());\n+  const char* msg = \"Concurrent marking roots\";\n+  ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_mark_roots);\n+  EventMark em(\"%s\", msg);\n+\n+  ShenandoahWorkerScope scope(heap->workers(),\n+                              ShenandoahWorkerPolicy::calc_workers_for_conc_marking(),\n+                              \"concurrent marking roots\");\n+\n+  heap->try_inject_alloc_failure();\n+  op_mark_roots();\n+}\n+\n+void ShenandoahConcurrentGC::entry_mark() {\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+  TraceCollectorStats tcs(heap->monitoring_support()->concurrent_collection_counters());\n+  const char* msg = conc_mark_event_message();\n+  ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_mark);\n+  EventMark em(\"%s\", msg);\n+\n+  ShenandoahWorkerScope scope(heap->workers(),\n+                              ShenandoahWorkerPolicy::calc_workers_for_conc_marking(),\n+                              \"concurrent marking\");\n+\n+  heap->try_inject_alloc_failure();\n+  op_mark();\n+}\n+\n+void ShenandoahConcurrentGC::entry_weak_refs() {\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+  static const char* msg = \"Concurrent weak references\";\n+  ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_weak_refs);\n+  EventMark em(\"%s\", msg);\n+\n+  ShenandoahWorkerScope scope(heap->workers(),\n+                              ShenandoahWorkerPolicy::calc_workers_for_conc_refs_processing(),\n+                              \"concurrent weak references\");\n+\n+  heap->try_inject_alloc_failure();\n+  op_weak_refs();\n+}\n+\n+void ShenandoahConcurrentGC::entry_weak_roots() {\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+  TraceCollectorStats tcs(heap->monitoring_support()->concurrent_collection_counters());\n+  static const char* msg = \"Concurrent weak roots\";\n+  ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_weak_roots);\n+  EventMark em(\"%s\", msg);\n+\n+  ShenandoahWorkerScope scope(heap->workers(),\n+                              ShenandoahWorkerPolicy::calc_workers_for_conc_root_processing(),\n+                              \"concurrent weak root\");\n+\n+  heap->try_inject_alloc_failure();\n+  op_weak_roots();\n+}\n+\n+void ShenandoahConcurrentGC::entry_class_unloading() {\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+  TraceCollectorStats tcs(heap->monitoring_support()->concurrent_collection_counters());\n+  static const char* msg = \"Concurrent class unloading\";\n+  ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_class_unload);\n+  EventMark em(\"%s\", msg);\n+\n+  ShenandoahWorkerScope scope(heap->workers(),\n+                              ShenandoahWorkerPolicy::calc_workers_for_conc_root_processing(),\n+                              \"concurrent class unloading\");\n+\n+  heap->try_inject_alloc_failure();\n+  op_class_unloading();\n+}\n+\n+void ShenandoahConcurrentGC::entry_strong_roots() {\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+  TraceCollectorStats tcs(heap->monitoring_support()->concurrent_collection_counters());\n+  static const char* msg = \"Concurrent strong roots\";\n+  ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_strong_roots);\n+  EventMark em(\"%s\", msg);\n+\n+  ShenandoahGCWorkerPhase worker_phase(ShenandoahPhaseTimings::conc_strong_roots);\n+\n+  ShenandoahWorkerScope scope(heap->workers(),\n+                              ShenandoahWorkerPolicy::calc_workers_for_conc_root_processing(),\n+                              \"concurrent strong root\");\n+\n+  heap->try_inject_alloc_failure();\n+  op_strong_roots();\n+}\n+\n+void ShenandoahConcurrentGC::entry_cleanup_early() {\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+  TraceCollectorStats tcs(heap->monitoring_support()->concurrent_collection_counters());\n+  static const char* msg = \"Concurrent cleanup\";\n+  ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_cleanup_early, true \/* log_heap_usage *\/);\n+  EventMark em(\"%s\", msg);\n+\n+  \/\/ This phase does not use workers, no need for setup\n+  heap->try_inject_alloc_failure();\n+  op_cleanup_early();\n+}\n+\n+void ShenandoahConcurrentGC::entry_rendezvous_roots() {\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+  TraceCollectorStats tcs(heap->monitoring_support()->concurrent_collection_counters());\n+  static const char* msg = \"Rendezvous roots\";\n+  ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_rendezvous_roots);\n+  EventMark em(\"%s\", msg);\n+\n+  \/\/ This phase does not use workers, no need for setup\n+  heap->try_inject_alloc_failure();\n+  op_rendezvous_roots();\n+}\n+\n+void ShenandoahConcurrentGC::entry_evacuate() {\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+  TraceCollectorStats tcs(heap->monitoring_support()->concurrent_collection_counters());\n+\n+  static const char* msg = \"Concurrent evacuation\";\n+  ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_evac);\n+  EventMark em(\"%s\", msg);\n+\n+  ShenandoahWorkerScope scope(heap->workers(),\n+                              ShenandoahWorkerPolicy::calc_workers_for_conc_evac(),\n+                              \"concurrent evacuation\");\n+\n+  heap->try_inject_alloc_failure();\n+  op_evacuate();\n+}\n+\n+void ShenandoahConcurrentGC::entry_update_thread_roots() {\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+  TraceCollectorStats tcs(heap->monitoring_support()->concurrent_collection_counters());\n+\n+  static const char* msg = \"Concurrent update thread roots\";\n+  ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_update_thread_roots);\n+  EventMark em(\"%s\", msg);\n+\n+  \/\/ No workers used in this phase, no setup required\n+  heap->try_inject_alloc_failure();\n+  op_update_thread_roots();\n+}\n+\n+void ShenandoahConcurrentGC::entry_updaterefs() {\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+  TraceCollectorStats tcs(heap->monitoring_support()->concurrent_collection_counters());\n+  static const char* msg = \"Concurrent update references\";\n+  ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_update_refs);\n+  EventMark em(\"%s\", msg);\n+\n+  ShenandoahWorkerScope scope(heap->workers(),\n+                              ShenandoahWorkerPolicy::calc_workers_for_conc_update_ref(),\n+                              \"concurrent reference update\");\n+\n+  heap->try_inject_alloc_failure();\n+  op_updaterefs();\n+}\n+\n+void ShenandoahConcurrentGC::entry_cleanup_complete() {\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+  TraceCollectorStats tcs(heap->monitoring_support()->concurrent_collection_counters());\n+  static const char* msg = \"Concurrent cleanup\";\n+  ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_cleanup_complete, true \/* log_heap_usage *\/);\n+  EventMark em(\"%s\", msg);\n+\n+  \/\/ This phase does not use workers, no need for setup\n+  heap->try_inject_alloc_failure();\n+  op_cleanup_complete();\n+}\n+\n+\/\/ Actual work for the phases\n+void ShenandoahConcurrentGC::op_reset() {\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+  if (ShenandoahPacing) {\n+    heap->pacer()->setup_for_reset();\n+  }\n+\n+  heap->prepare_gc();\n+}\n+\n+class ShenandoahInitMarkUpdateRegionStateClosure : public ShenandoahHeapRegionClosure {\n+private:\n+  ShenandoahMarkingContext* const _ctx;\n+public:\n+  ShenandoahInitMarkUpdateRegionStateClosure() : _ctx(ShenandoahHeap::heap()->marking_context()) {}\n+\n+  void heap_region_do(ShenandoahHeapRegion* r) {\n+    assert(!r->has_live(), \"Region \" SIZE_FORMAT \" should have no live data\", r->index());\n+    if (r->is_active()) {\n+      \/\/ Check if region needs updating its TAMS. We have updated it already during concurrent\n+      \/\/ reset, so it is very likely we don't need to do another write here.\n+      if (_ctx->top_at_mark_start(r) != r->top()) {\n+        _ctx->capture_top_at_mark_start(r);\n+      }\n+    } else {\n+      assert(_ctx->top_at_mark_start(r) == r->top(),\n+             \"Region \" SIZE_FORMAT \" should already have correct TAMS\", r->index());\n+    }\n+  }\n+\n+  bool is_thread_safe() { return true; }\n+};\n+\n+void ShenandoahConcurrentGC::op_init_mark() {\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+  assert(ShenandoahSafepoint::is_at_shenandoah_safepoint(), \"Should be at safepoint\");\n+  assert(Thread::current()->is_VM_thread(), \"can only do this in VMThread\");\n+\n+  assert(heap->marking_context()->is_bitmap_clear(), \"need clear marking bitmap\");\n+  assert(!heap->marking_context()->is_complete(), \"should not be complete\");\n+  assert(!heap->has_forwarded_objects(), \"No forwarded objects on this path\");\n+\n+  if (ShenandoahVerify) {\n+    heap->verifier()->verify_before_concmark();\n+  }\n+\n+  if (VerifyBeforeGC) {\n+    Universe::verify();\n+  }\n+\n+  heap->set_concurrent_mark_in_progress(true);\n+\n+  \/\/ We need to reset all TLABs because they might be below the TAMS, and we need to mark\n+  \/\/ the objects in them. Do not let mutators allocate any new objects in their current TLABs.\n+  \/\/ It is also a good place to resize the TLAB sizes for future allocations.\n+  if (UseTLAB) {\n+    ShenandoahGCPhase phase(ShenandoahPhaseTimings::init_manage_tlabs);\n+    heap->tlabs_retire(ResizeTLAB);\n+  }\n+\n+  {\n+    ShenandoahGCPhase phase(ShenandoahPhaseTimings::init_update_region_states);\n+    ShenandoahInitMarkUpdateRegionStateClosure cl;\n+    heap->parallel_heap_region_iterate(&cl);\n+  }\n+\n+  \/\/ Weak reference processing\n+  ShenandoahReferenceProcessor* rp = heap->ref_processor();\n+  rp->reset_thread_locals();\n+  rp->set_soft_reference_policy(heap->soft_ref_policy()->should_clear_all_soft_refs());\n+\n+  \/\/ Make above changes visible to worker threads\n+  OrderAccess::fence();\n+  \/\/ Arm nmethods for concurrent marking. When a nmethod is about to be executed,\n+  \/\/ we need to make sure that all its metadata are marked. alternative is to remark\n+  \/\/ thread roots at final mark pause, but it can be potential latency killer.\n+  if (ShenandoahConcurrentRoots::should_do_concurrent_class_unloading()) {\n+    ShenandoahCodeRoots::arm_nmethods();\n+  }\n+\n+  _mark.mark_stw_roots();\n+\n+  if (ShenandoahPacing) {\n+    heap->pacer()->setup_for_mark();\n+  }\n+}\n+\n+void ShenandoahConcurrentGC::op_mark_roots() {\n+  _mark.mark_concurrent_roots();\n+}\n+\n+void ShenandoahConcurrentGC::op_mark() {\n+  _mark.concurrent_mark();\n+}\n+\n+void ShenandoahConcurrentGC::op_final_mark() {\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+  assert(ShenandoahSafepoint::is_at_shenandoah_safepoint(), \"Should be at safepoint\");\n+  assert(!heap->has_forwarded_objects(), \"No forwarded objects on this path\");\n+\n+  if (ShenandoahVerify) {\n+    heap->verifier()->verify_roots_no_forwarded();\n+  }\n+\n+  if (!heap->cancelled_gc()) {\n+    _mark.finish_mark();\n+    assert(!heap->cancelled_gc(), \"STW mark cannot OOM\");\n+\n+    \/\/ Notify JVMTI that the tagmap table will need cleaning.\n+    JvmtiTagMap::set_needs_cleaning();\n+\n+    heap->prepare_regions_and_collection_set(true \/*concurrent*\/);\n+\n+    \/\/ Has to be done after cset selection\n+    heap->prepare_concurrent_roots();\n+\n+    if (!heap->collection_set()->is_empty()) {\n+      if (ShenandoahVerify) {\n+        heap->verifier()->verify_before_evacuation();\n+      }\n+\n+      heap->set_evacuation_in_progress(true);\n+      \/\/ From here on, we need to update references.\n+      heap->set_has_forwarded_objects(true);\n+\n+      \/\/ Arm nmethods for concurrent processing\n+      ShenandoahCodeRoots::arm_nmethods();\n+\n+      \/\/ Should be gone after 8212879 and concurrent stack processing\n+      heap->evacuate_and_update_roots();\n+\n+      \/\/ Notify JVMTI that oops are changed.\n+      JvmtiTagMap::set_needs_rehashing();\n+\n+      if (ShenandoahVerify) {\n+        heap->verifier()->verify_during_evacuation();\n+      }\n+\n+      if (ShenandoahPacing) {\n+        heap->pacer()->setup_for_evac();\n+      }\n+    } else {\n+      if (ShenandoahVerify) {\n+        heap->verifier()->verify_after_concmark();\n+      }\n+\n+      if (VerifyAfterGC) {\n+        Universe::verify();\n+      }\n+    }\n+  }\n+}\n+\n+void ShenandoahConcurrentGC::op_weak_refs() {\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+  assert(heap->is_concurrent_weak_root_in_progress(), \"Only during this phase\");\n+  \/\/ Concurrent weak refs processing\n+  ShenandoahTimingsTracker t(ShenandoahPhaseTimings::conc_weak_refs_work);\n+  ShenandoahGCWorkerPhase worker_phase(ShenandoahPhaseTimings::conc_weak_refs_work);\n+  heap->ref_processor()->process_references(ShenandoahPhaseTimings::conc_weak_refs_work, heap->workers(), true \/* concurrent *\/);\n+}\n+\n+class ShenandoahEvacUpdateCleanupOopStorageRootsClosure : public BasicOopIterateClosure {\n+private:\n+  ShenandoahHeap* const _heap;\n+  ShenandoahMarkingContext* const _mark_context;\n+  bool  _evac_in_progress;\n+  Thread* const _thread;\n+\n+public:\n+  ShenandoahEvacUpdateCleanupOopStorageRootsClosure();\n+  void do_oop(oop* p);\n+  void do_oop(narrowOop* p);\n+};\n+\n+ShenandoahEvacUpdateCleanupOopStorageRootsClosure::ShenandoahEvacUpdateCleanupOopStorageRootsClosure() :\n+  _heap(ShenandoahHeap::heap()),\n+  _mark_context(ShenandoahHeap::heap()->marking_context()),\n+  _evac_in_progress(ShenandoahHeap::heap()->is_evacuation_in_progress()),\n+  _thread(Thread::current()) {\n+}\n+\n+void ShenandoahEvacUpdateCleanupOopStorageRootsClosure::do_oop(oop* p) {\n+  const oop obj = RawAccess<>::oop_load(p);\n+  if (!CompressedOops::is_null(obj)) {\n+    if (!_mark_context->is_marked(obj)) {\n+      shenandoah_assert_correct(p, obj);\n+      Atomic::cmpxchg(p, obj, oop(NULL));\n+    } else if (_evac_in_progress && _heap->in_collection_set(obj)) {\n+      oop resolved = ShenandoahBarrierSet::resolve_forwarded_not_null(obj);\n+      if (resolved == obj) {\n+        resolved = _heap->evacuate_object(obj, _thread);\n+      }\n+      Atomic::cmpxchg(p, obj, resolved);\n+      assert(_heap->cancelled_gc() ||\n+             _mark_context->is_marked(resolved) && !_heap->in_collection_set(resolved),\n+             \"Sanity\");\n+    }\n+  }\n+}\n+\n+void ShenandoahEvacUpdateCleanupOopStorageRootsClosure::do_oop(narrowOop* p) {\n+  ShouldNotReachHere();\n+}\n+\n+class ShenandoahIsCLDAliveClosure : public CLDClosure {\n+public:\n+  void do_cld(ClassLoaderData* cld) {\n+    cld->is_alive();\n+  }\n+};\n+\n+class ShenandoahIsNMethodAliveClosure: public NMethodClosure {\n+public:\n+  void do_nmethod(nmethod* n) {\n+    n->is_unloading();\n+  }\n+};\n+\n+\/\/ This task not only evacuates\/updates marked weak roots, but also \"NULL\"\n+\/\/ dead weak roots.\n+class ShenandoahConcurrentWeakRootsEvacUpdateTask : public AbstractGangTask {\n+private:\n+  ShenandoahVMWeakRoots<true \/*concurrent*\/> _vm_roots;\n+\n+  \/\/ Roots related to concurrent class unloading\n+  ShenandoahClassLoaderDataRoots<true \/* concurrent *\/, true \/* single thread*\/>\n+                                             _cld_roots;\n+  ShenandoahConcurrentNMethodIterator        _nmethod_itr;\n+  ShenandoahConcurrentStringDedupRoots       _dedup_roots;\n+  ShenandoahPhaseTimings::Phase              _phase;\n+  bool                                       _concurrent_class_unloading;\n+\n+public:\n+  ShenandoahConcurrentWeakRootsEvacUpdateTask(ShenandoahPhaseTimings::Phase phase) :\n+    AbstractGangTask(\"Shenandoah Evacuate\/Update Concurrent Weak Roots\"),\n+    _vm_roots(phase),\n+    _cld_roots(phase, ShenandoahHeap::heap()->workers()->active_workers()),\n+    _nmethod_itr(ShenandoahCodeRoots::table()),\n+    _dedup_roots(phase),\n+    _phase(phase),\n+    _concurrent_class_unloading(ShenandoahConcurrentRoots::should_do_concurrent_class_unloading()) {\n+    if (_concurrent_class_unloading) {\n+      MutexLocker mu(CodeCache_lock, Mutex::_no_safepoint_check_flag);\n+      _nmethod_itr.nmethods_do_begin();\n+    }\n+\n+     _dedup_roots.prologue();\n+  }\n+\n+  ~ShenandoahConcurrentWeakRootsEvacUpdateTask() {\n+    _dedup_roots.epilogue();\n+\n+    if (_concurrent_class_unloading) {\n+      MutexLocker mu(CodeCache_lock, Mutex::_no_safepoint_check_flag);\n+      _nmethod_itr.nmethods_do_end();\n+    }\n+    \/\/ Notify runtime data structures of potentially dead oops\n+    _vm_roots.report_num_dead();\n+  }\n+\n+  void work(uint worker_id) {\n+    ShenandoahConcurrentWorkerSession worker_session(worker_id);\n+    {\n+      ShenandoahEvacOOMScope oom;\n+      \/\/ jni_roots and weak_roots are OopStorage backed roots, concurrent iteration\n+      \/\/ may race against OopStorage::release() calls.\n+      ShenandoahEvacUpdateCleanupOopStorageRootsClosure cl;\n+      _vm_roots.oops_do(&cl, worker_id);\n+\n+      \/\/ String dedup weak roots\n+      ShenandoahForwardedIsAliveClosure is_alive;\n+      ShenandoahEvacuateUpdateRootsClosure<MO_RELEASE> keep_alive;\n+      _dedup_roots.oops_do(&is_alive, &keep_alive, worker_id);\n+    }\n+\n+    \/\/ If we are going to perform concurrent class unloading later on, we need to\n+    \/\/ cleanup the weak oops in CLD and determinate nmethod's unloading state, so that we\n+    \/\/ can cleanup immediate garbage sooner.\n+    if (_concurrent_class_unloading) {\n+      \/\/ Applies ShenandoahIsCLDAlive closure to CLDs, native barrier will either NULL the\n+      \/\/ CLD's holder or evacuate it.\n+      {\n+        ShenandoahIsCLDAliveClosure is_cld_alive;\n+        _cld_roots.cld_do(&is_cld_alive, worker_id);\n+      }\n+\n+      \/\/ Applies ShenandoahIsNMethodAliveClosure to registered nmethods.\n+      \/\/ The closure calls nmethod->is_unloading(). The is_unloading\n+      \/\/ state is cached, therefore, during concurrent class unloading phase,\n+      \/\/ we will not touch the metadata of unloading nmethods\n+      {\n+        ShenandoahWorkerTimingsTracker timer(_phase, ShenandoahPhaseTimings::CodeCacheRoots, worker_id);\n+        ShenandoahIsNMethodAliveClosure is_nmethod_alive;\n+        _nmethod_itr.nmethods_do(&is_nmethod_alive);\n+      }\n+    }\n+  }\n+};\n+\n+void ShenandoahConcurrentGC::op_weak_roots() {\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+  assert(heap->is_concurrent_weak_root_in_progress(), \"Only during this phase\");\n+  \/\/ Concurrent weak root processing\n+  {\n+    ShenandoahTimingsTracker t(ShenandoahPhaseTimings::conc_weak_roots_work);\n+    ShenandoahGCWorkerPhase worker_phase(ShenandoahPhaseTimings::conc_weak_roots_work);\n+    ShenandoahConcurrentWeakRootsEvacUpdateTask task(ShenandoahPhaseTimings::conc_weak_roots_work);\n+    heap->workers()->run_task(&task);\n+  }\n+\n+  \/\/ Perform handshake to flush out dead oops\n+  {\n+    ShenandoahTimingsTracker t(ShenandoahPhaseTimings::conc_weak_roots_rendezvous);\n+    heap->rendezvous_threads();\n+  }\n+\n+  if (!ShenandoahConcurrentRoots::should_do_concurrent_class_unloading()) {\n+    heap->set_concurrent_weak_root_in_progress(false);\n+  }\n+}\n+\n+void ShenandoahConcurrentGC::op_class_unloading() {\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+  assert (heap->is_concurrent_weak_root_in_progress() &&\n+          ShenandoahConcurrentRoots::should_do_concurrent_class_unloading(),\n+          \"Checked by caller\");\n+  heap->do_class_unloading();\n+  heap->set_concurrent_weak_root_in_progress(false);\n+}\n+\n+class ShenandoahEvacUpdateCodeCacheClosure : public NMethodClosure {\n+private:\n+  BarrierSetNMethod* const               _bs;\n+  ShenandoahEvacuateUpdateRootsClosure<> _cl;\n+\n+public:\n+  ShenandoahEvacUpdateCodeCacheClosure() :\n+    _bs(BarrierSet::barrier_set()->barrier_set_nmethod()),\n+    _cl() {\n+  }\n+\n+  void do_nmethod(nmethod* n) {\n+    ShenandoahNMethod* data = ShenandoahNMethod::gc_data(n);\n+    ShenandoahReentrantLocker locker(data->lock());\n+    \/\/ Setup EvacOOM scope below reentrant lock to avoid deadlock with\n+    \/\/ nmethod_entry_barrier\n+    ShenandoahEvacOOMScope oom;\n+    data->oops_do(&_cl, true\/*fix relocation*\/);\n+    _bs->disarm(n);\n+  }\n+};\n+\n+class ShenandoahConcurrentRootsEvacUpdateTask : public AbstractGangTask {\n+private:\n+  ShenandoahPhaseTimings::Phase                 _phase;\n+  ShenandoahVMRoots<true \/*concurrent*\/>        _vm_roots;\n+  ShenandoahClassLoaderDataRoots<true \/*concurrent*\/, false \/*single threaded*\/> _cld_roots;\n+  ShenandoahConcurrentNMethodIterator           _nmethod_itr;\n+  bool                                          _process_codecache;\n+\n+public:\n+  ShenandoahConcurrentRootsEvacUpdateTask(ShenandoahPhaseTimings::Phase phase) :\n+    AbstractGangTask(\"Shenandoah Evacuate\/Update Concurrent Strong Roots\"),\n+    _phase(phase),\n+    _vm_roots(phase),\n+    _cld_roots(phase, ShenandoahHeap::heap()->workers()->active_workers()),\n+    _nmethod_itr(ShenandoahCodeRoots::table()),\n+    _process_codecache(!ShenandoahConcurrentRoots::should_do_concurrent_class_unloading()) {\n+    if (_process_codecache) {\n+      MutexLocker mu(CodeCache_lock, Mutex::_no_safepoint_check_flag);\n+      _nmethod_itr.nmethods_do_begin();\n+    }\n+  }\n+\n+  ~ShenandoahConcurrentRootsEvacUpdateTask() {\n+    if (_process_codecache) {\n+      MutexLocker mu(CodeCache_lock, Mutex::_no_safepoint_check_flag);\n+      _nmethod_itr.nmethods_do_end();\n+    }\n+  }\n+\n+  void work(uint worker_id) {\n+    ShenandoahConcurrentWorkerSession worker_session(worker_id);\n+    {\n+      ShenandoahEvacOOMScope oom;\n+      {\n+        \/\/ vm_roots and weak_roots are OopStorage backed roots, concurrent iteration\n+        \/\/ may race against OopStorage::release() calls.\n+        ShenandoahEvacUpdateOopStorageRootsClosure cl;\n+        _vm_roots.oops_do<ShenandoahEvacUpdateOopStorageRootsClosure>(&cl, worker_id);\n+      }\n+\n+      {\n+        ShenandoahEvacuateUpdateRootsClosure<> cl;\n+        CLDToOopClosure clds(&cl, ClassLoaderData::_claim_strong);\n+        _cld_roots.cld_do(&clds, worker_id);\n+      }\n+    }\n+\n+    \/\/ Cannot setup ShenandoahEvacOOMScope here, due to potential deadlock with nmethod_entry_barrier.\n+    if (_process_codecache) {\n+      ShenandoahWorkerTimingsTracker timer(_phase, ShenandoahPhaseTimings::CodeCacheRoots, worker_id);\n+      ShenandoahEvacUpdateCodeCacheClosure cl;\n+      _nmethod_itr.nmethods_do(&cl);\n+    }\n+  }\n+};\n+\n+void ShenandoahConcurrentGC::op_strong_roots() {\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+  assert(heap->is_concurrent_strong_root_in_progress(), \"Checked by caller\");\n+  ShenandoahConcurrentRootsEvacUpdateTask task(ShenandoahPhaseTimings::conc_strong_roots);\n+  heap->workers()->run_task(&task);\n+  heap->set_concurrent_strong_root_in_progress(false);\n+}\n+\n+void ShenandoahConcurrentGC::op_cleanup_early() {\n+  ShenandoahHeap::heap()->free_set()->recycle_trash();\n+}\n+\n+void ShenandoahConcurrentGC::op_rendezvous_roots() {\n+  ShenandoahHeap::heap()->rendezvous_threads();\n+}\n+\n+void ShenandoahConcurrentGC::op_evacuate() {\n+  ShenandoahHeap::heap()->evacuate_collection_set(true \/*concurrent*\/);\n+}\n+\n+void ShenandoahConcurrentGC::op_init_updaterefs() {\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+  heap->set_evacuation_in_progress(false);\n+  heap->prepare_update_heap_references(true \/*concurrent*\/);\n+  heap->set_update_refs_in_progress(true);\n+\n+  if (ShenandoahPacing) {\n+    heap->pacer()->setup_for_updaterefs();\n+  }\n+}\n+\n+void ShenandoahConcurrentGC::op_updaterefs() {\n+  ShenandoahHeap::heap()->update_heap_references(true \/*concurrent*\/);\n+}\n+\n+class ShenandoahUpdateThreadClosure : public HandshakeClosure {\n+private:\n+  ShenandoahUpdateRefsClosure _cl;\n+public:\n+  ShenandoahUpdateThreadClosure();\n+  void do_thread(Thread* thread);\n+};\n+\n+ShenandoahUpdateThreadClosure::ShenandoahUpdateThreadClosure() :\n+  HandshakeClosure(\"Shenandoah Update Thread Roots\") {\n+}\n+\n+void ShenandoahUpdateThreadClosure::do_thread(Thread* thread) {\n+  if (thread->is_Java_thread()) {\n+    JavaThread* jt = thread->as_Java_thread();\n+    ResourceMark rm;\n+    jt->oops_do(&_cl, NULL);\n+  }\n+}\n+\n+void ShenandoahConcurrentGC::op_update_thread_roots() {\n+  ShenandoahUpdateThreadClosure cl;\n+  Handshake::execute(&cl);\n+}\n+\n+void ShenandoahConcurrentGC::op_final_updaterefs() {\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+  assert(ShenandoahSafepoint::is_at_shenandoah_safepoint(), \"must be at safepoint\");\n+  assert(!heap->_update_refs_iterator.has_next(), \"Should have finished update references\");\n+\n+  heap->finish_concurrent_roots();\n+\n+  \/\/ Clear cancelled GC, if set. On cancellation path, the block before would handle\n+  \/\/ everything.\n+  if (heap->cancelled_gc()) {\n+    heap->clear_cancelled_gc();\n+  }\n+\n+  \/\/ Has to be done before cset is clear\n+  if (ShenandoahVerify) {\n+    heap->verifier()->verify_roots_in_to_space();\n+  }\n+\n+  heap->update_heap_region_states(true \/*concurrent*\/);\n+\n+  heap->set_update_refs_in_progress(false);\n+  heap->set_has_forwarded_objects(false);\n+\n+  if (ShenandoahVerify) {\n+    heap->verifier()->verify_after_updaterefs();\n+  }\n+\n+  if (VerifyAfterGC) {\n+    Universe::verify();\n+  }\n+\n+  heap->rebuild_free_set(true \/*concurrent*\/);\n+}\n+\n+void ShenandoahConcurrentGC::op_cleanup_complete() {\n+  ShenandoahHeap::heap()->free_set()->recycle_trash();\n+}\n+\n+bool ShenandoahConcurrentGC::check_cancellation_and_abort(ShenandoahDegenPoint point) {\n+  if (ShenandoahHeap::heap()->cancelled_gc()) {\n+    _degen_point = point;\n+    return true;\n+  }\n+  return false;\n+}\n+\n+const char* ShenandoahConcurrentGC::init_mark_event_message() const {\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+  assert(!heap->has_forwarded_objects(), \"Should not have forwarded objects here\");\n+  if (heap->unload_classes()) {\n+    return \"Pause Init Mark (unload classes)\";\n+  } else {\n+    return \"Pause Init Mark\";\n+  }\n+}\n+\n+const char* ShenandoahConcurrentGC::final_mark_event_message() const {\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+  assert(!heap->has_forwarded_objects(), \"Should not have forwarded objects here\");\n+  if (heap->unload_classes()) {\n+    return \"Pause Final Mark (unload classes)\";\n+  } else {\n+    return \"Pause Final Mark\";\n+  }\n+}\n+\n+const char* ShenandoahConcurrentGC::conc_mark_event_message() const {\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+  assert(!heap->has_forwarded_objects(), \"Should not have forwarded objects here\");\n+  if (heap->unload_classes()) {\n+    return \"Concurrent marking (unload classes)\";\n+  } else {\n+    return \"Concurrent marking\";\n+  }\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahConcurrentGC.cpp","additions":955,"deletions":0,"binary":false,"changes":955,"status":"added"},{"patch":"@@ -0,0 +1,115 @@\n+\/*\n+ * Copyright (c) 2021, Red Hat, Inc. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_SHENANDOAH_SHENANDOAHCONCURRENTGC_HPP\n+#define SHARE_GC_SHENANDOAH_SHENANDOAHCONCURRENTGC_HPP\n+\n+#include \"gc\/shared\/gcCause.hpp\"\n+#include \"gc\/shenandoah\/shenandoahConcurrentMark.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGC.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeap.hpp\"\n+\n+class VM_ShenandoahInitMark;\n+class VM_ShenandoahFinalMarkStartEvac;\n+class VM_ShenandoahInitUpdateRefs;\n+class VM_ShenandoahFinalUpdateRefs;\n+\n+class ShenandoahConcurrentGC : public ShenandoahGC {\n+  friend class VM_ShenandoahInitMark;\n+  friend class VM_ShenandoahFinalMarkStartEvac;\n+  friend class VM_ShenandoahInitUpdateRefs;\n+  friend class VM_ShenandoahFinalUpdateRefs;\n+\n+private:\n+  ShenandoahConcurrentMark  _mark;\n+  ShenandoahDegenPoint      _degen_point;\n+\n+public:\n+  ShenandoahConcurrentGC();\n+  bool collect(GCCause::Cause cause);\n+  ShenandoahDegenPoint degen_point() const;\n+\n+  \/\/ Cancel on going concurrent GC\n+  static void cancel();\n+private:\n+  \/\/ Entry points to STW GC operations, these cause a related safepoint, that then\n+  \/\/ call the entry method below\n+  void vmop_entry_init_mark();\n+  void vmop_entry_final_mark();\n+  void vmop_entry_init_updaterefs();\n+  void vmop_entry_final_updaterefs();\n+\n+  \/\/ Entry methods to normally STW GC operations. These set up logging, monitoring\n+  \/\/ and workers for net VM operation\n+  void entry_init_mark();\n+  void entry_final_mark();\n+  void entry_init_updaterefs();\n+  void entry_final_updaterefs();\n+\n+  \/\/ Entry methods to normally concurrent GC operations. These set up logging, monitoring\n+  \/\/ for concurrent operation.\n+  void entry_reset();\n+  void entry_mark_roots();\n+  void entry_mark();\n+  void entry_weak_refs();\n+  void entry_weak_roots();\n+  void entry_class_unloading();\n+  void entry_strong_roots();\n+  void entry_cleanup_early();\n+  void entry_rendezvous_roots();\n+  void entry_evacuate();\n+  void entry_update_thread_roots();\n+  void entry_updaterefs();\n+  void entry_cleanup_complete();\n+\n+  \/\/ Actual work for the phases\n+  void op_reset();\n+  void op_init_mark();\n+  void op_mark_roots();\n+  void op_mark();\n+  void op_final_mark();\n+  void op_weak_refs();\n+  void op_weak_roots();\n+  void op_class_unloading();\n+  void op_strong_roots();\n+  void op_cleanup_early();\n+  void op_rendezvous_roots();\n+  void op_evacuate();\n+  void op_init_updaterefs();\n+  void op_updaterefs();\n+  void op_update_thread_roots();\n+  void op_final_updaterefs();\n+  void op_cleanup_complete();\n+\n+  \/\/ Messages for GC trace events, they have to be immortal for\n+  \/\/ passing around the logging\/tracing systems\n+  const char* init_mark_event_message() const;\n+  const char* final_mark_event_message() const;\n+  const char* conc_mark_event_message() const;\n+\n+  \/\/ Check GC cancellation and abort concurrent GC\n+  bool check_cancellation_and_abort(ShenandoahDegenPoint point);\n+};\n+\n+#endif \/\/ SHARE_GC_SHENANDOAH_SHENANDOAHCONCURRENTGC_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahConcurrentGC.hpp","additions":115,"deletions":0,"binary":false,"changes":115,"status":"added"},{"patch":"@@ -226,66 +226,0 @@\n-void ShenandoahConcurrentMark::update_roots(ShenandoahPhaseTimings::Phase root_phase) {\n-  assert(ShenandoahSafepoint::is_at_shenandoah_safepoint(), \"Must be at a safepoint\");\n-  assert(root_phase == ShenandoahPhaseTimings::full_gc_update_roots ||\n-         root_phase == ShenandoahPhaseTimings::degen_gc_update_roots,\n-         \"Only for these phases\");\n-\n-  ShenandoahGCPhase phase(root_phase);\n-\n-  bool check_alive = root_phase == ShenandoahPhaseTimings::degen_gc_update_roots;\n-\n-#if COMPILER2_OR_JVMCI\n-  DerivedPointerTable::clear();\n-#endif\n-\n-  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n-  WorkGang* workers = heap->workers();\n-  uint nworkers = workers->active_workers();\n-\n-  ShenandoahRootUpdater root_updater(nworkers, root_phase);\n-  ShenandoahUpdateRootsTask update_roots(&root_updater, check_alive);\n-  workers->run_task(&update_roots);\n-\n-#if COMPILER2_OR_JVMCI\n-  DerivedPointerTable::update_pointers();\n-#endif\n-}\n-\n-class ShenandoahUpdateThreadRootsTask : public AbstractGangTask {\n-private:\n-  ShenandoahThreadRoots           _thread_roots;\n-  ShenandoahPhaseTimings::Phase   _phase;\n-  ShenandoahGCWorkerPhase         _worker_phase;\n-public:\n-  ShenandoahUpdateThreadRootsTask(bool is_par, ShenandoahPhaseTimings::Phase phase) :\n-    AbstractGangTask(\"Shenandoah Update Thread Roots\"),\n-    _thread_roots(phase, is_par),\n-    _phase(phase),\n-    _worker_phase(phase) {}\n-\n-  void work(uint worker_id) {\n-    ShenandoahParallelWorkerSession worker_session(worker_id);\n-    ShenandoahUpdateRefsClosure cl;\n-    _thread_roots.oops_do(&cl, NULL, worker_id);\n-  }\n-};\n-\n-void ShenandoahConcurrentMark::update_thread_roots(ShenandoahPhaseTimings::Phase root_phase) {\n-  assert(ShenandoahSafepoint::is_at_shenandoah_safepoint(), \"Must be at a safepoint\");\n-\n-  ShenandoahGCPhase phase(root_phase);\n-\n-#if COMPILER2_OR_JVMCI\n-  DerivedPointerTable::clear();\n-#endif\n-  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n-  WorkGang* workers = heap->workers();\n-  bool is_par = workers->active_workers() > 1;\n-\n-  ShenandoahUpdateThreadRootsTask task(is_par, root_phase);\n-  workers->run_task(&task);\n-\n-#if COMPILER2_OR_JVMCI\n-  DerivedPointerTable::update_pointers();\n-#endif\n-}\n-\n@@ -360,0 +294,4 @@\n+\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+  heap->set_concurrent_mark_in_progress(false);\n+  heap->mark_complete_marking_context();\n@@ -371,1 +309,1 @@\n-  ShenandoahGCPhase phase(ShenandoahPhaseTimings::finish_queues);\n+  ShenandoahGCPhase phase(ShenandoahPhaseTimings::finish_mark);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahConcurrentMark.cpp","additions":5,"deletions":67,"binary":false,"changes":72,"status":"modified"},{"patch":"@@ -56,4 +56,0 @@\n-  \/\/ TODO: where to put them\n-  static void update_roots(ShenandoahPhaseTimings::Phase root_phase);\n-  static void update_thread_roots(ShenandoahPhaseTimings::Phase root_phase);\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahConcurrentMark.hpp","additions":0,"deletions":4,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -27,1 +27,0 @@\n-#include \"gc\/shenandoah\/shenandoahConcurrentMark.hpp\"\n@@ -29,0 +28,1 @@\n+#include \"gc\/shenandoah\/shenandoahConcurrentGC.hpp\"\n@@ -30,0 +30,1 @@\n+#include \"gc\/shenandoah\/shenandoahDegeneratedGC.hpp\"\n@@ -34,0 +35,1 @@\n+#include \"gc\/shenandoah\/shenandoahMarkCompact.hpp\"\n@@ -50,1 +52,1 @@\n-  _degen_point(ShenandoahHeap::_degenerated_outside_cycle),\n+  _degen_point(ShenandoahGC::_degenerated_outside_cycle),\n@@ -113,1 +115,1 @@\n-    ShenandoahHeap::ShenandoahDegenPoint degen_point = ShenandoahHeap::_degenerated_unset;\n+    ShenandoahGC::ShenandoahDegenPoint degen_point = ShenandoahGC::_degenerated_unset;\n@@ -123,1 +125,1 @@\n-      _degen_point = ShenandoahHeap::_degenerated_outside_cycle;\n+      _degen_point = ShenandoahGC::_degenerated_outside_cycle;\n@@ -387,2 +389,1 @@\n-\n-  if (check_cancellation_or_degen(ShenandoahHeap::_degenerated_outside_cycle)) return;\n+  if (check_cancellation_or_degen(ShenandoahGC::_degenerated_outside_cycle)) return;\n@@ -395,66 +396,5 @@\n-  \/\/ Reset for upcoming marking\n-  heap->entry_reset();\n-\n-  \/\/ Start initial mark under STW\n-  heap->vmop_entry_init_mark();\n-\n-  \/\/ Concurrent mark roots\n-  heap->entry_mark_roots();\n-  if (check_cancellation_or_degen(ShenandoahHeap::_degenerated_outside_cycle)) return;\n-\n-  \/\/ Continue concurrent mark\n-  heap->entry_mark();\n-  if (check_cancellation_or_degen(ShenandoahHeap::_degenerated_mark)) return;\n-\n-  \/\/ Complete marking under STW, and start evacuation\n-  heap->vmop_entry_final_mark();\n-\n-  \/\/ Process weak roots that might still point to regions that would be broken by cleanup\n-  if (heap->is_concurrent_weak_root_in_progress()) {\n-    heap->entry_weak_refs();\n-    heap->entry_weak_roots();\n-  }\n-\n-  \/\/ Final mark might have reclaimed some immediate garbage, kick cleanup to reclaim\n-  \/\/ the space. This would be the last action if there is nothing to evacuate.\n-  heap->entry_cleanup_early();\n-\n-  {\n-    ShenandoahHeapLocker locker(heap->lock());\n-    heap->free_set()->log_status();\n-  }\n-\n-  \/\/ Perform concurrent class unloading\n-  if (heap->is_concurrent_weak_root_in_progress() &&\n-      ShenandoahConcurrentRoots::should_do_concurrent_class_unloading()) {\n-    heap->entry_class_unloading();\n-  }\n-\n-  \/\/ Processing strong roots\n-  \/\/ This may be skipped if there is nothing to update\/evacuate.\n-  \/\/ If so, strong_root_in_progress would be unset.\n-  if (heap->is_concurrent_strong_root_in_progress()) {\n-    heap->entry_strong_roots();\n-  }\n-\n-  \/\/ Continue the cycle with evacuation and optional update-refs.\n-  \/\/ This may be skipped if there is nothing to evacuate.\n-  \/\/ If so, evac_in_progress would be unset by collection set preparation code.\n-  if (heap->is_evacuation_in_progress()) {\n-    \/\/ Concurrently evacuate\n-    heap->entry_evac();\n-    if (check_cancellation_or_degen(ShenandoahHeap::_degenerated_evac)) return;\n-\n-    \/\/ Perform update-refs phase.\n-    heap->vmop_entry_init_updaterefs();\n-    heap->entry_updaterefs();\n-    if (check_cancellation_or_degen(ShenandoahHeap::_degenerated_updaterefs)) return;\n-\n-    \/\/ Concurrent update thread roots\n-    heap->entry_update_thread_roots();\n-    if (check_cancellation_or_degen(ShenandoahHeap::_degenerated_updaterefs)) return;\n-\n-    heap->vmop_entry_final_updaterefs();\n-\n-    \/\/ Update references freed up collection set, kick the cleanup to reclaim the space.\n-    heap->entry_cleanup_complete();\n+  ShenandoahConcurrentGC gc;\n+  if (gc.collect(cause)) {\n+    \/\/ Cycle is complete\n+    heap->heuristics()->record_success_concurrent();\n+    heap->shenandoah_policy()->record_success_concurrent();\n@@ -462,5 +402,2 @@\n-    \/\/ Concurrent weak\/strong root flags are unset concurrently. We depend on updateref GC safepoints\n-    \/\/ to ensure the changes are visible to all mutators before gc cycle is completed.\n-    \/\/ In case of no evacuation, updateref GC safepoints are skipped. Therefore, we will need\n-    \/\/ to perform thread handshake to ensure their consistences.\n-    heap->entry_rendezvous_roots();\n+    assert(heap->cancelled_gc(), \"Must have been cancelled\");\n+    check_cancellation_or_degen(gc.degen_point());\n@@ -468,4 +405,0 @@\n-\n-  \/\/ Cycle is complete\n-  heap->heuristics()->record_success_concurrent();\n-  heap->shenandoah_policy()->record_success_concurrent();\n@@ -474,1 +407,1 @@\n-bool ShenandoahControlThread::check_cancellation_or_degen(ShenandoahHeap::ShenandoahDegenPoint point) {\n+bool ShenandoahControlThread::check_cancellation_or_degen(ShenandoahGC::ShenandoahDegenPoint point) {\n@@ -479,2 +412,2 @@\n-      assert (_degen_point == ShenandoahHeap::_degenerated_outside_cycle,\n-              \"Should not be set yet: %s\", ShenandoahHeap::degen_point_to_string(_degen_point));\n+      assert (_degen_point == ShenandoahGC::_degenerated_outside_cycle,\n+              \"Should not be set yet: %s\", ShenandoahGC::degen_point_to_string(_degen_point));\n@@ -496,2 +429,2 @@\n-  ShenandoahHeap* heap = ShenandoahHeap::heap();\n-  heap->vmop_entry_full(cause);\n+  ShenandoahMarkCompact gc;\n+  gc.collect(cause);\n@@ -499,0 +432,1 @@\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n@@ -503,2 +437,2 @@\n-void ShenandoahControlThread::service_stw_degenerated_cycle(GCCause::Cause cause, ShenandoahHeap::ShenandoahDegenPoint point) {\n-  assert (point != ShenandoahHeap::_degenerated_unset, \"Degenerated point should be set\");\n+void ShenandoahControlThread::service_stw_degenerated_cycle(GCCause::Cause cause, ShenandoahGC::ShenandoahDegenPoint point) {\n+  assert (point != ShenandoahGC::_degenerated_unset, \"Degenerated point should be set\");\n@@ -509,2 +443,2 @@\n-  ShenandoahHeap* heap = ShenandoahHeap::heap();\n-  heap->vmop_degenerated(point);\n+  ShenandoahDegenGC gc(point);\n+  gc.collect(cause);\n@@ -512,0 +446,1 @@\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahControlThread.cpp","additions":25,"deletions":90,"binary":false,"changes":115,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2013, 2019, Red Hat, Inc. All rights reserved.\n+ * Copyright (c) 2013, 2021, Red Hat, Inc. All rights reserved.\n@@ -30,0 +30,1 @@\n+#include \"gc\/shenandoah\/shenandoahGC.hpp\"\n@@ -94,1 +95,1 @@\n-  ShenandoahHeap::ShenandoahDegenPoint _degen_point;\n+  ShenandoahGC::ShenandoahDegenPoint _degen_point;\n@@ -102,1 +103,1 @@\n-  bool check_cancellation_or_degen(ShenandoahHeap::ShenandoahDegenPoint point);\n+  bool check_cancellation_or_degen(ShenandoahGC::ShenandoahDegenPoint point);\n@@ -105,1 +106,1 @@\n-  void service_stw_degenerated_cycle(GCCause::Cause cause, ShenandoahHeap::ShenandoahDegenPoint point);\n+  void service_stw_degenerated_cycle(GCCause::Cause cause, ShenandoahGC::ShenandoahDegenPoint point);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahControlThread.hpp","additions":5,"deletions":4,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -0,0 +1,346 @@\n+\/*\n+ * Copyright (c) 2021, Red Hat, Inc. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+\n+#include \"gc\/shared\/collectorCounters.hpp\"\n+\n+#include \"gc\/shenandoah\/heuristics\/shenandoahHeuristics.hpp\"\n+#include \"gc\/shenandoah\/shenandoahCollectorPolicy.hpp\"\n+#include \"gc\/shenandoah\/shenandoahConcurrentMark.hpp\"\n+#include \"gc\/shenandoah\/shenandoahDegeneratedGC.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeap.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahMarkCompact.hpp\"\n+#include \"gc\/shenandoah\/shenandoahMetrics.hpp\"\n+#include \"gc\/shenandoah\/shenandoahMonitoringSupport.hpp\"\n+#include \"gc\/shenandoah\/shenandoahOopClosures.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahRootProcessor.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahSTWMark.hpp\"\n+#include \"gc\/shenandoah\/shenandoahUtils.hpp\"\n+#include \"gc\/shenandoah\/shenandoahVerifier.hpp\"\n+#include \"gc\/shenandoah\/shenandoahWorkerPolicy.hpp\"\n+#include \"gc\/shenandoah\/shenandoahVMOperations.hpp\"\n+\n+#include \"runtime\/vmThread.hpp\"\n+#include \"utilities\/events.hpp\"\n+\n+ShenandoahDegenGC::ShenandoahDegenGC(ShenandoahDegenPoint degen_point) :\n+  ShenandoahGC(),\n+  _degen_point(degen_point) {\n+}\n+\n+bool ShenandoahDegenGC::collect(GCCause::Cause cause) {\n+  vmop_degenerated();\n+  return true;\n+}\n+\n+void ShenandoahDegenGC::vmop_degenerated() {\n+  TraceCollectorStats tcs(ShenandoahHeap::heap()->monitoring_support()->full_stw_collection_counters());\n+  ShenandoahTimingsTracker timing(ShenandoahPhaseTimings::degen_gc_gross);\n+  VM_ShenandoahDegeneratedGC degenerated_gc(this);\n+  VMThread::execute(&degenerated_gc);\n+}\n+\n+void ShenandoahDegenGC::entry_degenerated() {\n+  const char* msg = degen_event_message(_degen_point);\n+  ShenandoahPausePhase gc_phase(msg, ShenandoahPhaseTimings::degen_gc, true \/* log_heap_usage *\/);\n+  EventMark em(\"%s\", msg);\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+\n+  ShenandoahWorkerScope scope(heap->workers(),\n+                              ShenandoahWorkerPolicy::calc_workers_for_stw_degenerated(),\n+                              \"stw degenerated gc\");\n+\n+  heap->set_degenerated_gc_in_progress(true);\n+  op_degenerated();\n+  heap->set_degenerated_gc_in_progress(false);\n+}\n+\n+void ShenandoahDegenGC::op_degenerated() {\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+  \/\/ Degenerated GC is STW, but it can also fail. Current mechanics communicates\n+  \/\/ GC failure via cancelled_concgc() flag. So, if we detect the failure after\n+  \/\/ some phase, we have to upgrade the Degenerate GC to Full GC.\n+  heap->clear_cancelled_gc();\n+\n+  ShenandoahMetricsSnapshot metrics;\n+  metrics.snap_before();\n+\n+  switch (_degen_point) {\n+    \/\/ The cases below form the Duff's-like device: it describes the actual GC cycle,\n+    \/\/ but enters it at different points, depending on which concurrent phase had\n+    \/\/ degenerated.\n+\n+    case _degenerated_outside_cycle:\n+      \/\/ We have degenerated from outside the cycle, which means something is bad with\n+      \/\/ the heap, most probably heavy humongous fragmentation, or we are very low on free\n+      \/\/ space. It makes little sense to wait for Full GC to reclaim as much as it can, when\n+      \/\/ we can do the most aggressive degen cycle, which includes processing references and\n+      \/\/ class unloading, unless those features are explicitly disabled.\n+      \/\/\n+\n+      \/\/ Degenerated from concurrent root mark, reset the flag for STW mark\n+      if (heap->is_concurrent_mark_in_progress()) {\n+        ShenandoahConcurrentMark::cancel();\n+        heap->set_concurrent_mark_in_progress(false);\n+      }\n+\n+      \/\/ Note that we can only do this for \"outside-cycle\" degens, otherwise we would risk\n+      \/\/ changing the cycle parameters mid-cycle during concurrent -> degenerated handover.\n+      heap->set_unload_classes(heap->heuristics()->can_unload_classes());\n+\n+      op_reset();\n+\n+      \/\/ STW mark\n+      op_mark();\n+    case _degenerated_mark:\n+      \/\/ No fallthrough. Continue mark, handed over from concurrent mark\n+      if (_degen_point == ShenandoahDegenPoint::_degenerated_mark) {\n+        op_finish_mark();\n+      }\n+      assert(!heap->cancelled_gc(), \"STW mark can not OOM\");\n+\n+      \/* Degen select Collection Set. etc. *\/\n+      op_prepare_evacuation();\n+\n+      op_cleanup_early();\n+\n+    case _degenerated_evac:\n+      \/\/ If heuristics thinks we should do the cycle, this flag would be set,\n+      \/\/ and we can do evacuation. Otherwise, it would be the shortcut cycle.\n+      if (heap->is_evacuation_in_progress()) {\n+\n+        \/\/ Degeneration under oom-evac protocol might have left some objects in\n+        \/\/ collection set un-evacuated. Restart evacuation from the beginning to\n+        \/\/ capture all objects. For all the objects that are already evacuated,\n+        \/\/ it would be a simple check, which is supposed to be fast. This is also\n+        \/\/ safe to do even without degeneration, as CSet iterator is at beginning\n+        \/\/ in preparation for evacuation anyway.\n+        \/\/\n+        \/\/ Before doing that, we need to make sure we never had any cset-pinned\n+        \/\/ regions. This may happen if allocation failure happened when evacuating\n+        \/\/ the about-to-be-pinned object, oom-evac protocol left the object in\n+        \/\/ the collection set, and then the pin reached the cset region. If we continue\n+        \/\/ the cycle here, we would trash the cset and alive objects in it. To avoid\n+        \/\/ it, we fail degeneration right away and slide into Full GC to recover.\n+\n+        {\n+          heap->sync_pinned_region_status();\n+          heap->collection_set()->clear_current_index();\n+\n+          ShenandoahHeapRegion* r;\n+          while ((r = heap->collection_set()->next()) != NULL) {\n+            if (r->is_pinned()) {\n+              heap->cancel_gc(GCCause::_shenandoah_upgrade_to_full_gc);\n+              op_degenerated_fail();\n+              return;\n+            }\n+          }\n+\n+          heap->collection_set()->clear_current_index();\n+        }\n+        op_evacuate();\n+        if (heap->cancelled_gc()) {\n+          op_degenerated_fail();\n+          return;\n+        }\n+      }\n+\n+      \/\/ If heuristics thinks we should do the cycle, this flag would be set,\n+      \/\/ and we need to do update-refs. Otherwise, it would be the shortcut cycle.\n+      if (heap->has_forwarded_objects()) {\n+        op_init_updaterefs();\n+        assert(!heap->cancelled_gc(), \"STW reference update can not OOM\");\n+      }\n+\n+    case _degenerated_updaterefs:\n+      if (heap->has_forwarded_objects()) {\n+        op_updaterefs();\n+        op_update_roots();\n+        assert(!heap->cancelled_gc(), \"STW reference update can not OOM\");\n+      }\n+\n+      if (ShenandoahConcurrentRoots::can_do_concurrent_class_unloading()) {\n+         \/\/ Disarm nmethods that armed in concurrent cycle.\n+         \/\/ In above case, update roots should disarm them\n+         ShenandoahCodeRoots::disarm_nmethods();\n+      }\n+\n+      op_cleanup_complete();\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+  }\n+\n+  if (ShenandoahVerify) {\n+    heap->verifier()->verify_after_degenerated();\n+  }\n+\n+  if (VerifyAfterGC) {\n+    Universe::verify();\n+  }\n+\n+  metrics.snap_after();\n+\n+  \/\/ Check for futility and fail. There is no reason to do several back-to-back Degenerated cycles,\n+  \/\/ because that probably means the heap is overloaded and\/or fragmented.\n+  if (!metrics.is_good_progress()) {\n+    heap->notify_gc_no_progress();\n+    heap->cancel_gc(GCCause::_shenandoah_upgrade_to_full_gc);\n+    op_degenerated_futile();\n+  } else {\n+    heap->notify_gc_progress();\n+  }\n+}\n+\n+void ShenandoahDegenGC::op_reset() {\n+  ShenandoahHeap::heap()->prepare_gc();\n+}\n+\n+void ShenandoahDegenGC::op_mark() {\n+  assert(!ShenandoahHeap::heap()->is_concurrent_mark_in_progress(), \"Should be reset\");\n+  ShenandoahGCPhase phase(ShenandoahPhaseTimings::degen_gc_stw_mark);\n+  ShenandoahSTWMark mark(false \/*full gc*\/);\n+  mark.clear();\n+  mark.mark();\n+}\n+\n+void ShenandoahDegenGC::op_finish_mark() {\n+  ShenandoahConcurrentMark mark;\n+  mark.finish_mark();\n+}\n+\n+void ShenandoahDegenGC::op_prepare_evacuation() {\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+  if (ShenandoahVerify) {\n+    heap->verifier()->verify_roots_no_forwarded();\n+  }\n+\n+  \/\/ STW cleanup weak roots and unload classes\n+  heap->parallel_cleaning(false \/*full gc*\/);\n+  \/\/ Prepare regions and collection set\n+  heap->prepare_regions_and_collection_set(false \/*concurrent*\/);\n+\n+  if (!heap->collection_set()->is_empty()) {\n+    heap->set_evacuation_in_progress(true);\n+    heap->set_has_forwarded_objects(true);\n+\n+    if(ShenandoahVerify) {\n+      heap->verifier()->verify_during_evacuation();\n+    }\n+  } else {\n+    if (ShenandoahVerify) {\n+      heap->verifier()->verify_after_concmark();\n+    }\n+\n+    if (VerifyAfterGC) {\n+      Universe::verify();\n+    }\n+  }\n+}\n+\n+void ShenandoahDegenGC::op_cleanup_early() {\n+  ShenandoahHeap::heap()->recycle_trash();\n+}\n+\n+void ShenandoahDegenGC::op_evacuate() {\n+  ShenandoahGCPhase phase(ShenandoahPhaseTimings::degen_gc_stw_evac);\n+  ShenandoahHeap::heap()->evacuate_collection_set(false \/* concurrent*\/);\n+}\n+\n+void ShenandoahDegenGC::op_init_updaterefs() {\n+  \/\/ Evacuation has completed\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+  heap->set_evacuation_in_progress(false);\n+  heap->set_concurrent_weak_root_in_progress(false);\n+  heap->set_concurrent_strong_root_in_progress(false);\n+\n+  heap->prepare_update_heap_references(false \/*concurrent*\/);\n+  heap->set_update_refs_in_progress(true);\n+}\n+\n+void ShenandoahDegenGC::op_updaterefs() {\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+  ShenandoahGCPhase phase(ShenandoahPhaseTimings::degen_gc_updaterefs);\n+  \/\/ Handed over from concurrent update references phase\n+  heap->update_heap_references(false \/*concurrent*\/);\n+\n+  heap->set_update_refs_in_progress(false);\n+  heap->set_has_forwarded_objects(false);\n+}\n+\n+void ShenandoahDegenGC::op_update_roots() {\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+\n+  update_roots(false \/*full_gc*\/);\n+\n+  heap->update_heap_region_states(false \/*concurrent*\/);\n+\n+  if (ShenandoahVerify) {\n+    heap->verifier()->verify_after_updaterefs();\n+  }\n+\n+  if (VerifyAfterGC) {\n+    Universe::verify();\n+  }\n+\n+  heap->rebuild_free_set(false \/*concurrent*\/);\n+}\n+\n+void ShenandoahDegenGC::op_cleanup_complete() {\n+  ShenandoahGCPhase phase(ShenandoahPhaseTimings::degen_gc_cleanup_complete);\n+  ShenandoahHeap::heap()->recycle_trash();\n+}\n+\n+void ShenandoahDegenGC::op_degenerated_fail() {\n+  log_info(gc)(\"Cannot finish degeneration, upgrading to Full GC\");\n+  ShenandoahHeap::heap()->shenandoah_policy()->record_degenerated_upgrade_to_full();\n+\n+  ShenandoahMarkCompact full_gc;\n+  full_gc.op_full(GCCause::_shenandoah_upgrade_to_full_gc);\n+}\n+\n+void ShenandoahDegenGC::op_degenerated_futile() {\n+  ShenandoahHeap::heap()->shenandoah_policy()->record_degenerated_upgrade_to_full();\n+  ShenandoahMarkCompact full_gc;\n+  full_gc.op_full(GCCause::_shenandoah_upgrade_to_full_gc);\n+}\n+\n+const char* ShenandoahDegenGC::degen_event_message(ShenandoahDegenPoint point) const {\n+  switch (point) {\n+    case _degenerated_unset:\n+      return \"Pause Degenerated GC (<UNSET>)\";\n+    case _degenerated_outside_cycle:\n+      return \"Pause Degenerated GC (Outside of Cycle)\";\n+    case _degenerated_mark:\n+      return \"Pause Degenerated GC (Mark)\";\n+    case _degenerated_evac:\n+      return \"Pause Degenerated GC (Evacuation)\";\n+    case _degenerated_updaterefs:\n+      return \"Pause Degenerated GC (Update Refs)\";\n+    default:\n+      ShouldNotReachHere();\n+      return \"ERROR\";\n+  }\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahDegeneratedGC.cpp","additions":346,"deletions":0,"binary":false,"changes":346,"status":"added"},{"patch":"@@ -0,0 +1,74 @@\n+\/*\n+ * Copyright (c) 2021, Red Hat, Inc. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_SHENANDOAH_SHENANDOAHDEGENDGC_HPP\n+#define SHARE_GC_SHENANDOAH_SHENANDOAHDEGENDGC_HPP\n+\n+#include \"gc\/shenandoah\/shenandoahGC.hpp\"\n+\n+class VM_ShenandoahDegeneratedGC;\n+\n+class ShenandoahDegenGC : public ShenandoahGC {\n+  friend class VM_ShenandoahDegeneratedGC;\n+private:\n+  const ShenandoahDegenPoint  _degen_point;\n+\n+public:\n+  ShenandoahDegenGC(ShenandoahDegenPoint degen_point);\n+  bool collect(GCCause::Cause cause);\n+\n+private:\n+  void vmop_degenerated();\n+  void entry_degenerated();\n+  void op_degenerated();\n+\n+  \/\/ Reset for GC\n+  void op_reset();\n+  \/\/ STW mark, start from roots\n+  void op_mark();\n+  \/\/ Finish mark handed over from concurrent mark\n+  void op_finish_mark();\n+  \/\/ Prepare STW evacuation\n+  void op_prepare_evacuation();\n+  \/\/\n+  void op_cleanup_early();\n+  \/\/ STW evacuation\n+  void op_evacuate();\n+  \/\/ Prepare STW update references\n+  void op_init_updaterefs();\n+  \/\/ STW update references\n+  void op_updaterefs();\n+  \/\/ STW update roots\n+  void op_update_roots();\n+  \/\/\n+  void op_cleanup_complete();\n+\n+  \/\/ Fail handling\n+  void op_degenerated_futile();\n+  void op_degenerated_fail();\n+\n+  const char* degen_event_message(ShenandoahDegenPoint point) const;\n+};\n+\n+#endif \/\/ SHARE_GC_SHENANDOAH_SHENANDOAHDEGENDGC_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahDegeneratedGC.hpp","additions":74,"deletions":0,"binary":false,"changes":74,"status":"added"},{"patch":"@@ -0,0 +1,108 @@\n+\/*\n+ * Copyright (c) 2021, Red Hat, Inc. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+\n+#include \"gc\/shared\/workgroup.hpp\"\n+\n+#include \"gc\/shenandoah\/shenandoahClosures.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGC.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeap.hpp\"\n+#include \"gc\/shenandoah\/shenandoahPhaseTimings.hpp\"\n+#include \"gc\/shenandoah\/shenandoahRootProcessor.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahUtils.hpp\"\n+\n+const char* ShenandoahGC::degen_point_to_string(ShenandoahDegenPoint point) {\n+  switch(point) {\n+    case _degenerated_unset:\n+      return \"<UNSET>\";\n+    case _degenerated_outside_cycle:\n+      return \"Outside of Cycle\";\n+    case _degenerated_mark:\n+      return \"Mark\";\n+    case _degenerated_evac:\n+      return \"Evacuation\";\n+    case _degenerated_updaterefs:\n+      return \"Update References\";\n+    default:\n+      ShouldNotReachHere();\n+      return \"ERROR\";\n+   }\n+}\n+\n+class ShenandoahUpdateRootsTask : public AbstractGangTask {\n+private:\n+  ShenandoahRootUpdater*  _root_updater;\n+  bool                    _check_alive;\n+public:\n+  ShenandoahUpdateRootsTask(ShenandoahRootUpdater* root_updater, bool check_alive) :\n+    AbstractGangTask(\"Shenandoah Update Roots\"),\n+    _root_updater(root_updater),\n+    _check_alive(check_alive){\n+  }\n+\n+  void work(uint worker_id) {\n+    assert(ShenandoahSafepoint::is_at_shenandoah_safepoint(), \"Must be at a safepoint\");\n+    ShenandoahParallelWorkerSession worker_session(worker_id);\n+\n+    ShenandoahHeap* heap = ShenandoahHeap::heap();\n+    ShenandoahUpdateRefsClosure cl;\n+    if (_check_alive) {\n+      ShenandoahForwardedIsAliveClosure is_alive;\n+      _root_updater->roots_do<ShenandoahForwardedIsAliveClosure, ShenandoahUpdateRefsClosure>(worker_id, &is_alive, &cl);\n+    } else {\n+      AlwaysTrueClosure always_true;;\n+      _root_updater->roots_do<AlwaysTrueClosure, ShenandoahUpdateRefsClosure>(worker_id, &always_true, &cl);\n+    }\n+  }\n+};\n+\n+void ShenandoahGC::update_roots(bool full_gc) {\n+  assert(ShenandoahSafepoint::is_at_shenandoah_safepoint(), \"Must be at a safepoint\");\n+  assert(ShenandoahHeap::heap()->is_full_gc_in_progress() ||\n+         ShenandoahHeap::heap()->is_degenerated_gc_in_progress(),\n+         \"Only for degenerated GC and full GC\");\n+\n+  bool check_alive = !full_gc;\n+  ShenandoahPhaseTimings::Phase p = full_gc ?\n+                                    ShenandoahPhaseTimings::full_gc_update_roots :\n+                                    ShenandoahPhaseTimings::degen_gc_update_roots;\n+\n+  ShenandoahGCPhase phase(p);\n+#if COMPILER2_OR_JVMCI\n+  DerivedPointerTable::clear();\n+#endif\n+\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+  WorkGang* workers = heap->workers();\n+  uint nworkers = workers->active_workers();\n+\n+  ShenandoahRootUpdater root_updater(nworkers, p);\n+  ShenandoahUpdateRootsTask update_roots(&root_updater, check_alive);\n+  workers->run_task(&update_roots);\n+\n+#if COMPILER2_OR_JVMCI\n+  DerivedPointerTable::update_pointers();\n+#endif\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGC.cpp","additions":108,"deletions":0,"binary":false,"changes":108,"status":"added"},{"patch":"@@ -0,0 +1,66 @@\n+\/*\n+ * Copyright (c) 2021, Red Hat, Inc. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_SHENANDOAH_SHENANDOAHGC_HPP\n+#define SHARE_GC_SHENANDOAH_SHENANDOAHGC_HPP\n+\n+#include \"memory\/allocation.hpp\"\n+#include \"gc\/shared\/gcCause.hpp\"\n+\n+\/*\n+ * Base class of three Shenandoah GC modes\n+ *\n+ * The relationship of the GC modes:\n+ *\n+ *  GC Start  -------------------------------------------------> Complete\n+ *      |                                                           |\n+ *  Concurrent GC ------------------------------------------------->|\n+ *      |             | upgrade from concurrent GC                  |\n+ *  Degenerated GC ---v-------------------------------------------->|\n+ *      |                             | upgrade from degenerated GC |\n+ *  Full GC---------------------------v---------------------------->|\n+ *\/\n+\n+class ShenandoahHeap;\n+\n+class ShenandoahGC : public StackObj {\n+public:\n+  \/\/ Fail point from concurrent GC\n+  enum ShenandoahDegenPoint {\n+    _degenerated_unset,\n+    _degenerated_outside_cycle,\n+    _degenerated_mark,\n+    _degenerated_evac,\n+    _degenerated_updaterefs,\n+    _DEGENERATED_LIMIT\n+  };\n+\n+  virtual bool collect(GCCause::Cause cause) = 0;\n+  static const char* degen_point_to_string(ShenandoahDegenPoint point);\n+\n+protected:\n+  static void update_roots(bool full_gc);\n+};\n+\n+#endif\n\\ No newline at end of file\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGC.hpp","additions":66,"deletions":0,"binary":false,"changes":66,"status":"added"},{"patch":"@@ -996,0 +996,5 @@\n+void ShenandoahHeap::evacuate_collection_set(bool concurrent) {\n+  ShenandoahEvacuationTask task(this, _collection_set, concurrent);\n+  workers()->run_task(&task);\n+}\n+\n@@ -1572,36 +1577,4 @@\n-void ShenandoahHeap::op_init_mark() {\n-  assert(ShenandoahSafepoint::is_at_shenandoah_safepoint(), \"Should be at safepoint\");\n-  assert(Thread::current()->is_VM_thread(), \"can only do this in VMThread\");\n-\n-  assert(marking_context()->is_bitmap_clear(), \"need clear marking bitmap\");\n-  assert(!marking_context()->is_complete(), \"should not be complete\");\n-  assert(!has_forwarded_objects(), \"No forwarded objects on this path\");\n-\n-  if (ShenandoahVerify) {\n-    verifier()->verify_before_concmark();\n-  }\n-\n-  if (VerifyBeforeGC) {\n-    Universe::verify();\n-  }\n-\n-  set_concurrent_mark_in_progress(true);\n-\n-  \/\/ We need to reset all TLABs because they might be below the TAMS, and we need to mark\n-  \/\/ the objects in them. Do not let mutators allocate any new objects in their current TLABs.\n-  \/\/ It is also a good place to resize the TLAB sizes for future allocations.\n-  if (UseTLAB) {\n-    ShenandoahGCPhase phase(ShenandoahPhaseTimings::init_manage_tlabs);\n-    tlabs_retire(ResizeTLAB);\n-  }\n-\n-  {\n-    ShenandoahGCPhase phase(ShenandoahPhaseTimings::init_update_region_states);\n-    ShenandoahInitMarkUpdateRegionStateClosure cl;\n-    parallel_heap_region_iterate(&cl);\n-  }\n-\n-  \/\/ Weak reference processing\n-  ShenandoahReferenceProcessor* rp = ref_processor();\n-  rp->reset_thread_locals();\n-  rp->set_soft_reference_policy(soft_ref_policy()->should_clear_all_soft_refs());\n+void ShenandoahHeap::rendezvous_threads() {\n+  ShenandoahRendezvousClosure cl;\n+  Handshake::execute(&cl);\n+}\n@@ -1609,2 +1582,3 @@\n-  \/\/ Make above changes visible to worker threads\n-  OrderAccess::fence();\n+void ShenandoahHeap::recycle_trash() {\n+  free_set()->recycle_trash();\n+}\n@@ -1612,2 +1586,5 @@\n-  ShenandoahConcurrentMark mark;\n-  mark.mark_stw_roots();\n+class ShenandoahResetUpdateRegionStateClosure : public ShenandoahHeapRegionClosure {\n+private:\n+  ShenandoahMarkingContext* const _ctx;\n+public:\n+  ShenandoahResetUpdateRegionStateClosure() : _ctx(ShenandoahHeap::heap()->marking_context()) {}\n@@ -1615,2 +1592,7 @@\n-  if (ShenandoahPacing) {\n-    pacer()->setup_for_mark();\n+  void heap_region_do(ShenandoahHeapRegion* r) {\n+    if (r->is_active()) {\n+      \/\/ Reset live data and set TAMS optimistically. We would recheck these under the pause\n+      \/\/ anyway to capture any updates that happened since now.\n+      r->clear_live_data();\n+      _ctx->capture_top_at_mark_start(r);\n+    }\n@@ -1619,7 +1601,2 @@\n-  \/\/ Arm nmethods for concurrent marking. When a nmethod is about to be executed,\n-  \/\/ we need to make sure that all its metadata are marked. alternative is to remark\n-  \/\/ thread roots at final mark pause, but it can be potential latency killer.\n-  if (ShenandoahConcurrentRoots::should_do_concurrent_class_unloading()) {\n-    ShenandoahCodeRoots::arm_nmethods();\n-  }\n-}\n+  bool is_thread_safe() { return true; }\n+};\n@@ -1627,4 +1604,2 @@\n-void ShenandoahHeap::op_mark_roots() {\n-  ShenandoahConcurrentMark mark;\n-  mark.mark_concurrent_roots();\n-}\n+void ShenandoahHeap::prepare_gc() {\n+  reset_mark_bitmap();\n@@ -1632,3 +1607,2 @@\n-void ShenandoahHeap::op_mark() {\n-  ShenandoahConcurrentMark mark;\n-  mark.concurrent_mark();\n+  ShenandoahResetUpdateRegionStateClosure cl;\n+  parallel_heap_region_iterate(&cl);\n@@ -1684,84 +1658,2 @@\n-void ShenandoahHeap::op_final_mark() {\n-  assert(ShenandoahSafepoint::is_at_shenandoah_safepoint(), \"Should be at safepoint\");\n-  assert(!has_forwarded_objects(), \"No forwarded objects on this path\");\n-\n-  if (!cancelled_gc()) {\n-    finish_mark();\n-    prepare_evacuation();\n-  } else {\n-    \/\/ If this cycle was updating references, we need to keep the has_forwarded_objects\n-    \/\/ flag on, for subsequent phases to deal with it.\n-    ShenandoahConcurrentMark::cancel();\n-    set_concurrent_mark_in_progress(false);\n-  }\n-}\n-\n-void ShenandoahHeap::op_conc_evac() {\n-  ShenandoahEvacuationTask task(this, _collection_set, true);\n-  workers()->run_task(&task);\n-}\n-\n-class ShenandoahUpdateThreadClosure : public HandshakeClosure {\n-private:\n-  ShenandoahUpdateRefsClosure _cl;\n-public:\n-  ShenandoahUpdateThreadClosure();\n-  void do_thread(Thread* thread);\n-};\n-\n-ShenandoahUpdateThreadClosure::ShenandoahUpdateThreadClosure() :\n-  HandshakeClosure(\"Shenandoah Update Thread Roots\") {\n-}\n-\n-void ShenandoahUpdateThreadClosure::do_thread(Thread* thread) {\n-  if (thread->is_Java_thread()) {\n-    JavaThread* jt = thread->as_Java_thread();\n-    ResourceMark rm;\n-    jt->oops_do(&_cl, NULL);\n-  }\n-}\n-\n-void ShenandoahHeap::op_update_thread_roots() {\n-  ShenandoahUpdateThreadClosure cl;\n-  Handshake::execute(&cl);\n-}\n-\n-void ShenandoahHeap::op_stw_evac() {\n-  ShenandoahEvacuationTask task(this, _collection_set, false);\n-  workers()->run_task(&task);\n-}\n-\n-void ShenandoahHeap::op_updaterefs() {\n-  update_heap_references(true);\n-}\n-\n-void ShenandoahHeap::op_cleanup_early() {\n-  free_set()->recycle_trash();\n-}\n-\n-void ShenandoahHeap::op_cleanup_complete() {\n-  free_set()->recycle_trash();\n-}\n-\n-\/\/ Helpers\n-void ShenandoahHeap::finish_mark() {\n-  assert(!cancelled_gc(), \"Should not continue\");\n-  ShenandoahConcurrentMark mark;\n-  mark.finish_mark();\n-  \/\/ Marking is completed, deactivate SATB barrier\n-  set_concurrent_mark_in_progress(false);\n-  mark_complete_marking_context();\n-}\n-\n-void ShenandoahHeap::prepare_evacuation() {\n-  \/\/ Notify JVMTI that the tagmap table will need cleaning.\n-  JvmtiTagMap::set_needs_cleaning();\n-\n-  if (is_degenerated_gc_in_progress()) {\n-    parallel_cleaning(false \/* full gc*\/);\n-  }\n-\n-  if (ShenandoahVerify) {\n-    verifier()->verify_roots_no_forwarded();\n-  }\n-\n+void ShenandoahHeap::prepare_regions_and_collection_set(bool concurrent) {\n+  assert(!is_full_gc_in_progress(), \"Only for concurrent and degenerated GC\");\n@@ -1769,1 +1661,2 @@\n-    ShenandoahGCPhase phase(ShenandoahPhaseTimings::final_update_region_states);\n+    ShenandoahGCPhase phase(concurrent ? ShenandoahPhaseTimings::final_update_region_states :\n+                                         ShenandoahPhaseTimings::degen_gc_final_update_region_states);\n@@ -1782,1 +1675,2 @@\n-    ShenandoahGCPhase phase(ShenandoahPhaseTimings::final_manage_labs);\n+    ShenandoahGCPhase phase(concurrent ? ShenandoahPhaseTimings::final_manage_labs :\n+                                         ShenandoahPhaseTimings::degen_gc_final_manage_labs);\n@@ -1787,1 +1681,2 @@\n-    ShenandoahGCPhase phase(ShenandoahPhaseTimings::choose_cset);\n+    ShenandoahGCPhase phase(concurrent ? ShenandoahPhaseTimings::choose_cset :\n+                                         ShenandoahPhaseTimings::degen_gc_choose_cset);\n@@ -1794,1 +1689,2 @@\n-    ShenandoahGCPhase phase(ShenandoahPhaseTimings::final_rebuild_freeset);\n+    ShenandoahGCPhase phase(concurrent ? ShenandoahPhaseTimings::final_rebuild_freeset :\n+                                         ShenandoahPhaseTimings::degen_gc_final_rebuild_freeset);\n@@ -1798,49 +1694,0 @@\n-\n-  if (!is_degenerated_gc_in_progress()) {\n-    prepare_concurrent_roots();\n-    prepare_concurrent_unloading();\n-  }\n-\n-  \/\/ If collection set has candidates, start evacuation.\n-  \/\/ Otherwise, bypass the rest of the cycle.\n-  if (!collection_set()->is_empty()) {\n-    ShenandoahGCPhase init_evac(ShenandoahPhaseTimings::init_evac);\n-\n-    if (ShenandoahVerify) {\n-      verifier()->verify_before_evacuation();\n-    }\n-\n-    set_evacuation_in_progress(true);\n-    \/\/ From here on, we need to update references.\n-    set_has_forwarded_objects(true);\n-\n-    if (!is_degenerated_gc_in_progress()) {\n-      \/\/ Arm nmethods for concurrent codecache processing.\n-      ShenandoahCodeRoots::arm_nmethods();\n-      evacuate_and_update_roots();\n-    }\n-\n-    \/\/ Notify JVMTI that oops are changed.\n-    JvmtiTagMap::set_needs_rehashing();\n-\n-    if (ShenandoahPacing) {\n-      pacer()->setup_for_evac();\n-    }\n-\n-    if (ShenandoahVerify) {\n-      \/\/ If OOM while evacuating\/updating of roots, there is no guarantee of their consistencies\n-      if (!cancelled_gc()) {\n-        \/\/ We only evacuate\/update thread at this pause\n-        verifier()->verify_roots_no_forwarded(ShenandoahRootVerifier::ThreadRoots);\n-      }\n-      verifier()->verify_during_evacuation();\n-    }\n-  } else {\n-    if (ShenandoahVerify) {\n-      verifier()->verify_after_concmark();\n-    }\n-\n-    if (VerifyAfterGC) {\n-      Universe::verify();\n-    }\n-  }\n@@ -1849,214 +1696,3 @@\n-class ShenandoahEvacUpdateCodeCacheClosure : public NMethodClosure {\n-private:\n-  BarrierSetNMethod* const               _bs;\n-  ShenandoahEvacuateUpdateRootsClosure<> _cl;\n-\n-public:\n-  ShenandoahEvacUpdateCodeCacheClosure() :\n-    _bs(BarrierSet::barrier_set()->barrier_set_nmethod()),\n-    _cl() {\n-  }\n-\n-  void do_nmethod(nmethod* n) {\n-    ShenandoahNMethod* data = ShenandoahNMethod::gc_data(n);\n-    ShenandoahReentrantLocker locker(data->lock());\n-    \/\/ Setup EvacOOM scope below reentrant lock to avoid deadlock with\n-    \/\/ nmethod_entry_barrier\n-    ShenandoahEvacOOMScope oom;\n-    data->oops_do(&_cl, true\/*fix relocation*\/);\n-    _bs->disarm(n);\n-  }\n-};\n-\n-class ShenandoahConcurrentRootsEvacUpdateTask : public AbstractGangTask {\n-private:\n-  ShenandoahPhaseTimings::Phase                 _phase;\n-  ShenandoahVMRoots<true \/*concurrent*\/>        _vm_roots;\n-  ShenandoahClassLoaderDataRoots<true \/*concurrent*\/, false \/*single threaded*\/> _cld_roots;\n-  ShenandoahConcurrentNMethodIterator           _nmethod_itr;\n-  bool                                          _process_codecache;\n-\n-public:\n-  ShenandoahConcurrentRootsEvacUpdateTask(ShenandoahPhaseTimings::Phase phase) :\n-    AbstractGangTask(\"Shenandoah Evacuate\/Update Concurrent Strong Roots\"),\n-    _phase(phase),\n-    _vm_roots(phase),\n-    _cld_roots(phase, ShenandoahHeap::heap()->workers()->active_workers()),\n-    _nmethod_itr(ShenandoahCodeRoots::table()),\n-    _process_codecache(!ShenandoahConcurrentRoots::should_do_concurrent_class_unloading()) {\n-    if (_process_codecache) {\n-      MutexLocker mu(CodeCache_lock, Mutex::_no_safepoint_check_flag);\n-      _nmethod_itr.nmethods_do_begin();\n-    }\n-  }\n-\n-  ~ShenandoahConcurrentRootsEvacUpdateTask() {\n-    if (_process_codecache) {\n-      MutexLocker mu(CodeCache_lock, Mutex::_no_safepoint_check_flag);\n-      _nmethod_itr.nmethods_do_end();\n-    }\n-  }\n-\n-  void work(uint worker_id) {\n-    ShenandoahConcurrentWorkerSession worker_session(worker_id);\n-    {\n-      ShenandoahEvacOOMScope oom;\n-      {\n-        \/\/ vm_roots and weak_roots are OopStorage backed roots, concurrent iteration\n-        \/\/ may race against OopStorage::release() calls.\n-        ShenandoahEvacUpdateOopStorageRootsClosure cl;\n-        _vm_roots.oops_do<ShenandoahEvacUpdateOopStorageRootsClosure>(&cl, worker_id);\n-      }\n-\n-      {\n-        ShenandoahEvacuateUpdateRootsClosure<> cl;\n-        CLDToOopClosure clds(&cl, ClassLoaderData::_claim_strong);\n-        _cld_roots.cld_do(&clds, worker_id);\n-      }\n-    }\n-\n-    \/\/ Cannot setup ShenandoahEvacOOMScope here, due to potential deadlock with nmethod_entry_barrier.\n-    if (_process_codecache) {\n-      ShenandoahWorkerTimingsTracker timer(_phase, ShenandoahPhaseTimings::CodeCacheRoots, worker_id);\n-      ShenandoahEvacUpdateCodeCacheClosure cl;\n-      _nmethod_itr.nmethods_do(&cl);\n-    }\n-  }\n-};\n-\n-class ShenandoahEvacUpdateCleanupOopStorageRootsClosure : public BasicOopIterateClosure {\n-private:\n-  ShenandoahHeap* const _heap;\n-  ShenandoahMarkingContext* const _mark_context;\n-  bool  _evac_in_progress;\n-  Thread* const _thread;\n-\n-public:\n-  ShenandoahEvacUpdateCleanupOopStorageRootsClosure();\n-  void do_oop(oop* p);\n-  void do_oop(narrowOop* p);\n-};\n-\n-ShenandoahEvacUpdateCleanupOopStorageRootsClosure::ShenandoahEvacUpdateCleanupOopStorageRootsClosure() :\n-  _heap(ShenandoahHeap::heap()),\n-  _mark_context(ShenandoahHeap::heap()->marking_context()),\n-  _evac_in_progress(ShenandoahHeap::heap()->is_evacuation_in_progress()),\n-  _thread(Thread::current()) {\n-}\n-\n-void ShenandoahEvacUpdateCleanupOopStorageRootsClosure::do_oop(oop* p) {\n-  const oop obj = RawAccess<>::oop_load(p);\n-  if (!CompressedOops::is_null(obj)) {\n-    if (!_mark_context->is_marked(obj)) {\n-      shenandoah_assert_correct(p, obj);\n-      Atomic::cmpxchg(p, obj, oop(NULL));\n-    } else if (_evac_in_progress && _heap->in_collection_set(obj)) {\n-      oop resolved = ShenandoahBarrierSet::resolve_forwarded_not_null(obj);\n-      if (resolved == obj) {\n-        resolved = _heap->evacuate_object(obj, _thread);\n-      }\n-      Atomic::cmpxchg(p, obj, resolved);\n-      assert(_heap->cancelled_gc() ||\n-             _mark_context->is_marked(resolved) && !_heap->in_collection_set(resolved),\n-             \"Sanity\");\n-    }\n-  }\n-}\n-\n-void ShenandoahEvacUpdateCleanupOopStorageRootsClosure::do_oop(narrowOop* p) {\n-  ShouldNotReachHere();\n-}\n-\n-class ShenandoahIsCLDAliveClosure : public CLDClosure {\n-public:\n-  void do_cld(ClassLoaderData* cld) {\n-    cld->is_alive();\n-  }\n-};\n-\n-class ShenandoahIsNMethodAliveClosure: public NMethodClosure {\n-public:\n-  void do_nmethod(nmethod* n) {\n-    n->is_unloading();\n-  }\n-};\n-\n-\/\/ This task not only evacuates\/updates marked weak roots, but also \"NULL\"\n-\/\/ dead weak roots.\n-class ShenandoahConcurrentWeakRootsEvacUpdateTask : public AbstractGangTask {\n-private:\n-  ShenandoahVMWeakRoots<true \/*concurrent*\/> _vm_roots;\n-\n-  \/\/ Roots related to concurrent class unloading\n-  ShenandoahClassLoaderDataRoots<true \/* concurrent *\/, true \/* single thread*\/>\n-                                             _cld_roots;\n-  ShenandoahConcurrentNMethodIterator        _nmethod_itr;\n-  ShenandoahConcurrentStringDedupRoots       _dedup_roots;\n-  bool                                       _concurrent_class_unloading;\n-\n-public:\n-  ShenandoahConcurrentWeakRootsEvacUpdateTask(ShenandoahPhaseTimings::Phase phase) :\n-    AbstractGangTask(\"Shenandoah Evacuate\/Update Concurrent Weak Roots\"),\n-    _vm_roots(phase),\n-    _cld_roots(phase, ShenandoahHeap::heap()->workers()->active_workers()),\n-    _nmethod_itr(ShenandoahCodeRoots::table()),\n-    _dedup_roots(phase),\n-    _concurrent_class_unloading(ShenandoahConcurrentRoots::should_do_concurrent_class_unloading()) {\n-    if (_concurrent_class_unloading) {\n-      MutexLocker mu(CodeCache_lock, Mutex::_no_safepoint_check_flag);\n-      _nmethod_itr.nmethods_do_begin();\n-    }\n-\n-    _dedup_roots.prologue();\n-  }\n-\n-  ~ShenandoahConcurrentWeakRootsEvacUpdateTask() {\n-    _dedup_roots.epilogue();\n-\n-    if (_concurrent_class_unloading) {\n-      MutexLocker mu(CodeCache_lock, Mutex::_no_safepoint_check_flag);\n-      _nmethod_itr.nmethods_do_end();\n-    }\n-    \/\/ Notify runtime data structures of potentially dead oops\n-    _vm_roots.report_num_dead();\n-  }\n-\n-  void work(uint worker_id) {\n-    ShenandoahConcurrentWorkerSession worker_session(worker_id);\n-    {\n-      ShenandoahEvacOOMScope oom;\n-      \/\/ jni_roots and weak_roots are OopStorage backed roots, concurrent iteration\n-      \/\/ may race against OopStorage::release() calls.\n-      ShenandoahEvacUpdateCleanupOopStorageRootsClosure cl;\n-      _vm_roots.oops_do(&cl, worker_id);\n-\n-      \/\/ String dedup weak roots\n-      ShenandoahForwardedIsAliveClosure is_alive;\n-      ShenandoahEvacuateUpdateRootsClosure<MO_RELEASE> keep_alive;\n-      _dedup_roots.oops_do(&is_alive, &keep_alive, worker_id);\n-    }\n-\n-    \/\/ If we are going to perform concurrent class unloading later on, we need to\n-    \/\/ cleanup the weak oops in CLD and determinate nmethod's unloading state, so that we\n-    \/\/ can cleanup immediate garbage sooner.\n-    if (_concurrent_class_unloading) {\n-      \/\/ Applies ShenandoahIsCLDAlive closure to CLDs, native barrier will either NULL the\n-      \/\/ CLD's holder or evacuate it.\n-      ShenandoahIsCLDAliveClosure is_cld_alive;\n-      _cld_roots.cld_do(&is_cld_alive, worker_id);\n-\n-      \/\/ Applies ShenandoahIsNMethodAliveClosure to registered nmethods.\n-      \/\/ The closure calls nmethod->is_unloading(). The is_unloading\n-      \/\/ state is cached, therefore, during concurrent class unloading phase,\n-      \/\/ we will not touch the metadata of unloading nmethods\n-      ShenandoahIsNMethodAliveClosure is_nmethod_alive;\n-      _nmethod_itr.nmethods_do(&is_nmethod_alive);\n-    }\n-  }\n-};\n-\n-void ShenandoahHeap::op_weak_refs() {\n-  \/\/ Concurrent weak refs processing\n-  ShenandoahTimingsTracker t(ShenandoahPhaseTimings::conc_weak_refs_work);\n-  ShenandoahGCWorkerPhase worker_phase(ShenandoahPhaseTimings::conc_weak_refs_work);\n-  ref_processor()->process_references(ShenandoahPhaseTimings::conc_weak_refs_work, workers(), true \/* concurrent *\/);\n+void ShenandoahHeap::do_class_unloading() {\n+  _unloader.unload();\n+  set_concurrent_weak_root_in_progress(false);\n@@ -2074,232 +1710,2 @@\n-void ShenandoahHeap::op_weak_roots() {\n-  if (is_concurrent_weak_root_in_progress()) {\n-    \/\/ Concurrent weak root processing\n-    {\n-      ShenandoahTimingsTracker t(ShenandoahPhaseTimings::conc_weak_roots_work);\n-      ShenandoahGCWorkerPhase worker_phase(ShenandoahPhaseTimings::conc_weak_roots_work);\n-      ShenandoahConcurrentWeakRootsEvacUpdateTask task(ShenandoahPhaseTimings::conc_weak_roots_work);\n-      workers()->run_task(&task);\n-    }\n-\n-    \/\/ Perform handshake to flush out dead oops\n-    {\n-      ShenandoahTimingsTracker t(ShenandoahPhaseTimings::conc_weak_roots_rendezvous);\n-      rendezvous_threads();\n-    }\n-\n-    if (!ShenandoahConcurrentRoots::should_do_concurrent_class_unloading()) {\n-      set_concurrent_weak_root_in_progress(false);\n-    }\n-  }\n-}\n-\n-void ShenandoahHeap::op_class_unloading() {\n-  assert (is_concurrent_weak_root_in_progress() &&\n-          ShenandoahConcurrentRoots::should_do_concurrent_class_unloading(),\n-          \"Checked by caller\");\n-  _unloader.unload();\n-  set_concurrent_weak_root_in_progress(false);\n-}\n-\n-void ShenandoahHeap::op_strong_roots() {\n-  assert(is_concurrent_strong_root_in_progress(), \"Checked by caller\");\n-  ShenandoahConcurrentRootsEvacUpdateTask task(ShenandoahPhaseTimings::conc_strong_roots);\n-  workers()->run_task(&task);\n-  set_concurrent_strong_root_in_progress(false);\n-}\n-\n-void ShenandoahHeap::op_rendezvous_roots() {\n-  rendezvous_threads();\n-}\n-\n-void ShenandoahHeap::rendezvous_threads() {\n-  ShenandoahRendezvousClosure cl;\n-  Handshake::execute(&cl);\n-}\n-\n-class ShenandoahResetUpdateRegionStateClosure : public ShenandoahHeapRegionClosure {\n-private:\n-  ShenandoahMarkingContext* const _ctx;\n-public:\n-  ShenandoahResetUpdateRegionStateClosure() : _ctx(ShenandoahHeap::heap()->marking_context()) {}\n-\n-  void heap_region_do(ShenandoahHeapRegion* r) {\n-    if (r->is_active()) {\n-      \/\/ Reset live data and set TAMS optimistically. We would recheck these under the pause\n-      \/\/ anyway to capture any updates that happened since now.\n-      r->clear_live_data();\n-      _ctx->capture_top_at_mark_start(r);\n-    }\n-  }\n-\n-  bool is_thread_safe() { return true; }\n-};\n-\n-void ShenandoahHeap::op_reset() {\n-  if (ShenandoahPacing) {\n-    pacer()->setup_for_reset();\n-  }\n-  reset_mark_bitmap();\n-\n-  ShenandoahResetUpdateRegionStateClosure cl;\n-  parallel_heap_region_iterate(&cl);\n-}\n-\n-void ShenandoahHeap::op_full(GCCause::Cause cause) {\n-  ShenandoahMetricsSnapshot metrics;\n-  metrics.snap_before();\n-\n-  ShenandoahMarkCompact full_gc;\n-  full_gc.initialize(_gc_timer);\n-  full_gc.do_it(cause);\n-\n-  metrics.snap_after();\n-\n-  if (metrics.is_good_progress()) {\n-    _progress_last_gc.set();\n-  } else {\n-    \/\/ Nothing to do. Tell the allocation path that we have failed to make\n-    \/\/ progress, and it can finally fail.\n-    _progress_last_gc.unset();\n-  }\n-}\n-\n-void ShenandoahHeap::op_degenerated(ShenandoahDegenPoint point) {\n-  \/\/ Degenerated GC is STW, but it can also fail. Current mechanics communicates\n-  \/\/ GC failure via cancelled_concgc() flag. So, if we detect the failure after\n-  \/\/ some phase, we have to upgrade the Degenerate GC to Full GC.\n-\n-  clear_cancelled_gc();\n-\n-  ShenandoahMetricsSnapshot metrics;\n-  metrics.snap_before();\n-\n-  switch (point) {\n-    \/\/ The cases below form the Duff's-like device: it describes the actual GC cycle,\n-    \/\/ but enters it at different points, depending on which concurrent phase had\n-    \/\/ degenerated.\n-\n-    case _degenerated_outside_cycle:\n-      \/\/ We have degenerated from outside the cycle, which means something is bad with\n-      \/\/ the heap, most probably heavy humongous fragmentation, or we are very low on free\n-      \/\/ space. It makes little sense to wait for Full GC to reclaim as much as it can, when\n-      \/\/ we can do the most aggressive degen cycle, which includes processing references and\n-      \/\/ class unloading, unless those features are explicitly disabled.\n-      \/\/\n-      \/\/ Note that we can only do this for \"outside-cycle\" degens, otherwise we would risk\n-      \/\/ changing the cycle parameters mid-cycle during concurrent -> degenerated handover.\n-\n-      \/\/ Degenerated from concurrent mark roots, reset for STW mark\n-      if (is_concurrent_mark_in_progress()) {\n-        ShenandoahConcurrentMark::cancel();\n-        set_concurrent_mark_in_progress(false);\n-      }\n-\n-      set_unload_classes(heuristics()->can_unload_classes());\n-\n-      op_reset();\n-\n-      \/\/ STW root scan\n-      {\n-        assert(!has_forwarded_objects(), \"Should not have forwarded heap\");\n-        ShenandoahSTWMark mark(false \/*full_gc*\/);\n-        mark.mark();\n-        assert(!cancelled_gc(), \"STW mark can not OOM\");\n-      }\n-    case _degenerated_mark:\n-      if (point == _degenerated_mark) {\n-        finish_mark();\n-      }\n-      prepare_evacuation();\n-\n-      if (cancelled_gc()) {\n-        op_degenerated_fail();\n-        return;\n-      }\n-\n-      if (!has_forwarded_objects() && ShenandoahConcurrentRoots::can_do_concurrent_class_unloading()) {\n-        \/\/ Disarm nmethods that armed for concurrent mark. On normal cycle, it would\n-        \/\/ be disarmed while conc-roots phase is running.\n-        \/\/ TODO: Call op_conc_roots() here instead\n-        ShenandoahCodeRoots::disarm_nmethods();\n-      }\n-\n-      op_cleanup_early();\n-\n-    case _degenerated_evac:\n-      \/\/ If heuristics thinks we should do the cycle, this flag would be set,\n-      \/\/ and we can do evacuation. Otherwise, it would be the shortcut cycle.\n-      if (is_evacuation_in_progress()) {\n-\n-        \/\/ Degeneration under oom-evac protocol might have left some objects in\n-        \/\/ collection set un-evacuated. Restart evacuation from the beginning to\n-        \/\/ capture all objects. For all the objects that are already evacuated,\n-        \/\/ it would be a simple check, which is supposed to be fast. This is also\n-        \/\/ safe to do even without degeneration, as CSet iterator is at beginning\n-        \/\/ in preparation for evacuation anyway.\n-        \/\/\n-        \/\/ Before doing that, we need to make sure we never had any cset-pinned\n-        \/\/ regions. This may happen if allocation failure happened when evacuating\n-        \/\/ the about-to-be-pinned object, oom-evac protocol left the object in\n-        \/\/ the collection set, and then the pin reached the cset region. If we continue\n-        \/\/ the cycle here, we would trash the cset and alive objects in it. To avoid\n-        \/\/ it, we fail degeneration right away and slide into Full GC to recover.\n-\n-        {\n-          sync_pinned_region_status();\n-          collection_set()->clear_current_index();\n-\n-          ShenandoahHeapRegion* r;\n-          while ((r = collection_set()->next()) != NULL) {\n-            if (r->is_pinned()) {\n-              cancel_gc(GCCause::_shenandoah_upgrade_to_full_gc);\n-              op_degenerated_fail();\n-              return;\n-            }\n-          }\n-\n-          collection_set()->clear_current_index();\n-        }\n-\n-        op_stw_evac();\n-        if (cancelled_gc()) {\n-          op_degenerated_fail();\n-          return;\n-        }\n-      }\n-\n-      \/\/ If heuristics thinks we should do the cycle, this flag would be set,\n-      \/\/ and we need to do update-refs. Otherwise, it would be the shortcut cycle.\n-      if (has_forwarded_objects()) {\n-        op_init_updaterefs();\n-        if (cancelled_gc()) {\n-          op_degenerated_fail();\n-          return;\n-        }\n-      }\n-\n-    case _degenerated_updaterefs:\n-      if (has_forwarded_objects()) {\n-        op_final_updaterefs();\n-        if (cancelled_gc()) {\n-          op_degenerated_fail();\n-          return;\n-        }\n-      }\n-\n-      op_cleanup_complete();\n-      break;\n-\n-    default:\n-      ShouldNotReachHere();\n-  }\n-\n-  if (ShenandoahVerify) {\n-    verifier()->verify_after_degenerated();\n-  }\n-\n-  if (VerifyAfterGC) {\n-    Universe::verify();\n-  }\n-\n-  metrics.snap_after();\n+void ShenandoahHeap::prepare_update_heap_references(bool concurrent) {\n+  assert(ShenandoahSafepoint::is_at_shenandoah_safepoint(), \"must be at safepoint\");\n@@ -2307,8 +1713,8 @@\n-  \/\/ Check for futility and fail. There is no reason to do several back-to-back Degenerated cycles,\n-  \/\/ because that probably means the heap is overloaded and\/or fragmented.\n-  if (!metrics.is_good_progress()) {\n-    _progress_last_gc.unset();\n-    cancel_gc(GCCause::_shenandoah_upgrade_to_full_gc);\n-    op_degenerated_futile();\n-  } else {\n-    _progress_last_gc.set();\n+  \/\/ Evacuation is over, no GCLABs are needed anymore. GCLABs are under URWM, so we need to\n+  \/\/ make them parsable for update code to work correctly. Plus, we can compute new sizes\n+  \/\/ for future GCLABs here.\n+  if (UseTLAB) {\n+    ShenandoahGCPhase phase(concurrent ?\n+                            ShenandoahPhaseTimings::init_update_refs_manage_gclabs :\n+                            ShenandoahPhaseTimings::degen_gc_init_update_refs_manage_gclabs);\n+    gclabs_retire(ResizeTLAB);\n@@ -2316,7 +1722,0 @@\n-}\n-\n-void ShenandoahHeap::op_degenerated_fail() {\n-  log_info(gc)(\"Cannot finish degeneration, upgrading to Full GC\");\n-  shenandoah_policy()->record_degenerated_upgrade_to_full();\n-  op_full(GCCause::_shenandoah_upgrade_to_full_gc);\n-}\n@@ -2324,3 +1723,1 @@\n-void ShenandoahHeap::op_degenerated_futile() {\n-  shenandoah_policy()->record_degenerated_upgrade_to_full();\n-  op_full(GCCause::_shenandoah_upgrade_to_full_gc);\n+  _update_refs_iterator.reset();\n@@ -2626,8 +2023,3 @@\n-  if (ShenandoahConcurrentRoots::should_do_concurrent_roots()) {\n-    set_concurrent_strong_root_in_progress(!collection_set()->is_empty());\n-    set_concurrent_weak_root_in_progress(true);\n-  }\n-}\n-\n-void ShenandoahHeap::prepare_concurrent_unloading() {\n-  assert(SafepointSynchronize::is_at_safepoint(), \"Must be at a safepoint\");\n+  assert(!is_stw_gc_in_progress(), \"Only concurrent GC\");\n+  set_concurrent_strong_root_in_progress(!collection_set()->is_empty());\n+  set_concurrent_weak_root_in_progress(true);\n@@ -2639,1 +2031,1 @@\n-void ShenandoahHeap::finish_concurrent_unloading() {\n+void ShenandoahHeap::finish_concurrent_roots() {\n@@ -2641,0 +2033,1 @@\n+  assert(!is_stw_gc_in_progress(), \"Only concurrent GC\");\n@@ -2723,0 +2116,2 @@\n+  assert(!is_full_gc_in_progress(), \"Only for concurrent and degenerated GC\");\n+\n@@ -2727,28 +2122,0 @@\n-void ShenandoahHeap::op_init_updaterefs() {\n-  assert(ShenandoahSafepoint::is_at_shenandoah_safepoint(), \"must be at safepoint\");\n-\n-  set_evacuation_in_progress(false);\n-\n-  \/\/ Evacuation is over, no GCLABs are needed anymore. GCLABs are under URWM, so we need to\n-  \/\/ make them parsable for update code to work correctly. Plus, we can compute new sizes\n-  \/\/ for future GCLABs here.\n-  if (UseTLAB) {\n-    ShenandoahGCPhase phase(ShenandoahPhaseTimings::init_update_refs_manage_gclabs);\n-    gclabs_retire(ResizeTLAB);\n-  }\n-\n-  if (ShenandoahVerify) {\n-    if (!is_degenerated_gc_in_progress()) {\n-      verifier()->verify_roots_in_to_space_except(ShenandoahRootVerifier::ThreadRoots);\n-    }\n-    verifier()->verify_before_updaterefs();\n-  }\n-\n-  set_update_refs_in_progress(true);\n-\n-  _update_refs_iterator.reset();\n-\n-  if (ShenandoahPacing) {\n-    pacer()->setup_for_updaterefs();\n-  }\n-}\n@@ -2785,33 +2152,3 @@\n-void ShenandoahHeap::op_final_updaterefs() {\n-  assert(ShenandoahSafepoint::is_at_shenandoah_safepoint(), \"must be at safepoint\");\n-\n-  finish_concurrent_unloading();\n-\n-  \/\/ Check if there is left-over work, and finish it\n-  if (_update_refs_iterator.has_next()) {\n-    ShenandoahGCPhase phase(ShenandoahPhaseTimings::final_update_refs_finish_work);\n-\n-    \/\/ Finish updating references where we left off.\n-    clear_cancelled_gc();\n-    update_heap_references(false);\n-  }\n-\n-  \/\/ Clear cancelled GC, if set. On cancellation path, the block before would handle\n-  \/\/ everything. On degenerated paths, cancelled gc would not be set anyway.\n-  if (cancelled_gc()) {\n-    clear_cancelled_gc();\n-  }\n-  assert(!cancelled_gc(), \"Should have been done right before\");\n-\n-  if (ShenandoahVerify && !is_degenerated_gc_in_progress()) {\n-    verifier()->verify_roots_in_to_space_except(ShenandoahRootVerifier::ThreadRoots);\n-  }\n-\n-  if (is_degenerated_gc_in_progress()) {\n-    ShenandoahConcurrentMark::update_roots(ShenandoahPhaseTimings::degen_gc_update_roots);\n-  }\n-\n-  \/\/ Has to be done before cset is clear\n-  if (ShenandoahVerify) {\n-    verifier()->verify_roots_in_to_space();\n-  }\n+void ShenandoahHeap::update_heap_region_states(bool concurrent) {\n+  assert(SafepointSynchronize::is_at_safepoint(), \"Must be at a safepoint\");\n+  assert(!is_full_gc_in_progress(), \"Only for concurrent and degenerated GC\");\n@@ -2820,1 +2157,3 @@\n-    ShenandoahGCPhase phase(ShenandoahPhaseTimings::final_update_refs_update_region_states);\n+    ShenandoahGCPhase phase(concurrent ?\n+                            ShenandoahPhaseTimings::final_update_refs_update_region_states :\n+                            ShenandoahPhaseTimings::degen_gc_final_update_refs_update_region_states);\n@@ -2828,1 +2167,3 @@\n-    ShenandoahGCPhase phase(ShenandoahPhaseTimings::final_update_refs_trash_cset);\n+    ShenandoahGCPhase phase(concurrent ?\n+                            ShenandoahPhaseTimings::final_update_refs_trash_cset :\n+                            ShenandoahPhaseTimings::degen_gc_final_update_refs_trash_cset);\n@@ -2831,0 +2172,1 @@\n+}\n@@ -2832,11 +2174,1 @@\n-  set_has_forwarded_objects(false);\n-  set_update_refs_in_progress(false);\n-\n-  if (ShenandoahVerify) {\n-    verifier()->verify_after_updaterefs();\n-  }\n-\n-  if (VerifyAfterGC) {\n-    Universe::verify();\n-  }\n-\n+void ShenandoahHeap::rebuild_free_set(bool concurrent) {\n@@ -2844,1 +2176,3 @@\n-    ShenandoahGCPhase phase(ShenandoahPhaseTimings::final_update_refs_rebuild_freeset);\n+    ShenandoahGCPhase phase(concurrent ?\n+                            ShenandoahPhaseTimings::final_update_refs_rebuild_freeset :\n+                            ShenandoahPhaseTimings::degen_gc_final_update_refs_rebuild_freeset);\n@@ -2937,296 +2271,0 @@\n-void ShenandoahHeap::vmop_entry_init_mark() {\n-  TraceCollectorStats tcs(monitoring_support()->stw_collection_counters());\n-  ShenandoahTimingsTracker timing(ShenandoahPhaseTimings::init_mark_gross);\n-\n-  try_inject_alloc_failure();\n-  VM_ShenandoahInitMark op;\n-  VMThread::execute(&op); \/\/ jump to entry_init_mark() under safepoint\n-}\n-\n-void ShenandoahHeap::vmop_entry_final_mark() {\n-  TraceCollectorStats tcs(monitoring_support()->stw_collection_counters());\n-  ShenandoahTimingsTracker timing(ShenandoahPhaseTimings::final_mark_gross);\n-\n-  try_inject_alloc_failure();\n-  VM_ShenandoahFinalMarkStartEvac op;\n-  VMThread::execute(&op); \/\/ jump to entry_final_mark under safepoint\n-}\n-\n-void ShenandoahHeap::vmop_entry_init_updaterefs() {\n-  TraceCollectorStats tcs(monitoring_support()->stw_collection_counters());\n-  ShenandoahTimingsTracker timing(ShenandoahPhaseTimings::init_update_refs_gross);\n-\n-  try_inject_alloc_failure();\n-  VM_ShenandoahInitUpdateRefs op;\n-  VMThread::execute(&op);\n-}\n-\n-void ShenandoahHeap::vmop_entry_final_updaterefs() {\n-  TraceCollectorStats tcs(monitoring_support()->stw_collection_counters());\n-  ShenandoahTimingsTracker timing(ShenandoahPhaseTimings::final_update_refs_gross);\n-\n-  try_inject_alloc_failure();\n-  VM_ShenandoahFinalUpdateRefs op;\n-  VMThread::execute(&op);\n-}\n-\n-void ShenandoahHeap::vmop_entry_full(GCCause::Cause cause) {\n-  TraceCollectorStats tcs(monitoring_support()->full_stw_collection_counters());\n-  ShenandoahTimingsTracker timing(ShenandoahPhaseTimings::full_gc_gross);\n-\n-  try_inject_alloc_failure();\n-  VM_ShenandoahFullGC op(cause);\n-  VMThread::execute(&op);\n-}\n-\n-void ShenandoahHeap::vmop_degenerated(ShenandoahDegenPoint point) {\n-  TraceCollectorStats tcs(monitoring_support()->full_stw_collection_counters());\n-  ShenandoahTimingsTracker timing(ShenandoahPhaseTimings::degen_gc_gross);\n-\n-  VM_ShenandoahDegeneratedGC degenerated_gc((int)point);\n-  VMThread::execute(&degenerated_gc);\n-}\n-\n-void ShenandoahHeap::entry_init_mark() {\n-  const char* msg = init_mark_event_message();\n-  ShenandoahPausePhase gc_phase(msg, ShenandoahPhaseTimings::init_mark);\n-  EventMark em(\"%s\", msg);\n-\n-  ShenandoahWorkerScope scope(workers(),\n-                              ShenandoahWorkerPolicy::calc_workers_for_init_marking(),\n-                              \"init marking\");\n-\n-  op_init_mark();\n-}\n-\n-void ShenandoahHeap::entry_final_mark() {\n-  const char* msg = final_mark_event_message();\n-  ShenandoahPausePhase gc_phase(msg, ShenandoahPhaseTimings::final_mark);\n-  EventMark em(\"%s\", msg);\n-\n-  ShenandoahWorkerScope scope(workers(),\n-                              ShenandoahWorkerPolicy::calc_workers_for_final_marking(),\n-                              \"final marking\");\n-\n-  op_final_mark();\n-}\n-\n-void ShenandoahHeap::entry_init_updaterefs() {\n-  static const char* msg = \"Pause Init Update Refs\";\n-  ShenandoahPausePhase gc_phase(msg, ShenandoahPhaseTimings::init_update_refs);\n-  EventMark em(\"%s\", msg);\n-\n-  \/\/ No workers used in this phase, no setup required\n-\n-  op_init_updaterefs();\n-}\n-\n-void ShenandoahHeap::entry_final_updaterefs() {\n-  static const char* msg = \"Pause Final Update Refs\";\n-  ShenandoahPausePhase gc_phase(msg, ShenandoahPhaseTimings::final_update_refs);\n-  EventMark em(\"%s\", msg);\n-\n-  ShenandoahWorkerScope scope(workers(),\n-                              ShenandoahWorkerPolicy::calc_workers_for_final_update_ref(),\n-                              \"final reference update\");\n-\n-  op_final_updaterefs();\n-}\n-\n-void ShenandoahHeap::entry_full(GCCause::Cause cause) {\n-  static const char* msg = \"Pause Full\";\n-  ShenandoahPausePhase gc_phase(msg, ShenandoahPhaseTimings::full_gc, true \/* log_heap_usage *\/);\n-  EventMark em(\"%s\", msg);\n-\n-  ShenandoahWorkerScope scope(workers(),\n-                              ShenandoahWorkerPolicy::calc_workers_for_fullgc(),\n-                              \"full gc\");\n-\n-  op_full(cause);\n-}\n-\n-void ShenandoahHeap::entry_degenerated(int point) {\n-  ShenandoahDegenPoint dpoint = (ShenandoahDegenPoint)point;\n-  const char* msg = degen_event_message(dpoint);\n-  ShenandoahPausePhase gc_phase(msg, ShenandoahPhaseTimings::degen_gc, true \/* log_heap_usage *\/);\n-  EventMark em(\"%s\", msg);\n-\n-  ShenandoahWorkerScope scope(workers(),\n-                              ShenandoahWorkerPolicy::calc_workers_for_stw_degenerated(),\n-                              \"stw degenerated gc\");\n-\n-  set_degenerated_gc_in_progress(true);\n-  op_degenerated(dpoint);\n-  set_degenerated_gc_in_progress(false);\n-}\n-\n-void ShenandoahHeap::entry_mark_roots() {\n-  TraceCollectorStats tcs(monitoring_support()->concurrent_collection_counters());\n-\n-  const char* msg = \"Concurrent marking roots\";\n-  ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_mark_roots);\n-  EventMark em(\"%s\", msg);\n-\n-  ShenandoahWorkerScope scope(workers(),\n-                              ShenandoahWorkerPolicy::calc_workers_for_conc_marking(),\n-                              \"concurrent marking roots\");\n-\n-  try_inject_alloc_failure();\n-  op_mark_roots();\n-}\n-\n-void ShenandoahHeap::entry_mark() {\n-  TraceCollectorStats tcs(monitoring_support()->concurrent_collection_counters());\n-\n-  const char* msg = conc_mark_event_message();\n-  ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_mark);\n-  EventMark em(\"%s\", msg);\n-\n-  ShenandoahWorkerScope scope(workers(),\n-                              ShenandoahWorkerPolicy::calc_workers_for_conc_marking(),\n-                              \"concurrent marking\");\n-\n-  try_inject_alloc_failure();\n-  op_mark();\n-}\n-\n-void ShenandoahHeap::entry_evac() {\n-  TraceCollectorStats tcs(monitoring_support()->concurrent_collection_counters());\n-\n-  static const char* msg = \"Concurrent evacuation\";\n-  ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_evac);\n-  EventMark em(\"%s\", msg);\n-\n-  ShenandoahWorkerScope scope(workers(),\n-                              ShenandoahWorkerPolicy::calc_workers_for_conc_evac(),\n-                              \"concurrent evacuation\");\n-\n-  try_inject_alloc_failure();\n-  op_conc_evac();\n-}\n-\n-void ShenandoahHeap::entry_update_thread_roots() {\n-  TraceCollectorStats tcs(monitoring_support()->concurrent_collection_counters());\n-\n-  static const char* msg = \"Concurrent update thread roots\";\n-  ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_update_thread_roots);\n-  EventMark em(\"%s\", msg);\n-\n-  \/\/ No workers used in this phase, no setup required\n-  try_inject_alloc_failure();\n-  op_update_thread_roots();\n-}\n-\n-\n-void ShenandoahHeap::entry_updaterefs() {\n-  static const char* msg = \"Concurrent update references\";\n-  ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_update_refs);\n-  EventMark em(\"%s\", msg);\n-\n-  ShenandoahWorkerScope scope(workers(),\n-                              ShenandoahWorkerPolicy::calc_workers_for_conc_update_ref(),\n-                              \"concurrent reference update\");\n-\n-  try_inject_alloc_failure();\n-  op_updaterefs();\n-}\n-\n-void ShenandoahHeap::entry_weak_refs() {\n-  static const char* msg = \"Concurrent weak references\";\n-  ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_weak_refs);\n-  EventMark em(\"%s\", msg);\n-\n-  ShenandoahWorkerScope scope(workers(),\n-                              ShenandoahWorkerPolicy::calc_workers_for_conc_refs_processing(),\n-                              \"concurrent weak references\");\n-\n-  try_inject_alloc_failure();\n-  op_weak_refs();\n-}\n-\n-void ShenandoahHeap::entry_weak_roots() {\n-  static const char* msg = \"Concurrent weak roots\";\n-  ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_weak_roots);\n-  EventMark em(\"%s\", msg);\n-\n-  ShenandoahWorkerScope scope(workers(),\n-                              ShenandoahWorkerPolicy::calc_workers_for_conc_root_processing(),\n-                              \"concurrent weak root\");\n-\n-  try_inject_alloc_failure();\n-  op_weak_roots();\n-}\n-\n-void ShenandoahHeap::entry_class_unloading() {\n-  static const char* msg = \"Concurrent class unloading\";\n-  ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_class_unload);\n-  EventMark em(\"%s\", msg);\n-\n-  ShenandoahWorkerScope scope(workers(),\n-                              ShenandoahWorkerPolicy::calc_workers_for_conc_root_processing(),\n-                              \"concurrent class unloading\");\n-\n-  try_inject_alloc_failure();\n-  op_class_unloading();\n-}\n-\n-void ShenandoahHeap::entry_strong_roots() {\n-  static const char* msg = \"Concurrent strong roots\";\n-  ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_strong_roots);\n-  EventMark em(\"%s\", msg);\n-\n-  ShenandoahGCWorkerPhase worker_phase(ShenandoahPhaseTimings::conc_strong_roots);\n-\n-  ShenandoahWorkerScope scope(workers(),\n-                              ShenandoahWorkerPolicy::calc_workers_for_conc_root_processing(),\n-                              \"concurrent strong root\");\n-\n-  try_inject_alloc_failure();\n-  op_strong_roots();\n-}\n-\n-void ShenandoahHeap::entry_cleanup_early() {\n-  static const char* msg = \"Concurrent cleanup\";\n-  ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_cleanup_early, true \/* log_heap_usage *\/);\n-  EventMark em(\"%s\", msg);\n-\n-  \/\/ This phase does not use workers, no need for setup\n-\n-  try_inject_alloc_failure();\n-  op_cleanup_early();\n-}\n-\n-void ShenandoahHeap::entry_rendezvous_roots() {\n-  static const char* msg = \"Rendezvous roots\";\n-  ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_rendezvous_roots);\n-  EventMark em(\"%s\", msg);\n-\n-  \/\/ This phase does not use workers, no need for setup\n-  try_inject_alloc_failure();\n-  op_rendezvous_roots();\n-}\n-\n-void ShenandoahHeap::entry_cleanup_complete() {\n-  static const char* msg = \"Concurrent cleanup\";\n-  ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_cleanup_complete, true \/* log_heap_usage *\/);\n-  EventMark em(\"%s\", msg);\n-\n-  \/\/ This phase does not use workers, no need for setup\n-\n-  try_inject_alloc_failure();\n-  op_cleanup_complete();\n-}\n-\n-void ShenandoahHeap::entry_reset() {\n-  static const char* msg = \"Concurrent reset\";\n-  ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_reset);\n-  EventMark em(\"%s\", msg);\n-\n-  ShenandoahWorkerScope scope(workers(),\n-                              ShenandoahWorkerPolicy::calc_workers_for_conc_reset(),\n-                              \"concurrent reset\");\n-\n-  try_inject_alloc_failure();\n-  op_reset();\n-}\n-\n@@ -3306,54 +2344,0 @@\n-const char* ShenandoahHeap::init_mark_event_message() const {\n-  assert(!has_forwarded_objects(), \"Should not have forwarded objects here\");\n-\n-  bool unload_cls = unload_classes();\n-\n-  if (unload_cls) {\n-    return \"Pause Init Mark (unload classes)\";\n-  } else {\n-    return \"Pause Init Mark\";\n-  }\n-}\n-\n-const char* ShenandoahHeap::final_mark_event_message() const {\n-  assert(!has_forwarded_objects(), \"Should not have forwarded objects here\");\n-\n-  bool unload_cls = unload_classes();\n-\n-  if (unload_cls) {\n-    return \"Pause Final Mark (unload classes)\";\n-  } else {\n-    return \"Pause Final Mark\";\n-  }\n-}\n-\n-const char* ShenandoahHeap::conc_mark_event_message() const {\n-  assert(!has_forwarded_objects(), \"Should not have forwarded objects here\");\n-\n-  bool unload_cls = unload_classes();\n-\n-  if (unload_cls) {\n-    return \"Concurrent marking (unload classes)\";\n-  } else {\n-    return \"Concurrent marking\";\n-  }\n-}\n-\n-const char* ShenandoahHeap::degen_event_message(ShenandoahDegenPoint point) const {\n-  switch (point) {\n-    case _degenerated_unset:\n-      return \"Pause Degenerated GC (<UNSET>)\";\n-    case _degenerated_outside_cycle:\n-      return \"Pause Degenerated GC (Outside of Cycle)\";\n-    case _degenerated_mark:\n-      return \"Pause Degenerated GC (Mark)\";\n-    case _degenerated_evac:\n-      return \"Pause Degenerated GC (Evacuation)\";\n-    case _degenerated_updaterefs:\n-      return \"Pause Degenerated GC (Update Refs)\";\n-    default:\n-      ShouldNotReachHere();\n-      return \"ERROR\";\n-  }\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":75,"deletions":1091,"binary":false,"changes":1166,"status":"modified"},{"patch":"@@ -58,0 +58,2 @@\n+class ShenandoahConcurrentMark;\n+class ShenandoahMarkCompact;\n@@ -123,0 +125,5 @@\n+  \/\/ Supported GC\n+  friend class ShenandoahConcurrentGC;\n+  friend class ShenandoahDegenGC;\n+  friend class ShenandoahMarkCompact;\n+\n@@ -304,32 +311,0 @@\n-\/\/ ---------- GC cancellation and degeneration machinery\n-\/\/\n-\/\/ Cancelled GC flag is used to notify concurrent phases that they should terminate.\n-\/\/\n-public:\n-  enum ShenandoahDegenPoint {\n-    _degenerated_unset,\n-    _degenerated_outside_cycle,\n-    _degenerated_mark,\n-    _degenerated_evac,\n-    _degenerated_updaterefs,\n-    _DEGENERATED_LIMIT\n-  };\n-\n-  static const char* degen_point_to_string(ShenandoahDegenPoint point) {\n-    switch (point) {\n-      case _degenerated_unset:\n-        return \"<UNSET>\";\n-      case _degenerated_outside_cycle:\n-        return \"Outside of Cycle\";\n-      case _degenerated_mark:\n-        return \"Mark\";\n-      case _degenerated_evac:\n-        return \"Evacuation\";\n-      case _degenerated_updaterefs:\n-        return \"Update Refs\";\n-      default:\n-        ShouldNotReachHere();\n-        return \"ERROR\";\n-    }\n-  };\n-\n@@ -365,2 +340,0 @@\n-\/\/ ---------- GC operations entry points\n-\/\/\n@@ -368,33 +341,1 @@\n-  \/\/ Entry points to STW GC operations, these cause a related safepoint, that then\n-  \/\/ call the entry method below\n-  void vmop_entry_init_mark();\n-  void vmop_entry_final_mark();\n-  void vmop_entry_init_updaterefs();\n-  void vmop_entry_final_updaterefs();\n-  void vmop_entry_full(GCCause::Cause cause);\n-  void vmop_degenerated(ShenandoahDegenPoint point);\n-\n-  \/\/ Entry methods to normally STW GC operations. These set up logging, monitoring\n-  \/\/ and workers for net VM operation\n-  void entry_init_mark();\n-  void entry_final_mark();\n-  void entry_init_updaterefs();\n-  void entry_final_updaterefs();\n-  void entry_full(GCCause::Cause cause);\n-  void entry_degenerated(int point);\n-\n-  \/\/ Entry methods to normally concurrent GC operations. These set up logging, monitoring\n-  \/\/ for concurrent operation.\n-  void entry_reset();\n-  void entry_mark_roots();\n-  void entry_mark();\n-  void entry_weak_refs();\n-  void entry_weak_roots();\n-  void entry_class_unloading();\n-  void entry_strong_roots();\n-  void entry_cleanup_early();\n-  void entry_rendezvous_roots();\n-  void entry_evac();\n-  void entry_update_thread_roots();\n-  void entry_updaterefs();\n-  void entry_cleanup_complete();\n+  \/\/ Elastic heap support\n@@ -402,0 +343,1 @@\n+  void op_uncommit(double shrink_before, size_t shrink_until);\n@@ -404,25 +346,18 @@\n-  \/\/ Actual work for the phases\n-  void op_init_mark();\n-  void op_final_mark();\n-  void op_init_updaterefs();\n-  void op_final_updaterefs();\n-  void op_full(GCCause::Cause cause);\n-  void op_degenerated(ShenandoahDegenPoint point);\n-  void op_degenerated_fail();\n-  void op_degenerated_futile();\n-\n-  void op_reset();\n-  void op_mark_roots();\n-  void op_mark();\n-  void op_weak_refs();\n-  void op_weak_roots();\n-  void op_class_unloading();\n-  void op_strong_roots();\n-  void op_cleanup_early();\n-  void op_rendezvous_roots();\n-  void op_conc_evac();\n-  void op_stw_evac();\n-  void op_update_thread_roots();\n-  void op_updaterefs();\n-  void op_cleanup_complete();\n-  void op_uncommit(double shrink_before, size_t shrink_until);\n+  \/\/ GC support\n+  \/\/ Reset bitmap, prepare regions for new GC cycle\n+  void prepare_gc();\n+  void prepare_regions_and_collection_set(bool concurrent);\n+  \/\/ Evacuation\n+  void prepare_evacuation(bool concurrent);\n+  void evacuate_collection_set(bool concurrent);\n+  \/\/ Concurrent root processing\n+  void prepare_concurrent_roots();\n+  void finish_concurrent_roots();\n+  \/\/ Concurrent class unloading support\n+  void do_class_unloading();\n+  \/\/ Reference updating\n+  void prepare_update_heap_references(bool concurrent);\n+  void update_heap_references(bool concurrent);\n+  \/\/ Final update region states\n+  void update_heap_region_states(bool concurrent);\n+  void rebuild_free_set(bool concurrent);\n@@ -431,0 +366,1 @@\n+  void recycle_trash();\n@@ -432,10 +368,3 @@\n-  \/\/ Messages for GC trace events, they have to be immortal for\n-  \/\/ passing around the logging\/tracing systems\n-  const char* init_mark_event_message() const;\n-  const char* final_mark_event_message() const;\n-  const char* conc_mark_event_message() const;\n-  const char* degen_event_message(ShenandoahDegenPoint point) const;\n-\n-\/\/ Helpers\n-  void finish_mark();\n-  void prepare_evacuation();\n+public:\n+  void notify_gc_progress()    { _progress_last_gc.set();}\n+  void notify_gc_no_progress() { _progress_last_gc.unset(); }\n@@ -443,1 +372,0 @@\n-\/\/ ---------- GC subsystems\n@@ -520,5 +448,0 @@\n-  \/\/ Prepare concurrent root processing\n-  void prepare_concurrent_roots();\n-  \/\/ Prepare and finish concurrent unloading\n-  void prepare_concurrent_unloading();\n-  void finish_concurrent_unloading();\n@@ -720,1 +643,0 @@\n-  void update_heap_references(bool concurrent);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.hpp","additions":31,"deletions":109,"binary":false,"changes":140,"status":"modified"},{"patch":"@@ -31,1 +31,1 @@\n-#include \"gc\/shenandoah\/shenandoahConcurrentMark.hpp\"\n+#include \"gc\/shenandoah\/shenandoahConcurrentGC.hpp\"\n@@ -38,0 +38,1 @@\n+#include \"gc\/shenandoah\/shenandoahMonitoringSupport.hpp\"\n@@ -45,1 +46,0 @@\n-#include \"gc\/shenandoah\/shenandoahTaskqueue.inline.hpp\"\n@@ -58,0 +58,1 @@\n+#include \"runtime\/vmThread.hpp\"\n@@ -59,0 +60,1 @@\n+#include \"utilities\/events.hpp\"\n@@ -63,1 +65,1 @@\n-  _gc_timer(NULL),\n+  _gc_timer(ShenandoahHeap::heap()->gc_timer()),\n@@ -66,2 +68,44 @@\n-void ShenandoahMarkCompact::initialize(GCTimer* gc_timer) {\n-  _gc_timer = gc_timer;\n+bool ShenandoahMarkCompact::collect(GCCause::Cause cause) {\n+  vmop_entry_full(cause);\n+  \/\/ Always success\n+  return true;\n+}\n+\n+void ShenandoahMarkCompact::vmop_entry_full(GCCause::Cause cause) {\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+  TraceCollectorStats tcs(heap->monitoring_support()->full_stw_collection_counters());\n+  ShenandoahTimingsTracker timing(ShenandoahPhaseTimings::full_gc_gross);\n+\n+  heap->try_inject_alloc_failure();\n+  VM_ShenandoahFullGC op(cause, this);\n+  VMThread::execute(&op);\n+}\n+\n+void ShenandoahMarkCompact::entry_full(GCCause::Cause cause) {\n+  static const char* msg = \"Pause Full\";\n+  ShenandoahPausePhase gc_phase(msg, ShenandoahPhaseTimings::full_gc, true \/* log_heap_usage *\/);\n+  EventMark em(\"%s\", msg);\n+\n+  ShenandoahWorkerScope scope(ShenandoahHeap::heap()->workers(),\n+                              ShenandoahWorkerPolicy::calc_workers_for_fullgc(),\n+                              \"full gc\");\n+\n+  op_full(cause);\n+}\n+\n+void ShenandoahMarkCompact::op_full(GCCause::Cause cause) {\n+  ShenandoahMetricsSnapshot metrics;\n+  metrics.snap_before();\n+\n+  \/\/ Perform full GC\n+  do_it(cause);\n+\n+  metrics.snap_after();\n+\n+  if (metrics.is_good_progress()) {\n+    ShenandoahHeap::heap()->notify_gc_progress();\n+  } else {\n+    \/\/ Nothing to do. Tell the allocation path that we have failed to make\n+    \/\/ progress, and it can finally fail.\n+    ShenandoahHeap::heap()->notify_gc_no_progress();\n+  }\n@@ -119,1 +163,1 @@\n-      ShenandoahConcurrentMark::cancel();\n+      ShenandoahConcurrentGC::cancel();\n@@ -126,1 +170,1 @@\n-      ShenandoahConcurrentMark::update_roots(ShenandoahPhaseTimings::full_gc_update_roots);\n+      update_roots(true \/*full_gc*\/);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMarkCompact.cpp","additions":51,"deletions":7,"binary":false,"changes":58,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"gc\/shenandoah\/shenandoahGC.hpp\"\n@@ -31,0 +32,1 @@\n+#include \"gc\/shenandoah\/shenandoahMetrics.hpp\"\n@@ -54,0 +56,2 @@\n+class VM_ShenandoahFullGC;\n+class ShenandoahDegenGC;\n@@ -55,1 +59,1 @@\n-class ShenandoahMarkCompact : public StackObj {\n+class ShenandoahMarkCompact : public ShenandoahGC {\n@@ -57,0 +61,3 @@\n+  friend class VM_ShenandoahFullGC;\n+  friend class ShenandoahDegenGC;\n+\n@@ -64,1 +71,7 @@\n-  void initialize(GCTimer* gc_timer);\n+  bool collect(GCCause::Cause cause);\n+\n+private:\n+  \/\/ GC entries\n+  void vmop_entry_full(GCCause::Cause cause);\n+  void entry_full(GCCause::Cause cause);\n+  void op_full(GCCause::Cause cause);\n@@ -68,1 +81,0 @@\n-private:\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMarkCompact.hpp","additions":15,"deletions":3,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -101,2 +101,2 @@\n-    case update_roots:\n-    case final_update_refs_roots:\n+    case finish_mark:\n+    case purge_weak_par:\n@@ -129,1 +129,1 @@\n-    case update_roots:\n+    case finish_mark:\n@@ -131,1 +131,0 @@\n-    case final_update_refs_roots:\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahPhaseTimings.cpp","additions":3,"deletions":4,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -59,1 +59,0 @@\n-  f(conc_mark,                                      \"Concurrent Marking\")              \\\n@@ -62,0 +61,1 @@\n+  f(conc_mark,                                      \"Concurrent Marking\")              \\\n@@ -65,5 +65,6 @@\n-  f(update_roots,                                   \"  Update Roots\")                  \\\n-  SHENANDOAH_PAR_PHASE_DO(update_,                  \"    U: \", f)                      \\\n-  f(finish_queues,                                  \"  Finish Queues\")                 \\\n-  f(weakrefs,                                       \"  Weak References\")               \\\n-  f(weakrefs_process,                               \"    Process\")                     \\\n+  f(finish_mark,                                    \"  Finish Mark\")                   \\\n+  SHENANDOAH_PAR_PHASE_DO(finish_mark_,             \"    FM: \", f)                     \\\n+  f(purge,                                          \"  System Purge\")                  \\\n+  SHENANDOAH_PAR_PHASE_DO(purge_cu_par_,            \"      CU: \", f)                   \\\n+  f(purge_weak_par,                                 \"    Weak Roots\")                  \\\n+  SHENANDOAH_PAR_PHASE_DO(purge_weak_par_,          \"      WR: \", f)                   \\\n@@ -104,1 +105,0 @@\n-  f(conc_update_thread_roots,                       \"Concurrent Update Thread Roots\")  \\\n@@ -106,0 +106,1 @@\n+  f(conc_update_thread_roots,                       \"Concurrent Update Thread Roots\")  \\\n@@ -110,2 +111,0 @@\n-  f(final_update_refs_roots,                        \"  Update Roots\")                  \\\n-  SHENANDOAH_PAR_PHASE_DO(final_update_,            \"    UR: \", f)                     \\\n@@ -132,0 +131,11 @@\n+  f(degen_gc_final_update_region_states,            \"  Update Region States\")          \\\n+  f(degen_gc_final_manage_labs,                     \"  Manage GC\/TLABs\")               \\\n+  f(degen_gc_choose_cset,                           \"  Choose Collection Set\")         \\\n+  f(degen_gc_final_rebuild_freeset,                 \"  Rebuild Free Set\")              \\\n+  f(degen_gc_stw_evac,                              \"  Evacuation\")                    \\\n+  f(degen_gc_init_update_refs_manage_gclabs,        \"  Manage GCLABs\")                 \\\n+  f(degen_gc_updaterefs,                            \"  Update References\")             \\\n+  f(degen_gc_final_update_refs_finish_work,         \"  Finish Work\")                   \\\n+  f(degen_gc_final_update_refs_update_region_states,\"  Update Region States\")          \\\n+  f(degen_gc_final_update_refs_trash_cset,          \"  Trash Collection Set\")          \\\n+  f(degen_gc_final_update_refs_rebuild_freeset,     \"  Rebuild Free Set\")              \\\n@@ -134,0 +144,1 @@\n+  f(degen_gc_cleanup_complete,                      \"  Cleanup\")                       \\\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahPhaseTimings.hpp","additions":20,"deletions":9,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -196,1 +196,0 @@\n-    AlwaysTrueClosure always_true;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahRootProcessor.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2013, 2019, Red Hat, Inc. All rights reserved.\n+ * Copyright (c) 2013, 2021, Red Hat, Inc. All rights reserved.\n@@ -27,0 +27,2 @@\n+#include \"gc\/shenandoah\/shenandoahConcurrentGC.hpp\"\n+#include \"gc\/shenandoah\/shenandoahDegeneratedGC.hpp\"\n@@ -28,0 +30,2 @@\n+#include \"gc\/shenandoah\/shenandoahMark.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahMarkCompact.hpp\"\n@@ -46,1 +50,1 @@\n-  ShenandoahHeap::heap()->entry_init_mark();\n+  _gc->entry_init_mark();\n@@ -51,1 +55,1 @@\n-  ShenandoahHeap::heap()->entry_final_mark();\n+  _gc->entry_final_mark();\n@@ -56,1 +60,1 @@\n-  ShenandoahHeap::heap()->entry_full(_gc_cause);\n+  _full_gc->entry_full(_gc_cause);\n@@ -61,1 +65,1 @@\n-  ShenandoahHeap::heap()->entry_degenerated(_point);\n+  _gc->entry_degenerated();\n@@ -66,1 +70,1 @@\n-  ShenandoahHeap::heap()->entry_init_updaterefs();\n+  _gc->entry_init_updaterefs();\n@@ -71,1 +75,1 @@\n-  ShenandoahHeap::heap()->entry_final_updaterefs();\n+  _gc->entry_final_updaterefs();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahVMOperations.cpp","additions":11,"deletions":7,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2013, 2019, Red Hat, Inc. All rights reserved.\n+ * Copyright (c) 2013, 2021, Red Hat, Inc. All rights reserved.\n@@ -30,0 +30,4 @@\n+class ShenandoahConcurrentGC;\n+class ShenandoahDegenGC;\n+class ShenandoahMarkCompact;\n+\n@@ -55,0 +59,2 @@\n+private:\n+  ShenandoahConcurrentGC* const _gc;\n@@ -56,1 +62,3 @@\n-  VM_ShenandoahInitMark() : VM_ShenandoahOperation() {};\n+  VM_ShenandoahInitMark(ShenandoahConcurrentGC* gc) :\n+    VM_ShenandoahOperation(),\n+    _gc(gc) {};\n@@ -63,0 +71,2 @@\n+private:\n+  ShenandoahConcurrentGC* const _gc;\n@@ -64,1 +74,3 @@\n-  VM_ShenandoahFinalMarkStartEvac() : VM_ShenandoahOperation() {};\n+  VM_ShenandoahFinalMarkStartEvac(ShenandoahConcurrentGC* gc) :\n+    VM_ShenandoahOperation(),\n+    _gc(gc) {};\n@@ -72,3 +84,1 @@\n-  \/\/ Really the ShenandoahHeap::ShenandoahDegenerationPoint, but casted to int here\n-  \/\/ in order to avoid dependency on ShenandoahHeap\n-  int _point;\n+  ShenandoahDegenGC* const _gc;\n@@ -76,1 +86,4 @@\n-  VM_ShenandoahDegeneratedGC(int point) : VM_ShenandoahReferenceOperation(), _point(point) {};\n+  VM_ShenandoahDegeneratedGC(ShenandoahDegenGC* gc) :\n+    VM_ShenandoahReferenceOperation(),\n+    _gc(gc) {};\n+\n@@ -84,1 +97,2 @@\n-  GCCause::Cause _gc_cause;\n+  GCCause::Cause                _gc_cause;\n+  ShenandoahMarkCompact* const  _full_gc;\n@@ -86,1 +100,4 @@\n-  VM_ShenandoahFullGC(GCCause::Cause gc_cause) : VM_ShenandoahReferenceOperation(), _gc_cause(gc_cause) {};\n+  VM_ShenandoahFullGC(GCCause::Cause gc_cause, ShenandoahMarkCompact* full_gc) :\n+    VM_ShenandoahReferenceOperation(),\n+    _gc_cause(gc_cause),\n+    _full_gc(full_gc) {};\n@@ -93,0 +110,1 @@\n+  ShenandoahConcurrentGC* const _gc;\n@@ -94,1 +112,3 @@\n-  VM_ShenandoahInitUpdateRefs() : VM_ShenandoahOperation() {};\n+  VM_ShenandoahInitUpdateRefs(ShenandoahConcurrentGC* gc) :\n+    VM_ShenandoahOperation(),\n+    _gc(gc) {};\n@@ -101,0 +121,1 @@\n+  ShenandoahConcurrentGC* const _gc;\n@@ -102,1 +123,3 @@\n-  VM_ShenandoahFinalUpdateRefs() : VM_ShenandoahOperation() {};\n+  VM_ShenandoahFinalUpdateRefs(ShenandoahConcurrentGC* gc) :\n+    VM_ShenandoahOperation(),\n+    _gc(gc) {};\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahVMOperations.hpp","additions":34,"deletions":11,"binary":false,"changes":45,"status":"modified"}]}