{"files":[{"patch":"@@ -220,1 +220,1 @@\n-    movptr(tmpReg, ExternalAddress((address) RTMLockingCounters::rtm_calculation_flag_addr()), tmpReg);\n+    movptr(tmpReg, ExternalAddress((address) RTMLockingCounters::rtm_calculation_flag_addr()));\n@@ -970,1 +970,1 @@\n-void C2_MacroAssembler::vabsnegd(int opcode, XMMRegister dst, XMMRegister src, Register scr) {\n+void C2_MacroAssembler::vabsnegd(int opcode, XMMRegister dst, XMMRegister src) {\n@@ -975,1 +975,1 @@\n-    andpd(dst, ExternalAddress(StubRoutines::x86::vector_double_sign_mask()), scr);\n+    andpd(dst, ExternalAddress(StubRoutines::x86::vector_double_sign_mask()), noreg);\n@@ -978,1 +978,1 @@\n-    xorpd(dst, ExternalAddress(StubRoutines::x86::vector_double_sign_flip()), scr);\n+    xorpd(dst, ExternalAddress(StubRoutines::x86::vector_double_sign_flip()), noreg);\n@@ -982,1 +982,1 @@\n-void C2_MacroAssembler::vabsnegd(int opcode, XMMRegister dst, XMMRegister src, int vector_len, Register scr) {\n+void C2_MacroAssembler::vabsnegd(int opcode, XMMRegister dst, XMMRegister src, int vector_len) {\n@@ -984,1 +984,1 @@\n-    vandpd(dst, src, ExternalAddress(StubRoutines::x86::vector_double_sign_mask()), vector_len, scr);\n+    vandpd(dst, src, ExternalAddress(StubRoutines::x86::vector_double_sign_mask()), vector_len, noreg);\n@@ -987,1 +987,1 @@\n-    vxorpd(dst, src, ExternalAddress(StubRoutines::x86::vector_double_sign_flip()), vector_len, scr);\n+    vxorpd(dst, src, ExternalAddress(StubRoutines::x86::vector_double_sign_flip()), vector_len, noreg);\n@@ -991,1 +991,1 @@\n-void C2_MacroAssembler::vabsnegf(int opcode, XMMRegister dst, XMMRegister src, Register scr) {\n+void C2_MacroAssembler::vabsnegf(int opcode, XMMRegister dst, XMMRegister src) {\n@@ -996,1 +996,1 @@\n-    andps(dst, ExternalAddress(StubRoutines::x86::vector_float_sign_mask()), scr);\n+    andps(dst, ExternalAddress(StubRoutines::x86::vector_float_sign_mask()), noreg);\n@@ -999,1 +999,1 @@\n-    xorps(dst, ExternalAddress(StubRoutines::x86::vector_float_sign_flip()), scr);\n+    xorps(dst, ExternalAddress(StubRoutines::x86::vector_float_sign_flip()), noreg);\n@@ -1003,1 +1003,1 @@\n-void C2_MacroAssembler::vabsnegf(int opcode, XMMRegister dst, XMMRegister src, int vector_len, Register scr) {\n+void C2_MacroAssembler::vabsnegf(int opcode, XMMRegister dst, XMMRegister src, int vector_len) {\n@@ -1005,1 +1005,1 @@\n-    vandps(dst, src, ExternalAddress(StubRoutines::x86::vector_float_sign_mask()), vector_len, scr);\n+    vandps(dst, src, ExternalAddress(StubRoutines::x86::vector_float_sign_mask()), vector_len, noreg);\n@@ -1008,1 +1008,1 @@\n-    vxorps(dst, src, ExternalAddress(StubRoutines::x86::vector_float_sign_flip()), vector_len, scr);\n+    vxorps(dst, src, ExternalAddress(StubRoutines::x86::vector_float_sign_flip()), vector_len, noreg);\n@@ -1181,3 +1181,1 @@\n-void C2_MacroAssembler::signum_fp(int opcode, XMMRegister dst,\n-                                  XMMRegister zero, XMMRegister one,\n-                                  Register scratch) {\n+void C2_MacroAssembler::signum_fp(int opcode, XMMRegister dst, XMMRegister zero, XMMRegister one) {\n@@ -1195,1 +1193,1 @@\n-    xorps(dst, ExternalAddress(StubRoutines::x86::vector_float_sign_flip()), scratch);\n+    xorps(dst, ExternalAddress(StubRoutines::x86::vector_float_sign_flip()), noreg);\n@@ -1203,1 +1201,1 @@\n-    xorpd(dst, ExternalAddress(StubRoutines::x86::vector_double_sign_flip()), scratch);\n+    xorpd(dst, ExternalAddress(StubRoutines::x86::vector_double_sign_flip()), noreg);\n@@ -1462,1 +1460,1 @@\n-void C2_MacroAssembler::varshiftbw(int opcode, XMMRegister dst, XMMRegister src, XMMRegister shift, int vector_len, XMMRegister vtmp, Register scratch) {\n+void C2_MacroAssembler::varshiftbw(int opcode, XMMRegister dst, XMMRegister src, XMMRegister shift, int vector_len, XMMRegister vtmp) {\n@@ -1471,1 +1469,1 @@\n-  vpand(dst, dst, ExternalAddress(StubRoutines::x86::vector_int_to_byte_mask()), 1, scratch);\n+  vpand(dst, dst, ExternalAddress(StubRoutines::x86::vector_int_to_byte_mask()), 1, noreg);\n@@ -1477,1 +1475,1 @@\n-void C2_MacroAssembler::evarshiftb(int opcode, XMMRegister dst, XMMRegister src, XMMRegister shift, int vector_len, XMMRegister vtmp, Register scratch) {\n+void C2_MacroAssembler::evarshiftb(int opcode, XMMRegister dst, XMMRegister src, XMMRegister shift, int vector_len, XMMRegister vtmp) {\n@@ -1486,1 +1484,1 @@\n-  vpand(dst, dst, ExternalAddress(StubRoutines::x86::vector_short_to_byte_mask()), ext_vector_len, scratch);\n+  vpand(dst, dst, ExternalAddress(StubRoutines::x86::vector_short_to_byte_mask()), ext_vector_len, noreg);\n@@ -1631,2 +1629,1 @@\n-void C2_MacroAssembler::load_vector_mask(KRegister dst, XMMRegister src, XMMRegister xtmp,\n-                                         Register tmp, bool novlbwdq, int vlen_enc) {\n+void C2_MacroAssembler::load_vector_mask(KRegister dst, XMMRegister src, XMMRegister xtmp, bool novlbwdq, int vlen_enc) {\n@@ -1636,1 +1633,1 @@\n-            Assembler::eq, true, vlen_enc, tmp);\n+            Assembler::eq, true, vlen_enc, noreg);\n@@ -1696,1 +1693,1 @@\n-void C2_MacroAssembler::load_iota_indices(XMMRegister dst, Register scratch, int vlen_in_bytes) {\n+void C2_MacroAssembler::load_iota_indices(XMMRegister dst, int vlen_in_bytes) {\n@@ -1703,1 +1700,1 @@\n-    movdqu(dst, addr, scratch);\n+    movdqu(dst, addr, noreg);\n@@ -1705,1 +1702,1 @@\n-    vmovdqu(dst, addr, scratch);\n+    vmovdqu(dst, addr, noreg);\n@@ -1708,1 +1705,1 @@\n-    evmovdqub(dst, k0, addr, false \/*merge*\/, Assembler::AVX_512bit, scratch);\n+    evmovdqub(dst, k0, addr, false \/*merge*\/, Assembler::AVX_512bit, noreg);\n@@ -2340,1 +2337,1 @@\n-void C2_MacroAssembler::get_elem(BasicType typ, XMMRegister dst, XMMRegister src, int elemindex, Register tmp, XMMRegister vtmp) {\n+void C2_MacroAssembler::get_elem(BasicType typ, XMMRegister dst, XMMRegister src, int elemindex, XMMRegister vtmp) {\n@@ -2369,2 +2366,2 @@\n-      assert((vtmp != xnoreg) && (tmp != noreg), \"required.\");\n-      movdqu(vtmp, ExternalAddress(StubRoutines::x86::vector_32_bit_mask()), tmp);\n+      assert(vtmp != xnoreg, \"required.\");\n+      movdqu(vtmp, ExternalAddress(StubRoutines::x86::vector_32_bit_mask()), noreg);\n@@ -2373,2 +2370,1 @@\n-      assert((tmp != noreg), \"required.\");\n-      vpand(dst, dst, ExternalAddress(StubRoutines::x86::vector_32_bit_mask()), Assembler::AVX_128bit, tmp);\n+      vpand(dst, dst, ExternalAddress(StubRoutines::x86::vector_32_bit_mask()), Assembler::AVX_128bit, noreg);\n@@ -2403,1 +2399,3 @@\n-void C2_MacroAssembler::evpcmp(BasicType typ, KRegister kdmask, KRegister ksmask, XMMRegister src1, AddressLiteral adr, int comparison, int vector_len, Register scratch) {\n+void C2_MacroAssembler::evpcmp(BasicType typ, KRegister kdmask, KRegister ksmask, XMMRegister src1, AddressLiteral adr, int comparison, int vector_len, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(adr), \"missing\");\n+\n@@ -2407,1 +2405,1 @@\n-      evpcmpb(kdmask, ksmask, src1, adr, comparison, \/*signed*\/ true, vector_len, scratch);\n+      evpcmpb(kdmask, ksmask, src1, adr, comparison, \/*signed*\/ true, vector_len, rscratch);\n@@ -2411,1 +2409,1 @@\n-      evpcmpw(kdmask, ksmask, src1, adr, comparison, \/*signed*\/ true, vector_len, scratch);\n+      evpcmpw(kdmask, ksmask, src1, adr, comparison, \/*signed*\/ true, vector_len, rscratch);\n@@ -2415,1 +2413,1 @@\n-      evpcmpd(kdmask, ksmask, src1, adr, comparison, \/*signed*\/ true, vector_len, scratch);\n+      evpcmpd(kdmask, ksmask, src1, adr, comparison, \/*signed*\/ true, vector_len, rscratch);\n@@ -2419,1 +2417,1 @@\n-      evpcmpq(kdmask, ksmask, src1, adr, comparison, \/*signed*\/ true, vector_len, scratch);\n+      evpcmpq(kdmask, ksmask, src1, adr, comparison, \/*signed*\/ true, vector_len, rscratch);\n@@ -4368,1 +4366,1 @@\n-  vmovdqu(xtmp1, float_sign_flip, scratch, vec_enc);\n+  vmovdqu(xtmp1, float_sign_flip, vec_enc, scratch);\n@@ -4973,1 +4971,1 @@\n-  vmovdqu(xtmp2, ExternalAddress(StubRoutines::x86::vector_popcount_lut()), rtmp, vec_enc);\n+  vmovdqu(xtmp2, ExternalAddress(StubRoutines::x86::vector_popcount_lut()), vec_enc, noreg);\n@@ -5078,1 +5076,1 @@\n-    vmovdqu(xtmp1, ExternalAddress(StubRoutines::x86::vector_reverse_bit_lut()), rtmp, vec_enc);\n+    vmovdqu(xtmp1, ExternalAddress(StubRoutines::x86::vector_reverse_bit_lut()), vec_enc, noreg);\n@@ -5092,1 +5090,1 @@\n-    vector_reverse_byte(bt, dst, xtmp2, rtmp, vec_enc);\n+    vector_reverse_byte(bt, dst, xtmp2, vec_enc);\n@@ -5111,1 +5109,1 @@\n-    vmovdqu(xtmp1, ExternalAddress(StubRoutines::x86::vector_reverse_bit_lut()), rtmp, vec_enc);\n+    vmovdqu(xtmp1, ExternalAddress(StubRoutines::x86::vector_reverse_bit_lut()), vec_enc, rtmp);\n@@ -5127,1 +5125,1 @@\n-    vector_reverse_byte(bt, dst, xtmp2, rtmp, vec_enc);\n+    vector_reverse_byte(bt, dst, xtmp2, vec_enc);\n@@ -5138,1 +5136,1 @@\n-  vector_reverse_byte(bt, dst, xtmp, rtmp, vec_enc);\n+  vector_reverse_byte(bt, dst, xtmp, vec_enc);\n@@ -5181,1 +5179,1 @@\n-void C2_MacroAssembler::vector_reverse_byte(BasicType bt, XMMRegister dst, XMMRegister src, Register rtmp, int vec_enc) {\n+void C2_MacroAssembler::vector_reverse_byte(BasicType bt, XMMRegister dst, XMMRegister src, int vec_enc) {\n@@ -5194,1 +5192,1 @@\n-      vmovdqu(dst, ExternalAddress(StubRoutines::x86::vector_reverse_byte_perm_mask_long()), rtmp, vec_enc);\n+      vmovdqu(dst, ExternalAddress(StubRoutines::x86::vector_reverse_byte_perm_mask_long()), vec_enc, noreg);\n@@ -5197,1 +5195,1 @@\n-      vmovdqu(dst, ExternalAddress(StubRoutines::x86::vector_reverse_byte_perm_mask_int()), rtmp, vec_enc);\n+      vmovdqu(dst, ExternalAddress(StubRoutines::x86::vector_reverse_byte_perm_mask_int()), vec_enc, noreg);\n@@ -5201,1 +5199,1 @@\n-      vmovdqu(dst, ExternalAddress(StubRoutines::x86::vector_reverse_byte_perm_mask_short()), rtmp, vec_enc);\n+      vmovdqu(dst, ExternalAddress(StubRoutines::x86::vector_reverse_byte_perm_mask_short()), vec_enc, noreg);\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":48,"deletions":50,"binary":false,"changes":98,"status":"modified"},{"patch":"@@ -73,4 +73,4 @@\n-  void vabsnegd(int opcode, XMMRegister dst, XMMRegister src, Register scr);\n-  void vabsnegd(int opcode, XMMRegister dst, XMMRegister src, int vector_len, Register scr);\n-  void vabsnegf(int opcode, XMMRegister dst, XMMRegister src, Register scr);\n-  void vabsnegf(int opcode, XMMRegister dst, XMMRegister src, int vector_len, Register scr);\n+  void vabsnegd(int opcode, XMMRegister dst, XMMRegister src);\n+  void vabsnegd(int opcode, XMMRegister dst, XMMRegister src, int vector_len);\n+  void vabsnegf(int opcode, XMMRegister dst, XMMRegister src);\n+  void vabsnegf(int opcode, XMMRegister dst, XMMRegister src, int vector_len);\n@@ -93,3 +93,1 @@\n-  void signum_fp(int opcode, XMMRegister dst,\n-                 XMMRegister zero, XMMRegister one,\n-                 Register scratch);\n+  void signum_fp(int opcode, XMMRegister dst, XMMRegister zero, XMMRegister one);\n@@ -124,2 +122,2 @@\n-  void varshiftbw(int opcode, XMMRegister dst, XMMRegister src, XMMRegister shift, int vector_len, XMMRegister vtmp, Register scratch);\n-  void evarshiftb(int opcode, XMMRegister dst, XMMRegister src, XMMRegister shift, int vector_len, XMMRegister vtmp, Register scratch);\n+  void varshiftbw(int opcode, XMMRegister dst, XMMRegister src, XMMRegister shift, int vector_len, XMMRegister vtmp);\n+  void evarshiftb(int opcode, XMMRegister dst, XMMRegister src, XMMRegister shift, int vector_len, XMMRegister vtmp);\n@@ -140,1 +138,1 @@\n-  void get_elem(BasicType typ, XMMRegister dst, XMMRegister src, int elemindex, Register tmp = noreg, XMMRegister vtmp = xnoreg);\n+  void get_elem(BasicType typ, XMMRegister dst, XMMRegister src, int elemindex, XMMRegister vtmp = xnoreg);\n@@ -153,1 +151,1 @@\n-  void evpcmp(BasicType typ, KRegister kdmask, KRegister ksmask, XMMRegister src1, AddressLiteral adr, int comparison, int vector_len, Register scratch = rscratch1);\n+  void evpcmp(BasicType typ, KRegister kdmask, KRegister ksmask, XMMRegister src1, AddressLiteral adr, int comparison, int vector_len, Register rscratch = rscratch1);\n@@ -158,1 +156,1 @@\n-  void load_vector_mask(KRegister dst, XMMRegister src, XMMRegister xtmp, Register tmp, bool novlbwdq, int vlen_enc);\n+  void load_vector_mask(KRegister dst, XMMRegister src, XMMRegister xtmp, bool novlbwdq, int vlen_enc);\n@@ -163,1 +161,1 @@\n-  void load_iota_indices(XMMRegister dst, Register scratch, int vlen_in_bytes);\n+  void load_iota_indices(XMMRegister dst, int vlen_in_bytes);\n@@ -393,1 +391,1 @@\n-  void vector_reverse_byte(BasicType bt, XMMRegister dst, XMMRegister src, Register rtmp, int vec_enc);\n+  void vector_reverse_byte(BasicType bt, XMMRegister dst, XMMRegister src, int vec_enc);\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.hpp","additions":12,"deletions":14,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -313,3 +313,1 @@\n-void MacroAssembler::movptr(Register dst, AddressLiteral src, Register scratch) {\n-  \/\/ scratch register is not used,\n-  \/\/ it is defined to match parameters of 64-bit version of this method.\n+void MacroAssembler::movptr(Register dst, AddressLiteral src) {\n@@ -665,1 +663,1 @@\n-void MacroAssembler::movptr(Register dst, AddressLiteral src, Register scratch) {\n+void MacroAssembler::movptr(Register dst, AddressLiteral src) {\n@@ -672,2 +670,2 @@\n-      lea(scratch, src);\n-      movq(dst, Address(scratch, 0));\n+      lea(dst, src);\n+      movq(dst, Address(dst, 0));\n@@ -2548,2 +2546,2 @@\n-    assert(((src->encoding() < 16) || VM_Version::supports_avx512vl()),\"XMM register should be 0-15\");\n-    Assembler::movdqu(dst, src);\n+  assert(((src->encoding() < 16) || VM_Version::supports_avx512vl()),\"XMM register should be 0-15\");\n+  Assembler::movdqu(dst, src);\n@@ -2553,2 +2551,2 @@\n-    assert(((dst->encoding() < 16) || VM_Version::supports_avx512vl()),\"XMM register should be 0-15\");\n-    Assembler::movdqu(dst, src);\n+  assert(((dst->encoding() < 16) || VM_Version::supports_avx512vl()),\"XMM register should be 0-15\");\n+  Assembler::movdqu(dst, src);\n@@ -2558,2 +2556,2 @@\n-    assert(((dst->encoding() < 16  && src->encoding() < 16) || VM_Version::supports_avx512vl()),\"XMM register should be 0-15\");\n-    Assembler::movdqu(dst, src);\n+  assert(((dst->encoding() < 16  && src->encoding() < 16) || VM_Version::supports_avx512vl()),\"XMM register should be 0-15\");\n+  Assembler::movdqu(dst, src);\n@@ -2562,1 +2560,3 @@\n-void MacroAssembler::movdqu(XMMRegister dst, AddressLiteral src, Register scratchReg) {\n+void MacroAssembler::movdqu(XMMRegister dst, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -2566,2 +2566,2 @@\n-    lea(scratchReg, src);\n-    movdqu(dst, Address(scratchReg, 0));\n+    lea(rscratch, src);\n+    movdqu(dst, Address(rscratch, 0));\n@@ -2572,2 +2572,2 @@\n-    assert(((src->encoding() < 16) || VM_Version::supports_avx512vl()),\"XMM register should be 0-15\");\n-    Assembler::vmovdqu(dst, src);\n+  assert(((src->encoding() < 16) || VM_Version::supports_avx512vl()),\"XMM register should be 0-15\");\n+  Assembler::vmovdqu(dst, src);\n@@ -2577,2 +2577,2 @@\n-    assert(((dst->encoding() < 16) || VM_Version::supports_avx512vl()),\"XMM register should be 0-15\");\n-    Assembler::vmovdqu(dst, src);\n+  assert(((dst->encoding() < 16) || VM_Version::supports_avx512vl()),\"XMM register should be 0-15\");\n+  Assembler::vmovdqu(dst, src);\n@@ -2582,2 +2582,2 @@\n-    assert(((dst->encoding() < 16  && src->encoding() < 16) || VM_Version::supports_avx512vl()),\"XMM register should be 0-15\");\n-    Assembler::vmovdqu(dst, src);\n+  assert(((dst->encoding() < 16  && src->encoding() < 16) || VM_Version::supports_avx512vl()),\"XMM register should be 0-15\");\n+  Assembler::vmovdqu(dst, src);\n@@ -2586,1 +2586,3 @@\n-void MacroAssembler::vmovdqu(XMMRegister dst, AddressLiteral src, Register scratch_reg) {\n+void MacroAssembler::vmovdqu(XMMRegister dst, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src),  \"missing\");\n+\n@@ -2591,2 +2593,2 @@\n-    lea(scratch_reg, src);\n-    vmovdqu(dst, Address(scratch_reg, 0));\n+    lea(rscratch, src);\n+    vmovdqu(dst, Address(rscratch, 0));\n@@ -2596,1 +2598,3 @@\n-void MacroAssembler::vmovdqu(XMMRegister dst, AddressLiteral src, Register scratch_reg, int vector_len) {\n+void MacroAssembler::vmovdqu(XMMRegister dst, AddressLiteral src, int vector_len, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src),  \"missing\");\n+\n@@ -2598,1 +2602,1 @@\n-    evmovdquq(dst, src, AVX_512bit, scratch_reg);\n+    evmovdquq(dst, src, AVX_512bit, rscratch);\n@@ -2600,1 +2604,1 @@\n-    vmovdqu(dst, src, scratch_reg);\n+    vmovdqu(dst, src, rscratch);\n@@ -2602,1 +2606,1 @@\n-    movdqu(dst, src, scratch_reg);\n+    movdqu(dst, src, rscratch);\n@@ -2660,1 +2664,3 @@\n-void MacroAssembler::kmovwl(KRegister dst, AddressLiteral src, Register scratch_reg) {\n+void MacroAssembler::kmovwl(KRegister dst, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -2664,2 +2670,2 @@\n-    lea(scratch_reg, src);\n-    kmovwl(dst, Address(scratch_reg, 0));\n+    lea(rscratch, src);\n+    kmovwl(dst, Address(rscratch, 0));\n@@ -2689,2 +2695,3 @@\n-void MacroAssembler::evmovdqul(XMMRegister dst, KRegister mask, AddressLiteral src, bool merge,\n-                               int vector_len, Register scratch_reg) {\n+void MacroAssembler::evmovdqul(XMMRegister dst, KRegister mask, AddressLiteral src, bool merge, int vector_len, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -2694,2 +2701,2 @@\n-    lea(scratch_reg, src);\n-    Assembler::evmovdqul(dst, mask, Address(scratch_reg, 0), merge, vector_len);\n+    lea(rscratch, src);\n+    Assembler::evmovdqul(dst, mask, Address(rscratch, 0), merge, vector_len);\n@@ -2699,2 +2706,1 @@\n-void MacroAssembler::evmovdquq(XMMRegister dst, KRegister mask, AddressLiteral src, bool merge,\n-                               int vector_len, Register rscratch) {\n+void MacroAssembler::evmovdquq(XMMRegister dst, KRegister mask, AddressLiteral src, bool merge, int vector_len, Register rscratch) {\n@@ -3152,1 +3158,3 @@\n-void MacroAssembler::roundsd(XMMRegister dst, AddressLiteral src, int32_t rmode, Register scratch_reg) {\n+void MacroAssembler::roundsd(XMMRegister dst, AddressLiteral src, int32_t rmode, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -3156,2 +3164,2 @@\n-    lea(scratch_reg, src);\n-    Assembler::roundsd(dst, Address(scratch_reg, 0), rmode);\n+    lea(rscratch, src);\n+    Assembler::roundsd(dst, Address(rscratch, 0), rmode);\n@@ -3188,1 +3196,3 @@\n-void MacroAssembler::xorpd(XMMRegister dst, AddressLiteral src, Register scratch_reg) {\n+void MacroAssembler::xorpd(XMMRegister dst, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -3194,2 +3204,2 @@\n-    lea(scratch_reg, src);\n-    Assembler::xorpd(dst, Address(scratch_reg, 0));\n+    lea(rscratch, src);\n+    Assembler::xorpd(dst, Address(rscratch, 0));\n@@ -3216,1 +3226,3 @@\n-void MacroAssembler::xorps(XMMRegister dst, AddressLiteral src, Register scratch_reg) {\n+void MacroAssembler::xorps(XMMRegister dst, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -3222,2 +3234,2 @@\n-    lea(scratch_reg, src);\n-    Assembler::xorps(dst, Address(scratch_reg, 0));\n+    lea(rscratch, src);\n+    Assembler::xorps(dst, Address(rscratch, 0));\n@@ -3261,0 +3273,2 @@\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -3311,1 +3325,3 @@\n-void MacroAssembler::vpand(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register scratch_reg) {\n+void MacroAssembler::vpand(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src),  \"missing\");\n+\n@@ -3315,2 +3331,2 @@\n-    lea(scratch_reg, src);\n-    Assembler::vpand(dst, nds, Address(scratch_reg, 0), vector_len);\n+    lea(rscratch, src);\n+    Assembler::vpand(dst, nds, Address(rscratch, 0), vector_len);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":65,"deletions":49,"binary":false,"changes":114,"status":"modified"},{"patch":"@@ -1165,11 +1165,11 @@\n-  void movdqu(Address     dst, XMMRegister src);\n-  void movdqu(XMMRegister dst, Address src);\n-  void movdqu(XMMRegister dst, XMMRegister src);\n-  void movdqu(XMMRegister dst, AddressLiteral src, Register scratchReg = rscratch1);\n-\n-  void kmovwl(KRegister dst, Register src) { Assembler::kmovwl(dst, src); }\n-  void kmovwl(Register dst, KRegister src) { Assembler::kmovwl(dst, src); }\n-  void kmovwl(KRegister dst, Address src) { Assembler::kmovwl(dst, src); }\n-  void kmovwl(KRegister dst, AddressLiteral src, Register scratch_reg = rscratch1);\n-  void kmovwl(Address dst,  KRegister src) { Assembler::kmovwl(dst, src); }\n-  void kmovwl(KRegister dst, KRegister src) { Assembler::kmovwl(dst, src); }\n+  void movdqu(Address     dst, XMMRegister    src);\n+  void movdqu(XMMRegister dst, XMMRegister    src);\n+  void movdqu(XMMRegister dst, Address        src);\n+  void movdqu(XMMRegister dst, AddressLiteral src, Register rscratch = rscratch1);\n+\n+  void kmovwl(Register  dst, KRegister      src) { Assembler::kmovwl(dst, src); }\n+  void kmovwl(Address   dst, KRegister      src) { Assembler::kmovwl(dst, src); }\n+  void kmovwl(KRegister dst, KRegister      src) { Assembler::kmovwl(dst, src); }\n+  void kmovwl(KRegister dst, Register       src) { Assembler::kmovwl(dst, src); }\n+  void kmovwl(KRegister dst, Address        src) { Assembler::kmovwl(dst, src); }\n+  void kmovwl(KRegister dst, AddressLiteral src, Register rscratch = rscratch1);\n@@ -1198,5 +1198,5 @@\n-  void vmovdqu(Address     dst, XMMRegister src);\n-  void vmovdqu(XMMRegister dst, Address src);\n-  void vmovdqu(XMMRegister dst, XMMRegister src);\n-  void vmovdqu(XMMRegister dst, AddressLiteral src, Register scratch_reg = rscratch1);\n-  void vmovdqu(XMMRegister dst, AddressLiteral src, Register scratch_reg, int vector_len);\n+  void vmovdqu(Address     dst, XMMRegister    src);\n+  void vmovdqu(XMMRegister dst, Address        src);\n+  void vmovdqu(XMMRegister dst, XMMRegister    src);\n+  void vmovdqu(XMMRegister dst, AddressLiteral src,                 Register rscratch = rscratch1);\n+  void vmovdqu(XMMRegister dst, AddressLiteral src, int vector_len, Register rscratch = rscratch1);\n@@ -1205,2 +1205,2 @@\n-  void evmovdqu(BasicType type, KRegister kmask, Address dst, XMMRegister src,  bool merge, int vector_len);\n-  void evmovdqu(BasicType type, KRegister kmask, XMMRegister dst, Address src, bool merge, int vector_len);\n+  void evmovdqu(BasicType type, KRegister kmask, Address     dst, XMMRegister src, bool merge, int vector_len);\n+  void evmovdqu(BasicType type, KRegister kmask, XMMRegister dst, Address     src, bool merge, int vector_len);\n@@ -1209,1 +1209,2 @@\n-  void evmovdqub(XMMRegister dst, Address src, int vector_len) { Assembler::evmovdqub(dst, src, vector_len); }\n+  void evmovdqub(XMMRegister dst, Address     src, int vector_len) { Assembler::evmovdqub(dst, src, vector_len); }\n+\n@@ -1243,3 +1244,3 @@\n-  void evmovdqul(XMMRegister dst, KRegister mask, Address src, bool merge, int vector_len) { Assembler::evmovdqul(dst, mask, src, merge, vector_len); }\n-  void evmovdqul(Address dst, KRegister mask, XMMRegister src, bool merge, int vector_len) { Assembler::evmovdqul(dst, mask, src, merge, vector_len); }\n-  void evmovdqul(XMMRegister dst, KRegister mask, AddressLiteral src, bool merge, int vector_len, Register scratch_reg);\n+  void evmovdqul(Address     dst, KRegister mask, XMMRegister    src, bool merge, int vector_len) { Assembler::evmovdqul(dst, mask, src, merge, vector_len); }\n+  void evmovdqul(XMMRegister dst, KRegister mask, Address        src, bool merge, int vector_len) { Assembler::evmovdqul(dst, mask, src, merge, vector_len); }\n+  void evmovdqul(XMMRegister dst, KRegister mask, AddressLiteral src, bool merge, int vector_len, Register rscratch);\n@@ -1314,3 +1315,3 @@\n-  void roundsd(XMMRegister dst, XMMRegister src, int32_t rmode)    { Assembler::roundsd(dst, src, rmode); }\n-  void roundsd(XMMRegister dst, Address src, int32_t rmode)        { Assembler::roundsd(dst, src, rmode); }\n-  void roundsd(XMMRegister dst, AddressLiteral src, int32_t rmode, Register scratch_reg);\n+  void roundsd(XMMRegister dst, XMMRegister    src, int32_t rmode) { Assembler::roundsd(dst, src, rmode); }\n+  void roundsd(XMMRegister dst, Address        src, int32_t rmode) { Assembler::roundsd(dst, src, rmode); }\n+  void roundsd(XMMRegister dst, AddressLiteral src, int32_t rmode, Register rscratch);\n@@ -1339,3 +1340,3 @@\n-  void xorpd(XMMRegister dst, XMMRegister src);\n-  void xorpd(XMMRegister dst, Address src)     { Assembler::xorpd(dst, src); }\n-  void xorpd(XMMRegister dst, AddressLiteral src, Register scratch_reg = rscratch1);\n+  void xorpd(XMMRegister dst, XMMRegister    src);\n+  void xorpd(XMMRegister dst, Address        src) { Assembler::xorpd(dst, src); }\n+  void xorpd(XMMRegister dst, AddressLiteral src, Register rscratch = rscratch1);\n@@ -1344,3 +1345,3 @@\n-  void xorps(XMMRegister dst, XMMRegister src);\n-  void xorps(XMMRegister dst, Address src)     { Assembler::xorps(dst, src); }\n-  void xorps(XMMRegister dst, AddressLiteral src, Register scratch_reg = rscratch1);\n+  void xorps(XMMRegister dst, XMMRegister    src);\n+  void xorps(XMMRegister dst, Address        src) { Assembler::xorps(dst, src); }\n+  void xorps(XMMRegister dst, AddressLiteral src, Register rscratch = rscratch1);\n@@ -1365,2 +1366,2 @@\n-  void vpaddb(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len);\n-  void vpaddb(XMMRegister dst, XMMRegister nds, Address src, int vector_len);\n+  void vpaddb(XMMRegister dst, XMMRegister nds, XMMRegister    src, int vector_len);\n+  void vpaddb(XMMRegister dst, XMMRegister nds, Address        src, int vector_len);\n@@ -1376,3 +1377,3 @@\n-  void vpand(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) { Assembler::vpand(dst, nds, src, vector_len); }\n-  void vpand(XMMRegister dst, XMMRegister nds, Address src, int vector_len) { Assembler::vpand(dst, nds, src, vector_len); }\n-  void vpand(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register scratch_reg = rscratch1);\n+  void vpand(XMMRegister dst, XMMRegister nds, XMMRegister    src, int vector_len) { Assembler::vpand(dst, nds, src, vector_len); }\n+  void vpand(XMMRegister dst, XMMRegister nds, Address        src, int vector_len) { Assembler::vpand(dst, nds, src, vector_len); }\n+  void vpand(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register rscratch = rscratch1);\n@@ -1848,17 +1849,8 @@\n-  void movptr(ArrayAddress dst, Register src);\n-  \/\/ can this do an lea?\n-  void movptr(Register dst, ArrayAddress src);\n-\n-  void movptr(Register dst, Address src);\n-\n-#ifdef _LP64\n-  void movptr(Register dst, AddressLiteral src, Register scratch=rscratch1);\n-#else\n-  void movptr(Register dst, AddressLiteral src, Register scratch=noreg); \/\/ Scratch reg is ignored in 32-bit\n-#endif\n-\n-  void movptr(Register dst, intptr_t src);\n-  void movptr(Register dst, Register src);\n-  void movptr(Address dst, intptr_t src);\n-\n-  void movptr(Address dst, Register src);\n+  void movptr(Register     dst, Register       src);\n+  void movptr(Register     dst, Address        src);\n+  void movptr(Register     dst, AddressLiteral src);\n+  void movptr(Register     dst, ArrayAddress   src);\n+  void movptr(Register     dst, intptr_t       src);\n+  void movptr(Address      dst, Register       src);\n+  void movptr(Address      dst, intptr_t       src);\n+  void movptr(ArrayAddress dst, Register       src);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":45,"deletions":53,"binary":false,"changes":98,"status":"modified"},{"patch":"@@ -3749,1 +3749,1 @@\n-instruct reinterpret_expand(vec dst, vec src, rRegP scratch) %{\n+instruct reinterpret_expand(vec dst, vec src) %{\n@@ -3754,2 +3754,2 @@\n-  effect(TEMP dst, TEMP scratch);\n-  format %{ \"vector_reinterpret_expand $dst,$src\\t! using $scratch as TEMP\" %}\n+  effect(TEMP dst);\n+  format %{ \"vector_reinterpret_expand $dst,$src\" %}\n@@ -3762,1 +3762,1 @@\n-      __ movdqu($dst$$XMMRegister, ExternalAddress(vector_32_bit_mask()), $scratch$$Register);\n+      __ movdqu($dst$$XMMRegister, ExternalAddress(vector_32_bit_mask()), noreg);\n@@ -3765,1 +3765,1 @@\n-      __ movdqu($dst$$XMMRegister, ExternalAddress(vector_64_bit_mask()), $scratch$$Register);\n+      __ movdqu($dst$$XMMRegister, ExternalAddress(vector_64_bit_mask()), noreg);\n@@ -3772,1 +3772,1 @@\n-instruct vreinterpret_expand4(legVec dst, vec src, rRegP scratch) %{\n+instruct vreinterpret_expand4(legVec dst, vec src) %{\n@@ -3779,2 +3779,1 @@\n-  effect(TEMP scratch);\n-  format %{ \"vector_reinterpret_expand $dst,$src\\t! using $scratch as TEMP\" %}\n+  format %{ \"vector_reinterpret_expand $dst,$src\" %}\n@@ -3782,1 +3781,1 @@\n-    __ vpand($dst$$XMMRegister, $src$$XMMRegister, ExternalAddress(vector_32_bit_mask()), 0, $scratch$$Register);\n+    __ vpand($dst$$XMMRegister, $src$$XMMRegister, ExternalAddress(vector_32_bit_mask()), 0, noreg);\n@@ -3850,1 +3849,1 @@\n-instruct roundD_imm(legRegD dst, immD con, immU8 rmode, rRegI scratch_reg) %{\n+instruct roundD_imm(legRegD dst, immD con, immU8 rmode) %{\n@@ -3852,1 +3851,0 @@\n-  effect(TEMP scratch_reg);\n@@ -3857,1 +3855,1 @@\n-    __ roundsd($dst$$XMMRegister, $constantaddress($con), $rmode$$constant, $scratch_reg$$Register);\n+    __ roundsd($dst$$XMMRegister, $constantaddress($con), $rmode$$constant, noreg);\n@@ -4018,1 +4016,1 @@\n-      __ movdqu($mask$$XMMRegister, ExternalAddress(vector_all_bits_set()));\n+      __ movdqu($mask$$XMMRegister, ExternalAddress(vector_all_bits_set()), noreg);\n@@ -4020,1 +4018,1 @@\n-      __ vmovdqu($mask$$XMMRegister, ExternalAddress(vector_all_bits_set()));\n+      __ vmovdqu($mask$$XMMRegister, ExternalAddress(vector_all_bits_set()), noreg);\n@@ -4041,1 +4039,1 @@\n-    __ kmovwl($ktmp$$KRegister, ExternalAddress(vector_all_bits_set()), $tmp$$Register);\n+    __ kmovwl($ktmp$$KRegister, ExternalAddress(vector_all_bits_set()), noreg);\n@@ -4082,1 +4080,1 @@\n-    __ kmovwl($ktmp$$KRegister, ExternalAddress(vector_all_bits_set()), $tmp$$Register);\n+    __ kmovwl($ktmp$$KRegister, ExternalAddress(vector_all_bits_set()), noreg);\n@@ -5620,1 +5618,1 @@\n-instruct mulB_reg(vec dst, vec src1, vec src2, vec tmp, rRegI scratch) %{\n+instruct mulB_reg(vec dst, vec src1, vec src2, vec tmp) %{\n@@ -5624,1 +5622,1 @@\n-  effect(TEMP dst, TEMP tmp, TEMP scratch);\n+  effect(TEMP dst, TEMP tmp);\n@@ -5631,1 +5629,1 @@\n-    __ movdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), $scratch$$Register);\n+    __ movdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), noreg);\n@@ -5638,1 +5636,1 @@\n-instruct mul16B_reg(vec dst, vec src1, vec src2, vec tmp1, vec tmp2, rRegI scratch) %{\n+instruct mul16B_reg(vec dst, vec src1, vec src2, vec tmp1, vec tmp2) %{\n@@ -5641,1 +5639,1 @@\n-  effect(TEMP dst, TEMP tmp1, TEMP tmp2, TEMP scratch);\n+  effect(TEMP dst, TEMP tmp1, TEMP tmp2);\n@@ -5653,1 +5651,1 @@\n-    __ movdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), $scratch$$Register);\n+    __ movdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), noreg);\n@@ -5661,1 +5659,1 @@\n-instruct vmul16B_reg_avx(vec dst, vec src1, vec src2, vec tmp, rRegI scratch) %{\n+instruct vmul16B_reg_avx(vec dst, vec src1, vec src2, vec tmp) %{\n@@ -5664,1 +5662,1 @@\n-  effect(TEMP dst, TEMP tmp, TEMP scratch);\n+  effect(TEMP dst, TEMP tmp);\n@@ -5671,1 +5669,1 @@\n-    __ vmovdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), $scratch$$Register);\n+    __ vmovdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), noreg);\n@@ -5679,1 +5677,1 @@\n-instruct vmul32B_reg_avx(vec dst, vec src1, vec src2, vec tmp1, vec tmp2, rRegI scratch) %{\n+instruct vmul32B_reg_avx(vec dst, vec src1, vec src2, vec tmp1, vec tmp2) %{\n@@ -5682,1 +5680,1 @@\n-  effect(TEMP dst, TEMP tmp1, TEMP tmp2, TEMP scratch);\n+  effect(TEMP dst, TEMP tmp1, TEMP tmp2);\n@@ -5695,1 +5693,1 @@\n-    __ vmovdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), $scratch$$Register);\n+    __ vmovdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), noreg);\n@@ -5705,1 +5703,1 @@\n-instruct vmul64B_reg_avx(vec dst, vec src1, vec src2, vec tmp1, vec tmp2, rRegI scratch) %{\n+instruct vmul64B_reg_avx(vec dst, vec src1, vec src2, vec tmp1, vec tmp2) %{\n@@ -5708,1 +5706,1 @@\n-  effect(TEMP dst, TEMP tmp1, TEMP tmp2, TEMP scratch);\n+  effect(TEMP dst, TEMP tmp1, TEMP tmp2);\n@@ -5721,1 +5719,1 @@\n-    __ vmovdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), $scratch$$Register);\n+    __ vmovdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), noreg);\n@@ -5726,1 +5724,1 @@\n-    __ evmovdquq($tmp2$$XMMRegister, ExternalAddress(vector_byte_perm_mask()), vlen_enc, $scratch$$Register);\n+    __ evmovdquq($tmp2$$XMMRegister, ExternalAddress(vector_byte_perm_mask()), vlen_enc, noreg);\n@@ -6190,1 +6188,1 @@\n-instruct signumF_reg(regF dst, regF zero, regF one, rRegP scratch, rFlagsReg cr) %{\n+instruct signumF_reg(regF dst, regF zero, regF one, rFlagsReg cr) %{\n@@ -6192,2 +6190,2 @@\n-  effect(TEMP scratch, KILL cr);\n-  format %{ \"signumF $dst, $dst\\t! using $scratch as TEMP\" %}\n+  effect(KILL cr);\n+  format %{ \"signumF $dst, $dst\" %}\n@@ -6196,1 +6194,1 @@\n-    __ signum_fp(opcode, $dst$$XMMRegister, $zero$$XMMRegister, $one$$XMMRegister, $scratch$$Register);\n+    __ signum_fp(opcode, $dst$$XMMRegister, $zero$$XMMRegister, $one$$XMMRegister);\n@@ -6201,1 +6199,1 @@\n-instruct signumD_reg(regD dst, regD zero, regD one, rRegP scratch, rFlagsReg cr) %{\n+instruct signumD_reg(regD dst, regD zero, regD one, rFlagsReg cr) %{\n@@ -6203,2 +6201,2 @@\n-  effect(TEMP scratch, KILL cr);\n-  format %{ \"signumD $dst, $dst\\t! using $scratch as TEMP\" %}\n+  effect(KILL cr);\n+  format %{ \"signumD $dst, $dst\" %}\n@@ -6207,1 +6205,1 @@\n-    __ signum_fp(opcode, $dst$$XMMRegister, $zero$$XMMRegister, $one$$XMMRegister, $scratch$$Register);\n+    __ signum_fp(opcode, $dst$$XMMRegister, $zero$$XMMRegister, $one$$XMMRegister);\n@@ -6400,1 +6398,1 @@\n-instruct vshiftB(vec dst, vec src, vec shift, vec tmp, rRegI scratch) %{\n+instruct vshiftB(vec dst, vec src, vec shift, vec tmp) %{\n@@ -6405,1 +6403,1 @@\n-  effect(TEMP dst, USE src, USE shift, TEMP tmp, TEMP scratch);\n+  effect(TEMP dst, USE src, USE shift, TEMP tmp);\n@@ -6413,1 +6411,1 @@\n-    __ movdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), $scratch$$Register);\n+    __ movdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), noreg);\n@@ -6420,1 +6418,1 @@\n-instruct vshift16B(vec dst, vec src, vec shift, vec tmp1, vec tmp2, rRegI scratch) %{\n+instruct vshift16B(vec dst, vec src, vec shift, vec tmp1, vec tmp2) %{\n@@ -6426,1 +6424,1 @@\n-  effect(TEMP dst, USE src, USE shift, TEMP tmp1, TEMP tmp2, TEMP scratch);\n+  effect(TEMP dst, USE src, USE shift, TEMP tmp1, TEMP tmp2);\n@@ -6437,1 +6435,1 @@\n-    __ movdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), $scratch$$Register);\n+    __ movdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), noreg);\n@@ -6445,1 +6443,1 @@\n-instruct vshift16B_avx(vec dst, vec src, vec shift, vec tmp, rRegI scratch) %{\n+instruct vshift16B_avx(vec dst, vec src, vec shift, vec tmp) %{\n@@ -6451,1 +6449,1 @@\n-  effect(TEMP dst, TEMP tmp, TEMP scratch);\n+  effect(TEMP dst, TEMP tmp);\n@@ -6459,1 +6457,1 @@\n-    __ vpand($tmp$$XMMRegister, $tmp$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), vlen_enc, $scratch$$Register);\n+    __ vpand($tmp$$XMMRegister, $tmp$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), vlen_enc, noreg);\n@@ -6466,1 +6464,1 @@\n-instruct vshift32B_avx(vec dst, vec src, vec shift, vec tmp, rRegI scratch) %{\n+instruct vshift32B_avx(vec dst, vec src, vec shift, vec tmp) %{\n@@ -6471,1 +6469,1 @@\n-  effect(TEMP dst, TEMP tmp, TEMP scratch);\n+  effect(TEMP dst, TEMP tmp);\n@@ -6483,2 +6481,2 @@\n-    __ vpand($tmp$$XMMRegister, $tmp$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), vlen_enc, $scratch$$Register);\n-    __ vpand($dst$$XMMRegister, $dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), vlen_enc, $scratch$$Register);\n+    __ vpand($tmp$$XMMRegister, $tmp$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), vlen_enc, noreg);\n+    __ vpand($dst$$XMMRegister, $dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), vlen_enc, noreg);\n@@ -6491,1 +6489,1 @@\n-instruct vshift64B_avx(vec dst, vec src, vec shift, vec tmp1, vec tmp2, rRegI scratch) %{\n+instruct vshift64B_avx(vec dst, vec src, vec shift, vec tmp1, vec tmp2) %{\n@@ -6496,1 +6494,1 @@\n-  effect(TEMP dst, TEMP tmp1, TEMP tmp2, TEMP scratch);\n+  effect(TEMP dst, TEMP tmp1, TEMP tmp2);\n@@ -6508,1 +6506,1 @@\n-    __ vmovdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), $scratch$$Register);\n+    __ vmovdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), noreg);\n@@ -6513,1 +6511,1 @@\n-    __ evmovdquq($tmp2$$XMMRegister, ExternalAddress(vector_byte_perm_mask()), vlen_enc, $scratch$$Register);\n+    __ evmovdquq($tmp2$$XMMRegister, ExternalAddress(vector_byte_perm_mask()), vlen_enc, noreg);\n@@ -6650,1 +6648,1 @@\n-instruct vshiftL_arith_reg(vec dst, vec src, vec shift, vec tmp, rRegI scratch) %{\n+instruct vshiftL_arith_reg(vec dst, vec src, vec shift, vec tmp) %{\n@@ -6653,1 +6651,1 @@\n-  effect(TEMP dst, TEMP tmp, TEMP scratch);\n+  effect(TEMP dst, TEMP tmp);\n@@ -6661,1 +6659,1 @@\n-      __ movdqu($tmp$$XMMRegister, ExternalAddress(vector_long_sign_mask()), $scratch$$Register);\n+      __ movdqu($tmp$$XMMRegister, ExternalAddress(vector_long_sign_mask()), noreg);\n@@ -6670,1 +6668,1 @@\n-      __ vmovdqu($tmp$$XMMRegister, ExternalAddress(vector_long_sign_mask()), $scratch$$Register);\n+      __ vmovdqu($tmp$$XMMRegister, ExternalAddress(vector_long_sign_mask()), noreg);\n@@ -6692,1 +6690,1 @@\n-instruct vshift8B_var_nobw(vec dst, vec src, vec shift, vec vtmp, rRegP scratch) %{\n+instruct vshift8B_var_nobw(vec dst, vec src, vec shift, vec vtmp) %{\n@@ -6699,2 +6697,2 @@\n-  effect(TEMP dst, TEMP vtmp, TEMP scratch);\n-  format %{ \"vector_varshift_byte $dst, $src, $shift\\n\\t! using $vtmp, $scratch as TEMP\" %}\n+  effect(TEMP dst, TEMP vtmp);\n+  format %{ \"vector_varshift_byte $dst, $src, $shift\\n\\t! using $vtmp as TEMP\" %}\n@@ -6706,1 +6704,1 @@\n-    __ varshiftbw(opcode, $dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vlen_enc, $vtmp$$XMMRegister, $scratch$$Register);\n+    __ varshiftbw(opcode, $dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vlen_enc, $vtmp$$XMMRegister);\n@@ -6712,1 +6710,1 @@\n-instruct vshift16B_var_nobw(vec dst, vec src, vec shift, vec vtmp1, vec vtmp2, rRegP scratch) %{\n+instruct vshift16B_var_nobw(vec dst, vec src, vec shift, vec vtmp1, vec vtmp2) %{\n@@ -6719,2 +6717,2 @@\n-  effect(TEMP dst, TEMP vtmp1, TEMP vtmp2, TEMP scratch);\n-  format %{ \"vector_varshift_byte $dst, $src, $shift\\n\\t! using $vtmp1, $vtmp2 and $scratch as TEMP\" %}\n+  effect(TEMP dst, TEMP vtmp1, TEMP vtmp2);\n+  format %{ \"vector_varshift_byte $dst, $src, $shift\\n\\t! using $vtmp1, $vtmp2 as TEMP\" %}\n@@ -6727,1 +6725,1 @@\n-    __ varshiftbw(opcode, $dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vlen_enc, $vtmp1$$XMMRegister, $scratch$$Register);\n+    __ varshiftbw(opcode, $dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vlen_enc, $vtmp1$$XMMRegister);\n@@ -6732,1 +6730,1 @@\n-    __ varshiftbw(opcode, $vtmp1$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister, vlen_enc, $vtmp2$$XMMRegister, $scratch$$Register);\n+    __ varshiftbw(opcode, $vtmp1$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister, vlen_enc, $vtmp2$$XMMRegister);\n@@ -6740,1 +6738,1 @@\n-instruct vshift32B_var_nobw(vec dst, vec src, vec shift, vec vtmp1, vec vtmp2, vec vtmp3, vec vtmp4, rRegP scratch) %{\n+instruct vshift32B_var_nobw(vec dst, vec src, vec shift, vec vtmp1, vec vtmp2, vec vtmp3, vec vtmp4) %{\n@@ -6747,2 +6745,2 @@\n-  effect(TEMP dst, TEMP vtmp1, TEMP vtmp2, TEMP vtmp3, TEMP vtmp4, TEMP scratch);\n-  format %{ \"vector_varshift_byte $dst, $src, $shift\\n\\t using $vtmp1, $vtmp2, $vtmp3, $vtmp4 and $scratch as TEMP\" %}\n+  effect(TEMP dst, TEMP vtmp1, TEMP vtmp2, TEMP vtmp3, TEMP vtmp4);\n+  format %{ \"vector_varshift_byte $dst, $src, $shift\\n\\t using $vtmp1, $vtmp2, $vtmp3, $vtmp4 as TEMP\" %}\n@@ -6755,1 +6753,1 @@\n-    __ varshiftbw(opcode, $dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vlen_enc, $vtmp1$$XMMRegister, $scratch$$Register);\n+    __ varshiftbw(opcode, $dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vlen_enc, $vtmp1$$XMMRegister);\n@@ -6758,1 +6756,1 @@\n-    __ varshiftbw(opcode, $vtmp1$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister, vlen_enc, $vtmp2$$XMMRegister, $scratch$$Register);\n+    __ varshiftbw(opcode, $vtmp1$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister, vlen_enc, $vtmp2$$XMMRegister);\n@@ -6764,1 +6762,1 @@\n-    __ varshiftbw(opcode, $vtmp3$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister, vlen_enc, $vtmp4$$XMMRegister, $scratch$$Register);\n+    __ varshiftbw(opcode, $vtmp3$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister, vlen_enc, $vtmp4$$XMMRegister);\n@@ -6767,1 +6765,1 @@\n-    __ varshiftbw(opcode, $vtmp1$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister, vlen_enc, $vtmp2$$XMMRegister, $scratch$$Register);\n+    __ varshiftbw(opcode, $vtmp1$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister, vlen_enc, $vtmp2$$XMMRegister);\n@@ -6776,1 +6774,1 @@\n-instruct vshiftB_var_evex_bw(vec dst, vec src, vec shift, vec vtmp, rRegP scratch) %{\n+instruct vshiftB_var_evex_bw(vec dst, vec src, vec shift, vec vtmp) %{\n@@ -6783,2 +6781,2 @@\n-  effect(TEMP dst, TEMP vtmp, TEMP scratch);\n-  format %{ \"vector_varshift_byte $dst, $src, $shift\\n\\t! using $vtmp, $scratch as TEMP\" %}\n+  effect(TEMP dst, TEMP vtmp);\n+  format %{ \"vector_varshift_byte $dst, $src, $shift\\n\\t! using $vtmp as TEMP\" %}\n@@ -6790,1 +6788,1 @@\n-    __ evarshiftb(opcode, $dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vlen_enc, $vtmp$$XMMRegister, $scratch$$Register);\n+    __ evarshiftb(opcode, $dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vlen_enc, $vtmp$$XMMRegister);\n@@ -6795,1 +6793,1 @@\n-instruct vshift64B_var_evex_bw(vec dst, vec src, vec shift, vec vtmp1, vec vtmp2, rRegP scratch) %{\n+instruct vshift64B_var_evex_bw(vec dst, vec src, vec shift, vec vtmp1, vec vtmp2) %{\n@@ -6802,2 +6800,2 @@\n-  effect(TEMP dst, TEMP vtmp1, TEMP vtmp2, TEMP scratch);\n-  format %{ \"vector_varshift_byte $dst, $src, $shift\\n\\t! using $vtmp1, $vtmp2 and $scratch as TEMP\" %}\n+  effect(TEMP dst, TEMP vtmp1, TEMP vtmp2);\n+  format %{ \"vector_varshift_byte $dst, $src, $shift\\n\\t! using $vtmp1, $vtmp2 as TEMP\" %}\n@@ -6809,1 +6807,1 @@\n-    __ evarshiftb(opcode, $dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vlen_enc, $vtmp1$$XMMRegister, $scratch$$Register);\n+    __ evarshiftb(opcode, $dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vlen_enc, $vtmp1$$XMMRegister);\n@@ -6812,1 +6810,1 @@\n-    __ evarshiftb(opcode, $vtmp1$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister, vlen_enc, $vtmp2$$XMMRegister, $scratch$$Register);\n+    __ evarshiftb(opcode, $vtmp1$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister, vlen_enc, $vtmp2$$XMMRegister);\n@@ -6819,1 +6817,1 @@\n-instruct vshift8S_var_nobw(vec dst, vec src, vec shift, vec vtmp, rRegP scratch) %{\n+instruct vshift8S_var_nobw(vec dst, vec src, vec shift, vec vtmp) %{\n@@ -6826,1 +6824,1 @@\n-  effect(TEMP dst, TEMP vtmp, TEMP scratch);\n+  effect(TEMP dst, TEMP vtmp);\n@@ -6837,1 +6835,1 @@\n-    __ vpand($dst$$XMMRegister, $dst$$XMMRegister, ExternalAddress(vector_int_to_short_mask()), vlen_enc, $scratch$$Register);\n+    __ vpand($dst$$XMMRegister, $dst$$XMMRegister, ExternalAddress(vector_int_to_short_mask()), vlen_enc, noreg);\n@@ -6844,1 +6842,1 @@\n-instruct vshift16S_var_nobw(vec dst, vec src, vec shift, vec vtmp1, vec vtmp2, rRegP scratch) %{\n+instruct vshift16S_var_nobw(vec dst, vec src, vec shift, vec vtmp1, vec vtmp2) %{\n@@ -6851,1 +6849,1 @@\n-  effect(TEMP dst, TEMP vtmp1, TEMP vtmp2, TEMP scratch);\n+  effect(TEMP dst, TEMP vtmp1, TEMP vtmp2);\n@@ -6863,1 +6861,1 @@\n-    __ vpand($vtmp2$$XMMRegister, $vtmp2$$XMMRegister, ExternalAddress(vector_int_to_short_mask()), vlen_enc, $scratch$$Register);\n+    __ vpand($vtmp2$$XMMRegister, $vtmp2$$XMMRegister, ExternalAddress(vector_int_to_short_mask()), vlen_enc, noreg);\n@@ -6871,1 +6869,1 @@\n-    __ vpand($dst$$XMMRegister, $dst$$XMMRegister, ExternalAddress(vector_int_to_short_mask()), vlen_enc, $scratch$$Register);\n+    __ vpand($dst$$XMMRegister, $dst$$XMMRegister, ExternalAddress(vector_int_to_short_mask()), vlen_enc, noreg);\n@@ -7083,1 +7081,1 @@\n-instruct castStoX(vec dst, vec src, rRegP scratch) %{\n+instruct castStoX(vec dst, vec src) %{\n@@ -7087,1 +7085,0 @@\n-  effect(TEMP scratch);\n@@ -7089,1 +7086,1 @@\n-  format %{ \"vector_cast_s2x $dst,$src\\t! using $scratch as TEMP\" %}\n+  format %{ \"vector_cast_s2x $dst,$src\" %}\n@@ -7093,1 +7090,1 @@\n-    __ vpand($dst$$XMMRegister, $src$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), 0, $scratch$$Register);\n+    __ vpand($dst$$XMMRegister, $src$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), 0, noreg);\n@@ -7099,1 +7096,1 @@\n-instruct vcastStoX(vec dst, vec src, vec vtmp, rRegP scratch) %{\n+instruct vcastStoX(vec dst, vec src, vec vtmp) %{\n@@ -7103,1 +7100,1 @@\n-  effect(TEMP dst, TEMP vtmp, TEMP scratch);\n+  effect(TEMP dst, TEMP vtmp);\n@@ -7105,1 +7102,1 @@\n-  format %{ \"vector_cast_s2x $dst,$src\\t! using $vtmp, $scratch as TEMP\" %}\n+  format %{ \"vector_cast_s2x $dst,$src\\t! using $vtmp as TEMP\" %}\n@@ -7110,1 +7107,1 @@\n-    __ vpand($dst$$XMMRegister, $src$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), vlen_enc, $scratch$$Register);\n+    __ vpand($dst$$XMMRegister, $src$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), vlen_enc, noreg);\n@@ -7156,1 +7153,1 @@\n-instruct castItoX(vec dst, vec src, rRegP scratch) %{\n+instruct castItoX(vec dst, vec src) %{\n@@ -7161,2 +7158,1 @@\n-  format %{ \"vector_cast_i2x $dst,$src\\t! using $scratch as TEMP\" %}\n-  effect(TEMP scratch);\n+  format %{ \"vector_cast_i2x $dst,$src\" %}\n@@ -7170,1 +7166,1 @@\n-      __ vpand($dst$$XMMRegister, $src$$XMMRegister, ExternalAddress(vector_int_to_byte_mask()), vlen_enc, $scratch$$Register);\n+      __ vpand($dst$$XMMRegister, $src$$XMMRegister, ExternalAddress(vector_int_to_byte_mask()), vlen_enc, noreg);\n@@ -7175,1 +7171,1 @@\n-      __ vpand($dst$$XMMRegister, $src$$XMMRegister, ExternalAddress(vector_int_to_short_mask()), vlen_enc, $scratch$$Register);\n+      __ vpand($dst$$XMMRegister, $src$$XMMRegister, ExternalAddress(vector_int_to_short_mask()), vlen_enc, noreg);\n@@ -7182,1 +7178,1 @@\n-instruct vcastItoX(vec dst, vec src, vec vtmp, rRegP scratch) %{\n+instruct vcastItoX(vec dst, vec src, vec vtmp) %{\n@@ -7187,2 +7183,2 @@\n-  format %{ \"vector_cast_i2x $dst,$src\\t! using $vtmp and $scratch as TEMP\" %}\n-  effect(TEMP dst, TEMP vtmp, TEMP scratch);\n+  format %{ \"vector_cast_i2x $dst,$src\\t! using $vtmp as TEMP\" %}\n+  effect(TEMP dst, TEMP vtmp);\n@@ -7196,1 +7192,1 @@\n-      __ vpand($vtmp$$XMMRegister, $src$$XMMRegister, ExternalAddress(vector_int_to_byte_mask()), vlen_enc, $scratch$$Register);\n+      __ vpand($vtmp$$XMMRegister, $src$$XMMRegister, ExternalAddress(vector_int_to_byte_mask()), vlen_enc, noreg);\n@@ -7202,1 +7198,1 @@\n-      __ vpand($vtmp$$XMMRegister, $src$$XMMRegister, ExternalAddress(vector_int_to_short_mask()), vlen_enc, $scratch$$Register);\n+      __ vpand($vtmp$$XMMRegister, $src$$XMMRegister, ExternalAddress(vector_int_to_short_mask()), vlen_enc, noreg);\n@@ -7250,1 +7246,1 @@\n-instruct vcastLtoBS(vec dst, vec src, rRegP scratch) %{\n+instruct vcastLtoBS(vec dst, vec src) %{\n@@ -7254,2 +7250,1 @@\n-  effect(TEMP scratch);\n-  format %{ \"vector_cast_l2x  $dst,$src\\t! using $scratch as TEMP\" %}\n+  format %{ \"vector_cast_l2x  $dst,$src\" %}\n@@ -7265,1 +7260,1 @@\n-      __ vpand($dst$$XMMRegister, $dst$$XMMRegister, mask_addr, Assembler::AVX_128bit, $scratch$$Register);\n+      __ vpand($dst$$XMMRegister, $dst$$XMMRegister, mask_addr, Assembler::AVX_128bit, noreg);\n@@ -7271,1 +7266,1 @@\n-      __ vpand($dst$$XMMRegister, $dst$$XMMRegister, mask_addr, Assembler::AVX_128bit, $scratch$$Register);\n+      __ vpand($dst$$XMMRegister, $dst$$XMMRegister, mask_addr, Assembler::AVX_128bit, noreg);\n@@ -7353,1 +7348,1 @@\n-instruct castFtoI_reg_avx(vec dst, vec src, vec xtmp1, vec xtmp2, vec xtmp3, vec xtmp4, rRegP scratch, rFlagsReg cr) %{\n+instruct castFtoI_reg_avx(vec dst, vec src, vec xtmp1, vec xtmp2, vec xtmp3, vec xtmp4, rFlagsReg cr) %{\n@@ -7360,2 +7355,2 @@\n-  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, TEMP xtmp4, TEMP scratch, KILL cr);\n-  format %{ \"vector_cast_f2i $dst,$src\\t! using $xtmp1, $xtmp2, $xtmp3, $xtmp4 and $scratch as TEMP\" %}\n+  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, TEMP xtmp4, KILL cr);\n+  format %{ \"vector_cast_f2i $dst,$src\\t! using $xtmp1, $xtmp2, $xtmp3, $xtmp4 as TEMP\" %}\n@@ -7366,1 +7361,1 @@\n-                          ExternalAddress(vector_float_signflip()), $scratch$$Register, vlen_enc);\n+                          ExternalAddress(vector_float_signflip()), noreg, vlen_enc);\n@@ -7371,1 +7366,1 @@\n-instruct castFtoI_reg_evex(vec dst, vec src, vec xtmp1, vec xtmp2, kReg ktmp1, kReg ktmp2, rRegP scratch, rFlagsReg cr) %{\n+instruct castFtoI_reg_evex(vec dst, vec src, vec xtmp1, vec xtmp2, kReg ktmp1, kReg ktmp2, rFlagsReg cr) %{\n@@ -7376,2 +7371,2 @@\n-  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP ktmp1, TEMP ktmp2, TEMP scratch, KILL cr);\n-  format %{ \"vector_cast_f2i $dst,$src\\t! using $xtmp1, $xtmp2, $ktmp1, $ktmp2 and $scratch as TEMP\" %}\n+  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP ktmp1, TEMP ktmp2, KILL cr);\n+  format %{ \"vector_cast_f2i $dst,$src\\t! using $xtmp1, $xtmp2, $ktmp1, $ktmp2 as TEMP\" %}\n@@ -7382,1 +7377,1 @@\n-                           ExternalAddress(vector_float_signflip()), $scratch$$Register, vlen_enc);\n+                           ExternalAddress(vector_float_signflip()), noreg, vlen_enc);\n@@ -7387,1 +7382,1 @@\n-instruct castFtoX_reg_evex(vec dst, vec src, vec xtmp1, vec xtmp2, kReg ktmp1, kReg ktmp2, rRegP scratch, rFlagsReg cr) %{\n+instruct castFtoX_reg_evex(vec dst, vec src, vec xtmp1, vec xtmp2, kReg ktmp1, kReg ktmp2, rFlagsReg cr) %{\n@@ -7393,2 +7388,2 @@\n-  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP ktmp1, TEMP ktmp2, TEMP scratch, KILL cr);\n-  format %{ \"vector_cast_f2x $dst,$src\\t! using $xtmp1, $xtmp2, $ktmp1, $ktmp2 and $scratch as TEMP\" %}\n+  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP ktmp1, TEMP ktmp2, KILL cr);\n+  format %{ \"vector_cast_f2x $dst,$src\\t! using $xtmp1, $xtmp2, $ktmp1, $ktmp2 as TEMP\" %}\n@@ -7401,1 +7396,1 @@\n-                             ExternalAddress(vector_double_signflip()), $scratch$$Register, vlen_enc);\n+                             ExternalAddress(vector_double_signflip()), noreg, vlen_enc);\n@@ -7406,1 +7401,1 @@\n-                             ExternalAddress(vector_float_signflip()), $scratch$$Register, vlen_enc);\n+                             ExternalAddress(vector_float_signflip()), noreg, vlen_enc);\n@@ -7429,1 +7424,1 @@\n-instruct castDtoX_reg_evex(vec dst, vec src, vec xtmp1, vec xtmp2, kReg ktmp1, kReg ktmp2, rRegP scratch, rFlagsReg cr) %{\n+instruct castDtoX_reg_evex(vec dst, vec src, vec xtmp1, vec xtmp2, kReg ktmp1, kReg ktmp2, rFlagsReg cr) %{\n@@ -7432,2 +7427,2 @@\n-  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP ktmp1, TEMP ktmp2, TEMP scratch, KILL cr);\n-  format %{ \"vector_cast_d2x $dst,$src\\t! using $xtmp1, $xtmp2, $ktmp1, $ktmp2 and $scratch as TEMP\" %}\n+  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP ktmp1, TEMP ktmp2, KILL cr);\n+  format %{ \"vector_cast_d2x $dst,$src\\t! using $xtmp1, $xtmp2, $ktmp1, $ktmp2 as TEMP\" %}\n@@ -7439,1 +7434,1 @@\n-                           ExternalAddress(vector_double_signflip()), $scratch$$Register, vlen_enc);\n+                           ExternalAddress(vector_double_signflip()), noreg, vlen_enc);\n@@ -7531,1 +7526,1 @@\n-instruct evcmpFD64(vec dst, vec src1, vec src2, immI8 cond, rRegP scratch, kReg ktmp) %{\n+instruct evcmpFD64(vec dst, vec src1, vec src2, immI8 cond, kReg ktmp) %{\n@@ -7536,2 +7531,2 @@\n-  effect(TEMP scratch, TEMP ktmp);\n-  format %{ \"vector_compare $dst,$src1,$src2,$cond\\t! using $scratch as TEMP\" %}\n+  effect(TEMP ktmp);\n+  format %{ \"vector_compare $dst,$src1,$src2,$cond\" %}\n@@ -7544,1 +7539,1 @@\n-      __ evmovdqul($dst$$XMMRegister, $ktmp$$KRegister, ExternalAddress(vector_all_bits_set()), false, vlen_enc, $scratch$$Register);\n+      __ evmovdqul($dst$$XMMRegister, $ktmp$$KRegister, ExternalAddress(vector_all_bits_set()), false, vlen_enc, noreg);\n@@ -7547,1 +7542,1 @@\n-      __ evmovdquq($dst$$XMMRegister, $ktmp$$KRegister, ExternalAddress(vector_all_bits_set()), false, vlen_enc, $scratch$$Register);\n+      __ evmovdquq($dst$$XMMRegister, $ktmp$$KRegister, ExternalAddress(vector_all_bits_set()), false, vlen_enc, noreg);\n@@ -7640,1 +7635,1 @@\n-instruct vcmp64(vec dst, vec src1, vec src2, immI8 cond, rRegP scratch, kReg ktmp) %{\n+instruct vcmp64(vec dst, vec src1, vec src2, immI8 cond, kReg ktmp) %{\n@@ -7645,2 +7640,2 @@\n-  effect(TEMP scratch, TEMP ktmp);\n-  format %{ \"vector_compare $dst,$src1,$src2,$cond\\t! using $scratch as TEMP\" %}\n+  effect(TEMP ktmp);\n+  format %{ \"vector_compare $dst,$src1,$src2,$cond\" %}\n@@ -7660,1 +7655,1 @@\n-        __ evmovdqul($dst$$XMMRegister, $ktmp$$KRegister, ExternalAddress(vector_all_bits_set()), merge, vlen_enc, $scratch$$Register);\n+        __ evmovdqul($dst$$XMMRegister, $ktmp$$KRegister, ExternalAddress(vector_all_bits_set()), merge, vlen_enc, noreg);\n@@ -7665,1 +7660,1 @@\n-        __ evmovdquq($dst$$XMMRegister, $ktmp$$KRegister, ExternalAddress(vector_all_bits_set()), merge, vlen_enc, $scratch$$Register);\n+        __ evmovdquq($dst$$XMMRegister, $ktmp$$KRegister, ExternalAddress(vector_all_bits_set()), merge, vlen_enc, noreg);\n@@ -7782,1 +7777,1 @@\n-instruct extractF(legRegF dst, legVec src, immU8 idx, rRegI tmp, legVec vtmp) %{\n+instruct extractF(legRegF dst, legVec src, immU8 idx, legVec vtmp) %{\n@@ -7785,2 +7780,2 @@\n-  effect(TEMP dst, TEMP tmp, TEMP vtmp);\n-  format %{ \"extractF $dst,$src,$idx\\t! using $tmp, $vtmp as TEMP\" %}\n+  effect(TEMP dst, TEMP vtmp);\n+  format %{ \"extractF $dst,$src,$idx\\t! using $vtmp as TEMP\" %}\n@@ -7790,1 +7785,1 @@\n-    __ get_elem(T_FLOAT, $dst$$XMMRegister, $src$$XMMRegister, $idx$$constant, $tmp$$Register, $vtmp$$XMMRegister);\n+    __ get_elem(T_FLOAT, $dst$$XMMRegister, $src$$XMMRegister, $idx$$constant, $vtmp$$XMMRegister);\n@@ -7795,1 +7790,1 @@\n-instruct vextractF(legRegF dst, legVec src, immU8 idx, rRegI tmp, legVec vtmp) %{\n+instruct vextractF(legRegF dst, legVec src, immU8 idx, legVec vtmp) %{\n@@ -7799,2 +7794,2 @@\n-  effect(TEMP tmp, TEMP vtmp);\n-  format %{ \"vextractF $dst,$src,$idx\\t! using $tmp, $vtmp as TEMP\" %}\n+  effect(TEMP vtmp);\n+  format %{ \"vextractF $dst,$src,$idx\\t! using $vtmp as TEMP\" %}\n@@ -7805,1 +7800,1 @@\n-    __ get_elem(T_FLOAT, $dst$$XMMRegister, lane_reg, $idx$$constant, $tmp$$Register);\n+    __ get_elem(T_FLOAT, $dst$$XMMRegister, lane_reg, $idx$$constant);\n@@ -7883,1 +7878,1 @@\n-instruct evblendvp64(vec dst, vec src1, vec src2, vec mask, rRegP scratch, kReg ktmp) %{\n+instruct evblendvp64(vec dst, vec src1, vec src2, vec mask, kReg ktmp) %{\n@@ -7887,2 +7882,2 @@\n-  format %{ \"vector_blend  $dst,$src1,$src2,$mask\\t! using $scratch and k2 as TEMP\" %}\n-  effect(TEMP scratch, TEMP ktmp);\n+  format %{ \"vector_blend  $dst,$src1,$src2,$mask\\t! using k2 as TEMP\" %}\n+  effect(TEMP ktmp);\n@@ -7892,1 +7887,1 @@\n-    __ evpcmp(elem_bt, $ktmp$$KRegister, k0, $mask$$XMMRegister, ExternalAddress(vector_all_bits_set()), Assembler::eq, vlen_enc, $scratch$$Register);\n+    __ evpcmp(elem_bt, $ktmp$$KRegister, k0, $mask$$XMMRegister, ExternalAddress(vector_all_bits_set()), Assembler::eq, vlen_enc, noreg);\n@@ -7899,1 +7894,1 @@\n-instruct evblendvp64_masked(vec dst, vec src1, vec src2, kReg mask, rRegP scratch) %{\n+instruct evblendvp64_masked(vec dst, vec src1, vec src2, kReg mask) %{\n@@ -7904,2 +7899,1 @@\n-  format %{ \"vector_blend  $dst,$src1,$src2,$mask\\t! using $scratch and k2 as TEMP\" %}\n-  effect(TEMP scratch);\n+  format %{ \"vector_blend  $dst,$src1,$src2,$mask\\t! using k2 as TEMP\" %}\n@@ -7981,1 +7975,1 @@\n-instruct vabsnegF(vec dst, vec src, rRegI scratch) %{\n+instruct vabsnegF(vec dst, vec src) %{\n@@ -7985,1 +7979,0 @@\n-  effect(TEMP scratch);\n@@ -7992,1 +7985,1 @@\n-      __ vabsnegf(opcode, $dst$$XMMRegister, $src$$XMMRegister, $scratch$$Register);\n+      __ vabsnegf(opcode, $dst$$XMMRegister, $src$$XMMRegister);\n@@ -7996,1 +7989,1 @@\n-      __ vabsnegf(opcode, $dst$$XMMRegister, $src$$XMMRegister, vlen_enc, $scratch$$Register);\n+      __ vabsnegf(opcode, $dst$$XMMRegister, $src$$XMMRegister, vlen_enc);\n@@ -8002,1 +7995,1 @@\n-instruct vabsneg4F(vec dst, rRegI scratch) %{\n+instruct vabsneg4F(vec dst) %{\n@@ -8006,1 +7999,0 @@\n-  effect(TEMP scratch);\n@@ -8011,1 +8003,1 @@\n-    __ vabsnegf(opcode, $dst$$XMMRegister, $dst$$XMMRegister, $scratch$$Register);\n+    __ vabsnegf(opcode, $dst$$XMMRegister, $dst$$XMMRegister);\n@@ -8016,1 +8008,1 @@\n-instruct vabsnegD(vec dst, vec src, rRegI scratch) %{\n+instruct vabsnegD(vec dst, vec src) %{\n@@ -8019,1 +8011,0 @@\n-  effect(TEMP scratch);\n@@ -8026,1 +8017,1 @@\n-      __ vabsnegd(opcode, $dst$$XMMRegister, $src$$XMMRegister, $scratch$$Register);\n+      __ vabsnegd(opcode, $dst$$XMMRegister, $src$$XMMRegister);\n@@ -8029,1 +8020,1 @@\n-      __ vabsnegd(opcode, $dst$$XMMRegister, $src$$XMMRegister, vlen_enc, $scratch$$Register);\n+      __ vabsnegd(opcode, $dst$$XMMRegister, $src$$XMMRegister, vlen_enc);\n@@ -8221,1 +8212,1 @@\n-instruct loadMask64(kReg dst, vec src, vec xtmp, rRegI tmp) %{\n+instruct loadMask64(kReg dst, vec src, vec xtmp) %{\n@@ -8224,2 +8215,2 @@\n-  effect(TEMP xtmp, TEMP tmp);\n-  format %{ \"vector_loadmask_64byte $dst, $src\\t! using $xtmp and $tmp as TEMP\" %}\n+  effect(TEMP xtmp);\n+  format %{ \"vector_loadmask_64byte $dst, $src\\t! using $xtmp as TEMP\" %}\n@@ -8228,1 +8219,1 @@\n-                        $tmp$$Register, true, Assembler::AVX_512bit);\n+                        true, Assembler::AVX_512bit);\n@@ -8241,1 +8232,1 @@\n-                        noreg, false, vlen_enc);\n+                        false, vlen_enc);\n@@ -8381,1 +8372,1 @@\n-instruct vstoreMask_evex_vectmask(vec dst, kReg mask, immI size, rRegI tmp) %{\n+instruct vstoreMask_evex_vectmask(vec dst, kReg mask, immI size) %{\n@@ -8384,1 +8375,1 @@\n-  effect(TEMP_DEF dst, TEMP tmp);\n+  effect(TEMP_DEF dst);\n@@ -8389,1 +8380,1 @@\n-                 false, Assembler::AVX_512bit, $tmp$$Register);\n+                 false, Assembler::AVX_512bit, noreg);\n@@ -8433,1 +8424,1 @@\n-instruct loadIotaIndices(vec dst, immI_0 src, rRegP scratch) %{\n+instruct loadIotaIndices(vec dst, immI_0 src) %{\n@@ -8436,1 +8427,0 @@\n-  effect(TEMP scratch);\n@@ -8440,1 +8430,1 @@\n-     __ load_iota_indices($dst$$XMMRegister, $scratch$$Register, vlen_in_bytes);\n+     __ load_iota_indices($dst$$XMMRegister, vlen_in_bytes);\n@@ -8446,1 +8436,1 @@\n-instruct VectorPopulateIndex(vec dst, rRegI src1, immI_1 src2, vec vtmp, rRegP scratch) %{\n+instruct VectorPopulateIndex(vec dst, rRegI src1, immI_1 src2, vec vtmp) %{\n@@ -8448,2 +8438,2 @@\n-  effect(TEMP dst, TEMP vtmp, TEMP scratch);\n-  format %{ \"vector_populate_index $dst $src1 $src2\\t! using $vtmp and $scratch as TEMP\" %}\n+  effect(TEMP dst, TEMP vtmp);\n+  format %{ \"vector_populate_index $dst $src1 $src2\\t! using $vtmp as TEMP\" %}\n@@ -8456,1 +8446,1 @@\n-     __ load_iota_indices($dst$$XMMRegister, $scratch$$Register, vlen);\n+     __ load_iota_indices($dst$$XMMRegister, vlen);\n@@ -8465,1 +8455,1 @@\n-instruct VectorPopulateLIndex(vec dst, rRegL src1, immI_1 src2, vec vtmp, rRegP scratch) %{\n+instruct VectorPopulateLIndex(vec dst, rRegL src1, immI_1 src2, vec vtmp) %{\n@@ -8467,2 +8457,2 @@\n-  effect(TEMP dst, TEMP vtmp, TEMP scratch);\n-  format %{ \"vector_populate_index $dst $src1 $src2\\t! using $vtmp and $scratch as TEMP\" %}\n+  effect(TEMP dst, TEMP vtmp);\n+  format %{ \"vector_populate_index $dst $src1 $src2\\t! using $vtmp as TEMP\" %}\n@@ -8475,1 +8465,1 @@\n-     __ load_iota_indices($dst$$XMMRegister, $scratch$$Register, vlen);\n+     __ load_iota_indices($dst$$XMMRegister, vlen);\n@@ -8510,1 +8500,1 @@\n-instruct rearrangeB_avx(legVec dst, legVec src, vec shuffle, legVec vtmp1, legVec vtmp2, rRegP scratch) %{\n+instruct rearrangeB_avx(legVec dst, legVec src, vec shuffle, legVec vtmp1, legVec vtmp2) %{\n@@ -8514,2 +8504,2 @@\n-  effect(TEMP dst, TEMP vtmp1, TEMP vtmp2, TEMP scratch);\n-  format %{ \"vector_rearrange $dst, $shuffle, $src\\t! using $vtmp1, $vtmp2, $scratch as TEMP\" %}\n+  effect(TEMP dst, TEMP vtmp1, TEMP vtmp2);\n+  format %{ \"vector_rearrange $dst, $shuffle, $src\\t! using $vtmp1, $vtmp2 as TEMP\" %}\n@@ -8525,1 +8515,1 @@\n-    __ vpaddb($vtmp2$$XMMRegister, $shuffle$$XMMRegister, ExternalAddress(vector_byte_shufflemask()), Assembler::AVX_256bit, $scratch$$Register);\n+    __ vpaddb($vtmp2$$XMMRegister, $shuffle$$XMMRegister, ExternalAddress(vector_byte_shufflemask()), Assembler::AVX_256bit, noreg);\n@@ -8546,1 +8536,1 @@\n-instruct loadShuffleS(vec dst, vec src, vec vtmp, rRegP scratch) %{\n+instruct loadShuffleS(vec dst, vec src, vec vtmp) %{\n@@ -8550,2 +8540,2 @@\n-  effect(TEMP dst, TEMP vtmp, TEMP scratch);\n-  format %{ \"vector_load_shuffle $dst, $src\\t! using $vtmp and $scratch as TEMP\" %}\n+  effect(TEMP dst, TEMP vtmp);\n+  format %{ \"vector_load_shuffle $dst, $src\\t! using $vtmp as TEMP\" %}\n@@ -8568,1 +8558,1 @@\n-      __ movdqu($vtmp$$XMMRegister, ExternalAddress(vector_short_shufflemask()), $scratch$$Register);\n+      __ movdqu($vtmp$$XMMRegister, ExternalAddress(vector_short_shufflemask()), noreg);\n@@ -8582,1 +8572,1 @@\n-      __ vpaddb($dst$$XMMRegister, $dst$$XMMRegister, ExternalAddress(vector_short_shufflemask()), vlen_enc, $scratch$$Register);\n+      __ vpaddb($dst$$XMMRegister, $dst$$XMMRegister, ExternalAddress(vector_short_shufflemask()), vlen_enc, noreg);\n@@ -8600,1 +8590,1 @@\n-instruct rearrangeS_avx(legVec dst, legVec src, vec shuffle, legVec vtmp1, legVec vtmp2, rRegP scratch) %{\n+instruct rearrangeS_avx(legVec dst, legVec src, vec shuffle, legVec vtmp1, legVec vtmp2) %{\n@@ -8604,2 +8594,2 @@\n-  effect(TEMP dst, TEMP vtmp1, TEMP vtmp2, TEMP scratch);\n-  format %{ \"vector_rearrange $dst, $shuffle, $src\\t! using $vtmp1, $vtmp2, $scratch as TEMP\" %}\n+  effect(TEMP dst, TEMP vtmp1, TEMP vtmp2);\n+  format %{ \"vector_rearrange $dst, $shuffle, $src\\t! using $vtmp1, $vtmp2 as TEMP\" %}\n@@ -8615,1 +8605,1 @@\n-    __ vpaddb($vtmp2$$XMMRegister, $shuffle$$XMMRegister, ExternalAddress(vector_byte_shufflemask()), Assembler::AVX_256bit, $scratch$$Register);\n+    __ vpaddb($vtmp2$$XMMRegister, $shuffle$$XMMRegister, ExternalAddress(vector_byte_shufflemask()), Assembler::AVX_256bit, noreg);\n@@ -8654,1 +8644,1 @@\n-instruct loadShuffleI(vec dst, vec src, vec vtmp, rRegP scratch) %{\n+instruct loadShuffleI(vec dst, vec src, vec vtmp) %{\n@@ -8658,2 +8648,2 @@\n-  effect(TEMP dst, TEMP vtmp, TEMP scratch);\n-  format %{ \"vector_load_shuffle $dst, $src\\t! using $vtmp and $scratch as TEMP\" %}\n+  effect(TEMP dst, TEMP vtmp);\n+  format %{ \"vector_load_shuffle $dst, $src\\t! using $vtmp as TEMP\" %}\n@@ -8678,1 +8668,1 @@\n-    __ movdqu($dst$$XMMRegister, ExternalAddress(vector_int_shufflemask()), $scratch$$Register);\n+    __ movdqu($dst$$XMMRegister, ExternalAddress(vector_int_shufflemask()), noreg);\n@@ -8725,1 +8715,1 @@\n-instruct loadShuffleL(vec dst, vec src, vec vtmp, rRegP scratch) %{\n+instruct loadShuffleL(vec dst, vec src, vec vtmp) %{\n@@ -8729,2 +8719,2 @@\n-  effect(TEMP dst, TEMP vtmp, TEMP scratch);\n-  format %{ \"vector_load_shuffle $dst, $src\\t! using $vtmp and $scratch as TEMP\" %}\n+  effect(TEMP dst, TEMP vtmp);\n+  format %{ \"vector_load_shuffle $dst, $src\\t! using $vtmp as TEMP\" %}\n@@ -8747,1 +8737,1 @@\n-    __ vpaddd($dst$$XMMRegister, $dst$$XMMRegister, ExternalAddress(vector_long_shufflemask()), vlen_enc, $scratch$$Register);\n+    __ vpaddd($dst$$XMMRegister, $dst$$XMMRegister, ExternalAddress(vector_long_shufflemask()), vlen_enc, noreg);\n@@ -9382,1 +9372,1 @@\n-instruct vreverse_reg_gfni(vec dst, vec src, vec xtmp, rRegI rtmp) %{\n+instruct vreverse_reg_gfni(vec dst, vec src, vec xtmp) %{\n@@ -9385,2 +9375,2 @@\n-  effect(TEMP dst, TEMP xtmp, TEMP rtmp);\n-  format %{ \"vector_reverse_bit_gfni $dst, $src!\\t using $rtmp and $xtmp as TEMP\" %}\n+  effect(TEMP dst, TEMP xtmp);\n+  format %{ \"vector_reverse_bit_gfni $dst, $src!\\t using $xtmp as TEMP\" %}\n@@ -9392,1 +9382,1 @@\n-                               addr, $rtmp$$Register, vec_enc);\n+                               addr, noreg, vec_enc);\n@@ -9397,1 +9387,1 @@\n-instruct vreverse_byte_reg(vec dst, vec src, rRegI rtmp) %{\n+instruct vreverse_byte_reg(vec dst, vec src) %{\n@@ -9400,2 +9390,2 @@\n-  effect(TEMP dst, TEMP rtmp);\n-  format %{ \"vector_reverse_byte $dst, $src!\\t using $rtmp as TEMP\" %}\n+  effect(TEMP dst);\n+  format %{ \"vector_reverse_byte $dst, $src\" %}\n@@ -9405,1 +9395,1 @@\n-    __ vector_reverse_byte(bt, $dst$$XMMRegister, $src$$XMMRegister, $rtmp$$Register, vec_enc);\n+    __ vector_reverse_byte(bt, $dst$$XMMRegister, $src$$XMMRegister, vec_enc);\n@@ -10084,1 +10074,1 @@\n-instruct evcmp_masked(kReg dst, vec src1, vec src2, immI8 cond, kReg mask, rRegP scratch) %{\n+instruct evcmp_masked(kReg dst, vec src1, vec src2, immI8 cond, kReg mask) %{\n@@ -10086,2 +10076,1 @@\n-  effect(TEMP scratch);\n-  format %{ \"vcmp_masked $dst, $src1, $src2, $cond, $mask\\t! using $scratch as TEMP\" %}\n+  format %{ \"vcmp_masked $dst, $src1, $src2, $cond, $mask\" %}\n","filename":"src\/hotspot\/cpu\/x86\/x86.ad","additions":209,"deletions":220,"binary":false,"changes":429,"status":"modified"}]}