{"files":[{"patch":"@@ -220,1 +220,1 @@\n-    movptr(tmpReg, ExternalAddress((address) RTMLockingCounters::rtm_calculation_flag_addr()), tmpReg);\n+    movptr(tmpReg, ExternalAddress((address) RTMLockingCounters::rtm_calculation_flag_addr()));\n@@ -969,1 +969,1 @@\n-void C2_MacroAssembler::vabsnegd(int opcode, XMMRegister dst, XMMRegister src, Register scr) {\n+void C2_MacroAssembler::vabsnegd(int opcode, XMMRegister dst, XMMRegister src) {\n@@ -974,1 +974,1 @@\n-    andpd(dst, ExternalAddress(StubRoutines::x86::vector_double_sign_mask()), scr);\n+    andpd(dst, ExternalAddress(StubRoutines::x86::vector_double_sign_mask()), noreg);\n@@ -977,1 +977,1 @@\n-    xorpd(dst, ExternalAddress(StubRoutines::x86::vector_double_sign_flip()), scr);\n+    xorpd(dst, ExternalAddress(StubRoutines::x86::vector_double_sign_flip()), noreg);\n@@ -981,1 +981,1 @@\n-void C2_MacroAssembler::vabsnegd(int opcode, XMMRegister dst, XMMRegister src, int vector_len, Register scr) {\n+void C2_MacroAssembler::vabsnegd(int opcode, XMMRegister dst, XMMRegister src, int vector_len) {\n@@ -983,1 +983,1 @@\n-    vandpd(dst, src, ExternalAddress(StubRoutines::x86::vector_double_sign_mask()), vector_len, scr);\n+    vandpd(dst, src, ExternalAddress(StubRoutines::x86::vector_double_sign_mask()), vector_len, noreg);\n@@ -986,1 +986,1 @@\n-    vxorpd(dst, src, ExternalAddress(StubRoutines::x86::vector_double_sign_flip()), vector_len, scr);\n+    vxorpd(dst, src, ExternalAddress(StubRoutines::x86::vector_double_sign_flip()), vector_len, noreg);\n@@ -990,1 +990,1 @@\n-void C2_MacroAssembler::vabsnegf(int opcode, XMMRegister dst, XMMRegister src, Register scr) {\n+void C2_MacroAssembler::vabsnegf(int opcode, XMMRegister dst, XMMRegister src) {\n@@ -995,1 +995,1 @@\n-    andps(dst, ExternalAddress(StubRoutines::x86::vector_float_sign_mask()), scr);\n+    andps(dst, ExternalAddress(StubRoutines::x86::vector_float_sign_mask()), noreg);\n@@ -998,1 +998,1 @@\n-    xorps(dst, ExternalAddress(StubRoutines::x86::vector_float_sign_flip()), scr);\n+    xorps(dst, ExternalAddress(StubRoutines::x86::vector_float_sign_flip()), noreg);\n@@ -1002,1 +1002,1 @@\n-void C2_MacroAssembler::vabsnegf(int opcode, XMMRegister dst, XMMRegister src, int vector_len, Register scr) {\n+void C2_MacroAssembler::vabsnegf(int opcode, XMMRegister dst, XMMRegister src, int vector_len) {\n@@ -1004,1 +1004,1 @@\n-    vandps(dst, src, ExternalAddress(StubRoutines::x86::vector_float_sign_mask()), vector_len, scr);\n+    vandps(dst, src, ExternalAddress(StubRoutines::x86::vector_float_sign_mask()), vector_len, noreg);\n@@ -1007,1 +1007,1 @@\n-    vxorps(dst, src, ExternalAddress(StubRoutines::x86::vector_float_sign_flip()), vector_len, scr);\n+    vxorps(dst, src, ExternalAddress(StubRoutines::x86::vector_float_sign_flip()), vector_len, noreg);\n@@ -1180,3 +1180,1 @@\n-void C2_MacroAssembler::signum_fp(int opcode, XMMRegister dst,\n-                                  XMMRegister zero, XMMRegister one,\n-                                  Register scratch) {\n+void C2_MacroAssembler::signum_fp(int opcode, XMMRegister dst, XMMRegister zero, XMMRegister one) {\n@@ -1194,1 +1192,1 @@\n-    xorps(dst, ExternalAddress(StubRoutines::x86::vector_float_sign_flip()), scratch);\n+    xorps(dst, ExternalAddress(StubRoutines::x86::vector_float_sign_flip()), noreg);\n@@ -1202,1 +1200,1 @@\n-    xorpd(dst, ExternalAddress(StubRoutines::x86::vector_double_sign_flip()), scratch);\n+    xorpd(dst, ExternalAddress(StubRoutines::x86::vector_double_sign_flip()), noreg);\n@@ -1461,1 +1459,1 @@\n-void C2_MacroAssembler::varshiftbw(int opcode, XMMRegister dst, XMMRegister src, XMMRegister shift, int vector_len, XMMRegister vtmp, Register scratch) {\n+void C2_MacroAssembler::varshiftbw(int opcode, XMMRegister dst, XMMRegister src, XMMRegister shift, int vector_len, XMMRegister vtmp) {\n@@ -1470,1 +1468,1 @@\n-  vpand(dst, dst, ExternalAddress(StubRoutines::x86::vector_int_to_byte_mask()), 1, scratch);\n+  vpand(dst, dst, ExternalAddress(StubRoutines::x86::vector_int_to_byte_mask()), 1, noreg);\n@@ -1476,1 +1474,1 @@\n-void C2_MacroAssembler::evarshiftb(int opcode, XMMRegister dst, XMMRegister src, XMMRegister shift, int vector_len, XMMRegister vtmp, Register scratch) {\n+void C2_MacroAssembler::evarshiftb(int opcode, XMMRegister dst, XMMRegister src, XMMRegister shift, int vector_len, XMMRegister vtmp) {\n@@ -1485,1 +1483,1 @@\n-  vpand(dst, dst, ExternalAddress(StubRoutines::x86::vector_short_to_byte_mask()), ext_vector_len, scratch);\n+  vpand(dst, dst, ExternalAddress(StubRoutines::x86::vector_short_to_byte_mask()), ext_vector_len, noreg);\n@@ -1630,2 +1628,1 @@\n-void C2_MacroAssembler::load_vector_mask(KRegister dst, XMMRegister src, XMMRegister xtmp,\n-                                         Register tmp, bool novlbwdq, int vlen_enc) {\n+void C2_MacroAssembler::load_vector_mask(KRegister dst, XMMRegister src, XMMRegister xtmp, bool novlbwdq, int vlen_enc) {\n@@ -1635,1 +1632,1 @@\n-            Assembler::eq, true, vlen_enc, tmp);\n+            Assembler::eq, true, vlen_enc, noreg);\n@@ -1695,1 +1692,1 @@\n-void C2_MacroAssembler::load_iota_indices(XMMRegister dst, Register scratch, int vlen_in_bytes) {\n+void C2_MacroAssembler::load_iota_indices(XMMRegister dst, int vlen_in_bytes) {\n@@ -1702,1 +1699,1 @@\n-    movdqu(dst, addr, scratch);\n+    movdqu(dst, addr, noreg);\n@@ -1704,1 +1701,1 @@\n-    vmovdqu(dst, addr, scratch);\n+    vmovdqu(dst, addr, noreg);\n@@ -1707,1 +1704,1 @@\n-    evmovdqub(dst, k0, addr, false \/*merge*\/, Assembler::AVX_512bit, scratch);\n+    evmovdqub(dst, k0, addr, false \/*merge*\/, Assembler::AVX_512bit, noreg);\n@@ -2339,1 +2336,1 @@\n-void C2_MacroAssembler::get_elem(BasicType typ, XMMRegister dst, XMMRegister src, int elemindex, Register tmp, XMMRegister vtmp) {\n+void C2_MacroAssembler::get_elem(BasicType typ, XMMRegister dst, XMMRegister src, int elemindex, XMMRegister vtmp) {\n@@ -2368,2 +2365,2 @@\n-      assert((vtmp != xnoreg) && (tmp != noreg), \"required.\");\n-      movdqu(vtmp, ExternalAddress(StubRoutines::x86::vector_32_bit_mask()), tmp);\n+      assert(vtmp != xnoreg, \"required.\");\n+      movdqu(vtmp, ExternalAddress(StubRoutines::x86::vector_32_bit_mask()), noreg);\n@@ -2372,2 +2369,1 @@\n-      assert((tmp != noreg), \"required.\");\n-      vpand(dst, dst, ExternalAddress(StubRoutines::x86::vector_32_bit_mask()), Assembler::AVX_128bit, tmp);\n+      vpand(dst, dst, ExternalAddress(StubRoutines::x86::vector_32_bit_mask()), Assembler::AVX_128bit, noreg);\n@@ -2402,1 +2398,3 @@\n-void C2_MacroAssembler::evpcmp(BasicType typ, KRegister kdmask, KRegister ksmask, XMMRegister src1, AddressLiteral adr, int comparison, int vector_len, Register scratch) {\n+void C2_MacroAssembler::evpcmp(BasicType typ, KRegister kdmask, KRegister ksmask, XMMRegister src1, AddressLiteral adr, int comparison, int vector_len, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(adr), \"missing\");\n+\n@@ -2406,1 +2404,1 @@\n-      evpcmpb(kdmask, ksmask, src1, adr, comparison, \/*signed*\/ true, vector_len, scratch);\n+      evpcmpb(kdmask, ksmask, src1, adr, comparison, \/*signed*\/ true, vector_len, rscratch);\n@@ -2410,1 +2408,1 @@\n-      evpcmpw(kdmask, ksmask, src1, adr, comparison, \/*signed*\/ true, vector_len, scratch);\n+      evpcmpw(kdmask, ksmask, src1, adr, comparison, \/*signed*\/ true, vector_len, rscratch);\n@@ -2414,1 +2412,1 @@\n-      evpcmpd(kdmask, ksmask, src1, adr, comparison, \/*signed*\/ true, vector_len, scratch);\n+      evpcmpd(kdmask, ksmask, src1, adr, comparison, \/*signed*\/ true, vector_len, rscratch);\n@@ -2418,1 +2416,1 @@\n-      evpcmpq(kdmask, ksmask, src1, adr, comparison, \/*signed*\/ true, vector_len, scratch);\n+      evpcmpq(kdmask, ksmask, src1, adr, comparison, \/*signed*\/ true, vector_len, rscratch);\n@@ -4367,1 +4365,1 @@\n-  vmovdqu(xtmp1, float_sign_flip, scratch, vec_enc);\n+  vmovdqu(xtmp1, float_sign_flip, vec_enc, scratch);\n@@ -4972,1 +4970,1 @@\n-  vmovdqu(xtmp2, ExternalAddress(StubRoutines::x86::vector_popcount_lut()), rtmp, vec_enc);\n+  vmovdqu(xtmp2, ExternalAddress(StubRoutines::x86::vector_popcount_lut()), vec_enc, noreg);\n@@ -5077,1 +5075,1 @@\n-    vmovdqu(xtmp1, ExternalAddress(StubRoutines::x86::vector_reverse_bit_lut()), rtmp, vec_enc);\n+    vmovdqu(xtmp1, ExternalAddress(StubRoutines::x86::vector_reverse_bit_lut()), vec_enc, noreg);\n@@ -5091,1 +5089,1 @@\n-    vector_reverse_byte(bt, dst, xtmp2, rtmp, vec_enc);\n+    vector_reverse_byte(bt, dst, xtmp2, vec_enc);\n@@ -5110,1 +5108,1 @@\n-    vmovdqu(xtmp1, ExternalAddress(StubRoutines::x86::vector_reverse_bit_lut()), rtmp, vec_enc);\n+    vmovdqu(xtmp1, ExternalAddress(StubRoutines::x86::vector_reverse_bit_lut()), vec_enc, rtmp);\n@@ -5126,1 +5124,1 @@\n-    vector_reverse_byte(bt, dst, xtmp2, rtmp, vec_enc);\n+    vector_reverse_byte(bt, dst, xtmp2, vec_enc);\n@@ -5137,1 +5135,1 @@\n-  vector_reverse_byte(bt, dst, xtmp, rtmp, vec_enc);\n+  vector_reverse_byte(bt, dst, xtmp, vec_enc);\n@@ -5180,1 +5178,1 @@\n-void C2_MacroAssembler::vector_reverse_byte(BasicType bt, XMMRegister dst, XMMRegister src, Register rtmp, int vec_enc) {\n+void C2_MacroAssembler::vector_reverse_byte(BasicType bt, XMMRegister dst, XMMRegister src, int vec_enc) {\n@@ -5193,1 +5191,1 @@\n-      vmovdqu(dst, ExternalAddress(StubRoutines::x86::vector_reverse_byte_perm_mask_long()), rtmp, vec_enc);\n+      vmovdqu(dst, ExternalAddress(StubRoutines::x86::vector_reverse_byte_perm_mask_long()), vec_enc, noreg);\n@@ -5196,1 +5194,1 @@\n-      vmovdqu(dst, ExternalAddress(StubRoutines::x86::vector_reverse_byte_perm_mask_int()), rtmp, vec_enc);\n+      vmovdqu(dst, ExternalAddress(StubRoutines::x86::vector_reverse_byte_perm_mask_int()), vec_enc, noreg);\n@@ -5200,1 +5198,1 @@\n-      vmovdqu(dst, ExternalAddress(StubRoutines::x86::vector_reverse_byte_perm_mask_short()), rtmp, vec_enc);\n+      vmovdqu(dst, ExternalAddress(StubRoutines::x86::vector_reverse_byte_perm_mask_short()), vec_enc, noreg);\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":48,"deletions":50,"binary":false,"changes":98,"status":"modified"},{"patch":"@@ -73,4 +73,4 @@\n-  void vabsnegd(int opcode, XMMRegister dst, XMMRegister src, Register scr);\n-  void vabsnegd(int opcode, XMMRegister dst, XMMRegister src, int vector_len, Register scr);\n-  void vabsnegf(int opcode, XMMRegister dst, XMMRegister src, Register scr);\n-  void vabsnegf(int opcode, XMMRegister dst, XMMRegister src, int vector_len, Register scr);\n+  void vabsnegd(int opcode, XMMRegister dst, XMMRegister src);\n+  void vabsnegd(int opcode, XMMRegister dst, XMMRegister src, int vector_len);\n+  void vabsnegf(int opcode, XMMRegister dst, XMMRegister src);\n+  void vabsnegf(int opcode, XMMRegister dst, XMMRegister src, int vector_len);\n@@ -93,3 +93,1 @@\n-  void signum_fp(int opcode, XMMRegister dst,\n-                 XMMRegister zero, XMMRegister one,\n-                 Register scratch);\n+  void signum_fp(int opcode, XMMRegister dst, XMMRegister zero, XMMRegister one);\n@@ -124,2 +122,2 @@\n-  void varshiftbw(int opcode, XMMRegister dst, XMMRegister src, XMMRegister shift, int vector_len, XMMRegister vtmp, Register scratch);\n-  void evarshiftb(int opcode, XMMRegister dst, XMMRegister src, XMMRegister shift, int vector_len, XMMRegister vtmp, Register scratch);\n+  void varshiftbw(int opcode, XMMRegister dst, XMMRegister src, XMMRegister shift, int vector_len, XMMRegister vtmp);\n+  void evarshiftb(int opcode, XMMRegister dst, XMMRegister src, XMMRegister shift, int vector_len, XMMRegister vtmp);\n@@ -140,1 +138,1 @@\n-  void get_elem(BasicType typ, XMMRegister dst, XMMRegister src, int elemindex, Register tmp = noreg, XMMRegister vtmp = xnoreg);\n+  void get_elem(BasicType typ, XMMRegister dst, XMMRegister src, int elemindex, XMMRegister vtmp = xnoreg);\n@@ -153,1 +151,1 @@\n-  void evpcmp(BasicType typ, KRegister kdmask, KRegister ksmask, XMMRegister src1, AddressLiteral adr, int comparison, int vector_len, Register scratch = rscratch1);\n+  void evpcmp(BasicType typ, KRegister kdmask, KRegister ksmask, XMMRegister src1, AddressLiteral adr, int comparison, int vector_len, Register rscratch = rscratch1);\n@@ -158,1 +156,1 @@\n-  void load_vector_mask(KRegister dst, XMMRegister src, XMMRegister xtmp, Register tmp, bool novlbwdq, int vlen_enc);\n+  void load_vector_mask(KRegister dst, XMMRegister src, XMMRegister xtmp, bool novlbwdq, int vlen_enc);\n@@ -163,1 +161,1 @@\n-  void load_iota_indices(XMMRegister dst, Register scratch, int vlen_in_bytes);\n+  void load_iota_indices(XMMRegister dst, int vlen_in_bytes);\n@@ -393,1 +391,1 @@\n-  void vector_reverse_byte(BasicType bt, XMMRegister dst, XMMRegister src, Register rtmp, int vec_enc);\n+  void vector_reverse_byte(BasicType bt, XMMRegister dst, XMMRegister src, int vec_enc);\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.hpp","additions":12,"deletions":14,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -313,3 +313,1 @@\n-void MacroAssembler::movptr(Register dst, AddressLiteral src, Register scratch) {\n-  \/\/ scratch register is not used,\n-  \/\/ it is defined to match parameters of 64-bit version of this method.\n+void MacroAssembler::movptr(Register dst, AddressLiteral src) {\n@@ -665,1 +663,1 @@\n-void MacroAssembler::movptr(Register dst, AddressLiteral src, Register scratch) {\n+void MacroAssembler::movptr(Register dst, AddressLiteral src) {\n@@ -672,2 +670,2 @@\n-      lea(scratch, src);\n-      movq(dst, Address(scratch, 0));\n+      lea(dst, src);\n+      movq(dst, Address(dst, 0));\n@@ -2544,2 +2542,2 @@\n-    assert(((src->encoding() < 16) || VM_Version::supports_avx512vl()),\"XMM register should be 0-15\");\n-    Assembler::movdqu(dst, src);\n+  assert(((src->encoding() < 16) || VM_Version::supports_avx512vl()),\"XMM register should be 0-15\");\n+  Assembler::movdqu(dst, src);\n@@ -2549,2 +2547,2 @@\n-    assert(((dst->encoding() < 16) || VM_Version::supports_avx512vl()),\"XMM register should be 0-15\");\n-    Assembler::movdqu(dst, src);\n+  assert(((dst->encoding() < 16) || VM_Version::supports_avx512vl()),\"XMM register should be 0-15\");\n+  Assembler::movdqu(dst, src);\n@@ -2554,2 +2552,2 @@\n-    assert(((dst->encoding() < 16  && src->encoding() < 16) || VM_Version::supports_avx512vl()),\"XMM register should be 0-15\");\n-    Assembler::movdqu(dst, src);\n+  assert(((dst->encoding() < 16  && src->encoding() < 16) || VM_Version::supports_avx512vl()),\"XMM register should be 0-15\");\n+  Assembler::movdqu(dst, src);\n@@ -2558,1 +2556,3 @@\n-void MacroAssembler::movdqu(XMMRegister dst, AddressLiteral src, Register scratchReg) {\n+void MacroAssembler::movdqu(XMMRegister dst, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -2562,2 +2562,2 @@\n-    lea(scratchReg, src);\n-    movdqu(dst, Address(scratchReg, 0));\n+    lea(rscratch, src);\n+    movdqu(dst, Address(rscratch, 0));\n@@ -2568,2 +2568,2 @@\n-    assert(((src->encoding() < 16) || VM_Version::supports_avx512vl()),\"XMM register should be 0-15\");\n-    Assembler::vmovdqu(dst, src);\n+  assert(((src->encoding() < 16) || VM_Version::supports_avx512vl()),\"XMM register should be 0-15\");\n+  Assembler::vmovdqu(dst, src);\n@@ -2573,2 +2573,2 @@\n-    assert(((dst->encoding() < 16) || VM_Version::supports_avx512vl()),\"XMM register should be 0-15\");\n-    Assembler::vmovdqu(dst, src);\n+  assert(((dst->encoding() < 16) || VM_Version::supports_avx512vl()),\"XMM register should be 0-15\");\n+  Assembler::vmovdqu(dst, src);\n@@ -2578,2 +2578,2 @@\n-    assert(((dst->encoding() < 16  && src->encoding() < 16) || VM_Version::supports_avx512vl()),\"XMM register should be 0-15\");\n-    Assembler::vmovdqu(dst, src);\n+  assert(((dst->encoding() < 16  && src->encoding() < 16) || VM_Version::supports_avx512vl()),\"XMM register should be 0-15\");\n+  Assembler::vmovdqu(dst, src);\n@@ -2582,1 +2582,3 @@\n-void MacroAssembler::vmovdqu(XMMRegister dst, AddressLiteral src, Register scratch_reg) {\n+void MacroAssembler::vmovdqu(XMMRegister dst, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src),  \"missing\");\n+\n@@ -2587,2 +2589,2 @@\n-    lea(scratch_reg, src);\n-    vmovdqu(dst, Address(scratch_reg, 0));\n+    lea(rscratch, src);\n+    vmovdqu(dst, Address(rscratch, 0));\n@@ -2592,1 +2594,3 @@\n-void MacroAssembler::vmovdqu(XMMRegister dst, AddressLiteral src, Register scratch_reg, int vector_len) {\n+void MacroAssembler::vmovdqu(XMMRegister dst, AddressLiteral src, int vector_len, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src),  \"missing\");\n+\n@@ -2594,1 +2598,1 @@\n-    evmovdquq(dst, src, AVX_512bit, scratch_reg);\n+    evmovdquq(dst, src, AVX_512bit, rscratch);\n@@ -2596,1 +2600,1 @@\n-    vmovdqu(dst, src, scratch_reg);\n+    vmovdqu(dst, src, rscratch);\n@@ -2598,1 +2602,1 @@\n-    movdqu(dst, src, scratch_reg);\n+    movdqu(dst, src, rscratch);\n@@ -2656,1 +2660,3 @@\n-void MacroAssembler::kmovwl(KRegister dst, AddressLiteral src, Register scratch_reg) {\n+void MacroAssembler::kmovwl(KRegister dst, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -2660,2 +2666,2 @@\n-    lea(scratch_reg, src);\n-    kmovwl(dst, Address(scratch_reg, 0));\n+    lea(rscratch, src);\n+    kmovwl(dst, Address(rscratch, 0));\n@@ -2685,2 +2691,3 @@\n-void MacroAssembler::evmovdqul(XMMRegister dst, KRegister mask, AddressLiteral src, bool merge,\n-                               int vector_len, Register scratch_reg) {\n+void MacroAssembler::evmovdqul(XMMRegister dst, KRegister mask, AddressLiteral src, bool merge, int vector_len, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -2690,2 +2697,2 @@\n-    lea(scratch_reg, src);\n-    Assembler::evmovdqul(dst, mask, Address(scratch_reg, 0), merge, vector_len);\n+    lea(rscratch, src);\n+    Assembler::evmovdqul(dst, mask, Address(rscratch, 0), merge, vector_len);\n@@ -3148,1 +3155,3 @@\n-void MacroAssembler::roundsd(XMMRegister dst, AddressLiteral src, int32_t rmode, Register scratch_reg) {\n+void MacroAssembler::roundsd(XMMRegister dst, AddressLiteral src, int32_t rmode, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -3152,2 +3161,2 @@\n-    lea(scratch_reg, src);\n-    Assembler::roundsd(dst, Address(scratch_reg, 0), rmode);\n+    lea(rscratch, src);\n+    Assembler::roundsd(dst, Address(rscratch, 0), rmode);\n@@ -3184,1 +3193,3 @@\n-void MacroAssembler::xorpd(XMMRegister dst, AddressLiteral src, Register scratch_reg) {\n+void MacroAssembler::xorpd(XMMRegister dst, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -3190,2 +3201,2 @@\n-    lea(scratch_reg, src);\n-    Assembler::xorpd(dst, Address(scratch_reg, 0));\n+    lea(rscratch, src);\n+    Assembler::xorpd(dst, Address(rscratch, 0));\n@@ -3212,1 +3223,3 @@\n-void MacroAssembler::xorps(XMMRegister dst, AddressLiteral src, Register scratch_reg) {\n+void MacroAssembler::xorps(XMMRegister dst, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -3218,2 +3231,2 @@\n-    lea(scratch_reg, src);\n-    Assembler::xorps(dst, Address(scratch_reg, 0));\n+    lea(rscratch, src);\n+    Assembler::xorps(dst, Address(rscratch, 0));\n@@ -3257,0 +3270,2 @@\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -3307,1 +3322,3 @@\n-void MacroAssembler::vpand(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register scratch_reg) {\n+void MacroAssembler::vpand(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src),  \"missing\");\n+\n@@ -3311,2 +3328,2 @@\n-    lea(scratch_reg, src);\n-    Assembler::vpand(dst, nds, Address(scratch_reg, 0), vector_len);\n+    lea(rscratch, src);\n+    Assembler::vpand(dst, nds, Address(rscratch, 0), vector_len);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":64,"deletions":47,"binary":false,"changes":111,"status":"modified"},{"patch":"@@ -1165,11 +1165,11 @@\n-  void movdqu(Address     dst, XMMRegister src);\n-  void movdqu(XMMRegister dst, Address src);\n-  void movdqu(XMMRegister dst, XMMRegister src);\n-  void movdqu(XMMRegister dst, AddressLiteral src, Register scratchReg = rscratch1);\n-\n-  void kmovwl(KRegister dst, Register src) { Assembler::kmovwl(dst, src); }\n-  void kmovwl(Register dst, KRegister src) { Assembler::kmovwl(dst, src); }\n-  void kmovwl(KRegister dst, Address src) { Assembler::kmovwl(dst, src); }\n-  void kmovwl(KRegister dst, AddressLiteral src, Register scratch_reg = rscratch1);\n-  void kmovwl(Address dst,  KRegister src) { Assembler::kmovwl(dst, src); }\n-  void kmovwl(KRegister dst, KRegister src) { Assembler::kmovwl(dst, src); }\n+  void movdqu(Address     dst, XMMRegister    src);\n+  void movdqu(XMMRegister dst, XMMRegister    src);\n+  void movdqu(XMMRegister dst, Address        src);\n+  void movdqu(XMMRegister dst, AddressLiteral src, Register rscratch = rscratch1);\n+\n+  void kmovwl(Register  dst, KRegister      src) { Assembler::kmovwl(dst, src); }\n+  void kmovwl(Address   dst, KRegister      src) { Assembler::kmovwl(dst, src); }\n+  void kmovwl(KRegister dst, KRegister      src) { Assembler::kmovwl(dst, src); }\n+  void kmovwl(KRegister dst, Register       src) { Assembler::kmovwl(dst, src); }\n+  void kmovwl(KRegister dst, Address        src) { Assembler::kmovwl(dst, src); }\n+  void kmovwl(KRegister dst, AddressLiteral src, Register rscratch = rscratch1);\n@@ -1198,5 +1198,5 @@\n-  void vmovdqu(Address     dst, XMMRegister src);\n-  void vmovdqu(XMMRegister dst, Address src);\n-  void vmovdqu(XMMRegister dst, XMMRegister src);\n-  void vmovdqu(XMMRegister dst, AddressLiteral src, Register scratch_reg = rscratch1);\n-  void vmovdqu(XMMRegister dst, AddressLiteral src, Register scratch_reg, int vector_len);\n+  void vmovdqu(Address     dst, XMMRegister    src);\n+  void vmovdqu(XMMRegister dst, Address        src);\n+  void vmovdqu(XMMRegister dst, XMMRegister    src);\n+  void vmovdqu(XMMRegister dst, AddressLiteral src,                 Register rscratch = rscratch1);\n+  void vmovdqu(XMMRegister dst, AddressLiteral src, int vector_len, Register rscratch = rscratch1);\n@@ -1205,2 +1205,2 @@\n-  void evmovdqu(BasicType type, KRegister kmask, Address dst, XMMRegister src,  bool merge, int vector_len);\n-  void evmovdqu(BasicType type, KRegister kmask, XMMRegister dst, Address src, bool merge, int vector_len);\n+  void evmovdqu(BasicType type, KRegister kmask, Address     dst, XMMRegister src, bool merge, int vector_len);\n+  void evmovdqu(BasicType type, KRegister kmask, XMMRegister dst, Address     src, bool merge, int vector_len);\n@@ -1209,1 +1209,2 @@\n-  void evmovdqub(XMMRegister dst, Address src, int vector_len) { Assembler::evmovdqub(dst, src, vector_len); }\n+  void evmovdqub(XMMRegister dst, Address     src, int vector_len) { Assembler::evmovdqub(dst, src, vector_len); }\n+\n@@ -1243,3 +1244,3 @@\n-  void evmovdqul(XMMRegister dst, KRegister mask, Address src, bool merge, int vector_len) { Assembler::evmovdqul(dst, mask, src, merge, vector_len); }\n-  void evmovdqul(Address dst, KRegister mask, XMMRegister src, bool merge, int vector_len) { Assembler::evmovdqul(dst, mask, src, merge, vector_len); }\n-  void evmovdqul(XMMRegister dst, KRegister mask, AddressLiteral src, bool merge, int vector_len, Register scratch_reg);\n+  void evmovdqul(Address     dst, KRegister mask, XMMRegister    src, bool merge, int vector_len) { Assembler::evmovdqul(dst, mask, src, merge, vector_len); }\n+  void evmovdqul(XMMRegister dst, KRegister mask, Address        src, bool merge, int vector_len) { Assembler::evmovdqul(dst, mask, src, merge, vector_len); }\n+  void evmovdqul(XMMRegister dst, KRegister mask, AddressLiteral src, bool merge, int vector_len, Register rscratch);\n@@ -1314,3 +1315,3 @@\n-  void roundsd(XMMRegister dst, XMMRegister src, int32_t rmode)    { Assembler::roundsd(dst, src, rmode); }\n-  void roundsd(XMMRegister dst, Address src, int32_t rmode)        { Assembler::roundsd(dst, src, rmode); }\n-  void roundsd(XMMRegister dst, AddressLiteral src, int32_t rmode, Register scratch_reg);\n+  void roundsd(XMMRegister dst, XMMRegister    src, int32_t rmode) { Assembler::roundsd(dst, src, rmode); }\n+  void roundsd(XMMRegister dst, Address        src, int32_t rmode) { Assembler::roundsd(dst, src, rmode); }\n+  void roundsd(XMMRegister dst, AddressLiteral src, int32_t rmode, Register rscratch);\n@@ -1339,3 +1340,3 @@\n-  void xorpd(XMMRegister dst, XMMRegister src);\n-  void xorpd(XMMRegister dst, Address src)     { Assembler::xorpd(dst, src); }\n-  void xorpd(XMMRegister dst, AddressLiteral src, Register scratch_reg = rscratch1);\n+  void xorpd(XMMRegister dst, XMMRegister    src);\n+  void xorpd(XMMRegister dst, Address        src) { Assembler::xorpd(dst, src); }\n+  void xorpd(XMMRegister dst, AddressLiteral src, Register rscratch = rscratch1);\n@@ -1344,3 +1345,3 @@\n-  void xorps(XMMRegister dst, XMMRegister src);\n-  void xorps(XMMRegister dst, Address src)     { Assembler::xorps(dst, src); }\n-  void xorps(XMMRegister dst, AddressLiteral src, Register scratch_reg = rscratch1);\n+  void xorps(XMMRegister dst, XMMRegister    src);\n+  void xorps(XMMRegister dst, Address        src) { Assembler::xorps(dst, src); }\n+  void xorps(XMMRegister dst, AddressLiteral src, Register rscratch = rscratch1);\n@@ -1365,2 +1366,2 @@\n-  void vpaddb(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len);\n-  void vpaddb(XMMRegister dst, XMMRegister nds, Address src, int vector_len);\n+  void vpaddb(XMMRegister dst, XMMRegister nds, XMMRegister    src, int vector_len);\n+  void vpaddb(XMMRegister dst, XMMRegister nds, Address        src, int vector_len);\n@@ -1376,3 +1377,3 @@\n-  void vpand(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) { Assembler::vpand(dst, nds, src, vector_len); }\n-  void vpand(XMMRegister dst, XMMRegister nds, Address src, int vector_len) { Assembler::vpand(dst, nds, src, vector_len); }\n-  void vpand(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register scratch_reg = rscratch1);\n+  void vpand(XMMRegister dst, XMMRegister nds, XMMRegister    src, int vector_len) { Assembler::vpand(dst, nds, src, vector_len); }\n+  void vpand(XMMRegister dst, XMMRegister nds, Address        src, int vector_len) { Assembler::vpand(dst, nds, src, vector_len); }\n+  void vpand(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register rscratch = rscratch1);\n@@ -1848,17 +1849,8 @@\n-  void movptr(ArrayAddress dst, Register src);\n-  \/\/ can this do an lea?\n-  void movptr(Register dst, ArrayAddress src);\n-\n-  void movptr(Register dst, Address src);\n-\n-#ifdef _LP64\n-  void movptr(Register dst, AddressLiteral src, Register scratch=rscratch1);\n-#else\n-  void movptr(Register dst, AddressLiteral src, Register scratch=noreg); \/\/ Scratch reg is ignored in 32-bit\n-#endif\n-\n-  void movptr(Register dst, intptr_t src);\n-  void movptr(Register dst, Register src);\n-  void movptr(Address dst, intptr_t src);\n-\n-  void movptr(Address dst, Register src);\n+  void movptr(Register     dst, Register       src);\n+  void movptr(Register     dst, Address        src);\n+  void movptr(Register     dst, AddressLiteral src);\n+  void movptr(Register     dst, ArrayAddress   src);\n+  void movptr(Register     dst, intptr_t       src);\n+  void movptr(Address      dst, Register       src);\n+  void movptr(Address      dst, intptr_t       src);\n+  void movptr(ArrayAddress dst, Register       src);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":45,"deletions":53,"binary":false,"changes":98,"status":"modified"},{"patch":"@@ -3745,1 +3745,1 @@\n-instruct reinterpret_expand(vec dst, vec src, rRegP scratch) %{\n+instruct reinterpret_expand(vec dst, vec src) %{\n@@ -3750,2 +3750,2 @@\n-  effect(TEMP dst, TEMP scratch);\n-  format %{ \"vector_reinterpret_expand $dst,$src\\t! using $scratch as TEMP\" %}\n+  effect(TEMP dst);\n+  format %{ \"vector_reinterpret_expand $dst,$src\" %}\n@@ -3758,1 +3758,1 @@\n-      __ movdqu($dst$$XMMRegister, ExternalAddress(vector_32_bit_mask()), $scratch$$Register);\n+      __ movdqu($dst$$XMMRegister, ExternalAddress(vector_32_bit_mask()), noreg);\n@@ -3761,1 +3761,1 @@\n-      __ movdqu($dst$$XMMRegister, ExternalAddress(vector_64_bit_mask()), $scratch$$Register);\n+      __ movdqu($dst$$XMMRegister, ExternalAddress(vector_64_bit_mask()), noreg);\n@@ -3768,1 +3768,1 @@\n-instruct vreinterpret_expand4(legVec dst, vec src, rRegP scratch) %{\n+instruct vreinterpret_expand4(legVec dst, vec src) %{\n@@ -3775,2 +3775,1 @@\n-  effect(TEMP scratch);\n-  format %{ \"vector_reinterpret_expand $dst,$src\\t! using $scratch as TEMP\" %}\n+  format %{ \"vector_reinterpret_expand $dst,$src\" %}\n@@ -3778,1 +3777,1 @@\n-    __ vpand($dst$$XMMRegister, $src$$XMMRegister, ExternalAddress(vector_32_bit_mask()), 0, $scratch$$Register);\n+    __ vpand($dst$$XMMRegister, $src$$XMMRegister, ExternalAddress(vector_32_bit_mask()), 0, noreg);\n@@ -3846,1 +3845,1 @@\n-instruct roundD_imm(legRegD dst, immD con, immU8 rmode, rRegI scratch_reg) %{\n+instruct roundD_imm(legRegD dst, immD con, immU8 rmode) %{\n@@ -3848,1 +3847,0 @@\n-  effect(TEMP scratch_reg);\n@@ -3853,1 +3851,1 @@\n-    __ roundsd($dst$$XMMRegister, $constantaddress($con), $rmode$$constant, $scratch_reg$$Register);\n+    __ roundsd($dst$$XMMRegister, $constantaddress($con), $rmode$$constant, noreg);\n@@ -4014,1 +4012,1 @@\n-      __ movdqu($mask$$XMMRegister, ExternalAddress(vector_all_bits_set()));\n+      __ movdqu($mask$$XMMRegister, ExternalAddress(vector_all_bits_set()), noreg);\n@@ -4016,1 +4014,1 @@\n-      __ vmovdqu($mask$$XMMRegister, ExternalAddress(vector_all_bits_set()));\n+      __ vmovdqu($mask$$XMMRegister, ExternalAddress(vector_all_bits_set()), noreg);\n@@ -4037,1 +4035,1 @@\n-    __ kmovwl($ktmp$$KRegister, ExternalAddress(vector_all_bits_set()), $tmp$$Register);\n+    __ kmovwl($ktmp$$KRegister, ExternalAddress(vector_all_bits_set()), noreg);\n@@ -4078,1 +4076,1 @@\n-    __ kmovwl($ktmp$$KRegister, ExternalAddress(vector_all_bits_set()), $tmp$$Register);\n+    __ kmovwl($ktmp$$KRegister, ExternalAddress(vector_all_bits_set()), noreg);\n@@ -5616,1 +5614,1 @@\n-instruct mulB_reg(vec dst, vec src1, vec src2, vec tmp, rRegI scratch) %{\n+instruct mulB_reg(vec dst, vec src1, vec src2, vec tmp) %{\n@@ -5620,1 +5618,1 @@\n-  effect(TEMP dst, TEMP tmp, TEMP scratch);\n+  effect(TEMP dst, TEMP tmp);\n@@ -5627,1 +5625,1 @@\n-    __ movdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), $scratch$$Register);\n+    __ movdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), noreg);\n@@ -5634,1 +5632,1 @@\n-instruct mul16B_reg(vec dst, vec src1, vec src2, vec tmp1, vec tmp2, rRegI scratch) %{\n+instruct mul16B_reg(vec dst, vec src1, vec src2, vec tmp1, vec tmp2) %{\n@@ -5637,1 +5635,1 @@\n-  effect(TEMP dst, TEMP tmp1, TEMP tmp2, TEMP scratch);\n+  effect(TEMP dst, TEMP tmp1, TEMP tmp2);\n@@ -5649,1 +5647,1 @@\n-    __ movdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), $scratch$$Register);\n+    __ movdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), noreg);\n@@ -5657,1 +5655,1 @@\n-instruct vmul16B_reg_avx(vec dst, vec src1, vec src2, vec tmp, rRegI scratch) %{\n+instruct vmul16B_reg_avx(vec dst, vec src1, vec src2, vec tmp) %{\n@@ -5660,1 +5658,1 @@\n-  effect(TEMP dst, TEMP tmp, TEMP scratch);\n+  effect(TEMP dst, TEMP tmp);\n@@ -5667,1 +5665,1 @@\n-    __ vmovdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), $scratch$$Register);\n+    __ vmovdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), noreg);\n@@ -5675,1 +5673,1 @@\n-instruct vmul32B_reg_avx(vec dst, vec src1, vec src2, vec tmp1, vec tmp2, rRegI scratch) %{\n+instruct vmul32B_reg_avx(vec dst, vec src1, vec src2, vec tmp1, vec tmp2) %{\n@@ -5678,1 +5676,1 @@\n-  effect(TEMP dst, TEMP tmp1, TEMP tmp2, TEMP scratch);\n+  effect(TEMP dst, TEMP tmp1, TEMP tmp2);\n@@ -5691,1 +5689,1 @@\n-    __ vmovdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), $scratch$$Register);\n+    __ vmovdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), noreg);\n@@ -5701,1 +5699,1 @@\n-instruct vmul64B_reg_avx(vec dst, vec src1, vec src2, vec tmp1, vec tmp2, rRegI scratch) %{\n+instruct vmul64B_reg_avx(vec dst, vec src1, vec src2, vec tmp1, vec tmp2) %{\n@@ -5704,1 +5702,1 @@\n-  effect(TEMP dst, TEMP tmp1, TEMP tmp2, TEMP scratch);\n+  effect(TEMP dst, TEMP tmp1, TEMP tmp2);\n@@ -5717,1 +5715,1 @@\n-    __ vmovdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), $scratch$$Register);\n+    __ vmovdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), noreg);\n@@ -5722,1 +5720,1 @@\n-    __ evmovdquq($tmp2$$XMMRegister, ExternalAddress(vector_byte_perm_mask()), vlen_enc, $scratch$$Register);\n+    __ evmovdquq($tmp2$$XMMRegister, ExternalAddress(vector_byte_perm_mask()), vlen_enc, noreg);\n@@ -6186,1 +6184,1 @@\n-instruct signumF_reg(regF dst, regF zero, regF one, rRegP scratch, rFlagsReg cr) %{\n+instruct signumF_reg(regF dst, regF zero, regF one, rFlagsReg cr) %{\n@@ -6188,2 +6186,2 @@\n-  effect(TEMP scratch, KILL cr);\n-  format %{ \"signumF $dst, $dst\\t! using $scratch as TEMP\" %}\n+  effect(KILL cr);\n+  format %{ \"signumF $dst, $dst\" %}\n@@ -6192,1 +6190,1 @@\n-    __ signum_fp(opcode, $dst$$XMMRegister, $zero$$XMMRegister, $one$$XMMRegister, $scratch$$Register);\n+    __ signum_fp(opcode, $dst$$XMMRegister, $zero$$XMMRegister, $one$$XMMRegister);\n@@ -6197,1 +6195,1 @@\n-instruct signumD_reg(regD dst, regD zero, regD one, rRegP scratch, rFlagsReg cr) %{\n+instruct signumD_reg(regD dst, regD zero, regD one, rFlagsReg cr) %{\n@@ -6199,2 +6197,2 @@\n-  effect(TEMP scratch, KILL cr);\n-  format %{ \"signumD $dst, $dst\\t! using $scratch as TEMP\" %}\n+  effect(KILL cr);\n+  format %{ \"signumD $dst, $dst\" %}\n@@ -6203,1 +6201,1 @@\n-    __ signum_fp(opcode, $dst$$XMMRegister, $zero$$XMMRegister, $one$$XMMRegister, $scratch$$Register);\n+    __ signum_fp(opcode, $dst$$XMMRegister, $zero$$XMMRegister, $one$$XMMRegister);\n@@ -6396,1 +6394,1 @@\n-instruct vshiftB(vec dst, vec src, vec shift, vec tmp, rRegI scratch) %{\n+instruct vshiftB(vec dst, vec src, vec shift, vec tmp) %{\n@@ -6401,1 +6399,1 @@\n-  effect(TEMP dst, USE src, USE shift, TEMP tmp, TEMP scratch);\n+  effect(TEMP dst, USE src, USE shift, TEMP tmp);\n@@ -6409,1 +6407,1 @@\n-    __ movdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), $scratch$$Register);\n+    __ movdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), noreg);\n@@ -6416,1 +6414,1 @@\n-instruct vshift16B(vec dst, vec src, vec shift, vec tmp1, vec tmp2, rRegI scratch) %{\n+instruct vshift16B(vec dst, vec src, vec shift, vec tmp1, vec tmp2) %{\n@@ -6422,1 +6420,1 @@\n-  effect(TEMP dst, USE src, USE shift, TEMP tmp1, TEMP tmp2, TEMP scratch);\n+  effect(TEMP dst, USE src, USE shift, TEMP tmp1, TEMP tmp2);\n@@ -6433,1 +6431,1 @@\n-    __ movdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), $scratch$$Register);\n+    __ movdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), noreg);\n@@ -6441,1 +6439,1 @@\n-instruct vshift16B_avx(vec dst, vec src, vec shift, vec tmp, rRegI scratch) %{\n+instruct vshift16B_avx(vec dst, vec src, vec shift, vec tmp) %{\n@@ -6447,1 +6445,1 @@\n-  effect(TEMP dst, TEMP tmp, TEMP scratch);\n+  effect(TEMP dst, TEMP tmp);\n@@ -6455,1 +6453,1 @@\n-    __ vpand($tmp$$XMMRegister, $tmp$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), vlen_enc, $scratch$$Register);\n+    __ vpand($tmp$$XMMRegister, $tmp$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), vlen_enc, noreg);\n@@ -6462,1 +6460,1 @@\n-instruct vshift32B_avx(vec dst, vec src, vec shift, vec tmp, rRegI scratch) %{\n+instruct vshift32B_avx(vec dst, vec src, vec shift, vec tmp) %{\n@@ -6467,1 +6465,1 @@\n-  effect(TEMP dst, TEMP tmp, TEMP scratch);\n+  effect(TEMP dst, TEMP tmp);\n@@ -6479,2 +6477,2 @@\n-    __ vpand($tmp$$XMMRegister, $tmp$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), vlen_enc, $scratch$$Register);\n-    __ vpand($dst$$XMMRegister, $dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), vlen_enc, $scratch$$Register);\n+    __ vpand($tmp$$XMMRegister, $tmp$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), vlen_enc, noreg);\n+    __ vpand($dst$$XMMRegister, $dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), vlen_enc, noreg);\n@@ -6487,1 +6485,1 @@\n-instruct vshift64B_avx(vec dst, vec src, vec shift, vec tmp1, vec tmp2, rRegI scratch) %{\n+instruct vshift64B_avx(vec dst, vec src, vec shift, vec tmp1, vec tmp2) %{\n@@ -6492,1 +6490,1 @@\n-  effect(TEMP dst, TEMP tmp1, TEMP tmp2, TEMP scratch);\n+  effect(TEMP dst, TEMP tmp1, TEMP tmp2);\n@@ -6504,1 +6502,1 @@\n-    __ vmovdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), $scratch$$Register);\n+    __ vmovdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), noreg);\n@@ -6509,1 +6507,1 @@\n-    __ evmovdquq($tmp2$$XMMRegister, ExternalAddress(vector_byte_perm_mask()), vlen_enc, $scratch$$Register);\n+    __ evmovdquq($tmp2$$XMMRegister, ExternalAddress(vector_byte_perm_mask()), vlen_enc, noreg);\n@@ -6646,1 +6644,1 @@\n-instruct vshiftL_arith_reg(vec dst, vec src, vec shift, vec tmp, rRegI scratch) %{\n+instruct vshiftL_arith_reg(vec dst, vec src, vec shift, vec tmp) %{\n@@ -6649,1 +6647,1 @@\n-  effect(TEMP dst, TEMP tmp, TEMP scratch);\n+  effect(TEMP dst, TEMP tmp);\n@@ -6657,1 +6655,1 @@\n-      __ movdqu($tmp$$XMMRegister, ExternalAddress(vector_long_sign_mask()), $scratch$$Register);\n+      __ movdqu($tmp$$XMMRegister, ExternalAddress(vector_long_sign_mask()), noreg);\n@@ -6666,1 +6664,1 @@\n-      __ vmovdqu($tmp$$XMMRegister, ExternalAddress(vector_long_sign_mask()), $scratch$$Register);\n+      __ vmovdqu($tmp$$XMMRegister, ExternalAddress(vector_long_sign_mask()), noreg);\n@@ -6688,1 +6686,1 @@\n-instruct vshift8B_var_nobw(vec dst, vec src, vec shift, vec vtmp, rRegP scratch) %{\n+instruct vshift8B_var_nobw(vec dst, vec src, vec shift, vec vtmp) %{\n@@ -6695,2 +6693,2 @@\n-  effect(TEMP dst, TEMP vtmp, TEMP scratch);\n-  format %{ \"vector_varshift_byte $dst, $src, $shift\\n\\t! using $vtmp, $scratch as TEMP\" %}\n+  effect(TEMP dst, TEMP vtmp);\n+  format %{ \"vector_varshift_byte $dst, $src, $shift\\n\\t! using $vtmp as TEMP\" %}\n@@ -6702,1 +6700,1 @@\n-    __ varshiftbw(opcode, $dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vlen_enc, $vtmp$$XMMRegister, $scratch$$Register);\n+    __ varshiftbw(opcode, $dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vlen_enc, $vtmp$$XMMRegister);\n@@ -6708,1 +6706,1 @@\n-instruct vshift16B_var_nobw(vec dst, vec src, vec shift, vec vtmp1, vec vtmp2, rRegP scratch) %{\n+instruct vshift16B_var_nobw(vec dst, vec src, vec shift, vec vtmp1, vec vtmp2) %{\n@@ -6715,2 +6713,2 @@\n-  effect(TEMP dst, TEMP vtmp1, TEMP vtmp2, TEMP scratch);\n-  format %{ \"vector_varshift_byte $dst, $src, $shift\\n\\t! using $vtmp1, $vtmp2 and $scratch as TEMP\" %}\n+  effect(TEMP dst, TEMP vtmp1, TEMP vtmp2);\n+  format %{ \"vector_varshift_byte $dst, $src, $shift\\n\\t! using $vtmp1, $vtmp2 as TEMP\" %}\n@@ -6723,1 +6721,1 @@\n-    __ varshiftbw(opcode, $dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vlen_enc, $vtmp1$$XMMRegister, $scratch$$Register);\n+    __ varshiftbw(opcode, $dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vlen_enc, $vtmp1$$XMMRegister);\n@@ -6728,1 +6726,1 @@\n-    __ varshiftbw(opcode, $vtmp1$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister, vlen_enc, $vtmp2$$XMMRegister, $scratch$$Register);\n+    __ varshiftbw(opcode, $vtmp1$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister, vlen_enc, $vtmp2$$XMMRegister);\n@@ -6736,1 +6734,1 @@\n-instruct vshift32B_var_nobw(vec dst, vec src, vec shift, vec vtmp1, vec vtmp2, vec vtmp3, vec vtmp4, rRegP scratch) %{\n+instruct vshift32B_var_nobw(vec dst, vec src, vec shift, vec vtmp1, vec vtmp2, vec vtmp3, vec vtmp4) %{\n@@ -6743,2 +6741,2 @@\n-  effect(TEMP dst, TEMP vtmp1, TEMP vtmp2, TEMP vtmp3, TEMP vtmp4, TEMP scratch);\n-  format %{ \"vector_varshift_byte $dst, $src, $shift\\n\\t using $vtmp1, $vtmp2, $vtmp3, $vtmp4 and $scratch as TEMP\" %}\n+  effect(TEMP dst, TEMP vtmp1, TEMP vtmp2, TEMP vtmp3, TEMP vtmp4);\n+  format %{ \"vector_varshift_byte $dst, $src, $shift\\n\\t using $vtmp1, $vtmp2, $vtmp3, $vtmp4 as TEMP\" %}\n@@ -6751,1 +6749,1 @@\n-    __ varshiftbw(opcode, $dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vlen_enc, $vtmp1$$XMMRegister, $scratch$$Register);\n+    __ varshiftbw(opcode, $dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vlen_enc, $vtmp1$$XMMRegister);\n@@ -6754,1 +6752,1 @@\n-    __ varshiftbw(opcode, $vtmp1$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister, vlen_enc, $vtmp2$$XMMRegister, $scratch$$Register);\n+    __ varshiftbw(opcode, $vtmp1$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister, vlen_enc, $vtmp2$$XMMRegister);\n@@ -6760,1 +6758,1 @@\n-    __ varshiftbw(opcode, $vtmp3$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister, vlen_enc, $vtmp4$$XMMRegister, $scratch$$Register);\n+    __ varshiftbw(opcode, $vtmp3$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister, vlen_enc, $vtmp4$$XMMRegister);\n@@ -6763,1 +6761,1 @@\n-    __ varshiftbw(opcode, $vtmp1$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister, vlen_enc, $vtmp2$$XMMRegister, $scratch$$Register);\n+    __ varshiftbw(opcode, $vtmp1$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister, vlen_enc, $vtmp2$$XMMRegister);\n@@ -6772,1 +6770,1 @@\n-instruct vshiftB_var_evex_bw(vec dst, vec src, vec shift, vec vtmp, rRegP scratch) %{\n+instruct vshiftB_var_evex_bw(vec dst, vec src, vec shift, vec vtmp) %{\n@@ -6779,2 +6777,2 @@\n-  effect(TEMP dst, TEMP vtmp, TEMP scratch);\n-  format %{ \"vector_varshift_byte $dst, $src, $shift\\n\\t! using $vtmp, $scratch as TEMP\" %}\n+  effect(TEMP dst, TEMP vtmp);\n+  format %{ \"vector_varshift_byte $dst, $src, $shift\\n\\t! using $vtmp as TEMP\" %}\n@@ -6786,1 +6784,1 @@\n-    __ evarshiftb(opcode, $dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vlen_enc, $vtmp$$XMMRegister, $scratch$$Register);\n+    __ evarshiftb(opcode, $dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vlen_enc, $vtmp$$XMMRegister);\n@@ -6791,1 +6789,1 @@\n-instruct vshift64B_var_evex_bw(vec dst, vec src, vec shift, vec vtmp1, vec vtmp2, rRegP scratch) %{\n+instruct vshift64B_var_evex_bw(vec dst, vec src, vec shift, vec vtmp1, vec vtmp2) %{\n@@ -6798,2 +6796,2 @@\n-  effect(TEMP dst, TEMP vtmp1, TEMP vtmp2, TEMP scratch);\n-  format %{ \"vector_varshift_byte $dst, $src, $shift\\n\\t! using $vtmp1, $vtmp2 and $scratch as TEMP\" %}\n+  effect(TEMP dst, TEMP vtmp1, TEMP vtmp2);\n+  format %{ \"vector_varshift_byte $dst, $src, $shift\\n\\t! using $vtmp1, $vtmp2 as TEMP\" %}\n@@ -6805,1 +6803,1 @@\n-    __ evarshiftb(opcode, $dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vlen_enc, $vtmp1$$XMMRegister, $scratch$$Register);\n+    __ evarshiftb(opcode, $dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vlen_enc, $vtmp1$$XMMRegister);\n@@ -6808,1 +6806,1 @@\n-    __ evarshiftb(opcode, $vtmp1$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister, vlen_enc, $vtmp2$$XMMRegister, $scratch$$Register);\n+    __ evarshiftb(opcode, $vtmp1$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister, vlen_enc, $vtmp2$$XMMRegister);\n@@ -6815,1 +6813,1 @@\n-instruct vshift8S_var_nobw(vec dst, vec src, vec shift, vec vtmp, rRegP scratch) %{\n+instruct vshift8S_var_nobw(vec dst, vec src, vec shift, vec vtmp) %{\n@@ -6822,1 +6820,1 @@\n-  effect(TEMP dst, TEMP vtmp, TEMP scratch);\n+  effect(TEMP dst, TEMP vtmp);\n@@ -6833,1 +6831,1 @@\n-    __ vpand($dst$$XMMRegister, $dst$$XMMRegister, ExternalAddress(vector_int_to_short_mask()), vlen_enc, $scratch$$Register);\n+    __ vpand($dst$$XMMRegister, $dst$$XMMRegister, ExternalAddress(vector_int_to_short_mask()), vlen_enc, noreg);\n@@ -6840,1 +6838,1 @@\n-instruct vshift16S_var_nobw(vec dst, vec src, vec shift, vec vtmp1, vec vtmp2, rRegP scratch) %{\n+instruct vshift16S_var_nobw(vec dst, vec src, vec shift, vec vtmp1, vec vtmp2) %{\n@@ -6847,1 +6845,1 @@\n-  effect(TEMP dst, TEMP vtmp1, TEMP vtmp2, TEMP scratch);\n+  effect(TEMP dst, TEMP vtmp1, TEMP vtmp2);\n@@ -6859,1 +6857,1 @@\n-    __ vpand($vtmp2$$XMMRegister, $vtmp2$$XMMRegister, ExternalAddress(vector_int_to_short_mask()), vlen_enc, $scratch$$Register);\n+    __ vpand($vtmp2$$XMMRegister, $vtmp2$$XMMRegister, ExternalAddress(vector_int_to_short_mask()), vlen_enc, noreg);\n@@ -6867,1 +6865,1 @@\n-    __ vpand($dst$$XMMRegister, $dst$$XMMRegister, ExternalAddress(vector_int_to_short_mask()), vlen_enc, $scratch$$Register);\n+    __ vpand($dst$$XMMRegister, $dst$$XMMRegister, ExternalAddress(vector_int_to_short_mask()), vlen_enc, noreg);\n@@ -7079,1 +7077,1 @@\n-instruct castStoX(vec dst, vec src, rRegP scratch) %{\n+instruct castStoX(vec dst, vec src) %{\n@@ -7083,1 +7081,0 @@\n-  effect(TEMP scratch);\n@@ -7085,1 +7082,1 @@\n-  format %{ \"vector_cast_s2x $dst,$src\\t! using $scratch as TEMP\" %}\n+  format %{ \"vector_cast_s2x $dst,$src\" %}\n@@ -7089,1 +7086,1 @@\n-    __ vpand($dst$$XMMRegister, $src$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), 0, $scratch$$Register);\n+    __ vpand($dst$$XMMRegister, $src$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), 0, noreg);\n@@ -7095,1 +7092,1 @@\n-instruct vcastStoX(vec dst, vec src, vec vtmp, rRegP scratch) %{\n+instruct vcastStoX(vec dst, vec src, vec vtmp) %{\n@@ -7099,1 +7096,1 @@\n-  effect(TEMP dst, TEMP vtmp, TEMP scratch);\n+  effect(TEMP dst, TEMP vtmp);\n@@ -7101,1 +7098,1 @@\n-  format %{ \"vector_cast_s2x $dst,$src\\t! using $vtmp, $scratch as TEMP\" %}\n+  format %{ \"vector_cast_s2x $dst,$src\\t! using $vtmp as TEMP\" %}\n@@ -7106,1 +7103,1 @@\n-    __ vpand($dst$$XMMRegister, $src$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), vlen_enc, $scratch$$Register);\n+    __ vpand($dst$$XMMRegister, $src$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), vlen_enc, noreg);\n@@ -7152,1 +7149,1 @@\n-instruct castItoX(vec dst, vec src, rRegP scratch) %{\n+instruct castItoX(vec dst, vec src) %{\n@@ -7157,2 +7154,1 @@\n-  format %{ \"vector_cast_i2x $dst,$src\\t! using $scratch as TEMP\" %}\n-  effect(TEMP scratch);\n+  format %{ \"vector_cast_i2x $dst,$src\" %}\n@@ -7166,1 +7162,1 @@\n-      __ vpand($dst$$XMMRegister, $src$$XMMRegister, ExternalAddress(vector_int_to_byte_mask()), vlen_enc, $scratch$$Register);\n+      __ vpand($dst$$XMMRegister, $src$$XMMRegister, ExternalAddress(vector_int_to_byte_mask()), vlen_enc, noreg);\n@@ -7171,1 +7167,1 @@\n-      __ vpand($dst$$XMMRegister, $src$$XMMRegister, ExternalAddress(vector_int_to_short_mask()), vlen_enc, $scratch$$Register);\n+      __ vpand($dst$$XMMRegister, $src$$XMMRegister, ExternalAddress(vector_int_to_short_mask()), vlen_enc, noreg);\n@@ -7178,1 +7174,1 @@\n-instruct vcastItoX(vec dst, vec src, vec vtmp, rRegP scratch) %{\n+instruct vcastItoX(vec dst, vec src, vec vtmp) %{\n@@ -7183,2 +7179,2 @@\n-  format %{ \"vector_cast_i2x $dst,$src\\t! using $vtmp and $scratch as TEMP\" %}\n-  effect(TEMP dst, TEMP vtmp, TEMP scratch);\n+  format %{ \"vector_cast_i2x $dst,$src\\t! using $vtmp as TEMP\" %}\n+  effect(TEMP dst, TEMP vtmp);\n@@ -7192,1 +7188,1 @@\n-      __ vpand($vtmp$$XMMRegister, $src$$XMMRegister, ExternalAddress(vector_int_to_byte_mask()), vlen_enc, $scratch$$Register);\n+      __ vpand($vtmp$$XMMRegister, $src$$XMMRegister, ExternalAddress(vector_int_to_byte_mask()), vlen_enc, noreg);\n@@ -7198,1 +7194,1 @@\n-      __ vpand($vtmp$$XMMRegister, $src$$XMMRegister, ExternalAddress(vector_int_to_short_mask()), vlen_enc, $scratch$$Register);\n+      __ vpand($vtmp$$XMMRegister, $src$$XMMRegister, ExternalAddress(vector_int_to_short_mask()), vlen_enc, noreg);\n@@ -7246,1 +7242,1 @@\n-instruct vcastLtoBS(vec dst, vec src, rRegP scratch) %{\n+instruct vcastLtoBS(vec dst, vec src) %{\n@@ -7250,2 +7246,1 @@\n-  effect(TEMP scratch);\n-  format %{ \"vector_cast_l2x  $dst,$src\\t! using $scratch as TEMP\" %}\n+  format %{ \"vector_cast_l2x  $dst,$src\" %}\n@@ -7261,1 +7256,1 @@\n-      __ vpand($dst$$XMMRegister, $dst$$XMMRegister, mask_addr, Assembler::AVX_128bit, $scratch$$Register);\n+      __ vpand($dst$$XMMRegister, $dst$$XMMRegister, mask_addr, Assembler::AVX_128bit, noreg);\n@@ -7267,1 +7262,1 @@\n-      __ vpand($dst$$XMMRegister, $dst$$XMMRegister, mask_addr, Assembler::AVX_128bit, $scratch$$Register);\n+      __ vpand($dst$$XMMRegister, $dst$$XMMRegister, mask_addr, Assembler::AVX_128bit, noreg);\n@@ -7349,1 +7344,1 @@\n-instruct castFtoI_reg_avx(vec dst, vec src, vec xtmp1, vec xtmp2, vec xtmp3, vec xtmp4, rRegP scratch, rFlagsReg cr) %{\n+instruct castFtoI_reg_avx(vec dst, vec src, vec xtmp1, vec xtmp2, vec xtmp3, vec xtmp4, rFlagsReg cr) %{\n@@ -7356,2 +7351,2 @@\n-  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, TEMP xtmp4, TEMP scratch, KILL cr);\n-  format %{ \"vector_cast_f2i $dst,$src\\t! using $xtmp1, $xtmp2, $xtmp3, $xtmp4 and $scratch as TEMP\" %}\n+  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, TEMP xtmp4, KILL cr);\n+  format %{ \"vector_cast_f2i $dst,$src\\t! using $xtmp1, $xtmp2, $xtmp3, $xtmp4 as TEMP\" %}\n@@ -7362,1 +7357,1 @@\n-                          ExternalAddress(vector_float_signflip()), $scratch$$Register, vlen_enc);\n+                          ExternalAddress(vector_float_signflip()), noreg, vlen_enc);\n@@ -7367,1 +7362,1 @@\n-instruct castFtoI_reg_evex(vec dst, vec src, vec xtmp1, vec xtmp2, kReg ktmp1, kReg ktmp2, rRegP scratch, rFlagsReg cr) %{\n+instruct castFtoI_reg_evex(vec dst, vec src, vec xtmp1, vec xtmp2, kReg ktmp1, kReg ktmp2, rFlagsReg cr) %{\n@@ -7372,2 +7367,2 @@\n-  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP ktmp1, TEMP ktmp2, TEMP scratch, KILL cr);\n-  format %{ \"vector_cast_f2i $dst,$src\\t! using $xtmp1, $xtmp2, $ktmp1, $ktmp2 and $scratch as TEMP\" %}\n+  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP ktmp1, TEMP ktmp2, KILL cr);\n+  format %{ \"vector_cast_f2i $dst,$src\\t! using $xtmp1, $xtmp2, $ktmp1, $ktmp2 as TEMP\" %}\n@@ -7378,1 +7373,1 @@\n-                           ExternalAddress(vector_float_signflip()), $scratch$$Register, vlen_enc);\n+                           ExternalAddress(vector_float_signflip()), noreg, vlen_enc);\n@@ -7383,1 +7378,1 @@\n-instruct castFtoX_reg_evex(vec dst, vec src, vec xtmp1, vec xtmp2, kReg ktmp1, kReg ktmp2, rRegP scratch, rFlagsReg cr) %{\n+instruct castFtoX_reg_evex(vec dst, vec src, vec xtmp1, vec xtmp2, kReg ktmp1, kReg ktmp2, rFlagsReg cr) %{\n@@ -7389,2 +7384,2 @@\n-  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP ktmp1, TEMP ktmp2, TEMP scratch, KILL cr);\n-  format %{ \"vector_cast_f2x $dst,$src\\t! using $xtmp1, $xtmp2, $ktmp1, $ktmp2 and $scratch as TEMP\" %}\n+  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP ktmp1, TEMP ktmp2, KILL cr);\n+  format %{ \"vector_cast_f2x $dst,$src\\t! using $xtmp1, $xtmp2, $ktmp1, $ktmp2 as TEMP\" %}\n@@ -7397,1 +7392,1 @@\n-                             ExternalAddress(vector_double_signflip()), $scratch$$Register, vlen_enc);\n+                             ExternalAddress(vector_double_signflip()), noreg, vlen_enc);\n@@ -7402,1 +7397,1 @@\n-                             ExternalAddress(vector_float_signflip()), $scratch$$Register, vlen_enc);\n+                             ExternalAddress(vector_float_signflip()), noreg, vlen_enc);\n@@ -7425,1 +7420,1 @@\n-instruct castDtoX_reg_evex(vec dst, vec src, vec xtmp1, vec xtmp2, kReg ktmp1, kReg ktmp2, rRegP scratch, rFlagsReg cr) %{\n+instruct castDtoX_reg_evex(vec dst, vec src, vec xtmp1, vec xtmp2, kReg ktmp1, kReg ktmp2, rFlagsReg cr) %{\n@@ -7428,2 +7423,2 @@\n-  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP ktmp1, TEMP ktmp2, TEMP scratch, KILL cr);\n-  format %{ \"vector_cast_d2x $dst,$src\\t! using $xtmp1, $xtmp2, $ktmp1, $ktmp2 and $scratch as TEMP\" %}\n+  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP ktmp1, TEMP ktmp2, KILL cr);\n+  format %{ \"vector_cast_d2x $dst,$src\\t! using $xtmp1, $xtmp2, $ktmp1, $ktmp2 as TEMP\" %}\n@@ -7435,1 +7430,1 @@\n-                           ExternalAddress(vector_double_signflip()), $scratch$$Register, vlen_enc);\n+                           ExternalAddress(vector_double_signflip()), noreg, vlen_enc);\n@@ -7527,1 +7522,1 @@\n-instruct evcmpFD64(vec dst, vec src1, vec src2, immI8 cond, rRegP scratch, kReg ktmp) %{\n+instruct evcmpFD64(vec dst, vec src1, vec src2, immI8 cond, kReg ktmp) %{\n@@ -7532,2 +7527,2 @@\n-  effect(TEMP scratch, TEMP ktmp);\n-  format %{ \"vector_compare $dst,$src1,$src2,$cond\\t! using $scratch as TEMP\" %}\n+  effect(TEMP ktmp);\n+  format %{ \"vector_compare $dst,$src1,$src2,$cond\" %}\n@@ -7540,1 +7535,1 @@\n-      __ evmovdqul($dst$$XMMRegister, $ktmp$$KRegister, ExternalAddress(vector_all_bits_set()), false, vlen_enc, $scratch$$Register);\n+      __ evmovdqul($dst$$XMMRegister, $ktmp$$KRegister, ExternalAddress(vector_all_bits_set()), false, vlen_enc, noreg);\n@@ -7543,1 +7538,1 @@\n-      __ evmovdquq($dst$$XMMRegister, $ktmp$$KRegister, ExternalAddress(vector_all_bits_set()), false, vlen_enc, $scratch$$Register);\n+      __ evmovdquq($dst$$XMMRegister, $ktmp$$KRegister, ExternalAddress(vector_all_bits_set()), false, vlen_enc, noreg);\n@@ -7636,1 +7631,1 @@\n-instruct vcmp64(vec dst, vec src1, vec src2, immI8 cond, rRegP scratch, kReg ktmp) %{\n+instruct vcmp64(vec dst, vec src1, vec src2, immI8 cond, kReg ktmp) %{\n@@ -7641,2 +7636,2 @@\n-  effect(TEMP scratch, TEMP ktmp);\n-  format %{ \"vector_compare $dst,$src1,$src2,$cond\\t! using $scratch as TEMP\" %}\n+  effect(TEMP ktmp);\n+  format %{ \"vector_compare $dst,$src1,$src2,$cond\" %}\n@@ -7656,1 +7651,1 @@\n-        __ evmovdqul($dst$$XMMRegister, $ktmp$$KRegister, ExternalAddress(vector_all_bits_set()), merge, vlen_enc, $scratch$$Register);\n+        __ evmovdqul($dst$$XMMRegister, $ktmp$$KRegister, ExternalAddress(vector_all_bits_set()), merge, vlen_enc, noreg);\n@@ -7661,1 +7656,1 @@\n-        __ evmovdquq($dst$$XMMRegister, $ktmp$$KRegister, ExternalAddress(vector_all_bits_set()), merge, vlen_enc, $scratch$$Register);\n+        __ evmovdquq($dst$$XMMRegister, $ktmp$$KRegister, ExternalAddress(vector_all_bits_set()), merge, vlen_enc, noreg);\n@@ -7778,1 +7773,1 @@\n-instruct extractF(legRegF dst, legVec src, immU8 idx, rRegI tmp, legVec vtmp) %{\n+instruct extractF(legRegF dst, legVec src, immU8 idx, legVec vtmp) %{\n@@ -7781,2 +7776,2 @@\n-  effect(TEMP dst, TEMP tmp, TEMP vtmp);\n-  format %{ \"extractF $dst,$src,$idx\\t! using $tmp, $vtmp as TEMP\" %}\n+  effect(TEMP dst, TEMP vtmp);\n+  format %{ \"extractF $dst,$src,$idx\\t! using $vtmp as TEMP\" %}\n@@ -7786,1 +7781,1 @@\n-    __ get_elem(T_FLOAT, $dst$$XMMRegister, $src$$XMMRegister, $idx$$constant, $tmp$$Register, $vtmp$$XMMRegister);\n+    __ get_elem(T_FLOAT, $dst$$XMMRegister, $src$$XMMRegister, $idx$$constant, $vtmp$$XMMRegister);\n@@ -7791,1 +7786,1 @@\n-instruct vextractF(legRegF dst, legVec src, immU8 idx, rRegI tmp, legVec vtmp) %{\n+instruct vextractF(legRegF dst, legVec src, immU8 idx, legVec vtmp) %{\n@@ -7795,2 +7790,2 @@\n-  effect(TEMP tmp, TEMP vtmp);\n-  format %{ \"vextractF $dst,$src,$idx\\t! using $tmp, $vtmp as TEMP\" %}\n+  effect(TEMP vtmp);\n+  format %{ \"vextractF $dst,$src,$idx\\t! using $vtmp as TEMP\" %}\n@@ -7801,1 +7796,1 @@\n-    __ get_elem(T_FLOAT, $dst$$XMMRegister, lane_reg, $idx$$constant, $tmp$$Register);\n+    __ get_elem(T_FLOAT, $dst$$XMMRegister, lane_reg, $idx$$constant);\n@@ -7879,1 +7874,1 @@\n-instruct evblendvp64(vec dst, vec src1, vec src2, vec mask, rRegP scratch, kReg ktmp) %{\n+instruct evblendvp64(vec dst, vec src1, vec src2, vec mask, kReg ktmp) %{\n@@ -7883,2 +7878,2 @@\n-  format %{ \"vector_blend  $dst,$src1,$src2,$mask\\t! using $scratch and k2 as TEMP\" %}\n-  effect(TEMP scratch, TEMP ktmp);\n+  format %{ \"vector_blend  $dst,$src1,$src2,$mask\\t! using k2 as TEMP\" %}\n+  effect(TEMP ktmp);\n@@ -7888,1 +7883,1 @@\n-    __ evpcmp(elem_bt, $ktmp$$KRegister, k0, $mask$$XMMRegister, ExternalAddress(vector_all_bits_set()), Assembler::eq, vlen_enc, $scratch$$Register);\n+    __ evpcmp(elem_bt, $ktmp$$KRegister, k0, $mask$$XMMRegister, ExternalAddress(vector_all_bits_set()), Assembler::eq, vlen_enc, noreg);\n@@ -7895,1 +7890,1 @@\n-instruct evblendvp64_masked(vec dst, vec src1, vec src2, kReg mask, rRegP scratch) %{\n+instruct evblendvp64_masked(vec dst, vec src1, vec src2, kReg mask) %{\n@@ -7900,2 +7895,1 @@\n-  format %{ \"vector_blend  $dst,$src1,$src2,$mask\\t! using $scratch and k2 as TEMP\" %}\n-  effect(TEMP scratch);\n+  format %{ \"vector_blend  $dst,$src1,$src2,$mask\\t! using k2 as TEMP\" %}\n@@ -7977,1 +7971,1 @@\n-instruct vabsnegF(vec dst, vec src, rRegI scratch) %{\n+instruct vabsnegF(vec dst, vec src) %{\n@@ -7981,1 +7975,0 @@\n-  effect(TEMP scratch);\n@@ -7988,1 +7981,1 @@\n-      __ vabsnegf(opcode, $dst$$XMMRegister, $src$$XMMRegister, $scratch$$Register);\n+      __ vabsnegf(opcode, $dst$$XMMRegister, $src$$XMMRegister);\n@@ -7992,1 +7985,1 @@\n-      __ vabsnegf(opcode, $dst$$XMMRegister, $src$$XMMRegister, vlen_enc, $scratch$$Register);\n+      __ vabsnegf(opcode, $dst$$XMMRegister, $src$$XMMRegister, vlen_enc);\n@@ -7998,1 +7991,1 @@\n-instruct vabsneg4F(vec dst, rRegI scratch) %{\n+instruct vabsneg4F(vec dst) %{\n@@ -8002,1 +7995,0 @@\n-  effect(TEMP scratch);\n@@ -8007,1 +7999,1 @@\n-    __ vabsnegf(opcode, $dst$$XMMRegister, $dst$$XMMRegister, $scratch$$Register);\n+    __ vabsnegf(opcode, $dst$$XMMRegister, $dst$$XMMRegister);\n@@ -8012,1 +8004,1 @@\n-instruct vabsnegD(vec dst, vec src, rRegI scratch) %{\n+instruct vabsnegD(vec dst, vec src) %{\n@@ -8015,1 +8007,0 @@\n-  effect(TEMP scratch);\n@@ -8022,1 +8013,1 @@\n-      __ vabsnegd(opcode, $dst$$XMMRegister, $src$$XMMRegister, $scratch$$Register);\n+      __ vabsnegd(opcode, $dst$$XMMRegister, $src$$XMMRegister);\n@@ -8025,1 +8016,1 @@\n-      __ vabsnegd(opcode, $dst$$XMMRegister, $src$$XMMRegister, vlen_enc, $scratch$$Register);\n+      __ vabsnegd(opcode, $dst$$XMMRegister, $src$$XMMRegister, vlen_enc);\n@@ -8217,1 +8208,1 @@\n-instruct loadMask64(kReg dst, vec src, vec xtmp, rRegI tmp) %{\n+instruct loadMask64(kReg dst, vec src, vec xtmp) %{\n@@ -8220,2 +8211,2 @@\n-  effect(TEMP xtmp, TEMP tmp);\n-  format %{ \"vector_loadmask_64byte $dst, $src\\t! using $xtmp and $tmp as TEMP\" %}\n+  effect(TEMP xtmp);\n+  format %{ \"vector_loadmask_64byte $dst, $src\\t! using $xtmp as TEMP\" %}\n@@ -8224,1 +8215,1 @@\n-                        $tmp$$Register, true, Assembler::AVX_512bit);\n+                        true, Assembler::AVX_512bit);\n@@ -8237,1 +8228,1 @@\n-                        noreg, false, vlen_enc);\n+                        false, vlen_enc);\n@@ -8377,1 +8368,1 @@\n-instruct vstoreMask_evex_vectmask(vec dst, kReg mask, immI size, rRegI tmp) %{\n+instruct vstoreMask_evex_vectmask(vec dst, kReg mask, immI size) %{\n@@ -8380,1 +8371,1 @@\n-  effect(TEMP_DEF dst, TEMP tmp);\n+  effect(TEMP_DEF dst);\n@@ -8385,1 +8376,1 @@\n-                 false, Assembler::AVX_512bit, $tmp$$Register);\n+                 false, Assembler::AVX_512bit, noreg);\n@@ -8429,1 +8420,1 @@\n-instruct loadIotaIndices(vec dst, immI_0 src, rRegP scratch) %{\n+instruct loadIotaIndices(vec dst, immI_0 src) %{\n@@ -8432,1 +8423,0 @@\n-  effect(TEMP scratch);\n@@ -8436,1 +8426,1 @@\n-     __ load_iota_indices($dst$$XMMRegister, $scratch$$Register, vlen_in_bytes);\n+     __ load_iota_indices($dst$$XMMRegister, vlen_in_bytes);\n@@ -8442,1 +8432,1 @@\n-instruct VectorPopulateIndex(vec dst, rRegI src1, immI_1 src2, vec vtmp, rRegP scratch) %{\n+instruct VectorPopulateIndex(vec dst, rRegI src1, immI_1 src2, vec vtmp) %{\n@@ -8444,2 +8434,2 @@\n-  effect(TEMP dst, TEMP vtmp, TEMP scratch);\n-  format %{ \"vector_populate_index $dst $src1 $src2\\t! using $vtmp and $scratch as TEMP\" %}\n+  effect(TEMP dst, TEMP vtmp);\n+  format %{ \"vector_populate_index $dst $src1 $src2\\t! using $vtmp as TEMP\" %}\n@@ -8452,1 +8442,1 @@\n-     __ load_iota_indices($dst$$XMMRegister, $scratch$$Register, vlen);\n+     __ load_iota_indices($dst$$XMMRegister, vlen);\n@@ -8461,1 +8451,1 @@\n-instruct VectorPopulateLIndex(vec dst, rRegL src1, immI_1 src2, vec vtmp, rRegP scratch) %{\n+instruct VectorPopulateLIndex(vec dst, rRegL src1, immI_1 src2, vec vtmp) %{\n@@ -8463,2 +8453,2 @@\n-  effect(TEMP dst, TEMP vtmp, TEMP scratch);\n-  format %{ \"vector_populate_index $dst $src1 $src2\\t! using $vtmp and $scratch as TEMP\" %}\n+  effect(TEMP dst, TEMP vtmp);\n+  format %{ \"vector_populate_index $dst $src1 $src2\\t! using $vtmp as TEMP\" %}\n@@ -8471,1 +8461,1 @@\n-     __ load_iota_indices($dst$$XMMRegister, $scratch$$Register, vlen);\n+     __ load_iota_indices($dst$$XMMRegister, vlen);\n@@ -8506,1 +8496,1 @@\n-instruct rearrangeB_avx(legVec dst, legVec src, vec shuffle, legVec vtmp1, legVec vtmp2, rRegP scratch) %{\n+instruct rearrangeB_avx(legVec dst, legVec src, vec shuffle, legVec vtmp1, legVec vtmp2) %{\n@@ -8510,2 +8500,2 @@\n-  effect(TEMP dst, TEMP vtmp1, TEMP vtmp2, TEMP scratch);\n-  format %{ \"vector_rearrange $dst, $shuffle, $src\\t! using $vtmp1, $vtmp2, $scratch as TEMP\" %}\n+  effect(TEMP dst, TEMP vtmp1, TEMP vtmp2);\n+  format %{ \"vector_rearrange $dst, $shuffle, $src\\t! using $vtmp1, $vtmp2 as TEMP\" %}\n@@ -8521,1 +8511,1 @@\n-    __ vpaddb($vtmp2$$XMMRegister, $shuffle$$XMMRegister, ExternalAddress(vector_byte_shufflemask()), Assembler::AVX_256bit, $scratch$$Register);\n+    __ vpaddb($vtmp2$$XMMRegister, $shuffle$$XMMRegister, ExternalAddress(vector_byte_shufflemask()), Assembler::AVX_256bit, noreg);\n@@ -8558,1 +8548,1 @@\n-instruct loadShuffleS(vec dst, vec src, vec vtmp, rRegP scratch) %{\n+instruct loadShuffleS(vec dst, vec src, vec vtmp) %{\n@@ -8562,2 +8552,2 @@\n-  effect(TEMP dst, TEMP vtmp, TEMP scratch);\n-  format %{ \"vector_load_shuffle $dst, $src\\t! using $vtmp and $scratch as TEMP\" %}\n+  effect(TEMP dst, TEMP vtmp);\n+  format %{ \"vector_load_shuffle $dst, $src\\t! using $vtmp as TEMP\" %}\n@@ -8580,1 +8570,1 @@\n-      __ movdqu($vtmp$$XMMRegister, ExternalAddress(vector_short_shufflemask()), $scratch$$Register);\n+      __ movdqu($vtmp$$XMMRegister, ExternalAddress(vector_short_shufflemask()), noreg);\n@@ -8594,1 +8584,1 @@\n-      __ vpaddb($dst$$XMMRegister, $dst$$XMMRegister, ExternalAddress(vector_short_shufflemask()), vlen_enc, $scratch$$Register);\n+      __ vpaddb($dst$$XMMRegister, $dst$$XMMRegister, ExternalAddress(vector_short_shufflemask()), vlen_enc, noreg);\n@@ -8612,1 +8602,1 @@\n-instruct rearrangeS_avx(legVec dst, legVec src, vec shuffle, legVec vtmp1, legVec vtmp2, rRegP scratch) %{\n+instruct rearrangeS_avx(legVec dst, legVec src, vec shuffle, legVec vtmp1, legVec vtmp2) %{\n@@ -8616,2 +8606,2 @@\n-  effect(TEMP dst, TEMP vtmp1, TEMP vtmp2, TEMP scratch);\n-  format %{ \"vector_rearrange $dst, $shuffle, $src\\t! using $vtmp1, $vtmp2, $scratch as TEMP\" %}\n+  effect(TEMP dst, TEMP vtmp1, TEMP vtmp2);\n+  format %{ \"vector_rearrange $dst, $shuffle, $src\\t! using $vtmp1, $vtmp2 as TEMP\" %}\n@@ -8627,1 +8617,1 @@\n-    __ vpaddb($vtmp2$$XMMRegister, $shuffle$$XMMRegister, ExternalAddress(vector_byte_shufflemask()), Assembler::AVX_256bit, $scratch$$Register);\n+    __ vpaddb($vtmp2$$XMMRegister, $shuffle$$XMMRegister, ExternalAddress(vector_byte_shufflemask()), Assembler::AVX_256bit, noreg);\n@@ -8666,1 +8656,1 @@\n-instruct loadShuffleI(vec dst, vec src, vec vtmp, rRegP scratch) %{\n+instruct loadShuffleI(vec dst, vec src, vec vtmp) %{\n@@ -8670,2 +8660,2 @@\n-  effect(TEMP dst, TEMP vtmp, TEMP scratch);\n-  format %{ \"vector_load_shuffle $dst, $src\\t! using $vtmp and $scratch as TEMP\" %}\n+  effect(TEMP dst, TEMP vtmp);\n+  format %{ \"vector_load_shuffle $dst, $src\\t! using $vtmp as TEMP\" %}\n@@ -8690,1 +8680,1 @@\n-    __ movdqu($dst$$XMMRegister, ExternalAddress(vector_int_shufflemask()), $scratch$$Register);\n+    __ movdqu($dst$$XMMRegister, ExternalAddress(vector_int_shufflemask()), noreg);\n@@ -8737,1 +8727,1 @@\n-instruct loadShuffleL(vec dst, vec src, vec vtmp, rRegP scratch) %{\n+instruct loadShuffleL(vec dst, vec src, vec vtmp) %{\n@@ -8741,2 +8731,2 @@\n-  effect(TEMP dst, TEMP vtmp, TEMP scratch);\n-  format %{ \"vector_load_shuffle $dst, $src\\t! using $vtmp and $scratch as TEMP\" %}\n+  effect(TEMP dst, TEMP vtmp);\n+  format %{ \"vector_load_shuffle $dst, $src\\t! using $vtmp as TEMP\" %}\n@@ -8759,1 +8749,1 @@\n-    __ vpaddd($dst$$XMMRegister, $dst$$XMMRegister, ExternalAddress(vector_long_shufflemask()), vlen_enc, $scratch$$Register);\n+    __ vpaddd($dst$$XMMRegister, $dst$$XMMRegister, ExternalAddress(vector_long_shufflemask()), vlen_enc, noreg);\n@@ -9394,1 +9384,1 @@\n-instruct vreverse_reg_gfni(vec dst, vec src, vec xtmp, rRegI rtmp) %{\n+instruct vreverse_reg_gfni(vec dst, vec src, vec xtmp) %{\n@@ -9397,2 +9387,2 @@\n-  effect(TEMP dst, TEMP xtmp, TEMP rtmp);\n-  format %{ \"vector_reverse_bit_gfni $dst, $src!\\t using $rtmp and $xtmp as TEMP\" %}\n+  effect(TEMP dst, TEMP xtmp);\n+  format %{ \"vector_reverse_bit_gfni $dst, $src!\\t using $xtmp as TEMP\" %}\n@@ -9404,1 +9394,1 @@\n-                               addr, $rtmp$$Register, vec_enc);\n+                               addr, noreg, vec_enc);\n@@ -9409,1 +9399,1 @@\n-instruct vreverse_byte_reg(vec dst, vec src, rRegI rtmp) %{\n+instruct vreverse_byte_reg(vec dst, vec src) %{\n@@ -9412,2 +9402,2 @@\n-  effect(TEMP dst, TEMP rtmp);\n-  format %{ \"vector_reverse_byte $dst, $src!\\t using $rtmp as TEMP\" %}\n+  effect(TEMP dst);\n+  format %{ \"vector_reverse_byte $dst, $src\" %}\n@@ -9417,1 +9407,1 @@\n-    __ vector_reverse_byte(bt, $dst$$XMMRegister, $src$$XMMRegister, $rtmp$$Register, vec_enc);\n+    __ vector_reverse_byte(bt, $dst$$XMMRegister, $src$$XMMRegister, vec_enc);\n@@ -10096,1 +10086,1 @@\n-instruct evcmp_masked(kReg dst, vec src1, vec src2, immI8 cond, kReg mask, rRegP scratch) %{\n+instruct evcmp_masked(kReg dst, vec src1, vec src2, immI8 cond, kReg mask) %{\n@@ -10098,2 +10088,1 @@\n-  effect(TEMP scratch);\n-  format %{ \"vcmp_masked $dst, $src1, $src2, $cond, $mask\\t! using $scratch as TEMP\" %}\n+  format %{ \"vcmp_masked $dst, $src1, $src2, $cond, $mask\" %}\n","filename":"src\/hotspot\/cpu\/x86\/x86.ad","additions":209,"deletions":220,"binary":false,"changes":429,"status":"modified"}]}