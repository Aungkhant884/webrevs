{"files":[{"patch":"@@ -73,1 +73,1 @@\n-    ShenandoahHeap::cas_oop(fwd, load_addr, obj);\n+    ShenandoahHeap::atomic_update_oop(fwd, load_addr, obj);\n@@ -133,1 +133,1 @@\n-    ShenandoahHeap::cas_oop(fwd, load_addr, obj);\n+    ShenandoahHeap::atomic_update_oop(fwd, load_addr, obj);\n@@ -352,1 +352,1 @@\n-        oop witness = ShenandoahHeap::cas_oop(fwd, elem_ptr, o);\n+        ShenandoahHeap::atomic_update_oop(fwd, elem_ptr, o);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahBarrierSet.inline.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -57,1 +57,1 @@\n-        ShenandoahHeap::cas_oop(fwd, p, o);\n+        ShenandoahHeap::atomic_update_oop(fwd, p, o);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahBarrierSetClone.inline.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -164,1 +164,1 @@\n-      _heap->cas_oop(resolved, p, o);\n+      ShenandoahHeap::atomic_update_oop(resolved, p, o);\n@@ -210,1 +210,1 @@\n-        Atomic::cmpxchg(p, obj, oop());\n+        ShenandoahHeap::atomic_clear_oop(p, obj);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahClosures.inline.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -689,1 +689,1 @@\n-      Atomic::cmpxchg(p, obj, oop(NULL));\n+      ShenandoahHeap::atomic_clear_oop(p, obj);\n@@ -695,1 +695,1 @@\n-      Atomic::cmpxchg(p, obj, resolved);\n+      ShenandoahHeap::atomic_update_oop(resolved, p, obj);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahConcurrentGC.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -638,3 +638,11 @@\n-  static inline oop cas_oop(oop n, narrowOop* addr, oop c);\n-  static inline oop cas_oop(oop n, oop* addr, oop c);\n-  static inline oop cas_oop(oop n, narrowOop* addr, narrowOop c);\n+  static inline void atomic_update_oop(oop update,       oop* addr,       oop compare);\n+  static inline void atomic_update_oop(oop update, narrowOop* addr,       oop compare);\n+  static inline void atomic_update_oop(oop update, narrowOop* addr, narrowOop compare);\n+\n+  static inline bool atomic_update_oop_check(oop update,       oop* addr,       oop compare);\n+  static inline bool atomic_update_oop_check(oop update, narrowOop* addr,       oop compare);\n+  static inline bool atomic_update_oop_check(oop update, narrowOop* addr, narrowOop compare);\n+\n+  static inline void atomic_clear_oop(      oop* addr,       oop compare);\n+  static inline void atomic_clear_oop(narrowOop* addr,       oop compare);\n+  static inline void atomic_clear_oop(narrowOop* addr, narrowOop compare);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.hpp","additions":11,"deletions":3,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -135,3 +135,1 @@\n-      \/\/ We only check that non-NULL store still updated with non-forwarded reference.\n-      oop witness = cas_oop(fwd, p, obj);\n-      shenandoah_assert_not_forwarded_except(p, witness, (witness == NULL) || (witness == obj));\n+      atomic_update_oop(fwd, p, obj);\n@@ -142,1 +140,86 @@\n-inline oop ShenandoahHeap::cas_oop(oop n, oop* addr, oop c) {\n+\/\/ Atomic updates of heap location. This is only expected to work with updating the same\n+\/\/ logical object with its forwardee. The reason why we need stronger-than-relaxed memory\n+\/\/ ordering has to do with coordination with GC barriers and mutator accesses.\n+\/\/\n+\/\/ In essence, stronger CAS access is required to maintain the transitive acq\/rel that mutator\n+\/\/ accesses build by themselves. To illustrate this point, consider the following example.\n+\/\/\n+\/\/ Suppose \"o\" is the object that has a field \"x\" and the reference to \"o\" is stored\n+\/\/ to field at \"addr\", which happens to be Java volatile field. Normally, the accesses to volatile\n+\/\/ field at \"addr\" would be matched with release\/acquire barriers. This changes when GC moves\n+\/\/ the object under mutator feet.\n+\/\/\n+\/\/ Thread 1 (Java)\n+\/\/         \/\/ --- previous access starts here\n+\/\/         ...\n+\/\/   T1.1: store(&o.x, 1, mo_relaxed);\n+\/\/   T1.2: store(&addr, o, mo_release); \/\/ volatile store\n+\/\/\n+\/\/         \/\/ --- new access starts here\n+\/\/         \/\/ LRB: copy and install the new copy to fwdptr\n+\/\/   T1.3: var copy = copy(o)\n+\/\/   T1.4: cas(&fwd, t, copy, mo_release)\n+\/\/         <access continues>\n+\/\/\n+\/\/ Thread 2 (GC updater)\n+\/\/   T2.1: var f = load(&fwd, mo_acquire)\n+\/\/   T2.2: cas(&addr, o, f, mo_release) \/\/ this method\n+\/\/\n+\/\/ Thread 3 (Java)\n+\/\/   T3.1: var o = load(&addr, mo_acquire) \/\/ volatile read\n+\/\/   T3.2: if (o != null)\n+\/\/   T3.3:   var r = load(&o.x, mo_relaxed)\n+\/\/\n+\/\/ r is guaranteed to contain \"1\".\n+\/\/\n+\/\/ Without GC involvement, there is synchronizes-with edge from T1.2 to T3.1,\n+\/\/ which guarantees this. With GC involvement, when LRB copies the object and\n+\/\/ another thread updates the reference to it, we need to have the transitive edge\n+\/\/ from T1.4 to T2.1 (that one is guaranteed by forwarding accesses), plus the edge\n+\/\/ from T2.2 to T3.1 (which is brought by this CAS).\n+\/\/\n+\/\/ Note that we do not need to \"acquire\" in these methods, because we do not read the\n+\/\/ failure witnesses contents on any path.\n+\/\/\n+\n+inline void ShenandoahHeap::atomic_update_oop(oop update, oop* addr, oop compare) {\n+  assert(is_aligned(addr, HeapWordSize), \"Address should be aligned: \" PTR_FORMAT, p2i(addr));\n+  Atomic::cmpxchg(addr, compare, update, memory_order_release);\n+}\n+\n+inline void ShenandoahHeap::atomic_update_oop(oop update, narrowOop* addr, narrowOop compare) {\n+  assert(is_aligned(addr, sizeof(narrowOop)), \"Address should be aligned: \" PTR_FORMAT, p2i(addr));\n+  narrowOop u = CompressedOops::encode(update);\n+  Atomic::cmpxchg(addr, compare, u, memory_order_release);\n+}\n+\n+inline void ShenandoahHeap::atomic_update_oop(oop update, narrowOop* addr, oop compare) {\n+  assert(is_aligned(addr, sizeof(narrowOop)), \"Address should be aligned: \" PTR_FORMAT, p2i(addr));\n+  narrowOop c = CompressedOops::encode(compare);\n+  narrowOop u = CompressedOops::encode(update);\n+  Atomic::cmpxchg(addr, c, u, memory_order_release);\n+}\n+\n+inline bool ShenandoahHeap::atomic_update_oop_check(oop update, oop* addr, oop compare) {\n+  assert(is_aligned(addr, HeapWordSize), \"Address should be aligned: \" PTR_FORMAT, p2i(addr));\n+  return (oop) Atomic::cmpxchg(addr, compare, update, memory_order_release) == compare;\n+}\n+\n+inline bool ShenandoahHeap::atomic_update_oop_check(oop update, narrowOop* addr, narrowOop compare) {\n+  assert(is_aligned(addr, sizeof(narrowOop)), \"Address should be aligned: \" PTR_FORMAT, p2i(addr));\n+  narrowOop u = CompressedOops::encode(update);\n+  return (narrowOop) Atomic::cmpxchg(addr, compare, u, memory_order_release) == compare;\n+}\n+\n+inline bool ShenandoahHeap::atomic_update_oop_check(oop update, narrowOop* addr, oop compare) {\n+  assert(is_aligned(addr, sizeof(narrowOop)), \"Address should be aligned: \" PTR_FORMAT, p2i(addr));\n+  narrowOop c = CompressedOops::encode(compare);\n+  narrowOop u = CompressedOops::encode(update);\n+  return CompressedOops::decode(Atomic::cmpxchg(addr, c, u, memory_order_release)) == compare;\n+}\n+\n+\/\/ The memory ordering discussion above does not apply for methods that store NULLs:\n+\/\/ then, there is no transitive reads in mutator (as we see NULLs), and we can do\n+\/\/ relaxed memory ordering there.\n+\n+inline void ShenandoahHeap::atomic_clear_oop(oop* addr, oop compare) {\n@@ -144,1 +227,1 @@\n-  return (oop) Atomic::cmpxchg(addr, c, n);\n+  Atomic::cmpxchg(addr, compare, oop(), memory_order_relaxed);\n@@ -147,1 +230,1 @@\n-inline oop ShenandoahHeap::cas_oop(oop n, narrowOop* addr, narrowOop c) {\n+inline void ShenandoahHeap::atomic_clear_oop(narrowOop* addr, oop compare) {\n@@ -149,2 +232,2 @@\n-  narrowOop val = CompressedOops::encode(n);\n-  return CompressedOops::decode(Atomic::cmpxchg(addr, c, val));\n+  narrowOop cmp = CompressedOops::encode(compare);\n+  Atomic::cmpxchg(addr, cmp, narrowOop(), memory_order_relaxed);\n@@ -153,1 +236,1 @@\n-inline oop ShenandoahHeap::cas_oop(oop n, narrowOop* addr, oop c) {\n+inline void ShenandoahHeap::atomic_clear_oop(narrowOop* addr, narrowOop compare) {\n@@ -155,3 +238,1 @@\n-  narrowOop cmp = CompressedOops::encode(c);\n-  narrowOop val = CompressedOops::encode(n);\n-  return CompressedOops::decode(Atomic::cmpxchg(addr, cmp, val));\n+  Atomic::cmpxchg(addr, compare, narrowOop(), memory_order_relaxed);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.inline.hpp","additions":93,"deletions":12,"binary":false,"changes":105,"status":"modified"},{"patch":"@@ -120,14 +120,3 @@\n-static bool reference_cas_discovered(oop reference, oop discovered);\n-\n-template<>\n-bool reference_cas_discovered<narrowOop>(oop reference, oop discovered) {\n-  volatile narrowOop* addr = reinterpret_cast<volatile narrowOop*>(java_lang_ref_Reference::discovered_addr_raw(reference));\n-  narrowOop compare = CompressedOops::encode(NULL);\n-  narrowOop exchange = CompressedOops::encode(discovered);\n-  return Atomic::cmpxchg(addr, compare, exchange) == compare;\n-}\n-\n-template<>\n-bool reference_cas_discovered<oop>(oop reference, oop discovered) {\n-  volatile oop* addr = reinterpret_cast<volatile oop*>(java_lang_ref_Reference::discovered_addr_raw(reference));\n-  return Atomic::cmpxchg(addr, oop(NULL), discovered) == NULL;\n+static bool reference_cas_discovered(oop reference, oop discovered) {\n+  T* addr = reinterpret_cast<T *>(java_lang_ref_Reference::discovered_addr_raw(reference));\n+  return ShenandoahHeap::atomic_update_oop_check(discovered, addr, NULL);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahReferenceProcessor.cpp","additions":3,"deletions":14,"binary":false,"changes":17,"status":"modified"}]}