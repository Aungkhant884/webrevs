{"files":[{"patch":"@@ -474,0 +474,3 @@\n+  product(bool, ReduceAllocationMerges, false,                              \\\n+          \"Try to simplify allocation merges before Scalar Replacement\")    \\\n+                                                                            \\\n","filename":"src\/hotspot\/share\/opto\/c2_globals.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -54,0 +54,3 @@\n+const char* C2Compiler::retry_no_reduce_allocation_merges() {\n+  return \"retry without trying to reduce allocation merges\";\n+}\n@@ -106,0 +109,1 @@\n+  bool do_reduce_allocation_merges = ReduceAllocationMerges;\n@@ -110,2 +114,3 @@\n-    \/\/ Attempt to compile while subsuming loads into machine instructions.\n-    Options options(subsume_loads, do_escape_analysis, do_iterative_escape_analysis, eliminate_boxing, do_locks_coarsening, install_code);\n+    Options options(subsume_loads, do_escape_analysis, do_iterative_escape_analysis,\n+                    do_reduce_allocation_merges, eliminate_boxing, do_locks_coarsening,\n+                    install_code);\n@@ -138,0 +143,6 @@\n+      if (C.failure_reason_is(retry_no_reduce_allocation_merges())) {\n+        assert(do_reduce_allocation_merges, \"must make progress\");\n+        do_reduce_allocation_merges = false;\n+        env->report_failure(C.failure_reason());\n+        continue;  \/\/ retry\n+      }\n","filename":"src\/hotspot\/share\/opto\/c2compiler.cpp","additions":13,"deletions":2,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -53,0 +53,1 @@\n+  static const char* retry_no_reduce_allocation_merges();\n","filename":"src\/hotspot\/share\/opto\/c2compiler.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -1098,0 +1098,6 @@\n+\/\/----------------------------is_uncommon_trap----------------------------\n+\/\/ Returns true if this is an uncommon trap.\n+bool CallStaticJavaNode::is_uncommon_trap() const {\n+  return (_name != NULL && !strcmp(_name, \"uncommon_trap\"));\n+}\n+\n@@ -1101,4 +1107,1 @@\n-  if (_name != NULL && !strcmp(_name, \"uncommon_trap\")) {\n-    return extract_uncommon_trap_request(this);\n-  }\n-  return 0;\n+  return is_uncommon_trap() ? extract_uncommon_trap_request(this) : 0;\n@@ -1477,1 +1480,2 @@\n-      && !(alloc->Opcode() == Op_VectorBox)) {\n+      && !(alloc->Opcode() == Op_VectorBox)\n+      && !alloc->is_ReducedAllocationMerge()) {\n@@ -1479,1 +1483,1 @@\n-    assert(false, \"unexpected call node\");\n+    assert(false, \"unexpected node.\");\n@@ -1617,0 +1621,227 @@\n+\/\/=============================================================================\n+ReducedAllocationMergeNode::ReducedAllocationMergeNode(Compile* C, PhaseIterGVN* igvn, const PhiNode* phi)\n+    : TypeNode(phi->type(), phi->req()) {\n+\n+  init_class_id(Class_ReducedAllocationMerge);\n+  init_flags(Flag_is_macro);\n+\n+  const Type* ram_t       = igvn->type(phi);\n+\n+  _num_orig_inputs        = phi->req();\n+  _needs_all_fields       = false;\n+  _memories_indexes_start = -1;\n+  _fields_and_values      = new (C->comp_arena()) Dict(cmpkey, hashkey);\n+  _klass                  = ram_t->make_oopptr()->is_instptr()->instance_klass();\n+\n+  for (uint i=0; i<phi->req(); i++) {\n+    init_req(i, phi->in(i));\n+  }\n+\n+  \/\/ Try to find a memory Phi coming from same region\n+  Node* reg = phi->region();\n+  for (DUIterator_Fast imax, i = reg->fast_outs(imax); i < imax; i++) {\n+    Node* n = reg->fast_out(i);\n+    if (n->is_Phi() && n->bottom_type() == Type::MEMORY) {\n+      _memories_indexes_start = req();\n+\n+      for (uint j=1; j<n->req(); j++) {\n+        add_req(n->in(j));\n+      }\n+\n+      break;\n+    }\n+  }\n+\n+  this->raise_bottom_type(ram_t);\n+  igvn->set_type(this, ram_t);\n+\n+  C->add_macro_node(this);\n+}\n+\n+bool ReducedAllocationMergeNode::register_addp(Node* n) {\n+  jlong field = n->in(AddPNode::Offset)->find_long_con(-1);\n+\n+  assert(field != -1, \"Didn't find constant for AddP.\");\n+\n+  if ((*_fields_and_values)[(void*)field] == NULL) {\n+    _fields_and_values->Insert((void*)field, (void*)new Node_Array(Compile::current()->node_arena(), _num_orig_inputs));\n+  }\n+\n+  \/\/ If we didn't find memory edges to use so far then try to\n+  \/\/ figure out which Memory edge subsequent Loads of this field use\n+  if (_memories_indexes_start == -1) {\n+    for (DUIterator_Fast imax, i = n->fast_outs(imax); i < imax; i++) {\n+      Node* addp_use = n->fast_out(i);\n+\n+      if (addp_use->is_Load()) {\n+        Node* load_mem = addp_use->in(LoadNode::Memory);\n+\n+        \/\/ Save the index where we are storing the memory edge\n+        _memories_indexes_start = req();\n+\n+        if (load_mem->is_Phi()) {\n+          for (uint j=1; j<_num_orig_inputs; j++) {\n+            add_req(load_mem->in(j));\n+          }\n+        }\n+        else {\n+          for (uint j=1; j<_num_orig_inputs; j++) {\n+            add_req(load_mem);\n+          }\n+        }\n+\n+        break;\n+      }\n+      else {\n+        assert(false, \"User of AddP in RAM is not a Load.\");\n+      }\n+    }\n+  }\n+\n+  return true;\n+}\n+\n+bool ReducedAllocationMergeNode::register_use(Node* n) {\n+  if (n->is_AddP()) {\n+    return register_addp(n);\n+  }\n+  else if (n->Opcode() == Op_SafePoint || (n->is_CallStaticJava() && n->as_CallStaticJava()->is_uncommon_trap())) {\n+    _needs_all_fields = true;\n+    return true;\n+  }\n+  else if (n->is_DecodeN()) {\n+    for (DUIterator_Fast imax, i = n->fast_outs(imax); i < imax; i++) {\n+      Node* dec_use = n->fast_out(i);\n+\n+      if (register_addp(dec_use) == false) {\n+        return false;\n+      }\n+    }\n+\n+    return true;\n+  }\n+  else {\n+    assert(false, \"Trying to register unsupported use in RAM -> %d : %s\", n->_idx, n->Name());\n+    return false;\n+  }\n+}\n+\n+Node* ReducedAllocationMergeNode::memory_for(jlong field, Node* base) const {\n+  assert(_memories_indexes_start != -1, \"Didn't find memory edges yet?\");\n+\n+  for (uint i=1; i<_num_orig_inputs; i++) {\n+    if (base == in(i)) {\n+      return in(_memories_indexes_start + i - 1);\n+    }\n+  }\n+\n+  assert(false, \"Did not find a matching base when searching for memory.\");\n+  return NULL;\n+}\n+\n+bool ReducedAllocationMergeNode::register_value_for_field(jlong field, Node* base, Node* value) {\n+  \/\/ It's possible that the entry for this field is null because we didn't\n+  \/\/ see a load to it, just a Safepoint using it all fields.\n+  if ((*_fields_and_values)[(void*)field] == NULL) {\n+    _fields_and_values->Insert((void*)field, (void*)new Node_Array(Compile::current()->node_arena(), _num_orig_inputs));\n+  }\n+\n+  Node_Array* values = (Node_Array*) ((*_fields_and_values)[(void*)field]);\n+\n+  if (_num_orig_inputs == 3) {\n+    if (base == in(1)) {\n+      values->map(1, value);\n+    }\n+    else if (base == in(2)) {\n+      values->map(2, value);\n+    }\n+    else {\n+      return false;\n+    }\n+  }\n+  else {\n+    uint i = 1;\n+    for (; i<_num_orig_inputs; i++) {\n+      if (base == in(i)) {\n+        values->map(i, value);\n+        break;\n+      }\n+    }\n+    if (i == _num_orig_inputs) {\n+      return false;\n+    }\n+  }\n+\n+  this->add_req(value);\n+\n+  return true;\n+}\n+\n+Node* ReducedAllocationMergeNode::make_load(Node* ctrl, Node* base, Node* mem, jlong offset, PhaseIterGVN* igvn) {\n+  Node* off                  = igvn->transform((Node*)ConLNode::make(offset));\n+  Node* addp                 = igvn->transform(new AddPNode(base, base, off));\n+\n+  const TypePtr* adr_type     = addp->bottom_type()->is_ptr();\n+  const TypeInstPtr* res_type = igvn->type(base)->is_instptr();\n+  ciInstanceKlass* iklass     = res_type->instance_klass();\n+  ciField* field              = iklass->get_field_by_offset(offset, \/*is_static=*\/false);\n+  ciType* elem_type           = field->type();\n+  BasicType basic_elem_type   = field->layout_type();\n+  const Type* field_type      = NULL;\n+\n+  if (is_reference_type(basic_elem_type)) {\n+    if (!elem_type->is_loaded()) {\n+      field_type = TypeInstPtr::BOTTOM;\n+    }\n+    else {\n+      field_type = TypeOopPtr::make_from_klass(elem_type->as_klass());\n+    }\n+    if (UseCompressedOops) {\n+      field_type = field_type->make_narrowoop();\n+      basic_elem_type = T_OBJECT;\n+    }\n+  }\n+  else {\n+    field_type = Type::get_const_basic_type(basic_elem_type);\n+  }\n+\n+  Node* load = LoadNode::make(*igvn, ctrl, mem, addp, adr_type, field_type, basic_elem_type, MemNode::unordered,\n+                                LoadNode::DependsOnlyOnTest, false, false, false, false, (uint8_t)0U, false);\n+  return igvn->register_new_node_with_optimizer(load);\n+}\n+\n+Node* ReducedAllocationMergeNode::value_phi_for_field(jlong field, PhaseIterGVN* igvn) {\n+  PhiNode* phi       = new PhiNode(this->in(0), Type::BOTTOM);\n+  Node_Array* values = (Node_Array*) ((*_fields_and_values)[(void*)field]);\n+  const Type *t      = Type::TOP;\n+\n+  for (uint i=1; i<_num_orig_inputs; i++) {\n+    if (values->at(i) == NULL) {\n+      Node* load = make_load(this->in(0)->in(i), in(i), in(_memories_indexes_start + i - 1), field, igvn);\n+      values->map(i, load);\n+    }\n+    phi->set_req(i, values->at(i));\n+    const Type* input_type = igvn->type(values->at(i));\n+    t = t->meet_speculative(input_type);\n+  }\n+\n+  igvn->set_type(phi, t);\n+  phi->raise_bottom_type(t);\n+\n+  return phi;\n+}\n+\n+ReducedAllocationMergeNode* ReducedAllocationMergeNode::make(Compile* C, PhaseIterGVN* igvn, PhiNode* phi) {\n+  ReducedAllocationMergeNode* ram = new ReducedAllocationMergeNode(C, igvn, phi);\n+\n+  for (DUIterator_Fast imax, i = phi->fast_outs(imax); i < imax; i++) {\n+    Node* n = phi->fast_out(i);\n+    if (!ram->register_use(n)) {\n+      return NULL;\n+    }\n+  }\n+\n+  return ram;\n+}\n+\n+\n","filename":"src\/hotspot\/share\/opto\/callnode.cpp","additions":237,"deletions":6,"binary":false,"changes":243,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"opto\/addnode.hpp\"\n@@ -31,0 +32,1 @@\n+#include \"opto\/cfgnode.hpp\"\n@@ -738,0 +740,1 @@\n+  bool is_uncommon_trap() const;\n@@ -1011,0 +1014,57 @@\n+\/\/ This node is used during SR to simplify allocation merges.\n+\/\/ It's in this file just because it's closely related to allocation.\n+\/\/\n+\/\/ Before elimination of macro nodes start, some Phi nodes that merge\n+\/\/ object allocations are replaced by a ReducedAllocationMergeNode (aka RAM).\n+\/\/ The users (mostly fields) of the merged allocation are _registered_ in\n+\/\/ the RAM node. During allocation node removal (macro node expansion \/\n+\/\/ scalar replacement), if an allocation is used by a RAM node, the\n+\/\/ nodes producing value for fields registered in the\n+\/\/ RAM, are also registered in the RAM node (in association with\n+\/\/ corresponding allocation base).\n+\/\/ After the inputs to the RAM node are scalar replaced the RAM node\n+\/\/ itself is scalar replaced. This consist basically in replacing the\n+\/\/ use(s) of the merged allocation field(s) value by a new Phi node\n+\/\/ merging the value produced in the different inputs to the RAM node.\n+\/\/ In some cases a reference to the whole merged object is needed and\n+\/\/ we handle that by creating an SafePointScalarObjectNode.\n+class ReducedAllocationMergeNode : public TypeNode {\n+private:\n+  ciKlass* _klass;                  \/\/ Which Klass is the merge for\n+\n+  uint _num_orig_inputs;            \/\/ Number of inputs to the original Phi\n+\n+  bool _needs_all_fields;           \/\/ This is set to true when there was a Safepoint or uncommon_trap\n+                                    \/\/ using the original Phi. In that situation we need information of\n+                                    \/\/ all fields reaching the Safepoint\/trap so that we can construct\n+                                    \/\/ a SafepoingScalarObjectNode\n+\n+  Dict* _fields_and_values;\n+\n+  int _memories_indexes_start;\n+\n+public:\n+  ReducedAllocationMergeNode(Compile* C, PhaseIterGVN* igvn, const PhiNode* phi) ;\n+\n+  virtual int Opcode() const;\n+\n+  ciKlass* klass() const { return _klass; }\n+\n+  bool needs_field(intptr_t field) const {\n+    return _needs_all_fields || ((*_fields_and_values)[(void*)field] != NULL);\n+  }\n+\n+  bool register_addp(Node* n);\n+  bool register_use(Node* n);\n+\n+  Node* memory_for(jlong field, Node* base) const ;\n+\n+  bool register_value_for_field(jlong field, Node* base, Node* value) ;\n+\n+  Node* make_load(Node* ctrl, Node* base, Node* mem, jlong offset, PhaseIterGVN* igvn);\n+  Node* value_phi_for_field(jlong field, PhaseIterGVN* igvn) ;\n+\n+  static ReducedAllocationMergeNode* make(Compile* C, PhaseIterGVN* igvn, PhiNode* phi) ;\n+};\n+\n+\n","filename":"src\/hotspot\/share\/opto\/callnode.hpp","additions":60,"deletions":0,"binary":false,"changes":60,"status":"modified"},{"patch":"@@ -128,2 +128,3 @@\n-\/\/ Helper function: Return any PhiNode that uses this region or NULL\n-PhiNode* RegionNode::has_phi() const {\n+\/\/ Helper function: Return true if there is any PhiNode or ReducedAllocationMerge\n+\/\/ using this region node.\n+bool RegionNode::has_phi() const {\n@@ -132,1 +133,1 @@\n-    if (phi->is_Phi()) {   \/\/ Check for Phi users\n+    if (phi->is_Phi() || phi->is_ReducedAllocationMerge()) {\n@@ -134,1 +135,1 @@\n-      return phi->as_Phi();  \/\/ this one is good enough\n+      return true;\n@@ -138,1 +139,1 @@\n-  return NULL;\n+  return false;\n@@ -450,1 +451,1 @@\n-    has_phis = (has_phi() != NULL);       \/\/ Cache result\n+    has_phis = has_phi();       \/\/ Cache result\n","filename":"src\/hotspot\/share\/opto\/cfgnode.cpp","additions":7,"deletions":6,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -89,1 +89,1 @@\n-  PhiNode* has_phi() const;        \/\/ returns an arbitrary phi user, or NULL\n+  bool has_phi() const;        \/\/ returns true if there is a Phi or RAM use of this region\n","filename":"src\/hotspot\/share\/opto\/cfgnode.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -307,0 +307,1 @@\n+macro(ReducedAllocationMerge)\n","filename":"src\/hotspot\/share\/opto\/classes.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -2210,0 +2210,13 @@\n+\n+    \/\/ TODO: Perform reduce_allocation_merges as part of 'compute_escape'\n+    if (do_reduce_allocation_merges() && ReduceAllocationMerges) {\n+      ConnectionGraph::do_analysis(this, &igvn, \/*only_analysis=*\/true);\n+      if (failing())  return;\n+      igvn.optimize();\n+      if (congraph() != NULL) {\n+        print_method(PHASE_BEFORE_REDUCE_ALLOCATION, 2);\n+        congraph()->reduce_allocation_merges();\n+        print_method(PHASE_AFTER_REDUCE_ALLOCATION, 2);\n+      }\n+    }\n+\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":13,"deletions":0,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -178,0 +178,1 @@\n+  const bool _do_reduce_allocation_merges;   \/\/ Do try to reduce allocation merges.\n@@ -184,0 +185,1 @@\n+          bool do_reduce_allocation_merges,\n@@ -189,0 +191,1 @@\n+          _do_reduce_allocation_merges(do_reduce_allocation_merges),\n@@ -199,0 +202,1 @@\n+       \/* do_reduce_allocation_merges = *\/ false,\n@@ -546,0 +550,1 @@\n+  bool              do_reduce_allocation_merges() const  { return _options._do_reduce_allocation_merges; }\n","filename":"src\/hotspot\/share\/opto\/compile.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -94,1 +94,1 @@\n-void ConnectionGraph::do_analysis(Compile *C, PhaseIterGVN *igvn) {\n+void ConnectionGraph::do_analysis(Compile *C, PhaseIterGVN *igvn, bool only_analysis) {\n@@ -103,1 +103,1 @@\n-  if (C->congraph() != NULL) {\n+  if (C->congraph() != NULL && !only_analysis) {\n@@ -108,1 +108,1 @@\n-  if (congraph->compute_escape()) {\n+  if (congraph->compute_escape(only_analysis)) {\n@@ -121,1 +121,1 @@\n-bool ConnectionGraph::compute_escape() {\n+bool ConnectionGraph::compute_escape(bool only_analysis) {\n@@ -281,0 +281,14 @@\n+  \/\/ 2.a The step above might have transitively marked objects in non_escaped_allocs_worklist\n+  \/\/ as ArgEscape or GlobalEscape\n+  for (int next = non_escaped_allocs_worklist.length()-1; next >= 0 ; --next) {\n+    JavaObjectNode* ptn = non_escaped_allocs_worklist.at(next);\n+    if (ptn->escape_state() >= PointsToNode::ArgEscape) {\n+      non_escaped_allocs_worklist.delete_at(next);\n+    }\n+  }\n+\n+  if (only_analysis) {\n+    _collecting = false;\n+    return non_escaped_allocs_worklist.length() > 0;\n+  }\n+\n@@ -390,0 +404,250 @@\n+void ConnectionGraph::reduce_allocation_merges() {\n+  Unique_Node_List ideal_nodes;\n+  ideal_nodes.map(_compile->live_nodes(), NULL);\n+  ideal_nodes.push(_compile->root());\n+\n+  bool prev_delay_transform = _igvn->delay_transform();\n+  _igvn->set_delay_transform(true);\n+\n+  for (uint next = 0; next < ideal_nodes.size(); ++next) {\n+    Node* candidate_region = ideal_nodes.at(next);\n+\n+    if (candidate_region->is_Region()) {\n+      Unique_Node_List target_phis;\n+\n+      for (DUIterator_Fast imax, i = candidate_region->fast_outs(imax); i < imax; i++) {\n+        Node* candidate_phi = candidate_region->fast_out(i);\n+\n+        \/\/ Performs several checks to see if we can\/should reduce this Phi\n+        if (candidate_phi->is_Phi() && can_reduce_this_phi(candidate_phi)) {\n+          target_phis.push(candidate_phi);\n+        }\n+      }\n+\n+      for (uint target_phi_idx = 0; target_phi_idx < target_phis.size(); ++target_phi_idx) {\n+        Node* target_phi = target_phis.at(target_phi_idx);\n+        reduce_this_phi(target_phi->as_Phi());\n+      }\n+    }\n+\n+    for (DUIterator_Fast imax, i = candidate_region->fast_outs(imax); i < imax; i++) {\n+      Node* m = candidate_region->fast_out(i);\n+      ideal_nodes.push(m);\n+    }\n+  }\n+\n+  _igvn->set_delay_transform(prev_delay_transform);\n+}\n+\n+bool ConnectionGraph::come_from_allocate(const Node* n) const {\n+  while (true) {\n+    switch (n->Opcode()) {\n+      case Op_CastPP:\n+      case Op_CheckCastPP:\n+      case Op_EncodeP:\n+      case Op_EncodePKlass:\n+      case Op_DecodeN:\n+      case Op_DecodeNKlass:\n+        n = n->in(1);\n+        break;\n+      case Op_Proj:\n+        assert(n->as_Proj()->_con == TypeFunc::Parms, \"Should be proj from a call\");\n+        n = n->in(0);\n+        break;\n+      case Op_Parm:\n+      case Op_GetAndSetN:\n+      case Op_GetAndSetP:\n+      case Op_LoadP:\n+      case Op_LoadN:\n+      case Op_LoadNKlass:\n+      SHENANDOAHGC_ONLY(case Op_ShenandoahLoadReferenceBarrier:)\n+      case Op_ConP:\n+      case Op_CreateEx:\n+      case Op_AllocateArray:\n+      case Op_Phi:\n+        return false;\n+      case Op_Allocate:\n+        return true;\n+      default:\n+        if (n->is_Call()) {\n+          return false;\n+        }\n+        assert(false, \"Should not reach here. Unmatched %d %s\", n->_idx, n->Name());\n+    }\n+  }\n+\n+  \/\/ should never reach here\n+  return false;\n+}\n+\n+bool ConnectionGraph::is_read_only(Node* merge_phi_region, Node* base) const {\n+  Unique_Node_List worklist;\n+  worklist.push(base);\n+\n+  for (uint next = 0; next < worklist.size(); ++next) {\n+    Node* n = worklist.at(next);\n+\n+    for (DUIterator_Fast imax, i = n->fast_outs(imax); i < imax; i++) {\n+      Node* m = n->fast_out(i);\n+\n+      switch (m->Opcode()) {\n+        case Op_CastPP:\n+        case Op_CheckCastPP:\n+        case Op_EncodeP:\n+        case Op_EncodePKlass:\n+        case Op_DecodeN:\n+        case Op_DecodeNKlass:\n+          worklist.push(m);\n+          break;\n+      }\n+\n+      if (m->is_AddP()) {\n+        for (DUIterator_Fast imax, i = m->fast_outs(imax); i < imax; i++) {\n+          Node* child = m->fast_out(i);\n+\n+          if (child->is_Store()) {\n+            assert(child->in(0) != NULL || m->in(0) != NULL, \"No control for store or AddP.\");\n+            if (_igvn->is_dominator(merge_phi_region, child->in(0) != NULL ? child->in(0) : m->in(0))) {\n+              return false;\n+            }\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  return true;\n+}\n+\n+bool ConnectionGraph::can_reduce_this_phi(const Node* phi) const {\n+  if (!is_ideal_node_in_graph(phi->_idx)) return false;\n+  if (ptnode_adr(phi->_idx)->escape_state() != PointsToNode::EscapeState::NoEscape) return false;\n+\n+  const Type* phi_t  = _igvn->type(phi);\n+\n+  if (phi_t == NULL || phi_t->make_oopptr() == NULL) {\n+    return false;\n+  }\n+\n+  \/\/ Validate inputs:\n+  \/\/    Check whether this Phi node actually point to Allocate nodes\n+  \/\/    of the same Klass and that they can be scalar replaced. Also\n+  \/\/    checks that there is no write to any of the inputs after the\n+  \/\/    merge occurs.\n+  bool has_any_allocate = false;\n+  for (uint in_idx=1; in_idx<phi->req(); in_idx++) {\n+    Node* input = phi->in(in_idx);\n+\n+    PointsToNode* input_ptn = ptnode_adr(input->_idx);\n+    if (input_ptn == NULL) {\n+      continue;\n+    }\n+\n+    \/\/ Check if input comes from Allocate and does not escape\n+    has_any_allocate |= (input_ptn->escape_state() == PointsToNode::EscapeState::NoEscape && come_from_allocate(input));\n+\n+    \/\/ Check if there is no write to the input after it is merged.\n+    \/\/ If there is a write to any input after the merge we need to bail out.\n+    if (!is_read_only(phi->in(0), input)) {\n+      NOT_PRODUCT(if (Verbose) tty->print_cr(\"Will NOT try to reduce Phi %d. The %dth input has a store after the merge.\", phi->_idx, in_idx);)\n+      return false;\n+    }\n+  }\n+\n+  \/\/ If there was no input that can be removed then there is\n+  \/\/ no profit doing the reduction of inputs.\n+  if (!has_any_allocate) {\n+    NOT_PRODUCT(if (Verbose) tty->print_cr(\"Will NOT try to reduce Phi %d. There is not any NoEscape Allocate as input.\", phi->_idx);)\n+    return false;\n+  }\n+\n+  \/\/ Validate outputs:\n+  \/\/    Check if we can in fact later replace the uses of the\n+  \/\/    current Phi by Phi's of individual fields.\n+  \/\/    Conditions checked:\n+  \/\/       - The only consumers of the Phi are:\n+  \/\/           - AddP (with constant offset)\n+  \/\/           -   - Load\n+  \/\/           - Safepoint\n+  \/\/           - uncommon_trap\n+  \/\/           - DecodeN\n+  \/\/\n+  \/\/ TODO: add support for other kind of users.\n+  for (DUIterator_Fast imax, i = phi->fast_outs(imax); i < imax; i++) {\n+    Node* use = phi->fast_out(i);\n+\n+    if (!use->is_AddP() && !use->is_CallStaticJava() && use->Opcode() != Op_SafePoint && !use->is_DecodeN()) {\n+      NOT_PRODUCT(if (Verbose) tty->print_cr(\"Will NOT try to reduce Phi %d. Has Allocate but cannot scalar replace it. One of the uses is: %d %s\", phi->_idx, use->_idx, use->Name());)\n+      return false;\n+    }\n+\n+    if (use->is_CallStaticJava() && !use->as_CallStaticJava()->is_uncommon_trap()) {\n+      NOT_PRODUCT(if (Verbose) tty->print_cr(\"Will NOT try to reduce Phi %d. Has Allocate but cannot scalar replace it. CallStaticJava is not a trap.\", phi->_idx);)\n+      return false;\n+    }\n+\n+    if (use->is_AddP()) {\n+      if (use->in(AddPNode::Offset)->find_long_con(-1) == -1) {\n+        NOT_PRODUCT(if (Verbose) tty->print_cr(\"Will NOT try to reduce Phi %d. Did not find constant input for %d : AddP.\", phi->_idx, use->_idx);)\n+        return false;\n+      }\n+\n+      for (DUIterator_Fast jmax, j = use->fast_outs(jmax); j < jmax; j++) {\n+        Node* use_use = use->fast_out(j);\n+\n+        if (!use_use->is_Load()) {\n+          NOT_PRODUCT(if (Verbose) tty->print_cr(\"Will NOT try to reduce Phi %d. AddP use is not a Load. %d %s\", phi->_idx, use_use->_idx, use_use->Name());)\n+          return false;\n+        }\n+      }\n+    }\n+\n+    if (use->is_DecodeN()) {\n+      for (DUIterator_Fast jmax, j = use->fast_outs(jmax); j < jmax; j++) {\n+        Node* use_use = use->fast_out(j);\n+\n+        if (!use_use->is_AddP()) {\n+          NOT_PRODUCT(if (Verbose) tty->print_cr(\"Will NOT try to reduce Phi %d. DecodeN use is not a Load. %d %s\", phi->_idx, use_use->_idx, use_use->Name());)\n+          return false;\n+        }\n+\n+        for (DUIterator_Fast kmax, k = use_use->fast_outs(kmax); k < kmax; k++) {\n+          Node* use_use_use = use_use->fast_out(k);\n+\n+          if (!use_use_use->is_Load()) {\n+            NOT_PRODUCT(if (Verbose) tty->print_cr(\"Will NOT try to reduce Phi %d. DecodeN.AddP use is not a Load. %d %s\", phi->_idx, use_use_use->_idx, use_use_use->Name());)\n+            return false;\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  NOT_PRODUCT(if (Verbose) tty->print_cr(\"Will reduce Phi %d.\", phi->_idx);)\n+  return true;\n+}\n+\n+bool ConnectionGraph::reduce_this_phi(PhiNode* n) {\n+  Node* ram = ReducedAllocationMergeNode::make(_compile, _igvn, n);\n+\n+  if (ram == NULL) {\n+    return false;\n+  }\n+\n+  _igvn->hash_insert(ram);\n+\n+  \/\/ Patch users of 'n' to instead use 'reduced'\n+  _igvn->replace_node(n, ram);\n+\n+  \/\/ The original phi now should have no users\n+  _igvn->remove_dead_node(n);\n+\n+  _igvn->_worklist.push(ram);\n+\n+\/\/  if (n->_idx == 257 && ram->_idx == 1239) {\n+\/\/    _compile->root()->dump(-100);\n+\/\/  }\n+\/\/\n+  return true;\n+}\n+\n@@ -487,1 +751,1 @@\n-  if (n->is_Call()) {\n+  if (n->is_Call() || n->is_ReducedAllocationMerge()) {\n@@ -496,1 +760,4 @@\n-    } else {\n+    } else if (n->is_ReducedAllocationMerge()) {\n+      add_java_object(n, PointsToNode::NoEscape);\n+    }\n+    else {\n@@ -594,1 +861,1 @@\n-      \/\/ Produces Null or notNull and is used in only in CmpP so\n+      \/\/ Produces Null or notNull and is used only in CmpP so\n@@ -1678,2 +1945,2 @@\n-      \/\/ Skip Allocate's fields which will be processed later.\n-      if (base->ideal_node()->is_Allocate()) {\n+      \/\/ Skip Allocate's & ReducedAllocationMerge fields which will be processed later.\n+      if (base->ideal_node()->is_Allocate() || base->ideal_node()->is_ReducedAllocationMerge()) {\n@@ -1698,0 +1965,2 @@\n+  \/\/ Do nothing for ReducedAllocationMerges because their fields is \"known\".\n+  \/\/\n@@ -1700,1 +1969,1 @@\n-  if (alloc->is_Allocate() && !pta->arraycopy_dst()) {\n+  if (alloc->is_ReducedAllocationMerge() || (alloc->is_Allocate() && !pta->arraycopy_dst())) {\n@@ -2008,2 +2277,2 @@\n-      \/\/ Verify that all fields have initializing values.\n-      if (field->edge_count() == 0) {\n+      \/\/ Verify that all fields have initializing values unless its base is RAM.\n+      if (!base->is_ReducedAllocationMerge() && field->edge_count() == 0) {\n@@ -2541,0 +2810,7 @@\n+  \/\/ case #10: RAM as base\n+  \/\/        {...}      {...}\n+  \/\/          \\          \/\n+  \/\/     ReducedAllocationMerge\n+  \/\/             ||\n+  \/\/            AddP\n+  \/\/\n@@ -3284,0 +3560,3 @@\n+    }\n+    else if (n->is_ReducedAllocationMerge()) {\n+      continue; \/\/ there is no need for this kind of node to have an instance id\n@@ -3368,0 +3647,1 @@\n+                 use->is_ReducedAllocationMerge() ||\n@@ -3514,0 +3794,2 @@\n+      } else if(use->is_ReducedAllocationMerge()) {\n+        continue; \/\/ don't do anything\n@@ -3669,1 +3951,1 @@\n-      assert(n->is_Allocate() || n->is_CheckCastPP() ||\n+      assert(n->is_Allocate() || n->is_ReducedAllocationMerge() || n->is_CheckCastPP() ||\n@@ -3727,1 +4009,1 @@\n-    out->print(\"(\");\n+    out->print(\"Bases: (\");\n@@ -3734,1 +4016,1 @@\n-  out->print(\"[\");\n+  out->print(\"Edges: [\");\n@@ -3739,1 +4021,1 @@\n-  out->print(\" [\");\n+  out->print(\"] Uses: [\");\n@@ -3749,1 +4031,1 @@\n-  out->print(\" ]]  \");\n+  out->print(\" ]  \");\n@@ -3864,1 +4146,0 @@\n-\n","filename":"src\/hotspot\/share\/opto\/escape.cpp","additions":299,"deletions":18,"binary":false,"changes":317,"status":"modified"},{"patch":"@@ -358,0 +358,6 @@\n+\n+  \/\/ Check if the ideal node with ID 'idx' is present in the Connection Graph.\n+  bool is_ideal_node_in_graph(uint idx) const {\n+    return idx < nodes_size() && _nodes.at(idx) != NULL;\n+  }\n+\n@@ -576,1 +582,28 @@\n-  bool compute_escape();\n+  bool compute_escape(bool only_analysis);\n+\n+  \/\/ -------------------------------------------\n+  \/\/ Methods related to Reduce Allocation Merges\n+\n+  \/\/ Returns true if there is a Store node dominated by\n+  \/\/ 'merge_phi_region' for which the associated AddP uses\n+  \/\/ 'base' as Base.\n+  bool is_read_only(Node* merge_phi_region, Node* base) const;\n+\n+  \/\/ Returns non-null if the node producing the initial value\n+  \/\/ for 'n' is an AllocateNode.\n+  bool come_from_allocate(const Node* n) const;\n+\n+  \/\/ Performs several checks to see if the Phi pointed by 'n'\n+  \/\/ can be reduced into a ReducedAllocationMergeNode. The\n+  \/\/ checks curently implemented are:\n+  \/\/  - The phi node should be NoEscape\n+  \/\/  - The Phi region must not dominate any store to any of the Phi inputs\n+  \/\/  - All inputs to the Phi node should come from an allocate node\n+  \/\/  - All inputs should be NoEscape\n+  \/\/  - The only uses of the Phi should be:\n+  \/\/    - AddP->Load\n+  \/\/    - SafePointNode or uncommon traps\n+  \/\/    - DecodeN->AddP->Loads\n+  bool can_reduce_this_phi(const Node* n) const;\n+\n+  bool reduce_this_phi(PhiNode* n);\n@@ -603,1 +636,6 @@\n-  static void do_analysis(Compile *C, PhaseIterGVN *igvn);\n+  static void do_analysis(Compile *C, PhaseIterGVN *igvn, bool only_analysis = false);\n+\n+  \/\/ Perform simplification of allocation merges by reducing Phi\n+  \/\/ nodes that merge scalar replaceable object allocations into\n+  \/\/ a ReduceAllocationMergeNode.\n+  void reduce_allocation_merges();\n","filename":"src\/hotspot\/share\/opto\/escape.hpp","additions":40,"deletions":2,"binary":false,"changes":42,"status":"modified"},{"patch":"@@ -825,1 +825,1 @@\n-        assert(r->has_phi() == NULL, \"simple region shouldn't have a phi\");\n+        assert(!r->has_phi(), \"simple region shouldn't have a phi\");\n","filename":"src\/hotspot\/share\/opto\/ifnode.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -37,0 +37,1 @@\n+#include \"opto\/c2compiler.hpp\"\n@@ -543,1 +544,1 @@\n-      if (sfpt_ctl->is_Proj() && sfpt_ctl->as_Proj()->is_uncommon_trap_proj(Deoptimization::Reason_none)) {\n+      if (sfpt_ctl != NULL && sfpt_ctl->is_Proj() && sfpt_ctl->as_Proj()->is_uncommon_trap_proj(Deoptimization::Reason_none)) {\n@@ -556,1 +557,1 @@\n-bool PhaseMacroExpand::can_eliminate_allocation(AllocateNode *alloc, GrowableArray <SafePointNode *>& safepoints) {\n+bool PhaseMacroExpand::can_eliminate_allocation(AllocateNode *alloc, GrowableArray <SafePointNode *>& safepoints, GrowableArray <ReducedAllocationMergeNode *>& rams) {\n@@ -619,0 +620,3 @@\n+      } else if (use->is_ReducedAllocationMerge()) {\n+        \/\/ also ok to eliminate\n+        rams.append_if_missing(use->as_ReducedAllocationMerge());\n@@ -683,1 +687,1 @@\n-bool PhaseMacroExpand::scalar_replacement(AllocateNode *alloc, GrowableArray <SafePointNode *>& safepoints) {\n+bool PhaseMacroExpand::scalar_replacement(AllocateNode *alloc, GrowableArray <SafePointNode *>& safepoints, GrowableArray <ReducedAllocationMergeNode *>& rams) {\n@@ -715,0 +719,78 @@\n+\n+  \/\/\n+  \/\/ Process the Reduced Allocation Merges uses\n+  \/\/\n+  while (rams.length() > 0) {\n+    ReducedAllocationMergeNode* ram = rams.pop();\n+\n+    _igvn.hash_delete(ram);\n+\n+    \/\/ Scan object's fields adding an input to the RAM for each field.\n+    for (int j = 0; j < nfields; j++) {\n+      const Type *field_type;\n+      \/\/ iklass wont be null here because RAM only merge instance types\n+      assert(iklass != NULL, \"iklass shouldn't be NULL here.\");\n+      ciField* field = iklass->nonstatic_field_at(j);\n+      intptr_t offset = field->offset();\n+      ciType* elem_type = field->type();\n+      basic_elem_type = field->layout_type();\n+\n+      if (!ram->needs_field(offset)) {\n+        continue;\n+      }\n+\n+      if (is_reference_type(basic_elem_type)) {\n+        if (!elem_type->is_loaded()) {\n+          field_type = TypeInstPtr::BOTTOM;\n+        } else if (field != NULL && field->is_static_constant()) {\n+          \/\/ This can happen if the constant oop is non-perm.\n+          ciObject* con = field->constant_value().as_object();\n+          \/\/ Do not \"join\" in the previous type; it doesn't add value,\n+          \/\/ and may yield a vacuous result if the field is of interface type.\n+          field_type = TypeOopPtr::make_from_constant(con)->isa_oopptr();\n+          assert(field_type != NULL, \"field singleton type must be consistent\");\n+        } else {\n+          field_type = TypeOopPtr::make_from_klass(elem_type->as_klass());\n+        }\n+        if (UseCompressedOops) {\n+          field_type = field_type->make_narrowoop();\n+          basic_elem_type = T_NARROWOOP;\n+        }\n+      } else {\n+        field_type = Type::get_const_basic_type(basic_elem_type);\n+      }\n+\n+      const TypeOopPtr *field_addr_type = res_type->add_offset(offset)->isa_oopptr();\n+      Node *memory = ram->memory_for(offset, res);\n+\n+      \/\/ If we can't actually find the memory to be used for this base we won't be able to remove the\n+      \/\/ RAM node and in that situation the only way out is to recompile the method with\n+      \/\/ ReduceAllocations disabled.\n+      \/\/\n+      \/\/ Same is true if we don't find a value that was computed for the field or if we aren't\n+      \/\/ able to register the value for the field in the RAM\n+      if (memory == NULL) {\n+        assert(false, \"Didn't find a matching base for this field!!!\");\n+        C->record_failure(C2Compiler::retry_no_reduce_allocation_merges());\n+        return false;\n+      }\n+\n+      Node *field_val = value_from_mem(memory, NULL, basic_elem_type, field_type, field_addr_type, alloc);\n+\n+      if (field_val == NULL) {\n+        assert(false, \"Didn't find value for field!!!\");\n+        C->record_failure(C2Compiler::retry_no_reduce_allocation_merges());\n+        return false;\n+      }\n+\n+      if (!ram->register_value_for_field(offset, res, field_val)) {\n+        assert(false, \"Didn't find a matching base for this field!!!\");\n+        C->record_failure(C2Compiler::retry_no_reduce_allocation_merges());\n+        return false;\n+      }\n+    }\n+\n+    _igvn.hash_insert(ram);\n+    _igvn._worklist.push(ram);\n+  }\n+\n@@ -934,0 +1016,5 @@\n+      } else if (use->is_ReducedAllocationMerge()) {\n+        _igvn.hash_delete(use);\n+        use->replace_edge(res, C->top());\n+        _igvn.hash_insert(use);\n+        _igvn._worklist.push(use);\n@@ -1036,1 +1123,2 @@\n-  if (!can_eliminate_allocation(alloc, safepoints)) {\n+  GrowableArray <ReducedAllocationMergeNode *> rams;\n+  if (!can_eliminate_allocation(alloc, safepoints, rams)) {\n@@ -1050,1 +1138,1 @@\n-  if (!scalar_replacement(alloc, safepoints)) {\n+  if (!scalar_replacement(alloc, safepoints, rams)) {\n@@ -1120,0 +1208,166 @@\n+bool PhaseMacroExpand::eliminate_reduced_allocation_merge(ReducedAllocationMergeNode *ram) {\n+  ciKlass* klass             = ram->klass();\n+  ciInstanceKlass* iklass    = klass->as_instance_klass();\n+  int nfields                = iklass->nof_nonstatic_fields();\n+  const TypeOopPtr* res_type = _igvn.type(ram)->isa_oopptr();\n+\n+  for (DUIterator_Fast imax, i = ram->fast_outs(imax); i < imax; i++) {\n+    Node* use = ram->fast_out(i);\n+\n+    if (use->is_AddP()) {\n+      Node* addp = use; \/\/ just for readability\n+      jlong offset = addp->in(AddPNode::Offset)->find_long_con(-1);\n+\n+      assert(offset != -1, \"Didn't find constant offset for AddP.\");\n+\n+      Node* value_phi = ram->value_phi_for_field(offset, &_igvn);\n+\n+      if (value_phi == NULL) {\n+        assert(false, \"At RAM node %d can't find value for a field.\", ram->_idx);\n+        return false;\n+      }\n+\n+      _igvn._worklist.push(value_phi);\n+\n+      for (DUIterator_Fast jmax, j = addp->fast_outs(jmax); j < jmax; j++) {\n+        Node* addp_use = addp->fast_out(j);\n+\n+        if (addp_use->is_Load()) {\n+          Node* load = addp_use; \/\/ just for readability\n+\n+          for (DUIterator_Last kmin, k = load->last_outs(kmin); k >= kmin; --k) {\n+            Node* load_use = load->last_out(k);\n+\n+            _igvn.hash_delete(load_use);\n+            load_use->replace_edge(load, value_phi, &_igvn);\n+            _igvn.hash_insert(load_use);\n+            _igvn._worklist.push(load_use);\n+          }\n+        }\n+        else {\n+          assert(false, \"Unexpected use of AddP.\");\n+          return false;\n+        }\n+      }\n+    }\n+    else if (use->Opcode() == Op_SafePoint || use->is_CallStaticJava()) {\n+      Node* sfpt = use;\n+      assert(sfpt->jvms() != NULL, \"missed JVMS\");\n+\n+      \/\/ Fields of scalar objs are referenced only at the end\n+      \/\/ of regular debuginfo at the last (youngest) JVMS.\n+      \/\/ Record relative start index.\n+      uint first_ind = (sfpt->req() - sfpt->jvms()->scloff());\n+      SafePointScalarObjectNode* sobj = new SafePointScalarObjectNode(res_type,\n+                                                                      #ifdef ASSERT\n+                                                                        ram,\n+                                                                      #endif\n+                                                                        first_ind,\n+                                                                        nfields);\n+      sobj->init_req(0, C->root());\n+      transform_later(sobj);\n+\n+      \/\/ Scan object's fields adding an input to the safepoint for each field.\n+      for (int j = 0; j < nfields; j++) {\n+        ciField* field = iklass->nonstatic_field_at(j);\n+        jlong offset = field->offset();\n+        ciType* elem_type = field->type();\n+        BasicType basic_elem_type = field->layout_type();\n+        const Type *field_type;\n+\n+        \/\/ The next code is taken from Parse::do_get_xxx().\n+        if (is_reference_type(basic_elem_type)) {\n+          if (!elem_type->is_loaded()) {\n+            field_type = TypeInstPtr::BOTTOM;\n+          } else if (field != NULL && field->is_static_constant()) {\n+            \/\/ This can happen if the constant oop is non-perm.\n+            ciObject* con = field->constant_value().as_object();\n+            \/\/ Do not \"join\" in the previous type; it doesn't add value,\n+            \/\/ and may yield a vacuous result if the field is of interface type.\n+            field_type = TypeOopPtr::make_from_constant(con)->isa_oopptr();\n+            assert(field_type != NULL, \"field singleton type must be consistent\");\n+          } else {\n+            field_type = TypeOopPtr::make_from_klass(elem_type->as_klass());\n+          }\n+          if (UseCompressedOops) {\n+            field_type = field_type->make_narrowoop();\n+          }\n+        } else {\n+          field_type = Type::get_const_basic_type(basic_elem_type);\n+        }\n+\n+        Node* field_val = ram->value_phi_for_field(offset, &_igvn);\n+\n+        if (field_val == NULL) {\n+          assert(false, \"At RAM node %d can't find value for a field.\", sfpt->_idx);\n+          return false;\n+        }\n+\n+        _igvn._worklist.push(field_val);\n+\n+        if (UseCompressedOops && field_type->isa_narrowoop()) {\n+          field_val = transform_later(new DecodeNNode(field_val, field_val->get_ptr_type()));\n+        }\n+\n+        sfpt->add_req(field_val);\n+      }\n+\n+      JVMState *jvms = sfpt->jvms();\n+      jvms->set_endoff(sfpt->req());\n+      \/\/ Now make a pass over the debug information replacing any references\n+      \/\/ to the allocated object with \"sobj\"\n+      int start = jvms->debug_start();\n+      int end   = jvms->debug_end();\n+      sfpt->replace_edges_in_range(ram, sobj, start, end, &_igvn);\n+      _igvn._worklist.push(sfpt);\n+\n+      --i; --imax;\n+    }\n+    else if (use->is_DecodeN()) {\n+      for (DUIterator_Fast jmax, j = use->fast_outs(jmax); j < jmax; j++) {\n+        Node* addp = use->fast_out(j);\n+\n+        jlong offset = addp->in(AddPNode::Offset)->find_long_con(-1);\n+\n+        assert(offset != -1, \"Didn't find constant offset for AddP.\");\n+\n+        Node* value_phi = ram->value_phi_for_field(offset, &_igvn);\n+\n+        if (value_phi == NULL) {\n+          assert(false, \"At RAM node %d can't find value for a field.\", ram->_idx);\n+          return false;\n+        }\n+\n+        _igvn._worklist.push(value_phi);\n+\n+        for (DUIterator_Fast kmax, k = addp->fast_outs(kmax); k < kmax; k++) {\n+          Node* addp_use = addp->fast_out(k);\n+\n+          if (addp_use->is_Load()) {\n+            Node* load = addp_use; \/\/ just for readability\n+\n+            for (DUIterator_Last lmin, l = load->last_outs(lmin); l >= lmin; --l) {\n+              Node* load_use = load->last_out(l);\n+\n+              _igvn.hash_delete(load_use);\n+              load_use->replace_edge(load, value_phi, &_igvn);\n+              _igvn.hash_insert(load_use);\n+              _igvn._worklist.push(load_use);\n+            }\n+          }\n+          else {\n+            assert(false, \"Unexpected use of AddP.\");\n+            return false;\n+          }\n+        }\n+      }\n+    }\n+    else {\n+      assert(false, \"Unknown use of RAM. %d:%s\", use->_idx, use->Name());\n+      return false;\n+    }\n+  }\n+\n+  return true;\n+}\n+\n@@ -2383,0 +2637,1 @@\n+\n@@ -2405,0 +2660,3 @@\n+      case Node::Class_ReducedAllocationMerge:\n+        \/\/ Needs to be processed after all others\n+        break;\n@@ -2430,0 +2688,14 @@\n+\n+  \/\/ Next, try to eliminate reduced allocation merges\n+  for (int i = C->macro_count(); i > 0; i--) {\n+    Node* n = C->macro_node(i - 1);\n+    if (n->is_ReducedAllocationMerge()) {\n+      bool success = eliminate_reduced_allocation_merge(n->as_ReducedAllocationMerge());\n+      if (!success) {\n+        assert(false, \"Failed to eliminate reduced allocation merge!!!\");\n+        C->record_failure(C2Compiler::retry_no_reduce_allocation_merges());\n+        return;\n+      }\n+    }\n+  }\n+\n@@ -2438,0 +2710,1 @@\n+\n","filename":"src\/hotspot\/share\/opto\/macro.cpp","additions":278,"deletions":5,"binary":false,"changes":283,"status":"modified"},{"patch":"@@ -103,2 +103,1 @@\n-  bool can_eliminate_allocation(AllocateNode *alloc, GrowableArray <SafePointNode *>& safepoints);\n-  bool scalar_replacement(AllocateNode *alloc, GrowableArray <SafePointNode *>& safepoints_done);\n+  bool can_eliminate_allocation(AllocateNode *alloc, GrowableArray <SafePointNode *>& safepoints, GrowableArray <ReducedAllocationMergeNode *>& rams);\n@@ -107,0 +106,16 @@\n+  \/\/ Effectivelly performs scalar replacement by replacing the uses of 'alloc' in\n+  \/\/ the nodes in 'safepoints' and 'rams' by a SafePointScalarObjectNode.\n+  bool scalar_replacement(AllocateNode *alloc, GrowableArray <SafePointNode *>& safepoints_done, GrowableArray <ReducedAllocationMergeNode *>& rams);\n+\n+  \/\/ This should be called only after all scalar replaceable Allocate nodes\n+  \/\/ have been scalar replaced. Therefore the nodes producing values for the\n+  \/\/ fields accessed by users of the RAM have already been registered.\n+  \/\/\n+  \/\/ The method will iterate over the users of 'ram' and replace the nodes\n+  \/\/ that use the _value_ of field 'x' by a value Phi merging nodes that\n+  \/\/ produce value for field 'x' in different control branches. Safepoints\n+  \/\/ and traps are special since they require they have a reference to the\n+  \/\/ 'ram' itself. For those cases we create an SafePointScalarObjectNode,\n+  \/\/ similar to what is done to regular scalar replacement.\n+  bool eliminate_reduced_allocation_merge(ReducedAllocationMergeNode *ram);\n+\n","filename":"src\/hotspot\/share\/opto\/macro.hpp","additions":17,"deletions":2,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -296,0 +296,6 @@\n+  Node *address = in(MemNode::Address);\n+  \/\/ Don't try to optimize loads under RAM\n+  if (address != NULL && address->is_AddP() && address->in(AddPNode::Base) != NULL &&\n+      address->in(AddPNode::Base)->is_ReducedAllocationMerge()) {\n+    return NodeSentinel;\n+  }\n@@ -329,1 +335,0 @@\n-  Node *address = in(MemNode::Address);\n@@ -878,1 +883,2 @@\n-                     ControlDependency control_dependency, bool require_atomic_access, bool unaligned, bool mismatched, bool unsafe, uint8_t barrier_data) {\n+                     ControlDependency control_dependency, bool require_atomic_access, bool unaligned, bool mismatched, bool unsafe, uint8_t barrier_data,\n+                     bool decode_narrow) {\n@@ -930,1 +936,1 @@\n-  if (load->Opcode() == Op_LoadN) {\n+  if (load->Opcode() == Op_LoadN && decode_narrow) {\n","filename":"src\/hotspot\/share\/opto\/memnode.cpp","additions":9,"deletions":3,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -233,1 +233,1 @@\n-                    uint8_t barrier_data = 0);\n+                    uint8_t barrier_data = 0, bool decode_narrow = true);\n","filename":"src\/hotspot\/share\/opto\/memnode.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -155,0 +155,1 @@\n+class ReducedAllocationMergeNode;\n@@ -713,0 +714,1 @@\n+      DEFINE_CLASS_ID(ReducedAllocationMerge,   Type, 8)\n@@ -951,0 +953,1 @@\n+  DEFINE_CLASS_QUERY(ReducedAllocationMerge)\n","filename":"src\/hotspot\/share\/opto\/node.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -819,1 +819,2 @@\n-      ciKlass* cik = t->is_oopptr()->exact_klass();\n+      ciKlass* cik = t->isa_instptr() != NULL ? t->isa_instptr()->instance_klass()\n+                                              : t->is_oopptr()->exact_klass();\n","filename":"src\/hotspot\/share\/opto\/output.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -40,0 +40,2 @@\n+  flags(BEFORE_REDUCE_ALLOCATION,     \"Before reducing allocation merges\") \\\n+  flags(AFTER_REDUCE_ALLOCATION,      \"After reducing allocation merges\") \\\n","filename":"src\/hotspot\/share\/opto\/phasetype.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -0,0 +1,719 @@\n+\/*\n+ * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+package compiler.c2.irTests.scalarReplacement;\n+\n+import compiler.lib.ir_framework.*;\n+\n+\/*\n+ * @test\n+ * @bug 8281429\n+ * @summary Tests that C2 can correctly scalar replace some object allocation merges.\n+ * @library \/test\/lib \/\n+ * @run driver compiler.c2.irTests.scalarReplacement.AllocationMergesTests\n+ *\/\n+public class AllocationMergesTests {\n+\n+    public static void main(String[] args) {\n+        TestFramework.runWithFlags(\"-XX:+ReduceAllocationMerges\", \"-XX:CompileCommand=exclude,*::dummy*\");\n+    }\n+\n+    \/\/ ------------------ No Scalar Replacement Should Happen in The Tests Below ------------------- \/\/\n+\n+    @Test\n+    @Arguments({ Argument.RANDOM_EACH, Argument.RANDOM_EACH })\n+    @IR(counts = { IRNode.ALLOC, \"1\" })\n+    int testGlobalEscape(int x, int y) {\n+        Point p = new Point(x, y);\n+\n+        AllocationMergesTests.global_escape = p;\n+\n+        return p.x * p.y;\n+    }\n+\n+    @Test\n+    @Arguments({ Argument.RANDOM_EACH, Argument.RANDOM_EACH })\n+    @IR(counts = { IRNode.ALLOC, \"1\" })\n+    int testArgEscape(int x, int y) {\n+        Point p = new Point(x, y);\n+\n+        int val = dummy(p);\n+\n+        return val + p.x + p.y;\n+    }\n+\n+    @Test\n+    @Arguments({ Argument.RANDOM_EACH, Argument.RANDOM_EACH, Argument.RANDOM_EACH, Argument.RANDOM_EACH })\n+    @IR(counts = { IRNode.ALLOC, \"2\" })\n+    int testEscapeInCallAfterMerge(boolean cond, boolean cond2, int x, int y) {\n+        Point p = new Point(x, x);\n+\n+        if (cond)\n+            p = new Point(y, y);\n+\n+        if (cond2) {\n+            dummy(p);\n+        }\n+\n+        return p.x * p.y;\n+    }\n+\n+    @Test\n+    @Arguments({ Argument.RANDOM_EACH, Argument.RANDOM_EACH, Argument.RANDOM_EACH, Argument.RANDOM_EACH })\n+    @IR(counts = { IRNode.ALLOC, \"2\" })\n+    \/\/ Merge won't be reduced because write to field inside the loop\n+    int testNoEscapeWithWriteInLoop(boolean cond, boolean cond2, int x, int y) {\n+        Point p = new Point(x, y);\n+        int res = 0;\n+\n+        if (cond)\n+            p = new Point(y, x);\n+\n+        for (int i=0; i<100; i++) {\n+            p.x += p.y + i;\n+            p.y += p.x + i;\n+        }\n+\n+        return res + p.x + p.y;\n+    }\n+\n+    @Test\n+    @Arguments({ Argument.RANDOM_EACH, Argument.RANDOM_EACH })\n+    @IR(counts = { IRNode.ALLOC, \"2\" })\n+    \/\/ Merge won't be reduced because write to field inside the loop\n+    int testPollutedWithWrite(boolean cond, int l) {\n+        Shape obj1 = new Square(l);\n+        Shape obj2 = new Square(l);\n+        Shape obj = null;\n+\n+        if (cond)\n+            obj = obj1;\n+        else\n+            obj = obj2;\n+\n+        for (int i = 1; i < 132; i++) {\n+            obj.x++;\n+        }\n+\n+        return obj1.x + obj2.y;\n+    }\n+\n+    @Test\n+    @Arguments({ Argument.RANDOM_EACH, Argument.RANDOM_EACH })\n+    @IR(counts = { IRNode.ALLOC, \"2\" })\n+    \/\/ Merge won't be reduced because objects have different types\n+    int testPollutedPolymorphic(boolean cond, int l) {\n+        Shape obj1 = new Square(l);\n+        Shape obj2 = new Circle(l);\n+        Shape obj = (cond ? obj1 : obj2);\n+        int res = 0;\n+\n+        for (int i = 1; i < 232; i++) {\n+            res += obj.x;\n+        }\n+\n+        return res + obj1.x + obj2.y;\n+    }\n+\n+    @Test\n+    @Arguments({ Argument.RANDOM_EACH, Argument.RANDOM_EACH, Argument.RANDOM_EACH })\n+    @IR(counts = { IRNode.ALLOC, \"2\" })\n+    \/\/ Merge won't be reduced because write to one of the inputs *after* the merge\n+    int testMergedLoadAfterDirectStore(boolean cond, int x, int y) {\n+        Point p0 = new Point(x, x);\n+        Point p1 = new Point(y, y);\n+        Point p = null;\n+\n+        if (cond)\n+            p = p0;\n+        else\n+            p = p1;\n+\n+        p0.x = x * y;\n+\n+        return p.x;\n+    }\n+\n+    @Test\n+    @Arguments({ Argument.RANDOM_EACH, Argument.RANDOM_EACH, Argument.RANDOM_EACH })\n+    @IR(counts = { IRNode.ALLOC, \"3\" })\n+    \/\/ p2 is ArgEscape\n+    \/\/ p is written inside the loop.\n+    int testMergedAccessAfterCallWithWrite(boolean cond, int x, int y) {\n+        Point p2 = new Point(x, x);\n+        Point p = new Point(y, y);\n+\n+        p.x = p.x * y;\n+\n+        if (cond)\n+            p = new Point(x, x);\n+\n+        dummy(p2);\n+\n+        for (int i=3; i<324; i++)\n+            p.x += i * x;\n+\n+        return p.x;\n+    }\n+\n+    @Test\n+    @Arguments({ Argument.RANDOM_EACH, Argument.RANDOM_EACH, Argument.RANDOM_EACH })\n+    @IR(counts = { IRNode.ALLOC, \"2\" })\n+    \/\/ Allocations will be NSR because they are used in a CallStaticJava\n+    int testLoadAfterTrap(boolean cond, int x, int y) {\n+        Point p = null;\n+\n+        if (cond)\n+            p = new Point(x, x);\n+        else\n+            p = new Point(y, y);\n+\n+        dummy(x+y);\n+\n+        return p.x + p.y;\n+    }\n+\n+    @Test\n+    @Arguments({ Argument.RANDOM_EACH, Argument.RANDOM_EACH, Argument.RANDOM_EACH, Argument.RANDOM_EACH })\n+    @IR(counts = { IRNode.ALLOC, \"1\" })\n+    \/\/ The merge won't be simplified because the merge with NULL instead of Allocate\n+    int testCondAfterMergeWithNull(boolean cond1, boolean cond2, int x, int y) {\n+        Point p = null;\n+\n+        if (cond1)\n+            p = new Point(y, x);\n+\n+        if (cond2 && cond1) {\n+            return p.x;\n+        }\n+        else {\n+            return 321;\n+        }\n+    }\n+\n+    @Test\n+    @Arguments({ Argument.RANDOM_EACH, Argument.RANDOM_EACH, Argument.RANDOM_EACH })\n+    @IR(counts = { IRNode.ALLOC, \"2\" })\n+    \/\/ A loop is appearing between the Phis in this method and is preventing the reduction\n+    int testLoadAfterLoopAlias(boolean cond, int x, int y) {\n+        Point a = new Point(x, y);\n+        Point b = new Point(y, x);\n+        Point c = a;\n+\n+        for (int i=10; i<832; i++) {\n+            if (i == 500) {\n+                c = b;\n+            }\n+        }\n+\n+        return cond ? c.x : c.y;\n+    }\n+\n+    @Test\n+    @Arguments({ Argument.RANDOM_EACH, Argument.RANDOM_EACH, Argument.RANDOM_EACH })\n+    @IR(counts = { IRNode.ALLOC, \"1\" })\n+    \/\/ Merge won't be reduced because one of the inputs come from a call\n+    int testCallOneSide(boolean cond1, int x, int y) {\n+        Point p = dummy(x, y);\n+\n+        if (cond1)\n+            p = new Point(y, x);\n+\n+        return p.x;\n+    }\n+\n+    @Test\n+    @Arguments({ Argument.RANDOM_EACH, Argument.RANDOM_EACH, Argument.RANDOM_EACH })\n+    @IR(counts = { IRNode.CALL, \"3\" })\n+    \/\/ Merge won't be reduced because both of the inputs come from a call\n+    \/\/ The additional \"Call\" node is because of the uncommon_trap for checking if\n+    \/\/ \"p\" is null\n+    int testCallTwoSide(boolean cond1, int x, int y) {\n+        Point p = dummy(x, y);\n+\n+        if (cond1)\n+            p = dummy(y, x);\n+\n+        return p.x;\n+    }\n+\n+    @Test\n+    @Arguments({ Argument.RANDOM_EACH, Argument.RANDOM_EACH, Argument.RANDOM_EACH })\n+    @IR(counts = { IRNode.ALLOC, \"3\" })\n+    \/\/ \"p\" won't be reduced because it's touched by Call to dummy\n+    int testMergedAccessAfterCallNoWrite(boolean cond, int x, int y) {\n+        Point p2 = new Point(x, x);\n+        Point p = new Point(y, y);\n+        int res = 0;\n+\n+        p.x = p.x * y;\n+\n+        if (cond)\n+            p = new Point(y, y);\n+\n+        dummy(p2);\n+\n+        for (int i=3; i<324; i++)\n+            res += p.x + i * x;\n+\n+        return res;\n+    }\n+\n+    @Test\n+    @Arguments({ Argument.RANDOM_EACH, Argument.RANDOM_EACH, Argument.RANDOM_EACH })\n+    @IR(counts = { IRNode.ALLOC, \"2\" })\n+    \/\/ \"p\" is not being reduced because the merge phi is being touched by the second Allocate\n+    int testTrappingAfterMerge(boolean cond, int x, int y) {\n+        Point p = new Point(x, y);\n+        int res = 0;\n+\n+        if (cond)\n+            p = new Point(y, y);\n+\n+        for (int i=832; i<932; i++) {\n+            res += p.x;\n+        }\n+\n+        if (x > y) {\n+            res += new Point(p.x, p.y).x;\n+        }\n+\n+        return res;\n+    }\n+\n+\n+    \/\/ ------------------ Some Objects Will be Scalar Replaced in These Tests ------------------- \/\/\n+\n+\n+    @Test\n+    @Arguments({ Argument.RANDOM_EACH, Argument.RANDOM_EACH, Argument.RANDOM_EACH })\n+    @IR(failOn = {IRNode.ALLOC})\n+    int simpleMerge(boolean cond, int x, int y) {\n+        Point p = new Point(x, y);\n+\n+        if (cond)\n+            p = new Point(y, x);\n+\n+        return p.x * p.y;\n+    }\n+\n+    @Test\n+    @Arguments({ Argument.RANDOM_EACH, Argument.RANDOM_EACH, Argument.RANDOM_EACH })\n+    @IR(failOn = { IRNode.ALLOC })\n+    int testSimpleAliasedAlloc(boolean cond, int x, int y) {\n+        Point p1 = new Point(x, y);\n+        Point p2 = new Point(y, x);\n+        Point p = p1;\n+\n+        if (cond)\n+            p = p2;\n+\n+        return p.x * p.y;\n+    }\n+\n+    @Test\n+    @Arguments({ Argument.RANDOM_EACH, Argument.RANDOM_EACH, Argument.RANDOM_EACH })\n+    @IR(failOn = { IRNode.ALLOC })\n+    int testSimpleDoubleMerge(boolean cond, int x, int y) {\n+        Point p1 = new Point(x, y);\n+        Point p2 = new Point(x+1, y+1);\n+\n+        if (cond) {\n+            p1 = new Point(y, x);\n+            p2 = new Point(y+1, x+1);\n+        }\n+\n+        return p1.x + p2.y;\n+    }\n+\n+    @Test\n+    @Arguments({ Argument.RANDOM_EACH, Argument.RANDOM_EACH })\n+    @IR(counts = { IRNode.ALLOC, \"1\" })\n+    \/\/ \"p2\" is scalar replaced because it's not used in \"dummy\" except as debug info\n+    int testSimpleMixedEscape(int x, int y) {\n+        Point p1 = new Point(x, y);\n+        Point p2 = new Point(x+1, y+1);\n+\n+        int val = dummy(p1);\n+\n+        return val + p1.x + p2.y;\n+    }\n+\n+    @Test\n+    @Arguments({ Argument.RANDOM_EACH, Argument.RANDOM_EACH })\n+    @IR(failOn = { IRNode.ALLOC })\n+    \/\/ Object p is scalar replaced because the \"p = dummy(...)\" calls\n+    \/\/ are actually converted to traps and therefore there is no merge phi\n+    int testMultiwayMerge(int x, int y) {\n+        Point p = new Point(0, 0);\n+\n+        if (x == y) {\n+            p = dummy(x, x);\n+        }\n+        else if (dummy(x) == 1) {\n+            p = dummy(x, y);\n+        }\n+        else if (dummy(y) == 1) {\n+            p = dummy(y, x);\n+        }\n+\n+        return p.x;\n+    }\n+\n+    @Test\n+    @Arguments({ Argument.RANDOM_EACH, Argument.RANDOM_EACH, Argument.RANDOM_EACH, Argument.RANDOM_EACH })\n+    @IR(failOn = { IRNode.ALLOC })\n+    int testConsecutiveSimpleMerge(boolean cond1, boolean cond2, int x, int y) {\n+        Point p0 = new Point(x, x);\n+        Point p1 = new Point(x, y);\n+        Point pA = null;\n+\n+        Point p2 = new Point(y, x);\n+        Point p3 = new Point(y, y);\n+        Point pB = null;\n+\n+        if (cond1)\n+            pA = p0;\n+        else\n+            pA = p1;\n+\n+        if (cond2)\n+            pB = p2;\n+        else\n+            pB = p3;\n+\n+        return pA.x * pA.y + pB.x * pB.y;\n+    }\n+\n+    @Test\n+    @Arguments({ Argument.RANDOM_EACH, Argument.RANDOM_EACH, Argument.RANDOM_EACH })\n+    @IR(failOn = { IRNode.ALLOC })\n+    int testDoubleIfElseMerge(boolean cond, int x, int y) {\n+        Point p1 = new Point(x, y);\n+        Point p2 = new Point(x+1, y+1);\n+\n+        if (cond) {\n+            p1 = new Point(y, x);\n+            p2 = new Point(y, x);\n+        }\n+        else {\n+            p1 = new Point(x, y);\n+            p2 = new Point(x+1, y+1);\n+        }\n+\n+        return p1.x * p2.y;\n+    }\n+\n+\/\/\/ is asserting\n+\/\/\/\/\/    @Test\n+\/\/\/\/\/    @Arguments({ Argument.RANDOM_EACH, Argument.RANDOM_EACH, Argument.RANDOM_EACH })\n+\/\/\/\/\/    @IR(failOn = { IRNode.ALLOC })\n+\/\/\/\/\/    int testNoEscapeWithLoadInLoop(boolean cond, int x, int y) {\n+\/\/\/\/\/        Point p = new Point(x, y);\n+\/\/\/\/\/        int res = 0;\n+\/\/\/\/\/\n+\/\/\/\/\/        if (cond)\n+\/\/\/\/\/            p = new Point(y, x);\n+\/\/\/\/\/\n+\/\/\/\/\/        for (int i=x; i<y; i++) {\n+\/\/\/\/\/            res += p.x + p.y + i;\n+\/\/\/\/\/        }\n+\/\/\/\/\/\n+\/\/\/\/\/        return res + p.x + p.y;\n+\/\/\/\/\/    }\n+\n+\n+    @Test\n+    @Arguments({ Argument.RANDOM_EACH, Argument.RANDOM_EACH, Argument.RANDOM_EACH, Argument.RANDOM_EACH })\n+    @IR(failOn = { IRNode.ALLOC })\n+    int testCmpAfterMerge(boolean cond, boolean cond2, int x, int y) {\n+        Point a = new Point(x, y);\n+        Point b = new Point(y, x);\n+        Point c = null;\n+\n+        if (x+2 >= y-5)\n+            c = a;\n+        else\n+            c = b;\n+\n+        return cond2 ? c.x : c.y;\n+    }\n+\n+    @Test\n+    @Arguments({ Argument.RANDOM_EACH, Argument.RANDOM_EACH, Argument.RANDOM_EACH })\n+    @IR(failOn = { IRNode.ALLOC })\n+    int testCmpMergeWithNull(boolean cond, int x, int y) {\n+        Point p = null;\n+\n+        if (cond)\n+            p = new Point(x*x, y*y);\n+        else if (x == y)\n+            p = new Point(x+y, x*y);\n+\n+        if (p != null)\n+            return p.x * p.y;\n+        else\n+            return 1984;\n+    }\n+\n+    @Test\n+    @Arguments({ Argument.RANDOM_EACH, Argument.RANDOM_EACH, Argument.RANDOM_EACH, Argument.RANDOM_EACH })\n+    @IR(failOn = { IRNode.ALLOC })\n+    int testCondAfterMergeWithAllocate(boolean cond1, boolean cond2, int x, int y) {\n+        Point p = new Point(x, y);\n+\n+        if (cond1)\n+            p = new Point(y, x);\n+\n+        if (cond2 && cond1) {\n+            return p.x;\n+        }\n+        else {\n+            return 321;\n+        }\n+    }\n+\n+    @Test\n+    @Arguments({ Argument.RANDOM_EACH, Argument.RANDOM_EACH, Argument.RANDOM_EACH, Argument.RANDOM_EACH })\n+    @IR(failOn = { IRNode.ALLOC })\n+    int testCondLoadAfterMerge(boolean cond1, boolean cond2, int x, int y) {\n+        Point p = new Point(x, y);\n+\n+        if (cond1)\n+            p = new Point(y, x);\n+\n+        if (cond1 == false && cond2 == false)\n+            return p.x + 1;\n+        else if (cond1 == false && cond2 == true)\n+            return p.x + 30;\n+        else if (cond1 == true && cond2 == false)\n+            return p.x + 40;\n+        else if (cond1 == true && cond2 == true)\n+            return p.x + 50;\n+        else\n+            return -1;\n+    }\n+\n+    @Test\n+    @IR(failOn = { IRNode.ALLOC })\n+    int testIfElseInLoop() {\n+        int res = 0;\n+\n+        for (int i = 1; i < 1000; i++) {\n+            Point obj = new Point(i, i);\n+\n+            if (i % 2 == 1)\n+                obj = new Point(i, i+1);\n+            else\n+                obj = new Point(i-1, i);\n+\n+            res += obj.x;\n+        }\n+\n+        return res;\n+    }\n+\n+    @Test\n+    @Arguments({ Argument.RANDOM_EACH, Argument.RANDOM_EACH, Argument.RANDOM_EACH })\n+    @IR(failOn = { IRNode.ALLOC })\n+    int testLoadInCondAfterMerge(boolean cond, int x, int y) {\n+        Point p = new Point(x, y);\n+\n+        if (cond)\n+            p = new Point(y, x);\n+\n+        if (p.x == 10) {\n+            if (p.y == 10) {\n+                return dummy(10);\n+            }\n+            else {\n+                return dummy(20);\n+            }\n+        }\n+        else if (p.x == 20) {\n+            if (p.y == 20) {\n+                return dummy(30);\n+            }\n+            else {\n+                return dummy(40);\n+            }\n+        }\n+\n+        return 1984;\n+    }\n+\n+    @Test\n+    @Arguments({ Argument.RANDOM_EACH, Argument.RANDOM_EACH, Argument.RANDOM_EACH })\n+    @IR(failOn = { IRNode.ALLOC })\n+    int testLoadInLoop(boolean cond, int x, int y) {\n+        Point obj1 = new Point(x, y);\n+        Point obj2 = new Point(y, x);\n+        Point obj = null;\n+        int res = 0;\n+\n+        if (cond)\n+            obj = obj1;\n+        else\n+            obj = obj2;\n+\n+        for (int i = 0; i < 532; i++) {\n+            res += obj.x;\n+        }\n+\n+        return res;\n+    }\n+\n+    @Test\n+    @Arguments({ Argument.RANDOM_EACH, Argument.RANDOM_EACH, Argument.RANDOM_EACH })\n+    @IR(counts = { IRNode.ALLOC, \"1\" })\n+    \/\/ p2 is ArgEscape\n+    \/\/ p1 can be scalar replaced\n+    int testMergesAndMixedEscape(boolean cond, int x, int y) {\n+        Point p1 = new Point(x, y);\n+        Point p2 = new Point(x, y);\n+        int val  = 0;\n+\n+        if (cond) {\n+            p1 = new Point(x+1, y+1);\n+            val = dummy(p2);\n+        }\n+\n+        return val + p1.x + p2.y;\n+    }\n+\n+    @Test\n+    @Arguments({ Argument.RANDOM_EACH, Argument.RANDOM_EACH, Argument.RANDOM_EACH, Argument.RANDOM_EACH })\n+    @IR(failOn = { IRNode.ALLOC })\n+    int testPartialPhis(boolean cond, int l, int x, int y) {\n+        int k = l;\n+\n+        if (l == 0) {\n+            k = l + 1;\n+        }\n+        else if (l == 2) {\n+            k = l + 2;\n+        }\n+        else if (l == 3) {\n+            new Point(x, y);\n+        }\n+        else if (l == 4) {\n+            new Point(y, x);\n+        }\n+\n+        return k;\n+    }\n+\n+    @Test\n+    @Arguments({ Argument.RANDOM_EACH, Argument.RANDOM_EACH })\n+    @IR(failOn = { IRNode.ALLOC })\n+    int testPollutedNoWrite(boolean cond, int l) {\n+        Shape obj1 = new Square(l);\n+        Shape obj2 = new Square(l);\n+        Shape obj = null;\n+        int res = 0;\n+\n+        if (cond)\n+            obj = obj1;\n+        else\n+            obj = obj2;\n+\n+        for (int i = 1; i < 132; i++) {\n+            res += obj.x;\n+        }\n+\n+        return res + obj1.x + obj2.y;\n+    }\n+\n+    @Test\n+    @Arguments({ Argument.RANDOM_EACH, Argument.RANDOM_EACH, Argument.RANDOM_EACH })\n+    @IR(failOn = { IRNode.ALLOC })\n+    int testThreeWayAliasedAlloc(boolean cond, int x, int y) {\n+        Point p1 = new Point(x, y);\n+        Point p2 = new Point(x+1, y+1);\n+        Point p3 = new Point(x+2, y+2);\n+\n+        if (cond)\n+            p3 = p1;\n+        else\n+            p3 = p2;\n+\n+        return p3.x + p3.y;\n+    }\n+\n+    @Test\n+    @Arguments({ Argument.RANDOM_EACH, Argument.RANDOM_EACH, Argument.RANDOM_EACH })\n+    @IR(failOn = { IRNode.ALLOC })\n+    int TestTrapAfterMerge(boolean cond, int x, int y) {\n+        Point p = new Point(x, x);\n+\n+        if (cond)\n+            p = new Point(y, y);\n+\n+        for (int i=402; i<432; i+=x) {\n+            x++;\n+        }\n+\n+        return p.x + x;\n+    }\n+\n+    \/\/ ------------------ Utility for Testing ------------------- \/\/\n+\n+    @DontCompile\n+    static int dummy(Point p) {\n+        return p.x * p.y;\n+    }\n+\n+    @DontCompile\n+    static int dummy(int x) {\n+        return x;\n+    }\n+\n+    static Point dummy(int x, int y) {\n+        return new Point(x, y);\n+    }\n+\n+    private static Point global_escape;\n+\n+    static class Point {\n+        int x, y;\n+        Point(int x, int y) { this.x = x; this.y = y; }\n+    }\n+\n+    class Shape {\n+        int x, y, l;\n+        Shape(int x, int y) { this.x = x; this.y = y; }\n+    }\n+\n+    class Square extends Shape {\n+        Square(int l) {\n+            super(0, 0);\n+            this.l = l;\n+        }\n+    }\n+\n+    class Circle extends Shape {\n+        Circle(int l) {\n+            super(0, 0);\n+            this.l = l;\n+        }\n+    }\n+}\n","filename":"test\/hotspot\/jtreg\/compiler\/c2\/irTests\/scalarReplacement\/AllocationMergesTests.java","additions":719,"deletions":0,"binary":false,"changes":719,"status":"added"}]}