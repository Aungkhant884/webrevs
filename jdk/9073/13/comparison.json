{"files":[{"patch":"@@ -96,1 +96,1 @@\n-\/\/ Usage:  for( DictI i(dict); i.test(); ++i ) { body = i.key; body = i.value;}\n+\/\/ Usage:  for( DictI i(dict); i.test(); ++i ) { body = i._key; body = i._value;}\n","filename":"src\/hotspot\/share\/libadt\/dict.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -478,0 +478,6 @@\n+  product(bool, ReduceAllocationMerges, true,                               \\\n+          \"Try to simplify allocation merges before Scalar Replacement\")    \\\n+                                                                            \\\n+  develop(bool, TraceReduceAllocationMerges, false,                         \\\n+          \"Trace decision for simplifying allocation merges.\")              \\\n+                                                                            \\\n","filename":"src\/hotspot\/share\/opto\/c2_globals.hpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -55,0 +55,3 @@\n+const char* C2Compiler::retry_no_reduce_allocation_merges() {\n+  return \"retry without trying to reduce allocation merges\";\n+}\n@@ -107,0 +110,1 @@\n+  bool do_reduce_allocation_merges = ReduceAllocationMerges;\n@@ -111,2 +115,3 @@\n-    \/\/ Attempt to compile while subsuming loads into machine instructions.\n-    Options options(subsume_loads, do_escape_analysis, do_iterative_escape_analysis, eliminate_boxing, do_locks_coarsening, install_code);\n+    Options options(subsume_loads, do_escape_analysis, do_iterative_escape_analysis,\n+                    do_reduce_allocation_merges, eliminate_boxing, do_locks_coarsening,\n+                    install_code);\n@@ -139,0 +144,6 @@\n+      if (C.failure_reason_is(retry_no_reduce_allocation_merges())) {\n+        assert(do_reduce_allocation_merges, \"must make progress\");\n+        do_reduce_allocation_merges = false;\n+        env->report_failure(C.failure_reason());\n+        continue;  \/\/ retry\n+      }\n","filename":"src\/hotspot\/share\/opto\/c2compiler.cpp","additions":13,"deletions":2,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -53,0 +53,1 @@\n+  static const char* retry_no_reduce_allocation_merges();\n","filename":"src\/hotspot\/share\/opto\/c2compiler.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -1106,0 +1106,6 @@\n+\/\/----------------------------is_uncommon_trap----------------------------\n+\/\/ Returns true if this is an uncommon trap.\n+bool CallStaticJavaNode::is_uncommon_trap() const {\n+  return (_name != NULL && !strcmp(_name, \"uncommon_trap\"));\n+}\n+\n@@ -1109,4 +1115,1 @@\n-  if (_name != NULL && !strcmp(_name, \"uncommon_trap\")) {\n-    return extract_uncommon_trap_request(this);\n-  }\n-  return 0;\n+  return is_uncommon_trap() ? extract_uncommon_trap_request(this) : 0;\n@@ -1478,1 +1481,2 @@\n-      && !(alloc->Opcode() == Op_VectorBox)) {\n+      && !(alloc->Opcode() == Op_VectorBox)\n+      && !alloc->is_ReducedAllocationMerge()) {\n@@ -1480,1 +1484,1 @@\n-    assert(false, \"unexpected call node\");\n+    assert(false, \"unexpected node.\");\n@@ -1621,0 +1625,271 @@\n+\/\/=============================================================================\n+ReducedAllocationMergeNode::ReducedAllocationMergeNode(Compile* C, PhaseIterGVN* igvn, const ConnectionGraph* cg, const PhiNode* phi)\n+    : TypeNode(phi->type(), phi->req()) {\n+\n+  init_class_id(Class_ReducedAllocationMerge);\n+  init_flags(Flag_is_macro);\n+\n+  const Type* ram_t       = igvn->type(phi);\n+\n+  _number_of_bases        = phi->req()-1;\n+  _fields_and_memories    = new (C->comp_arena()) Dict(cmpkey, hashkey, C->comp_arena());\n+  _klass                  = ram_t->make_oopptr()->is_instptr()->instance_klass();\n+\n+  init_req(0, phi->in(0));\n+  for (uint i = 1; i < phi->req(); i++) {\n+    Node* input = phi->in(i);\n+    PointsToNode* ptn = cg->unique_java_object(input);\n+\n+    \/\/ Source of allocation may not be in CG or may point to multiple Java objects\n+    if (ptn != NULL) {\n+      Node* may_be_allocate = ptn->ideal_node();\n+      \/\/ The source might be a node that is not scalar replaceable\n+      if (may_be_allocate->Opcode() == Op_Allocate && ptn->scalar_replaceable()) {\n+        input = may_be_allocate->as_Allocate()->result_cast();\n+        assert(input->is_CheckCastPP(), \"input to phi is not checkcastpp\");\n+      }\n+    }\n+\n+    init_req(i, input);\n+  }\n+\n+  initialize_memory_edges(C, igvn);\n+\n+  this->raise_bottom_type(ram_t);\n+  igvn->set_type(this, ram_t);\n+\n+  C->add_macro_node(this);\n+}\n+\n+void ReducedAllocationMergeNode::initialize_memory_edges(Compile* C, PhaseIterGVN* igvn) {\n+  Node* region            = this->in(0);\n+  ciInstanceKlass* iklass = _klass->as_instance_klass();\n+  int nfields             = iklass->nof_nonstatic_fields();\n+\n+  \/\/ Make sure we have an entry for each base+field combination\n+  register_offset_of_all_fields(NULL);\n+\n+  \/\/ Search for a memory edge matching base+field alias_index\n+  for (uint i = 1; i <= _number_of_bases; i++) {\n+    Node* base = this->in(i);\n+    const TypeOopPtr *base_t = igvn->type(base)->isa_oopptr();\n+\n+    if (base_t != NULL) {\n+      for (int j = 0; j < nfields; j++) {\n+        ciField* field          = iklass->nonstatic_field_at(j);\n+        int offset              = field->offset();\n+        const TypeOopPtr *tinst = base_t->add_offset(offset)->isa_oopptr();\n+        const int alias_idx     = C->get_alias_index(tinst);\n+        const int fields_offset = field_idx(offset);\n+\n+        for (DUIterator_Fast imax, i = region->fast_outs(imax); i < imax; i++) {\n+          Node* memory = region->fast_out(i);\n+          if (memory->is_Phi() && C->get_alias_index(memory->adr_type()) == alias_idx) {\n+            int base_matches = 0;\n+            int base_offset = base_idx(base, base_matches);\n+\n+            \/\/ We need to check if the same base is used multiple times in the Phi\n+            do {\n+              set_req(fields_offset + base_offset, memory);\n+              base_matches++;\n+              base_offset = base_idx(base, base_matches);\n+            } while (base_offset != -1);\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  \/\/ Try to find a BOT memory Phi coming from same region\n+  for (DUIterator_Fast imax, i = region->fast_outs(imax); i < imax; i++) {\n+    Node* n = region->fast_out(i);\n+    if (n->is_Phi() && n->bottom_type() == Type::MEMORY) {\n+      if (C->get_alias_index(n->adr_type()) == Compile::AliasIdxBot) {\n+        register_offset_of_all_fields(n);\n+        break;\n+      }\n+    }\n+  }\n+}\n+\n+void ReducedAllocationMergeNode::register_offset_of_all_fields(Node* memory) {\n+  ciInstanceKlass* iklass = _klass->as_instance_klass();\n+  int nfields             = iklass->nof_nonstatic_fields();\n+\n+  for (int j = 0; j < nfields; j++) {\n+    ciField* field = iklass->nonstatic_field_at(j);\n+    int offset = field->offset();\n+\n+    register_offset(offset, memory);\n+  }\n+}\n+\n+void ReducedAllocationMergeNode::register_offset(int offset, Node* memory) {\n+  assert(offset > 0, \"Offset of use should be >= 0.\");\n+\n+  if ((*_fields_and_memories)[(void*)(intptr_t)offset] == NULL) {\n+    _fields_and_memories->Insert((void*)(intptr_t)offset, (void*)(intptr_t)req());\n+\n+    for (uint b_idx = 1; b_idx <= _number_of_bases; ++b_idx) {\n+      add_req( (memory != NULL && memory->is_Phi() && memory->in(0) == in(0)) ?\n+                    memory->in(b_idx) :\n+                    memory);\n+    }\n+  } else {\n+    int fidx = field_idx(offset);\n+\n+    for (uint b_idx = 1; b_idx <= _number_of_bases; ++b_idx) {\n+      if (in(fidx) == NULL) {\n+        set_req(fidx, (memory != NULL && memory->is_Phi() && memory->in(0) == in(0)) ?\n+                        memory->in(b_idx) :\n+                        memory);\n+      }\n+      fidx++;\n+    }\n+  }\n+}\n+\n+void ReducedAllocationMergeNode::register_addp(AddPNode* addp) {\n+  assert(addp->outcnt() > 0 && addp->raw_out(0)->is_Load(), \"AddP output is not load.\");\n+  assert(addp->in(AddPNode::Address) == addp->in(AddPNode::Base), \"AddP base and address aren't the same.\");\n+\n+  int offset = addp->in(AddPNode::Offset)->find_intptr_t_con(-1);\n+  Node* memory = addp->raw_out(0)->in(LoadNode::Memory);\n+  register_offset(offset, memory);\n+}\n+\n+bool ReducedAllocationMergeNode::register_use(Node* n) {\n+  if (n->is_AddP()) {\n+    register_addp(n->as_AddP());\n+  } else if (n->Opcode() == Op_SafePoint || (n->is_CallStaticJava() && n->as_CallStaticJava()->is_uncommon_trap())) {\n+    Node* memory = n->in(TypeFunc::Memory);\n+    register_offset_of_all_fields(memory);\n+  } else if (n->is_DecodeN()) {\n+    for (DUIterator_Fast imax, i = n->fast_outs(imax); i < imax; i++) {\n+      Node* addp = n->fast_out(i);\n+      register_addp(addp->as_AddP());\n+    }\n+  } else {\n+    assert(false, \"Trying to register unsupported use in RAM -> %d : %s\", n->_idx, n->Name());\n+    return false;\n+  }\n+\n+  return true;\n+}\n+\n+Node* ReducedAllocationMergeNode::memory_for(int offset, Node* base, uint previous_matches) const {\n+  int field_start_offset = field_idx(offset);\n+  int base_offset = base_idx(base, previous_matches);\n+\n+  \/\/ Didn't find repeated occurrence of same base?\n+  if (base_offset == -1) {\n+    assert(previous_matches > 0, \"Didn't find any occurrence of base in RAM.\");\n+    return NULL;\n+  }\n+\n+  Node* memory = in(field_start_offset + base_offset);\n+  assert(memory->bottom_type()->base() == Type::Memory, \"memory_for isn't returning a Memory reference.\");\n+\n+  return memory;\n+}\n+\n+void ReducedAllocationMergeNode::register_value_for_field(int offset, Node* base, Node* value, uint previous_matches) {\n+  assert(value != NULL, \"trying to register null pointer as value.\");\n+  int field_start_offset = field_idx(offset);\n+  int base_offset = base_idx(base, previous_matches);\n+\n+  set_req(field_start_offset + base_offset, value);\n+}\n+\n+Node* ReducedAllocationMergeNode::make_load(Node* ctrl, Node* base, Node* mem, int offset, PhaseIterGVN* igvn) {\n+  base                        = base->is_EncodeP() ? base->in(1) : base;\n+  Node* addp                  = igvn->transform(new AddPNode(base, base, igvn->MakeConX(offset)));\n+  const TypePtr* adr_type     = addp->bottom_type()->is_ptr();\n+  const TypeInstPtr* res_type = igvn->type(base)->is_instptr();\n+  ciInstanceKlass* iklass     = res_type->instance_klass();\n+  ciField* field              = iklass->get_field_by_offset(offset, \/*is_static=*\/false);\n+  field                       = field != NULL ? field : iklass->get_field_by_offset(offset, \/*is_static=*\/true);\n+  \/\/ If for some reason we didn't find the field then bail out.\n+  if (field == NULL) return NULL;\n+\n+  ciType* elem_type           = field->type();\n+  BasicType basic_elem_type   = field->layout_type();\n+  const Type* field_type      = NULL;\n+\n+  if (is_reference_type(basic_elem_type)) {\n+    if (!elem_type->is_loaded()) {\n+      field_type = TypeInstPtr::BOTTOM;\n+    } else {\n+      field_type = TypeOopPtr::make_from_klass(elem_type->as_klass());\n+    }\n+\n+    if (UseCompressedOops) {\n+      field_type = field_type->make_narrowoop();\n+      basic_elem_type = T_OBJECT;\n+    }\n+  } else {\n+    field_type = Type::get_const_basic_type(basic_elem_type);\n+  }\n+\n+  \/\/ 'decode_narrow' is set to false because consumers of the return of this\n+  \/\/ method expect a Load to be returned.  The returned Load from this method\n+  \/\/ uses an specific base and it will be used to replace a Load where the\n+  \/\/ [AddP] base is a Phi. If the returned Load is a 'LoadN' then so was the\n+  \/\/ original Load.\n+  Node* load = LoadNode::make(*igvn, ctrl, mem, addp, adr_type, field_type, basic_elem_type, MemNode::unordered,\n+                                LoadNode::DependsOnlyOnTest, false, false, false, false, (uint8_t)0U, \/*decode_narrow*\/false);\n+  return igvn->register_new_node_with_optimizer(load);\n+}\n+\n+Node* ReducedAllocationMergeNode::value_phi_for_field(int field, PhaseIterGVN* igvn) {\n+  PhiNode* phi     = new PhiNode(this->in(0), Type::BOTTOM);\n+  int field_index  = field_idx(field);\n+  const Type *t    = Type::TOP;\n+\n+  for (uint i = 1; i <= _number_of_bases; i++) {\n+    Node* input = in(field_index);\n+\n+    \/\/ If the entry corresponding to the base is not TOP it means the allocation\n+    \/\/ wasn't scalarized. If the allocation wasn't scalarized we still have a\n+    \/\/ memory reference that we can use to find the value to be used in the value phi.\n+    if (!in(i)->is_top()) {\n+      Node* memory = input;\n+      memory = (memory->is_Phi() && memory->in(0) == in(0)) ? memory->in(i) : memory;\n+      input = make_load(this->in(0)->in(i), in(i), memory, field, igvn);\n+      if (input == NULL) return NULL;\n+    } else if (input->bottom_type()->base() == Type::Memory) {\n+      \/\/ Somehow the base was eliminated and we still have a memory reference left\n+      return NULL;\n+    }\n+\n+    \/\/ Simplification for a pattern that showed up often during RAM elimination\n+    if (input->is_Phi() && input->in(0) == in(0)) {\n+      input = input->in(i);\n+    }\n+\n+    phi->set_req(i, input);\n+    const Type* input_type = igvn->type(input);\n+    t = t->meet_speculative(input_type);\n+    field_index++;\n+  }\n+\n+  phi->raise_bottom_type(t);\n+  igvn->register_new_node_with_optimizer(phi);\n+\n+  return phi;\n+}\n+\n+ReducedAllocationMergeNode* ReducedAllocationMergeNode::make(Compile* C, PhaseIterGVN* igvn, const ConnectionGraph* cg, PhiNode* phi) {\n+  ReducedAllocationMergeNode* ram = new ReducedAllocationMergeNode(C, igvn, cg, phi);\n+\n+  for (DUIterator_Fast imax, i = phi->fast_outs(imax); i < imax; i++) {\n+    Node* n = phi->fast_out(i);\n+    if (!ram->register_use(n)) {\n+      return NULL;\n+    }\n+  }\n+\n+  return ram;\n+}\n+\n+\n","filename":"src\/hotspot\/share\/opto\/callnode.cpp","additions":281,"deletions":6,"binary":false,"changes":287,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"opto\/addnode.hpp\"\n@@ -31,0 +32,1 @@\n+#include \"opto\/cfgnode.hpp\"\n@@ -738,0 +740,1 @@\n+  bool is_uncommon_trap() const;\n@@ -1011,0 +1014,144 @@\n+\/\/ This node is used during SR to simplify allocation merges.\n+\/\/ It's in this file just because it's closely related to allocation.\n+\/\/\n+\/\/ Before elimination of macro nodes start, *some* Phi nodes that merge object\n+\/\/ allocations and match certain criteria (see escape.cpp::can_reduce_this_phi)\n+\/\/ are replaced by a ReducedAllocationMergeNode (aka RAM). The users of the\n+\/\/ merged allocation are registered_in the RAM node. During macro node expansion\n+\/\/ scalar replacement, if an allocation is used by a RAM node, the nodes\n+\/\/ producing value for fields registered in the RAM are also registered in the\n+\/\/ RAM node (in association with corresponding allocation base).\n+\/\/\n+\/\/ After the inputs to the RAM node are scalar replaced the RAM node itself is\n+\/\/ scalar replaced. This consist basically in replacing the use(s) of the merged\n+\/\/ allocations field(s) value by a new Phi node merging the value produced in\n+\/\/ the different inputs to the RAM node.  In some cases a reference to the whole\n+\/\/ merged object is needed and we handle that by creating an\n+\/\/ SafePointScalarObjectNode.\n+\/\/\n+\/\/ Please see below for an illustration of how the implementation operates.\n+\/\/\n+\/\/ When a RAM node is created the offset of fields accessed through the RAM is\n+\/\/ used to index into `_fields_and_memories` to store the RAM->in index where\n+\/\/ a memory edge can be used to retrieve the field's value. Note that there\n+\/\/ needs to be one memory edge for each base. Below is an illustration of a\n+\/\/ RAM node that merge two inputs and is used to access fields `f1` and `f2`.\n+\/\/\n+\/\/     0     1      2      3        4        5        6\n+\/\/ RAM(ctrl, base1, base2, f1_b1_m, f1_b2_m, f2_b1_m, f2_b2_m)\n+\/\/           |----------|   |        |       |        |\n+\/\/           |          |   |        |       |        \\-------------------> Memory edge used to search for value of b2.f2\n+\/\/           |          |   |        |       \\----------------------------> Memory edge used to search for value of b1.f2\n+\/\/           |          |   |        |\n+\/\/           |          |   |        \\------------------------------------> Memory edge used to search for value of b2.f1\n+\/\/           |          |   \\---------------------------------------------> Memory edge used to search for value of b1.f1\n+\/\/           |          |\n+\/\/           \\\\\\\\\\\\\\\\\\\\\\\\-------------------------------------------------> Bases pointing to a possible scalar replaceable object\n+\/\/\n+\/\/ This would be how the `_fields_and_memories` would look like when the RAM\n+\/\/ is created:\n+\/\/\n+\/\/  _fields_and_memories[f1.offset] -> 3\n+\/\/  _fields_and_memories[f2.offset] -> 5\n+\/\/\n+\/\/ Which means that RAM->in[3 + i] we have the memory edges to search for the\n+\/\/ value of field f1 for base i. In RAM->in[5 + i] we have the memory edges\n+\/\/ to search for the value of field f2 for base i.\n+\/\/\n+\/\/ After an allocation is scalar replaced the memory edges references are\n+\/\/ replaced by the actual value of the field found during scalar replacement.\n+\/\/ Suppose only the first base was scalar replaced, the inputs to RAM would\n+\/\/ look like this:\n+\/\/\n+\/\/     0     1    2      3        4        5        6\n+\/\/ RAM(ctrl, TOP, base2, f1_b1_V, f1_b2_m, f2_b1_V, f2_b2_m)\n+\/\/           |    |      |        |        |        |\n+\/\/           |    |      |        |        |        \\-------------------> Memory edge used to search for value of b2.f2\n+\/\/           |    |      |        |        \\----------------------------> VALUE for b1.f2\n+\/\/           |    |      |        |\n+\/\/           |    |      |        \\-------------------------------------> Memory edge used to search for value of b2.f1\n+\/\/           |    |      \\----------------------------------------------> VALUE for b1.f1\n+\/\/           |    |\n+\/\/           |    \\-----------------------------------------------------> This base wasn't scalar replaced.\n+\/\/           |\n+\/\/           \\----------------------------------------------------------> This base was scalar replaced.\n+\/\/\n+\/\/ During RAM node removal (see macro.cpp) \"value\" Phis are used to merge\n+\/\/ the possible values for each field. If a field reference is still pointing\n+\/\/ to a Memory edge then a Load is created to load the value from there.\n+\/\/ The illustration above would produce the following resulting graph.\n+\/\/\n+\/\/  f1_b1_V              AddP       f2_b1_V              AddP\n+\/\/   \\          f1_b2_m  \/           \\          f2_b2_m  \/\n+\/\/    \\           |     \/             \\           |     \/\n+\/\/     \\          |    \/               \\          |    \/\n+\/\/      \\         Load                  \\         Load\n+\/\/       \\       \/                       \\       \/\n+\/\/        \\     \/                         \\     \/\n+\/\/          Phi                             Phi\n+\/\/           |                               |\n+\/\/           v                               v\n+\/\/     {use1, use2, ...}               {use1, use2, ...}\n+\/\/\n+\/\/\n+\/\/ There should be no RAM node in the graph after macro nodes are eliminated.\n+\/\/\n+class ReducedAllocationMergeNode : public TypeNode {\n+private:\n+  ciKlass* _klass;                  \/\/ Which Klass is the merge for\n+\n+  uint _number_of_bases;            \/\/ Number of bases to the original Phi\n+\n+  Dict* _fields_and_memories;\n+\n+public:\n+  ReducedAllocationMergeNode(Compile* C, PhaseIterGVN* igvn, const ConnectionGraph* cg, const PhiNode* phi) ;\n+\n+  virtual int Opcode() const;\n+\n+  ciKlass* klass() const { return _klass; }\n+\n+  uint number_of_bases() const { return _number_of_bases; }\n+\n+  const DictI needed_offsets() const { return DictI(_fields_and_memories); }\n+\n+  int field_idx(int offset) const {\n+    assert(offset > 0, \"Offset should be positive.\");\n+    return (intptr_t) (*_fields_and_memories)[(void*)(intptr_t)offset];\n+  }\n+\n+  int base_idx(Node* base, uint previous_matches) const {\n+    assert(base != NULL, \"Base shouldn't be NULL.\");\n+    for (uint i = 1, matches = 0; i <= _number_of_bases; i++) {\n+      if (base == in(i)) {\n+        matches++;\n+        if (matches > previous_matches) {\n+          return i-1;\n+        }\n+      }\n+    }\n+\n+    return -1;\n+  }\n+\n+  bool needs_field(int offset) const {\n+    return (*_fields_and_memories)[(void*)(intptr_t)offset] != NULL;\n+  }\n+\n+  void initialize_memory_edges(Compile* C, PhaseIterGVN* igvn);\n+  void register_addp(AddPNode* addp);\n+  void register_offset_of_all_fields(Node* memory);\n+  void register_offset(int offset, Node* memory);\n+  bool register_use(Node* n);\n+\n+  Node* memory_for(int field, Node* base, uint previous_matches) const;\n+\n+  void register_value_for_field(int field, Node* base, Node* value, uint previous_matches) ;\n+\n+  Node* make_load(Node* ctrl, Node* base, Node* mem, int offset, PhaseIterGVN* igvn);\n+  Node* value_phi_for_field(int field, PhaseIterGVN* igvn) ;\n+\n+  static ReducedAllocationMergeNode* make(Compile* C, PhaseIterGVN* igvn, const ConnectionGraph* cg, PhiNode* phi) ;\n+};\n+\n+\n","filename":"src\/hotspot\/share\/opto\/callnode.hpp","additions":147,"deletions":0,"binary":false,"changes":147,"status":"modified"},{"patch":"@@ -129,2 +129,3 @@\n-\/\/ Helper function: Return any PhiNode that uses this region or NULL\n-PhiNode* RegionNode::has_phi() const {\n+\/\/ Helper function: Return true if there is any PhiNode or ReducedAllocationMerge\n+\/\/ using this region node.\n+bool RegionNode::has_phi() const {\n@@ -133,1 +134,1 @@\n-    if (phi->is_Phi()) {   \/\/ Check for Phi users\n+    if (phi->is_Phi() || phi->is_ReducedAllocationMerge()) {\n@@ -135,1 +136,1 @@\n-      return phi->as_Phi();  \/\/ this one is good enough\n+      return true;\n@@ -139,1 +140,1 @@\n-  return NULL;\n+  return false;\n@@ -492,1 +493,1 @@\n-    has_phis = (has_phi() != NULL);       \/\/ Cache result\n+    has_phis = has_phi();       \/\/ Cache result\n","filename":"src\/hotspot\/share\/opto\/cfgnode.cpp","additions":7,"deletions":6,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -90,1 +90,1 @@\n-  PhiNode* has_phi() const;        \/\/ returns an arbitrary phi user, or NULL\n+  bool has_phi() const;        \/\/ returns true if there is a Phi or RAM use of this region\n","filename":"src\/hotspot\/share\/opto\/cfgnode.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -312,0 +312,1 @@\n+macro(ReducedAllocationMerge)\n","filename":"src\/hotspot\/share\/opto\/classes.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -2292,0 +2292,1 @@\n+\n@@ -2310,1 +2311,1 @@\n-        igvn.set_delay_transform(false);\n+        if (failing())  return;\n@@ -2312,0 +2313,1 @@\n+        igvn.set_delay_transform(false);\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -180,0 +180,1 @@\n+  const bool _do_reduce_allocation_merges;   \/\/ Do try to reduce allocation merges.\n@@ -186,0 +187,1 @@\n+          bool do_reduce_allocation_merges,\n@@ -191,0 +193,1 @@\n+          _do_reduce_allocation_merges(do_reduce_allocation_merges),\n@@ -201,0 +204,1 @@\n+       \/* do_reduce_allocation_merges = *\/ false,\n@@ -548,0 +552,1 @@\n+  bool              do_reduce_allocation_merges() const  { return _options._do_reduce_allocation_merges; }\n","filename":"src\/hotspot\/share\/opto\/compile.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -281,1 +281,12 @@\n-  \/\/ 3. Adjust scalar_replaceable state of nonescaping objects and push\n+  \/\/ 3. Merge some object allocations into a ReduceAllocationMerge node\n+  int number_of_reduced_allocations = 0;\n+  if (ReduceAllocationMerges && C->do_reduce_allocation_merges()) {\n+    C->print_method(PHASE_BEFORE_REDUCE_ALLOCATION, 2);\n+    number_of_reduced_allocations = reduce_allocation_merges();\n+    if (C->failing()) {\n+      return false;\n+    }\n+    C->print_method(PHASE_AFTER_REDUCE_ALLOCATION, 2);\n+  }\n+\n+  \/\/ 4. Adjust scalar_replaceable state of nonescaping objects and push\n@@ -322,1 +333,1 @@\n-  assert(C->unique() == nodes_size(), \"no new ideal nodes should be added during ConnectionGraph build\");\n+  assert((C->unique() - number_of_reduced_allocations) == nodes_size(), \"no new ideal nodes should be added during ConnectionGraph build\");\n@@ -331,1 +342,1 @@\n-  } \/\/ TracePhase t3(\"connectionGraph\")\n+  } \/\/ TracePhase tp(\"connectionGraph\")\n@@ -333,1 +344,1 @@\n-  \/\/ 4. Optimize ideal graph based on EA information.\n+  \/\/ 5. Optimize ideal graph based on EA information.\n@@ -356,1 +367,1 @@\n-  \/\/ 5. Separate memory graph for scalar replaceable allcations.\n+  \/\/ 5. Separate memory graph for scalar replaceable allocations.\n@@ -404,0 +415,324 @@\n+int ConnectionGraph::reduce_allocation_merges() {\n+  Unique_Node_List ideal_nodes;\n+  ideal_nodes.map(_compile->live_nodes(), NULL);\n+  ideal_nodes.push(_compile->root());\n+\n+  bool prev_delay_transform = _igvn->delay_transform();\n+  int number_of_reductions = 0;\n+  _igvn->set_delay_transform(true);\n+\n+  for (uint next = 0; next < ideal_nodes.size(); ++next) {\n+    Node* candidate_region = ideal_nodes.at(next);\n+\n+    if (candidate_region->is_Region()) {\n+      Unique_Node_List target_phis;\n+\n+      for (DUIterator_Fast imax, i = candidate_region->fast_outs(imax); i < imax; i++) {\n+        Node* candidate_phi = candidate_region->fast_out(i);\n+\n+        \/\/ Performs several checks to see if we can\/should reduce this Phi\n+        if (candidate_phi->is_Phi() && can_reduce_this_phi(candidate_phi)) {\n+          target_phis.push(candidate_phi);\n+        }\n+      }\n+\n+      for (uint target_phi_idx = 0; target_phi_idx < target_phis.size(); ++target_phi_idx) {\n+        Node* target_phi = target_phis.at(target_phi_idx);\n+        PointsToNode* target_phi_ptn = ptnode_adr(target_phi->_idx);\n+\n+        if (reduce_this_phi(target_phi->as_Phi())) {\n+          remove_phi_node(target_phi_ptn);\n+          number_of_reductions++;\n+        } else {\n+          assert(false, \"Failed to create ReducedAllocationMerge\");\n+          _compile->record_failure(C2Compiler::retry_no_reduce_allocation_merges());\n+          return -1;\n+        }\n+      }\n+    }\n+\n+    for (DUIterator_Fast imax, i = candidate_region->fast_outs(imax); i < imax; i++) {\n+      Node* m = candidate_region->fast_out(i);\n+      ideal_nodes.push(m);\n+    }\n+  }\n+\n+  _igvn->set_delay_transform(prev_delay_transform);\n+\n+  return number_of_reductions;\n+}\n+\n+const Node* ConnectionGraph::come_from_allocate(const Node* n) const {\n+  int max_iterations = 100;\n+  while (--max_iterations > 0) {\n+    switch (n->Opcode()) {\n+      case Op_CastPP:\n+      case Op_CheckCastPP:\n+      case Op_EncodeP:\n+      case Op_EncodePKlass:\n+      case Op_DecodeN:\n+      case Op_DecodeNKlass:\n+        n = n->in(1);\n+        break;\n+      case Op_Proj:\n+        n = n->in(0);\n+        break;\n+      case Op_Parm:\n+      case Op_GetAndSetN:\n+      case Op_GetAndSetP:\n+      case Op_LoadP:\n+      case Op_LoadN:\n+      case Op_LoadKlass:\n+      case Op_LoadNKlass:\n+      SHENANDOAHGC_ONLY(case Op_ShenandoahLoadReferenceBarrier:)\n+      SHENANDOAHGC_ONLY(case Op_ShenandoahIUBarrier:)\n+      case Op_ConP:\n+      case Op_ConN:\n+      case Op_CreateEx:\n+      case Op_AllocateArray:\n+      case Op_Phi:\n+        return NULL;\n+      case Op_Allocate:\n+        return n;\n+      default:\n+        if (n->is_Call()) {\n+          return NULL;\n+        }\n+        assert(false, \"Should not reach here. Unmatched %d %s\", n->_idx, n->Name());\n+        return NULL;\n+    }\n+  }\n+\n+  return NULL;\n+}\n+\n+bool ConnectionGraph::is_read_only(Node* merge_phi_region, Node* base) const {\n+  Unique_Node_List worklist;\n+  worklist.push(base);\n+\n+  for (uint next = 0; next < worklist.size(); ++next) {\n+    Node* n = worklist.at(next);\n+\n+    for (DUIterator_Fast imax, i = n->fast_outs(imax); i < imax; i++) {\n+      Node* m = n->fast_out(i);\n+\n+      switch (m->Opcode()) {\n+        case Op_CastPP:\n+        case Op_CheckCastPP:\n+        case Op_EncodeP:\n+        case Op_EncodePKlass:\n+        case Op_DecodeN:\n+        case Op_DecodeNKlass:\n+          worklist.push(m);\n+          break;\n+      }\n+\n+      if (m->is_AddP()) {\n+        for (DUIterator_Fast imax, i = m->fast_outs(imax); i < imax; i++) {\n+          Node* child = m->fast_out(i);\n+\n+          if (child->is_Store()) {\n+            assert(child->in(0) != NULL || m->in(0) != NULL, \"No control for store or AddP.\");\n+            if (_igvn->is_dominator(merge_phi_region, child->in(0) != NULL ? child->in(0) : m->in(0))) {\n+              return false;\n+            }\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  return true;\n+}\n+\n+\/\/ Validate outputs:\n+\/\/    Check if we can in fact later replace the uses of the\n+\/\/    current Phi by Phi's of individual fields.\n+\/\/    Conditions checked:\n+\/\/       - The only consumers of the Phi are:\n+\/\/           - AddP (with constant offset) -> Load\n+\/\/           - Safepoint\n+\/\/           - uncommon_trap\n+\/\/           - DecodeN -> AddP (with constant offset) -> Load\n+bool ConnectionGraph::can_reduce_this_phi_users(const Node* phi, bool& has_call_as_user) const {\n+  for (DUIterator_Fast imax, i = phi->fast_outs(imax); i < imax; i++) {\n+    Node* use = phi->fast_out(i);\n+\n+    if (use->is_CallStaticJava() || use->Opcode() == Op_SafePoint) {\n+      has_call_as_user = true;\n+\n+      if (use->is_CallStaticJava() && use->as_CallStaticJava()->is_uncommon_trap() == false) {\n+        NOT_PRODUCT(if (TraceReduceAllocationMerges) tty->print_cr(\"Will NOT try to reduce Phi %d. Has Allocate but cannot scalar replace it. CallStaticJava is not a trap.\", phi->_idx);)\n+        return false;\n+      }\n+    } else if (use->is_AddP()) {\n+      if (use->in(AddPNode::Offset)->find_intptr_t_con(-1) == -1) {\n+        NOT_PRODUCT(if (TraceReduceAllocationMerges) tty->print_cr(\"Will NOT try to reduce Phi %d. Did not find constant input for %d : AddP.\", phi->_idx, use->_idx);)\n+        return false;\n+      }\n+\n+      for (DUIterator_Fast jmax, j = use->fast_outs(jmax); j < jmax; j++) {\n+        Node* use_use = use->fast_out(j);\n+\n+        if (!use_use->is_Load()) {\n+          NOT_PRODUCT(if (TraceReduceAllocationMerges) tty->print_cr(\"Will NOT try to reduce Phi %d. AddP use is not a Load. %d %s\", phi->_idx, use_use->_idx, use_use->Name());)\n+          return false;\n+        }\n+      }\n+    } else if (use->is_DecodeN()) {\n+      for (DUIterator_Fast jmax, j = use->fast_outs(jmax); j < jmax; j++) {\n+        Node* use_use = use->fast_out(j);\n+\n+        if (!use_use->is_AddP() || use_use->in(AddPNode::Offset)->find_intptr_t_con(-1) == -1) {\n+          NOT_PRODUCT(if (TraceReduceAllocationMerges) tty->print_cr(\"Will NOT try to reduce Phi %d. DecodeN use is not a AddP with constant offset. %d %s\", phi->_idx, use_use->_idx, use_use->Name());)\n+          return false;\n+        }\n+\n+        for (DUIterator_Fast kmax, k = use_use->fast_outs(kmax); k < kmax; k++) {\n+          Node* use_use_use = use_use->fast_out(k);\n+\n+          if (!use_use_use->is_Load()) {\n+            NOT_PRODUCT(if (TraceReduceAllocationMerges) tty->print_cr(\"Will NOT try to reduce Phi %d. DecodeN.AddP use is not a Load. %d %s\", phi->_idx, use_use_use->_idx, use_use_use->Name());)\n+            return false;\n+          }\n+        }\n+      }\n+    } else {\n+      NOT_PRODUCT(if (TraceReduceAllocationMerges) tty->print_cr(\"Will NOT try to reduce Phi %d. Has Allocate but cannot scalar replace it. One of the uses is: %d %s\", phi->_idx, use->_idx, use->Name());)\n+      return false;\n+    }\n+  }\n+\n+  return true;\n+}\n+\n+bool ConnectionGraph::can_reduce_this_phi(const Node* phi) const {\n+  if (!is_ideal_node_in_graph(phi->_idx)) return false;\n+  if (ptnode_adr(phi->_idx)->escape_state() != PointsToNode::EscapeState::NoEscape) return false;\n+\n+  const Type* phi_t  = _igvn->type(phi);\n+\n+  \/\/ Found a Memory edge coming from the same Region as the Phi\n+  bool found_memory_edge = false;\n+\n+  \/\/ Is any of the users of the Phi a Call node?\n+  bool has_call_as_user = false;\n+\n+  \/\/ Is any of the Phi inputs Non Scalar Replaceable?\n+  bool has_nonsr_input = false;\n+\n+  \/\/ Ignoring ConP#Null inputs, are all the inputs to the Phi of the same Klass?\n+  bool mixed_klasses = false;\n+\n+\n+  \/\/ If not an InstPtr bail out\n+  if (phi_t == NULL || phi_t->make_oopptr() == NULL || phi_t->make_oopptr()->isa_instptr() == NULL) {\n+    return false;\n+  }\n+\n+  \/\/ Validate inputs:\n+  \/\/    Check whether this Phi node actually point to only scalar replaceable\n+  \/\/    Allocate nodes of the same Klass as the Phi.\n+  \/\/    Also checks that there is no write to any of the inputs after the\n+  \/\/    merge occurs.\n+  bool has_noescape_allocate = false;\n+  ciInstanceKlass* klass = phi_t->make_oopptr()->is_instptr()->instance_klass();\n+  for (uint in_idx = 1; in_idx < phi->req(); in_idx++) {\n+    \/\/ come_from_allocate returns NULL if the source isn't an Allocate\n+    const Node* input = come_from_allocate(phi->in(in_idx));\n+    PointsToNode* input_ptn = input != NULL ? ptnode_adr(input->_idx) : NULL;\n+\n+    \/\/ Check if input comes from scalar replaceable Allocate\n+    bool is_sr_input = (input_ptn != NULL && input_ptn->scalar_replaceable());\n+    has_nonsr_input |= !is_sr_input;\n+    has_noescape_allocate |= is_sr_input;\n+\n+    \/\/ Check if there is no write to the input after it is merged.\n+    \/\/ If there is a write to any input after the merge we need to bail out.\n+    if (!is_read_only(phi->in(0), phi->in(in_idx))) {\n+      NOT_PRODUCT(if (TraceReduceAllocationMerges) tty->print_cr(\"Will NOT try to reduce Phi %d. The %dth input has a store after the merge.\", phi->_idx, in_idx);)\n+      return false;\n+    }\n+\n+    const Type* input_t = _igvn->type(phi->in(in_idx))->make_oopptr();\n+    if (input_t == NULL || input_t->isa_instptr() == NULL) {\n+      NOT_PRODUCT(if (TraceReduceAllocationMerges) tty->print_cr(\"Will NOT try to reduce Phi %d. The %dth input is not an InstPtr.\", phi->_idx, in_idx);)\n+      return false;\n+    }\n+\n+    if (klass != input_t->is_instptr()->instance_klass()) {\n+      mixed_klasses = true;\n+    }\n+  }\n+\n+  \/\/ If there was no input that can be removed then there is\n+  \/\/ no profit doing the reduction of inputs.\n+  if (!has_noescape_allocate) {\n+    NOT_PRODUCT(if (TraceReduceAllocationMerges) tty->print_cr(\"Will NOT try to reduce Phi %d. There is not any NoEscape Allocate as input.\", phi->_idx);)\n+    return false;\n+  }\n+\n+  \/\/ Try to find a BOT memory Phi coming from same region\n+  Node* reg = phi->in(0);\n+  for (DUIterator_Fast imax, i = reg->fast_outs(imax); i < imax; i++) {\n+    Node* n = reg->fast_out(i);\n+    if (n->is_Phi() && n->bottom_type() == Type::MEMORY) {\n+      if (_compile->get_alias_index(n->adr_type()) == Compile::AliasIdxBot) {\n+        found_memory_edge = true;\n+        break;\n+      }\n+    }\n+  }\n+\n+  if (!found_memory_edge && has_nonsr_input) {\n+    NOT_PRODUCT(if (TraceReduceAllocationMerges) tty->print_cr(\"Will NOT try to reduce Phi %d. Did not find memory edge on Region.\", phi->_idx);)\n+    return false;\n+  }\n+\n+  if (can_reduce_this_phi_users(phi, has_call_as_user) == false) {\n+    return false;\n+  }\n+\n+  \/\/ A few more restrictions apply if there is a SafePoint\/Uncommon Trap using the merge.\n+  if (has_call_as_user) {\n+    \/\/ If one of the inputs of the Phi aren't scalar replaceable we need to bail out because\n+    \/\/ current logic for handling deoptimization doesn't handle merges of scalar replaceable\n+    \/\/ allocations mixed with non-scalar replaceable ones.\n+    if (has_nonsr_input) {\n+      NOT_PRODUCT(if (TraceReduceAllocationMerges) tty->print_cr(\"Will NOT try to reduce Phi %d. One of the inputs is not a scalar replaceable Allocate and the original Phi is used by a Call.\", phi->_idx);)\n+      return false;\n+    }\n+\n+    \/\/ If the inputs to the original Phi have different Klass'es and the Phi has a SafePoint\/Uncommon trap as a\n+    \/\/ user we can't solve the merge because we can only have one Klass type when rematerializing the objects.\n+    if (mixed_klasses) {\n+      NOT_PRODUCT(if (TraceReduceAllocationMerges) tty->print_cr(\"Will NOT try to reduce Phi %d. Inputs aren't of the same instance klass.\", phi->_idx);)\n+      return false;\n+    }\n+  }\n+\n+  NOT_PRODUCT(if (TraceReduceAllocationMerges) tty->print_cr(\"Will reduce Phi %d during invocation %d\", phi->_idx, _invocation);)\n+\n+  return true;\n+}\n+\n+bool ConnectionGraph::reduce_this_phi(PhiNode* n) {\n+  Node* ram = ReducedAllocationMergeNode::make(_compile, _igvn, this, n);\n+\n+  if (ram == NULL) {\n+    return false;\n+  }\n+\n+  _igvn->hash_insert(ram);\n+\n+  \/\/ Patch users of 'n' to instead use 'ram'\n+  _igvn->replace_node(n, ram);\n+\n+  \/\/ The original phi now should have no users\n+  _igvn->remove_dead_node(n);\n+\n+  _igvn->_worklist.push(ram);\n+\n+  return true;\n+}\n+\n@@ -607,1 +942,1 @@\n-      \/\/ Produces Null or notNull and is used in only in CmpP so\n+      \/\/ Produces Null or notNull and is used only in CmpP so\n@@ -1086,0 +1421,58 @@\n+\/\/ Remove a Phi node (LocalVar) from the graph and update the edges accordingly.\n+\/\/ Note that users of the Phi node that are - due the Phi - fields of multiple\n+\/\/ Java objects will be disconnected from the JavaObject. Given a graph like this:\n+\/\/\n+\/\/ JavaObject(3) NoEscape(NoEscape) Edges: [ 91F 206F       ] Uses: [ 40 45 183   ]   28  Allocate ...\n+\/\/ JavaObject(4) NoEscape(NoEscape) Edges: [ 172F 206F      ] Uses: [ 119 124 183 ]  107  Allocate ...\n+\/\/ LocalVar(11)  NoEscape(NoEscape) Edges: [ 40 28P         ] Uses: [ 183         ]   45  CheckCastPP ...\n+\/\/ LocalVar(12)  NoEscape(NoEscape) Edges: [ 119 107P       ] Uses: [ 183         ]  124  CheckCastPP ...\n+\/\/ LocalVar(14)  NoEscape(NoEscape) Edges: [ 124 45 28P 107P] Uses: [ 206b        ]  183  Phi ...\n+\/\/\n+\/\/ It will become this:\n+\/\/\n+\/\/ JavaObject(3) NoEscape(NoEscape) Edges: [ 91F ----       ] Uses: [ 40 45 ---   ]   28  Allocate ...\n+\/\/ JavaObject(4) NoEscape(NoEscape) Edges: [ 172F ----      ] Uses: [ 119 124 --- ]  107  Allocate ...\n+\/\/ LocalVar(12)  NoEscape(NoEscape) Edges: [ 119 107P       ] Uses: [ ---         ]  124  CheckCastPP ...\n+\/\/ LocalVar(11)  NoEscape(NoEscape) Edges: [ 40 28P         ] Uses: [ ---         ]   45  CheckCastPP ...\n+void ConnectionGraph::remove_phi_node(PointsToNode* phi_ptn) {\n+  \/\/ First, disconnect all users of Phi (e.g., Fields) from their\n+  \/\/ parents (e.g., Phi, JavaObject\/Allocate)\n+  int uses_count = phi_ptn->use_count();\n+  for (int use_idx=uses_count-1; use_idx>=0; use_idx--) {\n+    PointsToNode* phi_use = phi_ptn->use(use_idx);\n+\n+    if (PointsToNode::is_base_use(phi_use)) {\n+      FieldNode* field = PointsToNode::get_use_node(phi_use)->as_Field();\n+      int base_count = field->base_count();\n+\n+      for (int base_idx=base_count-1; base_idx>=0; base_idx--) {\n+        PointsToNode* field_base = field->base(base_idx);\n+        field_base->remove_edge(field);     \/\/ Remove Base---F--->Field\n+        field->remove_base(field_base);     \/\/ Remove Field---b--->Base\n+      }\n+\n+      phi_ptn->remove_base_use(field); \/\/ Remove Phi---b--->Field\n+    }\n+    else if (phi_use->ideal_node()->is_DecodeN()) {\n+      phi_ptn->remove_use(phi_use);\n+#ifdef ASSERT\n+    } else {\n+      phi_use->ideal_node()->dump();\n+      assert(false, \"Trying to remove Phi from ConnectionGraph that has unsupported user (see dump above).\");\n+#endif\n+    }\n+  }\n+\n+  \/\/ Second, disconnect Phi from all its parents\n+  int edge_count = phi_ptn->edge_count();\n+  for (int edge_idx=edge_count-1; edge_idx>=0; edge_idx--) {\n+    PointsToNode* phi_parent = phi_ptn->edge(edge_idx);\n+\n+    phi_parent->remove_use(phi_ptn);\n+    phi_ptn->remove_edge(phi_parent);\n+  }\n+\n+  \/\/ Remove the Phi node itself from the Connection Graph\n+  _nodes.at_put(phi_ptn->ideal_node()->_idx, NULL);\n+}\n+\n@@ -1690,2 +2083,2 @@\n-      \/\/ Skip Allocate's fields which will be processed later.\n-      if (base->ideal_node()->is_Allocate()) {\n+      \/\/ Skip Allocate's & ReducedAllocationMerge fields which will be processed later.\n+      if (base->ideal_node()->is_Allocate() || base->ideal_node()->is_ReducedAllocationMerge()) {\n@@ -1710,0 +2103,2 @@\n+  \/\/ Do nothing for ReducedAllocationMerges because their fields is \"known\".\n+  \/\/\n@@ -1712,1 +2107,1 @@\n-  if (alloc->is_Allocate() && !pta->arraycopy_dst()) {\n+  if (alloc->is_ReducedAllocationMerge() || (alloc->is_Allocate() && !pta->arraycopy_dst())) {\n@@ -2042,0 +2437,4 @@\n+      \/\/ For now no verification applies to nodes associated with RAM\n+      if (base->is_ReducedAllocationMerge()) {\n+        continue;\n+      }\n@@ -2114,0 +2513,5 @@\n+      \/\/ The CmpP\/N here might be using an allocation merge (Phi).\n+      \/\/ These cases will be handled during macro node elimination.\n+      if (n->in(1)->is_ReducedAllocationMerge() || n->in(2)->is_ReducedAllocationMerge()) {\n+        continue;\n+      }\n@@ -2344,2 +2748,1 @@\n-JavaObjectNode* ConnectionGraph::unique_java_object(Node *n) {\n-  assert(!_collecting, \"should not call when constructed graph\");\n+JavaObjectNode* ConnectionGraph::unique_java_object(Node *n) const {\n@@ -2587,0 +2990,7 @@\n+  \/\/ case #10: RAM as base\n+  \/\/      {...}  ...   {...}\n+  \/\/         \\          \/\n+  \/\/   ReducedAllocationMergeNode\n+  \/\/             ||\n+  \/\/            AddP\n+  \/\/\n@@ -2735,1 +3145,1 @@\n-PhiNode *ConnectionGraph::create_split_phi(PhiNode *orig_phi, int alias_idx, GrowableArray<PhiNode *>  &orig_phi_worklist, bool &new_created) {\n+PhiNode *ConnectionGraph::create_split_phi(PhiNode *orig_phi, int alias_idx, GrowableArray<PhiNode *>  *orig_phi_worklist, bool &new_created) {\n@@ -2771,1 +3181,2 @@\n-  orig_phi_worklist.append_if_missing(orig_phi);\n+  if (orig_phi_worklist != NULL)\n+    orig_phi_worklist->append_if_missing(orig_phi);\n@@ -2786,1 +3197,1 @@\n-PhiNode *ConnectionGraph::split_memory_phi(PhiNode *orig_phi, int alias_idx, GrowableArray<PhiNode *>  &orig_phi_worklist) {\n+PhiNode *ConnectionGraph::split_memory_phi(PhiNode *orig_phi, int alias_idx, GrowableArray<PhiNode *>  *orig_phi_worklist) {\n@@ -2868,1 +3279,1 @@\n-void ConnectionGraph::move_inst_mem(Node* n, GrowableArray<PhiNode *>  &orig_phis) {\n+void ConnectionGraph::move_inst_mem(Node* n, GrowableArray<PhiNode *>  *orig_phis) {\n@@ -2915,0 +3326,2 @@\n+    } else if (use->is_ReducedAllocationMerge()) {\n+      continue;\n@@ -2944,1 +3357,1 @@\n-Node* ConnectionGraph::find_inst_mem(Node *orig_mem, int alias_idx, GrowableArray<PhiNode *>  &orig_phis) {\n+Node* ConnectionGraph::find_inst_mem(Node *orig_mem, int alias_idx, GrowableArray<PhiNode *>  *orig_phis) {\n@@ -3028,1 +3441,2 @@\n-        orig_phis.append_if_missing(result->as_Phi());\n+        if (orig_phis != NULL)\n+          orig_phis->append_if_missing(result->as_Phi());\n@@ -3083,1 +3497,2 @@\n-      orig_phis.append_if_missing(mphi);\n+      if (orig_phis != NULL)\n+        orig_phis->append_if_missing(mphi);\n@@ -3127,6 +3542,6 @@\n-\/\/     7 Parm #memory\n-\/\/    10  ConI  \"12\"\n-\/\/    19  CheckCastPP   \"Foo\"\n-\/\/    20  AddP  _ 19 19 10  Foo+12  alias_index=4\n-\/\/    29  CheckCastPP   \"Foo\"\n-\/\/    30  AddP  _ 29 29 10  Foo+12  alias_index=4\n+\/\/     7  Parm                     Memory\n+\/\/    10  ConI                     \"12\"\n+\/\/    19  CheckCastPP              Foo\n+\/\/    20  AddP     _ 19 19 10      Foo+12      alias_index=4\n+\/\/    29  CheckCastPP              Foo\n+\/\/    30  AddP     _ 29 29 10      Foo+12      alias_index=4\n@@ -3134,7 +3549,7 @@\n-\/\/    40  StoreP  25   7  20   ... alias_index=4\n-\/\/    50  StoreP  35  40  30   ... alias_index=4\n-\/\/    60  StoreP  45  50  20   ... alias_index=4\n-\/\/    70  LoadP    _  60  30   ... alias_index=4\n-\/\/    80  Phi     75  50  60   Memory alias_index=4\n-\/\/    90  LoadP    _  80  30   ... alias_index=4\n-\/\/   100  LoadP    _  80  20   ... alias_index=4\n+\/\/    40  StoreP  25   7  20       ...         alias_index=4\n+\/\/    50  StoreP  35  40  30       ...         alias_index=4\n+\/\/    60  StoreP  45  50  20       ...         alias_index=4\n+\/\/    70  LoadP    _  60  30       ...         alias_index=4\n+\/\/    80  Phi     75  50  60       Memory      alias_index=4\n+\/\/    90  LoadP    _  80  30       ...         alias_index=4\n+\/\/   100  LoadP    _  80  20       ...         alias_index=4\n@@ -3142,0 +3557,2 @@\n+\/\/ Phase 1 creates an instance type for node 29 assigning it an instance id of\n+\/\/ 24 and creating a new alias index for node 30. This gives:\n@@ -3143,2 +3560,6 @@\n-\/\/ Phase 1 creates an instance type for node 29 assigning it an instance id of 24\n-\/\/ and creating a new alias index for node 30.  This gives:\n+\/\/     7  Parm                    Memory\n+\/\/    10  ConI                    \"12\"\n+\/\/    19  CheckCastPP             Foo\n+\/\/    20  AddP     _ 19 19 10     Foo+12       alias_index=4\n+\/\/    29  CheckCastPP             Foo                            iid=24\n+\/\/    30  AddP     _ 29 29 10     Foo+12       alias_index=6     iid=24\n@@ -3146,6 +3567,7 @@\n-\/\/     7 Parm #memory\n-\/\/    10  ConI  \"12\"\n-\/\/    19  CheckCastPP   \"Foo\"\n-\/\/    20  AddP  _ 19 19 10  Foo+12  alias_index=4\n-\/\/    29  CheckCastPP   \"Foo\"  iid=24\n-\/\/    30  AddP  _ 29 29 10  Foo+12  alias_index=6  iid=24\n+\/\/    40  StoreP  25   7  20      ...          alias_index=4\n+\/\/    50  StoreP  35  40  30      ...          alias_index=6\n+\/\/    60  StoreP  45  50  20      ...          alias_index=4\n+\/\/    70  LoadP    _  60  30      ...          alias_index=6\n+\/\/    80  Phi     75  50  60      Memory       alias_index=4\n+\/\/    90  LoadP    _  80  30      ...          alias_index=6\n+\/\/   100  LoadP    _  80  20      ...          alias_index=4\n@@ -3153,7 +3575,4 @@\n-\/\/    40  StoreP  25   7  20   ... alias_index=4\n-\/\/    50  StoreP  35  40  30   ... alias_index=6\n-\/\/    60  StoreP  45  50  20   ... alias_index=4\n-\/\/    70  LoadP    _  60  30   ... alias_index=6\n-\/\/    80  Phi     75  50  60   Memory alias_index=4\n-\/\/    90  LoadP    _  80  30   ... alias_index=6\n-\/\/   100  LoadP    _  80  20   ... alias_index=4\n+\/\/ In phase 2, new memory inputs are computed for the loads and stores, and a\n+\/\/ new version of the phi is created. In phase 4, the inputs to node 80 are\n+\/\/ updated and then the memory nodes are updated with the values computed in\n+\/\/ phase 2. This results in:\n@@ -3161,4 +3580,6 @@\n-\/\/ In phase 2, new memory inputs are computed for the loads and stores,\n-\/\/ And a new version of the phi is created.  In phase 4, the inputs to\n-\/\/ node 80 are updated and then the memory nodes are updated with the\n-\/\/ values computed in phase 2.  This results in:\n+\/\/     7  Parm                    Memory\n+\/\/    10  ConI                    \"12\"\n+\/\/    19  CheckCastPP             Foo\n+\/\/    20  AddP     _ 19 19 10     Foo+12       alias_index=4\n+\/\/    29  CheckCastPP             Foo                            iid=24\n+\/\/    30  AddP     _ 29 29 10     Foo+12       alias_index=6     iid=24\n@@ -3166,15 +3587,8 @@\n-\/\/     7 Parm #memory\n-\/\/    10  ConI  \"12\"\n-\/\/    19  CheckCastPP   \"Foo\"\n-\/\/    20  AddP  _ 19 19 10  Foo+12  alias_index=4\n-\/\/    29  CheckCastPP   \"Foo\"  iid=24\n-\/\/    30  AddP  _ 29 29 10  Foo+12  alias_index=6  iid=24\n-\/\/\n-\/\/    40  StoreP  25  7   20   ... alias_index=4\n-\/\/    50  StoreP  35  7   30   ... alias_index=6\n-\/\/    60  StoreP  45  40  20   ... alias_index=4\n-\/\/    70  LoadP    _  50  30   ... alias_index=6\n-\/\/    80  Phi     75  40  60   Memory alias_index=4\n-\/\/   120  Phi     75  50  50   Memory alias_index=6\n-\/\/    90  LoadP    _ 120  30   ... alias_index=6\n-\/\/   100  LoadP    _  80  20   ... alias_index=4\n+\/\/    40  StoreP  25  7   20      ...          alias_index=4\n+\/\/    50  StoreP  35  7   30      ...          alias_index=6\n+\/\/    60  StoreP  45  40  20      ...          alias_index=4\n+\/\/    70  LoadP    _  50  30      ...          alias_index=6\n+\/\/    80  Phi     75  40  60      Memory       alias_index=4\n+\/\/   120  Phi     75  50  50      Memory       alias_index=6\n+\/\/    90  LoadP    _ 120  30      ...          alias_index=6\n+\/\/   100  LoadP    _  80  20      ...          alias_index=4\n@@ -3290,1 +3704,0 @@\n-\n@@ -3449,0 +3862,1 @@\n+              op == Op_ReducedAllocationMerge ||\n@@ -3528,1 +3942,1 @@\n-      Node *mem = find_inst_mem(n->in(MemNode::Memory), alias_idx, orig_phis);\n+      Node *mem = find_inst_mem(n->in(MemNode::Memory), alias_idx, &orig_phis);\n@@ -3560,0 +3974,2 @@\n+      } else if(use->is_ReducedAllocationMerge()) {\n+        continue; \/\/ don't do anything\n@@ -3633,1 +4049,1 @@\n-            Node* result = find_inst_mem(mem, ni, orig_phis);\n+            Node* result = find_inst_mem(mem, ni, &orig_phis);\n@@ -3649,1 +4065,1 @@\n-        result = find_inst_mem(result, ni, orig_phis);\n+        result = find_inst_mem(result, ni, &orig_phis);\n@@ -3673,1 +4089,1 @@\n-      Node *new_mem = find_inst_mem(mem, alias_idx, orig_phis);\n+      Node *new_mem = find_inst_mem(mem, alias_idx, &orig_phis);\n@@ -3687,6 +4103,0 @@\n-  \/\/ Disable memory split verification code until the fix for 6984348.\n-  \/\/ Currently it produces false negative results since it does not cover all cases.\n-#if 0 \/\/ ifdef ASSERT\n-  visited.Reset();\n-  Node_Stack old_mems(arena, _compile->unique() >> 2);\n-#endif\n@@ -3698,6 +4108,0 @@\n-#if 0 \/\/ ifdef ASSERT\n-      Node* old_mem = n->in(MemNode::Memory);\n-      if (!visited.test_set(old_mem->_idx)) {\n-        old_mems.push(old_mem, old_mem->outcnt());\n-      }\n-#endif\n@@ -3707,1 +4111,1 @@\n-        move_inst_mem(n, orig_phis);\n+        move_inst_mem(n, &orig_phis);\n@@ -3715,1 +4119,1 @@\n-      assert(n->is_Allocate() || n->is_CheckCastPP() ||\n+      assert(n->is_Allocate() || n->is_ReducedAllocationMerge() || n->is_CheckCastPP() ||\n@@ -3719,7 +4123,48 @@\n-#if 0 \/\/ ifdef ASSERT\n-  \/\/ Verify that memory was split correctly\n-  while (old_mems.is_nonempty()) {\n-    Node* old_mem = old_mems.node();\n-    uint  old_cnt = old_mems.index();\n-    old_mems.pop();\n-    assert(old_cnt == old_mem->outcnt(), \"old mem could be lost\");\n+\n+  \/\/ Update the memory inputs of ReducedAllocationMerge nodes\n+  Unique_Node_List ram_nodes;\n+  ram_nodes.push(_compile->root());\n+  for (uint next = 0; next < ram_nodes.size(); ++next) {\n+    Node* n = ram_nodes.at(next);\n+\n+    if (n->is_ReducedAllocationMerge()) {\n+      ReducedAllocationMergeNode* ram = n->as_ReducedAllocationMerge();\n+\n+      uint number_of_bases = ram->number_of_bases();\n+      DictI needed_offsets = ram->needed_offsets();\n+\n+      while (needed_offsets.test()) {\n+        jlong offset = (jlong)needed_offsets._key;\n+        int ram_field_input_idx = (intptr_t)needed_offsets._value;\n+\n+        for (uint b_idx = 1; b_idx <= number_of_bases; ++b_idx) {\n+          Node* base    = ram->in(b_idx);\n+          Node* cur_mem = ram->in(ram_field_input_idx);\n+          assert(base != NULL, \"Shouldn't be NULL!\");\n+          assert(cur_mem != NULL, \"Shouldn't be NULL!\");\n+\n+          const TypeOopPtr *base_t = igvn->type(base)->isa_oopptr();\n+\n+          if (base_t != NULL) {\n+            const TypeOopPtr *tinst = base_t->add_offset(offset)->isa_oopptr();\n+            const int alias_idx = _compile->get_alias_index(tinst);\n+            Node* new_mem = find_inst_mem(cur_mem, alias_idx, NULL);\n+\n+            if (new_mem != cur_mem) {\n+              igvn->hash_delete(ram);\n+              ram->set_req(ram_field_input_idx, new_mem);\n+              igvn->hash_insert(ram);\n+            }\n+          }\n+\n+          ram_field_input_idx++;\n+        }\n+\n+        ++needed_offsets;\n+      }\n+    }\n+\n+    for (DUIterator_Fast imax, i = n->fast_outs(imax); i < imax; i++) {\n+      Node* m = n->fast_out(i);\n+      ram_nodes.push(m);\n+    }\n@@ -3727,1 +4172,0 @@\n-#endif\n@@ -3773,1 +4217,1 @@\n-    out->print(\"(\");\n+    out->print(\"Bases: (\");\n@@ -3780,1 +4224,1 @@\n-  out->print(\"[\");\n+  out->print(\"Edges: [\");\n@@ -3785,1 +4229,1 @@\n-  out->print(\" [\");\n+  out->print(\"] Uses: [\");\n@@ -3795,1 +4239,1 @@\n-  out->print(\" ]]  \");\n+  out->print(\" ]  \");\n@@ -3838,0 +4282,4 @@\n+      for (EdgeIterator i(ptn); i.has_next(); i.next()) {\n+        PointsToNode* e = i.get();\n+        e->dump();\n+      }\n","filename":"src\/hotspot\/share\/opto\/escape.cpp","additions":544,"deletions":96,"binary":false,"changes":640,"status":"modified"},{"patch":"@@ -211,3 +211,4 @@\n-  int edge_count()              const { return _edges.length(); }\n-  PointsToNode* edge(int e)     const { return _edges.at(e); }\n-  bool add_edge(PointsToNode* edge)   { return _edges.append_if_missing(edge); }\n+  int edge_count()              const    { return _edges.length(); }\n+  PointsToNode* edge(int e)     const    { return _edges.at(e); }\n+  bool add_edge(PointsToNode* edge)      { return _edges.append_if_missing(edge); }\n+  bool remove_edge(PointsToNode* edge)   { return _edges.remove_if_existing(edge); }\n@@ -215,3 +216,4 @@\n-  int use_count()             const { return _uses.length(); }\n-  PointsToNode* use(int e)    const { return _uses.at(e); }\n-  bool add_use(PointsToNode* use)   { return _uses.append_if_missing(use); }\n+  int use_count()             const    { return _uses.length(); }\n+  PointsToNode* use(int e)    const    { return _uses.at(e); }\n+  bool add_use(PointsToNode* use)      { return _uses.append_if_missing(use); }\n+  bool remove_use(PointsToNode* use)   { _uses.remove(use); return true; }\n@@ -220,2 +222,2 @@\n-  bool add_base_use(FieldNode* use) { return _uses.append_if_missing((PointsToNode*)((intptr_t)use + 1)); }\n-  static bool is_base_use(PointsToNode* use) { return (((intptr_t)use) & 1); }\n+  bool add_base_use(FieldNode* use)                    { return _uses.append_if_missing((PointsToNode*)((intptr_t)use + 1)); }\n+  static bool is_base_use(PointsToNode* use)           { return (((intptr_t)use) & 1); }\n@@ -223,0 +225,1 @@\n+  bool remove_base_use(FieldNode* use)                 { _uses.remove((PointsToNode*)((intptr_t)use + 1)); return true; }\n@@ -270,2 +273,2 @@\n-  int base_count()              const { return _bases.length(); }\n-  PointsToNode* base(int e)     const { return _bases.at(e); }\n+  int base_count()              const  { return _bases.length(); }\n+  PointsToNode* base(int e)     const  { return _bases.at(e); }\n@@ -273,0 +276,1 @@\n+  bool remove_base(PointsToNode* base) { _bases.remove(base); return true; }\n@@ -358,0 +362,6 @@\n+\n+  \/\/ Check if the ideal node with ID 'idx' is present in the Connection Graph.\n+  bool is_ideal_node_in_graph(uint idx) const {\n+    return idx < nodes_size() && _nodes.at(idx) != NULL;\n+  }\n+\n@@ -475,3 +485,0 @@\n-  \/\/ Returns unique corresponding java object or NULL.\n-  JavaObjectNode* unique_java_object(Node *n);\n-\n@@ -541,2 +548,2 @@\n-  PhiNode *create_split_phi(PhiNode *orig_phi, int alias_idx, GrowableArray<PhiNode *>  &orig_phi_worklist, bool &new_created);\n-  PhiNode *split_memory_phi(PhiNode *orig_phi, int alias_idx, GrowableArray<PhiNode *>  &orig_phi_worklist);\n+  PhiNode *create_split_phi(PhiNode *orig_phi, int alias_idx, GrowableArray<PhiNode *>  *orig_phi_worklist, bool &new_created);\n+  PhiNode *split_memory_phi(PhiNode *orig_phi, int alias_idx, GrowableArray<PhiNode *>  *orig_phi_worklist);\n@@ -544,2 +551,2 @@\n-  void  move_inst_mem(Node* n, GrowableArray<PhiNode *>  &orig_phis);\n-  Node* find_inst_mem(Node* mem, int alias_idx,GrowableArray<PhiNode *>  &orig_phi_worklist);\n+  void  move_inst_mem(Node* n, GrowableArray<PhiNode *>  *orig_phis);\n+  Node* find_inst_mem(Node* mem, int alias_idx,GrowableArray<PhiNode *>  *orig_phi_worklist = NULL);\n@@ -581,0 +588,15 @@\n+  \/\/ -------------------------------------------\n+  \/\/ Methods related to Reduce Allocation Merges\n+\n+  \/\/ Returns true if there is a Store node dominated by\n+  \/\/ 'merge_phi_region' for which the associated AddP uses\n+  \/\/ 'base' as Base.\n+  bool is_read_only(Node* merge_phi_region, Node* base) const;\n+\n+  const Node* come_from_allocate(const Node* n) const;\n+\n+  bool can_reduce_this_phi(const Node* n) const;\n+  bool can_reduce_this_phi_users(const Node* phi, bool& has_call_as_user) const;\n+\n+  bool reduce_this_phi(PhiNode* n);\n+\n@@ -608,0 +630,5 @@\n+  \/\/ Perform simplification of allocation merges by reducing Phi\n+  \/\/ nodes that merge scalar replaceable object allocations into\n+  \/\/ a ReduceAllocationMergeNode.\n+  int reduce_allocation_merges();\n+\n@@ -613,0 +640,3 @@\n+  \/\/ Returns unique corresponding java object or NULL.\n+  JavaObjectNode* unique_java_object(Node *n) const;\n+\n@@ -632,0 +662,2 @@\n+  void remove_phi_node(PointsToNode* ptn);\n+\n","filename":"src\/hotspot\/share\/opto\/escape.hpp","additions":49,"deletions":17,"binary":false,"changes":66,"status":"modified"},{"patch":"@@ -826,1 +826,1 @@\n-        assert(r->has_phi() == NULL, \"simple region shouldn't have a phi\");\n+        assert(!r->has_phi(), \"simple region shouldn't have a phi\");\n","filename":"src\/hotspot\/share\/opto\/ifnode.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -37,0 +37,1 @@\n+#include \"opto\/c2compiler.hpp\"\n@@ -458,1 +459,0 @@\n-  Node *alloc_ctrl = alloc->in(TypeFunc::Control);\n@@ -460,0 +460,1 @@\n+  Node *alloc_ctrl = alloc->in(TypeFunc::Control);\n@@ -540,1 +541,1 @@\n-      if (sfpt_ctl->is_Proj() && sfpt_ctl->as_Proj()->is_uncommon_trap_proj(Deoptimization::Reason_none)) {\n+      if (sfpt_ctl != NULL && sfpt_ctl->is_Proj() && sfpt_ctl->as_Proj()->is_uncommon_trap_proj(Deoptimization::Reason_none)) {\n@@ -553,1 +554,1 @@\n-bool PhaseMacroExpand::can_eliminate_allocation(AllocateNode *alloc, GrowableArray <SafePointNode *>& safepoints) {\n+bool PhaseMacroExpand::can_eliminate_allocation(AllocateNode *alloc, GrowableArray <SafePointNode *>& safepoints, GrowableArray <ReducedAllocationMergeNode *>& rams) {\n@@ -616,0 +617,3 @@\n+      } else if (use->is_ReducedAllocationMerge()) {\n+        \/\/ also ok to eliminate\n+        rams.append_if_missing(use->as_ReducedAllocationMerge());\n@@ -680,1 +684,1 @@\n-bool PhaseMacroExpand::scalar_replacement(AllocateNode *alloc, GrowableArray <SafePointNode *>& safepoints) {\n+bool PhaseMacroExpand::scalar_replacement(AllocateNode *alloc, GrowableArray <SafePointNode *>& safepoints, GrowableArray <ReducedAllocationMergeNode *>& rams) {\n@@ -712,0 +716,70 @@\n+\n+  \/\/\n+  \/\/ Process the Reduced Allocation Merges uses\n+  \/\/\n+  while (rams.length() > 0) {\n+    ReducedAllocationMergeNode* ram = rams.pop();\n+\n+    _igvn.hash_delete(ram);\n+\n+    \/\/ Scan object's fields adding an input to the RAM for each field.\n+    for (int j = 0; j < nfields; j++) {\n+      const Type *field_type;\n+      \/\/ iklass wont be null here because RAM only merge instance types\n+      assert(iklass != NULL, \"iklass shouldn't be NULL here.\");\n+      ciField* field = iklass->nonstatic_field_at(j);\n+      intptr_t offset = field->offset();\n+      ciType* elem_type = field->type();\n+      basic_elem_type = field->layout_type();\n+\n+      if (!ram->needs_field(offset)) {\n+        continue;\n+      }\n+\n+      if (is_reference_type(basic_elem_type)) {\n+        if (!elem_type->is_loaded()) {\n+          field_type = TypeInstPtr::BOTTOM;\n+        } else if (field != NULL && field->is_static_constant()) {\n+          \/\/ This can happen if the constant oop is non-perm.\n+          ciObject* con = field->constant_value().as_object();\n+          \/\/ Do not \"join\" in the previous type; it doesn't add value,\n+          \/\/ and may yield a vacuous result if the field is of interface type.\n+          field_type = TypeOopPtr::make_from_constant(con)->isa_oopptr();\n+          assert(field_type != NULL, \"field singleton type must be consistent\");\n+        } else {\n+          field_type = TypeOopPtr::make_from_klass(elem_type->as_klass());\n+        }\n+        if (UseCompressedOops) {\n+          field_type = field_type->make_narrowoop();\n+          basic_elem_type = T_NARROWOOP;\n+        }\n+      } else {\n+        field_type = Type::get_const_basic_type(basic_elem_type);\n+      }\n+\n+      \/\/ Because the same base might be registered multiple times in the Phi \/ RAM, we\n+      \/\/ need to iterate on the different memory inputs that each base might be registered\n+      \/\/ to use.\n+      uint previous_matches = 0;\n+      while (true) {\n+        Node *memory = ram->memory_for(offset, res, previous_matches);\n+\n+        if (memory == NULL) {\n+          break;\n+        }\n+\n+        const TypeOopPtr *field_addr_type = res_type->add_offset(offset)->isa_oopptr();\n+        Node *field_val = value_from_mem(memory, NULL, basic_elem_type, field_type, field_addr_type, alloc);\n+\n+        assert(field_val != NULL, \"Didn't find value for field!!!\");\n+\n+        ram->register_value_for_field(offset, res, field_val, previous_matches);\n+\n+        previous_matches++;\n+      }\n+    }\n+\n+    _igvn.hash_insert(ram);\n+    _igvn._worklist.push(ram);\n+  }\n+\n@@ -931,0 +1005,5 @@\n+      } else if (use->is_ReducedAllocationMerge()) {\n+        _igvn.hash_delete(use);\n+        use->replace_edge(res, C->top());\n+        _igvn.hash_insert(use);\n+        _igvn._worklist.push(use);\n@@ -1033,1 +1112,2 @@\n-  if (!can_eliminate_allocation(alloc, safepoints)) {\n+  GrowableArray <ReducedAllocationMergeNode *> rams;\n+  if (!can_eliminate_allocation(alloc, safepoints, rams)) {\n@@ -1047,1 +1127,1 @@\n-  if (!scalar_replacement(alloc, safepoints)) {\n+  if (!scalar_replacement(alloc, safepoints, rams)) {\n@@ -1117,0 +1197,139 @@\n+bool PhaseMacroExpand::eliminate_ram_addp_use(ReducedAllocationMergeNode *ram, AddPNode* addp) {\n+  int offset = (int)addp->in(AddPNode::Offset)->find_intptr_t_con(-1);\n+  assert(offset != -1, \"Didn't find constant offset for AddP.\");\n+\n+  Node* value_phi = ram->value_phi_for_field(offset, &_igvn);\n+  if (value_phi == NULL) {\n+    C->record_failure(C2Compiler::retry_no_reduce_allocation_merges());\n+    return false;\n+  }\n+\n+  _igvn._worklist.push(value_phi);\n+\n+  for (DUIterator_Fast jmax, j = addp->fast_outs(jmax); j < jmax; j++) {\n+    Node* addp_use = addp->fast_out(j);\n+\n+    if (addp_use->is_Load()) {\n+      Node* load = addp_use; \/\/ just for readability\n+\n+      for (DUIterator_Last kmin, k = load->last_outs(kmin); k >= kmin;) {\n+        Node* load_use = load->last_out(k);\n+\n+        _igvn.hash_delete(load_use);\n+        int removed = load_use->replace_edge(load, value_phi, &_igvn);\n+        _igvn.hash_insert(load_use);\n+        _igvn._worklist.push(load_use);\n+\n+        assert(removed > 0, \"should be at least 1.\");\n+        k -= removed;\n+      }\n+    } else {\n+      assert(false, \"Unexpected use of AddP.\");\n+      return false;\n+    }\n+  }\n+\n+  return true;\n+}\n+\n+bool PhaseMacroExpand::eliminate_reduced_allocation_merge(ReducedAllocationMergeNode *ram) {\n+  ciKlass* klass             = ram->klass();\n+  ciInstanceKlass* iklass    = klass->as_instance_klass();\n+  int nfields                = iklass->nof_nonstatic_fields();\n+  const TypeOopPtr* res_type = _igvn.type(ram)->isa_oopptr();\n+\n+  for (DUIterator_Fast imax, i = ram->fast_outs(imax); i < imax; i++) {\n+    Node* use = ram->fast_out(i);\n+\n+    if (use->is_AddP()) {\n+      if (eliminate_ram_addp_use(ram, use->as_AddP()) == false) {\n+        return false;\n+      }\n+    } else if (use->Opcode() == Op_SafePoint || use->is_CallStaticJava()) {\n+      Node* sfpt = use;\n+      assert(sfpt->jvms() != NULL, \"missed JVMS\");\n+\n+      \/\/ Fields of scalar objs are referenced only at the end\n+      \/\/ of regular debuginfo at the last (youngest) JVMS.\n+      \/\/ Record relative start index.\n+      uint first_ind = (sfpt->req() - sfpt->jvms()->scloff());\n+      SafePointScalarObjectNode* sobj = new SafePointScalarObjectNode(res_type,\n+                                                                      #ifdef ASSERT\n+                                                                        ram,\n+                                                                      #endif\n+                                                                        first_ind,\n+                                                                        nfields);\n+      sobj->init_req(0, C->root());\n+      transform_later(sobj);\n+\n+      \/\/ Scan object's fields adding an input to the safepoint for each field.\n+      for (int j = 0; j < nfields; j++) {\n+        ciField* field = iklass->nonstatic_field_at(j);\n+        int offset = field->offset();\n+        ciType* elem_type = field->type();\n+        BasicType basic_elem_type = field->layout_type();\n+        const Type *field_type;\n+\n+        \/\/ The next code is taken from Parse::do_get_xxx().\n+        if (is_reference_type(basic_elem_type)) {\n+          if (!elem_type->is_loaded()) {\n+            field_type = TypeInstPtr::BOTTOM;\n+          } else if (field != NULL && field->is_static_constant()) {\n+            \/\/ This can happen if the constant oop is non-perm.\n+            ciObject* con = field->constant_value().as_object();\n+            \/\/ Do not \"join\" in the previous type; it doesn't add value,\n+            \/\/ and may yield a vacuous result if the field is of interface type.\n+            field_type = TypeOopPtr::make_from_constant(con)->isa_oopptr();\n+            assert(field_type != NULL, \"field singleton type must be consistent\");\n+          } else {\n+            field_type = TypeOopPtr::make_from_klass(elem_type->as_klass());\n+          }\n+          if (UseCompressedOops) {\n+            field_type = field_type->make_narrowoop();\n+          }\n+        } else {\n+          field_type = Type::get_const_basic_type(basic_elem_type);\n+        }\n+\n+        Node* field_val = ram->value_phi_for_field(offset, &_igvn);\n+\n+        if (field_val == NULL) {\n+          C->record_failure(C2Compiler::retry_no_reduce_allocation_merges());\n+          return false;\n+        }\n+\n+        _igvn._worklist.push(field_val);\n+\n+        if (UseCompressedOops && field_type->isa_narrowoop()) {\n+          field_val = transform_later(new DecodeNNode(field_val, field_val->get_ptr_type()));\n+        }\n+\n+        sfpt->add_req(field_val);\n+      }\n+\n+      JVMState *jvms = sfpt->jvms();\n+      jvms->set_endoff(sfpt->req());\n+      \/\/ Now make a pass over the debug information replacing any references\n+      \/\/ to the allocated object with \"sobj\"\n+      int start    = jvms->debug_start();\n+      int end      = jvms->debug_end();\n+      int replaced = sfpt->replace_edges_in_range(ram, sobj, start, end, &_igvn);\n+      _igvn._worklist.push(sfpt);\n+\n+      assert(replaced > 0, \"should be at least 1.\");\n+      --i;\n+      imax -= replaced;\n+    } else if (use->is_DecodeN()) {\n+      for (DUIterator_Fast jmax, j = use->fast_outs(jmax); j < jmax; j++) {\n+        if (eliminate_ram_addp_use(ram, use->fast_out(j)->as_AddP()) == false) {\n+          return false;\n+        }\n+      }\n+    } else {\n+      assert(false, \"Unknown use of RAM. %d:%s\", use->_idx, use->Name());\n+      return false;\n+    }\n+  }\n+\n+  return true;\n+}\n@@ -2359,0 +2578,3 @@\n+      case Node::Class_ReducedAllocationMerge:\n+        \/\/ Needs to be processed after all others\n+        break;\n@@ -2383,0 +2605,22 @@\n+\n+  \/\/ Next, try to eliminate reduced allocation merges\n+  if (ReduceAllocationMerges) {\n+    for (int i = C->macro_count(); i > 0; i--) {\n+      Node* n = C->macro_node(i - 1);\n+      if (n->is_ReducedAllocationMerge()) {\n+        \/\/ In some cases the region controlling the RAM might go away due to some simplification\n+        \/\/ of the IR graph. For now, we'll just bail out if this happens.\n+        if (n->in(0) == NULL || !n->in(0)->is_Region()) {\n+          C->record_failure(C2Compiler::retry_no_reduce_allocation_merges());\n+          return;\n+        }\n+\n+        bool success = eliminate_reduced_allocation_merge(n->as_ReducedAllocationMerge());\n+        if (!success) {\n+          C->record_failure(C2Compiler::retry_no_reduce_allocation_merges());\n+          return;\n+        }\n+      }\n+    }\n+  }\n+\n","filename":"src\/hotspot\/share\/opto\/macro.cpp","additions":250,"deletions":6,"binary":false,"changes":256,"status":"modified"},{"patch":"@@ -102,2 +102,1 @@\n-  bool can_eliminate_allocation(AllocateNode *alloc, GrowableArray <SafePointNode *>& safepoints);\n-  bool scalar_replacement(AllocateNode *alloc, GrowableArray <SafePointNode *>& safepoints_done);\n+  bool can_eliminate_allocation(AllocateNode *alloc, GrowableArray <SafePointNode *>& safepoints, GrowableArray <ReducedAllocationMergeNode *>& rams);\n@@ -106,0 +105,17 @@\n+  \/\/ Effectivelly performs scalar replacement by replacing the uses of 'alloc' in\n+  \/\/ the nodes in 'safepoints' and 'rams' by a SafePointScalarObjectNode.\n+  bool scalar_replacement(AllocateNode *alloc, GrowableArray <SafePointNode *>& safepoints_done, GrowableArray <ReducedAllocationMergeNode *>& rams);\n+\n+  \/\/ This should be called only after all scalar replaceable Allocate nodes\n+  \/\/ have been scalar replaced. Therefore the nodes producing values for the\n+  \/\/ fields accessed by users of the RAM have already been registered.\n+  \/\/\n+  \/\/ The method will iterate over the users of 'ram' and replace the nodes\n+  \/\/ that use the _value_ of field 'x' by a value Phi merging nodes that\n+  \/\/ produce value for field 'x' in different control branches. Safepoints\n+  \/\/ and traps are special since they require a reference to the\n+  \/\/ 'ram' itself. For those cases we create an SafePointScalarObjectNode,\n+  \/\/ similar to what is done to regular scalar replacement.\n+  bool eliminate_reduced_allocation_merge(ReducedAllocationMergeNode *ram);\n+  bool eliminate_ram_addp_use(ReducedAllocationMergeNode *ram, AddPNode* addp);\n+\n","filename":"src\/hotspot\/share\/opto\/macro.hpp","additions":18,"deletions":2,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -311,0 +311,6 @@\n+  Node *address = in(MemNode::Address);\n+  \/\/ Don't try to optimize loads under RAM\n+  if (address != NULL && address->is_AddP() && address->in(AddPNode::Base) != NULL &&\n+      address->in(AddPNode::Base)->is_ReducedAllocationMerge()) {\n+    return NodeSentinel;\n+  }\n@@ -344,1 +350,0 @@\n-  Node *address = in(MemNode::Address);\n@@ -900,1 +905,2 @@\n-                     ControlDependency control_dependency, bool require_atomic_access, bool unaligned, bool mismatched, bool unsafe, uint8_t barrier_data) {\n+                     ControlDependency control_dependency, bool require_atomic_access, bool unaligned, bool mismatched, bool unsafe, uint8_t barrier_data,\n+                     bool decode_narrow) {\n@@ -952,1 +958,1 @@\n-  if (load->Opcode() == Op_LoadN) {\n+  if (load->Opcode() == Op_LoadN && decode_narrow) {\n","filename":"src\/hotspot\/share\/opto\/memnode.cpp","additions":9,"deletions":3,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -233,1 +233,1 @@\n-                    uint8_t barrier_data = 0);\n+                    uint8_t barrier_data = 0, bool decode_narrow = true);\n","filename":"src\/hotspot\/share\/opto\/memnode.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -155,0 +155,1 @@\n+class ReducedAllocationMergeNode;\n@@ -714,0 +715,1 @@\n+      DEFINE_CLASS_ID(ReducedAllocationMerge,   Type, 8)\n@@ -955,0 +957,1 @@\n+  DEFINE_CLASS_QUERY(ReducedAllocationMerge)\n","filename":"src\/hotspot\/share\/opto\/node.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -43,0 +43,2 @@\n+  flags(BEFORE_REDUCE_ALLOCATION,     \"Before reducing allocation merges\") \\\n+  flags(AFTER_REDUCE_ALLOCATION,      \"After reducing allocation merges\") \\\n","filename":"src\/hotspot\/share\/opto\/phasetype.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -0,0 +1,982 @@\n+\/*\n+ * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+package compiler.c2.irTests.scalarReplacement;\n+\n+import compiler.lib.ir_framework.*;\n+\n+\/*\n+ * @test\n+ * @bug 8281429\n+ * @summary Tests that C2 can correctly scalar replace some object allocation merges.\n+ * @library \/test\/lib \/\n+ * @requires vm.compiler2.enabled\n+ * @run driver compiler.c2.irTests.scalarReplacement.AllocationMergesTests\n+ *\/\n+public class AllocationMergesTests {\n+    private static Point global_escape = new Point(2022, 2023);\n+\n+    public static void main(String[] args) {\n+        TestFramework.runWithFlags(\"-XX:+ReduceAllocationMerges\",\n+                                   \"-XX:CompileCommand=exclude,*::dummy*\");\n+    }\n+\n+    \/\/ ------------------ No Scalar Replacement Should Happen in The Tests Below ------------------- \/\/\n+\n+    @Test\n+    @Arguments({ Argument.RANDOM_EACH, Argument.RANDOM_EACH })\n+    @IR(counts = { IRNode.ALLOC, \"1\" })\n+    int testGlobalEscape(int x, int y) {\n+        Point p = new Point(x, y);\n+\n+        AllocationMergesTests.global_escape = p;\n+\n+        return p.x * p.y;\n+    }\n+\n+    @Test\n+    @Arguments({ Argument.RANDOM_EACH, Argument.RANDOM_EACH })\n+    @IR(counts = { IRNode.ALLOC, \"1\" })\n+    int testArgEscape(int x, int y) {\n+        Point p = new Point(x, y);\n+\n+        int val = dummy(p);\n+\n+        return val + p.x + p.y;\n+    }\n+\n+    @Test\n+    @Arguments({ Argument.RANDOM_EACH, Argument.RANDOM_EACH, Argument.RANDOM_EACH, Argument.RANDOM_EACH })\n+    @IR(counts = { IRNode.ALLOC, \"2\" })\n+    int testEscapeInCallAfterMerge(boolean cond, boolean cond2, int x, int y) {\n+        Point p = new Point(x, x);\n+\n+        if (cond) {\n+            p = new Point(y, y);\n+        }\n+\n+        if (cond2) {\n+            dummy(p);\n+        }\n+\n+        return p.x * p.y;\n+    }\n+\n+    @Test\n+    @Arguments({ Argument.RANDOM_EACH, Argument.RANDOM_EACH, Argument.RANDOM_EACH, Argument.RANDOM_EACH })\n+    @IR(counts = { IRNode.ALLOC, \"2\" })\n+    \/\/ Merge won't be reduced because write to field inside the loop\n+    int testNoEscapeWithWriteInLoop(boolean cond, boolean cond2, int x, int y) {\n+        Point p = new Point(x, y);\n+        int res = 0;\n+\n+        if (cond) {\n+            p = new Point(y, x);\n+        }\n+\n+        for (int i=0; i<100; i++) {\n+            p.x += p.y + i;\n+            p.y += p.x + i;\n+        }\n+\n+        return res + p.x + p.y;\n+    }\n+\n+    @Test\n+    @Arguments({ Argument.RANDOM_EACH, Argument.RANDOM_EACH })\n+    @IR(counts = { IRNode.ALLOC, \"2\" })\n+    \/\/ Merge won't be reduced because write to field inside the loop\n+    int testPollutedWithWrite(boolean cond, int l) {\n+        Shape obj1 = new Square(l);\n+        Shape obj2 = new Square(l);\n+        Shape obj = null;\n+\n+        if (cond) {\n+            obj = obj1;\n+        } else {\n+            obj = obj2;\n+        }\n+\n+        for (int i=1; i<132; i++) {\n+            obj.x++;\n+        }\n+\n+        return obj1.x + obj2.y;\n+    }\n+\n+    @Test\n+    @Arguments({ Argument.RANDOM_EACH, Argument.RANDOM_EACH })\n+    @IR(counts = { IRNode.ALLOC, \"2\" })\n+    \/\/ Merge won't be reduced because objects have different types\n+    int testPollutedPolymorphic(boolean cond, int l) {\n+        Shape obj1 = new Square(l);\n+        Shape obj2 = new Circle(l);\n+        Shape obj = (cond ? obj1 : obj2);\n+        int res = 0;\n+\n+        for (int i=1; i<232; i++) {\n+            res += obj.x;\n+        }\n+\n+        return res + obj1.x + obj2.y;\n+    }\n+\n+    @Test\n+    @Arguments({ Argument.RANDOM_EACH, Argument.RANDOM_EACH, Argument.RANDOM_EACH })\n+    @IR(counts = { IRNode.ALLOC, \"2\" })\n+    \/\/ Merge won't be reduced because write to one of the inputs *after* the merge\n+    int testMergedLoadAfterDirectStore(boolean cond, int x, int y) {\n+        Point p0 = new Point(x, x);\n+        Point p1 = new Point(y, y);\n+        Point p = null;\n+\n+        if (cond) {\n+            p = p0;\n+        } else {\n+            p = p1;\n+        }\n+\n+        p0.x = x * y;\n+\n+        return p.x;\n+    }\n+\n+    @Test\n+    @Arguments({ Argument.RANDOM_EACH, Argument.RANDOM_EACH, Argument.RANDOM_EACH })\n+    @IR(counts = { IRNode.ALLOC, \"3\" })\n+    \/\/ p2 is ArgEscape\n+    \/\/ p is written inside the loop.\n+    int testMergedAccessAfterCallWithWrite(boolean cond, int x, int y) {\n+        Point p2 = new Point(x, x);\n+        Point p = new Point(y, y);\n+\n+        p.x = p.x * y;\n+\n+        if (cond) {\n+            p = new Point(x, x);\n+        }\n+\n+        dummy(p2);\n+\n+        for (int i=3; i<324; i++) {\n+            p.x += i * x;\n+        }\n+\n+        return p.x;\n+    }\n+\n+    @Test\n+    @Arguments({ Argument.RANDOM_EACH, Argument.RANDOM_EACH, Argument.RANDOM_EACH })\n+    @IR(counts = { IRNode.ALLOC, \"2\" })\n+    \/\/ Allocations will be NSR because they are used in a CallStaticJava\n+    int testLoadAfterTrap(boolean cond, int x, int y) {\n+        Point p = null;\n+\n+        if (cond) {\n+            p = new Point(x, x);\n+        } else {\n+            p = new Point(y, y);\n+        }\n+\n+        dummy(x+y);\n+\n+        return p.x + p.y;\n+    }\n+\n+    @Test\n+    @Arguments({ Argument.RANDOM_EACH, Argument.RANDOM_EACH, Argument.RANDOM_EACH, Argument.RANDOM_EACH })\n+    @IR(counts = { IRNode.ALLOC, \"1\" })\n+    \/\/ The merge won't be simplified because the merge with NULL instead of Allocate\n+    int testCondAfterMergeWithNull(boolean cond1, boolean cond2, int x, int y) {\n+        Point p = null;\n+\n+        if (cond1) {\n+            p = new Point(y, x);\n+        }\n+\n+        if (cond2 && cond1) {\n+            return p.x;\n+        } else {\n+            return 321;\n+        }\n+    }\n+\n+    @Test\n+    @Arguments({ Argument.RANDOM_EACH, Argument.RANDOM_EACH, Argument.RANDOM_EACH })\n+    @IR(counts = { IRNode.ALLOC, \"2\" })\n+    \/\/ A loop is appearing between the Phis in this method and is preventing the reduction\n+    int testLoadAfterLoopAlias(boolean cond, int x, int y) {\n+        Point a = new Point(x, y);\n+        Point b = new Point(y, x);\n+        Point c = a;\n+\n+        for (int i=10; i<832; i++) {\n+            if (i == 500) {\n+                c = b;\n+            }\n+        }\n+\n+        return cond ? c.x : c.y;\n+    }\n+\n+    @Test\n+    @Arguments({ Argument.RANDOM_EACH, Argument.RANDOM_EACH, Argument.RANDOM_EACH })\n+    @IR(counts = { IRNode.ALLOC, \"1\" })\n+    \/\/ Merge won't be reduced because one of the inputs come from a call\n+    int testCallOneSide(boolean cond1, int x, int y) {\n+        Point p = dummy(x, y);\n+\n+        if (cond1) {\n+            p = new Point(y, x);\n+        }\n+\n+        return p.x;\n+    }\n+\n+    @Test\n+    @Arguments({ Argument.RANDOM_EACH, Argument.RANDOM_EACH, Argument.RANDOM_EACH })\n+    @IR(counts = { IRNode.CALL, \"3\" })\n+    \/\/ Merge won't be reduced because both of the inputs come from a call\n+    \/\/ The additional \"Call\" node is because of the uncommon_trap for checking if\n+    \/\/ \"p\" is null\n+    int testCallTwoSide(boolean cond1, int x, int y) {\n+        Point p = dummy(x, y);\n+\n+        if (cond1) {\n+            p = dummy(y, x);\n+        }\n+\n+        return p.x;\n+    }\n+\n+    @Test\n+    @Arguments({ Argument.RANDOM_EACH, Argument.RANDOM_EACH, Argument.RANDOM_EACH })\n+    @IR(counts = { IRNode.ALLOC, \"3\" })\n+    \/\/ \"p\" won't be reduced because it's touched by Call to dummy\n+    int testMergedAccessAfterCallNoWrite(boolean cond, int x, int y) {\n+        Point p2 = new Point(x, x);\n+        Point p = new Point(y, y);\n+        int res = 0;\n+\n+        p.x = p.x * y;\n+\n+        if (cond) {\n+            p = new Point(y, y);\n+        }\n+\n+        dummy(p2);\n+\n+        for (int i=3; i<324; i++) {\n+            res += p.x + i * x;\n+        }\n+\n+        return res;\n+    }\n+\n+    @Test\n+    @Arguments({ Argument.RANDOM_EACH, Argument.RANDOM_EACH, Argument.RANDOM_EACH })\n+    @IR(counts = { IRNode.ALLOC, \"1\" })\n+    int testCmpMergeWithNull_Second(boolean cond, int x, int y) {\n+        Point p = null;\n+\n+        if (cond) {\n+            p = new Point(x*x, y*y);\n+        }\n+\n+        dummy(x);\n+\n+        if (p != null) {\n+            return p.x * p.y;\n+        } else {\n+            return 1984;\n+        }\n+    }\n+\n+    @Test\n+    @Arguments({ Argument.RANDOM_EACH, Argument.RANDOM_EACH, Argument.RANDOM_EACH })\n+    @IR(counts = { IRNode.ALLOC, \"1\" })\n+    int testObjectIdentity(boolean cond, int x, int y) {\n+        Point o = new Point(x, y);\n+\n+        if (cond) {\n+            o = global_escape;\n+            dummy();\n+        }\n+\n+        dummy();\n+\n+        return o == global_escape ? o.x + o.y : 0;\n+    }\n+\n+\n+    @Test\n+    @Arguments({ Argument.RANDOM_EACH, Argument.RANDOM_EACH, Argument.RANDOM_EACH, Argument.RANDOM_EACH, Argument.RANDOM_EACH, Argument.RANDOM_EACH })\n+    @IR(counts = { IRNode.ALLOC, \"2\" })\n+    \/\/ The merge won't be reduced because there is a SafePoint\/Call referring to the merge\n+    int testSubclassesTrapping(boolean c1, boolean c2, int x, int y, int w, int z) {\n+        new A();\n+        Root s = new Home(x, y);\n+        new B();\n+\n+        if (c1) {\n+            new C();\n+            s = new Etc(\"Hello\");\n+            new D();\n+        } else {\n+            new E();\n+            s = new Usr(y, x, z);\n+            new F();\n+        }\n+\n+        dummy();\n+\n+        return s.a;\n+    }\n+\n+    @Test\n+    @Warmup(10)\n+    @Arguments({ Argument.RANDOM_EACH, Argument.RANDOM_EACH, Argument.RANDOM_EACH })\n+    @IR(counts = { IRNode.ALLOC, \"2\" })\n+    \/\/ Not reduced because there is CmpP using the merge\n+    int testCmpMergeWithNull(boolean cond, int x, int y) {\n+        Point p = null;\n+\n+        if (cond) {\n+            p = new Point(x*x, y*y);\n+        } else if (x == y) {\n+            p = new Point(x+y, x*y);\n+        }\n+\n+        if (p != null) {\n+            return p.x * p.y;\n+        } else {\n+            return 1984;\n+        }\n+    }\n+\n+    @Test\n+    @Warmup(10)\n+    @Arguments({ Argument.RANDOM_EACH, Argument.RANDOM_EACH })\n+    @IR(counts = { IRNode.ALLOC, \"1\" })\n+    \/\/ Not reduced because not all inputs are SR\n+    int testMultiwayMerge(int x, int y) {\n+        Point p = new Point(0, 0);\n+\n+        if (x == y) {\n+            p = dummy(x, x);\n+        } else if (dummy(x) == 1) {\n+            p = dummy(x, y);\n+        } else if (dummy(y) == 1) {\n+            p = dummy(y, x);\n+        }\n+\n+        return p.x;\n+    }\n+\n+\n+    \/\/ ------------------ Some Objects Will be Scalar Replaced in These Tests ------------------- \/\/\n+\n+\n+    @Test\n+    @Arguments({ Argument.RANDOM_EACH, Argument.RANDOM_EACH, Argument.RANDOM_EACH, Argument.RANDOM_EACH, Argument.RANDOM_EACH, Argument.RANDOM_EACH })\n+    @IR(failOn = { IRNode.ALLOC })\n+    \/\/ Since there is no Call\/SafePoint after the merge we can reduce the allocation\n+    int testSubclasses(boolean c1, boolean c2, int x, int y, int w, int z) {\n+        new A();\n+        Root s = new Home(x, y);\n+        new B();\n+\n+        if (c1) {\n+            new C();\n+            s = new Etc(\"Hello\");\n+            new D();\n+        } else {\n+            new E();\n+            s = new Usr(y, x, z);\n+            new F();\n+        }\n+\n+        new G();\n+\n+        return s.a;\n+    }\n+\n+    @Test\n+    @Warmup(10)\n+    @Arguments({ Argument.RANDOM_EACH, Argument.RANDOM_EACH, Argument.RANDOM_EACH })\n+    @IR(counts = { IRNode.ALLOC, \"2\" })\n+    Point testNestedObjectsObject(boolean cond, int x, int y) {\n+        Picture p = new Picture(x, x, y);\n+\n+        if (cond) {\n+            p = new Picture(y, y, x);\n+        }\n+\n+        return p.position;\n+    }\n+\n+    @Test\n+    @Warmup(10)\n+    @Arguments({ Argument.RANDOM_EACH, Argument.RANDOM_EACH, Argument.RANDOM_EACH })\n+    @IR(counts = { IRNode.ALLOC, \"0\" })\n+    int testNestedObjectsNoEscapeObject(boolean cond, int x, int y) {\n+        Picture p = new Picture(x, x, y);\n+\n+        if (cond) {\n+            p = new Picture(y, y, x);\n+        }\n+\n+        return p.position.x;\n+    }\n+\n+    @Test\n+    @Arguments({ Argument.RANDOM_EACH, Argument.RANDOM_EACH, Argument.RANDOM_EACH })\n+    @IR(counts = { IRNode.ALLOC_ARRAY, \"2\" })\n+    Point[] testNestedObjectsArray(boolean cond, int x, int y) {\n+        PicturePositions p = new PicturePositions(x, y, x+y);\n+\n+        if (cond) {\n+            p = new PicturePositions(x+1, y+1, x+y+1);\n+        }\n+\n+        return p.positions;\n+    }\n+\n+    @Test\n+    @Arguments({ Argument.RANDOM_EACH, Argument.RANDOM_EACH, Argument.RANDOM_EACH })\n+    @IR(failOn = { IRNode.ALLOC })\n+    \/\/ The merge for \"p\" will be removed during iterative EA; after the allocation\n+    \/\/ inside the last if is removed in a previous EA iteration.\n+    int testTrappingAfterMerge(boolean cond, int x, int y) {\n+        Point p = new Point(x, y);\n+        int res = 0;\n+\n+        if (cond) {\n+            p = new Point(y, y);\n+        }\n+\n+        for (int i=832; i<932; i++) {\n+            res += p.x;\n+        }\n+\n+        if (x > y) {\n+            res += new Point(p.x, p.y).x;\n+        }\n+\n+        return res;\n+    }\n+\n+    @Test\n+    @Arguments({ Argument.RANDOM_EACH, Argument.RANDOM_EACH, Argument.RANDOM_EACH })\n+    @IR(failOn = {IRNode.ALLOC})\n+    int simpleMerge(boolean cond, int x, int y) {\n+        Point p = new Point(x, y);\n+\n+        if (cond) {\n+            p = new Point(y, x);\n+        }\n+\n+        return p.x * p.y;\n+    }\n+\n+    @Test\n+    @Arguments({ Argument.RANDOM_EACH, Argument.RANDOM_EACH, Argument.RANDOM_EACH })\n+    @IR(failOn = { IRNode.ALLOC })\n+    int testSimpleAliasedAlloc(boolean cond, int x, int y) {\n+        Point p1 = new Point(x, y);\n+        Point p2 = new Point(y, x);\n+        Point p = p1;\n+\n+        if (cond) {\n+            p = p2;\n+        }\n+\n+        return p.x * p.y;\n+    }\n+\n+    @Test\n+    @Arguments({ Argument.RANDOM_EACH, Argument.RANDOM_EACH, Argument.RANDOM_EACH })\n+    @IR(failOn = { IRNode.ALLOC })\n+    int testSimpleDoubleMerge(boolean cond, int x, int y) {\n+        Point p1 = new Point(x, y);\n+        Point p2 = new Point(x+1, y+1);\n+\n+        if (cond) {\n+            p1 = new Point(y, x);\n+            p2 = new Point(y+1, x+1);\n+        }\n+\n+        return p1.x + p2.y;\n+    }\n+\n+    @Test\n+    @Arguments({ Argument.RANDOM_EACH, Argument.RANDOM_EACH })\n+    @IR(counts = { IRNode.ALLOC, \"1\" })\n+    \/\/ \"p2\" is scalar replaced because it's not used in \"dummy\" except as debug info\n+    int testSimpleMixedEscape(int x, int y) {\n+        Point p1 = new Point(x, y);\n+        Point p2 = new Point(x+1, y+1);\n+\n+        int val = dummy(p1);\n+\n+        return val + p1.x + p2.y;\n+    }\n+\n+    @Test\n+    @Arguments({ Argument.RANDOM_EACH, Argument.RANDOM_EACH, Argument.RANDOM_EACH, Argument.RANDOM_EACH })\n+    @IR(failOn = { IRNode.ALLOC })\n+    int testConsecutiveSimpleMerge(boolean cond1, boolean cond2, int x, int y) {\n+        Point p0 = new Point(x, x);\n+        Point p1 = new Point(x, y);\n+        Point pA = null;\n+\n+        Point p2 = new Point(y, x);\n+        Point p3 = new Point(y, y);\n+        Point pB = null;\n+\n+        if (cond1) {\n+            pA = p0;\n+        } else {\n+            pA = p1;\n+        }\n+\n+        if (cond2) {\n+            pB = p2;\n+        } else {\n+            pB = p3;\n+        }\n+\n+        return pA.x * pA.y + pB.x * pB.y;\n+    }\n+\n+    @Test\n+    @Arguments({ Argument.RANDOM_EACH, Argument.RANDOM_EACH, Argument.RANDOM_EACH })\n+    @IR(failOn = { IRNode.ALLOC })\n+    int testDoubleIfElseMerge(boolean cond, int x, int y) {\n+        Point p1 = new Point(x, y);\n+        Point p2 = new Point(x+1, y+1);\n+\n+        if (cond) {\n+            p1 = new Point(y, x);\n+            p2 = new Point(y, x);\n+        } else {\n+            p1 = new Point(x, y);\n+            p2 = new Point(x+1, y+1);\n+        }\n+\n+        return p1.x * p2.y;\n+    }\n+\n+    @Test\n+    @Arguments({ Argument.RANDOM_EACH, Argument.RANDOM_EACH, Argument.RANDOM_EACH })\n+    @IR(failOn = { IRNode.ALLOC })\n+    int testNoEscapeWithLoadInLoop(boolean cond, int x, int y) {\n+        Point p = new Point(x, y);\n+        int res = 0;\n+\n+        if (cond) {\n+            p = new Point(y, x);\n+        }\n+\n+        for (int i=3342; i<4234; i++) {\n+            res += p.x + p.y + i;\n+        }\n+\n+        return res + p.x + p.y;\n+    }\n+\n+    @Test\n+    @Arguments({ Argument.RANDOM_EACH, Argument.RANDOM_EACH, Argument.RANDOM_EACH, Argument.RANDOM_EACH })\n+    @IR(failOn = { IRNode.ALLOC })\n+    int testCmpAfterMerge(boolean cond, boolean cond2, int x, int y) {\n+        Point a = new Point(x, y);\n+        Point b = new Point(y, x);\n+        Point c = null;\n+\n+        if (x+2 >= y-5) {\n+            c = a;\n+        } else {\n+            c = b;\n+        }\n+\n+        return cond2 ? c.x : c.y;\n+    }\n+\n+\n+    @Test\n+    @Arguments({ Argument.RANDOM_EACH, Argument.RANDOM_EACH, Argument.RANDOM_EACH, Argument.RANDOM_EACH })\n+    @IR(failOn = { IRNode.ALLOC })\n+    int testCondAfterMergeWithAllocate(boolean cond1, boolean cond2, int x, int y) {\n+        Point p = new Point(x, y);\n+\n+        if (cond1) {\n+            p = new Point(y, x);\n+        }\n+\n+        if (cond2 && cond1) {\n+            return p.x;\n+        } else {\n+            return 321;\n+        }\n+    }\n+\n+    @Test\n+    @Arguments({ Argument.RANDOM_EACH, Argument.RANDOM_EACH, Argument.RANDOM_EACH, Argument.RANDOM_EACH })\n+    @IR(failOn = { IRNode.ALLOC })\n+    int testCondLoadAfterMerge(boolean cond1, boolean cond2, int x, int y) {\n+        Point p = new Point(x, y);\n+\n+        if (cond1) {\n+            p = new Point(y, x);\n+        }\n+\n+        if (cond1 == false && cond2 == false) {\n+            return p.x + 1;\n+        } else if (cond1 == false && cond2 == true) {\n+            return p.x + 30;\n+        } else if (cond1 == true && cond2 == false) {\n+            return p.x + 40;\n+        } else if (cond1 == true && cond2 == true) {\n+            return p.x + 50;\n+        } else {\n+            return -1;\n+        }\n+    }\n+\n+    @Test\n+    @IR(failOn = { IRNode.ALLOC })\n+    int testIfElseInLoop() {\n+        int res = 0;\n+\n+        for (int i=1; i<1000; i++) {\n+            Point obj = new Point(i, i);\n+\n+            if (i % 2 == 1) {\n+                obj = new Point(i, i+1);\n+            } else {\n+                obj = new Point(i-1, i);\n+            }\n+\n+            res += obj.x;\n+        }\n+\n+        return res;\n+    }\n+\n+    @Test\n+    @Arguments({ Argument.RANDOM_EACH, Argument.RANDOM_EACH, Argument.RANDOM_EACH })\n+    @IR(failOn = { IRNode.ALLOC })\n+    int testLoadInCondAfterMerge(boolean cond, int x, int y) {\n+        Point p = new Point(x, y);\n+\n+        if (cond) {\n+            p = new Point(y, x);\n+        }\n+\n+        if (p.x == 10) {\n+            if (p.y == 10) {\n+                return dummy(10);\n+            } else {\n+                return dummy(20);\n+            }\n+        } else if (p.x == 20) {\n+            if (p.y == 20) {\n+                return dummy(30);\n+            } else {\n+                return dummy(40);\n+            }\n+        }\n+\n+        return 1984;\n+    }\n+\n+    @Test\n+    @Arguments({ Argument.RANDOM_EACH, Argument.RANDOM_EACH, Argument.RANDOM_EACH })\n+    @IR(failOn = { IRNode.ALLOC })\n+    int testLoadInLoop(boolean cond, int x, int y) {\n+        Point obj1 = new Point(x, y);\n+        Point obj2 = new Point(y, x);\n+        Point obj = null;\n+        int res = 0;\n+\n+        if (cond) {\n+            obj = obj1;\n+        } else {\n+            obj = obj2;\n+        }\n+\n+        for (int i = 0; i < 532; i++) {\n+            res += obj.x;\n+        }\n+\n+        return res;\n+    }\n+\n+    @Test\n+    @Arguments({ Argument.RANDOM_EACH, Argument.RANDOM_EACH, Argument.RANDOM_EACH })\n+    @IR(counts = { IRNode.ALLOC, \"1\" })\n+    \/\/ p2 is ArgEscape\n+    \/\/ p1 can be scalar replaced\n+    int testMergesAndMixedEscape(boolean cond, int x, int y) {\n+        Point p1 = new Point(x, y);\n+        Point p2 = new Point(x, y);\n+        int val  = 0;\n+\n+        if (cond) {\n+            p1 = new Point(x+1, y+1);\n+            val = dummy(p2);\n+        }\n+\n+        return val + p1.x + p2.y;\n+    }\n+\n+    @Test\n+    @Arguments({ Argument.RANDOM_EACH, Argument.RANDOM_EACH, Argument.RANDOM_EACH, Argument.RANDOM_EACH })\n+    @IR(failOn = { IRNode.ALLOC })\n+    int testPartialPhis(boolean cond, int l, int x, int y) {\n+        int k = l;\n+\n+        if (l == 0) {\n+            k = l + 1;\n+        } else if (l == 2) {\n+            k = l + 2;\n+        } else if (l == 3) {\n+            new Point(x, y);\n+        } else if (l == 4) {\n+            new Point(y, x);\n+        }\n+\n+        return k;\n+    }\n+\n+    @Test\n+    @Arguments({ Argument.RANDOM_EACH, Argument.RANDOM_EACH })\n+    @IR(failOn = { IRNode.ALLOC })\n+    int testPollutedNoWrite(boolean cond, int l) {\n+        Shape obj1 = new Square(l);\n+        Shape obj2 = new Square(l);\n+        Shape obj = null;\n+        int res = 0;\n+\n+        if (cond) {\n+            obj = obj1;\n+        } else {\n+            obj = obj2;\n+        }\n+\n+        for (int i=1; i<132; i++) {\n+            res += obj.x;\n+        }\n+\n+        return res + obj1.x + obj2.y;\n+    }\n+\n+    @Test\n+    @Arguments({ Argument.RANDOM_EACH, Argument.RANDOM_EACH, Argument.RANDOM_EACH })\n+    @IR(failOn = { IRNode.ALLOC })\n+    int testThreeWayAliasedAlloc(boolean cond, int x, int y) {\n+        Point p1 = new Point(x, y);\n+        Point p2 = new Point(x+1, y+1);\n+        Point p3 = new Point(x+2, y+2);\n+\n+        if (cond) {\n+            p3 = p1;\n+        } else {\n+            p3 = p2;\n+        }\n+\n+        return p3.x + p3.y;\n+    }\n+\n+    @Test\n+    @Arguments({ Argument.RANDOM_EACH, Argument.RANDOM_EACH, Argument.RANDOM_EACH })\n+    @IR(failOn = { IRNode.ALLOC })\n+    int TestTrapAfterMerge(boolean cond, int x, int y) {\n+        Point p = new Point(x, x);\n+\n+        if (cond) {\n+            p = new Point(y, y);\n+        }\n+\n+        for (int i=402; i<432; i+=x) {\n+            x++;\n+        }\n+\n+        return p.x + x;\n+    }\n+\n+    @Test\n+    @Arguments({ Argument.RANDOM_EACH, Argument.RANDOM_EACH })\n+    @IR(failOn = { IRNode.ALLOC })\n+    int testMergedWithDeadCode(boolean cond, int x) {\n+        ADefaults obj1 = new ADefaults(x);\n+        ADefaults obj2 = new ADefaults();\n+        ADefaults obj = cond ? obj1 : obj2;\n+\n+        return obj1.i + obj.ble + 1082;\n+    }\n+\n+\n+    \/\/ ------------------ Utility for Testing ------------------- \/\/\n+\n+    @DontCompile\n+    static void dummy() {\n+    }\n+\n+    @DontCompile\n+    static int dummy(Point p) {\n+        return p.x * p.y;\n+    }\n+\n+    @DontCompile\n+    static int dummy(int x) {\n+        return x;\n+    }\n+\n+    @DontCompile\n+    static Point dummy(int x, int y) {\n+        return new Point(x, y);\n+    }\n+\n+    @DontCompile\n+    static ADefaults dummy_defaults() {\n+        return new ADefaults();\n+    }\n+\n+    static class Point {\n+        int x, y;\n+        Point(int x, int y) {\n+            this.x = x;\n+            this.y = y;\n+        }\n+    }\n+\n+    class Shape {\n+        int x, y, l;\n+        Shape(int x, int y) {\n+            this.x = x;\n+            this.y = y;\n+        }\n+    }\n+\n+    class Square extends Shape {\n+        Square(int l) {\n+            super(0, 0);\n+            this.l = l;\n+        }\n+    }\n+\n+    class Circle extends Shape {\n+        Circle(int l) {\n+            super(0, 0);\n+            this.l = l;\n+        }\n+    }\n+\n+   static class ADefaults {\n+        static int ble;\n+        int i;\n+        ADefaults(int i) { this.i = i; }\n+        ADefaults() { }\n+    }\n+\n+    static class Picture {\n+        public int id;\n+        public Point position;\n+\n+        public Picture(int id, int x, int y) {\n+            this.id = id;\n+            this.position = new Point(x, y);\n+        }\n+    }\n+\n+    static class PicturePositions {\n+        public int id;\n+        public Point[] positions;\n+\n+        public PicturePositions(int id, int x, int y) {\n+            this.id = id;\n+            this.positions = new Point[] { new Point(x, y), new Point(y, x) };\n+        }\n+    }\n+\n+    class Root {\n+        public int a;\n+        public int b;\n+        public int c;\n+        public int d;\n+        public int e;\n+\n+        public Root(int a, int b, int c, int d, int e) {\n+            this.a = a;\n+            this.b = b;\n+            this.c = c;\n+            this.d = d;\n+            this.e = e;\n+        }\n+    }\n+\n+    class Usr extends Root {\n+        public float flt;\n+\n+        public Usr(float a, float b, float c) {\n+            super((int)a, (int)b, (int)c, 0, 0);\n+            this.flt = a;\n+        }\n+    }\n+\n+    class Home extends Root {\n+        public double[] arr;\n+\n+        public Home(double a, double b) {\n+            super((int)a, (int)b, 0, 0, 0);\n+            this.arr = new double[] {a, b};\n+        }\n+\n+    }\n+\n+    class Tmp extends Root {\n+        public String s;\n+\n+        public Tmp(String s) {\n+            super((int)s.length(), 0, 0, 0, 0);\n+            this.s = s;\n+        }\n+    }\n+\n+    class Etc extends Root {\n+        public String a;\n+\n+        public Etc(String s) {\n+            super((int)s.length(), 0, 0, 0, 0);\n+            this.a = s;\n+        }\n+    }\n+\n+    class A { }\n+    class B { }\n+    class C { }\n+    class D { }\n+    class E { }\n+    class F { }\n+    class G { }\n+}\n","filename":"test\/hotspot\/jtreg\/compiler\/c2\/irTests\/scalarReplacement\/AllocationMergesTests.java","additions":982,"deletions":0,"binary":false,"changes":982,"status":"added"},{"patch":"@@ -0,0 +1,188 @@\n+\/*\n+ * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package org.openjdk.bench.vm.compiler;\n+\n+import org.openjdk.jmh.annotations.Benchmark;\n+import org.openjdk.jmh.annotations.BenchmarkMode;\n+import org.openjdk.jmh.annotations.Fork;\n+import org.openjdk.jmh.annotations.Measurement;\n+import org.openjdk.jmh.annotations.Mode;\n+import org.openjdk.jmh.annotations.OutputTimeUnit;\n+import org.openjdk.jmh.annotations.Scope;\n+import org.openjdk.jmh.annotations.State;\n+import org.openjdk.jmh.annotations.Warmup;\n+import org.openjdk.jmh.annotations.Setup;\n+import org.openjdk.jmh.infra.Blackhole;\n+\n+import java.util.Random;\n+import java.util.concurrent.TimeUnit;\n+\n+@BenchmarkMode(Mode.AverageTime)\n+@OutputTimeUnit(TimeUnit.NANOSECONDS)\n+@State(Scope.Thread)\n+@Warmup(iterations = 5, time = 2, timeUnit = TimeUnit.SECONDS)\n+@Measurement(iterations = 5, time = 2, timeUnit = TimeUnit.SECONDS)\n+@Fork(value = 1)\n+public class AllocationsMerge {\n+    private int SIZE = 10 * 1024;\n+    private int opaque_value1 = 3342;\n+    private int opaque_value2 = 4342;\n+\n+    private boolean[] conds = new boolean[SIZE];\n+    private int[] xs = new int[SIZE];\n+    private int[] ys = new int[SIZE];\n+\n+    @Setup\n+    public void init() {\n+        Random r = new Random(1024);\n+\n+        for (int i=0; i<SIZE; i++) {\n+            conds[i] = i % 2 == 0;\n+            xs[i] = r.nextInt();\n+            ys[i] = r.nextInt();\n+        }\n+    }\n+\n+    static class Point {\n+        int x, y;\n+        Point(int x, int y) { this.x = x; this.y = y; }\n+    }\n+\n+    static class Picture {\n+        public int id;\n+        public Point position;\n+\n+        public Picture(int id, int x, int y) {\n+            this.id = id;\n+            this.position = new Point(x, y);\n+        }\n+\n+        public Picture(int id, Point p) {\n+            this.id = id;\n+            this.position = p;\n+        }\n+    }\n+\n+    @Benchmark\n+    public void SimpleMerge(Blackhole bh) {\n+        for (int i=0; i<SIZE; i++) {\n+            bh.consume(run_SimpleMerge(conds[i], xs[i], ys[i]));\n+        }\n+    }\n+\n+    private int run_SimpleMerge(boolean cond, int x, int y) {\n+        Point p = new Point(x, y);\n+\n+        if (cond) {\n+            p = new Point(y, x);\n+        }\n+\n+        return p.x * p.y;\n+    }\n+\n+    @Benchmark\n+    public void NestedObjectsObject(Blackhole bh) {\n+        for (int i=0; i<SIZE; i++) {\n+            bh.consume(run_NestedObjectsObject(conds[i], xs[i], ys[i]));\n+        }\n+    }\n+\n+    private Point run_NestedObjectsObject(boolean cond, int x, int y) {\n+        Picture p = new Picture(x, x, y);\n+\n+        if (cond) {\n+            p = new Picture(y, y, x);\n+        }\n+\n+        return p.position;\n+    }\n+\n+    @Benchmark\n+    public void MergeAndIterative(Blackhole bh) {\n+        for (int i=0; i<SIZE; i++) {\n+            bh.consume(run_MergeAndIterative(conds[i], xs[i], ys[i]));\n+        }\n+    }\n+\n+    private int run_MergeAndIterative(boolean cond, int x, int y) {\n+        Point p = new Point(x, y);\n+\n+        if (cond) {\n+            p = new Point(y, x);\n+        }\n+\n+        Picture pic = new Picture(2022, p);\n+\n+        return pic.position.x + pic.position.y;\n+    }\n+\n+    @Benchmark\n+    public void IfElseInLoop(Blackhole bh) {\n+        for (int i=0; i<SIZE; i++) {\n+            bh.consume(run_IfElseInLoop());\n+        }\n+    }\n+\n+    private int run_IfElseInLoop() {\n+        int res = 0;\n+\n+        for (int i=this.opaque_value1; i<this.opaque_value2; i++) {\n+            Point obj = new Point(i, i);\n+\n+            if (i % 2 == 1) {\n+                obj = new Point(i, i+1);\n+            } else {\n+                obj = new Point(i-1, i);\n+            }\n+\n+            res += obj.x;\n+        }\n+\n+        return res;\n+    }\n+\n+    @Benchmark\n+    public void TrapAfterMerge(Blackhole bh) {\n+        for (int i=0; i<SIZE; i++) {\n+            bh.consume(run_TrapAfterMerge(conds[i], xs[i], ys[i]));\n+        }\n+    }\n+\n+    private int run_TrapAfterMerge(boolean cond, int x, int y) {\n+        Point p = new Point(x, x);\n+\n+        if (cond) {\n+            p = new Point(y, y);\n+        }\n+\n+        for (int i=this.opaque_value1; i<this.opaque_value2; i+=x) {\n+            x++;\n+        }\n+\n+        return p.x + x;\n+    }\n+\n+    @Fork(jvmArgsPrepend = {\"-XX:+ReduceAllocationMerges\"})\n+    public static class WithAllocationsMergeEnabled extends AllocationsMerge { }\n+}\n","filename":"test\/micro\/org\/openjdk\/bench\/vm\/compiler\/AllocationsMerge.java","additions":188,"deletions":0,"binary":false,"changes":188,"status":"added"}]}