{"files":[{"patch":"@@ -3124,0 +3124,3 @@\n+  static bool is_z_cfi(long x) {\n+    return (CFI_ZOPC == (x & RIL_MASK));\n+  }\n","filename":"src\/hotspot\/cpu\/s390\/assembler_s390.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -30,0 +30,2 @@\n+#include \"gc\/shared\/barrierSet.hpp\"\n+#include \"gc\/shared\/barrierSetAssembler.hpp\"\n@@ -75,0 +77,3 @@\n+\n+  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  bs->nmethod_entry_barrier(this);\n","filename":"src\/hotspot\/cpu\/s390\/c1_MacroAssembler_s390.cpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -217,0 +217,2 @@\n+  const int nbytes_save = (5 + 8) * BytesPerWord;\n+\n@@ -239,1 +241,2 @@\n-  __ push_frame_abi160(0); \/\/ Will use Z_R0 as tmp.\n+  __ push_frame_abi160(nbytes_save); \/\/ Will use Z_R0 as tmp.\n+  __ save_volatile_regs(Z_SP, frame::z_abi_160_size, true, false);\n@@ -244,0 +247,1 @@\n+  __ restore_volatile_regs(Z_SP, frame::z_abi_160_size, true, false);\n","filename":"src\/hotspot\/cpu\/s390\/gc\/g1\/g1BarrierSetAssembler_s390.cpp","additions":6,"deletions":2,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,0 +28,2 @@\n+#include \"classfile\/classLoaderData.hpp\"\n+#include \"gc\/shared\/barrierSet.hpp\"\n@@ -29,0 +31,1 @@\n+#include \"gc\/shared\/barrierSetNMethod.hpp\"\n@@ -32,0 +35,2 @@\n+#include \"runtime\/sharedRuntime.hpp\"\n+#include \"runtime\/stubRoutines.hpp\"\n@@ -122,0 +127,69 @@\n+\n+void BarrierSetAssembler::nmethod_entry_barrier(MacroAssembler* masm) {\n+  BarrierSetNMethod* bs_nm = BarrierSet::barrier_set()->barrier_set_nmethod();\n+  if (bs_nm == nullptr) {\n+    return;\n+  }\n+\n+  __ block_comment(\"nmethod_entry_barrier (nmethod_entry_barrier) {\");\n+\n+    \/\/ Load jump addr:\n+    __ load_const(Z_R1_scratch, (uint64_t)StubRoutines::zarch::nmethod_entry_barrier()); \/\/ 2*6 bytes\n+\n+    \/\/ Load value from current java object:\n+    __ z_lg(Z_R0_scratch, in_bytes(bs_nm->thread_disarmed_offset()), Z_thread); \/\/ 6 bytes\n+\n+    \/\/ Compare to current patched value:\n+    __ z_cfi(Z_R0_scratch, \/* to be patched *\/ -1); \/\/ 6 bytes (2 + 4 byte imm val)\n+\n+    \/\/ Conditional Jump\n+    __ z_larl(Z_R14, (Assembler::instr_len((unsigned long)LARL_ZOPC) + Assembler::instr_len((unsigned long)BCR_ZOPC)) \/ 2); \/\/ 6 bytes\n+    __ z_bcr(Assembler::bcondNotEqual, Z_R1_scratch); \/\/ 2 bytes\n+\n+    \/\/ Fall through to method body.\n+  __ block_comment(\"} nmethod_entry_barrier (nmethod_entry_barrier)\");\n+}\n+\n+void BarrierSetAssembler::c2i_entry_barrier(MacroAssembler* masm) {\n+  BarrierSetNMethod* bs_nm = BarrierSet::barrier_set()->barrier_set_nmethod();\n+  if (bs_nm == nullptr) {\n+    return;\n+  }\n+\n+  Label bad_call, skip_barrier;\n+\n+  Register Rtmp1, Rtmp2, Rtmp3;\n+  Rtmp1 = Z_R1_scratch;\n+  Rtmp2 = Z_R7;\n+  Rtmp3 = Z_R8;\n+\n+  __ block_comment(\"c2i_entry_barrier (c2i_entry_barrier) {\");\n+\n+  \/\/ Fast path: If no method is given, the call is definitely bad.\n+  __ z_cfi(Z_method, 0);\n+  __ z_bre(bad_call);\n+\n+  \/\/ Load class loader data to determine whether the method's holder is concurrently unloading.\n+  __ load_method_holder(Rtmp1, Z_method);\n+  __ z_lg(Rtmp1, in_bytes(InstanceKlass::class_loader_data_offset()), Rtmp1);\n+\n+  \/\/ Fast path: If class loader is strong, the holder cannot be unloaded.\n+  __ z_llgf(Rtmp2, in_bytes(ClassLoaderData::keep_alive_offset()), Rtmp1);\n+  __ z_cfi(Rtmp2, 0);\n+  __ z_brne(skip_barrier);\n+\n+  \/\/ Class loader is weak. Determine whether the holder is still alive.\n+  __ z_lg(Rtmp2, in_bytes(ClassLoaderData::holder_offset()), Rtmp1);\n+  __ resolve_weak_handle(Address(Rtmp2), Rtmp2, Rtmp1, Rtmp3);\n+  __ z_cfi(Rtmp2, 0);\n+  __ z_brne(skip_barrier);\n+\n+  __ bind(bad_call);\n+\n+  __ load_const_optimized(Rtmp1, SharedRuntime::get_handle_wrong_method_stub());\n+  __ z_br(Rtmp1); \/\/ Does not return\n+\n+  __ bind(skip_barrier);\n+\n+  __ block_comment(\"} c2i_entry_barrier (c2i_entry_barrier)\");\n+}\n","filename":"src\/hotspot\/cpu\/s390\/gc\/shared\/barrierSetAssembler_s390.cpp","additions":75,"deletions":1,"binary":false,"changes":76,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -44,0 +44,2 @@\n+  virtual void nmethod_entry_barrier(MacroAssembler* masm);\n+  virtual void c2i_entry_barrier(MacroAssembler* masm);\n","filename":"src\/hotspot\/cpu\/s390\/gc\/shared\/barrierSetAssembler_s390.hpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -26,0 +26,4 @@\n+#include \"asm\/assembler.inline.hpp\"\n+#include \"code\/codeBlob.hpp\"\n+#include \"code\/nativeInst.hpp\"\n+#include \"code\/nmethod.hpp\"\n@@ -29,0 +33,66 @@\n+class NativeMethodBarrier: public NativeInstruction {\n+  private:\n+    static const int PATCHABLE_INSTRUCTION_OFFSET = 3*6; \/\/ bytes\n+\n+    address get_barrier_start_address() const {\n+      return NativeInstruction::addr_at(0);\n+    }\n+\n+    address get_patchable_data_address() const {\n+      address inst_addr = get_barrier_start_address() + PATCHABLE_INSTRUCTION_OFFSET;\n+\n+      debug_only(Assembler::is_z_cfi(*((long*)inst_addr)));\n+      return inst_addr + 2;\n+    }\n+\n+  public:\n+    static const int BARRIER_TOTAL_LENGTH = PATCHABLE_INSTRUCTION_OFFSET + 2*6 + 2; \/\/ bytes\n+\n+    int get_guard_value() const {\n+      address data_addr = get_patchable_data_address();\n+      \/\/ Return guard instruction value\n+      return *((int32_t*)data_addr);\n+    }\n+\n+    void set_guard_value(int value) {\n+      int32_t* data_addr = (int32_t*)get_patchable_data_address();\n+\n+      \/\/ Set guard instruction value\n+      *data_addr = value;\n+    }\n+\n+    #ifdef ASSERT\n+      void verify() const {\n+        int offset = 0; \/\/ bytes\n+        const address start = get_barrier_start_address();\n+\n+        MacroAssembler::is_load_const(\/* address *\/ start + offset); \/\/ two instructions\n+        offset += Assembler::instr_len(&start[offset]);\n+        offset += Assembler::instr_len(&start[offset]);\n+\n+        Assembler::is_z_lg(*((long*)(start + offset)));\n+        offset += Assembler::instr_len(&start[offset]);\n+\n+        Assembler::is_z_cfi(*((long*)(start + offset)));\n+        offset += Assembler::instr_len(&start[offset]);\n+\n+        Assembler::is_z_larl(*((long*)(start + offset)));\n+        offset += Assembler::instr_len(&start[offset]);\n+\n+        Assembler::is_z_bcr(*((long*)(start + offset)));\n+        offset += Assembler::instr_len(&start[offset]);\n+\n+        assert(offset == BARRIER_TOTAL_LENGTH, \"check offset == barrier length constant\");\n+      }\n+    #endif\n+\n+};\n+\n+static NativeMethodBarrier* get_nmethod_barrier(nmethod* nm) {\n+  address barrier_address = nm->code_begin() + nm->frame_complete_offset() - NativeMethodBarrier::BARRIER_TOTAL_LENGTH;\n+  auto barrier = reinterpret_cast<NativeMethodBarrier*>(barrier_address);\n+\n+  debug_only(barrier->verify());\n+  return barrier;\n+}\n+\n@@ -30,1 +100,11 @@\n-  ShouldNotReachHere();\n+  \/\/ Not required on s390 as a valid backchain is present\n+  return;\n+}\n+\n+void BarrierSetNMethod::arm(nmethod* nm, int arm_value) {\n+  if (!supports_entry_barrier(nm)) {\n+    return;\n+  }\n+\n+  NativeMethodBarrier* barrier = get_nmethod_barrier(nm);\n+  barrier->set_guard_value(arm_value);\n@@ -34,1 +114,6 @@\n-  ShouldNotReachHere();\n+  if (!supports_entry_barrier(nm)) {\n+    return;\n+  }\n+\n+  NativeMethodBarrier* barrier = get_nmethod_barrier(nm);\n+  barrier->set_guard_value(disarmed_value());\n@@ -38,2 +123,6 @@\n-  ShouldNotReachHere();\n-  return false;\n+  if (!supports_entry_barrier(nm)) {\n+    return false;\n+  }\n+\n+  NativeMethodBarrier* barrier = get_nmethod_barrier(nm);\n+  return barrier->get_guard_value() != disarmed_value();\n","filename":"src\/hotspot\/cpu\/s390\/gc\/shared\/barrierSetNMethod_s390.cpp","additions":94,"deletions":5,"binary":false,"changes":99,"status":"modified"},{"patch":"@@ -4191,0 +4191,11 @@\n+void MacroAssembler::resolve_weak_handle(const Address& addr, Register result, Register tmp1, Register tmp2) {\n+  Label resolved;\n+\n+  \/\/ A null weak handle resolves to null.\n+  z_cfi(result, 0);\n+  z_bre(resolved);\n+\n+  access_load_at(T_OBJECT, IN_NATIVE | ON_PHANTOM_OOP_REF, addr, result, tmp1, tmp2);\n+  bind(resolved);\n+}\n+\n","filename":"src\/hotspot\/cpu\/s390\/macroAssembler_s390.cpp","additions":11,"deletions":0,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -814,0 +814,2 @@\n+  void resolve_weak_handle(const Address& addr, Register result, Register tmp1, Register tmp2);\n+\n","filename":"src\/hotspot\/cpu\/s390\/macroAssembler_s390.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -813,0 +813,2 @@\n+#include \"gc\/shared\/barrierSetAssembler.hpp\"\n+\n@@ -840,0 +842,4 @@\n+\n+  if (C->stub_function() == NULL) {\n+    st->print(\"nmethod entry barrier\\n\\t\");\n+  }\n@@ -893,0 +899,7 @@\n+\n+  if (C->stub_function() == NULL) {\n+    BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+    bs->nmethod_entry_barrier(&_masm);\n+  }\n+\n+  C->output()->set_frame_complete(cbuf.insts_size());\n","filename":"src\/hotspot\/cpu\/s390\/s390.ad","additions":13,"deletions":0,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"gc\/shared\/barrierSetAssembler.hpp\"\n@@ -1544,0 +1545,3 @@\n+  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  bs->nmethod_entry_barrier(masm);\n+\n@@ -2378,0 +2382,3 @@\n+  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  bs->c2i_entry_barrier(masm);\n+\n@@ -2380,1 +2387,2 @@\n-  return AdapterHandlerLibrary::new_entry(fingerprint, i2c_entry, c2i_entry, c2i_unverified_entry, c2i_no_clinit_check_entry);\n+  return AdapterHandlerLibrary::new_entry(fingerprint, i2c_entry, c2i_entry,\n+                                          c2i_unverified_entry, c2i_no_clinit_check_entry);\n","filename":"src\/hotspot\/cpu\/s390\/sharedRuntime_s390.cpp","additions":9,"deletions":1,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"gc\/shared\/barrierSetNMethod.hpp\"\n@@ -2860,0 +2861,41 @@\n+  address generate_nmethod_entry_barrier() {\n+    __ align(CodeEntryAlignment);\n+    StubCodeMark mark(this, \"StubRoutines\", \"nmethod_entry_barrier\");\n+\n+    address start = __ pc();\n+\n+    int nbytes_volatile = (8 + 5) * BytesPerWord;\n+\n+    \/\/ VM-Call Prologue\n+    __ save_return_pc();\n+    __ push_frame_abi160(nbytes_volatile);\n+    __ save_volatile_regs(Z_SP, frame::z_abi_160_size, true, false);\n+\n+    \/\/ Prep arg for VM call\n+    \/\/ Create ptr to stored return_pc in caller frame.\n+    __ z_la(Z_ARG1, _z_abi(return_pc) + frame::z_abi_160_size + nbytes_volatile, Z_R0, Z_SP);\n+\n+    \/\/ VM-Call: BarrierSetNMethod::nmethod_stub_entry_barrier(address* return_address_ptr)\n+    __ call_VM_leaf(CAST_FROM_FN_PTR(address, BarrierSetNMethod::nmethod_stub_entry_barrier));\n+    __ z_ltr(Z_R0_scratch, Z_RET);\n+\n+    \/\/ VM-Call Epilogue\n+    __ restore_volatile_regs(Z_SP, frame::z_abi_160_size, true, false);\n+    __ pop_frame();\n+    __ restore_return_pc();\n+\n+    \/\/ Check return val of VM-Call\n+    __ z_bcr(Assembler::bcondZero, Z_R14);\n+\n+    \/\/ Pop frame built in prologue.\n+    \/\/ Required so wrong_method_stub can deduce caller.\n+    __ pop_frame();\n+    __ restore_return_pc();\n+\n+    \/\/ VM-Call indicates deoptimization required\n+    __ load_const_optimized(Z_R1_scratch, SharedRuntime::get_handle_wrong_method_stub());\n+    __ z_br(Z_R1_scratch);\n+\n+    return start;\n+  }\n+\n@@ -3001,0 +3043,6 @@\n+    \/\/ nmethod entry barriers for concurrent class unloading\n+    BarrierSetNMethod* bs_nm = BarrierSet::barrier_set()->barrier_set_nmethod();\n+    if (bs_nm != NULL) {\n+      StubRoutines::zarch::_nmethod_entry_barrier = generate_nmethod_entry_barrier();\n+    }\n+\n","filename":"src\/hotspot\/cpu\/s390\/stubGenerator_s390.cpp","additions":48,"deletions":0,"binary":false,"changes":48,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, 2017, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -41,0 +41,2 @@\n+address StubRoutines::zarch::_nmethod_entry_barrier = NULL;\n+\n","filename":"src\/hotspot\/cpu\/s390\/stubRoutines_s390.cpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -81,0 +81,2 @@\n+  static address _nmethod_entry_barrier;\n+\n@@ -101,0 +103,2 @@\n+\n+  static address nmethod_entry_barrier() { return _nmethod_entry_barrier; }\n","filename":"src\/hotspot\/cpu\/s390\/stubRoutines_s390.hpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"}]}