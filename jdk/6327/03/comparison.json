{"files":[{"patch":"@@ -892,0 +892,16 @@\n+class G1PrecleanYieldClosure : public YieldClosure {\n+  G1ConcurrentMark* _cm;\n+\n+public:\n+  G1PrecleanYieldClosure(G1ConcurrentMark* cm) : _cm(cm) { }\n+\n+  bool should_return() override {\n+    return _cm->has_aborted();\n+  }\n+\n+  bool should_return_fine_grain() override {\n+    _cm->do_yield_check();\n+    return _cm->has_aborted();\n+  }\n+};\n+\n@@ -894,0 +910,11 @@\n+  ReferenceProcessor*   _cm_rp;\n+\n+  void do_marking(G1CMTask* task) {\n+    do {\n+      task->do_marking_step(G1ConcMarkStepDurationMillis,\n+                            true  \/* do_termination *\/,\n+                            false \/* is_serial*\/);\n+\n+      _cm->do_yield_check();\n+    } while (!_cm->has_aborted() && task->has_aborted());\n+  }\n@@ -895,0 +922,8 @@\n+  void preclean() {\n+    BarrierEnqueueDiscoveredFieldClosure enqueue;\n+    G1PrecleanYieldClosure yield_cl(_cm);\n+    _cm_rp->preclean_discovered_references(_cm_rp->is_alive_non_header(),\n+                                           &enqueue,\n+                                           &yield_cl,\n+                                           _cm->_gc_timer_cm);\n+  }\n@@ -909,4 +944,1 @@\n-        do {\n-          task->do_marking_step(G1ConcMarkStepDurationMillis,\n-                                true  \/* do_termination *\/,\n-                                false \/* is_serial*\/);\n+        do_marking(task);\n@@ -914,2 +946,4 @@\n-          _cm->do_yield_check();\n-        } while (!_cm->has_aborted() && task->has_aborted());\n+        \/\/ Marking is complete; preclean non-strong references\n+        if (G1UseReferencePrecleaning) {\n+          preclean();\n+        }\n@@ -917,1 +951,0 @@\n-      task->record_end_time();\n@@ -919,0 +952,1 @@\n+      task->record_end_time();\n@@ -925,4 +959,4 @@\n-  G1CMConcurrentMarkingTask(G1ConcurrentMark* cm) :\n-      WorkerTask(\"Concurrent Mark\"), _cm(cm) { }\n-\n-  ~G1CMConcurrentMarkingTask() { }\n+  G1CMConcurrentMarkingTask(G1ConcurrentMark* cm, ReferenceProcessor* cm_rp) :\n+      WorkerTask(\"Concurrent Mark\"),\n+      _cm(cm),\n+      _cm_rp(cm_rp) {}\n@@ -1055,1 +1089,1 @@\n-  G1CMConcurrentMarkingTask marking_task(this);\n+  G1CMConcurrentMarkingTask marking_task(this, _g1h->ref_processor_cm());\n@@ -1674,36 +1708,0 @@\n-class G1PrecleanYieldClosure : public YieldClosure {\n-  G1ConcurrentMark* _cm;\n-\n-public:\n-  G1PrecleanYieldClosure(G1ConcurrentMark* cm) : _cm(cm) { }\n-\n-  virtual bool should_return() {\n-    return _cm->has_aborted();\n-  }\n-\n-  virtual bool should_return_fine_grain() {\n-    _cm->do_yield_check();\n-    return _cm->has_aborted();\n-  }\n-};\n-\n-void G1ConcurrentMark::preclean() {\n-  assert(G1UseReferencePrecleaning, \"Precleaning must be enabled.\");\n-\n-  SuspendibleThreadSetJoiner joiner;\n-\n-  BarrierEnqueueDiscoveredFieldClosure enqueue;\n-\n-  set_concurrency_and_phase(1, true);\n-\n-  G1PrecleanYieldClosure yield_cl(this);\n-\n-  ReferenceProcessor* rp = _g1h->ref_processor_cm();\n-  \/\/ Precleaning is single threaded. Temporarily disable MT discovery.\n-  ReferenceProcessorMTDiscoveryMutator rp_mut_discovery(rp, false);\n-  rp->preclean_discovered_references(rp->is_alive_non_header(),\n-                                     &enqueue,\n-                                     &yield_cl,\n-                                     _gc_timer_cm);\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ConcurrentMark.cpp","additions":46,"deletions":48,"binary":false,"changes":94,"status":"modified"},{"patch":"@@ -556,3 +556,0 @@\n-  \/\/ Do concurrent preclean work.\n-  void preclean();\n-\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ConcurrentMark.hpp","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -194,6 +194,1 @@\n-    \/\/ Subphase 2: Preclean (optional)\n-    if (G1UseReferencePrecleaning) {\n-      if (subphase_preclean()) return true;\n-    }\n-\n-    \/\/ Subphase 3: Wait for Remark.\n+    \/\/ Subphase 2: Wait for Remark.\n@@ -202,1 +197,1 @@\n-    \/\/ Subphase 4: Remark pause\n+    \/\/ Subphase 3: Remark pause\n@@ -229,6 +224,0 @@\n-bool G1ConcurrentMarkThread::subphase_preclean() {\n-  G1ConcPhaseTimer p(_cm, \"Concurrent Preclean\");\n-  _cm->preclean();\n-  return _cm->has_aborted();\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ConcurrentMarkThread.cpp","additions":2,"deletions":13,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -66,1 +66,0 @@\n-  bool subphase_preclean();\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ConcurrentMarkThread.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -1071,32 +1071,13 @@\n-  \/\/ These lists can be handled here in any order and, indeed, concurrently.\n-\n-  \/\/ Soft references\n-  {\n-    GCTraceTime(Debug, gc, ref) tm(\"Preclean SoftReferences\", gc_timer);\n-    log_reflist(\"SoftRef before: \", _discoveredSoftRefs, _max_num_queues);\n-    for (uint i = 0; i < _max_num_queues; i++) {\n-      if (yield->should_return()) {\n-        return;\n-      }\n-      if (preclean_discovered_reflist(_discoveredSoftRefs[i], is_alive,\n-                                      enqueue, yield)) {\n-        log_reflist(\"SoftRef abort: \", _discoveredSoftRefs, _max_num_queues);\n-        return;\n-      }\n-    }\n-    log_reflist(\"SoftRef after: \", _discoveredSoftRefs, _max_num_queues);\n-  }\n-\n-  \/\/ Weak references\n-  {\n-    GCTraceTime(Debug, gc, ref) tm(\"Preclean WeakReferences\", gc_timer);\n-    log_reflist(\"WeakRef before: \", _discoveredWeakRefs, _max_num_queues);\n-    for (uint i = 0; i < _max_num_queues; i++) {\n-      if (yield->should_return()) {\n-        return;\n-      }\n-      if (preclean_discovered_reflist(_discoveredWeakRefs[i], is_alive,\n-                                      enqueue, yield)) {\n-        log_reflist(\"WeakRef abort: \", _discoveredWeakRefs, _max_num_queues);\n-        return;\n-      }\n+  Ticks preclean_start = Ticks::now();\n+\n+  constexpr int ref_kinds = 4;\n+  ReferenceType ref_type_arr[] = { REF_SOFT, REF_WEAK, REF_FINAL, REF_PHANTOM };\n+  static_assert(ARRAY_SIZE(ref_type_arr) == ref_kinds, \"invariant\");\n+  size_t ref_count_arr[ref_kinds] = {};\n+\n+  if (discovery_is_mt()) {\n+    for (int i = 0; i < ref_kinds; ++i) {\n+      ReferenceType ref_type = ref_type_arr[i];\n+      DiscoveredList* list = get_discovered_list(ref_type);\n+      ref_count_arr[i] = list->length();\n+      preclean_discovered_reflist(*list, is_alive, enqueue, yield);\n@@ -1104,15 +1085,9 @@\n-    log_reflist(\"WeakRef after: \", _discoveredWeakRefs, _max_num_queues);\n-  }\n-\n-  \/\/ Final references\n-  {\n-    GCTraceTime(Debug, gc, ref) tm(\"Preclean FinalReferences\", gc_timer);\n-    log_reflist(\"FinalRef before: \", _discoveredFinalRefs, _max_num_queues);\n-    for (uint i = 0; i < _max_num_queues; i++) {\n-      if (yield->should_return()) {\n-        return;\n-      }\n-      if (preclean_discovered_reflist(_discoveredFinalRefs[i], is_alive,\n-                                      enqueue, yield)) {\n-        log_reflist(\"FinalRef abort: \", _discoveredFinalRefs, _max_num_queues);\n-        return;\n+  } else {\n+    for (int i = 0; i < ref_kinds; ++i) {\n+      ReferenceType ref_type = ref_type_arr[i];\n+      \/\/ When discovery is *not* multi-threaded, discovered refs are stored in\n+      \/\/ list[0.._num_queues-1]. Loop _num_queues times to cover all lists.\n+      for (uint j = 0; j < _num_queues; ++j) {\n+        DiscoveredList* list = get_discovered_list(ref_type);\n+        ref_count_arr[i] += list->length();\n+        preclean_discovered_reflist(*list, is_alive, enqueue, yield);\n@@ -1121,1 +1096,0 @@\n-    log_reflist(\"FinalRef after: \", _discoveredFinalRefs, _max_num_queues);\n@@ -1124,16 +1098,4 @@\n-  \/\/ Phantom references\n-  {\n-    GCTraceTime(Debug, gc, ref) tm(\"Preclean PhantomReferences\", gc_timer);\n-    log_reflist(\"PhantomRef before: \", _discoveredPhantomRefs, _max_num_queues);\n-    for (uint i = 0; i < _max_num_queues; i++) {\n-      if (yield->should_return()) {\n-        return;\n-      }\n-      if (preclean_discovered_reflist(_discoveredPhantomRefs[i], is_alive,\n-                                      enqueue, yield)) {\n-        log_reflist(\"PhantomRef abort: \", _discoveredPhantomRefs, _max_num_queues);\n-        return;\n-      }\n-    }\n-    log_reflist(\"PhantomRef after: \", _discoveredPhantomRefs, _max_num_queues);\n-  }\n+  uint worker_id = WorkerThread::current()->id();\n+  log_trace(gc, ref)(\"Worker (%d): Precleaning Soft (%zu), Weak (%zu), Final (%zu), Phantom (%zu) %f ms\",\n+      worker_id, ref_count_arr[0], ref_count_arr[1], ref_count_arr[2], ref_count_arr[3],\n+      (Ticks::now() - preclean_start).seconds()*1000);\n","filename":"src\/hotspot\/share\/gc\/shared\/referenceProcessor.cpp","additions":26,"deletions":64,"binary":false,"changes":90,"status":"modified"},{"patch":"@@ -504,21 +504,0 @@\n-\/\/ A utility class to temporarily change the MT'ness of\n-\/\/ reference discovery for the given ReferenceProcessor\n-\/\/ in the scope that contains it.\n-class ReferenceProcessorMTDiscoveryMutator: StackObj {\n- private:\n-  ReferenceProcessor* _rp;\n-  bool                _saved_mt;\n-\n- public:\n-  ReferenceProcessorMTDiscoveryMutator(ReferenceProcessor* rp,\n-                                       bool mt):\n-    _rp(rp) {\n-    _saved_mt = _rp->discovery_is_mt();\n-    _rp->set_mt_discovery(mt);\n-  }\n-\n-  ~ReferenceProcessorMTDiscoveryMutator() {\n-    _rp->set_mt_discovery(_saved_mt);\n-  }\n-};\n-\n","filename":"src\/hotspot\/share\/gc\/shared\/referenceProcessor.hpp","additions":0,"deletions":21,"binary":false,"changes":21,"status":"modified"}]}