{"files":[{"patch":"@@ -585,0 +585,5 @@\n+\n+  inline void atomic_copy64(const volatile void *src, volatile void *dst) {\n+    *(jlong *) dst = *(const jlong *) src;\n+  }\n+\n@@ -589,1 +594,1 @@\n-        os::atomic_copy64(from++, to++);\n+        atomic_copy64(from++, to++);\n@@ -596,1 +601,1 @@\n-        os::atomic_copy64(from--, to--);\n+        atomic_copy64(from--, to--);\n","filename":"src\/hotspot\/os_cpu\/bsd_aarch64\/os_bsd_aarch64.cpp","additions":7,"deletions":2,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -38,5 +38,0 @@\n-  \/\/ Atomically copy 64 bits of data\n-  static void atomic_copy64(const volatile void *src, volatile void *dst) {\n-    *(jlong *) dst = *(const jlong *) src;\n-  }\n-\n","filename":"src\/hotspot\/os_cpu\/bsd_aarch64\/os_bsd_aarch64.hpp","additions":1,"deletions":6,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -288,0 +288,19 @@\n+\/\/ Atomically copy 64 bits of data\n+static void atomic_copy64(const volatile void *src, volatile void *dst) {\n+#if defined(PPC32)\n+  double tmp;\n+  asm volatile (\"lfd  %0, 0(%1)\\n\"\n+                \"stfd %0, 0(%2)\\n\"\n+                : \"=f\"(tmp)\n+                : \"b\"(src), \"b\"(dst));\n+#elif defined(S390) && !defined(_LP64)\n+  double tmp;\n+  asm volatile (\"ld  %0, 0(%1)\\n\"\n+                \"std %0, 0(%2)\\n\"\n+                : \"=r\"(tmp)\n+                : \"a\"(src), \"a\"(dst));\n+#else\n+  *(jlong *) dst = *(const jlong *) src;\n+#endif\n+}\n+\n@@ -293,1 +312,1 @@\n-  os::atomic_copy64(reinterpret_cast<const volatile int64_t*>(src), reinterpret_cast<volatile int64_t*>(&dest));\n+  atomic_copy64(reinterpret_cast<const volatile int64_t*>(src), reinterpret_cast<volatile int64_t*>(&dest));\n@@ -302,1 +321,1 @@\n-  os::atomic_copy64(reinterpret_cast<const volatile int64_t*>(&store_value), reinterpret_cast<volatile int64_t*>(dest));\n+  atomic_copy64(reinterpret_cast<const volatile int64_t*>(&store_value), reinterpret_cast<volatile int64_t*>(dest));\n","filename":"src\/hotspot\/os_cpu\/bsd_zero\/atomic_bsd_zero.hpp","additions":22,"deletions":3,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"atomic_bsd_zero.hpp\"\n@@ -298,1 +299,1 @@\n-        os::atomic_copy64(from++, to++);\n+        atomic_copy64(from++, to++);\n@@ -305,1 +306,1 @@\n-        os::atomic_copy64(from--, to--);\n+        atomic_copy64(from--, to--);\n","filename":"src\/hotspot\/os_cpu\/bsd_zero\/os_bsd_zero.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -35,19 +35,0 @@\n-  \/\/ Atomically copy 64 bits of data\n-  static void atomic_copy64(const volatile void *src, volatile void *dst) {\n-#if defined(PPC32)\n-    double tmp;\n-    asm volatile (\"lfd  %0, 0(%1)\\n\"\n-                  \"stfd %0, 0(%2)\\n\"\n-                  : \"=f\"(tmp)\n-                  : \"b\"(src), \"b\"(dst));\n-#elif defined(S390) && !defined(_LP64)\n-    double tmp;\n-    asm volatile (\"ld  %0, 0(%1)\\n\"\n-                  \"std %0, 0(%2)\\n\"\n-                  : \"=r\"(tmp)\n-                  : \"a\"(src), \"a\"(dst));\n-#else\n-    *(jlong *) dst = *(const jlong *) src;\n-#endif\n-  }\n-\n","filename":"src\/hotspot\/os_cpu\/bsd_zero\/os_bsd_zero.hpp","additions":1,"deletions":20,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -436,0 +436,5 @@\n+\n+  inline void atomic_copy64(const volatile void *src, volatile void *dst) {\n+    *(jlong *) dst = *(const jlong *) src;\n+  }\n+\n@@ -440,1 +445,1 @@\n-        os::atomic_copy64(from++, to++);\n+        atomic_copy64(from++, to++);\n@@ -447,1 +452,1 @@\n-        os::atomic_copy64(from--, to--);\n+        atomic_copy64(from--, to--);\n","filename":"src\/hotspot\/os_cpu\/linux_aarch64\/os_linux_aarch64.cpp","additions":7,"deletions":2,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -39,5 +39,0 @@\n-  \/\/ Atomically copy 64 bits of data\n-  static void atomic_copy64(const volatile void *src, volatile void *dst) {\n-    *(jlong *) dst = *(const jlong *) src;\n-  }\n-\n","filename":"src\/hotspot\/os_cpu\/linux_aarch64\/os_linux_aarch64.hpp","additions":1,"deletions":6,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2008, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2008, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -33,0 +33,34 @@\n+struct arm_atomic_funcs {\n+  int64_t (*cmpxchg_long_func)(int64_t compare_value,\n+                               int64_t exchange_value,\n+                               volatile int64_t *dest);\n+\n+  int64_t (*load_long_func)(const volatile int64_t*);\n+\n+  void (*store_long_func)(int64_t, volatile int64_t*);\n+\n+  int32_t  (*add_func)(int32_t add_value, volatile int32_t *dest);\n+\n+  int32_t  (*xchg_func)(int32_t exchange_value, volatile int32_t *dest);\n+\n+  int32_t  (*cmpxchg_func)(int32_t compare_value,\n+                           int32_t exchange_value,\n+                           volatile int32_t *dest);\n+\n+  static int64_t cmpxchg_long_bootstrap(int64_t, int64_t, volatile int64_t*);\n+\n+  static int64_t load_long_bootstrap(const volatile int64_t*);\n+\n+  static void store_long_bootstrap(int64_t, volatile int64_t*);\n+\n+  static int32_t  add_bootstrap(int32_t add_value, volatile int32_t *dest);\n+\n+  static int32_t  xchg_bootstrap(int32_t exchange_value, volatile int32_t *dest);\n+\n+  static int32_t  cmpxchg_bootstrap(int32_t compare_value,\n+                                    int32_t exchange_value,\n+                                    volatile int32_t *dest);\n+};\n+\n+extern arm_atomic_funcs _arm_atomic;\n+\n@@ -52,1 +86,1 @@\n-    (*os::atomic_load_long_func)(reinterpret_cast<const volatile int64_t*>(src)));\n+    (*_arm_atomic.load_long_func)(reinterpret_cast<const volatile int64_t*>(src)));\n@@ -60,1 +94,1 @@\n-  (*os::atomic_store_long_func)(\n+  (*_arm_atomic.store_long_func)(\n@@ -86,1 +120,1 @@\n-  return add_using_helper<int32_t>(os::atomic_add_func, dest, add_value);\n+  return add_using_helper<int32_t>(_arm_atomic.add_func, dest, add_value);\n@@ -96,1 +130,1 @@\n-  return xchg_using_helper<int32_t>(os::atomic_xchg_func, dest, exchange_value);\n+  return xchg_using_helper<int32_t>(_arm_atomic.xchg_func, dest, exchange_value);\n@@ -111,1 +145,1 @@\n-  return (*os::atomic_cmpxchg_func)(compare_value, exchange_value, dest);\n+  return (*_arm_atomic.cmpxchg_func)(compare_value, exchange_value, dest);\n@@ -119,1 +153,1 @@\n-  return (*os::atomic_cmpxchg_long_func)(compare_value, exchange_value, dest);\n+  return (*_arm_atomic.cmpxchg_long_func)(compare_value, exchange_value, dest);\n","filename":"src\/hotspot\/os_cpu\/linux_arm\/atomic_linux_arm.hpp","additions":41,"deletions":7,"binary":false,"changes":48,"status":"modified"},{"patch":"@@ -78,0 +78,7 @@\n+#ifndef __thumb__\n+enum {\n+  \/\/ Offset to add to frame::_fp when dealing with non-thumb C frames\n+  C_frame_offset =  -1,\n+};\n+#endif\n+\n@@ -497,0 +504,8 @@\n+arm_atomic_funcs _arm_atomic = {\n+  arm_atomic_funcs::cmpxchg_long_bootstrap,\n+  arm_atomic_funcs::load_long_bootstrap,\n+  arm_atomic_funcs::store_long_bootstrap,\n+  arm_atomic_funcs::add_bootstrap,\n+  arm_atomic_funcs::xchg_bootstrap,\n+  arm_atomic_funcs::cmpxchg_bootstrap,\n+};\n@@ -499,3 +514,1 @@\n-cmpxchg_long_func_t* os::atomic_cmpxchg_long_func = os::atomic_cmpxchg_long_bootstrap;\n-\n-int64_t os::atomic_cmpxchg_long_bootstrap(int64_t compare_value, int64_t exchange_value, volatile int64_t* dest) {\n+int64_t arm_atomic_funcs::cmpxchg_long_bootstrap(int64_t compare_value, int64_t exchange_value, volatile int64_t* dest) {\n@@ -506,1 +519,1 @@\n-    os::atomic_cmpxchg_long_func = func;\n+    _arm_atomic.cmpxchg_long_func = func;\n@@ -518,3 +531,1 @@\n-load_long_func_t* os::atomic_load_long_func = os::atomic_load_long_bootstrap;\n-\n-int64_t os::atomic_load_long_bootstrap(const volatile int64_t* src) {\n+int64_t arm_atomic_funcs::load_long_bootstrap(const volatile int64_t* src) {\n@@ -525,1 +536,1 @@\n-    os::atomic_load_long_func = func;\n+    _arm_atomic.load_long_func = func;\n@@ -536,3 +547,1 @@\n-store_long_func_t* os::atomic_store_long_func = os::atomic_store_long_bootstrap;\n-\n-void os::atomic_store_long_bootstrap(int64_t val, volatile int64_t* dest) {\n+void arm_atomic_funcs::store_long_bootstrap(int64_t val, volatile int64_t* dest) {\n@@ -543,1 +552,1 @@\n-    os::atomic_store_long_func = func;\n+    _arm_atomic.store_long_func = func;\n@@ -553,3 +562,1 @@\n-atomic_add_func_t * os::atomic_add_func = os::atomic_add_bootstrap;\n-\n-int32_t  os::atomic_add_bootstrap(int32_t add_value, volatile int32_t *dest) {\n+int32_t  arm_atomic_funcs::add_bootstrap(int32_t add_value, volatile int32_t *dest) {\n@@ -559,1 +566,1 @@\n-    os::atomic_add_func = func;\n+    _arm_atomic.add_func = func;\n@@ -570,3 +577,1 @@\n-atomic_xchg_func_t * os::atomic_xchg_func = os::atomic_xchg_bootstrap;\n-\n-int32_t  os::atomic_xchg_bootstrap(int32_t exchange_value, volatile int32_t *dest) {\n+int32_t  arm_atomic_funcs::xchg_bootstrap(int32_t exchange_value, volatile int32_t *dest) {\n@@ -576,1 +581,1 @@\n-    os::atomic_xchg_func = func;\n+    _arm_atomic.xchg_func = func;\n@@ -587,3 +592,1 @@\n-cmpxchg_func_t* os::atomic_cmpxchg_func = os::atomic_cmpxchg_bootstrap;\n-\n-int32_t os::atomic_cmpxchg_bootstrap(int32_t compare_value, int32_t exchange_value, volatile int32_t* dest) {\n+int32_t arm_atomic_funcs::cmpxchg_bootstrap(int32_t compare_value, int32_t exchange_value, volatile int32_t* dest) {\n@@ -594,1 +597,1 @@\n-    os::atomic_cmpxchg_func = func;\n+    _arm_atomic.cmpxchg_func = func;\n","filename":"src\/hotspot\/os_cpu\/linux_arm\/os_linux_arm.cpp","additions":27,"deletions":24,"binary":false,"changes":51,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2008, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2008, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,7 +28,0 @@\n-#ifndef __thumb__\n-  enum {\n-    \/\/ Offset to add to frame::_fp when dealing with non-thumb C frames\n-    C_frame_offset =  -1,\n-  };\n-#endif\n-\n@@ -41,30 +34,0 @@\n-  static int64_t (*atomic_cmpxchg_long_func)(int64_t compare_value,\n-                                             int64_t exchange_value,\n-                                             volatile int64_t *dest);\n-\n-  static int64_t (*atomic_load_long_func)(const volatile int64_t*);\n-\n-  static void (*atomic_store_long_func)(int64_t, volatile int64_t*);\n-\n-  static int32_t  (*atomic_add_func)(int32_t add_value, volatile int32_t *dest);\n-\n-  static int32_t  (*atomic_xchg_func)(int32_t exchange_value, volatile int32_t *dest);\n-\n-  static int32_t  (*atomic_cmpxchg_func)(int32_t compare_value,\n-                                         int32_t exchange_value,\n-                                         volatile int32_t *dest);\n-\n-  static int64_t atomic_cmpxchg_long_bootstrap(int64_t, int64_t, volatile int64_t*);\n-\n-  static int64_t atomic_load_long_bootstrap(const volatile int64_t*);\n-\n-  static void atomic_store_long_bootstrap(int64_t, volatile int64_t*);\n-\n-  static int32_t  atomic_add_bootstrap(int32_t add_value, volatile int32_t *dest);\n-\n-  static int32_t  atomic_xchg_bootstrap(int32_t exchange_value, volatile int32_t *dest);\n-\n-  static int32_t  atomic_cmpxchg_bootstrap(int32_t compare_value,\n-                                           int32_t exchange_value,\n-                                           volatile int32_t *dest);\n-\n","filename":"src\/hotspot\/os_cpu\/linux_arm\/os_linux_arm.hpp","additions":1,"deletions":38,"binary":false,"changes":39,"status":"modified"},{"patch":"@@ -433,0 +433,5 @@\n+\n+  \/\/ Atomically copy 64 bits of data\n+  inline void atomic_copy64(const volatile void *src, volatile void *dst) {\n+    *(jlong *) dst = *(const jlong *) src;\n+  }\n@@ -437,1 +442,1 @@\n-        os::atomic_copy64(from++, to++);\n+        atomic_copy64(from++, to++);\n@@ -444,1 +449,1 @@\n-        os::atomic_copy64(from--, to--);\n+        atomic_copy64(from--, to--);\n","filename":"src\/hotspot\/os_cpu\/linux_riscv\/os_linux_riscv.cpp","additions":7,"deletions":2,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -35,5 +35,0 @@\n-  \/\/ Atomically copy 64 bits of data\n-  static void atomic_copy64(const volatile void *src, volatile void *dst) {\n-    *(jlong *) dst = *(const jlong *) src;\n-  }\n-\n","filename":"src\/hotspot\/os_cpu\/linux_riscv\/os_linux_riscv.hpp","additions":1,"deletions":6,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -30,1 +30,0 @@\n-#include \"runtime\/os.hpp\"\n@@ -136,0 +135,43 @@\n+\/\/ Atomically copy 64 bits of data\n+inline void atomic_copy64(const volatile void *src, volatile void *dst) {\n+#if defined(PPC32) && !defined(__SPE__)\n+  double tmp;\n+  asm volatile (\"lfd  %0, %2\\n\"\n+                \"stfd %0, %1\\n\"\n+                : \"=&f\"(tmp), \"=Q\"(*(volatile double*)dst)\n+                : \"Q\"(*(volatile double*)src));\n+#elif defined(PPC32) && defined(__SPE__)\n+  long tmp;\n+  asm volatile (\"evldd  %0, %2\\n\"\n+                \"evstdd %0, %1\\n\"\n+                : \"=&r\"(tmp), \"=Q\"(*(volatile long*)dst)\n+                : \"Q\"(*(volatile long*)src));\n+#elif defined(S390) && !defined(_LP64)\n+  double tmp;\n+  asm volatile (\"ld  %0, %2\\n\"\n+                \"std %0, %1\\n\"\n+                : \"=&f\"(tmp), \"=Q\"(*(volatile double*)dst)\n+                : \"Q\"(*(volatile double*)src));\n+#elif defined(__ARM_ARCH_7A__)\n+  \/\/ The only way to perform the atomic 64-bit load\/store\n+  \/\/ is to use ldrexd\/strexd for both reads and writes.\n+  \/\/ For store, we need to have the matching (fake) load first.\n+  \/\/ Put clrex between exclusive ops on src and dst for clarity.\n+  uint64_t tmp_r, tmp_w;\n+  uint32_t flag_w;\n+  asm volatile (\"ldrexd %[tmp_r], [%[src]]\\n\"\n+                \"clrex\\n\"\n+                \"1:\\n\"\n+                \"ldrexd %[tmp_w], [%[dst]]\\n\"\n+                \"strexd %[flag_w], %[tmp_r], [%[dst]]\\n\"\n+                \"cmp    %[flag_w], 0\\n\"\n+                \"bne    1b\\n\"\n+                : [tmp_r] \"=&r\" (tmp_r), [tmp_w] \"=&r\" (tmp_w),\n+                  [flag_w] \"=&r\" (flag_w)\n+                : [src] \"r\" (src), [dst] \"r\" (dst)\n+                : \"cc\", \"memory\");\n+#else\n+  *(jlong *) dst = *(const jlong *) src;\n+#endif\n+}\n+\n@@ -141,1 +183,1 @@\n-  os::atomic_copy64(reinterpret_cast<const volatile int64_t*>(src), reinterpret_cast<volatile int64_t*>(&dest));\n+  atomic_copy64(reinterpret_cast<const volatile int64_t*>(src), reinterpret_cast<volatile int64_t*>(&dest));\n@@ -150,1 +192,1 @@\n-  os::atomic_copy64(reinterpret_cast<const volatile int64_t*>(&store_value), reinterpret_cast<volatile int64_t*>(dest));\n+  atomic_copy64(reinterpret_cast<const volatile int64_t*>(&store_value), reinterpret_cast<volatile int64_t*>(dest));\n","filename":"src\/hotspot\/os_cpu\/linux_zero\/atomic_linux_zero.hpp","additions":46,"deletions":4,"binary":false,"changes":50,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"atomic_linux_zero.hpp\"\n@@ -342,1 +343,1 @@\n-        os::atomic_copy64(from++, to++);\n+        atomic_copy64(from++, to++);\n@@ -349,1 +350,1 @@\n-        os::atomic_copy64(from--, to--);\n+        atomic_copy64(from--, to--);\n","filename":"src\/hotspot\/os_cpu\/linux_zero\/os_linux_zero.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -48,43 +48,0 @@\n-  \/\/ Atomically copy 64 bits of data\n-  static void atomic_copy64(const volatile void *src, volatile void *dst) {\n-#if defined(PPC32) && !defined(__SPE__)\n-    double tmp;\n-    asm volatile (\"lfd  %0, %2\\n\"\n-                  \"stfd %0, %1\\n\"\n-                  : \"=&f\"(tmp), \"=Q\"(*(volatile double*)dst)\n-                  : \"Q\"(*(volatile double*)src));\n-#elif defined(PPC32) && defined(__SPE__)\n-    long tmp;\n-    asm volatile (\"evldd  %0, %2\\n\"\n-                  \"evstdd %0, %1\\n\"\n-                  : \"=&r\"(tmp), \"=Q\"(*(volatile long*)dst)\n-                  : \"Q\"(*(volatile long*)src));\n-#elif defined(S390) && !defined(_LP64)\n-    double tmp;\n-    asm volatile (\"ld  %0, %2\\n\"\n-                  \"std %0, %1\\n\"\n-                  : \"=&f\"(tmp), \"=Q\"(*(volatile double*)dst)\n-                  : \"Q\"(*(volatile double*)src));\n-#elif defined(__ARM_ARCH_7A__)\n-    \/\/ The only way to perform the atomic 64-bit load\/store\n-    \/\/ is to use ldrexd\/strexd for both reads and writes.\n-    \/\/ For store, we need to have the matching (fake) load first.\n-    \/\/ Put clrex between exclusive ops on src and dst for clarity.\n-    uint64_t tmp_r, tmp_w;\n-    uint32_t flag_w;\n-    asm volatile (\"ldrexd %[tmp_r], [%[src]]\\n\"\n-                  \"clrex\\n\"\n-                  \"1:\\n\"\n-                  \"ldrexd %[tmp_w], [%[dst]]\\n\"\n-                  \"strexd %[flag_w], %[tmp_r], [%[dst]]\\n\"\n-                  \"cmp    %[flag_w], 0\\n\"\n-                  \"bne    1b\\n\"\n-                  : [tmp_r] \"=&r\" (tmp_r), [tmp_w] \"=&r\" (tmp_w),\n-                    [flag_w] \"=&r\" (flag_w)\n-                  : [src] \"r\" (src), [dst] \"r\" (dst)\n-                  : \"cc\", \"memory\");\n-#else\n-    *(jlong *) dst = *(const jlong *) src;\n-#endif\n-  }\n-\n","filename":"src\/hotspot\/os_cpu\/linux_zero\/os_linux_zero.hpp","additions":1,"deletions":44,"binary":false,"changes":45,"status":"modified"}]}