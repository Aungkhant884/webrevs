{"files":[{"patch":"@@ -552,0 +552,4 @@\n+static inline void atomic_copy64(const volatile void *src, volatile void *dst) {\n+  *(jlong *) dst = *(const jlong *) src;\n+}\n+\n@@ -585,0 +589,1 @@\n+\n@@ -589,1 +594,1 @@\n-        os::atomic_copy64(from++, to++);\n+        atomic_copy64(from++, to++);\n@@ -596,1 +601,1 @@\n-        os::atomic_copy64(from--, to--);\n+        atomic_copy64(from--, to--);\n","filename":"src\/hotspot\/os_cpu\/bsd_aarch64\/os_bsd_aarch64.cpp","additions":7,"deletions":2,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -38,5 +38,0 @@\n-  \/\/ Atomically copy 64 bits of data\n-  static void atomic_copy64(const volatile void *src, volatile void *dst) {\n-    *(jlong *) dst = *(const jlong *) src;\n-  }\n-\n","filename":"src\/hotspot\/os_cpu\/bsd_aarch64\/os_bsd_aarch64.hpp","additions":1,"deletions":6,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -288,0 +288,19 @@\n+\/\/ Atomically copy 64 bits of data\n+static void atomic_copy64(const volatile void *src, volatile void *dst) {\n+#if defined(PPC32)\n+  double tmp;\n+  asm volatile (\"lfd  %0, 0(%1)\\n\"\n+                \"stfd %0, 0(%2)\\n\"\n+                : \"=f\"(tmp)\n+                : \"b\"(src), \"b\"(dst));\n+#elif defined(S390) && !defined(_LP64)\n+  double tmp;\n+  asm volatile (\"ld  %0, 0(%1)\\n\"\n+                \"std %0, 0(%2)\\n\"\n+                : \"=r\"(tmp)\n+                : \"a\"(src), \"a\"(dst));\n+#else\n+  *(jlong *) dst = *(const jlong *) src;\n+#endif\n+}\n+\n@@ -293,1 +312,1 @@\n-  os::atomic_copy64(reinterpret_cast<const volatile int64_t*>(src), reinterpret_cast<volatile int64_t*>(&dest));\n+  atomic_copy64(reinterpret_cast<const volatile int64_t*>(src), reinterpret_cast<volatile int64_t*>(&dest));\n@@ -302,1 +321,1 @@\n-  os::atomic_copy64(reinterpret_cast<const volatile int64_t*>(&store_value), reinterpret_cast<volatile int64_t*>(dest));\n+  atomic_copy64(reinterpret_cast<const volatile int64_t*>(&store_value), reinterpret_cast<volatile int64_t*>(dest));\n","filename":"src\/hotspot\/os_cpu\/bsd_zero\/atomic_bsd_zero.hpp","additions":22,"deletions":3,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"atomic_bsd_zero.hpp\"\n@@ -298,1 +299,1 @@\n-        os::atomic_copy64(from++, to++);\n+        atomic_copy64(from++, to++);\n@@ -305,1 +306,1 @@\n-        os::atomic_copy64(from--, to--);\n+        atomic_copy64(from--, to--);\n","filename":"src\/hotspot\/os_cpu\/bsd_zero\/os_bsd_zero.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -35,19 +35,0 @@\n-  \/\/ Atomically copy 64 bits of data\n-  static void atomic_copy64(const volatile void *src, volatile void *dst) {\n-#if defined(PPC32)\n-    double tmp;\n-    asm volatile (\"lfd  %0, 0(%1)\\n\"\n-                  \"stfd %0, 0(%2)\\n\"\n-                  : \"=f\"(tmp)\n-                  : \"b\"(src), \"b\"(dst));\n-#elif defined(S390) && !defined(_LP64)\n-    double tmp;\n-    asm volatile (\"ld  %0, 0(%1)\\n\"\n-                  \"std %0, 0(%2)\\n\"\n-                  : \"=r\"(tmp)\n-                  : \"a\"(src), \"a\"(dst));\n-#else\n-    *(jlong *) dst = *(const jlong *) src;\n-#endif\n-  }\n-\n","filename":"src\/hotspot\/os_cpu\/bsd_zero\/os_bsd_zero.hpp","additions":1,"deletions":20,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -390,0 +390,4 @@\n+static inline void atomic_copy64(const volatile void *src, volatile void *dst) {\n+  *(jlong *) dst = *(const jlong *) src;\n+}\n+\n@@ -436,0 +440,1 @@\n+\n@@ -440,1 +445,1 @@\n-        os::atomic_copy64(from++, to++);\n+        atomic_copy64(from++, to++);\n@@ -447,1 +452,1 @@\n-        os::atomic_copy64(from--, to--);\n+        atomic_copy64(from--, to--);\n","filename":"src\/hotspot\/os_cpu\/linux_aarch64\/os_linux_aarch64.cpp","additions":7,"deletions":2,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -39,5 +39,0 @@\n-  \/\/ Atomically copy 64 bits of data\n-  static void atomic_copy64(const volatile void *src, volatile void *dst) {\n-    *(jlong *) dst = *(const jlong *) src;\n-  }\n-\n","filename":"src\/hotspot\/os_cpu\/linux_aarch64\/os_linux_aarch64.hpp","additions":1,"deletions":6,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2008, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2008, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,0 +28,1 @@\n+#include \"memory\/allStatic.hpp\"\n@@ -33,0 +34,31 @@\n+class ARMAtomicFuncs : AllStatic {\n+public:\n+  typedef int64_t (*cmpxchg_long_func_t)(int64_t, int64_t, volatile int64_t*);\n+  typedef int64_t (*load_long_func_t)(const volatile int64_t*);\n+  typedef void (*store_long_func_t)(int64_t, volatile int64_t*);\n+  typedef int32_t  (*atomic_add_func_t)(int32_t add_value, volatile int32_t *dest);\n+  typedef int32_t  (*atomic_xchg_func_t)(int32_t exchange_value, volatile int32_t *dest);\n+  typedef int32_t (*cmpxchg_func_t)(int32_t, int32_t, volatile int32_t*);\n+\n+  static cmpxchg_long_func_t  _cmpxchg_long_func;\n+  static load_long_func_t     _load_long_func;\n+  static store_long_func_t    _store_long_func;\n+  static atomic_add_func_t    _add_func;\n+  static atomic_xchg_func_t   _xchg_func;\n+  static cmpxchg_func_t       _cmpxchg_func;\n+\n+  static int64_t cmpxchg_long_bootstrap(int64_t, int64_t, volatile int64_t*);\n+\n+  static int64_t load_long_bootstrap(const volatile int64_t*);\n+\n+  static void store_long_bootstrap(int64_t, volatile int64_t*);\n+\n+  static int32_t  add_bootstrap(int32_t add_value, volatile int32_t *dest);\n+\n+  static int32_t  xchg_bootstrap(int32_t exchange_value, volatile int32_t *dest);\n+\n+  static int32_t  cmpxchg_bootstrap(int32_t compare_value,\n+                                    int32_t exchange_value,\n+                                    volatile int32_t *dest);\n+};\n+\n@@ -52,1 +84,1 @@\n-    (*os::atomic_load_long_func)(reinterpret_cast<const volatile int64_t*>(src)));\n+    (*ARMAtomicFuncs::_load_long_func)(reinterpret_cast<const volatile int64_t*>(src)));\n@@ -60,1 +92,1 @@\n-  (*os::atomic_store_long_func)(\n+  (*ARMAtomicFuncs::_store_long_func)(\n@@ -86,1 +118,1 @@\n-  return add_using_helper<int32_t>(os::atomic_add_func, dest, add_value);\n+  return add_using_helper<int32_t>(ARMAtomicFuncs::_add_func, dest, add_value);\n@@ -96,1 +128,1 @@\n-  return xchg_using_helper<int32_t>(os::atomic_xchg_func, dest, exchange_value);\n+  return xchg_using_helper<int32_t>(ARMAtomicFuncs::_xchg_func, dest, exchange_value);\n@@ -111,1 +143,1 @@\n-  return (*os::atomic_cmpxchg_func)(compare_value, exchange_value, dest);\n+  return (*ARMAtomicFuncs::_cmpxchg_func)(compare_value, exchange_value, dest);\n@@ -119,1 +151,1 @@\n-  return (*os::atomic_cmpxchg_long_func)(compare_value, exchange_value, dest);\n+  return (*ARMAtomicFuncs::_cmpxchg_long_func)(compare_value, exchange_value, dest);\n","filename":"src\/hotspot\/os_cpu\/linux_arm\/atomic_linux_arm.hpp","additions":39,"deletions":7,"binary":false,"changes":46,"status":"modified"},{"patch":"@@ -78,0 +78,7 @@\n+#ifndef __thumb__\n+enum {\n+  \/\/ Offset to add to frame::_fp when dealing with non-thumb C frames\n+  C_frame_offset =  -1,\n+};\n+#endif\n+\n@@ -157,1 +164,1 @@\n-        fp += os::C_frame_offset;\n+        fp += C_frame_offset;\n@@ -195,1 +202,1 @@\n-    return frame(fr->sender_sp(), fr->link() + os::C_frame_offset, pc);\n+    return frame(fr->sender_sp(), fr->link() + C_frame_offset, pc);\n@@ -214,1 +221,1 @@\n-  frame myframe((intptr_t*)os::current_stack_pointer(), fp + os::C_frame_offset,\n+  frame myframe((intptr_t*)os::current_stack_pointer(), fp + C_frame_offset,\n@@ -497,1 +504,0 @@\n-typedef int64_t cmpxchg_long_func_t(int64_t, int64_t, volatile int64_t*);\n@@ -499,1 +505,6 @@\n-cmpxchg_long_func_t* os::atomic_cmpxchg_long_func = os::atomic_cmpxchg_long_bootstrap;\n+ARMAtomicFuncs::cmpxchg_long_func_t ARMAtomicFuncs::_cmpxchg_long_func = ARMAtomicFuncs::cmpxchg_long_bootstrap;\n+ARMAtomicFuncs::load_long_func_t    ARMAtomicFuncs::_load_long_func    = ARMAtomicFuncs::load_long_bootstrap;\n+ARMAtomicFuncs::store_long_func_t   ARMAtomicFuncs::_store_long_func   = ARMAtomicFuncs::store_long_bootstrap;\n+ARMAtomicFuncs::atomic_add_func_t   ARMAtomicFuncs::_add_func          = ARMAtomicFuncs::add_bootstrap;\n+ARMAtomicFuncs::atomic_xchg_func_t  ARMAtomicFuncs::_xchg_func         = ARMAtomicFuncs::xchg_bootstrap;\n+ARMAtomicFuncs::cmpxchg_func_t      ARMAtomicFuncs::_cmpxchg_func      = ARMAtomicFuncs::cmpxchg_bootstrap;\n@@ -501,1 +512,1 @@\n-int64_t os::atomic_cmpxchg_long_bootstrap(int64_t compare_value, int64_t exchange_value, volatile int64_t* dest) {\n+int64_t ARMAtomicFuncs::cmpxchg_long_bootstrap(int64_t compare_value, int64_t exchange_value, volatile int64_t* dest) {\n@@ -503,1 +514,1 @@\n-  cmpxchg_long_func_t* func = CAST_TO_FN_PTR(cmpxchg_long_func_t*, StubRoutines::atomic_cmpxchg_long_entry());\n+  cmpxchg_long_func_t func = CAST_TO_FN_PTR(cmpxchg_long_func_t, StubRoutines::atomic_cmpxchg_long_entry());\n@@ -506,1 +517,1 @@\n-    os::atomic_cmpxchg_long_func = func;\n+    _cmpxchg_long_func = func;\n@@ -516,1 +527,0 @@\n-typedef int64_t load_long_func_t(const volatile int64_t*);\n@@ -518,3 +528,1 @@\n-load_long_func_t* os::atomic_load_long_func = os::atomic_load_long_bootstrap;\n-\n-int64_t os::atomic_load_long_bootstrap(const volatile int64_t* src) {\n+int64_t ARMAtomicFuncs::load_long_bootstrap(const volatile int64_t* src) {\n@@ -522,1 +530,1 @@\n-  load_long_func_t* func = CAST_TO_FN_PTR(load_long_func_t*, StubRoutines::atomic_load_long_entry());\n+  load_long_func_t func = CAST_TO_FN_PTR(load_long_func_t, StubRoutines::atomic_load_long_entry());\n@@ -525,1 +533,1 @@\n-    os::atomic_load_long_func = func;\n+    _load_long_func = func;\n@@ -534,5 +542,1 @@\n-typedef void store_long_func_t(int64_t, volatile int64_t*);\n-\n-store_long_func_t* os::atomic_store_long_func = os::atomic_store_long_bootstrap;\n-\n-void os::atomic_store_long_bootstrap(int64_t val, volatile int64_t* dest) {\n+void ARMAtomicFuncs::store_long_bootstrap(int64_t val, volatile int64_t* dest) {\n@@ -540,1 +544,1 @@\n-  store_long_func_t* func = CAST_TO_FN_PTR(store_long_func_t*, StubRoutines::atomic_store_long_entry());\n+  store_long_func_t func = CAST_TO_FN_PTR(store_long_func_t, StubRoutines::atomic_store_long_entry());\n@@ -543,1 +547,1 @@\n-    os::atomic_store_long_func = func;\n+    _store_long_func = func;\n@@ -551,7 +555,3 @@\n-typedef int32_t  atomic_add_func_t(int32_t add_value, volatile int32_t *dest);\n-\n-atomic_add_func_t * os::atomic_add_func = os::atomic_add_bootstrap;\n-\n-int32_t  os::atomic_add_bootstrap(int32_t add_value, volatile int32_t *dest) {\n-  atomic_add_func_t * func = CAST_TO_FN_PTR(atomic_add_func_t*,\n-                                            StubRoutines::atomic_add_entry());\n+int32_t ARMAtomicFuncs::add_bootstrap(int32_t add_value, volatile int32_t *dest) {\n+  atomic_add_func_t func = CAST_TO_FN_PTR(atomic_add_func_t,\n+                                          StubRoutines::atomic_add_entry());\n@@ -559,1 +559,1 @@\n-    os::atomic_add_func = func;\n+    _add_func = func;\n@@ -568,7 +568,3 @@\n-typedef int32_t  atomic_xchg_func_t(int32_t exchange_value, volatile int32_t *dest);\n-\n-atomic_xchg_func_t * os::atomic_xchg_func = os::atomic_xchg_bootstrap;\n-\n-int32_t  os::atomic_xchg_bootstrap(int32_t exchange_value, volatile int32_t *dest) {\n-  atomic_xchg_func_t * func = CAST_TO_FN_PTR(atomic_xchg_func_t*,\n-                                            StubRoutines::atomic_xchg_entry());\n+int32_t ARMAtomicFuncs::xchg_bootstrap(int32_t exchange_value, volatile int32_t *dest) {\n+  atomic_xchg_func_t func = CAST_TO_FN_PTR(atomic_xchg_func_t,\n+                                           StubRoutines::atomic_xchg_entry());\n@@ -576,1 +572,1 @@\n-    os::atomic_xchg_func = func;\n+    _xchg_func = func;\n@@ -585,5 +581,1 @@\n-typedef int32_t cmpxchg_func_t(int32_t, int32_t, volatile int32_t*);\n-\n-cmpxchg_func_t* os::atomic_cmpxchg_func = os::atomic_cmpxchg_bootstrap;\n-\n-int32_t os::atomic_cmpxchg_bootstrap(int32_t compare_value, int32_t exchange_value, volatile int32_t* dest) {\n+int32_t ARMAtomicFuncs::cmpxchg_bootstrap(int32_t compare_value, int32_t exchange_value, volatile int32_t* dest) {\n@@ -591,1 +583,1 @@\n-  cmpxchg_func_t* func = CAST_TO_FN_PTR(cmpxchg_func_t*, StubRoutines::atomic_cmpxchg_entry());\n+  cmpxchg_func_t func = CAST_TO_FN_PTR(cmpxchg_func_t, StubRoutines::atomic_cmpxchg_entry());\n@@ -594,1 +586,1 @@\n-    os::atomic_cmpxchg_func = func;\n+    _cmpxchg_func = func;\n","filename":"src\/hotspot\/os_cpu\/linux_arm\/os_linux_arm.cpp","additions":36,"deletions":44,"binary":false,"changes":80,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2008, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2008, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,7 +28,0 @@\n-#ifndef __thumb__\n-  enum {\n-    \/\/ Offset to add to frame::_fp when dealing with non-thumb C frames\n-    C_frame_offset =  -1,\n-  };\n-#endif\n-\n@@ -41,30 +34,0 @@\n-  static int64_t (*atomic_cmpxchg_long_func)(int64_t compare_value,\n-                                             int64_t exchange_value,\n-                                             volatile int64_t *dest);\n-\n-  static int64_t (*atomic_load_long_func)(const volatile int64_t*);\n-\n-  static void (*atomic_store_long_func)(int64_t, volatile int64_t*);\n-\n-  static int32_t  (*atomic_add_func)(int32_t add_value, volatile int32_t *dest);\n-\n-  static int32_t  (*atomic_xchg_func)(int32_t exchange_value, volatile int32_t *dest);\n-\n-  static int32_t  (*atomic_cmpxchg_func)(int32_t compare_value,\n-                                         int32_t exchange_value,\n-                                         volatile int32_t *dest);\n-\n-  static int64_t atomic_cmpxchg_long_bootstrap(int64_t, int64_t, volatile int64_t*);\n-\n-  static int64_t atomic_load_long_bootstrap(const volatile int64_t*);\n-\n-  static void atomic_store_long_bootstrap(int64_t, volatile int64_t*);\n-\n-  static int32_t  atomic_add_bootstrap(int32_t add_value, volatile int32_t *dest);\n-\n-  static int32_t  atomic_xchg_bootstrap(int32_t exchange_value, volatile int32_t *dest);\n-\n-  static int32_t  atomic_cmpxchg_bootstrap(int32_t compare_value,\n-                                           int32_t exchange_value,\n-                                           volatile int32_t *dest);\n-\n","filename":"src\/hotspot\/os_cpu\/linux_arm\/os_linux_arm.hpp","additions":1,"deletions":38,"binary":false,"changes":39,"status":"modified"},{"patch":"@@ -398,0 +398,4 @@\n+static inline void atomic_copy64(const volatile void *src, volatile void *dst) {\n+  *(jlong *) dst = *(const jlong *) src;\n+}\n+\n@@ -433,0 +437,1 @@\n+\n@@ -437,1 +442,1 @@\n-        os::atomic_copy64(from++, to++);\n+        atomic_copy64(from++, to++);\n@@ -444,1 +449,1 @@\n-        os::atomic_copy64(from--, to--);\n+        atomic_copy64(from--, to--);\n","filename":"src\/hotspot\/os_cpu\/linux_riscv\/os_linux_riscv.cpp","additions":7,"deletions":2,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -35,5 +35,0 @@\n-  \/\/ Atomically copy 64 bits of data\n-  static void atomic_copy64(const volatile void *src, volatile void *dst) {\n-    *(jlong *) dst = *(const jlong *) src;\n-  }\n-\n","filename":"src\/hotspot\/os_cpu\/linux_riscv\/os_linux_riscv.hpp","additions":1,"deletions":6,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -30,1 +30,0 @@\n-#include \"runtime\/os.hpp\"\n@@ -136,0 +135,43 @@\n+\/\/ Atomically copy 64 bits of data\n+inline void atomic_copy64(const volatile void *src, volatile void *dst) {\n+#if defined(PPC32) && !defined(__SPE__)\n+  double tmp;\n+  asm volatile (\"lfd  %0, %2\\n\"\n+                \"stfd %0, %1\\n\"\n+                : \"=&f\"(tmp), \"=Q\"(*(volatile double*)dst)\n+                : \"Q\"(*(volatile double*)src));\n+#elif defined(PPC32) && defined(__SPE__)\n+  long tmp;\n+  asm volatile (\"evldd  %0, %2\\n\"\n+                \"evstdd %0, %1\\n\"\n+                : \"=&r\"(tmp), \"=Q\"(*(volatile long*)dst)\n+                : \"Q\"(*(volatile long*)src));\n+#elif defined(S390) && !defined(_LP64)\n+  double tmp;\n+  asm volatile (\"ld  %0, %2\\n\"\n+                \"std %0, %1\\n\"\n+                : \"=&f\"(tmp), \"=Q\"(*(volatile double*)dst)\n+                : \"Q\"(*(volatile double*)src));\n+#elif defined(__ARM_ARCH_7A__)\n+  \/\/ The only way to perform the atomic 64-bit load\/store\n+  \/\/ is to use ldrexd\/strexd for both reads and writes.\n+  \/\/ For store, we need to have the matching (fake) load first.\n+  \/\/ Put clrex between exclusive ops on src and dst for clarity.\n+  uint64_t tmp_r, tmp_w;\n+  uint32_t flag_w;\n+  asm volatile (\"ldrexd %[tmp_r], [%[src]]\\n\"\n+                \"clrex\\n\"\n+                \"1:\\n\"\n+                \"ldrexd %[tmp_w], [%[dst]]\\n\"\n+                \"strexd %[flag_w], %[tmp_r], [%[dst]]\\n\"\n+                \"cmp    %[flag_w], 0\\n\"\n+                \"bne    1b\\n\"\n+                : [tmp_r] \"=&r\" (tmp_r), [tmp_w] \"=&r\" (tmp_w),\n+                  [flag_w] \"=&r\" (flag_w)\n+                : [src] \"r\" (src), [dst] \"r\" (dst)\n+                : \"cc\", \"memory\");\n+#else\n+  *(jlong *) dst = *(const jlong *) src;\n+#endif\n+}\n+\n@@ -141,1 +183,1 @@\n-  os::atomic_copy64(reinterpret_cast<const volatile int64_t*>(src), reinterpret_cast<volatile int64_t*>(&dest));\n+  atomic_copy64(reinterpret_cast<const volatile int64_t*>(src), reinterpret_cast<volatile int64_t*>(&dest));\n@@ -150,1 +192,1 @@\n-  os::atomic_copy64(reinterpret_cast<const volatile int64_t*>(&store_value), reinterpret_cast<volatile int64_t*>(dest));\n+  atomic_copy64(reinterpret_cast<const volatile int64_t*>(&store_value), reinterpret_cast<volatile int64_t*>(dest));\n","filename":"src\/hotspot\/os_cpu\/linux_zero\/atomic_linux_zero.hpp","additions":46,"deletions":4,"binary":false,"changes":50,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"atomic_linux_zero.hpp\"\n@@ -342,1 +343,1 @@\n-        os::atomic_copy64(from++, to++);\n+        atomic_copy64(from++, to++);\n@@ -349,1 +350,1 @@\n-        os::atomic_copy64(from--, to--);\n+        atomic_copy64(from--, to--);\n","filename":"src\/hotspot\/os_cpu\/linux_zero\/os_linux_zero.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -48,43 +48,0 @@\n-  \/\/ Atomically copy 64 bits of data\n-  static void atomic_copy64(const volatile void *src, volatile void *dst) {\n-#if defined(PPC32) && !defined(__SPE__)\n-    double tmp;\n-    asm volatile (\"lfd  %0, %2\\n\"\n-                  \"stfd %0, %1\\n\"\n-                  : \"=&f\"(tmp), \"=Q\"(*(volatile double*)dst)\n-                  : \"Q\"(*(volatile double*)src));\n-#elif defined(PPC32) && defined(__SPE__)\n-    long tmp;\n-    asm volatile (\"evldd  %0, %2\\n\"\n-                  \"evstdd %0, %1\\n\"\n-                  : \"=&r\"(tmp), \"=Q\"(*(volatile long*)dst)\n-                  : \"Q\"(*(volatile long*)src));\n-#elif defined(S390) && !defined(_LP64)\n-    double tmp;\n-    asm volatile (\"ld  %0, %2\\n\"\n-                  \"std %0, %1\\n\"\n-                  : \"=&f\"(tmp), \"=Q\"(*(volatile double*)dst)\n-                  : \"Q\"(*(volatile double*)src));\n-#elif defined(__ARM_ARCH_7A__)\n-    \/\/ The only way to perform the atomic 64-bit load\/store\n-    \/\/ is to use ldrexd\/strexd for both reads and writes.\n-    \/\/ For store, we need to have the matching (fake) load first.\n-    \/\/ Put clrex between exclusive ops on src and dst for clarity.\n-    uint64_t tmp_r, tmp_w;\n-    uint32_t flag_w;\n-    asm volatile (\"ldrexd %[tmp_r], [%[src]]\\n\"\n-                  \"clrex\\n\"\n-                  \"1:\\n\"\n-                  \"ldrexd %[tmp_w], [%[dst]]\\n\"\n-                  \"strexd %[flag_w], %[tmp_r], [%[dst]]\\n\"\n-                  \"cmp    %[flag_w], 0\\n\"\n-                  \"bne    1b\\n\"\n-                  : [tmp_r] \"=&r\" (tmp_r), [tmp_w] \"=&r\" (tmp_w),\n-                    [flag_w] \"=&r\" (flag_w)\n-                  : [src] \"r\" (src), [dst] \"r\" (dst)\n-                  : \"cc\", \"memory\");\n-#else\n-    *(jlong *) dst = *(const jlong *) src;\n-#endif\n-  }\n-\n","filename":"src\/hotspot\/os_cpu\/linux_zero\/os_linux_zero.hpp","additions":1,"deletions":44,"binary":false,"changes":45,"status":"modified"}]}