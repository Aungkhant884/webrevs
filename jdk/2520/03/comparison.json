{"files":[{"patch":"@@ -3008,0 +3008,10 @@\n+void MacroAssembler::vpaddb(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register rscratch) {\n+  assert(UseAVX > 0, \"requires some form of AVX\");\n+  if (reachable(src)) {\n+    Assembler::vpaddb(dst, nds, as_Address(src), vector_len);\n+  } else {\n+    lea(rscratch, src);\n+    Assembler::vpaddb(dst, nds, Address(rscratch, 0), vector_len);\n+  }\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":10,"deletions":0,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -1248,0 +1248,1 @@\n+  void vpaddb(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register rscratch);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -613,0 +613,15 @@\n+  address generate_vector_byte_shuffle_mask(const char *stub_name) {\n+    __ align(CodeEntryAlignment);\n+    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n+    address start = __ pc();\n+    __ emit_data(0x70707070, relocInfo::none, 0);\n+    __ emit_data(0x70707070, relocInfo::none, 0);\n+    __ emit_data(0x70707070, relocInfo::none, 0);\n+    __ emit_data(0x70707070, relocInfo::none, 0);\n+    __ emit_data(0xF0F0F0F0, relocInfo::none, 0);\n+    __ emit_data(0xF0F0F0F0, relocInfo::none, 0);\n+    __ emit_data(0xF0F0F0F0, relocInfo::none, 0);\n+    __ emit_data(0xF0F0F0F0, relocInfo::none, 0);\n+    return start;\n+  }\n+\n@@ -3984,0 +3999,1 @@\n+    StubRoutines::x86::_vector_byte_shuffle_mask = generate_vector_byte_shuffle_mask(\"vector_byte_shuffle_mask\");\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_32.cpp","additions":17,"deletions":1,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -811,0 +811,11 @@\n+  address generate_vector_byte_shuffle_mask(const char *stub_name) {\n+    __ align(CodeEntryAlignment);\n+    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n+    address start = __ pc();\n+    __ emit_data64(0x7070707070707070, relocInfo::none);\n+    __ emit_data64(0x7070707070707070, relocInfo::none);\n+    __ emit_data64(0xF0F0F0F0F0F0F0F0, relocInfo::none);\n+    __ emit_data64(0xF0F0F0F0F0F0F0F0, relocInfo::none);\n+    return start;\n+  }\n+\n@@ -6831,0 +6842,1 @@\n+    StubRoutines::x86::_vector_byte_shuffle_mask = generate_vector_byte_shuffle_mask(\"vector_byte_shuffle_mask\");\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.cpp","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2013, 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2013, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -50,0 +50,1 @@\n+address StubRoutines::x86::_vector_byte_shuffle_mask = NULL;\n","filename":"src\/hotspot\/cpu\/x86\/stubRoutines_x86.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2013, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2013, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -152,0 +152,1 @@\n+  static address _vector_byte_shuffle_mask;\n@@ -283,0 +284,4 @@\n+  static address vector_byte_shuffle_mask() {\n+    return _vector_byte_shuffle_mask;\n+  }\n+\n","filename":"src\/hotspot\/cpu\/x86\/stubRoutines_x86.hpp","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -1359,0 +1359,1 @@\n+  static address vector_byte_shufflemask() { return StubRoutines::x86::vector_byte_shuffle_mask(); }\n@@ -1696,1 +1697,1 @@\n-      } else if (bt == T_BYTE && size_in_bits >= 256 && !VM_Version::supports_avx512_vbmi())  {\n+      } else if (bt == T_BYTE && size_in_bits > 256 && !VM_Version::supports_avx512_vbmi())  {\n@@ -1698,1 +1699,1 @@\n-      } else if (bt == T_SHORT && size_in_bits >= 256 && !VM_Version::supports_avx512bw())  {\n+      } else if (bt == T_SHORT && size_in_bits > 256 && !VM_Version::supports_avx512bw())  {\n@@ -7503,1 +7504,1 @@\n-instruct rearrangeB_avx(vec dst, vec src, vec shuffle) %{\n+instruct rearrangeB_avx(legVec dst, legVec src, vec shuffle, legVec vtmp1, legVec vtmp2, rRegP scratch) %{\n@@ -7507,1 +7508,2 @@\n-  format %{ \"vector_rearrange $dst, $shuffle, $src\" %}\n+  effect(TEMP dst, TEMP vtmp1, TEMP vtmp2, TEMP scratch);\n+  format %{ \"vector_rearrange $dst, $shuffle, $src\\t! using $vtmp1, $vtmp2, $scratch as TEMP\" %}\n@@ -7509,1 +7511,11 @@\n-    __ vpshufb($dst$$XMMRegister, $shuffle$$XMMRegister, $src$$XMMRegister, Assembler::AVX_256bit);\n+    assert(UseAVX >= 2, \"required\");\n+    \/\/ Swap src into vtmp1\n+    __ vperm2i128($vtmp1$$XMMRegister, $src$$XMMRegister, $src$$XMMRegister, 1);\n+    \/\/ Shuffle swapped src to get entries from other 128 bit lane\n+    __ vpshufb($vtmp1$$XMMRegister, $vtmp1$$XMMRegister, $shuffle$$XMMRegister, Assembler::AVX_256bit);\n+    \/\/ Shuffle original src to get entries from self 128 bit lane\n+    __ vpshufb($dst$$XMMRegister, $src$$XMMRegister, $shuffle$$XMMRegister, Assembler::AVX_256bit);\n+    \/\/ Create a blend mask by setting high bits for entries coming from other lane in shuffle\n+    __ vpaddb($vtmp2$$XMMRegister, $shuffle$$XMMRegister, ExternalAddress(vector_byte_shufflemask()), Assembler::AVX_256bit, $scratch$$Register);\n+    \/\/ Perform the blend\n+    __ vpblendvb($dst$$XMMRegister, $dst$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister, Assembler::AVX_256bit);\n@@ -7530,1 +7542,1 @@\n-            vector_length(n) <= 8 && !VM_Version::supports_avx512bw()); \/\/ NB! aligned with rearrangeS\n+            vector_length(n) <= 16 && !VM_Version::supports_avx512bw()); \/\/ NB! aligned with rearrangeS\n@@ -7537,0 +7549,21 @@\n+    int vlen_in_bytes = vector_length_in_bytes(this);\n+    if (UseAVX == 0) {\n+      assert(vlen_in_bytes <= 16, \"required\");\n+      \/\/ Multiply each shuffle by two to get byte index\n+      __ pmovzxbw($vtmp$$XMMRegister, $src$$XMMRegister);\n+      __ psllw($vtmp$$XMMRegister, 1);\n+\n+      \/\/ Duplicate to create 2 copies of byte index\n+      __ movdqu($dst$$XMMRegister, $vtmp$$XMMRegister);\n+      __ psllw($dst$$XMMRegister, 8);\n+      __ por($dst$$XMMRegister, $vtmp$$XMMRegister);\n+\n+      \/\/ Add one to get alternate byte index\n+      __ movdqu($vtmp$$XMMRegister, ExternalAddress(vector_short_shufflemask()), $scratch$$Register);\n+      __ paddb($dst$$XMMRegister, $vtmp$$XMMRegister);\n+    } else {\n+      assert(UseAVX > 1 || vlen_in_bytes <= 16, \"required\");\n+      int vlen_enc = vector_length_encoding(this);\n+      \/\/ Multiply each shuffle by two to get byte index\n+      __ vpmovzxbw($vtmp$$XMMRegister, $src$$XMMRegister, vlen_enc);\n+      __ vpsllw($vtmp$$XMMRegister, $vtmp$$XMMRegister, 1, vlen_enc);\n@@ -7538,8 +7571,3 @@\n-    \/\/ Multiply each shuffle by two to get byte index\n-    __ pmovzxbw($vtmp$$XMMRegister, $src$$XMMRegister);\n-    __ psllw($vtmp$$XMMRegister, 1);\n-\n-    \/\/ Duplicate to create 2 copies of byte index\n-    __ movdqu($dst$$XMMRegister, $vtmp$$XMMRegister);\n-    __ psllw($dst$$XMMRegister, 8);\n-    __ por($dst$$XMMRegister, $vtmp$$XMMRegister);\n+      \/\/ Duplicate to create 2 copies of byte index\n+      __ vpsllw($dst$$XMMRegister, $vtmp$$XMMRegister,  8, vlen_enc);\n+      __ vpor($dst$$XMMRegister, $dst$$XMMRegister, $vtmp$$XMMRegister, vlen_enc);\n@@ -7547,3 +7575,3 @@\n-    \/\/ Add one to get alternate byte index\n-    __ movdqu($vtmp$$XMMRegister, ExternalAddress(vector_short_shufflemask()), $scratch$$Register);\n-    __ paddb($dst$$XMMRegister, $vtmp$$XMMRegister);\n+      \/\/ Add one to get alternate byte index\n+      __ vpaddb($dst$$XMMRegister, $dst$$XMMRegister, ExternalAddress(vector_short_shufflemask()), vlen_enc, $scratch$$Register);\n+    }\n@@ -7566,0 +7594,22 @@\n+instruct rearrangeS_avx(legVec dst, legVec src, vec shuffle, legVec vtmp1, legVec vtmp2, rRegP scratch) %{\n+  predicate(vector_element_basic_type(n) == T_SHORT &&\n+            vector_length(n) == 16 && !VM_Version::supports_avx512bw());\n+  match(Set dst (VectorRearrange src shuffle));\n+  effect(TEMP dst, TEMP vtmp1, TEMP vtmp2, TEMP scratch);\n+  format %{ \"vector_rearrange $dst, $shuffle, $src\\t! using $vtmp1, $vtmp2, $scratch as TEMP\" %}\n+  ins_encode %{\n+    assert(UseAVX >= 2, \"required\");\n+    \/\/ Swap src into vtmp1\n+    __ vperm2i128($vtmp1$$XMMRegister, $src$$XMMRegister, $src$$XMMRegister, 1);\n+    \/\/ Shuffle swapped src to get entries from other 128 bit lane\n+    __ vpshufb($vtmp1$$XMMRegister, $vtmp1$$XMMRegister, $shuffle$$XMMRegister, Assembler::AVX_256bit);\n+    \/\/ Shuffle original src to get entries from self 128 bit lane\n+    __ vpshufb($dst$$XMMRegister, $src$$XMMRegister, $shuffle$$XMMRegister, Assembler::AVX_256bit);\n+    \/\/ Create a blend mask by setting high bits for entries coming from other lane in shuffle\n+    __ vpaddb($vtmp2$$XMMRegister, $shuffle$$XMMRegister, ExternalAddress(vector_byte_shufflemask()), Assembler::AVX_256bit, $scratch$$Register);\n+    \/\/ Perform the blend\n+    __ vpblendvb($dst$$XMMRegister, $dst$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister, Assembler::AVX_256bit);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n","filename":"src\/hotspot\/cpu\/x86\/x86.ad","additions":67,"deletions":17,"binary":false,"changes":84,"status":"modified"}]}