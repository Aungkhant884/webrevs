{"files":[{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -35,1 +35,1 @@\n-#include \"gc\/g1\/g1FullGCPrepareTask.hpp\"\n+#include \"gc\/g1\/g1FullGCPrepareTask.inline.hpp\"\n@@ -300,1 +300,24 @@\n-  GCTraceTime(Info, gc, phases) info(\"Phase 2: Prepare for compaction\", scope()->timer());\n+  GCTraceTime(Info, gc, phases) info(\"Phase 2: Prepare compaction\", scope()->timer());\n+\n+  bool found_new_empty_regions = phase2a_determine_worklists();\n+\n+  bool has_free_compaction_targets = phase2b_forward_oops();\n+\n+  \/\/ To avoid OOM when there is memory left.\n+  if (!found_new_empty_regions && !has_free_compaction_targets) {\n+    phase2c_prepare_serial_compaction();\n+  }\n+}\n+\n+bool G1FullCollector::phase2a_determine_worklists() {\n+  GCTraceTime(Debug, gc, phases) debug(\"Phase 2: Determine work lists\", scope()->timer());\n+\n+  G1DetermineCompactionQueueClosure cl(this);\n+  _heap->heap_region_iterate(&cl);\n+\n+  return cl.found_empty_regions();\n+}\n+\n+bool G1FullCollector::phase2b_forward_oops() {\n+  GCTraceTime(Debug, gc, phases) debug(\"Phase 2: Prepare parallel compaction\", scope()->timer());\n+\n@@ -304,3 +327,32 @@\n-  \/\/ To avoid OOM when there is memory left.\n-  if (!task.has_freed_regions()) {\n-    task.prepare_serial_compaction();\n+  return task.has_free_compaction_targets();\n+}\n+\n+void G1FullCollector::phase2c_prepare_serial_compaction() {\n+  GCTraceTime(Debug, gc, phases) debug(\"Phase 2: Prepare serial compaction\", scope()->timer());\n+  \/\/ At this point we know that after parallel compaction there will be no\n+  \/\/ completely free regions. That means that the last region of\n+  \/\/ all compaction queues still have data in them. We try to compact\n+  \/\/ these regions in serial to avoid a premature OOM when the mutator wants\n+  \/\/ to allocate the first eden region after gc.\n+  for (uint i = 0; i < workers(); i++) {\n+    G1FullGCCompactionPoint* cp = compaction_point(i);\n+    if (cp->has_regions()) {\n+      serial_compaction_point()->add(cp->remove_last());\n+    }\n+  }\n+\n+  \/\/ Update the forwarding information for the regions in the serial\n+  \/\/ compaction point.\n+  G1FullGCCompactionPoint* cp = serial_compaction_point();\n+  for (GrowableArrayIterator<HeapRegion*> it = cp->regions()->begin(); it != cp->regions()->end(); ++it) {\n+    HeapRegion* current = *it;\n+    if (!cp->is_initialized()) {\n+      \/\/ Initialize the compaction point. Nothing more is needed for the first heap region\n+      \/\/ since it is already prepared for compaction.\n+      cp->initialize(current, false);\n+    } else {\n+      assert(!current->is_humongous(), \"Should be no humongous regions in compaction queue\");\n+      G1SerialRePrepareClosure re_prepare(cp, current);\n+      current->set_compaction_top(current->bottom());\n+      current->apply_to_marked_objects(mark_bitmap(), &re_prepare);\n+    }\n@@ -308,0 +360,1 @@\n+  cp->update();\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullCollector.cpp","additions":59,"deletions":6,"binary":false,"changes":65,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -113,1 +113,1 @@\n-  size_t live_words(uint region_index) {\n+  size_t live_words(uint region_index) const {\n@@ -124,0 +124,3 @@\n+  \/\/ Are we (potentially) going to compact into this region?\n+  inline bool is_compaction_target(uint region_index) const;\n+\n@@ -131,0 +134,5 @@\n+\n+  bool phase2a_determine_worklists();\n+  bool phase2b_forward_oops();\n+  void phase2c_prepare_serial_compaction();\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullCollector.hpp","additions":10,"deletions":2,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -46,0 +46,4 @@\n+bool G1FullCollector::is_compaction_target(uint region_index) const {\n+  return _region_attr_table.is_compacting(region_index) || is_free(region_index);\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullCollector.inline.hpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -73,0 +73,4 @@\n+  bool is_compacting(uint idx) const {\n+    return get_by_index(idx) == Compacting;\n+  }\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCHeapRegionAttr.hpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -26,1 +26,1 @@\n-#include \"gc\/g1\/g1CollectedHeap.hpp\"\n+#include \"gc\/g1\/g1CollectedHeap.inline.hpp\"\n@@ -32,1 +32,1 @@\n-#include \"gc\/g1\/g1FullGCPrepareTask.hpp\"\n+#include \"gc\/g1\/g1FullGCPrepareTask.inline.hpp\"\n@@ -42,11 +42,5 @@\n-template<bool is_humongous>\n-void G1FullGCPrepareTask::G1CalculatePointersClosure::free_pinned_region(HeapRegion* hr) {\n-  _regions_freed = true;\n-  if (is_humongous) {\n-    _g1h->free_humongous_region(hr, nullptr);\n-  } else {\n-    _g1h->free_region(hr, nullptr);\n-  }\n-  _collector->set_free(hr->hrm_index());\n-  prepare_for_compaction(hr);\n-}\n+G1DetermineCompactionQueueClosure::G1DetermineCompactionQueueClosure(G1FullCollector* collector) :\n+  _g1h(G1CollectedHeap::heap()),\n+  _collector(collector),\n+  _cur_worker(0),\n+  _found_empty_regions(false) { }\n@@ -55,37 +49,2 @@\n-  if (should_compact(hr)) {\n-    assert(!hr->is_humongous(), \"moving humongous objects not supported.\");\n-    prepare_for_compaction(hr);\n-  } else {\n-    \/\/ There is no need to iterate and forward objects in pinned regions ie.\n-    \/\/ prepare them for compaction. The adjust pointers phase will skip\n-    \/\/ work for them.\n-    assert(hr->containing_set() == nullptr, \"already cleared by PrepareRegionsClosure\");\n-    if (hr->is_humongous()) {\n-      oop obj = cast_to_oop(hr->humongous_start_region()->bottom());\n-      if (!_bitmap->is_marked(obj)) {\n-        free_pinned_region<true>(hr);\n-      }\n-    } else if (hr->is_open_archive()) {\n-      bool is_empty = _collector->live_words(hr->hrm_index()) == 0;\n-      if (is_empty) {\n-        free_pinned_region<false>(hr);\n-      }\n-    } else if (hr->is_closed_archive()) {\n-      \/\/ nothing to do with closed archive region\n-    } else {\n-      assert(MarkSweepDeadRatio > 0,\n-             \"only skip compaction for other regions when MarkSweepDeadRatio > 0\");\n-\n-      \/\/ Too many live objects; skip compacting it.\n-      _collector->update_from_compacting_to_skip_compacting(hr->hrm_index());\n-      if (hr->is_young()) {\n-        \/\/ G1 updates the BOT for old region contents incrementally, but young regions\n-        \/\/ lack BOT information for performance reasons.\n-        \/\/ Recreate BOT information of high live ratio young regions here to keep expected\n-        \/\/ performance during scanning their card tables in the collection pauses later.\n-        hr->update_bot();\n-      }\n-      log_trace(gc, phases)(\"Phase 2: skip compaction region index: %u, live words: \" SIZE_FORMAT,\n-                            hr->hrm_index(), _collector->live_words(hr->hrm_index()));\n-    }\n-  }\n+  uint region_idx = hr->hrm_index();\n+  assert(_collector->is_compaction_target(region_idx), \"must be\");\n@@ -93,2 +52,5 @@\n-  \/\/ Reset data structures not valid after Full GC.\n-  reset_region_metadata(hr);\n+  assert(!hr->is_pinned(), \"must be\");\n+  assert(!hr->is_closed_archive(), \"must be\");\n+  assert(!hr->is_open_archive(), \"must be\");\n+\n+  prepare_for_compaction(hr);\n@@ -101,1 +63,1 @@\n-    _freed_regions(false),\n+    _has_free_compaction_targets(false),\n@@ -105,3 +67,3 @@\n-void G1FullGCPrepareTask::set_freed_regions() {\n-  if (!_freed_regions) {\n-    _freed_regions = true;\n+void G1FullGCPrepareTask::set_has_free_compaction_targets() {\n+  if (!_has_free_compaction_targets) {\n+    _has_free_compaction_targets = true;\n@@ -111,2 +73,2 @@\n-bool G1FullGCPrepareTask::has_freed_regions() {\n-  return _freed_regions;\n+bool G1FullGCPrepareTask::has_free_compaction_targets() {\n+  return _has_free_compaction_targets;\n@@ -117,5 +79,21 @@\n-  G1FullGCCompactionPoint* compaction_point = collector()->compaction_point(worker_id);\n-  G1CalculatePointersClosure closure(collector(), compaction_point);\n-  G1CollectedHeap::heap()->heap_region_par_iterate_from_start(&closure, &_hrclaimer);\n-\n-  compaction_point->update();\n+  \/\/ Calculate the target locations for the objects in the non-free regions of\n+  \/\/ the compaction queues provided by the associate compaction point.\n+  {\n+    G1FullGCCompactionPoint* compaction_point = collector()->compaction_point(worker_id);\n+    G1CalculatePointersClosure closure(collector(), compaction_point);\n+\n+    for (GrowableArrayIterator<HeapRegion*> it = compaction_point->regions()->begin();\n+         it != compaction_point->regions()->end();\n+         ++it) {\n+      closure.do_heap_region(*it);\n+    }\n+    compaction_point->update();\n+    \/\/ Determine if there are any unused compaction targets. This is only the case if\n+    \/\/ there are\n+    \/\/ - any regions in queue, so no free ones either.\n+    \/\/ - and the current region is not the last one in the list.\n+    if (compaction_point->has_regions() &&\n+        compaction_point->current_region() != compaction_point->regions()->last()) {\n+      set_has_free_compaction_targets();\n+    }\n+  }\n@@ -123,3 +101,4 @@\n-  \/\/ Check if any regions was freed by this worker and store in task.\n-  if (closure.freed_regions()) {\n-    set_freed_regions();\n+  \/\/ Clear region metadata that is invalid after GC for all regions.\n+  {\n+    G1ResetMetadataClosure closure(collector());\n+    G1CollectedHeap::heap()->heap_region_par_iterate_from_start(&closure, &_hrclaimer);\n@@ -132,15 +111,4 @@\n-    _g1h(G1CollectedHeap::heap()),\n-    _collector(collector),\n-    _bitmap(collector->mark_bitmap()),\n-    _cp(cp),\n-    _regions_freed(false) { }\n-\n-bool G1FullGCPrepareTask::G1CalculatePointersClosure::should_compact(HeapRegion* hr) {\n-  if (hr->is_pinned()) {\n-    return false;\n-  }\n-  size_t live_words = _collector->live_words(hr->hrm_index());\n-  size_t live_words_threshold = _collector->scope()->region_compaction_threshold();\n-  \/\/ High live ratio region will not be compacted.\n-  return live_words <= live_words_threshold;\n-}\n+  _g1h(G1CollectedHeap::heap()),\n+  _collector(collector),\n+  _bitmap(collector->mark_bitmap()),\n+  _cp(cp) { }\n@@ -148,1 +116,5 @@\n-void G1FullGCPrepareTask::G1CalculatePointersClosure::reset_region_metadata(HeapRegion* hr) {\n+G1FullGCPrepareTask::G1ResetMetadataClosure::G1ResetMetadataClosure(G1FullCollector* collector) :\n+  _g1h(G1CollectedHeap::heap()),\n+  _collector(collector) { }\n+\n+void G1FullGCPrepareTask::G1ResetMetadataClosure::reset_region_metadata(HeapRegion* hr) {\n@@ -158,0 +130,20 @@\n+bool G1FullGCPrepareTask::G1ResetMetadataClosure::do_heap_region(HeapRegion* hr) {\n+  uint const region_idx = hr->hrm_index();\n+  if (!_collector->is_compaction_target(region_idx)) {\n+    assert(!hr->is_free(), \"all free regions should be compaction targets\");\n+    assert(_collector->is_skip_compacting(region_idx) || hr->is_closed_archive(), \"must be\"); \/\/ FIXME: second clause\n+    if (hr->is_young()) {\n+      \/\/ G1 updates the BOT for old region contents incrementally, but young regions\n+      \/\/ lack BOT information for performance reasons.\n+      \/\/ Recreate BOT information of high live ratio young regions here to keep expected\n+      \/\/ performance during scanning their card tables in the collection pauses later.\n+      hr->update_bot();\n+    }\n+  }\n+\n+  \/\/ Reset data structures not valid after Full GC.\n+  reset_region_metadata(hr);\n+\n+  return false;\n+}\n+\n@@ -167,17 +159,1 @@\n-size_t G1FullGCPrepareTask::G1RePrepareClosure::apply(oop obj) {\n-  \/\/ We only re-prepare objects forwarded within the current region, so\n-  \/\/ skip objects that are already forwarded to another region.\n-  if (obj->is_forwarded() && !_current->is_in(obj->forwardee())) {\n-    return obj->size();\n-  }\n-\n-  \/\/ Get size and forward.\n-  size_t size = obj->size();\n-  _cp->forward(obj, size);\n-\n-  return size;\n-}\n-\n-void G1FullGCPrepareTask::G1CalculatePointersClosure::prepare_for_compaction_work(G1FullGCCompactionPoint* cp,\n-                                                                                  HeapRegion* hr) {\n-  hr->set_compaction_top(hr->bottom());\n+void G1FullGCPrepareTask::G1CalculatePointersClosure::prepare_for_compaction(HeapRegion* hr) {\n@@ -185,1 +161,1 @@\n-    G1PrepareCompactLiveClosure prepare_compact(cp);\n+    G1PrepareCompactLiveClosure prepare_compact(_cp);\n@@ -189,62 +165,0 @@\n-\n-void G1FullGCPrepareTask::G1CalculatePointersClosure::prepare_for_compaction(HeapRegion* hr) {\n-  if (!_cp->is_initialized()) {\n-    hr->set_compaction_top(hr->bottom());\n-    _cp->initialize(hr, true);\n-  }\n-  \/\/ Add region to the compaction queue and prepare it.\n-  _cp->add(hr);\n-  prepare_for_compaction_work(_cp, hr);\n-}\n-\n-void G1FullGCPrepareTask::prepare_serial_compaction() {\n-  GCTraceTime(Debug, gc, phases) debug(\"Phase 2: Prepare Serial Compaction\", collector()->scope()->timer());\n-  \/\/ At this point we know that no regions were completely freed by\n-  \/\/ the parallel compaction. That means that the last region of\n-  \/\/ all compaction queues still have data in them. We try to compact\n-  \/\/ these regions in serial to avoid a premature OOM.\n-  for (uint i = 0; i < collector()->workers(); i++) {\n-    G1FullGCCompactionPoint* cp = collector()->compaction_point(i);\n-    if (cp->has_regions()) {\n-      collector()->serial_compaction_point()->add(cp->remove_last());\n-    }\n-  }\n-\n-  \/\/ Update the forwarding information for the regions in the serial\n-  \/\/ compaction point.\n-  G1FullGCCompactionPoint* cp = collector()->serial_compaction_point();\n-  for (GrowableArrayIterator<HeapRegion*> it = cp->regions()->begin(); it != cp->regions()->end(); ++it) {\n-    HeapRegion* current = *it;\n-    if (!cp->is_initialized()) {\n-      \/\/ Initialize the compaction point. Nothing more is needed for the first heap region\n-      \/\/ since it is already prepared for compaction.\n-      cp->initialize(current, false);\n-    } else {\n-      assert(!current->is_humongous(), \"Should be no humongous regions in compaction queue\");\n-      G1RePrepareClosure re_prepare(cp, current);\n-      current->set_compaction_top(current->bottom());\n-      current->apply_to_marked_objects(collector()->mark_bitmap(), &re_prepare);\n-    }\n-  }\n-  cp->update();\n-}\n-\n-bool G1FullGCPrepareTask::G1CalculatePointersClosure::freed_regions() {\n-  if (_regions_freed) {\n-    return true;\n-  }\n-\n-  if (!_cp->has_regions()) {\n-    \/\/ No regions in queue, so no free ones either.\n-    return false;\n-  }\n-\n-  if (_cp->current_region() != _cp->regions()->last()) {\n-    \/\/ The current region used for compaction is not the last in the\n-    \/\/ queue. That means there is at least one free region in the queue.\n-    return true;\n-  }\n-\n-  \/\/ No free regions in the queue.\n-  return false;\n-}\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCPrepareTask.cpp","additions":77,"deletions":163,"binary":false,"changes":240,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,2 +28,0 @@\n-#include \"gc\/g1\/g1FullGCCompactionPoint.hpp\"\n-#include \"gc\/g1\/g1FullGCScope.hpp\"\n@@ -31,3 +29,2 @@\n-#include \"gc\/g1\/g1RootProcessor.hpp\"\n-#include \"gc\/g1\/heapRegionManager.hpp\"\n-#include \"gc\/shared\/referenceProcessor.hpp\"\n+#include \"gc\/g1\/heapRegion.hpp\"\n+#include \"memory\/allocation.hpp\"\n@@ -35,0 +32,1 @@\n+class G1CollectedHeap;\n@@ -37,0 +35,31 @@\n+class G1FullGCCompactionPoint;\n+class HeapRegion;\n+\n+\/\/ Determines the regions in the heap that should be part of the compaction and\n+\/\/ distributes them among the compaction queues in round-robin fashion.\n+class G1DetermineCompactionQueueClosure : public HeapRegionClosure {\n+  G1CollectedHeap* _g1h;\n+  G1FullCollector* _collector;\n+  uint _cur_worker;\n+  bool _found_empty_regions;\n+\n+  template<bool is_humongous>\n+  inline void free_pinned_region(HeapRegion* hr);\n+\n+  inline bool should_compact(HeapRegion* hr) const;\n+\n+  \/\/ Returns the current worker id to assign a compaction point to, and selects\n+  \/\/ the next one round-robin style.\n+  inline uint next_worker();\n+\n+  inline G1FullGCCompactionPoint* next_compaction_point();\n+\n+  inline void add_to_compaction_queue(G1FullGCCompactionPoint* cp, HeapRegion* hr);\n+\n+public:\n+  G1DetermineCompactionQueueClosure(G1FullCollector* collector);\n+\n+  inline bool do_heap_region(HeapRegion* hr) override;\n+\n+  inline bool found_empty_regions() { return _found_empty_regions; }\n+};\n@@ -39,2 +68,1 @@\n-protected:\n-  volatile bool     _freed_regions;\n+  volatile bool     _has_free_compaction_targets;\n@@ -43,1 +71,1 @@\n-  void set_freed_regions();\n+  void set_has_free_compaction_targets();\n@@ -48,2 +76,3 @@\n-  void prepare_serial_compaction();\n-  bool has_freed_regions();\n+  \/\/ After the Prepare phase, are there any unused (empty) regions (compaction\n+  \/\/ targets) at the end of any compaction queues?\n+  bool has_free_compaction_targets();\n@@ -51,1 +80,1 @@\n-protected:\n+private:\n@@ -53,4 +82,0 @@\n-  private:\n-    template<bool is_humongous>\n-    void free_pinned_region(HeapRegion* hr);\n-  protected:\n@@ -61,1 +86,0 @@\n-    bool _regions_freed;\n@@ -63,1 +87,0 @@\n-    bool should_compact(HeapRegion* hr);\n@@ -65,3 +88,0 @@\n-    void prepare_for_compaction_work(G1FullGCCompactionPoint* cp, HeapRegion* hr);\n-\n-    void reset_region_metadata(HeapRegion* hr);\n@@ -74,1 +94,12 @@\n-    bool freed_regions();\n+  };\n+\n+  class G1ResetMetadataClosure : public HeapRegionClosure {\n+    G1CollectedHeap* _g1h;\n+    G1FullCollector* _collector;\n+\n+    void reset_region_metadata(HeapRegion* hr);\n+\n+  public:\n+    G1ResetMetadataClosure(G1FullCollector* collector);\n+\n+    bool do_heap_region(HeapRegion* hr);\n@@ -84,0 +115,1 @@\n+};\n@@ -85,3 +117,5 @@\n-  class G1RePrepareClosure : public StackObj {\n-    G1FullGCCompactionPoint* _cp;\n-    HeapRegion* _current;\n+\/\/ Closure to re-prepare objects in the serial compaction point queue regions for\n+\/\/ serial compaction.\n+class G1SerialRePrepareClosure : public StackObj {\n+  G1FullGCCompactionPoint* _cp;\n+  HeapRegion* _current;\n@@ -89,5 +123,4 @@\n-  public:\n-    G1RePrepareClosure(G1FullGCCompactionPoint* hrcp,\n-                       HeapRegion* hr) :\n-        _cp(hrcp),\n-        _current(hr) { }\n+public:\n+  G1SerialRePrepareClosure(G1FullGCCompactionPoint* hrcp, HeapRegion* hr) :\n+    _cp(hrcp),\n+    _current(hr) { }\n@@ -95,2 +128,1 @@\n-    size_t apply(oop object);\n-  };\n+  inline size_t apply(oop obj);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCPrepareTask.hpp","additions":64,"deletions":32,"binary":false,"changes":96,"status":"modified"},{"patch":"@@ -0,0 +1,126 @@\n+\/*\n+ * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_G1_G1FULLGCPREPARETASK_INLINE_HPP\n+#define SHARE_GC_G1_G1FULLGCPREPARETASK_INLINE_HPP\n+\n+#include \"gc\/g1\/g1FullGCPrepareTask.hpp\"\n+\n+#include \"gc\/g1\/g1CollectedHeap.inline.hpp\"\n+#include \"gc\/g1\/g1FullCollector.hpp\"\n+#include \"gc\/g1\/g1FullGCCompactionPoint.hpp\"\n+#include \"gc\/g1\/g1FullGCScope.hpp\"\n+#include \"gc\/g1\/heapRegion.inline.hpp\"\n+\n+template<bool is_humongous>\n+void G1DetermineCompactionQueueClosure::free_pinned_region(HeapRegion* hr) {\n+  _found_empty_regions = true;\n+  if (is_humongous) {\n+    _g1h->free_humongous_region(hr, nullptr);\n+  } else {\n+    _g1h->free_region(hr, nullptr);\n+  }\n+  _collector->set_free(hr->hrm_index());\n+  add_to_compaction_queue(next_compaction_point(), hr);\n+}\n+\n+inline bool G1DetermineCompactionQueueClosure::should_compact(HeapRegion* hr) const {\n+  \/\/ There is no need to iterate and forward objects in pinned regions ie.\n+  \/\/ prepare them for compaction.\n+  if (hr->is_pinned()) {\n+    return false;\n+  }\n+  size_t live_words = _collector->live_words(hr->hrm_index());\n+  size_t live_words_threshold = _collector->scope()->region_compaction_threshold();\n+  \/\/ High live ratio region will not be compacted.\n+  return live_words <= live_words_threshold;\n+}\n+\n+inline uint G1DetermineCompactionQueueClosure::next_worker() {\n+  uint result = _cur_worker;\n+  _cur_worker = (_cur_worker + 1) % _collector->workers();\n+  return result;\n+}\n+\n+inline G1FullGCCompactionPoint* G1DetermineCompactionQueueClosure::next_compaction_point() {\n+  return _collector->compaction_point(next_worker());\n+}\n+\n+inline void G1DetermineCompactionQueueClosure::add_to_compaction_queue(G1FullGCCompactionPoint* cp, HeapRegion* hr) {\n+  hr->set_compaction_top(hr->bottom());\n+  if (!cp->is_initialized()) {\n+    cp->initialize(hr, true);\n+  }\n+  \/\/ Add region to the compaction queue.\n+  cp->add(hr);\n+}\n+\n+inline bool G1DetermineCompactionQueueClosure::do_heap_region(HeapRegion* hr) {\n+  if (should_compact(hr)) {\n+    assert(!hr->is_humongous(), \"moving humongous objects not supported.\");\n+    add_to_compaction_queue(next_compaction_point(), hr);\n+  } else {\n+    assert(hr->containing_set() == nullptr, \"already cleared by PrepareRegionsClosure\");\n+    if (hr->is_humongous()) {\n+      oop obj = cast_to_oop(hr->humongous_start_region()->bottom());\n+      bool is_empty = !_collector->mark_bitmap()->is_marked(obj);\n+      if (is_empty) {\n+        free_pinned_region<true>(hr);\n+      }\n+    } else if (hr->is_open_archive()) {\n+      bool is_empty = _collector->live_words(hr->hrm_index()) == 0;\n+      if (is_empty) {\n+        free_pinned_region<false>(hr);\n+      }\n+    } else if (hr->is_closed_archive()) {\n+      \/\/ nothing to do with closed archive region\n+    } else {\n+      assert(MarkSweepDeadRatio > 0,\n+             \"only skip compaction for other regions when MarkSweepDeadRatio > 0\");\n+\n+      \/\/ Too many live objects in the region; skip compacting it.\n+      _collector->update_from_compacting_to_skip_compacting(hr->hrm_index());\n+      log_trace(gc, phases)(\"Phase 2: skip compaction region index: %u, live words: \" SIZE_FORMAT,\n+                            hr->hrm_index(), _collector->live_words(hr->hrm_index()));\n+    }\n+  }\n+\n+  return false;\n+}\n+\n+inline size_t G1SerialRePrepareClosure::apply(oop obj) {\n+  \/\/ We only re-prepare objects forwarded within the current region, so\n+  \/\/ skip objects that are already forwarded to another region.\n+  if (obj->is_forwarded() && !_current->is_in(obj->forwardee())) {\n+    return obj->size();\n+  }\n+\n+  \/\/ Get size and forward.\n+  size_t size = obj->size();\n+  _cp->forward(obj, size);\n+\n+  return size;\n+}\n+\n+#endif \/\/ SHARE_GC_G1_G1FULLGCPREPARETASK_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCPrepareTask.inline.hpp","additions":126,"deletions":0,"binary":false,"changes":126,"status":"added"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -73,1 +73,1 @@\n-size_t G1FullGCScope::region_compaction_threshold() {\n+size_t G1FullGCScope::region_compaction_threshold() const {\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCScope.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -74,1 +74,1 @@\n-  size_t region_compaction_threshold();\n+  size_t region_compaction_threshold() const;\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCScope.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"}]}