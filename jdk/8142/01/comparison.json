{"files":[{"patch":"@@ -2,2 +2,2 @@\n- * Copyright (c) 2016, 2021, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2016, 2019 SAP SE. All rights reserved.\n+ * Copyright (c) 2016, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2022 SAP SE. All rights reserved.\n@@ -4670,0 +4670,16 @@\n+void MacroAssembler::kmctr(Register dstBuff, Register ctrBuff, Register srcBuff) {\n+  \/\/ DstBuff and srcBuff are allowed to be the same register (encryption in-place).\n+  \/\/ DstBuff and srcBuff storage must not overlap destructively, and neither must overlap the parameter block.\n+  assert(srcBuff->encoding()     != 0, \"src buffer address can't be in Z_R0\");\n+  assert(dstBuff->encoding()     != 0, \"dst buffer address can't be in Z_R0\");\n+  assert(ctrBuff->encoding()     != 0, \"ctr buffer address can't be in Z_R0\");\n+  assert(ctrBuff->encoding() % 2 == 0, \"ctr buffer addr must be an even register\");\n+  assert(dstBuff->encoding() % 2 == 0, \"dst buffer addr must be an even register\");\n+  assert(srcBuff->encoding() % 2 == 0, \"src buffer addr\/len must be an even\/odd register pair\");\n+\n+  Label retry;\n+  bind(retry);\n+  Assembler::z_kmctr(dstBuff, ctrBuff, srcBuff);\n+  Assembler::z_brc(Assembler::bcondOverflow \/* CC==3 (iterate) *\/, retry);\n+}\n+\n","filename":"src\/hotspot\/cpu\/s390\/macroAssembler_s390.cpp","additions":18,"deletions":2,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -2,2 +2,2 @@\n- * Copyright (c) 2016, 2021, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2016, 2019 SAP SE. All rights reserved.\n+ * Copyright (c) 2016, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2022 SAP SE. All rights reserved.\n@@ -251,2 +251,3 @@\n-  void km( Register dstBuff, Register srcBuff);\n-  void kmc(Register dstBuff, Register srcBuff);\n+  void km(   Register dstBuff, Register srcBuff);\n+  void kmc(  Register dstBuff, Register srcBuff);\n+  void kmctr(Register dstBuff, Register ctrBuff, Register srcBuff);\n","filename":"src\/hotspot\/cpu\/s390\/macroAssembler_s390.hpp","additions":5,"deletions":4,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -45,0 +45,1 @@\n+#include \"utilities\/formatBuffer.hpp\"\n@@ -57,1 +58,1 @@\n-#define BLOCK_COMMENT(str) if (PrintAssembly) __ block_comment(str)\n+#define BLOCK_COMMENT(str) if (PrintAssembly || PrintStubCode) __ block_comment(str)\n@@ -60,0 +61,28 @@\n+\/\/ These static, partially const, variables are for the AES intrinsics.\n+\/\/ They are declared\/initialized here to make them available across function bodies.\n+\n+#if defined(JIT_TIMER)\n+    static const int JIT_TIMER_space      = 8;                   \/\/ extra space for JIT_TIMER data\n+#else\n+    static const int JIT_TIMER_space      = 0;\n+#endif\n+    static const int AES_parmBlk_align    = 32;                  \/\/ octoword alignment.\n+\n+    static int AES_ctrVal_len  = 0;                              \/\/ ctr init value len (in bytes), expected: length of dataBlk (16)\n+    static int AES_ctrVec_len  = 0;                              \/\/ # of ctr vector elements. That many block can be ciphered with one instruction execution\n+    static int AES_ctrArea_len = 0;                              \/\/ reserved stack space (in bytes) for ctr (= ctrVal_len * ctrVec_len)\n+\n+    static int AES_parmBlk_addspace = 0;  \/\/ Must be multiple of AES_parmblk_align.\n+                                          \/\/ Will be set by stub generator to stub specific value.\n+    static int AES_dataBlk_space    = 0;  \/\/ Must be multiple of AES_parmblk_align.\n+                                          \/\/ Will be set by stub generator to stub specific value.\n+\n+    static const int keylen_offset     =  -1;\n+    static const int fCode_offset      =  -2;\n+    static const int ctrVal_len_offset =  -4;\n+    static const int msglen_offset     =  -8;\n+    static const int unextSP_offset    = -16;\n+    static const int remmsg_len_offset = -20;\n+    static const int argsave_offset    = -2*AES_parmBlk_align;\n+    static const int localSpill_offset = argsave_offset + 24;  \/\/ arg2..arg4 are saved\n+\n@@ -1582,1 +1611,1 @@\n-  \/\/   :        :  alignment loss, 0..(AES_parmBlk_align-8) bytes\n+  \/\/   :        :  alignment loss (part 2), 0..(AES_parmBlk_align-1) bytes\n@@ -1592,0 +1621,4 @@\n+  \/\/   +--------+ <-- Z_SP + alignment loss, octoword-aligned\n+  \/\/   |        |\n+  \/\/   :        :  alignment loss (part 1), 0..(AES_parmBlk_align-1) bytes. DW @ Z_SP not usable!!!\n+  \/\/   |        |\n@@ -1596,3 +1629,0 @@\n-    const int AES_parmBlk_align    = 32;  \/\/ octoword alignment.\n-    const int AES_parmBlk_addspace = 24;  \/\/ Must be sufficiently large to hold all spilled registers\n-                                          \/\/ (currently 2) PLUS 1 DW for the frame pointer.\n@@ -1600,0 +1630,2 @@\n+    AES_parmBlk_addspace = AES_parmBlk_align; \/\/ Must be multiple of AES_parmblk_align.\n+                                              \/\/ spill space for regs etc., don't use DW @SP!\n@@ -1637,0 +1669,4 @@\n+    \/\/ We have just three cipher strengths which translates into three\n+    \/\/ possible extended key lengths: 44, 52, and 60 bytes.\n+    \/\/ We therefore can compare the actual length against the \"middle\" length\n+    \/\/ and get: lt -> len=44, eq -> len=52, gt -> len=60.\n@@ -1753,0 +1789,4 @@\n+    if (! VM_Version::has_Crypto_AES()) {\n+      __ should_not_reach_here();\n+    }\n+\n@@ -1820,0 +1860,4 @@\n+    if (! VM_Version::has_Crypto_AES()) {\n+      __ should_not_reach_here();\n+    }\n+\n@@ -1851,1 +1895,1 @@\n-  \/\/ Compute chained AES encrypt function.\n+  \/\/ Compute chained AES decrypt function.\n@@ -1862,0 +1906,556 @@\n+\/\/ *****************************************************************************\n+\n+  \/\/ AES CounterMode\n+  \/\/ Push a parameter block for the cipher\/decipher instruction on the stack.\n+  \/\/ Layout of the additional stack space allocated for counterMode_AES_cipherBlock\n+  \/\/\n+  \/\/   |        |\n+  \/\/   +--------+ <-- SP before expansion\n+  \/\/   |        | JIT_TIMER timestamp buffer, only if JIT_TIMER is defined.\n+  \/\/   +--------+\n+  \/\/   |        |\n+  \/\/   :        :  alignment loss (part 2), 0..(AES_parmBlk_align-1) bytes.\n+  \/\/   |        |\n+  \/\/   +--------+ <-- gap = parmBlk + parmBlk_len + ctrArea_len\n+  \/\/   |        |\n+  \/\/   :        :  byte[] ctr - kmctr expects a counter vector the size of the input vector.\n+  \/\/   :        :         The interface only provides byte[16] iv, the init vector.\n+  \/\/   :        :         The size of this area is a tradeoff between stack space, init effort, and speed.\n+  \/\/   |        |         Each counter is a 128bit int. Vector element i is formed by incrementing element (i-1).\n+  \/\/   +--------+ <-- ctr = parmBlk + parmBlk_len\n+  \/\/   |        |\n+  \/\/   :        :  space for parameter block, size VM_Version::Cipher::_AES*_parmBlk_G\n+  \/\/   |        |\n+  \/\/   +--------+ <-- parmBlk = Z_SP + (alignment loss (part 1+2)) + AES_dataBlk_space + AES_parmBlk_addSpace, octoword-aligned, start of parameter block\n+  \/\/   |        |\n+  \/\/   :        :  additional stack space for spills etc., min. size AES_parmBlk_addspace, all bytes usable.\n+  \/\/   |        |\n+  \/\/   +--------+ <-- Z_SP + alignment loss (part 1+2) + AES_dataBlk_space, octoword-aligned\n+  \/\/   |        |\n+  \/\/   :        :  space for one source data block and one dest data block.\n+  \/\/   |        |\n+  \/\/   +--------+ <-- Z_SP + alignment loss (part 1+2), octoword-aligned\n+  \/\/   |        |\n+  \/\/   :        :  additional alignment loss. Blocks above can't tolerate unusabe DW @SP.\n+  \/\/   |        |\n+  \/\/   +--------+ <-- Z_SP + alignment loss (part 1), octoword-aligned\n+  \/\/   |        |\n+  \/\/   :        :  alignment loss (part 1), 0..(AES_parmBlk_align-1) bytes. DW @ Z_SP holds frame ptr.\n+  \/\/   |        |\n+  \/\/   +--------+ <-- Z_SP after expansion\n+  \/\/\n+  \/\/   additional space allocation (per DW):\n+  \/\/    spillSpace = parmBlk - AES_parmBlk_addspace\n+  \/\/    dataBlocks = spillSpace - AES_dataBlk_space\n+  \/\/\n+  \/\/    parmBlk-8  various lengths\n+  \/\/                parmBlk-1: key_len (only one byte is stored at parmBlk-1)\n+  \/\/                parmBlk-2: fCode (only one byte is stored at parmBlk-2)\n+  \/\/                parmBlk-4: ctrVal_len (as retrieved from iv array), in bytes, as HW\n+  \/\/                parmBlk-8: msglen length (in bytes) of crypto msg, as passed in by caller\n+  \/\/                              return value is calculated from this: rv = msglen - processed.\n+  \/\/    parmBlk-16 old_SP (SP before resize)\n+  \/\/    parmBlk-24 temp values\n+  \/\/                up to and including main loop in generate_counterMode_AES\n+  \/\/                 - parmBlk-20: remmsg_len remaining msg len (aka unprocessed msg bytes)\n+  \/\/                after main loop in generate_counterMode_AES\n+  \/\/                 - parmBlk-24: spill slot for various address values\n+  \/\/\n+  \/\/    parmBlk-40 free spill slot, used for local spills.\n+  \/\/    parmBlk-64 ARG2(dst) ptr spill slot\n+  \/\/    parmBlk-56 ARG3(crypto key) ptr spill slot\n+  \/\/    parmBlk-48 ARG4(counter value) ptr spill slot\n+  \/\/\n+  \/\/\n+  \/\/ Layout of the parameter block (instruction KMCTR, function KMCTR-AES*\n+  \/\/\n+  \/\/   +--------+ key_len: +16 (AES-128), +24 (AES-192), +32 (AES-256)\n+  \/\/   |        |\n+  \/\/   |        |  cryptographic key\n+  \/\/   |        |\n+  \/\/   +--------+ <-- parmBlk\n+  \/\/\n+  \/\/ On exit:\n+  \/\/   Z_SP     points to resized frame\n+  \/\/            Z_SP before resize available from -16(parmBlk)\n+  \/\/   parmBlk  points to crypto instruction parameter block\n+  \/\/            parameter block is filled with crypto key.\n+  \/\/   msglen   unchanged, saved for later at -24(parmBlk)\n+  \/\/   fCode    contains function code for instruction\n+  \/\/   key      unchanged\n+  \/\/\n+  void generate_counterMode_prepare_Stack(Register parmBlk, Register ctr, Register counter, Register scratch) {\n+\n+    BLOCK_COMMENT(\"prepare stack counterMode_AESCrypt {\");\n+\n+    \/\/ save argument registers.\n+    \/\/   ARG1(from) is Z_RET as well. Not saved or restored.\n+    \/\/   ARG5(msglen) is restored by other means.\n+    __ z_stmg(Z_ARG2, Z_ARG4, argsave_offset,    parmBlk);\n+\n+#if defined(ASSERT)\n+    \/\/ save ctr byte array length for debugging and check length against expected.\n+    __ z_lgf(scratch, Address(ctr, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));\n+    __ z_sthy(scratch, ctrVal_len_offset, Z_R0, parmBlk);\n+    \/\/ check length against expected.\n+    __ z_chi(scratch, AES_ctrVal_len);\n+    __ asm_assert_eq(\"counter value needs same size as data block\", 0xb00b);\n+#endif\n+\n+    assert(AES_ctrVec_len > 0, \"sanity. We need a counter vector\");\n+    __ add2reg(counter, AES_parmBlk_align, parmBlk);             \/\/ counter array is located behind crypto key. Available range is disp12 only.\n+    __ z_mvc(0, AES_ctrVal_len-1, counter, 0, ctr);              \/\/ move first copy of iv\n+    for (int j = 1; j < AES_ctrVec_len; j+=j) {                  \/\/ j (and amount of moved data) doubles with every iteration\n+      int offset = j * AES_ctrVal_len;\n+      if (offset <= 256) {\n+        __ z_mvc(offset, offset-1, counter, 0, counter);         \/\/ move iv\n+      } else {\n+        for (int k = 0; k < offset; k += 256) {\n+          __ z_mvc(offset+k, 255, counter, 0, counter);\n+        }\n+      }\n+    }\n+    for (int j = 1; j < AES_ctrVec_len; j++) {                   \/\/ start with j = 1; no need to add 0 to the first counter value.\n+      int offset = j * AES_ctrVal_len;\n+      __ z_algsi(offset + 8, counter, j);                        \/\/ increment iv by index value\n+                                                                 \/\/ TODO: for correctness, use 128-bit add\n+    }\n+\n+    BLOCK_COMMENT(\"} prepare stack counterMode_AESCrypt\");\n+  }\n+\n+\n+  void generate_counterMode_increment_ctrVector(Register parmBlk, Register counter, bool v0_only) {\n+\n+    BLOCK_COMMENT(\"increment ctrVector counterMode_AESCrypt {\");\n+\n+    __ add2reg(counter, AES_parmBlk_align, parmBlk);             \/\/ ptr to counter array needs to be restored\n+    for (int j = 0; j < AES_ctrVec_len; j++) {\n+      int offset = j * AES_ctrVal_len;\n+      __ z_algsi(offset + 8, counter, AES_ctrVec_len);           \/\/ calculate new ctr vector elements (simple increment)\n+                                                                 \/\/ TODO: for correctness, use 128-bit add\n+      if (v0_only) break;\n+    }\n+\n+    BLOCK_COMMENT(\"} increment ctrVector counterMode_AESCrypt\");\n+  }\n+\n+\n+  void generate_counterMode_push_Block(int dataBlk_len, int parmBlk_len, int crypto_fCode,\n+                           Register parmBlk, Register msglen, Register fCode, Register key) {\n+\n+    BLOCK_COMMENT(err_msg(\"push_Block counterMode_AESCrypt%d {\", parmBlk_len*8));\n+\n+    AES_dataBlk_space    = (2*dataBlk_len + AES_parmBlk_align - 1) & (~(AES_parmBlk_align - 1)); \/\/ space for data blocks (src and dst, one each) for partial block processing)\n+    AES_parmBlk_addspace = AES_parmBlk_align    \/\/ spill space (temp data)\n+                         + AES_parmBlk_align    \/\/ for argument save\/restore\n+                         ;\n+    const int key_len    = parmBlk_len;         \/\/ The length of the unextended key (16, 24, 32)\n+\n+    assert((AES_ctrVal_len == 0) || (AES_ctrVal_len == dataBlk_len), \"varying dataBlk_len is not supported.\");\n+    AES_ctrVal_len  = dataBlk_len;               \/\/ ctr init value len (in bytes)\n+    AES_ctrArea_len = AES_ctrVec_len * AES_ctrVal_len; \/\/ space required on stack for ctr vector\n+\n+    \/\/ This len must be known at JIT compile time. Only then are we able to recalc the SP before resize.\n+    \/\/ We buy this knowledge by wasting some (up to AES_parmBlk_align) bytes of stack space.\n+    const int resize_len = JIT_TIMER_space       \/\/ timestamp storage for JIT_TIMER\n+                         + AES_parmBlk_align     \/\/ room for alignment of parmBlk\n+                         + AES_parmBlk_align     \/\/ extra room for alignment\n+                         + AES_dataBlk_space     \/\/ one src and one dst data blk\n+                         + AES_parmBlk_addspace  \/\/ spill space for local data\n+                         + ((parmBlk_len + AES_parmBlk_align - 1) & (~(AES_parmBlk_align - 1)))  \/\/ aligned length of parmBlk\n+                         + AES_ctrArea_len       \/\/ stack space for ctr vector\n+                         ;\n+    Register scratch     = fCode;  \/\/ We can use fCode as a scratch register. It's contents on entry\n+                                   \/\/ is irrelevant and it is set at the very end of this code block.\n+\n+    assert(key_len < 256, \"excessive crypto key len: %d, limit: 256\", key_len);\n+\n+    \/\/ After the frame is resized, the parmBlk is positioned such\n+    \/\/ that it is octoword-aligned. This potentially creates some\n+    \/\/ alignment waste in addspace and\/or in the gap area.\n+    \/\/ After resize_frame, scratch contains the frame pointer.\n+    __ resize_frame(-resize_len, scratch, true);\n+\n+    \/\/ calculate aligned parmBlk address from updated (resized) SP.\n+    __ add2reg(parmBlk, AES_parmBlk_addspace + (AES_parmBlk_align-1), Z_SP);\n+    __ z_nill(parmBlk, (~(AES_parmBlk_align-1)) & 0xffff); \/\/ Align parameter block.\n+\n+    \/\/ There is room to spill stuff in the range [parmBlk-AES_parmBlk_addspace+8, parmBlk).\n+    __ z_mviy(keylen_offset, parmBlk, key_len - 1);        \/\/ Spill crypto key length for later use. Decrement by one for direct use with xc template.\n+    __ z_mviy(fCode_offset,  parmBlk, crypto_fCode);       \/\/ Crypto function code, will be loaded into Z_R0 later.\n+    __ z_sty(msglen, msglen_offset, parmBlk);              \/\/ full plaintext\/ciphertext len.\n+    __ z_sra(msglen, exact_log2(dataBlk_len));             \/\/ # full cipher blocks that can be formed from input text.\n+    __ z_sty(msglen, remmsg_len_offset, parmBlk);\n+\n+    __ add2reg(scratch, resize_len, Z_SP);                 \/\/ calculate (SP before resize) from resized SP.\n+    __ z_stg(scratch, unextSP_offset, parmBlk);            \/\/ Spill unextended SP for easy revert.\n+\n+    \/\/ Fill parmBlk with all required data\n+    __ z_mvc(0, key_len-1, parmBlk, 0, key);               \/\/ Copy key. Need to do it here - key_len is only known here.\n+    BLOCK_COMMENT(err_msg(\"} push_Block counterMode_AESCrypt%d\", parmBlk_len*8));\n+  }\n+\n+\n+  void generate_counterMode_pop_Block(Register parmBlk, Register msglen, Label& eraser) {\n+    \/\/ For added safety, clear the stack area where the crypto key was stored.\n+    Register scratch = msglen;\n+    assert_different_registers(scratch, Z_R0);            \/\/ can't use Z_R0 for exrl.\n+\n+    \/\/ wipe out key on stack\n+    __ z_llgc(scratch, keylen_offset, parmBlk);           \/\/ get saved (key_len-1) value (we saved just one byte!)\n+    __ z_exrl(scratch, eraser);                           \/\/ template relies on parmBlk still pointing to key on stack\n+\n+    \/\/ restore argument registers.\n+    \/\/   ARG1(from) is Z_RET as well. Not restored - will hold return value anyway.\n+    \/\/   ARG5(msglen) is restored further down.\n+    __ z_lmg(Z_ARG2, Z_ARG4, argsave_offset,    parmBlk);\n+\n+    __ z_lgf(msglen, msglen_offset,  parmBlk);            \/\/ Restore msglen, only low order FW is valid\n+    __ z_lg(Z_SP,    unextSP_offset, parmBlk);            \/\/ trim stack back to unextended size\n+  }\n+\n+\n+  void generate_counterMode_push_parmBlk(Register parmBlk, Register msglen, Register fCode, Register key, bool is_decipher) {\n+    int       mode = is_decipher ? VM_Version::CipherMode::decipher : VM_Version::CipherMode::cipher;\n+    Label     parmBlk_128, parmBlk_192, parmBlk_256, parmBlk_set;\n+    Register  keylen = fCode;      \/\/ Expanded key length, as read from key array, Temp only.\n+                                   \/\/ use fCode as scratch; fCode receives its final value later.\n+\n+    BLOCK_COMMENT(\"push parmBlk counterMode_AESCrypt {\");\n+\n+    \/\/ Read key len of expanded key (in 4-byte words).\n+    __ z_lgf(keylen, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));\n+    __ z_cghi(keylen, 52);\n+    if (VM_Version::has_Crypto_AES_CTR256()) { __ z_brh(parmBlk_256); }  \/\/ keyLen >  52: AES256. Assume: most frequent\n+    if (VM_Version::has_Crypto_AES_CTR128()) { __ z_brl(parmBlk_128); }  \/\/ keyLen <  52: AES128.\n+    if (VM_Version::has_Crypto_AES_CTR192()) { __ z_bre(parmBlk_192); }  \/\/ keyLen == 52: AES192. Assume: least frequent\n+\n+    \/\/ Safety net: requested AES_CTR function for requested keylen not available on this CPU.\n+    __ stop_static(\"AES key strength not supported by CPU. Use -XX:-UseAESCTRIntrinsics as remedy.\", 0);\n+\n+    if (VM_Version::has_Crypto_AES_CTR128()) {\n+      __ bind(parmBlk_128);\n+      generate_counterMode_push_Block(VM_Version::Cipher::_AES128_dataBlk,\n+                          VM_Version::Cipher::_AES128_parmBlk_G,\n+                          VM_Version::Cipher::_AES128 + mode,\n+                          parmBlk, msglen, fCode, key);\n+      if (VM_Version::has_Crypto_AES_CTR256() || VM_Version::has_Crypto_AES_CTR192()) {\n+        __ z_bru(parmBlk_set);  \/\/ Fallthru otherwise.\n+      }\n+    }\n+\n+    if (VM_Version::has_Crypto_AES_CTR192()) {\n+      __ bind(parmBlk_192);\n+      generate_counterMode_push_Block(VM_Version::Cipher::_AES192_dataBlk,\n+                          VM_Version::Cipher::_AES192_parmBlk_G,\n+                          VM_Version::Cipher::_AES192 + mode,\n+                          parmBlk, msglen, fCode, key);\n+      if (VM_Version::has_Crypto_AES_CTR256()) {\n+        __ z_bru(parmBlk_set);  \/\/ Fallthru otherwise.\n+      }\n+    }\n+\n+    if (VM_Version::has_Crypto_AES_CTR256()) {\n+      __ bind(parmBlk_256);\n+      generate_counterMode_push_Block(VM_Version::Cipher::_AES256_dataBlk,\n+                          VM_Version::Cipher::_AES256_parmBlk_G,\n+                          VM_Version::Cipher::_AES256 + mode,\n+                          parmBlk, msglen, fCode, key);\n+      \/\/ Fallthru\n+    }\n+\n+    __ bind(parmBlk_set);\n+    BLOCK_COMMENT(\"} push parmBlk counterMode_AESCrypt\");\n+  }\n+\n+\n+  void generate_counterMode_pop_parmBlk(Register parmBlk, Register msglen, Label& eraser) {\n+\n+    BLOCK_COMMENT(\"pop parmBlk counterMode_AESCrypt {\");\n+\n+    generate_counterMode_pop_Block(parmBlk, msglen, eraser);\n+\n+    BLOCK_COMMENT(\"} pop parmBlk counterMode_AESCrypt\");\n+  }\n+\n+  \/\/ Resize current stack frame to make room for some register data which needs\n+  \/\/ to be spilled temporarily. All registers in the range [from..to] are spilled\n+  \/\/ automatically. The actual length of the allocated aux block is returned.\n+  \/\/ The extra spill space (if requested) is located at\n+  \/\/   [Z_SP+stackSpace-spillSpace, Z_SP+stackSpace)\n+  \/\/ Kills Z__R0 (contains fp afterwards) and Z_R1 (contains old SP afterwards).\n+  \/\/ All space in the range [SP..SP+regSpace) is reserved.\n+  \/\/ As always (here): 0(SP) - stack linkage, 8(SP) - SP before resize for easy pop.\n+  int generate_push_aux_block(Register from, Register to, unsigned int spillSpace) {\n+    BLOCK_COMMENT(\"push aux_block counterMode_AESCrypt {\");\n+    int n_regs     = to->encoding() - from->encoding() + 1;\n+    int linkSpace  = 2*wordSize;\n+    int regSpace   = n_regs*wordSize;\n+    int stackSpace = ((linkSpace + regSpace + spillSpace)+(AES_parmBlk_align-1)) & (~(AES_parmBlk_align-1));\n+    __ z_lgr(Z_R1, Z_SP);\n+    __ resize_frame(-stackSpace, Z_R0, true);\n+    __ z_stg(Z_R1, 8, Z_SP);\n+    __ z_stmg(from, to, linkSpace, Z_SP);\n+    BLOCK_COMMENT(\"} push aux_block counterMode_AESCrypt\");\n+    return stackSpace;\n+  }\n+  \/\/ Reverts everything done by generate_push_aux_block().\n+  void generate_pop_aux_block(Register from, Register to) {\n+    BLOCK_COMMENT(\"pop aux_block counterMode_AESCrypt {\");\n+    __ z_lmg(from, to, 16, Z_SP);\n+    __ z_lg(Z_SP, 8, Z_SP);\n+    BLOCK_COMMENT(\"} pop aux_block counterMode_AESCrypt\");\n+  }\n+\n+  \/\/ Implementation of counter-mode AES encrypt\/decrypt function.\n+  \/\/\n+  void generate_counterMode_AES_impl(bool is_decipher, int timerNum) {\n+\n+    Register       from    = Z_ARG1; \/\/ byte[], source byte array (clear text)\n+    Register       to      = Z_ARG2; \/\/ byte[], destination byte array (ciphered)\n+    Register       key     = Z_ARG3; \/\/ byte[], expanded key array.\n+    Register       ctr     = Z_ARG4; \/\/ byte[], counter byte array.\n+    const Register msglen  = Z_ARG5; \/\/ int, Total length of the msg to be encrypted. Value must be\n+                                     \/\/ returned in Z_RET upon completion of this stub.\n+                                     \/\/ This is a jint. Negative values are illegal, but technically possible.\n+                                     \/\/ Do not rely on high word. Contents is undefined.\n+\n+    const Register fCode   = Z_R0;   \/\/ crypto function code\n+    const Register parmBlk = Z_R1;   \/\/ parameter block address (points to crypto key)\n+    const Register src     = Z_ARG1; \/\/ is Z_R2\n+    const Register srclen  = Z_ARG2; \/\/ Overwrites destination address.\n+    const Register dst     = Z_ARG3; \/\/ Overwrites key address.\n+    const Register counter = Z_ARG5; \/\/ Overwrites msglen. Must have counter array in an even register.\n+\n+    Label srcMover, dstMover, fromMover, ctrXOR, dataEraser;  \/\/ EXRL (execution) templates.\n+    Label CryptoLoop, CryptoLoop_doit, CryptoLoop_end, CryptoLoop_setupAndDoLast, CryptoLoop_ctrVal_inc, allDone, Exit;\n+\n+    if (! VM_Version::has_Crypto_AES_CTR()) {\n+      __ should_not_reach_here();\n+    }\n+\n+#if defined(JIT_TIMER)\n+    __ JIT_TIMER_emit_start(-1, timerNum);\n+#endif\n+\n+    \/\/ Check if there is a leftover, partially used encrypted counter from last invocation.\n+    \/\/ If so, use those leftover counter bytes first before starting the \"normal\" encryption.\n+    {\n+      Register cvIndex   = Z_R10;  \/\/ # unused bytes of last encrypted counter value\n+      Register cvUnused  = Z_R11;  \/\/ # unused bytes of last encrypted counter value\n+      Register encCtr    = Z_R12;  \/\/ encrypted counter value, points to first ununsed byte.\n+      Label no_preLoop, preLoop_end;\n+\n+      \/\/ Before pushing an aux block, check if it's necessary at all (saves some cycles).\n+      __ z_lt(Z_R0, _z_abi(remaining_cargs) + 8 + 4, Z_R0, Z_SP); \/\/ arg7: # unused bytes in encCTR.\n+      __ z_brnp(no_preLoop);                                      \/\/ no unused bytes, nothing special to do.\n+\n+      int   oldSP_Offset  = generate_push_aux_block(Z_R10, Z_R12, 16);\n+      int   arg6_Offset   = oldSP_Offset + _z_abi(remaining_cargs);\n+      int   arg7_Offset   = oldSP_Offset + _z_abi(remaining_cargs) + 8;\n+\n+      __ z_ltgf(cvUnused, arg7_Offset+4, Z_R0, Z_SP); \/\/ arg7: # unused bytes in encCTR. (16-arg7) is index of first unused byte.\n+      __ z_brnp(preLoop_end);                  \/\/ \"not positive\" means no unused bytes left\n+      __ z_aghik(cvIndex, cvUnused, -16);      \/\/ calculate index of first unused byte. AES_ctrVal_len undefined at this point.\n+      __ z_brnl(preLoop_end);                  \/\/ NotLow(=NotNegative): unused bytes >= 16? How that?\n+      __ z_lcgr(cvIndex, cvIndex);\n+\n+      __ z_lg(encCtr, arg6_Offset, Z_SP);      \/\/ arg6: encrypted counter byte array.\n+      __ z_agr(encCtr, cvIndex);               \/\/ first unused byte of encrypted ctr. Used in ctrXOR.\n+\n+      __ z_cr(cvUnused, msglen);               \/\/ check if msg is long enough\n+      __ z_locr(cvUnused, msglen, Assembler::bcondHigh); \/\/ take the shorter length\n+\n+      __ z_aghi(cvUnused, -1);                 \/\/ decrement # unused bytes by 1 for exrl instruction\n+      __ z_brl(preLoop_end);                   \/\/ negative result means nothing to do (msglen is zero)\n+\n+      __ z_exrl(cvUnused, fromMover);\n+      __ z_exrl(cvUnused, ctrXOR);\n+\n+      __ add2reg(cvUnused, 1, cvUnused);\n+      __ z_agr(from, cvUnused);\n+      __ z_agr(to, cvUnused);\n+      __ z_sr(msglen, cvUnused);\n+      __ z_brnz(preLoop_end);                  \/\/ there is still work to do\n+\n+      \/\/ Remaining msglen is zero, i.e. all msg bytes were processed in preLoop.\n+      \/\/ Take an early exit.\n+      generate_pop_aux_block(Z_R10, Z_R12);\n+      __ z_bru(Exit);\n+\n+      \/\/-------------------------------------------\n+      \/\/---<  execution templates for preLoop  >---\n+      \/\/-------------------------------------------\n+      __ bind(fromMover);\n+      __ z_mvc(0, 0, to, 0, from);         \/\/ Template instruction to move input data to dst.\n+      __ bind(ctrXOR);\n+      __ z_xc(0,  0, to, 0, encCtr);       \/\/ Template instruction to XOR input data (now in to) with encrypted counter.\n+\n+      __ bind(preLoop_end);\n+      generate_pop_aux_block(Z_R10, Z_R12);\n+\n+      __ bind(no_preLoop);\n+    }\n+\n+    \/\/ Expand stack, load parm block address into parmBlk (== Z_R1), copy crypto key to parm block.\n+    generate_counterMode_push_parmBlk(parmBlk, msglen, fCode, key, is_decipher);\n+    \/\/ Create count vector on stack to accommodate up to AES_ctrVec_len blocks.\n+    generate_counterMode_prepare_Stack(parmBlk, ctr, counter, fCode);\n+\n+    \/\/ Prepare other registers for instruction.\n+    __ lgr_if_needed(src, from);     \/\/ Copy src address. Will not emit, src\/from are identical.\n+    __ z_lgr(dst, to);\n+    __ z_llgc(fCode, fCode_offset, Z_R0, parmBlk);\n+\n+    __ bind(CryptoLoop);\n+      __ z_lghi(srclen, AES_ctrArea_len);                     \/\/ preset len (#bytes) for next iteration: max possible.\n+      __ z_asi(remmsg_len_offset, parmBlk, -AES_ctrVec_len);  \/\/ decrement #remaining blocks (16 bytes each). Range: [+127..-128]\n+      __ z_brl(CryptoLoop_setupAndDoLast);                    \/\/ Handling the last iteration out-of-line\n+\n+      __ bind(CryptoLoop_doit);\n+      __ kmctr(dst, counter, src);   \/\/ Cipher the message.\n+\n+      __ z_lt(srclen, remmsg_len_offset, Z_R0, parmBlk);      \/\/ check if this was the last iteration\n+      __ z_brz(CryptoLoop_ctrVal_inc);                        \/\/ == 0: ctrVector fully used. Need to increment the first\n+                                                              \/\/       vector element to encrypt remaining unprocessed bytes.\n+\/\/    __ z_brl(CryptoLoop_end);                               \/\/  < 0: this was detected before and handled at CryptoLoop_setupAndDoLast\n+\n+      generate_counterMode_increment_ctrVector(parmBlk, counter, false);\n+      __ z_bru(CryptoLoop);\n+\n+    __ bind(CryptoLoop_end);\n+\n+    \/\/ OK, when we arrive here, we have encrypted all of the \"from\" byte stream\n+    \/\/ except for the last few [0..dataBlk_len) bytes. To encrypt these few bytes\n+    \/\/ we need to form an extra src and dst data block of dataBlk_len each. This\n+    \/\/ is because we can only process full blocks but we must not read or write\n+    \/\/ beyond the boundaries of the argument arrays. Here is what we do:\n+    \/\/  - The src data block is filled with the remaining \"from\" bytes, padded with 0x00's.\n+    \/\/  - The single src data block is encrypted into the dst data block.\n+    \/\/  - The dst data block is copied into the \"to\" array, but only the leftmost few bytes\n+    \/\/    (as many as were left in the source byte stream).\n+    \/\/  - The counter value to be used is is pointed at by the counter register.\n+    \/\/  - Fortunately, the crypto instruction (kmctr) updates all related addresses such that we\n+    \/\/    know where to continue with \"from\" and \"to\" and which counter value to use next.\n+\n+    \/\/ Use speaking alias for temp register\n+    Register dataBlk = counter;\n+    __ z_stg(counter, -24, parmBlk);                      \/\/ spill address of counter array\n+    __ add2reg(dataBlk, -(AES_parmBlk_addspace + AES_dataBlk_space), parmBlk);\n+\n+    __ z_lgf(srclen, msglen_offset, parmBlk);             \/\/ full plaintext\/ciphertext len.\n+    __ z_nilf(srclen, AES_ctrVal_len - 1);                \/\/ those rightmost bits indicate the unprocessed #bytes\n+    __ z_braz(allDone);                                   \/\/ no unprocessed bytes? Then we are done.\n+\n+    __ add2reg(srclen, -1);                               \/\/ decrement for exrl\n+    __ z_stg(srclen, localSpill_offset, parmBlk);         \/\/ save for later reuse\n+    __ z_xc(0, AES_ctrVal_len - 1, dataBlk, 0, dataBlk);  \/\/ clear src block (zero padding)\n+    __ z_exrl(srclen, srcMover);                          \/\/ copy src byte stream (remaining bytes)\n+    __ load_const_optimized(srclen, AES_ctrVal_len);      \/\/ kmctr processes only complete blocks\n+\n+    __ z_lgr(src, dataBlk);                               \/\/ tmp src address for kmctr\n+    __ z_lg(counter, -24, parmBlk);                       \/\/ restore counter\n+    __ z_stg(dst, -24, parmBlk);                          \/\/ save current dst\n+    __ add2reg(dst, AES_ctrVal_len, src);                 \/\/ tmp dst is right after tmp src\n+\n+    __ kmctr(dst, counter, src);   \/\/ Cipher the remaining bytes.\n+\n+    __ add2reg(dataBlk, -AES_ctrVal_len, dst);            \/\/ tmp dst address\n+    __ z_lg(dst, -24, parmBlk);                           \/\/ real dst address\n+    __ z_lg(srclen, localSpill_offset, parmBlk);          \/\/ reuse calc from above\n+    __ z_exrl(srclen, dstMover);\n+\n+    __ bind(allDone);\n+    __ z_llgf(srclen, msglen_offset, parmBlk);            \/\/ increment unencrypted ctr by #blocks processed.\n+    __ z_srag(srclen, srclen, exact_log2(AES_ctrVal_len));\n+    __ z_ag(srclen, 8, Z_R0, ctr);\n+    __ z_stg(srclen, 8, Z_R0, ctr);\n+    generate_counterMode_pop_parmBlk(parmBlk, msglen, dataEraser);\n+\n+    __ bind(Exit);\n+    __ z_lgfr(Z_RET, msglen);\n+\n+#if defined(JIT_TIMER)\n+    __ JIT_TIMER_emit_stop(-1, timerNum, msglen);\n+#endif\n+    __ z_br(Z_R14);\n+\n+    \/\/----------------------------\n+    \/\/---<  out-of-line code  >---\n+    \/\/----------------------------\n+    __ bind(CryptoLoop_setupAndDoLast);\n+      __ z_lgf(srclen, remmsg_len_offset, parmBlk);           \/\/ remaining #blocks in memory is < 0\n+      __ z_aghi(srclen, AES_ctrVec_len);                      \/\/ recalculate the actually remaining #blocks\n+      __ z_sllg(srclen, srclen, exact_log2(AES_ctrVal_len));  \/\/ convert to #bytes. Counter value is same length as data block\n+      __ kmctr(dst, counter, src);                            \/\/ Cipher the last integral blocks of the message.\n+      __ z_bru(CryptoLoop_end);\n+\n+    __ bind(CryptoLoop_ctrVal_inc);\n+      generate_counterMode_increment_ctrVector(parmBlk, counter, true);\n+      __ z_bru(CryptoLoop_end);\n+\n+    \/\/-------------------------------\n+    \/\/---<  execution templates  >---\n+    \/\/-------------------------------\n+    __ bind(dataEraser);\n+    __ z_xc(0, 0, parmBlk, 0, parmBlk);  \/\/ Template instruction to erase crypto key on stack.\n+    __ bind(dstMover);\n+    __ z_mvc(0, 0, dst, 0, dataBlk);     \/\/ Template instruction to move encrypted reminder from stack to dst.\n+    __ bind(srcMover);\n+    __ z_mvc(0, 0, dataBlk, 0, src);     \/\/ Template instruction to move reminder of source byte stream to stack.\n+  }\n+\n+\n+  \/\/ Create two intrinsic variants, optimized for short and long plaintexts.\n+  \/\/\n+  void generate_counterMode_AES(bool is_decipher, int timerNum) {\n+\n+    const Register msglen  = Z_ARG5;    \/\/ int, Total length of the msg to be encrypted. Value must be\n+                                        \/\/ returned in Z_RET upon completion of this stub.\n+    const int threshold = 256;          \/\/ above this length (in bytes), text is considered long.\n+    const int vec_short = threshold>>6; \/\/ that many blocks (16 bytes each) per iteration, max 4 loop iterations\n+    const int vec_long  = threshold>>2; \/\/ that many blocks (16 bytes each) per iteration.\n+\n+    Label AESCTR_short, AESCTR_long;\n+\n+    if (! VM_Version::has_Crypto_AES_CTR()) {\n+      __ should_not_reach_here();\n+    }\n+\n+    __ z_chi(msglen, threshold);\n+    __ z_brh(AESCTR_long);\n+\n+    BLOCK_COMMENT(err_msg(\"counterMode_AESCrypt (len <= %d, block size = %d) {\", threshold, vec_short*16));\n+\n+    __ bind(AESCTR_short);\n+    AES_ctrVec_len = vec_short;\n+    generate_counterMode_AES_impl(false, timerNum);   \/\/ control of generated code will not return\n+\n+    BLOCK_COMMENT(err_msg(\"} counterMode_AESCrypt (len <= %d, block size = %d)\", threshold, vec_short*16));\n+\n+    __ align(32); \/\/ Octoword alignment benefits branch targets.\n+\n+    BLOCK_COMMENT(err_msg(\"counterMode_AESCrypt (len > %d, block size = %d) {\", threshold, vec_long*16));\n+\n+    __ bind(AESCTR_long);\n+    AES_ctrVec_len = vec_long;\n+    generate_counterMode_AES_impl(false, timerNum+1); \/\/ control of generated code will not return\n+\n+    BLOCK_COMMENT(err_msg(\"} counterMode_AESCrypt (len > %d, block size = %d)\", threshold, vec_long*16));\n+  }\n+\n+\n+  \/\/ Compute AES-CTR crypto function.\n+  \/\/ Encrypt or decrypt is selected via parameters. Only one stub is necessary.\n+  address generate_counterMode_AESCrypt(const char* name) {\n+    __ align(CodeEntryAlignment);\n+    StubCodeMark mark(this, \"StubRoutines\", name);\n+    unsigned int   start_off = __ offset();  \/\/ Remember stub start address (is rtn value).\n+\n+    generate_counterMode_AES(false, 5);\n+\n+    return __ addr_at(start_off);\n+  }\n+\n+\/\/ *****************************************************************************\n@@ -2209,0 +2809,1 @@\n+\n@@ -2371,0 +2972,6 @@\n+    if (UseAESCTRIntrinsics) {\n+      if (VM_Version::has_Crypto_AES_CTR()) {\n+        StubRoutines::_counterMode_AESCrypt = generate_counterMode_AESCrypt(\"counterMode_AESCrypt\");\n+      }\n+    }\n+\n","filename":"src\/hotspot\/cpu\/s390\/stubGenerator_s390.cpp","additions":613,"deletions":6,"binary":false,"changes":619,"status":"modified"},{"patch":"@@ -2,2 +2,2 @@\n- * Copyright (c) 2016, 2021, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2016, 2021 SAP SE. All rights reserved.\n+ * Copyright (c) 2016, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2022 SAP SE. All rights reserved.\n@@ -39,1 +39,1 @@\n-const char*   VM_Version::_model_string;\n+const char*   VM_Version::_model_string = \"\";\n@@ -178,1 +178,1 @@\n-  if (FLAG_IS_DEFAULT(UseAES) && has_Crypto_AES()) {\n+  if (FLAG_IS_DEFAULT(UseAES) && (has_Crypto_AES() || has_Crypto_AES_CTR())) {\n@@ -181,1 +181,1 @@\n-  if (UseAES && !has_Crypto_AES()) {\n+  if (UseAES && !(has_Crypto_AES() || has_Crypto_AES_CTR())) {\n@@ -185,0 +185,1 @@\n+\n@@ -186,1 +187,1 @@\n-    if (FLAG_IS_DEFAULT(UseAESIntrinsics)) {\n+    if (FLAG_IS_DEFAULT(UseAESIntrinsics) && has_Crypto_AES()) {\n@@ -189,0 +190,3 @@\n+    if (FLAG_IS_DEFAULT(UseAESCTRIntrinsics) && has_Crypto_AES_CTR()) {\n+      FLAG_SET_DEFAULT(UseAESCTRIntrinsics, true);\n+    }\n@@ -190,0 +194,1 @@\n+\n@@ -194,0 +199,5 @@\n+  if (UseAESCTRIntrinsics && !has_Crypto_AES_CTR()) {\n+    warning(\"AES_CTR intrinsics are not available on this CPU\");\n+    FLAG_SET_DEFAULT(UseAESCTRIntrinsics, false);\n+  }\n+\n@@ -198,4 +208,2 @@\n-\n-  \/\/ TODO: implement AES\/CTR intrinsics\n-  if (UseAESCTRIntrinsics) {\n-    warning(\"AES\/CTR intrinsics are not available on this CPU\");\n+  if (UseAESCTRIntrinsics && !UseAES) {\n+    warning(\"AES_CTR intrinsics require UseAES flag to be enabled. Intrinsics will be disabled.\");\n","filename":"src\/hotspot\/cpu\/s390\/vm_version_s390.cpp","additions":18,"deletions":10,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -2,2 +2,2 @@\n- * Copyright (c) 2016, 2021, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2016, 2021 SAP SE. All rights reserved.\n+ * Copyright (c) 2016, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2022 SAP SE. All rights reserved.\n@@ -197,1 +197,1 @@\n-    enum { \/\/ KM only!!! KMC uses different parmBlk sizes.\n+    enum { \/\/ function codes, same for all KM* cipher instructions.\n@@ -257,0 +257,24 @@\n+      \/\/ Parameter block sizes (in bytes) for KMCTR (CounterMode) instruction.\n+      _Query_parmBlk_G              =  16,\n+      _DEA_parmBlk_G                =   8,\n+      _TDEA128_parmBlk_G            =  16,\n+      _TDEA192_parmBlk_G            =  24,\n+      _EncryptedDEA_parmBlk_G       =  32,\n+      _EncryptedDEA128_parmBlk_G    =  40,\n+      _EncryptedDEA192_parmBlk_G    =  48,\n+      _AES128_parmBlk_G             =  16,\n+      _AES192_parmBlk_G             =  24,\n+      _AES256_parmBlk_G             =  32,\n+      _EnccryptedAES128_parmBlk_G   =  48,\n+      _EnccryptedAES192_parmBlk_G   =  56,\n+      _EnccryptedAES256_parmBlk_G   =  64,\n+\n+      \/\/ Parameter block sizes (in bytes) for KMA instruction.\n+      _Query_parmBlk_A              =  16,\n+      _AES128_parmBlk_A             =  96,\n+      _AES192_parmBlk_A             = 104,\n+      _AES256_parmBlk_A             = 112,\n+      _EnccryptedAES128_parmBlk_A   = 128,\n+      _EnccryptedAES192_parmBlk_A   = 136,\n+      _EnccryptedAES256_parmBlk_A   = 144,\n+\n@@ -462,3 +486,11 @@\n-  static bool has_Crypto_AES128()             { return has_Crypto() && test_feature_bit(&_cipher_features_KM[0], Cipher::_AES128, Cipher::_featureBits); }\n-  static bool has_Crypto_AES192()             { return has_Crypto() && test_feature_bit(&_cipher_features_KM[0], Cipher::_AES192, Cipher::_featureBits); }\n-  static bool has_Crypto_AES256()             { return has_Crypto() && test_feature_bit(&_cipher_features_KM[0], Cipher::_AES256, Cipher::_featureBits); }\n+  static bool has_Crypto_AES_GCM128()         { return has_Crypto() && test_feature_bit(&_cipher_features_KMA[0], Cipher::_AES128, Cipher::_featureBits); }\n+  static bool has_Crypto_AES_GCM192()         { return has_Crypto() && test_feature_bit(&_cipher_features_KMA[0], Cipher::_AES192, Cipher::_featureBits); }\n+  static bool has_Crypto_AES_GCM256()         { return has_Crypto() && test_feature_bit(&_cipher_features_KMA[0], Cipher::_AES256, Cipher::_featureBits); }\n+  static bool has_Crypto_AES_GCM()            { return has_Crypto_AES_GCM128() || has_Crypto_AES_GCM192() || has_Crypto_AES_GCM256(); }\n+  static bool has_Crypto_AES_CTR128()         { return has_Crypto() && test_feature_bit(&_cipher_features_KMCTR[0], Cipher::_AES128, Cipher::_featureBits); }\n+  static bool has_Crypto_AES_CTR192()         { return has_Crypto() && test_feature_bit(&_cipher_features_KMCTR[0], Cipher::_AES192, Cipher::_featureBits); }\n+  static bool has_Crypto_AES_CTR256()         { return has_Crypto() && test_feature_bit(&_cipher_features_KMCTR[0], Cipher::_AES256, Cipher::_featureBits); }\n+  static bool has_Crypto_AES_CTR()            { return has_Crypto_AES_CTR128() || has_Crypto_AES_CTR192() || has_Crypto_AES_CTR256(); }\n+  static bool has_Crypto_AES128()             { return has_Crypto() && test_feature_bit(&_cipher_features_KM[0],  Cipher::_AES128, Cipher::_featureBits); }\n+  static bool has_Crypto_AES192()             { return has_Crypto() && test_feature_bit(&_cipher_features_KM[0],  Cipher::_AES192, Cipher::_featureBits); }\n+  static bool has_Crypto_AES256()             { return has_Crypto() && test_feature_bit(&_cipher_features_KM[0],  Cipher::_AES256, Cipher::_featureBits); }\n","filename":"src\/hotspot\/cpu\/s390\/vm_version_s390.hpp","additions":38,"deletions":6,"binary":false,"changes":44,"status":"modified"}]}