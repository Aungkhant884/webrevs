{"files":[{"patch":"@@ -112,1 +112,1 @@\n-    result = load_reference_barrier(access.gen(), result, LIR_OprFact::addressConst(0), false);\n+    result = load_reference_barrier(access.gen(), result, LIR_OprFact::addressConst(0), ShenandoahBarrierSet::NORMAL);\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/shenandoah\/c1\/shenandoahBarrierSetC1_aarch64.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -265,1 +265,1 @@\n-void ShenandoahBarrierSetAssembler::load_reference_barrier_native(MacroAssembler* masm, Register dst, Address load_addr) {\n+void ShenandoahBarrierSetAssembler::load_reference_barrier_native(MacroAssembler* masm, Register dst, Address load_addr, bool weak) {\n@@ -285,1 +285,1 @@\n-  __ tbz(rscratch2, ShenandoahHeap::EVACUATION_BITPOS, done);\n+  __ tbz(rscratch2, ShenandoahHeap::HAS_FORWARDED_BITPOS, done);\n@@ -289,1 +289,5 @@\n-  __ mov(lr, CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_native));\n+  if (UseCompressedOops && weak) {\n+    __ mov(lr, CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_native_narrow));\n+  } else {\n+    __ mov(lr, CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_native));\n+  }\n@@ -356,1 +360,1 @@\n-      load_reference_barrier_native(masm, dst, src);\n+      load_reference_barrier_native(masm, dst, src, (decorators & IN_NATIVE) == 0);\n@@ -672,4 +676,12 @@\n-  if (stub->is_native()) {\n-    __ far_call(RuntimeAddress(bs->load_reference_barrier_native_rt_code_blob()->code_begin()));\n-  } else {\n-    __ far_call(RuntimeAddress(bs->load_reference_barrier_rt_code_blob()->code_begin()));\n+  switch (stub->kind()) {\n+    case ShenandoahBarrierSet::NORMAL:\n+      __ far_call(RuntimeAddress(bs->load_reference_barrier_normal_rt_code_blob()->code_begin()));\n+      break;\n+    case ShenandoahBarrierSet::NATIVE:\n+      __ far_call(RuntimeAddress(bs->load_reference_barrier_native_rt_code_blob()->code_begin()));\n+      break;\n+    case ShenandoahBarrierSet::WEAK:\n+      __ far_call(RuntimeAddress(bs->load_reference_barrier_weakref_rt_code_blob()->code_begin()));\n+      break;\n+    default:\n+      ShouldNotReachHere();\n@@ -731,1 +743,1 @@\n-void ShenandoahBarrierSetAssembler::generate_c1_load_reference_barrier_runtime_stub(StubAssembler* sasm, bool is_native) {\n+void ShenandoahBarrierSetAssembler::generate_c1_load_reference_barrier_runtime_stub(StubAssembler* sasm, ShenandoahBarrierSet::ShenandoahLRBKind kind) {\n@@ -738,1 +750,1 @@\n-  if (is_native) {\n+  if (kind == ShenandoahBarrierSet::NATIVE) {\n@@ -740,2 +752,6 @@\n-  } else if (UseCompressedOops) {\n-    __ mov(lr, CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_narrow));\n+  } else if (kind == ShenandoahBarrierSet::WEAK) {\n+    if (UseCompressedOops) {\n+      __ mov(lr, CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_native_narrow));\n+    } else {\n+      __ mov(lr, CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_native));\n+    }\n@@ -743,1 +759,6 @@\n-    __ mov(lr, CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier));\n+    assert(kind == ShenandoahBarrierSet::NORMAL, \"what else?\");\n+    if (UseCompressedOops) {\n+      __ mov(lr, CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_narrow));\n+    } else {\n+      __ mov(lr, CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier));\n+    }\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/shenandoah\/shenandoahBarrierSetAssembler_aarch64.cpp","additions":34,"deletions":13,"binary":false,"changes":47,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"gc\/shenandoah\/shenandoahBarrierSet.hpp\"\n@@ -62,1 +63,1 @@\n-  void load_reference_barrier_native(MacroAssembler* masm, Register dst, Address load_addr);\n+  void load_reference_barrier_native(MacroAssembler* masm, Register dst, Address load_addr, bool weak);\n@@ -75,1 +76,1 @@\n-  void generate_c1_load_reference_barrier_runtime_stub(StubAssembler* sasm, bool is_native);\n+  void generate_c1_load_reference_barrier_runtime_stub(StubAssembler* sasm, ShenandoahBarrierSet::ShenandoahLRBKind kind);\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/shenandoah\/shenandoahBarrierSetAssembler_aarch64.hpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -114,1 +114,1 @@\n-    result = load_reference_barrier(access.gen(), result, LIR_OprFact::addressConst(0), false);\n+    result = load_reference_barrier(access.gen(), result, LIR_OprFact::addressConst(0), ShenandoahBarrierSet::NORMAL);\n","filename":"src\/hotspot\/cpu\/x86\/gc\/shenandoah\/c1\/shenandoahBarrierSetC1_x86.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -339,1 +339,1 @@\n-void ShenandoahBarrierSetAssembler::load_reference_barrier_native(MacroAssembler* masm, Register dst, Address src) {\n+void ShenandoahBarrierSetAssembler::load_reference_barrier_native(MacroAssembler* masm, Register dst, Address src, bool weak) {\n@@ -369,1 +369,1 @@\n-  __ testb(gc_state, ShenandoahHeap::EVACUATION);\n+  __ testb(gc_state, ShenandoahHeap::HAS_FORWARDED);\n@@ -399,1 +399,5 @@\n-  __ call_VM_leaf(CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_native), dst, rsi);\n+  if (UseCompressedOops && weak) {\n+    __ super_call_VM_leaf(CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_native_narrow), dst, rsi);\n+  } else {\n+    __ super_call_VM_leaf(CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_native), dst, rsi);\n+  }\n@@ -521,1 +525,1 @@\n-      load_reference_barrier_native(masm, dst, src);\n+      load_reference_barrier_native(masm, dst, src, (decorators & IN_NATIVE) == 0);\n@@ -873,4 +877,12 @@\n-  if (stub->is_native()) {\n-    __ call(RuntimeAddress(bs->load_reference_barrier_native_rt_code_blob()->code_begin()));\n-  } else {\n-    __ call(RuntimeAddress(bs->load_reference_barrier_rt_code_blob()->code_begin()));\n+  switch (stub->kind()) {\n+    case ShenandoahBarrierSet::NORMAL:\n+      __ call(RuntimeAddress(bs->load_reference_barrier_normal_rt_code_blob()->code_begin()));\n+      break;\n+    case ShenandoahBarrierSet::NATIVE:\n+      __ call(RuntimeAddress(bs->load_reference_barrier_native_rt_code_blob()->code_begin()));\n+      break;\n+    case ShenandoahBarrierSet::WEAK:\n+      __ call(RuntimeAddress(bs->load_reference_barrier_weakref_rt_code_blob()->code_begin()));\n+      break;\n+    default:\n+      ShouldNotReachHere();\n@@ -941,1 +953,1 @@\n-void ShenandoahBarrierSetAssembler::generate_c1_load_reference_barrier_runtime_stub(StubAssembler* sasm, bool is_native) {\n+void ShenandoahBarrierSetAssembler::generate_c1_load_reference_barrier_runtime_stub(StubAssembler* sasm, ShenandoahBarrierSet::ShenandoahLRBKind kind) {\n@@ -950,1 +962,1 @@\n-  if (is_native) {\n+  if (kind == ShenandoahBarrierSet::NATIVE) {\n@@ -952,2 +964,6 @@\n-  } else if (UseCompressedOops) {\n-    __ call_VM_leaf(CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_narrow), c_rarg0, c_rarg1);\n+  } else if (kind == ShenandoahBarrierSet::WEAK) {\n+    if (UseCompressedOops) {\n+      __ call_VM_leaf(CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_native_narrow), c_rarg0, c_rarg1);\n+    } else {\n+      __ call_VM_leaf(CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_native), c_rarg0, c_rarg1);\n+    }\n@@ -955,1 +971,6 @@\n-    __ call_VM_leaf(CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier), c_rarg0, c_rarg1);\n+    assert(kind == ShenandoahBarrierSet::NORMAL, \"what else?\");\n+    if (UseCompressedOops) {\n+      __ call_VM_leaf(CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_narrow), c_rarg0, c_rarg1);\n+    } else {\n+      __ call_VM_leaf(CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier), c_rarg0, c_rarg1);\n+    }\n","filename":"src\/hotspot\/cpu\/x86\/gc\/shenandoah\/shenandoahBarrierSetAssembler_x86.cpp","additions":34,"deletions":13,"binary":false,"changes":47,"status":"modified"},{"patch":"@@ -30,0 +30,2 @@\n+#include \"gc\/shenandoah\/shenandoahBarrierSet.hpp\"\n+\n@@ -73,1 +75,1 @@\n-  void generate_c1_load_reference_barrier_runtime_stub(StubAssembler* sasm, bool is_native);\n+  void generate_c1_load_reference_barrier_runtime_stub(StubAssembler* sasm, ShenandoahBarrierSet::ShenandoahLRBKind kind);\n@@ -77,1 +79,1 @@\n-  void load_reference_barrier_native(MacroAssembler* masm, Register dst, Address src);\n+  void load_reference_barrier_native(MacroAssembler* masm, Register dst, Address src, bool weak);\n","filename":"src\/hotspot\/cpu\/x86\/gc\/shenandoah\/shenandoahBarrierSetAssembler_x86.hpp","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -54,1 +54,3 @@\n-  _load_reference_barrier_rt_code_blob(NULL) {}\n+  _load_reference_barrier_normal_rt_code_blob(NULL),\n+  _load_reference_barrier_native_rt_code_blob(NULL),\n+  _load_reference_barrier_weakref_rt_code_blob(NULL) {}\n@@ -110,1 +112,1 @@\n-LIR_Opr ShenandoahBarrierSetC1::load_reference_barrier(LIRGenerator* gen, LIR_Opr obj, LIR_Opr addr, bool is_native) {\n+LIR_Opr ShenandoahBarrierSetC1::load_reference_barrier(LIRGenerator* gen, LIR_Opr obj, LIR_Opr addr, ShenandoahBarrierSet::ShenandoahLRBKind kind) {\n@@ -112,1 +114,1 @@\n-    return load_reference_barrier_impl(gen, obj, addr, is_native);\n+    return load_reference_barrier_impl(gen, obj, addr, kind);\n@@ -118,1 +120,1 @@\n-LIR_Opr ShenandoahBarrierSetC1::load_reference_barrier_impl(LIRGenerator* gen, LIR_Opr obj, LIR_Opr addr, bool is_native) {\n+LIR_Opr ShenandoahBarrierSetC1::load_reference_barrier_impl(LIRGenerator* gen, LIR_Opr obj, LIR_Opr addr, ShenandoahBarrierSet::ShenandoahLRBKind kind) {\n@@ -151,1 +153,1 @@\n-  CodeStub* slow = new ShenandoahLoadReferenceBarrierStub(obj, addr, result, tmp1, tmp2, is_native);\n+  CodeStub* slow = new ShenandoahLoadReferenceBarrierStub(obj, addr, result, tmp1, tmp2, kind);\n@@ -214,2 +216,2 @@\n-    bool is_native = ShenandoahBarrierSet::use_load_reference_barrier_native(decorators, type);\n-    tmp = load_reference_barrier(gen, tmp, access.resolved_addr(), is_native);\n+    ShenandoahBarrierSet::ShenandoahLRBKind kind = ShenandoahBarrierSet::access_kind(decorators, type);\n+    tmp = load_reference_barrier(gen, tmp, access.resolved_addr(), kind);\n@@ -254,1 +256,1 @@\n-  const bool _is_native;\n+  const ShenandoahBarrierSet::ShenandoahLRBKind _kind;\n@@ -257,1 +259,1 @@\n-  C1ShenandoahLoadReferenceBarrierCodeGenClosure(bool is_native) : _is_native(is_native) {}\n+  C1ShenandoahLoadReferenceBarrierCodeGenClosure(ShenandoahBarrierSet::ShenandoahLRBKind kind) : _kind(kind) {}\n@@ -261,1 +263,1 @@\n-    bs->generate_c1_load_reference_barrier_runtime_stub(sasm, _is_native);\n+    bs->generate_c1_load_reference_barrier_runtime_stub(sasm, _kind);\n@@ -272,2 +274,2 @@\n-    C1ShenandoahLoadReferenceBarrierCodeGenClosure lrb_code_gen_cl(false);\n-    _load_reference_barrier_rt_code_blob = Runtime1::generate_blob(buffer_blob, -1,\n+    C1ShenandoahLoadReferenceBarrierCodeGenClosure lrb_code_gen_cl(ShenandoahBarrierSet::NORMAL);\n+    _load_reference_barrier_normal_rt_code_blob = Runtime1::generate_blob(buffer_blob, -1,\n@@ -277,1 +279,1 @@\n-    C1ShenandoahLoadReferenceBarrierCodeGenClosure lrb_native_code_gen_cl(true);\n+    C1ShenandoahLoadReferenceBarrierCodeGenClosure lrb_native_code_gen_cl(ShenandoahBarrierSet::NATIVE);\n@@ -281,0 +283,5 @@\n+\n+    C1ShenandoahLoadReferenceBarrierCodeGenClosure lrb_weakref_code_gen_cl(ShenandoahBarrierSet::WEAK);\n+    _load_reference_barrier_weakref_rt_code_blob = Runtime1::generate_blob(buffer_blob, -1,\n+                                                                          \"shenandoah_load_reference_barrier_weakref_slow\",\n+                                                                          false, &lrb_weakref_code_gen_cl);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/c1\/shenandoahBarrierSetC1.cpp","additions":20,"deletions":13,"binary":false,"changes":33,"status":"modified"},{"patch":"@@ -97,1 +97,1 @@\n-  bool _is_native;\n+  ShenandoahBarrierSet::ShenandoahLRBKind _kind;\n@@ -99,2 +99,2 @@\n-  ShenandoahLoadReferenceBarrierStub(LIR_Opr obj, LIR_Opr addr, LIR_Opr result, LIR_Opr tmp1, LIR_Opr tmp2, bool is_native) :\n-          _obj(obj), _addr(addr), _result(result), _tmp1(tmp1), _tmp2(tmp2), _is_native(is_native)\n+  ShenandoahLoadReferenceBarrierStub(LIR_Opr obj, LIR_Opr addr, LIR_Opr result, LIR_Opr tmp1, LIR_Opr tmp2, ShenandoahBarrierSet::ShenandoahLRBKind kind) :\n+          _obj(obj), _addr(addr), _result(result), _tmp1(tmp1), _tmp2(tmp2), _kind(kind)\n@@ -114,1 +114,1 @@\n-  bool is_native() const { return _is_native; }\n+  ShenandoahBarrierSet::ShenandoahLRBKind kind() const { return _kind; }\n@@ -193,1 +193,1 @@\n-  CodeBlob* _load_reference_barrier_rt_code_blob;\n+  CodeBlob* _load_reference_barrier_normal_rt_code_blob;\n@@ -195,0 +195,1 @@\n+  CodeBlob* _load_reference_barrier_weakref_rt_code_blob;\n@@ -198,1 +199,1 @@\n-  LIR_Opr load_reference_barrier(LIRGenerator* gen, LIR_Opr obj, LIR_Opr addr, bool is_native);\n+  LIR_Opr load_reference_barrier(LIRGenerator* gen, LIR_Opr obj, LIR_Opr addr, ShenandoahBarrierSet::ShenandoahLRBKind kind);\n@@ -201,1 +202,1 @@\n-  LIR_Opr load_reference_barrier_impl(LIRGenerator* gen, LIR_Opr obj, LIR_Opr addr, bool is_native);\n+  LIR_Opr load_reference_barrier_impl(LIRGenerator* gen, LIR_Opr obj, LIR_Opr addr, ShenandoahBarrierSet::ShenandoahLRBKind kind);\n@@ -213,3 +214,3 @@\n-  CodeBlob* load_reference_barrier_rt_code_blob() {\n-    assert(_load_reference_barrier_rt_code_blob != NULL, \"\");\n-    return _load_reference_barrier_rt_code_blob;\n+  CodeBlob* load_reference_barrier_normal_rt_code_blob() {\n+    assert(_load_reference_barrier_normal_rt_code_blob != NULL, \"\");\n+    return _load_reference_barrier_normal_rt_code_blob;\n@@ -222,0 +223,5 @@\n+\n+  CodeBlob* load_reference_barrier_weakref_rt_code_blob() {\n+    assert(_load_reference_barrier_weakref_rt_code_blob != NULL, \"\");\n+    return _load_reference_barrier_weakref_rt_code_blob;\n+  }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/c1\/shenandoahBarrierSetC1.hpp","additions":16,"deletions":10,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -307,1 +307,2 @@\n-         (entry_point == CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_native));\n+         (entry_point == CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_native)) ||\n+         (entry_point == CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_native_narrow));\n@@ -547,3 +548,2 @@\n-    load = new ShenandoahLoadReferenceBarrierNode(NULL,\n-                                                  load,\n-                                                  ShenandoahBarrierSet::use_load_reference_barrier_native(decorators, type));\n+    ShenandoahBarrierSet::ShenandoahLRBKind kind = ShenandoahBarrierSet::access_kind(decorators, type);\n+    load = new ShenandoahLoadReferenceBarrierNode(NULL, load, kind);\n@@ -646,1 +646,1 @@\n-    load_store = kit->gvn().transform(new ShenandoahLoadReferenceBarrierNode(NULL, load_store, false));\n+    load_store = kit->gvn().transform(new ShenandoahLoadReferenceBarrierNode(NULL, load_store, ShenandoahBarrierSet::NORMAL));\n@@ -714,1 +714,1 @@\n-    result = kit->gvn().transform(new ShenandoahLoadReferenceBarrierNode(NULL, result, false));\n+    result = kit->gvn().transform(new ShenandoahLoadReferenceBarrierNode(NULL, result, ShenandoahBarrierSet::NORMAL));\n@@ -1061,1 +1061,3 @@\n-    if (in1->bottom_type() == TypePtr::NULL_PTR) {\n+    if (in1->bottom_type() == TypePtr::NULL_PTR &&\n+        (in2->Opcode() != Op_ShenandoahLoadReferenceBarrier ||\n+         ((ShenandoahLoadReferenceBarrierNode*)in2)->kind() == ShenandoahBarrierSet::NORMAL)) {\n@@ -1064,1 +1066,3 @@\n-    if (in2->bottom_type() == TypePtr::NULL_PTR) {\n+    if (in2->bottom_type() == TypePtr::NULL_PTR &&\n+        (in1->Opcode() != Op_ShenandoahLoadReferenceBarrier ||\n+         ((ShenandoahLoadReferenceBarrierNode*)in1)->kind() == ShenandoahBarrierSet::NORMAL)) {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/c2\/shenandoahBarrierSetC2.cpp","additions":12,"deletions":8,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -964,1 +964,2 @@\n-void ShenandoahBarrierC2Support::call_lrb_stub(Node*& ctrl, Node*& val, Node* load_addr, Node*& result_mem, Node* raw_mem, bool is_native, PhaseIdealLoop* phase) {\n+void ShenandoahBarrierC2Support::call_lrb_stub(Node*& ctrl, Node*& val, Node* load_addr, Node*& result_mem, Node* raw_mem,\n+                                               ShenandoahBarrierSet::ShenandoahLRBKind kind, PhaseIdealLoop* phase) {\n@@ -975,7 +976,22 @@\n-  address target = LP64_ONLY(UseCompressedOops) NOT_LP64(false) ?\n-          CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_narrow) :\n-          CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier);\n-\n-  address calladdr = is_native ? CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_native)\n-                               : target;\n-  const char* name = is_native ? \"load_reference_barrier_native\" : \"load_reference_barrier\";\n+  address calladdr;\n+  const char* name;\n+  switch (kind) {\n+    case ShenandoahBarrierSet::NATIVE:\n+      calladdr = CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_native);\n+      name = \"load_reference_barrier_native\";\n+      break;\n+    case ShenandoahBarrierSet::WEAK:\n+      calladdr = LP64_ONLY(UseCompressedOops) NOT_LP64(false) ?\n+                 CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_native_narrow) :\n+                 CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_native);\n+      name = \"load_reference_barrier_weak\";\n+      break;\n+    case ShenandoahBarrierSet::NORMAL:\n+      calladdr = LP64_ONLY(UseCompressedOops) NOT_LP64(false) ?\n+                 CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_narrow) :\n+                 CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier);\n+      name = \"load_reference_barrier\";\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+  }\n@@ -1345,1 +1361,3 @@\n-    test_in_cset(ctrl, not_cset_ctrl, val, raw_mem, phase);\n+    if (lrb->kind() == ShenandoahBarrierSet::NORMAL) {\n+      test_in_cset(ctrl, not_cset_ctrl, val, raw_mem, phase);\n+    }\n@@ -1390,1 +1408,1 @@\n-    call_lrb_stub(ctrl, val, addr, result_mem, raw_mem, lrb->is_native(), phase);\n+    call_lrb_stub(ctrl, val, addr, result_mem, raw_mem, lrb->kind(), phase);\n@@ -2880,2 +2898,2 @@\n-ShenandoahLoadReferenceBarrierNode::ShenandoahLoadReferenceBarrierNode(Node* ctrl, Node* obj, bool native)\n-: Node(ctrl, obj), _native(native) {\n+ShenandoahLoadReferenceBarrierNode::ShenandoahLoadReferenceBarrierNode(Node* ctrl, Node* obj, ShenandoahBarrierSet::ShenandoahLRBKind kind)\n+: Node(ctrl, obj), _kind(kind) {\n@@ -2885,2 +2903,2 @@\n-bool ShenandoahLoadReferenceBarrierNode::is_native() const {\n-  return _native;\n+ShenandoahBarrierSet::ShenandoahLRBKind ShenandoahLoadReferenceBarrierNode::kind() const {\n+  return _kind;\n@@ -2894,1 +2912,1 @@\n-  return Node::hash() + (_native ? 1 : 0);\n+  return Node::hash() + _kind;\n@@ -2899,1 +2917,1 @@\n-         _native == ((const ShenandoahLoadReferenceBarrierNode&)n)._native;\n+         _kind == ((const ShenandoahLoadReferenceBarrierNode&)n)._kind;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/c2\/shenandoahSupport.cpp","additions":34,"deletions":16,"binary":false,"changes":50,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"gc\/shenandoah\/shenandoahBarrierSet.hpp\"\n@@ -63,1 +64,2 @@\n-  static void call_lrb_stub(Node*& ctrl, Node*& val, Node* load_addr, Node*& result_mem, Node* raw_mem, bool is_native, PhaseIdealLoop* phase);\n+  static void call_lrb_stub(Node*& ctrl, Node*& val, Node* load_addr, Node*& result_mem, Node* raw_mem,\n+                            ShenandoahBarrierSet::ShenandoahLRBKind kind, PhaseIdealLoop* phase);\n@@ -232,1 +234,1 @@\n-  bool _native;\n+  ShenandoahBarrierSet::ShenandoahLRBKind _kind;\n@@ -235,1 +237,1 @@\n-  ShenandoahLoadReferenceBarrierNode(Node* ctrl, Node* val, bool native);\n+  ShenandoahLoadReferenceBarrierNode(Node* ctrl, Node* val, ShenandoahBarrierSet::ShenandoahLRBKind kind);\n@@ -237,1 +239,1 @@\n-  bool is_native() const;\n+  ShenandoahBarrierSet::ShenandoahLRBKind kind() const;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/c2\/shenandoahSupport.hpp","additions":6,"deletions":4,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -64,6 +64,0 @@\n-bool ShenandoahAggressiveHeuristics::should_process_references() {\n-  if (!can_process_references()) return false;\n-  \/\/ Randomly process refs with 50% chance.\n-  return (os::random() & 1) == 1;\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahAggressiveHeuristics.cpp","additions":0,"deletions":6,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -40,2 +40,0 @@\n-  virtual bool should_process_references();\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahAggressiveHeuristics.hpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -260,12 +260,0 @@\n-bool ShenandoahHeuristics::can_process_references() {\n-  if (ShenandoahRefProcFrequency == 0) return false;\n-  return true;\n-}\n-\n-bool ShenandoahHeuristics::should_process_references() {\n-  if (!can_process_references()) return false;\n-  size_t cycle = ShenandoahHeap::heap()->shenandoah_policy()->cycle_counter();\n-  \/\/ Process references every Nth GC cycle.\n-  return cycle % ShenandoahRefProcFrequency == 0;\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahHeuristics.cpp","additions":0,"deletions":12,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -123,3 +123,0 @@\n-  virtual bool can_process_references();\n-  virtual bool should_process_references();\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahHeuristics.hpp","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -39,5 +39,0 @@\n-bool ShenandoahPassiveHeuristics::should_process_references() {\n-  \/\/ Always process references, if we can.\n-  return can_process_references();\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahPassiveHeuristics.cpp","additions":0,"deletions":5,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -34,2 +34,0 @@\n-  virtual bool should_process_references();\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahPassiveHeuristics.hpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -71,1 +71,2 @@\n-  msg.append(\"    %3s marked \\n\",                    ctx->is_marked(obj) ? \"\" : \"not\");\n+  msg.append(\"    %3s marked strong\\n\",              ctx->is_marked_strong(obj) ? \"\" : \"not\");\n+  msg.append(\"    %3s marked final\\n\",               ctx->is_marked_final(obj) ? \"\" : \"not\");\n@@ -346,18 +347,0 @@\n-void ShenandoahAsserts::assert_rp_isalive_not_installed(const char *file, int line) {\n-  ShenandoahHeap* heap = ShenandoahHeap::heap();\n-  ReferenceProcessor* rp = heap->ref_processor();\n-  if (rp->is_alive_non_header() != NULL) {\n-    print_rp_failure(\"Shenandoah assert_rp_isalive_not_installed failed\", rp->is_alive_non_header(),\n-                     file, line);\n-  }\n-}\n-\n-void ShenandoahAsserts::assert_rp_isalive_installed(const char *file, int line) {\n-  ShenandoahHeap* heap = ShenandoahHeap::heap();\n-  ReferenceProcessor* rp = heap->ref_processor();\n-  if (rp->is_alive_non_header() == NULL) {\n-    print_rp_failure(\"Shenandoah assert_rp_isalive_installed failed\", rp->is_alive_non_header(),\n-                     file, line);\n-  }\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahAsserts.cpp","additions":2,"deletions":19,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -66,3 +66,0 @@\n-\n-  static void assert_rp_isalive_not_installed(const char *file, int line);\n-  static void assert_rp_isalive_installed(const char *file, int line);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahAsserts.hpp","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -91,1 +91,1 @@\n-  assert(need_load_reference_barrier(decorators, type), \"Should be subset of LRB\");\n+  \/\/assert(need_load_reference_barrier(decorators, type), \"Should be subset of LRB\");\n@@ -97,2 +97,4 @@\n-\n-  return (decorators & IN_NATIVE) != 0;\n+  bool on_native  = (decorators & IN_NATIVE) != 0;\n+  bool on_weak    = (decorators & ON_WEAK_OOP_REF) != 0;\n+  bool on_phantom = (decorators & ON_PHANTOM_OOP_REF) != 0;\n+  return on_native || on_weak || on_phantom;\n@@ -112,0 +114,17 @@\n+bool ShenandoahBarrierSet::is_access_on_jlr_reference(DecoratorSet decorators, BasicType type) {\n+  \/\/ Only needed for references\n+  if (!is_reference_type(type)) return false;\n+\n+  return (decorators & (ON_WEAK_OOP_REF | ON_PHANTOM_OOP_REF)) != 0;\n+}\n+\n+ShenandoahBarrierSet::ShenandoahLRBKind ShenandoahBarrierSet::access_kind(DecoratorSet decorators, BasicType type) {\n+  if ((decorators & IN_NATIVE) != 0) {\n+    return NATIVE;\n+  } else if ((decorators & (ON_WEAK_OOP_REF | ON_PHANTOM_OOP_REF)) != 0) {\n+    return WEAK;\n+  } else {\n+    return NORMAL;\n+  }\n+}\n+\n@@ -182,33 +201,0 @@\n-oop ShenandoahBarrierSet::load_reference_barrier_native(oop obj, oop* load_addr) {\n-  return load_reference_barrier_native_impl(obj, load_addr);\n-}\n-\n-oop ShenandoahBarrierSet::load_reference_barrier_native(oop obj, narrowOop* load_addr) {\n-  return load_reference_barrier_native_impl(obj, load_addr);\n-}\n-\n-template <class T>\n-oop ShenandoahBarrierSet::load_reference_barrier_native_impl(oop obj, T* load_addr) {\n-  if (CompressedOops::is_null(obj)) {\n-    return NULL;\n-  }\n-\n-  ShenandoahMarkingContext* const marking_context = _heap->marking_context();\n-  if (_heap->is_concurrent_weak_root_in_progress() && !marking_context->is_marked(obj)) {\n-    Thread* thr = Thread::current();\n-    if (thr->is_Java_thread()) {\n-      return NULL;\n-    } else {\n-      return obj;\n-    }\n-  }\n-\n-  oop fwd = load_reference_barrier_not_null(obj);\n-  if (ShenandoahSelfFixing && load_addr != NULL && fwd != obj) {\n-    \/\/ Since we are here and we know the load address, update the reference.\n-    ShenandoahHeap::cas_oop(fwd, load_addr, obj);\n-  }\n-\n-  return fwd;\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahBarrierSet.cpp","additions":22,"deletions":36,"binary":false,"changes":58,"status":"modified"},{"patch":"@@ -36,0 +36,6 @@\n+public:\n+  enum ShenandoahLRBKind {\n+    NORMAL,\n+    NATIVE,\n+    WEAK\n+  };\n@@ -58,0 +64,2 @@\n+  static bool is_access_on_jlr_reference(DecoratorSet decorators, BasicType type);\n+  static ShenandoahLRBKind access_kind(DecoratorSet decorators, BasicType type);\n@@ -96,2 +104,2 @@\n-  oop load_reference_barrier_native(oop obj, oop* load_addr);\n-  oop load_reference_barrier_native(oop obj, narrowOop* load_addr);\n+  template<typename T>\n+  inline oop load_reference_barrier_native(oop obj, T* load_addr);\n@@ -116,3 +124,0 @@\n-  template <class T>\n-  oop load_reference_barrier_native_impl(oop obj, T* load_addr);\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahBarrierSet.hpp","additions":10,"deletions":5,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -79,0 +79,27 @@\n+template <class T>\n+inline oop ShenandoahBarrierSet::load_reference_barrier_native(oop obj, T* load_addr) {\n+  if (CompressedOops::is_null(obj)) {\n+    return NULL;\n+  }\n+\n+  ShenandoahMarkingContext* const marking_context = _heap->marking_context();\n+  if (_heap->is_concurrent_weak_root_in_progress() && !marking_context->is_marked(obj)) {\n+    Thread* thr = Thread::current();\n+    if (thr->is_Java_thread()) {\n+      return NULL;\n+    } else {\n+      log_trace(gc,ref)(\"LRB-native returning naked oop: \" PTR_FORMAT, p2i(obj));\n+      return obj;\n+    }\n+  }\n+\n+  oop fwd = load_reference_barrier_not_null(obj);\n+  if (ShenandoahSelfFixing && load_addr != NULL && fwd != obj) {\n+    \/\/ Since we are here and we know the load address, update the reference.\n+    ShenandoahHeap::cas_oop(fwd, load_addr, obj);\n+  }\n+  assert(!_heap->in_collection_set(fwd) || !_heap->has_forwarded_objects() || _heap->cancelled_gc(), \"must not return cset-object from LRB-native\");\n+  log_trace(gc,ref)(\"Reference or native access\/resurrection: obj: \" PTR_FORMAT \", fwd: \" PTR_FORMAT \", has_fwd: %s\", p2i(obj), p2i(fwd), BOOL_TO_STR(_heap->has_forwarded_objects()));\n+  return fwd;\n+}\n+\n@@ -158,2 +185,8 @@\n-    value = bs->load_reference_barrier_not_null(value);\n-    bs->keep_alive_if_weak<decorators>(value);\n+    if (bs->use_load_reference_barrier_native(decorators, T_OBJECT)) {\n+      value = bs->load_reference_barrier_native(value, addr);\n+    } else {\n+      value = bs->load_reference_barrier_not_null(value);\n+    }\n+    if (value != NULL) {\n+      bs->keep_alive_if_weak<decorators>(value);\n+    }\n@@ -169,3 +202,9 @@\n-    value = bs->load_reference_barrier_not_null(value);\n-    bs->keep_alive_if_weak(AccessBarrierSupport::resolve_possibly_unknown_oop_ref_strength<decorators>(base, offset),\n-                           value);\n+    DecoratorSet resolved_decorators = AccessBarrierSupport::resolve_possibly_unknown_oop_ref_strength<decorators>(base, offset);\n+    if (bs->use_load_reference_barrier_native(resolved_decorators, T_OBJECT)) {\n+      value = bs->load_reference_barrier_native(value, AccessInternal::oop_field_addr<decorators>(base, offset));\n+    } else {\n+      value = bs->load_reference_barrier_not_null(value);\n+    }\n+    if (value != NULL) {\n+      bs->keep_alive_if_weak(resolved_decorators, value);\n+    }\n@@ -180,0 +219,1 @@\n+  shenandoah_assert_not_in_cset_if(addr, value, value != NULL && !ShenandoahHeap::heap()->cancelled_gc());\n@@ -306,1 +346,1 @@\n-      if (ENQUEUE && !ctx->is_marked(obj)) {\n+      if (ENQUEUE && !ctx->par_is_marked_strong(obj)) {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahBarrierSet.inline.hpp","additions":46,"deletions":6,"binary":false,"changes":52,"status":"modified"},{"patch":"@@ -34,2 +34,0 @@\n-#include \"gc\/shared\/referenceProcessor.hpp\"\n-#include \"gc\/shared\/referenceProcessorPhaseTimes.hpp\"\n@@ -43,0 +41,1 @@\n+#include \"gc\/shenandoah\/shenandoahReferenceProcessor.hpp\"\n@@ -64,1 +63,1 @@\n-    ShenandoahConcurrentMark::mark_through_ref<T, UPDATE_REFS, NO_DEDUP>(p, _heap, _queue, _mark_context);\n+    ShenandoahConcurrentMark::mark_through_ref<T, UPDATE_REFS, NO_DEDUP>(p, _heap, _queue, _mark_context, true);\n@@ -77,1 +76,1 @@\n-ShenandoahMarkRefsSuperClosure::ShenandoahMarkRefsSuperClosure(ShenandoahObjToScanQueue* q, ReferenceProcessor* rp) :\n+ShenandoahMarkRefsSuperClosure::ShenandoahMarkRefsSuperClosure(ShenandoahObjToScanQueue* q, ShenandoahReferenceProcessor* rp) :\n@@ -81,1 +80,2 @@\n-  _mark_context(_heap->marking_context())\n+  _mark_context(_heap->marking_context()),\n+  _strong(true)\n@@ -156,8 +156,2 @@\n-    ReferenceProcessor* rp;\n-    if (heap->process_references()) {\n-      rp = heap->ref_processor();\n-      shenandoah_assert_rp_isalive_installed();\n-    } else {\n-      rp = NULL;\n-    }\n-\n+    ShenandoahReferenceProcessor* rp = heap->ref_processor();\n+    assert(rp != NULL, \"need reference processor\");\n@@ -209,1 +203,1 @@\n-  ReferenceProcessor*             _rp;\n+  ShenandoahReferenceProcessor*   _rp;\n@@ -225,6 +219,1 @@\n-  _rp(NULL) {\n-  ShenandoahHeap* heap = ShenandoahHeap::heap();\n-  if (heap->process_references()) {\n-    _rp = heap->ref_processor();\n-    shenandoah_assert_rp_isalive_installed();\n-  }\n+  _rp(ShenandoahHeap::heap()->ref_processor()) {\n@@ -256,7 +245,1 @@\n-    ReferenceProcessor* rp;\n-    if (heap->process_references()) {\n-      rp = heap->ref_processor();\n-      shenandoah_assert_rp_isalive_installed();\n-    } else {\n-      rp = NULL;\n-    }\n+    ShenandoahReferenceProcessor* rp = heap->ref_processor();\n@@ -306,1 +289,0 @@\n-\n@@ -309,0 +291,4 @@\n+  ShenandoahReferenceProcessor* ref_processor = heap->ref_processor();\n+  ref_processor->reset_thread_locals();\n+  ref_processor->set_soft_reference_policy(_heap->soft_ref_policy()->should_clear_all_soft_refs());\n+\n@@ -410,1 +396,1 @@\n-  SuspendibleThreadSetJoiner         _sts_joiner;\n+  SuspendibleThreadSetJoiner          _sts_joiner;\n@@ -412,2 +398,2 @@\n-  ShenandoahObjToScanQueueSet* const _queue_set;\n-  ReferenceProcessor* const          _rp;\n+  ShenandoahObjToScanQueueSet* const  _queue_set;\n+  ShenandoahReferenceProcessor* const _rp;\n@@ -417,1 +403,1 @@\n-                                    ReferenceProcessor* rp,\n+                                    ShenandoahReferenceProcessor* rp,\n@@ -424,1 +410,1 @@\n-                                                                     ReferenceProcessor* rp,\n+                                                                     ShenandoahReferenceProcessor* rp,\n@@ -445,9 +431,2 @@\n-  ReferenceProcessor* rp = NULL;\n-  if (_heap->process_references()) {\n-    rp = _heap->ref_processor();\n-    rp->set_active_mt_degree(nworkers);\n-\n-    \/\/ enable (\"weak\") refs discovery\n-    rp->enable_discovery(true \/*verify_no_refs*\/);\n-    rp->setup_policy(_heap->soft_ref_policy()->should_clear_all_soft_refs());\n-  }\n+  ShenandoahReferenceProcessor* rp = _heap->ref_processor();\n+  rp->set_active_mt_degree(nworkers);\n@@ -455,3 +434,2 @@\n-  shenandoah_assert_rp_isalive_not_installed();\n-  ShenandoahIsAliveSelector is_alive;\n-  ReferenceProcessorIsAliveMutator fix_isalive(_heap->ref_processor(), is_alive.is_alive_closure());\n+  \/\/ enable (\"weak\") refs discovery\n+  rp->enable_discovery(true \/*verify_no_refs*\/);\n@@ -483,4 +461,0 @@\n-    shenandoah_assert_rp_isalive_not_installed();\n-    ShenandoahIsAliveSelector is_alive;\n-    ReferenceProcessorIsAliveMutator fix_isalive(_heap->ref_processor(), is_alive.is_alive_closure());\n-\n@@ -527,5 +501,0 @@\n-  \/\/ When we're done marking everything, we process weak references.\n-  if (_heap->process_references()) {\n-    weak_refs_work(full_gc);\n-  }\n-\n@@ -537,327 +506,0 @@\n-\/\/ Weak Reference Closures\n-class ShenandoahCMDrainMarkingStackClosure: public VoidClosure {\n-  uint _worker_id;\n-  TaskTerminator* _terminator;\n-  bool _reset_terminator;\n-\n-public:\n-  ShenandoahCMDrainMarkingStackClosure(uint worker_id, TaskTerminator* t, bool reset_terminator = false):\n-    _worker_id(worker_id),\n-    _terminator(t),\n-    _reset_terminator(reset_terminator) {\n-  }\n-\n-  void do_void() {\n-    assert(ShenandoahSafepoint::is_at_shenandoah_safepoint(), \"Must be at a safepoint\");\n-\n-    ShenandoahHeap* sh = ShenandoahHeap::heap();\n-    ShenandoahConcurrentMark* scm = sh->concurrent_mark();\n-    assert(sh->process_references(), \"why else would we be here?\");\n-    ReferenceProcessor* rp = sh->ref_processor();\n-\n-    shenandoah_assert_rp_isalive_installed();\n-\n-    scm->mark_loop(_worker_id, _terminator, rp,\n-                   false,   \/\/ not cancellable\n-                   false);  \/\/ do not do strdedup\n-\n-    if (_reset_terminator) {\n-      _terminator->reset_for_reuse();\n-    }\n-  }\n-};\n-\n-class ShenandoahCMKeepAliveClosure : public OopClosure {\n-private:\n-  ShenandoahObjToScanQueue* _queue;\n-  ShenandoahHeap* _heap;\n-  ShenandoahMarkingContext* const _mark_context;\n-\n-  template <class T>\n-  inline void do_oop_work(T* p) {\n-    ShenandoahConcurrentMark::mark_through_ref<T, NONE, NO_DEDUP>(p, _heap, _queue, _mark_context);\n-  }\n-\n-public:\n-  ShenandoahCMKeepAliveClosure(ShenandoahObjToScanQueue* q) :\n-    _queue(q),\n-    _heap(ShenandoahHeap::heap()),\n-    _mark_context(_heap->marking_context()) {}\n-\n-  void do_oop(narrowOop* p) { do_oop_work(p); }\n-  void do_oop(oop* p)       { do_oop_work(p); }\n-};\n-\n-class ShenandoahCMKeepAliveUpdateClosure : public OopClosure {\n-private:\n-  ShenandoahObjToScanQueue* _queue;\n-  ShenandoahHeap* _heap;\n-  ShenandoahMarkingContext* const _mark_context;\n-\n-  template <class T>\n-  inline void do_oop_work(T* p) {\n-    ShenandoahConcurrentMark::mark_through_ref<T, SIMPLE, NO_DEDUP>(p, _heap, _queue, _mark_context);\n-  }\n-\n-public:\n-  ShenandoahCMKeepAliveUpdateClosure(ShenandoahObjToScanQueue* q) :\n-    _queue(q),\n-    _heap(ShenandoahHeap::heap()),\n-    _mark_context(_heap->marking_context()) {}\n-\n-  void do_oop(narrowOop* p) { do_oop_work(p); }\n-  void do_oop(oop* p)       { do_oop_work(p); }\n-};\n-\n-class ShenandoahWeakUpdateClosure : public OopClosure {\n-private:\n-  ShenandoahHeap* const _heap;\n-\n-  template <class T>\n-  inline void do_oop_work(T* p) {\n-    oop o = _heap->maybe_update_with_forwarded(p);\n-    shenandoah_assert_marked_except(p, o, o == NULL);\n-  }\n-\n-public:\n-  ShenandoahWeakUpdateClosure() : _heap(ShenandoahHeap::heap()) {}\n-\n-  void do_oop(narrowOop* p) { do_oop_work(p); }\n-  void do_oop(oop* p)       { do_oop_work(p); }\n-};\n-\n-class ShenandoahRefProcTaskProxy : public AbstractGangTask {\n-private:\n-  AbstractRefProcTaskExecutor::ProcessTask& _proc_task;\n-  TaskTerminator* _terminator;\n-\n-public:\n-  ShenandoahRefProcTaskProxy(AbstractRefProcTaskExecutor::ProcessTask& proc_task,\n-                             TaskTerminator* t) :\n-    AbstractGangTask(\"Shenandoah Process Weak References\"),\n-    _proc_task(proc_task),\n-    _terminator(t) {\n-  }\n-\n-  void work(uint worker_id) {\n-    Thread* current_thread = Thread::current();\n-    ResourceMark rm(current_thread);\n-    HandleMark hm(current_thread);\n-    assert(ShenandoahSafepoint::is_at_shenandoah_safepoint(), \"Must be at a safepoint\");\n-    ShenandoahHeap* heap = ShenandoahHeap::heap();\n-    ShenandoahParallelWorkerSession worker_session(worker_id);\n-    ShenandoahCMDrainMarkingStackClosure complete_gc(worker_id, _terminator);\n-    if (heap->has_forwarded_objects()) {\n-      ShenandoahForwardedIsAliveClosure is_alive;\n-      ShenandoahCMKeepAliveUpdateClosure keep_alive(heap->concurrent_mark()->get_queue(worker_id));\n-      _proc_task.work(worker_id, is_alive, keep_alive, complete_gc);\n-    } else {\n-      ShenandoahIsAliveClosure is_alive;\n-      ShenandoahCMKeepAliveClosure keep_alive(heap->concurrent_mark()->get_queue(worker_id));\n-      _proc_task.work(worker_id, is_alive, keep_alive, complete_gc);\n-    }\n-  }\n-};\n-\n-class ShenandoahRefProcTaskExecutor : public AbstractRefProcTaskExecutor {\n-private:\n-  WorkGang* _workers;\n-\n-public:\n-  ShenandoahRefProcTaskExecutor(WorkGang* workers) :\n-    _workers(workers) {\n-  }\n-\n-  \/\/ Executes a task using worker threads.\n-  void execute(ProcessTask& task, uint ergo_workers) {\n-    assert(ShenandoahSafepoint::is_at_shenandoah_safepoint(), \"Must be at a safepoint\");\n-\n-    ShenandoahHeap* heap = ShenandoahHeap::heap();\n-    ShenandoahConcurrentMark* cm = heap->concurrent_mark();\n-    ShenandoahPushWorkerQueuesScope scope(_workers, cm->task_queues(),\n-                                          ergo_workers,\n-                                          \/* do_check = *\/ false);\n-    uint nworkers = _workers->active_workers();\n-    cm->task_queues()->reserve(nworkers);\n-    TaskTerminator terminator(nworkers, cm->task_queues());\n-    ShenandoahRefProcTaskProxy proc_task_proxy(task, &terminator);\n-    _workers->run_task(&proc_task_proxy);\n-  }\n-};\n-\n-void ShenandoahConcurrentMark::weak_refs_work(bool full_gc) {\n-  assert(_heap->process_references(), \"sanity\");\n-\n-  ShenandoahPhaseTimings::Phase phase_root =\n-          full_gc ?\n-          ShenandoahPhaseTimings::full_gc_weakrefs :\n-          ShenandoahPhaseTimings::weakrefs;\n-\n-  ShenandoahGCPhase phase(phase_root);\n-\n-  ReferenceProcessor* rp = _heap->ref_processor();\n-\n-  \/\/ NOTE: We cannot shortcut on has_discovered_references() here, because\n-  \/\/ we will miss marking JNI Weak refs then, see implementation in\n-  \/\/ ReferenceProcessor::process_discovered_references.\n-  weak_refs_work_doit(full_gc);\n-\n-  rp->verify_no_references_recorded();\n-  assert(!rp->discovery_enabled(), \"Post condition\");\n-\n-}\n-\n-void ShenandoahConcurrentMark::weak_refs_work_doit(bool full_gc) {\n-  ReferenceProcessor* rp = _heap->ref_processor();\n-\n-  ShenandoahPhaseTimings::Phase phase_process =\n-          full_gc ?\n-          ShenandoahPhaseTimings::full_gc_weakrefs_process :\n-          ShenandoahPhaseTimings::weakrefs_process;\n-\n-  shenandoah_assert_rp_isalive_not_installed();\n-  ShenandoahIsAliveSelector is_alive;\n-  ReferenceProcessorIsAliveMutator fix_isalive(rp, is_alive.is_alive_closure());\n-\n-  WorkGang* workers = _heap->workers();\n-  uint nworkers = workers->active_workers();\n-\n-  rp->setup_policy(_heap->soft_ref_policy()->should_clear_all_soft_refs());\n-  rp->set_active_mt_degree(nworkers);\n-\n-  assert(task_queues()->is_empty(), \"Should be empty\");\n-\n-  \/\/ complete_gc and keep_alive closures instantiated here are only needed for\n-  \/\/ single-threaded path in RP. They share the queue 0 for tracking work, which\n-  \/\/ simplifies implementation. Since RP may decide to call complete_gc several\n-  \/\/ times, we need to be able to reuse the terminator.\n-  uint serial_worker_id = 0;\n-  TaskTerminator terminator(1, task_queues());\n-  ShenandoahCMDrainMarkingStackClosure complete_gc(serial_worker_id, &terminator, \/* reset_terminator = *\/ true);\n-\n-  ShenandoahRefProcTaskExecutor executor(workers);\n-\n-  ReferenceProcessorPhaseTimes pt(_heap->gc_timer(), rp->num_queues());\n-\n-  {\n-    \/\/ Note: Don't emit JFR event for this phase, to avoid overflow nesting phase level.\n-    \/\/ Reference Processor emits 2 levels JFR event, that can get us over the JFR\n-    \/\/ event nesting level limits, in case of degenerated GC gets upgraded to\n-    \/\/ full GC.\n-    ShenandoahTimingsTracker phase_timing(phase_process);\n-\n-    if (_heap->has_forwarded_objects()) {\n-      ShenandoahCMKeepAliveUpdateClosure keep_alive(get_queue(serial_worker_id));\n-      const ReferenceProcessorStats& stats =\n-        rp->process_discovered_references(is_alive.is_alive_closure(), &keep_alive,\n-                                          &complete_gc, &executor,\n-                                          &pt);\n-       _heap->tracer()->report_gc_reference_stats(stats);\n-    } else {\n-      ShenandoahCMKeepAliveClosure keep_alive(get_queue(serial_worker_id));\n-      const ReferenceProcessorStats& stats =\n-        rp->process_discovered_references(is_alive.is_alive_closure(), &keep_alive,\n-                                          &complete_gc, &executor,\n-                                          &pt);\n-      _heap->tracer()->report_gc_reference_stats(stats);\n-    }\n-\n-    pt.print_all_references();\n-\n-    assert(task_queues()->is_empty(), \"Should be empty\");\n-  }\n-}\n-\n-class ShenandoahCancelledGCYieldClosure : public YieldClosure {\n-private:\n-  ShenandoahHeap* const _heap;\n-public:\n-  ShenandoahCancelledGCYieldClosure() : _heap(ShenandoahHeap::heap()) {};\n-  virtual bool should_return() { return _heap->cancelled_gc(); }\n-};\n-\n-class ShenandoahPrecleanCompleteGCClosure : public VoidClosure {\n-public:\n-  void do_void() {\n-    ShenandoahHeap* sh = ShenandoahHeap::heap();\n-    ShenandoahConcurrentMark* scm = sh->concurrent_mark();\n-    assert(sh->process_references(), \"why else would we be here?\");\n-    TaskTerminator terminator(1, scm->task_queues());\n-\n-    ReferenceProcessor* rp = sh->ref_processor();\n-    shenandoah_assert_rp_isalive_installed();\n-\n-    scm->mark_loop(0, &terminator, rp,\n-                   false, \/\/ not cancellable\n-                   false); \/\/ do not do strdedup\n-  }\n-};\n-\n-class ShenandoahPrecleanTask : public AbstractGangTask {\n-private:\n-  ReferenceProcessor* _rp;\n-\n-public:\n-  ShenandoahPrecleanTask(ReferenceProcessor* rp) :\n-          AbstractGangTask(\"Shenandoah Precleaning\"),\n-          _rp(rp) {}\n-\n-  void work(uint worker_id) {\n-    assert(worker_id == 0, \"The code below is single-threaded, only one worker is expected\");\n-    ShenandoahParallelWorkerSession worker_session(worker_id);\n-\n-    ShenandoahHeap* sh = ShenandoahHeap::heap();\n-    assert(!sh->has_forwarded_objects(), \"No forwarded objects expected here\");\n-\n-    ShenandoahObjToScanQueue* q = sh->concurrent_mark()->get_queue(worker_id);\n-\n-    ShenandoahCancelledGCYieldClosure yield;\n-    ShenandoahPrecleanCompleteGCClosure complete_gc;\n-\n-    ShenandoahIsAliveClosure is_alive;\n-    ShenandoahCMKeepAliveClosure keep_alive(q);\n-    ResourceMark rm;\n-    _rp->preclean_discovered_references(&is_alive, &keep_alive,\n-                                        &complete_gc, &yield,\n-                                        NULL);\n-  }\n-};\n-\n-void ShenandoahConcurrentMark::preclean_weak_refs() {\n-  \/\/ Pre-cleaning weak references before diving into STW makes sense at the\n-  \/\/ end of concurrent mark. This will filter out the references which referents\n-  \/\/ are alive. Note that ReferenceProcessor already filters out these on reference\n-  \/\/ discovery, and the bulk of work is done here. This phase processes leftovers\n-  \/\/ that missed the initial filtering, i.e. when referent was marked alive after\n-  \/\/ reference was discovered by RP.\n-\n-  assert(_heap->process_references(), \"sanity\");\n-\n-  \/\/ Shortcut if no references were discovered to avoid winding up threads.\n-  ReferenceProcessor* rp = _heap->ref_processor();\n-  if (!rp->has_discovered_references()) {\n-    return;\n-  }\n-\n-  assert(task_queues()->is_empty(), \"Should be empty\");\n-\n-  ReferenceProcessorMTDiscoveryMutator fix_mt_discovery(rp, false);\n-\n-  shenandoah_assert_rp_isalive_not_installed();\n-  ShenandoahIsAliveSelector is_alive;\n-  ReferenceProcessorIsAliveMutator fix_isalive(rp, is_alive.is_alive_closure());\n-\n-  \/\/ Execute precleaning in the worker thread: it will give us GCLABs, String dedup\n-  \/\/ queues and other goodies. When upstream ReferenceProcessor starts supporting\n-  \/\/ parallel precleans, we can extend this to more threads.\n-  WorkGang* workers = _heap->workers();\n-  uint nworkers = workers->active_workers();\n-  assert(nworkers == 1, \"This code uses only a single worker\");\n-  task_queues()->reserve(nworkers);\n-\n-  ShenandoahPrecleanTask task(rp);\n-  workers->run_task(&task);\n-\n-  assert(task_queues()->is_empty(), \"Should be empty\");\n-}\n-\n@@ -879,1 +521,1 @@\n-void ShenandoahConcurrentMark::mark_loop_prework(uint w, TaskTerminator *t, ReferenceProcessor *rp,\n+void ShenandoahConcurrentMark::mark_loop_prework(uint w, TaskTerminator *t, ShenandoahReferenceProcessor* rp,\n@@ -937,0 +579,2 @@\n+  ShenandoahThreadLocalData::set_mark_closure(Thread::current(), cl);\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahConcurrentMark.cpp","additions":26,"deletions":382,"binary":false,"changes":408,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+class ShenandoahReferenceProcessor;\n@@ -63,1 +64,1 @@\n-  void mark_loop_prework(uint worker_id, TaskTerminator *terminator, ReferenceProcessor *rp, bool strdedup);\n+  void mark_loop_prework(uint worker_id, TaskTerminator *terminator, ShenandoahReferenceProcessor* rp, bool strdedup);\n@@ -66,1 +67,1 @@\n-  void mark_loop(uint worker_id, TaskTerminator* terminator, ReferenceProcessor *rp,\n+  void mark_loop(uint worker_id, TaskTerminator* terminator, ShenandoahReferenceProcessor* rp,\n@@ -76,1 +77,1 @@\n-  static inline void mark_through_ref(T* p, ShenandoahHeap* heap, ShenandoahObjToScanQueue* q, ShenandoahMarkingContext* const mark_context);\n+  static inline void mark_through_ref(T* p, ShenandoahHeap* heap, ShenandoahObjToScanQueue* q, ShenandoahMarkingContext* const mark_context, bool strong);\n@@ -85,9 +86,0 @@\n-\/\/ ---------- Weak references\n-\/\/\n-private:\n-  void weak_refs_work(bool full_gc);\n-  void weak_refs_work_doit(bool full_gc);\n-\n-public:\n-  void preclean_weak_refs();\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahConcurrentMark.hpp","additions":4,"deletions":12,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2019, Red Hat, Inc. All rights reserved.\n+ * Copyright (c) 2015, 2020, Red Hat, Inc. All rights reserved.\n@@ -48,0 +48,8 @@\n+  ShenandoahMarkingContext* ctx = _heap->marking_context();\n+  uintptr_t mark = ctx->par_marking_bits(obj);\n+#ifdef ASSERT\n+  if (!ShenandoahMarkBitMap::is_marked_strong(mark)) {\n+    assert(ShenandoahMarkBitMap::is_marked_final(mark), \"must be marked final if not marked strong\");\n+  }\n+#endif\n+  cl->set_strong(ShenandoahMarkBitMap::is_marked_strong(mark));\n@@ -64,1 +72,5 @@\n-    count_liveness(live_data, obj);\n+    \/\/ Avoid double-counting objects that are visited twice due to upgrade\n+    \/\/ from final- to strong mark.\n+    if (!ShenandoahMarkBitMap::is_marked_strong_and_final(mark)) {\n+      count_liveness(live_data, obj);\n+    }\n@@ -218,1 +230,1 @@\n-      ShenandoahConcurrentMark::mark_through_ref<oop, NONE, STRING_DEDUP>(p, _heap, _queue, _mark_context);\n+      ShenandoahConcurrentMark::mark_through_ref<oop, NONE, STRING_DEDUP>(p, _heap, _queue, _mark_context, true);\n@@ -224,1 +236,1 @@\n-inline void ShenandoahConcurrentMark::mark_through_ref(T *p, ShenandoahHeap* heap, ShenandoahObjToScanQueue* q, ShenandoahMarkingContext* const mark_context) {\n+inline void ShenandoahConcurrentMark::mark_through_ref(T *p, ShenandoahHeap* heap, ShenandoahObjToScanQueue* q, ShenandoahMarkingContext* const mark_context, bool strong) {\n@@ -255,1 +267,7 @@\n-      if (mark_context->mark(obj)) {\n+      bool marked;\n+      if (strong) {\n+        marked = mark_context->mark_strong(obj);\n+      } else {\n+        marked = mark_context->mark_final(obj);\n+      }\n+      if (marked) {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahConcurrentMark.inline.hpp","additions":23,"deletions":5,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -144,1 +144,0 @@\n-        heap->set_process_references(heuristics->can_process_references());\n@@ -161,1 +160,0 @@\n-        heap->set_process_references(heuristics->can_process_references());\n@@ -175,1 +173,0 @@\n-      heap->set_process_references(heuristics->should_process_references());\n@@ -407,3 +404,0 @@\n-  \/\/ If not cancelled, can try to concurrently pre-clean\n-  heap->entry_preclean();\n-\n@@ -415,0 +409,1 @@\n+    heap->entry_weak_refs();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahControlThread.cpp","additions":1,"deletions":6,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -58,0 +58,1 @@\n+#include \"gc\/shenandoah\/shenandoahReferenceProcessor.hpp\"\n@@ -209,1 +210,1 @@\n-  _bitmap_size = MarkBitMap::compute_size(heap_rs.size());\n+  _bitmap_size = ShenandoahMarkBitMap::compute_size(heap_rs.size());\n@@ -212,1 +213,1 @@\n-  size_t bitmap_bytes_per_region = reg_size_bytes \/ MarkBitMap::heap_map_factor();\n+  size_t bitmap_bytes_per_region = reg_size_bytes \/ ShenandoahMarkBitMap::heap_map_factor();\n@@ -396,3 +397,0 @@\n-  _ref_proc_mt_processing = ParallelRefProcEnabled && (ParallelGCThreads > 1);\n-  _ref_proc_mt_discovery = _max_workers > 1;\n-\n@@ -478,1 +476,1 @@\n-  _ref_processor(NULL),\n+  _ref_processor(new ShenandoahReferenceProcessor(MAX2(_max_workers, 1U))),\n@@ -618,2 +616,0 @@\n-  ref_processing_init();\n-\n@@ -1732,0 +1728,6 @@\n+    if (!collection_set()->is_empty()) {\n+      if (ShenandoahVerify) {\n+        verifier()->verify_before_evacuation();\n+      }\n+    }\n+\n@@ -1742,4 +1744,0 @@\n-      if (ShenandoahVerify) {\n-        verifier()->verify_before_evacuation();\n-      }\n-\n@@ -1794,7 +1792,5 @@\n-    if (process_references()) {\n-      \/\/ Abandon reference processing right away: pre-cleaning must have failed.\n-      ReferenceProcessor *rp = ref_processor();\n-      rp->disable_discovery();\n-      rp->abandon_partial_discovery();\n-      rp->verify_no_references_recorded();\n-    }\n+    \/\/ Abandon reference processing right away: pre-cleaning must have failed.\n+    ShenandoahReferenceProcessor* rp = ref_processor();\n+    rp->disable_discovery();\n+    rp->abandon_partial_discovery();\n+    rp->verify_no_references_recorded();\n@@ -1982,0 +1978,9 @@\n+void ShenandoahHeap::op_weak_refs() {\n+  \/\/ Concurrent weak refs processing\n+  {\n+    ShenandoahTimingsTracker t(ShenandoahPhaseTimings::conc_weak_refs_work);\n+    ShenandoahGCWorkerPhase worker_phase(ShenandoahPhaseTimings::conc_weak_refs_work);\n+    ref_processor()->process_references(workers());\n+  }\n+}\n+\n@@ -2055,7 +2060,0 @@\n-void ShenandoahHeap::op_preclean() {\n-  if (ShenandoahPacing) {\n-    pacer()->setup_for_preclean();\n-  }\n-  concurrent_mark()->preclean_weak_refs();\n-}\n-\n@@ -2103,1 +2101,0 @@\n-      set_process_references(heuristics()->can_process_references());\n@@ -2128,0 +2125,2 @@\n+      op_weak_refs();\n+\n@@ -2288,16 +2287,0 @@\n-void ShenandoahHeap::ref_processing_init() {\n-  assert(_max_workers > 0, \"Sanity\");\n-\n-  _ref_processor =\n-    new ReferenceProcessor(&_subject_to_discovery,  \/\/ is_subject_to_discovery\n-                           _ref_proc_mt_processing, \/\/ MT processing\n-                           _max_workers,            \/\/ Degree of MT processing\n-                           _ref_proc_mt_discovery,  \/\/ MT discovery\n-                           _max_workers,            \/\/ Degree of MT discovery\n-                           false,                   \/\/ Reference discovery is not atomic\n-                           NULL,                    \/\/ No closure, should be installed before use\n-                           true);                   \/\/ Scale worker threads\n-\n-  shenandoah_assert_rp_isalive_not_installed();\n-}\n-\n@@ -2439,4 +2422,0 @@\n-void ShenandoahHeap::set_process_references(bool pr) {\n-  _process_references.set_cond(pr);\n-}\n-\n@@ -2447,4 +2426,0 @@\n-bool ShenandoahHeap::process_references() const {\n-  return _process_references.is_set();\n-}\n-\n@@ -3034,0 +3009,13 @@\n+void ShenandoahHeap::entry_weak_refs() {\n+  static const char* msg = \"Concurrent weak references\";\n+  ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_weak_refs);\n+  EventMark em(\"%s\", msg);\n+\n+  ShenandoahWorkerScope scope(workers(),\n+                              ShenandoahWorkerPolicy::calc_workers_for_conc_root_processing(),\n+                              \"concurrent weak references\");\n+\n+  try_inject_alloc_failure();\n+  op_weak_refs();\n+}\n+\n@@ -3120,16 +3108,0 @@\n-void ShenandoahHeap::entry_preclean() {\n-  if (ShenandoahPreclean && process_references()) {\n-    static const char* msg = \"Concurrent precleaning\";\n-    ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_preclean);\n-    EventMark em(\"%s\", msg);\n-\n-    ShenandoahWorkerScope scope(workers(),\n-                                ShenandoahWorkerPolicy::calc_workers_for_conc_preclean(),\n-                                \"concurrent preclean\",\n-                                \/* check_workers = *\/ false);\n-\n-    try_inject_alloc_failure();\n-    op_preclean();\n-  }\n-}\n-\n@@ -3212,1 +3184,0 @@\n-  bool proc_refs = process_references();\n@@ -3215,5 +3186,1 @@\n-  if (proc_refs && unload_cls) {\n-    return \"Pause Init Mark (process weakrefs) (unload classes)\";\n-  } else if (proc_refs) {\n-    return \"Pause Init Mark (process weakrefs)\";\n-  } else if (unload_cls) {\n+  if (unload_cls) {\n@@ -3229,1 +3196,0 @@\n-  bool proc_refs = process_references();\n@@ -3232,5 +3198,1 @@\n-  if (proc_refs && unload_cls) {\n-    return \"Pause Final Mark (process weakrefs) (unload classes)\";\n-  } else if (proc_refs) {\n-    return \"Pause Final Mark (process weakrefs)\";\n-  } else if (unload_cls) {\n+  if (unload_cls) {\n@@ -3246,1 +3208,0 @@\n-  bool proc_refs = process_references();\n@@ -3249,5 +3210,1 @@\n-  if (proc_refs && unload_cls) {\n-    return \"Concurrent marking (process weakrefs) (unload classes)\";\n-  } else if (proc_refs) {\n-    return \"Concurrent marking (process weakrefs)\";\n-  } else if (unload_cls) {\n+  if (unload_cls) {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":42,"deletions":85,"binary":false,"changes":127,"status":"modified"},{"patch":"@@ -44,1 +44,0 @@\n-class ReferenceProcessor;\n@@ -62,1 +61,1 @@\n-class ShenandoahObjToScanQueueSet;\n+class ShenandoahReferenceProcessor;\n@@ -392,1 +391,1 @@\n-  void entry_preclean();\n+  void entry_weak_refs();\n@@ -416,1 +415,1 @@\n-  void op_preclean();\n+  void op_weak_refs();\n@@ -494,7 +493,1 @@\n-  AlwaysTrueClosure    _subject_to_discovery;\n-  ReferenceProcessor*  _ref_processor;\n-  ShenandoahSharedFlag _process_references;\n-  bool                 _ref_proc_mt_discovery;\n-  bool                 _ref_proc_mt_processing;\n-\n-  void ref_processing_init();\n+  ShenandoahReferenceProcessor* const _ref_processor;\n@@ -503,5 +496,1 @@\n-  ReferenceProcessor* ref_processor() { return _ref_processor; }\n-  bool ref_processor_mt_discovery()   { return _ref_proc_mt_discovery;  }\n-  bool ref_processor_mt_processing()  { return _ref_proc_mt_processing; }\n-  void set_process_references(bool pr);\n-  bool process_references() const;\n+  ShenandoahReferenceProcessor* ref_processor() { return _ref_processor; }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.hpp","additions":5,"deletions":16,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -331,1 +331,1 @@\n-  return !_marking_context->is_marked(obj);\n+  return !_marking_context->par_is_marked(obj);\n@@ -404,1 +404,1 @@\n-  MarkBitMap* mark_bit_map = ctx->mark_bit_map();\n+  const ShenandoahMarkBitMap* mark_bit_map = ctx->mark_bit_map();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.inline.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -60,4 +60,0 @@\n-\n-  log_info(gc, init)(\"Reference Processing: %s discovery, %s processing\",\n-                     heap->ref_processor_mt_discovery() ? \"Parallel\" : \"Serial\",\n-                     heap->ref_processor_mt_processing() ? \"Parallel\" : \"Serial\");\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahInitLogger.cpp","additions":0,"deletions":4,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -0,0 +1,145 @@\n+\/*\n+ * Copyright (c) 2020, Red Hat, Inc. and\/or its affiliates.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/shenandoah\/shenandoahMarkBitMap.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeap.inline.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+ShenandoahMarkBitMap::ShenandoahMarkBitMap(MemRegion heap, MemRegion storage) :\n+  _shift(LogMinObjAlignment),\n+  _covered(heap),\n+  _map((BitMap::bm_word_t*) storage.start()),\n+  _size((heap.word_size() * 2) >> _shift) {\n+}\n+\n+size_t ShenandoahMarkBitMap::compute_size(size_t heap_size) {\n+  return ReservedSpace::allocation_align_size_up(heap_size \/ mark_distance());\n+}\n+\n+size_t ShenandoahMarkBitMap::mark_distance() {\n+  return MinObjAlignmentInBytes * BitsPerByte \/ 2;\n+}\n+\n+HeapWord* ShenandoahMarkBitMap::get_next_marked_addr(const HeapWord* addr,\n+                                                     const HeapWord* limit) const {\n+  assert(limit != NULL, \"limit must not be NULL\");\n+  \/\/ Round addr up to a possible object boundary to be safe.\n+  size_t const addr_offset = address_to_index(align_up(addr, HeapWordSize << LogMinObjAlignment));\n+  size_t const limit_offset = address_to_index(limit);\n+  size_t const nextOffset = get_next_one_offset(addr_offset, limit_offset);\n+  return index_to_address(nextOffset);\n+}\n+\n+void ShenandoahMarkBitMap::clear_range_within_word(idx_t beg, idx_t end) {\n+  \/\/ With a valid range (beg <= end), this test ensures that end != 0, as\n+  \/\/ required by inverted_bit_mask_for_range.  Also avoids an unnecessary write.\n+  if (beg != end) {\n+    bm_word_t mask = inverted_bit_mask_for_range(beg, end);\n+    *word_addr(beg) &= mask;\n+  }\n+}\n+\n+void ShenandoahMarkBitMap::clear_range(idx_t beg, idx_t end) {\n+  verify_range(beg, end);\n+\n+  idx_t beg_full_word = to_words_align_up(beg);\n+  idx_t end_full_word = to_words_align_down(end);\n+\n+  if (beg_full_word < end_full_word) {\n+    \/\/ The range includes at least one full word.\n+    clear_range_within_word(beg, bit_index(beg_full_word));\n+    clear_range_of_words(beg_full_word, end_full_word);\n+    clear_range_within_word(bit_index(end_full_word), end);\n+  } else {\n+    \/\/ The range spans at most 2 partial words.\n+    idx_t boundary = MIN2(bit_index(beg_full_word), end);\n+    clear_range_within_word(beg, boundary);\n+    clear_range_within_word(boundary, end);\n+  }\n+}\n+\n+bool ShenandoahMarkBitMap::is_small_range_of_words(idx_t beg_full_word, idx_t end_full_word) {\n+  \/\/ There is little point to call large version on small ranges.\n+  \/\/ Need to check carefully, keeping potential idx_t over\/underflow in mind,\n+  \/\/ because beg_full_word > end_full_word can occur when beg and end are in\n+  \/\/ the same word.\n+  \/\/ The threshold should be at least one word.\n+  STATIC_ASSERT(small_range_words >= 1);\n+  return beg_full_word + small_range_words >= end_full_word;\n+}\n+\n+\n+void ShenandoahMarkBitMap::clear_large_range(idx_t beg, idx_t end) {\n+  verify_range(beg, end);\n+\n+  idx_t beg_full_word = to_words_align_up(beg);\n+  idx_t end_full_word = to_words_align_down(end);\n+\n+  if (is_small_range_of_words(beg_full_word, end_full_word)) {\n+    clear_range(beg, end);\n+    return;\n+  }\n+\n+  \/\/ The range includes at least one full word.\n+  clear_range_within_word(beg, bit_index(beg_full_word));\n+  clear_large_range_of_words(beg_full_word, end_full_word);\n+  clear_range_within_word(bit_index(end_full_word), end);\n+}\n+\n+void ShenandoahMarkBitMap::clear_range_large(MemRegion mr) {\n+  MemRegion intersection = mr.intersection(_covered);\n+  assert(!intersection.is_empty(),\n+         \"Given range from \" PTR_FORMAT \" to \" PTR_FORMAT \" is completely outside the heap\",\n+          p2i(mr.start()), p2i(mr.end()));\n+  \/\/ convert address range into offset range\n+  size_t beg = address_to_index(intersection.start());\n+  size_t end = address_to_index(intersection.end());\n+  clear_large_range(beg, end);\n+}\n+\n+#ifdef ASSERT\n+void ShenandoahMarkBitMap::check_mark(HeapWord* addr) const {\n+  assert(ShenandoahHeap::heap()->is_in(addr),\n+         \"Trying to access bitmap \" PTR_FORMAT \" for address \" PTR_FORMAT \" not in the heap.\",\n+         p2i(this), p2i(addr));\n+}\n+\n+void ShenandoahMarkBitMap::verify_index(idx_t bit) const {\n+  assert(bit < _size,\n+         \"BitMap index out of bounds: \" SIZE_FORMAT \" >= \" SIZE_FORMAT,\n+         bit, _size);\n+}\n+\n+void ShenandoahMarkBitMap::verify_limit(idx_t bit) const {\n+  assert(bit <= _size,\n+         \"BitMap limit out of bounds: \" SIZE_FORMAT \" > \" SIZE_FORMAT,\n+         bit, _size);\n+}\n+\n+void ShenandoahMarkBitMap::verify_range(idx_t beg, idx_t end) const {\n+  assert(beg <= end,\n+         \"BitMap range error: \" SIZE_FORMAT \" > \" SIZE_FORMAT, beg, end);\n+  verify_limit(end);\n+}\n+#endif\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMarkBitMap.cpp","additions":145,"deletions":0,"binary":false,"changes":145,"status":"added"},{"patch":"@@ -0,0 +1,196 @@\n+\/*\n+ * Copyright (c) 2020, Red Hat, Inc. and\/or its affiliates.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_VM_GC_SHENANDOAH_SHENANDOAHMARKBITMAP_HPP\n+#define SHARE_VM_GC_SHENANDOAH_SHENANDOAHMARKBITMAP_HPP\n+\n+#include \"memory\/memRegion.hpp\"\n+#include \"runtime\/atomic.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+class ShenandoahMarkBitMap {\n+public:\n+  typedef size_t idx_t;         \/\/ Type used for bit and word indices.\n+  typedef uintptr_t bm_word_t;  \/\/ Element type of array that represents the\n+                                \/\/ bitmap, with BitsPerWord bits per element.\n+\n+private:\n+  \/\/ Values for get_next_bit_impl flip parameter.\n+  static const bm_word_t find_ones_flip = 0;\n+  static const bm_word_t find_zeros_flip = ~(bm_word_t)0;\n+\n+  int const _shift;\n+  MemRegion _covered;\n+\n+  bm_word_t* _map;     \/\/ First word in bitmap\n+  idx_t      _size;    \/\/ Size of bitmap (in bits)\n+\n+  \/\/ Threshold for performing small range operation, even when large range\n+  \/\/ operation was requested. Measured in words.\n+  static const size_t small_range_words = 32;\n+\n+  static bool is_small_range_of_words(idx_t beg_full_word, idx_t end_full_word);\n+\n+  inline size_t address_to_index(const HeapWord* addr) const;\n+  inline HeapWord* index_to_address(size_t offset) const;\n+\n+  void check_mark(HeapWord* addr) const NOT_DEBUG_RETURN;\n+\n+  \/\/ Return a mask that will select the specified bit, when applied to the word\n+  \/\/ containing the bit.\n+  static bm_word_t bit_mask(idx_t bit) { return (bm_word_t)1 << bit_in_word(bit); }\n+\n+  \/\/ Return the bit number of the first bit in the specified word.\n+  static idx_t bit_index(idx_t word)  { return word << LogBitsPerWord; }\n+\n+  \/\/ Return the position of bit within the word that contains it (e.g., if\n+  \/\/ bitmap words are 32 bits, return a number 0 <= n <= 31).\n+  static idx_t bit_in_word(idx_t bit) { return bit & (BitsPerWord - 1); }\n+\n+  bm_word_t* map()                 { return _map; }\n+  const bm_word_t* map() const     { return _map; }\n+  bm_word_t map(idx_t word) const { return _map[word]; }\n+\n+  \/\/ Return a pointer to the word containing the specified bit.\n+  bm_word_t* word_addr(idx_t bit) {\n+    return map() + to_words_align_down(bit);\n+  }\n+\n+  const bm_word_t* word_addr(idx_t bit) const {\n+    return map() + to_words_align_down(bit);\n+  }\n+\n+  static inline const bm_word_t load_word_ordered(const volatile bm_word_t* const addr, atomic_memory_order memory_order);\n+\n+  bool at(idx_t index) const {\n+    verify_index(index);\n+    return (*word_addr(index) & bit_mask(index)) != 0;\n+  }\n+\n+  \/\/ Assumes relevant validity checking for bit has already been done.\n+  static idx_t raw_to_words_align_up(idx_t bit) {\n+    return raw_to_words_align_down(bit + (BitsPerWord - 1));\n+  }\n+\n+  \/\/ Assumes relevant validity checking for bit has already been done.\n+  static idx_t raw_to_words_align_down(idx_t bit) {\n+    return bit >> LogBitsPerWord;\n+  }\n+\n+  \/\/ Word-aligns bit and converts it to a word offset.\n+  \/\/ precondition: bit <= size()\n+  idx_t to_words_align_up(idx_t bit) const {\n+    verify_limit(bit);\n+    return raw_to_words_align_up(bit);\n+  }\n+\n+  \/\/ Word-aligns bit and converts it to a word offset.\n+  \/\/ precondition: bit <= size()\n+  inline idx_t to_words_align_down(idx_t bit) const {\n+    verify_limit(bit);\n+    return raw_to_words_align_down(bit);\n+  }\n+\n+  \/\/ Helper for get_next_{zero,one}_bit variants.\n+  \/\/ - flip designates whether searching for 1s or 0s.  Must be one of\n+  \/\/   find_{zeros,ones}_flip.\n+  \/\/ - aligned_right is true if r_index is a priori on a bm_word_t boundary.\n+  template<bm_word_t flip, bool aligned_right>\n+  inline idx_t get_next_bit_impl(idx_t l_index, idx_t r_index) const;\n+\n+  inline idx_t get_next_one_offset (idx_t l_index, idx_t r_index) const;\n+\n+  void clear_large_range (idx_t beg, idx_t end);\n+\n+  \/\/ Verify bit is less than size().\n+  void verify_index(idx_t bit) const NOT_DEBUG_RETURN;\n+  \/\/ Verify bit is not greater than size().\n+  void verify_limit(idx_t bit) const NOT_DEBUG_RETURN;\n+  \/\/ Verify [beg,end) is a valid range, e.g. beg <= end <= size().\n+  void verify_range(idx_t beg, idx_t end) const NOT_DEBUG_RETURN;\n+\n+public:\n+  static size_t compute_size(size_t heap_size);\n+  \/\/ Returns the amount of bytes on the heap between two marks in the bitmap.\n+  static size_t mark_distance();\n+  \/\/ Returns how many bytes (or bits) of the heap a single byte (or bit) of the\n+  \/\/ mark bitmap corresponds to. This is the same as the mark distance above.\n+  static size_t heap_map_factor() {\n+    return mark_distance();\n+  }\n+\n+  ShenandoahMarkBitMap(MemRegion heap, MemRegion storage);\n+\n+  \/\/ Mark word as 'strong' if it hasn't been marked strong yet.\n+  \/\/ Return true if the word has been marked strong, false if it has already been\n+  \/\/ marked strong or if another thread has beat us by marking it\n+  \/\/ strong.\n+  \/\/ Words that have been marked final before or by a concurrent thread will be\n+  \/\/ upgraded to strong. In this case, this method also returns true.\n+  inline bool mark_strong(HeapWord* w);\n+\n+  \/\/ Mark word as 'final' if it hasn't been marked final or strong yet.\n+  \/\/ Return true if the word has been marked final, false if it has already been\n+  \/\/ marked strong or final or if another thread has beat us by marking it\n+  \/\/ strong or final.\n+  inline bool mark_final(HeapWord* w);\n+\n+  inline bool is_marked_strong(HeapWord* w)  const;\n+  inline bool is_marked_final(HeapWord* w) const;\n+  inline bool is_marked_strong_and_final(HeapWord* w) const;\n+  inline bool is_marked_strong_or_final(HeapWord* w) const;\n+\n+  inline bool par_is_marked_strong(HeapWord* w)  const;\n+  inline bool par_is_marked_final(HeapWord* w) const;\n+  inline bool par_is_marked_strong_and_final(HeapWord* w) const;\n+  inline bool par_is_marked_strong_or_final(HeapWord* w) const;\n+\n+  static bool is_marked_strong(uintptr_t mark) {\n+    return (mark & 1) != 0;\n+  }\n+  static bool is_marked_final(uintptr_t mark) {\n+    return (mark & 2) != 0;\n+  }\n+  static bool is_marked_strong_and_final(uintptr_t mark) {\n+    return (mark & 3) == 3;\n+  }\n+  inline uintptr_t par_marking_bits(HeapWord* w) const;\n+\n+  \/\/ Return the address corresponding to the next marked bit at or after\n+  \/\/ \"addr\", and before \"limit\", if \"limit\" is non-NULL.  If there is no\n+  \/\/ such bit, returns \"limit\" if that is non-NULL, or else \"endWord()\".\n+  HeapWord* get_next_marked_addr(const HeapWord* addr,\n+                                 const HeapWord* limit) const;\n+\n+  bm_word_t inverted_bit_mask_for_range(idx_t beg, idx_t end) const;\n+  void  clear_range_within_word    (idx_t beg, idx_t end);\n+  void clear_range (idx_t beg, idx_t end);\n+  void clear_range_large(MemRegion mr);\n+\n+  void clear_range_of_words(idx_t beg, idx_t end);\n+  void clear_large_range_of_words(idx_t beg, idx_t end);\n+  static void clear_range_of_words(bm_word_t* map, idx_t beg, idx_t end);\n+\n+};\n+\n+#endif \/\/ SHARE_VM_GC_SHENANDOAH_SHENANDOAHMARKBITMAP_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMarkBitMap.hpp","additions":196,"deletions":0,"binary":false,"changes":196,"status":"added"},{"patch":"@@ -0,0 +1,267 @@\n+\/*\n+ * Copyright (c) 2020, Red Hat, Inc. and\/or its affiliates.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_VM_GC_SHENANDOAH_SHENANDOAHMARKBITMAP_INLINE_HPP\n+#define SHARE_VM_GC_SHENANDOAH_SHENANDOAHMARKBITMAP_INLINE_HPP\n+\n+#include \"gc\/shenandoah\/shenandoahMarkBitMap.hpp\"\n+#include \"runtime\/atomic.hpp\"\n+#include \"utilities\/count_trailing_zeros.hpp\"\n+\n+inline size_t ShenandoahMarkBitMap::address_to_index(const HeapWord* addr) const {\n+  return (pointer_delta(addr, _covered.start()) << 1) >> _shift;\n+}\n+\n+inline HeapWord* ShenandoahMarkBitMap::index_to_address(size_t offset) const {\n+  return _covered.start() + ((offset >> 1) << _shift);\n+}\n+\n+inline bool ShenandoahMarkBitMap::mark_strong(HeapWord* heap_addr) {\n+  check_mark(heap_addr);\n+\n+  idx_t bit = address_to_index(heap_addr);\n+  verify_index(bit);\n+  volatile bm_word_t* const addr = word_addr(bit);\n+  const bm_word_t mask = bit_mask(bit);\n+  bm_word_t old_val = load_word_ordered(addr, memory_order_conservative);\n+\n+  do {\n+    const bm_word_t new_val = old_val | mask;\n+    if (new_val == old_val) {\n+      return false;     \/\/ Someone else beat us to it.\n+    }\n+    const bm_word_t cur_val = Atomic::cmpxchg(addr, old_val, new_val, memory_order_conservative);\n+    if (cur_val == old_val) {\n+      return true;      \/\/ Success.\n+    }\n+    old_val = cur_val;  \/\/ The value changed, try again.\n+  } while (true);\n+}\n+\n+inline bool ShenandoahMarkBitMap::mark_final(HeapWord* heap_addr) {\n+  check_mark(heap_addr);\n+\n+  idx_t bit = address_to_index(heap_addr);\n+  verify_index(bit);\n+  volatile bm_word_t* const addr = word_addr(bit);\n+  const bm_word_t mask_final = (bm_word_t)1 << (bit_in_word(bit) + 1);\n+  const bm_word_t mask_strong = (bm_word_t)1 << bit_in_word(bit);\n+  bm_word_t old_val = load_word_ordered(addr, memory_order_conservative);\n+\n+  do {\n+    if ((old_val & mask_strong) != 0) {\n+      return false; \/\/ Already marked strong,\n+    }\n+    const bm_word_t new_val = old_val | mask_final;\n+    if (new_val == old_val) {\n+      return false;     \/\/ Someone else beat us to it.\n+    }\n+    const bm_word_t cur_val = Atomic::cmpxchg(addr, old_val, new_val, memory_order_conservative);\n+    if (cur_val == old_val) {\n+      return true;      \/\/ Success.\n+    }\n+    old_val = cur_val;  \/\/ The value changed, try again.\n+  } while (true);\n+}\n+\n+inline bool ShenandoahMarkBitMap::is_marked_strong(HeapWord* addr)  const {\n+  check_mark(addr);\n+  return at(address_to_index(addr));\n+}\n+\n+inline bool ShenandoahMarkBitMap::is_marked_final(HeapWord* addr) const {\n+  check_mark(addr);\n+  return at(address_to_index(addr) + 1);\n+}\n+\n+inline bool ShenandoahMarkBitMap::is_marked_strong_and_final(HeapWord* addr) const {\n+  check_mark(addr);\n+  idx_t index = address_to_index(addr);\n+  verify_index(index);\n+  bm_word_t mask = (bm_word_t)3 << bit_in_word(index);\n+  return (*word_addr(index) & mask) == mask;\n+}\n+\n+inline bool ShenandoahMarkBitMap::is_marked_strong_or_final(HeapWord* addr) const {\n+  check_mark(addr);\n+  idx_t index = address_to_index(addr);\n+  verify_index(index);\n+  bm_word_t mask = (bm_word_t)3 << bit_in_word(index);\n+  return (*word_addr(index) & mask) != 0;\n+}\n+\n+inline bool ShenandoahMarkBitMap::par_is_marked_strong(HeapWord* heap_addr)  const {\n+  check_mark(heap_addr);\n+  idx_t index = address_to_index(heap_addr);\n+  const volatile bm_word_t* const addr = word_addr(index);\n+  bm_word_t word = load_word_ordered(addr, memory_order_acquire);\n+  return (word & bit_mask(index)) != 0;\n+}\n+\n+inline bool ShenandoahMarkBitMap::par_is_marked_final(HeapWord* heap_addr) const {\n+  check_mark(heap_addr);\n+  idx_t index = address_to_index(heap_addr) + 1;\n+  const volatile bm_word_t* const addr = word_addr(index);\n+  bm_word_t word = load_word_ordered(addr, memory_order_acquire);\n+  return (word & bit_mask(index)) != 0;\n+}\n+\n+inline bool ShenandoahMarkBitMap::par_is_marked_strong_and_final(HeapWord* heap_addr) const {\n+  check_mark(heap_addr);\n+  idx_t index = address_to_index(heap_addr);\n+  verify_index(index);\n+  bm_word_t mask = (bm_word_t)3 << bit_in_word(index);\n+  const volatile bm_word_t* const addr = word_addr(index);\n+  bm_word_t word = load_word_ordered(addr, memory_order_acquire);\n+  return (word & mask) == mask;\n+}\n+\n+inline bool ShenandoahMarkBitMap::par_is_marked_strong_or_final(HeapWord* heap_addr) const {\n+  check_mark(heap_addr);\n+  idx_t index = address_to_index(heap_addr);\n+  verify_index(index);\n+  bm_word_t mask = (bm_word_t)3 << bit_in_word(index);\n+  const volatile bm_word_t* const addr = word_addr(index);\n+  bm_word_t word = load_word_ordered(addr, memory_order_acquire);\n+  return (word & mask) != 0;\n+}\n+\n+inline uintptr_t ShenandoahMarkBitMap::par_marking_bits(HeapWord* heap_addr) const {\n+  check_mark(heap_addr);\n+  idx_t index = address_to_index(heap_addr);\n+  verify_index(index);\n+  const volatile bm_word_t* const addr = word_addr(index);\n+  bm_word_t word = load_word_ordered(addr, memory_order_acquire);\n+  return word >> bit_in_word(index);\n+}\n+\n+inline const ShenandoahMarkBitMap::bm_word_t ShenandoahMarkBitMap::load_word_ordered(const volatile bm_word_t* const addr, atomic_memory_order memory_order) {\n+  if (memory_order == memory_order_relaxed || memory_order == memory_order_release) {\n+    return Atomic::load(addr);\n+  } else {\n+    assert(memory_order == memory_order_acq_rel ||\n+           memory_order == memory_order_acquire ||\n+           memory_order == memory_order_conservative,\n+           \"unexpected memory ordering\");\n+    return Atomic::load_acquire(addr);\n+  }\n+}\n+\n+template<ShenandoahMarkBitMap::bm_word_t flip, bool aligned_right>\n+inline ShenandoahMarkBitMap::idx_t ShenandoahMarkBitMap::get_next_bit_impl(idx_t l_index, idx_t r_index) const {\n+  STATIC_ASSERT(flip == find_ones_flip || flip == find_zeros_flip);\n+  verify_range(l_index, r_index);\n+  assert(!aligned_right || is_aligned(r_index, BitsPerWord), \"r_index not aligned\");\n+\n+  \/\/ The first word often contains an interesting bit, either due to\n+  \/\/ density or because of features of the calling algorithm.  So it's\n+  \/\/ important to examine that first word with a minimum of fuss,\n+  \/\/ minimizing setup time for later words that will be wasted if the\n+  \/\/ first word is indeed interesting.\n+\n+  \/\/ The benefit from aligned_right being true is relatively small.\n+  \/\/ It saves an operation in the setup for the word search loop.\n+  \/\/ It also eliminates the range check on the final result.\n+  \/\/ However, callers often have a comparison with r_index, and\n+  \/\/ inlining often allows the two comparisons to be combined; it is\n+  \/\/ important when !aligned_right that return paths either return\n+  \/\/ r_index or a value dominated by a comparison with r_index.\n+  \/\/ aligned_right is still helpful when the caller doesn't have a\n+  \/\/ range check because features of the calling algorithm guarantee\n+  \/\/ an interesting bit will be present.\n+\n+  if (l_index < r_index) {\n+    \/\/ Get the word containing l_index, and shift out low bits.\n+    idx_t index = to_words_align_down(l_index);\n+    bm_word_t cword = (map(index) ^ flip) >> bit_in_word(l_index);\n+    if ((cword & 1) != 0) {\n+      \/\/ The first bit is similarly often interesting. When it matters\n+      \/\/ (density or features of the calling algorithm make it likely\n+      \/\/ the first bit is set), going straight to the next clause compares\n+      \/\/ poorly with doing this check first; count_trailing_zeros can be\n+      \/\/ relatively expensive, plus there is the additional range check.\n+      \/\/ But when the first bit isn't set, the cost of having tested for\n+      \/\/ it is relatively small compared to the rest of the search.\n+      return l_index;\n+    } else if (cword != 0) {\n+      \/\/ Flipped and shifted first word is non-zero.\n+      idx_t result = l_index + count_trailing_zeros(cword);\n+      if (aligned_right || (result < r_index)) return result;\n+      \/\/ Result is beyond range bound; return r_index.\n+    } else {\n+      \/\/ Flipped and shifted first word is zero.  Word search through\n+      \/\/ aligned up r_index for a non-zero flipped word.\n+      idx_t limit = aligned_right\n+                    ? to_words_align_down(r_index) \/\/ Miniscule savings when aligned.\n+                    : to_words_align_up(r_index);\n+      while (++index < limit) {\n+        cword = map(index) ^ flip;\n+        if (cword != 0) {\n+          idx_t result = bit_index(index) + count_trailing_zeros(cword);\n+          if (aligned_right || (result < r_index)) return result;\n+          \/\/ Result is beyond range bound; return r_index.\n+          assert((index + 1) == limit, \"invariant\");\n+          break;\n+        }\n+      }\n+      \/\/ No bits in range; return r_index.\n+    }\n+  }\n+  return r_index;\n+}\n+\n+inline ShenandoahMarkBitMap::idx_t ShenandoahMarkBitMap::get_next_one_offset(idx_t l_offset, idx_t r_offset) const {\n+  return get_next_bit_impl<find_ones_flip, false>(l_offset, r_offset);\n+}\n+\n+\/\/ Returns a bit mask for a range of bits [beg, end) within a single word.  Each\n+\/\/ bit in the mask is 0 if the bit is in the range, 1 if not in the range.  The\n+\/\/ returned mask can be used directly to clear the range, or inverted to set the\n+\/\/ range.  Note:  end must not be 0.\n+inline ShenandoahMarkBitMap::bm_word_t\n+ShenandoahMarkBitMap::inverted_bit_mask_for_range(idx_t beg, idx_t end) const {\n+  assert(end != 0, \"does not work when end == 0\");\n+  assert(beg == end || to_words_align_down(beg) == to_words_align_down(end - 1),\n+         \"must be a single-word range\");\n+  bm_word_t mask = bit_mask(beg) - 1;   \/\/ low (right) bits\n+  if (bit_in_word(end) != 0) {\n+    mask |= ~(bit_mask(end) - 1);       \/\/ high (left) bits\n+  }\n+  return mask;\n+}\n+\n+inline void ShenandoahMarkBitMap::clear_range_of_words(bm_word_t* map, idx_t beg, idx_t end) {\n+  for (idx_t i = beg; i < end; ++i) map[i] = 0;\n+}\n+\n+inline void ShenandoahMarkBitMap::clear_large_range_of_words(idx_t beg, idx_t end) {\n+  assert(beg <= end, \"underflow\");\n+  memset(_map + beg, 0, (end - beg) * sizeof(bm_word_t));\n+}\n+\n+inline void ShenandoahMarkBitMap::clear_range_of_words(idx_t beg, idx_t end) {\n+  clear_range_of_words(_map, beg, end);\n+}\n+\n+\n+#endif \/\/ SHARE_VM_GC_SHENANDOAH_SHENANDOAHMARKBITMAP_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMarkBitMap.inline.hpp","additions":267,"deletions":0,"binary":false,"changes":267,"status":"added"},{"patch":"@@ -41,0 +41,1 @@\n+#include \"gc\/shenandoah\/shenandoahReferenceProcessor.hpp\"\n@@ -132,1 +133,1 @@\n-    ReferenceProcessor* rp = heap->ref_processor();\n+    ShenandoahReferenceProcessor* rp = heap->ref_processor();\n@@ -244,1 +245,0 @@\n-  heap->set_process_references(heap->heuristics()->can_process_references());\n@@ -247,1 +247,1 @@\n-  ReferenceProcessor* rp = heap->ref_processor();\n+  ShenandoahReferenceProcessor* rp = heap->ref_processor();\n@@ -250,1 +250,1 @@\n-  rp->setup_policy(true); \/\/ forcefully purge all soft references\n+  rp->set_soft_reference_policy(true); \/\/ forcefully purge all soft references\n@@ -256,0 +256,1 @@\n+  rp->process_references(heap->workers());\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMarkCompact.cpp","additions":5,"deletions":4,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+  _mark_bit_map(heap_region, bitmap_region),\n@@ -36,1 +37,0 @@\n-  _mark_bit_map.initialize(heap_region, bitmap_region);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMarkingContext.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -28,1 +28,2 @@\n-#include \"gc\/shared\/markBitMap.hpp\"\n+#include \"gc\/shenandoah\/shenandoahMarkBitMap.hpp\"\n+#include \"gc\/shenandoah\/shenandoahSharedVariables.hpp\"\n@@ -38,1 +39,1 @@\n-  MarkBitMap _mark_bit_map;\n+  ShenandoahMarkBitMap _mark_bit_map;\n@@ -54,3 +55,15 @@\n-  inline bool mark(oop obj);\n-\n-  inline bool is_marked(oop obj) const;\n+  inline bool mark_strong(oop obj);\n+  inline bool mark_final(oop obj);\n+\n+  \/\/ Simple versions of marking accessors, to be used outside of marking (e.g. no possible concurrent updates)\n+  inline bool is_marked(oop) const;\n+  inline bool is_marked_strong(oop obj) const;\n+  inline bool is_marked_final(oop obj) const;\n+  inline bool is_marked_strong_and_final(oop obj) const;\n+\n+  \/\/ Paralles versions of marking accessors, to be used during marking (e.g. possible concurrent updates)\n+  inline bool par_is_marked(oop) const;\n+  inline bool par_is_marked_strong(oop obj) const;\n+  inline bool par_is_marked_final(oop obj) const;\n+  inline bool par_is_marked_strong_and_final(oop obj) const;\n+  inline uintptr_t par_marking_bits(oop obj) const;\n@@ -61,1 +74,1 @@\n-  inline MarkBitMap* mark_bit_map();\n+  inline const ShenandoahMarkBitMap* mark_bit_map();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMarkingContext.hpp","additions":19,"deletions":6,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"gc\/shenandoah\/shenandoahMarkBitMap.inline.hpp\"\n@@ -30,1 +31,1 @@\n-inline MarkBitMap* ShenandoahMarkingContext::mark_bit_map() {\n+inline const ShenandoahMarkBitMap* ShenandoahMarkingContext::mark_bit_map() {\n@@ -34,1 +35,1 @@\n-inline bool ShenandoahMarkingContext::mark(oop obj) {\n+inline bool ShenandoahMarkingContext::mark_strong(oop obj) {\n@@ -36,1 +37,6 @@\n-  return (! allocated_after_mark_start(obj)) && _mark_bit_map.par_mark(obj);\n+  return (! allocated_after_mark_start(obj)) && _mark_bit_map.mark_strong(cast_from_oop<HeapWord*>(obj));\n+}\n+\n+inline bool ShenandoahMarkingContext::mark_final(oop obj) {\n+  shenandoah_assert_not_forwarded(NULL, obj);\n+  return (! allocated_after_mark_start(obj)) && _mark_bit_map.mark_final(cast_from_oop<HeapWord*>(obj));\n@@ -40,1 +46,34 @@\n-  return allocated_after_mark_start(obj) || _mark_bit_map.is_marked(obj);\n+  return allocated_after_mark_start(obj) || _mark_bit_map.is_marked_strong_or_final(cast_from_oop<HeapWord*>(obj));\n+}\n+\n+inline bool ShenandoahMarkingContext::is_marked_strong(oop obj) const {\n+  return allocated_after_mark_start(obj) || _mark_bit_map.is_marked_strong(cast_from_oop<HeapWord*>(obj));\n+}\n+\n+inline bool ShenandoahMarkingContext::is_marked_final(oop obj) const {\n+  return allocated_after_mark_start(obj) || _mark_bit_map.is_marked_final(cast_from_oop<HeapWord*>(obj));\n+}\n+\n+inline bool ShenandoahMarkingContext::is_marked_strong_and_final(oop obj) const {\n+  return allocated_after_mark_start(obj) || _mark_bit_map.is_marked_strong_and_final(cast_from_oop<HeapWord*>(obj));\n+}\n+\n+inline bool ShenandoahMarkingContext::par_is_marked(oop obj) const {\n+  return allocated_after_mark_start(obj) || _mark_bit_map.par_is_marked_strong_or_final(cast_from_oop<HeapWord*>(obj));\n+}\n+\n+inline bool ShenandoahMarkingContext::par_is_marked_strong(oop obj) const {\n+  return allocated_after_mark_start(obj) || _mark_bit_map.par_is_marked_strong(cast_from_oop<HeapWord*>(obj));\n+}\n+\n+inline bool ShenandoahMarkingContext::par_is_marked_final(oop obj) const {\n+  return allocated_after_mark_start(obj) || _mark_bit_map.par_is_marked_final(cast_from_oop<HeapWord*>(obj));\n+}\n+\n+inline bool ShenandoahMarkingContext::par_is_marked_strong_and_final(oop obj) const {\n+  return allocated_after_mark_start(obj) || _mark_bit_map.par_is_marked_strong_and_final(cast_from_oop<HeapWord*>(obj));\n+}\n+\n+inline uintptr_t ShenandoahMarkingContext::par_marking_bits(oop obj) const {\n+  assert(!allocated_after_mark_start(obj), \"only on objects < TAMS\");\n+  return _mark_bit_map.par_marking_bits(cast_from_oop<HeapWord*>(obj));\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMarkingContext.inline.hpp","additions":43,"deletions":4,"binary":false,"changes":47,"status":"modified"},{"patch":"@@ -52,0 +52,1 @@\n+  bool _strong;\n@@ -58,1 +59,9 @@\n-  ShenandoahMarkRefsSuperClosure(ShenandoahObjToScanQueue* q, ReferenceProcessor* rp);\n+  ShenandoahMarkRefsSuperClosure(ShenandoahObjToScanQueue* q, ShenandoahReferenceProcessor* rp);\n+\n+  bool is_strong() const {\n+    return _strong;\n+  }\n+\n+  void set_strong(bool strong) {\n+    _strong = strong;\n+  }\n@@ -67,1 +76,1 @@\n-  ShenandoahMarkUpdateRefsClosure(ShenandoahObjToScanQueue* q, ReferenceProcessor* rp) :\n+  ShenandoahMarkUpdateRefsClosure(ShenandoahObjToScanQueue* q, ShenandoahReferenceProcessor* rp) :\n@@ -81,1 +90,1 @@\n-  ShenandoahMarkUpdateRefsDedupClosure(ShenandoahObjToScanQueue* q, ReferenceProcessor* rp) :\n+  ShenandoahMarkUpdateRefsDedupClosure(ShenandoahObjToScanQueue* q, ShenandoahReferenceProcessor* rp) :\n@@ -95,1 +104,1 @@\n-  ShenandoahMarkUpdateRefsMetadataClosure(ShenandoahObjToScanQueue* q, ReferenceProcessor* rp) :\n+  ShenandoahMarkUpdateRefsMetadataClosure(ShenandoahObjToScanQueue* q, ShenandoahReferenceProcessor* rp) :\n@@ -109,1 +118,1 @@\n-  ShenandoahMarkUpdateRefsMetadataDedupClosure(ShenandoahObjToScanQueue* q, ReferenceProcessor* rp) :\n+  ShenandoahMarkUpdateRefsMetadataDedupClosure(ShenandoahObjToScanQueue* q, ShenandoahReferenceProcessor* rp) :\n@@ -123,1 +132,1 @@\n-  ShenandoahMarkRefsClosure(ShenandoahObjToScanQueue* q, ReferenceProcessor* rp) :\n+  ShenandoahMarkRefsClosure(ShenandoahObjToScanQueue* q, ShenandoahReferenceProcessor* rp) :\n@@ -137,1 +146,1 @@\n-  ShenandoahMarkRefsDedupClosure(ShenandoahObjToScanQueue* q, ReferenceProcessor* rp) :\n+  ShenandoahMarkRefsDedupClosure(ShenandoahObjToScanQueue* q, ShenandoahReferenceProcessor* rp) :\n@@ -151,1 +160,1 @@\n-  ShenandoahMarkResolveRefsClosure(ShenandoahObjToScanQueue* q, ReferenceProcessor* rp) :\n+  ShenandoahMarkResolveRefsClosure(ShenandoahObjToScanQueue* q, ShenandoahReferenceProcessor* rp) :\n@@ -165,1 +174,1 @@\n-  ShenandoahMarkRefsMetadataClosure(ShenandoahObjToScanQueue* q, ReferenceProcessor* rp) :\n+  ShenandoahMarkRefsMetadataClosure(ShenandoahObjToScanQueue* q, ShenandoahReferenceProcessor* rp) :\n@@ -179,1 +188,1 @@\n-  ShenandoahMarkRefsMetadataDedupClosure(ShenandoahObjToScanQueue* q, ReferenceProcessor* rp) :\n+  ShenandoahMarkRefsMetadataDedupClosure(ShenandoahObjToScanQueue* q, ShenandoahReferenceProcessor* rp) :\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahOopClosures.hpp","additions":19,"deletions":10,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -33,1 +33,1 @@\n-  ShenandoahConcurrentMark::mark_through_ref<T, UPDATE_REFS, STRING_DEDUP>(p, _heap, _queue, _mark_context);\n+  ShenandoahConcurrentMark::mark_through_ref<T, UPDATE_REFS, STRING_DEDUP>(p, _heap, _queue, _mark_context, _strong);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahOopClosures.inline.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -116,0 +116,1 @@\n+    case conc_weak_refs_work:\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahPhaseTimings.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -85,0 +85,3 @@\n+  f(conc_weak_refs,                                 \"Concurrent Weak References\")      \\\n+  f(conc_weak_refs_work,                            \"  Process\")                       \\\n+  SHENANDOAH_PAR_PHASE_DO(conc_weak_refs_work_,     \"    CWR: \", f)                    \\\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahPhaseTimings.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -0,0 +1,497 @@\n+\/*\n+ * Copyright (c) 2015, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, Red Hat, Inc. and\/or its affiliates.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"classfile\/javaClasses.hpp\"\n+#include \"gc\/shenandoah\/shenandoahOopClosures.hpp\"\n+#include \"gc\/shenandoah\/shenandoahReferenceProcessor.hpp\"\n+#include \"gc\/shenandoah\/shenandoahThreadLocalData.hpp\"\n+#include \"gc\/shenandoah\/shenandoahUtils.hpp\"\n+#include \"runtime\/atomic.hpp\"\n+#include \"logging\/log.hpp\"\n+\n+static ReferenceType reference_type(oop reference) {\n+  return InstanceKlass::cast(reference->klass())->reference_type();\n+}\n+\n+static const char* reference_type_name(ReferenceType type) {\n+  switch (type) {\n+    case REF_SOFT:\n+      return \"Soft\";\n+\n+    case REF_WEAK:\n+      return \"Weak\";\n+\n+    case REF_FINAL:\n+      return \"Final\";\n+\n+    case REF_PHANTOM:\n+      return \"Phantom\";\n+\n+    default:\n+      ShouldNotReachHere();\n+      return NULL;\n+  }\n+}\n+\n+template <typename T>\n+static void set_oop_field(T* field, oop value);\n+\n+template <>\n+void set_oop_field<oop>(oop* field, oop value) {\n+  *field = value;\n+}\n+\n+template <>\n+void set_oop_field<narrowOop>(narrowOop* field, oop value) {\n+  *field = CompressedOops::encode(value);\n+}\n+\n+static oop lrb(oop obj) {\n+  if (obj != NULL && ShenandoahHeap::heap()->marking_context()->is_marked(obj)) {\n+    return ShenandoahBarrierSet::barrier_set()->load_reference_barrier_not_null(obj);\n+  } else {\n+    return obj;\n+  }\n+}\n+\n+template <typename T>\n+static volatile T* reference_referent_addr(oop reference) {\n+  return (volatile T*)java_lang_ref_Reference::referent_addr_raw(reference);\n+}\n+\n+template <typename T>\n+static oop reference_referent(oop reference) {\n+  T heap_oop = Atomic::load(reference_referent_addr<T>(reference));\n+  return lrb(CompressedOops::decode(heap_oop));\n+}\n+\n+static void reference_set_referent(oop reference, oop referent) {\n+  java_lang_ref_Reference::set_referent_raw(reference, referent);\n+}\n+\n+template <typename T>\n+static T* reference_discovered_addr(oop reference) {\n+  return reinterpret_cast<T*>(java_lang_ref_Reference::discovered_addr_raw(reference));\n+}\n+\n+template <typename T>\n+static oop reference_discovered(oop reference) {\n+  T heap_oop = *reference_discovered_addr<T>(reference);\n+  return lrb(CompressedOops::decode(heap_oop));\n+}\n+\n+template <typename T>\n+static void reference_set_discovered(oop reference, oop discovered);\n+\n+template <>\n+void reference_set_discovered<oop>(oop reference, oop discovered) {\n+  *reference_discovered_addr<oop>(reference) = discovered;\n+}\n+\n+template <>\n+void reference_set_discovered<narrowOop>(oop reference, oop discovered) {\n+  *reference_discovered_addr<narrowOop>(reference) = CompressedOops::encode(discovered);\n+}\n+\n+template <typename T>\n+static T* reference_next_addr(oop reference) {\n+  return reinterpret_cast<T*>(java_lang_ref_Reference::next_addr_raw(reference));\n+}\n+\n+template <typename T>\n+static oop reference_next(oop reference) {\n+  T heap_oop = RawAccess<>::oop_load(reference_next_addr<T>(reference));\n+  return lrb(CompressedOops::decode(heap_oop));\n+}\n+\n+static void reference_set_next(oop reference, oop next) {\n+  java_lang_ref_Reference::set_next_raw(reference, next);\n+}\n+\n+static void soft_reference_update_clock() {\n+  const jlong now = os::javaTimeNanos() \/ NANOSECS_PER_MILLISEC;\n+  java_lang_ref_SoftReference::set_clock(now);\n+}\n+\n+ShenandoahRefProcThreadLocal::ShenandoahRefProcThreadLocal() :\n+  _discovered_list(NULL) {\n+\n+}\n+\n+void ShenandoahRefProcThreadLocal::reset() {\n+  _discovered_list = NULL;\n+}\n+\n+template <typename T>\n+T* ShenandoahRefProcThreadLocal::discovered_list_addr() {\n+  return reinterpret_cast<T*>(&_discovered_list);\n+}\n+\n+template <>\n+oop ShenandoahRefProcThreadLocal::discovered_list_head<oop>() const {\n+  return *reinterpret_cast<const oop*>(&_discovered_list);\n+}\n+\n+template <>\n+oop ShenandoahRefProcThreadLocal::discovered_list_head<narrowOop>() const {\n+  return CompressedOops::decode(*reinterpret_cast<const narrowOop*>(&_discovered_list));\n+}\n+\n+template <>\n+void ShenandoahRefProcThreadLocal::set_discovered_list_head<narrowOop>(oop head) {\n+  *discovered_list_addr<narrowOop>() = CompressedOops::encode(head);\n+}\n+\n+template <>\n+void ShenandoahRefProcThreadLocal::set_discovered_list_head<oop>(oop head) {\n+  *discovered_list_addr<oop>() = head;\n+}\n+\n+ShenandoahReferenceProcessor::ShenandoahReferenceProcessor(uint max_workers) :\n+  _soft_reference_policy(NULL),\n+  _ref_proc_thread_locals(NEW_C_HEAP_ARRAY(ShenandoahRefProcThreadLocal, max_workers, mtGC)),\n+  _pending_list(NULL),\n+  _pending_list_tail(&_pending_list),\n+  _iterate_discovered_list_id(0U) {\n+  for (size_t i = 0; i < max_workers; i++) {\n+    _ref_proc_thread_locals[i].reset();\n+  }\n+}\n+\n+void ShenandoahReferenceProcessor::reset_thread_locals() {\n+  uint max_workers = ShenandoahHeap::heap()->max_workers();\n+  for (uint i = 0; i < max_workers; i++) {\n+    _ref_proc_thread_locals[i].reset();\n+  }\n+}\n+\n+void ShenandoahReferenceProcessor::set_soft_reference_policy(bool clear) {\n+  static AlwaysClearPolicy always_clear_policy;\n+  static LRUMaxHeapPolicy lru_max_heap_policy;\n+\n+  if (clear) {\n+    log_info(gc, ref)(\"Clearing All SoftReferences\");\n+    _soft_reference_policy = &always_clear_policy;\n+  } else {\n+    _soft_reference_policy = &lru_max_heap_policy;\n+  }\n+\n+  _soft_reference_policy->setup();\n+}\n+\n+template <typename T>\n+bool ShenandoahReferenceProcessor::is_inactive(oop reference, oop referent, ReferenceType type) const {\n+  if (type == REF_FINAL) {\n+    \/\/ A FinalReference is inactive if its next field is non-null. An application can't\n+    \/\/ call enqueue() or clear() on a FinalReference.\n+    return reference_next<T>(reference) != NULL;\n+  } else {\n+    \/\/ A non-FinalReference is inactive if the referent is null. The referent can only\n+    \/\/ be null if the application called Reference.enqueue() or Reference.clear().\n+    return referent == NULL;\n+  }\n+}\n+\n+bool ShenandoahReferenceProcessor::is_strongly_live(oop referent) const {\n+  return ShenandoahHeap::heap()->marking_context()->is_marked_strong(referent);\n+}\n+\n+bool ShenandoahReferenceProcessor::is_softly_live(oop reference, ReferenceType type) const {\n+  if (type != REF_SOFT) {\n+    \/\/ Not a SoftReference\n+    return false;\n+  }\n+\n+  \/\/ Ask SoftReference policy\n+  const jlong clock = java_lang_ref_SoftReference::clock();\n+  assert(clock != 0, \"Clock not initialized\");\n+  assert(_soft_reference_policy != NULL, \"Policy not initialized\");\n+  return !_soft_reference_policy->should_clear_reference(reference, clock);\n+}\n+\n+template <typename T>\n+bool ShenandoahReferenceProcessor::should_discover(oop reference, ReferenceType type) const {\n+  if (reference_discovered<T>(reference) != NULL) {\n+    \/\/ Already discovered. This can happen if the reference is marked finalizable first, and then strong,\n+    \/\/ in which case it will be seen 2x by marking.\n+    log_trace(gc,ref)(\"Reference already discovered: \" PTR_FORMAT, p2i(reference));\n+    return false;\n+  }\n+  T* referent_addr = (T*) java_lang_ref_Reference::referent_addr_raw(reference);\n+  T heap_oop = RawAccess<>::oop_load(referent_addr);\n+  oop referent = CompressedOops::decode_not_null(heap_oop);\n+\n+  if (is_inactive<T>(reference, referent, type)) {\n+    log_trace(gc,ref)(\"Reference inactive: \" PTR_FORMAT, p2i(reference));\n+    return false;\n+  }\n+\n+  if (is_strongly_live(referent)) {\n+    log_trace(gc,ref)(\"Reference strongly live: \" PTR_FORMAT, p2i(reference));\n+    return false;\n+  }\n+\n+  if (is_softly_live(reference, type)) {\n+    log_trace(gc,ref)(\"Reference softly live: \" PTR_FORMAT, p2i(reference));\n+    return false;\n+  }\n+\n+  return true;\n+}\n+\n+template <typename T>\n+bool ShenandoahReferenceProcessor::should_drop(oop reference, ReferenceType type) const {\n+  const oop referent = reference_referent<T>(reference);\n+  if (referent == NULL) {\n+    \/\/ Reference has been cleared, by a call to Reference.enqueue()\n+    \/\/ or Reference.clear() from the application, which means we\n+    \/\/ should drop the reference.\n+    return true;\n+  }\n+\n+  \/\/ Check if the referent is still alive, in which case we should\n+  \/\/ drop the reference.\n+  if (type == REF_PHANTOM) {\n+    return ShenandoahHeap::heap()->complete_marking_context()->is_marked(referent);\n+  } else {\n+    return ShenandoahHeap::heap()->complete_marking_context()->is_marked_strong(referent);\n+  }\n+}\n+\n+template <typename T>\n+void ShenandoahReferenceProcessor::make_inactive(oop reference, ReferenceType type) const {\n+  if (type == REF_FINAL) {\n+    \/\/ Don't clear referent. It is needed by the Finalizer thread to make the call\n+    \/\/ to finalize(). A FinalReference is instead made inactive by self-looping the\n+    \/\/ next field. An application can't call FinalReference.enqueue(), so there is\n+    \/\/ no race to worry about when setting the next field.\n+    assert(reference_next<T>(reference) == NULL, \"Already inactive\");\n+    assert(ShenandoahHeap::heap()->marking_context()->is_marked(reference_referent<T>(reference)), \"only make inactive final refs with alive referents\");\n+    reference_set_next(reference, reference);\n+  } else {\n+    \/\/ Clear referent\n+    reference_set_referent(reference, NULL);\n+  }\n+}\n+\n+template <typename T>\n+bool ShenandoahReferenceProcessor::discover(oop reference, ReferenceType type) {\n+  if (!should_discover<T>(reference, type)) {\n+    \/\/ Not discovered\n+    return false;\n+  }\n+\n+  \/\/ Update statistics\n+  \/\/_discovered_count.get()[type]++;\n+\n+  if (type == REF_FINAL) {\n+    Thread* thread = Thread::current();\n+    ShenandoahMarkRefsSuperClosure* cl = ShenandoahThreadLocalData::mark_closure(thread);\n+    bool strong = cl->is_strong();\n+    cl->set_strong(false);\n+    if (UseCompressedOops) {\n+      cl->do_oop(reinterpret_cast<narrowOop*>(java_lang_ref_Reference::referent_addr_raw(reference)));\n+    } else {\n+      cl->do_oop(reinterpret_cast<oop*>(java_lang_ref_Reference::referent_addr_raw(reference)));\n+    }\n+    cl->set_strong(strong);\n+  }\n+\n+  \/\/ Add reference to discovered list\n+  assert(reference_discovered<T>(reference) == NULL, \"Already discovered: \" PTR_FORMAT, p2i(reference));\n+  uint worker_id = ShenandoahThreadLocalData::worker_id(Thread::current());\n+  assert(worker_id != ShenandoahThreadLocalData::INVALID_WORKER_ID, \"need valid worker ID\");\n+  ShenandoahRefProcThreadLocal& refproc_data = _ref_proc_thread_locals[worker_id];\n+  oop discovered_head = refproc_data.discovered_list_head<T>();\n+  reference_set_discovered<T>(reference, discovered_head);\n+  refproc_data.set_discovered_list_head<T>(reference);\n+  assert(refproc_data.discovered_list_head<T>() == reference, \"reference must be new discovered head\");\n+\n+  log_trace(gc, ref)(\"Discovered Reference: \" PTR_FORMAT \" (%s)\", p2i(reference), reference_type_name(type));\n+\n+  return true;\n+}\n+\n+bool ShenandoahReferenceProcessor::discover_reference(oop reference, ReferenceType type) {\n+  if (!RegisterReferences) {\n+    \/\/ Reference processing disabled\n+    return false;\n+  }\n+\n+  log_trace(gc, ref)(\"Encountered Reference: \" PTR_FORMAT \" (%s)\", p2i(reference), reference_type_name(type));\n+\n+  if (UseCompressedOops) {\n+    return discover<narrowOop>(reference, type);\n+  } else {\n+    return discover<oop>(reference, type);\n+  }\n+}\n+\n+template <typename T>\n+oop ShenandoahReferenceProcessor::drop(oop reference, ReferenceType type) {\n+  log_trace(gc, ref)(\"Dropped Reference: \" PTR_FORMAT \" (%s)\", p2i(reference), reference_type_name(type));\n+\n+  assert(reference_referent<T>(reference) == NULL ||\n+         ShenandoahHeap::heap()->marking_context()->is_marked(reference_referent<T>(reference)), \"only drop references with alive referents\");\n+\n+  \/\/ Unlink and return next in list\n+  oop next = reference_discovered<T>(reference);\n+  reference_set_discovered<T>(reference, NULL);\n+  return next;\n+}\n+\n+template <typename T>\n+T* ShenandoahReferenceProcessor::keep(oop reference, ReferenceType type) {\n+  log_trace(gc, ref)(\"Enqueued Reference: \" PTR_FORMAT \" (%s)\", p2i(reference), reference_type_name(type));\n+\n+  \/\/ Update statistics\n+  \/\/ TODO _enqueued_count.get()[type]++;\n+\n+  \/\/ Make reference inactive\n+  make_inactive<T>(reference, type);\n+\n+  \/\/ Return next in list\n+  return reference_discovered_addr<T>(reference);\n+}\n+\n+template <typename T>\n+void ShenandoahReferenceProcessor::process_references(ShenandoahRefProcThreadLocal& refproc_data, uint worker_id) {;\n+  log_trace(gc, ref)(\"Processing discovered list #%u : \" PTR_FORMAT, worker_id, p2i(refproc_data.discovered_list_head<T>()));\n+  T* list = refproc_data.discovered_list_addr<T>();\n+  \/\/ The list head is basically a GC root, we need to resolve and update it,\n+  \/\/ otherwise we will later swap a from-space ref into Universe::pending_list().\n+  if (!CompressedOops::is_null(*list)) {\n+    oop first_resolved = lrb(CompressedOops::decode_not_null(*list));\n+    set_oop_field(list, first_resolved);\n+  }\n+  T* p = list;\n+  while (!CompressedOops::is_null(*p)) {\n+    const oop reference = lrb(CompressedOops::decode(*p));\n+    log_trace(gc, ref)(\"Processing reference: \" PTR_FORMAT, p2i(reference));\n+    const ReferenceType type = reference_type(reference);\n+\n+    if (should_drop<T>(reference, type)) {\n+      set_oop_field(p, drop<T>(reference, type));\n+    } else {\n+      p = keep<T>(reference, type);\n+    }\n+  }\n+\n+  \/\/ Prepend discovered references to internal pending list\n+  if (!CompressedOops::is_null(*list)) {\n+    oop head = lrb(CompressedOops::decode_not_null(*list));\n+    shenandoah_assert_not_in_cset_except(&head, head, ShenandoahHeap::heap()->cancelled_gc() || !ShenandoahLoadRefBarrier);\n+    oop prev = Atomic::xchg(&_pending_list, head);\n+    RawAccess<>::oop_store(p, prev);\n+    if (prev == NULL) {\n+      \/\/ First to prepend to list, record tail\n+      _pending_list_tail = reinterpret_cast<void*>(p);\n+    }\n+\n+    \/\/ Clear discovered list\n+    set_oop_field(list, oop(NULL));\n+  }\n+}\n+\n+void ShenandoahReferenceProcessor::work() {\n+  \/\/ Process discovered references\n+  uint max_workers = ShenandoahHeap::heap()->max_workers();\n+  uint worker_id = Atomic::add(&_iterate_discovered_list_id, 1U) - 1;\n+  while (worker_id < max_workers) {\n+    if (UseCompressedOops) {\n+      process_references<narrowOop>(_ref_proc_thread_locals[worker_id], worker_id);\n+    } else {\n+      process_references<oop>(_ref_proc_thread_locals[worker_id], worker_id);\n+    }\n+    worker_id = Atomic::add(&_iterate_discovered_list_id, 1U) - 1;\n+  }\n+}\n+\n+class ShenandoahReferenceProcessorTask : public AbstractGangTask {\n+private:\n+  ShenandoahReferenceProcessor* const _reference_processor;\n+\n+public:\n+  ShenandoahReferenceProcessorTask(ShenandoahReferenceProcessor* reference_processor) :\n+    AbstractGangTask(\"ShenandoahReferenceProcessorTask\"),\n+    _reference_processor(reference_processor) {\n+  }\n+\n+  virtual void work(uint worker_id) {\n+    ShenandoahConcurrentWorkerSession worker_session(worker_id);\n+    _reference_processor->work();\n+  }\n+};\n+\n+void ShenandoahReferenceProcessor::process_references(WorkGang* workers) {\n+\n+  Atomic::release_store_fence(&_iterate_discovered_list_id, 0U);\n+\n+  \/\/ Process discovered lists\n+  ShenandoahReferenceProcessorTask task(this);\n+  workers->run_task(&task);\n+\n+  \/\/ Update SoftReference clock\n+  soft_reference_update_clock();\n+\n+  \/\/ Collect, log and trace statistics\n+  \/\/ collect_statistics();\n+\n+  enqueue_references();\n+}\n+\n+void ShenandoahReferenceProcessor::enqueue_references_locked() {\n+  \/\/ Prepend internal pending list to external pending list\n+  shenandoah_assert_not_in_cset_except(&_pending_list, _pending_list, ShenandoahHeap::heap()->cancelled_gc() || !ShenandoahLoadRefBarrier);\n+  if (UseCompressedOops) {\n+    *reinterpret_cast<narrowOop*>(_pending_list_tail) = CompressedOops::encode(Universe::swap_reference_pending_list(_pending_list));\n+  } else {\n+    *reinterpret_cast<oop*>(_pending_list_tail) = Universe::swap_reference_pending_list(_pending_list);\n+  }\n+}\n+\n+void ShenandoahReferenceProcessor::enqueue_references() {\n+  if (_pending_list == NULL) {\n+    \/\/ Nothing to enqueue\n+    return;\n+  }\n+\n+  if (ShenandoahSafepoint::is_at_shenandoah_safepoint()) {\n+    enqueue_references_locked();\n+  } else {\n+    \/\/ Heap_lock protects external pending list\n+    MonitorLocker ml(Heap_lock);\n+\n+    enqueue_references_locked();\n+\n+    \/\/ Notify ReferenceHandler thread\n+    ml.notify_all();\n+  }\n+\n+  \/\/ Reset internal pending list\n+  _pending_list = NULL;\n+  _pending_list_tail = &_pending_list;\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahReferenceProcessor.cpp","additions":497,"deletions":0,"binary":false,"changes":497,"status":"added"},{"patch":"@@ -0,0 +1,114 @@\n+\/*\n+ * Copyright (c) 2015, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, Red Hat, Inc. and\/or its affiliates.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_VM_GC_SHENANDOAH_SHENANDOAHREFERENCEPROCESSOR_HPP\n+#define SHARE_VM_GC_SHENANDOAH_SHENANDOAHREFERENCEPROCESSOR_HPP\n+\n+#include \"gc\/shared\/referenceDiscoverer.hpp\"\n+#include \"memory\/allocation.hpp\"\n+\n+class WorkGang;\n+\n+class ShenandoahRefProcThreadLocal : public CHeapObj<mtGC> {\n+private:\n+  void* _discovered_list;\n+\n+public:\n+  ShenandoahRefProcThreadLocal();\n+\n+  ShenandoahRefProcThreadLocal(const ShenandoahRefProcThreadLocal&) = delete; \/\/ non construction-copyable\n+  ShenandoahRefProcThreadLocal& operator=(const ShenandoahRefProcThreadLocal&) = delete; \/\/ non copyable\n+\n+  void reset();\n+\n+  template<typename T>\n+  T* discovered_list_addr();\n+  template<typename T>\n+  oop discovered_list_head() const;\n+  template<typename T>\n+  void set_discovered_list_head(oop head);\n+};\n+\n+class ShenandoahReferenceProcessor : public ReferenceDiscoverer {\n+private:\n+  ReferencePolicy* _soft_reference_policy;\n+\n+  ShenandoahRefProcThreadLocal* _ref_proc_thread_locals;\n+\n+  oop _pending_list;\n+  void* _pending_list_tail; \/\/ T*\n+\n+  volatile uint _iterate_discovered_list_id;\n+\n+  template <typename T>\n+  bool is_inactive(oop reference, oop referent, ReferenceType type) const;\n+  bool is_strongly_live(oop referent) const;\n+  bool is_softly_live(oop reference, ReferenceType type) const;\n+\n+  template <typename T>\n+  bool should_discover(oop reference, ReferenceType type) const;\n+  template <typename T>\n+  bool should_drop(oop reference, ReferenceType type) const;\n+\n+  \/\/ template <typename T>\n+  \/\/ void keep_alive(oop reference, ReferenceType type) const;\n+\n+  template <typename T>\n+  void make_inactive(oop reference, ReferenceType type) const;\n+\n+  template <typename T>\n+  bool discover(oop reference, ReferenceType type);\n+\n+  template <typename T>\n+  oop drop(oop reference, ReferenceType type);\n+  template <typename T>\n+  T* keep(oop reference, ReferenceType type);\n+\n+  template <typename T>\n+  void process_references(ShenandoahRefProcThreadLocal& refproc_data, uint worker_id);\n+  void enqueue_references_locked();\n+  void enqueue_references();\n+\n+public:\n+  ShenandoahReferenceProcessor(uint max_workers);\n+\n+  void reset_thread_locals();\n+\n+  void set_soft_reference_policy(bool clear);\n+\n+  bool discover_reference(oop obj, ReferenceType type) override;\n+\n+  void process_references(WorkGang* workers);\n+\n+  void work();\n+\n+  \/\/ TODO: Temporary methods to allow transition.\n+  void set_active_mt_degree(uint num_workers) {};\n+  void enable_discovery(bool verify_no_refs) {};\n+  void disable_discovery() {}\n+  void abandon_partial_discovery() {}\n+  void verify_no_references_recorded() {}\n+};\n+\n+#endif \/\/ SHARE_VM_GC_SHENANDOAH_SHENANDOAHREFERENCEPROCESSOR_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahReferenceProcessor.hpp","additions":114,"deletions":0,"binary":false,"changes":114,"status":"added"},{"patch":"@@ -69,1 +69,5 @@\n-JRT_LEAF(oopDesc*, ShenandoahRuntime::load_reference_barrier_native(oopDesc * src, oop* load_addr))\n+JRT_LEAF(oopDesc*, ShenandoahRuntime::load_reference_barrier_native(oopDesc* src, oop* load_addr))\n+  return (oopDesc*) ShenandoahBarrierSet::barrier_set()->load_reference_barrier_native(oop(src), load_addr);\n+JRT_END\n+\n+JRT_LEAF(oopDesc*, ShenandoahRuntime::load_reference_barrier_native_narrow(oopDesc* src, narrowOop* load_addr))\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahRuntime.cpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -45,0 +45,1 @@\n+  static oopDesc* load_reference_barrier_native_narrow(oopDesc* src, narrowOop* load_addr);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahRuntime.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -37,0 +37,2 @@\n+class ShenandoahMarkRefsSuperClosure;\n+\n@@ -53,0 +55,1 @@\n+  ShenandoahMarkRefsSuperClosure* _mark_closure;\n@@ -64,1 +67,2 @@\n-    _paced_time(0) {\n+    _paced_time(0),\n+    _mark_closure(NULL) {\n@@ -193,0 +197,8 @@\n+  static ShenandoahMarkRefsSuperClosure* mark_closure(Thread* thread) {\n+    return data(thread)->_mark_closure;\n+  }\n+\n+  static void set_mark_closure(Thread* thread, ShenandoahMarkRefsSuperClosure* mark_closure) {\n+    data(thread)->_mark_closure = mark_closure;\n+  }\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahThreadLocalData.hpp","additions":13,"deletions":1,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -62,1 +62,1 @@\n-class VM_ShenandoahFinalMarkStartEvac: public VM_ShenandoahReferenceOperation {\n+class VM_ShenandoahFinalMarkStartEvac: public VM_ShenandoahOperation {\n@@ -64,1 +64,1 @@\n-  VM_ShenandoahFinalMarkStartEvac() : VM_ShenandoahReferenceOperation() {};\n+  VM_ShenandoahFinalMarkStartEvac() : VM_ShenandoahOperation() {};\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahVMOperations.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -50,0 +50,7 @@\n+class ShenandoahIgnoreReferenceDiscoverer : public ReferenceDiscoverer {\n+public:\n+  virtual bool discover_reference(oop obj, ReferenceType type) {\n+    return true;\n+  }\n+};\n+\n@@ -71,1 +78,6 @@\n-    _loc(NULL) { }\n+    _loc(NULL) {\n+    if (options._verify_marked == ShenandoahVerifier::_verify_marked_complete_except_references ||\n+        options._verify_marked == ShenandoahVerifier::_verify_marked_disable) {\n+      set_ref_discoverer_internal(new ShenandoahIgnoreReferenceDiscoverer());\n+    }\n+  }\n@@ -85,1 +97,1 @@\n-\n+      obj = ShenandoahForwarding::get_forwardee(obj);\n@@ -211,0 +223,4 @@\n+      case ShenandoahVerifier::_verify_marked_complete_except_references:\n+        check(ShenandoahAsserts::_safe_all, obj, _heap->complete_marking_context()->is_marked(obj),\n+              \"Must be marked in complete bitmap, except j.l.r.Reference referents\");\n+        break;\n@@ -529,1 +545,1 @@\n-    MarkBitMap* mark_bit_map = _heap->complete_marking_context()->mark_bit_map();\n+    const ShenandoahMarkBitMap* mark_bit_map = _heap->complete_marking_context()->mark_bit_map();\n@@ -569,3 +585,4 @@\n-    cl.verify_oops_from(obj);\n-    (*processed)++;\n-\n+    if (!obj->klass()->is_instance_ref_klass()) {\n+      cl.verify_oops_from(obj);\n+      (*processed)++;\n+    }\n@@ -738,1 +755,1 @@\n-  if (ShenandoahVerifyLevel >= 4 && marked == _verify_marked_complete) {\n+  if (ShenandoahVerifyLevel >= 4 && (marked == _verify_marked_complete || marked == _verify_marked_complete_except_references)) {\n@@ -813,1 +830,1 @@\n-          _verify_marked_complete,     \/\/ bitmaps as precise as we can get\n+          _verify_marked_complete_except_references, \/\/ bitmaps as precise as we can get, except dangling j.l.r.Refs\n@@ -830,6 +847,6 @@\n-          _verify_forwarded_none,    \/\/ no forwarded references\n-          _verify_marked_complete,   \/\/ walk over marked objects too\n-          _verify_cset_disable,      \/\/ non-forwarded references to cset expected\n-          _verify_liveness_complete, \/\/ liveness data must be complete here\n-          _verify_regions_disable,   \/\/ trash regions not yet recycled\n-          _verify_gcstate_stable,    \/\/ mark should have stabilized the heap\n+          _verify_forwarded_none,                  \/\/ no forwarded references\n+          _verify_marked_complete_except_references, \/\/ walk over marked objects too\n+          _verify_cset_disable,                        \/\/ non-forwarded references to cset expected\n+          _verify_liveness_complete,                \/\/ liveness data must be complete here\n+          _verify_regions_disable,                  \/\/ trash regions not yet recycled\n+          _verify_gcstate_stable,                   \/\/ mark should have stabilized the heap\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahVerifier.cpp","additions":31,"deletions":14,"binary":false,"changes":45,"status":"modified"},{"patch":"@@ -68,1 +68,5 @@\n-    _verify_marked_complete\n+    _verify_marked_complete,\n+\n+    \/\/ Objects should be marked in \"complete\" bitmap, except j.l.r.Reference referents, which\n+    \/\/ may be dangling after marking but before conc-weakrefs-processing.\n+    _verify_marked_complete_except_references\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahVerifier.hpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -79,7 +79,0 @@\n-  product(uintx, ShenandoahRefProcFrequency, 5, EXPERIMENTAL,               \\\n-          \"Process process weak (soft, phantom, finalizers) references \"    \\\n-          \"every Nth cycle. Normally affects concurrent GC cycles only, \"   \\\n-          \"as degenerated and full GCs would try to process references \"    \\\n-          \"regardless. Set to zero to disable reference processing \"        \\\n-          \"completely.\")                                                    \\\n-                                                                            \\\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoah_globals.hpp","additions":0,"deletions":7,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -61,0 +61,2 @@\n+  virtual bool is_instance_ref_klass()        const { return true; }\n+\n","filename":"src\/hotspot\/share\/oops\/instanceRefKlass.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -608,0 +608,2 @@\n+  virtual bool is_instance_ref_klass()        const { return false; }\n+\n","filename":"src\/hotspot\/share\/oops\/klass.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"}]}