{"files":[{"patch":"@@ -35,1 +35,0 @@\n-class G1CardSetBufferList;\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CardSet.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -34,2 +34,0 @@\n-class G1CardSetBuffer;\n-\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CardSetFreeMemoryTask.hpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -33,71 +33,0 @@\n-G1CardSetBuffer::G1CardSetBuffer(uint elem_size, uint num_instances, G1CardSetBuffer* next) :\n-    _elem_size(elem_size), _num_elems(num_instances), _next(next), _next_allocate(0) {\n-\n-  _buffer = NEW_C_HEAP_ARRAY(char, (size_t)_num_elems * elem_size, mtGCCardSet);\n-}\n-\n-G1CardSetBuffer::~G1CardSetBuffer() {\n-  FREE_C_HEAP_ARRAY(mtGCCardSet, _buffer);\n-}\n-\n-void* G1CardSetBuffer::get_new_buffer_elem() {\n-  if (_next_allocate >= _num_elems) {\n-    return nullptr;\n-  }\n-  uint result = Atomic::fetch_and_add(&_next_allocate, 1u, memory_order_relaxed);\n-  if (result >= _num_elems) {\n-    return nullptr;\n-  }\n-  void* r = _buffer + (uint)result * _elem_size;\n-  return r;\n-}\n-\n-void G1CardSetBufferList::bulk_add(G1CardSetBuffer& first, G1CardSetBuffer& last, size_t num, size_t mem_size) {\n-  _list.prepend(first, last);\n-  Atomic::add(&_num_buffers, num, memory_order_relaxed);\n-  Atomic::add(&_mem_size, mem_size, memory_order_relaxed);\n-}\n-\n-void G1CardSetBufferList::print_on(outputStream* out, const char* prefix) {\n-  out->print_cr(\"%s: buffers %zu size %zu\", prefix, Atomic::load(&_num_buffers), Atomic::load(&_mem_size));\n-}\n-\n-G1CardSetBuffer* G1CardSetBufferList::get() {\n-  GlobalCounter::CriticalSection cs(Thread::current());\n-\n-  G1CardSetBuffer* result = _list.pop();\n-  if (result != nullptr) {\n-    Atomic::dec(&_num_buffers, memory_order_relaxed);\n-    Atomic::sub(&_mem_size, result->mem_size(), memory_order_relaxed);\n-  }\n-  return result;\n-}\n-\n-G1CardSetBuffer* G1CardSetBufferList::get_all(size_t& num_buffers, size_t& mem_size) {\n-  GlobalCounter::CriticalSection cs(Thread::current());\n-\n-  G1CardSetBuffer* result = _list.pop_all();\n-  num_buffers = Atomic::load(&_num_buffers);\n-  mem_size = Atomic::load(&_mem_size);\n-\n-  if (result != nullptr) {\n-    Atomic::sub(&_num_buffers, num_buffers, memory_order_relaxed);\n-    Atomic::sub(&_mem_size, mem_size, memory_order_relaxed);\n-  }\n-  return result;\n-}\n-\n-void G1CardSetBufferList::free_all() {\n-  size_t num_freed = 0;\n-  size_t mem_size_freed = 0;\n-  G1CardSetBuffer* cur;\n-\n-  while ((cur = _list.pop()) != nullptr) {\n-    mem_size_freed += cur->mem_size();\n-    num_freed++;\n-    delete cur;\n-  }\n-\n-  Atomic::sub(&_num_buffers, num_freed, memory_order_relaxed);\n-  Atomic::sub(&_mem_size, mem_size_freed, memory_order_relaxed);\n-}\n@@ -109,6 +38,1 @@\n-  _alloc_options(buffer_options),\n-  _first(nullptr),\n-  _last(nullptr),\n-  _num_buffers(0),\n-  _mem_size(0),\n-  _free_buffer_list(free_buffer_list),\n+  _segmented_array(name, buffer_options, free_buffer_list),\n@@ -119,3 +43,1 @@\n-  _num_free_nodes(0),\n-  _num_allocated_nodes(0),\n-  _num_available_nodes(0)\n+  _num_free_nodes(0)\n@@ -123,3 +45,2 @@\n-  assert(elem_size() >= sizeof(G1CardSetContainer), \"Element instance size %u for allocator %s too small\",\n-         elem_size(), name);\n-  assert(_free_buffer_list != nullptr, \"precondition!\");\n+  uint elem_size = _segmented_array.elem_size();\n+  assert(elem_size >= sizeof(G1CardSetContainer), \"Element instance size %u for allocator %s too small\", elem_size, name);\n@@ -167,1 +88,0 @@\n-  assert(elem_size() >= sizeof(G1CardSetContainer), \"size mismatch\");\n@@ -195,31 +115,0 @@\n-  G1CardSetBuffer* cur = Atomic::load_acquire(&_first);\n-\n-  if (cur != nullptr) {\n-    assert(_last != nullptr, \"If there is at least one element, there must be a last one.\");\n-\n-    G1CardSetBuffer* first = cur;\n-#ifdef ASSERT\n-    \/\/ Check list consistency.\n-    G1CardSetBuffer* last = cur;\n-    uint num_buffers = 0;\n-    size_t mem_size = 0;\n-    while (cur != nullptr) {\n-      mem_size += cur->mem_size();\n-      num_buffers++;\n-\n-      G1CardSetBuffer* next = cur->next();\n-      last = cur;\n-      cur = next;\n-    }\n-#endif\n-    assert(num_buffers == _num_buffers, \"Buffer count inconsistent %u %u\", num_buffers, _num_buffers);\n-    assert(mem_size == _mem_size, \"Memory size inconsistent\");\n-    assert(last == _last, \"Inconsistent last element\");\n-\n-    _free_buffer_list->bulk_add(*first, *_last, _num_buffers, _mem_size);\n-  }\n-\n-  _first = nullptr;\n-  _last = nullptr;\n-  _num_available_nodes = 0;\n-  _num_allocated_nodes = 0;\n@@ -227,2 +116,0 @@\n-  _num_buffers = 0;\n-  _mem_size = 0;\n@@ -234,0 +121,4 @@\n+  uint num_allocated_nodes = _segmented_array.num_allocated_nodes();\n+  uint num_available_nodes = _segmented_array.num_available_nodes();\n+  const G1SegmentedArrayBuffer<mtGCCardSet>* first_array_buffer = _segmented_array.first_array_buffer();\n+  uint num_buffers = _segmented_array.num_buffers();\n@@ -235,1 +126,8 @@\n-                p2i(this), _num_pending_nodes, _num_allocated_nodes, _num_available_nodes, percent_of(_num_allocated_nodes - _num_pending_nodes, _num_available_nodes), _first != nullptr ? _first->num_elems() : 0, _num_buffers, mem_size());\n+            p2i(this),\n+            _num_pending_nodes,\n+            num_allocated_nodes,\n+            num_available_nodes,\n+            percent_of(num_allocated_nodes - _num_pending_nodes, num_available_nodes),\n+            first_array_buffer != nullptr ? first_array_buffer->num_elems() : 0,\n+            num_buffers,\n+            mem_size());\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CardSetMemory.cpp","additions":16,"deletions":118,"binary":false,"changes":134,"status":"modified"},{"patch":"@@ -30,0 +30,2 @@\n+#include \"gc\/g1\/g1CardSetContainers.inline.hpp\"\n+#include \"gc\/g1\/g1SegmentedArray.hpp\"\n@@ -39,5 +41,0 @@\n-class G1CardSetAllocOptions {\n-  uint _elem_size;\n-  uint _initial_num_elems;\n-  \/\/ Defines a limit to the number of elements in the buffer\n-  uint _max_num_elems;\n@@ -45,3 +42,1 @@\n-  uint exponential_expand(uint prev_num_elems) {\n-    return clamp(prev_num_elems * 2, _initial_num_elems, _max_num_elems);\n-  }\n+class G1CardSetAllocOptions : public G1SegmentedArrayAllocOptions {\n@@ -51,2 +46,0 @@\n-  static const uint MinimumBufferSize = 8;\n-  static const uint MaximumBufferSize =  UINT_MAX \/ 2;\n@@ -55,3 +48,1 @@\n-    _elem_size(align_up(elem_size, BufferAlignment)),\n-    _initial_num_elems(initial_num_elems),\n-    _max_num_elems(max_num_elems) {\n+    G1SegmentedArrayAllocOptions(align_up(elem_size, BufferAlignment), initial_num_elems, max_num_elems, BufferAlignment) {\n@@ -67,57 +58,1 @@\n-\/\/ A single buffer\/arena containing _num_elems blocks of memory of _elem_size.\n-\/\/ G1CardSetBuffers can be linked together using a singly linked list.\n-class G1CardSetBuffer : public CHeapObj<mtGCCardSet> {\n-  uint _elem_size;\n-  uint _num_elems;\n-\n-  G1CardSetBuffer* volatile _next;\n-\n-  char* _buffer;  \/\/ Actual data.\n-\n-  \/\/ Index into the next free block to allocate into. Full if equal (or larger)\n-  \/\/ to _num_elems (can be larger because we atomically increment this value and\n-  \/\/ check only afterwards if the allocation has been successful).\n-  uint volatile _next_allocate;\n-\n-public:\n-  G1CardSetBuffer(uint elem_size, uint num_elems, G1CardSetBuffer* next);\n-  ~G1CardSetBuffer();\n-\n-  G1CardSetBuffer* volatile* next_addr() { return &_next; }\n-\n-  void* get_new_buffer_elem();\n-\n-  uint num_elems() const { return _num_elems; }\n-\n-  G1CardSetBuffer* next() const { return _next; }\n-\n-  void set_next(G1CardSetBuffer* next) {\n-    assert(next != this, \" loop condition\");\n-    _next = next;\n-  }\n-\n-  void reset(G1CardSetBuffer* next) {\n-    _next_allocate = 0;\n-    assert(next != this, \" loop condition\");\n-    set_next(next);\n-    memset((void*)_buffer, 0, (size_t)_num_elems * _elem_size);\n-  }\n-\n-  uint elem_size() const { return _elem_size; }\n-\n-  size_t mem_size() const { return sizeof(*this) + (size_t)_num_elems * _elem_size; }\n-\n-  bool is_full() const { return _next_allocate >= _num_elems; }\n-};\n-\n-\/\/ Set of (free) G1CardSetBuffers. The assumed usage is that allocation\n-\/\/ to it and removal of elements is strictly separate, but every action may be\n-\/\/ performed by multiple threads at the same time.\n-\/\/ Counts and memory usage are current on a best-effort basis if accessed concurrently.\n-class G1CardSetBufferList {\n-  static G1CardSetBuffer* volatile* next_ptr(G1CardSetBuffer& node) {\n-    return node.next_addr();\n-  }\n-  typedef LockFreeStack<G1CardSetBuffer, &next_ptr> NodeStack;\n-\n-  NodeStack _list;\n+typedef G1SegmentedArrayBuffer<mtGCCardSet> G1CardSetBuffer;\n@@ -125,21 +60,1 @@\n-  volatile size_t _num_buffers;\n-  volatile size_t _mem_size;\n-\n-public:\n-  G1CardSetBufferList() : _list(), _num_buffers(0), _mem_size(0) { }\n-  ~G1CardSetBufferList() { free_all(); }\n-\n-  void bulk_add(G1CardSetBuffer& first, G1CardSetBuffer& last, size_t num, size_t mem_size);\n-  void add(G1CardSetBuffer& elem) { _list.prepend(elem); }\n-\n-  G1CardSetBuffer* get();\n-  G1CardSetBuffer* get_all(size_t& num_buffers, size_t& mem_size);\n-\n-  \/\/ Give back all memory to the OS.\n-  void free_all();\n-\n-  void print_on(outputStream* out, const char* prefix = \"\");\n-\n-  size_t num_buffers() const { return Atomic::load(&_num_buffers); }\n-  size_t mem_size() const { return Atomic::load(&_mem_size); }\n-};\n+typedef G1SegmentedArrayBufferList<mtGCCardSet> G1CardSetBufferList;\n@@ -149,14 +64,0 @@\n-\/\/ Actual allocation from the C heap occurs on G1CardSetBuffer basis, i.e. sets\n-\/\/ of elements. The assumed allocation pattern for these G1CardSetBuffer elements\n-\/\/ is assumed to be strictly two-phased:\n-\/\/\n-\/\/ - in the first phase, G1CardSetBuffers are allocated from the C heap (or a free\n-\/\/ list given at initialization time). This allocation may occur in parallel. This\n-\/\/ typically corresponds to a single mutator phase, but may extend over multiple.\n-\/\/\n-\/\/ - in the second phase, G1CardSetBuffers are given back in bulk to the free list.\n-\/\/ This is typically done during a GC pause.\n-\/\/\n-\/\/ Some third party is responsible for giving back memory from the free list to\n-\/\/ the operating system.\n-\/\/\n@@ -171,1 +72,1 @@\n-\/\/ The G1CardSetContainerOnHeaps free list is a linked list of G1CardSetContainers\n+\/\/ The NodeStack free list is a linked list of G1CardSetContainers\n@@ -187,12 +88,1 @@\n-  \/\/ G1CardSetAllocOptions provides parameters for allocation buffer\n-  \/\/ sizing and expansion.\n-  G1CardSetAllocOptions _alloc_options;\n-\n-  G1CardSetBuffer* volatile _first;       \/\/ The (start of the) list of all buffers.\n-  G1CardSetBuffer* _last;                 \/\/ The last element of the list of all buffers.\n-  volatile uint _num_buffers;             \/\/ Number of assigned buffers to this allocator.\n-  volatile size_t _mem_size;              \/\/ Memory used by all buffers.\n-\n-  G1CardSetBufferList* _free_buffer_list; \/\/ The global free buffer list to\n-                                          \/\/ preferentially get new buffers from.\n-\n+  typedef G1SegmentedArray<Elem, mtGCCardSet> SegmentedArray;\n@@ -201,1 +91,0 @@\n-\n@@ -205,0 +94,1 @@\n+  SegmentedArray _segmented_array;\n@@ -212,3 +102,0 @@\n-  volatile uint _num_allocated_nodes; \/\/ Number of total nodes allocated and in use.\n-  volatile uint _num_available_nodes; \/\/ Number of nodes available in all buffers (allocated + free + pending + not yet used).\n-\n@@ -222,4 +109,0 @@\n-  G1CardSetBuffer* create_new_buffer(G1CardSetBuffer* const prev);\n-\n-  uint elem_size() const { return _alloc_options.elem_size(); }\n-\n@@ -241,2 +124,0 @@\n-  uint num_buffers() const;\n-\n@@ -245,1 +126,2 @@\n-      num_buffers() * sizeof(G1CardSetBuffer) + (size_t)_num_available_nodes * elem_size();\n+      _segmented_array.num_buffers() * sizeof(G1CardSetBuffer)\n+            + _segmented_array.num_available_nodes() * _segmented_array.elem_size();\n@@ -249,1 +131,3 @@\n-    return ((size_t)_num_available_nodes - (_num_allocated_nodes - _num_pending_nodes)) * elem_size();\n+    return (_segmented_array.num_available_nodes()\n+              - (_segmented_array.num_allocated_nodes() - _num_pending_nodes))\n+                * _segmented_array.elem_size();\n@@ -251,0 +135,1 @@\n+  inline uint num_buffers() { return _segmented_array.num_buffers(); }\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CardSetMemory.hpp","additions":15,"deletions":130,"binary":false,"changes":145,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"gc\/g1\/g1SegmentedArray.inline.hpp\"\n@@ -40,33 +41,0 @@\n-template <class Elem>\n-G1CardSetBuffer* G1CardSetAllocator<Elem>::create_new_buffer(G1CardSetBuffer* const prev) {\n-\n-  \/\/ Take an existing buffer if available.\n-  G1CardSetBuffer* next = _free_buffer_list->get();\n-  if (next == nullptr) {\n-    uint prev_num_elems = (prev != nullptr) ? prev->num_elems() : 0;\n-    uint num_elems = _alloc_options.next_num_elems(prev_num_elems);\n-    next = new G1CardSetBuffer(elem_size(), num_elems, prev);\n-  } else {\n-    assert(elem_size() == next->elem_size() , \"Mismatch %d != %d Elem %zu\", elem_size(), next->elem_size(), sizeof(Elem));\n-    next->reset(prev);\n-  }\n-\n-  \/\/ Install it as current allocation buffer.\n-  G1CardSetBuffer* old = Atomic::cmpxchg(&_first, prev, next);\n-  if (old != prev) {\n-    \/\/ Somebody else installed the buffer, use that one.\n-    delete next;\n-    return old;\n-  } else {\n-    \/\/ Did we install the first element in the list? If so, this is also the last.\n-    if (prev == nullptr) {\n-      _last = next;\n-    }\n-    \/\/ Successfully installed the buffer into the list.\n-    Atomic::inc(&_num_buffers, memory_order_relaxed);\n-    Atomic::add(&_mem_size, next->mem_size(), memory_order_relaxed);\n-    Atomic::add(&_num_available_nodes, next->num_elems(), memory_order_relaxed);\n-    return next;\n-  }\n-}\n-\n@@ -75,1 +43,2 @@\n-  assert(elem_size() > 0, \"instance size not set.\");\n+  uint elem_size = _segmented_array.elem_size();\n+  assert(elem_size > 0, \"instance size not set.\");\n@@ -91,16 +60,3 @@\n-  G1CardSetBuffer* cur = Atomic::load_acquire(&_first);\n-  if (cur == nullptr) {\n-    cur = create_new_buffer(cur);\n-  }\n-\n-  while (true) {\n-    Elem* elem = (Elem*)cur->get_new_buffer_elem();\n-    if (elem != nullptr) {\n-      Atomic::inc(&_num_allocated_nodes, memory_order_relaxed);\n-      guarantee(is_aligned(elem, 8), \"result \" PTR_FORMAT \" not aligned\", p2i(elem));\n-      return elem;\n-    }\n-    \/\/ The buffer is full. Next round.\n-    assert(cur->is_full(), \"must be\");\n-    cur = create_new_buffer(cur);\n-  }\n+  Elem* elem = _segmented_array.allocate();\n+  assert(elem != nullptr, \"must be\");\n+  return elem;\n@@ -122,5 +78,0 @@\n-template <class Elem>\n-inline uint G1CardSetAllocator<Elem>::num_buffers() const {\n-  return Atomic::load(&_num_buffers);\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CardSetMemory.inline.hpp","additions":6,"deletions":55,"binary":false,"changes":61,"status":"modified"},{"patch":"@@ -0,0 +1,230 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2021, Huawei Technologies Co. Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_G1_G1SEGMENTEDARRAY_HPP\n+#define SHARE_GC_G1_G1SEGMENTEDARRAY_HPP\n+\n+#include \"memory\/allocation.hpp\"\n+#include \"utilities\/lockFreeStack.hpp\"\n+\n+\n+\/\/ A single buffer\/arena containing _num_elems blocks of memory of _elem_size.\n+\/\/ G1SegmentedArrayBuffers can be linked together using a singly linked list.\n+template<MEMFLAGS flag>\n+class G1SegmentedArrayBuffer : public CHeapObj<flag> {\n+  const uint _elem_size;\n+  const uint _num_elems;\n+\n+  G1SegmentedArrayBuffer* volatile _next;\n+\n+  char* _buffer;  \/\/ Actual data.\n+\n+  \/\/ Index into the next free block to allocate into. Full if equal (or larger)\n+  \/\/ to _num_elems (can be larger because we atomically increment this value and\n+  \/\/ check only afterwards if the allocation has been successful).\n+  uint volatile _next_allocate;\n+\n+public:\n+  G1SegmentedArrayBuffer(uint elem_size, uint num_elems, G1SegmentedArrayBuffer* next);\n+  ~G1SegmentedArrayBuffer();\n+\n+  G1SegmentedArrayBuffer* volatile* next_addr() { return &_next; }\n+\n+  void* get_new_buffer_elem();\n+\n+  uint num_elems() const { return _num_elems; }\n+\n+  G1SegmentedArrayBuffer* next() const { return _next; }\n+\n+  void set_next(G1SegmentedArrayBuffer* next) {\n+    assert(next != this, \" loop condition\");\n+    _next = next;\n+  }\n+\n+  void reset(G1SegmentedArrayBuffer* next) {\n+    _next_allocate = 0;\n+    assert(next != this, \" loop condition\");\n+    set_next(next);\n+    memset((void*)_buffer, 0, (size_t)_num_elems * _elem_size);\n+  }\n+\n+  uint elem_size() const { return _elem_size; }\n+\n+  size_t mem_size() const { return sizeof(*this) + (size_t)_num_elems * _elem_size; }\n+\n+  uint length() {\n+    \/\/ _next_allocate might grow greater than _num_elems in multi-thread env,\n+    \/\/ so, here we need to return the adjusted real length value.\n+    _next_allocate = _next_allocate > _num_elems ? _num_elems : _next_allocate;\n+    return _next_allocate;\n+  }\n+\n+  bool is_full() const { return _next_allocate >= _num_elems; }\n+};\n+\n+\n+\n+\/\/ Set of (free) G1SegmentedArrayBuffers. The assumed usage is that allocation\n+\/\/ to it and removal of elements is strictly separate, but every action may be\n+\/\/ performed by multiple threads at the same time.\n+\/\/ Counts and memory usage are current on a best-effort basis if accessed concurrently.\n+template<MEMFLAGS flag>\n+class G1SegmentedArrayBufferList {\n+  static G1SegmentedArrayBuffer<flag>* volatile* next_ptr(G1SegmentedArrayBuffer<flag>& node) {\n+    return node.next_addr();\n+  }\n+  typedef LockFreeStack<G1SegmentedArrayBuffer<flag>, &G1SegmentedArrayBufferList::next_ptr> NodeStack;\n+\n+  NodeStack _list;\n+\n+  volatile size_t _num_buffers;\n+  volatile size_t _mem_size;\n+\n+public:\n+  G1SegmentedArrayBufferList() : _list(), _num_buffers(0), _mem_size(0) { }\n+  ~G1SegmentedArrayBufferList() { free_all(); }\n+\n+  void bulk_add(G1SegmentedArrayBuffer<flag>& first, G1SegmentedArrayBuffer<flag>& last, size_t num, size_t mem_size);\n+  void add(G1SegmentedArrayBuffer<flag>& elem) { _list.prepend(elem); }\n+\n+  G1SegmentedArrayBuffer<flag>* get();\n+  G1SegmentedArrayBuffer<flag>* get_all(size_t& num_buffers, size_t& mem_size);\n+\n+  \/\/ Give back all memory to the OS.\n+  void free_all();\n+\n+  void print_on(outputStream* out, const char* prefix = \"\");\n+\n+  size_t num_buffers() const { return Atomic::load(&_num_buffers); }\n+  size_t mem_size() const { return Atomic::load(&_mem_size); }\n+};\n+\n+\n+\/\/ Configuration for G1SegmentedArray, e.g element size, element number of next G1SegmentedArrayBuffer.\n+class G1SegmentedArrayAllocOptions {\n+protected:\n+  uint _elem_size;\n+  uint _initial_num_elems;\n+  \/\/ Defines a limit to the number of elements in the buffer\n+  uint _max_num_elems;\n+  uint _alignment;\n+\n+  uint exponential_expand(uint prev_num_elems) {\n+    return clamp(prev_num_elems * 2, _initial_num_elems, _max_num_elems);\n+  }\n+public:\n+  static const uint BufferAlignment = 4;\n+  static const uint MinimumBufferSize = 8;\n+  static const uint MaximumBufferSize =  UINT_MAX \/ 2;\n+\n+  G1SegmentedArrayAllocOptions(uint elem_size, uint initial_num_elems = MinimumBufferSize,\n+                               uint max_num_elems = MaximumBufferSize, uint alignment = BufferAlignment) :\n+    _elem_size(elem_size),\n+    _initial_num_elems(initial_num_elems),\n+    _max_num_elems(max_num_elems),\n+    _alignment(alignment) {\n+  }\n+\n+  uint next_num_elems(uint prev_num_elems) {\n+    return _initial_num_elems;\n+  }\n+\n+  uint elem_size() const { return _elem_size; }\n+\n+  uint alignment() const { return _alignment; }\n+};\n+\n+\n+\/\/ A segmented array where G1SegmentedArrayBuffer is the segment, and\n+\/\/ G1SegmentedArrayBufferList is the free list to cache G1SegmentedArrayBuffer,\n+\/\/ and G1SegmentedArrayAllocOptions is the configuration for G1SegmentedArray\n+\/\/ attributes.\n+\/\/\n+\/\/ Implementation details as below:\n+\/\/\n+\/\/ Arena-like allocator for (card set, or ...) heap memory objects (Elem elements).\n+\/\/\n+\/\/ Actual allocation from the C heap occurs on G1SegmentedArrayBuffer basis, i.e. segments\n+\/\/ of elements. The assumed allocation pattern for these G1SegmentedArrayBuffer elements\n+\/\/ is assumed to be strictly two-phased:\n+\/\/\n+\/\/ - in the first phase, G1SegmentedArrayBuffers are allocated from the C heap (or a free\n+\/\/ list given at initialization time). This allocation may occur in parallel. This\n+\/\/ typically corresponds to a single mutator phase, but may extend over multiple.\n+\/\/\n+\/\/ - in the second phase, G1SegmentedArrayBuffers are given back in bulk to the free list.\n+\/\/ This is typically done during a GC pause.\n+\/\/\n+\/\/ Some third party is responsible for giving back memory from the free list to\n+\/\/ the operating system.\n+\/\/\n+\/\/ Allocation and deallocation in the first phase basis may occur by multiple threads at once.\n+\/\/\n+\/\/ The class also manages a few counters for statistics using atomic operations.\n+\/\/ Their values are only consistent within each other with extra global\n+\/\/ synchronization.\n+template <class Elem, MEMFLAGS flag>\n+class G1SegmentedArray {\n+  \/\/ G1CardSetAllocOptions provides parameters for allocation buffer\n+  \/\/ sizing and expansion.\n+  G1SegmentedArrayAllocOptions _alloc_options;\n+\n+  G1SegmentedArrayBuffer<flag>* volatile _first;       \/\/ The (start of the) list of all buffers.\n+  G1SegmentedArrayBuffer<flag>* _last;                 \/\/ The last element of the list of all buffers.\n+  volatile uint _num_buffers;             \/\/ Number of assigned buffers to this allocator.\n+  volatile size_t _mem_size;              \/\/ Memory used by all buffers.\n+\n+  G1SegmentedArrayBufferList<flag>* _free_buffer_list; \/\/ The global free buffer list to\n+                                                       \/\/ preferentially get new buffers from.\n+\n+  volatile uint _num_available_nodes; \/\/ Number of nodes available in all buffers (allocated + free + pending + not yet used).\n+  volatile uint _num_allocated_nodes; \/\/ Number of total nodes allocated and in use.\n+\n+private:\n+  inline G1SegmentedArrayBuffer<flag>* create_new_buffer(G1SegmentedArrayBuffer<flag>* const prev);\n+\n+public:\n+  uint num_available_nodes() const { return _num_available_nodes; }\n+  uint num_allocated_nodes() const { return _num_allocated_nodes; }\n+  const G1SegmentedArrayBuffer<flag>* first_array_buffer() const { return _first; }\n+  inline uint elem_size() const;\n+\n+  G1SegmentedArray(const char* name,\n+                   const G1SegmentedArrayAllocOptions& buffer_options,\n+                   G1SegmentedArrayBufferList<flag>* free_buffer_list);\n+  ~G1SegmentedArray() {\n+    drop_all();\n+  }\n+\n+  \/\/ Deallocate all buffers to the free buffer list and reset this allocator. Must\n+  \/\/ be called in a globally synchronized area.\n+  void drop_all();\n+\n+  inline Elem* allocate();\n+\n+  inline uint num_buffers() const;\n+};\n+\n+#endif \/\/SHARE_GC_G1_G1SEGMENTEDARRAY_HPP\n","filename":"src\/hotspot\/share\/gc\/g1\/g1SegmentedArray.hpp","additions":230,"deletions":0,"binary":false,"changes":230,"status":"added"},{"patch":"@@ -0,0 +1,241 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2021, Huawei Technologies Co. Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_G1_G1SEGMENTEDARRAY_INLINE_HPP\n+#define SHARE_GC_G1_G1SEGMENTEDARRAY_INLINE_HPP\n+\n+#include \"gc\/g1\/g1SegmentedArray.hpp\"\n+#include \"runtime\/atomic.hpp\"\n+#include \"utilities\/globalCounter.inline.hpp\"\n+\n+\n+template<MEMFLAGS flag>\n+G1SegmentedArrayBuffer<flag>::G1SegmentedArrayBuffer(uint elem_size, uint num_instances, G1SegmentedArrayBuffer* next) :\n+  _elem_size(elem_size), _num_elems(num_instances), _next(next), _next_allocate(0) {\n+\n+  _buffer = NEW_C_HEAP_ARRAY(char, (size_t)_num_elems * elem_size, mtGCCardSet);\n+}\n+\n+template<MEMFLAGS flag>\n+G1SegmentedArrayBuffer<flag>::~G1SegmentedArrayBuffer() {\n+  FREE_C_HEAP_ARRAY(mtGCCardSet, _buffer);\n+}\n+\n+template<MEMFLAGS flag>\n+void* G1SegmentedArrayBuffer<flag>::get_new_buffer_elem() {\n+  if (_next_allocate >= _num_elems) {\n+    return nullptr;\n+  }\n+  uint result = Atomic::fetch_and_add(&_next_allocate, 1u, memory_order_relaxed);\n+  if (result >= _num_elems) {\n+    return nullptr;\n+  }\n+  void* r = _buffer + (uint)result * _elem_size;\n+  return r;\n+}\n+\n+\n+template<MEMFLAGS flag>\n+void G1SegmentedArrayBufferList<flag>::bulk_add(G1SegmentedArrayBuffer<flag>& first,\n+                                                G1SegmentedArrayBuffer<flag>& last,\n+                                                size_t num,\n+                                                size_t mem_size) {\n+  _list.prepend(first, last);\n+  Atomic::add(&_num_buffers, num, memory_order_relaxed);\n+  Atomic::add(&_mem_size, mem_size, memory_order_relaxed);\n+}\n+\n+template<MEMFLAGS flag>\n+void G1SegmentedArrayBufferList<flag>::print_on(outputStream* out, const char* prefix) {\n+  out->print_cr(\"%s: buffers %zu size %zu\",\n+                prefix, Atomic::load(&_num_buffers), Atomic::load(&_mem_size));\n+}\n+\n+template<MEMFLAGS flag>\n+G1SegmentedArrayBuffer<flag>* G1SegmentedArrayBufferList<flag>::get() {\n+  GlobalCounter::CriticalSection cs(Thread::current());\n+\n+  G1SegmentedArrayBuffer<flag>* result = _list.pop();\n+  if (result != nullptr) {\n+    Atomic::dec(&_num_buffers, memory_order_relaxed);\n+    Atomic::sub(&_mem_size, result->mem_size(), memory_order_relaxed);\n+  }\n+  return result;\n+}\n+\n+template<MEMFLAGS flag>\n+G1SegmentedArrayBuffer<flag>* G1SegmentedArrayBufferList<flag>::get_all(size_t& num_buffers,\n+                                                                        size_t& mem_size) {\n+  GlobalCounter::CriticalSection cs(Thread::current());\n+\n+  G1SegmentedArrayBuffer<flag>* result = _list.pop_all();\n+  num_buffers = Atomic::load(&_num_buffers);\n+  mem_size = Atomic::load(&_mem_size);\n+\n+  if (result != nullptr) {\n+    Atomic::sub(&_num_buffers, num_buffers, memory_order_relaxed);\n+    Atomic::sub(&_mem_size, mem_size, memory_order_relaxed);\n+  }\n+  return result;\n+}\n+\n+template<MEMFLAGS flag>\n+void G1SegmentedArrayBufferList<flag>::free_all() {\n+  size_t num_freed = 0;\n+  size_t mem_size_freed = 0;\n+  G1SegmentedArrayBuffer<flag>* cur;\n+\n+  while ((cur = _list.pop()) != nullptr) {\n+    mem_size_freed += cur->mem_size();\n+    num_freed++;\n+    delete cur;\n+  }\n+\n+  Atomic::sub(&_num_buffers, num_freed, memory_order_relaxed);\n+  Atomic::sub(&_mem_size, mem_size_freed, memory_order_relaxed);\n+}\n+\n+\n+template <class Elem, MEMFLAGS flag>\n+G1SegmentedArrayBuffer<flag>* G1SegmentedArray<Elem, flag>::create_new_buffer(G1SegmentedArrayBuffer<flag>* const prev) {\n+\n+  \/\/ Take an existing buffer if available.\n+  G1SegmentedArrayBuffer<flag>* next = _free_buffer_list->get();\n+  if (next == nullptr) {\n+    uint prev_num_elems = (prev != nullptr) ? prev->num_elems() : 0;\n+    uint num_elems = _alloc_options.next_num_elems(prev_num_elems);\n+    next = new G1SegmentedArrayBuffer<flag>(elem_size(), num_elems, prev);\n+  } else {\n+    assert(elem_size() == next->elem_size() ,\n+           \"Mismatch %d != %d Elem %zu\", elem_size(), next->elem_size(), sizeof(Elem));\n+    next->reset(prev);\n+  }\n+\n+  \/\/ Install it as current allocation buffer.\n+  G1SegmentedArrayBuffer<flag>* old = Atomic::cmpxchg(&_first, prev, next);\n+  if (old != prev) {\n+    \/\/ Somebody else installed the buffer, use that one.\n+    delete next;\n+    return old;\n+  } else {\n+    \/\/ Did we install the first element in the list? If so, this is also the last.\n+    if (prev == nullptr) {\n+      _last = next;\n+    }\n+    \/\/ Successfully installed the buffer into the list.\n+    Atomic::inc(&_num_buffers, memory_order_relaxed);\n+    Atomic::add(&_mem_size, next->mem_size(), memory_order_relaxed);\n+    Atomic::add(&_num_available_nodes, next->num_elems(), memory_order_relaxed);\n+    return next;\n+  }\n+}\n+\n+template <class Elem, MEMFLAGS flag>\n+uint G1SegmentedArray<Elem, flag>::elem_size() const {\n+  return _alloc_options.elem_size();\n+}\n+\n+template <class Elem, MEMFLAGS flag>\n+G1SegmentedArray<Elem, flag>::G1SegmentedArray(const char* name,\n+                                               const G1SegmentedArrayAllocOptions& buffer_options,\n+                                               G1SegmentedArrayBufferList<flag>* free_buffer_list) :\n+     _alloc_options(buffer_options),\n+     _first(nullptr),\n+     _last(nullptr),\n+     _num_buffers(0),\n+     _mem_size(0),\n+     _free_buffer_list(free_buffer_list),\n+     _num_available_nodes(0),\n+     _num_allocated_nodes(0) {\n+  assert(_free_buffer_list != nullptr, \"precondition!\");\n+}\n+\n+template <class Elem, MEMFLAGS flag>\n+void G1SegmentedArray<Elem, flag>::drop_all() {\n+  G1SegmentedArrayBuffer<flag>* cur = Atomic::load_acquire(&_first);\n+\n+  if (cur != nullptr) {\n+    assert(_last != nullptr, \"If there is at least one element, there must be a last one.\");\n+\n+    G1SegmentedArrayBuffer<flag>* first = cur;\n+#ifdef ASSERT\n+    \/\/ Check list consistency.\n+    G1SegmentedArrayBuffer<flag>* last = cur;\n+    uint num_buffers = 0;\n+    size_t mem_size = 0;\n+    while (cur != nullptr) {\n+      mem_size += cur->mem_size();\n+      num_buffers++;\n+\n+      G1SegmentedArrayBuffer<flag>* next = cur->next();\n+      last = cur;\n+      cur = next;\n+    }\n+#endif\n+    assert(num_buffers == _num_buffers, \"Buffer count inconsistent %u %u\", num_buffers, _num_buffers);\n+    assert(mem_size == _mem_size, \"Memory size inconsistent\");\n+    assert(last == _last, \"Inconsistent last element\");\n+\n+    _free_buffer_list->bulk_add(*first, *_last, _num_buffers, _mem_size);\n+  }\n+\n+  _first = nullptr;\n+  _last = nullptr;\n+  _num_buffers = 0;\n+  _mem_size = 0;\n+  _num_available_nodes = 0;\n+  _num_allocated_nodes = 0;\n+}\n+\n+template <class Elem, MEMFLAGS flag>\n+Elem* G1SegmentedArray<Elem, flag>::allocate() {\n+  uint es = elem_size();\n+  assert(es > 0, \"instance size not set.\");\n+\n+  G1SegmentedArrayBuffer<flag>* cur = Atomic::load_acquire(&_first);\n+  if (cur == nullptr) {\n+    cur = create_new_buffer(cur);\n+  }\n+\n+  while (true) {\n+    Elem* elem = (Elem*)cur->get_new_buffer_elem();\n+    if (elem != nullptr) {\n+      Atomic::inc(&_num_allocated_nodes, memory_order_relaxed);\n+      guarantee(is_aligned(elem, _alloc_options.alignment()),\n+                \"result \" PTR_FORMAT \" not aligned at %u\", p2i(elem), _alloc_options.alignment());\n+      return elem;\n+    }\n+    \/\/ The buffer is full. Next round.\n+    assert(cur->is_full(), \"must be\");\n+    cur = create_new_buffer(cur);\n+  }\n+}\n+\n+template <class Elem, MEMFLAGS flag>\n+inline uint G1SegmentedArray<Elem, flag>::num_buffers() const {\n+  return Atomic::load(&_num_buffers);\n+}\n+\n+#endif \/\/SHARE_GC_G1_G1SEGMENTEDARRAY_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/g1\/g1SegmentedArray.inline.hpp","additions":241,"deletions":0,"binary":false,"changes":241,"status":"added"}]}