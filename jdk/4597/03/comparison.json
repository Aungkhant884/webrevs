{"files":[{"patch":"@@ -49,0 +49,5 @@\n+extern aarch64_atomic_stub_t aarch64_atomic_cmpxchg_4_seq_cst_impl;\n+extern aarch64_atomic_stub_t aarch64_atomic_cmpxchg_8_seq_cst_impl;\n+extern aarch64_atomic_stub_t aarch64_atomic_cmpxchg_4_release_impl;\n+extern aarch64_atomic_stub_t aarch64_atomic_cmpxchg_8_release_impl;\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/atomic_aarch64.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -5957,0 +5957,4 @@\n+      case memory_order_release:\n+        acquire = false;\n+        release = true;\n+        break;\n@@ -6038,0 +6042,14 @@\n+    AtomicStubMark mark_cmpxchg_4_release\n+      (_masm, &aarch64_atomic_cmpxchg_4_release_impl);\n+    gen_cas_entry(MacroAssembler::word, memory_order_release);\n+    AtomicStubMark mark_cmpxchg_8_release\n+      (_masm, &aarch64_atomic_cmpxchg_8_release_impl);\n+    gen_cas_entry(MacroAssembler::xword, memory_order_release);\n+\n+    AtomicStubMark mark_cmpxchg_4_seq_cst\n+      (_masm, &aarch64_atomic_cmpxchg_4_seq_cst_impl);\n+    gen_cas_entry(MacroAssembler::word, memory_order_seq_cst);\n+    AtomicStubMark mark_cmpxchg_8_seq_cst\n+      (_masm, &aarch64_atomic_cmpxchg_8_seq_cst_impl);\n+    gen_cas_entry(MacroAssembler::xword, memory_order_seq_cst);\n+\n@@ -7204,0 +7222,4 @@\n+DEFAULT_ATOMIC_OP(cmpxchg, 4, _release)\n+DEFAULT_ATOMIC_OP(cmpxchg, 8, _release)\n+DEFAULT_ATOMIC_OP(cmpxchg, 4, _seq_cst)\n+DEFAULT_ATOMIC_OP(cmpxchg, 8, _seq_cst)\n","filename":"src\/hotspot\/cpu\/aarch64\/stubGenerator_aarch64.cpp","additions":22,"deletions":0,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -30,0 +30,2 @@\n+#include \"utilities\/debug.hpp\"\n+\n@@ -67,1 +69,1 @@\n-  if (order == memory_order_relaxed) {\n+  if (order == memory_order_conservative) {\n@@ -69,0 +71,1 @@\n+    FULL_MEM_BARRIER;\n@@ -71,0 +74,1 @@\n+    FULL_MEM_BARRIER;\n@@ -73,0 +77,23 @@\n+    STATIC_ASSERT (\n+       \/\/ The modes that align with C++11 are intended to\n+       \/\/ follow the same semantics.\n+       memory_order_relaxed == __ATOMIC_RELAXED &&\n+       memory_order_acquire == __ATOMIC_ACQUIRE &&\n+       memory_order_release == __ATOMIC_RELEASE &&\n+       memory_order_acq_rel == __ATOMIC_ACQ_REL &&\n+       memory_order_seq_cst == __ATOMIC_SEQ_CST);\n+\n+    \/\/ Some sanity checking on the memory order. It makes no\n+    \/\/ sense to have a release operation for a store that never\n+    \/\/ happens.\n+    int failure_memory_order;\n+    switch (order) {\n+    case memory_order_release:\n+      failure_memory_order = memory_order_relaxed; break;\n+    case memory_order_acq_rel:\n+      failure_memory_order = memory_order_acquire; break;\n+    default:\n+      failure_memory_order = order;\n+    }\n+    assert(failure_memory_order <= order, \"must be\");\n+\n@@ -74,1 +101,0 @@\n-    FULL_MEM_BARRIER;\n@@ -76,2 +102,1 @@\n-                              __ATOMIC_RELAXED, __ATOMIC_RELAXED);\n-    FULL_MEM_BARRIER;\n+                              order, failure_memory_order);\n","filename":"src\/hotspot\/os_cpu\/bsd_aarch64\/atomic_bsd_aarch64.hpp","additions":29,"deletions":4,"binary":false,"changes":33,"status":"modified"},{"patch":"@@ -115,1 +115,49 @@\n-        .globl aarch64_atomic_cmpxchg_1_relaxed_default_impl\n+        .globl aarch64_atomic_cmpxchg_4_release_default_impl\n+        .align 5\n+aarch64_atomic_cmpxchg_4_release_default_impl:\n+        prfm    pstl1strm, [x0]\n+0:      ldxr    w3, [x0]\n+        cmp     w3, w1\n+        b.ne    1f\n+        stlxr    w8, w2, [x0]\n+        cbnz    w8, 0b\n+1:      mov     w0, w3\n+        ret\n+\n+        .globl aarch64_atomic_cmpxchg_8_release_default_impl\n+        .align 5\n+aarch64_atomic_cmpxchg_8_release_default_impl:\n+        prfm    pstl1strm, [x0]\n+0:      ldxr    x3, [x0]\n+        cmp     x3, x1\n+        b.ne    1f\n+        stlxr    w8, x2, [x0]\n+        cbnz    w8, 0b\n+1:      mov     x0, x3\n+        ret\n+\n+        .globl aarch64_atomic_cmpxchg_4_seq_cst_default_impl\n+        .align 5\n+aarch64_atomic_cmpxchg_4_seq_cst_default_impl:\n+        prfm    pstl1strm, [x0]\n+0:      ldaxr    w3, [x0]\n+        cmp     w3, w1\n+        b.ne    1f\n+        stlxr    w8, w2, [x0]\n+        cbnz    w8, 0b\n+1:      mov     w0, w3\n+        ret\n+\n+        .globl aarch64_atomic_cmpxchg_8_seq_cst_default_impl\n+        .align 5\n+aarch64_atomic_cmpxchg_8_seq_cst_default_impl:\n+        prfm    pstl1strm, [x0]\n+0:      ldaxr    x3, [x0]\n+        cmp     x3, x1\n+        b.ne    1f\n+        stlxr    w8, x2, [x0]\n+        cbnz    w8, 0b\n+1:      mov     x0, x3\n+        ret\n+\n+.globl aarch64_atomic_cmpxchg_1_relaxed_default_impl\n","filename":"src\/hotspot\/os_cpu\/linux_aarch64\/atomic_linux_aarch64.S","additions":49,"deletions":1,"binary":false,"changes":50,"status":"modified"},{"patch":"@@ -154,0 +154,5 @@\n+  case memory_order_release:\n+    stub = aarch64_atomic_cmpxchg_4_release_impl; break;\n+  case memory_order_seq_cst:\n+  case memory_order_acq_rel:\n+    stub = aarch64_atomic_cmpxchg_4_seq_cst_impl; break;\n@@ -172,0 +177,5 @@\n+  case memory_order_release:\n+    stub = aarch64_atomic_cmpxchg_8_release_impl; break;\n+  case memory_order_seq_cst:\n+  case memory_order_acq_rel:\n+    stub = aarch64_atomic_cmpxchg_8_seq_cst_impl; break;\n","filename":"src\/hotspot\/os_cpu\/linux_aarch64\/atomic_linux_aarch64.hpp","additions":10,"deletions":0,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -50,0 +50,1 @@\n+  memory_order_seq_cst = 5,\n","filename":"src\/hotspot\/share\/runtime\/atomic.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"}]}