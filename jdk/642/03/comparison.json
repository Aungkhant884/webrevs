{"files":[{"patch":"@@ -28,1 +28,1 @@\n-#include \"runtime\/objectMonitor.hpp\"\n+#include \"runtime\/objectMonitor.inline.hpp\"\n@@ -31,0 +31,33 @@\n+markWord markWord::displaced_mark_helper() const {\n+  assert(has_displaced_mark_helper(), \"check\");\n+  if (has_monitor()) {\n+    \/\/ Has an inflated monitor. Must be checked before has_locker().\n+    ObjectMonitor* monitor = this->monitor();\n+    return monitor->header();\n+  }\n+  if (has_locker()) {  \/\/ has a stack lock\n+    BasicLock* locker = this->locker();\n+    return locker->displaced_header();\n+  }\n+  \/\/ This should never happen:\n+  fatal(\"bad header=\" INTPTR_FORMAT, value());\n+  return markWord(value());\n+}\n+\n+void markWord::set_displaced_mark_helper(markWord m) const {\n+  assert(has_displaced_mark_helper(), \"check\");\n+  if (has_monitor()) {\n+    \/\/ Has an inflated monitor. Must be checked before has_locker().\n+    ObjectMonitor* monitor = this->monitor();\n+    monitor->set_header(m);\n+    return;\n+  }\n+  if (has_locker()) {  \/\/ has a stack lock\n+    BasicLock* locker = this->locker();\n+    locker->set_displaced_header(m);\n+    return;\n+  }\n+  \/\/ This should never happen:\n+  fatal(\"bad header=\" INTPTR_FORMAT, value());\n+}\n+\n","filename":"src\/hotspot\/share\/oops\/markWord.cpp","additions":34,"deletions":1,"binary":false,"changes":35,"status":"modified"},{"patch":"@@ -289,10 +289,2 @@\n-  markWord displaced_mark_helper() const {\n-    assert(has_displaced_mark_helper(), \"check\");\n-    uintptr_t ptr = (value() & ~monitor_value);\n-    return *(markWord*)ptr;\n-  }\n-  void set_displaced_mark_helper(markWord m) const {\n-    assert(has_displaced_mark_helper(), \"check\");\n-    uintptr_t ptr = (value() & ~monitor_value);\n-    ((markWord*)ptr)->_value = m._value;\n-  }\n+  markWord displaced_mark_helper() const;\n+  void set_displaced_mark_helper(markWord m) const;\n","filename":"src\/hotspot\/share\/oops\/markWord.hpp","additions":2,"deletions":10,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -734,0 +734,14 @@\n+  \/* notice: the max range value here is max_jint, not max_intx  *\/         \\\n+  \/* because of overflow issue                                   *\/         \\\n+  product(intx, AvgMonitorsPerThreadEstimate, 1024, DIAGNOSTIC,             \\\n+          \"Used to estimate a variable ceiling based on number of threads \" \\\n+          \"for use with MonitorUsedDeflationThreshold (0 is off).\")         \\\n+          range(0, max_jint)                                                \\\n+                                                                            \\\n+  \/* notice: the max range value here is max_jint, not max_intx  *\/         \\\n+  \/* because of overflow issue                                   *\/         \\\n+  product(intx, MonitorDeflationMax, 1000000, DIAGNOSTIC,                   \\\n+          \"The maximum number of monitors to deflate, unlink and delete \"   \\\n+          \"at one time (minimum is 1024).\")                      \\\n+          range(1024, max_jint)                                             \\\n+                                                                            \\\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":14,"deletions":0,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -0,0 +1,98 @@\n+\/*\n+ * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"classfile\/javaClasses.hpp\"\n+#include \"runtime\/interfaceSupport.inline.hpp\"\n+#include \"runtime\/java.hpp\"\n+#include \"runtime\/javaCalls.hpp\"\n+#include \"runtime\/monitorDeflationThread.hpp\"\n+#include \"runtime\/mutexLocker.hpp\"\n+\n+MonitorDeflationThread* MonitorDeflationThread::_instance = NULL;\n+\n+void MonitorDeflationThread::initialize() {\n+  EXCEPTION_MARK;\n+\n+  const char* name = \"Monitor Deflation Thread\";\n+  Handle string = java_lang_String::create_from_str(name, CHECK);\n+\n+  \/\/ Initialize thread_oop to put it into the system threadGroup\n+  Handle thread_group (THREAD, Universe::system_thread_group());\n+  Handle thread_oop = JavaCalls::construct_new_instance(\n+                          SystemDictionary::Thread_klass(),\n+                          vmSymbols::threadgroup_string_void_signature(),\n+                          thread_group,\n+                          string,\n+                          CHECK);\n+\n+  {\n+    MutexLocker mu(THREAD, Threads_lock);\n+    MonitorDeflationThread* thread =  new MonitorDeflationThread(&monitor_deflation_thread_entry);\n+\n+    \/\/ At this point it may be possible that no osthread was created for the\n+    \/\/ JavaThread due to lack of memory. We would have to throw an exception\n+    \/\/ in that case. However, since this must work and we do not allow\n+    \/\/ exceptions anyway, check and abort if this fails.\n+    if (thread == NULL || thread->osthread() == NULL) {\n+      vm_exit_during_initialization(\"java.lang.OutOfMemoryError\",\n+                                    os::native_thread_creation_failed_msg());\n+    }\n+\n+    java_lang_Thread::set_thread(thread_oop(), thread);\n+    java_lang_Thread::set_priority(thread_oop(), NearMaxPriority);\n+    java_lang_Thread::set_daemon(thread_oop());\n+    thread->set_threadObj(thread_oop());\n+    _instance = thread;\n+\n+    Threads::add(thread);\n+    Thread::start(thread);\n+  }\n+}\n+\n+void MonitorDeflationThread::monitor_deflation_thread_entry(JavaThread* jt, TRAPS) {\n+  while (true) {\n+    {\n+      \/\/ Need state transition ThreadBlockInVM so that this thread\n+      \/\/ will be handled by safepoint correctly when this thread is\n+      \/\/ notified at a safepoint.\n+\n+      \/\/ This ThreadBlockInVM object is not also considered to be\n+      \/\/ suspend-equivalent because MonitorDeflationThread is not\n+      \/\/ visible to external suspension.\n+\n+      ThreadBlockInVM tbivm(jt);\n+\n+      MonitorLocker ml(MonitorDeflation_lock, Mutex::_no_safepoint_check_flag);\n+      while (!ObjectSynchronizer::is_async_deflation_needed()) {\n+        \/\/ Wait until notified that there is some work to do.\n+        \/\/ We wait for GuaranteedSafepointInterval so that\n+        \/\/ is_async_deflation_needed() is checked at the same interval.\n+        ml.wait(GuaranteedSafepointInterval);\n+      }\n+    }\n+\n+    (void)ObjectSynchronizer::deflate_idle_monitors();\n+  }\n+}\n","filename":"src\/hotspot\/share\/runtime\/monitorDeflationThread.cpp","additions":98,"deletions":0,"binary":false,"changes":98,"status":"added"},{"patch":"@@ -0,0 +1,48 @@\n+\/*\n+ * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_RUNTIME_MONITORDEFLATIONTHREAD_HPP\n+#define SHARE_RUNTIME_MONITORDEFLATIONTHREAD_HPP\n+\n+#include \"runtime\/thread.hpp\"\n+\n+\/\/ A hidden from external view JavaThread for deflating idle monitors.\n+\n+class MonitorDeflationThread : public JavaThread {\n+  friend class VMStructs;\n+ private:\n+  static MonitorDeflationThread* _instance;\n+\n+  static void monitor_deflation_thread_entry(JavaThread* thread, TRAPS);\n+  MonitorDeflationThread(ThreadFunction entry_point) : JavaThread(entry_point) {};\n+\n+ public:\n+  static void initialize();\n+\n+  \/\/ Hide this thread from external view.\n+  bool is_hidden_from_external_view() const { return true; }\n+  bool is_monitor_deflation_thread() const { return true; }\n+};\n+\n+#endif \/\/ SHARE_RUNTIME_MONITORDEFLATIONTHREAD_HPP\n","filename":"src\/hotspot\/share\/runtime\/monitorDeflationThread.hpp","additions":48,"deletions":0,"binary":false,"changes":48,"status":"added"},{"patch":"@@ -119,0 +119,1 @@\n+Monitor* MonitorDeflation_lock        = NULL;\n@@ -247,0 +248,1 @@\n+  def(MonitorDeflation_lock        , PaddedMonitor, tty-2,       true,  _safepoint_check_never);      \/\/ used for monitor deflation thread operations\n","filename":"src\/hotspot\/share\/runtime\/mutexLocker.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -114,0 +114,1 @@\n+extern Monitor* MonitorDeflation_lock;           \/\/ a lock used for monitor deflation thread operation\n","filename":"src\/hotspot\/share\/runtime\/mutexLocker.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -268,0 +268,23 @@\n+ObjectMonitor::ObjectMonitor(oop object) :\n+  _header(markWord::zero()),\n+  _object(_oop_storage, object),\n+  _owner(NULL),\n+  _previous_owner_tid(0),\n+  _next_om(NULL),\n+  _recursions(0),\n+  _EntryList(NULL),\n+  _cxq(NULL),\n+  _succ(NULL),\n+  _Responsible(NULL),\n+  _Spinner(0),\n+  _SpinDuration(ObjectMonitor::Knob_SpinLimit),\n+  _contentions(0),\n+  _WaitSet(NULL),\n+  _waiters(0),\n+  _WaitSetLock(0)\n+{ }\n+\n+ObjectMonitor::~ObjectMonitor() {\n+  _object.release(_oop_storage);\n+}\n+\n@@ -283,9 +306,0 @@\n-void ObjectMonitor::set_object(oop obj) {\n-  check_object_context();\n-  if (_object.is_null()) {\n-    _object = WeakHandle(_oop_storage, obj);\n-  } else {\n-    _object.replace(obj);\n-  }\n-}\n-\n@@ -366,1 +380,4 @@\n-    event.set_address((uintptr_t)object_addr());\n+    \/\/ Set an address that is 'unique enough', such that events close in\n+    \/\/ time and with the same address are likely (but not guaranteed) to\n+    \/\/ belong to the same object.\n+    event.set_address((uintptr_t)this);\n@@ -478,0 +495,104 @@\n+\/\/ Deflate the specified ObjectMonitor if not in-use. Returns true if it\n+\/\/ was deflated and false otherwise.\n+\/\/\n+\/\/ The async deflation protocol sets owner to DEFLATER_MARKER and\n+\/\/ makes contentions negative as signals to contending threads that\n+\/\/ an async deflation is in progress. There are a number of checks\n+\/\/ as part of the protocol to make sure that the calling thread has\n+\/\/ not lost the race to a contending thread.\n+\/\/\n+\/\/ The ObjectMonitor has been successfully async deflated when:\n+\/\/   (contentions < 0)\n+\/\/ Contending threads that see that condition know to retry their operation.\n+\/\/\n+bool ObjectMonitor::deflate_monitor() {\n+  if (is_busy()) {\n+    \/\/ Easy checks are first - the ObjectMonitor is busy so no deflation.\n+    return false;\n+  }\n+\n+  if (ObjectSynchronizer::is_final_audit() && owner_is_DEFLATER_MARKER()) {\n+    \/\/ The final audit can see an already deflated ObjectMonitor on the\n+    \/\/ in-use list because MonitorList::unlink_deflated() might have\n+    \/\/ blocked for the final safepoint before unlinking all the deflated\n+    \/\/ monitors.\n+    assert(contentions() < 0, \"must be negative: contentions=%d\", contentions());\n+    \/\/ Already returned 'true' when it was originally deflated.\n+    return false;\n+  }\n+\n+  const oop obj = object_peek();\n+\n+  if (obj == NULL) {\n+    \/\/ If the object died, we can recycle the monitor without racing with\n+    \/\/ Java threads. The GC already broke the association with the object.\n+    set_owner_from(NULL, DEFLATER_MARKER);\n+    assert(contentions() >= 0, \"must be non-negative: contentions=%d\", contentions());\n+    _contentions = -max_jint;\n+  } else {\n+    \/\/ Attempt async deflation protocol.\n+\n+    \/\/ Set a NULL owner to DEFLATER_MARKER to force any contending thread\n+    \/\/ through the slow path. This is just the first part of the async\n+    \/\/ deflation dance.\n+    if (try_set_owner_from(NULL, DEFLATER_MARKER) != NULL) {\n+      \/\/ The owner field is no longer NULL so we lost the race since the\n+      \/\/ ObjectMonitor is now busy.\n+      return false;\n+    }\n+\n+    if (contentions() > 0 || _waiters != 0) {\n+      \/\/ Another thread has raced to enter the ObjectMonitor after\n+      \/\/ is_busy() above or has already entered and waited on\n+      \/\/ it which makes it busy so no deflation. Restore owner to\n+      \/\/ NULL if it is still DEFLATER_MARKER.\n+      if (try_set_owner_from(DEFLATER_MARKER, NULL) != DEFLATER_MARKER) {\n+        \/\/ Deferred decrement for the JT EnterI() that cancelled the async deflation.\n+        add_to_contentions(-1);\n+      }\n+      return false;\n+    }\n+\n+    \/\/ Make a zero contentions field negative to force any contending threads\n+    \/\/ to retry. This is the second part of the async deflation dance.\n+    if (Atomic::cmpxchg(&_contentions, (jint)0, -max_jint) != 0) {\n+      \/\/ Contentions was no longer 0 so we lost the race since the\n+      \/\/ ObjectMonitor is now busy. Restore owner to NULL if it is\n+      \/\/ still DEFLATER_MARKER:\n+      if (try_set_owner_from(DEFLATER_MARKER, NULL) != DEFLATER_MARKER) {\n+        \/\/ Deferred decrement for the JT EnterI() that cancelled the async deflation.\n+        add_to_contentions(-1);\n+      }\n+      return false;\n+    }\n+  }\n+\n+  \/\/ Sanity checks for the races:\n+  guarantee(owner_is_DEFLATER_MARKER(), \"must be deflater marker\");\n+  guarantee(contentions() < 0, \"must be negative: contentions=%d\",\n+            contentions());\n+  guarantee(_waiters == 0, \"must be 0: waiters=%d\", _waiters);\n+  guarantee(_cxq == NULL, \"must be no contending threads: cxq=\"\n+            INTPTR_FORMAT, p2i(_cxq));\n+  guarantee(_EntryList == NULL,\n+            \"must be no entering threads: EntryList=\" INTPTR_FORMAT,\n+            p2i(_EntryList));\n+\n+  if (obj != NULL) {\n+    if (log_is_enabled(Trace, monitorinflation)) {\n+      ResourceMark rm;\n+      log_trace(monitorinflation)(\"deflate_monitor: object=\" INTPTR_FORMAT\n+                                  \", mark=\" INTPTR_FORMAT \", type='%s'\",\n+                                  p2i(obj), obj->mark().value(),\n+                                  obj->klass()->external_name());\n+    }\n+\n+    \/\/ Install the old mark word if nobody else has already done it.\n+    install_displaced_markword_in_object(obj);\n+  }\n+\n+  \/\/ We leave owner == DEFLATER_MARKER and contentions < 0\n+  \/\/ to force any racing threads to retry.\n+  return true;  \/\/ Success, ObjectMonitor has been deflated.\n+}\n+\n@@ -1354,1 +1475,4 @@\n-  event->set_address((uintptr_t)monitor->object_addr());\n+  \/\/ Set an address that is 'unique enough', such that events close in\n+  \/\/ time and with the same address are likely (but not guaranteed) to\n+  \/\/ belong to the same object.\n+  event->set_address((uintptr_t)monitor);\n@@ -2127,1 +2251,0 @@\n-\/\/   _allocation_state = Old\n@@ -2158,11 +2281,0 @@\n-  st->print(\"  _allocation_state = \");\n-  if (is_free()) {\n-    st->print(\"Free\");\n-  } else if (is_old()) {\n-    st->print(\"Old\");\n-  } else if (is_new()) {\n-    st->print(\"New\");\n-  } else {\n-    st->print(\"unknown=%d\", _allocation_state);\n-  }\n-  st->cr();\n","filename":"src\/hotspot\/share\/runtime\/objectMonitor.cpp","additions":135,"deletions":23,"binary":false,"changes":158,"status":"modified"},{"patch":"@@ -130,1 +130,1 @@\n-class ObjectMonitor {\n+class ObjectMonitor : public CHeapObj<mtInternal> {\n@@ -142,7 +142,0 @@\n-  typedef enum {\n-    Free = 0,  \/\/ Free must be 0 for monitor to be free after memset(..,0,..).\n-    New,\n-    Old,\n-    ChainMarker\n-  } AllocationState;\n-  AllocationState _allocation_state;\n@@ -150,4 +143,3 @@\n-  \/\/ have busy multi-threaded access. _header, _object and _allocation_state\n-  \/\/ are set at initial inflation. _object and _allocation_state don't\n-  \/\/ change until deflation so _object and _allocation_state are good\n-  \/\/ choices to share the cache line with _header.\n+  \/\/ have busy multi-threaded access. _header and _object are set at initial\n+  \/\/ inflation. The _object does not change, so it is a good choice to share\n+  \/\/ its cache line with _header.\n@@ -155,1 +147,1 @@\n-                        sizeof(WeakHandle) + sizeof(AllocationState));\n+                        sizeof(WeakHandle));\n@@ -182,1 +174,1 @@\n-                                    \/\/ ObjectSynchronizer::deflate_monitor().\n+                                    \/\/ ObjectMonitor::deflate_monitor().\n@@ -271,2 +263,0 @@\n-  \/\/ Simply set _owner field to new_value; current value must match old_value1 or old_value2.\n-  void      set_owner_from(void* old_value1, void* old_value2, void* new_value);\n@@ -304,30 +294,2 @@\n- protected:\n-  \/\/ We don't typically expect or want the ctors or dtors to run.\n-  \/\/ normal ObjectMonitors are type-stable and immortal.\n-  ObjectMonitor() { ::memset((void*)this, 0, sizeof(*this)); }\n-\n-  ~ObjectMonitor() {\n-    \/\/ TODO: Add asserts ...\n-    \/\/ _cxq == 0 _succ == NULL _owner == NULL _waiters == 0\n-    \/\/ _contentions == 0 _EntryList  == NULL etc\n-  }\n-\n- private:\n-  void Recycle() {\n-    \/\/ TODO: add stronger asserts ...\n-    \/\/ _cxq == 0 _succ == NULL _owner == NULL _waiters == 0\n-    \/\/ _contentions == 0 EntryList  == NULL\n-    \/\/ _recursions == 0 _WaitSet == NULL\n-#ifdef ASSERT\n-    stringStream ss;\n-    assert((is_busy() | _recursions) == 0, \"freeing in-use monitor: %s, \"\n-           \"recursions=\" INTX_FORMAT, is_busy_to_string(&ss), _recursions);\n-#endif\n-    _succ          = NULL;\n-    _EntryList     = NULL;\n-    _cxq           = NULL;\n-    _WaitSet       = NULL;\n-    _recursions    = 0;\n-  }\n-\n- public:\n+  ObjectMonitor(oop object);\n+  ~ObjectMonitor();\n@@ -337,10 +299,0 @@\n-  oop*      object_addr();\n-  void      set_object(oop obj);\n-  void      release_set_allocation_state(AllocationState s);\n-  void      set_allocation_state(AllocationState s);\n-  AllocationState allocation_state() const;\n-  AllocationState allocation_state_acquire() const;\n-  bool      is_free() const;\n-  bool      is_old() const;\n-  bool      is_new() const;\n-  bool      is_chainmarker() const;\n@@ -351,2 +303,0 @@\n-  void      clear();\n-  void      clear_common();\n@@ -383,0 +333,3 @@\n+\n+  \/\/ Deflation support\n+  bool      deflate_monitor();\n","filename":"src\/hotspot\/share\/runtime\/objectMonitor.hpp","additions":11,"deletions":58,"binary":false,"changes":69,"status":"modified"},{"patch":"@@ -45,1 +45,0 @@\n-  assert((intptr_t)this == (intptr_t)&_header, \"sync code expects this\");\n@@ -75,31 +74,0 @@\n-inline void ObjectMonitor::clear() {\n-  assert(Atomic::load(&_header).value() != 0, \"must be non-zero\");\n-  assert(_owner == NULL, \"must be NULL: owner=\" INTPTR_FORMAT, p2i(_owner));\n-\n-  Atomic::store(&_header, markWord::zero());\n-\n-  clear_common();\n-}\n-\n-inline void ObjectMonitor::clear_common() {\n-  \/\/ Async deflation protocol uses the header, owner and contentions\n-  \/\/ fields. While the ObjectMonitor being deflated is on the global\n-  \/\/ free list, we leave those three fields alone; contentions < 0\n-  \/\/ will force any racing threads to retry. The header field is used\n-  \/\/ by install_displaced_markword_in_object() to restore the object's\n-  \/\/ header so we cannot check its value here.\n-  guarantee(_owner == NULL || _owner == DEFLATER_MARKER,\n-            \"must be NULL or DEFLATER_MARKER: owner=\" INTPTR_FORMAT,\n-            p2i(_owner));\n-  assert(contentions() <= 0, \"must not be positive: contentions=%d\", contentions());\n-  assert(_waiters == 0, \"must be 0: waiters=%d\", _waiters);\n-  assert(_recursions == 0, \"must be 0: recursions=\" INTX_FORMAT, _recursions);\n-\n-  set_allocation_state(Free);\n-  set_object(NULL);\n-}\n-\n-inline oop* ObjectMonitor::object_addr() {\n-  return (oop*)&_object;\n-}\n-\n@@ -144,17 +112,0 @@\n-\/\/ Simply set _owner field to new_value; current value must match old_value1 or old_value2.\n-\/\/ (Simple means no memory sync needed.)\n-inline void ObjectMonitor::set_owner_from(void* old_value1, void* old_value2, void* new_value) {\n-  void* prev = Atomic::load(&_owner);\n-  assert(prev == old_value1 || prev == old_value2,\n-         \"unexpected prev owner=\" INTPTR_FORMAT \", expected1=\"\n-         INTPTR_FORMAT \" or expected2=\" INTPTR_FORMAT, p2i(prev),\n-         p2i(old_value1), p2i(old_value2));\n-  _owner = new_value;\n-  log_trace(monitorinflation, owner)(\"set_owner_from(old1=\" INTPTR_FORMAT\n-                                     \", old2=\" INTPTR_FORMAT \"): mid=\"\n-                                     INTPTR_FORMAT \", prev=\" INTPTR_FORMAT\n-                                     \", new=\" INTPTR_FORMAT, p2i(old_value1),\n-                                     p2i(old_value2), p2i(this), p2i(prev),\n-                                     p2i(new_value));\n-}\n-\n@@ -191,32 +142,0 @@\n-inline void ObjectMonitor::release_set_allocation_state(ObjectMonitor::AllocationState s) {\n-  Atomic::release_store(&_allocation_state, s);\n-}\n-\n-inline void ObjectMonitor::set_allocation_state(ObjectMonitor::AllocationState s) {\n-  _allocation_state = s;\n-}\n-\n-inline ObjectMonitor::AllocationState ObjectMonitor::allocation_state() const {\n-  return _allocation_state;\n-}\n-\n-inline ObjectMonitor::AllocationState ObjectMonitor::allocation_state_acquire() const {\n-  return Atomic::load_acquire(&_allocation_state);\n-}\n-\n-inline bool ObjectMonitor::is_free() const {\n-  return _allocation_state == Free;\n-}\n-\n-inline bool ObjectMonitor::is_old() const {\n-  return allocation_state_acquire() == Old;\n-}\n-\n-inline bool ObjectMonitor::is_new() const {\n-  return _allocation_state == New;\n-}\n-\n-inline bool ObjectMonitor::is_chainmarker() const {\n-  return _allocation_state == ChainMarker;\n-}\n-\n","filename":"src\/hotspot\/share\/runtime\/objectMonitor.inline.hpp","additions":0,"deletions":81,"binary":false,"changes":81,"status":"modified"},{"patch":"@@ -547,5 +547,0 @@\n-    if (_subtasks.try_claim_task(SafepointSynchronize::SAFEPOINT_CLEANUP_DEFLATE_MONITORS)) {\n-      Tracer t(\"deflating idle monitors\");\n-      ObjectSynchronizer::do_safepoint_work();\n-    }\n-\n@@ -613,0 +608,6 @@\n+\n+  if (log_is_enabled(Debug, monitorinflation)) {\n+    \/\/ The VMThread calls do_final_audit_and_print_stats() which calls\n+    \/\/ audit_and_print_stats() at the Info level at VM exit time.\n+    ObjectSynchronizer::audit_and_print_stats(false \/* on_exit *\/);\n+  }\n","filename":"src\/hotspot\/share\/runtime\/safepoint.cpp","additions":6,"deletions":5,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -74,1 +74,0 @@\n-    SAFEPOINT_CLEANUP_DEFLATE_MONITORS,\n","filename":"src\/hotspot\/share\/runtime\/safepoint.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -146,1 +146,0 @@\n-    bool deflate_idle_monitors = false;\n@@ -177,2 +176,1 @@\n-              (cldg_cleanup_work = ClassLoaderDataGraph::should_clean_metaspaces_and_reset()) |\n-              (deflate_idle_monitors = ObjectSynchronizer::is_async_deflation_needed())\n+              (cldg_cleanup_work = ClassLoaderDataGraph::should_clean_metaspaces_and_reset())\n@@ -181,3 +179,1 @@\n-        \/\/ We wait for GuaranteedSafepointInterval so that\n-        \/\/ is_async_deflation_needed() is checked at the same interval.\n-        ml.wait(GuaranteedSafepointInterval);\n+        ml.wait();\n@@ -236,4 +232,0 @@\n-    if (deflate_idle_monitors) {\n-      ObjectSynchronizer::deflate_idle_monitors();\n-    }\n-\n","filename":"src\/hotspot\/share\/runtime\/serviceThread.cpp","additions":2,"deletions":10,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -27,2 +27,0 @@\n-#include \"logging\/log.hpp\"\n-#include \"logging\/logStream.hpp\"\n@@ -30,0 +28,1 @@\n+#include \"logging\/log.hpp\"\n@@ -60,0 +59,99 @@\n+void MonitorList::add(ObjectMonitor* m) {\n+  ObjectMonitor* head;\n+  do {\n+    head = Atomic::load(&_head);\n+    m->set_next_om(head);\n+  } while (Atomic::cmpxchg(&_head, head, m) != head);\n+\n+  size_t count = Atomic::add(&_count, 1u);\n+  if (count > max()) {\n+    Atomic::inc(&_max);\n+  }\n+}\n+\n+size_t MonitorList::count() const {\n+  return Atomic::load(&_count);\n+}\n+\n+size_t MonitorList::max() const {\n+  return Atomic::load(&_max);\n+}\n+\n+\/\/ Walk the in-use list and unlink (at most MonitorDeflationMax) deflated\n+\/\/ ObjectMonitors. Returns the number of unlinked ObjectMonitors.\n+size_t MonitorList::unlink_deflated(Thread* self, LogStream* ls,\n+                                    elapsedTimer* timer_p,\n+                                    GrowableArray<ObjectMonitor*>* unlinked_list) {\n+  size_t unlinked_count = 0;\n+  ObjectMonitor* prev = NULL;\n+  ObjectMonitor* head = Atomic::load_acquire(&_head);\n+  ObjectMonitor* m = head;\n+  do {\n+    if (m->is_being_async_deflated()) {\n+      \/\/ Find next live ObjectMonitor.\n+      ObjectMonitor* next = m;\n+      do {\n+        ObjectMonitor* next_next = next->next_om();\n+        unlinked_count++;\n+        unlinked_list->append(next);\n+        next = next_next;\n+        if (unlinked_count >= (size_t)MonitorDeflationMax) {\n+          \/\/ Reached the max so bail out on the gathering loop.\n+          break;\n+        }\n+      } while (next != NULL && next->is_being_async_deflated());\n+      if (prev == NULL) {\n+        ObjectMonitor* prev_head = Atomic::cmpxchg(&_head, head, next);\n+        if (prev_head != head) {\n+          \/\/ Find new prev ObjectMonitor that just got inserted.\n+          for (ObjectMonitor* n = prev_head; n != m; n = n->next_om()) {\n+            prev = n;\n+          }\n+          prev->set_next_om(next);\n+        }\n+      } else {\n+        prev->set_next_om(next);\n+      }\n+      if (unlinked_count >= (size_t)MonitorDeflationMax) {\n+        \/\/ Reached the max so bail out on the searching loop.\n+        break;\n+      }\n+      m = next;\n+    } else {\n+      prev = m;\n+      m = m->next_om();\n+    }\n+\n+    if (self->is_Java_thread() &&\n+        SafepointMechanism::should_process(self->as_Java_thread())) {\n+      \/\/ A safepoint\/handshake has started.\n+      if (ls != NULL) {\n+        timer_p->stop();\n+        ls->print_cr(\"pausing unlinking: unlinked_count=\" SIZE_FORMAT \", in_use_list stats: ceiling=\" SIZE_FORMAT \", count=\" SIZE_FORMAT \", max=\" SIZE_FORMAT,\n+                     unlinked_count, ObjectSynchronizer::in_use_list_ceiling(), count(), max());\n+      }\n+\n+      \/\/ Honor block request.\n+      ThreadBlockInVM tbivm(self->as_Java_thread());\n+\n+      if (ls != NULL) {\n+        ls->print_cr(\"resuming unlinking: in_use_list stats: ceiling=\" SIZE_FORMAT \", count=\" SIZE_FORMAT \", max=\" SIZE_FORMAT,\n+                     ObjectSynchronizer::in_use_list_ceiling(), count(), max());\n+        timer_p->start();\n+      }\n+    }\n+  } while (m != NULL);\n+  Atomic::sub(&_count, unlinked_count);\n+  return unlinked_count;\n+}\n+\n+MonitorList::Iterator MonitorList::iterator() const {\n+  return Iterator(Atomic::load_acquire(&_head));\n+}\n+\n+ObjectMonitor* MonitorList::Iterator::next() {\n+  ObjectMonitor* current = _current;\n+  _current = current->next_om();\n+  return current;\n+}\n+\n@@ -77,1 +175,1 @@\n-  Symbol* klassname = ((oop)(obj))->klass()->name();                       \\\n+  Symbol* klassname = obj->klass()->name();                                \\\n@@ -121,2 +219,3 @@\n-\/\/ global list of blocks of monitors\n-PaddedObjectMonitor* ObjectSynchronizer::g_block_list = NULL;\n+MonitorList ObjectSynchronizer::_in_use_list;\n+\/\/ Start the ceiling with the estimate for one thread:\n+jint ObjectSynchronizer::_in_use_list_ceiling = AvgMonitorsPerThreadEstimate;\n@@ -127,300 +226,0 @@\n-struct ObjectMonitorListGlobals {\n-  char         _pad_prefix[OM_CACHE_LINE_SIZE];\n-  \/\/ These are highly shared list related variables.\n-  \/\/ To avoid false-sharing they need to be the sole occupants of a cache line.\n-\n-  \/\/ Global ObjectMonitor free list. Newly allocated and deflated\n-  \/\/ ObjectMonitors are prepended here.\n-  ObjectMonitor* _free_list;\n-  DEFINE_PAD_MINUS_SIZE(1, OM_CACHE_LINE_SIZE, sizeof(ObjectMonitor*));\n-\n-  \/\/ Global ObjectMonitor in-use list. When a JavaThread is exiting,\n-  \/\/ ObjectMonitors on its per-thread in-use list are prepended here.\n-  ObjectMonitor* _in_use_list;\n-  DEFINE_PAD_MINUS_SIZE(2, OM_CACHE_LINE_SIZE, sizeof(ObjectMonitor*));\n-\n-  \/\/ Global ObjectMonitor wait list. Deflated ObjectMonitors wait on\n-  \/\/ this list until after a handshake or a safepoint for platforms\n-  \/\/ that don't support handshakes. After the handshake or safepoint,\n-  \/\/ the deflated ObjectMonitors are prepended to free_list.\n-  ObjectMonitor* _wait_list;\n-  DEFINE_PAD_MINUS_SIZE(3, OM_CACHE_LINE_SIZE, sizeof(ObjectMonitor*));\n-\n-  int _free_count;    \/\/ # on free_list\n-  DEFINE_PAD_MINUS_SIZE(4, OM_CACHE_LINE_SIZE, sizeof(int));\n-\n-  int _in_use_count;  \/\/ # on in_use_list\n-  DEFINE_PAD_MINUS_SIZE(5, OM_CACHE_LINE_SIZE, sizeof(int));\n-\n-  int _population;    \/\/ # Extant -- in circulation\n-  DEFINE_PAD_MINUS_SIZE(6, OM_CACHE_LINE_SIZE, sizeof(int));\n-\n-  int _wait_count;    \/\/ # on wait_list\n-  DEFINE_PAD_MINUS_SIZE(7, OM_CACHE_LINE_SIZE, sizeof(int));\n-};\n-static ObjectMonitorListGlobals om_list_globals;\n-\n-\n-\/\/ =====================> Spin-lock functions\n-\n-\/\/ ObjectMonitors are not lockable outside of this file. We use spin-locks\n-\/\/ implemented using a bit in the _next_om field instead of the heavier\n-\/\/ weight locking mechanisms for faster list management.\n-\n-#define OM_LOCK_BIT 0x1\n-\n-\/\/ Return true if the ObjectMonitor is locked.\n-\/\/ Otherwise returns false.\n-static bool is_locked(ObjectMonitor* om) {\n-  return ((intptr_t)om->next_om_acquire() & OM_LOCK_BIT) == OM_LOCK_BIT;\n-}\n-\n-\/\/ Mark an ObjectMonitor* with OM_LOCK_BIT and return it.\n-static ObjectMonitor* mark_om_ptr(ObjectMonitor* om) {\n-  return (ObjectMonitor*)((intptr_t)om | OM_LOCK_BIT);\n-}\n-\n-\/\/ Return the unmarked next field in an ObjectMonitor. Note: the next\n-\/\/ field may or may not have been marked with OM_LOCK_BIT originally.\n-static ObjectMonitor* unmarked_next(ObjectMonitor* om) {\n-  return (ObjectMonitor*)((intptr_t)om->next_om() & ~OM_LOCK_BIT);\n-}\n-\n-\/\/ Try to lock an ObjectMonitor. Returns true if locking was successful.\n-\/\/ Otherwise returns false.\n-static bool try_om_lock(ObjectMonitor* om) {\n-  \/\/ Get current next field without any OM_LOCK_BIT value.\n-  ObjectMonitor* next = unmarked_next(om);\n-  if (om->try_set_next_om(next, mark_om_ptr(next)) != next) {\n-    return false;  \/\/ Cannot lock the ObjectMonitor.\n-  }\n-  return true;\n-}\n-\n-\/\/ Lock an ObjectMonitor.\n-static void om_lock(ObjectMonitor* om) {\n-  while (true) {\n-    if (try_om_lock(om)) {\n-      return;\n-    }\n-  }\n-}\n-\n-\/\/ Unlock an ObjectMonitor.\n-static void om_unlock(ObjectMonitor* om) {\n-  ObjectMonitor* next = om->next_om();\n-  guarantee(((intptr_t)next & OM_LOCK_BIT) == OM_LOCK_BIT, \"next=\" INTPTR_FORMAT\n-            \" must have OM_LOCK_BIT=%x set.\", p2i(next), OM_LOCK_BIT);\n-\n-  next = (ObjectMonitor*)((intptr_t)next & ~OM_LOCK_BIT);  \/\/ Clear OM_LOCK_BIT.\n-  om->release_set_next_om(next);\n-}\n-\n-\/\/ Get the list head after locking it. Returns the list head or NULL\n-\/\/ if the list is empty.\n-static ObjectMonitor* get_list_head_locked(ObjectMonitor** list_p) {\n-  while (true) {\n-    \/\/ Acquire semantics not needed on this list load since we're\n-    \/\/ checking for NULL here or following up with a cmpxchg() via\n-    \/\/ try_om_lock() below and we retry on cmpxchg() failure.\n-    ObjectMonitor* mid = Atomic::load(list_p);\n-    if (mid == NULL) {\n-      return NULL;  \/\/ The list is empty.\n-    }\n-    if (try_om_lock(mid)) {\n-      \/\/ Acquire semantics not needed on this list load since memory is\n-      \/\/ already consistent due to the cmpxchg() via try_om_lock() above.\n-      if (Atomic::load(list_p) != mid) {\n-        \/\/ The list head changed before we could lock it so we have to retry.\n-        om_unlock(mid);\n-        continue;\n-      }\n-      return mid;\n-    }\n-  }\n-}\n-\n-#undef OM_LOCK_BIT\n-\n-\n-\/\/ =====================> List Management functions\n-\n-\/\/ Prepend a list of ObjectMonitors to the specified *list_p. 'tail' is\n-\/\/ the last ObjectMonitor in the list and there are 'count' on the list.\n-\/\/ Also updates the specified *count_p.\n-static void prepend_list_to_common(ObjectMonitor* list, ObjectMonitor* tail,\n-                                   int count, ObjectMonitor** list_p,\n-                                   int* count_p) {\n-  while (true) {\n-    \/\/ Acquire semantics not needed on this list load since we're\n-    \/\/ following up with a cmpxchg() via try_om_lock() below and we\n-    \/\/ retry on cmpxchg() failure.\n-    ObjectMonitor* cur = Atomic::load(list_p);\n-    \/\/ Prepend list to *list_p.\n-    if (!try_om_lock(tail)) {\n-      \/\/ Failed to lock tail due to a list walker so try it all again.\n-      continue;\n-    }\n-    \/\/ Release semantics not needed on this \"unlock\" since memory is\n-    \/\/ already consistent due to the cmpxchg() via try_om_lock() above.\n-    tail->set_next_om(cur);  \/\/ tail now points to cur (and unlocks tail)\n-    if (cur == NULL) {\n-      \/\/ No potential race with takers or other prependers since\n-      \/\/ *list_p is empty.\n-      if (Atomic::cmpxchg(list_p, cur, list) == cur) {\n-        \/\/ Successfully switched *list_p to the list value.\n-        Atomic::add(count_p, count);\n-        break;\n-      }\n-      \/\/ Implied else: try it all again\n-    } else {\n-      if (!try_om_lock(cur)) {\n-        continue;  \/\/ failed to lock cur so try it all again\n-      }\n-      \/\/ We locked cur so try to switch *list_p to the list value.\n-      if (Atomic::cmpxchg(list_p, cur, list) != cur) {\n-        \/\/ The list head has changed so unlock cur and try again:\n-        om_unlock(cur);\n-        continue;\n-      }\n-      Atomic::add(count_p, count);\n-      om_unlock(cur);\n-      break;\n-    }\n-  }\n-}\n-\n-\/\/ Prepend a newly allocated block of ObjectMonitors to g_block_list and\n-\/\/ om_list_globals._free_list. Also updates om_list_globals._population\n-\/\/ and om_list_globals._free_count.\n-void ObjectSynchronizer::prepend_block_to_lists(PaddedObjectMonitor* new_blk) {\n-  \/\/ First we handle g_block_list:\n-  while (true) {\n-    PaddedObjectMonitor* cur = Atomic::load(&g_block_list);\n-    \/\/ Prepend new_blk to g_block_list. The first ObjectMonitor in\n-    \/\/ a block is reserved for use as linkage to the next block.\n-    new_blk[0].set_next_om(cur);\n-    if (Atomic::cmpxchg(&g_block_list, cur, new_blk) == cur) {\n-      \/\/ Successfully switched g_block_list to the new_blk value.\n-      Atomic::add(&om_list_globals._population, _BLOCKSIZE - 1);\n-      break;\n-    }\n-    \/\/ Implied else: try it all again\n-  }\n-\n-  \/\/ Second we handle om_list_globals._free_list:\n-  prepend_list_to_common(new_blk + 1, &new_blk[_BLOCKSIZE - 1], _BLOCKSIZE - 1,\n-                         &om_list_globals._free_list, &om_list_globals._free_count);\n-}\n-\n-\/\/ Prepend a list of ObjectMonitors to om_list_globals._free_list.\n-\/\/ 'tail' is the last ObjectMonitor in the list and there are 'count'\n-\/\/ on the list. Also updates om_list_globals._free_count.\n-static void prepend_list_to_global_free_list(ObjectMonitor* list,\n-                                             ObjectMonitor* tail, int count) {\n-  prepend_list_to_common(list, tail, count, &om_list_globals._free_list,\n-                         &om_list_globals._free_count);\n-}\n-\n-\/\/ Prepend a list of ObjectMonitors to om_list_globals._wait_list.\n-\/\/ 'tail' is the last ObjectMonitor in the list and there are 'count'\n-\/\/ on the list. Also updates om_list_globals._wait_count.\n-static void prepend_list_to_global_wait_list(ObjectMonitor* list,\n-                                             ObjectMonitor* tail, int count) {\n-  prepend_list_to_common(list, tail, count, &om_list_globals._wait_list,\n-                         &om_list_globals._wait_count);\n-}\n-\n-\/\/ Prepend a list of ObjectMonitors to om_list_globals._in_use_list.\n-\/\/ 'tail' is the last ObjectMonitor in the list and there are 'count'\n-\/\/ on the list. Also updates om_list_globals._in_use_list.\n-static void prepend_list_to_global_in_use_list(ObjectMonitor* list,\n-                                               ObjectMonitor* tail, int count) {\n-  prepend_list_to_common(list, tail, count, &om_list_globals._in_use_list,\n-                         &om_list_globals._in_use_count);\n-}\n-\n-\/\/ Prepend an ObjectMonitor to the specified list. Also updates\n-\/\/ the specified counter.\n-static void prepend_to_common(ObjectMonitor* m, ObjectMonitor** list_p,\n-                              int* count_p) {\n-  while (true) {\n-    om_lock(m);  \/\/ Lock m so we can safely update its next field.\n-    ObjectMonitor* cur = NULL;\n-    \/\/ Lock the list head to guard against races with a list walker\n-    \/\/ or async deflater thread (which only races in om_in_use_list):\n-    if ((cur = get_list_head_locked(list_p)) != NULL) {\n-      \/\/ List head is now locked so we can safely switch it. Release\n-      \/\/ semantics not needed on this \"unlock\" since memory is already\n-      \/\/ consistent due to the cmpxchg() via get_list_head_locked() above.\n-      m->set_next_om(cur);  \/\/ m now points to cur (and unlocks m)\n-      OrderAccess::storestore();  \/\/ Make sure set_next_om() is seen first.\n-      Atomic::store(list_p, m);  \/\/ Switch list head to unlocked m.\n-      om_unlock(cur);\n-      break;\n-    }\n-    \/\/ The list is empty so try to set the list head.\n-    assert(cur == NULL, \"cur must be NULL: cur=\" INTPTR_FORMAT, p2i(cur));\n-    \/\/ Release semantics not needed on this \"unlock\" since memory\n-    \/\/ is already consistent.\n-    m->set_next_om(cur);  \/\/ m now points to NULL (and unlocks m)\n-    if (Atomic::cmpxchg(list_p, cur, m) == cur) {\n-      \/\/ List head is now unlocked m.\n-      break;\n-    }\n-    \/\/ Implied else: try it all again\n-  }\n-  Atomic::inc(count_p);\n-}\n-\n-\/\/ Prepend an ObjectMonitor to a per-thread om_free_list.\n-\/\/ Also updates the per-thread om_free_count.\n-static void prepend_to_om_free_list(Thread* self, ObjectMonitor* m) {\n-  prepend_to_common(m, &self->om_free_list, &self->om_free_count);\n-}\n-\n-\/\/ Prepend an ObjectMonitor to a per-thread om_in_use_list.\n-\/\/ Also updates the per-thread om_in_use_count.\n-static void prepend_to_om_in_use_list(Thread* self, ObjectMonitor* m) {\n-  prepend_to_common(m, &self->om_in_use_list, &self->om_in_use_count);\n-}\n-\n-\/\/ Take an ObjectMonitor from the start of the specified list. Also\n-\/\/ decrements the specified counter. Returns NULL if none are available.\n-static ObjectMonitor* take_from_start_of_common(ObjectMonitor** list_p,\n-                                                int* count_p) {\n-  ObjectMonitor* take = NULL;\n-  \/\/ Lock the list head to guard against races with a list walker\n-  \/\/ or async deflater thread (which only races in om_list_globals._free_list):\n-  if ((take = get_list_head_locked(list_p)) == NULL) {\n-    return NULL;  \/\/ None are available.\n-  }\n-  ObjectMonitor* next = unmarked_next(take);\n-  \/\/ Switch locked list head to next (which unlocks the list head, but\n-  \/\/ leaves take locked). Release semantics not needed on this \"unlock\"\n-  \/\/ since memory is already consistent due to the cmpxchg() via\n-  \/\/ get_list_head_locked() above.\n-  Atomic::store(list_p, next);\n-  Atomic::dec(count_p);\n-  \/\/ Unlock take, but leave the next value for any lagging list\n-  \/\/ walkers. It will get cleaned up when take is prepended to\n-  \/\/ the in-use list:\n-  om_unlock(take);\n-  return take;\n-}\n-\n-\/\/ Take an ObjectMonitor from the start of the om_list_globals._free_list.\n-\/\/ Also updates om_list_globals._free_count. Returns NULL if none are\n-\/\/ available.\n-static ObjectMonitor* take_from_start_of_global_free_list() {\n-  return take_from_start_of_common(&om_list_globals._free_list,\n-                                   &om_list_globals._free_count);\n-}\n-\n-\/\/ Take an ObjectMonitor from the start of a per-thread free-list.\n-\/\/ Also updates om_free_count. Returns NULL if none are available.\n-static ObjectMonitor* take_from_start_of_om_free_list(Thread* self) {\n-  return take_from_start_of_common(&self->om_free_list, &self->om_free_count);\n-}\n-\n-\n@@ -1311,16 +1110,13 @@\n-  PaddedObjectMonitor* block = Atomic::load(&g_block_list);\n-  while (block != NULL) {\n-    assert(block->is_chainmarker(), \"must be a block header\");\n-    for (int i = _BLOCKSIZE - 1; i > 0; i--) {\n-      ObjectMonitor* mid = (ObjectMonitor *)(block + i);\n-      if (mid->object_peek() != NULL) {\n-        \/\/ Only process with closure if the object is set.\n-\n-        \/\/ monitors_iterate() is only called at a safepoint or when the\n-        \/\/ target thread is suspended or when the target thread is\n-        \/\/ operating on itself. The current closures in use today are\n-        \/\/ only interested in an owned ObjectMonitor and ownership\n-        \/\/ cannot be dropped under the calling contexts so the\n-        \/\/ ObjectMonitor cannot be async deflated.\n-        closure->do_monitor(mid);\n-      }\n+  MonitorList::Iterator iter = _in_use_list.iterator();\n+  while (iter.has_next()) {\n+    ObjectMonitor* mid = iter.next();\n+    if (!mid->is_being_async_deflated() && mid->object_peek() != NULL) {\n+      \/\/ Only process with closure if the object is set.\n+\n+      \/\/ monitors_iterate() is only called at a safepoint or when the\n+      \/\/ target thread is suspended or when the target thread is\n+      \/\/ operating on itself. The current closures in use today are\n+      \/\/ only interested in an owned ObjectMonitor and ownership\n+      \/\/ cannot be dropped under the calling contexts so the\n+      \/\/ ObjectMonitor cannot be async deflated.\n+      closure->do_monitor(mid);\n@@ -1328,3 +1124,0 @@\n-    \/\/ unmarked_next() is not needed with g_block_list (no locking\n-    \/\/ used with block linkage _next_om fields).\n-    block = (PaddedObjectMonitor*)block->next_om();\n@@ -1334,3 +1127,8 @@\n-static bool monitors_used_above_threshold() {\n-  int population = Atomic::load(&om_list_globals._population);\n-  if (population == 0) {\n+static bool monitors_used_above_threshold(MonitorList* list) {\n+  \/\/ Start with ceiling based on a per-thread estimate:\n+  size_t ceiling = ObjectSynchronizer::in_use_list_ceiling();\n+  if (ceiling < list->max()) {\n+    \/\/ The max used by the system has exceeded the ceiling so use that:\n+    ceiling = list->max();\n+  }\n+  if (ceiling == 0) {\n@@ -1340,4 +1138,3 @@\n-    int monitors_used = population - Atomic::load(&om_list_globals._free_count) -\n-                        Atomic::load(&om_list_globals._wait_count);\n-    int monitor_usage = (monitors_used * 100LL) \/ population;\n-    return monitor_usage > MonitorUsedDeflationThreshold;\n+    size_t monitors_used = list->count();\n+    size_t monitor_usage = (monitors_used * 100LL) \/ ceiling;\n+    return int(monitor_usage) > MonitorUsedDeflationThreshold;\n@@ -1348,0 +1145,13 @@\n+void ObjectSynchronizer::dec_in_use_list_ceiling() {\n+  Atomic::add(&_in_use_list_ceiling, (jint)-AvgMonitorsPerThreadEstimate);\n+#ifdef ASSERT\n+  size_t l_in_use_list_ceiling = in_use_list_ceiling();\n+#endif\n+  assert(l_in_use_list_ceiling > 0, \"in_use_list_ceiling=\" SIZE_FORMAT\n+         \": must be > 0\", l_in_use_list_ceiling);\n+}\n+\n+void ObjectSynchronizer::inc_in_use_list_ceiling() {\n+  Atomic::add(&_in_use_list_ceiling, (jint)AvgMonitorsPerThreadEstimate);\n+}\n+\n@@ -1355,1 +1165,1 @@\n-      monitors_used_above_threshold()) {\n+      monitors_used_above_threshold(&_in_use_list)) {\n@@ -1359,1 +1169,1 @@\n-    \/\/ in order to not swamp the ServiceThread.\n+    \/\/ in order to not swamp the MonitorDeflationThread.\n@@ -1372,1 +1182,1 @@\n-    MonitorLocker ml(Service_lock, Mutex::_no_safepoint_check_flag);\n+    MonitorLocker ml(MonitorDeflation_lock, Mutex::_no_safepoint_check_flag);\n@@ -1401,404 +1211,0 @@\n-\n-\/\/ -----------------------------------------------------------------------------\n-\/\/ ObjectMonitor Lifecycle\n-\/\/ -----------------------\n-\/\/ Inflation unlinks monitors from om_list_globals._free_list or a per-thread\n-\/\/ free list and associates them with objects. Async deflation disassociates\n-\/\/ idle monitors from objects. Such scavenged monitors are returned to the\n-\/\/ om_list_globals._free_list.\n-\/\/\n-\/\/ ObjectMonitors reside in type-stable memory (TSM) and are immortal.\n-\/\/\n-\/\/ Lifecycle:\n-\/\/ --   unassigned and on the om_list_globals._free_list\n-\/\/ --   unassigned and on a per-thread free list\n-\/\/ --   assigned to an object.  The object is inflated and the mark refers\n-\/\/      to the ObjectMonitor.\n-\n-ObjectMonitor* ObjectSynchronizer::om_alloc(Thread* self) {\n-  \/\/ A large MAXPRIVATE value reduces both list lock contention\n-  \/\/ and list coherency traffic, but also tends to increase the\n-  \/\/ number of ObjectMonitors in circulation as well as the\n-  \/\/ scavenge costs.  As usual, we lean toward time in space-time\n-  \/\/ tradeoffs.\n-  const int MAXPRIVATE = 1024;\n-  NoSafepointVerifier nsv;\n-\n-  for (;;) {\n-    ObjectMonitor* m;\n-\n-    \/\/ 1: try to allocate from the thread's local om_free_list.\n-    \/\/ Threads will attempt to allocate first from their local list, then\n-    \/\/ from the global list, and only after those attempts fail will the\n-    \/\/ thread attempt to instantiate new monitors. Thread-local free lists\n-    \/\/ improve allocation latency, as well as reducing coherency traffic\n-    \/\/ on the shared global list.\n-    m = take_from_start_of_om_free_list(self);\n-    if (m != NULL) {\n-      guarantee(m->object_peek() == NULL, \"invariant\");\n-      m->set_allocation_state(ObjectMonitor::New);\n-      prepend_to_om_in_use_list(self, m);\n-      return m;\n-    }\n-\n-    \/\/ 2: try to allocate from the global om_list_globals._free_list\n-    \/\/ If we're using thread-local free lists then try\n-    \/\/ to reprovision the caller's free list.\n-    \/\/ Acquire semantics not needed on this list load since memory\n-    \/\/ is already consistent due to the cmpxchg() via\n-    \/\/ take_from_start_of_om_free_list() above.\n-    if (Atomic::load(&om_list_globals._free_list) != NULL) {\n-      \/\/ Reprovision the thread's om_free_list.\n-      \/\/ Use bulk transfers to reduce the allocation rate and heat\n-      \/\/ on various locks.\n-      for (int i = self->om_free_provision; --i >= 0;) {\n-        ObjectMonitor* take = take_from_start_of_global_free_list();\n-        if (take == NULL) {\n-          break;  \/\/ No more are available.\n-        }\n-        guarantee(take->object_peek() == NULL, \"invariant\");\n-        \/\/ We allowed 3 field values to linger during async deflation.\n-        \/\/ Clear or restore them as appropriate.\n-        take->set_header(markWord::zero());\n-        \/\/ DEFLATER_MARKER is the only non-NULL value we should see here.\n-        take->try_set_owner_from(DEFLATER_MARKER, NULL);\n-        if (take->contentions() < 0) {\n-          \/\/ Add back max_jint to restore the contentions field to its\n-          \/\/ proper value.\n-          take->add_to_contentions(max_jint);\n-\n-#ifdef ASSERT\n-          jint l_contentions = take->contentions();\n-          assert(l_contentions >= 0, \"must not be negative: l_contentions=%d, contentions=%d\",\n-                 l_contentions, take->contentions());\n-#endif\n-        }\n-        take->Recycle();\n-        \/\/ Since we're taking from the global free-list, take must be Free.\n-        \/\/ om_release() also sets the allocation state to Free because it\n-        \/\/ is called from other code paths.\n-        assert(take->is_free(), \"invariant\");\n-        om_release(self, take, false);\n-      }\n-      self->om_free_provision += 1 + (self->om_free_provision \/ 2);\n-      if (self->om_free_provision > MAXPRIVATE) self->om_free_provision = MAXPRIVATE;\n-      continue;\n-    }\n-\n-    \/\/ 3: allocate a block of new ObjectMonitors\n-    \/\/ Both the local and global free lists are empty -- resort to malloc().\n-    \/\/ In the current implementation ObjectMonitors are TSM - immortal.\n-    \/\/ Ideally, we'd write \"new ObjectMonitor[_BLOCKSIZE], but we want\n-    \/\/ each ObjectMonitor to start at the beginning of a cache line,\n-    \/\/ so we use align_up().\n-    \/\/ A better solution would be to use C++ placement-new.\n-    \/\/ BEWARE: As it stands currently, we don't run the ctors!\n-    assert(_BLOCKSIZE > 1, \"invariant\");\n-    size_t neededsize = sizeof(PaddedObjectMonitor) * _BLOCKSIZE;\n-    PaddedObjectMonitor* temp;\n-    size_t aligned_size = neededsize + (OM_CACHE_LINE_SIZE - 1);\n-    void* real_malloc_addr = NEW_C_HEAP_ARRAY(char, aligned_size, mtInternal);\n-    temp = (PaddedObjectMonitor*)align_up(real_malloc_addr, OM_CACHE_LINE_SIZE);\n-    (void)memset((void *) temp, 0, neededsize);\n-\n-    \/\/ Format the block.\n-    \/\/ initialize the linked list, each monitor points to its next\n-    \/\/ forming the single linked free list, the very first monitor\n-    \/\/ will points to next block, which forms the block list.\n-    \/\/ The trick of using the 1st element in the block as g_block_list\n-    \/\/ linkage should be reconsidered.  A better implementation would\n-    \/\/ look like: class Block { Block * next; int N; ObjectMonitor Body [N] ; }\n-\n-    for (int i = 1; i < _BLOCKSIZE; i++) {\n-      temp[i].set_next_om((ObjectMonitor*)&temp[i + 1]);\n-      assert(temp[i].is_free(), \"invariant\");\n-    }\n-\n-    \/\/ terminate the last monitor as the end of list\n-    temp[_BLOCKSIZE - 1].set_next_om((ObjectMonitor*)NULL);\n-\n-    \/\/ Element [0] is reserved for global list linkage\n-    temp[0].set_allocation_state(ObjectMonitor::ChainMarker);\n-\n-    \/\/ Consider carving out this thread's current request from the\n-    \/\/ block in hand.  This avoids some lock traffic and redundant\n-    \/\/ list activity.\n-\n-    prepend_block_to_lists(temp);\n-  }\n-}\n-\n-\/\/ Place \"m\" on the caller's private per-thread om_free_list.\n-\/\/ In practice there's no need to clamp or limit the number of\n-\/\/ monitors on a thread's om_free_list as the only non-allocation time\n-\/\/ we'll call om_release() is to return a monitor to the free list after\n-\/\/ a CAS attempt failed. This doesn't allow unbounded #s of monitors to\n-\/\/ accumulate on a thread's free list.\n-\/\/\n-\/\/ Key constraint: all ObjectMonitors on a thread's free list and the global\n-\/\/ free list must have their object field set to null. This prevents the\n-\/\/ scavenger -- deflate_monitor_list() -- from reclaiming them\n-\/\/ while we are trying to release them.\n-\n-void ObjectSynchronizer::om_release(Thread* self, ObjectMonitor* m,\n-                                    bool from_per_thread_alloc) {\n-  guarantee(m->header().value() == 0, \"invariant\");\n-  guarantee(m->object_peek() == NULL, \"invariant\");\n-  NoSafepointVerifier nsv;\n-\n-  if ((m->is_busy() | m->_recursions) != 0) {\n-    stringStream ss;\n-    fatal(\"freeing in-use monitor: %s, recursions=\" INTX_FORMAT,\n-          m->is_busy_to_string(&ss), m->_recursions);\n-  }\n-  m->set_allocation_state(ObjectMonitor::Free);\n-  \/\/ _next_om is used for both per-thread in-use and free lists so\n-  \/\/ we have to remove 'm' from the in-use list first (as needed).\n-  if (from_per_thread_alloc) {\n-    \/\/ Need to remove 'm' from om_in_use_list.\n-    ObjectMonitor* mid = NULL;\n-    ObjectMonitor* next = NULL;\n-\n-    \/\/ This list walk can race with another list walker or with async\n-    \/\/ deflation so we have to worry about an ObjectMonitor being\n-    \/\/ removed from this list while we are walking it.\n-\n-    \/\/ Lock the list head to avoid racing with another list walker\n-    \/\/ or with async deflation.\n-    if ((mid = get_list_head_locked(&self->om_in_use_list)) == NULL) {\n-      fatal(\"thread=\" INTPTR_FORMAT \" in-use list must not be empty.\", p2i(self));\n-    }\n-    next = unmarked_next(mid);\n-    if (m == mid) {\n-      \/\/ First special case:\n-      \/\/ 'm' matches mid, is the list head and is locked. Switch the list\n-      \/\/ head to next which unlocks the list head, but leaves the extracted\n-      \/\/ mid locked. Release semantics not needed on this \"unlock\" since\n-      \/\/ memory is already consistent due to the get_list_head_locked()\n-      \/\/ above.\n-      Atomic::store(&self->om_in_use_list, next);\n-    } else if (m == next) {\n-      \/\/ Second special case:\n-      \/\/ 'm' matches next after the list head and we already have the list\n-      \/\/ head locked so set mid to what we are extracting:\n-      mid = next;\n-      \/\/ Lock mid to prevent races with a list walker or an async\n-      \/\/ deflater thread that's ahead of us. The locked list head\n-      \/\/ prevents races from behind us.\n-      om_lock(mid);\n-      \/\/ Update next to what follows mid (if anything):\n-      next = unmarked_next(mid);\n-      \/\/ Switch next after the list head to new next which unlocks the\n-      \/\/ list head, but leaves the extracted mid locked. Release semantics\n-      \/\/ not needed on this \"unlock\" since memory is already consistent\n-      \/\/ due to the get_list_head_locked() above.\n-      self->om_in_use_list->set_next_om(next);\n-    } else {\n-      \/\/ We have to search the list to find 'm'.\n-      guarantee(next != NULL, \"thread=\" INTPTR_FORMAT \": om_in_use_list=\" INTPTR_FORMAT\n-                \" is too short.\", p2i(self), p2i(self->om_in_use_list));\n-      \/\/ Our starting anchor is next after the list head which is the\n-      \/\/ last ObjectMonitor we checked:\n-      ObjectMonitor* anchor = next;\n-      \/\/ Lock anchor to prevent races with a list walker or an async\n-      \/\/ deflater thread that's ahead of us. The locked list head\n-      \/\/ prevents races from behind us.\n-      om_lock(anchor);\n-      om_unlock(mid);  \/\/ Unlock the list head now that anchor is locked.\n-      while ((mid = unmarked_next(anchor)) != NULL) {\n-        if (m == mid) {\n-          \/\/ We found 'm' on the per-thread in-use list so extract it.\n-          \/\/ Update next to what follows mid (if anything):\n-          next = unmarked_next(mid);\n-          \/\/ Switch next after the anchor to new next which unlocks the\n-          \/\/ anchor, but leaves the extracted mid locked. Release semantics\n-          \/\/ not needed on this \"unlock\" since memory is already consistent\n-          \/\/ due to the om_unlock() above before entering the loop or the\n-          \/\/ om_unlock() below before looping again.\n-          anchor->set_next_om(next);\n-          break;\n-        } else {\n-          \/\/ Lock the next anchor to prevent races with a list walker\n-          \/\/ or an async deflater thread that's ahead of us. The locked\n-          \/\/ current anchor prevents races from behind us.\n-          om_lock(mid);\n-          \/\/ Unlock current anchor now that next anchor is locked:\n-          om_unlock(anchor);\n-          anchor = mid;  \/\/ Advance to new anchor and try again.\n-        }\n-      }\n-    }\n-\n-    if (mid == NULL) {\n-      \/\/ Reached end of the list and didn't find 'm' so:\n-      fatal(\"thread=\" INTPTR_FORMAT \" must find m=\" INTPTR_FORMAT \"on om_in_use_list=\"\n-            INTPTR_FORMAT, p2i(self), p2i(m), p2i(self->om_in_use_list));\n-    }\n-\n-    \/\/ At this point mid is disconnected from the in-use list so\n-    \/\/ its lock no longer has any effects on the in-use list.\n-    Atomic::dec(&self->om_in_use_count);\n-    \/\/ Unlock mid, but leave the next value for any lagging list\n-    \/\/ walkers. It will get cleaned up when mid is prepended to\n-    \/\/ the thread's free list:\n-    om_unlock(mid);\n-  }\n-\n-  prepend_to_om_free_list(self, m);\n-  guarantee(m->is_free(), \"invariant\");\n-}\n-\n-\/\/ Return ObjectMonitors on a moribund thread's free and in-use\n-\/\/ lists to the appropriate global lists. The ObjectMonitors on the\n-\/\/ per-thread in-use list may still be in use by other threads.\n-\/\/\n-\/\/ We currently call om_flush() from Threads::remove() before the\n-\/\/ thread has been excised from the thread list and is no longer a\n-\/\/ mutator.\n-\/\/\n-\/\/ deflate_global_idle_monitors() and deflate_per_thread_idle_monitors()\n-\/\/ (in another thread) can run at the same time as om_flush() so we have\n-\/\/ to follow a careful protocol to prevent list corruption.\n-\n-void ObjectSynchronizer::om_flush(Thread* self) {\n-  \/\/ Process the per-thread in-use list first to be consistent.\n-  int in_use_count = 0;\n-  ObjectMonitor* in_use_list = NULL;\n-  ObjectMonitor* in_use_tail = NULL;\n-  NoSafepointVerifier nsv;\n-\n-  \/\/ This function can race with a list walker or with an async\n-  \/\/ deflater thread so we lock the list head to prevent confusion.\n-  \/\/ An async deflater thread checks to see if the target thread\n-  \/\/ is exiting, but if it has made it past that check before we\n-  \/\/ started exiting, then it is racing to get to the in-use list.\n-  if ((in_use_list = get_list_head_locked(&self->om_in_use_list)) != NULL) {\n-    \/\/ At this point, we have locked the in-use list head so a racing\n-    \/\/ thread cannot come in after us. However, a racing thread could\n-    \/\/ be ahead of us; we'll detect that and delay to let it finish.\n-    \/\/\n-    \/\/ The thread is going away, however the ObjectMonitors on the\n-    \/\/ om_in_use_list may still be in-use by other threads. Link\n-    \/\/ them to in_use_tail, which will be linked into the global\n-    \/\/ in-use list (om_list_globals._in_use_list) below.\n-    \/\/\n-    \/\/ Account for the in-use list head before the loop since it is\n-    \/\/ already locked (by this thread):\n-    in_use_tail = in_use_list;\n-    in_use_count++;\n-    for (ObjectMonitor* cur_om = unmarked_next(in_use_list); cur_om != NULL;) {\n-      if (is_locked(cur_om)) {\n-        \/\/ cur_om is locked so there must be a racing walker or async\n-        \/\/ deflater thread ahead of us so we'll give it a chance to finish.\n-        while (is_locked(cur_om)) {\n-          os::naked_short_sleep(1);\n-        }\n-        \/\/ Refetch the possibly changed next field and try again.\n-        cur_om = unmarked_next(in_use_tail);\n-        continue;\n-      }\n-      if (cur_om->object_peek() == NULL) {\n-        \/\/ Two reasons for object() to be NULL here:\n-        \/\/ 1) cur_om was deflated and the object ref was cleared while it\n-        \/\/ was locked. We happened to see it just after it was unlocked\n-        \/\/ (and added to the free list).\n-        \/\/ 2) The object has been GC'ed so the association with object is\n-        \/\/ already broken, but we don't want to do the deflation work now.\n-\n-        \/\/ Refetch the possibly changed next field:\n-        ObjectMonitor* in_use_next = unmarked_next(in_use_tail);\n-        if (cur_om != in_use_next) {\n-          \/\/ The NULL is because of async deflation so try again:\n-          cur_om = in_use_next;\n-          continue;\n-        }\n-        \/\/ Implied else: The NULL is because of GC, but we leave the\n-        \/\/ node on the in-use list to be deflated after it has been\n-        \/\/ moved to the global in-use list.\n-      }\n-      in_use_tail = cur_om;\n-      in_use_count++;\n-      cur_om = unmarked_next(cur_om);\n-    }\n-    guarantee(in_use_tail != NULL, \"invariant\");\n-#ifdef ASSERT\n-    int l_om_in_use_count = Atomic::load(&self->om_in_use_count);\n-    assert(l_om_in_use_count == in_use_count, \"in-use counts don't match: \"\n-           \"l_om_in_use_count=%d, in_use_count=%d\", l_om_in_use_count, in_use_count);\n-#endif\n-    Atomic::store(&self->om_in_use_count, 0);\n-    OrderAccess::storestore();  \/\/ Make sure counter update is seen first.\n-    \/\/ Clear the in-use list head (which also unlocks it):\n-    Atomic::store(&self->om_in_use_list, (ObjectMonitor*)NULL);\n-    om_unlock(in_use_list);\n-  }\n-\n-  int free_count = 0;\n-  ObjectMonitor* free_list = NULL;\n-  ObjectMonitor* free_tail = NULL;\n-  \/\/ This function can race with a list walker thread so we lock the\n-  \/\/ list head to prevent confusion.\n-  if ((free_list = get_list_head_locked(&self->om_free_list)) != NULL) {\n-    \/\/ At this point, we have locked the free list head so a racing\n-    \/\/ thread cannot come in after us. However, a racing thread could\n-    \/\/ be ahead of us; we'll detect that and delay to let it finish.\n-    \/\/\n-    \/\/ The thread is going away. Set 'free_tail' to the last per-thread free\n-    \/\/ monitor which will be linked to om_list_globals._free_list below.\n-    \/\/\n-    \/\/ Account for the free list head before the loop since it is\n-    \/\/ already locked (by this thread):\n-    free_tail = free_list;\n-    free_count++;\n-    for (ObjectMonitor* s = unmarked_next(free_list); s != NULL; s = unmarked_next(s)) {\n-      if (is_locked(s)) {\n-        \/\/ s is locked so there must be a racing walker thread ahead\n-        \/\/ of us so we'll give it a chance to finish.\n-        while (is_locked(s)) {\n-          os::naked_short_sleep(1);\n-        }\n-      }\n-      free_tail = s;\n-      free_count++;\n-      guarantee(s->object_peek() == NULL, \"invariant\");\n-      if (s->is_busy()) {\n-        stringStream ss;\n-        fatal(\"must be !is_busy: %s\", s->is_busy_to_string(&ss));\n-      }\n-    }\n-    guarantee(free_tail != NULL, \"invariant\");\n-#ifdef ASSERT\n-    int l_om_free_count = Atomic::load(&self->om_free_count);\n-    assert(l_om_free_count == free_count, \"free counts don't match: \"\n-           \"l_om_free_count=%d, free_count=%d\", l_om_free_count, free_count);\n-#endif\n-    Atomic::store(&self->om_free_count, 0);\n-    OrderAccess::storestore();  \/\/ Make sure counter update is seen first.\n-    Atomic::store(&self->om_free_list, (ObjectMonitor*)NULL);\n-    om_unlock(free_list);\n-  }\n-\n-  if (free_tail != NULL) {\n-    prepend_list_to_global_free_list(free_list, free_tail, free_count);\n-  }\n-\n-  if (in_use_tail != NULL) {\n-    prepend_list_to_global_in_use_list(in_use_list, in_use_tail, in_use_count);\n-  }\n-\n-  LogStreamHandle(Debug, monitorinflation) lsh_debug;\n-  LogStreamHandle(Info, monitorinflation) lsh_info;\n-  LogStream* ls = NULL;\n-  if (log_is_enabled(Debug, monitorinflation)) {\n-    ls = &lsh_debug;\n-  } else if ((free_count != 0 || in_use_count != 0) &&\n-             log_is_enabled(Info, monitorinflation)) {\n-    ls = &lsh_info;\n-  }\n-  if (ls != NULL) {\n-    ls->print_cr(\"om_flush: jt=\" INTPTR_FORMAT \", free_count=%d\"\n-                 \", in_use_count=%d\" \", om_free_provision=%d\",\n-                 p2i(self), free_count, in_use_count, self->om_free_provision);\n-  }\n-}\n-\n@@ -1821,1 +1227,0 @@\n-    assert(ObjectSynchronizer::verify_objmon_isinpool(monitor), \"monitor=\" INTPTR_FORMAT \" is invalid\", p2i(monitor));\n@@ -1849,1 +1254,0 @@\n-      assert(ObjectSynchronizer::verify_objmon_isinpool(inf), \"monitor is invalid\");\n@@ -1873,9 +1277,0 @@\n-    \/\/\n-    \/\/ We now use per-thread private ObjectMonitor free lists.\n-    \/\/ These list are reprovisioned from the global free list outside the\n-    \/\/ critical INFLATING...ST interval.  A thread can transfer\n-    \/\/ multiple ObjectMonitors en-mass from the global free list to its local free list.\n-    \/\/ This reduces coherency traffic and lock contention on the global free list.\n-    \/\/ Using such local free lists, it doesn't matter if the om_alloc() call appears\n-    \/\/ before or after the CAS(INFLATING) operation.\n-    \/\/ See the comments in om_alloc().\n@@ -1886,1 +1281,1 @@\n-      ObjectMonitor* m = om_alloc(self);\n+      ObjectMonitor* m = new ObjectMonitor(object);\n@@ -1890,3 +1285,0 @@\n-      m->Recycle();\n-      m->_Responsible  = NULL;\n-      m->_SpinDuration = ObjectMonitor::Knob_SpinLimit;   \/\/ Consider: maintain by type\/class\n@@ -1896,2 +1288,1 @@\n-        \/\/ om_release() will reset the allocation state from New to Free.\n-        om_release(self, m, true);\n+        delete m;\n@@ -1944,2 +1335,1 @@\n-      m->set_owner_from(NULL, DEFLATER_MARKER, mark.locker());\n-      m->set_object(object);\n+      m->set_owner_from(NULL, mark.locker());\n@@ -1956,3 +1346,1 @@\n-      assert(m->is_new(), \"freshly allocated monitor must be new\");\n-      \/\/ Release semantics needed to keep allocation_state from floating up.\n-      m->release_set_allocation_state(ObjectMonitor::Old);\n+      _in_use_list.add(m);\n@@ -1987,1 +1375,1 @@\n-    ObjectMonitor* m = om_alloc(self);\n+    ObjectMonitor* m = new ObjectMonitor(object);\n@@ -1989,1 +1377,0 @@\n-    m->Recycle();\n@@ -1991,5 +1378,0 @@\n-    \/\/ DEFLATER_MARKER is the only non-NULL value we should see here.\n-    m->try_set_owner_from(DEFLATER_MARKER, NULL);\n-    m->set_object(object);\n-    m->_Responsible  = NULL;\n-    m->_SpinDuration = ObjectMonitor::Knob_SpinLimit;       \/\/ consider: keep metastats by type\/class\n@@ -1998,5 +1380,1 @@\n-      m->set_header(markWord::zero());\n-      m->set_object(NULL);\n-      m->Recycle();\n-      \/\/ om_release() will reset the allocation state from New to Free.\n-      om_release(self, m, true);\n+      delete m;\n@@ -2012,4 +1390,1 @@\n-    assert(m->is_new(), \"freshly allocated monitor must be new\");\n-    \/\/ Release semantics are not needed to keep allocation_state from\n-    \/\/ floating up since cas_set_mark() takes care of it.\n-    m->set_allocation_state(ObjectMonitor::Old);\n+    _in_use_list.add(m);\n@@ -2033,0 +1408,6 @@\n+\/\/ Walk the in-use list and deflate (at most MonitorDeflationMax) idle\n+\/\/ ObjectMonitors. Returns the number of deflated ObjectMonitors.\n+size_t ObjectSynchronizer::deflate_monitor_list(Thread *self, LogStream* ls,\n+                                                elapsedTimer* timer_p) {\n+  MonitorList::Iterator iter = _in_use_list.iterator();\n+  size_t deflated_count = 0;\n@@ -2034,59 +1415,3 @@\n-\/\/ An async deflation request is registered with the ServiceThread\n-\/\/ and it is notified.\n-void ObjectSynchronizer::do_safepoint_work() {\n-  assert(SafepointSynchronize::is_at_safepoint(), \"must be at safepoint\");\n-\n-  log_debug(monitorinflation)(\"requesting async deflation of idle monitors.\");\n-  \/\/ Request deflation of idle monitors by the ServiceThread:\n-  set_is_async_deflation_requested(true);\n-  MonitorLocker ml(Service_lock, Mutex::_no_safepoint_check_flag);\n-  ml.notify_all();\n-\n-  if (log_is_enabled(Debug, monitorinflation)) {\n-    \/\/ The VMThread calls do_final_audit_and_print_stats() which calls\n-    \/\/ audit_and_print_stats() at the Info level at VM exit time.\n-    ObjectSynchronizer::audit_and_print_stats(false \/* on_exit *\/);\n-  }\n-}\n-\n-\/\/ Deflate the specified ObjectMonitor if not in-use. Returns true if it\n-\/\/ was deflated and false otherwise.\n-\/\/\n-\/\/ The async deflation protocol sets owner to DEFLATER_MARKER and\n-\/\/ makes contentions negative as signals to contending threads that\n-\/\/ an async deflation is in progress. There are a number of checks\n-\/\/ as part of the protocol to make sure that the calling thread has\n-\/\/ not lost the race to a contending thread.\n-\/\/\n-\/\/ The ObjectMonitor has been successfully async deflated when:\n-\/\/   (contentions < 0)\n-\/\/ Contending threads that see that condition know to retry their operation.\n-\/\/\n-bool ObjectSynchronizer::deflate_monitor(ObjectMonitor* mid,\n-                                         ObjectMonitor** free_head_p,\n-                                         ObjectMonitor** free_tail_p) {\n-  \/\/ A newly allocated ObjectMonitor should not be seen here so we\n-  \/\/ avoid an endless inflate\/deflate cycle.\n-  assert(mid->is_old(), \"must be old: allocation_state=%d\",\n-         (int) mid->allocation_state());\n-\n-  if (mid->is_busy()) {\n-    \/\/ Easy checks are first - the ObjectMonitor is busy so no deflation.\n-    return false;\n-  }\n-\n-  const oop obj = mid->object_peek();\n-\n-  if (obj == NULL) {\n-    \/\/ If the object died, we can recycle the monitor without racing with\n-    \/\/ Java threads. The GC already broke the association with the object.\n-    mid->set_owner_from(NULL, DEFLATER_MARKER);\n-    mid->_contentions = -max_jint;\n-  } else {\n-    \/\/ Set a NULL owner to DEFLATER_MARKER to force any contending thread\n-    \/\/ through the slow path. This is just the first part of the async\n-    \/\/ deflation dance.\n-    if (mid->try_set_owner_from(NULL, DEFLATER_MARKER) != NULL) {\n-      \/\/ The owner field is no longer NULL so we lost the race since the\n-      \/\/ ObjectMonitor is now busy.\n-      return false;\n+  while (iter.has_next()) {\n+    if (deflated_count >= (size_t)MonitorDeflationMax) {\n+      break;\n@@ -2094,11 +1419,3 @@\n-\n-    if (mid->contentions() > 0 || mid->_waiters != 0) {\n-      \/\/ Another thread has raced to enter the ObjectMonitor after\n-      \/\/ mid->is_busy() above or has already entered and waited on\n-      \/\/ it which makes it busy so no deflation. Restore owner to\n-      \/\/ NULL if it is still DEFLATER_MARKER.\n-      if (mid->try_set_owner_from(DEFLATER_MARKER, NULL) != DEFLATER_MARKER) {\n-        \/\/ Deferred decrement for the JT EnterI() that cancelled the async deflation.\n-        mid->add_to_contentions(-1);\n-      }\n-      return false;\n+    ObjectMonitor* mid = iter.next();\n+    if (mid->deflate_monitor()) {\n+      deflated_count++;\n@@ -2107,9 +1424,7 @@\n-    \/\/ Make a zero contentions field negative to force any contending threads\n-    \/\/ to retry. This is the second part of the async deflation dance.\n-    if (Atomic::cmpxchg(&mid->_contentions, (jint)0, -max_jint) != 0) {\n-      \/\/ Contentions was no longer 0 so we lost the race since the\n-      \/\/ ObjectMonitor is now busy. Restore owner to NULL if it is\n-      \/\/ still DEFLATER_MARKER:\n-      if (mid->try_set_owner_from(DEFLATER_MARKER, NULL) != DEFLATER_MARKER) {\n-        \/\/ Deferred decrement for the JT EnterI() that cancelled the async deflation.\n-        mid->add_to_contentions(-1);\n+    if (self->is_Java_thread() &&\n+        SafepointMechanism::should_process(self->as_Java_thread())) {\n+      \/\/ A safepoint\/handshake has started.\n+      if (ls != NULL) {\n+        timer_p->stop();\n+        ls->print_cr(\"pausing deflation: deflated_count=\" SIZE_FORMAT \", in_use_list stats: ceiling=\" SIZE_FORMAT \", count=\" SIZE_FORMAT \", max=\" SIZE_FORMAT,\n+                     deflated_count, in_use_list_ceiling(), _in_use_list.count(), _in_use_list.max());\n@@ -2117,3 +1432,0 @@\n-      return false;\n-    }\n-  }\n@@ -2121,171 +1433,2 @@\n-  \/\/ Sanity checks for the races:\n-  guarantee(mid->owner_is_DEFLATER_MARKER(), \"must be deflater marker\");\n-  guarantee(mid->contentions() < 0, \"must be negative: contentions=%d\",\n-            mid->contentions());\n-  guarantee(mid->_waiters == 0, \"must be 0: waiters=%d\", mid->_waiters);\n-  guarantee(mid->_cxq == NULL, \"must be no contending threads: cxq=\"\n-            INTPTR_FORMAT, p2i(mid->_cxq));\n-  guarantee(mid->_EntryList == NULL,\n-            \"must be no entering threads: EntryList=\" INTPTR_FORMAT,\n-            p2i(mid->_EntryList));\n-\n-  if (obj != NULL) {\n-    if (log_is_enabled(Trace, monitorinflation)) {\n-      ResourceMark rm;\n-      log_trace(monitorinflation)(\"deflate_monitor: object=\" INTPTR_FORMAT\n-                                  \", mark=\" INTPTR_FORMAT \", type='%s'\",\n-                                  p2i(obj), obj->mark().value(),\n-                                  obj->klass()->external_name());\n-    }\n-\n-    \/\/ Install the old mark word if nobody else has already done it.\n-    mid->install_displaced_markword_in_object(obj);\n-  }\n-  mid->clear_common();\n-\n-  assert(mid->object_peek() == NULL, \"must be NULL: object=\" INTPTR_FORMAT,\n-         p2i(mid->object_peek()));\n-  assert(mid->is_free(), \"must be free: allocation_state=%d\",\n-         (int)mid->allocation_state());\n-\n-  \/\/ Move the deflated ObjectMonitor to the working free list\n-  \/\/ defined by free_head_p and free_tail_p.\n-  if (*free_head_p == NULL) {\n-    \/\/ First one on the list.\n-    *free_head_p = mid;\n-  }\n-  if (*free_tail_p != NULL) {\n-    \/\/ We append to the list so the caller can use mid->_next_om\n-    \/\/ to fix the linkages in its context.\n-    ObjectMonitor* prevtail = *free_tail_p;\n-    \/\/ prevtail should have been cleaned up by the caller:\n-#ifdef ASSERT\n-    ObjectMonitor* l_next_om = unmarked_next(prevtail);\n-    assert(l_next_om == NULL, \"must be NULL: _next_om=\" INTPTR_FORMAT, p2i(l_next_om));\n-#endif\n-    om_lock(prevtail);\n-    prevtail->set_next_om(mid);  \/\/ prevtail now points to mid (and is unlocked)\n-  }\n-  *free_tail_p = mid;\n-\n-  \/\/ At this point, mid->_next_om still refers to its current\n-  \/\/ value and another ObjectMonitor's _next_om field still\n-  \/\/ refers to this ObjectMonitor. Those linkages have to be\n-  \/\/ cleaned up by the caller who has the complete context.\n-\n-  \/\/ We leave owner == DEFLATER_MARKER and contentions < 0\n-  \/\/ to force any racing threads to retry.\n-  return true;  \/\/ Success, ObjectMonitor has been deflated.\n-}\n-\n-\/\/ Walk a given ObjectMonitor list and deflate idle ObjectMonitors.\n-\/\/ Returns the number of deflated ObjectMonitors. The given\n-\/\/ list could be a per-thread in-use list or the global in-use list.\n-\/\/ If self is a JavaThread and a safepoint has started, then we save state\n-\/\/ via saved_mid_in_use_p and return to the caller to honor the safepoint.\n-\/\/\n-int ObjectSynchronizer::deflate_monitor_list(Thread* self,\n-                                             ObjectMonitor** list_p,\n-                                             int* count_p,\n-                                             ObjectMonitor** free_head_p,\n-                                             ObjectMonitor** free_tail_p,\n-                                             ObjectMonitor** saved_mid_in_use_p) {\n-  ObjectMonitor* cur_mid_in_use = NULL;\n-  ObjectMonitor* mid = NULL;\n-  ObjectMonitor* next = NULL;\n-  ObjectMonitor* next_next = NULL;\n-  int deflated_count = 0;\n-  NoSafepointVerifier nsv;\n-\n-  \/\/ We use the more complicated lock-cur_mid_in_use-and-mid-as-we-go\n-  \/\/ protocol because om_release() can do list deletions in parallel;\n-  \/\/ this also prevents races with a list walker thread. We also\n-  \/\/ lock-next-next-as-we-go to prevent an om_flush() that is behind\n-  \/\/ this thread from passing us.\n-  if (*saved_mid_in_use_p == NULL) {\n-    \/\/ No saved state so start at the beginning.\n-    \/\/ Lock the list head so we can possibly deflate it:\n-    if ((mid = get_list_head_locked(list_p)) == NULL) {\n-      return 0;  \/\/ The list is empty so nothing to deflate.\n-    }\n-    next = unmarked_next(mid);\n-  } else {\n-    \/\/ We're restarting after a safepoint so restore the necessary state\n-    \/\/ before we resume.\n-    cur_mid_in_use = *saved_mid_in_use_p;\n-    \/\/ Lock cur_mid_in_use so we can possibly update its\n-    \/\/ next field to extract a deflated ObjectMonitor.\n-    om_lock(cur_mid_in_use);\n-    mid = unmarked_next(cur_mid_in_use);\n-    if (mid == NULL) {\n-      om_unlock(cur_mid_in_use);\n-      *saved_mid_in_use_p = NULL;\n-      return 0;  \/\/ The remainder is empty so nothing more to deflate.\n-    }\n-    \/\/ Lock mid so we can possibly deflate it:\n-    om_lock(mid);\n-    next = unmarked_next(mid);\n-  }\n-\n-  while (true) {\n-    \/\/ The current mid is locked at this point. If we have a\n-    \/\/ cur_mid_in_use, then it is also locked at this point.\n-\n-    if (next != NULL) {\n-      \/\/ We lock next so that an om_flush() thread that is behind us\n-      \/\/ cannot pass us when we unlock the current mid.\n-      om_lock(next);\n-      next_next = unmarked_next(next);\n-    }\n-\n-    \/\/ Only try to deflate if mid is old (is not newly allocated and\n-    \/\/ is not newly freed).\n-    if (mid->is_old() && deflate_monitor(mid, free_head_p, free_tail_p)) {\n-      \/\/ Deflation succeeded and already updated free_head_p and\n-      \/\/ free_tail_p as needed. Finish the move to the local free list\n-      \/\/ by unlinking mid from the global or per-thread in-use list.\n-      if (cur_mid_in_use == NULL) {\n-        \/\/ mid is the list head and it is locked. Switch the list head\n-        \/\/ to next which is also locked (if not NULL) and also leave\n-        \/\/ mid locked. Release semantics needed since not all code paths\n-        \/\/ in deflate_monitor() ensure memory consistency.\n-        Atomic::release_store(list_p, next);\n-      } else {\n-        ObjectMonitor* locked_next = mark_om_ptr(next);\n-        \/\/ mid and cur_mid_in_use are locked. Switch cur_mid_in_use's\n-        \/\/ next field to locked_next and also leave mid locked.\n-        \/\/ Release semantics needed since not all code paths in\n-        \/\/ deflate_monitor() ensure memory consistency.\n-        cur_mid_in_use->release_set_next_om(locked_next);\n-      }\n-      \/\/ At this point mid is disconnected from the in-use list so\n-      \/\/ its lock longer has any effects on in-use list.\n-      deflated_count++;\n-      Atomic::dec(count_p);\n-      \/\/ mid is current tail in the free_head_p list so NULL terminate\n-      \/\/ it (which also unlocks it). No release semantics needed since\n-      \/\/ Atomic::dec() already provides it.\n-      mid->set_next_om(NULL);\n-\n-      \/\/ All the list management is done so move on to the next one:\n-      mid = next;  \/\/ mid keeps non-NULL next's locked state\n-      next = next_next;\n-    } else {\n-      \/\/ mid is considered in-use if mid is not old or deflation did not\n-      \/\/ succeed. A mid->is_new() node can be seen here when it is freshly\n-      \/\/ returned by om_alloc() (and skips the deflation code path).\n-      \/\/ A mid->is_old() node can be seen here when deflation failed.\n-      \/\/ A mid->is_free() node can be seen here when a fresh node from\n-      \/\/ om_alloc() is released by om_release() due to losing the race\n-      \/\/ in inflate().\n-\n-      \/\/ All the list management is done so move on to the next one:\n-      if (cur_mid_in_use != NULL) {\n-        om_unlock(cur_mid_in_use);\n-      }\n-      \/\/ The next cur_mid_in_use keeps mid's lock state so\n-      \/\/ that it is stable for a possible next field change. It\n-      \/\/ cannot be modified by om_release() while it is locked.\n-      cur_mid_in_use = mid;\n-      mid = next;  \/\/ mid keeps non-NULL next's locked state\n-      next = next_next;\n+      \/\/ Honor block request.\n+      ThreadBlockInVM tbivm(self->as_Java_thread());\n@@ -2293,20 +1436,4 @@\n-      if (self->is_Java_thread() &&\n-          SafepointMechanism::should_process(self->as_Java_thread()) &&\n-          \/\/ Acquire semantics are not needed on this list load since\n-          \/\/ it is not dependent on the following load which does have\n-          \/\/ acquire semantics.\n-          cur_mid_in_use != Atomic::load(list_p) && cur_mid_in_use->is_old()) {\n-        \/\/ If a safepoint has started and cur_mid_in_use is not the list\n-        \/\/ head and is old, then it is safe to use as saved state. Return\n-        \/\/ to the caller before blocking.\n-        *saved_mid_in_use_p = cur_mid_in_use;\n-        om_unlock(cur_mid_in_use);\n-        if (mid != NULL) {\n-          om_unlock(mid);\n-        }\n-        return deflated_count;\n-      }\n-    }\n-    if (mid == NULL) {\n-      if (cur_mid_in_use != NULL) {\n-        om_unlock(cur_mid_in_use);\n+      if (ls != NULL) {\n+        ls->print_cr(\"resuming deflation: in_use_list stats: ceiling=\" SIZE_FORMAT \", count=\" SIZE_FORMAT \", max=\" SIZE_FORMAT,\n+                     in_use_list_ceiling(), _in_use_list.count(), _in_use_list.max());\n+        timer_p->start();\n@@ -2314,1 +1441,0 @@\n-      break;  \/\/ Reached end of the list so nothing more to deflate.\n@@ -2316,3 +1442,0 @@\n-\n-    \/\/ The current mid's next field is locked at this point. If we have\n-    \/\/ a cur_mid_in_use, then it is also locked at this point.\n@@ -2320,3 +1443,1 @@\n-  \/\/ We finished the list without a safepoint starting so there's\n-  \/\/ no need to save state.\n-  *saved_mid_in_use_p = NULL;\n+\n@@ -2336,3 +1457,4 @@\n-\/\/ This function is called by the ServiceThread to deflate monitors.\n-\/\/ It is also called by do_final_audit_and_print_stats() by the VMThread.\n-void ObjectSynchronizer::deflate_idle_monitors() {\n+\/\/ This function is called by the MonitorDeflationThread to deflate\n+\/\/ ObjectMonitors. It is also called via do_final_audit_and_print_stats()\n+\/\/ by the VMThread.\n+size_t ObjectSynchronizer::deflate_idle_monitors() {\n@@ -2340,26 +1462,0 @@\n-  \/\/ Deflate any global idle monitors.\n-  deflate_global_idle_monitors(self);\n-\n-  int count = 0;\n-  for (JavaThreadIteratorWithHandle jtiwh; JavaThread *jt = jtiwh.next(); ) {\n-    if (Atomic::load(&jt->om_in_use_count) > 0 && !jt->is_exiting()) {\n-      \/\/ This JavaThread is using ObjectMonitors so deflate any that\n-      \/\/ are idle unless this JavaThread is exiting; do not race with\n-      \/\/ ObjectSynchronizer::om_flush().\n-      deflate_per_thread_idle_monitors(self, jt);\n-      count++;\n-    }\n-  }\n-  if (count > 0) {\n-    log_debug(monitorinflation)(\"did async deflation of idle monitors for %d thread(s).\", count);\n-  }\n-\n-  log_info(monitorinflation)(\"async global_population=%d, global_in_use_count=%d, \"\n-                             \"global_free_count=%d, global_wait_count=%d\",\n-                             Atomic::load(&om_list_globals._population),\n-                             Atomic::load(&om_list_globals._in_use_count),\n-                             Atomic::load(&om_list_globals._free_count),\n-                             Atomic::load(&om_list_globals._wait_count));\n-\n-  GVars.stw_random = os::random();\n-\n@@ -2372,3 +1468,8 @@\n-  if (Atomic::load(&om_list_globals._wait_count) > 0) {\n-    \/\/ There are deflated ObjectMonitors waiting for a handshake\n-    \/\/ (or a safepoint) for safety.\n+  LogStreamHandle(Debug, monitorinflation) lsh_debug;\n+  LogStreamHandle(Info, monitorinflation) lsh_info;\n+  LogStream* ls = NULL;\n+  if (log_is_enabled(Debug, monitorinflation)) {\n+    ls = &lsh_debug;\n+  } else if (log_is_enabled(Info, monitorinflation)) {\n+    ls = &lsh_info;\n+  }\n@@ -2376,6 +1477,6 @@\n-    ObjectMonitor* list = Atomic::load(&om_list_globals._wait_list);\n-    assert(list != NULL, \"om_list_globals._wait_list must not be NULL\");\n-    int count = Atomic::load(&om_list_globals._wait_count);\n-    Atomic::store(&om_list_globals._wait_count, 0);\n-    OrderAccess::storestore();  \/\/ Make sure counter update is seen first.\n-    Atomic::store(&om_list_globals._wait_list, (ObjectMonitor*)NULL);\n+  elapsedTimer timer;\n+  if (ls != NULL) {\n+    ls->print_cr(\"begin deflating: in_use_list stats: ceiling=\" SIZE_FORMAT \", count=\" SIZE_FORMAT \", max=\" SIZE_FORMAT,\n+                 in_use_list_ceiling(), _in_use_list.count(), _in_use_list.max());\n+    timer.start();\n+  }\n@@ -2383,14 +1484,7 @@\n-    \/\/ Find the tail for prepend_list_to_common(). No need to mark\n-    \/\/ ObjectMonitors for this list walk since only the deflater\n-    \/\/ thread manages the wait list.\n-#ifdef ASSERT\n-    int l_count = 0;\n-#endif\n-    ObjectMonitor* tail = NULL;\n-    for (ObjectMonitor* n = list; n != NULL; n = unmarked_next(n)) {\n-      tail = n;\n-#ifdef ASSERT\n-      l_count++;\n-#endif\n-    }\n-    assert(count == l_count, \"count=%d != l_count=%d\", count, l_count);\n+  \/\/ Deflate some idle ObjectMonitors.\n+  size_t deflated_count = deflate_monitor_list(self, ls, &timer);\n+  if (deflated_count > 0 || is_final_audit()) {\n+    \/\/ There are ObjectMonitors that have been deflated or this is the\n+    \/\/ final audit and all the remaining ObjectMonitors have been\n+    \/\/ deflated, BUT the MonitorDeflationThread blocked for the final\n+    \/\/ safepoint during unlinking.\n@@ -2398,0 +1492,5 @@\n+    \/\/ Unlink deflated ObjectMonitors from the in-use list.\n+    ResourceMark rm;\n+    GrowableArray<ObjectMonitor*> delete_list((int)deflated_count);\n+    size_t unlinked_count = _in_use_list.unlink_deflated(self, ls, &timer,\n+                                                         &delete_list);\n@@ -2399,0 +1498,9 @@\n+      if (ls != NULL) {\n+        timer.stop();\n+        ls->print_cr(\"before handshaking: unlinked_count=\" SIZE_FORMAT\n+                     \", in_use_list stats: ceiling=\" SIZE_FORMAT \", count=\"\n+                     SIZE_FORMAT \", max=\" SIZE_FORMAT,\n+                     unlinked_count, in_use_list_ceiling(),\n+                     _in_use_list.count(), _in_use_list.max());\n+      }\n+\n@@ -2400,1 +1508,1 @@\n-      \/\/ monitors that were deflated in this cycle.\n+      \/\/ ObjectMonitors that were deflated in this cycle.\n@@ -2403,14 +1511,0 @@\n-   }\n-\n-    prepend_list_to_common(list, tail, count, &om_list_globals._free_list,\n-                           &om_list_globals._free_count);\n-\n-    log_info(monitorinflation)(\"moved %d idle monitors from global waiting list to global free list\", count);\n-  }\n-}\n-\n-\/\/ Deflate global idle ObjectMonitors.\n-\/\/\n-void ObjectSynchronizer::deflate_global_idle_monitors(Thread* self) {\n-  deflate_common_idle_monitors(self, true \/* is_global *\/, NULL \/* target *\/);\n-}\n@@ -2418,41 +1512,6 @@\n-\/\/ Deflate the specified JavaThread's idle ObjectMonitors.\n-\/\/\n-void ObjectSynchronizer::deflate_per_thread_idle_monitors(Thread* self,\n-                                                          JavaThread* target) {\n-  deflate_common_idle_monitors(self, false \/* !is_global *\/, target);\n-}\n-\n-\/\/ Deflate global or per-thread idle ObjectMonitors.\n-\/\/\n-void ObjectSynchronizer::deflate_common_idle_monitors(Thread* self,\n-                                                      bool is_global,\n-                                                      JavaThread* target) {\n-  int deflated_count = 0;\n-  ObjectMonitor* free_head_p = NULL;  \/\/ Local SLL of scavenged ObjectMonitors\n-  ObjectMonitor* free_tail_p = NULL;\n-  ObjectMonitor* saved_mid_in_use_p = NULL;\n-  elapsedTimer timer;\n-\n-  if (log_is_enabled(Info, monitorinflation)) {\n-    timer.start();\n-  }\n-\n-  if (is_global) {\n-    OM_PERFDATA_OP(MonExtant, set_value(Atomic::load(&om_list_globals._in_use_count)));\n-  } else {\n-    OM_PERFDATA_OP(MonExtant, inc(Atomic::load(&target->om_in_use_count)));\n-  }\n-\n-  do {\n-    int local_deflated_count;\n-    if (is_global) {\n-      local_deflated_count =\n-          deflate_monitor_list(self, &om_list_globals._in_use_list,\n-                               &om_list_globals._in_use_count,\n-                               &free_head_p, &free_tail_p,\n-                               &saved_mid_in_use_p);\n-    } else {\n-      local_deflated_count =\n-          deflate_monitor_list(self, &target->om_in_use_list,\n-                               &target->om_in_use_count, &free_head_p,\n-                               &free_tail_p, &saved_mid_in_use_p);\n+      if (ls != NULL) {\n+        ls->print_cr(\"after handshaking: in_use_list stats: ceiling=\"\n+                     SIZE_FORMAT \", count=\" SIZE_FORMAT \", max=\" SIZE_FORMAT,\n+                     in_use_list_ceiling(), _in_use_list.count(), _in_use_list.max());\n+        timer.start();\n+      }\n@@ -2460,17 +1519,0 @@\n-    deflated_count += local_deflated_count;\n-\n-    if (free_head_p != NULL) {\n-      \/\/ Move the deflated ObjectMonitors to the global free list.\n-      guarantee(free_tail_p != NULL && local_deflated_count > 0, \"free_tail_p=\" INTPTR_FORMAT \", local_deflated_count=%d\", p2i(free_tail_p), local_deflated_count);\n-      \/\/ Note: The target thread can be doing an om_alloc() that\n-      \/\/ is trying to prepend an ObjectMonitor on its in-use list\n-      \/\/ at the same time that we have deflated the current in-use\n-      \/\/ list head and put it on the local free list. prepend_to_common()\n-      \/\/ will detect the race and retry which avoids list corruption,\n-      \/\/ but the next field in free_tail_p can flicker to marked\n-      \/\/ and then unmarked while prepend_to_common() is sorting it\n-      \/\/ all out.\n-#ifdef ASSERT\n-      ObjectMonitor* l_next_om = unmarked_next(free_tail_p);\n-      assert(l_next_om == NULL, \"must be NULL: _next_om=\" INTPTR_FORMAT, p2i(l_next_om));\n-#endif\n@@ -2478,1 +1520,14 @@\n-      prepend_list_to_global_wait_list(free_head_p, free_tail_p, local_deflated_count);\n+    \/\/ After the handshake, safely free the ObjectMonitors that were\n+    \/\/ deflated in this cycle.\n+    size_t deleted_count = 0;\n+    for (ObjectMonitor* monitor: delete_list) {\n+      delete monitor;\n+      deleted_count++;\n+      if (self->is_Java_thread() &&\n+          SafepointMechanism::should_process(self->as_Java_thread())) {\n+        \/\/ A safepoint\/handshake has started.\n+        if (ls != NULL) {\n+          timer.stop();\n+          ls->print_cr(\"pausing deletion: deleted_count=\" SIZE_FORMAT \", in_use_list stats: ceiling=\" SIZE_FORMAT \", count=\" SIZE_FORMAT \", max=\" SIZE_FORMAT,\n+                       deleted_count, in_use_list_ceiling(), _in_use_list.count(), _in_use_list.max());\n+        }\n@@ -2480,2 +1535,2 @@\n-      OM_PERFDATA_OP(Deflations, inc(local_deflated_count));\n-    }\n+        \/\/ Honor block request.\n+        ThreadBlockInVM tbivm(self->as_Java_thread());\n@@ -2483,8 +1538,4 @@\n-    if (saved_mid_in_use_p != NULL) {\n-      \/\/ deflate_monitor_list() detected a safepoint starting.\n-      timer.stop();\n-      {\n-        if (is_global) {\n-          log_debug(monitorinflation)(\"pausing deflation of global idle monitors for a safepoint.\");\n-        } else {\n-          log_debug(monitorinflation)(\"jt=\" INTPTR_FORMAT \": pausing deflation of per-thread idle monitors for a safepoint.\", p2i(target));\n+        if (ls != NULL) {\n+          ls->print_cr(\"resuming deletion: in_use_list stats: ceiling=\" SIZE_FORMAT \", count=\" SIZE_FORMAT \", max=\" SIZE_FORMAT,\n+                       in_use_list_ceiling(), _in_use_list.count(), _in_use_list.max());\n+          timer.start();\n@@ -2492,10 +1543,0 @@\n-        assert(self->is_Java_thread() &&\n-               SafepointMechanism::should_process(self->as_Java_thread()),\n-               \"sanity check\");\n-        ThreadBlockInVM blocker(self->as_Java_thread());\n-      }\n-      \/\/ Prepare for another loop after the safepoint.\n-      free_head_p = NULL;\n-      free_tail_p = NULL;\n-      if (log_is_enabled(Info, monitorinflation)) {\n-        timer.start();\n@@ -2504,10 +1545,0 @@\n-  } while (saved_mid_in_use_p != NULL);\n-  timer.stop();\n-\n-  LogStreamHandle(Debug, monitorinflation) lsh_debug;\n-  LogStreamHandle(Info, monitorinflation) lsh_info;\n-  LogStream* ls = NULL;\n-  if (log_is_enabled(Debug, monitorinflation)) {\n-    ls = &lsh_debug;\n-  } else if (deflated_count != 0 && log_is_enabled(Info, monitorinflation)) {\n-    ls = &lsh_info;\n@@ -2515,0 +1546,1 @@\n+\n@@ -2516,4 +1548,4 @@\n-    if (is_global) {\n-      ls->print_cr(\"async-deflating global idle monitors, %3.7f secs, %d monitors\", timer.seconds(), deflated_count);\n-    } else {\n-      ls->print_cr(\"jt=\" INTPTR_FORMAT \": async-deflating per-thread idle monitors, %3.7f secs, %d monitors\", p2i(target), timer.seconds(), deflated_count);\n+    timer.stop();\n+    if (deflated_count != 0 || log_is_enabled(Debug, monitorinflation)) {\n+      ls->print_cr(\"deflated \" SIZE_FORMAT \" monitors in %3.7f secs\",\n+                   deflated_count, timer.seconds());\n@@ -2521,0 +1553,2 @@\n+    ls->print_cr(\"end deflating: in_use_list stats: ceiling=\" SIZE_FORMAT \", count=\" SIZE_FORMAT \", max=\" SIZE_FORMAT,\n+                 in_use_list_ceiling(), _in_use_list.count(), _in_use_list.max());\n@@ -2522,0 +1556,7 @@\n+\n+  OM_PERFDATA_OP(MonExtant, set_value(_in_use_list.count()));\n+  OM_PERFDATA_OP(Deflations, inc(deflated_count));\n+\n+  GVars.stw_random = os::random();\n+\n+  return deflated_count;\n@@ -2599,0 +1640,23 @@\n+\/\/ Do the final audit and print of ObjectMonitor stats; must be done\n+\/\/ by the VMThread (at VM exit time).\n+void ObjectSynchronizer::do_final_audit_and_print_stats() {\n+  assert(Thread::current()->is_VM_thread(), \"sanity check\");\n+\n+  if (is_final_audit()) {  \/\/ Only do the audit once.\n+    return;\n+  }\n+  set_is_final_audit();\n+\n+  if (log_is_enabled(Info, monitorinflation)) {\n+    \/\/ Do a deflation in order to reduce the in-use monitor population\n+    \/\/ that is reported by ObjectSynchronizer::log_in_use_monitor_details()\n+    \/\/ which is called by ObjectSynchronizer::audit_and_print_stats().\n+    while (ObjectSynchronizer::deflate_idle_monitors() != 0) {\n+      ; \/\/ empty\n+    }\n+    \/\/ The other audit_and_print_stats() call is done at the Debug\n+    \/\/ level at a safepoint in ObjectSynchronizer::do_safepoint_work().\n+    ObjectSynchronizer::audit_and_print_stats(true \/* on_exit *\/);\n+  }\n+}\n+\n@@ -2625,2 +1689,0 @@\n-  \/\/ Log counts for the global and per-thread monitor lists:\n-  int chk_om_population = log_monitor_list_counts(ls);\n@@ -2629,35 +1691,2 @@\n-  ls->print_cr(\"Checking global lists:\");\n-\n-  \/\/ Check om_list_globals._population:\n-  if (Atomic::load(&om_list_globals._population) == chk_om_population) {\n-    ls->print_cr(\"global_population=%d equals chk_om_population=%d\",\n-                 Atomic::load(&om_list_globals._population), chk_om_population);\n-  } else {\n-    \/\/ With fine grained locks on the monitor lists, it is possible for\n-    \/\/ log_monitor_list_counts() to return a value that doesn't match\n-    \/\/ om_list_globals._population. So far a higher value has been\n-    \/\/ seen in testing so something is being double counted by\n-    \/\/ log_monitor_list_counts().\n-    ls->print_cr(\"WARNING: global_population=%d is not equal to \"\n-                 \"chk_om_population=%d\",\n-                 Atomic::load(&om_list_globals._population), chk_om_population);\n-  }\n-\n-  \/\/ Check om_list_globals._in_use_list and om_list_globals._in_use_count:\n-  chk_global_in_use_list_and_count(ls, &error_cnt);\n-\n-  \/\/ Check om_list_globals._free_list and om_list_globals._free_count:\n-  chk_global_free_list_and_count(ls, &error_cnt);\n-\n-  \/\/ Check om_list_globals._wait_list and om_list_globals._wait_count:\n-  chk_global_wait_list_and_count(ls, &error_cnt);\n-\n-  ls->print_cr(\"Checking per-thread lists:\");\n-\n-  for (JavaThreadIteratorWithHandle jtiwh; JavaThread *jt = jtiwh.next(); ) {\n-    \/\/ Check om_in_use_list and om_in_use_count:\n-    chk_per_thread_in_use_list_and_count(jt, ls, &error_cnt);\n-\n-    \/\/ Check om_free_list and om_free_count:\n-    chk_per_thread_free_list_and_count(jt, ls, &error_cnt);\n-  }\n+  ls->print_cr(\"Checking in_use_list:\");\n+  chk_in_use_list(ls, &error_cnt);\n@@ -2666,1 +1695,1 @@\n-    ls->print_cr(\"No errors found in monitor list checks.\");\n+    ls->print_cr(\"No errors found in in_use_list checks.\");\n@@ -2668,1 +1697,1 @@\n-    log_error(monitorinflation)(\"found monitor list errors: error_cnt=%d\", error_cnt);\n+    log_error(monitorinflation)(\"found in_use_list errors: error_cnt=%d\", error_cnt);\n@@ -2684,56 +1713,6 @@\n-\/\/ Check a free monitor entry; log any errors.\n-void ObjectSynchronizer::chk_free_entry(JavaThread* jt, ObjectMonitor* n,\n-                                        outputStream * out, int *error_cnt_p) {\n-  stringStream ss;\n-  if (n->is_busy()) {\n-    if (jt != NULL) {\n-      out->print_cr(\"ERROR: jt=\" INTPTR_FORMAT \", monitor=\" INTPTR_FORMAT\n-                    \": free per-thread monitor must not be busy: %s\", p2i(jt),\n-                    p2i(n), n->is_busy_to_string(&ss));\n-    } else {\n-      out->print_cr(\"ERROR: monitor=\" INTPTR_FORMAT \": free global monitor \"\n-                    \"must not be busy: %s\", p2i(n), n->is_busy_to_string(&ss));\n-    }\n-    *error_cnt_p = *error_cnt_p + 1;\n-  }\n-  if (n->header().value() != 0) {\n-    if (jt != NULL) {\n-      out->print_cr(\"ERROR: jt=\" INTPTR_FORMAT \", monitor=\" INTPTR_FORMAT\n-                    \": free per-thread monitor must have NULL _header \"\n-                    \"field: _header=\" INTPTR_FORMAT, p2i(jt), p2i(n),\n-                    n->header().value());\n-      *error_cnt_p = *error_cnt_p + 1;\n-    }\n-  }\n-  if (n->object_peek() != NULL) {\n-    if (jt != NULL) {\n-      out->print_cr(\"ERROR: jt=\" INTPTR_FORMAT \", monitor=\" INTPTR_FORMAT\n-                    \": free per-thread monitor must have NULL _object \"\n-                    \"field: _object=\" INTPTR_FORMAT, p2i(jt), p2i(n),\n-                    p2i(n->object_peek()));\n-    } else {\n-      out->print_cr(\"ERROR: monitor=\" INTPTR_FORMAT \": free global monitor \"\n-                    \"must have NULL _object field: _object=\" INTPTR_FORMAT,\n-                    p2i(n), p2i(n->object_peek()));\n-    }\n-    *error_cnt_p = *error_cnt_p + 1;\n-  }\n-}\n-\n-\/\/ Lock the next ObjectMonitor for traversal and unlock the current\n-\/\/ ObjectMonitor. Returns the next ObjectMonitor if there is one.\n-\/\/ Otherwise returns NULL (after unlocking the current ObjectMonitor).\n-\/\/ This function is used by the various list walker functions to\n-\/\/ safely walk a list without allowing an ObjectMonitor to be moved\n-\/\/ to another list in the middle of a walk.\n-static ObjectMonitor* lock_next_for_traversal(ObjectMonitor* cur) {\n-  assert(is_locked(cur), \"cur=\" INTPTR_FORMAT \" must be locked\", p2i(cur));\n-  ObjectMonitor* next = unmarked_next(cur);\n-  if (next == NULL) {  \/\/ Reached the end of the list.\n-    om_unlock(cur);\n-    return NULL;\n-  }\n-  om_lock(next);   \/\/ Lock next before unlocking current to keep\n-  om_unlock(cur);  \/\/ from being by-passed by another thread.\n-  return next;\n-}\n+\/\/ Check the in_use_list; log the results of the checks.\n+void ObjectSynchronizer::chk_in_use_list(outputStream* out, int *error_cnt_p) {\n+  size_t l_in_use_count = _in_use_list.count();\n+  size_t l_in_use_max = _in_use_list.max();\n+  out->print_cr(\"count=\" SIZE_FORMAT \", max=\" SIZE_FORMAT, l_in_use_count,\n+                l_in_use_max);\n@@ -2741,16 +1720,6 @@\n-\/\/ Check the global free list and count; log the results of the checks.\n-void ObjectSynchronizer::chk_global_free_list_and_count(outputStream * out,\n-                                                        int *error_cnt_p) {\n-  int chk_om_free_count = 0;\n-  ObjectMonitor* cur = NULL;\n-  if ((cur = get_list_head_locked(&om_list_globals._free_list)) != NULL) {\n-    \/\/ Marked the global free list head so process the list.\n-    while (true) {\n-      chk_free_entry(NULL \/* jt *\/, cur, out, error_cnt_p);\n-      chk_om_free_count++;\n-\n-      cur = lock_next_for_traversal(cur);\n-      if (cur == NULL) {\n-        break;\n-      }\n-    }\n+  size_t ck_in_use_count = 0;\n+  MonitorList::Iterator iter = _in_use_list.iterator();\n+  while (iter.has_next()) {\n+    ObjectMonitor* mid = iter.next();\n+    chk_in_use_entry(mid, out, error_cnt_p);\n+    ck_in_use_count++;\n@@ -2758,14 +1727,0 @@\n-  int l_free_count = Atomic::load(&om_list_globals._free_count);\n-  if (l_free_count == chk_om_free_count) {\n-    out->print_cr(\"global_free_count=%d equals chk_om_free_count=%d\",\n-                  l_free_count, chk_om_free_count);\n-  } else {\n-    \/\/ With fine grained locks on om_list_globals._free_list, it\n-    \/\/ is possible for an ObjectMonitor to be prepended to\n-    \/\/ om_list_globals._free_list after we started calculating\n-    \/\/ chk_om_free_count so om_list_globals._free_count may not\n-    \/\/ match anymore.\n-    out->print_cr(\"WARNING: global_free_count=%d is not equal to \"\n-                  \"chk_om_free_count=%d\", l_free_count, chk_om_free_count);\n-  }\n-}\n@@ -2773,22 +1728,3 @@\n-\/\/ Check the global wait list and count; log the results of the checks.\n-void ObjectSynchronizer::chk_global_wait_list_and_count(outputStream * out,\n-                                                        int *error_cnt_p) {\n-  int chk_om_wait_count = 0;\n-  ObjectMonitor* cur = NULL;\n-  if ((cur = get_list_head_locked(&om_list_globals._wait_list)) != NULL) {\n-    \/\/ Marked the global wait list head so process the list.\n-    while (true) {\n-      \/\/ Rules for om_list_globals._wait_list are the same as for\n-      \/\/ om_list_globals._free_list:\n-      chk_free_entry(NULL \/* jt *\/, cur, out, error_cnt_p);\n-      chk_om_wait_count++;\n-\n-      cur = lock_next_for_traversal(cur);\n-      if (cur == NULL) {\n-        break;\n-      }\n-    }\n-  }\n-  if (Atomic::load(&om_list_globals._wait_count) == chk_om_wait_count) {\n-    out->print_cr(\"global_wait_count=%d equals chk_om_wait_count=%d\",\n-                  Atomic::load(&om_list_globals._wait_count), chk_om_wait_count);\n+  if (l_in_use_count == ck_in_use_count) {\n+    out->print_cr(\"in_use_count=\" SIZE_FORMAT \" equals ck_in_use_count=\"\n+                  SIZE_FORMAT, l_in_use_count, ck_in_use_count);\n@@ -2796,4 +1732,3 @@\n-    out->print_cr(\"ERROR: global_wait_count=%d is not equal to \"\n-                  \"chk_om_wait_count=%d\",\n-                  Atomic::load(&om_list_globals._wait_count), chk_om_wait_count);\n-    *error_cnt_p = *error_cnt_p + 1;\n+    out->print_cr(\"WARNING: in_use_count=\" SIZE_FORMAT \" is not equal to \"\n+                  \"ck_in_use_count=\" SIZE_FORMAT, l_in_use_count,\n+                  ck_in_use_count);\n@@ -2801,1 +1736,0 @@\n-}\n@@ -2803,21 +1737,4 @@\n-\/\/ Check the global in-use list and count; log the results of the checks.\n-void ObjectSynchronizer::chk_global_in_use_list_and_count(outputStream * out,\n-                                                          int *error_cnt_p) {\n-  int chk_om_in_use_count = 0;\n-  ObjectMonitor* cur = NULL;\n-  if ((cur = get_list_head_locked(&om_list_globals._in_use_list)) != NULL) {\n-    \/\/ Marked the global in-use list head so process the list.\n-    while (true) {\n-      chk_in_use_entry(NULL \/* jt *\/, cur, out, error_cnt_p);\n-      chk_om_in_use_count++;\n-\n-      cur = lock_next_for_traversal(cur);\n-      if (cur == NULL) {\n-        break;\n-      }\n-    }\n-  }\n-  int l_in_use_count = Atomic::load(&om_list_globals._in_use_count);\n-  if (l_in_use_count == chk_om_in_use_count) {\n-    out->print_cr(\"global_in_use_count=%d equals chk_om_in_use_count=%d\",\n-                  l_in_use_count, chk_om_in_use_count);\n+  size_t ck_in_use_max = _in_use_list.max();\n+  if (l_in_use_max == ck_in_use_max) {\n+    out->print_cr(\"in_use_max=\" SIZE_FORMAT \" equals ck_in_use_max=\"\n+                  SIZE_FORMAT, l_in_use_max, ck_in_use_max);\n@@ -2825,5 +1742,2 @@\n-    \/\/ With fine grained locks on the monitor lists, it is possible for\n-    \/\/ an exiting JavaThread to put its in-use ObjectMonitors on the\n-    \/\/ global in-use list after chk_om_in_use_count is calculated above.\n-    out->print_cr(\"WARNING: global_in_use_count=%d is not equal to chk_om_in_use_count=%d\",\n-                  l_in_use_count, chk_om_in_use_count);\n+    out->print_cr(\"WARNING: in_use_max=\" SIZE_FORMAT \" is not equal to \"\n+                  \"ck_in_use_max=\" SIZE_FORMAT, l_in_use_max, ck_in_use_max);\n@@ -2834,2 +1748,8 @@\n-void ObjectSynchronizer::chk_in_use_entry(JavaThread* jt, ObjectMonitor* n,\n-                                          outputStream * out, int *error_cnt_p) {\n+void ObjectSynchronizer::chk_in_use_entry(ObjectMonitor* n, outputStream* out,\n+                                          int* error_cnt_p) {\n+  if (n->owner_is_DEFLATER_MARKER()) {\n+    \/\/ This should not happen, but if it does, it is not fatal.\n+    out->print_cr(\"WARNING: monitor=\" INTPTR_FORMAT \": in-use monitor is \"\n+                  \"deflated.\", p2i(n));\n+    return;\n+  }\n@@ -2837,8 +1757,2 @@\n-    if (jt != NULL) {\n-      out->print_cr(\"ERROR: jt=\" INTPTR_FORMAT \", monitor=\" INTPTR_FORMAT\n-                    \": in-use per-thread monitor must have non-NULL _header \"\n-                    \"field.\", p2i(jt), p2i(n));\n-    } else {\n-      out->print_cr(\"ERROR: monitor=\" INTPTR_FORMAT \": in-use global monitor \"\n-                    \"must have non-NULL _header field.\", p2i(n));\n-    }\n+    out->print_cr(\"ERROR: monitor=\" INTPTR_FORMAT \": in-use monitor must \"\n+                  \"have non-NULL _header field.\", p2i(n));\n@@ -2851,11 +1765,4 @@\n-      if (jt != NULL) {\n-        out->print_cr(\"ERROR: jt=\" INTPTR_FORMAT \", monitor=\" INTPTR_FORMAT\n-                      \": in-use per-thread monitor's object does not think \"\n-                      \"it has a monitor: obj=\" INTPTR_FORMAT \", mark=\"\n-                      INTPTR_FORMAT,  p2i(jt), p2i(n), p2i(obj), mark.value());\n-      } else {\n-        out->print_cr(\"ERROR: monitor=\" INTPTR_FORMAT \": in-use global \"\n-                      \"monitor's object does not think it has a monitor: obj=\"\n-                      INTPTR_FORMAT \", mark=\" INTPTR_FORMAT, p2i(n),\n-                      p2i(obj), mark.value());\n-      }\n+      out->print_cr(\"ERROR: monitor=\" INTPTR_FORMAT \": in-use monitor's \"\n+                    \"object does not think it has a monitor: obj=\"\n+                    INTPTR_FORMAT \", mark=\" INTPTR_FORMAT, p2i(n),\n+                    p2i(obj), mark.value());\n@@ -2866,12 +1773,4 @@\n-      if (jt != NULL) {\n-        out->print_cr(\"ERROR: jt=\" INTPTR_FORMAT \", monitor=\" INTPTR_FORMAT\n-                      \": in-use per-thread monitor's object does not refer \"\n-                      \"to the same monitor: obj=\" INTPTR_FORMAT \", mark=\"\n-                      INTPTR_FORMAT \", obj_mon=\" INTPTR_FORMAT, p2i(jt),\n-                      p2i(n), p2i(obj), mark.value(), p2i(obj_mon));\n-      } else {\n-        out->print_cr(\"ERROR: monitor=\" INTPTR_FORMAT \": in-use global \"\n-                      \"monitor's object does not refer to the same monitor: obj=\"\n-                      INTPTR_FORMAT \", mark=\" INTPTR_FORMAT \", obj_mon=\"\n-                      INTPTR_FORMAT, p2i(n), p2i(obj), mark.value(), p2i(obj_mon));\n-      }\n+      out->print_cr(\"ERROR: monitor=\" INTPTR_FORMAT \": in-use monitor's \"\n+                    \"object does not refer to the same monitor: obj=\"\n+                    INTPTR_FORMAT \", mark=\" INTPTR_FORMAT \", obj_mon=\"\n+                    INTPTR_FORMAT, p2i(n), p2i(obj), mark.value(), p2i(obj_mon));\n@@ -2883,83 +1782,1 @@\n-\/\/ Check the thread's free list and count; log the results of the checks.\n-void ObjectSynchronizer::chk_per_thread_free_list_and_count(JavaThread *jt,\n-                                                            outputStream * out,\n-                                                            int *error_cnt_p) {\n-  int chk_om_free_count = 0;\n-  ObjectMonitor* cur = NULL;\n-  if ((cur = get_list_head_locked(&jt->om_free_list)) != NULL) {\n-    \/\/ Marked the per-thread free list head so process the list.\n-    while (true) {\n-      chk_free_entry(jt, cur, out, error_cnt_p);\n-      chk_om_free_count++;\n-\n-      cur = lock_next_for_traversal(cur);\n-      if (cur == NULL) {\n-        break;\n-      }\n-    }\n-  }\n-  int l_om_free_count = Atomic::load(&jt->om_free_count);\n-  if (l_om_free_count == chk_om_free_count) {\n-    out->print_cr(\"jt=\" INTPTR_FORMAT \": om_free_count=%d equals \"\n-                  \"chk_om_free_count=%d\", p2i(jt), l_om_free_count, chk_om_free_count);\n-  } else {\n-    out->print_cr(\"ERROR: jt=\" INTPTR_FORMAT \": om_free_count=%d is not \"\n-                  \"equal to chk_om_free_count=%d\", p2i(jt), l_om_free_count,\n-                  chk_om_free_count);\n-    *error_cnt_p = *error_cnt_p + 1;\n-  }\n-}\n-\n-\/\/ Check the thread's in-use list and count; log the results of the checks.\n-void ObjectSynchronizer::chk_per_thread_in_use_list_and_count(JavaThread *jt,\n-                                                              outputStream * out,\n-                                                              int *error_cnt_p) {\n-  int chk_om_in_use_count = 0;\n-  ObjectMonitor* cur = NULL;\n-  if ((cur = get_list_head_locked(&jt->om_in_use_list)) != NULL) {\n-    \/\/ Marked the per-thread in-use list head so process the list.\n-    while (true) {\n-      chk_in_use_entry(jt, cur, out, error_cnt_p);\n-      chk_om_in_use_count++;\n-\n-      cur = lock_next_for_traversal(cur);\n-      if (cur == NULL) {\n-        break;\n-      }\n-    }\n-  }\n-  int l_om_in_use_count = Atomic::load(&jt->om_in_use_count);\n-  if (l_om_in_use_count == chk_om_in_use_count) {\n-    out->print_cr(\"jt=\" INTPTR_FORMAT \": om_in_use_count=%d equals \"\n-                  \"chk_om_in_use_count=%d\", p2i(jt), l_om_in_use_count,\n-                  chk_om_in_use_count);\n-  } else {\n-    out->print_cr(\"ERROR: jt=\" INTPTR_FORMAT \": om_in_use_count=%d is not \"\n-                  \"equal to chk_om_in_use_count=%d\", p2i(jt), l_om_in_use_count,\n-                  chk_om_in_use_count);\n-    *error_cnt_p = *error_cnt_p + 1;\n-  }\n-}\n-\n-\/\/ Do the final audit and print of ObjectMonitor stats; must be done\n-\/\/ by the VMThread (at VM exit time).\n-void ObjectSynchronizer::do_final_audit_and_print_stats() {\n-  assert(Thread::current()->is_VM_thread(), \"sanity check\");\n-\n-  if (is_final_audit()) {  \/\/ Only do the audit once.\n-    return;\n-  }\n-  set_is_final_audit();\n-\n-  if (log_is_enabled(Info, monitorinflation)) {\n-    \/\/ Do a deflation in order to reduce the in-use monitor population\n-    \/\/ that is reported by ObjectSynchronizer::log_in_use_monitor_details()\n-    \/\/ which is called by ObjectSynchronizer::audit_and_print_stats().\n-    ObjectSynchronizer::deflate_idle_monitors();\n-    \/\/ The other audit_and_print_stats() call is done at the Debug\n-    \/\/ level at a safepoint in ObjectSynchronizer::do_safepoint_work().\n-    ObjectSynchronizer::audit_and_print_stats(true \/* on_exit *\/);\n-  }\n-}\n-\n-\/\/ Log details about ObjectMonitors on the in-use lists. The 'BHL'\n+\/\/ Log details about ObjectMonitors on the in_use_list. The 'BHL'\n@@ -2968,1 +1785,1 @@\n-void ObjectSynchronizer::log_in_use_monitor_details(outputStream * out) {\n+void ObjectSynchronizer::log_in_use_monitor_details(outputStream* out) {\n@@ -2970,2 +1787,2 @@\n-  if (Atomic::load(&om_list_globals._in_use_count) > 0) {\n-    out->print_cr(\"In-use global monitor info:\");\n+  if (_in_use_list.count() > 0) {\n+    out->print_cr(\"In-use monitor info:\");\n@@ -2976,51 +1793,12 @@\n-    ObjectMonitor* cur = NULL;\n-    if ((cur = get_list_head_locked(&om_list_globals._in_use_list)) != NULL) {\n-      \/\/ Marked the global in-use list head so process the list.\n-      while (true) {\n-        const oop obj = cur->object_peek();\n-        const markWord mark = cur->header();\n-        ResourceMark rm;\n-        out->print(INTPTR_FORMAT \"  %d%d%d  \" INTPTR_FORMAT \"  %s\", p2i(cur),\n-                   cur->is_busy() != 0, mark.hash() != 0, cur->owner() != NULL,\n-                   p2i(obj), obj == NULL ? \"\" : obj->klass()->external_name());\n-        if (cur->is_busy() != 0) {\n-          out->print(\" (%s)\", cur->is_busy_to_string(&ss));\n-          ss.reset();\n-        }\n-        out->cr();\n-\n-        cur = lock_next_for_traversal(cur);\n-        if (cur == NULL) {\n-          break;\n-        }\n-      }\n-    }\n-  }\n-\n-  out->print_cr(\"In-use per-thread monitor info:\");\n-  out->print_cr(\"(B -> is_busy, H -> has hash code, L -> lock status)\");\n-  out->print_cr(\"%18s  %18s  %s  %18s  %18s\",\n-                \"jt\", \"monitor\", \"BHL\", \"object\", \"object type\");\n-  out->print_cr(\"==================  ==================  ===  ==================  ==================\");\n-  for (JavaThreadIteratorWithHandle jtiwh; JavaThread *jt = jtiwh.next(); ) {\n-    ObjectMonitor* cur = NULL;\n-    if ((cur = get_list_head_locked(&jt->om_in_use_list)) != NULL) {\n-      \/\/ Marked the global in-use list head so process the list.\n-      while (true) {\n-        const oop obj = cur->object_peek();\n-        const markWord mark = cur->header();\n-        ResourceMark rm;\n-        out->print(INTPTR_FORMAT \"  \" INTPTR_FORMAT \"  %d%d%d  \" INTPTR_FORMAT\n-                   \"  %s\", p2i(jt), p2i(cur), cur->is_busy() != 0,\n-                   mark.hash() != 0, cur->owner() != NULL, p2i(obj),\n-                   obj == NULL ? \"\" : obj->klass()->external_name());\n-        if (cur->is_busy() != 0) {\n-          out->print(\" (%s)\", cur->is_busy_to_string(&ss));\n-          ss.reset();\n-        }\n-        out->cr();\n-\n-        cur = lock_next_for_traversal(cur);\n-        if (cur == NULL) {\n-          break;\n-        }\n+    MonitorList::Iterator iter = _in_use_list.iterator();\n+    while (iter.has_next()) {\n+      ObjectMonitor* mid = iter.next();\n+      const oop obj = mid->object_peek();\n+      const markWord mark = mid->header();\n+      ResourceMark rm;\n+      out->print(INTPTR_FORMAT \"  %d%d%d  \" INTPTR_FORMAT \"  %s\", p2i(mid),\n+                 mid->is_busy() != 0, mark.hash() != 0, mid->owner() != NULL,\n+                 p2i(obj), obj == NULL ? \"\" : obj->klass()->external_name());\n+      if (mid->is_busy() != 0) {\n+        out->print(\" (%s)\", mid->is_busy_to_string(&ss));\n+        ss.reset();\n@@ -3028,0 +1806,1 @@\n+      out->cr();\n@@ -3033,55 +1812,0 @@\n-\n-\/\/ Log counts for the global and per-thread monitor lists and return\n-\/\/ the population count.\n-int ObjectSynchronizer::log_monitor_list_counts(outputStream * out) {\n-  int pop_count = 0;\n-  out->print_cr(\"%18s  %10s  %10s  %10s  %10s\",\n-                \"Global Lists:\", \"InUse\", \"Free\", \"Wait\", \"Total\");\n-  out->print_cr(\"==================  ==========  ==========  ==========  ==========\");\n-  int l_in_use_count = Atomic::load(&om_list_globals._in_use_count);\n-  int l_free_count = Atomic::load(&om_list_globals._free_count);\n-  int l_wait_count = Atomic::load(&om_list_globals._wait_count);\n-  out->print_cr(\"%18s  %10d  %10d  %10d  %10d\", \"\", l_in_use_count,\n-                l_free_count, l_wait_count,\n-                Atomic::load(&om_list_globals._population));\n-  pop_count += l_in_use_count + l_free_count + l_wait_count;\n-\n-  out->print_cr(\"%18s  %10s  %10s  %10s\",\n-                \"Per-Thread Lists:\", \"InUse\", \"Free\", \"Provision\");\n-  out->print_cr(\"==================  ==========  ==========  ==========\");\n-\n-  for (JavaThreadIteratorWithHandle jtiwh; JavaThread *jt = jtiwh.next(); ) {\n-    int l_om_in_use_count = Atomic::load(&jt->om_in_use_count);\n-    int l_om_free_count = Atomic::load(&jt->om_free_count);\n-    out->print_cr(INTPTR_FORMAT \"  %10d  %10d  %10d\", p2i(jt),\n-                  l_om_in_use_count, l_om_free_count, jt->om_free_provision);\n-    pop_count += l_om_in_use_count + l_om_free_count;\n-  }\n-  return pop_count;\n-}\n-\n-#ifndef PRODUCT\n-\n-\/\/ Check if monitor belongs to the monitor cache\n-\/\/ The list is grow-only so it's *relatively* safe to traverse\n-\/\/ the list of extant blocks without taking a lock.\n-\n-int ObjectSynchronizer::verify_objmon_isinpool(ObjectMonitor *monitor) {\n-  PaddedObjectMonitor* block = Atomic::load(&g_block_list);\n-  while (block != NULL) {\n-    assert(block->is_chainmarker(), \"must be a block header\");\n-    if (monitor > &block[0] && monitor < &block[_BLOCKSIZE]) {\n-      address mon = (address)monitor;\n-      address blk = (address)block;\n-      size_t diff = mon - blk;\n-      assert((diff % sizeof(PaddedObjectMonitor)) == 0, \"must be aligned\");\n-      return 1;\n-    }\n-    \/\/ unmarked_next() is not needed with g_block_list (no locking\n-    \/\/ used with block linkage _next_om fields).\n-    block = (PaddedObjectMonitor*)block->next_om();\n-  }\n-  return 0;\n-}\n-\n-#endif\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.cpp","additions":343,"deletions":1619,"binary":false,"changes":1962,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"logging\/logStream.hpp\"\n@@ -37,5 +38,4 @@\n-#ifndef OM_CACHE_LINE_SIZE\n-\/\/ Use DEFAULT_CACHE_LINE_SIZE if not already specified for\n-\/\/ the current build platform.\n-#define OM_CACHE_LINE_SIZE DEFAULT_CACHE_LINE_SIZE\n-#endif\n+class MonitorList {\n+  ObjectMonitor* volatile _head;\n+  volatile size_t _count;\n+  volatile size_t _max;\n@@ -43,1 +43,19 @@\n-typedef PaddedEnd<ObjectMonitor, OM_CACHE_LINE_SIZE> PaddedObjectMonitor;\n+public:\n+  void add(ObjectMonitor* monitor);\n+  size_t unlink_deflated(Thread* self, LogStream* ls, elapsedTimer* timer_p,\n+                         GrowableArray<ObjectMonitor*>* unlinked_list);\n+  size_t count() const;\n+  size_t max() const;\n+\n+  class Iterator;\n+  Iterator iterator() const;\n+};\n+\n+class MonitorList::Iterator {\n+  ObjectMonitor* _current;\n+\n+public:\n+  Iterator(ObjectMonitor* head) : _current(head) {}\n+  bool has_next() const { return _current != NULL; }\n+  ObjectMonitor* next();\n+};\n@@ -47,0 +65,1 @@\n+\n@@ -102,6 +121,0 @@\n-  \/\/ thread-specific and global ObjectMonitor free list accessors\n-  static ObjectMonitor* om_alloc(Thread* self);\n-  static void om_release(Thread* self, ObjectMonitor* m,\n-                         bool FromPerThreadAlloc);\n-  static void om_flush(Thread* self);\n-\n@@ -130,17 +143,9 @@\n-  \/\/ Basically we deflate all monitors that are not busy.\n-  \/\/ An adaptive profile-based deflation policy could be used if needed\n-  static void deflate_idle_monitors();\n-  static void deflate_global_idle_monitors(Thread* self);\n-  static void deflate_per_thread_idle_monitors(Thread* self,\n-                                               JavaThread* target);\n-  static void deflate_common_idle_monitors(Thread* self, bool is_global,\n-                                           JavaThread* target);\n-\n-  \/\/ For a given in-use monitor list: global or per-thread, deflate idle\n-  \/\/ monitors.\n-  static int deflate_monitor_list(Thread* self, ObjectMonitor** list_p,\n-                                  int* count_p, ObjectMonitor** free_head_p,\n-                                  ObjectMonitor** free_tail_p,\n-                                  ObjectMonitor** saved_mid_in_use_p);\n-  static bool deflate_monitor(ObjectMonitor* mid, ObjectMonitor** free_head_p,\n-                              ObjectMonitor** free_tail_p);\n+  \/\/ Basically we try to deflate all monitors that are not busy.\n+  static size_t deflate_idle_monitors();\n+\n+  \/\/ Deflate idle monitors:\n+  static size_t deflate_monitor_list(Thread* self, LogStream* ls,\n+                                     elapsedTimer* timer_p);\n+  static size_t in_use_list_ceiling() { return (size_t)_in_use_list_ceiling; }\n+  static void dec_in_use_list_ceiling();\n+  static void inc_in_use_list_ceiling();\n@@ -158,16 +163,3 @@\n-  static void chk_free_entry(JavaThread* jt, ObjectMonitor* n,\n-                             outputStream * out, int *error_cnt_p);\n-  static void chk_global_free_list_and_count(outputStream * out,\n-                                             int *error_cnt_p);\n-  static void chk_global_wait_list_and_count(outputStream * out,\n-                                             int *error_cnt_p);\n-  static void chk_global_in_use_list_and_count(outputStream * out,\n-                                               int *error_cnt_p);\n-  static void chk_in_use_entry(JavaThread* jt, ObjectMonitor* n,\n-                               outputStream * out, int *error_cnt_p);\n-  static void chk_per_thread_in_use_list_and_count(JavaThread *jt,\n-                                                   outputStream * out,\n-                                                   int *error_cnt_p);\n-  static void chk_per_thread_free_list_and_count(JavaThread *jt,\n-                                                 outputStream * out,\n-                                                 int *error_cnt_p);\n+  static void chk_in_use_list(outputStream* out, int* error_cnt_p);\n+  static void chk_in_use_entry(ObjectMonitor* n, outputStream* out,\n+                               int* error_cnt_p);\n@@ -175,5 +167,1 @@\n-  static void log_in_use_monitor_details(outputStream * out);\n-  static int  log_monitor_list_counts(outputStream * out);\n-  static int  verify_objmon_isinpool(ObjectMonitor *addr) PRODUCT_RETURN0;\n-\n-  static void do_safepoint_work();\n+  static void log_in_use_monitor_details(outputStream* out);\n@@ -184,3 +172,12 @@\n-  enum { _BLOCKSIZE = 128 };\n-  \/\/ global list of blocks of monitors\n-  static PaddedObjectMonitor* g_block_list;\n+  static MonitorList   _in_use_list;\n+  \/\/ The ratio of the current _in_use_list count to the ceiling is used\n+  \/\/ to determine if we are above MonitorUsedDeflationThreshold and need\n+  \/\/ to do an async monitor deflation cycle. The ceiling is increased by\n+  \/\/ AvgMonitorsPerThreadEstimate when a thread is added to the system\n+  \/\/ and is decreased by AvgMonitorsPerThreadEstimate when a thread is\n+  \/\/ removed from the system.\n+  \/\/ Note: If the _in_use_list max exceeds the ceiling, then\n+  \/\/ monitors_used_above_threshold() will use the in_use_list max instead\n+  \/\/ of the thread count derived ceiling because we have used more\n+  \/\/ ObjectMonitors than the estimated average.\n+  static jint          _in_use_list_ceiling;\n@@ -191,3 +188,0 @@\n-  \/\/ Function to prepend new blocks to the appropriate lists:\n-  static void prepend_block_to_lists(PaddedObjectMonitor* new_blk);\n-\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.hpp","additions":50,"deletions":56,"binary":false,"changes":106,"status":"modified"},{"patch":"@@ -84,0 +84,1 @@\n+#include \"runtime\/monitorDeflationThread.hpp\"\n@@ -265,5 +266,0 @@\n-  om_free_list = NULL;\n-  om_free_count = 0;\n-  om_free_provision = 32;\n-  om_in_use_list = NULL;\n-  om_in_use_count = 0;\n@@ -3694,0 +3690,3 @@\n+  \/\/ Start the monitor deflation thread:\n+  MonitorDeflationThread::initialize();\n+\n@@ -4236,0 +4235,3 @@\n+  \/\/ Increase the ObjectMonitor ceiling for the new thread.\n+  ObjectSynchronizer::inc_in_use_list_ceiling();\n+\n@@ -4244,4 +4246,0 @@\n-\n-  \/\/ Reclaim the ObjectMonitors from the om_in_use_list and om_free_list of the moribund thread.\n-  ObjectSynchronizer::om_flush(p);\n-\n@@ -4252,5 +4250,4 @@\n-    \/\/ We must flush any deferred card marks and other various GC barrier\n-    \/\/ related buffers (e.g. G1 SATB buffer and G1 dirty card queue buffer)\n-    \/\/ before removing a thread from the list of active threads.\n-    \/\/ This must be done after ObjectSynchronizer::om_flush(), as GC barriers\n-    \/\/ are used in om_flush().\n+    \/\/ BarrierSet state must be destroyed after the last thread transition\n+    \/\/ before the thread terminates. Thread transitions result in calls to\n+    \/\/ StackWatermarkSet::on_safepoint(), which performs GC processing,\n+    \/\/ requiring the GC state to be alive.\n@@ -4286,0 +4283,3 @@\n+  \/\/ Reduce the ObjectMonitor ceiling for the exiting thread.\n+  ObjectSynchronizer::dec_in_use_list_ceiling();\n+\n","filename":"src\/hotspot\/share\/runtime\/thread.cpp","additions":14,"deletions":14,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -419,8 +419,0 @@\n-  \/\/ Per-thread ObjectMonitor lists:\n- public:\n-  ObjectMonitor* om_free_list;                  \/\/ SLL of free ObjectMonitors\n-  int om_free_count;                            \/\/ # on om_free_list\n-  int om_free_provision;                        \/\/ # to try to allocate next\n-  ObjectMonitor* om_in_use_list;                \/\/ SLL of in-use ObjectMonitors\n-  int om_in_use_count;                          \/\/ # on om_in_use_list\n-\n@@ -488,0 +480,1 @@\n+  virtual bool is_monitor_deflation_thread() const   { return false; }\n","filename":"src\/hotspot\/share\/runtime\/thread.hpp","additions":1,"deletions":8,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -89,0 +89,1 @@\n+#include \"runtime\/monitorDeflationThread.hpp\"\n@@ -890,1 +891,0 @@\n-  static_field(ObjectSynchronizer,             g_block_list,                                  PaddedObjectMonitor*)                  \\\n@@ -1341,0 +1341,1 @@\n+        declare_type(MonitorDeflationThread, JavaThread)                  \\\n@@ -1465,1 +1466,0 @@\n-  declare_toplevel_type(PaddedObjectMonitor)                              \\\n@@ -1996,1 +1996,0 @@\n-  declare_toplevel_type(PaddedObjectMonitor*)                             \\\n@@ -2526,6 +2525,0 @@\n-  \/* ObjectSynchronizer *\/                                                \\\n-  \/**********************\/                                                \\\n-                                                                          \\\n-  declare_constant(ObjectSynchronizer::_BLOCKSIZE)                        \\\n-                                                                          \\\n-  \/**********************\/                                                \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":2,"deletions":9,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -0,0 +1,41 @@\n+\/*\n+ * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+package sun.jvm.hotspot.runtime;\n+\n+import java.io.*;\n+\n+import sun.jvm.hotspot.debugger.*;\n+import sun.jvm.hotspot.types.*;\n+\n+public class MonitorDeflationThread extends JavaThread {\n+  public MonitorDeflationThread(Address addr) {\n+    super(addr);\n+  }\n+\n+  public boolean isJavaThread() { return false; }\n+  public boolean isHiddenFromExternalView() { return true; }\n+  public boolean isMonitorDeflationThread() { return true; }\n+\n+}\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/runtime\/MonitorDeflationThread.java","additions":41,"deletions":0,"binary":false,"changes":41,"status":"added"},{"patch":"@@ -106,4 +106,0 @@\n-  \/\/ FIXME\n-  \/\/  oop*      object_addr();\n-  \/\/  void      set_object(oop obj);\n-\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/runtime\/ObjectMonitor.java","additions":0,"deletions":4,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -124,0 +124,1 @@\n+  public boolean   isMonitorDeflationThread()    { return false; }\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/runtime\/Thread.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -152,0 +152,1 @@\n+        virtualConstructor.addMapping(\"MonitorDeflationThread\", MonitorDeflationThread.class);\n@@ -160,1 +161,1 @@\n-      JvmtiAgentThread, NotificationThread, and ServiceThread.\n+      JvmtiAgentThread, NotificationThread, MonitorDeflationThread and ServiceThread.\n@@ -191,1 +192,1 @@\n-            \" (expected type JavaThread, CompilerThread, ServiceThread, JvmtiAgentThread or CodeCacheSweeperThread)\", e);\n+            \" (expected type JavaThread, CompilerThread, MonitorDeflationThread, ServiceThread, JvmtiAgentThread or CodeCacheSweeperThread)\", e);\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/runtime\/Threads.java","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -30,3 +30,0 @@\n-\n- EXPECT_EQ(0, ObjectMonitor::header_offset_in_bytes()) << \"Offset for _header must be zero.\";\n-\n","filename":"test\/hotspot\/gtest\/runtime\/test_objectMonitor.cpp","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -42,1 +42,0 @@\n-        output.shouldContain(\"deflating idle monitors\");\n","filename":"test\/hotspot\/jtreg\/runtime\/logging\/SafepointCleanupTest.java","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -62,1 +62,0 @@\n-                    \"ObjectSynchronizer::g_block_list\",\n","filename":"test\/hotspot\/jtreg\/serviceability\/sa\/ClhsdbPrintStatics.java","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"}]}