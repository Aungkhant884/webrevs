{"files":[{"patch":"@@ -35,0 +35,13 @@\n+\n+\/\/ Some static assumptions:\n+\n+\/\/ We require the max arena alignment to be <= malloc alignment.\n+const size_t max_malloc_align = sizeof(long double); \/\/ max_align_t?\n+STATIC_ASSERT(Chunk::max_arena_alignment <= max_malloc_align);\n+\n+\/\/ We require the standard chunk sizes to be aligned to Chunk::max_arena_alignment.\n+STATIC_ASSERT(is_aligned((size_t)Chunk::tiny_size, Chunk::max_arena_alignment));\n+STATIC_ASSERT(is_aligned((size_t)Chunk::init_size, Chunk::max_arena_alignment));\n+STATIC_ASSERT(is_aligned((size_t)Chunk::medium_size, Chunk::max_arena_alignment));\n+STATIC_ASSERT(is_aligned((size_t)Chunk::size, Chunk::max_arena_alignment));\n+\n@@ -37,0 +50,2 @@\n+\/\/\n+\/\/ A ChunkPool caches chunks of the same size.\n@@ -44,1 +59,1 @@\n-  const size_t _size;         \/\/ size of each chunk (must be uniform)\n+  const size_t _size;         \/\/ the *outer* size of the chunks this pool manages\n@@ -64,1 +79,4 @@\n-   ChunkPool(size_t size) : _size(size) { _first = NULL; _num_chunks = _num_used = 0; }\n+   ChunkPool(size_t size) : _first(NULL), _num_chunks(0), _num_used(0), _size(size) {\n+     assert(is_aligned(_size, Chunk::max_arena_alignment),\n+            \"Standard pool sizes must be aligned to max arena alignment\");\n+   }\n@@ -80,0 +98,1 @@\n+    assert(is_aligned(p, Chunk::max_arena_alignment), \"malloc alignment too small?\");\n@@ -85,1 +104,1 @@\n-    assert(chunk->length() + Chunk::aligned_overhead_size() == _size, \"bad size\");\n+    assert(chunk->outer_size() == _size, \"bad size\");\n@@ -132,4 +151,4 @@\n-    _large_pool  = new ChunkPool(Chunk::size        + Chunk::aligned_overhead_size());\n-    _medium_pool = new ChunkPool(Chunk::medium_size + Chunk::aligned_overhead_size());\n-    _small_pool  = new ChunkPool(Chunk::init_size   + Chunk::aligned_overhead_size());\n-    _tiny_pool   = new ChunkPool(Chunk::tiny_size   + Chunk::aligned_overhead_size());\n+    _large_pool  = new ChunkPool(Chunk::calc_outer_size(Chunk::size));\n+    _medium_pool = new ChunkPool(Chunk::calc_outer_size(Chunk::medium_size));\n+    _small_pool  = new ChunkPool(Chunk::calc_outer_size(Chunk::init_size));\n+    _tiny_pool   = new ChunkPool(Chunk::calc_outer_size(Chunk::tiny_size));\n@@ -174,7 +193,5 @@\n-void* Chunk::operator new (size_t requested_size, AllocFailType alloc_failmode, size_t length) throw() {\n-  \/\/ requested_size is equal to sizeof(Chunk) but in order for the arena\n-  \/\/ allocations to come out aligned as expected the size must be aligned\n-  \/\/ to expected arena alignment.\n-  \/\/ expect requested_size but if sizeof(Chunk) doesn't match isn't proper size we must align it.\n-  assert(ARENA_ALIGN(requested_size) == aligned_overhead_size(), \"Bad alignment\");\n-  size_t bytes = ARENA_ALIGN(requested_size) + length;\n+void* Chunk::operator new (size_t size, AllocFailType alloc_failmode, size_t length) throw() {\n+  \/\/ - size == sizeof(Chunk), possibly unaligned\n+  \/\/ - length == requested payload length - possibly unaligned if non-standard size\n+  \/\/ We need to allocate enough memory to hold the aligned header + aligned payload.\n+  const size_t bytes = Chunk::calc_outer_size(length);\n@@ -209,4 +226,0 @@\n-Chunk::Chunk(size_t length) : _len(length) {\n-  _next = NULL;         \/\/ Chain on the linked list\n-}\n-\n@@ -366,0 +379,3 @@\n+  if (new_size == old_size) {\n+    return old_ptr; \/\/ Nothing to do.\n+  }\n@@ -389,1 +405,6 @@\n-  if( new_size <= old_size ) {  \/\/ Shrink in-place\n+  if (new_size < old_size) {  \/\/ Shrink in-place\n+#ifdef ASSERT\n+    if (ZapResourceArea) {\n+      ::memset(c_old + new_size, badResourceValue, old_size - new_size); \/\/ zap leftovers\n+    }\n+#endif\n@@ -395,3 +416,0 @@\n-  \/\/ make sure that new_size is legal\n-  size_t corrected_new_size = ARENA_ALIGN(new_size);\n-\n@@ -400,2 +418,2 @@\n-      (c_old+corrected_new_size <= _max) ) {      \/\/ Still fits where it sits\n-    _hwm = c_old+corrected_new_size;      \/\/ Adjust hwm\n+      (pointer_delta(_max, c_old, 1) >= new_size) ) {      \/\/ Still fits where it sits\n+    _hwm = c_old+new_size;      \/\/ Adjust hwm\n@@ -415,1 +433,0 @@\n-\n@@ -452,2 +469,5 @@\n-  char** save = (char**)internal_amalloc(sizeof(char*));\n-  return (*save = (char*)os::malloc(size, mtChunk));\n+  char** save = (char**)internal_amalloc(sizeof(char*), sizeof(char*));\n+  *save = (char*)os::malloc(size, mtChunk);\n+  assert(is_aligned(*save, Chunk::max_arena_alignment),\n+         \"malloc alignment too small?\"); \/\/ see comments at start of file.\n+  return *save;\n","filename":"src\/hotspot\/share\/memory\/arena.cpp","additions":47,"deletions":27,"binary":false,"changes":74,"status":"modified"},{"patch":"@@ -36,4 +36,0 @@\n-\/\/ The byte alignment to be used by Arena::Amalloc.\n-#define ARENA_AMALLOC_ALIGNMENT BytesPerLong\n-#define ARENA_ALIGN(x) (align_up((x), ARENA_AMALLOC_ALIGNMENT))\n-\n@@ -44,1 +40,32 @@\n- private:\n+  \/\/ Chunk layout:\n+  \/\/\n+  \/\/ |-------------|-------------------------------------------------|\n+  \/\/ |           |p|                                                 |\n+  \/\/ |   Chunk   |a|                 payload                         |\n+  \/\/ |  (header) |d|          |                                      |\n+  \/\/ |-------------|----------|--------------------------------------|\n+  \/\/ this         bottom      |                                      top\n+  \/\/                          _hwm (in arena)\n+  \/\/\n+  \/\/ Start of Chunk (this), bottom and top have to be aligned to max arena alignment.\n+  \/\/ _hwm does not have to be aligned to anything - it gets aligned on demand\n+  \/\/ at allocation time.\n+  \/\/\n+  \/\/ Size of Chunk may be unaligned (on 32-bit), therefore we may need to add padding\n+  \/\/ in front of the payload area. Otherwise, we just rely on malloc() returning memory\n+  \/\/ aligned to at least max arena alignment - that takes care of both Chunk start alignment\n+  \/\/ and allocation alignment if +UseMallocOnly.\n+  \/\/\n+  \/\/ ---\n+  \/\/\n+  \/\/ Allocation alignment:\n+  \/\/\n+  \/\/ Allocation from an arena is possible in alignments [1..Chunk::max_arena_alignment].\n+  \/\/  The arena will automatically align on each allocation if needed, adding padding if\n+  \/\/  necessary. In debug builds, those paddings will be zapped with a pattern ('GGGG..').\n+  \/\/\n+\n+  static const char gap_pattern = 'G';\n+  static const uint64_t chunk_canary = 0x4152454e41434e4bLL; \/\/ \"ARENACNK\"\n+\n+  const uint64_t _canary; \/\/ A Chunk is preceded by a canary\n@@ -46,1 +73,1 @@\n-  const size_t _len;      \/\/ Size of this Chunk\n+  const size_t _len;      \/\/ Size of the *payload area* of this chunk, guaranteed to be aligned\n@@ -48,0 +75,18 @@\n+\n+  \/\/ Maximum possible alignment which can be requested by an arena allocation.\n+  \/\/  (Note: currently, the implementation relies on this not being larger than\n+  \/\/   malloc alignment, see UseMallocOnly and Chunk allocation).\n+  static const size_t max_arena_alignment = BytesPerLong; \/\/ 64 bit\n+\n+  \/\/ Given a size, align it to max. alignment\n+  static size_t max_align(size_t s) { return align_up(s, max_arena_alignment); }\n+\n+  \/\/ Return aligned header size aka payload start offset\n+  static size_t header_size() { return max_align(sizeof(Chunk)); }\n+\n+  \/\/ Given a (possibly misaligned) payload size, return the total chunk size\n+  \/\/ including header\n+  static size_t calc_outer_size(size_t payload_size) {\n+    return header_size() + max_align(payload_size);\n+  }\n+\n@@ -50,1 +95,2 @@\n-  Chunk(size_t length);\n+\n+  Chunk(size_t length) : _canary(chunk_canary), _next(NULL), _len(max_align(length)) {}\n@@ -55,0 +101,2 @@\n+    \/\/\n+    \/\/ Note: standard chunk sizes need to be aligned to max arena alignment.\n@@ -59,1 +107,1 @@\n-    slack      = 20,            \/\/ suspected sizeof(Chunk) + internal malloc headers\n+    slack      = 24,            \/\/ suspected sizeof(Chunk) + internal malloc headers\n@@ -71,2 +119,0 @@\n-  static size_t aligned_overhead_size(void) { return ARENA_ALIGN(sizeof(Chunk)); }\n-  static size_t aligned_overhead_size(size_t byte_size) { return ARENA_ALIGN(byte_size); }\n@@ -74,1 +120,2 @@\n-  size_t length() const         { return _len;  }\n+  \/\/ Returns size, in bytes, of the *payload* area of this chunk\n+  size_t length() const         { assert(is_aligned(_len, max_arena_alignment), \"misaligned chunk length\"); return _len;  }\n@@ -77,0 +124,3 @@\n+\n+  \/\/ Returns size, in bytes, of the total chunk size including header\n+  size_t outer_size() const     { return calc_outer_size(length()); }\n@@ -78,2 +128,2 @@\n-  char* bottom() const          { return ((char*) this) + aligned_overhead_size();  }\n-  char* top()    const          { return bottom() + _len; }\n+  char* bottom() const          { return ((char*) this) + header_size(); }\n+  char* top() const             { return bottom() + length(); }\n@@ -105,2 +155,16 @@\n-  void* internal_amalloc(size_t x, AllocFailType alloc_failmode = AllocFailStrategy::EXIT_OOM)  {\n-    assert(is_aligned(x, BytesPerWord), \"misaligned size\");\n+#ifdef ASSERT\n+  static void zap_alignment_gap(char* p, size_t s) {\n+    if (ZapResourceArea && s > 0) {\n+      assert(s < Chunk::max_arena_alignment, \"weirdly large gap?\");\n+      ::memset(p, (int)'G', s);\n+    }\n+  }\n+#endif\n+\n+  void* internal_amalloc(size_t x, size_t alignment,\n+                         AllocFailType alloc_failmode = AllocFailStrategy::EXIT_OOM) {\n+    assert(alignment > 0 && alignment <= Chunk::max_arena_alignment, \"invalid alignment\");\n+    assert(is_aligned(_max, Chunk::max_arena_alignment), \"chunk end misaligned?\");\n+    DEBUG_ONLY(char* old_hwm = _hwm;)\n+    _hwm = align_up(_hwm, alignment);\n+    DEBUG_ONLY(zap_alignment_gap(old_hwm, _hwm - old_hwm);)\n@@ -132,6 +196,19 @@\n-  \/\/ Fast allocate in the arena.  Common case aligns to the size of jlong which is 64 bits\n-  \/\/ on both 32 and 64 bit platforms. Required for atomic jlong operations on 32 bits.\n-  void* Amalloc(size_t x, AllocFailType alloc_failmode = AllocFailStrategy::EXIT_OOM) {\n-    x = ARENA_ALIGN(x);  \/\/ note for 32 bits this should align _hwm as well.\n-    debug_only(if (UseMallocOnly) return malloc(x);)\n-    return internal_amalloc(x, alloc_failmode);\n+  \/\/ Allocate n bytes with a manually specified alignment (<= max arena alignment).\n+  void* Amalloc_aligned(size_t n, size_t alignment, AllocFailType failmode = AllocFailStrategy::EXIT_OOM) {\n+    debug_only(if (UseMallocOnly) return malloc(n);)\n+    return internal_amalloc(n, alignment, failmode);\n+  }\n+\n+  \/\/ Allocate n bytes with 64-bit alignment\n+  void* Amalloc64(size_t n, AllocFailType failmode = AllocFailStrategy::EXIT_OOM) {\n+    return Amalloc_aligned(n, sizeof(uint64_t), failmode);\n+  }\n+\n+  \/\/ Allocate n bytes with 32-bit alignment\n+  void* Amalloc32(size_t n, AllocFailType failmode = AllocFailStrategy::EXIT_OOM) {\n+    return Amalloc_aligned(n, sizeof(uint32_t), failmode);\n+  }\n+\n+  \/\/ Allocate n bytes with default alignment (which is 64 bit on both 32\/64-bit platforms)\n+  void* Amalloc(size_t n, AllocFailType failmode = AllocFailStrategy::EXIT_OOM) {\n+    return Amalloc64(n, failmode);\n@@ -140,6 +217,3 @@\n-  \/\/ Allocate in the arena, assuming the size has been aligned to size of pointer, which\n-  \/\/ is 4 bytes on 32 bits, hence the name.\n-  void* AmallocWords(size_t x, AllocFailType alloc_failmode = AllocFailStrategy::EXIT_OOM) {\n-    assert(is_aligned(x, BytesPerWord), \"misaligned size\");\n-    debug_only(if (UseMallocOnly) return malloc(x);)\n-    return internal_amalloc(x, alloc_failmode);\n+  \/\/ Allocate n bytes with word alignment (32\/64 bits on 32\/64 bit platform)\n+  void* AmallocWords(size_t n, AllocFailType failmode = AllocFailStrategy::EXIT_OOM) {\n+    return Amalloc_aligned(n, sizeof(void*), failmode);\n@@ -166,0 +240,2 @@\n+  \/\/ Reallocate; the returned pointer is guaranteed to be aligned to the original alignment.\n+  \/\/ Note that this is a potentially wasteful operation since the old allocation may just leak.\n","filename":"src\/hotspot\/share\/memory\/arena.hpp","additions":103,"deletions":27,"binary":false,"changes":130,"status":"modified"},{"patch":"@@ -37,1 +37,1 @@\n-    char** save = (char**)internal_amalloc(sizeof(char*));\n+    char** save = (char**)internal_amalloc(sizeof(char*), sizeof(char*));\n","filename":"src\/hotspot\/share\/memory\/resourceArea.inline.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -196,1 +196,1 @@\n-    oop* handle = (oop*)internal_amalloc(oopSize);\n+    oop* handle = (oop*)internal_amalloc(oopSize, oopSize);\n","filename":"src\/hotspot\/share\/runtime\/handles.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -0,0 +1,413 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2021 SAP SE. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"memory\/allocation.hpp\"\n+#include \"memory\/arena.hpp\"\n+#include \"runtime\/os.hpp\"\n+#include \"unittest.hpp\"\n+\n+#define ASSERT_NOT_NULL(p) ASSERT_NE(p, (char*)NULL)\n+#define ASSERT_NULL(p) ASSERT_EQ(p, (char*)NULL)\n+\n+#define ASSERT_CONTAINS(ar, p) ASSERT_TRUE(ar.contains(p))\n+\n+#define ASSERT_ALIGN(p, n) ASSERT_TRUE(is_aligned(p, n))\n+#define ASSERT_ALIGN_64(p) ASSERT_ALIGN(p, 8)\n+#define ASSERT_ALIGN_32(p) ASSERT_ALIGN(p, 4)\n+#define ASSERT_ALIGN_X(p) ASSERT_ALIGN(p, sizeof(void*))\n+\n+\/\/ #define LOG(s) tty->print_cr s;\n+#define LOG(s)\n+\n+\/\/ Given a memory range, check that it is filled with the expected byte.\n+\/\/ If not, print the surrounding bytes as hex and return false.\n+static bool check_range(const void* p, size_t s, int expected) {\n+\n+  \/\/ Omit the test for NULL or 0 ranges\n+  if (p == NULL || s == 0) {\n+    return true;\n+  }\n+\n+  const char* first_wrong = NULL;\n+  char* p2 = (char*)p;\n+  const char* const end = p2 + s;\n+  while (p2 < end) {\n+    if (*p2 != (char)expected) {\n+      first_wrong = p2;\n+      break;\n+    }\n+    p2 ++;\n+  }\n+\n+  if (first_wrong != NULL) {\n+    tty->print_cr(\"wrong pattern around \" PTR_FORMAT, p2i(first_wrong));\n+    os::print_hex_dump(tty, (address)(align_down(p2, 0x10) - 0x10),\n+                            (address)(align_up(end, 0x10) + 0x10), 1);\n+  }\n+\n+  return first_wrong == NULL;\n+}\n+\n+\/\/ We use this to fill the allocated ranges with a pattern to test for overwriters later.\n+static void mark_range(void* p, size_t s, int mark) {\n+  const char m = (char)((mark + 1) % 10); \/\/ valid marks are [1...10]\n+  if (p != NULL && s > 0) {\n+    ::memset(p, m, s);\n+  }\n+}\n+\n+\/\/ Check a range marked with mark_range()\n+static bool check_marked_range(const void* p, size_t s, int expected_mark) {\n+  const char m = (char)((expected_mark + 1) % 10); \/\/ valid marks are [1...10]\n+  return check_range(p, s, m);\n+}\n+\n+#define ASSERT_RANGE_IS_MARKED(p, size, mark) ASSERT_TRUE(check_marked_range(p, size, mark))\n+static const int some_random_mark = 4711;\n+\n+\/\/ Helper to check the arena alignment gap pattern.\n+\/\/ In debug, if arena allocation causes gaps due to alignment, a pattern is written into that gap:\n+\/\/\n+\/\/ |------------------|----|-------\n+\/\/ | first            |'G' | second\n+\/\/ | allocation       |    | allocation\n+\/\/ |------------------|----|-------\n+\/\/\n+\/\/ This function checks that pattern.\n+#ifdef ASSERT\n+static bool check_alignment_gap_pattern(const void* p1, size_t first_allocation_size, const void* p2) {\n+\n+  \/\/ Omit the test if one of them is NULL\n+  if (p1 == NULL || p2 == NULL) {\n+    return true;\n+  }\n+\n+  \/\/ Omit the test if it looks like the pointers are completely unrelated - p2 may live in a new Chunk.\n+  if ((p2 > p1 && pointer_delta(p2, p1, 1) >= sizeof(Chunk)) ||\n+      (p1 > p2 && pointer_delta(p1, p2, 1) >= sizeof(Chunk))) {\n+    return true;\n+  }\n+\n+  const char GAP_PATTERN = 'G'; \/\/ see arena.hpp\n+  const char* gap = ((const char*)p1) + first_allocation_size;\n+  assert(gap <= p2, \"sanity\");\n+  const size_t len = ((const char*)p2) - gap;\n+\n+  if (len > 0) {\n+    if (check_range(gap, len, GAP_PATTERN) == false) {\n+      return false;\n+    }\n+    \/\/ Test the first byte after the gap too: should *not* be \"G\" since mark_range() does not use that pattern\n+    if (gap[len] == GAP_PATTERN) {\n+      return false;\n+    }\n+  }\n+\n+  return true;\n+}\n+#define ASSERT_GAP_PATTERN(p1, size1, p2) ASSERT_TRUE(check_alignment_gap_pattern(p1, size1, p2));\n+#else\n+#define ASSERT_GAP_PATTERN(p1, size1, p2)\n+#endif\n+\n+\n+\/\/ Note: any tests making assumptions about the placement of allocations needs to\n+\/\/ be guarded from running with UseMallocOnly.\n+\n+TEST_VM(Arena, alloc_alignment_32) {\n+  Arena ar(mtTest);\n+  void* p1 = ar.Amalloc(1);   \/\/ just one byte, leaves _hwm unaligned at offset 1\n+  void* p2 = ar.Amalloc32(1); \/\/ Another byte, 32bit aligned\n+  ASSERT_NOT_NULL(p2);\n+  ASSERT_ALIGN_32(p2);\n+  ASSERT_GAP_PATTERN(p1, 1, p2);\n+  if (!UseMallocOnly) {\n+    ASSERT_EQ(pointer_delta(p2, p1, 1), sizeof(uint32_t));\n+  }\n+}\n+\n+TEST_VM(Arena, alloc_alignment_64) {\n+  Arena ar(mtTest);\n+  void* p1 = ar.Amalloc(1);   \/\/ just one byte, leaves _hwm unaligned at offset 1\n+  void* p2 = ar.Amalloc64(1); \/\/ Another byte, 64bit aligned\n+  ASSERT_NOT_NULL(p2);\n+  ASSERT_ALIGN_64(p2);\n+  ASSERT_GAP_PATTERN(p1, 1, p2);\n+  if (!UseMallocOnly) {\n+    ASSERT_EQ(pointer_delta(p2, p1, 1), sizeof(uint64_t));\n+  }\n+}\n+\n+TEST_VM(Arena, alloc_alignment_x) {\n+  Arena ar(mtTest);\n+  void* p1 = ar.Amalloc(1);      \/\/ just one byte, leaves _hwm unaligned at offset 1\n+  void* p2 = ar.AmallocWords(1); \/\/ Another byte, aligned to void* size\n+  ASSERT_NOT_NULL(p2);\n+  ASSERT_ALIGN_X(p2);\n+  ASSERT_GAP_PATTERN(p1, 1, p2);\n+  if (!UseMallocOnly) {\n+    ASSERT_EQ(pointer_delta(p2, p1, 1), sizeof(void*));\n+  }\n+}\n+\n+TEST_VM(Arena, alloc_default_alignment) {\n+  Arena ar(mtTest);\n+  void* p1 = ar.Amalloc(1);   \/\/ just one byte, leaves _hwm unaligned at offset 1\n+  void* p2 = ar.Amalloc(1);   \/\/ Another byte, 64bit (default) aligned\n+  ASSERT_NOT_NULL(p2);\n+  ASSERT_ALIGN_64(p2);        \/\/ default alignment is 64bit on all platforms\n+  ASSERT_GAP_PATTERN(p1, 1, p2);\n+  if (!UseMallocOnly) {\n+    ASSERT_EQ(pointer_delta(p2, p1, 1), sizeof(uint64_t));\n+  }\n+}\n+\n+TEST_VM(Arena, alloc_size_0) {\n+  \/\/ Amalloc(0) returns a non-NULL pointer. Note that in contrast to malloc(0), that pointer is *not* unique.\n+  \/\/ There is code in the hotpot relying on RA allocations with size 0 being successful.\n+  Arena ar(mtTest);\n+  void* p = ar.Amalloc32(0);\n+  ASSERT_NOT_NULL(p);\n+}\n+\n+\/\/ Check Arena.Afree: the free'd allocation (non-top) should be\n+\/\/ zapped (debug only), surrounding blocks should be unaffected.\n+TEST_VM(Arena, free) {\n+  Arena ar(mtTest);\n+\n+  void* p_before = ar.Amalloc(0x10);\n+  ASSERT_NOT_NULL(p_before);\n+  mark_range(p_before, 0x10, some_random_mark);\n+\n+  void* p = ar.Amalloc(0x10);\n+  ASSERT_NOT_NULL(p);\n+  mark_range(p, 0x10, some_random_mark + 1);\n+\n+  void* p_after = ar.Amalloc(0x10);\n+  ASSERT_NOT_NULL(p_after);\n+  mark_range(p_after, 0x10, some_random_mark);\n+\n+  ASSERT_RANGE_IS_MARKED(p_before, 0x10, some_random_mark);\n+  ASSERT_RANGE_IS_MARKED(p, 0x10, some_random_mark + 1);\n+  ASSERT_RANGE_IS_MARKED(p_before, 0x10, some_random_mark);\n+\n+  ar.Afree(p, 0x10);\n+\n+  ASSERT_RANGE_IS_MARKED(p_before, 0x10, some_random_mark);\n+#ifdef ASSERT\n+  ASSERT_TRUE(check_range(p, 0x10, badResourceValue));\n+#endif\n+  ASSERT_RANGE_IS_MARKED(p_before, 0x10, some_random_mark);\n+}\n+\n+\/\/ In-place shrinking.\n+TEST_VM(Arena, realloc_top_shrink) {\n+  if (!UseMallocOnly) {\n+    Arena ar(mtTest);\n+\n+    void* p1 = ar.Amalloc(0x200);\n+    ASSERT_NOT_NULL(p1);\n+    ASSERT_ALIGN_64(p1);\n+    mark_range(p1, 0x200, some_random_mark);\n+\n+    void* p2 = ar.Arealloc(p1, 0x200, 0x100);\n+    ASSERT_EQ(p1, p2);\n+    ASSERT_RANGE_IS_MARKED(p2, 0x100, some_random_mark); \/\/ realloc should preserve old content\n+\n+    \/\/ A subsequent allocation should be placed right after the shrunk first allocation\n+    void* p3 = ar.Amalloc(1);\n+    ASSERT_EQ(p3, ((char*)p2) + 0x100);\n+  }\n+}\n+\n+\/\/ not-in-place shrinking.\n+TEST_VM(Arena, realloc_nontop_shrink) {\n+  Arena ar(mtTest);\n+\n+  void* p1 = ar.Amalloc(200);\n+  ASSERT_NOT_NULL(p1);\n+  ASSERT_ALIGN_64(p1);\n+  mark_range(p1, 200, some_random_mark);\n+\n+  void* p_other = ar.Amalloc(20); \/\/ new top, p1 not top anymore\n+\n+  void* p2 = ar.Arealloc(p1, 200, 100);\n+  if (!UseMallocOnly) {\n+    ASSERT_EQ(p1, p2); \/\/ should still shrink in place\n+  }\n+  ASSERT_RANGE_IS_MARKED(p2, 100, some_random_mark); \/\/ realloc should preserve old content\n+}\n+\n+\/\/ in-place growing.\n+TEST_VM(Arena, realloc_top_grow) {\n+  Arena ar(mtTest);\n+\n+  void* p1 = ar.Amalloc(10);\n+  ASSERT_NOT_NULL(p1);\n+  ASSERT_ALIGN_64(p1);\n+  mark_range(p1, 10, some_random_mark);\n+\n+  void* p2 = ar.Arealloc(p1, 10, 20);\n+  if (!UseMallocOnly) {\n+    ASSERT_EQ(p1, p2); \/\/ The sizes should be small enough to be able to grow in place\n+  }\n+  ASSERT_RANGE_IS_MARKED(p2, 10, some_random_mark); \/\/ realloc should preserve old content\n+}\n+\n+\/\/ not-in-place growing.\n+TEST_VM(Arena, realloc_nontop_grow) {\n+  Arena ar(mtTest);\n+\n+  void* p1 = ar.Amalloc(10);\n+  ASSERT_NOT_NULL(p1);\n+  ASSERT_ALIGN_64(p1);\n+  mark_range(p1, 10, some_random_mark);\n+\n+  void* p_other = ar.Amalloc(20); \/\/ new top, p1 not top anymore\n+\n+  void* p2 = ar.Arealloc(p1, 10, 20);\n+  ASSERT_NOT_NULL(p2);\n+  ASSERT_ALIGN_64(p2);\n+  ASSERT_RANGE_IS_MARKED(p2, 10, some_random_mark); \/\/ realloc should preserve old content\n+}\n+\n+\/\/ realloc size 0 frees the allocation\n+TEST_VM(Arena, realloc_size_0) {\n+  \/\/ realloc to 0 is equivalent to free\n+  Arena ar(mtTest);\n+\n+  void* p1 = ar.Amalloc(200);\n+  ASSERT_NOT_NULL(p1);\n+  ASSERT_ALIGN_64(p1);\n+\n+  void* p2 = ar.Arealloc(p1, 200, 0); \/\/ -> Afree, completely roll back old allocation\n+  ASSERT_NULL(p2); \/\/ ... and should return NULL\n+\n+  \/\/ a subsequent allocation should get the same pointer\n+  if (!UseMallocOnly) {\n+    void* p3 = ar.Amalloc(1);\n+    ASSERT_EQ(p3, p1);\n+  }\n+}\n+\n+\/\/ Realloc equal sizes is a noop\n+TEST_VM(Arena, realloc_same_size) {\n+  Arena ar(mtTest);\n+  void* p1 = ar.Amalloc(0x200);\n+  ASSERT_NOT_NULL(p1);\n+  ASSERT_ALIGN_64(p1);\n+  mark_range(p1, 0x200, some_random_mark);\n+\n+  void* p2 = ar.Arealloc(p1, 0x200, 0x200);\n+\n+  if (!UseMallocOnly) {\n+    ASSERT_EQ(p2, p1);\n+  }\n+  ASSERT_RANGE_IS_MARKED(p2, 0x200, some_random_mark);\n+}\n+\n+\/\/ -------- random alloc test -------------\n+TEST_VM(Arena, random_allocs) {\n+\n+  \/\/ Randomly allocate with random sizes and random alignments;\n+  \/\/ check for overwriters. We do this a large number of times, to give\n+  \/\/ chunk handling a good workout too.\n+\n+  const int num_allocs = 250 * 1000;\n+  const int avg_alloc_size = 64;\n+\n+  void** ptrs = NEW_C_HEAP_ARRAY(void*, num_allocs, mtTest);\n+  size_t* sizes = NEW_C_HEAP_ARRAY(size_t, num_allocs, mtTest);\n+  size_t* alignments = NEW_C_HEAP_ARRAY(size_t, num_allocs, mtTest);\n+\n+  Arena ar(mtTest);\n+\n+  \/\/ Allocate\n+  for (int i = 0; i < num_allocs; i ++) {\n+    size_t s = MAX2(1, os::random() % (avg_alloc_size * 2));\n+    size_t al = ((size_t)1) << (os::random() % (LogBytesPerLong + 1));\n+    void* p = ar.Amalloc_aligned(s, al);\n+    ASSERT_NOT_NULL(p);\n+    ASSERT_CONTAINS(ar, p);\n+    ASSERT_ALIGN(p, al);\n+    ptrs[i] = p; sizes[i] = s; alignments[i] = al;\n+    mark_range(p, s, i); \/\/ canary\n+    LOG((\"[%d]: \" PTR_FORMAT \", size \" SIZE_FORMAT \", aligned \" SIZE_FORMAT,\n+         i, p2i(p), s, al));\n+  }\n+\n+  \/\/ Check pattern in allocations for overwriters.\n+  \/\/ Check gap patterns in debug.\n+  for (int i = 0; i < num_allocs; i ++) {\n+    ASSERT_RANGE_IS_MARKED(ptrs[i], sizes[i], i);\n+#ifdef ASSERT\n+    if (i > 0) {\n+      ASSERT_GAP_PATTERN(ptrs[i - 1], sizes[i - 1], ptrs[i]);\n+    }\n+#endif\n+  }\n+\n+  \/\/ realloc all of them randomly\n+  for (int i = 0; i < num_allocs; i ++) {\n+    size_t new_size = MAX2(1, os::random() % (avg_alloc_size * 2));\n+    void* p2 = ar.Arealloc(ptrs[i], sizes[i], new_size);\n+    ASSERT_NOT_NULL(p2);\n+    ASSERT_CONTAINS(ar, p2);\n+    ASSERT_ALIGN(p2, alignments[i]); \/\/ original alignment should have been preserved\n+    ASSERT_RANGE_IS_MARKED(p2, MIN2(sizes[i], new_size), i); \/\/ old content should have been preserved\n+    ptrs[i] = p2; sizes[i] = new_size;\n+    mark_range(p2, new_size, i); \/\/ canary\n+    LOG((\"[%d]: realloc \" PTR_FORMAT \", size \" SIZE_FORMAT \", aligned \" SIZE_FORMAT,\n+         i, p2i(p2), new_size, alignments[i]));\n+  }\n+\n+  \/\/ Check test pattern again\n+  \/\/  Note that we don't check the gap pattern anymore since if allocations had been shrunk in place\n+  \/\/  this now gets difficult.\n+  for (int i = 0; i < num_allocs; i ++) {\n+    ASSERT_RANGE_IS_MARKED(ptrs[i], sizes[i], i);\n+  }\n+\n+  \/\/ Randomly free a bunch of allocations.\n+  for (int i = 0; i < num_allocs; i ++) {\n+    if (os::random() % 10 == 0) {\n+      ar.Afree(ptrs[i], sizes[i]);\n+      \/\/ In debug builds the free should have filled the space with badResourceValue\n+      DEBUG_ONLY(check_range(ptrs[i], sizes[i], badResourceValue));\n+      ptrs[i] = NULL;\n+    }\n+  }\n+\n+  \/\/ Check test pattern again\n+  for (int i = 0; i < num_allocs; i ++) {\n+    ASSERT_RANGE_IS_MARKED(ptrs[i], sizes[i], i);\n+  }\n+\n+  FREE_C_HEAP_ARRAY(char*, ptrs);\n+  FREE_C_HEAP_ARRAY(size_t, sizes);\n+  FREE_C_HEAP_ARRAY(size_t, alignments);\n+\n+}\n+\n","filename":"test\/hotspot\/gtest\/memory\/test_arena.cpp","additions":413,"deletions":0,"binary":false,"changes":413,"status":"added"},{"patch":"@@ -0,0 +1,40 @@\n+\/*\n+ * Copyright (c) 2021 SAP SE. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+\/*\n+ * This runs the parts of the gtest which test hotspot arenas. We run the\n+ * tests with +UseMallocOnly (the standard case, -UseMallocOnly, is taken care\n+ * of by the standard gtests).\n+ *\/\n+\n+\/* @test id=use-malloc-only\n+ * @summary Run Arena-related gtests with +UseMallocOnly\n+ * @requires vm.debug\n+ * @library \/test\/lib\n+ * @modules java.base\/jdk.internal.misc\n+ *          java.xml\n+ * @requires vm.flagless\n+ * @run main\/native GTestWrapper --gtest_filter=Arena* -XX:+UseMallocOnly\n+ *\/\n+\n","filename":"test\/hotspot\/jtreg\/gtest\/ArenaGtests.java","additions":40,"deletions":0,"binary":false,"changes":40,"status":"added"}]}