{"files":[{"patch":"@@ -2051,27 +2051,1 @@\n-#if COMPILER2_OR_JVMCI\n-  void arraycopy_avx3_special_cases(XMMRegister xmm, KRegister mask, Register from,\n-                                    Register to, Register count, int shift,\n-                                    Register index, Register temp,\n-                                    bool use64byteVector, Label& L_entry, Label& L_exit);\n-\n-  void arraycopy_avx3_special_cases_conjoint(XMMRegister xmm, KRegister mask, Register from,\n-                                             Register to, Register start_index, Register end_index,\n-                                             Register count, int shift, Register temp,\n-                                             bool use64byteVector, Label& L_entry, Label& L_exit);\n-\n-  void copy64_masked_avx(Register dst, Register src, XMMRegister xmm,\n-                         KRegister mask, Register length, Register index,\n-                         Register temp, int shift = Address::times_1, int offset = 0,\n-                         bool use64byteVector = false);\n-\n-  void copy32_masked_avx(Register dst, Register src, XMMRegister xmm,\n-                         KRegister mask, Register length, Register index,\n-                         Register temp, int shift = Address::times_1, int offset = 0);\n-\n-  void copy32_avx(Register dst, Register src, Register index, XMMRegister xmm,\n-                  int shift = Address::times_1, int offset = 0);\n-\n-  void copy64_avx(Register dst, Register src, Register index, XMMRegister xmm,\n-                  bool conjoint, int shift = Address::times_1, int offset = 0,\n-                  bool use64byteVector = false);\n-\n+#ifdef COMPILER2_OR_JVMCI\n@@ -2080,1 +2054,0 @@\n-\n@@ -2086,1 +2059,0 @@\n-\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":1,"deletions":29,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -1,251 +0,0 @@\n-\/*\n-* Copyright (c) 2020, 2022, Intel Corporation. All rights reserved.\n-*\n-* DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n-*\n-* This code is free software; you can redistribute it and\/or modify it\n-* under the terms of the GNU General Public License version 2 only, as\n-* published by the Free Software Foundation.\n-*\n-* This code is distributed in the hope that it will be useful, but WITHOUT\n-* ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n-* FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n-* version 2 for more details (a copy is included in the LICENSE file that\n-* accompanied this code).\n-*\n-* You should have received a copy of the GNU General Public License version\n-* 2 along with this work; if not, write to the Free Software Foundation,\n-* Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n-*\n-* Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n-* or visit www.oracle.com if you need additional information or have any\n-* questions.\n-*\n-*\/\n-\n-#include \"precompiled.hpp\"\n-#include \"asm\/macroAssembler.hpp\"\n-#include \"asm\/macroAssembler.inline.hpp\"\n-#include \"compiler\/compiler_globals.hpp\"\n-\n-#ifdef PRODUCT\n-#define BLOCK_COMMENT(str) \/* nothing *\/\n-#else\n-#define BLOCK_COMMENT(str) block_comment(str)\n-#endif\n-\n-#define BIND(label) bind(label); BLOCK_COMMENT(#label \":\")\n-\n-#ifdef _LP64\n-\n-#if COMPILER2_OR_JVMCI\n-\n-void MacroAssembler::arraycopy_avx3_special_cases(XMMRegister xmm, KRegister mask, Register from,\n-                                                  Register to, Register count, int shift,\n-                                                  Register index, Register temp,\n-                                                  bool use64byteVector, Label& L_entry, Label& L_exit) {\n-  Label L_entry_64, L_entry_96, L_entry_128;\n-  Label L_entry_160, L_entry_192;\n-\n-  int size_mat[][6] = {\n-  \/* T_BYTE *\/ {32 , 64,  96 , 128 , 160 , 192 },\n-  \/* T_SHORT*\/ {16 , 32,  48 , 64  , 80  , 96  },\n-  \/* T_INT  *\/ {8  , 16,  24 , 32  , 40  , 48  },\n-  \/* T_LONG *\/ {4  ,  8,  12 , 16  , 20  , 24  }\n-  };\n-\n-  \/\/ Case A) Special case for length less than equal to 32 bytes.\n-  cmpq(count, size_mat[shift][0]);\n-  jccb(Assembler::greater, L_entry_64);\n-  copy32_masked_avx(to, from, xmm, mask, count, index, temp, shift);\n-  jmp(L_exit);\n-\n-  \/\/ Case B) Special case for length less than equal to 64 bytes.\n-  BIND(L_entry_64);\n-  cmpq(count, size_mat[shift][1]);\n-  jccb(Assembler::greater, L_entry_96);\n-  copy64_masked_avx(to, from, xmm, mask, count, index, temp, shift, 0, use64byteVector);\n-  jmp(L_exit);\n-\n-  \/\/ Case C) Special case for length less than equal to 96 bytes.\n-  BIND(L_entry_96);\n-  cmpq(count, size_mat[shift][2]);\n-  jccb(Assembler::greater, L_entry_128);\n-  copy64_avx(to, from, index, xmm, false, shift, 0, use64byteVector);\n-  subq(count, 64 >> shift);\n-  copy32_masked_avx(to, from, xmm, mask, count, index, temp, shift, 64);\n-  jmp(L_exit);\n-\n-  \/\/ Case D) Special case for length less than equal to 128 bytes.\n-  BIND(L_entry_128);\n-  cmpq(count, size_mat[shift][3]);\n-  jccb(Assembler::greater, L_entry_160);\n-  copy64_avx(to, from, index, xmm, false, shift, 0, use64byteVector);\n-  copy32_avx(to, from, index, xmm, shift, 64);\n-  subq(count, 96 >> shift);\n-  copy32_masked_avx(to, from, xmm, mask, count, index, temp, shift, 96);\n-  jmp(L_exit);\n-\n-  \/\/ Case E) Special case for length less than equal to 160 bytes.\n-  BIND(L_entry_160);\n-  cmpq(count, size_mat[shift][4]);\n-  jccb(Assembler::greater, L_entry_192);\n-  copy64_avx(to, from, index, xmm, false, shift, 0, use64byteVector);\n-  copy64_avx(to, from, index, xmm, false, shift, 64, use64byteVector);\n-  subq(count, 128 >> shift);\n-  copy32_masked_avx(to, from, xmm, mask, count, index, temp, shift, 128);\n-  jmp(L_exit);\n-\n-  \/\/ Case F) Special case for length less than equal to 192 bytes.\n-  BIND(L_entry_192);\n-  cmpq(count, size_mat[shift][5]);\n-  jcc(Assembler::greater, L_entry);\n-  copy64_avx(to, from, index, xmm, false, shift, 0, use64byteVector);\n-  copy64_avx(to, from, index, xmm, false, shift, 64, use64byteVector);\n-  copy32_avx(to, from, index, xmm, shift, 128);\n-  subq(count, 160 >> shift);\n-  copy32_masked_avx(to, from, xmm, mask, count, index, temp, shift, 160);\n-  jmp(L_exit);\n-}\n-\n-void MacroAssembler::arraycopy_avx3_special_cases_conjoint(XMMRegister xmm, KRegister mask, Register from,\n-                                                           Register to, Register start_index, Register end_index,\n-                                                           Register count, int shift, Register temp,\n-                                                           bool use64byteVector, Label& L_entry, Label& L_exit) {\n-  Label L_entry_64, L_entry_96, L_entry_128;\n-  Label L_entry_160, L_entry_192;\n-  bool avx3 = (MaxVectorSize > 32) && (VM_Version::avx3_threshold() == 0);\n-\n-  int size_mat[][6] = {\n-  \/* T_BYTE *\/ {32 , 64,  96 , 128 , 160 , 192 },\n-  \/* T_SHORT*\/ {16 , 32,  48 , 64  , 80  , 96  },\n-  \/* T_INT  *\/ {8  , 16,  24 , 32  , 40  , 48  },\n-  \/* T_LONG *\/ {4  ,  8,  12 , 16  , 20  , 24  }\n-  };\n-\n-  \/\/ Case A) Special case for length less than equal to 32 bytes.\n-  cmpq(count, size_mat[shift][0]);\n-  jccb(Assembler::greater, L_entry_64);\n-  copy32_masked_avx(to, from, xmm, mask, count, start_index, temp, shift);\n-  jmp(L_exit);\n-\n-  \/\/ Case B) Special case for length less than equal to 64 bytes.\n-  BIND(L_entry_64);\n-  cmpq(count, size_mat[shift][1]);\n-  jccb(Assembler::greater, L_entry_96);\n-  if (avx3) {\n-     copy64_masked_avx(to, from, xmm, mask, count, start_index, temp, shift, 0, true);\n-  } else {\n-     copy32_avx(to, from, end_index, xmm, shift, -32);\n-     subq(count, 32 >> shift);\n-     copy32_masked_avx(to, from, xmm, mask, count, start_index, temp, shift);\n-  }\n-  jmp(L_exit);\n-\n-  \/\/ Case C) Special case for length less than equal to 96 bytes.\n-  BIND(L_entry_96);\n-  cmpq(count, size_mat[shift][2]);\n-  jccb(Assembler::greater, L_entry_128);\n-  copy64_avx(to, from, end_index, xmm, true, shift, -64, use64byteVector);\n-  subq(count, 64 >> shift);\n-  copy32_masked_avx(to, from, xmm, mask, count, start_index, temp, shift);\n-  jmp(L_exit);\n-\n-  \/\/ Case D) Special case for length less than equal to 128 bytes.\n-  BIND(L_entry_128);\n-  cmpq(count, size_mat[shift][3]);\n-  jccb(Assembler::greater, L_entry_160);\n-  copy64_avx(to, from, end_index, xmm, true, shift, -64, use64byteVector);\n-  copy32_avx(to, from, end_index, xmm, shift, -96);\n-  subq(count, 96 >> shift);\n-  copy32_masked_avx(to, from, xmm, mask, count, start_index, temp, shift);\n-  jmp(L_exit);\n-\n-  \/\/ Case E) Special case for length less than equal to 160 bytes.\n-  BIND(L_entry_160);\n-  cmpq(count, size_mat[shift][4]);\n-  jccb(Assembler::greater, L_entry_192);\n-  copy64_avx(to, from, end_index, xmm, true, shift, -64, use64byteVector);\n-  copy64_avx(to, from, end_index, xmm, true, shift, -128, use64byteVector);\n-  subq(count, 128 >> shift);\n-  copy32_masked_avx(to, from, xmm, mask, count, start_index, temp, shift);\n-  jmp(L_exit);\n-\n-  \/\/ Case F) Special case for length less than equal to 192 bytes.\n-  BIND(L_entry_192);\n-  cmpq(count, size_mat[shift][5]);\n-  jcc(Assembler::greater, L_entry);\n-  copy64_avx(to, from, end_index, xmm, true, shift, -64, use64byteVector);\n-  copy64_avx(to, from, end_index, xmm, true, shift, -128, use64byteVector);\n-  copy32_avx(to, from, end_index, xmm, shift, -160);\n-  subq(count, 160 >> shift);\n-  copy32_masked_avx(to, from, xmm, mask, count, start_index, temp, shift);\n-  jmp(L_exit);\n-}\n-\n-void MacroAssembler::copy64_masked_avx(Register dst, Register src, XMMRegister xmm,\n-                                       KRegister mask, Register length, Register index,\n-                                       Register temp, int shift, int offset,\n-                                       bool use64byteVector) {\n-  BasicType type[] = { T_BYTE,  T_SHORT,  T_INT,   T_LONG};\n-  assert(MaxVectorSize >= 32, \"vector length should be >= 32\");\n-  if (!use64byteVector) {\n-    copy32_avx(dst, src, index, xmm, shift, offset);\n-    subptr(length, 32 >> shift);\n-    copy32_masked_avx(dst, src, xmm, mask, length, index, temp, shift, offset+32);\n-  } else {\n-    Address::ScaleFactor scale = (Address::ScaleFactor)(shift);\n-    assert(MaxVectorSize == 64, \"vector length != 64\");\n-    mov64(temp, -1L);\n-    bzhiq(temp, temp, length);\n-    kmovql(mask, temp);\n-    evmovdqu(type[shift], mask, xmm, Address(src, index, scale, offset), false, Assembler::AVX_512bit);\n-    evmovdqu(type[shift], mask, Address(dst, index, scale, offset), xmm, true, Assembler::AVX_512bit);\n-  }\n-}\n-\n-\n-void MacroAssembler::copy32_masked_avx(Register dst, Register src, XMMRegister xmm,\n-                                       KRegister mask, Register length, Register index,\n-                                       Register temp, int shift, int offset) {\n-  assert(MaxVectorSize >= 32, \"vector length should be >= 32\");\n-  BasicType type[] = { T_BYTE,  T_SHORT,  T_INT,   T_LONG};\n-  Address::ScaleFactor scale = (Address::ScaleFactor)(shift);\n-  mov64(temp, -1L);\n-  bzhiq(temp, temp, length);\n-  kmovql(mask, temp);\n-  evmovdqu(type[shift], mask, xmm, Address(src, index, scale, offset), false, Assembler::AVX_256bit);\n-  evmovdqu(type[shift], mask, Address(dst, index, scale, offset), xmm, true, Assembler::AVX_256bit);\n-}\n-\n-\n-void MacroAssembler::copy32_avx(Register dst, Register src, Register index, XMMRegister xmm,\n-                                int shift, int offset) {\n-  assert(MaxVectorSize >= 32, \"vector length should be >= 32\");\n-  Address::ScaleFactor scale = (Address::ScaleFactor)(shift);\n-  vmovdqu(xmm, Address(src, index, scale, offset));\n-  vmovdqu(Address(dst, index, scale, offset), xmm);\n-}\n-\n-\n-void MacroAssembler::copy64_avx(Register dst, Register src, Register index, XMMRegister xmm,\n-                                bool conjoint, int shift, int offset, bool use64byteVector) {\n-  assert(MaxVectorSize == 64 || MaxVectorSize == 32, \"vector length mismatch\");\n-  if (!use64byteVector) {\n-    if (conjoint) {\n-      copy32_avx(dst, src, index, xmm, shift, offset+32);\n-      copy32_avx(dst, src, index, xmm, shift, offset);\n-    } else {\n-      copy32_avx(dst, src, index, xmm, shift, offset);\n-      copy32_avx(dst, src, index, xmm, shift, offset+32);\n-    }\n-  } else {\n-    Address::ScaleFactor scale = (Address::ScaleFactor)(shift);\n-    evmovdquq(xmm, Address(src, index, scale, offset), Assembler::AVX_512bit);\n-    evmovdquq(Address(dst, index, scale, offset), xmm, Assembler::AVX_512bit);\n-  }\n-}\n-\n-#endif \/\/ COMPILER2_OR_JVMCI\n-\n-#endif\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86_arrayCopy_avx3.cpp","additions":0,"deletions":251,"binary":false,"changes":251,"status":"deleted"},{"patch":"@@ -34,1 +34,0 @@\n-#include \"oops\/objArrayKlass.hpp\"\n@@ -43,0 +42,1 @@\n+#include \"opto\/c2_globals.hpp\"\n@@ -68,27 +68,0 @@\n-#ifdef PRODUCT\n-#define INC_COUNTER_NP(counter, rscratch) ((void)0)\n-#else\n-#define INC_COUNTER_NP(counter, rscratch) \\\n-BLOCK_COMMENT(\"inc_counter \" #counter); \\\n-inc_counter_np(_masm, counter, rscratch);\n-\n-static void inc_counter_np(MacroAssembler* _masm, int& counter, Register rscratch) {\n-  __ incrementl(ExternalAddress((address)&counter), rscratch);\n-}\n-\n-#if COMPILER2_OR_JVMCI\n-static int& get_profile_ctr(int shift) {\n-  if (shift == 0) {\n-    return SharedRuntime::_jbyte_array_copy_ctr;\n-  } else if (shift == 1) {\n-    return SharedRuntime::_jshort_array_copy_ctr;\n-  } else if (shift == 2) {\n-    return SharedRuntime::_jint_array_copy_ctr;\n-  } else {\n-    assert(shift == 3, \"\");\n-    return SharedRuntime::_jlong_array_copy_ctr;\n-  }\n-}\n-#endif \/\/ COMPILER2_OR_JVMCI\n-#endif \/\/ !PRODUCT\n-\n@@ -1147,49 +1120,0 @@\n-\/\/\n-\/\/ Verify that a register contains clean 32-bits positive value\n-\/\/ (high 32-bits are 0) so it could be used in 64-bits shifts.\n-\/\/\n-\/\/  Input:\n-\/\/    Rint  -  32-bits value\n-\/\/    Rtmp  -  scratch\n-\/\/\n-void StubGenerator::assert_clean_int(Register Rint, Register Rtmp) {\n-#ifdef ASSERT\n-  Label L;\n-  assert_different_registers(Rtmp, Rint);\n-  __ movslq(Rtmp, Rint);\n-  __ cmpq(Rtmp, Rint);\n-  __ jcc(Assembler::equal, L);\n-  __ stop(\"high 32-bits of int value are not 0\");\n-  __ bind(L);\n-#endif\n-}\n-\n-\/\/  Generate overlap test for array copy stubs\n-\/\/\n-\/\/  Input:\n-\/\/     c_rarg0 - from\n-\/\/     c_rarg1 - to\n-\/\/     c_rarg2 - element count\n-\/\/\n-\/\/  Output:\n-\/\/     rax   - &from[element count - 1]\n-\/\/\n-void StubGenerator::array_overlap_test(address no_overlap_target, Label* NOLp, Address::ScaleFactor sf) {\n-  const Register from     = c_rarg0;\n-  const Register to       = c_rarg1;\n-  const Register count    = c_rarg2;\n-  const Register end_from = rax;\n-\n-  __ cmpptr(to, from);\n-  __ lea(end_from, Address(from, count, sf, 0));\n-  if (NOLp == NULL) {\n-    ExternalAddress no_overlap(no_overlap_target);\n-    __ jump_cc(Assembler::belowEqual, no_overlap);\n-    __ cmpptr(to, end_from);\n-    __ jump_cc(Assembler::aboveEqual, no_overlap);\n-  } else {\n-    __ jcc(Assembler::belowEqual, (*NOLp));\n-    __ cmpptr(to, end_from);\n-    __ jcc(Assembler::aboveEqual, (*NOLp));\n-  }\n-}\n@@ -1231,0 +1155,1 @@\n+\n@@ -1241,0 +1166,1 @@\n+\n@@ -1263,0 +1189,1 @@\n+\n@@ -1274,144 +1201,0 @@\n-\/\/ Copy big chunks forward\n-\/\/\n-\/\/ Inputs:\n-\/\/   end_from     - source arrays end address\n-\/\/   end_to       - destination array end address\n-\/\/   qword_count  - 64-bits element count, negative\n-\/\/   to           - scratch\n-\/\/   L_copy_bytes - entry label\n-\/\/   L_copy_8_bytes  - exit  label\n-\/\/\n-void StubGenerator::copy_bytes_forward(Register end_from, Register end_to,\n-                                       Register qword_count, Register to,\n-                                       Label& L_copy_bytes, Label& L_copy_8_bytes) {\n-  DEBUG_ONLY(__ stop(\"enter at entry label, not here\"));\n-  Label L_loop;\n-  __ align(OptoLoopAlignment);\n-  if (UseUnalignedLoadStores) {\n-    Label L_end;\n-    __ BIND(L_loop);\n-    if (UseAVX >= 2) {\n-      __ vmovdqu(xmm0, Address(end_from, qword_count, Address::times_8, -56));\n-      __ vmovdqu(Address(end_to, qword_count, Address::times_8, -56), xmm0);\n-      __ vmovdqu(xmm1, Address(end_from, qword_count, Address::times_8, -24));\n-      __ vmovdqu(Address(end_to, qword_count, Address::times_8, -24), xmm1);\n-    } else {\n-      __ movdqu(xmm0, Address(end_from, qword_count, Address::times_8, -56));\n-      __ movdqu(Address(end_to, qword_count, Address::times_8, -56), xmm0);\n-      __ movdqu(xmm1, Address(end_from, qword_count, Address::times_8, -40));\n-      __ movdqu(Address(end_to, qword_count, Address::times_8, -40), xmm1);\n-      __ movdqu(xmm2, Address(end_from, qword_count, Address::times_8, -24));\n-      __ movdqu(Address(end_to, qword_count, Address::times_8, -24), xmm2);\n-      __ movdqu(xmm3, Address(end_from, qword_count, Address::times_8, - 8));\n-      __ movdqu(Address(end_to, qword_count, Address::times_8, - 8), xmm3);\n-    }\n-\n-    __ BIND(L_copy_bytes);\n-    __ addptr(qword_count, 8);\n-    __ jcc(Assembler::lessEqual, L_loop);\n-    __ subptr(qword_count, 4);  \/\/ sub(8) and add(4)\n-    __ jccb(Assembler::greater, L_end);\n-    \/\/ Copy trailing 32 bytes\n-    if (UseAVX >= 2) {\n-      __ vmovdqu(xmm0, Address(end_from, qword_count, Address::times_8, -24));\n-      __ vmovdqu(Address(end_to, qword_count, Address::times_8, -24), xmm0);\n-    } else {\n-      __ movdqu(xmm0, Address(end_from, qword_count, Address::times_8, -24));\n-      __ movdqu(Address(end_to, qword_count, Address::times_8, -24), xmm0);\n-      __ movdqu(xmm1, Address(end_from, qword_count, Address::times_8, - 8));\n-      __ movdqu(Address(end_to, qword_count, Address::times_8, - 8), xmm1);\n-    }\n-    __ addptr(qword_count, 4);\n-    __ BIND(L_end);\n-  } else {\n-    \/\/ Copy 32-bytes per iteration\n-    __ BIND(L_loop);\n-    __ movq(to, Address(end_from, qword_count, Address::times_8, -24));\n-    __ movq(Address(end_to, qword_count, Address::times_8, -24), to);\n-    __ movq(to, Address(end_from, qword_count, Address::times_8, -16));\n-    __ movq(Address(end_to, qword_count, Address::times_8, -16), to);\n-    __ movq(to, Address(end_from, qword_count, Address::times_8, - 8));\n-    __ movq(Address(end_to, qword_count, Address::times_8, - 8), to);\n-    __ movq(to, Address(end_from, qword_count, Address::times_8, - 0));\n-    __ movq(Address(end_to, qword_count, Address::times_8, - 0), to);\n-\n-    __ BIND(L_copy_bytes);\n-    __ addptr(qword_count, 4);\n-    __ jcc(Assembler::lessEqual, L_loop);\n-  }\n-  __ subptr(qword_count, 4);\n-  __ jcc(Assembler::less, L_copy_8_bytes); \/\/ Copy trailing qwords\n-}\n-\n-\/\/ Copy big chunks backward\n-\/\/\n-\/\/ Inputs:\n-\/\/   from         - source arrays address\n-\/\/   dest         - destination array address\n-\/\/   qword_count  - 64-bits element count\n-\/\/   to           - scratch\n-\/\/   L_copy_bytes - entry label\n-\/\/   L_copy_8_bytes  - exit  label\n-\/\/\n-void StubGenerator::copy_bytes_backward(Register from, Register dest,\n-                                        Register qword_count, Register to,\n-                                        Label& L_copy_bytes, Label& L_copy_8_bytes) {\n-  DEBUG_ONLY(__ stop(\"enter at entry label, not here\"));\n-  Label L_loop;\n-  __ align(OptoLoopAlignment);\n-  if (UseUnalignedLoadStores) {\n-    Label L_end;\n-    __ BIND(L_loop);\n-    if (UseAVX >= 2) {\n-      __ vmovdqu(xmm0, Address(from, qword_count, Address::times_8, 32));\n-      __ vmovdqu(Address(dest, qword_count, Address::times_8, 32), xmm0);\n-      __ vmovdqu(xmm1, Address(from, qword_count, Address::times_8,  0));\n-      __ vmovdqu(Address(dest, qword_count, Address::times_8,  0), xmm1);\n-    } else {\n-      __ movdqu(xmm0, Address(from, qword_count, Address::times_8, 48));\n-      __ movdqu(Address(dest, qword_count, Address::times_8, 48), xmm0);\n-      __ movdqu(xmm1, Address(from, qword_count, Address::times_8, 32));\n-      __ movdqu(Address(dest, qword_count, Address::times_8, 32), xmm1);\n-      __ movdqu(xmm2, Address(from, qword_count, Address::times_8, 16));\n-      __ movdqu(Address(dest, qword_count, Address::times_8, 16), xmm2);\n-      __ movdqu(xmm3, Address(from, qword_count, Address::times_8,  0));\n-      __ movdqu(Address(dest, qword_count, Address::times_8,  0), xmm3);\n-    }\n-\n-    __ BIND(L_copy_bytes);\n-    __ subptr(qword_count, 8);\n-    __ jcc(Assembler::greaterEqual, L_loop);\n-\n-    __ addptr(qword_count, 4);  \/\/ add(8) and sub(4)\n-    __ jccb(Assembler::less, L_end);\n-    \/\/ Copy trailing 32 bytes\n-    if (UseAVX >= 2) {\n-      __ vmovdqu(xmm0, Address(from, qword_count, Address::times_8, 0));\n-      __ vmovdqu(Address(dest, qword_count, Address::times_8, 0), xmm0);\n-    } else {\n-      __ movdqu(xmm0, Address(from, qword_count, Address::times_8, 16));\n-      __ movdqu(Address(dest, qword_count, Address::times_8, 16), xmm0);\n-      __ movdqu(xmm1, Address(from, qword_count, Address::times_8,  0));\n-      __ movdqu(Address(dest, qword_count, Address::times_8,  0), xmm1);\n-    }\n-    __ subptr(qword_count, 4);\n-    __ BIND(L_end);\n-  } else {\n-    \/\/ Copy 32-bytes per iteration\n-    __ BIND(L_loop);\n-    __ movq(to, Address(from, qword_count, Address::times_8, 24));\n-    __ movq(Address(dest, qword_count, Address::times_8, 24), to);\n-    __ movq(to, Address(from, qword_count, Address::times_8, 16));\n-    __ movq(Address(dest, qword_count, Address::times_8, 16), to);\n-    __ movq(to, Address(from, qword_count, Address::times_8,  8));\n-    __ movq(Address(dest, qword_count, Address::times_8,  8), to);\n-    __ movq(to, Address(from, qword_count, Address::times_8,  0));\n-    __ movq(Address(dest, qword_count, Address::times_8,  0), to);\n-\n-    __ BIND(L_copy_bytes);\n-    __ subptr(qword_count, 4);\n-    __ jcc(Assembler::greaterEqual, L_loop);\n-  }\n-  __ addptr(qword_count, 4);\n-  __ jcc(Assembler::greater, L_copy_8_bytes); \/\/ Copy trailing qwords\n-}\n@@ -1429,0 +1212,1 @@\n+\n@@ -1437,1953 +1221,0 @@\n-#if COMPILER2_OR_JVMCI\n-\/\/ Note: Following rules apply to AVX3 optimized arraycopy stubs:-\n-\/\/ - If target supports AVX3 features (BW+VL+F) then implementation uses 32 byte vectors (YMMs)\n-\/\/   for both special cases (various small block sizes) and aligned copy loop. This is the\n-\/\/   default configuration.\n-\/\/ - If copy length is above AVX3Threshold, then implementation use 64 byte vectors (ZMMs)\n-\/\/   for main copy loop (and subsequent tail) since bulk of the cycles will be consumed in it.\n-\/\/ - If user forces MaxVectorSize=32 then above 4096 bytes its seen that REP MOVs shows a\n-\/\/   better performance for disjoint copies. For conjoint\/backward copy vector based\n-\/\/   copy performs better.\n-\/\/ - If user sets AVX3Threshold=0, then special cases for small blocks sizes operate over\n-\/\/   64 byte vector registers (ZMMs).\n-\n-\/\/ Inputs:\n-\/\/   c_rarg0   - source array address\n-\/\/   c_rarg1   - destination array address\n-\/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n-\/\/\n-\/\/\n-\/\/ Side Effects:\n-\/\/   disjoint_copy_avx3_masked is set to the no-overlap entry point\n-\/\/   used by generate_conjoint_[byte\/int\/short\/long]_copy().\n-\/\/\n-address StubGenerator::generate_disjoint_copy_avx3_masked(address* entry, const char *name,\n-                                                          int shift, bool aligned, bool is_oop,\n-                                                          bool dest_uninitialized) {\n-  __ align(CodeEntryAlignment);\n-  StubCodeMark mark(this, \"StubRoutines\", name);\n-  address start = __ pc();\n-\n-  int avx3threshold = VM_Version::avx3_threshold();\n-  bool use64byteVector = (MaxVectorSize > 32) && (avx3threshold == 0);\n-  Label L_main_loop, L_main_loop_64bytes, L_tail, L_tail64, L_exit, L_entry;\n-  Label L_repmovs, L_main_pre_loop, L_main_pre_loop_64bytes, L_pre_main_post_64;\n-  const Register from        = rdi;  \/\/ source array address\n-  const Register to          = rsi;  \/\/ destination array address\n-  const Register count       = rdx;  \/\/ elements count\n-  const Register temp1       = r8;\n-  const Register temp2       = r11;\n-  const Register temp3       = rax;\n-  const Register temp4       = rcx;\n-  \/\/ End pointers are inclusive, and if count is not zero they point\n-  \/\/ to the last unit copied:  end_to[0] := end_from[0]\n-\n-  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-  assert_clean_int(c_rarg2, rax);    \/\/ Make sure 'count' is clean int.\n-\n-  if (entry != NULL) {\n-    *entry = __ pc();\n-     \/\/ caller can pass a 64-bit byte count here (from Unsafe.copyMemory)\n-    BLOCK_COMMENT(\"Entry:\");\n-  }\n-\n-  BasicType type_vec[] = { T_BYTE,  T_SHORT,  T_INT,   T_LONG};\n-  BasicType type = is_oop ? T_OBJECT : type_vec[shift];\n-\n-  setup_argument_regs(type);\n-\n-  DecoratorSet decorators = IN_HEAP | IS_ARRAY | ARRAYCOPY_DISJOINT;\n-  if (dest_uninitialized) {\n-    decorators |= IS_DEST_UNINITIALIZED;\n-  }\n-  if (aligned) {\n-    decorators |= ARRAYCOPY_ALIGNED;\n-  }\n-  BarrierSetAssembler *bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-  bs->arraycopy_prologue(_masm, decorators, type, from, to, count);\n-\n-  {\n-    \/\/ Type(shift)           byte(0), short(1), int(2),   long(3)\n-    int loop_size[]        = { 192,     96,       48,      24};\n-    int threshold[]        = { 4096,    2048,     1024,    512};\n-\n-    \/\/ UnsafeCopyMemory page error: continue after ucm\n-    UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n-    \/\/ 'from', 'to' and 'count' are now valid\n-\n-    \/\/ temp1 holds remaining count and temp4 holds running count used to compute\n-    \/\/ next address offset for start of to\/from addresses (temp4 * scale).\n-    __ mov64(temp4, 0);\n-    __ movq(temp1, count);\n-\n-    \/\/ Zero length check.\n-    __ BIND(L_tail);\n-    __ cmpq(temp1, 0);\n-    __ jcc(Assembler::lessEqual, L_exit);\n-\n-    \/\/ Special cases using 32 byte [masked] vector copy operations.\n-    __ arraycopy_avx3_special_cases(xmm1, k2, from, to, temp1, shift,\n-                                    temp4, temp3, use64byteVector, L_entry, L_exit);\n-\n-    \/\/ PRE-MAIN-POST loop for aligned copy.\n-    __ BIND(L_entry);\n-\n-    if (avx3threshold != 0) {\n-      __ cmpq(count, threshold[shift]);\n-      if (MaxVectorSize == 64) {\n-        \/\/ Copy using 64 byte vectors.\n-        __ jcc(Assembler::greaterEqual, L_pre_main_post_64);\n-      } else {\n-        assert(MaxVectorSize < 64, \"vector size should be < 64 bytes\");\n-        \/\/ REP MOVS offer a faster copy path.\n-        __ jcc(Assembler::greaterEqual, L_repmovs);\n-      }\n-    }\n-\n-    if ((MaxVectorSize < 64)  || (avx3threshold != 0)) {\n-      \/\/ Partial copy to make dst address 32 byte aligned.\n-      __ movq(temp2, to);\n-      __ andq(temp2, 31);\n-      __ jcc(Assembler::equal, L_main_pre_loop);\n-\n-      __ negptr(temp2);\n-      __ addq(temp2, 32);\n-      if (shift) {\n-        __ shrq(temp2, shift);\n-      }\n-      __ movq(temp3, temp2);\n-      __ copy32_masked_avx(to, from, xmm1, k2, temp3, temp4, temp1, shift);\n-      __ movq(temp4, temp2);\n-      __ movq(temp1, count);\n-      __ subq(temp1, temp2);\n-\n-      __ cmpq(temp1, loop_size[shift]);\n-      __ jcc(Assembler::less, L_tail);\n-\n-      __ BIND(L_main_pre_loop);\n-      __ subq(temp1, loop_size[shift]);\n-\n-      \/\/ Main loop with aligned copy block size of 192 bytes at 32 byte granularity.\n-      __ align32();\n-      __ BIND(L_main_loop);\n-         __ copy64_avx(to, from, temp4, xmm1, false, shift, 0);\n-         __ copy64_avx(to, from, temp4, xmm1, false, shift, 64);\n-         __ copy64_avx(to, from, temp4, xmm1, false, shift, 128);\n-         __ addptr(temp4, loop_size[shift]);\n-         __ subq(temp1, loop_size[shift]);\n-         __ jcc(Assembler::greater, L_main_loop);\n-\n-      __ addq(temp1, loop_size[shift]);\n-\n-      \/\/ Tail loop.\n-      __ jmp(L_tail);\n-\n-      __ BIND(L_repmovs);\n-        __ movq(temp2, temp1);\n-        \/\/ Swap to(RSI) and from(RDI) addresses to comply with REP MOVs semantics.\n-        __ movq(temp3, to);\n-        __ movq(to,  from);\n-        __ movq(from, temp3);\n-        \/\/ Save to\/from for restoration post rep_mov.\n-        __ movq(temp1, to);\n-        __ movq(temp3, from);\n-        if(shift < 3) {\n-          __ shrq(temp2, 3-shift);     \/\/ quad word count\n-        }\n-        __ movq(temp4 , temp2);        \/\/ move quad ward count into temp4(RCX).\n-        __ rep_mov();\n-        __ shlq(temp2, 3);             \/\/ convert quad words into byte count.\n-        if(shift) {\n-          __ shrq(temp2, shift);       \/\/ type specific count.\n-        }\n-        \/\/ Restore original addresses in to\/from.\n-        __ movq(to, temp3);\n-        __ movq(from, temp1);\n-        __ movq(temp4, temp2);\n-        __ movq(temp1, count);\n-        __ subq(temp1, temp2);         \/\/ tailing part (less than a quad ward size).\n-        __ jmp(L_tail);\n-    }\n-\n-    if (MaxVectorSize > 32) {\n-      __ BIND(L_pre_main_post_64);\n-      \/\/ Partial copy to make dst address 64 byte aligned.\n-      __ movq(temp2, to);\n-      __ andq(temp2, 63);\n-      __ jcc(Assembler::equal, L_main_pre_loop_64bytes);\n-\n-      __ negptr(temp2);\n-      __ addq(temp2, 64);\n-      if (shift) {\n-        __ shrq(temp2, shift);\n-      }\n-      __ movq(temp3, temp2);\n-      __ copy64_masked_avx(to, from, xmm1, k2, temp3, temp4, temp1, shift, 0 , true);\n-      __ movq(temp4, temp2);\n-      __ movq(temp1, count);\n-      __ subq(temp1, temp2);\n-\n-      __ cmpq(temp1, loop_size[shift]);\n-      __ jcc(Assembler::less, L_tail64);\n-\n-      __ BIND(L_main_pre_loop_64bytes);\n-      __ subq(temp1, loop_size[shift]);\n-\n-      \/\/ Main loop with aligned copy block size of 192 bytes at\n-      \/\/ 64 byte copy granularity.\n-      __ align32();\n-      __ BIND(L_main_loop_64bytes);\n-         __ copy64_avx(to, from, temp4, xmm1, false, shift, 0 , true);\n-         __ copy64_avx(to, from, temp4, xmm1, false, shift, 64, true);\n-         __ copy64_avx(to, from, temp4, xmm1, false, shift, 128, true);\n-         __ addptr(temp4, loop_size[shift]);\n-         __ subq(temp1, loop_size[shift]);\n-         __ jcc(Assembler::greater, L_main_loop_64bytes);\n-\n-      __ addq(temp1, loop_size[shift]);\n-      \/\/ Zero length check.\n-      __ jcc(Assembler::lessEqual, L_exit);\n-\n-      __ BIND(L_tail64);\n-\n-      \/\/ Tail handling using 64 byte [masked] vector copy operations.\n-      use64byteVector = true;\n-      __ arraycopy_avx3_special_cases(xmm1, k2, from, to, temp1, shift,\n-                                      temp4, temp3, use64byteVector, L_entry, L_exit);\n-    }\n-    __ BIND(L_exit);\n-  }\n-\n-  address ucme_exit_pc = __ pc();\n-  \/\/ When called from generic_arraycopy r11 contains specific values\n-  \/\/ used during arraycopy epilogue, re-initializing r11.\n-  if (is_oop) {\n-    __ movq(r11, shift == 3 ? count : to);\n-  }\n-  bs->arraycopy_epilogue(_masm, decorators, type, from, to, count);\n-  restore_argument_regs(type);\n-  INC_COUNTER_NP(get_profile_ctr(shift), rscratch1); \/\/ Update counter after rscratch1 is free\n-  __ xorptr(rax, rax); \/\/ return 0\n-  __ vzeroupper();\n-  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-  __ ret(0);\n-\n-  return start;\n-}\n-\n-\/\/ Inputs:\n-\/\/   c_rarg0   - source array address\n-\/\/   c_rarg1   - destination array address\n-\/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n-\/\/\n-address StubGenerator::generate_conjoint_copy_avx3_masked(address* entry, const char *name, int shift,\n-                                                          address nooverlap_target, bool aligned,\n-                                                          bool is_oop, bool dest_uninitialized) {\n-  __ align(CodeEntryAlignment);\n-  StubCodeMark mark(this, \"StubRoutines\", name);\n-  address start = __ pc();\n-\n-  int avx3threshold = VM_Version::avx3_threshold();\n-  bool use64byteVector = (MaxVectorSize > 32) && (avx3threshold == 0);\n-\n-  Label L_main_pre_loop, L_main_pre_loop_64bytes, L_pre_main_post_64;\n-  Label L_main_loop, L_main_loop_64bytes, L_tail, L_tail64, L_exit, L_entry;\n-  const Register from        = rdi;  \/\/ source array address\n-  const Register to          = rsi;  \/\/ destination array address\n-  const Register count       = rdx;  \/\/ elements count\n-  const Register temp1       = r8;\n-  const Register temp2       = rcx;\n-  const Register temp3       = r11;\n-  const Register temp4       = rax;\n-  \/\/ End pointers are inclusive, and if count is not zero they point\n-  \/\/ to the last unit copied:  end_to[0] := end_from[0]\n-\n-  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-  assert_clean_int(c_rarg2, rax);    \/\/ Make sure 'count' is clean int.\n-\n-  if (entry != NULL) {\n-    *entry = __ pc();\n-     \/\/ caller can pass a 64-bit byte count here (from Unsafe.copyMemory)\n-    BLOCK_COMMENT(\"Entry:\");\n-  }\n-\n-  array_overlap_test(nooverlap_target, (Address::ScaleFactor)(shift));\n-\n-  BasicType type_vec[] = { T_BYTE,  T_SHORT,  T_INT,   T_LONG};\n-  BasicType type = is_oop ? T_OBJECT : type_vec[shift];\n-\n-  setup_argument_regs(type);\n-\n-  DecoratorSet decorators = IN_HEAP | IS_ARRAY;\n-  if (dest_uninitialized) {\n-    decorators |= IS_DEST_UNINITIALIZED;\n-  }\n-  if (aligned) {\n-    decorators |= ARRAYCOPY_ALIGNED;\n-  }\n-  BarrierSetAssembler *bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-  bs->arraycopy_prologue(_masm, decorators, type, from, to, count);\n-  {\n-    \/\/ Type(shift)       byte(0), short(1), int(2),   long(3)\n-    int loop_size[]   = { 192,     96,       48,      24};\n-    int threshold[]   = { 4096,    2048,     1024,    512};\n-\n-    \/\/ UnsafeCopyMemory page error: continue after ucm\n-    UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n-    \/\/ 'from', 'to' and 'count' are now valid\n-\n-    \/\/ temp1 holds remaining count.\n-    __ movq(temp1, count);\n-\n-    \/\/ Zero length check.\n-    __ BIND(L_tail);\n-    __ cmpq(temp1, 0);\n-    __ jcc(Assembler::lessEqual, L_exit);\n-\n-    __ mov64(temp2, 0);\n-    __ movq(temp3, temp1);\n-    \/\/ Special cases using 32 byte [masked] vector copy operations.\n-    __ arraycopy_avx3_special_cases_conjoint(xmm1, k2, from, to, temp2, temp3, temp1, shift,\n-                                             temp4, use64byteVector, L_entry, L_exit);\n-\n-    \/\/ PRE-MAIN-POST loop for aligned copy.\n-    __ BIND(L_entry);\n-\n-    if ((MaxVectorSize > 32) && (avx3threshold != 0)) {\n-      __ cmpq(temp1, threshold[shift]);\n-      __ jcc(Assembler::greaterEqual, L_pre_main_post_64);\n-    }\n-\n-    if ((MaxVectorSize < 64)  || (avx3threshold != 0)) {\n-      \/\/ Partial copy to make dst address 32 byte aligned.\n-      __ leaq(temp2, Address(to, temp1, (Address::ScaleFactor)(shift), 0));\n-      __ andq(temp2, 31);\n-      __ jcc(Assembler::equal, L_main_pre_loop);\n-\n-      if (shift) {\n-        __ shrq(temp2, shift);\n-      }\n-      __ subq(temp1, temp2);\n-      __ copy32_masked_avx(to, from, xmm1, k2, temp2, temp1, temp3, shift);\n-\n-      __ cmpq(temp1, loop_size[shift]);\n-      __ jcc(Assembler::less, L_tail);\n-\n-      __ BIND(L_main_pre_loop);\n-\n-      \/\/ Main loop with aligned copy block size of 192 bytes at 32 byte granularity.\n-      __ align32();\n-      __ BIND(L_main_loop);\n-         __ copy64_avx(to, from, temp1, xmm1, true, shift, -64);\n-         __ copy64_avx(to, from, temp1, xmm1, true, shift, -128);\n-         __ copy64_avx(to, from, temp1, xmm1, true, shift, -192);\n-         __ subptr(temp1, loop_size[shift]);\n-         __ cmpq(temp1, loop_size[shift]);\n-         __ jcc(Assembler::greater, L_main_loop);\n-\n-      \/\/ Tail loop.\n-      __ jmp(L_tail);\n-    }\n-\n-    if (MaxVectorSize > 32) {\n-      __ BIND(L_pre_main_post_64);\n-      \/\/ Partial copy to make dst address 64 byte aligned.\n-      __ leaq(temp2, Address(to, temp1, (Address::ScaleFactor)(shift), 0));\n-      __ andq(temp2, 63);\n-      __ jcc(Assembler::equal, L_main_pre_loop_64bytes);\n-\n-      if (shift) {\n-        __ shrq(temp2, shift);\n-      }\n-      __ subq(temp1, temp2);\n-      __ copy64_masked_avx(to, from, xmm1, k2, temp2, temp1, temp3, shift, 0 , true);\n-\n-      __ cmpq(temp1, loop_size[shift]);\n-      __ jcc(Assembler::less, L_tail64);\n-\n-      __ BIND(L_main_pre_loop_64bytes);\n-\n-      \/\/ Main loop with aligned copy block size of 192 bytes at\n-      \/\/ 64 byte copy granularity.\n-      __ align32();\n-      __ BIND(L_main_loop_64bytes);\n-         __ copy64_avx(to, from, temp1, xmm1, true, shift, -64 , true);\n-         __ copy64_avx(to, from, temp1, xmm1, true, shift, -128, true);\n-         __ copy64_avx(to, from, temp1, xmm1, true, shift, -192, true);\n-         __ subq(temp1, loop_size[shift]);\n-         __ cmpq(temp1, loop_size[shift]);\n-         __ jcc(Assembler::greater, L_main_loop_64bytes);\n-\n-      \/\/ Zero length check.\n-      __ cmpq(temp1, 0);\n-      __ jcc(Assembler::lessEqual, L_exit);\n-\n-      __ BIND(L_tail64);\n-\n-      \/\/ Tail handling using 64 byte [masked] vector copy operations.\n-      use64byteVector = true;\n-      __ mov64(temp2, 0);\n-      __ movq(temp3, temp1);\n-      __ arraycopy_avx3_special_cases_conjoint(xmm1, k2, from, to, temp2, temp3, temp1, shift,\n-                                               temp4, use64byteVector, L_entry, L_exit);\n-    }\n-    __ BIND(L_exit);\n-  }\n-  address ucme_exit_pc = __ pc();\n-  \/\/ When called from generic_arraycopy r11 contains specific values\n-  \/\/ used during arraycopy epilogue, re-initializing r11.\n-  if(is_oop) {\n-    __ movq(r11, count);\n-  }\n-  bs->arraycopy_epilogue(_masm, decorators, type, from, to, count);\n-  restore_argument_regs(type);\n-  INC_COUNTER_NP(get_profile_ctr(shift), rscratch1); \/\/ Update counter after rscratch1 is free\n-  __ xorptr(rax, rax); \/\/ return 0\n-  __ vzeroupper();\n-  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-  __ ret(0);\n-\n-  return start;\n-}\n-#endif \/\/ COMPILER2_OR_JVMCI\n-\n-\n-\/\/ Arguments:\n-\/\/   aligned - true => Input and output aligned on a HeapWord == 8-byte boundary\n-\/\/             ignored\n-\/\/   name    - stub name string\n-\/\/\n-\/\/ Inputs:\n-\/\/   c_rarg0   - source array address\n-\/\/   c_rarg1   - destination array address\n-\/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n-\/\/\n-\/\/ If 'from' and\/or 'to' are aligned on 4-, 2-, or 1-byte boundaries,\n-\/\/ we let the hardware handle it.  The one to eight bytes within words,\n-\/\/ dwords or qwords that span cache line boundaries will still be loaded\n-\/\/ and stored atomically.\n-\/\/\n-\/\/ Side Effects:\n-\/\/   disjoint_byte_copy_entry is set to the no-overlap entry point\n-\/\/   used by generate_conjoint_byte_copy().\n-\/\/\n-address StubGenerator::generate_disjoint_byte_copy(bool aligned, address* entry, const char *name) {\n-#if COMPILER2_OR_JVMCI\n-  if (VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2() && MaxVectorSize  >= 32) {\n-     return generate_disjoint_copy_avx3_masked(entry, \"jbyte_disjoint_arraycopy_avx3\", 0,\n-                                               aligned, false, false);\n-  }\n-#endif\n-  __ align(CodeEntryAlignment);\n-  StubCodeMark mark(this, \"StubRoutines\", name);\n-  address start = __ pc();\n-\n-  Label L_copy_bytes, L_copy_8_bytes, L_copy_4_bytes, L_copy_2_bytes;\n-  Label L_copy_byte, L_exit;\n-  const Register from        = rdi;  \/\/ source array address\n-  const Register to          = rsi;  \/\/ destination array address\n-  const Register count       = rdx;  \/\/ elements count\n-  const Register byte_count  = rcx;\n-  const Register qword_count = count;\n-  const Register end_from    = from; \/\/ source array end address\n-  const Register end_to      = to;   \/\/ destination array end address\n-  \/\/ End pointers are inclusive, and if count is not zero they point\n-  \/\/ to the last unit copied:  end_to[0] := end_from[0]\n-\n-  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-  assert_clean_int(c_rarg2, rax);    \/\/ Make sure 'count' is clean int.\n-\n-  if (entry != NULL) {\n-    *entry = __ pc();\n-     \/\/ caller can pass a 64-bit byte count here (from Unsafe.copyMemory)\n-    BLOCK_COMMENT(\"Entry:\");\n-  }\n-\n-  setup_arg_regs(); \/\/ from => rdi, to => rsi, count => rdx\n-                    \/\/ r9 and r10 may be used to save non-volatile registers\n-\n-  {\n-    \/\/ UnsafeCopyMemory page error: continue after ucm\n-    UnsafeCopyMemoryMark ucmm(this, !aligned, true);\n-    \/\/ 'from', 'to' and 'count' are now valid\n-    __ movptr(byte_count, count);\n-    __ shrptr(count, 3); \/\/ count => qword_count\n-\n-    \/\/ Copy from low to high addresses.  Use 'to' as scratch.\n-    __ lea(end_from, Address(from, qword_count, Address::times_8, -8));\n-    __ lea(end_to,   Address(to,   qword_count, Address::times_8, -8));\n-    __ negptr(qword_count); \/\/ make the count negative\n-    __ jmp(L_copy_bytes);\n-\n-    \/\/ Copy trailing qwords\n-    __ BIND(L_copy_8_bytes);\n-    __ movq(rax, Address(end_from, qword_count, Address::times_8, 8));\n-    __ movq(Address(end_to, qword_count, Address::times_8, 8), rax);\n-    __ increment(qword_count);\n-    __ jcc(Assembler::notZero, L_copy_8_bytes);\n-\n-    \/\/ Check for and copy trailing dword\n-    __ BIND(L_copy_4_bytes);\n-    __ testl(byte_count, 4);\n-    __ jccb(Assembler::zero, L_copy_2_bytes);\n-    __ movl(rax, Address(end_from, 8));\n-    __ movl(Address(end_to, 8), rax);\n-\n-    __ addptr(end_from, 4);\n-    __ addptr(end_to, 4);\n-\n-    \/\/ Check for and copy trailing word\n-    __ BIND(L_copy_2_bytes);\n-    __ testl(byte_count, 2);\n-    __ jccb(Assembler::zero, L_copy_byte);\n-    __ movw(rax, Address(end_from, 8));\n-    __ movw(Address(end_to, 8), rax);\n-\n-    __ addptr(end_from, 2);\n-    __ addptr(end_to, 2);\n-\n-    \/\/ Check for and copy trailing byte\n-    __ BIND(L_copy_byte);\n-    __ testl(byte_count, 1);\n-    __ jccb(Assembler::zero, L_exit);\n-    __ movb(rax, Address(end_from, 8));\n-    __ movb(Address(end_to, 8), rax);\n-  }\n-  __ BIND(L_exit);\n-  address ucme_exit_pc = __ pc();\n-  restore_arg_regs();\n-  INC_COUNTER_NP(SharedRuntime::_jbyte_array_copy_ctr, rscratch1); \/\/ Update counter after rscratch1 is free\n-  __ xorptr(rax, rax); \/\/ return 0\n-  __ vzeroupper();\n-  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-  __ ret(0);\n-\n-  {\n-    UnsafeCopyMemoryMark ucmm(this, !aligned, false, ucme_exit_pc);\n-    \/\/ Copy in multi-bytes chunks\n-    copy_bytes_forward(end_from, end_to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n-    __ jmp(L_copy_4_bytes);\n-  }\n-  return start;\n-}\n-\n-\/\/ Arguments:\n-\/\/   aligned - true => Input and output aligned on a HeapWord == 8-byte boundary\n-\/\/             ignored\n-\/\/   name    - stub name string\n-\/\/\n-\/\/ Inputs:\n-\/\/   c_rarg0   - source array address\n-\/\/   c_rarg1   - destination array address\n-\/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n-\/\/\n-\/\/ If 'from' and\/or 'to' are aligned on 4-, 2-, or 1-byte boundaries,\n-\/\/ we let the hardware handle it.  The one to eight bytes within words,\n-\/\/ dwords or qwords that span cache line boundaries will still be loaded\n-\/\/ and stored atomically.\n-\/\/\n-address StubGenerator::generate_conjoint_byte_copy(bool aligned, address nooverlap_target,\n-                                                   address* entry, const char *name) {\n-#if COMPILER2_OR_JVMCI\n-  if (VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2() && MaxVectorSize  >= 32) {\n-     return generate_conjoint_copy_avx3_masked(entry, \"jbyte_conjoint_arraycopy_avx3\", 0,\n-                                               nooverlap_target, aligned, false, false);\n-  }\n-#endif\n-  __ align(CodeEntryAlignment);\n-  StubCodeMark mark(this, \"StubRoutines\", name);\n-  address start = __ pc();\n-\n-  Label L_copy_bytes, L_copy_8_bytes, L_copy_4_bytes, L_copy_2_bytes;\n-  const Register from        = rdi;  \/\/ source array address\n-  const Register to          = rsi;  \/\/ destination array address\n-  const Register count       = rdx;  \/\/ elements count\n-  const Register byte_count  = rcx;\n-  const Register qword_count = count;\n-\n-  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-  assert_clean_int(c_rarg2, rax);    \/\/ Make sure 'count' is clean int.\n-\n-  if (entry != NULL) {\n-    *entry = __ pc();\n-    \/\/ caller can pass a 64-bit byte count here (from Unsafe.copyMemory)\n-    BLOCK_COMMENT(\"Entry:\");\n-  }\n-\n-  array_overlap_test(nooverlap_target, Address::times_1);\n-  setup_arg_regs(); \/\/ from => rdi, to => rsi, count => rdx\n-                    \/\/ r9 and r10 may be used to save non-volatile registers\n-\n-  {\n-    \/\/ UnsafeCopyMemory page error: continue after ucm\n-    UnsafeCopyMemoryMark ucmm(this, !aligned, true);\n-    \/\/ 'from', 'to' and 'count' are now valid\n-    __ movptr(byte_count, count);\n-    __ shrptr(count, 3);   \/\/ count => qword_count\n-\n-    \/\/ Copy from high to low addresses.\n-\n-    \/\/ Check for and copy trailing byte\n-    __ testl(byte_count, 1);\n-    __ jcc(Assembler::zero, L_copy_2_bytes);\n-    __ movb(rax, Address(from, byte_count, Address::times_1, -1));\n-    __ movb(Address(to, byte_count, Address::times_1, -1), rax);\n-    __ decrement(byte_count); \/\/ Adjust for possible trailing word\n-\n-    \/\/ Check for and copy trailing word\n-    __ BIND(L_copy_2_bytes);\n-    __ testl(byte_count, 2);\n-    __ jcc(Assembler::zero, L_copy_4_bytes);\n-    __ movw(rax, Address(from, byte_count, Address::times_1, -2));\n-    __ movw(Address(to, byte_count, Address::times_1, -2), rax);\n-\n-    \/\/ Check for and copy trailing dword\n-    __ BIND(L_copy_4_bytes);\n-    __ testl(byte_count, 4);\n-    __ jcc(Assembler::zero, L_copy_bytes);\n-    __ movl(rax, Address(from, qword_count, Address::times_8));\n-    __ movl(Address(to, qword_count, Address::times_8), rax);\n-    __ jmp(L_copy_bytes);\n-\n-    \/\/ Copy trailing qwords\n-    __ BIND(L_copy_8_bytes);\n-    __ movq(rax, Address(from, qword_count, Address::times_8, -8));\n-    __ movq(Address(to, qword_count, Address::times_8, -8), rax);\n-    __ decrement(qword_count);\n-    __ jcc(Assembler::notZero, L_copy_8_bytes);\n-  }\n-  restore_arg_regs();\n-  INC_COUNTER_NP(SharedRuntime::_jbyte_array_copy_ctr, rscratch1); \/\/ Update counter after rscratch1 is free\n-  __ xorptr(rax, rax); \/\/ return 0\n-  __ vzeroupper();\n-  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-  __ ret(0);\n-\n-  {\n-    \/\/ UnsafeCopyMemory page error: continue after ucm\n-    UnsafeCopyMemoryMark ucmm(this, !aligned, true);\n-    \/\/ Copy in multi-bytes chunks\n-    copy_bytes_backward(from, to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n-  }\n-  restore_arg_regs();\n-  INC_COUNTER_NP(SharedRuntime::_jbyte_array_copy_ctr, rscratch1); \/\/ Update counter after rscratch1 is free\n-  __ xorptr(rax, rax); \/\/ return 0\n-  __ vzeroupper();\n-  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-  __ ret(0);\n-\n-  return start;\n-}\n-\n-\/\/ Arguments:\n-\/\/   aligned - true => Input and output aligned on a HeapWord == 8-byte boundary\n-\/\/             ignored\n-\/\/   name    - stub name string\n-\/\/\n-\/\/ Inputs:\n-\/\/   c_rarg0   - source array address\n-\/\/   c_rarg1   - destination array address\n-\/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n-\/\/\n-\/\/ If 'from' and\/or 'to' are aligned on 4- or 2-byte boundaries, we\n-\/\/ let the hardware handle it.  The two or four words within dwords\n-\/\/ or qwords that span cache line boundaries will still be loaded\n-\/\/ and stored atomically.\n-\/\/\n-\/\/ Side Effects:\n-\/\/   disjoint_short_copy_entry is set to the no-overlap entry point\n-\/\/   used by generate_conjoint_short_copy().\n-\/\/\n-address StubGenerator::generate_disjoint_short_copy(bool aligned, address *entry, const char *name) {\n-#if COMPILER2_OR_JVMCI\n-  if (VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2() && MaxVectorSize  >= 32) {\n-     return generate_disjoint_copy_avx3_masked(entry, \"jshort_disjoint_arraycopy_avx3\", 1,\n-                                               aligned, false, false);\n-  }\n-#endif\n-\n-  __ align(CodeEntryAlignment);\n-  StubCodeMark mark(this, \"StubRoutines\", name);\n-  address start = __ pc();\n-\n-  Label L_copy_bytes, L_copy_8_bytes, L_copy_4_bytes,L_copy_2_bytes,L_exit;\n-  const Register from        = rdi;  \/\/ source array address\n-  const Register to          = rsi;  \/\/ destination array address\n-  const Register count       = rdx;  \/\/ elements count\n-  const Register word_count  = rcx;\n-  const Register qword_count = count;\n-  const Register end_from    = from; \/\/ source array end address\n-  const Register end_to      = to;   \/\/ destination array end address\n-  \/\/ End pointers are inclusive, and if count is not zero they point\n-  \/\/ to the last unit copied:  end_to[0] := end_from[0]\n-\n-  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-  assert_clean_int(c_rarg2, rax);    \/\/ Make sure 'count' is clean int.\n-\n-  if (entry != NULL) {\n-    *entry = __ pc();\n-    \/\/ caller can pass a 64-bit byte count here (from Unsafe.copyMemory)\n-    BLOCK_COMMENT(\"Entry:\");\n-  }\n-\n-  setup_arg_regs(); \/\/ from => rdi, to => rsi, count => rdx\n-                    \/\/ r9 and r10 may be used to save non-volatile registers\n-\n-  {\n-    \/\/ UnsafeCopyMemory page error: continue after ucm\n-    UnsafeCopyMemoryMark ucmm(this, !aligned, true);\n-    \/\/ 'from', 'to' and 'count' are now valid\n-    __ movptr(word_count, count);\n-    __ shrptr(count, 2); \/\/ count => qword_count\n-\n-    \/\/ Copy from low to high addresses.  Use 'to' as scratch.\n-    __ lea(end_from, Address(from, qword_count, Address::times_8, -8));\n-    __ lea(end_to,   Address(to,   qword_count, Address::times_8, -8));\n-    __ negptr(qword_count);\n-    __ jmp(L_copy_bytes);\n-\n-    \/\/ Copy trailing qwords\n-    __ BIND(L_copy_8_bytes);\n-    __ movq(rax, Address(end_from, qword_count, Address::times_8, 8));\n-    __ movq(Address(end_to, qword_count, Address::times_8, 8), rax);\n-    __ increment(qword_count);\n-    __ jcc(Assembler::notZero, L_copy_8_bytes);\n-\n-    \/\/ Original 'dest' is trashed, so we can't use it as a\n-    \/\/ base register for a possible trailing word copy\n-\n-    \/\/ Check for and copy trailing dword\n-    __ BIND(L_copy_4_bytes);\n-    __ testl(word_count, 2);\n-    __ jccb(Assembler::zero, L_copy_2_bytes);\n-    __ movl(rax, Address(end_from, 8));\n-    __ movl(Address(end_to, 8), rax);\n-\n-    __ addptr(end_from, 4);\n-    __ addptr(end_to, 4);\n-\n-    \/\/ Check for and copy trailing word\n-    __ BIND(L_copy_2_bytes);\n-    __ testl(word_count, 1);\n-    __ jccb(Assembler::zero, L_exit);\n-    __ movw(rax, Address(end_from, 8));\n-    __ movw(Address(end_to, 8), rax);\n-  }\n-  __ BIND(L_exit);\n-  address ucme_exit_pc = __ pc();\n-  restore_arg_regs();\n-  INC_COUNTER_NP(SharedRuntime::_jshort_array_copy_ctr, rscratch1); \/\/ Update counter after rscratch1 is free\n-  __ xorptr(rax, rax); \/\/ return 0\n-  __ vzeroupper();\n-  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-  __ ret(0);\n-\n-  {\n-    UnsafeCopyMemoryMark ucmm(this, !aligned, false, ucme_exit_pc);\n-    \/\/ Copy in multi-bytes chunks\n-    copy_bytes_forward(end_from, end_to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n-    __ jmp(L_copy_4_bytes);\n-  }\n-\n-  return start;\n-}\n-\n-address StubGenerator::generate_fill(BasicType t, bool aligned, const char *name) {\n-  __ align(CodeEntryAlignment);\n-  StubCodeMark mark(this, \"StubRoutines\", name);\n-  address start = __ pc();\n-\n-  BLOCK_COMMENT(\"Entry:\");\n-\n-  const Register to       = c_rarg0;  \/\/ destination array address\n-  const Register value    = c_rarg1;  \/\/ value\n-  const Register count    = c_rarg2;  \/\/ elements count\n-  __ mov(r11, count);\n-\n-  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-\n-  __ generate_fill(t, aligned, to, value, r11, rax, xmm0);\n-\n-  __ vzeroupper();\n-  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-  __ ret(0);\n-\n-  return start;\n-}\n-\n-\/\/ Arguments:\n-\/\/   aligned - true => Input and output aligned on a HeapWord == 8-byte boundary\n-\/\/             ignored\n-\/\/   name    - stub name string\n-\/\/\n-\/\/ Inputs:\n-\/\/   c_rarg0   - source array address\n-\/\/   c_rarg1   - destination array address\n-\/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n-\/\/\n-\/\/ If 'from' and\/or 'to' are aligned on 4- or 2-byte boundaries, we\n-\/\/ let the hardware handle it.  The two or four words within dwords\n-\/\/ or qwords that span cache line boundaries will still be loaded\n-\/\/ and stored atomically.\n-\/\/\n-address StubGenerator::generate_conjoint_short_copy(bool aligned, address nooverlap_target,\n-                                                    address *entry, const char *name) {\n-#if COMPILER2_OR_JVMCI\n-  if (VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2() && MaxVectorSize  >= 32) {\n-     return generate_conjoint_copy_avx3_masked(entry, \"jshort_conjoint_arraycopy_avx3\", 1,\n-                                               nooverlap_target, aligned, false, false);\n-  }\n-#endif\n-  __ align(CodeEntryAlignment);\n-  StubCodeMark mark(this, \"StubRoutines\", name);\n-  address start = __ pc();\n-\n-  Label L_copy_bytes, L_copy_8_bytes, L_copy_4_bytes;\n-  const Register from        = rdi;  \/\/ source array address\n-  const Register to          = rsi;  \/\/ destination array address\n-  const Register count       = rdx;  \/\/ elements count\n-  const Register word_count  = rcx;\n-  const Register qword_count = count;\n-\n-  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-  assert_clean_int(c_rarg2, rax);    \/\/ Make sure 'count' is clean int.\n-\n-  if (entry != NULL) {\n-    *entry = __ pc();\n-    \/\/ caller can pass a 64-bit byte count here (from Unsafe.copyMemory)\n-    BLOCK_COMMENT(\"Entry:\");\n-  }\n-\n-  array_overlap_test(nooverlap_target, Address::times_2);\n-  setup_arg_regs(); \/\/ from => rdi, to => rsi, count => rdx\n-                    \/\/ r9 and r10 may be used to save non-volatile registers\n-\n-  {\n-    \/\/ UnsafeCopyMemory page error: continue after ucm\n-    UnsafeCopyMemoryMark ucmm(this, !aligned, true);\n-    \/\/ 'from', 'to' and 'count' are now valid\n-    __ movptr(word_count, count);\n-    __ shrptr(count, 2); \/\/ count => qword_count\n-\n-    \/\/ Copy from high to low addresses.  Use 'to' as scratch.\n-\n-    \/\/ Check for and copy trailing word\n-    __ testl(word_count, 1);\n-    __ jccb(Assembler::zero, L_copy_4_bytes);\n-    __ movw(rax, Address(from, word_count, Address::times_2, -2));\n-    __ movw(Address(to, word_count, Address::times_2, -2), rax);\n-\n-   \/\/ Check for and copy trailing dword\n-    __ BIND(L_copy_4_bytes);\n-    __ testl(word_count, 2);\n-    __ jcc(Assembler::zero, L_copy_bytes);\n-    __ movl(rax, Address(from, qword_count, Address::times_8));\n-    __ movl(Address(to, qword_count, Address::times_8), rax);\n-    __ jmp(L_copy_bytes);\n-\n-    \/\/ Copy trailing qwords\n-    __ BIND(L_copy_8_bytes);\n-    __ movq(rax, Address(from, qword_count, Address::times_8, -8));\n-    __ movq(Address(to, qword_count, Address::times_8, -8), rax);\n-    __ decrement(qword_count);\n-    __ jcc(Assembler::notZero, L_copy_8_bytes);\n-  }\n-  restore_arg_regs();\n-  INC_COUNTER_NP(SharedRuntime::_jshort_array_copy_ctr, rscratch1); \/\/ Update counter after rscratch1 is free\n-  __ xorptr(rax, rax); \/\/ return 0\n-  __ vzeroupper();\n-  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-  __ ret(0);\n-\n-  {\n-    \/\/ UnsafeCopyMemory page error: continue after ucm\n-    UnsafeCopyMemoryMark ucmm(this, !aligned, true);\n-    \/\/ Copy in multi-bytes chunks\n-    copy_bytes_backward(from, to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n-  }\n-  restore_arg_regs();\n-  INC_COUNTER_NP(SharedRuntime::_jshort_array_copy_ctr, rscratch1); \/\/ Update counter after rscratch1 is free\n-  __ xorptr(rax, rax); \/\/ return 0\n-  __ vzeroupper();\n-  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-  __ ret(0);\n-\n-  return start;\n-}\n-\n-\/\/ Arguments:\n-\/\/   aligned - true => Input and output aligned on a HeapWord == 8-byte boundary\n-\/\/             ignored\n-\/\/   is_oop  - true => oop array, so generate store check code\n-\/\/   name    - stub name string\n-\/\/\n-\/\/ Inputs:\n-\/\/   c_rarg0   - source array address\n-\/\/   c_rarg1   - destination array address\n-\/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n-\/\/\n-\/\/ If 'from' and\/or 'to' are aligned on 4-byte boundaries, we let\n-\/\/ the hardware handle it.  The two dwords within qwords that span\n-\/\/ cache line boundaries will still be loaded and stored atomically.\n-\/\/\n-\/\/ Side Effects:\n-\/\/   disjoint_int_copy_entry is set to the no-overlap entry point\n-\/\/   used by generate_conjoint_int_oop_copy().\n-\/\/\n-address StubGenerator::generate_disjoint_int_oop_copy(bool aligned, bool is_oop, address* entry,\n-                                                      const char *name, bool dest_uninitialized) {\n-#if COMPILER2_OR_JVMCI\n-  if (VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2() && MaxVectorSize  >= 32) {\n-     return generate_disjoint_copy_avx3_masked(entry, \"jint_disjoint_arraycopy_avx3\", 2,\n-                                               aligned, is_oop, dest_uninitialized);\n-  }\n-#endif\n-\n-  __ align(CodeEntryAlignment);\n-  StubCodeMark mark(this, \"StubRoutines\", name);\n-  address start = __ pc();\n-\n-  Label L_copy_bytes, L_copy_8_bytes, L_copy_4_bytes, L_exit;\n-  const Register from        = rdi;  \/\/ source array address\n-  const Register to          = rsi;  \/\/ destination array address\n-  const Register count       = rdx;  \/\/ elements count\n-  const Register dword_count = rcx;\n-  const Register qword_count = count;\n-  const Register end_from    = from; \/\/ source array end address\n-  const Register end_to      = to;   \/\/ destination array end address\n-  \/\/ End pointers are inclusive, and if count is not zero they point\n-  \/\/ to the last unit copied:  end_to[0] := end_from[0]\n-\n-  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-  assert_clean_int(c_rarg2, rax);    \/\/ Make sure 'count' is clean int.\n-\n-  if (entry != NULL) {\n-    *entry = __ pc();\n-    \/\/ caller can pass a 64-bit byte count here (from Unsafe.copyMemory)\n-    BLOCK_COMMENT(\"Entry:\");\n-  }\n-\n-  setup_arg_regs_using_thread(); \/\/ from => rdi, to => rsi, count => rdx\n-                                 \/\/ r9 is used to save r15_thread\n-\n-  DecoratorSet decorators = IN_HEAP | IS_ARRAY | ARRAYCOPY_DISJOINT;\n-  if (dest_uninitialized) {\n-    decorators |= IS_DEST_UNINITIALIZED;\n-  }\n-  if (aligned) {\n-    decorators |= ARRAYCOPY_ALIGNED;\n-  }\n-\n-  BasicType type = is_oop ? T_OBJECT : T_INT;\n-  BarrierSetAssembler *bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-  bs->arraycopy_prologue(_masm, decorators, type, from, to, count);\n-\n-  {\n-    \/\/ UnsafeCopyMemory page error: continue after ucm\n-    UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n-    \/\/ 'from', 'to' and 'count' are now valid\n-    __ movptr(dword_count, count);\n-    __ shrptr(count, 1); \/\/ count => qword_count\n-\n-    \/\/ Copy from low to high addresses.  Use 'to' as scratch.\n-    __ lea(end_from, Address(from, qword_count, Address::times_8, -8));\n-    __ lea(end_to,   Address(to,   qword_count, Address::times_8, -8));\n-    __ negptr(qword_count);\n-    __ jmp(L_copy_bytes);\n-\n-    \/\/ Copy trailing qwords\n-    __ BIND(L_copy_8_bytes);\n-    __ movq(rax, Address(end_from, qword_count, Address::times_8, 8));\n-    __ movq(Address(end_to, qword_count, Address::times_8, 8), rax);\n-    __ increment(qword_count);\n-    __ jcc(Assembler::notZero, L_copy_8_bytes);\n-\n-    \/\/ Check for and copy trailing dword\n-    __ BIND(L_copy_4_bytes);\n-    __ testl(dword_count, 1); \/\/ Only byte test since the value is 0 or 1\n-    __ jccb(Assembler::zero, L_exit);\n-    __ movl(rax, Address(end_from, 8));\n-    __ movl(Address(end_to, 8), rax);\n-  }\n-  __ BIND(L_exit);\n-  address ucme_exit_pc = __ pc();\n-  bs->arraycopy_epilogue(_masm, decorators, type, from, to, dword_count);\n-  restore_arg_regs_using_thread();\n-  INC_COUNTER_NP(SharedRuntime::_jint_array_copy_ctr, rscratch1); \/\/ Update counter after rscratch1 is free\n-  __ vzeroupper();\n-  __ xorptr(rax, rax); \/\/ return 0\n-  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-  __ ret(0);\n-\n-  {\n-    UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, false, ucme_exit_pc);\n-    \/\/ Copy in multi-bytes chunks\n-    copy_bytes_forward(end_from, end_to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n-    __ jmp(L_copy_4_bytes);\n-  }\n-\n-  return start;\n-}\n-\n-\/\/ Arguments:\n-\/\/   aligned - true => Input and output aligned on a HeapWord == 8-byte boundary\n-\/\/             ignored\n-\/\/   is_oop  - true => oop array, so generate store check code\n-\/\/   name    - stub name string\n-\/\/\n-\/\/ Inputs:\n-\/\/   c_rarg0   - source array address\n-\/\/   c_rarg1   - destination array address\n-\/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n-\/\/\n-\/\/ If 'from' and\/or 'to' are aligned on 4-byte boundaries, we let\n-\/\/ the hardware handle it.  The two dwords within qwords that span\n-\/\/ cache line boundaries will still be loaded and stored atomically.\n-\/\/\n-address StubGenerator::generate_conjoint_int_oop_copy(bool aligned, bool is_oop, address nooverlap_target,\n-                                                      address *entry, const char *name,\n-                                                      bool dest_uninitialized) {\n-#if COMPILER2_OR_JVMCI\n-  if (VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2() && MaxVectorSize  >= 32) {\n-     return generate_conjoint_copy_avx3_masked(entry, \"jint_conjoint_arraycopy_avx3\", 2,\n-                                               nooverlap_target, aligned, is_oop, dest_uninitialized);\n-  }\n-#endif\n-  __ align(CodeEntryAlignment);\n-  StubCodeMark mark(this, \"StubRoutines\", name);\n-  address start = __ pc();\n-\n-  Label L_copy_bytes, L_copy_8_bytes, L_exit;\n-  const Register from        = rdi;  \/\/ source array address\n-  const Register to          = rsi;  \/\/ destination array address\n-  const Register count       = rdx;  \/\/ elements count\n-  const Register dword_count = rcx;\n-  const Register qword_count = count;\n-\n-  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-  assert_clean_int(c_rarg2, rax);    \/\/ Make sure 'count' is clean int.\n-\n-  if (entry != NULL) {\n-    *entry = __ pc();\n-     \/\/ caller can pass a 64-bit byte count here (from Unsafe.copyMemory)\n-    BLOCK_COMMENT(\"Entry:\");\n-  }\n-\n-  array_overlap_test(nooverlap_target, Address::times_4);\n-  setup_arg_regs_using_thread(); \/\/ from => rdi, to => rsi, count => rdx\n-                                 \/\/ r9 is used to save r15_thread\n-\n-  DecoratorSet decorators = IN_HEAP | IS_ARRAY;\n-  if (dest_uninitialized) {\n-    decorators |= IS_DEST_UNINITIALIZED;\n-  }\n-  if (aligned) {\n-    decorators |= ARRAYCOPY_ALIGNED;\n-  }\n-\n-  BasicType type = is_oop ? T_OBJECT : T_INT;\n-  BarrierSetAssembler *bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-  \/\/ no registers are destroyed by this call\n-  bs->arraycopy_prologue(_masm, decorators, type, from, to, count);\n-\n-  assert_clean_int(count, rax); \/\/ Make sure 'count' is clean int.\n-  {\n-    \/\/ UnsafeCopyMemory page error: continue after ucm\n-    UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n-    \/\/ 'from', 'to' and 'count' are now valid\n-    __ movptr(dword_count, count);\n-    __ shrptr(count, 1); \/\/ count => qword_count\n-\n-    \/\/ Copy from high to low addresses.  Use 'to' as scratch.\n-\n-    \/\/ Check for and copy trailing dword\n-    __ testl(dword_count, 1);\n-    __ jcc(Assembler::zero, L_copy_bytes);\n-    __ movl(rax, Address(from, dword_count, Address::times_4, -4));\n-    __ movl(Address(to, dword_count, Address::times_4, -4), rax);\n-    __ jmp(L_copy_bytes);\n-\n-    \/\/ Copy trailing qwords\n-    __ BIND(L_copy_8_bytes);\n-    __ movq(rax, Address(from, qword_count, Address::times_8, -8));\n-    __ movq(Address(to, qword_count, Address::times_8, -8), rax);\n-    __ decrement(qword_count);\n-    __ jcc(Assembler::notZero, L_copy_8_bytes);\n-  }\n-  if (is_oop) {\n-    __ jmp(L_exit);\n-  }\n-  restore_arg_regs_using_thread();\n-  INC_COUNTER_NP(SharedRuntime::_jint_array_copy_ctr, rscratch1); \/\/ Update counter after rscratch1 is free\n-  __ xorptr(rax, rax); \/\/ return 0\n-  __ vzeroupper();\n-  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-  __ ret(0);\n-\n-  {\n-    \/\/ UnsafeCopyMemory page error: continue after ucm\n-    UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n-    \/\/ Copy in multi-bytes chunks\n-    copy_bytes_backward(from, to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n-  }\n-\n-  __ BIND(L_exit);\n-  bs->arraycopy_epilogue(_masm, decorators, type, from, to, dword_count);\n-  restore_arg_regs_using_thread();\n-  INC_COUNTER_NP(SharedRuntime::_jint_array_copy_ctr, rscratch1); \/\/ Update counter after rscratch1 is free\n-  __ xorptr(rax, rax); \/\/ return 0\n-  __ vzeroupper();\n-  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-  __ ret(0);\n-\n-  return start;\n-}\n-\n-\/\/ Arguments:\n-\/\/   aligned - true => Input and output aligned on a HeapWord boundary == 8 bytes\n-\/\/             ignored\n-\/\/   is_oop  - true => oop array, so generate store check code\n-\/\/   name    - stub name string\n-\/\/\n-\/\/ Inputs:\n-\/\/   c_rarg0   - source array address\n-\/\/   c_rarg1   - destination array address\n-\/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n-\/\/\n- \/\/ Side Effects:\n-\/\/   disjoint_oop_copy_entry or disjoint_long_copy_entry is set to the\n-\/\/   no-overlap entry point used by generate_conjoint_long_oop_copy().\n-\/\/\n-address StubGenerator::generate_disjoint_long_oop_copy(bool aligned, bool is_oop, address *entry,\n-                                                       const char *name, bool dest_uninitialized) {\n-#if COMPILER2_OR_JVMCI\n-  if (VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2() && MaxVectorSize  >= 32) {\n-     return generate_disjoint_copy_avx3_masked(entry, \"jlong_disjoint_arraycopy_avx3\", 3,\n-                                               aligned, is_oop, dest_uninitialized);\n-  }\n-#endif\n-  __ align(CodeEntryAlignment);\n-  StubCodeMark mark(this, \"StubRoutines\", name);\n-  address start = __ pc();\n-\n-  Label L_copy_bytes, L_copy_8_bytes, L_exit;\n-  const Register from        = rdi;  \/\/ source array address\n-  const Register to          = rsi;  \/\/ destination array address\n-  const Register qword_count = rdx;  \/\/ elements count\n-  const Register end_from    = from; \/\/ source array end address\n-  const Register end_to      = rcx;  \/\/ destination array end address\n-  const Register saved_count = r11;\n-  \/\/ End pointers are inclusive, and if count is not zero they point\n-  \/\/ to the last unit copied:  end_to[0] := end_from[0]\n-\n-  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-  \/\/ Save no-overlap entry point for generate_conjoint_long_oop_copy()\n-  assert_clean_int(c_rarg2, rax);    \/\/ Make sure 'count' is clean int.\n-\n-  if (entry != NULL) {\n-    *entry = __ pc();\n-    \/\/ caller can pass a 64-bit byte count here (from Unsafe.copyMemory)\n-    BLOCK_COMMENT(\"Entry:\");\n-  }\n-\n-  setup_arg_regs_using_thread(); \/\/ from => rdi, to => rsi, count => rdx\n-                                   \/\/ r9 is used to save r15_thread\n-  \/\/ 'from', 'to' and 'qword_count' are now valid\n-\n-  DecoratorSet decorators = IN_HEAP | IS_ARRAY | ARRAYCOPY_DISJOINT;\n-  if (dest_uninitialized) {\n-    decorators |= IS_DEST_UNINITIALIZED;\n-  }\n-  if (aligned) {\n-    decorators |= ARRAYCOPY_ALIGNED;\n-  }\n-\n-  BasicType type = is_oop ? T_OBJECT : T_LONG;\n-  BarrierSetAssembler *bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-  bs->arraycopy_prologue(_masm, decorators, type, from, to, qword_count);\n-  {\n-    \/\/ UnsafeCopyMemory page error: continue after ucm\n-    UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n-\n-    \/\/ Copy from low to high addresses.  Use 'to' as scratch.\n-    __ lea(end_from, Address(from, qword_count, Address::times_8, -8));\n-    __ lea(end_to,   Address(to,   qword_count, Address::times_8, -8));\n-    __ negptr(qword_count);\n-    __ jmp(L_copy_bytes);\n-\n-    \/\/ Copy trailing qwords\n-    __ BIND(L_copy_8_bytes);\n-    __ movq(rax, Address(end_from, qword_count, Address::times_8, 8));\n-    __ movq(Address(end_to, qword_count, Address::times_8, 8), rax);\n-    __ increment(qword_count);\n-    __ jcc(Assembler::notZero, L_copy_8_bytes);\n-  }\n-  if (is_oop) {\n-    __ jmp(L_exit);\n-  } else {\n-    restore_arg_regs_using_thread();\n-    INC_COUNTER_NP(SharedRuntime::_jlong_array_copy_ctr, rscratch1); \/\/ Update counter after rscratch1 is free\n-    __ xorptr(rax, rax); \/\/ return 0\n-    __ vzeroupper();\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-  }\n-\n-  {\n-    \/\/ UnsafeCopyMemory page error: continue after ucm\n-    UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n-    \/\/ Copy in multi-bytes chunks\n-    copy_bytes_forward(end_from, end_to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n-  }\n-\n-  __ BIND(L_exit);\n-  bs->arraycopy_epilogue(_masm, decorators, type, from, to, qword_count);\n-  restore_arg_regs_using_thread();\n-  INC_COUNTER_NP(is_oop ? SharedRuntime::_oop_array_copy_ctr :\n-                          SharedRuntime::_jlong_array_copy_ctr,\n-                 rscratch1); \/\/ Update counter after rscratch1 is free\n-  __ vzeroupper();\n-  __ xorptr(rax, rax); \/\/ return 0\n-  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-  __ ret(0);\n-\n-  return start;\n-}\n-\n-\/\/ Arguments:\n-\/\/   aligned - true => Input and output aligned on a HeapWord boundary == 8 bytes\n-\/\/             ignored\n-\/\/   is_oop  - true => oop array, so generate store check code\n-\/\/   name    - stub name string\n-\/\/\n-\/\/ Inputs:\n-\/\/   c_rarg0   - source array address\n-\/\/   c_rarg1   - destination array address\n-\/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n-\/\/\n-address StubGenerator::generate_conjoint_long_oop_copy(bool aligned, bool is_oop, address nooverlap_target,\n-                                                       address *entry, const char *name,\n-                                                       bool dest_uninitialized) {\n-#if COMPILER2_OR_JVMCI\n-  if (VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2() && MaxVectorSize  >= 32) {\n-     return generate_conjoint_copy_avx3_masked(entry, \"jlong_conjoint_arraycopy_avx3\", 3,\n-                                               nooverlap_target, aligned, is_oop, dest_uninitialized);\n-  }\n-#endif\n-  __ align(CodeEntryAlignment);\n-  StubCodeMark mark(this, \"StubRoutines\", name);\n-  address start = __ pc();\n-\n-  Label L_copy_bytes, L_copy_8_bytes, L_exit;\n-  const Register from        = rdi;  \/\/ source array address\n-  const Register to          = rsi;  \/\/ destination array address\n-  const Register qword_count = rdx;  \/\/ elements count\n-  const Register saved_count = rcx;\n-\n-  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-  assert_clean_int(c_rarg2, rax);    \/\/ Make sure 'count' is clean int.\n-\n-  if (entry != NULL) {\n-    *entry = __ pc();\n-    \/\/ caller can pass a 64-bit byte count here (from Unsafe.copyMemory)\n-    BLOCK_COMMENT(\"Entry:\");\n-  }\n-\n-  array_overlap_test(nooverlap_target, Address::times_8);\n-  setup_arg_regs_using_thread(); \/\/ from => rdi, to => rsi, count => rdx\n-                                 \/\/ r9 is used to save r15_thread\n-  \/\/ 'from', 'to' and 'qword_count' are now valid\n-\n-  DecoratorSet decorators = IN_HEAP | IS_ARRAY;\n-  if (dest_uninitialized) {\n-    decorators |= IS_DEST_UNINITIALIZED;\n-  }\n-  if (aligned) {\n-    decorators |= ARRAYCOPY_ALIGNED;\n-  }\n-\n-  BasicType type = is_oop ? T_OBJECT : T_LONG;\n-  BarrierSetAssembler *bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-  bs->arraycopy_prologue(_masm, decorators, type, from, to, qword_count);\n-  {\n-    \/\/ UnsafeCopyMemory page error: continue after ucm\n-    UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n-\n-    __ jmp(L_copy_bytes);\n-\n-    \/\/ Copy trailing qwords\n-    __ BIND(L_copy_8_bytes);\n-    __ movq(rax, Address(from, qword_count, Address::times_8, -8));\n-    __ movq(Address(to, qword_count, Address::times_8, -8), rax);\n-    __ decrement(qword_count);\n-    __ jcc(Assembler::notZero, L_copy_8_bytes);\n-  }\n-  if (is_oop) {\n-    __ jmp(L_exit);\n-  } else {\n-    restore_arg_regs_using_thread();\n-    INC_COUNTER_NP(SharedRuntime::_jlong_array_copy_ctr, rscratch1); \/\/ Update counter after rscratch1 is free\n-    __ xorptr(rax, rax); \/\/ return 0\n-    __ vzeroupper();\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-  }\n-  {\n-    \/\/ UnsafeCopyMemory page error: continue after ucm\n-    UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n-\n-    \/\/ Copy in multi-bytes chunks\n-    copy_bytes_backward(from, to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n-  }\n-  __ BIND(L_exit);\n-  bs->arraycopy_epilogue(_masm, decorators, type, from, to, qword_count);\n-  restore_arg_regs_using_thread();\n-  INC_COUNTER_NP(is_oop ? SharedRuntime::_oop_array_copy_ctr :\n-                          SharedRuntime::_jlong_array_copy_ctr,\n-                 rscratch1); \/\/ Update counter after rscratch1 is free\n-  __ vzeroupper();\n-  __ xorptr(rax, rax); \/\/ return 0\n-  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-  __ ret(0);\n-\n-  return start;\n-}\n-\n-\n-\/\/ Helper for generating a dynamic type check.\n-\/\/ Smashes no registers.\n-void StubGenerator::generate_type_check(Register sub_klass,\n-                                        Register super_check_offset,\n-                                        Register super_klass,\n-                                        Label& L_success) {\n-  assert_different_registers(sub_klass, super_check_offset, super_klass);\n-\n-  BLOCK_COMMENT(\"type_check:\");\n-\n-  Label L_miss;\n-\n-  __ check_klass_subtype_fast_path(sub_klass, super_klass, noreg,        &L_success, &L_miss, NULL,\n-                                   super_check_offset);\n-  __ check_klass_subtype_slow_path(sub_klass, super_klass, noreg, noreg, &L_success, NULL);\n-\n-  \/\/ Fall through on failure!\n-  __ BIND(L_miss);\n-}\n-\n-\/\/  Generate checkcasting array copy stub\n-\/\/\n-\/\/  Input:\n-\/\/    c_rarg0   - source array address\n-\/\/    c_rarg1   - destination array address\n-\/\/    c_rarg2   - element count, treated as ssize_t, can be zero\n-\/\/    c_rarg3   - size_t ckoff (super_check_offset)\n-\/\/ not Win64\n-\/\/    c_rarg4   - oop ckval (super_klass)\n-\/\/ Win64\n-\/\/    rsp+40    - oop ckval (super_klass)\n-\/\/\n-\/\/  Output:\n-\/\/    rax ==  0  -  success\n-\/\/    rax == -1^K - failure, where K is partial transfer count\n-\/\/\n-address StubGenerator::generate_checkcast_copy(const char *name, address *entry, bool dest_uninitialized) {\n-\n-  Label L_load_element, L_store_element, L_do_card_marks, L_done;\n-\n-  \/\/ Input registers (after setup_arg_regs)\n-  const Register from        = rdi;   \/\/ source array address\n-  const Register to          = rsi;   \/\/ destination array address\n-  const Register length      = rdx;   \/\/ elements count\n-  const Register ckoff       = rcx;   \/\/ super_check_offset\n-  const Register ckval       = r8;    \/\/ super_klass\n-\n-  \/\/ Registers used as temps (r13, r14 are save-on-entry)\n-  const Register end_from    = from;  \/\/ source array end address\n-  const Register end_to      = r13;   \/\/ destination array end address\n-  const Register count       = rdx;   \/\/ -(count_remaining)\n-  const Register r14_length  = r14;   \/\/ saved copy of length\n-  \/\/ End pointers are inclusive, and if length is not zero they point\n-  \/\/ to the last unit copied:  end_to[0] := end_from[0]\n-\n-  const Register rax_oop    = rax;    \/\/ actual oop copied\n-  const Register r11_klass  = r11;    \/\/ oop._klass\n-\n-  \/\/---------------------------------------------------------------\n-  \/\/ Assembler stub will be used for this call to arraycopy\n-  \/\/ if the two arrays are subtypes of Object[] but the\n-  \/\/ destination array type is not equal to or a supertype\n-  \/\/ of the source type.  Each element must be separately\n-  \/\/ checked.\n-\n-  __ align(CodeEntryAlignment);\n-  StubCodeMark mark(this, \"StubRoutines\", name);\n-  address start = __ pc();\n-\n-  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-\n-#ifdef ASSERT\n-  \/\/ caller guarantees that the arrays really are different\n-  \/\/ otherwise, we would have to make conjoint checks\n-  { Label L;\n-    array_overlap_test(L, TIMES_OOP);\n-    __ stop(\"checkcast_copy within a single array\");\n-    __ bind(L);\n-  }\n-#endif \/\/ASSERT\n-\n-  setup_arg_regs(4); \/\/ from => rdi, to => rsi, length => rdx\n-                     \/\/ ckoff => rcx, ckval => r8\n-                     \/\/ r9 and r10 may be used to save non-volatile registers\n-#ifdef _WIN64\n-  \/\/ last argument (#4) is on stack on Win64\n-  __ movptr(ckval, Address(rsp, 6 * wordSize));\n-#endif\n-\n-  \/\/ Caller of this entry point must set up the argument registers.\n-  if (entry != NULL) {\n-    *entry = __ pc();\n-    BLOCK_COMMENT(\"Entry:\");\n-  }\n-\n-  \/\/ allocate spill slots for r13, r14\n-  enum {\n-    saved_r13_offset,\n-    saved_r14_offset,\n-    saved_r10_offset,\n-    saved_rbp_offset\n-  };\n-  __ subptr(rsp, saved_rbp_offset * wordSize);\n-  __ movptr(Address(rsp, saved_r13_offset * wordSize), r13);\n-  __ movptr(Address(rsp, saved_r14_offset * wordSize), r14);\n-  __ movptr(Address(rsp, saved_r10_offset * wordSize), r10);\n-\n-#ifdef ASSERT\n-    Label L2;\n-    __ get_thread(r14);\n-    __ cmpptr(r15_thread, r14);\n-    __ jcc(Assembler::equal, L2);\n-    __ stop(\"StubRoutines::call_stub: r15_thread is modified by call\");\n-    __ bind(L2);\n-#endif \/\/ ASSERT\n-\n-  \/\/ check that int operands are properly extended to size_t\n-  assert_clean_int(length, rax);\n-  assert_clean_int(ckoff, rax);\n-\n-#ifdef ASSERT\n-  BLOCK_COMMENT(\"assert consistent ckoff\/ckval\");\n-  \/\/ The ckoff and ckval must be mutually consistent,\n-  \/\/ even though caller generates both.\n-  { Label L;\n-    int sco_offset = in_bytes(Klass::super_check_offset_offset());\n-    __ cmpl(ckoff, Address(ckval, sco_offset));\n-    __ jcc(Assembler::equal, L);\n-    __ stop(\"super_check_offset inconsistent\");\n-    __ bind(L);\n-  }\n-#endif \/\/ASSERT\n-\n-  \/\/ Loop-invariant addresses.  They are exclusive end pointers.\n-  Address end_from_addr(from, length, TIMES_OOP, 0);\n-  Address   end_to_addr(to,   length, TIMES_OOP, 0);\n-  \/\/ Loop-variant addresses.  They assume post-incremented count < 0.\n-  Address from_element_addr(end_from, count, TIMES_OOP, 0);\n-  Address   to_element_addr(end_to,   count, TIMES_OOP, 0);\n-\n-  DecoratorSet decorators = IN_HEAP | IS_ARRAY | ARRAYCOPY_CHECKCAST | ARRAYCOPY_DISJOINT;\n-  if (dest_uninitialized) {\n-    decorators |= IS_DEST_UNINITIALIZED;\n-  }\n-\n-  BasicType type = T_OBJECT;\n-  BarrierSetAssembler *bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-  bs->arraycopy_prologue(_masm, decorators, type, from, to, count);\n-\n-  \/\/ Copy from low to high addresses, indexed from the end of each array.\n-  __ lea(end_from, end_from_addr);\n-  __ lea(end_to,   end_to_addr);\n-  __ movptr(r14_length, length);        \/\/ save a copy of the length\n-  assert(length == count, \"\");          \/\/ else fix next line:\n-  __ negptr(count);                     \/\/ negate and test the length\n-  __ jcc(Assembler::notZero, L_load_element);\n-\n-  \/\/ Empty array:  Nothing to do.\n-  __ xorptr(rax, rax);                  \/\/ return 0 on (trivial) success\n-  __ jmp(L_done);\n-\n-  \/\/ ======== begin loop ========\n-  \/\/ (Loop is rotated; its entry is L_load_element.)\n-  \/\/ Loop control:\n-  \/\/   for (count = -count; count != 0; count++)\n-  \/\/ Base pointers src, dst are biased by 8*(count-1),to last element.\n-  __ align(OptoLoopAlignment);\n-\n-  __ BIND(L_store_element);\n-  __ store_heap_oop(to_element_addr, rax_oop, noreg, noreg, noreg, AS_RAW);  \/\/ store the oop\n-  __ increment(count);               \/\/ increment the count toward zero\n-  __ jcc(Assembler::zero, L_do_card_marks);\n-\n-  \/\/ ======== loop entry is here ========\n-  __ BIND(L_load_element);\n-  __ load_heap_oop(rax_oop, from_element_addr, noreg, noreg, AS_RAW); \/\/ load the oop\n-  __ testptr(rax_oop, rax_oop);\n-  __ jcc(Assembler::zero, L_store_element);\n-\n-  __ load_klass(r11_klass, rax_oop, rscratch1);\/\/ query the object klass\n-  generate_type_check(r11_klass, ckoff, ckval, L_store_element);\n-  \/\/ ======== end loop ========\n-\n-  \/\/ It was a real error; we must depend on the caller to finish the job.\n-  \/\/ Register rdx = -1 * number of *remaining* oops, r14 = *total* oops.\n-  \/\/ Emit GC store barriers for the oops we have copied (r14 + rdx),\n-  \/\/ and report their number to the caller.\n-  assert_different_registers(rax, r14_length, count, to, end_to, rcx, rscratch1);\n-  Label L_post_barrier;\n-  __ addptr(r14_length, count);     \/\/ K = (original - remaining) oops\n-  __ movptr(rax, r14_length);       \/\/ save the value\n-  __ notptr(rax);                   \/\/ report (-1^K) to caller (does not affect flags)\n-  __ jccb(Assembler::notZero, L_post_barrier);\n-  __ jmp(L_done); \/\/ K == 0, nothing was copied, skip post barrier\n-\n-  \/\/ Come here on success only.\n-  __ BIND(L_do_card_marks);\n-  __ xorptr(rax, rax);              \/\/ return 0 on success\n-\n-  __ BIND(L_post_barrier);\n-  bs->arraycopy_epilogue(_masm, decorators, type, from, to, r14_length);\n-\n-  \/\/ Common exit point (success or failure).\n-  __ BIND(L_done);\n-  __ movptr(r13, Address(rsp, saved_r13_offset * wordSize));\n-  __ movptr(r14, Address(rsp, saved_r14_offset * wordSize));\n-  __ movptr(r10, Address(rsp, saved_r10_offset * wordSize));\n-  restore_arg_regs();\n-  INC_COUNTER_NP(SharedRuntime::_checkcast_array_copy_ctr, rscratch1); \/\/ Update counter after rscratch1 is free\n-  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-  __ ret(0);\n-\n-  return start;\n-}\n-\n-\/\/  Generate 'unsafe' array copy stub\n-\/\/  Though just as safe as the other stubs, it takes an unscaled\n-\/\/  size_t argument instead of an element count.\n-\/\/\n-\/\/  Input:\n-\/\/    c_rarg0   - source array address\n-\/\/    c_rarg1   - destination array address\n-\/\/    c_rarg2   - byte count, treated as ssize_t, can be zero\n-\/\/\n-\/\/ Examines the alignment of the operands and dispatches\n-\/\/ to a long, int, short, or byte copy loop.\n-\/\/\n-address StubGenerator::generate_unsafe_copy(const char *name,\n-                                            address byte_copy_entry, address short_copy_entry,\n-                                            address int_copy_entry, address long_copy_entry) {\n-\n-  Label L_long_aligned, L_int_aligned, L_short_aligned;\n-\n-  \/\/ Input registers (before setup_arg_regs)\n-  const Register from        = c_rarg0;  \/\/ source array address\n-  const Register to          = c_rarg1;  \/\/ destination array address\n-  const Register size        = c_rarg2;  \/\/ byte count (size_t)\n-\n-  \/\/ Register used as a temp\n-  const Register bits        = rax;      \/\/ test copy of low bits\n-\n-  __ align(CodeEntryAlignment);\n-  StubCodeMark mark(this, \"StubRoutines\", name);\n-  address start = __ pc();\n-\n-  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-\n-  \/\/ bump this on entry, not on exit:\n-  INC_COUNTER_NP(SharedRuntime::_unsafe_array_copy_ctr, rscratch1);\n-\n-  __ mov(bits, from);\n-  __ orptr(bits, to);\n-  __ orptr(bits, size);\n-\n-  __ testb(bits, BytesPerLong-1);\n-  __ jccb(Assembler::zero, L_long_aligned);\n-\n-  __ testb(bits, BytesPerInt-1);\n-  __ jccb(Assembler::zero, L_int_aligned);\n-\n-  __ testb(bits, BytesPerShort-1);\n-  __ jump_cc(Assembler::notZero, RuntimeAddress(byte_copy_entry));\n-\n-  __ BIND(L_short_aligned);\n-  __ shrptr(size, LogBytesPerShort); \/\/ size => short_count\n-  __ jump(RuntimeAddress(short_copy_entry));\n-\n-  __ BIND(L_int_aligned);\n-  __ shrptr(size, LogBytesPerInt); \/\/ size => int_count\n-  __ jump(RuntimeAddress(int_copy_entry));\n-\n-  __ BIND(L_long_aligned);\n-  __ shrptr(size, LogBytesPerLong); \/\/ size => qword_count\n-  __ jump(RuntimeAddress(long_copy_entry));\n-\n-  return start;\n-}\n-\n-\/\/ Perform range checks on the proposed arraycopy.\n-\/\/ Kills temp, but nothing else.\n-\/\/ Also, clean the sign bits of src_pos and dst_pos.\n-void StubGenerator::arraycopy_range_checks(Register src,     \/\/ source array oop (c_rarg0)\n-                                           Register src_pos, \/\/ source position (c_rarg1)\n-                                           Register dst,     \/\/ destination array oo (c_rarg2)\n-                                           Register dst_pos, \/\/ destination position (c_rarg3)\n-                                           Register length,\n-                                           Register temp,\n-                                           Label& L_failed) {\n-  BLOCK_COMMENT(\"arraycopy_range_checks:\");\n-\n-  \/\/  if (src_pos + length > arrayOop(src)->length())  FAIL;\n-  __ movl(temp, length);\n-  __ addl(temp, src_pos);             \/\/ src_pos + length\n-  __ cmpl(temp, Address(src, arrayOopDesc::length_offset_in_bytes()));\n-  __ jcc(Assembler::above, L_failed);\n-\n-  \/\/  if (dst_pos + length > arrayOop(dst)->length())  FAIL;\n-  __ movl(temp, length);\n-  __ addl(temp, dst_pos);             \/\/ dst_pos + length\n-  __ cmpl(temp, Address(dst, arrayOopDesc::length_offset_in_bytes()));\n-  __ jcc(Assembler::above, L_failed);\n-\n-  \/\/ Have to clean up high 32-bits of 'src_pos' and 'dst_pos'.\n-  \/\/ Move with sign extension can be used since they are positive.\n-  __ movslq(src_pos, src_pos);\n-  __ movslq(dst_pos, dst_pos);\n-\n-  BLOCK_COMMENT(\"arraycopy_range_checks done\");\n-}\n-\n-\/\/  Generate generic array copy stubs\n-\/\/\n-\/\/  Input:\n-\/\/    c_rarg0    -  src oop\n-\/\/    c_rarg1    -  src_pos (32-bits)\n-\/\/    c_rarg2    -  dst oop\n-\/\/    c_rarg3    -  dst_pos (32-bits)\n-\/\/ not Win64\n-\/\/    c_rarg4    -  element count (32-bits)\n-\/\/ Win64\n-\/\/    rsp+40     -  element count (32-bits)\n-\/\/\n-\/\/  Output:\n-\/\/    rax ==  0  -  success\n-\/\/    rax == -1^K - failure, where K is partial transfer count\n-\/\/\n-address StubGenerator::generate_generic_copy(const char *name,\n-                                             address byte_copy_entry, address short_copy_entry,\n-                                             address int_copy_entry, address oop_copy_entry,\n-                                             address long_copy_entry, address checkcast_copy_entry) {\n-\n-  Label L_failed, L_failed_0, L_objArray;\n-  Label L_copy_shorts, L_copy_ints, L_copy_longs;\n-\n-  \/\/ Input registers\n-  const Register src        = c_rarg0;  \/\/ source array oop\n-  const Register src_pos    = c_rarg1;  \/\/ source position\n-  const Register dst        = c_rarg2;  \/\/ destination array oop\n-  const Register dst_pos    = c_rarg3;  \/\/ destination position\n-#ifndef _WIN64\n-  const Register length     = c_rarg4;\n-  const Register rklass_tmp = r9;  \/\/ load_klass\n-#else\n-  const Address  length(rsp, 7 * wordSize);  \/\/ elements count is on stack on Win64\n-  const Register rklass_tmp = rdi;  \/\/ load_klass\n-#endif\n-\n-  { int modulus = CodeEntryAlignment;\n-    int target  = modulus - 5; \/\/ 5 = sizeof jmp(L_failed)\n-    int advance = target - (__ offset() % modulus);\n-    if (advance < 0)  advance += modulus;\n-    if (advance > 0)  __ nop(advance);\n-  }\n-  StubCodeMark mark(this, \"StubRoutines\", name);\n-\n-  \/\/ Short-hop target to L_failed.  Makes for denser prologue code.\n-  __ BIND(L_failed_0);\n-  __ jmp(L_failed);\n-  assert(__ offset() % CodeEntryAlignment == 0, \"no further alignment needed\");\n-\n-  __ align(CodeEntryAlignment);\n-  address start = __ pc();\n-\n-  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-\n-#ifdef _WIN64\n-  __ push(rklass_tmp); \/\/ rdi is callee-save on Windows\n-#endif\n-\n-  \/\/ bump this on entry, not on exit:\n-  INC_COUNTER_NP(SharedRuntime::_generic_array_copy_ctr, rscratch1);\n-\n-  \/\/-----------------------------------------------------------------------\n-  \/\/ Assembler stub will be used for this call to arraycopy\n-  \/\/ if the following conditions are met:\n-  \/\/\n-  \/\/ (1) src and dst must not be null.\n-  \/\/ (2) src_pos must not be negative.\n-  \/\/ (3) dst_pos must not be negative.\n-  \/\/ (4) length  must not be negative.\n-  \/\/ (5) src klass and dst klass should be the same and not NULL.\n-  \/\/ (6) src and dst should be arrays.\n-  \/\/ (7) src_pos + length must not exceed length of src.\n-  \/\/ (8) dst_pos + length must not exceed length of dst.\n-  \/\/\n-\n-  \/\/  if (src == NULL) return -1;\n-  __ testptr(src, src);         \/\/ src oop\n-  size_t j1off = __ offset();\n-  __ jccb(Assembler::zero, L_failed_0);\n-\n-  \/\/  if (src_pos < 0) return -1;\n-  __ testl(src_pos, src_pos); \/\/ src_pos (32-bits)\n-  __ jccb(Assembler::negative, L_failed_0);\n-\n-  \/\/  if (dst == NULL) return -1;\n-  __ testptr(dst, dst);         \/\/ dst oop\n-  __ jccb(Assembler::zero, L_failed_0);\n-\n-  \/\/  if (dst_pos < 0) return -1;\n-  __ testl(dst_pos, dst_pos); \/\/ dst_pos (32-bits)\n-  size_t j4off = __ offset();\n-  __ jccb(Assembler::negative, L_failed_0);\n-\n-  \/\/ The first four tests are very dense code,\n-  \/\/ but not quite dense enough to put four\n-  \/\/ jumps in a 16-byte instruction fetch buffer.\n-  \/\/ That's good, because some branch predicters\n-  \/\/ do not like jumps so close together.\n-  \/\/ Make sure of this.\n-  guarantee(((j1off ^ j4off) & ~15) != 0, \"I$ line of 1st & 4th jumps\");\n-\n-  \/\/ registers used as temp\n-  const Register r11_length    = r11; \/\/ elements count to copy\n-  const Register r10_src_klass = r10; \/\/ array klass\n-\n-  \/\/  if (length < 0) return -1;\n-  __ movl(r11_length, length);        \/\/ length (elements count, 32-bits value)\n-  __ testl(r11_length, r11_length);\n-  __ jccb(Assembler::negative, L_failed_0);\n-\n-  __ load_klass(r10_src_klass, src, rklass_tmp);\n-#ifdef ASSERT\n-  \/\/  assert(src->klass() != NULL);\n-  {\n-    BLOCK_COMMENT(\"assert klasses not null {\");\n-    Label L1, L2;\n-    __ testptr(r10_src_klass, r10_src_klass);\n-    __ jcc(Assembler::notZero, L2);   \/\/ it is broken if klass is NULL\n-    __ bind(L1);\n-    __ stop(\"broken null klass\");\n-    __ bind(L2);\n-    __ load_klass(rax, dst, rklass_tmp);\n-    __ cmpq(rax, 0);\n-    __ jcc(Assembler::equal, L1);     \/\/ this would be broken also\n-    BLOCK_COMMENT(\"} assert klasses not null done\");\n-  }\n-#endif\n-\n-  \/\/ Load layout helper (32-bits)\n-  \/\/\n-  \/\/  |array_tag|     | header_size | element_type |     |log2_element_size|\n-  \/\/ 32        30    24            16              8     2                 0\n-  \/\/\n-  \/\/   array_tag: typeArray = 0x3, objArray = 0x2, non-array = 0x0\n-  \/\/\n-\n-  const int lh_offset = in_bytes(Klass::layout_helper_offset());\n-\n-  \/\/ Handle objArrays completely differently...\n-  const jint objArray_lh = Klass::array_layout_helper(T_OBJECT);\n-  __ cmpl(Address(r10_src_klass, lh_offset), objArray_lh);\n-  __ jcc(Assembler::equal, L_objArray);\n-\n-  \/\/  if (src->klass() != dst->klass()) return -1;\n-  __ load_klass(rax, dst, rklass_tmp);\n-  __ cmpq(r10_src_klass, rax);\n-  __ jcc(Assembler::notEqual, L_failed);\n-\n-  const Register rax_lh = rax;  \/\/ layout helper\n-  __ movl(rax_lh, Address(r10_src_klass, lh_offset));\n-\n-  \/\/  if (!src->is_Array()) return -1;\n-  __ cmpl(rax_lh, Klass::_lh_neutral_value);\n-  __ jcc(Assembler::greaterEqual, L_failed);\n-\n-  \/\/ At this point, it is known to be a typeArray (array_tag 0x3).\n-#ifdef ASSERT\n-  {\n-    BLOCK_COMMENT(\"assert primitive array {\");\n-    Label L;\n-    __ cmpl(rax_lh, (Klass::_lh_array_tag_type_value << Klass::_lh_array_tag_shift));\n-    __ jcc(Assembler::greaterEqual, L);\n-    __ stop(\"must be a primitive array\");\n-    __ bind(L);\n-    BLOCK_COMMENT(\"} assert primitive array done\");\n-  }\n-#endif\n-\n-  arraycopy_range_checks(src, src_pos, dst, dst_pos, r11_length,\n-                         r10, L_failed);\n-\n-  \/\/ TypeArrayKlass\n-  \/\/\n-  \/\/ src_addr = (src + array_header_in_bytes()) + (src_pos << log2elemsize);\n-  \/\/ dst_addr = (dst + array_header_in_bytes()) + (dst_pos << log2elemsize);\n-  \/\/\n-\n-  const Register r10_offset = r10;    \/\/ array offset\n-  const Register rax_elsize = rax_lh; \/\/ element size\n-\n-  __ movl(r10_offset, rax_lh);\n-  __ shrl(r10_offset, Klass::_lh_header_size_shift);\n-  __ andptr(r10_offset, Klass::_lh_header_size_mask);   \/\/ array_offset\n-  __ addptr(src, r10_offset);           \/\/ src array offset\n-  __ addptr(dst, r10_offset);           \/\/ dst array offset\n-  BLOCK_COMMENT(\"choose copy loop based on element size\");\n-  __ andl(rax_lh, Klass::_lh_log2_element_size_mask); \/\/ rax_lh -> rax_elsize\n-\n-#ifdef _WIN64\n-  __ pop(rklass_tmp); \/\/ Restore callee-save rdi\n-#endif\n-\n-  \/\/ next registers should be set before the jump to corresponding stub\n-  const Register from     = c_rarg0;  \/\/ source array address\n-  const Register to       = c_rarg1;  \/\/ destination array address\n-  const Register count    = c_rarg2;  \/\/ elements count\n-\n-  \/\/ 'from', 'to', 'count' registers should be set in such order\n-  \/\/ since they are the same as 'src', 'src_pos', 'dst'.\n-\n-  __ cmpl(rax_elsize, 0);\n-  __ jccb(Assembler::notEqual, L_copy_shorts);\n-  __ lea(from, Address(src, src_pos, Address::times_1, 0));\/\/ src_addr\n-  __ lea(to,   Address(dst, dst_pos, Address::times_1, 0));\/\/ dst_addr\n-  __ movl2ptr(count, r11_length); \/\/ length\n-  __ jump(RuntimeAddress(byte_copy_entry));\n-\n-__ BIND(L_copy_shorts);\n-  __ cmpl(rax_elsize, LogBytesPerShort);\n-  __ jccb(Assembler::notEqual, L_copy_ints);\n-  __ lea(from, Address(src, src_pos, Address::times_2, 0));\/\/ src_addr\n-  __ lea(to,   Address(dst, dst_pos, Address::times_2, 0));\/\/ dst_addr\n-  __ movl2ptr(count, r11_length); \/\/ length\n-  __ jump(RuntimeAddress(short_copy_entry));\n-\n-__ BIND(L_copy_ints);\n-  __ cmpl(rax_elsize, LogBytesPerInt);\n-  __ jccb(Assembler::notEqual, L_copy_longs);\n-  __ lea(from, Address(src, src_pos, Address::times_4, 0));\/\/ src_addr\n-  __ lea(to,   Address(dst, dst_pos, Address::times_4, 0));\/\/ dst_addr\n-  __ movl2ptr(count, r11_length); \/\/ length\n-  __ jump(RuntimeAddress(int_copy_entry));\n-\n-__ BIND(L_copy_longs);\n-#ifdef ASSERT\n-  {\n-    BLOCK_COMMENT(\"assert long copy {\");\n-    Label L;\n-    __ cmpl(rax_elsize, LogBytesPerLong);\n-    __ jcc(Assembler::equal, L);\n-    __ stop(\"must be long copy, but elsize is wrong\");\n-    __ bind(L);\n-    BLOCK_COMMENT(\"} assert long copy done\");\n-  }\n-#endif\n-  __ lea(from, Address(src, src_pos, Address::times_8, 0));\/\/ src_addr\n-  __ lea(to,   Address(dst, dst_pos, Address::times_8, 0));\/\/ dst_addr\n-  __ movl2ptr(count, r11_length); \/\/ length\n-  __ jump(RuntimeAddress(long_copy_entry));\n-\n-  \/\/ ObjArrayKlass\n-__ BIND(L_objArray);\n-  \/\/ live at this point:  r10_src_klass, r11_length, src[_pos], dst[_pos]\n-\n-  Label L_plain_copy, L_checkcast_copy;\n-  \/\/  test array classes for subtyping\n-  __ load_klass(rax, dst, rklass_tmp);\n-  __ cmpq(r10_src_klass, rax); \/\/ usual case is exact equality\n-  __ jcc(Assembler::notEqual, L_checkcast_copy);\n-\n-  \/\/ Identically typed arrays can be copied without element-wise checks.\n-  arraycopy_range_checks(src, src_pos, dst, dst_pos, r11_length,\n-                         r10, L_failed);\n-\n-  __ lea(from, Address(src, src_pos, TIMES_OOP,\n-               arrayOopDesc::base_offset_in_bytes(T_OBJECT))); \/\/ src_addr\n-  __ lea(to,   Address(dst, dst_pos, TIMES_OOP,\n-               arrayOopDesc::base_offset_in_bytes(T_OBJECT))); \/\/ dst_addr\n-  __ movl2ptr(count, r11_length); \/\/ length\n-__ BIND(L_plain_copy);\n-#ifdef _WIN64\n-  __ pop(rklass_tmp); \/\/ Restore callee-save rdi\n-#endif\n-  __ jump(RuntimeAddress(oop_copy_entry));\n-\n-__ BIND(L_checkcast_copy);\n-  \/\/ live at this point:  r10_src_klass, r11_length, rax (dst_klass)\n-  {\n-    \/\/ Before looking at dst.length, make sure dst is also an objArray.\n-    __ cmpl(Address(rax, lh_offset), objArray_lh);\n-    __ jcc(Assembler::notEqual, L_failed);\n-\n-    \/\/ It is safe to examine both src.length and dst.length.\n-    arraycopy_range_checks(src, src_pos, dst, dst_pos, r11_length,\n-                           rax, L_failed);\n-\n-    const Register r11_dst_klass = r11;\n-    __ load_klass(r11_dst_klass, dst, rklass_tmp); \/\/ reload\n-\n-    \/\/ Marshal the base address arguments now, freeing registers.\n-    __ lea(from, Address(src, src_pos, TIMES_OOP,\n-                 arrayOopDesc::base_offset_in_bytes(T_OBJECT)));\n-    __ lea(to,   Address(dst, dst_pos, TIMES_OOP,\n-                 arrayOopDesc::base_offset_in_bytes(T_OBJECT)));\n-    __ movl(count, length);           \/\/ length (reloaded)\n-    Register sco_temp = c_rarg3;      \/\/ this register is free now\n-    assert_different_registers(from, to, count, sco_temp,\n-                               r11_dst_klass, r10_src_klass);\n-    assert_clean_int(count, sco_temp);\n-\n-    \/\/ Generate the type check.\n-    const int sco_offset = in_bytes(Klass::super_check_offset_offset());\n-    __ movl(sco_temp, Address(r11_dst_klass, sco_offset));\n-    assert_clean_int(sco_temp, rax);\n-    generate_type_check(r10_src_klass, sco_temp, r11_dst_klass, L_plain_copy);\n-\n-    \/\/ Fetch destination element klass from the ObjArrayKlass header.\n-    int ek_offset = in_bytes(ObjArrayKlass::element_klass_offset());\n-    __ movptr(r11_dst_klass, Address(r11_dst_klass, ek_offset));\n-    __ movl(  sco_temp,      Address(r11_dst_klass, sco_offset));\n-    assert_clean_int(sco_temp, rax);\n-\n-#ifdef _WIN64\n-    __ pop(rklass_tmp); \/\/ Restore callee-save rdi\n-#endif\n-\n-    \/\/ the checkcast_copy loop needs two extra arguments:\n-    assert(c_rarg3 == sco_temp, \"#3 already in place\");\n-    \/\/ Set up arguments for checkcast_copy_entry.\n-    setup_arg_regs(4);\n-    __ movptr(r8, r11_dst_klass);  \/\/ dst.klass.element_klass, r8 is c_rarg4 on Linux\/Solaris\n-    __ jump(RuntimeAddress(checkcast_copy_entry));\n-  }\n-\n-__ BIND(L_failed);\n-#ifdef _WIN64\n-  __ pop(rklass_tmp); \/\/ Restore callee-save rdi\n-#endif\n-  __ xorptr(rax, rax);\n-  __ notptr(rax); \/\/ return -1\n-  __ leave();   \/\/ required for proper stackwalking of RuntimeStub frame\n-  __ ret(0);\n-\n-  return start;\n-}\n-\n@@ -3431,97 +1262,0 @@\n-void StubGenerator::generate_arraycopy_stubs() {\n-  address entry;\n-  address entry_jbyte_arraycopy;\n-  address entry_jshort_arraycopy;\n-  address entry_jint_arraycopy;\n-  address entry_oop_arraycopy;\n-  address entry_jlong_arraycopy;\n-  address entry_checkcast_arraycopy;\n-\n-  StubRoutines::_jbyte_disjoint_arraycopy  = generate_disjoint_byte_copy(false, &entry,\n-                                                                         \"jbyte_disjoint_arraycopy\");\n-  StubRoutines::_jbyte_arraycopy           = generate_conjoint_byte_copy(false, entry, &entry_jbyte_arraycopy,\n-                                                                         \"jbyte_arraycopy\");\n-\n-  StubRoutines::_jshort_disjoint_arraycopy = generate_disjoint_short_copy(false, &entry,\n-                                                                          \"jshort_disjoint_arraycopy\");\n-  StubRoutines::_jshort_arraycopy          = generate_conjoint_short_copy(false, entry, &entry_jshort_arraycopy,\n-                                                                          \"jshort_arraycopy\");\n-\n-  StubRoutines::_jint_disjoint_arraycopy   = generate_disjoint_int_oop_copy(false, false, &entry,\n-                                                                            \"jint_disjoint_arraycopy\");\n-  StubRoutines::_jint_arraycopy            = generate_conjoint_int_oop_copy(false, false, entry,\n-                                                                            &entry_jint_arraycopy, \"jint_arraycopy\");\n-\n-  StubRoutines::_jlong_disjoint_arraycopy  = generate_disjoint_long_oop_copy(false, false, &entry,\n-                                                                             \"jlong_disjoint_arraycopy\");\n-  StubRoutines::_jlong_arraycopy           = generate_conjoint_long_oop_copy(false, false, entry,\n-                                                                             &entry_jlong_arraycopy, \"jlong_arraycopy\");\n-  if (UseCompressedOops) {\n-    StubRoutines::_oop_disjoint_arraycopy  = generate_disjoint_int_oop_copy(false, true, &entry,\n-                                                                            \"oop_disjoint_arraycopy\");\n-    StubRoutines::_oop_arraycopy           = generate_conjoint_int_oop_copy(false, true, entry,\n-                                                                            &entry_oop_arraycopy, \"oop_arraycopy\");\n-    StubRoutines::_oop_disjoint_arraycopy_uninit  = generate_disjoint_int_oop_copy(false, true, &entry,\n-                                                                                   \"oop_disjoint_arraycopy_uninit\",\n-                                                                                   \/*dest_uninitialized*\/true);\n-    StubRoutines::_oop_arraycopy_uninit           = generate_conjoint_int_oop_copy(false, true, entry,\n-                                                                                   NULL, \"oop_arraycopy_uninit\",\n-                                                                                   \/*dest_uninitialized*\/true);\n-  } else {\n-    StubRoutines::_oop_disjoint_arraycopy  = generate_disjoint_long_oop_copy(false, true, &entry,\n-                                                                             \"oop_disjoint_arraycopy\");\n-    StubRoutines::_oop_arraycopy           = generate_conjoint_long_oop_copy(false, true, entry,\n-                                                                             &entry_oop_arraycopy, \"oop_arraycopy\");\n-    StubRoutines::_oop_disjoint_arraycopy_uninit  = generate_disjoint_long_oop_copy(false, true, &entry,\n-                                                                                    \"oop_disjoint_arraycopy_uninit\",\n-                                                                                    \/*dest_uninitialized*\/true);\n-    StubRoutines::_oop_arraycopy_uninit           = generate_conjoint_long_oop_copy(false, true, entry,\n-                                                                                    NULL, \"oop_arraycopy_uninit\",\n-                                                                                    \/*dest_uninitialized*\/true);\n-  }\n-\n-  StubRoutines::_checkcast_arraycopy        = generate_checkcast_copy(\"checkcast_arraycopy\", &entry_checkcast_arraycopy);\n-  StubRoutines::_checkcast_arraycopy_uninit = generate_checkcast_copy(\"checkcast_arraycopy_uninit\", NULL,\n-                                                                      \/*dest_uninitialized*\/true);\n-\n-  StubRoutines::_unsafe_arraycopy    = generate_unsafe_copy(\"unsafe_arraycopy\",\n-                                                            entry_jbyte_arraycopy,\n-                                                            entry_jshort_arraycopy,\n-                                                            entry_jint_arraycopy,\n-                                                            entry_jlong_arraycopy);\n-  StubRoutines::_generic_arraycopy   = generate_generic_copy(\"generic_arraycopy\",\n-                                                             entry_jbyte_arraycopy,\n-                                                             entry_jshort_arraycopy,\n-                                                             entry_jint_arraycopy,\n-                                                             entry_oop_arraycopy,\n-                                                             entry_jlong_arraycopy,\n-                                                             entry_checkcast_arraycopy);\n-\n-  StubRoutines::_jbyte_fill = generate_fill(T_BYTE, false, \"jbyte_fill\");\n-  StubRoutines::_jshort_fill = generate_fill(T_SHORT, false, \"jshort_fill\");\n-  StubRoutines::_jint_fill = generate_fill(T_INT, false, \"jint_fill\");\n-  StubRoutines::_arrayof_jbyte_fill = generate_fill(T_BYTE, true, \"arrayof_jbyte_fill\");\n-  StubRoutines::_arrayof_jshort_fill = generate_fill(T_SHORT, true, \"arrayof_jshort_fill\");\n-  StubRoutines::_arrayof_jint_fill = generate_fill(T_INT, true, \"arrayof_jint_fill\");\n-\n-  \/\/ We don't generate specialized code for HeapWord-aligned source\n-  \/\/ arrays, so just use the code we've already generated\n-  StubRoutines::_arrayof_jbyte_disjoint_arraycopy  = StubRoutines::_jbyte_disjoint_arraycopy;\n-  StubRoutines::_arrayof_jbyte_arraycopy           = StubRoutines::_jbyte_arraycopy;\n-\n-  StubRoutines::_arrayof_jshort_disjoint_arraycopy = StubRoutines::_jshort_disjoint_arraycopy;\n-  StubRoutines::_arrayof_jshort_arraycopy          = StubRoutines::_jshort_arraycopy;\n-\n-  StubRoutines::_arrayof_jint_disjoint_arraycopy   = StubRoutines::_jint_disjoint_arraycopy;\n-  StubRoutines::_arrayof_jint_arraycopy            = StubRoutines::_jint_arraycopy;\n-\n-  StubRoutines::_arrayof_jlong_disjoint_arraycopy  = StubRoutines::_jlong_disjoint_arraycopy;\n-  StubRoutines::_arrayof_jlong_arraycopy           = StubRoutines::_jlong_arraycopy;\n-\n-  StubRoutines::_arrayof_oop_disjoint_arraycopy    = StubRoutines::_oop_disjoint_arraycopy;\n-  StubRoutines::_arrayof_oop_arraycopy             = StubRoutines::_oop_arraycopy;\n-\n-  StubRoutines::_arrayof_oop_disjoint_arraycopy_uninit    = StubRoutines::_oop_disjoint_arraycopy_uninit;\n-  StubRoutines::_arrayof_oop_arraycopy_uninit             = StubRoutines::_oop_arraycopy_uninit;\n-}\n-\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.cpp","additions":5,"deletions":2271,"binary":false,"changes":2276,"status":"modified"},{"patch":"@@ -180,0 +180,25 @@\n+  void arraycopy_avx3_special_cases(XMMRegister xmm, KRegister mask, Register from,\n+                                    Register to, Register count, int shift,\n+                                    Register index, Register temp,\n+                                    bool use64byteVector, Label& L_entry, Label& L_exit);\n+\n+  void arraycopy_avx3_special_cases_conjoint(XMMRegister xmm, KRegister mask, Register from,\n+                                             Register to, Register start_index, Register end_index,\n+                                             Register count, int shift, Register temp,\n+                                             bool use64byteVector, Label& L_entry, Label& L_exit);\n+\n+  void copy32_avx(Register dst, Register src, Register index, XMMRegister xmm,\n+                  int shift = Address::times_1, int offset = 0);\n+\n+  void copy64_avx(Register dst, Register src, Register index, XMMRegister xmm,\n+                  bool conjoint, int shift = Address::times_1, int offset = 0,\n+                  bool use64byteVector = false);\n+\n+  void copy64_masked_avx(Register dst, Register src, XMMRegister xmm,\n+                         KRegister mask, Register length, Register index,\n+                         Register temp, int shift = Address::times_1, int offset = 0,\n+                         bool use64byteVector = false);\n+\n+  void copy32_masked_avx(Register dst, Register src, XMMRegister xmm,\n+                         KRegister mask, Register length, Register index,\n+                         Register temp, int shift = Address::times_1, int offset = 0);\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.hpp","additions":25,"deletions":0,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -0,0 +1,2546 @@\n+\/*\n+ * Copyright (c) 2003, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"asm\/macroAssembler.hpp\"\n+#include \"gc\/shared\/barrierSet.hpp\"\n+#include \"gc\/shared\/barrierSetAssembler.hpp\"\n+#include \"oops\/objArrayKlass.hpp\"\n+#include \"runtime\/sharedRuntime.hpp\"\n+#include \"runtime\/stubRoutines.hpp\"\n+#include \"stubGenerator_x86_64.hpp\"\n+#ifdef COMPILER2\n+#include \"opto\/c2_globals.hpp\"\n+#endif\n+\n+#define __ _masm->\n+\n+#define TIMES_OOP (UseCompressedOops ? Address::times_4 : Address::times_8)\n+\n+#ifdef PRODUCT\n+#define BLOCK_COMMENT(str) \/* nothing *\/\n+#else\n+#define BLOCK_COMMENT(str) __ block_comment(str)\n+#endif \/\/ PRODUCT\n+\n+#define BIND(label) bind(label); BLOCK_COMMENT(#label \":\")\n+\n+#ifdef PRODUCT\n+#define INC_COUNTER_NP(counter, rscratch) ((void)0)\n+#else\n+#define INC_COUNTER_NP(counter, rscratch) \\\n+BLOCK_COMMENT(\"inc_counter \" #counter); \\\n+inc_counter_np(_masm, counter, rscratch);\n+\n+static void inc_counter_np(MacroAssembler* _masm, int& counter, Register rscratch) {\n+  __ incrementl(ExternalAddress((address)&counter), rscratch);\n+}\n+\n+#if COMPILER2_OR_JVMCI\n+static int& get_profile_ctr(int shift) {\n+  if (shift == 0) {\n+    return SharedRuntime::_jbyte_array_copy_ctr;\n+  } else if (shift == 1) {\n+    return SharedRuntime::_jshort_array_copy_ctr;\n+  } else if (shift == 2) {\n+    return SharedRuntime::_jint_array_copy_ctr;\n+  } else {\n+    assert(shift == 3, \"\");\n+    return SharedRuntime::_jlong_array_copy_ctr;\n+  }\n+}\n+#endif \/\/ COMPILER2_OR_JVMCI\n+#endif \/\/ !PRODUCT\n+\n+void StubGenerator::generate_arraycopy_stubs() {\n+  address entry;\n+  address entry_jbyte_arraycopy;\n+  address entry_jshort_arraycopy;\n+  address entry_jint_arraycopy;\n+  address entry_oop_arraycopy;\n+  address entry_jlong_arraycopy;\n+  address entry_checkcast_arraycopy;\n+\n+  StubRoutines::_jbyte_disjoint_arraycopy  = generate_disjoint_byte_copy(false, &entry,\n+                                                                         \"jbyte_disjoint_arraycopy\");\n+  StubRoutines::_jbyte_arraycopy           = generate_conjoint_byte_copy(false, entry, &entry_jbyte_arraycopy,\n+                                                                         \"jbyte_arraycopy\");\n+\n+  StubRoutines::_jshort_disjoint_arraycopy = generate_disjoint_short_copy(false, &entry,\n+                                                                          \"jshort_disjoint_arraycopy\");\n+  StubRoutines::_jshort_arraycopy          = generate_conjoint_short_copy(false, entry, &entry_jshort_arraycopy,\n+                                                                          \"jshort_arraycopy\");\n+\n+  StubRoutines::_jint_disjoint_arraycopy   = generate_disjoint_int_oop_copy(false, false, &entry,\n+                                                                            \"jint_disjoint_arraycopy\");\n+  StubRoutines::_jint_arraycopy            = generate_conjoint_int_oop_copy(false, false, entry,\n+                                                                            &entry_jint_arraycopy, \"jint_arraycopy\");\n+\n+  StubRoutines::_jlong_disjoint_arraycopy  = generate_disjoint_long_oop_copy(false, false, &entry,\n+                                                                             \"jlong_disjoint_arraycopy\");\n+  StubRoutines::_jlong_arraycopy           = generate_conjoint_long_oop_copy(false, false, entry,\n+                                                                             &entry_jlong_arraycopy, \"jlong_arraycopy\");\n+  if (UseCompressedOops) {\n+    StubRoutines::_oop_disjoint_arraycopy  = generate_disjoint_int_oop_copy(false, true, &entry,\n+                                                                            \"oop_disjoint_arraycopy\");\n+    StubRoutines::_oop_arraycopy           = generate_conjoint_int_oop_copy(false, true, entry,\n+                                                                            &entry_oop_arraycopy, \"oop_arraycopy\");\n+    StubRoutines::_oop_disjoint_arraycopy_uninit  = generate_disjoint_int_oop_copy(false, true, &entry,\n+                                                                                   \"oop_disjoint_arraycopy_uninit\",\n+                                                                                   \/*dest_uninitialized*\/true);\n+    StubRoutines::_oop_arraycopy_uninit           = generate_conjoint_int_oop_copy(false, true, entry,\n+                                                                                   NULL, \"oop_arraycopy_uninit\",\n+                                                                                   \/*dest_uninitialized*\/true);\n+  } else {\n+    StubRoutines::_oop_disjoint_arraycopy  = generate_disjoint_long_oop_copy(false, true, &entry,\n+                                                                             \"oop_disjoint_arraycopy\");\n+    StubRoutines::_oop_arraycopy           = generate_conjoint_long_oop_copy(false, true, entry,\n+                                                                             &entry_oop_arraycopy, \"oop_arraycopy\");\n+    StubRoutines::_oop_disjoint_arraycopy_uninit  = generate_disjoint_long_oop_copy(false, true, &entry,\n+                                                                                    \"oop_disjoint_arraycopy_uninit\",\n+                                                                                    \/*dest_uninitialized*\/true);\n+    StubRoutines::_oop_arraycopy_uninit           = generate_conjoint_long_oop_copy(false, true, entry,\n+                                                                                    NULL, \"oop_arraycopy_uninit\",\n+                                                                                    \/*dest_uninitialized*\/true);\n+  }\n+\n+  StubRoutines::_checkcast_arraycopy        = generate_checkcast_copy(\"checkcast_arraycopy\", &entry_checkcast_arraycopy);\n+  StubRoutines::_checkcast_arraycopy_uninit = generate_checkcast_copy(\"checkcast_arraycopy_uninit\", NULL,\n+                                                                      \/*dest_uninitialized*\/true);\n+\n+  StubRoutines::_unsafe_arraycopy    = generate_unsafe_copy(\"unsafe_arraycopy\",\n+                                                            entry_jbyte_arraycopy,\n+                                                            entry_jshort_arraycopy,\n+                                                            entry_jint_arraycopy,\n+                                                            entry_jlong_arraycopy);\n+  StubRoutines::_generic_arraycopy   = generate_generic_copy(\"generic_arraycopy\",\n+                                                             entry_jbyte_arraycopy,\n+                                                             entry_jshort_arraycopy,\n+                                                             entry_jint_arraycopy,\n+                                                             entry_oop_arraycopy,\n+                                                             entry_jlong_arraycopy,\n+                                                             entry_checkcast_arraycopy);\n+\n+  StubRoutines::_jbyte_fill = generate_fill(T_BYTE, false, \"jbyte_fill\");\n+  StubRoutines::_jshort_fill = generate_fill(T_SHORT, false, \"jshort_fill\");\n+  StubRoutines::_jint_fill = generate_fill(T_INT, false, \"jint_fill\");\n+  StubRoutines::_arrayof_jbyte_fill = generate_fill(T_BYTE, true, \"arrayof_jbyte_fill\");\n+  StubRoutines::_arrayof_jshort_fill = generate_fill(T_SHORT, true, \"arrayof_jshort_fill\");\n+  StubRoutines::_arrayof_jint_fill = generate_fill(T_INT, true, \"arrayof_jint_fill\");\n+\n+  \/\/ We don't generate specialized code for HeapWord-aligned source\n+  \/\/ arrays, so just use the code we've already generated\n+  StubRoutines::_arrayof_jbyte_disjoint_arraycopy  = StubRoutines::_jbyte_disjoint_arraycopy;\n+  StubRoutines::_arrayof_jbyte_arraycopy           = StubRoutines::_jbyte_arraycopy;\n+\n+  StubRoutines::_arrayof_jshort_disjoint_arraycopy = StubRoutines::_jshort_disjoint_arraycopy;\n+  StubRoutines::_arrayof_jshort_arraycopy          = StubRoutines::_jshort_arraycopy;\n+\n+  StubRoutines::_arrayof_jint_disjoint_arraycopy   = StubRoutines::_jint_disjoint_arraycopy;\n+  StubRoutines::_arrayof_jint_arraycopy            = StubRoutines::_jint_arraycopy;\n+\n+  StubRoutines::_arrayof_jlong_disjoint_arraycopy  = StubRoutines::_jlong_disjoint_arraycopy;\n+  StubRoutines::_arrayof_jlong_arraycopy           = StubRoutines::_jlong_arraycopy;\n+\n+  StubRoutines::_arrayof_oop_disjoint_arraycopy    = StubRoutines::_oop_disjoint_arraycopy;\n+  StubRoutines::_arrayof_oop_arraycopy             = StubRoutines::_oop_arraycopy;\n+\n+  StubRoutines::_arrayof_oop_disjoint_arraycopy_uninit    = StubRoutines::_oop_disjoint_arraycopy_uninit;\n+  StubRoutines::_arrayof_oop_arraycopy_uninit             = StubRoutines::_oop_arraycopy_uninit;\n+}\n+\n+\n+\/\/ Verify that a register contains clean 32-bits positive value\n+\/\/ (high 32-bits are 0) so it could be used in 64-bits shifts.\n+\/\/\n+\/\/  Input:\n+\/\/    Rint  -  32-bits value\n+\/\/    Rtmp  -  scratch\n+\/\/\n+void StubGenerator::assert_clean_int(Register Rint, Register Rtmp) {\n+#ifdef ASSERT\n+  Label L;\n+  assert_different_registers(Rtmp, Rint);\n+  __ movslq(Rtmp, Rint);\n+  __ cmpq(Rtmp, Rint);\n+  __ jcc(Assembler::equal, L);\n+  __ stop(\"high 32-bits of int value are not 0\");\n+  __ bind(L);\n+#endif\n+}\n+\n+\n+\/\/  Generate overlap test for array copy stubs\n+\/\/\n+\/\/  Input:\n+\/\/     c_rarg0 - from\n+\/\/     c_rarg1 - to\n+\/\/     c_rarg2 - element count\n+\/\/\n+\/\/  Output:\n+\/\/     rax   - &from[element count - 1]\n+\/\/\n+void StubGenerator::array_overlap_test(address no_overlap_target, Label* NOLp, Address::ScaleFactor sf) {\n+  const Register from     = c_rarg0;\n+  const Register to       = c_rarg1;\n+  const Register count    = c_rarg2;\n+  const Register end_from = rax;\n+\n+  __ cmpptr(to, from);\n+  __ lea(end_from, Address(from, count, sf, 0));\n+  if (NOLp == NULL) {\n+    ExternalAddress no_overlap(no_overlap_target);\n+    __ jump_cc(Assembler::belowEqual, no_overlap);\n+    __ cmpptr(to, end_from);\n+    __ jump_cc(Assembler::aboveEqual, no_overlap);\n+  } else {\n+    __ jcc(Assembler::belowEqual, (*NOLp));\n+    __ cmpptr(to, end_from);\n+    __ jcc(Assembler::aboveEqual, (*NOLp));\n+  }\n+}\n+\n+\n+\/\/ Copy big chunks forward\n+\/\/\n+\/\/ Inputs:\n+\/\/   end_from     - source arrays end address\n+\/\/   end_to       - destination array end address\n+\/\/   qword_count  - 64-bits element count, negative\n+\/\/   to           - scratch\n+\/\/   L_copy_bytes - entry label\n+\/\/   L_copy_8_bytes  - exit  label\n+\/\/\n+void StubGenerator::copy_bytes_forward(Register end_from, Register end_to,\n+                                       Register qword_count, Register to,\n+                                       Label& L_copy_bytes, Label& L_copy_8_bytes) {\n+  DEBUG_ONLY(__ stop(\"enter at entry label, not here\"));\n+  Label L_loop;\n+  __ align(OptoLoopAlignment);\n+  if (UseUnalignedLoadStores) {\n+    Label L_end;\n+    __ BIND(L_loop);\n+    if (UseAVX >= 2) {\n+      __ vmovdqu(xmm0, Address(end_from, qword_count, Address::times_8, -56));\n+      __ vmovdqu(Address(end_to, qword_count, Address::times_8, -56), xmm0);\n+      __ vmovdqu(xmm1, Address(end_from, qword_count, Address::times_8, -24));\n+      __ vmovdqu(Address(end_to, qword_count, Address::times_8, -24), xmm1);\n+    } else {\n+      __ movdqu(xmm0, Address(end_from, qword_count, Address::times_8, -56));\n+      __ movdqu(Address(end_to, qword_count, Address::times_8, -56), xmm0);\n+      __ movdqu(xmm1, Address(end_from, qword_count, Address::times_8, -40));\n+      __ movdqu(Address(end_to, qword_count, Address::times_8, -40), xmm1);\n+      __ movdqu(xmm2, Address(end_from, qword_count, Address::times_8, -24));\n+      __ movdqu(Address(end_to, qword_count, Address::times_8, -24), xmm2);\n+      __ movdqu(xmm3, Address(end_from, qword_count, Address::times_8, - 8));\n+      __ movdqu(Address(end_to, qword_count, Address::times_8, - 8), xmm3);\n+    }\n+\n+    __ BIND(L_copy_bytes);\n+    __ addptr(qword_count, 8);\n+    __ jcc(Assembler::lessEqual, L_loop);\n+    __ subptr(qword_count, 4);  \/\/ sub(8) and add(4)\n+    __ jccb(Assembler::greater, L_end);\n+    \/\/ Copy trailing 32 bytes\n+    if (UseAVX >= 2) {\n+      __ vmovdqu(xmm0, Address(end_from, qword_count, Address::times_8, -24));\n+      __ vmovdqu(Address(end_to, qword_count, Address::times_8, -24), xmm0);\n+    } else {\n+      __ movdqu(xmm0, Address(end_from, qword_count, Address::times_8, -24));\n+      __ movdqu(Address(end_to, qword_count, Address::times_8, -24), xmm0);\n+      __ movdqu(xmm1, Address(end_from, qword_count, Address::times_8, - 8));\n+      __ movdqu(Address(end_to, qword_count, Address::times_8, - 8), xmm1);\n+    }\n+    __ addptr(qword_count, 4);\n+    __ BIND(L_end);\n+  } else {\n+    \/\/ Copy 32-bytes per iteration\n+    __ BIND(L_loop);\n+    __ movq(to, Address(end_from, qword_count, Address::times_8, -24));\n+    __ movq(Address(end_to, qword_count, Address::times_8, -24), to);\n+    __ movq(to, Address(end_from, qword_count, Address::times_8, -16));\n+    __ movq(Address(end_to, qword_count, Address::times_8, -16), to);\n+    __ movq(to, Address(end_from, qword_count, Address::times_8, - 8));\n+    __ movq(Address(end_to, qword_count, Address::times_8, - 8), to);\n+    __ movq(to, Address(end_from, qword_count, Address::times_8, - 0));\n+    __ movq(Address(end_to, qword_count, Address::times_8, - 0), to);\n+\n+    __ BIND(L_copy_bytes);\n+    __ addptr(qword_count, 4);\n+    __ jcc(Assembler::lessEqual, L_loop);\n+  }\n+  __ subptr(qword_count, 4);\n+  __ jcc(Assembler::less, L_copy_8_bytes); \/\/ Copy trailing qwords\n+}\n+\n+\n+\/\/ Copy big chunks backward\n+\/\/\n+\/\/ Inputs:\n+\/\/   from         - source arrays address\n+\/\/   dest         - destination array address\n+\/\/   qword_count  - 64-bits element count\n+\/\/   to           - scratch\n+\/\/   L_copy_bytes - entry label\n+\/\/   L_copy_8_bytes  - exit  label\n+\/\/\n+void StubGenerator::copy_bytes_backward(Register from, Register dest,\n+                                        Register qword_count, Register to,\n+                                        Label& L_copy_bytes, Label& L_copy_8_bytes) {\n+  DEBUG_ONLY(__ stop(\"enter at entry label, not here\"));\n+  Label L_loop;\n+  __ align(OptoLoopAlignment);\n+  if (UseUnalignedLoadStores) {\n+    Label L_end;\n+    __ BIND(L_loop);\n+    if (UseAVX >= 2) {\n+      __ vmovdqu(xmm0, Address(from, qword_count, Address::times_8, 32));\n+      __ vmovdqu(Address(dest, qword_count, Address::times_8, 32), xmm0);\n+      __ vmovdqu(xmm1, Address(from, qword_count, Address::times_8,  0));\n+      __ vmovdqu(Address(dest, qword_count, Address::times_8,  0), xmm1);\n+    } else {\n+      __ movdqu(xmm0, Address(from, qword_count, Address::times_8, 48));\n+      __ movdqu(Address(dest, qword_count, Address::times_8, 48), xmm0);\n+      __ movdqu(xmm1, Address(from, qword_count, Address::times_8, 32));\n+      __ movdqu(Address(dest, qword_count, Address::times_8, 32), xmm1);\n+      __ movdqu(xmm2, Address(from, qword_count, Address::times_8, 16));\n+      __ movdqu(Address(dest, qword_count, Address::times_8, 16), xmm2);\n+      __ movdqu(xmm3, Address(from, qword_count, Address::times_8,  0));\n+      __ movdqu(Address(dest, qword_count, Address::times_8,  0), xmm3);\n+    }\n+\n+    __ BIND(L_copy_bytes);\n+    __ subptr(qword_count, 8);\n+    __ jcc(Assembler::greaterEqual, L_loop);\n+\n+    __ addptr(qword_count, 4);  \/\/ add(8) and sub(4)\n+    __ jccb(Assembler::less, L_end);\n+    \/\/ Copy trailing 32 bytes\n+    if (UseAVX >= 2) {\n+      __ vmovdqu(xmm0, Address(from, qword_count, Address::times_8, 0));\n+      __ vmovdqu(Address(dest, qword_count, Address::times_8, 0), xmm0);\n+    } else {\n+      __ movdqu(xmm0, Address(from, qword_count, Address::times_8, 16));\n+      __ movdqu(Address(dest, qword_count, Address::times_8, 16), xmm0);\n+      __ movdqu(xmm1, Address(from, qword_count, Address::times_8,  0));\n+      __ movdqu(Address(dest, qword_count, Address::times_8,  0), xmm1);\n+    }\n+    __ subptr(qword_count, 4);\n+    __ BIND(L_end);\n+  } else {\n+    \/\/ Copy 32-bytes per iteration\n+    __ BIND(L_loop);\n+    __ movq(to, Address(from, qword_count, Address::times_8, 24));\n+    __ movq(Address(dest, qword_count, Address::times_8, 24), to);\n+    __ movq(to, Address(from, qword_count, Address::times_8, 16));\n+    __ movq(Address(dest, qword_count, Address::times_8, 16), to);\n+    __ movq(to, Address(from, qword_count, Address::times_8,  8));\n+    __ movq(Address(dest, qword_count, Address::times_8,  8), to);\n+    __ movq(to, Address(from, qword_count, Address::times_8,  0));\n+    __ movq(Address(dest, qword_count, Address::times_8,  0), to);\n+\n+    __ BIND(L_copy_bytes);\n+    __ subptr(qword_count, 4);\n+    __ jcc(Assembler::greaterEqual, L_loop);\n+  }\n+  __ addptr(qword_count, 4);\n+  __ jcc(Assembler::greater, L_copy_8_bytes); \/\/ Copy trailing qwords\n+}\n+\n+#if COMPILER2_OR_JVMCI\n+\n+\/\/ Note: Following rules apply to AVX3 optimized arraycopy stubs:-\n+\/\/ - If target supports AVX3 features (BW+VL+F) then implementation uses 32 byte vectors (YMMs)\n+\/\/   for both special cases (various small block sizes) and aligned copy loop. This is the\n+\/\/   default configuration.\n+\/\/ - If copy length is above AVX3Threshold, then implementation use 64 byte vectors (ZMMs)\n+\/\/   for main copy loop (and subsequent tail) since bulk of the cycles will be consumed in it.\n+\/\/ - If user forces MaxVectorSize=32 then above 4096 bytes its seen that REP MOVs shows a\n+\/\/   better performance for disjoint copies. For conjoint\/backward copy vector based\n+\/\/   copy performs better.\n+\/\/ - If user sets AVX3Threshold=0, then special cases for small blocks sizes operate over\n+\/\/   64 byte vector registers (ZMMs).\n+\n+\/\/ Inputs:\n+\/\/   c_rarg0   - source array address\n+\/\/   c_rarg1   - destination array address\n+\/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n+\/\/\n+\/\/\n+\/\/ Side Effects:\n+\/\/   disjoint_copy_avx3_masked is set to the no-overlap entry point\n+\/\/   used by generate_conjoint_[byte\/int\/short\/long]_copy().\n+\/\/\n+address StubGenerator::generate_disjoint_copy_avx3_masked(address* entry, const char *name,\n+                                                          int shift, bool aligned, bool is_oop,\n+                                                          bool dest_uninitialized) {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", name);\n+  address start = __ pc();\n+\n+  int avx3threshold = VM_Version::avx3_threshold();\n+  bool use64byteVector = (MaxVectorSize > 32) && (avx3threshold == 0);\n+  Label L_main_loop, L_main_loop_64bytes, L_tail, L_tail64, L_exit, L_entry;\n+  Label L_repmovs, L_main_pre_loop, L_main_pre_loop_64bytes, L_pre_main_post_64;\n+  const Register from        = rdi;  \/\/ source array address\n+  const Register to          = rsi;  \/\/ destination array address\n+  const Register count       = rdx;  \/\/ elements count\n+  const Register temp1       = r8;\n+  const Register temp2       = r11;\n+  const Register temp3       = rax;\n+  const Register temp4       = rcx;\n+  \/\/ End pointers are inclusive, and if count is not zero they point\n+  \/\/ to the last unit copied:  end_to[0] := end_from[0]\n+\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  assert_clean_int(c_rarg2, rax);    \/\/ Make sure 'count' is clean int.\n+\n+  if (entry != NULL) {\n+    *entry = __ pc();\n+     \/\/ caller can pass a 64-bit byte count here (from Unsafe.copyMemory)\n+    BLOCK_COMMENT(\"Entry:\");\n+  }\n+\n+  BasicType type_vec[] = { T_BYTE,  T_SHORT,  T_INT,   T_LONG};\n+  BasicType type = is_oop ? T_OBJECT : type_vec[shift];\n+\n+  setup_argument_regs(type);\n+\n+  DecoratorSet decorators = IN_HEAP | IS_ARRAY | ARRAYCOPY_DISJOINT;\n+  if (dest_uninitialized) {\n+    decorators |= IS_DEST_UNINITIALIZED;\n+  }\n+  if (aligned) {\n+    decorators |= ARRAYCOPY_ALIGNED;\n+  }\n+  BarrierSetAssembler *bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  bs->arraycopy_prologue(_masm, decorators, type, from, to, count);\n+\n+  {\n+    \/\/ Type(shift)           byte(0), short(1), int(2),   long(3)\n+    int loop_size[]        = { 192,     96,       48,      24};\n+    int threshold[]        = { 4096,    2048,     1024,    512};\n+\n+    \/\/ UnsafeCopyMemory page error: continue after ucm\n+    UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n+    \/\/ 'from', 'to' and 'count' are now valid\n+\n+    \/\/ temp1 holds remaining count and temp4 holds running count used to compute\n+    \/\/ next address offset for start of to\/from addresses (temp4 * scale).\n+    __ mov64(temp4, 0);\n+    __ movq(temp1, count);\n+\n+    \/\/ Zero length check.\n+    __ BIND(L_tail);\n+    __ cmpq(temp1, 0);\n+    __ jcc(Assembler::lessEqual, L_exit);\n+\n+    \/\/ Special cases using 32 byte [masked] vector copy operations.\n+    arraycopy_avx3_special_cases(xmm1, k2, from, to, temp1, shift,\n+                                 temp4, temp3, use64byteVector, L_entry, L_exit);\n+\n+    \/\/ PRE-MAIN-POST loop for aligned copy.\n+    __ BIND(L_entry);\n+\n+    if (avx3threshold != 0) {\n+      __ cmpq(count, threshold[shift]);\n+      if (MaxVectorSize == 64) {\n+        \/\/ Copy using 64 byte vectors.\n+        __ jcc(Assembler::greaterEqual, L_pre_main_post_64);\n+      } else {\n+        assert(MaxVectorSize < 64, \"vector size should be < 64 bytes\");\n+        \/\/ REP MOVS offer a faster copy path.\n+        __ jcc(Assembler::greaterEqual, L_repmovs);\n+      }\n+    }\n+\n+    if ((MaxVectorSize < 64)  || (avx3threshold != 0)) {\n+      \/\/ Partial copy to make dst address 32 byte aligned.\n+      __ movq(temp2, to);\n+      __ andq(temp2, 31);\n+      __ jcc(Assembler::equal, L_main_pre_loop);\n+\n+      __ negptr(temp2);\n+      __ addq(temp2, 32);\n+      if (shift) {\n+        __ shrq(temp2, shift);\n+      }\n+      __ movq(temp3, temp2);\n+      copy32_masked_avx(to, from, xmm1, k2, temp3, temp4, temp1, shift);\n+      __ movq(temp4, temp2);\n+      __ movq(temp1, count);\n+      __ subq(temp1, temp2);\n+\n+      __ cmpq(temp1, loop_size[shift]);\n+      __ jcc(Assembler::less, L_tail);\n+\n+      __ BIND(L_main_pre_loop);\n+      __ subq(temp1, loop_size[shift]);\n+\n+      \/\/ Main loop with aligned copy block size of 192 bytes at 32 byte granularity.\n+      __ align32();\n+      __ BIND(L_main_loop);\n+         copy64_avx(to, from, temp4, xmm1, false, shift, 0);\n+         copy64_avx(to, from, temp4, xmm1, false, shift, 64);\n+         copy64_avx(to, from, temp4, xmm1, false, shift, 128);\n+         __ addptr(temp4, loop_size[shift]);\n+         __ subq(temp1, loop_size[shift]);\n+         __ jcc(Assembler::greater, L_main_loop);\n+\n+      __ addq(temp1, loop_size[shift]);\n+\n+      \/\/ Tail loop.\n+      __ jmp(L_tail);\n+\n+      __ BIND(L_repmovs);\n+        __ movq(temp2, temp1);\n+        \/\/ Swap to(RSI) and from(RDI) addresses to comply with REP MOVs semantics.\n+        __ movq(temp3, to);\n+        __ movq(to,  from);\n+        __ movq(from, temp3);\n+        \/\/ Save to\/from for restoration post rep_mov.\n+        __ movq(temp1, to);\n+        __ movq(temp3, from);\n+        if(shift < 3) {\n+          __ shrq(temp2, 3-shift);     \/\/ quad word count\n+        }\n+        __ movq(temp4 , temp2);        \/\/ move quad ward count into temp4(RCX).\n+        __ rep_mov();\n+        __ shlq(temp2, 3);             \/\/ convert quad words into byte count.\n+        if(shift) {\n+          __ shrq(temp2, shift);       \/\/ type specific count.\n+        }\n+        \/\/ Restore original addresses in to\/from.\n+        __ movq(to, temp3);\n+        __ movq(from, temp1);\n+        __ movq(temp4, temp2);\n+        __ movq(temp1, count);\n+        __ subq(temp1, temp2);         \/\/ tailing part (less than a quad ward size).\n+        __ jmp(L_tail);\n+    }\n+\n+    if (MaxVectorSize > 32) {\n+      __ BIND(L_pre_main_post_64);\n+      \/\/ Partial copy to make dst address 64 byte aligned.\n+      __ movq(temp2, to);\n+      __ andq(temp2, 63);\n+      __ jcc(Assembler::equal, L_main_pre_loop_64bytes);\n+\n+      __ negptr(temp2);\n+      __ addq(temp2, 64);\n+      if (shift) {\n+        __ shrq(temp2, shift);\n+      }\n+      __ movq(temp3, temp2);\n+      copy64_masked_avx(to, from, xmm1, k2, temp3, temp4, temp1, shift, 0 , true);\n+      __ movq(temp4, temp2);\n+      __ movq(temp1, count);\n+      __ subq(temp1, temp2);\n+\n+      __ cmpq(temp1, loop_size[shift]);\n+      __ jcc(Assembler::less, L_tail64);\n+\n+      __ BIND(L_main_pre_loop_64bytes);\n+      __ subq(temp1, loop_size[shift]);\n+\n+      \/\/ Main loop with aligned copy block size of 192 bytes at\n+      \/\/ 64 byte copy granularity.\n+      __ align32();\n+      __ BIND(L_main_loop_64bytes);\n+         copy64_avx(to, from, temp4, xmm1, false, shift, 0 , true);\n+         copy64_avx(to, from, temp4, xmm1, false, shift, 64, true);\n+         copy64_avx(to, from, temp4, xmm1, false, shift, 128, true);\n+         __ addptr(temp4, loop_size[shift]);\n+         __ subq(temp1, loop_size[shift]);\n+         __ jcc(Assembler::greater, L_main_loop_64bytes);\n+\n+      __ addq(temp1, loop_size[shift]);\n+      \/\/ Zero length check.\n+      __ jcc(Assembler::lessEqual, L_exit);\n+\n+      __ BIND(L_tail64);\n+\n+      \/\/ Tail handling using 64 byte [masked] vector copy operations.\n+      use64byteVector = true;\n+      arraycopy_avx3_special_cases(xmm1, k2, from, to, temp1, shift,\n+                                   temp4, temp3, use64byteVector, L_entry, L_exit);\n+    }\n+    __ BIND(L_exit);\n+  }\n+\n+  address ucme_exit_pc = __ pc();\n+  \/\/ When called from generic_arraycopy r11 contains specific values\n+  \/\/ used during arraycopy epilogue, re-initializing r11.\n+  if (is_oop) {\n+    __ movq(r11, shift == 3 ? count : to);\n+  }\n+  bs->arraycopy_epilogue(_masm, decorators, type, from, to, count);\n+  restore_argument_regs(type);\n+  INC_COUNTER_NP(get_profile_ctr(shift), rscratch1); \/\/ Update counter after rscratch1 is free\n+  __ xorptr(rax, rax); \/\/ return 0\n+  __ vzeroupper();\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n+\n+  return start;\n+}\n+\n+\n+\/\/ Inputs:\n+\/\/   c_rarg0   - source array address\n+\/\/   c_rarg1   - destination array address\n+\/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n+\/\/\n+\/\/\n+address StubGenerator::generate_conjoint_copy_avx3_masked(address* entry, const char *name, int shift,\n+                                                          address nooverlap_target, bool aligned,\n+                                                          bool is_oop, bool dest_uninitialized) {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", name);\n+  address start = __ pc();\n+\n+  int avx3threshold = VM_Version::avx3_threshold();\n+  bool use64byteVector = (MaxVectorSize > 32) && (avx3threshold == 0);\n+\n+  Label L_main_pre_loop, L_main_pre_loop_64bytes, L_pre_main_post_64;\n+  Label L_main_loop, L_main_loop_64bytes, L_tail, L_tail64, L_exit, L_entry;\n+  const Register from        = rdi;  \/\/ source array address\n+  const Register to          = rsi;  \/\/ destination array address\n+  const Register count       = rdx;  \/\/ elements count\n+  const Register temp1       = r8;\n+  const Register temp2       = rcx;\n+  const Register temp3       = r11;\n+  const Register temp4       = rax;\n+  \/\/ End pointers are inclusive, and if count is not zero they point\n+  \/\/ to the last unit copied:  end_to[0] := end_from[0]\n+\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  assert_clean_int(c_rarg2, rax);    \/\/ Make sure 'count' is clean int.\n+\n+  if (entry != NULL) {\n+    *entry = __ pc();\n+     \/\/ caller can pass a 64-bit byte count here (from Unsafe.copyMemory)\n+    BLOCK_COMMENT(\"Entry:\");\n+  }\n+\n+  array_overlap_test(nooverlap_target, (Address::ScaleFactor)(shift));\n+\n+  BasicType type_vec[] = { T_BYTE,  T_SHORT,  T_INT,   T_LONG};\n+  BasicType type = is_oop ? T_OBJECT : type_vec[shift];\n+\n+  setup_argument_regs(type);\n+\n+  DecoratorSet decorators = IN_HEAP | IS_ARRAY;\n+  if (dest_uninitialized) {\n+    decorators |= IS_DEST_UNINITIALIZED;\n+  }\n+  if (aligned) {\n+    decorators |= ARRAYCOPY_ALIGNED;\n+  }\n+  BarrierSetAssembler *bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  bs->arraycopy_prologue(_masm, decorators, type, from, to, count);\n+  {\n+    \/\/ Type(shift)       byte(0), short(1), int(2),   long(3)\n+    int loop_size[]   = { 192,     96,       48,      24};\n+    int threshold[]   = { 4096,    2048,     1024,    512};\n+\n+    \/\/ UnsafeCopyMemory page error: continue after ucm\n+    UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n+    \/\/ 'from', 'to' and 'count' are now valid\n+\n+    \/\/ temp1 holds remaining count.\n+    __ movq(temp1, count);\n+\n+    \/\/ Zero length check.\n+    __ BIND(L_tail);\n+    __ cmpq(temp1, 0);\n+    __ jcc(Assembler::lessEqual, L_exit);\n+\n+    __ mov64(temp2, 0);\n+    __ movq(temp3, temp1);\n+    \/\/ Special cases using 32 byte [masked] vector copy operations.\n+    arraycopy_avx3_special_cases_conjoint(xmm1, k2, from, to, temp2, temp3, temp1, shift,\n+                                          temp4, use64byteVector, L_entry, L_exit);\n+\n+    \/\/ PRE-MAIN-POST loop for aligned copy.\n+    __ BIND(L_entry);\n+\n+    if ((MaxVectorSize > 32) && (avx3threshold != 0)) {\n+      __ cmpq(temp1, threshold[shift]);\n+      __ jcc(Assembler::greaterEqual, L_pre_main_post_64);\n+    }\n+\n+    if ((MaxVectorSize < 64)  || (avx3threshold != 0)) {\n+      \/\/ Partial copy to make dst address 32 byte aligned.\n+      __ leaq(temp2, Address(to, temp1, (Address::ScaleFactor)(shift), 0));\n+      __ andq(temp2, 31);\n+      __ jcc(Assembler::equal, L_main_pre_loop);\n+\n+      if (shift) {\n+        __ shrq(temp2, shift);\n+      }\n+      __ subq(temp1, temp2);\n+      copy32_masked_avx(to, from, xmm1, k2, temp2, temp1, temp3, shift);\n+\n+      __ cmpq(temp1, loop_size[shift]);\n+      __ jcc(Assembler::less, L_tail);\n+\n+      __ BIND(L_main_pre_loop);\n+\n+      \/\/ Main loop with aligned copy block size of 192 bytes at 32 byte granularity.\n+      __ align32();\n+      __ BIND(L_main_loop);\n+         copy64_avx(to, from, temp1, xmm1, true, shift, -64);\n+         copy64_avx(to, from, temp1, xmm1, true, shift, -128);\n+         copy64_avx(to, from, temp1, xmm1, true, shift, -192);\n+         __ subptr(temp1, loop_size[shift]);\n+         __ cmpq(temp1, loop_size[shift]);\n+         __ jcc(Assembler::greater, L_main_loop);\n+\n+      \/\/ Tail loop.\n+      __ jmp(L_tail);\n+    }\n+\n+    if (MaxVectorSize > 32) {\n+      __ BIND(L_pre_main_post_64);\n+      \/\/ Partial copy to make dst address 64 byte aligned.\n+      __ leaq(temp2, Address(to, temp1, (Address::ScaleFactor)(shift), 0));\n+      __ andq(temp2, 63);\n+      __ jcc(Assembler::equal, L_main_pre_loop_64bytes);\n+\n+      if (shift) {\n+        __ shrq(temp2, shift);\n+      }\n+      __ subq(temp1, temp2);\n+      copy64_masked_avx(to, from, xmm1, k2, temp2, temp1, temp3, shift, 0 , true);\n+\n+      __ cmpq(temp1, loop_size[shift]);\n+      __ jcc(Assembler::less, L_tail64);\n+\n+      __ BIND(L_main_pre_loop_64bytes);\n+\n+      \/\/ Main loop with aligned copy block size of 192 bytes at\n+      \/\/ 64 byte copy granularity.\n+      __ align32();\n+      __ BIND(L_main_loop_64bytes);\n+         copy64_avx(to, from, temp1, xmm1, true, shift, -64 , true);\n+         copy64_avx(to, from, temp1, xmm1, true, shift, -128, true);\n+         copy64_avx(to, from, temp1, xmm1, true, shift, -192, true);\n+         __ subq(temp1, loop_size[shift]);\n+         __ cmpq(temp1, loop_size[shift]);\n+         __ jcc(Assembler::greater, L_main_loop_64bytes);\n+\n+      \/\/ Zero length check.\n+      __ cmpq(temp1, 0);\n+      __ jcc(Assembler::lessEqual, L_exit);\n+\n+      __ BIND(L_tail64);\n+\n+      \/\/ Tail handling using 64 byte [masked] vector copy operations.\n+      use64byteVector = true;\n+      __ mov64(temp2, 0);\n+      __ movq(temp3, temp1);\n+      arraycopy_avx3_special_cases_conjoint(xmm1, k2, from, to, temp2, temp3, temp1, shift,\n+                                            temp4, use64byteVector, L_entry, L_exit);\n+    }\n+    __ BIND(L_exit);\n+  }\n+  address ucme_exit_pc = __ pc();\n+  \/\/ When called from generic_arraycopy r11 contains specific values\n+  \/\/ used during arraycopy epilogue, re-initializing r11.\n+  if(is_oop) {\n+    __ movq(r11, count);\n+  }\n+  bs->arraycopy_epilogue(_masm, decorators, type, from, to, count);\n+  restore_argument_regs(type);\n+  INC_COUNTER_NP(get_profile_ctr(shift), rscratch1); \/\/ Update counter after rscratch1 is free\n+  __ xorptr(rax, rax); \/\/ return 0\n+  __ vzeroupper();\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n+\n+  return start;\n+}\n+\n+void StubGenerator::arraycopy_avx3_special_cases(XMMRegister xmm, KRegister mask, Register from,\n+                                                 Register to, Register count, int shift,\n+                                                 Register index, Register temp,\n+                                                 bool use64byteVector, Label& L_entry, Label& L_exit) {\n+  Label L_entry_64, L_entry_96, L_entry_128;\n+  Label L_entry_160, L_entry_192;\n+\n+  int size_mat[][6] = {\n+  \/* T_BYTE *\/ {32 , 64,  96 , 128 , 160 , 192 },\n+  \/* T_SHORT*\/ {16 , 32,  48 , 64  , 80  , 96  },\n+  \/* T_INT  *\/ {8  , 16,  24 , 32  , 40  , 48  },\n+  \/* T_LONG *\/ {4  ,  8,  12 , 16  , 20  , 24  }\n+  };\n+\n+  \/\/ Case A) Special case for length less than equal to 32 bytes.\n+  __ cmpq(count, size_mat[shift][0]);\n+  __ jccb(Assembler::greater, L_entry_64);\n+  copy32_masked_avx(to, from, xmm, mask, count, index, temp, shift);\n+  __ jmp(L_exit);\n+\n+  \/\/ Case B) Special case for length less than equal to 64 bytes.\n+  __ BIND(L_entry_64);\n+  __ cmpq(count, size_mat[shift][1]);\n+  __ jccb(Assembler::greater, L_entry_96);\n+  copy64_masked_avx(to, from, xmm, mask, count, index, temp, shift, 0, use64byteVector);\n+  __ jmp(L_exit);\n+\n+  \/\/ Case C) Special case for length less than equal to 96 bytes.\n+  __ BIND(L_entry_96);\n+  __ cmpq(count, size_mat[shift][2]);\n+  __ jccb(Assembler::greater, L_entry_128);\n+  copy64_avx(to, from, index, xmm, false, shift, 0, use64byteVector);\n+  __ subq(count, 64 >> shift);\n+  copy32_masked_avx(to, from, xmm, mask, count, index, temp, shift, 64);\n+  __ jmp(L_exit);\n+\n+  \/\/ Case D) Special case for length less than equal to 128 bytes.\n+  __ BIND(L_entry_128);\n+  __ cmpq(count, size_mat[shift][3]);\n+  __ jccb(Assembler::greater, L_entry_160);\n+  copy64_avx(to, from, index, xmm, false, shift, 0, use64byteVector);\n+  copy32_avx(to, from, index, xmm, shift, 64);\n+  __ subq(count, 96 >> shift);\n+  copy32_masked_avx(to, from, xmm, mask, count, index, temp, shift, 96);\n+  __ jmp(L_exit);\n+\n+  \/\/ Case E) Special case for length less than equal to 160 bytes.\n+  __ BIND(L_entry_160);\n+  __ cmpq(count, size_mat[shift][4]);\n+  __ jccb(Assembler::greater, L_entry_192);\n+  copy64_avx(to, from, index, xmm, false, shift, 0, use64byteVector);\n+  copy64_avx(to, from, index, xmm, false, shift, 64, use64byteVector);\n+  __ subq(count, 128 >> shift);\n+  copy32_masked_avx(to, from, xmm, mask, count, index, temp, shift, 128);\n+  __ jmp(L_exit);\n+\n+  \/\/ Case F) Special case for length less than equal to 192 bytes.\n+  __ BIND(L_entry_192);\n+  __ cmpq(count, size_mat[shift][5]);\n+  __ jcc(Assembler::greater, L_entry);\n+  copy64_avx(to, from, index, xmm, false, shift, 0, use64byteVector);\n+  copy64_avx(to, from, index, xmm, false, shift, 64, use64byteVector);\n+  copy32_avx(to, from, index, xmm, shift, 128);\n+  __ subq(count, 160 >> shift);\n+  copy32_masked_avx(to, from, xmm, mask, count, index, temp, shift, 160);\n+  __ jmp(L_exit);\n+}\n+\n+void StubGenerator::arraycopy_avx3_special_cases_conjoint(XMMRegister xmm, KRegister mask, Register from,\n+                                                           Register to, Register start_index, Register end_index,\n+                                                           Register count, int shift, Register temp,\n+                                                           bool use64byteVector, Label& L_entry, Label& L_exit) {\n+  Label L_entry_64, L_entry_96, L_entry_128;\n+  Label L_entry_160, L_entry_192;\n+  bool avx3 = (MaxVectorSize > 32) && (VM_Version::avx3_threshold() == 0);\n+\n+  int size_mat[][6] = {\n+  \/* T_BYTE *\/ {32 , 64,  96 , 128 , 160 , 192 },\n+  \/* T_SHORT*\/ {16 , 32,  48 , 64  , 80  , 96  },\n+  \/* T_INT  *\/ {8  , 16,  24 , 32  , 40  , 48  },\n+  \/* T_LONG *\/ {4  ,  8,  12 , 16  , 20  , 24  }\n+  };\n+\n+  \/\/ Case A) Special case for length less than equal to 32 bytes.\n+  __ cmpq(count, size_mat[shift][0]);\n+  __ jccb(Assembler::greater, L_entry_64);\n+  copy32_masked_avx(to, from, xmm, mask, count, start_index, temp, shift);\n+  __ jmp(L_exit);\n+\n+  \/\/ Case B) Special case for length less than equal to 64 bytes.\n+  __ BIND(L_entry_64);\n+  __ cmpq(count, size_mat[shift][1]);\n+  __ jccb(Assembler::greater, L_entry_96);\n+  if (avx3) {\n+     copy64_masked_avx(to, from, xmm, mask, count, start_index, temp, shift, 0, true);\n+  } else {\n+     copy32_avx(to, from, end_index, xmm, shift, -32);\n+     __ subq(count, 32 >> shift);\n+     copy32_masked_avx(to, from, xmm, mask, count, start_index, temp, shift);\n+  }\n+  __ jmp(L_exit);\n+\n+  \/\/ Case C) Special case for length less than equal to 96 bytes.\n+  __ BIND(L_entry_96);\n+  __ cmpq(count, size_mat[shift][2]);\n+  __ jccb(Assembler::greater, L_entry_128);\n+  copy64_avx(to, from, end_index, xmm, true, shift, -64, use64byteVector);\n+  __ subq(count, 64 >> shift);\n+  copy32_masked_avx(to, from, xmm, mask, count, start_index, temp, shift);\n+  __ jmp(L_exit);\n+\n+  \/\/ Case D) Special case for length less than equal to 128 bytes.\n+  __ BIND(L_entry_128);\n+  __ cmpq(count, size_mat[shift][3]);\n+  __ jccb(Assembler::greater, L_entry_160);\n+  copy64_avx(to, from, end_index, xmm, true, shift, -64, use64byteVector);\n+  copy32_avx(to, from, end_index, xmm, shift, -96);\n+  __ subq(count, 96 >> shift);\n+  copy32_masked_avx(to, from, xmm, mask, count, start_index, temp, shift);\n+  __ jmp(L_exit);\n+\n+  \/\/ Case E) Special case for length less than equal to 160 bytes.\n+  __ BIND(L_entry_160);\n+  __ cmpq(count, size_mat[shift][4]);\n+  __ jccb(Assembler::greater, L_entry_192);\n+  copy64_avx(to, from, end_index, xmm, true, shift, -64, use64byteVector);\n+  copy64_avx(to, from, end_index, xmm, true, shift, -128, use64byteVector);\n+  __ subq(count, 128 >> shift);\n+  copy32_masked_avx(to, from, xmm, mask, count, start_index, temp, shift);\n+  __ jmp(L_exit);\n+\n+  \/\/ Case F) Special case for length less than equal to 192 bytes.\n+  __ BIND(L_entry_192);\n+  __ cmpq(count, size_mat[shift][5]);\n+  __ jcc(Assembler::greater, L_entry);\n+  copy64_avx(to, from, end_index, xmm, true, shift, -64, use64byteVector);\n+  copy64_avx(to, from, end_index, xmm, true, shift, -128, use64byteVector);\n+  copy32_avx(to, from, end_index, xmm, shift, -160);\n+  __ subq(count, 160 >> shift);\n+  copy32_masked_avx(to, from, xmm, mask, count, start_index, temp, shift);\n+  __ jmp(L_exit);\n+}\n+\n+void StubGenerator::copy64_masked_avx(Register dst, Register src, XMMRegister xmm,\n+                                       KRegister mask, Register length, Register index,\n+                                       Register temp, int shift, int offset,\n+                                       bool use64byteVector) {\n+  BasicType type[] = { T_BYTE,  T_SHORT,  T_INT,   T_LONG};\n+  assert(MaxVectorSize >= 32, \"vector length should be >= 32\");\n+  if (!use64byteVector) {\n+    copy32_avx(dst, src, index, xmm, shift, offset);\n+    __ subptr(length, 32 >> shift);\n+    copy32_masked_avx(dst, src, xmm, mask, length, index, temp, shift, offset+32);\n+  } else {\n+    Address::ScaleFactor scale = (Address::ScaleFactor)(shift);\n+    assert(MaxVectorSize == 64, \"vector length != 64\");\n+    __ mov64(temp, -1L);\n+    __ bzhiq(temp, temp, length);\n+    __ kmovql(mask, temp);\n+    __ evmovdqu(type[shift], mask, xmm, Address(src, index, scale, offset), false, Assembler::AVX_512bit);\n+    __ evmovdqu(type[shift], mask, Address(dst, index, scale, offset), xmm, true, Assembler::AVX_512bit);\n+  }\n+}\n+\n+\n+void StubGenerator::copy32_masked_avx(Register dst, Register src, XMMRegister xmm,\n+                                       KRegister mask, Register length, Register index,\n+                                       Register temp, int shift, int offset) {\n+  assert(MaxVectorSize >= 32, \"vector length should be >= 32\");\n+  BasicType type[] = { T_BYTE,  T_SHORT,  T_INT,   T_LONG};\n+  Address::ScaleFactor scale = (Address::ScaleFactor)(shift);\n+  __ mov64(temp, -1L);\n+  __ bzhiq(temp, temp, length);\n+  __ kmovql(mask, temp);\n+  __ evmovdqu(type[shift], mask, xmm, Address(src, index, scale, offset), false, Assembler::AVX_256bit);\n+  __ evmovdqu(type[shift], mask, Address(dst, index, scale, offset), xmm, true, Assembler::AVX_256bit);\n+}\n+\n+\n+void StubGenerator::copy32_avx(Register dst, Register src, Register index, XMMRegister xmm,\n+                                int shift, int offset) {\n+  assert(MaxVectorSize >= 32, \"vector length should be >= 32\");\n+  Address::ScaleFactor scale = (Address::ScaleFactor)(shift);\n+  __ vmovdqu(xmm, Address(src, index, scale, offset));\n+  __ vmovdqu(Address(dst, index, scale, offset), xmm);\n+}\n+\n+\n+void StubGenerator::copy64_avx(Register dst, Register src, Register index, XMMRegister xmm,\n+                                bool conjoint, int shift, int offset, bool use64byteVector) {\n+  assert(MaxVectorSize == 64 || MaxVectorSize == 32, \"vector length mismatch\");\n+  if (!use64byteVector) {\n+    if (conjoint) {\n+      copy32_avx(dst, src, index, xmm, shift, offset+32);\n+      copy32_avx(dst, src, index, xmm, shift, offset);\n+    } else {\n+      copy32_avx(dst, src, index, xmm, shift, offset);\n+      copy32_avx(dst, src, index, xmm, shift, offset+32);\n+    }\n+  } else {\n+    Address::ScaleFactor scale = (Address::ScaleFactor)(shift);\n+    __ evmovdquq(xmm, Address(src, index, scale, offset), Assembler::AVX_512bit);\n+    __ evmovdquq(Address(dst, index, scale, offset), xmm, Assembler::AVX_512bit);\n+  }\n+}\n+\n+#endif \/\/ COMPILER2_OR_JVMCI\n+\n+\n+\/\/ Arguments:\n+\/\/   aligned - true => Input and output aligned on a HeapWord == 8-byte boundary\n+\/\/             ignored\n+\/\/   name    - stub name string\n+\/\/\n+\/\/ Inputs:\n+\/\/   c_rarg0   - source array address\n+\/\/   c_rarg1   - destination array address\n+\/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n+\/\/\n+\/\/ If 'from' and\/or 'to' are aligned on 4-, 2-, or 1-byte boundaries,\n+\/\/ we let the hardware handle it.  The one to eight bytes within words,\n+\/\/ dwords or qwords that span cache line boundaries will still be loaded\n+\/\/ and stored atomically.\n+\/\/\n+\/\/ Side Effects:\n+\/\/   disjoint_byte_copy_entry is set to the no-overlap entry point\n+\/\/   used by generate_conjoint_byte_copy().\n+\/\/\n+address StubGenerator::generate_disjoint_byte_copy(bool aligned, address* entry, const char *name) {\n+#if COMPILER2_OR_JVMCI\n+  if (VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2() && MaxVectorSize  >= 32) {\n+     return generate_disjoint_copy_avx3_masked(entry, \"jbyte_disjoint_arraycopy_avx3\", 0,\n+                                               aligned, false, false);\n+  }\n+#endif\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", name);\n+  address start = __ pc();\n+\n+  Label L_copy_bytes, L_copy_8_bytes, L_copy_4_bytes, L_copy_2_bytes;\n+  Label L_copy_byte, L_exit;\n+  const Register from        = rdi;  \/\/ source array address\n+  const Register to          = rsi;  \/\/ destination array address\n+  const Register count       = rdx;  \/\/ elements count\n+  const Register byte_count  = rcx;\n+  const Register qword_count = count;\n+  const Register end_from    = from; \/\/ source array end address\n+  const Register end_to      = to;   \/\/ destination array end address\n+  \/\/ End pointers are inclusive, and if count is not zero they point\n+  \/\/ to the last unit copied:  end_to[0] := end_from[0]\n+\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  assert_clean_int(c_rarg2, rax);    \/\/ Make sure 'count' is clean int.\n+\n+  if (entry != NULL) {\n+    *entry = __ pc();\n+     \/\/ caller can pass a 64-bit byte count here (from Unsafe.copyMemory)\n+    BLOCK_COMMENT(\"Entry:\");\n+  }\n+\n+  setup_arg_regs(); \/\/ from => rdi, to => rsi, count => rdx\n+                    \/\/ r9 and r10 may be used to save non-volatile registers\n+\n+  {\n+    \/\/ UnsafeCopyMemory page error: continue after ucm\n+    UnsafeCopyMemoryMark ucmm(this, !aligned, true);\n+    \/\/ 'from', 'to' and 'count' are now valid\n+    __ movptr(byte_count, count);\n+    __ shrptr(count, 3); \/\/ count => qword_count\n+\n+    \/\/ Copy from low to high addresses.  Use 'to' as scratch.\n+    __ lea(end_from, Address(from, qword_count, Address::times_8, -8));\n+    __ lea(end_to,   Address(to,   qword_count, Address::times_8, -8));\n+    __ negptr(qword_count); \/\/ make the count negative\n+    __ jmp(L_copy_bytes);\n+\n+    \/\/ Copy trailing qwords\n+  __ BIND(L_copy_8_bytes);\n+    __ movq(rax, Address(end_from, qword_count, Address::times_8, 8));\n+    __ movq(Address(end_to, qword_count, Address::times_8, 8), rax);\n+    __ increment(qword_count);\n+    __ jcc(Assembler::notZero, L_copy_8_bytes);\n+\n+    \/\/ Check for and copy trailing dword\n+  __ BIND(L_copy_4_bytes);\n+    __ testl(byte_count, 4);\n+    __ jccb(Assembler::zero, L_copy_2_bytes);\n+    __ movl(rax, Address(end_from, 8));\n+    __ movl(Address(end_to, 8), rax);\n+\n+    __ addptr(end_from, 4);\n+    __ addptr(end_to, 4);\n+\n+    \/\/ Check for and copy trailing word\n+  __ BIND(L_copy_2_bytes);\n+    __ testl(byte_count, 2);\n+    __ jccb(Assembler::zero, L_copy_byte);\n+    __ movw(rax, Address(end_from, 8));\n+    __ movw(Address(end_to, 8), rax);\n+\n+    __ addptr(end_from, 2);\n+    __ addptr(end_to, 2);\n+\n+    \/\/ Check for and copy trailing byte\n+  __ BIND(L_copy_byte);\n+    __ testl(byte_count, 1);\n+    __ jccb(Assembler::zero, L_exit);\n+    __ movb(rax, Address(end_from, 8));\n+    __ movb(Address(end_to, 8), rax);\n+  }\n+__ BIND(L_exit);\n+  address ucme_exit_pc = __ pc();\n+  restore_arg_regs();\n+  INC_COUNTER_NP(SharedRuntime::_jbyte_array_copy_ctr, rscratch1); \/\/ Update counter after rscratch1 is free\n+  __ xorptr(rax, rax); \/\/ return 0\n+  __ vzeroupper();\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n+\n+  {\n+    UnsafeCopyMemoryMark ucmm(this, !aligned, false, ucme_exit_pc);\n+    \/\/ Copy in multi-bytes chunks\n+    copy_bytes_forward(end_from, end_to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n+    __ jmp(L_copy_4_bytes);\n+  }\n+  return start;\n+}\n+\n+\n+\/\/ Arguments:\n+\/\/   aligned - true => Input and output aligned on a HeapWord == 8-byte boundary\n+\/\/             ignored\n+\/\/   name    - stub name string\n+\/\/\n+\/\/ Inputs:\n+\/\/   c_rarg0   - source array address\n+\/\/   c_rarg1   - destination array address\n+\/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n+\/\/\n+\/\/ If 'from' and\/or 'to' are aligned on 4-, 2-, or 1-byte boundaries,\n+\/\/ we let the hardware handle it.  The one to eight bytes within words,\n+\/\/ dwords or qwords that span cache line boundaries will still be loaded\n+\/\/ and stored atomically.\n+\/\/\n+address StubGenerator::generate_conjoint_byte_copy(bool aligned, address nooverlap_target,\n+                                                   address* entry, const char *name) {\n+#if COMPILER2_OR_JVMCI\n+  if (VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2() && MaxVectorSize  >= 32) {\n+     return generate_conjoint_copy_avx3_masked(entry, \"jbyte_conjoint_arraycopy_avx3\", 0,\n+                                               nooverlap_target, aligned, false, false);\n+  }\n+#endif\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", name);\n+  address start = __ pc();\n+\n+  Label L_copy_bytes, L_copy_8_bytes, L_copy_4_bytes, L_copy_2_bytes;\n+  const Register from        = rdi;  \/\/ source array address\n+  const Register to          = rsi;  \/\/ destination array address\n+  const Register count       = rdx;  \/\/ elements count\n+  const Register byte_count  = rcx;\n+  const Register qword_count = count;\n+\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  assert_clean_int(c_rarg2, rax);    \/\/ Make sure 'count' is clean int.\n+\n+  if (entry != NULL) {\n+    *entry = __ pc();\n+    \/\/ caller can pass a 64-bit byte count here (from Unsafe.copyMemory)\n+    BLOCK_COMMENT(\"Entry:\");\n+  }\n+\n+  array_overlap_test(nooverlap_target, Address::times_1);\n+  setup_arg_regs(); \/\/ from => rdi, to => rsi, count => rdx\n+                    \/\/ r9 and r10 may be used to save non-volatile registers\n+\n+  {\n+    \/\/ UnsafeCopyMemory page error: continue after ucm\n+    UnsafeCopyMemoryMark ucmm(this, !aligned, true);\n+    \/\/ 'from', 'to' and 'count' are now valid\n+    __ movptr(byte_count, count);\n+    __ shrptr(count, 3);   \/\/ count => qword_count\n+\n+    \/\/ Copy from high to low addresses.\n+\n+    \/\/ Check for and copy trailing byte\n+    __ testl(byte_count, 1);\n+    __ jcc(Assembler::zero, L_copy_2_bytes);\n+    __ movb(rax, Address(from, byte_count, Address::times_1, -1));\n+    __ movb(Address(to, byte_count, Address::times_1, -1), rax);\n+    __ decrement(byte_count); \/\/ Adjust for possible trailing word\n+\n+    \/\/ Check for and copy trailing word\n+  __ BIND(L_copy_2_bytes);\n+    __ testl(byte_count, 2);\n+    __ jcc(Assembler::zero, L_copy_4_bytes);\n+    __ movw(rax, Address(from, byte_count, Address::times_1, -2));\n+    __ movw(Address(to, byte_count, Address::times_1, -2), rax);\n+\n+    \/\/ Check for and copy trailing dword\n+  __ BIND(L_copy_4_bytes);\n+    __ testl(byte_count, 4);\n+    __ jcc(Assembler::zero, L_copy_bytes);\n+    __ movl(rax, Address(from, qword_count, Address::times_8));\n+    __ movl(Address(to, qword_count, Address::times_8), rax);\n+    __ jmp(L_copy_bytes);\n+\n+    \/\/ Copy trailing qwords\n+  __ BIND(L_copy_8_bytes);\n+    __ movq(rax, Address(from, qword_count, Address::times_8, -8));\n+    __ movq(Address(to, qword_count, Address::times_8, -8), rax);\n+    __ decrement(qword_count);\n+    __ jcc(Assembler::notZero, L_copy_8_bytes);\n+  }\n+  restore_arg_regs();\n+  INC_COUNTER_NP(SharedRuntime::_jbyte_array_copy_ctr, rscratch1); \/\/ Update counter after rscratch1 is free\n+  __ xorptr(rax, rax); \/\/ return 0\n+  __ vzeroupper();\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n+\n+  {\n+    \/\/ UnsafeCopyMemory page error: continue after ucm\n+    UnsafeCopyMemoryMark ucmm(this, !aligned, true);\n+    \/\/ Copy in multi-bytes chunks\n+    copy_bytes_backward(from, to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n+  }\n+  restore_arg_regs();\n+  INC_COUNTER_NP(SharedRuntime::_jbyte_array_copy_ctr, rscratch1); \/\/ Update counter after rscratch1 is free\n+  __ xorptr(rax, rax); \/\/ return 0\n+  __ vzeroupper();\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n+\n+  return start;\n+}\n+\n+\n+\/\/ Arguments:\n+\/\/   aligned - true => Input and output aligned on a HeapWord == 8-byte boundary\n+\/\/             ignored\n+\/\/   name    - stub name string\n+\/\/\n+\/\/ Inputs:\n+\/\/   c_rarg0   - source array address\n+\/\/   c_rarg1   - destination array address\n+\/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n+\/\/\n+\/\/ If 'from' and\/or 'to' are aligned on 4- or 2-byte boundaries, we\n+\/\/ let the hardware handle it.  The two or four words within dwords\n+\/\/ or qwords that span cache line boundaries will still be loaded\n+\/\/ and stored atomically.\n+\/\/\n+\/\/ Side Effects:\n+\/\/   disjoint_short_copy_entry is set to the no-overlap entry point\n+\/\/   used by generate_conjoint_short_copy().\n+\/\/\n+address StubGenerator::generate_disjoint_short_copy(bool aligned, address *entry, const char *name) {\n+#if COMPILER2_OR_JVMCI\n+  if (VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2() && MaxVectorSize  >= 32) {\n+     return generate_disjoint_copy_avx3_masked(entry, \"jshort_disjoint_arraycopy_avx3\", 1,\n+                                               aligned, false, false);\n+  }\n+#endif\n+\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", name);\n+  address start = __ pc();\n+\n+  Label L_copy_bytes, L_copy_8_bytes, L_copy_4_bytes,L_copy_2_bytes,L_exit;\n+  const Register from        = rdi;  \/\/ source array address\n+  const Register to          = rsi;  \/\/ destination array address\n+  const Register count       = rdx;  \/\/ elements count\n+  const Register word_count  = rcx;\n+  const Register qword_count = count;\n+  const Register end_from    = from; \/\/ source array end address\n+  const Register end_to      = to;   \/\/ destination array end address\n+  \/\/ End pointers are inclusive, and if count is not zero they point\n+  \/\/ to the last unit copied:  end_to[0] := end_from[0]\n+\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  assert_clean_int(c_rarg2, rax);    \/\/ Make sure 'count' is clean int.\n+\n+  if (entry != NULL) {\n+    *entry = __ pc();\n+    \/\/ caller can pass a 64-bit byte count here (from Unsafe.copyMemory)\n+    BLOCK_COMMENT(\"Entry:\");\n+  }\n+\n+  setup_arg_regs(); \/\/ from => rdi, to => rsi, count => rdx\n+                    \/\/ r9 and r10 may be used to save non-volatile registers\n+\n+  {\n+    \/\/ UnsafeCopyMemory page error: continue after ucm\n+    UnsafeCopyMemoryMark ucmm(this, !aligned, true);\n+    \/\/ 'from', 'to' and 'count' are now valid\n+    __ movptr(word_count, count);\n+    __ shrptr(count, 2); \/\/ count => qword_count\n+\n+    \/\/ Copy from low to high addresses.  Use 'to' as scratch.\n+    __ lea(end_from, Address(from, qword_count, Address::times_8, -8));\n+    __ lea(end_to,   Address(to,   qword_count, Address::times_8, -8));\n+    __ negptr(qword_count);\n+    __ jmp(L_copy_bytes);\n+\n+    \/\/ Copy trailing qwords\n+  __ BIND(L_copy_8_bytes);\n+    __ movq(rax, Address(end_from, qword_count, Address::times_8, 8));\n+    __ movq(Address(end_to, qword_count, Address::times_8, 8), rax);\n+    __ increment(qword_count);\n+    __ jcc(Assembler::notZero, L_copy_8_bytes);\n+\n+    \/\/ Original 'dest' is trashed, so we can't use it as a\n+    \/\/ base register for a possible trailing word copy\n+\n+    \/\/ Check for and copy trailing dword\n+  __ BIND(L_copy_4_bytes);\n+    __ testl(word_count, 2);\n+    __ jccb(Assembler::zero, L_copy_2_bytes);\n+    __ movl(rax, Address(end_from, 8));\n+    __ movl(Address(end_to, 8), rax);\n+\n+    __ addptr(end_from, 4);\n+    __ addptr(end_to, 4);\n+\n+    \/\/ Check for and copy trailing word\n+  __ BIND(L_copy_2_bytes);\n+    __ testl(word_count, 1);\n+    __ jccb(Assembler::zero, L_exit);\n+    __ movw(rax, Address(end_from, 8));\n+    __ movw(Address(end_to, 8), rax);\n+  }\n+__ BIND(L_exit);\n+  address ucme_exit_pc = __ pc();\n+  restore_arg_regs();\n+  INC_COUNTER_NP(SharedRuntime::_jshort_array_copy_ctr, rscratch1); \/\/ Update counter after rscratch1 is free\n+  __ xorptr(rax, rax); \/\/ return 0\n+  __ vzeroupper();\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n+\n+  {\n+    UnsafeCopyMemoryMark ucmm(this, !aligned, false, ucme_exit_pc);\n+    \/\/ Copy in multi-bytes chunks\n+    copy_bytes_forward(end_from, end_to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n+    __ jmp(L_copy_4_bytes);\n+  }\n+\n+  return start;\n+}\n+\n+\n+address StubGenerator::generate_fill(BasicType t, bool aligned, const char *name) {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", name);\n+  address start = __ pc();\n+\n+  BLOCK_COMMENT(\"Entry:\");\n+\n+  const Register to       = c_rarg0;  \/\/ destination array address\n+  const Register value    = c_rarg1;  \/\/ value\n+  const Register count    = c_rarg2;  \/\/ elements count\n+  __ mov(r11, count);\n+\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+\n+  __ generate_fill(t, aligned, to, value, r11, rax, xmm0);\n+\n+  __ vzeroupper();\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n+\n+  return start;\n+}\n+\n+\n+\/\/ Arguments:\n+\/\/   aligned - true => Input and output aligned on a HeapWord == 8-byte boundary\n+\/\/             ignored\n+\/\/   name    - stub name string\n+\/\/\n+\/\/ Inputs:\n+\/\/   c_rarg0   - source array address\n+\/\/   c_rarg1   - destination array address\n+\/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n+\/\/\n+\/\/ If 'from' and\/or 'to' are aligned on 4- or 2-byte boundaries, we\n+\/\/ let the hardware handle it.  The two or four words within dwords\n+\/\/ or qwords that span cache line boundaries will still be loaded\n+\/\/ and stored atomically.\n+\/\/\n+address StubGenerator::generate_conjoint_short_copy(bool aligned, address nooverlap_target,\n+                                                    address *entry, const char *name) {\n+#if COMPILER2_OR_JVMCI\n+  if (VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2() && MaxVectorSize  >= 32) {\n+     return generate_conjoint_copy_avx3_masked(entry, \"jshort_conjoint_arraycopy_avx3\", 1,\n+                                               nooverlap_target, aligned, false, false);\n+  }\n+#endif\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", name);\n+  address start = __ pc();\n+\n+  Label L_copy_bytes, L_copy_8_bytes, L_copy_4_bytes;\n+  const Register from        = rdi;  \/\/ source array address\n+  const Register to          = rsi;  \/\/ destination array address\n+  const Register count       = rdx;  \/\/ elements count\n+  const Register word_count  = rcx;\n+  const Register qword_count = count;\n+\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  assert_clean_int(c_rarg2, rax);    \/\/ Make sure 'count' is clean int.\n+\n+  if (entry != NULL) {\n+    *entry = __ pc();\n+    \/\/ caller can pass a 64-bit byte count here (from Unsafe.copyMemory)\n+    BLOCK_COMMENT(\"Entry:\");\n+  }\n+\n+  array_overlap_test(nooverlap_target, Address::times_2);\n+  setup_arg_regs(); \/\/ from => rdi, to => rsi, count => rdx\n+                    \/\/ r9 and r10 may be used to save non-volatile registers\n+\n+  {\n+    \/\/ UnsafeCopyMemory page error: continue after ucm\n+    UnsafeCopyMemoryMark ucmm(this, !aligned, true);\n+    \/\/ 'from', 'to' and 'count' are now valid\n+    __ movptr(word_count, count);\n+    __ shrptr(count, 2); \/\/ count => qword_count\n+\n+    \/\/ Copy from high to low addresses.  Use 'to' as scratch.\n+\n+    \/\/ Check for and copy trailing word\n+    __ testl(word_count, 1);\n+    __ jccb(Assembler::zero, L_copy_4_bytes);\n+    __ movw(rax, Address(from, word_count, Address::times_2, -2));\n+    __ movw(Address(to, word_count, Address::times_2, -2), rax);\n+\n+   \/\/ Check for and copy trailing dword\n+  __ BIND(L_copy_4_bytes);\n+    __ testl(word_count, 2);\n+    __ jcc(Assembler::zero, L_copy_bytes);\n+    __ movl(rax, Address(from, qword_count, Address::times_8));\n+    __ movl(Address(to, qword_count, Address::times_8), rax);\n+    __ jmp(L_copy_bytes);\n+\n+    \/\/ Copy trailing qwords\n+  __ BIND(L_copy_8_bytes);\n+    __ movq(rax, Address(from, qword_count, Address::times_8, -8));\n+    __ movq(Address(to, qword_count, Address::times_8, -8), rax);\n+    __ decrement(qword_count);\n+    __ jcc(Assembler::notZero, L_copy_8_bytes);\n+  }\n+  restore_arg_regs();\n+  INC_COUNTER_NP(SharedRuntime::_jshort_array_copy_ctr, rscratch1); \/\/ Update counter after rscratch1 is free\n+  __ xorptr(rax, rax); \/\/ return 0\n+  __ vzeroupper();\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n+\n+  {\n+    \/\/ UnsafeCopyMemory page error: continue after ucm\n+    UnsafeCopyMemoryMark ucmm(this, !aligned, true);\n+    \/\/ Copy in multi-bytes chunks\n+    copy_bytes_backward(from, to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n+  }\n+  restore_arg_regs();\n+  INC_COUNTER_NP(SharedRuntime::_jshort_array_copy_ctr, rscratch1); \/\/ Update counter after rscratch1 is free\n+  __ xorptr(rax, rax); \/\/ return 0\n+  __ vzeroupper();\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n+\n+  return start;\n+}\n+\n+\n+\/\/ Arguments:\n+\/\/   aligned - true => Input and output aligned on a HeapWord == 8-byte boundary\n+\/\/             ignored\n+\/\/   is_oop  - true => oop array, so generate store check code\n+\/\/   name    - stub name string\n+\/\/\n+\/\/ Inputs:\n+\/\/   c_rarg0   - source array address\n+\/\/   c_rarg1   - destination array address\n+\/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n+\/\/\n+\/\/ If 'from' and\/or 'to' are aligned on 4-byte boundaries, we let\n+\/\/ the hardware handle it.  The two dwords within qwords that span\n+\/\/ cache line boundaries will still be loaded and stored atomically.\n+\/\/\n+\/\/ Side Effects:\n+\/\/   disjoint_int_copy_entry is set to the no-overlap entry point\n+\/\/   used by generate_conjoint_int_oop_copy().\n+\/\/\n+address StubGenerator::generate_disjoint_int_oop_copy(bool aligned, bool is_oop, address* entry,\n+                                                      const char *name, bool dest_uninitialized) {\n+#if COMPILER2_OR_JVMCI\n+  if (VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2() && MaxVectorSize  >= 32) {\n+     return generate_disjoint_copy_avx3_masked(entry, \"jint_disjoint_arraycopy_avx3\", 2,\n+                                               aligned, is_oop, dest_uninitialized);\n+  }\n+#endif\n+\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", name);\n+  address start = __ pc();\n+\n+  Label L_copy_bytes, L_copy_8_bytes, L_copy_4_bytes, L_exit;\n+  const Register from        = rdi;  \/\/ source array address\n+  const Register to          = rsi;  \/\/ destination array address\n+  const Register count       = rdx;  \/\/ elements count\n+  const Register dword_count = rcx;\n+  const Register qword_count = count;\n+  const Register end_from    = from; \/\/ source array end address\n+  const Register end_to      = to;   \/\/ destination array end address\n+  \/\/ End pointers are inclusive, and if count is not zero they point\n+  \/\/ to the last unit copied:  end_to[0] := end_from[0]\n+\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  assert_clean_int(c_rarg2, rax);    \/\/ Make sure 'count' is clean int.\n+\n+  if (entry != NULL) {\n+    *entry = __ pc();\n+    \/\/ caller can pass a 64-bit byte count here (from Unsafe.copyMemory)\n+    BLOCK_COMMENT(\"Entry:\");\n+  }\n+\n+  setup_arg_regs_using_thread(); \/\/ from => rdi, to => rsi, count => rdx\n+                                 \/\/ r9 is used to save r15_thread\n+\n+  DecoratorSet decorators = IN_HEAP | IS_ARRAY | ARRAYCOPY_DISJOINT;\n+  if (dest_uninitialized) {\n+    decorators |= IS_DEST_UNINITIALIZED;\n+  }\n+  if (aligned) {\n+    decorators |= ARRAYCOPY_ALIGNED;\n+  }\n+\n+  BasicType type = is_oop ? T_OBJECT : T_INT;\n+  BarrierSetAssembler *bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  bs->arraycopy_prologue(_masm, decorators, type, from, to, count);\n+\n+  {\n+    \/\/ UnsafeCopyMemory page error: continue after ucm\n+    UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n+    \/\/ 'from', 'to' and 'count' are now valid\n+    __ movptr(dword_count, count);\n+    __ shrptr(count, 1); \/\/ count => qword_count\n+\n+    \/\/ Copy from low to high addresses.  Use 'to' as scratch.\n+    __ lea(end_from, Address(from, qword_count, Address::times_8, -8));\n+    __ lea(end_to,   Address(to,   qword_count, Address::times_8, -8));\n+    __ negptr(qword_count);\n+    __ jmp(L_copy_bytes);\n+\n+    \/\/ Copy trailing qwords\n+  __ BIND(L_copy_8_bytes);\n+    __ movq(rax, Address(end_from, qword_count, Address::times_8, 8));\n+    __ movq(Address(end_to, qword_count, Address::times_8, 8), rax);\n+    __ increment(qword_count);\n+    __ jcc(Assembler::notZero, L_copy_8_bytes);\n+\n+    \/\/ Check for and copy trailing dword\n+  __ BIND(L_copy_4_bytes);\n+    __ testl(dword_count, 1); \/\/ Only byte test since the value is 0 or 1\n+    __ jccb(Assembler::zero, L_exit);\n+    __ movl(rax, Address(end_from, 8));\n+    __ movl(Address(end_to, 8), rax);\n+  }\n+__ BIND(L_exit);\n+  address ucme_exit_pc = __ pc();\n+  bs->arraycopy_epilogue(_masm, decorators, type, from, to, dword_count);\n+  restore_arg_regs_using_thread();\n+  INC_COUNTER_NP(SharedRuntime::_jint_array_copy_ctr, rscratch1); \/\/ Update counter after rscratch1 is free\n+  __ vzeroupper();\n+  __ xorptr(rax, rax); \/\/ return 0\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n+\n+  {\n+    UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, false, ucme_exit_pc);\n+    \/\/ Copy in multi-bytes chunks\n+    copy_bytes_forward(end_from, end_to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n+    __ jmp(L_copy_4_bytes);\n+  }\n+\n+  return start;\n+}\n+\n+\n+\/\/ Arguments:\n+\/\/   aligned - true => Input and output aligned on a HeapWord == 8-byte boundary\n+\/\/             ignored\n+\/\/   is_oop  - true => oop array, so generate store check code\n+\/\/   name    - stub name string\n+\/\/\n+\/\/ Inputs:\n+\/\/   c_rarg0   - source array address\n+\/\/   c_rarg1   - destination array address\n+\/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n+\/\/\n+\/\/ If 'from' and\/or 'to' are aligned on 4-byte boundaries, we let\n+\/\/ the hardware handle it.  The two dwords within qwords that span\n+\/\/ cache line boundaries will still be loaded and stored atomically.\n+\/\/\n+address StubGenerator::generate_conjoint_int_oop_copy(bool aligned, bool is_oop, address nooverlap_target,\n+                                                      address *entry, const char *name,\n+                                                      bool dest_uninitialized) {\n+#if COMPILER2_OR_JVMCI\n+  if (VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2() && MaxVectorSize  >= 32) {\n+     return generate_conjoint_copy_avx3_masked(entry, \"jint_conjoint_arraycopy_avx3\", 2,\n+                                               nooverlap_target, aligned, is_oop, dest_uninitialized);\n+  }\n+#endif\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", name);\n+  address start = __ pc();\n+\n+  Label L_copy_bytes, L_copy_8_bytes, L_exit;\n+  const Register from        = rdi;  \/\/ source array address\n+  const Register to          = rsi;  \/\/ destination array address\n+  const Register count       = rdx;  \/\/ elements count\n+  const Register dword_count = rcx;\n+  const Register qword_count = count;\n+\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  assert_clean_int(c_rarg2, rax);    \/\/ Make sure 'count' is clean int.\n+\n+  if (entry != NULL) {\n+    *entry = __ pc();\n+     \/\/ caller can pass a 64-bit byte count here (from Unsafe.copyMemory)\n+    BLOCK_COMMENT(\"Entry:\");\n+  }\n+\n+  array_overlap_test(nooverlap_target, Address::times_4);\n+  setup_arg_regs_using_thread(); \/\/ from => rdi, to => rsi, count => rdx\n+                                 \/\/ r9 is used to save r15_thread\n+\n+  DecoratorSet decorators = IN_HEAP | IS_ARRAY;\n+  if (dest_uninitialized) {\n+    decorators |= IS_DEST_UNINITIALIZED;\n+  }\n+  if (aligned) {\n+    decorators |= ARRAYCOPY_ALIGNED;\n+  }\n+\n+  BasicType type = is_oop ? T_OBJECT : T_INT;\n+  BarrierSetAssembler *bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  \/\/ no registers are destroyed by this call\n+  bs->arraycopy_prologue(_masm, decorators, type, from, to, count);\n+\n+  assert_clean_int(count, rax); \/\/ Make sure 'count' is clean int.\n+  {\n+    \/\/ UnsafeCopyMemory page error: continue after ucm\n+    UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n+    \/\/ 'from', 'to' and 'count' are now valid\n+    __ movptr(dword_count, count);\n+    __ shrptr(count, 1); \/\/ count => qword_count\n+\n+    \/\/ Copy from high to low addresses.  Use 'to' as scratch.\n+\n+    \/\/ Check for and copy trailing dword\n+    __ testl(dword_count, 1);\n+    __ jcc(Assembler::zero, L_copy_bytes);\n+    __ movl(rax, Address(from, dword_count, Address::times_4, -4));\n+    __ movl(Address(to, dword_count, Address::times_4, -4), rax);\n+    __ jmp(L_copy_bytes);\n+\n+    \/\/ Copy trailing qwords\n+  __ BIND(L_copy_8_bytes);\n+    __ movq(rax, Address(from, qword_count, Address::times_8, -8));\n+    __ movq(Address(to, qword_count, Address::times_8, -8), rax);\n+    __ decrement(qword_count);\n+    __ jcc(Assembler::notZero, L_copy_8_bytes);\n+  }\n+  if (is_oop) {\n+    __ jmp(L_exit);\n+  }\n+  restore_arg_regs_using_thread();\n+  INC_COUNTER_NP(SharedRuntime::_jint_array_copy_ctr, rscratch1); \/\/ Update counter after rscratch1 is free\n+  __ xorptr(rax, rax); \/\/ return 0\n+  __ vzeroupper();\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n+\n+  {\n+    \/\/ UnsafeCopyMemory page error: continue after ucm\n+    UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n+    \/\/ Copy in multi-bytes chunks\n+    copy_bytes_backward(from, to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n+  }\n+\n+__ BIND(L_exit);\n+  bs->arraycopy_epilogue(_masm, decorators, type, from, to, dword_count);\n+  restore_arg_regs_using_thread();\n+  INC_COUNTER_NP(SharedRuntime::_jint_array_copy_ctr, rscratch1); \/\/ Update counter after rscratch1 is free\n+  __ xorptr(rax, rax); \/\/ return 0\n+  __ vzeroupper();\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n+\n+  return start;\n+}\n+\n+\n+\/\/ Arguments:\n+\/\/   aligned - true => Input and output aligned on a HeapWord boundary == 8 bytes\n+\/\/             ignored\n+\/\/   is_oop  - true => oop array, so generate store check code\n+\/\/   name    - stub name string\n+\/\/\n+\/\/ Inputs:\n+\/\/   c_rarg0   - source array address\n+\/\/   c_rarg1   - destination array address\n+\/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n+\/\/\n+ \/\/ Side Effects:\n+\/\/   disjoint_oop_copy_entry or disjoint_long_copy_entry is set to the\n+\/\/   no-overlap entry point used by generate_conjoint_long_oop_copy().\n+\/\/\n+address StubGenerator::generate_disjoint_long_oop_copy(bool aligned, bool is_oop, address *entry,\n+                                                       const char *name, bool dest_uninitialized) {\n+#if COMPILER2_OR_JVMCI\n+  if (VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2() && MaxVectorSize  >= 32) {\n+     return generate_disjoint_copy_avx3_masked(entry, \"jlong_disjoint_arraycopy_avx3\", 3,\n+                                               aligned, is_oop, dest_uninitialized);\n+  }\n+#endif\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", name);\n+  address start = __ pc();\n+\n+  Label L_copy_bytes, L_copy_8_bytes, L_exit;\n+  const Register from        = rdi;  \/\/ source array address\n+  const Register to          = rsi;  \/\/ destination array address\n+  const Register qword_count = rdx;  \/\/ elements count\n+  const Register end_from    = from; \/\/ source array end address\n+  const Register end_to      = rcx;  \/\/ destination array end address\n+  const Register saved_count = r11;\n+  \/\/ End pointers are inclusive, and if count is not zero they point\n+  \/\/ to the last unit copied:  end_to[0] := end_from[0]\n+\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  \/\/ Save no-overlap entry point for generate_conjoint_long_oop_copy()\n+  assert_clean_int(c_rarg2, rax);    \/\/ Make sure 'count' is clean int.\n+\n+  if (entry != NULL) {\n+    *entry = __ pc();\n+    \/\/ caller can pass a 64-bit byte count here (from Unsafe.copyMemory)\n+    BLOCK_COMMENT(\"Entry:\");\n+  }\n+\n+  setup_arg_regs_using_thread(); \/\/ from => rdi, to => rsi, count => rdx\n+                                   \/\/ r9 is used to save r15_thread\n+  \/\/ 'from', 'to' and 'qword_count' are now valid\n+\n+  DecoratorSet decorators = IN_HEAP | IS_ARRAY | ARRAYCOPY_DISJOINT;\n+  if (dest_uninitialized) {\n+    decorators |= IS_DEST_UNINITIALIZED;\n+  }\n+  if (aligned) {\n+    decorators |= ARRAYCOPY_ALIGNED;\n+  }\n+\n+  BasicType type = is_oop ? T_OBJECT : T_LONG;\n+  BarrierSetAssembler *bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  bs->arraycopy_prologue(_masm, decorators, type, from, to, qword_count);\n+  {\n+    \/\/ UnsafeCopyMemory page error: continue after ucm\n+    UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n+\n+    \/\/ Copy from low to high addresses.  Use 'to' as scratch.\n+    __ lea(end_from, Address(from, qword_count, Address::times_8, -8));\n+    __ lea(end_to,   Address(to,   qword_count, Address::times_8, -8));\n+    __ negptr(qword_count);\n+    __ jmp(L_copy_bytes);\n+\n+    \/\/ Copy trailing qwords\n+  __ BIND(L_copy_8_bytes);\n+    __ movq(rax, Address(end_from, qword_count, Address::times_8, 8));\n+    __ movq(Address(end_to, qword_count, Address::times_8, 8), rax);\n+    __ increment(qword_count);\n+    __ jcc(Assembler::notZero, L_copy_8_bytes);\n+  }\n+  if (is_oop) {\n+    __ jmp(L_exit);\n+  } else {\n+    restore_arg_regs_using_thread();\n+    INC_COUNTER_NP(SharedRuntime::_jlong_array_copy_ctr, rscratch1); \/\/ Update counter after rscratch1 is free\n+    __ xorptr(rax, rax); \/\/ return 0\n+    __ vzeroupper();\n+    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+    __ ret(0);\n+  }\n+\n+  {\n+    \/\/ UnsafeCopyMemory page error: continue after ucm\n+    UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n+    \/\/ Copy in multi-bytes chunks\n+    copy_bytes_forward(end_from, end_to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n+  }\n+\n+  __ BIND(L_exit);\n+  bs->arraycopy_epilogue(_masm, decorators, type, from, to, qword_count);\n+  restore_arg_regs_using_thread();\n+  INC_COUNTER_NP(is_oop ? SharedRuntime::_oop_array_copy_ctr :\n+                          SharedRuntime::_jlong_array_copy_ctr,\n+                 rscratch1); \/\/ Update counter after rscratch1 is free\n+  __ vzeroupper();\n+  __ xorptr(rax, rax); \/\/ return 0\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n+\n+  return start;\n+}\n+\n+\n+\/\/ Arguments:\n+\/\/   aligned - true => Input and output aligned on a HeapWord boundary == 8 bytes\n+\/\/             ignored\n+\/\/   is_oop  - true => oop array, so generate store check code\n+\/\/   name    - stub name string\n+\/\/\n+\/\/ Inputs:\n+\/\/   c_rarg0   - source array address\n+\/\/   c_rarg1   - destination array address\n+\/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n+\/\/\n+address StubGenerator::generate_conjoint_long_oop_copy(bool aligned, bool is_oop, address nooverlap_target,\n+                                                       address *entry, const char *name,\n+                                                       bool dest_uninitialized) {\n+#if COMPILER2_OR_JVMCI\n+  if (VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2() && MaxVectorSize  >= 32) {\n+     return generate_conjoint_copy_avx3_masked(entry, \"jlong_conjoint_arraycopy_avx3\", 3,\n+                                               nooverlap_target, aligned, is_oop, dest_uninitialized);\n+  }\n+#endif\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", name);\n+  address start = __ pc();\n+\n+  Label L_copy_bytes, L_copy_8_bytes, L_exit;\n+  const Register from        = rdi;  \/\/ source array address\n+  const Register to          = rsi;  \/\/ destination array address\n+  const Register qword_count = rdx;  \/\/ elements count\n+  const Register saved_count = rcx;\n+\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  assert_clean_int(c_rarg2, rax);    \/\/ Make sure 'count' is clean int.\n+\n+  if (entry != NULL) {\n+    *entry = __ pc();\n+    \/\/ caller can pass a 64-bit byte count here (from Unsafe.copyMemory)\n+    BLOCK_COMMENT(\"Entry:\");\n+  }\n+\n+  array_overlap_test(nooverlap_target, Address::times_8);\n+  setup_arg_regs_using_thread(); \/\/ from => rdi, to => rsi, count => rdx\n+                                 \/\/ r9 is used to save r15_thread\n+  \/\/ 'from', 'to' and 'qword_count' are now valid\n+\n+  DecoratorSet decorators = IN_HEAP | IS_ARRAY;\n+  if (dest_uninitialized) {\n+    decorators |= IS_DEST_UNINITIALIZED;\n+  }\n+  if (aligned) {\n+    decorators |= ARRAYCOPY_ALIGNED;\n+  }\n+\n+  BasicType type = is_oop ? T_OBJECT : T_LONG;\n+  BarrierSetAssembler *bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  bs->arraycopy_prologue(_masm, decorators, type, from, to, qword_count);\n+  {\n+    \/\/ UnsafeCopyMemory page error: continue after ucm\n+    UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n+\n+    __ jmp(L_copy_bytes);\n+\n+    \/\/ Copy trailing qwords\n+  __ BIND(L_copy_8_bytes);\n+    __ movq(rax, Address(from, qword_count, Address::times_8, -8));\n+    __ movq(Address(to, qword_count, Address::times_8, -8), rax);\n+    __ decrement(qword_count);\n+    __ jcc(Assembler::notZero, L_copy_8_bytes);\n+  }\n+  if (is_oop) {\n+    __ jmp(L_exit);\n+  } else {\n+    restore_arg_regs_using_thread();\n+    INC_COUNTER_NP(SharedRuntime::_jlong_array_copy_ctr, rscratch1); \/\/ Update counter after rscratch1 is free\n+    __ xorptr(rax, rax); \/\/ return 0\n+    __ vzeroupper();\n+    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+    __ ret(0);\n+  }\n+  {\n+    \/\/ UnsafeCopyMemory page error: continue after ucm\n+    UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n+\n+    \/\/ Copy in multi-bytes chunks\n+    copy_bytes_backward(from, to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n+  }\n+  __ BIND(L_exit);\n+  bs->arraycopy_epilogue(_masm, decorators, type, from, to, qword_count);\n+  restore_arg_regs_using_thread();\n+  INC_COUNTER_NP(is_oop ? SharedRuntime::_oop_array_copy_ctr :\n+                          SharedRuntime::_jlong_array_copy_ctr,\n+                 rscratch1); \/\/ Update counter after rscratch1 is free\n+  __ vzeroupper();\n+  __ xorptr(rax, rax); \/\/ return 0\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n+\n+  return start;\n+}\n+\n+\n+\/\/ Helper for generating a dynamic type check.\n+\/\/ Smashes no registers.\n+void StubGenerator::generate_type_check(Register sub_klass,\n+                                        Register super_check_offset,\n+                                        Register super_klass,\n+                                        Label& L_success) {\n+  assert_different_registers(sub_klass, super_check_offset, super_klass);\n+\n+  BLOCK_COMMENT(\"type_check:\");\n+\n+  Label L_miss;\n+\n+  __ check_klass_subtype_fast_path(sub_klass, super_klass, noreg,        &L_success, &L_miss, NULL,\n+                                   super_check_offset);\n+  __ check_klass_subtype_slow_path(sub_klass, super_klass, noreg, noreg, &L_success, NULL);\n+\n+  \/\/ Fall through on failure!\n+  __ BIND(L_miss);\n+}\n+\n+\/\/\n+\/\/  Generate checkcasting array copy stub\n+\/\/\n+\/\/  Input:\n+\/\/    c_rarg0   - source array address\n+\/\/    c_rarg1   - destination array address\n+\/\/    c_rarg2   - element count, treated as ssize_t, can be zero\n+\/\/    c_rarg3   - size_t ckoff (super_check_offset)\n+\/\/ not Win64\n+\/\/    c_rarg4   - oop ckval (super_klass)\n+\/\/ Win64\n+\/\/    rsp+40    - oop ckval (super_klass)\n+\/\/\n+\/\/  Output:\n+\/\/    rax ==  0  -  success\n+\/\/    rax == -1^K - failure, where K is partial transfer count\n+\/\/\n+address StubGenerator::generate_checkcast_copy(const char *name, address *entry, bool dest_uninitialized) {\n+\n+  Label L_load_element, L_store_element, L_do_card_marks, L_done;\n+\n+  \/\/ Input registers (after setup_arg_regs)\n+  const Register from        = rdi;   \/\/ source array address\n+  const Register to          = rsi;   \/\/ destination array address\n+  const Register length      = rdx;   \/\/ elements count\n+  const Register ckoff       = rcx;   \/\/ super_check_offset\n+  const Register ckval       = r8;    \/\/ super_klass\n+\n+  \/\/ Registers used as temps (r13, r14 are save-on-entry)\n+  const Register end_from    = from;  \/\/ source array end address\n+  const Register end_to      = r13;   \/\/ destination array end address\n+  const Register count       = rdx;   \/\/ -(count_remaining)\n+  const Register r14_length  = r14;   \/\/ saved copy of length\n+  \/\/ End pointers are inclusive, and if length is not zero they point\n+  \/\/ to the last unit copied:  end_to[0] := end_from[0]\n+\n+  const Register rax_oop    = rax;    \/\/ actual oop copied\n+  const Register r11_klass  = r11;    \/\/ oop._klass\n+\n+  \/\/---------------------------------------------------------------\n+  \/\/ Assembler stub will be used for this call to arraycopy\n+  \/\/ if the two arrays are subtypes of Object[] but the\n+  \/\/ destination array type is not equal to or a supertype\n+  \/\/ of the source type.  Each element must be separately\n+  \/\/ checked.\n+\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", name);\n+  address start = __ pc();\n+\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+\n+#ifdef ASSERT\n+  \/\/ caller guarantees that the arrays really are different\n+  \/\/ otherwise, we would have to make conjoint checks\n+  { Label L;\n+    array_overlap_test(L, TIMES_OOP);\n+    __ stop(\"checkcast_copy within a single array\");\n+    __ bind(L);\n+  }\n+#endif \/\/ASSERT\n+\n+  setup_arg_regs(4); \/\/ from => rdi, to => rsi, length => rdx\n+                     \/\/ ckoff => rcx, ckval => r8\n+                     \/\/ r9 and r10 may be used to save non-volatile registers\n+#ifdef _WIN64\n+  \/\/ last argument (#4) is on stack on Win64\n+  __ movptr(ckval, Address(rsp, 6 * wordSize));\n+#endif\n+\n+  \/\/ Caller of this entry point must set up the argument registers.\n+  if (entry != NULL) {\n+    *entry = __ pc();\n+    BLOCK_COMMENT(\"Entry:\");\n+  }\n+\n+  \/\/ allocate spill slots for r13, r14\n+  enum {\n+    saved_r13_offset,\n+    saved_r14_offset,\n+    saved_r10_offset,\n+    saved_rbp_offset\n+  };\n+  __ subptr(rsp, saved_rbp_offset * wordSize);\n+  __ movptr(Address(rsp, saved_r13_offset * wordSize), r13);\n+  __ movptr(Address(rsp, saved_r14_offset * wordSize), r14);\n+  __ movptr(Address(rsp, saved_r10_offset * wordSize), r10);\n+\n+#ifdef ASSERT\n+    Label L2;\n+    __ get_thread(r14);\n+    __ cmpptr(r15_thread, r14);\n+    __ jcc(Assembler::equal, L2);\n+    __ stop(\"StubRoutines::call_stub: r15_thread is modified by call\");\n+    __ bind(L2);\n+#endif \/\/ ASSERT\n+\n+  \/\/ check that int operands are properly extended to size_t\n+  assert_clean_int(length, rax);\n+  assert_clean_int(ckoff, rax);\n+\n+#ifdef ASSERT\n+  BLOCK_COMMENT(\"assert consistent ckoff\/ckval\");\n+  \/\/ The ckoff and ckval must be mutually consistent,\n+  \/\/ even though caller generates both.\n+  { Label L;\n+    int sco_offset = in_bytes(Klass::super_check_offset_offset());\n+    __ cmpl(ckoff, Address(ckval, sco_offset));\n+    __ jcc(Assembler::equal, L);\n+    __ stop(\"super_check_offset inconsistent\");\n+    __ bind(L);\n+  }\n+#endif \/\/ASSERT\n+\n+  \/\/ Loop-invariant addresses.  They are exclusive end pointers.\n+  Address end_from_addr(from, length, TIMES_OOP, 0);\n+  Address   end_to_addr(to,   length, TIMES_OOP, 0);\n+  \/\/ Loop-variant addresses.  They assume post-incremented count < 0.\n+  Address from_element_addr(end_from, count, TIMES_OOP, 0);\n+  Address   to_element_addr(end_to,   count, TIMES_OOP, 0);\n+\n+  DecoratorSet decorators = IN_HEAP | IS_ARRAY | ARRAYCOPY_CHECKCAST | ARRAYCOPY_DISJOINT;\n+  if (dest_uninitialized) {\n+    decorators |= IS_DEST_UNINITIALIZED;\n+  }\n+\n+  BasicType type = T_OBJECT;\n+  BarrierSetAssembler *bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  bs->arraycopy_prologue(_masm, decorators, type, from, to, count);\n+\n+  \/\/ Copy from low to high addresses, indexed from the end of each array.\n+  __ lea(end_from, end_from_addr);\n+  __ lea(end_to,   end_to_addr);\n+  __ movptr(r14_length, length);        \/\/ save a copy of the length\n+  assert(length == count, \"\");          \/\/ else fix next line:\n+  __ negptr(count);                     \/\/ negate and test the length\n+  __ jcc(Assembler::notZero, L_load_element);\n+\n+  \/\/ Empty array:  Nothing to do.\n+  __ xorptr(rax, rax);                  \/\/ return 0 on (trivial) success\n+  __ jmp(L_done);\n+\n+  \/\/ ======== begin loop ========\n+  \/\/ (Loop is rotated; its entry is L_load_element.)\n+  \/\/ Loop control:\n+  \/\/   for (count = -count; count != 0; count++)\n+  \/\/ Base pointers src, dst are biased by 8*(count-1),to last element.\n+  __ align(OptoLoopAlignment);\n+\n+  __ BIND(L_store_element);\n+  __ store_heap_oop(to_element_addr, rax_oop, noreg, noreg, noreg, AS_RAW);  \/\/ store the oop\n+  __ increment(count);               \/\/ increment the count toward zero\n+  __ jcc(Assembler::zero, L_do_card_marks);\n+\n+  \/\/ ======== loop entry is here ========\n+  __ BIND(L_load_element);\n+  __ load_heap_oop(rax_oop, from_element_addr, noreg, noreg, AS_RAW); \/\/ load the oop\n+  __ testptr(rax_oop, rax_oop);\n+  __ jcc(Assembler::zero, L_store_element);\n+\n+  __ load_klass(r11_klass, rax_oop, rscratch1);\/\/ query the object klass\n+  generate_type_check(r11_klass, ckoff, ckval, L_store_element);\n+  \/\/ ======== end loop ========\n+\n+  \/\/ It was a real error; we must depend on the caller to finish the job.\n+  \/\/ Register rdx = -1 * number of *remaining* oops, r14 = *total* oops.\n+  \/\/ Emit GC store barriers for the oops we have copied (r14 + rdx),\n+  \/\/ and report their number to the caller.\n+  assert_different_registers(rax, r14_length, count, to, end_to, rcx, rscratch1);\n+  Label L_post_barrier;\n+  __ addptr(r14_length, count);     \/\/ K = (original - remaining) oops\n+  __ movptr(rax, r14_length);       \/\/ save the value\n+  __ notptr(rax);                   \/\/ report (-1^K) to caller (does not affect flags)\n+  __ jccb(Assembler::notZero, L_post_barrier);\n+  __ jmp(L_done); \/\/ K == 0, nothing was copied, skip post barrier\n+\n+  \/\/ Come here on success only.\n+  __ BIND(L_do_card_marks);\n+  __ xorptr(rax, rax);              \/\/ return 0 on success\n+\n+  __ BIND(L_post_barrier);\n+  bs->arraycopy_epilogue(_masm, decorators, type, from, to, r14_length);\n+\n+  \/\/ Common exit point (success or failure).\n+  __ BIND(L_done);\n+  __ movptr(r13, Address(rsp, saved_r13_offset * wordSize));\n+  __ movptr(r14, Address(rsp, saved_r14_offset * wordSize));\n+  __ movptr(r10, Address(rsp, saved_r10_offset * wordSize));\n+  restore_arg_regs();\n+  INC_COUNTER_NP(SharedRuntime::_checkcast_array_copy_ctr, rscratch1); \/\/ Update counter after rscratch1 is free\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n+\n+  return start;\n+}\n+\n+\n+\/\/  Generate 'unsafe' array copy stub\n+\/\/  Though just as safe as the other stubs, it takes an unscaled\n+\/\/  size_t argument instead of an element count.\n+\/\/\n+\/\/  Input:\n+\/\/    c_rarg0   - source array address\n+\/\/    c_rarg1   - destination array address\n+\/\/    c_rarg2   - byte count, treated as ssize_t, can be zero\n+\/\/\n+\/\/ Examines the alignment of the operands and dispatches\n+\/\/ to a long, int, short, or byte copy loop.\n+\/\/\n+address StubGenerator::generate_unsafe_copy(const char *name,\n+                                            address byte_copy_entry, address short_copy_entry,\n+                                            address int_copy_entry, address long_copy_entry) {\n+\n+  Label L_long_aligned, L_int_aligned, L_short_aligned;\n+\n+  \/\/ Input registers (before setup_arg_regs)\n+  const Register from        = c_rarg0;  \/\/ source array address\n+  const Register to          = c_rarg1;  \/\/ destination array address\n+  const Register size        = c_rarg2;  \/\/ byte count (size_t)\n+\n+  \/\/ Register used as a temp\n+  const Register bits        = rax;      \/\/ test copy of low bits\n+\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", name);\n+  address start = __ pc();\n+\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+\n+  \/\/ bump this on entry, not on exit:\n+  INC_COUNTER_NP(SharedRuntime::_unsafe_array_copy_ctr, rscratch1);\n+\n+  __ mov(bits, from);\n+  __ orptr(bits, to);\n+  __ orptr(bits, size);\n+\n+  __ testb(bits, BytesPerLong-1);\n+  __ jccb(Assembler::zero, L_long_aligned);\n+\n+  __ testb(bits, BytesPerInt-1);\n+  __ jccb(Assembler::zero, L_int_aligned);\n+\n+  __ testb(bits, BytesPerShort-1);\n+  __ jump_cc(Assembler::notZero, RuntimeAddress(byte_copy_entry));\n+\n+  __ BIND(L_short_aligned);\n+  __ shrptr(size, LogBytesPerShort); \/\/ size => short_count\n+  __ jump(RuntimeAddress(short_copy_entry));\n+\n+  __ BIND(L_int_aligned);\n+  __ shrptr(size, LogBytesPerInt); \/\/ size => int_count\n+  __ jump(RuntimeAddress(int_copy_entry));\n+\n+  __ BIND(L_long_aligned);\n+  __ shrptr(size, LogBytesPerLong); \/\/ size => qword_count\n+  __ jump(RuntimeAddress(long_copy_entry));\n+\n+  return start;\n+}\n+\n+\n+\/\/ Perform range checks on the proposed arraycopy.\n+\/\/ Kills temp, but nothing else.\n+\/\/ Also, clean the sign bits of src_pos and dst_pos.\n+void StubGenerator::arraycopy_range_checks(Register src,     \/\/ source array oop (c_rarg0)\n+                                           Register src_pos, \/\/ source position (c_rarg1)\n+                                           Register dst,     \/\/ destination array oo (c_rarg2)\n+                                           Register dst_pos, \/\/ destination position (c_rarg3)\n+                                           Register length,\n+                                           Register temp,\n+                                           Label& L_failed) {\n+  BLOCK_COMMENT(\"arraycopy_range_checks:\");\n+\n+  \/\/  if (src_pos + length > arrayOop(src)->length())  FAIL;\n+  __ movl(temp, length);\n+  __ addl(temp, src_pos);             \/\/ src_pos + length\n+  __ cmpl(temp, Address(src, arrayOopDesc::length_offset_in_bytes()));\n+  __ jcc(Assembler::above, L_failed);\n+\n+  \/\/  if (dst_pos + length > arrayOop(dst)->length())  FAIL;\n+  __ movl(temp, length);\n+  __ addl(temp, dst_pos);             \/\/ dst_pos + length\n+  __ cmpl(temp, Address(dst, arrayOopDesc::length_offset_in_bytes()));\n+  __ jcc(Assembler::above, L_failed);\n+\n+  \/\/ Have to clean up high 32-bits of 'src_pos' and 'dst_pos'.\n+  \/\/ Move with sign extension can be used since they are positive.\n+  __ movslq(src_pos, src_pos);\n+  __ movslq(dst_pos, dst_pos);\n+\n+  BLOCK_COMMENT(\"arraycopy_range_checks done\");\n+}\n+\n+\n+\/\/  Generate generic array copy stubs\n+\/\/\n+\/\/  Input:\n+\/\/    c_rarg0    -  src oop\n+\/\/    c_rarg1    -  src_pos (32-bits)\n+\/\/    c_rarg2    -  dst oop\n+\/\/    c_rarg3    -  dst_pos (32-bits)\n+\/\/ not Win64\n+\/\/    c_rarg4    -  element count (32-bits)\n+\/\/ Win64\n+\/\/    rsp+40     -  element count (32-bits)\n+\/\/\n+\/\/  Output:\n+\/\/    rax ==  0  -  success\n+\/\/    rax == -1^K - failure, where K is partial transfer count\n+\/\/\n+address StubGenerator::generate_generic_copy(const char *name,\n+                                             address byte_copy_entry, address short_copy_entry,\n+                                             address int_copy_entry, address oop_copy_entry,\n+                                             address long_copy_entry, address checkcast_copy_entry) {\n+\n+  Label L_failed, L_failed_0, L_objArray;\n+  Label L_copy_shorts, L_copy_ints, L_copy_longs;\n+\n+  \/\/ Input registers\n+  const Register src        = c_rarg0;  \/\/ source array oop\n+  const Register src_pos    = c_rarg1;  \/\/ source position\n+  const Register dst        = c_rarg2;  \/\/ destination array oop\n+  const Register dst_pos    = c_rarg3;  \/\/ destination position\n+#ifndef _WIN64\n+  const Register length     = c_rarg4;\n+  const Register rklass_tmp = r9;  \/\/ load_klass\n+#else\n+  const Address  length(rsp, 7 * wordSize);  \/\/ elements count is on stack on Win64\n+  const Register rklass_tmp = rdi;  \/\/ load_klass\n+#endif\n+\n+  { int modulus = CodeEntryAlignment;\n+    int target  = modulus - 5; \/\/ 5 = sizeof jmp(L_failed)\n+    int advance = target - (__ offset() % modulus);\n+    if (advance < 0)  advance += modulus;\n+    if (advance > 0)  __ nop(advance);\n+  }\n+  StubCodeMark mark(this, \"StubRoutines\", name);\n+\n+  \/\/ Short-hop target to L_failed.  Makes for denser prologue code.\n+  __ BIND(L_failed_0);\n+  __ jmp(L_failed);\n+  assert(__ offset() % CodeEntryAlignment == 0, \"no further alignment needed\");\n+\n+  __ align(CodeEntryAlignment);\n+  address start = __ pc();\n+\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+\n+#ifdef _WIN64\n+  __ push(rklass_tmp); \/\/ rdi is callee-save on Windows\n+#endif\n+\n+  \/\/ bump this on entry, not on exit:\n+  INC_COUNTER_NP(SharedRuntime::_generic_array_copy_ctr, rscratch1);\n+\n+  \/\/-----------------------------------------------------------------------\n+  \/\/ Assembler stub will be used for this call to arraycopy\n+  \/\/ if the following conditions are met:\n+  \/\/\n+  \/\/ (1) src and dst must not be null.\n+  \/\/ (2) src_pos must not be negative.\n+  \/\/ (3) dst_pos must not be negative.\n+  \/\/ (4) length  must not be negative.\n+  \/\/ (5) src klass and dst klass should be the same and not NULL.\n+  \/\/ (6) src and dst should be arrays.\n+  \/\/ (7) src_pos + length must not exceed length of src.\n+  \/\/ (8) dst_pos + length must not exceed length of dst.\n+  \/\/\n+\n+  \/\/  if (src == NULL) return -1;\n+  __ testptr(src, src);         \/\/ src oop\n+  size_t j1off = __ offset();\n+  __ jccb(Assembler::zero, L_failed_0);\n+\n+  \/\/  if (src_pos < 0) return -1;\n+  __ testl(src_pos, src_pos); \/\/ src_pos (32-bits)\n+  __ jccb(Assembler::negative, L_failed_0);\n+\n+  \/\/  if (dst == NULL) return -1;\n+  __ testptr(dst, dst);         \/\/ dst oop\n+  __ jccb(Assembler::zero, L_failed_0);\n+\n+  \/\/  if (dst_pos < 0) return -1;\n+  __ testl(dst_pos, dst_pos); \/\/ dst_pos (32-bits)\n+  size_t j4off = __ offset();\n+  __ jccb(Assembler::negative, L_failed_0);\n+\n+  \/\/ The first four tests are very dense code,\n+  \/\/ but not quite dense enough to put four\n+  \/\/ jumps in a 16-byte instruction fetch buffer.\n+  \/\/ That's good, because some branch predicters\n+  \/\/ do not like jumps so close together.\n+  \/\/ Make sure of this.\n+  guarantee(((j1off ^ j4off) & ~15) != 0, \"I$ line of 1st & 4th jumps\");\n+\n+  \/\/ registers used as temp\n+  const Register r11_length    = r11; \/\/ elements count to copy\n+  const Register r10_src_klass = r10; \/\/ array klass\n+\n+  \/\/  if (length < 0) return -1;\n+  __ movl(r11_length, length);        \/\/ length (elements count, 32-bits value)\n+  __ testl(r11_length, r11_length);\n+  __ jccb(Assembler::negative, L_failed_0);\n+\n+  __ load_klass(r10_src_klass, src, rklass_tmp);\n+#ifdef ASSERT\n+  \/\/  assert(src->klass() != NULL);\n+  {\n+    BLOCK_COMMENT(\"assert klasses not null {\");\n+    Label L1, L2;\n+    __ testptr(r10_src_klass, r10_src_klass);\n+    __ jcc(Assembler::notZero, L2);   \/\/ it is broken if klass is NULL\n+    __ bind(L1);\n+    __ stop(\"broken null klass\");\n+    __ bind(L2);\n+    __ load_klass(rax, dst, rklass_tmp);\n+    __ cmpq(rax, 0);\n+    __ jcc(Assembler::equal, L1);     \/\/ this would be broken also\n+    BLOCK_COMMENT(\"} assert klasses not null done\");\n+  }\n+#endif\n+\n+  \/\/ Load layout helper (32-bits)\n+  \/\/\n+  \/\/  |array_tag|     | header_size | element_type |     |log2_element_size|\n+  \/\/ 32        30    24            16              8     2                 0\n+  \/\/\n+  \/\/   array_tag: typeArray = 0x3, objArray = 0x2, non-array = 0x0\n+  \/\/\n+\n+  const int lh_offset = in_bytes(Klass::layout_helper_offset());\n+\n+  \/\/ Handle objArrays completely differently...\n+  const jint objArray_lh = Klass::array_layout_helper(T_OBJECT);\n+  __ cmpl(Address(r10_src_klass, lh_offset), objArray_lh);\n+  __ jcc(Assembler::equal, L_objArray);\n+\n+  \/\/  if (src->klass() != dst->klass()) return -1;\n+  __ load_klass(rax, dst, rklass_tmp);\n+  __ cmpq(r10_src_klass, rax);\n+  __ jcc(Assembler::notEqual, L_failed);\n+\n+  const Register rax_lh = rax;  \/\/ layout helper\n+  __ movl(rax_lh, Address(r10_src_klass, lh_offset));\n+\n+  \/\/  if (!src->is_Array()) return -1;\n+  __ cmpl(rax_lh, Klass::_lh_neutral_value);\n+  __ jcc(Assembler::greaterEqual, L_failed);\n+\n+  \/\/ At this point, it is known to be a typeArray (array_tag 0x3).\n+#ifdef ASSERT\n+  {\n+    BLOCK_COMMENT(\"assert primitive array {\");\n+    Label L;\n+    __ cmpl(rax_lh, (Klass::_lh_array_tag_type_value << Klass::_lh_array_tag_shift));\n+    __ jcc(Assembler::greaterEqual, L);\n+    __ stop(\"must be a primitive array\");\n+    __ bind(L);\n+    BLOCK_COMMENT(\"} assert primitive array done\");\n+  }\n+#endif\n+\n+  arraycopy_range_checks(src, src_pos, dst, dst_pos, r11_length,\n+                         r10, L_failed);\n+\n+  \/\/ TypeArrayKlass\n+  \/\/\n+  \/\/ src_addr = (src + array_header_in_bytes()) + (src_pos << log2elemsize);\n+  \/\/ dst_addr = (dst + array_header_in_bytes()) + (dst_pos << log2elemsize);\n+  \/\/\n+\n+  const Register r10_offset = r10;    \/\/ array offset\n+  const Register rax_elsize = rax_lh; \/\/ element size\n+\n+  __ movl(r10_offset, rax_lh);\n+  __ shrl(r10_offset, Klass::_lh_header_size_shift);\n+  __ andptr(r10_offset, Klass::_lh_header_size_mask);   \/\/ array_offset\n+  __ addptr(src, r10_offset);           \/\/ src array offset\n+  __ addptr(dst, r10_offset);           \/\/ dst array offset\n+  BLOCK_COMMENT(\"choose copy loop based on element size\");\n+  __ andl(rax_lh, Klass::_lh_log2_element_size_mask); \/\/ rax_lh -> rax_elsize\n+\n+#ifdef _WIN64\n+  __ pop(rklass_tmp); \/\/ Restore callee-save rdi\n+#endif\n+\n+  \/\/ next registers should be set before the jump to corresponding stub\n+  const Register from     = c_rarg0;  \/\/ source array address\n+  const Register to       = c_rarg1;  \/\/ destination array address\n+  const Register count    = c_rarg2;  \/\/ elements count\n+\n+  \/\/ 'from', 'to', 'count' registers should be set in such order\n+  \/\/ since they are the same as 'src', 'src_pos', 'dst'.\n+\n+  __ cmpl(rax_elsize, 0);\n+  __ jccb(Assembler::notEqual, L_copy_shorts);\n+  __ lea(from, Address(src, src_pos, Address::times_1, 0));\/\/ src_addr\n+  __ lea(to,   Address(dst, dst_pos, Address::times_1, 0));\/\/ dst_addr\n+  __ movl2ptr(count, r11_length); \/\/ length\n+  __ jump(RuntimeAddress(byte_copy_entry));\n+\n+__ BIND(L_copy_shorts);\n+  __ cmpl(rax_elsize, LogBytesPerShort);\n+  __ jccb(Assembler::notEqual, L_copy_ints);\n+  __ lea(from, Address(src, src_pos, Address::times_2, 0));\/\/ src_addr\n+  __ lea(to,   Address(dst, dst_pos, Address::times_2, 0));\/\/ dst_addr\n+  __ movl2ptr(count, r11_length); \/\/ length\n+  __ jump(RuntimeAddress(short_copy_entry));\n+\n+__ BIND(L_copy_ints);\n+  __ cmpl(rax_elsize, LogBytesPerInt);\n+  __ jccb(Assembler::notEqual, L_copy_longs);\n+  __ lea(from, Address(src, src_pos, Address::times_4, 0));\/\/ src_addr\n+  __ lea(to,   Address(dst, dst_pos, Address::times_4, 0));\/\/ dst_addr\n+  __ movl2ptr(count, r11_length); \/\/ length\n+  __ jump(RuntimeAddress(int_copy_entry));\n+\n+__ BIND(L_copy_longs);\n+#ifdef ASSERT\n+  {\n+    BLOCK_COMMENT(\"assert long copy {\");\n+    Label L;\n+    __ cmpl(rax_elsize, LogBytesPerLong);\n+    __ jcc(Assembler::equal, L);\n+    __ stop(\"must be long copy, but elsize is wrong\");\n+    __ bind(L);\n+    BLOCK_COMMENT(\"} assert long copy done\");\n+  }\n+#endif\n+  __ lea(from, Address(src, src_pos, Address::times_8, 0));\/\/ src_addr\n+  __ lea(to,   Address(dst, dst_pos, Address::times_8, 0));\/\/ dst_addr\n+  __ movl2ptr(count, r11_length); \/\/ length\n+  __ jump(RuntimeAddress(long_copy_entry));\n+\n+  \/\/ ObjArrayKlass\n+__ BIND(L_objArray);\n+  \/\/ live at this point:  r10_src_klass, r11_length, src[_pos], dst[_pos]\n+\n+  Label L_plain_copy, L_checkcast_copy;\n+  \/\/  test array classes for subtyping\n+  __ load_klass(rax, dst, rklass_tmp);\n+  __ cmpq(r10_src_klass, rax); \/\/ usual case is exact equality\n+  __ jcc(Assembler::notEqual, L_checkcast_copy);\n+\n+  \/\/ Identically typed arrays can be copied without element-wise checks.\n+  arraycopy_range_checks(src, src_pos, dst, dst_pos, r11_length,\n+                         r10, L_failed);\n+\n+  __ lea(from, Address(src, src_pos, TIMES_OOP,\n+               arrayOopDesc::base_offset_in_bytes(T_OBJECT))); \/\/ src_addr\n+  __ lea(to,   Address(dst, dst_pos, TIMES_OOP,\n+               arrayOopDesc::base_offset_in_bytes(T_OBJECT))); \/\/ dst_addr\n+  __ movl2ptr(count, r11_length); \/\/ length\n+__ BIND(L_plain_copy);\n+#ifdef _WIN64\n+  __ pop(rklass_tmp); \/\/ Restore callee-save rdi\n+#endif\n+  __ jump(RuntimeAddress(oop_copy_entry));\n+\n+__ BIND(L_checkcast_copy);\n+  \/\/ live at this point:  r10_src_klass, r11_length, rax (dst_klass)\n+  {\n+    \/\/ Before looking at dst.length, make sure dst is also an objArray.\n+    __ cmpl(Address(rax, lh_offset), objArray_lh);\n+    __ jcc(Assembler::notEqual, L_failed);\n+\n+    \/\/ It is safe to examine both src.length and dst.length.\n+    arraycopy_range_checks(src, src_pos, dst, dst_pos, r11_length,\n+                           rax, L_failed);\n+\n+    const Register r11_dst_klass = r11;\n+    __ load_klass(r11_dst_klass, dst, rklass_tmp); \/\/ reload\n+\n+    \/\/ Marshal the base address arguments now, freeing registers.\n+    __ lea(from, Address(src, src_pos, TIMES_OOP,\n+                 arrayOopDesc::base_offset_in_bytes(T_OBJECT)));\n+    __ lea(to,   Address(dst, dst_pos, TIMES_OOP,\n+                 arrayOopDesc::base_offset_in_bytes(T_OBJECT)));\n+    __ movl(count, length);           \/\/ length (reloaded)\n+    Register sco_temp = c_rarg3;      \/\/ this register is free now\n+    assert_different_registers(from, to, count, sco_temp,\n+                               r11_dst_klass, r10_src_klass);\n+    assert_clean_int(count, sco_temp);\n+\n+    \/\/ Generate the type check.\n+    const int sco_offset = in_bytes(Klass::super_check_offset_offset());\n+    __ movl(sco_temp, Address(r11_dst_klass, sco_offset));\n+    assert_clean_int(sco_temp, rax);\n+    generate_type_check(r10_src_klass, sco_temp, r11_dst_klass, L_plain_copy);\n+\n+    \/\/ Fetch destination element klass from the ObjArrayKlass header.\n+    int ek_offset = in_bytes(ObjArrayKlass::element_klass_offset());\n+    __ movptr(r11_dst_klass, Address(r11_dst_klass, ek_offset));\n+    __ movl(  sco_temp,      Address(r11_dst_klass, sco_offset));\n+    assert_clean_int(sco_temp, rax);\n+\n+#ifdef _WIN64\n+    __ pop(rklass_tmp); \/\/ Restore callee-save rdi\n+#endif\n+\n+    \/\/ the checkcast_copy loop needs two extra arguments:\n+    assert(c_rarg3 == sco_temp, \"#3 already in place\");\n+    \/\/ Set up arguments for checkcast_copy_entry.\n+    setup_arg_regs(4);\n+    __ movptr(r8, r11_dst_klass);  \/\/ dst.klass.element_klass, r8 is c_rarg4 on Linux\/Solaris\n+    __ jump(RuntimeAddress(checkcast_copy_entry));\n+  }\n+\n+__ BIND(L_failed);\n+#ifdef _WIN64\n+  __ pop(rklass_tmp); \/\/ Restore callee-save rdi\n+#endif\n+  __ xorptr(rax, rax);\n+  __ notptr(rax); \/\/ return -1\n+  __ leave();   \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n+\n+  return start;\n+}\n+\n+#undef __\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64_arraycopy.cpp","additions":2546,"deletions":0,"binary":false,"changes":2546,"status":"added"}]}