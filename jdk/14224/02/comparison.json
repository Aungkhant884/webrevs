{"files":[{"patch":"@@ -3558,0 +3558,8 @@\n+void Assembler::vmovsd(XMMRegister dst, XMMRegister src, XMMRegister src2) {\n+  assert(UseAVX > 0, \"Requires some form of AVX\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(src2->encoding(), src->encoding(), dst->encoding(), VEX_SIMD_F2, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x11, (0xC0 | encode));\n+}\n+\n@@ -6549,0 +6557,23 @@\n+void Assembler::evfnmadd213sd(XMMRegister dst, XMMRegister src1, XMMRegister src2, EvexRoundPrefix rmode) { \/\/ Need to add rmode for rounding mode support\n+  assert(VM_Version::supports_evex(), \"\");\n+  InstructionAttr attributes(rmode, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  attributes.set_extended_context();\n+  attributes.set_is_evex_instruction();\n+  int encode = vex_prefix_and_encode(dst->encoding(), src1->encoding(), src2->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16((unsigned char)0xAD, (0xC0 | encode));\n+}\n+\n+void Assembler::vfnmadd213sd(XMMRegister dst, XMMRegister src1, XMMRegister src2) {\n+  assert(VM_Version::supports_fma(), \"\");\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src1->encoding(), src2->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16((unsigned char)0xAD, (0xC0 | encode));\n+}\n+\n+void Assembler::vfnmadd231sd(XMMRegister dst, XMMRegister src1, XMMRegister src2) {\n+  assert(VM_Version::supports_fma(), \"\");\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src1->encoding(), src2->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16((unsigned char)0xBD, (0xC0 | encode));\n+}\n+\n@@ -6910,0 +6941,16 @@\n+void Assembler::vroundsd(XMMRegister dst, XMMRegister src, XMMRegister src2, int32_t rmode) {\n+  assert(VM_Version::supports_avx(), \"\");\n+  assert(rmode <= 0x0f, \"rmode 0x%x\", rmode);\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src->encoding(), src2->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &attributes);\n+  emit_int24(0x0B, (0xC0 | encode), (rmode));\n+}\n+\n+void Assembler::vrndscalesd(XMMRegister dst,  XMMRegister src1, XMMRegister src2, int32_t rmode) {\n+  assert(VM_Version::supports_evex(), \"requires EVEX support\");\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  attributes.set_is_evex_instruction();\n+  int encode = vex_prefix_and_encode(dst->encoding(), src1->encoding(), src2->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &attributes);\n+  emit_int24(0x0B, (0xC0 | encode), (rmode));\n+}\n+\n@@ -8875,0 +8922,13 @@\n+void Assembler::extractps(Register dst, XMMRegister src, uint8_t imm8) {\n+  assert(VM_Version::supports_sse4_1(), \"\");\n+  assert(imm8 <= 0x03, \"imm8: %u\", imm8);\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = simd_prefix_and_encode(src, xnoreg, as_XMMRegister(dst->encoding()), VEX_SIMD_66, VEX_OPCODE_0F_3A, &attributes);\n+  \/\/ imm8:\n+  \/\/ 0x00 - extract from bits 31:0\n+  \/\/ 0x01 - extract from bits 63:32\n+  \/\/ 0x02 - extract from bits 95:64\n+  \/\/ 0x03 - extract from bits 127:96\n+  emit_int24(0x17, (0xC0 | encode), imm8 & 0x03);\n+}\n+\n@@ -9549,0 +9609,9 @@\n+void Assembler::evdivsd(XMMRegister dst, XMMRegister nds, XMMRegister src, EvexRoundPrefix rmode) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  InstructionAttr attributes(rmode, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  attributes.set_extended_context();\n+  attributes.set_is_evex_instruction();\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_F2, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x5E, (0xC0 | encode));\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.cpp","additions":69,"deletions":0,"binary":false,"changes":69,"status":"modified"},{"patch":"@@ -531,0 +531,7 @@\n+  enum EvexRoundPrefix {\n+    EVEX_RNE = 0x0,\n+    EVEX_RD  = 0x1,\n+    EVEX_RU  = 0x2,\n+    EVEX_RZ  = 0x3\n+  };\n+\n@@ -889,0 +896,2 @@\n+  void vmovsd(XMMRegister dst, XMMRegister src, XMMRegister src2);\n+\n@@ -2251,0 +2260,1 @@\n+  void evdivsd(XMMRegister dst, XMMRegister nds, XMMRegister src, EvexRoundPrefix rmode);\n@@ -2254,0 +2264,3 @@\n+  void vfnmadd213sd(XMMRegister dst, XMMRegister nds, XMMRegister src);\n+  void evfnmadd213sd(XMMRegister dst, XMMRegister nds, XMMRegister src, EvexRoundPrefix rmode);\n+  void vfnmadd231sd(XMMRegister dst, XMMRegister src1, XMMRegister src2);\n@@ -2343,0 +2356,1 @@\n+  void vrndscalesd(XMMRegister dst,  XMMRegister src1,  XMMRegister src2, int32_t rmode);\n@@ -2345,0 +2359,2 @@\n+  void vroundsd(XMMRegister dst, XMMRegister src, XMMRegister src2, int32_t rmode);\n+  void vroundsd(XMMRegister dst, XMMRegister src, Address src2, int32_t rmode);\n@@ -2728,0 +2744,2 @@\n+  void extractps(Register dst, XMMRegister src, uint8_t imm8);\n+\n@@ -2961,0 +2979,2 @@\n+  void set_extended_context(void) { _is_extended_context = true; }\n+\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.hpp","additions":20,"deletions":0,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -971,1 +971,1 @@\n-      __ call_runtime_leaf(StubRoutines::dpow(), getThreadTemp(), result_reg, cc->args());\n+        __ call_runtime_leaf(StubRoutines::dpow(), getThreadTemp(), result_reg, cc->args());\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRGenerator_x86.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -90,0 +90,2 @@\n+  const bool is_LP64 = LP64_ONLY(true) NOT_LP64(false);\n+  if (!is_LP64 || UseAVX < 1 || !UseFMA) {\n@@ -100,0 +102,8 @@\n+  } else {\n+    assert(StubRoutines::fmod() != nullptr, \"\");\n+    jdouble (*addr)(jdouble, jdouble) = (double (*)(double, double))StubRoutines::fmod();\n+    jdouble dx = (jdouble) x;\n+    jdouble dy = (jdouble) y;\n+\n+    retval = (jfloat) (*addr)(dx, dy);\n+  }\n@@ -105,0 +115,2 @@\n+  const bool is_LP64 = LP64_ONLY(true) NOT_LP64(false);\n+  if (!is_LP64 || UseAVX < 1 || !UseFMA) {\n@@ -115,0 +127,6 @@\n+  } else {\n+    assert(StubRoutines::fmod() != nullptr, \"\");\n+    jdouble (*addr)(jdouble, jdouble) = (double (*)(double, double))StubRoutines::fmod();\n+\n+    retval = (*addr)(x, y);\n+  }\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86.cpp","additions":18,"deletions":0,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -3940,0 +3940,4 @@\n+\n+  if ((UseAVX >= 1) && (VM_Version::supports_avx512vlbwdq() || VM_Version::supports_fma())) {\n+    StubRoutines::_fmod = generate_libmFmod(); \/\/ from stubGenerator_x86_64_fmod.cpp\n+  }\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -489,0 +489,1 @@\n+  address generate_libmFmod();\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -0,0 +1,779 @@\n+\/*\n+ * Copyright (c) 2023, Intel Corporation. All rights reserved.\n+ * Intel Math Library (LIBM) Source Code\n+ *\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"macroAssembler_x86.hpp\"\n+#include \"stubGenerator_x86_64.hpp\"\n+\n+\/******************************************************************************\/\n+\/\/                     ALGORITHM DESCRIPTION - FMOD()\n+\/\/                     ---------------------\n+\/\/\n+\/\/ If either value1 or value2 is NaN, the result is NaN.\n+\/\/\n+\/\/ If neither value1 nor value2 is NaN, the sign of the result equals the sign of the dividend.\n+\/\/\n+\/\/ If the dividend is an infinity or the divisor is a zero or both, the result is NaN.\n+\/\/\n+\/\/ If the dividend is finite and the divisor is an infinity, the result equals the dividend.\n+\/\/\n+\/\/ If the dividend is a zero and the divisor is finite, the result equals the dividend.\n+\/\/\n+\/\/ In the remaining cases, where neither operand is an infinity, a zero, or NaN, the floating-point\n+\/\/ remainder result from a dividend value1 and a divisor value2 is defined by the mathematical\n+\/\/ relation result = value1 - (value2 * q), where q is an integer that is negative only if\n+\/\/ value1 \/ value2 is negative, and positive only if value1 \/ value2 is positive, and whose magnitude\n+\/\/ is as large as possible without exceeding the magnitude of the true mathematical quotient of value1 and value2.\n+\/\/\n+\/******************************************************************************\/\n+\n+#define __ _masm->\n+\n+ATTRIBUTE_ALIGNED(32) static const uint64_t CONST_NaN[] = {\n+    0x7FFFFFFFFFFFFFFFULL, 0x7FFFFFFFFFFFFFFFULL   \/\/ NaN vector\n+};\n+ATTRIBUTE_ALIGNED(32) static const uint64_t CONST_1p260[] = {\n+    0x5030000000000000ULL,    \/\/ 0x1p+260\n+};\n+\n+ATTRIBUTE_ALIGNED(32) static const uint64_t CONST_MAX[] = {\n+    0x7FEFFFFFFFFFFFFFULL,    \/\/ Max\n+};\n+\n+ATTRIBUTE_ALIGNED(32) static const uint64_t CONST_INF[] = {\n+    0x7FF0000000000000ULL,    \/\/ Inf\n+};\n+\n+ATTRIBUTE_ALIGNED(32) static const uint64_t CONST_e307[] = {\n+    0x7FE0000000000000ULL\n+};\n+\n+address StubGenerator::generate_libmFmod() {\n+  StubCodeMark mark(this, \"StubRoutines\", \"libmFmod\");\n+  address start = __ pc();\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+\n+  if (VM_Version::supports_avx512vlbwdq()) {     \/\/ AVX512 version\n+\n+#if 1\n+\n+  Label L_5280, L_52a0, L_5256, L_5300, L_5320, L_52c0, L_52d0, L_5360, L_5380, L_53b0, L_5390;\n+  Label L_53c0, L_52a6, L_53d0, L_exit;\n+\n+\/\/    0x0000555555555200 <+0>:     c5 f9 6f d0                     vmovdqa xmm2,xmm0\n+\/\/    0x0000555555555204 <+4>:     c5 fa 7e c0                     vmovq  xmm0,xmm0\n+\/\/    0x0000555555555208 <+8>:     c4 e2 79 59 1d 0f 0e 00 00      vpbroadcastq xmm3,QWORD PTR [rip+0xe0f]        # CONST_NaN\n+  __ movdqa(xmm2, xmm0);\n+  __ movq(xmm0, xmm0);\n+  __ mov64(rax, 0x7FFFFFFFFFFFFFFF);\n+  __ evpbroadcastq(xmm3, rax, Assembler::AVX_128bit);\n+\/\/    0x0000555555555211 <+17>:    c5 f9 db f3                     vpand  xmm6,xmm0,xmm3\n+\/\/    0x0000555555555215 <+21>:    c5 f1 db e3                     vpand  xmm4,xmm1,xmm3\n+\/\/    0x0000555555555219 <+25>:    c5 c9 ef d8                     vpxor  xmm3,xmm6,xmm0\n+\/\/    0x000055555555521d <+29>:    c5 fa 7e ec                     vmovq  xmm5,xmm4\n+\/\/    0x0000555555555221 <+33>:    62 f1 cf 78 5e c5               vdivsd xmm0,xmm6,xmm5,{rz-sae}\n+\/\/    0x0000555555555227 <+39>:    c5 fa 7e c0                     vmovq  xmm0,xmm0\n+\/\/    0x000055555555522b <+43>:    c5 c1 57 ff                     vxorpd xmm7,xmm7,xmm7\n+\/\/    0x000055555555522f <+47>:    c4 e3 41 0b c0 0b               vroundsd xmm0,xmm7,xmm0,0xb\n+\/\/    0x0000555555555235 <+53>:    c4 e3 79 17 c0 01               vextractps eax,xmm0,0x1\n+\/\/    0x000055555555523b <+59>:    85 c0                           test   eax,eax\n+\/\/    0x000055555555523d <+61>:    74 41                           je     0x555555555280 <fmod+128>\n+  __ vpand(xmm6, xmm0, xmm3, Assembler::AVX_128bit);\n+  __ vpand(xmm4, xmm1, xmm3, Assembler::AVX_128bit);\n+  __ vpxor(xmm3, xmm6, xmm0, Assembler::AVX_128bit);\n+  __ movq(xmm5, xmm4);\n+  __ evdivsd(xmm0, xmm6, xmm5, Assembler::EVEX_RZ);\n+  __ movq(xmm0, xmm0);\n+  __ vxorpd(xmm7, xmm7, xmm7, Assembler::AVX_128bit);\n+  __ vroundsd(xmm0, xmm7, xmm0, 0xb);\n+  __ extractps(rax, xmm0, 1);\n+  __ testl(rax, rax);\n+  __ jcc(Assembler::equal, L_5280);\n+\n+\/\/    0x000055555555523f <+63>:    3d fe ff ef 7f                  cmp    eax,0x7feffffe\n+\/\/    0x0000555555555244 <+68>:    76 5a                           jbe    0x5555555552a0 <fmod+160>\n+\/\/    0x0000555555555246 <+70>:    c5 e9 ef d2                     vpxor  xmm2,xmm2,xmm2\n+\/\/    0x000055555555524a <+74>:    c5 f9 2e e2                     vucomisd xmm4,xmm2\n+\/\/    0x000055555555524e <+78>:    75 06                           jne    0x555555555256 <fmod+86>\n+\/\/    0x0000555555555250 <+80>:    0f 8b aa 00 00 00               jnp    0x555555555300 <fmod+256>\n+\/\/    0x0000555555555256 <+86>:    c5 fb 10 15 ca 0d 00 00         vmovsd xmm2,QWORD PTR [rip+0xdca]        # CONST_MAX\n+\/\/    0x000055555555525e <+94>:    c5 f9 2e d6                     vucomisd xmm2,xmm6\n+\/\/    0x0000555555555262 <+98>:    0f 82 98 00 00 00               jb     0x555555555300 <fmod+256>\n+\/\/    0x0000555555555268 <+104>:   c5 fb 10 05 c0 0d 00 00         vmovsd xmm0,QWORD PTR [rip+0xdc0]        # CONST_INF\n+\/\/    0x0000555555555270 <+112>:   c5 f9 2e c4                     vucomisd xmm0,xmm4\n+\/\/    0x0000555555555274 <+116>:   0f 83 a6 00 00 00               jae    0x555555555320 <fmod+288>\n+\/\/    0x000055555555527a <+122>:   c5 f3 58 c1                     vaddsd xmm0,xmm1,xmm1\n+\/\/    0x000055555555527e <+126>:   c3                              ret\n+\/\/    0x000055555555527f <+127>:   90                              nop\n+  __ cmpl(rax, 0x7feffffe);\n+  __ jcc(Assembler::belowEqual, L_52a0);\n+  __ vpxor(xmm2, xmm2, xmm2, Assembler::AVX_128bit);\n+  __ ucomisd(xmm4, xmm2);\n+  __ jcc(Assembler::notEqual, L_5256);\n+  __ jcc(Assembler::noParity, L_5300);\n+\n+  __ bind(L_5256);\n+  __ movsd(xmm2, ExternalAddress((address)CONST_MAX), rax);\n+  __ ucomisd(xmm2, xmm6);\n+  __ jcc(Assembler::below, L_5300);\n+  __ movsd(xmm0, ExternalAddress((address)CONST_INF), rax);\n+  __ ucomisd(xmm0, xmm4);\n+  __ jcc(Assembler::aboveEqual, L_5320);\n+\n+  __ vaddsd(xmm0, xmm1, xmm1);\n+  __ jmp(L_exit);\n+\/\/    0x0000555555555280 <+128>:   c5 e3 58 c2                     vaddsd xmm0,xmm3,xmm2\n+\/\/    0x0000555555555284 <+132>:   c3                              ret\n+\/\/    0x0000555555555285 <+133>:   66 66 66 66 66 66 2e 0f 1f 84 00 00 00 00 00    data16 data16 data16 data16 data16 cs nop WORD PTR [rax+rax*1+0x0]\n+\/\/    0x0000555555555294 <+148>:   66 66 66 2e 0f 1f 84 00 00 00 00 00     data16 data16 cs nop WORD PTR [rax+rax*1+0x0]\n+\/\/    0x00005555555552a0 <+160>:   62 f2 dd 78 ad c6               vfnmadd213sd xmm0,xmm4,xmm6,{rz-sae}\n+\/\/    0x00005555555552a6 <+166>:   c5 f9 2e c4                     vucomisd xmm0,xmm4\n+\/\/    0x00005555555552aa <+170>:   73 14                           jae    0x5555555552c0 <fmod+192>\n+\/\/    0x00005555555552ac <+172>:   c5 e1 ef c0                     vpxor  xmm0,xmm3,xmm0\n+\/\/    0x00005555555552b0 <+176>:   c3                              ret\n+  __ align32();\n+  __ bind(L_5280);\n+  __ vaddsd(xmm0, xmm3, xmm2);\n+  __ jmp(L_exit);\n+\n+  __ align(8);\n+  __ bind(L_52a0);\n+  __ evfnmadd213sd(xmm0, xmm4, xmm6, Assembler::EVEX_RZ);\n+  __ bind(L_52a6);\n+  __ ucomisd(xmm0, xmm4);\n+  __ jcc(Assembler::aboveEqual, L_52c0);\n+  __ vpxor(xmm0, xmm3, xmm0, Assembler::AVX_128bit);\n+  __ jmp(L_exit);\n+\n+  __ bind(L_52c0);\n+\/\/    0x00005555555552b1 <+177>:   66 66 66 66 66 66 2e 0f 1f 84 00 00 00 00 00    data16 data16 data16 data16 data16 cs nop WORD PTR [rax+rax*1+0x0]\n+\/\/    0x00005555555552c0 <+192>:   c5 fa 7e f0                     vmovq  xmm6,xmm0\n+\/\/    0x00005555555552c4 <+196>:   c5 f1 ef c9                     vpxor  xmm1,xmm1,xmm1\n+\/\/    0x00005555555552c8 <+200>:   0f 1f 84 00 00 00 00 00         nop    DWORD PTR [rax+rax*1+0x0]\n+\/\/    0x00005555555552d0 <+208>:   62 f1 cf 78 5e d5               vdivsd xmm2,xmm6,xmm5,{rz-sae}\n+\/\/    0x00005555555552d6 <+214>:   c5 fa 7e d2                     vmovq  xmm2,xmm2\n+\/\/    0x00005555555552da <+218>:   c4 e3 71 0b d2 0b               vroundsd xmm2,xmm1,xmm2,0xb\n+\/\/    0x00005555555552e0 <+224>:   62 f2 dd 78 ad d0               vfnmadd213sd xmm2,xmm4,xmm0,{rz-sae}\n+\/\/    0x00005555555552e6 <+230>:   c5 f9 2e d4                     vucomisd xmm2,xmm4\n+\/\/    0x00005555555552ea <+234>:   c5 fa 7e f2                     vmovq  xmm6,xmm2\n+\/\/    0x00005555555552ee <+238>:   c5 f9 28 c2                     vmovapd xmm0,xmm2\n+\/\/    0x00005555555552f2 <+242>:   73 dc                           jae    0x5555555552d0 <fmod+208>\n+\/\/    0x00005555555552f4 <+244>:   c5 e1 ef c2                     vpxor  xmm0,xmm3,xmm2\n+\/\/    0x00005555555552f8 <+248>:   c3                              ret\n+  __ movq(xmm6, xmm0);\n+  __ vpxor(xmm1, xmm1, xmm1, Assembler::AVX_128bit);\n+  __ align32();\n+  __ bind(L_52d0);\n+\n+  __ evdivsd(xmm2, xmm6, xmm5, Assembler::EVEX_RZ);\n+  __ movq(xmm2, xmm2);\n+  __ vroundsd(xmm2, xmm1, xmm2, 0xb);\n+  __ evfnmadd213sd(xmm2, xmm4, xmm0, Assembler::EVEX_RZ);\n+  __ ucomisd(xmm2, xmm4);\n+  __ movq(xmm6, xmm2);\n+  __ movapd(xmm0, xmm2);\n+  __ jcc(Assembler::aboveEqual, L_52d0);\n+\n+  __ vpxor(xmm0, xmm3, xmm2, Assembler::AVX_128bit);\n+  __ jmp(L_exit);\n+\/\/    0x00005555555552f9 <+249>:   0f 1f 80 00 00 00 00            nop    DWORD PTR [rax+0x0]\n+\/\/    0x0000555555555300 <+256>:   c4 e2 d9 ad c6                  vfnmadd213sd xmm0,xmm4,xmm6\n+\/\/    0x0000555555555305 <+261>:   c3                              ret\n+  __ bind(L_5300);\n+  __ vfnmadd213sd(xmm0, xmm4, xmm6);\n+  __ jmp(L_exit);\n+\/\/    0x0000555555555306 <+262>:   66 66 66 66 66 66 2e 0f 1f 84 00 00 00 00 00    data16 data16 data16 data16 data16 cs nop WORD PTR [rax+rax*1+0x0]\n+\/\/    0x0000555555555315 <+277>:   66 66 2e 0f 1f 84 00 00 00 00 00        data16 cs nop WORD PTR [rax+rax*1+0x0]\n+\/\/    0x0000555555555320 <+288>:   c5 db 59 0d 10 0d 00 00         vmulsd xmm1,xmm4,QWORD PTR [rip+0xd10]        # CONST_e307\n+\/\/    0x0000555555555328 <+296>:   c5 fa 7e d1                     vmovq  xmm2,xmm1\n+\/\/    0x000055555555532c <+300>:   62 f1 cf 78 5e c2               vdivsd xmm0,xmm6,xmm2,{rz-sae}\n+\/\/    0x0000555555555332 <+306>:   c5 fa 7e c0                     vmovq  xmm0,xmm0\n+\/\/    0x0000555555555336 <+310>:   c4 e3 41 0b f8 0b               vroundsd xmm7,xmm7,xmm0,0xb\n+\/\/    0x000055555555533c <+316>:   c4 e3 79 17 f8 01               vextractps eax,xmm7,0x1\n+\/\/    0x0000555555555342 <+322>:   3d ff ff ef 7f                  cmp    eax,0x7fefffff\n+\/\/    0x0000555555555347 <+327>:   72 17                           jb     0x555555555360 <fmod+352>\n+\/\/    0x0000555555555349 <+329>:   c5 f3 59 05 e7 0c 00 00         vmulsd xmm0,xmm1,QWORD PTR [rip+0xce7]        # CONST_e307\n+\/\/    0x0000555555555351 <+337>:   c5 f9 2e f0                     vucomisd xmm6,xmm0\n+\/\/    0x0000555555555355 <+341>:   73 29                           jae    0x555555555380 <fmod+384>\n+\/\/    0x0000555555555357 <+343>:   c5 f9 28 fe                     vmovapd xmm7,xmm6\n+\/\/    0x000055555555535b <+347>:   eb 53                           jmp    0x5555555553b0 <fmod+432>\n+  __ bind(L_5320);\n+  __ vmulsd(xmm1, xmm4, ExternalAddress((address)CONST_e307), rax);\n+  __ movq(xmm2, xmm1);\n+  __ evdivsd(xmm0, xmm6, xmm2, Assembler::EVEX_RZ);\n+  __ movq(xmm0, xmm0);\n+  __ vroundsd(xmm7, xmm7, xmm0, 0xb);\n+  __ extractps(rax, xmm7, 1);\n+  __ cmpl(rax, 0x7fefffff);\n+  __ jcc(Assembler::below, L_5360);\n+  __ vmulsd(xmm0, xmm1, ExternalAddress((address)CONST_e307), rax);\n+  __ ucomisd(xmm6, xmm0);\n+  __ jcc(Assembler::aboveEqual, L_5380);\n+  __ movapd(xmm7, xmm6);\n+  __ jmp(L_53b0);\n+\/\/    0x000055555555535d <+349>:   0f 1f 00                        nop    DWORD PTR [rax]\n+\/\/    0x0000555555555360 <+352>:   62 f2 f5 78 ad fe               vfnmadd213sd xmm7,xmm1,xmm6,{rz-sae}\n+\/\/    0x0000555555555366 <+358>:   eb 48                           jmp    0x5555555553b0 <fmod+432>\n+  __ bind(L_5360);\n+  __ evfnmadd213sd(xmm7, xmm1, xmm6, Assembler::EVEX_RZ);\n+  __ jmp(L_53b0);\n+\/\/    0x0000555555555368 <+360>:   66 66 66 66 66 66 2e 0f 1f 84 00 00 00 00 00    data16 data16 data16 data16 data16 cs nop WORD PTR [rax+rax*1+0x0]\n+\/\/    0x0000555555555377 <+375>:   66 0f 1f 84 00 00 00 00 00      nop    WORD PTR [rax+rax*1+0x0]\n+\/\/    0x0000555555555380 <+384>:   c4 41 39 57 c0                  vxorpd xmm8,xmm8,xmm8\n+\/\/    0x0000555555555385 <+389>:   66 66 2e 0f 1f 84 00 00 00 00 00        data16 cs nop WORD PTR [rax+rax*1+0x0]\n+  __ bind(L_5380);\n+  __ vxorpd(xmm8, xmm8, xmm8, Assembler::AVX_128bit);\n+\n+  __ align32();\n+  __ bind(L_5390);\n+\/\/    0x0000555555555390 <+400>:   62 f1 cf 78 5e f8               vdivsd xmm7,xmm6,xmm0,{rz-sae}\n+\/\/    0x0000555555555396 <+406>:   c5 fa 7e ff                     vmovq  xmm7,xmm7\n+\/\/    0x000055555555539a <+410>:   c4 e3 39 0b ff 0b               vroundsd xmm7,xmm8,xmm7,0xb\n+\/\/    0x00005555555553a0 <+416>:   62 f2 fd 78 ad fe               vfnmadd213sd xmm7,xmm0,xmm6,{rz-sae}\n+\/\/    0x00005555555553a6 <+422>:   c5 f9 2e f8                     vucomisd xmm7,xmm0\n+\/\/    0x00005555555553aa <+426>:   c5 f9 28 f7                     vmovapd xmm6,xmm7\n+\/\/    0x00005555555553ae <+430>:   73 e0                           jae    0x555555555390 <fmod+400>\n+\/\/    0x00005555555553b0 <+432>:   c5 f9 2e f9                     vucomisd xmm7,xmm1\n+\/\/    0x00005555555553b4 <+436>:   73 0a                           jae    0x5555555553c0 <fmod+448>\n+\/\/    0x00005555555553b6 <+438>:   c5 f9 28 c7                     vmovapd xmm0,xmm7\n+\/\/    0x00005555555553ba <+442>:   e9 e7 fe ff ff                  jmp    0x5555555552a6 <fmod+166>\n+  __ evdivsd(xmm7, xmm6, xmm0, Assembler::EVEX_RZ);\n+  __ movq(xmm7, xmm7);\n+  __ vroundsd(xmm7, xmm8, xmm7, 0xb);\n+  __ evfnmadd213sd(xmm7, xmm0, xmm6, Assembler::EVEX_RZ);\n+  __ ucomisd(xmm7, xmm0);\n+  __ movapd(xmm6, xmm7);\n+  __ jcc(Assembler::aboveEqual, L_5390);\n+  __ bind(L_53b0);\n+  __ ucomisd(xmm7, xmm1);\n+  __ jcc(Assembler::aboveEqual, L_53c0);\n+  __ movapd(xmm0, xmm7);\n+  __ jmp(L_52a6);\n+\/\/    0x00005555555553bf <+447>:   90                              nop\n+\/\/    0x00005555555553c0 <+448>:   c5 c9 57 f6                     vxorpd xmm6,xmm6,xmm6\n+\/\/    0x00005555555553c4 <+452>:   66 66 66 2e 0f 1f 84 00 00 00 00 00     data16 data16 cs nop WORD PTR [rax+rax*1+0x0]\n+\/\/    0x00005555555553d0 <+464>:   62 f1 c7 78 5e c2               vdivsd xmm0,xmm7,xmm2,{rz-sae}\n+\/\/    0x00005555555553d6 <+470>:   c5 fa 7e c0                     vmovq  xmm0,xmm0\n+\/\/    0x00005555555553da <+474>:   c4 e3 49 0b c0 0b               vroundsd xmm0,xmm6,xmm0,0xb\n+\/\/    0x00005555555553e0 <+480>:   62 f2 f5 78 ad c7               vfnmadd213sd xmm0,xmm1,xmm7,{rz-sae}\n+\/\/    0x00005555555553e6 <+486>:   c5 f9 2e c1                     vucomisd xmm0,xmm1\n+\/\/    0x00005555555553ea <+490>:   c5 f9 28 f8                     vmovapd xmm7,xmm0\n+\/\/    0x00005555555553ee <+494>:   73 e0                           jae    0x5555555553d0 <fmod+464>\n+\/\/    0x00005555555553f0 <+496>:   e9 b1 fe ff ff                  jmp    0x5555555552a6 <fmod+166>\n+  __ bind(L_53c0);\n+  __ vxorpd(xmm6, xmm6, xmm6, Assembler::AVX_128bit);\n+  __ align32();\n+  __ bind(L_53d0);\n+  __ evdivsd(xmm0, xmm7, xmm2, Assembler::EVEX_RZ);\n+  __ movq(xmm0, xmm0);\n+  __ vroundsd(xmm0, xmm6, xmm0, 0xb);\n+  __ evfnmadd213sd(xmm0, xmm1, xmm7, Assembler::EVEX_RZ);\n+  __ ucomisd(xmm0, xmm1);\n+  __ movapd(xmm7, xmm0);\n+  __ jcc(Assembler::aboveEqual, L_53d0);\n+  __ jmp(L_52a6);\n+\n+\n+\n+\n+\n+\/\/ (gdb) x\/30gx 0x555555556020\n+\/\/ 0x555555556020: 0x7fffffffffffffff      0x7fefffffffffffff\n+\/\/ 0x555555556030: 0x7ff0000000000000      0x7fe0000000000000\n+\/\/ 0x555555556040: 0x000000343b031b01      0xffffefe000000005\n+\/\/ 0x555555556050: 0xffffeff000000068      0xfffff00000000090\n+\/\/ 0x555555556060: 0xfffff0e900000050      0xfffff1c0000000a8\n+\/\/ 0x555555556070: 0x00000000000000c8      0x0000000000000014\n+\n+  __ bind(L_exit);\n+\n+#else\n+\n+  Label L_12ca, L_1280, L_115a, L_1271, L_1268, L_1227, L_1231, L_11f4, L_128a, L_1237, L_12bf, L_1290, L_exit;\n+\n+\/\/ #include <ia32intrin.h>\n+\/\/ #include <emmintrin.h>\n+\/\/ #pragma float_control(precise, on)\n+\n+\/\/ #define UINT32 unsigned int\n+\/\/ #define SINT32 int\n+\/\/ #define UINT64 unsigned __int64\n+\/\/ #define SINT64 __int64\n+\n+\/\/ #define DP_FMA(a, b, c)    __fence(_mm_cvtsd_f64(_mm_fmadd_sd(_mm_set_sd(a), _mm_set_sd(b), _mm_set_sd(c))))\n+\/\/ #define DP_FMA_RN(a, b, c)    _mm_cvtsd_f64(_mm_fmadd_round_sd(_mm_set_sd(a), _mm_set_sd(b), _mm_set_sd(c), (_MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC)))\n+\/\/ #define DP_FMA_RZ(a, b, c) __fence(_mm_cvtsd_f64(_mm_fmadd_round_sd(_mm_set_sd(a), _mm_set_sd(b), _mm_set_sd(c), (_MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC))))\n+\n+\/\/ #define DP_ROUND_RZ(a)   _mm_cvtsd_f64(_mm_roundscale_sd(_mm_setzero_pd(), _mm_set_sd(a), (_MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC)))\n+\n+\/\/ #define DP_CONST(C)    _castu64_f64(0x##C##ull)\n+\/\/ #define DP_AND(X, Y)   _mm_cvtsd_f64(_mm_and_pd(_mm_set_sd(X), _mm_set_sd(Y)))\n+\/\/ #define DP_XOR(X, Y)   _mm_cvtsd_f64(_mm_xor_pd(_mm_set_sd(X), _mm_set_sd(Y)))\n+\/\/ #define DP_OR(X, Y)    _mm_cvtsd_f64(_mm_or_pd(_mm_set_sd(X), _mm_set_sd(Y)))\n+\/\/ #define DP_DIV_RZ(a, b) __fence(_mm_cvtsd_f64(_mm_div_round_sd(_mm_set_sd(a), _mm_set_sd(b), (_MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC))))\n+\/\/ #define DP_FNMA(a, b, c)    __fence(_mm_cvtsd_f64(_mm_fnmadd_sd(_mm_set_sd(a), _mm_set_sd(b), _mm_set_sd(c))))\n+\/\/ #define DP_FNMA_RZ(a, b, c) __fence(_mm_cvtsd_f64(_mm_fnmadd_round_sd(_mm_set_sd(a), _mm_set_sd(b), _mm_set_sd(c), (_MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC))))\n+\n+\/\/ #define D2L(x)  _mm_castpd_si128(x)\n+\/\/ \/\/ transfer highest 32 bits (of low 64b) to GPR\n+\/\/ #define TRANSFER_HIGH_INT32(X)   _mm_extract_epi32(D2L(_mm_set_sd(X)), 1)\n+\n+\n+\/\/ double fmod(double x, double y)\n+\/\/ {\n+  __ subptr(rsp, 0x28);\n+  __ mov64(rax, 0x7fffffffffffffff);\n+  __ movapd(xmm20, xmm0);\n+  __ movapd(xmm3, xmm1);\n+\/\/ double a, b, sgn_a, q, bs, bs2;\n+\/\/ unsigned eq;\n+\n+\/\/     \/\/ |x|, |y|\n+\/\/     a = DP_AND(x, DP_CONST(7fffffffffffffff));\n+  __ vxorpd(xmm18, xmm18, xmm18, Assembler::AVX_128bit);\n+  __ vmovsd(xmm5, xmm18, xmm20);\n+  __ movq(xmm17, rax);\n+  __ vandpd(xmm0, xmm5, xmm17, Assembler::AVX_128bit);\n+\/\/     b = DP_AND(y, DP_CONST(7fffffffffffffff));\n+  __ vandpd(xmm1, xmm3, xmm17, Assembler::AVX_128bit);\n+\/\/   \/\/ sign(x)\n+\/\/   sgn_a = DP_XOR(x, a);\n+  __ vmovsd(xmm17, xmm18, xmm0);\n+  __ vxorpd(xmm16, xmm5, xmm17, Assembler::AVX_128bit);\n+\n+\/\/   q = DP_DIV_RZ(a, b);\n+  __ vmovsd(xmm5, xmm18, xmm1);\n+  __ evdivsd(xmm2, xmm17, xmm5, Assembler::EVEX_RZ);\n+\/\/   q = DP_ROUND_RZ(q);\n+  __ vmovsd(xmm4, xmm18, xmm2);\n+  __ vrndscalesd(xmm19, xmm18, xmm4, 0x0B);\n+\n+\/\/   eq = TRANSFER_HIGH_INT32(q);\n+  __ vmovsd(xmm4, xmm18, xmm19);\n+  __ pextrd(rax, xmm4, 1);\n+\n+\/\/   if (!eq)  return x + sgn_a;\n+__ testl(rax, rax);\n+__ jcc(Assembler::equal, L_12ca);\n+\n+\/\/   if (eq >= 0x7fefffffu) goto SPECIAL_FMOD;\n+__ cmpl(rax, 0x7FEFFFFF);\n+__ jcc(Assembler::below, L_1280);\n+\n+\/\/ SPECIAL_FMOD:\n+\n+\/\/   \/\/ y==0 or x==Inf?\n+\/\/   if ((b == 0.0) || (!(a <= DP_CONST(7fefffffffffffff))))\n+  __ vxorpd(xmm2, xmm2, xmm2, Assembler::AVX_128bit);\n+  __ ucomisd(xmm1, xmm2);\n+  __ jcc(Assembler::parity, L_115a);\n+  __ jcc(Assembler::equal, L_1271);\n+\n+  __ bind(L_115a);\n+  __ movsd(xmm2, ExternalAddress((address)CONST_MAX), rax);\n+  __ comisd(xmm2, xmm0);\n+  __ jcc(Assembler::below, L_1271);\n+\/\/   \/\/ y is NaN?\n+\/\/   if (!(b <= DP_CONST(7ff0000000000000))) return y + y;\n+  __ movsd(xmm2, ExternalAddress((address)CONST_INF), rax);\n+  __ comisd(xmm2, xmm1);\n+  __ jcc(Assembler::below, L_1268);\n+\n+\/\/   \/\/ b* 2*1023\n+\/\/   bs = b * DP_CONST(7fe0000000000000);\n+  __ movsd(xmm21, ExternalAddress((address)CONST_e307), rax);\n+  __ vmulsd(xmm2, xmm1, xmm21);\n+\n+\/\/   q = DP_DIV_RZ(a, bs);\n+  __ vmovsd(xmm4, xmm18, xmm2);\n+  __ evdivsd(xmm3, xmm17, xmm4, Assembler::EVEX_RZ);\n+\/\/   q = DP_ROUND_RZ(q);\n+  __ vmovsd(xmm19, xmm18, xmm3);\n+  __ vrndscalesd(xmm20, xmm18, xmm19, 0x0B);\n+\n+\/\/   eq = TRANSFER_HIGH_INT32(q);\n+  __ vmovsd(xmm3, xmm18, xmm20);\n+  __ pextrd(rdx, xmm3, 1);\n+\n+\/\/   if (eq >= 0x7fefffffu)\n+  __ cmpl(rdx, 0x7FEFFFFF);\n+  __ jcc(Assembler::below, L_1227);\n+\/\/   {\n+\/\/     \/\/ b* 2*1023 * 2^1023\n+\/\/     bs2 = bs * DP_CONST(7fe0000000000000);\n+  __ vmulsd(xmm3, xmm2, xmm21);\n+\/\/     while (bs2 <= a)\n+  __ comisd(xmm0, xmm3);\n+  __ jcc(Assembler::below, L_1231);\n+  __ vmovsd(xmm17, xmm18, xmm3);\n+\/\/     {\n+\/\/       q = DP_DIV_RZ(a, bs2);\n+  __ align32();\n+  __ bind(L_11f4);\n+  __ vmovsd(xmm22, xmm18, xmm0);\n+  __ evdivsd(xmm0, xmm22, xmm17, Assembler::EVEX_RZ);\n+\/\/       q = DP_ROUND_RZ(q);\n+  __ vmovsd(xmm19, xmm18, xmm0);\n+  __ vrndscalesd(xmm20, xmm18, xmm19, 0x0B);\n+\/\/       a = DP_FNMA_RZ(bs2, q, a);\n+  __ movapd(xmm0, xmm17);\n+  __ vmovsd(xmm21, xmm18, xmm20);\n+  __ evfnmadd213sd(xmm0, xmm21, xmm22, Assembler::EVEX_RZ);\n+\/\/     while (bs2 <= a)\n+  __ comisd(xmm0, xmm3);\n+  __ jcc(Assembler::aboveEqual, L_11f4);\n+  __ jmp(L_1231);\n+\/\/     }\n+\/\/   }\n+\/\/   else\n+\/\/   a = DP_FNMA_RZ(bs, q, a);\n+  __ bind(L_1227);\n+  __ movapd(xmm0, xmm4);\n+  __ evfnmadd213sd(xmm0, xmm3, xmm17, Assembler::EVEX_RZ);\n+\n+\/\/   while (bs <= a)\n+  __ bind(L_1231);\n+  __ comisd(xmm0, xmm2);\n+  __ jcc(Assembler::below, L_128a);\n+\/\/   {\n+\/\/     q = DP_DIV_RZ(a, bs);\n+  __ align32();\n+  __ bind(L_1237);\n+  __ vmovsd(xmm20, xmm18, xmm0);\n+  __ evdivsd(xmm0, xmm20, xmm4, Assembler::EVEX_RZ);\n+\/\/     q = DP_ROUND_RZ(q);\n+  __ vmovsd(xmm3, xmm18, xmm0);\n+  __ vrndscalesd(xmm17, xmm18, xmm3, 0x0B);\n+\/\/     a = DP_FNMA_RZ(bs, q, a);\n+  __ movapd(xmm0, xmm4);\n+  __ vmovsd(xmm19, xmm18, xmm17);\n+  __ evfnmadd213sd(xmm0, xmm19, xmm20, Assembler::EVEX_RZ);\n+\n+\/\/   while (bs <= a)\n+  __ comisd(xmm0, xmm2);\n+  __ jcc(Assembler::aboveEqual, L_1237);\n+  __ jmp(L_128a);\n+\/\/   \/\/ y is NaN?\n+\/\/   if (!(b <= DP_CONST(7ff0000000000000))) return y + y;\n+__ bind(L_1268);\n+__ vaddsd(xmm0, xmm3, xmm3);\n+__ jmp(L_exit);\n+\/\/     return DP_FNMA(b, q, a);    \/\/ NaN\n+\n+  __ bind(L_1271);\n+  __ vfnmadd213sd(xmm5, xmm4, xmm17);\n+  __ movapd(xmm0, xmm5);\n+  __ jmp(L_exit);\n+\n+\/\/   a = DP_FNMA_RZ(b, q, a);\n+  __ bind(L_1280);\n+  __ movapd(xmm0, xmm5);\n+  __ evfnmadd213sd(xmm0, xmm4, xmm17, Assembler::EVEX_RZ);\n+\n+\/\/ FMOD_CONT:\n+\/\/   while (b <= a)\n+  __ bind(L_128a);\n+  __ comisd(xmm0, xmm1);\n+  __ jcc(Assembler::below, L_12bf);\n+\/\/   {\n+\/\/     q = DP_DIV_RZ(a, b);\n+  __ align32();\n+  __ bind(L_1290);\n+  __ vmovsd(xmm17, xmm18, xmm0);\n+  __ evdivsd(xmm0, xmm17, xmm5, Assembler::EVEX_RZ);\n+\/\/     q = DP_ROUND_RZ(q);\n+  __ vmovsd(xmm2, xmm18, xmm0);\n+  __ vrndscalesd(xmm3, xmm18, xmm2, 0x0B);\n+\/\/     a = DP_FNMA_RZ(b, q, a);\n+  __ movapd(xmm0, xmm5);\n+  __ vmovsd(xmm4, xmm18, xmm3);\n+  __ evfnmadd213sd(xmm0, xmm4, xmm17, Assembler::EVEX_RZ);\n+\n+\/\/ FMOD_CONT:\n+\/\/   while (b <= a)\n+\/\/   }\n+  __ comisd(xmm0, xmm1);\n+  __ jcc(Assembler::aboveEqual, L_1290);\n+\n+\/\/   a = DP_XOR(a, sgn_a);\n+  __ bind(L_12bf);\n+  __ vxorpd(xmm0, xmm0, xmm16, Assembler::AVX_128bit);\n+\n+\/\/   return a;\n+__ jmp(L_exit);\n+\n+\/\/   if (!eq)  return x + sgn_a;\n+  __ bind(L_12ca);\n+  __ vaddsd(xmm0, xmm20, xmm16);\n+  __ bind(L_exit);\n+  __ addptr(rsp, 0x28);\n+\n+#endif\n+\n+  } else if (VM_Version::supports_fma()) {       \/\/ AVX2 version\n+\n+    Label L_104a, L_11bd, L_10c1, L_1090, L_11b9, L_10e7, L_11af, L_111c, L_10f3, L_116e, L_112a;\n+    Label L_1173, L_1157, L_117f, L_11a0;\n+\n+\/\/   double fmod(double x, double y)\n+\/\/ {\n+\/\/ double a, b, sgn_a, q, bs, bs2, corr, res;\n+\/\/ unsigned eq;\n+\/\/ unsigned mxcsr, mxcsr_rz;\n+\n+\/\/   __asm { stmxcsr DWORD PTR[mxcsr] }\n+\/\/   mxcsr_rz = 0x7f80 | mxcsr;\n+  __ push(rax);\n+  __ stmxcsr(Address(rsp, 0));\n+  __ movl(rax, Address(rsp, 0));\n+  __ movl(rcx, rax);\n+  __ orl(rcx, 0x7f80);\n+  __ movl(Address(rsp, 0x04), rcx);\n+\n+\/\/     \/\/ |x|, |y|\n+\/\/     a = DP_AND(x, DP_CONST(7fffffffffffffff));\n+  __ movq(xmm2, xmm0);\n+  __ vmovdqu(xmm3, ExternalAddress((address)CONST_NaN), rcx);\n+  __ vpand(xmm4, xmm2, xmm3, Assembler::AVX_128bit);\n+\/\/     b = DP_AND(y, DP_CONST(7fffffffffffffff));\n+  __ vpand(xmm3, xmm1, xmm3, Assembler::AVX_128bit);\n+\/\/   \/\/ sign(x)\n+\/\/   sgn_a = DP_XOR(x, a);\n+  __ mov64(rcx, 0x8000000000000000);\n+  __ movq(xmm5, rcx);\n+  __ vpand(xmm2, xmm2, xmm5, Assembler::AVX_128bit);\n+\n+\/\/   if (a < b)  return x + sgn_a;\n+  __ ucomisd(xmm3, xmm4);\n+  __ jcc(Assembler::belowEqual, L_104a);\n+  __ vaddsd(xmm0, xmm2, xmm0);\n+  __ jmp(L_11bd);\n+\n+\/\/   if (((mxcsr & 0x6000)!=0x2000) && (a < b * 0x1p+260))\n+  __ bind(L_104a);\n+  __ andl(rax, 0x6000);\n+  __ cmpl(rax, 0x2000);\n+  __ jcc(Assembler::equal, L_10c1);\n+  __ vmulsd(xmm0, xmm3, ExternalAddress((address)CONST_1p260), rax);\n+  __ ucomisd(xmm0, xmm4);\n+  __ jcc(Assembler::belowEqual, L_10c1);\n+\/\/   {\n+\/\/     q = DP_DIV(a, b);\n+  __ vdivpd(xmm0, xmm4, xmm3, Assembler::AVX_128bit);\n+\/\/     corr = DP_SHR(DP_FNMA(b, q, a), 63);\n+  __ movapd(xmm1, xmm0);\n+  __ vfnmadd213sd(xmm1, xmm3, xmm4);\n+  __ movq(xmm5, xmm1);\n+  __ vpxor(xmm1, xmm1, xmm1, Assembler::AVX_128bit);\n+  __ vpcmpgtq(xmm5, xmm1, xmm5, Assembler::AVX_128bit);\n+\/\/     q = DP_PSUBQ(q, corr);\n+  __ vpaddq(xmm0, xmm5, xmm0, Assembler::AVX_128bit);\n+\/\/     q = DP_TRUNC(q);\n+  __ vroundsd(xmm0, xmm0, xmm0, 3);\n+\/\/     a = DP_FNMA(b, q, a);\n+  __ vfnmadd213sd(xmm0, xmm3, xmm4);\n+  __ align32();\n+\/\/     while (b <= a)\n+  __ bind(L_1090);\n+  __ ucomisd(xmm0, xmm3);\n+  __ jcc(Assembler::below, L_11b9);\n+\/\/     {\n+\/\/       q = DP_DIV(a, b);\n+  __ vdivsd(xmm4, xmm0, xmm3);\n+\/\/       corr = DP_SHR(DP_FNMA(b, q, a), 63);\n+  __ movapd(xmm5, xmm4);\n+  __ vfnmadd213sd(xmm5, xmm3, xmm0);\n+  __ movq(xmm5, xmm5);\n+  __ vpcmpgtq(xmm5, xmm1, xmm5, Assembler::AVX_128bit);\n+\/\/       q = DP_PSUBQ(q, corr);\n+  __ vpaddq(xmm4, xmm5, xmm4, Assembler::AVX_128bit);\n+\/\/       q = DP_TRUNC(q);\n+  __ vroundsd(xmm4, xmm4, xmm4, 3);\n+\/\/       a = DP_FNMA(b, q, a);\n+  __ vfnmadd231sd(xmm0, xmm3, xmm4);\n+  __ jmp(L_1090);\n+\/\/     }\n+\/\/     return DP_XOR(a, sgn_a);\n+\/\/   }\n+\n+\/\/   __asm { ldmxcsr DWORD PTR [mxcsr_rz] }\n+  __ bind(L_10c1);\n+  __ ldmxcsr(Address(rsp, 0x04));\n+\n+\/\/   q = DP_DIV(a, b);\n+  __ vdivpd(xmm0, xmm4, xmm3, Assembler::AVX_128bit);\n+\/\/   q = DP_TRUNC(q);\n+  __ vroundsd(xmm0, xmm0, xmm0, 3);\n+\n+\/\/   eq = TRANSFER_HIGH_INT32(q);\n+  __ extractps(rax, xmm0, 1);\n+\n+\/\/   if (__builtin_expect((eq >= 0x7fefffffu), (0==1))) goto SPECIAL_FMOD;\n+  __ cmpl(rax, 0x7feffffe);\n+  __ jcc(Assembler::above, L_10e7);\n+\n+\/\/   a = DP_FNMA(b, q, a);\n+  __ vfnmadd213sd(xmm0, xmm3, xmm4);\n+  __ jmp(L_11af);\n+\n+\/\/ SPECIAL_FMOD:\n+\n+\/\/   \/\/ y==0 or x==Inf?\n+\/\/   if ((b == 0.0) || (!(a <= DP_CONST(7fefffffffffffff))))\n+  __ bind(L_10e7);\n+  __ vpxor(xmm5, xmm5, xmm5, Assembler::AVX_128bit);\n+  __ ucomisd(xmm3, xmm5);\n+  __ jcc(Assembler::notEqual, L_10f3);\n+  __ jcc(Assembler::noParity, L_111c);\n+\n+  __ bind(L_10f3);\n+  __ movsd(xmm5, ExternalAddress((address)CONST_MAX), rax);\n+  __ ucomisd(xmm5, xmm4);\n+  __ jcc(Assembler::below, L_111c);\n+\/\/     return res;\n+\/\/   }\n+\/\/   \/\/ y is NaN?\n+\/\/   if (!(b <= DP_CONST(7ff0000000000000))) {\n+  __ movsd(xmm0, ExternalAddress((address)CONST_INF), rax);\n+  __ ucomisd(xmm0, xmm3);\n+  __ jcc(Assembler::aboveEqual, L_112a);\n+\/\/     res = y + y;\n+  __ vaddsd(xmm0, xmm0, xmm1);\n+\/\/     __asm { ldmxcsr DWORD PTR[mxcsr] }\n+  __ ldmxcsr(Address(rsp, 0));\n+  __ jmp(L_11bd);\n+\/\/   {\n+\/\/     res = DP_FNMA(b, q, a);    \/\/ NaN\n+  __ bind(L_111c);\n+  __ vfnmadd213sd(xmm0, xmm3, xmm4);\n+\/\/     __asm { ldmxcsr DWORD PTR[mxcsr] }\n+  __ ldmxcsr(Address(rsp, 0));\n+  __ jmp(L_11bd);\n+\/\/     return res;\n+\/\/   }\n+\n+\/\/   \/\/ b* 2*1023\n+\/\/   bs = b * DP_CONST(7fe0000000000000);\n+  __ bind(L_112a);\n+  __ vmulsd(xmm1, xmm3, ExternalAddress((address)CONST_e307), rax);\n+\n+\/\/   q = DP_DIV(a, bs);\n+  __ vdivsd(xmm0, xmm4, xmm1);\n+\/\/   q = DP_TRUNC(q);\n+  __ vroundsd(xmm0, xmm0, xmm0, 3);\n+\n+\/\/   eq = TRANSFER_HIGH_INT32(q);\n+  __ extractps(rax, xmm0, 1);\n+\n+\/\/   if (eq >= 0x7fefffffu)\n+  __ cmpl(rax, 0x7fefffff);\n+  __ jcc(Assembler::below, L_116e);\n+\/\/   {\n+\/\/     \/\/ b* 2*1023 * 2^1023\n+\/\/     bs2 = bs * DP_CONST(7fe0000000000000);\n+  __ vmulsd(xmm0, xmm1, ExternalAddress((address)CONST_e307), rax);\n+\/\/     while (bs2 <= a)\n+  __ ucomisd(xmm4, xmm0);\n+  __ jcc(Assembler::below, L_1173);\n+\/\/     {\n+\/\/       q = DP_DIV(a, bs2);\n+  __ bind(L_1157);\n+  __ vdivsd(xmm5, xmm4, xmm0);\n+\/\/       q = DP_TRUNC(q);\n+  __ vroundsd(xmm5, xmm5, xmm5, 3);\n+\/\/       a = DP_FNMA(bs2, q, a);\n+  __ vfnmadd231sd(xmm4, xmm0, xmm5);\n+\/\/     while (bs2 <= a)\n+  __ ucomisd(xmm4, xmm0);\n+  __ jcc(Assembler::aboveEqual, L_1157);\n+  __ jmp(L_1173);\n+\/\/     }\n+\/\/   }\n+\/\/   else\n+\/\/   a = DP_FNMA(bs, q, a);\n+  __ bind(L_116e);\n+  __ vfnmadd231sd(xmm4, xmm1, xmm0);\n+\n+\/\/   while (bs <= a)\n+  __ bind(L_1173);\n+  __ ucomisd(xmm4, xmm1);\n+  __ jcc(Assembler::aboveEqual, L_117f);\n+  __ movapd(xmm0, xmm4);\n+  __ jmp(L_11af);\n+\/\/   {\n+\/\/     q = DP_DIV(a, bs);\n+  __ bind(L_117f);\n+  __ vdivsd(xmm0, xmm4, xmm1);\n+\/\/     q = DP_TRUNC(q);\n+  __ vroundsd(xmm0, xmm0, xmm0, 3);\n+\/\/     a = DP_FNMA(bs, q, a);\n+  __ vfnmadd213sd(xmm0, xmm1, xmm4);\n+\n+\/\/   while (bs <= a)\n+  __ ucomisd(xmm0, xmm1);\n+  __ movapd(xmm4, xmm0);\n+  __ jcc(Assembler::aboveEqual, L_117f);\n+  __ jmp(L_11af);\n+  __ align32();\n+\/\/   {\n+\/\/     q = DP_DIV(a, b);\n+  __ bind(L_11a0);\n+  __ vdivsd(xmm1, xmm0, xmm3);\n+\/\/     q = DP_TRUNC(q);\n+  __ vroundsd(xmm1, xmm1, xmm1, 3);\n+\/\/     a = DP_FNMA(b, q, a);\n+  __ vfnmadd231sd(xmm0, xmm3, xmm1);\n+\n+\/\/ FMOD_CONT:\n+\/\/   while (b <= a)\n+  __ bind(L_11af);\n+  __ ucomisd(xmm0, xmm3);\n+  __ jcc(Assembler::aboveEqual, L_11a0);\n+\/\/   }\n+\n+\/\/   __asm { ldmxcsr DWORD PTR[mxcsr] }\n+  __ ldmxcsr(Address(rsp, 0));\n+  __ bind(L_11b9);\n+  __ vpxor(xmm0, xmm2, xmm0, Assembler::AVX_128bit);\n+\/\/   }\n+\n+\/\/   goto FMOD_CONT;\n+\n+\/\/ }\n+  __ bind(L_11bd);\n+  __ pop(rax);\n+\n+  } else {                                       \/\/ SSE version\n+    assert(false, \"SSE not implemented\");\n+  }\n+\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n+\n+  return start;\n+}\n+\n+#undef __\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64_fmod.cpp","additions":779,"deletions":0,"binary":false,"changes":779,"status":"added"},{"patch":"@@ -164,0 +164,1 @@\n+address StubRoutines::_fmod = nullptr;\n","filename":"src\/hotspot\/share\/runtime\/stubRoutines.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -252,0 +252,1 @@\n+  static address _fmod;\n@@ -428,0 +429,1 @@\n+  static address fmod()                { return _fmod; }\n","filename":"src\/hotspot\/share\/runtime\/stubRoutines.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -559,0 +559,1 @@\n+     static_field(StubRoutines,                _fmod,                                         address)                               \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"}]}