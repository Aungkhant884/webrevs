{"files":[{"patch":"@@ -2947,17 +2947,0 @@\n-  \/\/ Big-endian 128-bit + 64-bit -> 128-bit addition.\n-  \/\/ Inputs: 128-bits. in is preserved.\n-  \/\/ The least-significant 64-bit word is in the upper dword of each vector.\n-  \/\/ inc (the 64-bit increment) is preserved. Its lower dword must be zero.\n-  \/\/ Output: result\n-  void be_add_128_64(FloatRegister result, FloatRegister in,\n-                     FloatRegister inc, FloatRegister tmp) {\n-    assert_different_registers(result, tmp, inc);\n-\n-    __ addv(result, __ T2D, in, inc);      \/\/ Add inc to the least-significant dword of\n-                                           \/\/ input\n-    __ cm(__ HI, tmp, __ T2D, inc, result);\/\/ Check for result overflowing\n-    __ ext(tmp, __ T16B, tmp, tmp, 0x08);  \/\/ Swap LSD of comparison result to MSD and\n-                                           \/\/ MSD == 0 (must be!) to LSD\n-    __ subv(result, __ T2D, result, tmp);  \/\/ Subtract -1 from MSD if there was an overflow\n-  }\n-\n@@ -3073,1 +3056,1 @@\n-      __ ins(v4, __ S, v5, 2, 2); \/\/ v4 contains { 0, 1 }\n+      __ ins(v4, __ S, v5, 3, 3); \/\/ v4 contains { 0, 0, 0, 1 }\n@@ -3075,8 +3058,5 @@\n-      \/\/ 128-bit big-endian increment\n-      __ ld1(v0, __ T16B, counter);\n-      __ rev64(v16, __ T16B, v0);\n-      be_add_128_64(v16, v16, v4, \/*tmp*\/v5);\n-      __ rev64(v16, __ T16B, v16);\n-      __ st1(v16, __ T16B, counter);\n-      \/\/ Previous counter value is in v0\n-      \/\/ v4 contains { 0, 1 }\n+      __ ld1(v0, __ T16B, counter); \/\/ Load the counter into v0\n+      __ rev32(v16, __ T16B, v0);\n+      __ addv(v16, __ T4S, v16, v4);\n+      __ rev32(v16, __ T16B, v16);\n+      __ st1(v16, __ T16B, counter); \/\/ Save the incremented counter back\n@@ -3114,3 +3094,3 @@\n-        __ rev64(v16, __ T16B, v16);\n-        be_add_128_64(v16, v16, v4, \/*tmp*\/v5);\n-        __ rev64(v16, __ T16B, v16);\n+        __ rev32(v16, __ T16B, v16);\n+        __ addv(v16, __ T4S, v16, v4);\n+        __ rev32(v16, __ T16B, v16);\n@@ -3164,1 +3144,1 @@\n-    __ rev64(v16, __ T16B, v0); \/\/ v16 contains byte-reversed counter\n+    __ rev32(v16, __ T16B, v0); \/\/ v16 contains byte-reversed counter\n@@ -3174,1 +3154,1 @@\n-      __ ins(v8, __ S, v9, 2, 2); \/\/ v8 contains { 0, 1 }\n+      __ ins(v8, __ S, v9, 3, 3); \/\/ v8 contains { 0, 0, 0, 1 }\n@@ -3178,2 +3158,2 @@\n-        __ rev64(v0_ofs, __ T16B, v16);\n-        be_add_128_64(v16, v16, v8, \/*tmp*\/v9);\n+        __ rev32(v0_ofs, __ T16B, v16);\n+        __ addv(v16, __ T4S, v16, v8);\n@@ -3209,1 +3189,1 @@\n-    __ rev64(v16, __ T16B, v16);\n+    __ rev32(v16, __ T16B, v16);\n","filename":"src\/hotspot\/cpu\/aarch64\/stubGenerator_aarch64.cpp","additions":14,"deletions":34,"binary":false,"changes":48,"status":"modified"},{"patch":"@@ -4434,8 +4434,0 @@\n-void Assembler::evpcmpuq(KRegister kdst, XMMRegister nds, XMMRegister src, ComparisonPredicate vcc, int vector_len) {\n-  assert(VM_Version::supports_avx512vl(), \"\");\n-  InstructionAttr attributes(vector_len, \/* rex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n-  attributes.set_is_evex_instruction();\n-  int encode = vex_prefix_and_encode(kdst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &attributes);\n-  emit_int24(0x1E, (0xC0 | encode), vcc);\n-}\n-\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.cpp","additions":0,"deletions":8,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -1809,2 +1809,0 @@\n-  void evpcmpuq(KRegister kdst, XMMRegister nds, XMMRegister src, ComparisonPredicate vcc, int vector_len);\n-\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.hpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -9260,11 +9260,0 @@\n-void MacroAssembler::evpaddq(XMMRegister dst, KRegister mask, XMMRegister nds, AddressLiteral src, bool merge, int vector_len, Register rscratch) {\n-  assert(rscratch != noreg || always_reachable(src), \"missing\");\n-\n-  if (reachable(src)) {\n-    Assembler::evpaddq(dst, mask, nds, as_Address(src), merge, vector_len);\n-  } else {\n-    lea(rscratch, src);\n-    Assembler::evpaddq(dst, mask, nds, Address(rscratch, 0), merge, vector_len);\n-  }\n-}\n-\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":0,"deletions":11,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -1791,3 +1791,0 @@\n-  using Assembler::evpaddq;\n-  void evpaddq(XMMRegister dst, KRegister mask, XMMRegister nds, AddressLiteral src, bool merge, int vector_len, Register rscratch = noreg);\n-\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -367,2 +367,1 @@\n-  void ev_add128(XMMRegister xmmdst, XMMRegister xmmsrc1, XMMRegister xmmsrc2,\n-                 int vector_len, KRegister ktmp, Register rscratch = noreg);\n+\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.hpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -124,10 +124,0 @@\n-ATTRIBUTE_ALIGNED(64) uint64_t COUNTER_MASK_ONES[] = {\n-    0x0000000000000000UL, 0x0000000000000001UL,\n-    0x0000000000000000UL, 0x0000000000000001UL,\n-    0x0000000000000000UL, 0x0000000000000001UL,\n-    0x0000000000000000UL, 0x0000000000000001UL,\n-};\n-static address counter_mask_ones_addr() {\n-  return (address)COUNTER_MASK_ONES;\n-}\n-\n@@ -1636,11 +1626,0 @@\n-\/\/ Add 128-bit integers in xmmsrc1 to xmmsrc2, then place the result in xmmdst.\n-\/\/ Clobber ktmp and rscratch.\n-\/\/ Used by aesctr_encrypt.\n-void StubGenerator::ev_add128(XMMRegister xmmdst, XMMRegister xmmsrc1, XMMRegister xmmsrc2,\n-                            int vector_len, KRegister ktmp, Register rscratch) {\n-  __ vpaddq(xmmdst, xmmsrc1, xmmsrc2, vector_len);\n-  __ evpcmpuq(ktmp, xmmdst, xmmsrc2, __ lt, vector_len);\n-  __ kshiftlbl(ktmp, ktmp, 1);\n-  __ evpaddq(xmmdst, ktmp, xmmdst, ExternalAddress(counter_mask_ones_addr()), \/*merge*\/true,\n-             vector_len, rscratch);\n-}\n@@ -2070,0 +2049,1 @@\n+\n@@ -2127,11 +2107,8 @@\n-\n-  __ evmovdquq(xmm19, ExternalAddress(counter_mask_linc0_addr()), Assembler::AVX_512bit, r15 \/*rscratch*\/);\n-  ev_add128(xmm8, xmm8, xmm19, Assembler::AVX_512bit, \/*ktmp*\/k1, r15 \/*rscratch*\/);\n-  __ evmovdquq(xmm19, ExternalAddress(counter_mask_linc4_addr()), Assembler::AVX_512bit, r15 \/*rscratch*\/);\n-  ev_add128(xmm9,  xmm8,  xmm19, Assembler::AVX_512bit, \/*ktmp*\/k1, r15 \/*rscratch*\/);\n-  ev_add128(xmm10, xmm9,  xmm19, Assembler::AVX_512bit, \/*ktmp*\/k1, r15 \/*rscratch*\/);\n-  ev_add128(xmm11, xmm10, xmm19, Assembler::AVX_512bit, \/*ktmp*\/k1, r15 \/*rscratch*\/);\n-  ev_add128(xmm12, xmm11, xmm19, Assembler::AVX_512bit, \/*ktmp*\/k1, r15 \/*rscratch*\/);\n-  ev_add128(xmm13, xmm12, xmm19, Assembler::AVX_512bit, \/*ktmp*\/k1, r15 \/*rscratch*\/);\n-  ev_add128(xmm14, xmm13, xmm19, Assembler::AVX_512bit, \/*ktmp*\/k1, r15 \/*rscratch*\/);\n-  ev_add128(xmm15, xmm14, xmm19, Assembler::AVX_512bit, \/*ktmp*\/k1, r15 \/*rscratch*\/);\n+  __ vpaddd(xmm8,  xmm8,  ExternalAddress(counter_mask_linc0_addr()), Assembler::AVX_512bit, r15 \/*rscratch*\/);\n+  __ vpaddd(xmm9,  xmm8,  ExternalAddress(counter_mask_linc4_addr()), Assembler::AVX_512bit, r15 \/*rscratch*\/);\n+  __ vpaddd(xmm10, xmm9,  ExternalAddress(counter_mask_linc4_addr()), Assembler::AVX_512bit, r15 \/*rscratch*\/);\n+  __ vpaddd(xmm11, xmm10, ExternalAddress(counter_mask_linc4_addr()), Assembler::AVX_512bit, r15 \/*rscratch*\/);\n+  __ vpaddd(xmm12, xmm11, ExternalAddress(counter_mask_linc4_addr()), Assembler::AVX_512bit, r15 \/*rscratch*\/);\n+  __ vpaddd(xmm13, xmm12, ExternalAddress(counter_mask_linc4_addr()), Assembler::AVX_512bit, r15 \/*rscratch*\/);\n+  __ vpaddd(xmm14, xmm13, ExternalAddress(counter_mask_linc4_addr()), Assembler::AVX_512bit, r15 \/*rscratch*\/);\n+  __ vpaddd(xmm15, xmm14, ExternalAddress(counter_mask_linc4_addr()), Assembler::AVX_512bit, r15 \/*rscratch*\/);\n@@ -2185,1 +2162,1 @@\n-  ev_add128(xmm8,   xmm8, xmm19, Assembler::AVX_512bit, \/*ktmp*\/k1, r15 \/*rscratch*\/);\n+  __ vpaddq(xmm8, xmm8, xmm19, Assembler::AVX_512bit);\n@@ -2187,1 +2164,1 @@\n-  ev_add128(xmm9,   xmm9, xmm19, Assembler::AVX_512bit, \/*ktmp*\/k1, r15 \/*rscratch*\/);\n+  __ vpaddq(xmm9, xmm9, xmm19, Assembler::AVX_512bit);\n@@ -2189,1 +2166,1 @@\n-  ev_add128(xmm10, xmm10, xmm19, Assembler::AVX_512bit, \/*ktmp*\/k1, r15 \/*rscratch*\/);\n+  __ vpaddq(xmm10, xmm10, xmm19, Assembler::AVX_512bit);\n@@ -2191,1 +2168,1 @@\n-  ev_add128(xmm11, xmm11, xmm19, Assembler::AVX_512bit, \/*ktmp*\/k1, r15 \/*rscratch*\/);\n+  __ vpaddq(xmm11, xmm11, xmm19, Assembler::AVX_512bit);\n@@ -2193,1 +2170,1 @@\n-  ev_add128(xmm12, xmm12, xmm19, Assembler::AVX_512bit, \/*ktmp*\/k1, r15 \/*rscratch*\/);\n+  __ vpaddq(xmm12, xmm12, xmm19, Assembler::AVX_512bit);\n@@ -2195,1 +2172,1 @@\n-  ev_add128(xmm13, xmm13, xmm19, Assembler::AVX_512bit, \/*ktmp*\/k1, r15 \/*rscratch*\/);\n+  __ vpaddq(xmm13, xmm13, xmm19, Assembler::AVX_512bit);\n@@ -2197,1 +2174,1 @@\n-  ev_add128(xmm14, xmm14, xmm19, Assembler::AVX_512bit, \/*ktmp*\/k1, r15 \/*rscratch*\/);\n+  __ vpaddq(xmm14, xmm14, xmm19, Assembler::AVX_512bit);\n@@ -2199,1 +2176,1 @@\n-  ev_add128(xmm15, xmm15, xmm19, Assembler::AVX_512bit, \/*ktmp*\/k1, r15 \/*rscratch*\/);\n+  __ vpaddq(xmm15, xmm15, xmm19, Assembler::AVX_512bit);\n@@ -2277,2 +2254,2 @@\n-  ev_add128(xmm8, xmm8, xmm19, Assembler::AVX_512bit, \/*ktmp*\/k1, r15 \/*rscratch*\/);\n-  ev_add128(xmm9, xmm9, xmm19, Assembler::AVX_512bit, \/*ktmp*\/k1, r15 \/*rscratch*\/);\n+  __ vpaddq(xmm8, xmm8, xmm19, Assembler::AVX_512bit);\n+  __ vpaddq(xmm9, xmm9, xmm19, Assembler::AVX_512bit);\n@@ -2345,1 +2322,1 @@\n-  ev_add128(xmm8, xmm8, xmm19, Assembler::AVX_512bit, \/*ktmp*\/k1, r15 \/*rscratch*\/);\n+  __ vpaddq(xmm8, xmm8, xmm19, Assembler::AVX_512bit);\n@@ -2402,1 +2379,0 @@\n-\n@@ -2404,1 +2380,1 @@\n-  ev_add128(xmm8, xmm8, xmm19, Assembler::AVX_512bit, \/*ktmp*\/k1, r15 \/*rscratch*\/);\n+  __ vpaddq(xmm8, xmm8, xmm19, Assembler::AVX_512bit);\n@@ -2454,1 +2430,1 @@\n-  ev_add128(xmm8, xmm8, xmm19, Assembler::AVX_128bit, \/*ktmp*\/k1, r15 \/*rscratch*\/);\n+  __ vpaddq(xmm8, xmm8, xmm19, Assembler::AVX_128bit);\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64_aes.cpp","additions":22,"deletions":46,"binary":false,"changes":68,"status":"modified"}]}