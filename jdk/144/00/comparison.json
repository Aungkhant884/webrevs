{"files":[{"patch":"@@ -2592,0 +2592,32 @@\n+void Assembler::evmovdqu(XMMRegister dst, KRegister mask, Address src, int vector_len, int type) {\n+  assert(VM_Version::supports_avx512vlbw(), \"\");\n+  InstructionMark im(this);\n+  bool wide = type == T_SHORT || type == T_LONG || type == T_CHAR;\n+  bool bwinstr = type == T_BYTE ||  type == T_SHORT || type == T_CHAR;\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ wide, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FVM, \/* input_size_in_bits *\/ EVEX_NObit);\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  attributes.set_is_evex_instruction();\n+  int prefix = bwinstr ? VEX_SIMD_F2 : VEX_SIMD_F3;\n+  vex_prefix(src, 0, dst->encoding(), (Assembler::VexSimdPrefix)prefix, VEX_OPCODE_0F, &attributes);\n+  emit_int8(0x6F);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evmovdqu(Address dst, KRegister mask, XMMRegister src, int vector_len, int type) {\n+  assert(VM_Version::supports_avx512vlbw(), \"\");\n+  assert(src != xnoreg, \"sanity\");\n+  InstructionMark im(this);\n+  bool wide = type == T_SHORT || type == T_LONG || type == T_CHAR;\n+  bool bwinstr = type == T_BYTE ||  type == T_SHORT || type == T_CHAR;\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ wide, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FVM, \/* input_size_in_bits *\/ EVEX_NObit);\n+  attributes.reset_is_clear_context();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  attributes.set_is_evex_instruction();\n+  int prefix = bwinstr ? VEX_SIMD_F2 : VEX_SIMD_F3;\n+  vex_prefix(dst, 0, src->encoding(), (Assembler::VexSimdPrefix)prefix, VEX_OPCODE_0F, &attributes);\n+  emit_int8(0x7F);\n+  emit_operand(src, dst);\n+}\n+\n@@ -7806,0 +7838,7 @@\n+void Assembler::shrxq(Register dst, Register src1, Register src2) {\n+  assert(VM_Version::supports_bmi2(), \"\");\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ true, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src2->encoding(), src1->encoding(), VEX_SIMD_F2, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16((unsigned char)0xF7, (0xC0 | encode));\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.cpp","additions":39,"deletions":0,"binary":false,"changes":39,"status":"modified"},{"patch":"@@ -829,1 +829,0 @@\n-  void decq(Register dst);\n@@ -914,0 +913,1 @@\n+  void decq(Register dst);\n@@ -1522,0 +1522,4 @@\n+  \/\/ Generic move instructions.\n+  void evmovdqu(Address dst, KRegister mask, XMMRegister src, int vector_len, int type);\n+  void evmovdqu(XMMRegister dst, KRegister mask, Address src, int vector_len, int type);\n+\n@@ -2024,0 +2028,2 @@\n+  void shrxq(Register dst, Register src1, Register src2);\n+\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.hpp","additions":7,"deletions":1,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -1221,0 +1221,14 @@\n+\n+void C2_MacroAssembler::genmask(Register dst, Register len, Register temp) {\n+  if (ArrayCopyPartialInlineSize <= 32) {\n+    mov64(dst, 1);\n+    shlxq(dst, dst, len);\n+    decq(dst);\n+  } else {\n+    mov64(dst, -1);\n+    movq(temp, len);\n+    negptr(temp);\n+    addptr(temp, 64);\n+    shrxq(dst, dst, temp);\n+  }\n+}\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":14,"deletions":0,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -96,0 +96,1 @@\n+  void genmask(Register dst, Register len, Register temp);\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -764,0 +764,2 @@\n+      _features &= ~CPU_AVX512BW;\n+      _features &= ~CPU_AVX512VL;\n@@ -1165,1 +1167,1 @@\n-    if (!is_power_of_2(AVX3Threshold)) {\n+    if (AVX3Threshold !=0 && !is_power_of_2(AVX3Threshold)) {\n@@ -1414,0 +1416,23 @@\n+\n+      if (FLAG_IS_DEFAULT(ArrayCopyPartialInlineSize) ||\n+          (!FLAG_IS_DEFAULT(ArrayCopyPartialInlineSize) &&\n+           ArrayCopyPartialInlineSize != 0 &&\n+           ArrayCopyPartialInlineSize != 32 &&\n+           ArrayCopyPartialInlineSize != 64)) {\n+        int pi_size = 0;\n+        if (MaxVectorSize > 32 && AVX3Threshold == 0) {\n+          pi_size = 64;\n+        } else if (MaxVectorSize >= 32) {\n+          pi_size = 32;\n+        }\n+        if(!FLAG_IS_DEFAULT(ArrayCopyPartialInlineSize)) {\n+          warning(\"Setting ArrayCopyPartialInlineSize as %d\", pi_size);\n+        }\n+        ArrayCopyPartialInlineSize = pi_size;\n+      }\n+\n+      if (ArrayCopyPartialInlineSize > MaxVectorSize) {\n+        ArrayCopyPartialInlineSize = MaxVectorSize;\n+        warning(\"Setting ArrayCopyPartialInlineSize as MaxVectorSize\");\n+      }\n+\n","filename":"src\/hotspot\/cpu\/x86\/vm_version_x86.cpp","additions":26,"deletions":1,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -1408,0 +1408,7 @@\n+    case Op_VectorMaskGen:\n+    case Op_VectorMaskedLoad:\n+    case Op_VectorMaskedStore:\n+      if (UseAVX < 3) {\n+        return false;\n+      }\n+      break;\n@@ -1480,0 +1487,10 @@\n+    case Op_VectorMaskGen:\n+    case Op_VectorMaskedLoad:\n+    case Op_VectorMaskedStore:\n+      if (!VM_Version::supports_avx512bw()) {\n+        return false;\n+      }\n+      if ((size_in_bits != 512) && !VM_Version::supports_avx512vl()) {\n+        return false;\n+      }\n+      break;\n@@ -5447,0 +5464,49 @@\n+#ifdef _LP64\n+\/\/ ---------------------------------- Masked Block Copy ------------------------------------\n+\n+instruct vmasked_load64(vec dst, memory mem, rRegL mask) %{\n+  match(Set dst (VectorMaskedLoad mem mask));\n+  format %{ \"vector_masked_load $dst, $mem, $mask \\t! vector masked copy\" %}\n+  ins_encode %{\n+    BasicType elmType =  this->bottom_type()->is_vect()->element_basic_type();\n+    int vector_len = vector_length_encoding(this);\n+    \/\/TODO: KRegister to be made valid \"bound\" operand to promote sharing.\n+    __ kmovql(k2, $mask$$Register);\n+    __ evmovdqu($dst$$XMMRegister, k2, $mem$$Address, vector_len, elmType);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vmask_gen(rRegL dst, rRegL len, rRegL tempLen) %{\n+  match(Set dst (VectorMaskGen len));\n+  effect(TEMP_DEF dst, TEMP tempLen);\n+  format %{ \"vector_mask_gen $len \\t! vector mask generator\" %}\n+  ins_encode %{\n+    __ genmask($dst$$Register, $len$$Register, $tempLen$$Register);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vmask_gen_imm(rRegL dst, immL len) %{\n+  match(Set dst (VectorMaskGen len));\n+  format %{ \"vector_mask_gen $len \\t! vector mask generator\" %}\n+  ins_encode %{\n+    __ mov64($dst$$Register, (1L << ($len$$constant & 63)) -1);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vmasked_store64(memory mem, vec src, rRegL mask) %{\n+  match(Set mem (VectorMaskedStore mem (Binary src mask)));\n+  format %{ \"vector_masked_store $mem, $src, $mask \\t! vector masked store\" %}\n+  ins_encode %{\n+    const MachNode* src_node = static_cast<const MachNode*>(this->in(this->operand_index($src)));\n+    BasicType elmType =  src_node->bottom_type()->is_vect()->element_basic_type();\n+    int vector_len = vector_length_encoding(src_node);\n+    \/\/TODO: KRegister to be made valid \"bound\" operand to promote sharing.\n+    __ kmovql(k2, $mask$$Register);\n+    __ evmovdqu($mem$$Address, k2, $src$$XMMRegister, vector_len, elmType);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+#endif \/\/ _LP64\n","filename":"src\/hotspot\/cpu\/x86\/x86.ad","additions":66,"deletions":0,"binary":false,"changes":66,"status":"modified"},{"patch":"@@ -271,0 +271,1 @@\n+  if( strcmp(opType,\"VectorMaskedLoad\")==0 )  return Form::idealV;\n@@ -287,0 +288,1 @@\n+  if( strcmp(opType,\"VectorMaskedStore\")==0 )  return Form::idealV;\n","filename":"src\/hotspot\/share\/adlc\/forms.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -782,0 +782,1 @@\n+       !strcmp(_matrule->_rChild->_opType,\"VectorMaskGen\")||\n@@ -3487,1 +3488,1 @@\n-    \"StoreVector\", \"LoadVector\",\n+    \"StoreVector\", \"LoadVector\", \"VectorMaskedLoad\", \"VectorMaskedStore\",\n@@ -4171,2 +4172,2 @@\n-    \"RoundDoubleModeV\",\"RotateLeftV\" , \"RotateRightV\", \"LoadVector\",\"StoreVector\",\n-    \"FmaVD\", \"FmaVF\",\"PopCountVI\",\n+    \"RoundDoubleModeV\", \"RotateLeftV\" , \"RotateRightV\", \"LoadVector\",\"StoreVector\",\n+    \"FmaVD\", \"FmaVF\",\"PopCountVI\",\"VectorMaskedLoad\",\"VectorMaskedStore\",\n","filename":"src\/hotspot\/share\/adlc\/formssel.cpp","additions":4,"deletions":3,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -88,0 +88,1 @@\n+\n@@ -673,0 +674,1 @@\n+    PhiNode* phi = NULL;\n@@ -680,0 +682,14 @@\n+        } else if (n != NULL && n->is_Region() &&\n+                   (phi = n->as_Region()->has_phi()) &&\n+                    phi->in(1)->Opcode() == Op_VectorMaskedStore) {\n+          return true;\n+        } else  {\n+          for (DUIterator_Fast imax, i = c->fast_outs(imax); i < imax; i++) {\n+            Node* phi = c->fast_out(i);\n+            if (phi->is_Phi()) {\n+              assert(phi->in(0) == c, \"phi region validation\");\n+              if(phi->in(1) && phi->in(1)->Opcode() == Op_VectorMaskedStore) {\n+                return true;\n+              }\n+            }\n+          }\n@@ -737,0 +753,13 @@\n+\n+\/\/ As an optimization, choose optimum vector size for copy length known at compile time.\n+int ArrayCopyNode::get_partial_inline_vector_lane_count(BasicType type, int con_len) {\n+  int lane_count = ArrayCopyPartialInlineSize\/type2aelembytes(type);\n+  if (con_len > 0) {\n+    int size_in_bytes = con_len * type2aelembytes(type);\n+    if (size_in_bytes <= 16)\n+      lane_count = 16\/type2aelembytes(type);\n+    else if (size_in_bytes > 16 && size_in_bytes <= 32)\n+      lane_count = 32\/type2aelembytes(type);\n+  }\n+  return lane_count;\n+}\n","filename":"src\/hotspot\/share\/opto\/arraycopynode.cpp","additions":29,"deletions":0,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -183,0 +183,3 @@\n+\n+  static int get_partial_inline_vector_lane_count(BasicType type, int con_len);\n+\n","filename":"src\/hotspot\/share\/opto\/arraycopynode.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -75,0 +75,4 @@\n+  product(intx, ArrayCopyPartialInlineSize, -1, DIAGNOSTIC,                 \\\n+          \"Partial inline size used for array copy acceleration.\")          \\\n+          range(-1, 64)                                                     \\\n+                                                                            \\\n","filename":"src\/hotspot\/share\/opto\/c2_globals.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -374,0 +374,57 @@\n+bool RegionNode::is_self_loop(Node* n) {\n+  Node_List nstack;\n+  VectorSet visited;\n+  nstack.push(n);\n+  visited.set(n->_idx);\n+\n+  while(nstack.size()) {\n+    n = nstack.pop();\n+    for(unsigned i = 1; i < n->req() ; i++) {\n+      Node* in = n->in(i);\n+      if (in == NULL) {\n+        continue;\n+      }\n+      if (visited.test_set(in->_idx)) {\n+        return true;\n+      } else {\n+        nstack.push(in);\n+        visited.set(in->_idx);\n+      }\n+    }\n+  }\n+\n+  return false;\n+}\n+\n+\/\/ If a two input non-loop region has dead input\n+\/\/ edge[s] degenerate any phi node contained within it.\n+bool RegionNode::try_phi_disintegration(PhaseGVN *phase) {\n+  if (req() != 3 || isa_Loop() || !in(1) || !in(2) ||\n+       (!in(1)->is_top() && !in(2)->is_top())) {\n+     return false;\n+  }\n+\n+  PhiNode* phi = has_unique_phi();\n+  if (!phi) {\n+    return false;\n+  }\n+\n+  Node* rep_node = NULL;\n+  PhaseIterGVN *igvn = phase->is_IterGVN();\n+  if (in(1)->is_top() && !in(2)->is_top()) {\n+    rep_node = phi->in(2);\n+  } else if(in(2)->is_top() && !in(1)->is_top()) {\n+    rep_node = phi->in(1);\n+  } else {\n+    rep_node = phase->C->top();\n+  }\n+\n+  \/\/ Safety check to avoid dead\/self loop creation.\n+  if (is_self_loop(rep_node)) {\n+    return false;\n+  }\n+\n+  igvn->replace_node(phi, rep_node);\n+  return true;\n+}\n+\n@@ -432,0 +489,4 @@\n+    has_phis = (has_phi() != NULL);       \/\/ Cache result\n+    if (has_phis && try_phi_disintegration(phase)) {\n+      has_phis = false;\n+    }\n","filename":"src\/hotspot\/share\/opto\/cfgnode.cpp","additions":61,"deletions":0,"binary":false,"changes":61,"status":"modified"},{"patch":"@@ -98,0 +98,1 @@\n+  bool is_self_loop(Node* n);\n@@ -99,0 +100,1 @@\n+  bool try_phi_disintegration(PhaseGVN *phase);\n","filename":"src\/hotspot\/share\/opto\/cfgnode.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -399,0 +399,3 @@\n+macro(VectorMaskedLoad)\n+macro(VectorMaskedStore)\n+macro(VectorMaskGen)\n","filename":"src\/hotspot\/share\/opto\/classes.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -3335,0 +3335,3 @@\n+  case Op_VectorMaskGen:\n+  case Op_VectorMaskedLoad:\n+  case Op_VectorMaskedStore:\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -689,0 +689,1 @@\n+        case Op_VectorMaskedStore:\n","filename":"src\/hotspot\/share\/opto\/lcm.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -283,2 +283,4 @@\n-          assert(ac != NULL && ac->is_clonebasic(), \"Only basic clone is a non escaping clone\");\n-          return ac;\n+          if (ac != NULL) {\n+            assert(ac->is_clonebasic(), \"Only basic clone is a non escaping clone\");\n+            return ac;\n+          }\n","filename":"src\/hotspot\/share\/opto\/macro.cpp","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -129,0 +129,5 @@\n+\n+  bool generate_partial_inlining_block(Node** ctrl, MergeMemNode** mem, const TypePtr* adr_type,\n+                                       RegionNode** exit_block, Node** result_memory, Node* length,\n+                                       Node* src_start, Node* dst_start, BasicType type);\n+\n","filename":"src\/hotspot\/share\/opto\/macro.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"opto\/vectornode.hpp\"\n@@ -172,0 +173,127 @@\n+\/\/\n+\/\/ Partial in-lining handling for smaller conjoint\/disjoint array copies having\n+\/\/ length(in bytes) less than ArrayCopyPartialInlineSize.\n+\/\/  if (length <= ArrayCopyPartialInlineSize) {\n+\/\/    partial_inlining_block:\n+\/\/      mask = Mask_Gen\n+\/\/      vload = VectorMaskedLoad src , mask\n+\/\/      VectorMaskedStore dst, mask, vload\n+\/\/  } else {\n+\/\/    stub_block:\n+\/\/      callstub array_copy\n+\/\/  }\n+\/\/  exit_block:\n+\/\/    Phi = label partial_inlining_block:mem , label stub_block:mem (filled by caller)\n+\/\/    mem = MergeMem (Phi)\n+\/\/    control = stub_block\n+\/\/\n+\/\/  Exit_block and associated phi(memory) are partially initialized for partial_in-lining_block\n+\/\/  edges. Remaining edges for exit_block coming from stub_block are connected by the caller\n+\/\/  post stub nodes creation.\n+\/\/\n+\/\/  Constant copy length operation having size less than ArrayCopyPartialInlineSize prevents\n+\/\/  creation of additional control flow for stub_block and exit_block.\n+\/\/\n+\/\/  Return true to emit subsequent stub calling code.\n+\/\/\n+\n+bool PhaseMacroExpand::generate_partial_inlining_block(Node** ctrl, MergeMemNode** mem, const TypePtr* adr_type,\n+                                                       RegionNode** exit_block, Node** result_memory, Node* length,\n+                                                       Node* src_start, Node* dst_start, BasicType type) {\n+  const TypePtr *src_adr_type = _igvn.type(src_start)->isa_ptr();\n+\n+  Node* orig_mem = *mem;\n+  Node* is_lt64bytes_tp = NULL;\n+  Node* is_lt64bytes_fp = NULL;\n+\n+  int con_len = -1;\n+  const TypeInt* lty = NULL;\n+  uint shift  = exact_log2(type2aelembytes(type));\n+  if (length->Opcode() == Op_ConvI2L && (lty = _igvn.type(length->in(1))->isa_int()) && lty->is_con()) {\n+    con_len = lty->get_con() << shift;\n+  } else if ((lty = _igvn.type(length)->isa_int()) && lty->is_con()) {\n+    con_len = lty->get_con() << shift;\n+  }\n+\n+  \/\/ Return if copy length is greater than partial inline size limit or\n+  \/\/ target does not supports masked load\/stores.\n+  int lane_count = ArrayCopyNode::get_partial_inline_vector_lane_count(type, con_len);\n+  if ( con_len > ArrayCopyPartialInlineSize ||\n+      !Matcher::match_rule_supported_vector(Op_VectorMaskedLoad, lane_count, type)  ||\n+      !Matcher::match_rule_supported_vector(Op_VectorMaskedStore, lane_count, type) ||\n+      !Matcher::match_rule_supported_vector(Op_VectorMaskGen, lane_count, type)) {\n+    return true;\n+  }\n+\n+  Node* inline_block = NULL;\n+  \/\/ Emit length comparison check for non-constant length.\n+  if (con_len < 0) {\n+    Node* copy_bytes = new LShiftXNode(length, intcon(shift));\n+    transform_later(copy_bytes);\n+\n+    Node* cmp_le = new CmpULNode(copy_bytes, longcon(ArrayCopyPartialInlineSize));\n+    transform_later(cmp_le);\n+    Node* bol_le = new BoolNode(cmp_le, BoolTest::le);\n+    transform_later(bol_le);\n+    is_lt64bytes_tp  = generate_guard(ctrl, bol_le, NULL, PROB_FAIR);\n+    is_lt64bytes_fp = *ctrl;\n+\n+    inline_block = new RegionNode(2);\n+    transform_later(inline_block);\n+    inline_block->init_req(1, is_lt64bytes_tp);\n+  } else {\n+    inline_block = *ctrl;\n+  }\n+\n+  Node* mask_gen =  new VectorMaskGenNode(length, TypeLong::LONG, Type::get_const_basic_type(type));\n+  transform_later(mask_gen);\n+\n+  unsigned vec_size = lane_count *  type2aelembytes(type);\n+  if (C->max_vector_size() < vec_size) {\n+    C->set_max_vector_size(vec_size);\n+  }\n+\n+  int alias_idx = C->get_alias_index(src_adr_type);\n+  Node* mm = (*mem)->memory_at(alias_idx);\n+  const TypeVect * vt = TypeVect::make(type, lane_count);\n+  Node* masked_load = new VectorMaskedLoadNode(inline_block, mm, src_start,\n+                                               src_adr_type, vt, mask_gen);\n+  transform_later(masked_load);\n+\n+  mm = (*mem)->memory_at(C->get_alias_index(adr_type));\n+  Node* masked_store = new VectorMaskedStoreNode(inline_block, mm, dst_start,\n+                                                 masked_load, adr_type, mask_gen);\n+  transform_later(masked_store);\n+\n+  \/\/ Stub region is created for non-constant copy length.\n+  if (con_len < 0) {\n+    \/\/ Region containing stub calling node.\n+    RegionNode* stub_block = new RegionNode(2);\n+    transform_later(stub_block);\n+    stub_block->init_req(1, is_lt64bytes_fp);\n+\n+    \/\/ Convergence region for inline_block and stub_block.\n+    *exit_block = new RegionNode(3);\n+    transform_later(*exit_block);\n+    (*exit_block)->init_req(1, masked_store);\n+    *result_memory = new PhiNode(*exit_block, Type::MEMORY, adr_type);\n+    transform_later(*result_memory);\n+    (*result_memory)->init_req(1, masked_store);\n+\n+    *ctrl = stub_block;\n+    return true;\n+  } else {\n+    \/\/ Prevent stub call generation for constant length less\n+    \/\/ than partial inline size.\n+    uint alias_idx = C->get_alias_index(adr_type);\n+    if (alias_idx != Compile::AliasIdxBot) {\n+      *mem = MergeMemNode::make(*mem);\n+      (*mem)->set_memory_at(alias_idx, masked_store);\n+    } else {\n+      *mem = MergeMemNode::make(masked_store);\n+    }\n+    transform_later(*mem);\n+    return false;\n+  }\n+}\n+\n@@ -1078,3 +1206,8 @@\n-  const TypeFunc* call_type = OptoRuntime::fast_arraycopy_Type();\n-  Node* call = make_leaf_call(*ctrl, *mem, call_type, copyfunc_addr, copyfunc_name, adr_type,\n-                              src_start, dest_start, copy_length XTOP);\n+  Node* result_memory = NULL;\n+  RegionNode* exit_block = NULL;\n+  bool gen_stub_call = true;\n+  if (ArrayCopyPartialInlineSize > 0 && is_subword_type(basic_elem_type) &&\n+    Matcher::vector_width_in_bytes(basic_elem_type) >= 32) {\n+    gen_stub_call = generate_partial_inlining_block(ctrl, mem, adr_type, &exit_block, &result_memory,\n+                                                    copy_length, src_start, dest_start, basic_elem_type);\n+  }\n@@ -1082,1 +1215,25 @@\n-  finish_arraycopy_call(call, ctrl, mem, adr_type);\n+  if (gen_stub_call) {\n+    const TypeFunc* call_type = OptoRuntime::fast_arraycopy_Type();\n+    Node* call = make_leaf_call(*ctrl, *mem, call_type, copyfunc_addr, copyfunc_name, adr_type,\n+                                src_start, dest_start, copy_length XTOP);\n+\n+    finish_arraycopy_call(call, ctrl, mem, adr_type);\n+  }\n+\n+  \/\/ Connecting remaining edges for exit_block coming from stub_block.\n+  if (exit_block) {\n+    exit_block->init_req(2, *ctrl);\n+\n+    \/\/ Memory edge corresponding to stub_region.\n+    result_memory->init_req(2, *mem);\n+\n+    uint alias_idx = C->get_alias_index(adr_type);\n+    if (alias_idx != Compile::AliasIdxBot) {\n+      *mem = MergeMemNode::make(*mem);\n+      (*mem)->set_memory_at(alias_idx, result_memory);\n+    } else {\n+      *mem = MergeMemNode::make(result_memory);\n+    }\n+    transform_later(*mem);\n+    *ctrl = exit_block;\n+  }\n","filename":"src\/hotspot\/share\/opto\/macroArrayCopy.cpp","additions":161,"deletions":4,"binary":false,"changes":165,"status":"modified"},{"patch":"@@ -2154,0 +2154,1 @@\n+    case Op_VectorMaskedLoad:\n@@ -2256,0 +2257,6 @@\n+    case Op_VectorMaskedStore: {\n+      Node* pair = new BinaryNode(n->in(3), n->in(4));\n+      n->set_req(3, pair);\n+      n->del_req(4);\n+      break;\n+    }\n","filename":"src\/hotspot\/share\/opto\/matcher.cpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -644,1 +644,2 @@\n-        const int MAX_STORE = BytesPerLong;\n+        const int MAX_STORE = MAX2(BytesPerLong, (int)MaxVectorSize);\n+        assert(mem->as_Store()->memory_size() <= MAX_STORE, \"\");\n@@ -3747,1 +3748,2 @@\n-  const int FAIL = 0, MAX_STORE = BytesPerLong;\n+  const int FAIL = 0, MAX_STORE = MAX2(BytesPerLong, (int)MaxVectorSize);\n+\n","filename":"src\/hotspot\/share\/opto\/memnode.cpp","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -155,0 +155,1 @@\n+class VectorMaskedLoad;\n@@ -156,0 +157,1 @@\n+class VectorMaskedStore;\n@@ -692,0 +694,1 @@\n+         DEFINE_CLASS_ID(VectorMaskedLoad, LoadVector, 0)\n@@ -694,0 +697,1 @@\n+         DEFINE_CLASS_ID(VectorMaskedStore, StoreVector, 0)\n","filename":"src\/hotspot\/share\/opto\/node.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -680,0 +680,50 @@\n+VectorMaskedLoadNode* make(int opc, Node* ctl, Node* mem, Node* src,\n+                       const TypePtr* atype, const TypeVect* vt,\n+                       Node* mask) {\n+  return new VectorMaskedLoadNode(ctl, mem, src, atype, vt, mask);\n+}\n+\n+VectorMaskedStoreNode* make(int opc, Node* ctl, Node* mem, Node* dst,\n+                            Node* src, const TypePtr* atype, Node* mask) {\n+  return new VectorMaskedStoreNode(ctl, mem, dst, src, atype, mask);\n+}\n+\n+VectorMaskGenNode* make(int opc, Node* src, const Type* ty, const Type* ety) {\n+  return new VectorMaskGenNode(src, ty, ety);\n+}\n+\n+Node* VectorMaskedLoadNode::Ideal(PhaseGVN* phase, bool can_reshape) {\n+  Node* mask_len = in(3)->in(1);\n+  const TypeLong* ty = phase->type(mask_len)->isa_long();\n+  if (ty && ty->is_con()) {\n+    BasicType mask_bt = ((VectorMaskGenNode*)in(3))->get_elem_type()->array_element_basic_type();\n+    uint load_sz      = type2aelembytes(mask_bt) * ty->get_con();\n+    if ( load_sz == 32 || load_sz == 64) {\n+      assert(load_sz == 32 || MaxVectorSize > 32, \"Unexpected load size\");\n+      Node* ctr = in(MemNode::Control);\n+      Node* mem = in(MemNode::Memory);\n+      Node* adr = in(MemNode::Address);\n+      return phase->transform(new LoadVectorNode(ctr, mem, adr, adr_type(), vect_type()));\n+    }\n+  }\n+  return NULL;\n+}\n+\n+Node* VectorMaskedStoreNode::Ideal(PhaseGVN* phase, bool can_reshape) {\n+  Node* mask_len = in(4)->in(1);\n+  const TypeLong* ty = phase->type(mask_len)->isa_long();\n+  if (ty && ty->is_con()) {\n+    BasicType mask_bt = ((VectorMaskGenNode*)in(4))->get_elem_type()->array_element_basic_type();\n+    uint load_sz      = type2aelembytes(mask_bt) * ty->get_con();\n+    if ( load_sz == 32 || load_sz == 64) {\n+      assert(load_sz == 32 || MaxVectorSize > 32, \"Unexpected store size\");\n+      Node* ctr = in(MemNode::Control);\n+      Node* mem = in(MemNode::Memory);\n+      Node* adr = in(MemNode::Address);\n+      Node* val = in(MemNode::ValueIn);\n+      return phase->transform(new StoreVectorNode(ctr, mem, adr, adr_type(), val));\n+    }\n+  }\n+  return NULL;\n+}\n+\n","filename":"src\/hotspot\/share\/opto\/vectornode.cpp","additions":50,"deletions":0,"binary":false,"changes":50,"status":"modified"},{"patch":"@@ -752,0 +752,57 @@\n+class VectorMaskedStoreNode : public StoreVectorNode {\n+ public:\n+  VectorMaskedStoreNode(Node* c, Node* mem, Node* dst, Node* src, const TypePtr* at, Node* mask)\n+   : StoreVectorNode(c, mem, dst, at, src) {\n+    assert(mask->bottom_type()->is_long(), \"sanity\");\n+    init_class_id(Class_StoreVector);\n+    set_mismatched_access();\n+    add_req(mask);\n+  }\n+\n+  virtual int Opcode() const;\n+\n+  virtual uint match_edge(uint idx) const {\n+    return idx > 1;\n+  }\n+  Node* Ideal(PhaseGVN* phase, bool can_reshape);\n+\n+  static VectorMaskedStoreNode* make(int opc, Node* ctl, Node* mem, Node* dst, Node* src,\n+                                    const TypePtr* atype, Node* mask);\n+};\n+\n+class VectorMaskedLoadNode : public LoadVectorNode {\n+ public:\n+  VectorMaskedLoadNode(Node* c, Node* mem, Node* src, const TypePtr* at, const TypeVect* vt, Node* mask)\n+   : LoadVectorNode(c, mem, src, at, vt) {\n+    assert(mask->bottom_type()->is_long(), \"sanity\");\n+    init_class_id(Class_LoadVector);\n+    set_mismatched_access();\n+    add_req(mask);\n+  }\n+\n+  virtual int Opcode() const;\n+\n+  virtual uint match_edge(uint idx) const {\n+    return idx > 1;\n+  }\n+  Node* Ideal(PhaseGVN* phase, bool can_reshape);\n+\n+  static VectorMaskedLoadNode* make(int opc, Node* ctl, Node* mem, Node* src,\n+                                const TypePtr* atype, const TypeVect* vt,\n+                                Node* mask);\n+};\n+\n+class VectorMaskGenNode : public TypeNode {\n+ public:\n+  VectorMaskGenNode(Node* src, const Type* ty, const Type* ety): TypeNode(ty, 2), _elemType(ety) {\n+    init_req(1, src);\n+  }\n+\n+  virtual int Opcode() const;\n+  const Type* get_elem_type()  { return _elemType;}\n+\n+  static VectorMaskGenNode* make(int opc, Node* src, const Type* ty, const Type* ety);\n+  private:\n+   const Type* _elemType;\n+};\n+\n","filename":"src\/hotspot\/share\/opto\/vectornode.hpp","additions":57,"deletions":0,"binary":false,"changes":57,"status":"modified"},{"patch":"@@ -87,1 +87,1 @@\n-        assert CodeUtil.isPowerOf2(useAVX3Threshold) : \"AVX3Threshold must be power of 2\";\n+        assert useAVX3Threshold == 0 || CodeUtil.isPowerOf2(useAVX3Threshold) : \"AVX3Threshold must be power of 2\";\n","filename":"src\/jdk.internal.vm.compiler\/share\/classes\/org.graalvm.compiler.lir.amd64\/src\/org\/graalvm\/compiler\/lir\/amd64\/AMD64ArrayCompareToOp.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -74,1 +74,1 @@\n-        assert CodeUtil.isPowerOf2(useAVX3Threshold) : \"AVX3Threshold must be power of 2\";\n+        assert useAVX3Threshold == 0 || CodeUtil.isPowerOf2(useAVX3Threshold) : \"AVX3Threshold must be power of 2\";\n","filename":"src\/jdk.internal.vm.compiler\/share\/classes\/org.graalvm.compiler.lir.amd64\/src\/org\/graalvm\/compiler\/lir\/amd64\/AMD64StringLatin1InflateOp.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -79,1 +79,1 @@\n-        assert CodeUtil.isPowerOf2(useAVX3Threshold) : \"AVX3Threshold must be power of 2\";\n+        assert useAVX3Threshold == 0 || CodeUtil.isPowerOf2(useAVX3Threshold) : \"AVX3Threshold must be power of 2\";\n","filename":"src\/jdk.internal.vm.compiler\/share\/classes\/org.graalvm.compiler.lir.amd64\/src\/org\/graalvm\/compiler\/lir\/amd64\/AMD64StringUTF16CompressOp.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"}]}