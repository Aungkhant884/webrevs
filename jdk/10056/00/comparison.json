{"files":[{"patch":"@@ -313,1 +313,1 @@\n-  bs->nmethod_entry_barrier(this);\n+  bs->nmethod_entry_barrier(this, NULL \/* slow_path *\/, NULL \/* continuation *\/, NULL \/* guard *\/);\n","filename":"src\/hotspot\/cpu\/riscv\/c1_MacroAssembler_riscv.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"opto\/output.hpp\"\n@@ -244,0 +245,29 @@\n+void C2_MacroAssembler::emit_entry_barrier_stub(C2EntryBarrierStub* stub) {\n+  \/\/ make guard value 4-byte aligned so that it can be accessed by atomic instructions on riscv\n+  int alignment_bytes = align(4);\n+\n+  bind(stub->slow_path());\n+\n+  int32_t _offset = 0;\n+  movptr_with_offset(t0, StubRoutines::riscv::method_entry_barrier(), _offset);\n+  jalr(ra, t0, _offset);\n+  j(stub->continuation());\n+\n+  bind(stub->guard());\n+  relocate(entry_guard_Relocation::spec());\n+  assert(offset() % 4 == 0, \"bad alignment\");\n+  emit_int32(0);  \/\/ nmethod guard value\n+  \/\/ make sure the stub with a fixed code size\n+  if (alignment_bytes == 2) {\n+    assert(UseRVC, \"bad alignment\");\n+    c_nop();\n+  } else {\n+    assert(alignment_bytes == 0, \"bad alignment\");\n+    nop();\n+  }\n+}\n+\n+int C2_MacroAssembler::entry_barrier_stub_size() {\n+  return 8 * 4 + 4; \/\/ 4 bytes for alignment margin\n+}\n+\n","filename":"src\/hotspot\/cpu\/riscv\/c2_MacroAssembler_riscv.cpp","additions":30,"deletions":0,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -39,2 +39,2 @@\n-  void emit_entry_barrier_stub(C2EntryBarrierStub* stub) {}\n-  static int entry_barrier_stub_size() { return 0; }\n+  void emit_entry_barrier_stub(C2EntryBarrierStub* stub);\n+  static int entry_barrier_stub_size();\n","filename":"src\/hotspot\/cpu\/riscv\/c2_MacroAssembler_riscv.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -160,4 +160,0 @@\n-  \/\/ save the live input values\n-  RegSet saved = RegSet::of(pre_val);\n-  if (tosca_live) { saved += RegSet::of(x10); }\n-  if (obj != noreg) { saved += RegSet::of(obj); }\n@@ -165,1 +161,1 @@\n-  __ push_reg(saved, sp);\n+  __ push_call_clobbered_registers();\n@@ -174,1 +170,1 @@\n-  __ pop_reg(saved, sp);\n+  __ pop_call_clobbered_registers();\n","filename":"src\/hotspot\/cpu\/riscv\/gc\/g1\/g1BarrierSetAssembler_riscv.cpp","additions":2,"deletions":6,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -181,1 +181,15 @@\n-void BarrierSetAssembler::nmethod_entry_barrier(MacroAssembler* masm) {\n+static volatile uint32_t _patching_epoch = 0;\n+\n+address BarrierSetAssembler::patching_epoch_addr() {\n+  return (address)&_patching_epoch;\n+}\n+\n+void BarrierSetAssembler::increment_patching_epoch() {\n+  Atomic::inc(&_patching_epoch);\n+}\n+\n+void BarrierSetAssembler::clear_patching_epoch() {\n+  _patching_epoch = 0;\n+}\n+\n+void BarrierSetAssembler::nmethod_entry_barrier(MacroAssembler* masm, Label* slow_path, Label* continuation, Label* guard) {\n@@ -188,2 +202,2 @@\n-  \/\/ RISCV atomic operations require that the memory address be naturally aligned.\n-  __ align(4);\n+  Label local_guard;\n+  NMethodPatchingType patching_type = nmethod_patching_type();\n@@ -191,2 +205,2 @@\n-  Label skip, guard;\n-  Address thread_disarmed_addr(xthread, in_bytes(bs_nm->thread_disarmed_offset()));\n+  if (slow_path == NULL) {\n+    guard = &local_guard;\n@@ -194,1 +208,3 @@\n-  __ lwu(t0, guard);\n+    \/\/ RISCV atomic operations require that the memory address be naturally aligned.\n+    __ align(4);\n+  }\n@@ -196,5 +212,47 @@\n-  \/\/ Subsequent loads of oops must occur after load of guard value.\n-  \/\/ BarrierSetNMethod::disarm sets guard with release semantics.\n-  __ membar(MacroAssembler::LoadLoad);\n-  __ lwu(t1, thread_disarmed_addr);\n-  __ beq(t0, t1, skip);\n+  __ lwu(t0, *guard);\n+\n+  switch (patching_type) {\n+    case NMethodPatchingType::conc_data_patch:\n+      \/\/ Subsequent loads of oops must occur after load of guard value.\n+      \/\/ BarrierSetNMethod::disarm sets guard with release semantics.\n+      __ membar(MacroAssembler::LoadLoad); \/\/ fall through to stw_instruction_and_data_patch\n+    case NMethodPatchingType::stw_instruction_and_data_patch:\n+      {\n+        \/\/ With STW patching, no data or instructions are updated concurrently,\n+        \/\/ which means there isn't really any need for any fencing for neither\n+        \/\/ data nor instruction modification happening concurrently. The\n+        \/\/ instruction patching is synchronized with glocal icache_flush() by\n+        \/\/ the write hart on riscv. So here we can do a plain conditional\n+        \/\/ branch with no fencing.\n+        Address thread_disarmed_addr(xthread, in_bytes(bs_nm->thread_disarmed_offset()));\n+        __ lwu(t1, thread_disarmed_addr);\n+        break;\n+      }\n+    case NMethodPatchingType::conc_instruction_and_data_patch:\n+      {\n+        \/\/ If we patch code we need both a code patching and a loadload\n+        \/\/ fence. It's not super cheap, so we use a global epoch mechanism\n+        \/\/ to hide them in a slow path.\n+        \/\/ The high level idea of the global epoch mechanism is to detect\n+        \/\/ when any thread has performed the required fencing, after the\n+        \/\/ last nmethod was disarmed. This implies that the required\n+        \/\/ fencing has been performed for all preceding nmethod disarms\n+        \/\/ as well. Therefore, we do not need any further fencing.\n+        __ la(t1, ExternalAddress((address)&_patching_epoch));\n+        \/\/ Embed an artificial data dependency to order the guard load\n+        \/\/ before the epoch load.\n+        __ srli(ra, t0, 32);\n+        __ orr(t1, t1, ra);\n+        \/\/ Read the global epoch value.\n+        __ lwu(t1, t1);\n+        \/\/ Combine the guard value (low order) with the epoch value (high order).\n+        __ slli(t1, t1, 32);\n+        __ orr(t0, t0, t1);\n+        \/\/ Compare the global values with the thread-local values\n+        Address thread_disarmed_and_epoch_addr(xthread, in_bytes(bs_nm->thread_disarmed_offset()));\n+        __ ld(t1, thread_disarmed_and_epoch_addr);\n+        break;\n+      }\n+    default:\n+      ShouldNotReachHere();\n+  }\n@@ -202,4 +260,3 @@\n-  int32_t offset = 0;\n-  __ movptr_with_offset(t0, StubRoutines::riscv::method_entry_barrier(), offset);\n-  __ jalr(ra, t0, offset);\n-  __ j(skip);\n+  if (slow_path == NULL) {\n+    Label skip_barrier;\n+    __ beq(t0, t1, skip_barrier);\n@@ -207,1 +264,4 @@\n-  __ bind(guard);\n+    int32_t offset = 0;\n+    __ movptr_with_offset(t0, StubRoutines::riscv::method_entry_barrier(), offset);\n+    __ jalr(ra, t0, offset);\n+    __ j(skip_barrier);\n@@ -209,2 +269,1 @@\n-  assert(__ offset() % 4 == 0, \"bad alignment\");\n-  __ emit_int32(0); \/\/ nmethod guard value. Skipped over in common case.\n+    __ bind(local_guard);\n@@ -212,1 +271,8 @@\n-  __ bind(skip);\n+    assert(__ offset() % 4 == 0, \"bad alignment\");\n+    __ emit_int32(0); \/\/ nmethod guard value. Skipped over in common case.\n+    __ bind(skip_barrier);\n+  } else {\n+    __ beq(t0, t1, *continuation);\n+    __ j(*slow_path);\n+    __ bind(*continuation);\n+  }\n","filename":"src\/hotspot\/cpu\/riscv\/gc\/shared\/barrierSetAssembler_riscv.cpp","additions":85,"deletions":19,"binary":false,"changes":104,"status":"modified"},{"patch":"@@ -35,0 +35,6 @@\n+enum class NMethodPatchingType {\n+  stw_instruction_and_data_patch,\n+  conc_instruction_and_data_patch,\n+  conc_data_patch\n+};\n+\n@@ -66,1 +72,3 @@\n-  virtual void nmethod_entry_barrier(MacroAssembler* masm);\n+  virtual NMethodPatchingType nmethod_patching_type() { return NMethodPatchingType::stw_instruction_and_data_patch; }\n+\n+  virtual void nmethod_entry_barrier(MacroAssembler* masm, Label* slow_path, Label* continuation, Label* guard);\n@@ -68,1 +76,4 @@\n-  virtual ~BarrierSetAssembler() {}\n+\n+  static address patching_epoch_addr();\n+  static void clear_patching_epoch();\n+  static void increment_patching_epoch();\n","filename":"src\/hotspot\/cpu\/riscv\/gc\/shared\/barrierSetAssembler_riscv.hpp","additions":13,"deletions":2,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"gc\/shared\/barrierSetAssembler.hpp\"\n@@ -39,0 +40,20 @@\n+static int slow_path_size(nmethod* nm) {\n+  \/\/ The slow path code is out of line with C2.\n+  \/\/ Leave a jal to the stub in the fast path.\n+  return nm->is_compiled_by_c2() ? 1 : 8;\n+}\n+\n+static int entry_barrier_offset(nmethod* nm) {\n+  BarrierSetAssembler* bs_asm = BarrierSet::barrier_set()->barrier_set_assembler();\n+  switch (bs_asm->nmethod_patching_type()) {\n+    case NMethodPatchingType::stw_instruction_and_data_patch:\n+      return -4 * (4 + slow_path_size(nm));\n+    case NMethodPatchingType::conc_data_patch:\n+      return -4 * (5 + slow_path_size(nm));\n+    case NMethodPatchingType::conc_instruction_and_data_patch:\n+      return -4 * (15 + slow_path_size(nm));\n+  }\n+  ShouldNotReachHere();\n+  return 0;\n+}\n+\n@@ -42,3 +63,19 @@\n-  int *guard_addr() {\n-    \/* auipc + lwu + fence + lwu + beq + lui + addi + slli + addi + slli + jalr + j *\/\n-    return reinterpret_cast<int*>(instruction_address() + 12 * 4);\n+  int local_guard_offset(nmethod* nm) {\n+    \/\/ It's the last instruction\n+    return (-entry_barrier_offset(nm)) - 4;\n+  }\n+\n+  int *guard_addr(nmethod* nm) {\n+    if (nm->is_compiled_by_c2()) {\n+      \/\/ With c2 compiled code, the guard is out-of-line in a stub\n+      \/\/ We find it using the RelocIterator.\n+      RelocIterator iter(nm);\n+      while (iter.next()) {\n+        if (iter.type() == relocInfo::entry_guard_type) {\n+          entry_guard_Relocation* const reloc = iter.entry_guard_reloc();\n+          return reinterpret_cast<int*>(reloc->addr());\n+        }\n+      }\n+      ShouldNotReachHere();\n+    }\n+    return reinterpret_cast<int*>(instruction_address() + local_guard_offset(nm));\n@@ -48,2 +85,2 @@\n-  int get_value() {\n-    return Atomic::load_acquire(guard_addr());\n+  int get_value(nmethod* nm) {\n+    return Atomic::load_acquire(guard_addr(nm));\n@@ -52,2 +89,2 @@\n-  void set_value(int value) {\n-    Atomic::release_store(guard_addr(), value);\n+  void set_value(nmethod* nm, int value) {\n+    Atomic::release_store(guard_addr(nm), value);\n@@ -68,11 +105,3 @@\n-  { 0x000fffff, 0x0002e283, \"lwu    t0, 48(t0)      \"},\n-  { 0xffffffff, 0x0aa0000f, \"fence  ir, ir          \"},\n-  { 0x000fffff, 0x000be303, \"lwu    t1, 112(xthread)\"},\n-  { 0x01fff07f, 0x00628063, \"beq    t0, t1, skip    \"},\n-  { 0x00000fff, 0x000002b7, \"lui    t0, imm0        \"},\n-  { 0x000fffff, 0x00028293, \"addi   t0, t0, imm1    \"},\n-  { 0xffffffff, 0x00b29293, \"slli   t0, t0, 11      \"},\n-  { 0x000fffff, 0x00028293, \"addi   t0, t0, imm2    \"},\n-  { 0xffffffff, 0x00629293, \"slli   t0, t0, 6       \"},\n-  { 0x000fffff, 0x000280e7, \"jalr   ra, imm3(t0)    \"},\n-  { 0x00000fff, 0x0000006f, \"j      skip            \"}\n+  { 0x000fffff, 0x0002e283, \"lwu    t0, guard_offset(t0)      \"},\n+  \/* ...... *\/\n+  \/* ...... *\/\n@@ -81,1 +110,0 @@\n-  \/* skip: *\/\n@@ -139,11 +167,0 @@\n-\/\/ This is the offset of the entry barrier from where the frame is completed.\n-\/\/ If any code changes between the end of the verified entry where the entry\n-\/\/ barrier resides, and the completion of the frame, then\n-\/\/ NativeNMethodCmpBarrier::verify() will immediately complain when it does\n-\/\/ not find the expected native instruction at this offset, which needs updating.\n-\/\/ Note that this offset is invariant of PreserveFramePointer.\n-\n-\/\/ see BarrierSetAssembler::nmethod_entry_barrier\n-\/\/ auipc + lwu + fence + lwu + beq + movptr_with_offset(5 instructions) + jalr + j + int32\n-static const int entry_barrier_offset = -4 * 13;\n-\n@@ -151,1 +168,1 @@\n-  address barrier_address = nm->code_begin() + nm->frame_complete_offset() + entry_barrier_offset;\n+  address barrier_address = nm->code_begin() + nm->frame_complete_offset() + entry_barrier_offset(nm);\n@@ -162,0 +179,9 @@\n+  \/\/ The patching code is incremented before the nmethod is disarmed. Disarming\n+  \/\/ is performed with a release store. In the nmethod entry barrier, the values\n+  \/\/ are read in the opposite order, such that the load of the nmethod guard\n+  \/\/ acquires the patching epoch. This way, the guard is guaranteed to block\n+  \/\/ entries to the nmethod, util it has safely published the requirement for\n+  \/\/ further fencing by mutators, before they are allowed to enter.\n+  BarrierSetAssembler* bs_asm = BarrierSet::barrier_set()->barrier_set_assembler();\n+  bs_asm->increment_patching_epoch();\n+\n@@ -163,0 +189,1 @@\n+  \/\/ Symmetric \"LD; FENCE IR, IR\" is in the nmethod barrier.\n@@ -164,2 +191,1 @@\n-\n-  barrier->set_value(disarmed_value());\n+  barrier->set_value(nm, disarmed_value());\n@@ -169,1 +195,17 @@\n-  Unimplemented();\n+  if (!supports_entry_barrier(nm)) {\n+    return;\n+  }\n+\n+  if (arm_value == disarmed_value()) {\n+    \/\/ The patching code is incremented before the nmethod is disarmed. Disarming\n+    \/\/ is performed with a release store. In the nmethod entry barrier, the values\n+    \/\/ are read in the opposite order, such that the load of the nmethod guard\n+    \/\/ acquires the patching epoch. This way, the guard is guaranteed to block\n+    \/\/ entries to the nmethod, util it has safely published the requirement for\n+    \/\/ further fencing by mutators, before they are allowed to enter.\n+    BarrierSetAssembler* bs_asm = BarrierSet::barrier_set()->barrier_set_assembler();\n+    bs_asm->increment_patching_epoch();\n+  }\n+\n+  NativeNMethodBarrier* barrier = native_nmethod_barrier(nm);\n+  barrier->set_value(nm, arm_value);\n@@ -178,1 +220,1 @@\n-  return barrier->get_value() != disarmed_value();\n+  return barrier->get_value(nm) != disarmed_value();\n","filename":"src\/hotspot\/cpu\/riscv\/gc\/shared\/barrierSetNMethod_riscv.cpp","additions":77,"deletions":35,"binary":false,"changes":112,"status":"modified"},{"patch":"@@ -66,0 +66,2 @@\n+  virtual NMethodPatchingType nmethod_patching_type() { return NMethodPatchingType::conc_data_patch; }\n+\n","filename":"src\/hotspot\/cpu\/riscv\/gc\/shenandoah\/shenandoahBarrierSetAssembler_riscv.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -81,0 +81,2 @@\n+  virtual NMethodPatchingType nmethod_patching_type() { return NMethodPatchingType::conc_data_patch; }\n+\n","filename":"src\/hotspot\/cpu\/riscv\/gc\/z\/zBarrierSetAssembler_riscv.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -91,1 +91,1 @@\n-void MacroAssembler::align(int modulus, int extra_offset) {\n+int MacroAssembler::align(int modulus, int extra_offset) {\n@@ -93,0 +93,1 @@\n+  intptr_t before = offset();\n@@ -94,0 +95,1 @@\n+  return (int)(offset() - before);\n@@ -1670,1 +1672,3 @@\n-  if (BarrierSet::barrier_set()->barrier_set_nmethod() != NULL || !immediate) {\n+  \/\/ Using immediate literals would necessitate fence.i.\n+  BarrierSet* bs = BarrierSet::barrier_set();\n+  if ((bs->barrier_set_nmethod() != NULL && bs->barrier_set_assembler()->nmethod_patching_type() == NMethodPatchingType::conc_data_patch) || !immediate) {\n","filename":"src\/hotspot\/cpu\/riscv\/macroAssembler_riscv.cpp","additions":6,"deletions":2,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -50,1 +50,1 @@\n-  void align(int modulus, int extra_offset = 0);\n+  int align(int modulus, int extra_offset = 0);\n","filename":"src\/hotspot\/cpu\/riscv\/macroAssembler_riscv.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1345,1 +1345,18 @@\n-    bs->nmethod_entry_barrier(&_masm);\n+    if (BarrierSet::barrier_set()->barrier_set_nmethod() != NULL) {\n+      \/\/ Dummy labels for just measuring the code size\n+      Label dummy_slow_path;\n+      Label dummy_continuation;\n+      Label dummy_guard;\n+      Label* slow_path = &dummy_slow_path;\n+      Label* continuation = &dummy_continuation;\n+      Label* guard = &dummy_guard;\n+      if (!Compile::current()->output()->in_scratch_emit_size()) {\n+        \/\/ Use real labels from actual stub when not emitting code for purpose of measuring its size\n+        C2EntryBarrierStub* stub = Compile::current()->output()->entry_barrier_table()->add_entry_barrier();\n+        slow_path = &stub->slow_path();\n+        continuation = &stub->continuation();\n+        guard = &stub->guard();\n+      }\n+      \/\/ In the C2 code, we move the non-hot part of nmethod entry barriers out-of-line to a stub.\n+      bs->nmethod_entry_barrier(&_masm, slow_path, continuation, guard);\n+    }\n","filename":"src\/hotspot\/cpu\/riscv\/riscv.ad","additions":18,"deletions":1,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -1328,1 +1328,1 @@\n-  bs->nmethod_entry_barrier(masm);\n+  bs->nmethod_entry_barrier(masm, NULL \/* slow_path *\/, NULL \/* continuation *\/, NULL \/* guard *\/);\n","filename":"src\/hotspot\/cpu\/riscv\/sharedRuntime_riscv.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2344,0 +2344,11 @@\n+    BarrierSetAssembler* bs_asm = BarrierSet::barrier_set()->barrier_set_assembler();\n+\n+    if (bs_asm->nmethod_patching_type() == NMethodPatchingType::conc_instruction_and_data_patch) {\n+      BarrierSetNMethod* bs_nm = BarrierSet::barrier_set()->barrier_set_nmethod();\n+      Address thread_epoch_addr(xthread, in_bytes(bs_nm->thread_disarmed_offset()) + 4);\n+      __ la(t1, ExternalAddress(bs_asm->patching_epoch_addr()));\n+      __ lwu(t1, t1);\n+      __ sw(t1, thread_epoch_addr);\n+      __ membar(__ LoadLoad);\n+    }\n+\n","filename":"src\/hotspot\/cpu\/riscv\/stubGenerator_riscv.cpp","additions":11,"deletions":0,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -133,0 +133,1 @@\n+#if defined(AARCH64) || defined(RISCV)\n@@ -135,1 +136,2 @@\n-  AARCH64_PORT_ONLY(BarrierSetAssembler::clear_patching_epoch());\n+  BarrierSetAssembler::clear_patching_epoch();\n+#endif\n","filename":"src\/hotspot\/share\/gc\/shared\/barrierSetNMethod.cpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -121,1 +121,1 @@\n-  Label _guard; \/\/ Used on AArch64\n+  Label _guard; \/\/ Used on AArch64 and RISCV\n","filename":"src\/hotspot\/share\/opto\/output.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"}]}