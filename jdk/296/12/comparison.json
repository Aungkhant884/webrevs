{"files":[{"patch":"@@ -1969,3 +1969,4 @@\n-    st->print(\"# touch polling page\\n\\t\");\n-    st->print(\"ldr rscratch1, [rthread],#polling_page_offset\\n\\t\");\n-    st->print(\"ldr zr, [rscratch1]\");\n+    st->print(\"# test polling word\\n\\t\");\n+    st->print(\"ldr  rscratch1, [rthread],#%d\\n\\t\", in_bytes(JavaThread::polling_word_offset()));\n+    st->print(\"cmp  sp, rscratch1\\n\\t\");\n+    st->print(\"bhi #slow_path\");\n@@ -1988,1 +1989,7 @@\n-    __ fetch_and_read_polling_page(rscratch1, relocInfo::poll_return_type);\n+    Label dummy_label;\n+    Label* code_stub = &dummy_label;\n+    if (!C->output()->in_scratch_emit_size()) {\n+      code_stub = &C->output()->safepoint_poll_table()->add_safepoint(__ offset());\n+    }\n+    __ relocate(relocInfo::poll_return_type);\n+    __ safepoint_poll(*code_stub, true \/* at_return *\/, false \/* acquire *\/, true \/* in_nmethod *\/);\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":11,"deletions":4,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -41,0 +41,13 @@\n+void C1SafepointPollStub::emit_code(LIR_Assembler* ce) {\n+  __ bind(_entry);\n+  InternalAddress safepoint_pc(ce->masm()->pc() - ce->masm()->offset() + safepoint_offset());\n+  __ adr(rscratch1, safepoint_pc);\n+  __ str(rscratch1, Address(rthread, JavaThread::saved_exception_pc_offset()));\n+\n+  assert(SharedRuntime::polling_page_return_handler_blob() != NULL,\n+         \"polling page return stub not created yet\");\n+  address stub = SharedRuntime::polling_page_return_handler_blob()->entry_point();\n+\n+  __ far_jump(RuntimeAddress(stub));\n+}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_CodeStubs_aarch64.cpp","additions":13,"deletions":0,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -507,1 +507,1 @@\n-void LIR_Assembler::return_op(LIR_Opr result) {\n+void LIR_Assembler::return_op(LIR_Opr result, C1SafepointPollStub* code_stub) {\n@@ -517,1 +517,3 @@\n-  __ fetch_and_read_polling_page(rscratch1, relocInfo::poll_return_type);\n+  code_stub->set_safepoint_offset(__ offset());\n+  __ relocate(relocInfo::poll_return_type);\n+  __ safepoint_poll(*code_stub->entry(), true \/* at_return *\/, false \/* acquire *\/, true \/* in_nmethod *\/);\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIRAssembler_aarch64.cpp","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -0,0 +1,46 @@\n+\/*\n+ * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"asm\/macroAssembler.hpp\"\n+#include \"opto\/compile.hpp\"\n+#include \"opto\/node.hpp\"\n+#include \"opto\/output.hpp\"\n+#include \"runtime\/sharedRuntime.hpp\"\n+\n+#define __ masm.\n+void C2SafepointPollStubTable::emit_stub_impl(MacroAssembler& masm, C2SafepointPollStub* entry) const {\n+  assert(SharedRuntime::polling_page_return_handler_blob() != NULL,\n+         \"polling page return stub not created yet\");\n+  address stub = SharedRuntime::polling_page_return_handler_blob()->entry_point();\n+\n+  RuntimeAddress callback_addr(stub);\n+\n+  __ bind(entry->_stub_label);\n+  InternalAddress safepoint_pc(masm.pc() - masm.offset() + entry->_safepoint_offset);\n+  __ adr(rscratch1, safepoint_pc);\n+  __ str(rscratch1, Address(rthread, JavaThread::saved_exception_pc_offset()));\n+  __ far_jump(callback_addr);\n+}\n+#undef __\n","filename":"src\/hotspot\/cpu\/aarch64\/c2_safepointPollStubTable_aarch64.cpp","additions":46,"deletions":0,"binary":false,"changes":46,"status":"added"},{"patch":"@@ -40,0 +40,1 @@\n+#include \"runtime\/stackWatermarkSet.hpp\"\n@@ -479,2 +480,2 @@\n-\/\/ frame::sender\n-frame frame::sender(RegisterMap* map) const {\n+\/\/ frame::sender_raw\n+frame frame::sender_raw(RegisterMap* map) const {\n@@ -502,0 +503,10 @@\n+frame frame::sender(RegisterMap* map) const {\n+  frame result = sender_raw(map);\n+\n+  if (map->process_frames()) {\n+    StackWatermarkSet::on_iteration(map->thread(), result);\n+  }\n+\n+  return result;\n+}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/frame_aarch64.cpp","additions":13,"deletions":2,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -164,0 +164,3 @@\n+  \/\/ returns the sending frame, without applying any barriers\n+  frame sender_raw(RegisterMap* map) const;\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/frame_aarch64.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -27,4 +27,3 @@\n-const size_t ZPlatformGranuleSizeShift      = 21; \/\/ 2MB\n-const size_t ZPlatformHeapViews             = 3;\n-const size_t ZPlatformNMethodDisarmedOffset = 4;\n-const size_t ZPlatformCacheLineSize         = 64;\n+const size_t ZPlatformGranuleSizeShift = 21; \/\/ 2MB\n+const size_t ZPlatformHeapViews        = 3;\n+const size_t ZPlatformCacheLineSize    = 64;\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/z\/zGlobals_aarch64.hpp","additions":3,"deletions":4,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -476,1 +476,1 @@\n-    ldr(rscratch2, Address(rthread, Thread::polling_page_offset()));\n+    ldr(rscratch2, Address(rthread, Thread::polling_word_offset()));\n@@ -524,0 +524,1 @@\n+\/\/ Apply stack watermark barrier.\n@@ -544,0 +545,13 @@\n+  \/\/ The below poll is for the stack watermark barrier. It allows fixing up frames lazily,\n+  \/\/ that would normally not be safe to use. Such bad returns into unsafe territory of\n+  \/\/ the stack, will call InterpreterRuntime::at_unwind.\n+  Label slow_path;\n+  Label fast_path;\n+  safepoint_poll(slow_path, true \/* at_return *\/, false \/* acquire *\/, false \/* in_nmethod *\/);\n+  br(Assembler::AL, fast_path);\n+  bind(slow_path);\n+  push(state);\n+  call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::at_unwind));\n+  pop(state);\n+  bind(fast_path);\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/interp_masm_aarch64.cpp","additions":15,"deletions":1,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -291,21 +291,15 @@\n-void MacroAssembler::safepoint_poll(Label& slow_path) {\n-  ldr(rscratch1, Address(rthread, Thread::polling_page_offset()));\n-  tbnz(rscratch1, exact_log2(SafepointMechanism::poll_bit()), slow_path);\n-}\n-\n-\/\/ Just like safepoint_poll, but use an acquiring load for thread-\n-\/\/ local polling.\n-\/\/\n-\/\/ We need an acquire here to ensure that any subsequent load of the\n-\/\/ global SafepointSynchronize::_state flag is ordered after this load\n-\/\/ of the local Thread::_polling page.  We don't want this poll to\n-\/\/ return false (i.e. not safepointing) and a later poll of the global\n-\/\/ SafepointSynchronize::_state spuriously to return true.\n-\/\/\n-\/\/ This is to avoid a race when we're in a native->Java transition\n-\/\/ racing the code which wakes up from a safepoint.\n-\/\/\n-void MacroAssembler::safepoint_poll_acquire(Label& slow_path) {\n-  lea(rscratch1, Address(rthread, Thread::polling_page_offset()));\n-  ldar(rscratch1, rscratch1);\n-  tbnz(rscratch1, exact_log2(SafepointMechanism::poll_bit()), slow_path);\n+void MacroAssembler::safepoint_poll(Label& slow_path, bool at_return, bool acquire, bool in_nmethod) {\n+  if (acquire) {\n+    lea(rscratch1, Address(rthread, Thread::polling_word_offset()));\n+    ldar(rscratch1, rscratch1);\n+  } else {\n+    ldr(rscratch1, Address(rthread, Thread::polling_word_offset()));\n+  }\n+  if (at_return) {\n+    \/\/ Note that when in_nmethod is set, the stack pointer is incremented before the poll. Therefore,\n+    \/\/ we may safely use the sp instead to perform the stack watermark check.\n+    cmp(in_nmethod ? sp : rfp, rscratch1);\n+    br(Assembler::HI, slow_path);\n+  } else {\n+    tbnz(rscratch1, exact_log2(SafepointMechanism::poll_bit()), slow_path);\n+  }\n@@ -4408,7 +4402,0 @@\n-\/\/ Move the address of the polling page into r, then read the polling\n-\/\/ page.\n-address MacroAssembler::fetch_and_read_polling_page(Register r, relocInfo::relocType rtype) {\n-  get_polling_page(r, rtype);\n-  return read_polling_page(r, rtype);\n-}\n-\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":15,"deletions":28,"binary":false,"changes":43,"status":"modified"},{"patch":"@@ -105,2 +105,1 @@\n-  void safepoint_poll(Label& slow_path);\n-  void safepoint_poll_acquire(Label& slow_path);\n+  void safepoint_poll(Label& slow_path, bool at_return, bool acquire, bool in_nmethod);\n@@ -1234,1 +1233,0 @@\n-  address fetch_and_read_polling_page(Register r, relocInfo::relocType rtype);\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.hpp","additions":1,"deletions":3,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1880,1 +1880,10 @@\n-    __ safepoint_poll_acquire(safepoint_in_progress);\n+    \/\/ We need an acquire here to ensure that any subsequent load of the\n+    \/\/ global SafepointSynchronize::_state flag is ordered after this load\n+    \/\/ of the thread-local polling word.  We don't want this poll to\n+    \/\/ return false (i.e. not safepointing) and a later poll of the global\n+    \/\/ SafepointSynchronize::_state spuriously to return true.\n+    \/\/\n+    \/\/ This is to avoid a race when we're in a native->Java transition\n+    \/\/ racing the code which wakes up from a safepoint.\n+\n+    __ safepoint_poll(safepoint_in_progress, true \/* at_return *\/, true \/* acquire *\/, false \/* in_nmethod *\/);\n","filename":"src\/hotspot\/cpu\/aarch64\/sharedRuntime_aarch64.cpp","additions":10,"deletions":1,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -983,1 +983,1 @@\n-    __ safepoint_poll(slow_path);\n+    __ safepoint_poll(slow_path, false \/* at_return *\/, false \/* acquire *\/, false \/* in_nmethod *\/);\n@@ -1032,1 +1032,1 @@\n-    __ safepoint_poll(slow_path);\n+    __ safepoint_poll(slow_path, false \/* at_return *\/, false \/* acquire *\/, false \/* in_nmethod *\/);\n@@ -1391,1 +1391,10 @@\n-    __ safepoint_poll_acquire(L);\n+\n+    \/\/ We need an acquire here to ensure that any subsequent load of the\n+    \/\/ global SafepointSynchronize::_state flag is ordered after this load\n+    \/\/ of the thread-local polling word.  We don't want this poll to\n+    \/\/ return false (i.e. not safepointing) and a later poll of the global\n+    \/\/ SafepointSynchronize::_state spuriously to return true.\n+    \/\/\n+    \/\/ This is to avoid a race when we're in a native->Java transition\n+    \/\/ racing the code which wakes up from a safepoint.\n+    __ safepoint_poll(L, true \/* at_return *\/, true \/* acquire *\/, false \/* in_nmethod *\/);\n","filename":"src\/hotspot\/cpu\/aarch64\/templateInterpreterGenerator_aarch64.cpp","additions":12,"deletions":3,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -131,0 +131,1 @@\n+  constexpr static bool supports_stack_watermark_barrier() { return true; }\n","filename":"src\/hotspot\/cpu\/aarch64\/vm_version_aarch64.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -41,0 +41,4 @@\n+void C1SafepointPollStub::emit_code(LIR_Assembler* ce) {\n+  ShouldNotReachHere();\n+}\n+\n","filename":"src\/hotspot\/cpu\/arm\/c1_CodeStubs_arm.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -286,1 +286,1 @@\n-void LIR_Assembler::return_op(LIR_Opr result) {\n+void LIR_Assembler::return_op(LIR_Opr result, C1SafepointPollStub* code_stub) {\n","filename":"src\/hotspot\/cpu\/arm\/c1_LIRAssembler_arm.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -583,1 +583,1 @@\n-    ldr(Rtemp, Address(Rthread, Thread::polling_page_offset()));\n+    ldr(Rtemp, Address(Rthread, Thread::polling_word_offset()));\n","filename":"src\/hotspot\/cpu\/arm\/interp_masm_arm.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1917,1 +1917,1 @@\n-  ldr_u32(tmp1, Address(Rthread, Thread::polling_page_offset()));\n+  ldr_u32(tmp1, Address(Rthread, Thread::polling_word_offset()));\n","filename":"src\/hotspot\/cpu\/arm\/macroAssembler_arm.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -41,0 +41,3 @@\n+void C1SafepointPollStub::emit_code(LIR_Assembler* ce) {\n+  ShouldNotReachHere();\n+}\n","filename":"src\/hotspot\/cpu\/ppc\/c1_CodeStubs_ppc.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -1327,1 +1327,1 @@\n-void LIR_Assembler::return_op(LIR_Opr result) {\n+void LIR_Assembler::return_op(LIR_Opr result, C1SafepointPollStub* code_stub) {\n","filename":"src\/hotspot\/cpu\/ppc\/c1_LIRAssembler_ppc.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -226,1 +226,1 @@\n-      ld(R0, in_bytes(Thread::polling_page_offset()), R16_thread);\n+      ld(R0, in_bytes(Thread::polling_word_offset()), R16_thread);\n","filename":"src\/hotspot\/cpu\/ppc\/interp_masm_ppc_64.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -3047,1 +3047,1 @@\n-  ld(temp_reg, in_bytes(Thread::polling_page_offset()), R16_thread);\n+  ld(temp_reg, in_bytes(Thread::polling_word_offset()), R16_thread);\n","filename":"src\/hotspot\/cpu\/ppc\/macroAssembler_ppc.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2176,1 +2176,1 @@\n-    __ ld(R11_scratch1, in_bytes(Thread::polling_page_offset()), R16_thread);\n+    __ ld(R11_scratch1, in_bytes(Thread::polling_word_offset()), R16_thread);\n","filename":"src\/hotspot\/cpu\/ppc\/templateTable_ppc_64.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -44,0 +44,4 @@\n+void C1SafepointPollStub::emit_code(LIR_Assembler* ce) {\n+  ShouldNotReachHere();\n+}\n+\n","filename":"src\/hotspot\/cpu\/s390\/c1_CodeStubs_s390.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1210,1 +1210,1 @@\n-void LIR_Assembler::return_op(LIR_Opr result) {\n+void LIR_Assembler::return_op(LIR_Opr result, C1SafepointPollStub* code_stub) {\n","filename":"src\/hotspot\/cpu\/s390\/c1_LIRAssembler_s390.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -124,1 +124,1 @@\n-      const Address poll_byte_addr(Z_thread, in_bytes(Thread::polling_page_offset()) + 7 \/* Big Endian *\/);\n+      const Address poll_byte_addr(Z_thread, in_bytes(Thread::polling_word_offset()) + 7 \/* Big Endian *\/);\n","filename":"src\/hotspot\/cpu\/s390\/interp_masm_s390.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2683,1 +2683,1 @@\n-  const Address poll_byte_addr(Z_thread, in_bytes(Thread::polling_page_offset()) + 7 \/* Big Endian *\/);\n+  const Address poll_byte_addr(Z_thread, in_bytes(Thread::polling_word_offset()) + 7 \/* Big Endian *\/);\n","filename":"src\/hotspot\/cpu\/s390\/macroAssembler_s390.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2380,1 +2380,1 @@\n-    const Address poll_byte_addr(Z_thread, in_bytes(Thread::polling_page_offset()) + 7 \/* Big Endian *\/);\n+    const Address poll_byte_addr(Z_thread, in_bytes(Thread::polling_word_offset()) + 7 \/* Big Endian *\/);\n","filename":"src\/hotspot\/cpu\/s390\/templateTable_s390.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -82,0 +82,16 @@\n+void C1SafepointPollStub::emit_code(LIR_Assembler* ce) {\n+#ifdef _LP64\n+  __ bind(_entry);\n+  InternalAddress safepoint_pc(ce->masm()->pc() - ce->masm()->offset() + safepoint_offset());\n+  __ lea(rscratch1, safepoint_pc);\n+  __ movptr(Address(r15_thread, JavaThread::saved_exception_pc_offset()), rscratch1);\n+\n+  assert(SharedRuntime::polling_page_return_handler_blob() != NULL,\n+         \"polling page return stub not created yet\");\n+  address stub = SharedRuntime::polling_page_return_handler_blob()->entry_point();\n+  __ jump(RuntimeAddress(stub));\n+#else\n+  ShouldNotReachHere();\n+#endif \/* _LP64 *\/\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/c1_CodeStubs_x86.cpp","additions":16,"deletions":0,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"c1\/c1_CodeStubs.hpp\"\n@@ -520,2 +521,1 @@\n-\n-void LIR_Assembler::return_op(LIR_Opr result) {\n+void LIR_Assembler::return_op(LIR_Opr result, C1SafepointPollStub* code_stub) {\n@@ -534,2 +534,0 @@\n-  bool result_is_oop = result->is_valid() ? result->is_oop() : false;\n-\n@@ -540,2 +538,3 @@\n-  const Register poll_addr = rscratch1;\n-  __ movptr(poll_addr, Address(r15_thread, Thread::polling_page_offset()));\n+  code_stub->set_safepoint_offset(__ offset());\n+  __ relocate(relocInfo::poll_return_type);\n+  __ safepoint_poll(*code_stub->entry(), r15_thread, true \/* at_return *\/, true \/* in_nmethod *\/);\n@@ -547,1 +546,0 @@\n-#endif\n@@ -550,0 +548,1 @@\n+#endif\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRAssembler_x86.cpp","additions":6,"deletions":7,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -0,0 +1,46 @@\n+\/*\n+ * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"asm\/macroAssembler.hpp\"\n+#include \"opto\/compile.hpp\"\n+#include \"opto\/node.hpp\"\n+#include \"opto\/output.hpp\"\n+#include \"runtime\/sharedRuntime.hpp\"\n+\n+#define __ masm.\n+void C2SafepointPollStubTable::emit_stub_impl(MacroAssembler& masm, C2SafepointPollStub* entry) const {\n+  assert(SharedRuntime::polling_page_return_handler_blob() != NULL,\n+         \"polling page return stub not created yet\");\n+  address stub = SharedRuntime::polling_page_return_handler_blob()->entry_point();\n+\n+  RuntimeAddress callback_addr(stub);\n+\n+  __ bind(entry->_stub_label);\n+  InternalAddress safepoint_pc(masm.pc() - masm.offset() + entry->_safepoint_offset);\n+  __ lea(rscratch1, safepoint_pc);\n+  __ movptr(Address(r15_thread, JavaThread::saved_exception_pc_offset()), rscratch1);\n+  __ jump(callback_addr);\n+}\n+#undef __\n","filename":"src\/hotspot\/cpu\/x86\/c2_safepointPollStubTable_x86.cpp","additions":46,"deletions":0,"binary":false,"changes":46,"status":"added"},{"patch":"@@ -39,0 +39,1 @@\n+#include \"runtime\/stackWatermarkSet.hpp\"\n@@ -472,2 +473,2 @@\n-\/\/ frame::sender\n-frame frame::sender(RegisterMap* map) const {\n+\/\/ frame::sender_raw\n+frame frame::sender_raw(RegisterMap* map) const {\n@@ -490,0 +491,10 @@\n+frame frame::sender(RegisterMap* map) const {\n+  frame result = sender_raw(map);\n+\n+  if (map->process_frames()) {\n+    StackWatermarkSet::on_iteration(map->thread(), result);\n+  }\n+\n+  return result;\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/frame_x86.cpp","additions":13,"deletions":2,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -159,0 +159,3 @@\n+  \/\/ returns the sending frame, without applying any barriers\n+  frame sender_raw(RegisterMap* map) const;\n+\n","filename":"src\/hotspot\/cpu\/x86\/frame_x86.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -27,4 +27,3 @@\n-const size_t ZPlatformGranuleSizeShift      = 21; \/\/ 2MB\n-const size_t ZPlatformHeapViews             = 3;\n-const size_t ZPlatformNMethodDisarmedOffset = 4;\n-const size_t ZPlatformCacheLineSize         = 64;\n+const size_t ZPlatformGranuleSizeShift = 21; \/\/ 2MB\n+const size_t ZPlatformHeapViews        = 3;\n+const size_t ZPlatformCacheLineSize    = 64;\n","filename":"src\/hotspot\/cpu\/x86\/gc\/z\/zGlobals_x86.hpp","additions":3,"deletions":4,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -856,1 +856,1 @@\n-    testb(Address(r15_thread, Thread::polling_page_offset()), SafepointMechanism::poll_bit());\n+    testb(Address(r15_thread, Thread::polling_word_offset()), SafepointMechanism::poll_bit());\n@@ -875,1 +875,1 @@\n-    testb(Address(thread, Thread::polling_page_offset()), SafepointMechanism::poll_bit());\n+    testb(Address(thread, Thread::polling_word_offset()), SafepointMechanism::poll_bit());\n@@ -964,0 +964,1 @@\n+\/\/ Apply stack watermark barrier.\n@@ -992,0 +993,13 @@\n+  \/\/ The below poll is for the stack watermark barrier. It allows fixing up frames lazily,\n+  \/\/ that would normally not be safe to use. Such bad returns into unsafe territory of\n+  \/\/ the stack, will call InterpreterRuntime::at_unwind.\n+  Label slow_path;\n+  Label fast_path;\n+  safepoint_poll(slow_path, rthread, true \/* at_return *\/, false \/* in_nmethod *\/);\n+  jmp(fast_path);\n+  bind(slow_path);\n+  push(state);\n+  call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::at_unwind));\n+  pop(state);\n+  bind(fast_path);\n+\n","filename":"src\/hotspot\/cpu\/x86\/interp_masm_x86.cpp","additions":16,"deletions":2,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -2702,1 +2702,1 @@\n-void MacroAssembler::safepoint_poll(Label& slow_path, Register thread_reg, Register temp_reg) {\n+void MacroAssembler::safepoint_poll(Label& slow_path, Register thread_reg, bool at_return, bool in_nmethod) {\n@@ -2704,5 +2704,6 @@\n-  assert(thread_reg == r15_thread, \"should be\");\n-#else\n-  if (thread_reg == noreg) {\n-    thread_reg = temp_reg;\n-    get_thread(thread_reg);\n+  if (at_return) {\n+    \/\/ Note that when in_nmethod is set, the stack pointer is incremented before the poll. Therefore,\n+    \/\/ we may safely use rsp instead to perform the stack watermark check.\n+    cmpq(Address(thread_reg, Thread::polling_word_offset()), in_nmethod ? rsp : rbp);\n+    jcc(Assembler::above, slow_path);\n+    return;\n@@ -2711,1 +2712,1 @@\n-  testb(Address(thread_reg, Thread::polling_page_offset()), SafepointMechanism::poll_bit());\n+  testb(Address(thread_reg, Thread::polling_word_offset()), SafepointMechanism::poll_bit());\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":8,"deletions":7,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -650,3 +650,1 @@\n-  \/\/ If thread_reg is != noreg the code assumes the register passed contains\n-  \/\/ the thread (required on 64 bit).\n-  void safepoint_poll(Label& slow_path, Register thread_reg, Register temp_reg);\n+  void safepoint_poll(Label& slow_path, Register thread_reg, bool at_return, bool in_nmethod);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":1,"deletions":3,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2240,1 +2240,1 @@\n-    __ safepoint_poll(slow_path, thread, noreg);\n+    __ safepoint_poll(slow_path, thread, true \/* at_return *\/, false \/* in_nmethod *\/);\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_32.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2598,1 +2598,1 @@\n-    __ safepoint_poll(slow_path, r15_thread, rscratch1);\n+    __ safepoint_poll(slow_path, r15_thread, true \/* at_return *\/, false \/* in_nmethod *\/);\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_64.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1109,5 +1109,1 @@\n-#ifndef _LP64\n-    __ safepoint_poll(slow_path, thread, noreg);\n-#else\n-    __ safepoint_poll(slow_path, r15_thread, rscratch1);\n-#endif\n+    __ safepoint_poll(slow_path, thread, true \/* at_return *\/, false \/* in_nmethod *\/);\n","filename":"src\/hotspot\/cpu\/x86\/templateInterpreterGenerator_x86.cpp","additions":1,"deletions":5,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -65,1 +65,2 @@\n-    __ safepoint_poll(slow_path, noreg, rdi);\n+    __ get_thread(rdi);\n+    __ safepoint_poll(slow_path, rdi, false \/* at_return *\/, false \/* in_nmethod *\/);\n@@ -114,1 +115,2 @@\n-    __ safepoint_poll(slow_path, noreg, rdi);\n+    __ get_thread(rdi);\n+    __ safepoint_poll(slow_path, rdi, false \/* at_return *\/, false \/* in_nmethod *\/);\n","filename":"src\/hotspot\/cpu\/x86\/templateInterpreterGenerator_x86_32.cpp","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -194,1 +194,1 @@\n-    __ safepoint_poll(slow_path, r15_thread, rscratch1);\n+    __ safepoint_poll(slow_path, r15_thread, true \/* at_return *\/, false \/* in_nmethod *\/);\n@@ -240,1 +240,1 @@\n-    __ safepoint_poll(slow_path, r15_thread, rscratch1);\n+    __ safepoint_poll(slow_path, r15_thread, false \/* at_return *\/, false \/* in_nmethod *\/);\n","filename":"src\/hotspot\/cpu\/x86\/templateInterpreterGenerator_x86_64.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2661,1 +2661,1 @@\n-    __ testb(Address(r15_thread, Thread::polling_page_offset()), SafepointMechanism::poll_bit());\n+    __ testb(Address(r15_thread, Thread::polling_word_offset()), SafepointMechanism::poll_bit());\n@@ -2665,1 +2665,1 @@\n-    __ testb(Address(thread, Thread::polling_page_offset()), SafepointMechanism::poll_bit());\n+    __ testb(Address(thread, Thread::polling_word_offset()), SafepointMechanism::poll_bit());\n@@ -2670,1 +2670,1 @@\n-                                    InterpreterRuntime::at_safepoint));\n+                                       InterpreterRuntime::at_safepoint));\n","filename":"src\/hotspot\/cpu\/x86\/templateTable_x86.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"utilities\/macros.hpp\"\n@@ -1024,0 +1025,4 @@\n+  constexpr static bool supports_stack_watermark_barrier() {\n+    return LP64_ONLY(true) NOT_LP64(false);\n+  }\n+\n","filename":"src\/hotspot\/cpu\/x86\/vm_version_x86.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -935,2 +935,2 @@\n-    st->print_cr(\"movq    rscratch1, poll_offset[r15_thread] #polling_page_address\\n\\t\"\n-                 \"testl   rax, [rscratch1]\\t\"\n+    st->print_cr(\"cmpq    poll_offset[r15_thread], rsp\\n\\t\"\n+                 \"ja      #safepoint_stub\\t\"\n@@ -983,1 +983,5 @@\n-    __ movq(rscratch1, Address(r15_thread, Thread::polling_page_offset()));\n+    Label dummy_label;\n+    Label* code_stub = &dummy_label;\n+    if (!C->output()->in_scratch_emit_size()) {\n+      code_stub = &C->output()->safepoint_poll_table()->add_safepoint(__ offset());\n+    }\n@@ -985,1 +989,1 @@\n-    __ testl(rax, Address(rscratch1, 0));\n+    __ safepoint_poll(*code_stub, r15_thread, true \/* at_return *\/, true \/* in_nmethod *\/);\n","filename":"src\/hotspot\/cpu\/x86\/x86_64.ad","additions":8,"deletions":4,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -92,0 +92,22 @@\n+class C1SafepointPollStub: public CodeStub {\n+ private:\n+  uintptr_t _safepoint_offset;\n+\n+ public:\n+  C1SafepointPollStub() :\n+      _safepoint_offset(0) {\n+  }\n+\n+  uintptr_t safepoint_offset() { return _safepoint_offset; }\n+  void set_safepoint_offset(uintptr_t safepoint_offset) { _safepoint_offset = safepoint_offset; }\n+\n+  virtual void emit_code(LIR_Assembler* e);\n+  virtual void visit(LIR_OpVisitState* visitor) {\n+    \/\/ don't pass in the code emit info since it's processed in the fast path\n+    visitor->do_slow_case();\n+  }\n+#ifndef PRODUCT\n+  virtual void print_name(outputStream* out) const { out->print(\"C1SafepointPollStub\"); }\n+#endif \/\/ PRODUCT\n+};\n+\n","filename":"src\/hotspot\/share\/c1\/c1_CodeStubs.hpp","additions":22,"deletions":0,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -26,0 +26,1 @@\n+#include \"c1\/c1_CodeStubs.hpp\"\n@@ -31,0 +32,1 @@\n+#include \"runtime\/safepointMechanism.inline.hpp\"\n@@ -450,1 +452,0 @@\n-    case lir_return:         \/\/ input always valid, result and info always invalid\n@@ -466,0 +467,13 @@\n+    case lir_return:\n+    {\n+      assert(op->as_OpReturn() != NULL, \"must be\");\n+      LIR_OpReturn* op_ret = (LIR_OpReturn*)op;\n+\n+      if (op_ret->_info)               do_info(op_ret->_info);\n+      if (op_ret->_opr->is_valid())    do_input(op_ret->_opr);\n+      if (op_ret->_result->is_valid()) do_output(op_ret->_result);\n+      if (op_ret->stub() != NULL)      do_stub(op_ret->stub());\n+\n+      break;\n+    }\n+\n@@ -951,0 +965,9 @@\n+\/\/ LIR_OpReturn\n+LIR_OpReturn::LIR_OpReturn(LIR_Opr opr) :\n+    LIR_Op1(lir_return, opr, (CodeEmitInfo*)NULL \/* info *\/),\n+    _stub(NULL) {\n+  if (VM_Version::supports_stack_watermark_barrier()) {\n+    _stub = new C1SafepointPollStub();\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/c1\/c1_LIR.cpp","additions":24,"deletions":1,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -39,0 +39,1 @@\n+class C1SafepointPollStub;\n@@ -859,0 +860,1 @@\n+class      LIR_OpReturn;\n@@ -1119,0 +1121,1 @@\n+  virtual LIR_OpReturn* as_OpReturn() { return NULL; }\n@@ -1442,0 +1445,12 @@\n+class LIR_OpReturn: public LIR_Op1 {\n+ friend class LIR_OpVisitState;\n+\n+ private:\n+  C1SafepointPollStub* _stub;\n+\n+ public:\n+  LIR_OpReturn(LIR_Opr opr);\n+\n+  C1SafepointPollStub* stub() const { return _stub; }\n+  virtual LIR_OpReturn* as_OpReturn() { return this; }\n+};\n@@ -2097,2 +2112,0 @@\n-  void return_op(LIR_Opr result)                 { append(new LIR_Op1(lir_return, result)); }\n-\n@@ -2100,0 +2113,1 @@\n+  void return_op(LIR_Opr result)                   { append(new LIR_OpReturn(result)); }\n","filename":"src\/hotspot\/share\/c1\/c1_LIR.hpp","additions":16,"deletions":2,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -524,2 +524,7 @@\n-    case lir_return:\n-      return_op(op->in_opr());\n+    case lir_return: {\n+      assert(op->as_OpReturn() != NULL, \"sanity\");\n+      LIR_OpReturn *ret_op = (LIR_OpReturn*)op;\n+      return_op(ret_op->in_opr(), ret_op->stub());\n+      if (ret_op->stub() != NULL) {\n+        append_code_stub(ret_op->stub());\n+      }\n@@ -527,0 +532,1 @@\n+    }\n","filename":"src\/hotspot\/share\/c1\/c1_LIRAssembler.cpp","additions":8,"deletions":2,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -160,1 +160,1 @@\n-  void return_op(LIR_Opr result);\n+  void return_op(LIR_Opr result, C1SafepointPollStub* code_stub);\n","filename":"src\/hotspot\/share\/c1\/c1_LIRAssembler.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -6413,1 +6413,1 @@\n-            pred_instructions->at_put(pred_instructions->length() - 1, new LIR_Op1(lir_return, return_opr));\n+            pred_instructions->at_put(pred_instructions->length() - 1, new LIR_OpReturn(return_opr));\n","filename":"src\/hotspot\/share\/c1\/c1_LinearScan.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -66,0 +66,1 @@\n+#include \"runtime\/stackWatermarkSet.hpp\"\n@@ -508,0 +509,11 @@\n+\n+  \/\/ This function is called when we are about to throw an exception. Therefore,\n+  \/\/ we have to poll the stack watermark barrier to make sure that not yet safe\n+  \/\/ stack frames are made safe before returning into them.\n+  if (thread->last_frame().cb() == Runtime1::blob_for(Runtime1::handle_exception_from_callee_id)) {\n+    \/\/ The Runtime1::handle_exception_from_callee_id handler is invoked after the\n+    \/\/ frame has been unwound. It instead builds its own stub frame, to call the\n+    \/\/ runtime. But the throwing frame has already been unwound here.\n+    StackWatermarkSet::after_unwind(thread);\n+  }\n+\n","filename":"src\/hotspot\/share\/c1\/c1_Runtime1.cpp","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -2439,1 +2439,1 @@\n-  vframeStream st(thread);\n+  vframeStream st(thread, false \/* stop_at_java_call_stub *\/, false \/* process_frames *\/);\n@@ -2442,1 +2442,1 @@\n-  RegisterMap map(thread, false);\n+  RegisterMap map(thread, false \/* update *\/, false \/* process_frames *\/);\n@@ -2584,1 +2584,1 @@\n-  vframeStream st(THREAD);\n+  vframeStream st(THREAD, false \/* stop_at_java_call_stub *\/, false \/* process_frames *\/);\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -40,0 +40,1 @@\n+#include \"runtime\/stackWatermarkSet.inline.hpp\"\n@@ -192,5 +193,1 @@\n-static void add_derived_oop(oop* base, oop* derived) {\n-#if !defined(TIERED) && !INCLUDE_JVMCI\n-  COMPILER1_PRESENT(ShouldNotReachHere();)\n-#endif \/\/ !defined(TIERED) && !INCLUDE_JVMCI\n-#if COMPILER2_OR_JVMCI\n+static void add_derived_oop(oop* base, oop* derived, OopClosure* oop_fn) {\n@@ -198,1 +195,13 @@\n-#endif \/\/ COMPILER2_OR_JVMCI\n+}\n+\n+static void ignore_derived_oop(oop* base, oop* derived, OopClosure* oop_fn) {\n+}\n+\n+static void process_derived_oop(oop* base, oop* derived, OopClosure* oop_fn) {\n+  \/\/ All derived pointers must be processed before the base pointer of any derived pointer is processed.\n+  \/\/ Otherwise, if two derived pointers use the same base, the second derived pointer will get an obscured\n+  \/\/ offset, if the base pointer is processed in the first derived pointer.\n+  uintptr_t offset = cast_from_oop<uintptr_t>(*derived) - cast_from_oop<uintptr_t>(*base);\n+  *derived = *base;\n+  oop_fn->do_oop(derived);\n+  *derived = cast_to_oop(cast_from_oop<uintptr_t>(*derived) + offset);\n@@ -230,3 +239,12 @@\n-void OopMapSet::oops_do(const frame *fr, const RegisterMap* reg_map, OopClosure* f) {\n-  \/\/ add derived oops to a table\n-  all_do(fr, reg_map, f, add_derived_oop, &do_nothing_cl);\n+void OopMapSet::oops_do(const frame *fr, const RegisterMap* reg_map, OopClosure* f, DerivedPointerIterationMode mode) {\n+  switch (mode) {\n+  case DerivedPointerIterationMode::_directly:\n+    all_do(fr, reg_map, f, process_derived_oop, &do_nothing_cl);\n+    break;\n+  case DerivedPointerIterationMode::_with_table:\n+    all_do(fr, reg_map, f, add_derived_oop, &do_nothing_cl);\n+    break;\n+  case DerivedPointerIterationMode::_ignore:\n+    all_do(fr, reg_map, f, ignore_derived_oop, &do_nothing_cl);\n+    break;\n+  }\n@@ -237,1 +255,1 @@\n-                       OopClosure* oop_fn, void derived_oop_fn(oop*, oop*),\n+                       OopClosure* oop_fn, void derived_oop_fn(oop*, oop*, OopClosure*),\n@@ -274,1 +292,1 @@\n-        derived_oop_fn(base_loc, derived_loc);\n+        derived_oop_fn(base_loc, derived_loc, oop_fn);\n@@ -299,14 +317,0 @@\n-#ifdef ASSERT\n-        if ((((uintptr_t)loc & (sizeof(*loc)-1)) != 0) ||\n-            !Universe::heap()->is_in_or_null(*loc)) {\n-          tty->print_cr(\"# Found non oop pointer.  Dumping state at failure\");\n-          \/\/ try to dump out some helpful debugging information\n-          trace_codeblob_maps(fr, reg_map);\n-          omv.print();\n-          tty->print_cr(\"register r\");\n-          omv.reg()->print();\n-          tty->print_cr(\"loc = %p *loc = %p\\n\", loc, cast_from_oop<address>(*loc));\n-          \/\/ do the real assert.\n-          assert(Universe::heap()->is_in_or_null(*loc), \"found non oop pointer\");\n-        }\n-#endif \/\/ ASSERT\n@@ -697,20 +701,15 @@\n-  if (_active) {\n-    assert(*derived_loc != (void*)base_loc, \"location already added\");\n-    assert(Entry::_list != NULL, \"list must exist\");\n-    intptr_t offset = value_of_loc(derived_loc) - value_of_loc(base_loc);\n-    \/\/ This assert is invalid because derived pointers can be\n-    \/\/ arbitrarily far away from their base.\n-    \/\/ assert(offset >= -1000000, \"wrong derived pointer info\");\n-\n-    if (TraceDerivedPointers) {\n-      tty->print_cr(\n-        \"Add derived pointer@\" INTPTR_FORMAT\n-        \" - Derived: \" INTPTR_FORMAT\n-        \" Base: \" INTPTR_FORMAT \" (@\" INTPTR_FORMAT \") (Offset: \" INTX_FORMAT \")\",\n-        p2i(derived_loc), p2i(*derived_loc), p2i(*base_loc), p2i(base_loc), offset\n-      );\n-    }\n-    \/\/ Set derived oop location to point to base.\n-    *derived_loc = (oop)base_loc;\n-    Entry* entry = new Entry(derived_loc, offset);\n-    Entry::_list->push(*entry);\n+  assert(*derived_loc != (void*)base_loc, \"location already added\");\n+  assert(Entry::_list != NULL, \"list must exist\");\n+  assert(is_active(), \"table must be active here\");\n+  intptr_t offset = value_of_loc(derived_loc) - value_of_loc(base_loc);\n+  \/\/ This assert is invalid because derived pointers can be\n+  \/\/ arbitrarily far away from their base.\n+  \/\/ assert(offset >= -1000000, \"wrong derived pointer info\");\n+\n+  if (TraceDerivedPointers) {\n+    tty->print_cr(\n+      \"Add derived pointer@\" INTPTR_FORMAT\n+      \" - Derived: \" INTPTR_FORMAT\n+      \" Base: \" INTPTR_FORMAT \" (@\" INTPTR_FORMAT \") (Offset: \" INTX_FORMAT \")\",\n+      p2i(derived_loc), p2i(*derived_loc), p2i(*base_loc), p2i(base_loc), offset\n+    );\n@@ -718,0 +717,4 @@\n+  \/\/ Set derived oop location to point to base.\n+  *derived_loc = (oop)base_loc;\n+  Entry* entry = new Entry(derived_loc, offset);\n+  Entry::_list->push(*entry);\n","filename":"src\/hotspot\/share\/compiler\/oopMap.cpp","additions":48,"deletions":45,"binary":false,"changes":93,"status":"modified"},{"patch":"@@ -43,0 +43,1 @@\n+enum class DerivedPointerIterationMode;\n@@ -199,1 +200,0 @@\n-\n@@ -224,1 +224,3 @@\n-                                  const RegisterMap* reg_map, OopClosure* f);\n+                                  const RegisterMap* reg_map,\n+                                  OopClosure* f,\n+                                  DerivedPointerIterationMode mode);\n@@ -230,1 +232,1 @@\n-                     void derived_oop_fn(oop* base, oop* derived),\n+                     void derived_oop_fn(oop* base, oop* derived, OopClosure* oop_fn),\n","filename":"src\/hotspot\/share\/compiler\/oopMap.hpp","additions":5,"deletions":3,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -373,0 +373,5 @@\n+  \/\/ If a GC uses a stack watermark barrier, the stack processing is lazy, concurrent,\n+  \/\/ incremental and cooperative. In order for that to work well, mechanisms that stop\n+  \/\/ another thread might want to ensure its roots are in a sane state.\n+  virtual bool uses_stack_watermark_barrier() const { return false; }\n+\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n@@ -117,0 +117,4 @@\n+uintptr_t ZBarrier::relocate_or_mark_no_follow(uintptr_t addr) {\n+  return during_relocate() ? relocate(addr) : mark<DontFollow, Strong, Publish>(addr);\n+}\n+\n@@ -128,0 +132,4 @@\n+uintptr_t ZBarrier::load_barrier_on_invisible_root_oop_slow_path(uintptr_t addr) {\n+  return relocate_or_mark_no_follow(addr);\n+}\n+\n@@ -201,8 +209,0 @@\n-uintptr_t ZBarrier::mark_barrier_on_invisible_root_oop_slow_path(uintptr_t addr) {\n-  assert(SafepointSynchronize::is_at_safepoint(), \"Should be at safepoint\");\n-  assert(during_mark(), \"Invalid phase\");\n-\n-  \/\/ Mark\n-  return mark<DontFollow, Strong, Publish>(addr);\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/z\/zBarrier.cpp","additions":9,"deletions":9,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n@@ -61,0 +61,1 @@\n+  static uintptr_t relocate_or_mark_no_follow(uintptr_t addr);\n@@ -64,0 +65,1 @@\n+  static uintptr_t load_barrier_on_invisible_root_oop_slow_path(uintptr_t addr);\n@@ -75,1 +77,0 @@\n-  static uintptr_t mark_barrier_on_invisible_root_oop_slow_path(uintptr_t addr);\n@@ -89,0 +90,1 @@\n+  static void load_barrier_on_invisible_root_oop_field(oop* p);\n","filename":"src\/hotspot\/share\/gc\/z\/zBarrier.hpp","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -280,0 +280,5 @@\n+inline void ZBarrier::load_barrier_on_invisible_root_oop_field(oop* p) {\n+  const oop o = *p;\n+  root_barrier<is_good_or_null_fast_path, load_barrier_on_invisible_root_oop_slow_path>(p, o);\n+}\n+\n@@ -410,5 +415,0 @@\n-inline void ZBarrier::mark_barrier_on_invisible_root_oop_field(oop* p) {\n-  const oop o = *p;\n-  root_barrier<is_good_or_null_fast_path, mark_barrier_on_invisible_root_oop_slow_path>(p, o);\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/z\/zBarrier.inline.hpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2020, Oracle and\/or its affiliates. All rights reserved.\n@@ -30,0 +30,1 @@\n+#include \"gc\/z\/zStackWatermark.hpp\"\n@@ -43,9 +44,0 @@\n-static BarrierSetNMethod* make_barrier_set_nmethod() {\n-  \/\/ NMethod barriers are only used when class unloading is enabled\n-  if (!ClassUnloading) {\n-    return NULL;\n-  }\n-\n-  return new ZBarrierSetNMethod();\n-}\n-\n@@ -56,1 +48,1 @@\n-               make_barrier_set_nmethod(),\n+               new ZBarrierSetNMethod(),\n@@ -92,0 +84,5 @@\n+  if (thread->is_Java_thread()) {\n+    JavaThread* const jt = thread->as_Java_thread();\n+    StackWatermark* const watermark = new ZStackWatermark(jt);\n+    StackWatermarkSet::add_watermark(jt, watermark);\n+  }\n","filename":"src\/hotspot\/share\/gc\/z\/zBarrierSet.cpp","additions":8,"deletions":11,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2020, Oracle and\/or its affiliates. All rights reserved.\n@@ -65,3 +65,1 @@\n-  const uintptr_t mask_addr = reinterpret_cast<uintptr_t>(&ZAddressBadMask);\n-  const uintptr_t disarmed_addr = mask_addr + ZNMethodDisarmedOffset;\n-  return reinterpret_cast<int*>(disarmed_addr);\n+  return (int*)ZAddressBadMaskHighOrderBitsAddr;\n","filename":"src\/hotspot\/share\/gc\/z\/zBarrierSetNMethod.cpp","additions":2,"deletions":4,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -224,0 +224,4 @@\n+bool ZCollectedHeap::uses_stack_watermark_barrier() const {\n+  return true;\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/z\/zCollectedHeap.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -91,0 +91,2 @@\n+  virtual bool uses_stack_watermark_barrier() const;\n+\n","filename":"src\/hotspot\/share\/gc\/z\/zCollectedHeap.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -73,0 +73,4 @@\n+  virtual bool skip_thread_oop_barriers() const {\n+    return true;\n+  }\n+\n@@ -221,0 +225,4 @@\n+  virtual bool skip_thread_oop_barriers() const {\n+    return true;\n+  }\n+\n","filename":"src\/hotspot\/share\/gc\/z\/zDriver.cpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n@@ -45,0 +45,7 @@\n+static uint32_t* ZAddressCalculateBadMaskHighOrderBitsAddr() {\n+  const uintptr_t addr = reinterpret_cast<uintptr_t>(&ZAddressBadMask);\n+  return reinterpret_cast<uint32_t*>(addr + ZAddressBadMaskHighOrderBitsOffset);\n+}\n+\n+uint32_t*  ZAddressBadMaskHighOrderBitsAddr = ZAddressCalculateBadMaskHighOrderBitsAddr();\n+\n","filename":"src\/hotspot\/share\/gc\/z\/zGlobals.cpp","additions":8,"deletions":1,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n@@ -97,0 +97,7 @@\n+\/\/ The bad mask is 64 bit. Its high order 32 bits contain all possible value combinations\n+\/\/ that this mask will have. Therefore, the memory where the 32 high order bits are stored,\n+\/\/ can be used as a 32 bit GC epoch counter, that has a different bit pattern every time\n+\/\/ the bad mask is flipped. This provides a pointer to said 32 bits.\n+extern uint32_t*  ZAddressBadMaskHighOrderBitsAddr;\n+const int         ZAddressBadMaskHighOrderBitsOffset = LITTLE_ENDIAN_ONLY(4) BIG_ENDIAN_ONLY(0);\n+\n@@ -115,3 +122,0 @@\n-\/\/ NMethod entry barrier\n-const size_t      ZNMethodDisarmedOffset        = ZPlatformNMethodDisarmedOffset;\n-\n","filename":"src\/hotspot\/share\/gc\/z\/zGlobals.hpp","additions":8,"deletions":4,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n@@ -36,0 +36,1 @@\n+#include \"runtime\/stackWatermarkSet.hpp\"\n@@ -86,0 +87,5 @@\n+\n+  virtual void do_thread(Thread* thread) {\n+    CodeBlobToOopClosure code_cl(this, false \/* fix_oop_relocations *\/);\n+    thread->oops_do(this, &code_cl);\n+  }\n","filename":"src\/hotspot\/share\/gc\/z\/zHeapIterator.cpp","additions":7,"deletions":1,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"gc\/z\/zStackWatermark.hpp\"\n@@ -49,0 +50,2 @@\n+#include \"runtime\/stackWatermark.hpp\"\n+#include \"runtime\/stackWatermarkSet.inline.hpp\"\n@@ -125,23 +128,0 @@\n-  ZMarkRootsIteratorClosure() {\n-    ZThreadLocalAllocBuffer::reset_statistics();\n-  }\n-\n-  ~ZMarkRootsIteratorClosure() {\n-    ZThreadLocalAllocBuffer::publish_statistics();\n-  }\n-\n-  virtual void do_thread(Thread* thread) {\n-    \/\/ Update thread local address bad mask\n-    ZThreadLocalData::set_address_bad_mask(thread, ZAddressBadMask);\n-\n-    \/\/ Mark invisible root\n-    ZThreadLocalData::do_invisible_root(thread, ZBarrier::mark_barrier_on_invisible_root_oop_field);\n-\n-    \/\/ Retire TLAB\n-    ZThreadLocalAllocBuffer::retire(thread);\n-  }\n-\n-  virtual bool should_disarm_nmethods() const {\n-    return true;\n-  }\n-\n@@ -634,0 +614,18 @@\n+  ZMarkConcurrentRootsIteratorClosure() {\n+    ZThreadLocalAllocBuffer::reset_statistics();\n+  }\n+\n+  ~ZMarkConcurrentRootsIteratorClosure() {\n+    ZThreadLocalAllocBuffer::publish_statistics();\n+  }\n+\n+  virtual bool should_disarm_nmethods() const {\n+    return true;\n+  }\n+\n+  virtual void do_thread(Thread* thread) {\n+    JavaThread* const jt = thread->as_Java_thread();\n+    StackWatermarkSet::finish_processing(jt, this, StackWatermarkKind::gc);\n+    ZThreadLocalAllocBuffer::update_stats(jt);\n+  }\n+\n","filename":"src\/hotspot\/share\/gc\/z\/zMark.cpp","additions":21,"deletions":23,"binary":false,"changes":44,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n@@ -192,5 +192,1 @@\n-  if (bs != NULL) {\n-    return bs->supports_entry_barrier(nm);\n-  }\n-\n-  return false;\n+  return bs->supports_entry_barrier(nm);\n@@ -201,5 +197,1 @@\n-  if (bs != NULL) {\n-    return bs->is_armed(nm);\n-  }\n-\n-  return false;\n+  return bs->is_armed(nm);\n@@ -210,3 +202,1 @@\n-  if (bs != NULL) {\n-    bs->disarm(nm);\n-  }\n+  bs->disarm(nm);\n@@ -248,1 +238,2 @@\n-  OopClosure* _cl;\n+  OopClosure* const _cl;\n+  const bool        _should_disarm_nmethods;\n@@ -251,2 +242,3 @@\n-  ZNMethodToOopsDoClosure(OopClosure* cl) :\n-      _cl(cl) {}\n+  ZNMethodToOopsDoClosure(OopClosure* cl, bool should_disarm_nmethods) :\n+      _cl(cl),\n+      _should_disarm_nmethods(should_disarm_nmethods) {}\n@@ -255,1 +247,13 @@\n-    ZNMethod::nmethod_oops_do(nm, _cl);\n+    ZLocker<ZReentrantLock> locker(ZNMethod::lock_for_nmethod(nm));\n+    if (!nm->is_alive()) {\n+      return;\n+    }\n+\n+    if (_should_disarm_nmethods) {\n+      if (ZNMethod::is_armed(nm)) {\n+        ZNMethod::nmethod_oops_do(nm, _cl);\n+        ZNMethod::disarm(nm);\n+      }\n+    } else {\n+      ZNMethod::nmethod_oops_do(nm, _cl);\n+    }\n@@ -267,2 +271,2 @@\n-void ZNMethod::oops_do(OopClosure* cl) {\n-  ZNMethodToOopsDoClosure nmethod_cl(cl);\n+void ZNMethod::oops_do(OopClosure* cl, bool should_disarm_nmethods) {\n+  ZNMethodToOopsDoClosure nmethod_cl(cl, should_disarm_nmethods);\n","filename":"src\/hotspot\/share\/gc\/z\/zNMethod.cpp","additions":24,"deletions":20,"binary":false,"changes":44,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n@@ -55,1 +55,1 @@\n-  static void oops_do(OopClosure* cl);\n+  static void oops_do(OopClosure* cl, bool should_disarm_nmethods);\n","filename":"src\/hotspot\/share\/gc\/z\/zNMethod.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n@@ -48,15 +48,0 @@\n-  virtual void do_thread(Thread* thread) {\n-    \/\/ Update thread local address bad mask\n-    ZThreadLocalData::set_address_bad_mask(thread, ZAddressBadMask);\n-\n-    \/\/ Relocate invisible root\n-    ZThreadLocalData::do_invisible_root(thread, ZBarrier::relocate_barrier_on_root_oop_field);\n-\n-    \/\/ Remap TLAB\n-    ZThreadLocalAllocBuffer::remap(thread);\n-  }\n-\n-  virtual bool should_disarm_nmethods() const {\n-    return true;\n-  }\n-\n","filename":"src\/hotspot\/share\/gc\/z\/zRelocate.cpp","additions":1,"deletions":16,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -37,0 +37,1 @@\n+#include \"gc\/z\/zLock.inline.hpp\"\n@@ -38,0 +39,1 @@\n+#include \"gc\/z\/zNMethodTable.hpp\"\n@@ -49,0 +51,1 @@\n+#include \"runtime\/stackWatermark.hpp\"\n@@ -54,1 +57,0 @@\n-static const ZStatSubPhase ZSubPhasePauseRootsSetup(\"Pause Roots Setup\");\n@@ -56,1 +58,0 @@\n-static const ZStatSubPhase ZSubPhasePauseRootsTeardown(\"Pause Roots Teardown\");\n@@ -58,3 +59,0 @@\n-static const ZStatSubPhase ZSubPhasePauseRootsVMThread(\"Pause Roots VM Thread\");\n-static const ZStatSubPhase ZSubPhasePauseRootsJavaThreads(\"Pause Roots Java Threads\");\n-static const ZStatSubPhase ZSubPhasePauseRootsCodeCache(\"Pause Roots CodeCache\");\n@@ -67,0 +65,2 @@\n+static const ZStatSubPhase ZSubPhaseConcurrentRootsJavaThreads(\"Concurrent Roots Java Threads\");\n+static const ZStatSubPhase ZSubPhaseConcurrentRootsCodeCache(\"Concurrent Roots CodeCache\");\n@@ -130,39 +130,0 @@\n-class ZRootsIteratorCodeBlobClosure : public CodeBlobClosure {\n-private:\n-  ZRootsIteratorClosure* const _cl;\n-  const bool                   _should_disarm_nmethods;\n-\n-public:\n-  ZRootsIteratorCodeBlobClosure(ZRootsIteratorClosure* cl) :\n-      _cl(cl),\n-      _should_disarm_nmethods(cl->should_disarm_nmethods()) {}\n-\n-  virtual void do_code_blob(CodeBlob* cb) {\n-    nmethod* const nm = cb->as_nmethod_or_null();\n-    if (nm != NULL && nm->oops_do_try_claim()) {\n-      ZNMethod::nmethod_oops_do(nm, _cl);\n-      assert(!ZNMethod::supports_entry_barrier(nm) ||\n-             ZNMethod::is_armed(nm) == _should_disarm_nmethods, \"Invalid state\");\n-      if (_should_disarm_nmethods) {\n-        ZNMethod::disarm(nm);\n-      }\n-    }\n-  }\n-};\n-\n-class ZRootsIteratorThreadClosure : public ThreadClosure {\n-private:\n-  ZRootsIteratorClosure* const _cl;\n-  ResourceMark                 _rm;\n-\n-public:\n-  ZRootsIteratorThreadClosure(ZRootsIteratorClosure* cl) :\n-      _cl(cl) {}\n-\n-  virtual void do_thread(Thread* thread) {\n-    ZRootsIteratorCodeBlobClosure code_cl(_cl);\n-    thread->oops_do(_cl, ClassUnloading ? &code_cl : NULL);\n-    _cl->do_thread(thread);\n-  }\n-};\n-\n@@ -185,5 +146,1 @@\n-    _java_threads_iter(),\n-    _jvmti_weak_export(this),\n-    _vm_thread(this),\n-    _java_threads(this),\n-    _code_cache(this) {\n+    _jvmti_weak_export(this) {\n@@ -191,19 +148,0 @@\n-  ZStatTimer timer(ZSubPhasePauseRootsSetup);\n-  COMPILER2_OR_JVMCI_PRESENT(DerivedPointerTable::clear());\n-  if (ClassUnloading) {\n-    nmethod::oops_do_marking_prologue();\n-  } else {\n-    ZNMethod::oops_do_begin();\n-  }\n-}\n-\n-ZRootsIterator::~ZRootsIterator() {\n-  ZStatTimer timer(ZSubPhasePauseRootsTeardown);\n-  ResourceMark rm;\n-  if (ClassUnloading) {\n-    nmethod::oops_do_marking_epilogue();\n-  } else {\n-    ZNMethod::oops_do_end();\n-  }\n-\n-  COMPILER2_OR_JVMCI_PRESENT(DerivedPointerTable::update_pointers());\n@@ -218,17 +156,0 @@\n-void ZRootsIterator::do_vm_thread(ZRootsIteratorClosure* cl) {\n-  ZStatTimer timer(ZSubPhasePauseRootsVMThread);\n-  ZRootsIteratorThreadClosure thread_cl(cl);\n-  thread_cl.do_thread(VMThread::vm_thread());\n-}\n-\n-void ZRootsIterator::do_java_threads(ZRootsIteratorClosure* cl) {\n-  ZStatTimer timer(ZSubPhasePauseRootsJavaThreads);\n-  ZRootsIteratorThreadClosure thread_cl(cl);\n-  _java_threads_iter.threads_do(&thread_cl);\n-}\n-\n-void ZRootsIterator::do_code_cache(ZRootsIteratorClosure* cl) {\n-  ZStatTimer timer(ZSubPhasePauseRootsCodeCache);\n-  ZNMethod::oops_do(cl);\n-}\n-\n@@ -237,5 +158,0 @@\n-  _vm_thread.oops_do(cl);\n-  _java_threads.oops_do(cl);\n-  if (!ClassUnloading) {\n-    _code_cache.oops_do(cl);\n-  }\n@@ -249,0 +165,1 @@\n+    _java_threads_iter(),\n@@ -251,1 +168,3 @@\n-    _class_loader_data_graph(this) {\n+    _class_loader_data_graph(this),\n+    _java_threads(this),\n+    _code_cache(this) {\n@@ -254,0 +173,3 @@\n+  if (!ClassUnloading) {\n+    ZNMethodTable::nmethods_do_begin();\n+  }\n@@ -258,0 +180,3 @@\n+  if (!ClassUnloading) {\n+    ZNMethodTable::nmethods_do_end();\n+  }\n@@ -271,0 +196,27 @@\n+class ZConcurrentRootsIteratorThreadClosure : public ThreadClosure {\n+private:\n+  ZRootsIteratorClosure* const _cl;\n+  \/\/ The resource mark is needed because interpreter oop maps are not reused in concurrent mode.\n+  \/\/ Instead, they are temporary and resource allocated.\n+  ResourceMark                 _rm;\n+\n+public:\n+  ZConcurrentRootsIteratorThreadClosure(ZRootsIteratorClosure* cl) :\n+      _cl(cl) {}\n+\n+  virtual void do_thread(Thread* thread) {\n+    _cl->do_thread(thread);\n+  }\n+};\n+\n+void ZConcurrentRootsIterator::do_code_cache(ZRootsIteratorClosure* cl) {\n+  ZStatTimer timer(ZSubPhaseConcurrentRootsCodeCache);\n+  ZNMethod::oops_do(cl, cl->should_disarm_nmethods());\n+}\n+\n+void ZConcurrentRootsIterator::do_java_threads(ZRootsIteratorClosure* cl) {\n+  ZStatTimer timer(ZSubPhaseConcurrentRootsJavaThreads);\n+  ZConcurrentRootsIteratorThreadClosure thread_cl(cl);\n+  _java_threads_iter.threads_do(&thread_cl);\n+}\n+\n@@ -275,0 +227,4 @@\n+  _java_threads.oops_do(cl);\n+  if (!ClassUnloading) {\n+    _code_cache.oops_do(cl);\n+  }\n","filename":"src\/hotspot\/share\/gc\/z\/zRootsIterator.cpp","additions":47,"deletions":91,"binary":false,"changes":138,"status":"modified"},{"patch":"@@ -110,2 +110,1 @@\n-  const bool           _visit_jvmti_weak_export;\n-  ZJavaThreadsIterator _java_threads_iter;\n+  const bool _visit_jvmti_weak_export;\n@@ -114,3 +113,0 @@\n-  void do_vm_thread(ZRootsIteratorClosure* cl);\n-  void do_java_threads(ZRootsIteratorClosure* cl);\n-  void do_code_cache(ZRootsIteratorClosure* cl);\n@@ -119,3 +115,0 @@\n-  ZSerialOopsDo<ZRootsIterator, &ZRootsIterator::do_vm_thread>           _vm_thread;\n-  ZParallelOopsDo<ZRootsIterator, &ZRootsIterator::do_java_threads>      _java_threads;\n-  ZParallelOopsDo<ZRootsIterator, &ZRootsIterator::do_code_cache>        _code_cache;\n@@ -125,1 +118,0 @@\n-  ~ZRootsIterator();\n@@ -133,0 +125,1 @@\n+  ZJavaThreadsIterator         _java_threads_iter;\n@@ -136,0 +129,1 @@\n+  void do_java_threads(ZRootsIteratorClosure* cl);\n@@ -137,0 +131,1 @@\n+  void do_code_cache(ZRootsIteratorClosure* cl);\n@@ -140,0 +135,2 @@\n+  ZParallelOopsDo<ZConcurrentRootsIterator, &ZConcurrentRootsIterator::do_java_threads>            _java_threads;\n+  ZParallelOopsDo<ZConcurrentRootsIterator, &ZConcurrentRootsIterator::do_code_cache>              _code_cache;\n","filename":"src\/hotspot\/share\/gc\/z\/zRootsIterator.hpp","additions":6,"deletions":9,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -0,0 +1,100 @@\n+\/*\n+ * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/z\/zAddress.hpp\"\n+#include \"gc\/z\/zBarrier.inline.hpp\"\n+#include \"gc\/z\/zOopClosures.inline.hpp\"\n+#include \"gc\/z\/zStackWatermark.hpp\"\n+#include \"gc\/z\/zThread.inline.hpp\"\n+#include \"gc\/z\/zThreadLocalAllocBuffer.hpp\"\n+#include \"gc\/z\/zThreadLocalData.hpp\"\n+#include \"gc\/z\/zVerify.hpp\"\n+#include \"memory\/resourceArea.inline.hpp\"\n+#include \"runtime\/frame.inline.hpp\"\n+#include \"utilities\/preserveException.hpp\"\n+\n+ZOnStackCodeBlobClosure::ZOnStackCodeBlobClosure() :\n+    _bs_nm(BarrierSet::barrier_set()->barrier_set_nmethod()) {}\n+\n+void ZOnStackCodeBlobClosure::do_code_blob(CodeBlob* cb) {\n+  nmethod* const nm = cb->as_nmethod_or_null();\n+  if (nm != NULL) {\n+    const bool result = _bs_nm->nmethod_entry_barrier(nm);\n+    assert(result, \"NMethod on-stack must be alive\");\n+  }\n+}\n+\n+ThreadLocalAllocStats& ZStackWatermark::stats() {\n+  return _stats;\n+}\n+\n+uint32_t ZStackWatermark::epoch_id() const {\n+  return *ZAddressBadMaskHighOrderBitsAddr;\n+}\n+\n+ZStackWatermark::ZStackWatermark(JavaThread* jt) :\n+    StackWatermark(jt, StackWatermarkKind::gc, *ZAddressBadMaskHighOrderBitsAddr),\n+    _jt_cl(),\n+    _cb_cl(),\n+    _stats() {}\n+\n+OopClosure* ZStackWatermark::closure_from_context(void* context) {\n+  if (context != NULL) {\n+    assert(ZThread::is_worker(), \"Unexpected thread passing in context: \" PTR_FORMAT, p2i(context));\n+    return reinterpret_cast<OopClosure*>(context);\n+  } else {\n+    return &_jt_cl;\n+  }\n+}\n+\n+void ZStackWatermark::start_processing_impl(void* context) {\n+  \/\/ Verify the head (no_frames) of the thread is bad before fixing it.\n+  ZVerify::verify_thread_head_bad(_jt);\n+\n+  \/\/ Process the non-frame part of the thread\n+  _jt->oops_do_no_frames(closure_from_context(context), &_cb_cl);\n+  ZThreadLocalData::do_invisible_root(_jt, ZBarrier::load_barrier_on_invisible_root_oop_field);\n+\n+  \/\/ Verification of frames is done after processing of the \"head\" (no_frames).\n+  \/\/ The reason is that the exception oop is fiddled with during frame processing.\n+  ZVerify::verify_thread_frames_bad(_jt);\n+\n+  \/\/ Update thread local address bad mask\n+  ZThreadLocalData::set_address_bad_mask(_jt, ZAddressBadMask);\n+\n+  \/\/ Retire TLAB\n+  if (ZGlobalPhase == ZPhaseMark) {\n+    ZThreadLocalAllocBuffer::retire(_jt, &_stats);\n+  } else {\n+    ZThreadLocalAllocBuffer::remap(_jt);\n+  }\n+\n+  \/\/ Publishes the processing start to concurrent threads\n+  StackWatermark::start_processing_impl(context);\n+}\n+\n+void ZStackWatermark::process(const frame& fr, RegisterMap& register_map, void* context) {\n+  ZVerify::verify_frame_bad(fr, register_map);\n+  fr.oops_do(closure_from_context(context), &_cb_cl, &register_map, DerivedPointerIterationMode::_directly);\n+}\n","filename":"src\/hotspot\/share\/gc\/z\/zStackWatermark.cpp","additions":100,"deletions":0,"binary":false,"changes":100,"status":"added"},{"patch":"@@ -0,0 +1,67 @@\n+\/*\n+ * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_Z_ZSTACKWATERMARK_HPP\n+#define SHARE_GC_Z_ZSTACKWATERMARK_HPP\n+\n+#include \"gc\/shared\/barrierSet.hpp\"\n+#include \"gc\/shared\/barrierSetNMethod.hpp\"\n+#include \"gc\/z\/zOopClosures.hpp\"\n+#include \"memory\/allocation.hpp\"\n+#include \"memory\/iterator.hpp\"\n+#include \"oops\/oopsHierarchy.hpp\"\n+#include \"runtime\/stackWatermark.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+class frame;\n+class JavaThread;\n+\n+class ZOnStackCodeBlobClosure : public CodeBlobClosure {\n+private:\n+  BarrierSetNMethod* _bs_nm;\n+\n+  virtual void do_code_blob(CodeBlob* cb);\n+\n+public:\n+  ZOnStackCodeBlobClosure();\n+};\n+\n+class ZStackWatermark : public StackWatermark {\n+private:\n+  ZLoadBarrierOopClosure  _jt_cl;\n+  ZOnStackCodeBlobClosure _cb_cl;\n+  ThreadLocalAllocStats   _stats;\n+\n+  OopClosure* closure_from_context(void* context);\n+\n+  virtual uint32_t epoch_id() const;\n+  virtual void start_processing_impl(void* context);\n+  virtual void process(const frame& fr, RegisterMap& register_map, void* context);\n+\n+public:\n+  ZStackWatermark(JavaThread* jt);\n+\n+  ThreadLocalAllocStats& stats();\n+};\n+\n+#endif \/\/ SHARE_GC_Z_ZSTACKWATERMARK_HPP\n","filename":"src\/hotspot\/share\/gc\/z\/zStackWatermark.hpp","additions":67,"deletions":0,"binary":false,"changes":67,"status":"added"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2020, Oracle and\/or its affiliates. All rights reserved.\n@@ -26,0 +26,1 @@\n+#include \"gc\/z\/zStackWatermark.hpp\"\n@@ -29,0 +30,1 @@\n+#include \"runtime\/stackWatermarkSet.inline.hpp\"\n@@ -67,3 +69,3 @@\n-void ZThreadLocalAllocBuffer::retire(Thread* thread) {\n-  if (UseTLAB && thread->is_Java_thread()) {\n-    ThreadLocalAllocStats* const stats = _stats->addr();\n+void ZThreadLocalAllocBuffer::retire(JavaThread* thread, ThreadLocalAllocStats* stats) {\n+  if (UseTLAB) {\n+    stats->reset();\n@@ -78,2 +80,2 @@\n-void ZThreadLocalAllocBuffer::remap(Thread* thread) {\n-  if (UseTLAB && thread->is_Java_thread()) {\n+void ZThreadLocalAllocBuffer::remap(JavaThread* thread) {\n+  if (UseTLAB) {\n@@ -83,0 +85,7 @@\n+\n+void ZThreadLocalAllocBuffer::update_stats(JavaThread* thread) {\n+  if (UseTLAB) {\n+    ZStackWatermark* const watermark = StackWatermarkSet::get<ZStackWatermark>(thread, StackWatermarkKind::gc);\n+    _stats->addr()->update(watermark->stats());\n+  }\n+}\n","filename":"src\/hotspot\/share\/gc\/z\/zThreadLocalAllocBuffer.cpp","additions":15,"deletions":6,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2020, Oracle and\/or its affiliates. All rights reserved.\n@@ -31,0 +31,2 @@\n+class JavaThread;\n+\n@@ -41,2 +43,3 @@\n-  static void retire(Thread* thread);\n-  static void remap(Thread* thread);\n+  static void retire(JavaThread* thread, ThreadLocalAllocStats* stats);\n+  static void remap(JavaThread* thread);\n+  static void update_stats(JavaThread* thread);\n","filename":"src\/hotspot\/share\/gc\/z\/zThreadLocalAllocBuffer.hpp","additions":6,"deletions":3,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2020, Oracle and\/or its affiliates. All rights reserved.\n@@ -87,1 +87,1 @@\n-    return address_bad_mask_offset() + in_ByteSize(ZNMethodDisarmedOffset);\n+    return address_bad_mask_offset() + in_ByteSize(ZAddressBadMaskHighOrderBitsOffset);\n","filename":"src\/hotspot\/share\/gc\/z\/zThreadLocalData.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"gc\/z\/zStackWatermark.hpp\"\n@@ -35,0 +36,1 @@\n+#include \"memory\/resourceArea.inline.hpp\"\n@@ -36,0 +38,4 @@\n+#include \"runtime\/frame.inline.hpp\"\n+#include \"runtime\/stackWatermark.inline.hpp\"\n+#include \"runtime\/stackWatermarkSet.inline.hpp\"\n+#include \"utilities\/preserveException.hpp\"\n@@ -58,0 +64,3 @@\n+private:\n+  const bool _verify_fixed;\n+\n@@ -59,0 +68,3 @@\n+  ZVerifyRootClosure(bool verify_fixed) :\n+      _verify_fixed(verify_fixed) {}\n+\n@@ -60,1 +72,8 @@\n-    z_verify_oop(p);\n+    if (_verify_fixed) {\n+      z_verify_oop(p);\n+    } else {\n+      \/\/ Don't know the state of the oop.\n+      oop obj = *p;\n+      obj = NativeAccess<AS_NO_KEEPALIVE>::oop_load(&obj);\n+      z_verify_oop(&obj);\n+    }\n@@ -66,0 +85,86 @@\n+\n+  virtual void do_thread(Thread* thread);\n+\n+  bool verify_fixed() const {\n+    return _verify_fixed;\n+  }\n+};\n+\n+class ZVerifyCodeBlobClosure : public CodeBlobToOopClosure {\n+public:\n+  ZVerifyCodeBlobClosure(ZVerifyRootClosure* _cl) :\n+      CodeBlobToOopClosure(_cl, false \/* fix_relocations *\/) {}\n+\n+  virtual void do_code_blob(CodeBlob* cb) {\n+    CodeBlobToOopClosure::do_code_blob(cb);\n+  }\n+};\n+\n+class ZVerifyStack : public OopClosure {\n+private:\n+  ZVerifyRootClosure* const _cl;\n+  JavaThread*         const _jt;\n+  uint64_t                  _last_good;\n+  bool                      _verifying_bad_frames;\n+\n+public:\n+  ZVerifyStack(ZVerifyRootClosure* cl, JavaThread* jt) :\n+      _cl(cl),\n+      _jt(jt),\n+      _last_good(0),\n+      _verifying_bad_frames(false) {\n+    ZStackWatermark* const stack_watermark = StackWatermarkSet::get<ZStackWatermark>(jt, StackWatermarkKind::gc);\n+\n+    if (_cl->verify_fixed()) {\n+      assert(stack_watermark->processing_started(), \"Should already have been fixed\");\n+      assert(stack_watermark->processing_completed(), \"Should already have been fixed\");\n+    } else {\n+      \/\/ We don't really know the state of the stack, verify watermark.\n+      if (!stack_watermark->processing_started()) {\n+        _verifying_bad_frames = true;\n+      } else {\n+        \/\/ Not time yet to verify bad frames\n+        _last_good = stack_watermark->last_processed();\n+      }\n+    }\n+  }\n+\n+  void do_oop(oop* p) {\n+    if (_verifying_bad_frames) {\n+      const oop obj = *p;\n+      guarantee(!ZAddress::is_good(ZOop::to_address(obj)), BAD_OOP_ARG(obj, p));\n+    }\n+    _cl->do_oop(p);\n+  }\n+\n+  void do_oop(narrowOop* p) {\n+    ShouldNotReachHere();\n+  }\n+\n+  void prepare_next_frame(frame& frame) {\n+    if (_cl->verify_fixed()) {\n+      \/\/ All frames need to be good\n+      return;\n+    }\n+\n+    \/\/ The verification has two modes, depending on whether we have reached the\n+    \/\/ last processed frame or not. Before it is reached, we expect everything to\n+    \/\/ be good. After reaching it, we expect everything to be bad.\n+    const uintptr_t sp = reinterpret_cast<uintptr_t>(frame.sp());\n+\n+    if (!_verifying_bad_frames && sp == _last_good) {\n+      \/\/ Found the last good frame, now verify the bad ones\n+      _verifying_bad_frames = true;\n+    }\n+  }\n+\n+  void verify_frames() {\n+    ZVerifyCodeBlobClosure cb_cl(_cl);\n+    for (StackFrameStream frames(_jt, true \/* update *\/, false \/* process_frames *\/);\n+         !frames.is_done();\n+         frames.next()) {\n+      frame& frame = *frames.current();\n+      frame.oops_do(this, &cb_cl, frames.register_map(), DerivedPointerIterationMode::_ignore);\n+      prepare_next_frame(frame);\n+    }\n+  }\n@@ -68,0 +173,12 @@\n+void ZVerifyRootClosure::do_thread(Thread* thread) {\n+  thread->oops_do_no_frames(this, NULL);\n+\n+  JavaThread* const jt = thread->as_Java_thread();\n+  if (!jt->has_last_Java_frame()) {\n+    return;\n+  }\n+\n+  ZVerifyStack verify_stack(this, jt);\n+  verify_stack.verify_frames();\n+}\n+\n@@ -104,1 +221,1 @@\n-void ZVerify::roots() {\n+void ZVerify::roots(bool verify_fixed) {\n@@ -109,1 +226,1 @@\n-    ZVerifyRootClosure cl;\n+    ZVerifyRootClosure cl(verify_fixed);\n@@ -116,1 +233,1 @@\n-  roots<ZRootsIterator>();\n+  roots<ZRootsIterator>(true \/* verify_fixed *\/);\n@@ -120,1 +237,1 @@\n-  roots<ZWeakRootsIterator>();\n+  roots<ZWeakRootsIterator>(true \/* verify_fixed *\/);\n@@ -123,2 +240,2 @@\n-void ZVerify::roots_concurrent_strong() {\n-  roots<ZConcurrentRootsIteratorClaimNone>();\n+void ZVerify::roots_concurrent_strong(bool verify_fixed) {\n+  roots<ZConcurrentRootsIteratorClaimNone>(verify_fixed);\n@@ -128,1 +245,1 @@\n-  roots<ZConcurrentWeakRootsIterator>();\n+  roots<ZConcurrentWeakRootsIterator>(true \/* verify_fixed *\/);\n@@ -131,1 +248,1 @@\n-void ZVerify::roots(bool verify_weaks) {\n+void ZVerify::roots(bool verify_concurrent_strong, bool verify_weaks) {\n@@ -133,1 +250,1 @@\n-  roots_concurrent_strong();\n+  roots_concurrent_strong(verify_concurrent_strong);\n@@ -152,2 +269,2 @@\n-void ZVerify::roots_and_objects(bool verify_weaks) {\n-  roots(verify_weaks);\n+void ZVerify::roots_and_objects(bool verify_concurrent_strong, bool verify_weaks) {\n+  roots(verify_concurrent_strong, verify_weaks);\n@@ -160,1 +277,1 @@\n-  roots_strong();\n+  roots(false \/* verify_concurrent_strong *\/, false \/* verify_weaks *\/);\n@@ -166,1 +283,1 @@\n-  roots_and_objects(false \/* verify_weaks *\/);\n+  roots_and_objects(true \/* verify_concurrent_strong*\/, false \/* verify_weaks *\/);\n@@ -172,1 +289,1 @@\n-  roots_and_objects(true \/* verify_weaks *\/);\n+  roots_and_objects(true \/* verify_concurrent_strong*\/, true \/* verify_weaks *\/);\n@@ -209,0 +326,56 @@\n+\n+#ifdef ASSERT\n+\n+class ZVerifyBadOopClosure : public OopClosure {\n+public:\n+  virtual void do_oop(oop* p) {\n+    const oop o = *p;\n+    assert(!ZAddress::is_good(ZOop::to_address(o)), \"Should not be good: \" PTR_FORMAT, p2i(o));\n+  }\n+\n+  virtual void do_oop(narrowOop* p) {\n+    ShouldNotReachHere();\n+  }\n+};\n+\n+\/\/ This class encapsulates various marks we need to deal with calling the\n+\/\/ frame iteration code from arbitrary points in the runtime. It is mostly\n+\/\/ due to problems that we might want to eventually clean up inside of the\n+\/\/ frame iteration code, such as creating random handles even though there\n+\/\/ is no safepoint to protect against, and fiddling around with exceptions.\n+class StackWatermarkProcessingMark {\n+  ResetNoHandleMark     _rnhm;\n+  HandleMark            _hm;\n+  PreserveExceptionMark _pem;\n+  ResourceMark          _rm;\n+\n+public:\n+  StackWatermarkProcessingMark(Thread* thread) :\n+    _rnhm(),\n+    _hm(thread),\n+    _pem(thread),\n+    _rm(thread) { }\n+};\n+\n+void ZVerify::verify_frame_bad(const frame& fr, RegisterMap& register_map) {\n+  ZVerifyBadOopClosure verify_cl;\n+  fr.oops_do(&verify_cl, NULL, &register_map, DerivedPointerIterationMode::_ignore);\n+}\n+\n+void ZVerify::verify_thread_head_bad(JavaThread* jt) {\n+  ZVerifyBadOopClosure verify_cl;\n+  jt->oops_do_no_frames(&verify_cl, NULL);\n+}\n+\n+void ZVerify::verify_thread_frames_bad(JavaThread* jt) {\n+  if (jt->has_last_Java_frame()) {\n+    ZVerifyBadOopClosure verify_cl;\n+    StackWatermarkProcessingMark swpm(Thread::current());\n+    \/\/ Traverse the execution stack\n+    for (StackFrameStream fst(jt, true \/* update *\/, false \/* process_frames *\/); !fst.is_done(); fst.next()) {\n+      fst.current()->oops_do(&verify_cl, NULL \/* code_cl *\/, fst.register_map(), DerivedPointerIterationMode::_ignore);\n+    }\n+  }\n+}\n+\n+#endif \/\/ ASSERT\n","filename":"src\/hotspot\/share\/gc\/z\/zVerify.cpp","additions":188,"deletions":15,"binary":false,"changes":203,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2019, 2020, Oracle and\/or its affiliates. All rights reserved.\n@@ -29,0 +29,1 @@\n+class frame;\n@@ -33,1 +34,1 @@\n-  template <typename RootsIterator> static void roots();\n+  template <typename RootsIterator> static void roots(bool verify_fixed);\n@@ -37,1 +38,1 @@\n-  static void roots_concurrent_strong();\n+  static void roots_concurrent_strong(bool verify_fixed);\n@@ -40,1 +41,1 @@\n-  static void roots(bool verify_weaks);\n+  static void roots(bool verify_concurrent_strong, bool verify_weaks);\n@@ -42,1 +43,1 @@\n-  static void roots_and_objects(bool verify_weaks);\n+  static void roots_and_objects(bool verify_concurrent_strong, bool verify_weaks);\n@@ -48,0 +49,4 @@\n+\n+  static void verify_thread_head_bad(JavaThread* thread) NOT_DEBUG_RETURN;\n+  static void verify_thread_frames_bad(JavaThread* thread) NOT_DEBUG_RETURN;\n+  static void verify_frame_bad(const frame& fr, RegisterMap& register_map) NOT_DEBUG_RETURN;\n","filename":"src\/hotspot\/share\/gc\/z\/zVerify.hpp","additions":10,"deletions":5,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -67,0 +67,1 @@\n+#include \"runtime\/stackWatermarkSet.hpp\"\n@@ -450,0 +451,4 @@\n+  \/\/ We get here after we have unwound from a callee throwing an exception\n+  \/\/ into the interpreter. Any deferred stack processing is notified of\n+  \/\/ the event via the StackWatermarkSet.\n+  StackWatermarkSet::after_unwind(thread);\n@@ -1156,0 +1161,5 @@\n+    \/\/ This function is called by the interpreter when single stepping. Such single\n+    \/\/ stepping could unwind a frame. Then, it is important that we process any frames\n+    \/\/ that we might return into.\n+    StackWatermarkSet::before_unwind(thread);\n+\n@@ -1164,0 +1174,14 @@\n+JRT_ENTRY(void, InterpreterRuntime::at_unwind(JavaThread* thread))\n+  \/\/ JRT_END does an implicit safepoint check, hence we are guaranteed to block\n+  \/\/ if this is called during a safepoint\n+\n+  \/\/ This function is called by the interpreter when the return poll found a reason\n+  \/\/ to call the VM. The reason could be that we are returning into a not yet safe\n+  \/\/ to access frame. We handle that below.\n+  \/\/ Note that this path does not check for single stepping, because we do not want\n+  \/\/ to single step when unwinding frames for an exception being thrown. Instead,\n+  \/\/ such single stepping code will use the safepoint table, which will use the\n+  \/\/ InterpreterRuntime::at_safepoint callback.\n+  StackWatermarkSet::before_unwind(thread);\n+JRT_END\n+\n","filename":"src\/hotspot\/share\/interpreter\/interpreterRuntime.cpp","additions":24,"deletions":0,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -116,0 +116,1 @@\n+  static void    at_unwind(JavaThread* thread);\n","filename":"src\/hotspot\/share\/interpreter\/interpreterRuntime.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -261,3 +261,0 @@\n-    \/\/ traverse the registered growable array gc_array\n-    \/\/ can't do this as it is not reachable from outside\n-\n@@ -275,1 +272,1 @@\n-    for (StackFrameStream fst(jt); !fst.is_done(); fst.next()) {\n+    for (StackFrameStream fst(jt, true \/* update *\/, true \/* process_frames *\/); !fst.is_done(); fst.next()) {\n@@ -300,1 +297,0 @@\n-  f->do_oop((oop*) &_threadObj);\n","filename":"src\/hotspot\/share\/jfr\/leakprofiler\/checkpoint\/rootResolver.cpp","additions":1,"deletions":5,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -39,1 +39,1 @@\n-  RegisterMap map(_thread, false);\n+  RegisterMap map(_thread, false, false);\n","filename":"src\/hotspot\/share\/jfr\/periodic\/sampling\/jfrCallTrace.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -137,1 +137,1 @@\n-  vframeStreamSamples(JavaThread *jt, frame fr, bool stop_at_java_call_stub) : vframeStreamCommon(jt) {\n+  vframeStreamSamples(JavaThread *jt, frame fr, bool stop_at_java_call_stub) : vframeStreamCommon(jt, false \/* process_frames *\/) {\n@@ -236,1 +236,1 @@\n-  vframeStream vfs(thread);\n+  vframeStream vfs(thread, false \/* stop_at_java_call_stub *\/, false \/* process_frames *\/);\n","filename":"src\/hotspot\/share\/jfr\/recorder\/stacktrace\/jfrStackTrace.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1226,1 +1226,1 @@\n-  StackFrameStream fst(thread);\n+  StackFrameStream fst(thread, true \/* update *\/, true \/* process_frames *\/);\n@@ -1333,1 +1333,1 @@\n-          fst = StackFrameStream(thread);\n+          fst = StackFrameStream(thread, true \/* update *\/, true \/* process_frames *\/);\n@@ -1465,1 +1465,1 @@\n-  StackFrameStream fst(thread, false);\n+  StackFrameStream fst(thread, false \/* update *\/, true \/* process_frames *\/);\n@@ -1483,1 +1483,1 @@\n-  StackFrameStream fstAfterDeopt(thread);\n+  StackFrameStream fstAfterDeopt(thread, true \/* update *\/, true \/* process_frames *\/);\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompilerToVM.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -331,1 +331,1 @@\n-  nonstatic_field(Thread,                   _polling_page,                                    volatile void*)                        \\\n+  nonstatic_field(Thread,                   _poll_data,                                       SafepointMechanism::ThreadData)        \\\n@@ -343,0 +343,3 @@\n+  nonstatic_field(SafepointMechanism::ThreadData, _polling_word,                              volatile uintptr_t)                    \\\n+  nonstatic_field(SafepointMechanism::ThreadData, _polling_page,                              volatile uintptr_t)                    \\\n+                                                                                                                                     \\\n","filename":"src\/hotspot\/share\/jvmci\/vmStructs_jvmci.cpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -157,0 +157,1 @@\n+  LOG_TAG(stackbarrier) \\\n","filename":"src\/hotspot\/share\/logging\/logTag.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -227,0 +227,66 @@\n+volatile int C2SafepointPollStubTable::_stub_size = 0;\n+\n+Label& C2SafepointPollStubTable::add_safepoint(uintptr_t safepoint_offset) {\n+  C2SafepointPollStub* entry = new (Compile::current()->comp_arena()) C2SafepointPollStub(safepoint_offset);\n+  _safepoints.append(entry);\n+  return entry->_stub_label;\n+}\n+\n+void C2SafepointPollStubTable::emit(CodeBuffer& cb) {\n+  MacroAssembler masm(&cb);\n+  for (int i = _safepoints.length() - 1; i >= 0; i--) {\n+    \/\/ Make sure there is enough space in the code buffer\n+    if (cb.insts()->maybe_expand_to_ensure_remaining(PhaseOutput::MAX_inst_size) && cb.blob() == NULL) {\n+      ciEnv::current()->record_failure(\"CodeCache is full\");\n+      return;\n+    }\n+\n+    C2SafepointPollStub* entry = _safepoints.at(i);\n+    emit_stub(masm, entry);\n+  }\n+}\n+\n+int C2SafepointPollStubTable::stub_size_lazy() const {\n+  int size = Atomic::load(&_stub_size);\n+\n+  if (size != 0) {\n+    return size;\n+  }\n+\n+  Compile* const C = Compile::current();\n+  BufferBlob* const blob = C->output()->scratch_buffer_blob();\n+  CodeBuffer cb(blob->content_begin(), C->output()->scratch_buffer_code_size());\n+  MacroAssembler masm(&cb);\n+  C2SafepointPollStub* entry = _safepoints.at(0);\n+  emit_stub(masm, entry);\n+  size += cb.insts_size();\n+\n+  Atomic::store(&_stub_size, size);\n+\n+  return size;\n+}\n+\n+int C2SafepointPollStubTable::estimate_stub_size() const {\n+  if (_safepoints.length() == 0) {\n+    return 0;\n+  }\n+\n+  int result = stub_size_lazy() * _safepoints.length();\n+\n+#ifdef ASSERT\n+  Compile* const C = Compile::current();\n+  BufferBlob* const blob = C->output()->scratch_buffer_blob();\n+  int size = 0;\n+\n+  for (int i = _safepoints.length() - 1; i >= 0; i--) {\n+    CodeBuffer cb(blob->content_begin(), C->output()->scratch_buffer_code_size());\n+    MacroAssembler masm(&cb);\n+    C2SafepointPollStub* entry = _safepoints.at(i);\n+    emit_stub(masm, entry);\n+    size += cb.insts_size();\n+  }\n+  assert(size == result, \"stubs should not have variable size\");\n+#endif\n+\n+  return result;\n+}\n@@ -1238,0 +1304,1 @@\n+  stub_req += safepoint_poll_table()->estimate_stub_size();\n@@ -1740,0 +1807,4 @@\n+  \/\/ Fill in stubs for calling the runtime from safepoint polls.\n+  safepoint_poll_table()->emit(*cb);\n+  if (C->failing())  return;\n+\n","filename":"src\/hotspot\/share\/opto\/output.cpp","additions":71,"deletions":0,"binary":false,"changes":71,"status":"modified"},{"patch":"@@ -28,0 +28,3 @@\n+#include \"code\/debugInfo.hpp\"\n+#include \"code\/exceptionHandlerTable.hpp\"\n+#include \"metaprogramming\/enableIf.hpp\"\n@@ -31,2 +34,1 @@\n-#include \"code\/debugInfo.hpp\"\n-#include \"code\/exceptionHandlerTable.hpp\"\n+#include \"runtime\/vm_version.hpp\"\n@@ -73,0 +75,41 @@\n+class C2SafepointPollStubTable {\n+private:\n+  struct C2SafepointPollStub: public ResourceObj {\n+    uintptr_t _safepoint_offset;\n+    Label     _stub_label;\n+    Label     _trampoline_label;\n+    C2SafepointPollStub(uintptr_t safepoint_offset) :\n+      _safepoint_offset(safepoint_offset),\n+      _stub_label(),\n+      _trampoline_label() {}\n+  };\n+\n+  GrowableArray<C2SafepointPollStub*> _safepoints;\n+\n+  static volatile int _stub_size;\n+\n+  void emit_stub_impl(MacroAssembler& masm, C2SafepointPollStub* entry) const;\n+\n+  \/\/ The selection logic below relieves the need to add dummy files to unsupported platforms.\n+  template <bool enabled>\n+  typename EnableIf<enabled>::type\n+  select_emit_stub(MacroAssembler& masm, C2SafepointPollStub* entry) const {\n+    emit_stub_impl(masm, entry);\n+  }\n+\n+  template <bool enabled>\n+  typename EnableIf<!enabled>::type\n+  select_emit_stub(MacroAssembler& masm, C2SafepointPollStub* entry) const {}\n+\n+  void emit_stub(MacroAssembler& masm, C2SafepointPollStub* entry) const {\n+    select_emit_stub<VM_Version::supports_stack_watermark_barrier()>(masm, entry);\n+  }\n+\n+  int stub_size_lazy() const;\n+\n+public:\n+  Label& add_safepoint(uintptr_t safepoint_offset);\n+  int estimate_stub_size() const;\n+  void emit(CodeBuffer& cb);\n+};\n+\n@@ -81,0 +124,1 @@\n+  C2SafepointPollStubTable _safepoint_poll_table;\/\/ Table for safepoint polls\n@@ -129,0 +173,3 @@\n+  \/\/ Safepoint poll table\n+  C2SafepointPollStubTable* safepoint_poll_table() { return &_safepoint_poll_table; }\n+\n@@ -176,0 +223,1 @@\n+  int               scratch_buffer_code_size()  { return (address)scratch_locs_memory() - _scratch_buffer_blob->content_begin(); }\n","filename":"src\/hotspot\/share\/opto\/output.hpp","additions":50,"deletions":2,"binary":false,"changes":52,"status":"modified"},{"patch":"@@ -71,0 +71,1 @@\n+#include \"runtime\/stackWatermarkSet.hpp\"\n@@ -1289,1 +1290,0 @@\n-\n@@ -1467,0 +1467,5 @@\n+  \/\/ The frame we rethrow the exception to might not have been processed by the GC yet.\n+  \/\/ The stack watermark barrier takes care of detecting that and ensuring the frame\n+  \/\/ has updated oops.\n+  StackWatermarkSet::after_unwind(thread);\n+\n","filename":"src\/hotspot\/share\/opto\/runtime.cpp","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -94,1 +94,1 @@\n-                                     bool stop_at_java_call_stub) : vframeStreamCommon(jt) {\n+                                     bool stop_at_java_call_stub) : vframeStreamCommon(jt, false \/* process_frames *\/) {\n","filename":"src\/hotspot\/share\/prims\/forte.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -218,1 +218,1 @@\n-      for (StackFrameStream fst(jt, false); !fst.is_done(); fst.next()) {\n+      for (StackFrameStream fst(jt, false \/* update *\/, false \/* process_frames *\/); !fst.is_done(); fst.next()) {\n","filename":"src\/hotspot\/share\/prims\/jvmtiEventController.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"runtime\/thread.inline.hpp\"\n","filename":"src\/hotspot\/share\/prims\/jvmtiTrace.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -39,0 +39,1 @@\n+#include \"runtime\/stackWatermarkSet.hpp\"\n@@ -483,0 +484,5 @@\n+    \/\/ If we have to get back here for even more frames, then 1) the user did not supply\n+    \/\/ an accurate hint suggesting the depth of the stack walk, and 2) we are not just\n+    \/\/ peeking  at a few frames. Take the cost of flushing out any pending deferred GC\n+    \/\/ processing of the stack.\n+    StackWatermarkSet::finish_processing(jt, NULL \/* context *\/, StackWatermarkKind::gc);\n","filename":"src\/hotspot\/share\/prims\/stackwalk.cpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -860,1 +860,1 @@\n-        for (StackFrameStream fst(t, false); !fst.is_done(); fst.next()) {\n+        for (StackFrameStream fst(t, false \/* update *\/, true \/* process_frames *\/); !fst.is_done(); fst.next()) {\n","filename":"src\/hotspot\/share\/prims\/whitebox.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -188,0 +188,3 @@\n+  \/\/ Does platform support stack watermark barriers for concurrent stack processing?\n+  constexpr static bool supports_stack_watermark_barrier() { return false; }\n+\n","filename":"src\/hotspot\/share\/runtime\/abstract_vm_version.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -64,0 +64,1 @@\n+#include \"runtime\/stackWatermarkSet.hpp\"\n@@ -163,0 +164,7 @@\n+  if (exec_mode == Unpack_exception) {\n+    \/\/ When we get here, a callee has thrown an exception into a deoptimized\n+    \/\/ frame. That throw might have deferred stack watermark checking until\n+    \/\/ after unwinding. So we deal with such deferred requests here.\n+    StackWatermarkSet::after_unwind(thread);\n+  }\n+\n@@ -257,0 +265,4 @@\n+  \/\/ When we get here we are about to unwind the deoptee frame. In order to\n+  \/\/ catch not yet safe to use frames, the following stack watermark barrier\n+  \/\/ poll will make such frames safe to use.\n+  StackWatermarkSet::before_unwind(thread);\n@@ -1513,1 +1525,1 @@\n-    StackFrameStream sfs(thread, true);\n+    StackFrameStream sfs(thread, true \/* update *\/, true \/* process_frames *\/);\n","filename":"src\/hotspot\/share\/runtime\/deoptimization.cpp","additions":13,"deletions":1,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -56,1 +56,1 @@\n-RegisterMap::RegisterMap(JavaThread *thread, bool update_map) {\n+RegisterMap::RegisterMap(JavaThread *thread, bool update_map, bool process_frames) {\n@@ -59,0 +59,1 @@\n+  _process_frames = process_frames;\n@@ -71,0 +72,1 @@\n+  _process_frames        = map->process_frames();\n@@ -899,1 +901,2 @@\n-void frame::oops_code_blob_do(OopClosure* f, CodeBlobClosure* cf, const RegisterMap* reg_map) const {\n+void frame::oops_code_blob_do(OopClosure* f, CodeBlobClosure* cf, const RegisterMap* reg_map,\n+                              DerivedPointerIterationMode derived_mode) const {\n@@ -902,1 +905,1 @@\n-    OopMapSet::oops_do(this, reg_map, f);\n+    OopMapSet::oops_do(this, reg_map, f, derived_mode);\n@@ -1038,0 +1041,10 @@\n+void frame::oops_do(OopClosure* f, CodeBlobClosure* cf, const RegisterMap* map,\n+                    DerivedPointerIterationMode derived_mode) const {\n+  oops_do_internal(f, cf, map, true, derived_mode);\n+}\n+\n+void frame::oops_do(OopClosure* f, CodeBlobClosure* cf, const RegisterMap* map) const {\n+  oops_do_internal(f, cf, map, true, DerivedPointerTable::is_active() ?\n+                                     DerivedPointerIterationMode::_with_table :\n+                                     DerivedPointerIterationMode::_ignore);\n+}\n@@ -1039,1 +1052,2 @@\n-void frame::oops_do_internal(OopClosure* f, CodeBlobClosure* cf, const RegisterMap* map, bool use_interpreter_oop_map_cache) const {\n+void frame::oops_do_internal(OopClosure* f, CodeBlobClosure* cf, const RegisterMap* map,\n+                             bool use_interpreter_oop_map_cache, DerivedPointerIterationMode derived_mode) const {\n@@ -1052,1 +1066,1 @@\n-    oops_code_blob_do(f, cf, map);\n+    oops_code_blob_do(f, cf, map, derived_mode);\n@@ -1089,1 +1103,1 @@\n-  oops_do_internal(&VerifyOopClosure::verify_oop, NULL, map, false);\n+  oops_do_internal(&VerifyOopClosure::verify_oop, NULL, map, false, DerivedPointerIterationMode::_ignore);\n@@ -1222,1 +1236,1 @@\n-StackFrameStream::StackFrameStream(JavaThread *thread, bool update) : _reg_map(thread, update) {\n+StackFrameStream::StackFrameStream(JavaThread *thread, bool update, bool process_frames) : _reg_map(thread, update, process_frames) {\n","filename":"src\/hotspot\/share\/runtime\/frame.cpp","additions":21,"deletions":7,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -44,0 +44,5 @@\n+enum class DerivedPointerIterationMode {\n+  _with_table,\n+  _directly,\n+  _ignore\n+};\n@@ -369,1 +374,2 @@\n-  void oops_do_internal(OopClosure* f, CodeBlobClosure* cf, const RegisterMap* map, bool use_interpreter_oop_map_cache) const;\n+  void oops_do_internal(OopClosure* f, CodeBlobClosure* cf, const RegisterMap* map,\n+                        bool use_interpreter_oop_map_cache, DerivedPointerIterationMode derived_mode) const;\n@@ -371,1 +377,2 @@\n-  void oops_code_blob_do(OopClosure* f, CodeBlobClosure* cf, const RegisterMap* map) const;\n+  void oops_code_blob_do(OopClosure* f, CodeBlobClosure* cf, const RegisterMap* map,\n+                         DerivedPointerIterationMode derived_mode) const;\n@@ -375,1 +382,3 @@\n-  void oops_do(OopClosure* f, CodeBlobClosure* cf, const RegisterMap* map) const { oops_do_internal(f, cf, map, true); }\n+  void oops_do(OopClosure* f, CodeBlobClosure* cf, const RegisterMap* map,\n+               DerivedPointerIterationMode derived_mode) const;\n+  void oops_do(OopClosure* f, CodeBlobClosure* cf, const RegisterMap* map) const;\n@@ -442,2 +451,7 @@\n-\/\/ all (callee-saved) registers. Notice: If a thread is stopped at\n-\/\/ a safepoint, all registers are saved, not only the callee-saved ones.\n+\/\/ all (callee-saved) registers iff the update flag is set. It also\n+\/\/ automatically takes care of lazily applying deferred GC processing\n+\/\/ onto exposed frames, such that all oops are valid iff the process_frames\n+\/\/ flag is set.\n+\/\/\n+\/\/ Notice: If a thread is stopped at a safepoint, all registers are saved,\n+\/\/ not only the callee-saved ones.\n@@ -447,1 +461,3 @@\n-\/\/   for(StackFrameStream fst(thread); !fst.is_done(); fst.next()) {\n+\/\/   for(StackFrameStream fst(thread, true \/* update *\/, true \/* process_frames *\/);\n+\/\/       !fst.is_done();\n+\/\/       fst.next()) {\n@@ -457,1 +473,1 @@\n-   StackFrameStream(JavaThread *thread, bool update = true);\n+  StackFrameStream(JavaThread *thread, bool update, bool process_frames);\n","filename":"src\/hotspot\/share\/runtime\/frame.hpp","additions":23,"deletions":7,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"runtime\/stackWatermarkSet.hpp\"\n@@ -523,0 +524,4 @@\n+      if (!_handshakee->is_terminated()) {\n+        StackWatermarkSet::start_processing(_handshakee, StackWatermarkKind::gc);\n+      }\n+\n","filename":"src\/hotspot\/share\/runtime\/handshake.cpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -237,1 +237,1 @@\n-    StackFrameStream sfs(thread);\n+    StackFrameStream sfs(thread, true \/* update *\/, true \/* process_frames *\/);\n","filename":"src\/hotspot\/share\/runtime\/interfaceSupport.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -139,0 +139,8 @@\n+\n+  if (_thread->has_pending_exception() && _thread->has_last_Java_frame()) {\n+    \/\/ If we get here, the Java code threw an exception that unwound a frame.\n+    \/\/ It could be that the new frame anchor has not passed through the required\n+    \/\/ StackWatermark barriers. Therefore, we process any such deferred unwind\n+    \/\/ requests here.\n+    StackWatermarkSet::after_unwind(_thread);\n+  }\n","filename":"src\/hotspot\/share\/runtime\/javaCalls.cpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -248,1 +248,1 @@\n-  def(Service_lock                 , PaddedMonitor, special,     true,  _safepoint_check_never);      \/\/ used for service thread operations\n+  def(Service_lock                 , PaddedMonitor, tty-2,       true,  _safepoint_check_never);      \/\/ used for service thread operations\n@@ -325,1 +325,1 @@\n-  def(JfrStacktrace_lock           , PaddedMutex  , special - 1, true,  _safepoint_check_never);\n+  def(JfrStacktrace_lock           , PaddedMutex  , tty-2,       true,  _safepoint_check_never);\n","filename":"src\/hotspot\/share\/runtime\/mutexLocker.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"oops\/access.inline.hpp\"\n@@ -30,0 +31,1 @@\n+#include \"runtime\/synchronizer.hpp\"\n","filename":"src\/hotspot\/share\/runtime\/objectMonitor.inline.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -78,0 +78,1 @@\n+  bool        _process_frames;          \/\/ Should frames be processed by stack watermark barriers?\n@@ -87,1 +88,1 @@\n-  RegisterMap(JavaThread *thread, bool update_map = true);\n+  RegisterMap(JavaThread *thread, bool update_map = true, bool process_frames = true);\n@@ -117,2 +118,3 @@\n-  JavaThread *thread() const { return _thread; }\n-  bool update_map()    const { return _update_map; }\n+  JavaThread *thread()  const { return _thread; }\n+  bool update_map()     const { return _update_map; }\n+  bool process_frames() const { return _process_frames; }\n","filename":"src\/hotspot\/share\/runtime\/registerMap.hpp","additions":5,"deletions":3,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -61,0 +61,1 @@\n+#include \"runtime\/stackWatermarkSet.inline.hpp\"\n@@ -500,0 +501,9 @@\n+class ParallelSPCleanupThreadClosure : public ThreadClosure {\n+public:\n+  void do_thread(Thread* thread) {\n+    if (thread->is_Java_thread()) {\n+      StackWatermarkSet::start_processing(thread->as_Java_thread(), StackWatermarkKind::gc);\n+    }\n+  }\n+};\n+\n@@ -504,0 +514,1 @@\n+  bool _do_lazy_roots;\n@@ -525,1 +536,3 @@\n-    _num_workers(num_workers) {}\n+    _num_workers(num_workers),\n+    _do_lazy_roots(!VMThread::vm_operation()->skip_thread_oop_barriers() &&\n+                   Universe::heap()->uses_stack_watermark_barrier()) {}\n@@ -528,0 +541,6 @@\n+    if (_do_lazy_roots && _subtasks.try_claim_task(SafepointSynchronize::SAFEPOINT_CLEANUP_LAZY_ROOT_PROCESSING)) {\n+      Tracer t(\"lazy partial thread root processing\");\n+      ParallelSPCleanupThreadClosure cl;\n+      Threads::threads_do(&cl);\n+    }\n+\n@@ -776,1 +795,1 @@\n-  \/\/ cross_modify_fence is done by SafepointMechanism::process_operation_if_requested_slow\n+  \/\/ cross_modify_fence is done by SafepointMechanism::process_if_requested\n@@ -939,1 +958,1 @@\n-  RegisterMap map(thread(), true);\n+  RegisterMap map(thread(), true, false);\n@@ -964,0 +983,5 @@\n+    \/\/ We get here if compiled return polls found a reason to call into the VM.\n+    \/\/ One condition for that is that the top frame is not yet safe to use.\n+    \/\/ The following stack watermark barrier poll will catch such situations.\n+    StackWatermarkSet::after_unwind(thread());\n+\n@@ -995,1 +1019,1 @@\n-      RegisterMap map(thread(), true);\n+      RegisterMap map(thread(), true, false);\n","filename":"src\/hotspot\/share\/runtime\/safepoint.cpp","additions":28,"deletions":4,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -73,0 +73,1 @@\n+    SAFEPOINT_CLEANUP_LAZY_ROOT_PROCESSING,\n","filename":"src\/hotspot\/share\/runtime\/safepoint.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"runtime\/stackWatermarkSet.hpp\"\n@@ -34,2 +35,4 @@\n-void* SafepointMechanism::_poll_armed_value;\n-void* SafepointMechanism::_poll_disarmed_value;\n+uintptr_t SafepointMechanism::_poll_word_armed_value;\n+uintptr_t SafepointMechanism::_poll_word_disarmed_value;\n+uintptr_t SafepointMechanism::_poll_page_armed_value;\n+uintptr_t SafepointMechanism::_poll_page_disarmed_value;\n@@ -40,2 +43,4 @@\n-  intptr_t poll_armed_value = poll_bit();\n-  intptr_t poll_disarmed_value = 0;\n+  _poll_word_armed_value    = poll_bit();\n+  _poll_word_disarmed_value = ~_poll_word_armed_value;\n+\n+  bool poll_bit_only = false;\n@@ -44,1 +49,1 @@\n-  if (!USE_POLL_BIT_ONLY)\n+  poll_bit_only = USE_POLL_BIT_ONLY;\n@@ -46,1 +51,5 @@\n-  {\n+\n+  if (poll_bit_only) {\n+    _poll_page_armed_value    = poll_bit();\n+    _poll_page_disarmed_value = 0;\n+  } else {\n@@ -61,1 +70,0 @@\n-    _polling_page = (address)(bad_page);\n@@ -64,4 +72,3 @@\n-    intptr_t bad_page_val  = reinterpret_cast<intptr_t>(bad_page),\n-             good_page_val = reinterpret_cast<intptr_t>(good_page);\n-    poll_armed_value    |= bad_page_val;\n-    poll_disarmed_value |= good_page_val;\n+    _poll_page_armed_value    = reinterpret_cast<uintptr_t>(bad_page);\n+    _poll_page_disarmed_value = reinterpret_cast<uintptr_t>(good_page);\n+    _polling_page = (address)bad_page;\n@@ -69,3 +76,0 @@\n-\n-  _poll_armed_value    = reinterpret_cast<void*>(poll_armed_value);\n-  _poll_disarmed_value = reinterpret_cast<void*>(poll_disarmed_value);\n@@ -81,0 +85,9 @@\n+\n+  \/\/ The call to start_processing fixes the thread's oops and the first few frames.\n+  \/\/\n+  \/\/ The call has been carefully placed here to cater for a few situations:\n+  \/\/ 1) After we exit from block after a global poll\n+  \/\/ 2) After a thread races with the disarming of the global poll and transitions from native\/blocked\n+  \/\/ 3) Before the handshake code is run\n+  StackWatermarkSet::start_processing(thread, StackWatermarkKind::gc);\n+\n@@ -86,0 +99,32 @@\n+uintptr_t SafepointMechanism::compute_poll_word(bool armed, uintptr_t stack_watermark) {\n+  if (armed) {\n+    log_debug(stackbarrier)(\"Computed armed for tid %d\", Thread::current()->osthread()->thread_id());\n+    return _poll_word_armed_value;\n+  }\n+  if (stack_watermark == 0) {\n+    log_debug(stackbarrier)(\"Computed disarmed for tid %d\", Thread::current()->osthread()->thread_id());\n+    return _poll_word_disarmed_value;\n+  }\n+  log_debug(stackbarrier)(\"Computed watermark for tid %d\", Thread::current()->osthread()->thread_id());\n+  return stack_watermark;\n+}\n+\n+void SafepointMechanism::update_poll_values(JavaThread* thread) {\n+  for (;;) {\n+    bool armed = global_poll() || thread->handshake_state()->has_operation();\n+    uintptr_t stack_watermark = StackWatermarkSet::lowest_watermark(thread);\n+    uintptr_t poll_page = armed ? _poll_page_armed_value\n+                                : _poll_page_disarmed_value;\n+    uintptr_t poll_word = compute_poll_word(armed, stack_watermark);\n+    thread->poll_data()->set_polling_page(poll_page);\n+    thread->poll_data()->set_polling_word(poll_word);\n+    OrderAccess::fence();\n+    if (!armed && (global_poll() || thread->handshake_state()->has_operation())) {\n+      \/\/ We disarmed an old safepoint, but a new one is synchronizing.\n+      \/\/ We need to arm the poll for the subsequent safepoint poll.\n+      continue;\n+    }\n+    break;\n+  }\n+}\n+\n@@ -92,12 +137,1 @@\n-\n-  OrderAccess::loadload();\n-\n-  if (local_poll_armed(thread)) {\n-    disarm_local_poll_release(thread);\n-    \/\/ We might have disarmed next safepoint\/handshake\n-    OrderAccess::storeload();\n-    if (global_poll() || thread->handshake_state()->has_operation()) {\n-      arm_local_poll(thread);\n-    }\n-  }\n-\n+  update_poll_values(thread);\n","filename":"src\/hotspot\/share\/runtime\/safepointMechanism.cpp","additions":60,"deletions":26,"binary":false,"changes":86,"status":"modified"},{"patch":"@@ -33,0 +33,3 @@\n+class JavaThread;\n+class Thread;\n+\n@@ -35,3 +38,3 @@\n-  static void* _poll_armed_value;\n-  static void* _poll_disarmed_value;\n-  static address _polling_page;\n+  friend class StackWatermark;\n+  static uintptr_t _poll_page_armed_value;\n+  static uintptr_t _poll_page_disarmed_value;\n@@ -39,2 +42,4 @@\n-  static void* poll_armed_value()                     { return _poll_armed_value; }\n-  static void* poll_disarmed_value()                  { return _poll_disarmed_value; }\n+  static uintptr_t _poll_word_armed_value;\n+  static uintptr_t _poll_word_disarmed_value;\n+\n+  static address _polling_page;\n@@ -44,1 +49,0 @@\n-  static inline void disarm_local_poll_release(JavaThread* thread);\n@@ -56,3 +60,3 @@\n-  \/\/ By adding 8 to the base address of the protected polling page we can differentiate\n-  \/\/ between the armed and disarmed value by masking out this bit.\n-  const static intptr_t _poll_bit = 8;\n+  static uintptr_t compute_poll_word(bool armed, uintptr_t stack_watermark);\n+\n+  const static intptr_t _poll_bit = 1;\n@@ -66,0 +70,11 @@\n+  struct ThreadData {\n+    volatile uintptr_t _polling_word;\n+    volatile uintptr_t _polling_page;\n+\n+    inline void set_polling_word(uintptr_t poll_value);\n+    inline uintptr_t get_polling_word();\n+\n+    inline void set_polling_page(uintptr_t poll_value);\n+    inline uintptr_t get_polling_page();\n+  };\n+\n@@ -71,0 +86,2 @@\n+  \/\/ Compute what the poll values should be and install them.\n+  static void update_poll_values(JavaThread* thread);\n","filename":"src\/hotspot\/share\/runtime\/safepointMechanism.hpp","additions":26,"deletions":9,"binary":false,"changes":35,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"runtime\/atomic.hpp\"\n@@ -32,0 +33,22 @@\n+\/\/ Caller is responsible for using a memory barrier if needed.\n+inline void SafepointMechanism::ThreadData::set_polling_page(uintptr_t poll_value) {\n+  Atomic::store(&_polling_page, poll_value);\n+}\n+\n+\/\/ The acquire makes sure reading of polling page is done before\n+\/\/ the reading the handshake operation or the global state\n+inline uintptr_t SafepointMechanism::ThreadData::get_polling_page() {\n+  return Atomic::load_acquire(&_polling_page);\n+}\n+\n+\/\/ Caller is responsible for using a memory barrier if needed.\n+inline void SafepointMechanism::ThreadData::set_polling_word(uintptr_t poll_value) {\n+  Atomic::store(&_polling_word, poll_value);\n+}\n+\n+\/\/ The acquire makes sure reading of polling page is done before\n+\/\/ the reading the handshake operation or the global state\n+inline uintptr_t SafepointMechanism::ThreadData::get_polling_word() {\n+  return Atomic::load_acquire(&_polling_word);\n+}\n+\n@@ -33,2 +56,1 @@\n-  const intptr_t poll_word = reinterpret_cast<intptr_t>(thread->get_polling_page());\n-  return mask_bits_are_true(poll_word, poll_bit());\n+  return thread->poll_data()->get_polling_word() & poll_bit();\n@@ -62,1 +84,2 @@\n-  thread->set_polling_page(poll_armed_value());\n+  thread->poll_data()->set_polling_word(_poll_word_armed_value);\n+  thread->poll_data()->set_polling_page(_poll_page_armed_value);\n@@ -66,1 +89,2 @@\n-  thread->set_polling_page(poll_disarmed_value());\n+  thread->poll_data()->set_polling_word(_poll_word_disarmed_value);\n+  thread->poll_data()->set_polling_page(_poll_page_disarmed_value);\n@@ -70,5 +94,3 @@\n-  thread->set_polling_page_release(poll_armed_value());\n-}\n-\n-void SafepointMechanism::disarm_local_poll_release(JavaThread* thread) {\n-  thread->set_polling_page_release(poll_disarmed_value());\n+  OrderAccess::release();\n+  thread->poll_data()->set_polling_word(_poll_word_armed_value);\n+  thread->poll_data()->set_polling_page(_poll_page_armed_value);\n","filename":"src\/hotspot\/share\/runtime\/safepointMechanism.inline.hpp","additions":31,"deletions":9,"binary":false,"changes":40,"status":"modified"},{"patch":"@@ -260,2 +260,2 @@\n-void ServiceThread::oops_do(OopClosure* f, CodeBlobClosure* cf) {\n-  JavaThread::oops_do(f, cf);\n+void ServiceThread::oops_do_no_frames(OopClosure* f, CodeBlobClosure* cf) {\n+  JavaThread::oops_do_no_frames(f, cf);\n","filename":"src\/hotspot\/share\/runtime\/serviceThread.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -58,1 +58,1 @@\n-  void oops_do(OopClosure* f, CodeBlobClosure* cf);\n+  void oops_do_no_frames(OopClosure* f, CodeBlobClosure* cf);\n","filename":"src\/hotspot\/share\/runtime\/serviceThread.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -67,0 +67,1 @@\n+#include \"runtime\/stackWatermarkSet.hpp\"\n@@ -457,0 +458,6 @@\n+  \/\/ Note: This is called when we have unwound the frame of the callee that did\n+  \/\/ throw an exception. So far, no check has been performed by the StackWatermarkSet.\n+  \/\/ Notably, the stack is not walkable at this point, and hence the check must\n+  \/\/ be deferred until later. Specifically, any of the handlers returned here in\n+  \/\/ this function, will get dispatched to, and call deferred checks to\n+  \/\/ StackWatermarkSet::after_unwind at a point where the stack is walkable.\n@@ -489,0 +496,2 @@\n+      \/\/ The deferred StackWatermarkSet::after_unwind check will be performed in\n+      \/\/ Deoptimization::fetch_unroll_info (with exec_mode == Unpack_exception)\n@@ -491,0 +500,3 @@\n+      \/\/ The deferred StackWatermarkSet::after_unwind check will be performed in\n+      \/\/ * OptoRuntime::rethrow_C for C2 code\n+      \/\/ * exception_handler_for_pc_helper via Runtime1::handle_exception_from_callee_id for C1 code\n@@ -497,0 +509,2 @@\n+    \/\/ The deferred StackWatermarkSet::after_unwind check will be performed in\n+    \/\/ JavaCallWrapper::~JavaCallWrapper\n@@ -501,0 +515,2 @@\n+    \/\/ The deferred StackWatermarkSet::after_unwind check will be performed in\n+    \/\/ InterpreterRuntime::exception_handler_for_exception\n@@ -3036,0 +3052,6 @@\n+  \/\/ During OSR migration, we unwind the interpreted frame and replace it with a compiled\n+  \/\/ frame. The stack watermark code below ensures that the interpreted frame is processed\n+  \/\/ before it gets unwound. This is helpful as the size of the compiled frame could be\n+  \/\/ larger than the interpreted frame, which could result in the new frame not being\n+  \/\/ processed correctly.\n+  StackWatermarkSet::before_unwind(thread);\n","filename":"src\/hotspot\/share\/runtime\/sharedRuntime.cpp","additions":22,"deletions":0,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -0,0 +1,301 @@\n+\/*\n+ * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"logging\/log.hpp\"\n+#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/frame.inline.hpp\"\n+#include \"runtime\/safepoint.hpp\"\n+#include \"runtime\/stackWatermark.inline.hpp\"\n+#include \"runtime\/thread.hpp\"\n+#include \"utilities\/debug.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include \"utilities\/macros.hpp\"\n+#include \"utilities\/preserveException.hpp\"\n+\n+class StackWatermarkFramesIterator : public CHeapObj<mtInternal> {\n+  JavaThread* _jt;\n+  uintptr_t _caller;\n+  uintptr_t _callee;\n+  StackFrameStream _frame_stream;\n+  StackWatermark& _owner;\n+  bool _is_done;\n+\n+  void set_watermark(uintptr_t sp);\n+  RegisterMap& register_map();\n+  frame& current();\n+  void next();\n+\n+public:\n+  StackWatermarkFramesIterator(StackWatermark& owner);\n+  uintptr_t caller() const { return _caller; }\n+  uintptr_t callee() const { return _callee; }\n+  void process_one(void* context);\n+  void process_all(void* context);\n+  bool has_next() const;\n+};\n+\n+void StackWatermarkFramesIterator::set_watermark(uintptr_t sp) {\n+  if (!has_next()) {\n+    return;\n+  }\n+\n+  if (_callee == 0) {\n+    _callee = sp;\n+  } else if (_caller == 0) {\n+    _caller = sp;\n+  } else {\n+    _callee = _caller;\n+    _caller = sp;\n+  }\n+}\n+\n+\/\/ This class encapsulates various marks we need to deal with calling the\n+\/\/ frame processing code from arbitrary points in the runtime. It is mostly\n+\/\/ due to problems that we might want to eventually clean up inside of the\n+\/\/ frame processing code, such as creating random handles even though there\n+\/\/ is no safepoint to protect against, and fiddling around with exceptions.\n+class StackWatermarkProcessingMark {\n+  ResetNoHandleMark _rnhm;\n+  HandleMark _hm;\n+  PreserveExceptionMark _pem;\n+  ResourceMark _rm;\n+\n+public:\n+  StackWatermarkProcessingMark(Thread* thread) :\n+      _rnhm(),\n+      _hm(thread),\n+      _pem(thread),\n+      _rm(thread) { }\n+};\n+\n+void StackWatermarkFramesIterator::process_one(void* context) {\n+  StackWatermarkProcessingMark swpm(Thread::current());\n+  while (has_next()) {\n+    frame f = current();\n+    uintptr_t sp = reinterpret_cast<uintptr_t>(f.sp());\n+    bool frame_has_barrier = StackWatermark::has_barrier(f);\n+    _owner.process(f, register_map(), context);\n+    next();\n+    if (frame_has_barrier) {\n+      set_watermark(sp);\n+      break;\n+    }\n+  }\n+}\n+\n+void StackWatermarkFramesIterator::process_all(void* context) {\n+  const uintptr_t frames_per_poll_gc = 5;\n+\n+  ResourceMark rm;\n+  log_info(stackbarrier)(\"Processing whole stack for tid %d\",\n+                         _jt->osthread()->thread_id());\n+  uint i = 0;\n+  while (has_next()) {\n+    frame f = current();\n+    uintptr_t sp = reinterpret_cast<uintptr_t>(f.sp());\n+    assert(sp >= _caller, \"invariant\");\n+    bool frame_has_barrier = StackWatermark::has_barrier(f);\n+    _owner.process(f, register_map(), context);\n+    next();\n+    if (frame_has_barrier) {\n+      set_watermark(sp);\n+      if (++i == frames_per_poll_gc) {\n+        \/\/ Yield every N frames so mutator can progress faster.\n+        i = 0;\n+        _owner.yield_processing();\n+      }\n+    }\n+  }\n+}\n+\n+StackWatermarkFramesIterator::StackWatermarkFramesIterator(StackWatermark& owner) :\n+    _jt(owner._jt),\n+    _caller(0),\n+    _callee(0),\n+    _frame_stream(owner._jt, true \/* update_registers *\/, false \/* process_frames *\/),\n+    _owner(owner),\n+    _is_done(_frame_stream.is_done()) {\n+}\n+\n+frame& StackWatermarkFramesIterator::current() {\n+  return *_frame_stream.current();\n+}\n+\n+RegisterMap& StackWatermarkFramesIterator::register_map() {\n+  return *_frame_stream.register_map();\n+}\n+\n+bool StackWatermarkFramesIterator::has_next() const {\n+  return !_is_done;\n+}\n+\n+void StackWatermarkFramesIterator::next() {\n+  _frame_stream.next();\n+  _is_done = _frame_stream.is_done();\n+}\n+\n+StackWatermark::StackWatermark(JavaThread* jt, StackWatermarkKind kind, uint32_t epoch) :\n+    _state(StackWatermarkState::create(epoch, true \/* is_done *\/)),\n+    _watermark(0),\n+    _next(NULL),\n+    _jt(jt),\n+    _iterator(NULL),\n+    _lock(Mutex::tty - 1, \"stack_watermark_lock\", true, Mutex::_safepoint_check_never),\n+    _kind(kind) {\n+}\n+\n+StackWatermark::~StackWatermark() {\n+  delete _iterator;\n+}\n+\n+#ifdef ASSERT\n+void StackWatermark::assert_is_frame_safe(const frame& f) {\n+  MutexLocker ml(&_lock, Mutex::_no_safepoint_check_flag);\n+  assert(is_frame_safe(f), \"Frame must be safe\");\n+}\n+#endif\n+\n+\/\/ A frame is \"safe\" if it *and* its caller have been processed. This is the invariant\n+\/\/ that allows exposing a frame, and for that frame to directly access its caller frame\n+\/\/ without going through any hooks.\n+bool StackWatermark::is_frame_safe(const frame& f) {\n+  assert(_lock.owned_by_self(), \"Must be locked\");\n+  uint32_t state = Atomic::load(&_state);\n+  if (!processing_started(state)) {\n+    return false;\n+  }\n+  if (processing_completed(state)) {\n+    return true;\n+  }\n+  return reinterpret_cast<uintptr_t>(f.sp()) < _iterator->caller();\n+}\n+\n+void StackWatermark::start_processing_impl(void* context) {\n+  log_info(stackbarrier)(\"Starting stack processing for tid %d\",\n+                         _jt->osthread()->thread_id());\n+  delete _iterator;\n+  if (_jt->has_last_Java_frame()) {\n+    _iterator = new StackWatermarkFramesIterator(*this);\n+    \/\/ Always process three frames when starting an iteration.\n+    \/\/\n+    \/\/ The three frames corresponds to:\n+    \/\/ 1) The callee frame\n+    \/\/ 2) The caller frame\n+    \/\/ This allows a callee to always be able to read state from its caller\n+    \/\/ without needing any special barriers.\n+    \/\/\n+    \/\/ 3) An extra frame to deal with unwinding safepointing on the way out.\n+    \/\/ Sometimes, we also call into the runtime to on_unwind(), but then\n+    \/\/ hit a safepoint poll on the way out from the runtime.\n+    _iterator->process_one(context);\n+    _iterator->process_one(context);\n+    _iterator->process_one(context);\n+  } else {\n+    _iterator = NULL;\n+  }\n+  update_watermark();\n+}\n+\n+void StackWatermark::yield_processing() {\n+  update_watermark();\n+  MutexUnlocker mul(&_lock, Mutex::_no_safepoint_check_flag);\n+}\n+\n+void StackWatermark::update_watermark() {\n+  assert(_lock.owned_by_self(), \"invariant\");\n+  if (_iterator != NULL && _iterator->has_next()) {\n+    assert(_iterator->callee() != 0, \"sanity\");\n+    Atomic::release_store(&_watermark, _iterator->callee());\n+    Atomic::release_store(&_state, StackWatermarkState::create(epoch_id(), false \/* is_done *\/)); \/\/ release watermark w.r.t. epoch\n+  } else {\n+    Atomic::release_store(&_watermark, uintptr_t(0)); \/\/ Release stack data modifications w.r.t. watermark\n+    Atomic::release_store(&_state, StackWatermarkState::create(epoch_id(), true \/* is_done *\/)); \/\/ release watermark w.r.t. epoch\n+    log_info(stackbarrier)(\"Finished stack processing iteration for tid %d\",\n+                           _jt->osthread()->thread_id());\n+  }\n+}\n+\n+void StackWatermark::process_one() {\n+  MutexLocker ml(&_lock, Mutex::_no_safepoint_check_flag);\n+  if (!processing_started()) {\n+    start_processing_impl(NULL \/* context *\/);\n+  } else if (!processing_completed()) {\n+    _iterator->process_one(NULL \/* context *\/);\n+    update_watermark();\n+  }\n+}\n+\n+uintptr_t StackWatermark::watermark() {\n+  return Atomic::load_acquire(&_watermark);\n+}\n+\n+uintptr_t StackWatermark::last_processed() {\n+  MutexLocker ml(&_lock, Mutex::_no_safepoint_check_flag);\n+  if (!processing_started()) {\n+    \/\/ Stale state; no last processed\n+    return 0;\n+  }\n+  if (processing_completed()) {\n+    \/\/ Already processed all; no last processed\n+    return 0;\n+  }\n+  return _iterator->caller();\n+}\n+\n+bool StackWatermark::processing_started() const {\n+  return processing_started(Atomic::load(&_state));\n+}\n+\n+bool StackWatermark::processing_started_acquire() const {\n+  return processing_started(Atomic::load_acquire(&_state));\n+}\n+\n+bool StackWatermark::processing_completed() const {\n+  return processing_completed(Atomic::load(&_state));\n+}\n+\n+bool StackWatermark::processing_completed_acquire() const {\n+  return processing_completed(Atomic::load_acquire(&_state));\n+}\n+\n+void StackWatermark::start_processing() {\n+  if (!processing_started_acquire()) {\n+    MutexLocker ml(&_lock, Mutex::_no_safepoint_check_flag);\n+    if (!processing_started()) {\n+      start_processing_impl(NULL \/* context *\/);\n+    }\n+  }\n+}\n+\n+void StackWatermark::finish_processing(void* context) {\n+  MutexLocker ml(&_lock, Mutex::_no_safepoint_check_flag);\n+  if (!processing_started()) {\n+    start_processing_impl(context);\n+  }\n+  if (!processing_completed()) {\n+    _iterator->process_all(context);\n+    update_watermark();\n+  }\n+}\n","filename":"src\/hotspot\/share\/runtime\/stackWatermark.cpp","additions":301,"deletions":0,"binary":false,"changes":301,"status":"added"},{"patch":"@@ -0,0 +1,144 @@\n+\/*\n+ * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_RUNTIME_STACKWATERMARK_HPP\n+#define SHARE_RUNTIME_STACKWATERMARK_HPP\n+\n+#include \"memory\/allocation.hpp\"\n+#include \"runtime\/stackWatermarkKind.hpp\"\n+\n+class JavaThread;\n+class StackWatermarkFramesIterator;\n+\n+\/\/ The StackWatermark state is a tuple comprising the last epoch in which\n+\/\/ the watermark has been processed, and a boolean denoting whether the whole\n+\/\/ processing of the lazy snapshot has been processed or not. It is written\n+\/\/ in a way that can be used outside of locks, so that fast path checks can\n+\/\/ be performed without the need for any locking. The boolean can only be\n+\/\/ trusted if the epoch of the state is the same as the epoch_id() of the\n+\/\/ watermark. Incrementing the epoch_id() will implicitly initiate a new lazy\n+\/\/ stack snapshot, and trigger processing on it as needed, due to the cached\n+\/\/ epoch of the state being outdated. When the snapshot is_done for the current\n+\/\/ epoch_id(), there is no need to do anything further.\n+class StackWatermarkState : public AllStatic {\n+public:\n+  inline static bool is_done(uint32_t state) {\n+    return state & 1;\n+  }\n+\n+  inline static uint32_t epoch(uint32_t state) {\n+    return state >> 1;\n+  }\n+\n+  inline static uint32_t create(uint32_t epoch, bool is_done) {\n+    return (epoch << 1) | (is_done ? 1u : 0u);\n+  }\n+};\n+\n+\/\/ The StackWatermark allows lazy incremental concurrent processing of a\n+\/\/ snapshot of a stack. The lazy and incremental nature is implemented by\n+\/\/ marking a frame (the watermark) from which returns (or other forms of\n+\/\/ unwinding) will take a slow path to perform additional processing\n+\/\/ required when exposing more frames that were part of the snapshot to\n+\/\/ the system. The watermark pointer always denotes the SP of the watermark.\n+\/\/ However, active frames can grow and shrink arbitrarily compared to the\n+\/\/ snapshot view that is being processed, due to things like c2i adapters,\n+\/\/ and various register saving techniques to get into the runtime. Therefore,\n+\/\/ in order to cope with the frames growing and shrinking, comparisons\n+\/\/ against the watermark are performed with the frame pointer of a given\n+\/\/ frame against the watermark (denoting the SP).\n+\/\/\n+\/\/  ----------\n+\/\/ |          |\n+\/\/ |  caller  |\n+\/\/ |          |\n+\/\/  ----------\n+\/\/ |          | <-- frame fp  (always above the watermark of the same frame,\n+\/\/ |  callee  |                regardless of frame resizing)\n+\/\/ |          |\n+\/\/  ----------  <-- watermark (callee SP from the snapshot, SP at the\n+\/\/                             point of unwinding, might be above or below\n+\/\/                             due to frame resizing)\n+class StackWatermark : public CHeapObj<mtInternal> {\n+  friend class StackWatermarkFramesIterator;\n+protected:\n+  volatile uint32_t _state;\n+  volatile uintptr_t _watermark;\n+  StackWatermark* _next;\n+  JavaThread* _jt;\n+  StackWatermarkFramesIterator* _iterator;\n+  Mutex _lock;\n+  StackWatermarkKind _kind;\n+\n+  void process_one();\n+\n+  void update_watermark();\n+  void yield_processing();\n+  static bool has_barrier(const frame& f);\n+  void ensure_safe(const frame& f);\n+  void assert_is_frame_safe(const frame& f) PRODUCT_RETURN;\n+  bool is_frame_safe(const frame& f);\n+\n+  \/\/ API for consumers of the stack watermark barrier.\n+  \/\/ The rule for consumers is: do not perform thread transitions\n+  \/\/ or take locks of rank >= special. This is all very special code.\n+  virtual uint32_t epoch_id() const = 0;\n+  virtual void process(const frame& f, RegisterMap& register_map, void* context) = 0;\n+  virtual void start_processing_impl(void* context);\n+\n+  \/\/ Set process_on_iteration to false if you don't want to move the\n+  \/\/ watermark when new frames are discovered from stack walkers, as\n+  \/\/ opposed to due to frames being unwound by the owning thread.\n+  virtual bool process_on_iteration() { return true; }\n+\n+  bool processing_started(uint32_t state) const;\n+  bool processing_completed(uint32_t state) const;\n+\n+public:\n+  StackWatermark(JavaThread* jt, StackWatermarkKind kind, uint32_t epoch);\n+  virtual ~StackWatermark();\n+\n+\n+  \/\/ StackWatermarkSet support\n+  StackWatermarkKind kind() const { return _kind; }\n+  StackWatermark* next() const { return _next; }\n+  void set_next(StackWatermark* n) { _next = n; }\n+\n+  uintptr_t watermark();\n+  uintptr_t last_processed();\n+\n+  bool processing_started() const;\n+  bool processing_started_acquire() const;\n+  bool processing_completed() const;\n+  bool processing_completed_acquire() const;\n+\n+  void before_unwind();\n+  void after_unwind();\n+\n+  void on_iteration(const frame& f);\n+  void start_processing();\n+  void finish_processing(void* context);\n+};\n+\n+#endif \/\/ SHARE_RUNTIME_STACKWATERMARK_HPP\n","filename":"src\/hotspot\/share\/runtime\/stackWatermark.hpp","additions":144,"deletions":0,"binary":false,"changes":144,"status":"added"},{"patch":"@@ -0,0 +1,123 @@\n+\/*\n+ * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_RUNTIME_STACKWATERMARK_INLINE_HPP\n+#define SHARE_RUNTIME_STACKWATERMARK_INLINE_HPP\n+\n+#include \"runtime\/stackWatermark.hpp\"\n+#include \"runtime\/thread.hpp\"\n+\n+static inline bool is_above_watermark(uintptr_t sp, uintptr_t watermark) {\n+  if (watermark == 0) {\n+    return false;\n+  }\n+  return sp > watermark;\n+}\n+\n+\/\/ Returns true for frames where stack watermark barriers have been inserted.\n+\/\/ This function may return false negatives, but may never return true if a\n+\/\/ frame has no barrier.\n+inline bool StackWatermark::has_barrier(const frame& f) {\n+  if (f.is_interpreted_frame()) {\n+    return true;\n+  }\n+  if (f.is_compiled_frame()) {\n+    nmethod* nm = f.cb()->as_nmethod();\n+    if (nm->is_compiled_by_c1() || nm->is_compiled_by_c2()) {\n+      return true;\n+    }\n+    if (nm->is_native_method()) {\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n+inline bool StackWatermark::processing_started(uint32_t state) const {\n+  return StackWatermarkState::epoch(state) == epoch_id();\n+}\n+\n+inline bool StackWatermark::processing_completed(uint32_t state) const {\n+  assert(processing_started(state), \"Check is only valid if processing has been started\");\n+  return StackWatermarkState::is_done(state);\n+}\n+\n+inline void StackWatermark::ensure_safe(const frame& f) {\n+  assert(processing_started(), \"Processing should already have started\");\n+\n+  if (processing_completed_acquire()) {\n+    return;\n+  }\n+\n+  uintptr_t f_fp = reinterpret_cast<uintptr_t>(f.real_fp());\n+\n+  if (is_above_watermark(f_fp, watermark())) {\n+    process_one();\n+  }\n+\n+  assert_is_frame_safe(f);\n+}\n+\n+inline void StackWatermark::before_unwind() {\n+  frame f = _jt->last_frame();\n+\n+  \/\/ Skip any stub frames etc up until the frame that triggered before_unwind().\n+  RegisterMap map(_jt, false \/* update_map *\/, false \/* process_frames *\/);\n+  if (f.is_safepoint_blob_frame() || f.is_runtime_frame()) {\n+    f = f.sender(&map);\n+  }\n+\n+  assert_is_frame_safe(f);\n+  assert(!f.is_runtime_frame(), \"should have skipped all runtime stubs\");\n+\n+  \/\/ before_unwind() potentially exposes a new frame. The new exposed frame is\n+  \/\/ always the caller of the top frame.\n+  if (!f.is_first_frame()) {\n+    f = f.sender(&map);\n+    ensure_safe(f);\n+  }\n+}\n+\n+inline void StackWatermark::after_unwind() {\n+  frame f = _jt->last_frame();\n+\n+  if (f.is_safepoint_blob_frame() || f.is_runtime_frame()) {\n+    \/\/ Skip safepoint blob.\n+    RegisterMap map(_jt, false \/* update_map *\/, false \/* process_frames *\/);\n+    f = f.sender(&map);\n+  }\n+\n+  assert(!f.is_runtime_frame(), \"should have skipped all runtime stubs\");\n+\n+  \/\/ after_unwind() potentially exposes the top frame.\n+  ensure_safe(f);\n+}\n+\n+inline void StackWatermark::on_iteration(const frame& f) {\n+  if (process_on_iteration()) {\n+    ensure_safe(f);\n+  }\n+}\n+\n+#endif \/\/ SHARE_RUNTIME_STACKWATERMARK_INLINE_HPP\n","filename":"src\/hotspot\/share\/runtime\/stackWatermark.inline.hpp","additions":123,"deletions":0,"binary":false,"changes":123,"status":"added"},{"patch":"@@ -0,0 +1,32 @@\n+\/*\n+ * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_RUNTIME_STACKWATERMARKKIND_HPP\n+#define SHARE_RUNTIME_STACKWATERMARKKIND_HPP\n+\n+enum class StackWatermarkKind {\n+  gc\n+};\n+\n+#endif \/\/ SHARE_RUNTIME_STACKWATERMARKKIND_HPP\n","filename":"src\/hotspot\/share\/runtime\/stackWatermarkKind.hpp","additions":32,"deletions":0,"binary":false,"changes":32,"status":"added"},{"patch":"@@ -0,0 +1,144 @@\n+\/*\n+ * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"logging\/log.hpp\"\n+#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/frame.inline.hpp\"\n+#include \"runtime\/safepoint.hpp\"\n+#include \"runtime\/safepointMechanism.inline.hpp\"\n+#include \"runtime\/stackWatermark.inline.hpp\"\n+#include \"runtime\/stackWatermarkSet.inline.hpp\"\n+#include \"runtime\/thread.hpp\"\n+#include \"utilities\/debug.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include \"utilities\/preserveException.hpp\"\n+#include \"utilities\/vmError.hpp\"\n+\n+StackWatermarks::StackWatermarks() :\n+    _head(NULL) {}\n+\n+StackWatermarks::~StackWatermarks() {\n+  StackWatermark* current = _head;\n+  while (current != NULL) {\n+    StackWatermark* next = current->next();\n+    delete current;\n+    current = next;\n+  }\n+}\n+\n+StackWatermark* StackWatermarkSet::head(JavaThread* jt) {\n+  return jt->stack_watermarks()->_head;\n+}\n+\n+void StackWatermarkSet::set_head(JavaThread* jt, StackWatermark* watermark) {\n+  jt->stack_watermarks()->_head = watermark;\n+}\n+\n+void StackWatermarkSet::add_watermark(JavaThread* jt, StackWatermark* watermark) {\n+  assert(!has_watermark(jt, watermark->kind()), \"Two instances of same kind\");\n+  StackWatermark* prev = head(jt);\n+  watermark->set_next(prev);\n+  set_head(jt, watermark);\n+}\n+\n+static void verify_processing_context() {\n+#ifdef ASSERT\n+  Thread* thread = Thread::current();\n+  if (thread->is_Java_thread()) {\n+    JavaThread* jt = thread->as_Java_thread();\n+    JavaThreadState state = jt->thread_state();\n+    assert(state != _thread_in_native, \"unsafe thread state\");\n+    assert(state != _thread_blocked, \"unsafe thread state\");\n+  } else if (thread->is_VM_thread()) {\n+  } else {\n+    assert_locked_or_safepoint(Threads_lock);\n+  }\n+#endif\n+}\n+\n+void StackWatermarkSet::before_unwind(JavaThread* jt) {\n+  verify_processing_context();\n+  assert(jt->has_last_Java_frame(), \"must have a Java frame\");\n+  for (StackWatermark* current = head(jt); current != NULL; current = current->next()) {\n+    current->before_unwind();\n+  }\n+  SafepointMechanism::update_poll_values(jt);\n+}\n+\n+void StackWatermarkSet::after_unwind(JavaThread* jt) {\n+  verify_processing_context();\n+  assert(jt->has_last_Java_frame(), \"must have a Java frame\");\n+  for (StackWatermark* current = head(jt); current != NULL; current = current->next()) {\n+    current->after_unwind();\n+  }\n+  SafepointMechanism::update_poll_values(jt);\n+}\n+\n+void StackWatermarkSet::on_iteration(JavaThread* jt, const frame& fr) {\n+  if (VMError::is_error_reported()) {\n+    \/\/ Don't perform barrier when error reporting walks the stack.\n+    return;\n+  }\n+  verify_processing_context();\n+  for (StackWatermark* current = head(jt); current != NULL; current = current->next()) {\n+    current->on_iteration(fr);\n+  }\n+  \/\/ We don't call SafepointMechanism::update_poll_values here, because the thread\n+  \/\/ calling this might not be Thread::current().\n+}\n+\n+void StackWatermarkSet::start_processing(JavaThread* jt, StackWatermarkKind kind) {\n+  verify_processing_context();\n+  assert(!jt->is_terminated(), \"Poll after termination is a bug\");\n+  StackWatermark* watermark = get(jt, kind);\n+  if (watermark != NULL) {\n+    watermark->start_processing();\n+  }\n+  \/\/ We don't call SafepointMechanism::update_poll_values here, because the thread\n+  \/\/ calling this might not be Thread::current(). The thread the stack belongs to\n+  \/\/ will always update the poll values when waking up from a safepoint.\n+}\n+\n+void StackWatermarkSet::finish_processing(JavaThread* jt, void* context, StackWatermarkKind kind) {\n+  StackWatermark* watermark = get(jt, kind);\n+  if (watermark != NULL) {\n+    watermark->finish_processing(context);\n+  }\n+  \/\/ We don't call SafepointMechanism::update_poll_values here, because the thread\n+  \/\/ calling this might not be Thread::current().\n+}\n+\n+uintptr_t StackWatermarkSet::lowest_watermark(JavaThread* jt) {\n+  uintptr_t max_watermark = uintptr_t(0) - 1;\n+  uintptr_t watermark = max_watermark;\n+  for (StackWatermark* current = head(jt); current != NULL; current = current->next()) {\n+    watermark = MIN2(watermark, current->watermark());\n+  }\n+  if (watermark == max_watermark) {\n+    return 0;\n+  } else {\n+    return watermark;\n+  }\n+}\n","filename":"src\/hotspot\/share\/runtime\/stackWatermarkSet.cpp","additions":144,"deletions":0,"binary":false,"changes":144,"status":"added"},{"patch":"@@ -0,0 +1,87 @@\n+\/*\n+ * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_RUNTIME_STACKWATERMARKSET_HPP\n+#define SHARE_RUNTIME_STACKWATERMARKSET_HPP\n+\n+#include \"memory\/allStatic.hpp\"\n+#include \"runtime\/stackWatermarkKind.hpp\"\n+\n+class frame;\n+class JavaThread;\n+class StackWatermark;\n+\n+\/\/ A thread may have multiple StackWatermarks installed, for different unrelated client\n+\/\/ applications of lazy stack processing. The StackWatermarks class is the thread-local\n+\/\/ data structure used to store said watermarks. The StackWatermarkSet is the corresponding\n+\/\/ AllStatic class you use to interact with watermarks from shared runtime code. It allows\n+\/\/ hooks for all watermarks, or requesting specific action for specific client StackWatermark\n+\/\/ instances (if they have been installed).\n+\n+class StackWatermarks {\n+  friend class StackWatermarkSet;\n+private:\n+  StackWatermark* _head;\n+\n+public:\n+  StackWatermarks();\n+  ~StackWatermarks();\n+};\n+\n+class StackWatermarkSet : public AllStatic {\n+private:\n+  static StackWatermark* head(JavaThread* jt);\n+  static void set_head(JavaThread* jt, StackWatermark* watermark);\n+\n+public:\n+  static void add_watermark(JavaThread* jt, StackWatermark* watermark);\n+\n+  static StackWatermark* get(JavaThread* jt, StackWatermarkKind kind);\n+\n+  template <typename T>\n+  static T* get(JavaThread* jt, StackWatermarkKind kind);\n+\n+  static bool has_watermark(JavaThread* jt, StackWatermarkKind kind);\n+\n+  \/\/ Called when a thread is about to unwind a frame\n+  static void before_unwind(JavaThread* jt);\n+\n+  \/\/ Called when a thread just unwound a frame\n+  static void after_unwind(JavaThread* jt);\n+\n+  \/\/ Called by stack walkers when walking into a frame\n+  static void on_iteration(JavaThread* jt, const frame& fr);\n+\n+  \/\/ Called to ensure that processing of the thread is started\n+  static void start_processing(JavaThread* jt, StackWatermarkKind kind);\n+\n+  \/\/ Called to finish the processing of a thread\n+  static void finish_processing(JavaThread* jt, void* context, StackWatermarkKind kind);\n+\n+  \/\/ The lowest watermark among the watermarks in the set (the first encountered\n+  \/\/ watermark in the set as you unwind frames)\n+  static uintptr_t lowest_watermark(JavaThread* jt);\n+};\n+\n+#endif \/\/ SHARE_RUNTIME_STACKWATERMARKSET_HPP\n","filename":"src\/hotspot\/share\/runtime\/stackWatermarkSet.hpp","additions":87,"deletions":0,"binary":false,"changes":87,"status":"added"},{"patch":"@@ -0,0 +1,49 @@\n+\/*\n+ * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_RUNTIME_STACKWATERMARKSET_INLINE_HPP\n+#define SHARE_RUNTIME_STACKWATERMARKSET_INLINE_HPP\n+\n+#include \"runtime\/stackWatermark.hpp\"\n+#include \"runtime\/stackWatermarkSet.hpp\"\n+\n+inline StackWatermark* StackWatermarkSet::get(JavaThread* jt, StackWatermarkKind kind) {\n+  for (StackWatermark* stack_watermark = head(jt); stack_watermark != NULL; stack_watermark = stack_watermark->next()) {\n+    if (stack_watermark->kind() == kind) {\n+      return stack_watermark;\n+    }\n+  }\n+  return NULL;\n+}\n+\n+template <typename T>\n+inline T* StackWatermarkSet::get(JavaThread* jt, StackWatermarkKind kind) {\n+  return static_cast<T*>(get(jt, kind));\n+}\n+\n+inline bool StackWatermarkSet::has_watermark(JavaThread* jt, StackWatermarkKind kind) {\n+  return get(jt, kind) != NULL;\n+}\n+\n+#endif \/\/ SHARE_RUNTIME_STACKWATERMARKSET_INLINE_HPP\n","filename":"src\/hotspot\/share\/runtime\/stackWatermarkSet.inline.hpp","additions":49,"deletions":0,"binary":false,"changes":49,"status":"added"},{"patch":"@@ -93,0 +93,1 @@\n+#include \"runtime\/stackWatermarkSet.hpp\"\n@@ -871,1 +872,1 @@\n-void Thread::oops_do(OopClosure* f, CodeBlobClosure* cf) {\n+void Thread::oops_do_no_frames(OopClosure* f, CodeBlobClosure* cf) {\n@@ -880,0 +881,31 @@\n+\/\/ If the caller is a NamedThread, then remember, in the current scope,\n+\/\/ the given JavaThread in its _processed_thread field.\n+class RememberProcessedThread: public StackObj {\n+  NamedThread* _cur_thr;\n+public:\n+  RememberProcessedThread(Thread* thread) {\n+    Thread* self = Thread::current();\n+    if (self->is_Named_thread()) {\n+      _cur_thr = (NamedThread *)self;\n+      assert(_cur_thr->processed_thread() == NULL, \"nesting not supported\");\n+      _cur_thr->set_processed_thread(thread);\n+    } else {\n+      _cur_thr = NULL;\n+    }\n+  }\n+\n+  ~RememberProcessedThread() {\n+    if (_cur_thr) {\n+      assert(_cur_thr->processed_thread() != NULL, \"nesting not supported\");\n+      _cur_thr->set_processed_thread(NULL);\n+    }\n+  }\n+};\n+\n+void Thread::oops_do(OopClosure* f, CodeBlobClosure* cf) {\n+  \/\/ Record JavaThread to GC thread\n+  RememberProcessedThread rpt(this);\n+  oops_do_no_frames(f, cf);\n+  oops_do_frames(f, cf);\n+}\n+\n@@ -2627,0 +2659,5 @@\n+  \/\/ After returning from native, it could be that the stack frames are not\n+  \/\/ yet safe to use. We catch such situations in the subsequent stack watermark\n+  \/\/ barrier, which will trap unsafe stack frames.\n+  StackWatermarkSet::before_unwind(thread);\n+\n@@ -2681,1 +2718,1 @@\n-  StackFrameStream fst(this, false);\n+  StackFrameStream fst(this, false \/* update *\/, true \/* process_frames *\/);\n@@ -2731,1 +2768,1 @@\n-  for (StackFrameStream fst(this); !fst.is_done(); fst.next()) {\n+  for (StackFrameStream fst(this, true \/* update *\/, true \/* process_frames *\/); !fst.is_done(); fst.next()) {\n@@ -2744,1 +2781,1 @@\n-  StackFrameStream fst(this, false);\n+  StackFrameStream fst(this, false \/* update *\/, true \/* process_frames *\/);\n@@ -2752,23 +2789,1 @@\n-\/\/ If the caller is a NamedThread, then remember, in the current scope,\n-\/\/ the given JavaThread in its _processed_thread field.\n-class RememberProcessedThread: public StackObj {\n-  NamedThread* _cur_thr;\n- public:\n-  RememberProcessedThread(JavaThread* jthr) {\n-    Thread* thread = Thread::current();\n-    if (thread->is_Named_thread()) {\n-      _cur_thr = (NamedThread *)thread;\n-      _cur_thr->set_processed_thread(jthr);\n-    } else {\n-      _cur_thr = NULL;\n-    }\n-  }\n-\n-  ~RememberProcessedThread() {\n-    if (_cur_thr) {\n-      _cur_thr->set_processed_thread(NULL);\n-    }\n-  }\n-};\n-\n-void JavaThread::oops_do(OopClosure* f, CodeBlobClosure* cf) {\n+void JavaThread::oops_do_no_frames(OopClosure* f, CodeBlobClosure* cf) {\n@@ -2779,1 +2794,1 @@\n-  Thread::oops_do(f, cf);\n+  Thread::oops_do_no_frames(f, cf);\n@@ -2785,3 +2800,0 @@\n-    \/\/ Record JavaThread to GC thread\n-    RememberProcessedThread rpt(this);\n-\n@@ -2792,5 +2804,0 @@\n-\n-    \/\/ Traverse the execution stack\n-    for (StackFrameStream fst(this); !fst.is_done(); fst.next()) {\n-      fst.current()->oops_do(f, cf, fst.register_map());\n-    }\n@@ -2820,0 +2827,12 @@\n+void JavaThread::oops_do_frames(OopClosure* f, CodeBlobClosure* cf) {\n+  if (!has_last_Java_frame()) {\n+    return;\n+  }\n+  \/\/ Finish any pending lazy GC activity for the frames\n+  StackWatermarkSet::finish_processing(this, NULL \/* context *\/, StackWatermarkKind::gc);\n+  \/\/ Traverse the execution stack\n+  for (StackFrameStream fst(this, true \/* update *\/, false \/* process_frames *\/); !fst.is_done(); fst.next()) {\n+    fst.current()->oops_do(f, cf, fst.register_map());\n+  }\n+}\n+\n@@ -2838,1 +2857,1 @@\n-    for (StackFrameStream fst(this); !fst.is_done(); fst.next()) {\n+    for (StackFrameStream fst(this, true \/* update *\/, true \/* process_frames *\/); !fst.is_done(); fst.next()) {\n@@ -2851,1 +2870,1 @@\n-    for (StackFrameStream fst(this); !fst.is_done(); fst.next()) {\n+    for (StackFrameStream fst(this, true \/* update *\/, true \/* process_frames *\/); !fst.is_done(); fst.next()) {\n@@ -2957,1 +2976,1 @@\n-  for (StackFrameStream fst(this); !fst.is_done(); fst.next()) {\n+  for (StackFrameStream fst(this, true \/* update *\/, true \/* process_frames *\/); !fst.is_done(); fst.next()) {\n@@ -3135,1 +3154,1 @@\n-  for (StackFrameStream fst(this); !fst.is_done(); fst.next()) {\n+  for (StackFrameStream fst(this, true \/* update *\/, true \/* process_frames *\/); !fst.is_done(); fst.next()) {\n@@ -3171,1 +3190,1 @@\n-  for (StackFrameStream fst(this, false); !fst.is_done(); fst.next()) {\n+  for (StackFrameStream fst(this, false \/* update *\/, true \/* process_frames *\/); !fst.is_done(); fst.next()) {\n@@ -3333,2 +3352,2 @@\n-void CodeCacheSweeperThread::oops_do(OopClosure* f, CodeBlobClosure* cf) {\n-  JavaThread::oops_do(f, cf);\n+void CodeCacheSweeperThread::oops_do_no_frames(OopClosure* f, CodeBlobClosure* cf) {\n+  JavaThread::oops_do_no_frames(f, cf);\n@@ -4379,7 +4398,0 @@\n-  \/\/ We must flush any deferred card marks and other various GC barrier\n-  \/\/ related buffers (e.g. G1 SATB buffer and G1 dirty card queue buffer)\n-  \/\/ before removing a thread from the list of active threads.\n-  \/\/ This must be done after ObjectSynchronizer::om_flush(), as GC barriers\n-  \/\/ are used in om_flush().\n-  BarrierSet::barrier_set()->on_thread_detach(p);\n-\n@@ -4390,0 +4402,7 @@\n+    \/\/ We must flush any deferred card marks and other various GC barrier\n+    \/\/ related buffers (e.g. G1 SATB buffer and G1 dirty card queue buffer)\n+    \/\/ before removing a thread from the list of active threads.\n+    \/\/ This must be done after ObjectSynchronizer::om_flush(), as GC barriers\n+    \/\/ are used in om_flush().\n+    BarrierSet::barrier_set()->on_thread_detach(p);\n+\n","filename":"src\/hotspot\/share\/runtime\/thread.cpp","additions":69,"deletions":50,"binary":false,"changes":119,"status":"modified"},{"patch":"@@ -43,0 +43,2 @@\n+#include \"runtime\/safepointMechanism.hpp\"\n+#include \"runtime\/stackWatermarkSet.hpp\"\n@@ -287,0 +289,6 @@\n+  \/\/ Stack watermark barriers.\n+  StackWatermarks _stack_watermarks;\n+\n+ public:\n+  inline StackWatermarks* stack_watermarks() { return &_stack_watermarks; }\n+\n@@ -394,1 +402,2 @@\n-  volatile void* _polling_page;                 \/\/ Thread local polling page\n+ protected:\n+  SafepointMechanism::ThreadData _poll_data;\n@@ -396,0 +405,1 @@\n+ private:\n@@ -657,1 +667,3 @@\n-  virtual void oops_do(OopClosure* f, CodeBlobClosure* cf);\n+  virtual void oops_do_no_frames(OopClosure* f, CodeBlobClosure* cf);\n+  virtual void oops_do_frames(OopClosure* f, CodeBlobClosure* cf) {}\n+  void oops_do(OopClosure* f, CodeBlobClosure* cf);\n@@ -752,2 +764,0 @@\n-  volatile void** polling_page_addr() { return &_polling_page; }\n-\n@@ -817,1 +827,2 @@\n-  static ByteSize polling_page_offset()          { return byte_offset_of(Thread, _polling_page); }\n+  static ByteSize polling_word_offset()          { return byte_offset_of(Thread, _poll_data) + byte_offset_of(SafepointMechanism::ThreadData, _polling_word);}\n+  static ByteSize polling_page_offset()          { return byte_offset_of(Thread, _poll_data) + byte_offset_of(SafepointMechanism::ThreadData, _polling_page);}\n@@ -927,2 +938,2 @@\n-  \/\/ log JavaThread being processed by oops_do\n-  JavaThread* _processed_thread;\n+  \/\/ log Thread being processed by oops_do\n+  Thread* _processed_thread;\n@@ -938,2 +949,2 @@\n-  JavaThread *processed_thread() { return _processed_thread; }\n-  void set_processed_thread(JavaThread *thread) { _processed_thread = thread; }\n+  Thread *processed_thread() { return _processed_thread; }\n+  void set_processed_thread(Thread *thread) { _processed_thread = thread; }\n@@ -1329,3 +1340,1 @@\n-  inline void set_polling_page_release(void* poll_value);\n-  inline void set_polling_page(void* poll_value);\n-  inline volatile void* get_polling_page();\n+  SafepointMechanism::ThreadData* poll_data() { return &_poll_data; }\n@@ -1693,1 +1702,2 @@\n-  void oops_do(OopClosure* f, CodeBlobClosure* cf);\n+  void oops_do_frames(OopClosure* f, CodeBlobClosure* cf);\n+  void oops_do_no_frames(OopClosure* f, CodeBlobClosure* cf);\n@@ -1945,1 +1955,1 @@\n-  void oops_do(OopClosure* f, CodeBlobClosure* cf);\n+  void oops_do_no_frames(OopClosure* f, CodeBlobClosure* cf);\n","filename":"src\/hotspot\/share\/runtime\/thread.hpp","additions":24,"deletions":14,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -169,17 +169,0 @@\n-\/\/ The release make sure this store is done after storing the handshake\n-\/\/ operation or global state\n-inline void JavaThread::set_polling_page_release(void* poll_value) {\n-  Atomic::release_store(polling_page_addr(), poll_value);\n-}\n-\n-\/\/ Caller is responsible for using a memory barrier if needed.\n-inline void JavaThread::set_polling_page(void* poll_value) {\n-  *polling_page_addr() = poll_value;\n-}\n-\n-\/\/ The aqcquire make sure reading of polling page is done before\n-\/\/ the reading the handshake operation or the global state\n-inline volatile void* JavaThread::get_polling_page() {\n-  return Atomic::load_acquire(polling_page_addr());\n-}\n-\n","filename":"src\/hotspot\/share\/runtime\/thread.inline.hpp","additions":0,"deletions":17,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -478,1 +478,2 @@\n-  bool stop_at_java_call_stub) : vframeStreamCommon(thread) {\n+                           bool stop_at_java_call_stub) :\n+    vframeStreamCommon(thread, true \/* process_frames *\/) {\n","filename":"src\/hotspot\/share\/runtime\/vframe.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -302,1 +302,1 @@\n-  inline vframeStreamCommon(JavaThread* thread);\n+  inline vframeStreamCommon(JavaThread* thread, bool process_frames);\n@@ -340,1 +340,1 @@\n-  vframeStream(JavaThread* thread, bool stop_at_java_call_stub = false);\n+  vframeStream(JavaThread* thread, bool stop_at_java_call_stub = false, bool process_frames = true);\n","filename":"src\/hotspot\/share\/runtime\/vframe.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -32,1 +32,1 @@\n-inline vframeStreamCommon::vframeStreamCommon(JavaThread* thread) : _reg_map(thread, false) {\n+inline vframeStreamCommon::vframeStreamCommon(JavaThread* thread, bool process_frames) : _reg_map(thread, false, process_frames) {\n@@ -53,2 +53,2 @@\n-inline vframeStream::vframeStream(JavaThread* thread, bool stop_at_java_call_stub)\n-  : vframeStreamCommon(thread) {\n+inline vframeStream::vframeStream(JavaThread* thread, bool stop_at_java_call_stub, bool process_frame)\n+  : vframeStreamCommon(thread, process_frame \/* process_frames *\/) {\n","filename":"src\/hotspot\/share\/runtime\/vframe.inline.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -137,1 +137,1 @@\n-          for(StackFrameStream fst(thread, false); !fst.is_done(); fst.next()) {\n+          for(StackFrameStream fst(thread, false \/* update *\/, true \/* process_frames *\/); !fst.is_done(); fst.next()) {\n","filename":"src\/hotspot\/share\/runtime\/vmOperations.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -153,0 +153,4 @@\n+  \/\/ You may override skip_thread_oop_barriers to return true if the operation\n+  \/\/ does not access thread-private oops (including frames).\n+  virtual bool skip_thread_oop_barriers() const { return false; }\n+\n@@ -216,0 +220,1 @@\n+  virtual bool skip_thread_oop_barriers() const { return true; }\n","filename":"src\/hotspot\/share\/runtime\/vmOperations.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -742,1 +742,1 @@\n-  nonstatic_field(NamedThread,                 _processed_thread,                             JavaThread*)                           \\\n+  nonstatic_field(NamedThread,                 _processed_thread,                             Thread*)                               \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -227,1 +227,1 @@\n-    for(StackFrameStream sfs(jt); !sfs.is_done(); sfs.next()) {\n+    for (StackFrameStream sfs(jt, true \/* update *\/, true \/* process_frames *\/); !sfs.is_done(); sfs.next()) {\n@@ -752,2 +752,3 @@\n-       JavaThread*  jt = ((NamedThread *)_thread)->processed_thread();\n-       if (jt != NULL) {\n+       Thread* thread = ((NamedThread *)_thread)->processed_thread();\n+       if (thread != NULL && thread->is_Java_thread()) {\n+         JavaThread* jt = thread->as_Java_thread();\n","filename":"src\/hotspot\/share\/utilities\/vmError.cpp","additions":4,"deletions":3,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -661,1 +661,1 @@\n-        } else {\n+        } else if (JDK < 16) {\n@@ -663,0 +663,3 @@\n+        } else {\n+            threadPollingPageOffset = getFieldOffset(\"Thread::_poll_data\", Integer.class, \"SafepointMechanism::ThreadData\") +\n+                                      getFieldOffset(\"SafepointMechanism::ThreadData::_polling_page\", Integer.class, \"volatile uintptr_t\");\n","filename":"src\/jdk.internal.vm.compiler\/share\/classes\/org.graalvm.compiler.hotspot\/src\/org\/graalvm\/compiler\/hotspot\/GraalHotSpotVMConfig.java","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"}]}