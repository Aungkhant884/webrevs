{"files":[{"patch":"@@ -881,1 +881,1 @@\n-void SharedRuntime::move32_64(MacroAssembler* masm, VMRegPair src, VMRegPair dst) {\n+static void move32_64(MacroAssembler* masm, VMRegPair src, VMRegPair dst) {\n@@ -982,1 +982,1 @@\n-void SharedRuntime::float_move(MacroAssembler* masm, VMRegPair src, VMRegPair dst) {\n+static void float_move(MacroAssembler* masm, VMRegPair src, VMRegPair dst) {\n@@ -1001,1 +1001,1 @@\n-void SharedRuntime::long_move(MacroAssembler* masm, VMRegPair src, VMRegPair dst) {\n+static void long_move(MacroAssembler* masm, VMRegPair src, VMRegPair dst) {\n@@ -1025,1 +1025,1 @@\n-void SharedRuntime::double_move(MacroAssembler* masm, VMRegPair src, VMRegPair dst) {\n+static void double_move(MacroAssembler* masm, VMRegPair src, VMRegPair dst) {\n","filename":"src\/hotspot\/cpu\/aarch64\/sharedRuntime_aarch64.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -37,0 +37,5 @@\n+\n+const CallRegs ForeignGlobals::parse_call_regs_impl(jobject jconv) const {\n+  Unimplemented();\n+  return {};\n+}\n","filename":"src\/hotspot\/cpu\/arm\/foreign_globals_arm.cpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -32,0 +32,9 @@\n+\n+address ProgrammableUpcallHandler::generate_optimized_upcall_stub(jobject mh, Method* entry, jobject jabi, jobject jconv) {\n+  ShouldNotCallThis();\n+  return nullptr;\n+}\n+\n+bool ProgrammableUpcallHandler::supports_optimized_upcalls() {\n+  return false;\n+}\n","filename":"src\/hotspot\/cpu\/arm\/universalUpcallHandle_arm.cpp","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -39,0 +39,5 @@\n+\n+const CallRegs ForeignGlobals::parse_call_regs_impl(jobject jconv) const {\n+  Unimplemented();\n+  return {};\n+}\n","filename":"src\/hotspot\/cpu\/ppc\/foreign_globals_ppc.cpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -33,0 +33,9 @@\n+\n+address ProgrammableUpcallHandler::generate_optimized_upcall_stub(jobject mh, Method* entry, jobject jabi, jobject jconv) {\n+  ShouldNotCallThis();\n+  return nullptr;\n+}\n+\n+bool ProgrammableUpcallHandler::supports_optimized_upcalls() {\n+  return false;\n+}\n","filename":"src\/hotspot\/cpu\/ppc\/universalUpcallHandle_ppc.cpp","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -37,0 +37,5 @@\n+\n+const CallRegs ForeignGlobals::parse_call_regs_impl(jobject jconv) const {\n+  Unimplemented();\n+  return {};\n+}\n","filename":"src\/hotspot\/cpu\/s390\/foreign_globals_s390.cpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -32,0 +32,9 @@\n+\n+address ProgrammableUpcallHandler::generate_optimized_upcall_stub(jobject mh, Method* entry, jobject jabi, jobject jconv) {\n+  ShouldNotCallThis();\n+  return nullptr;\n+}\n+\n+bool ProgrammableUpcallHandler::supports_optimized_upcalls() {\n+  return false;\n+}\n","filename":"src\/hotspot\/cpu\/s390\/universalUpcallHandle_s390.cpp","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -916,0 +916,215 @@\n+\/\/ The java_calling_convention describes stack locations as ideal slots on\n+\/\/ a frame with no abi restrictions. Since we must observe abi restrictions\n+\/\/ (like the placement of the register window) the slots must be biased by\n+\/\/ the following value.\n+static int reg2offset_in(VMReg r) {\n+  \/\/ Account for saved rbp and return address\n+  \/\/ This should really be in_preserve_stack_slots\n+  return (r->reg2stack() + 4) * VMRegImpl::stack_slot_size;\n+}\n+\n+static int reg2offset_out(VMReg r) {\n+  return (r->reg2stack() + SharedRuntime::out_preserve_stack_slots()) * VMRegImpl::stack_slot_size;\n+}\n+\n+\/\/ A long move\n+void MacroAssembler::long_move(VMRegPair src, VMRegPair dst) {\n+\n+  \/\/ The calling conventions assures us that each VMregpair is either\n+  \/\/ all really one physical register or adjacent stack slots.\n+\n+  if (src.is_single_phys_reg() ) {\n+    if (dst.is_single_phys_reg()) {\n+      if (dst.first() != src.first()) {\n+        mov(dst.first()->as_Register(), src.first()->as_Register());\n+      }\n+    } else {\n+      assert(dst.is_single_reg(), \"not a stack pair\");\n+      movq(Address(rsp, reg2offset_out(dst.first())), src.first()->as_Register());\n+    }\n+  } else if (dst.is_single_phys_reg()) {\n+    assert(src.is_single_reg(),  \"not a stack pair\");\n+    movq(dst.first()->as_Register(), Address(rbp, reg2offset_out(src.first())));\n+  } else {\n+    assert(src.is_single_reg() && dst.is_single_reg(), \"not stack pairs\");\n+    movq(rax, Address(rbp, reg2offset_in(src.first())));\n+    movq(Address(rsp, reg2offset_out(dst.first())), rax);\n+  }\n+}\n+\n+\/\/ A double move\n+void MacroAssembler::double_move(VMRegPair src, VMRegPair dst) {\n+\n+  \/\/ The calling conventions assures us that each VMregpair is either\n+  \/\/ all really one physical register or adjacent stack slots.\n+\n+  if (src.is_single_phys_reg() ) {\n+    if (dst.is_single_phys_reg()) {\n+      \/\/ In theory these overlap but the ordering is such that this is likely a nop\n+      if ( src.first() != dst.first()) {\n+        movdbl(dst.first()->as_XMMRegister(), src.first()->as_XMMRegister());\n+      }\n+    } else {\n+      assert(dst.is_single_reg(), \"not a stack pair\");\n+      movdbl(Address(rsp, reg2offset_out(dst.first())), src.first()->as_XMMRegister());\n+    }\n+  } else if (dst.is_single_phys_reg()) {\n+    assert(src.is_single_reg(),  \"not a stack pair\");\n+    movdbl(dst.first()->as_XMMRegister(), Address(rbp, reg2offset_out(src.first())));\n+  } else {\n+    assert(src.is_single_reg() && dst.is_single_reg(), \"not stack pairs\");\n+    movq(rax, Address(rbp, reg2offset_in(src.first())));\n+    movq(Address(rsp, reg2offset_out(dst.first())), rax);\n+  }\n+}\n+\n+\n+\/\/ A float arg may have to do float reg int reg conversion\n+void MacroAssembler::float_move(VMRegPair src, VMRegPair dst) {\n+  assert(!src.second()->is_valid() && !dst.second()->is_valid(), \"bad float_move\");\n+\n+  \/\/ The calling conventions assures us that each VMregpair is either\n+  \/\/ all really one physical register or adjacent stack slots.\n+\n+  if (src.first()->is_stack()) {\n+    if (dst.first()->is_stack()) {\n+      movl(rax, Address(rbp, reg2offset_in(src.first())));\n+      movptr(Address(rsp, reg2offset_out(dst.first())), rax);\n+    } else {\n+      \/\/ stack to reg\n+      assert(dst.first()->is_XMMRegister(), \"only expect xmm registers as parameters\");\n+      movflt(dst.first()->as_XMMRegister(), Address(rbp, reg2offset_in(src.first())));\n+    }\n+  } else if (dst.first()->is_stack()) {\n+    \/\/ reg to stack\n+    assert(src.first()->is_XMMRegister(), \"only expect xmm registers as parameters\");\n+    movflt(Address(rsp, reg2offset_out(dst.first())), src.first()->as_XMMRegister());\n+  } else {\n+    \/\/ reg to reg\n+    \/\/ In theory these overlap but the ordering is such that this is likely a nop\n+    if ( src.first() != dst.first()) {\n+      movdbl(dst.first()->as_XMMRegister(),  src.first()->as_XMMRegister());\n+    }\n+  }\n+}\n+\n+\/\/ On 64 bit we will store integer like items to the stack as\n+\/\/ 64 bits items (x86_32\/64 abi) even though java would only store\n+\/\/ 32bits for a parameter. On 32bit it will simply be 32 bits\n+\/\/ So this routine will do 32->32 on 32bit and 32->64 on 64bit\n+void MacroAssembler::move32_64(VMRegPair src, VMRegPair dst) {\n+  if (src.first()->is_stack()) {\n+    if (dst.first()->is_stack()) {\n+      \/\/ stack to stack\n+      movslq(rax, Address(rbp, reg2offset_in(src.first())));\n+      movq(Address(rsp, reg2offset_out(dst.first())), rax);\n+    } else {\n+      \/\/ stack to reg\n+      movslq(dst.first()->as_Register(), Address(rbp, reg2offset_in(src.first())));\n+    }\n+  } else if (dst.first()->is_stack()) {\n+    \/\/ reg to stack\n+    \/\/ Do we really have to sign extend???\n+    \/\/ __ movslq(src.first()->as_Register(), src.first()->as_Register());\n+    movq(Address(rsp, reg2offset_out(dst.first())), src.first()->as_Register());\n+  } else {\n+    \/\/ Do we really have to sign extend???\n+    \/\/ __ movslq(dst.first()->as_Register(), src.first()->as_Register());\n+    if (dst.first() != src.first()) {\n+      movq(dst.first()->as_Register(), src.first()->as_Register());\n+    }\n+  }\n+}\n+\n+void MacroAssembler::move_ptr(VMRegPair src, VMRegPair dst) {\n+  if (src.first()->is_stack()) {\n+    if (dst.first()->is_stack()) {\n+      \/\/ stack to stack\n+      movq(rax, Address(rbp, reg2offset_in(src.first())));\n+      movq(Address(rsp, reg2offset_out(dst.first())), rax);\n+    } else {\n+      \/\/ stack to reg\n+      movq(dst.first()->as_Register(), Address(rbp, reg2offset_in(src.first())));\n+    }\n+  } else if (dst.first()->is_stack()) {\n+    \/\/ reg to stack\n+    movq(Address(rsp, reg2offset_out(dst.first())), src.first()->as_Register());\n+  } else {\n+    if (dst.first() != src.first()) {\n+      movq(dst.first()->as_Register(), src.first()->as_Register());\n+    }\n+  }\n+}\n+\n+\/\/ An oop arg. Must pass a handle not the oop itself\n+void MacroAssembler::object_move(OopMap* map,\n+                        int oop_handle_offset,\n+                        int framesize_in_slots,\n+                        VMRegPair src,\n+                        VMRegPair dst,\n+                        bool is_receiver,\n+                        int* receiver_offset) {\n+\n+  \/\/ must pass a handle. First figure out the location we use as a handle\n+\n+  Register rHandle = dst.first()->is_stack() ? rax : dst.first()->as_Register();\n+\n+  \/\/ See if oop is NULL if it is we need no handle\n+\n+  if (src.first()->is_stack()) {\n+\n+    \/\/ Oop is already on the stack as an argument\n+    int offset_in_older_frame = src.first()->reg2stack() + SharedRuntime::out_preserve_stack_slots();\n+    map->set_oop(VMRegImpl::stack2reg(offset_in_older_frame + framesize_in_slots));\n+    if (is_receiver) {\n+      *receiver_offset = (offset_in_older_frame + framesize_in_slots) * VMRegImpl::stack_slot_size;\n+    }\n+\n+    cmpptr(Address(rbp, reg2offset_in(src.first())), (int32_t)NULL_WORD);\n+    lea(rHandle, Address(rbp, reg2offset_in(src.first())));\n+    \/\/ conditionally move a NULL\n+    cmovptr(Assembler::equal, rHandle, Address(rbp, reg2offset_in(src.first())));\n+  } else {\n+\n+    \/\/ Oop is in an a register we must store it to the space we reserve\n+    \/\/ on the stack for oop_handles and pass a handle if oop is non-NULL\n+\n+    const Register rOop = src.first()->as_Register();\n+    int oop_slot;\n+    if (rOop == j_rarg0)\n+      oop_slot = 0;\n+    else if (rOop == j_rarg1)\n+      oop_slot = 1;\n+    else if (rOop == j_rarg2)\n+      oop_slot = 2;\n+    else if (rOop == j_rarg3)\n+      oop_slot = 3;\n+    else if (rOop == j_rarg4)\n+      oop_slot = 4;\n+    else {\n+      assert(rOop == j_rarg5, \"wrong register\");\n+      oop_slot = 5;\n+    }\n+\n+    oop_slot = oop_slot * VMRegImpl::slots_per_word + oop_handle_offset;\n+    int offset = oop_slot*VMRegImpl::stack_slot_size;\n+\n+    map->set_oop(VMRegImpl::stack2reg(oop_slot));\n+    \/\/ Store oop in handle area, may be NULL\n+    movptr(Address(rsp, offset), rOop);\n+    if (is_receiver) {\n+      *receiver_offset = offset;\n+    }\n+\n+    cmpptr(rOop, (int32_t)NULL_WORD);\n+    lea(rHandle, Address(rsp, offset));\n+    \/\/ conditionally move a NULL from the handle area where it was just stored\n+    cmovptr(Assembler::equal, rHandle, Address(rsp, offset));\n+  }\n+\n+  \/\/ If arg is on the stack then place it otherwise it is already in correct reg.\n+  if (dst.first()->is_stack()) {\n+    movptr(Address(rsp, reg2offset_out(dst.first())), rHandle);\n+  }\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":215,"deletions":0,"binary":false,"changes":215,"status":"modified"},{"patch":"@@ -29,0 +29,2 @@\n+#include \"code\/vmreg.inline.hpp\"\n+#include \"compiler\/oopMap.hpp\"\n@@ -209,0 +211,16 @@\n+#ifdef _LP64\n+  \/\/ Support for argument shuffling\n+\n+  void move32_64(VMRegPair src, VMRegPair dst);\n+  void long_move(VMRegPair src, VMRegPair dst);\n+  void float_move(VMRegPair src, VMRegPair dst);\n+  void double_move(VMRegPair src, VMRegPair dst);\n+  void move_ptr(VMRegPair src, VMRegPair dst);\n+  void object_move(OopMap* map,\n+                   int oop_handle_offset,\n+                   int framesize_in_slots,\n+                   VMRegPair src,\n+                   VMRegPair dst,\n+                   bool is_receiver,\n+                   int* receiver_offset);\n+#endif \/\/ _LP64\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":18,"deletions":0,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -1127,1 +1127,1 @@\n-void SharedRuntime::float_move(MacroAssembler* masm, VMRegPair src, VMRegPair dst) {\n+static void float_move(MacroAssembler* masm, VMRegPair src, VMRegPair dst) {\n@@ -1145,1 +1145,1 @@\n-void SharedRuntime::long_move(MacroAssembler* masm, VMRegPair src, VMRegPair dst) {\n+static void long_move(MacroAssembler* masm, VMRegPair src, VMRegPair dst) {\n@@ -1164,1 +1164,1 @@\n-void SharedRuntime::double_move(MacroAssembler* masm, VMRegPair src, VMRegPair dst) {\n+static void double_move(MacroAssembler* masm, VMRegPair src, VMRegPair dst) {\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_32.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -461,14 +461,0 @@\n-\/\/ The java_calling_convention describes stack locations as ideal slots on\n-\/\/ a frame with no abi restrictions. Since we must observe abi restrictions\n-\/\/ (like the placement of the register window) the slots must be biased by\n-\/\/ the following value.\n-static int reg2offset_in(VMReg r) {\n-  \/\/ Account for saved rbp and return address\n-  \/\/ This should really be in_preserve_stack_slots\n-  return (r->reg2stack() + 4) * VMRegImpl::stack_slot_size;\n-}\n-\n-static int reg2offset_out(VMReg r) {\n-  return (r->reg2stack() + SharedRuntime::out_preserve_stack_slots()) * VMRegImpl::stack_slot_size;\n-}\n-\n@@ -1166,202 +1152,0 @@\n-\/\/ On 64 bit we will store integer like items to the stack as\n-\/\/ 64 bits items (x86_32\/64 abi) even though java would only store\n-\/\/ 32bits for a parameter. On 32bit it will simply be 32 bits\n-\/\/ So this routine will do 32->32 on 32bit and 32->64 on 64bit\n-void SharedRuntime::move32_64(MacroAssembler* masm, VMRegPair src, VMRegPair dst) {\n-  if (src.first()->is_stack()) {\n-    if (dst.first()->is_stack()) {\n-      \/\/ stack to stack\n-      __ movslq(rax, Address(rbp, reg2offset_in(src.first())));\n-      __ movq(Address(rsp, reg2offset_out(dst.first())), rax);\n-    } else {\n-      \/\/ stack to reg\n-      __ movslq(dst.first()->as_Register(), Address(rbp, reg2offset_in(src.first())));\n-    }\n-  } else if (dst.first()->is_stack()) {\n-    \/\/ reg to stack\n-    \/\/ Do we really have to sign extend???\n-    \/\/ __ movslq(src.first()->as_Register(), src.first()->as_Register());\n-    __ movq(Address(rsp, reg2offset_out(dst.first())), src.first()->as_Register());\n-  } else {\n-    \/\/ Do we really have to sign extend???\n-    \/\/ __ movslq(dst.first()->as_Register(), src.first()->as_Register());\n-    if (dst.first() != src.first()) {\n-      __ movq(dst.first()->as_Register(), src.first()->as_Register());\n-    }\n-  }\n-}\n-\n-static void move_ptr(MacroAssembler* masm, VMRegPair src, VMRegPair dst) {\n-  if (src.first()->is_stack()) {\n-    if (dst.first()->is_stack()) {\n-      \/\/ stack to stack\n-      __ movq(rax, Address(rbp, reg2offset_in(src.first())));\n-      __ movq(Address(rsp, reg2offset_out(dst.first())), rax);\n-    } else {\n-      \/\/ stack to reg\n-      __ movq(dst.first()->as_Register(), Address(rbp, reg2offset_in(src.first())));\n-    }\n-  } else if (dst.first()->is_stack()) {\n-    \/\/ reg to stack\n-    __ movq(Address(rsp, reg2offset_out(dst.first())), src.first()->as_Register());\n-  } else {\n-    if (dst.first() != src.first()) {\n-      __ movq(dst.first()->as_Register(), src.first()->as_Register());\n-    }\n-  }\n-}\n-\n-\/\/ An oop arg. Must pass a handle not the oop itself\n-static void object_move(MacroAssembler* masm,\n-                        OopMap* map,\n-                        int oop_handle_offset,\n-                        int framesize_in_slots,\n-                        VMRegPair src,\n-                        VMRegPair dst,\n-                        bool is_receiver,\n-                        int* receiver_offset) {\n-\n-  \/\/ must pass a handle. First figure out the location we use as a handle\n-\n-  Register rHandle = dst.first()->is_stack() ? rax : dst.first()->as_Register();\n-\n-  \/\/ See if oop is NULL if it is we need no handle\n-\n-  if (src.first()->is_stack()) {\n-\n-    \/\/ Oop is already on the stack as an argument\n-    int offset_in_older_frame = src.first()->reg2stack() + SharedRuntime::out_preserve_stack_slots();\n-    map->set_oop(VMRegImpl::stack2reg(offset_in_older_frame + framesize_in_slots));\n-    if (is_receiver) {\n-      *receiver_offset = (offset_in_older_frame + framesize_in_slots) * VMRegImpl::stack_slot_size;\n-    }\n-\n-    __ cmpptr(Address(rbp, reg2offset_in(src.first())), (int32_t)NULL_WORD);\n-    __ lea(rHandle, Address(rbp, reg2offset_in(src.first())));\n-    \/\/ conditionally move a NULL\n-    __ cmovptr(Assembler::equal, rHandle, Address(rbp, reg2offset_in(src.first())));\n-  } else {\n-\n-    \/\/ Oop is in an a register we must store it to the space we reserve\n-    \/\/ on the stack for oop_handles and pass a handle if oop is non-NULL\n-\n-    const Register rOop = src.first()->as_Register();\n-    int oop_slot;\n-    if (rOop == j_rarg0)\n-      oop_slot = 0;\n-    else if (rOop == j_rarg1)\n-      oop_slot = 1;\n-    else if (rOop == j_rarg2)\n-      oop_slot = 2;\n-    else if (rOop == j_rarg3)\n-      oop_slot = 3;\n-    else if (rOop == j_rarg4)\n-      oop_slot = 4;\n-    else {\n-      assert(rOop == j_rarg5, \"wrong register\");\n-      oop_slot = 5;\n-    }\n-\n-    oop_slot = oop_slot * VMRegImpl::slots_per_word + oop_handle_offset;\n-    int offset = oop_slot*VMRegImpl::stack_slot_size;\n-\n-    map->set_oop(VMRegImpl::stack2reg(oop_slot));\n-    \/\/ Store oop in handle area, may be NULL\n-    __ movptr(Address(rsp, offset), rOop);\n-    if (is_receiver) {\n-      *receiver_offset = offset;\n-    }\n-\n-    __ cmpptr(rOop, (int32_t)NULL_WORD);\n-    __ lea(rHandle, Address(rsp, offset));\n-    \/\/ conditionally move a NULL from the handle area where it was just stored\n-    __ cmovptr(Assembler::equal, rHandle, Address(rsp, offset));\n-  }\n-\n-  \/\/ If arg is on the stack then place it otherwise it is already in correct reg.\n-  if (dst.first()->is_stack()) {\n-    __ movptr(Address(rsp, reg2offset_out(dst.first())), rHandle);\n-  }\n-}\n-\n-\/\/ A float arg may have to do float reg int reg conversion\n-void SharedRuntime::float_move(MacroAssembler* masm, VMRegPair src, VMRegPair dst) {\n-  assert(!src.second()->is_valid() && !dst.second()->is_valid(), \"bad float_move\");\n-\n-  \/\/ The calling conventions assures us that each VMregpair is either\n-  \/\/ all really one physical register or adjacent stack slots.\n-\n-  if (src.first()->is_stack()) {\n-    if (dst.first()->is_stack()) {\n-      __ movl(rax, Address(rbp, reg2offset_in(src.first())));\n-      __ movptr(Address(rsp, reg2offset_out(dst.first())), rax);\n-    } else {\n-      \/\/ stack to reg\n-      assert(dst.first()->is_XMMRegister(), \"only expect xmm registers as parameters\");\n-      __ movflt(dst.first()->as_XMMRegister(), Address(rbp, reg2offset_in(src.first())));\n-    }\n-  } else if (dst.first()->is_stack()) {\n-    \/\/ reg to stack\n-    assert(src.first()->is_XMMRegister(), \"only expect xmm registers as parameters\");\n-    __ movflt(Address(rsp, reg2offset_out(dst.first())), src.first()->as_XMMRegister());\n-  } else {\n-    \/\/ reg to reg\n-    \/\/ In theory these overlap but the ordering is such that this is likely a nop\n-    if ( src.first() != dst.first()) {\n-      __ movdbl(dst.first()->as_XMMRegister(),  src.first()->as_XMMRegister());\n-    }\n-  }\n-}\n-\n-\/\/ A long move\n-void SharedRuntime::long_move(MacroAssembler* masm, VMRegPair src, VMRegPair dst) {\n-\n-  \/\/ The calling conventions assures us that each VMregpair is either\n-  \/\/ all really one physical register or adjacent stack slots.\n-\n-  if (src.is_single_phys_reg() ) {\n-    if (dst.is_single_phys_reg()) {\n-      if (dst.first() != src.first()) {\n-        __ mov(dst.first()->as_Register(), src.first()->as_Register());\n-      }\n-    } else {\n-      assert(dst.is_single_reg(), \"not a stack pair\");\n-      __ movq(Address(rsp, reg2offset_out(dst.first())), src.first()->as_Register());\n-    }\n-  } else if (dst.is_single_phys_reg()) {\n-    assert(src.is_single_reg(),  \"not a stack pair\");\n-    __ movq(dst.first()->as_Register(), Address(rbp, reg2offset_out(src.first())));\n-  } else {\n-    assert(src.is_single_reg() && dst.is_single_reg(), \"not stack pairs\");\n-    __ movq(rax, Address(rbp, reg2offset_in(src.first())));\n-    __ movq(Address(rsp, reg2offset_out(dst.first())), rax);\n-  }\n-}\n-\n-\/\/ A double move\n-void SharedRuntime::double_move(MacroAssembler* masm, VMRegPair src, VMRegPair dst) {\n-\n-  \/\/ The calling conventions assures us that each VMregpair is either\n-  \/\/ all really one physical register or adjacent stack slots.\n-\n-  if (src.is_single_phys_reg() ) {\n-    if (dst.is_single_phys_reg()) {\n-      \/\/ In theory these overlap but the ordering is such that this is likely a nop\n-      if ( src.first() != dst.first()) {\n-        __ movdbl(dst.first()->as_XMMRegister(), src.first()->as_XMMRegister());\n-      }\n-    } else {\n-      assert(dst.is_single_reg(), \"not a stack pair\");\n-      __ movdbl(Address(rsp, reg2offset_out(dst.first())), src.first()->as_XMMRegister());\n-    }\n-  } else if (dst.is_single_phys_reg()) {\n-    assert(src.is_single_reg(),  \"not a stack pair\");\n-    __ movdbl(dst.first()->as_XMMRegister(), Address(rbp, reg2offset_out(src.first())));\n-  } else {\n-    assert(src.is_single_reg() && dst.is_single_reg(), \"not stack pairs\");\n-    __ movq(rax, Address(rbp, reg2offset_in(src.first())));\n-    __ movq(Address(rsp, reg2offset_out(dst.first())), rax);\n-  }\n-}\n-\n-\n@@ -1441,1 +1225,1 @@\n-    move_ptr(masm, reg, tmp);\n+    __ move_ptr(reg, tmp);\n@@ -1447,1 +1231,1 @@\n-  move_ptr(masm, tmp, body_arg);\n+  __ move_ptr(tmp, body_arg);\n@@ -1451,1 +1235,1 @@\n-  SharedRuntime::move32_64(masm, tmp, length_arg);\n+  __ move32_64(tmp, length_arg);\n@@ -1456,2 +1240,2 @@\n-  move_ptr(masm, tmp, body_arg);\n-  SharedRuntime::move32_64(masm, tmp, length_arg);\n+  __ move_ptr(tmp, body_arg);\n+  __ move32_64(tmp, length_arg);\n@@ -2141,1 +1925,1 @@\n-        object_move(masm, map, oop_handle_offset, stack_slots, in_regs[i], out_regs[c_arg],\n+        __ object_move(map, oop_handle_offset, stack_slots, in_regs[i], out_regs[c_arg],\n@@ -2149,1 +1933,1 @@\n-        float_move(masm, in_regs[i], out_regs[c_arg]);\n+        __ float_move(in_regs[i], out_regs[c_arg]);\n@@ -2156,1 +1940,1 @@\n-        double_move(masm, in_regs[i], out_regs[c_arg]);\n+        __ double_move(in_regs[i], out_regs[c_arg]);\n@@ -2160,1 +1944,1 @@\n-        long_move(masm, in_regs[i], out_regs[c_arg]);\n+        __ long_move(in_regs[i], out_regs[c_arg]);\n@@ -2166,1 +1950,1 @@\n-        move32_64(masm, in_regs[i], out_regs[c_arg]);\n+        __ move32_64(in_regs[i], out_regs[c_arg]);\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_64.cpp","additions":10,"deletions":226,"binary":false,"changes":236,"status":"modified"},{"patch":"@@ -556,1 +556,1 @@\n-       SharedRuntime::move32_64(_masm, from_vmreg, to_vmreg);\n+       __ move32_64(from_vmreg, to_vmreg);\n@@ -560,1 +560,1 @@\n-        SharedRuntime::float_move(_masm, from_vmreg, to_vmreg);\n+        __ float_move(from_vmreg, to_vmreg);\n@@ -564,1 +564,1 @@\n-        SharedRuntime::double_move(_masm, from_vmreg, to_vmreg);\n+        __ double_move(from_vmreg, to_vmreg);\n@@ -568,1 +568,1 @@\n-        SharedRuntime::long_move(_masm, from_vmreg, to_vmreg);\n+        __ long_move(from_vmreg, to_vmreg);\n","filename":"src\/hotspot\/cpu\/x86\/universalUpcallHandler_x86_64.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -37,0 +37,5 @@\n+\n+const CallRegs ForeignGlobals::parse_call_regs_impl(jobject jconv) const {\n+  ShouldNotCallThis();\n+  return {};\n+}\n","filename":"src\/hotspot\/cpu\/zero\/foreign_globals_zero.cpp","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -31,0 +31,9 @@\n+\n+address ProgrammableUpcallHandler::generate_optimized_upcall_stub(jobject mh, Method* entry, jobject jabi, jobject jconv) {\n+  ShouldNotCallThis();\n+  return nullptr;\n+}\n+\n+bool ProgrammableUpcallHandler::supports_optimized_upcalls() {\n+  return false;\n+}\n","filename":"src\/hotspot\/cpu\/zero\/universalUpcallHandle_zero.cpp","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -465,5 +465,0 @@\n-  static void   move32_64(MacroAssembler* masm, VMRegPair src, VMRegPair dst);\n-  static void   long_move(MacroAssembler* masm, VMRegPair src, VMRegPair dst);\n-  static void  float_move(MacroAssembler* masm, VMRegPair src, VMRegPair dst);\n-  static void double_move(MacroAssembler* masm, VMRegPair src, VMRegPair dst);\n-\n","filename":"src\/hotspot\/share\/runtime\/sharedRuntime.hpp","additions":0,"deletions":5,"binary":false,"changes":5,"status":"modified"}]}