{"files":[{"patch":"@@ -105,58 +105,0 @@\n-struct zmm_vector<uint32_t> {\n-    using type_t = uint32_t;\n-    using zmm_t = __m512i;\n-    using ymm_t = __m256i;\n-    using opmask_t = __mmask16;\n-    static const uint8_t numlanes = 16;\n-\n-    static type_t type_max() { return X86_SIMD_SORT_MAX_UINT32; }\n-    static type_t type_min() { return 0; }\n-    static zmm_t zmm_max() {\n-        return _mm512_set1_epi32(type_max());\n-    }  \/\/ TODO: this should broadcast bits as is?\n-\n-    template <int scale>\n-    static ymm_t i64gather(__m512i index, void const *base) {\n-        return _mm512_i64gather_epi32(index, base, scale);\n-    }\n-    static zmm_t merge(ymm_t y1, ymm_t y2) {\n-        zmm_t z1 = _mm512_castsi256_si512(y1);\n-        return _mm512_inserti32x8(z1, y2, 1);\n-    }\n-    static opmask_t knot_opmask(opmask_t x) { return _mm512_knot(x); }\n-    static opmask_t ge(zmm_t x, zmm_t y) {\n-        return _mm512_cmp_epu32_mask(x, y, _MM_CMPINT_NLT);\n-    }\n-    static zmm_t loadu(void const *mem) { return _mm512_loadu_si512(mem); }\n-    static zmm_t max(zmm_t x, zmm_t y) { return _mm512_max_epu32(x, y); }\n-    static void mask_compressstoreu(void *mem, opmask_t mask, zmm_t x) {\n-        return _mm512_mask_compressstoreu_epi32(mem, mask, x);\n-    }\n-    static zmm_t mask_loadu(zmm_t x, opmask_t mask, void const *mem) {\n-        return _mm512_mask_loadu_epi32(x, mask, mem);\n-    }\n-    static zmm_t mask_mov(zmm_t x, opmask_t mask, zmm_t y) {\n-        return _mm512_mask_mov_epi32(x, mask, y);\n-    }\n-    static void mask_storeu(void *mem, opmask_t mask, zmm_t x) {\n-        return _mm512_mask_storeu_epi32(mem, mask, x);\n-    }\n-    static zmm_t min(zmm_t x, zmm_t y) { return _mm512_min_epu32(x, y); }\n-    static zmm_t permutexvar(__m512i idx, zmm_t zmm) {\n-        return _mm512_permutexvar_epi32(idx, zmm);\n-    }\n-    static type_t reducemax(zmm_t v) { return _mm512_reduce_max_epu32(v); }\n-    static type_t reducemin(zmm_t v) { return _mm512_reduce_min_epu32(v); }\n-    static zmm_t set1(type_t v) { return _mm512_set1_epi32(v); }\n-    template <uint8_t mask>\n-    static zmm_t shuffle(zmm_t zmm) {\n-        return _mm512_shuffle_epi32(zmm, (_MM_PERM_ENUM)mask);\n-    }\n-    static void storeu(void *mem, zmm_t x) {\n-        return _mm512_storeu_si512(mem, x);\n-    }\n-\n-    static ymm_t max(ymm_t x, ymm_t y) { return _mm256_max_epu32(x, y); }\n-    static ymm_t min(ymm_t x, ymm_t y) { return _mm256_min_epu32(x, y); }\n-};\n-template <>\n@@ -535,8 +477,0 @@\n-template <>\n-void avx512_qsort<uint32_t>(uint32_t *arr, int64_t arrsize) {\n-    if (arrsize > 1) {\n-        qsort_32bit_<zmm_vector<uint32_t>, uint32_t>(\n-            arr, 0, arrsize - 1, 2 * (int64_t)log2(arrsize));\n-    }\n-}\n-\n","filename":"src\/java.base\/linux\/native\/libavx512_x86_64\/avx512-32bit-qsort.hpp","additions":0,"deletions":66,"binary":false,"changes":66,"status":"modified"},{"patch":"@@ -44,192 +44,0 @@\n-template <>\n-struct ymm_vector<float> {\n-    using type_t = float;\n-    using zmm_t = __m256;\n-    using zmmi_t = __m256i;\n-    using opmask_t = __mmask8;\n-    static const uint8_t numlanes = 8;\n-\n-    static type_t type_max() { return X86_SIMD_SORT_INFINITYF; }\n-    static type_t type_min() { return -X86_SIMD_SORT_INFINITYF; }\n-    static zmm_t zmm_max() { return _mm256_set1_ps(type_max()); }\n-\n-    static zmmi_t seti(int v1, int v2, int v3, int v4, int v5, int v6, int v7,\n-                       int v8) {\n-        return _mm256_set_epi32(v1, v2, v3, v4, v5, v6, v7, v8);\n-    }\n-    static opmask_t kxor_opmask(opmask_t x, opmask_t y) {\n-        return _kxor_mask8(x, y);\n-    }\n-    static opmask_t knot_opmask(opmask_t x) { return _knot_mask8(x); }\n-    static opmask_t le(zmm_t x, zmm_t y) {\n-        return _mm256_cmp_ps_mask(x, y, _CMP_LE_OQ);\n-    }\n-    static opmask_t ge(zmm_t x, zmm_t y) {\n-        return _mm256_cmp_ps_mask(x, y, _CMP_GE_OQ);\n-    }\n-    static opmask_t eq(zmm_t x, zmm_t y) {\n-        return _mm256_cmp_ps_mask(x, y, _CMP_EQ_OQ);\n-    }\n-    template <int type>\n-    static opmask_t fpclass(zmm_t x) {\n-        return _mm256_fpclass_ps_mask(x, type);\n-    }\n-    template <int scale>\n-    static zmm_t mask_i64gather(zmm_t src, opmask_t mask, __m512i index,\n-                                void const *base) {\n-        return _mm512_mask_i64gather_ps(src, mask, index, base, scale);\n-    }\n-    template <int scale>\n-    static zmm_t i64gather(__m512i index, void const *base) {\n-        return _mm512_i64gather_ps(index, base, scale);\n-    }\n-    static zmm_t loadu(void const *mem) {\n-        return _mm256_loadu_ps((float *)mem);\n-    }\n-    static zmm_t max(zmm_t x, zmm_t y) { return _mm256_max_ps(x, y); }\n-    static void mask_compressstoreu(void *mem, opmask_t mask, zmm_t x) {\n-        return _mm256_mask_compressstoreu_ps(mem, mask, x);\n-    }\n-    static zmm_t maskz_loadu(opmask_t mask, void const *mem) {\n-        return _mm256_maskz_loadu_ps(mask, mem);\n-    }\n-    static zmm_t mask_loadu(zmm_t x, opmask_t mask, void const *mem) {\n-        return _mm256_mask_loadu_ps(x, mask, mem);\n-    }\n-    static zmm_t mask_mov(zmm_t x, opmask_t mask, zmm_t y) {\n-        return _mm256_mask_mov_ps(x, mask, y);\n-    }\n-    static void mask_storeu(void *mem, opmask_t mask, zmm_t x) {\n-        return _mm256_mask_storeu_ps(mem, mask, x);\n-    }\n-    static zmm_t min(zmm_t x, zmm_t y) { return _mm256_min_ps(x, y); }\n-    static zmm_t permutexvar(__m256i idx, zmm_t zmm) {\n-        return _mm256_permutexvar_ps(idx, zmm);\n-    }\n-    static type_t reducemax(zmm_t v) {\n-        __m128 v128 =\n-            _mm_max_ps(_mm256_castps256_ps128(v), _mm256_extractf32x4_ps(v, 1));\n-        __m128 v64 = _mm_max_ps(\n-            v128, _mm_shuffle_ps(v128, v128, _MM_SHUFFLE(1, 0, 3, 2)));\n-        __m128 v32 =\n-            _mm_max_ps(v64, _mm_shuffle_ps(v64, v64, _MM_SHUFFLE(0, 0, 0, 1)));\n-        return _mm_cvtss_f32(v32);\n-    }\n-    static type_t reducemin(zmm_t v) {\n-        __m128 v128 =\n-            _mm_min_ps(_mm256_castps256_ps128(v), _mm256_extractf32x4_ps(v, 1));\n-        __m128 v64 = _mm_min_ps(\n-            v128, _mm_shuffle_ps(v128, v128, _MM_SHUFFLE(1, 0, 3, 2)));\n-        __m128 v32 =\n-            _mm_min_ps(v64, _mm_shuffle_ps(v64, v64, _MM_SHUFFLE(0, 0, 0, 1)));\n-        return _mm_cvtss_f32(v32);\n-    }\n-    static zmm_t set1(type_t v) { return _mm256_set1_ps(v); }\n-    template <uint8_t mask, bool = (mask == 0b01010101)>\n-    static zmm_t shuffle(zmm_t zmm) {\n-        \/* Hack!: have to make shuffles within 128-bit lanes work for both\n-         * 32-bit and 64-bit *\/\n-        return _mm256_shuffle_ps(zmm, zmm, 0b10110001);\n-        \/\/ if constexpr (mask == 0b01010101) {\n-        \/\/ }\n-        \/\/ else {\n-        \/\/     \/* Not used, so far *\/\n-        \/\/     return _mm256_shuffle_ps(zmm, zmm, mask);\n-        \/\/ }\n-    }\n-    static void storeu(void *mem, zmm_t x) {\n-        _mm256_storeu_ps((float *)mem, x);\n-    }\n-};\n-template <>\n-struct ymm_vector<int32_t> {\n-    using type_t = int32_t;\n-    using zmm_t = __m256i;\n-    using zmmi_t = __m256i;\n-    using opmask_t = __mmask8;\n-    static const uint8_t numlanes = 8;\n-\n-    static type_t type_max() { return X86_SIMD_SORT_MAX_INT32; }\n-    static type_t type_min() { return X86_SIMD_SORT_MIN_INT32; }\n-    static zmm_t zmm_max() {\n-        return _mm256_set1_epi32(type_max());\n-    }  \/\/ TODO: this should broadcast bits as is?\n-\n-    static zmmi_t seti(int v1, int v2, int v3, int v4, int v5, int v6, int v7,\n-                       int v8) {\n-        return _mm256_set_epi32(v1, v2, v3, v4, v5, v6, v7, v8);\n-    }\n-    static opmask_t kxor_opmask(opmask_t x, opmask_t y) {\n-        return _kxor_mask8(x, y);\n-    }\n-    static opmask_t knot_opmask(opmask_t x) { return _knot_mask8(x); }\n-    static opmask_t le(zmm_t x, zmm_t y) {\n-        return _mm256_cmp_epi32_mask(x, y, _MM_CMPINT_LE);\n-    }\n-    static opmask_t ge(zmm_t x, zmm_t y) {\n-        return _mm256_cmp_epi32_mask(x, y, _MM_CMPINT_NLT);\n-    }\n-    static opmask_t eq(zmm_t x, zmm_t y) {\n-        return _mm256_cmp_epi32_mask(x, y, _MM_CMPINT_EQ);\n-    }\n-    template <int scale>\n-    static zmm_t mask_i64gather(zmm_t src, opmask_t mask, __m512i index,\n-                                void const *base) {\n-        return _mm512_mask_i64gather_epi32(src, mask, index, base, scale);\n-    }\n-    template <int scale>\n-    static zmm_t i64gather(__m512i index, void const *base) {\n-        return _mm512_i64gather_epi32(index, base, scale);\n-    }\n-    static zmm_t loadu(void const *mem) {\n-        return _mm256_loadu_si256((__m256i *)mem);\n-    }\n-    static zmm_t max(zmm_t x, zmm_t y) { return _mm256_max_epi32(x, y); }\n-    static void mask_compressstoreu(void *mem, opmask_t mask, zmm_t x) {\n-        return _mm256_mask_compressstoreu_epi32(mem, mask, x);\n-    }\n-    static zmm_t maskz_loadu(opmask_t mask, void const *mem) {\n-        return _mm256_maskz_loadu_epi32(mask, mem);\n-    }\n-    static zmm_t mask_loadu(zmm_t x, opmask_t mask, void const *mem) {\n-        return _mm256_mask_loadu_epi32(x, mask, mem);\n-    }\n-    static zmm_t mask_mov(zmm_t x, opmask_t mask, zmm_t y) {\n-        return _mm256_mask_mov_epi32(x, mask, y);\n-    }\n-    static void mask_storeu(void *mem, opmask_t mask, zmm_t x) {\n-        return _mm256_mask_storeu_epi32(mem, mask, x);\n-    }\n-    static zmm_t min(zmm_t x, zmm_t y) { return _mm256_min_epi32(x, y); }\n-    static zmm_t permutexvar(__m256i idx, zmm_t zmm) {\n-        return _mm256_permutexvar_epi32(idx, zmm);\n-    }\n-    static type_t reducemax(zmm_t v) {\n-        __m128i v128 = _mm_max_epi32(_mm256_castsi256_si128(v),\n-                                     _mm256_extracti128_si256(v, 1));\n-        __m128i v64 = _mm_max_epi32(\n-            v128, _mm_shuffle_epi32(v128, _MM_SHUFFLE(1, 0, 3, 2)));\n-        __m128i v32 =\n-            _mm_max_epi32(v64, _mm_shuffle_epi32(v64, _MM_SHUFFLE(0, 0, 0, 1)));\n-        return (type_t)_mm_cvtsi128_si32(v32);\n-    }\n-    static type_t reducemin(zmm_t v) {\n-        __m128i v128 = _mm_min_epi32(_mm256_castsi256_si128(v),\n-                                     _mm256_extracti128_si256(v, 1));\n-        __m128i v64 = _mm_min_epi32(\n-            v128, _mm_shuffle_epi32(v128, _MM_SHUFFLE(1, 0, 3, 2)));\n-        __m128i v32 =\n-            _mm_min_epi32(v64, _mm_shuffle_epi32(v64, _MM_SHUFFLE(0, 0, 0, 1)));\n-        return (type_t)_mm_cvtsi128_si32(v32);\n-    }\n-    static zmm_t set1(type_t v) { return _mm256_set1_epi32(v); }\n-    template <uint8_t mask, bool = (mask == 0b01010101)>\n-    static zmm_t shuffle(zmm_t zmm) {\n-        \/* Hack!: have to make shuffles within 128-bit lanes work for both\n-         * 32-bit and 64-bit *\/\n-        return _mm256_shuffle_epi32(zmm, 0b10110001);\n-    }\n-    static void storeu(void *mem, zmm_t x) {\n-        _mm256_storeu_si256((__m256i *)mem, x);\n-    }\n-};\n","filename":"src\/java.base\/linux\/native\/libavx512_x86_64\/avx512-64bit-common.h","additions":0,"deletions":192,"binary":false,"changes":192,"status":"modified"}]}