{"files":[{"patch":"@@ -7857,0 +7857,12 @@\n+void Assembler::vbroadcastf128(XMMRegister dst, Address src, int vector_len) {\n+  assert(VM_Version::supports_avx(), \"\");\n+  assert(vector_len == AVX_256bit || vector_len == AVX_512bit, \"\");\n+  assert(dst != xnoreg, \"sanity\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_T4, \/* input_size_in_bits *\/ EVEX_32bit);\n+  \/\/ swap src<->dst for encoding\n+  vex_prefix(src, 0, dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8(0x1A);\n+  emit_operand(dst, src);\n+}\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.cpp","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -2397,1 +2397,1 @@\n-  \/\/ scalar single\/double precision replicate\n+  \/\/ scalar single\/double\/128bit precision replicate\n@@ -2402,0 +2402,1 @@\n+  void vbroadcastf128(XMMRegister dst, Address src, int vector_len);\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -3247,0 +3247,12 @@\n+void MacroAssembler::vpmulld(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len) {\n+  \/\/ Used in sign-bit flipping with aligned address.\n+  bool aligned_adr = (((intptr_t)src.target() & 15) == 0);\n+  assert((UseAVX > 0) || aligned_adr, \"SSE mode requires address alignment 16 bytes\");\n+  if (reachable(src)) {\n+    Assembler::vpmulld(dst, nds, as_Address(src), vector_len);\n+  } else {\n+    lea(rscratch1, src);\n+    Assembler::vpmulld(dst, nds, Address(rscratch1, 0), vector_len);\n+  }\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -1316,0 +1316,7 @@\n+  void vpmulld(XMMRegister dst, XMMRegister nds, Address src, int vector_len) {\n+    Assembler::vpmulld(dst, nds, src, vector_len);\n+  };\n+  void vpmulld(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {\n+    Assembler::vpmulld(dst, nds, src, vector_len);\n+  }\n+  void vpmulld(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -5793,0 +5793,209 @@\n+\n+  \/***\n+   *  Arguments:\n+   *\n+   *  Inputs:\n+   *   c_rarg0   - int   adler\n+   *   c_rarg1   - byte* buff\n+   *   c_rarg2   - int   len\n+   *\n+   * Output:\n+   *   rax   - int adler result\n+   *\/\n+\n+  address generate_updateBytesAdler32() {\n+      assert(UseAdler32Intrinsics, \"need AVX2\");\n+\n+      __ align(CodeEntryAlignment);\n+      StubCodeMark mark(this, \"StubRoutines\", \"updateBytesAdler32\");\n+\n+      address start = __ pc();\n+\n+      const int LIMIT = 5552;\n+      const int BASE = 65521;\n+      const int CHUNKSIZE =  16;\n+      const int CHUNKSIZE_M1 = CHUNKSIZE - 1;\n+\n+      const Register init_d = c_rarg0; \/\/init adler\n+      const Register data = r9;\n+      const Register size = r10;\n+      const Register s = r11;\n+      const Register a_d = r12; \/\/r12d\n+      const Register b_d = r8; \/\/r8d\n+      const Register end = r13;\n+\n+      const XMMRegister ya = xmm0;\n+      const XMMRegister yb = xmm1;\n+      const XMMRegister ydata0 = xmm2;\n+      const XMMRegister ydata1 = xmm3;\n+      const XMMRegister ysa = xmm4;\n+      const XMMRegister ydata = ysa;\n+      const XMMRegister ytmp0 = ydata0;\n+      const XMMRegister ytmp1 = ydata1;\n+      const XMMRegister ytmp2 = xmm5;\n+      const XMMRegister xa = xmm0;\n+      const XMMRegister xb = xmm1;\n+      const XMMRegister xtmp0 = xmm2;\n+      const XMMRegister xtmp1 = xmm3;\n+      const XMMRegister xsa = xmm4;\n+      const XMMRegister xtmp2 = xmm5;\n+      const XMMRegister yshuf0 = xmm6;\n+      const XMMRegister yshuf1 = xmm7;\n+      assert_different_registers(init_d, c_rarg1, c_rarg2, data, size, s, a_d, b_d, end, rax);\n+\n+      Label SLOOP1, SLOOP1A, SKIP_LOOP_1A, FINISH, LT64, DO_FINAL, FINAL_LOOP, ZERO_SIZE, END;\n+\n+      BLOCK_COMMENT(\"Entry:\");\n+      __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+\n+      __ push(r12);\n+      __ push(r13);\n+      __ vmovdqu(yshuf0, ExternalAddress((address) StubRoutines::x86::_adler32_shuf0_table));\n+      __ vmovdqu(yshuf1, ExternalAddress((address) StubRoutines::x86::_adler32_shuf1_table));\n+      __ movptr(data, c_rarg1); \/\/data\n+      __ movl(size, c_rarg2); \/\/length\n+      __ movl(b_d, init_d); \/\/adler\n+      __ shrl(b_d, 16);\n+      __ andl(init_d, 0xFFFF);\n+      __ cmpl(size, 32);\n+      __ jcc(Assembler::below, LT64);\n+      __ movdl(xa, init_d); \/\/vmovd - 32bit\n+      __ vpxor(yb, yb, yb, Assembler::AVX_256bit);\n+\n+      __ BIND(SLOOP1);\n+      __ movl(s, LIMIT);\n+      __ cmpl(s, size);\n+      __ cmovl(Assembler::above, s, size); \/\/ s = min(size, LIMIT)\n+      __ lea(end, Address(s, data, Address::times_1, -CHUNKSIZE_M1));\n+      __ cmpq(data, end);\n+      __ jcc(Assembler::aboveEqual, SKIP_LOOP_1A);\n+\n+      __ BIND(SLOOP1A);\n+      __ vbroadcastf128(ydata, Address(data, 0), Assembler::AVX_256bit);\n+      __ addptr(data, CHUNKSIZE);\n+      __ vpshufb(ydata0, ydata, yshuf0, Assembler::AVX_256bit);\n+      __ vpaddd(ya, ya, ydata0, Assembler::AVX_256bit);\n+      __ vpaddd(yb, yb, ya, Assembler::AVX_256bit);\n+      __ vpshufb(ydata1, ydata, yshuf1, Assembler::AVX_256bit);\n+      __ vpaddd(ya, ya, ydata1, Assembler::AVX_256bit);\n+      __ vpaddd(yb, yb, ya, Assembler::AVX_256bit);\n+      __ cmpptr(data, end);\n+      __ jcc(Assembler::below, SLOOP1A);\n+\n+      __ BIND(SKIP_LOOP_1A);\n+      __ addptr(end, CHUNKSIZE_M1);\n+      __ testl(s, CHUNKSIZE_M1);\n+      __ jcc(Assembler::notEqual, DO_FINAL);\n+\n+      \/\/ either we're done, or we just did LIMIT\n+      __ subl(size, s);\n+\n+      \/\/ reduce\n+      __ vpslld(yb, yb, 3, Assembler::AVX_256bit); \/\/b is scaled by 8\n+      __ vpmulld(ysa, ya, ExternalAddress((address) StubRoutines::x86::_adler32_ascale_table), Assembler::AVX_256bit); \/\/need scratch register??\n+\n+      \/\/ compute horizontal sums of ya, yb, ysa\n+      __ vextracti128(xtmp0, ya, 1);\n+      __ vextracti128(xtmp1, yb, 1);\n+      __ vextracti128(xtmp2, ysa, 1);\n+      __ vpaddd(xa, xa, xtmp0, Assembler::AVX_256bit);\n+      __ vpaddd(xb, xb, xtmp1, Assembler::AVX_256bit);\n+      __ vpaddd(xsa, xsa, xtmp2, Assembler::AVX_256bit);\n+      __ vphaddd(xa, xa, xa, Assembler::AVX_128bit);\n+      __ vphaddd(xb, xb, xb, Assembler::AVX_128bit);\n+      __ vphaddd(xsa, xsa, xsa, Assembler::AVX_128bit);\n+      __ vphaddd(xa, xa, xa, Assembler::AVX_128bit);\n+      __ vphaddd(xb, xb, xb, Assembler::AVX_128bit);\n+      __ vphaddd(xsa, xsa, xsa, Assembler::AVX_128bit);\n+\n+      __ movdl(rax, xa);\n+      __ xorl(rdx, rdx);\n+      __ movl(rcx, BASE);\n+      __ divl(rcx); \/\/ divide edx:eax by ecx, quot->eax, rem->edx\n+      __ movl(a_d, rdx);\n+\n+      __ vpsubd(xb, xb, xsa, Assembler::AVX_128bit);\n+      __ movdl(rax, xb);\n+      __ addl(rax, b_d);\n+      __ xorl(rdx, rdx);\n+      __ movl(rcx, BASE);\n+      __ divl(rcx); \/\/ divide edx:eax by ecx, quot->eax, rem->edx\n+      __ movl(b_d, rdx);\n+\n+      __ testl(size, size);\n+      __ jcc(Assembler::zero, FINISH);\n+\n+      \/\/ continue loop\n+      __ movdl(xa, a_d);\n+      __ vpxor(yb, yb, yb, Assembler::AVX_256bit);\n+      __ jmp(SLOOP1);\n+\n+      __ BIND(FINISH);\n+      __ movl(rax, b_d);\n+      __ shll(rax, 16);\n+      __ orl(rax, a_d);\n+      __ jmp(END);\n+\n+      __ BIND(LT64);\n+      __ movl(a_d, init_d);\n+      __ lea(end, Address(data, size, Address::times_1));\n+      __ testl(size, size);\n+      __ jcc(Assembler::notZero, FINAL_LOOP);\n+      __ jmp(ZERO_SIZE);\n+\n+      \/\/ handle remaining 1...15 bytes\n+      __ BIND(DO_FINAL);\n+      \/\/ reduce\n+      __ vpslld(yb, yb, 3, Assembler::AVX_256bit); \/\/b is scaled by 8\n+      __ vpmulld(ysa, ya, ExternalAddress((address) StubRoutines::x86::_adler32_ascale_table), Assembler::AVX_256bit); \/\/scaled a\n+\n+      __ vextracti128(xtmp0, ya, 1);\n+      __ vextracti128(xtmp1, yb, 1);\n+      __ vextracti128(xtmp2, ysa, 1);\n+      __ vpaddd(xa, xa, xtmp0, Assembler::AVX_128bit);\n+      __ vpaddd(xb, xb, xtmp1, Assembler::AVX_128bit);\n+      __ vpaddd(xsa, xsa, xtmp2, Assembler::AVX_128bit);\n+      __ vphaddd(xa, xa, xa, Assembler::AVX_128bit);\n+      __ vphaddd(xb, xb, xb, Assembler::AVX_128bit);\n+      __ vphaddd(xsa, xsa, xsa, Assembler::AVX_128bit);\n+      __ vphaddd(xa, xa, xa, Assembler::AVX_128bit);\n+      __ vphaddd(xb, xb, xb, Assembler::AVX_128bit);\n+      __ vphaddd(xsa, xsa, xsa, Assembler::AVX_128bit);\n+      __ vpsubd(xb, xb, xsa, Assembler::AVX_128bit);\n+\n+      __ movdl(a_d, xa);\n+      __ movdl(rax, xb);\n+      __ addl(b_d, rax);\n+\n+      __ BIND(FINAL_LOOP);\n+      __ movzbl(rax, Address(data, 0)); \/\/movzx   eax, byte[data]\n+      __ addl(a_d, rax);\n+      __ incl(data);\n+      __ addl(b_d, a_d);\n+      __ cmpptr(data, end);\n+      __ jcc(Assembler::below, FINAL_LOOP);\n+\n+      __ BIND(ZERO_SIZE);\n+\n+      __ movl(rax, a_d);\n+      __ xorl(rdx, rdx);\n+      __ movl(rcx, BASE);\n+      __ divl(rcx); \/\/ div ecx -- divide edx:eax by ecx, quot->eax, rem->edx\n+      __ movl(a_d, rdx);\n+\n+      __ movl(rax, b_d);\n+      __ xorl(rdx, rdx);\n+      __ movl(rcx, BASE);\n+      __ divl(rcx); \/\/ divide edx:eax by ecx, quot->eax, rem->edx\n+      __ shll(rdx, 16);\n+      __ orl(rdx, a_d);\n+      __ movl(rax, rdx);\n+\n+      __ BIND(END);\n+      __ pop(r13);\n+      __ pop(r12);\n+      __ leave();\n+      __ ret(0);\n+      return start;\n+  }\n+\n@@ -6757,0 +6966,5 @@\n+\n+    if (VM_Version::supports_avx2() && UseAdler32Intrinsics) {\n+       StubRoutines::_updateBytesAdler32 = generate_updateBytesAdler32();\n+    }\n+\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.cpp","additions":214,"deletions":0,"binary":false,"changes":214,"status":"modified"},{"patch":"@@ -227,0 +227,19 @@\n+\n+juint StubRoutines::x86::_adler32_ascale_table[] =\n+{\n+    0x00000000UL, 0x00000001UL, 0x00000002UL, 0x00000003UL,\n+    0x00000004UL, 0x00000005UL, 0x00000006UL, 0x00000007UL\n+};\n+\n+juint StubRoutines::x86::_adler32_shuf0_table[] =\n+{\n+    0xFFFFFF00UL, 0xFFFFFF01UL, 0xFFFFFF02UL, 0xFFFFFF03UL,\n+    0xFFFFFF04UL, 0xFFFFFF05UL, 0xFFFFFF06UL, 0xFFFFFF07UL\n+};\n+\n+juint StubRoutines::x86::_adler32_shuf1_table[] =\n+{\n+    0xFFFFFF08UL, 0xFFFFFF09, 0xFFFFFF0AUL, 0xFFFFFF0BUL,\n+    0xFFFFFF0CUL, 0xFFFFFF0D, 0xFFFFFF0EUL, 0xFFFFFF0FUL\n+};\n+\n","filename":"src\/hotspot\/cpu\/x86\/stubRoutines_x86.cpp","additions":19,"deletions":0,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -122,0 +122,3 @@\n+  static juint    _adler32_shuf0_table[];\n+  static juint    _adler32_shuf1_table[];\n+  static juint    _adler32_ascale_table[];\n","filename":"src\/hotspot\/cpu\/x86\/stubRoutines_x86.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -901,0 +901,10 @@\n+  if (supports_avx2() && UseAdler32Intrinsics) {\n+    if (FLAG_IS_DEFAULT(UseAdler32Intrinsics)) {\n+      UseAdler32Intrinsics = true;\n+    }\n+  } else if (UseAdler32Intrinsics) {\n+    if (!FLAG_IS_DEFAULT(UseAdler32Intrinsics))\n+      warning(\"Adler32 Intrinsics requires avx2 instructions (not available on this CPU)\");\n+    FLAG_SET_DEFAULT(UseAdler32Intrinsics, false);\n+  }\n+\n@@ -996,5 +1006,0 @@\n-  if (UseAdler32Intrinsics) {\n-    warning(\"Adler32Intrinsics not available on this CPU.\");\n-    FLAG_SET_DEFAULT(UseAdler32Intrinsics, false);\n-  }\n-\n","filename":"src\/hotspot\/cpu\/x86\/vm_version_x86.cpp","additions":10,"deletions":5,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -96,0 +96,1 @@\n+  case vmIntrinsics::_updateBytesAdler32:\n","filename":"src\/hotspot\/share\/classfile\/vmIntrinsics.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -234,0 +234,1 @@\n+  do_bool_flag(UseAdler32Intrinsics)                                       \\\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompilerToVMInit.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -578,0 +578,1 @@\n+     static_field(StubRoutines,                _updateBytesAdler32,                           address)                               \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"}]}