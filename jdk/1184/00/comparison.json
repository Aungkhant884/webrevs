{"files":[{"patch":"@@ -32,1 +32,5 @@\n-void ZVirtualMemoryManager::pd_initialize() {\n+void ZVirtualMemoryManager::pd_initialize_before_reserve() {\n+  \/\/ Does nothing\n+}\n+\n+void ZVirtualMemoryManager::pd_initialize_after_reserve() {\n","filename":"src\/hotspot\/os\/posix\/gc\/z\/zVirtualMemory_posix.cpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -25,0 +25,1 @@\n+#include \"gc\/shared\/gcLogPrecious.hpp\"\n@@ -26,0 +27,2 @@\n+#include \"gc\/z\/zSyscall_windows.hpp\"\n+#include \"runtime\/globals.hpp\"\n@@ -28,0 +31,8 @@\n+  if (UseLargePages) {\n+    if (ZSyscall::is_large_pages_supported()) {\n+      _state = Explicit;\n+      return;\n+    }\n+    log_info_p(gc, init)(\"Shared large pages not supported on this OS version\");\n+  }\n+\n","filename":"src\/hotspot\/os\/windows\/gc\/z\/zLargePages_windows.cpp","additions":11,"deletions":0,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -202,0 +202,56 @@\n+HANDLE ZMapper::create_shared_awe_section() {\n+  MEM_EXTENDED_PARAMETER parameter = { 0 };\n+  parameter.Type = MemSectionExtendedParameterUserPhysicalFlags;\n+  parameter.ULong64 = 0;\n+\n+  HANDLE section = ZSyscall::CreateFileMapping2(\n+    INVALID_HANDLE_VALUE,                 \/\/ File\n+    NULL,                                 \/\/ SecurityAttributes\n+    SECTION_MAP_READ | SECTION_MAP_WRITE, \/\/ DesiredAccess\n+    PAGE_READWRITE,                       \/\/ PageProtection\n+    SEC_RESERVE | SEC_LARGE_PAGES,        \/\/ AllocationAttributes\n+    0,                                    \/\/ MaximumSize\n+    NULL,                                 \/\/ Name\n+    &parameter,                           \/\/ ExtendedParameters\n+    1                                     \/\/ ParameterCount\n+    );\n+\n+  if (section == NULL) {\n+    fatal(\"Could not create shared AWE section (%d)\", GetLastError());\n+  }\n+\n+  return section;\n+}\n+\n+uintptr_t ZMapper::reserve_for_shared_awe(HANDLE awe_section, uintptr_t addr, size_t size) {\n+  MEM_EXTENDED_PARAMETER parameter = { 0 };\n+  parameter.Type = MemExtendedParameterUserPhysicalHandle;\n+  parameter.Handle = awe_section;\n+\n+  void* const res = ZSyscall::VirtualAlloc2(\n+    GetCurrentProcess(),        \/\/ Process\n+    (void*)addr,                \/\/ BaseAddress\n+    size,                       \/\/ Size\n+    MEM_RESERVE | MEM_PHYSICAL, \/\/ AllocationType\n+    PAGE_READWRITE,             \/\/ PageProtection\n+    &parameter,                 \/\/ ExtendedParameters\n+    1                           \/\/ ParameterCount\n+    );\n+\n+  \/\/ Caller responsible for error handling\n+  return (uintptr_t)res;\n+}\n+\n+void ZMapper::unreserve_for_shared_awe(uintptr_t addr, size_t size) {\n+  bool res = VirtualFree(\n+    (void*)addr, \/\/ lpAddress\n+    0,           \/\/ dwSize\n+    MEM_RELEASE  \/\/ dwFreeType\n+    );\n+\n+  if (!res) {\n+    fatal(\"Failed to unreserve memory: \" PTR_FORMAT \" \" SIZE_FORMAT \"M (%d)\",\n+          addr, size \/ M, GetLastError());\n+  }\n+}\n+\n","filename":"src\/hotspot\/os\/windows\/gc\/z\/zMapper_windows.cpp","additions":56,"deletions":0,"binary":false,"changes":56,"status":"modified"},{"patch":"@@ -62,0 +62,9 @@\n+  \/\/ Create a shared AWE section\n+  static HANDLE create_shared_awe_section();\n+\n+  \/\/ Reserve memory attached to the shared AWE section\n+  static uintptr_t reserve_for_shared_awe(HANDLE awe_section, uintptr_t addr, size_t size);\n+\n+  \/\/ Unreserve memory attached to a shared AWE section\n+  static void unreserve_for_shared_awe(uintptr_t addr, size_t size);\n+\n","filename":"src\/hotspot\/os\/windows\/gc\/z\/zMapper_windows.hpp","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"gc\/z\/zLargePages.inline.hpp\"\n@@ -33,0 +34,10 @@\n+class ZPhysicalMemoryBackingImpl : public CHeapObj<mtGC> {\n+public:\n+  virtual size_t commit(size_t offset, size_t size) = 0;\n+  virtual size_t uncommit(size_t offset, size_t size) = 0;\n+  virtual void map(uintptr_t addr, size_t size, size_t offset) const = 0;\n+  virtual void unmap(uintptr_t addr, size_t size) const = 0;\n+};\n+\n+\/\/ Implements small pages (paged) support using placeholder reservation.\n+\/\/\n@@ -38,2 +49,3 @@\n-ZPhysicalMemoryBacking::ZPhysicalMemoryBacking(size_t max_capacity) :\n-    _handles(max_capacity) {}\n+class ZPhysicalMemoryBackingSmallPages : public ZPhysicalMemoryBackingImpl {\n+private:\n+  ZGranuleMap<HANDLE> _handles;\n@@ -41,3 +53,5 @@\n-bool ZPhysicalMemoryBacking::is_initialized() const {\n-  return true;\n-}\n+  HANDLE get_handle(uintptr_t offset) const {\n+    HANDLE const handle = _handles.get(offset);\n+    assert(handle != 0, \"Should be set\");\n+    return handle;\n+  }\n@@ -45,3 +59,5 @@\n-void ZPhysicalMemoryBacking::warn_commit_limits(size_t max_capacity) const {\n-  \/\/ Does nothing\n-}\n+  void put_handle(uintptr_t offset, HANDLE handle) {\n+    assert(handle != INVALID_HANDLE_VALUE, \"Invalid handle\");\n+    assert(_handles.get(offset) == 0, \"Should be cleared\");\n+    _handles.put(offset, handle);\n+  }\n@@ -49,5 +65,4 @@\n-HANDLE ZPhysicalMemoryBacking::get_handle(uintptr_t offset) const {\n-  HANDLE const handle = _handles.get(offset);\n-  assert(handle != 0, \"Should be set\");\n-  return handle;\n-}\n+  void clear_handle(uintptr_t offset) {\n+    assert(_handles.get(offset) != 0, \"Should be set\");\n+    _handles.put(offset, 0);\n+  }\n@@ -55,5 +70,4 @@\n-void ZPhysicalMemoryBacking::put_handle(uintptr_t offset, HANDLE handle) {\n-  assert(handle != INVALID_HANDLE_VALUE, \"Invalid handle\");\n-  assert(_handles.get(offset) == 0, \"Should be cleared\");\n-  _handles.put(offset, handle);\n-}\n+public:\n+  ZPhysicalMemoryBackingSmallPages(size_t max_capacity) :\n+      ZPhysicalMemoryBackingImpl(),\n+      _handles(max_capacity) {}\n@@ -61,4 +75,6 @@\n-void ZPhysicalMemoryBacking::clear_handle(uintptr_t offset) {\n-  assert(_handles.get(offset) != 0, \"Should be set\");\n-  _handles.put(offset, 0);\n-}\n+  size_t commit(size_t offset, size_t size) {\n+    for (size_t i = 0; i < size; i += ZGranuleSize) {\n+      HANDLE const handle = ZMapper::create_and_commit_paging_file_mapping(ZGranuleSize);\n+      if (handle == 0) {\n+        return i;\n+      }\n@@ -66,5 +82,1 @@\n-size_t ZPhysicalMemoryBacking::commit_from_paging_file(size_t offset, size_t size) {\n-  for (size_t i = 0; i < size; i += ZGranuleSize) {\n-    HANDLE const handle = ZMapper::create_and_commit_paging_file_mapping(ZGranuleSize);\n-    if (handle == 0) {\n-      return i;\n+      put_handle(offset + i, handle);\n@@ -73,1 +85,1 @@\n-    put_handle(offset + i, handle);\n+    return size;\n@@ -76,2 +88,76 @@\n-  return size;\n-}\n+  size_t uncommit(size_t offset, size_t size) {\n+    for (size_t i = 0; i < size; i += ZGranuleSize) {\n+      HANDLE const handle = get_handle(offset + i);\n+      clear_handle(offset + i);\n+      ZMapper::close_paging_file_mapping(handle);\n+    }\n+\n+    return size;\n+  }\n+\n+  void map(uintptr_t addr, size_t size, size_t offset) const {\n+    assert(is_aligned(offset, ZGranuleSize), \"Misaligned\");\n+    assert(is_aligned(addr, ZGranuleSize), \"Misaligned\");\n+    assert(is_aligned(size, ZGranuleSize), \"Misaligned\");\n+\n+    for (size_t i = 0; i < size; i += ZGranuleSize) {\n+      HANDLE const handle = get_handle(offset + i);\n+      ZMapper::map_view_replace_placeholder(handle, 0 \/* offset *\/, addr + i, ZGranuleSize);\n+    }\n+  }\n+\n+  void unmap(uintptr_t addr, size_t size) const {\n+    assert(is_aligned(addr, ZGranuleSize), \"Misaligned\");\n+    assert(is_aligned(size, ZGranuleSize), \"Misaligned\");\n+\n+    for (size_t i = 0; i < size; i += ZGranuleSize) {\n+      ZMapper::unmap_view_preserve_placeholder(addr + i, ZGranuleSize);\n+    }\n+  }\n+};\n+\n+\/\/ Implements Large Pages (locked) support using shared AWE physical memory.\n+\/\/\n+\/\/ Shared AWE physical memory also works with small pages, but it has\n+\/\/ a few drawbacks that makes it a no-go to use it at this point:\n+\/\/\n+\/\/ 1) It seems to use 8 bytes of committed memory per *reserved* memory.\n+\/\/ Given our scheme to use a large address space range this turns out to\n+\/\/ use too much memory.\n+\/\/\n+\/\/ 2) It requires memory locking privilages, even for small pages. This\n+\/\/ has always been a requirement for large pages, and would be an extra\n+\/\/ restriction for usage with small pages.\n+\/\/\n+\/\/ Note: The large pages size is tied to our ZGranuleSize.\n+\n+extern HANDLE ZAWESection;\n+\n+class ZPhysicalMemoryBackingLargePages : public ZPhysicalMemoryBackingImpl {\n+private:\n+  ULONG_PTR* const _page_array;\n+\n+  static ULONG_PTR* alloc_page_array(size_t max_capacity) {\n+    const size_t npages = max_capacity \/ ZGranuleSize;\n+    const size_t array_size = npages * sizeof(ULONG_PTR);\n+\n+    return (ULONG_PTR*)os::malloc(array_size, mtGC);\n+  }\n+\n+public:\n+  ZPhysicalMemoryBackingLargePages(size_t max_capacity) :\n+      ZPhysicalMemoryBackingImpl(),\n+      _page_array(alloc_page_array(max_capacity)) {}\n+\n+  size_t commit(size_t offset, size_t size) {\n+    const size_t index = offset >> ZGranuleSizeShift;\n+    const size_t npages = size >> ZGranuleSizeShift;\n+\n+    size_t npages_res = npages;\n+    const bool res = AllocateUserPhysicalPages(ZAWESection, &npages_res, &_page_array[index]);\n+    if (!res) {\n+      fatal(\"Failed to allocate physical memory \" SIZE_FORMAT \"M @ \" PTR_FORMAT \" (%d)\",\n+            size \/ M, offset, GetLastError());\n+    } else {\n+      log_debug(gc)(\"Allocated physical memory: \" SIZE_FORMAT \"M @ \" PTR_FORMAT, size \/ M, offset);\n+    }\n@@ -79,5 +165,3 @@\n-size_t ZPhysicalMemoryBacking::uncommit_from_paging_file(size_t offset, size_t size) {\n-  for (size_t i = 0; i < size; i += ZGranuleSize) {\n-    HANDLE const handle = get_handle(offset + i);\n-    clear_handle(offset + i);\n-    ZMapper::close_paging_file_mapping(handle);\n+    \/\/ AllocateUserPhysicalPages might not be able to allocate the requested amount of memory.\n+    \/\/ The allocated number of pages are written in npages_res.\n+    return npages_res << ZGranuleSizeShift;\n@@ -86,1 +170,53 @@\n-  return size;\n+  size_t uncommit(size_t offset, size_t size) {\n+    const size_t index = offset >> ZGranuleSizeShift;\n+    const size_t npages = size >> ZGranuleSizeShift;\n+\n+    size_t npages_res = npages;\n+    const bool res = FreeUserPhysicalPages(ZAWESection, &npages_res, &_page_array[index]);\n+    if (!res) {\n+      fatal(\"Failed to uncommit physical memory \" SIZE_FORMAT \"M @ \" PTR_FORMAT \" (%d)\",\n+            size, offset, GetLastError());\n+    }\n+\n+    return npages_res << ZGranuleSizeShift;\n+  }\n+\n+  void map(uintptr_t addr, size_t size, size_t offset) const {\n+    const size_t npages = size >> ZGranuleSizeShift;\n+    const size_t index = offset >> ZGranuleSizeShift;\n+\n+    const bool res = MapUserPhysicalPages((char*)addr, npages, &_page_array[index]);\n+    if (!res) {\n+      fatal(\"Failed to map view \" PTR_FORMAT \" \" SIZE_FORMAT \"M @ \" PTR_FORMAT \" (%d)\",\n+            addr, size \/ M, offset, GetLastError());\n+    }\n+  }\n+\n+  void unmap(uintptr_t addr, size_t size) const {\n+    const size_t npages = size >> ZGranuleSizeShift;\n+\n+    const bool res = MapUserPhysicalPages((char*)addr, npages, NULL);\n+    if (!res) {\n+      fatal(\"Failed to unmap view \" PTR_FORMAT \" \" SIZE_FORMAT \"M (%d)\",\n+            addr, size \/ M, GetLastError());\n+    }\n+  }\n+};\n+\n+static ZPhysicalMemoryBackingImpl* select_impl(size_t max_capacity) {\n+  if (ZLargePages::is_enabled()) {\n+    return new ZPhysicalMemoryBackingLargePages(max_capacity);\n+  }\n+\n+  return new ZPhysicalMemoryBackingSmallPages(max_capacity);\n+}\n+\n+ZPhysicalMemoryBacking::ZPhysicalMemoryBacking(size_t max_capacity) :\n+    _impl(select_impl(max_capacity)) {}\n+\n+bool ZPhysicalMemoryBacking::is_initialized() const {\n+  return true;\n+}\n+\n+void ZPhysicalMemoryBacking::warn_commit_limits(size_t max_capacity) const {\n+  \/\/ Does nothing\n@@ -93,1 +229,1 @@\n-  return commit_from_paging_file(offset, length);\n+  return _impl->commit(offset, length);\n@@ -100,1 +236,1 @@\n-  return uncommit_from_paging_file(offset, length);\n+  return _impl->uncommit(offset, length);\n@@ -104,3 +240,3 @@\n-  assert(is_aligned(offset, ZGranuleSize), \"Misaligned\");\n-  assert(is_aligned(addr, ZGranuleSize), \"Misaligned\");\n-  assert(is_aligned(size, ZGranuleSize), \"Misaligned\");\n+  assert(is_aligned(offset, ZGranuleSize), \"Misaligned: \" PTR_FORMAT, offset);\n+  assert(is_aligned(addr, ZGranuleSize), \"Misaligned: \" PTR_FORMAT, addr);\n+  assert(is_aligned(size, ZGranuleSize), \"Misaligned: \" PTR_FORMAT, size);\n@@ -108,4 +244,1 @@\n-  for (size_t i = 0; i < size; i += ZGranuleSize) {\n-    HANDLE const handle = get_handle(offset + i);\n-    ZMapper::map_view_replace_placeholder(handle, 0 \/* offset *\/, addr + i, ZGranuleSize);\n-  }\n+  _impl->map(addr, size, offset);\n@@ -118,3 +251,1 @@\n-  for (size_t i = 0; i < size; i += ZGranuleSize) {\n-    ZMapper::unmap_view_preserve_placeholder(addr + i, ZGranuleSize);\n-  }\n+  _impl->unmap(addr, size);\n","filename":"src\/hotspot\/os\/windows\/gc\/z\/zPhysicalMemoryBacking_windows.cpp","additions":179,"deletions":48,"binary":false,"changes":227,"status":"modified"},{"patch":"@@ -27,1 +27,1 @@\n-#include \"gc\/z\/zGranuleMap.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -31,0 +31,2 @@\n+class ZPhysicalMemoryBackingImpl;\n+\n@@ -33,8 +35,1 @@\n-  ZGranuleMap<HANDLE> _handles;\n-\n-  HANDLE get_handle(uintptr_t offset) const;\n-  void put_handle(uintptr_t offset, HANDLE handle);\n-  void clear_handle(uintptr_t offset);\n-\n-  size_t commit_from_paging_file(size_t offset, size_t size);\n-  size_t uncommit_from_paging_file(size_t offset, size_t size);\n+  ZPhysicalMemoryBackingImpl* _impl;\n","filename":"src\/hotspot\/os\/windows\/gc\/z\/zPhysicalMemoryBacking_windows.hpp","additions":4,"deletions":9,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+ZSyscall::CreateFileMapping2Fn ZSyscall::CreateFileMapping2;\n@@ -36,2 +37,2 @@\n-template <typename Fn>\n-static void lookup_symbol(Fn*& fn, const char* library, const char* symbol) {\n+static void* lookup_kernelbase_library() {\n+  const char* const name = \"KernelBase\";\n@@ -39,1 +40,1 @@\n-  void* const handle = os::dll_load(library, ebuf, sizeof(ebuf));\n+  void* const handle = os::dll_load(name, ebuf, sizeof(ebuf));\n@@ -41,2 +42,1 @@\n-    log_error_p(gc)(\"Failed to load library: %s\", library);\n-    vm_exit_during_initialization(\"ZGC requires Windows version 1803 or later\");\n+    log_error_p(gc)(\"Failed to load library: %s\", name);\n@@ -44,0 +44,14 @@\n+  return handle;\n+}\n+\n+static void* lookup_kernelbase_symbol(const char* name) {\n+  static void* const handle = lookup_kernelbase_library();\n+  if (handle == NULL) {\n+    return NULL;\n+  }\n+  return os::dll_lookup(handle, name);\n+}\n+\n+static bool has_kernelbase_symbol(const char* name) {\n+  return lookup_kernelbase_symbol(name) != NULL;\n+}\n@@ -45,1 +59,8 @@\n-  fn = reinterpret_cast<Fn*>(os::dll_lookup(handle, symbol));\n+template <typename Fn>\n+static void install_kernelbase_symbol(Fn*& fn, const char* name) {\n+  fn = reinterpret_cast<Fn*>(lookup_kernelbase_symbol(name));\n+}\n+\n+template <typename Fn>\n+static void install_kernelbase_1803_symbol_or_exit(Fn*& fn, const char* name) {\n+  install_kernelbase_symbol(fn, name);\n@@ -47,1 +68,1 @@\n-    log_error_p(gc)(\"Failed to lookup symbol: %s\", symbol);\n+    log_error_p(gc)(\"Failed to lookup symbol: %s\", name);\n@@ -53,5 +74,9 @@\n-  lookup_symbol(CreateFileMappingW, \"KernelBase\", \"CreateFileMappingW\");\n-  lookup_symbol(VirtualAlloc2,      \"KernelBase\", \"VirtualAlloc2\");\n-  lookup_symbol(VirtualFreeEx,      \"KernelBase\", \"VirtualFreeEx\");\n-  lookup_symbol(MapViewOfFile3,     \"KernelBase\", \"MapViewOfFile3\");\n-  lookup_symbol(UnmapViewOfFile2,   \"KernelBase\", \"UnmapViewOfFile2\");\n+  \/\/ Required\n+  install_kernelbase_1803_symbol_or_exit(CreateFileMappingW, \"CreateFileMappingW\");\n+  install_kernelbase_1803_symbol_or_exit(VirtualAlloc2,      \"VirtualAlloc2\");\n+  install_kernelbase_1803_symbol_or_exit(VirtualFreeEx,      \"VirtualFreeEx\");\n+  install_kernelbase_1803_symbol_or_exit(MapViewOfFile3,     \"MapViewOfFile3\");\n+  install_kernelbase_1803_symbol_or_exit(UnmapViewOfFile2,   \"UnmapViewOfFile2\");\n+\n+  \/\/ Optional - for large pages support\n+  install_kernelbase_symbol(CreateFileMapping2, \"CreateFileMapping2\");\n@@ -61,6 +86,3 @@\n-  char ebuf[1024];\n-  void* const handle = os::dll_load(\"KernelBase\", ebuf, sizeof(ebuf));\n-  if (handle == NULL) {\n-    assert(false, \"Failed to load library: KernelBase\");\n-    return false;\n-  }\n+  \/\/ Available in Windows version 1803 and later\n+  return has_kernelbase_symbol(\"VirtualAlloc2\");\n+}\n@@ -68,1 +90,3 @@\n-  return os::dll_lookup(handle, \"VirtualAlloc2\") != NULL;\n+bool ZSyscall::is_large_pages_supported() {\n+  \/\/ Available in Windows version 1809 and later\n+  return has_kernelbase_symbol(\"CreateFileMapping2\");\n","filename":"src\/hotspot\/os\/windows\/gc\/z\/zSyscall_windows.cpp","additions":43,"deletions":19,"binary":false,"changes":62,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+  typedef HANDLE (*CreateFileMapping2Fn)(HANDLE, LPSECURITY_ATTRIBUTES, ULONG, ULONG, ULONG, ULONG64, PCWSTR, PMEM_EXTENDED_PARAMETER, ULONG);\n@@ -42,0 +43,1 @@\n+  static CreateFileMapping2Fn CreateFileMapping2;\n@@ -50,0 +52,1 @@\n+  static bool is_large_pages_supported();\n","filename":"src\/hotspot\/os\/windows\/gc\/z\/zSyscall_windows.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"gc\/z\/zLargePages.inline.hpp\"\n@@ -28,0 +29,1 @@\n+#include \"gc\/z\/zSyscall_windows.hpp\"\n@@ -32,5 +34,104 @@\n-static void split_placeholder(uintptr_t start, size_t size) {\n-  ZMapper::split_placeholder(ZAddress::marked0(start), size);\n-  ZMapper::split_placeholder(ZAddress::marked1(start), size);\n-  ZMapper::split_placeholder(ZAddress::remapped(start), size);\n-}\n+class ZVirtualMemoryManagerImpl : public CHeapObj<mtGC> {\n+public:\n+  virtual void initialize_before_reserve() {}\n+  virtual void initialize_after_reserve(ZMemoryManager* manager) {}\n+  virtual bool reserve(uintptr_t addr, size_t size) = 0;\n+  virtual void unreserve(uintptr_t addr, size_t size) = 0;\n+};\n+\n+\/\/ Implements small pages (paged) support using placeholder reservation.\n+class ZVirtualMemoryManagerSmallPages : public ZVirtualMemoryManagerImpl {\n+private:\n+  class PlaceholderCallbacks : public AllStatic {\n+  public:\n+    static void split_placeholder(uintptr_t start, size_t size) {\n+      ZMapper::split_placeholder(ZAddress::marked0(start), size);\n+      ZMapper::split_placeholder(ZAddress::marked1(start), size);\n+      ZMapper::split_placeholder(ZAddress::remapped(start), size);\n+    }\n+\n+    static void coalesce_placeholders(uintptr_t start, size_t size) {\n+      ZMapper::coalesce_placeholders(ZAddress::marked0(start), size);\n+      ZMapper::coalesce_placeholders(ZAddress::marked1(start), size);\n+      ZMapper::coalesce_placeholders(ZAddress::remapped(start), size);\n+    }\n+\n+    static void split_into_placeholder_granules(uintptr_t start, size_t size) {\n+      for (uintptr_t addr = start; addr < start + size; addr += ZGranuleSize) {\n+        split_placeholder(addr, ZGranuleSize);\n+      }\n+    }\n+\n+    static void coalesce_into_one_placeholder(uintptr_t start, size_t size) {\n+      assert(is_aligned(size, ZGranuleSize), \"Must be granule aligned\");\n+\n+      if (size > ZGranuleSize) {\n+        coalesce_placeholders(start, size);\n+      }\n+    }\n+\n+    static void create_callback(const ZMemory* area) {\n+      assert(is_aligned(area->size(), ZGranuleSize), \"Must be granule aligned\");\n+      coalesce_into_one_placeholder(area->start(), area->size());\n+    }\n+\n+    static void destroy_callback(const ZMemory* area) {\n+      assert(is_aligned(area->size(), ZGranuleSize), \"Must be granule aligned\");\n+      \/\/ Don't try split the last granule - VirtualFree will fail\n+      split_into_placeholder_granules(area->start(), area->size() - ZGranuleSize);\n+    }\n+\n+    static void shrink_from_front_callback(const ZMemory* area, size_t size) {\n+      assert(is_aligned(size, ZGranuleSize), \"Must be granule aligned\");\n+      split_into_placeholder_granules(area->start(), size);\n+    }\n+\n+    static void shrink_from_back_callback(const ZMemory* area, size_t size) {\n+      assert(is_aligned(size, ZGranuleSize), \"Must be granule aligned\");\n+      \/\/ Don't try split the last granule - VirtualFree will fail\n+      split_into_placeholder_granules(area->end() - size, size - ZGranuleSize);\n+    }\n+\n+    static void grow_from_front_callback(const ZMemory* area, size_t size) {\n+      assert(is_aligned(area->size(), ZGranuleSize), \"Must be granule aligned\");\n+      coalesce_into_one_placeholder(area->start() - size, area->size() + size);\n+    }\n+\n+    static void grow_from_back_callback(const ZMemory* area, size_t size) {\n+      assert(is_aligned(area->size(), ZGranuleSize), \"Must be granule aligned\");\n+      coalesce_into_one_placeholder(area->start(), area->size() + size);\n+    }\n+\n+    static void register_with(ZMemoryManager* manager) {\n+      \/\/ Each reserved virtual memory address area registered in _manager is\n+      \/\/ exactly covered by a single placeholder. Callbacks are installed so\n+      \/\/ that whenever a memory area changes, the corresponding placeholder\n+      \/\/ is adjusted.\n+      \/\/\n+      \/\/ The create and grow callbacks are called when virtual memory is\n+      \/\/ returned to the memory manager. The new memory area is then covered\n+      \/\/ by a new single placeholder.\n+      \/\/\n+      \/\/ The destroy and shrink callbacks are called when virtual memory is\n+      \/\/ allocated from the memory manager. The memory area is then is split\n+      \/\/ into granule-sized placeholders.\n+      \/\/\n+      \/\/ See comment in zMapper_windows.cpp explaining why placeholders are\n+      \/\/ split into ZGranuleSize sized placeholders.\n+\n+      ZMemoryManager::Callbacks callbacks;\n+\n+      callbacks._create = &create_callback;\n+      callbacks._destroy = &destroy_callback;\n+      callbacks._shrink_from_front = &shrink_from_front_callback;\n+      callbacks._shrink_from_back = &shrink_from_back_callback;\n+      callbacks._grow_from_front = &grow_from_front_callback;\n+      callbacks._grow_from_back = &grow_from_back_callback;\n+\n+      manager->register_callbacks(callbacks);\n+    }\n+  };\n+\n+  virtual void initialize_after_reserve(ZMemoryManager* manager) {\n+    PlaceholderCallbacks::register_with(manager);\n+  }\n@@ -38,5 +139,2 @@\n-static void coalesce_placeholders(uintptr_t start, size_t size) {\n-  ZMapper::coalesce_placeholders(ZAddress::marked0(start), size);\n-  ZMapper::coalesce_placeholders(ZAddress::marked1(start), size);\n-  ZMapper::coalesce_placeholders(ZAddress::remapped(start), size);\n-}\n+  virtual bool reserve(uintptr_t addr, size_t size) {\n+    const uintptr_t res = ZMapper::reserve(addr, size);\n@@ -44,3 +142,2 @@\n-static void split_into_placeholder_granules(uintptr_t start, size_t size) {\n-  for (uintptr_t addr = start; addr < start + size; addr += ZGranuleSize) {\n-    split_placeholder(addr, ZGranuleSize);\n+    assert(res == addr || res == NULL, \"Should not reserve other memory than requested\");\n+    return res == addr;\n@@ -48,4 +145,0 @@\n-}\n-\n-static void coalesce_into_one_placeholder(uintptr_t start, size_t size) {\n-  assert(is_aligned(size, ZGranuleSize), \"Must be granule aligned\");\n@@ -53,2 +146,2 @@\n-  if (size > ZGranuleSize) {\n-    coalesce_placeholders(start, size);\n+  virtual void unreserve(uintptr_t addr, size_t size) {\n+    ZMapper::unreserve(addr, size);\n@@ -56,1 +149,1 @@\n-}\n+};\n@@ -58,4 +151,1 @@\n-static void create_callback(const ZMemory* area) {\n-  assert(is_aligned(area->size(), ZGranuleSize), \"Must be granule aligned\");\n-  coalesce_into_one_placeholder(area->start(), area->size());\n-}\n+\/\/ Implements Large Pages (locked) support using shared AWE physical memory.\n@@ -63,5 +153,2 @@\n-static void destroy_callback(const ZMemory* area) {\n-  assert(is_aligned(area->size(), ZGranuleSize), \"Must be granule aligned\");\n-  \/\/ Don't try split the last granule - VirtualFree will fail\n-  split_into_placeholder_granules(area->start(), area->size() - ZGranuleSize);\n-}\n+\/\/ ZPhysicalMemory layer needs access to the section\n+HANDLE ZAWESection;\n@@ -69,4 +156,5 @@\n-static void shrink_from_front_callback(const ZMemory* area, size_t size) {\n-  assert(is_aligned(size, ZGranuleSize), \"Must be granule aligned\");\n-  split_into_placeholder_granules(area->start(), size);\n-}\n+class ZVirtualMemoryManagerLargePages : public ZVirtualMemoryManagerImpl {\n+private:\n+  virtual void initialize_before_reserve() {\n+    ZAWESection = ZMapper::create_shared_awe_section();\n+  }\n@@ -74,5 +162,2 @@\n-static void shrink_from_back_callback(const ZMemory* area, size_t size) {\n-  assert(is_aligned(size, ZGranuleSize), \"Must be granule aligned\");\n-  \/\/ Don't try split the last granule - VirtualFree will fail\n-  split_into_placeholder_granules(area->end() - size, size - ZGranuleSize);\n-}\n+  virtual bool reserve(uintptr_t addr, size_t size) {\n+    const uintptr_t res = ZMapper::reserve_for_shared_awe(ZAWESection, addr, size);\n@@ -80,4 +165,3 @@\n-static void grow_from_front_callback(const ZMemory* area, size_t size) {\n-  assert(is_aligned(area->size(), ZGranuleSize), \"Must be granule aligned\");\n-  coalesce_into_one_placeholder(area->start() - size, area->size() + size);\n-}\n+    assert(res == addr || res == NULL, \"Should not reserve other memory than requested\");\n+    return res == addr;\n+  }\n@@ -85,3 +169,14 @@\n-static void grow_from_back_callback(const ZMemory* area, size_t size) {\n-  assert(is_aligned(area->size(), ZGranuleSize), \"Must be granule aligned\");\n-  coalesce_into_one_placeholder(area->start(), area->size() + size);\n+  virtual void unreserve(uintptr_t addr, size_t size) {\n+    ZMapper::unreserve_for_shared_awe(addr, size);\n+  }\n+};\n+\n+static ZVirtualMemoryManagerImpl* _impl = NULL;\n+\n+void ZVirtualMemoryManager::pd_initialize_before_reserve() {\n+  if (ZLargePages::is_enabled()) {\n+    _impl = new ZVirtualMemoryManagerLargePages();\n+  } else {\n+    _impl = new ZVirtualMemoryManagerSmallPages();\n+  }\n+  _impl->initialize_before_reserve();\n@@ -90,27 +185,2 @@\n-void ZVirtualMemoryManager::pd_initialize() {\n-  \/\/ Each reserved virtual memory address area registered in _manager is\n-  \/\/ exactly covered by a single placeholder. Callbacks are installed so\n-  \/\/ that whenever a memory area changes, the corresponding placeholder\n-  \/\/ is adjusted.\n-  \/\/\n-  \/\/ The create and grow callbacks are called when virtual memory is\n-  \/\/ returned to the memory manager. The new memory area is then covered\n-  \/\/ by a new single placeholder.\n-  \/\/\n-  \/\/ The destroy and shrink callbacks are called when virtual memory is\n-  \/\/ allocated from the memory manager. The memory area is then is split\n-  \/\/ into granule-sized placeholders.\n-  \/\/\n-  \/\/ See comment in zMapper_windows.cpp explaining why placeholders are\n-  \/\/ split into ZGranuleSize sized placeholders.\n-\n-  ZMemoryManager::Callbacks callbacks;\n-\n-  callbacks._create = &create_callback;\n-  callbacks._destroy = &destroy_callback;\n-  callbacks._shrink_from_front = &shrink_from_front_callback;\n-  callbacks._shrink_from_back = &shrink_from_back_callback;\n-  callbacks._grow_from_front = &grow_from_front_callback;\n-  callbacks._grow_from_back = &grow_from_back_callback;\n-\n-  _manager.register_callbacks(callbacks);\n+void ZVirtualMemoryManager::pd_initialize_after_reserve() {\n+  _impl->initialize_after_reserve(&_manager);\n@@ -120,4 +190,1 @@\n-  uintptr_t res = ZMapper::reserve(addr, size);\n-\n-  assert(res == addr || res == NULL, \"Should not reserve other memory than requested\");\n-  return res == addr;\n+  return _impl->reserve(addr, size);\n@@ -127,1 +194,1 @@\n-  ZMapper::unreserve(addr, size);\n+  _impl->unreserve(addr, size);\n","filename":"src\/hotspot\/os\/windows\/gc\/z\/zVirtualMemory_windows.cpp","additions":144,"deletions":77,"binary":false,"changes":221,"status":"modified"},{"patch":"@@ -45,0 +45,3 @@\n+  \/\/ Initialize platform specific parts before reserving address space\n+  pd_initialize_before_reserve();\n+\n@@ -51,2 +54,2 @@\n-  \/\/ Initialize platform specific parts\n-  pd_initialize();\n+  \/\/ Initialize platform specific parts after reserving address space\n+  pd_initialize_after_reserve();\n","filename":"src\/hotspot\/share\/gc\/z\/zVirtualMemory.cpp","additions":5,"deletions":2,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -54,1 +54,2 @@\n-  void pd_initialize();\n+  void pd_initialize_before_reserve();\n+  void pd_initialize_after_reserve();\n","filename":"src\/hotspot\/share\/gc\/z\/zVirtualMemory.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"}]}