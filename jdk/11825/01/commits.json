[{"commit":{"message":"Changed the copyright year to 2023"},"files":[{"filename":"src\/hotspot\/cpu\/aarch64\/aarch64_vector.ad"},{"filename":"src\/hotspot\/cpu\/aarch64\/aarch64_vector_ad.m4"},{"filename":"src\/hotspot\/cpu\/aarch64\/assembler_aarch64.hpp"},{"filename":"test\/hotspot\/jtreg\/compiler\/vectorization\/TestFloatConversionsVector.java"}],"sha":"49c0b7b7fb2f7488c330458a892ef113202eb62d"},{"commit":{"message":"8299038: Add AArch64 backend support for auto-vectorized FP16 conversions\n\nThis patch adds aarch64 backend support for auto-vectorized FP16\nconversions namely, half precision to single precision and vice versa.\nBoth Neon and SVE versions are included. The performance of this patch\nwas tested on aarch64 machines with vector size of 128-bit, 256-bit and\n512-bit.\n\nFollowing are the performance improvements in throughput observed with\nthe vectorized version versus the scalar code (which is the current\nimplementation) for the JMH micro benchmark -\ntest\/micro\/org\/openjdk\/bench\/java\/math\/Fp16ConversionBenchmark.java\n\nBenchmark                               128-bit  256-bit  512-bit\nFp16ConversionBenchmark.float16ToFloat  6.02     8.72     24.71\nFp16ConversionBenchmark.floatToFloat16  2.00     3.29     10.84\n\nThe numbers shown are the ratios between throughput of vectorized\nversion and that of the scalar version."},"files":[{"filename":"src\/hotspot\/cpu\/aarch64\/aarch64_vector.ad"},{"filename":"src\/hotspot\/cpu\/aarch64\/aarch64_vector_ad.m4"},{"filename":"src\/hotspot\/cpu\/aarch64\/assembler_aarch64.hpp"},{"filename":"test\/hotspot\/gtest\/aarch64\/aarch64-asmtest.py"},{"filename":"test\/hotspot\/gtest\/aarch64\/asmtest.out.h"},{"filename":"test\/hotspot\/jtreg\/compiler\/vectorization\/TestFloatConversionsVector.java"}],"sha":"4ba1cf5dbddfd88189309e3df42bf8b5784d4d83"}]