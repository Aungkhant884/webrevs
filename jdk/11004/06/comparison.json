{"files":[{"patch":"@@ -3,1 +3,2 @@\n- * Copyright (c) 2022, Institute of Software, Chinese Academy of Sciences. All rights reserved.\n+ * Copyright (c) 2022, Institute of Software, Chinese Academy of Sciences.\n+ * All rights reserved.\n","filename":"src\/hotspot\/cpu\/riscv\/codeBuffer_riscv.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -3,1 +3,1 @@\n- * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * Copyright (c) 2020, 2023, Huawei Technologies Co., Ltd. All rights reserved.\n@@ -27,0 +27,7 @@\n+#include \"asm\/macroAssembler.hpp\"\n+#include \"code\/codeBlob.hpp\"\n+#include \"code\/codeCache.hpp\"\n+#include \"code\/vmreg.inline.hpp\"\n+#include \"compiler\/oopMap.hpp\"\n+#include \"logging\/logStream.hpp\"\n+#include \"memory\/resourceArea.hpp\"\n@@ -28,1 +35,60 @@\n-#include \"utilities\/debug.hpp\"\n+#include \"runtime\/globals.hpp\"\n+#include \"runtime\/stubCodeGenerator.hpp\"\n+\n+#define __ _masm->\n+\n+class DowncallStubGenerator : public StubCodeGenerator {\n+  BasicType* _signature;\n+  int _num_args;\n+  BasicType _ret_bt;\n+\n+  const ABIDescriptor& _abi;\n+  const GrowableArray<VMStorage>& _input_registers;\n+  const GrowableArray<VMStorage>& _output_registers;\n+\n+  bool _needs_return_buffer;\n+  int _captured_state_mask;\n+\n+  int _frame_complete;\n+  int _frame_size_slots;\n+  OopMapSet* _oop_maps;\n+public:\n+  DowncallStubGenerator(CodeBuffer* buffer,\n+                        BasicType* signature,\n+                        int num_args,\n+                        BasicType ret_bt,\n+                        const ABIDescriptor& abi,\n+                        const GrowableArray<VMStorage>& input_registers,\n+                        const GrowableArray<VMStorage>& output_registers,\n+                        bool needs_return_buffer,\n+                        int captured_state_mask)\n+   : StubCodeGenerator(buffer, PrintMethodHandleStubs),\n+     _signature(signature),\n+     _num_args(num_args),\n+     _ret_bt(ret_bt),\n+     _abi(abi),\n+     _input_registers(input_registers),\n+     _output_registers(output_registers),\n+     _needs_return_buffer(needs_return_buffer),\n+     _captured_state_mask(captured_state_mask),\n+     _frame_complete(0),\n+     _frame_size_slots(0),\n+     _oop_maps(NULL) {\n+  }\n+\n+  void generate();\n+\n+  int frame_complete() const {\n+    return _frame_complete;\n+  }\n+\n+  int framesize() const {\n+    return (_frame_size_slots >> (LogBytesPerWord - LogBytesPerInt));\n+  }\n+\n+  OopMapSet* oop_maps() const {\n+    return _oop_maps;\n+  }\n+};\n+\n+static const int native_invoker_code_size = 1024;\n@@ -38,2 +104,232 @@\n-  Unimplemented();\n-  return nullptr;\n+  int locs_size = 64;\n+  CodeBuffer code(\"nep_invoker_blob\", native_invoker_code_size, locs_size);\n+  DowncallStubGenerator g(&code, signature, num_args, ret_bt, abi,\n+                          input_registers, output_registers,\n+                          needs_return_buffer, captured_state_mask);\n+  g.generate();\n+  code.log_section_sizes(\"nep_invoker_blob\");\n+\n+  RuntimeStub* stub =\n+    RuntimeStub::new_runtime_stub(\"nep_invoker_blob\",\n+                                  &code,\n+                                  g.frame_complete(),\n+                                  g.framesize(),\n+                                  g.oop_maps(), false);\n+\n+#ifndef PRODUCT\n+  LogTarget(Trace, foreign, downcall) lt;\n+  if (lt.is_enabled()) {\n+    ResourceMark rm;\n+    LogStream ls(lt);\n+    stub->print_on(&ls);\n+  }\n+#endif\n+\n+  return stub;\n+}\n+\n+void DowncallStubGenerator::generate() {\n+  enum layout {\n+    fp_off,\n+    fp_off2,\n+    ra_off,\n+    ra_off2,\n+    framesize \/\/ inclusive of return address\n+    \/\/ The following are also computed dynamically:\n+    \/\/ spill area for return value\n+    \/\/ out arg area (e.g. for stack args)\n+  };\n+\n+  VMStorage shuffle_reg = as_VMStorage(x9);\n+  JavaCallingConvention in_conv;\n+  NativeCallingConvention out_conv(_input_registers);\n+  ArgumentShuffle arg_shuffle(_signature, _num_args, _signature, _num_args, &in_conv, &out_conv, shuffle_reg);\n+\n+#ifndef PRODUCT\n+  LogTarget(Trace, foreign, downcall) lt;\n+  if (lt.is_enabled()) {\n+    ResourceMark rm;\n+    LogStream ls(lt);\n+    arg_shuffle.print_on(&ls);\n+  }\n+#endif\n+\n+  int allocated_frame_size = 0;\n+  assert(_abi._shadow_space_bytes == 0, \"not expecting shadow space on RISCV64\");\n+  allocated_frame_size += arg_shuffle.out_arg_bytes();\n+\n+  bool should_save_return_value = !_needs_return_buffer;\n+  RegSpiller out_reg_spiller(_output_registers);\n+  int spill_offset = -1;\n+\n+  if (should_save_return_value) {\n+    spill_offset = 0;\n+    \/\/ spill area can be shared with shadow space and out args,\n+    \/\/ since they are only used before the call,\n+    \/\/ and spill area is only used after.\n+    allocated_frame_size = out_reg_spiller.spill_size_bytes() > allocated_frame_size\n+                           ? out_reg_spiller.spill_size_bytes()\n+                           : allocated_frame_size;\n+  }\n+\n+  StubLocations locs;\n+  locs.set(StubLocations::TARGET_ADDRESS, _abi._scratch1);\n+  if (_needs_return_buffer) {\n+    locs.set_frame_data(StubLocations::RETURN_BUFFER, allocated_frame_size);\n+    allocated_frame_size += BytesPerWord; \/\/ for address spill\n+  }\n+  if (_captured_state_mask != 0) {\n+    locs.set_frame_data(StubLocations::CAPTURED_STATE_BUFFER, allocated_frame_size);\n+    allocated_frame_size += BytesPerWord;\n+  }\n+\n+  allocated_frame_size = align_up(allocated_frame_size, 16);\n+  \/\/ _frame_size_slots is in 32-bit stack slots:\n+  _frame_size_slots += framesize + (allocated_frame_size >> LogBytesPerInt);\n+  assert(is_even(_frame_size_slots \/ 2), \"sp not 16-byte aligned\");\n+\n+  _oop_maps = new OopMapSet();\n+  address start = __ pc();\n+\n+  __ enter();\n+\n+  \/\/ ra and fp are already in place\n+  __ sub(sp, sp, allocated_frame_size); \/\/ prolog\n+\n+  _frame_complete = __ pc() - start; \/\/ frame build complete.\n+\n+  __ block_comment(\"{ thread java2native\");\n+  address the_pc = __ pc();\n+  __ set_last_Java_frame(sp, fp, the_pc, t0);\n+  OopMap* map = new OopMap(_frame_size_slots, 0);\n+  _oop_maps->add_gc_map(the_pc - start, map);\n+\n+  \/\/ State transition\n+  __ mv(t0, _thread_in_native);\n+  __ membar(MacroAssembler::LoadStore | MacroAssembler::StoreStore);\n+  __ sw(t0, Address(xthread, JavaThread::thread_state_offset()));\n+  __ block_comment(\"} thread java2native\");\n+\n+  __ block_comment(\"{ argument shuffle\");\n+  arg_shuffle.generate(_masm, shuffle_reg, 0, _abi._shadow_space_bytes, locs);\n+  __ block_comment(\"} argument shuffle\");\n+\n+  __ jalr(as_Register(locs.get(StubLocations::TARGET_ADDRESS)));\n+  \/\/ this call is assumed not to have killed xthread\n+\n+  if (_needs_return_buffer) {\n+    \/\/ when use return buffer, copy content of return registers to return buffer,\n+    \/\/ then operations created in BoxBindingCalculator will be operated.\n+    __ ld(t0, Address(sp, locs.data_offset(StubLocations::RETURN_BUFFER)));\n+    int offset = 0;\n+    for (int i = 0; i < _output_registers.length(); i++) {\n+      VMStorage reg = _output_registers.at(i);\n+      if (reg.type() == StorageType::INTEGER) {\n+        __ sd(as_Register(reg), Address(t0, offset));\n+        offset += 8;\n+      } else if (reg.type() == StorageType::FLOAT) {\n+        __ fsd(as_FloatRegister(reg), Address(t0, offset));\n+        offset += 8;\n+      } else {\n+        ShouldNotReachHere();\n+      }\n+    }\n+  }\n+\n+  \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+  if (_captured_state_mask != 0) {\n+    __ block_comment(\"{ save thread local\");\n+\n+    if (should_save_return_value) {\n+      out_reg_spiller.generate_spill(_masm, spill_offset);\n+    }\n+\n+    __ ld(c_rarg0, Address(sp, locs.data_offset(StubLocations::CAPTURED_STATE_BUFFER)));\n+    __ mv(c_rarg1, _captured_state_mask);\n+    __ rt_call(CAST_FROM_FN_PTR(address, DowncallLinker::capture_state));\n+\n+    if (should_save_return_value) {\n+      out_reg_spiller.generate_fill(_masm, spill_offset);\n+    }\n+\n+    __ block_comment(\"} save thread local\");\n+  }\n+\n+  \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+  __ block_comment(\"{ thread native2java\");\n+  __ mv(t0, _thread_in_native_trans);\n+  __ sw(t0, Address(xthread, JavaThread::thread_state_offset()));\n+\n+  \/\/ Force this write out before the read below\n+  __ membar(MacroAssembler::AnyAny);\n+\n+  Label L_after_safepoint_poll;\n+  Label L_safepoint_poll_slow_path;\n+  __ safepoint_poll(L_safepoint_poll_slow_path, true \/* at_return *\/, true \/* acquire *\/, false \/* in_nmethod *\/);\n+  __ lwu(t0, Address(xthread, JavaThread::suspend_flags_offset()));\n+  __ bnez(t0, L_safepoint_poll_slow_path);\n+\n+  __ bind(L_after_safepoint_poll);\n+\n+  __ mv(t0, _thread_in_Java);\n+  __ membar(MacroAssembler::LoadStore | MacroAssembler::StoreStore);\n+  __ sw(t0, Address(xthread, JavaThread::thread_state_offset()));\n+\n+  __ block_comment(\"reguard stack check\");\n+  Label L_reguard;\n+  Label L_after_reguard;\n+  __ lbu(t0, Address(xthread, JavaThread::stack_guard_state_offset()));\n+  __ mv(t1, StackOverflow::stack_guard_yellow_reserved_disabled);\n+  __ beq(t0, t1, L_reguard);\n+  __ bind(L_after_reguard);\n+\n+  __ reset_last_Java_frame(true);\n+  __ block_comment(\"} thread native2java\");\n+\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret();\n+\n+  \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+  __ block_comment(\"{ L_safepoint_poll_slow_path\");\n+  __ bind(L_safepoint_poll_slow_path);\n+\n+  if (should_save_return_value) {\n+    \/\/ Need to save the native result registers around any runtime calls.\n+    out_reg_spiller.generate_spill(_masm, spill_offset);\n+  }\n+\n+  __ mv(c_rarg0, xthread);\n+  assert(frame::arg_reg_save_area_bytes == 0, \"not expecting frame reg save area\");\n+  __ rt_call(CAST_FROM_FN_PTR(address, JavaThread::check_special_condition_for_native_trans));\n+\n+  if (should_save_return_value) {\n+    out_reg_spiller.generate_fill(_masm, spill_offset);\n+  }\n+  __ j(L_after_safepoint_poll);\n+  __ block_comment(\"} L_safepoint_poll_slow_path\");\n+\n+  \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+  __ block_comment(\"{ L_reguard\");\n+  __ bind(L_reguard);\n+\n+  if (should_save_return_value) {\n+    \/\/ Need to save the native result registers around any runtime calls.\n+    out_reg_spiller.generate_spill(_masm, spill_offset);\n+  }\n+\n+  __ rt_call(CAST_FROM_FN_PTR(address, SharedRuntime::reguard_yellow_pages));\n+\n+  if (should_save_return_value) {\n+    out_reg_spiller.generate_fill(_masm, spill_offset);\n+  }\n+\n+  __ j(L_after_reguard);\n+  __ block_comment(\"} L_reguard\");\n+\n+  \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+  __ flush();\n","filename":"src\/hotspot\/cpu\/riscv\/downcallLinker_riscv.cpp","additions":300,"deletions":4,"binary":false,"changes":304,"status":"modified"},{"patch":"@@ -3,1 +3,1 @@\n- * Copyright (c) 2020, 2021, Huawei Technologies Co., Ltd. All rights reserved.\n+ * Copyright (c) 2020, 2023, Huawei Technologies Co., Ltd. All rights reserved.\n@@ -27,1 +27,5 @@\n-#include \"code\/vmreg.hpp\"\n+#include \"code\/vmreg.inline.hpp\"\n+#include \"runtime\/jniHandles.hpp\"\n+#include \"runtime\/jniHandles.inline.hpp\"\n+#include \"oops\/typeArrayOop.inline.hpp\"\n+#include \"oops\/oopCast.inline.hpp\"\n@@ -29,1 +33,3 @@\n-#include \"utilities\/debug.hpp\"\n+#include \"prims\/foreignGlobals.inline.hpp\"\n+#include \"prims\/vmstorage.hpp\"\n+#include \"utilities\/formatBuffer.hpp\"\n@@ -31,1 +37,9 @@\n-class MacroAssembler;\n+bool ABIDescriptor::is_volatile_reg(Register reg) const {\n+  return _integer_argument_registers.contains(reg)\n+         || _integer_additional_volatile_registers.contains(reg);\n+}\n+\n+bool ABIDescriptor::is_volatile_reg(FloatRegister reg) const {\n+  return _float_argument_registers.contains(reg)\n+         || _float_additional_volatile_registers.contains(reg);\n+}\n@@ -34,2 +48,22 @@\n-  ShouldNotCallThis();\n-  return {};\n+  oop abi_oop = JNIHandles::resolve_non_null(jabi);\n+  ABIDescriptor abi;\n+\n+  objArrayOop inputStorage = jdk_internal_foreign_abi_ABIDescriptor::inputStorage(abi_oop);\n+  parse_register_array(inputStorage, StorageType::INTEGER, abi._integer_argument_registers, as_Register);\n+  parse_register_array(inputStorage, StorageType::FLOAT, abi._float_argument_registers, as_FloatRegister);\n+\n+  objArrayOop outputStorage = jdk_internal_foreign_abi_ABIDescriptor::outputStorage(abi_oop);\n+  parse_register_array(outputStorage, StorageType::INTEGER, abi._integer_return_registers, as_Register);\n+  parse_register_array(outputStorage, StorageType::FLOAT, abi._float_return_registers, as_FloatRegister);\n+\n+  objArrayOop volatileStorage = jdk_internal_foreign_abi_ABIDescriptor::volatileStorage(abi_oop);\n+  parse_register_array(volatileStorage, StorageType::INTEGER, abi._integer_additional_volatile_registers, as_Register);\n+  parse_register_array(volatileStorage, StorageType::FLOAT, abi._float_additional_volatile_registers, as_FloatRegister);\n+\n+  abi._stack_alignment_bytes = jdk_internal_foreign_abi_ABIDescriptor::stackAlignment(abi_oop);\n+  abi._shadow_space_bytes = jdk_internal_foreign_abi_ABIDescriptor::shadowSpace(abi_oop);\n+\n+  abi._scratch1 = parse_vmstorage(jdk_internal_foreign_abi_ABIDescriptor::scratch1(abi_oop));\n+  abi._scratch2 = parse_vmstorage(jdk_internal_foreign_abi_ABIDescriptor::scratch2(abi_oop));\n+\n+  return abi;\n@@ -39,2 +73,4 @@\n-  Unimplemented();\n-  return -1;\n+  if (reg.type() == StorageType::INTEGER || reg.type() == StorageType::FLOAT) {\n+    return 8;\n+  }\n+  return 0; \/\/ stack and BAD\n@@ -44,1 +80,7 @@\n-  Unimplemented();\n+  if (reg.type() == StorageType::INTEGER) {\n+    masm->sd(as_Register(reg), Address(sp, offset));\n+  } else if (reg.type() == StorageType::FLOAT) {\n+    masm->fsd(as_FloatRegister(reg), Address(sp, offset));\n+  } else {\n+    \/\/ stack and BAD\n+  }\n@@ -48,1 +90,70 @@\n-  Unimplemented();\n+  if (reg.type() == StorageType::INTEGER) {\n+    masm->ld(as_Register(reg), Address(sp, offset));\n+  } else if (reg.type() == StorageType::FLOAT) {\n+    masm->fld(as_FloatRegister(reg), Address(sp, offset));\n+  } else {\n+    \/\/ stack and BAD\n+  }\n+}\n+\n+static constexpr int FP_BIAS = 0; \/\/ sender_sp_offset is 0 on RISCV\n+\n+static void move_reg64(MacroAssembler* masm, int out_stk_bias,\n+                       Register from_reg, VMStorage to_reg) {\n+  int out_bias = 0;\n+  switch (to_reg.type()) {\n+    case StorageType::INTEGER:\n+      assert(to_reg.segment_mask() == REG64_MASK, \"only moves to 64-bit integer registers supported\");\n+      masm->mv(as_Register(to_reg), from_reg);\n+      break;\n+    case StorageType::STACK:\n+      out_bias = out_stk_bias;\n+    case StorageType::FRAME_DATA: {\n+      Address dest(sp, to_reg.offset() + out_bias);\n+      masm->sd(from_reg, dest);\n+    } break;\n+    default: ShouldNotReachHere();\n+  }\n+}\n+\n+static void move_stack(MacroAssembler* masm, Register tmp_reg, int in_stk_bias, int out_stk_bias,\n+                       VMStorage from_reg, VMStorage to_reg) {\n+  Address from_addr(fp, FP_BIAS + from_reg.offset() + in_stk_bias);\n+  int out_bias = 0;\n+  switch (to_reg.type()) {\n+    case StorageType::INTEGER:\n+      assert(to_reg.segment_mask() == REG64_MASK, \"only moves to 64-bit integer registers supported\");\n+      masm->ld(as_Register(to_reg), from_addr);\n+      break;\n+    case StorageType::FLOAT:\n+      assert(to_reg.segment_mask() == FP_MASK, \"only moves to floating-point registers supported\");\n+      masm->fld(as_FloatRegister(to_reg), from_addr);\n+      break;\n+    case StorageType::STACK:\n+      out_bias = out_stk_bias;\n+    case StorageType::FRAME_DATA: {\n+      masm->ld(tmp_reg, from_addr);\n+      Address dest(sp, to_reg.offset() + out_bias);\n+      masm->sd(tmp_reg, dest); break;\n+    } break;\n+    default: ShouldNotReachHere();\n+  }\n+}\n+\n+static void move_fp(MacroAssembler* masm, int out_stk_bias,\n+                    FloatRegister from_reg, VMStorage to_reg) {\n+  switch (to_reg.type()) {\n+    case StorageType::INTEGER:\n+      assert(to_reg.segment_mask() == REG64_MASK, \"only moves to 64-bit integer registers supported\");\n+      masm->fmv_x_d(as_Register(to_reg), from_reg);\n+      break;\n+    case StorageType::FLOAT:\n+      assert(to_reg.segment_mask() == FP_MASK, \"only moves to floating-point registers supported\");\n+      masm->fmv_d(as_FloatRegister(to_reg), from_reg); break;\n+      break;\n+    case StorageType::STACK: {\n+      Address dest(sp, to_reg.offset() + out_stk_bias);\n+      masm->fsd(from_reg, dest); break;\n+    } break;\n+    default: ShouldNotReachHere();\n+  }\n@@ -52,1 +163,29 @@\n-  Unimplemented();\n+  Register tmp_reg = as_Register(tmp);\n+  for (int i = 0; i < _moves.length(); i++) {\n+    Move move = _moves.at(i);\n+    VMStorage from_reg = move.from;\n+    VMStorage to_reg   = move.to;\n+\n+    \/\/ replace any placeholders\n+    if (from_reg.type() == StorageType::PLACEHOLDER) {\n+      from_reg = locs.get(from_reg);\n+    }\n+    if (to_reg.type() == StorageType::PLACEHOLDER) {\n+      to_reg = locs.get(to_reg);\n+    }\n+\n+    switch (from_reg.type()) {\n+      case StorageType::INTEGER:\n+        assert(from_reg.segment_mask() == REG64_MASK, \"only 64-bit integer register supported\");\n+        move_reg64(masm, out_stk_bias, as_Register(from_reg), to_reg);\n+        break;\n+      case StorageType::FLOAT:\n+        assert(from_reg.segment_mask() == FP_MASK, \"only floating-point register supported\");\n+        move_fp(masm, out_stk_bias, as_FloatRegister(from_reg), to_reg);\n+        break;\n+      case StorageType::STACK:\n+        move_stack(masm, tmp_reg, in_stk_bias, out_stk_bias, from_reg, to_reg);\n+        break;\n+      default: ShouldNotReachHere();\n+    }\n+  }\n","filename":"src\/hotspot\/cpu\/riscv\/foreignGlobals_riscv.cpp","additions":150,"deletions":11,"binary":false,"changes":161,"status":"modified"},{"patch":"@@ -3,1 +3,1 @@\n- * Copyright (c) 2020, 2021, Huawei Technologies Co., Ltd. All rights reserved.\n+ * Copyright (c) 2020, 2023, Huawei Technologies Co., Ltd. All rights reserved.\n@@ -29,1 +29,22 @@\n-class ABIDescriptor {};\n+#include \"asm\/macroAssembler.hpp\"\n+#include \"utilities\/growableArray.hpp\"\n+\n+struct ABIDescriptor {\n+  GrowableArray<Register> _integer_argument_registers;\n+  GrowableArray<Register> _integer_return_registers;\n+\n+  GrowableArray<FloatRegister> _float_argument_registers;\n+  GrowableArray<FloatRegister> _float_return_registers;\n+\n+  GrowableArray<Register> _integer_additional_volatile_registers;\n+  GrowableArray<FloatRegister> _float_additional_volatile_registers;\n+\n+  int32_t _stack_alignment_bytes;\n+  int32_t _shadow_space_bytes;\n+\n+  VMStorage _scratch1;\n+  VMStorage _scratch2;\n+\n+  bool is_volatile_reg(Register reg) const;\n+  bool is_volatile_reg(FloatRegister reg) const;\n+};\n","filename":"src\/hotspot\/cpu\/riscv\/foreignGlobals_riscv.hpp","additions":23,"deletions":2,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -4,1 +4,1 @@\n- * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * Copyright (c) 2020, 2023, Huawei Technologies Co., Ltd. All rights reserved.\n@@ -349,2 +349,4 @@\n-  ShouldNotCallThis();\n-  return nullptr;\n+  assert(frame.is_upcall_stub_frame(), \"wrong frame\");\n+  \/\/ need unextended_sp here, since normal sp is wrong for interpreter callees\n+  return reinterpret_cast<UpcallStub::FrameData*>(\n+          reinterpret_cast<address>(frame.unextended_sp()) + in_bytes(_frame_data_offset));\n@@ -354,2 +356,4 @@\n-  ShouldNotCallThis();\n-  return false;\n+  assert(is_upcall_stub_frame(), \"must be optimzed entry frame\");\n+  UpcallStub* blob = _cb->as_upcall_stub();\n+  JavaFrameAnchor* jfa = blob->jfa_for_frame(*this);\n+  return jfa->last_Java_sp() == NULL;\n@@ -359,2 +363,15 @@\n-  ShouldNotCallThis();\n-  return {};\n+  assert(map != NULL, \"map must be set\");\n+  UpcallStub* blob = _cb->as_upcall_stub();\n+  \/\/ Java frame called from C; skip all C frames and return top C\n+  \/\/ frame of that chunk as the sender\n+  JavaFrameAnchor* jfa = blob->jfa_for_frame(*this);\n+  assert(!upcall_stub_frame_is_first(), \"must have a frame anchor to go back to\");\n+  assert(jfa->last_Java_sp() > sp(), \"must be above this frame on stack\");\n+  \/\/ Since we are walking the stack now this nested anchor is obviously walkable\n+  \/\/ even if it wasn't when it was stacked.\n+  jfa->make_walkable();\n+  map->clear();\n+  assert(map->include_argument_oops(), \"should be set by clear\");\n+  frame fr(jfa->last_Java_sp(), jfa->last_Java_fp(), jfa->last_Java_pc());\n+\n+  return fr;\n","filename":"src\/hotspot\/cpu\/riscv\/frame_riscv.cpp","additions":24,"deletions":7,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -4,1 +4,1 @@\n- * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * Copyright (c) 2020, 2023, Huawei Technologies Co., Ltd. All rights reserved.\n@@ -387,1 +387,3 @@\n-\n+  if (is_upcall_stub_frame()) {\n+    return sender_for_upcall_stub_frame(map);\n+  }\n","filename":"src\/hotspot\/cpu\/riscv\/frame_riscv.inline.hpp","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -4,1 +4,1 @@\n- * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * Copyright (c) 2020, 2023, Huawei Technologies Co., Ltd. All rights reserved.\n@@ -911,1 +911,1 @@\n-          int32_t offset= 0;                                                                       \\\n+          int32_t offset = 0;                                                                      \\\n@@ -1316,1 +1316,0 @@\n-\n","filename":"src\/hotspot\/cpu\/riscv\/macroAssembler_riscv.hpp","additions":2,"deletions":3,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -4,1 +4,1 @@\n- * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * Copyright (c) 2020, 2023, Huawei Technologies Co., Ltd. All rights reserved.\n@@ -258,1 +258,10 @@\n-  __ stop(\"Should not reach here\");\n+  assert_different_registers(nep_reg, temp_target);\n+  assert(nep_reg != noreg, \"required register\");\n+\n+  \/\/ Load the invoker, as NEP -> .invoker\n+  __ verify_oop(nep_reg);\n+  __ access_load_at(T_ADDRESS, IN_HEAP, temp_target,\n+                    Address(nep_reg, NONZERO(jdk_internal_foreign_abi_NativeEntryPoint::downcall_stub_address_offset_in_bytes())),\n+                    noreg, noreg);\n+\n+  __ jr(temp_target);\n@@ -273,1 +282,1 @@\n-    assert(receiver_reg == (iid == vmIntrinsics::_linkToStatic ? noreg : j_rarg0), \"only valid assignment\");\n+    assert(receiver_reg == (iid == vmIntrinsics::_linkToStatic || iid == vmIntrinsics::_linkToNative ? noreg : j_rarg0), \"only valid assignment\");\n","filename":"src\/hotspot\/cpu\/riscv\/methodHandles_riscv.cpp","additions":12,"deletions":3,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -3,1 +3,1 @@\n- * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * Copyright (c) 2020, 2023, Huawei Technologies Co., Ltd. All rights reserved.\n@@ -73,3 +73,3 @@\n-    int raw_encoding() const { return this - first(); }\n-    int encoding() const     { assert(is_valid(), \"invalid register\"); return raw_encoding(); }\n-    bool is_valid() const    { return 0 <= raw_encoding() && raw_encoding() < number_of_registers; }\n+    constexpr int raw_encoding() const { return this - first(); }\n+    constexpr int     encoding() const { assert(is_valid(), \"invalid register\"); return raw_encoding(); }\n+    constexpr bool    is_valid() const { return 0 <= raw_encoding() && raw_encoding() < number_of_registers; }\n@@ -107,1 +107,1 @@\n-  const RegisterImpl* operator->() const { return RegisterImpl::first() + _encoding; }\n+  constexpr const RegisterImpl* operator->() const { return RegisterImpl::first() + _encoding; }\n@@ -190,3 +190,3 @@\n-    int raw_encoding() const { return this - first(); }\n-    int encoding() const     { assert(is_valid(), \"invalid register\"); return raw_encoding(); }\n-    bool is_valid() const    { return 0 <= raw_encoding() && raw_encoding() < number_of_registers; }\n+    constexpr int raw_encoding() const { return this - first(); }\n+    constexpr int     encoding() const { assert(is_valid(), \"invalid register\"); return raw_encoding(); }\n+    constexpr bool    is_valid() const { return 0 <= raw_encoding() && raw_encoding() < number_of_registers; }\n@@ -222,1 +222,1 @@\n-  const FloatRegisterImpl* operator->() const { return FloatRegisterImpl::first() + _encoding; }\n+  constexpr const FloatRegisterImpl* operator->() const { return FloatRegisterImpl::first() + _encoding; }\n@@ -300,3 +300,3 @@\n-    int raw_encoding() const { return this - first(); }\n-    int encoding() const     { assert(is_valid(), \"invalid register\"); return raw_encoding(); }\n-    bool is_valid() const    { return 0 <= raw_encoding() && raw_encoding() < number_of_registers; }\n+    constexpr int raw_encoding() const { return this - first(); }\n+    constexpr int     encoding() const { assert(is_valid(), \"invalid register\"); return raw_encoding(); }\n+    constexpr bool    is_valid() const { return 0 <= raw_encoding() && raw_encoding() < number_of_registers; }\n@@ -317,1 +317,1 @@\n-  const VectorRegisterImpl* operator->() const { return VectorRegisterImpl::first() + _encoding; }\n+  constexpr const VectorRegisterImpl* operator->() const { return VectorRegisterImpl::first() + _encoding; }\n","filename":"src\/hotspot\/cpu\/riscv\/register_riscv.hpp","additions":13,"deletions":13,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -3,1 +3,1 @@\n- * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * Copyright (c) 2020, 2023, Huawei Technologies Co., Ltd. All rights reserved.\n@@ -27,0 +27,3 @@\n+#include \"asm\/macroAssembler.hpp\"\n+#include \"logging\/logStream.hpp\"\n+#include \"memory\/resourceArea.hpp\"\n@@ -28,1 +31,85 @@\n-#include \"utilities\/debug.hpp\"\n+#include \"runtime\/sharedRuntime.hpp\"\n+#include \"runtime\/signature.hpp\"\n+#include \"runtime\/stubRoutines.hpp\"\n+#include \"utilities\/formatBuffer.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include \"vmreg_riscv.inline.hpp\"\n+\n+#define __ _masm->\n+\n+\/\/ for callee saved regs, according to the caller's ABI\n+static int compute_reg_save_area_size(const ABIDescriptor& abi) {\n+  int size = 0;\n+  for (int i = 0; i < Register::number_of_registers; i++) {\n+    Register reg = as_Register(i);\n+    if (reg == fp || reg == sp) continue; \/\/ saved\/restored by prologue\/epilogue\n+    if (!abi.is_volatile_reg(reg)) {\n+      size += 8; \/\/ bytes\n+    }\n+  }\n+\n+  for (int i = 0; i < FloatRegister::number_of_registers; i++) {\n+    FloatRegister reg = as_FloatRegister(i);\n+    if (!abi.is_volatile_reg(reg)) {\n+      size += 8; \/\/ bytes\n+    }\n+  }\n+\n+  return size;\n+}\n+\n+static void preserve_callee_saved_registers(MacroAssembler* _masm, const ABIDescriptor& abi, int reg_save_area_offset) {\n+  \/\/ 1. iterate all registers in the architecture\n+  \/\/     - check if they are volatile or not for the given abi\n+  \/\/     - if NOT, we need to save it here\n+\n+  int offset = reg_save_area_offset;\n+\n+  __ block_comment(\"{ preserve_callee_saved_regs \");\n+  for (int i = 0; i < Register::number_of_registers; i++) {\n+    Register reg = as_Register(i);\n+    if (reg == fp || reg == sp) continue; \/\/ saved\/restored by prologue\/epilogue\n+    if (!abi.is_volatile_reg(reg)) {\n+      __ sd(reg, Address(sp, offset));\n+      offset += 8;\n+    }\n+  }\n+\n+  for (int i = 0; i < FloatRegister::number_of_registers; i++) {\n+    FloatRegister reg = as_FloatRegister(i);\n+    if (!abi.is_volatile_reg(reg)) {\n+      __ fsd(reg, Address(sp, offset));\n+      offset += 8;\n+    }\n+  }\n+\n+  __ block_comment(\"} preserve_callee_saved_regs \");\n+}\n+\n+static void restore_callee_saved_registers(MacroAssembler* _masm, const ABIDescriptor& abi, int reg_save_area_offset) {\n+  \/\/ 1. iterate all registers in the architecture\n+  \/\/     - check if they are volatile or not for the given abi\n+  \/\/     - if NOT, we need to restore it here\n+\n+  int offset = reg_save_area_offset;\n+\n+  __ block_comment(\"{ restore_callee_saved_regs \");\n+  for (int i = 0; i < Register::number_of_registers; i++) {\n+    Register reg = as_Register(i);\n+    if (reg == fp || reg == sp) continue; \/\/ saved\/restored by prologue\/epilogue\n+    if (!abi.is_volatile_reg(reg)) {\n+      __ ld(reg, Address(sp, offset));\n+      offset += 8;\n+    }\n+  }\n+\n+  for (int i = 0; i < FloatRegister::number_of_registers; i++) {\n+    FloatRegister reg = as_FloatRegister(i);\n+    if (!abi.is_volatile_reg(reg)) {\n+      __ fld(reg, Address(sp, offset));\n+      offset += 8;\n+    }\n+  }\n+\n+  __ block_comment(\"} restore_callee_saved_regs \");\n+}\n@@ -36,2 +123,238 @@\n-  ShouldNotCallThis();\n-  return nullptr;\n+\n+  ResourceMark rm;\n+  const ABIDescriptor abi = ForeignGlobals::parse_abi_descriptor(jabi);\n+  const CallRegs call_regs = ForeignGlobals::parse_call_regs(jconv);\n+  CodeBuffer buffer(\"upcall_stub\", \/* code_size = *\/ 2048, \/* locs_size = *\/ 1024);\n+\n+  Register shuffle_reg = x9;\n+  JavaCallingConvention out_conv;\n+  NativeCallingConvention in_conv(call_regs._arg_regs);\n+  ArgumentShuffle arg_shuffle(in_sig_bt, total_in_args, out_sig_bt, total_out_args, &in_conv, &out_conv, as_VMStorage(shuffle_reg));\n+  int preserved_bytes = SharedRuntime::out_preserve_stack_slots() * VMRegImpl::stack_slot_size;\n+  int stack_bytes = preserved_bytes + arg_shuffle.out_arg_bytes();\n+  int out_arg_area = align_up(stack_bytes , StackAlignmentInBytes);\n+\n+#ifndef PRODUCT\n+  LogTarget(Trace, foreign, upcall) lt;\n+  if (lt.is_enabled()) {\n+    ResourceMark rm;\n+    LogStream ls(lt);\n+    arg_shuffle.print_on(&ls);\n+  }\n+#endif\n+\n+  \/\/ out_arg_area (for stack arguments) doubles as shadow space for native calls.\n+  \/\/ make sure it is big enough.\n+  if (out_arg_area < frame::arg_reg_save_area_bytes) {\n+    out_arg_area = frame::arg_reg_save_area_bytes;\n+  }\n+\n+  int reg_save_area_size = compute_reg_save_area_size(abi);\n+  RegSpiller arg_spiller(call_regs._arg_regs);\n+  RegSpiller result_spiller(call_regs._ret_regs);\n+\n+  int shuffle_area_offset   = 0;\n+  int res_save_area_offset  = shuffle_area_offset   + out_arg_area;\n+  int arg_save_area_offset  = res_save_area_offset  + result_spiller.spill_size_bytes();\n+  int reg_save_area_offset  = arg_save_area_offset  + arg_spiller.spill_size_bytes();\n+  int frame_data_offset     = reg_save_area_offset  + reg_save_area_size;\n+  int frame_bottom_offset   = frame_data_offset     + sizeof(UpcallStub::FrameData);\n+\n+  StubLocations locs;\n+  int ret_buf_offset = -1;\n+  if (needs_return_buffer) {\n+    ret_buf_offset = frame_bottom_offset;\n+    frame_bottom_offset += ret_buf_size;\n+    \/\/ use a free register for shuffling code to pick up return\n+    \/\/ buffer address from\n+    locs.set(StubLocations::RETURN_BUFFER, abi._scratch1);\n+  }\n+\n+  int frame_size = frame_bottom_offset;\n+  frame_size = align_up(frame_size, StackAlignmentInBytes);\n+\n+  \/\/ The space we have allocated will look like:\n+  \/\/\n+  \/\/\n+  \/\/ FP-> |                     |\n+  \/\/      |---------------------| = frame_bottom_offset = frame_size\n+  \/\/      | (optional)          |\n+  \/\/      | ret_buf             |\n+  \/\/      |---------------------| = ret_buf_offset\n+  \/\/      |                     |\n+  \/\/      | FrameData           |\n+  \/\/      |---------------------| = frame_data_offset\n+  \/\/      |                     |\n+  \/\/      | reg_save_area       |\n+  \/\/      |---------------------| = reg_save_area_offset\n+  \/\/      |                     |\n+  \/\/      | arg_save_area       |\n+  \/\/      |---------------------| = arg_save_area_offset\n+  \/\/      |                     |\n+  \/\/      | res_save_area       |\n+  \/\/      |---------------------| = res_save_area_offset\n+  \/\/      |                     |\n+  \/\/ SP-> | out_arg_area        |   needs to be at end for shadow space\n+  \/\/\n+  \/\/\n+\n+  \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+  MacroAssembler* _masm = new MacroAssembler(&buffer);\n+  address start = __ pc();\n+  __ enter(); \/\/ set up frame\n+  assert((abi._stack_alignment_bytes % 16) == 0, \"must be 16 byte aligned\");\n+  \/\/ allocate frame (frame_size is also aligned, so stack is still aligned)\n+  __ sub(sp, sp, frame_size);\n+\n+  \/\/ we have to always spill args since we need to do a call to get the thread\n+  \/\/ (and maybe attach it). so store those registers temporarily.\n+  arg_spiller.generate_spill(_masm, arg_save_area_offset);\n+  preserve_callee_saved_registers(_masm, abi, reg_save_area_offset);\n+\n+  __ block_comment(\"{ on_entry\");\n+  __ la(c_rarg0, Address(sp, frame_data_offset));\n+  __ rt_call(CAST_FROM_FN_PTR(address, UpcallLinker::on_entry));\n+  __ mv(xthread, x10);\n+  __ reinit_heapbase();\n+  __ block_comment(\"} on_entry\");\n+\n+  __ block_comment(\"{ argument shuffle\");\n+  arg_spiller.generate_fill(_masm, arg_save_area_offset);\n+\n+  if (needs_return_buffer) {\n+    assert(ret_buf_offset != -1, \"no return buffer allocated\");\n+\n+    \/\/ According to RISC-V ISA SPEC, when multiple floating-point precisions are supported,\n+    \/\/ then valid values of narrower n-bit types, n < FLEN , are represented in the lower n\n+    \/\/ bits of an FLEN-bit NaN value, in a process termed NaN-boxing. The upper bits of a\n+    \/\/ valid NaN-boxed value must be all 1s. Any operation that writes a narrower result to\n+    \/\/ an f register must write all 1s to the uppermost FLEN - n bits to yield a legal\n+    \/\/ NaN-boxed value. We could make use of this initializing all bits of return buffer with\n+    \/\/ 1s so that we could always transfer returned floating-point value from return buffer\n+    \/\/ into register with a single fld without knowing the current type of the value.\n+    __ mv(t1, -1L);\n+    int offset = 0;\n+    for (int i = 0; i < ret_buf_size \/ 8; i++) {\n+      __ sd(t1, Address(sp, ret_buf_offset + offset));\n+      offset += 8;\n+    }\n+    for (int i = 0; i < ret_buf_size % 8; i++) {\n+      __ sb(t1, Address(sp, ret_buf_offset + offset));\n+      offset += 1;\n+    }\n+\n+    __ la(as_Register(locs.get(StubLocations::RETURN_BUFFER)), Address(sp, ret_buf_offset));\n+  }\n+\n+  arg_shuffle.generate(_masm, as_VMStorage(shuffle_reg), abi._shadow_space_bytes, 0, locs);\n+  __ block_comment(\"} argument shuffle\");\n+\n+  __ block_comment(\"{ receiver \");\n+  __ movptr(shuffle_reg, (intptr_t) receiver);\n+  __ resolve_jobject(shuffle_reg, t0, t1);\n+  __ mv(j_rarg0, shuffle_reg);\n+  __ block_comment(\"} receiver \");\n+\n+  __ mov_metadata(xmethod, entry);\n+  __ sd(xmethod, Address(xthread, JavaThread::callee_target_offset())); \/\/ just in case callee is deoptimized\n+\n+  __ ld(t0, Address(xmethod, Method::from_compiled_offset()));\n+  __ jalr(t0);\n+\n+  \/\/ return value shuffle\n+  if (!needs_return_buffer) {\n+#ifdef ASSERT\n+    if (call_regs._ret_regs.length() == 1) { \/\/ 0 or 1\n+      VMStorage j_expected_result_reg;\n+      switch (ret_type) {\n+        case T_BOOLEAN:\n+        case T_BYTE:\n+        case T_SHORT:\n+        case T_CHAR:\n+        case T_INT:\n+        case T_LONG:\n+          j_expected_result_reg = as_VMStorage(x10);\n+          break;\n+        case T_FLOAT:\n+        case T_DOUBLE:\n+          j_expected_result_reg = as_VMStorage(f10);\n+          break;\n+        default:\n+          fatal(\"unexpected return type: %s\", type2name(ret_type));\n+      }\n+      \/\/ No need to move for now, since CallArranger can pick a return type\n+      \/\/ that goes in the same reg for both CCs. But, at least assert they are the same\n+      assert(call_regs._ret_regs.at(0) == j_expected_result_reg, \"unexpected result register\");\n+    }\n+#endif\n+  } else {\n+    assert(ret_buf_offset != -1, \"no return buffer allocated\");\n+    __ la(t0, Address(sp, ret_buf_offset));\n+    int offset = 0;\n+    for (int i = 0; i < call_regs._ret_regs.length(); i++) {\n+      VMStorage reg = call_regs._ret_regs.at(i);\n+      if (reg.type() == StorageType::INTEGER) {\n+        __ ld(as_Register(reg), Address(t0, offset));\n+      } else if (reg.type() == StorageType::FLOAT) {\n+        __ fld(as_FloatRegister(reg), Address(t0, offset));\n+      } else {\n+        ShouldNotReachHere();\n+      }\n+      offset += 8;\n+    }\n+  }\n+\n+  result_spiller.generate_spill(_masm, res_save_area_offset);\n+\n+  __ block_comment(\"{ on_exit\");\n+  __ la(c_rarg0, Address(sp, frame_data_offset));\n+  \/\/ stack already aligned\n+  __ rt_call(CAST_FROM_FN_PTR(address, UpcallLinker::on_exit));\n+  __ block_comment(\"} on_exit\");\n+\n+  restore_callee_saved_registers(_masm, abi, reg_save_area_offset);\n+\n+  result_spiller.generate_fill(_masm, res_save_area_offset);\n+\n+  __ leave();\n+  __ ret();\n+\n+  \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+  __ block_comment(\"{ exception handler\");\n+\n+  intptr_t exception_handler_offset = __ pc() - start;\n+\n+  \/\/ Native caller has no idea how to handle exceptions,\n+  \/\/ so we just crash here. Up to callee to catch exceptions.\n+  __ verify_oop(x10); \/\/ return a exception oop in a0\n+  __ rt_call(CAST_FROM_FN_PTR(address, UpcallLinker::handle_uncaught_exception));\n+  __ should_not_reach_here();\n+\n+  __ block_comment(\"} exception handler\");\n+  __ flush();\n+\n+#ifndef PRODUCT\n+  stringStream ss;\n+  ss.print(\"upcall_stub_%s\", entry->signature()->as_C_string());\n+  const char *name = _masm->code_string(ss.as_string());\n+#else \/\/ PRODUCT\n+  const char* name = \"upcall_stub\";\n+#endif \/\/ PRODUCT\n+\n+  UpcallStub* blob\n+    = UpcallStub::create(name,\n+                         &buffer,\n+                         exception_handler_offset,\n+                         receiver,\n+                         in_ByteSize(frame_data_offset));\n+#ifndef PRODUCT\n+  if (lt.is_enabled()) {\n+    ResourceMark rm;\n+    LogStream ls(lt);\n+    blob->print_on(&ls);\n+  }\n+#endif\n+\n+  return blob->code_begin();\n","filename":"src\/hotspot\/cpu\/riscv\/upcallLinker_riscv.cpp","additions":327,"deletions":4,"binary":false,"changes":331,"status":"modified"},{"patch":"@@ -3,1 +3,1 @@\n- * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * Copyright (c) 2020, 2023, Huawei Technologies Co., Ltd. All rights reserved.\n@@ -29,0 +29,1 @@\n+#include \"vmreg_riscv.inline.hpp\"\n","filename":"src\/hotspot\/cpu\/riscv\/vmreg_riscv.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+\/\/ keep in sync with jdk\/internal\/foreign\/abi\/riscv64\/RISCV64Architecture\n@@ -32,2 +33,4 @@\n-  STACK = 0,\n-  PLACEHOLDER = 1,\n+  INTEGER = 0,\n+  FLOAT = 1,\n+  STACK = 2,\n+  PLACEHOLDER = 3,\n@@ -41,1 +44,1 @@\n-   return false;\n+   return type == StorageType::INTEGER || type == StorageType::FLOAT;\n@@ -47,0 +50,21 @@\n+constexpr uint16_t REG64_MASK = 0b0000000000000001;\n+constexpr uint16_t FP_MASK    = 0b0000000000000001;\n+\n+inline Register as_Register(VMStorage vms) {\n+  assert(vms.type() == StorageType::INTEGER, \"not the right type\");\n+  return ::as_Register(vms.index());\n+}\n+\n+inline FloatRegister as_FloatRegister(VMStorage vms) {\n+  assert(vms.type() == StorageType::FLOAT, \"not the right type\");\n+  return ::as_FloatRegister(vms.index());\n+}\n+\n+constexpr inline VMStorage as_VMStorage(Register reg) {\n+  return VMStorage::reg_storage(StorageType::INTEGER, REG64_MASK, reg->encoding());\n+}\n+\n+constexpr inline VMStorage as_VMStorage(FloatRegister reg) {\n+  return VMStorage::reg_storage(StorageType::FLOAT, FP_MASK, reg->encoding());\n+}\n+\n@@ -48,0 +72,10 @@\n+  if (reg->is_Register()) {\n+    return as_VMStorage(reg->as_Register());\n+  } else if (reg->is_FloatRegister()) {\n+    return as_VMStorage(reg->as_FloatRegister());\n+  } else if (reg->is_stack()) {\n+    return VMStorage::stack_storage(reg);\n+  } else if (!reg->is_valid()) {\n+    return VMStorage::invalid();\n+  }\n+\n","filename":"src\/hotspot\/cpu\/riscv\/vmstorage_riscv.hpp","additions":37,"deletions":3,"binary":false,"changes":40,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+import jdk.internal.foreign.abi.riscv64.linux.LinuxRISCV64VaList;\n@@ -107,1 +108,1 @@\n-public sealed interface VaList permits WinVaList, SysVVaList, LinuxAArch64VaList, MacOsAArch64VaList, SharedUtils.EmptyVaList {\n+public sealed interface VaList permits WinVaList, SysVVaList, LinuxAArch64VaList, MacOsAArch64VaList, LinuxRISCV64VaList, SharedUtils.EmptyVaList {\n@@ -302,1 +303,1 @@\n-    sealed interface Builder permits WinVaList.Builder, SysVVaList.Builder, LinuxAArch64VaList.Builder, MacOsAArch64VaList.Builder {\n+    sealed interface Builder permits WinVaList.Builder, SysVVaList.Builder, LinuxAArch64VaList.Builder, MacOsAArch64VaList.Builder, LinuxRISCV64VaList.Builder {\n","filename":"src\/java.base\/share\/classes\/java\/lang\/foreign\/VaList.java","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -35,1 +35,2 @@\n-    MAC_OS_AARCH_64;\n+    MAC_OS_AARCH_64,\n+    LINUX_RISCV_64;\n@@ -61,0 +62,7 @@\n+        } else if (ARCH.equals(\"riscv64\")) {\n+            if (OS.startsWith(\"Linux\")) {\n+                ABI = LINUX_RISCV_64;\n+            } else {\n+                \/\/ unsupported\n+                ABI = null;\n+            }\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/foreign\/CABI.java","additions":9,"deletions":1,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -213,0 +213,56 @@\n+\n+    public static final class RISCV64 {\n+        private RISCV64() {\n+            \/\/just the one\n+        }\n+\n+        \/**\n+         * The {@code bool} native type.\n+         *\/\n+        public static final ValueLayout.OfBoolean C_BOOL = ValueLayout.JAVA_BOOLEAN;\n+\n+        \/**\n+         * The {@code char} native type.\n+         *\/\n+        public static final ValueLayout.OfByte C_CHAR = ValueLayout.JAVA_BYTE;\n+\n+        \/**\n+         * The {@code short} native type.\n+         *\/\n+        public static final ValueLayout.OfShort C_SHORT = ValueLayout.JAVA_SHORT.withBitAlignment(16);\n+\n+        \/**\n+         * The {@code int} native type.\n+         *\/\n+        public static final ValueLayout.OfInt C_INT = ValueLayout.JAVA_INT.withBitAlignment(32);\n+\n+        \/**\n+         * The {@code long} native type.\n+         *\/\n+        public static final ValueLayout.OfLong C_LONG = ValueLayout.JAVA_LONG.withBitAlignment(64);\n+\n+        \/**\n+         * The {@code long long} native type.\n+         *\/\n+        public static final ValueLayout.OfLong C_LONG_LONG = ValueLayout.JAVA_LONG.withBitAlignment(64);\n+\n+        \/**\n+         * The {@code float} native type.\n+         *\/\n+        public static final ValueLayout.OfFloat C_FLOAT = ValueLayout.JAVA_FLOAT.withBitAlignment(32);\n+\n+        \/**\n+         * The {@code double} native type.\n+         *\/\n+        public static final ValueLayout.OfDouble C_DOUBLE = ValueLayout.JAVA_DOUBLE.withBitAlignment(64);\n+\n+        \/**\n+         * The {@code T*} native type.\n+         *\/\n+        public static final ValueLayout.OfAddress C_POINTER = ValueLayout.ADDRESS.withBitAlignment(64).asUnbounded();\n+\n+        \/**\n+         * The {@code va_list} native type, as it is passed to a function.\n+         *\/\n+        public static final ValueLayout.OfAddress C_VA_LIST = RISCV64.C_POINTER;\n+    }\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/foreign\/PlatformLayouts.java","additions":56,"deletions":0,"binary":false,"changes":56,"status":"modified"},{"patch":"@@ -61,1 +61,1 @@\n-                case SYS_V, LINUX_AARCH_64, MAC_OS_AARCH_64 -> libLookup(libs -> libs.load(jdkLibraryPath(\"syslookup\")));\n+                case SYS_V, LINUX_AARCH_64, MAC_OS_AARCH_64, LINUX_RISCV_64 -> libLookup(libs -> libs.load(jdkLibraryPath(\"syslookup\")));\n@@ -122,1 +122,1 @@\n-            case SYS_V, LINUX_AARCH_64, MAC_OS_AARCH_64 -> \"lib\";\n+            case SYS_V, LINUX_AARCH_64, MAC_OS_AARCH_64, LINUX_RISCV_64 -> \"lib\";\n@@ -196,2 +196,1 @@\n-        gmtime\n-        ;\n+        gmtime;\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/foreign\/SystemLookup.java","additions":3,"deletions":4,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+import jdk.internal.foreign.abi.riscv64.linux.LinuxRISCV64Linker;\n@@ -46,1 +47,1 @@\n-                                                                      SysVx64Linker, Windowsx64Linker {\n+                                                                      SysVx64Linker, Windowsx64Linker, LinuxRISCV64Linker {\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/foreign\/abi\/AbstractLinker.java","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+import jdk.internal.foreign.abi.riscv64.linux.LinuxRISCV64Linker;\n@@ -186,0 +187,1 @@\n+            case LINUX_RISCV_64 -> LinuxRISCV64Linker.getInstance();\n@@ -297,0 +299,1 @@\n+            case LINUX_RISCV_64 -> LinuxRISCV64Linker.newVaList(actions, scope);\n@@ -306,0 +309,1 @@\n+            case LINUX_RISCV_64 -> LinuxRISCV64Linker.newVaListOfAddress(address, scope);\n@@ -315,0 +319,1 @@\n+            case LINUX_RISCV_64 -> LinuxRISCV64Linker.emptyVaList();\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/foreign\/abi\/SharedUtils.java","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -0,0 +1,177 @@\n+\/*\n+ * Copyright (c) 2020, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2023, Institute of Software, Chinese Academy of Sciences.\n+ * All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+package jdk.internal.foreign.abi.riscv64;\n+\n+import jdk.internal.foreign.abi.ABIDescriptor;\n+import jdk.internal.foreign.abi.Architecture;\n+import jdk.internal.foreign.abi.StubLocations;\n+import jdk.internal.foreign.abi.VMStorage;\n+import jdk.internal.foreign.abi.riscv64.linux.TypeClass;\n+\n+public class RISCV64Architecture implements Architecture {\n+    public static final Architecture INSTANCE = new RISCV64Architecture();\n+\n+    private static final short REG64_MASK = 0b0000_0000_0000_0001;\n+    private static final short FP_MASK = 0b0000_0000_0000_0001;\n+\n+    private static final int INTEGER_REG_SIZE = 8; \/\/ bytes\n+    private static final int FLOAT_REG_SIZE = 8;\n+\n+    @Override\n+    public boolean isStackType(int cls) {\n+        return cls == StorageType.STACK;\n+    }\n+\n+    @Override\n+    public int typeSize(int cls) {\n+        switch (cls) {\n+            case StorageType.INTEGER: return INTEGER_REG_SIZE;\n+            case StorageType.FLOAT: return FLOAT_REG_SIZE;\n+            \/\/ STACK is deliberately omitted\n+        }\n+\n+        throw new IllegalArgumentException(\"Invalid Storage Class: \" + cls);\n+    }\n+\n+    public interface StorageType {\n+        byte INTEGER = 0;\n+        byte FLOAT = 1;\n+        byte STACK = 2;\n+        byte PLACEHOLDER = 3;\n+    }\n+\n+    public static class Regs { \/\/ break circular dependency\n+        public static final VMStorage x0 = integerRegister(0, \"zr\");\n+        public static final VMStorage x1 = integerRegister(1, \"ra\");\n+        public static final VMStorage x2 = integerRegister(2, \"sp\");\n+        public static final VMStorage x3 = integerRegister(3, \"gp\");\n+        public static final VMStorage x4 = integerRegister(4, \"tp\");\n+        public static final VMStorage x5 = integerRegister(5, \"t0\");\n+        public static final VMStorage x6 = integerRegister(6, \"t1\");\n+        public static final VMStorage x7 = integerRegister(7, \"t2\");\n+        public static final VMStorage x8 = integerRegister(8, \"s0\/fp\");\n+        public static final VMStorage x9 = integerRegister(9, \"s1\");\n+        public static final VMStorage x10 = integerRegister(10, \"a0\");\n+        public static final VMStorage x11 = integerRegister(11, \"a1\");\n+        public static final VMStorage x12 = integerRegister(12, \"a2\");\n+        public static final VMStorage x13 = integerRegister(13, \"a3\");\n+        public static final VMStorage x14 = integerRegister(14, \"a4\");\n+        public static final VMStorage x15 = integerRegister(15, \"a5\");\n+        public static final VMStorage x16 = integerRegister(16, \"a6\");\n+        public static final VMStorage x17 = integerRegister(17, \"a7\");\n+        public static final VMStorage x18 = integerRegister(18, \"s2\");\n+        public static final VMStorage x19 = integerRegister(19, \"s3\");\n+        public static final VMStorage x20 = integerRegister(20, \"s4\");\n+        public static final VMStorage x21 = integerRegister(21, \"s5\");\n+        public static final VMStorage x22 = integerRegister(22, \"s6\");\n+        public static final VMStorage x23 = integerRegister(23, \"s7\");\n+        public static final VMStorage x24 = integerRegister(24, \"s8\");\n+        public static final VMStorage x25 = integerRegister(25, \"s9\");\n+        public static final VMStorage x26 = integerRegister(26, \"s10\");\n+        public static final VMStorage x27 = integerRegister(27, \"s11\");\n+        public static final VMStorage x28 = integerRegister(28, \"t3\");\n+        public static final VMStorage x29 = integerRegister(29, \"t4\");\n+        public static final VMStorage x30 = integerRegister(30, \"t5\");\n+        public static final VMStorage x31 = integerRegister(31, \"t6\");\n+\n+        public static final VMStorage f0 = floatRegister(0, \"ft0\");\n+        public static final VMStorage f1 = floatRegister(1, \"ft1\");\n+        public static final VMStorage f2 = floatRegister(2, \"ft2\");\n+        public static final VMStorage f3 = floatRegister(3, \"ft3\");\n+        public static final VMStorage f4 = floatRegister(4, \"ft4\");\n+        public static final VMStorage f5 = floatRegister(5, \"ft5\");\n+        public static final VMStorage f6 = floatRegister(6, \"ft6\");\n+        public static final VMStorage f7 = floatRegister(7, \"ft7\");\n+        public static final VMStorage f8 = floatRegister(8, \"fs0\");\n+        public static final VMStorage f9 = floatRegister(9, \"fs1\");\n+        public static final VMStorage f10 = floatRegister(10, \"fa0\");\n+        public static final VMStorage f11 = floatRegister(11, \"fa1\");\n+        public static final VMStorage f12 = floatRegister(12, \"fa2\");\n+        public static final VMStorage f13 = floatRegister(13, \"fa3\");\n+        public static final VMStorage f14 = floatRegister(14, \"fa4\");\n+        public static final VMStorage f15 = floatRegister(15, \"fa5\");\n+        public static final VMStorage f16 = floatRegister(16, \"fa6\");\n+        public static final VMStorage f17 = floatRegister(17, \"fa7\");\n+        public static final VMStorage f18 = floatRegister(18, \"fs2\");\n+        public static final VMStorage f19 = floatRegister(19, \"fs3\");\n+        public static final VMStorage f20 = floatRegister(20, \"fs4\");\n+        public static final VMStorage f21 = floatRegister(21, \"fs5\");\n+        public static final VMStorage f22 = floatRegister(22, \"fs6\");\n+        public static final VMStorage f23 = floatRegister(23, \"fs7\");\n+        public static final VMStorage f24 = floatRegister(24, \"fs8\");\n+        public static final VMStorage f25 = floatRegister(25, \"fs9\");\n+        public static final VMStorage f26 = floatRegister(26, \"fs10\");\n+        public static final VMStorage f27 = floatRegister(27, \"fs11\");\n+        public static final VMStorage f28 = floatRegister(28, \"ft8\");\n+        public static final VMStorage f29 = floatRegister(29, \"ft9\");\n+        public static final VMStorage f30 = floatRegister(30, \"ft10\");\n+        public static final VMStorage f31 = floatRegister(31, \"ft11\");\n+    }\n+\n+    private static VMStorage integerRegister(int index, String debugName) {\n+        return new VMStorage(StorageType.INTEGER, REG64_MASK, index, debugName);\n+    }\n+\n+    private static VMStorage floatRegister(int index, String debugName) {\n+        return new VMStorage(StorageType.FLOAT, FP_MASK, index, debugName);\n+    }\n+\n+    public static VMStorage stackStorage(short size, int byteOffset) {\n+        return new VMStorage(StorageType.STACK, size, byteOffset);\n+    }\n+\n+    public static ABIDescriptor abiFor(VMStorage[] inputIntRegs,\n+                                       VMStorage[] inputFloatRegs,\n+                                       VMStorage[] outputIntRegs,\n+                                       VMStorage[] outputFloatRegs,\n+                                       VMStorage[] volatileIntRegs,\n+                                       VMStorage[] volatileFloatRegs,\n+                                       int stackAlignment,\n+                                       int shadowSpace,\n+                                       VMStorage scratch1, VMStorage scratch2) {\n+        return new ABIDescriptor(\n+            INSTANCE,\n+            new VMStorage[][]{\n+                inputIntRegs,\n+                inputFloatRegs,\n+            },\n+            new VMStorage[][]{\n+                outputIntRegs,\n+                outputFloatRegs,\n+            },\n+            new VMStorage[][]{\n+                volatileIntRegs,\n+                volatileFloatRegs,\n+            },\n+            stackAlignment,\n+            shadowSpace,\n+            scratch1, scratch2,\n+            StubLocations.TARGET_ADDRESS.storage(StorageType.PLACEHOLDER),\n+            StubLocations.RETURN_BUFFER.storage(StorageType.PLACEHOLDER),\n+            StubLocations.CAPTURED_STATE_BUFFER.storage(StorageType.PLACEHOLDER));\n+    }\n+}\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/foreign\/abi\/riscv64\/RISCV64Architecture.java","additions":177,"deletions":0,"binary":false,"changes":177,"status":"added"},{"patch":"@@ -0,0 +1,473 @@\n+\/*\n+ * Copyright (c) 2020, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2023, Institute of Software, Chinese Academy of Sciences.\n+ * All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+package jdk.internal.foreign.abi.riscv64.linux;\n+\n+import java.lang.foreign.FunctionDescriptor;\n+import java.lang.foreign.GroupLayout;\n+import java.lang.foreign.MemoryLayout;\n+import java.lang.foreign.MemorySegment;\n+import jdk.internal.foreign.abi.ABIDescriptor;\n+import jdk.internal.foreign.abi.Binding;\n+import jdk.internal.foreign.abi.CallingSequence;\n+import jdk.internal.foreign.abi.CallingSequenceBuilder;\n+import jdk.internal.foreign.abi.DowncallLinker;\n+import jdk.internal.foreign.abi.LinkerOptions;\n+import jdk.internal.foreign.abi.UpcallLinker;\n+import jdk.internal.foreign.abi.SharedUtils;\n+import jdk.internal.foreign.abi.VMStorage;\n+import jdk.internal.foreign.Utils;\n+\n+import java.lang.foreign.SegmentScope;\n+import java.lang.invoke.MethodHandle;\n+import java.lang.invoke.MethodType;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+\n+import static jdk.internal.foreign.abi.riscv64.linux.TypeClass.*;\n+import static jdk.internal.foreign.abi.riscv64.RISCV64Architecture.*;\n+import static jdk.internal.foreign.abi.riscv64.RISCV64Architecture.Regs.*;\n+import static jdk.internal.foreign.PlatformLayouts.*;\n+\n+\/**\n+ * For the RISCV64 C ABI specifically, this class uses CallingSequenceBuilder\n+ * to translate a C FunctionDescriptor into a CallingSequence, which can then be turned into a MethodHandle.\n+ *\n+ * This includes taking care of synthetic arguments like pointers to return buffers for 'in-memory' returns.\n+ *\/\n+public class LinuxRISCV64CallArranger {\n+    private static final int STACK_SLOT_SIZE = 8;\n+    public static final int MAX_REGISTER_ARGUMENTS = 8;\n+    private static final ABIDescriptor CLinux = abiFor(\n+            new VMStorage[]{x10, x11, x12, x13, x14, x15, x16, x17},\n+            new VMStorage[]{f10, f11, f12, f13, f14, f15, f16, f17},\n+            new VMStorage[]{x10, x11},\n+            new VMStorage[]{f10, f11},\n+            new VMStorage[]{x5, x6, x7, x28, x29, x30, x31},\n+            new VMStorage[]{f0, f1, f2, f3, f4, f5, f6, f7, f28, f29, f30, f31},\n+            16, \/\/ stackAlignment\n+            0,  \/\/ no shadow space\n+            x28, x29 \/\/ scratch 1 & 2\n+    );\n+\n+    public record Bindings(CallingSequence callingSequence,\n+                           boolean isInMemoryReturn) {\n+    }\n+\n+    public static Bindings getBindings(MethodType mt, FunctionDescriptor cDesc, boolean forUpcall) {\n+        return getBindings(mt, cDesc, forUpcall, LinkerOptions.empty());\n+    }\n+\n+    public static Bindings getBindings(MethodType mt, FunctionDescriptor cDesc, boolean forUpcall, LinkerOptions options) {\n+        CallingSequenceBuilder csb = new CallingSequenceBuilder(CLinux, forUpcall, options);\n+        BindingCalculator argCalc = forUpcall ? new BoxBindingCalculator(true) : new UnboxBindingCalculator(true);\n+        BindingCalculator retCalc = forUpcall ? new UnboxBindingCalculator(false) : new BoxBindingCalculator(false);\n+\n+        boolean returnInMemory = isInMemoryReturn(cDesc.returnLayout());\n+        if (returnInMemory) {\n+            Class<?> carrier = MemorySegment.class;\n+            MemoryLayout layout = RISCV64.C_POINTER;\n+            csb.addArgumentBindings(carrier, layout, argCalc.getBindings(carrier, layout, false));\n+        } else if (cDesc.returnLayout().isPresent()) {\n+            Class<?> carrier = mt.returnType();\n+            MemoryLayout layout = cDesc.returnLayout().get();\n+            csb.setReturnBindings(carrier, layout, retCalc.getBindings(carrier, layout, false));\n+        }\n+\n+        for (int i = 0; i < mt.parameterCount(); i++) {\n+            Class<?> carrier = mt.parameterType(i);\n+            MemoryLayout layout = cDesc.argumentLayouts().get(i);\n+            boolean isVar = options.isVarargsIndex(i);\n+            csb.addArgumentBindings(carrier, layout, argCalc.getBindings(carrier, layout, isVar));\n+        }\n+\n+        return new Bindings(csb.build(), returnInMemory);\n+    }\n+\n+    public static MethodHandle arrangeDowncall(MethodType mt, FunctionDescriptor cDesc, LinkerOptions options) {\n+        Bindings bindings = getBindings(mt, cDesc, false, options);\n+\n+        MethodHandle handle = new DowncallLinker(CLinux, bindings.callingSequence).getBoundMethodHandle();\n+\n+        if (bindings.isInMemoryReturn) {\n+            handle = SharedUtils.adaptDowncallForIMR(handle, cDesc, bindings.callingSequence);\n+        }\n+\n+        return handle;\n+    }\n+\n+    public static MemorySegment arrangeUpcall(MethodHandle target, MethodType mt, FunctionDescriptor cDesc, SegmentScope scope) {\n+\n+        Bindings bindings = getBindings(mt, cDesc, true);\n+\n+        if (bindings.isInMemoryReturn) {\n+            target = SharedUtils.adaptUpcallForIMR(target, true \/* drop return, since we don't have bindings for it *\/);\n+        }\n+\n+        return UpcallLinker.make(CLinux, target, bindings.callingSequence, scope);\n+    }\n+\n+    private static boolean isInMemoryReturn(Optional<MemoryLayout> returnLayout) {\n+        return returnLayout\n+                .filter(GroupLayout.class::isInstance)\n+                .filter(g -> TypeClass.classifyLayout(g) == TypeClass.STRUCT_REFERENCE)\n+                .isPresent();\n+    }\n+\n+    static class StorageCalculator {\n+        private final boolean forArguments;\n+        \/\/ next available register index. 0=integerRegIdx, 1=floatRegIdx\n+        private final int IntegerRegIdx = 0;\n+        private final int FloatRegIdx = 1;\n+        private final int[] nRegs = {0, 0};\n+\n+        private long stackOffset = 0;\n+\n+        public StorageCalculator(boolean forArguments) {\n+            this.forArguments = forArguments;\n+        }\n+\n+        \/\/ Aggregates or scalars passed on the stack are aligned to the greater of\n+        \/\/ the type alignment and XLEN bits, but never more than the stack alignment.\n+        void alignStack(long alignment) {\n+            alignment = Utils.alignUp(Math.min(Math.max(alignment, STACK_SLOT_SIZE), 16), STACK_SLOT_SIZE);\n+            stackOffset = Utils.alignUp(stackOffset, alignment);\n+        }\n+\n+        VMStorage stackAlloc() {\n+            assert forArguments : \"no stack returns\";\n+            VMStorage storage = stackStorage((short) STACK_SLOT_SIZE, (int) stackOffset);\n+            stackOffset += STACK_SLOT_SIZE;\n+            return storage;\n+        }\n+\n+        Optional<VMStorage> regAlloc(int storageClass) {\n+            if (nRegs[storageClass] < MAX_REGISTER_ARGUMENTS) {\n+                VMStorage[] source = (forArguments ? CLinux.inputStorage : CLinux.outputStorage)[storageClass];\n+                Optional<VMStorage> result = Optional.of(source[nRegs[storageClass]]);\n+                nRegs[storageClass] += 1;\n+                return result;\n+            }\n+            return Optional.empty();\n+        }\n+\n+        VMStorage getStorage(int storageClass) {\n+            Optional<VMStorage> storage = regAlloc(storageClass);\n+            if (storage.isPresent()) {\n+                return storage.get();\n+            }\n+            \/\/ If storageClass is StorageType.FLOAT, and no floating-point register is available,\n+            \/\/ try to allocate an integer register.\n+            if (storageClass == StorageType.FLOAT) {\n+                storage = regAlloc(StorageType.INTEGER);\n+                if (storage.isPresent()) {\n+                    return storage.get();\n+                }\n+            }\n+            return stackAlloc();\n+        }\n+\n+        VMStorage[] getStorages(MemoryLayout layout, boolean isVariadicArg) {\n+            int regCnt = (int) SharedUtils.alignUp(layout.byteSize(), 8) \/ 8;\n+            if (isVariadicArg && layout.byteAlignment() == 16 && layout.byteSize() <= 16) {\n+                alignStorage();\n+                \/\/ Two registers or stack slots will be allocated, even layout.byteSize <= 8B.\n+                regCnt = 2;\n+            }\n+            VMStorage[] storages = new VMStorage[regCnt];\n+            for (int i = 0; i < regCnt; i++) {\n+                \/\/ use integer calling convention.\n+                storages[i] = getStorage(StorageType.INTEGER);\n+            }\n+            return storages;\n+        }\n+\n+        boolean regsAvailable(int integerRegs, int floatRegs) {\n+            return nRegs[IntegerRegIdx] + integerRegs <= MAX_REGISTER_ARGUMENTS &&\n+                   nRegs[FloatRegIdx] + floatRegs <= MAX_REGISTER_ARGUMENTS;\n+        }\n+\n+        \/\/ Variadic arguments with 2 * XLEN-bit alignment and size at most 2 * XLEN bits\n+        \/\/ are passed in an aligned register pair (i.e., the first register in the pair\n+        \/\/ is even-numbered), or on the stack by value if none is available.\n+        \/\/ After a variadic argument has been passed on the stack, all future arguments\n+        \/\/ will also be passed on the stack.\n+        void alignStorage() {\n+            if (nRegs[IntegerRegIdx] + 2 <= MAX_REGISTER_ARGUMENTS) {\n+                nRegs[IntegerRegIdx] = (nRegs[IntegerRegIdx] + 1) & -2;\n+            } else {\n+                nRegs[IntegerRegIdx] = MAX_REGISTER_ARGUMENTS;\n+                stackOffset = Utils.alignUp(stackOffset, 16);\n+            }\n+        }\n+\n+        @Override\n+        public String toString() {\n+            String nReg = \"iReg: \" + nRegs[IntegerRegIdx] + \", fReg: \" + nRegs[FloatRegIdx];\n+            String stack = \", stackOffset: \" + stackOffset;\n+            return \"{\" + nReg + stack + \"}\";\n+        }\n+    }\n+\n+    abstract static class BindingCalculator {\n+        protected final StorageCalculator storageCalculator;\n+\n+        @Override\n+        public String toString() {\n+            return storageCalculator.toString();\n+        }\n+\n+        protected BindingCalculator(boolean forArguments) {\n+            this.storageCalculator = new LinuxRISCV64CallArranger.StorageCalculator(forArguments);\n+        }\n+\n+        abstract List<Binding> getBindings(Class<?> carrier, MemoryLayout layout, boolean isVariadicArg);\n+\n+        \/\/ When handling variadic part, integer calling convention should be used.\n+        static final Map<TypeClass, TypeClass> conventionConverterMap =\n+                Map.ofEntries(Map.entry(FLOAT, INTEGER),\n+                              Map.entry(STRUCT_REGISTER_F, STRUCT_REGISTER_X),\n+                              Map.entry(STRUCT_REGISTER_XF, STRUCT_REGISTER_X));\n+    }\n+\n+    static class UnboxBindingCalculator extends BindingCalculator {\n+        boolean forArguments;\n+\n+        UnboxBindingCalculator(boolean forArguments) {\n+            super(forArguments);\n+            this.forArguments = forArguments;\n+        }\n+\n+        @Override\n+        List<Binding> getBindings(Class<?> carrier, MemoryLayout layout, boolean isVariadicArg) {\n+            TypeClass typeClass = TypeClass.classifyLayout(layout);\n+            if (isVariadicArg) {\n+                typeClass = BindingCalculator.conventionConverterMap.getOrDefault(typeClass, typeClass);\n+            }\n+            return getBindings(carrier, layout, typeClass, isVariadicArg);\n+        }\n+\n+        List<Binding> getBindings(Class<?> carrier, MemoryLayout layout, TypeClass argumentClass, boolean isVariadicArg) {\n+            Binding.Builder bindings = Binding.builder();\n+            switch (argumentClass) {\n+                case INTEGER -> {\n+                    VMStorage storage = storageCalculator.getStorage(StorageType.INTEGER);\n+                    bindings.vmStore(storage, carrier);\n+                }\n+                case FLOAT -> {\n+                    VMStorage storage = storageCalculator.getStorage(StorageType.FLOAT);\n+                    bindings.vmStore(storage, carrier);\n+                }\n+                case POINTER -> {\n+                    bindings.unboxAddress();\n+                    VMStorage storage = storageCalculator.getStorage(StorageType.INTEGER);\n+                    bindings.vmStore(storage, long.class);\n+                }\n+                case STRUCT_REGISTER_X -> {\n+                    assert carrier == MemorySegment.class;\n+\n+                    \/\/ When no register is available, struct will be passed by stack.\n+                    \/\/ Before allocation, stack must be aligned.\n+                    if (!storageCalculator.regsAvailable(1, 0)) {\n+                        storageCalculator.alignStack(layout.byteAlignment());\n+                    }\n+                    VMStorage[] locations = storageCalculator.getStorages(layout, isVariadicArg);\n+                    int locIndex = 0;\n+                    long offset = 0;\n+                    while (offset < layout.byteSize()) {\n+                        final long copy = Math.min(layout.byteSize() - offset, 8);\n+                        VMStorage storage = locations[locIndex++];\n+                        Class<?> type = SharedUtils.primitiveCarrierForSize(copy, false);\n+                        if (offset + copy < layout.byteSize()) {\n+                            bindings.dup();\n+                        }\n+                        bindings.bufferLoad(offset, type)\n+                                .vmStore(storage, type);\n+                        offset += copy;\n+                    }\n+                }\n+                case STRUCT_REGISTER_F -> {\n+                    assert carrier == MemorySegment.class;\n+                    List<FlattenedFieldDesc> descs = getFlattenedFields((GroupLayout) layout);\n+                    if (storageCalculator.regsAvailable(0, descs.size())) {\n+                        for (int i = 0; i < descs.size(); i++) {\n+                            FlattenedFieldDesc desc = descs.get(i);\n+                            Class<?> type = desc.layout().carrier();\n+                            VMStorage storage = storageCalculator.getStorage(StorageType.FLOAT);\n+                            if (i < descs.size() - 1) {\n+                                bindings.dup();\n+                            }\n+                            bindings.bufferLoad(desc.offset(), type)\n+                                    .vmStore(storage, type);\n+                        }\n+                    } else {\n+                        \/\/ If there is not enough register can be used, then fall back to integer calling convention.\n+                        return getBindings(carrier, layout, STRUCT_REGISTER_X, isVariadicArg);\n+                    }\n+                }\n+                case STRUCT_REGISTER_XF -> {\n+                    assert carrier == MemorySegment.class;\n+                    if (storageCalculator.regsAvailable(1, 1)) {\n+                        List<FlattenedFieldDesc> descs = getFlattenedFields((GroupLayout) layout);\n+                        for (int i = 0; i < 2; i++) {\n+                            FlattenedFieldDesc desc = descs.get(i);\n+                            int storageClass;\n+                            if (desc.typeClass() == INTEGER) {\n+                                storageClass = StorageType.INTEGER;\n+                            } else {\n+                                storageClass = StorageType.FLOAT;\n+                            }\n+                            VMStorage storage = storageCalculator.getStorage(storageClass);\n+                            Class<?> type = desc.layout().carrier();\n+                            if (i < 1) {\n+                                bindings.dup();\n+                            }\n+                            bindings.bufferLoad(desc.offset(), type)\n+                                    .vmStore(storage, type);\n+                        }\n+                    } else {\n+                        return getBindings(carrier, layout, STRUCT_REGISTER_X, isVariadicArg);\n+                    }\n+                }\n+                case STRUCT_REFERENCE -> {\n+                    assert carrier == MemorySegment.class;\n+                    bindings.copy(layout)\n+                            .unboxAddress();\n+                    VMStorage storage = storageCalculator.getStorage(StorageType.INTEGER);\n+                    bindings.vmStore(storage, long.class);\n+                }\n+                default -> throw new UnsupportedOperationException(\"Unhandled class \" + argumentClass);\n+            }\n+\n+            return bindings.build();\n+        }\n+    }\n+\n+    static class BoxBindingCalculator extends BindingCalculator {\n+\n+        BoxBindingCalculator(boolean forArguments) {\n+            super(forArguments);\n+        }\n+\n+        @Override\n+        List<Binding> getBindings(Class<?> carrier, MemoryLayout layout, boolean isVariadicArg) {\n+            TypeClass typeClass = TypeClass.classifyLayout(layout);\n+            if (isVariadicArg) {\n+                typeClass = BindingCalculator.conventionConverterMap.getOrDefault(typeClass, typeClass);\n+            }\n+            return getBindings(carrier, layout, typeClass, isVariadicArg);\n+        }\n+\n+        List<Binding> getBindings(Class<?> carrier, MemoryLayout layout, TypeClass argumentClass, boolean isVariadicArg) {\n+            Binding.Builder bindings = Binding.builder();\n+            switch (argumentClass) {\n+                case INTEGER -> {\n+                    VMStorage storage = storageCalculator.getStorage(StorageType.INTEGER);\n+                    bindings.vmLoad(storage, carrier);\n+                }\n+                case FLOAT -> {\n+                    VMStorage storage = storageCalculator.getStorage(StorageType.FLOAT);\n+                    bindings.vmLoad(storage, carrier);\n+                }\n+                case POINTER -> {\n+                    VMStorage storage = storageCalculator.getStorage(StorageType.INTEGER);\n+                    bindings.vmLoad(storage, long.class)\n+                            .boxAddressRaw(Utils.pointeeSize(layout));\n+                }\n+                case STRUCT_REGISTER_X -> {\n+                    assert carrier == MemorySegment.class;\n+\n+                    \/\/ When no register is available, struct will be passed by stack.\n+                    \/\/ Before allocation, stack must be aligned.\n+                    if (!storageCalculator.regsAvailable(1, 0)) {\n+                        storageCalculator.alignStack(layout.byteAlignment());\n+                    }\n+                    bindings.allocate(layout);\n+                    VMStorage[] locations = storageCalculator.getStorages(layout, isVariadicArg);\n+                    int locIndex = 0;\n+                    long offset = 0;\n+                    while (offset < layout.byteSize()) {\n+                        final long copy = Math.min(layout.byteSize() - offset, 8);\n+                        VMStorage storage = locations[locIndex++];\n+                        Class<?> type = SharedUtils.primitiveCarrierForSize(copy, false);\n+                        bindings.dup().vmLoad(storage, type)\n+                                .bufferStore(offset, type);\n+                        offset += copy;\n+                    }\n+                }\n+                case STRUCT_REGISTER_F -> {\n+                    assert carrier == MemorySegment.class;\n+                    bindings.allocate(layout);\n+                    List<FlattenedFieldDesc> descs = getFlattenedFields((GroupLayout) layout);\n+                    if (storageCalculator.regsAvailable(0, descs.size())) {\n+                        for (FlattenedFieldDesc desc : descs) {\n+                            Class<?> type = desc.layout().carrier();\n+                            VMStorage storage = storageCalculator.getStorage(StorageType.FLOAT);\n+                            bindings.dup()\n+                                    .vmLoad(storage, type)\n+                                    .bufferStore(desc.offset(), type);\n+                        }\n+                    } else {\n+                        return getBindings(carrier, layout, STRUCT_REGISTER_X, isVariadicArg);\n+                    }\n+                }\n+                case STRUCT_REGISTER_XF -> {\n+                    assert carrier == MemorySegment.class;\n+                    bindings.allocate(layout);\n+                    if (storageCalculator.regsAvailable(1, 1)) {\n+                        List<FlattenedFieldDesc> descs = getFlattenedFields((GroupLayout) layout);\n+                        for (int i = 0; i < 2; i++) {\n+                            FlattenedFieldDesc desc = descs.get(i);\n+                            int storageClass;\n+                            if (desc.typeClass() == INTEGER) {\n+                                storageClass = StorageType.INTEGER;\n+                            } else {\n+                                storageClass = StorageType.FLOAT;\n+                            }\n+                            VMStorage storage = storageCalculator.getStorage(storageClass);\n+                            Class<?> type = desc.layout().carrier();\n+                            bindings.dup()\n+                                    .vmLoad(storage, type)\n+                                    .bufferStore(desc.offset(), type);\n+                        }\n+                    } else {\n+                        return getBindings(carrier, layout, STRUCT_REGISTER_X, isVariadicArg);\n+                    }\n+                }\n+                case STRUCT_REFERENCE -> {\n+                    assert carrier == MemorySegment.class;\n+                    VMStorage storage = storageCalculator.getStorage(StorageType.INTEGER);\n+                    bindings.vmLoad(storage, long.class)\n+                            .boxAddress(layout);\n+                }\n+                default -> throw new UnsupportedOperationException(\"Unhandled class \" + argumentClass);\n+            }\n+\n+            return bindings.build();\n+        }\n+    }\n+}\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/foreign\/abi\/riscv64\/linux\/LinuxRISCV64CallArranger.java","additions":473,"deletions":0,"binary":false,"changes":473,"status":"added"},{"patch":"@@ -0,0 +1,77 @@\n+\/*\n+ * Copyright (c) 2020, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2023, Institute of Software, Chinese Academy of Sciences.\n+ * All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+package jdk.internal.foreign.abi.riscv64.linux;\n+\n+import jdk.internal.foreign.abi.AbstractLinker;\n+import jdk.internal.foreign.abi.LinkerOptions;\n+\n+import java.lang.foreign.SegmentScope;\n+import java.lang.foreign.FunctionDescriptor;\n+import java.lang.foreign.MemorySegment;\n+import java.lang.foreign.VaList;\n+import java.lang.invoke.MethodHandle;\n+import java.lang.invoke.MethodType;\n+import java.util.function.Consumer;\n+\n+public final class LinuxRISCV64Linker extends AbstractLinker {\n+\n+    public static LinuxRISCV64Linker getInstance() {\n+        final class Holder {\n+            private static final LinuxRISCV64Linker INSTANCE = new LinuxRISCV64Linker();\n+        }\n+\n+        return Holder.INSTANCE;\n+    }\n+\n+    private LinuxRISCV64Linker() {\n+        \/\/ Ensure there is only one instance\n+    }\n+\n+    @Override\n+    protected MethodHandle arrangeDowncall(MethodType inferredMethodType, FunctionDescriptor function, LinkerOptions options) {\n+        return LinuxRISCV64CallArranger.arrangeDowncall(inferredMethodType, function, options);\n+    }\n+\n+    @Override\n+    protected MemorySegment arrangeUpcall(MethodHandle target, MethodType targetType, FunctionDescriptor function, SegmentScope scope) {\n+        return LinuxRISCV64CallArranger.arrangeUpcall(target, targetType, function, scope);\n+    }\n+\n+    public static VaList newVaList(Consumer<VaList.Builder> actions, SegmentScope scope) {\n+        LinuxRISCV64VaList.Builder builder = LinuxRISCV64VaList.builder(scope);\n+        actions.accept(builder);\n+        return builder.build();\n+    }\n+\n+    public static VaList newVaListOfAddress(long address, SegmentScope scope) {\n+        return LinuxRISCV64VaList.ofAddress(address, scope);\n+    }\n+\n+    public static VaList emptyVaList() {\n+        return LinuxRISCV64VaList.empty();\n+    }\n+}\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/foreign\/abi\/riscv64\/linux\/LinuxRISCV64Linker.java","additions":77,"deletions":0,"binary":false,"changes":77,"status":"added"},{"patch":"@@ -0,0 +1,302 @@\n+\/*\n+ * Copyright (c) 2020, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2023, Institute of Software, Chinese Academy of Sciences.\n+ * All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+package jdk.internal.foreign.abi.riscv64.linux;\n+\n+import java.lang.foreign.GroupLayout;\n+import java.lang.foreign.MemoryLayout;\n+import java.lang.foreign.MemorySegment;\n+import java.lang.foreign.SegmentScope;\n+import java.lang.foreign.SegmentAllocator;\n+import java.lang.foreign.ValueLayout;\n+import java.lang.foreign.VaList;\n+import jdk.internal.foreign.abi.SharedUtils;\n+import jdk.internal.foreign.MemorySessionImpl;\n+import jdk.internal.foreign.Utils;\n+\n+import java.lang.invoke.VarHandle;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Objects;\n+\n+import static java.lang.foreign.ValueLayout.ADDRESS;\n+import static jdk.internal.foreign.abi.SharedUtils.SimpleVaArg;\n+import static jdk.internal.foreign.abi.SharedUtils.THROWING_ALLOCATOR;\n+\n+\/**\n+ * Standard va_list implementation as defined by RISC-V ABI document and used on Linux.\n+ * In the base integer calling convention, variadic arguments are passed in the same\n+ * manner as named arguments, with one exception. Variadic arguments with 2 * XLEN-bit\n+ * alignment and size at most 2 * XLEN bits are passed in an aligned register pair\n+ * (i.e., the first register in the pair is even-numbered), or on the stack by value\n+ * if none is available. After a variadic argument has been passed on the stack, all\n+ * future arguments will also be passed on the stack (i.e. the last argument register\n+ * may be left unused due to the aligned register pair rule).\n+ *\/\n+\n+public non-sealed class LinuxRISCV64VaList implements VaList {\n+    \/\/ The va_list type is void* on RISCV64.\n+    \/\/ See https:\/\/github.com\/riscv-non-isa\/riscv-elf-psabi-doc\/blob\/master\/riscv-cc.adoc#cc-type-representations\n+\n+    private final MemorySegment segment;\n+    private long offset;\n+\n+    private static final long STACK_SLOT_SIZE = 8;\n+    private static final VaList EMPTY\n+            = new SharedUtils.EmptyVaList(MemorySegment.NULL);\n+\n+    public static VaList empty() {\n+        return EMPTY;\n+    }\n+\n+    public LinuxRISCV64VaList(MemorySegment segment, long offset) {\n+        this.segment = segment;\n+        this.offset = offset;\n+    }\n+\n+    private static LinuxRISCV64VaList readFromAddress(long address, SegmentScope scope) {\n+        MemorySegment segment = MemorySegment.ofAddress(address, Long.MAX_VALUE, scope); \/\/ size unknown\n+        return new LinuxRISCV64VaList(segment, 0);\n+    }\n+\n+    @Override\n+    public int nextVarg(ValueLayout.OfInt layout) {\n+        return (int) read(layout);\n+    }\n+\n+    @Override\n+    public long nextVarg(ValueLayout.OfLong layout) {\n+        return (long) read(layout);\n+    }\n+\n+    @Override\n+    public double nextVarg(ValueLayout.OfDouble layout) {\n+        return (double) read(layout);\n+    }\n+\n+    @Override\n+    public MemorySegment nextVarg(ValueLayout.OfAddress layout) {\n+        return (MemorySegment) read(layout);\n+    }\n+\n+    @Override\n+    public MemorySegment nextVarg(GroupLayout layout, SegmentAllocator allocator) {\n+        Objects.requireNonNull(allocator);\n+        return (MemorySegment) read(layout, allocator);\n+    }\n+\n+    private Object read(MemoryLayout layout) {\n+        return read(layout, THROWING_ALLOCATOR);\n+    }\n+\n+    private Object read(MemoryLayout layout, SegmentAllocator allocator) {\n+        Objects.requireNonNull(layout);\n+        TypeClass typeClass = TypeClass.classifyLayout(layout);\n+        preAlignStack(layout);\n+\n+        return switch (typeClass) {\n+            case INTEGER, FLOAT, POINTER -> {\n+                checkStackElement(layout);\n+                VarHandle reader = layout.varHandle();\n+                MemorySegment slice = segment.asSlice(offset, layout.byteSize());\n+                Object res = reader.get(slice);\n+                postAlignStack(layout);\n+                yield res;\n+            }\n+            case STRUCT_REGISTER_X, STRUCT_REGISTER_F, STRUCT_REGISTER_XF -> {\n+                checkStackElement(layout);\n+                \/\/ Struct is passed indirectly via a pointer in an integer register.\n+                MemorySegment slice = segment.asSlice(offset, layout.byteSize());\n+                MemorySegment seg = allocator.allocate(layout);\n+                seg.copyFrom(slice);\n+                postAlignStack(layout);\n+                yield seg;\n+            }\n+            case STRUCT_REFERENCE -> {\n+                checkStackElement(ADDRESS);\n+                VarHandle addrReader = ADDRESS.varHandle();\n+                MemorySegment slice = segment.asSlice(offset, ADDRESS.byteSize());\n+                MemorySegment addr = (MemorySegment) addrReader.get(slice);\n+                MemorySegment seg = allocator.allocate(layout);\n+                seg.copyFrom(MemorySegment.ofAddress(addr.address(), layout.byteSize(), segment.scope()));\n+                postAlignStack(ADDRESS);\n+                yield seg;\n+            }\n+        };\n+    }\n+\n+    private void checkStackElement(MemoryLayout layout) {\n+        if (Utils.alignUp(layout.byteSize(), STACK_SLOT_SIZE) > segment.byteSize()) {\n+            throw SharedUtils.newVaListNSEE(layout);\n+        }\n+    }\n+\n+    private void preAlignStack(MemoryLayout layout) {\n+        if (layout.byteSize() <= 16 && layout.byteAlignment() == 16) {\n+            offset = Utils.alignUp(offset, 16);\n+        } else {\n+            offset = Utils.alignUp(offset, STACK_SLOT_SIZE);\n+        }\n+    }\n+\n+    private void postAlignStack(MemoryLayout layout) {\n+        if (layout.byteSize() <= 16 && layout.byteAlignment() == 16) {\n+            offset += 16;\n+        } else {\n+            offset += Utils.alignUp(layout.byteSize(), STACK_SLOT_SIZE);\n+        }\n+    }\n+\n+    @Override\n+    public void skip(MemoryLayout... layouts) {\n+        Objects.requireNonNull(layouts);\n+        ((MemorySessionImpl) segment.scope()).checkValidState();\n+        for (MemoryLayout layout : layouts) {\n+            Objects.requireNonNull(layout);\n+            preAlignStack(layout);\n+            postAlignStack(layout);\n+        }\n+    }\n+\n+    static LinuxRISCV64VaList.Builder builder(SegmentScope scope) {\n+        return new LinuxRISCV64VaList.Builder(scope);\n+    }\n+\n+    public static VaList ofAddress(long address, SegmentScope scope) {\n+        return readFromAddress(address, scope);\n+    }\n+\n+    @Override\n+    public VaList copy() {\n+        MemorySessionImpl sessionImpl = (MemorySessionImpl) segment.scope();\n+        sessionImpl.checkValidState();\n+        return new LinuxRISCV64VaList(segment, offset);\n+    }\n+\n+    @Override\n+    public MemorySegment segment() {\n+        \/\/ make sure that returned segment cannot be accessed\n+        return segment.asSlice(0, 0);\n+    }\n+\n+    public long address() {\n+        return segment.address() + offset;\n+    }\n+\n+    @Override\n+    public String toString() {\n+        return \"LinuxRISCV64VaList{\" + \"seg: \" + address() + \", \" + \"offset: \" + offset + '}';\n+    }\n+\n+    public static non-sealed class Builder implements VaList.Builder {\n+\n+        private final SegmentScope scope;\n+        private final List<SimpleVaArg> stackArgs = new ArrayList<>();\n+\n+        Builder(SegmentScope scope) {\n+            this.scope = scope;\n+        }\n+\n+        @Override\n+        public Builder addVarg(ValueLayout.OfInt layout, int value) {\n+            return arg(layout, value);\n+        }\n+\n+        @Override\n+        public Builder addVarg(ValueLayout.OfLong layout, long value) {\n+            return arg(layout, value);\n+        }\n+\n+        @Override\n+        public Builder addVarg(ValueLayout.OfDouble layout, double value) {\n+            return arg(layout, value);\n+        }\n+\n+        @Override\n+        public Builder addVarg(ValueLayout.OfAddress layout, MemorySegment value) {\n+            return arg(layout, value);\n+        }\n+\n+        @Override\n+        public Builder addVarg(GroupLayout layout, MemorySegment value) {\n+            return arg(layout, value);\n+        }\n+\n+        private Builder arg(MemoryLayout layout, Object value) {\n+            Objects.requireNonNull(layout);\n+            Objects.requireNonNull(value);\n+            stackArgs.add(new SimpleVaArg(layout, value));\n+            return this;\n+        }\n+\n+        boolean isEmpty() {\n+            return stackArgs.isEmpty();\n+        }\n+\n+        public VaList build() {\n+            if (isEmpty()) {\n+                return EMPTY;\n+            }\n+            long stackArgsSize = 0;\n+            for (SimpleVaArg arg : stackArgs) {\n+                MemoryLayout layout = arg.layout;\n+                long elementSize = TypeClass.classifyLayout(layout) == TypeClass.STRUCT_REFERENCE ?\n+                    ADDRESS.byteSize() : layout.byteSize();\n+                \/\/ arguments with 2 * XLEN-bit alignment and size at most 2 * XLEN bits\n+                \/\/ are saved on memory aligned with 2 * XLEN (XLEN=64 for RISCV64).\n+                if (layout.byteSize() <= 16 && layout.byteAlignment() == 16) {\n+                    stackArgsSize = Utils.alignUp(stackArgsSize, 16);\n+                    elementSize = 16;\n+                }\n+                stackArgsSize += Utils.alignUp(elementSize, STACK_SLOT_SIZE);\n+            }\n+            MemorySegment argsSegment = MemorySegment.allocateNative(stackArgsSize, 16, scope);\n+            MemorySegment writeCursor = argsSegment;\n+            for (SimpleVaArg arg : stackArgs) {\n+                MemoryLayout layout;\n+                Object value = arg.value;\n+                if (TypeClass.classifyLayout(arg.layout) == TypeClass.STRUCT_REFERENCE) {\n+                    layout = ADDRESS;\n+                } else {\n+                    layout = arg.layout;\n+                }\n+                long alignedSize = Utils.alignUp(layout.byteSize(), STACK_SLOT_SIZE);\n+                if (layout.byteSize() <= 16 && layout.byteAlignment() == 16) {\n+                    writeCursor = Utils.alignUp(writeCursor, 16);\n+                    alignedSize = 16;\n+                }\n+                if (layout instanceof GroupLayout) {\n+                    writeCursor.copyFrom((MemorySegment) value);\n+                } else {\n+                    VarHandle writer = layout.varHandle();\n+                    writer.set(writeCursor, value);\n+                }\n+                writeCursor = writeCursor.asSlice(alignedSize);\n+            }\n+            return new LinuxRISCV64VaList(argsSegment, 0L);\n+        }\n+    }\n+}\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/foreign\/abi\/riscv64\/linux\/LinuxRISCV64VaList.java","additions":302,"deletions":0,"binary":false,"changes":302,"status":"added"},{"patch":"@@ -0,0 +1,215 @@\n+\/*\n+ * Copyright (c) 2020, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2023, Institute of Software, Chinese Academy of Sciences.\n+ * All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+package jdk.internal.foreign.abi.riscv64.linux;\n+\n+import java.lang.foreign.GroupLayout;\n+import java.lang.foreign.MemoryLayout;\n+import java.lang.foreign.MemorySegment;\n+import java.lang.foreign.PaddingLayout;\n+import java.lang.foreign.SequenceLayout;\n+import java.lang.foreign.UnionLayout;\n+import java.lang.foreign.ValueLayout;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+public enum TypeClass {\n+    \/*\n+     * STRUCT_REFERENCE: Aggregates larger than 2 * XLEN bits are passed by reference and are replaced\n+     *     in the argument list with the address. The address will be passed in a register if at least\n+     *     one register is available, otherwise it will be passed on the stack.\n+     *\n+     * STRUCT_REGISTER_F: A struct containing just one floating-point real is passed as though it were\n+     *     a standalone floating-point real. A struct containing two floating-point reals is passed in two\n+     *     floating-point registers, if neither real is more than ABI_FLEN bits wide and at least two\n+     *     floating-point argument registers are available. (The registers need not be an aligned pair.)\n+     *     Otherwise, it is passed according to the integer calling convention.\n+     *\n+     * STRUCT_REGISTER_XF: A struct containing one floating-point real and one integer (or bitfield), in either\n+     *     order, is passed in a floating-point register and an integer register, provided the floating-point real\n+     *     is no more than ABI_FLEN bits wide and the integer is no more than XLEN bits wide, and at least one\n+     *     floating-point argument register and at least one integer argument register is available. If the struct\n+     *     is not passed in this manner, then it is passed according to the integer calling convention.\n+     *\n+     * STRUCT_REGISTER_X: Aggregates whose total size is no more than XLEN bits are passed in a register, with the\n+     *     fields laid out as though they were passed in memory. If no register is available, the aggregate is\n+     *     passed on the stack. Aggregates whose total size is no more than 2 * XLEN bits are passed in a pair of\n+     *     registers; if only one register is available, the first XLEN bits are passed in a register and the\n+     *     remaining bits are passed on the stack. If no registers are available, the aggregate is passed on the stack.\n+     *\n+     * See https:\/\/github.com\/riscv-non-isa\/riscv-elf-psabi-doc\/blob\/master\/riscv-cc.adoc\n+     * *\/\n+    INTEGER,\n+    FLOAT,\n+    POINTER,\n+    STRUCT_REFERENCE,\n+    STRUCT_REGISTER_F,\n+    STRUCT_REGISTER_XF,\n+    STRUCT_REGISTER_X;\n+\n+    private static final int MAX_AGGREGATE_REGS_SIZE = 2;\n+\n+    \/*\n+     * Struct will be flattened while classifying. That is, struct{struct{int, double}} will be treated\n+     * same as struct{int, double} and struct{int[2]} will be treated same as struct{int, int}.\n+     * *\/\n+    private static record FieldCounter(long integerCnt, long floatCnt, long pointerCnt) {\n+        static final FieldCounter EMPTY = new FieldCounter(0, 0, 0);\n+        static final FieldCounter SINGLE_INTEGER = new FieldCounter(1, 0, 0);\n+        static final FieldCounter SINGLE_FLOAT = new FieldCounter(0, 1, 0);\n+        static final FieldCounter SINGLE_POINTER = new FieldCounter(0, 0, 1);\n+\n+        static FieldCounter flatten(MemoryLayout layout) {\n+            if (layout instanceof ValueLayout valueLayout) {\n+                return switch (classifyValueType(valueLayout)) {\n+                    case INTEGER -> FieldCounter.SINGLE_INTEGER;\n+                    case FLOAT -> FieldCounter.SINGLE_FLOAT;\n+                    case POINTER -> FieldCounter.SINGLE_POINTER;\n+                    default -> throw new IllegalStateException(\"Should not reach here.\");\n+                };\n+            } else if (layout instanceof GroupLayout groupLayout) {\n+                FieldCounter currCounter = FieldCounter.EMPTY;\n+                for (MemoryLayout memberLayout : groupLayout.memberLayouts()) {\n+                    if (memberLayout instanceof PaddingLayout) {\n+                        continue;\n+                    }\n+                    currCounter = currCounter.add(flatten(memberLayout));\n+                }\n+                return currCounter;\n+            } else if (layout instanceof SequenceLayout sequenceLayout) {\n+                long elementCount = sequenceLayout.elementCount();\n+                if (elementCount == 0) {\n+                    return FieldCounter.EMPTY;\n+                }\n+                return flatten(sequenceLayout.elementLayout()).mul(elementCount);\n+            } else {\n+                throw new IllegalStateException(\"Cannot get here: \" + layout);\n+            }\n+        }\n+\n+        FieldCounter mul(long m) {\n+            return new FieldCounter(integerCnt * m,\n+                                    floatCnt * m,\n+                                    pointerCnt * m);\n+        }\n+\n+        FieldCounter add(FieldCounter other) {\n+            return new FieldCounter(integerCnt + other.integerCnt,\n+                                    floatCnt + other.floatCnt,\n+                                    pointerCnt + other.pointerCnt);\n+        }\n+    }\n+\n+    public static record FlattenedFieldDesc(TypeClass typeClass, long offset, ValueLayout layout) {\n+\n+    }\n+\n+    private static List<FlattenedFieldDesc> getFlattenedFieldsInner(long offset, MemoryLayout layout) {\n+        if (layout instanceof ValueLayout valueLayout) {\n+            TypeClass typeClass = classifyValueType(valueLayout);\n+            return List.of(switch (typeClass) {\n+                case INTEGER, FLOAT -> new FlattenedFieldDesc(typeClass, offset, valueLayout);\n+                default -> throw new IllegalStateException(\"Should not reach here.\");\n+            });\n+        } else if (layout instanceof GroupLayout groupLayout) {\n+            List<FlattenedFieldDesc> fields = new ArrayList<>();\n+            for (MemoryLayout memberLayout : groupLayout.memberLayouts()) {\n+                if (memberLayout instanceof PaddingLayout) {\n+                    offset += memberLayout.byteSize();\n+                    continue;\n+                }\n+                fields.addAll(getFlattenedFieldsInner(offset, memberLayout));\n+                offset += memberLayout.byteSize();\n+            }\n+            return fields;\n+        } else if (layout instanceof SequenceLayout sequenceLayout) {\n+            List<FlattenedFieldDesc> fields = new ArrayList<>();\n+            MemoryLayout elementLayout = sequenceLayout.elementLayout();\n+            for (long i = 0; i < sequenceLayout.elementCount(); i++) {\n+                fields.addAll(getFlattenedFieldsInner(offset, elementLayout));\n+                offset += elementLayout.byteSize();\n+            }\n+            return fields;\n+        } else {\n+            throw new IllegalStateException(\"Cannot get here: \" + layout);\n+        }\n+    }\n+\n+    public static List<FlattenedFieldDesc> getFlattenedFields(GroupLayout layout) {\n+        return getFlattenedFieldsInner(0, layout);\n+    }\n+\n+    \/\/ ValueLayout will be classified by its carrier type.\n+    private static TypeClass classifyValueType(ValueLayout type) {\n+        Class<?> carrier = type.carrier();\n+        if (carrier == boolean.class || carrier == byte.class || carrier == char.class ||\n+            carrier == short.class || carrier == int.class || carrier == long.class) {\n+            return INTEGER;\n+        } else if (carrier == float.class || carrier == double.class) {\n+            return FLOAT;\n+        } else if (carrier == MemorySegment.class) {\n+            return POINTER;\n+        } else {\n+            throw new IllegalStateException(\"Cannot get here: \" + carrier.getName());\n+        }\n+    }\n+\n+    private static boolean isRegisterAggregate(MemoryLayout type) {\n+        return type.bitSize() <= MAX_AGGREGATE_REGS_SIZE * 64;\n+    }\n+\n+    private static TypeClass classifyStructType(GroupLayout layout) {\n+        if (layout instanceof UnionLayout) {\n+            return isRegisterAggregate(layout) ? STRUCT_REGISTER_X : STRUCT_REFERENCE;\n+        }\n+\n+        if (!isRegisterAggregate(layout)) {\n+            return STRUCT_REFERENCE;\n+        }\n+\n+        \/\/ classify struct by its fields.\n+        FieldCounter counter = FieldCounter.flatten(layout);\n+        if (counter.integerCnt == 0 && counter.pointerCnt == 0 &&\n+            (counter.floatCnt == 1 || counter.floatCnt == 2)) {\n+            return STRUCT_REGISTER_F;\n+        } else if (counter.integerCnt == 1 && counter.floatCnt == 1 &&\n+                   counter.pointerCnt == 0) {\n+            return STRUCT_REGISTER_XF;\n+        } else {\n+            return STRUCT_REGISTER_X;\n+        }\n+    }\n+\n+    public static TypeClass classifyLayout(MemoryLayout type) {\n+        if (type instanceof ValueLayout vt) {\n+            return classifyValueType(vt);\n+        } else if (type instanceof GroupLayout gt) {\n+            return classifyStructType(gt);\n+        } else {\n+            throw new IllegalArgumentException(\"Unsupported layout: \" + type);\n+        }\n+    }\n+}\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/foreign\/abi\/riscv64\/linux\/TypeClass.java","additions":215,"deletions":0,"binary":false,"changes":215,"status":"added"},{"patch":"@@ -43,1 +43,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n","filename":"test\/jdk\/java\/foreign\/LibraryLookupTest.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -27,1 +27,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n","filename":"test\/jdk\/java\/foreign\/SafeFunctionAccessTest.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -27,1 +27,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n","filename":"test\/jdk\/java\/foreign\/StdLibTest.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -27,1 +27,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n","filename":"test\/jdk\/java\/foreign\/TestClassLoaderFindNative.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -27,1 +27,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n","filename":"test\/jdk\/java\/foreign\/TestDowncallScope.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -27,1 +27,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n","filename":"test\/jdk\/java\/foreign\/TestDowncallStack.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -28,1 +28,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n","filename":"test\/jdk\/java\/foreign\/TestFunctionDescriptor.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -28,1 +28,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n","filename":"test\/jdk\/java\/foreign\/TestHeapAlignment.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -28,1 +28,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n","filename":"test\/jdk\/java\/foreign\/TestIllegalLink.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -27,1 +27,1 @@\n- * @requires os.arch==\"amd64\" | os.arch==\"x86_64\" | os.arch==\"aarch64\"\n+ * @requires os.arch==\"amd64\" | os.arch==\"x86_64\" | os.arch==\"aarch64\" | os.arch==\"riscv64\"\n","filename":"test\/jdk\/java\/foreign\/TestIntrinsics.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -70,0 +70,1 @@\n+        addLayoutConstants(testValues, PlatformLayouts.RISCV64.class);\n","filename":"test\/jdk\/java\/foreign\/TestLayoutEquality.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -27,1 +27,1 @@\n- * @requires os.arch==\"amd64\" | os.arch==\"x86_64\" | os.arch==\"aarch64\"\n+ * @requires os.arch==\"amd64\" | os.arch==\"x86_64\" | os.arch==\"aarch64\" | os.arch==\"riscv64\"\n","filename":"test\/jdk\/java\/foreign\/TestLinker.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -36,1 +36,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n@@ -48,1 +48,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n@@ -60,1 +60,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n@@ -72,1 +72,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n@@ -84,1 +84,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n@@ -95,1 +95,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n@@ -106,1 +106,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n@@ -117,1 +117,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n@@ -128,1 +128,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n@@ -140,1 +140,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n@@ -152,1 +152,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n@@ -164,1 +164,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n@@ -176,1 +176,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n@@ -188,1 +188,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n@@ -200,1 +200,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n@@ -212,1 +212,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n@@ -224,1 +224,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n@@ -236,1 +236,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n@@ -248,1 +248,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n@@ -260,1 +260,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n@@ -273,1 +273,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n","filename":"test\/jdk\/java\/foreign\/TestMatrix.java","additions":21,"deletions":21,"binary":false,"changes":42,"status":"modified"},{"patch":"@@ -27,1 +27,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n","filename":"test\/jdk\/java\/foreign\/TestNULLAddress.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -28,1 +28,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n","filename":"test\/jdk\/java\/foreign\/TestNative.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -27,1 +27,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n","filename":"test\/jdk\/java\/foreign\/TestNulls.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -27,1 +27,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n","filename":"test\/jdk\/java\/foreign\/TestScopedOperations.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -27,1 +27,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n","filename":"test\/jdk\/java\/foreign\/TestSegments.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -34,1 +34,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n","filename":"test\/jdk\/java\/foreign\/TestStringEncoding.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -27,1 +27,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n","filename":"test\/jdk\/java\/foreign\/TestUpcallAsync.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -27,1 +27,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n","filename":"test\/jdk\/java\/foreign\/TestUpcallException.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -28,1 +28,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n","filename":"test\/jdk\/java\/foreign\/TestUpcallHighArity.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -27,1 +27,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n","filename":"test\/jdk\/java\/foreign\/TestUpcallScope.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -27,1 +27,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n","filename":"test\/jdk\/java\/foreign\/TestUpcallStack.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -28,1 +28,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n","filename":"test\/jdk\/java\/foreign\/TestUpcallStructScope.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -28,1 +28,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n","filename":"test\/jdk\/java\/foreign\/TestVarArgs.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -0,0 +1,543 @@\n+\/*\n+ * Copyright (c) 2020, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2023, Institute of Software, Chinese Academy of Sciences.\n+ * All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+\/*\n+ * @test\n+ * @enablePreview\n+ * @requires sun.arch.data.model == \"64\"\n+ * @modules java.base\/jdk.internal.foreign\n+ *          java.base\/jdk.internal.foreign.abi\n+ *          java.base\/jdk.internal.foreign.abi.riscv64\n+ *          java.base\/jdk.internal.foreign.abi.riscv64.linux\n+ * @build CallArrangerTestBase\n+ * @run testng TestRISCV64CallArranger\n+ *\/\n+\n+import java.lang.foreign.FunctionDescriptor;\n+import java.lang.foreign.MemoryLayout;\n+import java.lang.foreign.MemorySegment;\n+import java.lang.foreign.StructLayout;\n+import jdk.internal.foreign.abi.Binding;\n+import jdk.internal.foreign.abi.CallingSequence;\n+import jdk.internal.foreign.abi.LinkerOptions;\n+import jdk.internal.foreign.abi.riscv64.linux.LinuxRISCV64CallArranger;\n+import jdk.internal.foreign.abi.StubLocations;\n+import jdk.internal.foreign.abi.VMStorage;\n+import org.testng.annotations.DataProvider;\n+import org.testng.annotations.Test;\n+\n+import java.lang.invoke.MethodType;\n+\n+import static java.lang.foreign.Linker.Option.firstVariadicArg;\n+import static java.lang.foreign.ValueLayout.ADDRESS;\n+import static jdk.internal.foreign.PlatformLayouts.RISCV64.*;\n+import static jdk.internal.foreign.abi.Binding.*;\n+import static jdk.internal.foreign.abi.riscv64.RISCV64Architecture.*;\n+import static jdk.internal.foreign.abi.riscv64.RISCV64Architecture.Regs.*;\n+\n+import static org.testng.Assert.assertEquals;\n+import static org.testng.Assert.assertFalse;\n+import static org.testng.Assert.assertTrue;\n+\n+public class TestRISCV64CallArranger extends CallArrangerTestBase {\n+\n+    private static final short STACK_SLOT_SIZE = 8;\n+    private static final VMStorage TARGET_ADDRESS_STORAGE = StubLocations.TARGET_ADDRESS.storage(StorageType.PLACEHOLDER);\n+    private static final VMStorage RETURN_BUFFER_STORAGE = StubLocations.RETURN_BUFFER.storage(StorageType.PLACEHOLDER);\n+\n+    @Test\n+    public void testEmpty() {\n+        MethodType mt = MethodType.methodType(void.class);\n+        FunctionDescriptor fd = FunctionDescriptor.ofVoid();\n+        LinuxRISCV64CallArranger.Bindings bindings = LinuxRISCV64CallArranger.getBindings(mt, fd, false);\n+\n+        assertFalse(bindings.isInMemoryReturn());\n+        CallingSequence callingSequence = bindings.callingSequence();\n+        assertEquals(callingSequence.callerMethodType(), mt.insertParameterTypes(0, MemorySegment.class));\n+        assertEquals(callingSequence.functionDesc(), fd.insertArgumentLayouts(0, ADDRESS));\n+\n+        checkArgumentBindings(callingSequence, new Binding[][]{\n+            { unboxAddress(), vmStore(TARGET_ADDRESS_STORAGE, long.class) }\n+        });\n+\n+        checkReturnBindings(callingSequence, new Binding[]{});\n+    }\n+\n+    @Test\n+    public void testInteger() {\n+        MethodType mt = MethodType.methodType(void.class,\n+            byte.class, short.class, int.class, int.class,\n+            int.class, int.class, long.class, int.class,\n+            int.class, byte.class);\n+        FunctionDescriptor fd = FunctionDescriptor.ofVoid(\n+            C_CHAR, C_SHORT, C_INT, C_INT,\n+            C_INT, C_INT, C_LONG, C_INT,\n+            C_INT, C_CHAR);\n+        LinuxRISCV64CallArranger.Bindings bindings = LinuxRISCV64CallArranger.getBindings(mt, fd, false);\n+\n+        assertFalse(bindings.isInMemoryReturn());\n+        CallingSequence callingSequence = bindings.callingSequence();\n+        assertEquals(callingSequence.callerMethodType(), mt.insertParameterTypes(0, MemorySegment.class));\n+        assertEquals(callingSequence.functionDesc(), fd.insertArgumentLayouts(0, ADDRESS));\n+\n+        checkArgumentBindings(callingSequence, new Binding[][]{\n+            { unboxAddress(), vmStore(TARGET_ADDRESS_STORAGE, long.class) },\n+            { cast(byte.class, int.class), vmStore(x10, int.class) },\n+            { cast(short.class, int.class), vmStore(x11, int.class) },\n+            { vmStore(x12, int.class) },\n+            { vmStore(x13, int.class) },\n+            { vmStore(x14, int.class) },\n+            { vmStore(x15, int.class) },\n+            { vmStore(x16, long.class) },\n+            { vmStore(x17, int.class) },\n+            { vmStore(stackStorage(STACK_SLOT_SIZE, 0), int.class) },\n+            { cast(byte.class, int.class), vmStore(stackStorage(STACK_SLOT_SIZE, 8), int.class) }\n+        });\n+\n+        checkReturnBindings(callingSequence, new Binding[]{});\n+    }\n+\n+    @Test\n+    public void testTwoIntTwoFloat() {\n+        MethodType mt = MethodType.methodType(void.class, int.class, int.class, float.class, float.class);\n+        FunctionDescriptor fd = FunctionDescriptor.ofVoid(C_INT, C_INT, C_FLOAT, C_FLOAT);\n+        LinuxRISCV64CallArranger.Bindings bindings = LinuxRISCV64CallArranger.getBindings(mt, fd, false);\n+\n+        assertFalse(bindings.isInMemoryReturn());\n+        CallingSequence callingSequence = bindings.callingSequence();\n+        assertEquals(callingSequence.callerMethodType(), mt.insertParameterTypes(0, MemorySegment.class));\n+        assertEquals(callingSequence.functionDesc(), fd.insertArgumentLayouts(0, ADDRESS));\n+\n+        checkArgumentBindings(callingSequence, new Binding[][]{\n+            { unboxAddress(), vmStore(TARGET_ADDRESS_STORAGE, long.class) },\n+            { vmStore(x10, int.class) },\n+            { vmStore(x11, int.class) },\n+            { vmStore(f10, float.class) },\n+            { vmStore(f11, float.class) }\n+        });\n+\n+        checkReturnBindings(callingSequence, new Binding[]{});\n+    }\n+\n+    @Test(dataProvider = \"structs\")\n+    public void testStruct(MemoryLayout struct, Binding[] expectedBindings) {\n+        MethodType mt = MethodType.methodType(void.class, MemorySegment.class);\n+        FunctionDescriptor fd = FunctionDescriptor.ofVoid(struct);\n+        LinuxRISCV64CallArranger.Bindings bindings = LinuxRISCV64CallArranger.getBindings(mt, fd, false);\n+\n+        assertFalse(bindings.isInMemoryReturn());\n+        CallingSequence callingSequence = bindings.callingSequence();\n+        assertEquals(callingSequence.callerMethodType(), mt.insertParameterTypes(0, MemorySegment.class));\n+        assertEquals(callingSequence.functionDesc(), fd.insertArgumentLayouts(0, ADDRESS));\n+\n+        checkArgumentBindings(callingSequence, new Binding[][]{\n+            { unboxAddress(), vmStore(TARGET_ADDRESS_STORAGE, long.class) },\n+            expectedBindings\n+        });\n+\n+        checkReturnBindings(callingSequence, new Binding[]{});\n+    }\n+\n+    @DataProvider\n+    public static Object[][] structs() {\n+        MemoryLayout struct1 = MemoryLayout.structLayout(C_INT, C_INT, C_DOUBLE, C_INT);\n+        return new Object[][]{\n+            \/\/ struct s { void* a; double c; };\n+            {\n+                MemoryLayout.structLayout(C_POINTER, C_DOUBLE),\n+                new Binding[]{\n+                    dup(),\n+                    bufferLoad(0, long.class), vmStore(x10, long.class),\n+                    bufferLoad(8, long.class), vmStore(x11, long.class)\n+                }\n+            },\n+            \/\/ struct s { int32_t a, b; double c; };\n+            { MemoryLayout.structLayout(C_INT, C_INT, C_DOUBLE),\n+                new Binding[]{\n+                    dup(),\n+                    \/\/ s.a & s.b\n+                    bufferLoad(0, long.class), vmStore(x10, long.class),\n+                    \/\/ s.c\n+                    bufferLoad(8, long.class), vmStore(x11, long.class)\n+                }\n+            },\n+            \/\/ struct s { int32_t a, b; double c; int32_t d; };\n+            { struct1,\n+                new Binding[]{\n+                    copy(struct1),\n+                    unboxAddress(),\n+                    vmStore(x10, long.class)\n+                }\n+            },\n+            \/\/ struct s { int32_t a[1]; float b[1]; };\n+            { MemoryLayout.structLayout(MemoryLayout.sequenceLayout(1, C_INT),\n+                MemoryLayout.sequenceLayout(1, C_FLOAT)),\n+                new Binding[]{\n+                    dup(),\n+                    \/\/ s.a[0]\n+                    bufferLoad(0, int.class), vmStore(x10, int.class),\n+                    \/\/ s.b[0]\n+                    bufferLoad(4, float.class), vmStore(f10, float.class)\n+                }\n+            },\n+            \/\/ struct s { float a; \/* padding *\/ double b };\n+            { MemoryLayout.structLayout(C_FLOAT, MemoryLayout.paddingLayout(32), C_DOUBLE),\n+                new Binding[]{\n+                    dup(),\n+                    \/\/ s.a\n+                    bufferLoad(0, float.class), vmStore(f10, float.class),\n+                    \/\/ s.b\n+                    bufferLoad(8, double.class), vmStore(f11, double.class),\n+                }\n+            },\n+            \/\/ struct __attribute__((__packed__)) s { float a; double b; };\n+            { MemoryLayout.structLayout(C_FLOAT, C_DOUBLE),\n+                new Binding[]{\n+                    dup(),\n+                    \/\/ s.a\n+                    bufferLoad(0, float.class), vmStore(f10, float.class),\n+                    \/\/ s.b\n+                    bufferLoad(4, double.class), vmStore(f11, double.class),\n+                }\n+            },\n+            \/\/ struct s { float a; float b __attribute__ ((aligned (8))); }\n+            { MemoryLayout.structLayout(C_FLOAT, MemoryLayout.paddingLayout(32),\n+                C_FLOAT, MemoryLayout.paddingLayout(32)),\n+                new Binding[]{\n+                    dup(),\n+                    \/\/ s.a\n+                    bufferLoad(0, float.class), vmStore(f10, float.class),\n+                    \/\/ s.b\n+                    bufferLoad(8, float.class), vmStore(f11, float.class),\n+                }\n+            }\n+        };\n+    }\n+\n+    @Test\n+    public void testStructFA1() {\n+        MemoryLayout fa = MemoryLayout.structLayout(C_FLOAT, C_FLOAT);\n+\n+        MethodType mt = MethodType.methodType(MemorySegment.class, float.class, int.class, MemorySegment.class);\n+        FunctionDescriptor fd = FunctionDescriptor.of(fa, C_FLOAT, C_INT, fa);\n+        LinuxRISCV64CallArranger.Bindings bindings = LinuxRISCV64CallArranger.getBindings(mt, fd, false);\n+\n+        assertFalse(bindings.isInMemoryReturn());\n+        CallingSequence callingSequence = bindings.callingSequence();\n+        assertEquals(callingSequence.callerMethodType(), mt.insertParameterTypes(0, MemorySegment.class, MemorySegment.class));\n+        assertEquals(callingSequence.functionDesc(), fd.insertArgumentLayouts(0, ADDRESS, ADDRESS));\n+\n+        checkArgumentBindings(callingSequence, new Binding[][]{\n+            { unboxAddress(), vmStore(RETURN_BUFFER_STORAGE, long.class) },\n+            { unboxAddress(), vmStore(TARGET_ADDRESS_STORAGE, long.class) },\n+            { vmStore(f10, float.class) },\n+            { vmStore(x10, int.class) },\n+            {\n+                dup(),\n+                bufferLoad(0, float.class),\n+                vmStore(f11, float.class),\n+                bufferLoad(4, float.class),\n+                vmStore(f12, float.class)\n+            }\n+        });\n+\n+        checkReturnBindings(callingSequence, new Binding[]{\n+            allocate(fa),\n+            dup(),\n+            vmLoad(f10, float.class),\n+            bufferStore(0, float.class),\n+            dup(),\n+            vmLoad(f11, float.class),\n+            bufferStore(4, float.class)\n+        });\n+    }\n+\n+    @Test\n+    public void testStructFA2() {\n+        MemoryLayout fa = MemoryLayout.structLayout(C_FLOAT, C_DOUBLE);\n+\n+        MethodType mt = MethodType.methodType(MemorySegment.class, float.class, int.class, MemorySegment.class);\n+        FunctionDescriptor fd = FunctionDescriptor.of(fa, C_FLOAT, C_INT, fa);\n+        LinuxRISCV64CallArranger.Bindings bindings = LinuxRISCV64CallArranger.getBindings(mt, fd, false);\n+\n+        assertFalse(bindings.isInMemoryReturn());\n+        CallingSequence callingSequence = bindings.callingSequence();\n+        assertEquals(callingSequence.callerMethodType(), mt.insertParameterTypes(0, MemorySegment.class, MemorySegment.class));\n+        assertEquals(callingSequence.functionDesc(), fd.insertArgumentLayouts(0, ADDRESS, ADDRESS));\n+\n+        checkArgumentBindings(callingSequence, new Binding[][]{\n+            { unboxAddress(), vmStore(RETURN_BUFFER_STORAGE, long.class) },\n+            { unboxAddress(), vmStore(TARGET_ADDRESS_STORAGE, long.class) },\n+            { vmStore(f10, float.class) },\n+            { vmStore(x10, int.class) },\n+            {\n+                dup(),\n+                bufferLoad(0, float.class),\n+                vmStore(f11, float.class),\n+                bufferLoad(4, double.class),\n+                vmStore(f12, double.class)\n+            }\n+        });\n+\n+        checkReturnBindings(callingSequence, new Binding[]{\n+            allocate(fa),\n+            dup(),\n+            vmLoad(f10, float.class),\n+            bufferStore(0, float.class),\n+            dup(),\n+            vmLoad(f11, double.class),\n+            bufferStore(4, double.class)\n+        });\n+    }\n+\n+    @Test\n+    void spillFloatingPointStruct() {\n+        MemoryLayout struct = MemoryLayout.structLayout(C_FLOAT, C_FLOAT);\n+        \/\/ void f(float, float, float, float, float, float, float, struct)\n+        MethodType mt = MethodType.methodType(void.class, float.class, float.class,\n+            float.class, float.class, float.class,\n+            float.class, float.class, MemorySegment.class);\n+        FunctionDescriptor fd = FunctionDescriptor.ofVoid(C_FLOAT, C_FLOAT, C_FLOAT, C_FLOAT,\n+            C_FLOAT, C_FLOAT, C_FLOAT, struct);\n+        LinuxRISCV64CallArranger.Bindings bindings = LinuxRISCV64CallArranger.getBindings(mt, fd, false);\n+\n+        assertFalse(bindings.isInMemoryReturn());\n+        CallingSequence callingSequence = bindings.callingSequence();\n+        assertEquals(callingSequence.callerMethodType(), mt.insertParameterTypes(0, MemorySegment.class));\n+        assertEquals(callingSequence.functionDesc(), fd.insertArgumentLayouts(0, ADDRESS));\n+\n+        checkArgumentBindings(callingSequence, new Binding[][]{\n+            { unboxAddress(), vmStore(TARGET_ADDRESS_STORAGE, long.class) },\n+            { vmStore(f10, float.class) },\n+            { vmStore(f11, float.class) },\n+            { vmStore(f12, float.class) },\n+            { vmStore(f13, float.class) },\n+            { vmStore(f14, float.class) },\n+            { vmStore(f15, float.class) },\n+            { vmStore(f16, float.class) },\n+            {\n+                bufferLoad(0, long.class),\n+                vmStore(x10, long.class)\n+            }\n+        });\n+\n+        checkReturnBindings(callingSequence, new Binding[]{});\n+    }\n+\n+    @Test\n+    public void testStructBoth() {\n+        MemoryLayout struct = MemoryLayout.structLayout(C_INT, C_FLOAT);\n+\n+        MethodType mt = MethodType.methodType(void.class, MemorySegment.class, MemorySegment.class, MemorySegment.class);\n+        FunctionDescriptor fd = FunctionDescriptor.ofVoid(struct, struct, struct);\n+        LinuxRISCV64CallArranger.Bindings bindings = LinuxRISCV64CallArranger.getBindings(mt, fd, false);\n+\n+        assertFalse(bindings.isInMemoryReturn());\n+        CallingSequence callingSequence = bindings.callingSequence();\n+        assertEquals(callingSequence.callerMethodType(), mt.insertParameterTypes(0, MemorySegment.class));\n+        assertEquals(callingSequence.functionDesc(), fd.insertArgumentLayouts(0, ADDRESS));\n+\n+        checkArgumentBindings(callingSequence, new Binding[][]{\n+            { unboxAddress(), vmStore(TARGET_ADDRESS_STORAGE, long.class) },\n+            {\n+                dup(),\n+                bufferLoad(0, int.class),\n+                vmStore(x10, int.class),\n+                bufferLoad(4, float.class),\n+                vmStore(f10, float.class)\n+            },\n+            {\n+                dup(),\n+                bufferLoad(0, int.class),\n+                vmStore(x11, int.class),\n+                bufferLoad(4, float.class),\n+                vmStore(f11, float.class)\n+            },\n+            {\n+                dup(),\n+                bufferLoad(0, int.class),\n+                vmStore(x12, int.class),\n+                bufferLoad(4, float.class),\n+                vmStore(f12, float.class)\n+            }\n+        });\n+\n+        checkReturnBindings(callingSequence, new Binding[]{});\n+    }\n+\n+    @Test\n+    public void testStructStackSpill() {\n+        \/\/ A large (> 16 byte) struct argument that is spilled to the\n+        \/\/ stack should be passed as a pointer to a copy and occupy one\n+        \/\/ stack slot.\n+\n+        MemoryLayout struct = MemoryLayout.structLayout(C_INT, C_INT, C_DOUBLE, C_INT);\n+\n+        MethodType mt = MethodType.methodType(\n+            void.class, MemorySegment.class, MemorySegment.class, int.class, int.class,\n+            int.class, int.class, int.class, int.class, MemorySegment.class, int.class);\n+        FunctionDescriptor fd = FunctionDescriptor.ofVoid(\n+            struct, struct, C_INT, C_INT, C_INT, C_INT, C_INT, C_INT, struct, C_INT);\n+        LinuxRISCV64CallArranger.Bindings bindings = LinuxRISCV64CallArranger.getBindings(mt, fd, false);\n+\n+        assertFalse(bindings.isInMemoryReturn());\n+        CallingSequence callingSequence = bindings.callingSequence();\n+        assertEquals(callingSequence.callerMethodType(), mt.insertParameterTypes(0, MemorySegment.class));\n+        assertEquals(callingSequence.functionDesc(), fd.insertArgumentLayouts(0, ADDRESS));\n+\n+        checkArgumentBindings(callingSequence, new Binding[][]{\n+            { unboxAddress(), vmStore(TARGET_ADDRESS_STORAGE, long.class) },\n+            { copy(struct), unboxAddress(), vmStore(x10, long.class) },\n+            { copy(struct), unboxAddress(), vmStore(x11, long.class) },\n+            { vmStore(x12, int.class) },\n+            { vmStore(x13, int.class) },\n+            { vmStore(x14, int.class) },\n+            { vmStore(x15, int.class) },\n+            { vmStore(x16, int.class) },\n+            { vmStore(x17, int.class) },\n+            { copy(struct), unboxAddress(), vmStore(stackStorage(STACK_SLOT_SIZE, 0), long.class) },\n+            { vmStore(stackStorage(STACK_SLOT_SIZE, 8), int.class) }\n+        });\n+\n+        checkReturnBindings(callingSequence, new Binding[]{});\n+    }\n+\n+    @Test\n+    public void testVarArgsInRegs() {\n+        MethodType mt = MethodType.methodType(void.class, int.class, int.class, float.class);\n+        FunctionDescriptor fd = FunctionDescriptor.ofVoid(C_INT, C_INT, C_FLOAT);\n+        FunctionDescriptor fdExpected = FunctionDescriptor.ofVoid(ADDRESS, C_INT, C_INT, C_FLOAT);\n+        LinuxRISCV64CallArranger.Bindings bindings = LinuxRISCV64CallArranger.getBindings(mt, fd, false, LinkerOptions.forDowncall(fd, firstVariadicArg(1)));\n+\n+        assertFalse(bindings.isInMemoryReturn());\n+        CallingSequence callingSequence = bindings.callingSequence();\n+        assertEquals(callingSequence.callerMethodType(), mt.insertParameterTypes(0, MemorySegment.class));\n+        assertEquals(callingSequence.functionDesc(), fdExpected);\n+\n+        \/\/ This is identical to the non-variadic calling sequence\n+        checkArgumentBindings(callingSequence, new Binding[][]{\n+            { unboxAddress(), vmStore(TARGET_ADDRESS_STORAGE, long.class) },\n+            { vmStore(x10, int.class) },\n+            { vmStore(x11, int.class) },\n+            { vmStore(x12, float.class) }\n+        });\n+\n+        checkReturnBindings(callingSequence, new Binding[]{});\n+    }\n+\n+    @Test\n+    public void testVarArgsLong() {\n+        MethodType mt = MethodType.methodType(void.class, int.class, int.class, int.class, double.class,\n+            double.class, long.class, long.class, int.class,\n+            double.class, double.class, long.class);\n+        FunctionDescriptor fd = FunctionDescriptor.ofVoid(C_INT, C_INT, C_INT, C_DOUBLE, C_DOUBLE,\n+            C_LONG, C_LONG, C_INT, C_DOUBLE,\n+            C_DOUBLE, C_LONG);\n+        FunctionDescriptor fdExpected = FunctionDescriptor.ofVoid(ADDRESS, C_INT, C_INT, C_INT, C_DOUBLE,\n+            C_DOUBLE, C_LONG, C_LONG, C_INT,\n+            C_DOUBLE, C_DOUBLE, C_LONG);\n+        LinuxRISCV64CallArranger.Bindings bindings = LinuxRISCV64CallArranger.getBindings(mt, fd, false, LinkerOptions.forDowncall(fd, firstVariadicArg(1)));\n+\n+        assertFalse(bindings.isInMemoryReturn());\n+        CallingSequence callingSequence = bindings.callingSequence();\n+        assertEquals(callingSequence.callerMethodType(), mt.insertParameterTypes(0, MemorySegment.class));\n+        assertEquals(callingSequence.functionDesc(), fdExpected);\n+\n+        \/\/ This is identical to the non-variadic calling sequence\n+        checkArgumentBindings(callingSequence, new Binding[][]{\n+            { unboxAddress(), vmStore(TARGET_ADDRESS_STORAGE, long.class) },\n+            { vmStore(x10, int.class) },\n+            { vmStore(x11, int.class) },\n+            { vmStore(x12, int.class) },\n+            { vmStore(x13, double.class) },\n+            { vmStore(x14, double.class) },\n+            { vmStore(x15, long.class) },\n+            { vmStore(x16, long.class) },\n+            { vmStore(x17, int.class) },\n+            { vmStore(stackStorage(STACK_SLOT_SIZE, 0), double.class) },\n+            { vmStore(stackStorage(STACK_SLOT_SIZE, 8), double.class) },\n+            { vmStore(stackStorage(STACK_SLOT_SIZE, 16), long.class) }\n+        });\n+\n+        checkReturnBindings(callingSequence, new Binding[]{});\n+    }\n+\n+    @Test\n+    public void testReturnStruct1() {\n+        MemoryLayout struct = MemoryLayout.structLayout(C_LONG, C_LONG, C_FLOAT);\n+\n+        MethodType mt = MethodType.methodType(MemorySegment.class, int.class, int.class, float.class);\n+        FunctionDescriptor fd = FunctionDescriptor.of(struct, C_INT, C_INT, C_FLOAT);\n+        LinuxRISCV64CallArranger.Bindings bindings = LinuxRISCV64CallArranger.getBindings(mt, fd, false);\n+\n+        assertTrue(bindings.isInMemoryReturn());\n+        CallingSequence callingSequence = bindings.callingSequence();\n+        assertEquals(callingSequence.callerMethodType(),\n+            MethodType.methodType(void.class, MemorySegment.class, MemorySegment.class,\n+                int.class, int.class, float.class));\n+        assertEquals(callingSequence.functionDesc(),\n+            FunctionDescriptor.ofVoid(ADDRESS, C_POINTER, C_INT, C_INT, C_FLOAT));\n+\n+        checkArgumentBindings(callingSequence, new Binding[][]{\n+            { unboxAddress(), vmStore(TARGET_ADDRESS_STORAGE, long.class) },\n+            { unboxAddress(), vmStore(x10, long.class) },\n+            { vmStore(x11, int.class) },\n+            { vmStore(x12, int.class) },\n+            { vmStore(f10, float.class) }\n+        });\n+\n+        checkReturnBindings(callingSequence, new Binding[]{});\n+    }\n+\n+    @Test\n+    public void testReturnStruct2() {\n+        MemoryLayout struct = MemoryLayout.structLayout(C_LONG, C_LONG);\n+\n+        MethodType mt = MethodType.methodType(MemorySegment.class);\n+        FunctionDescriptor fd = FunctionDescriptor.of(struct);\n+        LinuxRISCV64CallArranger.Bindings bindings = LinuxRISCV64CallArranger.getBindings(mt, fd, false);\n+\n+        assertFalse(bindings.isInMemoryReturn());\n+        CallingSequence callingSequence = bindings.callingSequence();\n+        assertEquals(callingSequence.callerMethodType(), mt.insertParameterTypes(0, MemorySegment.class, MemorySegment.class));\n+        assertEquals(callingSequence.functionDesc(), fd.insertArgumentLayouts(0, ADDRESS, ADDRESS));\n+\n+        checkArgumentBindings(callingSequence, new Binding[][]{\n+            { unboxAddress(), vmStore(RETURN_BUFFER_STORAGE, long.class) },\n+            { unboxAddress(), vmStore(TARGET_ADDRESS_STORAGE, long.class) }\n+        });\n+\n+        checkReturnBindings(callingSequence, new Binding[]{\n+            allocate(struct),\n+            dup(),\n+            vmLoad(x10, long.class),\n+            bufferStore(0, long.class),\n+            dup(),\n+            vmLoad(x11, long.class),\n+            bufferStore(8, long.class)\n+        });\n+    }\n+}\n","filename":"test\/jdk\/java\/foreign\/callarranger\/TestRISCV64CallArranger.java","additions":543,"deletions":0,"binary":false,"changes":543,"status":"added"},{"patch":"@@ -28,1 +28,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n","filename":"test\/jdk\/java\/foreign\/capturecallstate\/TestCaptureCallState.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -28,1 +28,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n","filename":"test\/jdk\/java\/foreign\/dontrelease\/TestDontRelease.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -27,1 +27,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n","filename":"test\/jdk\/java\/foreign\/enablenativeaccess\/TestEnableNativeAccess.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -27,1 +27,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n","filename":"test\/jdk\/java\/foreign\/enablenativeaccess\/TestEnableNativeAccessDynamic.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -27,1 +27,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n","filename":"test\/jdk\/java\/foreign\/handles\/Driver.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -27,1 +27,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n","filename":"test\/jdk\/java\/foreign\/loaderLookup\/TestLoaderLookup.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -33,1 +33,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n","filename":"test\/jdk\/java\/foreign\/loaderLookup\/TestLoaderLookupJNI.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -28,1 +28,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n","filename":"test\/jdk\/java\/foreign\/normalize\/TestNormalize.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -28,1 +28,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n","filename":"test\/jdk\/java\/foreign\/passheapsegment\/TestPassHeapSegment.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -27,1 +27,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n@@ -45,1 +45,1 @@\n- * @requires (((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\")\n+ * @requires (((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\")\n@@ -64,1 +64,1 @@\n- * @requires (((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\")\n+ * @requires (((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\")\n","filename":"test\/jdk\/java\/foreign\/stackwalk\/TestAsyncStackWalk.java","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -27,1 +27,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n@@ -45,1 +45,1 @@\n- * @requires (((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\")\n+ * @requires (((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\")\n@@ -64,1 +64,1 @@\n- * @requires (((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\")\n+ * @requires (((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\")\n","filename":"test\/jdk\/java\/foreign\/stackwalk\/TestStackWalk.java","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -28,1 +28,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n","filename":"test\/jdk\/java\/foreign\/upcalldeopt\/TestUpcallDeopt.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -29,1 +29,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n@@ -39,0 +39,2 @@\n+ *          java.base\/jdk.internal.foreign.abi.riscv64\n+ *          java.base\/jdk.internal.foreign.abi.riscv64.linux\n@@ -47,0 +49,1 @@\n+import jdk.internal.foreign.abi.riscv64.linux.LinuxRISCV64Linker;\n@@ -137,0 +140,2 @@\n+    private static final Function<Consumer<VaList.Builder>, VaList> linuxRISCV64VaListFactory\n+            = actions -> LinuxRISCV64Linker.newVaList(actions, SegmentScope.auto());\n@@ -148,0 +153,2 @@\n+    private static final BiFunction<Consumer<VaList.Builder>, SegmentScope, VaList> linuxRISCV64VaListScopedFactory\n+            = LinuxRISCV64Linker::newVaList;\n@@ -163,0 +170,1 @@\n+                { linuxRISCV64VaListFactory, sumIntsJavaFact.apply(RISCV64.C_INT), RISCV64.C_INT },\n@@ -191,0 +199,1 @@\n+                { linuxRISCV64VaListFactory, sumDoublesJavaFact.apply(RISCV64.C_DOUBLE), RISCV64.C_DOUBLE },\n@@ -221,0 +230,1 @@\n+                { linuxRISCV64VaListFactory, getIntJavaFact.apply(RISCV64.C_POINTER), RISCV64.C_POINTER },\n@@ -276,0 +286,1 @@\n+                argsFact.apply(linuxRISCV64VaListFactory, RISCV64.C_INT, sumStructJavaFact),\n@@ -329,0 +340,1 @@\n+                argsFact.apply(linuxRISCV64VaListFactory, RISCV64.C_LONG_LONG, sumStructJavaFact),\n@@ -382,0 +394,1 @@\n+                argsFact.apply(linuxRISCV64VaListFactory, RISCV64.C_FLOAT, sumStructJavaFact),\n@@ -444,0 +457,1 @@\n+                argsFact.apply(linuxRISCV64VaListFactory, RISCV64.C_LONG_LONG, sumStructJavaFact),\n@@ -498,0 +512,1 @@\n+                { linuxRISCV64VaListFactory,  sumStackJavaFact.apply(RISCV64.C_LONG_LONG, RISCV64.C_DOUBLE), RISCV64.C_LONG_LONG, RISCV64.C_DOUBLE },\n@@ -552,0 +567,2 @@\n+                { LinuxRISCV64Linker.emptyVaList()         },\n+                { linuxRISCV64VaListFactory.apply(b -> {}) },\n@@ -567,0 +584,1 @@\n+                { linuxRISCV64VaListScopedFactory, sumIntsJavaFact.apply(RISCV64.C_INT), RISCV64.C_INT },\n@@ -615,0 +633,1 @@\n+                { linuxRISCV64VaListScopedFactory, RISCV64.C_INT },\n","filename":"test\/jdk\/java\/foreign\/valist\/VaListTest.java","additions":20,"deletions":1,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -27,1 +27,1 @@\n- * @requires os.arch==\"amd64\" | os.arch==\"x86_64\" | os.arch==\"aarch64\"\n+ * @requires os.arch==\"amd64\" | os.arch==\"x86_64\" | os.arch==\"aarch64\" | os.arch==\"riscv64\"\n","filename":"test\/jdk\/java\/foreign\/virtual\/TestVirtualCalls.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"}]}