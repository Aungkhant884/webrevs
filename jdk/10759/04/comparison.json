{"files":[{"patch":"@@ -483,0 +483,4 @@\n+  \/\/ Visits nodes for buckets in range [start_idx, stop_id) with FUNC.\n+  template <typename FUNC>\n+  static bool do_scan_for_range(FUNC& scan_f, size_t start_idx, size_t stop_idx, InternalTable *table);\n+\n@@ -532,0 +536,1 @@\n+  class ScanTask;\n","filename":"src\/hotspot\/share\/utilities\/concurrentHashTable.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -1153,1 +1153,21 @@\n-  for (size_t bucket_it = 0; bucket_it < table->_size; bucket_it++) {\n+  do_scan_for_range(scan_f, 0, table->_size, table);\n+\n+  \/\/ If there is a paused resize we also need to visit the already resized items.\n+  table = get_new_table();\n+  if (table == NULL) {\n+    return;\n+  }\n+  DEBUG_ONLY(if (table == POISON_PTR) { return; })\n+\n+  do_scan_for_range(scan_f, 0, table->_size, table);\n+}\n+\n+template <typename CONFIG, MEMFLAGS F>\n+template <typename FUNC>\n+inline bool ConcurrentHashTable<CONFIG, F>::\n+  do_scan_for_range(FUNC& scan_f, size_t start_idx, size_t stop_idx, InternalTable* table)\n+{\n+  assert(start_idx < stop_idx, \"Must be\");\n+  assert(stop_idx <= table->_size, \"Must be\");\n+\n+  for (size_t bucket_it = start_idx; bucket_it < stop_idx; ++bucket_it) {\n@@ -1161,1 +1181,1 @@\n-        return;\n+        return false;\n@@ -1167,13 +1187,1 @@\n-  \/\/ If there is a paused resize we also need to visit the already resized items.\n-  table = get_new_table();\n-  if (table == NULL) {\n-    return;\n-  }\n-  DEBUG_ONLY(if (table == POISON_PTR) { return; })\n-  for (size_t bucket_it = 0; bucket_it < table->_size; bucket_it++) {\n-    Bucket* bucket = table->get_bucket(bucket_it);\n-    assert(!bucket->is_locked(), \"Bucket must be unlocked.\");\n-    if (!visit_nodes(bucket, scan_f)) {\n-      return;\n-    }\n-  }\n+  return true;\n","filename":"src\/hotspot\/share\/utilities\/concurrentHashTable.inline.hpp","additions":23,"deletions":15,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -43,0 +43,41 @@\n+  class InternalTableClaimer {\n+    volatile size_t _next;\n+    size_t _limit;\n+    size_t _size;\n+\n+public:\n+    InternalTableClaimer() : _next(0), _limit(0), _size(0){ }\n+\n+    InternalTableClaimer(size_t claim_size, InternalTable* table) :\n+      InternalTableClaimer()\n+    {\n+      set(claim_size, table);\n+    }\n+\n+    void set(size_t claim_size, InternalTable* table) {\n+      assert(table != nullptr, \"precondition\");\n+      _limit = table->_size;\n+      _size  = MIN2(claim_size, _limit);\n+    }\n+\n+    bool claim(size_t* start, size_t* stop) {\n+      if (Atomic::load(&_next) < _limit) {\n+        size_t claimed = Atomic::fetch_and_add(&_next, _size);\n+        if (claimed < _limit) {\n+          *start = claimed;\n+          *stop  = MIN2(claimed + _size, _limit);\n+          return true;\n+        }\n+      }\n+      return false;\n+    }\n+\n+    bool have_work() {\n+      return _limit > 0;\n+    }\n+\n+    bool have_more_work() {\n+      return Atomic::load_acquire(&_next) >= _limit;\n+    }\n+  };\n+\n@@ -46,6 +87,2 @@\n-  \/\/ The table is split into ranges, every increment is one range.\n-  volatile size_t _next_to_claim;\n-  size_t _task_size_log2; \/\/ Number of buckets.\n-  size_t _stop_task;      \/\/ Last task\n-  size_t _size_log2;      \/\/ Table size.\n-  bool   _is_mt;\n+  InternalTableClaimer _table_claimer;\n+  bool _is_mt;\n@@ -54,2 +91,1 @@\n-    : _cht(cht), _next_to_claim(0), _task_size_log2(DEFAULT_TASK_SIZE_LOG2),\n-    _stop_task(0), _size_log2(0), _is_mt(is_mt) {}\n+    : _cht(cht), _table_claimer(DEFAULT_TASK_SIZE_LOG2, _cht->_table), _is_mt(is_mt) {}\n@@ -59,7 +95,1 @@\n-    size_t claimed = Atomic::fetch_and_add(&_next_to_claim, 1u);\n-    if (claimed >= _stop_task) {\n-      return false;\n-    }\n-    *start = claimed * (((size_t)1) << _task_size_log2);\n-    *stop  = ((*start) + (((size_t)1) << _task_size_log2));\n-    return true;\n+    return _table_claimer.claim(start, stop);\n@@ -71,5 +101,1 @@\n-    _size_log2 = _cht->_table->_log2_size;\n-    _task_size_log2 = MIN2(_task_size_log2, _size_log2);\n-    size_t tmp = _size_log2 > _task_size_log2 ?\n-                 _size_log2 - _task_size_log2 : 0;\n-    _stop_task = (((size_t)1) << tmp);\n+    _table_claimer.set(DEFAULT_TASK_SIZE_LOG2, _cht->_table);\n@@ -80,1 +106,1 @@\n-    return Atomic::load_acquire(&_next_to_claim) >= _stop_task;\n+    return _table_claimer.have_more_work();\n@@ -205,0 +231,59 @@\n+template <typename CONFIG, MEMFLAGS F>\n+class ConcurrentHashTable<CONFIG, F>::ScanTask :\n+  public BucketsOperation\n+{\n+  \/\/ If there is a paused resize, we need to scan items already\n+  \/\/ moved to the new resized table.\n+  typename BucketsOperation::InternalTableClaimer _new_table_claimer;\n+\n+  \/\/ Returns true if you succeeded to claim the range [start, stop).\n+  bool claim(size_t* start, size_t* stop, InternalTable** table) {\n+    if (this->_table_claimer.claim(start, stop)) {\n+      *table = this->_cht->get_table();\n+      return true;\n+    }\n+\n+    \/\/ If there is a paused resize, we also need to operate on the already resized items.\n+    if (!_new_table_claimer.have_work()) {\n+      assert(this->_cht->get_new_table() == nullptr || this->_cht->get_new_table() == POISON_PTR, \"Precondition\");\n+      return false;\n+    }\n+\n+    *table = this->_cht->get_new_table();\n+    return _new_table_claimer.claim(start, stop);\n+  }\n+\n+ public:\n+  ScanTask(ConcurrentHashTable<CONFIG, F>* cht, size_t claim_size) : BucketsOperation(cht), _new_table_claimer() {\n+    set(cht, claim_size);\n+  }\n+\n+  void set(ConcurrentHashTable<CONFIG, F>* cht, size_t claim_size) {\n+    this->_table_claimer.set(claim_size, cht->get_table());\n+\n+    InternalTable* new_table = cht->get_new_table();\n+    if (new_table == nullptr) { return; }\n+\n+    DEBUG_ONLY(if (new_table == POISON_PTR) { return; })\n+\n+    _new_table_claimer.set(claim_size, new_table);\n+  }\n+\n+  template <typename SCAN_FUNC>\n+  void  do_safepoint_scan(SCAN_FUNC& scan_f) {\n+    assert(SafepointSynchronize::is_at_safepoint(),\n+           \"must only be called in a safepoint\");\n+\n+    size_t start_idx = 0, stop_idx = 0;\n+    InternalTable* table = nullptr;\n+\n+    while (claim(&start_idx, &stop_idx, &table)) {\n+      assert(table != nullptr, \"precondition\");\n+      if (!this->_cht->do_scan_for_range(scan_f, start_idx, stop_idx, table)) {\n+        return;\n+      }\n+      table = nullptr;\n+    }\n+  }\n+};\n+\n","filename":"src\/hotspot\/share\/utilities\/concurrentHashTableTasks.inline.hpp","additions":107,"deletions":22,"binary":false,"changes":129,"status":"modified"},{"patch":"@@ -25,0 +25,1 @@\n+#include \"gc\/shared\/workerThread.hpp\"\n@@ -1148,0 +1149,76 @@\n+\n+class CHTParallelScanTask: public WorkerTask {\n+  TestTable* _cht;\n+  TestTable::ScanTask* _scan_task;\n+  size_t *_total_scanned;\n+\n+public:\n+  CHTParallelScanTask(TestTable* cht,\n+                      TestTable::ScanTask* bc,\n+                      size_t *total_scanned) :\n+    WorkerTask(\"CHT Parallel Scan\"),\n+    _cht(cht),\n+    _scan_task(bc),\n+    _total_scanned(total_scanned)\n+  { }\n+\n+  void work(uint worker_id) {\n+    ChtCountScan par_scan;\n+    _scan_task->do_safepoint_scan(par_scan);\n+    Atomic::add(_total_scanned, par_scan._count);\n+  }\n+};\n+\n+class CHTWorkers : AllStatic {\n+  static WorkerThreads* _workers;\n+  static WorkerThreads* workers() {\n+    if (_workers == nullptr) {\n+      _workers = new WorkerThreads(\"CHT Workers\", MaxWorkers);\n+      _workers->initialize_workers();\n+      _workers->set_active_workers(MaxWorkers);\n+    }\n+    return _workers;\n+  }\n+\n+public:\n+  static const uint MaxWorkers = 8;\n+  static void run_task(WorkerTask* task) {\n+    workers()->run_task(task);\n+  }\n+};\n+\n+WorkerThreads* CHTWorkers::_workers = nullptr;\n+\n+class CHTParallelScan: public VM_GTestExecuteAtSafepoint {\n+  TestTable* _cht;\n+  uintptr_t _num_items;\n+public:\n+  CHTParallelScan(TestTable* cht, uintptr_t num_items) :\n+    _cht(cht), _num_items(num_items)\n+  {}\n+\n+  void doit() {\n+    size_t total_scanned = 0;\n+    TestTable::ScanTask scan_task(_cht, 64);\n+\n+    CHTParallelScanTask task(_cht, &scan_task, &total_scanned);\n+    CHTWorkers::run_task(&task);\n+\n+     EXPECT_TRUE(total_scanned == (size_t)_num_items) << \" Should scan all inserted items: \" << total_scanned;\n+  }\n+};\n+\n+TEST_VM(ConcurrentHashTable, concurrent_par_scan) {\n+  TestTable* cht = new TestTable(16, 16, 2);\n+\n+  uintptr_t num_items = 999999;\n+  for (uintptr_t v = 1; v <= num_items; v++ ) {\n+    TestLookup tl(v);\n+    EXPECT_TRUE(cht->insert(JavaThread::current(), tl, v)) << \"Inserting an unique value should work.\";\n+  }\n+\n+  \/\/ Run the test at a safepoint.\n+  CHTParallelScan op(cht, num_items);\n+  ThreadInVMfromNative invm(JavaThread::current());\n+  VMThread::execute(&op);\n+}\n","filename":"test\/hotspot\/gtest\/utilities\/test_concurrentHashtable.cpp","additions":77,"deletions":0,"binary":false,"changes":77,"status":"modified"}]}