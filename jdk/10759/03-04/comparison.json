{"files":[{"patch":"@@ -349,4 +349,0 @@\n-  \/\/ Visits nodes for buckets in range [start_idx, stop_id) with FUNC.\n-  template <typename FUNC>\n-  static bool do_scan_for_range(FUNC& scan_f, size_t start_idx, size_t stop_idx, InternalTable *table);\n-\n@@ -487,0 +483,3 @@\n+  \/\/ Visits nodes for buckets in range [start_idx, stop_id) with FUNC.\n+  template <typename FUNC>\n+  static bool do_scan_for_range(FUNC& scan_f, size_t start_idx, size_t stop_idx, InternalTable *table);\n@@ -494,6 +493,0 @@\n-  class BucketsClaimer;\n-  \/\/ Visit all items with SCAN_FUNC without any protection.\n-  \/\/ Thread-safe, but must be called at safepoint.\n-  template <typename SCAN_FUNC>\n-  void do_safepoint_scan(SCAN_FUNC& scan_f, BucketsClaimer* bucket_claimer);\n-\n@@ -543,0 +536,1 @@\n+  class ScanTask;\n","filename":"src\/hotspot\/share\/utilities\/concurrentHashTable.hpp","additions":4,"deletions":10,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -1153,1 +1153,21 @@\n-  for (size_t bucket_it = 0; bucket_it < table->_size; bucket_it++) {\n+  do_scan_for_range(scan_f, 0, table->_size, table);\n+\n+  \/\/ If there is a paused resize we also need to visit the already resized items.\n+  table = get_new_table();\n+  if (table == NULL) {\n+    return;\n+  }\n+  DEBUG_ONLY(if (table == POISON_PTR) { return; })\n+\n+  do_scan_for_range(scan_f, 0, table->_size, table);\n+}\n+\n+template <typename CONFIG, MEMFLAGS F>\n+template <typename FUNC>\n+inline bool ConcurrentHashTable<CONFIG, F>::\n+  do_scan_for_range(FUNC& scan_f, size_t start_idx, size_t stop_idx, InternalTable* table)\n+{\n+  assert(start_idx < stop_idx, \"Must be\");\n+  assert(stop_idx <= table->_size, \"Must be\");\n+\n+  for (size_t bucket_it = start_idx; bucket_it < stop_idx; ++bucket_it) {\n@@ -1161,1 +1181,1 @@\n-        return;\n+        return false;\n@@ -1167,13 +1187,1 @@\n-  \/\/ If there is a paused resize we also need to visit the already resized items.\n-  table = get_new_table();\n-  if (table == NULL) {\n-    return;\n-  }\n-  DEBUG_ONLY(if (table == POISON_PTR) { return; })\n-  for (size_t bucket_it = 0; bucket_it < table->_size; bucket_it++) {\n-    Bucket* bucket = table->get_bucket(bucket_it);\n-    assert(!bucket->is_locked(), \"Bucket must be unlocked.\");\n-    if (!visit_nodes(bucket, scan_f)) {\n-      return;\n-    }\n-  }\n+  return true;\n@@ -1301,114 +1309,0 @@\n-template <typename CONFIG, MEMFLAGS F>\n-class ConcurrentHashTable<CONFIG, F>::BucketsClaimer {\n-  using InternalTable = ConcurrentHashTable<CONFIG, F>::InternalTable;\n-\n-  ConcurrentHashTable<CONFIG, F>* _cht;\n-\n-  struct InternalTableClaimer {\n-    volatile size_t _next;\n-    size_t _limit;\n-    size_t _size;\n-    InternalTable* _table;\n-\n-    InternalTableClaimer() : _next(0), _limit(0), _size(0), _table(nullptr) { }\n-\n-    InternalTableClaimer(size_t claim_size, InternalTable* table) :\n-      InternalTableClaimer()\n-    {\n-      set(claim_size, table);\n-    }\n-\n-    void set(size_t claim_size, InternalTable* table) {\n-      assert(table != nullptr, \"precondition\");\n-      _limit = table->_size;\n-      _size  = MIN2(claim_size, _limit);\n-      _table = table;\n-    }\n-\n-    bool claim(size_t* start, size_t* stop, InternalTable** table) {\n-      if (Atomic::load(&_next) < _limit) {\n-        size_t claimed = Atomic::fetch_and_add(&_next, _size);\n-        if (claimed < _limit) {\n-          *start = claimed;\n-          *stop  = MIN2(claimed + _size, _limit);\n-          *table = _table;\n-          return true;\n-        }\n-      }\n-      return false;\n-    }\n-  };\n-\n-  InternalTableClaimer _table_claimer;\n-  \/\/ If there is a paused resize, we need to claim items already\n-  \/\/ moved to the new resized table.\n-  InternalTableClaimer _new_table_claimer;\n-public:\n-  BucketsClaimer(ConcurrentHashTable<CONFIG, F>* cht, size_t claim_size) :\n-    _cht(cht),\n-    _table_claimer(claim_size, _cht->_table),\n-    _new_table_claimer()\n-  {\n-    InternalTable* new_table = _cht->get_new_table();\n-\n-    if (new_table == nullptr) { return; }\n-\n-    DEBUG_ONLY(if (new_table == POISON_PTR) { return; })\n-\n-    _new_table_claimer.set(claim_size, new_table);\n-  }\n-\n-  \/\/ Returns true if you succeeded to claim the range [start, stop).\n-  bool claim(size_t* start, size_t* stop, InternalTable** table) {\n-    if (_table_claimer.claim(start, stop, table)) {\n-      return true;\n-    }\n-\n-    \/\/ If there is a paused resize, we also need to operate on the already resized items.\n-    if (_new_table_claimer._limit == 0) {\n-      assert(_cht->get_new_table() == nullptr || _cht->get_new_table() == POISON_PTR, \"Precondition\");\n-      return false;\n-    }\n-    return _new_table_claimer.claim(start, stop, table);\n-  }\n-};\n-\n-template <typename CONFIG, MEMFLAGS F>\n-template <typename SCAN_FUNC>\n-inline void ConcurrentHashTable<CONFIG, F>::\n-  do_safepoint_scan(SCAN_FUNC& scan_f, BucketsClaimer* bucket_claimer)\n-{\n-  assert(SafepointSynchronize::is_at_safepoint(),\n-         \"must only be called in a safepoint\");\n-  size_t start_idx = 0, stop_idx = 0;\n-  InternalTable* table = nullptr;\n-  while (bucket_claimer->claim(&start_idx, &stop_idx, &table)) {\n-    assert(table != nullptr, \"precondition\");\n-    if (!do_scan_for_range(scan_f, start_idx, stop_idx, table)) {\n-      return;\n-    }\n-    table = nullptr;\n-  }\n-}\n-\n-template <typename CONFIG, MEMFLAGS F>\n-template <typename FUNC>\n-inline bool ConcurrentHashTable<CONFIG, F>::\n-  do_scan_for_range(FUNC& scan_f, size_t start_idx, size_t stop_idx, InternalTable* table)\n-{\n-  assert(start_idx < stop_idx, \"Must be\");\n-  assert(stop_idx <= table->_size, \"Must be\");\n-  for (size_t bucket_it = start_idx; bucket_it < stop_idx; ++bucket_it) {\n-    Bucket* bucket = table->get_bucket(bucket_it);\n-    \/\/ If bucket has a redirect, the items will be in the new table.\n-    if (!bucket->have_redirect()) {\n-      if(!visit_nodes(bucket, scan_f)) {\n-        return false;\n-      }\n-    } else {\n-      assert(bucket->is_locked(), \"Bucket must be locked.\");\n-    }\n-  }\n-  return true;\n-}\n-\n","filename":"src\/hotspot\/share\/utilities\/concurrentHashTable.inline.hpp","additions":23,"deletions":129,"binary":false,"changes":152,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -43,0 +43,41 @@\n+  class InternalTableClaimer {\n+    volatile size_t _next;\n+    size_t _limit;\n+    size_t _size;\n+\n+public:\n+    InternalTableClaimer() : _next(0), _limit(0), _size(0){ }\n+\n+    InternalTableClaimer(size_t claim_size, InternalTable* table) :\n+      InternalTableClaimer()\n+    {\n+      set(claim_size, table);\n+    }\n+\n+    void set(size_t claim_size, InternalTable* table) {\n+      assert(table != nullptr, \"precondition\");\n+      _limit = table->_size;\n+      _size  = MIN2(claim_size, _limit);\n+    }\n+\n+    bool claim(size_t* start, size_t* stop) {\n+      if (Atomic::load(&_next) < _limit) {\n+        size_t claimed = Atomic::fetch_and_add(&_next, _size);\n+        if (claimed < _limit) {\n+          *start = claimed;\n+          *stop  = MIN2(claimed + _size, _limit);\n+          return true;\n+        }\n+      }\n+      return false;\n+    }\n+\n+    bool have_work() {\n+      return _limit > 0;\n+    }\n+\n+    bool have_more_work() {\n+      return Atomic::load_acquire(&_next) >= _limit;\n+    }\n+  };\n+\n@@ -46,6 +87,2 @@\n-  \/\/ The table is split into ranges, every increment is one range.\n-  volatile size_t _next_to_claim;\n-  size_t _task_size_log2; \/\/ Number of buckets.\n-  size_t _stop_task;      \/\/ Last task\n-  size_t _size_log2;      \/\/ Table size.\n-  bool   _is_mt;\n+  InternalTableClaimer _table_claimer;\n+  bool _is_mt;\n@@ -54,2 +91,1 @@\n-    : _cht(cht), _next_to_claim(0), _task_size_log2(DEFAULT_TASK_SIZE_LOG2),\n-    _stop_task(0), _size_log2(0), _is_mt(is_mt) {}\n+    : _cht(cht), _table_claimer(DEFAULT_TASK_SIZE_LOG2, _cht->_table), _is_mt(is_mt) {}\n@@ -59,7 +95,1 @@\n-    size_t claimed = Atomic::fetch_and_add(&_next_to_claim, 1u);\n-    if (claimed >= _stop_task) {\n-      return false;\n-    }\n-    *start = claimed * (((size_t)1) << _task_size_log2);\n-    *stop  = ((*start) + (((size_t)1) << _task_size_log2));\n-    return true;\n+    return _table_claimer.claim(start, stop);\n@@ -71,5 +101,1 @@\n-    _size_log2 = _cht->_table->_log2_size;\n-    _task_size_log2 = MIN2(_task_size_log2, _size_log2);\n-    size_t tmp = _size_log2 > _task_size_log2 ?\n-                 _size_log2 - _task_size_log2 : 0;\n-    _stop_task = (((size_t)1) << tmp);\n+    _table_claimer.set(DEFAULT_TASK_SIZE_LOG2, _cht->_table);\n@@ -80,1 +106,1 @@\n-    return Atomic::load_acquire(&_next_to_claim) >= _stop_task;\n+    return _table_claimer.have_more_work();\n@@ -205,0 +231,59 @@\n+template <typename CONFIG, MEMFLAGS F>\n+class ConcurrentHashTable<CONFIG, F>::ScanTask :\n+  public BucketsOperation\n+{\n+  \/\/ If there is a paused resize, we need to scan items already\n+  \/\/ moved to the new resized table.\n+  typename BucketsOperation::InternalTableClaimer _new_table_claimer;\n+\n+  \/\/ Returns true if you succeeded to claim the range [start, stop).\n+  bool claim(size_t* start, size_t* stop, InternalTable** table) {\n+    if (this->_table_claimer.claim(start, stop)) {\n+      *table = this->_cht->get_table();\n+      return true;\n+    }\n+\n+    \/\/ If there is a paused resize, we also need to operate on the already resized items.\n+    if (!_new_table_claimer.have_work()) {\n+      assert(this->_cht->get_new_table() == nullptr || this->_cht->get_new_table() == POISON_PTR, \"Precondition\");\n+      return false;\n+    }\n+\n+    *table = this->_cht->get_new_table();\n+    return _new_table_claimer.claim(start, stop);\n+  }\n+\n+ public:\n+  ScanTask(ConcurrentHashTable<CONFIG, F>* cht, size_t claim_size) : BucketsOperation(cht), _new_table_claimer() {\n+    set(cht, claim_size);\n+  }\n+\n+  void set(ConcurrentHashTable<CONFIG, F>* cht, size_t claim_size) {\n+    this->_table_claimer.set(claim_size, cht->get_table());\n+\n+    InternalTable* new_table = cht->get_new_table();\n+    if (new_table == nullptr) { return; }\n+\n+    DEBUG_ONLY(if (new_table == POISON_PTR) { return; })\n+\n+    _new_table_claimer.set(claim_size, new_table);\n+  }\n+\n+  template <typename SCAN_FUNC>\n+  void  do_safepoint_scan(SCAN_FUNC& scan_f) {\n+    assert(SafepointSynchronize::is_at_safepoint(),\n+           \"must only be called in a safepoint\");\n+\n+    size_t start_idx = 0, stop_idx = 0;\n+    InternalTable* table = nullptr;\n+\n+    while (claim(&start_idx, &stop_idx, &table)) {\n+      assert(table != nullptr, \"precondition\");\n+      if (!this->_cht->do_scan_for_range(scan_f, start_idx, stop_idx, table)) {\n+        return;\n+      }\n+      table = nullptr;\n+    }\n+  }\n+};\n+\n","filename":"src\/hotspot\/share\/utilities\/concurrentHashTableTasks.inline.hpp","additions":107,"deletions":22,"binary":false,"changes":129,"status":"modified"},{"patch":"@@ -1152,1 +1152,1 @@\n-  TestTable::BucketsClaimer* _bucket_claimer;\n+  TestTable::ScanTask* _scan_task;\n@@ -1157,1 +1157,1 @@\n-                      TestTable::BucketsClaimer* bc,\n+                      TestTable::ScanTask* bc,\n@@ -1161,1 +1161,1 @@\n-    _bucket_claimer(bc),\n+    _scan_task(bc),\n@@ -1167,1 +1167,1 @@\n-    _cht->do_safepoint_scan(par_scan, _bucket_claimer);\n+    _scan_task->do_safepoint_scan(par_scan);\n@@ -1202,1 +1202,1 @@\n-    TestTable::BucketsClaimer bucket_claimer(_cht, 64);\n+    TestTable::ScanTask scan_task(_cht, 64);\n@@ -1204,1 +1204,1 @@\n-    CHTParallelScanTask task(_cht, &bucket_claimer, &total_scanned);\n+    CHTParallelScanTask task(_cht, &scan_task, &total_scanned);\n","filename":"test\/hotspot\/gtest\/utilities\/test_concurrentHashtable.cpp","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"}]}