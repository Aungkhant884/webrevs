{"files":[{"patch":"@@ -981,1 +981,0 @@\n-\n@@ -1183,40 +1182,0 @@\n-template <typename CONFIG, MEMFLAGS F>\n-template <typename FUNC>\n-inline bool ConcurrentHashTable<CONFIG, F>::\n-  do_scan_for_range(FUNC& scan_f, size_t start_idx, size_t stop_idx, InternalTable* table)\n-{\n-  assert(start_idx < stop_idx, \"Must be\");\n-  assert(stop_idx <= table->_size, \"Must be\");\n-\n-  for (size_t bucket_it = start_idx; bucket_it < stop_idx; ++bucket_it) {\n-    Bucket* bucket = table->get_bucket(bucket_it);\n-    \/\/ If bucket has a redirect, the items will be in the new table.\n-    if (!bucket->have_redirect()) {\n-      if(!visit_nodes(bucket, scan_f)) {\n-        return false;\n-      }\n-    } else {\n-      assert(bucket->is_locked(), \"Bucket must be locked.\");\n-    }\n-  }\n-  return true;\n-}\n-\n-template <typename CONFIG, MEMFLAGS F>\n-template <typename SCAN_FUNC>\n-inline void ConcurrentHashTable<CONFIG, F>::\n-  do_safepoint_scan(SCAN_FUNC& scan_f, BucketsClaimer* bucket_claimer)\n-{\n-  assert(SafepointSynchronize::is_at_safepoint(),\n-         \"must only be called in a safepoint\");\n-  size_t start_idx = 0, stop_idx = 0;\n-  InternalTable* table = nullptr;\n-  while (bucket_claimer->claim(&start_idx, &stop_idx, &table)) {\n-    assert(table != nullptr, \"precondition\");\n-    if (!do_scan_for_range(scan_f, start_idx, stop_idx, table)) {\n-      return;\n-    }\n-    table = nullptr;\n-  }\n-}\n-\n@@ -1344,2 +1303,1 @@\n-  \/\/ Default size of _claim_size_log2\n-  static const size_t DEFAULT_CLAIM_SIZE_LOG2 = 7;\n+  using InternalTable = ConcurrentHashTable<CONFIG, F>::InternalTable;\n@@ -1348,4 +1306,0 @@\n-  \/\/ The table is split into ranges, every increment is one range.\n-  volatile size_t _next_to_claim;\n-  size_t _claim_size_log2; \/\/ Log number of buckets in claimed range.\n-  size_t _limit;      \/\/ Limit to number of claims\n@@ -1353,4 +1307,21 @@\n-  \/\/ If there is a paused resize, we also need to operate on the already resized items.\n-  volatile size_t _next_to_claim_new_table;\n-  size_t _claim_size_log2_new_table;\n-  size_t _limit_new_table;\n+  struct InternalTableClaimer {\n+    volatile size_t _next;\n+    size_t _limit;\n+    size_t _size;\n+    InternalTable* _table;\n+\n+    InternalTableClaimer() : _next(0), _limit(0), _size(0), _table(nullptr) { }\n+\n+    InternalTableClaimer(size_t claim_size, InternalTable* table) :\n+      InternalTableClaimer()\n+    {\n+      set(claim_size, table);\n+    }\n+\n+    void set(size_t claim_size, InternalTable* table) {\n+      assert(table != nullptr, \"precondition\");\n+      _limit = table->_size;\n+      _size  = MIN2(claim_size, _limit);\n+      _table = table;\n+    }\n+  };\n@@ -1358,0 +1329,3 @@\n+  InternalTableClaimer _claimer;\n+  \/\/ If there is a paused resize, we also need to operate on the already resized items.\n+  InternalTableClaimer _new_table_claimer;\n@@ -1359,1 +1333,1 @@\n-  BucketsClaimer(ConcurrentHashTable<CONFIG, F>* cht) :\n+  BucketsClaimer(ConcurrentHashTable<CONFIG, F>* cht, size_t claim_size) :\n@@ -1361,6 +1335,2 @@\n-    _next_to_claim(0),\n-    _claim_size_log2(DEFAULT_CLAIM_SIZE_LOG2),\n-    _limit(0),\n-    _next_to_claim_new_table(0),\n-    _claim_size_log2_new_table(0),\n-    _limit_new_table(0)\n+    _claimer(claim_size, _cht->_table),\n+    _new_table_claimer()\n@@ -1368,5 +1338,1 @@\n-    size_t size_log2 = _cht->_table->_log2_size;\n-    _claim_size_log2 = MIN2(_claim_size_log2, size_log2);\n-    _limit = (size_t)1 << (size_log2 - _claim_size_log2);\n-\n-    ConcurrentHashTable<CONFIG, F>::InternalTable* new_table = _cht->get_new_table();\n+    InternalTable* new_table = _cht->get_new_table();\n@@ -1378,3 +1344,1 @@\n-    size_t size_log2_new_table = new_table->_log2_size;\n-    _claim_size_log2_new_table = MIN2(DEFAULT_CLAIM_SIZE_LOG2, size_log2_new_table);\n-    _limit_new_table = (size_t)1 << (size_log2_new_table - _claim_size_log2_new_table);\n+    _new_table_claimer.set(claim_size, new_table);\n@@ -1383,8 +1347,7 @@\n-  \/\/ Returns true if you succeeded to claim the range [start, stop).\n-  bool claim(size_t* start, size_t* stop, ConcurrentHashTable<CONFIG, F>::InternalTable** table) {\n-    if (Atomic::load(&_next_to_claim) < _limit) {\n-      size_t claimed = Atomic::fetch_and_add(&_next_to_claim, 1u);\n-      if (claimed < _limit) {\n-        *start = claimed << _claim_size_log2;\n-        *stop  = (*start) + ((size_t)1 << _claim_size_log2);\n-        *table = _cht->get_table();\n+  bool claim(InternalTableClaimer* claimer, size_t* start, size_t* stop, InternalTable** table) {\n+    if (Atomic::load(&claimer->_next) < claimer->_limit) {\n+      size_t claimed = Atomic::fetch_and_add(&claimer->_next, claimer->_size);\n+      if (claimed < claimer->_limit) {\n+        *start = claimed;\n+        *stop  = MIN2(claimed + claimer->_size, claimer->_limit);\n+        *table = claimer->_table;\n@@ -1394,0 +1357,8 @@\n+    return false;\n+  }\n+\n+  \/\/ Returns true if you succeeded to claim the range [start, stop).\n+  bool claim(size_t* start, size_t* stop, InternalTable** table) {\n+    if (claim(&_claimer, start, stop, table)) {\n+      return true;\n+    }\n@@ -1395,1 +1366,2 @@\n-    if (_limit_new_table == 0) {\n+    \/\/ If there is a paused resize, we also need to operate on the already resized items.\n+    if (_new_table_claimer._limit == 0) {\n@@ -1400,10 +1372,1 @@\n-    ConcurrentHashTable<CONFIG, F>::InternalTable* new_table = _cht->get_new_table();\n-    assert(new_table != nullptr, \"Precondition\");\n-    size_t claimed = Atomic::fetch_and_add(&_next_to_claim_new_table, 1u);\n-    if (claimed < _limit_new_table) {\n-      *start = claimed << _claim_size_log2_new_table;\n-      *stop  = (*start) + ((size_t)1 << _claim_size_log2_new_table);\n-      *table = new_table;\n-      return true;\n-    }\n-    return false;\n+    return claim(&_new_table_claimer, start, stop, table);\n@@ -1413,0 +1376,39 @@\n+template <typename CONFIG, MEMFLAGS F>\n+template <typename SCAN_FUNC>\n+inline void ConcurrentHashTable<CONFIG, F>::\n+  do_safepoint_scan(SCAN_FUNC& scan_f, BucketsClaimer* bucket_claimer)\n+{\n+  assert(SafepointSynchronize::is_at_safepoint(),\n+         \"must only be called in a safepoint\");\n+  size_t start_idx = 0, stop_idx = 0;\n+  InternalTable* table = nullptr;\n+  while (bucket_claimer->claim(&start_idx, &stop_idx, &table)) {\n+    assert(table != nullptr, \"precondition\");\n+    if (!do_scan_for_range(scan_f, start_idx, stop_idx, table)) {\n+      return;\n+    }\n+    table = nullptr;\n+  }\n+}\n+\n+template <typename CONFIG, MEMFLAGS F>\n+template <typename FUNC>\n+inline bool ConcurrentHashTable<CONFIG, F>::\n+  do_scan_for_range(FUNC& scan_f, size_t start_idx, size_t stop_idx, InternalTable* table)\n+{\n+  assert(start_idx < stop_idx, \"Must be\");\n+  assert(stop_idx <= table->_size, \"Must be\");\n+  for (size_t bucket_it = start_idx; bucket_it < stop_idx; ++bucket_it) {\n+    Bucket* bucket = table->get_bucket(bucket_it);\n+    \/\/ If bucket has a redirect, the items will be in the new table.\n+    if (!bucket->have_redirect()) {\n+      if(!visit_nodes(bucket, scan_f)) {\n+        return false;\n+      }\n+    } else {\n+      assert(bucket->is_locked(), \"Bucket must be locked.\");\n+    }\n+  }\n+  return true;\n+}\n+\n","filename":"src\/hotspot\/share\/utilities\/concurrentHashTable.inline.hpp","additions":87,"deletions":85,"binary":false,"changes":172,"status":"modified"},{"patch":"@@ -1202,1 +1202,1 @@\n-    TestTable::BucketsClaimer bucket_claimer(_cht);\n+    TestTable::BucketsClaimer bucket_claimer(_cht, 64);\n","filename":"test\/hotspot\/gtest\/utilities\/test_concurrentHashtable.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"}]}