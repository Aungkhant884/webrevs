{"files":[{"patch":"@@ -0,0 +1,96 @@\n+\/\/ Copyright (c) 2021, Red Hat Inc. All rights reserved.\n+\/\/ DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+\n+\/\/ This code is free software; you can redistribute it and\/or modify it\n+\/\/ under the terms of the GNU General Public License version 2 only, as\n+\/\/ published by the Free Software Foundation.\n+\n+\/\/ This code is distributed in the hope that it will be useful, but WITHOUT\n+\/\/ ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+\/\/ FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+\/\/ version 2 for more details (a copy is included in the LICENSE file that\n+\/\/ accompanied this code).\n+\n+\/\/ You should have received a copy of the GNU General Public License version\n+\/\/ 2 along with this work; if not, write to the Free Software Foundation,\n+\/\/ Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+\n+\/\/ Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+\/\/ or visit www.oracle.com if you need additional information or have any\n+\/\/ questions.\n+\n+\n+\n+        .text\n+\n+        .globl aarch64_atomic_fetch_add_8_default_impl\n+        .align 5\n+aarch64_atomic_fetch_add_8_default_impl:\n+0:      ldaxr   x2, [x0]\n+        add     x8, x2, x1\n+        stlxr   w9, x8, [x0]\n+        cbnz    w9, 0b\n+        mov     x0, x2\n+        ret\n+\n+        .globl aarch64_atomic_fetch_add_4_default_impl\n+        .align 5\n+aarch64_atomic_fetch_add_4_default_impl:\n+0:      ldaxr   w2, [x0]\n+        add     w8, w2, w1\n+        stlxr   w9, w8, [x0]\n+        cbnz    w9, 0b\n+        mov     w0, w2\n+        ret\n+\n+        .globl aarch64_atomic_xchg_4_default_impl\n+        .align 5\n+aarch64_atomic_xchg_4_default_impl:\n+0:      ldaxr   w2, [x0]\n+        stlxr   w8, w1, [x0]\n+        cbnz    w8, 0b\n+        mov     w0, w2\n+        ret\n+\n+        .globl aarch64_atomic_xchg_8_default_impl\n+        .align 5\n+aarch64_atomic_xchg_8_default_impl:\n+0:      ldaxr   x2, [x0]\n+        stlxr   w8, x1, [x0]\n+        cbnz    w8, 0b\n+        mov     x0, x2\n+        ret\n+\n+        .globl aarch64_atomic_cmpxchg_1_default_impl\n+        .align 5\n+aarch64_atomic_cmpxchg_1_default_impl:\n+0:      ldxrb   w3, [x0]\n+        eor     w8, w3, w1\n+        tst     x8, #0xff\n+        b.ne    1f\n+        stxrb   w8, w2, [x0]\n+        cbnz    w8, 0b\n+1:      mov     w0, w3\n+        ret\n+\n+        .globl aarch64_atomic_cmpxchg_4_default_impl\n+        .align 5\n+aarch64_atomic_cmpxchg_4_default_impl:\n+0:      ldxr    w3, [x0]\n+        cmp     w3, w1\n+        b.ne    1f\n+        stxr    w8, w2, [x0]\n+        cbnz    w8, 0b\n+1:      mov     w0, w3\n+        ret\n+\n+        .globl aarch64_atomic_cmpxchg_8_default_impl\n+        .align 5\n+aarch64_atomic_cmpxchg_8_default_impl:\n+0:      ldxr    x3, [x0]\n+        cmp     x3, x1\n+        b.ne    1f\n+        stxr    w8, x2, [x0]\n+        cbnz    w8, 0b\n+1:      mov     x0, x3\n+        ret\n","filename":"src\/hotspot\/cpu\/aarch64\/atomic_aarch64.S","additions":96,"deletions":0,"binary":false,"changes":96,"status":"added"},{"patch":"@@ -0,0 +1,46 @@\n+\/* Copyright (c) 2021, Red Hat Inc. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef CPU_AARCH64_ATOMIC_AARCH64_HPP\n+#define CPU_AARCH64_ATOMIC_AARCH64_HPP\n+\n+\/\/ Atomic stub implementation.\n+\/\/ Default implementations are in atomic_aarch64.S\n+\/\/\n+\/\/ All stubs pass arguments the same way\n+\/\/ x0: src\/dest address\n+\/\/ x1: arg1\n+\/\/ x2: arg2 (optional)\n+\/\/ x3, x8, x9: scratch\n+typedef uint64_t (*aarch64_atomic_stub_t)(volatile void *ptr, uint64_t arg1, uint64_t arg2);\n+\n+\/\/ Pointers to stubs\n+extern aarch64_atomic_stub_t aarch64_atomic_fetch_add_4_impl;\n+extern aarch64_atomic_stub_t aarch64_atomic_fetch_add_8_impl;\n+extern aarch64_atomic_stub_t aarch64_atomic_xchg_4_impl;\n+extern aarch64_atomic_stub_t aarch64_atomic_xchg_8_impl;\n+extern aarch64_atomic_stub_t aarch64_atomic_cmpxchg_1_impl;\n+extern aarch64_atomic_stub_t aarch64_atomic_cmpxchg_4_impl;\n+extern aarch64_atomic_stub_t aarch64_atomic_cmpxchg_8_impl;\n+\n+#endif \/\/ CPU_AARCH64_ATOMIC_AARCH64_HPP\n","filename":"src\/hotspot\/cpu\/aarch64\/atomic_aarch64.hpp","additions":46,"deletions":0,"binary":false,"changes":46,"status":"added"},{"patch":"@@ -2569,0 +2569,2 @@\n+ATOMIC_XCHG(xchgl, swpl, ldxr, stlxr, Assembler::xword)\n+ATOMIC_XCHG(xchglw, swpl, ldxrw, stlxrw, Assembler::word)\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1042,0 +1042,2 @@\n+  void atomic_xchgl(Register prev, Register newv, Register addr);\n+  void atomic_xchglw(Register prev, Register newv, Register addr);\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"atomic_aarch64.hpp\"\n@@ -41,0 +42,1 @@\n+#include \"runtime\/atomic.hpp\"\n@@ -1364,1 +1366,1 @@\n-  \/\/ cache line boundaries will still be loaded and stored atomicly.\n+  \/\/ cache line boundaries will still be loaded and stored atomically.\n@@ -1434,1 +1436,1 @@\n-  \/\/ cache line boundaries will still be loaded and stored atomicly.\n+  \/\/ cache line boundaries will still be loaded and stored atomically.\n@@ -1599,1 +1601,1 @@\n-  \/\/ cache line boundaries will still be loaded and stored atomicly.\n+  \/\/ cache line boundaries will still be loaded and stored atomically.\n@@ -1623,1 +1625,1 @@\n-  \/\/ cache line boundaries will still be loaded and stored atomicly.\n+  \/\/ cache line boundaries will still be loaded and stored atomically.\n@@ -5574,0 +5576,83 @@\n+  \/\/ ARMv8.1 LSE versions of the atomic stubs used by Atomic::PlatformXX.\n+  \/\/\n+  \/\/ If LSE is in use, generate LSE versions of all the stubs. The\n+  \/\/ non-LSE versions are in atomic_aarch64.S.\n+  void generate_atomic_entry_points() {\n+\n+    if (! UseLSE) {\n+      return;\n+    }\n+\n+    __ align(CodeEntryAlignment);\n+    StubCodeMark mark(this, \"StubRoutines\", \"atomic entry points\");\n+\n+    __ align(32);\n+    aarch64_atomic_fetch_add_8_impl = (aarch64_atomic_stub_t)__ pc();\n+    {\n+      Register prev = r2, addr = c_rarg0, incr = c_rarg1;\n+      __ atomic_addal(prev, incr, addr);\n+      __ mov(r0, prev);\n+      __ ret(lr);\n+    }\n+    __ align(32);\n+    aarch64_atomic_fetch_add_4_impl = (aarch64_atomic_stub_t)__ pc();\n+    {\n+      Register prev = r2, addr = c_rarg0, incr = c_rarg1;\n+      __ atomic_addalw(prev, incr, addr);\n+      __ movw(r0, prev);\n+      __ ret(lr);\n+    }\n+    __ align(32);\n+    aarch64_atomic_xchg_4_impl = (aarch64_atomic_stub_t)__ pc();\n+    {\n+      Register prev = r2, addr = c_rarg0, newv = c_rarg1;\n+      __ atomic_xchglw(prev, newv, addr);\n+      __ movw(r0, prev);\n+      __ ret(lr);\n+    }\n+    __ align(32);\n+    aarch64_atomic_xchg_8_impl = (aarch64_atomic_stub_t)__ pc();\n+    {\n+      Register prev = r2, addr = c_rarg0, newv = c_rarg1;\n+      __ atomic_xchgl(prev, newv, addr);\n+      __ mov(r0, prev);\n+      __ ret(lr);\n+    }\n+    __ align(32);\n+    aarch64_atomic_cmpxchg_1_impl = (aarch64_atomic_stub_t)__ pc();\n+    {\n+      Register prev = r3, ptr = c_rarg0, compare_val = c_rarg1,\n+        exchange_val = c_rarg2;\n+      __ cmpxchg(ptr, compare_val, exchange_val,\n+                 MacroAssembler::byte,\n+                 \/*acquire*\/false, \/*release*\/false, \/*weak*\/false,\n+                 prev);\n+      __ movw(r0, prev);\n+      __ ret(lr);\n+    }\n+    __ align(32);\n+    aarch64_atomic_cmpxchg_4_impl = (aarch64_atomic_stub_t)__ pc();\n+    {\n+      Register prev = r3, ptr = c_rarg0, compare_val = c_rarg1,\n+        exchange_val = c_rarg2;\n+      __ cmpxchg(ptr, compare_val, exchange_val,\n+                 MacroAssembler::word,\n+                 \/*acquire*\/false, \/*release*\/false, \/*weak*\/false,\n+                 prev);\n+      __ movw(r0, prev);\n+      __ ret(lr);\n+    }\n+    __ align(32);\n+    aarch64_atomic_cmpxchg_8_impl = (aarch64_atomic_stub_t)__ pc();\n+    {\n+      Register prev = r3, ptr = c_rarg0, compare_val = c_rarg1,\n+        exchange_val = c_rarg2;\n+      __ cmpxchg(ptr, compare_val, exchange_val,\n+                 MacroAssembler::xword,\n+                 \/*acquire*\/false, \/*release*\/false, \/*weak*\/false,\n+                 prev);\n+      __ mov(r0, prev);\n+      __ ret(lr);\n+    }\n+  }\n+\n@@ -6686,0 +6771,2 @@\n+    generate_atomic_entry_points();\n+\n@@ -6706,0 +6793,20 @@\n+\n+\n+\/\/ Define pointers to atomic stubs and initialize them to point to the\n+\/\/ code in atomic_aarch64.S.\n+\n+#define DEFAULT_ATOMIC_OP(OPNAME, SIZE)                                 \\\n+  extern \"C\" uint64_t aarch64_atomic_ ## OPNAME ## _ ## SIZE ## _default_impl \\\n+    (volatile void *ptr, uint64_t arg1, uint64_t arg2);                 \\\n+  aarch64_atomic_stub_t aarch64_atomic_ ## OPNAME ## _ ## SIZE ## _impl \\\n+    = aarch64_atomic_ ## OPNAME ## _ ## SIZE ## _default_impl;\n+\n+DEFAULT_ATOMIC_OP(fetch_add, 4)\n+DEFAULT_ATOMIC_OP(fetch_add, 8)\n+DEFAULT_ATOMIC_OP(xchg, 4)\n+DEFAULT_ATOMIC_OP(xchg, 8)\n+DEFAULT_ATOMIC_OP(cmpxchg, 1)\n+DEFAULT_ATOMIC_OP(cmpxchg, 4)\n+DEFAULT_ATOMIC_OP(cmpxchg, 8)\n+\n+#undef DEFAULT_ATOMIC_OP\n","filename":"src\/hotspot\/cpu\/aarch64\/stubGenerator_aarch64.cpp","additions":111,"deletions":4,"binary":false,"changes":115,"status":"modified"},{"patch":"@@ -3,1 +3,1 @@\n- * Copyright (c) 2014, 2019, Red Hat Inc. All rights reserved.\n+ * Copyright (c) 2014, 2021, Red Hat Inc. All rights reserved.\n@@ -30,0 +30,1 @@\n+#include \"atomic_aarch64.hpp\"\n@@ -32,0 +33,10 @@\n+\n+\/\/ Atomic stub implementation.\n+\/\/ Default implementations are in atomic_aarch64.S\n+\/\/\n+\/\/ All stubs pass arguments the same way\n+\/\/ x0: src\/dest address\n+\/\/ x1: arg1\n+\/\/ x2: arg2 (optional)\n+\/\/ x3, x8, x9: scratch\n+\n@@ -35,0 +46,35 @@\n+\/\/ Call one of the stubs from C++. This uses the C calling convention,\n+\/\/ but this asm definition is used in order only to clobber the\n+\/\/ resisters we use. If we called the stubs via an ABI call we'd have\n+\/\/ to save X0 - X18.\n+\/\/\n+\/\/ This really ought to be a template definition, but see GCC Bug\n+\/\/ 33661, template methods forget explicit local register asm\n+\/\/ vars. The problem is that register specifiers attached to local\n+\/\/ variables are ignored in any template function.\n+inline uint64_t bare_atomic_fastcall(address stub, volatile void *ptr, uint64_t arg1, uint64_t arg2 = 0) {\n+  register uint64_t reg0 __asm__(\"x0\") = (uint64_t)ptr;\n+  register uint64_t reg1 __asm__(\"x1\") = arg1;\n+  register uint64_t reg2 __asm__(\"x2\") = arg2;\n+  register uint64_t reg3 __asm__(\"x3\") = (uint64_t)stub;\n+  register uint64_t result __asm__(\"x0\");\n+  asm volatile(\/\/ \"stp x29, x30, [sp, #-16]!;\"\n+               \" blr %1;\"\n+               \/\/ \" ldp x29, x30, [sp], #16 \/\/ regs %0, %1, %2, %3, %4\"\n+               : \"=r\"(result), \"+r\"(reg3), \"+r\"(reg2)\n+               : \"r\"(reg1), \"0\"(reg0) : \"x8\", \"x9\", \"x30\", \"cc\", \"memory\");\n+  return result;\n+}\n+\n+template <typename F, typename D, typename T1>\n+inline D atomic_fastcall(F stub, volatile D *dest, T1 arg1) {\n+  return (D)bare_atomic_fastcall(CAST_FROM_FN_PTR(address, stub),\n+                                 dest, (uint64_t)arg1);\n+}\n+\n+template <typename F, typename D, typename T1, typename T2>\n+inline D atomic_fastcall(F stub, volatile D *dest, T1 arg1, T2 arg2) {\n+  return (D)bare_atomic_fastcall(CAST_FROM_FN_PTR(address, stub),\n+                                 dest, (uint64_t)arg1, (uint64_t)arg2);\n+}\n+\n@@ -38,5 +84,1 @@\n-  D add_and_fetch(D volatile* dest, I add_value, atomic_memory_order order) const {\n-    D res = __atomic_add_fetch(dest, add_value, __ATOMIC_RELEASE);\n-    FULL_MEM_BARRIER;\n-    return res;\n-  }\n+  D fetch_and_add(D volatile* dest, I add_value, atomic_memory_order order) const;\n@@ -45,2 +87,3 @@\n-  D fetch_and_add(D volatile* dest, I add_value, atomic_memory_order order) const {\n-    return add_and_fetch(dest, add_value, order) - add_value;\n+  D add_and_fetch(D volatile* dest, I add_value, atomic_memory_order order) const {\n+    D value = fetch_and_add(dest, add_value, order) + add_value;\n+    return value;\n@@ -50,1 +93,25 @@\n-template<size_t byte_size>\n+template<>\n+template<typename D, typename I>\n+inline D Atomic::PlatformAdd<4>::fetch_and_add(D volatile* dest, I add_value,\n+                                               atomic_memory_order order) const {\n+  STATIC_ASSERT(4 == sizeof(I));\n+  STATIC_ASSERT(4 == sizeof(D));\n+  D old_value\n+    = atomic_fastcall(aarch64_atomic_fetch_add_4_impl, dest, add_value);\n+    FULL_MEM_BARRIER;\n+  return old_value;\n+}\n+\n+template<>\n+template<typename D, typename I>\n+inline D Atomic::PlatformAdd<8>::fetch_and_add(D volatile* dest, I add_value,\n+                                               atomic_memory_order order) const {\n+  STATIC_ASSERT(8 == sizeof(I));\n+  STATIC_ASSERT(8 == sizeof(D));\n+  D old_value\n+    = atomic_fastcall(aarch64_atomic_fetch_add_8_impl, dest, add_value);\n+    FULL_MEM_BARRIER;\n+  return old_value;\n+}\n+\n+template<>\n@@ -52,5 +119,5 @@\n-inline T Atomic::PlatformXchg<byte_size>::operator()(T volatile* dest,\n-                                                     T exchange_value,\n-                                                     atomic_memory_order order) const {\n-  STATIC_ASSERT(byte_size == sizeof(T));\n-  T res = __atomic_exchange_n(dest, exchange_value, __ATOMIC_RELEASE);\n+inline T Atomic::PlatformXchg<4>::operator()(T volatile* dest,\n+                                             T exchange_value,\n+                                             atomic_memory_order order) const {\n+  STATIC_ASSERT(4 == sizeof(T));\n+  T old_value = atomic_fastcall(aarch64_atomic_xchg_4_impl, dest, exchange_value);\n@@ -58,1 +125,1 @@\n-  return res;\n+  return old_value;\n@@ -61,2 +128,1 @@\n-\/\/ __attribute__((unused)) on dest is to get rid of spurious GCC warnings.\n-template<size_t byte_size>\n+template<>\n@@ -64,5 +130,16 @@\n-inline T Atomic::PlatformCmpxchg<byte_size>::operator()(T volatile* dest __attribute__((unused)),\n-                                                        T compare_value,\n-                                                        T exchange_value,\n-                                                        atomic_memory_order order) const {\n-  STATIC_ASSERT(byte_size == sizeof(T));\n+inline T Atomic::PlatformXchg<8>::operator()(T volatile* dest, T exchange_value,\n+                                             atomic_memory_order order) const {\n+  STATIC_ASSERT(8 == sizeof(T));\n+  T old_value = atomic_fastcall(aarch64_atomic_xchg_8_impl, dest, exchange_value);\n+  FULL_MEM_BARRIER;\n+  return old_value;\n+}\n+\n+template<>\n+template<typename T>\n+inline T Atomic::PlatformCmpxchg<1>::operator()(T volatile* dest,\n+                                                T compare_value,\n+                                                T exchange_value,\n+                                                atomic_memory_order order) const {\n+  STATIC_ASSERT(1 == sizeof(T));\n+  aarch64_atomic_stub_t stub = aarch64_atomic_cmpxchg_1_impl;\n@@ -70,4 +147,3 @@\n-    T value = compare_value;\n-    __atomic_compare_exchange(dest, &value, &exchange_value, \/*weak*\/false,\n-                              __ATOMIC_RELAXED, __ATOMIC_RELAXED);\n-    return value;\n+    T old_value = atomic_fastcall(stub, dest,\n+                                  compare_value, exchange_value);\n+    return old_value;\n@@ -75,1 +151,0 @@\n-    T value = compare_value;\n@@ -77,2 +152,2 @@\n-    __atomic_compare_exchange(dest, &value, &exchange_value, \/*weak*\/false,\n-                              __ATOMIC_RELAXED, __ATOMIC_RELAXED);\n+    T old_value = atomic_fastcall(stub, dest,\n+                                  compare_value, exchange_value);\n@@ -80,1 +155,43 @@\n-    return value;\n+    return old_value;\n+  }\n+}\n+\n+template<>\n+template<typename T>\n+inline T Atomic::PlatformCmpxchg<4>::operator()(T volatile* dest,\n+                                                T compare_value,\n+                                                T exchange_value,\n+                                                atomic_memory_order order) const {\n+  STATIC_ASSERT(4 == sizeof(T));\n+  aarch64_atomic_stub_t stub = aarch64_atomic_cmpxchg_4_impl;\n+  if (order == memory_order_relaxed) {\n+    T old_value = atomic_fastcall(stub, dest,\n+                                  compare_value, exchange_value);\n+    return old_value;\n+  } else {\n+    FULL_MEM_BARRIER;\n+    T old_value = atomic_fastcall(stub, dest,\n+                                  compare_value, exchange_value);\n+    FULL_MEM_BARRIER;\n+    return old_value;\n+  }\n+}\n+\n+template<>\n+template<typename T>\n+inline T Atomic::PlatformCmpxchg<8>::operator()(T volatile* dest,\n+                                                T compare_value,\n+                                                T exchange_value,\n+                                                atomic_memory_order order) const {\n+  STATIC_ASSERT(8 == sizeof(T));\n+  aarch64_atomic_stub_t stub = aarch64_atomic_cmpxchg_8_impl;\n+  if (order == memory_order_relaxed) {\n+    T old_value = atomic_fastcall(stub, dest,\n+                                  compare_value, exchange_value);\n+    return old_value;\n+  } else {\n+    FULL_MEM_BARRIER;\n+    T old_value = atomic_fastcall(stub, dest,\n+                                  compare_value, exchange_value);\n+    FULL_MEM_BARRIER;\n+    return old_value;\n","filename":"src\/hotspot\/os_cpu\/linux_aarch64\/atomic_linux_aarch64.hpp","additions":147,"deletions":30,"binary":false,"changes":177,"status":"modified"}]}