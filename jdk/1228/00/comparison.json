{"files":[{"patch":"@@ -34,14 +34,10 @@\n-\/\/   7   4 3 2 1 0\n-\/\/  +---+-+-+-+-+-+\n-\/\/  |000|1|1|1|1|1|\n-\/\/  +---+-+-+-+-+-+\n-\/\/  |   | | | | |\n-\/\/  |   | | | | * 0-0 Worker Thread Flag (1-bit)\n-\/\/  |   | | | |\n-\/\/  |   | | | * 1-1 Non-Blocking Flag (1-bit)\n-\/\/  |   | | |\n-\/\/  |   | | * 2-2 Relocation Flag (1-bit)\n-\/\/  |   | |\n-\/\/  |   | * 3-3 No Reserve Flag (1-bit)\n-\/\/  |   |\n-\/\/  |   * 4-4 Low Address Flag (1-bit)\n+\/\/   7     2 1 0\n+\/\/  +-----+-+-+-+\n+\/\/  |00000|1|1|1|\n+\/\/  +-----+-+-+-+\n+\/\/  |     | | |\n+\/\/  |     | | * 0-0 Non-Blocking Flag (1-bit)\n+\/\/  |     | |\n+\/\/  |     | * 1-1 Worker Relocation Flag (1-bit)\n+\/\/  |     |\n+\/\/  |     * 2-2 Low Address Flag (1-bit)\n@@ -49,1 +45,1 @@\n-\/\/  * 7-5 Unused (3-bits)\n+\/\/  * 7-3 Unused (5-bits)\n@@ -54,5 +50,3 @@\n-  typedef ZBitField<uint8_t, bool, 0, 1> field_worker_thread;\n-  typedef ZBitField<uint8_t, bool, 1, 1> field_non_blocking;\n-  typedef ZBitField<uint8_t, bool, 2, 1> field_relocation;\n-  typedef ZBitField<uint8_t, bool, 3, 1> field_no_reserve;\n-  typedef ZBitField<uint8_t, bool, 4, 1> field_low_address;\n+  typedef ZBitField<uint8_t, bool, 0, 1> field_non_blocking;\n+  typedef ZBitField<uint8_t, bool, 1, 1> field_worker_relocation;\n+  typedef ZBitField<uint8_t, bool, 2, 1> field_low_address;\n@@ -66,4 +60,0 @@\n-  void set_worker_thread() {\n-    _flags |= field_worker_thread::encode(true);\n-  }\n-\n@@ -74,6 +64,2 @@\n-  void set_relocation() {\n-    _flags |= field_relocation::encode(true);\n-  }\n-\n-  void set_no_reserve() {\n-    _flags |= field_no_reserve::encode(true);\n+  void set_worker_relocation() {\n+    _flags |= field_worker_relocation::encode(true);\n@@ -86,4 +72,0 @@\n-  bool worker_thread() const {\n-    return field_worker_thread::decode(_flags);\n-  }\n-\n@@ -94,6 +76,2 @@\n-  bool relocation() const {\n-    return field_relocation::decode(_flags);\n-  }\n-\n-  bool no_reserve() const {\n-    return field_no_reserve::decode(_flags);\n+  bool worker_relocation() const {\n+    return field_worker_relocation::decode(_flags);\n","filename":"src\/hotspot\/share\/gc\/z\/zAllocationFlags.hpp","additions":18,"deletions":40,"binary":false,"changes":58,"status":"modified"},{"patch":"@@ -74,12 +74,0 @@\n-  \/\/ Select medium page size so that we can calculate the max reserve\n-  ZHeuristics::set_medium_page_size();\n-\n-  \/\/ MinHeapSize\/InitialHeapSize must be at least as large as the max reserve\n-  const size_t max_reserve = ZHeuristics::max_reserve();\n-  if (MinHeapSize < max_reserve) {\n-    FLAG_SET_ERGO(MinHeapSize, max_reserve);\n-  }\n-  if (InitialHeapSize < max_reserve) {\n-    FLAG_SET_ERGO(InitialHeapSize, max_reserve);\n-  }\n-\n","filename":"src\/hotspot\/share\/gc\/z\/zArguments.cpp","additions":0,"deletions":12,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"gc\/z\/zHeuristics.hpp\"\n@@ -34,0 +35,1 @@\n+    _relocation_headroom(ZHeuristics::relocation_headroom()),\n@@ -98,3 +100,2 @@\n-  \/\/ Calculate amount of free memory available to Java threads. Note that\n-  \/\/ the heap reserve is not available to Java threads and is therefore not\n-  \/\/ considered part of the free memory.\n+  \/\/ Calculate amount of free memory available. Note that we take the\n+  \/\/ relocation headroom into account to avoid in-place relocation.\n@@ -102,1 +103,0 @@\n-  const size_t max_reserve = ZHeap::heap()->max_reserve();\n@@ -104,2 +104,2 @@\n-  const size_t free_with_reserve = soft_max_capacity - MIN2(soft_max_capacity, used);\n-  const size_t free = free_with_reserve - MIN2(free_with_reserve, max_reserve);\n+  const size_t free_including_headroom = soft_max_capacity - MIN2(soft_max_capacity, used);\n+  const size_t free = free_including_headroom - MIN2(free_including_headroom, _relocation_headroom);\n@@ -182,3 +182,2 @@\n-  \/\/ Calculate amount of free memory available to Java threads. Note that\n-  \/\/ the heap reserve is not available to Java threads and is therefore not\n-  \/\/ considered part of the free memory.\n+  \/\/ Calculate amount of free memory available. Note that we take the\n+  \/\/ relocation headroom into account to avoid in-place relocation.\n@@ -186,1 +185,0 @@\n-  const size_t max_reserve = ZHeap::heap()->max_reserve();\n@@ -188,2 +186,2 @@\n-  const size_t free_with_reserve = soft_max_capacity - MIN2(soft_max_capacity, used);\n-  const size_t free = free_with_reserve - MIN2(free_with_reserve, max_reserve);\n+  const size_t free_including_headroom = soft_max_capacity - MIN2(soft_max_capacity, used);\n+  const size_t free = free_including_headroom - MIN2(free_including_headroom, _relocation_headroom);\n","filename":"src\/hotspot\/share\/gc\/z\/zDirector.cpp","additions":10,"deletions":12,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2017, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n@@ -35,1 +35,2 @@\n-  ZMetronome _metronome;\n+  const size_t _relocation_headroom;\n+  ZMetronome   _metronome;\n","filename":"src\/hotspot\/share\/gc\/z\/zDirector.hpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -25,0 +25,1 @@\n+#include \"gc\/z\/zAddress.inline.hpp\"\n@@ -26,1 +27,129 @@\n-#include \"utilities\/debug.hpp\"\n+#include \"gc\/z\/zStat.hpp\"\n+#include \"gc\/z\/zUtils.inline.hpp\"\n+#include \"utilities\/align.hpp\"\n+\n+\/\/\n+\/\/ Reference count states:\n+\/\/\n+\/\/ * If the reference count is zero, it will never change again.\n+\/\/\n+\/\/ * If the reference count is positive, it can be both retained\n+\/\/   (increased) and released (decreased).\n+\/\/\n+\/\/ * If the reference count is negative, is can only be released\n+\/\/   (increased). A negative reference count means that one or more\n+\/\/   threads are waiting for one or more other threads to release\n+\/\/   their references.\n+\/\/\n+\/\/ The reference lock is used for waiting until the reference\n+\/\/ count has become zero (released) or negative one (claimed).\n+\/\/\n+\n+static const ZStatCriticalPhase ZCriticalPhaseRelocationStall(\"Relocation Stall\");\n+\n+bool ZForwarding::retain_page() {\n+  for (;;) {\n+    const int32_t ref_count = Atomic::load(&_ref_count);\n+\n+    if (ref_count == 0) {\n+      \/\/ Released\n+      return false;\n+    }\n+\n+    if (ref_count < 0) {\n+      \/\/ Claimed\n+      wait_page_released();\n+      return false;\n+    }\n+\n+    if (Atomic::cmpxchg(&_ref_count, ref_count, ref_count + 1) == ref_count) {\n+      \/\/ Retained\n+      return true;\n+    }\n+  }\n+}\n+\n+ZPage* ZForwarding::claim_page() {\n+  for (;;) {\n+    const int32_t ref_count = Atomic::load(&_ref_count);\n+    assert(ref_count > 0, \"Invalid state\");\n+\n+    \/\/ Invert reference count\n+    if (Atomic::cmpxchg(&_ref_count, ref_count, -ref_count) != ref_count) {\n+      continue;\n+    }\n+\n+    \/\/ Wait until claimed\n+    if (Atomic::load(&_ref_count) != -1) {\n+      ZLocker<ZConditionLock> locker(&_ref_lock);\n+      while (Atomic::load(&_ref_count) != -1) {\n+        _ref_lock.wait();\n+      }\n+    }\n+\n+    return _page;\n+  }\n+}\n+\n+void ZForwarding::release_page() {\n+  for (;;) {\n+    const int32_t ref_count = Atomic::load(&_ref_count);\n+    assert(ref_count != 0, \"Invalid state\");\n+\n+    if (ref_count > 0) {\n+      \/\/ Decrement reference count\n+      if (Atomic::cmpxchg(&_ref_count, ref_count, ref_count - 1) != ref_count) {\n+        continue;\n+      }\n+\n+      \/\/ If the previous ref_count was 1, then we just decremented\n+      \/\/ it to 0 and we should signal that the page is now released.\n+      if (ref_count == 1) {\n+        \/\/ Notify released\n+        ZLocker<ZConditionLock> locker(&_ref_lock);\n+        _ref_lock.notify_all();\n+      }\n+    } else {\n+      \/\/ Increment reference count\n+      if (Atomic::cmpxchg(&_ref_count, ref_count, ref_count + 1) != ref_count) {\n+        continue;\n+      }\n+\n+      \/\/ If the previous ref_count was -2 or -1, then we just incremented\n+      \/\/ it to -1 or 0, and we should signal the that page is now claimed\n+      \/\/ or released.\n+      if (ref_count == -2 || ref_count == -1) {\n+        \/\/ Notify claimed or released\n+        ZLocker<ZConditionLock> locker(&_ref_lock);\n+        _ref_lock.notify_all();\n+      }\n+    }\n+\n+    return;\n+  }\n+}\n+\n+void ZForwarding::wait_page_released() const {\n+  if (Atomic::load(&_ref_count) != 0) {\n+    ZStatTimer timer(ZCriticalPhaseRelocationStall);\n+    ZLocker<ZConditionLock> locker(&_ref_lock);\n+    while (Atomic::load(&_ref_count) != 0) {\n+      _ref_lock.wait();\n+    }\n+  }\n+}\n+\n+ZPage* ZForwarding::detach_page() {\n+  \/\/ Wait until released\n+  if (Atomic::load(&_ref_count) != 0) {\n+    ZLocker<ZConditionLock> locker(&_ref_lock);\n+    while (Atomic::load(&_ref_count) != 0) {\n+      _ref_lock.wait();\n+    }\n+  }\n+\n+  \/\/ Detach and return page\n+  ZPage* const page = _page;\n+  _page = NULL;\n+  return page;\n+}\n@@ -29,1 +158,1 @@\n-  guarantee(_refcount > 0, \"Invalid refcount\");\n+  guarantee(_ref_count != 0, \"Invalid reference count\");\n@@ -32,1 +161,2 @@\n-  size_t live_objects = 0;\n+  uint32_t live_objects = 0;\n+  size_t live_bytes = 0;\n@@ -56,0 +186,4 @@\n+    const uintptr_t to_addr = ZAddress::good(entry.to_offset());\n+    const size_t size = ZUtils::object_size(to_addr);\n+    const size_t aligned_size = align_up(size, _page->object_alignment());\n+    live_bytes += aligned_size;\n@@ -59,2 +193,3 @@\n-  \/\/ Check number of non-empty entries\n-  guarantee(live_objects == _page->live_objects(), \"Invalid number of entries\");\n+  \/\/ Verify number of live objects and bytes\n+  _page->verify_live_objects(live_objects);\n+  _page->verify_live_bytes(live_bytes);\n","filename":"src\/hotspot\/share\/gc\/z\/zForwarding.cpp","additions":140,"deletions":5,"binary":false,"changes":145,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"gc\/z\/zLock.hpp\"\n@@ -31,0 +32,1 @@\n+class ObjectClosure;\n@@ -43,9 +45,7 @@\n-  const ZVirtualMemory _virtual;\n-  const size_t         _object_alignment_shift;\n-  const AttachedArray  _entries;\n-  ZPage*               _page;\n-  volatile uint32_t    _refcount;\n-  volatile bool        _pinned;\n-\n-  bool inc_refcount();\n-  bool dec_refcount();\n+  const ZVirtualMemory   _virtual;\n+  const size_t           _object_alignment_shift;\n+  const AttachedArray    _entries;\n+  ZPage*                 _page;\n+  mutable ZConditionLock _ref_lock;\n+  volatile int32_t       _ref_count;\n+  bool                   _in_place;\n@@ -64,0 +64,1 @@\n+  uint8_t type() const;\n@@ -67,4 +68,1 @@\n-  ZPage* page() const;\n-\n-  bool is_pinned() const;\n-  void set_pinned();\n+  void object_iterate(ObjectClosure *cl);\n@@ -73,0 +71,1 @@\n+  ZPage* claim_page();\n@@ -74,0 +73,5 @@\n+  void wait_page_released() const;\n+  ZPage* detach_page();\n+\n+  void set_in_place();\n+  bool in_place() const;\n@@ -75,1 +79,0 @@\n-  ZForwardingEntry find(uintptr_t from_index) const;\n","filename":"src\/hotspot\/share\/gc\/z\/zForwarding.hpp","additions":17,"deletions":14,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"gc\/z\/zLock.inline.hpp\"\n@@ -59,2 +60,7 @@\n-    _refcount(1),\n-    _pinned(false) {}\n+    _ref_lock(),\n+    _ref_count(1),\n+    _in_place(false) {}\n+\n+inline uint8_t ZForwarding::type() const {\n+  return _page->type();\n+}\n@@ -74,6 +80,2 @@\n-inline ZPage* ZForwarding::page() const {\n-  return _page;\n-}\n-\n-inline bool ZForwarding::is_pinned() const {\n-  return Atomic::load(&_pinned);\n+inline void ZForwarding::object_iterate(ObjectClosure *cl) {\n+  return _page->object_iterate(cl);\n@@ -82,2 +84,2 @@\n-inline void ZForwarding::set_pinned() {\n-  Atomic::store(&_pinned, true);\n+inline void ZForwarding::set_in_place() {\n+  _in_place = true;\n@@ -86,31 +88,2 @@\n-inline bool ZForwarding::inc_refcount() {\n-  uint32_t refcount = Atomic::load(&_refcount);\n-\n-  while (refcount > 0) {\n-    const uint32_t old_refcount = refcount;\n-    const uint32_t new_refcount = old_refcount + 1;\n-    const uint32_t prev_refcount = Atomic::cmpxchg(&_refcount, old_refcount, new_refcount);\n-    if (prev_refcount == old_refcount) {\n-      return true;\n-    }\n-\n-    refcount = prev_refcount;\n-  }\n-\n-  return false;\n-}\n-\n-inline bool ZForwarding::dec_refcount() {\n-  assert(_refcount > 0, \"Invalid state\");\n-  return Atomic::sub(&_refcount, 1u) == 0u;\n-}\n-\n-inline bool ZForwarding::retain_page() {\n-  return inc_refcount();\n-}\n-\n-inline void ZForwarding::release_page() {\n-  if (dec_refcount()) {\n-    ZHeap::heap()->free_page(_page, true \/* reclaimed *\/);\n-    _page = NULL;\n-  }\n+inline bool ZForwarding::in_place() const {\n+  return _in_place;\n@@ -140,5 +113,0 @@\n-inline ZForwardingEntry ZForwarding::find(uintptr_t from_index) const {\n-  ZForwardingCursor dummy;\n-  return find(from_index, &dummy);\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/z\/zForwarding.inline.hpp","additions":14,"deletions":46,"binary":false,"changes":60,"status":"modified"},{"patch":"@@ -62,1 +62,1 @@\n-    _page_allocator(&_workers, MinHeapSize, InitialHeapSize, MaxHeapSize, ZHeuristics::max_reserve()),\n+    _page_allocator(&_workers, MinHeapSize, InitialHeapSize, MaxHeapSize),\n@@ -77,1 +77,1 @@\n-  ZStatHeap::set_at_initialize(min_capacity(), max_capacity(), max_reserve());\n+  ZStatHeap::set_at_initialize(_page_allocator.stats());\n@@ -100,12 +100,0 @@\n-size_t ZHeap::max_reserve() const {\n-  return _page_allocator.max_reserve();\n-}\n-\n-size_t ZHeap::used_high() const {\n-  return _page_allocator.used_high();\n-}\n-\n-size_t ZHeap::used_low() const {\n-  return _page_allocator.used_low();\n-}\n-\n@@ -120,8 +108,0 @@\n-size_t ZHeap::allocated() const {\n-  return _page_allocator.allocated();\n-}\n-\n-size_t ZHeap::reclaimed() const {\n-  return _page_allocator.reclaimed();\n-}\n-\n@@ -270,1 +250,1 @@\n-  ZStatHeap::set_at_mark_start(soft_max_capacity(), capacity(), used());\n+  ZStatHeap::set_at_mark_start(_page_allocator.stats());\n@@ -298,1 +278,1 @@\n-  ZStatHeap::set_at_mark_end(capacity(), allocated(), used());\n+  ZStatHeap::set_at_mark_end(_page_allocator.stats());\n@@ -419,1 +399,1 @@\n-  ZStatHeap::set_at_select_relocation_set(selector.stats(), reclaimed());\n+  ZStatHeap::set_at_select_relocation_set(selector.stats());\n@@ -447,1 +427,1 @@\n-  ZStatHeap::set_at_relocate_start(capacity(), allocated(), used());\n+  ZStatHeap::set_at_relocate_start(_page_allocator.stats());\n@@ -455,1 +435,1 @@\n-  const bool success = _relocate.relocate(&_relocation_set);\n+  _relocate.relocate(&_relocation_set);\n@@ -459,3 +439,1 @@\n-  ZStatRelocation::set_at_relocate_end(success);\n-  ZStatHeap::set_at_relocate_end(capacity(), allocated(), reclaimed(),\n-                                 used(), used_high(), used_low());\n+  ZStatHeap::set_at_relocate_end(_page_allocator.stats());\n","filename":"src\/hotspot\/share\/gc\/z\/zHeap.cpp","additions":8,"deletions":30,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -84,3 +84,0 @@\n-  size_t max_reserve() const;\n-  size_t used_high() const;\n-  size_t used_low() const;\n@@ -89,2 +86,0 @@\n-  size_t allocated() const;\n-  size_t reclaimed() const;\n@@ -122,2 +117,2 @@\n-  uintptr_t alloc_object_for_relocation(size_t size);\n-  void undo_alloc_object_for_relocation(uintptr_t addr, size_t size);\n+  uintptr_t alloc_object_non_blocking(size_t size);\n+  void undo_alloc_object(uintptr_t addr, size_t size);\n","filename":"src\/hotspot\/share\/gc\/z\/zHeap.hpp","additions":2,"deletions":7,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -83,2 +83,2 @@\n-inline uintptr_t ZHeap::alloc_object_for_relocation(size_t size) {\n-  uintptr_t addr = _object_allocator.alloc_object_for_relocation(size);\n+inline uintptr_t ZHeap::alloc_object_non_blocking(size_t size) {\n+  uintptr_t addr = _object_allocator.alloc_object_non_blocking(size);\n@@ -89,1 +89,1 @@\n-inline void ZHeap::undo_alloc_object_for_relocation(uintptr_t addr, size_t size) {\n+inline void ZHeap::undo_alloc_object(uintptr_t addr, size_t size) {\n@@ -91,1 +91,1 @@\n-  _object_allocator.undo_alloc_object_for_relocation(page, addr, size);\n+  _object_allocator.undo_alloc_object(page, addr, size);\n@@ -104,7 +104,1 @@\n-  const bool retained = forwarding->retain_page();\n-  const uintptr_t new_addr = _relocate.relocate_object(forwarding, addr);\n-  if (retained) {\n-    forwarding->release_page();\n-  }\n-\n-  return new_addr;\n+  return _relocate.relocate_object(forwarding, ZAddress::good(addr));\n@@ -124,1 +118,1 @@\n-  return _relocate.forward_object(forwarding, addr);\n+  return _relocate.forward_object(forwarding, ZAddress::good(addr));\n","filename":"src\/hotspot\/share\/gc\/z\/zHeap.inline.hpp","additions":6,"deletions":12,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -55,7 +55,4 @@\n-size_t ZHeuristics::max_reserve() {\n-  \/\/ Reserve one small page per worker plus one shared medium page. This is\n-  \/\/ still just an estimate and doesn't guarantee that we can't run out of\n-  \/\/ memory during relocation.\n-  const uint nworkers = MAX2(ParallelGCThreads, ConcGCThreads);\n-  const size_t reserve = (nworkers * ZPageSizeSmall) + ZPageSizeMedium;\n-  return MIN2(MaxHeapSize, reserve);\n+size_t ZHeuristics::relocation_headroom() {\n+  \/\/ Calculate headroom needed to avoid in-place relocation. Each worker will try\n+  \/\/ to allocate a small page, and all workers will share a single medium page.\n+  return (MAX2(ParallelGCThreads, ConcGCThreads) * ZPageSizeSmall) + ZPageSizeMedium;\n@@ -76,2 +73,2 @@\n-static uint nworkers_based_on_heap_size(double reserve_share_in_percent) {\n-  const int nworkers = (MaxHeapSize * (reserve_share_in_percent \/ 100.0)) \/ ZPageSizeSmall;\n+static uint nworkers_based_on_heap_size(double heap_share_in_percent) {\n+  const int nworkers = (MaxHeapSize * (heap_share_in_percent \/ 100.0)) \/ ZPageSizeSmall;\n@@ -82,3 +79,2 @@\n-  \/\/ Cap number of workers so that we don't use more than 2% of the max heap\n-  \/\/ for the small page reserve. This is useful when using small heaps on\n-  \/\/ large machines.\n+  \/\/ Cap number of workers so that they don't use more than 2% of the max heap\n+  \/\/ during relocation. This is useful when using small heaps on large machines.\n","filename":"src\/hotspot\/share\/gc\/z\/zHeuristics.cpp","additions":8,"deletions":12,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -33,1 +33,1 @@\n-  static size_t max_reserve();\n+  static size_t relocation_headroom();\n","filename":"src\/hotspot\/share\/gc\/z\/zHeuristics.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -53,0 +53,1 @@\n+  ZHeuristics::set_medium_page_size();\n","filename":"src\/hotspot\/share\/gc\/z\/zInitialize.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n@@ -146,0 +146,4 @@\n+    \/\/ Get the size of the object before calling the closure, which\n+    \/\/ might overwrite the object in case we are relocating in-place.\n+    const size_t size = ZUtils::object_size(addr);\n+\n@@ -150,1 +154,0 @@\n-    const size_t size = ZUtils::object_size(addr);\n","filename":"src\/hotspot\/share\/gc\/z\/zLiveMap.inline.hpp","additions":5,"deletions":2,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -47,2 +47,1 @@\n-    _shared_small_page(NULL),\n-    _worker_small_page(NULL) {}\n+    _shared_small_page(NULL) {}\n@@ -125,2 +124,0 @@\n-  assert(ZThread::is_java(), \"Should be a Java thread\");\n-\n@@ -144,31 +141,0 @@\n-uintptr_t ZObjectAllocator::alloc_small_object_from_nonworker(size_t size, ZAllocationFlags flags) {\n-  assert(!ZThread::is_worker(), \"Should not be a worker thread\");\n-\n-  \/\/ Non-worker small page allocation can never use the reserve\n-  flags.set_no_reserve();\n-\n-  return alloc_object_in_shared_page(shared_small_page_addr(), ZPageTypeSmall, ZPageSizeSmall, size, flags);\n-}\n-\n-uintptr_t ZObjectAllocator::alloc_small_object_from_worker(size_t size, ZAllocationFlags flags) {\n-  assert(ZThread::is_worker(), \"Should be a worker thread\");\n-\n-  ZPage* page = _worker_small_page.get();\n-  uintptr_t addr = 0;\n-\n-  if (page != NULL) {\n-    addr = page->alloc_object(size);\n-  }\n-\n-  if (addr == 0) {\n-    \/\/ Allocate new page\n-    page = alloc_page(ZPageTypeSmall, ZPageSizeSmall, flags);\n-    if (page != NULL) {\n-      addr = page->alloc_object(size);\n-    }\n-    _worker_small_page.set(page);\n-  }\n-\n-  return addr;\n-}\n-\n@@ -176,5 +142,1 @@\n-  if (flags.worker_thread()) {\n-    return alloc_small_object_from_worker(size, flags);\n-  } else {\n-    return alloc_small_object_from_nonworker(size, flags);\n-  }\n+  return alloc_object_in_shared_page(shared_small_page_addr(), ZPageTypeSmall, ZPageSizeSmall, size, flags);\n@@ -197,2 +159,0 @@\n-  assert(ZThread::is_java(), \"Must be a Java thread\");\n-\n@@ -200,2 +160,0 @@\n-  flags.set_no_reserve();\n-\n@@ -205,1 +163,1 @@\n-uintptr_t ZObjectAllocator::alloc_object_for_relocation(size_t size) {\n+uintptr_t ZObjectAllocator::alloc_object_non_blocking(size_t size) {\n@@ -207,1 +165,0 @@\n-  flags.set_relocation();\n@@ -209,5 +166,0 @@\n-\n-  if (ZThread::is_worker()) {\n-    flags.set_worker_thread();\n-  }\n-\n@@ -217,41 +169,1 @@\n-bool ZObjectAllocator::undo_alloc_large_object(ZPage* page) {\n-  assert(page->type() == ZPageTypeLarge, \"Invalid page type\");\n-\n-  \/\/ Undo page allocation\n-  undo_alloc_page(page);\n-  return true;\n-}\n-\n-bool ZObjectAllocator::undo_alloc_medium_object(ZPage* page, uintptr_t addr, size_t size) {\n-  assert(page->type() == ZPageTypeMedium, \"Invalid page type\");\n-\n-  \/\/ Try atomic undo on shared page\n-  return page->undo_alloc_object_atomic(addr, size);\n-}\n-\n-bool ZObjectAllocator::undo_alloc_small_object_from_nonworker(ZPage* page, uintptr_t addr, size_t size) {\n-  assert(page->type() == ZPageTypeSmall, \"Invalid page type\");\n-\n-  \/\/ Try atomic undo on shared page\n-  return page->undo_alloc_object_atomic(addr, size);\n-}\n-\n-bool ZObjectAllocator::undo_alloc_small_object_from_worker(ZPage* page, uintptr_t addr, size_t size) {\n-  assert(page->type() == ZPageTypeSmall, \"Invalid page type\");\n-  assert(page == _worker_small_page.get(), \"Invalid page\");\n-\n-  \/\/ Non-atomic undo on worker-local page\n-  const bool success = page->undo_alloc_object(addr, size);\n-  assert(success, \"Should always succeed\");\n-  return success;\n-}\n-\n-bool ZObjectAllocator::undo_alloc_small_object(ZPage* page, uintptr_t addr, size_t size) {\n-  if (ZThread::is_worker()) {\n-    return undo_alloc_small_object_from_worker(page, addr, size);\n-  } else {\n-    return undo_alloc_small_object_from_nonworker(page, addr, size);\n-  }\n-}\n-\n-bool ZObjectAllocator::undo_alloc_object(ZPage* page, uintptr_t addr, size_t size) {\n+void ZObjectAllocator::undo_alloc_object(ZPage* page, uintptr_t addr, size_t size) {\n@@ -260,11 +172,2 @@\n-  if (type == ZPageTypeSmall) {\n-    return undo_alloc_small_object(page, addr, size);\n-  } else if (type == ZPageTypeMedium) {\n-    return undo_alloc_medium_object(page, addr, size);\n-  } else {\n-    return undo_alloc_large_object(page);\n-  }\n-}\n-\n-void ZObjectAllocator::undo_alloc_object_for_relocation(ZPage* page, uintptr_t addr, size_t size) {\n-  if (undo_alloc_object(page, addr, size)) {\n+  if (type == ZPageTypeLarge) {\n+    undo_alloc_page(page);\n@@ -273,3 +176,5 @@\n-    ZStatInc(ZCounterUndoObjectAllocationFailed);\n-    log_trace(gc)(\"Failed to undo object allocation: \" PTR_FORMAT \", Size: \" SIZE_FORMAT \", Thread: \" PTR_FORMAT \" (%s)\",\n-                  addr, size, ZThread::id(), ZThread::name());\n+    if (page->undo_alloc_object_atomic(addr, size)) {\n+      ZStatInc(ZCounterUndoObjectAllocationSucceeded);\n+    } else {\n+      ZStatInc(ZCounterUndoObjectAllocationFailed);\n+    }\n@@ -317,1 +222,0 @@\n-  _worker_small_page.set_all(NULL);\n","filename":"src\/hotspot\/share\/gc\/z\/zObjectAllocator.cpp","additions":11,"deletions":107,"binary":false,"changes":118,"status":"modified"},{"patch":"@@ -39,1 +39,0 @@\n-  ZPerWorker<ZPage*> _worker_small_page;\n@@ -57,2 +56,0 @@\n-  uintptr_t alloc_small_object_from_nonworker(size_t size, ZAllocationFlags flags);\n-  uintptr_t alloc_small_object_from_worker(size_t size, ZAllocationFlags flags);\n@@ -62,7 +59,0 @@\n-  bool undo_alloc_large_object(ZPage* page);\n-  bool undo_alloc_medium_object(ZPage* page, uintptr_t addr, size_t size);\n-  bool undo_alloc_small_object_from_nonworker(ZPage* page, uintptr_t addr, size_t size);\n-  bool undo_alloc_small_object_from_worker(ZPage* page, uintptr_t addr, size_t size);\n-  bool undo_alloc_small_object(ZPage* page, uintptr_t addr, size_t size);\n-  bool undo_alloc_object(ZPage* page, uintptr_t addr, size_t size);\n-\n@@ -73,3 +63,2 @@\n-\n-  uintptr_t alloc_object_for_relocation(size_t size);\n-  void undo_alloc_object_for_relocation(ZPage* page, uintptr_t addr, size_t size);\n+  uintptr_t alloc_object_non_blocking(size_t size);\n+  void undo_alloc_object(ZPage* page, uintptr_t addr, size_t size);\n","filename":"src\/hotspot\/share\/gc\/z\/zObjectAllocator.hpp","additions":2,"deletions":13,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -66,0 +66,5 @@\n+void ZPage::reset_for_in_place_relocation() {\n+  _seqnum = ZGlobalSeqNum;\n+  _top = start();\n+}\n+\n@@ -125,0 +130,8 @@\n+\n+void ZPage::verify_live_objects(uint32_t live_objects) const {\n+  guarantee(live_objects == _livemap.live_objects(), \"Invalid number of live objects\");\n+}\n+\n+void ZPage::verify_live_bytes(size_t live_bytes) const {\n+  guarantee(live_bytes == _livemap.live_bytes(), \"Invalid number of live bytes %zu vs %zu\", live_bytes, _livemap.live_bytes());\n+}\n","filename":"src\/hotspot\/share\/gc\/z\/zPage.cpp","additions":13,"deletions":0,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -85,0 +85,1 @@\n+  void reset_for_in_place_relocation();\n@@ -112,0 +113,3 @@\n+\n+  void verify_live_objects(uint32_t live_objects) const;\n+  void verify_live_bytes(size_t live_bytes) const;\n","filename":"src\/hotspot\/share\/gc\/z\/zPage.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -33,1 +33,1 @@\n-#include \"gc\/z\/zPageAllocator.hpp\"\n+#include \"gc\/z\/zPageAllocator.inline.hpp\"\n@@ -133,2 +133,1 @@\n-                               size_t max_capacity,\n-                               size_t max_reserve) :\n+                               size_t max_capacity) :\n@@ -141,1 +140,0 @@\n-    _max_reserve(max_reserve),\n@@ -144,1 +142,0 @@\n-    _claimed(0),\n@@ -148,1 +145,1 @@\n-    _allocated(0),\n+    _claimed(0),\n@@ -164,1 +161,0 @@\n-  log_info_p(gc, init)(\"Max Reserve: \" SIZE_FORMAT \"M\", max_reserve \/ M);\n@@ -262,12 +258,0 @@\n-size_t ZPageAllocator::max_reserve() const {\n-  return _max_reserve;\n-}\n-\n-size_t ZPageAllocator::used_high() const {\n-  return _used_high;\n-}\n-\n-size_t ZPageAllocator::used_low() const {\n-  return _used_low;\n-}\n-\n@@ -279,6 +263,2 @@\n-  const ssize_t capacity = (ssize_t)Atomic::load(&_capacity);\n-  const ssize_t used = (ssize_t)Atomic::load(&_used);\n-  const ssize_t claimed = (ssize_t)Atomic::load(&_claimed);\n-  const ssize_t max_reserve = (ssize_t)_max_reserve;\n-  const ssize_t unused = capacity - used - claimed - max_reserve;\n-  return unused > 0 ? (size_t)unused : 0;\n+  const ZPageAllocatorStats st = stats();\n+  return st.capacity() - st.used() - st.claimed();\n@@ -287,6 +267,11 @@\n-size_t ZPageAllocator::allocated() const {\n-  return _allocated;\n-}\n-\n-size_t ZPageAllocator::reclaimed() const {\n-  return _reclaimed > 0 ? (size_t)_reclaimed : 0;\n+ZPageAllocatorStats ZPageAllocator::stats() const {\n+  ZLocker<ZLock> locker(&_lock);\n+  return ZPageAllocatorStats(_min_capacity,\n+                             _max_capacity,\n+                             soft_max_capacity(),\n+                             _capacity,\n+                             _used,\n+                             _used_high,\n+                             _used_low,\n+                             _claimed,\n+                             _reclaimed);\n@@ -297,1 +282,0 @@\n-  _allocated = 0;\n@@ -336,4 +320,4 @@\n-void ZPageAllocator::increase_used(size_t size, bool relocation) {\n-  if (relocation) {\n-    \/\/ Allocating a page for the purpose of relocation has a\n-    \/\/ negative contribution to the number of reclaimed bytes.\n+void ZPageAllocator::increase_used(size_t size, bool worker_relocation) {\n+  if (worker_relocation) {\n+    \/\/ Allocating a page for the purpose of worker relocation has\n+    \/\/ a negative contribution to the number of reclaimed bytes.\n@@ -342,1 +326,0 @@\n-  _allocated += size;\n@@ -358,2 +341,0 @@\n-  } else {\n-    _allocated -= size;\n@@ -404,22 +385,2 @@\n-bool ZPageAllocator::is_alloc_allowed(size_t size, bool no_reserve) const {\n-  size_t available = _current_max_capacity - _used - _claimed;\n-\n-  if (no_reserve) {\n-    \/\/ The reserve should not be considered available\n-    available -= MIN2(available, _max_reserve);\n-  }\n-\n-  return available >= size;\n-}\n-\n-bool ZPageAllocator::is_alloc_allowed_from_cache(size_t size, bool no_reserve) const {\n-  size_t available = _capacity - _used - _claimed;\n-\n-  if (no_reserve) {\n-    \/\/ The reserve should not be considered available\n-    available -= MIN2(available, _max_reserve);\n-  } else if (_capacity != _current_max_capacity) {\n-    \/\/ Always increase capacity before using the reserve\n-    return false;\n-  }\n-\n+bool ZPageAllocator::is_alloc_allowed(size_t size) const {\n+  const size_t available = _current_max_capacity - _used - _claimed;\n@@ -429,2 +390,2 @@\n-bool ZPageAllocator::alloc_page_common_inner(uint8_t type, size_t size, bool no_reserve, ZList<ZPage>* pages) {\n-  if (!is_alloc_allowed(size, no_reserve)) {\n+bool ZPageAllocator::alloc_page_common_inner(uint8_t type, size_t size, ZList<ZPage>* pages) {\n+  if (!is_alloc_allowed(size)) {\n@@ -436,7 +397,5 @@\n-  if (is_alloc_allowed_from_cache(size, no_reserve)) {\n-    ZPage* const page = _cache.alloc_page(type, size);\n-    if (page != NULL) {\n-      \/\/ Success\n-      pages->insert_last(page);\n-      return true;\n-    }\n+  ZPage* const page = _cache.alloc_page(type, size);\n+  if (page != NULL) {\n+    \/\/ Success\n+    pages->insert_last(page);\n+    return true;\n@@ -464,7 +423,3 @@\n-  \/\/ Try allocate without using the reserve\n-  if (!alloc_page_common_inner(type, size, true \/* no_reserve *\/, pages)) {\n-    \/\/ If allowed to, try allocate using the reserve\n-    if (flags.no_reserve() || !alloc_page_common_inner(type, size, false \/* no_reserve *\/, pages)) {\n-      \/\/ Out of memory\n-      return false;\n-    }\n+  if (!alloc_page_common_inner(type, size, pages)) {\n+    \/\/ Out of memory\n+    return false;\n@@ -474,1 +429,1 @@\n-  increase_used(size, flags.relocation());\n+  increase_used(size, flags.worker_relocation());\n@@ -692,3 +647,3 @@\n-  \/\/ Update allocation statistics. Exclude worker threads to avoid\n-  \/\/ artificial inflation of the allocation rate due to relocation.\n-  if (!flags.worker_thread()) {\n+  \/\/ Update allocation statistics. Exclude worker relocations to avoid\n+  \/\/ artificial inflation of the allocation rate during relocation.\n+  if (!flags.worker_relocation()) {\n@@ -704,1 +659,1 @@\n-               page->physical_memory().nsegments(), flags.non_blocking(), flags.no_reserve());\n+               page->physical_memory().nsegments(), flags.non_blocking());\n@@ -779,5 +734,4 @@\n-    \/\/ Never uncommit the reserve, and never uncommit below min capacity. We flush\n-    \/\/ out and uncommit chunks at a time (~0.8% of the max capacity, but at least\n-    \/\/ one granule and at most 256M), in case demand for memory increases while we\n-    \/\/ are uncommitting.\n-    const size_t retain = clamp(_used + _max_reserve, _min_capacity, _capacity);\n+    \/\/ Never uncommit below min capacity. We flush out and uncommit chunks at\n+    \/\/ a time (~0.8% of the max capacity, but at least one granule and at most\n+    \/\/ 256M), in case demand for memory increases while we are uncommitting.\n+    const size_t retain = MAX2(_used, _min_capacity);\n@@ -796,1 +750,1 @@\n-    Atomic::add(&_claimed, flushed);\n+    _claimed += flushed;\n@@ -812,1 +766,1 @@\n-    Atomic::sub(&_claimed, flushed);\n+    _claimed -= flushed;\n","filename":"src\/hotspot\/share\/gc\/z\/zPageAllocator.cpp","additions":43,"deletions":89,"binary":false,"changes":132,"status":"modified"},{"patch":"@@ -38,0 +38,1 @@\n+class ZPageAllocatorStats;\n@@ -48,1 +49,1 @@\n-  ZLock                      _lock;\n+  mutable ZLock              _lock;\n@@ -54,1 +55,0 @@\n-  const size_t               _max_reserve;\n@@ -57,1 +57,0 @@\n-  volatile size_t            _claimed;\n@@ -61,1 +60,1 @@\n-  size_t                     _allocated;\n+  size_t                     _claimed;\n@@ -86,2 +85,1 @@\n-  bool is_alloc_allowed(size_t size, bool no_reserve) const;\n-  bool is_alloc_allowed_from_cache(size_t size, bool no_reserve) const;\n+  bool is_alloc_allowed(size_t size) const;\n@@ -89,1 +87,1 @@\n-  bool alloc_page_common_inner(uint8_t type, size_t size, bool no_reserve, ZList<ZPage>* pages);\n+  bool alloc_page_common_inner(uint8_t type, size_t size, ZList<ZPage>* pages);\n@@ -107,2 +105,1 @@\n-                 size_t max_capacity,\n-                 size_t max_reserve);\n+                 size_t max_capacity);\n@@ -116,3 +113,0 @@\n-  size_t max_reserve() const;\n-  size_t used_high() const;\n-  size_t used_low() const;\n@@ -121,2 +115,2 @@\n-  size_t allocated() const;\n-  size_t reclaimed() const;\n+\n+  ZPageAllocatorStats stats() const;\n@@ -144,0 +138,35 @@\n+class ZPageAllocatorStats {\n+private:\n+  size_t _min_capacity;\n+  size_t _max_capacity;\n+  size_t _soft_max_capacity;\n+  size_t _current_max_capacity;\n+  size_t _capacity;\n+  size_t _used;\n+  size_t _used_high;\n+  size_t _used_low;\n+  size_t _claimed;\n+  size_t _reclaimed;\n+\n+public:\n+  ZPageAllocatorStats(size_t min_capacity,\n+                      size_t max_capacity,\n+                      size_t soft_max_capacity,\n+                      size_t capacity,\n+                      size_t used,\n+                      size_t used_high,\n+                      size_t used_low,\n+                      size_t claimed,\n+                      size_t reclaimed);\n+\n+  size_t min_capacity() const;\n+  size_t max_capacity() const;\n+  size_t soft_max_capacity() const;\n+  size_t capacity() const;\n+  size_t used() const;\n+  size_t used_high() const;\n+  size_t used_low() const;\n+  size_t claimed() const;\n+  size_t reclaimed() const;\n+};\n+\n","filename":"src\/hotspot\/share\/gc\/z\/zPageAllocator.hpp","additions":43,"deletions":14,"binary":false,"changes":57,"status":"modified"},{"patch":"@@ -0,0 +1,84 @@\n+\/*\n+ * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_Z_ZPAGEALLOCATOR_INLINE_HPP\n+#define SHARE_GC_Z_ZPAGEALLOCATOR_INLINE_HPP\n+\n+#include \"gc\/z\/zPageAllocator.hpp\"\n+\n+inline ZPageAllocatorStats::ZPageAllocatorStats(size_t min_capacity,\n+                                                size_t max_capacity,\n+                                                size_t soft_max_capacity,\n+                                                size_t capacity,\n+                                                size_t used,\n+                                                size_t used_high,\n+                                                size_t used_low,\n+                                                size_t claimed,\n+                                                size_t reclaimed) :\n+    _min_capacity(min_capacity),\n+    _max_capacity(max_capacity),\n+    _soft_max_capacity(soft_max_capacity),\n+    _capacity(capacity),\n+    _used(used),\n+    _used_high(used_high),\n+    _used_low(used_low),\n+    _claimed(claimed),\n+    _reclaimed(reclaimed) {}\n+\n+inline size_t ZPageAllocatorStats::min_capacity() const {\n+  return _min_capacity;\n+}\n+\n+inline size_t ZPageAllocatorStats::max_capacity() const {\n+  return _max_capacity;\n+}\n+\n+inline size_t ZPageAllocatorStats::soft_max_capacity() const {\n+  return _soft_max_capacity;\n+}\n+\n+inline size_t ZPageAllocatorStats::capacity() const {\n+  return _capacity;\n+}\n+\n+inline size_t ZPageAllocatorStats::used() const {\n+  return _used;\n+}\n+\n+inline size_t ZPageAllocatorStats::used_high() const {\n+  return _used_high;\n+}\n+\n+inline size_t ZPageAllocatorStats::used_low() const {\n+  return _used_low;\n+}\n+\n+inline size_t ZPageAllocatorStats::claimed() const {\n+  return _claimed;\n+}\n+\n+inline size_t ZPageAllocatorStats::reclaimed() const {\n+  return _reclaimed;\n+}\n+\n+#endif \/\/ SHARE_GC_Z_ZPAGEALLOCATOR_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/z\/zPageAllocator.inline.hpp","additions":84,"deletions":0,"binary":false,"changes":84,"status":"added"},{"patch":"@@ -28,2 +28,1 @@\n-#include \"gc\/z\/zHeap.hpp\"\n-#include \"gc\/z\/zOopClosures.inline.hpp\"\n+#include \"gc\/z\/zHeap.inline.hpp\"\n@@ -33,1 +32,0 @@\n-#include \"gc\/z\/zRootsIterator.hpp\"\n@@ -38,1 +36,0 @@\n-#include \"logging\/log.hpp\"\n@@ -40,0 +37,2 @@\n+#include \"runtime\/atomic.hpp\"\n+#include \"utilities\/debug.hpp\"\n@@ -41,1 +40,0 @@\n-static const ZStatCounter ZCounterRelocationContention(\"Contention\", \"Relocation Contention\", ZStatUnitOpsPerSecond);\n@@ -84,2 +82,4 @@\n-uintptr_t ZRelocate::relocate_object_inner(ZForwarding* forwarding, uintptr_t from_index, uintptr_t from_offset) const {\n-  ZForwardingCursor cursor;\n+static uintptr_t forwarding_index(ZForwarding* forwarding, uintptr_t from_addr) {\n+  const uintptr_t from_offset = ZAddress::offset(from_addr);\n+  return (from_offset - forwarding->start()) >> forwarding->object_alignment_shift();\n+}\n@@ -87,6 +87,12 @@\n-  \/\/ Lookup forwarding entry\n-  const ZForwardingEntry entry = forwarding->find(from_index, &cursor);\n-  if (entry.populated() && entry.from_index() == from_index) {\n-    \/\/ Already relocated, return new address\n-    return entry.to_offset();\n-  }\n+static uintptr_t forwarding_find(ZForwarding* forwarding, uintptr_t from_addr, ZForwardingCursor* cursor) {\n+  const uintptr_t from_index = forwarding_index(forwarding, from_addr);\n+  const ZForwardingEntry entry = forwarding->find(from_index, cursor);\n+  return entry.populated() ? ZAddress::good(entry.to_offset()) : 0;\n+}\n+\n+static uintptr_t forwarding_insert(ZForwarding* forwarding, uintptr_t from_addr, uintptr_t to_addr, ZForwardingCursor* cursor) {\n+  const uintptr_t from_index = forwarding_index(forwarding, from_addr);\n+  const uintptr_t to_offset = ZAddress::offset(to_addr);\n+  const uintptr_t to_offset_final = forwarding->insert(from_index, to_offset, cursor);\n+  return ZAddress::good(to_offset_final);\n+}\n@@ -94,1 +100,2 @@\n-  assert(ZHeap::heap()->is_object_live(ZAddress::good(from_offset)), \"Should be live\");\n+uintptr_t ZRelocate::relocate_object_inner(ZForwarding* forwarding, uintptr_t from_addr) const {\n+  ZForwardingCursor cursor;\n@@ -96,3 +103,5 @@\n-  if (forwarding->is_pinned()) {\n-    \/\/ In-place forward\n-    return forwarding->insert(from_index, from_offset, &cursor);\n+  \/\/ Lookup forwarding\n+  uintptr_t to_addr = forwarding_find(forwarding, from_addr, &cursor);\n+  if (to_addr != 0) {\n+    \/\/ Already relocated\n+    return to_addr;\n@@ -101,0 +110,2 @@\n+  assert(ZHeap::heap()->is_object_live(from_addr), \"Should be live\");\n+\n@@ -102,6 +113,5 @@\n-  const uintptr_t from_good = ZAddress::good(from_offset);\n-  const size_t size = ZUtils::object_size(from_good);\n-  const uintptr_t to_good = ZHeap::heap()->alloc_object_for_relocation(size);\n-  if (to_good == 0) {\n-    \/\/ Failed, in-place forward\n-    return forwarding->insert(from_index, from_offset, &cursor);\n+  const size_t size = ZUtils::object_size(from_addr);\n+  to_addr = ZHeap::heap()->alloc_object_non_blocking(size);\n+  if (to_addr == 0) {\n+    \/\/ Allocation failed\n+    return 0;\n@@ -111,1 +121,1 @@\n-  ZUtils::object_copy(from_good, to_good, size);\n+  ZUtils::object_copy_disjoint(from_addr, to_addr, size);\n@@ -113,6 +123,5 @@\n-  \/\/ Insert forwarding entry\n-  const uintptr_t to_offset = ZAddress::offset(to_good);\n-  const uintptr_t to_offset_final = forwarding->insert(from_index, to_offset, &cursor);\n-  if (to_offset_final == to_offset) {\n-    \/\/ Relocation succeeded\n-    return to_offset;\n+  \/\/ Insert forwarding\n+  const uintptr_t to_addr_final = forwarding_insert(forwarding, from_addr, to_addr, &cursor);\n+  if (to_addr_final != to_addr) {\n+    \/\/ Already relocated, try undo allocation\n+    ZHeap::heap()->undo_alloc_object(to_addr, size);\n@@ -121,10 +130,1 @@\n-  \/\/ Relocation contention\n-  ZStatInc(ZCounterRelocationContention);\n-  log_trace(gc)(\"Relocation contention, thread: \" PTR_FORMAT \" (%s), forwarding: \" PTR_FORMAT\n-                \", entry: \" SIZE_FORMAT \", oop: \" PTR_FORMAT \", size: \" SIZE_FORMAT,\n-                ZThread::id(), ZThread::name(), p2i(forwarding), cursor, from_good, size);\n-\n-  \/\/ Try undo allocation\n-  ZHeap::heap()->undo_alloc_object_for_relocation(to_good, size);\n-\n-  return to_offset_final;\n+  return to_addr_final;\n@@ -134,3 +134,9 @@\n-  const uintptr_t from_offset = ZAddress::offset(from_addr);\n-  const uintptr_t from_index = (from_offset - forwarding->start()) >> forwarding->object_alignment_shift();\n-  const uintptr_t to_offset = relocate_object_inner(forwarding, from_index, from_offset);\n+  \/\/ Relocate object\n+  if (forwarding->retain_page()) {\n+    const uintptr_t to_addr = relocate_object_inner(forwarding, from_addr);\n+    forwarding->release_page();\n+\n+    if (to_addr != 0) {\n+      \/\/ Success\n+      return to_addr;\n+    }\n@@ -138,3 +144,3 @@\n-  if (from_offset == to_offset) {\n-    \/\/ In-place forwarding, pin page\n-    forwarding->set_pinned();\n+    \/\/ Failed to relocate obejct. Wait for a worker thread to\n+    \/\/ complete relocation of this page, and then forward object.\n+    forwarding->wait_page_released();\n@@ -143,1 +149,2 @@\n-  return ZAddress::good(to_offset);\n+  \/\/ Forward object\n+  return forward_object(forwarding, from_addr);\n@@ -147,3 +154,12 @@\n-  const uintptr_t from_offset = ZAddress::offset(from_addr);\n-  const uintptr_t from_index = (from_offset - forwarding->start()) >> forwarding->object_alignment_shift();\n-  const ZForwardingEntry entry = forwarding->find(from_index);\n+  ZForwardingCursor cursor;\n+  const uintptr_t to_addr = forwarding_find(forwarding, from_addr, &cursor);\n+  assert(to_addr != 0, \"Should be forwarded\");\n+  return to_addr;\n+}\n+\n+static ZPage* alloc_page(const ZForwarding* forwarding) {\n+  if (ZStressRelocateInPlace) {\n+    \/\/ Simulate failure to allocate a new page. This will\n+    \/\/ cause the page being relocated to be relocated in-place.\n+    return NULL;\n+  }\n@@ -151,2 +167,9 @@\n-  assert(entry.populated(), \"Should be forwarded\");\n-  assert(entry.from_index() == from_index, \"Should be forwarded\");\n+  ZAllocationFlags flags;\n+  flags.set_non_blocking();\n+  flags.set_worker_relocation();\n+  return ZHeap::heap()->alloc_page(forwarding->type(), forwarding->size(), flags);\n+}\n+\n+static void free_page(ZPage* page) {\n+  ZHeap::heap()->free_page(page, true \/* reclaimed *\/);\n+}\n@@ -154,1 +177,6 @@\n-  return ZAddress::good(entry.to_offset());\n+static bool should_free_target_page(ZPage* page) {\n+  \/\/ Free target page if it is empty. We can end up with an empty target\n+  \/\/ page if we allocated a new target page, and then lost the race to\n+  \/\/ relocate the remaining objects, leaving the target page empty when\n+  \/\/ relocation completed.\n+  return page != NULL && page->top() == page->start();\n@@ -157,1 +185,1 @@\n-class ZRelocateObjectClosure : public ObjectClosure {\n+class ZRelocateSmallAllocator {\n@@ -159,2 +187,1 @@\n-  ZRelocate* const   _relocate;\n-  ZForwarding* const _forwarding;\n+  volatile size_t _in_place_count;\n@@ -163,3 +190,33 @@\n-  ZRelocateObjectClosure(ZRelocate* relocate, ZForwarding* forwarding) :\n-      _relocate(relocate),\n-      _forwarding(forwarding) {}\n+  ZRelocateSmallAllocator() :\n+      _in_place_count(0) {}\n+\n+  ZPage* alloc_target_page(ZForwarding* forwarding, ZPage* target) {\n+    ZPage* const page = alloc_page(forwarding);\n+    if (page == NULL) {\n+      Atomic::inc(&_in_place_count);\n+    }\n+\n+    return page;\n+  }\n+\n+  void share_target_page(ZPage* page) {\n+    \/\/ Does nothing\n+  }\n+\n+  void free_target_page(ZPage* page) {\n+    if (should_free_target_page(page)) {\n+      free_page(page);\n+    }\n+  }\n+\n+  void free_relocated_page(ZPage* page) {\n+    free_page(page);\n+  }\n+\n+  uintptr_t alloc_object(ZPage* page, size_t size) const {\n+    return (page != NULL) ? page->alloc_object(size) : 0;\n+  }\n+\n+  void undo_alloc_object(ZPage* page, uintptr_t addr, size_t size) const {\n+    page->undo_alloc_object(addr, size);\n+  }\n@@ -167,2 +224,2 @@\n-  virtual void do_object(oop o) {\n-    _relocate->relocate_object(_forwarding, ZOop::to_address(o));\n+  const size_t in_place_count() const {\n+    return _in_place_count;\n@@ -172,2 +229,19 @@\n-bool ZRelocate::work(ZRelocationSetParallelIterator* iter) {\n-  bool success = true;\n+class ZRelocateMediumAllocator {\n+private:\n+  ZConditionLock      _lock;\n+  ZPage*              _shared;\n+  bool                _in_place;\n+  volatile size_t     _in_place_count;\n+\n+public:\n+  ZRelocateMediumAllocator() :\n+      _lock(),\n+      _shared(NULL),\n+      _in_place(false),\n+      _in_place_count(0) {}\n+\n+  ~ZRelocateMediumAllocator() {\n+    if (should_free_target_page(_shared)) {\n+      free_page(_shared);\n+    }\n+  }\n@@ -175,5 +249,2 @@\n-  \/\/ Relocate pages in the relocation set\n-  for (ZForwarding* forwarding; iter->next(&forwarding);) {\n-    \/\/ Relocate objects in page\n-    ZRelocateObjectClosure cl(this, forwarding);\n-    forwarding->page()->object_iterate(&cl);\n+  ZPage* alloc_target_page(ZForwarding* forwarding, ZPage* target) {\n+    ZLocker<ZConditionLock> locker(&_lock);\n@@ -181,2 +252,3 @@\n-    if (ZVerifyForwarding) {\n-      forwarding->verify();\n+    \/\/ Wait for any ongoing in-place relocation to complete\n+    while (_in_place) {\n+      _lock.wait();\n@@ -185,3 +257,77 @@\n-    if (forwarding->is_pinned()) {\n-      \/\/ Relocation failed, page is now pinned\n-      success = false;\n+    \/\/ Allocate a new page only if the shared page is the same as the\n+    \/\/ current target page. The shared page will be different from the\n+    \/\/ current target page if another thread shared a page, or allocated\n+    \/\/ a new page.\n+    if (_shared == target) {\n+      _shared = alloc_page(forwarding);\n+      if (_shared == NULL) {\n+        Atomic::inc(&_in_place_count);\n+        _in_place = true;\n+      }\n+    }\n+\n+    return _shared;\n+  }\n+\n+  void share_target_page(ZPage* page) {\n+    ZLocker<ZConditionLock> locker(&_lock);\n+\n+    assert(_in_place, \"Invalid state\");\n+    assert(_shared == NULL, \"Invalid state\");\n+    assert(page != NULL, \"Invalid page\");\n+\n+    _shared = page;\n+    _in_place = false;\n+\n+    _lock.notify_all();\n+  }\n+\n+  void free_target_page(ZPage* page) {\n+    \/\/ Does nothing\n+  }\n+\n+  void free_relocated_page(ZPage* page) {\n+    free_page(page);\n+  }\n+\n+  uintptr_t alloc_object(ZPage* page, size_t size) const {\n+    return (page != NULL) ? page->alloc_object_atomic(size) : 0;\n+  }\n+\n+  void undo_alloc_object(ZPage* page, uintptr_t addr, size_t size) const {\n+    page->undo_alloc_object_atomic(addr, size);\n+  }\n+\n+  const size_t in_place_count() const {\n+    return _in_place_count;\n+  }\n+};\n+\n+template <typename Allocator>\n+class ZRelocateClosure : public ObjectClosure {\n+private:\n+  Allocator* const _allocator;\n+  ZForwarding*     _forwarding;\n+  ZPage*           _target;\n+\n+  bool relocate_object(uintptr_t from_addr) const {\n+    ZForwardingCursor cursor;\n+\n+    \/\/ Lookup forwarding\n+    if (forwarding_find(_forwarding, from_addr, &cursor) != 0) {\n+      \/\/ Already relocated\n+      return true;\n+    }\n+\n+    \/\/ Allocate object\n+    const size_t size = ZUtils::object_size(from_addr);\n+    const uintptr_t to_addr = _allocator->alloc_object(_target, size);\n+    if (to_addr == 0) {\n+      \/\/ Allocation failed\n+      return false;\n+    }\n+\n+    \/\/ Copy object. Use conjoint copying if we are relocating\n+    \/\/ in-place and the new object overlapps with the old object.\n+    if (_forwarding->in_place() && to_addr + size > from_addr) {\n+      ZUtils::object_copy_conjoint(from_addr, to_addr, size);\n@@ -189,2 +335,1 @@\n-      \/\/ Relocation succeeded, release page\n-      forwarding->release_page();\n+      ZUtils::object_copy_disjoint(from_addr, to_addr, size);\n@@ -192,0 +337,8 @@\n+\n+    \/\/ Insert forwarding\n+    if (forwarding_insert(_forwarding, from_addr, to_addr, &cursor) != to_addr) {\n+      \/\/ Already relocated, undo allocation\n+      _allocator->undo_alloc_object(_target, to_addr, size);\n+    }\n+\n+    return true;\n@@ -194,2 +347,57 @@\n-  return success;\n-}\n+  virtual void do_object(oop obj) {\n+    const uintptr_t addr = ZOop::to_address(obj);\n+    assert(ZHeap::heap()->is_object_live(addr), \"Should be live\");\n+\n+    while (!relocate_object(addr)) {\n+      \/\/ Allocate a new target page, or if that fails, use the page being\n+      \/\/ relocated as the new target, which will cause it to be relocated\n+      \/\/ in-place.\n+      _target = _allocator->alloc_target_page(_forwarding, _target);\n+      if (_target != NULL) {\n+        continue;\n+      }\n+\n+      \/\/ Claim the page being relocated to block other threads from accessing\n+      \/\/ it, or its forwarding table, until it has been released (relocation\n+      \/\/ completed).\n+      _target = _forwarding->claim_page();\n+      _target->reset_for_in_place_relocation();\n+      _forwarding->set_in_place();\n+    }\n+  }\n+\n+public:\n+  ZRelocateClosure(Allocator* allocator) :\n+      _allocator(allocator),\n+      _forwarding(NULL),\n+      _target(NULL) {}\n+\n+  ~ZRelocateClosure() {\n+    _allocator->free_target_page(_target);\n+  }\n+\n+  void do_forwarding(ZForwarding* forwarding) {\n+    \/\/ Relocate objects\n+    _forwarding = forwarding;\n+    _forwarding->object_iterate(this);\n+\n+    \/\/ Verify\n+    if (ZVerifyForwarding) {\n+      _forwarding->verify();\n+    }\n+\n+    \/\/ Release relocated page\n+    _forwarding->release_page();\n+\n+    if (_forwarding->in_place()) {\n+      \/\/ The relocated page has been relocated in-place and should not\n+      \/\/ be freed. Keep it as target page until it is full, and offer to\n+      \/\/ share it with other worker threads.\n+      _allocator->share_target_page(_target);\n+    } else {\n+      \/\/ Detach and free relocated page\n+      ZPage* const page = _forwarding->detach_page();\n+      _allocator->free_relocated_page(page);\n+    }\n+  }\n+};\n@@ -199,1 +407,0 @@\n-  ZRelocate* const               _relocate;\n@@ -201,1 +408,6 @@\n-  bool                           _failed;\n+  ZRelocateSmallAllocator        _small_allocator;\n+  ZRelocateMediumAllocator       _medium_allocator;\n+\n+  static bool is_small(ZForwarding* forwarding) {\n+    return forwarding->type() == ZPageTypeSmall;\n+  }\n@@ -204,1 +416,1 @@\n-  ZRelocateTask(ZRelocate* relocate, ZRelocationSet* relocation_set) :\n+  ZRelocateTask(ZRelocationSet* relocation_set) :\n@@ -206,1 +418,0 @@\n-      _relocate(relocate),\n@@ -208,1 +419,2 @@\n-      _failed(false) {}\n+      _small_allocator(),\n+      _medium_allocator() {}\n@@ -210,4 +422,3 @@\n-  virtual void work() {\n-    if (!_relocate->work(&_iter)) {\n-      _failed = true;\n-    }\n+  ~ZRelocateTask() {\n+    ZStatRelocation::set_at_relocate_end(_small_allocator.in_place_count(),\n+                                         _medium_allocator.in_place_count());\n@@ -216,2 +427,11 @@\n-  bool failed() const {\n-    return _failed;\n+  virtual void work() {\n+    ZRelocateClosure<ZRelocateSmallAllocator> small(&_small_allocator);\n+    ZRelocateClosure<ZRelocateMediumAllocator> medium(&_medium_allocator);\n+\n+    for (ZForwarding* forwarding; _iter.next(&forwarding);) {\n+      if (is_small(forwarding)) {\n+        small.do_forwarding(forwarding);\n+      } else {\n+        medium.do_forwarding(forwarding);\n+      }\n+    }\n@@ -221,2 +441,2 @@\n-bool ZRelocate::relocate(ZRelocationSet* relocation_set) {\n-  ZRelocateTask task(this, relocation_set);\n+void ZRelocate::relocate(ZRelocationSet* relocation_set) {\n+  ZRelocateTask task(relocation_set);\n@@ -224,1 +444,0 @@\n-  return !task.failed();\n","filename":"src\/hotspot\/share\/gc\/z\/zRelocate.cpp","additions":310,"deletions":91,"binary":false,"changes":401,"status":"modified"},{"patch":"@@ -38,3 +38,2 @@\n-  ZForwarding* forwarding_for_page(ZPage* page) const;\n-  uintptr_t relocate_object_inner(ZForwarding* forwarding, uintptr_t from_index, uintptr_t from_offset) const;\n-  bool work(ZRelocationSetParallelIterator* iter);\n+  uintptr_t relocate_object_inner(ZForwarding* forwarding, uintptr_t from_addr) const;\n+  void work(ZRelocationSetParallelIterator* iter);\n@@ -49,1 +48,1 @@\n-  bool relocate(ZRelocationSet* relocation_set);\n+  void relocate(ZRelocationSet* relocation_set);\n","filename":"src\/hotspot\/share\/gc\/z\/zRelocate.hpp","additions":3,"deletions":4,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -28,1 +28,1 @@\n-#include \"gc\/z\/zRelocationSet.hpp\"\n+#include \"gc\/z\/zRelocationSet.inline.hpp\"\n@@ -128,0 +128,6 @@\n+  \/\/ Destroy forwardings\n+  ZRelocationSetIterator iter(this);\n+  for (ZForwarding* forwarding; iter.next(&forwarding);) {\n+    forwarding->~ZForwarding();\n+  }\n+\n","filename":"src\/hotspot\/share\/gc\/z\/zRelocationSet.cpp","additions":7,"deletions":1,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -39,1 +39,0 @@\n-    _garbage(0),\n@@ -41,2 +40,1 @@\n-    _compacting_from(0),\n-    _compacting_to(0) {}\n+    _relocate(0) {}\n@@ -115,0 +113,1 @@\n+  size_t selected_live_bytes = 0;\n@@ -143,0 +142,1 @@\n+      selected_live_bytes = from_live_bytes;\n@@ -157,2 +157,1 @@\n-  _stats._compacting_from = selected_from * _page_size;\n-  _stats._compacting_to = selected_to * _page_size;\n+  _stats._relocate = selected_live_bytes;\n@@ -176,2 +175,1 @@\n-  event.commit(_page_type, _stats.npages(), _stats.total(), _stats.empty(),\n-               _stats.compacting_from(), _stats.compacting_to());\n+  event.commit(_page_type, _stats.npages(), _stats.total(), _stats.empty(), _stats.relocate());\n@@ -201,1 +199,1 @@\n-  event.commit(total(), empty(), compacting_from(), compacting_to());\n+  event.commit(total(), empty(), relocate());\n","filename":"src\/hotspot\/share\/gc\/z\/zRelocationSetSelector.cpp","additions":6,"deletions":8,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -39,1 +39,0 @@\n-  size_t _garbage;\n@@ -41,2 +40,1 @@\n-  size_t _compacting_from;\n-  size_t _compacting_to;\n+  size_t _relocate;\n@@ -50,1 +48,0 @@\n-  size_t garbage() const;\n@@ -52,2 +49,1 @@\n-  size_t compacting_from() const;\n-  size_t compacting_to() const;\n+  size_t relocate() const;\n@@ -111,2 +107,1 @@\n-  size_t compacting_from() const;\n-  size_t compacting_to() const;\n+  size_t relocate() const;\n","filename":"src\/hotspot\/share\/gc\/z\/zRelocationSetSelector.hpp","additions":3,"deletions":8,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -43,4 +43,0 @@\n-inline size_t ZRelocationSetSelectorGroupStats::garbage() const {\n-  return _garbage;\n-}\n-\n@@ -51,6 +47,2 @@\n-inline size_t ZRelocationSetSelectorGroupStats::compacting_from() const {\n-  return _compacting_from;\n-}\n-\n-inline size_t ZRelocationSetSelectorGroupStats::compacting_to() const {\n-  return _compacting_to;\n+inline size_t ZRelocationSetSelectorGroupStats::relocate() const {\n+  return _relocate;\n@@ -84,1 +76,0 @@\n-  _stats._garbage += garbage;\n@@ -92,1 +83,0 @@\n-  _stats._garbage += size;\n@@ -154,6 +144,2 @@\n-inline size_t ZRelocationSetSelector::compacting_from() const {\n-  return _small.stats().compacting_from() + _medium.stats().compacting_from() + _large.stats().compacting_from();\n-}\n-\n-inline size_t ZRelocationSetSelector::compacting_to() const {\n-  return _small.stats().compacting_to() + _medium.stats().compacting_to() + _large.stats().compacting_to();\n+inline size_t ZRelocationSetSelector::relocate() const {\n+  return _small.stats().relocate() + _medium.stats().relocate() + _large.stats().relocate();\n","filename":"src\/hotspot\/share\/gc\/z\/zRelocationSetSelector.inline.hpp","additions":4,"deletions":18,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"gc\/z\/zPageAllocator.inline.hpp\"\n@@ -1142,1 +1143,1 @@\n-ZRelocationSetSelectorStats ZStatRelocation::_stats;\n+ZRelocationSetSelectorStats ZStatRelocation::_selector_stats;\n@@ -1144,1 +1145,2 @@\n-bool                        ZStatRelocation::_success;\n+size_t                      ZStatRelocation::_small_in_place_count;\n+size_t                      ZStatRelocation::_medium_in_place_count;\n@@ -1146,2 +1148,2 @@\n-void ZStatRelocation::set_at_select_relocation_set(const ZRelocationSetSelectorStats& stats) {\n-  _stats = stats;\n+void ZStatRelocation::set_at_select_relocation_set(const ZRelocationSetSelectorStats& selector_stats) {\n+  _selector_stats = selector_stats;\n@@ -1154,2 +1156,3 @@\n-void ZStatRelocation::set_at_relocate_end(bool success) {\n-  _success = success;\n+void ZStatRelocation::set_at_relocate_end(size_t small_in_place_count, size_t medium_in_place_count) {\n+  _small_in_place_count = small_in_place_count;\n+  _medium_in_place_count = medium_in_place_count;\n@@ -1158,4 +1161,5 @@\n-void ZStatRelocation::print(const char* name, const ZRelocationSetSelectorGroupStats& group) {\n-  const size_t total = _stats.small().total() + _stats.medium().total() + _stats.large().total();\n-\n-  log_info(gc, reloc)(\"%s Pages: \" SIZE_FORMAT \" \/ \" ZSIZE_FMT \", Empty: \" ZSIZE_FMT \", Compacting: \" ZSIZE_FMT \"->\" ZSIZE_FMT,\n+void ZStatRelocation::print(const char* name,\n+                            const ZRelocationSetSelectorGroupStats& selector_group,\n+                            size_t in_place_count) {\n+  log_info(gc, reloc)(\"%s Pages: \" SIZE_FORMAT \" \/ \" SIZE_FORMAT \"M, Empty: \" SIZE_FORMAT \"M, \"\n+                      \"Relocated: \" SIZE_FORMAT \"M, In-Place: \" SIZE_FORMAT,\n@@ -1163,5 +1167,5 @@\n-                      group.npages(),\n-                      ZSIZE_ARGS_WITH_MAX(group.total(), total),\n-                      ZSIZE_ARGS_WITH_MAX(group.empty(), total),\n-                      ZSIZE_ARGS_WITH_MAX(group.compacting_from(), total),\n-                      ZSIZE_ARGS_WITH_MAX(group.compacting_to(), total));\n+                      selector_group.npages(),\n+                      selector_group.total() \/ M,\n+                      selector_group.empty() \/ M,\n+                      selector_group.relocate() \/ M,\n+                      in_place_count);\n@@ -1171,1 +1175,1 @@\n-  print(\"Small\", _stats.small());\n+  print(\"Small\", _selector_stats.small(), _small_in_place_count);\n@@ -1173,1 +1177,1 @@\n-    print(\"Medium\", _stats.medium());\n+    print(\"Medium\", _selector_stats.medium(), _medium_in_place_count);\n@@ -1175,1 +1179,1 @@\n-  print(\"Large\", _stats.large());\n+  print(\"Large\", _selector_stats.large(), 0 \/* in_place_count *\/);\n@@ -1178,1 +1182,0 @@\n-  log_info(gc, reloc)(\"Relocation: %s\", _success ? \"Successful\" : \"Incomplete\");\n@@ -1273,1 +1276,1 @@\n-size_t ZStatHeap::available(size_t used) {\n+size_t ZStatHeap::free(size_t used) {\n@@ -1277,2 +1280,2 @@\n-size_t ZStatHeap::reserve(size_t used) {\n-  return MIN2(_at_initialize.max_reserve, available(used));\n+size_t ZStatHeap::allocated(size_t used, size_t reclaimed) {\n+  return (used + reclaimed) - _at_mark_start.used;\n@@ -1281,2 +1284,2 @@\n-size_t ZStatHeap::free(size_t used) {\n-  return available(used) - reserve(used);\n+size_t ZStatHeap::garbage(size_t reclaimed) {\n+  return _at_mark_end.garbage - reclaimed;\n@@ -1285,6 +1288,3 @@\n-void ZStatHeap::set_at_initialize(size_t min_capacity,\n-                                  size_t max_capacity,\n-                                  size_t max_reserve) {\n-  _at_initialize.min_capacity = min_capacity;\n-  _at_initialize.max_capacity = max_capacity;\n-  _at_initialize.max_reserve = max_reserve;\n+void ZStatHeap::set_at_initialize(const ZPageAllocatorStats& stats) {\n+  _at_initialize.min_capacity = stats.min_capacity();\n+  _at_initialize.max_capacity = stats.max_capacity();\n@@ -1293,8 +1293,5 @@\n-void ZStatHeap::set_at_mark_start(size_t soft_max_capacity,\n-                                  size_t capacity,\n-                                  size_t used) {\n-  _at_mark_start.soft_max_capacity = soft_max_capacity;\n-  _at_mark_start.capacity = capacity;\n-  _at_mark_start.reserve = reserve(used);\n-  _at_mark_start.used = used;\n-  _at_mark_start.free = free(used);\n+void ZStatHeap::set_at_mark_start(const ZPageAllocatorStats& stats) {\n+  _at_mark_start.soft_max_capacity = stats.soft_max_capacity();\n+  _at_mark_start.capacity = stats.capacity();\n+  _at_mark_start.free = free(stats.used());\n+  _at_mark_start.used = stats.used();\n@@ -1303,8 +1300,5 @@\n-void ZStatHeap::set_at_mark_end(size_t capacity,\n-                                size_t allocated,\n-                                size_t used) {\n-  _at_mark_end.capacity = capacity;\n-  _at_mark_end.reserve = reserve(used);\n-  _at_mark_end.allocated = allocated;\n-  _at_mark_end.used = used;\n-  _at_mark_end.free = free(used);\n+void ZStatHeap::set_at_mark_end(const ZPageAllocatorStats& stats) {\n+  _at_mark_end.capacity = stats.capacity();\n+  _at_mark_end.free = free(stats.used());\n+  _at_mark_end.used = stats.used();\n+  _at_mark_end.allocated = allocated(stats.used(), 0 \/* reclaimed *\/);\n@@ -1313,1 +1307,1 @@\n-void ZStatHeap::set_at_select_relocation_set(const ZRelocationSetSelectorStats& stats, size_t reclaimed) {\n+void ZStatHeap::set_at_select_relocation_set(const ZRelocationSetSelectorStats& stats) {\n@@ -1315,2 +1309,0 @@\n-  const size_t garbage = stats.small().garbage() + stats.medium().garbage() + stats.large().garbage();\n-\n@@ -1318,4 +1310,1 @@\n-  _at_mark_end.garbage = garbage;\n-\n-  _at_relocate_start.garbage = garbage - reclaimed;\n-  _at_relocate_start.reclaimed = reclaimed;\n+  _at_mark_end.garbage = _at_mark_start.used - live;\n@@ -1324,8 +1313,7 @@\n-void ZStatHeap::set_at_relocate_start(size_t capacity,\n-                                      size_t allocated,\n-                                      size_t used) {\n-  _at_relocate_start.capacity = capacity;\n-  _at_relocate_start.reserve = reserve(used);\n-  _at_relocate_start.allocated = allocated;\n-  _at_relocate_start.used = used;\n-  _at_relocate_start.free = free(used);\n+void ZStatHeap::set_at_relocate_start(const ZPageAllocatorStats& stats) {\n+  _at_relocate_start.capacity = stats.capacity();\n+  _at_relocate_start.free = free(stats.used());\n+  _at_relocate_start.used = stats.used();\n+  _at_relocate_start.allocated = allocated(stats.used(), stats.reclaimed());\n+  _at_relocate_start.garbage = garbage(stats.reclaimed());\n+  _at_relocate_start.reclaimed = stats.reclaimed();\n@@ -1334,7 +1322,2 @@\n-void ZStatHeap::set_at_relocate_end(size_t capacity,\n-                                    size_t allocated,\n-                                    size_t reclaimed,\n-                                    size_t used,\n-                                    size_t used_high,\n-                                    size_t used_low) {\n-  _at_relocate_end.capacity = capacity;\n+void ZStatHeap::set_at_relocate_end(const ZPageAllocatorStats& stats) {\n+  _at_relocate_end.capacity = stats.capacity();\n@@ -1343,12 +1326,9 @@\n-  _at_relocate_end.reserve = reserve(used);\n-  _at_relocate_end.reserve_high = reserve(used_low);\n-  _at_relocate_end.reserve_low = reserve(used_high);\n-  _at_relocate_end.garbage = _at_mark_end.garbage - reclaimed;\n-  _at_relocate_end.allocated = allocated;\n-  _at_relocate_end.reclaimed = reclaimed;\n-  _at_relocate_end.used = used;\n-  _at_relocate_end.used_high = used_high;\n-  _at_relocate_end.used_low = used_low;\n-  _at_relocate_end.free = free(used);\n-  _at_relocate_end.free_high = free(used_low);\n-  _at_relocate_end.free_low = free(used_high);\n+  _at_relocate_end.free = free(stats.used());\n+  _at_relocate_end.free_high = free(stats.used_low());\n+  _at_relocate_end.free_low = free(stats.used_high());\n+  _at_relocate_end.used = stats.used();\n+  _at_relocate_end.used_high = stats.used_high();\n+  _at_relocate_end.used_low = stats.used_low();\n+  _at_relocate_end.allocated = allocated(stats.used(), stats.reclaimed());\n+  _at_relocate_end.garbage = garbage(stats.reclaimed());\n+  _at_relocate_end.reclaimed = stats.reclaimed();\n@@ -1396,9 +1376,0 @@\n-  log_info(gc, heap)(\"%s\", table()\n-                     .right(\"Reserve:\")\n-                     .left(ZTABLE_ARGS(_at_mark_start.reserve))\n-                     .left(ZTABLE_ARGS(_at_mark_end.reserve))\n-                     .left(ZTABLE_ARGS(_at_relocate_start.reserve))\n-                     .left(ZTABLE_ARGS(_at_relocate_end.reserve))\n-                     .left(ZTABLE_ARGS(_at_relocate_end.reserve_high))\n-                     .left(ZTABLE_ARGS(_at_relocate_end.reserve_low))\n-                     .end());\n","filename":"src\/hotspot\/share\/gc\/z\/zStat.cpp","additions":60,"deletions":89,"binary":false,"changes":149,"status":"modified"},{"patch":"@@ -38,0 +38,1 @@\n+class ZPageAllocatorStats;\n@@ -425,1 +426,1 @@\n-  static ZRelocationSetSelectorStats _stats;\n+  static ZRelocationSetSelectorStats _selector_stats;\n@@ -427,1 +428,2 @@\n-  static bool                        _success;\n+  static size_t                      _small_in_place_count;\n+  static size_t                      _medium_in_place_count;\n@@ -429,1 +431,3 @@\n-  static void print(const char* name, const ZRelocationSetSelectorGroupStats& group);\n+  static void print(const char* name,\n+                    const ZRelocationSetSelectorGroupStats& selector_group,\n+                    size_t in_place_count);\n@@ -432,1 +436,1 @@\n-  static void set_at_select_relocation_set(const ZRelocationSetSelectorStats& stats);\n+  static void set_at_select_relocation_set(const ZRelocationSetSelectorStats& selector_stats);\n@@ -434,1 +438,1 @@\n-  static void set_at_relocate_end(bool success);\n+  static void set_at_relocate_end(size_t small_in_place_count, size_t medium_in_place_count);\n@@ -486,1 +490,0 @@\n-    size_t max_reserve;\n@@ -492,2 +495,0 @@\n-    size_t reserve;\n-    size_t used;\n@@ -495,0 +496,1 @@\n+    size_t used;\n@@ -499,3 +501,0 @@\n-    size_t reserve;\n-    size_t allocated;\n-    size_t used;\n@@ -503,0 +502,1 @@\n+    size_t used;\n@@ -504,0 +504,1 @@\n+    size_t allocated;\n@@ -509,2 +510,2 @@\n-    size_t reserve;\n-    size_t garbage;\n+    size_t free;\n+    size_t used;\n@@ -512,0 +513,1 @@\n+    size_t garbage;\n@@ -513,2 +515,0 @@\n-    size_t used;\n-    size_t free;\n@@ -521,9 +521,0 @@\n-    size_t reserve;\n-    size_t reserve_high;\n-    size_t reserve_low;\n-    size_t garbage;\n-    size_t allocated;\n-    size_t reclaimed;\n-    size_t used;\n-    size_t used_high;\n-    size_t used_low;\n@@ -533,0 +524,6 @@\n+    size_t used;\n+    size_t used_high;\n+    size_t used_low;\n+    size_t allocated;\n+    size_t garbage;\n+    size_t reclaimed;\n@@ -537,2 +534,0 @@\n-  static size_t available(size_t used);\n-  static size_t reserve(size_t used);\n@@ -540,0 +535,2 @@\n+  static size_t allocated(size_t used, size_t reclaiemd);\n+  static size_t garbage(size_t reclaiemd);\n@@ -542,20 +539,6 @@\n-  static void set_at_initialize(size_t min_capacity,\n-                                size_t max_capacity,\n-                                size_t max_reserve);\n-  static void set_at_mark_start(size_t soft_max_capacity,\n-                                size_t capacity,\n-                                size_t used);\n-  static void set_at_mark_end(size_t capacity,\n-                              size_t allocated,\n-                              size_t used);\n-  static void set_at_select_relocation_set(const ZRelocationSetSelectorStats& stats,\n-                                           size_t reclaimed);\n-  static void set_at_relocate_start(size_t capacity,\n-                                    size_t allocated,\n-                                    size_t used);\n-  static void set_at_relocate_end(size_t capacity,\n-                                  size_t allocated,\n-                                  size_t reclaimed,\n-                                  size_t used,\n-                                  size_t used_high,\n-                                  size_t used_low);\n+  static void set_at_initialize(const ZPageAllocatorStats& stats);\n+  static void set_at_mark_start(const ZPageAllocatorStats& stats);\n+  static void set_at_mark_end(const ZPageAllocatorStats& stats);\n+  static void set_at_select_relocation_set(const ZRelocationSetSelectorStats& stats);\n+  static void set_at_relocate_start(const ZPageAllocatorStats& stats);\n+  static void set_at_relocate_end(const ZPageAllocatorStats& stats);\n","filename":"src\/hotspot\/share\/gc\/z\/zStat.hpp","additions":29,"deletions":46,"binary":false,"changes":75,"status":"modified"},{"patch":"@@ -40,1 +40,2 @@\n-  static void object_copy(uintptr_t from, uintptr_t to, size_t size);\n+  static void object_copy_disjoint(uintptr_t from, uintptr_t to, size_t size);\n+  static void object_copy_conjoint(uintptr_t from, uintptr_t to, size_t size);\n","filename":"src\/hotspot\/share\/gc\/z\/zUtils.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2017, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n@@ -48,1 +48,1 @@\n-inline void ZUtils::object_copy(uintptr_t from, uintptr_t to, size_t size) {\n+inline void ZUtils::object_copy_disjoint(uintptr_t from, uintptr_t to, size_t size) {\n@@ -52,0 +52,6 @@\n+inline void ZUtils::object_copy_conjoint(uintptr_t from, uintptr_t to, size_t size) {\n+  if (from != to) {\n+    Copy::aligned_conjoint_words((HeapWord*)from, (HeapWord*)to, bytes_to_words(size));\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/z\/zUtils.inline.hpp","additions":8,"deletions":2,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -62,0 +62,3 @@\n+  product(bool, ZStressRelocateInPlace, false, DIAGNOSTIC,                  \\\n+          \"Always relocate pages in-place\")                                 \\\n+                                                                            \\\n","filename":"src\/hotspot\/share\/gc\/z\/z_globals.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -1013,1 +1013,0 @@\n-     <Field type=\"boolean\" name=\"noReserve\" label=\"No Reserve\" \/>\n@@ -1019,2 +1018,1 @@\n-    <Field type=\"ulong\" contentType=\"bytes\" name=\"compactingFrom\" label=\"Compacting From\" \/>\n-    <Field type=\"ulong\" contentType=\"bytes\" name=\"compactingTo\" label=\"Compacting To\" \/>\n+    <Field type=\"ulong\" contentType=\"bytes\" name=\"relocate\" label=\"Relocate\" \/>\n@@ -1028,2 +1026,1 @@\n-    <Field type=\"ulong\" contentType=\"bytes\" name=\"compactingFrom\" label=\"Compacting From\" \/>\n-    <Field type=\"ulong\" contentType=\"bytes\" name=\"compactingTo\" label=\"Compacting To\" \/>\n+    <Field type=\"ulong\" contentType=\"bytes\" name=\"relocate\" label=\"Relocate\" \/>\n","filename":"src\/hotspot\/share\/jfr\/metadata\/metadata.xml","additions":2,"deletions":5,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -70,1 +70,3 @@\n-      EXPECT_FALSE(forwarding->find(from_index).populated()) << CAPTURE2(from_index, size);\n+      ZForwardingCursor cursor;\n+      ZForwardingEntry entry = forwarding->find(from_index, &cursor);\n+      EXPECT_FALSE(entry.populated()) << CAPTURE2(from_index, size);\n@@ -93,1 +95,2 @@\n-      ZForwardingEntry entry = forwarding->find(from_index);\n+      ZForwardingCursor cursor;\n+      ZForwardingEntry entry = forwarding->find(from_index, &cursor);\n@@ -135,1 +138,2 @@\n-      ZForwardingEntry entry = forwarding->find(from_index);\n+      ZForwardingCursor cursor;\n+      ZForwardingEntry entry = forwarding->find(from_index, &cursor);\n","filename":"test\/hotspot\/gtest\/gc\/z\/test_zForwarding.cpp","additions":7,"deletions":3,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -0,0 +1,74 @@\n+\/*\n+ * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package gc.z;\n+\n+\/*\n+ * @test TestRelocateInPlace\n+ * @requires vm.gc.Z\n+ * @summary Test ZGC in-place relocateion\n+ * @run main\/othervm -XX:+UseZGC -Xlog:gc*,gc+stats=off -Xmx256M -XX:+UnlockDiagnosticVMOptions -XX:+ZStressRelocateInPlace gc.z.TestRelocateInPlace\n+ *\/\n+\n+import java.util.ArrayList;\n+\n+public class TestRelocateInPlace {\n+    private static final int allocSize = 100 * 1024 * 1024; \/\/ 100M\n+    private static final int smallObjectSize = 4 * 1024; \/\/ 4K\n+    private static final int mediumObjectSize = 2 * 1024 * 1024; \/\/ 2M\n+\n+    private static volatile ArrayList<byte[]> keepAlive;\n+\n+    private static void allocate(int objectSize) {\n+        keepAlive = new ArrayList<>();\n+        for (int i = 0; i < allocSize; i+= objectSize) {\n+            keepAlive.add(new byte[objectSize]);\n+        }\n+    }\n+\n+    private static void fragment() {\n+        \/\/ Release every other reference to cause lots of fragmentation\n+        for (int i = 0; i < keepAlive.size(); i += 2) {\n+            keepAlive.set(i, null);\n+        }\n+    }\n+\n+    private static void test(int objectSize) throws Exception {\n+        System.out.println(\"Allocating\");\n+        allocate(objectSize);\n+\n+        System.out.println(\"Fragmenting\");\n+        fragment();\n+\n+        System.out.println(\"Reclaiming\");\n+        System.gc();\n+    }\n+\n+    public static void main(String[] args) throws Exception {\n+        for (int i = 0; i < 10; i++) {\n+            System.out.println(\"Iteration \" + i);\n+            test(smallObjectSize);\n+            test(mediumObjectSize);\n+        }\n+    }\n+}\n","filename":"test\/hotspot\/jtreg\/gc\/z\/TestRelocateInPlace.java","additions":74,"deletions":0,"binary":false,"changes":74,"status":"added"}]}