{"files":[{"patch":"@@ -70,6 +70,0 @@\n-#ifdef ASSERT\n-bool G1Allocator::has_mutator_alloc_region() {\n-  uint node_index = current_node_index();\n-  return mutator_alloc_region(node_index)->get() != nullptr;\n-}\n-#endif\n","filename":"src\/hotspot\/share\/gc\/g1\/g1Allocator.cpp","additions":0,"deletions":6,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -91,3 +91,0 @@\n-  \/\/ Node index of current thread.\n-  inline uint current_node_index() const;\n-\n@@ -100,4 +97,2 @@\n-#ifdef ASSERT\n-  \/\/ Do we currently have an active mutator region to allocate into?\n-  bool has_mutator_alloc_region();\n-#endif\n+  \/\/ Node index of current thread.\n+  inline uint current_node_index() const;\n@@ -123,0 +118,1 @@\n+  inline HeapWord* attempt_allocation_locked(size_t word_size, uint node_index);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1Allocator.hpp","additions":3,"deletions":7,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -67,0 +67,4 @@\n+  return attempt_allocation_locked(word_size, node_index);\n+}\n+\n+inline HeapWord* G1Allocator::attempt_allocation_locked(size_t word_size, uint node_index) {\n","filename":"src\/hotspot\/share\/gc\/g1\/g1Allocator.inline.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -77,0 +77,1 @@\n+#include \"gc\/shared\/collectedHeap.inline.hpp\"\n@@ -116,0 +117,1 @@\n+#include \"utilities\/doublyLinkedList.inline.hpp\"\n@@ -403,0 +405,15 @@\n+void G1CollectedHeap::attempt_allocation_after_gc(size_t word_size,\n+                                                  uint gc_count_before,\n+                                                  bool should_try_gc,\n+                                                  GCCause::Cause gc_cause) {\n+\n+  if (should_try_gc) {\n+    do_collection_pause(word_size, gc_count_before, gc_cause);\n+  } else {\n+    log_trace(gc, alloc)(\"%s: Stall until clear\", Thread::current()->name());\n+    \/\/ The GCLocker is either active or the GCLocker initiated\n+    \/\/ GC has not yet been performed. Stall until it is completed.\n+    GCLocker::stall_until_clear();\n+  }\n+}\n+\n@@ -419,2 +436,2 @@\n-  HeapWord* result = nullptr;\n-  for (uint try_count = 1, gclocker_retry_count = 0; \/* we'll return *\/; try_count += 1) {\n+  StalledAllocReq request(word_size, _allocator->current_node_index(), this);\n+  while (true) {\n@@ -427,0 +444,5 @@\n+      \/\/ Allocation request could have been satisfied while thread was blocked on the lock.\n+      if (!request.pending()){\n+        return request.result();\n+      }\n+\n@@ -429,1 +451,1 @@\n-      result = _allocator->attempt_allocation_locked(word_size);\n+      HeapWord* result = _allocator->attempt_allocation_locked(word_size);\n@@ -431,0 +453,1 @@\n+        assert(request.pending(), \"Sanity!\");\n@@ -442,0 +465,1 @@\n+          assert(request.pending(), \"Sanity!\");\n@@ -454,9 +478,1 @@\n-    if (should_try_gc) {\n-      bool succeeded;\n-      result = do_collection_pause(word_size, gc_count_before, &succeeded, GCCause::_g1_inc_collection_pause);\n-      if (result != nullptr) {\n-        assert(succeeded, \"only way to get back a non-null result\");\n-        log_trace(gc, alloc)(\"%s: Successfully scheduled collection returning \" PTR_FORMAT,\n-                             Thread::current()->name(), p2i(result));\n-        return result;\n-      }\n+    attempt_allocation_after_gc(request.size(), gc_count_before, should_try_gc, GCCause::_g1_inc_collection_pause);\n@@ -464,22 +480,7 @@\n-      if (succeeded) {\n-        \/\/ We successfully scheduled a collection which failed to allocate. No\n-        \/\/ point in trying to allocate further. We'll just return null.\n-        log_trace(gc, alloc)(\"%s: Successfully scheduled collection failing to allocate \"\n-                             SIZE_FORMAT \" words\", Thread::current()->name(), word_size);\n-        return nullptr;\n-      }\n-      log_trace(gc, alloc)(\"%s: Unsuccessfully scheduled collection allocating \" SIZE_FORMAT \" words\",\n-                           Thread::current()->name(), word_size);\n-    } else {\n-      \/\/ Failed to schedule a collection.\n-      if (gclocker_retry_count > GCLockerRetryAllocationCount) {\n-        log_warning(gc, alloc)(\"%s: Retried waiting for GCLocker too often allocating \"\n-                               SIZE_FORMAT \" words\", Thread::current()->name(), word_size);\n-        return nullptr;\n-      }\n-      log_trace(gc, alloc)(\"%s: Stall until clear\", Thread::current()->name());\n-      \/\/ The GCLocker is either active or the GCLocker initiated\n-      \/\/ GC has not yet been performed. Stall until it is and\n-      \/\/ then retry the allocation.\n-      GCLocker::stall_until_clear();\n-      gclocker_retry_count += 1;\n+    if (request.pending()) {\n+      \/\/ GC Safepoint did not handle our allocation request. We should retry.\n+      continue;\n+    } else if (request.failed()) {\n+      \/\/ VM successfully scheduled a collection which failed to allocate. No\n+      \/\/ point in trying to allocate further. We'll just return null.\n+      log_debug(gc, alloc)(\"%s: Failed to allocate %zu words\", Thread::current()->name(), request.size());\n@@ -488,20 +489,1 @@\n-    \/\/ We can reach here if we were unsuccessful in scheduling a\n-    \/\/ collection (because another thread beat us to it) or if we were\n-    \/\/ stalled due to the GC locker. In either can we should retry the\n-    \/\/ allocation attempt in case another thread successfully\n-    \/\/ performed a collection and reclaimed enough space. We do the\n-    \/\/ first attempt (without holding the Heap_lock) here and the\n-    \/\/ follow-on attempt will be at the start of the next loop\n-    \/\/ iteration (after taking the Heap_lock).\n-    size_t dummy = 0;\n-    result = _allocator->attempt_allocation(word_size, word_size, &dummy);\n-    if (result != nullptr) {\n-      return result;\n-    }\n-\n-    \/\/ Give a warning if we seem to be looping forever.\n-    if ((QueuedAllocationWarningCount > 0) &&\n-        (try_count % QueuedAllocationWarningCount == 0)) {\n-      log_warning(gc, alloc)(\"%s:  Retried allocation %u times for \" SIZE_FORMAT \" words\",\n-                             Thread::current()->name(), try_count, word_size);\n-    }\n+    return request.result();\n@@ -688,2 +670,2 @@\n-  HeapWord* result = nullptr;\n-  for (uint try_count = 1, gclocker_retry_count = 0; \/* we'll return *\/; try_count += 1) {\n+  StalledAllocReq request(word_size, _allocator->current_node_index(), this);\n+  while (true) {\n@@ -693,1 +675,0 @@\n-\n@@ -697,0 +678,5 @@\n+      \/\/ Allocation request could have been satisfied while thread was blocked on the lock.\n+      if (!request.pending()){\n+        return request.result();\n+      }\n+\n@@ -701,1 +687,1 @@\n-      result = humongous_obj_allocate(word_size);\n+      HeapWord* result = humongous_obj_allocate(word_size);\n@@ -705,0 +691,2 @@\n+\n+        assert(request.pending(), \"Sanity!\");\n@@ -716,12 +704,1 @@\n-    if (should_try_gc) {\n-      bool succeeded;\n-      result = do_collection_pause(word_size, gc_count_before, &succeeded, GCCause::_g1_humongous_allocation);\n-      if (result != nullptr) {\n-        assert(succeeded, \"only way to get back a non-null result\");\n-        log_trace(gc, alloc)(\"%s: Successfully scheduled collection returning \" PTR_FORMAT,\n-                             Thread::current()->name(), p2i(result));\n-        size_t size_in_regions = humongous_obj_size_in_regions(word_size);\n-        policy()->old_gen_alloc_tracker()->\n-          record_collection_pause_humongous_allocation(size_in_regions * HeapRegion::GrainBytes);\n-        return result;\n-      }\n+    attempt_allocation_after_gc(request.size(), gc_count_before, should_try_gc, GCCause::_g1_humongous_allocation);\n@@ -729,22 +706,7 @@\n-      if (succeeded) {\n-        \/\/ We successfully scheduled a collection which failed to allocate. No\n-        \/\/ point in trying to allocate further. We'll just return null.\n-        log_trace(gc, alloc)(\"%s: Successfully scheduled collection failing to allocate \"\n-                             SIZE_FORMAT \" words\", Thread::current()->name(), word_size);\n-        return nullptr;\n-      }\n-      log_trace(gc, alloc)(\"%s: Unsuccessfully scheduled collection allocating \" SIZE_FORMAT \"\",\n-                           Thread::current()->name(), word_size);\n-    } else {\n-      \/\/ Failed to schedule a collection.\n-      if (gclocker_retry_count > GCLockerRetryAllocationCount) {\n-        log_warning(gc, alloc)(\"%s: Retried waiting for GCLocker too often allocating \"\n-                               SIZE_FORMAT \" words\", Thread::current()->name(), word_size);\n-        return nullptr;\n-      }\n-      log_trace(gc, alloc)(\"%s: Stall until clear\", Thread::current()->name());\n-      \/\/ The GCLocker is either active or the GCLocker initiated\n-      \/\/ GC has not yet been performed. Stall until it is and\n-      \/\/ then retry the allocation.\n-      GCLocker::stall_until_clear();\n-      gclocker_retry_count += 1;\n+    if (request.pending()) {\n+      \/\/ GC Safepoint did not handle our allocation request. We should retry.\n+      continue;\n+    } else if (request.failed()) {\n+      \/\/ VM successfully scheduled a collection which failed to allocate. No\n+      \/\/ point in trying to allocate further. We'll just return null.\n+      log_debug(gc, alloc)(\"%s: Failed to allocate %zu words\", Thread::current()->name(), request.size());\n@@ -753,15 +715,1 @@\n-\n-    \/\/ We can reach here if we were unsuccessful in scheduling a\n-    \/\/ collection (because another thread beat us to it) or if we were\n-    \/\/ stalled due to the GC locker. In either can we should retry the\n-    \/\/ allocation attempt in case another thread successfully\n-    \/\/ performed a collection and reclaimed enough space.\n-    \/\/ Humongous object allocation always needs a lock, so we wait for the retry\n-    \/\/ in the next iteration of the loop, unlike for the regular iteration case.\n-    \/\/ Give a warning if we seem to be looping forever.\n-\n-    if ((QueuedAllocationWarningCount > 0) &&\n-        (try_count % QueuedAllocationWarningCount == 0)) {\n-      log_warning(gc, alloc)(\"%s: Retried allocation %u times for \" SIZE_FORMAT \" words\",\n-                             Thread::current()->name(), try_count, word_size);\n-    }\n+    return request.result();\n@@ -775,1 +723,1 @@\n-                                                           bool expect_null_mutator_alloc_region) {\n+                                                           uint node_index) {\n@@ -777,2 +725,0 @@\n-  assert(!_allocator->has_mutator_alloc_region() || !expect_null_mutator_alloc_region,\n-         \"the current alloc region was unexpectedly found to be non-null\");\n@@ -781,1 +727,1 @@\n-    return _allocator->attempt_allocation_locked(word_size);\n+    return _allocator->attempt_allocation_locked(word_size, node_index);\n@@ -950,0 +896,3 @@\n+\n+  reset_allocation_requests();\n+\n@@ -975,5 +924,2 @@\n-                                                            bool do_gc,\n-                                                            bool maximal_compaction,\n-                                                            bool expect_null_mutator_alloc_region,\n-                                                            bool* gc_succeeded) {\n-  *gc_succeeded = true;\n+                                                            uint node_index,\n+                                                            bool expand_heap) {\n@@ -981,4 +927,2 @@\n-  HeapWord* result =\n-    attempt_allocation_at_safepoint(word_size,\n-                                    expect_null_mutator_alloc_region);\n-  if (result != nullptr) {\n+  HeapWord* result = attempt_allocation_at_safepoint(word_size, node_index);\n+  if (result != nullptr || !expand_heap) {\n@@ -992,3 +936,2 @@\n-  result = expand_and_allocate(word_size);\n-  if (result != nullptr) {\n-    return result;\n+  if (expand(word_size)) {\n+    return attempt_allocation_at_safepoint(word_size, node_index);\n@@ -996,0 +939,2 @@\n+  return nullptr;\n+}\n@@ -997,12 +942,3 @@\n-  if (do_gc) {\n-    GCCauseSetter compaction(this, GCCause::_g1_compaction_pause);\n-    \/\/ Expansion didn't work, we'll try to do a Full GC.\n-    \/\/ If maximal_compaction is set we clear all soft references and don't\n-    \/\/ allow any dead wood to be left on the heap.\n-    if (maximal_compaction) {\n-      log_info(gc, ergo)(\"Attempting maximal full compaction clearing soft references\");\n-    } else {\n-      log_info(gc, ergo)(\"Attempting full compaction\");\n-    }\n-    *gc_succeeded = do_full_collection(maximal_compaction \/* clear_all_soft_refs *\/ ,\n-                                       maximal_compaction \/* do_maximal_compaction *\/);\n+bool G1CollectedHeap::is_unclaimed_allocation(HeapWord *obj) {\n+  if (!has_stalled_allocations()) {\n+    return false;\n@@ -1011,1 +947,6 @@\n-  return nullptr;\n+  for(StalledAllocReq* alloc_req : _stalled_allocations) {\n+    if (alloc_req->succeeded() && alloc_req->result() == obj) {\n+      return true;\n+    }\n+  }\n+  return false;\n@@ -1014,2 +955,1 @@\n-HeapWord* G1CollectedHeap::satisfy_failed_allocation(size_t word_size,\n-                                                     bool* succeeded) {\n+bool G1CollectedHeap::satisfy_failed_allocations(bool* gc_succeeded) {\n@@ -1018,7 +958,1 @@\n-  \/\/ Attempts to allocate followed by Full GC.\n-  HeapWord* result =\n-    satisfy_failed_allocation_helper(word_size,\n-                                     true,  \/* do_gc *\/\n-                                     false, \/* maximum_collection *\/\n-                                     false, \/* expect_null_mutator_alloc_region *\/\n-                                     succeeded);\n+  bool alloc_succeeded = handle_allocation_requests(true \/* expand_heap *\/ );\n@@ -1026,2 +960,2 @@\n-  if (result != nullptr || !*succeeded) {\n-    return result;\n+  if (alloc_succeeded) {\n+    return alloc_succeeded;\n@@ -1030,6 +964,3 @@\n-  \/\/ Attempts to allocate followed by Full GC that will collect all soft references.\n-  result = satisfy_failed_allocation_helper(word_size,\n-                                            true, \/* do_gc *\/\n-                                            true, \/* maximum_collection *\/\n-                                            true, \/* expect_null_mutator_alloc_region *\/\n-                                            succeeded);\n+  \/\/ Attempt to satisfy allocation requests failed; reset the satisfied requests,\n+  \/\/ execute a full gc, then try again.\n+  reset_allocation_requests();\n@@ -1037,2 +968,4 @@\n-  if (result != nullptr || !*succeeded) {\n-    return result;\n+  *gc_succeeded = do_full_collection(false \/* clear_all_soft_refs *\/, false \/* do_maximal_compaction *\/);\n+\n+  if (!*gc_succeeded) {\n+    return false;\n@@ -1041,6 +974,1 @@\n-  \/\/ Attempts to allocate, no GC\n-  result = satisfy_failed_allocation_helper(word_size,\n-                                            false, \/* do_gc *\/\n-                                            false, \/* maximum_collection *\/\n-                                            true,  \/* expect_null_mutator_alloc_region *\/\n-                                            succeeded);\n+  alloc_succeeded = handle_allocation_requests(true \/* expand_heap *\/);\n@@ -1048,2 +976,26 @@\n-  if (result != nullptr) {\n-    return result;\n+  if (alloc_succeeded) {\n+    return alloc_succeeded;\n+  }\n+\n+  \/\/ Attempt to satisfy allocation requests after full gc also failed. We reset the allocation requests\n+  \/\/ then execute a maximal compaction full gc before retrying the allocations.\n+  reset_allocation_requests();\n+\n+  *gc_succeeded = do_full_collection(true \/* clear_all_soft_refs *\/, true \/* do_maximal_compaction *\/);\n+\n+  if (!*gc_succeeded) {\n+    return false;\n+  }\n+\n+  alloc_succeeded = handle_allocation_requests(true \/* expand_heap *\/);\n+\n+  if (alloc_succeeded) {\n+    return alloc_succeeded;\n+  }\n+\n+  \/\/ Even after maximal compaction full gc, we failed to satisfy all the pending allocation requests.\n+  \/\/ We can declare the pending requests as failed.\n+  for (StalledAllocReq* alloc_req : _stalled_allocations) {\n+    if (alloc_req->pending()) {\n+      alloc_req->set_state(StalledAllocReq::AllocationState::Failed);\n+    }\n@@ -1054,0 +1006,2 @@\n+  return false;\n+}\n@@ -1055,5 +1009,8 @@\n-  \/\/ What else?  We might try synchronous finalization later.  If the total\n-  \/\/ space available is large enough for the allocation, then a more\n-  \/\/ complete compaction phase than we've tried so far might be\n-  \/\/ appropriate.\n-  return nullptr;\n+void G1CollectedHeap::reset_allocation_requests() {\n+  assert_at_safepoint_on_vm_thread();\n+\n+  for (StalledAllocReq* alloc_req : _stalled_allocations) {\n+    if (alloc_req->succeeded()) {\n+      alloc_req->set_state(StalledAllocReq::AllocationState::Pending);\n+    }\n+  }\n@@ -1062,4 +1019,33 @@\n-\/\/ Attempting to expand the heap sufficiently\n-\/\/ to support an allocation of the given \"word_size\".  If\n-\/\/ successful, perform the allocation and return the address of the\n-\/\/ allocated block, or else null.\n+bool G1CollectedHeap::handle_allocation_requests(bool expand_heap) {\n+  assert_at_safepoint_on_vm_thread();\n+\n+  for (StalledAllocReq* alloc_req : _stalled_allocations) {\n+    if (alloc_req->failed()) {\n+      \/\/ If an allocation request was declared failed, we maintain the failed state.\n+      \/\/ This prevents requests from flip-floping between Pending->Failed->Pending states.\n+      \/\/ Once any allocation attempt declares a request as failed, that request will not be re-attempted.\n+      continue;\n+     }\n+\n+    HeapWord* result = satisfy_failed_allocation_helper(alloc_req->size(),\n+                                                        alloc_req->node_index(),\n+                                                        expand_heap);\n+\n+    if (result == nullptr) {\n+      \/\/ Failed to allocate, give up.\n+      return false;\n+    }\n+\n+    \/\/ Allocation succeeded, update the state and result of the allocation request.\n+    alloc_req->set_state(StalledAllocReq::AllocationState::Success, result);\n+\n+    \/\/ Initialize memory locations with filler objects such that the heap is passable\n+    \/\/ even when there is a delay between the allocation and object initialization\n+    \/\/ by the requesting mutator thread.\n+    if (is_humongous(alloc_req->size())) {\n+      \/\/ Calculate payload size and initialize the humongous object with a fillerArray.\n+      size_t words = alloc_req->size();\n+\n+      const size_t payload_size = words - CollectedHeap::filler_array_hdr_size();\n+      const size_t len = payload_size * HeapWordSize \/ sizeof(jint);\n+      assert((int)len >= 0, \"size too large %zu becomes %d\", words, (int)len);\n@@ -1067,1 +1053,14 @@\n-HeapWord* G1CollectedHeap::expand_and_allocate(size_t word_size) {\n+      ObjArrayAllocator allocator(Universe::fillerArrayKlassObj(), words, (int)len, \/* do_zero *\/ false);\n+      allocator.initialize(result);\n+\n+      size_t size_in_regions = humongous_obj_size_in_regions(alloc_req->size());\n+      policy()->old_gen_alloc_tracker()->\n+          record_collection_pause_humongous_allocation(size_in_regions * HeapRegion::GrainBytes);\n+    } else {\n+      CollectedHeap::fill_with_objects(result, alloc_req->size());\n+    }\n+  }\n+  return true;\n+}\n+\n+bool G1CollectedHeap::expand(size_t word_size) {\n@@ -1073,3 +1072,1 @@\n-  log_debug(gc, ergo, heap)(\"Attempt heap expansion (allocation request failed). Allocation request: \" SIZE_FORMAT \"B\",\n-                            word_size * HeapWordSize);\n-\n+  log_debug(gc, ergo, heap)(\"Attempt heap expansion. Expansion request: %zuB\", expand_bytes);\n@@ -1080,2 +1077,1 @@\n-    return attempt_allocation_at_safepoint(word_size,\n-                                           false \/* expect_null_mutator_alloc_region *\/);\n+    return true;\n@@ -1083,1 +1079,1 @@\n-  return nullptr;\n+  return false;\n@@ -1223,0 +1219,2 @@\n+  _alloc_request_lock(Mutex::nosafepoint, \"G1 Stalled Allocation List\"),\n+  _stalled_allocations(),\n@@ -2369,1 +2367,0 @@\n-                                               bool* succeeded,\n@@ -2379,1 +2376,0 @@\n-  *succeeded = ret_succeeded;\n@@ -2498,0 +2494,2 @@\n+  reset_allocation_requests();\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectedHeap.cpp","additions":179,"deletions":181,"binary":false,"changes":360,"status":"modified"},{"patch":"@@ -62,0 +62,1 @@\n+#include \"utilities\/doublyLinkedList.inline.hpp\"\n@@ -171,0 +172,77 @@\n+  \/\/ StalledAllocReq represents a stalled allocation request in a doubly linked list.\n+  \/\/\n+  \/\/ It provides functionality to track and manage an allocation request. The\n+  \/\/ allocation request encapsulates information such as the size of the allocation,\n+  \/\/ the NUMA (Non-Uniform Memory Access) node index, the state of the allocation,\n+  \/\/ and the resulting memory address.\n+  \/\/\n+  \/\/ The state of an allocation request can transition from Pending to Failed or\n+  \/\/ Success. Failed state is a terminating state, indicating that the allocation\n+  \/\/ request has failed. However, the Success state may be reset to Pending if an\n+  \/\/ interfering safepoint observes the successful allocation before the requesting\n+  \/\/ thread.\n+  class StalledAllocReq : public DoublyLinkedListNode {\n+  public:\n+\n+    enum class AllocationState {\n+      Pending,\n+      Success,\n+      Failed,\n+    };\n+\n+    StalledAllocReq(size_t size, uint numa_node, G1CollectedHeap* g1h) :\n+      _size(size),\n+      _result(nullptr),\n+      _node_index(numa_node),\n+      _state(AllocationState::Pending),\n+      _g1h(g1h)\n+    {\n+      if (_g1h != nullptr) {\n+        _g1h->enqueue_req(this);\n+      }\n+    }\n+\n+    ~StalledAllocReq() {\n+      if (_g1h != nullptr) {\n+        assert(_result == nullptr || succeeded(), \"Sanity\");\n+        _g1h->dequeue_req(this);\n+      }\n+    }\n+\n+    size_t size() { return _size; }\n+\n+    uint node_index() const { return _node_index; }\n+\n+    void set_state(AllocationState state, HeapWord* result = nullptr) {\n+      _state = state;\n+      _result = result;\n+    }\n+\n+    AllocationState state() { return _state; }\n+    bool failed() { return _state == AllocationState::Failed; }\n+    bool succeeded() { return _state == AllocationState::Success; }\n+    bool pending() { return _state == AllocationState::Pending; }\n+\n+    HeapWord* result() { return _result; }\n+\n+  private:\n+    const size_t _size;\n+    HeapWord* volatile _result;\n+    const uint _node_index;\n+    volatile AllocationState _state;\n+    G1CollectedHeap* _g1h;\n+  };\n+\n+  Mutex _alloc_request_lock;\n+  DoublyLinkedList<StalledAllocReq> _stalled_allocations;\n+\n+  void enqueue_req(StalledAllocReq *req) {\n+    MutexLocker ml(&_alloc_request_lock, Mutex::_no_safepoint_check_flag);\n+    _stalled_allocations.insert_last(req);\n+  }\n+\n+  void dequeue_req(StalledAllocReq *req) {\n+    MutexLocker ml(&_alloc_request_lock, Mutex::_no_safepoint_check_flag);\n+    _stalled_allocations.remove(req);\n+  }\n+\n@@ -198,0 +276,4 @@\n+\n+  bool has_stalled_allocations() { return !_stalled_allocations.is_empty(); }\n+  bool is_unclaimed_allocation(HeapWord *obj);\n+\n@@ -462,0 +544,5 @@\n+  void attempt_allocation_after_gc(size_t word_size,\n+                                   uint gc_count_before,\n+                                   bool should_try_gc,\n+                                   GCCause::Cause gc_cause);\n+\n@@ -463,5 +550,3 @@\n-  \/\/ at the end of a successful GC). expect_null_mutator_alloc_region\n-  \/\/ specifies whether the mutator alloc region is expected to be null\n-  \/\/ or not.\n-  HeapWord* attempt_allocation_at_safepoint(size_t word_size,\n-                                            bool expect_null_mutator_alloc_region);\n+  \/\/ at the end of a successful GC). node_index speficies the memory node\n+  \/\/ to use for allocation.\n+  HeapWord* attempt_allocation_at_safepoint(size_t word_size, uint node_index);\n@@ -500,2 +585,6 @@\n-  HeapWord* satisfy_failed_allocation(size_t word_size,\n-                                      bool* succeeded);\n+  bool satisfy_failed_allocations(bool* gc_succeeded);\n+  bool handle_allocation_requests(bool expand_heap);\n+\n+  \/\/ Reset any allocated but unclaimed allocation requests.\n+  void reset_allocation_requests();\n+\n@@ -512,1 +601,1 @@\n-  \/\/ Helper method for satisfy_failed_allocation()\n+  \/\/ Helper method for handle_allocation_requests()\n@@ -514,4 +603,2 @@\n-                                             bool do_gc,\n-                                             bool maximal_compaction,\n-                                             bool expect_null_mutator_alloc_region,\n-                                             bool* gc_succeeded);\n+                                             uint node_index,\n+                                             bool expand_heap);\n@@ -519,5 +606,2 @@\n-  \/\/ Attempting to expand the heap sufficiently\n-  \/\/ to support an allocation of the given \"word_size\".  If\n-  \/\/ successful, perform the allocation and return the address of the\n-  \/\/ allocated block, or else null.\n-  HeapWord* expand_and_allocate(size_t word_size);\n+  \/\/ Attempts to expand the heap by \"word_size\" words.\n+  bool expand(size_t word_size);\n@@ -577,1 +661,1 @@\n-  bool expand(size_t expand_bytes, WorkerThreads* pretouch_workers = nullptr, double* expand_time_ms = nullptr);\n+  bool expand(size_t expand_bytes, WorkerThreads* pretouch_workers, double* expand_time_ms = nullptr);\n@@ -737,6 +821,3 @@\n-  \/\/ satisfy an allocation request of word_size. *succeeded will\n-  \/\/ return whether the VM operation was successful (it did do an\n-  \/\/ evacuation pause) or not (another thread beat us to it or the GC\n-  \/\/ locker was active). Given that we should not be holding the\n-  \/\/ Heap_lock when we enter this method, we will pass the\n-  \/\/ gc_count_before (i.e., total_collections()) as a parameter since\n+  \/\/ satisfy any pending allocation requests. Given that we should\n+  \/\/ not be holding the Heap_lock when we enter this method, we will pass\n+  \/\/ the gc_count_before (i.e., total_collections()) as a parameter since\n@@ -748,1 +829,0 @@\n-                                bool*          succeeded,\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectedHeap.hpp","additions":105,"deletions":25,"binary":false,"changes":130,"status":"modified"},{"patch":"@@ -1109,0 +1109,3 @@\n+        if (_g1h->is_unclaimed_allocation(hr->humongous_start_region()->bottom())) {\n+          return;\n+        }\n@@ -1158,0 +1161,3 @@\n+          if (_g1h->is_unclaimed_allocation(hr->humongous_start_region()->bottom())) {\n+            return;\n+          }\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ConcurrentMark.cpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -54,0 +54,7 @@\n+\n+  \/\/ Any allocation requests that were handled during a previous GC safepoint but\n+  \/\/ have not been observed by the requesting mutator thread should be reset to\n+  \/\/ pending. This makes it easier for the current GC to treat the unclaimed memory\n+  \/\/ as garbage.\n+  g1h->reset_allocation_requests();\n+\n@@ -128,0 +135,1 @@\n+  \/\/ Only if operation was really triggered by an allocation.\n@@ -129,7 +137,3 @@\n-    \/\/ An allocation has been requested. So, try to do that first.\n-    _result = g1h->attempt_allocation_at_safepoint(_word_size,\n-                                                   false \/* expect_null_cur_alloc_region *\/);\n-    if (_result != nullptr) {\n-      \/\/ If we can successfully allocate before we actually do the\n-      \/\/ pause then we will consider this pause successful.\n-      _gc_succeeded = true;\n+    bool alloc_succeeded = g1h->handle_allocation_requests(false \/* expand_heap *\/);\n+\n+    if (alloc_succeeded) {\n@@ -140,0 +144,3 @@\n+  \/\/ Reset any satisfied allocation requests before attempting the collection.\n+  g1h->reset_allocation_requests();\n+\n@@ -141,0 +148,1 @@\n+  bool gc_succeeded = false;\n@@ -144,11 +152,11 @@\n-  if (_gc_succeeded) {\n-    if (_word_size > 0) {\n-      \/\/ An allocation had been requested. Do it, eventually trying a stronger\n-      \/\/ kind of GC.\n-      _result = g1h->satisfy_failed_allocation(_word_size, &_gc_succeeded);\n-    } else if (g1h->should_upgrade_to_full_gc()) {\n-      \/\/ There has been a request to perform a GC to free some space. We have no\n-      \/\/ information on how much memory has been asked for. In case there are\n-      \/\/ absolutely no regions left to allocate into, do a full compaction.\n-      _gc_succeeded = g1h->upgrade_to_full_collection();\n-    }\n+  if (!_gc_succeeded) {\n+    return;\n+  }\n+\n+  if (!g1h->_stalled_allocations.is_empty()) {\n+    g1h->satisfy_failed_allocations(&_gc_succeeded);\n+  } else if (g1h->should_upgrade_to_full_gc()) {\n+    \/\/ There has been a request to perform a GC to free some space. We have no\n+    \/\/ information on how much memory has been asked for. In case there are\n+    \/\/ absolutely no regions left to allocate into, do a full compaction.\n+    _gc_succeeded = g1h->upgrade_to_full_collection();\n","filename":"src\/hotspot\/share\/gc\/g1\/g1VMOperations.cpp","additions":26,"deletions":18,"binary":false,"changes":44,"status":"modified"},{"patch":"@@ -429,8 +429,0 @@\n-size_t CollectedHeap::filler_array_hdr_size() {\n-  return align_object_offset(arrayOopDesc::header_size(T_INT)); \/\/ align to Long\n-}\n-\n-size_t CollectedHeap::filler_array_min_size() {\n-  return align_object_size(filler_array_hdr_size()); \/\/ align to MinObjAlignment\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.cpp","additions":0,"deletions":8,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -49,0 +49,8 @@\n+size_t CollectedHeap::filler_array_hdr_size() {\n+  return align_object_offset(arrayOopDesc::header_size(T_INT)); \/\/ Align to INT.\n+}\n+\n+size_t CollectedHeap::filler_array_min_size() {\n+  return align_object_size(filler_array_hdr_size()); \/\/ Align to MinObjAlignment.\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.inline.hpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -0,0 +1,176 @@\n+\/*\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_UTILITIES_DOUBLYLINKEDLIST_HPP\n+#define SHARE_UTILITIES_DOUBLYLINKEDLIST_HPP\n+\n+#include \"memory\/allocation.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+template <typename T>\n+class DoublyLinkedList;\n+\n+\/\/ Element in a doubly linked list.\n+\n+class DoublyLinkedListNode {\n+  template<typename T>\n+  friend class DoublyLinkedList;\n+\n+  DoublyLinkedListNode* _next;\n+  DoublyLinkedListNode* _prev;\n+\n+  NONCOPYABLE(DoublyLinkedListNode);\n+\n+  void swap(DoublyLinkedListNode* rhs);\n+\n+  void verify_links() const;\n+  void verify_links_linked() const;\n+  void verify_links_unlinked() const;\n+\n+protected:\n+  ~DoublyLinkedListNode();\n+\n+public:\n+  DoublyLinkedListNode();\n+};\n+\n+\/\/\n+\/\/ The DoublyLinkedList template provides efficient insertion, removal, and traversal of elements in a doubly linked list structure.\n+\/\/ Each node in the list contains an element of type T, and the nodes are connected bidirectionally.\n+\/\/ The class supports forward and backward traversal, as well as insertion and removal operations at the beginning, end,\n+\/\/ or any position within the list.\n+\/\/\n+\/\/ \\tparam T The type of elements stored in the linked list. It must be default-constructible and derived from DoublyLinkedListNode.\n+\/\/\n+\/\/ Note: The DoublyLinkedList class does not perform memory allocation or deallocation for the elements.\n+\/\/       Therefore, it is the responsibility of the class template users to manage the memory of the elements added or removed from the list.\n+\/\/\n+template <typename T>\n+class DoublyLinkedList {\n+  static_assert(std::is_base_of<DoublyLinkedListNode, T>::value,\n+                \"DoublyLinkedList requires elements derived from DoublyLinkedListNode\");\n+\n+  DoublyLinkedListNode _head;\n+  size_t _size;\n+\n+  NONCOPYABLE(DoublyLinkedList);\n+\n+  void verify_head() const;\n+\n+  void insert(DoublyLinkedListNode* before, DoublyLinkedListNode* node);\n+\n+  T* cast_to_outer(DoublyLinkedListNode* node) const;\n+\n+  T* next(T* elem) const;\n+  T* prev(T* elem) const;\n+\n+public:\n+  DoublyLinkedList();\n+\n+  size_t size() const;\n+  bool is_empty() const;\n+\n+  T* first() const;\n+  T* last() const;\n+\n+  void insert_first(T* elem);\n+  void insert_last(T* elem);\n+  void insert_before(T* before, T* elem);\n+  void insert_after(T* after, T* elem);\n+\n+  void remove(T* elem);\n+  T* remove_first();\n+  T* remove_last();\n+\n+  void swap(DoublyLinkedList& other);\n+\n+  class Iterator;\n+\n+  class RemoveIterator;\n+\n+  Iterator begin();\n+  Iterator end();\n+};\n+\n+template <typename T>\n+class DoublyLinkedList<T>::Iterator : public StackObj {\n+  friend class DoublyLinkedList<T>;\n+\n+  const DoublyLinkedList<T>* const _list;\n+  T* _cur_node;\n+\n+  Iterator(const DoublyLinkedList<T>* list, T* start) :\n+    _list(list),\n+    _cur_node(start)\n+  { }\n+\n+public:\n+  Iterator& operator++() {\n+    _cur_node = _list->next(_cur_node);\n+    return *this;\n+  }\n+\n+  Iterator operator++(int) {\n+    Iterator tmp = *this;\n+    ++(*this);\n+    return tmp;\n+  }\n+\n+  Iterator& operator--() {\n+    assert(_cur_node != nullptr, \"Sanity\");\n+    _cur_node = _list->prev(_cur_node);\n+    return *this;\n+  }\n+\n+  Iterator operator--(int) {\n+    Iterator tmp = *this;\n+    --(*this);\n+    return tmp;\n+  }\n+\n+  T* operator*() { return _cur_node; }\n+\n+  bool operator==(const Iterator& rhs) {\n+    assert(_list == rhs._list, \"iterator belongs to different List\");\n+    return _cur_node == rhs._cur_node;\n+  }\n+\n+  bool operator!=(const Iterator& rhs) {\n+    assert(_list == rhs._list, \"iterator belongs to different List\");\n+    return _cur_node != rhs._cur_node;\n+  }\n+};\n+\n+template <typename T>\n+class DoublyLinkedList<T>::RemoveIterator : public StackObj {\n+private:\n+  DoublyLinkedList<T>* const _list;\n+  const bool _forward;\n+\n+public:\n+  explicit RemoveIterator(DoublyLinkedList<T>* list, bool forward_iterate = true);\n+\n+  bool next(T** elem);\n+};\n+\n+#endif \/\/ SHARE_UTILITIES_DOUBLYLINKEDLIST_HPP\n","filename":"src\/hotspot\/share\/utilities\/doublyLinkedList.hpp","additions":176,"deletions":0,"binary":false,"changes":176,"status":"added"},{"patch":"@@ -0,0 +1,243 @@\n+\/*\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_UTILITIES_DOUBLYLINKEDLIST_INLINE_HPP\n+#define SHARE_UTILITIES_DOUBLYLINKEDLIST_INLINE_HPP\n+\n+#include \"utilities\/doublyLinkedList.hpp\"\n+#include \"utilities\/debug.hpp\"\n+\n+inline DoublyLinkedListNode::DoublyLinkedListNode() :\n+    _next(this),\n+    _prev(this) {}\n+\n+inline DoublyLinkedListNode::~DoublyLinkedListNode() {\n+  verify_links_unlinked();\n+}\n+\n+inline void DoublyLinkedListNode::swap(DoublyLinkedListNode* rhs) {\n+  if (this == rhs) {\n+    return;\n+  }\n+\n+  if (this->_next != this) {\n+    if (rhs->_next != rhs) {\n+      ::swap(this->_next, rhs->_next);\n+      ::swap(this->_prev,rhs->_prev);\n+      this->_next->_prev = this->_prev->_next = this;\n+      rhs->_next->_prev = rhs->_prev->_next = rhs;\n+    } else {\n+      rhs->_next = this->_next;\n+      rhs->_prev = this->_prev;\n+      rhs->_next->_prev = rhs->_prev->_next = rhs;\n+      this->_next = this->_prev = this;\n+    }\n+  } else if (rhs->_next != rhs) {\n+    this->_next = rhs->_next;\n+    this->_prev = rhs->_prev;\n+    this->_next->_prev = this->_prev->_next = this;\n+    rhs->_next = rhs->_prev = rhs;\n+  }\n+}\n+\n+inline void DoublyLinkedListNode::verify_links() const {\n+  assert(_next->_prev == this, \"Corrupt list node\");\n+  assert(_prev->_next == this, \"Corrupt list node\");\n+}\n+\n+inline void DoublyLinkedListNode::verify_links_linked() const {\n+  assert(_next != this, \"Should be in a list\");\n+  assert(_prev != this, \"Should be in a list\");\n+  verify_links();\n+}\n+\n+inline void DoublyLinkedListNode::verify_links_unlinked() const {\n+  assert(_next == this, \"Should not be in a list\");\n+  assert(_prev == this, \"Should not be in a list\");\n+}\n+\n+template <typename T>\n+inline void DoublyLinkedList<T>::verify_head() const {\n+  _head.verify_links();\n+}\n+\n+template <typename T>\n+inline void DoublyLinkedList<T>::insert(DoublyLinkedListNode* before, DoublyLinkedListNode* node) {\n+  verify_head();\n+\n+  before->verify_links();\n+  node->verify_links_unlinked();\n+\n+  node->_prev = before;\n+  node->_next = before->_next;\n+  before->_next = node;\n+  node->_next->_prev = node;\n+\n+  before->verify_links_linked();\n+  node->verify_links_linked();\n+\n+  _size++;\n+}\n+\n+template <typename T>\n+inline T* DoublyLinkedList<T>::cast_to_outer(DoublyLinkedListNode* node) const {\n+  return static_cast<T*>(node);\n+}\n+\n+template <typename T>\n+inline DoublyLinkedList<T>::DoublyLinkedList() :\n+    _head(),\n+    _size(0) {\n+  verify_head();\n+}\n+\n+template <typename T>\n+inline void DoublyLinkedList<T>::swap(DoublyLinkedList& other) {\n+\n+  _head.swap(&other._head);\n+  ::swap(this->_size, other._size);\n+\n+  other.verify_head();\n+  verify_head();\n+}\n+\n+template <typename T>\n+inline size_t DoublyLinkedList<T>::size() const {\n+  verify_head();\n+  return _size;\n+}\n+\n+template <typename T>\n+inline bool DoublyLinkedList<T>::is_empty() const {\n+  return size() == 0;\n+}\n+\n+template <typename T>\n+inline T* DoublyLinkedList<T>::first() const {\n+  return is_empty() ? nullptr : cast_to_outer(_head._next);\n+}\n+\n+template <typename T>\n+inline T* DoublyLinkedList<T>::last() const {\n+  return is_empty() ? nullptr : cast_to_outer(_head._prev);\n+}\n+\n+template <typename T>\n+inline T* DoublyLinkedList<T>::next(T* elem) const {\n+  verify_head();\n+\n+  return cast_to_outer(elem->_next);\n+}\n+\n+template <typename T>\n+inline T* DoublyLinkedList<T>::prev(T* elem) const {\n+  verify_head();\n+\n+  return cast_to_outer(elem->_prev);\n+}\n+\n+template <typename T>\n+inline void DoublyLinkedList<T>::insert_first(T* elem) {\n+  insert(&_head, elem);\n+}\n+\n+template <typename T>\n+inline void DoublyLinkedList<T>::insert_last(T* elem) {\n+  insert(_head._prev, elem);\n+}\n+\n+template <typename T>\n+inline void DoublyLinkedList<T>::insert_before(T* before, T* elem) {\n+  insert(before->_prev, elem);\n+}\n+\n+template <typename T>\n+inline void DoublyLinkedList<T>::insert_after(T* after, T* elem) {\n+  insert(after, elem);\n+}\n+\n+template <typename T>\n+inline void DoublyLinkedList<T>::remove(T* elem) {\n+  verify_head();\n+\n+  elem->verify_links_linked();\n+\n+  DoublyLinkedListNode* const next = elem->_next;\n+  DoublyLinkedListNode* const prev = elem->_prev;\n+  next->verify_links_linked();\n+  prev->verify_links_linked();\n+\n+  elem->_next = prev->_next;\n+  elem->_prev = next->_prev;\n+  elem->verify_links_unlinked();\n+\n+  next->_prev = prev;\n+  prev->_next = next;\n+  next->verify_links();\n+  prev->verify_links();\n+\n+  assert(_size > 0, \"Sanity check!\");\n+  _size--;\n+}\n+\n+template <typename T>\n+inline T* DoublyLinkedList<T>::remove_first() {\n+  T* elem = first();\n+  if (elem != nullptr) {\n+    remove(elem);\n+  }\n+  return elem;\n+}\n+\n+template <typename T>\n+inline T* DoublyLinkedList<T>::remove_last() {\n+  T* elem = last();\n+  if (elem != nullptr) {\n+    remove(elem);\n+  }\n+  return elem;\n+}\n+\n+template <typename T>\n+inline typename DoublyLinkedList<T>::Iterator DoublyLinkedList<T>::begin() {\n+ return Iterator(this, cast_to_outer(_head._next));\n+}\n+\n+template <typename T>\n+inline typename DoublyLinkedList<T>::Iterator DoublyLinkedList<T>::end() {\n+ return Iterator(this, cast_to_outer(&_head));\n+}\n+\n+template <typename T>\n+inline DoublyLinkedList<T>::RemoveIterator::RemoveIterator(DoublyLinkedList<T>* list, bool forward_iterate):\n+    _list(list),\n+    _forward(forward_iterate) {}\n+\n+\n+template <typename T>\n+inline bool DoublyLinkedList<T>::RemoveIterator::next(T** elem) {\n+  *elem = _forward ? _list->remove_first() : _list->remove_last();\n+  return *elem != nullptr;\n+}\n+\n+#endif \/\/ SHARE_UTILITIES_DOUBLYLINKEDLIST_INLINE_HPP\n","filename":"src\/hotspot\/share\/utilities\/doublyLinkedList.inline.hpp","additions":243,"deletions":0,"binary":false,"changes":243,"status":"added"}]}