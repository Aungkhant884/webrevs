{"files":[{"patch":"@@ -73,0 +73,4 @@\n+  return has_mutator_alloc_region(node_index);\n+}\n+\n+bool G1Allocator::has_mutator_alloc_region(uint node_index) {\n","filename":"src\/hotspot\/share\/gc\/g1\/g1Allocator.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -91,3 +91,0 @@\n-  \/\/ Node index of current thread.\n-  inline uint current_node_index() const;\n-\n@@ -100,0 +97,3 @@\n+  \/\/ Node index of current thread.\n+  inline uint current_node_index() const;\n+\n@@ -103,0 +103,1 @@\n+  bool has_mutator_alloc_region(uint node_index);\n@@ -123,0 +124,1 @@\n+  inline HeapWord* attempt_allocation_locked(size_t word_size, uint node_index);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1Allocator.hpp","additions":5,"deletions":3,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -67,0 +67,4 @@\n+  return attempt_allocation_locked(word_size, node_index);\n+}\n+\n+inline HeapWord* G1Allocator::attempt_allocation_locked(size_t word_size, uint node_index) {\n","filename":"src\/hotspot\/share\/gc\/g1\/g1Allocator.inline.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -77,0 +77,1 @@\n+#include \"gc\/shared\/collectedHeap.inline.hpp\"\n@@ -116,0 +117,1 @@\n+#include \"utilities\/doublyLinkedList.inline.hpp\"\n@@ -403,0 +405,47 @@\n+bool G1CollectedHeap::attempt_allocation_after_gc(size_t word_size,\n+                                                  uint gc_count_before,\n+                                                  bool should_try_gc,\n+                                                  HeapWord** result,\n+                                                  GCCause::Cause gc_cause) {\n+\n+  StalledAllocReq request(word_size, _allocator->current_node_index());\n+\n+  {\n+    MutexLocker ml(&_alloc_request_lock, Mutex::_no_safepoint_check_flag);\n+    _stalled_allocations.insert_last(&request);\n+  }\n+\n+  if (should_try_gc) {\n+    bool succeeded = false;\n+    do_collection_pause(word_size, gc_count_before, &succeeded, gc_cause);\n+  } else {\n+    log_trace(gc, alloc)(\"%s: Stall until clear\", Thread::current()->name());\n+    \/\/ The GCLocker is either active or the GCLocker initiated\n+    \/\/ GC has not yet been performed. Stall until it is completed.\n+    GCLocker::stall_until_clear();\n+  }\n+\n+  {\n+    MutexLocker ml(&_alloc_request_lock, Mutex::_no_safepoint_check_flag);\n+    if (request.state() == StalledAllocReq::AllocationState::Pending) {\n+      \/\/ GC Safepoint did not handle our allocation request. We should retry.\n+      _stalled_allocations.remove(&request);\n+      return false;\n+    } else {\n+      _satisfied_allocations.remove(&request);\n+    }\n+  }\n+\n+  if (request.state() == StalledAllocReq::AllocationState::Success) {\n+    *result = request.result();\n+  } else {\n+    assert(request.state() == StalledAllocReq::AllocationState::Failed, \"Sanity check!\");\n+    \/\/ VM successfully scheduled a collection which failed to allocate. No\n+    \/\/ point in trying to allocate further. We'll just return null.\n+    log_debug(gc, alloc)(\"%s: Failed to allocate \"\n+                         SIZE_FORMAT \" words\", Thread::current()->name(), word_size);\n+    *result = nullptr;\n+  }\n+  return true;\n+}\n+\n@@ -420,1 +469,1 @@\n-  for (uint try_count = 1, gclocker_retry_count = 0; \/* we'll return *\/; try_count += 1) {\n+  while (true) {\n@@ -454,45 +503,1 @@\n-    if (should_try_gc) {\n-      bool succeeded;\n-      result = do_collection_pause(word_size, gc_count_before, &succeeded, GCCause::_g1_inc_collection_pause);\n-      if (result != nullptr) {\n-        assert(succeeded, \"only way to get back a non-null result\");\n-        log_trace(gc, alloc)(\"%s: Successfully scheduled collection returning \" PTR_FORMAT,\n-                             Thread::current()->name(), p2i(result));\n-        return result;\n-      }\n-\n-      if (succeeded) {\n-        \/\/ We successfully scheduled a collection which failed to allocate. No\n-        \/\/ point in trying to allocate further. We'll just return null.\n-        log_trace(gc, alloc)(\"%s: Successfully scheduled collection failing to allocate \"\n-                             SIZE_FORMAT \" words\", Thread::current()->name(), word_size);\n-        return nullptr;\n-      }\n-      log_trace(gc, alloc)(\"%s: Unsuccessfully scheduled collection allocating \" SIZE_FORMAT \" words\",\n-                           Thread::current()->name(), word_size);\n-    } else {\n-      \/\/ Failed to schedule a collection.\n-      if (gclocker_retry_count > GCLockerRetryAllocationCount) {\n-        log_warning(gc, alloc)(\"%s: Retried waiting for GCLocker too often allocating \"\n-                               SIZE_FORMAT \" words\", Thread::current()->name(), word_size);\n-        return nullptr;\n-      }\n-      log_trace(gc, alloc)(\"%s: Stall until clear\", Thread::current()->name());\n-      \/\/ The GCLocker is either active or the GCLocker initiated\n-      \/\/ GC has not yet been performed. Stall until it is and\n-      \/\/ then retry the allocation.\n-      GCLocker::stall_until_clear();\n-      gclocker_retry_count += 1;\n-    }\n-\n-    \/\/ We can reach here if we were unsuccessful in scheduling a\n-    \/\/ collection (because another thread beat us to it) or if we were\n-    \/\/ stalled due to the GC locker. In either can we should retry the\n-    \/\/ allocation attempt in case another thread successfully\n-    \/\/ performed a collection and reclaimed enough space. We do the\n-    \/\/ first attempt (without holding the Heap_lock) here and the\n-    \/\/ follow-on attempt will be at the start of the next loop\n-    \/\/ iteration (after taking the Heap_lock).\n-    size_t dummy = 0;\n-    result = _allocator->attempt_allocation(word_size, word_size, &dummy);\n-    if (result != nullptr) {\n+    if (attempt_allocation_after_gc(word_size, gc_count_before, should_try_gc, &result, GCCause::_g1_inc_collection_pause)) {\n@@ -501,7 +506,0 @@\n-\n-    \/\/ Give a warning if we seem to be looping forever.\n-    if ((QueuedAllocationWarningCount > 0) &&\n-        (try_count % QueuedAllocationWarningCount == 0)) {\n-      log_warning(gc, alloc)(\"%s:  Retried allocation %u times for \" SIZE_FORMAT \" words\",\n-                             Thread::current()->name(), try_count, word_size);\n-    }\n@@ -689,1 +687,1 @@\n-  for (uint try_count = 1, gclocker_retry_count = 0; \/* we'll return *\/; try_count += 1) {\n+  while (true) {\n@@ -693,1 +691,0 @@\n-\n@@ -716,51 +713,2 @@\n-    if (should_try_gc) {\n-      bool succeeded;\n-      result = do_collection_pause(word_size, gc_count_before, &succeeded, GCCause::_g1_humongous_allocation);\n-      if (result != nullptr) {\n-        assert(succeeded, \"only way to get back a non-null result\");\n-        log_trace(gc, alloc)(\"%s: Successfully scheduled collection returning \" PTR_FORMAT,\n-                             Thread::current()->name(), p2i(result));\n-        size_t size_in_regions = humongous_obj_size_in_regions(word_size);\n-        policy()->old_gen_alloc_tracker()->\n-          record_collection_pause_humongous_allocation(size_in_regions * HeapRegion::GrainBytes);\n-        return result;\n-      }\n-\n-      if (succeeded) {\n-        \/\/ We successfully scheduled a collection which failed to allocate. No\n-        \/\/ point in trying to allocate further. We'll just return null.\n-        log_trace(gc, alloc)(\"%s: Successfully scheduled collection failing to allocate \"\n-                             SIZE_FORMAT \" words\", Thread::current()->name(), word_size);\n-        return nullptr;\n-      }\n-      log_trace(gc, alloc)(\"%s: Unsuccessfully scheduled collection allocating \" SIZE_FORMAT \"\",\n-                           Thread::current()->name(), word_size);\n-    } else {\n-      \/\/ Failed to schedule a collection.\n-      if (gclocker_retry_count > GCLockerRetryAllocationCount) {\n-        log_warning(gc, alloc)(\"%s: Retried waiting for GCLocker too often allocating \"\n-                               SIZE_FORMAT \" words\", Thread::current()->name(), word_size);\n-        return nullptr;\n-      }\n-      log_trace(gc, alloc)(\"%s: Stall until clear\", Thread::current()->name());\n-      \/\/ The GCLocker is either active or the GCLocker initiated\n-      \/\/ GC has not yet been performed. Stall until it is and\n-      \/\/ then retry the allocation.\n-      GCLocker::stall_until_clear();\n-      gclocker_retry_count += 1;\n-    }\n-\n-\n-    \/\/ We can reach here if we were unsuccessful in scheduling a\n-    \/\/ collection (because another thread beat us to it) or if we were\n-    \/\/ stalled due to the GC locker. In either can we should retry the\n-    \/\/ allocation attempt in case another thread successfully\n-    \/\/ performed a collection and reclaimed enough space.\n-    \/\/ Humongous object allocation always needs a lock, so we wait for the retry\n-    \/\/ in the next iteration of the loop, unlike for the regular iteration case.\n-    \/\/ Give a warning if we seem to be looping forever.\n-\n-    if ((QueuedAllocationWarningCount > 0) &&\n-        (try_count % QueuedAllocationWarningCount == 0)) {\n-      log_warning(gc, alloc)(\"%s: Retried allocation %u times for \" SIZE_FORMAT \" words\",\n-                             Thread::current()->name(), try_count, word_size);\n+    if (attempt_allocation_after_gc(word_size, gc_count_before, should_try_gc, &result, GCCause::_g1_humongous_allocation)) {\n+      return result;\n@@ -775,0 +723,1 @@\n+                                                           uint node_index,\n@@ -777,1 +726,1 @@\n-  assert(!_allocator->has_mutator_alloc_region() || !expect_null_mutator_alloc_region,\n+  assert(!_allocator->has_mutator_alloc_region(node_index) || !expect_null_mutator_alloc_region,\n@@ -781,1 +730,1 @@\n-    return _allocator->attempt_allocation_locked(word_size);\n+    return _allocator->attempt_allocation_locked(word_size, node_index);\n@@ -793,0 +742,7 @@\n+HeapWord* G1CollectedHeap::attempt_allocation_at_safepoint(size_t word_size,\n+                                                           bool expect_null_mutator_alloc_region) {\n+  assert_at_safepoint_on_vm_thread();\n+  uint node_index = _allocator->current_node_index();\n+  return attempt_allocation_at_safepoint(word_size, expect_null_mutator_alloc_region, node_index);\n+}\n+\n@@ -950,0 +906,3 @@\n+  \/\/ Reset any allocated but unclaimed allocation requests.\n+  reset_allocation_requests();\n+\n@@ -975,5 +934,2 @@\n-                                                            bool do_gc,\n-                                                            bool maximal_compaction,\n-                                                            bool expect_null_mutator_alloc_region,\n-                                                            bool* gc_succeeded) {\n-  *gc_succeeded = true;\n+                                                            uint node_index,\n+                                                            bool expect_null_mutator_alloc_region) {\n@@ -983,0 +939,1 @@\n+                                    node_index,\n@@ -992,3 +949,5 @@\n-  result = expand_and_allocate(word_size);\n-  if (result != nullptr) {\n-    return result;\n+  if (expand(word_size)) {\n+    return attempt_allocation_at_safepoint(word_size,\n+                                           node_index,\n+                                           expect_null_mutator_alloc_region);\n+\n@@ -996,0 +955,2 @@\n+  return nullptr;\n+}\n@@ -997,12 +958,3 @@\n-  if (do_gc) {\n-    GCCauseSetter compaction(this, GCCause::_g1_compaction_pause);\n-    \/\/ Expansion didn't work, we'll try to do a Full GC.\n-    \/\/ If maximal_compaction is set we clear all soft references and don't\n-    \/\/ allow any dead wood to be left on the heap.\n-    if (maximal_compaction) {\n-      log_info(gc, ergo)(\"Attempting maximal full compaction clearing soft references\");\n-    } else {\n-      log_info(gc, ergo)(\"Attempting full compaction\");\n-    }\n-    *gc_succeeded = do_full_collection(maximal_compaction \/* clear_all_soft_refs *\/ ,\n-                                       maximal_compaction \/* do_maximal_compaction *\/);\n+bool G1CollectedHeap::is_unclaimed_allocation(HeapWord *obj) {\n+  if (!has_satisfied_allocations()) {\n+    return false;\n@@ -1011,1 +963,7 @@\n-  return nullptr;\n+  for(StalledAllocReq* alloc_req : _satisfied_allocations) {\n+    if (alloc_req->state() == StalledAllocReq::AllocationState::Success &&\n+        alloc_req->result() == obj) {\n+      return true;\n+    }\n+  }\n+  return false;\n@@ -1014,2 +972,1 @@\n-HeapWord* G1CollectedHeap::satisfy_failed_allocation(size_t word_size,\n-                                                     bool* succeeded) {\n+bool G1CollectedHeap::satisfy_failed_allocations(bool* gc_succeeded) {\n@@ -1018,7 +975,1 @@\n-  \/\/ Attempts to allocate followed by Full GC.\n-  HeapWord* result =\n-    satisfy_failed_allocation_helper(word_size,\n-                                     true,  \/* do_gc *\/\n-                                     false, \/* maximum_collection *\/\n-                                     false, \/* expect_null_mutator_alloc_region *\/\n-                                     succeeded);\n+  bool alloc_succeeded = handle_allocation_requests(false \/* expect_null_mutator_alloc_region*\/);\n@@ -1026,2 +977,2 @@\n-  if (result != nullptr || !*succeeded) {\n-    return result;\n+  if (alloc_succeeded) {\n+    return alloc_succeeded;\n@@ -1030,6 +981,3 @@\n-  \/\/ Attempts to allocate followed by Full GC that will collect all soft references.\n-  result = satisfy_failed_allocation_helper(word_size,\n-                                            true, \/* do_gc *\/\n-                                            true, \/* maximum_collection *\/\n-                                            true, \/* expect_null_mutator_alloc_region *\/\n-                                            succeeded);\n+  \/\/ Attempt to satisfy allocation requests failed; reset the requests, execute a full-gc,\n+  \/\/ then try again\n+  reset_allocation_requests();\n@@ -1037,2 +985,4 @@\n-  if (result != nullptr || !*succeeded) {\n-    return result;\n+  *gc_succeeded = do_full_collection(false \/* clear_all_soft_refs *\/, false \/* do_maximal_compaction *\/);\n+\n+  if (!*gc_succeeded) {\n+    return false;\n@@ -1041,6 +991,1 @@\n-  \/\/ Attempts to allocate, no GC\n-  result = satisfy_failed_allocation_helper(word_size,\n-                                            false, \/* do_gc *\/\n-                                            false, \/* maximum_collection *\/\n-                                            true,  \/* expect_null_mutator_alloc_region *\/\n-                                            succeeded);\n+  alloc_succeeded = handle_allocation_requests(true \/* expect_null_mutator_alloc_region*\/);\n@@ -1048,2 +993,27 @@\n-  if (result != nullptr) {\n-    return result;\n+  if (alloc_succeeded) {\n+    return alloc_succeeded;\n+  }\n+\n+  \/\/ Attempt to satisfy allocation requests after full-gc also failed. We reset the allocation requests\n+  \/\/ then execute a maximal compaction full-gc before retrying the allocations\n+  reset_allocation_requests();\n+\n+  *gc_succeeded = do_full_collection(true \/* clear_all_soft_refs *\/, true \/* do_maximal_compaction *\/);;\n+\n+  if (!*gc_succeeded) {\n+    return false;\n+  }\n+\n+  alloc_succeeded = handle_allocation_requests(true \/* expect_null_mutator_alloc_region*\/);\n+\n+  if (alloc_succeeded) {\n+    return alloc_succeeded;\n+  }\n+\n+  \/\/ Even after maximal compaction full gc, we failed to satisfy all the pending allocation requests.\n+  \/\/ We can declare the pending requests as failed.\n+  DoublyLinkedList<StalledAllocReq>::RemoveIterator iter(&_stalled_allocations);\n+\n+  for (StalledAllocReq* alloc_req; iter.next(&alloc_req);) {\n+    alloc_req->set_state(StalledAllocReq::AllocationState::Failed);\n+    _satisfied_allocations.insert_last(alloc_req);\n@@ -1054,0 +1024,2 @@\n+  return false;\n+}\n@@ -1055,5 +1027,22 @@\n-  \/\/ What else?  We might try synchronous finalization later.  If the total\n-  \/\/ space available is large enough for the allocation, then a more\n-  \/\/ complete compaction phase than we've tried so far might be\n-  \/\/ appropriate.\n-  return nullptr;\n+void G1CollectedHeap::reset_allocation_requests() {\n+  assert_at_safepoint_on_vm_thread();\n+\n+  DoublyLinkedList<StalledAllocReq> prev_allocations;\n+\n+  prev_allocations.swap(_satisfied_allocations);\n+\n+  assert(_satisfied_allocations.is_empty(), \"Sanity check!\");\n+\n+  DoublyLinkedList<StalledAllocReq>::RemoveIterator iter(&prev_allocations);\n+\n+  for (StalledAllocReq* alloc_req; iter.next(&alloc_req);) {\n+    if (alloc_req->state() == StalledAllocReq::AllocationState::Failed) {\n+      \/\/ If an allocation request was declared failed, we maintain the failed state.\n+      \/\/ This prevents requests from flip-floping between Pending->Failed->Pending states.\n+      \/\/ Once any allocation attempt declares a request as failed, that request will not be re-attempted.\n+      _satisfied_allocations.insert_first(alloc_req);\n+    } else {\n+      alloc_req->set_state(StalledAllocReq::AllocationState::Pending);\n+      _stalled_allocations.insert_first(alloc_req);\n+    }\n+  }\n@@ -1062,4 +1051,2 @@\n-\/\/ Attempting to expand the heap sufficiently\n-\/\/ to support an allocation of the given \"word_size\".  If\n-\/\/ successful, perform the allocation and return the address of the\n-\/\/ allocated block, or else null.\n+bool G1CollectedHeap::handle_allocation_requests(bool expect_null_mutator_alloc_region) {\n+  assert_at_safepoint_on_vm_thread();\n@@ -1067,1 +1054,54 @@\n-HeapWord* G1CollectedHeap::expand_and_allocate(size_t word_size) {\n+  const uint active_numa_nodes = G1NUMA::numa()->num_active_nodes();\n+  bool *expect_null_alloc_regions = (bool*)alloca(active_numa_nodes * sizeof(bool));\n+  for (uint i = 0; i < active_numa_nodes; i++) {\n+    expect_null_alloc_regions[i] = expect_null_mutator_alloc_region;\n+  }\n+\n+  while (true) {\n+    StalledAllocReq* alloc_req = _stalled_allocations.first();\n+    if (alloc_req == nullptr) {\n+      \/\/ No more pending requests, all allocations succeeded\n+      return true;\n+    }\n+\n+    HeapWord* result =\n+      satisfy_failed_allocation_helper(alloc_req->size(),\n+                                       alloc_req->node_index(),\n+                                       expect_null_alloc_regions[alloc_req->node_index()]);\n+\n+    if (result == nullptr) {\n+      \/\/ Failed to allocate, give up.\n+      return false;\n+    }\n+\n+    expect_null_alloc_regions[alloc_req->node_index()] = false;\n+\n+    \/\/ Allocation succeeded, update the state and result of the allocation request.\n+    alloc_req->set_state(StalledAllocReq::AllocationState::Success, result);\n+\n+    if (is_humongous(alloc_req->size())) {\n+      \/\/ Calculate payload size and initialize the humongous object with a fillerArray.\n+      size_t words = alloc_req->size();\n+\n+      const size_t payload_size = words - CollectedHeap::filler_array_hdr_size();\n+      const size_t len = payload_size * HeapWordSize \/ sizeof(jint);\n+      assert((int)len >= 0, \"size too large \" SIZE_FORMAT \" becomes %d\", words, (int)len);\n+\n+      ObjArrayAllocator allocator(Universe::fillerArrayKlassObj(), words, (int)len, \/* do_zero *\/ false);\n+      allocator.initialize(result);\n+\n+      size_t size_in_regions = humongous_obj_size_in_regions(alloc_req->size());\n+      policy()->old_gen_alloc_tracker()->\n+          record_collection_pause_humongous_allocation(size_in_regions * HeapRegion::GrainBytes);\n+    } else {\n+      \/\/ Fill the allocated memory with filler objects.\n+      CollectedHeap::fill_with_objects(result, alloc_req->size());\n+    }\n+\n+    \/\/ Move the allocation request from stalled to satisfied list.\n+    _stalled_allocations.remove(alloc_req);\n+    _satisfied_allocations.insert_last(alloc_req);\n+  }\n+}\n+\n+bool G1CollectedHeap::expand(size_t word_size) {\n@@ -1076,1 +1116,0 @@\n-\n@@ -1080,0 +1119,9 @@\n+    return true;\n+  }\n+  return false;\n+}\n+\n+HeapWord* G1CollectedHeap::expand_and_allocate(size_t word_size) {\n+  assert_at_safepoint_on_vm_thread();\n+\n+  if (expand(word_size)) {\n@@ -1223,0 +1271,3 @@\n+  _alloc_request_lock(Mutex::nosafepoint, \"G1 Stalled Allocation List\"),\n+  _stalled_allocations(),\n+  _satisfied_allocations(),\n@@ -2498,0 +2549,2 @@\n+  reset_allocation_requests();\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectedHeap.cpp","additions":220,"deletions":167,"binary":false,"changes":387,"status":"modified"},{"patch":"@@ -62,0 +62,1 @@\n+#include \"utilities\/doublyLinkedList.inline.hpp\"\n@@ -171,0 +172,54 @@\n+  \/\/ StalledAllocReq represents a stalled allocation request in a doubly linked list.\n+  \/\/\n+  \/\/ It provides functionality to track and manage an allocation request. The\n+  \/\/ allocation request encapsulates information such as the size of the allocation,\n+  \/\/ the NUMA (Non-Uniform Memory Access) node index, the state of the allocation,\n+  \/\/ and the resulting memory address.\n+  \/\/\n+  \/\/ The state of an allocation request can transition from Pending to Failed or\n+  \/\/ Success. Failed state is a terminating state, indicating that the allocation\n+  \/\/ request has failed. However, the Success state may be reset to Pending if an\n+  \/\/ interfering safepoint observes the successful allocation before the requesting\n+  \/\/ thread.\n+  class StalledAllocReq : public DoublyLinkedListNode {\n+  public:\n+\n+    enum class AllocationState {\n+      Success,\n+      Failed,\n+      Pending,\n+    };\n+\n+    StalledAllocReq(size_t size, uint numa_node) :\n+      _size(size),\n+      _result(nullptr),\n+      _node_index(numa_node),\n+      _state(AllocationState::Pending)\n+    { }\n+\n+    StalledAllocReq() : StalledAllocReq(0, 0) {}\n+\n+    size_t size() { return _size; }\n+\n+    uint node_index() const { return _node_index; }\n+\n+    void set_state(AllocationState state, HeapWord* result = nullptr) {\n+      _state = state;\n+      _result = result;\n+    }\n+\n+    AllocationState state() { return _state; }\n+\n+    HeapWord* result() { return _result; }\n+\n+  private:\n+    const size_t _size;\n+    HeapWord* _result;\n+    const uint _node_index;\n+    AllocationState _state;\n+  };\n+\n+  Mutex _alloc_request_lock;\n+  DoublyLinkedList<StalledAllocReq> _stalled_allocations;\n+  DoublyLinkedList<StalledAllocReq> _satisfied_allocations;\n+\n@@ -198,0 +253,4 @@\n+\n+  bool has_satisfied_allocations() { return !_satisfied_allocations.is_empty(); }\n+  bool is_unclaimed_allocation(HeapWord *obj);\n+\n@@ -462,0 +521,6 @@\n+bool attempt_allocation_after_gc(size_t word_size,\n+                                 uint gc_count_before,\n+                                 bool should_try_gc,\n+                                 HeapWord** result,\n+                                 GCCause::Cause gc_cause);\n+\n@@ -469,0 +534,4 @@\n+  HeapWord* attempt_allocation_at_safepoint(size_t word_size,\n+                                            uint node_index,\n+                                            bool expect_null_mutator_alloc_region);\n+\n@@ -500,2 +569,3 @@\n-  HeapWord* satisfy_failed_allocation(size_t word_size,\n-                                      bool* succeeded);\n+  bool satisfy_failed_allocations(bool* gc_succeeded);\n+  bool handle_allocation_requests(bool expect_null_mutator_alloc_region);\n+  void reset_allocation_requests();\n@@ -514,9 +584,6 @@\n-                                             bool do_gc,\n-                                             bool maximal_compaction,\n-                                             bool expect_null_mutator_alloc_region,\n-                                             bool* gc_succeeded);\n-\n-  \/\/ Attempting to expand the heap sufficiently\n-  \/\/ to support an allocation of the given \"word_size\".  If\n-  \/\/ successful, perform the allocation and return the address of the\n-  \/\/ allocated block, or else null.\n+                                             uint node_index,\n+                                             bool expect_null_mutator_alloc_region);\n+\n+  \/\/ Attempts to expand the heap sufficiently to support an allocation of the\n+  \/\/ given \"word_size\". If successful, perform the allocation and return the address\n+  \/\/ of the allocated block, or else null.\n@@ -524,0 +591,1 @@\n+  bool expand(size_t word_size);\n@@ -577,1 +645,1 @@\n-  bool expand(size_t expand_bytes, WorkerThreads* pretouch_workers = nullptr, double* expand_time_ms = nullptr);\n+  bool expand(size_t expand_bytes, WorkerThreads* pretouch_workers, double* expand_time_ms = nullptr);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectedHeap.hpp","additions":80,"deletions":12,"binary":false,"changes":92,"status":"modified"},{"patch":"@@ -1109,0 +1109,3 @@\n+        if (_g1h->is_unclaimed_allocation(hr->humongous_start_region()->bottom())) {\n+          return;\n+        }\n@@ -1158,0 +1161,3 @@\n+          if (_g1h->is_unclaimed_allocation(hr->humongous_start_region()->bottom())) {\n+            return;\n+          }\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ConcurrentMark.cpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -54,0 +54,7 @@\n+\n+  \/\/ Any allocation requests that were handled during a previous GC safepoint but\n+  \/\/ have not been observed by the requesting mutator thread should be reset to\n+  \/\/ pending. This makes it easier for the current GC to treat the unclaimed memory\n+  \/\/ as garbage.\n+  g1h->reset_allocation_requests();\n+\n@@ -128,8 +135,6 @@\n-  if (_word_size > 0) {\n-    \/\/ An allocation has been requested. So, try to do that first.\n-    _result = g1h->attempt_allocation_at_safepoint(_word_size,\n-                                                   false \/* expect_null_cur_alloc_region *\/);\n-    if (_result != nullptr) {\n-      \/\/ If we can successfully allocate before we actually do the\n-      \/\/ pause then we will consider this pause successful.\n-      _gc_succeeded = true;\n+  \/\/ If any allocation has been requested, try to do that first.\n+  bool has_allocation_requests = !g1h->_stalled_allocations.is_empty();\n+  if (has_allocation_requests) {\n+    bool alloc_succeeded = g1h->handle_allocation_requests(false \/* expect_null_mutator_alloc_region*\/);\n+\n+    if (alloc_succeeded) {\n@@ -140,0 +145,3 @@\n+  \/\/ Reset any satisfied allocation requests before attempting the collection.\n+  g1h->reset_allocation_requests();\n+\n@@ -141,0 +149,1 @@\n+  bool gc_succeeded = false;\n@@ -144,11 +153,13 @@\n-  if (_gc_succeeded) {\n-    if (_word_size > 0) {\n-      \/\/ An allocation had been requested. Do it, eventually trying a stronger\n-      \/\/ kind of GC.\n-      _result = g1h->satisfy_failed_allocation(_word_size, &_gc_succeeded);\n-    } else if (g1h->should_upgrade_to_full_gc()) {\n-      \/\/ There has been a request to perform a GC to free some space. We have no\n-      \/\/ information on how much memory has been asked for. In case there are\n-      \/\/ absolutely no regions left to allocate into, do a full compaction.\n-      _gc_succeeded = g1h->upgrade_to_full_collection();\n-    }\n+  if (!_gc_succeeded) {\n+    return;\n+  }\n+\n+  has_allocation_requests = !g1h->_stalled_allocations.is_empty();\n+\n+  if (has_allocation_requests) {\n+    g1h->satisfy_failed_allocations(&_gc_succeeded);\n+  } else if (g1h->should_upgrade_to_full_gc()) {\n+    \/\/ There has been a request to perform a GC to free some space. We have no\n+    \/\/ information on how much memory has been asked for. In case there are\n+    \/\/ absolutely no regions left to allocate into, do a full compaction.\n+    _gc_succeeded = g1h->upgrade_to_full_collection();\n","filename":"src\/hotspot\/share\/gc\/g1\/g1VMOperations.cpp","additions":30,"deletions":19,"binary":false,"changes":49,"status":"modified"},{"patch":"@@ -429,8 +429,0 @@\n-size_t CollectedHeap::filler_array_hdr_size() {\n-  return align_object_offset(arrayOopDesc::header_size(T_INT)); \/\/ align to Long\n-}\n-\n-size_t CollectedHeap::filler_array_min_size() {\n-  return align_object_size(filler_array_hdr_size()); \/\/ align to MinObjAlignment\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.cpp","additions":0,"deletions":8,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -49,0 +49,8 @@\n+size_t CollectedHeap::filler_array_hdr_size() {\n+  return align_object_offset(arrayOopDesc::header_size(T_INT)); \/\/ Align to INT.\n+}\n+\n+size_t CollectedHeap::filler_array_min_size() {\n+  return align_object_size(filler_array_hdr_size()); \/\/ Align to MinObjAlignment.\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.inline.hpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -0,0 +1,179 @@\n+\/*\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_UTILITIES_DOUBLYLINKEDLIST_HPP\n+#define SHARE_UTILITIES_DOUBLYLINKEDLIST_HPP\n+\n+#include \"memory\/allocation.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+template <typename T>\n+class DoublyLinkedList;\n+\n+\/\/ Element in a doubly linked list\n+\n+class DoublyLinkedListNode {\n+  template<typename T>\n+  friend class DoublyLinkedList;\n+\n+  DoublyLinkedListNode* _next;\n+  DoublyLinkedListNode* _prev;\n+\n+  NONCOPYABLE(DoublyLinkedListNode);\n+\n+  void swap(DoublyLinkedListNode* rhs);\n+\n+  void verify_links() const;\n+  void verify_links_linked() const;\n+  void verify_links_unlinked() const;\n+\n+protected:\n+  ~DoublyLinkedListNode();\n+\n+public:\n+  DoublyLinkedListNode();\n+};\n+\n+\/\/\n+\/\/ The DoublyLinkedList template provides efficient insertion, removal, and traversal of elements in a doubly linked list structure.\n+\/\/ Each node in the list contains an element of type T, and the nodes are connected bidirectionally.\n+\/\/ The class supports forward and backward traversal, as well as insertion and removal operations at the beginning, end,\n+\/\/ or any position within the list.\n+\/\/\n+\/\/ \\tparam T The type of elements stored in the linked list. It must be default-constructible and derived from DoublyLinkedListNode.\n+\/\/\n+\/\/ Note: The DoublyLinkedList class does not perform memory allocation or deallocation for the elements.\n+\/\/       Therefore, it is the responsibility of the class template users to manage the memory of the elements added or removed from the list.\n+\/\/\n+template <typename T>\n+class DoublyLinkedList {\n+  static_assert(std::is_default_constructible<T>::value,\n+                \"DoublyLinkedList requires default-constructible elements\");\n+\n+  static_assert(std::is_base_of<DoublyLinkedListNode, T>::value,\n+                \"DoublyLinkedList requires elements derived from DoublyLinkedListNode\");\n+\n+  T _head;\n+  size_t _size;\n+\n+  NONCOPYABLE(DoublyLinkedList);\n+\n+  void verify_head() const;\n+\n+  void insert(DoublyLinkedListNode* before, DoublyLinkedListNode* node);\n+\n+  T* cast_to_outer(DoublyLinkedListNode* node) const;\n+\n+  T* next(T* elem) const;\n+  T* prev(T* elem) const;\n+\n+public:\n+  DoublyLinkedList();\n+\n+  size_t size() const;\n+  bool is_empty() const;\n+\n+  T* first() const;\n+  T* last() const;\n+\n+  void insert_first(T* elem);\n+  void insert_last(T* elem);\n+  void insert_before(T* before, T* elem);\n+  void insert_after(T* after, T* elem);\n+\n+  void remove(T* elem);\n+  T* remove_first();\n+  T* remove_last();\n+\n+  void swap(DoublyLinkedList& other);\n+\n+  class Iterator;\n+\n+  class RemoveIterator;\n+\n+  Iterator begin();\n+  Iterator end();\n+};\n+\n+template <typename T>\n+class DoublyLinkedList<T>::Iterator : public StackObj {\n+  friend class DoublyLinkedList<T>;\n+\n+  const DoublyLinkedList<T>* const _list;\n+  T* _cur_node;\n+\n+  Iterator(const DoublyLinkedList<T>* list, T* start) :\n+    _list(list),\n+    _cur_node(start)\n+  { }\n+\n+public:\n+  Iterator& operator++() {\n+    _cur_node = _list->next(_cur_node);\n+    return *this;\n+  }\n+\n+  Iterator operator++(int) {\n+    Iterator tmp = *this;\n+    ++(*this);\n+    return tmp;\n+  }\n+\n+  Iterator& operator--() {\n+    assert(_cur_node != nullptr, \"Sanity\");\n+    _cur_node = _list->prev(_cur_node);\n+    return *this;\n+  }\n+\n+  Iterator operator--(int) {\n+    Iterator tmp = *this;\n+    --(*this);\n+    return tmp;\n+  }\n+\n+  T* operator*() { return _cur_node; }\n+\n+  bool operator==(const Iterator& rhs) {\n+    assert(_list == rhs._list, \"iterator belongs to different List\");\n+    return _cur_node == rhs._cur_node;\n+  }\n+\n+  bool operator!=(const Iterator& rhs) {\n+    assert(_list == rhs._list, \"iterator belongs to different List\");\n+    return _cur_node != rhs._cur_node;\n+  }\n+};\n+\n+template <typename T>\n+class DoublyLinkedList<T>::RemoveIterator : public StackObj {\n+private:\n+  DoublyLinkedList<T>* const _list;\n+  const bool _forward;\n+\n+public:\n+  explicit RemoveIterator(DoublyLinkedList<T>* list, bool forward_iterate = true);\n+\n+  bool next(T** elem);\n+};\n+\n+#endif \/\/ SHARE_UTILITIES_DOUBLYLINKEDLIST_HPP\n","filename":"src\/hotspot\/share\/utilities\/doublyLinkedList.hpp","additions":179,"deletions":0,"binary":false,"changes":179,"status":"added"},{"patch":"@@ -0,0 +1,243 @@\n+\/*\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_UTILITIES_DOUBLYLINKEDLIST_INLINE_HPP\n+#define SHARE_UTILITIES_DOUBLYLINKEDLIST_INLINE_HPP\n+\n+#include \"utilities\/doublyLinkedList.hpp\"\n+#include \"utilities\/debug.hpp\"\n+\n+inline DoublyLinkedListNode::DoublyLinkedListNode() :\n+    _next(this),\n+    _prev(this) {}\n+\n+inline DoublyLinkedListNode::~DoublyLinkedListNode() {\n+  verify_links_unlinked();\n+}\n+\n+inline void DoublyLinkedListNode::swap(DoublyLinkedListNode* rhs) {\n+  if (this == rhs) {\n+    return;\n+  }\n+\n+  if (this->_next != this) {\n+    if (rhs->_next != rhs) {\n+      ::swap(this->_next, rhs->_next);\n+      ::swap(this->_prev,rhs->_prev);\n+      this->_next->_prev = this->_prev->_next = this;\n+      rhs->_next->_prev = rhs->_prev->_next = rhs;\n+    } else {\n+      rhs->_next = this->_next;\n+      rhs->_prev = this->_prev;\n+      rhs->_next->_prev = rhs->_prev->_next = rhs;\n+      this->_next = this->_prev = this;\n+    }\n+  } else if (rhs->_next != rhs) {\n+    this->_next = rhs->_next;\n+    this->_prev = rhs->_prev;\n+    this->_next->_prev = this->_prev->_next = this;\n+    rhs->_next = rhs->_prev = rhs;\n+  }\n+}\n+\n+inline void DoublyLinkedListNode::verify_links() const {\n+  assert(_next->_prev == this, \"Corrupt list node\");\n+  assert(_prev->_next == this, \"Corrupt list node\");\n+}\n+\n+inline void DoublyLinkedListNode::verify_links_linked() const {\n+  assert(_next != this, \"Should be in a list\");\n+  assert(_prev != this, \"Should be in a list\");\n+  verify_links();\n+}\n+\n+inline void DoublyLinkedListNode::verify_links_unlinked() const {\n+  assert(_next == this, \"Should not be in a list\");\n+  assert(_prev == this, \"Should not be in a list\");\n+}\n+\n+template <typename T>\n+inline void DoublyLinkedList<T>::verify_head() const {\n+  _head.verify_links();\n+}\n+\n+template <typename T>\n+inline void DoublyLinkedList<T>::insert(DoublyLinkedListNode* before, DoublyLinkedListNode* node) {\n+  verify_head();\n+\n+  before->verify_links();\n+  node->verify_links_unlinked();\n+\n+  node->_prev = before;\n+  node->_next = before->_next;\n+  before->_next = node;\n+  node->_next->_prev = node;\n+\n+  before->verify_links_linked();\n+  node->verify_links_linked();\n+\n+  _size++;\n+}\n+\n+template <typename T>\n+inline T* DoublyLinkedList<T>::cast_to_outer(DoublyLinkedListNode* node) const {\n+  return static_cast<T*>(node);\n+}\n+\n+template <typename T>\n+inline DoublyLinkedList<T>::DoublyLinkedList() :\n+    _head(),\n+    _size(0) {\n+  verify_head();\n+}\n+\n+template <typename T>\n+inline void DoublyLinkedList<T>::swap(DoublyLinkedList& other) {\n+\n+  _head.swap(&other._head);\n+  ::swap(this->_size, other._size);\n+\n+  other.verify_head();\n+  verify_head();\n+}\n+\n+template <typename T>\n+inline size_t DoublyLinkedList<T>::size() const {\n+  verify_head();\n+  return _size;\n+}\n+\n+template <typename T>\n+inline bool DoublyLinkedList<T>::is_empty() const {\n+  return size() == 0;\n+}\n+\n+template <typename T>\n+inline T* DoublyLinkedList<T>::first() const {\n+  return is_empty() ? nullptr : cast_to_outer(_head._next);\n+}\n+\n+template <typename T>\n+inline T* DoublyLinkedList<T>::last() const {\n+  return is_empty() ? nullptr : cast_to_outer(_head._prev);\n+}\n+\n+template <typename T>\n+inline T* DoublyLinkedList<T>::next(T* elem) const {\n+  verify_head();\n+\n+  return cast_to_outer(elem->_next);\n+}\n+\n+template <typename T>\n+inline T* DoublyLinkedList<T>::prev(T* elem) const {\n+  verify_head();\n+\n+  return cast_to_outer(elem->_prev);\n+}\n+\n+template <typename T>\n+inline void DoublyLinkedList<T>::insert_first(T* elem) {\n+  insert(&_head, elem);\n+}\n+\n+template <typename T>\n+inline void DoublyLinkedList<T>::insert_last(T* elem) {\n+  insert(_head._prev, elem);\n+}\n+\n+template <typename T>\n+inline void DoublyLinkedList<T>::insert_before(T* before, T* elem) {\n+  insert(before->_prev, elem);\n+}\n+\n+template <typename T>\n+inline void DoublyLinkedList<T>::insert_after(T* after, T* elem) {\n+  insert(after, elem);\n+}\n+\n+template <typename T>\n+inline void DoublyLinkedList<T>::remove(T* elem) {\n+  verify_head();\n+\n+  elem->verify_links_linked();\n+\n+  DoublyLinkedListNode* const next = elem->_next;\n+  DoublyLinkedListNode* const prev = elem->_prev;\n+  next->verify_links_linked();\n+  prev->verify_links_linked();\n+\n+  elem->_next = prev->_next;\n+  elem->_prev = next->_prev;\n+  elem->verify_links_unlinked();\n+\n+  next->_prev = prev;\n+  prev->_next = next;\n+  next->verify_links();\n+  prev->verify_links();\n+\n+  assert(_size > 0, \"Sanity check!\");\n+  _size--;\n+}\n+\n+template <typename T>\n+inline T* DoublyLinkedList<T>::remove_first() {\n+  T* elem = first();\n+  if (elem != nullptr) {\n+    remove(elem);\n+  }\n+  return elem;\n+}\n+\n+template <typename T>\n+inline T* DoublyLinkedList<T>::remove_last() {\n+  T* elem = last();\n+  if (elem != nullptr) {\n+    remove(elem);\n+  }\n+  return elem;\n+}\n+\n+template <typename T>\n+inline typename DoublyLinkedList<T>::Iterator DoublyLinkedList<T>::begin() {\n+ return Iterator(this, cast_to_outer(_head._next));\n+}\n+\n+template <typename T>\n+inline typename DoublyLinkedList<T>::Iterator DoublyLinkedList<T>::end() {\n+ return Iterator(this, &_head);\n+}\n+\n+template <typename T>\n+inline DoublyLinkedList<T>::RemoveIterator::RemoveIterator(DoublyLinkedList<T>* list, bool forward_iterate):\n+    _list(list),\n+    _forward(forward_iterate) {}\n+\n+\n+template <typename T>\n+inline bool DoublyLinkedList<T>::RemoveIterator::next(T** elem) {\n+  *elem = _forward ? _list->remove_first() : _list->remove_last();\n+  return *elem != nullptr;\n+}\n+\n+#endif \/\/ SHARE_UTILITIES_DOUBLYLINKEDLIST_INLINE_HPP\n","filename":"src\/hotspot\/share\/utilities\/doublyLinkedList.inline.hpp","additions":243,"deletions":0,"binary":false,"changes":243,"status":"added"}]}