{"files":[{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -29,0 +29,1 @@\n+#include \"compiler\/compiler_globals.hpp\"\n@@ -33,0 +34,1 @@\n+#include \"gc\/shared\/tlab_globals.hpp\"\n@@ -4928,2 +4930,4 @@\n-\/\/ clear memory of size 'cnt' qwords, starting at 'base' using XMM\/YMM registers\n-void MacroAssembler::xmm_clear_mem(Register base, Register cnt, XMMRegister xtmp) {\n+#if COMPILER2_OR_JVMCI\n+\n+\/\/ clear memory of size 'cnt' qwords, starting at 'base' using XMM\/YMM\/ZMM registers\n+void MacroAssembler::xmm_clear_mem(Register base, Register cnt, Register rtmp, XMMRegister xtmp) {\n@@ -4933,1 +4937,4 @@\n-  if (UseAVX >= 2) {\n+  bool use64byteVector = MaxVectorSize == 64 && AVX3Threshold == 0;\n+  if (use64byteVector) {\n+    vpxor(xtmp, xtmp, xtmp, AVX_512bit);\n+  } else if (MaxVectorSize >= 32) {\n@@ -4941,3 +4948,2 @@\n-  if (UseAVX >= 2) {\n-    vmovdqu(Address(base,  0), xtmp);\n-    vmovdqu(Address(base, 32), xtmp);\n+  if (MaxVectorSize >= 32) {\n+    fill64_avx(base, 0, xtmp, use64byteVector);\n@@ -4955,8 +4961,16 @@\n-  addptr(cnt, 4);\n-  jccb(Assembler::less, L_tail);\n-  \/\/ Copy trailing 32 bytes\n-  if (UseAVX >= 2) {\n-    vmovdqu(Address(base, 0), xtmp);\n-  } else {\n-    movdqu(Address(base,  0), xtmp);\n-    movdqu(Address(base, 16), xtmp);\n+\n+  \/\/ Copy trailing 64 bytes\n+  if (use64byteVector) {\n+    addptr(cnt, 8);\n+    jccb(Assembler::equal, L_end);\n+    fill64_masked_avx(3, base, 0, xtmp, k2, cnt, rtmp, true);\n+    jmp(L_end);\n+  } else {\n+    addptr(cnt, 4);\n+    jccb(Assembler::less, L_tail);\n+    if (MaxVectorSize >= 32) {\n+      vmovdqu(Address(base, 0), xtmp);\n+    } else {\n+      movdqu(Address(base,  0), xtmp);\n+      movdqu(Address(base, 16), xtmp);\n+    }\n@@ -4970,1 +4984,4 @@\n-  decrement(cnt);\n+  if (UseAVX > 2 && MaxVectorSize >= 32 && VM_Version::supports_avx512vl()) {\n+    fill32_masked_avx(3, base, 0, xtmp, k2, cnt, rtmp);\n+  } else {\n+    decrement(cnt);\n@@ -4972,5 +4989,6 @@\n-  BIND(L_sloop);\n-  movq(Address(base, 0), xtmp);\n-  addptr(base, 8);\n-  decrement(cnt);\n-  jccb(Assembler::greaterEqual, L_sloop);\n+    BIND(L_sloop);\n+    movq(Address(base, 0), xtmp);\n+    addptr(base, 8);\n+    decrement(cnt);\n+    jccb(Assembler::greaterEqual, L_sloop);\n+  }\n@@ -4980,0 +4998,71 @@\n+\/\/ Clearing constant sized memory using YMM\/ZMM registers.\n+void MacroAssembler::clear_mem(Register base, int cnt, Register rtmp, XMMRegister xtmp) {\n+  assert(UseAVX > 2 && VM_Version::supports_avx512vlbw(), \"\");\n+  bool use64byteVector = MaxVectorSize > 32 && AVX3Threshold == 0;\n+\n+  int vector64_count = (cnt & (~0x7)) >> 3;\n+  cnt = cnt & 0x7;\n+\n+  \/\/ 64 byte initialization loop.\n+  vpxor(xtmp, xtmp, xtmp, use64byteVector ? AVX_512bit : AVX_256bit);\n+  for (int i = 0; i < vector64_count; i++) {\n+    fill64_avx(base, i * 64, xtmp, use64byteVector);\n+  }\n+\n+  \/\/ Clear remaining 64 byte tail.\n+  int disp = vector64_count * 64;\n+  if (cnt) {\n+    switch (cnt) {\n+      case 1:\n+        movq(Address(base, disp), xtmp);\n+        break;\n+      case 2:\n+        evmovdqu(T_LONG, k0, Address(base, disp), xtmp, Assembler::AVX_128bit);\n+        break;\n+      case 3:\n+        movl(rtmp, 0x7);\n+        kmovwl(k2, rtmp);\n+        evmovdqu(T_LONG, k2, Address(base, disp), xtmp, Assembler::AVX_256bit);\n+        break;\n+      case 4:\n+        evmovdqu(T_LONG, k0, Address(base, disp), xtmp, Assembler::AVX_256bit);\n+        break;\n+      case 5:\n+        if (use64byteVector) {\n+          movl(rtmp, 0x1F);\n+          kmovwl(k2, rtmp);\n+          evmovdqu(T_LONG, k2, Address(base, disp), xtmp, Assembler::AVX_512bit);\n+        } else {\n+          evmovdqu(T_LONG, k0, Address(base, disp), xtmp, Assembler::AVX_256bit);\n+          movq(Address(base, disp + 32), xtmp);\n+        }\n+        break;\n+      case 6:\n+        if (use64byteVector) {\n+          movl(rtmp, 0x3F);\n+          kmovwl(k2, rtmp);\n+          evmovdqu(T_LONG, k2, Address(base, disp), xtmp, Assembler::AVX_512bit);\n+        } else {\n+          evmovdqu(T_LONG, k0, Address(base, disp), xtmp, Assembler::AVX_256bit);\n+          evmovdqu(T_LONG, k0, Address(base, disp + 32), xtmp, Assembler::AVX_128bit);\n+        }\n+        break;\n+      case 7:\n+        if (use64byteVector) {\n+          movl(rtmp, 0x7F);\n+          kmovwl(k2, rtmp);\n+          evmovdqu(T_LONG, k2, Address(base, disp), xtmp, Assembler::AVX_512bit);\n+        } else {\n+          evmovdqu(T_LONG, k0, Address(base, disp), xtmp, Assembler::AVX_256bit);\n+          movl(rtmp, 0x7);\n+          kmovwl(k2, rtmp);\n+          evmovdqu(T_LONG, k2, Address(base, disp + 32), xtmp, Assembler::AVX_256bit);\n+        }\n+        break;\n+      default:\n+        fatal(\"Unexpected length : %d\\n\",cnt);\n+        break;\n+    }\n+  }\n+}\n+\n@@ -4981,2 +5070,2 @@\n-  \/\/ cnt - number of qwords (8-byte words).\n-  \/\/ base - start address, qword aligned.\n+  \/\/ cnt      - number of qwords (8-byte words).\n+  \/\/ base     - start address, qword aligned.\n@@ -4991,1 +5080,0 @@\n-\n@@ -5021,2 +5109,1 @@\n-    movptr(tmp, base);\n-    xmm_clear_mem(tmp, cnt, xtmp);\n+    xmm_clear_mem(base, cnt, tmp, xtmp);\n@@ -5031,0 +5118,3 @@\n+#endif \/\/COMPILER2_OR_JVMCI\n+\n+\n@@ -8050,0 +8140,57 @@\n+#if COMPILER2_OR_JVMCI\n+\n+\n+\/\/ Set memory operation for length \"less than\" 64 bytes.\n+void MacroAssembler::fill64_masked_avx(uint shift, Register dst, int disp,\n+                                       XMMRegister xmm, KRegister mask, Register length,\n+                                       Register temp, bool use64byteVector) {\n+  assert(MaxVectorSize >= 32, \"vector length should be >= 32\");\n+  assert(shift != 0, \"shift value should be 1 (short),2(int) or 3(long)\");\n+  BasicType type[] = { T_BYTE, T_SHORT,  T_INT,   T_LONG};\n+  if (!use64byteVector) {\n+    fill32_avx(dst, disp, xmm);\n+    subptr(length, 32 >> shift);\n+    fill32_masked_avx(shift, dst, disp + 32, xmm, mask, length, temp);\n+  } else {\n+    assert(MaxVectorSize == 64, \"vector length != 64\");\n+    movl(temp, 1);\n+    shlxl(temp, temp, length);\n+    subptr(temp, 1);\n+    kmovwl(mask, temp);\n+    evmovdqu(type[shift], mask, Address(dst, disp), xmm, Assembler::AVX_512bit);\n+  }\n+}\n+\n+\n+void MacroAssembler::fill32_masked_avx(uint shift, Register dst, int disp,\n+                                       XMMRegister xmm, KRegister mask, Register length,\n+                                       Register temp) {\n+  assert(MaxVectorSize >= 32, \"vector length should be >= 32\");\n+  assert(shift != 0, \"shift value should be 1 (short), 2(int) or 3(long)\");\n+  BasicType type[] = { T_BYTE, T_SHORT,  T_INT,   T_LONG};\n+  movl(temp, 1);\n+  shlxl(temp, temp, length);\n+  subptr(temp, 1);\n+  kmovwl(mask, temp);\n+  evmovdqu(type[shift], mask, Address(dst, disp), xmm, Assembler::AVX_256bit);\n+}\n+\n+\n+void MacroAssembler::fill32_avx(Register dst, int disp, XMMRegister xmm) {\n+  assert(MaxVectorSize >= 32, \"vector length should be >= 32\");\n+  vmovdqu(Address(dst, disp), xmm);\n+}\n+\n+void MacroAssembler::fill64_avx(Register dst, int disp, XMMRegister xmm, bool use64byteVector) {\n+  assert(MaxVectorSize >= 32, \"vector length should be >= 32\");\n+  BasicType type[] = {T_BYTE,  T_SHORT,  T_INT,   T_LONG};\n+  if (!use64byteVector) {\n+    fill32_avx(dst, disp, xmm);\n+    fill32_avx(dst, disp + 32, xmm);\n+  } else {\n+    evmovdquq(Address(dst, disp), xmm, Assembler::AVX_512bit);\n+  }\n+}\n+\n+#endif \/\/COMPILER2_OR_JVMCI\n+\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":173,"deletions":26,"binary":false,"changes":199,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -31,0 +31,1 @@\n+#include \"runtime\/vm_version.hpp\"\n@@ -1686,0 +1687,3 @@\n+  \/\/ clear memory initialization sequence for constant size;\n+  void clear_mem(Register base, int cnt, Register rtmp, XMMRegister xtmp);\n+\n@@ -1687,1 +1691,1 @@\n-  void xmm_clear_mem(Register base, Register cnt, XMMRegister xtmp);\n+  void xmm_clear_mem(Register base, Register cnt, Register rtmp, XMMRegister xtmp);\n@@ -1806,0 +1810,12 @@\n+  void fill64_masked_avx(uint shift, Register dst, int disp,\n+                         XMMRegister xmm, KRegister mask, Register length,\n+                         Register temp, bool use64byteVector = false);\n+\n+  void fill32_masked_avx(uint shift, Register dst, int disp,\n+                         XMMRegister xmm, KRegister mask, Register length,\n+                         Register temp);\n+\n+  void fill32_avx(Register dst, int disp, XMMRegister xmm);\n+\n+  void fill64_avx(Register dst, int dis, XMMRegister xmm, bool use64byteVector = false);\n+\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":18,"deletions":2,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n-\/\/ Copyright (c) 2011, 2020, Oracle and\/or its affiliates. All rights reserved.\n+\/\/ Copyright (c) 2011, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -1116,0 +1116,2 @@\n+#include \"runtime\/vm_version.hpp\"\n+\n@@ -1604,0 +1606,1 @@\n+    case Op_ClearArray:\n","filename":"src\/hotspot\/cpu\/x86\/x86.ad","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2001, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2001, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -89,0 +89,1 @@\n+#include \"gc\/shared\/tlab_globals.hpp\"\n@@ -2844,1 +2845,1 @@\n-    double expand_ms;\n+    double expand_ms = 0.0;\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectedHeap.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -1850,3 +1850,4 @@\n-     * {@code position()}&nbsp;+&nbsp;{@code start}, and its limit will be\n-     * {@code position()}&nbsp;+&nbsp;{@code end}.  The new buffer will be\n-     * direct if, and only if, this buffer is direct, and it will be read-only\n+     * {@code position()}&nbsp;+&nbsp;{@code start}, its limit will be\n+     * {@code position()}&nbsp;+&nbsp;{@code end}, and its byte order\n+     * will be identical to that of this buffer. The new buffer will be direct\n+     * if, and only if, this buffer is direct, and it will be read-only\n","filename":"src\/java.base\/share\/classes\/java\/nio\/X-Buffer.java.template","additions":5,"deletions":4,"binary":false,"changes":9,"status":"modified"}]}